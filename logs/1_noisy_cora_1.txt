Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10484])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.528755187988281 = 1.9319204092025757 + 1.0 * 8.596835136413574
Epoch 0, val loss: 1.9294544458389282
Epoch 10, training loss: 10.518928527832031 = 1.9224886894226074 + 1.0 * 8.596439361572266
Epoch 10, val loss: 1.9205831289291382
Epoch 20, training loss: 10.503497123718262 = 1.910714030265808 + 1.0 * 8.592782974243164
Epoch 20, val loss: 1.909193754196167
Epoch 30, training loss: 10.458885192871094 = 1.8946808576583862 + 1.0 * 8.564204216003418
Epoch 30, val loss: 1.8933779001235962
Epoch 40, training loss: 10.269153594970703 = 1.8747800588607788 + 1.0 * 8.394373893737793
Epoch 40, val loss: 1.8740946054458618
Epoch 50, training loss: 9.81302261352539 = 1.8529244661331177 + 1.0 * 7.960097789764404
Epoch 50, val loss: 1.853312373161316
Epoch 60, training loss: 9.477952003479004 = 1.8351786136627197 + 1.0 * 7.642773628234863
Epoch 60, val loss: 1.837517499923706
Epoch 70, training loss: 9.089482307434082 = 1.822037935256958 + 1.0 * 7.267444610595703
Epoch 70, val loss: 1.8249369859695435
Epoch 80, training loss: 8.882125854492188 = 1.8102060556411743 + 1.0 * 7.071919918060303
Epoch 80, val loss: 1.8133254051208496
Epoch 90, training loss: 8.724990844726562 = 1.7978743314743042 + 1.0 * 6.927116870880127
Epoch 90, val loss: 1.8015280961990356
Epoch 100, training loss: 8.60033893585205 = 1.7853825092315674 + 1.0 * 6.814956188201904
Epoch 100, val loss: 1.7895089387893677
Epoch 110, training loss: 8.51829719543457 = 1.7722030878067017 + 1.0 * 6.746094226837158
Epoch 110, val loss: 1.7766436338424683
Epoch 120, training loss: 8.449263572692871 = 1.7580301761627197 + 1.0 * 6.6912336349487305
Epoch 120, val loss: 1.7630391120910645
Epoch 130, training loss: 8.392200469970703 = 1.742737054824829 + 1.0 * 6.649463653564453
Epoch 130, val loss: 1.7486181259155273
Epoch 140, training loss: 8.339930534362793 = 1.7252182960510254 + 1.0 * 6.614712238311768
Epoch 140, val loss: 1.7324925661087036
Epoch 150, training loss: 8.291313171386719 = 1.7049272060394287 + 1.0 * 6.586386203765869
Epoch 150, val loss: 1.7142835855484009
Epoch 160, training loss: 8.247098922729492 = 1.6811100244522095 + 1.0 * 6.565988540649414
Epoch 160, val loss: 1.693177342414856
Epoch 170, training loss: 8.199406623840332 = 1.653395652770996 + 1.0 * 6.546010971069336
Epoch 170, val loss: 1.6687685251235962
Epoch 180, training loss: 8.150300979614258 = 1.621142864227295 + 1.0 * 6.529158115386963
Epoch 180, val loss: 1.640534520149231
Epoch 190, training loss: 8.10380744934082 = 1.5837531089782715 + 1.0 * 6.520054817199707
Epoch 190, val loss: 1.6078475713729858
Epoch 200, training loss: 8.04096794128418 = 1.5416781902313232 + 1.0 * 6.499289512634277
Epoch 200, val loss: 1.5715653896331787
Epoch 210, training loss: 7.980843544006348 = 1.4948699474334717 + 1.0 * 6.485973358154297
Epoch 210, val loss: 1.5314432382583618
Epoch 220, training loss: 7.918210983276367 = 1.4432990550994873 + 1.0 * 6.474912166595459
Epoch 220, val loss: 1.487757682800293
Epoch 230, training loss: 7.85301399230957 = 1.3875932693481445 + 1.0 * 6.465420722961426
Epoch 230, val loss: 1.441375494003296
Epoch 240, training loss: 7.792358875274658 = 1.3301361799240112 + 1.0 * 6.462222576141357
Epoch 240, val loss: 1.394639492034912
Epoch 250, training loss: 7.72534704208374 = 1.2732235193252563 + 1.0 * 6.452123641967773
Epoch 250, val loss: 1.3490484952926636
Epoch 260, training loss: 7.660632610321045 = 1.217095971107483 + 1.0 * 6.443536758422852
Epoch 260, val loss: 1.3052327632904053
Epoch 270, training loss: 7.611250877380371 = 1.1626429557800293 + 1.0 * 6.448607921600342
Epoch 270, val loss: 1.2636511325836182
Epoch 280, training loss: 7.544643878936768 = 1.1120543479919434 + 1.0 * 6.432589530944824
Epoch 280, val loss: 1.2265325784683228
Epoch 290, training loss: 7.490882873535156 = 1.064703345298767 + 1.0 * 6.4261794090271
Epoch 290, val loss: 1.1928198337554932
Epoch 300, training loss: 7.439510345458984 = 1.0198205709457397 + 1.0 * 6.419689655303955
Epoch 300, val loss: 1.161869764328003
Epoch 310, training loss: 7.394026279449463 = 0.9769467711448669 + 1.0 * 6.417079448699951
Epoch 310, val loss: 1.1331334114074707
Epoch 320, training loss: 7.346787929534912 = 0.9361817240715027 + 1.0 * 6.410606384277344
Epoch 320, val loss: 1.106484293937683
Epoch 330, training loss: 7.304101943969727 = 0.8969032764434814 + 1.0 * 6.407198905944824
Epoch 330, val loss: 1.0814751386642456
Epoch 340, training loss: 7.262180805206299 = 0.8582849502563477 + 1.0 * 6.403895854949951
Epoch 340, val loss: 1.057167649269104
Epoch 350, training loss: 7.220808982849121 = 0.8202049732208252 + 1.0 * 6.400603771209717
Epoch 350, val loss: 1.0333123207092285
Epoch 360, training loss: 7.1781229972839355 = 0.7827243804931641 + 1.0 * 6.3953986167907715
Epoch 360, val loss: 1.0102988481521606
Epoch 370, training loss: 7.136510372161865 = 0.7457427382469177 + 1.0 * 6.390767574310303
Epoch 370, val loss: 0.9878040552139282
Epoch 380, training loss: 7.095749855041504 = 0.7090975046157837 + 1.0 * 6.38665246963501
Epoch 380, val loss: 0.965855598449707
Epoch 390, training loss: 7.069920539855957 = 0.6730842590332031 + 1.0 * 6.396836280822754
Epoch 390, val loss: 0.9447376132011414
Epoch 400, training loss: 7.022096633911133 = 0.6384694576263428 + 1.0 * 6.383627414703369
Epoch 400, val loss: 0.9250352382659912
Epoch 410, training loss: 6.981356143951416 = 0.6050739288330078 + 1.0 * 6.376282215118408
Epoch 410, val loss: 0.9070714116096497
Epoch 420, training loss: 6.946066379547119 = 0.5727138519287109 + 1.0 * 6.373352527618408
Epoch 420, val loss: 0.8904823064804077
Epoch 430, training loss: 6.912376403808594 = 0.5414259433746338 + 1.0 * 6.370950222015381
Epoch 430, val loss: 0.8754392266273499
Epoch 440, training loss: 6.879490375518799 = 0.5115284323692322 + 1.0 * 6.367961883544922
Epoch 440, val loss: 0.862178385257721
Epoch 450, training loss: 6.850805759429932 = 0.4831313490867615 + 1.0 * 6.367674350738525
Epoch 450, val loss: 0.850719153881073
Epoch 460, training loss: 6.819070816040039 = 0.4561200737953186 + 1.0 * 6.362950801849365
Epoch 460, val loss: 0.8409706354141235
Epoch 470, training loss: 6.797895908355713 = 0.43032774329185486 + 1.0 * 6.367568016052246
Epoch 470, val loss: 0.8325491547584534
Epoch 480, training loss: 6.76478385925293 = 0.4059872627258301 + 1.0 * 6.3587965965271
Epoch 480, val loss: 0.82557612657547
Epoch 490, training loss: 6.738741397857666 = 0.3828435242176056 + 1.0 * 6.355897903442383
Epoch 490, val loss: 0.8199487924575806
Epoch 500, training loss: 6.71937894821167 = 0.3608538508415222 + 1.0 * 6.358525276184082
Epoch 500, val loss: 0.8152275085449219
Epoch 510, training loss: 6.694056510925293 = 0.34004703164100647 + 1.0 * 6.354009628295898
Epoch 510, val loss: 0.8115119934082031
Epoch 520, training loss: 6.668735027313232 = 0.320262610912323 + 1.0 * 6.348472595214844
Epoch 520, val loss: 0.8087676167488098
Epoch 530, training loss: 6.648853778839111 = 0.3014095425605774 + 1.0 * 6.3474440574646
Epoch 530, val loss: 0.8068287968635559
Epoch 540, training loss: 6.6363396644592285 = 0.28354158997535706 + 1.0 * 6.352797985076904
Epoch 540, val loss: 0.8054568767547607
Epoch 550, training loss: 6.614831924438477 = 0.26679566502571106 + 1.0 * 6.348036289215088
Epoch 550, val loss: 0.8049637675285339
Epoch 560, training loss: 6.592745780944824 = 0.2509121894836426 + 1.0 * 6.341833591461182
Epoch 560, val loss: 0.8050976991653442
Epoch 570, training loss: 6.575137138366699 = 0.23578251898288727 + 1.0 * 6.339354515075684
Epoch 570, val loss: 0.8057921528816223
Epoch 580, training loss: 6.558769702911377 = 0.2213854044675827 + 1.0 * 6.337384223937988
Epoch 580, val loss: 0.8070975542068481
Epoch 590, training loss: 6.562896251678467 = 0.2077043503522873 + 1.0 * 6.355191707611084
Epoch 590, val loss: 0.8088656663894653
Epoch 600, training loss: 6.534199237823486 = 0.19497446715831757 + 1.0 * 6.339224815368652
Epoch 600, val loss: 0.8111478686332703
Epoch 610, training loss: 6.519488334655762 = 0.18301422894001007 + 1.0 * 6.3364739418029785
Epoch 610, val loss: 0.81404048204422
Epoch 620, training loss: 6.5035529136657715 = 0.17173263430595398 + 1.0 * 6.331820487976074
Epoch 620, val loss: 0.8174141645431519
Epoch 630, training loss: 6.491786956787109 = 0.1610841006040573 + 1.0 * 6.330702781677246
Epoch 630, val loss: 0.8211787343025208
Epoch 640, training loss: 6.4805521965026855 = 0.151107057929039 + 1.0 * 6.3294453620910645
Epoch 640, val loss: 0.8253262042999268
Epoch 650, training loss: 6.470391750335693 = 0.1418272852897644 + 1.0 * 6.328564643859863
Epoch 650, val loss: 0.8297865986824036
Epoch 660, training loss: 6.459957599639893 = 0.13314951956272125 + 1.0 * 6.326807975769043
Epoch 660, val loss: 0.8345882296562195
Epoch 670, training loss: 6.450527191162109 = 0.12502773106098175 + 1.0 * 6.325499534606934
Epoch 670, val loss: 0.8397062420845032
Epoch 680, training loss: 6.450922966003418 = 0.11743081361055374 + 1.0 * 6.333492279052734
Epoch 680, val loss: 0.8451189994812012
Epoch 690, training loss: 6.437094688415527 = 0.11039853096008301 + 1.0 * 6.326695919036865
Epoch 690, val loss: 0.8506779670715332
Epoch 700, training loss: 6.429428577423096 = 0.10387565195560455 + 1.0 * 6.325552940368652
Epoch 700, val loss: 0.8564996719360352
Epoch 710, training loss: 6.425171375274658 = 0.09780183434486389 + 1.0 * 6.327369689941406
Epoch 710, val loss: 0.8624916076660156
Epoch 720, training loss: 6.413054466247559 = 0.09218413382768631 + 1.0 * 6.320870399475098
Epoch 720, val loss: 0.8686245083808899
Epoch 730, training loss: 6.405343532562256 = 0.08693591505289078 + 1.0 * 6.3184075355529785
Epoch 730, val loss: 0.874956488609314
Epoch 740, training loss: 6.40199089050293 = 0.0820346549153328 + 1.0 * 6.319956302642822
Epoch 740, val loss: 0.8814500570297241
Epoch 750, training loss: 6.396281719207764 = 0.07748320698738098 + 1.0 * 6.318798542022705
Epoch 750, val loss: 0.8879727721214294
Epoch 760, training loss: 6.38986873626709 = 0.07326795905828476 + 1.0 * 6.316600799560547
Epoch 760, val loss: 0.8945301175117493
Epoch 770, training loss: 6.383695125579834 = 0.06933936476707458 + 1.0 * 6.314355850219727
Epoch 770, val loss: 0.9011680483818054
Epoch 780, training loss: 6.381796836853027 = 0.06567778438329697 + 1.0 * 6.316119194030762
Epoch 780, val loss: 0.9078617691993713
Epoch 790, training loss: 6.3807806968688965 = 0.06227321922779083 + 1.0 * 6.318507671356201
Epoch 790, val loss: 0.9145581126213074
Epoch 800, training loss: 6.373816967010498 = 0.05911954119801521 + 1.0 * 6.314697265625
Epoch 800, val loss: 0.9211094379425049
Epoch 810, training loss: 6.366714954376221 = 0.056176524609327316 + 1.0 * 6.310538291931152
Epoch 810, val loss: 0.9277283549308777
Epoch 820, training loss: 6.36357307434082 = 0.0534256249666214 + 1.0 * 6.310147285461426
Epoch 820, val loss: 0.9344192743301392
Epoch 830, training loss: 6.3661932945251465 = 0.0508476123213768 + 1.0 * 6.315345764160156
Epoch 830, val loss: 0.9410490393638611
Epoch 840, training loss: 6.358839511871338 = 0.04845287650823593 + 1.0 * 6.310386657714844
Epoch 840, val loss: 0.9475284218788147
Epoch 850, training loss: 6.356052875518799 = 0.04621037095785141 + 1.0 * 6.309842586517334
Epoch 850, val loss: 0.9540085196495056
Epoch 860, training loss: 6.3578314781188965 = 0.044119443744421005 + 1.0 * 6.313712120056152
Epoch 860, val loss: 0.960373044013977
Epoch 870, training loss: 6.348635196685791 = 0.04216485098004341 + 1.0 * 6.3064703941345215
Epoch 870, val loss: 0.9667332172393799
Epoch 880, training loss: 6.34609317779541 = 0.04032844677567482 + 1.0 * 6.305764675140381
Epoch 880, val loss: 0.9729991555213928
Epoch 890, training loss: 6.343597888946533 = 0.03860465809702873 + 1.0 * 6.304993152618408
Epoch 890, val loss: 0.97919762134552
Epoch 900, training loss: 6.338828086853027 = 0.036981672048568726 + 1.0 * 6.301846504211426
Epoch 900, val loss: 0.9853222966194153
Epoch 910, training loss: 6.351817607879639 = 0.03545311838388443 + 1.0 * 6.316364288330078
Epoch 910, val loss: 0.9913400411605835
Epoch 920, training loss: 6.338507175445557 = 0.03403589874505997 + 1.0 * 6.304471492767334
Epoch 920, val loss: 0.9972524046897888
Epoch 930, training loss: 6.331465721130371 = 0.03269122913479805 + 1.0 * 6.298774719238281
Epoch 930, val loss: 1.0030815601348877
Epoch 940, training loss: 6.331343650817871 = 0.031420592218637466 + 1.0 * 6.299922943115234
Epoch 940, val loss: 1.0088245868682861
Epoch 950, training loss: 6.3325395584106445 = 0.030225802212953568 + 1.0 * 6.302313804626465
Epoch 950, val loss: 1.0145405530929565
Epoch 960, training loss: 6.325948238372803 = 0.029096415266394615 + 1.0 * 6.296851634979248
Epoch 960, val loss: 1.0200693607330322
Epoch 970, training loss: 6.324265003204346 = 0.028028782457113266 + 1.0 * 6.296236038208008
Epoch 970, val loss: 1.0255407094955444
Epoch 980, training loss: 6.327538967132568 = 0.02701643481850624 + 1.0 * 6.300522327423096
Epoch 980, val loss: 1.0310156345367432
Epoch 990, training loss: 6.3274335861206055 = 0.02606089599430561 + 1.0 * 6.301372528076172
Epoch 990, val loss: 1.036289095878601
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 10.555158615112305 = 1.9583358764648438 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.9574857950210571
Epoch 10, training loss: 10.544551849365234 = 1.9481027126312256 + 1.0 * 8.59644889831543
Epoch 10, val loss: 1.9477430582046509
Epoch 20, training loss: 10.52808666229248 = 1.9352327585220337 + 1.0 * 8.592853546142578
Epoch 20, val loss: 1.935022234916687
Epoch 30, training loss: 10.48085880279541 = 1.9171178340911865 + 1.0 * 8.563740730285645
Epoch 30, val loss: 1.9169937372207642
Epoch 40, training loss: 10.2799654006958 = 1.894021987915039 + 1.0 * 8.385943412780762
Epoch 40, val loss: 1.8949991464614868
Epoch 50, training loss: 9.80702018737793 = 1.8680899143218994 + 1.0 * 7.938930034637451
Epoch 50, val loss: 1.8709560632705688
Epoch 60, training loss: 9.499898910522461 = 1.8446284532546997 + 1.0 * 7.655270576477051
Epoch 60, val loss: 1.8497430086135864
Epoch 70, training loss: 9.078877449035645 = 1.8279751539230347 + 1.0 * 7.2509026527404785
Epoch 70, val loss: 1.8345115184783936
Epoch 80, training loss: 8.7825288772583 = 1.8151466846466064 + 1.0 * 6.967382431030273
Epoch 80, val loss: 1.8228297233581543
Epoch 90, training loss: 8.623537063598633 = 1.8008661270141602 + 1.0 * 6.822670936584473
Epoch 90, val loss: 1.809752106666565
Epoch 100, training loss: 8.51035213470459 = 1.785377025604248 + 1.0 * 6.724975109100342
Epoch 100, val loss: 1.7959787845611572
Epoch 110, training loss: 8.43746566772461 = 1.770964503288269 + 1.0 * 6.666501045227051
Epoch 110, val loss: 1.7830063104629517
Epoch 120, training loss: 8.383462905883789 = 1.7563586235046387 + 1.0 * 6.62710428237915
Epoch 120, val loss: 1.769660234451294
Epoch 130, training loss: 8.332934379577637 = 1.740373134613037 + 1.0 * 6.5925612449646
Epoch 130, val loss: 1.7552450895309448
Epoch 140, training loss: 8.286725997924805 = 1.7226734161376953 + 1.0 * 6.564052581787109
Epoch 140, val loss: 1.7396876811981201
Epoch 150, training loss: 8.239742279052734 = 1.7027956247329712 + 1.0 * 6.536946773529053
Epoch 150, val loss: 1.72243332862854
Epoch 160, training loss: 8.192049980163574 = 1.6800123453140259 + 1.0 * 6.51203727722168
Epoch 160, val loss: 1.7027403116226196
Epoch 170, training loss: 8.14454460144043 = 1.654067873954773 + 1.0 * 6.490477085113525
Epoch 170, val loss: 1.6804912090301514
Epoch 180, training loss: 8.09742259979248 = 1.624411702156067 + 1.0 * 6.473011016845703
Epoch 180, val loss: 1.6548638343811035
Epoch 190, training loss: 8.050393104553223 = 1.5905200242996216 + 1.0 * 6.459872722625732
Epoch 190, val loss: 1.6254658699035645
Epoch 200, training loss: 8.000031471252441 = 1.5526310205459595 + 1.0 * 6.4474005699157715
Epoch 200, val loss: 1.5925177335739136
Epoch 210, training loss: 7.947375297546387 = 1.510948896408081 + 1.0 * 6.436426639556885
Epoch 210, val loss: 1.5564112663269043
Epoch 220, training loss: 7.894917011260986 = 1.466349720954895 + 1.0 * 6.428567409515381
Epoch 220, val loss: 1.5180772542953491
Epoch 230, training loss: 7.8381805419921875 = 1.4195533990859985 + 1.0 * 6.4186272621154785
Epoch 230, val loss: 1.4784364700317383
Epoch 240, training loss: 7.781712532043457 = 1.3710254430770874 + 1.0 * 6.41068696975708
Epoch 240, val loss: 1.438193917274475
Epoch 250, training loss: 7.726998329162598 = 1.3217628002166748 + 1.0 * 6.405235767364502
Epoch 250, val loss: 1.3983962535858154
Epoch 260, training loss: 7.672403335571289 = 1.2730824947357178 + 1.0 * 6.39932107925415
Epoch 260, val loss: 1.3600635528564453
Epoch 270, training loss: 7.6178059577941895 = 1.2249308824539185 + 1.0 * 6.3928751945495605
Epoch 270, val loss: 1.32309889793396
Epoch 280, training loss: 7.572206497192383 = 1.1775211095809937 + 1.0 * 6.3946852684021
Epoch 280, val loss: 1.2875665426254272
Epoch 290, training loss: 7.515849590301514 = 1.131906509399414 + 1.0 * 6.3839430809021
Epoch 290, val loss: 1.2539851665496826
Epoch 300, training loss: 7.465351581573486 = 1.0874089002609253 + 1.0 * 6.3779425621032715
Epoch 300, val loss: 1.2221125364303589
Epoch 310, training loss: 7.421424865722656 = 1.0438587665557861 + 1.0 * 6.377565860748291
Epoch 310, val loss: 1.1913056373596191
Epoch 320, training loss: 7.371548652648926 = 1.0016324520111084 + 1.0 * 6.3699164390563965
Epoch 320, val loss: 1.161696434020996
Epoch 330, training loss: 7.326144695281982 = 0.9606414437294006 + 1.0 * 6.365503311157227
Epoch 330, val loss: 1.133471131324768
Epoch 340, training loss: 7.283885478973389 = 0.9205292463302612 + 1.0 * 6.363356113433838
Epoch 340, val loss: 1.1060447692871094
Epoch 350, training loss: 7.244626522064209 = 0.8814853429794312 + 1.0 * 6.363141059875488
Epoch 350, val loss: 1.0794352293014526
Epoch 360, training loss: 7.198314189910889 = 0.8435465693473816 + 1.0 * 6.354767799377441
Epoch 360, val loss: 1.0537590980529785
Epoch 370, training loss: 7.158326625823975 = 0.8064207434654236 + 1.0 * 6.351905822753906
Epoch 370, val loss: 1.0288735628128052
Epoch 380, training loss: 7.120614051818848 = 0.7702475190162659 + 1.0 * 6.350366592407227
Epoch 380, val loss: 1.0046201944351196
Epoch 390, training loss: 7.081586837768555 = 0.7352954745292664 + 1.0 * 6.346291542053223
Epoch 390, val loss: 0.9816496968269348
Epoch 400, training loss: 7.04671573638916 = 0.7011959552764893 + 1.0 * 6.34552001953125
Epoch 400, val loss: 0.9596030712127686
Epoch 410, training loss: 7.011281967163086 = 0.6682433485984802 + 1.0 * 6.343038558959961
Epoch 410, val loss: 0.9384089708328247
Epoch 420, training loss: 6.974120616912842 = 0.6361972689628601 + 1.0 * 6.337923526763916
Epoch 420, val loss: 0.9186196327209473
Epoch 430, training loss: 6.940535068511963 = 0.6049833297729492 + 1.0 * 6.335551738739014
Epoch 430, val loss: 0.8999033570289612
Epoch 440, training loss: 6.909886837005615 = 0.5746622681617737 + 1.0 * 6.335224628448486
Epoch 440, val loss: 0.8820908665657043
Epoch 450, training loss: 6.877160549163818 = 0.5453513860702515 + 1.0 * 6.331809043884277
Epoch 450, val loss: 0.8658275008201599
Epoch 460, training loss: 6.84954309463501 = 0.5168429017066956 + 1.0 * 6.332700252532959
Epoch 460, val loss: 0.8505139350891113
Epoch 470, training loss: 6.818251132965088 = 0.48919835686683655 + 1.0 * 6.329052925109863
Epoch 470, val loss: 0.836311936378479
Epoch 480, training loss: 6.7878804206848145 = 0.46236947178840637 + 1.0 * 6.3255109786987305
Epoch 480, val loss: 0.8232581615447998
Epoch 490, training loss: 6.764327526092529 = 0.4363686740398407 + 1.0 * 6.327959060668945
Epoch 490, val loss: 0.8112220764160156
Epoch 500, training loss: 6.7415995597839355 = 0.4114871919155121 + 1.0 * 6.330112457275391
Epoch 500, val loss: 0.8000044226646423
Epoch 510, training loss: 6.708739280700684 = 0.3878183662891388 + 1.0 * 6.320920944213867
Epoch 510, val loss: 0.790177583694458
Epoch 520, training loss: 6.682929515838623 = 0.3652561604976654 + 1.0 * 6.317673206329346
Epoch 520, val loss: 0.7813345193862915
Epoch 530, training loss: 6.661952495574951 = 0.3437100052833557 + 1.0 * 6.31824254989624
Epoch 530, val loss: 0.7732948064804077
Epoch 540, training loss: 6.638963222503662 = 0.32327398657798767 + 1.0 * 6.3156890869140625
Epoch 540, val loss: 0.7660331726074219
Epoch 550, training loss: 6.619051456451416 = 0.3040449321269989 + 1.0 * 6.315006732940674
Epoch 550, val loss: 0.7600345015525818
Epoch 560, training loss: 6.598684310913086 = 0.28582674264907837 + 1.0 * 6.312857627868652
Epoch 560, val loss: 0.7547620534896851
Epoch 570, training loss: 6.58881950378418 = 0.2687075436115265 + 1.0 * 6.3201117515563965
Epoch 570, val loss: 0.7501927018165588
Epoch 580, training loss: 6.565630912780762 = 0.2527473270893097 + 1.0 * 6.312883377075195
Epoch 580, val loss: 0.7466164231300354
Epoch 590, training loss: 6.548186302185059 = 0.23778516054153442 + 1.0 * 6.31040096282959
Epoch 590, val loss: 0.7438604831695557
Epoch 600, training loss: 6.530008792877197 = 0.2237873375415802 + 1.0 * 6.3062214851379395
Epoch 600, val loss: 0.7417172193527222
Epoch 610, training loss: 6.523824691772461 = 0.21071794629096985 + 1.0 * 6.313106536865234
Epoch 610, val loss: 0.7402657270431519
Epoch 620, training loss: 6.505669593811035 = 0.19853316247463226 + 1.0 * 6.307136535644531
Epoch 620, val loss: 0.739529013633728
Epoch 630, training loss: 6.493485927581787 = 0.18715879321098328 + 1.0 * 6.3063273429870605
Epoch 630, val loss: 0.7394688129425049
Epoch 640, training loss: 6.478386878967285 = 0.1765412986278534 + 1.0 * 6.301845550537109
Epoch 640, val loss: 0.7396280765533447
Epoch 650, training loss: 6.468751430511475 = 0.16664540767669678 + 1.0 * 6.302105903625488
Epoch 650, val loss: 0.740730345249176
Epoch 660, training loss: 6.455625534057617 = 0.15735742449760437 + 1.0 * 6.2982683181762695
Epoch 660, val loss: 0.7420831322669983
Epoch 670, training loss: 6.458695411682129 = 0.1486392319202423 + 1.0 * 6.310056209564209
Epoch 670, val loss: 0.7437930703163147
Epoch 680, training loss: 6.438773155212402 = 0.14053021371364594 + 1.0 * 6.298243045806885
Epoch 680, val loss: 0.7457044720649719
Epoch 690, training loss: 6.428489685058594 = 0.1329849511384964 + 1.0 * 6.295504570007324
Epoch 690, val loss: 0.7484531402587891
Epoch 700, training loss: 6.420170307159424 = 0.12588393688201904 + 1.0 * 6.294286251068115
Epoch 700, val loss: 0.7513042688369751
Epoch 710, training loss: 6.412143230438232 = 0.11919207125902176 + 1.0 * 6.2929511070251465
Epoch 710, val loss: 0.7543957829475403
Epoch 720, training loss: 6.419498920440674 = 0.11290013045072556 + 1.0 * 6.306598663330078
Epoch 720, val loss: 0.7577628493309021
Epoch 730, training loss: 6.407436847686768 = 0.10704229027032852 + 1.0 * 6.300394535064697
Epoch 730, val loss: 0.7614197731018066
Epoch 740, training loss: 6.394944190979004 = 0.10156586766242981 + 1.0 * 6.2933783531188965
Epoch 740, val loss: 0.7652971148490906
Epoch 750, training loss: 6.387068271636963 = 0.09643383324146271 + 1.0 * 6.290634632110596
Epoch 750, val loss: 0.7695451378822327
Epoch 760, training loss: 6.380145072937012 = 0.09159291535615921 + 1.0 * 6.288552284240723
Epoch 760, val loss: 0.7737683653831482
Epoch 770, training loss: 6.376585006713867 = 0.08702965080738068 + 1.0 * 6.289555549621582
Epoch 770, val loss: 0.7781816720962524
Epoch 780, training loss: 6.379688739776611 = 0.0827547162771225 + 1.0 * 6.296934127807617
Epoch 780, val loss: 0.7826752066612244
Epoch 790, training loss: 6.370716571807861 = 0.07878635823726654 + 1.0 * 6.291930198669434
Epoch 790, val loss: 0.7872893214225769
Epoch 800, training loss: 6.360426902770996 = 0.07506101578474045 + 1.0 * 6.285366058349609
Epoch 800, val loss: 0.7922736406326294
Epoch 810, training loss: 6.355777740478516 = 0.07154414057731628 + 1.0 * 6.284233570098877
Epoch 810, val loss: 0.797179639339447
Epoch 820, training loss: 6.352324962615967 = 0.06822207570075989 + 1.0 * 6.284102916717529
Epoch 820, val loss: 0.8021886944770813
Epoch 830, training loss: 6.34822416305542 = 0.06510696560144424 + 1.0 * 6.283117294311523
Epoch 830, val loss: 0.8070382475852966
Epoch 840, training loss: 6.34477424621582 = 0.062205903232097626 + 1.0 * 6.282568454742432
Epoch 840, val loss: 0.8122607469558716
Epoch 850, training loss: 6.340936660766602 = 0.059469643980264664 + 1.0 * 6.281466960906982
Epoch 850, val loss: 0.8175964951515198
Epoch 860, training loss: 6.339755058288574 = 0.05688249692320824 + 1.0 * 6.282872676849365
Epoch 860, val loss: 0.822773277759552
Epoch 870, training loss: 6.335124969482422 = 0.054450422525405884 + 1.0 * 6.280674457550049
Epoch 870, val loss: 0.8278286457061768
Epoch 880, training loss: 6.33308219909668 = 0.05216347426176071 + 1.0 * 6.280918598175049
Epoch 880, val loss: 0.833162784576416
Epoch 890, training loss: 6.327726364135742 = 0.050006523728370667 + 1.0 * 6.277719974517822
Epoch 890, val loss: 0.8385263085365295
Epoch 900, training loss: 6.326428413391113 = 0.04795994609594345 + 1.0 * 6.278468608856201
Epoch 900, val loss: 0.8437380194664001
Epoch 910, training loss: 6.329708099365234 = 0.04602494835853577 + 1.0 * 6.2836833000183105
Epoch 910, val loss: 0.8487713932991028
Epoch 920, training loss: 6.325605869293213 = 0.04422738403081894 + 1.0 * 6.281378269195557
Epoch 920, val loss: 0.8539509177207947
Epoch 930, training loss: 6.318489074707031 = 0.04252012073993683 + 1.0 * 6.2759690284729
Epoch 930, val loss: 0.8593738675117493
Epoch 940, training loss: 6.315683841705322 = 0.040896911174058914 + 1.0 * 6.274786949157715
Epoch 940, val loss: 0.8645095825195312
Epoch 950, training loss: 6.316066265106201 = 0.039352864027023315 + 1.0 * 6.2767133712768555
Epoch 950, val loss: 0.869560182094574
Epoch 960, training loss: 6.314156532287598 = 0.03789043426513672 + 1.0 * 6.276266098022461
Epoch 960, val loss: 0.8745802044868469
Epoch 970, training loss: 6.312090873718262 = 0.036508917808532715 + 1.0 * 6.2755818367004395
Epoch 970, val loss: 0.8797864317893982
Epoch 980, training loss: 6.3087687492370605 = 0.03519205003976822 + 1.0 * 6.273576736450195
Epoch 980, val loss: 0.8848620653152466
Epoch 990, training loss: 6.3114190101623535 = 0.033941663801670074 + 1.0 * 6.277477264404297
Epoch 990, val loss: 0.8898034691810608
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 10.525376319885254 = 1.928519606590271 + 1.0 * 8.596857070922852
Epoch 0, val loss: 1.9309157133102417
Epoch 10, training loss: 10.516135215759277 = 1.9195233583450317 + 1.0 * 8.596611976623535
Epoch 10, val loss: 1.9214396476745605
Epoch 20, training loss: 10.50246524810791 = 1.9078667163848877 + 1.0 * 8.594598770141602
Epoch 20, val loss: 1.9091672897338867
Epoch 30, training loss: 10.469889640808105 = 1.891193151473999 + 1.0 * 8.578696250915527
Epoch 30, val loss: 1.8919501304626465
Epoch 40, training loss: 10.364086151123047 = 1.8684395551681519 + 1.0 * 8.495646476745605
Epoch 40, val loss: 1.8696414232254028
Epoch 50, training loss: 9.936897277832031 = 1.8440184593200684 + 1.0 * 8.092879295349121
Epoch 50, val loss: 1.847232699394226
Epoch 60, training loss: 9.528553009033203 = 1.8202447891235352 + 1.0 * 7.70830774307251
Epoch 60, val loss: 1.8273794651031494
Epoch 70, training loss: 9.28090763092041 = 1.8019980192184448 + 1.0 * 7.478909492492676
Epoch 70, val loss: 1.811800479888916
Epoch 80, training loss: 9.155558586120605 = 1.785367727279663 + 1.0 * 7.370190620422363
Epoch 80, val loss: 1.7971173524856567
Epoch 90, training loss: 8.984111785888672 = 1.7679152488708496 + 1.0 * 7.216196537017822
Epoch 90, val loss: 1.7819222211837769
Epoch 100, training loss: 8.761672019958496 = 1.7514318227767944 + 1.0 * 7.010240077972412
Epoch 100, val loss: 1.7678701877593994
Epoch 110, training loss: 8.602294921875 = 1.734716773033142 + 1.0 * 6.867578506469727
Epoch 110, val loss: 1.7524538040161133
Epoch 120, training loss: 8.499367713928223 = 1.7140483856201172 + 1.0 * 6.7853193283081055
Epoch 120, val loss: 1.7336952686309814
Epoch 130, training loss: 8.419317245483398 = 1.6883091926574707 + 1.0 * 6.731008052825928
Epoch 130, val loss: 1.711174726486206
Epoch 140, training loss: 8.345261573791504 = 1.6590712070465088 + 1.0 * 6.686190128326416
Epoch 140, val loss: 1.6858797073364258
Epoch 150, training loss: 8.275330543518066 = 1.6260626316070557 + 1.0 * 6.649267673492432
Epoch 150, val loss: 1.6572035551071167
Epoch 160, training loss: 8.20536994934082 = 1.5876622200012207 + 1.0 * 6.617708206176758
Epoch 160, val loss: 1.6244760751724243
Epoch 170, training loss: 8.136451721191406 = 1.5444599390029907 + 1.0 * 6.591991424560547
Epoch 170, val loss: 1.5888373851776123
Epoch 180, training loss: 8.067770004272461 = 1.4976062774658203 + 1.0 * 6.570163726806641
Epoch 180, val loss: 1.550736665725708
Epoch 190, training loss: 7.999173164367676 = 1.4483933448791504 + 1.0 * 6.550779819488525
Epoch 190, val loss: 1.510776162147522
Epoch 200, training loss: 7.932362079620361 = 1.3983243703842163 + 1.0 * 6.5340375900268555
Epoch 200, val loss: 1.4705581665039062
Epoch 210, training loss: 7.872585296630859 = 1.3490768671035767 + 1.0 * 6.523508548736572
Epoch 210, val loss: 1.431585431098938
Epoch 220, training loss: 7.809751987457275 = 1.3017778396606445 + 1.0 * 6.507974147796631
Epoch 220, val loss: 1.3947746753692627
Epoch 230, training loss: 7.750237464904785 = 1.2561250925064087 + 1.0 * 6.494112491607666
Epoch 230, val loss: 1.359976053237915
Epoch 240, training loss: 7.6947174072265625 = 1.2119789123535156 + 1.0 * 6.482738494873047
Epoch 240, val loss: 1.3268229961395264
Epoch 250, training loss: 7.644505500793457 = 1.169555425643921 + 1.0 * 6.474950313568115
Epoch 250, val loss: 1.2952543497085571
Epoch 260, training loss: 7.593380451202393 = 1.1292425394058228 + 1.0 * 6.464138031005859
Epoch 260, val loss: 1.265600562095642
Epoch 270, training loss: 7.545951843261719 = 1.090141773223877 + 1.0 * 6.455810070037842
Epoch 270, val loss: 1.2373133897781372
Epoch 280, training loss: 7.501895427703857 = 1.0517544746398926 + 1.0 * 6.450140953063965
Epoch 280, val loss: 1.2097867727279663
Epoch 290, training loss: 7.462233543395996 = 1.014431357383728 + 1.0 * 6.4478020668029785
Epoch 290, val loss: 1.1830641031265259
Epoch 300, training loss: 7.412933349609375 = 0.9781797528266907 + 1.0 * 6.43475341796875
Epoch 300, val loss: 1.157296061515808
Epoch 310, training loss: 7.37178373336792 = 0.942383348941803 + 1.0 * 6.429400444030762
Epoch 310, val loss: 1.1320565938949585
Epoch 320, training loss: 7.330330848693848 = 0.9066131711006165 + 1.0 * 6.423717498779297
Epoch 320, val loss: 1.1067225933074951
Epoch 330, training loss: 7.29300594329834 = 0.8707892894744873 + 1.0 * 6.422216892242432
Epoch 330, val loss: 1.0814048051834106
Epoch 340, training loss: 7.250504493713379 = 0.8353416323661804 + 1.0 * 6.415163040161133
Epoch 340, val loss: 1.0562564134597778
Epoch 350, training loss: 7.209003925323486 = 0.8003554940223694 + 1.0 * 6.408648490905762
Epoch 350, val loss: 1.0319750308990479
Epoch 360, training loss: 7.169557571411133 = 0.7656782269477844 + 1.0 * 6.403879165649414
Epoch 360, val loss: 1.008062720298767
Epoch 370, training loss: 7.146457672119141 = 0.731456458568573 + 1.0 * 6.415001392364502
Epoch 370, val loss: 0.9850242137908936
Epoch 380, training loss: 7.0956926345825195 = 0.6986860036849976 + 1.0 * 6.397006511688232
Epoch 380, val loss: 0.9634742736816406
Epoch 390, training loss: 7.059451103210449 = 0.6671227812767029 + 1.0 * 6.392328262329102
Epoch 390, val loss: 0.943860650062561
Epoch 400, training loss: 7.0250959396362305 = 0.6365419030189514 + 1.0 * 6.388554096221924
Epoch 400, val loss: 0.9255445599555969
Epoch 410, training loss: 6.991866588592529 = 0.6068586707115173 + 1.0 * 6.385007858276367
Epoch 410, val loss: 0.9087700247764587
Epoch 420, training loss: 6.959716796875 = 0.5782210826873779 + 1.0 * 6.381495475769043
Epoch 420, val loss: 0.8934863209724426
Epoch 430, training loss: 6.9289326667785645 = 0.5507169365882874 + 1.0 * 6.378215789794922
Epoch 430, val loss: 0.8800087571144104
Epoch 440, training loss: 6.903276443481445 = 0.5241055488586426 + 1.0 * 6.379170894622803
Epoch 440, val loss: 0.8678330779075623
Epoch 450, training loss: 6.872857570648193 = 0.4983850419521332 + 1.0 * 6.374472618103027
Epoch 450, val loss: 0.8569457530975342
Epoch 460, training loss: 6.843507289886475 = 0.47355586290359497 + 1.0 * 6.369951248168945
Epoch 460, val loss: 0.8474749326705933
Epoch 470, training loss: 6.816237449645996 = 0.44946858286857605 + 1.0 * 6.366768836975098
Epoch 470, val loss: 0.8390935659408569
Epoch 480, training loss: 6.796392917633057 = 0.42616191506385803 + 1.0 * 6.3702311515808105
Epoch 480, val loss: 0.83177250623703
Epoch 490, training loss: 6.769620895385742 = 0.40389910340309143 + 1.0 * 6.365721702575684
Epoch 490, val loss: 0.8252296447753906
Epoch 500, training loss: 6.744308948516846 = 0.38264915347099304 + 1.0 * 6.361660003662109
Epoch 500, val loss: 0.8200309872627258
Epoch 510, training loss: 6.72479248046875 = 0.36233997344970703 + 1.0 * 6.362452507019043
Epoch 510, val loss: 0.8154782652854919
Epoch 520, training loss: 6.699112892150879 = 0.3429795205593109 + 1.0 * 6.356133460998535
Epoch 520, val loss: 0.8119052052497864
Epoch 530, training loss: 6.678053855895996 = 0.32451605796813965 + 1.0 * 6.3535380363464355
Epoch 530, val loss: 0.8091063499450684
Epoch 540, training loss: 6.661038875579834 = 0.3068728446960449 + 1.0 * 6.354166030883789
Epoch 540, val loss: 0.8069065809249878
Epoch 550, training loss: 6.641739845275879 = 0.29004666209220886 + 1.0 * 6.351693153381348
Epoch 550, val loss: 0.8055084347724915
Epoch 560, training loss: 6.625943183898926 = 0.27406933903694153 + 1.0 * 6.351873874664307
Epoch 560, val loss: 0.8046380281448364
Epoch 570, training loss: 6.604238033294678 = 0.25885263085365295 + 1.0 * 6.345385551452637
Epoch 570, val loss: 0.8045147657394409
Epoch 580, training loss: 6.602622985839844 = 0.2443748414516449 + 1.0 * 6.358248233795166
Epoch 580, val loss: 0.8047294020652771
Epoch 590, training loss: 6.57625675201416 = 0.23072531819343567 + 1.0 * 6.345531463623047
Epoch 590, val loss: 0.8056032657623291
Epoch 600, training loss: 6.5586256980896 = 0.21777519583702087 + 1.0 * 6.340850353240967
Epoch 600, val loss: 0.807037889957428
Epoch 610, training loss: 6.544393062591553 = 0.20546641945838928 + 1.0 * 6.338926792144775
Epoch 610, val loss: 0.8087709546089172
Epoch 620, training loss: 6.530209541320801 = 0.19381052255630493 + 1.0 * 6.336399078369141
Epoch 620, val loss: 0.8108257055282593
Epoch 630, training loss: 6.517101764678955 = 0.18280437588691711 + 1.0 * 6.334297180175781
Epoch 630, val loss: 0.8135054111480713
Epoch 640, training loss: 6.505023956298828 = 0.1723872572183609 + 1.0 * 6.332636833190918
Epoch 640, val loss: 0.8164548873901367
Epoch 650, training loss: 6.504124164581299 = 0.16253948211669922 + 1.0 * 6.3415846824646
Epoch 650, val loss: 0.8196743130683899
Epoch 660, training loss: 6.485487937927246 = 0.15331539511680603 + 1.0 * 6.332172393798828
Epoch 660, val loss: 0.823042094707489
Epoch 670, training loss: 6.472810745239258 = 0.14462965726852417 + 1.0 * 6.328181266784668
Epoch 670, val loss: 0.8267951607704163
Epoch 680, training loss: 6.462681770324707 = 0.13643808662891388 + 1.0 * 6.326243877410889
Epoch 680, val loss: 0.8307067155838013
Epoch 690, training loss: 6.454161643981934 = 0.12871256470680237 + 1.0 * 6.325448989868164
Epoch 690, val loss: 0.8349047899246216
Epoch 700, training loss: 6.455084800720215 = 0.12147291004657745 + 1.0 * 6.333611965179443
Epoch 700, val loss: 0.8387672305107117
Epoch 710, training loss: 6.443230628967285 = 0.11474660784006119 + 1.0 * 6.328484058380127
Epoch 710, val loss: 0.843346118927002
Epoch 720, training loss: 6.429245471954346 = 0.10843988507986069 + 1.0 * 6.320805549621582
Epoch 720, val loss: 0.847963273525238
Epoch 730, training loss: 6.4221601486206055 = 0.10250835865736008 + 1.0 * 6.3196516036987305
Epoch 730, val loss: 0.8525218963623047
Epoch 740, training loss: 6.415093421936035 = 0.09692525118589401 + 1.0 * 6.3181681632995605
Epoch 740, val loss: 0.8572735786437988
Epoch 750, training loss: 6.408405780792236 = 0.09167240560054779 + 1.0 * 6.316733360290527
Epoch 750, val loss: 0.8621243238449097
Epoch 760, training loss: 6.416114807128906 = 0.086738720536232 + 1.0 * 6.329376220703125
Epoch 760, val loss: 0.8668994903564453
Epoch 770, training loss: 6.400767803192139 = 0.08214486390352249 + 1.0 * 6.318623065948486
Epoch 770, val loss: 0.8717259764671326
Epoch 780, training loss: 6.39312219619751 = 0.07786138355731964 + 1.0 * 6.315260887145996
Epoch 780, val loss: 0.876788318157196
Epoch 790, training loss: 6.387249946594238 = 0.07384856790304184 + 1.0 * 6.313401222229004
Epoch 790, val loss: 0.8817193508148193
Epoch 800, training loss: 6.389601230621338 = 0.0700930505990982 + 1.0 * 6.319508075714111
Epoch 800, val loss: 0.8866181373596191
Epoch 810, training loss: 6.377814292907715 = 0.06657509505748749 + 1.0 * 6.311239242553711
Epoch 810, val loss: 0.8914048671722412
Epoch 820, training loss: 6.372451305389404 = 0.0632973164319992 + 1.0 * 6.309154033660889
Epoch 820, val loss: 0.8964821100234985
Epoch 830, training loss: 6.368666648864746 = 0.060224536806344986 + 1.0 * 6.308442115783691
Epoch 830, val loss: 0.9014583826065063
Epoch 840, training loss: 6.367038726806641 = 0.05734012648463249 + 1.0 * 6.309698581695557
Epoch 840, val loss: 0.906402051448822
Epoch 850, training loss: 6.361888885498047 = 0.05463730916380882 + 1.0 * 6.307251453399658
Epoch 850, val loss: 0.9110972285270691
Epoch 860, training loss: 6.357464790344238 = 0.05210835859179497 + 1.0 * 6.305356502532959
Epoch 860, val loss: 0.9161171317100525
Epoch 870, training loss: 6.357098579406738 = 0.049738477915525436 + 1.0 * 6.3073601722717285
Epoch 870, val loss: 0.9210001826286316
Epoch 880, training loss: 6.353681564331055 = 0.04751858115196228 + 1.0 * 6.3061628341674805
Epoch 880, val loss: 0.9255965948104858
Epoch 890, training loss: 6.348874568939209 = 0.04544055834412575 + 1.0 * 6.303433895111084
Epoch 890, val loss: 0.930544376373291
Epoch 900, training loss: 6.344906806945801 = 0.04348163679242134 + 1.0 * 6.301424980163574
Epoch 900, val loss: 0.9353566765785217
Epoch 910, training loss: 6.346735954284668 = 0.0416402630507946 + 1.0 * 6.305095672607422
Epoch 910, val loss: 0.9401030540466309
Epoch 920, training loss: 6.345487594604492 = 0.0399116650223732 + 1.0 * 6.305575847625732
Epoch 920, val loss: 0.9447224736213684
Epoch 930, training loss: 6.338871479034424 = 0.03828023001551628 + 1.0 * 6.300591468811035
Epoch 930, val loss: 0.9494054913520813
Epoch 940, training loss: 6.334765434265137 = 0.03674919158220291 + 1.0 * 6.29801607131958
Epoch 940, val loss: 0.9541233777999878
Epoch 950, training loss: 6.334626197814941 = 0.03530089184641838 + 1.0 * 6.299325466156006
Epoch 950, val loss: 0.958626389503479
Epoch 960, training loss: 6.337306022644043 = 0.033935271203517914 + 1.0 * 6.303370952606201
Epoch 960, val loss: 0.9629580974578857
Epoch 970, training loss: 6.329629421234131 = 0.03265392407774925 + 1.0 * 6.296975612640381
Epoch 970, val loss: 0.9676278829574585
Epoch 980, training loss: 6.327548980712891 = 0.031441450119018555 + 1.0 * 6.296107292175293
Epoch 980, val loss: 0.9722175598144531
Epoch 990, training loss: 6.334808349609375 = 0.030294178053736687 + 1.0 * 6.304514408111572
Epoch 990, val loss: 0.976508378982544
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.816025303110174
The final CL Acc:0.78765, 0.00873, The final GNN Acc:0.81304, 0.00221
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13150])
remove edge: torch.Size([2, 7964])
updated graph: torch.Size([2, 10558])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.534804344177246 = 1.9379438161849976 + 1.0 * 8.596860885620117
Epoch 0, val loss: 1.9382622241973877
Epoch 10, training loss: 10.525003433227539 = 1.9283853769302368 + 1.0 * 8.596617698669434
Epoch 10, val loss: 1.9284337759017944
Epoch 20, training loss: 10.510662078857422 = 1.9164007902145386 + 1.0 * 8.594261169433594
Epoch 20, val loss: 1.9159295558929443
Epoch 30, training loss: 10.472819328308105 = 1.899505615234375 + 1.0 * 8.57331371307373
Epoch 30, val loss: 1.8984168767929077
Epoch 40, training loss: 10.313199996948242 = 1.8769577741622925 + 1.0 * 8.43624210357666
Epoch 40, val loss: 1.8760364055633545
Epoch 50, training loss: 9.83537483215332 = 1.8521045446395874 + 1.0 * 7.983270645141602
Epoch 50, val loss: 1.852622389793396
Epoch 60, training loss: 9.537696838378906 = 1.8287239074707031 + 1.0 * 7.708972454071045
Epoch 60, val loss: 1.8310415744781494
Epoch 70, training loss: 9.12559986114502 = 1.810912013053894 + 1.0 * 7.314688205718994
Epoch 70, val loss: 1.8148736953735352
Epoch 80, training loss: 8.847044944763184 = 1.797737956047058 + 1.0 * 7.049306869506836
Epoch 80, val loss: 1.8027158975601196
Epoch 90, training loss: 8.696619033813477 = 1.7820634841918945 + 1.0 * 6.914555549621582
Epoch 90, val loss: 1.7872521877288818
Epoch 100, training loss: 8.579571723937988 = 1.763388991355896 + 1.0 * 6.816183090209961
Epoch 100, val loss: 1.7695733308792114
Epoch 110, training loss: 8.489522933959961 = 1.7442854642868042 + 1.0 * 6.745237350463867
Epoch 110, val loss: 1.7523881196975708
Epoch 120, training loss: 8.41677188873291 = 1.72450852394104 + 1.0 * 6.692263126373291
Epoch 120, val loss: 1.73508882522583
Epoch 130, training loss: 8.356549263000488 = 1.701804518699646 + 1.0 * 6.654744625091553
Epoch 130, val loss: 1.7153955698013306
Epoch 140, training loss: 8.29780101776123 = 1.6757701635360718 + 1.0 * 6.622031211853027
Epoch 140, val loss: 1.6929165124893188
Epoch 150, training loss: 8.23378849029541 = 1.646401047706604 + 1.0 * 6.587387561798096
Epoch 150, val loss: 1.667947769165039
Epoch 160, training loss: 8.169808387756348 = 1.6128660440444946 + 1.0 * 6.556941986083984
Epoch 160, val loss: 1.6398169994354248
Epoch 170, training loss: 8.107799530029297 = 1.5740004777908325 + 1.0 * 6.533798694610596
Epoch 170, val loss: 1.607534646987915
Epoch 180, training loss: 8.043624877929688 = 1.5305933952331543 + 1.0 * 6.513031482696533
Epoch 180, val loss: 1.5719349384307861
Epoch 190, training loss: 7.980266571044922 = 1.4833579063415527 + 1.0 * 6.496908664703369
Epoch 190, val loss: 1.5335825681686401
Epoch 200, training loss: 7.915952682495117 = 1.433353304862976 + 1.0 * 6.482599258422852
Epoch 200, val loss: 1.49367094039917
Epoch 210, training loss: 7.853076457977295 = 1.382248044013977 + 1.0 * 6.470828533172607
Epoch 210, val loss: 1.453614354133606
Epoch 220, training loss: 7.791423797607422 = 1.3309757709503174 + 1.0 * 6.460447788238525
Epoch 220, val loss: 1.4142245054244995
Epoch 230, training loss: 7.731631278991699 = 1.2801496982574463 + 1.0 * 6.451481819152832
Epoch 230, val loss: 1.3759552240371704
Epoch 240, training loss: 7.672671794891357 = 1.2305055856704712 + 1.0 * 6.442166328430176
Epoch 240, val loss: 1.3392665386199951
Epoch 250, training loss: 7.61667013168335 = 1.182010531425476 + 1.0 * 6.434659481048584
Epoch 250, val loss: 1.3041702508926392
Epoch 260, training loss: 7.562954425811768 = 1.1347357034683228 + 1.0 * 6.428218841552734
Epoch 260, val loss: 1.2704538106918335
Epoch 270, training loss: 7.511785507202148 = 1.0889570713043213 + 1.0 * 6.422828197479248
Epoch 270, val loss: 1.2380884885787964
Epoch 280, training loss: 7.4607744216918945 = 1.0442887544631958 + 1.0 * 6.416485786437988
Epoch 280, val loss: 1.2068114280700684
Epoch 290, training loss: 7.413308143615723 = 1.0009233951568604 + 1.0 * 6.412384986877441
Epoch 290, val loss: 1.1765409708023071
Epoch 300, training loss: 7.367506980895996 = 0.9589319229125977 + 1.0 * 6.408575057983398
Epoch 300, val loss: 1.147186517715454
Epoch 310, training loss: 7.32055139541626 = 0.9185481071472168 + 1.0 * 6.402003288269043
Epoch 310, val loss: 1.1187069416046143
Epoch 320, training loss: 7.274852752685547 = 0.8795960545539856 + 1.0 * 6.395256519317627
Epoch 320, val loss: 1.0910677909851074
Epoch 330, training loss: 7.2364020347595215 = 0.8420872092247009 + 1.0 * 6.394314765930176
Epoch 330, val loss: 1.0642465353012085
Epoch 340, training loss: 7.193681240081787 = 0.8063982129096985 + 1.0 * 6.387282848358154
Epoch 340, val loss: 1.0387074947357178
Epoch 350, training loss: 7.153480052947998 = 0.7723727822303772 + 1.0 * 6.381107330322266
Epoch 350, val loss: 1.0144013166427612
Epoch 360, training loss: 7.126039028167725 = 0.7400178909301758 + 1.0 * 6.386021137237549
Epoch 360, val loss: 0.9913508892059326
Epoch 370, training loss: 7.084234714508057 = 0.7098104357719421 + 1.0 * 6.374424457550049
Epoch 370, val loss: 0.9701517820358276
Epoch 380, training loss: 7.0515546798706055 = 0.68141770362854 + 1.0 * 6.3701372146606445
Epoch 380, val loss: 0.9505665898323059
Epoch 390, training loss: 7.020159721374512 = 0.6544005274772644 + 1.0 * 6.365759372711182
Epoch 390, val loss: 0.9322577118873596
Epoch 400, training loss: 6.997109889984131 = 0.6284455060958862 + 1.0 * 6.368664264678955
Epoch 400, val loss: 0.9150291085243225
Epoch 410, training loss: 6.9632391929626465 = 0.6034388542175293 + 1.0 * 6.359800338745117
Epoch 410, val loss: 0.8988021612167358
Epoch 420, training loss: 6.936460971832275 = 0.5789518356323242 + 1.0 * 6.357509136199951
Epoch 420, val loss: 0.8832365274429321
Epoch 430, training loss: 6.912004470825195 = 0.5548823475837708 + 1.0 * 6.35712194442749
Epoch 430, val loss: 0.8682881593704224
Epoch 440, training loss: 6.884649753570557 = 0.5313085317611694 + 1.0 * 6.353341102600098
Epoch 440, val loss: 0.8538888692855835
Epoch 450, training loss: 6.857835292816162 = 0.5079359412193298 + 1.0 * 6.3498992919921875
Epoch 450, val loss: 0.8399814367294312
Epoch 460, training loss: 6.832024097442627 = 0.4846917390823364 + 1.0 * 6.34733247756958
Epoch 460, val loss: 0.8264899849891663
Epoch 470, training loss: 6.809880256652832 = 0.46173542737960815 + 1.0 * 6.348145008087158
Epoch 470, val loss: 0.813460111618042
Epoch 480, training loss: 6.785326957702637 = 0.4393327236175537 + 1.0 * 6.345994472503662
Epoch 480, val loss: 0.801081120967865
Epoch 490, training loss: 6.759156227111816 = 0.4174504280090332 + 1.0 * 6.341705799102783
Epoch 490, val loss: 0.7893751263618469
Epoch 500, training loss: 6.73773193359375 = 0.3960278034210205 + 1.0 * 6.34170389175415
Epoch 500, val loss: 0.7784988284111023
Epoch 510, training loss: 6.712672233581543 = 0.37516266107559204 + 1.0 * 6.337509632110596
Epoch 510, val loss: 0.7685163617134094
Epoch 520, training loss: 6.692555904388428 = 0.3548056185245514 + 1.0 * 6.337750434875488
Epoch 520, val loss: 0.7592707276344299
Epoch 530, training loss: 6.676540851593018 = 0.33515772223472595 + 1.0 * 6.34138298034668
Epoch 530, val loss: 0.7511246204376221
Epoch 540, training loss: 6.6490020751953125 = 0.31624338030815125 + 1.0 * 6.332758903503418
Epoch 540, val loss: 0.7438278794288635
Epoch 550, training loss: 6.62923002243042 = 0.2979317605495453 + 1.0 * 6.331298351287842
Epoch 550, val loss: 0.7373616099357605
Epoch 560, training loss: 6.609958171844482 = 0.2802017629146576 + 1.0 * 6.329756259918213
Epoch 560, val loss: 0.7318931221961975
Epoch 570, training loss: 6.593055725097656 = 0.26312944293022156 + 1.0 * 6.329926490783691
Epoch 570, val loss: 0.7274252772331238
Epoch 580, training loss: 6.574434757232666 = 0.24681347608566284 + 1.0 * 6.3276214599609375
Epoch 580, val loss: 0.7236435413360596
Epoch 590, training loss: 6.557100296020508 = 0.23117558658123016 + 1.0 * 6.325924873352051
Epoch 590, val loss: 0.720636785030365
Epoch 600, training loss: 6.545173645019531 = 0.2162465900182724 + 1.0 * 6.328927040100098
Epoch 600, val loss: 0.7186567783355713
Epoch 610, training loss: 6.525277137756348 = 0.20212620496749878 + 1.0 * 6.323151111602783
Epoch 610, val loss: 0.7173076272010803
Epoch 620, training loss: 6.509238243103027 = 0.18872863054275513 + 1.0 * 6.320509433746338
Epoch 620, val loss: 0.7165249586105347
Epoch 630, training loss: 6.507419109344482 = 0.17604032158851624 + 1.0 * 6.331378936767578
Epoch 630, val loss: 0.7167064547538757
Epoch 640, training loss: 6.487081050872803 = 0.16422194242477417 + 1.0 * 6.322859287261963
Epoch 640, val loss: 0.7177124619483948
Epoch 650, training loss: 6.472611904144287 = 0.15322408080101013 + 1.0 * 6.319387912750244
Epoch 650, val loss: 0.7188347578048706
Epoch 660, training loss: 6.458789348602295 = 0.14294126629829407 + 1.0 * 6.315847873687744
Epoch 660, val loss: 0.7205211520195007
Epoch 670, training loss: 6.446805953979492 = 0.13334466516971588 + 1.0 * 6.3134613037109375
Epoch 670, val loss: 0.72300124168396
Epoch 680, training loss: 6.447455406188965 = 0.12443381547927856 + 1.0 * 6.323021411895752
Epoch 680, val loss: 0.726093590259552
Epoch 690, training loss: 6.431059837341309 = 0.11621715873479843 + 1.0 * 6.314842700958252
Epoch 690, val loss: 0.7295441031455994
Epoch 700, training loss: 6.418129920959473 = 0.10865521430969238 + 1.0 * 6.309474468231201
Epoch 700, val loss: 0.7331739664077759
Epoch 710, training loss: 6.414607524871826 = 0.10167910158634186 + 1.0 * 6.312928199768066
Epoch 710, val loss: 0.7372971177101135
Epoch 720, training loss: 6.402863025665283 = 0.09527762234210968 + 1.0 * 6.3075852394104
Epoch 720, val loss: 0.7419188618659973
Epoch 730, training loss: 6.401904582977295 = 0.08938907831907272 + 1.0 * 6.312515735626221
Epoch 730, val loss: 0.7465575337409973
Epoch 740, training loss: 6.393002986907959 = 0.08400517702102661 + 1.0 * 6.308997631072998
Epoch 740, val loss: 0.7515905499458313
Epoch 750, training loss: 6.383365631103516 = 0.07906221598386765 + 1.0 * 6.3043036460876465
Epoch 750, val loss: 0.7565280795097351
Epoch 760, training loss: 6.37686014175415 = 0.07450097799301147 + 1.0 * 6.302359104156494
Epoch 760, val loss: 0.7616157531738281
Epoch 770, training loss: 6.3785624504089355 = 0.0702950656414032 + 1.0 * 6.308267593383789
Epoch 770, val loss: 0.7670166492462158
Epoch 780, training loss: 6.367770195007324 = 0.06642449647188187 + 1.0 * 6.3013458251953125
Epoch 780, val loss: 0.7724282741546631
Epoch 790, training loss: 6.361901760101318 = 0.06285018473863602 + 1.0 * 6.299051761627197
Epoch 790, val loss: 0.7777695655822754
Epoch 800, training loss: 6.366389274597168 = 0.05954409018158913 + 1.0 * 6.306845188140869
Epoch 800, val loss: 0.7832499742507935
Epoch 810, training loss: 6.357083797454834 = 0.056493740528821945 + 1.0 * 6.3005900382995605
Epoch 810, val loss: 0.7889048457145691
Epoch 820, training loss: 6.350042819976807 = 0.053664322942495346 + 1.0 * 6.2963786125183105
Epoch 820, val loss: 0.79424649477005
Epoch 830, training loss: 6.358017921447754 = 0.05104226991534233 + 1.0 * 6.306975841522217
Epoch 830, val loss: 0.7998279333114624
Epoch 840, training loss: 6.343102931976318 = 0.04861525446176529 + 1.0 * 6.294487476348877
Epoch 840, val loss: 0.8053138852119446
Epoch 850, training loss: 6.339468002319336 = 0.04635361209511757 + 1.0 * 6.293114185333252
Epoch 850, val loss: 0.8105049133300781
Epoch 860, training loss: 6.335221290588379 = 0.04423220083117485 + 1.0 * 6.290988922119141
Epoch 860, val loss: 0.8158201575279236
Epoch 870, training loss: 6.34016752243042 = 0.04224509745836258 + 1.0 * 6.297922611236572
Epoch 870, val loss: 0.821189820766449
Epoch 880, training loss: 6.337745666503906 = 0.040389277040958405 + 1.0 * 6.297356605529785
Epoch 880, val loss: 0.8268075585365295
Epoch 890, training loss: 6.327126979827881 = 0.038666240870952606 + 1.0 * 6.288460731506348
Epoch 890, val loss: 0.8318552374839783
Epoch 900, training loss: 6.325923919677734 = 0.0370459221303463 + 1.0 * 6.288877964019775
Epoch 900, val loss: 0.8368512988090515
Epoch 910, training loss: 6.335510730743408 = 0.035516347736120224 + 1.0 * 6.299994468688965
Epoch 910, val loss: 0.8421357274055481
Epoch 920, training loss: 6.321522235870361 = 0.03409016877412796 + 1.0 * 6.2874321937561035
Epoch 920, val loss: 0.8473251461982727
Epoch 930, training loss: 6.322508811950684 = 0.03275110572576523 + 1.0 * 6.28975772857666
Epoch 930, val loss: 0.8522030711174011
Epoch 940, training loss: 6.317660808563232 = 0.0314863845705986 + 1.0 * 6.286174297332764
Epoch 940, val loss: 0.8571299910545349
Epoch 950, training loss: 6.316019535064697 = 0.030291780829429626 + 1.0 * 6.2857279777526855
Epoch 950, val loss: 0.8619835376739502
Epoch 960, training loss: 6.315164566040039 = 0.029163388535380363 + 1.0 * 6.286001205444336
Epoch 960, val loss: 0.8668875098228455
Epoch 970, training loss: 6.319905757904053 = 0.028096867725253105 + 1.0 * 6.29180908203125
Epoch 970, val loss: 0.8717413544654846
Epoch 980, training loss: 6.312398910522461 = 0.02709151990711689 + 1.0 * 6.28530740737915
Epoch 980, val loss: 0.8765683770179749
Epoch 990, training loss: 6.30828332901001 = 0.026140518486499786 + 1.0 * 6.282142639160156
Epoch 990, val loss: 0.8811334371566772
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 10.52844524383545 = 1.931625485420227 + 1.0 * 8.596819877624512
Epoch 0, val loss: 1.9262186288833618
Epoch 10, training loss: 10.518553733825684 = 1.9220751523971558 + 1.0 * 8.596478462219238
Epoch 10, val loss: 1.9172847270965576
Epoch 20, training loss: 10.503519058227539 = 1.9101650714874268 + 1.0 * 8.593354225158691
Epoch 20, val loss: 1.9059261083602905
Epoch 30, training loss: 10.459402084350586 = 1.8933281898498535 + 1.0 * 8.56607437133789
Epoch 30, val loss: 1.8898310661315918
Epoch 40, training loss: 10.253758430480957 = 1.8713890314102173 + 1.0 * 8.382369041442871
Epoch 40, val loss: 1.8691855669021606
Epoch 50, training loss: 9.72249984741211 = 1.8469921350479126 + 1.0 * 7.875507831573486
Epoch 50, val loss: 1.8466854095458984
Epoch 60, training loss: 9.379056930541992 = 1.826139211654663 + 1.0 * 7.55291748046875
Epoch 60, val loss: 1.828171968460083
Epoch 70, training loss: 8.980992317199707 = 1.8117731809616089 + 1.0 * 7.169219017028809
Epoch 70, val loss: 1.8154600858688354
Epoch 80, training loss: 8.704561233520508 = 1.7993108034133911 + 1.0 * 6.905250072479248
Epoch 80, val loss: 1.8043440580368042
Epoch 90, training loss: 8.562788963317871 = 1.7844526767730713 + 1.0 * 6.778336048126221
Epoch 90, val loss: 1.7913877964019775
Epoch 100, training loss: 8.473494529724121 = 1.767008900642395 + 1.0 * 6.706485748291016
Epoch 100, val loss: 1.7769906520843506
Epoch 110, training loss: 8.407692909240723 = 1.7481615543365479 + 1.0 * 6.659531593322754
Epoch 110, val loss: 1.7615126371383667
Epoch 120, training loss: 8.34853458404541 = 1.7272948026657104 + 1.0 * 6.621240139007568
Epoch 120, val loss: 1.7439178228378296
Epoch 130, training loss: 8.291525840759277 = 1.7035869359970093 + 1.0 * 6.5879387855529785
Epoch 130, val loss: 1.7236042022705078
Epoch 140, training loss: 8.239362716674805 = 1.676550269126892 + 1.0 * 6.562812805175781
Epoch 140, val loss: 1.7008275985717773
Epoch 150, training loss: 8.18286418914795 = 1.6455903053283691 + 1.0 * 6.53727388381958
Epoch 150, val loss: 1.6751185655593872
Epoch 160, training loss: 8.124987602233887 = 1.609761118888855 + 1.0 * 6.5152268409729
Epoch 160, val loss: 1.6456539630889893
Epoch 170, training loss: 8.065305709838867 = 1.568604588508606 + 1.0 * 6.496700763702393
Epoch 170, val loss: 1.6116409301757812
Epoch 180, training loss: 8.007608413696289 = 1.522247076034546 + 1.0 * 6.485361576080322
Epoch 180, val loss: 1.5731843709945679
Epoch 190, training loss: 7.940597057342529 = 1.4726924896240234 + 1.0 * 6.467904567718506
Epoch 190, val loss: 1.5318634510040283
Epoch 200, training loss: 7.87746000289917 = 1.4206632375717163 + 1.0 * 6.456796646118164
Epoch 200, val loss: 1.4884916543960571
Epoch 210, training loss: 7.81177282333374 = 1.3674277067184448 + 1.0 * 6.444344997406006
Epoch 210, val loss: 1.4440492391586304
Epoch 220, training loss: 7.749955654144287 = 1.3136024475097656 + 1.0 * 6.4363532066345215
Epoch 220, val loss: 1.3989821672439575
Epoch 230, training loss: 7.6871538162231445 = 1.260829210281372 + 1.0 * 6.426324844360352
Epoch 230, val loss: 1.3548532724380493
Epoch 240, training loss: 7.625295639038086 = 1.2092154026031494 + 1.0 * 6.416080474853516
Epoch 240, val loss: 1.3118696212768555
Epoch 250, training loss: 7.574415683746338 = 1.1585845947265625 + 1.0 * 6.415831089019775
Epoch 250, val loss: 1.2698266506195068
Epoch 260, training loss: 7.514492034912109 = 1.1097898483276367 + 1.0 * 6.404702186584473
Epoch 260, val loss: 1.2293851375579834
Epoch 270, training loss: 7.461197853088379 = 1.062540888786316 + 1.0 * 6.398656845092773
Epoch 270, val loss: 1.1904460191726685
Epoch 280, training loss: 7.409458160400391 = 1.0168136358261108 + 1.0 * 6.39264440536499
Epoch 280, val loss: 1.152701735496521
Epoch 290, training loss: 7.362884998321533 = 0.9723337888717651 + 1.0 * 6.3905510902404785
Epoch 290, val loss: 1.1160508394241333
Epoch 300, training loss: 7.314052581787109 = 0.9292539358139038 + 1.0 * 6.384798526763916
Epoch 300, val loss: 1.080634355545044
Epoch 310, training loss: 7.269357681274414 = 0.8875677585601807 + 1.0 * 6.381789684295654
Epoch 310, val loss: 1.0465824604034424
Epoch 320, training loss: 7.224476337432861 = 0.8471528887748718 + 1.0 * 6.377323627471924
Epoch 320, val loss: 1.0136592388153076
Epoch 330, training loss: 7.186389923095703 = 0.8081110119819641 + 1.0 * 6.378278732299805
Epoch 330, val loss: 0.9820695519447327
Epoch 340, training loss: 7.145810127258301 = 0.7709071636199951 + 1.0 * 6.374903202056885
Epoch 340, val loss: 0.952351450920105
Epoch 350, training loss: 7.1031012535095215 = 0.7354956269264221 + 1.0 * 6.367605686187744
Epoch 350, val loss: 0.9245995879173279
Epoch 360, training loss: 7.065643310546875 = 0.7016004323959351 + 1.0 * 6.36404275894165
Epoch 360, val loss: 0.8985702991485596
Epoch 370, training loss: 7.034395694732666 = 0.6691612005233765 + 1.0 * 6.365234375
Epoch 370, val loss: 0.8744013905525208
Epoch 380, training loss: 6.999171257019043 = 0.6383483409881592 + 1.0 * 6.360823154449463
Epoch 380, val loss: 0.8523054718971252
Epoch 390, training loss: 6.966955661773682 = 0.6090419888496399 + 1.0 * 6.357913494110107
Epoch 390, val loss: 0.8319819569587708
Epoch 400, training loss: 6.934640407562256 = 0.5809404253959656 + 1.0 * 6.353700160980225
Epoch 400, val loss: 0.8132790923118591
Epoch 410, training loss: 6.9079389572143555 = 0.5538000464439392 + 1.0 * 6.3541388511657715
Epoch 410, val loss: 0.7959041595458984
Epoch 420, training loss: 6.877251148223877 = 0.5275769829750061 + 1.0 * 6.349674224853516
Epoch 420, val loss: 0.779819667339325
Epoch 430, training loss: 6.852186679840088 = 0.5021012425422668 + 1.0 * 6.350085258483887
Epoch 430, val loss: 0.7647285461425781
Epoch 440, training loss: 6.82209587097168 = 0.4772627055644989 + 1.0 * 6.3448333740234375
Epoch 440, val loss: 0.7504478096961975
Epoch 450, training loss: 6.793225288391113 = 0.4528840184211731 + 1.0 * 6.340341091156006
Epoch 450, val loss: 0.7366721630096436
Epoch 460, training loss: 6.768651008605957 = 0.4288363456726074 + 1.0 * 6.33981466293335
Epoch 460, val loss: 0.7233685255050659
Epoch 470, training loss: 6.744771957397461 = 0.4052433669567108 + 1.0 * 6.339528560638428
Epoch 470, val loss: 0.7106091380119324
Epoch 480, training loss: 6.717983722686768 = 0.3822217583656311 + 1.0 * 6.335762023925781
Epoch 480, val loss: 0.6983543634414673
Epoch 490, training loss: 6.692266464233398 = 0.3598562180995941 + 1.0 * 6.3324103355407715
Epoch 490, val loss: 0.6865696907043457
Epoch 500, training loss: 6.670745849609375 = 0.3381941020488739 + 1.0 * 6.332551956176758
Epoch 500, val loss: 0.6754079461097717
Epoch 510, training loss: 6.64932107925415 = 0.31741347908973694 + 1.0 * 6.331907749176025
Epoch 510, val loss: 0.6650571823120117
Epoch 520, training loss: 6.625298023223877 = 0.29765060544013977 + 1.0 * 6.3276472091674805
Epoch 520, val loss: 0.6554310321807861
Epoch 530, training loss: 6.610753059387207 = 0.2788846492767334 + 1.0 * 6.331868648529053
Epoch 530, val loss: 0.6466795802116394
Epoch 540, training loss: 6.588229179382324 = 0.2611795663833618 + 1.0 * 6.327049732208252
Epoch 540, val loss: 0.6388171315193176
Epoch 550, training loss: 6.56677770614624 = 0.24447783827781677 + 1.0 * 6.322299957275391
Epoch 550, val loss: 0.6318053007125854
Epoch 560, training loss: 6.54744291305542 = 0.22870582342147827 + 1.0 * 6.318737030029297
Epoch 560, val loss: 0.6256198287010193
Epoch 570, training loss: 6.531448841094971 = 0.2137918770313263 + 1.0 * 6.317656993865967
Epoch 570, val loss: 0.6202848553657532
Epoch 580, training loss: 6.520830154418945 = 0.19974082708358765 + 1.0 * 6.321089267730713
Epoch 580, val loss: 0.6157945990562439
Epoch 590, training loss: 6.508697509765625 = 0.18658074736595154 + 1.0 * 6.322116851806641
Epoch 590, val loss: 0.6120933294296265
Epoch 600, training loss: 6.487908840179443 = 0.1742929071187973 + 1.0 * 6.313615798950195
Epoch 600, val loss: 0.6091600656509399
Epoch 610, training loss: 6.472867488861084 = 0.16279412806034088 + 1.0 * 6.310073375701904
Epoch 610, val loss: 0.6068817377090454
Epoch 620, training loss: 6.462386131286621 = 0.15203286707401276 + 1.0 * 6.3103532791137695
Epoch 620, val loss: 0.6053913235664368
Epoch 630, training loss: 6.455910682678223 = 0.14200499653816223 + 1.0 * 6.313905715942383
Epoch 630, val loss: 0.6046957969665527
Epoch 640, training loss: 6.444849967956543 = 0.1327352523803711 + 1.0 * 6.312114715576172
Epoch 640, val loss: 0.6045648455619812
Epoch 650, training loss: 6.4286322593688965 = 0.12415970861911774 + 1.0 * 6.30447244644165
Epoch 650, val loss: 0.60490483045578
Epoch 660, training loss: 6.421206474304199 = 0.11620762199163437 + 1.0 * 6.304998874664307
Epoch 660, val loss: 0.6059321165084839
Epoch 670, training loss: 6.414709568023682 = 0.10884720087051392 + 1.0 * 6.3058624267578125
Epoch 670, val loss: 0.6075401306152344
Epoch 680, training loss: 6.406576156616211 = 0.10205399990081787 + 1.0 * 6.3045220375061035
Epoch 680, val loss: 0.6095824241638184
Epoch 690, training loss: 6.397326946258545 = 0.0957842618227005 + 1.0 * 6.30154275894165
Epoch 690, val loss: 0.6119877099990845
Epoch 700, training loss: 6.389031410217285 = 0.08998610079288483 + 1.0 * 6.299045085906982
Epoch 700, val loss: 0.6148163676261902
Epoch 710, training loss: 6.3945136070251465 = 0.08461755514144897 + 1.0 * 6.309895992279053
Epoch 710, val loss: 0.6181139945983887
Epoch 720, training loss: 6.377071857452393 = 0.07966767251491547 + 1.0 * 6.2974042892456055
Epoch 720, val loss: 0.6217206716537476
Epoch 730, training loss: 6.370400428771973 = 0.07509568333625793 + 1.0 * 6.295304775238037
Epoch 730, val loss: 0.6254359483718872
Epoch 740, training loss: 6.366183280944824 = 0.07085192948579788 + 1.0 * 6.2953314781188965
Epoch 740, val loss: 0.6295588612556458
Epoch 750, training loss: 6.364132881164551 = 0.06691896915435791 + 1.0 * 6.297214031219482
Epoch 750, val loss: 0.634026825428009
Epoch 760, training loss: 6.358649253845215 = 0.06329623609781265 + 1.0 * 6.295352935791016
Epoch 760, val loss: 0.6384748220443726
Epoch 770, training loss: 6.350259304046631 = 0.05993033945560455 + 1.0 * 6.2903289794921875
Epoch 770, val loss: 0.6430422067642212
Epoch 780, training loss: 6.345835208892822 = 0.05679592490196228 + 1.0 * 6.289039134979248
Epoch 780, val loss: 0.6478583216667175
Epoch 790, training loss: 6.365056991577148 = 0.053882572799921036 + 1.0 * 6.311174392700195
Epoch 790, val loss: 0.6528306007385254
Epoch 800, training loss: 6.342773914337158 = 0.05118977278470993 + 1.0 * 6.291584014892578
Epoch 800, val loss: 0.6578983068466187
Epoch 810, training loss: 6.335504055023193 = 0.04869247227907181 + 1.0 * 6.286811351776123
Epoch 810, val loss: 0.6627467274665833
Epoch 820, training loss: 6.331316947937012 = 0.046358462423086166 + 1.0 * 6.284958362579346
Epoch 820, val loss: 0.6678268313407898
Epoch 830, training loss: 6.328019618988037 = 0.044171638786792755 + 1.0 * 6.283847808837891
Epoch 830, val loss: 0.6730064749717712
Epoch 840, training loss: 6.343023777008057 = 0.042123932391405106 + 1.0 * 6.300899982452393
Epoch 840, val loss: 0.6782561540603638
Epoch 850, training loss: 6.330863952636719 = 0.040224865078926086 + 1.0 * 6.2906389236450195
Epoch 850, val loss: 0.6835264563560486
Epoch 860, training loss: 6.322514533996582 = 0.038461122661828995 + 1.0 * 6.284053325653076
Epoch 860, val loss: 0.6884684562683105
Epoch 870, training loss: 6.318451404571533 = 0.0368056558072567 + 1.0 * 6.281645774841309
Epoch 870, val loss: 0.6934510469436646
Epoch 880, training loss: 6.316028118133545 = 0.03524668887257576 + 1.0 * 6.280781269073486
Epoch 880, val loss: 0.6985396146774292
Epoch 890, training loss: 6.3189005851745605 = 0.03377917781472206 + 1.0 * 6.285121440887451
Epoch 890, val loss: 0.7036755084991455
Epoch 900, training loss: 6.317718982696533 = 0.03240421414375305 + 1.0 * 6.285314559936523
Epoch 900, val loss: 0.7087045311927795
Epoch 910, training loss: 6.311330795288086 = 0.031107978895306587 + 1.0 * 6.2802228927612305
Epoch 910, val loss: 0.7136841416358948
Epoch 920, training loss: 6.307061672210693 = 0.02989043854176998 + 1.0 * 6.2771711349487305
Epoch 920, val loss: 0.7185860872268677
Epoch 930, training loss: 6.307470321655273 = 0.028738979250192642 + 1.0 * 6.278731346130371
Epoch 930, val loss: 0.7235091328620911
Epoch 940, training loss: 6.302569389343262 = 0.027653541415929794 + 1.0 * 6.27491569519043
Epoch 940, val loss: 0.7284145355224609
Epoch 950, training loss: 6.301896572113037 = 0.026628727093338966 + 1.0 * 6.275268077850342
Epoch 950, val loss: 0.7332078218460083
Epoch 960, training loss: 6.309218883514404 = 0.025659771636128426 + 1.0 * 6.283559322357178
Epoch 960, val loss: 0.7380120158195496
Epoch 970, training loss: 6.303868293762207 = 0.024750225245952606 + 1.0 * 6.279118061065674
Epoch 970, val loss: 0.7427603602409363
Epoch 980, training loss: 6.297416687011719 = 0.023891344666481018 + 1.0 * 6.273525238037109
Epoch 980, val loss: 0.7472513318061829
Epoch 990, training loss: 6.294460773468018 = 0.0230752844363451 + 1.0 * 6.271385669708252
Epoch 990, val loss: 0.7517619132995605
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8323668950975225
=== training gcn model ===
Epoch 0, training loss: 10.52962589263916 = 1.932765245437622 + 1.0 * 8.596860885620117
Epoch 0, val loss: 1.9247736930847168
Epoch 10, training loss: 10.519514083862305 = 1.9229217767715454 + 1.0 * 8.59659194946289
Epoch 10, val loss: 1.914683222770691
Epoch 20, training loss: 10.504809379577637 = 1.9106167554855347 + 1.0 * 8.594192504882812
Epoch 20, val loss: 1.9019140005111694
Epoch 30, training loss: 10.468613624572754 = 1.8936878442764282 + 1.0 * 8.574925422668457
Epoch 30, val loss: 1.8843634128570557
Epoch 40, training loss: 10.33043384552002 = 1.8712409734725952 + 1.0 * 8.459193229675293
Epoch 40, val loss: 1.8618826866149902
Epoch 50, training loss: 9.85116958618164 = 1.8457409143447876 + 1.0 * 8.005428314208984
Epoch 50, val loss: 1.8374671936035156
Epoch 60, training loss: 9.506661415100098 = 1.823166847229004 + 1.0 * 7.683494567871094
Epoch 60, val loss: 1.8171846866607666
Epoch 70, training loss: 9.059293746948242 = 1.8046669960021973 + 1.0 * 7.254627227783203
Epoch 70, val loss: 1.800737738609314
Epoch 80, training loss: 8.83620834350586 = 1.7896628379821777 + 1.0 * 7.04654598236084
Epoch 80, val loss: 1.7873464822769165
Epoch 90, training loss: 8.694883346557617 = 1.7726218700408936 + 1.0 * 6.9222612380981445
Epoch 90, val loss: 1.772479772567749
Epoch 100, training loss: 8.581283569335938 = 1.7540171146392822 + 1.0 * 6.827266693115234
Epoch 100, val loss: 1.7573797702789307
Epoch 110, training loss: 8.500334739685059 = 1.7352708578109741 + 1.0 * 6.765064239501953
Epoch 110, val loss: 1.742478370666504
Epoch 120, training loss: 8.434688568115234 = 1.71485435962677 + 1.0 * 6.719833850860596
Epoch 120, val loss: 1.7256330251693726
Epoch 130, training loss: 8.374117851257324 = 1.691718578338623 + 1.0 * 6.682399272918701
Epoch 130, val loss: 1.7062348127365112
Epoch 140, training loss: 8.317159652709961 = 1.6649448871612549 + 1.0 * 6.652215003967285
Epoch 140, val loss: 1.6838711500167847
Epoch 150, training loss: 8.257789611816406 = 1.6331520080566406 + 1.0 * 6.624637603759766
Epoch 150, val loss: 1.6574729681015015
Epoch 160, training loss: 8.196752548217773 = 1.5957039594650269 + 1.0 * 6.601048946380615
Epoch 160, val loss: 1.6263577938079834
Epoch 170, training loss: 8.131631851196289 = 1.5518417358398438 + 1.0 * 6.579789638519287
Epoch 170, val loss: 1.590013861656189
Epoch 180, training loss: 8.061501502990723 = 1.5015199184417725 + 1.0 * 6.559981346130371
Epoch 180, val loss: 1.5485023260116577
Epoch 190, training loss: 7.9850592613220215 = 1.445014476776123 + 1.0 * 6.540044784545898
Epoch 190, val loss: 1.5019521713256836
Epoch 200, training loss: 7.906508922576904 = 1.3829360008239746 + 1.0 * 6.52357292175293
Epoch 200, val loss: 1.451004147529602
Epoch 210, training loss: 7.825860023498535 = 1.3171494007110596 + 1.0 * 6.508710861206055
Epoch 210, val loss: 1.397743821144104
Epoch 220, training loss: 7.745002269744873 = 1.2500776052474976 + 1.0 * 6.494924545288086
Epoch 220, val loss: 1.3441444635391235
Epoch 230, training loss: 7.668381214141846 = 1.183691143989563 + 1.0 * 6.484690189361572
Epoch 230, val loss: 1.2918517589569092
Epoch 240, training loss: 7.594264030456543 = 1.120195984840393 + 1.0 * 6.4740681648254395
Epoch 240, val loss: 1.242429256439209
Epoch 250, training loss: 7.524608612060547 = 1.0595775842666626 + 1.0 * 6.465031147003174
Epoch 250, val loss: 1.1959679126739502
Epoch 260, training loss: 7.466132164001465 = 1.0018900632858276 + 1.0 * 6.464241981506348
Epoch 260, val loss: 1.1524683237075806
Epoch 270, training loss: 7.399683475494385 = 0.9485830068588257 + 1.0 * 6.4511003494262695
Epoch 270, val loss: 1.1125131845474243
Epoch 280, training loss: 7.3425164222717285 = 0.8984302282333374 + 1.0 * 6.444086074829102
Epoch 280, val loss: 1.0752379894256592
Epoch 290, training loss: 7.288655757904053 = 0.8506684899330139 + 1.0 * 6.437987327575684
Epoch 290, val loss: 1.040001630783081
Epoch 300, training loss: 7.236422538757324 = 0.805444061756134 + 1.0 * 6.430978298187256
Epoch 300, val loss: 1.0065935850143433
Epoch 310, training loss: 7.192193984985352 = 0.7619128823280334 + 1.0 * 6.430281162261963
Epoch 310, val loss: 0.974550724029541
Epoch 320, training loss: 7.1412672996521 = 0.7204697728157043 + 1.0 * 6.420797348022461
Epoch 320, val loss: 0.9440164566040039
Epoch 330, training loss: 7.09674072265625 = 0.6806225776672363 + 1.0 * 6.416118144989014
Epoch 330, val loss: 0.9149596095085144
Epoch 340, training loss: 7.052306652069092 = 0.6420098543167114 + 1.0 * 6.41029691696167
Epoch 340, val loss: 0.8872458338737488
Epoch 350, training loss: 7.017801761627197 = 0.6047305464744568 + 1.0 * 6.413071155548096
Epoch 350, val loss: 0.8609969615936279
Epoch 360, training loss: 6.9733052253723145 = 0.5693501234054565 + 1.0 * 6.403954982757568
Epoch 360, val loss: 0.8367844820022583
Epoch 370, training loss: 6.9335126876831055 = 0.5353286266326904 + 1.0 * 6.398184299468994
Epoch 370, val loss: 0.8143600225448608
Epoch 380, training loss: 6.906857967376709 = 0.5024667382240295 + 1.0 * 6.404391288757324
Epoch 380, val loss: 0.7934110760688782
Epoch 390, training loss: 6.8654704093933105 = 0.47127261757850647 + 1.0 * 6.394197940826416
Epoch 390, val loss: 0.7742000222206116
Epoch 400, training loss: 6.831370830535889 = 0.44152551889419556 + 1.0 * 6.389845371246338
Epoch 400, val loss: 0.7567659020423889
Epoch 410, training loss: 6.799144744873047 = 0.4132404625415802 + 1.0 * 6.385904312133789
Epoch 410, val loss: 0.740986168384552
Epoch 420, training loss: 6.770769119262695 = 0.3865567147731781 + 1.0 * 6.384212493896484
Epoch 420, val loss: 0.726974606513977
Epoch 430, training loss: 6.742221832275391 = 0.36154431104660034 + 1.0 * 6.380677700042725
Epoch 430, val loss: 0.7147141695022583
Epoch 440, training loss: 6.713470458984375 = 0.33802303671836853 + 1.0 * 6.3754472732543945
Epoch 440, val loss: 0.7041130661964417
Epoch 450, training loss: 6.703976154327393 = 0.3159303367137909 + 1.0 * 6.388045787811279
Epoch 450, val loss: 0.6950495839118958
Epoch 460, training loss: 6.668023109436035 = 0.2954305410385132 + 1.0 * 6.372592449188232
Epoch 460, val loss: 0.6874744296073914
Epoch 470, training loss: 6.64479398727417 = 0.2763083875179291 + 1.0 * 6.368485450744629
Epoch 470, val loss: 0.6813696026802063
Epoch 480, training loss: 6.625306129455566 = 0.25838208198547363 + 1.0 * 6.366924285888672
Epoch 480, val loss: 0.6764782667160034
Epoch 490, training loss: 6.608208179473877 = 0.2416498363018036 + 1.0 * 6.36655855178833
Epoch 490, val loss: 0.6727029085159302
Epoch 500, training loss: 6.588908672332764 = 0.2261352688074112 + 1.0 * 6.362773418426514
Epoch 500, val loss: 0.6699925065040588
Epoch 510, training loss: 6.571586608886719 = 0.21166101098060608 + 1.0 * 6.359925746917725
Epoch 510, val loss: 0.6682514548301697
Epoch 520, training loss: 6.563738822937012 = 0.1981484442949295 + 1.0 * 6.365590572357178
Epoch 520, val loss: 0.6673336029052734
Epoch 530, training loss: 6.542769432067871 = 0.1856459230184555 + 1.0 * 6.357123374938965
Epoch 530, val loss: 0.6672160029411316
Epoch 540, training loss: 6.5287251472473145 = 0.17404140532016754 + 1.0 * 6.354683876037598
Epoch 540, val loss: 0.6678512096405029
Epoch 550, training loss: 6.5216383934021 = 0.16323943436145782 + 1.0 * 6.358398914337158
Epoch 550, val loss: 0.66914302110672
Epoch 560, training loss: 6.504714488983154 = 0.15323923528194427 + 1.0 * 6.351475238800049
Epoch 560, val loss: 0.6709285974502563
Epoch 570, training loss: 6.492523193359375 = 0.1439998745918274 + 1.0 * 6.348523139953613
Epoch 570, val loss: 0.6733067631721497
Epoch 580, training loss: 6.481939315795898 = 0.1353956162929535 + 1.0 * 6.346543788909912
Epoch 580, val loss: 0.676117479801178
Epoch 590, training loss: 6.472063064575195 = 0.12737444043159485 + 1.0 * 6.344688415527344
Epoch 590, val loss: 0.6793845891952515
Epoch 600, training loss: 6.467129707336426 = 0.11993160098791122 + 1.0 * 6.347198009490967
Epoch 600, val loss: 0.6829667091369629
Epoch 610, training loss: 6.456446647644043 = 0.11309164017438889 + 1.0 * 6.343355178833008
Epoch 610, val loss: 0.6868691444396973
Epoch 620, training loss: 6.447717189788818 = 0.10672798752784729 + 1.0 * 6.340989112854004
Epoch 620, val loss: 0.6910759806632996
Epoch 630, training loss: 6.441229343414307 = 0.10078006237745285 + 1.0 * 6.340449333190918
Epoch 630, val loss: 0.6955528259277344
Epoch 640, training loss: 6.433061122894287 = 0.09523969888687134 + 1.0 * 6.3378214836120605
Epoch 640, val loss: 0.7001898884773254
Epoch 650, training loss: 6.4276909828186035 = 0.0900997519493103 + 1.0 * 6.337591171264648
Epoch 650, val loss: 0.705071747303009
Epoch 660, training loss: 6.41821813583374 = 0.08528988063335419 + 1.0 * 6.33292818069458
Epoch 660, val loss: 0.7101096510887146
Epoch 670, training loss: 6.427620887756348 = 0.08079437166452408 + 1.0 * 6.346826553344727
Epoch 670, val loss: 0.715306282043457
Epoch 680, training loss: 6.407910346984863 = 0.07662788033485413 + 1.0 * 6.331282615661621
Epoch 680, val loss: 0.7206059694290161
Epoch 690, training loss: 6.403229236602783 = 0.07274140417575836 + 1.0 * 6.3304877281188965
Epoch 690, val loss: 0.7259911894798279
Epoch 700, training loss: 6.39829683303833 = 0.06910181790590286 + 1.0 * 6.329195022583008
Epoch 700, val loss: 0.7315030097961426
Epoch 710, training loss: 6.393963813781738 = 0.06569232791662216 + 1.0 * 6.328271389007568
Epoch 710, val loss: 0.7370691299438477
Epoch 720, training loss: 6.394241809844971 = 0.06250642985105515 + 1.0 * 6.331735610961914
Epoch 720, val loss: 0.742681622505188
Epoch 730, training loss: 6.387270927429199 = 0.05953150987625122 + 1.0 * 6.327739238739014
Epoch 730, val loss: 0.7483694553375244
Epoch 740, training loss: 6.382809638977051 = 0.05674748122692108 + 1.0 * 6.326062202453613
Epoch 740, val loss: 0.7540410161018372
Epoch 750, training loss: 6.376013278961182 = 0.05413594841957092 + 1.0 * 6.321877479553223
Epoch 750, val loss: 0.7597265839576721
Epoch 760, training loss: 6.372460842132568 = 0.051682595163583755 + 1.0 * 6.3207783699035645
Epoch 760, val loss: 0.765441358089447
Epoch 770, training loss: 6.374573230743408 = 0.04937896877527237 + 1.0 * 6.325194358825684
Epoch 770, val loss: 0.7711682915687561
Epoch 780, training loss: 6.372920989990234 = 0.04722977429628372 + 1.0 * 6.325691223144531
Epoch 780, val loss: 0.7768654227256775
Epoch 790, training loss: 6.362987995147705 = 0.04520634561777115 + 1.0 * 6.317781448364258
Epoch 790, val loss: 0.7824864387512207
Epoch 800, training loss: 6.358876705169678 = 0.04330421984195709 + 1.0 * 6.315572261810303
Epoch 800, val loss: 0.7881278395652771
Epoch 810, training loss: 6.371929168701172 = 0.04151030629873276 + 1.0 * 6.330419063568115
Epoch 810, val loss: 0.7937434315681458
Epoch 820, training loss: 6.352842330932617 = 0.03982435539364815 + 1.0 * 6.313017845153809
Epoch 820, val loss: 0.7993106245994568
Epoch 830, training loss: 6.350679874420166 = 0.038239188492298126 + 1.0 * 6.312440872192383
Epoch 830, val loss: 0.8048985004425049
Epoch 840, training loss: 6.34753942489624 = 0.036736130714416504 + 1.0 * 6.310803413391113
Epoch 840, val loss: 0.8104719519615173
Epoch 850, training loss: 6.35883092880249 = 0.03531332686543465 + 1.0 * 6.323517799377441
Epoch 850, val loss: 0.8159911036491394
Epoch 860, training loss: 6.354094505310059 = 0.0339684821665287 + 1.0 * 6.320126056671143
Epoch 860, val loss: 0.8213966488838196
Epoch 870, training loss: 6.342709541320801 = 0.03270686790347099 + 1.0 * 6.31000280380249
Epoch 870, val loss: 0.8267477750778198
Epoch 880, training loss: 6.339085578918457 = 0.031516171991825104 + 1.0 * 6.30756950378418
Epoch 880, val loss: 0.8320954442024231
Epoch 890, training loss: 6.3386616706848145 = 0.030379952862858772 + 1.0 * 6.308281898498535
Epoch 890, val loss: 0.8374491930007935
Epoch 900, training loss: 6.3361124992370605 = 0.029300391674041748 + 1.0 * 6.306812286376953
Epoch 900, val loss: 0.8426797986030579
Epoch 910, training loss: 6.336434364318848 = 0.02827685885131359 + 1.0 * 6.308157444000244
Epoch 910, val loss: 0.8478355407714844
Epoch 920, training loss: 6.332308292388916 = 0.02731066197156906 + 1.0 * 6.304997444152832
Epoch 920, val loss: 0.852963924407959
Epoch 930, training loss: 6.329254150390625 = 0.026394514366984367 + 1.0 * 6.302859783172607
Epoch 930, val loss: 0.8580294847488403
Epoch 940, training loss: 6.3271636962890625 = 0.025518283247947693 + 1.0 * 6.301645278930664
Epoch 940, val loss: 0.8630512356758118
Epoch 950, training loss: 6.328479766845703 = 0.02468111366033554 + 1.0 * 6.303798675537109
Epoch 950, val loss: 0.8680683970451355
Epoch 960, training loss: 6.3317646980285645 = 0.023885352537035942 + 1.0 * 6.307879447937012
Epoch 960, val loss: 0.8729695081710815
Epoch 970, training loss: 6.32526969909668 = 0.023133032023906708 + 1.0 * 6.3021368980407715
Epoch 970, val loss: 0.8778757452964783
Epoch 980, training loss: 6.3216142654418945 = 0.022415725514292717 + 1.0 * 6.299198627471924
Epoch 980, val loss: 0.8826292157173157
Epoch 990, training loss: 6.322282314300537 = 0.021726902574300766 + 1.0 * 6.300555229187012
Epoch 990, val loss: 0.887359082698822
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8371112282551397
The final CL Acc:0.82469, 0.00698, The final GNN Acc:0.83483, 0.00194
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10540])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.53478717803955 = 1.9379533529281616 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9340434074401855
Epoch 10, training loss: 10.525102615356445 = 1.9285873174667358 + 1.0 * 8.596515655517578
Epoch 10, val loss: 1.924178957939148
Epoch 20, training loss: 10.510383605957031 = 1.916923999786377 + 1.0 * 8.593460083007812
Epoch 20, val loss: 1.9118362665176392
Epoch 30, training loss: 10.468591690063477 = 1.9007019996643066 + 1.0 * 8.567889213562012
Epoch 30, val loss: 1.8947961330413818
Epoch 40, training loss: 10.291869163513184 = 1.879616141319275 + 1.0 * 8.412253379821777
Epoch 40, val loss: 1.8735586404800415
Epoch 50, training loss: 9.815024375915527 = 1.8557745218276978 + 1.0 * 7.959249496459961
Epoch 50, val loss: 1.8502745628356934
Epoch 60, training loss: 9.47185230255127 = 1.835124135017395 + 1.0 * 7.636727809906006
Epoch 60, val loss: 1.8309133052825928
Epoch 70, training loss: 9.143569946289062 = 1.8203692436218262 + 1.0 * 7.323200225830078
Epoch 70, val loss: 1.8171522617340088
Epoch 80, training loss: 8.874385833740234 = 1.807084083557129 + 1.0 * 7.0673017501831055
Epoch 80, val loss: 1.8055788278579712
Epoch 90, training loss: 8.669135093688965 = 1.7949979305267334 + 1.0 * 6.8741374015808105
Epoch 90, val loss: 1.7954214811325073
Epoch 100, training loss: 8.536837577819824 = 1.782575249671936 + 1.0 * 6.754262447357178
Epoch 100, val loss: 1.7852392196655273
Epoch 110, training loss: 8.455697059631348 = 1.7696363925933838 + 1.0 * 6.686060428619385
Epoch 110, val loss: 1.7740824222564697
Epoch 120, training loss: 8.390898704528809 = 1.7554481029510498 + 1.0 * 6.635450839996338
Epoch 120, val loss: 1.7616807222366333
Epoch 130, training loss: 8.337535858154297 = 1.7397104501724243 + 1.0 * 6.597825050354004
Epoch 130, val loss: 1.7480738162994385
Epoch 140, training loss: 8.29084300994873 = 1.7219995260238647 + 1.0 * 6.568843841552734
Epoch 140, val loss: 1.7327241897583008
Epoch 150, training loss: 8.248409271240234 = 1.7014867067337036 + 1.0 * 6.54692268371582
Epoch 150, val loss: 1.7148635387420654
Epoch 160, training loss: 8.206070899963379 = 1.6777452230453491 + 1.0 * 6.528326034545898
Epoch 160, val loss: 1.694276213645935
Epoch 170, training loss: 8.162821769714355 = 1.6504062414169312 + 1.0 * 6.512415409088135
Epoch 170, val loss: 1.6706750392913818
Epoch 180, training loss: 8.117152214050293 = 1.618936538696289 + 1.0 * 6.498215675354004
Epoch 180, val loss: 1.6436586380004883
Epoch 190, training loss: 8.072050094604492 = 1.5829739570617676 + 1.0 * 6.489076137542725
Epoch 190, val loss: 1.6128265857696533
Epoch 200, training loss: 8.016107559204102 = 1.5429953336715698 + 1.0 * 6.4731125831604
Epoch 200, val loss: 1.5786980390548706
Epoch 210, training loss: 7.9587507247924805 = 1.4988386631011963 + 1.0 * 6.459911823272705
Epoch 210, val loss: 1.54127836227417
Epoch 220, training loss: 7.90580415725708 = 1.4511960744857788 + 1.0 * 6.454607963562012
Epoch 220, val loss: 1.5015039443969727
Epoch 230, training loss: 7.841269016265869 = 1.4021087884902954 + 1.0 * 6.439160346984863
Epoch 230, val loss: 1.4609887599945068
Epoch 240, training loss: 7.78182315826416 = 1.351427435874939 + 1.0 * 6.430395603179932
Epoch 240, val loss: 1.4197906255722046
Epoch 250, training loss: 7.721789360046387 = 1.2993232011795044 + 1.0 * 6.422466278076172
Epoch 250, val loss: 1.3779680728912354
Epoch 260, training loss: 7.661716461181641 = 1.2463171482086182 + 1.0 * 6.415399551391602
Epoch 260, val loss: 1.3361186981201172
Epoch 270, training loss: 7.607906818389893 = 1.1930814981460571 + 1.0 * 6.414825439453125
Epoch 270, val loss: 1.2948107719421387
Epoch 280, training loss: 7.547098159790039 = 1.1414844989776611 + 1.0 * 6.405613899230957
Epoch 280, val loss: 1.2557319402694702
Epoch 290, training loss: 7.490291595458984 = 1.0915368795394897 + 1.0 * 6.398754596710205
Epoch 290, val loss: 1.2185523509979248
Epoch 300, training loss: 7.435342788696289 = 1.042893886566162 + 1.0 * 6.392448902130127
Epoch 300, val loss: 1.1828733682632446
Epoch 310, training loss: 7.396646976470947 = 0.9956666231155396 + 1.0 * 6.400980472564697
Epoch 310, val loss: 1.148736596107483
Epoch 320, training loss: 7.336798191070557 = 0.9509854912757874 + 1.0 * 6.385812759399414
Epoch 320, val loss: 1.1167500019073486
Epoch 330, training loss: 7.287845134735107 = 0.9084474444389343 + 1.0 * 6.379397869110107
Epoch 330, val loss: 1.0869042873382568
Epoch 340, training loss: 7.2442474365234375 = 0.8678667545318604 + 1.0 * 6.376380443572998
Epoch 340, val loss: 1.0587899684906006
Epoch 350, training loss: 7.208579063415527 = 0.8297607898712158 + 1.0 * 6.378818035125732
Epoch 350, val loss: 1.0325331687927246
Epoch 360, training loss: 7.1630353927612305 = 0.7941806316375732 + 1.0 * 6.368854999542236
Epoch 360, val loss: 1.0085184574127197
Epoch 370, training loss: 7.125843524932861 = 0.7606401443481445 + 1.0 * 6.365203380584717
Epoch 370, val loss: 0.9863104820251465
Epoch 380, training loss: 7.090858459472656 = 0.7290772199630737 + 1.0 * 6.361781120300293
Epoch 380, val loss: 0.9656541347503662
Epoch 390, training loss: 7.058714866638184 = 0.6993497610092163 + 1.0 * 6.359364986419678
Epoch 390, val loss: 0.9469744563102722
Epoch 400, training loss: 7.027085781097412 = 0.6709950566291809 + 1.0 * 6.356090545654297
Epoch 400, val loss: 0.9296396970748901
Epoch 410, training loss: 6.998049259185791 = 0.6439310312271118 + 1.0 * 6.354118347167969
Epoch 410, val loss: 0.9137638211250305
Epoch 420, training loss: 6.968151569366455 = 0.6180850267410278 + 1.0 * 6.350066661834717
Epoch 420, val loss: 0.8992513418197632
Epoch 430, training loss: 6.9399847984313965 = 0.5930686593055725 + 1.0 * 6.346916198730469
Epoch 430, val loss: 0.8859175443649292
Epoch 440, training loss: 6.926865577697754 = 0.5687835216522217 + 1.0 * 6.358082294464111
Epoch 440, val loss: 0.8735392093658447
Epoch 450, training loss: 6.8877739906311035 = 0.54546719789505 + 1.0 * 6.342306613922119
Epoch 450, val loss: 0.8622636795043945
Epoch 460, training loss: 6.861953258514404 = 0.5227912068367004 + 1.0 * 6.3391618728637695
Epoch 460, val loss: 0.852083683013916
Epoch 470, training loss: 6.8369011878967285 = 0.5005831122398376 + 1.0 * 6.336318016052246
Epoch 470, val loss: 0.8426660895347595
Epoch 480, training loss: 6.817528247833252 = 0.4788254201412201 + 1.0 * 6.33870267868042
Epoch 480, val loss: 0.8340223431587219
Epoch 490, training loss: 6.797107696533203 = 0.45783933997154236 + 1.0 * 6.339268207550049
Epoch 490, val loss: 0.8261486291885376
Epoch 500, training loss: 6.768554210662842 = 0.43765029311180115 + 1.0 * 6.330904006958008
Epoch 500, val loss: 0.8194044828414917
Epoch 510, training loss: 6.748572826385498 = 0.4181413948535919 + 1.0 * 6.3304314613342285
Epoch 510, val loss: 0.8134891986846924
Epoch 520, training loss: 6.727923393249512 = 0.3993513584136963 + 1.0 * 6.3285722732543945
Epoch 520, val loss: 0.8083252310752869
Epoch 530, training loss: 6.707070827484131 = 0.3813519775867462 + 1.0 * 6.325718879699707
Epoch 530, val loss: 0.8041515350341797
Epoch 540, training loss: 6.690759181976318 = 0.36405354738235474 + 1.0 * 6.326705455780029
Epoch 540, val loss: 0.8009054064750671
Epoch 550, training loss: 6.672208309173584 = 0.34749922156333923 + 1.0 * 6.324708938598633
Epoch 550, val loss: 0.7983193397521973
Epoch 560, training loss: 6.654542922973633 = 0.33168625831604004 + 1.0 * 6.322856426239014
Epoch 560, val loss: 0.7966073155403137
Epoch 570, training loss: 6.636481285095215 = 0.3165367543697357 + 1.0 * 6.319944381713867
Epoch 570, val loss: 0.7956612706184387
Epoch 580, training loss: 6.6190924644470215 = 0.3019031286239624 + 1.0 * 6.3171892166137695
Epoch 580, val loss: 0.7951223254203796
Epoch 590, training loss: 6.610687255859375 = 0.28776970505714417 + 1.0 * 6.322917461395264
Epoch 590, val loss: 0.7951613664627075
Epoch 600, training loss: 6.588619232177734 = 0.274152934551239 + 1.0 * 6.31446647644043
Epoch 600, val loss: 0.7954555749893188
Epoch 610, training loss: 6.574156761169434 = 0.26102304458618164 + 1.0 * 6.313133716583252
Epoch 610, val loss: 0.7965072393417358
Epoch 620, training loss: 6.559661865234375 = 0.24823223054409027 + 1.0 * 6.311429500579834
Epoch 620, val loss: 0.797623872756958
Epoch 630, training loss: 6.547726631164551 = 0.23576240241527557 + 1.0 * 6.31196403503418
Epoch 630, val loss: 0.7989594340324402
Epoch 640, training loss: 6.537409782409668 = 0.22367151081562042 + 1.0 * 6.3137383460998535
Epoch 640, val loss: 0.8004707098007202
Epoch 650, training loss: 6.522237300872803 = 0.2120194286108017 + 1.0 * 6.31021785736084
Epoch 650, val loss: 0.8022546172142029
Epoch 660, training loss: 6.506640434265137 = 0.2007834017276764 + 1.0 * 6.305857181549072
Epoch 660, val loss: 0.8042423129081726
Epoch 670, training loss: 6.499050617218018 = 0.18994323909282684 + 1.0 * 6.309107303619385
Epoch 670, val loss: 0.8063806891441345
Epoch 680, training loss: 6.487270832061768 = 0.179570272564888 + 1.0 * 6.3077006340026855
Epoch 680, val loss: 0.808373212814331
Epoch 690, training loss: 6.474361896514893 = 0.16973792016506195 + 1.0 * 6.304624080657959
Epoch 690, val loss: 0.8110259771347046
Epoch 700, training loss: 6.461209297180176 = 0.16037166118621826 + 1.0 * 6.300837516784668
Epoch 700, val loss: 0.8136278390884399
Epoch 710, training loss: 6.452517032623291 = 0.15147361159324646 + 1.0 * 6.301043510437012
Epoch 710, val loss: 0.8163707852363586
Epoch 720, training loss: 6.445842742919922 = 0.14309410750865936 + 1.0 * 6.302748680114746
Epoch 720, val loss: 0.8192589282989502
Epoch 730, training loss: 6.433298110961914 = 0.13523030281066895 + 1.0 * 6.298067569732666
Epoch 730, val loss: 0.8224380016326904
Epoch 740, training loss: 6.426407814025879 = 0.1278272122144699 + 1.0 * 6.298580646514893
Epoch 740, val loss: 0.8256721496582031
Epoch 750, training loss: 6.4302544593811035 = 0.12086000293493271 + 1.0 * 6.309394359588623
Epoch 750, val loss: 0.8288066387176514
Epoch 760, training loss: 6.409867763519287 = 0.11436758190393448 + 1.0 * 6.2955002784729
Epoch 760, val loss: 0.8324279189109802
Epoch 770, training loss: 6.403134822845459 = 0.1082562729716301 + 1.0 * 6.2948784828186035
Epoch 770, val loss: 0.8362841010093689
Epoch 780, training loss: 6.396883010864258 = 0.10249396413564682 + 1.0 * 6.294389247894287
Epoch 780, val loss: 0.8399181962013245
Epoch 790, training loss: 6.395394325256348 = 0.09708812087774277 + 1.0 * 6.298305988311768
Epoch 790, val loss: 0.8437634706497192
Epoch 800, training loss: 6.38479471206665 = 0.09202452749013901 + 1.0 * 6.2927703857421875
Epoch 800, val loss: 0.8477016687393188
Epoch 810, training loss: 6.379218578338623 = 0.08726396411657333 + 1.0 * 6.291954517364502
Epoch 810, val loss: 0.8517447113990784
Epoch 820, training loss: 6.377598285675049 = 0.08279885351657867 + 1.0 * 6.294799327850342
Epoch 820, val loss: 0.8555890321731567
Epoch 830, training loss: 6.369765281677246 = 0.0786357894539833 + 1.0 * 6.2911295890808105
Epoch 830, val loss: 0.8597975373268127
Epoch 840, training loss: 6.364836692810059 = 0.07473140954971313 + 1.0 * 6.29010534286499
Epoch 840, val loss: 0.8638288974761963
Epoch 850, training loss: 6.358786106109619 = 0.07108139246702194 + 1.0 * 6.287704944610596
Epoch 850, val loss: 0.8679647445678711
Epoch 860, training loss: 6.353911876678467 = 0.06765355914831161 + 1.0 * 6.286258220672607
Epoch 860, val loss: 0.8720992207527161
Epoch 870, training loss: 6.364892482757568 = 0.0644378587603569 + 1.0 * 6.300454616546631
Epoch 870, val loss: 0.8762987852096558
Epoch 880, training loss: 6.354207515716553 = 0.06141885370016098 + 1.0 * 6.292788505554199
Epoch 880, val loss: 0.879907488822937
Epoch 890, training loss: 6.342764377593994 = 0.0586194284260273 + 1.0 * 6.284144878387451
Epoch 890, val loss: 0.8842487931251526
Epoch 900, training loss: 6.338825702667236 = 0.05597692355513573 + 1.0 * 6.282848834991455
Epoch 900, val loss: 0.8883354663848877
Epoch 910, training loss: 6.340125560760498 = 0.05348774045705795 + 1.0 * 6.286637783050537
Epoch 910, val loss: 0.8922635912895203
Epoch 920, training loss: 6.333467483520508 = 0.05115591734647751 + 1.0 * 6.28231143951416
Epoch 920, val loss: 0.8962177634239197
Epoch 930, training loss: 6.333448886871338 = 0.04896661266684532 + 1.0 * 6.284482479095459
Epoch 930, val loss: 0.9002612233161926
Epoch 940, training loss: 6.32819128036499 = 0.04690833017230034 + 1.0 * 6.281282901763916
Epoch 940, val loss: 0.9042738080024719
Epoch 950, training loss: 6.328253269195557 = 0.044966693967580795 + 1.0 * 6.2832865715026855
Epoch 950, val loss: 0.9082346558570862
Epoch 960, training loss: 6.324813365936279 = 0.043145403265953064 + 1.0 * 6.281668186187744
Epoch 960, val loss: 0.9119549989700317
Epoch 970, training loss: 6.3206095695495605 = 0.04143684729933739 + 1.0 * 6.279172897338867
Epoch 970, val loss: 0.9160586595535278
Epoch 980, training loss: 6.318085193634033 = 0.039822448045015335 + 1.0 * 6.278262615203857
Epoch 980, val loss: 0.9200429916381836
Epoch 990, training loss: 6.323792934417725 = 0.03829469904303551 + 1.0 * 6.285498142242432
Epoch 990, val loss: 0.9239051342010498
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8202424881391671
=== training gcn model ===
Epoch 0, training loss: 10.535666465759277 = 1.9388195276260376 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9354913234710693
Epoch 10, training loss: 10.525298118591309 = 1.928715705871582 + 1.0 * 8.596582412719727
Epoch 10, val loss: 1.9251444339752197
Epoch 20, training loss: 10.510126113891602 = 1.9159953594207764 + 1.0 * 8.594130516052246
Epoch 20, val loss: 1.91219961643219
Epoch 30, training loss: 10.471831321716309 = 1.8979823589324951 + 1.0 * 8.573848724365234
Epoch 30, val loss: 1.893977165222168
Epoch 40, training loss: 10.327613830566406 = 1.8738608360290527 + 1.0 * 8.453752517700195
Epoch 40, val loss: 1.8703105449676514
Epoch 50, training loss: 9.860584259033203 = 1.8470683097839355 + 1.0 * 8.01351547241211
Epoch 50, val loss: 1.8448365926742554
Epoch 60, training loss: 9.60621452331543 = 1.8246735334396362 + 1.0 * 7.781541347503662
Epoch 60, val loss: 1.8248777389526367
Epoch 70, training loss: 9.159623146057129 = 1.809320330619812 + 1.0 * 7.3503031730651855
Epoch 70, val loss: 1.8108789920806885
Epoch 80, training loss: 8.840437889099121 = 1.7993676662445068 + 1.0 * 7.041070461273193
Epoch 80, val loss: 1.801513433456421
Epoch 90, training loss: 8.672477722167969 = 1.7889569997787476 + 1.0 * 6.883520603179932
Epoch 90, val loss: 1.791172742843628
Epoch 100, training loss: 8.56587028503418 = 1.7761763334274292 + 1.0 * 6.789694309234619
Epoch 100, val loss: 1.7790430784225464
Epoch 110, training loss: 8.495072364807129 = 1.7622759342193604 + 1.0 * 6.732796669006348
Epoch 110, val loss: 1.7665284872055054
Epoch 120, training loss: 8.434096336364746 = 1.7480005025863647 + 1.0 * 6.68609619140625
Epoch 120, val loss: 1.7536015510559082
Epoch 130, training loss: 8.374567985534668 = 1.7330483198165894 + 1.0 * 6.641520023345947
Epoch 130, val loss: 1.7398303747177124
Epoch 140, training loss: 8.319549560546875 = 1.7155481576919556 + 1.0 * 6.604001522064209
Epoch 140, val loss: 1.724217176437378
Epoch 150, training loss: 8.261750221252441 = 1.6948407888412476 + 1.0 * 6.566909313201904
Epoch 150, val loss: 1.7064310312271118
Epoch 160, training loss: 8.207815170288086 = 1.6708390712738037 + 1.0 * 6.536976337432861
Epoch 160, val loss: 1.6860265731811523
Epoch 170, training loss: 8.154548645019531 = 1.6429004669189453 + 1.0 * 6.511648654937744
Epoch 170, val loss: 1.6622265577316284
Epoch 180, training loss: 8.102020263671875 = 1.6102794408798218 + 1.0 * 6.491740703582764
Epoch 180, val loss: 1.63433837890625
Epoch 190, training loss: 8.048776626586914 = 1.5725088119506836 + 1.0 * 6.476268291473389
Epoch 190, val loss: 1.6022385358810425
Epoch 200, training loss: 7.99542236328125 = 1.5302152633666992 + 1.0 * 6.465207099914551
Epoch 200, val loss: 1.5667765140533447
Epoch 210, training loss: 7.93765115737915 = 1.4845668077468872 + 1.0 * 6.453084468841553
Epoch 210, val loss: 1.529201865196228
Epoch 220, training loss: 7.878345489501953 = 1.4362609386444092 + 1.0 * 6.442084312438965
Epoch 220, val loss: 1.490231990814209
Epoch 230, training loss: 7.824652671813965 = 1.3865981101989746 + 1.0 * 6.43805456161499
Epoch 230, val loss: 1.4509679079055786
Epoch 240, training loss: 7.765279769897461 = 1.337510108947754 + 1.0 * 6.427769660949707
Epoch 240, val loss: 1.4129565954208374
Epoch 250, training loss: 7.708812713623047 = 1.289793848991394 + 1.0 * 6.419018745422363
Epoch 250, val loss: 1.3769726753234863
Epoch 260, training loss: 7.655271053314209 = 1.243642807006836 + 1.0 * 6.411628246307373
Epoch 260, val loss: 1.3428598642349243
Epoch 270, training loss: 7.608746528625488 = 1.198628544807434 + 1.0 * 6.410118103027344
Epoch 270, val loss: 1.310179352760315
Epoch 280, training loss: 7.557924270629883 = 1.1550581455230713 + 1.0 * 6.402866363525391
Epoch 280, val loss: 1.2791208028793335
Epoch 290, training loss: 7.508887767791748 = 1.112311840057373 + 1.0 * 6.396575927734375
Epoch 290, val loss: 1.249030351638794
Epoch 300, training loss: 7.466299533843994 = 1.069846272468567 + 1.0 * 6.396453380584717
Epoch 300, val loss: 1.2191487550735474
Epoch 310, training loss: 7.417501449584961 = 1.0280218124389648 + 1.0 * 6.389479637145996
Epoch 310, val loss: 1.1895520687103271
Epoch 320, training loss: 7.371541976928711 = 0.9863004684448242 + 1.0 * 6.385241508483887
Epoch 320, val loss: 1.16022789478302
Epoch 330, training loss: 7.331494331359863 = 0.9447882771492004 + 1.0 * 6.3867058753967285
Epoch 330, val loss: 1.1308298110961914
Epoch 340, training loss: 7.284421920776367 = 0.9040977954864502 + 1.0 * 6.380323886871338
Epoch 340, val loss: 1.1020773649215698
Epoch 350, training loss: 7.239286422729492 = 0.8640780448913574 + 1.0 * 6.375208377838135
Epoch 350, val loss: 1.0739585161209106
Epoch 360, training loss: 7.198723793029785 = 0.8245092630386353 + 1.0 * 6.3742146492004395
Epoch 360, val loss: 1.046218991279602
Epoch 370, training loss: 7.161896705627441 = 0.7859876751899719 + 1.0 * 6.375908851623535
Epoch 370, val loss: 1.0191160440444946
Epoch 380, training loss: 7.115193843841553 = 0.7486760020256042 + 1.0 * 6.366518020629883
Epoch 380, val loss: 0.993454098701477
Epoch 390, training loss: 7.07495641708374 = 0.7121886014938354 + 1.0 * 6.362767696380615
Epoch 390, val loss: 0.9686193466186523
Epoch 400, training loss: 7.0352373123168945 = 0.6764082312583923 + 1.0 * 6.358829021453857
Epoch 400, val loss: 0.9446497559547424
Epoch 410, training loss: 7.013498783111572 = 0.6414257287979126 + 1.0 * 6.372073173522949
Epoch 410, val loss: 0.9216518998146057
Epoch 420, training loss: 6.967519760131836 = 0.607852041721344 + 1.0 * 6.359667778015137
Epoch 420, val loss: 0.9000926613807678
Epoch 430, training loss: 6.927491664886475 = 0.5755630731582642 + 1.0 * 6.3519287109375
Epoch 430, val loss: 0.880195140838623
Epoch 440, training loss: 6.893690586090088 = 0.5443255305290222 + 1.0 * 6.349365234375
Epoch 440, val loss: 0.8614625334739685
Epoch 450, training loss: 6.865569591522217 = 0.5142517685890198 + 1.0 * 6.351317882537842
Epoch 450, val loss: 0.8439451456069946
Epoch 460, training loss: 6.83366584777832 = 0.48560020327568054 + 1.0 * 6.3480658531188965
Epoch 460, val loss: 0.8280215859413147
Epoch 470, training loss: 6.8012237548828125 = 0.45816144347190857 + 1.0 * 6.343062400817871
Epoch 470, val loss: 0.8136041164398193
Epoch 480, training loss: 6.7756876945495605 = 0.4319290518760681 + 1.0 * 6.343758583068848
Epoch 480, val loss: 0.8001275062561035
Epoch 490, training loss: 6.744541645050049 = 0.40695080161094666 + 1.0 * 6.33759069442749
Epoch 490, val loss: 0.7880702614784241
Epoch 500, training loss: 6.7201924324035645 = 0.3830968737602234 + 1.0 * 6.337095737457275
Epoch 500, val loss: 0.7771823406219482
Epoch 510, training loss: 6.697307586669922 = 0.3605048656463623 + 1.0 * 6.336802959442139
Epoch 510, val loss: 0.7672672271728516
Epoch 520, training loss: 6.671908378601074 = 0.33921098709106445 + 1.0 * 6.33269739151001
Epoch 520, val loss: 0.758639395236969
Epoch 530, training loss: 6.64939546585083 = 0.31897929310798645 + 1.0 * 6.330416202545166
Epoch 530, val loss: 0.750886082649231
Epoch 540, training loss: 6.631819725036621 = 0.29983192682266235 + 1.0 * 6.3319878578186035
Epoch 540, val loss: 0.7437800765037537
Epoch 550, training loss: 6.609743595123291 = 0.2817870080471039 + 1.0 * 6.327956676483154
Epoch 550, val loss: 0.7377458214759827
Epoch 560, training loss: 6.594095230102539 = 0.2647191286087036 + 1.0 * 6.329376220703125
Epoch 560, val loss: 0.732587993144989
Epoch 570, training loss: 6.574761390686035 = 0.24864114820957184 + 1.0 * 6.326120376586914
Epoch 570, val loss: 0.7279441356658936
Epoch 580, training loss: 6.559659481048584 = 0.23353807628154755 + 1.0 * 6.3261213302612305
Epoch 580, val loss: 0.7242299318313599
Epoch 590, training loss: 6.539853096008301 = 0.2193225920200348 + 1.0 * 6.320530414581299
Epoch 590, val loss: 0.7212685346603394
Epoch 600, training loss: 6.525705814361572 = 0.20592428743839264 + 1.0 * 6.319781303405762
Epoch 600, val loss: 0.718988835811615
Epoch 610, training loss: 6.526965141296387 = 0.19329434633255005 + 1.0 * 6.333670616149902
Epoch 610, val loss: 0.7172566652297974
Epoch 620, training loss: 6.499035835266113 = 0.18157662451267242 + 1.0 * 6.3174591064453125
Epoch 620, val loss: 0.7162087559700012
Epoch 630, training loss: 6.486564636230469 = 0.17058444023132324 + 1.0 * 6.315980434417725
Epoch 630, val loss: 0.7157940864562988
Epoch 640, training loss: 6.473563194274902 = 0.16024994850158691 + 1.0 * 6.3133134841918945
Epoch 640, val loss: 0.7159296274185181
Epoch 650, training loss: 6.46995210647583 = 0.1505919247865677 + 1.0 * 6.319360256195068
Epoch 650, val loss: 0.7164146900177002
Epoch 660, training loss: 6.45442533493042 = 0.1416396051645279 + 1.0 * 6.312785625457764
Epoch 660, val loss: 0.717362105846405
Epoch 670, training loss: 6.442748069763184 = 0.13327166438102722 + 1.0 * 6.309476375579834
Epoch 670, val loss: 0.7188478112220764
Epoch 680, training loss: 6.444057464599609 = 0.12543602287769318 + 1.0 * 6.318621635437012
Epoch 680, val loss: 0.7206949591636658
Epoch 690, training loss: 6.430086612701416 = 0.11818306893110275 + 1.0 * 6.311903476715088
Epoch 690, val loss: 0.7228385210037231
Epoch 700, training loss: 6.423309326171875 = 0.1114305630326271 + 1.0 * 6.311878681182861
Epoch 700, val loss: 0.7253584861755371
Epoch 710, training loss: 6.41199254989624 = 0.10514737665653229 + 1.0 * 6.306845188140869
Epoch 710, val loss: 0.7281202673912048
Epoch 720, training loss: 6.403465747833252 = 0.09928548336029053 + 1.0 * 6.304180145263672
Epoch 720, val loss: 0.7312725782394409
Epoch 730, training loss: 6.401443958282471 = 0.09380080550909042 + 1.0 * 6.307642936706543
Epoch 730, val loss: 0.7345660328865051
Epoch 740, training loss: 6.390461444854736 = 0.08871065825223923 + 1.0 * 6.301750659942627
Epoch 740, val loss: 0.7380584478378296
Epoch 750, training loss: 6.393372058868408 = 0.08395887166261673 + 1.0 * 6.309412956237793
Epoch 750, val loss: 0.7417617440223694
Epoch 760, training loss: 6.380562782287598 = 0.07954876869916916 + 1.0 * 6.301013946533203
Epoch 760, val loss: 0.7455973029136658
Epoch 770, training loss: 6.373977184295654 = 0.0754304826259613 + 1.0 * 6.29854679107666
Epoch 770, val loss: 0.7496310472488403
Epoch 780, training loss: 6.372517108917236 = 0.0715685561299324 + 1.0 * 6.300948619842529
Epoch 780, val loss: 0.753851056098938
Epoch 790, training loss: 6.367048740386963 = 0.06796742975711823 + 1.0 * 6.299081325531006
Epoch 790, val loss: 0.7579557299613953
Epoch 800, training loss: 6.361673355102539 = 0.06460089236497879 + 1.0 * 6.297072410583496
Epoch 800, val loss: 0.7622562646865845
Epoch 810, training loss: 6.360438346862793 = 0.061454836279153824 + 1.0 * 6.298983573913574
Epoch 810, val loss: 0.7666093707084656
Epoch 820, training loss: 6.355319023132324 = 0.0585208460688591 + 1.0 * 6.296798229217529
Epoch 820, val loss: 0.7710139751434326
Epoch 830, training loss: 6.34896183013916 = 0.05577866733074188 + 1.0 * 6.293183326721191
Epoch 830, val loss: 0.7753642201423645
Epoch 840, training loss: 6.344338417053223 = 0.05320461466908455 + 1.0 * 6.291133880615234
Epoch 840, val loss: 0.7799532413482666
Epoch 850, training loss: 6.363327503204346 = 0.05079047381877899 + 1.0 * 6.31253719329834
Epoch 850, val loss: 0.784511387348175
Epoch 860, training loss: 6.340332984924316 = 0.04853007197380066 + 1.0 * 6.291802883148193
Epoch 860, val loss: 0.7887514233589172
Epoch 870, training loss: 6.337710857391357 = 0.04642101004719734 + 1.0 * 6.291289806365967
Epoch 870, val loss: 0.7931874394416809
Epoch 880, training loss: 6.3318657875061035 = 0.04443499445915222 + 1.0 * 6.287430763244629
Epoch 880, val loss: 0.7977986335754395
Epoch 890, training loss: 6.341688632965088 = 0.04255791753530502 + 1.0 * 6.299130916595459
Epoch 890, val loss: 0.8023515343666077
Epoch 900, training loss: 6.334680080413818 = 0.04079669341444969 + 1.0 * 6.293883323669434
Epoch 900, val loss: 0.8067606091499329
Epoch 910, training loss: 6.326865196228027 = 0.03914657235145569 + 1.0 * 6.287718772888184
Epoch 910, val loss: 0.8111964464187622
Epoch 920, training loss: 6.322333335876465 = 0.0375896692276001 + 1.0 * 6.284743785858154
Epoch 920, val loss: 0.8156716823577881
Epoch 930, training loss: 6.320244789123535 = 0.03611429035663605 + 1.0 * 6.284130573272705
Epoch 930, val loss: 0.8202373385429382
Epoch 940, training loss: 6.334537029266357 = 0.03471530228853226 + 1.0 * 6.299821853637695
Epoch 940, val loss: 0.8247068524360657
Epoch 950, training loss: 6.316973686218262 = 0.03340715542435646 + 1.0 * 6.283566474914551
Epoch 950, val loss: 0.8289737701416016
Epoch 960, training loss: 6.3158650398254395 = 0.032171618193387985 + 1.0 * 6.283693313598633
Epoch 960, val loss: 0.8334111571311951
Epoch 970, training loss: 6.312659740447998 = 0.030998775735497475 + 1.0 * 6.281661033630371
Epoch 970, val loss: 0.8379181027412415
Epoch 980, training loss: 6.316904067993164 = 0.029882162809371948 + 1.0 * 6.287022113800049
Epoch 980, val loss: 0.8423645496368408
Epoch 990, training loss: 6.312821865081787 = 0.028825191780924797 + 1.0 * 6.28399658203125
Epoch 990, val loss: 0.8467279672622681
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.819715340010543
=== training gcn model ===
Epoch 0, training loss: 10.542140007019043 = 1.9453089237213135 + 1.0 * 8.596831321716309
Epoch 0, val loss: 1.9424976110458374
Epoch 10, training loss: 10.531617164611816 = 1.9350827932357788 + 1.0 * 8.596534729003906
Epoch 10, val loss: 1.9318184852600098
Epoch 20, training loss: 10.516388893127441 = 1.922277808189392 + 1.0 * 8.594111442565918
Epoch 20, val loss: 1.9181396961212158
Epoch 30, training loss: 10.479146957397461 = 1.9043259620666504 + 1.0 * 8.574821472167969
Epoch 30, val loss: 1.898896336555481
Epoch 40, training loss: 10.3384428024292 = 1.8804436922073364 + 1.0 * 8.457999229431152
Epoch 40, val loss: 1.8745352029800415
Epoch 50, training loss: 9.775336265563965 = 1.8550280332565308 + 1.0 * 7.9203081130981445
Epoch 50, val loss: 1.850203514099121
Epoch 60, training loss: 9.397313117980957 = 1.8355554342269897 + 1.0 * 7.561757564544678
Epoch 60, val loss: 1.8329086303710938
Epoch 70, training loss: 8.978796005249023 = 1.8201971054077148 + 1.0 * 7.15859842300415
Epoch 70, val loss: 1.8192660808563232
Epoch 80, training loss: 8.735957145690918 = 1.8068656921386719 + 1.0 * 6.929091453552246
Epoch 80, val loss: 1.807129144668579
Epoch 90, training loss: 8.603042602539062 = 1.793336033821106 + 1.0 * 6.809706687927246
Epoch 90, val loss: 1.794655203819275
Epoch 100, training loss: 8.510093688964844 = 1.7794240713119507 + 1.0 * 6.730669975280762
Epoch 100, val loss: 1.782136082649231
Epoch 110, training loss: 8.43947982788086 = 1.765468955039978 + 1.0 * 6.67401123046875
Epoch 110, val loss: 1.7699017524719238
Epoch 120, training loss: 8.375518798828125 = 1.7513898611068726 + 1.0 * 6.624128818511963
Epoch 120, val loss: 1.757581353187561
Epoch 130, training loss: 8.32376480102539 = 1.7358945608139038 + 1.0 * 6.587870121002197
Epoch 130, val loss: 1.7443345785140991
Epoch 140, training loss: 8.277772903442383 = 1.7176846265792847 + 1.0 * 6.560088157653809
Epoch 140, val loss: 1.729086995124817
Epoch 150, training loss: 8.233963966369629 = 1.6961805820465088 + 1.0 * 6.537783622741699
Epoch 150, val loss: 1.7112107276916504
Epoch 160, training loss: 8.192296028137207 = 1.6710330247879028 + 1.0 * 6.5212626457214355
Epoch 160, val loss: 1.6901909112930298
Epoch 170, training loss: 8.144354820251465 = 1.6418536901474 + 1.0 * 6.502501010894775
Epoch 170, val loss: 1.666088581085205
Epoch 180, training loss: 8.09598445892334 = 1.6079362630844116 + 1.0 * 6.488048076629639
Epoch 180, val loss: 1.6383121013641357
Epoch 190, training loss: 8.043764114379883 = 1.5685672760009766 + 1.0 * 6.475196361541748
Epoch 190, val loss: 1.6061128377914429
Epoch 200, training loss: 7.99299955368042 = 1.5233272314071655 + 1.0 * 6.469672203063965
Epoch 200, val loss: 1.5692859888076782
Epoch 210, training loss: 7.929941654205322 = 1.4738208055496216 + 1.0 * 6.45612096786499
Epoch 210, val loss: 1.5290805101394653
Epoch 220, training loss: 7.864171981811523 = 1.4205387830734253 + 1.0 * 6.443633079528809
Epoch 220, val loss: 1.485659122467041
Epoch 230, training loss: 7.798669338226318 = 1.3642185926437378 + 1.0 * 6.434450626373291
Epoch 230, val loss: 1.4399253129959106
Epoch 240, training loss: 7.73432731628418 = 1.3064162731170654 + 1.0 * 6.427910804748535
Epoch 240, val loss: 1.3934764862060547
Epoch 250, training loss: 7.670776844024658 = 1.2506394386291504 + 1.0 * 6.420137405395508
Epoch 250, val loss: 1.349346399307251
Epoch 260, training loss: 7.610234260559082 = 1.198089838027954 + 1.0 * 6.412144660949707
Epoch 260, val loss: 1.3083239793777466
Epoch 270, training loss: 7.553481578826904 = 1.1489821672439575 + 1.0 * 6.404499530792236
Epoch 270, val loss: 1.270701289176941
Epoch 280, training loss: 7.502206802368164 = 1.1034764051437378 + 1.0 * 6.398730278015137
Epoch 280, val loss: 1.2366875410079956
Epoch 290, training loss: 7.463024616241455 = 1.062455177307129 + 1.0 * 6.400569438934326
Epoch 290, val loss: 1.2069296836853027
Epoch 300, training loss: 7.415295600891113 = 1.025638461112976 + 1.0 * 6.389657020568848
Epoch 300, val loss: 1.181270956993103
Epoch 310, training loss: 7.374811172485352 = 0.991797149181366 + 1.0 * 6.38301420211792
Epoch 310, val loss: 1.1581361293792725
Epoch 320, training loss: 7.337884426116943 = 0.9597883224487305 + 1.0 * 6.378096103668213
Epoch 320, val loss: 1.1369268894195557
Epoch 330, training loss: 7.3041911125183105 = 0.9288576245307922 + 1.0 * 6.375333309173584
Epoch 330, val loss: 1.116819977760315
Epoch 340, training loss: 7.270503997802734 = 0.8988243341445923 + 1.0 * 6.371679782867432
Epoch 340, val loss: 1.097358226776123
Epoch 350, training loss: 7.235464572906494 = 0.8690619468688965 + 1.0 * 6.366402626037598
Epoch 350, val loss: 1.0783960819244385
Epoch 360, training loss: 7.201358795166016 = 0.8386924862861633 + 1.0 * 6.362666130065918
Epoch 360, val loss: 1.0588347911834717
Epoch 370, training loss: 7.1661176681518555 = 0.807262122631073 + 1.0 * 6.358855724334717
Epoch 370, val loss: 1.0382864475250244
Epoch 380, training loss: 7.140331745147705 = 0.7747079730033875 + 1.0 * 6.365623950958252
Epoch 380, val loss: 1.0168300867080688
Epoch 390, training loss: 7.095155239105225 = 0.7415342926979065 + 1.0 * 6.353621006011963
Epoch 390, val loss: 0.9948602914810181
Epoch 400, training loss: 7.0583815574646 = 0.7077122926712036 + 1.0 * 6.3506693840026855
Epoch 400, val loss: 0.9723606705665588
Epoch 410, training loss: 7.020542621612549 = 0.6732305884361267 + 1.0 * 6.347311973571777
Epoch 410, val loss: 0.9492892622947693
Epoch 420, training loss: 6.9899797439575195 = 0.6385608315467834 + 1.0 * 6.351418972015381
Epoch 420, val loss: 0.926139771938324
Epoch 430, training loss: 6.948000431060791 = 0.6047013401985168 + 1.0 * 6.34329891204834
Epoch 430, val loss: 0.9039934277534485
Epoch 440, training loss: 6.912550926208496 = 0.5714249610900879 + 1.0 * 6.341125965118408
Epoch 440, val loss: 0.8829492926597595
Epoch 450, training loss: 6.877666473388672 = 0.5389134883880615 + 1.0 * 6.3387532234191895
Epoch 450, val loss: 0.8628408908843994
Epoch 460, training loss: 6.844263076782227 = 0.5076311230659485 + 1.0 * 6.336631774902344
Epoch 460, val loss: 0.8443156480789185
Epoch 470, training loss: 6.812429904937744 = 0.4778769612312317 + 1.0 * 6.334552764892578
Epoch 470, val loss: 0.8278886079788208
Epoch 480, training loss: 6.781687259674072 = 0.4494767487049103 + 1.0 * 6.332210540771484
Epoch 480, val loss: 0.8131871819496155
Epoch 490, training loss: 6.762927532196045 = 0.4223558306694031 + 1.0 * 6.340571880340576
Epoch 490, val loss: 0.8001987934112549
Epoch 500, training loss: 6.72685432434082 = 0.39693766832351685 + 1.0 * 6.329916477203369
Epoch 500, val loss: 0.7891072630882263
Epoch 510, training loss: 6.69968318939209 = 0.3729511499404907 + 1.0 * 6.326732158660889
Epoch 510, val loss: 0.7799398899078369
Epoch 520, training loss: 6.675737380981445 = 0.35027021169662476 + 1.0 * 6.325467109680176
Epoch 520, val loss: 0.7722287178039551
Epoch 530, training loss: 6.663582801818848 = 0.3289017975330353 + 1.0 * 6.334681034088135
Epoch 530, val loss: 0.7658873796463013
Epoch 540, training loss: 6.633365154266357 = 0.3091551959514618 + 1.0 * 6.324210166931152
Epoch 540, val loss: 0.7611334323883057
Epoch 550, training loss: 6.611314296722412 = 0.2906496822834015 + 1.0 * 6.320664405822754
Epoch 550, val loss: 0.7579521536827087
Epoch 560, training loss: 6.591451644897461 = 0.27325090765953064 + 1.0 * 6.318200588226318
Epoch 560, val loss: 0.7556934356689453
Epoch 570, training loss: 6.576786518096924 = 0.25686338543891907 + 1.0 * 6.319922924041748
Epoch 570, val loss: 0.754652738571167
Epoch 580, training loss: 6.563882827758789 = 0.24159026145935059 + 1.0 * 6.322292804718018
Epoch 580, val loss: 0.75445556640625
Epoch 590, training loss: 6.545948028564453 = 0.22739702463150024 + 1.0 * 6.318551063537598
Epoch 590, val loss: 0.7554528117179871
Epoch 600, training loss: 6.526346206665039 = 0.21415869891643524 + 1.0 * 6.312187671661377
Epoch 600, val loss: 0.7572566866874695
Epoch 610, training loss: 6.519905090332031 = 0.20174844563007355 + 1.0 * 6.318156719207764
Epoch 610, val loss: 0.7597573399543762
Epoch 620, training loss: 6.5017805099487305 = 0.19015280902385712 + 1.0 * 6.3116278648376465
Epoch 620, val loss: 0.762848436832428
Epoch 630, training loss: 6.489796161651611 = 0.17935344576835632 + 1.0 * 6.310442924499512
Epoch 630, val loss: 0.766826331615448
Epoch 640, training loss: 6.477943420410156 = 0.16923540830612183 + 1.0 * 6.308708190917969
Epoch 640, val loss: 0.7711596488952637
Epoch 650, training loss: 6.464761734008789 = 0.15981325507164001 + 1.0 * 6.304948329925537
Epoch 650, val loss: 0.7760920524597168
Epoch 660, training loss: 6.460029602050781 = 0.15099111199378967 + 1.0 * 6.3090386390686035
Epoch 660, val loss: 0.7814789414405823
Epoch 670, training loss: 6.4524827003479 = 0.1427909880876541 + 1.0 * 6.309691905975342
Epoch 670, val loss: 0.7870531678199768
Epoch 680, training loss: 6.437493324279785 = 0.13513892889022827 + 1.0 * 6.302354335784912
Epoch 680, val loss: 0.7931915521621704
Epoch 690, training loss: 6.428771018981934 = 0.12798526883125305 + 1.0 * 6.300785541534424
Epoch 690, val loss: 0.7995274662971497
Epoch 700, training loss: 6.4303388595581055 = 0.12127658724784851 + 1.0 * 6.309062480926514
Epoch 700, val loss: 0.8059410452842712
Epoch 710, training loss: 6.414585113525391 = 0.1150476336479187 + 1.0 * 6.299537658691406
Epoch 710, val loss: 0.8127831816673279
Epoch 720, training loss: 6.405400276184082 = 0.10921316593885422 + 1.0 * 6.296186923980713
Epoch 720, val loss: 0.819891095161438
Epoch 730, training loss: 6.4029669761657715 = 0.10373368859291077 + 1.0 * 6.299233436584473
Epoch 730, val loss: 0.8268815279006958
Epoch 740, training loss: 6.395227909088135 = 0.09859718382358551 + 1.0 * 6.296630859375
Epoch 740, val loss: 0.8340312838554382
Epoch 750, training loss: 6.388027191162109 = 0.09380184859037399 + 1.0 * 6.294225215911865
Epoch 750, val loss: 0.841499924659729
Epoch 760, training loss: 6.390134811401367 = 0.08930569887161255 + 1.0 * 6.30082893371582
Epoch 760, val loss: 0.8484801650047302
Epoch 770, training loss: 6.375942707061768 = 0.0850987657904625 + 1.0 * 6.290843963623047
Epoch 770, val loss: 0.8560894727706909
Epoch 780, training loss: 6.371433734893799 = 0.08114533871412277 + 1.0 * 6.29028844833374
Epoch 780, val loss: 0.8635725975036621
Epoch 790, training loss: 6.36663818359375 = 0.07741737365722656 + 1.0 * 6.289220809936523
Epoch 790, val loss: 0.8709685206413269
Epoch 800, training loss: 6.365789890289307 = 0.07390892505645752 + 1.0 * 6.291881084442139
Epoch 800, val loss: 0.87815260887146
Epoch 810, training loss: 6.360686779022217 = 0.07063038647174835 + 1.0 * 6.290056228637695
Epoch 810, val loss: 0.8859214782714844
Epoch 820, training loss: 6.354407787322998 = 0.06753623485565186 + 1.0 * 6.286871433258057
Epoch 820, val loss: 0.8934533596038818
Epoch 830, training loss: 6.3554301261901855 = 0.06462263315916061 + 1.0 * 6.290807723999023
Epoch 830, val loss: 0.9003651142120361
Epoch 840, training loss: 6.34695291519165 = 0.06189360469579697 + 1.0 * 6.285059452056885
Epoch 840, val loss: 0.9081045389175415
Epoch 850, training loss: 6.343131065368652 = 0.059310030192136765 + 1.0 * 6.283821105957031
Epoch 850, val loss: 0.9155855178833008
Epoch 860, training loss: 6.3391242027282715 = 0.056856419891119 + 1.0 * 6.2822675704956055
Epoch 860, val loss: 0.922842800617218
Epoch 870, training loss: 6.341599464416504 = 0.054532140493392944 + 1.0 * 6.287067413330078
Epoch 870, val loss: 0.9301186203956604
Epoch 880, training loss: 6.341309070587158 = 0.05233673378825188 + 1.0 * 6.2889723777771
Epoch 880, val loss: 0.9371700286865234
Epoch 890, training loss: 6.3389081954956055 = 0.05028007924556732 + 1.0 * 6.288628101348877
Epoch 890, val loss: 0.9443860650062561
Epoch 900, training loss: 6.330097675323486 = 0.048337120562791824 + 1.0 * 6.2817606925964355
Epoch 900, val loss: 0.951659619808197
Epoch 910, training loss: 6.32602596282959 = 0.04649357497692108 + 1.0 * 6.279532432556152
Epoch 910, val loss: 0.958778977394104
Epoch 920, training loss: 6.322689533233643 = 0.04473454877734184 + 1.0 * 6.277955055236816
Epoch 920, val loss: 0.9657643437385559
Epoch 930, training loss: 6.335665225982666 = 0.04306786134839058 + 1.0 * 6.29259729385376
Epoch 930, val loss: 0.9726319313049316
Epoch 940, training loss: 6.322345733642578 = 0.04148523882031441 + 1.0 * 6.280860424041748
Epoch 940, val loss: 0.9791513085365295
Epoch 950, training loss: 6.315963268280029 = 0.03999389708042145 + 1.0 * 6.275969505310059
Epoch 950, val loss: 0.9863579869270325
Epoch 960, training loss: 6.313404083251953 = 0.038565970957279205 + 1.0 * 6.274837970733643
Epoch 960, val loss: 0.9929115772247314
Epoch 970, training loss: 6.318938255310059 = 0.037205442786216736 + 1.0 * 6.28173303604126
Epoch 970, val loss: 0.999401867389679
Epoch 980, training loss: 6.31428861618042 = 0.035917408764362335 + 1.0 * 6.278371334075928
Epoch 980, val loss: 1.0059127807617188
Epoch 990, training loss: 6.312504291534424 = 0.03469470515847206 + 1.0 * 6.2778096199035645
Epoch 990, val loss: 1.0123776197433472
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8181338956246705
The final CL Acc:0.77160, 0.01062, The final GNN Acc:0.81936, 0.00090
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13214])
remove edge: torch.Size([2, 7810])
updated graph: torch.Size([2, 10468])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.53271484375 = 1.9358954429626465 + 1.0 * 8.596819877624512
Epoch 0, val loss: 1.9276067018508911
Epoch 10, training loss: 10.522896766662598 = 1.9264060258865356 + 1.0 * 8.596490859985352
Epoch 10, val loss: 1.9181586503982544
Epoch 20, training loss: 10.50804328918457 = 1.9142484664916992 + 1.0 * 8.593794822692871
Epoch 20, val loss: 1.90548837184906
Epoch 30, training loss: 10.469897270202637 = 1.8970348834991455 + 1.0 * 8.57286262512207
Epoch 30, val loss: 1.8870829343795776
Epoch 40, training loss: 10.333240509033203 = 1.8738338947296143 + 1.0 * 8.459406852722168
Epoch 40, val loss: 1.8628525733947754
Epoch 50, training loss: 9.790779113769531 = 1.8485774993896484 + 1.0 * 7.942201614379883
Epoch 50, val loss: 1.837614893913269
Epoch 60, training loss: 9.260826110839844 = 1.8266911506652832 + 1.0 * 7.434134483337402
Epoch 60, val loss: 1.8171815872192383
Epoch 70, training loss: 8.901957511901855 = 1.8094453811645508 + 1.0 * 7.092512130737305
Epoch 70, val loss: 1.8015838861465454
Epoch 80, training loss: 8.712714195251465 = 1.7930020093917847 + 1.0 * 6.919712066650391
Epoch 80, val loss: 1.7864339351654053
Epoch 90, training loss: 8.608577728271484 = 1.7759451866149902 + 1.0 * 6.832632541656494
Epoch 90, val loss: 1.7704803943634033
Epoch 100, training loss: 8.534357070922852 = 1.7577160596847534 + 1.0 * 6.776640892028809
Epoch 100, val loss: 1.7537661790847778
Epoch 110, training loss: 8.480239868164062 = 1.7389413118362427 + 1.0 * 6.741298198699951
Epoch 110, val loss: 1.7373768091201782
Epoch 120, training loss: 8.424398422241211 = 1.718366026878357 + 1.0 * 6.706032752990723
Epoch 120, val loss: 1.720051646232605
Epoch 130, training loss: 8.360980033874512 = 1.6948950290679932 + 1.0 * 6.6660847663879395
Epoch 130, val loss: 1.7004618644714355
Epoch 140, training loss: 8.296523094177246 = 1.6676322221755981 + 1.0 * 6.6288909912109375
Epoch 140, val loss: 1.6773885488510132
Epoch 150, training loss: 8.24059009552002 = 1.6350396871566772 + 1.0 * 6.605550765991211
Epoch 150, val loss: 1.6494876146316528
Epoch 160, training loss: 8.183675765991211 = 1.5960873365402222 + 1.0 * 6.587588787078857
Epoch 160, val loss: 1.616075038909912
Epoch 170, training loss: 8.118866920471191 = 1.5500847101211548 + 1.0 * 6.568781852722168
Epoch 170, val loss: 1.577211856842041
Epoch 180, training loss: 8.04493236541748 = 1.49736487865448 + 1.0 * 6.547567844390869
Epoch 180, val loss: 1.5332351922988892
Epoch 190, training loss: 7.962558269500732 = 1.4381804466247559 + 1.0 * 6.524377822875977
Epoch 190, val loss: 1.4846155643463135
Epoch 200, training loss: 7.879145622253418 = 1.375145673751831 + 1.0 * 6.503999710083008
Epoch 200, val loss: 1.4342193603515625
Epoch 210, training loss: 7.799951076507568 = 1.3110466003417969 + 1.0 * 6.4889044761657715
Epoch 210, val loss: 1.3839343786239624
Epoch 220, training loss: 7.719723701477051 = 1.245156168937683 + 1.0 * 6.474567413330078
Epoch 220, val loss: 1.332971453666687
Epoch 230, training loss: 7.6435465812683105 = 1.1786956787109375 + 1.0 * 6.464850902557373
Epoch 230, val loss: 1.2822431325912476
Epoch 240, training loss: 7.566324234008789 = 1.1141769886016846 + 1.0 * 6.452147006988525
Epoch 240, val loss: 1.2331286668777466
Epoch 250, training loss: 7.493894577026367 = 1.0513790845870972 + 1.0 * 6.4425153732299805
Epoch 250, val loss: 1.1859380006790161
Epoch 260, training loss: 7.424840927124023 = 0.991574764251709 + 1.0 * 6.4332661628723145
Epoch 260, val loss: 1.1414111852645874
Epoch 270, training loss: 7.359759330749512 = 0.9350799322128296 + 1.0 * 6.424679279327393
Epoch 270, val loss: 1.0997047424316406
Epoch 280, training loss: 7.300574779510498 = 0.8814901113510132 + 1.0 * 6.419084548950195
Epoch 280, val loss: 1.0606778860092163
Epoch 290, training loss: 7.246194362640381 = 0.8317405581474304 + 1.0 * 6.414453983306885
Epoch 290, val loss: 1.0249693393707275
Epoch 300, training loss: 7.1920928955078125 = 0.7862064838409424 + 1.0 * 6.405886173248291
Epoch 300, val loss: 0.9927682280540466
Epoch 310, training loss: 7.143214225769043 = 0.7438475489616394 + 1.0 * 6.399366855621338
Epoch 310, val loss: 0.9633535742759705
Epoch 320, training loss: 7.104901313781738 = 0.7044174075126648 + 1.0 * 6.400484085083008
Epoch 320, val loss: 0.9363825917243958
Epoch 330, training loss: 7.057710647583008 = 0.6680898666381836 + 1.0 * 6.389620780944824
Epoch 330, val loss: 0.9120505452156067
Epoch 340, training loss: 7.0187859535217285 = 0.634150505065918 + 1.0 * 6.3846354484558105
Epoch 340, val loss: 0.8899701237678528
Epoch 350, training loss: 6.984467029571533 = 0.6021806597709656 + 1.0 * 6.382286548614502
Epoch 350, val loss: 0.8697674870491028
Epoch 360, training loss: 6.952916145324707 = 0.5722872018814087 + 1.0 * 6.380629062652588
Epoch 360, val loss: 0.851481556892395
Epoch 370, training loss: 6.916960716247559 = 0.5442401170730591 + 1.0 * 6.372720718383789
Epoch 370, val loss: 0.8350416421890259
Epoch 380, training loss: 6.886610507965088 = 0.5174413323402405 + 1.0 * 6.369169235229492
Epoch 380, val loss: 0.8200268149375916
Epoch 390, training loss: 6.858216285705566 = 0.49188441038131714 + 1.0 * 6.366332054138184
Epoch 390, val loss: 0.8061810731887817
Epoch 400, training loss: 6.830193042755127 = 0.4674622118473053 + 1.0 * 6.362730979919434
Epoch 400, val loss: 0.7937206625938416
Epoch 410, training loss: 6.8034796714782715 = 0.4439430832862854 + 1.0 * 6.359536647796631
Epoch 410, val loss: 0.782288134098053
Epoch 420, training loss: 6.781773090362549 = 0.4213522970676422 + 1.0 * 6.3604207038879395
Epoch 420, val loss: 0.7717705368995667
Epoch 430, training loss: 6.754632472991943 = 0.39979782700538635 + 1.0 * 6.35483455657959
Epoch 430, val loss: 0.7622820138931274
Epoch 440, training loss: 6.731118202209473 = 0.3790033161640167 + 1.0 * 6.352114677429199
Epoch 440, val loss: 0.7535468339920044
Epoch 450, training loss: 6.710351467132568 = 0.3588353991508484 + 1.0 * 6.351516246795654
Epoch 450, val loss: 0.7454519867897034
Epoch 460, training loss: 6.698598861694336 = 0.33943620324134827 + 1.0 * 6.3591628074646
Epoch 460, val loss: 0.7380536198616028
Epoch 470, training loss: 6.669439792633057 = 0.32085883617401123 + 1.0 * 6.348580837249756
Epoch 470, val loss: 0.7313470840454102
Epoch 480, training loss: 6.646346569061279 = 0.3028966784477234 + 1.0 * 6.34345006942749
Epoch 480, val loss: 0.7251355051994324
Epoch 490, training loss: 6.625615119934082 = 0.2855035066604614 + 1.0 * 6.34011173248291
Epoch 490, val loss: 0.7194197773933411
Epoch 500, training loss: 6.6133904457092285 = 0.2686827480792999 + 1.0 * 6.344707489013672
Epoch 500, val loss: 0.7141903638839722
Epoch 510, training loss: 6.592552661895752 = 0.252652108669281 + 1.0 * 6.339900493621826
Epoch 510, val loss: 0.7095456719398499
Epoch 520, training loss: 6.5731964111328125 = 0.23737074434757233 + 1.0 * 6.335825443267822
Epoch 520, val loss: 0.7054182291030884
Epoch 530, training loss: 6.560656547546387 = 0.2228040099143982 + 1.0 * 6.337852478027344
Epoch 530, val loss: 0.7018654942512512
Epoch 540, training loss: 6.54119348526001 = 0.2089877873659134 + 1.0 * 6.332205772399902
Epoch 540, val loss: 0.6988879442214966
Epoch 550, training loss: 6.535324573516846 = 0.19605907797813416 + 1.0 * 6.3392653465271
Epoch 550, val loss: 0.696523129940033
Epoch 560, training loss: 6.516020774841309 = 0.1840345561504364 + 1.0 * 6.331986427307129
Epoch 560, val loss: 0.6947200894355774
Epoch 570, training loss: 6.498647689819336 = 0.17271541059017181 + 1.0 * 6.325932502746582
Epoch 570, val loss: 0.6934275031089783
Epoch 580, training loss: 6.485518455505371 = 0.1620512157678604 + 1.0 * 6.323467254638672
Epoch 580, val loss: 0.6926783919334412
Epoch 590, training loss: 6.473328113555908 = 0.15201224386692047 + 1.0 * 6.321315765380859
Epoch 590, val loss: 0.6924178004264832
Epoch 600, training loss: 6.462059497833252 = 0.14257830381393433 + 1.0 * 6.319481372833252
Epoch 600, val loss: 0.6927003264427185
Epoch 610, training loss: 6.463777542114258 = 0.13374003767967224 + 1.0 * 6.330037593841553
Epoch 610, val loss: 0.6934903264045715
Epoch 620, training loss: 6.445707321166992 = 0.12559416890144348 + 1.0 * 6.320113182067871
Epoch 620, val loss: 0.6948548555374146
Epoch 630, training loss: 6.436523914337158 = 0.11804769933223724 + 1.0 * 6.31847620010376
Epoch 630, val loss: 0.696370005607605
Epoch 640, training loss: 6.425250053405762 = 0.11099492013454437 + 1.0 * 6.314255237579346
Epoch 640, val loss: 0.69833904504776
Epoch 650, training loss: 6.4351606369018555 = 0.10441360622644424 + 1.0 * 6.330747127532959
Epoch 650, val loss: 0.7007710933685303
Epoch 660, training loss: 6.411099433898926 = 0.0983557254076004 + 1.0 * 6.312743663787842
Epoch 660, val loss: 0.7034193873405457
Epoch 670, training loss: 6.403297424316406 = 0.09270227700471878 + 1.0 * 6.3105950355529785
Epoch 670, val loss: 0.7063164710998535
Epoch 680, training loss: 6.399781703948975 = 0.08742054551839828 + 1.0 * 6.312361240386963
Epoch 680, val loss: 0.7096083760261536
Epoch 690, training loss: 6.393129825592041 = 0.082516610622406 + 1.0 * 6.31061315536499
Epoch 690, val loss: 0.7131946682929993
Epoch 700, training loss: 6.385737895965576 = 0.0779528096318245 + 1.0 * 6.3077850341796875
Epoch 700, val loss: 0.7169467806816101
Epoch 710, training loss: 6.386708736419678 = 0.07368937134742737 + 1.0 * 6.313019275665283
Epoch 710, val loss: 0.7209898829460144
Epoch 720, training loss: 6.382333755493164 = 0.06975173205137253 + 1.0 * 6.312582015991211
Epoch 720, val loss: 0.725222110748291
Epoch 730, training loss: 6.370884895324707 = 0.06608612090349197 + 1.0 * 6.304798603057861
Epoch 730, val loss: 0.7295168042182922
Epoch 740, training loss: 6.364760875701904 = 0.06266486644744873 + 1.0 * 6.302095890045166
Epoch 740, val loss: 0.7339595556259155
Epoch 750, training loss: 6.360579013824463 = 0.0594613216817379 + 1.0 * 6.301117897033691
Epoch 750, val loss: 0.7385839223861694
Epoch 760, training loss: 6.3816704750061035 = 0.05646387115120888 + 1.0 * 6.325206756591797
Epoch 760, val loss: 0.7434135675430298
Epoch 770, training loss: 6.355943202972412 = 0.05370669811964035 + 1.0 * 6.302236557006836
Epoch 770, val loss: 0.7482767105102539
Epoch 780, training loss: 6.349456787109375 = 0.051130540668964386 + 1.0 * 6.298326015472412
Epoch 780, val loss: 0.7530064582824707
Epoch 790, training loss: 6.345874786376953 = 0.048717040568590164 + 1.0 * 6.2971577644348145
Epoch 790, val loss: 0.7578960061073303
Epoch 800, training loss: 6.343154430389404 = 0.046453870832920074 + 1.0 * 6.296700477600098
Epoch 800, val loss: 0.7629075646400452
Epoch 810, training loss: 6.342799663543701 = 0.0443381704390049 + 1.0 * 6.298461437225342
Epoch 810, val loss: 0.7680498361587524
Epoch 820, training loss: 6.338674068450928 = 0.04236586391925812 + 1.0 * 6.2963080406188965
Epoch 820, val loss: 0.7730624675750732
Epoch 830, training loss: 6.333773136138916 = 0.04051676765084267 + 1.0 * 6.2932562828063965
Epoch 830, val loss: 0.778052568435669
Epoch 840, training loss: 6.3510894775390625 = 0.03877658769488335 + 1.0 * 6.312313079833984
Epoch 840, val loss: 0.7832131385803223
Epoch 850, training loss: 6.329810619354248 = 0.03716530278325081 + 1.0 * 6.292645454406738
Epoch 850, val loss: 0.7882687449455261
Epoch 860, training loss: 6.326404094696045 = 0.03565177693963051 + 1.0 * 6.290752410888672
Epoch 860, val loss: 0.7931662797927856
Epoch 870, training loss: 6.324401378631592 = 0.03422405198216438 + 1.0 * 6.290177345275879
Epoch 870, val loss: 0.7981372475624084
Epoch 880, training loss: 6.334969997406006 = 0.03287884220480919 + 1.0 * 6.302091121673584
Epoch 880, val loss: 0.8032081127166748
Epoch 890, training loss: 6.323802947998047 = 0.03161182627081871 + 1.0 * 6.292191028594971
Epoch 890, val loss: 0.8082581162452698
Epoch 900, training loss: 6.319135665893555 = 0.030421918258070946 + 1.0 * 6.2887139320373535
Epoch 900, val loss: 0.8130987286567688
Epoch 910, training loss: 6.316639423370361 = 0.029290731996297836 + 1.0 * 6.287348747253418
Epoch 910, val loss: 0.8180573582649231
Epoch 920, training loss: 6.314382076263428 = 0.028225289657711983 + 1.0 * 6.28615665435791
Epoch 920, val loss: 0.8230934143066406
Epoch 930, training loss: 6.315124034881592 = 0.027222497388720512 + 1.0 * 6.287901401519775
Epoch 930, val loss: 0.827904224395752
Epoch 940, training loss: 6.309471607208252 = 0.026272635906934738 + 1.0 * 6.283198833465576
Epoch 940, val loss: 0.8326399922370911
Epoch 950, training loss: 6.319775104522705 = 0.025368107482790947 + 1.0 * 6.294406890869141
Epoch 950, val loss: 0.8375198245048523
Epoch 960, training loss: 6.311951160430908 = 0.024518808349967003 + 1.0 * 6.2874321937561035
Epoch 960, val loss: 0.8423647880554199
Epoch 970, training loss: 6.307214736938477 = 0.023714309558272362 + 1.0 * 6.2835001945495605
Epoch 970, val loss: 0.8469732403755188
Epoch 980, training loss: 6.303335189819336 = 0.02294909954071045 + 1.0 * 6.280385971069336
Epoch 980, val loss: 0.8515541553497314
Epoch 990, training loss: 6.302096843719482 = 0.022217394784092903 + 1.0 * 6.279879570007324
Epoch 990, val loss: 0.8562272191047668
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 10.563423156738281 = 1.9665883779525757 + 1.0 * 8.596835136413574
Epoch 0, val loss: 1.9656604528427124
Epoch 10, training loss: 10.55295181274414 = 1.9563839435577393 + 1.0 * 8.59656810760498
Epoch 10, val loss: 1.9555535316467285
Epoch 20, training loss: 10.537878036499023 = 1.943885326385498 + 1.0 * 8.593993186950684
Epoch 20, val loss: 1.9431227445602417
Epoch 30, training loss: 10.498390197753906 = 1.9265472888946533 + 1.0 * 8.571843147277832
Epoch 30, val loss: 1.9258036613464355
Epoch 40, training loss: 10.341297149658203 = 1.9036003351211548 + 1.0 * 8.43769645690918
Epoch 40, val loss: 1.9037967920303345
Epoch 50, training loss: 9.872438430786133 = 1.8778725862503052 + 1.0 * 7.994565963745117
Epoch 50, val loss: 1.8802348375320435
Epoch 60, training loss: 9.629731178283691 = 1.853712797164917 + 1.0 * 7.7760186195373535
Epoch 60, val loss: 1.8590136766433716
Epoch 70, training loss: 9.218429565429688 = 1.8343195915222168 + 1.0 * 7.384110450744629
Epoch 70, val loss: 1.8420065641403198
Epoch 80, training loss: 8.886542320251465 = 1.8192824125289917 + 1.0 * 7.067260265350342
Epoch 80, val loss: 1.8287231922149658
Epoch 90, training loss: 8.719701766967773 = 1.8035608530044556 + 1.0 * 6.916141033172607
Epoch 90, val loss: 1.8142318725585938
Epoch 100, training loss: 8.578695297241211 = 1.784254550933838 + 1.0 * 6.794441223144531
Epoch 100, val loss: 1.7980189323425293
Epoch 110, training loss: 8.492400169372559 = 1.7665833234786987 + 1.0 * 6.72581672668457
Epoch 110, val loss: 1.7834322452545166
Epoch 120, training loss: 8.418212890625 = 1.750717282295227 + 1.0 * 6.667495250701904
Epoch 120, val loss: 1.769357681274414
Epoch 130, training loss: 8.360798835754395 = 1.7341721057891846 + 1.0 * 6.626626968383789
Epoch 130, val loss: 1.7541284561157227
Epoch 140, training loss: 8.31110954284668 = 1.715169906616211 + 1.0 * 6.595940113067627
Epoch 140, val loss: 1.7367868423461914
Epoch 150, training loss: 8.263984680175781 = 1.6926732063293457 + 1.0 * 6.571311950683594
Epoch 150, val loss: 1.7169380187988281
Epoch 160, training loss: 8.223885536193848 = 1.6666522026062012 + 1.0 * 6.5572333335876465
Epoch 160, val loss: 1.694298267364502
Epoch 170, training loss: 8.166940689086914 = 1.6371678113937378 + 1.0 * 6.529772758483887
Epoch 170, val loss: 1.6690772771835327
Epoch 180, training loss: 8.114307403564453 = 1.603650689125061 + 1.0 * 6.510656356811523
Epoch 180, val loss: 1.640325903892517
Epoch 190, training loss: 8.059381484985352 = 1.5648951530456543 + 1.0 * 6.4944868087768555
Epoch 190, val loss: 1.6071661710739136
Epoch 200, training loss: 8.000276565551758 = 1.520442008972168 + 1.0 * 6.479834079742432
Epoch 200, val loss: 1.5691839456558228
Epoch 210, training loss: 7.941890716552734 = 1.470453143119812 + 1.0 * 6.471437454223633
Epoch 210, val loss: 1.5266234874725342
Epoch 220, training loss: 7.8719940185546875 = 1.4173200130462646 + 1.0 * 6.454674243927002
Epoch 220, val loss: 1.481753945350647
Epoch 230, training loss: 7.804479122161865 = 1.3619424104690552 + 1.0 * 6.4425368309021
Epoch 230, val loss: 1.4350076913833618
Epoch 240, training loss: 7.7366180419921875 = 1.3046176433563232 + 1.0 * 6.432000160217285
Epoch 240, val loss: 1.3865963220596313
Epoch 250, training loss: 7.67266321182251 = 1.2464922666549683 + 1.0 * 6.426170825958252
Epoch 250, val loss: 1.3379307985305786
Epoch 260, training loss: 7.605011940002441 = 1.189391016960144 + 1.0 * 6.415620803833008
Epoch 260, val loss: 1.2905833721160889
Epoch 270, training loss: 7.542300224304199 = 1.1340105533599854 + 1.0 * 6.408289432525635
Epoch 270, val loss: 1.244958758354187
Epoch 280, training loss: 7.482844352722168 = 1.0809142589569092 + 1.0 * 6.401930332183838
Epoch 280, val loss: 1.2017767429351807
Epoch 290, training loss: 7.426519393920898 = 1.0310003757476807 + 1.0 * 6.395519256591797
Epoch 290, val loss: 1.161515474319458
Epoch 300, training loss: 7.375713348388672 = 0.9845674633979797 + 1.0 * 6.391145706176758
Epoch 300, val loss: 1.1245566606521606
Epoch 310, training loss: 7.324504852294922 = 0.9405043125152588 + 1.0 * 6.384000778198242
Epoch 310, val loss: 1.0900444984436035
Epoch 320, training loss: 7.279060363769531 = 0.8985931873321533 + 1.0 * 6.380466938018799
Epoch 320, val loss: 1.057535171508789
Epoch 330, training loss: 7.235748291015625 = 0.859024167060852 + 1.0 * 6.3767242431640625
Epoch 330, val loss: 1.027105450630188
Epoch 340, training loss: 7.1921916007995605 = 0.8210654258728027 + 1.0 * 6.371126174926758
Epoch 340, val loss: 0.9983418583869934
Epoch 350, training loss: 7.170583724975586 = 0.7844998836517334 + 1.0 * 6.386084079742432
Epoch 350, val loss: 0.9709537029266357
Epoch 360, training loss: 7.115127086639404 = 0.7501837611198425 + 1.0 * 6.364943504333496
Epoch 360, val loss: 0.9458614587783813
Epoch 370, training loss: 7.077404022216797 = 0.7174059748649597 + 1.0 * 6.3599982261657715
Epoch 370, val loss: 0.9226688146591187
Epoch 380, training loss: 7.047459602355957 = 0.6859830021858215 + 1.0 * 6.361476421356201
Epoch 380, val loss: 0.9010511040687561
Epoch 390, training loss: 7.011593818664551 = 0.6561291217803955 + 1.0 * 6.355464458465576
Epoch 390, val loss: 0.881264328956604
Epoch 400, training loss: 6.978129863739014 = 0.6274943351745605 + 1.0 * 6.350635528564453
Epoch 400, val loss: 0.8631135821342468
Epoch 410, training loss: 6.956077575683594 = 0.5998955368995667 + 1.0 * 6.356182098388672
Epoch 410, val loss: 0.8466539978981018
Epoch 420, training loss: 6.922684192657471 = 0.5737109780311584 + 1.0 * 6.348973274230957
Epoch 420, val loss: 0.832044243812561
Epoch 430, training loss: 6.892738342285156 = 0.5487217903137207 + 1.0 * 6.3440165519714355
Epoch 430, val loss: 0.8190956115722656
Epoch 440, training loss: 6.871774196624756 = 0.5248337984085083 + 1.0 * 6.346940517425537
Epoch 440, val loss: 0.8076578974723816
Epoch 450, training loss: 6.840138912200928 = 0.5020809769630432 + 1.0 * 6.338057994842529
Epoch 450, val loss: 0.7978382110595703
Epoch 460, training loss: 6.815304756164551 = 0.4802398383617401 + 1.0 * 6.335064888000488
Epoch 460, val loss: 0.7893213629722595
Epoch 470, training loss: 6.791479110717773 = 0.4591294825077057 + 1.0 * 6.33234977722168
Epoch 470, val loss: 0.7818878293037415
Epoch 480, training loss: 6.792968273162842 = 0.43879225850105286 + 1.0 * 6.354176044464111
Epoch 480, val loss: 0.7754711508750916
Epoch 490, training loss: 6.753717422485352 = 0.41932493448257446 + 1.0 * 6.334392547607422
Epoch 490, val loss: 0.7702627778053284
Epoch 500, training loss: 6.727836608886719 = 0.4005400538444519 + 1.0 * 6.327296733856201
Epoch 500, val loss: 0.7659627199172974
Epoch 510, training loss: 6.710198879241943 = 0.38228023052215576 + 1.0 * 6.327918529510498
Epoch 510, val loss: 0.762334406375885
Epoch 520, training loss: 6.691699028015137 = 0.36457377672195435 + 1.0 * 6.327125072479248
Epoch 520, val loss: 0.7595295310020447
Epoch 530, training loss: 6.669453144073486 = 0.34742680191993713 + 1.0 * 6.322026252746582
Epoch 530, val loss: 0.7573902606964111
Epoch 540, training loss: 6.656111240386963 = 0.3306845426559448 + 1.0 * 6.3254265785217285
Epoch 540, val loss: 0.7560011744499207
Epoch 550, training loss: 6.636784076690674 = 0.3144912123680115 + 1.0 * 6.322292804718018
Epoch 550, val loss: 0.7550811171531677
Epoch 560, training loss: 6.624105453491211 = 0.29879769682884216 + 1.0 * 6.325307846069336
Epoch 560, val loss: 0.7548808455467224
Epoch 570, training loss: 6.600874423980713 = 0.28371065855026245 + 1.0 * 6.317163944244385
Epoch 570, val loss: 0.7551977634429932
Epoch 580, training loss: 6.583776473999023 = 0.26908788084983826 + 1.0 * 6.314688682556152
Epoch 580, val loss: 0.756052553653717
Epoch 590, training loss: 6.578122615814209 = 0.2550235390663147 + 1.0 * 6.323099136352539
Epoch 590, val loss: 0.7573599815368652
Epoch 600, training loss: 6.55634880065918 = 0.24151864647865295 + 1.0 * 6.314830303192139
Epoch 600, val loss: 0.7592968940734863
Epoch 610, training loss: 6.540304660797119 = 0.2286711037158966 + 1.0 * 6.311633586883545
Epoch 610, val loss: 0.7616682052612305
Epoch 620, training loss: 6.527744770050049 = 0.2164234071969986 + 1.0 * 6.311321258544922
Epoch 620, val loss: 0.7644967436790466
Epoch 630, training loss: 6.517307758331299 = 0.20483283698558807 + 1.0 * 6.312474727630615
Epoch 630, val loss: 0.7677889466285706
Epoch 640, training loss: 6.501486301422119 = 0.193932443857193 + 1.0 * 6.307553768157959
Epoch 640, val loss: 0.7714954614639282
Epoch 650, training loss: 6.491568088531494 = 0.18360285460948944 + 1.0 * 6.307965278625488
Epoch 650, val loss: 0.7756673693656921
Epoch 660, training loss: 6.482760429382324 = 0.17390452325344086 + 1.0 * 6.308856010437012
Epoch 660, val loss: 0.7800447940826416
Epoch 670, training loss: 6.471864223480225 = 0.16482050716876984 + 1.0 * 6.307043552398682
Epoch 670, val loss: 0.7848491668701172
Epoch 680, training loss: 6.457958698272705 = 0.15628406405448914 + 1.0 * 6.301674842834473
Epoch 680, val loss: 0.7899832725524902
Epoch 690, training loss: 6.458893775939941 = 0.1482430100440979 + 1.0 * 6.310650825500488
Epoch 690, val loss: 0.7953531742095947
Epoch 700, training loss: 6.445432662963867 = 0.14068038761615753 + 1.0 * 6.304752349853516
Epoch 700, val loss: 0.8009142279624939
Epoch 710, training loss: 6.436583518981934 = 0.13360972702503204 + 1.0 * 6.302973747253418
Epoch 710, val loss: 0.806698739528656
Epoch 720, training loss: 6.427177906036377 = 0.12694673240184784 + 1.0 * 6.300230979919434
Epoch 720, val loss: 0.8125758767127991
Epoch 730, training loss: 6.417543888092041 = 0.12067113071680069 + 1.0 * 6.296872615814209
Epoch 730, val loss: 0.8186444044113159
Epoch 740, training loss: 6.424344062805176 = 0.11475211381912231 + 1.0 * 6.309591770172119
Epoch 740, val loss: 0.824836254119873
Epoch 750, training loss: 6.405752658843994 = 0.10919259488582611 + 1.0 * 6.296560287475586
Epoch 750, val loss: 0.8310783505439758
Epoch 760, training loss: 6.397989749908447 = 0.103969506919384 + 1.0 * 6.294020175933838
Epoch 760, val loss: 0.8374149203300476
Epoch 770, training loss: 6.398486137390137 = 0.09903206676244736 + 1.0 * 6.299454212188721
Epoch 770, val loss: 0.843864381313324
Epoch 780, training loss: 6.390329837799072 = 0.0943867415189743 + 1.0 * 6.295943260192871
Epoch 780, val loss: 0.850229024887085
Epoch 790, training loss: 6.382284641265869 = 0.09000936895608902 + 1.0 * 6.292275428771973
Epoch 790, val loss: 0.8567196130752563
Epoch 800, training loss: 6.3802056312561035 = 0.08587959408760071 + 1.0 * 6.294325828552246
Epoch 800, val loss: 0.863277018070221
Epoch 810, training loss: 6.374996662139893 = 0.08198487013578415 + 1.0 * 6.293011665344238
Epoch 810, val loss: 0.8697384595870972
Epoch 820, training loss: 6.368748664855957 = 0.07833171635866165 + 1.0 * 6.290416717529297
Epoch 820, val loss: 0.8761143088340759
Epoch 830, training loss: 6.363063812255859 = 0.07487253099679947 + 1.0 * 6.288191318511963
Epoch 830, val loss: 0.8826216459274292
Epoch 840, training loss: 6.370204448699951 = 0.07159265875816345 + 1.0 * 6.298611640930176
Epoch 840, val loss: 0.8890860080718994
Epoch 850, training loss: 6.361688613891602 = 0.06853488087654114 + 1.0 * 6.293153762817383
Epoch 850, val loss: 0.8953998684883118
Epoch 860, training loss: 6.351321220397949 = 0.0656258687376976 + 1.0 * 6.285695552825928
Epoch 860, val loss: 0.9017689824104309
Epoch 870, training loss: 6.348227024078369 = 0.06287828087806702 + 1.0 * 6.285348892211914
Epoch 870, val loss: 0.9081488847732544
Epoch 880, training loss: 6.348521709442139 = 0.06028179079294205 + 1.0 * 6.2882399559021
Epoch 880, val loss: 0.914431095123291
Epoch 890, training loss: 6.342234134674072 = 0.057828936725854874 + 1.0 * 6.28440523147583
Epoch 890, val loss: 0.9206188917160034
Epoch 900, training loss: 6.338541030883789 = 0.05550779029726982 + 1.0 * 6.28303337097168
Epoch 900, val loss: 0.9268548488616943
Epoch 910, training loss: 6.34512186050415 = 0.05330237001180649 + 1.0 * 6.2918195724487305
Epoch 910, val loss: 0.9330297112464905
Epoch 920, training loss: 6.3390583992004395 = 0.05122670903801918 + 1.0 * 6.287831783294678
Epoch 920, val loss: 0.9390753507614136
Epoch 930, training loss: 6.329854488372803 = 0.04926103726029396 + 1.0 * 6.280593395233154
Epoch 930, val loss: 0.9450870752334595
Epoch 940, training loss: 6.329578876495361 = 0.047395505011081696 + 1.0 * 6.2821831703186035
Epoch 940, val loss: 0.951097846031189
Epoch 950, training loss: 6.329346656799316 = 0.04562358558177948 + 1.0 * 6.283722877502441
Epoch 950, val loss: 0.957030713558197
Epoch 960, training loss: 6.323099136352539 = 0.04394831135869026 + 1.0 * 6.27915096282959
Epoch 960, val loss: 0.9628044962882996
Epoch 970, training loss: 6.320812225341797 = 0.042356304824352264 + 1.0 * 6.27845573425293
Epoch 970, val loss: 0.9686869382858276
Epoch 980, training loss: 6.319118976593018 = 0.0408363975584507 + 1.0 * 6.278282642364502
Epoch 980, val loss: 0.9745482206344604
Epoch 990, training loss: 6.321292877197266 = 0.03939095884561539 + 1.0 * 6.281901836395264
Epoch 990, val loss: 0.980263352394104
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 10.53299617767334 = 1.9361650943756104 + 1.0 * 8.596831321716309
Epoch 0, val loss: 1.928681492805481
Epoch 10, training loss: 10.522290229797363 = 1.925744891166687 + 1.0 * 8.596545219421387
Epoch 10, val loss: 1.9180570840835571
Epoch 20, training loss: 10.506818771362305 = 1.912804126739502 + 1.0 * 8.594015121459961
Epoch 20, val loss: 1.9048055410385132
Epoch 30, training loss: 10.468565940856934 = 1.8949710130691528 + 1.0 * 8.57359504699707
Epoch 30, val loss: 1.8867548704147339
Epoch 40, training loss: 10.328388214111328 = 1.871342658996582 + 1.0 * 8.457045555114746
Epoch 40, val loss: 1.8636976480484009
Epoch 50, training loss: 9.824479103088379 = 1.8438259363174438 + 1.0 * 7.980653285980225
Epoch 50, val loss: 1.8374981880187988
Epoch 60, training loss: 9.506553649902344 = 1.8187263011932373 + 1.0 * 7.687827110290527
Epoch 60, val loss: 1.8146147727966309
Epoch 70, training loss: 9.099953651428223 = 1.8006685972213745 + 1.0 * 7.299285411834717
Epoch 70, val loss: 1.7985663414001465
Epoch 80, training loss: 8.84533405303955 = 1.7879937887191772 + 1.0 * 7.057340145111084
Epoch 80, val loss: 1.7869505882263184
Epoch 90, training loss: 8.679141998291016 = 1.774777889251709 + 1.0 * 6.904363632202148
Epoch 90, val loss: 1.7754504680633545
Epoch 100, training loss: 8.561073303222656 = 1.7582902908325195 + 1.0 * 6.802783489227295
Epoch 100, val loss: 1.7621225118637085
Epoch 110, training loss: 8.467954635620117 = 1.7396303415298462 + 1.0 * 6.7283244132995605
Epoch 110, val loss: 1.747376561164856
Epoch 120, training loss: 8.394769668579102 = 1.7203176021575928 + 1.0 * 6.67445182800293
Epoch 120, val loss: 1.7314155101776123
Epoch 130, training loss: 8.330784797668457 = 1.6983503103256226 + 1.0 * 6.632434368133545
Epoch 130, val loss: 1.7127676010131836
Epoch 140, training loss: 8.269615173339844 = 1.671728253364563 + 1.0 * 6.59788703918457
Epoch 140, val loss: 1.6903164386749268
Epoch 150, training loss: 8.212886810302734 = 1.639733910560608 + 1.0 * 6.573152542114258
Epoch 150, val loss: 1.6637479066848755
Epoch 160, training loss: 8.153587341308594 = 1.6020662784576416 + 1.0 * 6.551521301269531
Epoch 160, val loss: 1.632631540298462
Epoch 170, training loss: 8.091822624206543 = 1.5580617189407349 + 1.0 * 6.533761024475098
Epoch 170, val loss: 1.5962414741516113
Epoch 180, training loss: 8.027091979980469 = 1.5076110363006592 + 1.0 * 6.5194807052612305
Epoch 180, val loss: 1.554769515991211
Epoch 190, training loss: 7.961913108825684 = 1.4541513919830322 + 1.0 * 6.5077619552612305
Epoch 190, val loss: 1.5114914178848267
Epoch 200, training loss: 7.895605087280273 = 1.3995689153671265 + 1.0 * 6.496036052703857
Epoch 200, val loss: 1.467791199684143
Epoch 210, training loss: 7.829977035522461 = 1.3446917533874512 + 1.0 * 6.48528528213501
Epoch 210, val loss: 1.4245857000350952
Epoch 220, training loss: 7.7663984298706055 = 1.2905614376068115 + 1.0 * 6.475836753845215
Epoch 220, val loss: 1.3827226161956787
Epoch 230, training loss: 7.705936908721924 = 1.2394903898239136 + 1.0 * 6.466446399688721
Epoch 230, val loss: 1.3444932699203491
Epoch 240, training loss: 7.650094032287598 = 1.1916794776916504 + 1.0 * 6.458414554595947
Epoch 240, val loss: 1.3092808723449707
Epoch 250, training loss: 7.595587730407715 = 1.1454545259475708 + 1.0 * 6.450133323669434
Epoch 250, val loss: 1.2756142616271973
Epoch 260, training loss: 7.545729637145996 = 1.1001309156417847 + 1.0 * 6.445598602294922
Epoch 260, val loss: 1.2429864406585693
Epoch 270, training loss: 7.489670753479004 = 1.055598258972168 + 1.0 * 6.434072494506836
Epoch 270, val loss: 1.2110028266906738
Epoch 280, training loss: 7.441440105438232 = 1.0110851526260376 + 1.0 * 6.430355072021484
Epoch 280, val loss: 1.179042100906372
Epoch 290, training loss: 7.388350963592529 = 0.967133104801178 + 1.0 * 6.421217918395996
Epoch 290, val loss: 1.1473103761672974
Epoch 300, training loss: 7.337961196899414 = 0.9234971404075623 + 1.0 * 6.414463996887207
Epoch 300, val loss: 1.115959644317627
Epoch 310, training loss: 7.303225517272949 = 0.8801781535148621 + 1.0 * 6.4230475425720215
Epoch 310, val loss: 1.0848184823989868
Epoch 320, training loss: 7.247713088989258 = 0.8388454914093018 + 1.0 * 6.408867835998535
Epoch 320, val loss: 1.0549145936965942
Epoch 330, training loss: 7.199112892150879 = 0.7991958260536194 + 1.0 * 6.399917125701904
Epoch 330, val loss: 1.026227593421936
Epoch 340, training loss: 7.155257225036621 = 0.7609719038009644 + 1.0 * 6.394285202026367
Epoch 340, val loss: 0.998672604560852
Epoch 350, training loss: 7.113603115081787 = 0.7243499755859375 + 1.0 * 6.38925313949585
Epoch 350, val loss: 0.9724743962287903
Epoch 360, training loss: 7.074434757232666 = 0.6894364953041077 + 1.0 * 6.384998321533203
Epoch 360, val loss: 0.9476526379585266
Epoch 370, training loss: 7.04829216003418 = 0.6566849946975708 + 1.0 * 6.391607284545898
Epoch 370, val loss: 0.9246155619621277
Epoch 380, training loss: 7.004863739013672 = 0.6264339089393616 + 1.0 * 6.378429889678955
Epoch 380, val loss: 0.9036824703216553
Epoch 390, training loss: 6.971347332000732 = 0.5979240536689758 + 1.0 * 6.373423099517822
Epoch 390, val loss: 0.884326696395874
Epoch 400, training loss: 6.958301067352295 = 0.5709191560745239 + 1.0 * 6.3873820304870605
Epoch 400, val loss: 0.8666646480560303
Epoch 410, training loss: 6.91448450088501 = 0.5457770228385925 + 1.0 * 6.368707656860352
Epoch 410, val loss: 0.8508149981498718
Epoch 420, training loss: 6.887770652770996 = 0.5219826698303223 + 1.0 * 6.365787982940674
Epoch 420, val loss: 0.8364064693450928
Epoch 430, training loss: 6.877498626708984 = 0.499283105134964 + 1.0 * 6.378215312957764
Epoch 430, val loss: 0.823471188545227
Epoch 440, training loss: 6.840083599090576 = 0.4779585897922516 + 1.0 * 6.362124919891357
Epoch 440, val loss: 0.8119288682937622
Epoch 450, training loss: 6.814326286315918 = 0.45753905177116394 + 1.0 * 6.356787204742432
Epoch 450, val loss: 0.8014069199562073
Epoch 460, training loss: 6.792341709136963 = 0.43780168890953064 + 1.0 * 6.35453987121582
Epoch 460, val loss: 0.7918781042098999
Epoch 470, training loss: 6.770569801330566 = 0.418593168258667 + 1.0 * 6.3519768714904785
Epoch 470, val loss: 0.7832143306732178
Epoch 480, training loss: 6.759167194366455 = 0.39987480640411377 + 1.0 * 6.359292507171631
Epoch 480, val loss: 0.7751893997192383
Epoch 490, training loss: 6.733606338500977 = 0.3817722797393799 + 1.0 * 6.351834297180176
Epoch 490, val loss: 0.7680484652519226
Epoch 500, training loss: 6.711480140686035 = 0.36416229605674744 + 1.0 * 6.347317695617676
Epoch 500, val loss: 0.7613547444343567
Epoch 510, training loss: 6.691549777984619 = 0.34685084223747253 + 1.0 * 6.344698905944824
Epoch 510, val loss: 0.7551738619804382
Epoch 520, training loss: 6.67661190032959 = 0.3298732340335846 + 1.0 * 6.346738815307617
Epoch 520, val loss: 0.7495599389076233
Epoch 530, training loss: 6.655719757080078 = 0.31339073181152344 + 1.0 * 6.342329025268555
Epoch 530, val loss: 0.7443512082099915
Epoch 540, training loss: 6.636503219604492 = 0.2973281145095825 + 1.0 * 6.339175224304199
Epoch 540, val loss: 0.7397273182868958
Epoch 550, training loss: 6.622382640838623 = 0.28171107172966003 + 1.0 * 6.340671539306641
Epoch 550, val loss: 0.7357630133628845
Epoch 560, training loss: 6.6062164306640625 = 0.2667486369609833 + 1.0 * 6.339468002319336
Epoch 560, val loss: 0.7323454022407532
Epoch 570, training loss: 6.58743143081665 = 0.2524756193161011 + 1.0 * 6.33495569229126
Epoch 570, val loss: 0.7294440865516663
Epoch 580, training loss: 6.57597017288208 = 0.23887157440185547 + 1.0 * 6.337098598480225
Epoch 580, val loss: 0.7272302508354187
Epoch 590, training loss: 6.5573954582214355 = 0.22597596049308777 + 1.0 * 6.331419467926025
Epoch 590, val loss: 0.7257236242294312
Epoch 600, training loss: 6.5451741218566895 = 0.21378491818904877 + 1.0 * 6.331389427185059
Epoch 600, val loss: 0.7247414588928223
Epoch 610, training loss: 6.530554294586182 = 0.20227232575416565 + 1.0 * 6.328281879425049
Epoch 610, val loss: 0.7244524359703064
Epoch 620, training loss: 6.534967422485352 = 0.19140060245990753 + 1.0 * 6.34356689453125
Epoch 620, val loss: 0.7248166799545288
Epoch 630, training loss: 6.510439872741699 = 0.18127353489398956 + 1.0 * 6.329166412353516
Epoch 630, val loss: 0.7255854606628418
Epoch 640, training loss: 6.4962029457092285 = 0.17175884544849396 + 1.0 * 6.32444429397583
Epoch 640, val loss: 0.7268462181091309
Epoch 650, training loss: 6.485854148864746 = 0.1627916395664215 + 1.0 * 6.323062419891357
Epoch 650, val loss: 0.7286691069602966
Epoch 660, training loss: 6.482116222381592 = 0.15434613823890686 + 1.0 * 6.327770233154297
Epoch 660, val loss: 0.7309494018554688
Epoch 670, training loss: 6.469790935516357 = 0.14644566178321838 + 1.0 * 6.323345184326172
Epoch 670, val loss: 0.733588457107544
Epoch 680, training loss: 6.460659980773926 = 0.13898152112960815 + 1.0 * 6.321678638458252
Epoch 680, val loss: 0.736606240272522
Epoch 690, training loss: 6.451916694641113 = 0.1319735050201416 + 1.0 * 6.319942951202393
Epoch 690, val loss: 0.740018367767334
Epoch 700, training loss: 6.448003768920898 = 0.12536199390888214 + 1.0 * 6.322641849517822
Epoch 700, val loss: 0.7436519265174866
Epoch 710, training loss: 6.436434268951416 = 0.11917559802532196 + 1.0 * 6.317258834838867
Epoch 710, val loss: 0.7475123405456543
Epoch 720, training loss: 6.4304423332214355 = 0.1133364737033844 + 1.0 * 6.317105770111084
Epoch 720, val loss: 0.7516489624977112
Epoch 730, training loss: 6.422740459442139 = 0.1078399196267128 + 1.0 * 6.3149003982543945
Epoch 730, val loss: 0.7559996247291565
Epoch 740, training loss: 6.414584636688232 = 0.10266965627670288 + 1.0 * 6.311914920806885
Epoch 740, val loss: 0.7604842782020569
Epoch 750, training loss: 6.410062313079834 = 0.09777367115020752 + 1.0 * 6.312288761138916
Epoch 750, val loss: 0.7652374505996704
Epoch 760, training loss: 6.408953666687012 = 0.09315641224384308 + 1.0 * 6.315797328948975
Epoch 760, val loss: 0.7701476812362671
Epoch 770, training loss: 6.4020466804504395 = 0.08880089223384857 + 1.0 * 6.31324577331543
Epoch 770, val loss: 0.775128960609436
Epoch 780, training loss: 6.3928351402282715 = 0.08469993621110916 + 1.0 * 6.308135032653809
Epoch 780, val loss: 0.780264675617218
Epoch 790, training loss: 6.387888431549072 = 0.08082954585552216 + 1.0 * 6.307058811187744
Epoch 790, val loss: 0.7854620218276978
Epoch 800, training loss: 6.386984348297119 = 0.07716487348079681 + 1.0 * 6.30981969833374
Epoch 800, val loss: 0.7907842993736267
Epoch 810, training loss: 6.378325939178467 = 0.0737115889787674 + 1.0 * 6.304614543914795
Epoch 810, val loss: 0.7961950302124023
Epoch 820, training loss: 6.374066352844238 = 0.07044507563114166 + 1.0 * 6.303621292114258
Epoch 820, val loss: 0.8016207218170166
Epoch 830, training loss: 6.373063087463379 = 0.06734709441661835 + 1.0 * 6.305716037750244
Epoch 830, val loss: 0.8071956634521484
Epoch 840, training loss: 6.367669105529785 = 0.06442674994468689 + 1.0 * 6.303242206573486
Epoch 840, val loss: 0.8127833008766174
Epoch 850, training loss: 6.364635944366455 = 0.061675071716308594 + 1.0 * 6.3029608726501465
Epoch 850, val loss: 0.818274974822998
Epoch 860, training loss: 6.358864784240723 = 0.059068463742733 + 1.0 * 6.299796104431152
Epoch 860, val loss: 0.8238411545753479
Epoch 870, training loss: 6.356058597564697 = 0.056591492146253586 + 1.0 * 6.299467086791992
Epoch 870, val loss: 0.8295198678970337
Epoch 880, training loss: 6.36440372467041 = 0.05424226447939873 + 1.0 * 6.310161590576172
Epoch 880, val loss: 0.8352187275886536
Epoch 890, training loss: 6.351297855377197 = 0.052040331065654755 + 1.0 * 6.299257755279541
Epoch 890, val loss: 0.8407929539680481
Epoch 900, training loss: 6.346353530883789 = 0.049955181777477264 + 1.0 * 6.296398162841797
Epoch 900, val loss: 0.8463322520256042
Epoch 910, training loss: 6.345074653625488 = 0.04797179996967316 + 1.0 * 6.297102928161621
Epoch 910, val loss: 0.8519826531410217
Epoch 920, training loss: 6.344555854797363 = 0.04609175771474838 + 1.0 * 6.298464298248291
Epoch 920, val loss: 0.8576309084892273
Epoch 930, training loss: 6.344605445861816 = 0.04431498050689697 + 1.0 * 6.300290584564209
Epoch 930, val loss: 0.8631473183631897
Epoch 940, training loss: 6.3381571769714355 = 0.04262879863381386 + 1.0 * 6.295528411865234
Epoch 940, val loss: 0.868657648563385
Epoch 950, training loss: 6.334778308868408 = 0.04103126749396324 + 1.0 * 6.2937469482421875
Epoch 950, val loss: 0.874127984046936
Epoch 960, training loss: 6.331955909729004 = 0.03951661288738251 + 1.0 * 6.2924394607543945
Epoch 960, val loss: 0.8796004056930542
Epoch 970, training loss: 6.332576751708984 = 0.0380718931555748 + 1.0 * 6.294504642486572
Epoch 970, val loss: 0.8851262331008911
Epoch 980, training loss: 6.337776184082031 = 0.03669873625040054 + 1.0 * 6.301077365875244
Epoch 980, val loss: 0.8905715346336365
Epoch 990, training loss: 6.327928066253662 = 0.035409148782491684 + 1.0 * 6.2925190925598145
Epoch 990, val loss: 0.895861029624939
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8365840801265156
The final CL Acc:0.80247, 0.00630, The final GNN Acc:0.83764, 0.00149
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9576])
updated graph: torch.Size([2, 10616])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.542098045349121 = 1.945231318473816 + 1.0 * 8.596866607666016
Epoch 0, val loss: 1.952261209487915
Epoch 10, training loss: 10.532421112060547 = 1.9357237815856934 + 1.0 * 8.596696853637695
Epoch 10, val loss: 1.9424363374710083
Epoch 20, training loss: 10.519024848937988 = 1.9238107204437256 + 1.0 * 8.595213890075684
Epoch 20, val loss: 1.9299190044403076
Epoch 30, training loss: 10.489062309265137 = 1.9068752527236938 + 1.0 * 8.582186698913574
Epoch 30, val loss: 1.9122954607009888
Epoch 40, training loss: 10.385210037231445 = 1.8836127519607544 + 1.0 * 8.50159740447998
Epoch 40, val loss: 1.8888845443725586
Epoch 50, training loss: 9.98885726928711 = 1.8575949668884277 + 1.0 * 8.131261825561523
Epoch 50, val loss: 1.8637574911117554
Epoch 60, training loss: 9.663961410522461 = 1.8333086967468262 + 1.0 * 7.830652236938477
Epoch 60, val loss: 1.8413358926773071
Epoch 70, training loss: 9.177688598632812 = 1.815018653869629 + 1.0 * 7.362669467926025
Epoch 70, val loss: 1.8246467113494873
Epoch 80, training loss: 8.906434059143066 = 1.7986083030700684 + 1.0 * 7.107825756072998
Epoch 80, val loss: 1.8093992471694946
Epoch 90, training loss: 8.780282974243164 = 1.7813926935195923 + 1.0 * 6.998890399932861
Epoch 90, val loss: 1.7935348749160767
Epoch 100, training loss: 8.648134231567383 = 1.7641762495040894 + 1.0 * 6.883958339691162
Epoch 100, val loss: 1.7782803773880005
Epoch 110, training loss: 8.551527976989746 = 1.7466849088668823 + 1.0 * 6.804842948913574
Epoch 110, val loss: 1.762716293334961
Epoch 120, training loss: 8.467939376831055 = 1.7281250953674316 + 1.0 * 6.739814281463623
Epoch 120, val loss: 1.7461858987808228
Epoch 130, training loss: 8.40671157836914 = 1.7077767848968506 + 1.0 * 6.698935031890869
Epoch 130, val loss: 1.7282987833023071
Epoch 140, training loss: 8.343130111694336 = 1.6849889755249023 + 1.0 * 6.658141136169434
Epoch 140, val loss: 1.708601474761963
Epoch 150, training loss: 8.283734321594238 = 1.659051537513733 + 1.0 * 6.624682426452637
Epoch 150, val loss: 1.6866875886917114
Epoch 160, training loss: 8.223006248474121 = 1.6296286582946777 + 1.0 * 6.593377590179443
Epoch 160, val loss: 1.661808729171753
Epoch 170, training loss: 8.170024871826172 = 1.5962340831756592 + 1.0 * 6.573791027069092
Epoch 170, val loss: 1.634078025817871
Epoch 180, training loss: 8.103848457336426 = 1.5591312646865845 + 1.0 * 6.544716835021973
Epoch 180, val loss: 1.6037408113479614
Epoch 190, training loss: 8.043939590454102 = 1.5179667472839355 + 1.0 * 6.525973320007324
Epoch 190, val loss: 1.5702905654907227
Epoch 200, training loss: 7.982616424560547 = 1.4728025197982788 + 1.0 * 6.5098137855529785
Epoch 200, val loss: 1.533703088760376
Epoch 210, training loss: 7.92264461517334 = 1.424077033996582 + 1.0 * 6.498567581176758
Epoch 210, val loss: 1.4945496320724487
Epoch 220, training loss: 7.862952709197998 = 1.3729122877120972 + 1.0 * 6.490040302276611
Epoch 220, val loss: 1.453891396522522
Epoch 230, training loss: 7.798415184020996 = 1.3209989070892334 + 1.0 * 6.477416515350342
Epoch 230, val loss: 1.4130384922027588
Epoch 240, training loss: 7.7368268966674805 = 1.2690002918243408 + 1.0 * 6.467826843261719
Epoch 240, val loss: 1.3724470138549805
Epoch 250, training loss: 7.6766357421875 = 1.217390537261963 + 1.0 * 6.459245204925537
Epoch 250, val loss: 1.3326513767242432
Epoch 260, training loss: 7.628459453582764 = 1.1668291091918945 + 1.0 * 6.461630344390869
Epoch 260, val loss: 1.2941035032272339
Epoch 270, training loss: 7.564495086669922 = 1.1188809871673584 + 1.0 * 6.445613861083984
Epoch 270, val loss: 1.2579971551895142
Epoch 280, training loss: 7.511701583862305 = 1.0731120109558105 + 1.0 * 6.438589572906494
Epoch 280, val loss: 1.2237434387207031
Epoch 290, training loss: 7.461465835571289 = 1.0293463468551636 + 1.0 * 6.432119369506836
Epoch 290, val loss: 1.1914269924163818
Epoch 300, training loss: 7.422317981719971 = 0.9874840974807739 + 1.0 * 6.434834003448486
Epoch 300, val loss: 1.1607087850570679
Epoch 310, training loss: 7.370713233947754 = 0.947892427444458 + 1.0 * 6.422821044921875
Epoch 310, val loss: 1.131925106048584
Epoch 320, training loss: 7.327567100524902 = 0.9099496603012085 + 1.0 * 6.417617321014404
Epoch 320, val loss: 1.1044541597366333
Epoch 330, training loss: 7.290963649749756 = 0.8732979893684387 + 1.0 * 6.417665481567383
Epoch 330, val loss: 1.0780127048492432
Epoch 340, training loss: 7.247182846069336 = 0.8381748795509338 + 1.0 * 6.409008026123047
Epoch 340, val loss: 1.052765130996704
Epoch 350, training loss: 7.2077531814575195 = 0.8041620254516602 + 1.0 * 6.403591156005859
Epoch 350, val loss: 1.0287374258041382
Epoch 360, training loss: 7.171450614929199 = 0.7712934017181396 + 1.0 * 6.4001569747924805
Epoch 360, val loss: 1.0056781768798828
Epoch 370, training loss: 7.137872695922852 = 0.7398584485054016 + 1.0 * 6.398014068603516
Epoch 370, val loss: 0.9840410947799683
Epoch 380, training loss: 7.10272216796875 = 0.7099807262420654 + 1.0 * 6.3927412033081055
Epoch 380, val loss: 0.9643107056617737
Epoch 390, training loss: 7.069698810577393 = 0.6813359260559082 + 1.0 * 6.388362884521484
Epoch 390, val loss: 0.9460124969482422
Epoch 400, training loss: 7.0379204750061035 = 0.6536828279495239 + 1.0 * 6.384237766265869
Epoch 400, val loss: 0.9290871024131775
Epoch 410, training loss: 7.030023574829102 = 0.6269486546516418 + 1.0 * 6.403074741363525
Epoch 410, val loss: 0.9136473536491394
Epoch 420, training loss: 6.982011795043945 = 0.6015985012054443 + 1.0 * 6.38041353225708
Epoch 420, val loss: 0.9000082015991211
Epoch 430, training loss: 6.954410076141357 = 0.5773085355758667 + 1.0 * 6.377101421356201
Epoch 430, val loss: 0.8882735967636108
Epoch 440, training loss: 6.927531719207764 = 0.5538080930709839 + 1.0 * 6.37372350692749
Epoch 440, val loss: 0.8778172731399536
Epoch 450, training loss: 6.908167362213135 = 0.5310322642326355 + 1.0 * 6.377135276794434
Epoch 450, val loss: 0.8685917258262634
Epoch 460, training loss: 6.882264137268066 = 0.5091422200202942 + 1.0 * 6.373121738433838
Epoch 460, val loss: 0.8608739376068115
Epoch 470, training loss: 6.85407018661499 = 0.4881954491138458 + 1.0 * 6.365874767303467
Epoch 470, val loss: 0.8545783758163452
Epoch 480, training loss: 6.830622673034668 = 0.46798041462898254 + 1.0 * 6.362642288208008
Epoch 480, val loss: 0.8493950366973877
Epoch 490, training loss: 6.822593688964844 = 0.4485732913017273 + 1.0 * 6.374020576477051
Epoch 490, val loss: 0.8452489972114563
Epoch 500, training loss: 6.792750835418701 = 0.43010076880455017 + 1.0 * 6.362649917602539
Epoch 500, val loss: 0.8423565030097961
Epoch 510, training loss: 6.769728183746338 = 0.4124563932418823 + 1.0 * 6.357271671295166
Epoch 510, val loss: 0.8405440449714661
Epoch 520, training loss: 6.751598834991455 = 0.3954997658729553 + 1.0 * 6.3560991287231445
Epoch 520, val loss: 0.8395124077796936
Epoch 530, training loss: 6.737193584442139 = 0.3793102502822876 + 1.0 * 6.357883453369141
Epoch 530, val loss: 0.8392660021781921
Epoch 540, training loss: 6.716083526611328 = 0.3639819920063019 + 1.0 * 6.3521013259887695
Epoch 540, val loss: 0.8399429321289062
Epoch 550, training loss: 6.698049545288086 = 0.3493291735649109 + 1.0 * 6.348720550537109
Epoch 550, val loss: 0.8412627577781677
Epoch 560, training loss: 6.681500434875488 = 0.33528363704681396 + 1.0 * 6.346216678619385
Epoch 560, val loss: 0.8431854844093323
Epoch 570, training loss: 6.675759315490723 = 0.32182836532592773 + 1.0 * 6.353930950164795
Epoch 570, val loss: 0.8456950783729553
Epoch 580, training loss: 6.656145095825195 = 0.309051513671875 + 1.0 * 6.34709358215332
Epoch 580, val loss: 0.8487188816070557
Epoch 590, training loss: 6.638568878173828 = 0.29692068696022034 + 1.0 * 6.341648101806641
Epoch 590, val loss: 0.852342963218689
Epoch 600, training loss: 6.6263203620910645 = 0.28532278537750244 + 1.0 * 6.340997695922852
Epoch 600, val loss: 0.8563595414161682
Epoch 610, training loss: 6.614686489105225 = 0.2742372155189514 + 1.0 * 6.340449333190918
Epoch 610, val loss: 0.8606687784194946
Epoch 620, training loss: 6.603233814239502 = 0.2636963427066803 + 1.0 * 6.339537620544434
Epoch 620, val loss: 0.8654400706291199
Epoch 630, training loss: 6.589466571807861 = 0.2535760998725891 + 1.0 * 6.335890293121338
Epoch 630, val loss: 0.8705302476882935
Epoch 640, training loss: 6.577356338500977 = 0.24381722509860992 + 1.0 * 6.333539009094238
Epoch 640, val loss: 0.8758367300033569
Epoch 650, training loss: 6.57642936706543 = 0.23438963294029236 + 1.0 * 6.342039585113525
Epoch 650, val loss: 0.8813697695732117
Epoch 660, training loss: 6.557578086853027 = 0.22528624534606934 + 1.0 * 6.332292079925537
Epoch 660, val loss: 0.8871496915817261
Epoch 670, training loss: 6.546950340270996 = 0.21643556654453278 + 1.0 * 6.330514907836914
Epoch 670, val loss: 0.8931017518043518
Epoch 680, training loss: 6.541136741638184 = 0.20775841176509857 + 1.0 * 6.333378314971924
Epoch 680, val loss: 0.8991307616233826
Epoch 690, training loss: 6.531242370605469 = 0.1992521584033966 + 1.0 * 6.3319902420043945
Epoch 690, val loss: 0.9052186012268066
Epoch 700, training loss: 6.516798496246338 = 0.1908751130104065 + 1.0 * 6.325923442840576
Epoch 700, val loss: 0.9114565253257751
Epoch 710, training loss: 6.510640621185303 = 0.18256352841854095 + 1.0 * 6.32807731628418
Epoch 710, val loss: 0.9176627993583679
Epoch 720, training loss: 6.497725486755371 = 0.17431282997131348 + 1.0 * 6.3234124183654785
Epoch 720, val loss: 0.9237483739852905
Epoch 730, training loss: 6.489419937133789 = 0.16614781320095062 + 1.0 * 6.323272228240967
Epoch 730, val loss: 0.9299452900886536
Epoch 740, training loss: 6.479959011077881 = 0.15807464718818665 + 1.0 * 6.3218841552734375
Epoch 740, val loss: 0.9359816908836365
Epoch 750, training loss: 6.481800079345703 = 0.15012326836585999 + 1.0 * 6.331676959991455
Epoch 750, val loss: 0.9419258832931519
Epoch 760, training loss: 6.463338375091553 = 0.14235840737819672 + 1.0 * 6.320980072021484
Epoch 760, val loss: 0.9476950168609619
Epoch 770, training loss: 6.451529502868652 = 0.13481895625591278 + 1.0 * 6.316710472106934
Epoch 770, val loss: 0.9533831477165222
Epoch 780, training loss: 6.443445682525635 = 0.12750810384750366 + 1.0 * 6.315937519073486
Epoch 780, val loss: 0.9589008688926697
Epoch 790, training loss: 6.442698001861572 = 0.1204727366566658 + 1.0 * 6.322225093841553
Epoch 790, val loss: 0.9643347859382629
Epoch 800, training loss: 6.432491302490234 = 0.11376136541366577 + 1.0 * 6.318729877471924
Epoch 800, val loss: 0.9695428013801575
Epoch 810, training loss: 6.423896312713623 = 0.1074291542172432 + 1.0 * 6.31646728515625
Epoch 810, val loss: 0.9749465584754944
Epoch 820, training loss: 6.413301944732666 = 0.1014140397310257 + 1.0 * 6.311887741088867
Epoch 820, val loss: 0.9802648425102234
Epoch 830, training loss: 6.406836986541748 = 0.0957229882478714 + 1.0 * 6.3111138343811035
Epoch 830, val loss: 0.9855535626411438
Epoch 840, training loss: 6.406233787536621 = 0.09035059809684753 + 1.0 * 6.315883159637451
Epoch 840, val loss: 0.9907940626144409
Epoch 850, training loss: 6.396684646606445 = 0.08532840013504028 + 1.0 * 6.311356067657471
Epoch 850, val loss: 0.9961445927619934
Epoch 860, training loss: 6.390975475311279 = 0.08061958104372025 + 1.0 * 6.3103556632995605
Epoch 860, val loss: 1.0015240907669067
Epoch 870, training loss: 6.383160591125488 = 0.07620406895875931 + 1.0 * 6.3069562911987305
Epoch 870, val loss: 1.0069893598556519
Epoch 880, training loss: 6.38032341003418 = 0.0720505490899086 + 1.0 * 6.308272838592529
Epoch 880, val loss: 1.0124456882476807
Epoch 890, training loss: 6.377377986907959 = 0.06816848367452621 + 1.0 * 6.30920934677124
Epoch 890, val loss: 1.0178461074829102
Epoch 900, training loss: 6.368968486785889 = 0.06454813480377197 + 1.0 * 6.304420471191406
Epoch 900, val loss: 1.023388147354126
Epoch 910, training loss: 6.364605903625488 = 0.06115831062197685 + 1.0 * 6.303447723388672
Epoch 910, val loss: 1.028961181640625
Epoch 920, training loss: 6.365275859832764 = 0.05798334628343582 + 1.0 * 6.307292461395264
Epoch 920, val loss: 1.0345131158828735
Epoch 930, training loss: 6.35916805267334 = 0.05502381920814514 + 1.0 * 6.304144382476807
Epoch 930, val loss: 1.040010929107666
Epoch 940, training loss: 6.354667663574219 = 0.05226951092481613 + 1.0 * 6.302398204803467
Epoch 940, val loss: 1.0456948280334473
Epoch 950, training loss: 6.351283073425293 = 0.04968840256333351 + 1.0 * 6.3015947341918945
Epoch 950, val loss: 1.0513174533843994
Epoch 960, training loss: 6.349079132080078 = 0.047284409403800964 + 1.0 * 6.301794528961182
Epoch 960, val loss: 1.0568567514419556
Epoch 970, training loss: 6.344871997833252 = 0.04504594951868057 + 1.0 * 6.299826145172119
Epoch 970, val loss: 1.062471866607666
Epoch 980, training loss: 6.339840412139893 = 0.04296109825372696 + 1.0 * 6.296879291534424
Epoch 980, val loss: 1.0682154893875122
Epoch 990, training loss: 6.340305328369141 = 0.04100145399570465 + 1.0 * 6.299304008483887
Epoch 990, val loss: 1.073839783668518
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 10.567225456237793 = 1.9703528881072998 + 1.0 * 8.596872329711914
Epoch 0, val loss: 1.9632470607757568
Epoch 10, training loss: 10.555855751037598 = 1.9591723680496216 + 1.0 * 8.596683502197266
Epoch 10, val loss: 1.9527779817581177
Epoch 20, training loss: 10.540033340454102 = 1.9452316761016846 + 1.0 * 8.594801902770996
Epoch 20, val loss: 1.9393585920333862
Epoch 30, training loss: 10.50307559967041 = 1.9255743026733398 + 1.0 * 8.57750129699707
Epoch 30, val loss: 1.9201716184616089
Epoch 40, training loss: 10.375594139099121 = 1.8987938165664673 + 1.0 * 8.476799964904785
Epoch 40, val loss: 1.8947727680206299
Epoch 50, training loss: 9.964096069335938 = 1.868045449256897 + 1.0 * 8.096050262451172
Epoch 50, val loss: 1.866563320159912
Epoch 60, training loss: 9.77725887298584 = 1.8390015363693237 + 1.0 * 7.938257217407227
Epoch 60, val loss: 1.8414548635482788
Epoch 70, training loss: 9.411914825439453 = 1.8162245750427246 + 1.0 * 7.595690727233887
Epoch 70, val loss: 1.8219224214553833
Epoch 80, training loss: 8.988870620727539 = 1.798829436302185 + 1.0 * 7.190041542053223
Epoch 80, val loss: 1.8071656227111816
Epoch 90, training loss: 8.762845039367676 = 1.7840843200683594 + 1.0 * 6.978760719299316
Epoch 90, val loss: 1.7948355674743652
Epoch 100, training loss: 8.653543472290039 = 1.7678110599517822 + 1.0 * 6.885732173919678
Epoch 100, val loss: 1.781774878501892
Epoch 110, training loss: 8.550552368164062 = 1.751179575920105 + 1.0 * 6.799373149871826
Epoch 110, val loss: 1.7680387496948242
Epoch 120, training loss: 8.459847450256348 = 1.734544038772583 + 1.0 * 6.7253031730651855
Epoch 120, val loss: 1.7537586688995361
Epoch 130, training loss: 8.379708290100098 = 1.7169419527053833 + 1.0 * 6.662766456604004
Epoch 130, val loss: 1.7387146949768066
Epoch 140, training loss: 8.318772315979004 = 1.6966310739517212 + 1.0 * 6.622140884399414
Epoch 140, val loss: 1.7218645811080933
Epoch 150, training loss: 8.268776893615723 = 1.6730597019195557 + 1.0 * 6.595717430114746
Epoch 150, val loss: 1.702765703201294
Epoch 160, training loss: 8.2185640335083 = 1.6463541984558105 + 1.0 * 6.57220983505249
Epoch 160, val loss: 1.6810085773468018
Epoch 170, training loss: 8.168305397033691 = 1.6162781715393066 + 1.0 * 6.552027225494385
Epoch 170, val loss: 1.6561551094055176
Epoch 180, training loss: 8.116716384887695 = 1.5826932191848755 + 1.0 * 6.534022808074951
Epoch 180, val loss: 1.6281691789627075
Epoch 190, training loss: 8.064873695373535 = 1.5461000204086304 + 1.0 * 6.518774032592773
Epoch 190, val loss: 1.5982692241668701
Epoch 200, training loss: 8.012190818786621 = 1.5073227882385254 + 1.0 * 6.504868030548096
Epoch 200, val loss: 1.5670287609100342
Epoch 210, training loss: 7.958179950714111 = 1.4666763544082642 + 1.0 * 6.491503715515137
Epoch 210, val loss: 1.5345617532730103
Epoch 220, training loss: 7.906675338745117 = 1.4250808954238892 + 1.0 * 6.481594562530518
Epoch 220, val loss: 1.501861333847046
Epoch 230, training loss: 7.85374116897583 = 1.3839665651321411 + 1.0 * 6.4697747230529785
Epoch 230, val loss: 1.4702116250991821
Epoch 240, training loss: 7.803626537322998 = 1.343212604522705 + 1.0 * 6.460413932800293
Epoch 240, val loss: 1.4397356510162354
Epoch 250, training loss: 7.755343914031982 = 1.3028364181518555 + 1.0 * 6.452507495880127
Epoch 250, val loss: 1.4104247093200684
Epoch 260, training loss: 7.709953308105469 = 1.263051986694336 + 1.0 * 6.446901321411133
Epoch 260, val loss: 1.3824704885482788
Epoch 270, training loss: 7.663553237915039 = 1.2240326404571533 + 1.0 * 6.439520835876465
Epoch 270, val loss: 1.355682134628296
Epoch 280, training loss: 7.618570804595947 = 1.1852890253067017 + 1.0 * 6.433281898498535
Epoch 280, val loss: 1.329783320426941
Epoch 290, training loss: 7.579288005828857 = 1.1466336250305176 + 1.0 * 6.43265438079834
Epoch 290, val loss: 1.3045496940612793
Epoch 300, training loss: 7.533965110778809 = 1.1083714962005615 + 1.0 * 6.425593376159668
Epoch 300, val loss: 1.2798993587493896
Epoch 310, training loss: 7.489501476287842 = 1.0703740119934082 + 1.0 * 6.419127464294434
Epoch 310, val loss: 1.2556973695755005
Epoch 320, training loss: 7.450617790222168 = 1.0325138568878174 + 1.0 * 6.41810417175293
Epoch 320, val loss: 1.2316633462905884
Epoch 330, training loss: 7.406733512878418 = 0.9951636791229248 + 1.0 * 6.411569595336914
Epoch 330, val loss: 1.208337426185608
Epoch 340, training loss: 7.364819526672363 = 0.958373486995697 + 1.0 * 6.4064459800720215
Epoch 340, val loss: 1.1853649616241455
Epoch 350, training loss: 7.327783107757568 = 0.9222284555435181 + 1.0 * 6.40555477142334
Epoch 350, val loss: 1.162894368171692
Epoch 360, training loss: 7.287227630615234 = 0.8872554898262024 + 1.0 * 6.399971961975098
Epoch 360, val loss: 1.1413110494613647
Epoch 370, training loss: 7.255436420440674 = 0.8532791137695312 + 1.0 * 6.402157306671143
Epoch 370, val loss: 1.1205477714538574
Epoch 380, training loss: 7.212007522583008 = 0.8205326795578003 + 1.0 * 6.391474723815918
Epoch 380, val loss: 1.1008447408676147
Epoch 390, training loss: 7.177048683166504 = 0.7887199521064758 + 1.0 * 6.388328552246094
Epoch 390, val loss: 1.0817903280258179
Epoch 400, training loss: 7.146166801452637 = 0.7576242685317993 + 1.0 * 6.388542652130127
Epoch 400, val loss: 1.0632394552230835
Epoch 410, training loss: 7.114311695098877 = 0.727295458316803 + 1.0 * 6.387016296386719
Epoch 410, val loss: 1.0453962087631226
Epoch 420, training loss: 7.078636169433594 = 0.6977149248123169 + 1.0 * 6.380921363830566
Epoch 420, val loss: 1.0279136896133423
Epoch 430, training loss: 7.044987678527832 = 0.6685940027236938 + 1.0 * 6.376393795013428
Epoch 430, val loss: 1.0108699798583984
Epoch 440, training loss: 7.012288570404053 = 0.6396293044090271 + 1.0 * 6.372659206390381
Epoch 440, val loss: 0.9939663410186768
Epoch 450, training loss: 6.998257637023926 = 0.6107326745986938 + 1.0 * 6.3875250816345215
Epoch 450, val loss: 0.9771525263786316
Epoch 460, training loss: 6.955188274383545 = 0.5822007060050964 + 1.0 * 6.372987747192383
Epoch 460, val loss: 0.9604241251945496
Epoch 470, training loss: 6.918570518493652 = 0.5539016127586365 + 1.0 * 6.364668846130371
Epoch 470, val loss: 0.9441344738006592
Epoch 480, training loss: 6.887997150421143 = 0.5256417989730835 + 1.0 * 6.3623552322387695
Epoch 480, val loss: 0.9278329014778137
Epoch 490, training loss: 6.857817649841309 = 0.4974258542060852 + 1.0 * 6.360391616821289
Epoch 490, val loss: 0.9117099642753601
Epoch 500, training loss: 6.8291544914245605 = 0.46951183676719666 + 1.0 * 6.359642505645752
Epoch 500, val loss: 0.8959763050079346
Epoch 510, training loss: 6.801647186279297 = 0.44222909212112427 + 1.0 * 6.359417915344238
Epoch 510, val loss: 0.8810964226722717
Epoch 520, training loss: 6.768985748291016 = 0.41571471095085144 + 1.0 * 6.353271007537842
Epoch 520, val loss: 0.8670554161071777
Epoch 530, training loss: 6.741238594055176 = 0.39011743664741516 + 1.0 * 6.351120948791504
Epoch 530, val loss: 0.8541464805603027
Epoch 540, training loss: 6.714022159576416 = 0.3654298484325409 + 1.0 * 6.348592281341553
Epoch 540, val loss: 0.8423053026199341
Epoch 550, training loss: 6.69001579284668 = 0.34174686670303345 + 1.0 * 6.348268985748291
Epoch 550, val loss: 0.8317153453826904
Epoch 560, training loss: 6.670497417449951 = 0.31929275393486023 + 1.0 * 6.351204872131348
Epoch 560, val loss: 0.8223772048950195
Epoch 570, training loss: 6.642518997192383 = 0.29825568199157715 + 1.0 * 6.344263553619385
Epoch 570, val loss: 0.8146096467971802
Epoch 580, training loss: 6.619692802429199 = 0.2785360515117645 + 1.0 * 6.341156959533691
Epoch 580, val loss: 0.8081158399581909
Epoch 590, training loss: 6.609374523162842 = 0.2600727379322052 + 1.0 * 6.349301815032959
Epoch 590, val loss: 0.8026374578475952
Epoch 600, training loss: 6.58470344543457 = 0.24302704632282257 + 1.0 * 6.341676235198975
Epoch 600, val loss: 0.7984427809715271
Epoch 610, training loss: 6.563713073730469 = 0.2271880954504013 + 1.0 * 6.336524963378906
Epoch 610, val loss: 0.7953671216964722
Epoch 620, training loss: 6.5460896492004395 = 0.21244217455387115 + 1.0 * 6.33364725112915
Epoch 620, val loss: 0.7930591702461243
Epoch 630, training loss: 6.540189743041992 = 0.19869883358478546 + 1.0 * 6.341490745544434
Epoch 630, val loss: 0.7914934754371643
Epoch 640, training loss: 6.517725467681885 = 0.18599149584770203 + 1.0 * 6.3317341804504395
Epoch 640, val loss: 0.7907724380493164
Epoch 650, training loss: 6.504288196563721 = 0.17423781752586365 + 1.0 * 6.330050468444824
Epoch 650, val loss: 0.7908410429954529
Epoch 660, training loss: 6.492968559265137 = 0.16331462562084198 + 1.0 * 6.329653739929199
Epoch 660, val loss: 0.7914087176322937
Epoch 670, training loss: 6.483569145202637 = 0.15318278968334198 + 1.0 * 6.330386161804199
Epoch 670, val loss: 0.792478621006012
Epoch 680, training loss: 6.470951557159424 = 0.14379887282848358 + 1.0 * 6.327152729034424
Epoch 680, val loss: 0.7940399646759033
Epoch 690, training loss: 6.459650993347168 = 0.13508830964565277 + 1.0 * 6.3245625495910645
Epoch 690, val loss: 0.7960942387580872
Epoch 700, training loss: 6.452494144439697 = 0.12699195742607117 + 1.0 * 6.325502395629883
Epoch 700, val loss: 0.7984434366226196
Epoch 710, training loss: 6.443533420562744 = 0.11949590593576431 + 1.0 * 6.324037551879883
Epoch 710, val loss: 0.8011767268180847
Epoch 720, training loss: 6.434382438659668 = 0.11257514357566833 + 1.0 * 6.321807384490967
Epoch 720, val loss: 0.8041626214981079
Epoch 730, training loss: 6.425415992736816 = 0.10617116838693619 + 1.0 * 6.319244861602783
Epoch 730, val loss: 0.8076625466346741
Epoch 740, training loss: 6.417519569396973 = 0.10020937770605087 + 1.0 * 6.317310333251953
Epoch 740, val loss: 0.8112265467643738
Epoch 750, training loss: 6.411411762237549 = 0.09465523809194565 + 1.0 * 6.316756725311279
Epoch 750, val loss: 0.8150216937065125
Epoch 760, training loss: 6.4115095138549805 = 0.08949673175811768 + 1.0 * 6.322012901306152
Epoch 760, val loss: 0.8189178705215454
Epoch 770, training loss: 6.400699138641357 = 0.08473039418458939 + 1.0 * 6.3159685134887695
Epoch 770, val loss: 0.8231510519981384
Epoch 780, training loss: 6.396825313568115 = 0.08030498027801514 + 1.0 * 6.3165202140808105
Epoch 780, val loss: 0.8274514675140381
Epoch 790, training loss: 6.393206596374512 = 0.07619200646877289 + 1.0 * 6.317014694213867
Epoch 790, val loss: 0.8317449688911438
Epoch 800, training loss: 6.383889198303223 = 0.07237961143255234 + 1.0 * 6.311509609222412
Epoch 800, val loss: 0.8363862037658691
Epoch 810, training loss: 6.378536224365234 = 0.06882067769765854 + 1.0 * 6.309715747833252
Epoch 810, val loss: 0.8410043716430664
Epoch 820, training loss: 6.375814914703369 = 0.06549148261547089 + 1.0 * 6.310323238372803
Epoch 820, val loss: 0.8456685543060303
Epoch 830, training loss: 6.375882625579834 = 0.062386248260736465 + 1.0 * 6.3134965896606445
Epoch 830, val loss: 0.8503693342208862
Epoch 840, training loss: 6.368988037109375 = 0.059490494430065155 + 1.0 * 6.309497356414795
Epoch 840, val loss: 0.8550816774368286
Epoch 850, training loss: 6.363405704498291 = 0.056791387498378754 + 1.0 * 6.306614398956299
Epoch 850, val loss: 0.860031247138977
Epoch 860, training loss: 6.368661403656006 = 0.05426260828971863 + 1.0 * 6.314398765563965
Epoch 860, val loss: 0.8647429347038269
Epoch 870, training loss: 6.355709552764893 = 0.0518980398774147 + 1.0 * 6.303811550140381
Epoch 870, val loss: 0.8695585131645203
Epoch 880, training loss: 6.353066921234131 = 0.04968402907252312 + 1.0 * 6.303382873535156
Epoch 880, val loss: 0.8744914531707764
Epoch 890, training loss: 6.350046157836914 = 0.0475931279361248 + 1.0 * 6.30245304107666
Epoch 890, val loss: 0.8792824149131775
Epoch 900, training loss: 6.357512950897217 = 0.04562626779079437 + 1.0 * 6.311886787414551
Epoch 900, val loss: 0.8840547800064087
Epoch 910, training loss: 6.346096515655518 = 0.04377768561244011 + 1.0 * 6.302319049835205
Epoch 910, val loss: 0.888757586479187
Epoch 920, training loss: 6.344330787658691 = 0.04204358160495758 + 1.0 * 6.3022871017456055
Epoch 920, val loss: 0.8935961723327637
Epoch 930, training loss: 6.339317798614502 = 0.04040930047631264 + 1.0 * 6.298908710479736
Epoch 930, val loss: 0.8984230756759644
Epoch 940, training loss: 6.3398284912109375 = 0.03885992616415024 + 1.0 * 6.300968647003174
Epoch 940, val loss: 0.9031413793563843
Epoch 950, training loss: 6.334601402282715 = 0.03739459440112114 + 1.0 * 6.297206878662109
Epoch 950, val loss: 0.9076794385910034
Epoch 960, training loss: 6.3379316329956055 = 0.03601084649562836 + 1.0 * 6.3019208908081055
Epoch 960, val loss: 0.9123913049697876
Epoch 970, training loss: 6.336241245269775 = 0.03470546007156372 + 1.0 * 6.301535606384277
Epoch 970, val loss: 0.9168150424957275
Epoch 980, training loss: 6.330446720123291 = 0.03347248584032059 + 1.0 * 6.296974182128906
Epoch 980, val loss: 0.9213353395462036
Epoch 990, training loss: 6.327353477478027 = 0.03230462223291397 + 1.0 * 6.295048713684082
Epoch 990, val loss: 0.925923764705658
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 10.544870376586914 = 1.9479937553405762 + 1.0 * 8.596877098083496
Epoch 0, val loss: 1.9398443698883057
Epoch 10, training loss: 10.535470962524414 = 1.9387353658676147 + 1.0 * 8.596735954284668
Epoch 10, val loss: 1.9303516149520874
Epoch 20, training loss: 10.522978782653809 = 1.9276782274246216 + 1.0 * 8.595300674438477
Epoch 20, val loss: 1.9185972213745117
Epoch 30, training loss: 10.493499755859375 = 1.9127084016799927 + 1.0 * 8.580791473388672
Epoch 30, val loss: 1.9024572372436523
Epoch 40, training loss: 10.358859062194824 = 1.8927011489868164 + 1.0 * 8.466157913208008
Epoch 40, val loss: 1.8814741373062134
Epoch 50, training loss: 9.897372245788574 = 1.8694002628326416 + 1.0 * 8.027972221374512
Epoch 50, val loss: 1.8581695556640625
Epoch 60, training loss: 9.61446475982666 = 1.849349021911621 + 1.0 * 7.765115737915039
Epoch 60, val loss: 1.8398009538650513
Epoch 70, training loss: 9.165496826171875 = 1.832940936088562 + 1.0 * 7.332556247711182
Epoch 70, val loss: 1.8246586322784424
Epoch 80, training loss: 8.85922622680664 = 1.8194609880447388 + 1.0 * 7.039765357971191
Epoch 80, val loss: 1.8124680519104004
Epoch 90, training loss: 8.687653541564941 = 1.8052996397018433 + 1.0 * 6.882353782653809
Epoch 90, val loss: 1.7995134592056274
Epoch 100, training loss: 8.584959983825684 = 1.790535569190979 + 1.0 * 6.794424057006836
Epoch 100, val loss: 1.7863481044769287
Epoch 110, training loss: 8.502374649047852 = 1.7767083644866943 + 1.0 * 6.725666046142578
Epoch 110, val loss: 1.7740193605422974
Epoch 120, training loss: 8.433841705322266 = 1.763610601425171 + 1.0 * 6.670231342315674
Epoch 120, val loss: 1.7621936798095703
Epoch 130, training loss: 8.37602710723877 = 1.7500015497207642 + 1.0 * 6.626025676727295
Epoch 130, val loss: 1.7499414682388306
Epoch 140, training loss: 8.326800346374512 = 1.7348495721817017 + 1.0 * 6.591950416564941
Epoch 140, val loss: 1.7364212274551392
Epoch 150, training loss: 8.282308578491211 = 1.717513084411621 + 1.0 * 6.564795017242432
Epoch 150, val loss: 1.7210979461669922
Epoch 160, training loss: 8.24140453338623 = 1.697495698928833 + 1.0 * 6.543909072875977
Epoch 160, val loss: 1.703467607498169
Epoch 170, training loss: 8.201107025146484 = 1.6744214296340942 + 1.0 * 6.52668571472168
Epoch 170, val loss: 1.6831660270690918
Epoch 180, training loss: 8.158329010009766 = 1.6475752592086792 + 1.0 * 6.510754108428955
Epoch 180, val loss: 1.6596767902374268
Epoch 190, training loss: 8.119945526123047 = 1.6163129806518555 + 1.0 * 6.503632068634033
Epoch 190, val loss: 1.6324925422668457
Epoch 200, training loss: 8.068992614746094 = 1.5806853771209717 + 1.0 * 6.488306999206543
Epoch 200, val loss: 1.6019521951675415
Epoch 210, training loss: 8.015137672424316 = 1.540408968925476 + 1.0 * 6.474729061126709
Epoch 210, val loss: 1.5678993463516235
Epoch 220, training loss: 7.960109233856201 = 1.4949382543563843 + 1.0 * 6.465170860290527
Epoch 220, val loss: 1.5300053358078003
Epoch 230, training loss: 7.901636123657227 = 1.4443531036376953 + 1.0 * 6.457283020019531
Epoch 230, val loss: 1.4887664318084717
Epoch 240, training loss: 7.8380231857299805 = 1.3901920318603516 + 1.0 * 6.447831153869629
Epoch 240, val loss: 1.4459112882614136
Epoch 250, training loss: 7.774579048156738 = 1.333514928817749 + 1.0 * 6.441064357757568
Epoch 250, val loss: 1.4021998643875122
Epoch 260, training loss: 7.712314128875732 = 1.2749501466751099 + 1.0 * 6.437364101409912
Epoch 260, val loss: 1.3581619262695312
Epoch 270, training loss: 7.645134925842285 = 1.2157865762710571 + 1.0 * 6.429348468780518
Epoch 270, val loss: 1.3147509098052979
Epoch 280, training loss: 7.579473972320557 = 1.1568764448165894 + 1.0 * 6.422597408294678
Epoch 280, val loss: 1.272065281867981
Epoch 290, training loss: 7.518322944641113 = 1.0989423990249634 + 1.0 * 6.4193806648254395
Epoch 290, val loss: 1.2306488752365112
Epoch 300, training loss: 7.458224296569824 = 1.0431315898895264 + 1.0 * 6.415092468261719
Epoch 300, val loss: 1.1909735202789307
Epoch 310, training loss: 7.398436069488525 = 0.9899454712867737 + 1.0 * 6.4084906578063965
Epoch 310, val loss: 1.1532350778579712
Epoch 320, training loss: 7.34702205657959 = 0.9396384358406067 + 1.0 * 6.407383441925049
Epoch 320, val loss: 1.1177608966827393
Epoch 330, training loss: 7.291896820068359 = 0.8927501440048218 + 1.0 * 6.399146556854248
Epoch 330, val loss: 1.0849857330322266
Epoch 340, training loss: 7.255902290344238 = 0.849353551864624 + 1.0 * 6.406548500061035
Epoch 340, val loss: 1.0550428628921509
Epoch 350, training loss: 7.205559730529785 = 0.810094952583313 + 1.0 * 6.395464897155762
Epoch 350, val loss: 1.0282996892929077
Epoch 360, training loss: 7.163935661315918 = 0.7743331789970398 + 1.0 * 6.3896026611328125
Epoch 360, val loss: 1.0045017004013062
Epoch 370, training loss: 7.126975059509277 = 0.7413700819015503 + 1.0 * 6.3856048583984375
Epoch 370, val loss: 0.9831717014312744
Epoch 380, training loss: 7.102400779724121 = 0.7107371687889099 + 1.0 * 6.391663551330566
Epoch 380, val loss: 0.9638308882713318
Epoch 390, training loss: 7.062149524688721 = 0.6821555495262146 + 1.0 * 6.379993915557861
Epoch 390, val loss: 0.9464241862297058
Epoch 400, training loss: 7.03624963760376 = 0.6550790071487427 + 1.0 * 6.381170749664307
Epoch 400, val loss: 0.9304435849189758
Epoch 410, training loss: 7.005312442779541 = 0.6291354298591614 + 1.0 * 6.376176834106445
Epoch 410, val loss: 0.9156461358070374
Epoch 420, training loss: 6.975525856018066 = 0.6039252281188965 + 1.0 * 6.37160062789917
Epoch 420, val loss: 0.9018052816390991
Epoch 430, training loss: 6.952131271362305 = 0.5791488885879517 + 1.0 * 6.372982501983643
Epoch 430, val loss: 0.8887037038803101
Epoch 440, training loss: 6.928638935089111 = 0.5548487305641174 + 1.0 * 6.373790264129639
Epoch 440, val loss: 0.8764985799789429
Epoch 450, training loss: 6.896661281585693 = 0.5310095548629761 + 1.0 * 6.365651607513428
Epoch 450, val loss: 0.8649489283561707
Epoch 460, training loss: 6.870393753051758 = 0.5073851943016052 + 1.0 * 6.363008499145508
Epoch 460, val loss: 0.8541608452796936
Epoch 470, training loss: 6.843757152557373 = 0.483913391828537 + 1.0 * 6.359843730926514
Epoch 470, val loss: 0.8441324234008789
Epoch 480, training loss: 6.822640419006348 = 0.460610032081604 + 1.0 * 6.362030506134033
Epoch 480, val loss: 0.8349113464355469
Epoch 490, training loss: 6.805150508880615 = 0.4377628266811371 + 1.0 * 6.367387771606445
Epoch 490, val loss: 0.826687753200531
Epoch 500, training loss: 6.772686004638672 = 0.41556379199028015 + 1.0 * 6.357122421264648
Epoch 500, val loss: 0.8193098902702332
Epoch 510, training loss: 6.74655818939209 = 0.39387083053588867 + 1.0 * 6.352687358856201
Epoch 510, val loss: 0.8129076957702637
Epoch 520, training loss: 6.740160942077637 = 0.37276771664619446 + 1.0 * 6.3673930168151855
Epoch 520, val loss: 0.8073868751525879
Epoch 530, training loss: 6.704352378845215 = 0.35242342948913574 + 1.0 * 6.3519287109375
Epoch 530, val loss: 0.8026664853096008
Epoch 540, training loss: 6.679604530334473 = 0.33277812600135803 + 1.0 * 6.346826553344727
Epoch 540, val loss: 0.7986801862716675
Epoch 550, training loss: 6.65793514251709 = 0.31376415491104126 + 1.0 * 6.344171047210693
Epoch 550, val loss: 0.7954444885253906
Epoch 560, training loss: 6.656014919281006 = 0.2954011857509613 + 1.0 * 6.360613822937012
Epoch 560, val loss: 0.7928363084793091
Epoch 570, training loss: 6.623856067657471 = 0.2779223620891571 + 1.0 * 6.34593391418457
Epoch 570, val loss: 0.7907043695449829
Epoch 580, training loss: 6.601714611053467 = 0.2612074017524719 + 1.0 * 6.3405070304870605
Epoch 580, val loss: 0.7891378402709961
Epoch 590, training loss: 6.583736896514893 = 0.24525558948516846 + 1.0 * 6.338481426239014
Epoch 590, val loss: 0.7882241606712341
Epoch 600, training loss: 6.570334434509277 = 0.23014149069786072 + 1.0 * 6.340192794799805
Epoch 600, val loss: 0.7878533005714417
Epoch 610, training loss: 6.550164699554443 = 0.21593372523784637 + 1.0 * 6.334230899810791
Epoch 610, val loss: 0.787895679473877
Epoch 620, training loss: 6.534673690795898 = 0.2025381624698639 + 1.0 * 6.3321356773376465
Epoch 620, val loss: 0.7884907126426697
Epoch 630, training loss: 6.523691654205322 = 0.18993963301181793 + 1.0 * 6.333752155303955
Epoch 630, val loss: 0.789620041847229
Epoch 640, training loss: 6.514934062957764 = 0.17817293107509613 + 1.0 * 6.336760997772217
Epoch 640, val loss: 0.7911351919174194
Epoch 650, training loss: 6.499634265899658 = 0.16725416481494904 + 1.0 * 6.332380294799805
Epoch 650, val loss: 0.7929408550262451
Epoch 660, training loss: 6.485099792480469 = 0.15707722306251526 + 1.0 * 6.328022480010986
Epoch 660, val loss: 0.7951173186302185
Epoch 670, training loss: 6.473801612854004 = 0.1475561559200287 + 1.0 * 6.326245307922363
Epoch 670, val loss: 0.7977991700172424
Epoch 680, training loss: 6.470645904541016 = 0.13865816593170166 + 1.0 * 6.3319878578186035
Epoch 680, val loss: 0.8008097410202026
Epoch 690, training loss: 6.4550557136535645 = 0.13036535680294037 + 1.0 * 6.324690341949463
Epoch 690, val loss: 0.8041018843650818
Epoch 700, training loss: 6.447299480438232 = 0.12266093492507935 + 1.0 * 6.324638366699219
Epoch 700, val loss: 0.8076748847961426
Epoch 710, training loss: 6.435857772827148 = 0.11548877507448196 + 1.0 * 6.320368766784668
Epoch 710, val loss: 0.8114752769470215
Epoch 720, training loss: 6.42710018157959 = 0.1088070422410965 + 1.0 * 6.31829309463501
Epoch 720, val loss: 0.8154935836791992
Epoch 730, training loss: 6.435399055480957 = 0.10257767885923386 + 1.0 * 6.332821369171143
Epoch 730, val loss: 0.819705605506897
Epoch 740, training loss: 6.417688369750977 = 0.09681179374456406 + 1.0 * 6.320876598358154
Epoch 740, val loss: 0.8240146636962891
Epoch 750, training loss: 6.4075727462768555 = 0.09145462512969971 + 1.0 * 6.316118240356445
Epoch 750, val loss: 0.8283491730690002
Epoch 760, training loss: 6.402789115905762 = 0.08646821975708008 + 1.0 * 6.316320896148682
Epoch 760, val loss: 0.8329050540924072
Epoch 770, training loss: 6.398722171783447 = 0.0818219929933548 + 1.0 * 6.316900253295898
Epoch 770, val loss: 0.8376243710517883
Epoch 780, training loss: 6.38933801651001 = 0.07749790698289871 + 1.0 * 6.311840057373047
Epoch 780, val loss: 0.8423287272453308
Epoch 790, training loss: 6.397544860839844 = 0.07346080988645554 + 1.0 * 6.324084281921387
Epoch 790, val loss: 0.8471624255180359
Epoch 800, training loss: 6.380241394042969 = 0.06970731168985367 + 1.0 * 6.3105340003967285
Epoch 800, val loss: 0.8519923686981201
Epoch 810, training loss: 6.3744025230407715 = 0.0662168636918068 + 1.0 * 6.308185577392578
Epoch 810, val loss: 0.8567837476730347
Epoch 820, training loss: 6.370476245880127 = 0.0629502609372139 + 1.0 * 6.307526111602783
Epoch 820, val loss: 0.8617109656333923
Epoch 830, training loss: 6.379823207855225 = 0.059893082827329636 + 1.0 * 6.319930076599121
Epoch 830, val loss: 0.8666940927505493
Epoch 840, training loss: 6.3629865646362305 = 0.05705016478896141 + 1.0 * 6.305936336517334
Epoch 840, val loss: 0.8716090321540833
Epoch 850, training loss: 6.360466480255127 = 0.054393839091062546 + 1.0 * 6.30607271194458
Epoch 850, val loss: 0.8764180541038513
Epoch 860, training loss: 6.354724884033203 = 0.05190332233905792 + 1.0 * 6.302821636199951
Epoch 860, val loss: 0.881328821182251
Epoch 870, training loss: 6.363574028015137 = 0.04956316202878952 + 1.0 * 6.314011096954346
Epoch 870, val loss: 0.8863149285316467
Epoch 880, training loss: 6.354104518890381 = 0.04737686365842819 + 1.0 * 6.306727886199951
Epoch 880, val loss: 0.8911951184272766
Epoch 890, training loss: 6.346087455749512 = 0.045328620821237564 + 1.0 * 6.3007588386535645
Epoch 890, val loss: 0.8959988355636597
Epoch 900, training loss: 6.342645645141602 = 0.043401554226875305 + 1.0 * 6.299243927001953
Epoch 900, val loss: 0.900848925113678
Epoch 910, training loss: 6.344141483306885 = 0.04158579185605049 + 1.0 * 6.302555561065674
Epoch 910, val loss: 0.9057161211967468
Epoch 920, training loss: 6.346328258514404 = 0.03988174349069595 + 1.0 * 6.306446552276611
Epoch 920, val loss: 0.9105123281478882
Epoch 930, training loss: 6.337002277374268 = 0.03828044235706329 + 1.0 * 6.298721790313721
Epoch 930, val loss: 0.9151700735092163
Epoch 940, training loss: 6.333597660064697 = 0.03677630051970482 + 1.0 * 6.296821594238281
Epoch 940, val loss: 0.9197589159011841
Epoch 950, training loss: 6.329586982727051 = 0.03535265848040581 + 1.0 * 6.294234275817871
Epoch 950, val loss: 0.9244623184204102
Epoch 960, training loss: 6.33795690536499 = 0.03400249406695366 + 1.0 * 6.303954601287842
Epoch 960, val loss: 0.929149866104126
Epoch 970, training loss: 6.331599712371826 = 0.03273136913776398 + 1.0 * 6.298868179321289
Epoch 970, val loss: 0.9337424039840698
Epoch 980, training loss: 6.323815822601318 = 0.03153068199753761 + 1.0 * 6.292284965515137
Epoch 980, val loss: 0.9381664991378784
Epoch 990, training loss: 6.324553489685059 = 0.03039523959159851 + 1.0 * 6.294158458709717
Epoch 990, val loss: 0.9425904154777527
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.812335266209805
The final CL Acc:0.75679, 0.01552, The final GNN Acc:0.81198, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13260])
remove edge: torch.Size([2, 7906])
updated graph: torch.Size([2, 10610])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.557575225830078 = 1.9606951475143433 + 1.0 * 8.596879959106445
Epoch 0, val loss: 1.9575837850570679
Epoch 10, training loss: 10.547066688537598 = 1.9503304958343506 + 1.0 * 8.596735954284668
Epoch 10, val loss: 1.9470940828323364
Epoch 20, training loss: 10.533381462097168 = 1.937950849533081 + 1.0 * 8.595430374145508
Epoch 20, val loss: 1.9346250295639038
Epoch 30, training loss: 10.504887580871582 = 1.9212127923965454 + 1.0 * 8.583674430847168
Epoch 30, val loss: 1.9177931547164917
Epoch 40, training loss: 10.409788131713867 = 1.8978605270385742 + 1.0 * 8.511927604675293
Epoch 40, val loss: 1.8947677612304688
Epoch 50, training loss: 10.00250244140625 = 1.8700830936431885 + 1.0 * 8.13241958618164
Epoch 50, val loss: 1.8683931827545166
Epoch 60, training loss: 9.709002494812012 = 1.8413416147232056 + 1.0 * 7.867660999298096
Epoch 60, val loss: 1.842700481414795
Epoch 70, training loss: 9.303089141845703 = 1.818331241607666 + 1.0 * 7.484758377075195
Epoch 70, val loss: 1.8216936588287354
Epoch 80, training loss: 8.928730964660645 = 1.7994654178619385 + 1.0 * 7.129265308380127
Epoch 80, val loss: 1.8041481971740723
Epoch 90, training loss: 8.745777130126953 = 1.7823443412780762 + 1.0 * 6.963432788848877
Epoch 90, val loss: 1.7878005504608154
Epoch 100, training loss: 8.666820526123047 = 1.7590093612670898 + 1.0 * 6.907810688018799
Epoch 100, val loss: 1.7666071653366089
Epoch 110, training loss: 8.585489273071289 = 1.733755111694336 + 1.0 * 6.851734161376953
Epoch 110, val loss: 1.7443592548370361
Epoch 120, training loss: 8.50519847869873 = 1.710192084312439 + 1.0 * 6.79500675201416
Epoch 120, val loss: 1.7225711345672607
Epoch 130, training loss: 8.418797492980957 = 1.6850553750991821 + 1.0 * 6.733741760253906
Epoch 130, val loss: 1.6990045309066772
Epoch 140, training loss: 8.341506958007812 = 1.6561256647109985 + 1.0 * 6.685380935668945
Epoch 140, val loss: 1.6728929281234741
Epoch 150, training loss: 8.266022682189941 = 1.6225863695144653 + 1.0 * 6.643436431884766
Epoch 150, val loss: 1.6434694528579712
Epoch 160, training loss: 8.19290828704834 = 1.5835696458816528 + 1.0 * 6.609338760375977
Epoch 160, val loss: 1.6093250513076782
Epoch 170, training loss: 8.119373321533203 = 1.538598656654358 + 1.0 * 6.580774784088135
Epoch 170, val loss: 1.5700432062149048
Epoch 180, training loss: 8.044301986694336 = 1.4875919818878174 + 1.0 * 6.556710243225098
Epoch 180, val loss: 1.5258266925811768
Epoch 190, training loss: 7.967710494995117 = 1.4307814836502075 + 1.0 * 6.536929130554199
Epoch 190, val loss: 1.477368712425232
Epoch 200, training loss: 7.891393184661865 = 1.370064616203308 + 1.0 * 6.521328449249268
Epoch 200, val loss: 1.4267760515213013
Epoch 210, training loss: 7.813154220581055 = 1.3075690269470215 + 1.0 * 6.505585193634033
Epoch 210, val loss: 1.3756840229034424
Epoch 220, training loss: 7.743636131286621 = 1.2442251443862915 + 1.0 * 6.499411106109619
Epoch 220, val loss: 1.3250939846038818
Epoch 230, training loss: 7.667065620422363 = 1.1828030347824097 + 1.0 * 6.484262466430664
Epoch 230, val loss: 1.2771743535995483
Epoch 240, training loss: 7.5971784591674805 = 1.1230762004852295 + 1.0 * 6.474102020263672
Epoch 240, val loss: 1.2313114404678345
Epoch 250, training loss: 7.530722618103027 = 1.064902424812317 + 1.0 * 6.4658203125
Epoch 250, val loss: 1.1872040033340454
Epoch 260, training loss: 7.4666643142700195 = 1.0082480907440186 + 1.0 * 6.458415985107422
Epoch 260, val loss: 1.1447720527648926
Epoch 270, training loss: 7.410837650299072 = 0.9544309973716736 + 1.0 * 6.456406593322754
Epoch 270, val loss: 1.1049095392227173
Epoch 280, training loss: 7.349917888641357 = 0.9044280648231506 + 1.0 * 6.445489883422852
Epoch 280, val loss: 1.0678467750549316
Epoch 290, training loss: 7.296011924743652 = 0.8569130301475525 + 1.0 * 6.439098834991455
Epoch 290, val loss: 1.032596230506897
Epoch 300, training loss: 7.244379043579102 = 0.8116249442100525 + 1.0 * 6.432754039764404
Epoch 300, val loss: 0.9991924166679382
Epoch 310, training loss: 7.210909366607666 = 0.768711268901825 + 1.0 * 6.442198276519775
Epoch 310, val loss: 0.9676008224487305
Epoch 320, training loss: 7.156306743621826 = 0.7292662262916565 + 1.0 * 6.4270405769348145
Epoch 320, val loss: 0.9385979771614075
Epoch 330, training loss: 7.109679698944092 = 0.692200243473053 + 1.0 * 6.417479515075684
Epoch 330, val loss: 0.9118199944496155
Epoch 340, training loss: 7.079928398132324 = 0.6570480465888977 + 1.0 * 6.422880172729492
Epoch 340, val loss: 0.8868372440338135
Epoch 350, training loss: 7.033183574676514 = 0.6239520311355591 + 1.0 * 6.409231662750244
Epoch 350, val loss: 0.8637952208518982
Epoch 360, training loss: 6.996890544891357 = 0.5922347903251648 + 1.0 * 6.404655933380127
Epoch 360, val loss: 0.8425174355506897
Epoch 370, training loss: 6.96065092086792 = 0.5614112019538879 + 1.0 * 6.399239540100098
Epoch 370, val loss: 0.822485089302063
Epoch 380, training loss: 6.929065704345703 = 0.5314837098121643 + 1.0 * 6.397582054138184
Epoch 380, val loss: 0.8037700057029724
Epoch 390, training loss: 6.89560604095459 = 0.5025858879089355 + 1.0 * 6.393020153045654
Epoch 390, val loss: 0.7864755392074585
Epoch 400, training loss: 6.863437175750732 = 0.47444528341293335 + 1.0 * 6.388991832733154
Epoch 400, val loss: 0.7705885171890259
Epoch 410, training loss: 6.836760997772217 = 0.4473276138305664 + 1.0 * 6.38943338394165
Epoch 410, val loss: 0.7561084628105164
Epoch 420, training loss: 6.802647590637207 = 0.4213942885398865 + 1.0 * 6.381253242492676
Epoch 420, val loss: 0.7432077527046204
Epoch 430, training loss: 6.773374557495117 = 0.39652371406555176 + 1.0 * 6.3768510818481445
Epoch 430, val loss: 0.7317769527435303
Epoch 440, training loss: 6.7585954666137695 = 0.3727569282054901 + 1.0 * 6.385838508605957
Epoch 440, val loss: 0.7217615246772766
Epoch 450, training loss: 6.721691131591797 = 0.3503219783306122 + 1.0 * 6.371369361877441
Epoch 450, val loss: 0.7131275534629822
Epoch 460, training loss: 6.696983337402344 = 0.3291340470314026 + 1.0 * 6.367849349975586
Epoch 460, val loss: 0.7058354616165161
Epoch 470, training loss: 6.677768230438232 = 0.3089973032474518 + 1.0 * 6.368771076202393
Epoch 470, val loss: 0.6996904015541077
Epoch 480, training loss: 6.65827751159668 = 0.29006391763687134 + 1.0 * 6.368213653564453
Epoch 480, val loss: 0.6945058703422546
Epoch 490, training loss: 6.6334404945373535 = 0.27220699191093445 + 1.0 * 6.361233711242676
Epoch 490, val loss: 0.6903547048568726
Epoch 500, training loss: 6.614730358123779 = 0.2553056478500366 + 1.0 * 6.359424591064453
Epoch 500, val loss: 0.6870755553245544
Epoch 510, training loss: 6.601365566253662 = 0.23944473266601562 + 1.0 * 6.3619208335876465
Epoch 510, val loss: 0.6844667792320251
Epoch 520, training loss: 6.5794477462768555 = 0.22456935048103333 + 1.0 * 6.3548784255981445
Epoch 520, val loss: 0.6826518177986145
Epoch 530, training loss: 6.561478137969971 = 0.21060366928577423 + 1.0 * 6.350874423980713
Epoch 530, val loss: 0.6815312504768372
Epoch 540, training loss: 6.5461835861206055 = 0.19748279452323914 + 1.0 * 6.348701000213623
Epoch 540, val loss: 0.6810405254364014
Epoch 550, training loss: 6.536744117736816 = 0.1852181851863861 + 1.0 * 6.351525783538818
Epoch 550, val loss: 0.681125283241272
Epoch 560, training loss: 6.523377418518066 = 0.17384213209152222 + 1.0 * 6.3495354652404785
Epoch 560, val loss: 0.6817152500152588
Epoch 570, training loss: 6.5089030265808105 = 0.16325975954532623 + 1.0 * 6.345643043518066
Epoch 570, val loss: 0.6829215288162231
Epoch 580, training loss: 6.49620246887207 = 0.1534218192100525 + 1.0 * 6.342780590057373
Epoch 580, val loss: 0.6845051646232605
Epoch 590, training loss: 6.483234405517578 = 0.14427927136421204 + 1.0 * 6.338954925537109
Epoch 590, val loss: 0.6864975094795227
Epoch 600, training loss: 6.477485179901123 = 0.1357681155204773 + 1.0 * 6.34171724319458
Epoch 600, val loss: 0.6888858079910278
Epoch 610, training loss: 6.472770690917969 = 0.12788088619709015 + 1.0 * 6.3448896408081055
Epoch 610, val loss: 0.691482424736023
Epoch 620, training loss: 6.454725742340088 = 0.1205844134092331 + 1.0 * 6.334141254425049
Epoch 620, val loss: 0.6944877505302429
Epoch 630, training loss: 6.444780349731445 = 0.11376776546239853 + 1.0 * 6.331012725830078
Epoch 630, val loss: 0.697834849357605
Epoch 640, training loss: 6.436840057373047 = 0.10739922523498535 + 1.0 * 6.329441070556641
Epoch 640, val loss: 0.7014437913894653
Epoch 650, training loss: 6.4515461921691895 = 0.1014663428068161 + 1.0 * 6.3500800132751465
Epoch 650, val loss: 0.7052080035209656
Epoch 660, training loss: 6.424051284790039 = 0.09595026820898056 + 1.0 * 6.32810115814209
Epoch 660, val loss: 0.7089868783950806
Epoch 670, training loss: 6.415958881378174 = 0.09083741903305054 + 1.0 * 6.3251214027404785
Epoch 670, val loss: 0.7130374312400818
Epoch 680, training loss: 6.409549713134766 = 0.0860513299703598 + 1.0 * 6.323498249053955
Epoch 680, val loss: 0.7173144221305847
Epoch 690, training loss: 6.403048038482666 = 0.08156634867191315 + 1.0 * 6.321481704711914
Epoch 690, val loss: 0.7216960787773132
Epoch 700, training loss: 6.409587860107422 = 0.07737086713314056 + 1.0 * 6.332217216491699
Epoch 700, val loss: 0.726117730140686
Epoch 710, training loss: 6.395874500274658 = 0.07347389310598373 + 1.0 * 6.3224005699157715
Epoch 710, val loss: 0.7305892109870911
Epoch 720, training loss: 6.389151096343994 = 0.06983139365911484 + 1.0 * 6.319319725036621
Epoch 720, val loss: 0.7351160645484924
Epoch 730, training loss: 6.385342597961426 = 0.06641892343759537 + 1.0 * 6.318923473358154
Epoch 730, val loss: 0.7397617697715759
Epoch 740, training loss: 6.379985809326172 = 0.06321997195482254 + 1.0 * 6.316765785217285
Epoch 740, val loss: 0.744381844997406
Epoch 750, training loss: 6.377676010131836 = 0.06022252142429352 + 1.0 * 6.317453384399414
Epoch 750, val loss: 0.7490634918212891
Epoch 760, training loss: 6.37060022354126 = 0.057416319847106934 + 1.0 * 6.313183784484863
Epoch 760, val loss: 0.7537738084793091
Epoch 770, training loss: 6.371373176574707 = 0.05478367209434509 + 1.0 * 6.31658935546875
Epoch 770, val loss: 0.7584801316261292
Epoch 780, training loss: 6.363533020019531 = 0.05230911821126938 + 1.0 * 6.311223983764648
Epoch 780, val loss: 0.7632192969322205
Epoch 790, training loss: 6.361506938934326 = 0.04998749867081642 + 1.0 * 6.311519622802734
Epoch 790, val loss: 0.7679261565208435
Epoch 800, training loss: 6.364137649536133 = 0.047805484384298325 + 1.0 * 6.3163323402404785
Epoch 800, val loss: 0.7726786136627197
Epoch 810, training loss: 6.353996753692627 = 0.04576323181390762 + 1.0 * 6.308233737945557
Epoch 810, val loss: 0.7773158550262451
Epoch 820, training loss: 6.349608898162842 = 0.04383668303489685 + 1.0 * 6.305772304534912
Epoch 820, val loss: 0.7820075750350952
Epoch 830, training loss: 6.350549221038818 = 0.04201885312795639 + 1.0 * 6.308530330657959
Epoch 830, val loss: 0.7867353558540344
Epoch 840, training loss: 6.34458589553833 = 0.04030904173851013 + 1.0 * 6.304276943206787
Epoch 840, val loss: 0.7913641333580017
Epoch 850, training loss: 6.34917688369751 = 0.038696225732564926 + 1.0 * 6.31048059463501
Epoch 850, val loss: 0.7959964871406555
Epoch 860, training loss: 6.342372417449951 = 0.037170954048633575 + 1.0 * 6.305201530456543
Epoch 860, val loss: 0.8004978895187378
Epoch 870, training loss: 6.336282253265381 = 0.03573598712682724 + 1.0 * 6.300546169281006
Epoch 870, val loss: 0.805047333240509
Epoch 880, training loss: 6.337878704071045 = 0.03437884896993637 + 1.0 * 6.303499698638916
Epoch 880, val loss: 0.8095725178718567
Epoch 890, training loss: 6.333128929138184 = 0.03309221565723419 + 1.0 * 6.300036907196045
Epoch 890, val loss: 0.814022958278656
Epoch 900, training loss: 6.3299736976623535 = 0.03187602013349533 + 1.0 * 6.298097610473633
Epoch 900, val loss: 0.8184095621109009
Epoch 910, training loss: 6.329276084899902 = 0.030723366886377335 + 1.0 * 6.298552513122559
Epoch 910, val loss: 0.8228425979614258
Epoch 920, training loss: 6.330280303955078 = 0.029629452154040337 + 1.0 * 6.3006510734558105
Epoch 920, val loss: 0.8272091746330261
Epoch 930, training loss: 6.326948642730713 = 0.028597062453627586 + 1.0 * 6.298351764678955
Epoch 930, val loss: 0.8314603567123413
Epoch 940, training loss: 6.323038578033447 = 0.027616316452622414 + 1.0 * 6.295422077178955
Epoch 940, val loss: 0.8356433510780334
Epoch 950, training loss: 6.320332050323486 = 0.026680998504161835 + 1.0 * 6.293651103973389
Epoch 950, val loss: 0.8398818969726562
Epoch 960, training loss: 6.325509071350098 = 0.025789868086576462 + 1.0 * 6.299719333648682
Epoch 960, val loss: 0.8440623879432678
Epoch 970, training loss: 6.326165199279785 = 0.02494480088353157 + 1.0 * 6.301220417022705
Epoch 970, val loss: 0.8482855558395386
Epoch 980, training loss: 6.317717552185059 = 0.024142341688275337 + 1.0 * 6.293575286865234
Epoch 980, val loss: 0.8522385954856873
Epoch 990, training loss: 6.313198089599609 = 0.023379141464829445 + 1.0 * 6.28981876373291
Epoch 990, val loss: 0.8562396168708801
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.558226585388184 = 1.9613630771636963 + 1.0 * 8.596863746643066
Epoch 0, val loss: 1.952347993850708
Epoch 10, training loss: 10.546932220458984 = 1.9503426551818848 + 1.0 * 8.596589088439941
Epoch 10, val loss: 1.94194495677948
Epoch 20, training loss: 10.530301094055176 = 1.9366827011108398 + 1.0 * 8.593618392944336
Epoch 20, val loss: 1.9286940097808838
Epoch 30, training loss: 10.486353874206543 = 1.9175264835357666 + 1.0 * 8.568827629089355
Epoch 30, val loss: 1.9099894762039185
Epoch 40, training loss: 10.318163871765137 = 1.8922075033187866 + 1.0 * 8.425956726074219
Epoch 40, val loss: 1.8863797187805176
Epoch 50, training loss: 9.893659591674805 = 1.8625928163528442 + 1.0 * 8.03106689453125
Epoch 50, val loss: 1.8596502542495728
Epoch 60, training loss: 9.683215141296387 = 1.8342126607894897 + 1.0 * 7.849002838134766
Epoch 60, val loss: 1.8353298902511597
Epoch 70, training loss: 9.334136009216309 = 1.810629963874817 + 1.0 * 7.523505687713623
Epoch 70, val loss: 1.814865231513977
Epoch 80, training loss: 8.949222564697266 = 1.793938159942627 + 1.0 * 7.1552839279174805
Epoch 80, val loss: 1.8004640340805054
Epoch 90, training loss: 8.781481742858887 = 1.7770798206329346 + 1.0 * 7.004402160644531
Epoch 90, val loss: 1.785064458847046
Epoch 100, training loss: 8.642865180969238 = 1.7556476593017578 + 1.0 * 6.8872175216674805
Epoch 100, val loss: 1.7659285068511963
Epoch 110, training loss: 8.53698444366455 = 1.734074354171753 + 1.0 * 6.802910327911377
Epoch 110, val loss: 1.7472211122512817
Epoch 120, training loss: 8.443470001220703 = 1.7115943431854248 + 1.0 * 6.731875896453857
Epoch 120, val loss: 1.727493405342102
Epoch 130, training loss: 8.36651611328125 = 1.6860203742980957 + 1.0 * 6.680495738983154
Epoch 130, val loss: 1.7051836252212524
Epoch 140, training loss: 8.297967910766602 = 1.655592441558838 + 1.0 * 6.642375946044922
Epoch 140, val loss: 1.679074764251709
Epoch 150, training loss: 8.233819961547852 = 1.6204578876495361 + 1.0 * 6.613361835479736
Epoch 150, val loss: 1.6496752500534058
Epoch 160, training loss: 8.169784545898438 = 1.5818394422531128 + 1.0 * 6.587945461273193
Epoch 160, val loss: 1.6179227828979492
Epoch 170, training loss: 8.105813026428223 = 1.540549635887146 + 1.0 * 6.565263748168945
Epoch 170, val loss: 1.5844314098358154
Epoch 180, training loss: 8.046708106994629 = 1.4968541860580444 + 1.0 * 6.549853801727295
Epoch 180, val loss: 1.5497187376022339
Epoch 190, training loss: 7.9799299240112305 = 1.4526225328445435 + 1.0 * 6.527307510375977
Epoch 190, val loss: 1.5153721570968628
Epoch 200, training loss: 7.916397571563721 = 1.4080995321273804 + 1.0 * 6.508297920227051
Epoch 200, val loss: 1.4816126823425293
Epoch 210, training loss: 7.8612518310546875 = 1.3639124631881714 + 1.0 * 6.497339248657227
Epoch 210, val loss: 1.4490323066711426
Epoch 220, training loss: 7.7996907234191895 = 1.321085810661316 + 1.0 * 6.478604793548584
Epoch 220, val loss: 1.4183284044265747
Epoch 230, training loss: 7.747143745422363 = 1.2793357372283936 + 1.0 * 6.467807769775391
Epoch 230, val loss: 1.3893147706985474
Epoch 240, training loss: 7.6983184814453125 = 1.238527536392212 + 1.0 * 6.45979118347168
Epoch 240, val loss: 1.3616520166397095
Epoch 250, training loss: 7.652679920196533 = 1.1988946199417114 + 1.0 * 6.453785419464111
Epoch 250, val loss: 1.3355395793914795
Epoch 260, training loss: 7.606988906860352 = 1.1609597206115723 + 1.0 * 6.446029186248779
Epoch 260, val loss: 1.310915231704712
Epoch 270, training loss: 7.560126781463623 = 1.1243659257888794 + 1.0 * 6.435760974884033
Epoch 270, val loss: 1.2878273725509644
Epoch 280, training loss: 7.517261981964111 = 1.088612675666809 + 1.0 * 6.428649425506592
Epoch 280, val loss: 1.2655465602874756
Epoch 290, training loss: 7.476438522338867 = 1.0535953044891357 + 1.0 * 6.422842979431152
Epoch 290, val loss: 1.2437268495559692
Epoch 300, training loss: 7.438906669616699 = 1.0194405317306519 + 1.0 * 6.419466018676758
Epoch 300, val loss: 1.2223981618881226
Epoch 310, training loss: 7.403269290924072 = 0.9860712289810181 + 1.0 * 6.417198181152344
Epoch 310, val loss: 1.2015368938446045
Epoch 320, training loss: 7.365029811859131 = 0.9534674286842346 + 1.0 * 6.411562442779541
Epoch 320, val loss: 1.181025743484497
Epoch 330, training loss: 7.325374603271484 = 0.9210744500160217 + 1.0 * 6.404300212860107
Epoch 330, val loss: 1.1604571342468262
Epoch 340, training loss: 7.290724754333496 = 0.8882739543914795 + 1.0 * 6.4024505615234375
Epoch 340, val loss: 1.1392555236816406
Epoch 350, training loss: 7.253705024719238 = 0.8549493551254272 + 1.0 * 6.3987555503845215
Epoch 350, val loss: 1.117477536201477
Epoch 360, training loss: 7.219426155090332 = 0.8208773136138916 + 1.0 * 6.398548603057861
Epoch 360, val loss: 1.095071792602539
Epoch 370, training loss: 7.178757667541504 = 0.7860314249992371 + 1.0 * 6.392726421356201
Epoch 370, val loss: 1.0719623565673828
Epoch 380, training loss: 7.138859748840332 = 0.7504270076751709 + 1.0 * 6.388432502746582
Epoch 380, val loss: 1.0485742092132568
Epoch 390, training loss: 7.102233409881592 = 0.7141580581665039 + 1.0 * 6.388075351715088
Epoch 390, val loss: 1.0248773097991943
Epoch 400, training loss: 7.063457489013672 = 0.6777881979942322 + 1.0 * 6.385669231414795
Epoch 400, val loss: 1.001510500907898
Epoch 410, training loss: 7.023243427276611 = 0.6419576406478882 + 1.0 * 6.381285667419434
Epoch 410, val loss: 0.9795741438865662
Epoch 420, training loss: 6.985942363739014 = 0.6068819165229797 + 1.0 * 6.3790602684021
Epoch 420, val loss: 0.9593086242675781
Epoch 430, training loss: 6.952183723449707 = 0.572983980178833 + 1.0 * 6.379199504852295
Epoch 430, val loss: 0.941087543964386
Epoch 440, training loss: 6.914797782897949 = 0.5406937599182129 + 1.0 * 6.374104022979736
Epoch 440, val loss: 0.9254893064498901
Epoch 450, training loss: 6.881593227386475 = 0.5100275874137878 + 1.0 * 6.371565818786621
Epoch 450, val loss: 0.9124078750610352
Epoch 460, training loss: 6.86018180847168 = 0.48093464970588684 + 1.0 * 6.379247188568115
Epoch 460, val loss: 0.9016337990760803
Epoch 470, training loss: 6.8217926025390625 = 0.4537031352519989 + 1.0 * 6.36808967590332
Epoch 470, val loss: 0.8933019638061523
Epoch 480, training loss: 6.794022560119629 = 0.42803096771240234 + 1.0 * 6.365991592407227
Epoch 480, val loss: 0.8871026039123535
Epoch 490, training loss: 6.771209716796875 = 0.4037584662437439 + 1.0 * 6.367451190948486
Epoch 490, val loss: 0.8826261758804321
Epoch 500, training loss: 6.741976261138916 = 0.38081100583076477 + 1.0 * 6.3611650466918945
Epoch 500, val loss: 0.8797705173492432
Epoch 510, training loss: 6.734218120574951 = 0.3590264916419983 + 1.0 * 6.375191688537598
Epoch 510, val loss: 0.8782195448875427
Epoch 520, training loss: 6.695866584777832 = 0.33846816420555115 + 1.0 * 6.357398509979248
Epoch 520, val loss: 0.8778219223022461
Epoch 530, training loss: 6.673546314239502 = 0.31884416937828064 + 1.0 * 6.354701995849609
Epoch 530, val loss: 0.8784825205802917
Epoch 540, training loss: 6.654038429260254 = 0.3000313341617584 + 1.0 * 6.354007244110107
Epoch 540, val loss: 0.8799830675125122
Epoch 550, training loss: 6.638130187988281 = 0.28210505843162537 + 1.0 * 6.356025218963623
Epoch 550, val loss: 0.8822987675666809
Epoch 560, training loss: 6.61466646194458 = 0.26510581374168396 + 1.0 * 6.349560737609863
Epoch 560, val loss: 0.8854647278785706
Epoch 570, training loss: 6.595855236053467 = 0.24892333149909973 + 1.0 * 6.3469319343566895
Epoch 570, val loss: 0.8894171714782715
Epoch 580, training loss: 6.579288959503174 = 0.23351925611495972 + 1.0 * 6.345769882202148
Epoch 580, val loss: 0.8941028714179993
Epoch 590, training loss: 6.579967021942139 = 0.2189435213804245 + 1.0 * 6.361023426055908
Epoch 590, val loss: 0.8993784785270691
Epoch 600, training loss: 6.552219867706299 = 0.20536808669567108 + 1.0 * 6.346851825714111
Epoch 600, val loss: 0.9053000211715698
Epoch 610, training loss: 6.534828186035156 = 0.19262725114822388 + 1.0 * 6.342200756072998
Epoch 610, val loss: 0.9118062853813171
Epoch 620, training loss: 6.518741607666016 = 0.1806478202342987 + 1.0 * 6.3380937576293945
Epoch 620, val loss: 0.9188829660415649
Epoch 630, training loss: 6.505909442901611 = 0.16940157115459442 + 1.0 * 6.336507797241211
Epoch 630, val loss: 0.9264476895332336
Epoch 640, training loss: 6.513006210327148 = 0.1588616669178009 + 1.0 * 6.35414457321167
Epoch 640, val loss: 0.934410810470581
Epoch 650, training loss: 6.493645191192627 = 0.14916415512561798 + 1.0 * 6.344480991363525
Epoch 650, val loss: 0.9424750804901123
Epoch 660, training loss: 6.475720405578613 = 0.14021028578281403 + 1.0 * 6.33551025390625
Epoch 660, val loss: 0.9508740305900574
Epoch 670, training loss: 6.462883949279785 = 0.13187956809997559 + 1.0 * 6.3310041427612305
Epoch 670, val loss: 0.9595729112625122
Epoch 680, training loss: 6.453317165374756 = 0.12409313023090363 + 1.0 * 6.329224109649658
Epoch 680, val loss: 0.9685637354850769
Epoch 690, training loss: 6.445228099822998 = 0.11682766675949097 + 1.0 * 6.328400611877441
Epoch 690, val loss: 0.977753221988678
Epoch 700, training loss: 6.441345691680908 = 0.11007028818130493 + 1.0 * 6.331275463104248
Epoch 700, val loss: 0.9869859218597412
Epoch 710, training loss: 6.433284282684326 = 0.10384673625230789 + 1.0 * 6.329437732696533
Epoch 710, val loss: 0.9961467385292053
Epoch 720, training loss: 6.423957824707031 = 0.09808174520730972 + 1.0 * 6.325876235961914
Epoch 720, val loss: 1.005349040031433
Epoch 730, training loss: 6.414948463439941 = 0.09271073341369629 + 1.0 * 6.322237491607666
Epoch 730, val loss: 1.0146092176437378
Epoch 740, training loss: 6.42134952545166 = 0.0877184197306633 + 1.0 * 6.3336310386657715
Epoch 740, val loss: 1.0238336324691772
Epoch 750, training loss: 6.408982276916504 = 0.08308165520429611 + 1.0 * 6.325900554656982
Epoch 750, val loss: 1.0329540967941284
Epoch 760, training loss: 6.399399757385254 = 0.07879874110221863 + 1.0 * 6.320600986480713
Epoch 760, val loss: 1.04200279712677
Epoch 770, training loss: 6.3922882080078125 = 0.07479677349328995 + 1.0 * 6.31749153137207
Epoch 770, val loss: 1.0510538816452026
Epoch 780, training loss: 6.398468494415283 = 0.07107196003198624 + 1.0 * 6.327396392822266
Epoch 780, val loss: 1.059998631477356
Epoch 790, training loss: 6.387075424194336 = 0.0676226019859314 + 1.0 * 6.31945276260376
Epoch 790, val loss: 1.0687485933303833
Epoch 800, training loss: 6.379403591156006 = 0.06440995633602142 + 1.0 * 6.314993858337402
Epoch 800, val loss: 1.077428936958313
Epoch 810, training loss: 6.383256435394287 = 0.06140165776014328 + 1.0 * 6.321854591369629
Epoch 810, val loss: 1.0860978364944458
Epoch 820, training loss: 6.375141620635986 = 0.058609455823898315 + 1.0 * 6.316532135009766
Epoch 820, val loss: 1.094534158706665
Epoch 830, training loss: 6.367716312408447 = 0.05599820613861084 + 1.0 * 6.311717987060547
Epoch 830, val loss: 1.1028833389282227
Epoch 840, training loss: 6.363222599029541 = 0.05354907363653183 + 1.0 * 6.309673309326172
Epoch 840, val loss: 1.111223816871643
Epoch 850, training loss: 6.359316825866699 = 0.05124356597661972 + 1.0 * 6.308073043823242
Epoch 850, val loss: 1.119477391242981
Epoch 860, training loss: 6.367132186889648 = 0.049077730625867844 + 1.0 * 6.318054676055908
Epoch 860, val loss: 1.1276150941848755
Epoch 870, training loss: 6.360015392303467 = 0.04704897478222847 + 1.0 * 6.312966346740723
Epoch 870, val loss: 1.1356147527694702
Epoch 880, training loss: 6.350797176361084 = 0.045152273029088974 + 1.0 * 6.305644989013672
Epoch 880, val loss: 1.1433426141738892
Epoch 890, training loss: 6.348515033721924 = 0.043365269899368286 + 1.0 * 6.305149555206299
Epoch 890, val loss: 1.151023268699646
Epoch 900, training loss: 6.3519768714904785 = 0.04167618602514267 + 1.0 * 6.310300827026367
Epoch 900, val loss: 1.1586413383483887
Epoch 910, training loss: 6.348809719085693 = 0.04008781537413597 + 1.0 * 6.308722019195557
Epoch 910, val loss: 1.166154146194458
Epoch 920, training loss: 6.3401641845703125 = 0.03859546780586243 + 1.0 * 6.301568508148193
Epoch 920, val loss: 1.1734082698822021
Epoch 930, training loss: 6.337934970855713 = 0.03718137741088867 + 1.0 * 6.300753593444824
Epoch 930, val loss: 1.1806857585906982
Epoch 940, training loss: 6.346071243286133 = 0.035839296877384186 + 1.0 * 6.310232162475586
Epoch 940, val loss: 1.1878689527511597
Epoch 950, training loss: 6.35093355178833 = 0.0345706008374691 + 1.0 * 6.3163628578186035
Epoch 950, val loss: 1.194958209991455
Epoch 960, training loss: 6.335918426513672 = 0.03338746726512909 + 1.0 * 6.302530765533447
Epoch 960, val loss: 1.2016733884811401
Epoch 970, training loss: 6.330161094665527 = 0.03225712478160858 + 1.0 * 6.297904014587402
Epoch 970, val loss: 1.2084484100341797
Epoch 980, training loss: 6.326608657836914 = 0.031180091202259064 + 1.0 * 6.29542875289917
Epoch 980, val loss: 1.2152162790298462
Epoch 990, training loss: 6.326420783996582 = 0.030152449384331703 + 1.0 * 6.296268463134766
Epoch 990, val loss: 1.22190523147583
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 10.550045013427734 = 1.9531795978546143 + 1.0 * 8.5968656539917
Epoch 0, val loss: 1.949029803276062
Epoch 10, training loss: 10.538818359375 = 1.9422023296356201 + 1.0 * 8.5966157913208
Epoch 10, val loss: 1.9383199214935303
Epoch 20, training loss: 10.52220630645752 = 1.9279625415802002 + 1.0 * 8.594244003295898
Epoch 20, val loss: 1.9240410327911377
Epoch 30, training loss: 10.480693817138672 = 1.9077410697937012 + 1.0 * 8.572953224182129
Epoch 30, val loss: 1.9034652709960938
Epoch 40, training loss: 10.323169708251953 = 1.8808259963989258 + 1.0 * 8.442343711853027
Epoch 40, val loss: 1.8770798444747925
Epoch 50, training loss: 9.861689567565918 = 1.8507245779037476 + 1.0 * 8.010965347290039
Epoch 50, val loss: 1.8492915630340576
Epoch 60, training loss: 9.58549976348877 = 1.823696494102478 + 1.0 * 7.76180362701416
Epoch 60, val loss: 1.8256409168243408
Epoch 70, training loss: 9.134855270385742 = 1.8030457496643066 + 1.0 * 7.331809043884277
Epoch 70, val loss: 1.8081837892532349
Epoch 80, training loss: 8.847501754760742 = 1.7862497568130493 + 1.0 * 7.061252117156982
Epoch 80, val loss: 1.7943446636199951
Epoch 90, training loss: 8.712757110595703 = 1.7675458192825317 + 1.0 * 6.945211410522461
Epoch 90, val loss: 1.7783973217010498
Epoch 100, training loss: 8.62016487121582 = 1.7467844486236572 + 1.0 * 6.873380661010742
Epoch 100, val loss: 1.7608188390731812
Epoch 110, training loss: 8.534467697143555 = 1.7264827489852905 + 1.0 * 6.807984828948975
Epoch 110, val loss: 1.7431354522705078
Epoch 120, training loss: 8.461585998535156 = 1.704164981842041 + 1.0 * 6.757421493530273
Epoch 120, val loss: 1.723282814025879
Epoch 130, training loss: 8.395862579345703 = 1.678800106048584 + 1.0 * 6.717061996459961
Epoch 130, val loss: 1.7011491060256958
Epoch 140, training loss: 8.324594497680664 = 1.6502705812454224 + 1.0 * 6.674324035644531
Epoch 140, val loss: 1.6768869161605835
Epoch 150, training loss: 8.254493713378906 = 1.6177986860275269 + 1.0 * 6.63669490814209
Epoch 150, val loss: 1.6497559547424316
Epoch 160, training loss: 8.184632301330566 = 1.5808221101760864 + 1.0 * 6.603809833526611
Epoch 160, val loss: 1.6188182830810547
Epoch 170, training loss: 8.112994194030762 = 1.5391212701797485 + 1.0 * 6.573873043060303
Epoch 170, val loss: 1.583847999572754
Epoch 180, training loss: 8.046788215637207 = 1.4934930801391602 + 1.0 * 6.553295135498047
Epoch 180, val loss: 1.5457285642623901
Epoch 190, training loss: 7.9767560958862305 = 1.4452321529388428 + 1.0 * 6.531523704528809
Epoch 190, val loss: 1.5056543350219727
Epoch 200, training loss: 7.909278392791748 = 1.394823670387268 + 1.0 * 6.5144548416137695
Epoch 200, val loss: 1.4640593528747559
Epoch 210, training loss: 7.842479228973389 = 1.3441094160079956 + 1.0 * 6.4983696937561035
Epoch 210, val loss: 1.4227861166000366
Epoch 220, training loss: 7.778579235076904 = 1.2937434911727905 + 1.0 * 6.484835624694824
Epoch 220, val loss: 1.3819013833999634
Epoch 230, training loss: 7.716423034667969 = 1.2436513900756836 + 1.0 * 6.472771644592285
Epoch 230, val loss: 1.3415021896362305
Epoch 240, training loss: 7.665596008300781 = 1.1943258047103882 + 1.0 * 6.4712700843811035
Epoch 240, val loss: 1.3019671440124512
Epoch 250, training loss: 7.599372386932373 = 1.1472997665405273 + 1.0 * 6.452072620391846
Epoch 250, val loss: 1.2645373344421387
Epoch 260, training loss: 7.546911716461182 = 1.1021318435668945 + 1.0 * 6.444779872894287
Epoch 260, val loss: 1.2287594079971313
Epoch 270, training loss: 7.4978814125061035 = 1.05863618850708 + 1.0 * 6.439245223999023
Epoch 270, val loss: 1.1947435140609741
Epoch 280, training loss: 7.450737953186035 = 1.017335057258606 + 1.0 * 6.433403015136719
Epoch 280, val loss: 1.162817358970642
Epoch 290, training loss: 7.402776718139648 = 0.9781562685966492 + 1.0 * 6.424620628356934
Epoch 290, val loss: 1.133082389831543
Epoch 300, training loss: 7.358058452606201 = 0.9402933120727539 + 1.0 * 6.417765140533447
Epoch 300, val loss: 1.1047292947769165
Epoch 310, training loss: 7.321152687072754 = 0.903106153011322 + 1.0 * 6.418046474456787
Epoch 310, val loss: 1.0770732164382935
Epoch 320, training loss: 7.274372100830078 = 0.8666919469833374 + 1.0 * 6.407680034637451
Epoch 320, val loss: 1.0500253438949585
Epoch 330, training loss: 7.233643531799316 = 0.8303524851799011 + 1.0 * 6.40329122543335
Epoch 330, val loss: 1.0232679843902588
Epoch 340, training loss: 7.192652702331543 = 0.7939621210098267 + 1.0 * 6.398690700531006
Epoch 340, val loss: 0.9964560270309448
Epoch 350, training loss: 7.152989387512207 = 0.7577669024467468 + 1.0 * 6.3952226638793945
Epoch 350, val loss: 0.9699225425720215
Epoch 360, training loss: 7.118351936340332 = 0.721866250038147 + 1.0 * 6.396485805511475
Epoch 360, val loss: 0.9438627362251282
Epoch 370, training loss: 7.075401306152344 = 0.6869150996208191 + 1.0 * 6.388486385345459
Epoch 370, val loss: 0.9187315702438354
Epoch 380, training loss: 7.03622579574585 = 0.652892529964447 + 1.0 * 6.383333206176758
Epoch 380, val loss: 0.894836962223053
Epoch 390, training loss: 7.007198333740234 = 0.6201134324073792 + 1.0 * 6.3870849609375
Epoch 390, val loss: 0.8723238110542297
Epoch 400, training loss: 6.967618942260742 = 0.5891165137290955 + 1.0 * 6.378502368927002
Epoch 400, val loss: 0.8517517447471619
Epoch 410, training loss: 6.9427008628845215 = 0.5596786737442017 + 1.0 * 6.383022308349609
Epoch 410, val loss: 0.8330898284912109
Epoch 420, training loss: 6.9021315574646 = 0.5319929122924805 + 1.0 * 6.370138645172119
Epoch 420, val loss: 0.8162220120429993
Epoch 430, training loss: 6.874396800994873 = 0.505806565284729 + 1.0 * 6.368590354919434
Epoch 430, val loss: 0.8012114763259888
Epoch 440, training loss: 6.845127105712891 = 0.48090988397598267 + 1.0 * 6.364217281341553
Epoch 440, val loss: 0.7878048419952393
Epoch 450, training loss: 6.819467067718506 = 0.4571208357810974 + 1.0 * 6.362346172332764
Epoch 450, val loss: 0.7757626175880432
Epoch 460, training loss: 6.794944763183594 = 0.4343034327030182 + 1.0 * 6.3606414794921875
Epoch 460, val loss: 0.7649598717689514
Epoch 470, training loss: 6.770341873168945 = 0.412265419960022 + 1.0 * 6.358076572418213
Epoch 470, val loss: 0.7552136778831482
Epoch 480, training loss: 6.743564605712891 = 0.3909764885902405 + 1.0 * 6.352588176727295
Epoch 480, val loss: 0.746319591999054
Epoch 490, training loss: 6.721121788024902 = 0.37030506134033203 + 1.0 * 6.35081672668457
Epoch 490, val loss: 0.7381823658943176
Epoch 500, training loss: 6.707917213439941 = 0.3501904010772705 + 1.0 * 6.35772705078125
Epoch 500, val loss: 0.7307291030883789
Epoch 510, training loss: 6.679801940917969 = 0.3308422863483429 + 1.0 * 6.348959445953369
Epoch 510, val loss: 0.7239585518836975
Epoch 520, training loss: 6.656124114990234 = 0.3121447265148163 + 1.0 * 6.343979358673096
Epoch 520, val loss: 0.7178491353988647
Epoch 530, training loss: 6.635941505432129 = 0.2940432131290436 + 1.0 * 6.341898441314697
Epoch 530, val loss: 0.7123451232910156
Epoch 540, training loss: 6.6252570152282715 = 0.27661436796188354 + 1.0 * 6.348642826080322
Epoch 540, val loss: 0.7074704170227051
Epoch 550, training loss: 6.597592830657959 = 0.26005837321281433 + 1.0 * 6.337534427642822
Epoch 550, val loss: 0.7032695412635803
Epoch 560, training loss: 6.583325386047363 = 0.24431434273719788 + 1.0 * 6.339011192321777
Epoch 560, val loss: 0.6997137069702148
Epoch 570, training loss: 6.567151069641113 = 0.22942058742046356 + 1.0 * 6.337730407714844
Epoch 570, val loss: 0.6968263983726501
Epoch 580, training loss: 6.548393249511719 = 0.21547114849090576 + 1.0 * 6.332921981811523
Epoch 580, val loss: 0.6946521401405334
Epoch 590, training loss: 6.535994529724121 = 0.2023739069700241 + 1.0 * 6.333620548248291
Epoch 590, val loss: 0.6931723952293396
Epoch 600, training loss: 6.521905899047852 = 0.1901261806488037 + 1.0 * 6.331779479980469
Epoch 600, val loss: 0.6923314332962036
Epoch 610, training loss: 6.507985591888428 = 0.17872564494609833 + 1.0 * 6.329259872436523
Epoch 610, val loss: 0.6921735405921936
Epoch 620, training loss: 6.495172023773193 = 0.1681244671344757 + 1.0 * 6.327047348022461
Epoch 620, val loss: 0.6925686597824097
Epoch 630, training loss: 6.485408306121826 = 0.1582375317811966 + 1.0 * 6.3271708488464355
Epoch 630, val loss: 0.6935828328132629
Epoch 640, training loss: 6.477710723876953 = 0.1490490585565567 + 1.0 * 6.3286614418029785
Epoch 640, val loss: 0.6951038241386414
Epoch 650, training loss: 6.464277267456055 = 0.14057599008083344 + 1.0 * 6.32370138168335
Epoch 650, val loss: 0.697021484375
Epoch 660, training loss: 6.4537811279296875 = 0.1326737254858017 + 1.0 * 6.321107387542725
Epoch 660, val loss: 0.6993821263313293
Epoch 670, training loss: 6.4481000900268555 = 0.12528669834136963 + 1.0 * 6.322813510894775
Epoch 670, val loss: 0.7021864056587219
Epoch 680, training loss: 6.439507961273193 = 0.11841955780982971 + 1.0 * 6.3210883140563965
Epoch 680, val loss: 0.7054023742675781
Epoch 690, training loss: 6.430437088012695 = 0.11203297227621078 + 1.0 * 6.318404197692871
Epoch 690, val loss: 0.7088377475738525
Epoch 700, training loss: 6.424118518829346 = 0.106064073741436 + 1.0 * 6.318054676055908
Epoch 700, val loss: 0.7126263976097107
Epoch 710, training loss: 6.416688442230225 = 0.10050030052661896 + 1.0 * 6.316188335418701
Epoch 710, val loss: 0.7166786789894104
Epoch 720, training loss: 6.412310600280762 = 0.09531164169311523 + 1.0 * 6.3169989585876465
Epoch 720, val loss: 0.7209138870239258
Epoch 730, training loss: 6.403748512268066 = 0.09046704322099686 + 1.0 * 6.313281536102295
Epoch 730, val loss: 0.7253579497337341
Epoch 740, training loss: 6.397030830383301 = 0.08593624830245972 + 1.0 * 6.311094760894775
Epoch 740, val loss: 0.729947566986084
Epoch 750, training loss: 6.393568515777588 = 0.08169253915548325 + 1.0 * 6.311875820159912
Epoch 750, val loss: 0.7347339391708374
Epoch 760, training loss: 6.3859052658081055 = 0.07771267741918564 + 1.0 * 6.308192729949951
Epoch 760, val loss: 0.7397017478942871
Epoch 770, training loss: 6.381399154663086 = 0.07399333268404007 + 1.0 * 6.307405948638916
Epoch 770, val loss: 0.744702160358429
Epoch 780, training loss: 6.382254123687744 = 0.07050006836652756 + 1.0 * 6.31175422668457
Epoch 780, val loss: 0.7498305439949036
Epoch 790, training loss: 6.373538494110107 = 0.06723424047231674 + 1.0 * 6.306304454803467
Epoch 790, val loss: 0.7550460696220398
Epoch 800, training loss: 6.375246524810791 = 0.06416888535022736 + 1.0 * 6.31107759475708
Epoch 800, val loss: 0.7602534294128418
Epoch 810, training loss: 6.366643905639648 = 0.06129372492432594 + 1.0 * 6.305350303649902
Epoch 810, val loss: 0.7655110955238342
Epoch 820, training loss: 6.362697601318359 = 0.058597221970558167 + 1.0 * 6.304100513458252
Epoch 820, val loss: 0.7708194255828857
Epoch 830, training loss: 6.3559088706970215 = 0.05605766549706459 + 1.0 * 6.299851417541504
Epoch 830, val loss: 0.7761624455451965
Epoch 840, training loss: 6.351902484893799 = 0.053661298006772995 + 1.0 * 6.298241138458252
Epoch 840, val loss: 0.7815583348274231
Epoch 850, training loss: 6.364376544952393 = 0.05140315741300583 + 1.0 * 6.312973499298096
Epoch 850, val loss: 0.7870146632194519
Epoch 860, training loss: 6.348230361938477 = 0.04927528277039528 + 1.0 * 6.298954963684082
Epoch 860, val loss: 0.7923785448074341
Epoch 870, training loss: 6.343389987945557 = 0.047279879450798035 + 1.0 * 6.296110153198242
Epoch 870, val loss: 0.7976359724998474
Epoch 880, training loss: 6.340824127197266 = 0.04538986459374428 + 1.0 * 6.295434474945068
Epoch 880, val loss: 0.8030247688293457
Epoch 890, training loss: 6.343996524810791 = 0.043600570410490036 + 1.0 * 6.300395965576172
Epoch 890, val loss: 0.8084824085235596
Epoch 900, training loss: 6.3451247215271 = 0.04191041365265846 + 1.0 * 6.303214073181152
Epoch 900, val loss: 0.8138254284858704
Epoch 910, training loss: 6.332836627960205 = 0.0403122752904892 + 1.0 * 6.292524337768555
Epoch 910, val loss: 0.8190726041793823
Epoch 920, training loss: 6.328919410705566 = 0.038803327828645706 + 1.0 * 6.290116310119629
Epoch 920, val loss: 0.8243116140365601
Epoch 930, training loss: 6.328309059143066 = 0.037367790937423706 + 1.0 * 6.29094123840332
Epoch 930, val loss: 0.8296106457710266
Epoch 940, training loss: 6.332409381866455 = 0.03600598871707916 + 1.0 * 6.296403408050537
Epoch 940, val loss: 0.8349446058273315
Epoch 950, training loss: 6.330700874328613 = 0.03471739590167999 + 1.0 * 6.29598331451416
Epoch 950, val loss: 0.8401680588722229
Epoch 960, training loss: 6.323184490203857 = 0.0335003100335598 + 1.0 * 6.289684295654297
Epoch 960, val loss: 0.8452107310295105
Epoch 970, training loss: 6.31861686706543 = 0.03234340250492096 + 1.0 * 6.28627347946167
Epoch 970, val loss: 0.8502688407897949
Epoch 980, training loss: 6.316582202911377 = 0.03123842179775238 + 1.0 * 6.285343647003174
Epoch 980, val loss: 0.8554337620735168
Epoch 990, training loss: 6.322413444519043 = 0.0301857590675354 + 1.0 * 6.292227745056152
Epoch 990, val loss: 0.8606077432632446
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8323668950975225
The final CL Acc:0.79506, 0.01062, The final GNN Acc:0.83641, 0.00287
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9548])
updated graph: torch.Size([2, 10624])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.545868873596191 = 1.949031949043274 + 1.0 * 8.596837043762207
Epoch 0, val loss: 1.9581750631332397
Epoch 10, training loss: 10.535810470581055 = 1.9392238855361938 + 1.0 * 8.596586227416992
Epoch 10, val loss: 1.9485821723937988
Epoch 20, training loss: 10.521211624145508 = 1.9267497062683105 + 1.0 * 8.594462394714355
Epoch 20, val loss: 1.9359012842178345
Epoch 30, training loss: 10.486063003540039 = 1.908923864364624 + 1.0 * 8.577138900756836
Epoch 30, val loss: 1.9176602363586426
Epoch 40, training loss: 10.370351791381836 = 1.8847357034683228 + 1.0 * 8.485615730285645
Epoch 40, val loss: 1.8937655687332153
Epoch 50, training loss: 9.979840278625488 = 1.8575589656829834 + 1.0 * 8.122281074523926
Epoch 50, val loss: 1.8678362369537354
Epoch 60, training loss: 9.7301025390625 = 1.831632375717163 + 1.0 * 7.898470401763916
Epoch 60, val loss: 1.8441892862319946
Epoch 70, training loss: 9.337270736694336 = 1.8111107349395752 + 1.0 * 7.52616024017334
Epoch 70, val loss: 1.8254550695419312
Epoch 80, training loss: 8.991799354553223 = 1.7979509830474854 + 1.0 * 7.193848133087158
Epoch 80, val loss: 1.811966896057129
Epoch 90, training loss: 8.8328857421875 = 1.785083293914795 + 1.0 * 7.047802448272705
Epoch 90, val loss: 1.7973275184631348
Epoch 100, training loss: 8.682659149169922 = 1.767189860343933 + 1.0 * 6.915469646453857
Epoch 100, val loss: 1.7798575162887573
Epoch 110, training loss: 8.58289623260498 = 1.7489782571792603 + 1.0 * 6.83391809463501
Epoch 110, val loss: 1.7623140811920166
Epoch 120, training loss: 8.503751754760742 = 1.7296613454818726 + 1.0 * 6.774090766906738
Epoch 120, val loss: 1.7437857389450073
Epoch 130, training loss: 8.434253692626953 = 1.7092194557189941 + 1.0 * 6.725034713745117
Epoch 130, val loss: 1.725079894065857
Epoch 140, training loss: 8.37045955657959 = 1.6874152421951294 + 1.0 * 6.683043956756592
Epoch 140, val loss: 1.7059813737869263
Epoch 150, training loss: 8.312899589538574 = 1.6620820760726929 + 1.0 * 6.650817394256592
Epoch 150, val loss: 1.6844764947891235
Epoch 160, training loss: 8.256796836853027 = 1.6324663162231445 + 1.0 * 6.624330520629883
Epoch 160, val loss: 1.6598275899887085
Epoch 170, training loss: 8.198932647705078 = 1.5990886688232422 + 1.0 * 6.599843502044678
Epoch 170, val loss: 1.6321847438812256
Epoch 180, training loss: 8.139680862426758 = 1.5621004104614258 + 1.0 * 6.577580451965332
Epoch 180, val loss: 1.601534128189087
Epoch 190, training loss: 8.07784652709961 = 1.5215927362442017 + 1.0 * 6.556253910064697
Epoch 190, val loss: 1.568085789680481
Epoch 200, training loss: 8.018152236938477 = 1.4788684844970703 + 1.0 * 6.539283275604248
Epoch 200, val loss: 1.533475637435913
Epoch 210, training loss: 7.956117153167725 = 1.4348491430282593 + 1.0 * 6.521267890930176
Epoch 210, val loss: 1.498506784439087
Epoch 220, training loss: 7.895508766174316 = 1.3894965648651123 + 1.0 * 6.506011962890625
Epoch 220, val loss: 1.4630279541015625
Epoch 230, training loss: 7.838771343231201 = 1.343459963798523 + 1.0 * 6.495311260223389
Epoch 230, val loss: 1.4278137683868408
Epoch 240, training loss: 7.783336639404297 = 1.2980213165283203 + 1.0 * 6.485315322875977
Epoch 240, val loss: 1.3936903476715088
Epoch 250, training loss: 7.726376533508301 = 1.2527878284454346 + 1.0 * 6.473588466644287
Epoch 250, val loss: 1.3603163957595825
Epoch 260, training loss: 7.673037528991699 = 1.2076389789581299 + 1.0 * 6.465398788452148
Epoch 260, val loss: 1.3275115489959717
Epoch 270, training loss: 7.62256383895874 = 1.1629782915115356 + 1.0 * 6.459585666656494
Epoch 270, val loss: 1.295472264289856
Epoch 280, training loss: 7.571428298950195 = 1.1191717386245728 + 1.0 * 6.452256679534912
Epoch 280, val loss: 1.2643376588821411
Epoch 290, training loss: 7.5209574699401855 = 1.0759053230285645 + 1.0 * 6.445052146911621
Epoch 290, val loss: 1.2336385250091553
Epoch 300, training loss: 7.475886344909668 = 1.0331599712371826 + 1.0 * 6.442726135253906
Epoch 300, val loss: 1.203102469444275
Epoch 310, training loss: 7.432185173034668 = 0.9914371371269226 + 1.0 * 6.44074821472168
Epoch 310, val loss: 1.173367977142334
Epoch 320, training loss: 7.3807692527771 = 0.951230525970459 + 1.0 * 6.429538726806641
Epoch 320, val loss: 1.1448547840118408
Epoch 330, training loss: 7.335970878601074 = 0.9119596481323242 + 1.0 * 6.42401123046875
Epoch 330, val loss: 1.1169638633728027
Epoch 340, training loss: 7.29278039932251 = 0.8735308051109314 + 1.0 * 6.419249534606934
Epoch 340, val loss: 1.0896676778793335
Epoch 350, training loss: 7.251035690307617 = 0.8362411260604858 + 1.0 * 6.414794445037842
Epoch 350, val loss: 1.0631968975067139
Epoch 360, training loss: 7.211509704589844 = 0.8002932667732239 + 1.0 * 6.4112162590026855
Epoch 360, val loss: 1.0380560159683228
Epoch 370, training loss: 7.172390937805176 = 0.7653019428253174 + 1.0 * 6.4070892333984375
Epoch 370, val loss: 1.0139867067337036
Epoch 380, training loss: 7.135383129119873 = 0.7313122153282166 + 1.0 * 6.404070854187012
Epoch 380, val loss: 0.9909471273422241
Epoch 390, training loss: 7.1004557609558105 = 0.6986531615257263 + 1.0 * 6.4018025398254395
Epoch 390, val loss: 0.9696211218833923
Epoch 400, training loss: 7.062996864318848 = 0.6669106483459473 + 1.0 * 6.3960862159729
Epoch 400, val loss: 0.9499161243438721
Epoch 410, training loss: 7.033359527587891 = 0.6359561085700989 + 1.0 * 6.397403240203857
Epoch 410, val loss: 0.9313368797302246
Epoch 420, training loss: 6.9975199699401855 = 0.6059747934341431 + 1.0 * 6.391545295715332
Epoch 420, val loss: 0.914790153503418
Epoch 430, training loss: 6.966874599456787 = 0.576738715171814 + 1.0 * 6.390135765075684
Epoch 430, val loss: 0.8999592065811157
Epoch 440, training loss: 6.934566974639893 = 0.5485711693763733 + 1.0 * 6.385995864868164
Epoch 440, val loss: 0.887075662612915
Epoch 450, training loss: 6.902128219604492 = 0.521221399307251 + 1.0 * 6.38090705871582
Epoch 450, val loss: 0.8760274052619934
Epoch 460, training loss: 6.879404067993164 = 0.49480026960372925 + 1.0 * 6.384603977203369
Epoch 460, val loss: 0.8669199347496033
Epoch 470, training loss: 6.846982955932617 = 0.46971166133880615 + 1.0 * 6.3772711753845215
Epoch 470, val loss: 0.8597617745399475
Epoch 480, training loss: 6.82206392288208 = 0.445893257856369 + 1.0 * 6.376170635223389
Epoch 480, val loss: 0.855089008808136
Epoch 490, training loss: 6.795456886291504 = 0.42320752143859863 + 1.0 * 6.372249126434326
Epoch 490, val loss: 0.8518570065498352
Epoch 500, training loss: 6.779281139373779 = 0.4016357362270355 + 1.0 * 6.377645492553711
Epoch 500, val loss: 0.8501836657524109
Epoch 510, training loss: 6.749490737915039 = 0.38139936327934265 + 1.0 * 6.368091583251953
Epoch 510, val loss: 0.8506011962890625
Epoch 520, training loss: 6.734202861785889 = 0.3621774911880493 + 1.0 * 6.372025489807129
Epoch 520, val loss: 0.8519570827484131
Epoch 530, training loss: 6.708332061767578 = 0.3440686762332916 + 1.0 * 6.364263534545898
Epoch 530, val loss: 0.8544242978096008
Epoch 540, training loss: 6.6878790855407715 = 0.3269044756889343 + 1.0 * 6.3609747886657715
Epoch 540, val loss: 0.8581318855285645
Epoch 550, training loss: 6.669719696044922 = 0.31053102016448975 + 1.0 * 6.359188556671143
Epoch 550, val loss: 0.8627312183380127
Epoch 560, training loss: 6.652163982391357 = 0.2948993444442749 + 1.0 * 6.357264518737793
Epoch 560, val loss: 0.8679473400115967
Epoch 570, training loss: 6.640651226043701 = 0.28000134229660034 + 1.0 * 6.360650062561035
Epoch 570, val loss: 0.8742190599441528
Epoch 580, training loss: 6.622483730316162 = 0.26584452390670776 + 1.0 * 6.356639385223389
Epoch 580, val loss: 0.8809918761253357
Epoch 590, training loss: 6.604095935821533 = 0.2522306740283966 + 1.0 * 6.351865291595459
Epoch 590, val loss: 0.8883423805236816
Epoch 600, training loss: 6.5885539054870605 = 0.23916490375995636 + 1.0 * 6.34938907623291
Epoch 600, val loss: 0.8961743116378784
Epoch 610, training loss: 6.589694976806641 = 0.2265968918800354 + 1.0 * 6.36309814453125
Epoch 610, val loss: 0.9044506549835205
Epoch 620, training loss: 6.5631489753723145 = 0.2145957052707672 + 1.0 * 6.34855318069458
Epoch 620, val loss: 0.9132444262504578
Epoch 630, training loss: 6.548257350921631 = 0.20311681926250458 + 1.0 * 6.34514045715332
Epoch 630, val loss: 0.9225912094116211
Epoch 640, training loss: 6.53927755355835 = 0.19212330877780914 + 1.0 * 6.347154140472412
Epoch 640, val loss: 0.931938111782074
Epoch 650, training loss: 6.53469705581665 = 0.1817271113395691 + 1.0 * 6.352970123291016
Epoch 650, val loss: 0.9418565630912781
Epoch 660, training loss: 6.5127081871032715 = 0.17189210653305054 + 1.0 * 6.340816020965576
Epoch 660, val loss: 0.9522837400436401
Epoch 670, training loss: 6.500821590423584 = 0.16255520284175873 + 1.0 * 6.338266372680664
Epoch 670, val loss: 0.9627292156219482
Epoch 680, training loss: 6.490572452545166 = 0.15369316935539246 + 1.0 * 6.336879253387451
Epoch 680, val loss: 0.9733171463012695
Epoch 690, training loss: 6.483092308044434 = 0.14528536796569824 + 1.0 * 6.337806701660156
Epoch 690, val loss: 0.9843144416809082
Epoch 700, training loss: 6.472630977630615 = 0.13738535344600677 + 1.0 * 6.335245609283447
Epoch 700, val loss: 0.9953901767730713
Epoch 710, training loss: 6.464771747589111 = 0.12994365394115448 + 1.0 * 6.334827899932861
Epoch 710, val loss: 1.0069937705993652
Epoch 720, training loss: 6.4558844566345215 = 0.12293663620948792 + 1.0 * 6.332947731018066
Epoch 720, val loss: 1.0184626579284668
Epoch 730, training loss: 6.448143005371094 = 0.11634529381990433 + 1.0 * 6.3317975997924805
Epoch 730, val loss: 1.0299290418624878
Epoch 740, training loss: 6.439602851867676 = 0.1101565733551979 + 1.0 * 6.329446315765381
Epoch 740, val loss: 1.0419750213623047
Epoch 750, training loss: 6.432521820068359 = 0.1043420135974884 + 1.0 * 6.328179836273193
Epoch 750, val loss: 1.0538371801376343
Epoch 760, training loss: 6.436769962310791 = 0.09888383746147156 + 1.0 * 6.337886333465576
Epoch 760, val loss: 1.0658624172210693
Epoch 770, training loss: 6.422666549682617 = 0.0937492772936821 + 1.0 * 6.328917503356934
Epoch 770, val loss: 1.0777148008346558
Epoch 780, training loss: 6.412561893463135 = 0.08895350247621536 + 1.0 * 6.3236083984375
Epoch 780, val loss: 1.0900778770446777
Epoch 790, training loss: 6.4093451499938965 = 0.08443703502416611 + 1.0 * 6.324908256530762
Epoch 790, val loss: 1.1021233797073364
Epoch 800, training loss: 6.405187129974365 = 0.08021673560142517 + 1.0 * 6.324970245361328
Epoch 800, val loss: 1.114087462425232
Epoch 810, training loss: 6.398521423339844 = 0.0762779638171196 + 1.0 * 6.322243690490723
Epoch 810, val loss: 1.126619577407837
Epoch 820, training loss: 6.392542839050293 = 0.07258208841085434 + 1.0 * 6.319960594177246
Epoch 820, val loss: 1.1387255191802979
Epoch 830, training loss: 6.390512943267822 = 0.06911277770996094 + 1.0 * 6.321400165557861
Epoch 830, val loss: 1.1507208347320557
Epoch 840, training loss: 6.383089065551758 = 0.06585521996021271 + 1.0 * 6.317234039306641
Epoch 840, val loss: 1.1626317501068115
Epoch 850, training loss: 6.379472732543945 = 0.06280152499675751 + 1.0 * 6.316671371459961
Epoch 850, val loss: 1.174817442893982
Epoch 860, training loss: 6.385226726531982 = 0.05994349718093872 + 1.0 * 6.325283050537109
Epoch 860, val loss: 1.1864502429962158
Epoch 870, training loss: 6.373704433441162 = 0.05726666748523712 + 1.0 * 6.316437721252441
Epoch 870, val loss: 1.1983146667480469
Epoch 880, training loss: 6.367590427398682 = 0.054755743592977524 + 1.0 * 6.312834739685059
Epoch 880, val loss: 1.2102130651474
Epoch 890, training loss: 6.3645172119140625 = 0.05238592252135277 + 1.0 * 6.312131404876709
Epoch 890, val loss: 1.221554160118103
Epoch 900, training loss: 6.369734287261963 = 0.050151679664850235 + 1.0 * 6.319582462310791
Epoch 900, val loss: 1.232990026473999
Epoch 910, training loss: 6.362020492553711 = 0.04804811626672745 + 1.0 * 6.313972473144531
Epoch 910, val loss: 1.2443228960037231
Epoch 920, training loss: 6.366072654724121 = 0.04607492685317993 + 1.0 * 6.319997787475586
Epoch 920, val loss: 1.2558575868606567
Epoch 930, training loss: 6.355706691741943 = 0.044208623468875885 + 1.0 * 6.311498165130615
Epoch 930, val loss: 1.2665363550186157
Epoch 940, training loss: 6.350586891174316 = 0.042461857199668884 + 1.0 * 6.308125019073486
Epoch 940, val loss: 1.2779269218444824
Epoch 950, training loss: 6.347036838531494 = 0.04079878330230713 + 1.0 * 6.306238174438477
Epoch 950, val loss: 1.2885078191757202
Epoch 960, training loss: 6.344491004943848 = 0.03922368213534355 + 1.0 * 6.305267333984375
Epoch 960, val loss: 1.2991435527801514
Epoch 970, training loss: 6.356298446655273 = 0.03773036599159241 + 1.0 * 6.318568229675293
Epoch 970, val loss: 1.3095626831054688
Epoch 980, training loss: 6.343405246734619 = 0.03632792457938194 + 1.0 * 6.307077407836914
Epoch 980, val loss: 1.3197214603424072
Epoch 990, training loss: 6.340743064880371 = 0.03500425070524216 + 1.0 * 6.305738925933838
Epoch 990, val loss: 1.330488681793213
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 10.535350799560547 = 1.9384984970092773 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.9399988651275635
Epoch 10, training loss: 10.525737762451172 = 1.9290908575057983 + 1.0 * 8.596647262573242
Epoch 10, val loss: 1.9308110475540161
Epoch 20, training loss: 10.512578010559082 = 1.9178060293197632 + 1.0 * 8.594772338867188
Epoch 20, val loss: 1.919328212738037
Epoch 30, training loss: 10.480509757995605 = 1.9023629426956177 + 1.0 * 8.578146934509277
Epoch 30, val loss: 1.9032552242279053
Epoch 40, training loss: 10.36055850982666 = 1.881914734840393 + 1.0 * 8.478643417358398
Epoch 40, val loss: 1.8824858665466309
Epoch 50, training loss: 9.984822273254395 = 1.8593257665634155 + 1.0 * 8.125496864318848
Epoch 50, val loss: 1.8602745532989502
Epoch 60, training loss: 9.753352165222168 = 1.8375084400177002 + 1.0 * 7.915843963623047
Epoch 60, val loss: 1.840080738067627
Epoch 70, training loss: 9.390161514282227 = 1.8199771642684937 + 1.0 * 7.570184707641602
Epoch 70, val loss: 1.8246376514434814
Epoch 80, training loss: 8.988100051879883 = 1.8090980052947998 + 1.0 * 7.179002285003662
Epoch 80, val loss: 1.8154581785202026
Epoch 90, training loss: 8.805465698242188 = 1.7985706329345703 + 1.0 * 7.006895065307617
Epoch 90, val loss: 1.8063749074935913
Epoch 100, training loss: 8.636058807373047 = 1.7856876850128174 + 1.0 * 6.85037088394165
Epoch 100, val loss: 1.7951583862304688
Epoch 110, training loss: 8.533702850341797 = 1.7742077112197876 + 1.0 * 6.759495258331299
Epoch 110, val loss: 1.7845977544784546
Epoch 120, training loss: 8.455201148986816 = 1.7630306482315063 + 1.0 * 6.6921706199646
Epoch 120, val loss: 1.7742962837219238
Epoch 130, training loss: 8.397297859191895 = 1.7504160404205322 + 1.0 * 6.646881580352783
Epoch 130, val loss: 1.763441562652588
Epoch 140, training loss: 8.350225448608398 = 1.7357195615768433 + 1.0 * 6.614505767822266
Epoch 140, val loss: 1.7515403032302856
Epoch 150, training loss: 8.305791854858398 = 1.718843698501587 + 1.0 * 6.586948394775391
Epoch 150, val loss: 1.7382371425628662
Epoch 160, training loss: 8.262887001037598 = 1.699302077293396 + 1.0 * 6.563584804534912
Epoch 160, val loss: 1.7230316400527954
Epoch 170, training loss: 8.221153259277344 = 1.676358699798584 + 1.0 * 6.544795036315918
Epoch 170, val loss: 1.7048450708389282
Epoch 180, training loss: 8.17944049835205 = 1.64963698387146 + 1.0 * 6.529803276062012
Epoch 180, val loss: 1.6835565567016602
Epoch 190, training loss: 8.134217262268066 = 1.618826985359192 + 1.0 * 6.515389919281006
Epoch 190, val loss: 1.6590135097503662
Epoch 200, training loss: 8.086336135864258 = 1.5832446813583374 + 1.0 * 6.503091812133789
Epoch 200, val loss: 1.6305769681930542
Epoch 210, training loss: 8.03572940826416 = 1.5428965091705322 + 1.0 * 6.492832660675049
Epoch 210, val loss: 1.598323106765747
Epoch 220, training loss: 7.979546546936035 = 1.4983913898468018 + 1.0 * 6.4811553955078125
Epoch 220, val loss: 1.5626380443572998
Epoch 230, training loss: 7.921370029449463 = 1.449902892112732 + 1.0 * 6.471467018127441
Epoch 230, val loss: 1.5239671468734741
Epoch 240, training loss: 7.865200519561768 = 1.3984808921813965 + 1.0 * 6.466719627380371
Epoch 240, val loss: 1.483341932296753
Epoch 250, training loss: 7.801705837249756 = 1.3460060358047485 + 1.0 * 6.455699920654297
Epoch 250, val loss: 1.4424009323120117
Epoch 260, training loss: 7.741345405578613 = 1.2928234338760376 + 1.0 * 6.448522090911865
Epoch 260, val loss: 1.401245355606079
Epoch 270, training loss: 7.6810431480407715 = 1.2393068075180054 + 1.0 * 6.441736221313477
Epoch 270, val loss: 1.3604055643081665
Epoch 280, training loss: 7.6292338371276855 = 1.1860872507095337 + 1.0 * 6.443146705627441
Epoch 280, val loss: 1.3204175233840942
Epoch 290, training loss: 7.569125175476074 = 1.134800672531128 + 1.0 * 6.434324741363525
Epoch 290, val loss: 1.282598614692688
Epoch 300, training loss: 7.5115437507629395 = 1.0854201316833496 + 1.0 * 6.42612361907959
Epoch 300, val loss: 1.2467812299728394
Epoch 310, training loss: 7.460378646850586 = 1.0374219417572021 + 1.0 * 6.422956943511963
Epoch 310, val loss: 1.212686538696289
Epoch 320, training loss: 7.4109930992126465 = 0.9912603497505188 + 1.0 * 6.419732570648193
Epoch 320, val loss: 1.1803759336471558
Epoch 330, training loss: 7.359978199005127 = 0.9466333985328674 + 1.0 * 6.413344860076904
Epoch 330, val loss: 1.14979088306427
Epoch 340, training loss: 7.311346054077148 = 0.9030866622924805 + 1.0 * 6.408259391784668
Epoch 340, val loss: 1.120328664779663
Epoch 350, training loss: 7.269723892211914 = 0.8608142733573914 + 1.0 * 6.408909797668457
Epoch 350, val loss: 1.0921374559402466
Epoch 360, training loss: 7.226242542266846 = 0.820473313331604 + 1.0 * 6.405769348144531
Epoch 360, val loss: 1.0656527280807495
Epoch 370, training loss: 7.181530952453613 = 0.7813379764556885 + 1.0 * 6.400192737579346
Epoch 370, val loss: 1.0404481887817383
Epoch 380, training loss: 7.1383819580078125 = 0.7431122660636902 + 1.0 * 6.395269870758057
Epoch 380, val loss: 1.016196370124817
Epoch 390, training loss: 7.099926948547363 = 0.7057902812957764 + 1.0 * 6.394136428833008
Epoch 390, val loss: 0.9931923151016235
Epoch 400, training loss: 7.0665602684021 = 0.6699448823928833 + 1.0 * 6.396615505218506
Epoch 400, val loss: 0.9716230630874634
Epoch 410, training loss: 7.023571014404297 = 0.6358022689819336 + 1.0 * 6.387768745422363
Epoch 410, val loss: 0.9520745277404785
Epoch 420, training loss: 6.990736961364746 = 0.6029689908027649 + 1.0 * 6.387767791748047
Epoch 420, val loss: 0.934269368648529
Epoch 430, training loss: 6.955168724060059 = 0.5714737772941589 + 1.0 * 6.383695125579834
Epoch 430, val loss: 0.9178966879844666
Epoch 440, training loss: 6.923343181610107 = 0.5414020419120789 + 1.0 * 6.381941318511963
Epoch 440, val loss: 0.9036020040512085
Epoch 450, training loss: 6.891060829162598 = 0.5124649405479431 + 1.0 * 6.37859582901001
Epoch 450, val loss: 0.890681266784668
Epoch 460, training loss: 6.866178035736084 = 0.48480212688446045 + 1.0 * 6.381375789642334
Epoch 460, val loss: 0.8794382810592651
Epoch 470, training loss: 6.833752155303955 = 0.4585017263889313 + 1.0 * 6.375250339508057
Epoch 470, val loss: 0.8697797060012817
Epoch 480, training loss: 6.808554172515869 = 0.4334466755390167 + 1.0 * 6.375107288360596
Epoch 480, val loss: 0.8617281317710876
Epoch 490, training loss: 6.78400182723999 = 0.4095449149608612 + 1.0 * 6.374456882476807
Epoch 490, val loss: 0.8548367023468018
Epoch 500, training loss: 6.755806922912598 = 0.3868463337421417 + 1.0 * 6.368960380554199
Epoch 500, val loss: 0.8493159413337708
Epoch 510, training loss: 6.729321002960205 = 0.365163654088974 + 1.0 * 6.364157199859619
Epoch 510, val loss: 0.8449686169624329
Epoch 520, training loss: 6.710244655609131 = 0.3443590998649597 + 1.0 * 6.3658857345581055
Epoch 520, val loss: 0.8414087891578674
Epoch 530, training loss: 6.695899963378906 = 0.3247162401676178 + 1.0 * 6.3711838722229
Epoch 530, val loss: 0.838575005531311
Epoch 540, training loss: 6.6662821769714355 = 0.3061782121658325 + 1.0 * 6.360104084014893
Epoch 540, val loss: 0.837315559387207
Epoch 550, training loss: 6.645115375518799 = 0.28856581449508667 + 1.0 * 6.3565497398376465
Epoch 550, val loss: 0.8363104462623596
Epoch 560, training loss: 6.62571382522583 = 0.27181464433670044 + 1.0 * 6.353899002075195
Epoch 560, val loss: 0.8361290693283081
Epoch 570, training loss: 6.607936382293701 = 0.25594016909599304 + 1.0 * 6.351996421813965
Epoch 570, val loss: 0.836811900138855
Epoch 580, training loss: 6.6091766357421875 = 0.24095213413238525 + 1.0 * 6.368224620819092
Epoch 580, val loss: 0.8379305005073547
Epoch 590, training loss: 6.578204154968262 = 0.22698938846588135 + 1.0 * 6.35121488571167
Epoch 590, val loss: 0.839777946472168
Epoch 600, training loss: 6.562293529510498 = 0.213954359292984 + 1.0 * 6.348339080810547
Epoch 600, val loss: 0.842545747756958
Epoch 610, training loss: 6.558868885040283 = 0.2017335444688797 + 1.0 * 6.35713529586792
Epoch 610, val loss: 0.8453414440155029
Epoch 620, training loss: 6.540621757507324 = 0.19040119647979736 + 1.0 * 6.350220680236816
Epoch 620, val loss: 0.8488620519638062
Epoch 630, training loss: 6.525148868560791 = 0.17984944581985474 + 1.0 * 6.345299243927002
Epoch 630, val loss: 0.8531786799430847
Epoch 640, training loss: 6.511125087738037 = 0.1699780374765396 + 1.0 * 6.341146945953369
Epoch 640, val loss: 0.8574777245521545
Epoch 650, training loss: 6.50089693069458 = 0.16072909533977509 + 1.0 * 6.340167999267578
Epoch 650, val loss: 0.8622728586196899
Epoch 660, training loss: 6.503073215484619 = 0.15209050476551056 + 1.0 * 6.350982666015625
Epoch 660, val loss: 0.8673083782196045
Epoch 670, training loss: 6.48757791519165 = 0.14407800137996674 + 1.0 * 6.343500137329102
Epoch 670, val loss: 0.8728317618370056
Epoch 680, training loss: 6.474086284637451 = 0.13661998510360718 + 1.0 * 6.337466239929199
Epoch 680, val loss: 0.8787291049957275
Epoch 690, training loss: 6.463913917541504 = 0.12962029874324799 + 1.0 * 6.334293842315674
Epoch 690, val loss: 0.8845279216766357
Epoch 700, training loss: 6.455967903137207 = 0.12304951250553131 + 1.0 * 6.332918167114258
Epoch 700, val loss: 0.8908395171165466
Epoch 710, training loss: 6.457245349884033 = 0.11687393486499786 + 1.0 * 6.340371608734131
Epoch 710, val loss: 0.8972885608673096
Epoch 720, training loss: 6.446069240570068 = 0.11110841482877731 + 1.0 * 6.3349609375
Epoch 720, val loss: 0.9039052724838257
Epoch 730, training loss: 6.4363532066345215 = 0.10570751130580902 + 1.0 * 6.330645561218262
Epoch 730, val loss: 0.9109169244766235
Epoch 740, training loss: 6.43468713760376 = 0.10064487904310226 + 1.0 * 6.334042072296143
Epoch 740, val loss: 0.9174708127975464
Epoch 750, training loss: 6.424204349517822 = 0.09591318666934967 + 1.0 * 6.328290939331055
Epoch 750, val loss: 0.9246578216552734
Epoch 760, training loss: 6.417586803436279 = 0.09147240221500397 + 1.0 * 6.326114177703857
Epoch 760, val loss: 0.9319278597831726
Epoch 770, training loss: 6.411465167999268 = 0.08727822452783585 + 1.0 * 6.3241868019104
Epoch 770, val loss: 0.9389801025390625
Epoch 780, training loss: 6.40992546081543 = 0.08331989496946335 + 1.0 * 6.326605796813965
Epoch 780, val loss: 0.9464287757873535
Epoch 790, training loss: 6.408573627471924 = 0.07958872616291046 + 1.0 * 6.32898473739624
Epoch 790, val loss: 0.9533284306526184
Epoch 800, training loss: 6.397225856781006 = 0.07609521597623825 + 1.0 * 6.321130752563477
Epoch 800, val loss: 0.9609402418136597
Epoch 810, training loss: 6.393362045288086 = 0.07280156016349792 + 1.0 * 6.320560455322266
Epoch 810, val loss: 0.9683949947357178
Epoch 820, training loss: 6.389179706573486 = 0.0696823000907898 + 1.0 * 6.319497585296631
Epoch 820, val loss: 0.975662350654602
Epoch 830, training loss: 6.3922119140625 = 0.06673373281955719 + 1.0 * 6.3254780769348145
Epoch 830, val loss: 0.9828648567199707
Epoch 840, training loss: 6.380367279052734 = 0.06395545601844788 + 1.0 * 6.316411972045898
Epoch 840, val loss: 0.9902629852294922
Epoch 850, training loss: 6.378378868103027 = 0.061338089406490326 + 1.0 * 6.317040920257568
Epoch 850, val loss: 0.9977671504020691
Epoch 860, training loss: 6.373503684997559 = 0.0588558129966259 + 1.0 * 6.314647674560547
Epoch 860, val loss: 1.0049500465393066
Epoch 870, training loss: 6.381616115570068 = 0.05649905279278755 + 1.0 * 6.325117111206055
Epoch 870, val loss: 1.0120006799697876
Epoch 880, training loss: 6.372650146484375 = 0.054277386516332626 + 1.0 * 6.31837272644043
Epoch 880, val loss: 1.0194580554962158
Epoch 890, training loss: 6.365612030029297 = 0.052175089716911316 + 1.0 * 6.313436985015869
Epoch 890, val loss: 1.0268040895462036
Epoch 900, training loss: 6.363923072814941 = 0.050183240324258804 + 1.0 * 6.313739776611328
Epoch 900, val loss: 1.0333950519561768
Epoch 910, training loss: 6.358293533325195 = 0.048305075615644455 + 1.0 * 6.309988498687744
Epoch 910, val loss: 1.0408291816711426
Epoch 920, training loss: 6.355836391448975 = 0.04651931673288345 + 1.0 * 6.309317111968994
Epoch 920, val loss: 1.047985315322876
Epoch 930, training loss: 6.3528265953063965 = 0.04481920599937439 + 1.0 * 6.30800724029541
Epoch 930, val loss: 1.054854154586792
Epoch 940, training loss: 6.357795715332031 = 0.04319905862212181 + 1.0 * 6.314596652984619
Epoch 940, val loss: 1.061706304550171
Epoch 950, training loss: 6.350702285766602 = 0.04166318476200104 + 1.0 * 6.309039115905762
Epoch 950, val loss: 1.0684553384780884
Epoch 960, training loss: 6.346391677856445 = 0.040204163640737534 + 1.0 * 6.306187629699707
Epoch 960, val loss: 1.075418472290039
Epoch 970, training loss: 6.3501129150390625 = 0.03881600871682167 + 1.0 * 6.3112969398498535
Epoch 970, val loss: 1.0819305181503296
Epoch 980, training loss: 6.342070579528809 = 0.03749978169798851 + 1.0 * 6.30457067489624
Epoch 980, val loss: 1.0886003971099854
Epoch 990, training loss: 6.340274333953857 = 0.03624497354030609 + 1.0 * 6.30402946472168
Epoch 990, val loss: 1.0954746007919312
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 10.557051658630371 = 1.960199236869812 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.968932867050171
Epoch 10, training loss: 10.546709060668945 = 1.9501121044158936 + 1.0 * 8.596596717834473
Epoch 10, val loss: 1.9580423831939697
Epoch 20, training loss: 10.531689643859863 = 1.9374620914459229 + 1.0 * 8.59422779083252
Epoch 20, val loss: 1.9440796375274658
Epoch 30, training loss: 10.49551010131836 = 1.9196486473083496 + 1.0 * 8.575861930847168
Epoch 30, val loss: 1.9242881536483765
Epoch 40, training loss: 10.377532005310059 = 1.8962332010269165 + 1.0 * 8.481298446655273
Epoch 40, val loss: 1.8996927738189697
Epoch 50, training loss: 10.034979820251465 = 1.8716627359390259 + 1.0 * 8.16331672668457
Epoch 50, val loss: 1.8756855726242065
Epoch 60, training loss: 9.832948684692383 = 1.8486645221710205 + 1.0 * 7.984283924102783
Epoch 60, val loss: 1.8539155721664429
Epoch 70, training loss: 9.388818740844727 = 1.830435872077942 + 1.0 * 7.558382987976074
Epoch 70, val loss: 1.8374305963516235
Epoch 80, training loss: 9.045392036437988 = 1.8154762983322144 + 1.0 * 7.229916095733643
Epoch 80, val loss: 1.8229862451553345
Epoch 90, training loss: 8.760449409484863 = 1.8001846075057983 + 1.0 * 6.960265159606934
Epoch 90, val loss: 1.8079010248184204
Epoch 100, training loss: 8.606315612792969 = 1.785226821899414 + 1.0 * 6.8210883140563965
Epoch 100, val loss: 1.7934191226959229
Epoch 110, training loss: 8.510916709899902 = 1.770871877670288 + 1.0 * 6.740045070648193
Epoch 110, val loss: 1.7795870304107666
Epoch 120, training loss: 8.44556999206543 = 1.7564396858215332 + 1.0 * 6.689129829406738
Epoch 120, val loss: 1.7655730247497559
Epoch 130, training loss: 8.388425827026367 = 1.7410962581634521 + 1.0 * 6.647329330444336
Epoch 130, val loss: 1.7509816884994507
Epoch 140, training loss: 8.341178894042969 = 1.724192500114441 + 1.0 * 6.6169867515563965
Epoch 140, val loss: 1.7355055809020996
Epoch 150, training loss: 8.29262638092041 = 1.7053645849227905 + 1.0 * 6.587262153625488
Epoch 150, val loss: 1.7188870906829834
Epoch 160, training loss: 8.249778747558594 = 1.6838502883911133 + 1.0 * 6.5659284591674805
Epoch 160, val loss: 1.700287103652954
Epoch 170, training loss: 8.204452514648438 = 1.6596349477767944 + 1.0 * 6.544817924499512
Epoch 170, val loss: 1.6794800758361816
Epoch 180, training loss: 8.15621566772461 = 1.6323225498199463 + 1.0 * 6.523893356323242
Epoch 180, val loss: 1.6559995412826538
Epoch 190, training loss: 8.108877182006836 = 1.6014586687088013 + 1.0 * 6.507418632507324
Epoch 190, val loss: 1.6295124292373657
Epoch 200, training loss: 8.058801651000977 = 1.5665028095245361 + 1.0 * 6.492298603057861
Epoch 200, val loss: 1.599850058555603
Epoch 210, training loss: 8.01007080078125 = 1.5277397632598877 + 1.0 * 6.482330799102783
Epoch 210, val loss: 1.5674262046813965
Epoch 220, training loss: 7.955749034881592 = 1.4860140085220337 + 1.0 * 6.469735145568848
Epoch 220, val loss: 1.533017873764038
Epoch 230, training loss: 7.901873588562012 = 1.4412710666656494 + 1.0 * 6.460602760314941
Epoch 230, val loss: 1.496846079826355
Epoch 240, training loss: 7.848528861999512 = 1.3937175273895264 + 1.0 * 6.454811096191406
Epoch 240, val loss: 1.4591938257217407
Epoch 250, training loss: 7.790505886077881 = 1.3446696996688843 + 1.0 * 6.445836067199707
Epoch 250, val loss: 1.4213100671768188
Epoch 260, training loss: 7.735538482666016 = 1.2946609258651733 + 1.0 * 6.440877437591553
Epoch 260, val loss: 1.3833692073822021
Epoch 270, training loss: 7.67741584777832 = 1.2435195446014404 + 1.0 * 6.433896541595459
Epoch 270, val loss: 1.3452410697937012
Epoch 280, training loss: 7.619692325592041 = 1.191385269165039 + 1.0 * 6.428307056427002
Epoch 280, val loss: 1.3067631721496582
Epoch 290, training loss: 7.563568115234375 = 1.1390119791030884 + 1.0 * 6.424556255340576
Epoch 290, val loss: 1.2685010433197021
Epoch 300, training loss: 7.506990432739258 = 1.0875154733657837 + 1.0 * 6.419475078582764
Epoch 300, val loss: 1.2310150861740112
Epoch 310, training loss: 7.451248645782471 = 1.0365992784500122 + 1.0 * 6.414649486541748
Epoch 310, val loss: 1.1938296556472778
Epoch 320, training loss: 7.402627468109131 = 0.9864751100540161 + 1.0 * 6.416152477264404
Epoch 320, val loss: 1.157121181488037
Epoch 330, training loss: 7.346555233001709 = 0.9383118748664856 + 1.0 * 6.408243179321289
Epoch 330, val loss: 1.1217069625854492
Epoch 340, training loss: 7.29595422744751 = 0.8922710418701172 + 1.0 * 6.403683185577393
Epoch 340, val loss: 1.087969183921814
Epoch 350, training loss: 7.24668550491333 = 0.8480983972549438 + 1.0 * 6.398587226867676
Epoch 350, val loss: 1.05582594871521
Epoch 360, training loss: 7.207493305206299 = 0.8059317469596863 + 1.0 * 6.401561737060547
Epoch 360, val loss: 1.0254757404327393
Epoch 370, training loss: 7.162644386291504 = 0.7666900157928467 + 1.0 * 6.395954132080078
Epoch 370, val loss: 0.9978798031806946
Epoch 380, training loss: 7.119748592376709 = 0.7303716540336609 + 1.0 * 6.389377117156982
Epoch 380, val loss: 0.9731788039207458
Epoch 390, training loss: 7.082980632781982 = 0.6962155103683472 + 1.0 * 6.386765003204346
Epoch 390, val loss: 0.9506282210350037
Epoch 400, training loss: 7.04702615737915 = 0.663905680179596 + 1.0 * 6.383120536804199
Epoch 400, val loss: 0.9301882386207581
Epoch 410, training loss: 7.016127109527588 = 0.6332358717918396 + 1.0 * 6.3828911781311035
Epoch 410, val loss: 0.911760687828064
Epoch 420, training loss: 6.9889373779296875 = 0.6044358611106873 + 1.0 * 6.3845014572143555
Epoch 420, val loss: 0.8951869606971741
Epoch 430, training loss: 6.95343542098999 = 0.5772704482078552 + 1.0 * 6.37616491317749
Epoch 430, val loss: 0.8805193305015564
Epoch 440, training loss: 6.923384666442871 = 0.5513259768486023 + 1.0 * 6.372058868408203
Epoch 440, val loss: 0.8672468662261963
Epoch 450, training loss: 6.896117687225342 = 0.5263381004333496 + 1.0 * 6.369779586791992
Epoch 450, val loss: 0.8551650643348694
Epoch 460, training loss: 6.878599643707275 = 0.502266526222229 + 1.0 * 6.376333236694336
Epoch 460, val loss: 0.8441182971000671
Epoch 470, training loss: 6.846233367919922 = 0.47914543747901917 + 1.0 * 6.3670878410339355
Epoch 470, val loss: 0.8341740965843201
Epoch 480, training loss: 6.8187255859375 = 0.4568064510822296 + 1.0 * 6.361918926239014
Epoch 480, val loss: 0.8251259326934814
Epoch 490, training loss: 6.7959699630737305 = 0.43502023816108704 + 1.0 * 6.360949516296387
Epoch 490, val loss: 0.8168143033981323
Epoch 500, training loss: 6.774666786193848 = 0.4138438105583191 + 1.0 * 6.360823154449463
Epoch 500, val loss: 0.8092489242553711
Epoch 510, training loss: 6.750248432159424 = 0.39335763454437256 + 1.0 * 6.356890678405762
Epoch 510, val loss: 0.8025354743003845
Epoch 520, training loss: 6.727668285369873 = 0.3734573423862457 + 1.0 * 6.35421085357666
Epoch 520, val loss: 0.7966299653053284
Epoch 530, training loss: 6.7102766036987305 = 0.35411199927330017 + 1.0 * 6.356164455413818
Epoch 530, val loss: 0.791502058506012
Epoch 540, training loss: 6.690561771392822 = 0.33545544743537903 + 1.0 * 6.355106353759766
Epoch 540, val loss: 0.7871957421302795
Epoch 550, training loss: 6.665690898895264 = 0.3175729513168335 + 1.0 * 6.348117828369141
Epoch 550, val loss: 0.7838897705078125
Epoch 560, training loss: 6.646472930908203 = 0.300354540348053 + 1.0 * 6.346118450164795
Epoch 560, val loss: 0.7814118266105652
Epoch 570, training loss: 6.640679836273193 = 0.28382083773612976 + 1.0 * 6.35685920715332
Epoch 570, val loss: 0.7797281742095947
Epoch 580, training loss: 6.613155364990234 = 0.26803189516067505 + 1.0 * 6.345123291015625
Epoch 580, val loss: 0.7789629101753235
Epoch 590, training loss: 6.594405651092529 = 0.2530289590358734 + 1.0 * 6.341376781463623
Epoch 590, val loss: 0.7790152430534363
Epoch 600, training loss: 6.581751346588135 = 0.23870722949504852 + 1.0 * 6.343044281005859
Epoch 600, val loss: 0.7798336744308472
Epoch 610, training loss: 6.563546657562256 = 0.22514116764068604 + 1.0 * 6.338405609130859
Epoch 610, val loss: 0.7814264297485352
Epoch 620, training loss: 6.55531120300293 = 0.21231986582279205 + 1.0 * 6.342991352081299
Epoch 620, val loss: 0.7838158011436462
Epoch 630, training loss: 6.535818099975586 = 0.20025508105754852 + 1.0 * 6.3355631828308105
Epoch 630, val loss: 0.7868221402168274
Epoch 640, training loss: 6.521908760070801 = 0.18888814747333527 + 1.0 * 6.3330206871032715
Epoch 640, val loss: 0.7904017567634583
Epoch 650, training loss: 6.512048721313477 = 0.17823028564453125 + 1.0 * 6.333818435668945
Epoch 650, val loss: 0.7947065830230713
Epoch 660, training loss: 6.498392581939697 = 0.16815128922462463 + 1.0 * 6.3302412033081055
Epoch 660, val loss: 0.7995073199272156
Epoch 670, training loss: 6.501318454742432 = 0.15866418182849884 + 1.0 * 6.342654228210449
Epoch 670, val loss: 0.8047680854797363
Epoch 680, training loss: 6.480962753295898 = 0.14976708590984344 + 1.0 * 6.331195831298828
Epoch 680, val loss: 0.8105130791664124
Epoch 690, training loss: 6.466709136962891 = 0.14144417643547058 + 1.0 * 6.325264930725098
Epoch 690, val loss: 0.816720724105835
Epoch 700, training loss: 6.463810920715332 = 0.13361187279224396 + 1.0 * 6.330199241638184
Epoch 700, val loss: 0.8232547640800476
Epoch 710, training loss: 6.454309940338135 = 0.12631013989448547 + 1.0 * 6.327999591827393
Epoch 710, val loss: 0.830078661441803
Epoch 720, training loss: 6.443302154541016 = 0.11947748810052872 + 1.0 * 6.323824882507324
Epoch 720, val loss: 0.8373124599456787
Epoch 730, training loss: 6.442927837371826 = 0.11306045949459076 + 1.0 * 6.329867362976074
Epoch 730, val loss: 0.8447274565696716
Epoch 740, training loss: 6.4287919998168945 = 0.10709279775619507 + 1.0 * 6.321699142456055
Epoch 740, val loss: 0.8523731827735901
Epoch 750, training loss: 6.420759201049805 = 0.10147783905267715 + 1.0 * 6.319281578063965
Epoch 750, val loss: 0.8602759838104248
Epoch 760, training loss: 6.422682285308838 = 0.09622173011302948 + 1.0 * 6.326460361480713
Epoch 760, val loss: 0.8683342933654785
Epoch 770, training loss: 6.410248756408691 = 0.09129214286804199 + 1.0 * 6.31895637512207
Epoch 770, val loss: 0.8764838576316833
Epoch 780, training loss: 6.402504920959473 = 0.0866938903927803 + 1.0 * 6.3158111572265625
Epoch 780, val loss: 0.8848286867141724
Epoch 790, training loss: 6.4000678062438965 = 0.08238113671541214 + 1.0 * 6.317686557769775
Epoch 790, val loss: 0.8932026624679565
Epoch 800, training loss: 6.393497467041016 = 0.07834216952323914 + 1.0 * 6.315155506134033
Epoch 800, val loss: 0.901627242565155
Epoch 810, training loss: 6.388239860534668 = 0.07455886900424957 + 1.0 * 6.313681125640869
Epoch 810, val loss: 0.9101279973983765
Epoch 820, training loss: 6.381775856018066 = 0.07101204991340637 + 1.0 * 6.310763835906982
Epoch 820, val loss: 0.91867995262146
Epoch 830, training loss: 6.387538909912109 = 0.06767931580543518 + 1.0 * 6.319859504699707
Epoch 830, val loss: 0.9272156357765198
Epoch 840, training loss: 6.379464626312256 = 0.06457163393497467 + 1.0 * 6.314892768859863
Epoch 840, val loss: 0.9356164932250977
Epoch 850, training loss: 6.370054721832275 = 0.06166336312890053 + 1.0 * 6.308391571044922
Epoch 850, val loss: 0.944137692451477
Epoch 860, training loss: 6.366567611694336 = 0.05892447754740715 + 1.0 * 6.307642936706543
Epoch 860, val loss: 0.9526345729827881
Epoch 870, training loss: 6.372227668762207 = 0.05634583532810211 + 1.0 * 6.315881729125977
Epoch 870, val loss: 0.9609753489494324
Epoch 880, training loss: 6.361276626586914 = 0.05391351133584976 + 1.0 * 6.307363033294678
Epoch 880, val loss: 0.9693385362625122
Epoch 890, training loss: 6.355729103088379 = 0.051631756126880646 + 1.0 * 6.3040971755981445
Epoch 890, val loss: 0.9777398109436035
Epoch 900, training loss: 6.356895923614502 = 0.049467939883470535 + 1.0 * 6.307427883148193
Epoch 900, val loss: 0.9859915375709534
Epoch 910, training loss: 6.354794979095459 = 0.04743039980530739 + 1.0 * 6.307364463806152
Epoch 910, val loss: 0.9941238760948181
Epoch 920, training loss: 6.34846830368042 = 0.04551940783858299 + 1.0 * 6.302948951721191
Epoch 920, val loss: 1.002265453338623
Epoch 930, training loss: 6.344441890716553 = 0.04370524361729622 + 1.0 * 6.300736427307129
Epoch 930, val loss: 1.010338306427002
Epoch 940, training loss: 6.342485427856445 = 0.04198785126209259 + 1.0 * 6.300497531890869
Epoch 940, val loss: 1.0182991027832031
Epoch 950, training loss: 6.343926906585693 = 0.0403604581952095 + 1.0 * 6.3035664558410645
Epoch 950, val loss: 1.0260281562805176
Epoch 960, training loss: 6.341662406921387 = 0.03883742168545723 + 1.0 * 6.302824974060059
Epoch 960, val loss: 1.0337495803833008
Epoch 970, training loss: 6.334769248962402 = 0.03739587217569351 + 1.0 * 6.297373294830322
Epoch 970, val loss: 1.0414105653762817
Epoch 980, training loss: 6.333620548248291 = 0.03602468967437744 + 1.0 * 6.297595977783203
Epoch 980, val loss: 1.0489318370819092
Epoch 990, training loss: 6.342729091644287 = 0.03472461551427841 + 1.0 * 6.308004379272461
Epoch 990, val loss: 1.0563151836395264
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8144438587243016
The final CL Acc:0.77284, 0.00698, The final GNN Acc:0.81269, 0.00131
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 8032])
updated graph: torch.Size([2, 10628])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.544221878051758 = 1.9473985433578491 + 1.0 * 8.596823692321777
Epoch 0, val loss: 1.9497188329696655
Epoch 10, training loss: 10.53400707244873 = 1.9374171495437622 + 1.0 * 8.596590042114258
Epoch 10, val loss: 1.9404019117355347
Epoch 20, training loss: 10.519889831542969 = 1.925230860710144 + 1.0 * 8.594658851623535
Epoch 20, val loss: 1.9286035299301147
Epoch 30, training loss: 10.486320495605469 = 1.9078998565673828 + 1.0 * 8.578420639038086
Epoch 30, val loss: 1.911428451538086
Epoch 40, training loss: 10.35311222076416 = 1.8837133646011353 + 1.0 * 8.469398498535156
Epoch 40, val loss: 1.887969970703125
Epoch 50, training loss: 9.75761890411377 = 1.856866478919983 + 1.0 * 7.900752544403076
Epoch 50, val loss: 1.862863302230835
Epoch 60, training loss: 9.299405097961426 = 1.8351770639419556 + 1.0 * 7.464227676391602
Epoch 60, val loss: 1.8428524732589722
Epoch 70, training loss: 8.940701484680176 = 1.8200031518936157 + 1.0 * 7.12069845199585
Epoch 70, val loss: 1.8274904489517212
Epoch 80, training loss: 8.783559799194336 = 1.8054368495941162 + 1.0 * 6.978123188018799
Epoch 80, val loss: 1.8128376007080078
Epoch 90, training loss: 8.66912841796875 = 1.7884840965270996 + 1.0 * 6.88064432144165
Epoch 90, val loss: 1.7965037822723389
Epoch 100, training loss: 8.583385467529297 = 1.7701784372329712 + 1.0 * 6.813206672668457
Epoch 100, val loss: 1.7793265581130981
Epoch 110, training loss: 8.510242462158203 = 1.7514041662216187 + 1.0 * 6.758838176727295
Epoch 110, val loss: 1.7619291543960571
Epoch 120, training loss: 8.442252159118652 = 1.7318933010101318 + 1.0 * 6.7103590965271
Epoch 120, val loss: 1.744012713432312
Epoch 130, training loss: 8.37684154510498 = 1.7102161645889282 + 1.0 * 6.666625499725342
Epoch 130, val loss: 1.7248482704162598
Epoch 140, training loss: 8.316131591796875 = 1.6851682662963867 + 1.0 * 6.630963325500488
Epoch 140, val loss: 1.703418254852295
Epoch 150, training loss: 8.257423400878906 = 1.6553491353988647 + 1.0 * 6.60207462310791
Epoch 150, val loss: 1.678330898284912
Epoch 160, training loss: 8.201271057128906 = 1.6196824312210083 + 1.0 * 6.581588268280029
Epoch 160, val loss: 1.6482638120651245
Epoch 170, training loss: 8.139819145202637 = 1.5783635377883911 + 1.0 * 6.561455726623535
Epoch 170, val loss: 1.6133674383163452
Epoch 180, training loss: 8.074345588684082 = 1.531174898147583 + 1.0 * 6.54317045211792
Epoch 180, val loss: 1.573304533958435
Epoch 190, training loss: 8.008394241333008 = 1.479100227355957 + 1.0 * 6.529294490814209
Epoch 190, val loss: 1.5290946960449219
Epoch 200, training loss: 7.942227363586426 = 1.4249193668365479 + 1.0 * 6.517307758331299
Epoch 200, val loss: 1.4836537837982178
Epoch 210, training loss: 7.875965118408203 = 1.3694465160369873 + 1.0 * 6.506518363952637
Epoch 210, val loss: 1.437143325805664
Epoch 220, training loss: 7.811412811279297 = 1.314156413078308 + 1.0 * 6.497256278991699
Epoch 220, val loss: 1.3911035060882568
Epoch 230, training loss: 7.749083995819092 = 1.26028573513031 + 1.0 * 6.488798141479492
Epoch 230, val loss: 1.3469319343566895
Epoch 240, training loss: 7.693132400512695 = 1.209498643875122 + 1.0 * 6.483633518218994
Epoch 240, val loss: 1.3059741258621216
Epoch 250, training loss: 7.63541316986084 = 1.161855936050415 + 1.0 * 6.473556995391846
Epoch 250, val loss: 1.2682446241378784
Epoch 260, training loss: 7.586944580078125 = 1.1163033246994019 + 1.0 * 6.470641136169434
Epoch 260, val loss: 1.2329128980636597
Epoch 270, training loss: 7.5325517654418945 = 1.0723989009857178 + 1.0 * 6.460152626037598
Epoch 270, val loss: 1.1994770765304565
Epoch 280, training loss: 7.4815263748168945 = 1.0292683839797974 + 1.0 * 6.452258110046387
Epoch 280, val loss: 1.1670893430709839
Epoch 290, training loss: 7.4424147605896 = 0.986449658870697 + 1.0 * 6.455965042114258
Epoch 290, val loss: 1.135347604751587
Epoch 300, training loss: 7.392104625701904 = 0.9448596239089966 + 1.0 * 6.447245121002197
Epoch 300, val loss: 1.1048479080200195
Epoch 310, training loss: 7.341501712799072 = 0.9040489792823792 + 1.0 * 6.437452793121338
Epoch 310, val loss: 1.075163722038269
Epoch 320, training loss: 7.294375419616699 = 0.8635867238044739 + 1.0 * 6.430788516998291
Epoch 320, val loss: 1.0459201335906982
Epoch 330, training loss: 7.252695083618164 = 0.8233574032783508 + 1.0 * 6.429337501525879
Epoch 330, val loss: 1.017173171043396
Epoch 340, training loss: 7.205870151519775 = 0.7840301394462585 + 1.0 * 6.421840190887451
Epoch 340, val loss: 0.9893863201141357
Epoch 350, training loss: 7.164473533630371 = 0.7457417249679565 + 1.0 * 6.418731689453125
Epoch 350, val loss: 0.9626129269599915
Epoch 360, training loss: 7.122579097747803 = 0.7083505988121033 + 1.0 * 6.414228439331055
Epoch 360, val loss: 0.9367763996124268
Epoch 370, training loss: 7.0852742195129395 = 0.6724826097488403 + 1.0 * 6.412791728973389
Epoch 370, val loss: 0.9123533964157104
Epoch 380, training loss: 7.043678283691406 = 0.6381865739822388 + 1.0 * 6.405491828918457
Epoch 380, val loss: 0.8896445035934448
Epoch 390, training loss: 7.005588054656982 = 0.6054250597953796 + 1.0 * 6.400163173675537
Epoch 390, val loss: 0.8685271143913269
Epoch 400, training loss: 6.982546806335449 = 0.5742065906524658 + 1.0 * 6.408339977264404
Epoch 400, val loss: 0.8491438627243042
Epoch 410, training loss: 6.939420700073242 = 0.5447756052017212 + 1.0 * 6.3946452140808105
Epoch 410, val loss: 0.831714391708374
Epoch 420, training loss: 6.91002082824707 = 0.5168786644935608 + 1.0 * 6.393142223358154
Epoch 420, val loss: 0.8160632252693176
Epoch 430, training loss: 6.880884647369385 = 0.4905937612056732 + 1.0 * 6.3902907371521
Epoch 430, val loss: 0.8021697998046875
Epoch 440, training loss: 6.849920749664307 = 0.46560096740722656 + 1.0 * 6.38431978225708
Epoch 440, val loss: 0.7898775935173035
Epoch 450, training loss: 6.822976112365723 = 0.4416126608848572 + 1.0 * 6.381363391876221
Epoch 450, val loss: 0.7788905501365662
Epoch 460, training loss: 6.80020809173584 = 0.41855913400650024 + 1.0 * 6.381649017333984
Epoch 460, val loss: 0.7690942883491516
Epoch 470, training loss: 6.7736735343933105 = 0.396540105342865 + 1.0 * 6.377133369445801
Epoch 470, val loss: 0.7603567838668823
Epoch 480, training loss: 6.751159191131592 = 0.37529081106185913 + 1.0 * 6.375868320465088
Epoch 480, val loss: 0.7525393962860107
Epoch 490, training loss: 6.72578763961792 = 0.354794979095459 + 1.0 * 6.370992660522461
Epoch 490, val loss: 0.7455592751502991
Epoch 500, training loss: 6.70670223236084 = 0.33511272072792053 + 1.0 * 6.371589660644531
Epoch 500, val loss: 0.7393593192100525
Epoch 510, training loss: 6.68342924118042 = 0.31630462408065796 + 1.0 * 6.367124557495117
Epoch 510, val loss: 0.7338873147964478
Epoch 520, training loss: 6.664134979248047 = 0.298342764377594 + 1.0 * 6.365792274475098
Epoch 520, val loss: 0.72911536693573
Epoch 530, training loss: 6.647012233734131 = 0.2812472879886627 + 1.0 * 6.36576509475708
Epoch 530, val loss: 0.7250085473060608
Epoch 540, training loss: 6.624589443206787 = 0.26511213183403015 + 1.0 * 6.359477519989014
Epoch 540, val loss: 0.7215766906738281
Epoch 550, training loss: 6.610321998596191 = 0.24991858005523682 + 1.0 * 6.360403537750244
Epoch 550, val loss: 0.718798816204071
Epoch 560, training loss: 6.5892109870910645 = 0.23565256595611572 + 1.0 * 6.353558540344238
Epoch 560, val loss: 0.716639518737793
Epoch 570, training loss: 6.581362247467041 = 0.22225183248519897 + 1.0 * 6.359110355377197
Epoch 570, val loss: 0.7151011228561401
Epoch 580, training loss: 6.569377422332764 = 0.2097417265176773 + 1.0 * 6.359635829925537
Epoch 580, val loss: 0.7140666842460632
Epoch 590, training loss: 6.550412178039551 = 0.19814978539943695 + 1.0 * 6.352262496948242
Epoch 590, val loss: 0.7136791944503784
Epoch 600, training loss: 6.5352067947387695 = 0.18735049664974213 + 1.0 * 6.347856521606445
Epoch 600, val loss: 0.7138490676879883
Epoch 610, training loss: 6.523575782775879 = 0.17724236845970154 + 1.0 * 6.3463335037231445
Epoch 610, val loss: 0.7145330905914307
Epoch 620, training loss: 6.515425682067871 = 0.16778109967708588 + 1.0 * 6.347644805908203
Epoch 620, val loss: 0.7156623601913452
Epoch 630, training loss: 6.503570556640625 = 0.1589718461036682 + 1.0 * 6.344598770141602
Epoch 630, val loss: 0.7171544432640076
Epoch 640, training loss: 6.493227481842041 = 0.15074831247329712 + 1.0 * 6.342479228973389
Epoch 640, val loss: 0.7191522121429443
Epoch 650, training loss: 6.484964847564697 = 0.14304672181606293 + 1.0 * 6.341917991638184
Epoch 650, val loss: 0.7214690446853638
Epoch 660, training loss: 6.472750663757324 = 0.13584165275096893 + 1.0 * 6.33690881729126
Epoch 660, val loss: 0.7241671085357666
Epoch 670, training loss: 6.465938568115234 = 0.12908373773097992 + 1.0 * 6.336854934692383
Epoch 670, val loss: 0.7271804213523865
Epoch 680, training loss: 6.4666523933410645 = 0.12273366004228592 + 1.0 * 6.343918800354004
Epoch 680, val loss: 0.7303404808044434
Epoch 690, training loss: 6.451886177062988 = 0.11681775748729706 + 1.0 * 6.335068225860596
Epoch 690, val loss: 0.7337961196899414
Epoch 700, training loss: 6.442257404327393 = 0.11124911904335022 + 1.0 * 6.331008434295654
Epoch 700, val loss: 0.737555980682373
Epoch 710, training loss: 6.437161445617676 = 0.10598570108413696 + 1.0 * 6.331175804138184
Epoch 710, val loss: 0.7414833903312683
Epoch 720, training loss: 6.437486171722412 = 0.10102643072605133 + 1.0 * 6.336459636688232
Epoch 720, val loss: 0.7455071806907654
Epoch 730, training loss: 6.4280781745910645 = 0.096384696662426 + 1.0 * 6.331693649291992
Epoch 730, val loss: 0.7496472001075745
Epoch 740, training loss: 6.419207572937012 = 0.09201149642467499 + 1.0 * 6.32719612121582
Epoch 740, val loss: 0.7541041374206543
Epoch 750, training loss: 6.413021564483643 = 0.08786658942699432 + 1.0 * 6.325154781341553
Epoch 750, val loss: 0.7587035298347473
Epoch 760, training loss: 6.414079666137695 = 0.08393397182226181 + 1.0 * 6.330145835876465
Epoch 760, val loss: 0.763343334197998
Epoch 770, training loss: 6.412802219390869 = 0.08022501319646835 + 1.0 * 6.332577228546143
Epoch 770, val loss: 0.7680889368057251
Epoch 780, training loss: 6.400634288787842 = 0.07672449946403503 + 1.0 * 6.323909759521484
Epoch 780, val loss: 0.7729108929634094
Epoch 790, training loss: 6.401069164276123 = 0.07340870797634125 + 1.0 * 6.32766056060791
Epoch 790, val loss: 0.777862012386322
Epoch 800, training loss: 6.396457195281982 = 0.07026439160108566 + 1.0 * 6.326192855834961
Epoch 800, val loss: 0.7826064229011536
Epoch 810, training loss: 6.386964321136475 = 0.0672997236251831 + 1.0 * 6.319664478302002
Epoch 810, val loss: 0.7876590490341187
Epoch 820, training loss: 6.38319730758667 = 0.06447820365428925 + 1.0 * 6.318718910217285
Epoch 820, val loss: 0.7927404046058655
Epoch 830, training loss: 6.401434898376465 = 0.06178918853402138 + 1.0 * 6.339645862579346
Epoch 830, val loss: 0.7977371215820312
Epoch 840, training loss: 6.3786396980285645 = 0.05926283821463585 + 1.0 * 6.3193769454956055
Epoch 840, val loss: 0.8026696443557739
Epoch 850, training loss: 6.373734951019287 = 0.05686397850513458 + 1.0 * 6.316871166229248
Epoch 850, val loss: 0.8077617883682251
Epoch 860, training loss: 6.369674205780029 = 0.05457998439669609 + 1.0 * 6.315093994140625
Epoch 860, val loss: 0.8128497004508972
Epoch 870, training loss: 6.366098880767822 = 0.052407801151275635 + 1.0 * 6.313691139221191
Epoch 870, val loss: 0.8179363012313843
Epoch 880, training loss: 6.376819610595703 = 0.05034198239445686 + 1.0 * 6.326477527618408
Epoch 880, val loss: 0.8230262994766235
Epoch 890, training loss: 6.366055488586426 = 0.048378460109233856 + 1.0 * 6.317677021026611
Epoch 890, val loss: 0.8278862833976746
Epoch 900, training loss: 6.35909366607666 = 0.046526793390512466 + 1.0 * 6.312566757202148
Epoch 900, val loss: 0.8329604864120483
Epoch 910, training loss: 6.355630397796631 = 0.044764235615730286 + 1.0 * 6.310866355895996
Epoch 910, val loss: 0.8379945158958435
Epoch 920, training loss: 6.3556413650512695 = 0.043083786964416504 + 1.0 * 6.312557697296143
Epoch 920, val loss: 0.8429770469665527
Epoch 930, training loss: 6.356044292449951 = 0.04148721322417259 + 1.0 * 6.314557075500488
Epoch 930, val loss: 0.847870409488678
Epoch 940, training loss: 6.357079029083252 = 0.03997037187218666 + 1.0 * 6.317108631134033
Epoch 940, val loss: 0.8526076674461365
Epoch 950, training loss: 6.347188949584961 = 0.03853882849216461 + 1.0 * 6.308650016784668
Epoch 950, val loss: 0.8574838042259216
Epoch 960, training loss: 6.343850612640381 = 0.037173081189394 + 1.0 * 6.306677341461182
Epoch 960, val loss: 0.8623045086860657
Epoch 970, training loss: 6.345201015472412 = 0.03586965054273605 + 1.0 * 6.30933141708374
Epoch 970, val loss: 0.867041826248169
Epoch 980, training loss: 6.34237003326416 = 0.034630510956048965 + 1.0 * 6.307739734649658
Epoch 980, val loss: 0.8717779517173767
Epoch 990, training loss: 6.3412017822265625 = 0.03344922885298729 + 1.0 * 6.30775260925293
Epoch 990, val loss: 0.8763414621353149
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.536300659179688 = 1.9394768476486206 + 1.0 * 8.596823692321777
Epoch 0, val loss: 1.9471476078033447
Epoch 10, training loss: 10.526009559631348 = 1.9294569492340088 + 1.0 * 8.596552848815918
Epoch 10, val loss: 1.937536597251892
Epoch 20, training loss: 10.511216163635254 = 1.9167850017547607 + 1.0 * 8.594430923461914
Epoch 20, val loss: 1.9249211549758911
Epoch 30, training loss: 10.47558307647705 = 1.8987360000610352 + 1.0 * 8.576847076416016
Epoch 30, val loss: 1.9065357446670532
Epoch 40, training loss: 10.344841003417969 = 1.8740296363830566 + 1.0 * 8.470810890197754
Epoch 40, val loss: 1.8822383880615234
Epoch 50, training loss: 9.844133377075195 = 1.847373127937317 + 1.0 * 7.99675989151001
Epoch 50, val loss: 1.857079029083252
Epoch 60, training loss: 9.417046546936035 = 1.8246533870697021 + 1.0 * 7.592393398284912
Epoch 60, val loss: 1.83661949634552
Epoch 70, training loss: 9.020450592041016 = 1.8086977005004883 + 1.0 * 7.211752414703369
Epoch 70, val loss: 1.8212487697601318
Epoch 80, training loss: 8.82832145690918 = 1.7937021255493164 + 1.0 * 7.034619331359863
Epoch 80, val loss: 1.8064464330673218
Epoch 90, training loss: 8.695364952087402 = 1.776715636253357 + 1.0 * 6.918649673461914
Epoch 90, val loss: 1.7899770736694336
Epoch 100, training loss: 8.587486267089844 = 1.7593227624893188 + 1.0 * 6.8281636238098145
Epoch 100, val loss: 1.7728503942489624
Epoch 110, training loss: 8.498461723327637 = 1.740983247756958 + 1.0 * 6.7574782371521
Epoch 110, val loss: 1.7554432153701782
Epoch 120, training loss: 8.420668601989746 = 1.721184492111206 + 1.0 * 6.699483871459961
Epoch 120, val loss: 1.7380659580230713
Epoch 130, training loss: 8.353708267211914 = 1.6990463733673096 + 1.0 * 6.654661655426025
Epoch 130, val loss: 1.7191611528396606
Epoch 140, training loss: 8.295916557312012 = 1.6729553937911987 + 1.0 * 6.622961044311523
Epoch 140, val loss: 1.6965012550354004
Epoch 150, training loss: 8.23996353149414 = 1.6420172452926636 + 1.0 * 6.597946643829346
Epoch 150, val loss: 1.6696220636367798
Epoch 160, training loss: 8.181153297424316 = 1.6061413288116455 + 1.0 * 6.57501220703125
Epoch 160, val loss: 1.6388860940933228
Epoch 170, training loss: 8.120637893676758 = 1.5651979446411133 + 1.0 * 6.555440425872803
Epoch 170, val loss: 1.6040652990341187
Epoch 180, training loss: 8.06339168548584 = 1.5189169645309448 + 1.0 * 6.544475078582764
Epoch 180, val loss: 1.5648658275604248
Epoch 190, training loss: 7.995552062988281 = 1.4695929288864136 + 1.0 * 6.525959014892578
Epoch 190, val loss: 1.5235189199447632
Epoch 200, training loss: 7.930981636047363 = 1.4178835153579712 + 1.0 * 6.513098239898682
Epoch 200, val loss: 1.4801987409591675
Epoch 210, training loss: 7.865161418914795 = 1.364486575126648 + 1.0 * 6.500674724578857
Epoch 210, val loss: 1.435728669166565
Epoch 220, training loss: 7.802154541015625 = 1.3114655017852783 + 1.0 * 6.490688800811768
Epoch 220, val loss: 1.3921371698379517
Epoch 230, training loss: 7.737619876861572 = 1.259702205657959 + 1.0 * 6.477917671203613
Epoch 230, val loss: 1.349790096282959
Epoch 240, training loss: 7.681723594665527 = 1.208951473236084 + 1.0 * 6.472772121429443
Epoch 240, val loss: 1.3083263635635376
Epoch 250, training loss: 7.619755744934082 = 1.1595858335494995 + 1.0 * 6.460169792175293
Epoch 250, val loss: 1.268743872642517
Epoch 260, training loss: 7.5621843338012695 = 1.1112260818481445 + 1.0 * 6.450958251953125
Epoch 260, val loss: 1.2304270267486572
Epoch 270, training loss: 7.511880397796631 = 1.0636383295059204 + 1.0 * 6.4482421875
Epoch 270, val loss: 1.1930546760559082
Epoch 280, training loss: 7.454318046569824 = 1.0172611474990845 + 1.0 * 6.437057018280029
Epoch 280, val loss: 1.1573363542556763
Epoch 290, training loss: 7.402085304260254 = 0.9717938899993896 + 1.0 * 6.430291652679443
Epoch 290, val loss: 1.1225029230117798
Epoch 300, training loss: 7.35279655456543 = 0.9268335103988647 + 1.0 * 6.425962924957275
Epoch 300, val loss: 1.0883857011795044
Epoch 310, training loss: 7.303322792053223 = 0.8827646970748901 + 1.0 * 6.420557975769043
Epoch 310, val loss: 1.0550329685211182
Epoch 320, training loss: 7.253114700317383 = 0.8389620780944824 + 1.0 * 6.4141526222229
Epoch 320, val loss: 1.0222196578979492
Epoch 330, training loss: 7.211238861083984 = 0.7955252528190613 + 1.0 * 6.415713787078857
Epoch 330, val loss: 0.9895599484443665
Epoch 340, training loss: 7.163119316101074 = 0.7531046271324158 + 1.0 * 6.410014629364014
Epoch 340, val loss: 0.9580159783363342
Epoch 350, training loss: 7.1128249168396 = 0.7118037343025208 + 1.0 * 6.4010210037231445
Epoch 350, val loss: 0.9275267720222473
Epoch 360, training loss: 7.068993091583252 = 0.6712998151779175 + 1.0 * 6.397693157196045
Epoch 360, val loss: 0.8977296948432922
Epoch 370, training loss: 7.043633937835693 = 0.63177889585495 + 1.0 * 6.411855220794678
Epoch 370, val loss: 0.8689177632331848
Epoch 380, training loss: 6.986875534057617 = 0.5941933393478394 + 1.0 * 6.392682075500488
Epoch 380, val loss: 0.8421443700790405
Epoch 390, training loss: 6.9461565017700195 = 0.5582093000411987 + 1.0 * 6.387947082519531
Epoch 390, val loss: 0.8170929551124573
Epoch 400, training loss: 6.908017635345459 = 0.5235477685928345 + 1.0 * 6.384469985961914
Epoch 400, val loss: 0.7935147285461426
Epoch 410, training loss: 6.871386528015137 = 0.49014097452163696 + 1.0 * 6.3812456130981445
Epoch 410, val loss: 0.7716286182403564
Epoch 420, training loss: 6.837002277374268 = 0.4582522213459015 + 1.0 * 6.378749847412109
Epoch 420, val loss: 0.7515255808830261
Epoch 430, training loss: 6.807793140411377 = 0.42764991521835327 + 1.0 * 6.380143165588379
Epoch 430, val loss: 0.7330985069274902
Epoch 440, training loss: 6.769860744476318 = 0.39858391880989075 + 1.0 * 6.37127685546875
Epoch 440, val loss: 0.7164289951324463
Epoch 450, training loss: 6.739190101623535 = 0.37091997265815735 + 1.0 * 6.368269920349121
Epoch 450, val loss: 0.7013393044471741
Epoch 460, training loss: 6.714155197143555 = 0.34452787041664124 + 1.0 * 6.369627475738525
Epoch 460, val loss: 0.6876459121704102
Epoch 470, training loss: 6.6870951652526855 = 0.31971031427383423 + 1.0 * 6.367384910583496
Epoch 470, val loss: 0.6752691864967346
Epoch 480, training loss: 6.658361434936523 = 0.29635393619537354 + 1.0 * 6.3620076179504395
Epoch 480, val loss: 0.6643124222755432
Epoch 490, training loss: 6.636676788330078 = 0.2745235860347748 + 1.0 * 6.362153053283691
Epoch 490, val loss: 0.6547436714172363
Epoch 500, training loss: 6.612397193908691 = 0.25436389446258545 + 1.0 * 6.358033180236816
Epoch 500, val loss: 0.6465187072753906
Epoch 510, training loss: 6.589948654174805 = 0.23563259840011597 + 1.0 * 6.354316234588623
Epoch 510, val loss: 0.6395966410636902
Epoch 520, training loss: 6.581256866455078 = 0.2183089405298233 + 1.0 * 6.362947940826416
Epoch 520, val loss: 0.6339644193649292
Epoch 530, training loss: 6.5520100593566895 = 0.20249614119529724 + 1.0 * 6.349514007568359
Epoch 530, val loss: 0.6295742988586426
Epoch 540, training loss: 6.5361785888671875 = 0.18800419569015503 + 1.0 * 6.348174571990967
Epoch 540, val loss: 0.6263474822044373
Epoch 550, training loss: 6.521150588989258 = 0.1747681200504303 + 1.0 * 6.3463826179504395
Epoch 550, val loss: 0.624161422252655
Epoch 560, training loss: 6.508124828338623 = 0.16275173425674438 + 1.0 * 6.345373153686523
Epoch 560, val loss: 0.6229546070098877
Epoch 570, training loss: 6.498085021972656 = 0.15176500380039215 + 1.0 * 6.346320152282715
Epoch 570, val loss: 0.6225736737251282
Epoch 580, training loss: 6.485279560089111 = 0.14175698161125183 + 1.0 * 6.343522548675537
Epoch 580, val loss: 0.6229338645935059
Epoch 590, training loss: 6.471252918243408 = 0.13261452317237854 + 1.0 * 6.3386383056640625
Epoch 590, val loss: 0.6239600777626038
Epoch 600, training loss: 6.461670875549316 = 0.12422606348991394 + 1.0 * 6.33744478225708
Epoch 600, val loss: 0.6255266070365906
Epoch 610, training loss: 6.456108570098877 = 0.1165517196059227 + 1.0 * 6.339556694030762
Epoch 610, val loss: 0.6276093125343323
Epoch 620, training loss: 6.444644451141357 = 0.10958274453878403 + 1.0 * 6.335061550140381
Epoch 620, val loss: 0.6300723552703857
Epoch 630, training loss: 6.4352126121521 = 0.10316576063632965 + 1.0 * 6.332046985626221
Epoch 630, val loss: 0.6329247355461121
Epoch 640, training loss: 6.426826000213623 = 0.09723283350467682 + 1.0 * 6.329593181610107
Epoch 640, val loss: 0.6361157298088074
Epoch 650, training loss: 6.433390140533447 = 0.0917392373085022 + 1.0 * 6.34165096282959
Epoch 650, val loss: 0.6395885944366455
Epoch 660, training loss: 6.416806697845459 = 0.08671016246080399 + 1.0 * 6.33009672164917
Epoch 660, val loss: 0.643234133720398
Epoch 670, training loss: 6.4100189208984375 = 0.08207923918962479 + 1.0 * 6.327939510345459
Epoch 670, val loss: 0.6470325589179993
Epoch 680, training loss: 6.403507232666016 = 0.07778076082468033 + 1.0 * 6.325726509094238
Epoch 680, val loss: 0.6510143280029297
Epoch 690, training loss: 6.40106201171875 = 0.07378765195608139 + 1.0 * 6.327274322509766
Epoch 690, val loss: 0.6551573276519775
Epoch 700, training loss: 6.3994035720825195 = 0.07008879631757736 + 1.0 * 6.329314708709717
Epoch 700, val loss: 0.6593905091285706
Epoch 710, training loss: 6.387073516845703 = 0.06666054576635361 + 1.0 * 6.320413112640381
Epoch 710, val loss: 0.6636666655540466
Epoch 720, training loss: 6.382936477661133 = 0.06346368789672852 + 1.0 * 6.319472789764404
Epoch 720, val loss: 0.668053388595581
Epoch 730, training loss: 6.380645275115967 = 0.06047062948346138 + 1.0 * 6.320174694061279
Epoch 730, val loss: 0.6725040674209595
Epoch 740, training loss: 6.375194072723389 = 0.05767768248915672 + 1.0 * 6.317516326904297
Epoch 740, val loss: 0.6770186424255371
Epoch 750, training loss: 6.3798604011535645 = 0.055069081485271454 + 1.0 * 6.324791431427002
Epoch 750, val loss: 0.6815040111541748
Epoch 760, training loss: 6.367822170257568 = 0.05265389755368233 + 1.0 * 6.315168380737305
Epoch 760, val loss: 0.686024010181427
Epoch 770, training loss: 6.3635711669921875 = 0.05037732422351837 + 1.0 * 6.3131937980651855
Epoch 770, val loss: 0.6905329823493958
Epoch 780, training loss: 6.3658246994018555 = 0.048236336559057236 + 1.0 * 6.3175883293151855
Epoch 780, val loss: 0.6950896382331848
Epoch 790, training loss: 6.360990047454834 = 0.04623114690184593 + 1.0 * 6.314758777618408
Epoch 790, val loss: 0.6996552348136902
Epoch 800, training loss: 6.358474254608154 = 0.04435001686215401 + 1.0 * 6.31412410736084
Epoch 800, val loss: 0.7041425108909607
Epoch 810, training loss: 6.352222442626953 = 0.04257882758975029 + 1.0 * 6.309643745422363
Epoch 810, val loss: 0.7086507678031921
Epoch 820, training loss: 6.350347518920898 = 0.04090603440999985 + 1.0 * 6.309441566467285
Epoch 820, val loss: 0.7131625413894653
Epoch 830, training loss: 6.347738742828369 = 0.039326056838035583 + 1.0 * 6.308412551879883
Epoch 830, val loss: 0.7176558375358582
Epoch 840, training loss: 6.346879482269287 = 0.03784032166004181 + 1.0 * 6.309039115905762
Epoch 840, val loss: 0.7221107482910156
Epoch 850, training loss: 6.342208385467529 = 0.03643478453159332 + 1.0 * 6.305773735046387
Epoch 850, val loss: 0.7265360355377197
Epoch 860, training loss: 6.34086275100708 = 0.03510790318250656 + 1.0 * 6.305754661560059
Epoch 860, val loss: 0.7309529185295105
Epoch 870, training loss: 6.346775531768799 = 0.03385162726044655 + 1.0 * 6.312923908233643
Epoch 870, val loss: 0.7353291511535645
Epoch 880, training loss: 6.337118148803711 = 0.03266022726893425 + 1.0 * 6.304458141326904
Epoch 880, val loss: 0.7396715879440308
Epoch 890, training loss: 6.334689617156982 = 0.031538691371679306 + 1.0 * 6.3031511306762695
Epoch 890, val loss: 0.7439627647399902
Epoch 900, training loss: 6.33643102645874 = 0.03046712651848793 + 1.0 * 6.30596399307251
Epoch 900, val loss: 0.7482689023017883
Epoch 910, training loss: 6.330390453338623 = 0.029451001435518265 + 1.0 * 6.300939559936523
Epoch 910, val loss: 0.7525057196617126
Epoch 920, training loss: 6.326106548309326 = 0.02848133258521557 + 1.0 * 6.2976250648498535
Epoch 920, val loss: 0.7567307949066162
Epoch 930, training loss: 6.333939552307129 = 0.027554940432310104 + 1.0 * 6.306384563446045
Epoch 930, val loss: 0.7609730958938599
Epoch 940, training loss: 6.333228588104248 = 0.026682235300540924 + 1.0 * 6.306546211242676
Epoch 940, val loss: 0.7650961875915527
Epoch 950, training loss: 6.321552753448486 = 0.025851435959339142 + 1.0 * 6.295701503753662
Epoch 950, val loss: 0.7691285014152527
Epoch 960, training loss: 6.321040153503418 = 0.025059523060917854 + 1.0 * 6.295980453491211
Epoch 960, val loss: 0.7731963396072388
Epoch 970, training loss: 6.319214344024658 = 0.024298161268234253 + 1.0 * 6.294916152954102
Epoch 970, val loss: 0.7773188948631287
Epoch 980, training loss: 6.332085609436035 = 0.0235674437135458 + 1.0 * 6.308517932891846
Epoch 980, val loss: 0.7813845872879028
Epoch 990, training loss: 6.319970607757568 = 0.0228803139179945 + 1.0 * 6.297090530395508
Epoch 990, val loss: 0.7853549718856812
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 10.54040241241455 = 1.9435926675796509 + 1.0 * 8.596809387207031
Epoch 0, val loss: 1.9392160177230835
Epoch 10, training loss: 10.530169486999512 = 1.9337413311004639 + 1.0 * 8.596427917480469
Epoch 10, val loss: 1.9302372932434082
Epoch 20, training loss: 10.514836311340332 = 1.921531081199646 + 1.0 * 8.593305587768555
Epoch 20, val loss: 1.9188374280929565
Epoch 30, training loss: 10.474103927612305 = 1.9047131538391113 + 1.0 * 8.569390296936035
Epoch 30, val loss: 1.9028257131576538
Epoch 40, training loss: 10.301968574523926 = 1.8829458951950073 + 1.0 * 8.419022560119629
Epoch 40, val loss: 1.8826031684875488
Epoch 50, training loss: 9.784854888916016 = 1.8593761920928955 + 1.0 * 7.925478935241699
Epoch 50, val loss: 1.861257553100586
Epoch 60, training loss: 9.277304649353027 = 1.8405635356903076 + 1.0 * 7.436740875244141
Epoch 60, val loss: 1.84409499168396
Epoch 70, training loss: 8.986217498779297 = 1.8231302499771118 + 1.0 * 7.163087368011475
Epoch 70, val loss: 1.8274682760238647
Epoch 80, training loss: 8.83835220336914 = 1.805026650428772 + 1.0 * 7.033325672149658
Epoch 80, val loss: 1.8107441663742065
Epoch 90, training loss: 8.672826766967773 = 1.7863383293151855 + 1.0 * 6.886488437652588
Epoch 90, val loss: 1.7945637702941895
Epoch 100, training loss: 8.548007011413574 = 1.768649697303772 + 1.0 * 6.779357433319092
Epoch 100, val loss: 1.7793394327163696
Epoch 110, training loss: 8.463375091552734 = 1.7512344121932983 + 1.0 * 6.7121405601501465
Epoch 110, val loss: 1.7637394666671753
Epoch 120, training loss: 8.391168594360352 = 1.732846975326538 + 1.0 * 6.658321380615234
Epoch 120, val loss: 1.747178316116333
Epoch 130, training loss: 8.32894229888916 = 1.7122094631195068 + 1.0 * 6.616733074188232
Epoch 130, val loss: 1.7286831140518188
Epoch 140, training loss: 8.272234916687012 = 1.6880251169204712 + 1.0 * 6.584209442138672
Epoch 140, val loss: 1.7071928977966309
Epoch 150, training loss: 8.218888282775879 = 1.65982985496521 + 1.0 * 6.55905818939209
Epoch 150, val loss: 1.6826926469802856
Epoch 160, training loss: 8.163681030273438 = 1.6270192861557007 + 1.0 * 6.5366621017456055
Epoch 160, val loss: 1.6545442342758179
Epoch 170, training loss: 8.109079360961914 = 1.5885462760925293 + 1.0 * 6.520533561706543
Epoch 170, val loss: 1.6216267347335815
Epoch 180, training loss: 8.053120613098145 = 1.5444225072860718 + 1.0 * 6.508697986602783
Epoch 180, val loss: 1.583770990371704
Epoch 190, training loss: 7.991445064544678 = 1.4947344064712524 + 1.0 * 6.496710777282715
Epoch 190, val loss: 1.5412075519561768
Epoch 200, training loss: 7.926168441772461 = 1.4401272535324097 + 1.0 * 6.486041069030762
Epoch 200, val loss: 1.4947073459625244
Epoch 210, training loss: 7.858463287353516 = 1.3821947574615479 + 1.0 * 6.476268291473389
Epoch 210, val loss: 1.4461548328399658
Epoch 220, training loss: 7.79454231262207 = 1.3239275217056274 + 1.0 * 6.470614910125732
Epoch 220, val loss: 1.3979741334915161
Epoch 230, training loss: 7.724538803100586 = 1.267024278640747 + 1.0 * 6.45751428604126
Epoch 230, val loss: 1.3515703678131104
Epoch 240, training loss: 7.662599563598633 = 1.2118282318115234 + 1.0 * 6.450771331787109
Epoch 240, val loss: 1.3069802522659302
Epoch 250, training loss: 7.604038238525391 = 1.1588256359100342 + 1.0 * 6.4452128410339355
Epoch 250, val loss: 1.2646986246109009
Epoch 260, training loss: 7.543698310852051 = 1.1083794832229614 + 1.0 * 6.435318946838379
Epoch 260, val loss: 1.224515676498413
Epoch 270, training loss: 7.486870288848877 = 1.0595608949661255 + 1.0 * 6.427309513092041
Epoch 270, val loss: 1.1858271360397339
Epoch 280, training loss: 7.441288948059082 = 1.0120750665664673 + 1.0 * 6.429214000701904
Epoch 280, val loss: 1.1482118368148804
Epoch 290, training loss: 7.383911609649658 = 0.9667183756828308 + 1.0 * 6.417193412780762
Epoch 290, val loss: 1.1119234561920166
Epoch 300, training loss: 7.333182334899902 = 0.9224799871444702 + 1.0 * 6.410702228546143
Epoch 300, val loss: 1.0763956308364868
Epoch 310, training loss: 7.289678573608398 = 0.8788832426071167 + 1.0 * 6.410795211791992
Epoch 310, val loss: 1.0412404537200928
Epoch 320, training loss: 7.241680145263672 = 0.8369925022125244 + 1.0 * 6.404687881469727
Epoch 320, val loss: 1.007120966911316
Epoch 330, training loss: 7.19343376159668 = 0.7961761951446533 + 1.0 * 6.397257328033447
Epoch 330, val loss: 0.9740232229232788
Epoch 340, training loss: 7.149523735046387 = 0.7562205195426941 + 1.0 * 6.393303394317627
Epoch 340, val loss: 0.9417542815208435
Epoch 350, training loss: 7.121769905090332 = 0.7173235416412354 + 1.0 * 6.404446601867676
Epoch 350, val loss: 0.9108073711395264
Epoch 360, training loss: 7.071825981140137 = 0.6806533336639404 + 1.0 * 6.391172409057617
Epoch 360, val loss: 0.8820679783821106
Epoch 370, training loss: 7.029730796813965 = 0.6459283232688904 + 1.0 * 6.38380241394043
Epoch 370, val loss: 0.8558182716369629
Epoch 380, training loss: 6.992225646972656 = 0.6129747629165649 + 1.0 * 6.379251003265381
Epoch 380, val loss: 0.8317994475364685
Epoch 390, training loss: 6.957998275756836 = 0.5816791653633118 + 1.0 * 6.37631893157959
Epoch 390, val loss: 0.8100277781486511
Epoch 400, training loss: 6.930532455444336 = 0.5522280335426331 + 1.0 * 6.378304481506348
Epoch 400, val loss: 0.7906663417816162
Epoch 410, training loss: 6.899959564208984 = 0.5249056220054626 + 1.0 * 6.375053882598877
Epoch 410, val loss: 0.7737776637077332
Epoch 420, training loss: 6.866820812225342 = 0.49914324283599854 + 1.0 * 6.367677688598633
Epoch 420, val loss: 0.7588983774185181
Epoch 430, training loss: 6.840163230895996 = 0.4746166467666626 + 1.0 * 6.365546703338623
Epoch 430, val loss: 0.7456852793693542
Epoch 440, training loss: 6.814333915710449 = 0.45132139325141907 + 1.0 * 6.363012313842773
Epoch 440, val loss: 0.7340344190597534
Epoch 450, training loss: 6.789631366729736 = 0.42932066321372986 + 1.0 * 6.3603105545043945
Epoch 450, val loss: 0.7238266468048096
Epoch 460, training loss: 6.766617298126221 = 0.4083369970321655 + 1.0 * 6.358280181884766
Epoch 460, val loss: 0.7148299217224121
Epoch 470, training loss: 6.74462366104126 = 0.38832318782806396 + 1.0 * 6.356300354003906
Epoch 470, val loss: 0.7068027257919312
Epoch 480, training loss: 6.723886966705322 = 0.36928945779800415 + 1.0 * 6.354597568511963
Epoch 480, val loss: 0.6997436285018921
Epoch 490, training loss: 6.7031025886535645 = 0.35106295347213745 + 1.0 * 6.352039813995361
Epoch 490, val loss: 0.693574070930481
Epoch 500, training loss: 6.692736625671387 = 0.33355817198753357 + 1.0 * 6.35917854309082
Epoch 500, val loss: 0.6881290078163147
Epoch 510, training loss: 6.664556503295898 = 0.3168763220310211 + 1.0 * 6.34768009185791
Epoch 510, val loss: 0.6834487915039062
Epoch 520, training loss: 6.647943019866943 = 0.30082640051841736 + 1.0 * 6.347116470336914
Epoch 520, val loss: 0.6794584393501282
Epoch 530, training loss: 6.629948139190674 = 0.28544363379478455 + 1.0 * 6.344504356384277
Epoch 530, val loss: 0.6759918332099915
Epoch 540, training loss: 6.613720417022705 = 0.2706749737262726 + 1.0 * 6.343045234680176
Epoch 540, val loss: 0.6730685234069824
Epoch 550, training loss: 6.59686279296875 = 0.25643646717071533 + 1.0 * 6.340426445007324
Epoch 550, val loss: 0.6706933975219727
Epoch 560, training loss: 6.587606430053711 = 0.2426883578300476 + 1.0 * 6.344918251037598
Epoch 560, val loss: 0.6687878370285034
Epoch 570, training loss: 6.568118095397949 = 0.22948989272117615 + 1.0 * 6.33862829208374
Epoch 570, val loss: 0.6673133373260498
Epoch 580, training loss: 6.553449630737305 = 0.21678847074508667 + 1.0 * 6.336661338806152
Epoch 580, val loss: 0.6662928462028503
Epoch 590, training loss: 6.539770126342773 = 0.20462912321090698 + 1.0 * 6.335141181945801
Epoch 590, val loss: 0.6657030582427979
Epoch 600, training loss: 6.524941444396973 = 0.1930171400308609 + 1.0 * 6.3319244384765625
Epoch 600, val loss: 0.6654912233352661
Epoch 610, training loss: 6.524112701416016 = 0.18192361295223236 + 1.0 * 6.342189311981201
Epoch 610, val loss: 0.6656918525695801
Epoch 620, training loss: 6.502683162689209 = 0.1714860498905182 + 1.0 * 6.331197261810303
Epoch 620, val loss: 0.6662167906761169
Epoch 630, training loss: 6.489233016967773 = 0.16160015761852264 + 1.0 * 6.327632904052734
Epoch 630, val loss: 0.6671094298362732
Epoch 640, training loss: 6.4776611328125 = 0.15223228931427002 + 1.0 * 6.3254289627075195
Epoch 640, val loss: 0.6684077382087708
Epoch 650, training loss: 6.477717399597168 = 0.1434057652950287 + 1.0 * 6.334311485290527
Epoch 650, val loss: 0.6700210571289062
Epoch 660, training loss: 6.461108207702637 = 0.1351475864648819 + 1.0 * 6.325960636138916
Epoch 660, val loss: 0.6718982458114624
Epoch 670, training loss: 6.454676628112793 = 0.127401202917099 + 1.0 * 6.327275276184082
Epoch 670, val loss: 0.674111008644104
Epoch 680, training loss: 6.442299842834473 = 0.12014317512512207 + 1.0 * 6.3221564292907715
Epoch 680, val loss: 0.6765649318695068
Epoch 690, training loss: 6.433356761932373 = 0.11335646361112595 + 1.0 * 6.320000171661377
Epoch 690, val loss: 0.6793017387390137
Epoch 700, training loss: 6.426159858703613 = 0.10698678344488144 + 1.0 * 6.3191728591918945
Epoch 700, val loss: 0.6822763085365295
Epoch 710, training loss: 6.421349048614502 = 0.1010388657450676 + 1.0 * 6.320310115814209
Epoch 710, val loss: 0.6854402422904968
Epoch 720, training loss: 6.410628318786621 = 0.09550538659095764 + 1.0 * 6.315123081207275
Epoch 720, val loss: 0.6887780427932739
Epoch 730, training loss: 6.40471076965332 = 0.09033027291297913 + 1.0 * 6.314380645751953
Epoch 730, val loss: 0.6923125386238098
Epoch 740, training loss: 6.408840179443359 = 0.08549058437347412 + 1.0 * 6.323349475860596
Epoch 740, val loss: 0.6960395574569702
Epoch 750, training loss: 6.3949713706970215 = 0.08096829056739807 + 1.0 * 6.314002990722656
Epoch 750, val loss: 0.6998196244239807
Epoch 760, training loss: 6.389530181884766 = 0.07676370441913605 + 1.0 * 6.3127665519714355
Epoch 760, val loss: 0.7037556171417236
Epoch 770, training loss: 6.382916450500488 = 0.0728284940123558 + 1.0 * 6.310088157653809
Epoch 770, val loss: 0.7078034281730652
Epoch 780, training loss: 6.377192974090576 = 0.06915314495563507 + 1.0 * 6.308039665222168
Epoch 780, val loss: 0.7119112610816956
Epoch 790, training loss: 6.381954193115234 = 0.06571690738201141 + 1.0 * 6.316237449645996
Epoch 790, val loss: 0.7160826325416565
Epoch 800, training loss: 6.36997127532959 = 0.06250558793544769 + 1.0 * 6.307465553283691
Epoch 800, val loss: 0.7203011512756348
Epoch 810, training loss: 6.363883972167969 = 0.059504080563783646 + 1.0 * 6.304379940032959
Epoch 810, val loss: 0.724571943283081
Epoch 820, training loss: 6.3612494468688965 = 0.05668388679623604 + 1.0 * 6.3045654296875
Epoch 820, val loss: 0.7289328575134277
Epoch 830, training loss: 6.368035793304443 = 0.054038554430007935 + 1.0 * 6.313997268676758
Epoch 830, val loss: 0.7332509160041809
Epoch 840, training loss: 6.3565192222595215 = 0.051587384194135666 + 1.0 * 6.304931640625
Epoch 840, val loss: 0.73753821849823
Epoch 850, training loss: 6.3518548011779785 = 0.04928649589419365 + 1.0 * 6.302568435668945
Epoch 850, val loss: 0.7418313026428223
Epoch 860, training loss: 6.3474836349487305 = 0.047117505222558975 + 1.0 * 6.300365924835205
Epoch 860, val loss: 0.7461795806884766
Epoch 870, training loss: 6.34953498840332 = 0.04507385939359665 + 1.0 * 6.3044610023498535
Epoch 870, val loss: 0.7505374550819397
Epoch 880, training loss: 6.3455119132995605 = 0.04315194860100746 + 1.0 * 6.3023600578308105
Epoch 880, val loss: 0.7548446655273438
Epoch 890, training loss: 6.339494228363037 = 0.04134537652134895 + 1.0 * 6.2981486320495605
Epoch 890, val loss: 0.7590987682342529
Epoch 900, training loss: 6.336461067199707 = 0.03964252769947052 + 1.0 * 6.296818733215332
Epoch 900, val loss: 0.7633556723594666
Epoch 910, training loss: 6.341791152954102 = 0.0380377396941185 + 1.0 * 6.30375337600708
Epoch 910, val loss: 0.7676175236701965
Epoch 920, training loss: 6.336087226867676 = 0.036528270691633224 + 1.0 * 6.299559116363525
Epoch 920, val loss: 0.7717977166175842
Epoch 930, training loss: 6.33344030380249 = 0.03510737046599388 + 1.0 * 6.298333168029785
Epoch 930, val loss: 0.7759336829185486
Epoch 940, training loss: 6.327696800231934 = 0.03376152738928795 + 1.0 * 6.293935298919678
Epoch 940, val loss: 0.7800726294517517
Epoch 950, training loss: 6.326219081878662 = 0.032487403601408005 + 1.0 * 6.293731689453125
Epoch 950, val loss: 0.7842137813568115
Epoch 960, training loss: 6.328973770141602 = 0.03128223121166229 + 1.0 * 6.297691345214844
Epoch 960, val loss: 0.7883316278457642
Epoch 970, training loss: 6.3254289627075195 = 0.03014357201755047 + 1.0 * 6.295285224914551
Epoch 970, val loss: 0.7923027276992798
Epoch 980, training loss: 6.320221900939941 = 0.02906801365315914 + 1.0 * 6.291153907775879
Epoch 980, val loss: 0.7962595820426941
Epoch 990, training loss: 6.317948818206787 = 0.028047051280736923 + 1.0 * 6.2899017333984375
Epoch 990, val loss: 0.800207257270813
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8392198207696363
The final CL Acc:0.81111, 0.01600, The final GNN Acc:0.83975, 0.00114
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10604])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.543417930603027 = 1.9465898275375366 + 1.0 * 8.59682846069336
Epoch 0, val loss: 1.9440232515335083
Epoch 10, training loss: 10.532483100891113 = 1.935976266860962 + 1.0 * 8.59650707244873
Epoch 10, val loss: 1.9334580898284912
Epoch 20, training loss: 10.516602516174316 = 1.9226685762405396 + 1.0 * 8.593934059143066
Epoch 20, val loss: 1.9195832014083862
Epoch 30, training loss: 10.479584693908691 = 1.9041885137557983 + 1.0 * 8.575396537780762
Epoch 30, val loss: 1.9000080823898315
Epoch 40, training loss: 10.354763984680176 = 1.8803428411483765 + 1.0 * 8.474421501159668
Epoch 40, val loss: 1.875886082649231
Epoch 50, training loss: 9.973917007446289 = 1.8547704219818115 + 1.0 * 8.119146347045898
Epoch 50, val loss: 1.8517512083053589
Epoch 60, training loss: 9.662203788757324 = 1.833000659942627 + 1.0 * 7.829203128814697
Epoch 60, val loss: 1.8321162462234497
Epoch 70, training loss: 9.172554016113281 = 1.818513035774231 + 1.0 * 7.354040622711182
Epoch 70, val loss: 1.8185874223709106
Epoch 80, training loss: 8.833456993103027 = 1.809138298034668 + 1.0 * 7.024318695068359
Epoch 80, val loss: 1.8094592094421387
Epoch 90, training loss: 8.69575023651123 = 1.7972794771194458 + 1.0 * 6.898470401763916
Epoch 90, val loss: 1.797876000404358
Epoch 100, training loss: 8.604714393615723 = 1.7819582223892212 + 1.0 * 6.822755813598633
Epoch 100, val loss: 1.7841508388519287
Epoch 110, training loss: 8.535465240478516 = 1.7673544883728027 + 1.0 * 6.768110275268555
Epoch 110, val loss: 1.7713285684585571
Epoch 120, training loss: 8.46706485748291 = 1.7535960674285889 + 1.0 * 6.7134690284729
Epoch 120, val loss: 1.759042501449585
Epoch 130, training loss: 8.404373168945312 = 1.7388194799423218 + 1.0 * 6.665554046630859
Epoch 130, val loss: 1.7460817098617554
Epoch 140, training loss: 8.348450660705566 = 1.7220573425292969 + 1.0 * 6.6263933181762695
Epoch 140, val loss: 1.7317932844161987
Epoch 150, training loss: 8.29830551147461 = 1.7020504474639893 + 1.0 * 6.596255302429199
Epoch 150, val loss: 1.715139389038086
Epoch 160, training loss: 8.248209953308105 = 1.67807936668396 + 1.0 * 6.570130825042725
Epoch 160, val loss: 1.6952052116394043
Epoch 170, training loss: 8.2012939453125 = 1.6497312784194946 + 1.0 * 6.551562786102295
Epoch 170, val loss: 1.6716972589492798
Epoch 180, training loss: 8.145964622497559 = 1.6170767545700073 + 1.0 * 6.528887748718262
Epoch 180, val loss: 1.6445391178131104
Epoch 190, training loss: 8.091012954711914 = 1.579131007194519 + 1.0 * 6.5118818283081055
Epoch 190, val loss: 1.613119125366211
Epoch 200, training loss: 8.033149719238281 = 1.5361965894699097 + 1.0 * 6.49695348739624
Epoch 200, val loss: 1.5776373147964478
Epoch 210, training loss: 7.971775531768799 = 1.488429069519043 + 1.0 * 6.483346462249756
Epoch 210, val loss: 1.5386189222335815
Epoch 220, training loss: 7.912811756134033 = 1.4368911981582642 + 1.0 * 6.475920677185059
Epoch 220, val loss: 1.4973140954971313
Epoch 230, training loss: 7.8466291427612305 = 1.3840380907058716 + 1.0 * 6.462591171264648
Epoch 230, val loss: 1.4560281038284302
Epoch 240, training loss: 7.784083843231201 = 1.3309879302978516 + 1.0 * 6.45309591293335
Epoch 240, val loss: 1.4157723188400269
Epoch 250, training loss: 7.726265907287598 = 1.279196858406067 + 1.0 * 6.44706916809082
Epoch 250, val loss: 1.378007173538208
Epoch 260, training loss: 7.667637825012207 = 1.2293466329574585 + 1.0 * 6.438291072845459
Epoch 260, val loss: 1.342702865600586
Epoch 270, training loss: 7.611921787261963 = 1.1809606552124023 + 1.0 * 6.4309611320495605
Epoch 270, val loss: 1.3095685243606567
Epoch 280, training loss: 7.558965682983398 = 1.1337229013442993 + 1.0 * 6.425242900848389
Epoch 280, val loss: 1.2779289484024048
Epoch 290, training loss: 7.513358116149902 = 1.0870057344436646 + 1.0 * 6.426352500915527
Epoch 290, val loss: 1.247253656387329
Epoch 300, training loss: 7.4569501876831055 = 1.0414636135101318 + 1.0 * 6.415486812591553
Epoch 300, val loss: 1.2175266742706299
Epoch 310, training loss: 7.410171031951904 = 0.996587872505188 + 1.0 * 6.413583278656006
Epoch 310, val loss: 1.1884711980819702
Epoch 320, training loss: 7.3594746589660645 = 0.9529867768287659 + 1.0 * 6.406487941741943
Epoch 320, val loss: 1.160056710243225
Epoch 330, training loss: 7.311594009399414 = 0.9103881120681763 + 1.0 * 6.401206016540527
Epoch 330, val loss: 1.1325563192367554
Epoch 340, training loss: 7.267002582550049 = 0.8690282702445984 + 1.0 * 6.397974491119385
Epoch 340, val loss: 1.106019139289856
Epoch 350, training loss: 7.2234344482421875 = 0.8294012546539307 + 1.0 * 6.394033432006836
Epoch 350, val loss: 1.0808887481689453
Epoch 360, training loss: 7.181183338165283 = 0.7912208437919617 + 1.0 * 6.389962673187256
Epoch 360, val loss: 1.0569300651550293
Epoch 370, training loss: 7.153723239898682 = 0.7544204592704773 + 1.0 * 6.399302959442139
Epoch 370, val loss: 1.0343387126922607
Epoch 380, training loss: 7.1088361740112305 = 0.7199459671974182 + 1.0 * 6.388890266418457
Epoch 380, val loss: 1.0135618448257446
Epoch 390, training loss: 7.067068099975586 = 0.6873226165771484 + 1.0 * 6.3797454833984375
Epoch 390, val loss: 0.9945386052131653
Epoch 400, training loss: 7.034139156341553 = 0.6562612652778625 + 1.0 * 6.377877712249756
Epoch 400, val loss: 0.9770263433456421
Epoch 410, training loss: 7.004101753234863 = 0.6269829273223877 + 1.0 * 6.377119064331055
Epoch 410, val loss: 0.961227297782898
Epoch 420, training loss: 6.972813606262207 = 0.5995767116546631 + 1.0 * 6.373236656188965
Epoch 420, val loss: 0.9473119974136353
Epoch 430, training loss: 6.943343639373779 = 0.5736594796180725 + 1.0 * 6.369684219360352
Epoch 430, val loss: 0.9348072409629822
Epoch 440, training loss: 6.91654109954834 = 0.5492110848426819 + 1.0 * 6.367330074310303
Epoch 440, val loss: 0.9238066673278809
Epoch 450, training loss: 6.89212703704834 = 0.526282548904419 + 1.0 * 6.365844249725342
Epoch 450, val loss: 0.9144207239151001
Epoch 460, training loss: 6.866249084472656 = 0.5045314431190491 + 1.0 * 6.361717700958252
Epoch 460, val loss: 0.9062548279762268
Epoch 470, training loss: 6.855142116546631 = 0.4838379919528961 + 1.0 * 6.371304035186768
Epoch 470, val loss: 0.8994089365005493
Epoch 480, training loss: 6.823292255401611 = 0.46445998549461365 + 1.0 * 6.358832359313965
Epoch 480, val loss: 0.8938636779785156
Epoch 490, training loss: 6.80233907699585 = 0.4461477994918823 + 1.0 * 6.356191158294678
Epoch 490, val loss: 0.8894582390785217
Epoch 500, training loss: 6.782594680786133 = 0.4287071228027344 + 1.0 * 6.353887557983398
Epoch 500, val loss: 0.8859722018241882
Epoch 510, training loss: 6.762585639953613 = 0.41211363673210144 + 1.0 * 6.3504719734191895
Epoch 510, val loss: 0.8834857940673828
Epoch 520, training loss: 6.7507643699646 = 0.39634010195732117 + 1.0 * 6.354424476623535
Epoch 520, val loss: 0.8818511366844177
Epoch 530, training loss: 6.728355884552002 = 0.3812653124332428 + 1.0 * 6.347090721130371
Epoch 530, val loss: 0.8810067772865295
Epoch 540, training loss: 6.7111005783081055 = 0.36674946546554565 + 1.0 * 6.344351291656494
Epoch 540, val loss: 0.8807356953620911
Epoch 550, training loss: 6.705808639526367 = 0.3526613712310791 + 1.0 * 6.353147506713867
Epoch 550, val loss: 0.881065845489502
Epoch 560, training loss: 6.680935382843018 = 0.3390999734401703 + 1.0 * 6.3418354988098145
Epoch 560, val loss: 0.8818110227584839
Epoch 570, training loss: 6.666056156158447 = 0.32582393288612366 + 1.0 * 6.3402323722839355
Epoch 570, val loss: 0.8830386400222778
Epoch 580, training loss: 6.656213760375977 = 0.3127589523792267 + 1.0 * 6.343454837799072
Epoch 580, val loss: 0.8845516443252563
Epoch 590, training loss: 6.636070728302002 = 0.2998238801956177 + 1.0 * 6.336246967315674
Epoch 590, val loss: 0.886437714099884
Epoch 600, training loss: 6.621793746948242 = 0.28691548109054565 + 1.0 * 6.334878444671631
Epoch 600, val loss: 0.8885975480079651
Epoch 610, training loss: 6.611412048339844 = 0.2739647328853607 + 1.0 * 6.337447166442871
Epoch 610, val loss: 0.8909627795219421
Epoch 620, training loss: 6.594473361968994 = 0.2610134780406952 + 1.0 * 6.333459854125977
Epoch 620, val loss: 0.8936141729354858
Epoch 630, training loss: 6.589723110198975 = 0.24808910489082336 + 1.0 * 6.3416337966918945
Epoch 630, val loss: 0.8964639902114868
Epoch 640, training loss: 6.565582752227783 = 0.23531854152679443 + 1.0 * 6.330264091491699
Epoch 640, val loss: 0.8995487689971924
Epoch 650, training loss: 6.548737525939941 = 0.22270752489566803 + 1.0 * 6.3260297775268555
Epoch 650, val loss: 0.9029330611228943
Epoch 660, training loss: 6.538741588592529 = 0.21036329865455627 + 1.0 * 6.328378200531006
Epoch 660, val loss: 0.9067022204399109
Epoch 670, training loss: 6.532467842102051 = 0.1984618455171585 + 1.0 * 6.334005832672119
Epoch 670, val loss: 0.9109658002853394
Epoch 680, training loss: 6.51067590713501 = 0.18716150522232056 + 1.0 * 6.323514461517334
Epoch 680, val loss: 0.9155319929122925
Epoch 690, training loss: 6.4985551834106445 = 0.17643249034881592 + 1.0 * 6.322122573852539
Epoch 690, val loss: 0.9204011559486389
Epoch 700, training loss: 6.49038553237915 = 0.16630131006240845 + 1.0 * 6.324084281921387
Epoch 700, val loss: 0.9257519841194153
Epoch 710, training loss: 6.477261543273926 = 0.15680666267871857 + 1.0 * 6.320455074310303
Epoch 710, val loss: 0.931528627872467
Epoch 720, training loss: 6.4669928550720215 = 0.1479208618402481 + 1.0 * 6.3190717697143555
Epoch 720, val loss: 0.9375925660133362
Epoch 730, training loss: 6.45825719833374 = 0.13965123891830444 + 1.0 * 6.318605899810791
Epoch 730, val loss: 0.9439924359321594
Epoch 740, training loss: 6.447650909423828 = 0.1319112777709961 + 1.0 * 6.315739631652832
Epoch 740, val loss: 0.9506154656410217
Epoch 750, training loss: 6.450293064117432 = 0.12467729300260544 + 1.0 * 6.325615882873535
Epoch 750, val loss: 0.9575200080871582
Epoch 760, training loss: 6.435550212860107 = 0.11794494092464447 + 1.0 * 6.317605495452881
Epoch 760, val loss: 0.9646483063697815
Epoch 770, training loss: 6.424270153045654 = 0.11168668419122696 + 1.0 * 6.3125834465026855
Epoch 770, val loss: 0.9718092083930969
Epoch 780, training loss: 6.419193267822266 = 0.10582077503204346 + 1.0 * 6.313372611999512
Epoch 780, val loss: 0.9791461229324341
Epoch 790, training loss: 6.416677474975586 = 0.1003175750374794 + 1.0 * 6.316359996795654
Epoch 790, val loss: 0.9866840243339539
Epoch 800, training loss: 6.408003330230713 = 0.09517664462327957 + 1.0 * 6.312826633453369
Epoch 800, val loss: 0.9942538738250732
Epoch 810, training loss: 6.400022983551025 = 0.09037946909666061 + 1.0 * 6.309643745422363
Epoch 810, val loss: 1.0017802715301514
Epoch 820, training loss: 6.395975589752197 = 0.08586518466472626 + 1.0 * 6.310110569000244
Epoch 820, val loss: 1.009423851966858
Epoch 830, training loss: 6.388043403625488 = 0.08163340389728546 + 1.0 * 6.30640983581543
Epoch 830, val loss: 1.0170899629592896
Epoch 840, training loss: 6.384117603302002 = 0.07765985280275345 + 1.0 * 6.30645751953125
Epoch 840, val loss: 1.0247770547866821
Epoch 850, training loss: 6.383161544799805 = 0.07392982393503189 + 1.0 * 6.309231758117676
Epoch 850, val loss: 1.0324660539627075
Epoch 860, training loss: 6.381017208099365 = 0.07046012580394745 + 1.0 * 6.310556888580322
Epoch 860, val loss: 1.0399835109710693
Epoch 870, training loss: 6.369515895843506 = 0.06721583753824234 + 1.0 * 6.302299976348877
Epoch 870, val loss: 1.047511339187622
Epoch 880, training loss: 6.367183685302734 = 0.06416039913892746 + 1.0 * 6.303023338317871
Epoch 880, val loss: 1.0549755096435547
Epoch 890, training loss: 6.370572566986084 = 0.061294928193092346 + 1.0 * 6.309277534484863
Epoch 890, val loss: 1.062372088432312
Epoch 900, training loss: 6.3595476150512695 = 0.05859575420618057 + 1.0 * 6.300951957702637
Epoch 900, val loss: 1.0697989463806152
Epoch 910, training loss: 6.354996204376221 = 0.05606541037559509 + 1.0 * 6.298930644989014
Epoch 910, val loss: 1.0770517587661743
Epoch 920, training loss: 6.3521223068237305 = 0.05366946756839752 + 1.0 * 6.298452854156494
Epoch 920, val loss: 1.0843600034713745
Epoch 930, training loss: 6.354608535766602 = 0.05141812190413475 + 1.0 * 6.303190231323242
Epoch 930, val loss: 1.0915687084197998
Epoch 940, training loss: 6.346895694732666 = 0.04930169880390167 + 1.0 * 6.29759407043457
Epoch 940, val loss: 1.098686933517456
Epoch 950, training loss: 6.345370292663574 = 0.04731052741408348 + 1.0 * 6.298059940338135
Epoch 950, val loss: 1.1056640148162842
Epoch 960, training loss: 6.3419013023376465 = 0.045425817370414734 + 1.0 * 6.296475410461426
Epoch 960, val loss: 1.1126657724380493
Epoch 970, training loss: 6.337289810180664 = 0.04364418610930443 + 1.0 * 6.293645858764648
Epoch 970, val loss: 1.1196160316467285
Epoch 980, training loss: 6.3457794189453125 = 0.041961874812841415 + 1.0 * 6.3038177490234375
Epoch 980, val loss: 1.1264474391937256
Epoch 990, training loss: 6.340846061706543 = 0.04036957398056984 + 1.0 * 6.300476551055908
Epoch 990, val loss: 1.1331950426101685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 10.565378189086914 = 1.9685616493225098 + 1.0 * 8.596817016601562
Epoch 0, val loss: 1.972593903541565
Epoch 10, training loss: 10.554490089416504 = 1.9580050706863403 + 1.0 * 8.596485137939453
Epoch 10, val loss: 1.9624812602996826
Epoch 20, training loss: 10.538647651672363 = 1.9449052810668945 + 1.0 * 8.593742370605469
Epoch 20, val loss: 1.949415683746338
Epoch 30, training loss: 10.499638557434082 = 1.9264326095581055 + 1.0 * 8.573205947875977
Epoch 30, val loss: 1.9305813312530518
Epoch 40, training loss: 10.36536979675293 = 1.9022282361984253 + 1.0 * 8.463141441345215
Epoch 40, val loss: 1.907023549079895
Epoch 50, training loss: 9.989673614501953 = 1.875795841217041 + 1.0 * 8.11387825012207
Epoch 50, val loss: 1.881882905960083
Epoch 60, training loss: 9.722347259521484 = 1.851383924484253 + 1.0 * 7.8709635734558105
Epoch 60, val loss: 1.8589674234390259
Epoch 70, training loss: 9.266447067260742 = 1.8325634002685547 + 1.0 * 7.433884143829346
Epoch 70, val loss: 1.841171383857727
Epoch 80, training loss: 8.946995735168457 = 1.8185226917266846 + 1.0 * 7.128473281860352
Epoch 80, val loss: 1.8273040056228638
Epoch 90, training loss: 8.729433059692383 = 1.8051698207855225 + 1.0 * 6.9242634773254395
Epoch 90, val loss: 1.8141423463821411
Epoch 100, training loss: 8.606124877929688 = 1.7913508415222168 + 1.0 * 6.814774036407471
Epoch 100, val loss: 1.801138162612915
Epoch 110, training loss: 8.527900695800781 = 1.7779484987258911 + 1.0 * 6.74995231628418
Epoch 110, val loss: 1.7882180213928223
Epoch 120, training loss: 8.45561695098877 = 1.765752911567688 + 1.0 * 6.689864158630371
Epoch 120, val loss: 1.7760170698165894
Epoch 130, training loss: 8.402762413024902 = 1.7536762952804565 + 1.0 * 6.649085998535156
Epoch 130, val loss: 1.764215111732483
Epoch 140, training loss: 8.348320007324219 = 1.7404128313064575 + 1.0 * 6.607907295227051
Epoch 140, val loss: 1.7522811889648438
Epoch 150, training loss: 8.300085067749023 = 1.7252262830734253 + 1.0 * 6.574859142303467
Epoch 150, val loss: 1.7395731210708618
Epoch 160, training loss: 8.257198333740234 = 1.7076818943023682 + 1.0 * 6.549516201019287
Epoch 160, val loss: 1.7252483367919922
Epoch 170, training loss: 8.215381622314453 = 1.6872968673706055 + 1.0 * 6.5280842781066895
Epoch 170, val loss: 1.708548665046692
Epoch 180, training loss: 8.173898696899414 = 1.6633158922195435 + 1.0 * 6.510582447052002
Epoch 180, val loss: 1.6888339519500732
Epoch 190, training loss: 8.131597518920898 = 1.6352269649505615 + 1.0 * 6.496370792388916
Epoch 190, val loss: 1.6655960083007812
Epoch 200, training loss: 8.084630012512207 = 1.60258948802948 + 1.0 * 6.482040882110596
Epoch 200, val loss: 1.638640284538269
Epoch 210, training loss: 8.03730297088623 = 1.5649452209472656 + 1.0 * 6.472357749938965
Epoch 210, val loss: 1.607584834098816
Epoch 220, training loss: 7.984731197357178 = 1.5231590270996094 + 1.0 * 6.461572170257568
Epoch 220, val loss: 1.5732638835906982
Epoch 230, training loss: 7.928936958312988 = 1.47756028175354 + 1.0 * 6.451376438140869
Epoch 230, val loss: 1.5359736680984497
Epoch 240, training loss: 7.871936798095703 = 1.4284884929656982 + 1.0 * 6.443448066711426
Epoch 240, val loss: 1.496264100074768
Epoch 250, training loss: 7.818386077880859 = 1.3769489526748657 + 1.0 * 6.441437244415283
Epoch 250, val loss: 1.4552686214447021
Epoch 260, training loss: 7.755403995513916 = 1.3252663612365723 + 1.0 * 6.430137634277344
Epoch 260, val loss: 1.4148614406585693
Epoch 270, training loss: 7.700221061706543 = 1.2740850448608398 + 1.0 * 6.426136016845703
Epoch 270, val loss: 1.3756396770477295
Epoch 280, training loss: 7.642935276031494 = 1.2242870330810547 + 1.0 * 6.4186482429504395
Epoch 280, val loss: 1.3382147550582886
Epoch 290, training loss: 7.590689659118652 = 1.1762017011642456 + 1.0 * 6.414487838745117
Epoch 290, val loss: 1.3030864000320435
Epoch 300, training loss: 7.540401458740234 = 1.1306781768798828 + 1.0 * 6.409723281860352
Epoch 300, val loss: 1.2706942558288574
Epoch 310, training loss: 7.493099689483643 = 1.0878920555114746 + 1.0 * 6.405207633972168
Epoch 310, val loss: 1.2409809827804565
Epoch 320, training loss: 7.447103977203369 = 1.047819972038269 + 1.0 * 6.3992838859558105
Epoch 320, val loss: 1.2137771844863892
Epoch 330, training loss: 7.4050188064575195 = 1.010347843170166 + 1.0 * 6.3946709632873535
Epoch 330, val loss: 1.188978672027588
Epoch 340, training loss: 7.366216659545898 = 0.9753754138946533 + 1.0 * 6.390841007232666
Epoch 340, val loss: 1.1662565469741821
Epoch 350, training loss: 7.3279900550842285 = 0.9421507716178894 + 1.0 * 6.385839462280273
Epoch 350, val loss: 1.144970417022705
Epoch 360, training loss: 7.294002056121826 = 0.9104523658752441 + 1.0 * 6.383549690246582
Epoch 360, val loss: 1.125012993812561
Epoch 370, training loss: 7.260045051574707 = 0.8802070021629333 + 1.0 * 6.379837989807129
Epoch 370, val loss: 1.106322169303894
Epoch 380, training loss: 7.226406097412109 = 0.8507611751556396 + 1.0 * 6.375645160675049
Epoch 380, val loss: 1.088309645652771
Epoch 390, training loss: 7.194953918457031 = 0.8217896223068237 + 1.0 * 6.373164176940918
Epoch 390, val loss: 1.0708454847335815
Epoch 400, training loss: 7.1627607345581055 = 0.7933480739593506 + 1.0 * 6.369412899017334
Epoch 400, val loss: 1.0538495779037476
Epoch 410, training loss: 7.140256881713867 = 0.7650551199913025 + 1.0 * 6.37520170211792
Epoch 410, val loss: 1.0371766090393066
Epoch 420, training loss: 7.1011881828308105 = 0.7370219230651855 + 1.0 * 6.364166259765625
Epoch 420, val loss: 1.020743727684021
Epoch 430, training loss: 7.070149898529053 = 0.7089799642562866 + 1.0 * 6.361169815063477
Epoch 430, val loss: 1.004539966583252
Epoch 440, training loss: 7.040124893188477 = 0.6807542443275452 + 1.0 * 6.359370708465576
Epoch 440, val loss: 0.9884112477302551
Epoch 450, training loss: 7.009729862213135 = 0.6523714065551758 + 1.0 * 6.357358455657959
Epoch 450, val loss: 0.9723764061927795
Epoch 460, training loss: 6.980887413024902 = 0.6240522861480713 + 1.0 * 6.35683536529541
Epoch 460, val loss: 0.9566901326179504
Epoch 470, training loss: 6.948522090911865 = 0.5956292152404785 + 1.0 * 6.352892875671387
Epoch 470, val loss: 0.9409143924713135
Epoch 480, training loss: 6.920647621154785 = 0.5670478343963623 + 1.0 * 6.353600025177002
Epoch 480, val loss: 0.925268828868866
Epoch 490, training loss: 6.898008823394775 = 0.5385473966598511 + 1.0 * 6.359461307525635
Epoch 490, val loss: 0.909969687461853
Epoch 500, training loss: 6.860734939575195 = 0.5104289054870605 + 1.0 * 6.350306034088135
Epoch 500, val loss: 0.8950163722038269
Epoch 510, training loss: 6.828548431396484 = 0.4826132655143738 + 1.0 * 6.345935344696045
Epoch 510, val loss: 0.8804827332496643
Epoch 520, training loss: 6.804134368896484 = 0.4551904797554016 + 1.0 * 6.348943710327148
Epoch 520, val loss: 0.8664593696594238
Epoch 530, training loss: 6.780845642089844 = 0.42851170897483826 + 1.0 * 6.352334022521973
Epoch 530, val loss: 0.8532529473304749
Epoch 540, training loss: 6.7453694343566895 = 0.4029492139816284 + 1.0 * 6.3424201011657715
Epoch 540, val loss: 0.8410456776618958
Epoch 550, training loss: 6.717766284942627 = 0.37843015789985657 + 1.0 * 6.339335918426514
Epoch 550, val loss: 0.829962968826294
Epoch 560, training loss: 6.706606864929199 = 0.3550916612148285 + 1.0 * 6.351515293121338
Epoch 560, val loss: 0.8200686573982239
Epoch 570, training loss: 6.669895172119141 = 0.33319926261901855 + 1.0 * 6.336696147918701
Epoch 570, val loss: 0.8115406036376953
Epoch 580, training loss: 6.64673376083374 = 0.3125911056995392 + 1.0 * 6.334142684936523
Epoch 580, val loss: 0.8043410181999207
Epoch 590, training loss: 6.631013870239258 = 0.2931852638721466 + 1.0 * 6.337828636169434
Epoch 590, val loss: 0.7982722520828247
Epoch 600, training loss: 6.60914421081543 = 0.2750951945781708 + 1.0 * 6.334049224853516
Epoch 600, val loss: 0.7933566570281982
Epoch 610, training loss: 6.5886077880859375 = 0.2581130862236023 + 1.0 * 6.3304948806762695
Epoch 610, val loss: 0.7895205020904541
Epoch 620, training loss: 6.57633113861084 = 0.24225476384162903 + 1.0 * 6.334076404571533
Epoch 620, val loss: 0.7866525053977966
Epoch 630, training loss: 6.555337429046631 = 0.22750231623649597 + 1.0 * 6.3278350830078125
Epoch 630, val loss: 0.7845509648323059
Epoch 640, training loss: 6.540895462036133 = 0.2136697769165039 + 1.0 * 6.327225685119629
Epoch 640, val loss: 0.7833147644996643
Epoch 650, training loss: 6.5289411544799805 = 0.20068864524364471 + 1.0 * 6.32825231552124
Epoch 650, val loss: 0.7828834056854248
Epoch 660, training loss: 6.512209415435791 = 0.1885339915752411 + 1.0 * 6.323675632476807
Epoch 660, val loss: 0.7829998731613159
Epoch 670, training loss: 6.501185894012451 = 0.1771163046360016 + 1.0 * 6.324069499969482
Epoch 670, val loss: 0.7837682366371155
Epoch 680, training loss: 6.485856533050537 = 0.1664179414510727 + 1.0 * 6.319438457489014
Epoch 680, val loss: 0.7849589586257935
Epoch 690, training loss: 6.475891590118408 = 0.15636688470840454 + 1.0 * 6.319524765014648
Epoch 690, val loss: 0.7866271138191223
Epoch 700, training loss: 6.4674859046936035 = 0.1469603031873703 + 1.0 * 6.320525646209717
Epoch 700, val loss: 0.7887997031211853
Epoch 710, training loss: 6.461272239685059 = 0.1382014900445938 + 1.0 * 6.323070526123047
Epoch 710, val loss: 0.791243314743042
Epoch 720, training loss: 6.445805549621582 = 0.1300686150789261 + 1.0 * 6.315736770629883
Epoch 720, val loss: 0.793926477432251
Epoch 730, training loss: 6.43735933303833 = 0.12246368825435638 + 1.0 * 6.3148956298828125
Epoch 730, val loss: 0.7970358729362488
Epoch 740, training loss: 6.4358344078063965 = 0.11535608023405075 + 1.0 * 6.320478439331055
Epoch 740, val loss: 0.8005262613296509
Epoch 750, training loss: 6.421114444732666 = 0.10872945934534073 + 1.0 * 6.312385082244873
Epoch 750, val loss: 0.804021954536438
Epoch 760, training loss: 6.415533542633057 = 0.10255470126867294 + 1.0 * 6.312978744506836
Epoch 760, val loss: 0.8079376220703125
Epoch 770, training loss: 6.40772819519043 = 0.09679988026618958 + 1.0 * 6.3109283447265625
Epoch 770, val loss: 0.8119933009147644
Epoch 780, training loss: 6.402151107788086 = 0.09145504236221313 + 1.0 * 6.310696125030518
Epoch 780, val loss: 0.8161659240722656
Epoch 790, training loss: 6.393497943878174 = 0.08647118508815765 + 1.0 * 6.3070268630981445
Epoch 790, val loss: 0.8203619122505188
Epoch 800, training loss: 6.388843059539795 = 0.08180864155292511 + 1.0 * 6.307034492492676
Epoch 800, val loss: 0.8248676657676697
Epoch 810, training loss: 6.387821197509766 = 0.0774482935667038 + 1.0 * 6.310372829437256
Epoch 810, val loss: 0.8294759392738342
Epoch 820, training loss: 6.378515720367432 = 0.07340893149375916 + 1.0 * 6.3051066398620605
Epoch 820, val loss: 0.8341996073722839
Epoch 830, training loss: 6.37483549118042 = 0.06963144242763519 + 1.0 * 6.305203914642334
Epoch 830, val loss: 0.8387927412986755
Epoch 840, training loss: 6.369497776031494 = 0.06611226499080658 + 1.0 * 6.3033857345581055
Epoch 840, val loss: 0.843694269657135
Epoch 850, training loss: 6.364063739776611 = 0.06281451135873795 + 1.0 * 6.301249027252197
Epoch 850, val loss: 0.8484594821929932
Epoch 860, training loss: 6.361715793609619 = 0.0597342886030674 + 1.0 * 6.301981449127197
Epoch 860, val loss: 0.85340416431427
Epoch 870, training loss: 6.360274314880371 = 0.05685753747820854 + 1.0 * 6.303416728973389
Epoch 870, val loss: 0.8583922982215881
Epoch 880, training loss: 6.352626800537109 = 0.054177310317754745 + 1.0 * 6.298449516296387
Epoch 880, val loss: 0.8631293773651123
Epoch 890, training loss: 6.34929895401001 = 0.05166107043623924 + 1.0 * 6.297637939453125
Epoch 890, val loss: 0.8679561614990234
Epoch 900, training loss: 6.3520588874816895 = 0.049311794340610504 + 1.0 * 6.3027472496032715
Epoch 900, val loss: 0.87297523021698
Epoch 910, training loss: 6.343252182006836 = 0.047115568071603775 + 1.0 * 6.296136379241943
Epoch 910, val loss: 0.8776857852935791
Epoch 920, training loss: 6.341050624847412 = 0.045054491609334946 + 1.0 * 6.295996189117432
Epoch 920, val loss: 0.8823626637458801
Epoch 930, training loss: 6.3405866622924805 = 0.043120358139276505 + 1.0 * 6.297466278076172
Epoch 930, val loss: 0.8873242139816284
Epoch 940, training loss: 6.333564758300781 = 0.041308145970106125 + 1.0 * 6.292256832122803
Epoch 940, val loss: 0.8919519782066345
Epoch 950, training loss: 6.331722259521484 = 0.03960051015019417 + 1.0 * 6.292121887207031
Epoch 950, val loss: 0.8965233564376831
Epoch 960, training loss: 6.336173057556152 = 0.037987757474184036 + 1.0 * 6.298185348510742
Epoch 960, val loss: 0.9012712240219116
Epoch 970, training loss: 6.330430030822754 = 0.036480359733104706 + 1.0 * 6.293949604034424
Epoch 970, val loss: 0.9060296416282654
Epoch 980, training loss: 6.3278069496154785 = 0.035053130239248276 + 1.0 * 6.29275369644165
Epoch 980, val loss: 0.9105350971221924
Epoch 990, training loss: 6.3224687576293945 = 0.03371385857462883 + 1.0 * 6.288754940032959
Epoch 990, val loss: 0.9149687886238098
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 10.538283348083496 = 1.9414886236190796 + 1.0 * 8.596795082092285
Epoch 0, val loss: 1.9354498386383057
Epoch 10, training loss: 10.527322769165039 = 1.9310277700424194 + 1.0 * 8.596295356750488
Epoch 10, val loss: 1.9254117012023926
Epoch 20, training loss: 10.509862899780273 = 1.9177250862121582 + 1.0 * 8.592138290405273
Epoch 20, val loss: 1.912048101425171
Epoch 30, training loss: 10.456886291503906 = 1.8991717100143433 + 1.0 * 8.557714462280273
Epoch 30, val loss: 1.8929853439331055
Epoch 40, training loss: 10.135148048400879 = 1.8773807287216187 + 1.0 * 8.257767677307129
Epoch 40, val loss: 1.8716577291488647
Epoch 50, training loss: 9.5518159866333 = 1.85918128490448 + 1.0 * 7.6926350593566895
Epoch 50, val loss: 1.8550827503204346
Epoch 60, training loss: 8.968283653259277 = 1.8474947214126587 + 1.0 * 7.120789051055908
Epoch 60, val loss: 1.8443621397018433
Epoch 70, training loss: 8.72933578491211 = 1.838220238685608 + 1.0 * 6.891115188598633
Epoch 70, val loss: 1.8357608318328857
Epoch 80, training loss: 8.584907531738281 = 1.827142357826233 + 1.0 * 6.75776481628418
Epoch 80, val loss: 1.8256556987762451
Epoch 90, training loss: 8.502249717712402 = 1.8148014545440674 + 1.0 * 6.687448024749756
Epoch 90, val loss: 1.8147445917129517
Epoch 100, training loss: 8.44058895111084 = 1.8026608228683472 + 1.0 * 6.637928009033203
Epoch 100, val loss: 1.804071307182312
Epoch 110, training loss: 8.38862419128418 = 1.791162133216858 + 1.0 * 6.597462177276611
Epoch 110, val loss: 1.7939475774765015
Epoch 120, training loss: 8.345916748046875 = 1.7801153659820557 + 1.0 * 6.565801620483398
Epoch 120, val loss: 1.7843239307403564
Epoch 130, training loss: 8.305838584899902 = 1.7688947916030884 + 1.0 * 6.536943435668945
Epoch 130, val loss: 1.7746258974075317
Epoch 140, training loss: 8.269679069519043 = 1.7566710710525513 + 1.0 * 6.513008117675781
Epoch 140, val loss: 1.7642003297805786
Epoch 150, training loss: 8.236396789550781 = 1.7427315711975098 + 1.0 * 6.49366569519043
Epoch 150, val loss: 1.7524811029434204
Epoch 160, training loss: 8.203096389770508 = 1.7268149852752686 + 1.0 * 6.47628116607666
Epoch 160, val loss: 1.7392024993896484
Epoch 170, training loss: 8.170937538146973 = 1.7084782123565674 + 1.0 * 6.462459087371826
Epoch 170, val loss: 1.7239279747009277
Epoch 180, training loss: 8.135741233825684 = 1.6870254278182983 + 1.0 * 6.448715686798096
Epoch 180, val loss: 1.705942988395691
Epoch 190, training loss: 8.09912395477295 = 1.6616178750991821 + 1.0 * 6.437506198883057
Epoch 190, val loss: 1.684707760810852
Epoch 200, training loss: 8.061619758605957 = 1.632198452949524 + 1.0 * 6.429421424865723
Epoch 200, val loss: 1.6602123975753784
Epoch 210, training loss: 8.019811630249023 = 1.5990715026855469 + 1.0 * 6.420739650726318
Epoch 210, val loss: 1.6327016353607178
Epoch 220, training loss: 7.973583698272705 = 1.5618866682052612 + 1.0 * 6.411696910858154
Epoch 220, val loss: 1.602002739906311
Epoch 230, training loss: 7.926748752593994 = 1.520624041557312 + 1.0 * 6.406124591827393
Epoch 230, val loss: 1.5682580471038818
Epoch 240, training loss: 7.877776145935059 = 1.4765889644622803 + 1.0 * 6.401187419891357
Epoch 240, val loss: 1.532865047454834
Epoch 250, training loss: 7.827458381652832 = 1.4310057163238525 + 1.0 * 6.3964524269104
Epoch 250, val loss: 1.4968875646591187
Epoch 260, training loss: 7.7738847732543945 = 1.3839248418807983 + 1.0 * 6.389959812164307
Epoch 260, val loss: 1.460498332977295
Epoch 270, training loss: 7.721639156341553 = 1.3357954025268555 + 1.0 * 6.385843753814697
Epoch 270, val loss: 1.4238859415054321
Epoch 280, training loss: 7.674498558044434 = 1.2879289388656616 + 1.0 * 6.386569499969482
Epoch 280, val loss: 1.38837468624115
Epoch 290, training loss: 7.621848106384277 = 1.2422504425048828 + 1.0 * 6.3795976638793945
Epoch 290, val loss: 1.3551784753799438
Epoch 300, training loss: 7.57306432723999 = 1.1978063583374023 + 1.0 * 6.375257968902588
Epoch 300, val loss: 1.3233052492141724
Epoch 310, training loss: 7.525325775146484 = 1.1545779705047607 + 1.0 * 6.3707475662231445
Epoch 310, val loss: 1.2926820516586304
Epoch 320, training loss: 7.479682922363281 = 1.1126668453216553 + 1.0 * 6.367015838623047
Epoch 320, val loss: 1.263375997543335
Epoch 330, training loss: 7.439876556396484 = 1.0724222660064697 + 1.0 * 6.3674540519714355
Epoch 330, val loss: 1.2355862855911255
Epoch 340, training loss: 7.398456573486328 = 1.0344716310501099 + 1.0 * 6.363985061645508
Epoch 340, val loss: 1.2098298072814941
Epoch 350, training loss: 7.359282970428467 = 0.9987022280693054 + 1.0 * 6.360580921173096
Epoch 350, val loss: 1.1858103275299072
Epoch 360, training loss: 7.3199663162231445 = 0.9646434783935547 + 1.0 * 6.35532283782959
Epoch 360, val loss: 1.1632531881332397
Epoch 370, training loss: 7.284117698669434 = 0.9318058490753174 + 1.0 * 6.352311611175537
Epoch 370, val loss: 1.1418427228927612
Epoch 380, training loss: 7.261831283569336 = 0.8998643159866333 + 1.0 * 6.361967086791992
Epoch 380, val loss: 1.1213655471801758
Epoch 390, training loss: 7.219324111938477 = 0.8689868450164795 + 1.0 * 6.350337505340576
Epoch 390, val loss: 1.1018191576004028
Epoch 400, training loss: 7.184227466583252 = 0.8385595083236694 + 1.0 * 6.345667839050293
Epoch 400, val loss: 1.082777738571167
Epoch 410, training loss: 7.155996799468994 = 0.8083568811416626 + 1.0 * 6.347640037536621
Epoch 410, val loss: 1.0641440153121948
Epoch 420, training loss: 7.121877670288086 = 0.7783628106117249 + 1.0 * 6.343514919281006
Epoch 420, val loss: 1.0460079908370972
Epoch 430, training loss: 7.087578773498535 = 0.7483035326004028 + 1.0 * 6.339275360107422
Epoch 430, val loss: 1.0281065702438354
Epoch 440, training loss: 7.059045314788818 = 0.7180823683738708 + 1.0 * 6.340962886810303
Epoch 440, val loss: 1.010406255722046
Epoch 450, training loss: 7.028346061706543 = 0.6880351305007935 + 1.0 * 6.340311050415039
Epoch 450, val loss: 0.9929466843605042
Epoch 460, training loss: 6.9928297996521 = 0.6583301424980164 + 1.0 * 6.334499835968018
Epoch 460, val loss: 0.9762803912162781
Epoch 470, training loss: 6.962109565734863 = 0.6286443471908569 + 1.0 * 6.333465099334717
Epoch 470, val loss: 0.9598194360733032
Epoch 480, training loss: 6.930191993713379 = 0.598961353302002 + 1.0 * 6.331230640411377
Epoch 480, val loss: 0.9437495470046997
Epoch 490, training loss: 6.912864685058594 = 0.5694707036018372 + 1.0 * 6.343393802642822
Epoch 490, val loss: 0.9280356168746948
Epoch 500, training loss: 6.872356414794922 = 0.540743887424469 + 1.0 * 6.331612586975098
Epoch 500, val loss: 0.9132971167564392
Epoch 510, training loss: 6.839562892913818 = 0.5126222372055054 + 1.0 * 6.326940536499023
Epoch 510, val loss: 0.8994088768959045
Epoch 520, training loss: 6.812539577484131 = 0.4851645231246948 + 1.0 * 6.3273749351501465
Epoch 520, val loss: 0.8863410949707031
Epoch 530, training loss: 6.786713600158691 = 0.45876604318618774 + 1.0 * 6.327947616577148
Epoch 530, val loss: 0.8744078874588013
Epoch 540, training loss: 6.757258415222168 = 0.4335419237613678 + 1.0 * 6.323716640472412
Epoch 540, val loss: 0.8638775944709778
Epoch 550, training loss: 6.734274864196777 = 0.40959253907203674 + 1.0 * 6.324682235717773
Epoch 550, val loss: 0.8547401428222656
Epoch 560, training loss: 6.7116312980651855 = 0.38709986209869385 + 1.0 * 6.324531555175781
Epoch 560, val loss: 0.8467977046966553
Epoch 570, training loss: 6.687754154205322 = 0.3661976456642151 + 1.0 * 6.321556568145752
Epoch 570, val loss: 0.8405719995498657
Epoch 580, training loss: 6.6651811599731445 = 0.34680765867233276 + 1.0 * 6.318373680114746
Epoch 580, val loss: 0.8355077505111694
Epoch 590, training loss: 6.645429611206055 = 0.3288574516773224 + 1.0 * 6.316572189331055
Epoch 590, val loss: 0.8318414092063904
Epoch 600, training loss: 6.630550384521484 = 0.31224432587623596 + 1.0 * 6.318305969238281
Epoch 600, val loss: 0.8293974995613098
Epoch 610, training loss: 6.618524551391602 = 0.2969679534435272 + 1.0 * 6.321556568145752
Epoch 610, val loss: 0.8278283476829529
Epoch 620, training loss: 6.595664024353027 = 0.28297531604766846 + 1.0 * 6.312688827514648
Epoch 620, val loss: 0.8276104927062988
Epoch 630, training loss: 6.580977439880371 = 0.27001503109931946 + 1.0 * 6.310962200164795
Epoch 630, val loss: 0.8282574415206909
Epoch 640, training loss: 6.5710296630859375 = 0.25794529914855957 + 1.0 * 6.313084602355957
Epoch 640, val loss: 0.8295740485191345
Epoch 650, training loss: 6.5583109855651855 = 0.24670742452144623 + 1.0 * 6.311603546142578
Epoch 650, val loss: 0.83136385679245
Epoch 660, training loss: 6.545888900756836 = 0.2362046241760254 + 1.0 * 6.3096842765808105
Epoch 660, val loss: 0.8337506055831909
Epoch 670, training loss: 6.537278652191162 = 0.22626245021820068 + 1.0 * 6.311016082763672
Epoch 670, val loss: 0.8364091515541077
Epoch 680, training loss: 6.524575710296631 = 0.21679624915122986 + 1.0 * 6.307779312133789
Epoch 680, val loss: 0.8394341468811035
Epoch 690, training loss: 6.512325286865234 = 0.20761753618717194 + 1.0 * 6.3047075271606445
Epoch 690, val loss: 0.8426079154014587
Epoch 700, training loss: 6.501253604888916 = 0.19859184324741364 + 1.0 * 6.302661895751953
Epoch 700, val loss: 0.8459872603416443
Epoch 710, training loss: 6.501944065093994 = 0.18960294127464294 + 1.0 * 6.312341213226318
Epoch 710, val loss: 0.8494444489479065
Epoch 720, training loss: 6.485393047332764 = 0.18060827255249023 + 1.0 * 6.304784774780273
Epoch 720, val loss: 0.8529280424118042
Epoch 730, training loss: 6.475395202636719 = 0.17154313623905182 + 1.0 * 6.303852081298828
Epoch 730, val loss: 0.8564600944519043
Epoch 740, training loss: 6.461361408233643 = 0.162371426820755 + 1.0 * 6.298989772796631
Epoch 740, val loss: 0.8601094484329224
Epoch 750, training loss: 6.456782817840576 = 0.1531110256910324 + 1.0 * 6.303671836853027
Epoch 750, val loss: 0.8638178706169128
Epoch 760, training loss: 6.44773530960083 = 0.14393652975559235 + 1.0 * 6.303798675537109
Epoch 760, val loss: 0.8675335645675659
Epoch 770, training loss: 6.4324870109558105 = 0.1349930316209793 + 1.0 * 6.297493934631348
Epoch 770, val loss: 0.8714006543159485
Epoch 780, training loss: 6.42190408706665 = 0.12635201215744019 + 1.0 * 6.2955522537231445
Epoch 780, val loss: 0.8755748271942139
Epoch 790, training loss: 6.412837982177734 = 0.1181100532412529 + 1.0 * 6.294727802276611
Epoch 790, val loss: 0.879865288734436
Epoch 800, training loss: 6.414970397949219 = 0.11037097871303558 + 1.0 * 6.304599285125732
Epoch 800, val loss: 0.8843554258346558
Epoch 810, training loss: 6.397510528564453 = 0.10323832929134369 + 1.0 * 6.294272422790527
Epoch 810, val loss: 0.8891022205352783
Epoch 820, training loss: 6.3906025886535645 = 0.09663423895835876 + 1.0 * 6.293968200683594
Epoch 820, val loss: 0.894027829170227
Epoch 830, training loss: 6.383118152618408 = 0.09058254957199097 + 1.0 * 6.292535781860352
Epoch 830, val loss: 0.8990446925163269
Epoch 840, training loss: 6.383450031280518 = 0.08504606038331985 + 1.0 * 6.298403739929199
Epoch 840, val loss: 0.9042061567306519
Epoch 850, training loss: 6.369471073150635 = 0.07999186217784882 + 1.0 * 6.2894792556762695
Epoch 850, val loss: 0.9095229506492615
Epoch 860, training loss: 6.363675117492676 = 0.07534665614366531 + 1.0 * 6.288328647613525
Epoch 860, val loss: 0.9148236513137817
Epoch 870, training loss: 6.368142604827881 = 0.07107605040073395 + 1.0 * 6.297066688537598
Epoch 870, val loss: 0.9201928973197937
Epoch 880, training loss: 6.358642101287842 = 0.06716613471508026 + 1.0 * 6.291475772857666
Epoch 880, val loss: 0.9254583716392517
Epoch 890, training loss: 6.350386619567871 = 0.06357729434967041 + 1.0 * 6.28680944442749
Epoch 890, val loss: 0.9308280944824219
Epoch 900, training loss: 6.349945068359375 = 0.06026380509138107 + 1.0 * 6.289681434631348
Epoch 900, val loss: 0.9362518191337585
Epoch 910, training loss: 6.342217445373535 = 0.057201534509658813 + 1.0 * 6.285016059875488
Epoch 910, val loss: 0.9415357708930969
Epoch 920, training loss: 6.33957576751709 = 0.05436444282531738 + 1.0 * 6.285211563110352
Epoch 920, val loss: 0.9469137191772461
Epoch 930, training loss: 6.342401504516602 = 0.051732275635004044 + 1.0 * 6.2906694412231445
Epoch 930, val loss: 0.9522641897201538
Epoch 940, training loss: 6.334414958953857 = 0.04928923025727272 + 1.0 * 6.285125732421875
Epoch 940, val loss: 0.9574497938156128
Epoch 950, training loss: 6.328861236572266 = 0.04701012745499611 + 1.0 * 6.281851291656494
Epoch 950, val loss: 0.9627711772918701
Epoch 960, training loss: 6.329124450683594 = 0.04488140717148781 + 1.0 * 6.284243106842041
Epoch 960, val loss: 0.9680684208869934
Epoch 970, training loss: 6.326594352722168 = 0.042895786464214325 + 1.0 * 6.283698558807373
Epoch 970, val loss: 0.9731928110122681
Epoch 980, training loss: 6.3224196434021 = 0.041039202362298965 + 1.0 * 6.281380653381348
Epoch 980, val loss: 0.9783511757850647
Epoch 990, training loss: 6.32309103012085 = 0.039305251091718674 + 1.0 * 6.283785820007324
Epoch 990, val loss: 0.983403205871582
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.812335266209805
The final CL Acc:0.78148, 0.02475, The final GNN Acc:0.81269, 0.00050
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13162])
remove edge: torch.Size([2, 7898])
updated graph: torch.Size([2, 10504])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.54387092590332 = 1.947074294090271 + 1.0 * 8.596796989440918
Epoch 0, val loss: 1.9456669092178345
Epoch 10, training loss: 10.532489776611328 = 1.9361306428909302 + 1.0 * 8.596359252929688
Epoch 10, val loss: 1.9347907304763794
Epoch 20, training loss: 10.515216827392578 = 1.9224143028259277 + 1.0 * 8.592803001403809
Epoch 20, val loss: 1.92123281955719
Epoch 30, training loss: 10.469852447509766 = 1.9032647609710693 + 1.0 * 8.566587448120117
Epoch 30, val loss: 1.9022678136825562
Epoch 40, training loss: 10.296255111694336 = 1.8788976669311523 + 1.0 * 8.417357444763184
Epoch 40, val loss: 1.8793959617614746
Epoch 50, training loss: 9.770896911621094 = 1.8520934581756592 + 1.0 * 7.9188032150268555
Epoch 50, val loss: 1.8543351888656616
Epoch 60, training loss: 9.335922241210938 = 1.8303734064102173 + 1.0 * 7.50554895401001
Epoch 60, val loss: 1.8346325159072876
Epoch 70, training loss: 8.976908683776855 = 1.8144781589508057 + 1.0 * 7.162430763244629
Epoch 70, val loss: 1.8199790716171265
Epoch 80, training loss: 8.799434661865234 = 1.8006372451782227 + 1.0 * 6.9987969398498535
Epoch 80, val loss: 1.8075592517852783
Epoch 90, training loss: 8.66507625579834 = 1.7842531204223633 + 1.0 * 6.880823135375977
Epoch 90, val loss: 1.7931514978408813
Epoch 100, training loss: 8.558695793151855 = 1.767930269241333 + 1.0 * 6.790765762329102
Epoch 100, val loss: 1.7788373231887817
Epoch 110, training loss: 8.472317695617676 = 1.752198576927185 + 1.0 * 6.720119476318359
Epoch 110, val loss: 1.7643080949783325
Epoch 120, training loss: 8.397828102111816 = 1.7343958616256714 + 1.0 * 6.6634321212768555
Epoch 120, val loss: 1.7474989891052246
Epoch 130, training loss: 8.33544921875 = 1.7134218215942383 + 1.0 * 6.6220269203186035
Epoch 130, val loss: 1.7282809019088745
Epoch 140, training loss: 8.277328491210938 = 1.6891367435455322 + 1.0 * 6.588191986083984
Epoch 140, val loss: 1.7066190242767334
Epoch 150, training loss: 8.222789764404297 = 1.6602997779846191 + 1.0 * 6.562489986419678
Epoch 150, val loss: 1.6812270879745483
Epoch 160, training loss: 8.163983345031738 = 1.6261065006256104 + 1.0 * 6.537876605987549
Epoch 160, val loss: 1.6516027450561523
Epoch 170, training loss: 8.102578163146973 = 1.585898518562317 + 1.0 * 6.516679286956787
Epoch 170, val loss: 1.6169214248657227
Epoch 180, training loss: 8.038881301879883 = 1.5395028591156006 + 1.0 * 6.499378204345703
Epoch 180, val loss: 1.5772056579589844
Epoch 190, training loss: 7.9714860916137695 = 1.4883897304534912 + 1.0 * 6.483096599578857
Epoch 190, val loss: 1.534029245376587
Epoch 200, training loss: 7.903222560882568 = 1.4339674711227417 + 1.0 * 6.469254970550537
Epoch 200, val loss: 1.4884604215621948
Epoch 210, training loss: 7.838724136352539 = 1.3776649236679077 + 1.0 * 6.461059093475342
Epoch 210, val loss: 1.441760540008545
Epoch 220, training loss: 7.771014213562012 = 1.3222792148590088 + 1.0 * 6.448735237121582
Epoch 220, val loss: 1.3960739374160767
Epoch 230, training loss: 7.708765983581543 = 1.268601655960083 + 1.0 * 6.440164089202881
Epoch 230, val loss: 1.3522295951843262
Epoch 240, training loss: 7.6508378982543945 = 1.2170900106430054 + 1.0 * 6.4337477684021
Epoch 240, val loss: 1.310287356376648
Epoch 250, training loss: 7.600282192230225 = 1.1681842803955078 + 1.0 * 6.432097911834717
Epoch 250, val loss: 1.2704335451126099
Epoch 260, training loss: 7.543313503265381 = 1.121565341949463 + 1.0 * 6.421748161315918
Epoch 260, val loss: 1.2325283288955688
Epoch 270, training loss: 7.489620208740234 = 1.0758092403411865 + 1.0 * 6.413810729980469
Epoch 270, val loss: 1.1953063011169434
Epoch 280, training loss: 7.43879508972168 = 1.0300854444503784 + 1.0 * 6.408709526062012
Epoch 280, val loss: 1.1578608751296997
Epoch 290, training loss: 7.390934944152832 = 0.9847016334533691 + 1.0 * 6.406233310699463
Epoch 290, val loss: 1.1206859350204468
Epoch 300, training loss: 7.338835716247559 = 0.9398547410964966 + 1.0 * 6.398981094360352
Epoch 300, val loss: 1.0840297937393188
Epoch 310, training loss: 7.29050350189209 = 0.8958224058151245 + 1.0 * 6.394680976867676
Epoch 310, val loss: 1.0477731227874756
Epoch 320, training loss: 7.242340087890625 = 0.8529037237167358 + 1.0 * 6.3894362449646
Epoch 320, val loss: 1.012783169746399
Epoch 330, training loss: 7.196927547454834 = 0.8112366795539856 + 1.0 * 6.385690689086914
Epoch 330, val loss: 0.9790326356887817
Epoch 340, training loss: 7.157065391540527 = 0.7712148427963257 + 1.0 * 6.385850429534912
Epoch 340, val loss: 0.9468898773193359
Epoch 350, training loss: 7.111559867858887 = 0.7334339618682861 + 1.0 * 6.3781256675720215
Epoch 350, val loss: 0.9169999957084656
Epoch 360, training loss: 7.072534561157227 = 0.6976693868637085 + 1.0 * 6.3748650550842285
Epoch 360, val loss: 0.8892560601234436
Epoch 370, training loss: 7.043347358703613 = 0.6641944646835327 + 1.0 * 6.379152774810791
Epoch 370, val loss: 0.8638932108879089
Epoch 380, training loss: 7.00188684463501 = 0.6331426501274109 + 1.0 * 6.368744373321533
Epoch 380, val loss: 0.8410844802856445
Epoch 390, training loss: 6.969583511352539 = 0.6038101315498352 + 1.0 * 6.3657732009887695
Epoch 390, val loss: 0.8202742338180542
Epoch 400, training loss: 6.939948081970215 = 0.5758620500564575 + 1.0 * 6.364086151123047
Epoch 400, val loss: 0.8011804223060608
Epoch 410, training loss: 6.914952754974365 = 0.5493730902671814 + 1.0 * 6.365579605102539
Epoch 410, val loss: 0.7840265035629272
Epoch 420, training loss: 6.881436824798584 = 0.5242308378219604 + 1.0 * 6.357205867767334
Epoch 420, val loss: 0.7684447169303894
Epoch 430, training loss: 6.855411529541016 = 0.4998747706413269 + 1.0 * 6.355536937713623
Epoch 430, val loss: 0.754092276096344
Epoch 440, training loss: 6.829926490783691 = 0.47603869438171387 + 1.0 * 6.353887557983398
Epoch 440, val loss: 0.7407874464988708
Epoch 450, training loss: 6.808298587799072 = 0.4527733623981476 + 1.0 * 6.355525016784668
Epoch 450, val loss: 0.7285141348838806
Epoch 460, training loss: 6.78298807144165 = 0.4300423860549927 + 1.0 * 6.352945804595947
Epoch 460, val loss: 0.7171981334686279
Epoch 470, training loss: 6.756227970123291 = 0.40777498483657837 + 1.0 * 6.348453044891357
Epoch 470, val loss: 0.7066788673400879
Epoch 480, training loss: 6.732577800750732 = 0.3858768343925476 + 1.0 * 6.346701145172119
Epoch 480, val loss: 0.6968407034873962
Epoch 490, training loss: 6.70794677734375 = 0.3644442558288574 + 1.0 * 6.343502521514893
Epoch 490, val loss: 0.6877477765083313
Epoch 500, training loss: 6.685519695281982 = 0.34367653727531433 + 1.0 * 6.341843128204346
Epoch 500, val loss: 0.6794130802154541
Epoch 510, training loss: 6.6623053550720215 = 0.32348743081092834 + 1.0 * 6.338818073272705
Epoch 510, val loss: 0.6718240976333618
Epoch 520, training loss: 6.653354167938232 = 0.3039951026439667 + 1.0 * 6.349359035491943
Epoch 520, val loss: 0.6649426221847534
Epoch 530, training loss: 6.622106075286865 = 0.2855450510978699 + 1.0 * 6.33656120300293
Epoch 530, val loss: 0.6589447855949402
Epoch 540, training loss: 6.602902412414551 = 0.2680525481700897 + 1.0 * 6.334849834442139
Epoch 540, val loss: 0.6536981463432312
Epoch 550, training loss: 6.5834245681762695 = 0.251433789730072 + 1.0 * 6.331990718841553
Epoch 550, val loss: 0.6491912007331848
Epoch 560, training loss: 6.574631214141846 = 0.2357165515422821 + 1.0 * 6.33891487121582
Epoch 560, val loss: 0.6454496383666992
Epoch 570, training loss: 6.552347183227539 = 0.2211022675037384 + 1.0 * 6.331244945526123
Epoch 570, val loss: 0.6424684524536133
Epoch 580, training loss: 6.535294055938721 = 0.20759403705596924 + 1.0 * 6.327700138092041
Epoch 580, val loss: 0.640062153339386
Epoch 590, training loss: 6.520880699157715 = 0.1949760615825653 + 1.0 * 6.325904846191406
Epoch 590, val loss: 0.6381940841674805
Epoch 600, training loss: 6.507229328155518 = 0.18317440152168274 + 1.0 * 6.324054718017578
Epoch 600, val loss: 0.6368863582611084
Epoch 610, training loss: 6.508084297180176 = 0.17219890654087067 + 1.0 * 6.335885524749756
Epoch 610, val loss: 0.6360586285591125
Epoch 620, training loss: 6.483989715576172 = 0.16205079853534698 + 1.0 * 6.321938991546631
Epoch 620, val loss: 0.6356844902038574
Epoch 630, training loss: 6.478798866271973 = 0.1526603400707245 + 1.0 * 6.326138496398926
Epoch 630, val loss: 0.635622501373291
Epoch 640, training loss: 6.462418556213379 = 0.14394471049308777 + 1.0 * 6.318473815917969
Epoch 640, val loss: 0.636037290096283
Epoch 650, training loss: 6.452247142791748 = 0.13586924970149994 + 1.0 * 6.316378116607666
Epoch 650, val loss: 0.6367043256759644
Epoch 660, training loss: 6.4560370445251465 = 0.12834914028644562 + 1.0 * 6.327687740325928
Epoch 660, val loss: 0.6376791000366211
Epoch 670, training loss: 6.43657112121582 = 0.12138719111680984 + 1.0 * 6.315184116363525
Epoch 670, val loss: 0.639035701751709
Epoch 680, training loss: 6.428035736083984 = 0.11494459211826324 + 1.0 * 6.313091278076172
Epoch 680, val loss: 0.6405376195907593
Epoch 690, training loss: 6.425893783569336 = 0.10891776531934738 + 1.0 * 6.316976070404053
Epoch 690, val loss: 0.642333447933197
Epoch 700, training loss: 6.42223596572876 = 0.10332770645618439 + 1.0 * 6.318908214569092
Epoch 700, val loss: 0.6443778872489929
Epoch 710, training loss: 6.411578178405762 = 0.09814748167991638 + 1.0 * 6.3134307861328125
Epoch 710, val loss: 0.6465449333190918
Epoch 720, training loss: 6.401634693145752 = 0.09331806004047394 + 1.0 * 6.308316707611084
Epoch 720, val loss: 0.6488544344902039
Epoch 730, training loss: 6.396737098693848 = 0.08878816664218903 + 1.0 * 6.307949066162109
Epoch 730, val loss: 0.6513746976852417
Epoch 740, training loss: 6.393704414367676 = 0.08455156534910202 + 1.0 * 6.309153079986572
Epoch 740, val loss: 0.6541157960891724
Epoch 750, training loss: 6.386013507843018 = 0.08060497790575027 + 1.0 * 6.305408477783203
Epoch 750, val loss: 0.6568519473075867
Epoch 760, training loss: 6.380954265594482 = 0.07689449936151505 + 1.0 * 6.304059982299805
Epoch 760, val loss: 0.6597200632095337
Epoch 770, training loss: 6.3801655769348145 = 0.07339973002672195 + 1.0 * 6.306766033172607
Epoch 770, val loss: 0.6627814769744873
Epoch 780, training loss: 6.3787522315979 = 0.07013151049613953 + 1.0 * 6.308620929718018
Epoch 780, val loss: 0.6659058928489685
Epoch 790, training loss: 6.367424488067627 = 0.06708060204982758 + 1.0 * 6.300343990325928
Epoch 790, val loss: 0.6690111756324768
Epoch 800, training loss: 6.36349630355835 = 0.0641922801733017 + 1.0 * 6.299304008483887
Epoch 800, val loss: 0.6722347140312195
Epoch 810, training loss: 6.363508224487305 = 0.06146439164876938 + 1.0 * 6.302043914794922
Epoch 810, val loss: 0.6755679845809937
Epoch 820, training loss: 6.356383800506592 = 0.05889308452606201 + 1.0 * 6.29749059677124
Epoch 820, val loss: 0.679003119468689
Epoch 830, training loss: 6.354435443878174 = 0.05647721514105797 + 1.0 * 6.2979583740234375
Epoch 830, val loss: 0.6823440194129944
Epoch 840, training loss: 6.356929302215576 = 0.054189056158065796 + 1.0 * 6.302740097045898
Epoch 840, val loss: 0.6857938170433044
Epoch 850, training loss: 6.347856521606445 = 0.05203346163034439 + 1.0 * 6.295823097229004
Epoch 850, val loss: 0.6892978549003601
Epoch 860, training loss: 6.34765100479126 = 0.04998862370848656 + 1.0 * 6.297662258148193
Epoch 860, val loss: 0.6927767992019653
Epoch 870, training loss: 6.34259033203125 = 0.048057619482278824 + 1.0 * 6.294532775878906
Epoch 870, val loss: 0.6963609457015991
Epoch 880, training loss: 6.338559627532959 = 0.0462372750043869 + 1.0 * 6.292322158813477
Epoch 880, val loss: 0.6998372673988342
Epoch 890, training loss: 6.335432529449463 = 0.04450090974569321 + 1.0 * 6.290931701660156
Epoch 890, val loss: 0.7033754587173462
Epoch 900, training loss: 6.335896968841553 = 0.04285071790218353 + 1.0 * 6.293046474456787
Epoch 900, val loss: 0.7069597840309143
Epoch 910, training loss: 6.335571765899658 = 0.04128870740532875 + 1.0 * 6.294282913208008
Epoch 910, val loss: 0.7106053233146667
Epoch 920, training loss: 6.3309431076049805 = 0.03981203958392143 + 1.0 * 6.291131019592285
Epoch 920, val loss: 0.7141758799552917
Epoch 930, training loss: 6.326786518096924 = 0.0384061373770237 + 1.0 * 6.288380146026611
Epoch 930, val loss: 0.7176868319511414
Epoch 940, training loss: 6.327996730804443 = 0.03706251084804535 + 1.0 * 6.290934085845947
Epoch 940, val loss: 0.7212693095207214
Epoch 950, training loss: 6.327362537384033 = 0.0357881523668766 + 1.0 * 6.291574478149414
Epoch 950, val loss: 0.7249106764793396
Epoch 960, training loss: 6.32261323928833 = 0.03458085283637047 + 1.0 * 6.288032531738281
Epoch 960, val loss: 0.7284213304519653
Epoch 970, training loss: 6.318722248077393 = 0.033428214490413666 + 1.0 * 6.285294055938721
Epoch 970, val loss: 0.7318945527076721
Epoch 980, training loss: 6.317826271057129 = 0.03232046589255333 + 1.0 * 6.285505771636963
Epoch 980, val loss: 0.7354316115379333
Epoch 990, training loss: 6.3186235427856445 = 0.03126611188054085 + 1.0 * 6.287357330322266
Epoch 990, val loss: 0.738997757434845
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 10.53857421875 = 1.9417446851730347 + 1.0 * 8.596829414367676
Epoch 0, val loss: 1.9357872009277344
Epoch 10, training loss: 10.528572082519531 = 1.932045817375183 + 1.0 * 8.596526145935059
Epoch 10, val loss: 1.9266854524612427
Epoch 20, training loss: 10.514129638671875 = 1.920250415802002 + 1.0 * 8.593879699707031
Epoch 20, val loss: 1.9151710271835327
Epoch 30, training loss: 10.47683334350586 = 1.9039453268051147 + 1.0 * 8.572888374328613
Epoch 30, val loss: 1.899057388305664
Epoch 40, training loss: 10.328069686889648 = 1.8824923038482666 + 1.0 * 8.445577621459961
Epoch 40, val loss: 1.878469705581665
Epoch 50, training loss: 9.878130912780762 = 1.8572341203689575 + 1.0 * 8.020896911621094
Epoch 50, val loss: 1.8552218675613403
Epoch 60, training loss: 9.584477424621582 = 1.8340598344802856 + 1.0 * 7.750417232513428
Epoch 60, val loss: 1.8344186544418335
Epoch 70, training loss: 9.113216400146484 = 1.818001389503479 + 1.0 * 7.295214653015137
Epoch 70, val loss: 1.8200500011444092
Epoch 80, training loss: 8.785859107971191 = 1.806424856185913 + 1.0 * 6.979434490203857
Epoch 80, val loss: 1.8088804483413696
Epoch 90, training loss: 8.649702072143555 = 1.7917890548706055 + 1.0 * 6.857913494110107
Epoch 90, val loss: 1.7947036027908325
Epoch 100, training loss: 8.559625625610352 = 1.7738983631134033 + 1.0 * 6.785727500915527
Epoch 100, val loss: 1.7786449193954468
Epoch 110, training loss: 8.495375633239746 = 1.7562006711959839 + 1.0 * 6.739175319671631
Epoch 110, val loss: 1.762736439704895
Epoch 120, training loss: 8.433693885803223 = 1.739363431930542 + 1.0 * 6.694330215454102
Epoch 120, val loss: 1.7470340728759766
Epoch 130, training loss: 8.379776954650879 = 1.7213948965072632 + 1.0 * 6.658382415771484
Epoch 130, val loss: 1.7303053140640259
Epoch 140, training loss: 8.32892894744873 = 1.7007324695587158 + 1.0 * 6.6281962394714355
Epoch 140, val loss: 1.7122944593429565
Epoch 150, training loss: 8.276970863342285 = 1.6765390634536743 + 1.0 * 6.600431442260742
Epoch 150, val loss: 1.6917095184326172
Epoch 160, training loss: 8.226303100585938 = 1.647864818572998 + 1.0 * 6.578437805175781
Epoch 160, val loss: 1.6672834157943726
Epoch 170, training loss: 8.173468589782715 = 1.6138288974761963 + 1.0 * 6.5596394538879395
Epoch 170, val loss: 1.6383622884750366
Epoch 180, training loss: 8.118016242980957 = 1.5742743015289307 + 1.0 * 6.543741703033447
Epoch 180, val loss: 1.6047570705413818
Epoch 190, training loss: 8.056641578674316 = 1.5286730527877808 + 1.0 * 6.527968406677246
Epoch 190, val loss: 1.5659542083740234
Epoch 200, training loss: 8.000894546508789 = 1.4772652387619019 + 1.0 * 6.523629188537598
Epoch 200, val loss: 1.5225355625152588
Epoch 210, training loss: 7.925289630889893 = 1.4225468635559082 + 1.0 * 6.502742767333984
Epoch 210, val loss: 1.4765392541885376
Epoch 220, training loss: 7.853572845458984 = 1.3645679950714111 + 1.0 * 6.489005088806152
Epoch 220, val loss: 1.4275741577148438
Epoch 230, training loss: 7.781019687652588 = 1.3035434484481812 + 1.0 * 6.477476119995117
Epoch 230, val loss: 1.3762807846069336
Epoch 240, training loss: 7.7081074714660645 = 1.240332007408142 + 1.0 * 6.467775344848633
Epoch 240, val loss: 1.3234379291534424
Epoch 250, training loss: 7.638474464416504 = 1.1769821643829346 + 1.0 * 6.461492538452148
Epoch 250, val loss: 1.2709332704544067
Epoch 260, training loss: 7.5672173500061035 = 1.114892840385437 + 1.0 * 6.452324390411377
Epoch 260, val loss: 1.2196884155273438
Epoch 270, training loss: 7.501008033752441 = 1.0539308786392212 + 1.0 * 6.44707727432251
Epoch 270, val loss: 1.1697725057601929
Epoch 280, training loss: 7.433529853820801 = 0.9951373934745789 + 1.0 * 6.438392639160156
Epoch 280, val loss: 1.1219655275344849
Epoch 290, training loss: 7.3731818199157715 = 0.9384071230888367 + 1.0 * 6.434774875640869
Epoch 290, val loss: 1.076292872428894
Epoch 300, training loss: 7.312958240509033 = 0.884607195854187 + 1.0 * 6.428350925445557
Epoch 300, val loss: 1.0332363843917847
Epoch 310, training loss: 7.255701065063477 = 0.8335361480712891 + 1.0 * 6.4221649169921875
Epoch 310, val loss: 0.9926366806030273
Epoch 320, training loss: 7.2012457847595215 = 0.7847684621810913 + 1.0 * 6.416477203369141
Epoch 320, val loss: 0.9541391134262085
Epoch 330, training loss: 7.161616325378418 = 0.7385237216949463 + 1.0 * 6.423092365264893
Epoch 330, val loss: 0.9179954528808594
Epoch 340, training loss: 7.106302738189697 = 0.6957860589027405 + 1.0 * 6.410516738891602
Epoch 340, val loss: 0.8851324319839478
Epoch 350, training loss: 7.062920093536377 = 0.656112015247345 + 1.0 * 6.406807899475098
Epoch 350, val loss: 0.8552402853965759
Epoch 360, training loss: 7.018307685852051 = 0.6193815469741821 + 1.0 * 6.398926258087158
Epoch 360, val loss: 0.8283356428146362
Epoch 370, training loss: 6.982595920562744 = 0.5850887894630432 + 1.0 * 6.397507190704346
Epoch 370, val loss: 0.8041806817054749
Epoch 380, training loss: 6.945816993713379 = 0.5531457662582397 + 1.0 * 6.39267110824585
Epoch 380, val loss: 0.7827541828155518
Epoch 390, training loss: 6.911314010620117 = 0.523181140422821 + 1.0 * 6.3881330490112305
Epoch 390, val loss: 0.7637224197387695
Epoch 400, training loss: 6.879847049713135 = 0.4946011006832123 + 1.0 * 6.3852458000183105
Epoch 400, val loss: 0.7466998100280762
Epoch 410, training loss: 6.852168083190918 = 0.46747422218322754 + 1.0 * 6.3846940994262695
Epoch 410, val loss: 0.7314990162849426
Epoch 420, training loss: 6.819433689117432 = 0.4416247010231018 + 1.0 * 6.377809047698975
Epoch 420, val loss: 0.7180615663528442
Epoch 430, training loss: 6.794377326965332 = 0.41674959659576416 + 1.0 * 6.377627849578857
Epoch 430, val loss: 0.7058873176574707
Epoch 440, training loss: 6.777302265167236 = 0.3930961787700653 + 1.0 * 6.384206295013428
Epoch 440, val loss: 0.6949775815010071
Epoch 450, training loss: 6.740330696105957 = 0.3706294298171997 + 1.0 * 6.369701385498047
Epoch 450, val loss: 0.6852120161056519
Epoch 460, training loss: 6.716867446899414 = 0.34907734394073486 + 1.0 * 6.367790222167969
Epoch 460, val loss: 0.6763387322425842
Epoch 470, training loss: 6.696777820587158 = 0.3283728063106537 + 1.0 * 6.368404865264893
Epoch 470, val loss: 0.6683188676834106
Epoch 480, training loss: 6.67913818359375 = 0.308856338262558 + 1.0 * 6.37028169631958
Epoch 480, val loss: 0.6611782908439636
Epoch 490, training loss: 6.65110969543457 = 0.29044270515441895 + 1.0 * 6.360666751861572
Epoch 490, val loss: 0.6549051403999329
Epoch 500, training loss: 6.630862236022949 = 0.2729784846305847 + 1.0 * 6.357883930206299
Epoch 500, val loss: 0.649418294429779
Epoch 510, training loss: 6.612574100494385 = 0.2564131021499634 + 1.0 * 6.356161117553711
Epoch 510, val loss: 0.6446459889411926
Epoch 520, training loss: 6.605696201324463 = 0.24090895056724548 + 1.0 * 6.3647871017456055
Epoch 520, val loss: 0.6406683921813965
Epoch 530, training loss: 6.5813493728637695 = 0.22650790214538574 + 1.0 * 6.354841709136963
Epoch 530, val loss: 0.6374174952507019
Epoch 540, training loss: 6.562854290008545 = 0.21302896738052368 + 1.0 * 6.349825382232666
Epoch 540, val loss: 0.6349071264266968
Epoch 550, training loss: 6.549300670623779 = 0.20032896101474762 + 1.0 * 6.348971843719482
Epoch 550, val loss: 0.6330233812332153
Epoch 560, training loss: 6.535358428955078 = 0.18851393461227417 + 1.0 * 6.346844673156738
Epoch 560, val loss: 0.6317657232284546
Epoch 570, training loss: 6.522314548492432 = 0.1775451898574829 + 1.0 * 6.344769477844238
Epoch 570, val loss: 0.6311241388320923
Epoch 580, training loss: 6.511650085449219 = 0.16730760037899017 + 1.0 * 6.3443427085876465
Epoch 580, val loss: 0.6310554146766663
Epoch 590, training loss: 6.509806156158447 = 0.15782015025615692 + 1.0 * 6.351985931396484
Epoch 590, val loss: 0.6313697099685669
Epoch 600, training loss: 6.489999771118164 = 0.14907868206501007 + 1.0 * 6.340920925140381
Epoch 600, val loss: 0.6321825981140137
Epoch 610, training loss: 6.477704048156738 = 0.1409231275320053 + 1.0 * 6.336781024932861
Epoch 610, val loss: 0.6334197521209717
Epoch 620, training loss: 6.468652725219727 = 0.13328221440315247 + 1.0 * 6.3353705406188965
Epoch 620, val loss: 0.635036289691925
Epoch 630, training loss: 6.4706573486328125 = 0.12615153193473816 + 1.0 * 6.344505786895752
Epoch 630, val loss: 0.636998176574707
Epoch 640, training loss: 6.45365047454834 = 0.1195550262928009 + 1.0 * 6.334095478057861
Epoch 640, val loss: 0.6392523050308228
Epoch 650, training loss: 6.4442548751831055 = 0.11339215934276581 + 1.0 * 6.330862522125244
Epoch 650, val loss: 0.6417941451072693
Epoch 660, training loss: 6.446432590484619 = 0.10760601609945297 + 1.0 * 6.338826656341553
Epoch 660, val loss: 0.6445850729942322
Epoch 670, training loss: 6.433868885040283 = 0.1022271141409874 + 1.0 * 6.331641674041748
Epoch 670, val loss: 0.6475186944007874
Epoch 680, training loss: 6.423825263977051 = 0.09721642732620239 + 1.0 * 6.326608657836914
Epoch 680, val loss: 0.6507087349891663
Epoch 690, training loss: 6.417437553405762 = 0.09250373393297195 + 1.0 * 6.324934005737305
Epoch 690, val loss: 0.6540852785110474
Epoch 700, training loss: 6.412959575653076 = 0.08805783838033676 + 1.0 * 6.324901580810547
Epoch 700, val loss: 0.657635509967804
Epoch 710, training loss: 6.406771183013916 = 0.08389578014612198 + 1.0 * 6.322875499725342
Epoch 710, val loss: 0.6612926125526428
Epoch 720, training loss: 6.404868125915527 = 0.08000741899013519 + 1.0 * 6.324860572814941
Epoch 720, val loss: 0.6650495529174805
Epoch 730, training loss: 6.400619029998779 = 0.07635104656219482 + 1.0 * 6.324267864227295
Epoch 730, val loss: 0.6689071655273438
Epoch 740, training loss: 6.392500877380371 = 0.07290194183588028 + 1.0 * 6.319599151611328
Epoch 740, val loss: 0.6728454232215881
Epoch 750, training loss: 6.391442775726318 = 0.0696505606174469 + 1.0 * 6.321792125701904
Epoch 750, val loss: 0.6768807768821716
Epoch 760, training loss: 6.385998725891113 = 0.06658939272165298 + 1.0 * 6.319409370422363
Epoch 760, val loss: 0.6809642910957336
Epoch 770, training loss: 6.386771202087402 = 0.06369919329881668 + 1.0 * 6.3230719566345215
Epoch 770, val loss: 0.6850849986076355
Epoch 780, training loss: 6.374664306640625 = 0.06097472831606865 + 1.0 * 6.313689708709717
Epoch 780, val loss: 0.6892675161361694
Epoch 790, training loss: 6.373135089874268 = 0.05839650705456734 + 1.0 * 6.314738750457764
Epoch 790, val loss: 0.6935126185417175
Epoch 800, training loss: 6.369157791137695 = 0.05596121400594711 + 1.0 * 6.313196659088135
Epoch 800, val loss: 0.6977080702781677
Epoch 810, training loss: 6.365488052368164 = 0.05366167798638344 + 1.0 * 6.311826229095459
Epoch 810, val loss: 0.7019638419151306
Epoch 820, training loss: 6.361635208129883 = 0.0514778271317482 + 1.0 * 6.310157299041748
Epoch 820, val loss: 0.7062553763389587
Epoch 830, training loss: 6.358487129211426 = 0.04940895363688469 + 1.0 * 6.309078216552734
Epoch 830, val loss: 0.7105895280838013
Epoch 840, training loss: 6.359458923339844 = 0.047442544251680374 + 1.0 * 6.312016487121582
Epoch 840, val loss: 0.7149085998535156
Epoch 850, training loss: 6.358316898345947 = 0.04558611661195755 + 1.0 * 6.31273078918457
Epoch 850, val loss: 0.7192193269729614
Epoch 860, training loss: 6.3507890701293945 = 0.04383586719632149 + 1.0 * 6.306953430175781
Epoch 860, val loss: 0.7235475182533264
Epoch 870, training loss: 6.351550579071045 = 0.04217395931482315 + 1.0 * 6.3093767166137695
Epoch 870, val loss: 0.7278198003768921
Epoch 880, training loss: 6.346789360046387 = 0.0405917726457119 + 1.0 * 6.306197643280029
Epoch 880, val loss: 0.7320922017097473
Epoch 890, training loss: 6.342159748077393 = 0.039093658328056335 + 1.0 * 6.303066253662109
Epoch 890, val loss: 0.7363693118095398
Epoch 900, training loss: 6.340451717376709 = 0.03766491264104843 + 1.0 * 6.302786827087402
Epoch 900, val loss: 0.7406385540962219
Epoch 910, training loss: 6.344066143035889 = 0.036302600055933 + 1.0 * 6.307763576507568
Epoch 910, val loss: 0.7448890209197998
Epoch 920, training loss: 6.338294982910156 = 0.03501169756054878 + 1.0 * 6.303283214569092
Epoch 920, val loss: 0.7490965127944946
Epoch 930, training loss: 6.339055061340332 = 0.033790260553359985 + 1.0 * 6.305264949798584
Epoch 930, val loss: 0.7531847357749939
Epoch 940, training loss: 6.331833839416504 = 0.03263354301452637 + 1.0 * 6.299200057983398
Epoch 940, val loss: 0.7572896480560303
Epoch 950, training loss: 6.329063892364502 = 0.03152626007795334 + 1.0 * 6.297537803649902
Epoch 950, val loss: 0.7614026069641113
Epoch 960, training loss: 6.327104568481445 = 0.030464492738246918 + 1.0 * 6.296639919281006
Epoch 960, val loss: 0.7655089497566223
Epoch 970, training loss: 6.333775043487549 = 0.029451578855514526 + 1.0 * 6.304323673248291
Epoch 970, val loss: 0.7695500254631042
Epoch 980, training loss: 6.33070182800293 = 0.028489727526903152 + 1.0 * 6.302212238311768
Epoch 980, val loss: 0.77353835105896
Epoch 990, training loss: 6.322911739349365 = 0.027575787156820297 + 1.0 * 6.29533576965332
Epoch 990, val loss: 0.7775148153305054
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 10.534524917602539 = 1.9377102851867676 + 1.0 * 8.59681510925293
Epoch 0, val loss: 1.93553626537323
Epoch 10, training loss: 10.524545669555664 = 1.928122639656067 + 1.0 * 8.596423149108887
Epoch 10, val loss: 1.9264039993286133
Epoch 20, training loss: 10.508849143981934 = 1.9155349731445312 + 1.0 * 8.593314170837402
Epoch 20, val loss: 1.9138498306274414
Epoch 30, training loss: 10.46764087677002 = 1.897147536277771 + 1.0 * 8.570493698120117
Epoch 30, val loss: 1.8951321840286255
Epoch 40, training loss: 10.32217025756836 = 1.8725507259368896 + 1.0 * 8.44961929321289
Epoch 40, val loss: 1.8711766004562378
Epoch 50, training loss: 9.840087890625 = 1.8446378707885742 + 1.0 * 7.995449542999268
Epoch 50, val loss: 1.8447407484054565
Epoch 60, training loss: 9.50632095336914 = 1.8204922676086426 + 1.0 * 7.68582820892334
Epoch 60, val loss: 1.8227076530456543
Epoch 70, training loss: 9.10533618927002 = 1.8025190830230713 + 1.0 * 7.302817344665527
Epoch 70, val loss: 1.8062894344329834
Epoch 80, training loss: 8.870366096496582 = 1.7883497476577759 + 1.0 * 7.0820159912109375
Epoch 80, val loss: 1.7935808897018433
Epoch 90, training loss: 8.681516647338867 = 1.7729533910751343 + 1.0 * 6.908563613891602
Epoch 90, val loss: 1.7798346281051636
Epoch 100, training loss: 8.574153900146484 = 1.7564897537231445 + 1.0 * 6.81766414642334
Epoch 100, val loss: 1.7649563550949097
Epoch 110, training loss: 8.500701904296875 = 1.7367647886276245 + 1.0 * 6.763936996459961
Epoch 110, val loss: 1.7472093105316162
Epoch 120, training loss: 8.43155288696289 = 1.7144801616668701 + 1.0 * 6.717072486877441
Epoch 120, val loss: 1.7271685600280762
Epoch 130, training loss: 8.374834060668945 = 1.6894867420196533 + 1.0 * 6.685347557067871
Epoch 130, val loss: 1.7041971683502197
Epoch 140, training loss: 8.31391429901123 = 1.6597132682800293 + 1.0 * 6.654201030731201
Epoch 140, val loss: 1.677214503288269
Epoch 150, training loss: 8.247664451599121 = 1.6249147653579712 + 1.0 * 6.6227498054504395
Epoch 150, val loss: 1.6464502811431885
Epoch 160, training loss: 8.179621696472168 = 1.5857278108596802 + 1.0 * 6.593894004821777
Epoch 160, val loss: 1.6125234365463257
Epoch 170, training loss: 8.110198020935059 = 1.5428892374038696 + 1.0 * 6.5673089027404785
Epoch 170, val loss: 1.5754776000976562
Epoch 180, training loss: 8.040816307067871 = 1.4973785877227783 + 1.0 * 6.543437480926514
Epoch 180, val loss: 1.5364010334014893
Epoch 190, training loss: 7.977346420288086 = 1.4519250392913818 + 1.0 * 6.525421619415283
Epoch 190, val loss: 1.4982876777648926
Epoch 200, training loss: 7.917322635650635 = 1.4078606367111206 + 1.0 * 6.509461879730225
Epoch 200, val loss: 1.4618338346481323
Epoch 210, training loss: 7.859318733215332 = 1.364549160003662 + 1.0 * 6.49476957321167
Epoch 210, val loss: 1.426348328590393
Epoch 220, training loss: 7.8050127029418945 = 1.3220962285995483 + 1.0 * 6.482916355133057
Epoch 220, val loss: 1.3922101259231567
Epoch 230, training loss: 7.7536163330078125 = 1.2813148498535156 + 1.0 * 6.472301483154297
Epoch 230, val loss: 1.3601422309875488
Epoch 240, training loss: 7.704065322875977 = 1.2419853210449219 + 1.0 * 6.462080001831055
Epoch 240, val loss: 1.329777479171753
Epoch 250, training loss: 7.655463218688965 = 1.2031219005584717 + 1.0 * 6.452341556549072
Epoch 250, val loss: 1.3001433610916138
Epoch 260, training loss: 7.614069938659668 = 1.1644775867462158 + 1.0 * 6.449592113494873
Epoch 260, val loss: 1.2709580659866333
Epoch 270, training loss: 7.56204080581665 = 1.1265290975570679 + 1.0 * 6.435511589050293
Epoch 270, val loss: 1.2424136400222778
Epoch 280, training loss: 7.519806861877441 = 1.08873450756073 + 1.0 * 6.431072235107422
Epoch 280, val loss: 1.2141609191894531
Epoch 290, training loss: 7.474541187286377 = 1.0517841577529907 + 1.0 * 6.422757148742676
Epoch 290, val loss: 1.1864193677902222
Epoch 300, training loss: 7.432757377624512 = 1.0157675743103027 + 1.0 * 6.416989803314209
Epoch 300, val loss: 1.1593904495239258
Epoch 310, training loss: 7.390087604522705 = 0.9805179238319397 + 1.0 * 6.40956974029541
Epoch 310, val loss: 1.1328465938568115
Epoch 320, training loss: 7.350151538848877 = 0.9461060166358948 + 1.0 * 6.404045581817627
Epoch 320, val loss: 1.106837511062622
Epoch 330, training loss: 7.311691761016846 = 0.9127741456031799 + 1.0 * 6.3989176750183105
Epoch 330, val loss: 1.081705927848816
Epoch 340, training loss: 7.277273654937744 = 0.8808385133743286 + 1.0 * 6.396435260772705
Epoch 340, val loss: 1.0576649904251099
Epoch 350, training loss: 7.239321708679199 = 0.8499090671539307 + 1.0 * 6.3894124031066895
Epoch 350, val loss: 1.0345762968063354
Epoch 360, training loss: 7.212052822113037 = 0.8196856379508972 + 1.0 * 6.392367362976074
Epoch 360, val loss: 1.0123347043991089
Epoch 370, training loss: 7.18058967590332 = 0.7907798290252686 + 1.0 * 6.389810085296631
Epoch 370, val loss: 0.9912005662918091
Epoch 380, training loss: 7.143307209014893 = 0.7628793716430664 + 1.0 * 6.380427837371826
Epoch 380, val loss: 0.9713216423988342
Epoch 390, training loss: 7.110698699951172 = 0.7355073094367981 + 1.0 * 6.3751912117004395
Epoch 390, val loss: 0.952252209186554
Epoch 400, training loss: 7.079685211181641 = 0.7086093425750732 + 1.0 * 6.3710761070251465
Epoch 400, val loss: 0.9339665174484253
Epoch 410, training loss: 7.051422119140625 = 0.6821292638778687 + 1.0 * 6.369292736053467
Epoch 410, val loss: 0.9164807796478271
Epoch 420, training loss: 7.02665376663208 = 0.6563854217529297 + 1.0 * 6.37026834487915
Epoch 420, val loss: 0.8999763131141663
Epoch 430, training loss: 6.994586944580078 = 0.6315123438835144 + 1.0 * 6.363074779510498
Epoch 430, val loss: 0.8845447301864624
Epoch 440, training loss: 6.967375755310059 = 0.6072614192962646 + 1.0 * 6.360114097595215
Epoch 440, val loss: 0.8700519800186157
Epoch 450, training loss: 6.943248271942139 = 0.5836986899375916 + 1.0 * 6.359549522399902
Epoch 450, val loss: 0.8565526604652405
Epoch 460, training loss: 6.917666435241699 = 0.5609776377677917 + 1.0 * 6.356688976287842
Epoch 460, val loss: 0.8441459536552429
Epoch 470, training loss: 6.892343044281006 = 0.5388933420181274 + 1.0 * 6.353449821472168
Epoch 470, val loss: 0.8327206373214722
Epoch 480, training loss: 6.872694969177246 = 0.5174593925476074 + 1.0 * 6.355235576629639
Epoch 480, val loss: 0.8223538398742676
Epoch 490, training loss: 6.847261428833008 = 0.49667543172836304 + 1.0 * 6.3505859375
Epoch 490, val loss: 0.8130171895027161
Epoch 500, training loss: 6.8229475021362305 = 0.4763760566711426 + 1.0 * 6.346571445465088
Epoch 500, val loss: 0.804568350315094
Epoch 510, training loss: 6.800921440124512 = 0.4564477205276489 + 1.0 * 6.344473838806152
Epoch 510, val loss: 0.7969220280647278
Epoch 520, training loss: 6.788464546203613 = 0.43693140149116516 + 1.0 * 6.351532936096191
Epoch 520, val loss: 0.790023922920227
Epoch 530, training loss: 6.762299060821533 = 0.417911559343338 + 1.0 * 6.344387531280518
Epoch 530, val loss: 0.7837905287742615
Epoch 540, training loss: 6.739132881164551 = 0.39925405383110046 + 1.0 * 6.339879035949707
Epoch 540, val loss: 0.7781375050544739
Epoch 550, training loss: 6.718277454376221 = 0.3808918297290802 + 1.0 * 6.337385654449463
Epoch 550, val loss: 0.7730284929275513
Epoch 560, training loss: 6.71384859085083 = 0.36293044686317444 + 1.0 * 6.350918292999268
Epoch 560, val loss: 0.7684735059738159
Epoch 570, training loss: 6.680323123931885 = 0.34567004442214966 + 1.0 * 6.334652900695801
Epoch 570, val loss: 0.7645612955093384
Epoch 580, training loss: 6.664894104003906 = 0.3289637565612793 + 1.0 * 6.335930347442627
Epoch 580, val loss: 0.7609719634056091
Epoch 590, training loss: 6.643803119659424 = 0.31267499923706055 + 1.0 * 6.331128120422363
Epoch 590, val loss: 0.7579922080039978
Epoch 600, training loss: 6.627195835113525 = 0.29682570695877075 + 1.0 * 6.33036994934082
Epoch 600, val loss: 0.7555285692214966
Epoch 610, training loss: 6.6212310791015625 = 0.2814772427082062 + 1.0 * 6.3397536277771
Epoch 610, val loss: 0.7537471652030945
Epoch 620, training loss: 6.593690395355225 = 0.2668183445930481 + 1.0 * 6.326871871948242
Epoch 620, val loss: 0.752456545829773
Epoch 630, training loss: 6.579927444458008 = 0.25270676612854004 + 1.0 * 6.327220916748047
Epoch 630, val loss: 0.7516571879386902
Epoch 640, training loss: 6.571166515350342 = 0.2391587346792221 + 1.0 * 6.332007884979248
Epoch 640, val loss: 0.7514925599098206
Epoch 650, training loss: 6.553768157958984 = 0.2263314425945282 + 1.0 * 6.327436923980713
Epoch 650, val loss: 0.7519872784614563
Epoch 660, training loss: 6.537885665893555 = 0.2141994684934616 + 1.0 * 6.323686122894287
Epoch 660, val loss: 0.7528279423713684
Epoch 670, training loss: 6.523315906524658 = 0.20265981554985046 + 1.0 * 6.3206562995910645
Epoch 670, val loss: 0.7543808817863464
Epoch 680, training loss: 6.52052640914917 = 0.1917649805545807 + 1.0 * 6.328761577606201
Epoch 680, val loss: 0.7565248608589172
Epoch 690, training loss: 6.502058029174805 = 0.18155650794506073 + 1.0 * 6.320501327514648
Epoch 690, val loss: 0.7590398192405701
Epoch 700, training loss: 6.4892988204956055 = 0.17193077504634857 + 1.0 * 6.317368030548096
Epoch 700, val loss: 0.7619143724441528
Epoch 710, training loss: 6.495818614959717 = 0.1628861427307129 + 1.0 * 6.332932472229004
Epoch 710, val loss: 0.7653727531433105
Epoch 720, training loss: 6.470271110534668 = 0.15440598130226135 + 1.0 * 6.3158650398254395
Epoch 720, val loss: 0.7691513895988464
Epoch 730, training loss: 6.460239410400391 = 0.14646311104297638 + 1.0 * 6.31377649307251
Epoch 730, val loss: 0.7730857133865356
Epoch 740, training loss: 6.4512939453125 = 0.13897642493247986 + 1.0 * 6.312317371368408
Epoch 740, val loss: 0.7773987054824829
Epoch 750, training loss: 6.45152473449707 = 0.13192962110042572 + 1.0 * 6.3195953369140625
Epoch 750, val loss: 0.7820461988449097
Epoch 760, training loss: 6.438328266143799 = 0.12530769407749176 + 1.0 * 6.313020706176758
Epoch 760, val loss: 0.7869387269020081
Epoch 770, training loss: 6.428694248199463 = 0.11909589171409607 + 1.0 * 6.309598445892334
Epoch 770, val loss: 0.7918912768363953
Epoch 780, training loss: 6.425685405731201 = 0.11322160810232162 + 1.0 * 6.312463760375977
Epoch 780, val loss: 0.7970713973045349
Epoch 790, training loss: 6.42335319519043 = 0.10774698853492737 + 1.0 * 6.315606117248535
Epoch 790, val loss: 0.8025913834571838
Epoch 800, training loss: 6.409300327301025 = 0.10259148478507996 + 1.0 * 6.306708812713623
Epoch 800, val loss: 0.8078820705413818
Epoch 810, training loss: 6.402507781982422 = 0.0977269783616066 + 1.0 * 6.304780960083008
Epoch 810, val loss: 0.813243567943573
Epoch 820, training loss: 6.39926815032959 = 0.09313339740037918 + 1.0 * 6.3061347007751465
Epoch 820, val loss: 0.8187752962112427
Epoch 830, training loss: 6.391759395599365 = 0.08880572021007538 + 1.0 * 6.302953720092773
Epoch 830, val loss: 0.8243337273597717
Epoch 840, training loss: 6.39413595199585 = 0.08472024649381638 + 1.0 * 6.309415817260742
Epoch 840, val loss: 0.8300068974494934
Epoch 850, training loss: 6.383302211761475 = 0.08087978512048721 + 1.0 * 6.302422523498535
Epoch 850, val loss: 0.8356497287750244
Epoch 860, training loss: 6.3773274421691895 = 0.0772414281964302 + 1.0 * 6.30008602142334
Epoch 860, val loss: 0.8412570357322693
Epoch 870, training loss: 6.388472557067871 = 0.07380117475986481 + 1.0 * 6.314671516418457
Epoch 870, val loss: 0.8470675945281982
Epoch 880, training loss: 6.369257926940918 = 0.07058600336313248 + 1.0 * 6.298671722412109
Epoch 880, val loss: 0.8527818918228149
Epoch 890, training loss: 6.365082263946533 = 0.06754961609840393 + 1.0 * 6.297532558441162
Epoch 890, val loss: 0.858243465423584
Epoch 900, training loss: 6.3607940673828125 = 0.06466871500015259 + 1.0 * 6.296125411987305
Epoch 900, val loss: 0.8638926148414612
Epoch 910, training loss: 6.366235733032227 = 0.061946142464876175 + 1.0 * 6.304289817810059
Epoch 910, val loss: 0.8696540594100952
Epoch 920, training loss: 6.354991436004639 = 0.059365756809711456 + 1.0 * 6.295625686645508
Epoch 920, val loss: 0.8752962350845337
Epoch 930, training loss: 6.350174427032471 = 0.05693903565406799 + 1.0 * 6.2932353019714355
Epoch 930, val loss: 0.8808609843254089
Epoch 940, training loss: 6.360358238220215 = 0.05463910847902298 + 1.0 * 6.305718898773193
Epoch 940, val loss: 0.8865550756454468
Epoch 950, training loss: 6.347033500671387 = 0.05246759206056595 + 1.0 * 6.294565677642822
Epoch 950, val loss: 0.8922521471977234
Epoch 960, training loss: 6.341416358947754 = 0.05042184516787529 + 1.0 * 6.290994644165039
Epoch 960, val loss: 0.8976436257362366
Epoch 970, training loss: 6.339323997497559 = 0.04847292602062225 + 1.0 * 6.29085111618042
Epoch 970, val loss: 0.9031367301940918
Epoch 980, training loss: 6.343576908111572 = 0.04662441089749336 + 1.0 * 6.296952724456787
Epoch 980, val loss: 0.9087849259376526
Epoch 990, training loss: 6.334848403930664 = 0.044883936643600464 + 1.0 * 6.28996467590332
Epoch 990, val loss: 0.914165735244751
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8371112282551397
The final CL Acc:0.81728, 0.00630, The final GNN Acc:0.83904, 0.00203
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9538])
updated graph: torch.Size([2, 10566])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.54481029510498 = 1.9479472637176514 + 1.0 * 8.59686279296875
Epoch 0, val loss: 1.9603931903839111
Epoch 10, training loss: 10.534646987915039 = 1.9379488229751587 + 1.0 * 8.596697807312012
Epoch 10, val loss: 1.9501980543136597
Epoch 20, training loss: 10.52124309539795 = 1.9259766340255737 + 1.0 * 8.595266342163086
Epoch 20, val loss: 1.9378349781036377
Epoch 30, training loss: 10.492104530334473 = 1.9095784425735474 + 1.0 * 8.582526206970215
Epoch 30, val loss: 1.9210304021835327
Epoch 40, training loss: 10.387771606445312 = 1.8872538805007935 + 1.0 * 8.500517845153809
Epoch 40, val loss: 1.8988200426101685
Epoch 50, training loss: 9.999892234802246 = 1.8613301515579224 + 1.0 * 8.138562202453613
Epoch 50, val loss: 1.873660922050476
Epoch 60, training loss: 9.66588020324707 = 1.838059425354004 + 1.0 * 7.827820301055908
Epoch 60, val loss: 1.852301836013794
Epoch 70, training loss: 9.210675239562988 = 1.82205069065094 + 1.0 * 7.388624668121338
Epoch 70, val loss: 1.8365312814712524
Epoch 80, training loss: 8.944128036499023 = 1.8078802824020386 + 1.0 * 7.136247634887695
Epoch 80, val loss: 1.8226174116134644
Epoch 90, training loss: 8.762210845947266 = 1.792598009109497 + 1.0 * 6.9696125984191895
Epoch 90, val loss: 1.8079442977905273
Epoch 100, training loss: 8.630337715148926 = 1.7761831283569336 + 1.0 * 6.854154586791992
Epoch 100, val loss: 1.792009711265564
Epoch 110, training loss: 8.541661262512207 = 1.7603071928024292 + 1.0 * 6.781353950500488
Epoch 110, val loss: 1.7764631509780884
Epoch 120, training loss: 8.465156555175781 = 1.743865966796875 + 1.0 * 6.721290111541748
Epoch 120, val loss: 1.7609682083129883
Epoch 130, training loss: 8.392931938171387 = 1.7255818843841553 + 1.0 * 6.6673502922058105
Epoch 130, val loss: 1.744876742362976
Epoch 140, training loss: 8.330575942993164 = 1.7048505544662476 + 1.0 * 6.625725269317627
Epoch 140, val loss: 1.7276217937469482
Epoch 150, training loss: 8.27495288848877 = 1.6810039281845093 + 1.0 * 6.593948841094971
Epoch 150, val loss: 1.7077081203460693
Epoch 160, training loss: 8.221091270446777 = 1.6532734632492065 + 1.0 * 6.5678181648254395
Epoch 160, val loss: 1.684564232826233
Epoch 170, training loss: 8.169293403625488 = 1.6211074590682983 + 1.0 * 6.548186302185059
Epoch 170, val loss: 1.6576318740844727
Epoch 180, training loss: 8.114677429199219 = 1.5843137502670288 + 1.0 * 6.530364036560059
Epoch 180, val loss: 1.6267657279968262
Epoch 190, training loss: 8.058178901672363 = 1.5426055192947388 + 1.0 * 6.515573024749756
Epoch 190, val loss: 1.5918433666229248
Epoch 200, training loss: 8.002874374389648 = 1.4961206912994385 + 1.0 * 6.506753444671631
Epoch 200, val loss: 1.5530105829238892
Epoch 210, training loss: 7.939816951751709 = 1.446144938468933 + 1.0 * 6.493671894073486
Epoch 210, val loss: 1.5115859508514404
Epoch 220, training loss: 7.877655506134033 = 1.3936715126037598 + 1.0 * 6.483983993530273
Epoch 220, val loss: 1.468356966972351
Epoch 230, training loss: 7.815511226654053 = 1.3395916223526 + 1.0 * 6.475919723510742
Epoch 230, val loss: 1.4242998361587524
Epoch 240, training loss: 7.757544040679932 = 1.285664439201355 + 1.0 * 6.471879482269287
Epoch 240, val loss: 1.381187915802002
Epoch 250, training loss: 7.69568395614624 = 1.2332514524459839 + 1.0 * 6.462432384490967
Epoch 250, val loss: 1.340027928352356
Epoch 260, training loss: 7.637984752655029 = 1.1824928522109985 + 1.0 * 6.45549201965332
Epoch 260, val loss: 1.301072120666504
Epoch 270, training loss: 7.583635330200195 = 1.1339309215545654 + 1.0 * 6.449704170227051
Epoch 270, val loss: 1.2648359537124634
Epoch 280, training loss: 7.531930923461914 = 1.0878832340240479 + 1.0 * 6.444047451019287
Epoch 280, val loss: 1.2315187454223633
Epoch 290, training loss: 7.485696792602539 = 1.044029712677002 + 1.0 * 6.441667079925537
Epoch 290, val loss: 1.200656533241272
Epoch 300, training loss: 7.438261032104492 = 1.0023596286773682 + 1.0 * 6.435901641845703
Epoch 300, val loss: 1.1721991300582886
Epoch 310, training loss: 7.3924665451049805 = 0.9624383449554443 + 1.0 * 6.430028438568115
Epoch 310, val loss: 1.1456676721572876
Epoch 320, training loss: 7.34861421585083 = 0.9235416650772095 + 1.0 * 6.42507266998291
Epoch 320, val loss: 1.1204338073730469
Epoch 330, training loss: 7.311659336090088 = 0.8854486346244812 + 1.0 * 6.426210880279541
Epoch 330, val loss: 1.0961506366729736
Epoch 340, training loss: 7.266659736633301 = 0.8483860492706299 + 1.0 * 6.41827392578125
Epoch 340, val loss: 1.07310152053833
Epoch 350, training loss: 7.225412845611572 = 0.8117974996566772 + 1.0 * 6.4136152267456055
Epoch 350, val loss: 1.0508073568344116
Epoch 360, training loss: 7.1848063468933105 = 0.7753225564956665 + 1.0 * 6.409483909606934
Epoch 360, val loss: 1.029044508934021
Epoch 370, training loss: 7.1540141105651855 = 0.7388691902160645 + 1.0 * 6.415144920349121
Epoch 370, val loss: 1.007812738418579
Epoch 380, training loss: 7.106960296630859 = 0.7030181288719177 + 1.0 * 6.403942108154297
Epoch 380, val loss: 0.9875761866569519
Epoch 390, training loss: 7.067910671234131 = 0.6675828695297241 + 1.0 * 6.400327682495117
Epoch 390, val loss: 0.9682819247245789
Epoch 400, training loss: 7.030244827270508 = 0.6325778365135193 + 1.0 * 6.397666931152344
Epoch 400, val loss: 0.9498739838600159
Epoch 410, training loss: 6.9975199699401855 = 0.5983050465583801 + 1.0 * 6.399214744567871
Epoch 410, val loss: 0.9326273202896118
Epoch 420, training loss: 6.960644245147705 = 0.5653005838394165 + 1.0 * 6.395343780517578
Epoch 420, val loss: 0.9168061017990112
Epoch 430, training loss: 6.923896789550781 = 0.5333687663078308 + 1.0 * 6.390528202056885
Epoch 430, val loss: 0.902299165725708
Epoch 440, training loss: 6.890046119689941 = 0.5023606419563293 + 1.0 * 6.387685298919678
Epoch 440, val loss: 0.8889578580856323
Epoch 450, training loss: 6.863681793212891 = 0.4723094403743744 + 1.0 * 6.391372203826904
Epoch 450, val loss: 0.8767917156219482
Epoch 460, training loss: 6.828332901000977 = 0.4435625970363617 + 1.0 * 6.384770393371582
Epoch 460, val loss: 0.8658208250999451
Epoch 470, training loss: 6.8148393630981445 = 0.4160579442977905 + 1.0 * 6.3987812995910645
Epoch 470, val loss: 0.8559823036193848
Epoch 480, training loss: 6.768863201141357 = 0.3901301324367523 + 1.0 * 6.378733158111572
Epoch 480, val loss: 0.847251296043396
Epoch 490, training loss: 6.741891860961914 = 0.3654916286468506 + 1.0 * 6.376400470733643
Epoch 490, val loss: 0.8396176695823669
Epoch 500, training loss: 6.715511322021484 = 0.3420334756374359 + 1.0 * 6.373477935791016
Epoch 500, val loss: 0.8328515291213989
Epoch 510, training loss: 6.6908955574035645 = 0.31970691680908203 + 1.0 * 6.371188640594482
Epoch 510, val loss: 0.8269181847572327
Epoch 520, training loss: 6.674995422363281 = 0.29857397079467773 + 1.0 * 6.3764214515686035
Epoch 520, val loss: 0.8217984437942505
Epoch 530, training loss: 6.648410797119141 = 0.27882489562034607 + 1.0 * 6.369585990905762
Epoch 530, val loss: 0.8175744414329529
Epoch 540, training loss: 6.6266069412231445 = 0.2602282464504242 + 1.0 * 6.3663787841796875
Epoch 540, val loss: 0.8141230940818787
Epoch 550, training loss: 6.611138343811035 = 0.242747500538826 + 1.0 * 6.368391036987305
Epoch 550, val loss: 0.8113331198692322
Epoch 560, training loss: 6.588585376739502 = 0.2264164835214615 + 1.0 * 6.362168788909912
Epoch 560, val loss: 0.809333324432373
Epoch 570, training loss: 6.576530456542969 = 0.21112708747386932 + 1.0 * 6.365403175354004
Epoch 570, val loss: 0.8080243468284607
Epoch 580, training loss: 6.55550479888916 = 0.19691601395606995 + 1.0 * 6.358588695526123
Epoch 580, val loss: 0.8074631690979004
Epoch 590, training loss: 6.539673328399658 = 0.18365946412086487 + 1.0 * 6.356013774871826
Epoch 590, val loss: 0.8076689839363098
Epoch 600, training loss: 6.531467437744141 = 0.17136725783348083 + 1.0 * 6.360100269317627
Epoch 600, val loss: 0.8084433674812317
Epoch 610, training loss: 6.5135979652404785 = 0.16001321375370026 + 1.0 * 6.3535847663879395
Epoch 610, val loss: 0.8098838329315186
Epoch 620, training loss: 6.499513149261475 = 0.14952890574932098 + 1.0 * 6.349984169006348
Epoch 620, val loss: 0.8119680285453796
Epoch 630, training loss: 6.49808931350708 = 0.13983693718910217 + 1.0 * 6.35825252532959
Epoch 630, val loss: 0.8145341277122498
Epoch 640, training loss: 6.480269908905029 = 0.13094711303710938 + 1.0 * 6.34932279586792
Epoch 640, val loss: 0.8175758123397827
Epoch 650, training loss: 6.468258380889893 = 0.12273978441953659 + 1.0 * 6.345518589019775
Epoch 650, val loss: 0.8211680054664612
Epoch 660, training loss: 6.4650349617004395 = 0.11517371982336044 + 1.0 * 6.349861145019531
Epoch 660, val loss: 0.8249567747116089
Epoch 670, training loss: 6.453458309173584 = 0.10825172066688538 + 1.0 * 6.3452067375183105
Epoch 670, val loss: 0.8291506767272949
Epoch 680, training loss: 6.443437099456787 = 0.10186260938644409 + 1.0 * 6.341574668884277
Epoch 680, val loss: 0.833765983581543
Epoch 690, training loss: 6.436853885650635 = 0.09593340009450912 + 1.0 * 6.340920448303223
Epoch 690, val loss: 0.8385568857192993
Epoch 700, training loss: 6.428630352020264 = 0.09044851362705231 + 1.0 * 6.338181972503662
Epoch 700, val loss: 0.8434738516807556
Epoch 710, training loss: 6.420783042907715 = 0.0853811502456665 + 1.0 * 6.335402011871338
Epoch 710, val loss: 0.8487442135810852
Epoch 720, training loss: 6.415139675140381 = 0.0806780606508255 + 1.0 * 6.334461688995361
Epoch 720, val loss: 0.8541982173919678
Epoch 730, training loss: 6.419795989990234 = 0.0763057991862297 + 1.0 * 6.343490123748779
Epoch 730, val loss: 0.8596926927566528
Epoch 740, training loss: 6.403972148895264 = 0.072258360683918 + 1.0 * 6.331713676452637
Epoch 740, val loss: 0.8652523159980774
Epoch 750, training loss: 6.3991193771362305 = 0.06848888844251633 + 1.0 * 6.330630302429199
Epoch 750, val loss: 0.8710144758224487
Epoch 760, training loss: 6.394753456115723 = 0.06496647745370865 + 1.0 * 6.329786777496338
Epoch 760, val loss: 0.8768044114112854
Epoch 770, training loss: 6.401466369628906 = 0.061680205166339874 + 1.0 * 6.339786052703857
Epoch 770, val loss: 0.8824813961982727
Epoch 780, training loss: 6.390476226806641 = 0.058644361793994904 + 1.0 * 6.331831932067871
Epoch 780, val loss: 0.8881756663322449
Epoch 790, training loss: 6.381991386413574 = 0.055817991495132446 + 1.0 * 6.326173305511475
Epoch 790, val loss: 0.8940294981002808
Epoch 800, training loss: 6.3773908615112305 = 0.05317484214901924 + 1.0 * 6.324215888977051
Epoch 800, val loss: 0.8998334407806396
Epoch 810, training loss: 6.37659215927124 = 0.05069452524185181 + 1.0 * 6.325897693634033
Epoch 810, val loss: 0.9055861830711365
Epoch 820, training loss: 6.37696647644043 = 0.048377253115177155 + 1.0 * 6.32858943939209
Epoch 820, val loss: 0.9111247658729553
Epoch 830, training loss: 6.3705291748046875 = 0.04622679203748703 + 1.0 * 6.3243021965026855
Epoch 830, val loss: 0.9166603088378906
Epoch 840, training loss: 6.365649223327637 = 0.04421800374984741 + 1.0 * 6.3214311599731445
Epoch 840, val loss: 0.9223245978355408
Epoch 850, training loss: 6.361382961273193 = 0.04232633858919144 + 1.0 * 6.319056510925293
Epoch 850, val loss: 0.927869439125061
Epoch 860, training loss: 6.359838008880615 = 0.040539808571338654 + 1.0 * 6.319298267364502
Epoch 860, val loss: 0.9333391785621643
Epoch 870, training loss: 6.3586530685424805 = 0.038860663771629333 + 1.0 * 6.3197922706604
Epoch 870, val loss: 0.9385678172111511
Epoch 880, training loss: 6.3572893142700195 = 0.0372905470430851 + 1.0 * 6.319998741149902
Epoch 880, val loss: 0.943859338760376
Epoch 890, training loss: 6.35122537612915 = 0.03581206873059273 + 1.0 * 6.315413475036621
Epoch 890, val loss: 0.9492149353027344
Epoch 900, training loss: 6.34890604019165 = 0.03441276773810387 + 1.0 * 6.314493179321289
Epoch 900, val loss: 0.9544495940208435
Epoch 910, training loss: 6.360342502593994 = 0.033089637756347656 + 1.0 * 6.3272528648376465
Epoch 910, val loss: 0.9595229625701904
Epoch 920, training loss: 6.346713066101074 = 0.03184446692466736 + 1.0 * 6.314868450164795
Epoch 920, val loss: 0.9643499255180359
Epoch 930, training loss: 6.344517230987549 = 0.03067712113261223 + 1.0 * 6.313839912414551
Epoch 930, val loss: 0.969363272190094
Epoch 940, training loss: 6.341320514678955 = 0.02956932969391346 + 1.0 * 6.311751365661621
Epoch 940, val loss: 0.9743889570236206
Epoch 950, training loss: 6.339005947113037 = 0.028514092788100243 + 1.0 * 6.310492038726807
Epoch 950, val loss: 0.9792695045471191
Epoch 960, training loss: 6.348199367523193 = 0.02751338668167591 + 1.0 * 6.320685863494873
Epoch 960, val loss: 0.9840019941329956
Epoch 970, training loss: 6.337301731109619 = 0.0265647042542696 + 1.0 * 6.310737133026123
Epoch 970, val loss: 0.9885661005973816
Epoch 980, training loss: 6.333917617797852 = 0.025668064132332802 + 1.0 * 6.308249473571777
Epoch 980, val loss: 0.9932973384857178
Epoch 990, training loss: 6.332167625427246 = 0.02481515146791935 + 1.0 * 6.307352542877197
Epoch 990, val loss: 0.9979802966117859
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 10.54719352722168 = 1.9503264427185059 + 1.0 * 8.596867561340332
Epoch 0, val loss: 1.9500404596328735
Epoch 10, training loss: 10.537245750427246 = 1.9405361413955688 + 1.0 * 8.596709251403809
Epoch 10, val loss: 1.9399070739746094
Epoch 20, training loss: 10.524080276489258 = 1.9286904335021973 + 1.0 * 8.595390319824219
Epoch 20, val loss: 1.9274468421936035
Epoch 30, training loss: 10.49537467956543 = 1.9122885465621948 + 1.0 * 8.583086013793945
Epoch 30, val loss: 1.910257339477539
Epoch 40, training loss: 10.379353523254395 = 1.8898521661758423 + 1.0 * 8.489500999450684
Epoch 40, val loss: 1.8876054286956787
Epoch 50, training loss: 9.949620246887207 = 1.8642442226409912 + 1.0 * 8.085375785827637
Epoch 50, val loss: 1.863165259361267
Epoch 60, training loss: 9.574904441833496 = 1.8414394855499268 + 1.0 * 7.73346471786499
Epoch 60, val loss: 1.8421337604522705
Epoch 70, training loss: 9.03878402709961 = 1.8243297338485718 + 1.0 * 7.214454174041748
Epoch 70, val loss: 1.8261055946350098
Epoch 80, training loss: 8.802145004272461 = 1.809258222579956 + 1.0 * 6.992887020111084
Epoch 80, val loss: 1.8122776746749878
Epoch 90, training loss: 8.662056922912598 = 1.7924405336380005 + 1.0 * 6.869616508483887
Epoch 90, val loss: 1.7975469827651978
Epoch 100, training loss: 8.574566841125488 = 1.7750999927520752 + 1.0 * 6.799467086791992
Epoch 100, val loss: 1.7830053567886353
Epoch 110, training loss: 8.504138946533203 = 1.7587813138961792 + 1.0 * 6.745357990264893
Epoch 110, val loss: 1.769336462020874
Epoch 120, training loss: 8.440041542053223 = 1.7424280643463135 + 1.0 * 6.69761323928833
Epoch 120, val loss: 1.7556544542312622
Epoch 130, training loss: 8.388225555419922 = 1.7244161367416382 + 1.0 * 6.663809776306152
Epoch 130, val loss: 1.7410001754760742
Epoch 140, training loss: 8.33742618560791 = 1.7039457559585571 + 1.0 * 6.633480072021484
Epoch 140, val loss: 1.7246527671813965
Epoch 150, training loss: 8.291728973388672 = 1.680403470993042 + 1.0 * 6.611325740814209
Epoch 150, val loss: 1.7061752080917358
Epoch 160, training loss: 8.244451522827148 = 1.653410792350769 + 1.0 * 6.591041088104248
Epoch 160, val loss: 1.6850305795669556
Epoch 170, training loss: 8.196062088012695 = 1.6224533319473267 + 1.0 * 6.5736083984375
Epoch 170, val loss: 1.6607826948165894
Epoch 180, training loss: 8.145509719848633 = 1.587064504623413 + 1.0 * 6.558444976806641
Epoch 180, val loss: 1.6330270767211914
Epoch 190, training loss: 8.089892387390137 = 1.5473732948303223 + 1.0 * 6.5425190925598145
Epoch 190, val loss: 1.6020234823226929
Epoch 200, training loss: 8.030929565429688 = 1.5034904479980469 + 1.0 * 6.527439594268799
Epoch 200, val loss: 1.5677958726882935
Epoch 210, training loss: 7.9730610847473145 = 1.4557398557662964 + 1.0 * 6.5173211097717285
Epoch 210, val loss: 1.530678391456604
Epoch 220, training loss: 7.907951354980469 = 1.4053853750228882 + 1.0 * 6.502565860748291
Epoch 220, val loss: 1.4916329383850098
Epoch 230, training loss: 7.843395709991455 = 1.35297691822052 + 1.0 * 6.490418910980225
Epoch 230, val loss: 1.4512782096862793
Epoch 240, training loss: 7.7816033363342285 = 1.2999836206436157 + 1.0 * 6.481619834899902
Epoch 240, val loss: 1.4108377695083618
Epoch 250, training loss: 7.718183517456055 = 1.2477600574493408 + 1.0 * 6.470423221588135
Epoch 250, val loss: 1.3713117837905884
Epoch 260, training loss: 7.660133361816406 = 1.1964292526245117 + 1.0 * 6.4637041091918945
Epoch 260, val loss: 1.332977294921875
Epoch 270, training loss: 7.60216760635376 = 1.1467547416687012 + 1.0 * 6.455412864685059
Epoch 270, val loss: 1.29659104347229
Epoch 280, training loss: 7.545732498168945 = 1.0986912250518799 + 1.0 * 6.447041034698486
Epoch 280, val loss: 1.2618484497070312
Epoch 290, training loss: 7.4946608543396 = 1.0518749952316284 + 1.0 * 6.442785739898682
Epoch 290, val loss: 1.2287789583206177
Epoch 300, training loss: 7.444798946380615 = 1.0066109895706177 + 1.0 * 6.438188076019287
Epoch 300, val loss: 1.1975440979003906
Epoch 310, training loss: 7.396183013916016 = 0.9628037810325623 + 1.0 * 6.433379173278809
Epoch 310, val loss: 1.1679753065109253
Epoch 320, training loss: 7.348748207092285 = 0.9204647541046143 + 1.0 * 6.42828369140625
Epoch 320, val loss: 1.1401084661483765
Epoch 330, training loss: 7.301021099090576 = 0.8793819546699524 + 1.0 * 6.4216389656066895
Epoch 330, val loss: 1.113861083984375
Epoch 340, training loss: 7.259932518005371 = 0.8395683765411377 + 1.0 * 6.420363903045654
Epoch 340, val loss: 1.0891903638839722
Epoch 350, training loss: 7.216991424560547 = 0.8013537526130676 + 1.0 * 6.415637493133545
Epoch 350, val loss: 1.066214919090271
Epoch 360, training loss: 7.175954818725586 = 0.7648613452911377 + 1.0 * 6.411093711853027
Epoch 360, val loss: 1.0453022718429565
Epoch 370, training loss: 7.142826080322266 = 0.7300224900245667 + 1.0 * 6.412803649902344
Epoch 370, val loss: 1.0262311697006226
Epoch 380, training loss: 7.102038383483887 = 0.6970766186714172 + 1.0 * 6.404961585998535
Epoch 380, val loss: 1.0091890096664429
Epoch 390, training loss: 7.066515922546387 = 0.665553867816925 + 1.0 * 6.400961875915527
Epoch 390, val loss: 0.9939888119697571
Epoch 400, training loss: 7.041187763214111 = 0.6353205442428589 + 1.0 * 6.405867099761963
Epoch 400, val loss: 0.9803648591041565
Epoch 410, training loss: 7.0014848709106445 = 0.6065059304237366 + 1.0 * 6.394979000091553
Epoch 410, val loss: 0.9682101607322693
Epoch 420, training loss: 6.971046447753906 = 0.5788428783416748 + 1.0 * 6.3922038078308105
Epoch 420, val loss: 0.9575016498565674
Epoch 430, training loss: 6.946354866027832 = 0.552132785320282 + 1.0 * 6.394222259521484
Epoch 430, val loss: 0.9478012323379517
Epoch 440, training loss: 6.915466785430908 = 0.5264374613761902 + 1.0 * 6.389029502868652
Epoch 440, val loss: 0.9391761422157288
Epoch 450, training loss: 6.885325908660889 = 0.5015687346458435 + 1.0 * 6.3837571144104
Epoch 450, val loss: 0.9315762519836426
Epoch 460, training loss: 6.865307331085205 = 0.47751858830451965 + 1.0 * 6.387788772583008
Epoch 460, val loss: 0.9247949719429016
Epoch 470, training loss: 6.838402271270752 = 0.4543638527393341 + 1.0 * 6.38403844833374
Epoch 470, val loss: 0.918989896774292
Epoch 480, training loss: 6.808645248413086 = 0.43194642663002014 + 1.0 * 6.376698970794678
Epoch 480, val loss: 0.9140521883964539
Epoch 490, training loss: 6.786560535430908 = 0.410184383392334 + 1.0 * 6.376376152038574
Epoch 490, val loss: 0.9098687767982483
Epoch 500, training loss: 6.7661662101745605 = 0.3890943229198456 + 1.0 * 6.377071857452393
Epoch 500, val loss: 0.9063758850097656
Epoch 510, training loss: 6.741085052490234 = 0.36876940727233887 + 1.0 * 6.372315406799316
Epoch 510, val loss: 0.9039127230644226
Epoch 520, training loss: 6.716961860656738 = 0.3489973843097687 + 1.0 * 6.367964267730713
Epoch 520, val loss: 0.9022157788276672
Epoch 530, training loss: 6.701417446136475 = 0.329751193523407 + 1.0 * 6.371666431427002
Epoch 530, val loss: 0.9012921452522278
Epoch 540, training loss: 6.678201198577881 = 0.31116148829460144 + 1.0 * 6.367039680480957
Epoch 540, val loss: 0.9010436534881592
Epoch 550, training loss: 6.656567096710205 = 0.2932039201259613 + 1.0 * 6.363363265991211
Epoch 550, val loss: 0.9017105102539062
Epoch 560, training loss: 6.63749361038208 = 0.2758539617061615 + 1.0 * 6.361639499664307
Epoch 560, val loss: 0.9030352830886841
Epoch 570, training loss: 6.618541717529297 = 0.2592044472694397 + 1.0 * 6.359337329864502
Epoch 570, val loss: 0.9049107432365417
Epoch 580, training loss: 6.601898193359375 = 0.24330934882164001 + 1.0 * 6.358588695526123
Epoch 580, val loss: 0.9076647758483887
Epoch 590, training loss: 6.588013172149658 = 0.22815415263175964 + 1.0 * 6.359858989715576
Epoch 590, val loss: 0.9109328389167786
Epoch 600, training loss: 6.5721211433410645 = 0.21377716958522797 + 1.0 * 6.358344078063965
Epoch 600, val loss: 0.9147707223892212
Epoch 610, training loss: 6.561282157897949 = 0.20023129880428314 + 1.0 * 6.361051082611084
Epoch 610, val loss: 0.9191780686378479
Epoch 620, training loss: 6.538811206817627 = 0.18754157423973083 + 1.0 * 6.351269721984863
Epoch 620, val loss: 0.9239768981933594
Epoch 630, training loss: 6.524774551391602 = 0.17563261091709137 + 1.0 * 6.349142074584961
Epoch 630, val loss: 0.9293622970581055
Epoch 640, training loss: 6.51052713394165 = 0.16443677246570587 + 1.0 * 6.346090316772461
Epoch 640, val loss: 0.9351858496665955
Epoch 650, training loss: 6.521801948547363 = 0.15393084287643433 + 1.0 * 6.367871284484863
Epoch 650, val loss: 0.9412571787834167
Epoch 660, training loss: 6.492292881011963 = 0.14423240721225739 + 1.0 * 6.348060607910156
Epoch 660, val loss: 0.947595477104187
Epoch 670, training loss: 6.478745460510254 = 0.13521352410316467 + 1.0 * 6.343532085418701
Epoch 670, val loss: 0.9543373584747314
Epoch 680, training loss: 6.475183963775635 = 0.12681527435779572 + 1.0 * 6.3483686447143555
Epoch 680, val loss: 0.9613348841667175
Epoch 690, training loss: 6.461642742156982 = 0.11901234835386276 + 1.0 * 6.342630386352539
Epoch 690, val loss: 0.9685016870498657
Epoch 700, training loss: 6.456024646759033 = 0.11180727183818817 + 1.0 * 6.344217300415039
Epoch 700, val loss: 0.9759069085121155
Epoch 710, training loss: 6.442886829376221 = 0.10514194518327713 + 1.0 * 6.33774471282959
Epoch 710, val loss: 0.9834563136100769
Epoch 720, training loss: 6.434994220733643 = 0.0989535003900528 + 1.0 * 6.336040496826172
Epoch 720, val loss: 0.991243839263916
Epoch 730, training loss: 6.428281784057617 = 0.09320397675037384 + 1.0 * 6.33507776260376
Epoch 730, val loss: 0.9990938305854797
Epoch 740, training loss: 6.429887771606445 = 0.08787311613559723 + 1.0 * 6.342014789581299
Epoch 740, val loss: 1.0068823099136353
Epoch 750, training loss: 6.424658298492432 = 0.08297474682331085 + 1.0 * 6.341683387756348
Epoch 750, val loss: 1.0145610570907593
Epoch 760, training loss: 6.413275241851807 = 0.07844924926757812 + 1.0 * 6.3348259925842285
Epoch 760, val loss: 1.0224472284317017
Epoch 770, training loss: 6.403704643249512 = 0.07424569129943848 + 1.0 * 6.329459190368652
Epoch 770, val loss: 1.030342698097229
Epoch 780, training loss: 6.398524761199951 = 0.07033076137304306 + 1.0 * 6.3281941413879395
Epoch 780, val loss: 1.0382823944091797
Epoch 790, training loss: 6.399694919586182 = 0.06668372452259064 + 1.0 * 6.333011150360107
Epoch 790, val loss: 1.046311616897583
Epoch 800, training loss: 6.397994041442871 = 0.06329646706581116 + 1.0 * 6.334697723388672
Epoch 800, val loss: 1.0540072917938232
Epoch 810, training loss: 6.38492488861084 = 0.060168154537677765 + 1.0 * 6.324756622314453
Epoch 810, val loss: 1.0617918968200684
Epoch 820, training loss: 6.382296085357666 = 0.057254187762737274 + 1.0 * 6.325041770935059
Epoch 820, val loss: 1.0696130990982056
Epoch 830, training loss: 6.377909183502197 = 0.054524581879377365 + 1.0 * 6.323384761810303
Epoch 830, val loss: 1.0773839950561523
Epoch 840, training loss: 6.375432968139648 = 0.051978014409542084 + 1.0 * 6.323454856872559
Epoch 840, val loss: 1.0849237442016602
Epoch 850, training loss: 6.370692253112793 = 0.04960836097598076 + 1.0 * 6.321084022521973
Epoch 850, val loss: 1.0925319194793701
Epoch 860, training loss: 6.367880821228027 = 0.047387976199388504 + 1.0 * 6.320492744445801
Epoch 860, val loss: 1.1001094579696655
Epoch 870, training loss: 6.376835823059082 = 0.04529976844787598 + 1.0 * 6.331535816192627
Epoch 870, val loss: 1.1076384782791138
Epoch 880, training loss: 6.37005090713501 = 0.0433468259871006 + 1.0 * 6.326704025268555
Epoch 880, val loss: 1.1148192882537842
Epoch 890, training loss: 6.358398914337158 = 0.0415232852101326 + 1.0 * 6.316875457763672
Epoch 890, val loss: 1.122164249420166
Epoch 900, training loss: 6.357316017150879 = 0.03980636969208717 + 1.0 * 6.317509651184082
Epoch 900, val loss: 1.1294981241226196
Epoch 910, training loss: 6.355072498321533 = 0.03818441927433014 + 1.0 * 6.316887855529785
Epoch 910, val loss: 1.1366760730743408
Epoch 920, training loss: 6.353447437286377 = 0.036653827875852585 + 1.0 * 6.316793441772461
Epoch 920, val loss: 1.1437902450561523
Epoch 930, training loss: 6.350213527679443 = 0.035218436270952225 + 1.0 * 6.314995288848877
Epoch 930, val loss: 1.1506848335266113
Epoch 940, training loss: 6.3491010665893555 = 0.03386916220188141 + 1.0 * 6.315231800079346
Epoch 940, val loss: 1.1575545072555542
Epoch 950, training loss: 6.34442663192749 = 0.032593339681625366 + 1.0 * 6.311833381652832
Epoch 950, val loss: 1.1644552946090698
Epoch 960, training loss: 6.344626426696777 = 0.03138147294521332 + 1.0 * 6.313244819641113
Epoch 960, val loss: 1.1712220907211304
Epoch 970, training loss: 6.341001987457275 = 0.030234050005674362 + 1.0 * 6.310768127441406
Epoch 970, val loss: 1.1778039932250977
Epoch 980, training loss: 6.338166236877441 = 0.029150042682886124 + 1.0 * 6.309016227722168
Epoch 980, val loss: 1.1843794584274292
Epoch 990, training loss: 6.339269161224365 = 0.028120161965489388 + 1.0 * 6.3111491203308105
Epoch 990, val loss: 1.1908537149429321
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 10.56484603881836 = 1.968017816543579 + 1.0 * 8.59682846069336
Epoch 0, val loss: 1.9693610668182373
Epoch 10, training loss: 10.5536527633667 = 1.9572657346725464 + 1.0 * 8.596386909484863
Epoch 10, val loss: 1.958512783050537
Epoch 20, training loss: 10.536015510559082 = 1.9436229467391968 + 1.0 * 8.592392921447754
Epoch 20, val loss: 1.944595456123352
Epoch 30, training loss: 10.486331939697266 = 1.924550175666809 + 1.0 * 8.561781883239746
Epoch 30, val loss: 1.9251567125320435
Epoch 40, training loss: 10.269440650939941 = 1.9006900787353516 + 1.0 * 8.36875057220459
Epoch 40, val loss: 1.9017738103866577
Epoch 50, training loss: 9.714200973510742 = 1.876921534538269 + 1.0 * 7.837279796600342
Epoch 50, val loss: 1.879879117012024
Epoch 60, training loss: 9.306414604187012 = 1.8593155145645142 + 1.0 * 7.447098731994629
Epoch 60, val loss: 1.8633663654327393
Epoch 70, training loss: 9.045114517211914 = 1.8431649208068848 + 1.0 * 7.201949119567871
Epoch 70, val loss: 1.8476217985153198
Epoch 80, training loss: 8.85992431640625 = 1.827330470085144 + 1.0 * 7.032594203948975
Epoch 80, val loss: 1.832407832145691
Epoch 90, training loss: 8.70649528503418 = 1.8104279041290283 + 1.0 * 6.896067142486572
Epoch 90, val loss: 1.816880702972412
Epoch 100, training loss: 8.607038497924805 = 1.7940497398376465 + 1.0 * 6.812988758087158
Epoch 100, val loss: 1.802565336227417
Epoch 110, training loss: 8.536344528198242 = 1.7785910367965698 + 1.0 * 6.757753372192383
Epoch 110, val loss: 1.7896380424499512
Epoch 120, training loss: 8.470091819763184 = 1.763564109802246 + 1.0 * 6.7065277099609375
Epoch 120, val loss: 1.7768901586532593
Epoch 130, training loss: 8.415033340454102 = 1.7477285861968994 + 1.0 * 6.667304515838623
Epoch 130, val loss: 1.7631803750991821
Epoch 140, training loss: 8.368032455444336 = 1.7300618886947632 + 1.0 * 6.637970924377441
Epoch 140, val loss: 1.7478753328323364
Epoch 150, training loss: 8.326379776000977 = 1.7102675437927246 + 1.0 * 6.616111755371094
Epoch 150, val loss: 1.7309321165084839
Epoch 160, training loss: 8.278119087219238 = 1.6878525018692017 + 1.0 * 6.590266227722168
Epoch 160, val loss: 1.7118041515350342
Epoch 170, training loss: 8.231914520263672 = 1.6619848012924194 + 1.0 * 6.569930076599121
Epoch 170, val loss: 1.6898313760757446
Epoch 180, training loss: 8.18416690826416 = 1.6318819522857666 + 1.0 * 6.5522847175598145
Epoch 180, val loss: 1.664155125617981
Epoch 190, training loss: 8.140987396240234 = 1.5966533422470093 + 1.0 * 6.5443339347839355
Epoch 190, val loss: 1.6342458724975586
Epoch 200, training loss: 8.080601692199707 = 1.5566575527191162 + 1.0 * 6.523943901062012
Epoch 200, val loss: 1.6002593040466309
Epoch 210, training loss: 8.022676467895508 = 1.5114415884017944 + 1.0 * 6.511235237121582
Epoch 210, val loss: 1.5619608163833618
Epoch 220, training loss: 7.969426155090332 = 1.4612220525741577 + 1.0 * 6.508203983306885
Epoch 220, val loss: 1.5201709270477295
Epoch 230, training loss: 7.8995819091796875 = 1.4081904888153076 + 1.0 * 6.491391181945801
Epoch 230, val loss: 1.4767144918441772
Epoch 240, training loss: 7.833440780639648 = 1.352325439453125 + 1.0 * 6.481115341186523
Epoch 240, val loss: 1.4315204620361328
Epoch 250, training loss: 7.770649433135986 = 1.2946181297302246 + 1.0 * 6.476031303405762
Epoch 250, val loss: 1.385905146598816
Epoch 260, training loss: 7.701251029968262 = 1.2367463111877441 + 1.0 * 6.464504718780518
Epoch 260, val loss: 1.3410876989364624
Epoch 270, training loss: 7.635581970214844 = 1.1790268421173096 + 1.0 * 6.456554889678955
Epoch 270, val loss: 1.297577977180481
Epoch 280, training loss: 7.578293323516846 = 1.1223269701004028 + 1.0 * 6.455966472625732
Epoch 280, val loss: 1.2557722330093384
Epoch 290, training loss: 7.512951374053955 = 1.0679283142089844 + 1.0 * 6.445023059844971
Epoch 290, val loss: 1.2164568901062012
Epoch 300, training loss: 7.458871364593506 = 1.0158313512802124 + 1.0 * 6.443039894104004
Epoch 300, val loss: 1.1798664331436157
Epoch 310, training loss: 7.400108337402344 = 0.9666366577148438 + 1.0 * 6.4334716796875
Epoch 310, val loss: 1.1461124420166016
Epoch 320, training loss: 7.348404407501221 = 0.919948935508728 + 1.0 * 6.428455352783203
Epoch 320, val loss: 1.1150801181793213
Epoch 330, training loss: 7.3038201332092285 = 0.8760529160499573 + 1.0 * 6.427767276763916
Epoch 330, val loss: 1.0868699550628662
Epoch 340, training loss: 7.256166458129883 = 0.8352271914482117 + 1.0 * 6.4209394454956055
Epoch 340, val loss: 1.061651349067688
Epoch 350, training loss: 7.2138237953186035 = 0.7972069978713989 + 1.0 * 6.416616916656494
Epoch 350, val loss: 1.0393667221069336
Epoch 360, training loss: 7.175793647766113 = 0.7618697285652161 + 1.0 * 6.413923740386963
Epoch 360, val loss: 1.0198636054992676
Epoch 370, training loss: 7.136364459991455 = 0.7290744185447693 + 1.0 * 6.407289981842041
Epoch 370, val loss: 1.0030180215835571
Epoch 380, training loss: 7.106066703796387 = 0.6984491348266602 + 1.0 * 6.407617568969727
Epoch 380, val loss: 0.9885430335998535
Epoch 390, training loss: 7.0705952644348145 = 0.6698498129844666 + 1.0 * 6.400745391845703
Epoch 390, val loss: 0.9763198494911194
Epoch 400, training loss: 7.039813041687012 = 0.6428443789482117 + 1.0 * 6.396968841552734
Epoch 400, val loss: 0.9660376906394958
Epoch 410, training loss: 7.013319969177246 = 0.61726975440979 + 1.0 * 6.396049976348877
Epoch 410, val loss: 0.9574862718582153
Epoch 420, training loss: 6.984495162963867 = 0.5930753350257874 + 1.0 * 6.391419887542725
Epoch 420, val loss: 0.9506799578666687
Epoch 430, training loss: 6.958415508270264 = 0.5699471831321716 + 1.0 * 6.388468265533447
Epoch 430, val loss: 0.9452794790267944
Epoch 440, training loss: 6.949505805969238 = 0.5477082133293152 + 1.0 * 6.401797771453857
Epoch 440, val loss: 0.9411607384681702
Epoch 450, training loss: 6.914160251617432 = 0.52660071849823 + 1.0 * 6.387559413909912
Epoch 450, val loss: 0.9383092522621155
Epoch 460, training loss: 6.887176990509033 = 0.506317675113678 + 1.0 * 6.380859375
Epoch 460, val loss: 0.9366068243980408
Epoch 470, training loss: 6.866826057434082 = 0.4866853654384613 + 1.0 * 6.380140781402588
Epoch 470, val loss: 0.9358504414558411
Epoch 480, training loss: 6.844682216644287 = 0.46770209074020386 + 1.0 * 6.376980304718018
Epoch 480, val loss: 0.9359644055366516
Epoch 490, training loss: 6.825514793395996 = 0.4493929147720337 + 1.0 * 6.376121997833252
Epoch 490, val loss: 0.9369059205055237
Epoch 500, training loss: 6.8034162521362305 = 0.431694895029068 + 1.0 * 6.371721267700195
Epoch 500, val loss: 0.9386399984359741
Epoch 510, training loss: 6.790334224700928 = 0.4144921898841858 + 1.0 * 6.375842094421387
Epoch 510, val loss: 0.9410308599472046
Epoch 520, training loss: 6.767673969268799 = 0.3978101313114166 + 1.0 * 6.369863986968994
Epoch 520, val loss: 0.9441942572593689
Epoch 530, training loss: 6.745884895324707 = 0.38165661692619324 + 1.0 * 6.364228248596191
Epoch 530, val loss: 0.9480193257331848
Epoch 540, training loss: 6.7310872077941895 = 0.36592191457748413 + 1.0 * 6.3651652336120605
Epoch 540, val loss: 0.9524461627006531
Epoch 550, training loss: 6.717213153839111 = 0.3506799638271332 + 1.0 * 6.366533279418945
Epoch 550, val loss: 0.9573785662651062
Epoch 560, training loss: 6.698266506195068 = 0.336031049489975 + 1.0 * 6.3622355461120605
Epoch 560, val loss: 0.9629820585250854
Epoch 570, training loss: 6.679203987121582 = 0.32180142402648926 + 1.0 * 6.357402324676514
Epoch 570, val loss: 0.9690271615982056
Epoch 580, training loss: 6.666167259216309 = 0.3079579472541809 + 1.0 * 6.358209133148193
Epoch 580, val loss: 0.9755764603614807
Epoch 590, training loss: 6.655782699584961 = 0.2945823073387146 + 1.0 * 6.361200332641602
Epoch 590, val loss: 0.9826897382736206
Epoch 600, training loss: 6.634613513946533 = 0.2816988229751587 + 1.0 * 6.352914810180664
Epoch 600, val loss: 0.9903193712234497
Epoch 610, training loss: 6.620995998382568 = 0.2692337930202484 + 1.0 * 6.351762294769287
Epoch 610, val loss: 0.9983664155006409
Epoch 620, training loss: 6.607908248901367 = 0.2571781873703003 + 1.0 * 6.350729942321777
Epoch 620, val loss: 1.0069457292556763
Epoch 630, training loss: 6.594601631164551 = 0.24558840692043304 + 1.0 * 6.349013328552246
Epoch 630, val loss: 1.0161021947860718
Epoch 640, training loss: 6.581460952758789 = 0.23440372943878174 + 1.0 * 6.347057342529297
Epoch 640, val loss: 1.0256924629211426
Epoch 650, training loss: 6.574501037597656 = 0.22364585101604462 + 1.0 * 6.350855350494385
Epoch 650, val loss: 1.0357146263122559
Epoch 660, training loss: 6.558358192443848 = 0.21336719393730164 + 1.0 * 6.344991207122803
Epoch 660, val loss: 1.0462368726730347
Epoch 670, training loss: 6.546205043792725 = 0.2034662812948227 + 1.0 * 6.342738628387451
Epoch 670, val loss: 1.0570929050445557
Epoch 680, training loss: 6.533542633056641 = 0.19396932423114777 + 1.0 * 6.339573383331299
Epoch 680, val loss: 1.06842041015625
Epoch 690, training loss: 6.540655136108398 = 0.1848650425672531 + 1.0 * 6.355790138244629
Epoch 690, val loss: 1.0801228284835815
Epoch 700, training loss: 6.521787166595459 = 0.1762671023607254 + 1.0 * 6.34552001953125
Epoch 700, val loss: 1.0920699834823608
Epoch 710, training loss: 6.507387638092041 = 0.16807502508163452 + 1.0 * 6.339312553405762
Epoch 710, val loss: 1.1043145656585693
Epoch 720, training loss: 6.496440410614014 = 0.16027779877185822 + 1.0 * 6.336162567138672
Epoch 720, val loss: 1.1167572736740112
Epoch 730, training loss: 6.490270614624023 = 0.15286187827587128 + 1.0 * 6.337408542633057
Epoch 730, val loss: 1.1295119524002075
Epoch 740, training loss: 6.478819847106934 = 0.14580292999744415 + 1.0 * 6.333016872406006
Epoch 740, val loss: 1.142443299293518
Epoch 750, training loss: 6.473349094390869 = 0.13909542560577393 + 1.0 * 6.334253787994385
Epoch 750, val loss: 1.1555582284927368
Epoch 760, training loss: 6.4647216796875 = 0.13273964822292328 + 1.0 * 6.331982135772705
Epoch 760, val loss: 1.1689039468765259
Epoch 770, training loss: 6.454951286315918 = 0.12672238051891327 + 1.0 * 6.328228950500488
Epoch 770, val loss: 1.182332992553711
Epoch 780, training loss: 6.448243141174316 = 0.12101473659276962 + 1.0 * 6.327228546142578
Epoch 780, val loss: 1.1958998441696167
Epoch 790, training loss: 6.451603889465332 = 0.11559849232435226 + 1.0 * 6.336005210876465
Epoch 790, val loss: 1.2095379829406738
Epoch 800, training loss: 6.441117763519287 = 0.11046940833330154 + 1.0 * 6.330648422241211
Epoch 800, val loss: 1.2232415676116943
Epoch 810, training loss: 6.432333469390869 = 0.1056266650557518 + 1.0 * 6.326706886291504
Epoch 810, val loss: 1.2369120121002197
Epoch 820, training loss: 6.430613040924072 = 0.10102590173482895 + 1.0 * 6.329586982727051
Epoch 820, val loss: 1.2506208419799805
Epoch 830, training loss: 6.421489238739014 = 0.0966818556189537 + 1.0 * 6.324807167053223
Epoch 830, val loss: 1.2642937898635864
Epoch 840, training loss: 6.414900779724121 = 0.09256449341773987 + 1.0 * 6.322336196899414
Epoch 840, val loss: 1.2779682874679565
Epoch 850, training loss: 6.419363498687744 = 0.08866070210933685 + 1.0 * 6.330702781677246
Epoch 850, val loss: 1.2915453910827637
Epoch 860, training loss: 6.408911228179932 = 0.08495660126209259 + 1.0 * 6.3239545822143555
Epoch 860, val loss: 1.305129885673523
Epoch 870, training loss: 6.400636672973633 = 0.08145280182361603 + 1.0 * 6.319183826446533
Epoch 870, val loss: 1.3186256885528564
Epoch 880, training loss: 6.394557952880859 = 0.07811746746301651 + 1.0 * 6.316440582275391
Epoch 880, val loss: 1.3321166038513184
Epoch 890, training loss: 6.40611457824707 = 0.07494433224201202 + 1.0 * 6.331170082092285
Epoch 890, val loss: 1.3455092906951904
Epoch 900, training loss: 6.392739295959473 = 0.07194352149963379 + 1.0 * 6.32079553604126
Epoch 900, val loss: 1.3588463068008423
Epoch 910, training loss: 6.382927417755127 = 0.06910315901041031 + 1.0 * 6.31382417678833
Epoch 910, val loss: 1.3720248937606812
Epoch 920, training loss: 6.3816752433776855 = 0.06639707833528519 + 1.0 * 6.315278053283691
Epoch 920, val loss: 1.3850843906402588
Epoch 930, training loss: 6.3841938972473145 = 0.06382187455892563 + 1.0 * 6.320372104644775
Epoch 930, val loss: 1.398048758506775
Epoch 940, training loss: 6.377076625823975 = 0.06138487532734871 + 1.0 * 6.315691947937012
Epoch 940, val loss: 1.4109861850738525
Epoch 950, training loss: 6.372628688812256 = 0.05905928835272789 + 1.0 * 6.31356954574585
Epoch 950, val loss: 1.423701524734497
Epoch 960, training loss: 6.368307590484619 = 0.05684733763337135 + 1.0 * 6.311460018157959
Epoch 960, val loss: 1.4363712072372437
Epoch 970, training loss: 6.367472171783447 = 0.054742708802223206 + 1.0 * 6.312729358673096
Epoch 970, val loss: 1.4489866495132446
Epoch 980, training loss: 6.37005090713501 = 0.052735909819602966 + 1.0 * 6.317315101623535
Epoch 980, val loss: 1.4614301919937134
Epoch 990, training loss: 6.359614849090576 = 0.05083466321229935 + 1.0 * 6.308780193328857
Epoch 990, val loss: 1.4737128019332886
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.808645229309436
The final CL Acc:0.75185, 0.04201, The final GNN Acc:0.81058, 0.00151
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13224])
remove edge: torch.Size([2, 7906])
updated graph: torch.Size([2, 10574])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.552144050598145 = 1.9552942514419556 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.9467999935150146
Epoch 10, training loss: 10.54107666015625 = 1.9444737434387207 + 1.0 * 8.596603393554688
Epoch 10, val loss: 1.9364699125289917
Epoch 20, training loss: 10.525455474853516 = 1.9313515424728394 + 1.0 * 8.594103813171387
Epoch 20, val loss: 1.9235044717788696
Epoch 30, training loss: 10.484709739685059 = 1.913366436958313 + 1.0 * 8.571343421936035
Epoch 30, val loss: 1.9054676294326782
Epoch 40, training loss: 10.2963228225708 = 1.8902219533920288 + 1.0 * 8.40610122680664
Epoch 40, val loss: 1.882941484451294
Epoch 50, training loss: 9.902122497558594 = 1.8650002479553223 + 1.0 * 8.03712272644043
Epoch 50, val loss: 1.8591010570526123
Epoch 60, training loss: 9.610295295715332 = 1.8424309492111206 + 1.0 * 7.767864227294922
Epoch 60, val loss: 1.8383926153182983
Epoch 70, training loss: 9.145710945129395 = 1.8253737688064575 + 1.0 * 7.320336818695068
Epoch 70, val loss: 1.8227561712265015
Epoch 80, training loss: 8.842249870300293 = 1.8122252225875854 + 1.0 * 7.030024528503418
Epoch 80, val loss: 1.810160517692566
Epoch 90, training loss: 8.704100608825684 = 1.7940270900726318 + 1.0 * 6.910073280334473
Epoch 90, val loss: 1.7930684089660645
Epoch 100, training loss: 8.594905853271484 = 1.7726777791976929 + 1.0 * 6.822227954864502
Epoch 100, val loss: 1.774362564086914
Epoch 110, training loss: 8.513011932373047 = 1.7526021003723145 + 1.0 * 6.760410308837891
Epoch 110, val loss: 1.7570856809616089
Epoch 120, training loss: 8.436763763427734 = 1.7324645519256592 + 1.0 * 6.704298973083496
Epoch 120, val loss: 1.7392752170562744
Epoch 130, training loss: 8.369120597839355 = 1.710391640663147 + 1.0 * 6.65872859954834
Epoch 130, val loss: 1.719639539718628
Epoch 140, training loss: 8.300512313842773 = 1.6854946613311768 + 1.0 * 6.615017890930176
Epoch 140, val loss: 1.697629690170288
Epoch 150, training loss: 8.242573738098145 = 1.6569257974624634 + 1.0 * 6.5856475830078125
Epoch 150, val loss: 1.672560214996338
Epoch 160, training loss: 8.184253692626953 = 1.6244378089904785 + 1.0 * 6.559815406799316
Epoch 160, val loss: 1.644108533859253
Epoch 170, training loss: 8.126157760620117 = 1.587908387184143 + 1.0 * 6.5382490158081055
Epoch 170, val loss: 1.6122468709945679
Epoch 180, training loss: 8.073750495910645 = 1.547577142715454 + 1.0 * 6.526173114776611
Epoch 180, val loss: 1.5774184465408325
Epoch 190, training loss: 8.01211929321289 = 1.5048078298568726 + 1.0 * 6.507311820983887
Epoch 190, val loss: 1.5409950017929077
Epoch 200, training loss: 7.955358505249023 = 1.459932804107666 + 1.0 * 6.495425701141357
Epoch 200, val loss: 1.5032906532287598
Epoch 210, training loss: 7.8964362144470215 = 1.4136500358581543 + 1.0 * 6.482786178588867
Epoch 210, val loss: 1.464976191520691
Epoch 220, training loss: 7.837650299072266 = 1.366335153579712 + 1.0 * 6.471315383911133
Epoch 220, val loss: 1.4263097047805786
Epoch 230, training loss: 7.78484582901001 = 1.3181205987930298 + 1.0 * 6.4667253494262695
Epoch 230, val loss: 1.3875796794891357
Epoch 240, training loss: 7.723880767822266 = 1.2696212530136108 + 1.0 * 6.454259395599365
Epoch 240, val loss: 1.3492201566696167
Epoch 250, training loss: 7.667455673217773 = 1.2206430435180664 + 1.0 * 6.446812629699707
Epoch 250, val loss: 1.3108632564544678
Epoch 260, training loss: 7.616495609283447 = 1.1711269617080688 + 1.0 * 6.445368766784668
Epoch 260, val loss: 1.2726037502288818
Epoch 270, training loss: 7.557068824768066 = 1.12185537815094 + 1.0 * 6.435213565826416
Epoch 270, val loss: 1.2348281145095825
Epoch 280, training loss: 7.501847743988037 = 1.072310447692871 + 1.0 * 6.429537296295166
Epoch 280, val loss: 1.1969329118728638
Epoch 290, training loss: 7.44678258895874 = 1.0223392248153687 + 1.0 * 6.424443244934082
Epoch 290, val loss: 1.1588152647018433
Epoch 300, training loss: 7.393311500549316 = 0.9724665880203247 + 1.0 * 6.420845031738281
Epoch 300, val loss: 1.12083101272583
Epoch 310, training loss: 7.341048717498779 = 0.923658549785614 + 1.0 * 6.4173903465271
Epoch 310, val loss: 1.0836352109909058
Epoch 320, training loss: 7.288941383361816 = 0.8765292167663574 + 1.0 * 6.412412166595459
Epoch 320, val loss: 1.047842264175415
Epoch 330, training loss: 7.24078893661499 = 0.8315526843070984 + 1.0 * 6.409236431121826
Epoch 330, val loss: 1.0140446424484253
Epoch 340, training loss: 7.19502067565918 = 0.7892857789993286 + 1.0 * 6.405735015869141
Epoch 340, val loss: 0.98284912109375
Epoch 350, training loss: 7.1520209312438965 = 0.7499635815620422 + 1.0 * 6.40205717086792
Epoch 350, val loss: 0.9545326232910156
Epoch 360, training loss: 7.1114821434021 = 0.713165819644928 + 1.0 * 6.398316383361816
Epoch 360, val loss: 0.9290022850036621
Epoch 370, training loss: 7.0773701667785645 = 0.6784104108810425 + 1.0 * 6.398959636688232
Epoch 370, val loss: 0.906006932258606
Epoch 380, training loss: 7.043346881866455 = 0.6458348631858826 + 1.0 * 6.397511959075928
Epoch 380, val loss: 0.8855828642845154
Epoch 390, training loss: 7.006218910217285 = 0.6151381134986877 + 1.0 * 6.391080856323242
Epoch 390, val loss: 0.8673847913742065
Epoch 400, training loss: 6.973167419433594 = 0.585498571395874 + 1.0 * 6.387669086456299
Epoch 400, val loss: 0.8509251475334167
Epoch 410, training loss: 6.94111967086792 = 0.5565590262413025 + 1.0 * 6.384560585021973
Epoch 410, val loss: 0.835831880569458
Epoch 420, training loss: 6.910129070281982 = 0.5283093452453613 + 1.0 * 6.381819725036621
Epoch 420, val loss: 0.8220434188842773
Epoch 430, training loss: 6.892390251159668 = 0.5008012652397156 + 1.0 * 6.391589164733887
Epoch 430, val loss: 0.8096204996109009
Epoch 440, training loss: 6.852296829223633 = 0.47439202666282654 + 1.0 * 6.377904891967773
Epoch 440, val loss: 0.7988094091415405
Epoch 450, training loss: 6.824100017547607 = 0.4490866959095001 + 1.0 * 6.37501335144043
Epoch 450, val loss: 0.7895147204399109
Epoch 460, training loss: 6.805386066436768 = 0.4248908758163452 + 1.0 * 6.380495071411133
Epoch 460, val loss: 0.7818346619606018
Epoch 470, training loss: 6.774171352386475 = 0.4019409716129303 + 1.0 * 6.372230529785156
Epoch 470, val loss: 0.7757238745689392
Epoch 480, training loss: 6.751531600952148 = 0.3802196979522705 + 1.0 * 6.371311664581299
Epoch 480, val loss: 0.771056592464447
Epoch 490, training loss: 6.726944923400879 = 0.35969820618629456 + 1.0 * 6.367246627807617
Epoch 490, val loss: 0.7678929567337036
Epoch 500, training loss: 6.706714153289795 = 0.3402779698371887 + 1.0 * 6.366436004638672
Epoch 500, val loss: 0.7660676836967468
Epoch 510, training loss: 6.685070991516113 = 0.3219044804573059 + 1.0 * 6.363166332244873
Epoch 510, val loss: 0.7655524611473083
Epoch 520, training loss: 6.666065216064453 = 0.30451351404190063 + 1.0 * 6.361551761627197
Epoch 520, val loss: 0.7661585211753845
Epoch 530, training loss: 6.647665023803711 = 0.28804925084114075 + 1.0 * 6.359615802764893
Epoch 530, val loss: 0.7678279280662537
Epoch 540, training loss: 6.63090705871582 = 0.272526353597641 + 1.0 * 6.3583807945251465
Epoch 540, val loss: 0.7703861594200134
Epoch 550, training loss: 6.61355447769165 = 0.2579149305820465 + 1.0 * 6.355639457702637
Epoch 550, val loss: 0.7736973762512207
Epoch 560, training loss: 6.596804618835449 = 0.24403917789459229 + 1.0 * 6.3527655601501465
Epoch 560, val loss: 0.7778311371803284
Epoch 570, training loss: 6.588067054748535 = 0.23083868622779846 + 1.0 * 6.3572282791137695
Epoch 570, val loss: 0.7827391028404236
Epoch 580, training loss: 6.573192596435547 = 0.21834829449653625 + 1.0 * 6.354844093322754
Epoch 580, val loss: 0.7883282899856567
Epoch 590, training loss: 6.555415153503418 = 0.2065727263689041 + 1.0 * 6.348842620849609
Epoch 590, val loss: 0.7942380905151367
Epoch 600, training loss: 6.542045593261719 = 0.19537441432476044 + 1.0 * 6.346671104431152
Epoch 600, val loss: 0.8008471727371216
Epoch 610, training loss: 6.532105445861816 = 0.18479116261005402 + 1.0 * 6.347314357757568
Epoch 610, val loss: 0.8079550266265869
Epoch 620, training loss: 6.5164594650268555 = 0.17479288578033447 + 1.0 * 6.3416666984558105
Epoch 620, val loss: 0.8153626322746277
Epoch 630, training loss: 6.50659704208374 = 0.16533344984054565 + 1.0 * 6.341263771057129
Epoch 630, val loss: 0.8233000040054321
Epoch 640, training loss: 6.502114295959473 = 0.15639019012451172 + 1.0 * 6.345724105834961
Epoch 640, val loss: 0.8317089676856995
Epoch 650, training loss: 6.486318588256836 = 0.14796341955661774 + 1.0 * 6.33835506439209
Epoch 650, val loss: 0.8401821255683899
Epoch 660, training loss: 6.475829124450684 = 0.14001688361167908 + 1.0 * 6.335812091827393
Epoch 660, val loss: 0.8490239977836609
Epoch 670, training loss: 6.4697980880737305 = 0.13250212371349335 + 1.0 * 6.337296009063721
Epoch 670, val loss: 0.8581938147544861
Epoch 680, training loss: 6.461603164672852 = 0.12541185319423676 + 1.0 * 6.336191177368164
Epoch 680, val loss: 0.8676322102546692
Epoch 690, training loss: 6.454929351806641 = 0.11877711117267609 + 1.0 * 6.336152076721191
Epoch 690, val loss: 0.8770797252655029
Epoch 700, training loss: 6.442911624908447 = 0.11254151910543442 + 1.0 * 6.33036994934082
Epoch 700, val loss: 0.8866705298423767
Epoch 710, training loss: 6.4349141120910645 = 0.10667313635349274 + 1.0 * 6.328240871429443
Epoch 710, val loss: 0.8964151740074158
Epoch 720, training loss: 6.431684494018555 = 0.10115332156419754 + 1.0 * 6.330531120300293
Epoch 720, val loss: 0.9063059687614441
Epoch 730, training loss: 6.425693988800049 = 0.09596353024244308 + 1.0 * 6.32973051071167
Epoch 730, val loss: 0.9164155125617981
Epoch 740, training loss: 6.416962623596191 = 0.09111615270376205 + 1.0 * 6.3258466720581055
Epoch 740, val loss: 0.9261687397956848
Epoch 750, training loss: 6.409409999847412 = 0.08654943108558655 + 1.0 * 6.3228607177734375
Epoch 750, val loss: 0.9362003207206726
Epoch 760, training loss: 6.417741298675537 = 0.08226129412651062 + 1.0 * 6.335480213165283
Epoch 760, val loss: 0.9463532567024231
Epoch 770, training loss: 6.399174690246582 = 0.07823684811592102 + 1.0 * 6.320937633514404
Epoch 770, val loss: 0.9561550617218018
Epoch 780, training loss: 6.3944244384765625 = 0.07445693016052246 + 1.0 * 6.319967746734619
Epoch 780, val loss: 0.9660729169845581
Epoch 790, training loss: 6.388835906982422 = 0.07090220600366592 + 1.0 * 6.317933559417725
Epoch 790, val loss: 0.9760816693305969
Epoch 800, training loss: 6.393299579620361 = 0.06754638999700546 + 1.0 * 6.325753211975098
Epoch 800, val loss: 0.9860790371894836
Epoch 810, training loss: 6.386536598205566 = 0.06439512968063354 + 1.0 * 6.322141647338867
Epoch 810, val loss: 0.995856523513794
Epoch 820, training loss: 6.3772101402282715 = 0.06144162267446518 + 1.0 * 6.315768718719482
Epoch 820, val loss: 1.0055491924285889
Epoch 830, training loss: 6.371842861175537 = 0.05866619572043419 + 1.0 * 6.31317663192749
Epoch 830, val loss: 1.0151174068450928
Epoch 840, training loss: 6.368659973144531 = 0.056045930832624435 + 1.0 * 6.3126139640808105
Epoch 840, val loss: 1.0247007608413696
Epoch 850, training loss: 6.377911567687988 = 0.05357595533132553 + 1.0 * 6.32433557510376
Epoch 850, val loss: 1.0341854095458984
Epoch 860, training loss: 6.3648295402526855 = 0.05124606564640999 + 1.0 * 6.3135833740234375
Epoch 860, val loss: 1.0436939001083374
Epoch 870, training loss: 6.359447002410889 = 0.04905878007411957 + 1.0 * 6.310388088226318
Epoch 870, val loss: 1.0527122020721436
Epoch 880, training loss: 6.355937480926514 = 0.046987686306238174 + 1.0 * 6.308949947357178
Epoch 880, val loss: 1.062018632888794
Epoch 890, training loss: 6.3700432777404785 = 0.04503364488482475 + 1.0 * 6.325009822845459
Epoch 890, val loss: 1.0711828470230103
Epoch 900, training loss: 6.356450080871582 = 0.04318837448954582 + 1.0 * 6.31326150894165
Epoch 900, val loss: 1.0800142288208008
Epoch 910, training loss: 6.347089767456055 = 0.041451018303632736 + 1.0 * 6.305638790130615
Epoch 910, val loss: 1.0886716842651367
Epoch 920, training loss: 6.3441667556762695 = 0.03980528563261032 + 1.0 * 6.304361343383789
Epoch 920, val loss: 1.0975443124771118
Epoch 930, training loss: 6.353536605834961 = 0.03824274614453316 + 1.0 * 6.315293788909912
Epoch 930, val loss: 1.1062676906585693
Epoch 940, training loss: 6.346968650817871 = 0.03677473962306976 + 1.0 * 6.31019401550293
Epoch 940, val loss: 1.114752173423767
Epoch 950, training loss: 6.337249755859375 = 0.03538049757480621 + 1.0 * 6.3018693923950195
Epoch 950, val loss: 1.1229703426361084
Epoch 960, training loss: 6.336009502410889 = 0.034060053527355194 + 1.0 * 6.301949501037598
Epoch 960, val loss: 1.1313930749893188
Epoch 970, training loss: 6.3343505859375 = 0.03280487284064293 + 1.0 * 6.3015456199646
Epoch 970, val loss: 1.1397563219070435
Epoch 980, training loss: 6.336170673370361 = 0.0316176675260067 + 1.0 * 6.304553031921387
Epoch 980, val loss: 1.1478691101074219
Epoch 990, training loss: 6.332015514373779 = 0.030495494604110718 + 1.0 * 6.301519870758057
Epoch 990, val loss: 1.1555848121643066
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.527692794799805 = 1.930843472480774 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.9391294717788696
Epoch 10, training loss: 10.51801872253418 = 1.9213725328445435 + 1.0 * 8.596646308898926
Epoch 10, val loss: 1.9293707609176636
Epoch 20, training loss: 10.504578590393066 = 1.9097182750701904 + 1.0 * 8.594860076904297
Epoch 20, val loss: 1.916774034500122
Epoch 30, training loss: 10.473123550415039 = 1.8934351205825806 + 1.0 * 8.57968807220459
Epoch 30, val loss: 1.8986783027648926
Epoch 40, training loss: 10.363594055175781 = 1.8714141845703125 + 1.0 * 8.492179870605469
Epoch 40, val loss: 1.87467360496521
Epoch 50, training loss: 9.980513572692871 = 1.8476358652114868 + 1.0 * 8.132877349853516
Epoch 50, val loss: 1.850182294845581
Epoch 60, training loss: 9.673324584960938 = 1.8244801759719849 + 1.0 * 7.848844528198242
Epoch 60, val loss: 1.8274378776550293
Epoch 70, training loss: 9.252605438232422 = 1.80472731590271 + 1.0 * 7.447878360748291
Epoch 70, val loss: 1.8081049919128418
Epoch 80, training loss: 8.955135345458984 = 1.787785291671753 + 1.0 * 7.1673502922058105
Epoch 80, val loss: 1.7911162376403809
Epoch 90, training loss: 8.769259452819824 = 1.7701975107192993 + 1.0 * 6.9990620613098145
Epoch 90, val loss: 1.7744683027267456
Epoch 100, training loss: 8.630010604858398 = 1.7498047351837158 + 1.0 * 6.8802056312561035
Epoch 100, val loss: 1.7556582689285278
Epoch 110, training loss: 8.539546966552734 = 1.727293848991394 + 1.0 * 6.812252998352051
Epoch 110, val loss: 1.7348883152008057
Epoch 120, training loss: 8.464337348937988 = 1.7025202512741089 + 1.0 * 6.761817455291748
Epoch 120, val loss: 1.7127379179000854
Epoch 130, training loss: 8.386879920959473 = 1.675392746925354 + 1.0 * 6.71148681640625
Epoch 130, val loss: 1.6895880699157715
Epoch 140, training loss: 8.314454078674316 = 1.644228219985962 + 1.0 * 6.670226097106934
Epoch 140, val loss: 1.6632964611053467
Epoch 150, training loss: 8.247568130493164 = 1.6069598197937012 + 1.0 * 6.640608787536621
Epoch 150, val loss: 1.6320323944091797
Epoch 160, training loss: 8.17681884765625 = 1.563152551651001 + 1.0 * 6.61366605758667
Epoch 160, val loss: 1.595535397529602
Epoch 170, training loss: 8.107342720031738 = 1.5126104354858398 + 1.0 * 6.594732284545898
Epoch 170, val loss: 1.5533109903335571
Epoch 180, training loss: 8.035804748535156 = 1.4550557136535645 + 1.0 * 6.580748558044434
Epoch 180, val loss: 1.5052587985992432
Epoch 190, training loss: 7.95706844329834 = 1.3917748928070068 + 1.0 * 6.565293788909912
Epoch 190, val loss: 1.4529365301132202
Epoch 200, training loss: 7.877011775970459 = 1.323995590209961 + 1.0 * 6.553016185760498
Epoch 200, val loss: 1.3975359201431274
Epoch 210, training loss: 7.799319267272949 = 1.2554373741149902 + 1.0 * 6.543881893157959
Epoch 210, val loss: 1.3424030542373657
Epoch 220, training loss: 7.716549873352051 = 1.1903514862060547 + 1.0 * 6.526198387145996
Epoch 220, val loss: 1.2903000116348267
Epoch 230, training loss: 7.640467643737793 = 1.1278973817825317 + 1.0 * 6.512570381164551
Epoch 230, val loss: 1.2411043643951416
Epoch 240, training loss: 7.569581985473633 = 1.0684881210327148 + 1.0 * 6.501093864440918
Epoch 240, val loss: 1.1948320865631104
Epoch 250, training loss: 7.512699127197266 = 1.012584924697876 + 1.0 * 6.5001139640808105
Epoch 250, val loss: 1.1519566774368286
Epoch 260, training loss: 7.443413734436035 = 0.9614851474761963 + 1.0 * 6.48192834854126
Epoch 260, val loss: 1.1133449077606201
Epoch 270, training loss: 7.386388778686523 = 0.9142007827758789 + 1.0 * 6.4721879959106445
Epoch 270, val loss: 1.077921748161316
Epoch 280, training loss: 7.335249423980713 = 0.8702325224876404 + 1.0 * 6.465016841888428
Epoch 280, val loss: 1.045335292816162
Epoch 290, training loss: 7.285516738891602 = 0.8293642997741699 + 1.0 * 6.456152439117432
Epoch 290, val loss: 1.0151159763336182
Epoch 300, training loss: 7.241868019104004 = 0.7907663583755493 + 1.0 * 6.451101779937744
Epoch 300, val loss: 0.9869168996810913
Epoch 310, training loss: 7.202345848083496 = 0.7546521425247192 + 1.0 * 6.447693824768066
Epoch 310, val loss: 0.9606715440750122
Epoch 320, training loss: 7.160245895385742 = 0.7205952405929565 + 1.0 * 6.439650535583496
Epoch 320, val loss: 0.9365397691726685
Epoch 330, training loss: 7.126472473144531 = 0.6881572604179382 + 1.0 * 6.438315391540527
Epoch 330, val loss: 0.9141152501106262
Epoch 340, training loss: 7.086277484893799 = 0.6570682525634766 + 1.0 * 6.429209232330322
Epoch 340, val loss: 0.893272340297699
Epoch 350, training loss: 7.054986953735352 = 0.6270689368247986 + 1.0 * 6.427917957305908
Epoch 350, val loss: 0.8739926815032959
Epoch 360, training loss: 7.018349647521973 = 0.5980916023254395 + 1.0 * 6.420258045196533
Epoch 360, val loss: 0.8562561273574829
Epoch 370, training loss: 6.9882707595825195 = 0.56990647315979 + 1.0 * 6.418364524841309
Epoch 370, val loss: 0.8400346636772156
Epoch 380, training loss: 6.957033634185791 = 0.5428570508956909 + 1.0 * 6.4141764640808105
Epoch 380, val loss: 0.825477659702301
Epoch 390, training loss: 6.924915313720703 = 0.5170556306838989 + 1.0 * 6.407859802246094
Epoch 390, val loss: 0.8127498626708984
Epoch 400, training loss: 6.897079944610596 = 0.49237483739852905 + 1.0 * 6.404705047607422
Epoch 400, val loss: 0.8016963005065918
Epoch 410, training loss: 6.870834827423096 = 0.4689961373806 + 1.0 * 6.401838779449463
Epoch 410, val loss: 0.7923439145088196
Epoch 420, training loss: 6.843384265899658 = 0.4471612572669983 + 1.0 * 6.396223068237305
Epoch 420, val loss: 0.784787118434906
Epoch 430, training loss: 6.82015323638916 = 0.42662352323532104 + 1.0 * 6.393529891967773
Epoch 430, val loss: 0.778732180595398
Epoch 440, training loss: 6.807584762573242 = 0.40732091665267944 + 1.0 * 6.400263786315918
Epoch 440, val loss: 0.7739594578742981
Epoch 450, training loss: 6.777824878692627 = 0.38943296670913696 + 1.0 * 6.388391971588135
Epoch 450, val loss: 0.7703837752342224
Epoch 460, training loss: 6.757807731628418 = 0.37274524569511414 + 1.0 * 6.3850626945495605
Epoch 460, val loss: 0.7678926587104797
Epoch 470, training loss: 6.739875793457031 = 0.35708948969841003 + 1.0 * 6.382786273956299
Epoch 470, val loss: 0.7662584781646729
Epoch 480, training loss: 6.725854873657227 = 0.34241586923599243 + 1.0 * 6.383439064025879
Epoch 480, val loss: 0.7654006481170654
Epoch 490, training loss: 6.70720911026001 = 0.3287551999092102 + 1.0 * 6.378453731536865
Epoch 490, val loss: 0.7652716040611267
Epoch 500, training loss: 6.688663959503174 = 0.3158588111400604 + 1.0 * 6.372805118560791
Epoch 500, val loss: 0.7657868266105652
Epoch 510, training loss: 6.688019752502441 = 0.3036521375179291 + 1.0 * 6.3843674659729
Epoch 510, val loss: 0.766827404499054
Epoch 520, training loss: 6.661491394042969 = 0.29213154315948486 + 1.0 * 6.369359970092773
Epoch 520, val loss: 0.7683208584785461
Epoch 530, training loss: 6.649155139923096 = 0.2811777889728546 + 1.0 * 6.367977142333984
Epoch 530, val loss: 0.770355761051178
Epoch 540, training loss: 6.640162467956543 = 0.27065029740333557 + 1.0 * 6.36951208114624
Epoch 540, val loss: 0.7727380990982056
Epoch 550, training loss: 6.626873016357422 = 0.26051798462867737 + 1.0 * 6.366354942321777
Epoch 550, val loss: 0.7754219174385071
Epoch 560, training loss: 6.610414981842041 = 0.2505990266799927 + 1.0 * 6.359816074371338
Epoch 560, val loss: 0.7784322500228882
Epoch 570, training loss: 6.598807334899902 = 0.2407638132572174 + 1.0 * 6.358043670654297
Epoch 570, val loss: 0.7816548943519592
Epoch 580, training loss: 6.59300422668457 = 0.23089604079723358 + 1.0 * 6.36210823059082
Epoch 580, val loss: 0.7849605083465576
Epoch 590, training loss: 6.576000213623047 = 0.22092629969120026 + 1.0 * 6.355073928833008
Epoch 590, val loss: 0.7883284091949463
Epoch 600, training loss: 6.563496112823486 = 0.2106936275959015 + 1.0 * 6.352802276611328
Epoch 600, val loss: 0.7918226718902588
Epoch 610, training loss: 6.56627893447876 = 0.20014327764511108 + 1.0 * 6.366135597229004
Epoch 610, val loss: 0.7952423095703125
Epoch 620, training loss: 6.540764808654785 = 0.18942168354988098 + 1.0 * 6.351343154907227
Epoch 620, val loss: 0.7985642552375793
Epoch 630, training loss: 6.5255022048950195 = 0.17857475578784943 + 1.0 * 6.346927642822266
Epoch 630, val loss: 0.8020052909851074
Epoch 640, training loss: 6.513777256011963 = 0.16774839162826538 + 1.0 * 6.346028804779053
Epoch 640, val loss: 0.805513858795166
Epoch 650, training loss: 6.513732433319092 = 0.15717430412769318 + 1.0 * 6.356558322906494
Epoch 650, val loss: 0.809191107749939
Epoch 660, training loss: 6.493162631988525 = 0.14707323908805847 + 1.0 * 6.3460893630981445
Epoch 660, val loss: 0.8130975961685181
Epoch 670, training loss: 6.488064765930176 = 0.13758942484855652 + 1.0 * 6.350475311279297
Epoch 670, val loss: 0.8174266219139099
Epoch 680, training loss: 6.473600387573242 = 0.12880603969097137 + 1.0 * 6.344794273376465
Epoch 680, val loss: 0.822081983089447
Epoch 690, training loss: 6.460630893707275 = 0.12067310512065887 + 1.0 * 6.3399577140808105
Epoch 690, val loss: 0.8270625472068787
Epoch 700, training loss: 6.454344749450684 = 0.11317700147628784 + 1.0 * 6.34116792678833
Epoch 700, val loss: 0.8323231935501099
Epoch 710, training loss: 6.44519567489624 = 0.10629535466432571 + 1.0 * 6.338900089263916
Epoch 710, val loss: 0.8379031419754028
Epoch 720, training loss: 6.437429904937744 = 0.09997671097517014 + 1.0 * 6.337453365325928
Epoch 720, val loss: 0.843643307685852
Epoch 730, training loss: 6.427609920501709 = 0.09414897859096527 + 1.0 * 6.333460807800293
Epoch 730, val loss: 0.8495779633522034
Epoch 740, training loss: 6.42403507232666 = 0.08875704556703568 + 1.0 * 6.335278034210205
Epoch 740, val loss: 0.8556624054908752
Epoch 750, training loss: 6.418059825897217 = 0.08378975093364716 + 1.0 * 6.334270000457764
Epoch 750, val loss: 0.8618344664573669
Epoch 760, training loss: 6.411805152893066 = 0.07920277863740921 + 1.0 * 6.332602500915527
Epoch 760, val loss: 0.8681021332740784
Epoch 770, training loss: 6.403625011444092 = 0.07495404034852982 + 1.0 * 6.328670978546143
Epoch 770, val loss: 0.8744287490844727
Epoch 780, training loss: 6.399026870727539 = 0.07099781930446625 + 1.0 * 6.328029155731201
Epoch 780, val loss: 0.8807874917984009
Epoch 790, training loss: 6.394439697265625 = 0.06732768565416336 + 1.0 * 6.327112197875977
Epoch 790, val loss: 0.8871784210205078
Epoch 800, training loss: 6.394698619842529 = 0.06392813473939896 + 1.0 * 6.330770492553711
Epoch 800, val loss: 0.8935510516166687
Epoch 810, training loss: 6.3868632316589355 = 0.06076560169458389 + 1.0 * 6.32609748840332
Epoch 810, val loss: 0.8998371362686157
Epoch 820, training loss: 6.382522106170654 = 0.05782878026366234 + 1.0 * 6.324693202972412
Epoch 820, val loss: 0.9061342477798462
Epoch 830, training loss: 6.381174564361572 = 0.05508539080619812 + 1.0 * 6.326089382171631
Epoch 830, val loss: 0.9123760461807251
Epoch 840, training loss: 6.374638557434082 = 0.05252461135387421 + 1.0 * 6.322113990783691
Epoch 840, val loss: 0.918571412563324
Epoch 850, training loss: 6.371093273162842 = 0.05012776330113411 + 1.0 * 6.32096529006958
Epoch 850, val loss: 0.9246832728385925
Epoch 860, training loss: 6.370617866516113 = 0.04788317158818245 + 1.0 * 6.322734832763672
Epoch 860, val loss: 0.930785059928894
Epoch 870, training loss: 6.374375343322754 = 0.04578462615609169 + 1.0 * 6.3285908699035645
Epoch 870, val loss: 0.9368108510971069
Epoch 880, training loss: 6.364563941955566 = 0.0438162237405777 + 1.0 * 6.3207478523254395
Epoch 880, val loss: 0.9427606463432312
Epoch 890, training loss: 6.358598709106445 = 0.04197271540760994 + 1.0 * 6.316626071929932
Epoch 890, val loss: 0.9486624002456665
Epoch 900, training loss: 6.366617679595947 = 0.04023106023669243 + 1.0 * 6.326386451721191
Epoch 900, val loss: 0.954508364200592
Epoch 910, training loss: 6.356509685516357 = 0.03860324248671532 + 1.0 * 6.317906379699707
Epoch 910, val loss: 0.9602652192115784
Epoch 920, training loss: 6.351140022277832 = 0.03706371784210205 + 1.0 * 6.3140764236450195
Epoch 920, val loss: 0.9659508466720581
Epoch 930, training loss: 6.352575302124023 = 0.03561253473162651 + 1.0 * 6.316962718963623
Epoch 930, val loss: 0.9715699553489685
Epoch 940, training loss: 6.347473621368408 = 0.034246690571308136 + 1.0 * 6.313226699829102
Epoch 940, val loss: 0.9771265387535095
Epoch 950, training loss: 6.356260776519775 = 0.03295104578137398 + 1.0 * 6.323309898376465
Epoch 950, val loss: 0.9826071262359619
Epoch 960, training loss: 6.344711780548096 = 0.03173446282744408 + 1.0 * 6.312977313995361
Epoch 960, val loss: 0.9879345893859863
Epoch 970, training loss: 6.339645862579346 = 0.030579213052988052 + 1.0 * 6.3090667724609375
Epoch 970, val loss: 0.9932883977890015
Epoch 980, training loss: 6.337756633758545 = 0.02948186732828617 + 1.0 * 6.308274745941162
Epoch 980, val loss: 0.9985895156860352
Epoch 990, training loss: 6.341203689575195 = 0.02844270132482052 + 1.0 * 6.312760829925537
Epoch 990, val loss: 1.0037506818771362
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 10.56475830078125 = 1.9679045677185059 + 1.0 * 8.596854209899902
Epoch 0, val loss: 1.9584814310073853
Epoch 10, training loss: 10.552974700927734 = 1.9563374519348145 + 1.0 * 8.596637725830078
Epoch 10, val loss: 1.9474482536315918
Epoch 20, training loss: 10.53708267211914 = 1.9424775838851929 + 1.0 * 8.594605445861816
Epoch 20, val loss: 1.9337031841278076
Epoch 30, training loss: 10.501360893249512 = 1.9235808849334717 + 1.0 * 8.577779769897461
Epoch 30, val loss: 1.9144723415374756
Epoch 40, training loss: 10.384886741638184 = 1.898401141166687 + 1.0 * 8.486485481262207
Epoch 40, val loss: 1.889504075050354
Epoch 50, training loss: 10.015247344970703 = 1.870013952255249 + 1.0 * 8.145233154296875
Epoch 50, val loss: 1.8624870777130127
Epoch 60, training loss: 9.739725112915039 = 1.8460450172424316 + 1.0 * 7.893680095672607
Epoch 60, val loss: 1.8415429592132568
Epoch 70, training loss: 9.264719009399414 = 1.827191710472107 + 1.0 * 7.437527179718018
Epoch 70, val loss: 1.8259888887405396
Epoch 80, training loss: 8.915735244750977 = 1.8107144832611084 + 1.0 * 7.105020523071289
Epoch 80, val loss: 1.8123122453689575
Epoch 90, training loss: 8.692578315734863 = 1.7937570810317993 + 1.0 * 6.898820877075195
Epoch 90, val loss: 1.7984129190444946
Epoch 100, training loss: 8.581018447875977 = 1.774509072303772 + 1.0 * 6.806509017944336
Epoch 100, val loss: 1.783060073852539
Epoch 110, training loss: 8.4969482421875 = 1.7549798488616943 + 1.0 * 6.741968154907227
Epoch 110, val loss: 1.7675412893295288
Epoch 120, training loss: 8.432930946350098 = 1.7355979681015015 + 1.0 * 6.697333335876465
Epoch 120, val loss: 1.7517616748809814
Epoch 130, training loss: 8.377038955688477 = 1.7151503562927246 + 1.0 * 6.661888599395752
Epoch 130, val loss: 1.734832763671875
Epoch 140, training loss: 8.322019577026367 = 1.69234037399292 + 1.0 * 6.629679203033447
Epoch 140, val loss: 1.7160708904266357
Epoch 150, training loss: 8.26873779296875 = 1.66591477394104 + 1.0 * 6.602823257446289
Epoch 150, val loss: 1.6946263313293457
Epoch 160, training loss: 8.213780403137207 = 1.6351585388183594 + 1.0 * 6.578621864318848
Epoch 160, val loss: 1.6699864864349365
Epoch 170, training loss: 8.157777786254883 = 1.5997058153152466 + 1.0 * 6.558072090148926
Epoch 170, val loss: 1.641709566116333
Epoch 180, training loss: 8.101123809814453 = 1.5601215362548828 + 1.0 * 6.541001796722412
Epoch 180, val loss: 1.6101799011230469
Epoch 190, training loss: 8.041529655456543 = 1.5164495706558228 + 1.0 * 6.525079727172852
Epoch 190, val loss: 1.5751696825027466
Epoch 200, training loss: 7.98388671875 = 1.4699771404266357 + 1.0 * 6.513909816741943
Epoch 200, val loss: 1.5379624366760254
Epoch 210, training loss: 7.922025203704834 = 1.4221516847610474 + 1.0 * 6.499873638153076
Epoch 210, val loss: 1.4998195171356201
Epoch 220, training loss: 7.862066745758057 = 1.373475432395935 + 1.0 * 6.488591194152832
Epoch 220, val loss: 1.461099624633789
Epoch 230, training loss: 7.807758808135986 = 1.3251519203186035 + 1.0 * 6.482606887817383
Epoch 230, val loss: 1.4232755899429321
Epoch 240, training loss: 7.751628875732422 = 1.2795194387435913 + 1.0 * 6.472109317779541
Epoch 240, val loss: 1.3880490064620972
Epoch 250, training loss: 7.69685697555542 = 1.2353858947753906 + 1.0 * 6.461471080780029
Epoch 250, val loss: 1.3544235229492188
Epoch 260, training loss: 7.644133567810059 = 1.1922311782836914 + 1.0 * 6.451902389526367
Epoch 260, val loss: 1.3217560052871704
Epoch 270, training loss: 7.593432426452637 = 1.1498630046844482 + 1.0 * 6.443569183349609
Epoch 270, val loss: 1.2900744676589966
Epoch 280, training loss: 7.551283836364746 = 1.1083943843841553 + 1.0 * 6.44288969039917
Epoch 280, val loss: 1.2594013214111328
Epoch 290, training loss: 7.501900672912598 = 1.0686533451080322 + 1.0 * 6.4332475662231445
Epoch 290, val loss: 1.2299460172653198
Epoch 300, training loss: 7.454649925231934 = 1.029670000076294 + 1.0 * 6.4249796867370605
Epoch 300, val loss: 1.2012755870819092
Epoch 310, training loss: 7.409460067749023 = 0.9911466240882874 + 1.0 * 6.418313503265381
Epoch 310, val loss: 1.1729612350463867
Epoch 320, training loss: 7.375552177429199 = 0.9530701637268066 + 1.0 * 6.422482013702393
Epoch 320, val loss: 1.1450169086456299
Epoch 330, training loss: 7.326664924621582 = 0.9158177375793457 + 1.0 * 6.410847187042236
Epoch 330, val loss: 1.1177836656570435
Epoch 340, training loss: 7.293771266937256 = 0.8791333436965942 + 1.0 * 6.414638042449951
Epoch 340, val loss: 1.0911160707473755
Epoch 350, training loss: 7.246056079864502 = 0.8432459235191345 + 1.0 * 6.402810096740723
Epoch 350, val loss: 1.064992070198059
Epoch 360, training loss: 7.205772876739502 = 0.8079765439033508 + 1.0 * 6.397796154022217
Epoch 360, val loss: 1.0396292209625244
Epoch 370, training loss: 7.166835784912109 = 0.7731326818466187 + 1.0 * 6.393702983856201
Epoch 370, val loss: 1.0148730278015137
Epoch 380, training loss: 7.128620147705078 = 0.7387349009513855 + 1.0 * 6.389885425567627
Epoch 380, val loss: 0.9907491207122803
Epoch 390, training loss: 7.098866939544678 = 0.704927384853363 + 1.0 * 6.39393949508667
Epoch 390, val loss: 0.9674233794212341
Epoch 400, training loss: 7.057843208312988 = 0.6722850799560547 + 1.0 * 6.385558128356934
Epoch 400, val loss: 0.9452986717224121
Epoch 410, training loss: 7.023950576782227 = 0.6409001350402832 + 1.0 * 6.383050441741943
Epoch 410, val loss: 0.9247370362281799
Epoch 420, training loss: 6.993094444274902 = 0.6107122898101807 + 1.0 * 6.382382392883301
Epoch 420, val loss: 0.9055055379867554
Epoch 430, training loss: 6.95887565612793 = 0.5818172097206116 + 1.0 * 6.377058506011963
Epoch 430, val loss: 0.8877661824226379
Epoch 440, training loss: 6.931643486022949 = 0.55418860912323 + 1.0 * 6.37745475769043
Epoch 440, val loss: 0.8714977502822876
Epoch 450, training loss: 6.9016289710998535 = 0.5279203653335571 + 1.0 * 6.373708724975586
Epoch 450, val loss: 0.856536328792572
Epoch 460, training loss: 6.876556396484375 = 0.5028650164604187 + 1.0 * 6.373691558837891
Epoch 460, val loss: 0.8429449796676636
Epoch 470, training loss: 6.848178863525391 = 0.47894051671028137 + 1.0 * 6.369238376617432
Epoch 470, val loss: 0.830414354801178
Epoch 480, training loss: 6.822097301483154 = 0.4558381140232086 + 1.0 * 6.3662590980529785
Epoch 480, val loss: 0.8187622427940369
Epoch 490, training loss: 6.802471160888672 = 0.4334261119365692 + 1.0 * 6.369045257568359
Epoch 490, val loss: 0.8077184557914734
Epoch 500, training loss: 6.7728190422058105 = 0.4117230474948883 + 1.0 * 6.361095905303955
Epoch 500, val loss: 0.7972837090492249
Epoch 510, training loss: 6.750438690185547 = 0.39060086011886597 + 1.0 * 6.359838008880615
Epoch 510, val loss: 0.787457287311554
Epoch 520, training loss: 6.729156970977783 = 0.36981984972953796 + 1.0 * 6.359337329864502
Epoch 520, val loss: 0.7781996726989746
Epoch 530, training loss: 6.706154823303223 = 0.34941837191581726 + 1.0 * 6.356736660003662
Epoch 530, val loss: 0.7693541049957275
Epoch 540, training loss: 6.683541297912598 = 0.32950934767723083 + 1.0 * 6.354032039642334
Epoch 540, val loss: 0.7610469460487366
Epoch 550, training loss: 6.6634039878845215 = 0.3100373148918152 + 1.0 * 6.353366851806641
Epoch 550, val loss: 0.7533502578735352
Epoch 560, training loss: 6.647301197052002 = 0.29110726714134216 + 1.0 * 6.356194019317627
Epoch 560, val loss: 0.7463082075119019
Epoch 570, training loss: 6.623698711395264 = 0.2729281783103943 + 1.0 * 6.350770473480225
Epoch 570, val loss: 0.7400704026222229
Epoch 580, training loss: 6.6021647453308105 = 0.25544947385787964 + 1.0 * 6.346715450286865
Epoch 580, val loss: 0.7346364259719849
Epoch 590, training loss: 6.58641242980957 = 0.2387029379606247 + 1.0 * 6.347709655761719
Epoch 590, val loss: 0.7299957871437073
Epoch 600, training loss: 6.568694591522217 = 0.22284327447414398 + 1.0 * 6.345851421356201
Epoch 600, val loss: 0.7260987758636475
Epoch 610, training loss: 6.553391933441162 = 0.2079048901796341 + 1.0 * 6.345487117767334
Epoch 610, val loss: 0.7231282591819763
Epoch 620, training loss: 6.5341315269470215 = 0.19385801255702972 + 1.0 * 6.340273380279541
Epoch 620, val loss: 0.7209634184837341
Epoch 630, training loss: 6.5277276039123535 = 0.18067604303359985 + 1.0 * 6.347051620483398
Epoch 630, val loss: 0.7196235656738281
Epoch 640, training loss: 6.507925510406494 = 0.1684289425611496 + 1.0 * 6.339496612548828
Epoch 640, val loss: 0.7190989851951599
Epoch 650, training loss: 6.49432373046875 = 0.15705081820487976 + 1.0 * 6.337273120880127
Epoch 650, val loss: 0.7192952632904053
Epoch 660, training loss: 6.488602638244629 = 0.1465054303407669 + 1.0 * 6.342097282409668
Epoch 660, val loss: 0.7203025817871094
Epoch 670, training loss: 6.473352909088135 = 0.13681910932064056 + 1.0 * 6.336534023284912
Epoch 670, val loss: 0.7218685746192932
Epoch 680, training loss: 6.461112022399902 = 0.12786200642585754 + 1.0 * 6.333250045776367
Epoch 680, val loss: 0.7241753935813904
Epoch 690, training loss: 6.456494331359863 = 0.11962950974702835 + 1.0 * 6.336864948272705
Epoch 690, val loss: 0.7269523739814758
Epoch 700, training loss: 6.448395252227783 = 0.11208156496286392 + 1.0 * 6.336313724517822
Epoch 700, val loss: 0.730254590511322
Epoch 710, training loss: 6.433992862701416 = 0.10516692698001862 + 1.0 * 6.328825950622559
Epoch 710, val loss: 0.7338721752166748
Epoch 720, training loss: 6.428268909454346 = 0.09878259897232056 + 1.0 * 6.32948637008667
Epoch 720, val loss: 0.7379584908485413
Epoch 730, training loss: 6.419170379638672 = 0.0928945243358612 + 1.0 * 6.326275825500488
Epoch 730, val loss: 0.7424145936965942
Epoch 740, training loss: 6.417111396789551 = 0.08746060729026794 + 1.0 * 6.32965087890625
Epoch 740, val loss: 0.7471100687980652
Epoch 750, training loss: 6.40816068649292 = 0.08244828879833221 + 1.0 * 6.325712203979492
Epoch 750, val loss: 0.7520372271537781
Epoch 760, training loss: 6.400845527648926 = 0.07781688123941422 + 1.0 * 6.323028564453125
Epoch 760, val loss: 0.7571768164634705
Epoch 770, training loss: 6.395997524261475 = 0.07352881133556366 + 1.0 * 6.3224687576293945
Epoch 770, val loss: 0.762469470500946
Epoch 780, training loss: 6.39799165725708 = 0.06956170499324799 + 1.0 * 6.32843017578125
Epoch 780, val loss: 0.7679231762886047
Epoch 790, training loss: 6.385624408721924 = 0.0659194365143776 + 1.0 * 6.319705009460449
Epoch 790, val loss: 0.7732966542243958
Epoch 800, training loss: 6.380646228790283 = 0.06253660470247269 + 1.0 * 6.318109512329102
Epoch 800, val loss: 0.7787861824035645
Epoch 810, training loss: 6.375763893127441 = 0.05937967076897621 + 1.0 * 6.316384315490723
Epoch 810, val loss: 0.7844749093055725
Epoch 820, training loss: 6.384604454040527 = 0.05643801763653755 + 1.0 * 6.328166484832764
Epoch 820, val loss: 0.790128767490387
Epoch 830, training loss: 6.371757984161377 = 0.053704727441072464 + 1.0 * 6.318053245544434
Epoch 830, val loss: 0.7957801818847656
Epoch 840, training loss: 6.366527080535889 = 0.05116603150963783 + 1.0 * 6.315361022949219
Epoch 840, val loss: 0.8014343976974487
Epoch 850, training loss: 6.36126184463501 = 0.04879559203982353 + 1.0 * 6.312466144561768
Epoch 850, val loss: 0.8071394562721252
Epoch 860, training loss: 6.361629962921143 = 0.0465698316693306 + 1.0 * 6.315060138702393
Epoch 860, val loss: 0.8128839135169983
Epoch 870, training loss: 6.355947494506836 = 0.04448983818292618 + 1.0 * 6.311457633972168
Epoch 870, val loss: 0.8185715675354004
Epoch 880, training loss: 6.352059841156006 = 0.04254442825913429 + 1.0 * 6.309515476226807
Epoch 880, val loss: 0.8241387009620667
Epoch 890, training loss: 6.3482489585876465 = 0.040714286267757416 + 1.0 * 6.307534694671631
Epoch 890, val loss: 0.8298066854476929
Epoch 900, training loss: 6.3526787757873535 = 0.038990844041109085 + 1.0 * 6.313687801361084
Epoch 900, val loss: 0.8354718685150146
Epoch 910, training loss: 6.350406169891357 = 0.03737688437104225 + 1.0 * 6.3130292892456055
Epoch 910, val loss: 0.8410153388977051
Epoch 920, training loss: 6.341816425323486 = 0.03586364537477493 + 1.0 * 6.305952548980713
Epoch 920, val loss: 0.8464414477348328
Epoch 930, training loss: 6.352876663208008 = 0.03444264456629753 + 1.0 * 6.318434238433838
Epoch 930, val loss: 0.8518737554550171
Epoch 940, training loss: 6.339362144470215 = 0.03310293331742287 + 1.0 * 6.3062591552734375
Epoch 940, val loss: 0.8573106527328491
Epoch 950, training loss: 6.3349151611328125 = 0.03184124454855919 + 1.0 * 6.303073883056641
Epoch 950, val loss: 0.8625504374504089
Epoch 960, training loss: 6.332237243652344 = 0.030641185119748116 + 1.0 * 6.301596164703369
Epoch 960, val loss: 0.8679106831550598
Epoch 970, training loss: 6.333855628967285 = 0.02950263023376465 + 1.0 * 6.304352760314941
Epoch 970, val loss: 0.873213529586792
Epoch 980, training loss: 6.333580493927002 = 0.02843126468360424 + 1.0 * 6.305149078369141
Epoch 980, val loss: 0.8784821033477783
Epoch 990, training loss: 6.327617168426514 = 0.027421217411756516 + 1.0 * 6.300196170806885
Epoch 990, val loss: 0.8835293650627136
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8397469688982605
The final CL Acc:0.80123, 0.02145, The final GNN Acc:0.83869, 0.00086
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9458])
updated graph: torch.Size([2, 10534])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.556455612182617 = 1.959610939025879 + 1.0 * 8.596844673156738
Epoch 0, val loss: 1.9617881774902344
Epoch 10, training loss: 10.545924186706543 = 1.9492918252944946 + 1.0 * 8.59663200378418
Epoch 10, val loss: 1.9511083364486694
Epoch 20, training loss: 10.531270980834961 = 1.9364056587219238 + 1.0 * 8.594864845275879
Epoch 20, val loss: 1.9373146295547485
Epoch 30, training loss: 10.498649597167969 = 1.918186068534851 + 1.0 * 8.580463409423828
Epoch 30, val loss: 1.917357087135315
Epoch 40, training loss: 10.386602401733398 = 1.8927979469299316 + 1.0 * 8.493803977966309
Epoch 40, val loss: 1.8906033039093018
Epoch 50, training loss: 9.983137130737305 = 1.864851474761963 + 1.0 * 8.118285179138184
Epoch 50, val loss: 1.863605260848999
Epoch 60, training loss: 9.563065528869629 = 1.8425098657608032 + 1.0 * 7.720555782318115
Epoch 60, val loss: 1.842948317527771
Epoch 70, training loss: 9.1707763671875 = 1.8264213800430298 + 1.0 * 7.34435510635376
Epoch 70, val loss: 1.828051209449768
Epoch 80, training loss: 8.869148254394531 = 1.8123068809509277 + 1.0 * 7.056841850280762
Epoch 80, val loss: 1.8150036334991455
Epoch 90, training loss: 8.695138931274414 = 1.7995295524597168 + 1.0 * 6.8956098556518555
Epoch 90, val loss: 1.8035897016525269
Epoch 100, training loss: 8.602960586547852 = 1.7852977514266968 + 1.0 * 6.817663192749023
Epoch 100, val loss: 1.7898856401443481
Epoch 110, training loss: 8.521004676818848 = 1.7710826396942139 + 1.0 * 6.749921798706055
Epoch 110, val loss: 1.7761547565460205
Epoch 120, training loss: 8.450227737426758 = 1.7577472925186157 + 1.0 * 6.692480564117432
Epoch 120, val loss: 1.7636455297470093
Epoch 130, training loss: 8.392133712768555 = 1.743767499923706 + 1.0 * 6.6483659744262695
Epoch 130, val loss: 1.7506542205810547
Epoch 140, training loss: 8.340802192687988 = 1.7274247407913208 + 1.0 * 6.613377571105957
Epoch 140, val loss: 1.7360607385635376
Epoch 150, training loss: 8.294794082641602 = 1.7085115909576416 + 1.0 * 6.586282253265381
Epoch 150, val loss: 1.7193289995193481
Epoch 160, training loss: 8.250029563903809 = 1.6865023374557495 + 1.0 * 6.5635271072387695
Epoch 160, val loss: 1.7000011205673218
Epoch 170, training loss: 8.20539379119873 = 1.6610174179077148 + 1.0 * 6.544376373291016
Epoch 170, val loss: 1.6775339841842651
Epoch 180, training loss: 8.162140846252441 = 1.6316323280334473 + 1.0 * 6.530508518218994
Epoch 180, val loss: 1.6518330574035645
Epoch 190, training loss: 8.112839698791504 = 1.5980660915374756 + 1.0 * 6.514773845672607
Epoch 190, val loss: 1.6223174333572388
Epoch 200, training loss: 8.061549186706543 = 1.5595719814300537 + 1.0 * 6.50197696685791
Epoch 200, val loss: 1.5886491537094116
Epoch 210, training loss: 8.009672164916992 = 1.515882134437561 + 1.0 * 6.493790149688721
Epoch 210, val loss: 1.550724744796753
Epoch 220, training loss: 7.951015949249268 = 1.4681395292282104 + 1.0 * 6.482876300811768
Epoch 220, val loss: 1.5097198486328125
Epoch 230, training loss: 7.891386032104492 = 1.417046308517456 + 1.0 * 6.474339485168457
Epoch 230, val loss: 1.4664111137390137
Epoch 240, training loss: 7.833486557006836 = 1.3633230924606323 + 1.0 * 6.470163345336914
Epoch 240, val loss: 1.421506643295288
Epoch 250, training loss: 7.76899528503418 = 1.3089497089385986 + 1.0 * 6.46004581451416
Epoch 250, val loss: 1.3765491247177124
Epoch 260, training loss: 7.707676887512207 = 1.2548996210098267 + 1.0 * 6.45277738571167
Epoch 260, val loss: 1.3325854539871216
Epoch 270, training loss: 7.655660152435303 = 1.2019776105880737 + 1.0 * 6.4536824226379395
Epoch 270, val loss: 1.2902536392211914
Epoch 280, training loss: 7.593384742736816 = 1.1519334316253662 + 1.0 * 6.441451549530029
Epoch 280, val loss: 1.2510902881622314
Epoch 290, training loss: 7.539839744567871 = 1.104689359664917 + 1.0 * 6.435150146484375
Epoch 290, val loss: 1.2147629261016846
Epoch 300, training loss: 7.4961066246032715 = 1.0602389574050903 + 1.0 * 6.435867786407471
Epoch 300, val loss: 1.181256651878357
Epoch 310, training loss: 7.444225788116455 = 1.0190776586532593 + 1.0 * 6.425148010253906
Epoch 310, val loss: 1.151021957397461
Epoch 320, training loss: 7.401315689086914 = 0.9808474779129028 + 1.0 * 6.420468330383301
Epoch 320, val loss: 1.123544692993164
Epoch 330, training loss: 7.3626203536987305 = 0.9451526403427124 + 1.0 * 6.4174675941467285
Epoch 330, val loss: 1.0984289646148682
Epoch 340, training loss: 7.323460578918457 = 0.9117714762687683 + 1.0 * 6.411689281463623
Epoch 340, val loss: 1.0756655931472778
Epoch 350, training loss: 7.287047863006592 = 0.8803075551986694 + 1.0 * 6.406740188598633
Epoch 350, val loss: 1.054739236831665
Epoch 360, training loss: 7.256182670593262 = 0.8504638671875 + 1.0 * 6.405718803405762
Epoch 360, val loss: 1.0354403257369995
Epoch 370, training loss: 7.222384929656982 = 0.8219528198242188 + 1.0 * 6.400432109832764
Epoch 370, val loss: 1.0175734758377075
Epoch 380, training loss: 7.190638542175293 = 0.794399619102478 + 1.0 * 6.396238803863525
Epoch 380, val loss: 1.0008240938186646
Epoch 390, training loss: 7.168962001800537 = 0.767803966999054 + 1.0 * 6.401157855987549
Epoch 390, val loss: 0.9851483106613159
Epoch 400, training loss: 7.1320624351501465 = 0.7422612309455872 + 1.0 * 6.389801025390625
Epoch 400, val loss: 0.970993161201477
Epoch 410, training loss: 7.1023125648498535 = 0.7171605825424194 + 1.0 * 6.3851518630981445
Epoch 410, val loss: 0.9574829339981079
Epoch 420, training loss: 7.075748920440674 = 0.6923192143440247 + 1.0 * 6.383429527282715
Epoch 420, val loss: 0.9449166655540466
Epoch 430, training loss: 7.056197166442871 = 0.6680824756622314 + 1.0 * 6.3881144523620605
Epoch 430, val loss: 0.9333708882331848
Epoch 440, training loss: 7.024531364440918 = 0.64463871717453 + 1.0 * 6.379892826080322
Epoch 440, val loss: 0.923210859298706
Epoch 450, training loss: 6.997100830078125 = 0.6217054724693298 + 1.0 * 6.37539529800415
Epoch 450, val loss: 0.913890540599823
Epoch 460, training loss: 6.974535942077637 = 0.5992173552513123 + 1.0 * 6.37531852722168
Epoch 460, val loss: 0.9058036804199219
Epoch 470, training loss: 6.956648826599121 = 0.5773424506187439 + 1.0 * 6.379306316375732
Epoch 470, val loss: 0.8989936709403992
Epoch 480, training loss: 6.923315048217773 = 0.5561665892601013 + 1.0 * 6.367148399353027
Epoch 480, val loss: 0.8933103084564209
Epoch 490, training loss: 6.901516914367676 = 0.5354747772216797 + 1.0 * 6.366042137145996
Epoch 490, val loss: 0.8888710141181946
Epoch 500, training loss: 6.889316082000732 = 0.5151443481445312 + 1.0 * 6.374171733856201
Epoch 500, val loss: 0.8854463696479797
Epoch 510, training loss: 6.858068943023682 = 0.49539652466773987 + 1.0 * 6.362672328948975
Epoch 510, val loss: 0.8828366994857788
Epoch 520, training loss: 6.837055206298828 = 0.4759771525859833 + 1.0 * 6.361078262329102
Epoch 520, val loss: 0.8811252117156982
Epoch 530, training loss: 6.816325664520264 = 0.4567601978778839 + 1.0 * 6.359565258026123
Epoch 530, val loss: 0.8801244497299194
Epoch 540, training loss: 6.795586109161377 = 0.4377952218055725 + 1.0 * 6.357790946960449
Epoch 540, val loss: 0.8796429634094238
Epoch 550, training loss: 6.774836540222168 = 0.4191143214702606 + 1.0 * 6.355722427368164
Epoch 550, val loss: 0.8798509240150452
Epoch 560, training loss: 6.7771172523498535 = 0.4007008969783783 + 1.0 * 6.376416206359863
Epoch 560, val loss: 0.880357027053833
Epoch 570, training loss: 6.740736961364746 = 0.3827480673789978 + 1.0 * 6.3579888343811035
Epoch 570, val loss: 0.881438672542572
Epoch 580, training loss: 6.7163848876953125 = 0.36514997482299805 + 1.0 * 6.3512349128723145
Epoch 580, val loss: 0.8829172253608704
Epoch 590, training loss: 6.696463108062744 = 0.3478790819644928 + 1.0 * 6.348584175109863
Epoch 590, val loss: 0.8848239183425903
Epoch 600, training loss: 6.691198825836182 = 0.33101993799209595 + 1.0 * 6.3601789474487305
Epoch 600, val loss: 0.8874638676643372
Epoch 610, training loss: 6.661388397216797 = 0.3147701323032379 + 1.0 * 6.346618175506592
Epoch 610, val loss: 0.8901755809783936
Epoch 620, training loss: 6.643496036529541 = 0.2991078495979309 + 1.0 * 6.344388008117676
Epoch 620, val loss: 0.8936491012573242
Epoch 630, training loss: 6.627856731414795 = 0.2839784324169159 + 1.0 * 6.343878269195557
Epoch 630, val loss: 0.8977208733558655
Epoch 640, training loss: 6.614743232727051 = 0.2695251703262329 + 1.0 * 6.345218181610107
Epoch 640, val loss: 0.9022372364997864
Epoch 650, training loss: 6.596686363220215 = 0.255836546421051 + 1.0 * 6.340849876403809
Epoch 650, val loss: 0.9073748588562012
Epoch 660, training loss: 6.581527233123779 = 0.2427472472190857 + 1.0 * 6.338779926300049
Epoch 660, val loss: 0.9129847884178162
Epoch 670, training loss: 6.574777603149414 = 0.2302587479352951 + 1.0 * 6.344518661499023
Epoch 670, val loss: 0.9190515875816345
Epoch 680, training loss: 6.561633110046387 = 0.21843838691711426 + 1.0 * 6.343194484710693
Epoch 680, val loss: 0.925659716129303
Epoch 690, training loss: 6.543668746948242 = 0.20728394389152527 + 1.0 * 6.3363847732543945
Epoch 690, val loss: 0.9327287673950195
Epoch 700, training loss: 6.53151798248291 = 0.19669459760189056 + 1.0 * 6.3348236083984375
Epoch 700, val loss: 0.9402361512184143
Epoch 710, training loss: 6.526730060577393 = 0.18666695058345795 + 1.0 * 6.340063095092773
Epoch 710, val loss: 0.9481964111328125
Epoch 720, training loss: 6.515195369720459 = 0.17721126973628998 + 1.0 * 6.337984085083008
Epoch 720, val loss: 0.9565408229827881
Epoch 730, training loss: 6.501988887786865 = 0.1682465523481369 + 1.0 * 6.333742141723633
Epoch 730, val loss: 0.965151309967041
Epoch 740, training loss: 6.492615222930908 = 0.1597674936056137 + 1.0 * 6.332847595214844
Epoch 740, val loss: 0.9740668535232544
Epoch 750, training loss: 6.479842662811279 = 0.1517382264137268 + 1.0 * 6.328104496002197
Epoch 750, val loss: 0.9833672642707825
Epoch 760, training loss: 6.473265171051025 = 0.1441327929496765 + 1.0 * 6.329132556915283
Epoch 760, val loss: 0.9928702116012573
Epoch 770, training loss: 6.467763423919678 = 0.13695552945137024 + 1.0 * 6.330807685852051
Epoch 770, val loss: 1.0021727085113525
Epoch 780, training loss: 6.459051132202148 = 0.13022370636463165 + 1.0 * 6.328827381134033
Epoch 780, val loss: 1.0120245218276978
Epoch 790, training loss: 6.447375774383545 = 0.12387166917324066 + 1.0 * 6.3235039710998535
Epoch 790, val loss: 1.0217605829238892
Epoch 800, training loss: 6.443344593048096 = 0.11786238849163055 + 1.0 * 6.325482368469238
Epoch 800, val loss: 1.0315197706222534
Epoch 810, training loss: 6.4350080490112305 = 0.11221170425415039 + 1.0 * 6.32279634475708
Epoch 810, val loss: 1.0412843227386475
Epoch 820, training loss: 6.430797576904297 = 0.10690060257911682 + 1.0 * 6.323896884918213
Epoch 820, val loss: 1.051155924797058
Epoch 830, training loss: 6.42965841293335 = 0.10188107192516327 + 1.0 * 6.32777738571167
Epoch 830, val loss: 1.0610673427581787
Epoch 840, training loss: 6.41697883605957 = 0.09716018289327621 + 1.0 * 6.319818496704102
Epoch 840, val loss: 1.070837378501892
Epoch 850, training loss: 6.4101738929748535 = 0.09268943220376968 + 1.0 * 6.317484378814697
Epoch 850, val loss: 1.0807727575302124
Epoch 860, training loss: 6.406412601470947 = 0.08845583349466324 + 1.0 * 6.317956924438477
Epoch 860, val loss: 1.0906753540039062
Epoch 870, training loss: 6.406410217285156 = 0.08444929867982864 + 1.0 * 6.321960926055908
Epoch 870, val loss: 1.1005685329437256
Epoch 880, training loss: 6.4065470695495605 = 0.0806800127029419 + 1.0 * 6.325867176055908
Epoch 880, val loss: 1.110097885131836
Epoch 890, training loss: 6.391746997833252 = 0.07713548839092255 + 1.0 * 6.314611434936523
Epoch 890, val loss: 1.119831919670105
Epoch 900, training loss: 6.387966632843018 = 0.0737789049744606 + 1.0 * 6.314187526702881
Epoch 900, val loss: 1.1295640468597412
Epoch 910, training loss: 6.383520126342773 = 0.07059471309185028 + 1.0 * 6.312925338745117
Epoch 910, val loss: 1.139150619506836
Epoch 920, training loss: 6.386094093322754 = 0.0675746276974678 + 1.0 * 6.318519592285156
Epoch 920, val loss: 1.1486265659332275
Epoch 930, training loss: 6.378467559814453 = 0.06473007798194885 + 1.0 * 6.313737392425537
Epoch 930, val loss: 1.1580969095230103
Epoch 940, training loss: 6.373752593994141 = 0.06203605234622955 + 1.0 * 6.311716556549072
Epoch 940, val loss: 1.1675472259521484
Epoch 950, training loss: 6.377941608428955 = 0.05947772413492203 + 1.0 * 6.3184638023376465
Epoch 950, val loss: 1.1766464710235596
Epoch 960, training loss: 6.366474151611328 = 0.057064805179834366 + 1.0 * 6.309409141540527
Epoch 960, val loss: 1.1859756708145142
Epoch 970, training loss: 6.362095355987549 = 0.054754726588726044 + 1.0 * 6.307340621948242
Epoch 970, val loss: 1.195249319076538
Epoch 980, training loss: 6.362762928009033 = 0.05254499241709709 + 1.0 * 6.31021785736084
Epoch 980, val loss: 1.2044193744659424
Epoch 990, training loss: 6.357262134552002 = 0.05045110359787941 + 1.0 * 6.3068108558654785
Epoch 990, val loss: 1.2134344577789307
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 10.535524368286133 = 1.9386754035949707 + 1.0 * 8.596848487854004
Epoch 0, val loss: 1.9461538791656494
Epoch 10, training loss: 10.525410652160645 = 1.9287481307983398 + 1.0 * 8.596662521362305
Epoch 10, val loss: 1.9359804391860962
Epoch 20, training loss: 10.51170539855957 = 1.9165160655975342 + 1.0 * 8.595189094543457
Epoch 20, val loss: 1.923218846321106
Epoch 30, training loss: 10.482213020324707 = 1.8993847370147705 + 1.0 * 8.582828521728516
Epoch 30, val loss: 1.905186414718628
Epoch 40, training loss: 10.386675834655762 = 1.876295804977417 + 1.0 * 8.510379791259766
Epoch 40, val loss: 1.8815428018569946
Epoch 50, training loss: 9.98035717010498 = 1.8529644012451172 + 1.0 * 8.127392768859863
Epoch 50, val loss: 1.8589597940444946
Epoch 60, training loss: 9.525955200195312 = 1.8337913751602173 + 1.0 * 7.692163467407227
Epoch 60, val loss: 1.8413687944412231
Epoch 70, training loss: 9.19283676147461 = 1.8190674781799316 + 1.0 * 7.3737688064575195
Epoch 70, val loss: 1.8269832134246826
Epoch 80, training loss: 8.931280136108398 = 1.8040393590927124 + 1.0 * 7.1272406578063965
Epoch 80, val loss: 1.8126941919326782
Epoch 90, training loss: 8.777444839477539 = 1.7892704010009766 + 1.0 * 6.9881744384765625
Epoch 90, val loss: 1.7986823320388794
Epoch 100, training loss: 8.684305191040039 = 1.7743161916732788 + 1.0 * 6.909988880157471
Epoch 100, val loss: 1.7846535444259644
Epoch 110, training loss: 8.588590621948242 = 1.759552001953125 + 1.0 * 6.829038619995117
Epoch 110, val loss: 1.7714604139328003
Epoch 120, training loss: 8.507620811462402 = 1.7451695203781128 + 1.0 * 6.762451171875
Epoch 120, val loss: 1.758294701576233
Epoch 130, training loss: 8.435432434082031 = 1.7298351526260376 + 1.0 * 6.705597400665283
Epoch 130, val loss: 1.7444641590118408
Epoch 140, training loss: 8.372001647949219 = 1.7125838994979858 + 1.0 * 6.659418106079102
Epoch 140, val loss: 1.7296620607376099
Epoch 150, training loss: 8.314738273620605 = 1.6924480199813843 + 1.0 * 6.622290134429932
Epoch 150, val loss: 1.712702751159668
Epoch 160, training loss: 8.260347366333008 = 1.668794870376587 + 1.0 * 6.591552257537842
Epoch 160, val loss: 1.6931679248809814
Epoch 170, training loss: 8.20915412902832 = 1.641035795211792 + 1.0 * 6.568118572235107
Epoch 170, val loss: 1.6701457500457764
Epoch 180, training loss: 8.159152030944824 = 1.608726143836975 + 1.0 * 6.5504255294799805
Epoch 180, val loss: 1.6430927515029907
Epoch 190, training loss: 8.10571575164795 = 1.5714921951293945 + 1.0 * 6.534223556518555
Epoch 190, val loss: 1.6119694709777832
Epoch 200, training loss: 8.050639152526855 = 1.5292117595672607 + 1.0 * 6.521427154541016
Epoch 200, val loss: 1.5765422582626343
Epoch 210, training loss: 7.994711875915527 = 1.4825271368026733 + 1.0 * 6.5121846199035645
Epoch 210, val loss: 1.5375721454620361
Epoch 220, training loss: 7.9350175857543945 = 1.4328349828720093 + 1.0 * 6.502182483673096
Epoch 220, val loss: 1.496412992477417
Epoch 230, training loss: 7.872951507568359 = 1.380511999130249 + 1.0 * 6.4924397468566895
Epoch 230, val loss: 1.453222393989563
Epoch 240, training loss: 7.812015533447266 = 1.3267621994018555 + 1.0 * 6.48525333404541
Epoch 240, val loss: 1.4094709157943726
Epoch 250, training loss: 7.750621318817139 = 1.2731293439865112 + 1.0 * 6.477491855621338
Epoch 250, val loss: 1.3664909601211548
Epoch 260, training loss: 7.691361427307129 = 1.219855546951294 + 1.0 * 6.471505641937256
Epoch 260, val loss: 1.324446439743042
Epoch 270, training loss: 7.633626937866211 = 1.1674604415893555 + 1.0 * 6.4661664962768555
Epoch 270, val loss: 1.2842156887054443
Epoch 280, training loss: 7.5738959312438965 = 1.11622953414917 + 1.0 * 6.457666397094727
Epoch 280, val loss: 1.2453471422195435
Epoch 290, training loss: 7.518009662628174 = 1.0655080080032349 + 1.0 * 6.4525017738342285
Epoch 290, val loss: 1.2076371908187866
Epoch 300, training loss: 7.466272830963135 = 1.0156623125076294 + 1.0 * 6.450610637664795
Epoch 300, val loss: 1.1713762283325195
Epoch 310, training loss: 7.409005641937256 = 0.9672189354896545 + 1.0 * 6.441786766052246
Epoch 310, val loss: 1.1366591453552246
Epoch 320, training loss: 7.357712745666504 = 0.9196563363075256 + 1.0 * 6.438056468963623
Epoch 320, val loss: 1.103081226348877
Epoch 330, training loss: 7.310455799102783 = 0.8736068606376648 + 1.0 * 6.436849117279053
Epoch 330, val loss: 1.0712380409240723
Epoch 340, training loss: 7.259034633636475 = 0.8296015858650208 + 1.0 * 6.4294328689575195
Epoch 340, val loss: 1.0411046743392944
Epoch 350, training loss: 7.212247848510742 = 0.7873010635375977 + 1.0 * 6.4249467849731445
Epoch 350, val loss: 1.0126307010650635
Epoch 360, training loss: 7.170128345489502 = 0.7469374537467957 + 1.0 * 6.423191070556641
Epoch 360, val loss: 0.985992968082428
Epoch 370, training loss: 7.127612590789795 = 0.7090104818344116 + 1.0 * 6.418601989746094
Epoch 370, val loss: 0.961392343044281
Epoch 380, training loss: 7.086416244506836 = 0.672893762588501 + 1.0 * 6.413522243499756
Epoch 380, val loss: 0.9387759566307068
Epoch 390, training loss: 7.0582499504089355 = 0.6384611129760742 + 1.0 * 6.419788837432861
Epoch 390, val loss: 0.917877733707428
Epoch 400, training loss: 7.015070915222168 = 0.6059935092926025 + 1.0 * 6.409077167510986
Epoch 400, val loss: 0.8992677330970764
Epoch 410, training loss: 6.9783759117126465 = 0.5751887559890747 + 1.0 * 6.403187274932861
Epoch 410, val loss: 0.8825551867485046
Epoch 420, training loss: 6.946785926818848 = 0.5456051826477051 + 1.0 * 6.401180744171143
Epoch 420, val loss: 0.867389440536499
Epoch 430, training loss: 6.918917179107666 = 0.5173805952072144 + 1.0 * 6.401536464691162
Epoch 430, val loss: 0.8538503050804138
Epoch 440, training loss: 6.8843889236450195 = 0.49049386382102966 + 1.0 * 6.393895149230957
Epoch 440, val loss: 0.8419745564460754
Epoch 450, training loss: 6.8564910888671875 = 0.46477529406547546 + 1.0 * 6.391716003417969
Epoch 450, val loss: 0.8314622044563293
Epoch 460, training loss: 6.8368330001831055 = 0.4400263726711273 + 1.0 * 6.396806716918945
Epoch 460, val loss: 0.8222024440765381
Epoch 470, training loss: 6.801928997039795 = 0.41652679443359375 + 1.0 * 6.385402202606201
Epoch 470, val loss: 0.8140130639076233
Epoch 480, training loss: 6.778325080871582 = 0.3939973711967468 + 1.0 * 6.3843278884887695
Epoch 480, val loss: 0.8069314956665039
Epoch 490, training loss: 6.759714126586914 = 0.3722999691963196 + 1.0 * 6.38741397857666
Epoch 490, val loss: 0.8007975220680237
Epoch 500, training loss: 6.731590270996094 = 0.351566344499588 + 1.0 * 6.380023956298828
Epoch 500, val loss: 0.795518696308136
Epoch 510, training loss: 6.707953929901123 = 0.3316558301448822 + 1.0 * 6.376297950744629
Epoch 510, val loss: 0.7911019325256348
Epoch 520, training loss: 6.686565399169922 = 0.31250888109207153 + 1.0 * 6.374056339263916
Epoch 520, val loss: 0.7874402403831482
Epoch 530, training loss: 6.674118995666504 = 0.2941065728664398 + 1.0 * 6.380012512207031
Epoch 530, val loss: 0.7844021320343018
Epoch 540, training loss: 6.646652698516846 = 0.2765333950519562 + 1.0 * 6.370119094848633
Epoch 540, val loss: 0.7821441292762756
Epoch 550, training loss: 6.630173683166504 = 0.259701132774353 + 1.0 * 6.370472431182861
Epoch 550, val loss: 0.7805859446525574
Epoch 560, training loss: 6.609933853149414 = 0.24363844096660614 + 1.0 * 6.366295337677002
Epoch 560, val loss: 0.7796570658683777
Epoch 570, training loss: 6.600266456604004 = 0.22836938500404358 + 1.0 * 6.371897220611572
Epoch 570, val loss: 0.779569149017334
Epoch 580, training loss: 6.581518650054932 = 0.21402837336063385 + 1.0 * 6.367490291595459
Epoch 580, val loss: 0.7800453305244446
Epoch 590, training loss: 6.5600481033325195 = 0.20046697556972504 + 1.0 * 6.359580993652344
Epoch 590, val loss: 0.7813510298728943
Epoch 600, training loss: 6.546677112579346 = 0.18767575919628143 + 1.0 * 6.359001159667969
Epoch 600, val loss: 0.783329427242279
Epoch 610, training loss: 6.535708904266357 = 0.17570056021213531 + 1.0 * 6.360008239746094
Epoch 610, val loss: 0.785864531993866
Epoch 620, training loss: 6.5221452713012695 = 0.16460955142974854 + 1.0 * 6.3575358390808105
Epoch 620, val loss: 0.789057195186615
Epoch 630, training loss: 6.512192726135254 = 0.1543033868074417 + 1.0 * 6.357889175415039
Epoch 630, val loss: 0.7928166389465332
Epoch 640, training loss: 6.499335765838623 = 0.14471948146820068 + 1.0 * 6.354616165161133
Epoch 640, val loss: 0.7971369028091431
Epoch 650, training loss: 6.487286567687988 = 0.13585780560970306 + 1.0 * 6.351428985595703
Epoch 650, val loss: 0.8019344806671143
Epoch 660, training loss: 6.476168155670166 = 0.1276264786720276 + 1.0 * 6.348541736602783
Epoch 660, val loss: 0.8071519136428833
Epoch 670, training loss: 6.472381114959717 = 0.12000177055597305 + 1.0 * 6.352379322052002
Epoch 670, val loss: 0.8127182722091675
Epoch 680, training loss: 6.459144115447998 = 0.11295066773891449 + 1.0 * 6.346193313598633
Epoch 680, val loss: 0.8186960816383362
Epoch 690, training loss: 6.454906463623047 = 0.10642126947641373 + 1.0 * 6.348484992980957
Epoch 690, val loss: 0.8250625133514404
Epoch 700, training loss: 6.448061466217041 = 0.10039664059877396 + 1.0 * 6.347664833068848
Epoch 700, val loss: 0.8314364552497864
Epoch 710, training loss: 6.435269832611084 = 0.09481614828109741 + 1.0 * 6.340453624725342
Epoch 710, val loss: 0.838290274143219
Epoch 720, training loss: 6.429049968719482 = 0.08961961418390274 + 1.0 * 6.339430332183838
Epoch 720, val loss: 0.8452340960502625
Epoch 730, training loss: 6.424124717712402 = 0.08476874977350235 + 1.0 * 6.339355945587158
Epoch 730, val loss: 0.8523080348968506
Epoch 740, training loss: 6.419436931610107 = 0.08025996387004852 + 1.0 * 6.339177131652832
Epoch 740, val loss: 0.8593177795410156
Epoch 750, training loss: 6.4141621589660645 = 0.07610666006803513 + 1.0 * 6.338055610656738
Epoch 750, val loss: 0.8666662573814392
Epoch 760, training loss: 6.407162189483643 = 0.0722317025065422 + 1.0 * 6.334930419921875
Epoch 760, val loss: 0.8740238547325134
Epoch 770, training loss: 6.4016852378845215 = 0.06860904395580292 + 1.0 * 6.333076000213623
Epoch 770, val loss: 0.88145512342453
Epoch 780, training loss: 6.404271125793457 = 0.06521937251091003 + 1.0 * 6.339051723480225
Epoch 780, val loss: 0.8888821601867676
Epoch 790, training loss: 6.399653911590576 = 0.062066372483968735 + 1.0 * 6.337587356567383
Epoch 790, val loss: 0.8962888717651367
Epoch 800, training loss: 6.391002178192139 = 0.05912297964096069 + 1.0 * 6.331879138946533
Epoch 800, val loss: 0.9037219882011414
Epoch 810, training loss: 6.385337829589844 = 0.05637386068701744 + 1.0 * 6.328963756561279
Epoch 810, val loss: 0.9111711382865906
Epoch 820, training loss: 6.383159160614014 = 0.05378655716776848 + 1.0 * 6.329372406005859
Epoch 820, val loss: 0.9185534715652466
Epoch 830, training loss: 6.378434181213379 = 0.05136838182806969 + 1.0 * 6.327065944671631
Epoch 830, val loss: 0.925862193107605
Epoch 840, training loss: 6.376420497894287 = 0.049099165946245193 + 1.0 * 6.327321529388428
Epoch 840, val loss: 0.9332793354988098
Epoch 850, training loss: 6.375378608703613 = 0.046973809599876404 + 1.0 * 6.328404903411865
Epoch 850, val loss: 0.940566897392273
Epoch 860, training loss: 6.367369651794434 = 0.044978637248277664 + 1.0 * 6.322391033172607
Epoch 860, val loss: 0.9477983713150024
Epoch 870, training loss: 6.368715763092041 = 0.04309865087270737 + 1.0 * 6.32561731338501
Epoch 870, val loss: 0.9549989104270935
Epoch 880, training loss: 6.36850643157959 = 0.04132433980703354 + 1.0 * 6.327182292938232
Epoch 880, val loss: 0.9620063900947571
Epoch 890, training loss: 6.362941741943359 = 0.03967507183551788 + 1.0 * 6.323266506195068
Epoch 890, val loss: 0.9690728187561035
Epoch 900, training loss: 6.3575568199157715 = 0.0381121002137661 + 1.0 * 6.31944465637207
Epoch 900, val loss: 0.9761276841163635
Epoch 910, training loss: 6.36017370223999 = 0.03663569316267967 + 1.0 * 6.323537826538086
Epoch 910, val loss: 0.9829685688018799
Epoch 920, training loss: 6.352085113525391 = 0.03523758426308632 + 1.0 * 6.316847324371338
Epoch 920, val loss: 0.9897091388702393
Epoch 930, training loss: 6.35051155090332 = 0.03391784429550171 + 1.0 * 6.316593647003174
Epoch 930, val loss: 0.9965216517448425
Epoch 940, training loss: 6.355238914489746 = 0.032671935856342316 + 1.0 * 6.322566986083984
Epoch 940, val loss: 1.0031359195709229
Epoch 950, training loss: 6.347461700439453 = 0.031491052359342575 + 1.0 * 6.315970420837402
Epoch 950, val loss: 1.0097558498382568
Epoch 960, training loss: 6.349234580993652 = 0.030376678332686424 + 1.0 * 6.318857669830322
Epoch 960, val loss: 1.0163198709487915
Epoch 970, training loss: 6.3423285484313965 = 0.029317587614059448 + 1.0 * 6.313011169433594
Epoch 970, val loss: 1.0226612091064453
Epoch 980, training loss: 6.341432094573975 = 0.028315283358097076 + 1.0 * 6.313117027282715
Epoch 980, val loss: 1.0291047096252441
Epoch 990, training loss: 6.341551780700684 = 0.027362782508134842 + 1.0 * 6.3141889572143555
Epoch 990, val loss: 1.0352907180786133
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 10.538219451904297 = 1.9413647651672363 + 1.0 * 8.596854209899902
Epoch 0, val loss: 1.9419550895690918
Epoch 10, training loss: 10.527511596679688 = 1.9308685064315796 + 1.0 * 8.596643447875977
Epoch 10, val loss: 1.930811882019043
Epoch 20, training loss: 10.512856483459473 = 1.9177719354629517 + 1.0 * 8.595084190368652
Epoch 20, val loss: 1.9167256355285645
Epoch 30, training loss: 10.481383323669434 = 1.8996951580047607 + 1.0 * 8.581687927246094
Epoch 30, val loss: 1.8973015546798706
Epoch 40, training loss: 10.365560531616211 = 1.8757081031799316 + 1.0 * 8.489851951599121
Epoch 40, val loss: 1.8726062774658203
Epoch 50, training loss: 9.941485404968262 = 1.851717233657837 + 1.0 * 8.089768409729004
Epoch 50, val loss: 1.8494564294815063
Epoch 60, training loss: 9.38039779663086 = 1.8355376720428467 + 1.0 * 7.544859886169434
Epoch 60, val loss: 1.8351362943649292
Epoch 70, training loss: 8.931401252746582 = 1.8233259916305542 + 1.0 * 7.108075141906738
Epoch 70, val loss: 1.8240498304367065
Epoch 80, training loss: 8.72354507446289 = 1.8117363452911377 + 1.0 * 6.911808967590332
Epoch 80, val loss: 1.8127771615982056
Epoch 90, training loss: 8.597993850708008 = 1.7989530563354492 + 1.0 * 6.799040794372559
Epoch 90, val loss: 1.800527572631836
Epoch 100, training loss: 8.51564884185791 = 1.7856390476226807 + 1.0 * 6.730010032653809
Epoch 100, val loss: 1.788353443145752
Epoch 110, training loss: 8.452075004577637 = 1.772301197052002 + 1.0 * 6.679773807525635
Epoch 110, val loss: 1.7763763666152954
Epoch 120, training loss: 8.398412704467773 = 1.7584612369537354 + 1.0 * 6.639951229095459
Epoch 120, val loss: 1.7639563083648682
Epoch 130, training loss: 8.351987838745117 = 1.743199348449707 + 1.0 * 6.60878849029541
Epoch 130, val loss: 1.7504268884658813
Epoch 140, training loss: 8.309268951416016 = 1.7258368730545044 + 1.0 * 6.583431720733643
Epoch 140, val loss: 1.7353336811065674
Epoch 150, training loss: 8.271562576293945 = 1.7058583498001099 + 1.0 * 6.565704345703125
Epoch 150, val loss: 1.7181752920150757
Epoch 160, training loss: 8.226402282714844 = 1.6828842163085938 + 1.0 * 6.54351806640625
Epoch 160, val loss: 1.698676586151123
Epoch 170, training loss: 8.182710647583008 = 1.6564613580703735 + 1.0 * 6.526248931884766
Epoch 170, val loss: 1.6763250827789307
Epoch 180, training loss: 8.138202667236328 = 1.6259502172470093 + 1.0 * 6.512252330780029
Epoch 180, val loss: 1.6504825353622437
Epoch 190, training loss: 8.089922904968262 = 1.59116792678833 + 1.0 * 6.498754978179932
Epoch 190, val loss: 1.6210095882415771
Epoch 200, training loss: 8.041666984558105 = 1.5519330501556396 + 1.0 * 6.489733695983887
Epoch 200, val loss: 1.5879545211791992
Epoch 210, training loss: 7.988619327545166 = 1.5092353820800781 + 1.0 * 6.479383945465088
Epoch 210, val loss: 1.5522069931030273
Epoch 220, training loss: 7.930727481842041 = 1.4634042978286743 + 1.0 * 6.467323303222656
Epoch 220, val loss: 1.5138682126998901
Epoch 230, training loss: 7.873945236206055 = 1.414499282836914 + 1.0 * 6.459445953369141
Epoch 230, val loss: 1.4731982946395874
Epoch 240, training loss: 7.816834449768066 = 1.3627920150756836 + 1.0 * 6.454042434692383
Epoch 240, val loss: 1.4302119016647339
Epoch 250, training loss: 7.756194114685059 = 1.310075283050537 + 1.0 * 6.4461188316345215
Epoch 250, val loss: 1.3871450424194336
Epoch 260, training loss: 7.696732044219971 = 1.2572790384292603 + 1.0 * 6.439453125
Epoch 260, val loss: 1.344260334968567
Epoch 270, training loss: 7.641237735748291 = 1.2044477462768555 + 1.0 * 6.4367899894714355
Epoch 270, val loss: 1.3017511367797852
Epoch 280, training loss: 7.581867694854736 = 1.1523351669311523 + 1.0 * 6.429532527923584
Epoch 280, val loss: 1.260359287261963
Epoch 290, training loss: 7.524344444274902 = 1.1011748313903809 + 1.0 * 6.4231696128845215
Epoch 290, val loss: 1.2200978994369507
Epoch 300, training loss: 7.472291946411133 = 1.0513014793395996 + 1.0 * 6.420990467071533
Epoch 300, val loss: 1.1813708543777466
Epoch 310, training loss: 7.4196319580078125 = 1.0036834478378296 + 1.0 * 6.415948390960693
Epoch 310, val loss: 1.144876480102539
Epoch 320, training loss: 7.367386817932129 = 0.9579468965530396 + 1.0 * 6.409440040588379
Epoch 320, val loss: 1.1102668046951294
Epoch 330, training loss: 7.319883346557617 = 0.9139893651008606 + 1.0 * 6.405893802642822
Epoch 330, val loss: 1.0772868394851685
Epoch 340, training loss: 7.282876014709473 = 0.8721897602081299 + 1.0 * 6.410686016082764
Epoch 340, val loss: 1.0463707447052002
Epoch 350, training loss: 7.233309745788574 = 0.8331568837165833 + 1.0 * 6.400152683258057
Epoch 350, val loss: 1.0181180238723755
Epoch 360, training loss: 7.192582607269287 = 0.7965642809867859 + 1.0 * 6.3960185050964355
Epoch 360, val loss: 0.9921662211418152
Epoch 370, training loss: 7.155387878417969 = 0.7622295022010803 + 1.0 * 6.393158435821533
Epoch 370, val loss: 0.9683414101600647
Epoch 380, training loss: 7.119927406311035 = 0.7299284934997559 + 1.0 * 6.389998912811279
Epoch 380, val loss: 0.9470269083976746
Epoch 390, training loss: 7.0864434242248535 = 0.6992993354797363 + 1.0 * 6.387144088745117
Epoch 390, val loss: 0.9273838996887207
Epoch 400, training loss: 7.058446884155273 = 0.6701348423957825 + 1.0 * 6.388311862945557
Epoch 400, val loss: 0.9096433520317078
Epoch 410, training loss: 7.023917198181152 = 0.6424833536148071 + 1.0 * 6.381433963775635
Epoch 410, val loss: 0.8935774564743042
Epoch 420, training loss: 6.9936299324035645 = 0.6158352494239807 + 1.0 * 6.3777947425842285
Epoch 420, val loss: 0.8789263367652893
Epoch 430, training loss: 6.969269275665283 = 0.5899872183799744 + 1.0 * 6.379281997680664
Epoch 430, val loss: 0.8655582070350647
Epoch 440, training loss: 6.942119598388672 = 0.5652341246604919 + 1.0 * 6.376885414123535
Epoch 440, val loss: 0.8532923460006714
Epoch 450, training loss: 6.911880970001221 = 0.5414242148399353 + 1.0 * 6.370456695556641
Epoch 450, val loss: 0.8425371050834656
Epoch 460, training loss: 6.891138076782227 = 0.5184595584869385 + 1.0 * 6.372678279876709
Epoch 460, val loss: 0.8327332735061646
Epoch 470, training loss: 6.863868236541748 = 0.4963245689868927 + 1.0 * 6.367543697357178
Epoch 470, val loss: 0.8239884376525879
Epoch 480, training loss: 6.840963363647461 = 0.4750196933746338 + 1.0 * 6.365943431854248
Epoch 480, val loss: 0.8161783814430237
Epoch 490, training loss: 6.822677135467529 = 0.4545806348323822 + 1.0 * 6.368096351623535
Epoch 490, val loss: 0.8091302514076233
Epoch 500, training loss: 6.796987056732178 = 0.43500450253486633 + 1.0 * 6.361982345581055
Epoch 500, val loss: 0.8031350374221802
Epoch 510, training loss: 6.773797035217285 = 0.41608646512031555 + 1.0 * 6.357710361480713
Epoch 510, val loss: 0.7977935671806335
Epoch 520, training loss: 6.76269006729126 = 0.39772137999534607 + 1.0 * 6.364968776702881
Epoch 520, val loss: 0.7929868102073669
Epoch 530, training loss: 6.736142158508301 = 0.38002148270606995 + 1.0 * 6.356120586395264
Epoch 530, val loss: 0.7886067032814026
Epoch 540, training loss: 6.716137409210205 = 0.362890362739563 + 1.0 * 6.353247165679932
Epoch 540, val loss: 0.7846909761428833
Epoch 550, training loss: 6.69765043258667 = 0.3461969196796417 + 1.0 * 6.3514533042907715
Epoch 550, val loss: 0.781130850315094
Epoch 560, training loss: 6.6802167892456055 = 0.33002597093582153 + 1.0 * 6.35019063949585
Epoch 560, val loss: 0.7778497338294983
Epoch 570, training loss: 6.661445617675781 = 0.31432899832725525 + 1.0 * 6.347116470336914
Epoch 570, val loss: 0.7749861478805542
Epoch 580, training loss: 6.645326614379883 = 0.2990536093711853 + 1.0 * 6.346272945404053
Epoch 580, val loss: 0.7722571492195129
Epoch 590, training loss: 6.635562896728516 = 0.2841707766056061 + 1.0 * 6.3513922691345215
Epoch 590, val loss: 0.7697368860244751
Epoch 600, training loss: 6.611722946166992 = 0.2697376012802124 + 1.0 * 6.34198522567749
Epoch 600, val loss: 0.7674239873886108
Epoch 610, training loss: 6.59632682800293 = 0.2556895315647125 + 1.0 * 6.34063720703125
Epoch 610, val loss: 0.7652574181556702
Epoch 620, training loss: 6.5854973793029785 = 0.24207761883735657 + 1.0 * 6.343419551849365
Epoch 620, val loss: 0.7632773518562317
Epoch 630, training loss: 6.568342208862305 = 0.228974848985672 + 1.0 * 6.339367389678955
Epoch 630, val loss: 0.7616233229637146
Epoch 640, training loss: 6.554961204528809 = 0.21638666093349457 + 1.0 * 6.338574409484863
Epoch 640, val loss: 0.7602195143699646
Epoch 650, training loss: 6.544713497161865 = 0.20433899760246277 + 1.0 * 6.34037446975708
Epoch 650, val loss: 0.7590737342834473
Epoch 660, training loss: 6.530911445617676 = 0.19286829233169556 + 1.0 * 6.338043212890625
Epoch 660, val loss: 0.7582880258560181
Epoch 670, training loss: 6.514036178588867 = 0.1819697767496109 + 1.0 * 6.332066535949707
Epoch 670, val loss: 0.7578575015068054
Epoch 680, training loss: 6.50487756729126 = 0.17163397371768951 + 1.0 * 6.333243370056152
Epoch 680, val loss: 0.7578726410865784
Epoch 690, training loss: 6.496016979217529 = 0.16187581419944763 + 1.0 * 6.334141254425049
Epoch 690, val loss: 0.7582525610923767
Epoch 700, training loss: 6.481841087341309 = 0.15270821750164032 + 1.0 * 6.329133033752441
Epoch 700, val loss: 0.7589977979660034
Epoch 710, training loss: 6.47539758682251 = 0.14406731724739075 + 1.0 * 6.331330299377441
Epoch 710, val loss: 0.7602055668830872
Epoch 720, training loss: 6.4635162353515625 = 0.13598884642124176 + 1.0 * 6.3275275230407715
Epoch 720, val loss: 0.7616613507270813
Epoch 730, training loss: 6.456356048583984 = 0.1284014731645584 + 1.0 * 6.3279547691345215
Epoch 730, val loss: 0.7636390924453735
Epoch 740, training loss: 6.446455001831055 = 0.12131448090076447 + 1.0 * 6.325140476226807
Epoch 740, val loss: 0.7658975124359131
Epoch 750, training loss: 6.4390950202941895 = 0.11466877907514572 + 1.0 * 6.324426174163818
Epoch 750, val loss: 0.7685475945472717
Epoch 760, training loss: 6.430734157562256 = 0.108455128967762 + 1.0 * 6.32227897644043
Epoch 760, val loss: 0.7715425491333008
Epoch 770, training loss: 6.4310126304626465 = 0.10263824462890625 + 1.0 * 6.32837438583374
Epoch 770, val loss: 0.7748122215270996
Epoch 780, training loss: 6.418074131011963 = 0.09722436219453812 + 1.0 * 6.320849895477295
Epoch 780, val loss: 0.7783157229423523
Epoch 790, training loss: 6.41920280456543 = 0.09214434027671814 + 1.0 * 6.3270583152771
Epoch 790, val loss: 0.7820268273353577
Epoch 800, training loss: 6.407067775726318 = 0.0874217078089714 + 1.0 * 6.319645881652832
Epoch 800, val loss: 0.7858924269676208
Epoch 810, training loss: 6.399374008178711 = 0.08298389613628387 + 1.0 * 6.316390037536621
Epoch 810, val loss: 0.7900406718254089
Epoch 820, training loss: 6.402200222015381 = 0.07884139567613602 + 1.0 * 6.32335901260376
Epoch 820, val loss: 0.7943356037139893
Epoch 830, training loss: 6.389169692993164 = 0.07496807724237442 + 1.0 * 6.314201831817627
Epoch 830, val loss: 0.798792839050293
Epoch 840, training loss: 6.383909702301025 = 0.07134349644184113 + 1.0 * 6.31256628036499
Epoch 840, val loss: 0.8033931851387024
Epoch 850, training loss: 6.381256580352783 = 0.06793037801980972 + 1.0 * 6.313326358795166
Epoch 850, val loss: 0.8081328272819519
Epoch 860, training loss: 6.375728130340576 = 0.06472756713628769 + 1.0 * 6.311000347137451
Epoch 860, val loss: 0.8129910826683044
Epoch 870, training loss: 6.372346878051758 = 0.06173068657517433 + 1.0 * 6.3106160163879395
Epoch 870, val loss: 0.8179394602775574
Epoch 880, training loss: 6.369024276733398 = 0.058918774127960205 + 1.0 * 6.310105323791504
Epoch 880, val loss: 0.8229240775108337
Epoch 890, training loss: 6.364470958709717 = 0.05627324432134628 + 1.0 * 6.308197498321533
Epoch 890, val loss: 0.8279969096183777
Epoch 900, training loss: 6.370917320251465 = 0.053784873336553574 + 1.0 * 6.317132472991943
Epoch 900, val loss: 0.8330879211425781
Epoch 910, training loss: 6.35980749130249 = 0.051465488970279694 + 1.0 * 6.308341979980469
Epoch 910, val loss: 0.8382242918014526
Epoch 920, training loss: 6.354729175567627 = 0.04928288236260414 + 1.0 * 6.305446147918701
Epoch 920, val loss: 0.8433592319488525
Epoch 930, training loss: 6.3504958152771 = 0.047213416546583176 + 1.0 * 6.303282260894775
Epoch 930, val loss: 0.8485854864120483
Epoch 940, training loss: 6.34865140914917 = 0.04525498300790787 + 1.0 * 6.303396224975586
Epoch 940, val loss: 0.8538656234741211
Epoch 950, training loss: 6.356527328491211 = 0.04340847581624985 + 1.0 * 6.313118934631348
Epoch 950, val loss: 0.8590718507766724
Epoch 960, training loss: 6.351327896118164 = 0.0416816808283329 + 1.0 * 6.309646129608154
Epoch 960, val loss: 0.8641394376754761
Epoch 970, training loss: 6.341218948364258 = 0.040056947618722916 + 1.0 * 6.301161766052246
Epoch 970, val loss: 0.8692620992660522
Epoch 980, training loss: 6.339569568634033 = 0.03851745277643204 + 1.0 * 6.301052093505859
Epoch 980, val loss: 0.8743916749954224
Epoch 990, training loss: 6.338043689727783 = 0.037053368985652924 + 1.0 * 6.300990104675293
Epoch 990, val loss: 0.8795740604400635
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8039008961518187
The final CL Acc:0.75926, 0.02885, The final GNN Acc:0.80408, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13162])
remove edge: torch.Size([2, 7936])
updated graph: torch.Size([2, 10542])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.549331665039062 = 1.9525010585784912 + 1.0 * 8.596830368041992
Epoch 0, val loss: 1.9563860893249512
Epoch 10, training loss: 10.538259506225586 = 1.941697597503662 + 1.0 * 8.596561431884766
Epoch 10, val loss: 1.945164442062378
Epoch 20, training loss: 10.52273941040039 = 1.9281750917434692 + 1.0 * 8.594564437866211
Epoch 20, val loss: 1.9307866096496582
Epoch 30, training loss: 10.488662719726562 = 1.9090187549591064 + 1.0 * 8.579644203186035
Epoch 30, val loss: 1.9102286100387573
Epoch 40, training loss: 10.391538619995117 = 1.8826208114624023 + 1.0 * 8.508917808532715
Epoch 40, val loss: 1.8828767538070679
Epoch 50, training loss: 10.061098098754883 = 1.8544011116027832 + 1.0 * 8.206696510314941
Epoch 50, val loss: 1.8548039197921753
Epoch 60, training loss: 9.714468002319336 = 1.8271116018295288 + 1.0 * 7.887356758117676
Epoch 60, val loss: 1.829427719116211
Epoch 70, training loss: 9.252461433410645 = 1.808040738105774 + 1.0 * 7.44442081451416
Epoch 70, val loss: 1.811585783958435
Epoch 80, training loss: 8.93889045715332 = 1.795093297958374 + 1.0 * 7.143797397613525
Epoch 80, val loss: 1.7995221614837646
Epoch 90, training loss: 8.762592315673828 = 1.778555989265442 + 1.0 * 6.984036445617676
Epoch 90, val loss: 1.7845730781555176
Epoch 100, training loss: 8.642443656921387 = 1.7590545415878296 + 1.0 * 6.883389472961426
Epoch 100, val loss: 1.7679991722106934
Epoch 110, training loss: 8.542349815368652 = 1.7395362854003906 + 1.0 * 6.802813529968262
Epoch 110, val loss: 1.7512253522872925
Epoch 120, training loss: 8.46601676940918 = 1.718785285949707 + 1.0 * 6.747231960296631
Epoch 120, val loss: 1.7325295209884644
Epoch 130, training loss: 8.399895668029785 = 1.6943024396896362 + 1.0 * 6.705593585968018
Epoch 130, val loss: 1.7106159925460815
Epoch 140, training loss: 8.33840560913086 = 1.665446400642395 + 1.0 * 6.672958850860596
Epoch 140, val loss: 1.6857504844665527
Epoch 150, training loss: 8.272006034851074 = 1.6325712203979492 + 1.0 * 6.639434814453125
Epoch 150, val loss: 1.6581333875656128
Epoch 160, training loss: 8.208877563476562 = 1.5945515632629395 + 1.0 * 6.614326000213623
Epoch 160, val loss: 1.6263936758041382
Epoch 170, training loss: 8.144667625427246 = 1.5518639087677002 + 1.0 * 6.592803478240967
Epoch 170, val loss: 1.59123957157135
Epoch 180, training loss: 8.077792167663574 = 1.505582571029663 + 1.0 * 6.572209358215332
Epoch 180, val loss: 1.5536913871765137
Epoch 190, training loss: 8.012451171875 = 1.456716775894165 + 1.0 * 6.555734634399414
Epoch 190, val loss: 1.5148942470550537
Epoch 200, training loss: 7.9439191818237305 = 1.4079043865203857 + 1.0 * 6.536014556884766
Epoch 200, val loss: 1.4773895740509033
Epoch 210, training loss: 7.880443096160889 = 1.3594962358474731 + 1.0 * 6.520946979522705
Epoch 210, val loss: 1.4411094188690186
Epoch 220, training loss: 7.820055961608887 = 1.311484456062317 + 1.0 * 6.508571624755859
Epoch 220, val loss: 1.4061543941497803
Epoch 230, training loss: 7.762073516845703 = 1.2649071216583252 + 1.0 * 6.497166633605957
Epoch 230, val loss: 1.3732393980026245
Epoch 240, training loss: 7.706976890563965 = 1.2197623252868652 + 1.0 * 6.4872145652771
Epoch 240, val loss: 1.3419286012649536
Epoch 250, training loss: 7.653221607208252 = 1.1762075424194336 + 1.0 * 6.477014064788818
Epoch 250, val loss: 1.3122543096542358
Epoch 260, training loss: 7.601981163024902 = 1.134494662284851 + 1.0 * 6.467486381530762
Epoch 260, val loss: 1.2843717336654663
Epoch 270, training loss: 7.553482532501221 = 1.0944432020187378 + 1.0 * 6.459039211273193
Epoch 270, val loss: 1.2580424547195435
Epoch 280, training loss: 7.507591247558594 = 1.0565108060836792 + 1.0 * 6.451080322265625
Epoch 280, val loss: 1.2334626913070679
Epoch 290, training loss: 7.463870048522949 = 1.0207858085632324 + 1.0 * 6.443084239959717
Epoch 290, val loss: 1.210382103919983
Epoch 300, training loss: 7.4228835105896 = 0.9868577122688293 + 1.0 * 6.436025619506836
Epoch 300, val loss: 1.1886968612670898
Epoch 310, training loss: 7.384788513183594 = 0.9546056389808655 + 1.0 * 6.430182933807373
Epoch 310, val loss: 1.168251395225525
Epoch 320, training loss: 7.347545146942139 = 0.9239449501037598 + 1.0 * 6.423600196838379
Epoch 320, val loss: 1.148810863494873
Epoch 330, training loss: 7.315478324890137 = 0.894171953201294 + 1.0 * 6.421306610107422
Epoch 330, val loss: 1.1299903392791748
Epoch 340, training loss: 7.282085418701172 = 0.8651182651519775 + 1.0 * 6.416967391967773
Epoch 340, val loss: 1.111473560333252
Epoch 350, training loss: 7.245889186859131 = 0.8365222811698914 + 1.0 * 6.409367084503174
Epoch 350, val loss: 1.0930836200714111
Epoch 360, training loss: 7.211996555328369 = 0.8078498244285583 + 1.0 * 6.404146671295166
Epoch 360, val loss: 1.074444055557251
Epoch 370, training loss: 7.182331085205078 = 0.7788543105125427 + 1.0 * 6.403476715087891
Epoch 370, val loss: 1.0552875995635986
Epoch 380, training loss: 7.149682998657227 = 0.7499487996101379 + 1.0 * 6.399734020233154
Epoch 380, val loss: 1.0359256267547607
Epoch 390, training loss: 7.114941596984863 = 0.72098308801651 + 1.0 * 6.393958568572998
Epoch 390, val loss: 1.0166946649551392
Epoch 400, training loss: 7.081303596496582 = 0.6918677687644958 + 1.0 * 6.389435768127441
Epoch 400, val loss: 0.9972269535064697
Epoch 410, training loss: 7.059067726135254 = 0.662688672542572 + 1.0 * 6.396378993988037
Epoch 410, val loss: 0.9777280688285828
Epoch 420, training loss: 7.016623497009277 = 0.6340557932853699 + 1.0 * 6.382567882537842
Epoch 420, val loss: 0.9589371085166931
Epoch 430, training loss: 6.986355781555176 = 0.6057988405227661 + 1.0 * 6.380557060241699
Epoch 430, val loss: 0.9407682418823242
Epoch 440, training loss: 6.95462703704834 = 0.5779398679733276 + 1.0 * 6.376687049865723
Epoch 440, val loss: 0.9231709837913513
Epoch 450, training loss: 6.927377223968506 = 0.5508238077163696 + 1.0 * 6.376553535461426
Epoch 450, val loss: 0.9065238237380981
Epoch 460, training loss: 6.900343894958496 = 0.5249621868133545 + 1.0 * 6.375381946563721
Epoch 460, val loss: 0.891599714756012
Epoch 470, training loss: 6.8698015213012695 = 0.4998863935470581 + 1.0 * 6.369915008544922
Epoch 470, val loss: 0.8780983686447144
Epoch 480, training loss: 6.842522621154785 = 0.4755609333515167 + 1.0 * 6.366961479187012
Epoch 480, val loss: 0.8655653595924377
Epoch 490, training loss: 6.8321967124938965 = 0.4519776999950409 + 1.0 * 6.380218982696533
Epoch 490, val loss: 0.8544164896011353
Epoch 500, training loss: 6.793994903564453 = 0.4293907880783081 + 1.0 * 6.3646039962768555
Epoch 500, val loss: 0.8448396325111389
Epoch 510, training loss: 6.767781734466553 = 0.4076407849788666 + 1.0 * 6.360140800476074
Epoch 510, val loss: 0.8368632793426514
Epoch 520, training loss: 6.744862079620361 = 0.3866417706012726 + 1.0 * 6.358220100402832
Epoch 520, val loss: 0.8300745487213135
Epoch 530, training loss: 6.743739128112793 = 0.36642852425575256 + 1.0 * 6.377310752868652
Epoch 530, val loss: 0.8245092034339905
Epoch 540, training loss: 6.703555583953857 = 0.3471914827823639 + 1.0 * 6.3563642501831055
Epoch 540, val loss: 0.8204195499420166
Epoch 550, training loss: 6.687701225280762 = 0.3286723792552948 + 1.0 * 6.3590288162231445
Epoch 550, val loss: 0.817552924156189
Epoch 560, training loss: 6.663352966308594 = 0.3107723295688629 + 1.0 * 6.352580547332764
Epoch 560, val loss: 0.8153683543205261
Epoch 570, training loss: 6.642573833465576 = 0.2933872640132904 + 1.0 * 6.349186420440674
Epoch 570, val loss: 0.8142206072807312
Epoch 580, training loss: 6.626224040985107 = 0.27641651034355164 + 1.0 * 6.3498077392578125
Epoch 580, val loss: 0.813723623752594
Epoch 590, training loss: 6.61107063293457 = 0.25999152660369873 + 1.0 * 6.351078987121582
Epoch 590, val loss: 0.813830554485321
Epoch 600, training loss: 6.588234901428223 = 0.24412144720554352 + 1.0 * 6.344113349914551
Epoch 600, val loss: 0.8151358962059021
Epoch 610, training loss: 6.571367263793945 = 0.22882556915283203 + 1.0 * 6.342541694641113
Epoch 610, val loss: 0.8169715404510498
Epoch 620, training loss: 6.5563578605651855 = 0.21414855122566223 + 1.0 * 6.342209339141846
Epoch 620, val loss: 0.8195878863334656
Epoch 630, training loss: 6.546846866607666 = 0.2002919763326645 + 1.0 * 6.346554756164551
Epoch 630, val loss: 0.8226856589317322
Epoch 640, training loss: 6.528910160064697 = 0.18735429644584656 + 1.0 * 6.341556072235107
Epoch 640, val loss: 0.8268880844116211
Epoch 650, training loss: 6.5156636238098145 = 0.17530323565006256 + 1.0 * 6.340360164642334
Epoch 650, val loss: 0.8317068219184875
Epoch 660, training loss: 6.502877235412598 = 0.16415910422801971 + 1.0 * 6.338717937469482
Epoch 660, val loss: 0.8368669748306274
Epoch 670, training loss: 6.488426208496094 = 0.15389291942119598 + 1.0 * 6.334533214569092
Epoch 670, val loss: 0.8430962562561035
Epoch 680, training loss: 6.477044582366943 = 0.14441069960594177 + 1.0 * 6.332633972167969
Epoch 680, val loss: 0.8495721220970154
Epoch 690, training loss: 6.477025985717773 = 0.13569921255111694 + 1.0 * 6.341326713562012
Epoch 690, val loss: 0.8561740517616272
Epoch 700, training loss: 6.461813449859619 = 0.12774425745010376 + 1.0 * 6.33406925201416
Epoch 700, val loss: 0.8636265397071838
Epoch 710, training loss: 6.449606895446777 = 0.12041370570659637 + 1.0 * 6.329193115234375
Epoch 710, val loss: 0.871211588382721
Epoch 720, training loss: 6.44088077545166 = 0.1136331707239151 + 1.0 * 6.327247619628906
Epoch 720, val loss: 0.878849446773529
Epoch 730, training loss: 6.433429718017578 = 0.10734035819768906 + 1.0 * 6.326089382171631
Epoch 730, val loss: 0.8868533372879028
Epoch 740, training loss: 6.442806720733643 = 0.10150366276502609 + 1.0 * 6.341302871704102
Epoch 740, val loss: 0.8946138620376587
Epoch 750, training loss: 6.420380592346191 = 0.09613808989524841 + 1.0 * 6.32424259185791
Epoch 750, val loss: 0.9027907252311707
Epoch 760, training loss: 6.414315223693848 = 0.09115569293498993 + 1.0 * 6.323159694671631
Epoch 760, val loss: 0.9110132455825806
Epoch 770, training loss: 6.408085823059082 = 0.08651254326105118 + 1.0 * 6.321573257446289
Epoch 770, val loss: 0.919036328792572
Epoch 780, training loss: 6.417917251586914 = 0.08218750357627869 + 1.0 * 6.335729598999023
Epoch 780, val loss: 0.927001953125
Epoch 790, training loss: 6.398636817932129 = 0.0781862735748291 + 1.0 * 6.320450782775879
Epoch 790, val loss: 0.9351745247840881
Epoch 800, training loss: 6.400207042694092 = 0.07446076720952988 + 1.0 * 6.325746059417725
Epoch 800, val loss: 0.9432322978973389
Epoch 810, training loss: 6.390881061553955 = 0.07098071277141571 + 1.0 * 6.3199005126953125
Epoch 810, val loss: 0.9509871006011963
Epoch 820, training loss: 6.384028434753418 = 0.0677318125963211 + 1.0 * 6.316296577453613
Epoch 820, val loss: 0.9590268731117249
Epoch 830, training loss: 6.380792617797852 = 0.06468137353658676 + 1.0 * 6.316111087799072
Epoch 830, val loss: 0.9667448997497559
Epoch 840, training loss: 6.3820600509643555 = 0.06181984767317772 + 1.0 * 6.320240020751953
Epoch 840, val loss: 0.9743913412094116
Epoch 850, training loss: 6.375357151031494 = 0.059131283313035965 + 1.0 * 6.316226005554199
Epoch 850, val loss: 0.9820518493652344
Epoch 860, training loss: 6.378047466278076 = 0.056612905114889145 + 1.0 * 6.321434497833252
Epoch 860, val loss: 0.9894570708274841
Epoch 870, training loss: 6.368330478668213 = 0.054254718124866486 + 1.0 * 6.314075946807861
Epoch 870, val loss: 0.9968963861465454
Epoch 880, training loss: 6.363922119140625 = 0.05202867090702057 + 1.0 * 6.311893463134766
Epoch 880, val loss: 1.0042431354522705
Epoch 890, training loss: 6.3646392822265625 = 0.04992954432964325 + 1.0 * 6.314709663391113
Epoch 890, val loss: 1.0113365650177002
Epoch 900, training loss: 6.358531475067139 = 0.04794898256659508 + 1.0 * 6.310582637786865
Epoch 900, val loss: 1.0183945894241333
Epoch 910, training loss: 6.353498935699463 = 0.04607390984892845 + 1.0 * 6.307425022125244
Epoch 910, val loss: 1.0255125761032104
Epoch 920, training loss: 6.354269027709961 = 0.044295623898506165 + 1.0 * 6.309973239898682
Epoch 920, val loss: 1.0323468446731567
Epoch 930, training loss: 6.355319976806641 = 0.042617134749889374 + 1.0 * 6.312702655792236
Epoch 930, val loss: 1.0387675762176514
Epoch 940, training loss: 6.348494529724121 = 0.04103357344865799 + 1.0 * 6.307460784912109
Epoch 940, val loss: 1.0457977056503296
Epoch 950, training loss: 6.345332145690918 = 0.039534568786621094 + 1.0 * 6.305797576904297
Epoch 950, val loss: 1.0524497032165527
Epoch 960, training loss: 6.3421735763549805 = 0.03810334578156471 + 1.0 * 6.304069995880127
Epoch 960, val loss: 1.0588479042053223
Epoch 970, training loss: 6.349242210388184 = 0.03674378991127014 + 1.0 * 6.312498569488525
Epoch 970, val loss: 1.0651428699493408
Epoch 980, training loss: 6.337937831878662 = 0.035450685769319534 + 1.0 * 6.302487373352051
Epoch 980, val loss: 1.0716588497161865
Epoch 990, training loss: 6.337029933929443 = 0.03422271087765694 + 1.0 * 6.302807331085205
Epoch 990, val loss: 1.0779554843902588
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8318397469688983
=== training gcn model ===
Epoch 0, training loss: 10.555702209472656 = 1.9588673114776611 + 1.0 * 8.596835136413574
Epoch 0, val loss: 1.9580494165420532
Epoch 10, training loss: 10.544918060302734 = 1.9482803344726562 + 1.0 * 8.596637725830078
Epoch 10, val loss: 1.9476664066314697
Epoch 20, training loss: 10.530308723449707 = 1.935276746749878 + 1.0 * 8.59503173828125
Epoch 20, val loss: 1.9345662593841553
Epoch 30, training loss: 10.498625755310059 = 1.9168529510498047 + 1.0 * 8.581772804260254
Epoch 30, val loss: 1.9157987833023071
Epoch 40, training loss: 10.402957916259766 = 1.890892505645752 + 1.0 * 8.512065887451172
Epoch 40, val loss: 1.8901898860931396
Epoch 50, training loss: 10.016355514526367 = 1.861738920211792 + 1.0 * 8.154616355895996
Epoch 50, val loss: 1.8625608682632446
Epoch 60, training loss: 9.617401123046875 = 1.8345426321029663 + 1.0 * 7.782858371734619
Epoch 60, val loss: 1.8386592864990234
Epoch 70, training loss: 9.149163246154785 = 1.8166362047195435 + 1.0 * 7.332527160644531
Epoch 70, val loss: 1.8234416246414185
Epoch 80, training loss: 8.89097785949707 = 1.8015614748001099 + 1.0 * 7.089416027069092
Epoch 80, val loss: 1.8100354671478271
Epoch 90, training loss: 8.74474048614502 = 1.783900499343872 + 1.0 * 6.960840225219727
Epoch 90, val loss: 1.7942432165145874
Epoch 100, training loss: 8.651883125305176 = 1.7642408609390259 + 1.0 * 6.887641906738281
Epoch 100, val loss: 1.7772600650787354
Epoch 110, training loss: 8.554640769958496 = 1.7456610202789307 + 1.0 * 6.8089799880981445
Epoch 110, val loss: 1.761082410812378
Epoch 120, training loss: 8.478586196899414 = 1.7267862558364868 + 1.0 * 6.751799583435059
Epoch 120, val loss: 1.7437371015548706
Epoch 130, training loss: 8.412344932556152 = 1.70502507686615 + 1.0 * 6.707319736480713
Epoch 130, val loss: 1.723777413368225
Epoch 140, training loss: 8.350333213806152 = 1.679580807685852 + 1.0 * 6.67075252532959
Epoch 140, val loss: 1.7013195753097534
Epoch 150, training loss: 8.285109519958496 = 1.6498881578445435 + 1.0 * 6.635221481323242
Epoch 150, val loss: 1.6760655641555786
Epoch 160, training loss: 8.219826698303223 = 1.6160862445831299 + 1.0 * 6.603740692138672
Epoch 160, val loss: 1.6477054357528687
Epoch 170, training loss: 8.155252456665039 = 1.5778437852859497 + 1.0 * 6.577408790588379
Epoch 170, val loss: 1.6157429218292236
Epoch 180, training loss: 8.092784881591797 = 1.5350961685180664 + 1.0 * 6.557689189910889
Epoch 180, val loss: 1.580073356628418
Epoch 190, training loss: 8.028393745422363 = 1.4890846014022827 + 1.0 * 6.539309024810791
Epoch 190, val loss: 1.5417219400405884
Epoch 200, training loss: 7.968949317932129 = 1.440844178199768 + 1.0 * 6.52810525894165
Epoch 200, val loss: 1.501781940460205
Epoch 210, training loss: 7.9063520431518555 = 1.392574667930603 + 1.0 * 6.513777256011963
Epoch 210, val loss: 1.462294101715088
Epoch 220, training loss: 7.8460259437561035 = 1.3446941375732422 + 1.0 * 6.501331806182861
Epoch 220, val loss: 1.4233946800231934
Epoch 230, training loss: 7.788938522338867 = 1.2971606254577637 + 1.0 * 6.4917778968811035
Epoch 230, val loss: 1.3851897716522217
Epoch 240, training loss: 7.733463287353516 = 1.2511154413223267 + 1.0 * 6.4823479652404785
Epoch 240, val loss: 1.3483587503433228
Epoch 250, training loss: 7.678581237792969 = 1.2063159942626953 + 1.0 * 6.472265243530273
Epoch 250, val loss: 1.3129394054412842
Epoch 260, training loss: 7.624948501586914 = 1.1616986989974976 + 1.0 * 6.463249683380127
Epoch 260, val loss: 1.2779173851013184
Epoch 270, training loss: 7.572449207305908 = 1.1167789697647095 + 1.0 * 6.455670356750488
Epoch 270, val loss: 1.2429461479187012
Epoch 280, training loss: 7.524248123168945 = 1.0718978643417358 + 1.0 * 6.45235013961792
Epoch 280, val loss: 1.2083265781402588
Epoch 290, training loss: 7.472546100616455 = 1.0278021097183228 + 1.0 * 6.444744110107422
Epoch 290, val loss: 1.174342155456543
Epoch 300, training loss: 7.421215057373047 = 0.9840569496154785 + 1.0 * 6.437158107757568
Epoch 300, val loss: 1.140567660331726
Epoch 310, training loss: 7.3717451095581055 = 0.9403054118156433 + 1.0 * 6.4314398765563965
Epoch 310, val loss: 1.1066744327545166
Epoch 320, training loss: 7.32297945022583 = 0.8966875076293945 + 1.0 * 6.4262919425964355
Epoch 320, val loss: 1.072813868522644
Epoch 330, training loss: 7.275137424468994 = 0.8536333441734314 + 1.0 * 6.421504020690918
Epoch 330, val loss: 1.0394549369812012
Epoch 340, training loss: 7.231130123138428 = 0.8120408654212952 + 1.0 * 6.419089317321777
Epoch 340, val loss: 1.0071024894714355
Epoch 350, training loss: 7.191216468811035 = 0.7716543078422546 + 1.0 * 6.419562339782715
Epoch 350, val loss: 0.9758509993553162
Epoch 360, training loss: 7.143143177032471 = 0.7329307794570923 + 1.0 * 6.410212516784668
Epoch 360, val loss: 0.9458367228507996
Epoch 370, training loss: 7.101749897003174 = 0.6955336928367615 + 1.0 * 6.406216144561768
Epoch 370, val loss: 0.9173396229743958
Epoch 380, training loss: 7.069515705108643 = 0.6595129370689392 + 1.0 * 6.410002708435059
Epoch 380, val loss: 0.8901770114898682
Epoch 390, training loss: 7.025470733642578 = 0.6250489354133606 + 1.0 * 6.400421619415283
Epoch 390, val loss: 0.8648964166641235
Epoch 400, training loss: 6.988678455352783 = 0.5920295119285583 + 1.0 * 6.39664888381958
Epoch 400, val loss: 0.8414928913116455
Epoch 410, training loss: 6.952292442321777 = 0.5604543089866638 + 1.0 * 6.391838073730469
Epoch 410, val loss: 0.8197047710418701
Epoch 420, training loss: 6.9184489250183105 = 0.5301344990730286 + 1.0 * 6.388314247131348
Epoch 420, val loss: 0.7996945381164551
Epoch 430, training loss: 6.890870571136475 = 0.5009322762489319 + 1.0 * 6.3899383544921875
Epoch 430, val loss: 0.7813793420791626
Epoch 440, training loss: 6.8583784103393555 = 0.4732988178730011 + 1.0 * 6.385079383850098
Epoch 440, val loss: 0.7650636434555054
Epoch 450, training loss: 6.8284010887146 = 0.44715583324432373 + 1.0 * 6.381245136260986
Epoch 450, val loss: 0.7509086728096008
Epoch 460, training loss: 6.799541473388672 = 0.42236313223838806 + 1.0 * 6.377178192138672
Epoch 460, val loss: 0.738603413105011
Epoch 470, training loss: 6.780664920806885 = 0.3990030586719513 + 1.0 * 6.381661891937256
Epoch 470, val loss: 0.7281609773635864
Epoch 480, training loss: 6.752090930938721 = 0.37740272283554077 + 1.0 * 6.374688148498535
Epoch 480, val loss: 0.7195934057235718
Epoch 490, training loss: 6.727427959442139 = 0.35732734203338623 + 1.0 * 6.370100498199463
Epoch 490, val loss: 0.7126753926277161
Epoch 500, training loss: 6.7135701179504395 = 0.3387390673160553 + 1.0 * 6.374831199645996
Epoch 500, val loss: 0.7071331739425659
Epoch 510, training loss: 6.687849998474121 = 0.32160767912864685 + 1.0 * 6.366242408752441
Epoch 510, val loss: 0.7029620409011841
Epoch 520, training loss: 6.668015480041504 = 0.30574697256088257 + 1.0 * 6.362268447875977
Epoch 520, val loss: 0.6999037265777588
Epoch 530, training loss: 6.6622090339660645 = 0.2909817695617676 + 1.0 * 6.371227264404297
Epoch 530, val loss: 0.6977124214172363
Epoch 540, training loss: 6.6383442878723145 = 0.27733930945396423 + 1.0 * 6.361004829406738
Epoch 540, val loss: 0.6963005065917969
Epoch 550, training loss: 6.6209516525268555 = 0.26450464129447937 + 1.0 * 6.356447219848633
Epoch 550, val loss: 0.6954820156097412
Epoch 560, training loss: 6.607250213623047 = 0.2522733807563782 + 1.0 * 6.354976654052734
Epoch 560, val loss: 0.6951019763946533
Epoch 570, training loss: 6.596755027770996 = 0.24053749442100525 + 1.0 * 6.356217384338379
Epoch 570, val loss: 0.6950674057006836
Epoch 580, training loss: 6.581772327423096 = 0.22923915088176727 + 1.0 * 6.352533340454102
Epoch 580, val loss: 0.695260763168335
Epoch 590, training loss: 6.568185806274414 = 0.21824152767658234 + 1.0 * 6.349944114685059
Epoch 590, val loss: 0.695590078830719
Epoch 600, training loss: 6.555449485778809 = 0.20743855834007263 + 1.0 * 6.348011016845703
Epoch 600, val loss: 0.695995032787323
Epoch 610, training loss: 6.548943042755127 = 0.19680346548557281 + 1.0 * 6.352139472961426
Epoch 610, val loss: 0.6964116096496582
Epoch 620, training loss: 6.5347113609313965 = 0.1864362210035324 + 1.0 * 6.348275184631348
Epoch 620, val loss: 0.6968821287155151
Epoch 630, training loss: 6.520129680633545 = 0.17624430358409882 + 1.0 * 6.34388542175293
Epoch 630, val loss: 0.6973883509635925
Epoch 640, training loss: 6.507636070251465 = 0.16624458134174347 + 1.0 * 6.341391563415527
Epoch 640, val loss: 0.6979327201843262
Epoch 650, training loss: 6.504755973815918 = 0.15649695694446564 + 1.0 * 6.348258972167969
Epoch 650, val loss: 0.6985766291618347
Epoch 660, training loss: 6.487572193145752 = 0.1471385657787323 + 1.0 * 6.340433597564697
Epoch 660, val loss: 0.6993355751037598
Epoch 670, training loss: 6.479257583618164 = 0.13816925883293152 + 1.0 * 6.34108829498291
Epoch 670, val loss: 0.7003233432769775
Epoch 680, training loss: 6.465486526489258 = 0.12966091930866241 + 1.0 * 6.335825443267822
Epoch 680, val loss: 0.7015213370323181
Epoch 690, training loss: 6.456306457519531 = 0.12163826078176498 + 1.0 * 6.334668159484863
Epoch 690, val loss: 0.7030189633369446
Epoch 700, training loss: 6.448421478271484 = 0.11410187929868698 + 1.0 * 6.334319591522217
Epoch 700, val loss: 0.7047995328903198
Epoch 710, training loss: 6.449853420257568 = 0.10706310719251633 + 1.0 * 6.342790126800537
Epoch 710, val loss: 0.7069088816642761
Epoch 720, training loss: 6.434341907501221 = 0.10059337317943573 + 1.0 * 6.3337483406066895
Epoch 720, val loss: 0.7092121243476868
Epoch 730, training loss: 6.423846244812012 = 0.09457835555076599 + 1.0 * 6.329267978668213
Epoch 730, val loss: 0.7117978930473328
Epoch 740, training loss: 6.417031764984131 = 0.08899234980344772 + 1.0 * 6.328039646148682
Epoch 740, val loss: 0.7146862149238586
Epoch 750, training loss: 6.417363166809082 = 0.08380922675132751 + 1.0 * 6.333553791046143
Epoch 750, val loss: 0.7178025841712952
Epoch 760, training loss: 6.4128241539001465 = 0.07905475050210953 + 1.0 * 6.33376932144165
Epoch 760, val loss: 0.7211923003196716
Epoch 770, training loss: 6.401241302490234 = 0.07467974722385406 + 1.0 * 6.326561450958252
Epoch 770, val loss: 0.7247514128684998
Epoch 780, training loss: 6.394618511199951 = 0.0706304982304573 + 1.0 * 6.32398796081543
Epoch 780, val loss: 0.7284969687461853
Epoch 790, training loss: 6.406254291534424 = 0.06688053905963898 + 1.0 * 6.339373588562012
Epoch 790, val loss: 0.7324399352073669
Epoch 800, training loss: 6.38657283782959 = 0.06340736895799637 + 1.0 * 6.323165416717529
Epoch 800, val loss: 0.736501157283783
Epoch 810, training loss: 6.382518291473389 = 0.06020311638712883 + 1.0 * 6.322315216064453
Epoch 810, val loss: 0.7406508326530457
Epoch 820, training loss: 6.376947402954102 = 0.057214245200157166 + 1.0 * 6.319733142852783
Epoch 820, val loss: 0.7449989914894104
Epoch 830, training loss: 6.378612518310547 = 0.05442273989319801 + 1.0 * 6.32418966293335
Epoch 830, val loss: 0.7494347095489502
Epoch 840, training loss: 6.37582540512085 = 0.05182894319295883 + 1.0 * 6.323996543884277
Epoch 840, val loss: 0.7539288997650146
Epoch 850, training loss: 6.368489742279053 = 0.04941091313958168 + 1.0 * 6.3190789222717285
Epoch 850, val loss: 0.7585076689720154
Epoch 860, training loss: 6.370323181152344 = 0.047152452170848846 + 1.0 * 6.3231706619262695
Epoch 860, val loss: 0.7631474137306213
Epoch 870, training loss: 6.363568305969238 = 0.04505932703614235 + 1.0 * 6.318509101867676
Epoch 870, val loss: 0.7677662372589111
Epoch 880, training loss: 6.357857704162598 = 0.0430816151201725 + 1.0 * 6.3147759437561035
Epoch 880, val loss: 0.7724818587303162
Epoch 890, training loss: 6.354763031005859 = 0.041227374225854874 + 1.0 * 6.313535690307617
Epoch 890, val loss: 0.777212381362915
Epoch 900, training loss: 6.3742899894714355 = 0.039477866142988205 + 1.0 * 6.334812164306641
Epoch 900, val loss: 0.7820331454277039
Epoch 910, training loss: 6.3535990715026855 = 0.03787223994731903 + 1.0 * 6.3157267570495605
Epoch 910, val loss: 0.7867182493209839
Epoch 920, training loss: 6.3475775718688965 = 0.03635108098387718 + 1.0 * 6.3112263679504395
Epoch 920, val loss: 0.791367769241333
Epoch 930, training loss: 6.345434665679932 = 0.034915491938591 + 1.0 * 6.310519218444824
Epoch 930, val loss: 0.7961302995681763
Epoch 940, training loss: 6.342434406280518 = 0.03355734795331955 + 1.0 * 6.308876991271973
Epoch 940, val loss: 0.8009142875671387
Epoch 950, training loss: 6.341660022735596 = 0.03227007016539574 + 1.0 * 6.309390068054199
Epoch 950, val loss: 0.8056925535202026
Epoch 960, training loss: 6.340034484863281 = 0.03105606697499752 + 1.0 * 6.30897855758667
Epoch 960, val loss: 0.8104715943336487
Epoch 970, training loss: 6.344089031219482 = 0.029919512569904327 + 1.0 * 6.314169406890869
Epoch 970, val loss: 0.8151257038116455
Epoch 980, training loss: 6.3364152908325195 = 0.028848735615611076 + 1.0 * 6.3075666427612305
Epoch 980, val loss: 0.8197513222694397
Epoch 990, training loss: 6.333647727966309 = 0.02783111296594143 + 1.0 * 6.305816650390625
Epoch 990, val loss: 0.8243551850318909
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 10.537924766540527 = 1.9410663843154907 + 1.0 * 8.596858024597168
Epoch 0, val loss: 1.943048357963562
Epoch 10, training loss: 10.52758502960205 = 1.9309217929840088 + 1.0 * 8.596663475036621
Epoch 10, val loss: 1.9326212406158447
Epoch 20, training loss: 10.51296615600586 = 1.9180982112884521 + 1.0 * 8.594867706298828
Epoch 20, val loss: 1.9193472862243652
Epoch 30, training loss: 10.4784574508667 = 1.899657130241394 + 1.0 * 8.578800201416016
Epoch 30, val loss: 1.9004663228988647
Epoch 40, training loss: 10.349371910095215 = 1.8741968870162964 + 1.0 * 8.475174903869629
Epoch 40, val loss: 1.8752399682998657
Epoch 50, training loss: 9.989977836608887 = 1.84536612033844 + 1.0 * 8.144611358642578
Epoch 50, val loss: 1.847813606262207
Epoch 60, training loss: 9.741378784179688 = 1.820517659187317 + 1.0 * 7.920860767364502
Epoch 60, val loss: 1.8252744674682617
Epoch 70, training loss: 9.372694969177246 = 1.8018375635147095 + 1.0 * 7.570857524871826
Epoch 70, val loss: 1.807565450668335
Epoch 80, training loss: 8.983718872070312 = 1.7918611764907837 + 1.0 * 7.191857814788818
Epoch 80, val loss: 1.7987979650497437
Epoch 90, training loss: 8.738543510437012 = 1.7822787761688232 + 1.0 * 6.956264972686768
Epoch 90, val loss: 1.7903025150299072
Epoch 100, training loss: 8.612260818481445 = 1.767120361328125 + 1.0 * 6.84514045715332
Epoch 100, val loss: 1.7768083810806274
Epoch 110, training loss: 8.524105072021484 = 1.750569224357605 + 1.0 * 6.773536205291748
Epoch 110, val loss: 1.761096477508545
Epoch 120, training loss: 8.463622093200684 = 1.7330684661865234 + 1.0 * 6.73055362701416
Epoch 120, val loss: 1.7443960905075073
Epoch 130, training loss: 8.407979965209961 = 1.713422179222107 + 1.0 * 6.694558143615723
Epoch 130, val loss: 1.7269150018692017
Epoch 140, training loss: 8.352606773376465 = 1.6913734674453735 + 1.0 * 6.661233425140381
Epoch 140, val loss: 1.7080678939819336
Epoch 150, training loss: 8.299921035766602 = 1.6660397052764893 + 1.0 * 6.633881092071533
Epoch 150, val loss: 1.6865496635437012
Epoch 160, training loss: 8.24151611328125 = 1.636340618133545 + 1.0 * 6.605175971984863
Epoch 160, val loss: 1.6614670753479004
Epoch 170, training loss: 8.184569358825684 = 1.6016501188278198 + 1.0 * 6.582919597625732
Epoch 170, val loss: 1.6320874691009521
Epoch 180, training loss: 8.126998901367188 = 1.5611640214920044 + 1.0 * 6.565834999084473
Epoch 180, val loss: 1.5977411270141602
Epoch 190, training loss: 8.06725788116455 = 1.5154900550842285 + 1.0 * 6.551767826080322
Epoch 190, val loss: 1.5593652725219727
Epoch 200, training loss: 8.003961563110352 = 1.4659955501556396 + 1.0 * 6.537966251373291
Epoch 200, val loss: 1.5180935859680176
Epoch 210, training loss: 7.938110828399658 = 1.413023591041565 + 1.0 * 6.525087356567383
Epoch 210, val loss: 1.4743595123291016
Epoch 220, training loss: 7.877741813659668 = 1.3582597970962524 + 1.0 * 6.519482135772705
Epoch 220, val loss: 1.429879903793335
Epoch 230, training loss: 7.809288024902344 = 1.303863763809204 + 1.0 * 6.505424499511719
Epoch 230, val loss: 1.3861175775527954
Epoch 240, training loss: 7.744024276733398 = 1.249276876449585 + 1.0 * 6.494747161865234
Epoch 240, val loss: 1.342320203781128
Epoch 250, training loss: 7.685939788818359 = 1.1945443153381348 + 1.0 * 6.491395473480225
Epoch 250, val loss: 1.2986899614334106
Epoch 260, training loss: 7.6211724281311035 = 1.1406151056289673 + 1.0 * 6.480557441711426
Epoch 260, val loss: 1.2559412717819214
Epoch 270, training loss: 7.558230400085449 = 1.0871769189834595 + 1.0 * 6.471053600311279
Epoch 270, val loss: 1.2138242721557617
Epoch 280, training loss: 7.500049114227295 = 1.0339850187301636 + 1.0 * 6.466063976287842
Epoch 280, val loss: 1.1718699932098389
Epoch 290, training loss: 7.4400954246521 = 0.9818252921104431 + 1.0 * 6.458270072937012
Epoch 290, val loss: 1.1306984424591064
Epoch 300, training loss: 7.381736755371094 = 0.9310063123703003 + 1.0 * 6.450730323791504
Epoch 300, val loss: 1.090561866760254
Epoch 310, training loss: 7.327797889709473 = 0.8818491101264954 + 1.0 * 6.445948600769043
Epoch 310, val loss: 1.0516902208328247
Epoch 320, training loss: 7.2743730545043945 = 0.8350169658660889 + 1.0 * 6.439356327056885
Epoch 320, val loss: 1.0148035287857056
Epoch 330, training loss: 7.225729465484619 = 0.7908901572227478 + 1.0 * 6.434839248657227
Epoch 330, val loss: 0.980283260345459
Epoch 340, training loss: 7.1800217628479 = 0.7500779032707214 + 1.0 * 6.429944038391113
Epoch 340, val loss: 0.9487652778625488
Epoch 350, training loss: 7.137785911560059 = 0.7120099067687988 + 1.0 * 6.42577600479126
Epoch 350, val loss: 0.9200493097305298
Epoch 360, training loss: 7.102049827575684 = 0.6767374873161316 + 1.0 * 6.425312519073486
Epoch 360, val loss: 0.8942927122116089
Epoch 370, training loss: 7.060197353363037 = 0.6442317962646484 + 1.0 * 6.415965557098389
Epoch 370, val loss: 0.871532678604126
Epoch 380, training loss: 7.025177001953125 = 0.6138005256652832 + 1.0 * 6.411376476287842
Epoch 380, val loss: 0.8511447310447693
Epoch 390, training loss: 6.994772911071777 = 0.5852257609367371 + 1.0 * 6.409547328948975
Epoch 390, val loss: 0.8330230116844177
Epoch 400, training loss: 6.962961196899414 = 0.5585448145866394 + 1.0 * 6.404416561126709
Epoch 400, val loss: 0.8171501159667969
Epoch 410, training loss: 6.933406352996826 = 0.5332576036453247 + 1.0 * 6.400148868560791
Epoch 410, val loss: 0.8030523061752319
Epoch 420, training loss: 6.907319068908691 = 0.5089884996414185 + 1.0 * 6.3983306884765625
Epoch 420, val loss: 0.790412187576294
Epoch 430, training loss: 6.887405872344971 = 0.48574304580688477 + 1.0 * 6.401662826538086
Epoch 430, val loss: 0.7791474461555481
Epoch 440, training loss: 6.855044364929199 = 0.46356767416000366 + 1.0 * 6.391476631164551
Epoch 440, val loss: 0.7691978216171265
Epoch 450, training loss: 6.828629016876221 = 0.4421527087688446 + 1.0 * 6.386476516723633
Epoch 450, val loss: 0.7603254318237305
Epoch 460, training loss: 6.810782432556152 = 0.42142513394355774 + 1.0 * 6.389357089996338
Epoch 460, val loss: 0.7523887753486633
Epoch 470, training loss: 6.783628940582275 = 0.4014453589916229 + 1.0 * 6.38218355178833
Epoch 470, val loss: 0.745349645614624
Epoch 480, training loss: 6.777510643005371 = 0.3820865750312805 + 1.0 * 6.395423889160156
Epoch 480, val loss: 0.7391334772109985
Epoch 490, training loss: 6.746294975280762 = 0.3637126684188843 + 1.0 * 6.382582187652588
Epoch 490, val loss: 0.7337288856506348
Epoch 500, training loss: 6.720104217529297 = 0.3460359275341034 + 1.0 * 6.374068260192871
Epoch 500, val loss: 0.7290338277816772
Epoch 510, training loss: 6.700227737426758 = 0.3288945257663727 + 1.0 * 6.371333122253418
Epoch 510, val loss: 0.724984884262085
Epoch 520, training loss: 6.680420875549316 = 0.3122437000274658 + 1.0 * 6.3681769371032715
Epoch 520, val loss: 0.7215274572372437
Epoch 530, training loss: 6.6652398109436035 = 0.2960822284221649 + 1.0 * 6.369157791137695
Epoch 530, val loss: 0.7186520099639893
Epoch 540, training loss: 6.6559038162231445 = 0.28055182099342346 + 1.0 * 6.375351905822754
Epoch 540, val loss: 0.7163668870925903
Epoch 550, training loss: 6.629059314727783 = 0.2657456696033478 + 1.0 * 6.363313674926758
Epoch 550, val loss: 0.7146171927452087
Epoch 560, training loss: 6.612179756164551 = 0.25149214267730713 + 1.0 * 6.360687732696533
Epoch 560, val loss: 0.7134974598884583
Epoch 570, training loss: 6.59895133972168 = 0.23779723048210144 + 1.0 * 6.361154079437256
Epoch 570, val loss: 0.7129763960838318
Epoch 580, training loss: 6.584506511688232 = 0.22473061084747314 + 1.0 * 6.359776020050049
Epoch 580, val loss: 0.713018000125885
Epoch 590, training loss: 6.569706916809082 = 0.21236027777194977 + 1.0 * 6.357346534729004
Epoch 590, val loss: 0.71365886926651
Epoch 600, training loss: 6.560235023498535 = 0.20059792697429657 + 1.0 * 6.359637260437012
Epoch 600, val loss: 0.7148716449737549
Epoch 610, training loss: 6.542080879211426 = 0.18947559595108032 + 1.0 * 6.35260534286499
Epoch 610, val loss: 0.7166117429733276
Epoch 620, training loss: 6.53400993347168 = 0.1789896935224533 + 1.0 * 6.355020046234131
Epoch 620, val loss: 0.7189333438873291
Epoch 630, training loss: 6.521533489227295 = 0.169127956032753 + 1.0 * 6.352405548095703
Epoch 630, val loss: 0.7217265367507935
Epoch 640, training loss: 6.508296966552734 = 0.15986782312393188 + 1.0 * 6.348429203033447
Epoch 640, val loss: 0.7250734567642212
Epoch 650, training loss: 6.498421669006348 = 0.15114052593708038 + 1.0 * 6.347280979156494
Epoch 650, val loss: 0.7288713455200195
Epoch 660, training loss: 6.488767147064209 = 0.14296618103981018 + 1.0 * 6.345800876617432
Epoch 660, val loss: 0.7330637574195862
Epoch 670, training loss: 6.4804511070251465 = 0.13533611595630646 + 1.0 * 6.3451151847839355
Epoch 670, val loss: 0.7376267910003662
Epoch 680, training loss: 6.476380825042725 = 0.12817689776420593 + 1.0 * 6.348204135894775
Epoch 680, val loss: 0.7425743937492371
Epoch 690, training loss: 6.462458610534668 = 0.12149778008460999 + 1.0 * 6.34096097946167
Epoch 690, val loss: 0.747771680355072
Epoch 700, training loss: 6.459911823272705 = 0.11521313339471817 + 1.0 * 6.344698905944824
Epoch 700, val loss: 0.7533033490180969
Epoch 710, training loss: 6.452221870422363 = 0.10935349017381668 + 1.0 * 6.342868328094482
Epoch 710, val loss: 0.7590800523757935
Epoch 720, training loss: 6.439405918121338 = 0.10385958105325699 + 1.0 * 6.335546493530273
Epoch 720, val loss: 0.7650134563446045
Epoch 730, training loss: 6.436398029327393 = 0.09869718551635742 + 1.0 * 6.337700843811035
Epoch 730, val loss: 0.7712206840515137
Epoch 740, training loss: 6.432093620300293 = 0.09386396408081055 + 1.0 * 6.338229656219482
Epoch 740, val loss: 0.7774764895439148
Epoch 750, training loss: 6.422399044036865 = 0.08934865146875381 + 1.0 * 6.33305025100708
Epoch 750, val loss: 0.7838053703308105
Epoch 760, training loss: 6.4166789054870605 = 0.08509919792413712 + 1.0 * 6.331579685211182
Epoch 760, val loss: 0.7902731895446777
Epoch 770, training loss: 6.414971828460693 = 0.08109855651855469 + 1.0 * 6.333873271942139
Epoch 770, val loss: 0.7968108057975769
Epoch 780, training loss: 6.410592555999756 = 0.07735259085893631 + 1.0 * 6.333240032196045
Epoch 780, val loss: 0.8033709526062012
Epoch 790, training loss: 6.401153564453125 = 0.07382794469594955 + 1.0 * 6.327325820922852
Epoch 790, val loss: 0.8099163770675659
Epoch 800, training loss: 6.395144462585449 = 0.07050488144159317 + 1.0 * 6.324639797210693
Epoch 800, val loss: 0.8165307641029358
Epoch 810, training loss: 6.3940911293029785 = 0.0673690214753151 + 1.0 * 6.326722145080566
Epoch 810, val loss: 0.823168158531189
Epoch 820, training loss: 6.396580696105957 = 0.06442128121852875 + 1.0 * 6.332159519195557
Epoch 820, val loss: 0.8297064304351807
Epoch 830, training loss: 6.384139537811279 = 0.06165274232625961 + 1.0 * 6.322486877441406
Epoch 830, val loss: 0.8361915946006775
Epoch 840, training loss: 6.379691123962402 = 0.05903952568769455 + 1.0 * 6.320651531219482
Epoch 840, val loss: 0.8427026867866516
Epoch 850, training loss: 6.385167598724365 = 0.05656806007027626 + 1.0 * 6.328599452972412
Epoch 850, val loss: 0.8492196202278137
Epoch 860, training loss: 6.3830461502075195 = 0.054239023476839066 + 1.0 * 6.328807353973389
Epoch 860, val loss: 0.8555833101272583
Epoch 870, training loss: 6.367934703826904 = 0.05203896388411522 + 1.0 * 6.3158955574035645
Epoch 870, val loss: 0.8618418574333191
Epoch 880, training loss: 6.36464262008667 = 0.049959905445575714 + 1.0 * 6.314682483673096
Epoch 880, val loss: 0.8681716918945312
Epoch 890, training loss: 6.361743927001953 = 0.04798540472984314 + 1.0 * 6.313758373260498
Epoch 890, val loss: 0.8744632005691528
Epoch 900, training loss: 6.369361877441406 = 0.046112433075904846 + 1.0 * 6.323249340057373
Epoch 900, val loss: 0.8806735277175903
Epoch 910, training loss: 6.360712051391602 = 0.04434617981314659 + 1.0 * 6.316365718841553
Epoch 910, val loss: 0.8867999315261841
Epoch 920, training loss: 6.355170726776123 = 0.04267941787838936 + 1.0 * 6.312491416931152
Epoch 920, val loss: 0.8928189277648926
Epoch 930, training loss: 6.35074520111084 = 0.04109865054488182 + 1.0 * 6.3096466064453125
Epoch 930, val loss: 0.8987923264503479
Epoch 940, training loss: 6.348475933074951 = 0.039593469351530075 + 1.0 * 6.308882236480713
Epoch 940, val loss: 0.904784619808197
Epoch 950, training loss: 6.353269100189209 = 0.038161493837833405 + 1.0 * 6.315107822418213
Epoch 950, val loss: 0.9107333421707153
Epoch 960, training loss: 6.347795486450195 = 0.03680157661437988 + 1.0 * 6.3109941482543945
Epoch 960, val loss: 0.9165322184562683
Epoch 970, training loss: 6.341845989227295 = 0.035509996116161346 + 1.0 * 6.306335926055908
Epoch 970, val loss: 0.922267735004425
Epoch 980, training loss: 6.340026378631592 = 0.03427956625819206 + 1.0 * 6.305747032165527
Epoch 980, val loss: 0.9279854893684387
Epoch 990, training loss: 6.340663909912109 = 0.033110056072473526 + 1.0 * 6.307553768157959
Epoch 990, val loss: 0.9336352348327637
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8365840801265156
The final CL Acc:0.79136, 0.01364, The final GNN Acc:0.83518, 0.00237
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9558])
updated graph: torch.Size([2, 10652])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.534440994262695 = 1.9376001358032227 + 1.0 * 8.596840858459473
Epoch 0, val loss: 1.933806300163269
Epoch 10, training loss: 10.524884223937988 = 1.9283409118652344 + 1.0 * 8.596543312072754
Epoch 10, val loss: 1.925062656402588
Epoch 20, training loss: 10.510884284973145 = 1.9165056943893433 + 1.0 * 8.594378471374512
Epoch 20, val loss: 1.9134663343429565
Epoch 30, training loss: 10.477160453796387 = 1.8995287418365479 + 1.0 * 8.577631950378418
Epoch 30, val loss: 1.896452784538269
Epoch 40, training loss: 10.343729019165039 = 1.8765556812286377 + 1.0 * 8.46717357635498
Epoch 40, val loss: 1.8741772174835205
Epoch 50, training loss: 9.83337688446045 = 1.8527390956878662 + 1.0 * 7.980637550354004
Epoch 50, val loss: 1.851901888847351
Epoch 60, training loss: 9.368152618408203 = 1.8329638242721558 + 1.0 * 7.535189151763916
Epoch 60, val loss: 1.8335908651351929
Epoch 70, training loss: 8.974227905273438 = 1.8209028244018555 + 1.0 * 7.153325080871582
Epoch 70, val loss: 1.8220131397247314
Epoch 80, training loss: 8.742242813110352 = 1.81173837184906 + 1.0 * 6.930504322052002
Epoch 80, val loss: 1.8130512237548828
Epoch 90, training loss: 8.623225212097168 = 1.7999236583709717 + 1.0 * 6.823301792144775
Epoch 90, val loss: 1.8020844459533691
Epoch 100, training loss: 8.533815383911133 = 1.7866852283477783 + 1.0 * 6.747129917144775
Epoch 100, val loss: 1.7902491092681885
Epoch 110, training loss: 8.460172653198242 = 1.7741907835006714 + 1.0 * 6.6859822273254395
Epoch 110, val loss: 1.7789747714996338
Epoch 120, training loss: 8.403438568115234 = 1.7617610692977905 + 1.0 * 6.6416778564453125
Epoch 120, val loss: 1.7679128646850586
Epoch 130, training loss: 8.35486125946045 = 1.7479186058044434 + 1.0 * 6.606942653656006
Epoch 130, val loss: 1.755988597869873
Epoch 140, training loss: 8.31187629699707 = 1.7317651510238647 + 1.0 * 6.580111026763916
Epoch 140, val loss: 1.7424699068069458
Epoch 150, training loss: 8.269975662231445 = 1.712860345840454 + 1.0 * 6.55711555480957
Epoch 150, val loss: 1.726921796798706
Epoch 160, training loss: 8.227919578552246 = 1.6905477046966553 + 1.0 * 6.537371635437012
Epoch 160, val loss: 1.7085789442062378
Epoch 170, training loss: 8.18556022644043 = 1.6637868881225586 + 1.0 * 6.521772861480713
Epoch 170, val loss: 1.6867018938064575
Epoch 180, training loss: 8.140190124511719 = 1.6323367357254028 + 1.0 * 6.5078535079956055
Epoch 180, val loss: 1.660783290863037
Epoch 190, training loss: 8.091279983520508 = 1.5956833362579346 + 1.0 * 6.495596885681152
Epoch 190, val loss: 1.6307870149612427
Epoch 200, training loss: 8.037895202636719 = 1.5534636974334717 + 1.0 * 6.484431266784668
Epoch 200, val loss: 1.596152424812317
Epoch 210, training loss: 7.98295259475708 = 1.5067421197891235 + 1.0 * 6.476210594177246
Epoch 210, val loss: 1.5577486753463745
Epoch 220, training loss: 7.922127723693848 = 1.4566417932510376 + 1.0 * 6.4654860496521
Epoch 220, val loss: 1.5168468952178955
Epoch 230, training loss: 7.8605523109436035 = 1.4040230512619019 + 1.0 * 6.456529140472412
Epoch 230, val loss: 1.4738900661468506
Epoch 240, training loss: 7.80685567855835 = 1.3500221967697144 + 1.0 * 6.456833362579346
Epoch 240, val loss: 1.4301837682724
Epoch 250, training loss: 7.739780902862549 = 1.2972453832626343 + 1.0 * 6.442535400390625
Epoch 250, val loss: 1.387490153312683
Epoch 260, training loss: 7.681642532348633 = 1.2452315092086792 + 1.0 * 6.436410903930664
Epoch 260, val loss: 1.3460856676101685
Epoch 270, training loss: 7.625545978546143 = 1.193891167640686 + 1.0 * 6.431654930114746
Epoch 270, val loss: 1.3055866956710815
Epoch 280, training loss: 7.56995153427124 = 1.1439208984375 + 1.0 * 6.42603063583374
Epoch 280, val loss: 1.2669180631637573
Epoch 290, training loss: 7.5157341957092285 = 1.0952023267745972 + 1.0 * 6.420531749725342
Epoch 290, val loss: 1.2296301126480103
Epoch 300, training loss: 7.460875511169434 = 1.0467960834503174 + 1.0 * 6.414079666137695
Epoch 300, val loss: 1.1928372383117676
Epoch 310, training loss: 7.418046474456787 = 0.9985185861587524 + 1.0 * 6.419528007507324
Epoch 310, val loss: 1.1565228700637817
Epoch 320, training loss: 7.358084201812744 = 0.9516569972038269 + 1.0 * 6.406427383422852
Epoch 320, val loss: 1.1219613552093506
Epoch 330, training loss: 7.308098316192627 = 0.9062217473983765 + 1.0 * 6.401876449584961
Epoch 330, val loss: 1.0887300968170166
Epoch 340, training loss: 7.260201454162598 = 0.8621166944503784 + 1.0 * 6.39808464050293
Epoch 340, val loss: 1.0567662715911865
Epoch 350, training loss: 7.216860294342041 = 0.8199421167373657 + 1.0 * 6.396918296813965
Epoch 350, val loss: 1.0267049074172974
Epoch 360, training loss: 7.176731109619141 = 0.7809461355209351 + 1.0 * 6.395784854888916
Epoch 360, val loss: 0.9995595216751099
Epoch 370, training loss: 7.132339954376221 = 0.7442435622215271 + 1.0 * 6.388096332550049
Epoch 370, val loss: 0.9747548699378967
Epoch 380, training loss: 7.094148635864258 = 0.709614634513855 + 1.0 * 6.384533882141113
Epoch 380, val loss: 0.9520784616470337
Epoch 390, training loss: 7.074052810668945 = 0.6769589185714722 + 1.0 * 6.397093772888184
Epoch 390, val loss: 0.9316771626472473
Epoch 400, training loss: 7.03013801574707 = 0.6468325257301331 + 1.0 * 6.383305549621582
Epoch 400, val loss: 0.9138966798782349
Epoch 410, training loss: 6.996223449707031 = 0.6186497211456299 + 1.0 * 6.3775739669799805
Epoch 410, val loss: 0.8984212279319763
Epoch 420, training loss: 6.967498779296875 = 0.5920818448066711 + 1.0 * 6.3754167556762695
Epoch 420, val loss: 0.8850095868110657
Epoch 430, training loss: 6.94457483291626 = 0.5672352910041809 + 1.0 * 6.3773393630981445
Epoch 430, val loss: 0.8736189603805542
Epoch 440, training loss: 6.913569450378418 = 0.5439110994338989 + 1.0 * 6.369658470153809
Epoch 440, val loss: 0.8642569780349731
Epoch 450, training loss: 6.888482093811035 = 0.521818995475769 + 1.0 * 6.366662979125977
Epoch 450, val loss: 0.8564454317092896
Epoch 460, training loss: 6.8719658851623535 = 0.5009096264839172 + 1.0 * 6.371056079864502
Epoch 460, val loss: 0.8501245379447937
Epoch 470, training loss: 6.844808101654053 = 0.4812068045139313 + 1.0 * 6.363601207733154
Epoch 470, val loss: 0.8453450798988342
Epoch 480, training loss: 6.822685241699219 = 0.4625035524368286 + 1.0 * 6.36018180847168
Epoch 480, val loss: 0.8416769504547119
Epoch 490, training loss: 6.807580947875977 = 0.4445723593235016 + 1.0 * 6.363008499145508
Epoch 490, val loss: 0.8390217423439026
Epoch 500, training loss: 6.787379264831543 = 0.4274551570415497 + 1.0 * 6.35992431640625
Epoch 500, val loss: 0.8373478651046753
Epoch 510, training loss: 6.765761852264404 = 0.41104018688201904 + 1.0 * 6.354721546173096
Epoch 510, val loss: 0.836564302444458
Epoch 520, training loss: 6.748691558837891 = 0.3951359987258911 + 1.0 * 6.353555679321289
Epoch 520, val loss: 0.8363473415374756
Epoch 530, training loss: 6.7420759201049805 = 0.37965112924575806 + 1.0 * 6.362424850463867
Epoch 530, val loss: 0.8367429971694946
Epoch 540, training loss: 6.716699600219727 = 0.3646048903465271 + 1.0 * 6.352094650268555
Epoch 540, val loss: 0.8376739621162415
Epoch 550, training loss: 6.698909759521484 = 0.34991928935050964 + 1.0 * 6.348990440368652
Epoch 550, val loss: 0.8391968607902527
Epoch 560, training loss: 6.683398246765137 = 0.33542829751968384 + 1.0 * 6.347970008850098
Epoch 560, val loss: 0.8409829139709473
Epoch 570, training loss: 6.667064189910889 = 0.32119670510292053 + 1.0 * 6.34586763381958
Epoch 570, val loss: 0.8431772589683533
Epoch 580, training loss: 6.650935649871826 = 0.307221919298172 + 1.0 * 6.343713760375977
Epoch 580, val loss: 0.8458219170570374
Epoch 590, training loss: 6.6478166580200195 = 0.2935250401496887 + 1.0 * 6.3542914390563965
Epoch 590, val loss: 0.8488349318504333
Epoch 600, training loss: 6.623953819274902 = 0.2802017331123352 + 1.0 * 6.343751907348633
Epoch 600, val loss: 0.8523010611534119
Epoch 610, training loss: 6.6083550453186035 = 0.26735755801200867 + 1.0 * 6.340997695922852
Epoch 610, val loss: 0.856153130531311
Epoch 620, training loss: 6.595691680908203 = 0.2549695372581482 + 1.0 * 6.34072208404541
Epoch 620, val loss: 0.8605018854141235
Epoch 630, training loss: 6.579521656036377 = 0.24301131069660187 + 1.0 * 6.336510181427002
Epoch 630, val loss: 0.8652025461196899
Epoch 640, training loss: 6.5680742263793945 = 0.23146086931228638 + 1.0 * 6.336613178253174
Epoch 640, val loss: 0.8704048991203308
Epoch 650, training loss: 6.558138847351074 = 0.22040849924087524 + 1.0 * 6.337730407714844
Epoch 650, val loss: 0.8760140538215637
Epoch 660, training loss: 6.546420574188232 = 0.209917351603508 + 1.0 * 6.336503028869629
Epoch 660, val loss: 0.8820203542709351
Epoch 670, training loss: 6.533068656921387 = 0.19991452991962433 + 1.0 * 6.333154201507568
Epoch 670, val loss: 0.8883952498435974
Epoch 680, training loss: 6.528378963470459 = 0.19034868478775024 + 1.0 * 6.3380303382873535
Epoch 680, val loss: 0.895179271697998
Epoch 690, training loss: 6.513270378112793 = 0.18129956722259521 + 1.0 * 6.331970691680908
Epoch 690, val loss: 0.9022281169891357
Epoch 700, training loss: 6.502906799316406 = 0.17266684770584106 + 1.0 * 6.330239772796631
Epoch 700, val loss: 0.9096917510032654
Epoch 710, training loss: 6.500074863433838 = 0.16448350250720978 + 1.0 * 6.3355913162231445
Epoch 710, val loss: 0.9172624945640564
Epoch 720, training loss: 6.485898017883301 = 0.15673385560512543 + 1.0 * 6.329164028167725
Epoch 720, val loss: 0.9252842664718628
Epoch 730, training loss: 6.475698471069336 = 0.1494085043668747 + 1.0 * 6.326290130615234
Epoch 730, val loss: 0.933481752872467
Epoch 740, training loss: 6.469144821166992 = 0.14244168996810913 + 1.0 * 6.326703071594238
Epoch 740, val loss: 0.941929042339325
Epoch 750, training loss: 6.462525367736816 = 0.13584701716899872 + 1.0 * 6.326678276062012
Epoch 750, val loss: 0.9505407214164734
Epoch 760, training loss: 6.458032131195068 = 0.12964500486850739 + 1.0 * 6.328387260437012
Epoch 760, val loss: 0.9593678712844849
Epoch 770, training loss: 6.444564342498779 = 0.12378410249948502 + 1.0 * 6.320780277252197
Epoch 770, val loss: 0.9683666229248047
Epoch 780, training loss: 6.439906597137451 = 0.11821234971284866 + 1.0 * 6.321694374084473
Epoch 780, val loss: 0.9775554537773132
Epoch 790, training loss: 6.441439628601074 = 0.11293401569128036 + 1.0 * 6.328505516052246
Epoch 790, val loss: 0.9868885278701782
Epoch 800, training loss: 6.426555633544922 = 0.10796105861663818 + 1.0 * 6.318594455718994
Epoch 800, val loss: 0.996306836605072
Epoch 810, training loss: 6.4219865798950195 = 0.10326657444238663 + 1.0 * 6.318719863891602
Epoch 810, val loss: 1.0059055089950562
Epoch 820, training loss: 6.415470123291016 = 0.09879714995622635 + 1.0 * 6.3166728019714355
Epoch 820, val loss: 1.0155607461929321
Epoch 830, training loss: 6.414405345916748 = 0.09454868733882904 + 1.0 * 6.319856643676758
Epoch 830, val loss: 1.025412917137146
Epoch 840, training loss: 6.413469314575195 = 0.09052898734807968 + 1.0 * 6.322940349578857
Epoch 840, val loss: 1.0350967645645142
Epoch 850, training loss: 6.401371955871582 = 0.08675146847963333 + 1.0 * 6.314620494842529
Epoch 850, val loss: 1.0450572967529297
Epoch 860, training loss: 6.397251605987549 = 0.08315432816743851 + 1.0 * 6.3140974044799805
Epoch 860, val loss: 1.0550013780593872
Epoch 870, training loss: 6.39292573928833 = 0.07972634583711624 + 1.0 * 6.313199520111084
Epoch 870, val loss: 1.0650361776351929
Epoch 880, training loss: 6.395330905914307 = 0.07647032290697098 + 1.0 * 6.3188605308532715
Epoch 880, val loss: 1.0750318765640259
Epoch 890, training loss: 6.387062072753906 = 0.07337956130504608 + 1.0 * 6.313682556152344
Epoch 890, val loss: 1.085127592086792
Epoch 900, training loss: 6.390366077423096 = 0.07044415175914764 + 1.0 * 6.319921970367432
Epoch 900, val loss: 1.095162272453308
Epoch 910, training loss: 6.3792595863342285 = 0.06765399873256683 + 1.0 * 6.311605453491211
Epoch 910, val loss: 1.1052087545394897
Epoch 920, training loss: 6.375273704528809 = 0.06500324606895447 + 1.0 * 6.310270309448242
Epoch 920, val loss: 1.115256905555725
Epoch 930, training loss: 6.371583461761475 = 0.06248169392347336 + 1.0 * 6.309101581573486
Epoch 930, val loss: 1.125248908996582
Epoch 940, training loss: 6.368615627288818 = 0.06008803844451904 + 1.0 * 6.30852746963501
Epoch 940, val loss: 1.1353070735931396
Epoch 950, training loss: 6.371901035308838 = 0.057807859033346176 + 1.0 * 6.314093112945557
Epoch 950, val loss: 1.14519464969635
Epoch 960, training loss: 6.363347053527832 = 0.0556405745446682 + 1.0 * 6.307706356048584
Epoch 960, val loss: 1.1551741361618042
Epoch 970, training loss: 6.3579325675964355 = 0.05357404798269272 + 1.0 * 6.30435848236084
Epoch 970, val loss: 1.1651300191879272
Epoch 980, training loss: 6.362229347229004 = 0.051599208265542984 + 1.0 * 6.3106303215026855
Epoch 980, val loss: 1.174999713897705
Epoch 990, training loss: 6.3581223487854 = 0.0497249960899353 + 1.0 * 6.30839729309082
Epoch 990, val loss: 1.184756875038147
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 10.546212196350098 = 1.9493920803070068 + 1.0 * 8.596819877624512
Epoch 0, val loss: 1.952250361442566
Epoch 10, training loss: 10.535782814025879 = 1.9392949342727661 + 1.0 * 8.596487998962402
Epoch 10, val loss: 1.9425774812698364
Epoch 20, training loss: 10.520544052124023 = 1.9267363548278809 + 1.0 * 8.593807220458984
Epoch 20, val loss: 1.9301939010620117
Epoch 30, training loss: 10.48320198059082 = 1.9094529151916504 + 1.0 * 8.573749542236328
Epoch 30, val loss: 1.9128859043121338
Epoch 40, training loss: 10.354647636413574 = 1.8873322010040283 + 1.0 * 8.467315673828125
Epoch 40, val loss: 1.8915878534317017
Epoch 50, training loss: 10.0416259765625 = 1.8642257452011108 + 1.0 * 8.177400588989258
Epoch 50, val loss: 1.8706625699996948
Epoch 60, training loss: 9.783157348632812 = 1.8436291217803955 + 1.0 * 7.939527988433838
Epoch 60, val loss: 1.8528344631195068
Epoch 70, training loss: 9.249200820922852 = 1.828500747680664 + 1.0 * 7.4207000732421875
Epoch 70, val loss: 1.8388773202896118
Epoch 80, training loss: 8.873912811279297 = 1.8180615901947021 + 1.0 * 7.055851459503174
Epoch 80, val loss: 1.828629493713379
Epoch 90, training loss: 8.727416038513184 = 1.8060840368270874 + 1.0 * 6.921331882476807
Epoch 90, val loss: 1.8168715238571167
Epoch 100, training loss: 8.597216606140137 = 1.7917815446853638 + 1.0 * 6.8054351806640625
Epoch 100, val loss: 1.8039470911026
Epoch 110, training loss: 8.501322746276855 = 1.7790144681930542 + 1.0 * 6.72230863571167
Epoch 110, val loss: 1.7921392917633057
Epoch 120, training loss: 8.428950309753418 = 1.7663087844848633 + 1.0 * 6.662641525268555
Epoch 120, val loss: 1.779889702796936
Epoch 130, training loss: 8.371663093566895 = 1.75216805934906 + 1.0 * 6.619495391845703
Epoch 130, val loss: 1.7663872241973877
Epoch 140, training loss: 8.324665069580078 = 1.7360854148864746 + 1.0 * 6.588580131530762
Epoch 140, val loss: 1.751546025276184
Epoch 150, training loss: 8.279623985290527 = 1.717588186264038 + 1.0 * 6.56203556060791
Epoch 150, val loss: 1.7352105379104614
Epoch 160, training loss: 8.23779010772705 = 1.6961970329284668 + 1.0 * 6.541593074798584
Epoch 160, val loss: 1.7168077230453491
Epoch 170, training loss: 8.194732666015625 = 1.6713263988494873 + 1.0 * 6.523406505584717
Epoch 170, val loss: 1.695773720741272
Epoch 180, training loss: 8.150572776794434 = 1.6423989534378052 + 1.0 * 6.508173942565918
Epoch 180, val loss: 1.6713082790374756
Epoch 190, training loss: 8.105671882629395 = 1.6090060472488403 + 1.0 * 6.4966654777526855
Epoch 190, val loss: 1.6432323455810547
Epoch 200, training loss: 8.054130554199219 = 1.5711231231689453 + 1.0 * 6.483007431030273
Epoch 200, val loss: 1.6114827394485474
Epoch 210, training loss: 8.001434326171875 = 1.5285866260528564 + 1.0 * 6.472847938537598
Epoch 210, val loss: 1.5758187770843506
Epoch 220, training loss: 7.952077865600586 = 1.4817883968353271 + 1.0 * 6.47028923034668
Epoch 220, val loss: 1.5366125106811523
Epoch 230, training loss: 7.88969612121582 = 1.432340383529663 + 1.0 * 6.457355499267578
Epoch 230, val loss: 1.4954522848129272
Epoch 240, training loss: 7.829021453857422 = 1.3810096979141235 + 1.0 * 6.448011875152588
Epoch 240, val loss: 1.4529025554656982
Epoch 250, training loss: 7.77455997467041 = 1.3286819458007812 + 1.0 * 6.445878028869629
Epoch 250, val loss: 1.409767508506775
Epoch 260, training loss: 7.714166164398193 = 1.277649998664856 + 1.0 * 6.436516284942627
Epoch 260, val loss: 1.368127465248108
Epoch 270, training loss: 7.657844543457031 = 1.228654146194458 + 1.0 * 6.429190635681152
Epoch 270, val loss: 1.3286648988723755
Epoch 280, training loss: 7.605916500091553 = 1.1817936897277832 + 1.0 * 6.4241228103637695
Epoch 280, val loss: 1.291441559791565
Epoch 290, training loss: 7.556823253631592 = 1.1380534172058105 + 1.0 * 6.418769836425781
Epoch 290, val loss: 1.257053017616272
Epoch 300, training loss: 7.511654853820801 = 1.0974690914154053 + 1.0 * 6.414185523986816
Epoch 300, val loss: 1.2258044481277466
Epoch 310, training loss: 7.467801570892334 = 1.059275507926941 + 1.0 * 6.4085259437561035
Epoch 310, val loss: 1.1969757080078125
Epoch 320, training loss: 7.4270710945129395 = 1.0229696035385132 + 1.0 * 6.404101371765137
Epoch 320, val loss: 1.1699916124343872
Epoch 330, training loss: 7.3888068199157715 = 0.9884087443351746 + 1.0 * 6.400398254394531
Epoch 330, val loss: 1.1445708274841309
Epoch 340, training loss: 7.350674629211426 = 0.9550313949584961 + 1.0 * 6.39564323425293
Epoch 340, val loss: 1.1204577684402466
Epoch 350, training loss: 7.315054893493652 = 0.9222257733345032 + 1.0 * 6.392828941345215
Epoch 350, val loss: 1.0969362258911133
Epoch 360, training loss: 7.2784624099731445 = 0.8895310759544373 + 1.0 * 6.3889312744140625
Epoch 360, val loss: 1.0736162662506104
Epoch 370, training loss: 7.244363784790039 = 0.8563082218170166 + 1.0 * 6.388055801391602
Epoch 370, val loss: 1.0501244068145752
Epoch 380, training loss: 7.206965923309326 = 0.8226895928382874 + 1.0 * 6.384276390075684
Epoch 380, val loss: 1.02619469165802
Epoch 390, training loss: 7.170924186706543 = 0.7885109782218933 + 1.0 * 6.382413387298584
Epoch 390, val loss: 1.0023211240768433
Epoch 400, training loss: 7.131371021270752 = 0.7542896866798401 + 1.0 * 6.377081394195557
Epoch 400, val loss: 0.9786188006401062
Epoch 410, training loss: 7.094027996063232 = 0.7202053070068359 + 1.0 * 6.3738226890563965
Epoch 410, val loss: 0.9555618762969971
Epoch 420, training loss: 7.060954570770264 = 0.6867253184318542 + 1.0 * 6.374229431152344
Epoch 420, val loss: 0.9335653185844421
Epoch 430, training loss: 7.025968074798584 = 0.6544867753982544 + 1.0 * 6.371481418609619
Epoch 430, val loss: 0.913314163684845
Epoch 440, training loss: 6.98988151550293 = 0.6237308979034424 + 1.0 * 6.366150379180908
Epoch 440, val loss: 0.8949747681617737
Epoch 450, training loss: 6.961395263671875 = 0.5943892598152161 + 1.0 * 6.367005825042725
Epoch 450, val loss: 0.878568172454834
Epoch 460, training loss: 6.932394504547119 = 0.5666236877441406 + 1.0 * 6.3657708168029785
Epoch 460, val loss: 0.8642372488975525
Epoch 470, training loss: 6.899921417236328 = 0.5404679775238037 + 1.0 * 6.3594536781311035
Epoch 470, val loss: 0.8521440625190735
Epoch 480, training loss: 6.878460884094238 = 0.5156572461128235 + 1.0 * 6.3628034591674805
Epoch 480, val loss: 0.8418575525283813
Epoch 490, training loss: 6.849212169647217 = 0.492147833108902 + 1.0 * 6.357064247131348
Epoch 490, val loss: 0.8333478569984436
Epoch 500, training loss: 6.823795318603516 = 0.4698358178138733 + 1.0 * 6.353959560394287
Epoch 500, val loss: 0.8265134692192078
Epoch 510, training loss: 6.804014682769775 = 0.448471337556839 + 1.0 * 6.35554313659668
Epoch 510, val loss: 0.8209347128868103
Epoch 520, training loss: 6.778512001037598 = 0.4279906153678894 + 1.0 * 6.350521564483643
Epoch 520, val loss: 0.8166570067405701
Epoch 530, training loss: 6.756550312042236 = 0.4082474410533905 + 1.0 * 6.348302841186523
Epoch 530, val loss: 0.8134244680404663
Epoch 540, training loss: 6.741682529449463 = 0.3891009986400604 + 1.0 * 6.35258150100708
Epoch 540, val loss: 0.8109728693962097
Epoch 550, training loss: 6.722648620605469 = 0.37061530351638794 + 1.0 * 6.3520331382751465
Epoch 550, val loss: 0.8094267845153809
Epoch 560, training loss: 6.698221206665039 = 0.3527427315711975 + 1.0 * 6.345478534698486
Epoch 560, val loss: 0.808714747428894
Epoch 570, training loss: 6.67616081237793 = 0.33545398712158203 + 1.0 * 6.340706825256348
Epoch 570, val loss: 0.808722972869873
Epoch 580, training loss: 6.66014289855957 = 0.3186766803264618 + 1.0 * 6.341466426849365
Epoch 580, val loss: 0.80938321352005
Epoch 590, training loss: 6.641159534454346 = 0.3024858832359314 + 1.0 * 6.3386735916137695
Epoch 590, val loss: 0.8106426000595093
Epoch 600, training loss: 6.625204563140869 = 0.28698381781578064 + 1.0 * 6.338220596313477
Epoch 600, val loss: 0.8125749230384827
Epoch 610, training loss: 6.608072757720947 = 0.2722371816635132 + 1.0 * 6.3358354568481445
Epoch 610, val loss: 0.8151939511299133
Epoch 620, training loss: 6.592798233032227 = 0.25815802812576294 + 1.0 * 6.334640026092529
Epoch 620, val loss: 0.8184149265289307
Epoch 630, training loss: 6.580384254455566 = 0.2447204887866974 + 1.0 * 6.335663795471191
Epoch 630, val loss: 0.8220685720443726
Epoch 640, training loss: 6.562433242797852 = 0.2319749891757965 + 1.0 * 6.330458164215088
Epoch 640, val loss: 0.8263109922409058
Epoch 650, training loss: 6.549592018127441 = 0.21985694766044617 + 1.0 * 6.329735279083252
Epoch 650, val loss: 0.8310136198997498
Epoch 660, training loss: 6.539461612701416 = 0.2083682268857956 + 1.0 * 6.3310933113098145
Epoch 660, val loss: 0.8358542323112488
Epoch 670, training loss: 6.527521133422852 = 0.19755785167217255 + 1.0 * 6.329963207244873
Epoch 670, val loss: 0.8413387537002563
Epoch 680, training loss: 6.5125226974487305 = 0.18729308247566223 + 1.0 * 6.325229644775391
Epoch 680, val loss: 0.8470375537872314
Epoch 690, training loss: 6.503422737121582 = 0.17755980789661407 + 1.0 * 6.325862884521484
Epoch 690, val loss: 0.8530677556991577
Epoch 700, training loss: 6.496845245361328 = 0.1683904081583023 + 1.0 * 6.328454971313477
Epoch 700, val loss: 0.8593989610671997
Epoch 710, training loss: 6.483222007751465 = 0.15975327789783478 + 1.0 * 6.3234686851501465
Epoch 710, val loss: 0.8660457730293274
Epoch 720, training loss: 6.472349166870117 = 0.1515895128250122 + 1.0 * 6.3207597732543945
Epoch 720, val loss: 0.8729684948921204
Epoch 730, training loss: 6.4635114669799805 = 0.14385974407196045 + 1.0 * 6.3196516036987305
Epoch 730, val loss: 0.880165696144104
Epoch 740, training loss: 6.460235595703125 = 0.13654044270515442 + 1.0 * 6.323695182800293
Epoch 740, val loss: 0.8875871300697327
Epoch 750, training loss: 6.454395294189453 = 0.12963591516017914 + 1.0 * 6.324759483337402
Epoch 750, val loss: 0.895112931728363
Epoch 760, training loss: 6.442746162414551 = 0.12314651906490326 + 1.0 * 6.319599628448486
Epoch 760, val loss: 0.9028457403182983
Epoch 770, training loss: 6.433003902435303 = 0.11702192574739456 + 1.0 * 6.315981864929199
Epoch 770, val loss: 0.9107914566993713
Epoch 780, training loss: 6.4325737953186035 = 0.1112460270524025 + 1.0 * 6.3213276863098145
Epoch 780, val loss: 0.9188886880874634
Epoch 790, training loss: 6.422580242156982 = 0.10579504072666168 + 1.0 * 6.3167853355407715
Epoch 790, val loss: 0.9269875288009644
Epoch 800, training loss: 6.414337635040283 = 0.10068313777446747 + 1.0 * 6.31365442276001
Epoch 800, val loss: 0.9353444576263428
Epoch 810, training loss: 6.409389972686768 = 0.09585017710924149 + 1.0 * 6.313539981842041
Epoch 810, val loss: 0.9436919093132019
Epoch 820, training loss: 6.404354572296143 = 0.09130507707595825 + 1.0 * 6.31304931640625
Epoch 820, val loss: 0.9520770311355591
Epoch 830, training loss: 6.39631462097168 = 0.0870257169008255 + 1.0 * 6.30928897857666
Epoch 830, val loss: 0.9605767130851746
Epoch 840, training loss: 6.3929643630981445 = 0.08298525959253311 + 1.0 * 6.30997896194458
Epoch 840, val loss: 0.969118595123291
Epoch 850, training loss: 6.394071578979492 = 0.07918746769428253 + 1.0 * 6.314884185791016
Epoch 850, val loss: 0.9776052832603455
Epoch 860, training loss: 6.3834004402160645 = 0.07562624663114548 + 1.0 * 6.307774066925049
Epoch 860, val loss: 0.9861555099487305
Epoch 870, training loss: 6.378388404846191 = 0.07226990163326263 + 1.0 * 6.306118488311768
Epoch 870, val loss: 0.9947081208229065
Epoch 880, training loss: 6.375705718994141 = 0.06908699870109558 + 1.0 * 6.306618690490723
Epoch 880, val loss: 1.0032188892364502
Epoch 890, training loss: 6.37086820602417 = 0.06609153747558594 + 1.0 * 6.304776668548584
Epoch 890, val loss: 1.0117523670196533
Epoch 900, training loss: 6.372165203094482 = 0.06327147036790848 + 1.0 * 6.30889368057251
Epoch 900, val loss: 1.020310640335083
Epoch 910, training loss: 6.366853713989258 = 0.060613926500082016 + 1.0 * 6.306239604949951
Epoch 910, val loss: 1.028626561164856
Epoch 920, training loss: 6.360778331756592 = 0.05811391398310661 + 1.0 * 6.302664279937744
Epoch 920, val loss: 1.0371110439300537
Epoch 930, training loss: 6.356260776519775 = 0.055748697370290756 + 1.0 * 6.300512313842773
Epoch 930, val loss: 1.045488715171814
Epoch 940, training loss: 6.353847503662109 = 0.05350499227643013 + 1.0 * 6.300342559814453
Epoch 940, val loss: 1.0538722276687622
Epoch 950, training loss: 6.35577917098999 = 0.05138242244720459 + 1.0 * 6.304396629333496
Epoch 950, val loss: 1.0621041059494019
Epoch 960, training loss: 6.353713512420654 = 0.04937227815389633 + 1.0 * 6.3043413162231445
Epoch 960, val loss: 1.0702723264694214
Epoch 970, training loss: 6.346800327301025 = 0.04748532176017761 + 1.0 * 6.299314975738525
Epoch 970, val loss: 1.0783450603485107
Epoch 980, training loss: 6.342676639556885 = 0.0456978902220726 + 1.0 * 6.296978950500488
Epoch 980, val loss: 1.0864619016647339
Epoch 990, training loss: 6.339926719665527 = 0.04400006681680679 + 1.0 * 6.295926570892334
Epoch 990, val loss: 1.0944758653640747
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 10.52762222290039 = 1.9308273792266846 + 1.0 * 8.596795082092285
Epoch 0, val loss: 1.9315803050994873
Epoch 10, training loss: 10.51762866973877 = 1.9213000535964966 + 1.0 * 8.596328735351562
Epoch 10, val loss: 1.9222429990768433
Epoch 20, training loss: 10.501605987548828 = 1.909338355064392 + 1.0 * 8.592267990112305
Epoch 20, val loss: 1.9100192785263062
Epoch 30, training loss: 10.456657409667969 = 1.8929188251495361 + 1.0 * 8.563738822937012
Epoch 30, val loss: 1.8930801153182983
Epoch 40, training loss: 10.282427787780762 = 1.87326979637146 + 1.0 * 8.409157752990723
Epoch 40, val loss: 1.8738877773284912
Epoch 50, training loss: 9.918766021728516 = 1.8516366481781006 + 1.0 * 8.067129135131836
Epoch 50, val loss: 1.8532342910766602
Epoch 60, training loss: 9.4776611328125 = 1.8338894844055176 + 1.0 * 7.643772125244141
Epoch 60, val loss: 1.8378188610076904
Epoch 70, training loss: 9.021193504333496 = 1.82184636592865 + 1.0 * 7.199347019195557
Epoch 70, val loss: 1.8267011642456055
Epoch 80, training loss: 8.830031394958496 = 1.810229778289795 + 1.0 * 7.019801616668701
Epoch 80, val loss: 1.8154966831207275
Epoch 90, training loss: 8.699158668518066 = 1.7957658767700195 + 1.0 * 6.903392791748047
Epoch 90, val loss: 1.8026469945907593
Epoch 100, training loss: 8.588939666748047 = 1.782286524772644 + 1.0 * 6.8066534996032715
Epoch 100, val loss: 1.7912347316741943
Epoch 110, training loss: 8.511167526245117 = 1.7698777914047241 + 1.0 * 6.741290092468262
Epoch 110, val loss: 1.7800990343093872
Epoch 120, training loss: 8.44128131866455 = 1.7561630010604858 + 1.0 * 6.685118198394775
Epoch 120, val loss: 1.7674272060394287
Epoch 130, training loss: 8.383951187133789 = 1.7402538061141968 + 1.0 * 6.643697261810303
Epoch 130, val loss: 1.753176212310791
Epoch 140, training loss: 8.331592559814453 = 1.7219051122665405 + 1.0 * 6.609687805175781
Epoch 140, val loss: 1.7373346090316772
Epoch 150, training loss: 8.279725074768066 = 1.7006553411483765 + 1.0 * 6.5790696144104
Epoch 150, val loss: 1.719300627708435
Epoch 160, training loss: 8.23511028289795 = 1.675687551498413 + 1.0 * 6.559422492980957
Epoch 160, val loss: 1.6983575820922852
Epoch 170, training loss: 8.18147087097168 = 1.6471294164657593 + 1.0 * 6.534341812133789
Epoch 170, val loss: 1.6741806268692017
Epoch 180, training loss: 8.130033493041992 = 1.6141774654388428 + 1.0 * 6.51585578918457
Epoch 180, val loss: 1.6462708711624146
Epoch 190, training loss: 8.07732105255127 = 1.576141357421875 + 1.0 * 6.5011796951293945
Epoch 190, val loss: 1.614058017730713
Epoch 200, training loss: 8.022814750671387 = 1.5327485799789429 + 1.0 * 6.490066051483154
Epoch 200, val loss: 1.57745361328125
Epoch 210, training loss: 7.963724136352539 = 1.4851106405258179 + 1.0 * 6.478613376617432
Epoch 210, val loss: 1.5376567840576172
Epoch 220, training loss: 7.904300689697266 = 1.4341546297073364 + 1.0 * 6.470146179199219
Epoch 220, val loss: 1.4953266382217407
Epoch 230, training loss: 7.841736793518066 = 1.3806867599487305 + 1.0 * 6.461050033569336
Epoch 230, val loss: 1.4513912200927734
Epoch 240, training loss: 7.779229164123535 = 1.3257368803024292 + 1.0 * 6.453492164611816
Epoch 240, val loss: 1.406777262687683
Epoch 250, training loss: 7.7259297370910645 = 1.2711849212646484 + 1.0 * 6.454744815826416
Epoch 250, val loss: 1.3634113073349
Epoch 260, training loss: 7.661960124969482 = 1.2189525365829468 + 1.0 * 6.443007469177246
Epoch 260, val loss: 1.3223769664764404
Epoch 270, training loss: 7.604101181030273 = 1.1682839393615723 + 1.0 * 6.435817241668701
Epoch 270, val loss: 1.283687949180603
Epoch 280, training loss: 7.548950672149658 = 1.1193361282348633 + 1.0 * 6.429614543914795
Epoch 280, val loss: 1.2469686269760132
Epoch 290, training loss: 7.500293254852295 = 1.0724220275878906 + 1.0 * 6.427871227264404
Epoch 290, val loss: 1.2126432657241821
Epoch 300, training loss: 7.449540138244629 = 1.0280895233154297 + 1.0 * 6.421450614929199
Epoch 300, val loss: 1.181016445159912
Epoch 310, training loss: 7.401090621948242 = 0.9856027960777283 + 1.0 * 6.415487766265869
Epoch 310, val loss: 1.151215672492981
Epoch 320, training loss: 7.355254650115967 = 0.9444957971572876 + 1.0 * 6.410758972167969
Epoch 320, val loss: 1.1228702068328857
Epoch 330, training loss: 7.3191094398498535 = 0.9046074748039246 + 1.0 * 6.414502143859863
Epoch 330, val loss: 1.0958563089370728
Epoch 340, training loss: 7.273331642150879 = 0.8663228750228882 + 1.0 * 6.407008647918701
Epoch 340, val loss: 1.0702686309814453
Epoch 350, training loss: 7.229066371917725 = 0.8295370936393738 + 1.0 * 6.399529457092285
Epoch 350, val loss: 1.0459858179092407
Epoch 360, training loss: 7.189203262329102 = 0.7938876152038574 + 1.0 * 6.395315647125244
Epoch 360, val loss: 1.0228042602539062
Epoch 370, training loss: 7.161299705505371 = 0.759285032749176 + 1.0 * 6.40201473236084
Epoch 370, val loss: 1.000675082206726
Epoch 380, training loss: 7.1169633865356445 = 0.7258739471435547 + 1.0 * 6.39108943939209
Epoch 380, val loss: 0.9798557758331299
Epoch 390, training loss: 7.079033851623535 = 0.6934071779251099 + 1.0 * 6.385626792907715
Epoch 390, val loss: 0.9600910544395447
Epoch 400, training loss: 7.04722785949707 = 0.6617050170898438 + 1.0 * 6.385522842407227
Epoch 400, val loss: 0.9412345886230469
Epoch 410, training loss: 7.012531280517578 = 0.6310684680938721 + 1.0 * 6.381463050842285
Epoch 410, val loss: 0.9236671924591064
Epoch 420, training loss: 6.977787971496582 = 0.6014622449874878 + 1.0 * 6.376325607299805
Epoch 420, val loss: 0.9073511958122253
Epoch 430, training loss: 6.949029922485352 = 0.5725905299186707 + 1.0 * 6.376439571380615
Epoch 430, val loss: 0.8921851515769958
Epoch 440, training loss: 6.918858051300049 = 0.544622004032135 + 1.0 * 6.374236106872559
Epoch 440, val loss: 0.8782134652137756
Epoch 450, training loss: 6.890790939331055 = 0.5174331665039062 + 1.0 * 6.373357772827148
Epoch 450, val loss: 0.8654604554176331
Epoch 460, training loss: 6.859739303588867 = 0.4910822808742523 + 1.0 * 6.368657112121582
Epoch 460, val loss: 0.8540139198303223
Epoch 470, training loss: 6.829217910766602 = 0.4654395580291748 + 1.0 * 6.363778591156006
Epoch 470, val loss: 0.8437934517860413
Epoch 480, training loss: 6.809028148651123 = 0.4404936730861664 + 1.0 * 6.368534564971924
Epoch 480, val loss: 0.8347155451774597
Epoch 490, training loss: 6.782331466674805 = 0.4164707064628601 + 1.0 * 6.365860939025879
Epoch 490, val loss: 0.8269796967506409
Epoch 500, training loss: 6.751453399658203 = 0.3934546411037445 + 1.0 * 6.357998847961426
Epoch 500, val loss: 0.8205605149269104
Epoch 510, training loss: 6.725788593292236 = 0.371311753988266 + 1.0 * 6.3544769287109375
Epoch 510, val loss: 0.8152576684951782
Epoch 520, training loss: 6.703071594238281 = 0.3499973714351654 + 1.0 * 6.353074073791504
Epoch 520, val loss: 0.811191737651825
Epoch 530, training loss: 6.682132244110107 = 0.3296395242214203 + 1.0 * 6.352492809295654
Epoch 530, val loss: 0.8083917498588562
Epoch 540, training loss: 6.662197113037109 = 0.3104078471660614 + 1.0 * 6.351789474487305
Epoch 540, val loss: 0.8067172765731812
Epoch 550, training loss: 6.64179801940918 = 0.2923365831375122 + 1.0 * 6.349461555480957
Epoch 550, val loss: 0.806033730506897
Epoch 560, training loss: 6.621035099029541 = 0.2753623425960541 + 1.0 * 6.345672607421875
Epoch 560, val loss: 0.8064412474632263
Epoch 570, training loss: 6.602701663970947 = 0.25938189029693604 + 1.0 * 6.343319892883301
Epoch 570, val loss: 0.8077165484428406
Epoch 580, training loss: 6.5854811668396 = 0.24430575966835022 + 1.0 * 6.341175556182861
Epoch 580, val loss: 0.8098015189170837
Epoch 590, training loss: 6.586755275726318 = 0.2301391363143921 + 1.0 * 6.356616020202637
Epoch 590, val loss: 0.8126422762870789
Epoch 600, training loss: 6.558844089508057 = 0.2169431447982788 + 1.0 * 6.341900825500488
Epoch 600, val loss: 0.8160805702209473
Epoch 610, training loss: 6.542651653289795 = 0.2046271413564682 + 1.0 * 6.338024616241455
Epoch 610, val loss: 0.8201147317886353
Epoch 620, training loss: 6.533581256866455 = 0.19305938482284546 + 1.0 * 6.340521812438965
Epoch 620, val loss: 0.8246225714683533
Epoch 630, training loss: 6.52136754989624 = 0.18227837979793549 + 1.0 * 6.339089393615723
Epoch 630, val loss: 0.8297128081321716
Epoch 640, training loss: 6.5050482749938965 = 0.17219863831996918 + 1.0 * 6.332849502563477
Epoch 640, val loss: 0.8351401090621948
Epoch 650, training loss: 6.498121738433838 = 0.16278032958507538 + 1.0 * 6.335341453552246
Epoch 650, val loss: 0.8410249352455139
Epoch 660, training loss: 6.484975337982178 = 0.15398341417312622 + 1.0 * 6.330991744995117
Epoch 660, val loss: 0.8472849726676941
Epoch 670, training loss: 6.474122524261475 = 0.14578336477279663 + 1.0 * 6.328339099884033
Epoch 670, val loss: 0.8538166284561157
Epoch 680, training loss: 6.46353006362915 = 0.13809552788734436 + 1.0 * 6.325434684753418
Epoch 680, val loss: 0.8606724143028259
Epoch 690, training loss: 6.462241172790527 = 0.13088084757328033 + 1.0 * 6.331360340118408
Epoch 690, val loss: 0.8678287863731384
Epoch 700, training loss: 6.450672626495361 = 0.12417419254779816 + 1.0 * 6.326498508453369
Epoch 700, val loss: 0.8749865293502808
Epoch 710, training loss: 6.441379547119141 = 0.11793436855077744 + 1.0 * 6.3234453201293945
Epoch 710, val loss: 0.8825103044509888
Epoch 720, training loss: 6.432318687438965 = 0.11206786334514618 + 1.0 * 6.320250988006592
Epoch 720, val loss: 0.8901187777519226
Epoch 730, training loss: 6.426309585571289 = 0.10654138773679733 + 1.0 * 6.31976842880249
Epoch 730, val loss: 0.8980032801628113
Epoch 740, training loss: 6.42260217666626 = 0.1013491302728653 + 1.0 * 6.321252822875977
Epoch 740, val loss: 0.9058488607406616
Epoch 750, training loss: 6.414012432098389 = 0.09649867564439774 + 1.0 * 6.317513942718506
Epoch 750, val loss: 0.9138839244842529
Epoch 760, training loss: 6.408024787902832 = 0.0919288694858551 + 1.0 * 6.31609582901001
Epoch 760, val loss: 0.9219128489494324
Epoch 770, training loss: 6.41077184677124 = 0.0876137763261795 + 1.0 * 6.323158264160156
Epoch 770, val loss: 0.9300030469894409
Epoch 780, training loss: 6.405991554260254 = 0.08357615023851395 + 1.0 * 6.322415351867676
Epoch 780, val loss: 0.9381372332572937
Epoch 790, training loss: 6.3929595947265625 = 0.07979100942611694 + 1.0 * 6.313168525695801
Epoch 790, val loss: 0.9462306499481201
Epoch 800, training loss: 6.386702060699463 = 0.07620711624622345 + 1.0 * 6.310494899749756
Epoch 800, val loss: 0.9543625712394714
Epoch 810, training loss: 6.382629871368408 = 0.07281339913606644 + 1.0 * 6.309816360473633
Epoch 810, val loss: 0.9625439643859863
Epoch 820, training loss: 6.389396667480469 = 0.06959613412618637 + 1.0 * 6.31980037689209
Epoch 820, val loss: 0.9705312252044678
Epoch 830, training loss: 6.378629207611084 = 0.06658073514699936 + 1.0 * 6.312048435211182
Epoch 830, val loss: 0.9787712693214417
Epoch 840, training loss: 6.379457950592041 = 0.06371840089559555 + 1.0 * 6.315739631652832
Epoch 840, val loss: 0.9867697358131409
Epoch 850, training loss: 6.369727611541748 = 0.06102427467703819 + 1.0 * 6.308703422546387
Epoch 850, val loss: 0.9944794774055481
Epoch 860, training loss: 6.363744735717773 = 0.058481551706790924 + 1.0 * 6.305263042449951
Epoch 860, val loss: 1.0024945735931396
Epoch 870, training loss: 6.360318183898926 = 0.05606852471828461 + 1.0 * 6.3042497634887695
Epoch 870, val loss: 1.0102770328521729
Epoch 880, training loss: 6.360811710357666 = 0.05378900095820427 + 1.0 * 6.307022571563721
Epoch 880, val loss: 1.0179089307785034
Epoch 890, training loss: 6.354091644287109 = 0.05163983628153801 + 1.0 * 6.3024516105651855
Epoch 890, val loss: 1.0256452560424805
Epoch 900, training loss: 6.350813865661621 = 0.04960169643163681 + 1.0 * 6.301212310791016
Epoch 900, val loss: 1.0331531763076782
Epoch 910, training loss: 6.353421688079834 = 0.047667019069194794 + 1.0 * 6.305754661560059
Epoch 910, val loss: 1.0405806303024292
Epoch 920, training loss: 6.3486762046813965 = 0.04584037512540817 + 1.0 * 6.302835941314697
Epoch 920, val loss: 1.047861099243164
Epoch 930, training loss: 6.349461555480957 = 0.044112395495176315 + 1.0 * 6.305349349975586
Epoch 930, val loss: 1.0551435947418213
Epoch 940, training loss: 6.341790676116943 = 0.042472586035728455 + 1.0 * 6.299318313598633
Epoch 940, val loss: 1.0620429515838623
Epoch 950, training loss: 6.338533878326416 = 0.04092315956950188 + 1.0 * 6.297610759735107
Epoch 950, val loss: 1.069193720817566
Epoch 960, training loss: 6.336362838745117 = 0.03944411501288414 + 1.0 * 6.296918869018555
Epoch 960, val loss: 1.0759855508804321
Epoch 970, training loss: 6.3403449058532715 = 0.03803912177681923 + 1.0 * 6.302305698394775
Epoch 970, val loss: 1.0826622247695923
Epoch 980, training loss: 6.3318190574646 = 0.03671109303832054 + 1.0 * 6.295107841491699
Epoch 980, val loss: 1.0894176959991455
Epoch 990, training loss: 6.328333377838135 = 0.03544480353593826 + 1.0 * 6.292888641357422
Epoch 990, val loss: 1.09608793258667
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8149710068529257
The final CL Acc:0.75309, 0.01062, The final GNN Acc:0.81515, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13082])
remove edge: torch.Size([2, 7918])
updated graph: torch.Size([2, 10444])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.5633544921875 = 1.9665117263793945 + 1.0 * 8.596842765808105
Epoch 0, val loss: 1.9657723903656006
Epoch 10, training loss: 10.552083969116211 = 1.955567479133606 + 1.0 * 8.596516609191895
Epoch 10, val loss: 1.9548226594924927
Epoch 20, training loss: 10.53561019897461 = 1.941792368888855 + 1.0 * 8.593817710876465
Epoch 20, val loss: 1.940489411354065
Epoch 30, training loss: 10.493335723876953 = 1.9225677251815796 + 1.0 * 8.570768356323242
Epoch 30, val loss: 1.9201940298080444
Epoch 40, training loss: 10.292889595031738 = 1.8975974321365356 + 1.0 * 8.395292282104492
Epoch 40, val loss: 1.8948330879211426
Epoch 50, training loss: 9.780961990356445 = 1.8708319664001465 + 1.0 * 7.910130023956299
Epoch 50, val loss: 1.868999719619751
Epoch 60, training loss: 9.26289176940918 = 1.852908730506897 + 1.0 * 7.409983158111572
Epoch 60, val loss: 1.852669358253479
Epoch 70, training loss: 8.913899421691895 = 1.8391114473342896 + 1.0 * 7.074787616729736
Epoch 70, val loss: 1.8398494720458984
Epoch 80, training loss: 8.762341499328613 = 1.8247228860855103 + 1.0 * 6.937618732452393
Epoch 80, val loss: 1.8259426355361938
Epoch 90, training loss: 8.640420913696289 = 1.8067348003387451 + 1.0 * 6.833685874938965
Epoch 90, val loss: 1.8091822862625122
Epoch 100, training loss: 8.54762077331543 = 1.7892429828643799 + 1.0 * 6.758378028869629
Epoch 100, val loss: 1.7936222553253174
Epoch 110, training loss: 8.475982666015625 = 1.7737102508544922 + 1.0 * 6.702272415161133
Epoch 110, val loss: 1.779888391494751
Epoch 120, training loss: 8.417540550231934 = 1.758547067642212 + 1.0 * 6.658993244171143
Epoch 120, val loss: 1.7662549018859863
Epoch 130, training loss: 8.359393119812012 = 1.7426270246505737 + 1.0 * 6.616765975952148
Epoch 130, val loss: 1.7520477771759033
Epoch 140, training loss: 8.309847831726074 = 1.7250639200210571 + 1.0 * 6.584783554077148
Epoch 140, val loss: 1.7365295886993408
Epoch 150, training loss: 8.26519775390625 = 1.704970359802246 + 1.0 * 6.560227394104004
Epoch 150, val loss: 1.7189823389053345
Epoch 160, training loss: 8.222670555114746 = 1.6819729804992676 + 1.0 * 6.5406975746154785
Epoch 160, val loss: 1.6988298892974854
Epoch 170, training loss: 8.177873611450195 = 1.655300498008728 + 1.0 * 6.522573471069336
Epoch 170, val loss: 1.6755266189575195
Epoch 180, training loss: 8.134298324584961 = 1.6241841316223145 + 1.0 * 6.510114669799805
Epoch 180, val loss: 1.6484514474868774
Epoch 190, training loss: 8.081783294677734 = 1.5885646343231201 + 1.0 * 6.493218898773193
Epoch 190, val loss: 1.6176210641860962
Epoch 200, training loss: 8.026329040527344 = 1.5478944778442383 + 1.0 * 6.4784345626831055
Epoch 200, val loss: 1.5827573537826538
Epoch 210, training loss: 7.969452857971191 = 1.502469778060913 + 1.0 * 6.466983318328857
Epoch 210, val loss: 1.5441033840179443
Epoch 220, training loss: 7.910191535949707 = 1.4541971683502197 + 1.0 * 6.455994129180908
Epoch 220, val loss: 1.5038056373596191
Epoch 230, training loss: 7.84907341003418 = 1.404085397720337 + 1.0 * 6.444987773895264
Epoch 230, val loss: 1.4626076221466064
Epoch 240, training loss: 7.788357257843018 = 1.3523502349853516 + 1.0 * 6.436007022857666
Epoch 240, val loss: 1.4208745956420898
Epoch 250, training loss: 7.733138561248779 = 1.2996362447738647 + 1.0 * 6.433502197265625
Epoch 250, val loss: 1.3791128396987915
Epoch 260, training loss: 7.672240257263184 = 1.247973084449768 + 1.0 * 6.424267292022705
Epoch 260, val loss: 1.3391175270080566
Epoch 270, training loss: 7.611924171447754 = 1.197201132774353 + 1.0 * 6.414722919464111
Epoch 270, val loss: 1.3005719184875488
Epoch 280, training loss: 7.556321620941162 = 1.1470627784729004 + 1.0 * 6.409258842468262
Epoch 280, val loss: 1.2627755403518677
Epoch 290, training loss: 7.502614498138428 = 1.097969889640808 + 1.0 * 6.40464448928833
Epoch 290, val loss: 1.2259101867675781
Epoch 300, training loss: 7.45379114151001 = 1.0509839057922363 + 1.0 * 6.402807235717773
Epoch 300, val loss: 1.190694808959961
Epoch 310, training loss: 7.401098251342773 = 1.005311369895935 + 1.0 * 6.395786762237549
Epoch 310, val loss: 1.156386375427246
Epoch 320, training loss: 7.354434013366699 = 0.9607979655265808 + 1.0 * 6.393636226654053
Epoch 320, val loss: 1.1224952936172485
Epoch 330, training loss: 7.308885097503662 = 0.9183868169784546 + 1.0 * 6.390498161315918
Epoch 330, val loss: 1.0896519422531128
Epoch 340, training loss: 7.2625017166137695 = 0.8777303099632263 + 1.0 * 6.384771347045898
Epoch 340, val loss: 1.0582449436187744
Epoch 350, training loss: 7.220097541809082 = 0.838555097579956 + 1.0 * 6.381542205810547
Epoch 350, val loss: 1.0277179479599
Epoch 360, training loss: 7.18241024017334 = 0.8007957935333252 + 1.0 * 6.3816142082214355
Epoch 360, val loss: 0.9980208873748779
Epoch 370, training loss: 7.141483783721924 = 0.7647227644920349 + 1.0 * 6.376760959625244
Epoch 370, val loss: 0.96983402967453
Epoch 380, training loss: 7.10430383682251 = 0.7303479909896851 + 1.0 * 6.373955726623535
Epoch 380, val loss: 0.9434382319450378
Epoch 390, training loss: 7.06916618347168 = 0.6975762844085693 + 1.0 * 6.371589660644531
Epoch 390, val loss: 0.9186831116676331
Epoch 400, training loss: 7.035581588745117 = 0.6665042042732239 + 1.0 * 6.369077205657959
Epoch 400, val loss: 0.8958898186683655
Epoch 410, training loss: 7.003435134887695 = 0.6367904543876648 + 1.0 * 6.366644859313965
Epoch 410, val loss: 0.8747885823249817
Epoch 420, training loss: 6.976745128631592 = 0.6084501147270203 + 1.0 * 6.368295192718506
Epoch 420, val loss: 0.8553951978683472
Epoch 430, training loss: 6.943854808807373 = 0.5815975069999695 + 1.0 * 6.362257480621338
Epoch 430, val loss: 0.8377995491027832
Epoch 440, training loss: 6.914578914642334 = 0.5558676719665527 + 1.0 * 6.358711242675781
Epoch 440, val loss: 0.8216543793678284
Epoch 450, training loss: 6.892443656921387 = 0.5310811400413513 + 1.0 * 6.361362457275391
Epoch 450, val loss: 0.8066301941871643
Epoch 460, training loss: 6.867406368255615 = 0.5072596669197083 + 1.0 * 6.360146522521973
Epoch 460, val loss: 0.7927157878875732
Epoch 470, training loss: 6.837779998779297 = 0.48432210087776184 + 1.0 * 6.353457927703857
Epoch 470, val loss: 0.7797623872756958
Epoch 480, training loss: 6.813804626464844 = 0.46192383766174316 + 1.0 * 6.35188102722168
Epoch 480, val loss: 0.7674476504325867
Epoch 490, training loss: 6.788959980010986 = 0.43996116518974304 + 1.0 * 6.3489990234375
Epoch 490, val loss: 0.7556724548339844
Epoch 500, training loss: 6.773313522338867 = 0.41841769218444824 + 1.0 * 6.354896068572998
Epoch 500, val loss: 0.7442737221717834
Epoch 510, training loss: 6.744089603424072 = 0.3972407877445221 + 1.0 * 6.346848964691162
Epoch 510, val loss: 0.7334054708480835
Epoch 520, training loss: 6.720698833465576 = 0.37649601697921753 + 1.0 * 6.344202995300293
Epoch 520, val loss: 0.722920835018158
Epoch 530, training loss: 6.7008795738220215 = 0.35618776082992554 + 1.0 * 6.344691753387451
Epoch 530, val loss: 0.7128751873970032
Epoch 540, training loss: 6.67802095413208 = 0.33646059036254883 + 1.0 * 6.341560363769531
Epoch 540, val loss: 0.7034810781478882
Epoch 550, training loss: 6.65598201751709 = 0.3173498809337616 + 1.0 * 6.338632106781006
Epoch 550, val loss: 0.6947159767150879
Epoch 560, training loss: 6.652078628540039 = 0.2989121973514557 + 1.0 * 6.353166580200195
Epoch 560, val loss: 0.686668872833252
Epoch 570, training loss: 6.6199235916137695 = 0.2814490497112274 + 1.0 * 6.338474750518799
Epoch 570, val loss: 0.6795210838317871
Epoch 580, training loss: 6.5997467041015625 = 0.2648669481277466 + 1.0 * 6.3348798751831055
Epoch 580, val loss: 0.6732707619667053
Epoch 590, training loss: 6.584537982940674 = 0.2490689903497696 + 1.0 * 6.335468769073486
Epoch 590, val loss: 0.6679250597953796
Epoch 600, training loss: 6.572388172149658 = 0.23423980176448822 + 1.0 * 6.338148593902588
Epoch 600, val loss: 0.6635097861289978
Epoch 610, training loss: 6.5508952140808105 = 0.22028207778930664 + 1.0 * 6.330613136291504
Epoch 610, val loss: 0.6600403189659119
Epoch 620, training loss: 6.53641414642334 = 0.20713678002357483 + 1.0 * 6.329277515411377
Epoch 620, val loss: 0.6574785113334656
Epoch 630, training loss: 6.5334343910217285 = 0.1947757750749588 + 1.0 * 6.338658809661865
Epoch 630, val loss: 0.6557997465133667
Epoch 640, training loss: 6.5123209953308105 = 0.1832897812128067 + 1.0 * 6.329030990600586
Epoch 640, val loss: 0.6548841595649719
Epoch 650, training loss: 6.497309684753418 = 0.17254869639873505 + 1.0 * 6.324760913848877
Epoch 650, val loss: 0.6547213792800903
Epoch 660, training loss: 6.495282173156738 = 0.16251026093959808 + 1.0 * 6.3327717781066895
Epoch 660, val loss: 0.6553111672401428
Epoch 670, training loss: 6.4786696434021 = 0.15321505069732666 + 1.0 * 6.3254547119140625
Epoch 670, val loss: 0.656554639339447
Epoch 680, training loss: 6.470673084259033 = 0.14457255601882935 + 1.0 * 6.3261003494262695
Epoch 680, val loss: 0.6583635210990906
Epoch 690, training loss: 6.456925868988037 = 0.13655982911586761 + 1.0 * 6.320365905761719
Epoch 690, val loss: 0.6607113480567932
Epoch 700, training loss: 6.450063705444336 = 0.12911631166934967 + 1.0 * 6.320947170257568
Epoch 700, val loss: 0.6635240912437439
Epoch 710, training loss: 6.443563461303711 = 0.12221231311559677 + 1.0 * 6.321351051330566
Epoch 710, val loss: 0.6668118238449097
Epoch 720, training loss: 6.436124801635742 = 0.11583703756332397 + 1.0 * 6.320287704467773
Epoch 720, val loss: 0.670356810092926
Epoch 730, training loss: 6.4254350662231445 = 0.10989511758089066 + 1.0 * 6.315539836883545
Epoch 730, val loss: 0.6742410659790039
Epoch 740, training loss: 6.4310302734375 = 0.10434222221374512 + 1.0 * 6.326688289642334
Epoch 740, val loss: 0.6784896850585938
Epoch 750, training loss: 6.418292045593262 = 0.09920036047697067 + 1.0 * 6.319091796875
Epoch 750, val loss: 0.6828849911689758
Epoch 760, training loss: 6.406294822692871 = 0.09439129382371902 + 1.0 * 6.311903476715088
Epoch 760, val loss: 0.6874480843544006
Epoch 770, training loss: 6.401919364929199 = 0.08987811952829361 + 1.0 * 6.312041282653809
Epoch 770, val loss: 0.6922816634178162
Epoch 780, training loss: 6.407517433166504 = 0.08564390242099762 + 1.0 * 6.321873664855957
Epoch 780, val loss: 0.697286069393158
Epoch 790, training loss: 6.397154808044434 = 0.08169939368963242 + 1.0 * 6.315455436706543
Epoch 790, val loss: 0.7023223042488098
Epoch 800, training loss: 6.389175891876221 = 0.07799557596445084 + 1.0 * 6.311180114746094
Epoch 800, val loss: 0.7074105143547058
Epoch 810, training loss: 6.384400844573975 = 0.07450830191373825 + 1.0 * 6.309892654418945
Epoch 810, val loss: 0.7126818895339966
Epoch 820, training loss: 6.377495288848877 = 0.07121603190898895 + 1.0 * 6.306279182434082
Epoch 820, val loss: 0.7179785966873169
Epoch 830, training loss: 6.383483409881592 = 0.06810982525348663 + 1.0 * 6.315373420715332
Epoch 830, val loss: 0.7233442664146423
Epoch 840, training loss: 6.37541389465332 = 0.06518600881099701 + 1.0 * 6.310227870941162
Epoch 840, val loss: 0.7287542223930359
Epoch 850, training loss: 6.36624002456665 = 0.06242729350924492 + 1.0 * 6.303812503814697
Epoch 850, val loss: 0.7340916395187378
Epoch 860, training loss: 6.365346908569336 = 0.05981755629181862 + 1.0 * 6.3055291175842285
Epoch 860, val loss: 0.7395156025886536
Epoch 870, training loss: 6.362594127655029 = 0.05735031142830849 + 1.0 * 6.305243968963623
Epoch 870, val loss: 0.7449836730957031
Epoch 880, training loss: 6.3617730140686035 = 0.0550212524831295 + 1.0 * 6.306751728057861
Epoch 880, val loss: 0.7504093647003174
Epoch 890, training loss: 6.3595781326293945 = 0.0528133288025856 + 1.0 * 6.306764602661133
Epoch 890, val loss: 0.7557998299598694
Epoch 900, training loss: 6.351247310638428 = 0.05073042958974838 + 1.0 * 6.3005170822143555
Epoch 900, val loss: 0.7612008452415466
Epoch 910, training loss: 6.348019599914551 = 0.048750657588243484 + 1.0 * 6.29926872253418
Epoch 910, val loss: 0.7665566802024841
Epoch 920, training loss: 6.350813865661621 = 0.046867724508047104 + 1.0 * 6.303946018218994
Epoch 920, val loss: 0.7719689607620239
Epoch 930, training loss: 6.3426079750061035 = 0.045085422694683075 + 1.0 * 6.29752254486084
Epoch 930, val loss: 0.7773496508598328
Epoch 940, training loss: 6.347308158874512 = 0.043389692902565 + 1.0 * 6.303918361663818
Epoch 940, val loss: 0.7826381921768188
Epoch 950, training loss: 6.342118263244629 = 0.04178881272673607 + 1.0 * 6.300329685211182
Epoch 950, val loss: 0.7879325151443481
Epoch 960, training loss: 6.3364458084106445 = 0.04026740416884422 + 1.0 * 6.296178340911865
Epoch 960, val loss: 0.7930588126182556
Epoch 970, training loss: 6.332675933837891 = 0.03882237896323204 + 1.0 * 6.293853759765625
Epoch 970, val loss: 0.7981994152069092
Epoch 980, training loss: 6.332037925720215 = 0.03743956238031387 + 1.0 * 6.294598579406738
Epoch 980, val loss: 0.8033927083015442
Epoch 990, training loss: 6.329614639282227 = 0.03612237423658371 + 1.0 * 6.293492317199707
Epoch 990, val loss: 0.8085695505142212
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 10.54653263092041 = 1.9497108459472656 + 1.0 * 8.596821784973145
Epoch 0, val loss: 1.942875623703003
Epoch 10, training loss: 10.535530090332031 = 1.939065933227539 + 1.0 * 8.596464157104492
Epoch 10, val loss: 1.9330562353134155
Epoch 20, training loss: 10.519119262695312 = 1.9257625341415405 + 1.0 * 8.59335708618164
Epoch 20, val loss: 1.9204835891723633
Epoch 30, training loss: 10.475784301757812 = 1.9070569276809692 + 1.0 * 8.568727493286133
Epoch 30, val loss: 1.9027440547943115
Epoch 40, training loss: 10.305169105529785 = 1.8833671808242798 + 1.0 * 8.421801567077637
Epoch 40, val loss: 1.8811249732971191
Epoch 50, training loss: 9.863483428955078 = 1.8577476739883423 + 1.0 * 8.005735397338867
Epoch 50, val loss: 1.857752799987793
Epoch 60, training loss: 9.561182022094727 = 1.8335182666778564 + 1.0 * 7.727663516998291
Epoch 60, val loss: 1.835594654083252
Epoch 70, training loss: 9.160038948059082 = 1.8140432834625244 + 1.0 * 7.345995903015137
Epoch 70, val loss: 1.817910075187683
Epoch 80, training loss: 8.917318344116211 = 1.7979586124420166 + 1.0 * 7.119359970092773
Epoch 80, val loss: 1.8034896850585938
Epoch 90, training loss: 8.74185562133789 = 1.7802289724349976 + 1.0 * 6.961627006530762
Epoch 90, val loss: 1.7884151935577393
Epoch 100, training loss: 8.574029922485352 = 1.7637662887573242 + 1.0 * 6.810263156890869
Epoch 100, val loss: 1.7751744985580444
Epoch 110, training loss: 8.480605125427246 = 1.7474850416183472 + 1.0 * 6.733119964599609
Epoch 110, val loss: 1.761094093322754
Epoch 120, training loss: 8.405888557434082 = 1.7288252115249634 + 1.0 * 6.67706298828125
Epoch 120, val loss: 1.74381685256958
Epoch 130, training loss: 8.344408988952637 = 1.7078402042388916 + 1.0 * 6.636568546295166
Epoch 130, val loss: 1.7244341373443604
Epoch 140, training loss: 8.288370132446289 = 1.6838229894638062 + 1.0 * 6.604547023773193
Epoch 140, val loss: 1.7029290199279785
Epoch 150, training loss: 8.231308937072754 = 1.656357765197754 + 1.0 * 6.574951171875
Epoch 150, val loss: 1.6790791749954224
Epoch 160, training loss: 8.173807144165039 = 1.6246415376663208 + 1.0 * 6.549165725708008
Epoch 160, val loss: 1.6516642570495605
Epoch 170, training loss: 8.121089935302734 = 1.5878949165344238 + 1.0 * 6.533194541931152
Epoch 170, val loss: 1.6199982166290283
Epoch 180, training loss: 8.057133674621582 = 1.5469270944595337 + 1.0 * 6.510206699371338
Epoch 180, val loss: 1.5847517251968384
Epoch 190, training loss: 7.998088836669922 = 1.5019347667694092 + 1.0 * 6.496154308319092
Epoch 190, val loss: 1.5463902950286865
Epoch 200, training loss: 7.937837600708008 = 1.4538719654083252 + 1.0 * 6.483965873718262
Epoch 200, val loss: 1.5058259963989258
Epoch 210, training loss: 7.8807373046875 = 1.4049662351608276 + 1.0 * 6.475770950317383
Epoch 210, val loss: 1.4652594327926636
Epoch 220, training loss: 7.820470333099365 = 1.3567389249801636 + 1.0 * 6.463731288909912
Epoch 220, val loss: 1.4258806705474854
Epoch 230, training loss: 7.763779163360596 = 1.3089240789413452 + 1.0 * 6.454854965209961
Epoch 230, val loss: 1.3872864246368408
Epoch 240, training loss: 7.710468292236328 = 1.2619807720184326 + 1.0 * 6.448487281799316
Epoch 240, val loss: 1.3501262664794922
Epoch 250, training loss: 7.655869007110596 = 1.2167235612869263 + 1.0 * 6.439145565032959
Epoch 250, val loss: 1.3146569728851318
Epoch 260, training loss: 7.604043483734131 = 1.1726317405700684 + 1.0 * 6.4314117431640625
Epoch 260, val loss: 1.280439853668213
Epoch 270, training loss: 7.5550007820129395 = 1.1298717260360718 + 1.0 * 6.425128936767578
Epoch 270, val loss: 1.2476325035095215
Epoch 280, training loss: 7.508915901184082 = 1.089050054550171 + 1.0 * 6.41986608505249
Epoch 280, val loss: 1.2167128324508667
Epoch 290, training loss: 7.46404504776001 = 1.0500258207321167 + 1.0 * 6.4140191078186035
Epoch 290, val loss: 1.1873376369476318
Epoch 300, training loss: 7.421908378601074 = 1.0129828453063965 + 1.0 * 6.408925533294678
Epoch 300, val loss: 1.1595302820205688
Epoch 310, training loss: 7.3806281089782715 = 0.9773486256599426 + 1.0 * 6.4032793045043945
Epoch 310, val loss: 1.1329787969589233
Epoch 320, training loss: 7.349941253662109 = 0.9427890181541443 + 1.0 * 6.40715217590332
Epoch 320, val loss: 1.1072907447814941
Epoch 330, training loss: 7.304162502288818 = 0.9096769094467163 + 1.0 * 6.3944854736328125
Epoch 330, val loss: 1.082459568977356
Epoch 340, training loss: 7.267608165740967 = 0.8770236372947693 + 1.0 * 6.390584468841553
Epoch 340, val loss: 1.0581330060958862
Epoch 350, training loss: 7.2435126304626465 = 0.8444910645484924 + 1.0 * 6.399021625518799
Epoch 350, val loss: 1.033787488937378
Epoch 360, training loss: 7.19657564163208 = 0.8123742341995239 + 1.0 * 6.384201526641846
Epoch 360, val loss: 1.0098627805709839
Epoch 370, training loss: 7.159891128540039 = 0.7804253697395325 + 1.0 * 6.379465579986572
Epoch 370, val loss: 0.9863303899765015
Epoch 380, training loss: 7.134594440460205 = 0.7486324310302734 + 1.0 * 6.385962009429932
Epoch 380, val loss: 0.963172972202301
Epoch 390, training loss: 7.090785503387451 = 0.7177481055259705 + 1.0 * 6.373037338256836
Epoch 390, val loss: 0.9409424662590027
Epoch 400, training loss: 7.058126926422119 = 0.6876453161239624 + 1.0 * 6.370481491088867
Epoch 400, val loss: 0.9201744794845581
Epoch 410, training loss: 7.038695335388184 = 0.6585123538970947 + 1.0 * 6.380183219909668
Epoch 410, val loss: 0.9007519483566284
Epoch 420, training loss: 6.998424530029297 = 0.630885124206543 + 1.0 * 6.367539405822754
Epoch 420, val loss: 0.8833422064781189
Epoch 430, training loss: 6.967823028564453 = 0.6045430302619934 + 1.0 * 6.363279819488525
Epoch 430, val loss: 0.8679489493370056
Epoch 440, training loss: 6.940666198730469 = 0.5793396234512329 + 1.0 * 6.361326694488525
Epoch 440, val loss: 0.8542681932449341
Epoch 450, training loss: 6.918580055236816 = 0.5556177496910095 + 1.0 * 6.362962245941162
Epoch 450, val loss: 0.842383861541748
Epoch 460, training loss: 6.89061975479126 = 0.5331951379776001 + 1.0 * 6.357424736022949
Epoch 460, val loss: 0.8325833678245544
Epoch 470, training loss: 6.863529682159424 = 0.5118106007575989 + 1.0 * 6.351718902587891
Epoch 470, val loss: 0.8240787386894226
Epoch 480, training loss: 6.841210842132568 = 0.4912324845790863 + 1.0 * 6.349978446960449
Epoch 480, val loss: 0.8168683648109436
Epoch 490, training loss: 6.826852798461914 = 0.4715177118778229 + 1.0 * 6.355335235595703
Epoch 490, val loss: 0.8108275532722473
Epoch 500, training loss: 6.800241947174072 = 0.4527481198310852 + 1.0 * 6.347493648529053
Epoch 500, val loss: 0.8059968948364258
Epoch 510, training loss: 6.778099536895752 = 0.43462520837783813 + 1.0 * 6.343474388122559
Epoch 510, val loss: 0.8019951581954956
Epoch 520, training loss: 6.760300636291504 = 0.4170469343662262 + 1.0 * 6.3432536125183105
Epoch 520, val loss: 0.7986934781074524
Epoch 530, training loss: 6.748530864715576 = 0.400098592042923 + 1.0 * 6.3484320640563965
Epoch 530, val loss: 0.796105682849884
Epoch 540, training loss: 6.722989082336426 = 0.3837898075580597 + 1.0 * 6.339199066162109
Epoch 540, val loss: 0.7942016124725342
Epoch 550, training loss: 6.704545974731445 = 0.36799150705337524 + 1.0 * 6.336554527282715
Epoch 550, val loss: 0.7929210066795349
Epoch 560, training loss: 6.687470436096191 = 0.3525449335575104 + 1.0 * 6.334925651550293
Epoch 560, val loss: 0.7921903133392334
Epoch 570, training loss: 6.680178165435791 = 0.33742713928222656 + 1.0 * 6.3427510261535645
Epoch 570, val loss: 0.7919796705245972
Epoch 580, training loss: 6.655063152313232 = 0.3226972222328186 + 1.0 * 6.332365989685059
Epoch 580, val loss: 0.7921502590179443
Epoch 590, training loss: 6.638280868530273 = 0.30828386545181274 + 1.0 * 6.3299970626831055
Epoch 590, val loss: 0.7928119897842407
Epoch 600, training loss: 6.621792793273926 = 0.29401493072509766 + 1.0 * 6.327777862548828
Epoch 600, val loss: 0.7938888072967529
Epoch 610, training loss: 6.611288070678711 = 0.27982646226882935 + 1.0 * 6.331461429595947
Epoch 610, val loss: 0.795371949672699
Epoch 620, training loss: 6.598953723907471 = 0.2658023238182068 + 1.0 * 6.333151340484619
Epoch 620, val loss: 0.7972425222396851
Epoch 630, training loss: 6.577898979187012 = 0.2520225942134857 + 1.0 * 6.325876235961914
Epoch 630, val loss: 0.7995328903198242
Epoch 640, training loss: 6.563752174377441 = 0.23846666514873505 + 1.0 * 6.3252854347229
Epoch 640, val loss: 0.8023768067359924
Epoch 650, training loss: 6.549223899841309 = 0.22525791823863983 + 1.0 * 6.323966026306152
Epoch 650, val loss: 0.8057037591934204
Epoch 660, training loss: 6.535365104675293 = 0.21250830590724945 + 1.0 * 6.322856903076172
Epoch 660, val loss: 0.8096022009849548
Epoch 670, training loss: 6.522125720977783 = 0.20027504861354828 + 1.0 * 6.321850776672363
Epoch 670, val loss: 0.8141031265258789
Epoch 680, training loss: 6.514610767364502 = 0.18866486847400665 + 1.0 * 6.325945854187012
Epoch 680, val loss: 0.8190813064575195
Epoch 690, training loss: 6.498427391052246 = 0.17778177559375763 + 1.0 * 6.320645809173584
Epoch 690, val loss: 0.8243826627731323
Epoch 700, training loss: 6.483953475952148 = 0.1675400733947754 + 1.0 * 6.316413402557373
Epoch 700, val loss: 0.8301934003829956
Epoch 710, training loss: 6.473046779632568 = 0.15793950855731964 + 1.0 * 6.315107345581055
Epoch 710, val loss: 0.8363959193229675
Epoch 720, training loss: 6.46741247177124 = 0.1489795446395874 + 1.0 * 6.318432807922363
Epoch 720, val loss: 0.8428378105163574
Epoch 730, training loss: 6.456489562988281 = 0.1407134085893631 + 1.0 * 6.315776348114014
Epoch 730, val loss: 0.8492777347564697
Epoch 740, training loss: 6.4450836181640625 = 0.1330324411392212 + 1.0 * 6.312051296234131
Epoch 740, val loss: 0.8559444546699524
Epoch 750, training loss: 6.4394850730896 = 0.12585781514644623 + 1.0 * 6.313627243041992
Epoch 750, val loss: 0.8629149794578552
Epoch 760, training loss: 6.429378032684326 = 0.11915141344070435 + 1.0 * 6.3102264404296875
Epoch 760, val loss: 0.8700039982795715
Epoch 770, training loss: 6.421413898468018 = 0.11288479715585709 + 1.0 * 6.308528900146484
Epoch 770, val loss: 0.8772038817405701
Epoch 780, training loss: 6.427114486694336 = 0.10702097415924072 + 1.0 * 6.320093631744385
Epoch 780, val loss: 0.8845339417457581
Epoch 790, training loss: 6.409718036651611 = 0.10157275944948196 + 1.0 * 6.308145046234131
Epoch 790, val loss: 0.8917176127433777
Epoch 800, training loss: 6.402288436889648 = 0.09648188948631287 + 1.0 * 6.305806636810303
Epoch 800, val loss: 0.8989328742027283
Epoch 810, training loss: 6.409627437591553 = 0.09170815348625183 + 1.0 * 6.3179192543029785
Epoch 810, val loss: 0.9062386751174927
Epoch 820, training loss: 6.394804000854492 = 0.08727636933326721 + 1.0 * 6.307527542114258
Epoch 820, val loss: 0.9132864475250244
Epoch 830, training loss: 6.389094829559326 = 0.08313928544521332 + 1.0 * 6.305955410003662
Epoch 830, val loss: 0.9201774597167969
Epoch 840, training loss: 6.381896018981934 = 0.07926704734563828 + 1.0 * 6.302628993988037
Epoch 840, val loss: 0.9271575808525085
Epoch 850, training loss: 6.376322269439697 = 0.07561814039945602 + 1.0 * 6.300704002380371
Epoch 850, val loss: 0.9341666102409363
Epoch 860, training loss: 6.371771335601807 = 0.07219160348176956 + 1.0 * 6.299579620361328
Epoch 860, val loss: 0.9410594701766968
Epoch 870, training loss: 6.373648166656494 = 0.06897091120481491 + 1.0 * 6.304677486419678
Epoch 870, val loss: 0.9479213356971741
Epoch 880, training loss: 6.375883102416992 = 0.0659506693482399 + 1.0 * 6.309932231903076
Epoch 880, val loss: 0.9544308185577393
Epoch 890, training loss: 6.362468719482422 = 0.06313812732696533 + 1.0 * 6.299330711364746
Epoch 890, val loss: 0.9607754349708557
Epoch 900, training loss: 6.357324600219727 = 0.06048628315329552 + 1.0 * 6.296838283538818
Epoch 900, val loss: 0.967231810092926
Epoch 910, training loss: 6.364258289337158 = 0.057984184473752975 + 1.0 * 6.306273937225342
Epoch 910, val loss: 0.9736691117286682
Epoch 920, training loss: 6.353420734405518 = 0.05562049150466919 + 1.0 * 6.297800064086914
Epoch 920, val loss: 0.9799000024795532
Epoch 930, training loss: 6.348154544830322 = 0.05339817702770233 + 1.0 * 6.2947564125061035
Epoch 930, val loss: 0.9859935641288757
Epoch 940, training loss: 6.346534252166748 = 0.05128959193825722 + 1.0 * 6.2952446937561035
Epoch 940, val loss: 0.992154061794281
Epoch 950, training loss: 6.345865249633789 = 0.049294162541627884 + 1.0 * 6.296571254730225
Epoch 950, val loss: 0.9981516599655151
Epoch 960, training loss: 6.342579364776611 = 0.04741692170500755 + 1.0 * 6.295162677764893
Epoch 960, val loss: 1.0039535760879517
Epoch 970, training loss: 6.339857578277588 = 0.045633960515260696 + 1.0 * 6.294223785400391
Epoch 970, val loss: 1.0096718072891235
Epoch 980, training loss: 6.3353705406188965 = 0.04394795373082161 + 1.0 * 6.291422367095947
Epoch 980, val loss: 1.0153380632400513
Epoch 990, training loss: 6.332992076873779 = 0.04234883934259415 + 1.0 * 6.290643215179443
Epoch 990, val loss: 1.0209698677062988
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 10.551608085632324 = 1.9547615051269531 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9623010158538818
Epoch 10, training loss: 10.541557312011719 = 1.9449594020843506 + 1.0 * 8.596597671508789
Epoch 10, val loss: 1.9523588418960571
Epoch 20, training loss: 10.52733325958252 = 1.9328175783157349 + 1.0 * 8.594515800476074
Epoch 20, val loss: 1.9398826360702515
Epoch 30, training loss: 10.492429733276367 = 1.9158107042312622 + 1.0 * 8.576619148254395
Epoch 30, val loss: 1.9224952459335327
Epoch 40, training loss: 10.339855194091797 = 1.8926074504852295 + 1.0 * 8.447247505187988
Epoch 40, val loss: 1.8994816541671753
Epoch 50, training loss: 9.880391120910645 = 1.86551034450531 + 1.0 * 8.014881134033203
Epoch 50, val loss: 1.8737765550613403
Epoch 60, training loss: 9.43242073059082 = 1.8446784019470215 + 1.0 * 7.587742805480957
Epoch 60, val loss: 1.8543297052383423
Epoch 70, training loss: 8.977128028869629 = 1.8309552669525146 + 1.0 * 7.146172523498535
Epoch 70, val loss: 1.8402982950210571
Epoch 80, training loss: 8.771602630615234 = 1.818047046661377 + 1.0 * 6.953556060791016
Epoch 80, val loss: 1.8268160820007324
Epoch 90, training loss: 8.639603614807129 = 1.8012170791625977 + 1.0 * 6.838386535644531
Epoch 90, val loss: 1.810782790184021
Epoch 100, training loss: 8.5418701171875 = 1.7842354774475098 + 1.0 * 6.757635116577148
Epoch 100, val loss: 1.7948317527770996
Epoch 110, training loss: 8.461681365966797 = 1.7692203521728516 + 1.0 * 6.692460536956787
Epoch 110, val loss: 1.7800477743148804
Epoch 120, training loss: 8.396968841552734 = 1.7550448179244995 + 1.0 * 6.641923904418945
Epoch 120, val loss: 1.765574336051941
Epoch 130, training loss: 8.337989807128906 = 1.7396899461746216 + 1.0 * 6.598299503326416
Epoch 130, val loss: 1.7504533529281616
Epoch 140, training loss: 8.286499977111816 = 1.7220507860183716 + 1.0 * 6.564448833465576
Epoch 140, val loss: 1.7339974641799927
Epoch 150, training loss: 8.243903160095215 = 1.7014518976211548 + 1.0 * 6.542450904846191
Epoch 150, val loss: 1.7153974771499634
Epoch 160, training loss: 8.19709587097168 = 1.6778173446655273 + 1.0 * 6.519278526306152
Epoch 160, val loss: 1.6944516897201538
Epoch 170, training loss: 8.154016494750977 = 1.6508591175079346 + 1.0 * 6.503157615661621
Epoch 170, val loss: 1.6705082654953003
Epoch 180, training loss: 8.105995178222656 = 1.6196163892745972 + 1.0 * 6.4863786697387695
Epoch 180, val loss: 1.6428427696228027
Epoch 190, training loss: 8.0560302734375 = 1.58327054977417 + 1.0 * 6.472760200500488
Epoch 190, val loss: 1.61082124710083
Epoch 200, training loss: 8.002789497375488 = 1.5414817333221436 + 1.0 * 6.461308002471924
Epoch 200, val loss: 1.5740933418273926
Epoch 210, training loss: 7.947137355804443 = 1.4949871301651 + 1.0 * 6.452150344848633
Epoch 210, val loss: 1.533631443977356
Epoch 220, training loss: 7.887372970581055 = 1.444612741470337 + 1.0 * 6.442760467529297
Epoch 220, val loss: 1.4900914430618286
Epoch 230, training loss: 7.825721263885498 = 1.39098060131073 + 1.0 * 6.4347405433654785
Epoch 230, val loss: 1.4441248178482056
Epoch 240, training loss: 7.765996932983398 = 1.3359699249267578 + 1.0 * 6.430027008056641
Epoch 240, val loss: 1.3978890180587769
Epoch 250, training loss: 7.702406406402588 = 1.2811881303787231 + 1.0 * 6.421218395233154
Epoch 250, val loss: 1.3524795770645142
Epoch 260, training loss: 7.641952991485596 = 1.2268589735031128 + 1.0 * 6.415093898773193
Epoch 260, val loss: 1.3080869913101196
Epoch 270, training loss: 7.588490009307861 = 1.1735734939575195 + 1.0 * 6.414916515350342
Epoch 270, val loss: 1.2653619050979614
Epoch 280, training loss: 7.5290021896362305 = 1.1223126649856567 + 1.0 * 6.406689643859863
Epoch 280, val loss: 1.22481107711792
Epoch 290, training loss: 7.473491191864014 = 1.0723333358764648 + 1.0 * 6.401157855987549
Epoch 290, val loss: 1.1858810186386108
Epoch 300, training loss: 7.421489715576172 = 1.0235235691070557 + 1.0 * 6.397965908050537
Epoch 300, val loss: 1.1481759548187256
Epoch 310, training loss: 7.3691725730896 = 0.976087749004364 + 1.0 * 6.39308500289917
Epoch 310, val loss: 1.111726999282837
Epoch 320, training loss: 7.317902088165283 = 0.9294356107711792 + 1.0 * 6.3884663581848145
Epoch 320, val loss: 1.0760698318481445
Epoch 330, training loss: 7.27075719833374 = 0.8835610747337341 + 1.0 * 6.387196063995361
Epoch 330, val loss: 1.041037678718567
Epoch 340, training loss: 7.221418380737305 = 0.8388575911521912 + 1.0 * 6.382560729980469
Epoch 340, val loss: 1.0069584846496582
Epoch 350, training loss: 7.174263000488281 = 0.7951667904853821 + 1.0 * 6.379096031188965
Epoch 350, val loss: 0.9738287925720215
Epoch 360, training loss: 7.137784957885742 = 0.7527613639831543 + 1.0 * 6.385023593902588
Epoch 360, val loss: 0.9418213963508606
Epoch 370, training loss: 7.088283538818359 = 0.7126815319061279 + 1.0 * 6.3756022453308105
Epoch 370, val loss: 0.9117008447647095
Epoch 380, training loss: 7.045505523681641 = 0.67464280128479 + 1.0 * 6.37086296081543
Epoch 380, val loss: 0.8835233449935913
Epoch 390, training loss: 7.007452487945557 = 0.6388195157051086 + 1.0 * 6.368632793426514
Epoch 390, val loss: 0.8574594855308533
Epoch 400, training loss: 6.97235631942749 = 0.6054321527481079 + 1.0 * 6.366924285888672
Epoch 400, val loss: 0.8338225483894348
Epoch 410, training loss: 6.937319755554199 = 0.5741701722145081 + 1.0 * 6.363149642944336
Epoch 410, val loss: 0.8125050663948059
Epoch 420, training loss: 6.906862735748291 = 0.5449339747428894 + 1.0 * 6.361928939819336
Epoch 420, val loss: 0.7933773994445801
Epoch 430, training loss: 6.877816200256348 = 0.5176653861999512 + 1.0 * 6.3601508140563965
Epoch 430, val loss: 0.7764086127281189
Epoch 440, training loss: 6.8498687744140625 = 0.4920516610145569 + 1.0 * 6.35781717300415
Epoch 440, val loss: 0.7612618803977966
Epoch 450, training loss: 6.821770191192627 = 0.46788331866264343 + 1.0 * 6.35388708114624
Epoch 450, val loss: 0.747768223285675
Epoch 460, training loss: 6.796652317047119 = 0.44479987025260925 + 1.0 * 6.3518524169921875
Epoch 460, val loss: 0.7356041669845581
Epoch 470, training loss: 6.775494575500488 = 0.42273613810539246 + 1.0 * 6.352758407592773
Epoch 470, val loss: 0.7245990037918091
Epoch 480, training loss: 6.750283718109131 = 0.40161436796188354 + 1.0 * 6.348669528961182
Epoch 480, val loss: 0.7146521806716919
Epoch 490, training loss: 6.726585388183594 = 0.3812103867530823 + 1.0 * 6.345375061035156
Epoch 490, val loss: 0.705491304397583
Epoch 500, training loss: 6.716996192932129 = 0.3613775968551636 + 1.0 * 6.355618476867676
Epoch 500, val loss: 0.6970910429954529
Epoch 510, training loss: 6.6843366622924805 = 0.34225985407829285 + 1.0 * 6.342076778411865
Epoch 510, val loss: 0.6892664432525635
Epoch 520, training loss: 6.663416862487793 = 0.32367438077926636 + 1.0 * 6.339742660522461
Epoch 520, val loss: 0.6818888783454895
Epoch 530, training loss: 6.656435966491699 = 0.3056078255176544 + 1.0 * 6.350828170776367
Epoch 530, val loss: 0.6750460863113403
Epoch 540, training loss: 6.62491512298584 = 0.28824037313461304 + 1.0 * 6.336674690246582
Epoch 540, val loss: 0.6686156392097473
Epoch 550, training loss: 6.606441020965576 = 0.27144289016723633 + 1.0 * 6.33499813079834
Epoch 550, val loss: 0.6625553369522095
Epoch 560, training loss: 6.590479373931885 = 0.2552088499069214 + 1.0 * 6.335270404815674
Epoch 560, val loss: 0.6570077538490295
Epoch 570, training loss: 6.578616619110107 = 0.2397235631942749 + 1.0 * 6.338892936706543
Epoch 570, val loss: 0.6519139409065247
Epoch 580, training loss: 6.558168411254883 = 0.22497400641441345 + 1.0 * 6.333194255828857
Epoch 580, val loss: 0.6472598910331726
Epoch 590, training loss: 6.5397796630859375 = 0.21093083918094635 + 1.0 * 6.328848838806152
Epoch 590, val loss: 0.6431087255477905
Epoch 600, training loss: 6.525826454162598 = 0.19757311046123505 + 1.0 * 6.328253269195557
Epoch 600, val loss: 0.6395077109336853
Epoch 610, training loss: 6.513996601104736 = 0.18499048054218292 + 1.0 * 6.329006195068359
Epoch 610, val loss: 0.6366639733314514
Epoch 620, training loss: 6.498830795288086 = 0.17321710288524628 + 1.0 * 6.325613498687744
Epoch 620, val loss: 0.6342183351516724
Epoch 630, training loss: 6.485144138336182 = 0.16216321289539337 + 1.0 * 6.322980880737305
Epoch 630, val loss: 0.6323181390762329
Epoch 640, training loss: 6.474935531616211 = 0.15179722011089325 + 1.0 * 6.323138236999512
Epoch 640, val loss: 0.6310579776763916
Epoch 650, training loss: 6.469537258148193 = 0.1421581655740738 + 1.0 * 6.32737922668457
Epoch 650, val loss: 0.6305162906646729
Epoch 660, training loss: 6.453004837036133 = 0.13325636088848114 + 1.0 * 6.319748401641846
Epoch 660, val loss: 0.6302674412727356
Epoch 670, training loss: 6.441969394683838 = 0.12496376037597656 + 1.0 * 6.317005634307861
Epoch 670, val loss: 0.6304800510406494
Epoch 680, training loss: 6.434447288513184 = 0.1172402948141098 + 1.0 * 6.317206859588623
Epoch 680, val loss: 0.6312251091003418
Epoch 690, training loss: 6.429727077484131 = 0.11008547991514206 + 1.0 * 6.319641590118408
Epoch 690, val loss: 0.63254714012146
Epoch 700, training loss: 6.420019626617432 = 0.10349731147289276 + 1.0 * 6.316522121429443
Epoch 700, val loss: 0.6340835094451904
Epoch 710, training loss: 6.41156530380249 = 0.09738482534885406 + 1.0 * 6.314180374145508
Epoch 710, val loss: 0.6359381675720215
Epoch 720, training loss: 6.403446197509766 = 0.09169065952301025 + 1.0 * 6.311755657196045
Epoch 720, val loss: 0.6381897926330566
Epoch 730, training loss: 6.406236171722412 = 0.08638672530651093 + 1.0 * 6.319849491119385
Epoch 730, val loss: 0.6407911777496338
Epoch 740, training loss: 6.395573616027832 = 0.08146185427904129 + 1.0 * 6.314111709594727
Epoch 740, val loss: 0.643729031085968
Epoch 750, training loss: 6.386145114898682 = 0.07689729332923889 + 1.0 * 6.309247970581055
Epoch 750, val loss: 0.6466871500015259
Epoch 760, training loss: 6.383640289306641 = 0.0726303830742836 + 1.0 * 6.311009883880615
Epoch 760, val loss: 0.6499753594398499
Epoch 770, training loss: 6.383296966552734 = 0.06868720799684525 + 1.0 * 6.314609527587891
Epoch 770, val loss: 0.6534676551818848
Epoch 780, training loss: 6.371461868286133 = 0.06503081321716309 + 1.0 * 6.306431293487549
Epoch 780, val loss: 0.6569936871528625
Epoch 790, training loss: 6.365642070770264 = 0.06161743029952049 + 1.0 * 6.304024696350098
Epoch 790, val loss: 0.6605993509292603
Epoch 800, training loss: 6.362518787384033 = 0.05842224508523941 + 1.0 * 6.304096698760986
Epoch 800, val loss: 0.6644243001937866
Epoch 810, training loss: 6.363835334777832 = 0.0554424524307251 + 1.0 * 6.3083930015563965
Epoch 810, val loss: 0.6684290766716003
Epoch 820, training loss: 6.356694221496582 = 0.05268469825387001 + 1.0 * 6.304009437561035
Epoch 820, val loss: 0.6724143624305725
Epoch 830, training loss: 6.350569725036621 = 0.05010903999209404 + 1.0 * 6.3004608154296875
Epoch 830, val loss: 0.676342785358429
Epoch 840, training loss: 6.349496841430664 = 0.04769773408770561 + 1.0 * 6.301799297332764
Epoch 840, val loss: 0.6804037690162659
Epoch 850, training loss: 6.345076560974121 = 0.04545073211193085 + 1.0 * 6.299625873565674
Epoch 850, val loss: 0.6846508383750916
Epoch 860, training loss: 6.344285488128662 = 0.04335910081863403 + 1.0 * 6.300926208496094
Epoch 860, val loss: 0.6887176036834717
Epoch 870, training loss: 6.340521812438965 = 0.04140045866370201 + 1.0 * 6.299121379852295
Epoch 870, val loss: 0.6928028464317322
Epoch 880, training loss: 6.337549209594727 = 0.039567943662405014 + 1.0 * 6.297981262207031
Epoch 880, val loss: 0.6969206929206848
Epoch 890, training loss: 6.3333635330200195 = 0.03784593567252159 + 1.0 * 6.295517444610596
Epoch 890, val loss: 0.7010526061058044
Epoch 900, training loss: 6.348201751708984 = 0.03623315319418907 + 1.0 * 6.311968803405762
Epoch 900, val loss: 0.7051892876625061
Epoch 910, training loss: 6.330020427703857 = 0.03472653776407242 + 1.0 * 6.295293807983398
Epoch 910, val loss: 0.7093803882598877
Epoch 920, training loss: 6.328845977783203 = 0.033318325877189636 + 1.0 * 6.295527458190918
Epoch 920, val loss: 0.7133331894874573
Epoch 930, training loss: 6.323884963989258 = 0.03198937699198723 + 1.0 * 6.291895389556885
Epoch 930, val loss: 0.7173360586166382
Epoch 940, training loss: 6.325901031494141 = 0.03073383867740631 + 1.0 * 6.295166969299316
Epoch 940, val loss: 0.7213692665100098
Epoch 950, training loss: 6.322962284088135 = 0.02955402061343193 + 1.0 * 6.293408393859863
Epoch 950, val loss: 0.7253509759902954
Epoch 960, training loss: 6.321329116821289 = 0.02844437211751938 + 1.0 * 6.292884826660156
Epoch 960, val loss: 0.7293014526367188
Epoch 970, training loss: 6.32107400894165 = 0.027398670092225075 + 1.0 * 6.293675422668457
Epoch 970, val loss: 0.7331078052520752
Epoch 980, training loss: 6.31694221496582 = 0.026411477476358414 + 1.0 * 6.290530681610107
Epoch 980, val loss: 0.7369065880775452
Epoch 990, training loss: 6.317698955535889 = 0.025476044043898582 + 1.0 * 6.29222297668457
Epoch 990, val loss: 0.7406740784645081
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8355297838692674
The final CL Acc:0.80617, 0.01848, The final GNN Acc:0.83817, 0.00269
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10528])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.545955657958984 = 1.94911789894104 + 1.0 * 8.596837997436523
Epoch 0, val loss: 1.9470447301864624
Epoch 10, training loss: 10.535433769226074 = 1.9388741254806519 + 1.0 * 8.596559524536133
Epoch 10, val loss: 1.9361522197723389
Epoch 20, training loss: 10.520334243774414 = 1.926322340965271 + 1.0 * 8.594012260437012
Epoch 20, val loss: 1.9226871728897095
Epoch 30, training loss: 10.479065895080566 = 1.9090219736099243 + 1.0 * 8.570043563842773
Epoch 30, val loss: 1.904276728630066
Epoch 40, training loss: 10.27995491027832 = 1.8874943256378174 + 1.0 * 8.392460823059082
Epoch 40, val loss: 1.8830046653747559
Epoch 50, training loss: 9.88200855255127 = 1.8663575649261475 + 1.0 * 8.015650749206543
Epoch 50, val loss: 1.8632737398147583
Epoch 60, training loss: 9.332148551940918 = 1.850785732269287 + 1.0 * 7.481362819671631
Epoch 60, val loss: 1.8497415781021118
Epoch 70, training loss: 8.920910835266113 = 1.8396462202072144 + 1.0 * 7.081264972686768
Epoch 70, val loss: 1.8400942087173462
Epoch 80, training loss: 8.736672401428223 = 1.827419400215149 + 1.0 * 6.909253120422363
Epoch 80, val loss: 1.8292407989501953
Epoch 90, training loss: 8.621273040771484 = 1.8122663497924805 + 1.0 * 6.809006214141846
Epoch 90, val loss: 1.8160674571990967
Epoch 100, training loss: 8.536992073059082 = 1.7966240644454956 + 1.0 * 6.740367889404297
Epoch 100, val loss: 1.8026471138000488
Epoch 110, training loss: 8.46893310546875 = 1.7818902730941772 + 1.0 * 6.687042713165283
Epoch 110, val loss: 1.7903114557266235
Epoch 120, training loss: 8.41083812713623 = 1.7677110433578491 + 1.0 * 6.64312744140625
Epoch 120, val loss: 1.7786422967910767
Epoch 130, training loss: 8.360696792602539 = 1.753063440322876 + 1.0 * 6.607633590698242
Epoch 130, val loss: 1.7668014764785767
Epoch 140, training loss: 8.315539360046387 = 1.7371608018875122 + 1.0 * 6.578378677368164
Epoch 140, val loss: 1.7539119720458984
Epoch 150, training loss: 8.275672912597656 = 1.7193512916564941 + 1.0 * 6.556321620941162
Epoch 150, val loss: 1.7395614385604858
Epoch 160, training loss: 8.234139442443848 = 1.6993643045425415 + 1.0 * 6.5347747802734375
Epoch 160, val loss: 1.7235606908798218
Epoch 170, training loss: 8.193196296691895 = 1.6768250465393066 + 1.0 * 6.516371250152588
Epoch 170, val loss: 1.7054249048233032
Epoch 180, training loss: 8.1531400680542 = 1.6511852741241455 + 1.0 * 6.501955032348633
Epoch 180, val loss: 1.6847268342971802
Epoch 190, training loss: 8.112605094909668 = 1.6222647428512573 + 1.0 * 6.490340232849121
Epoch 190, val loss: 1.6612614393234253
Epoch 200, training loss: 8.067570686340332 = 1.5899178981781006 + 1.0 * 6.4776530265808105
Epoch 200, val loss: 1.635050892829895
Epoch 210, training loss: 8.021839141845703 = 1.5537768602371216 + 1.0 * 6.468061923980713
Epoch 210, val loss: 1.6056636571884155
Epoch 220, training loss: 7.976772785186768 = 1.5138596296310425 + 1.0 * 6.4629130363464355
Epoch 220, val loss: 1.5732901096343994
Epoch 230, training loss: 7.923335075378418 = 1.4709861278533936 + 1.0 * 6.452348709106445
Epoch 230, val loss: 1.538551688194275
Epoch 240, training loss: 7.870035648345947 = 1.4248942136764526 + 1.0 * 6.445141315460205
Epoch 240, val loss: 1.5013153553009033
Epoch 250, training loss: 7.816982746124268 = 1.3758125305175781 + 1.0 * 6.4411702156066895
Epoch 250, val loss: 1.4618579149246216
Epoch 260, training loss: 7.761075019836426 = 1.3248960971832275 + 1.0 * 6.436178684234619
Epoch 260, val loss: 1.420958161354065
Epoch 270, training loss: 7.701498508453369 = 1.2727166414260864 + 1.0 * 6.428781986236572
Epoch 270, val loss: 1.3794013261795044
Epoch 280, training loss: 7.648554801940918 = 1.219874620437622 + 1.0 * 6.428679943084717
Epoch 280, val loss: 1.3376717567443848
Epoch 290, training loss: 7.588252067565918 = 1.1676477193832397 + 1.0 * 6.420604228973389
Epoch 290, val loss: 1.2967649698257446
Epoch 300, training loss: 7.535685062408447 = 1.116469383239746 + 1.0 * 6.419215679168701
Epoch 300, val loss: 1.2571821212768555
Epoch 310, training loss: 7.481045246124268 = 1.0672017335891724 + 1.0 * 6.413843631744385
Epoch 310, val loss: 1.2193537950515747
Epoch 320, training loss: 7.432733535766602 = 1.0199623107910156 + 1.0 * 6.412771224975586
Epoch 320, val loss: 1.18405282497406
Epoch 330, training loss: 7.380695343017578 = 0.9752988219261169 + 1.0 * 6.405396461486816
Epoch 330, val loss: 1.1514042615890503
Epoch 340, training loss: 7.337807655334473 = 0.9328047037124634 + 1.0 * 6.405003070831299
Epoch 340, val loss: 1.121212124824524
Epoch 350, training loss: 7.294888496398926 = 0.8929316997528076 + 1.0 * 6.401957035064697
Epoch 350, val loss: 1.093574047088623
Epoch 360, training loss: 7.2527971267700195 = 0.8555917143821716 + 1.0 * 6.397205352783203
Epoch 360, val loss: 1.0687806606292725
Epoch 370, training loss: 7.213085174560547 = 0.8204642534255981 + 1.0 * 6.392621040344238
Epoch 370, val loss: 1.0462957620620728
Epoch 380, training loss: 7.181845664978027 = 0.7875969409942627 + 1.0 * 6.3942484855651855
Epoch 380, val loss: 1.0260664224624634
Epoch 390, training loss: 7.1449995040893555 = 0.7570881843566895 + 1.0 * 6.387911319732666
Epoch 390, val loss: 1.0083768367767334
Epoch 400, training loss: 7.113674640655518 = 0.728531539440155 + 1.0 * 6.385143280029297
Epoch 400, val loss: 0.9927451610565186
Epoch 410, training loss: 7.092435359954834 = 0.7016046643257141 + 1.0 * 6.3908305168151855
Epoch 410, val loss: 0.9788442850112915
Epoch 420, training loss: 7.056510925292969 = 0.6762409210205078 + 1.0 * 6.380270004272461
Epoch 420, val loss: 0.9663718938827515
Epoch 430, training loss: 7.030174732208252 = 0.6518917679786682 + 1.0 * 6.3782830238342285
Epoch 430, val loss: 0.9553295969963074
Epoch 440, training loss: 7.005650043487549 = 0.6282941699028015 + 1.0 * 6.377356052398682
Epoch 440, val loss: 0.9452956318855286
Epoch 450, training loss: 6.977956771850586 = 0.6053239703178406 + 1.0 * 6.37263298034668
Epoch 450, val loss: 0.936097264289856
Epoch 460, training loss: 6.9542741775512695 = 0.5827009677886963 + 1.0 * 6.371573448181152
Epoch 460, val loss: 0.9275532960891724
Epoch 470, training loss: 6.937212944030762 = 0.5603765249252319 + 1.0 * 6.37683629989624
Epoch 470, val loss: 0.9195725321769714
Epoch 480, training loss: 6.906142234802246 = 0.5384355783462524 + 1.0 * 6.367706775665283
Epoch 480, val loss: 0.9121537208557129
Epoch 490, training loss: 6.881557941436768 = 0.5167399644851685 + 1.0 * 6.364818096160889
Epoch 490, val loss: 0.9054000377655029
Epoch 500, training loss: 6.873208522796631 = 0.49537843465805054 + 1.0 * 6.3778300285339355
Epoch 500, val loss: 0.8991764187812805
Epoch 510, training loss: 6.837967872619629 = 0.4744681119918823 + 1.0 * 6.363499641418457
Epoch 510, val loss: 0.8940024375915527
Epoch 520, training loss: 6.8143134117126465 = 0.4540197253227234 + 1.0 * 6.360293865203857
Epoch 520, val loss: 0.889642059803009
Epoch 530, training loss: 6.794525623321533 = 0.4340076148509979 + 1.0 * 6.360517978668213
Epoch 530, val loss: 0.885960578918457
Epoch 540, training loss: 6.773860454559326 = 0.41466134786605835 + 1.0 * 6.359199047088623
Epoch 540, val loss: 0.8830029368400574
Epoch 550, training loss: 6.75142765045166 = 0.39588111639022827 + 1.0 * 6.355546474456787
Epoch 550, val loss: 0.8811936378479004
Epoch 560, training loss: 6.731812953948975 = 0.3777129054069519 + 1.0 * 6.354100227355957
Epoch 560, val loss: 0.880050003528595
Epoch 570, training loss: 6.716164588928223 = 0.36015084385871887 + 1.0 * 6.356013774871826
Epoch 570, val loss: 0.8795065879821777
Epoch 580, training loss: 6.697906017303467 = 0.34330639243125916 + 1.0 * 6.354599475860596
Epoch 580, val loss: 0.8798011541366577
Epoch 590, training loss: 6.676473140716553 = 0.3270687162876129 + 1.0 * 6.349404335021973
Epoch 590, val loss: 0.8807085752487183
Epoch 600, training loss: 6.665693283081055 = 0.311420202255249 + 1.0 * 6.354272842407227
Epoch 600, val loss: 0.8820741772651672
Epoch 610, training loss: 6.645326614379883 = 0.2963573932647705 + 1.0 * 6.348968982696533
Epoch 610, val loss: 0.884195864200592
Epoch 620, training loss: 6.629604339599609 = 0.28194910287857056 + 1.0 * 6.347655296325684
Epoch 620, val loss: 0.8867887258529663
Epoch 630, training loss: 6.6132588386535645 = 0.2680896818637848 + 1.0 * 6.3451690673828125
Epoch 630, val loss: 0.8899667263031006
Epoch 640, training loss: 6.599946022033691 = 0.2547842264175415 + 1.0 * 6.3451619148254395
Epoch 640, val loss: 0.8934902548789978
Epoch 650, training loss: 6.588899612426758 = 0.24206610023975372 + 1.0 * 6.3468337059021
Epoch 650, val loss: 0.8975524306297302
Epoch 660, training loss: 6.571298599243164 = 0.2299405038356781 + 1.0 * 6.341358184814453
Epoch 660, val loss: 0.9022020101547241
Epoch 670, training loss: 6.561395168304443 = 0.21834027767181396 + 1.0 * 6.34305477142334
Epoch 670, val loss: 0.9072206616401672
Epoch 680, training loss: 6.54945182800293 = 0.20728610455989838 + 1.0 * 6.342165946960449
Epoch 680, val loss: 0.9125106930732727
Epoch 690, training loss: 6.5324602127075195 = 0.19681811332702637 + 1.0 * 6.335641860961914
Epoch 690, val loss: 0.9185552597045898
Epoch 700, training loss: 6.521144866943359 = 0.1868465393781662 + 1.0 * 6.334298133850098
Epoch 700, val loss: 0.9248268008232117
Epoch 710, training loss: 6.524365425109863 = 0.17731307446956635 + 1.0 * 6.347052574157715
Epoch 710, val loss: 0.9314867854118347
Epoch 720, training loss: 6.503631591796875 = 0.16833318769931793 + 1.0 * 6.335298538208008
Epoch 720, val loss: 0.9384582042694092
Epoch 730, training loss: 6.496846675872803 = 0.15980859100818634 + 1.0 * 6.337038040161133
Epoch 730, val loss: 0.9458513259887695
Epoch 740, training loss: 6.481196403503418 = 0.15175694227218628 + 1.0 * 6.329439640045166
Epoch 740, val loss: 0.9535025358200073
Epoch 750, training loss: 6.474635124206543 = 0.14412342011928558 + 1.0 * 6.330511569976807
Epoch 750, val loss: 0.961446225643158
Epoch 760, training loss: 6.465003967285156 = 0.13687776029109955 + 1.0 * 6.328126430511475
Epoch 760, val loss: 0.9695360064506531
Epoch 770, training loss: 6.457280158996582 = 0.13001300394535065 + 1.0 * 6.327267169952393
Epoch 770, val loss: 0.9779955744743347
Epoch 780, training loss: 6.452850341796875 = 0.12353713065385818 + 1.0 * 6.329313278198242
Epoch 780, val loss: 0.9863966107368469
Epoch 790, training loss: 6.445495128631592 = 0.11745880544185638 + 1.0 * 6.328036308288574
Epoch 790, val loss: 0.9950899481773376
Epoch 800, training loss: 6.438392162322998 = 0.11172439903020859 + 1.0 * 6.326667785644531
Epoch 800, val loss: 1.0038936138153076
Epoch 810, training loss: 6.429346084594727 = 0.10631342977285385 + 1.0 * 6.323032855987549
Epoch 810, val loss: 1.0126818418502808
Epoch 820, training loss: 6.423437595367432 = 0.1011868491768837 + 1.0 * 6.322250843048096
Epoch 820, val loss: 1.0217524766921997
Epoch 830, training loss: 6.419057846069336 = 0.09636744111776352 + 1.0 * 6.322690486907959
Epoch 830, val loss: 1.0306618213653564
Epoch 840, training loss: 6.4170002937316895 = 0.09182824939489365 + 1.0 * 6.325171947479248
Epoch 840, val loss: 1.0397818088531494
Epoch 850, training loss: 6.406520366668701 = 0.08752591162919998 + 1.0 * 6.318994522094727
Epoch 850, val loss: 1.0488290786743164
Epoch 860, training loss: 6.401275634765625 = 0.0834868773818016 + 1.0 * 6.317788600921631
Epoch 860, val loss: 1.058043360710144
Epoch 870, training loss: 6.397122383117676 = 0.07967057079076767 + 1.0 * 6.3174519538879395
Epoch 870, val loss: 1.0671571493148804
Epoch 880, training loss: 6.396332263946533 = 0.0760759711265564 + 1.0 * 6.320256233215332
Epoch 880, val loss: 1.0762569904327393
Epoch 890, training loss: 6.391691207885742 = 0.07267557829618454 + 1.0 * 6.3190155029296875
Epoch 890, val loss: 1.0853654146194458
Epoch 900, training loss: 6.383231163024902 = 0.06947872042655945 + 1.0 * 6.3137526512146
Epoch 900, val loss: 1.0943628549575806
Epoch 910, training loss: 6.377490043640137 = 0.06645655632019043 + 1.0 * 6.311033725738525
Epoch 910, val loss: 1.1034537553787231
Epoch 920, training loss: 6.3744120597839355 = 0.0635949969291687 + 1.0 * 6.310817241668701
Epoch 920, val loss: 1.1124019622802734
Epoch 930, training loss: 6.381143093109131 = 0.06089518964290619 + 1.0 * 6.320248126983643
Epoch 930, val loss: 1.1211589574813843
Epoch 940, training loss: 6.3705244064331055 = 0.058334823697805405 + 1.0 * 6.31218957901001
Epoch 940, val loss: 1.1300729513168335
Epoch 950, training loss: 6.368688583374023 = 0.05593330040574074 + 1.0 * 6.312755107879639
Epoch 950, val loss: 1.1386213302612305
Epoch 960, training loss: 6.362166881561279 = 0.05368083715438843 + 1.0 * 6.308485984802246
Epoch 960, val loss: 1.147475242614746
Epoch 970, training loss: 6.358236789703369 = 0.05152711644768715 + 1.0 * 6.3067097663879395
Epoch 970, val loss: 1.1560472249984741
Epoch 980, training loss: 6.354137897491455 = 0.0494910292327404 + 1.0 * 6.304646968841553
Epoch 980, val loss: 1.1645504236221313
Epoch 990, training loss: 6.362728595733643 = 0.047561537474393845 + 1.0 * 6.31516695022583
Epoch 990, val loss: 1.1729286909103394
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 10.547367095947266 = 1.95050847530365 + 1.0 * 8.596858978271484
Epoch 0, val loss: 1.948038935661316
Epoch 10, training loss: 10.536835670471191 = 1.9401592016220093 + 1.0 * 8.59667682647705
Epoch 10, val loss: 1.9380828142166138
Epoch 20, training loss: 10.522964477539062 = 1.9276682138442993 + 1.0 * 8.595295906066895
Epoch 20, val loss: 1.925459623336792
Epoch 30, training loss: 10.493851661682129 = 1.910686731338501 + 1.0 * 8.583165168762207
Epoch 30, val loss: 1.9078595638275146
Epoch 40, training loss: 10.39157772064209 = 1.887959361076355 + 1.0 * 8.503618240356445
Epoch 40, val loss: 1.8845897912979126
Epoch 50, training loss: 10.015703201293945 = 1.8630540370941162 + 1.0 * 8.15264892578125
Epoch 50, val loss: 1.8602170944213867
Epoch 60, training loss: 9.575078964233398 = 1.8422304391860962 + 1.0 * 7.732848644256592
Epoch 60, val loss: 1.84080970287323
Epoch 70, training loss: 9.046540260314941 = 1.8265514373779297 + 1.0 * 7.219988822937012
Epoch 70, val loss: 1.8254013061523438
Epoch 80, training loss: 8.80756664276123 = 1.8122438192367554 + 1.0 * 6.995323181152344
Epoch 80, val loss: 1.8118376731872559
Epoch 90, training loss: 8.676593780517578 = 1.7962740659713745 + 1.0 * 6.880319595336914
Epoch 90, val loss: 1.7974159717559814
Epoch 100, training loss: 8.574546813964844 = 1.779403805732727 + 1.0 * 6.795142650604248
Epoch 100, val loss: 1.7824158668518066
Epoch 110, training loss: 8.496950149536133 = 1.762848138809204 + 1.0 * 6.734102249145508
Epoch 110, val loss: 1.7676787376403809
Epoch 120, training loss: 8.434762001037598 = 1.7460319995880127 + 1.0 * 6.688730239868164
Epoch 120, val loss: 1.7526960372924805
Epoch 130, training loss: 8.382137298583984 = 1.727531909942627 + 1.0 * 6.654605388641357
Epoch 130, val loss: 1.7363076210021973
Epoch 140, training loss: 8.334646224975586 = 1.7067371606826782 + 1.0 * 6.627909183502197
Epoch 140, val loss: 1.7182179689407349
Epoch 150, training loss: 8.287166595458984 = 1.6829955577850342 + 1.0 * 6.604171276092529
Epoch 150, val loss: 1.697840690612793
Epoch 160, training loss: 8.238783836364746 = 1.6557940244674683 + 1.0 * 6.582989692687988
Epoch 160, val loss: 1.674667477607727
Epoch 170, training loss: 8.191923141479492 = 1.6245627403259277 + 1.0 * 6.567360877990723
Epoch 170, val loss: 1.6482393741607666
Epoch 180, training loss: 8.14108943939209 = 1.588961124420166 + 1.0 * 6.552128314971924
Epoch 180, val loss: 1.6184107065200806
Epoch 190, training loss: 8.086978912353516 = 1.5489729642868042 + 1.0 * 6.53800630569458
Epoch 190, val loss: 1.584993600845337
Epoch 200, training loss: 8.032776832580566 = 1.5042446851730347 + 1.0 * 6.528532028198242
Epoch 200, val loss: 1.5479215383529663
Epoch 210, training loss: 7.972568035125732 = 1.4553793668746948 + 1.0 * 6.517188549041748
Epoch 210, val loss: 1.507898211479187
Epoch 220, training loss: 7.9109296798706055 = 1.4031423330307007 + 1.0 * 6.507787227630615
Epoch 220, val loss: 1.4653565883636475
Epoch 230, training loss: 7.850189208984375 = 1.348229169845581 + 1.0 * 6.501960277557373
Epoch 230, val loss: 1.4210807085037231
Epoch 240, training loss: 7.787135124206543 = 1.2922285795211792 + 1.0 * 6.494906425476074
Epoch 240, val loss: 1.376331090927124
Epoch 250, training loss: 7.724693775177002 = 1.2368264198303223 + 1.0 * 6.48786735534668
Epoch 250, val loss: 1.3328343629837036
Epoch 260, training loss: 7.663045883178711 = 1.1829031705856323 + 1.0 * 6.480142593383789
Epoch 260, val loss: 1.2911086082458496
Epoch 270, training loss: 7.603649616241455 = 1.1304696798324585 + 1.0 * 6.473179817199707
Epoch 270, val loss: 1.2511680126190186
Epoch 280, training loss: 7.5472846031188965 = 1.0796993970870972 + 1.0 * 6.46758508682251
Epoch 280, val loss: 1.2132676839828491
Epoch 290, training loss: 7.499794006347656 = 1.031144380569458 + 1.0 * 6.468649387359619
Epoch 290, val loss: 1.177881121635437
Epoch 300, training loss: 7.442727565765381 = 0.984993577003479 + 1.0 * 6.457734107971191
Epoch 300, val loss: 1.1450366973876953
Epoch 310, training loss: 7.392448425292969 = 0.9406939148902893 + 1.0 * 6.451754570007324
Epoch 310, val loss: 1.114287257194519
Epoch 320, training loss: 7.3465962409973145 = 0.8976459503173828 + 1.0 * 6.448950290679932
Epoch 320, val loss: 1.0853110551834106
Epoch 330, training loss: 7.300868511199951 = 0.8559860587120056 + 1.0 * 6.444882392883301
Epoch 330, val loss: 1.0579570531845093
Epoch 340, training loss: 7.253274917602539 = 0.8157732486724854 + 1.0 * 6.437501907348633
Epoch 340, val loss: 1.032247543334961
Epoch 350, training loss: 7.209527969360352 = 0.7764449715614319 + 1.0 * 6.4330830574035645
Epoch 350, val loss: 1.007839560508728
Epoch 360, training loss: 7.169260025024414 = 0.7378707528114319 + 1.0 * 6.431389331817627
Epoch 360, val loss: 0.9846050143241882
Epoch 370, training loss: 7.135855674743652 = 0.7002730369567871 + 1.0 * 6.435582637786865
Epoch 370, val loss: 0.9626730680465698
Epoch 380, training loss: 7.088644504547119 = 0.6641790270805359 + 1.0 * 6.424465656280518
Epoch 380, val loss: 0.9423855543136597
Epoch 390, training loss: 7.048256874084473 = 0.6292608976364136 + 1.0 * 6.4189958572387695
Epoch 390, val loss: 0.923556923866272
Epoch 400, training loss: 7.011115550994873 = 0.5955235362052917 + 1.0 * 6.415592193603516
Epoch 400, val loss: 0.9062799215316772
Epoch 410, training loss: 6.981426239013672 = 0.562997579574585 + 1.0 * 6.418428897857666
Epoch 410, val loss: 0.8906391263008118
Epoch 420, training loss: 6.945755481719971 = 0.5319886803627014 + 1.0 * 6.413766860961914
Epoch 420, val loss: 0.8767770528793335
Epoch 430, training loss: 6.909759044647217 = 0.5023501515388489 + 1.0 * 6.407408714294434
Epoch 430, val loss: 0.8645856976509094
Epoch 440, training loss: 6.8776655197143555 = 0.4738776385784149 + 1.0 * 6.403788089752197
Epoch 440, val loss: 0.8540117740631104
Epoch 450, training loss: 6.852614402770996 = 0.4464799463748932 + 1.0 * 6.406134605407715
Epoch 450, val loss: 0.8448633551597595
Epoch 460, training loss: 6.82234525680542 = 0.42027705907821655 + 1.0 * 6.402068138122559
Epoch 460, val loss: 0.8371719121932983
Epoch 470, training loss: 6.791554927825928 = 0.39520925283432007 + 1.0 * 6.396345615386963
Epoch 470, val loss: 0.8307999968528748
Epoch 480, training loss: 6.764355659484863 = 0.37112319469451904 + 1.0 * 6.393232345581055
Epoch 480, val loss: 0.8255519270896912
Epoch 490, training loss: 6.750240802764893 = 0.34801408648490906 + 1.0 * 6.40222692489624
Epoch 490, val loss: 0.8214152455329895
Epoch 500, training loss: 6.717639446258545 = 0.32603585720062256 + 1.0 * 6.391603469848633
Epoch 500, val loss: 0.8183861970901489
Epoch 510, training loss: 6.694530010223389 = 0.305252343416214 + 1.0 * 6.389277458190918
Epoch 510, val loss: 0.8164886236190796
Epoch 520, training loss: 6.676487445831299 = 0.2855343222618103 + 1.0 * 6.390953063964844
Epoch 520, val loss: 0.8154739141464233
Epoch 530, training loss: 6.651133060455322 = 0.2669312059879303 + 1.0 * 6.384202003479004
Epoch 530, val loss: 0.8153851628303528
Epoch 540, training loss: 6.629958629608154 = 0.2494436353445053 + 1.0 * 6.380515098571777
Epoch 540, val loss: 0.8162077069282532
Epoch 550, training loss: 6.612430095672607 = 0.23300547897815704 + 1.0 * 6.379424571990967
Epoch 550, val loss: 0.8177518248558044
Epoch 560, training loss: 6.6016526222229 = 0.21760576963424683 + 1.0 * 6.384047031402588
Epoch 560, val loss: 0.8200711011886597
Epoch 570, training loss: 6.579432487487793 = 0.2033066749572754 + 1.0 * 6.376125812530518
Epoch 570, val loss: 0.8230969905853271
Epoch 580, training loss: 6.5648369789123535 = 0.19007228314876556 + 1.0 * 6.374764919281006
Epoch 580, val loss: 0.8267862200737
Epoch 590, training loss: 6.548948764801025 = 0.17777155339717865 + 1.0 * 6.3711771965026855
Epoch 590, val loss: 0.8310799598693848
Epoch 600, training loss: 6.551228046417236 = 0.16636884212493896 + 1.0 * 6.384859085083008
Epoch 600, val loss: 0.8359630703926086
Epoch 610, training loss: 6.523725509643555 = 0.1558285355567932 + 1.0 * 6.367897033691406
Epoch 610, val loss: 0.8412724137306213
Epoch 620, training loss: 6.514170169830322 = 0.14610841870307922 + 1.0 * 6.368061542510986
Epoch 620, val loss: 0.8470613360404968
Epoch 630, training loss: 6.504857063293457 = 0.13711585104465485 + 1.0 * 6.367741107940674
Epoch 630, val loss: 0.8532520532608032
Epoch 640, training loss: 6.490673065185547 = 0.1288129836320877 + 1.0 * 6.361860275268555
Epoch 640, val loss: 0.8598528504371643
Epoch 650, training loss: 6.486018657684326 = 0.12112068384885788 + 1.0 * 6.364898204803467
Epoch 650, val loss: 0.8667850494384766
Epoch 660, training loss: 6.477170944213867 = 0.11400096118450165 + 1.0 * 6.363170146942139
Epoch 660, val loss: 0.8738229274749756
Epoch 670, training loss: 6.466070652008057 = 0.1074342429637909 + 1.0 * 6.358636379241943
Epoch 670, val loss: 0.8812708258628845
Epoch 680, training loss: 6.4567179679870605 = 0.10132462531328201 + 1.0 * 6.355393409729004
Epoch 680, val loss: 0.8888040781021118
Epoch 690, training loss: 6.452788352966309 = 0.09563878923654556 + 1.0 * 6.357149600982666
Epoch 690, val loss: 0.8965187668800354
Epoch 700, training loss: 6.4509477615356445 = 0.09035695344209671 + 1.0 * 6.360590934753418
Epoch 700, val loss: 0.9044484496116638
Epoch 710, training loss: 6.442779064178467 = 0.0854795053601265 + 1.0 * 6.357299327850342
Epoch 710, val loss: 0.912379264831543
Epoch 720, training loss: 6.434756755828857 = 0.08097025752067566 + 1.0 * 6.353786468505859
Epoch 720, val loss: 0.9204148650169373
Epoch 730, training loss: 6.425512790679932 = 0.07676420360803604 + 1.0 * 6.348748683929443
Epoch 730, val loss: 0.9285112023353577
Epoch 740, training loss: 6.422378063201904 = 0.07283773273229599 + 1.0 * 6.3495402336120605
Epoch 740, val loss: 0.9365953207015991
Epoch 750, training loss: 6.4152703285217285 = 0.06918005645275116 + 1.0 * 6.346090316772461
Epoch 750, val loss: 0.9447600245475769
Epoch 760, training loss: 6.413407802581787 = 0.06577116250991821 + 1.0 * 6.347636699676514
Epoch 760, val loss: 0.9529585242271423
Epoch 770, training loss: 6.407619953155518 = 0.0625847801566124 + 1.0 * 6.345035076141357
Epoch 770, val loss: 0.9610938429832458
Epoch 780, training loss: 6.405361652374268 = 0.05962198227643967 + 1.0 * 6.345739841461182
Epoch 780, val loss: 0.9691933989524841
Epoch 790, training loss: 6.399435997009277 = 0.05685177817940712 + 1.0 * 6.342584133148193
Epoch 790, val loss: 0.9772945642471313
Epoch 800, training loss: 6.394678115844727 = 0.05425313860177994 + 1.0 * 6.34042501449585
Epoch 800, val loss: 0.9853473901748657
Epoch 810, training loss: 6.396418571472168 = 0.051811475306749344 + 1.0 * 6.344606876373291
Epoch 810, val loss: 0.9933018684387207
Epoch 820, training loss: 6.388704776763916 = 0.049535684287548065 + 1.0 * 6.339169025421143
Epoch 820, val loss: 1.0012470483779907
Epoch 830, training loss: 6.383764743804932 = 0.04739784821867943 + 1.0 * 6.336367130279541
Epoch 830, val loss: 1.0091158151626587
Epoch 840, training loss: 6.380618572235107 = 0.045386604964733124 + 1.0 * 6.335231781005859
Epoch 840, val loss: 1.0168484449386597
Epoch 850, training loss: 6.3848748207092285 = 0.04349042475223541 + 1.0 * 6.341384410858154
Epoch 850, val loss: 1.0245436429977417
Epoch 860, training loss: 6.382114410400391 = 0.04170873016119003 + 1.0 * 6.340405464172363
Epoch 860, val loss: 1.0321999788284302
Epoch 870, training loss: 6.371616363525391 = 0.04003951698541641 + 1.0 * 6.331576824188232
Epoch 870, val loss: 1.0396983623504639
Epoch 880, training loss: 6.36907434463501 = 0.038465168327093124 + 1.0 * 6.330609321594238
Epoch 880, val loss: 1.0471798181533813
Epoch 890, training loss: 6.366404056549072 = 0.03697275370359421 + 1.0 * 6.329431533813477
Epoch 890, val loss: 1.0545850992202759
Epoch 900, training loss: 6.374098300933838 = 0.035560205578804016 + 1.0 * 6.33853816986084
Epoch 900, val loss: 1.0618677139282227
Epoch 910, training loss: 6.361607551574707 = 0.034238532185554504 + 1.0 * 6.327369213104248
Epoch 910, val loss: 1.0691348314285278
Epoch 920, training loss: 6.358443737030029 = 0.03298337757587433 + 1.0 * 6.325460433959961
Epoch 920, val loss: 1.0763053894042969
Epoch 930, training loss: 6.357461452484131 = 0.0317918136715889 + 1.0 * 6.325669765472412
Epoch 930, val loss: 1.0833430290222168
Epoch 940, training loss: 6.360085487365723 = 0.030662821605801582 + 1.0 * 6.329422473907471
Epoch 940, val loss: 1.0902690887451172
Epoch 950, training loss: 6.359787940979004 = 0.029592592269182205 + 1.0 * 6.330195426940918
Epoch 950, val loss: 1.0971041917800903
Epoch 960, training loss: 6.353312969207764 = 0.028583424165844917 + 1.0 * 6.3247294425964355
Epoch 960, val loss: 1.1039528846740723
Epoch 970, training loss: 6.34909725189209 = 0.027625948190689087 + 1.0 * 6.321471214294434
Epoch 970, val loss: 1.1106231212615967
Epoch 980, training loss: 6.34905481338501 = 0.026711691170930862 + 1.0 * 6.322343349456787
Epoch 980, val loss: 1.1172422170639038
Epoch 990, training loss: 6.349878311157227 = 0.025841129943728447 + 1.0 * 6.324037075042725
Epoch 990, val loss: 1.1237201690673828
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 10.555334091186523 = 1.958484172821045 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.951431155204773
Epoch 10, training loss: 10.544605255126953 = 1.947923183441162 + 1.0 * 8.59668254852295
Epoch 10, val loss: 1.9413994550704956
Epoch 20, training loss: 10.530707359313965 = 1.9353972673416138 + 1.0 * 8.59531021118164
Epoch 20, val loss: 1.9290382862091064
Epoch 30, training loss: 10.502198219299316 = 1.9184696674346924 + 1.0 * 8.583728790283203
Epoch 30, val loss: 1.9118369817733765
Epoch 40, training loss: 10.416528701782227 = 1.8955376148223877 + 1.0 * 8.520991325378418
Epoch 40, val loss: 1.8889548778533936
Epoch 50, training loss: 10.089757919311523 = 1.8719263076782227 + 1.0 * 8.2178316116333
Epoch 50, val loss: 1.8663852214813232
Epoch 60, training loss: 9.785236358642578 = 1.8504891395568848 + 1.0 * 7.934747695922852
Epoch 60, val loss: 1.846968173980713
Epoch 70, training loss: 9.342598915100098 = 1.8333786725997925 + 1.0 * 7.509220123291016
Epoch 70, val loss: 1.8316168785095215
Epoch 80, training loss: 9.07303237915039 = 1.8164937496185303 + 1.0 * 7.2565388679504395
Epoch 80, val loss: 1.8161306381225586
Epoch 90, training loss: 8.865418434143066 = 1.8004305362701416 + 1.0 * 7.064988136291504
Epoch 90, val loss: 1.8014931678771973
Epoch 100, training loss: 8.721272468566895 = 1.7851907014846802 + 1.0 * 6.936081409454346
Epoch 100, val loss: 1.7882874011993408
Epoch 110, training loss: 8.606827735900879 = 1.7699581384658813 + 1.0 * 6.836869239807129
Epoch 110, val loss: 1.7757834196090698
Epoch 120, training loss: 8.512810707092285 = 1.7544766664505005 + 1.0 * 6.758333683013916
Epoch 120, val loss: 1.7628915309906006
Epoch 130, training loss: 8.428686141967773 = 1.737767219543457 + 1.0 * 6.690919399261475
Epoch 130, val loss: 1.7486696243286133
Epoch 140, training loss: 8.361429214477539 = 1.7190124988555908 + 1.0 * 6.642416477203369
Epoch 140, val loss: 1.7325557470321655
Epoch 150, training loss: 8.304802894592285 = 1.6976603269577026 + 1.0 * 6.607142448425293
Epoch 150, val loss: 1.714267611503601
Epoch 160, training loss: 8.253396034240723 = 1.6734318733215332 + 1.0 * 6.5799641609191895
Epoch 160, val loss: 1.6935900449752808
Epoch 170, training loss: 8.203163146972656 = 1.6458383798599243 + 1.0 * 6.557324409484863
Epoch 170, val loss: 1.6700973510742188
Epoch 180, training loss: 8.153854370117188 = 1.6143484115600586 + 1.0 * 6.539506435394287
Epoch 180, val loss: 1.6433508396148682
Epoch 190, training loss: 8.112101554870605 = 1.5786042213439941 + 1.0 * 6.533497333526611
Epoch 190, val loss: 1.6133801937103271
Epoch 200, training loss: 8.055024147033691 = 1.5395852327346802 + 1.0 * 6.515438556671143
Epoch 200, val loss: 1.5810383558273315
Epoch 210, training loss: 7.999178886413574 = 1.4972717761993408 + 1.0 * 6.5019073486328125
Epoch 210, val loss: 1.5466095209121704
Epoch 220, training loss: 7.943828582763672 = 1.4519842863082886 + 1.0 * 6.491844177246094
Epoch 220, val loss: 1.5106747150421143
Epoch 230, training loss: 7.886956691741943 = 1.4042640924453735 + 1.0 * 6.482692718505859
Epoch 230, val loss: 1.473897933959961
Epoch 240, training loss: 7.837568759918213 = 1.355242371559143 + 1.0 * 6.482326507568359
Epoch 240, val loss: 1.4373033046722412
Epoch 250, training loss: 7.774979591369629 = 1.3064545392990112 + 1.0 * 6.468524932861328
Epoch 250, val loss: 1.4022358655929565
Epoch 260, training loss: 7.719019889831543 = 1.2575881481170654 + 1.0 * 6.461431980133057
Epoch 260, val loss: 1.36823570728302
Epoch 270, training loss: 7.663264274597168 = 1.2084662914276123 + 1.0 * 6.454798221588135
Epoch 270, val loss: 1.3347615003585815
Epoch 280, training loss: 7.625145435333252 = 1.1592689752578735 + 1.0 * 6.465876579284668
Epoch 280, val loss: 1.3019267320632935
Epoch 290, training loss: 7.560450553894043 = 1.1108763217926025 + 1.0 * 6.449573993682861
Epoch 290, val loss: 1.2703205347061157
Epoch 300, training loss: 7.503715515136719 = 1.0629886388778687 + 1.0 * 6.4407267570495605
Epoch 300, val loss: 1.239444613456726
Epoch 310, training loss: 7.450839042663574 = 1.0154495239257812 + 1.0 * 6.435389518737793
Epoch 310, val loss: 1.2087862491607666
Epoch 320, training loss: 7.399360179901123 = 0.9684769511222839 + 1.0 * 6.430883407592773
Epoch 320, val loss: 1.178597092628479
Epoch 330, training loss: 7.35800313949585 = 0.9225253462791443 + 1.0 * 6.4354777336120605
Epoch 330, val loss: 1.1491692066192627
Epoch 340, training loss: 7.3061017990112305 = 0.8787003755569458 + 1.0 * 6.427401542663574
Epoch 340, val loss: 1.1214216947555542
Epoch 350, training loss: 7.256236553192139 = 0.837384045124054 + 1.0 * 6.41885232925415
Epoch 350, val loss: 1.0958774089813232
Epoch 360, training loss: 7.214121341705322 = 0.7984751462936401 + 1.0 * 6.415646076202393
Epoch 360, val loss: 1.0722988843917847
Epoch 370, training loss: 7.18949556350708 = 0.7622566819190979 + 1.0 * 6.427238941192627
Epoch 370, val loss: 1.050764799118042
Epoch 380, training loss: 7.142001628875732 = 0.729369580745697 + 1.0 * 6.412631988525391
Epoch 380, val loss: 1.0325112342834473
Epoch 390, training loss: 7.1060051918029785 = 0.698573887348175 + 1.0 * 6.407431125640869
Epoch 390, val loss: 1.016000747680664
Epoch 400, training loss: 7.071817398071289 = 0.6692426800727844 + 1.0 * 6.40257453918457
Epoch 400, val loss: 1.0010462999343872
Epoch 410, training loss: 7.039641857147217 = 0.6409350633621216 + 1.0 * 6.398706912994385
Epoch 410, val loss: 0.9872956275939941
Epoch 420, training loss: 7.008576393127441 = 0.6133091449737549 + 1.0 * 6.395267009735107
Epoch 420, val loss: 0.9746894836425781
Epoch 430, training loss: 6.979345321655273 = 0.5861660242080688 + 1.0 * 6.393179416656494
Epoch 430, val loss: 0.9629963040351868
Epoch 440, training loss: 6.95766544342041 = 0.5596375465393066 + 1.0 * 6.3980278968811035
Epoch 440, val loss: 0.9520808458328247
Epoch 450, training loss: 6.922740936279297 = 0.5336506962776184 + 1.0 * 6.389090061187744
Epoch 450, val loss: 0.9428587555885315
Epoch 460, training loss: 6.892611026763916 = 0.5079589486122131 + 1.0 * 6.384652137756348
Epoch 460, val loss: 0.9343957901000977
Epoch 470, training loss: 6.86452054977417 = 0.48248574137687683 + 1.0 * 6.382034778594971
Epoch 470, val loss: 0.9269893765449524
Epoch 480, training loss: 6.839984893798828 = 0.4572890102863312 + 1.0 * 6.38269567489624
Epoch 480, val loss: 0.9206723570823669
Epoch 490, training loss: 6.817748069763184 = 0.43253520131111145 + 1.0 * 6.3852128982543945
Epoch 490, val loss: 0.9153009057044983
Epoch 500, training loss: 6.784682273864746 = 0.40873029828071594 + 1.0 * 6.375951766967773
Epoch 500, val loss: 0.9115708470344543
Epoch 510, training loss: 6.759183406829834 = 0.38564199209213257 + 1.0 * 6.373541355133057
Epoch 510, val loss: 0.9090598821640015
Epoch 520, training loss: 6.734421730041504 = 0.363336443901062 + 1.0 * 6.371085166931152
Epoch 520, val loss: 0.907853901386261
Epoch 530, training loss: 6.72005033493042 = 0.34187543392181396 + 1.0 * 6.378174781799316
Epoch 530, val loss: 0.9078232049942017
Epoch 540, training loss: 6.692328929901123 = 0.32151663303375244 + 1.0 * 6.37081241607666
Epoch 540, val loss: 0.909089207649231
Epoch 550, training loss: 6.668060302734375 = 0.30236124992370605 + 1.0 * 6.365699291229248
Epoch 550, val loss: 0.9116485118865967
Epoch 560, training loss: 6.6494059562683105 = 0.28426992893218994 + 1.0 * 6.36513614654541
Epoch 560, val loss: 0.9153630137443542
Epoch 570, training loss: 6.62883996963501 = 0.2671643793582916 + 1.0 * 6.36167573928833
Epoch 570, val loss: 0.920013964176178
Epoch 580, training loss: 6.622174263000488 = 0.25105974078178406 + 1.0 * 6.371114730834961
Epoch 580, val loss: 0.9256772398948669
Epoch 590, training loss: 6.599426746368408 = 0.2359301745891571 + 1.0 * 6.363496780395508
Epoch 590, val loss: 0.93168044090271
Epoch 600, training loss: 6.579646587371826 = 0.22190748155117035 + 1.0 * 6.357738971710205
Epoch 600, val loss: 0.9388019442558289
Epoch 610, training loss: 6.5640339851379395 = 0.20870403945446014 + 1.0 * 6.355329990386963
Epoch 610, val loss: 0.9464136362075806
Epoch 620, training loss: 6.549836158752441 = 0.19628788530826569 + 1.0 * 6.353548049926758
Epoch 620, val loss: 0.9546234011650085
Epoch 630, training loss: 6.5485429763793945 = 0.18465200066566467 + 1.0 * 6.363891124725342
Epoch 630, val loss: 0.9632118344306946
Epoch 640, training loss: 6.528404235839844 = 0.17381498217582703 + 1.0 * 6.354589462280273
Epoch 640, val loss: 0.9721987247467041
Epoch 650, training loss: 6.514081954956055 = 0.16369380056858063 + 1.0 * 6.350388050079346
Epoch 650, val loss: 0.9814744591712952
Epoch 660, training loss: 6.516287326812744 = 0.1541975438594818 + 1.0 * 6.36208963394165
Epoch 660, val loss: 0.9909011125564575
Epoch 670, training loss: 6.4980034828186035 = 0.14541254937648773 + 1.0 * 6.352591037750244
Epoch 670, val loss: 1.0007988214492798
Epoch 680, training loss: 6.482694625854492 = 0.13716477155685425 + 1.0 * 6.345530033111572
Epoch 680, val loss: 1.010583758354187
Epoch 690, training loss: 6.472872734069824 = 0.12946343421936035 + 1.0 * 6.343409538269043
Epoch 690, val loss: 1.0206998586654663
Epoch 700, training loss: 6.470434188842773 = 0.12223007529973984 + 1.0 * 6.348204135894775
Epoch 700, val loss: 1.0308207273483276
Epoch 710, training loss: 6.459805488586426 = 0.11549735814332962 + 1.0 * 6.344307899475098
Epoch 710, val loss: 1.0411815643310547
Epoch 720, training loss: 6.450470924377441 = 0.10920342057943344 + 1.0 * 6.3412675857543945
Epoch 720, val loss: 1.0514039993286133
Epoch 730, training loss: 6.447996139526367 = 0.10332389920949936 + 1.0 * 6.344672203063965
Epoch 730, val loss: 1.0614516735076904
Epoch 740, training loss: 6.437896251678467 = 0.09782737493515015 + 1.0 * 6.340068817138672
Epoch 740, val loss: 1.071651577949524
Epoch 750, training loss: 6.430291175842285 = 0.0927155539393425 + 1.0 * 6.337575435638428
Epoch 750, val loss: 1.0816725492477417
Epoch 760, training loss: 6.422672271728516 = 0.08793434500694275 + 1.0 * 6.334737777709961
Epoch 760, val loss: 1.091909408569336
Epoch 770, training loss: 6.4171142578125 = 0.08344881981611252 + 1.0 * 6.333665370941162
Epoch 770, val loss: 1.1019316911697388
Epoch 780, training loss: 6.411899566650391 = 0.07924370467662811 + 1.0 * 6.332655906677246
Epoch 780, val loss: 1.1120197772979736
Epoch 790, training loss: 6.420415878295898 = 0.07530950754880905 + 1.0 * 6.345106601715088
Epoch 790, val loss: 1.1218115091323853
Epoch 800, training loss: 6.403594970703125 = 0.07163013517856598 + 1.0 * 6.33196496963501
Epoch 800, val loss: 1.131502628326416
Epoch 810, training loss: 6.397750377655029 = 0.06822080910205841 + 1.0 * 6.329529762268066
Epoch 810, val loss: 1.1411970853805542
Epoch 820, training loss: 6.392455101013184 = 0.0650036633014679 + 1.0 * 6.327451229095459
Epoch 820, val loss: 1.1507558822631836
Epoch 830, training loss: 6.3889641761779785 = 0.061975687742233276 + 1.0 * 6.326988697052002
Epoch 830, val loss: 1.1603213548660278
Epoch 840, training loss: 6.397187232971191 = 0.05912225320935249 + 1.0 * 6.338065147399902
Epoch 840, val loss: 1.1695425510406494
Epoch 850, training loss: 6.384720802307129 = 0.056469984352588654 + 1.0 * 6.328250885009766
Epoch 850, val loss: 1.17911696434021
Epoch 860, training loss: 6.37907600402832 = 0.05395781621336937 + 1.0 * 6.325118064880371
Epoch 860, val loss: 1.1882503032684326
Epoch 870, training loss: 6.379177570343018 = 0.05160306394100189 + 1.0 * 6.327574729919434
Epoch 870, val loss: 1.1974332332611084
Epoch 880, training loss: 6.374821186065674 = 0.04937965050339699 + 1.0 * 6.325441360473633
Epoch 880, val loss: 1.2062304019927979
Epoch 890, training loss: 6.369461536407471 = 0.04729261249303818 + 1.0 * 6.322168827056885
Epoch 890, val loss: 1.215205430984497
Epoch 900, training loss: 6.3677144050598145 = 0.045322928577661514 + 1.0 * 6.322391510009766
Epoch 900, val loss: 1.223829746246338
Epoch 910, training loss: 6.364978790283203 = 0.04346884787082672 + 1.0 * 6.321509838104248
Epoch 910, val loss: 1.23253333568573
Epoch 920, training loss: 6.378101825714111 = 0.04172338545322418 + 1.0 * 6.336378574371338
Epoch 920, val loss: 1.2410653829574585
Epoch 930, training loss: 6.361944198608398 = 0.04006420820951462 + 1.0 * 6.321879863739014
Epoch 930, val loss: 1.249051809310913
Epoch 940, training loss: 6.3560791015625 = 0.03852048143744469 + 1.0 * 6.317558765411377
Epoch 940, val loss: 1.2575597763061523
Epoch 950, training loss: 6.352497100830078 = 0.03705042600631714 + 1.0 * 6.315446853637695
Epoch 950, val loss: 1.2655341625213623
Epoch 960, training loss: 6.350249290466309 = 0.035656314343214035 + 1.0 * 6.3145928382873535
Epoch 960, val loss: 1.2735790014266968
Epoch 970, training loss: 6.3508429527282715 = 0.034332241863012314 + 1.0 * 6.3165106773376465
Epoch 970, val loss: 1.2814524173736572
Epoch 980, training loss: 6.3504791259765625 = 0.03307386860251427 + 1.0 * 6.3174052238464355
Epoch 980, val loss: 1.2890836000442505
Epoch 990, training loss: 6.351287364959717 = 0.03189079090952873 + 1.0 * 6.319396495819092
Epoch 990, val loss: 1.2966645956039429
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8102266736953084
The final CL Acc:0.76914, 0.00175, The final GNN Acc:0.80917, 0.00149
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13264])
remove edge: torch.Size([2, 7910])
updated graph: torch.Size([2, 10618])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.550193786621094 = 1.9533438682556152 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.9597899913787842
Epoch 10, training loss: 10.539705276489258 = 1.9430863857269287 + 1.0 * 8.59661865234375
Epoch 10, val loss: 1.9493223428726196
Epoch 20, training loss: 10.5250825881958 = 1.9305579662322998 + 1.0 * 8.594524383544922
Epoch 20, val loss: 1.9364436864852905
Epoch 30, training loss: 10.49155330657959 = 1.9130933284759521 + 1.0 * 8.578459739685059
Epoch 30, val loss: 1.9182060956954956
Epoch 40, training loss: 10.391541481018066 = 1.889512062072754 + 1.0 * 8.502029418945312
Epoch 40, val loss: 1.8941829204559326
Epoch 50, training loss: 10.082603454589844 = 1.863216757774353 + 1.0 * 8.21938705444336
Epoch 50, val loss: 1.8682576417922974
Epoch 60, training loss: 9.874312400817871 = 1.838651418685913 + 1.0 * 8.035660743713379
Epoch 60, val loss: 1.8457977771759033
Epoch 70, training loss: 9.445830345153809 = 1.8181945085525513 + 1.0 * 7.627635955810547
Epoch 70, val loss: 1.8271924257278442
Epoch 80, training loss: 9.05579662322998 = 1.80231511592865 + 1.0 * 7.253481864929199
Epoch 80, val loss: 1.811814546585083
Epoch 90, training loss: 8.85151195526123 = 1.786126732826233 + 1.0 * 7.065384864807129
Epoch 90, val loss: 1.7951264381408691
Epoch 100, training loss: 8.701505661010742 = 1.7672755718231201 + 1.0 * 6.934229850769043
Epoch 100, val loss: 1.776634931564331
Epoch 110, training loss: 8.587028503417969 = 1.747714638710022 + 1.0 * 6.839313983917236
Epoch 110, val loss: 1.7579171657562256
Epoch 120, training loss: 8.490397453308105 = 1.727209448814392 + 1.0 * 6.763187885284424
Epoch 120, val loss: 1.7381632328033447
Epoch 130, training loss: 8.412175178527832 = 1.70460045337677 + 1.0 * 6.707574844360352
Epoch 130, val loss: 1.7168039083480835
Epoch 140, training loss: 8.345516204833984 = 1.6786220073699951 + 1.0 * 6.66689395904541
Epoch 140, val loss: 1.693142294883728
Epoch 150, training loss: 8.287111282348633 = 1.6483803987503052 + 1.0 * 6.638731002807617
Epoch 150, val loss: 1.6663553714752197
Epoch 160, training loss: 8.229958534240723 = 1.613672137260437 + 1.0 * 6.616286754608154
Epoch 160, val loss: 1.6359443664550781
Epoch 170, training loss: 8.172294616699219 = 1.5741595029830933 + 1.0 * 6.598134994506836
Epoch 170, val loss: 1.6014591455459595
Epoch 180, training loss: 8.113219261169434 = 1.5295159816741943 + 1.0 * 6.583703517913818
Epoch 180, val loss: 1.562733769416809
Epoch 190, training loss: 8.047261238098145 = 1.4808086156845093 + 1.0 * 6.566452503204346
Epoch 190, val loss: 1.5208029747009277
Epoch 200, training loss: 7.978366851806641 = 1.4286878108978271 + 1.0 * 6.549679279327393
Epoch 200, val loss: 1.4765255451202393
Epoch 210, training loss: 7.909867286682129 = 1.373541235923767 + 1.0 * 6.536325931549072
Epoch 210, val loss: 1.430212378501892
Epoch 220, training loss: 7.839407920837402 = 1.317528486251831 + 1.0 * 6.52187967300415
Epoch 220, val loss: 1.3838016986846924
Epoch 230, training loss: 7.771945476531982 = 1.2611006498336792 + 1.0 * 6.510844707489014
Epoch 230, val loss: 1.3374943733215332
Epoch 240, training loss: 7.703141212463379 = 1.204331398010254 + 1.0 * 6.498809814453125
Epoch 240, val loss: 1.2911431789398193
Epoch 250, training loss: 7.637645244598389 = 1.1479077339172363 + 1.0 * 6.489737510681152
Epoch 250, val loss: 1.2454091310501099
Epoch 260, training loss: 7.575655460357666 = 1.093652606010437 + 1.0 * 6.4820027351379395
Epoch 260, val loss: 1.2013736963272095
Epoch 270, training loss: 7.514385223388672 = 1.0415081977844238 + 1.0 * 6.472877025604248
Epoch 270, val loss: 1.1591181755065918
Epoch 280, training loss: 7.46144962310791 = 0.9918705821037292 + 1.0 * 6.469579219818115
Epoch 280, val loss: 1.1188710927963257
Epoch 290, training loss: 7.405659198760986 = 0.9453615546226501 + 1.0 * 6.460297584533691
Epoch 290, val loss: 1.0812474489212036
Epoch 300, training loss: 7.357820987701416 = 0.9024427533149719 + 1.0 * 6.45537805557251
Epoch 300, val loss: 1.0464893579483032
Epoch 310, training loss: 7.3103156089782715 = 0.8627298474311829 + 1.0 * 6.447585582733154
Epoch 310, val loss: 1.0144795179367065
Epoch 320, training loss: 7.266684532165527 = 0.8257104158401489 + 1.0 * 6.440974235534668
Epoch 320, val loss: 0.9849148392677307
Epoch 330, training loss: 7.228255271911621 = 0.7910021543502808 + 1.0 * 6.437252998352051
Epoch 330, val loss: 0.9575985670089722
Epoch 340, training loss: 7.19492769241333 = 0.7588602304458618 + 1.0 * 6.436067581176758
Epoch 340, val loss: 0.9326436519622803
Epoch 350, training loss: 7.159133434295654 = 0.7288791537284851 + 1.0 * 6.4302544593811035
Epoch 350, val loss: 0.9103017449378967
Epoch 360, training loss: 7.125728130340576 = 0.7008355259895325 + 1.0 * 6.424892425537109
Epoch 360, val loss: 0.8900725245475769
Epoch 370, training loss: 7.092541217803955 = 0.6741269826889038 + 1.0 * 6.418414115905762
Epoch 370, val loss: 0.8718240857124329
Epoch 380, training loss: 7.0705695152282715 = 0.6485158801078796 + 1.0 * 6.422053813934326
Epoch 380, val loss: 0.8551995754241943
Epoch 390, training loss: 7.034789562225342 = 0.6239568591117859 + 1.0 * 6.41083288192749
Epoch 390, val loss: 0.840300977230072
Epoch 400, training loss: 7.007437705993652 = 0.6003115773200989 + 1.0 * 6.407125949859619
Epoch 400, val loss: 0.8270236253738403
Epoch 410, training loss: 6.981460094451904 = 0.5772933959960938 + 1.0 * 6.4041666984558105
Epoch 410, val loss: 0.8151235580444336
Epoch 420, training loss: 6.958650588989258 = 0.5550945997238159 + 1.0 * 6.403555870056152
Epoch 420, val loss: 0.8044884204864502
Epoch 430, training loss: 6.932186603546143 = 0.5335630774497986 + 1.0 * 6.398623466491699
Epoch 430, val loss: 0.795113742351532
Epoch 440, training loss: 6.906310081481934 = 0.5126270055770874 + 1.0 * 6.393682956695557
Epoch 440, val loss: 0.7867475152015686
Epoch 450, training loss: 6.882407188415527 = 0.49212077260017395 + 1.0 * 6.390286445617676
Epoch 450, val loss: 0.7791320085525513
Epoch 460, training loss: 6.858049392700195 = 0.47181105613708496 + 1.0 * 6.386238098144531
Epoch 460, val loss: 0.7720681428909302
Epoch 470, training loss: 6.837740421295166 = 0.4516189992427826 + 1.0 * 6.3861212730407715
Epoch 470, val loss: 0.7653842568397522
Epoch 480, training loss: 6.822202682495117 = 0.4315941631793976 + 1.0 * 6.390608310699463
Epoch 480, val loss: 0.7590411901473999
Epoch 490, training loss: 6.792481422424316 = 0.4118572473526001 + 1.0 * 6.380624294281006
Epoch 490, val loss: 0.7530426383018494
Epoch 500, training loss: 6.768807411193848 = 0.39219412207603455 + 1.0 * 6.376613140106201
Epoch 500, val loss: 0.7475217580795288
Epoch 510, training loss: 6.757399082183838 = 0.3728526830673218 + 1.0 * 6.384546279907227
Epoch 510, val loss: 0.7424845099449158
Epoch 520, training loss: 6.725687503814697 = 0.35406696796417236 + 1.0 * 6.3716206550598145
Epoch 520, val loss: 0.7380579113960266
Epoch 530, training loss: 6.705857753753662 = 0.3359454572200775 + 1.0 * 6.369912147521973
Epoch 530, val loss: 0.7342957854270935
Epoch 540, training loss: 6.695199489593506 = 0.3185516595840454 + 1.0 * 6.37664794921875
Epoch 540, val loss: 0.7313085794448853
Epoch 550, training loss: 6.671814918518066 = 0.30195340514183044 + 1.0 * 6.369861602783203
Epoch 550, val loss: 0.7291567921638489
Epoch 560, training loss: 6.650417804718018 = 0.286251038312912 + 1.0 * 6.364166736602783
Epoch 560, val loss: 0.7275061011314392
Epoch 570, training loss: 6.632267475128174 = 0.2711797058582306 + 1.0 * 6.361087799072266
Epoch 570, val loss: 0.7265557050704956
Epoch 580, training loss: 6.615838050842285 = 0.25664952397346497 + 1.0 * 6.359188556671143
Epoch 580, val loss: 0.7260683178901672
Epoch 590, training loss: 6.604732036590576 = 0.24258078634738922 + 1.0 * 6.362151145935059
Epoch 590, val loss: 0.7259601354598999
Epoch 600, training loss: 6.591421127319336 = 0.2290288358926773 + 1.0 * 6.362392425537109
Epoch 600, val loss: 0.7261069416999817
Epoch 610, training loss: 6.571864128112793 = 0.21588607132434845 + 1.0 * 6.355978012084961
Epoch 610, val loss: 0.7264221906661987
Epoch 620, training loss: 6.558742046356201 = 0.2030661404132843 + 1.0 * 6.35567569732666
Epoch 620, val loss: 0.7270714044570923
Epoch 630, training loss: 6.543952941894531 = 0.19063392281532288 + 1.0 * 6.35331916809082
Epoch 630, val loss: 0.7278881669044495
Epoch 640, training loss: 6.530023097991943 = 0.17863810062408447 + 1.0 * 6.351385116577148
Epoch 640, val loss: 0.7288846373558044
Epoch 650, training loss: 6.515758514404297 = 0.16710637509822845 + 1.0 * 6.348652362823486
Epoch 650, val loss: 0.7302136421203613
Epoch 660, training loss: 6.510932922363281 = 0.1560637205839157 + 1.0 * 6.354869365692139
Epoch 660, val loss: 0.7319015264511108
Epoch 670, training loss: 6.494843482971191 = 0.14568108320236206 + 1.0 * 6.349162578582764
Epoch 670, val loss: 0.7338797450065613
Epoch 680, training loss: 6.480752468109131 = 0.13597260415554047 + 1.0 * 6.344779968261719
Epoch 680, val loss: 0.7361695170402527
Epoch 690, training loss: 6.470141410827637 = 0.12691223621368408 + 1.0 * 6.343229293823242
Epoch 690, val loss: 0.7389867901802063
Epoch 700, training loss: 6.4636454582214355 = 0.11850903183221817 + 1.0 * 6.345136642456055
Epoch 700, val loss: 0.742232084274292
Epoch 710, training loss: 6.453981399536133 = 0.11081425100564957 + 1.0 * 6.343167304992676
Epoch 710, val loss: 0.7458434104919434
Epoch 720, training loss: 6.444451332092285 = 0.10376264899969101 + 1.0 * 6.340688705444336
Epoch 720, val loss: 0.7497731447219849
Epoch 730, training loss: 6.434521198272705 = 0.09732739627361298 + 1.0 * 6.337193965911865
Epoch 730, val loss: 0.7540051341056824
Epoch 740, training loss: 6.431064605712891 = 0.09142285585403442 + 1.0 * 6.339641571044922
Epoch 740, val loss: 0.7586617469787598
Epoch 750, training loss: 6.421933174133301 = 0.08601333200931549 + 1.0 * 6.3359198570251465
Epoch 750, val loss: 0.763576328754425
Epoch 760, training loss: 6.414754390716553 = 0.08107205480337143 + 1.0 * 6.333682537078857
Epoch 760, val loss: 0.7685825824737549
Epoch 770, training loss: 6.413919448852539 = 0.07653944939374924 + 1.0 * 6.3373799324035645
Epoch 770, val loss: 0.773827850818634
Epoch 780, training loss: 6.405999660491943 = 0.07237499952316284 + 1.0 * 6.333624839782715
Epoch 780, val loss: 0.7792161703109741
Epoch 790, training loss: 6.399138450622559 = 0.06853378564119339 + 1.0 * 6.330604553222656
Epoch 790, val loss: 0.7846532464027405
Epoch 800, training loss: 6.407262325286865 = 0.06498155742883682 + 1.0 * 6.342280864715576
Epoch 800, val loss: 0.7902394533157349
Epoch 810, training loss: 6.394192695617676 = 0.06170571967959404 + 1.0 * 6.332487106323242
Epoch 810, val loss: 0.7959548234939575
Epoch 820, training loss: 6.386211395263672 = 0.0586765892803669 + 1.0 * 6.3275346755981445
Epoch 820, val loss: 0.801426887512207
Epoch 830, training loss: 6.38113260269165 = 0.05585309863090515 + 1.0 * 6.325279712677002
Epoch 830, val loss: 0.807074785232544
Epoch 840, training loss: 6.392578601837158 = 0.053210075944662094 + 1.0 * 6.3393683433532715
Epoch 840, val loss: 0.8128550052642822
Epoch 850, training loss: 6.38201904296875 = 0.05077598616480827 + 1.0 * 6.33124303817749
Epoch 850, val loss: 0.8185047507286072
Epoch 860, training loss: 6.370850563049316 = 0.04848885536193848 + 1.0 * 6.322361469268799
Epoch 860, val loss: 0.8240298628807068
Epoch 870, training loss: 6.367993354797363 = 0.04634365811944008 + 1.0 * 6.321649551391602
Epoch 870, val loss: 0.8297175765037537
Epoch 880, training loss: 6.364095687866211 = 0.04433196038007736 + 1.0 * 6.319763660430908
Epoch 880, val loss: 0.835363507270813
Epoch 890, training loss: 6.374867916107178 = 0.04243718832731247 + 1.0 * 6.332430839538574
Epoch 890, val loss: 0.8409874439239502
Epoch 900, training loss: 6.3650970458984375 = 0.04065442085266113 + 1.0 * 6.324442386627197
Epoch 900, val loss: 0.8466161489486694
Epoch 910, training loss: 6.359368324279785 = 0.03899795934557915 + 1.0 * 6.320370197296143
Epoch 910, val loss: 0.8519800305366516
Epoch 920, training loss: 6.353761196136475 = 0.037428684532642365 + 1.0 * 6.3163323402404785
Epoch 920, val loss: 0.8574491739273071
Epoch 930, training loss: 6.3508830070495605 = 0.03594528138637543 + 1.0 * 6.314937591552734
Epoch 930, val loss: 0.8629218339920044
Epoch 940, training loss: 6.3508124351501465 = 0.03453906625509262 + 1.0 * 6.316273212432861
Epoch 940, val loss: 0.8683302402496338
Epoch 950, training loss: 6.349255084991455 = 0.03321157395839691 + 1.0 * 6.316043376922607
Epoch 950, val loss: 0.8738222718238831
Epoch 960, training loss: 6.3465375900268555 = 0.03197045996785164 + 1.0 * 6.3145670890808105
Epoch 960, val loss: 0.8789461255073547
Epoch 970, training loss: 6.343882083892822 = 0.030793629586696625 + 1.0 * 6.313088417053223
Epoch 970, val loss: 0.88409823179245
Epoch 980, training loss: 6.343212604522705 = 0.0296779777854681 + 1.0 * 6.313534736633301
Epoch 980, val loss: 0.8893083930015564
Epoch 990, training loss: 6.341301918029785 = 0.028620239347219467 + 1.0 * 6.312681674957275
Epoch 990, val loss: 0.8945178985595703
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 10.544340133666992 = 1.947514295578003 + 1.0 * 8.59682559967041
Epoch 0, val loss: 1.9413338899612427
Epoch 10, training loss: 10.532337188720703 = 1.9359641075134277 + 1.0 * 8.596372604370117
Epoch 10, val loss: 1.9295278787612915
Epoch 20, training loss: 10.513525009155273 = 1.9209668636322021 + 1.0 * 8.592557907104492
Epoch 20, val loss: 1.9143620729446411
Epoch 30, training loss: 10.46368408203125 = 1.8999427556991577 + 1.0 * 8.563741683959961
Epoch 30, val loss: 1.8934643268585205
Epoch 40, training loss: 10.256964683532715 = 1.8742018938064575 + 1.0 * 8.382762908935547
Epoch 40, val loss: 1.8693068027496338
Epoch 50, training loss: 9.809743881225586 = 1.850559115409851 + 1.0 * 7.959184646606445
Epoch 50, val loss: 1.84837806224823
Epoch 60, training loss: 9.333593368530273 = 1.834168553352356 + 1.0 * 7.499424457550049
Epoch 60, val loss: 1.8337740898132324
Epoch 70, training loss: 9.026984214782715 = 1.8185467720031738 + 1.0 * 7.208437442779541
Epoch 70, val loss: 1.8185365200042725
Epoch 80, training loss: 8.78812313079834 = 1.8019355535507202 + 1.0 * 6.986187934875488
Epoch 80, val loss: 1.8026812076568604
Epoch 90, training loss: 8.655255317687988 = 1.7855061292648315 + 1.0 * 6.869749069213867
Epoch 90, val loss: 1.7882150411605835
Epoch 100, training loss: 8.549869537353516 = 1.768760085105896 + 1.0 * 6.78110933303833
Epoch 100, val loss: 1.774173617362976
Epoch 110, training loss: 8.475546836853027 = 1.7520405054092407 + 1.0 * 6.723505973815918
Epoch 110, val loss: 1.7595583200454712
Epoch 120, training loss: 8.4121675491333 = 1.734481930732727 + 1.0 * 6.677685737609863
Epoch 120, val loss: 1.7432706356048584
Epoch 130, training loss: 8.356523513793945 = 1.7148736715316772 + 1.0 * 6.6416497230529785
Epoch 130, val loss: 1.7253679037094116
Epoch 140, training loss: 8.301183700561523 = 1.6926805973052979 + 1.0 * 6.6085028648376465
Epoch 140, val loss: 1.7057017087936401
Epoch 150, training loss: 8.249189376831055 = 1.6671028137207031 + 1.0 * 6.582086563110352
Epoch 150, val loss: 1.6834701299667358
Epoch 160, training loss: 8.199054718017578 = 1.6372332572937012 + 1.0 * 6.561821937561035
Epoch 160, val loss: 1.6575641632080078
Epoch 170, training loss: 8.144157409667969 = 1.6032052040100098 + 1.0 * 6.540952682495117
Epoch 170, val loss: 1.628149151802063
Epoch 180, training loss: 8.088930130004883 = 1.5642427206039429 + 1.0 * 6.52468729019165
Epoch 180, val loss: 1.594547152519226
Epoch 190, training loss: 8.03661823272705 = 1.5212559700012207 + 1.0 * 6.51536226272583
Epoch 190, val loss: 1.5578885078430176
Epoch 200, training loss: 7.9747185707092285 = 1.4756059646606445 + 1.0 * 6.499112606048584
Epoch 200, val loss: 1.5195438861846924
Epoch 210, training loss: 7.915556907653809 = 1.4275273084640503 + 1.0 * 6.488029479980469
Epoch 210, val loss: 1.4795717000961304
Epoch 220, training loss: 7.860395431518555 = 1.3776648044586182 + 1.0 * 6.482730388641357
Epoch 220, val loss: 1.4387950897216797
Epoch 230, training loss: 7.7974443435668945 = 1.32743239402771 + 1.0 * 6.470012187957764
Epoch 230, val loss: 1.3985675573349
Epoch 240, training loss: 7.739116668701172 = 1.2770850658416748 + 1.0 * 6.462031364440918
Epoch 240, val loss: 1.3590753078460693
Epoch 250, training loss: 7.682878494262695 = 1.2274398803710938 + 1.0 * 6.455438613891602
Epoch 250, val loss: 1.3208563327789307
Epoch 260, training loss: 7.627403259277344 = 1.1793551445007324 + 1.0 * 6.448048114776611
Epoch 260, val loss: 1.2846189737319946
Epoch 270, training loss: 7.5739922523498535 = 1.1324256658554077 + 1.0 * 6.441566467285156
Epoch 270, val loss: 1.2498925924301147
Epoch 280, training loss: 7.523978233337402 = 1.0870393514633179 + 1.0 * 6.436938762664795
Epoch 280, val loss: 1.2167426347732544
Epoch 290, training loss: 7.473773002624512 = 1.0433542728424072 + 1.0 * 6.430418491363525
Epoch 290, val loss: 1.1855754852294922
Epoch 300, training loss: 7.426115036010742 = 1.0010970830917358 + 1.0 * 6.425017833709717
Epoch 300, val loss: 1.155789852142334
Epoch 310, training loss: 7.38148307800293 = 0.9604954719543457 + 1.0 * 6.420987606048584
Epoch 310, val loss: 1.1273925304412842
Epoch 320, training loss: 7.338431358337402 = 0.9217755794525146 + 1.0 * 6.416656017303467
Epoch 320, val loss: 1.1007322072982788
Epoch 330, training loss: 7.29672384262085 = 0.8846246600151062 + 1.0 * 6.412099361419678
Epoch 330, val loss: 1.075433611869812
Epoch 340, training loss: 7.256318092346191 = 0.8489649295806885 + 1.0 * 6.407353401184082
Epoch 340, val loss: 1.051422119140625
Epoch 350, training loss: 7.217928409576416 = 0.8147018551826477 + 1.0 * 6.403226375579834
Epoch 350, val loss: 1.0287169218063354
Epoch 360, training loss: 7.183977127075195 = 0.7817327976226807 + 1.0 * 6.4022440910339355
Epoch 360, val loss: 1.0071079730987549
Epoch 370, training loss: 7.145739555358887 = 0.7499304413795471 + 1.0 * 6.395809173583984
Epoch 370, val loss: 0.9867863655090332
Epoch 380, training loss: 7.11850118637085 = 0.7190309166908264 + 1.0 * 6.399470329284668
Epoch 380, val loss: 0.9674695134162903
Epoch 390, training loss: 7.079958438873291 = 0.6892459988594055 + 1.0 * 6.390712261199951
Epoch 390, val loss: 0.9491405487060547
Epoch 400, training loss: 7.045491695404053 = 0.6602091789245605 + 1.0 * 6.385282516479492
Epoch 400, val loss: 0.9318886995315552
Epoch 410, training loss: 7.019589424133301 = 0.6318727731704712 + 1.0 * 6.387716770172119
Epoch 410, val loss: 0.9154631495475769
Epoch 420, training loss: 6.987546920776367 = 0.6046416163444519 + 1.0 * 6.38290548324585
Epoch 420, val loss: 0.9000669717788696
Epoch 430, training loss: 6.955705642700195 = 0.5783609747886658 + 1.0 * 6.377344608306885
Epoch 430, val loss: 0.8858838677406311
Epoch 440, training loss: 6.925908088684082 = 0.5529636740684509 + 1.0 * 6.372944355010986
Epoch 440, val loss: 0.8726710081100464
Epoch 450, training loss: 6.903314590454102 = 0.5283621549606323 + 1.0 * 6.37495231628418
Epoch 450, val loss: 0.8604693412780762
Epoch 460, training loss: 6.880681037902832 = 0.5047383308410645 + 1.0 * 6.375942707061768
Epoch 460, val loss: 0.8494578003883362
Epoch 470, training loss: 6.8484883308410645 = 0.48227742314338684 + 1.0 * 6.3662109375
Epoch 470, val loss: 0.8396156430244446
Epoch 480, training loss: 6.824105262756348 = 0.46056607365608215 + 1.0 * 6.363539218902588
Epoch 480, val loss: 0.8307690024375916
Epoch 490, training loss: 6.800827503204346 = 0.43955788016319275 + 1.0 * 6.361269474029541
Epoch 490, val loss: 0.8229186534881592
Epoch 500, training loss: 6.778209209442139 = 0.4193550646305084 + 1.0 * 6.358854293823242
Epoch 500, val loss: 0.8161231875419617
Epoch 510, training loss: 6.762164115905762 = 0.4001076817512512 + 1.0 * 6.362056255340576
Epoch 510, val loss: 0.8104234337806702
Epoch 520, training loss: 6.738546371459961 = 0.3815903067588806 + 1.0 * 6.3569560050964355
Epoch 520, val loss: 0.8057026267051697
Epoch 530, training loss: 6.717921257019043 = 0.36376526951789856 + 1.0 * 6.354156017303467
Epoch 530, val loss: 0.8017901182174683
Epoch 540, training loss: 6.701080799102783 = 0.3465009331703186 + 1.0 * 6.354579925537109
Epoch 540, val loss: 0.7987232208251953
Epoch 550, training loss: 6.6855645179748535 = 0.3298882842063904 + 1.0 * 6.355676174163818
Epoch 550, val loss: 0.7964993119239807
Epoch 560, training loss: 6.662992477416992 = 0.31398481130599976 + 1.0 * 6.349007606506348
Epoch 560, val loss: 0.795068085193634
Epoch 570, training loss: 6.648233413696289 = 0.29866689443588257 + 1.0 * 6.349566459655762
Epoch 570, val loss: 0.7942943572998047
Epoch 580, training loss: 6.629748821258545 = 0.28394508361816406 + 1.0 * 6.345803737640381
Epoch 580, val loss: 0.7942194938659668
Epoch 590, training loss: 6.619530200958252 = 0.2697279155254364 + 1.0 * 6.349802494049072
Epoch 590, val loss: 0.7947568893432617
Epoch 600, training loss: 6.599819183349609 = 0.256071537733078 + 1.0 * 6.343747615814209
Epoch 600, val loss: 0.7958760261535645
Epoch 610, training loss: 6.587883949279785 = 0.24297963082790375 + 1.0 * 6.34490442276001
Epoch 610, val loss: 0.7976164817810059
Epoch 620, training loss: 6.571722984313965 = 0.23048758506774902 + 1.0 * 6.341235637664795
Epoch 620, val loss: 0.7998214960098267
Epoch 630, training loss: 6.557243824005127 = 0.21850213408470154 + 1.0 * 6.338741779327393
Epoch 630, val loss: 0.8025804758071899
Epoch 640, training loss: 6.54749059677124 = 0.20703859627246857 + 1.0 * 6.340452194213867
Epoch 640, val loss: 0.8057828545570374
Epoch 650, training loss: 6.535529136657715 = 0.19614139199256897 + 1.0 * 6.339387893676758
Epoch 650, val loss: 0.8093504905700684
Epoch 660, training loss: 6.519955635070801 = 0.18579521775245667 + 1.0 * 6.334160327911377
Epoch 660, val loss: 0.8135437369346619
Epoch 670, training loss: 6.509481430053711 = 0.1759444773197174 + 1.0 * 6.3335371017456055
Epoch 670, val loss: 0.8179997801780701
Epoch 680, training loss: 6.503875732421875 = 0.16661551594734192 + 1.0 * 6.3372602462768555
Epoch 680, val loss: 0.8227872848510742
Epoch 690, training loss: 6.493187427520752 = 0.15785154700279236 + 1.0 * 6.335335731506348
Epoch 690, val loss: 0.8279953598976135
Epoch 700, training loss: 6.479434013366699 = 0.1495811492204666 + 1.0 * 6.329853057861328
Epoch 700, val loss: 0.8334641456604004
Epoch 710, training loss: 6.4694671630859375 = 0.14174430072307587 + 1.0 * 6.327723026275635
Epoch 710, val loss: 0.8391870260238647
Epoch 720, training loss: 6.463536262512207 = 0.1343243271112442 + 1.0 * 6.329211711883545
Epoch 720, val loss: 0.8452437520027161
Epoch 730, training loss: 6.45864725112915 = 0.12735553085803986 + 1.0 * 6.331291675567627
Epoch 730, val loss: 0.8514767289161682
Epoch 740, training loss: 6.445666790008545 = 0.12082185596227646 + 1.0 * 6.324844837188721
Epoch 740, val loss: 0.8580197691917419
Epoch 750, training loss: 6.442065715789795 = 0.1146651953458786 + 1.0 * 6.3274006843566895
Epoch 750, val loss: 0.8646478056907654
Epoch 760, training loss: 6.431968688964844 = 0.10887160152196884 + 1.0 * 6.323097229003906
Epoch 760, val loss: 0.8712906241416931
Epoch 770, training loss: 6.425319671630859 = 0.10341911762952805 + 1.0 * 6.321900367736816
Epoch 770, val loss: 0.8782153725624084
Epoch 780, training loss: 6.4318318367004395 = 0.09829440712928772 + 1.0 * 6.333537578582764
Epoch 780, val loss: 0.8852261304855347
Epoch 790, training loss: 6.414952278137207 = 0.09346017986536026 + 1.0 * 6.3214921951293945
Epoch 790, val loss: 0.8921797871589661
Epoch 800, training loss: 6.4085588455200195 = 0.08893424272537231 + 1.0 * 6.319624423980713
Epoch 800, val loss: 0.8993315100669861
Epoch 810, training loss: 6.402638912200928 = 0.08465613424777985 + 1.0 * 6.3179826736450195
Epoch 810, val loss: 0.9064528346061707
Epoch 820, training loss: 6.407756805419922 = 0.08061841130256653 + 1.0 * 6.327138423919678
Epoch 820, val loss: 0.9135458469390869
Epoch 830, training loss: 6.3980326652526855 = 0.07683593779802322 + 1.0 * 6.321196556091309
Epoch 830, val loss: 0.9208101630210876
Epoch 840, training loss: 6.390462398529053 = 0.07326590269804001 + 1.0 * 6.317196369171143
Epoch 840, val loss: 0.9279603362083435
Epoch 850, training loss: 6.385152816772461 = 0.06990734487771988 + 1.0 * 6.315245628356934
Epoch 850, val loss: 0.9351819157600403
Epoch 860, training loss: 6.386136054992676 = 0.06673252582550049 + 1.0 * 6.319403648376465
Epoch 860, val loss: 0.9422099590301514
Epoch 870, training loss: 6.377763748168945 = 0.06375057250261307 + 1.0 * 6.3140130043029785
Epoch 870, val loss: 0.949335515499115
Epoch 880, training loss: 6.373722553253174 = 0.06093559414148331 + 1.0 * 6.312787055969238
Epoch 880, val loss: 0.9565554261207581
Epoch 890, training loss: 6.3697919845581055 = 0.058267172425985336 + 1.0 * 6.311524868011475
Epoch 890, val loss: 0.963498055934906
Epoch 900, training loss: 6.367097854614258 = 0.05574468895792961 + 1.0 * 6.3113532066345215
Epoch 900, val loss: 0.9704769849777222
Epoch 910, training loss: 6.369189262390137 = 0.05335990712046623 + 1.0 * 6.315829277038574
Epoch 910, val loss: 0.9773328900337219
Epoch 920, training loss: 6.3604912757873535 = 0.051121555268764496 + 1.0 * 6.3093695640563965
Epoch 920, val loss: 0.9843845963478088
Epoch 930, training loss: 6.360868453979492 = 0.049001842737197876 + 1.0 * 6.311866760253906
Epoch 930, val loss: 0.9912444949150085
Epoch 940, training loss: 6.356679916381836 = 0.0470028854906559 + 1.0 * 6.3096771240234375
Epoch 940, val loss: 0.9979113340377808
Epoch 950, training loss: 6.3509016036987305 = 0.04511495679616928 + 1.0 * 6.305786609649658
Epoch 950, val loss: 1.0047218799591064
Epoch 960, training loss: 6.34822940826416 = 0.04331815242767334 + 1.0 * 6.304911136627197
Epoch 960, val loss: 1.0113886594772339
Epoch 970, training loss: 6.346076965332031 = 0.041614219546318054 + 1.0 * 6.304462909698486
Epoch 970, val loss: 1.0180083513259888
Epoch 980, training loss: 6.347903251647949 = 0.04000043123960495 + 1.0 * 6.307902812957764
Epoch 980, val loss: 1.0245425701141357
Epoch 990, training loss: 6.34522819519043 = 0.038474660366773605 + 1.0 * 6.306753635406494
Epoch 990, val loss: 1.0311442613601685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.538002014160156 = 1.9411474466323853 + 1.0 * 8.596854209899902
Epoch 0, val loss: 1.940546989440918
Epoch 10, training loss: 10.527660369873047 = 1.931024193763733 + 1.0 * 8.596635818481445
Epoch 10, val loss: 1.9309669733047485
Epoch 20, training loss: 10.512933731079102 = 1.9182839393615723 + 1.0 * 8.594650268554688
Epoch 20, val loss: 1.9185140132904053
Epoch 30, training loss: 10.477607727050781 = 1.9001182317733765 + 1.0 * 8.577489852905273
Epoch 30, val loss: 1.9004387855529785
Epoch 40, training loss: 10.346278190612793 = 1.8752061128616333 + 1.0 * 8.47107219696045
Epoch 40, val loss: 1.8762600421905518
Epoch 50, training loss: 9.92544937133789 = 1.8463736772537231 + 1.0 * 8.079075813293457
Epoch 50, val loss: 1.849044680595398
Epoch 60, training loss: 9.647732734680176 = 1.8200780153274536 + 1.0 * 7.8276543617248535
Epoch 60, val loss: 1.8249123096466064
Epoch 70, training loss: 9.235306739807129 = 1.8006842136383057 + 1.0 * 7.434622764587402
Epoch 70, val loss: 1.8077772855758667
Epoch 80, training loss: 8.878915786743164 = 1.7880229949951172 + 1.0 * 7.090892314910889
Epoch 80, val loss: 1.796769142150879
Epoch 90, training loss: 8.741971969604492 = 1.7720057964324951 + 1.0 * 6.969965934753418
Epoch 90, val loss: 1.7817314863204956
Epoch 100, training loss: 8.632359504699707 = 1.7512482404708862 + 1.0 * 6.8811116218566895
Epoch 100, val loss: 1.76272714138031
Epoch 110, training loss: 8.541107177734375 = 1.7299245595932007 + 1.0 * 6.811182975769043
Epoch 110, val loss: 1.7431623935699463
Epoch 120, training loss: 8.474364280700684 = 1.7070064544677734 + 1.0 * 6.76735782623291
Epoch 120, val loss: 1.7219823598861694
Epoch 130, training loss: 8.41545295715332 = 1.6803770065307617 + 1.0 * 6.735075950622559
Epoch 130, val loss: 1.6980394124984741
Epoch 140, training loss: 8.359251022338867 = 1.6490771770477295 + 1.0 * 6.710174083709717
Epoch 140, val loss: 1.6706417798995972
Epoch 150, training loss: 8.299144744873047 = 1.6128573417663574 + 1.0 * 6.6862874031066895
Epoch 150, val loss: 1.6391466856002808
Epoch 160, training loss: 8.234247207641602 = 1.571756362915039 + 1.0 * 6.662491321563721
Epoch 160, val loss: 1.6034854650497437
Epoch 170, training loss: 8.166707038879395 = 1.5270720720291138 + 1.0 * 6.63963508605957
Epoch 170, val loss: 1.5646510124206543
Epoch 180, training loss: 8.097926139831543 = 1.4788880348205566 + 1.0 * 6.619038105010986
Epoch 180, val loss: 1.5230740308761597
Epoch 190, training loss: 8.031574249267578 = 1.428404688835144 + 1.0 * 6.6031694412231445
Epoch 190, val loss: 1.4801486730575562
Epoch 200, training loss: 7.961515426635742 = 1.3773620128631592 + 1.0 * 6.584153175354004
Epoch 200, val loss: 1.437041163444519
Epoch 210, training loss: 7.8981218338012695 = 1.3261501789093018 + 1.0 * 6.571971416473389
Epoch 210, val loss: 1.3944189548492432
Epoch 220, training loss: 7.829456329345703 = 1.2760984897613525 + 1.0 * 6.5533576011657715
Epoch 220, val loss: 1.3535277843475342
Epoch 230, training loss: 7.766644477844238 = 1.2271747589111328 + 1.0 * 6.5394697189331055
Epoch 230, val loss: 1.3140991926193237
Epoch 240, training loss: 7.708980560302734 = 1.1799747943878174 + 1.0 * 6.529005527496338
Epoch 240, val loss: 1.276590347290039
Epoch 250, training loss: 7.652057647705078 = 1.135430932044983 + 1.0 * 6.516626834869385
Epoch 250, val loss: 1.2416950464248657
Epoch 260, training loss: 7.600579261779785 = 1.093355655670166 + 1.0 * 6.507223606109619
Epoch 260, val loss: 1.209294319152832
Epoch 270, training loss: 7.553560256958008 = 1.0543181896209717 + 1.0 * 6.499241828918457
Epoch 270, val loss: 1.1797661781311035
Epoch 280, training loss: 7.507232189178467 = 1.0182863473892212 + 1.0 * 6.488945960998535
Epoch 280, val loss: 1.1529879570007324
Epoch 290, training loss: 7.468541145324707 = 0.9846097826957703 + 1.0 * 6.483931541442871
Epoch 290, val loss: 1.1283626556396484
Epoch 300, training loss: 7.424479961395264 = 0.9530447721481323 + 1.0 * 6.471435070037842
Epoch 300, val loss: 1.1055034399032593
Epoch 310, training loss: 7.387363433837891 = 0.9227132797241211 + 1.0 * 6.4646501541137695
Epoch 310, val loss: 1.084031105041504
Epoch 320, training loss: 7.349995136260986 = 0.8933333158493042 + 1.0 * 6.456661701202393
Epoch 320, val loss: 1.0634971857070923
Epoch 330, training loss: 7.314640522003174 = 0.8645051121711731 + 1.0 * 6.450135231018066
Epoch 330, val loss: 1.0436190366744995
Epoch 340, training loss: 7.279520511627197 = 0.8356385827064514 + 1.0 * 6.443881988525391
Epoch 340, val loss: 1.0239940881729126
Epoch 350, training loss: 7.246259689331055 = 0.8066207766532898 + 1.0 * 6.439639091491699
Epoch 350, val loss: 1.0044292211532593
Epoch 360, training loss: 7.209711074829102 = 0.7774403095245361 + 1.0 * 6.4322710037231445
Epoch 360, val loss: 0.9850336313247681
Epoch 370, training loss: 7.177517414093018 = 0.7479170560836792 + 1.0 * 6.429600238800049
Epoch 370, val loss: 0.9656659364700317
Epoch 380, training loss: 7.145463943481445 = 0.7184457182884216 + 1.0 * 6.427018165588379
Epoch 380, val loss: 0.9465840458869934
Epoch 390, training loss: 7.109167575836182 = 0.6892198324203491 + 1.0 * 6.419947624206543
Epoch 390, val loss: 0.9278534650802612
Epoch 400, training loss: 7.0749430656433105 = 0.6601376533508301 + 1.0 * 6.4148054122924805
Epoch 400, val loss: 0.9094461798667908
Epoch 410, training loss: 7.045523166656494 = 0.6315107345581055 + 1.0 * 6.414012432098389
Epoch 410, val loss: 0.8916217088699341
Epoch 420, training loss: 7.012967109680176 = 0.6039394736289978 + 1.0 * 6.409027576446533
Epoch 420, val loss: 0.8746350407600403
Epoch 430, training loss: 6.981383800506592 = 0.5773338079452515 + 1.0 * 6.404049873352051
Epoch 430, val loss: 0.8585240840911865
Epoch 440, training loss: 6.954720973968506 = 0.5518772006034851 + 1.0 * 6.402843952178955
Epoch 440, val loss: 0.8435620069503784
Epoch 450, training loss: 6.927159309387207 = 0.5278131365776062 + 1.0 * 6.399346351623535
Epoch 450, val loss: 0.8297170400619507
Epoch 460, training loss: 6.90023136138916 = 0.505065381526947 + 1.0 * 6.395165920257568
Epoch 460, val loss: 0.8170820474624634
Epoch 470, training loss: 6.890636444091797 = 0.48369356989860535 + 1.0 * 6.406942844390869
Epoch 470, val loss: 0.8055981397628784
Epoch 480, training loss: 6.854862689971924 = 0.46383970975875854 + 1.0 * 6.3910231590271
Epoch 480, val loss: 0.7953974604606628
Epoch 490, training loss: 6.83247709274292 = 0.4451797306537628 + 1.0 * 6.3872971534729
Epoch 490, val loss: 0.7862706780433655
Epoch 500, training loss: 6.826867580413818 = 0.4275532364845276 + 1.0 * 6.3993144035339355
Epoch 500, val loss: 0.7780789136886597
Epoch 510, training loss: 6.794075965881348 = 0.4108407497406006 + 1.0 * 6.383234977722168
Epoch 510, val loss: 0.7708806991577148
Epoch 520, training loss: 6.775914192199707 = 0.3948385715484619 + 1.0 * 6.381075382232666
Epoch 520, val loss: 0.7642426490783691
Epoch 530, training loss: 6.757752418518066 = 0.37919384241104126 + 1.0 * 6.37855863571167
Epoch 530, val loss: 0.758141040802002
Epoch 540, training loss: 6.755382061004639 = 0.36370566487312317 + 1.0 * 6.391676425933838
Epoch 540, val loss: 0.7523521780967712
Epoch 550, training loss: 6.724107265472412 = 0.3482416868209839 + 1.0 * 6.375865459442139
Epoch 550, val loss: 0.7468150854110718
Epoch 560, training loss: 6.705135822296143 = 0.3326672315597534 + 1.0 * 6.3724684715271
Epoch 560, val loss: 0.7413586378097534
Epoch 570, training loss: 6.6873626708984375 = 0.3167923092842102 + 1.0 * 6.370570182800293
Epoch 570, val loss: 0.7360537648200989
Epoch 580, training loss: 6.673155784606934 = 0.30072787404060364 + 1.0 * 6.372427940368652
Epoch 580, val loss: 0.730937123298645
Epoch 590, training loss: 6.654404640197754 = 0.2846081256866455 + 1.0 * 6.3697967529296875
Epoch 590, val loss: 0.7261224985122681
Epoch 600, training loss: 6.635743618011475 = 0.26844918727874756 + 1.0 * 6.3672943115234375
Epoch 600, val loss: 0.7217854857444763
Epoch 610, training loss: 6.6169753074646 = 0.25247684121131897 + 1.0 * 6.364498615264893
Epoch 610, val loss: 0.718156099319458
Epoch 620, training loss: 6.603038787841797 = 0.23695003986358643 + 1.0 * 6.3660888671875
Epoch 620, val loss: 0.715524435043335
Epoch 630, training loss: 6.583014488220215 = 0.22203129529953003 + 1.0 * 6.360983371734619
Epoch 630, val loss: 0.7139401435852051
Epoch 640, training loss: 6.567001819610596 = 0.20792339742183685 + 1.0 * 6.359078407287598
Epoch 640, val loss: 0.7136043310165405
Epoch 650, training loss: 6.552699089050293 = 0.1946364790201187 + 1.0 * 6.358062744140625
Epoch 650, val loss: 0.7144631743431091
Epoch 660, training loss: 6.541173934936523 = 0.18225501477718353 + 1.0 * 6.358919143676758
Epoch 660, val loss: 0.7164410352706909
Epoch 670, training loss: 6.530673503875732 = 0.17079171538352966 + 1.0 * 6.35988187789917
Epoch 670, val loss: 0.719457745552063
Epoch 680, training loss: 6.514106750488281 = 0.16022349894046783 + 1.0 * 6.353883266448975
Epoch 680, val loss: 0.7232411503791809
Epoch 690, training loss: 6.504031181335449 = 0.15051387250423431 + 1.0 * 6.353517532348633
Epoch 690, val loss: 0.7279033660888672
Epoch 700, training loss: 6.4910149574279785 = 0.14154282212257385 + 1.0 * 6.3494720458984375
Epoch 700, val loss: 0.7330904006958008
Epoch 710, training loss: 6.4951300621032715 = 0.13327088952064514 + 1.0 * 6.361859321594238
Epoch 710, val loss: 0.7386490702629089
Epoch 720, training loss: 6.472054958343506 = 0.12570874392986298 + 1.0 * 6.346346378326416
Epoch 720, val loss: 0.7447133660316467
Epoch 730, training loss: 6.464809417724609 = 0.11871649324893951 + 1.0 * 6.346092700958252
Epoch 730, val loss: 0.7510927319526672
Epoch 740, training loss: 6.4672532081604 = 0.11225274950265884 + 1.0 * 6.3550004959106445
Epoch 740, val loss: 0.7575530409812927
Epoch 750, training loss: 6.450941562652588 = 0.10631749033927917 + 1.0 * 6.344624042510986
Epoch 750, val loss: 0.7642432451248169
Epoch 760, training loss: 6.442841053009033 = 0.10081209987401962 + 1.0 * 6.342029094696045
Epoch 760, val loss: 0.7711024284362793
Epoch 770, training loss: 6.437771797180176 = 0.09568368643522263 + 1.0 * 6.342088222503662
Epoch 770, val loss: 0.7779929637908936
Epoch 780, training loss: 6.429807186126709 = 0.09091580659151077 + 1.0 * 6.338891506195068
Epoch 780, val loss: 0.7849670052528381
Epoch 790, training loss: 6.423729419708252 = 0.08647911995649338 + 1.0 * 6.337250232696533
Epoch 790, val loss: 0.792043924331665
Epoch 800, training loss: 6.435128211975098 = 0.08233960717916489 + 1.0 * 6.35278844833374
Epoch 800, val loss: 0.7991156578063965
Epoch 810, training loss: 6.416199207305908 = 0.0784701481461525 + 1.0 * 6.337728977203369
Epoch 810, val loss: 0.8059604167938232
Epoch 820, training loss: 6.410489559173584 = 0.07487667351961136 + 1.0 * 6.335612773895264
Epoch 820, val loss: 0.8131223917007446
Epoch 830, training loss: 6.40347146987915 = 0.07149528712034225 + 1.0 * 6.331976413726807
Epoch 830, val loss: 0.8201175332069397
Epoch 840, training loss: 6.413074970245361 = 0.0683181881904602 + 1.0 * 6.344756603240967
Epoch 840, val loss: 0.8271770477294922
Epoch 850, training loss: 6.399337291717529 = 0.06534397602081299 + 1.0 * 6.333993434906006
Epoch 850, val loss: 0.8341094851493835
Epoch 860, training loss: 6.392269611358643 = 0.06255696713924408 + 1.0 * 6.329712867736816
Epoch 860, val loss: 0.8411778807640076
Epoch 870, training loss: 6.395442962646484 = 0.05993250384926796 + 1.0 * 6.33551025390625
Epoch 870, val loss: 0.8480666875839233
Epoch 880, training loss: 6.386892318725586 = 0.05746663734316826 + 1.0 * 6.329425811767578
Epoch 880, val loss: 0.8550280928611755
Epoch 890, training loss: 6.38314962387085 = 0.0551358237862587 + 1.0 * 6.328013896942139
Epoch 890, val loss: 0.8619710803031921
Epoch 900, training loss: 6.379512786865234 = 0.052938275039196014 + 1.0 * 6.326574325561523
Epoch 900, val loss: 0.8687530755996704
Epoch 910, training loss: 6.38045597076416 = 0.05086701363325119 + 1.0 * 6.329588890075684
Epoch 910, val loss: 0.8755592107772827
Epoch 920, training loss: 6.373863697052002 = 0.04891963303089142 + 1.0 * 6.324944019317627
Epoch 920, val loss: 0.882412314414978
Epoch 930, training loss: 6.368390083312988 = 0.047070086002349854 + 1.0 * 6.321320056915283
Epoch 930, val loss: 0.8891641497612
Epoch 940, training loss: 6.366161823272705 = 0.045314110815525055 + 1.0 * 6.320847511291504
Epoch 940, val loss: 0.8958608508110046
Epoch 950, training loss: 6.372365951538086 = 0.04364604130387306 + 1.0 * 6.3287200927734375
Epoch 950, val loss: 0.9024351835250854
Epoch 960, training loss: 6.369366645812988 = 0.04207281395792961 + 1.0 * 6.327293872833252
Epoch 960, val loss: 0.909003496170044
Epoch 970, training loss: 6.360602855682373 = 0.040583960711956024 + 1.0 * 6.320018768310547
Epoch 970, val loss: 0.9155431389808655
Epoch 980, training loss: 6.356958389282227 = 0.03917764499783516 + 1.0 * 6.3177809715271
Epoch 980, val loss: 0.9220612645149231
Epoch 990, training loss: 6.354345321655273 = 0.03783169761300087 + 1.0 * 6.316513538360596
Epoch 990, val loss: 0.9285084009170532
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8429098576700054
The final CL Acc:0.81111, 0.01090, The final GNN Acc:0.83852, 0.00366
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9518])
updated graph: torch.Size([2, 10552])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.524202346801758 = 1.927368402481079 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9258958101272583
Epoch 10, training loss: 10.514622688293457 = 1.9180452823638916 + 1.0 * 8.596577644348145
Epoch 10, val loss: 1.9160794019699097
Epoch 20, training loss: 10.501411437988281 = 1.9066669940948486 + 1.0 * 8.594744682312012
Epoch 20, val loss: 1.9038524627685547
Epoch 30, training loss: 10.472654342651367 = 1.8912134170532227 + 1.0 * 8.581440925598145
Epoch 30, val loss: 1.8872103691101074
Epoch 40, training loss: 10.369424819946289 = 1.8708715438842773 + 1.0 * 8.498553276062012
Epoch 40, val loss: 1.8660870790481567
Epoch 50, training loss: 9.950931549072266 = 1.8488962650299072 + 1.0 * 8.102035522460938
Epoch 50, val loss: 1.8439788818359375
Epoch 60, training loss: 9.552515983581543 = 1.8293215036392212 + 1.0 * 7.723194122314453
Epoch 60, val loss: 1.8257404565811157
Epoch 70, training loss: 9.178394317626953 = 1.816654920578003 + 1.0 * 7.361739158630371
Epoch 70, val loss: 1.8138633966445923
Epoch 80, training loss: 8.903999328613281 = 1.805169701576233 + 1.0 * 7.098829746246338
Epoch 80, val loss: 1.8024547100067139
Epoch 90, training loss: 8.759057998657227 = 1.7920993566513062 + 1.0 * 6.966958999633789
Epoch 90, val loss: 1.7894612550735474
Epoch 100, training loss: 8.645892143249512 = 1.7783814668655396 + 1.0 * 6.8675103187561035
Epoch 100, val loss: 1.7762329578399658
Epoch 110, training loss: 8.561750411987305 = 1.7651854753494263 + 1.0 * 6.79656457901001
Epoch 110, val loss: 1.7638667821884155
Epoch 120, training loss: 8.495711326599121 = 1.7510260343551636 + 1.0 * 6.744685649871826
Epoch 120, val loss: 1.751013159751892
Epoch 130, training loss: 8.440332412719727 = 1.7347650527954102 + 1.0 * 6.705566883087158
Epoch 130, val loss: 1.7364494800567627
Epoch 140, training loss: 8.384634971618652 = 1.7160491943359375 + 1.0 * 6.668585777282715
Epoch 140, val loss: 1.7200195789337158
Epoch 150, training loss: 8.332756996154785 = 1.6943881511688232 + 1.0 * 6.638368606567383
Epoch 150, val loss: 1.7011206150054932
Epoch 160, training loss: 8.28172492980957 = 1.6689528226852417 + 1.0 * 6.612771987915039
Epoch 160, val loss: 1.6790841817855835
Epoch 170, training loss: 8.228619575500488 = 1.6392405033111572 + 1.0 * 6.58937931060791
Epoch 170, val loss: 1.6536052227020264
Epoch 180, training loss: 8.1746826171875 = 1.604416847229004 + 1.0 * 6.570265293121338
Epoch 180, val loss: 1.6238086223602295
Epoch 190, training loss: 8.118701934814453 = 1.564632534980774 + 1.0 * 6.5540690422058105
Epoch 190, val loss: 1.5900517702102661
Epoch 200, training loss: 8.0584135055542 = 1.5197913646697998 + 1.0 * 6.5386223793029785
Epoch 200, val loss: 1.5521153211593628
Epoch 210, training loss: 7.995607376098633 = 1.4699459075927734 + 1.0 * 6.525661468505859
Epoch 210, val loss: 1.5103226900100708
Epoch 220, training loss: 7.931005477905273 = 1.4162733554840088 + 1.0 * 6.5147318840026855
Epoch 220, val loss: 1.4656190872192383
Epoch 230, training loss: 7.865153789520264 = 1.3603849411010742 + 1.0 * 6.5047688484191895
Epoch 230, val loss: 1.4194765090942383
Epoch 240, training loss: 7.798222064971924 = 1.3029261827468872 + 1.0 * 6.495296001434326
Epoch 240, val loss: 1.3724029064178467
Epoch 250, training loss: 7.733548641204834 = 1.2454490661621094 + 1.0 * 6.488099575042725
Epoch 250, val loss: 1.3258004188537598
Epoch 260, training loss: 7.670128345489502 = 1.1893247365951538 + 1.0 * 6.480803489685059
Epoch 260, val loss: 1.2807801961898804
Epoch 270, training loss: 7.608644008636475 = 1.1345609426498413 + 1.0 * 6.474082946777344
Epoch 270, val loss: 1.2371550798416138
Epoch 280, training loss: 7.551967620849609 = 1.0812408924102783 + 1.0 * 6.47072696685791
Epoch 280, val loss: 1.195015549659729
Epoch 290, training loss: 7.491418838500977 = 1.0302784442901611 + 1.0 * 6.461140155792236
Epoch 290, val loss: 1.1552273035049438
Epoch 300, training loss: 7.437753200531006 = 0.9817124605178833 + 1.0 * 6.456040859222412
Epoch 300, val loss: 1.1177748441696167
Epoch 310, training loss: 7.386801719665527 = 0.9355493187904358 + 1.0 * 6.451252460479736
Epoch 310, val loss: 1.0828009843826294
Epoch 320, training loss: 7.338286399841309 = 0.8922802209854126 + 1.0 * 6.4460062980651855
Epoch 320, val loss: 1.050951600074768
Epoch 330, training loss: 7.291965484619141 = 0.8521316051483154 + 1.0 * 6.439833641052246
Epoch 330, val loss: 1.0221813917160034
Epoch 340, training loss: 7.261641025543213 = 0.8148428797721863 + 1.0 * 6.446798324584961
Epoch 340, val loss: 0.9964156746864319
Epoch 350, training loss: 7.212048530578613 = 0.7807157635688782 + 1.0 * 6.431332588195801
Epoch 350, val loss: 0.9739412665367126
Epoch 360, training loss: 7.176216125488281 = 0.7492146492004395 + 1.0 * 6.427001476287842
Epoch 360, val loss: 0.9541391134262085
Epoch 370, training loss: 7.141622543334961 = 0.7196215391159058 + 1.0 * 6.422000885009766
Epoch 370, val loss: 0.9365560412406921
Epoch 380, training loss: 7.112329483032227 = 0.6916216611862183 + 1.0 * 6.420707702636719
Epoch 380, val loss: 0.920828104019165
Epoch 390, training loss: 7.080913543701172 = 0.6650188565254211 + 1.0 * 6.415894508361816
Epoch 390, val loss: 0.9067122340202332
Epoch 400, training loss: 7.049098014831543 = 0.6393588781356812 + 1.0 * 6.409739017486572
Epoch 400, val loss: 0.8937660455703735
Epoch 410, training loss: 7.02200984954834 = 0.614122748374939 + 1.0 * 6.407886981964111
Epoch 410, val loss: 0.8816046118736267
Epoch 420, training loss: 6.998478412628174 = 0.5891126394271851 + 1.0 * 6.409365653991699
Epoch 420, val loss: 0.8700812458992004
Epoch 430, training loss: 6.964193344116211 = 0.5642628073692322 + 1.0 * 6.399930477142334
Epoch 430, val loss: 0.8590553998947144
Epoch 440, training loss: 6.93670654296875 = 0.5392941832542419 + 1.0 * 6.397412300109863
Epoch 440, val loss: 0.8484532833099365
Epoch 450, training loss: 6.918940544128418 = 0.5140655040740967 + 1.0 * 6.4048752784729
Epoch 450, val loss: 0.8380590081214905
Epoch 460, training loss: 6.882439613342285 = 0.4888748526573181 + 1.0 * 6.393564701080322
Epoch 460, val loss: 0.8279407024383545
Epoch 470, training loss: 6.854295253753662 = 0.46362265944480896 + 1.0 * 6.39067268371582
Epoch 470, val loss: 0.8182323575019836
Epoch 480, training loss: 6.826218128204346 = 0.43839600682258606 + 1.0 * 6.387822151184082
Epoch 480, val loss: 0.8089802265167236
Epoch 490, training loss: 6.80179500579834 = 0.4134305715560913 + 1.0 * 6.388364315032959
Epoch 490, val loss: 0.8003066778182983
Epoch 500, training loss: 6.772424221038818 = 0.38909265398979187 + 1.0 * 6.383331775665283
Epoch 500, val loss: 0.7921707034111023
Epoch 510, training loss: 6.745732307434082 = 0.36534690856933594 + 1.0 * 6.380385398864746
Epoch 510, val loss: 0.7849979996681213
Epoch 520, training loss: 6.730679035186768 = 0.3423460125923157 + 1.0 * 6.388332843780518
Epoch 520, val loss: 0.778768002986908
Epoch 530, training loss: 6.697707176208496 = 0.3204757571220398 + 1.0 * 6.377231597900391
Epoch 530, val loss: 0.773552417755127
Epoch 540, training loss: 6.676853179931641 = 0.29974299669265747 + 1.0 * 6.377110004425049
Epoch 540, val loss: 0.7693984508514404
Epoch 550, training loss: 6.65316915512085 = 0.28015726804733276 + 1.0 * 6.373012065887451
Epoch 550, val loss: 0.7663753032684326
Epoch 560, training loss: 6.639272689819336 = 0.26179003715515137 + 1.0 * 6.3774824142456055
Epoch 560, val loss: 0.7643669843673706
Epoch 570, training loss: 6.616885185241699 = 0.244727224111557 + 1.0 * 6.372158050537109
Epoch 570, val loss: 0.7633475065231323
Epoch 580, training loss: 6.596164703369141 = 0.2289222776889801 + 1.0 * 6.367242336273193
Epoch 580, val loss: 0.7632826566696167
Epoch 590, training loss: 6.578876495361328 = 0.21423085033893585 + 1.0 * 6.364645481109619
Epoch 590, val loss: 0.764192521572113
Epoch 600, training loss: 6.571423530578613 = 0.2005912959575653 + 1.0 * 6.370832443237305
Epoch 600, val loss: 0.7659252882003784
Epoch 610, training loss: 6.554469585418701 = 0.1881033182144165 + 1.0 * 6.366366386413574
Epoch 610, val loss: 0.7683241963386536
Epoch 620, training loss: 6.537837982177734 = 0.17659249901771545 + 1.0 * 6.361245632171631
Epoch 620, val loss: 0.7713554501533508
Epoch 630, training loss: 6.526341438293457 = 0.1659393608570099 + 1.0 * 6.3604021072387695
Epoch 630, val loss: 0.7751232385635376
Epoch 640, training loss: 6.513495922088623 = 0.1561269611120224 + 1.0 * 6.3573689460754395
Epoch 640, val loss: 0.7793368697166443
Epoch 650, training loss: 6.503118991851807 = 0.14705947041511536 + 1.0 * 6.356059551239014
Epoch 650, val loss: 0.7839676737785339
Epoch 660, training loss: 6.502787113189697 = 0.13867118954658508 + 1.0 * 6.3641157150268555
Epoch 660, val loss: 0.7891278862953186
Epoch 670, training loss: 6.485352516174316 = 0.13094878196716309 + 1.0 * 6.354403495788574
Epoch 670, val loss: 0.7944170236587524
Epoch 680, training loss: 6.473851680755615 = 0.12378086149692535 + 1.0 * 6.350070953369141
Epoch 680, val loss: 0.8000889420509338
Epoch 690, training loss: 6.469577312469482 = 0.11710720509290695 + 1.0 * 6.3524699211120605
Epoch 690, val loss: 0.8061162829399109
Epoch 700, training loss: 6.459216594696045 = 0.11092421412467957 + 1.0 * 6.348292350769043
Epoch 700, val loss: 0.8122957944869995
Epoch 710, training loss: 6.452176094055176 = 0.1051645427942276 + 1.0 * 6.347011566162109
Epoch 710, val loss: 0.8185486793518066
Epoch 720, training loss: 6.445579528808594 = 0.09978990256786346 + 1.0 * 6.345789432525635
Epoch 720, val loss: 0.8250990509986877
Epoch 730, training loss: 6.438624382019043 = 0.09476599097251892 + 1.0 * 6.343858242034912
Epoch 730, val loss: 0.8316662311553955
Epoch 740, training loss: 6.433830738067627 = 0.09007938951253891 + 1.0 * 6.343751430511475
Epoch 740, val loss: 0.838289737701416
Epoch 750, training loss: 6.426552772521973 = 0.08569000661373138 + 1.0 * 6.34086275100708
Epoch 750, val loss: 0.8450366854667664
Epoch 760, training loss: 6.427910327911377 = 0.08156948536634445 + 1.0 * 6.346340656280518
Epoch 760, val loss: 0.8518573045730591
Epoch 770, training loss: 6.414913654327393 = 0.07771361619234085 + 1.0 * 6.337200164794922
Epoch 770, val loss: 0.85860276222229
Epoch 780, training loss: 6.41004753112793 = 0.0740884393453598 + 1.0 * 6.335958957672119
Epoch 780, val loss: 0.8654853105545044
Epoch 790, training loss: 6.409537315368652 = 0.07067158818244934 + 1.0 * 6.338865756988525
Epoch 790, val loss: 0.8724101781845093
Epoch 800, training loss: 6.40992546081543 = 0.06745216250419617 + 1.0 * 6.34247350692749
Epoch 800, val loss: 0.8791905045509338
Epoch 810, training loss: 6.399961948394775 = 0.06444612890481949 + 1.0 * 6.335515975952148
Epoch 810, val loss: 0.8859673738479614
Epoch 820, training loss: 6.392850399017334 = 0.06161385402083397 + 1.0 * 6.331236362457275
Epoch 820, val loss: 0.8927596211433411
Epoch 830, training loss: 6.390049457550049 = 0.058932654559612274 + 1.0 * 6.331116676330566
Epoch 830, val loss: 0.8996292352676392
Epoch 840, training loss: 6.398763656616211 = 0.056400660425424576 + 1.0 * 6.342362880706787
Epoch 840, val loss: 0.9064029455184937
Epoch 850, training loss: 6.383264541625977 = 0.054005127400159836 + 1.0 * 6.329259395599365
Epoch 850, val loss: 0.9130226373672485
Epoch 860, training loss: 6.379017353057861 = 0.05174997076392174 + 1.0 * 6.327267169952393
Epoch 860, val loss: 0.9196997284889221
Epoch 870, training loss: 6.38046932220459 = 0.04961267486214638 + 1.0 * 6.330856800079346
Epoch 870, val loss: 0.9263182878494263
Epoch 880, training loss: 6.373170375823975 = 0.04759090393781662 + 1.0 * 6.325579643249512
Epoch 880, val loss: 0.932884693145752
Epoch 890, training loss: 6.371918678283691 = 0.04568060114979744 + 1.0 * 6.32623815536499
Epoch 890, val loss: 0.9393469095230103
Epoch 900, training loss: 6.369663715362549 = 0.043868593871593475 + 1.0 * 6.3257951736450195
Epoch 900, val loss: 0.9458045959472656
Epoch 910, training loss: 6.371638774871826 = 0.04215964302420616 + 1.0 * 6.329479217529297
Epoch 910, val loss: 0.9521088600158691
Epoch 920, training loss: 6.363566875457764 = 0.04054393619298935 + 1.0 * 6.323022842407227
Epoch 920, val loss: 0.9583240747451782
Epoch 930, training loss: 6.360074043273926 = 0.0390128567814827 + 1.0 * 6.321061134338379
Epoch 930, val loss: 0.9645754098892212
Epoch 940, training loss: 6.357290744781494 = 0.037550900131464005 + 1.0 * 6.319739818572998
Epoch 940, val loss: 0.9707837104797363
Epoch 950, training loss: 6.358518600463867 = 0.036161068826913834 + 1.0 * 6.322357654571533
Epoch 950, val loss: 0.9769119620323181
Epoch 960, training loss: 6.354549884796143 = 0.03484467789530754 + 1.0 * 6.319705009460449
Epoch 960, val loss: 0.9830219149589539
Epoch 970, training loss: 6.3516340255737305 = 0.03359513729810715 + 1.0 * 6.3180389404296875
Epoch 970, val loss: 0.9889979362487793
Epoch 980, training loss: 6.358874320983887 = 0.032412610948085785 + 1.0 * 6.3264617919921875
Epoch 980, val loss: 0.9949424266815186
Epoch 990, training loss: 6.349007606506348 = 0.031290266662836075 + 1.0 * 6.317717552185059
Epoch 990, val loss: 1.0007057189941406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 10.538681030273438 = 1.941858172416687 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.9493412971496582
Epoch 10, training loss: 10.528764724731445 = 1.9322105646133423 + 1.0 * 8.596553802490234
Epoch 10, val loss: 1.9393771886825562
Epoch 20, training loss: 10.514945983886719 = 1.9203709363937378 + 1.0 * 8.594574928283691
Epoch 20, val loss: 1.926802396774292
Epoch 30, training loss: 10.483535766601562 = 1.9038323163986206 + 1.0 * 8.579703330993652
Epoch 30, val loss: 1.9090797901153564
Epoch 40, training loss: 10.377180099487305 = 1.8816063404083252 + 1.0 * 8.495573997497559
Epoch 40, val loss: 1.8860353231430054
Epoch 50, training loss: 9.988972663879395 = 1.8580774068832397 + 1.0 * 8.130895614624023
Epoch 50, val loss: 1.8628407716751099
Epoch 60, training loss: 9.5870361328125 = 1.837290644645691 + 1.0 * 7.7497453689575195
Epoch 60, val loss: 1.843123435974121
Epoch 70, training loss: 9.084015846252441 = 1.8230959177017212 + 1.0 * 7.26092004776001
Epoch 70, val loss: 1.8287179470062256
Epoch 80, training loss: 8.888703346252441 = 1.8108494281768799 + 1.0 * 7.077853679656982
Epoch 80, val loss: 1.8157968521118164
Epoch 90, training loss: 8.791871070861816 = 1.7940653562545776 + 1.0 * 6.997806072235107
Epoch 90, val loss: 1.7993435859680176
Epoch 100, training loss: 8.696842193603516 = 1.7770498991012573 + 1.0 * 6.919792652130127
Epoch 100, val loss: 1.7834255695343018
Epoch 110, training loss: 8.607803344726562 = 1.7622883319854736 + 1.0 * 6.84551477432251
Epoch 110, val loss: 1.7692633867263794
Epoch 120, training loss: 8.53394603729248 = 1.7471927404403687 + 1.0 * 6.786753177642822
Epoch 120, val loss: 1.7544809579849243
Epoch 130, training loss: 8.464065551757812 = 1.729714274406433 + 1.0 * 6.734351634979248
Epoch 130, val loss: 1.7379823923110962
Epoch 140, training loss: 8.398313522338867 = 1.7092640399932861 + 1.0 * 6.689049243927002
Epoch 140, val loss: 1.719594120979309
Epoch 150, training loss: 8.339276313781738 = 1.6852041482925415 + 1.0 * 6.654071807861328
Epoch 150, val loss: 1.6983550786972046
Epoch 160, training loss: 8.288208961486816 = 1.656467318534851 + 1.0 * 6.631741523742676
Epoch 160, val loss: 1.6731839179992676
Epoch 170, training loss: 8.228014945983887 = 1.623225450515747 + 1.0 * 6.6047892570495605
Epoch 170, val loss: 1.6445289850234985
Epoch 180, training loss: 8.169563293457031 = 1.585242748260498 + 1.0 * 6.584320545196533
Epoch 180, val loss: 1.6119457483291626
Epoch 190, training loss: 8.109134674072266 = 1.5422801971435547 + 1.0 * 6.566854953765869
Epoch 190, val loss: 1.5755277872085571
Epoch 200, training loss: 8.04684066772461 = 1.4947614669799805 + 1.0 * 6.552078723907471
Epoch 200, val loss: 1.5359848737716675
Epoch 210, training loss: 7.981597423553467 = 1.444962978363037 + 1.0 * 6.53663444519043
Epoch 210, val loss: 1.4953625202178955
Epoch 220, training loss: 7.918719291687012 = 1.3943672180175781 + 1.0 * 6.524352073669434
Epoch 220, val loss: 1.4549907445907593
Epoch 230, training loss: 7.8619794845581055 = 1.3440226316452026 + 1.0 * 6.517956733703613
Epoch 230, val loss: 1.4157276153564453
Epoch 240, training loss: 7.798083782196045 = 1.2954564094543457 + 1.0 * 6.502627372741699
Epoch 240, val loss: 1.3787992000579834
Epoch 250, training loss: 7.740505695343018 = 1.2484849691390991 + 1.0 * 6.492020606994629
Epoch 250, val loss: 1.3437585830688477
Epoch 260, training loss: 7.687312602996826 = 1.2026795148849487 + 1.0 * 6.484632968902588
Epoch 260, val loss: 1.310274362564087
Epoch 270, training loss: 7.639752388000488 = 1.1583635807037354 + 1.0 * 6.481389045715332
Epoch 270, val loss: 1.278197169303894
Epoch 280, training loss: 7.584731101989746 = 1.1152875423431396 + 1.0 * 6.4694437980651855
Epoch 280, val loss: 1.2473472356796265
Epoch 290, training loss: 7.534401893615723 = 1.0725691318511963 + 1.0 * 6.4618330001831055
Epoch 290, val loss: 1.217050313949585
Epoch 300, training loss: 7.485562324523926 = 1.0297901630401611 + 1.0 * 6.4557719230651855
Epoch 300, val loss: 1.1867022514343262
Epoch 310, training loss: 7.447566509246826 = 0.9872421622276306 + 1.0 * 6.460324287414551
Epoch 310, val loss: 1.1565101146697998
Epoch 320, training loss: 7.390076160430908 = 0.9455363154411316 + 1.0 * 6.444540023803711
Epoch 320, val loss: 1.1268854141235352
Epoch 330, training loss: 7.344785213470459 = 0.9041869044303894 + 1.0 * 6.440598487854004
Epoch 330, val loss: 1.0974477529525757
Epoch 340, training loss: 7.298605442047119 = 0.86308354139328 + 1.0 * 6.435522079467773
Epoch 340, val loss: 1.068130612373352
Epoch 350, training loss: 7.261814117431641 = 0.8228601813316345 + 1.0 * 6.438953876495361
Epoch 350, val loss: 1.0394893884658813
Epoch 360, training loss: 7.213912487030029 = 0.7843565344810486 + 1.0 * 6.429555892944336
Epoch 360, val loss: 1.0121349096298218
Epoch 370, training loss: 7.170485496520996 = 0.7472564578056335 + 1.0 * 6.423229217529297
Epoch 370, val loss: 0.9861886501312256
Epoch 380, training loss: 7.13609504699707 = 0.7118246555328369 + 1.0 * 6.4242706298828125
Epoch 380, val loss: 0.961819589138031
Epoch 390, training loss: 7.096569061279297 = 0.6785282492637634 + 1.0 * 6.418040752410889
Epoch 390, val loss: 0.9395168423652649
Epoch 400, training loss: 7.061038017272949 = 0.6470940113067627 + 1.0 * 6.413944244384766
Epoch 400, val loss: 0.9194658994674683
Epoch 410, training loss: 7.035295486450195 = 0.6175771951675415 + 1.0 * 6.417718410491943
Epoch 410, val loss: 0.9015777111053467
Epoch 420, training loss: 6.995271682739258 = 0.5899060964584351 + 1.0 * 6.405365467071533
Epoch 420, val loss: 0.8859569430351257
Epoch 430, training loss: 6.965292930603027 = 0.5639341473579407 + 1.0 * 6.401358604431152
Epoch 430, val loss: 0.8722951412200928
Epoch 440, training loss: 6.940308570861816 = 0.5393883585929871 + 1.0 * 6.400920391082764
Epoch 440, val loss: 0.8603850603103638
Epoch 450, training loss: 6.918543815612793 = 0.5163094997406006 + 1.0 * 6.402234077453613
Epoch 450, val loss: 0.8501867055892944
Epoch 460, training loss: 6.887789726257324 = 0.49472588300704956 + 1.0 * 6.393064022064209
Epoch 460, val loss: 0.8415432572364807
Epoch 470, training loss: 6.868072509765625 = 0.47441497445106506 + 1.0 * 6.393657684326172
Epoch 470, val loss: 0.8341684937477112
Epoch 480, training loss: 6.844781875610352 = 0.4552444815635681 + 1.0 * 6.389537334442139
Epoch 480, val loss: 0.8278616666793823
Epoch 490, training loss: 6.821484565734863 = 0.4368622601032257 + 1.0 * 6.384622097015381
Epoch 490, val loss: 0.8223351836204529
Epoch 500, training loss: 6.8003153800964355 = 0.4191034138202667 + 1.0 * 6.381211757659912
Epoch 500, val loss: 0.8174991011619568
Epoch 510, training loss: 6.791454315185547 = 0.4018409848213196 + 1.0 * 6.389613151550293
Epoch 510, val loss: 0.813277006149292
Epoch 520, training loss: 6.761361122131348 = 0.3851885199546814 + 1.0 * 6.3761725425720215
Epoch 520, val loss: 0.8095217943191528
Epoch 530, training loss: 6.744049072265625 = 0.3689837157726288 + 1.0 * 6.375065326690674
Epoch 530, val loss: 0.8062171936035156
Epoch 540, training loss: 6.7271809577941895 = 0.35308778285980225 + 1.0 * 6.374093055725098
Epoch 540, val loss: 0.8033256530761719
Epoch 550, training loss: 6.713196754455566 = 0.33752986788749695 + 1.0 * 6.375667095184326
Epoch 550, val loss: 0.800921618938446
Epoch 560, training loss: 6.693139553070068 = 0.3223974108695984 + 1.0 * 6.370742321014404
Epoch 560, val loss: 0.7990193367004395
Epoch 570, training loss: 6.673464775085449 = 0.307564377784729 + 1.0 * 6.36590051651001
Epoch 570, val loss: 0.7976319193840027
Epoch 580, training loss: 6.661944389343262 = 0.29300856590270996 + 1.0 * 6.368935585021973
Epoch 580, val loss: 0.7967703342437744
Epoch 590, training loss: 6.648141384124756 = 0.2789546251296997 + 1.0 * 6.369186878204346
Epoch 590, val loss: 0.7964591979980469
Epoch 600, training loss: 6.626389503479004 = 0.2653818726539612 + 1.0 * 6.3610076904296875
Epoch 600, val loss: 0.7966938018798828
Epoch 610, training loss: 6.616659641265869 = 0.25230103731155396 + 1.0 * 6.364358425140381
Epoch 610, val loss: 0.7974854707717896
Epoch 620, training loss: 6.599798202514648 = 0.23978646099567413 + 1.0 * 6.360011577606201
Epoch 620, val loss: 0.7988277673721313
Epoch 630, training loss: 6.585414409637451 = 0.2278991937637329 + 1.0 * 6.357515335083008
Epoch 630, val loss: 0.800665020942688
Epoch 640, training loss: 6.572668075561523 = 0.21654587984085083 + 1.0 * 6.356122016906738
Epoch 640, val loss: 0.8029857277870178
Epoch 650, training loss: 6.560520648956299 = 0.20580922067165375 + 1.0 * 6.354711532592773
Epoch 650, val loss: 0.8058322072029114
Epoch 660, training loss: 6.548956394195557 = 0.19562754034996033 + 1.0 * 6.353328704833984
Epoch 660, val loss: 0.8091052174568176
Epoch 670, training loss: 6.538094997406006 = 0.18600259721279144 + 1.0 * 6.352092266082764
Epoch 670, val loss: 0.812793493270874
Epoch 680, training loss: 6.525536060333252 = 0.17690756916999817 + 1.0 * 6.348628520965576
Epoch 680, val loss: 0.8168802857398987
Epoch 690, training loss: 6.517228126525879 = 0.16832907497882843 + 1.0 * 6.348898887634277
Epoch 690, val loss: 0.8213196396827698
Epoch 700, training loss: 6.5110063552856445 = 0.16023732721805573 + 1.0 * 6.35076904296875
Epoch 700, val loss: 0.826106071472168
Epoch 710, training loss: 6.496025085449219 = 0.15257228910923004 + 1.0 * 6.3434529304504395
Epoch 710, val loss: 0.8311583995819092
Epoch 720, training loss: 6.495200157165527 = 0.14532878994941711 + 1.0 * 6.3498711585998535
Epoch 720, val loss: 0.836449146270752
Epoch 730, training loss: 6.487668514251709 = 0.13852913677692413 + 1.0 * 6.349139213562012
Epoch 730, val loss: 0.8419297933578491
Epoch 740, training loss: 6.472908020019531 = 0.13207605481147766 + 1.0 * 6.340831756591797
Epoch 740, val loss: 0.8475238084793091
Epoch 750, training loss: 6.464984893798828 = 0.1259721964597702 + 1.0 * 6.339012622833252
Epoch 750, val loss: 0.8531956672668457
Epoch 760, training loss: 6.466334819793701 = 0.12017855048179626 + 1.0 * 6.346156120300293
Epoch 760, val loss: 0.859063446521759
Epoch 770, training loss: 6.454689025878906 = 0.11475320160388947 + 1.0 * 6.339935779571533
Epoch 770, val loss: 0.864971399307251
Epoch 780, training loss: 6.446303844451904 = 0.10961033403873444 + 1.0 * 6.336693286895752
Epoch 780, val loss: 0.8710169792175293
Epoch 790, training loss: 6.447136402130127 = 0.10474970191717148 + 1.0 * 6.342386722564697
Epoch 790, val loss: 0.8771606087684631
Epoch 800, training loss: 6.435394287109375 = 0.1001565009355545 + 1.0 * 6.335237979888916
Epoch 800, val loss: 0.8833449482917786
Epoch 810, training loss: 6.428894519805908 = 0.09578411281108856 + 1.0 * 6.333110332489014
Epoch 810, val loss: 0.8895940184593201
Epoch 820, training loss: 6.429752349853516 = 0.09163515269756317 + 1.0 * 6.3381171226501465
Epoch 820, val loss: 0.8959445953369141
Epoch 830, training loss: 6.418308734893799 = 0.08771425485610962 + 1.0 * 6.330594539642334
Epoch 830, val loss: 0.9022282361984253
Epoch 840, training loss: 6.415652275085449 = 0.08399494737386703 + 1.0 * 6.331657409667969
Epoch 840, val loss: 0.908566415309906
Epoch 850, training loss: 6.411015033721924 = 0.0804586261510849 + 1.0 * 6.330556392669678
Epoch 850, val loss: 0.9149133563041687
Epoch 860, training loss: 6.405825614929199 = 0.07711219042539597 + 1.0 * 6.328713417053223
Epoch 860, val loss: 0.921191394329071
Epoch 870, training loss: 6.402652263641357 = 0.07392016798257828 + 1.0 * 6.328732013702393
Epoch 870, val loss: 0.9275579452514648
Epoch 880, training loss: 6.397069931030273 = 0.07087957859039307 + 1.0 * 6.32619047164917
Epoch 880, val loss: 0.9338310956954956
Epoch 890, training loss: 6.395686149597168 = 0.06798316538333893 + 1.0 * 6.32770299911499
Epoch 890, val loss: 0.9401875138282776
Epoch 900, training loss: 6.38940954208374 = 0.06520771235227585 + 1.0 * 6.324202060699463
Epoch 900, val loss: 0.9464015960693359
Epoch 910, training loss: 6.396223068237305 = 0.0625593289732933 + 1.0 * 6.3336639404296875
Epoch 910, val loss: 0.9527774453163147
Epoch 920, training loss: 6.3840484619140625 = 0.060064896941185 + 1.0 * 6.323983669281006
Epoch 920, val loss: 0.9588751792907715
Epoch 930, training loss: 6.378084659576416 = 0.05767454579472542 + 1.0 * 6.320410251617432
Epoch 930, val loss: 0.9650515913963318
Epoch 940, training loss: 6.373116493225098 = 0.05539592728018761 + 1.0 * 6.317720413208008
Epoch 940, val loss: 0.9711592793464661
Epoch 950, training loss: 6.378777503967285 = 0.05321574956178665 + 1.0 * 6.3255615234375
Epoch 950, val loss: 0.9772740006446838
Epoch 960, training loss: 6.377407073974609 = 0.051140520721673965 + 1.0 * 6.326266765594482
Epoch 960, val loss: 0.9832677841186523
Epoch 970, training loss: 6.36625337600708 = 0.04917800426483154 + 1.0 * 6.317075252532959
Epoch 970, val loss: 0.9890831708908081
Epoch 980, training loss: 6.362351417541504 = 0.047301627695560455 + 1.0 * 6.315049648284912
Epoch 980, val loss: 0.9949986338615417
Epoch 990, training loss: 6.360610485076904 = 0.045508336275815964 + 1.0 * 6.3151021003723145
Epoch 990, val loss: 1.0008316040039062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 10.542235374450684 = 1.945407509803772 + 1.0 * 8.596827507019043
Epoch 0, val loss: 1.9382163286209106
Epoch 10, training loss: 10.532466888427734 = 1.9358903169631958 + 1.0 * 8.596576690673828
Epoch 10, val loss: 1.9288930892944336
Epoch 20, training loss: 10.518836975097656 = 1.9242615699768066 + 1.0 * 8.594574928283691
Epoch 20, val loss: 1.916758418083191
Epoch 30, training loss: 10.488225936889648 = 1.9083482027053833 + 1.0 * 8.579877853393555
Epoch 30, val loss: 1.8994543552398682
Epoch 40, training loss: 10.389116287231445 = 1.8873494863510132 + 1.0 * 8.5017671585083
Epoch 40, val loss: 1.8771460056304932
Epoch 50, training loss: 10.08938217163086 = 1.8651422262191772 + 1.0 * 8.22424030303955
Epoch 50, val loss: 1.854583501815796
Epoch 60, training loss: 9.775620460510254 = 1.84474778175354 + 1.0 * 7.930872917175293
Epoch 60, val loss: 1.8347214460372925
Epoch 70, training loss: 9.23861312866211 = 1.8291915655136108 + 1.0 * 7.409421920776367
Epoch 70, val loss: 1.8205821514129639
Epoch 80, training loss: 8.923851013183594 = 1.8168957233428955 + 1.0 * 7.106955051422119
Epoch 80, val loss: 1.8089311122894287
Epoch 90, training loss: 8.719717025756836 = 1.8026702404022217 + 1.0 * 6.917047023773193
Epoch 90, val loss: 1.7949883937835693
Epoch 100, training loss: 8.60582447052002 = 1.7875922918319702 + 1.0 * 6.818232536315918
Epoch 100, val loss: 1.780613660812378
Epoch 110, training loss: 8.526551246643066 = 1.7718702554702759 + 1.0 * 6.754680633544922
Epoch 110, val loss: 1.7665458917617798
Epoch 120, training loss: 8.467399597167969 = 1.7560738325119019 + 1.0 * 6.7113261222839355
Epoch 120, val loss: 1.752967357635498
Epoch 130, training loss: 8.41688060760498 = 1.7398043870925903 + 1.0 * 6.67707633972168
Epoch 130, val loss: 1.7391483783721924
Epoch 140, training loss: 8.36998462677002 = 1.722213864326477 + 1.0 * 6.647770404815674
Epoch 140, val loss: 1.7244033813476562
Epoch 150, training loss: 8.3254976272583 = 1.7023100852966309 + 1.0 * 6.62318754196167
Epoch 150, val loss: 1.7077810764312744
Epoch 160, training loss: 8.28236198425293 = 1.6794044971466064 + 1.0 * 6.602957248687744
Epoch 160, val loss: 1.6886874437332153
Epoch 170, training loss: 8.237189292907715 = 1.6535886526107788 + 1.0 * 6.5836005210876465
Epoch 170, val loss: 1.6672061681747437
Epoch 180, training loss: 8.189146995544434 = 1.6243197917938232 + 1.0 * 6.5648274421691895
Epoch 180, val loss: 1.6428561210632324
Epoch 190, training loss: 8.14189338684082 = 1.5911518335342407 + 1.0 * 6.550741672515869
Epoch 190, val loss: 1.615371584892273
Epoch 200, training loss: 8.089408874511719 = 1.5547680854797363 + 1.0 * 6.534641265869141
Epoch 200, val loss: 1.585168719291687
Epoch 210, training loss: 8.037548065185547 = 1.5152255296707153 + 1.0 * 6.522322177886963
Epoch 210, val loss: 1.552693247795105
Epoch 220, training loss: 7.983689308166504 = 1.4726994037628174 + 1.0 * 6.510990142822266
Epoch 220, val loss: 1.5179979801177979
Epoch 230, training loss: 7.930838108062744 = 1.4274958372116089 + 1.0 * 6.503342151641846
Epoch 230, val loss: 1.4814927577972412
Epoch 240, training loss: 7.874660015106201 = 1.380994439125061 + 1.0 * 6.49366569519043
Epoch 240, val loss: 1.4446552991867065
Epoch 250, training loss: 7.818545818328857 = 1.3339056968688965 + 1.0 * 6.484640121459961
Epoch 250, val loss: 1.4080687761306763
Epoch 260, training loss: 7.763368606567383 = 1.2860333919525146 + 1.0 * 6.477335453033447
Epoch 260, val loss: 1.371496319770813
Epoch 270, training loss: 7.714800834655762 = 1.237926959991455 + 1.0 * 6.476873874664307
Epoch 270, val loss: 1.335255742073059
Epoch 280, training loss: 7.654767036437988 = 1.1907745599746704 + 1.0 * 6.463992595672607
Epoch 280, val loss: 1.3001823425292969
Epoch 290, training loss: 7.6018900871276855 = 1.1443743705749512 + 1.0 * 6.457515716552734
Epoch 290, val loss: 1.2662127017974854
Epoch 300, training loss: 7.5576019287109375 = 1.0986944437026978 + 1.0 * 6.458907604217529
Epoch 300, val loss: 1.2331538200378418
Epoch 310, training loss: 7.499841690063477 = 1.0545796155929565 + 1.0 * 6.4452619552612305
Epoch 310, val loss: 1.2015172243118286
Epoch 320, training loss: 7.451085090637207 = 1.0118201971054077 + 1.0 * 6.43926477432251
Epoch 320, val loss: 1.1711699962615967
Epoch 330, training loss: 7.404273986816406 = 0.9702605605125427 + 1.0 * 6.434013366699219
Epoch 330, val loss: 1.1419867277145386
Epoch 340, training loss: 7.360851764678955 = 0.9298452138900757 + 1.0 * 6.43100643157959
Epoch 340, val loss: 1.1140319108963013
Epoch 350, training loss: 7.318188667297363 = 0.8912222981452942 + 1.0 * 6.426966190338135
Epoch 350, val loss: 1.087733268737793
Epoch 360, training loss: 7.276052474975586 = 0.8546512126922607 + 1.0 * 6.421401500701904
Epoch 360, val loss: 1.0630627870559692
Epoch 370, training loss: 7.236748695373535 = 0.819433867931366 + 1.0 * 6.4173150062561035
Epoch 370, val loss: 1.0398294925689697
Epoch 380, training loss: 7.198324680328369 = 0.7851818203926086 + 1.0 * 6.413142681121826
Epoch 380, val loss: 1.0176433324813843
Epoch 390, training loss: 7.165306568145752 = 0.7523370385169983 + 1.0 * 6.412969589233398
Epoch 390, val loss: 0.9968942403793335
Epoch 400, training loss: 7.129434585571289 = 0.7208836078643799 + 1.0 * 6.408551216125488
Epoch 400, val loss: 0.9775930643081665
Epoch 410, training loss: 7.093534469604492 = 0.6904293894767761 + 1.0 * 6.40310525894165
Epoch 410, val loss: 0.9593727588653564
Epoch 420, training loss: 7.065317153930664 = 0.6609510183334351 + 1.0 * 6.4043660163879395
Epoch 420, val loss: 0.9424377679824829
Epoch 430, training loss: 7.0314788818359375 = 0.6326681971549988 + 1.0 * 6.398810863494873
Epoch 430, val loss: 0.9266847968101501
Epoch 440, training loss: 6.999333381652832 = 0.6051270961761475 + 1.0 * 6.394206523895264
Epoch 440, val loss: 0.9121671319007874
Epoch 450, training loss: 6.972446441650391 = 0.5782787799835205 + 1.0 * 6.394167423248291
Epoch 450, val loss: 0.8987938761711121
Epoch 460, training loss: 6.941210746765137 = 0.5521600246429443 + 1.0 * 6.3890509605407715
Epoch 460, val loss: 0.8865948915481567
Epoch 470, training loss: 6.915727615356445 = 0.5266644954681396 + 1.0 * 6.389063358306885
Epoch 470, val loss: 0.8756021857261658
Epoch 480, training loss: 6.894682884216309 = 0.5018233060836792 + 1.0 * 6.39285945892334
Epoch 480, val loss: 0.8660961985588074
Epoch 490, training loss: 6.860984802246094 = 0.47795429825782776 + 1.0 * 6.383030414581299
Epoch 490, val loss: 0.8577562570571899
Epoch 500, training loss: 6.835484504699707 = 0.45470526814460754 + 1.0 * 6.380779266357422
Epoch 500, val loss: 0.8506699800491333
Epoch 510, training loss: 6.817039966583252 = 0.432099848985672 + 1.0 * 6.384940147399902
Epoch 510, val loss: 0.8448030352592468
Epoch 520, training loss: 6.790558338165283 = 0.41042888164520264 + 1.0 * 6.380129337310791
Epoch 520, val loss: 0.8398767113685608
Epoch 530, training loss: 6.765385150909424 = 0.389567494392395 + 1.0 * 6.375817775726318
Epoch 530, val loss: 0.8360182046890259
Epoch 540, training loss: 6.742071151733398 = 0.36943677067756653 + 1.0 * 6.372634410858154
Epoch 540, val loss: 0.8331129550933838
Epoch 550, training loss: 6.750802040100098 = 0.35000911355018616 + 1.0 * 6.400793075561523
Epoch 550, val loss: 0.831036388874054
Epoch 560, training loss: 6.7128095626831055 = 0.3317279517650604 + 1.0 * 6.381081581115723
Epoch 560, val loss: 0.8295299410820007
Epoch 570, training loss: 6.681607723236084 = 0.3143175542354584 + 1.0 * 6.367290019989014
Epoch 570, val loss: 0.8287188410758972
Epoch 580, training loss: 6.664474010467529 = 0.29760149121284485 + 1.0 * 6.366872310638428
Epoch 580, val loss: 0.8285993933677673
Epoch 590, training loss: 6.645447731018066 = 0.28153595328330994 + 1.0 * 6.3639116287231445
Epoch 590, val loss: 0.8290773630142212
Epoch 600, training loss: 6.628076076507568 = 0.26607638597488403 + 1.0 * 6.36199951171875
Epoch 600, val loss: 0.8300779461860657
Epoch 610, training loss: 6.623584270477295 = 0.2512269914150238 + 1.0 * 6.372357368469238
Epoch 610, val loss: 0.8315664529800415
Epoch 620, training loss: 6.598162651062012 = 0.2372114211320877 + 1.0 * 6.3609514236450195
Epoch 620, val loss: 0.8333098888397217
Epoch 630, training loss: 6.581948757171631 = 0.22392693161964417 + 1.0 * 6.3580217361450195
Epoch 630, val loss: 0.8354754447937012
Epoch 640, training loss: 6.567783355712891 = 0.21129107475280762 + 1.0 * 6.356492042541504
Epoch 640, val loss: 0.8381681442260742
Epoch 650, training loss: 6.555392742156982 = 0.19926951825618744 + 1.0 * 6.356123447418213
Epoch 650, val loss: 0.8412858843803406
Epoch 660, training loss: 6.544691562652588 = 0.18789848685264587 + 1.0 * 6.35679292678833
Epoch 660, val loss: 0.8447813391685486
Epoch 670, training loss: 6.532055377960205 = 0.17725446820259094 + 1.0 * 6.354800701141357
Epoch 670, val loss: 0.8484751582145691
Epoch 680, training loss: 6.51926326751709 = 0.16724753379821777 + 1.0 * 6.352015495300293
Epoch 680, val loss: 0.8525864481925964
Epoch 690, training loss: 6.507962226867676 = 0.1578180491924286 + 1.0 * 6.350144386291504
Epoch 690, val loss: 0.8571100234985352
Epoch 700, training loss: 6.500488758087158 = 0.14897771179676056 + 1.0 * 6.351511001586914
Epoch 700, val loss: 0.8619502186775208
Epoch 710, training loss: 6.489004611968994 = 0.14069493114948273 + 1.0 * 6.348309516906738
Epoch 710, val loss: 0.8670517206192017
Epoch 720, training loss: 6.481292724609375 = 0.13301554322242737 + 1.0 * 6.3482770919799805
Epoch 720, val loss: 0.8723087906837463
Epoch 730, training loss: 6.4701924324035645 = 0.12580333650112152 + 1.0 * 6.344388961791992
Epoch 730, val loss: 0.8779256939888
Epoch 740, training loss: 6.4617838859558105 = 0.11904080957174301 + 1.0 * 6.342742919921875
Epoch 740, val loss: 0.8838077187538147
Epoch 750, training loss: 6.461735725402832 = 0.11270586401224136 + 1.0 * 6.349030017852783
Epoch 750, val loss: 0.889820396900177
Epoch 760, training loss: 6.452001571655273 = 0.10680456459522247 + 1.0 * 6.3451972007751465
Epoch 760, val loss: 0.8958713412284851
Epoch 770, training loss: 6.443237781524658 = 0.10129266232252121 + 1.0 * 6.341945171356201
Epoch 770, val loss: 0.9020042419433594
Epoch 780, training loss: 6.437462329864502 = 0.09612944722175598 + 1.0 * 6.341332912445068
Epoch 780, val loss: 0.9082990884780884
Epoch 790, training loss: 6.432399272918701 = 0.09128783643245697 + 1.0 * 6.341111660003662
Epoch 790, val loss: 0.9146829843521118
Epoch 800, training loss: 6.423590660095215 = 0.08678138256072998 + 1.0 * 6.336809158325195
Epoch 800, val loss: 0.9209761619567871
Epoch 810, training loss: 6.417579174041748 = 0.08252198249101639 + 1.0 * 6.335057258605957
Epoch 810, val loss: 0.9274723529815674
Epoch 820, training loss: 6.412719249725342 = 0.07852659374475479 + 1.0 * 6.334192752838135
Epoch 820, val loss: 0.9340559840202332
Epoch 830, training loss: 6.40946626663208 = 0.07476092129945755 + 1.0 * 6.334705352783203
Epoch 830, val loss: 0.9406536221504211
Epoch 840, training loss: 6.404727935791016 = 0.07121512293815613 + 1.0 * 6.333512783050537
Epoch 840, val loss: 0.9471985697746277
Epoch 850, training loss: 6.409276008605957 = 0.06787002086639404 + 1.0 * 6.341405868530273
Epoch 850, val loss: 0.9536936283111572
Epoch 860, training loss: 6.393732070922852 = 0.06476359814405441 + 1.0 * 6.328968524932861
Epoch 860, val loss: 0.9600604176521301
Epoch 870, training loss: 6.389828681945801 = 0.0618298314511776 + 1.0 * 6.327998638153076
Epoch 870, val loss: 0.9665434956550598
Epoch 880, training loss: 6.38541316986084 = 0.059050966054201126 + 1.0 * 6.326362133026123
Epoch 880, val loss: 0.9731534719467163
Epoch 890, training loss: 6.381659507751465 = 0.05642170459032059 + 1.0 * 6.32523775100708
Epoch 890, val loss: 0.979706346988678
Epoch 900, training loss: 6.386128902435303 = 0.05393598973751068 + 1.0 * 6.332192897796631
Epoch 900, val loss: 0.9862707257270813
Epoch 910, training loss: 6.378787040710449 = 0.051608625799417496 + 1.0 * 6.327178478240967
Epoch 910, val loss: 0.9927137494087219
Epoch 920, training loss: 6.374100208282471 = 0.049416858702898026 + 1.0 * 6.32468318939209
Epoch 920, val loss: 0.999003529548645
Epoch 930, training loss: 6.370631694793701 = 0.04734814539551735 + 1.0 * 6.323283672332764
Epoch 930, val loss: 1.0054419040679932
Epoch 940, training loss: 6.368853569030762 = 0.04540093243122101 + 1.0 * 6.323452472686768
Epoch 940, val loss: 1.0118141174316406
Epoch 950, training loss: 6.367917537689209 = 0.04356197640299797 + 1.0 * 6.324355602264404
Epoch 950, val loss: 1.0180244445800781
Epoch 960, training loss: 6.362011432647705 = 0.041830629110336304 + 1.0 * 6.320180892944336
Epoch 960, val loss: 1.02420973777771
Epoch 970, training loss: 6.359055519104004 = 0.04020285978913307 + 1.0 * 6.318852424621582
Epoch 970, val loss: 1.0302889347076416
Epoch 980, training loss: 6.355905532836914 = 0.03865770623087883 + 1.0 * 6.3172478675842285
Epoch 980, val loss: 1.0365008115768433
Epoch 990, training loss: 6.356771945953369 = 0.03719455376267433 + 1.0 * 6.319577217102051
Epoch 990, val loss: 1.0426782369613647
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8149710068529257
The final CL Acc:0.75185, 0.01386, The final GNN Acc:0.81357, 0.00199
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13254])
remove edge: torch.Size([2, 7948])
updated graph: torch.Size([2, 10646])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.5299711227417 = 1.9331527948379517 + 1.0 * 8.596817970275879
Epoch 0, val loss: 1.9339410066604614
Epoch 10, training loss: 10.520376205444336 = 1.923960566520691 + 1.0 * 8.596415519714355
Epoch 10, val loss: 1.9253880977630615
Epoch 20, training loss: 10.505465507507324 = 1.9125279188156128 + 1.0 * 8.592937469482422
Epoch 20, val loss: 1.9143853187561035
Epoch 30, training loss: 10.461623191833496 = 1.8968056440353394 + 1.0 * 8.564817428588867
Epoch 30, val loss: 1.8990697860717773
Epoch 40, training loss: 10.247902870178223 = 1.8776180744171143 + 1.0 * 8.370285034179688
Epoch 40, val loss: 1.8808358907699585
Epoch 50, training loss: 9.794892311096191 = 1.8566206693649292 + 1.0 * 7.938271522521973
Epoch 50, val loss: 1.8613026142120361
Epoch 60, training loss: 9.38920783996582 = 1.83872652053833 + 1.0 * 7.550481796264648
Epoch 60, val loss: 1.8449515104293823
Epoch 70, training loss: 9.009977340698242 = 1.8256423473358154 + 1.0 * 7.184335231781006
Epoch 70, val loss: 1.8321542739868164
Epoch 80, training loss: 8.831371307373047 = 1.8120741844177246 + 1.0 * 7.0192975997924805
Epoch 80, val loss: 1.819104790687561
Epoch 90, training loss: 8.682229042053223 = 1.795810580253601 + 1.0 * 6.886418342590332
Epoch 90, val loss: 1.8047763109207153
Epoch 100, training loss: 8.57026481628418 = 1.7809360027313232 + 1.0 * 6.789328575134277
Epoch 100, val loss: 1.791071891784668
Epoch 110, training loss: 8.485383987426758 = 1.7666597366333008 + 1.0 * 6.718723773956299
Epoch 110, val loss: 1.7772598266601562
Epoch 120, training loss: 8.415197372436523 = 1.7514451742172241 + 1.0 * 6.66375207901001
Epoch 120, val loss: 1.7628107070922852
Epoch 130, training loss: 8.356179237365723 = 1.7346380949020386 + 1.0 * 6.6215410232543945
Epoch 130, val loss: 1.7475368976593018
Epoch 140, training loss: 8.304617881774902 = 1.7155901193618774 + 1.0 * 6.589027404785156
Epoch 140, val loss: 1.7309859991073608
Epoch 150, training loss: 8.2560453414917 = 1.6935175657272339 + 1.0 * 6.562528133392334
Epoch 150, val loss: 1.7123292684555054
Epoch 160, training loss: 8.209283828735352 = 1.6677556037902832 + 1.0 * 6.54152774810791
Epoch 160, val loss: 1.6908890008926392
Epoch 170, training loss: 8.160505294799805 = 1.6375865936279297 + 1.0 * 6.522918701171875
Epoch 170, val loss: 1.6659883260726929
Epoch 180, training loss: 8.109443664550781 = 1.6024937629699707 + 1.0 * 6.5069499015808105
Epoch 180, val loss: 1.6370279788970947
Epoch 190, training loss: 8.055044174194336 = 1.5619187355041504 + 1.0 * 6.493124961853027
Epoch 190, val loss: 1.6035939455032349
Epoch 200, training loss: 7.997210502624512 = 1.5153535604476929 + 1.0 * 6.481856822967529
Epoch 200, val loss: 1.5652307271957397
Epoch 210, training loss: 7.935102462768555 = 1.4630478620529175 + 1.0 * 6.472054481506348
Epoch 210, val loss: 1.522334337234497
Epoch 220, training loss: 7.869001388549805 = 1.4063427448272705 + 1.0 * 6.462658405303955
Epoch 220, val loss: 1.476004958152771
Epoch 230, training loss: 7.8007588386535645 = 1.3462265729904175 + 1.0 * 6.454532146453857
Epoch 230, val loss: 1.4269351959228516
Epoch 240, training loss: 7.732826232910156 = 1.284651517868042 + 1.0 * 6.448174476623535
Epoch 240, val loss: 1.3768935203552246
Epoch 250, training loss: 7.666171073913574 = 1.2242863178253174 + 1.0 * 6.441884517669678
Epoch 250, val loss: 1.3282164335250854
Epoch 260, training loss: 7.60152006149292 = 1.166845440864563 + 1.0 * 6.4346747398376465
Epoch 260, val loss: 1.282136082649231
Epoch 270, training loss: 7.5414814949035645 = 1.1129544973373413 + 1.0 * 6.428526878356934
Epoch 270, val loss: 1.2392959594726562
Epoch 280, training loss: 7.493837356567383 = 1.0632127523422241 + 1.0 * 6.430624485015869
Epoch 280, val loss: 1.200141429901123
Epoch 290, training loss: 7.442043304443359 = 1.018437385559082 + 1.0 * 6.423605918884277
Epoch 290, val loss: 1.165247917175293
Epoch 300, training loss: 7.392506122589111 = 0.9779891967773438 + 1.0 * 6.414516925811768
Epoch 300, val loss: 1.1339290142059326
Epoch 310, training loss: 7.3500566482543945 = 0.9405639171600342 + 1.0 * 6.409492492675781
Epoch 310, val loss: 1.1051613092422485
Epoch 320, training loss: 7.311666011810303 = 0.9053815007209778 + 1.0 * 6.406284332275391
Epoch 320, val loss: 1.0781303644180298
Epoch 330, training loss: 7.274216175079346 = 0.8718275427818298 + 1.0 * 6.402388572692871
Epoch 330, val loss: 1.0523217916488647
Epoch 340, training loss: 7.2362589836120605 = 0.8389665484428406 + 1.0 * 6.397292613983154
Epoch 340, val loss: 1.0270565748214722
Epoch 350, training loss: 7.201479434967041 = 0.8064316511154175 + 1.0 * 6.395047664642334
Epoch 350, val loss: 1.0019601583480835
Epoch 360, training loss: 7.164996147155762 = 0.774093747138977 + 1.0 * 6.390902519226074
Epoch 360, val loss: 0.9772182703018188
Epoch 370, training loss: 7.128959655761719 = 0.7417101860046387 + 1.0 * 6.38724946975708
Epoch 370, val loss: 0.9526174068450928
Epoch 380, training loss: 7.094934940338135 = 0.7091323733329773 + 1.0 * 6.385802745819092
Epoch 380, val loss: 0.9281608462333679
Epoch 390, training loss: 7.064235687255859 = 0.6765877604484558 + 1.0 * 6.387648105621338
Epoch 390, val loss: 0.9042741656303406
Epoch 400, training loss: 7.025043487548828 = 0.6446542739868164 + 1.0 * 6.380389213562012
Epoch 400, val loss: 0.8814418315887451
Epoch 410, training loss: 6.990477561950684 = 0.6132382750511169 + 1.0 * 6.377239227294922
Epoch 410, val loss: 0.8597707152366638
Epoch 420, training loss: 6.957025527954102 = 0.5824600458145142 + 1.0 * 6.374565601348877
Epoch 420, val loss: 0.8394339084625244
Epoch 430, training loss: 6.936564922332764 = 0.5524917840957642 + 1.0 * 6.384073257446289
Epoch 430, val loss: 0.8205969929695129
Epoch 440, training loss: 6.898662567138672 = 0.5238199234008789 + 1.0 * 6.374842643737793
Epoch 440, val loss: 0.8035059571266174
Epoch 450, training loss: 6.8659467697143555 = 0.49626585841178894 + 1.0 * 6.369680881500244
Epoch 450, val loss: 0.7880844473838806
Epoch 460, training loss: 6.839295387268066 = 0.4697651267051697 + 1.0 * 6.369530200958252
Epoch 460, val loss: 0.7740974426269531
Epoch 470, training loss: 6.809787273406982 = 0.4443584680557251 + 1.0 * 6.365428924560547
Epoch 470, val loss: 0.7614949345588684
Epoch 480, training loss: 6.7887654304504395 = 0.41998201608657837 + 1.0 * 6.368783473968506
Epoch 480, val loss: 0.7501221299171448
Epoch 490, training loss: 6.75978946685791 = 0.3968036472797394 + 1.0 * 6.362985610961914
Epoch 490, val loss: 0.7399513721466064
Epoch 500, training loss: 6.732792377471924 = 0.37458622455596924 + 1.0 * 6.358206272125244
Epoch 500, val loss: 0.7307935357093811
Epoch 510, training loss: 6.712924957275391 = 0.3532935678958893 + 1.0 * 6.359631538391113
Epoch 510, val loss: 0.7225301265716553
Epoch 520, training loss: 6.696159839630127 = 0.3329768478870392 + 1.0 * 6.36318302154541
Epoch 520, val loss: 0.7151451706886292
Epoch 530, training loss: 6.6697258949279785 = 0.31370919942855835 + 1.0 * 6.356016635894775
Epoch 530, val loss: 0.7087311744689941
Epoch 540, training loss: 6.6473469734191895 = 0.2952256202697754 + 1.0 * 6.352121353149414
Epoch 540, val loss: 0.7030959129333496
Epoch 550, training loss: 6.629972457885742 = 0.27745428681373596 + 1.0 * 6.352518081665039
Epoch 550, val loss: 0.6982021927833557
Epoch 560, training loss: 6.619462490081787 = 0.26045557856559753 + 1.0 * 6.359006881713867
Epoch 560, val loss: 0.6939266324043274
Epoch 570, training loss: 6.592342376708984 = 0.24424929916858673 + 1.0 * 6.348093032836914
Epoch 570, val loss: 0.6905576586723328
Epoch 580, training loss: 6.574501037597656 = 0.22881090641021729 + 1.0 * 6.3456902503967285
Epoch 580, val loss: 0.68794846534729
Epoch 590, training loss: 6.565829753875732 = 0.21409431099891663 + 1.0 * 6.351735591888428
Epoch 590, val loss: 0.6859996914863586
Epoch 600, training loss: 6.543812274932861 = 0.20021376013755798 + 1.0 * 6.343598365783691
Epoch 600, val loss: 0.6847947239875793
Epoch 610, training loss: 6.528927326202393 = 0.18709665536880493 + 1.0 * 6.341830730438232
Epoch 610, val loss: 0.6843087673187256
Epoch 620, training loss: 6.519664764404297 = 0.174763023853302 + 1.0 * 6.3449015617370605
Epoch 620, val loss: 0.6844603419303894
Epoch 630, training loss: 6.507994651794434 = 0.16329868137836456 + 1.0 * 6.344696044921875
Epoch 630, val loss: 0.6853251457214355
Epoch 640, training loss: 6.492241859436035 = 0.15262335538864136 + 1.0 * 6.339618682861328
Epoch 640, val loss: 0.6867735981941223
Epoch 650, training loss: 6.479827880859375 = 0.14276275038719177 + 1.0 * 6.33706521987915
Epoch 650, val loss: 0.6888527274131775
Epoch 660, training loss: 6.46907377243042 = 0.13362126052379608 + 1.0 * 6.335452556610107
Epoch 660, val loss: 0.6914554834365845
Epoch 670, training loss: 6.464272499084473 = 0.12518183887004852 + 1.0 * 6.339090824127197
Epoch 670, val loss: 0.6945236921310425
Epoch 680, training loss: 6.44962739944458 = 0.11742899566888809 + 1.0 * 6.332198619842529
Epoch 680, val loss: 0.697936475276947
Epoch 690, training loss: 6.443338394165039 = 0.11028236895799637 + 1.0 * 6.3330559730529785
Epoch 690, val loss: 0.7017563581466675
Epoch 700, training loss: 6.434739112854004 = 0.1036885529756546 + 1.0 * 6.331050395965576
Epoch 700, val loss: 0.7058269381523132
Epoch 710, training loss: 6.427433967590332 = 0.09761318564414978 + 1.0 * 6.32982063293457
Epoch 710, val loss: 0.7102184295654297
Epoch 720, training loss: 6.421043395996094 = 0.09200242906808853 + 1.0 * 6.329041004180908
Epoch 720, val loss: 0.7148534059524536
Epoch 730, training loss: 6.414156913757324 = 0.08681964129209518 + 1.0 * 6.327337265014648
Epoch 730, val loss: 0.7196245789527893
Epoch 740, training loss: 6.409487724304199 = 0.08203204721212387 + 1.0 * 6.327455520629883
Epoch 740, val loss: 0.7245835065841675
Epoch 750, training loss: 6.400974750518799 = 0.07758700102567673 + 1.0 * 6.323387622833252
Epoch 750, val loss: 0.729659378528595
Epoch 760, training loss: 6.3970160484313965 = 0.07344397902488708 + 1.0 * 6.323572158813477
Epoch 760, val loss: 0.7347960472106934
Epoch 770, training loss: 6.392650604248047 = 0.06959802657365799 + 1.0 * 6.323052406311035
Epoch 770, val loss: 0.7400630712509155
Epoch 780, training loss: 6.38933801651001 = 0.06602751463651657 + 1.0 * 6.323310375213623
Epoch 780, val loss: 0.7452579736709595
Epoch 790, training loss: 6.385125637054443 = 0.06271194666624069 + 1.0 * 6.322413921356201
Epoch 790, val loss: 0.7505548596382141
Epoch 800, training loss: 6.37909460067749 = 0.05961904302239418 + 1.0 * 6.3194756507873535
Epoch 800, val loss: 0.7559167146682739
Epoch 810, training loss: 6.374094486236572 = 0.056732263416051865 + 1.0 * 6.317362308502197
Epoch 810, val loss: 0.7612320780754089
Epoch 820, training loss: 6.372905731201172 = 0.05402561277151108 + 1.0 * 6.318880081176758
Epoch 820, val loss: 0.7665554285049438
Epoch 830, training loss: 6.371487617492676 = 0.05150556191802025 + 1.0 * 6.319982051849365
Epoch 830, val loss: 0.7718713283538818
Epoch 840, training loss: 6.366205215454102 = 0.04915272817015648 + 1.0 * 6.317052364349365
Epoch 840, val loss: 0.7771891951560974
Epoch 850, training loss: 6.360987663269043 = 0.046950142830610275 + 1.0 * 6.314037322998047
Epoch 850, val loss: 0.7824348211288452
Epoch 860, training loss: 6.359375953674316 = 0.044885601848363876 + 1.0 * 6.31449031829834
Epoch 860, val loss: 0.7876895070075989
Epoch 870, training loss: 6.353649616241455 = 0.04294578358530998 + 1.0 * 6.310703754425049
Epoch 870, val loss: 0.7928460240364075
Epoch 880, training loss: 6.362969398498535 = 0.04112166538834572 + 1.0 * 6.321847915649414
Epoch 880, val loss: 0.7980070114135742
Epoch 890, training loss: 6.349074363708496 = 0.03941819816827774 + 1.0 * 6.309656143188477
Epoch 890, val loss: 0.8031095266342163
Epoch 900, training loss: 6.3448638916015625 = 0.03781354799866676 + 1.0 * 6.3070502281188965
Epoch 900, val loss: 0.8081478476524353
Epoch 910, training loss: 6.344147682189941 = 0.036299046128988266 + 1.0 * 6.3078484535217285
Epoch 910, val loss: 0.8131560683250427
Epoch 920, training loss: 6.344868183135986 = 0.03486993536353111 + 1.0 * 6.309998035430908
Epoch 920, val loss: 0.8181248307228088
Epoch 930, training loss: 6.339861869812012 = 0.033527448773384094 + 1.0 * 6.306334495544434
Epoch 930, val loss: 0.8229109644889832
Epoch 940, training loss: 6.339004993438721 = 0.032260626554489136 + 1.0 * 6.306744575500488
Epoch 940, val loss: 0.8277669548988342
Epoch 950, training loss: 6.333462715148926 = 0.03106451779603958 + 1.0 * 6.302398204803467
Epoch 950, val loss: 0.8325077891349792
Epoch 960, training loss: 6.331968307495117 = 0.02993449755012989 + 1.0 * 6.3020339012146
Epoch 960, val loss: 0.8371919989585876
Epoch 970, training loss: 6.330958366394043 = 0.02886144630610943 + 1.0 * 6.302096843719482
Epoch 970, val loss: 0.8418706655502319
Epoch 980, training loss: 6.333498954772949 = 0.027843646705150604 + 1.0 * 6.305655479431152
Epoch 980, val loss: 0.8464481234550476
Epoch 990, training loss: 6.326988697052002 = 0.026885373517870903 + 1.0 * 6.300103187561035
Epoch 990, val loss: 0.8509476184844971
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 10.538935661315918 = 1.94210684299469 + 1.0 * 8.59682846069336
Epoch 0, val loss: 1.941410779953003
Epoch 10, training loss: 10.528157234191895 = 1.9315954446792603 + 1.0 * 8.596561431884766
Epoch 10, val loss: 1.9314385652542114
Epoch 20, training loss: 10.51295280456543 = 1.918500304222107 + 1.0 * 8.594452857971191
Epoch 20, val loss: 1.9185541868209839
Epoch 30, training loss: 10.476295471191406 = 1.90012526512146 + 1.0 * 8.576169967651367
Epoch 30, val loss: 1.900217056274414
Epoch 40, training loss: 10.339954376220703 = 1.8758212327957153 + 1.0 * 8.464133262634277
Epoch 40, val loss: 1.8766779899597168
Epoch 50, training loss: 9.923877716064453 = 1.849890112876892 + 1.0 * 8.07398796081543
Epoch 50, val loss: 1.8522545099258423
Epoch 60, training loss: 9.463321685791016 = 1.8261648416519165 + 1.0 * 7.637156963348389
Epoch 60, val loss: 1.8303821086883545
Epoch 70, training loss: 9.072080612182617 = 1.8096152544021606 + 1.0 * 7.262465000152588
Epoch 70, val loss: 1.8148515224456787
Epoch 80, training loss: 8.81887435913086 = 1.7944880723953247 + 1.0 * 7.024386405944824
Epoch 80, val loss: 1.7998812198638916
Epoch 90, training loss: 8.67574691772461 = 1.7781939506530762 + 1.0 * 6.897552490234375
Epoch 90, val loss: 1.7847108840942383
Epoch 100, training loss: 8.597702026367188 = 1.7582134008407593 + 1.0 * 6.839488983154297
Epoch 100, val loss: 1.7672456502914429
Epoch 110, training loss: 8.518919944763184 = 1.7378236055374146 + 1.0 * 6.781096458435059
Epoch 110, val loss: 1.7499430179595947
Epoch 120, training loss: 8.4461088180542 = 1.7172902822494507 + 1.0 * 6.728818416595459
Epoch 120, val loss: 1.731687307357788
Epoch 130, training loss: 8.379860877990723 = 1.6938841342926025 + 1.0 * 6.685976505279541
Epoch 130, val loss: 1.710891842842102
Epoch 140, training loss: 8.315412521362305 = 1.6659026145935059 + 1.0 * 6.649510383605957
Epoch 140, val loss: 1.6867680549621582
Epoch 150, training loss: 8.252446174621582 = 1.6341168880462646 + 1.0 * 6.618329048156738
Epoch 150, val loss: 1.6598856449127197
Epoch 160, training loss: 8.18857479095459 = 1.5989689826965332 + 1.0 * 6.589605808258057
Epoch 160, val loss: 1.6301122903823853
Epoch 170, training loss: 8.125195503234863 = 1.559479832649231 + 1.0 * 6.565715312957764
Epoch 170, val loss: 1.5967479944229126
Epoch 180, training loss: 8.061724662780762 = 1.5157965421676636 + 1.0 * 6.545928001403809
Epoch 180, val loss: 1.5599968433380127
Epoch 190, training loss: 7.999175071716309 = 1.4689247608184814 + 1.0 * 6.530250072479248
Epoch 190, val loss: 1.520842432975769
Epoch 200, training loss: 7.938391208648682 = 1.4211649894714355 + 1.0 * 6.517226219177246
Epoch 200, val loss: 1.481322169303894
Epoch 210, training loss: 7.877182483673096 = 1.3726496696472168 + 1.0 * 6.504532814025879
Epoch 210, val loss: 1.4417595863342285
Epoch 220, training loss: 7.815862655639648 = 1.3241573572158813 + 1.0 * 6.491705417633057
Epoch 220, val loss: 1.4027451276779175
Epoch 230, training loss: 7.758896827697754 = 1.2759592533111572 + 1.0 * 6.482937335968018
Epoch 230, val loss: 1.3647853136062622
Epoch 240, training loss: 7.70159912109375 = 1.2289817333221436 + 1.0 * 6.472617149353027
Epoch 240, val loss: 1.3286031484603882
Epoch 250, training loss: 7.645763397216797 = 1.1830732822418213 + 1.0 * 6.4626898765563965
Epoch 250, val loss: 1.293964147567749
Epoch 260, training loss: 7.59283447265625 = 1.1378867626190186 + 1.0 * 6.4549479484558105
Epoch 260, val loss: 1.260615587234497
Epoch 270, training loss: 7.539741039276123 = 1.0935970544815063 + 1.0 * 6.446144104003906
Epoch 270, val loss: 1.228641390800476
Epoch 280, training loss: 7.493413925170898 = 1.0501139163970947 + 1.0 * 6.443300247192383
Epoch 280, val loss: 1.1978274583816528
Epoch 290, training loss: 7.4411821365356445 = 1.0079376697540283 + 1.0 * 6.433244705200195
Epoch 290, val loss: 1.1683906316757202
Epoch 300, training loss: 7.393014430999756 = 0.9668091535568237 + 1.0 * 6.426205158233643
Epoch 300, val loss: 1.140147089958191
Epoch 310, training loss: 7.3517303466796875 = 0.9265609979629517 + 1.0 * 6.425169467926025
Epoch 310, val loss: 1.1127734184265137
Epoch 320, training loss: 7.304285526275635 = 0.8878716826438904 + 1.0 * 6.4164137840271
Epoch 320, val loss: 1.0864075422286987
Epoch 330, training loss: 7.260473251342773 = 0.8505774140357971 + 1.0 * 6.409895896911621
Epoch 330, val loss: 1.061236023902893
Epoch 340, training loss: 7.219692230224609 = 0.8143858909606934 + 1.0 * 6.405306339263916
Epoch 340, val loss: 1.037129521369934
Epoch 350, training loss: 7.183772087097168 = 0.7795031666755676 + 1.0 * 6.404268741607666
Epoch 350, val loss: 1.0140936374664307
Epoch 360, training loss: 7.146331787109375 = 0.746455729007721 + 1.0 * 6.399876117706299
Epoch 360, val loss: 0.9925206899642944
Epoch 370, training loss: 7.107691764831543 = 0.7146711945533752 + 1.0 * 6.3930206298828125
Epoch 370, val loss: 0.9722425937652588
Epoch 380, training loss: 7.078232765197754 = 0.6839516162872314 + 1.0 * 6.394281387329102
Epoch 380, val loss: 0.9530426263809204
Epoch 390, training loss: 7.041219711303711 = 0.6545962691307068 + 1.0 * 6.386623382568359
Epoch 390, val loss: 0.9349366426467896
Epoch 400, training loss: 7.009515285491943 = 0.6263002157211304 + 1.0 * 6.383214950561523
Epoch 400, val loss: 0.918039083480835
Epoch 410, training loss: 6.983356952667236 = 0.5989474058151245 + 1.0 * 6.384409427642822
Epoch 410, val loss: 0.9021126627922058
Epoch 420, training loss: 6.95151424407959 = 0.572749674320221 + 1.0 * 6.378764629364014
Epoch 420, val loss: 0.8872627019882202
Epoch 430, training loss: 6.922754764556885 = 0.5477362275123596 + 1.0 * 6.37501859664917
Epoch 430, val loss: 0.8734857439994812
Epoch 440, training loss: 6.894610404968262 = 0.5235135555267334 + 1.0 * 6.371096611022949
Epoch 440, val loss: 0.8605095744132996
Epoch 450, training loss: 6.8791303634643555 = 0.5000520944595337 + 1.0 * 6.379078388214111
Epoch 450, val loss: 0.8484476804733276
Epoch 460, training loss: 6.844676971435547 = 0.4774908423423767 + 1.0 * 6.367186069488525
Epoch 460, val loss: 0.8373329639434814
Epoch 470, training loss: 6.819732189178467 = 0.4556967318058014 + 1.0 * 6.364035606384277
Epoch 470, val loss: 0.8270969390869141
Epoch 480, training loss: 6.8003010749816895 = 0.4344913363456726 + 1.0 * 6.365809917449951
Epoch 480, val loss: 0.8177326917648315
Epoch 490, training loss: 6.7792582511901855 = 0.4140831232070923 + 1.0 * 6.365175247192383
Epoch 490, val loss: 0.8094193339347839
Epoch 500, training loss: 6.751375198364258 = 0.3944270610809326 + 1.0 * 6.356948375701904
Epoch 500, val loss: 0.8020720481872559
Epoch 510, training loss: 6.729933738708496 = 0.3752206861972809 + 1.0 * 6.354712963104248
Epoch 510, val loss: 0.7955894470214844
Epoch 520, training loss: 6.713066101074219 = 0.3564162254333496 + 1.0 * 6.356649875640869
Epoch 520, val loss: 0.7900010347366333
Epoch 530, training loss: 6.694390296936035 = 0.33820757269859314 + 1.0 * 6.35618257522583
Epoch 530, val loss: 0.7854411602020264
Epoch 540, training loss: 6.670739650726318 = 0.32055309414863586 + 1.0 * 6.350186347961426
Epoch 540, val loss: 0.7818535566329956
Epoch 550, training loss: 6.651665210723877 = 0.30333101749420166 + 1.0 * 6.348334312438965
Epoch 550, val loss: 0.7791385054588318
Epoch 560, training loss: 6.633771896362305 = 0.28665557503700256 + 1.0 * 6.347116470336914
Epoch 560, val loss: 0.7773241996765137
Epoch 570, training loss: 6.616497993469238 = 0.27065926790237427 + 1.0 * 6.34583854675293
Epoch 570, val loss: 0.776549756526947
Epoch 580, training loss: 6.608786582946777 = 0.25530049204826355 + 1.0 * 6.353486061096191
Epoch 580, val loss: 0.7767488360404968
Epoch 590, training loss: 6.583774566650391 = 0.2407790869474411 + 1.0 * 6.342995643615723
Epoch 590, val loss: 0.7779306769371033
Epoch 600, training loss: 6.568296432495117 = 0.22698432207107544 + 1.0 * 6.341311931610107
Epoch 600, val loss: 0.7801608443260193
Epoch 610, training loss: 6.554192543029785 = 0.21390406787395477 + 1.0 * 6.3402886390686035
Epoch 610, val loss: 0.7832303643226624
Epoch 620, training loss: 6.539999961853027 = 0.20162972807884216 + 1.0 * 6.338370323181152
Epoch 620, val loss: 0.7872197031974792
Epoch 630, training loss: 6.528890609741211 = 0.1901555359363556 + 1.0 * 6.338735103607178
Epoch 630, val loss: 0.7920242547988892
Epoch 640, training loss: 6.515028476715088 = 0.1793779730796814 + 1.0 * 6.335650444030762
Epoch 640, val loss: 0.7974830865859985
Epoch 650, training loss: 6.514566898345947 = 0.16928508877754211 + 1.0 * 6.345281600952148
Epoch 650, val loss: 0.8035684823989868
Epoch 660, training loss: 6.494019031524658 = 0.1599542200565338 + 1.0 * 6.334064960479736
Epoch 660, val loss: 0.8102900981903076
Epoch 670, training loss: 6.484237194061279 = 0.15123103559017181 + 1.0 * 6.333006381988525
Epoch 670, val loss: 0.8174389600753784
Epoch 680, training loss: 6.47634220123291 = 0.14309051632881165 + 1.0 * 6.333251476287842
Epoch 680, val loss: 0.8249529004096985
Epoch 690, training loss: 6.466174125671387 = 0.13552899658679962 + 1.0 * 6.3306450843811035
Epoch 690, val loss: 0.8328308463096619
Epoch 700, training loss: 6.4562907218933105 = 0.12846282124519348 + 1.0 * 6.3278279304504395
Epoch 700, val loss: 0.8409796357154846
Epoch 710, training loss: 6.44844388961792 = 0.12182975560426712 + 1.0 * 6.326613903045654
Epoch 710, val loss: 0.8494060039520264
Epoch 720, training loss: 6.45577335357666 = 0.11561265587806702 + 1.0 * 6.340160846710205
Epoch 720, val loss: 0.858014702796936
Epoch 730, training loss: 6.439228534698486 = 0.10986597836017609 + 1.0 * 6.329362392425537
Epoch 730, val loss: 0.8666850328445435
Epoch 740, training loss: 6.429232120513916 = 0.10450409352779388 + 1.0 * 6.324728012084961
Epoch 740, val loss: 0.8755091428756714
Epoch 750, training loss: 6.421698093414307 = 0.0994572788476944 + 1.0 * 6.322240829467773
Epoch 750, val loss: 0.8844521641731262
Epoch 760, training loss: 6.424668788909912 = 0.09472224861383438 + 1.0 * 6.329946517944336
Epoch 760, val loss: 0.8934968113899231
Epoch 770, training loss: 6.4184889793396 = 0.09028374403715134 + 1.0 * 6.328205108642578
Epoch 770, val loss: 0.9025006294250488
Epoch 780, training loss: 6.405009746551514 = 0.08615323901176453 + 1.0 * 6.318856716156006
Epoch 780, val loss: 0.911625862121582
Epoch 790, training loss: 6.400551795959473 = 0.08225459605455399 + 1.0 * 6.318297386169434
Epoch 790, val loss: 0.9207233786582947
Epoch 800, training loss: 6.397068977355957 = 0.07857539504766464 + 1.0 * 6.318493366241455
Epoch 800, val loss: 0.9298620223999023
Epoch 810, training loss: 6.398816108703613 = 0.07512340694665909 + 1.0 * 6.323692798614502
Epoch 810, val loss: 0.93894362449646
Epoch 820, training loss: 6.388841152191162 = 0.071879081428051 + 1.0 * 6.316962242126465
Epoch 820, val loss: 0.9479202628135681
Epoch 830, training loss: 6.384049892425537 = 0.06883682310581207 + 1.0 * 6.315213203430176
Epoch 830, val loss: 0.9568820595741272
Epoch 840, training loss: 6.378981590270996 = 0.06594684720039368 + 1.0 * 6.313034534454346
Epoch 840, val loss: 0.9657832384109497
Epoch 850, training loss: 6.386685371398926 = 0.06322251260280609 + 1.0 * 6.323462963104248
Epoch 850, val loss: 0.9747260212898254
Epoch 860, training loss: 6.376945495605469 = 0.06065559759736061 + 1.0 * 6.316289901733398
Epoch 860, val loss: 0.9833632707595825
Epoch 870, training loss: 6.3705644607543945 = 0.05824447050690651 + 1.0 * 6.312319755554199
Epoch 870, val loss: 0.9919894933700562
Epoch 880, training loss: 6.365325450897217 = 0.05594855919480324 + 1.0 * 6.3093767166137695
Epoch 880, val loss: 1.0005426406860352
Epoch 890, training loss: 6.369349002838135 = 0.05376855283975601 + 1.0 * 6.315580368041992
Epoch 890, val loss: 1.0090587139129639
Epoch 900, training loss: 6.368696689605713 = 0.051712047308683395 + 1.0 * 6.3169846534729
Epoch 900, val loss: 1.0175138711929321
Epoch 910, training loss: 6.35684871673584 = 0.0497727170586586 + 1.0 * 6.3070759773254395
Epoch 910, val loss: 1.0257869958877563
Epoch 920, training loss: 6.354728698730469 = 0.04792516678571701 + 1.0 * 6.3068037033081055
Epoch 920, val loss: 1.0339630842208862
Epoch 930, training loss: 6.351156234741211 = 0.046162985265254974 + 1.0 * 6.304993152618408
Epoch 930, val loss: 1.0421632528305054
Epoch 940, training loss: 6.354456424713135 = 0.044486161321401596 + 1.0 * 6.309970378875732
Epoch 940, val loss: 1.0502289533615112
Epoch 950, training loss: 6.349996566772461 = 0.0428922213613987 + 1.0 * 6.307104110717773
Epoch 950, val loss: 1.058241605758667
Epoch 960, training loss: 6.3508219718933105 = 0.04138891026377678 + 1.0 * 6.3094329833984375
Epoch 960, val loss: 1.0659360885620117
Epoch 970, training loss: 6.342955112457275 = 0.0399605855345726 + 1.0 * 6.302994728088379
Epoch 970, val loss: 1.0736764669418335
Epoch 980, training loss: 6.340364933013916 = 0.03859161585569382 + 1.0 * 6.301773548126221
Epoch 980, val loss: 1.081368327140808
Epoch 990, training loss: 6.34192419052124 = 0.03728427737951279 + 1.0 * 6.30463981628418
Epoch 990, val loss: 1.088966727256775
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 10.567697525024414 = 1.970883846282959 + 1.0 * 8.596813201904297
Epoch 0, val loss: 1.971650242805481
Epoch 10, training loss: 10.557215690612793 = 1.9606561660766602 + 1.0 * 8.596559524536133
Epoch 10, val loss: 1.9611059427261353
Epoch 20, training loss: 10.542892456054688 = 1.9484405517578125 + 1.0 * 8.594451904296875
Epoch 20, val loss: 1.9485841989517212
Epoch 30, training loss: 10.508281707763672 = 1.931746482849121 + 1.0 * 8.57653522491455
Epoch 30, val loss: 1.9315911531448364
Epoch 40, training loss: 10.369633674621582 = 1.9103243350982666 + 1.0 * 8.459309577941895
Epoch 40, val loss: 1.9104851484298706
Epoch 50, training loss: 9.978826522827148 = 1.8877062797546387 + 1.0 * 8.091119766235352
Epoch 50, val loss: 1.8888975381851196
Epoch 60, training loss: 9.554980278015137 = 1.865169882774353 + 1.0 * 7.689810752868652
Epoch 60, val loss: 1.867928385734558
Epoch 70, training loss: 9.100521087646484 = 1.846635341644287 + 1.0 * 7.253885269165039
Epoch 70, val loss: 1.8501849174499512
Epoch 80, training loss: 8.899344444274902 = 1.8294051885604858 + 1.0 * 7.069939613342285
Epoch 80, val loss: 1.8337677717208862
Epoch 90, training loss: 8.719110488891602 = 1.8101441860198975 + 1.0 * 6.908966064453125
Epoch 90, val loss: 1.8169915676116943
Epoch 100, training loss: 8.599308013916016 = 1.7907689809799194 + 1.0 * 6.808539390563965
Epoch 100, val loss: 1.8001950979232788
Epoch 110, training loss: 8.510880470275879 = 1.7709628343582153 + 1.0 * 6.739917755126953
Epoch 110, val loss: 1.7822176218032837
Epoch 120, training loss: 8.437907218933105 = 1.7506248950958252 + 1.0 * 6.687282085418701
Epoch 120, val loss: 1.7633247375488281
Epoch 130, training loss: 8.377978324890137 = 1.7293825149536133 + 1.0 * 6.648595809936523
Epoch 130, val loss: 1.7437800168991089
Epoch 140, training loss: 8.321529388427734 = 1.7067607641220093 + 1.0 * 6.6147685050964355
Epoch 140, val loss: 1.7231616973876953
Epoch 150, training loss: 8.265582084655762 = 1.6820167303085327 + 1.0 * 6.583565711975098
Epoch 150, val loss: 1.7008461952209473
Epoch 160, training loss: 8.211962699890137 = 1.654065728187561 + 1.0 * 6.557897090911865
Epoch 160, val loss: 1.6758583784103394
Epoch 170, training loss: 8.158342361450195 = 1.622349739074707 + 1.0 * 6.535992622375488
Epoch 170, val loss: 1.647827386856079
Epoch 180, training loss: 8.105428695678711 = 1.5863897800445557 + 1.0 * 6.519039154052734
Epoch 180, val loss: 1.6163122653961182
Epoch 190, training loss: 8.049464225769043 = 1.545636534690857 + 1.0 * 6.5038275718688965
Epoch 190, val loss: 1.5806853771209717
Epoch 200, training loss: 7.993369102478027 = 1.5000966787338257 + 1.0 * 6.493272304534912
Epoch 200, val loss: 1.541220784187317
Epoch 210, training loss: 7.932862281799316 = 1.4505360126495361 + 1.0 * 6.482326507568359
Epoch 210, val loss: 1.498417854309082
Epoch 220, training loss: 7.869834899902344 = 1.3972289562225342 + 1.0 * 6.472606182098389
Epoch 220, val loss: 1.4526901245117188
Epoch 230, training loss: 7.811470031738281 = 1.341099739074707 + 1.0 * 6.470370292663574
Epoch 230, val loss: 1.4050638675689697
Epoch 240, training loss: 7.741555213928223 = 1.2841397523880005 + 1.0 * 6.457415580749512
Epoch 240, val loss: 1.3571577072143555
Epoch 250, training loss: 7.677163600921631 = 1.2269047498703003 + 1.0 * 6.450258731842041
Epoch 250, val loss: 1.3093562126159668
Epoch 260, training loss: 7.61751127243042 = 1.1706210374832153 + 1.0 * 6.446890354156494
Epoch 260, val loss: 1.2630162239074707
Epoch 270, training loss: 7.554485321044922 = 1.1165640354156494 + 1.0 * 6.437921047210693
Epoch 270, val loss: 1.2188071012496948
Epoch 280, training loss: 7.4959845542907715 = 1.0646166801452637 + 1.0 * 6.431367874145508
Epoch 280, val loss: 1.176891565322876
Epoch 290, training loss: 7.445347785949707 = 1.0148789882659912 + 1.0 * 6.430469036102295
Epoch 290, val loss: 1.1371606588363647
Epoch 300, training loss: 7.3933186531066895 = 0.9681618809700012 + 1.0 * 6.425156593322754
Epoch 300, val loss: 1.1001510620117188
Epoch 310, training loss: 7.341275215148926 = 0.9244924783706665 + 1.0 * 6.416782855987549
Epoch 310, val loss: 1.065877079963684
Epoch 320, training loss: 7.294532775878906 = 0.8832902312278748 + 1.0 * 6.411242485046387
Epoch 320, val loss: 1.0337891578674316
Epoch 330, training loss: 7.259974002838135 = 0.8443812131881714 + 1.0 * 6.415592670440674
Epoch 330, val loss: 1.0036977529525757
Epoch 340, training loss: 7.2157769203186035 = 0.8083702325820923 + 1.0 * 6.407406806945801
Epoch 340, val loss: 0.9761577248573303
Epoch 350, training loss: 7.1729631423950195 = 0.7749263048171997 + 1.0 * 6.398036956787109
Epoch 350, val loss: 0.9511111974716187
Epoch 360, training loss: 7.137799263000488 = 0.7434729337692261 + 1.0 * 6.394326210021973
Epoch 360, val loss: 0.9280866980552673
Epoch 370, training loss: 7.105867385864258 = 0.7137728333473206 + 1.0 * 6.392094612121582
Epoch 370, val loss: 0.9067837595939636
Epoch 380, training loss: 7.076813697814941 = 0.6858788132667542 + 1.0 * 6.390934944152832
Epoch 380, val loss: 0.8871365189552307
Epoch 390, training loss: 7.042540073394775 = 0.6593940854072571 + 1.0 * 6.383145809173584
Epoch 390, val loss: 0.8691267371177673
Epoch 400, training loss: 7.014731407165527 = 0.6337680816650391 + 1.0 * 6.380963325500488
Epoch 400, val loss: 0.8520760536193848
Epoch 410, training loss: 6.988754749298096 = 0.6087054014205933 + 1.0 * 6.380049228668213
Epoch 410, val loss: 0.8355647921562195
Epoch 420, training loss: 6.9599480628967285 = 0.5840546488761902 + 1.0 * 6.375893592834473
Epoch 420, val loss: 0.8196473121643066
Epoch 430, training loss: 6.93131685256958 = 0.5595189332962036 + 1.0 * 6.371798038482666
Epoch 430, val loss: 0.8041321635246277
Epoch 440, training loss: 6.90460729598999 = 0.5349186062812805 + 1.0 * 6.369688510894775
Epoch 440, val loss: 0.7887929081916809
Epoch 450, training loss: 6.880430698394775 = 0.5103972554206848 + 1.0 * 6.370033264160156
Epoch 450, val loss: 0.7738317251205444
Epoch 460, training loss: 6.853503227233887 = 0.48618072271347046 + 1.0 * 6.3673224449157715
Epoch 460, val loss: 0.7595598697662354
Epoch 470, training loss: 6.824528694152832 = 0.46226903796195984 + 1.0 * 6.362259864807129
Epoch 470, val loss: 0.7459538578987122
Epoch 480, training loss: 6.798927307128906 = 0.4387063682079315 + 1.0 * 6.360220909118652
Epoch 480, val loss: 0.7331828474998474
Epoch 490, training loss: 6.7754998207092285 = 0.415688157081604 + 1.0 * 6.359811782836914
Epoch 490, val loss: 0.7214387655258179
Epoch 500, training loss: 6.7523345947265625 = 0.3934729993343353 + 1.0 * 6.358861446380615
Epoch 500, val loss: 0.7110633850097656
Epoch 510, training loss: 6.727744102478027 = 0.37224873900413513 + 1.0 * 6.355495452880859
Epoch 510, val loss: 0.7019425630569458
Epoch 520, training loss: 6.709912300109863 = 0.3519960641860962 + 1.0 * 6.357916355133057
Epoch 520, val loss: 0.6940987706184387
Epoch 530, training loss: 6.685450553894043 = 0.33284926414489746 + 1.0 * 6.352601528167725
Epoch 530, val loss: 0.6874789595603943
Epoch 540, training loss: 6.6631951332092285 = 0.3147425949573517 + 1.0 * 6.348452568054199
Epoch 540, val loss: 0.6821136474609375
Epoch 550, training loss: 6.650034427642822 = 0.29761630296707153 + 1.0 * 6.352417945861816
Epoch 550, val loss: 0.677812933921814
Epoch 560, training loss: 6.632640838623047 = 0.2815176546573639 + 1.0 * 6.351123332977295
Epoch 560, val loss: 0.6745318174362183
Epoch 570, training loss: 6.613916397094727 = 0.2664361894130707 + 1.0 * 6.347480297088623
Epoch 570, val loss: 0.6721857190132141
Epoch 580, training loss: 6.59599494934082 = 0.25220686197280884 + 1.0 * 6.343788146972656
Epoch 580, val loss: 0.6706265211105347
Epoch 590, training loss: 6.580638408660889 = 0.2387578785419464 + 1.0 * 6.3418803215026855
Epoch 590, val loss: 0.6697669625282288
Epoch 600, training loss: 6.572023868560791 = 0.22602955996990204 + 1.0 * 6.345994472503662
Epoch 600, val loss: 0.6695523858070374
Epoch 610, training loss: 6.553743362426758 = 0.21407979726791382 + 1.0 * 6.339663505554199
Epoch 610, val loss: 0.6698433756828308
Epoch 620, training loss: 6.540750026702881 = 0.2027844786643982 + 1.0 * 6.337965488433838
Epoch 620, val loss: 0.6706635355949402
Epoch 630, training loss: 6.527853488922119 = 0.192088782787323 + 1.0 * 6.3357648849487305
Epoch 630, val loss: 0.6719304919242859
Epoch 640, training loss: 6.520352840423584 = 0.18195602297782898 + 1.0 * 6.338397026062012
Epoch 640, val loss: 0.673680305480957
Epoch 650, training loss: 6.5096282958984375 = 0.17240272462368011 + 1.0 * 6.337225437164307
Epoch 650, val loss: 0.675701379776001
Epoch 660, training loss: 6.495238304138184 = 0.16339005529880524 + 1.0 * 6.33184814453125
Epoch 660, val loss: 0.6781100630760193
Epoch 670, training loss: 6.485198020935059 = 0.15489041805267334 + 1.0 * 6.330307483673096
Epoch 670, val loss: 0.6808168888092041
Epoch 680, training loss: 6.475366115570068 = 0.14684124290943146 + 1.0 * 6.328525066375732
Epoch 680, val loss: 0.6838198304176331
Epoch 690, training loss: 6.474360942840576 = 0.13921983540058136 + 1.0 * 6.335141181945801
Epoch 690, val loss: 0.687066376209259
Epoch 700, training loss: 6.458442687988281 = 0.13206225633621216 + 1.0 * 6.326380252838135
Epoch 700, val loss: 0.690520703792572
Epoch 710, training loss: 6.449989318847656 = 0.1253003180027008 + 1.0 * 6.324688911437988
Epoch 710, val loss: 0.6941962838172913
Epoch 720, training loss: 6.445103645324707 = 0.11890299618244171 + 1.0 * 6.326200485229492
Epoch 720, val loss: 0.6980831623077393
Epoch 730, training loss: 6.437004089355469 = 0.11287350952625275 + 1.0 * 6.324130535125732
Epoch 730, val loss: 0.7021655440330505
Epoch 740, training loss: 6.43350887298584 = 0.10721588134765625 + 1.0 * 6.326292991638184
Epoch 740, val loss: 0.7063364386558533
Epoch 750, training loss: 6.421361446380615 = 0.101896733045578 + 1.0 * 6.319464683532715
Epoch 750, val loss: 0.7106691002845764
Epoch 760, training loss: 6.415319442749023 = 0.09688767790794373 + 1.0 * 6.318431854248047
Epoch 760, val loss: 0.7151679992675781
Epoch 770, training loss: 6.409391403198242 = 0.09215469658374786 + 1.0 * 6.31723690032959
Epoch 770, val loss: 0.7197669148445129
Epoch 780, training loss: 6.411023139953613 = 0.08768513798713684 + 1.0 * 6.323338031768799
Epoch 780, val loss: 0.7245025634765625
Epoch 790, training loss: 6.407871246337891 = 0.08350303769111633 + 1.0 * 6.324368000030518
Epoch 790, val loss: 0.7292185425758362
Epoch 800, training loss: 6.3944549560546875 = 0.07955548167228699 + 1.0 * 6.314899444580078
Epoch 800, val loss: 0.7340468168258667
Epoch 810, training loss: 6.389153957366943 = 0.07584637403488159 + 1.0 * 6.313307762145996
Epoch 810, val loss: 0.7389708757400513
Epoch 820, training loss: 6.384980201721191 = 0.07235180586576462 + 1.0 * 6.312628269195557
Epoch 820, val loss: 0.7440037727355957
Epoch 830, training loss: 6.3882646560668945 = 0.06905736029148102 + 1.0 * 6.319207191467285
Epoch 830, val loss: 0.7489656805992126
Epoch 840, training loss: 6.377621650695801 = 0.06596405059099197 + 1.0 * 6.311657428741455
Epoch 840, val loss: 0.754012405872345
Epoch 850, training loss: 6.373317241668701 = 0.06305436044931412 + 1.0 * 6.310262680053711
Epoch 850, val loss: 0.7590208649635315
Epoch 860, training loss: 6.372791290283203 = 0.060314472764730453 + 1.0 * 6.312476634979248
Epoch 860, val loss: 0.7641161680221558
Epoch 870, training loss: 6.365440845489502 = 0.057727329432964325 + 1.0 * 6.307713508605957
Epoch 870, val loss: 0.7692183256149292
Epoch 880, training loss: 6.363743782043457 = 0.05528648570179939 + 1.0 * 6.308457374572754
Epoch 880, val loss: 0.7742601037025452
Epoch 890, training loss: 6.358932018280029 = 0.05298386886715889 + 1.0 * 6.305948257446289
Epoch 890, val loss: 0.77936190366745
Epoch 900, training loss: 6.358409404754639 = 0.05080490559339523 + 1.0 * 6.3076043128967285
Epoch 900, val loss: 0.7844664454460144
Epoch 910, training loss: 6.352925777435303 = 0.04874739423394203 + 1.0 * 6.304178237915039
Epoch 910, val loss: 0.7894935011863708
Epoch 920, training loss: 6.350831031799316 = 0.04680190235376358 + 1.0 * 6.3040289878845215
Epoch 920, val loss: 0.7945923209190369
Epoch 930, training loss: 6.351083755493164 = 0.044959720224142075 + 1.0 * 6.306124210357666
Epoch 930, val loss: 0.7995721697807312
Epoch 940, training loss: 6.346901893615723 = 0.04321730136871338 + 1.0 * 6.303684711456299
Epoch 940, val loss: 0.8045692443847656
Epoch 950, training loss: 6.341929912567139 = 0.04157433658838272 + 1.0 * 6.300355434417725
Epoch 950, val loss: 0.8094775676727295
Epoch 960, training loss: 6.3384904861450195 = 0.04001404717564583 + 1.0 * 6.298476219177246
Epoch 960, val loss: 0.8144192099571228
Epoch 970, training loss: 6.338743209838867 = 0.03853322193026543 + 1.0 * 6.300209999084473
Epoch 970, val loss: 0.8193315267562866
Epoch 980, training loss: 6.336251735687256 = 0.037124548107385635 + 1.0 * 6.299127101898193
Epoch 980, val loss: 0.8240902423858643
Epoch 990, training loss: 6.333403587341309 = 0.035790760070085526 + 1.0 * 6.29761266708374
Epoch 990, val loss: 0.8288583159446716
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8344754876120191
The final CL Acc:0.81852, 0.01090, The final GNN Acc:0.83781, 0.00237
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11670])
remove edge: torch.Size([2, 9546])
updated graph: torch.Size([2, 10660])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.542498588562012 = 1.9456723928451538 + 1.0 * 8.596826553344727
Epoch 0, val loss: 1.9356377124786377
Epoch 10, training loss: 10.531952857971191 = 1.9353818893432617 + 1.0 * 8.59657096862793
Epoch 10, val loss: 1.9260367155075073
Epoch 20, training loss: 10.517491340637207 = 1.9228887557983398 + 1.0 * 8.594602584838867
Epoch 20, val loss: 1.913834810256958
Epoch 30, training loss: 10.48526668548584 = 1.9056522846221924 + 1.0 * 8.579614639282227
Epoch 30, val loss: 1.8968185186386108
Epoch 40, training loss: 10.376811981201172 = 1.8826439380645752 + 1.0 * 8.494168281555176
Epoch 40, val loss: 1.874881386756897
Epoch 50, training loss: 10.007619857788086 = 1.8565951585769653 + 1.0 * 8.15102481842041
Epoch 50, val loss: 1.8506965637207031
Epoch 60, training loss: 9.641559600830078 = 1.8341728448867798 + 1.0 * 7.80738639831543
Epoch 60, val loss: 1.8301823139190674
Epoch 70, training loss: 9.165037155151367 = 1.8214794397354126 + 1.0 * 7.343557834625244
Epoch 70, val loss: 1.8186181783676147
Epoch 80, training loss: 8.929993629455566 = 1.8115451335906982 + 1.0 * 7.118448257446289
Epoch 80, val loss: 1.8096208572387695
Epoch 90, training loss: 8.757182121276855 = 1.7983514070510864 + 1.0 * 6.958830833435059
Epoch 90, val loss: 1.7986340522766113
Epoch 100, training loss: 8.626575469970703 = 1.7860493659973145 + 1.0 * 6.840526103973389
Epoch 100, val loss: 1.788789987564087
Epoch 110, training loss: 8.533203125 = 1.7752392292022705 + 1.0 * 6.75796365737915
Epoch 110, val loss: 1.7788772583007812
Epoch 120, training loss: 8.468839645385742 = 1.7632609605789185 + 1.0 * 6.705578327178955
Epoch 120, val loss: 1.7675014734268188
Epoch 130, training loss: 8.41496467590332 = 1.7495086193084717 + 1.0 * 6.6654558181762695
Epoch 130, val loss: 1.7549854516983032
Epoch 140, training loss: 8.36469841003418 = 1.7342300415039062 + 1.0 * 6.630468368530273
Epoch 140, val loss: 1.7414495944976807
Epoch 150, training loss: 8.317078590393066 = 1.7169891595840454 + 1.0 * 6.6000895500183105
Epoch 150, val loss: 1.72642183303833
Epoch 160, training loss: 8.273027420043945 = 1.6970081329345703 + 1.0 * 6.576019763946533
Epoch 160, val loss: 1.7090827226638794
Epoch 170, training loss: 8.227778434753418 = 1.6735035181045532 + 1.0 * 6.554275035858154
Epoch 170, val loss: 1.6886346340179443
Epoch 180, training loss: 8.186467170715332 = 1.6455405950546265 + 1.0 * 6.540926933288574
Epoch 180, val loss: 1.6643339395523071
Epoch 190, training loss: 8.136127471923828 = 1.6129835844039917 + 1.0 * 6.523143768310547
Epoch 190, val loss: 1.6360880136489868
Epoch 200, training loss: 8.084568977355957 = 1.5752074718475342 + 1.0 * 6.509361743927002
Epoch 200, val loss: 1.6031408309936523
Epoch 210, training loss: 8.032313346862793 = 1.5316755771636963 + 1.0 * 6.500637531280518
Epoch 210, val loss: 1.5652738809585571
Epoch 220, training loss: 7.97285270690918 = 1.4841855764389038 + 1.0 * 6.488667011260986
Epoch 220, val loss: 1.524535059928894
Epoch 230, training loss: 7.911247730255127 = 1.4333020448684692 + 1.0 * 6.477945804595947
Epoch 230, val loss: 1.4813979864120483
Epoch 240, training loss: 7.8487229347229 = 1.3798617124557495 + 1.0 * 6.468861103057861
Epoch 240, val loss: 1.436803936958313
Epoch 250, training loss: 7.7888641357421875 = 1.3254703283309937 + 1.0 * 6.463393688201904
Epoch 250, val loss: 1.3928978443145752
Epoch 260, training loss: 7.728391170501709 = 1.2725991010665894 + 1.0 * 6.45579195022583
Epoch 260, val loss: 1.3512780666351318
Epoch 270, training loss: 7.668139934539795 = 1.2203549146652222 + 1.0 * 6.447784900665283
Epoch 270, val loss: 1.3113481998443604
Epoch 280, training loss: 7.61242151260376 = 1.1684328317642212 + 1.0 * 6.443988800048828
Epoch 280, val loss: 1.2725869417190552
Epoch 290, training loss: 7.554689884185791 = 1.1171380281448364 + 1.0 * 6.437551975250244
Epoch 290, val loss: 1.2350527048110962
Epoch 300, training loss: 7.499538421630859 = 1.0657923221588135 + 1.0 * 6.433746337890625
Epoch 300, val loss: 1.1982237100601196
Epoch 310, training loss: 7.4415154457092285 = 1.0148729085922241 + 1.0 * 6.426642417907715
Epoch 310, val loss: 1.162053108215332
Epoch 320, training loss: 7.388442039489746 = 0.9640792608261108 + 1.0 * 6.424362659454346
Epoch 320, val loss: 1.1266111135482788
Epoch 330, training loss: 7.334709644317627 = 0.914964497089386 + 1.0 * 6.419744968414307
Epoch 330, val loss: 1.0928215980529785
Epoch 340, training loss: 7.282204627990723 = 0.8676653504371643 + 1.0 * 6.414539337158203
Epoch 340, val loss: 1.061113953590393
Epoch 350, training loss: 7.239864349365234 = 0.8224260807037354 + 1.0 * 6.417438507080078
Epoch 350, val loss: 1.03152334690094
Epoch 360, training loss: 7.187819957733154 = 0.7803028225898743 + 1.0 * 6.407516956329346
Epoch 360, val loss: 1.0048612356185913
Epoch 370, training loss: 7.143560886383057 = 0.7407059669494629 + 1.0 * 6.402854919433594
Epoch 370, val loss: 0.9807243943214417
Epoch 380, training loss: 7.10795259475708 = 0.7034540176391602 + 1.0 * 6.40449857711792
Epoch 380, val loss: 0.9591471552848816
Epoch 390, training loss: 7.067075252532959 = 0.6688148379325867 + 1.0 * 6.398260593414307
Epoch 390, val loss: 0.9401291608810425
Epoch 400, training loss: 7.03026008605957 = 0.6365585923194885 + 1.0 * 6.393701553344727
Epoch 400, val loss: 0.9235982894897461
Epoch 410, training loss: 6.997998237609863 = 0.6063878536224365 + 1.0 * 6.391610622406006
Epoch 410, val loss: 0.9092270731925964
Epoch 420, training loss: 6.965005874633789 = 0.5777003765106201 + 1.0 * 6.387305736541748
Epoch 420, val loss: 0.8967252969741821
Epoch 430, training loss: 6.935781955718994 = 0.5502925515174866 + 1.0 * 6.385489463806152
Epoch 430, val loss: 0.8858935832977295
Epoch 440, training loss: 6.914623260498047 = 0.5241963863372803 + 1.0 * 6.390427112579346
Epoch 440, val loss: 0.8767437934875488
Epoch 450, training loss: 6.880863189697266 = 0.49948611855506897 + 1.0 * 6.381377220153809
Epoch 450, val loss: 0.869210422039032
Epoch 460, training loss: 6.854149341583252 = 0.4757353663444519 + 1.0 * 6.378414154052734
Epoch 460, val loss: 0.8627910017967224
Epoch 470, training loss: 6.8325724601745605 = 0.4529622197151184 + 1.0 * 6.379610061645508
Epoch 470, val loss: 0.8576036691665649
Epoch 480, training loss: 6.80375337600708 = 0.4312497675418854 + 1.0 * 6.372503757476807
Epoch 480, val loss: 0.8536017537117004
Epoch 490, training loss: 6.785105228424072 = 0.4102884531021118 + 1.0 * 6.37481689453125
Epoch 490, val loss: 0.8503391146659851
Epoch 500, training loss: 6.764559745788574 = 0.3902168273925781 + 1.0 * 6.374342918395996
Epoch 500, val loss: 0.8480526208877563
Epoch 510, training loss: 6.739433765411377 = 0.37093794345855713 + 1.0 * 6.368495941162109
Epoch 510, val loss: 0.8466663360595703
Epoch 520, training loss: 6.716505527496338 = 0.3523351848125458 + 1.0 * 6.364170551300049
Epoch 520, val loss: 0.8458243012428284
Epoch 530, training loss: 6.713130474090576 = 0.33444517850875854 + 1.0 * 6.378685474395752
Epoch 530, val loss: 0.8457712531089783
Epoch 540, training loss: 6.681214332580566 = 0.31744855642318726 + 1.0 * 6.363765716552734
Epoch 540, val loss: 0.846415638923645
Epoch 550, training loss: 6.6618733406066895 = 0.30126646161079407 + 1.0 * 6.360606670379639
Epoch 550, val loss: 0.8477713465690613
Epoch 560, training loss: 6.653367042541504 = 0.28581807017326355 + 1.0 * 6.367548942565918
Epoch 560, val loss: 0.8497036695480347
Epoch 570, training loss: 6.632042407989502 = 0.2713412642478943 + 1.0 * 6.360701084136963
Epoch 570, val loss: 0.8524213433265686
Epoch 580, training loss: 6.612612247467041 = 0.25769534707069397 + 1.0 * 6.354917049407959
Epoch 580, val loss: 0.8556763529777527
Epoch 590, training loss: 6.596972942352295 = 0.2447756826877594 + 1.0 * 6.352197170257568
Epoch 590, val loss: 0.8593299984931946
Epoch 600, training loss: 6.588466644287109 = 0.23255574703216553 + 1.0 * 6.355910778045654
Epoch 600, val loss: 0.8636634945869446
Epoch 610, training loss: 6.573542594909668 = 0.22109724581241608 + 1.0 * 6.352445125579834
Epoch 610, val loss: 0.8684788346290588
Epoch 620, training loss: 6.559488773345947 = 0.21030251681804657 + 1.0 * 6.349186420440674
Epoch 620, val loss: 0.8738083243370056
Epoch 630, training loss: 6.5497517585754395 = 0.200125053524971 + 1.0 * 6.349626541137695
Epoch 630, val loss: 0.8794244527816772
Epoch 640, training loss: 6.543587684631348 = 0.19059960544109344 + 1.0 * 6.352988243103027
Epoch 640, val loss: 0.8855238556861877
Epoch 650, training loss: 6.528434753417969 = 0.1817004531621933 + 1.0 * 6.346734523773193
Epoch 650, val loss: 0.8919646739959717
Epoch 660, training loss: 6.515791893005371 = 0.17330195009708405 + 1.0 * 6.342489719390869
Epoch 660, val loss: 0.8984798789024353
Epoch 670, training loss: 6.510360240936279 = 0.1653626710176468 + 1.0 * 6.344997406005859
Epoch 670, val loss: 0.9053469896316528
Epoch 680, training loss: 6.501513481140137 = 0.15793219208717346 + 1.0 * 6.343581199645996
Epoch 680, val loss: 0.9125083684921265
Epoch 690, training loss: 6.492692947387695 = 0.15094111859798431 + 1.0 * 6.341752052307129
Epoch 690, val loss: 0.9197437167167664
Epoch 700, training loss: 6.481867790222168 = 0.14436262845993042 + 1.0 * 6.337505340576172
Epoch 700, val loss: 0.9270586967468262
Epoch 710, training loss: 6.477103233337402 = 0.1381303071975708 + 1.0 * 6.338973045349121
Epoch 710, val loss: 0.9345430135726929
Epoch 720, training loss: 6.466236114501953 = 0.13222096860408783 + 1.0 * 6.334015369415283
Epoch 720, val loss: 0.9421960115432739
Epoch 730, training loss: 6.460853576660156 = 0.12662333250045776 + 1.0 * 6.334230422973633
Epoch 730, val loss: 0.9498606324195862
Epoch 740, training loss: 6.461061954498291 = 0.121327243745327 + 1.0 * 6.3397345542907715
Epoch 740, val loss: 0.9574437141418457
Epoch 750, training loss: 6.448972225189209 = 0.1163439080119133 + 1.0 * 6.33262825012207
Epoch 750, val loss: 0.9651963710784912
Epoch 760, training loss: 6.443713665008545 = 0.11162491142749786 + 1.0 * 6.332088947296143
Epoch 760, val loss: 0.9727268815040588
Epoch 770, training loss: 6.4409589767456055 = 0.10714603960514069 + 1.0 * 6.333812713623047
Epoch 770, val loss: 0.9803146719932556
Epoch 780, training loss: 6.432131290435791 = 0.10288158804178238 + 1.0 * 6.329249858856201
Epoch 780, val loss: 0.9880021810531616
Epoch 790, training loss: 6.424835205078125 = 0.09882360696792603 + 1.0 * 6.326011657714844
Epoch 790, val loss: 0.9955626130104065
Epoch 800, training loss: 6.432567596435547 = 0.0949549749493599 + 1.0 * 6.337612628936768
Epoch 800, val loss: 1.0031177997589111
Epoch 810, training loss: 6.415031433105469 = 0.09127670526504517 + 1.0 * 6.323754787445068
Epoch 810, val loss: 1.0106247663497925
Epoch 820, training loss: 6.4113664627075195 = 0.0877692773938179 + 1.0 * 6.323596954345703
Epoch 820, val loss: 1.0180760622024536
Epoch 830, training loss: 6.406721115112305 = 0.08441252261400223 + 1.0 * 6.322308540344238
Epoch 830, val loss: 1.0254566669464111
Epoch 840, training loss: 6.4211530685424805 = 0.0811895877122879 + 1.0 * 6.339963436126709
Epoch 840, val loss: 1.0328341722488403
Epoch 850, training loss: 6.3984694480896 = 0.07814286649227142 + 1.0 * 6.320326805114746
Epoch 850, val loss: 1.0401185750961304
Epoch 860, training loss: 6.39491605758667 = 0.0752323567867279 + 1.0 * 6.31968355178833
Epoch 860, val loss: 1.0472681522369385
Epoch 870, training loss: 6.3904314041137695 = 0.07244080305099487 + 1.0 * 6.317990779876709
Epoch 870, val loss: 1.054339051246643
Epoch 880, training loss: 6.387613296508789 = 0.06976292282342911 + 1.0 * 6.317850589752197
Epoch 880, val loss: 1.0615315437316895
Epoch 890, training loss: 6.388807773590088 = 0.0671989768743515 + 1.0 * 6.321609020233154
Epoch 890, val loss: 1.0685662031173706
Epoch 900, training loss: 6.381360054016113 = 0.0647619217634201 + 1.0 * 6.316597938537598
Epoch 900, val loss: 1.075637698173523
Epoch 910, training loss: 6.378526210784912 = 0.062431905418634415 + 1.0 * 6.316094398498535
Epoch 910, val loss: 1.0825133323669434
Epoch 920, training loss: 6.380593776702881 = 0.06020045652985573 + 1.0 * 6.320393085479736
Epoch 920, val loss: 1.0892999172210693
Epoch 930, training loss: 6.372226715087891 = 0.058059245347976685 + 1.0 * 6.314167499542236
Epoch 930, val loss: 1.096056342124939
Epoch 940, training loss: 6.367797374725342 = 0.05601576715707779 + 1.0 * 6.311781406402588
Epoch 940, val loss: 1.1028295755386353
Epoch 950, training loss: 6.369132041931152 = 0.05405545234680176 + 1.0 * 6.3150763511657715
Epoch 950, val loss: 1.1094858646392822
Epoch 960, training loss: 6.368821144104004 = 0.052177153527736664 + 1.0 * 6.316644191741943
Epoch 960, val loss: 1.1159765720367432
Epoch 970, training loss: 6.362541675567627 = 0.05039086565375328 + 1.0 * 6.312150955200195
Epoch 970, val loss: 1.1226003170013428
Epoch 980, training loss: 6.361390113830566 = 0.0486781969666481 + 1.0 * 6.312711715698242
Epoch 980, val loss: 1.1290138959884644
Epoch 990, training loss: 6.3565850257873535 = 0.047038737684488297 + 1.0 * 6.30954647064209
Epoch 990, val loss: 1.1352407932281494
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 10.531208038330078 = 1.934380054473877 + 1.0 * 8.59682846069336
Epoch 0, val loss: 1.9373193979263306
Epoch 10, training loss: 10.52150821685791 = 1.9249662160873413 + 1.0 * 8.596542358398438
Epoch 10, val loss: 1.9269769191741943
Epoch 20, training loss: 10.507699966430664 = 1.9133411645889282 + 1.0 * 8.594358444213867
Epoch 20, val loss: 1.913971185684204
Epoch 30, training loss: 10.473991394042969 = 1.8971502780914307 + 1.0 * 8.576841354370117
Epoch 30, val loss: 1.8957958221435547
Epoch 40, training loss: 10.332870483398438 = 1.876298427581787 + 1.0 * 8.456571578979492
Epoch 40, val loss: 1.8736001253128052
Epoch 50, training loss: 9.913045883178711 = 1.8545341491699219 + 1.0 * 8.058511734008789
Epoch 50, val loss: 1.851840615272522
Epoch 60, training loss: 9.371505737304688 = 1.8368662595748901 + 1.0 * 7.534639835357666
Epoch 60, val loss: 1.836096167564392
Epoch 70, training loss: 8.880525588989258 = 1.8239928483963013 + 1.0 * 7.056532859802246
Epoch 70, val loss: 1.8239870071411133
Epoch 80, training loss: 8.713213920593262 = 1.8112596273422241 + 1.0 * 6.901954174041748
Epoch 80, val loss: 1.8112266063690186
Epoch 90, training loss: 8.606650352478027 = 1.7956361770629883 + 1.0 * 6.811014175415039
Epoch 90, val loss: 1.795716643333435
Epoch 100, training loss: 8.528907775878906 = 1.7800854444503784 + 1.0 * 6.7488226890563965
Epoch 100, val loss: 1.7809182405471802
Epoch 110, training loss: 8.464620590209961 = 1.7652747631072998 + 1.0 * 6.699345588684082
Epoch 110, val loss: 1.7669260501861572
Epoch 120, training loss: 8.411705017089844 = 1.7501379251480103 + 1.0 * 6.661566734313965
Epoch 120, val loss: 1.7529525756835938
Epoch 130, training loss: 8.361992835998535 = 1.7336182594299316 + 1.0 * 6.6283745765686035
Epoch 130, val loss: 1.7381024360656738
Epoch 140, training loss: 8.315695762634277 = 1.7148113250732422 + 1.0 * 6.600884437561035
Epoch 140, val loss: 1.721610188484192
Epoch 150, training loss: 8.27035903930664 = 1.692876935005188 + 1.0 * 6.577481746673584
Epoch 150, val loss: 1.7025701999664307
Epoch 160, training loss: 8.22724723815918 = 1.6671032905578613 + 1.0 * 6.56014347076416
Epoch 160, val loss: 1.6804132461547852
Epoch 170, training loss: 8.178869247436523 = 1.6370518207550049 + 1.0 * 6.541817665100098
Epoch 170, val loss: 1.6546180248260498
Epoch 180, training loss: 8.125998497009277 = 1.6021496057510376 + 1.0 * 6.523849010467529
Epoch 180, val loss: 1.6246930360794067
Epoch 190, training loss: 8.071802139282227 = 1.5615382194519043 + 1.0 * 6.5102643966674805
Epoch 190, val loss: 1.5899955034255981
Epoch 200, training loss: 8.013368606567383 = 1.5144953727722168 + 1.0 * 6.498872756958008
Epoch 200, val loss: 1.550133466720581
Epoch 210, training loss: 7.951736927032471 = 1.4613548517227173 + 1.0 * 6.490382194519043
Epoch 210, val loss: 1.5056122541427612
Epoch 220, training loss: 7.88325309753418 = 1.4034521579742432 + 1.0 * 6.479800701141357
Epoch 220, val loss: 1.457284688949585
Epoch 230, training loss: 7.81409215927124 = 1.3418484926223755 + 1.0 * 6.472243785858154
Epoch 230, val loss: 1.4064911603927612
Epoch 240, training loss: 7.744389533996582 = 1.2789864540100098 + 1.0 * 6.465403079986572
Epoch 240, val loss: 1.355590581893921
Epoch 250, training loss: 7.674270153045654 = 1.2168315649032593 + 1.0 * 6.4574384689331055
Epoch 250, val loss: 1.3064953088760376
Epoch 260, training loss: 7.607246398925781 = 1.1566767692565918 + 1.0 * 6.4505696296691895
Epoch 260, val loss: 1.2603198289871216
Epoch 270, training loss: 7.546534538269043 = 1.0999253988265991 + 1.0 * 6.446609020233154
Epoch 270, val loss: 1.2182103395462036
Epoch 280, training loss: 7.488539695739746 = 1.0477265119552612 + 1.0 * 6.440813064575195
Epoch 280, val loss: 1.1810647249221802
Epoch 290, training loss: 7.433011054992676 = 0.9990964531898499 + 1.0 * 6.433914661407471
Epoch 290, val loss: 1.1479023694992065
Epoch 300, training loss: 7.382556915283203 = 0.9536626935005188 + 1.0 * 6.42889404296875
Epoch 300, val loss: 1.1181175708770752
Epoch 310, training loss: 7.336045742034912 = 0.9114026427268982 + 1.0 * 6.424643039703369
Epoch 310, val loss: 1.0916465520858765
Epoch 320, training loss: 7.290275573730469 = 0.8712993264198303 + 1.0 * 6.418976306915283
Epoch 320, val loss: 1.0674521923065186
Epoch 330, training loss: 7.247706413269043 = 0.8329963088035583 + 1.0 * 6.41471004486084
Epoch 330, val loss: 1.0450631380081177
Epoch 340, training loss: 7.217597484588623 = 0.7964752316474915 + 1.0 * 6.421122074127197
Epoch 340, val loss: 1.0245754718780518
Epoch 350, training loss: 7.168846130371094 = 0.7616729736328125 + 1.0 * 6.407173156738281
Epoch 350, val loss: 1.005839228630066
Epoch 360, training loss: 7.131084442138672 = 0.728156328201294 + 1.0 * 6.402927875518799
Epoch 360, val loss: 0.9885749220848083
Epoch 370, training loss: 7.094157695770264 = 0.6956856846809387 + 1.0 * 6.398471832275391
Epoch 370, val loss: 0.9724771976470947
Epoch 380, training loss: 7.080162048339844 = 0.6642361879348755 + 1.0 * 6.415925979614258
Epoch 380, val loss: 0.957901120185852
Epoch 390, training loss: 7.0346198081970215 = 0.6347694993019104 + 1.0 * 6.399850368499756
Epoch 390, val loss: 0.9450165033340454
Epoch 400, training loss: 6.99899435043335 = 0.6067091226577759 + 1.0 * 6.392285346984863
Epoch 400, val loss: 0.9338429570198059
Epoch 410, training loss: 6.965892314910889 = 0.5796504020690918 + 1.0 * 6.386241912841797
Epoch 410, val loss: 0.9238675236701965
Epoch 420, training loss: 6.93628454208374 = 0.5535238981246948 + 1.0 * 6.382760524749756
Epoch 420, val loss: 0.9152262806892395
Epoch 430, training loss: 6.923246383666992 = 0.5283063650131226 + 1.0 * 6.39493989944458
Epoch 430, val loss: 0.9078161120414734
Epoch 440, training loss: 6.896343231201172 = 0.5043022632598877 + 1.0 * 6.392041206359863
Epoch 440, val loss: 0.9015623331069946
Epoch 450, training loss: 6.859586715698242 = 0.4814630150794983 + 1.0 * 6.378123760223389
Epoch 450, val loss: 0.8963055610656738
Epoch 460, training loss: 6.832723617553711 = 0.459285706281662 + 1.0 * 6.373437881469727
Epoch 460, val loss: 0.8920242786407471
Epoch 470, training loss: 6.80859899520874 = 0.437627911567688 + 1.0 * 6.370971202850342
Epoch 470, val loss: 0.8884522318840027
Epoch 480, training loss: 6.789132595062256 = 0.41647234559059143 + 1.0 * 6.372660160064697
Epoch 480, val loss: 0.8855246901512146
Epoch 490, training loss: 6.764167308807373 = 0.3959888517856598 + 1.0 * 6.368178367614746
Epoch 490, val loss: 0.883170485496521
Epoch 500, training loss: 6.741168022155762 = 0.37622493505477905 + 1.0 * 6.364943027496338
Epoch 500, val loss: 0.8815760016441345
Epoch 510, training loss: 6.731253623962402 = 0.35709524154663086 + 1.0 * 6.3741583824157715
Epoch 510, val loss: 0.8804409503936768
Epoch 520, training loss: 6.7024383544921875 = 0.3387896716594696 + 1.0 * 6.363648891448975
Epoch 520, val loss: 0.8801810145378113
Epoch 530, training loss: 6.681480407714844 = 0.32136270403862 + 1.0 * 6.3601179122924805
Epoch 530, val loss: 0.8805351257324219
Epoch 540, training loss: 6.662083625793457 = 0.3046909272670746 + 1.0 * 6.35739278793335
Epoch 540, val loss: 0.8813626766204834
Epoch 550, training loss: 6.644627094268799 = 0.28878071904182434 + 1.0 * 6.355846405029297
Epoch 550, val loss: 0.8828936815261841
Epoch 560, training loss: 6.629083633422852 = 0.27376505732536316 + 1.0 * 6.355318546295166
Epoch 560, val loss: 0.8850686550140381
Epoch 570, training loss: 6.611073017120361 = 0.2596735656261444 + 1.0 * 6.3513994216918945
Epoch 570, val loss: 0.8878289461135864
Epoch 580, training loss: 6.597036361694336 = 0.24636857211589813 + 1.0 * 6.350667953491211
Epoch 580, val loss: 0.8910267949104309
Epoch 590, training loss: 6.59335470199585 = 0.23387163877487183 + 1.0 * 6.359483242034912
Epoch 590, val loss: 0.8947479724884033
Epoch 600, training loss: 6.57184362411499 = 0.22218731045722961 + 1.0 * 6.349656105041504
Epoch 600, val loss: 0.8990344405174255
Epoch 610, training loss: 6.556942939758301 = 0.21122685074806213 + 1.0 * 6.3457159996032715
Epoch 610, val loss: 0.9035785794258118
Epoch 620, training loss: 6.545275688171387 = 0.20087265968322754 + 1.0 * 6.344403266906738
Epoch 620, val loss: 0.9085350632667542
Epoch 630, training loss: 6.546015739440918 = 0.19111616909503937 + 1.0 * 6.3548994064331055
Epoch 630, val loss: 0.9136981964111328
Epoch 640, training loss: 6.523735523223877 = 0.18197482824325562 + 1.0 * 6.341760635375977
Epoch 640, val loss: 0.9192662835121155
Epoch 650, training loss: 6.517796993255615 = 0.17336052656173706 + 1.0 * 6.3444366455078125
Epoch 650, val loss: 0.9250503778457642
Epoch 660, training loss: 6.506710529327393 = 0.16527484357357025 + 1.0 * 6.34143590927124
Epoch 660, val loss: 0.9308086633682251
Epoch 670, training loss: 6.495291709899902 = 0.1576487421989441 + 1.0 * 6.337643146514893
Epoch 670, val loss: 0.9369709491729736
Epoch 680, training loss: 6.486618518829346 = 0.15042944252490997 + 1.0 * 6.336189270019531
Epoch 680, val loss: 0.9431101679801941
Epoch 690, training loss: 6.480518817901611 = 0.14358237385749817 + 1.0 * 6.3369364738464355
Epoch 690, val loss: 0.9494379162788391
Epoch 700, training loss: 6.471132755279541 = 0.13713569939136505 + 1.0 * 6.3339972496032715
Epoch 700, val loss: 0.9558109045028687
Epoch 710, training loss: 6.465763092041016 = 0.1310519129037857 + 1.0 * 6.334711074829102
Epoch 710, val loss: 0.9623150825500488
Epoch 720, training loss: 6.459012985229492 = 0.12528899312019348 + 1.0 * 6.333724021911621
Epoch 720, val loss: 0.9687540531158447
Epoch 730, training loss: 6.450646877288818 = 0.11983106285333633 + 1.0 * 6.33081579208374
Epoch 730, val loss: 0.9753676652908325
Epoch 740, training loss: 6.446160793304443 = 0.11464998871088028 + 1.0 * 6.3315110206604
Epoch 740, val loss: 0.9820134043693542
Epoch 750, training loss: 6.442257881164551 = 0.10973378270864487 + 1.0 * 6.332524299621582
Epoch 750, val loss: 0.988497793674469
Epoch 760, training loss: 6.43345308303833 = 0.10510173439979553 + 1.0 * 6.3283514976501465
Epoch 760, val loss: 0.9951437711715698
Epoch 770, training loss: 6.426080703735352 = 0.10068415850400925 + 1.0 * 6.325396537780762
Epoch 770, val loss: 1.0016359090805054
Epoch 780, training loss: 6.424623012542725 = 0.09647230058908463 + 1.0 * 6.328150749206543
Epoch 780, val loss: 1.0081703662872314
Epoch 790, training loss: 6.419195652008057 = 0.09246646612882614 + 1.0 * 6.3267292976379395
Epoch 790, val loss: 1.0147991180419922
Epoch 800, training loss: 6.411813735961914 = 0.08866976201534271 + 1.0 * 6.32314395904541
Epoch 800, val loss: 1.0212212800979614
Epoch 810, training loss: 6.407557010650635 = 0.08506075292825699 + 1.0 * 6.32249641418457
Epoch 810, val loss: 1.0276883840560913
Epoch 820, training loss: 6.401551246643066 = 0.08161104470491409 + 1.0 * 6.319940090179443
Epoch 820, val loss: 1.0340611934661865
Epoch 830, training loss: 6.396528244018555 = 0.07830474525690079 + 1.0 * 6.318223476409912
Epoch 830, val loss: 1.040425181388855
Epoch 840, training loss: 6.3951497077941895 = 0.07514394074678421 + 1.0 * 6.320005893707275
Epoch 840, val loss: 1.0468413829803467
Epoch 850, training loss: 6.389739990234375 = 0.07212613523006439 + 1.0 * 6.3176140785217285
Epoch 850, val loss: 1.0532793998718262
Epoch 860, training loss: 6.389155387878418 = 0.06925271451473236 + 1.0 * 6.3199028968811035
Epoch 860, val loss: 1.0595406293869019
Epoch 870, training loss: 6.3917036056518555 = 0.06650976836681366 + 1.0 * 6.325193881988525
Epoch 870, val loss: 1.0658353567123413
Epoch 880, training loss: 6.379083633422852 = 0.06390786170959473 + 1.0 * 6.315175533294678
Epoch 880, val loss: 1.0720309019088745
Epoch 890, training loss: 6.375896453857422 = 0.06141646206378937 + 1.0 * 6.314479827880859
Epoch 890, val loss: 1.0782285928726196
Epoch 900, training loss: 6.3708906173706055 = 0.05902482196688652 + 1.0 * 6.31186580657959
Epoch 900, val loss: 1.0843431949615479
Epoch 910, training loss: 6.3819732666015625 = 0.05674111843109131 + 1.0 * 6.325232028961182
Epoch 910, val loss: 1.0904996395111084
Epoch 920, training loss: 6.36952543258667 = 0.05456273630261421 + 1.0 * 6.314962863922119
Epoch 920, val loss: 1.0966380834579468
Epoch 930, training loss: 6.366060256958008 = 0.05248767510056496 + 1.0 * 6.313572406768799
Epoch 930, val loss: 1.1026411056518555
Epoch 940, training loss: 6.359828948974609 = 0.05050843581557274 + 1.0 * 6.309320449829102
Epoch 940, val loss: 1.1086622476577759
Epoch 950, training loss: 6.355990409851074 = 0.04862212389707565 + 1.0 * 6.307368278503418
Epoch 950, val loss: 1.1147551536560059
Epoch 960, training loss: 6.353575229644775 = 0.04681772738695145 + 1.0 * 6.30675745010376
Epoch 960, val loss: 1.1206828355789185
Epoch 970, training loss: 6.363980770111084 = 0.045097824186086655 + 1.0 * 6.318882942199707
Epoch 970, val loss: 1.1266218423843384
Epoch 980, training loss: 6.352878570556641 = 0.04344901815056801 + 1.0 * 6.30942964553833
Epoch 980, val loss: 1.132457971572876
Epoch 990, training loss: 6.3470048904418945 = 0.04189196601510048 + 1.0 * 6.305112838745117
Epoch 990, val loss: 1.138352394104004
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 10.551355361938477 = 1.9545865058898926 + 1.0 * 8.596769332885742
Epoch 0, val loss: 1.9528276920318604
Epoch 10, training loss: 10.540188789367676 = 1.9438360929489136 + 1.0 * 8.596352577209473
Epoch 10, val loss: 1.9426045417785645
Epoch 20, training loss: 10.52382755279541 = 1.9305530786514282 + 1.0 * 8.593274116516113
Epoch 20, val loss: 1.9294062852859497
Epoch 30, training loss: 10.483444213867188 = 1.9118521213531494 + 1.0 * 8.571592330932617
Epoch 30, val loss: 1.9105052947998047
Epoch 40, training loss: 10.317974090576172 = 1.8885865211486816 + 1.0 * 8.429388046264648
Epoch 40, val loss: 1.8879362344741821
Epoch 50, training loss: 9.808732986450195 = 1.8642010688781738 + 1.0 * 7.944531440734863
Epoch 50, val loss: 1.864719033241272
Epoch 60, training loss: 9.262413024902344 = 1.8478416204452515 + 1.0 * 7.414571762084961
Epoch 60, val loss: 1.8505833148956299
Epoch 70, training loss: 8.882932662963867 = 1.8371373414993286 + 1.0 * 7.04579496383667
Epoch 70, val loss: 1.8410452604293823
Epoch 80, training loss: 8.689387321472168 = 1.8276089429855347 + 1.0 * 6.861778259277344
Epoch 80, val loss: 1.8322564363479614
Epoch 90, training loss: 8.57409381866455 = 1.8154646158218384 + 1.0 * 6.758628845214844
Epoch 90, val loss: 1.8213366270065308
Epoch 100, training loss: 8.489063262939453 = 1.80275559425354 + 1.0 * 6.686307430267334
Epoch 100, val loss: 1.8100197315216064
Epoch 110, training loss: 8.42789077758789 = 1.7911309003829956 + 1.0 * 6.636760234832764
Epoch 110, val loss: 1.7996304035186768
Epoch 120, training loss: 8.380270957946777 = 1.7800476551055908 + 1.0 * 6.600223541259766
Epoch 120, val loss: 1.7895082235336304
Epoch 130, training loss: 8.345161437988281 = 1.7684504985809326 + 1.0 * 6.576711177825928
Epoch 130, val loss: 1.7789188623428345
Epoch 140, training loss: 8.304814338684082 = 1.755846619606018 + 1.0 * 6.548967361450195
Epoch 140, val loss: 1.7676030397415161
Epoch 150, training loss: 8.271320343017578 = 1.7415771484375 + 1.0 * 6.529743194580078
Epoch 150, val loss: 1.7552299499511719
Epoch 160, training loss: 8.237667083740234 = 1.725117802619934 + 1.0 * 6.51254940032959
Epoch 160, val loss: 1.7412011623382568
Epoch 170, training loss: 8.206806182861328 = 1.7055957317352295 + 1.0 * 6.5012102127075195
Epoch 170, val loss: 1.724673867225647
Epoch 180, training loss: 8.168822288513184 = 1.6827585697174072 + 1.0 * 6.486063480377197
Epoch 180, val loss: 1.7054115533828735
Epoch 190, training loss: 8.12937068939209 = 1.655824899673462 + 1.0 * 6.473545551300049
Epoch 190, val loss: 1.6827082633972168
Epoch 200, training loss: 8.088726043701172 = 1.62354576587677 + 1.0 * 6.465179920196533
Epoch 200, val loss: 1.6554834842681885
Epoch 210, training loss: 8.041217803955078 = 1.5862011909484863 + 1.0 * 6.455016136169434
Epoch 210, val loss: 1.6239395141601562
Epoch 220, training loss: 7.989963531494141 = 1.5434606075286865 + 1.0 * 6.446503162384033
Epoch 220, val loss: 1.587774395942688
Epoch 230, training loss: 7.938788414001465 = 1.495714783668518 + 1.0 * 6.443073749542236
Epoch 230, val loss: 1.5476760864257812
Epoch 240, training loss: 7.878448009490967 = 1.4448555707931519 + 1.0 * 6.433592319488525
Epoch 240, val loss: 1.5054233074188232
Epoch 250, training loss: 7.819455146789551 = 1.3917062282562256 + 1.0 * 6.427748680114746
Epoch 250, val loss: 1.4614107608795166
Epoch 260, training loss: 7.76135778427124 = 1.3380612134933472 + 1.0 * 6.4232964515686035
Epoch 260, val loss: 1.4180265665054321
Epoch 270, training loss: 7.701716423034668 = 1.285132646560669 + 1.0 * 6.416584014892578
Epoch 270, val loss: 1.3758265972137451
Epoch 280, training loss: 7.6453752517700195 = 1.2333729267120361 + 1.0 * 6.4120025634765625
Epoch 280, val loss: 1.3353195190429688
Epoch 290, training loss: 7.591770172119141 = 1.183232069015503 + 1.0 * 6.408537864685059
Epoch 290, val loss: 1.2970354557037354
Epoch 300, training loss: 7.537407398223877 = 1.1350833177566528 + 1.0 * 6.402324199676514
Epoch 300, val loss: 1.261008381843567
Epoch 310, training loss: 7.486172676086426 = 1.0880177021026611 + 1.0 * 6.3981547355651855
Epoch 310, val loss: 1.2264615297317505
Epoch 320, training loss: 7.441159248352051 = 1.0424176454544067 + 1.0 * 6.398741722106934
Epoch 320, val loss: 1.1936242580413818
Epoch 330, training loss: 7.388781547546387 = 0.9983955025672913 + 1.0 * 6.39038610458374
Epoch 330, val loss: 1.1623473167419434
Epoch 340, training loss: 7.34136962890625 = 0.9555134773254395 + 1.0 * 6.3858561515808105
Epoch 340, val loss: 1.1323552131652832
Epoch 350, training loss: 7.3011555671691895 = 0.913941502571106 + 1.0 * 6.387214183807373
Epoch 350, val loss: 1.1038932800292969
Epoch 360, training loss: 7.254593849182129 = 0.874468982219696 + 1.0 * 6.380125045776367
Epoch 360, val loss: 1.0772569179534912
Epoch 370, training loss: 7.213571548461914 = 0.8366299867630005 + 1.0 * 6.376941680908203
Epoch 370, val loss: 1.0521111488342285
Epoch 380, training loss: 7.175304889678955 = 0.8002414107322693 + 1.0 * 6.375063419342041
Epoch 380, val loss: 1.028538465499878
Epoch 390, training loss: 7.140300750732422 = 0.7658693790435791 + 1.0 * 6.374431610107422
Epoch 390, val loss: 1.0069193840026855
Epoch 400, training loss: 7.101890563964844 = 0.7335219979286194 + 1.0 * 6.368368625640869
Epoch 400, val loss: 0.9875035881996155
Epoch 410, training loss: 7.069362163543701 = 0.7028697729110718 + 1.0 * 6.36649227142334
Epoch 410, val loss: 0.969698429107666
Epoch 420, training loss: 7.036843299865723 = 0.6739168763160706 + 1.0 * 6.362926483154297
Epoch 420, val loss: 0.9539446830749512
Epoch 430, training loss: 7.0086188316345215 = 0.6467772722244263 + 1.0 * 6.361841678619385
Epoch 430, val loss: 0.9402671456336975
Epoch 440, training loss: 6.979101657867432 = 0.6209973692893982 + 1.0 * 6.358104228973389
Epoch 440, val loss: 0.9283953905105591
Epoch 450, training loss: 6.955669403076172 = 0.5966247916221619 + 1.0 * 6.359044551849365
Epoch 450, val loss: 0.9182775616645813
Epoch 460, training loss: 6.930145740509033 = 0.5738665461540222 + 1.0 * 6.356279373168945
Epoch 460, val loss: 0.9099694490432739
Epoch 470, training loss: 6.9045867919921875 = 0.5521437525749207 + 1.0 * 6.352443218231201
Epoch 470, val loss: 0.9029910564422607
Epoch 480, training loss: 6.880908966064453 = 0.5312321186065674 + 1.0 * 6.349677085876465
Epoch 480, val loss: 0.8971979022026062
Epoch 490, training loss: 6.861159324645996 = 0.5111087560653687 + 1.0 * 6.350050449371338
Epoch 490, val loss: 0.8923584818840027
Epoch 500, training loss: 6.839719772338867 = 0.4917851984500885 + 1.0 * 6.347934722900391
Epoch 500, val loss: 0.8884434700012207
Epoch 510, training loss: 6.81882905960083 = 0.4730958640575409 + 1.0 * 6.345733165740967
Epoch 510, val loss: 0.8852120637893677
Epoch 520, training loss: 6.801755428314209 = 0.4549587368965149 + 1.0 * 6.34679651260376
Epoch 520, val loss: 0.8827111721038818
Epoch 530, training loss: 6.780760288238525 = 0.43745094537734985 + 1.0 * 6.34330940246582
Epoch 530, val loss: 0.8807858228683472
Epoch 540, training loss: 6.7680487632751465 = 0.4204219877719879 + 1.0 * 6.347626686096191
Epoch 540, val loss: 0.8793907761573792
Epoch 550, training loss: 6.742768287658691 = 0.4040623605251312 + 1.0 * 6.338706016540527
Epoch 550, val loss: 0.8785288333892822
Epoch 560, training loss: 6.724676132202148 = 0.3881697654724121 + 1.0 * 6.336506366729736
Epoch 560, val loss: 0.878102719783783
Epoch 570, training loss: 6.714550495147705 = 0.3727644383907318 + 1.0 * 6.341785907745361
Epoch 570, val loss: 0.8780901432037354
Epoch 580, training loss: 6.693828105926514 = 0.3579297661781311 + 1.0 * 6.335898399353027
Epoch 580, val loss: 0.878627359867096
Epoch 590, training loss: 6.677231311798096 = 0.3436208665370941 + 1.0 * 6.333610534667969
Epoch 590, val loss: 0.87962806224823
Epoch 600, training loss: 6.662010192871094 = 0.3297816812992096 + 1.0 * 6.332228660583496
Epoch 600, val loss: 0.8811051249504089
Epoch 610, training loss: 6.6464972496032715 = 0.3163687586784363 + 1.0 * 6.3301286697387695
Epoch 610, val loss: 0.8829789161682129
Epoch 620, training loss: 6.634159564971924 = 0.3033606708049774 + 1.0 * 6.330799102783203
Epoch 620, val loss: 0.8852491974830627
Epoch 630, training loss: 6.62269926071167 = 0.2907164394855499 + 1.0 * 6.331982612609863
Epoch 630, val loss: 0.8878735303878784
Epoch 640, training loss: 6.60530424118042 = 0.2784140408039093 + 1.0 * 6.326889991760254
Epoch 640, val loss: 0.8909465074539185
Epoch 650, training loss: 6.594918251037598 = 0.26642411947250366 + 1.0 * 6.328494071960449
Epoch 650, val loss: 0.8942685723304749
Epoch 660, training loss: 6.57847785949707 = 0.25471803545951843 + 1.0 * 6.323760032653809
Epoch 660, val loss: 0.8979341983795166
Epoch 670, training loss: 6.565576553344727 = 0.24318194389343262 + 1.0 * 6.322394371032715
Epoch 670, val loss: 0.9017675518989563
Epoch 680, training loss: 6.562589168548584 = 0.23184339702129364 + 1.0 * 6.330745697021484
Epoch 680, val loss: 0.905859112739563
Epoch 690, training loss: 6.545651912689209 = 0.2207375168800354 + 1.0 * 6.324914455413818
Epoch 690, val loss: 0.9101458191871643
Epoch 700, training loss: 6.530144691467285 = 0.20988839864730835 + 1.0 * 6.320256233215332
Epoch 700, val loss: 0.9146246314048767
Epoch 710, training loss: 6.517078876495361 = 0.19929757714271545 + 1.0 * 6.317781448364258
Epoch 710, val loss: 0.9193240404129028
Epoch 720, training loss: 6.506852149963379 = 0.188957080245018 + 1.0 * 6.31789493560791
Epoch 720, val loss: 0.9242657423019409
Epoch 730, training loss: 6.502907752990723 = 0.17895199358463287 + 1.0 * 6.323955535888672
Epoch 730, val loss: 0.9294517040252686
Epoch 740, training loss: 6.487561225891113 = 0.16933637857437134 + 1.0 * 6.318224906921387
Epoch 740, val loss: 0.934811532497406
Epoch 750, training loss: 6.478536605834961 = 0.1601671278476715 + 1.0 * 6.318369388580322
Epoch 750, val loss: 0.9403805732727051
Epoch 760, training loss: 6.466119289398193 = 0.15143899619579315 + 1.0 * 6.314680099487305
Epoch 760, val loss: 0.9461376667022705
Epoch 770, training loss: 6.456006050109863 = 0.14317113161087036 + 1.0 * 6.312834739685059
Epoch 770, val loss: 0.9520983695983887
Epoch 780, training loss: 6.455334663391113 = 0.1353609412908554 + 1.0 * 6.319973945617676
Epoch 780, val loss: 0.9582325220108032
Epoch 790, training loss: 6.4427170753479 = 0.12803015112876892 + 1.0 * 6.3146867752075195
Epoch 790, val loss: 0.9645182490348816
Epoch 800, training loss: 6.431405067443848 = 0.12116863578557968 + 1.0 * 6.31023645401001
Epoch 800, val loss: 0.9709886312484741
Epoch 810, training loss: 6.427073001861572 = 0.11472918838262558 + 1.0 * 6.312343597412109
Epoch 810, val loss: 0.9774408340454102
Epoch 820, training loss: 6.418534755706787 = 0.10870200395584106 + 1.0 * 6.309832572937012
Epoch 820, val loss: 0.9840300679206848
Epoch 830, training loss: 6.4134345054626465 = 0.10308580100536346 + 1.0 * 6.3103485107421875
Epoch 830, val loss: 0.9906837940216064
Epoch 840, training loss: 6.4051079750061035 = 0.09783237427473068 + 1.0 * 6.307275772094727
Epoch 840, val loss: 0.9973226189613342
Epoch 850, training loss: 6.39935302734375 = 0.09293252974748611 + 1.0 * 6.30642032623291
Epoch 850, val loss: 1.0040127038955688
Epoch 860, training loss: 6.392918586730957 = 0.08831808716058731 + 1.0 * 6.304600715637207
Epoch 860, val loss: 1.010727047920227
Epoch 870, training loss: 6.392281532287598 = 0.08399675041437149 + 1.0 * 6.308284759521484
Epoch 870, val loss: 1.0174986124038696
Epoch 880, training loss: 6.384007930755615 = 0.07993778586387634 + 1.0 * 6.304069995880127
Epoch 880, val loss: 1.024254560470581
Epoch 890, training loss: 6.381248950958252 = 0.0761512964963913 + 1.0 * 6.305097579956055
Epoch 890, val loss: 1.0310041904449463
Epoch 900, training loss: 6.3765645027160645 = 0.07258745282888412 + 1.0 * 6.303977012634277
Epoch 900, val loss: 1.0377362966537476
Epoch 910, training loss: 6.371034145355225 = 0.06925522536039352 + 1.0 * 6.301778793334961
Epoch 910, val loss: 1.0444724559783936
Epoch 920, training loss: 6.36752462387085 = 0.0661085769534111 + 1.0 * 6.301415920257568
Epoch 920, val loss: 1.051160216331482
Epoch 930, training loss: 6.3647308349609375 = 0.06315430998802185 + 1.0 * 6.301576614379883
Epoch 930, val loss: 1.05785071849823
Epoch 940, training loss: 6.358860492706299 = 0.06038222089409828 + 1.0 * 6.298478126525879
Epoch 940, val loss: 1.0645384788513184
Epoch 950, training loss: 6.354918003082275 = 0.0577726811170578 + 1.0 * 6.297145366668701
Epoch 950, val loss: 1.0712347030639648
Epoch 960, training loss: 6.354379177093506 = 0.055313173681497574 + 1.0 * 6.299066066741943
Epoch 960, val loss: 1.0778872966766357
Epoch 970, training loss: 6.348122596740723 = 0.05299617722630501 + 1.0 * 6.295126438140869
Epoch 970, val loss: 1.0845847129821777
Epoch 980, training loss: 6.351593017578125 = 0.05080726370215416 + 1.0 * 6.300785541534424
Epoch 980, val loss: 1.091225028038025
Epoch 990, training loss: 6.345163822174072 = 0.04876064881682396 + 1.0 * 6.296403408050537
Epoch 990, val loss: 1.0978423357009888
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8133895624670533
The final CL Acc:0.77407, 0.01048, The final GNN Acc:0.81163, 0.00131
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13128])
remove edge: torch.Size([2, 7910])
updated graph: torch.Size([2, 10482])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.550332069396973 = 1.9535187482833862 + 1.0 * 8.596813201904297
Epoch 0, val loss: 1.9567086696624756
Epoch 10, training loss: 10.539179801940918 = 1.942657470703125 + 1.0 * 8.596522331237793
Epoch 10, val loss: 1.9451097249984741
Epoch 20, training loss: 10.523367881774902 = 1.928998589515686 + 1.0 * 8.594368934631348
Epoch 20, val loss: 1.9305942058563232
Epoch 30, training loss: 10.486824035644531 = 1.9097638130187988 + 1.0 * 8.577059745788574
Epoch 30, val loss: 1.9098894596099854
Epoch 40, training loss: 10.35399055480957 = 1.884095549583435 + 1.0 * 8.469895362854004
Epoch 40, val loss: 1.8828797340393066
Epoch 50, training loss: 9.850030899047852 = 1.857743740081787 + 1.0 * 7.992286682128906
Epoch 50, val loss: 1.8563802242279053
Epoch 60, training loss: 9.375301361083984 = 1.8373970985412598 + 1.0 * 7.537904739379883
Epoch 60, val loss: 1.8374499082565308
Epoch 70, training loss: 8.961685180664062 = 1.823061466217041 + 1.0 * 7.13862419128418
Epoch 70, val loss: 1.8238201141357422
Epoch 80, training loss: 8.766183853149414 = 1.8090170621871948 + 1.0 * 6.95716667175293
Epoch 80, val loss: 1.8106077909469604
Epoch 90, training loss: 8.672439575195312 = 1.7913984060287476 + 1.0 * 6.881041049957275
Epoch 90, val loss: 1.7939558029174805
Epoch 100, training loss: 8.577528953552246 = 1.7732419967651367 + 1.0 * 6.804286956787109
Epoch 100, val loss: 1.7771199941635132
Epoch 110, training loss: 8.492713928222656 = 1.7570446729660034 + 1.0 * 6.7356696128845215
Epoch 110, val loss: 1.7626190185546875
Epoch 120, training loss: 8.41895580291748 = 1.7408568859100342 + 1.0 * 6.678099155426025
Epoch 120, val loss: 1.7483925819396973
Epoch 130, training loss: 8.353411674499512 = 1.7220343351364136 + 1.0 * 6.631377696990967
Epoch 130, val loss: 1.7322477102279663
Epoch 140, training loss: 8.294867515563965 = 1.7002097368240356 + 1.0 * 6.5946574211120605
Epoch 140, val loss: 1.7140390872955322
Epoch 150, training loss: 8.240044593811035 = 1.6747711896896362 + 1.0 * 6.565273761749268
Epoch 150, val loss: 1.6930814981460571
Epoch 160, training loss: 8.185941696166992 = 1.6448208093643188 + 1.0 * 6.541121006011963
Epoch 160, val loss: 1.6685384511947632
Epoch 170, training loss: 8.130420684814453 = 1.6098027229309082 + 1.0 * 6.520617485046387
Epoch 170, val loss: 1.6397992372512817
Epoch 180, training loss: 8.072311401367188 = 1.5689003467559814 + 1.0 * 6.503410816192627
Epoch 180, val loss: 1.6065146923065186
Epoch 190, training loss: 8.012133598327637 = 1.5222713947296143 + 1.0 * 6.489861965179443
Epoch 190, val loss: 1.5689630508422852
Epoch 200, training loss: 7.948110580444336 = 1.4713666439056396 + 1.0 * 6.476744174957275
Epoch 200, val loss: 1.5285221338272095
Epoch 210, training loss: 7.882667541503906 = 1.4171011447906494 + 1.0 * 6.465566158294678
Epoch 210, val loss: 1.486092448234558
Epoch 220, training loss: 7.820701599121094 = 1.3607491254806519 + 1.0 * 6.459952354431152
Epoch 220, val loss: 1.4427777528762817
Epoch 230, training loss: 7.75551700592041 = 1.304673194885254 + 1.0 * 6.450843811035156
Epoch 230, val loss: 1.4003393650054932
Epoch 240, training loss: 7.691829204559326 = 1.2498852014541626 + 1.0 * 6.441944122314453
Epoch 240, val loss: 1.359281301498413
Epoch 250, training loss: 7.633147239685059 = 1.1968400478363037 + 1.0 * 6.436306953430176
Epoch 250, val loss: 1.3200011253356934
Epoch 260, training loss: 7.57753849029541 = 1.1465826034545898 + 1.0 * 6.43095588684082
Epoch 260, val loss: 1.2831456661224365
Epoch 270, training loss: 7.52359676361084 = 1.100043535232544 + 1.0 * 6.423552989959717
Epoch 270, val loss: 1.2494628429412842
Epoch 280, training loss: 7.47976016998291 = 1.0569322109222412 + 1.0 * 6.422828197479248
Epoch 280, val loss: 1.2184875011444092
Epoch 290, training loss: 7.430177688598633 = 1.0175201892852783 + 1.0 * 6.412657260894775
Epoch 290, val loss: 1.1904826164245605
Epoch 300, training loss: 7.387109756469727 = 0.9807523488998413 + 1.0 * 6.406357288360596
Epoch 300, val loss: 1.1647576093673706
Epoch 310, training loss: 7.347136497497559 = 0.9456905722618103 + 1.0 * 6.4014458656311035
Epoch 310, val loss: 1.140335202217102
Epoch 320, training loss: 7.31654167175293 = 0.9116926193237305 + 1.0 * 6.404849052429199
Epoch 320, val loss: 1.1167502403259277
Epoch 330, training loss: 7.278236389160156 = 0.8792040348052979 + 1.0 * 6.3990325927734375
Epoch 330, val loss: 1.094069480895996
Epoch 340, training loss: 7.236719131469727 = 0.8478599190711975 + 1.0 * 6.388859272003174
Epoch 340, val loss: 1.0720571279525757
Epoch 350, training loss: 7.202045440673828 = 0.8172144889831543 + 1.0 * 6.384830951690674
Epoch 350, val loss: 1.0505164861679077
Epoch 360, training loss: 7.1713480949401855 = 0.7872361540794373 + 1.0 * 6.3841118812561035
Epoch 360, val loss: 1.0294389724731445
Epoch 370, training loss: 7.13873291015625 = 0.7584319114685059 + 1.0 * 6.380300998687744
Epoch 370, val loss: 1.009312391281128
Epoch 380, training loss: 7.105686664581299 = 0.7308937907218933 + 1.0 * 6.37479305267334
Epoch 380, val loss: 0.9901708364486694
Epoch 390, training loss: 7.0764360427856445 = 0.7045133113861084 + 1.0 * 6.371922969818115
Epoch 390, val loss: 0.9722216725349426
Epoch 400, training loss: 7.050199031829834 = 0.6794301867485046 + 1.0 * 6.370769023895264
Epoch 400, val loss: 0.9554445743560791
Epoch 410, training loss: 7.021791458129883 = 0.6554815173149109 + 1.0 * 6.366310119628906
Epoch 410, val loss: 0.9398468732833862
Epoch 420, training loss: 6.997855186462402 = 0.6323093771934509 + 1.0 * 6.365545749664307
Epoch 420, val loss: 0.9252176880836487
Epoch 430, training loss: 6.969133377075195 = 0.6099920272827148 + 1.0 * 6.3591413497924805
Epoch 430, val loss: 0.9114974141120911
Epoch 440, training loss: 6.951566696166992 = 0.5881640911102295 + 1.0 * 6.363402843475342
Epoch 440, val loss: 0.8986675143241882
Epoch 450, training loss: 6.920535564422607 = 0.5669191479682922 + 1.0 * 6.353616237640381
Epoch 450, val loss: 0.8866336941719055
Epoch 460, training loss: 6.896528244018555 = 0.5460723638534546 + 1.0 * 6.3504557609558105
Epoch 460, val loss: 0.8753225207328796
Epoch 470, training loss: 6.881232261657715 = 0.5255398154258728 + 1.0 * 6.355692386627197
Epoch 470, val loss: 0.8646715879440308
Epoch 480, training loss: 6.853024482727051 = 0.5054346323013306 + 1.0 * 6.34758996963501
Epoch 480, val loss: 0.8547711372375488
Epoch 490, training loss: 6.831624984741211 = 0.4858092665672302 + 1.0 * 6.345815658569336
Epoch 490, val loss: 0.8456140756607056
Epoch 500, training loss: 6.809544563293457 = 0.4665319323539734 + 1.0 * 6.343012809753418
Epoch 500, val loss: 0.837127685546875
Epoch 510, training loss: 6.789336681365967 = 0.44752711057662964 + 1.0 * 6.3418097496032715
Epoch 510, val loss: 0.8292846083641052
Epoch 520, training loss: 6.768880367279053 = 0.42878592014312744 + 1.0 * 6.340094566345215
Epoch 520, val loss: 0.8220847249031067
Epoch 530, training loss: 6.753509998321533 = 0.4104313850402832 + 1.0 * 6.34307861328125
Epoch 530, val loss: 0.8155078887939453
Epoch 540, training loss: 6.728957176208496 = 0.39243289828300476 + 1.0 * 6.336524486541748
Epoch 540, val loss: 0.8095211386680603
Epoch 550, training loss: 6.709005355834961 = 0.37462353706359863 + 1.0 * 6.334381580352783
Epoch 550, val loss: 0.8039399981498718
Epoch 560, training loss: 6.692551136016846 = 0.3568780720233917 + 1.0 * 6.335672855377197
Epoch 560, val loss: 0.7988522052764893
Epoch 570, training loss: 6.672632217407227 = 0.3393086791038513 + 1.0 * 6.3333234786987305
Epoch 570, val loss: 0.7942143678665161
Epoch 580, training loss: 6.652533531188965 = 0.32180488109588623 + 1.0 * 6.330728530883789
Epoch 580, val loss: 0.7898771166801453
Epoch 590, training loss: 6.6335530281066895 = 0.30436816811561584 + 1.0 * 6.3291850090026855
Epoch 590, val loss: 0.7859369516372681
Epoch 600, training loss: 6.62192964553833 = 0.28709062933921814 + 1.0 * 6.3348388671875
Epoch 600, val loss: 0.7825644612312317
Epoch 610, training loss: 6.6000447273254395 = 0.2701900601387024 + 1.0 * 6.329854488372803
Epoch 610, val loss: 0.7794397473335266
Epoch 620, training loss: 6.578334808349609 = 0.2535995543003082 + 1.0 * 6.324735164642334
Epoch 620, val loss: 0.7769507765769958
Epoch 630, training loss: 6.563798904418945 = 0.23738910257816315 + 1.0 * 6.326409816741943
Epoch 630, val loss: 0.7752200365066528
Epoch 640, training loss: 6.550650119781494 = 0.22189639508724213 + 1.0 * 6.32875394821167
Epoch 640, val loss: 0.774237871170044
Epoch 650, training loss: 6.527892112731934 = 0.20715588331222534 + 1.0 * 6.320736408233643
Epoch 650, val loss: 0.773991048336029
Epoch 660, training loss: 6.520921230316162 = 0.19322547316551208 + 1.0 * 6.327695846557617
Epoch 660, val loss: 0.7747008800506592
Epoch 670, training loss: 6.502664089202881 = 0.18027757108211517 + 1.0 * 6.322386741638184
Epoch 670, val loss: 0.7762388586997986
Epoch 680, training loss: 6.485128402709961 = 0.16824233531951904 + 1.0 * 6.316885948181152
Epoch 680, val loss: 0.7785618305206299
Epoch 690, training loss: 6.484122276306152 = 0.15709912776947021 + 1.0 * 6.327023029327393
Epoch 690, val loss: 0.7818174958229065
Epoch 700, training loss: 6.462527275085449 = 0.14682354032993317 + 1.0 * 6.315703868865967
Epoch 700, val loss: 0.7856303453445435
Epoch 710, training loss: 6.452207088470459 = 0.13737766444683075 + 1.0 * 6.314829349517822
Epoch 710, val loss: 0.7900570631027222
Epoch 720, training loss: 6.445918560028076 = 0.12868593633174896 + 1.0 * 6.317232608795166
Epoch 720, val loss: 0.795128345489502
Epoch 730, training loss: 6.433845520019531 = 0.12072374671697617 + 1.0 * 6.313121795654297
Epoch 730, val loss: 0.8004260063171387
Epoch 740, training loss: 6.423720836639404 = 0.11337965726852417 + 1.0 * 6.3103413581848145
Epoch 740, val loss: 0.8061813712120056
Epoch 750, training loss: 6.4161529541015625 = 0.1065841093659401 + 1.0 * 6.309568881988525
Epoch 750, val loss: 0.8123090863227844
Epoch 760, training loss: 6.416001319885254 = 0.1003151461482048 + 1.0 * 6.315686225891113
Epoch 760, val loss: 0.8187586069107056
Epoch 770, training loss: 6.406062126159668 = 0.09458628296852112 + 1.0 * 6.31147575378418
Epoch 770, val loss: 0.8251216411590576
Epoch 780, training loss: 6.396459102630615 = 0.08929584920406342 + 1.0 * 6.307163238525391
Epoch 780, val loss: 0.8316762447357178
Epoch 790, training loss: 6.389477252960205 = 0.08437378704547882 + 1.0 * 6.305103302001953
Epoch 790, val loss: 0.8385235667228699
Epoch 800, training loss: 6.396827220916748 = 0.07982424646615982 + 1.0 * 6.317002773284912
Epoch 800, val loss: 0.8455148339271545
Epoch 810, training loss: 6.380185127258301 = 0.07562902569770813 + 1.0 * 6.304555892944336
Epoch 810, val loss: 0.8523204922676086
Epoch 820, training loss: 6.375035285949707 = 0.07174047082662582 + 1.0 * 6.303294658660889
Epoch 820, val loss: 0.8591822981834412
Epoch 830, training loss: 6.371841907501221 = 0.06811226159334183 + 1.0 * 6.30372953414917
Epoch 830, val loss: 0.8662846088409424
Epoch 840, training loss: 6.366020202636719 = 0.06474167853593826 + 1.0 * 6.301278591156006
Epoch 840, val loss: 0.8733726143836975
Epoch 850, training loss: 6.362821102142334 = 0.061603184789419174 + 1.0 * 6.301218032836914
Epoch 850, val loss: 0.8803739547729492
Epoch 860, training loss: 6.359869956970215 = 0.05866751819849014 + 1.0 * 6.301202297210693
Epoch 860, val loss: 0.8874077796936035
Epoch 870, training loss: 6.3542704582214355 = 0.05593441426753998 + 1.0 * 6.298336029052734
Epoch 870, val loss: 0.8943704962730408
Epoch 880, training loss: 6.354416370391846 = 0.053374215960502625 + 1.0 * 6.301042079925537
Epoch 880, val loss: 0.9013987183570862
Epoch 890, training loss: 6.352806091308594 = 0.05098648741841316 + 1.0 * 6.301819801330566
Epoch 890, val loss: 0.9083675146102905
Epoch 900, training loss: 6.343822479248047 = 0.04877106472849846 + 1.0 * 6.295051574707031
Epoch 900, val loss: 0.9149443507194519
Epoch 910, training loss: 6.341630935668945 = 0.0466848649084568 + 1.0 * 6.294946193695068
Epoch 910, val loss: 0.921748161315918
Epoch 920, training loss: 6.338198184967041 = 0.04471687227487564 + 1.0 * 6.293481349945068
Epoch 920, val loss: 0.9284955859184265
Epoch 930, training loss: 6.344459056854248 = 0.042865436524152756 + 1.0 * 6.301593780517578
Epoch 930, val loss: 0.9352072477340698
Epoch 940, training loss: 6.336097240447998 = 0.04114393889904022 + 1.0 * 6.294953346252441
Epoch 940, val loss: 0.9417043328285217
Epoch 950, training loss: 6.334403991699219 = 0.039519816637039185 + 1.0 * 6.294884204864502
Epoch 950, val loss: 0.9482190012931824
Epoch 960, training loss: 6.329823017120361 = 0.03799261525273323 + 1.0 * 6.291830539703369
Epoch 960, val loss: 0.9544347524642944
Epoch 970, training loss: 6.326594829559326 = 0.03655129298567772 + 1.0 * 6.290043354034424
Epoch 970, val loss: 0.9607904553413391
Epoch 980, training loss: 6.329259872436523 = 0.03518502041697502 + 1.0 * 6.294075012207031
Epoch 980, val loss: 0.9670948386192322
Epoch 990, training loss: 6.3229570388793945 = 0.03389330580830574 + 1.0 * 6.289063930511475
Epoch 990, val loss: 0.9733067154884338
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 10.544611930847168 = 1.9477895498275757 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.9451816082000732
Epoch 10, training loss: 10.534704208374023 = 1.9381277561187744 + 1.0 * 8.596576690673828
Epoch 10, val loss: 1.9357478618621826
Epoch 20, training loss: 10.521245956420898 = 1.9265692234039307 + 1.0 * 8.594676971435547
Epoch 20, val loss: 1.9239085912704468
Epoch 30, training loss: 10.48971939086914 = 1.9108374118804932 + 1.0 * 8.578882217407227
Epoch 30, val loss: 1.9074283838272095
Epoch 40, training loss: 10.357701301574707 = 1.889577031135559 + 1.0 * 8.468124389648438
Epoch 40, val loss: 1.8854955434799194
Epoch 50, training loss: 9.84078598022461 = 1.8658864498138428 + 1.0 * 7.9748992919921875
Epoch 50, val loss: 1.8617817163467407
Epoch 60, training loss: 9.301434516906738 = 1.8465713262557983 + 1.0 * 7.45486307144165
Epoch 60, val loss: 1.8443902730941772
Epoch 70, training loss: 8.94218635559082 = 1.8333967924118042 + 1.0 * 7.108789443969727
Epoch 70, val loss: 1.8317079544067383
Epoch 80, training loss: 8.770267486572266 = 1.8190151453018188 + 1.0 * 6.951252460479736
Epoch 80, val loss: 1.8180416822433472
Epoch 90, training loss: 8.644591331481934 = 1.8025814294815063 + 1.0 * 6.842009544372559
Epoch 90, val loss: 1.8030951023101807
Epoch 100, training loss: 8.54319953918457 = 1.7863492965698242 + 1.0 * 6.756850242614746
Epoch 100, val loss: 1.7889213562011719
Epoch 110, training loss: 8.468900680541992 = 1.7708845138549805 + 1.0 * 6.698016166687012
Epoch 110, val loss: 1.775425910949707
Epoch 120, training loss: 8.406777381896973 = 1.754703402519226 + 1.0 * 6.652073860168457
Epoch 120, val loss: 1.761148452758789
Epoch 130, training loss: 8.35478687286377 = 1.7370456457138062 + 1.0 * 6.617741107940674
Epoch 130, val loss: 1.7455159425735474
Epoch 140, training loss: 8.303548812866211 = 1.7173457145690918 + 1.0 * 6.586202621459961
Epoch 140, val loss: 1.7284284830093384
Epoch 150, training loss: 8.2564115524292 = 1.694702386856079 + 1.0 * 6.561709403991699
Epoch 150, val loss: 1.7089204788208008
Epoch 160, training loss: 8.208542823791504 = 1.6679292917251587 + 1.0 * 6.540613651275635
Epoch 160, val loss: 1.6858948469161987
Epoch 170, training loss: 8.162504196166992 = 1.6359412670135498 + 1.0 * 6.5265631675720215
Epoch 170, val loss: 1.6583915948867798
Epoch 180, training loss: 8.109417915344238 = 1.5986436605453491 + 1.0 * 6.510774612426758
Epoch 180, val loss: 1.626206398010254
Epoch 190, training loss: 8.05432415008545 = 1.5553227663040161 + 1.0 * 6.4990010261535645
Epoch 190, val loss: 1.5887974500656128
Epoch 200, training loss: 7.996064186096191 = 1.5059093236923218 + 1.0 * 6.49015474319458
Epoch 200, val loss: 1.5462615489959717
Epoch 210, training loss: 7.933347702026367 = 1.4518934488296509 + 1.0 * 6.481454372406006
Epoch 210, val loss: 1.4999898672103882
Epoch 220, training loss: 7.86673641204834 = 1.3945090770721436 + 1.0 * 6.472227573394775
Epoch 220, val loss: 1.4509820938110352
Epoch 230, training loss: 7.805575847625732 = 1.3355708122253418 + 1.0 * 6.470005035400391
Epoch 230, val loss: 1.4009835720062256
Epoch 240, training loss: 7.736905097961426 = 1.2777092456817627 + 1.0 * 6.459195613861084
Epoch 240, val loss: 1.3524150848388672
Epoch 250, training loss: 7.674124717712402 = 1.2215681076049805 + 1.0 * 6.452556610107422
Epoch 250, val loss: 1.3055158853530884
Epoch 260, training loss: 7.618992328643799 = 1.167809009552002 + 1.0 * 6.451183319091797
Epoch 260, val loss: 1.2612146139144897
Epoch 270, training loss: 7.55897331237793 = 1.117854356765747 + 1.0 * 6.441119194030762
Epoch 270, val loss: 1.2206003665924072
Epoch 280, training loss: 7.507359504699707 = 1.071484088897705 + 1.0 * 6.435875415802002
Epoch 280, val loss: 1.1836049556732178
Epoch 290, training loss: 7.4610114097595215 = 1.0291043519973755 + 1.0 * 6.4319071769714355
Epoch 290, val loss: 1.1506778001785278
Epoch 300, training loss: 7.4162445068359375 = 0.9903547763824463 + 1.0 * 6.42588996887207
Epoch 300, val loss: 1.1210366487503052
Epoch 310, training loss: 7.374357223510742 = 0.9539633393287659 + 1.0 * 6.420393943786621
Epoch 310, val loss: 1.093958854675293
Epoch 320, training loss: 7.334076881408691 = 0.9190540313720703 + 1.0 * 6.415022850036621
Epoch 320, val loss: 1.0686070919036865
Epoch 330, training loss: 7.297287464141846 = 0.885344922542572 + 1.0 * 6.411942481994629
Epoch 330, val loss: 1.0446312427520752
Epoch 340, training loss: 7.261112689971924 = 0.8528385162353516 + 1.0 * 6.408274173736572
Epoch 340, val loss: 1.0220602750778198
Epoch 350, training loss: 7.224521636962891 = 0.8208322525024414 + 1.0 * 6.403689384460449
Epoch 350, val loss: 1.0003622770309448
Epoch 360, training loss: 7.195094108581543 = 0.7889280319213867 + 1.0 * 6.406166076660156
Epoch 360, val loss: 0.9790228009223938
Epoch 370, training loss: 7.15341854095459 = 0.7574831247329712 + 1.0 * 6.395935535430908
Epoch 370, val loss: 0.9580915570259094
Epoch 380, training loss: 7.119176864624023 = 0.7262467741966248 + 1.0 * 6.392930030822754
Epoch 380, val loss: 0.9376435875892639
Epoch 390, training loss: 7.091302394866943 = 0.6951006650924683 + 1.0 * 6.3962016105651855
Epoch 390, val loss: 0.9174951910972595
Epoch 400, training loss: 7.050870895385742 = 0.6643427014350891 + 1.0 * 6.386528015136719
Epoch 400, val loss: 0.8978309631347656
Epoch 410, training loss: 7.021191120147705 = 0.6339805722236633 + 1.0 * 6.387210369110107
Epoch 410, val loss: 0.8786671161651611
Epoch 420, training loss: 6.9888482093811035 = 0.6042748093605042 + 1.0 * 6.384573459625244
Epoch 420, val loss: 0.8600329756736755
Epoch 430, training loss: 6.954748630523682 = 0.5755141377449036 + 1.0 * 6.379234313964844
Epoch 430, val loss: 0.8422508835792542
Epoch 440, training loss: 6.925920009613037 = 0.5474479794502258 + 1.0 * 6.378471851348877
Epoch 440, val loss: 0.8253797888755798
Epoch 450, training loss: 6.896503925323486 = 0.5203638672828674 + 1.0 * 6.376140117645264
Epoch 450, val loss: 0.8092373013496399
Epoch 460, training loss: 6.867216110229492 = 0.49428752064704895 + 1.0 * 6.372928619384766
Epoch 460, val loss: 0.7943570017814636
Epoch 470, training loss: 6.838764667510986 = 0.469198614358902 + 1.0 * 6.369565963745117
Epoch 470, val loss: 0.7803916931152344
Epoch 480, training loss: 6.812709808349609 = 0.44498345255851746 + 1.0 * 6.3677263259887695
Epoch 480, val loss: 0.7675013542175293
Epoch 490, training loss: 6.789880752563477 = 0.4217904806137085 + 1.0 * 6.3680901527404785
Epoch 490, val loss: 0.755676805973053
Epoch 500, training loss: 6.763149738311768 = 0.3995843529701233 + 1.0 * 6.363565444946289
Epoch 500, val loss: 0.7450501322746277
Epoch 510, training loss: 6.7409796714782715 = 0.3782545328140259 + 1.0 * 6.362725257873535
Epoch 510, val loss: 0.7355553507804871
Epoch 520, training loss: 6.715400218963623 = 0.3577737808227539 + 1.0 * 6.357626438140869
Epoch 520, val loss: 0.7271345853805542
Epoch 530, training loss: 6.702719688415527 = 0.33811163902282715 + 1.0 * 6.364608287811279
Epoch 530, val loss: 0.7198118567466736
Epoch 540, training loss: 6.676146507263184 = 0.3194591999053955 + 1.0 * 6.356687545776367
Epoch 540, val loss: 0.7135322093963623
Epoch 550, training loss: 6.6553449630737305 = 0.3016684651374817 + 1.0 * 6.3536763191223145
Epoch 550, val loss: 0.7084153294563293
Epoch 560, training loss: 6.639308452606201 = 0.2847229540348053 + 1.0 * 6.354585647583008
Epoch 560, val loss: 0.7043379545211792
Epoch 570, training loss: 6.6257781982421875 = 0.2686809301376343 + 1.0 * 6.357097148895264
Epoch 570, val loss: 0.7011858820915222
Epoch 580, training loss: 6.602739334106445 = 0.2536240518093109 + 1.0 * 6.349115371704102
Epoch 580, val loss: 0.6992414593696594
Epoch 590, training loss: 6.5870442390441895 = 0.23936349153518677 + 1.0 * 6.347680568695068
Epoch 590, val loss: 0.6982895135879517
Epoch 600, training loss: 6.570577144622803 = 0.22592118382453918 + 1.0 * 6.344655990600586
Epoch 600, val loss: 0.6981139183044434
Epoch 610, training loss: 6.555205345153809 = 0.21330907940864563 + 1.0 * 6.341896057128906
Epoch 610, val loss: 0.699011504650116
Epoch 620, training loss: 6.541871070861816 = 0.20145726203918457 + 1.0 * 6.340413570404053
Epoch 620, val loss: 0.7007884979248047
Epoch 630, training loss: 6.536755561828613 = 0.19035704433918 + 1.0 * 6.34639835357666
Epoch 630, val loss: 0.7031701803207397
Epoch 640, training loss: 6.519647598266602 = 0.18012014031410217 + 1.0 * 6.339527606964111
Epoch 640, val loss: 0.7062754034996033
Epoch 650, training loss: 6.5069355964660645 = 0.17056399583816528 + 1.0 * 6.336371421813965
Epoch 650, val loss: 0.710129976272583
Epoch 660, training loss: 6.4961748123168945 = 0.1616145819425583 + 1.0 * 6.334560394287109
Epoch 660, val loss: 0.7144354581832886
Epoch 670, training loss: 6.486682891845703 = 0.15323525667190552 + 1.0 * 6.333447456359863
Epoch 670, val loss: 0.719300389289856
Epoch 680, training loss: 6.4931416511535645 = 0.14544378221035004 + 1.0 * 6.347697734832764
Epoch 680, val loss: 0.7243473529815674
Epoch 690, training loss: 6.47128438949585 = 0.13823649287223816 + 1.0 * 6.333047866821289
Epoch 690, val loss: 0.7296099662780762
Epoch 700, training loss: 6.463553428649902 = 0.13153371214866638 + 1.0 * 6.332019805908203
Epoch 700, val loss: 0.7354361414909363
Epoch 710, training loss: 6.453481674194336 = 0.12524114549160004 + 1.0 * 6.328240394592285
Epoch 710, val loss: 0.741351842880249
Epoch 720, training loss: 6.445735931396484 = 0.11931752413511276 + 1.0 * 6.326418399810791
Epoch 720, val loss: 0.7475174069404602
Epoch 730, training loss: 6.440009117126465 = 0.11373453587293625 + 1.0 * 6.326274394989014
Epoch 730, val loss: 0.7538691759109497
Epoch 740, training loss: 6.44244909286499 = 0.10850547254085541 + 1.0 * 6.333943843841553
Epoch 740, val loss: 0.7601290941238403
Epoch 750, training loss: 6.432138919830322 = 0.10362619906663895 + 1.0 * 6.328512668609619
Epoch 750, val loss: 0.7665449976921082
Epoch 760, training loss: 6.422572612762451 = 0.09903325140476227 + 1.0 * 6.3235392570495605
Epoch 760, val loss: 0.7731956243515015
Epoch 770, training loss: 6.415994644165039 = 0.09468462318181992 + 1.0 * 6.321310043334961
Epoch 770, val loss: 0.7797902822494507
Epoch 780, training loss: 6.422093868255615 = 0.09057874232530594 + 1.0 * 6.331515312194824
Epoch 780, val loss: 0.7865193486213684
Epoch 790, training loss: 6.410552024841309 = 0.08669543266296387 + 1.0 * 6.323856353759766
Epoch 790, val loss: 0.793133556842804
Epoch 800, training loss: 6.404412746429443 = 0.08304458111524582 + 1.0 * 6.321368217468262
Epoch 800, val loss: 0.7999265193939209
Epoch 810, training loss: 6.396495342254639 = 0.07958272099494934 + 1.0 * 6.316912651062012
Epoch 810, val loss: 0.8066101670265198
Epoch 820, training loss: 6.393319606781006 = 0.07630249857902527 + 1.0 * 6.317017078399658
Epoch 820, val loss: 0.8133680820465088
Epoch 830, training loss: 6.393701553344727 = 0.07319337874650955 + 1.0 * 6.320508003234863
Epoch 830, val loss: 0.8200362324714661
Epoch 840, training loss: 6.393492221832275 = 0.07026142627000809 + 1.0 * 6.323230743408203
Epoch 840, val loss: 0.8267292380332947
Epoch 850, training loss: 6.382169246673584 = 0.06748221069574356 + 1.0 * 6.314687252044678
Epoch 850, val loss: 0.8333901166915894
Epoch 860, training loss: 6.377059459686279 = 0.06485085934400558 + 1.0 * 6.312208652496338
Epoch 860, val loss: 0.8401009440422058
Epoch 870, training loss: 6.379408836364746 = 0.06234706565737724 + 1.0 * 6.317061901092529
Epoch 870, val loss: 0.8467467427253723
Epoch 880, training loss: 6.369777202606201 = 0.05996773764491081 + 1.0 * 6.309809684753418
Epoch 880, val loss: 0.8532106876373291
Epoch 890, training loss: 6.372094631195068 = 0.05770944058895111 + 1.0 * 6.314385414123535
Epoch 890, val loss: 0.8598157167434692
Epoch 900, training loss: 6.369510173797607 = 0.05557646602392197 + 1.0 * 6.313933849334717
Epoch 900, val loss: 0.8661074042320251
Epoch 910, training loss: 6.361349105834961 = 0.0535503588616848 + 1.0 * 6.307798862457275
Epoch 910, val loss: 0.8725916147232056
Epoch 920, training loss: 6.361318588256836 = 0.05162285640835762 + 1.0 * 6.309695720672607
Epoch 920, val loss: 0.8790448307991028
Epoch 930, training loss: 6.356020450592041 = 0.04978786036372185 + 1.0 * 6.306232452392578
Epoch 930, val loss: 0.8852466940879822
Epoch 940, training loss: 6.352485179901123 = 0.04804317653179169 + 1.0 * 6.304441928863525
Epoch 940, val loss: 0.8916585445404053
Epoch 950, training loss: 6.351708889007568 = 0.04637669026851654 + 1.0 * 6.305332183837891
Epoch 950, val loss: 0.8979787230491638
Epoch 960, training loss: 6.3510003089904785 = 0.04478554055094719 + 1.0 * 6.306214809417725
Epoch 960, val loss: 0.9041550159454346
Epoch 970, training loss: 6.353405952453613 = 0.04327003285288811 + 1.0 * 6.310135841369629
Epoch 970, val loss: 0.9103707671165466
Epoch 980, training loss: 6.346065998077393 = 0.04182298853993416 + 1.0 * 6.304243087768555
Epoch 980, val loss: 0.9163514971733093
Epoch 990, training loss: 6.341738700866699 = 0.04044506698846817 + 1.0 * 6.301293849945068
Epoch 990, val loss: 0.9225138425827026
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8465998945703743
=== training gcn model ===
Epoch 0, training loss: 10.535442352294922 = 1.9386690855026245 + 1.0 * 8.596773147583008
Epoch 0, val loss: 1.9404218196868896
Epoch 10, training loss: 10.52501392364502 = 1.928775429725647 + 1.0 * 8.596238136291504
Epoch 10, val loss: 1.9307481050491333
Epoch 20, training loss: 10.507906913757324 = 1.9160250425338745 + 1.0 * 8.59188175201416
Epoch 20, val loss: 1.9177601337432861
Epoch 30, training loss: 10.454547882080078 = 1.8985363245010376 + 1.0 * 8.556011199951172
Epoch 30, val loss: 1.8996509313583374
Epoch 40, training loss: 10.188337326049805 = 1.8784369230270386 + 1.0 * 8.309900283813477
Epoch 40, val loss: 1.8800368309020996
Epoch 50, training loss: 9.612658500671387 = 1.8600842952728271 + 1.0 * 7.752574443817139
Epoch 50, val loss: 1.8626288175582886
Epoch 60, training loss: 9.063522338867188 = 1.8458328247070312 + 1.0 * 7.217689514160156
Epoch 60, val loss: 1.8494139909744263
Epoch 70, training loss: 8.77621841430664 = 1.8329282999038696 + 1.0 * 6.943289756774902
Epoch 70, val loss: 1.8372865915298462
Epoch 80, training loss: 8.623924255371094 = 1.8203041553497314 + 1.0 * 6.803619861602783
Epoch 80, val loss: 1.8250457048416138
Epoch 90, training loss: 8.520369529724121 = 1.806892991065979 + 1.0 * 6.713476657867432
Epoch 90, val loss: 1.8121628761291504
Epoch 100, training loss: 8.448509216308594 = 1.7930784225463867 + 1.0 * 6.655430316925049
Epoch 100, val loss: 1.7990566492080688
Epoch 110, training loss: 8.394012451171875 = 1.7788342237472534 + 1.0 * 6.615178108215332
Epoch 110, val loss: 1.7857508659362793
Epoch 120, training loss: 8.345705032348633 = 1.764003872871399 + 1.0 * 6.581700801849365
Epoch 120, val loss: 1.7720956802368164
Epoch 130, training loss: 8.302406311035156 = 1.7480411529541016 + 1.0 * 6.554365634918213
Epoch 130, val loss: 1.7575432062149048
Epoch 140, training loss: 8.26213264465332 = 1.7301214933395386 + 1.0 * 6.53201150894165
Epoch 140, val loss: 1.7415449619293213
Epoch 150, training loss: 8.224159240722656 = 1.709555745124817 + 1.0 * 6.514603614807129
Epoch 150, val loss: 1.7235493659973145
Epoch 160, training loss: 8.182232856750488 = 1.6856606006622314 + 1.0 * 6.496572494506836
Epoch 160, val loss: 1.7029496431350708
Epoch 170, training loss: 8.139528274536133 = 1.6575721502304077 + 1.0 * 6.4819560050964355
Epoch 170, val loss: 1.6789973974227905
Epoch 180, training loss: 8.095036506652832 = 1.6243928670883179 + 1.0 * 6.470643997192383
Epoch 180, val loss: 1.6508780717849731
Epoch 190, training loss: 8.044814109802246 = 1.585936427116394 + 1.0 * 6.458878040313721
Epoch 190, val loss: 1.61830735206604
Epoch 200, training loss: 7.989969730377197 = 1.541629672050476 + 1.0 * 6.448339939117432
Epoch 200, val loss: 1.581024169921875
Epoch 210, training loss: 7.934080123901367 = 1.491278886795044 + 1.0 * 6.442800998687744
Epoch 210, val loss: 1.5389548540115356
Epoch 220, training loss: 7.868954658508301 = 1.4363925457000732 + 1.0 * 6.432562351226807
Epoch 220, val loss: 1.4933542013168335
Epoch 230, training loss: 7.803524971008301 = 1.377634048461914 + 1.0 * 6.425890922546387
Epoch 230, val loss: 1.4448622465133667
Epoch 240, training loss: 7.741913795471191 = 1.3159509897232056 + 1.0 * 6.425962924957275
Epoch 240, val loss: 1.3946681022644043
Epoch 250, training loss: 7.671902656555176 = 1.2550030946731567 + 1.0 * 6.416899681091309
Epoch 250, val loss: 1.3451135158538818
Epoch 260, training loss: 7.604724407196045 = 1.1954164505004883 + 1.0 * 6.409307956695557
Epoch 260, val loss: 1.2970741987228394
Epoch 270, training loss: 7.543290615081787 = 1.1378132104873657 + 1.0 * 6.405477523803711
Epoch 270, val loss: 1.251072883605957
Epoch 280, training loss: 7.485929489135742 = 1.083545446395874 + 1.0 * 6.402383804321289
Epoch 280, val loss: 1.207997441291809
Epoch 290, training loss: 7.428624153137207 = 1.0327966213226318 + 1.0 * 6.395827293395996
Epoch 290, val loss: 1.168176293373108
Epoch 300, training loss: 7.378846645355225 = 0.9849499464035034 + 1.0 * 6.393896579742432
Epoch 300, val loss: 1.1311064958572388
Epoch 310, training loss: 7.33069372177124 = 0.940226137638092 + 1.0 * 6.390467643737793
Epoch 310, val loss: 1.096807599067688
Epoch 320, training loss: 7.282961845397949 = 0.8979976773262024 + 1.0 * 6.3849639892578125
Epoch 320, val loss: 1.0650402307510376
Epoch 330, training loss: 7.23983907699585 = 0.8575387001037598 + 1.0 * 6.38230037689209
Epoch 330, val loss: 1.0351520776748657
Epoch 340, training loss: 7.197054862976074 = 0.8188884854316711 + 1.0 * 6.378166198730469
Epoch 340, val loss: 1.0070668458938599
Epoch 350, training loss: 7.158271789550781 = 0.7818810939788818 + 1.0 * 6.37639045715332
Epoch 350, val loss: 0.981099009513855
Epoch 360, training loss: 7.122838020324707 = 0.7467178702354431 + 1.0 * 6.376120090484619
Epoch 360, val loss: 0.9572734832763672
Epoch 370, training loss: 7.0818586349487305 = 0.7135775089263916 + 1.0 * 6.36828088760376
Epoch 370, val loss: 0.9357941746711731
Epoch 380, training loss: 7.04758882522583 = 0.6820756793022156 + 1.0 * 6.365513324737549
Epoch 380, val loss: 0.9164072871208191
Epoch 390, training loss: 7.020693778991699 = 0.6520295739173889 + 1.0 * 6.368664264678955
Epoch 390, val loss: 0.8989000916481018
Epoch 400, training loss: 6.986316204071045 = 0.6236410140991211 + 1.0 * 6.362675189971924
Epoch 400, val loss: 0.88328617811203
Epoch 410, training loss: 6.954510688781738 = 0.5966200828552246 + 1.0 * 6.357890605926514
Epoch 410, val loss: 0.8695289492607117
Epoch 420, training loss: 6.931358337402344 = 0.5708179473876953 + 1.0 * 6.360540390014648
Epoch 420, val loss: 0.8572730422019958
Epoch 430, training loss: 6.901852130889893 = 0.5461775660514832 + 1.0 * 6.355674743652344
Epoch 430, val loss: 0.8464177846908569
Epoch 440, training loss: 6.87282657623291 = 0.5224831104278564 + 1.0 * 6.350343704223633
Epoch 440, val loss: 0.8368514180183411
Epoch 450, training loss: 6.8485212326049805 = 0.49949756264686584 + 1.0 * 6.349023818969727
Epoch 450, val loss: 0.8283627033233643
Epoch 460, training loss: 6.8319902420043945 = 0.4772965610027313 + 1.0 * 6.35469388961792
Epoch 460, val loss: 0.8207206726074219
Epoch 470, training loss: 6.802603244781494 = 0.45584502816200256 + 1.0 * 6.3467583656311035
Epoch 470, val loss: 0.8140630722045898
Epoch 480, training loss: 6.7775983810424805 = 0.4348350167274475 + 1.0 * 6.342763423919678
Epoch 480, val loss: 0.8081802129745483
Epoch 490, training loss: 6.75848913192749 = 0.41413232684135437 + 1.0 * 6.344357013702393
Epoch 490, val loss: 0.8029603958129883
Epoch 500, training loss: 6.740786552429199 = 0.3938939571380615 + 1.0 * 6.346892833709717
Epoch 500, val loss: 0.7984274625778198
Epoch 510, training loss: 6.7112627029418945 = 0.3740132451057434 + 1.0 * 6.337249279022217
Epoch 510, val loss: 0.7945224642753601
Epoch 520, training loss: 6.691443920135498 = 0.35446053743362427 + 1.0 * 6.3369832038879395
Epoch 520, val loss: 0.791225016117096
Epoch 530, training loss: 6.675786972045898 = 0.33527272939682007 + 1.0 * 6.340514183044434
Epoch 530, val loss: 0.7885721325874329
Epoch 540, training loss: 6.653450965881348 = 0.3166293203830719 + 1.0 * 6.336821556091309
Epoch 540, val loss: 0.7865846753120422
Epoch 550, training loss: 6.631176948547363 = 0.2985313832759857 + 1.0 * 6.332645416259766
Epoch 550, val loss: 0.7852135300636292
Epoch 560, training loss: 6.612100124359131 = 0.2810731530189514 + 1.0 * 6.331027030944824
Epoch 560, val loss: 0.7845256328582764
Epoch 570, training loss: 6.5933403968811035 = 0.2642627954483032 + 1.0 * 6.32907772064209
Epoch 570, val loss: 0.7845988273620605
Epoch 580, training loss: 6.581470966339111 = 0.24822106957435608 + 1.0 * 6.333250045776367
Epoch 580, val loss: 0.7854793667793274
Epoch 590, training loss: 6.562665939331055 = 0.2330561727285385 + 1.0 * 6.3296098709106445
Epoch 590, val loss: 0.7870477437973022
Epoch 600, training loss: 6.5456366539001465 = 0.21885520219802856 + 1.0 * 6.326781272888184
Epoch 600, val loss: 0.7893030047416687
Epoch 610, training loss: 6.528674125671387 = 0.20551174879074097 + 1.0 * 6.32316255569458
Epoch 610, val loss: 0.7922561168670654
Epoch 620, training loss: 6.514873504638672 = 0.1929721236228943 + 1.0 * 6.321901321411133
Epoch 620, val loss: 0.7959122657775879
Epoch 630, training loss: 6.504332542419434 = 0.18122029304504395 + 1.0 * 6.3231120109558105
Epoch 630, val loss: 0.8001621961593628
Epoch 640, training loss: 6.497792720794678 = 0.17029400169849396 + 1.0 * 6.327498912811279
Epoch 640, val loss: 0.8048985600471497
Epoch 650, training loss: 6.4782609939575195 = 0.1601712703704834 + 1.0 * 6.318089485168457
Epoch 650, val loss: 0.8099907040596008
Epoch 660, training loss: 6.470223903656006 = 0.1507338136434555 + 1.0 * 6.3194899559021
Epoch 660, val loss: 0.8155458569526672
Epoch 670, training loss: 6.458192825317383 = 0.14196506142616272 + 1.0 * 6.316227912902832
Epoch 670, val loss: 0.8214705586433411
Epoch 680, training loss: 6.4514875411987305 = 0.13379456102848053 + 1.0 * 6.317692756652832
Epoch 680, val loss: 0.8277413845062256
Epoch 690, training loss: 6.441274166107178 = 0.1262132227420807 + 1.0 * 6.315061092376709
Epoch 690, val loss: 0.8342453241348267
Epoch 700, training loss: 6.43257999420166 = 0.11920475959777832 + 1.0 * 6.313375473022461
Epoch 700, val loss: 0.8408874869346619
Epoch 710, training loss: 6.423084735870361 = 0.11266157776117325 + 1.0 * 6.310423374176025
Epoch 710, val loss: 0.8478311896324158
Epoch 720, training loss: 6.415762901306152 = 0.10655193030834198 + 1.0 * 6.309210777282715
Epoch 720, val loss: 0.8550363183021545
Epoch 730, training loss: 6.425595760345459 = 0.10085345059633255 + 1.0 * 6.324742317199707
Epoch 730, val loss: 0.862436830997467
Epoch 740, training loss: 6.405780792236328 = 0.0956108421087265 + 1.0 * 6.3101701736450195
Epoch 740, val loss: 0.869756817817688
Epoch 750, training loss: 6.396904468536377 = 0.09072665125131607 + 1.0 * 6.306177616119385
Epoch 750, val loss: 0.8771288394927979
Epoch 760, training loss: 6.3926897048950195 = 0.08615878969430923 + 1.0 * 6.306530952453613
Epoch 760, val loss: 0.8846691846847534
Epoch 770, training loss: 6.389242649078369 = 0.0818869024515152 + 1.0 * 6.307355880737305
Epoch 770, val loss: 0.8923041224479675
Epoch 780, training loss: 6.382711887359619 = 0.07790455222129822 + 1.0 * 6.304807186126709
Epoch 780, val loss: 0.8999036550521851
Epoch 790, training loss: 6.3801422119140625 = 0.0741795003414154 + 1.0 * 6.305962562561035
Epoch 790, val loss: 0.907521665096283
Epoch 800, training loss: 6.374179363250732 = 0.07070054858922958 + 1.0 * 6.303478717803955
Epoch 800, val loss: 0.9151925444602966
Epoch 810, training loss: 6.3704609870910645 = 0.06743936240673065 + 1.0 * 6.303021430969238
Epoch 810, val loss: 0.9228535890579224
Epoch 820, training loss: 6.367791175842285 = 0.06437257677316666 + 1.0 * 6.3034186363220215
Epoch 820, val loss: 0.9305382370948792
Epoch 830, training loss: 6.363467216491699 = 0.06150774285197258 + 1.0 * 6.30195951461792
Epoch 830, val loss: 0.9381908178329468
Epoch 840, training loss: 6.358901023864746 = 0.05881006643176079 + 1.0 * 6.300090789794922
Epoch 840, val loss: 0.945785403251648
Epoch 850, training loss: 6.361983299255371 = 0.05627435818314552 + 1.0 * 6.305708885192871
Epoch 850, val loss: 0.9533407688140869
Epoch 860, training loss: 6.352802276611328 = 0.05389061197638512 + 1.0 * 6.2989115715026855
Epoch 860, val loss: 0.9608758091926575
Epoch 870, training loss: 6.347636699676514 = 0.051641929894685745 + 1.0 * 6.295994758605957
Epoch 870, val loss: 0.9683310985565186
Epoch 880, training loss: 6.3503875732421875 = 0.04951324686408043 + 1.0 * 6.30087423324585
Epoch 880, val loss: 0.9758421778678894
Epoch 890, training loss: 6.342640399932861 = 0.047518447041511536 + 1.0 * 6.295122146606445
Epoch 890, val loss: 0.9832660555839539
Epoch 900, training loss: 6.343550682067871 = 0.045627955347299576 + 1.0 * 6.297922611236572
Epoch 900, val loss: 0.9905257821083069
Epoch 910, training loss: 6.337296485900879 = 0.04385063424706459 + 1.0 * 6.293446063995361
Epoch 910, val loss: 0.9977787733078003
Epoch 920, training loss: 6.333912372589111 = 0.0421658530831337 + 1.0 * 6.291746616363525
Epoch 920, val loss: 1.0048773288726807
Epoch 930, training loss: 6.333827018737793 = 0.040571246296167374 + 1.0 * 6.293255805969238
Epoch 930, val loss: 1.0120289325714111
Epoch 940, training loss: 6.3294453620910645 = 0.03906171768903732 + 1.0 * 6.290383815765381
Epoch 940, val loss: 1.0191134214401245
Epoch 950, training loss: 6.331241607666016 = 0.037626318633556366 + 1.0 * 6.293615341186523
Epoch 950, val loss: 1.0261825323104858
Epoch 960, training loss: 6.3290205001831055 = 0.036269303411245346 + 1.0 * 6.292751312255859
Epoch 960, val loss: 1.0331480503082275
Epoch 970, training loss: 6.324987888336182 = 0.03498505800962448 + 1.0 * 6.290002822875977
Epoch 970, val loss: 1.0400131940841675
Epoch 980, training loss: 6.32261323928833 = 0.03377106785774231 + 1.0 * 6.28884220123291
Epoch 980, val loss: 1.0465999841690063
Epoch 990, training loss: 6.319209098815918 = 0.032611533999443054 + 1.0 * 6.286597728729248
Epoch 990, val loss: 1.053320288658142
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.83078545071165
The final CL Acc:0.80000, 0.01600, The final GNN Acc:0.83711, 0.00683
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9414])
updated graph: torch.Size([2, 10476])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.551680564880371 = 1.9548308849334717 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.952131986618042
Epoch 10, training loss: 10.541375160217285 = 1.944756269454956 + 1.0 * 8.59661865234375
Epoch 10, val loss: 1.9422862529754639
Epoch 20, training loss: 10.527226448059082 = 1.932463526725769 + 1.0 * 8.594762802124023
Epoch 20, val loss: 1.9298330545425415
Epoch 30, training loss: 10.4951753616333 = 1.9156643152236938 + 1.0 * 8.579510688781738
Epoch 30, val loss: 1.9124177694320679
Epoch 40, training loss: 10.373300552368164 = 1.893460988998413 + 1.0 * 8.479839324951172
Epoch 40, val loss: 1.8900773525238037
Epoch 50, training loss: 10.020069122314453 = 1.8700535297393799 + 1.0 * 8.150015830993652
Epoch 50, val loss: 1.8678518533706665
Epoch 60, training loss: 9.575148582458496 = 1.851400375366211 + 1.0 * 7.723748207092285
Epoch 60, val loss: 1.8509453535079956
Epoch 70, training loss: 9.143717765808105 = 1.8377479314804077 + 1.0 * 7.305969715118408
Epoch 70, val loss: 1.8387959003448486
Epoch 80, training loss: 8.87059211730957 = 1.825380563735962 + 1.0 * 7.0452117919921875
Epoch 80, val loss: 1.8275103569030762
Epoch 90, training loss: 8.73101806640625 = 1.8090553283691406 + 1.0 * 6.921962738037109
Epoch 90, val loss: 1.813156247138977
Epoch 100, training loss: 8.624991416931152 = 1.792474389076233 + 1.0 * 6.832517147064209
Epoch 100, val loss: 1.7994372844696045
Epoch 110, training loss: 8.551751136779785 = 1.7770037651062012 + 1.0 * 6.774747371673584
Epoch 110, val loss: 1.7869513034820557
Epoch 120, training loss: 8.491403579711914 = 1.761427879333496 + 1.0 * 6.729976177215576
Epoch 120, val loss: 1.7744568586349487
Epoch 130, training loss: 8.439664840698242 = 1.7445836067199707 + 1.0 * 6.695080757141113
Epoch 130, val loss: 1.760624647140503
Epoch 140, training loss: 8.39021110534668 = 1.7257542610168457 + 1.0 * 6.664456367492676
Epoch 140, val loss: 1.7451343536376953
Epoch 150, training loss: 8.339555740356445 = 1.7044111490249634 + 1.0 * 6.635144233703613
Epoch 150, val loss: 1.7275786399841309
Epoch 160, training loss: 8.290201187133789 = 1.6796984672546387 + 1.0 * 6.610502243041992
Epoch 160, val loss: 1.7073118686676025
Epoch 170, training loss: 8.246057510375977 = 1.6507889032363892 + 1.0 * 6.595268726348877
Epoch 170, val loss: 1.683698296546936
Epoch 180, training loss: 8.189826011657715 = 1.617616891860962 + 1.0 * 6.572209358215332
Epoch 180, val loss: 1.6566627025604248
Epoch 190, training loss: 8.135926246643066 = 1.579643964767456 + 1.0 * 6.556282043457031
Epoch 190, val loss: 1.6256729364395142
Epoch 200, training loss: 8.084480285644531 = 1.536853551864624 + 1.0 * 6.547626495361328
Epoch 200, val loss: 1.5909819602966309
Epoch 210, training loss: 8.020293235778809 = 1.490302324295044 + 1.0 * 6.5299906730651855
Epoch 210, val loss: 1.5535629987716675
Epoch 220, training loss: 7.958800315856934 = 1.4402581453323364 + 1.0 * 6.518542289733887
Epoch 220, val loss: 1.5138293504714966
Epoch 230, training loss: 7.897948741912842 = 1.3880246877670288 + 1.0 * 6.509923934936523
Epoch 230, val loss: 1.4728889465332031
Epoch 240, training loss: 7.8364152908325195 = 1.335366129875183 + 1.0 * 6.501049041748047
Epoch 240, val loss: 1.4323859214782715
Epoch 250, training loss: 7.772860050201416 = 1.28261137008667 + 1.0 * 6.490248680114746
Epoch 250, val loss: 1.3929049968719482
Epoch 260, training loss: 7.713809967041016 = 1.2302652597427368 + 1.0 * 6.483544826507568
Epoch 260, val loss: 1.3548798561096191
Epoch 270, training loss: 7.658791542053223 = 1.1791765689849854 + 1.0 * 6.479614734649658
Epoch 270, val loss: 1.3190522193908691
Epoch 280, training loss: 7.600251197814941 = 1.1292474269866943 + 1.0 * 6.471003532409668
Epoch 280, val loss: 1.2849440574645996
Epoch 290, training loss: 7.546046733856201 = 1.0805234909057617 + 1.0 * 6.4655232429504395
Epoch 290, val loss: 1.25291907787323
Epoch 300, training loss: 7.4916558265686035 = 1.0332170724868774 + 1.0 * 6.458438873291016
Epoch 300, val loss: 1.222503423690796
Epoch 310, training loss: 7.441258907318115 = 0.9873732924461365 + 1.0 * 6.453885555267334
Epoch 310, val loss: 1.1940476894378662
Epoch 320, training loss: 7.3909687995910645 = 0.9433978199958801 + 1.0 * 6.44757080078125
Epoch 320, val loss: 1.1674127578735352
Epoch 330, training loss: 7.3457183837890625 = 0.9015672206878662 + 1.0 * 6.444151401519775
Epoch 330, val loss: 1.1428567171096802
Epoch 340, training loss: 7.29901647567749 = 0.8620015382766724 + 1.0 * 6.437015056610107
Epoch 340, val loss: 1.1205697059631348
Epoch 350, training loss: 7.2581562995910645 = 0.8243750333786011 + 1.0 * 6.433781147003174
Epoch 350, val loss: 1.1000641584396362
Epoch 360, training loss: 7.218601703643799 = 0.7887968420982361 + 1.0 * 6.429804801940918
Epoch 360, val loss: 1.0815373659133911
Epoch 370, training loss: 7.181576251983643 = 0.7553433179855347 + 1.0 * 6.426232814788818
Epoch 370, val loss: 1.0649551153182983
Epoch 380, training loss: 7.144948959350586 = 0.7238372564315796 + 1.0 * 6.421111583709717
Epoch 380, val loss: 1.0503661632537842
Epoch 390, training loss: 7.112264156341553 = 0.69391268491745 + 1.0 * 6.418351650238037
Epoch 390, val loss: 1.037354826927185
Epoch 400, training loss: 7.077812671661377 = 0.6652960181236267 + 1.0 * 6.4125165939331055
Epoch 400, val loss: 1.0258591175079346
Epoch 410, training loss: 7.052801609039307 = 0.6377323865890503 + 1.0 * 6.415069103240967
Epoch 410, val loss: 1.0157051086425781
Epoch 420, training loss: 7.0244221687316895 = 0.6112540364265442 + 1.0 * 6.413167953491211
Epoch 420, val loss: 1.006715178489685
Epoch 430, training loss: 6.991112232208252 = 0.585845947265625 + 1.0 * 6.405266284942627
Epoch 430, val loss: 0.9989275932312012
Epoch 440, training loss: 6.967314720153809 = 0.5610899925231934 + 1.0 * 6.406224727630615
Epoch 440, val loss: 0.992082953453064
Epoch 450, training loss: 6.939248085021973 = 0.5369873642921448 + 1.0 * 6.402260780334473
Epoch 450, val loss: 0.9861454963684082
Epoch 460, training loss: 6.9086480140686035 = 0.5133005976676941 + 1.0 * 6.395347595214844
Epoch 460, val loss: 0.9810370802879333
Epoch 470, training loss: 6.889898300170898 = 0.4898921847343445 + 1.0 * 6.400006294250488
Epoch 470, val loss: 0.9764332175254822
Epoch 480, training loss: 6.860435485839844 = 0.46689069271087646 + 1.0 * 6.393544673919678
Epoch 480, val loss: 0.9727570414543152
Epoch 490, training loss: 6.837986946105957 = 0.44414767622947693 + 1.0 * 6.393839359283447
Epoch 490, val loss: 0.9698461294174194
Epoch 500, training loss: 6.809710502624512 = 0.42187952995300293 + 1.0 * 6.38783073425293
Epoch 500, val loss: 0.9677233695983887
Epoch 510, training loss: 6.783565998077393 = 0.399951308965683 + 1.0 * 6.383614540100098
Epoch 510, val loss: 0.966680109500885
Epoch 520, training loss: 6.772151947021484 = 0.3784946799278259 + 1.0 * 6.393657207489014
Epoch 520, val loss: 0.9665499329566956
Epoch 530, training loss: 6.7379655838012695 = 0.3577711582183838 + 1.0 * 6.380194187164307
Epoch 530, val loss: 0.967473566532135
Epoch 540, training loss: 6.717349529266357 = 0.33782026171684265 + 1.0 * 6.3795294761657715
Epoch 540, val loss: 0.9698227643966675
Epoch 550, training loss: 6.706241607666016 = 0.3186360001564026 + 1.0 * 6.387605667114258
Epoch 550, val loss: 0.9729124903678894
Epoch 560, training loss: 6.677657127380371 = 0.3004067540168762 + 1.0 * 6.3772501945495605
Epoch 560, val loss: 0.9773790240287781
Epoch 570, training loss: 6.659236907958984 = 0.28309565782546997 + 1.0 * 6.37614107131958
Epoch 570, val loss: 0.9830182194709778
Epoch 580, training loss: 6.637662410736084 = 0.2667086720466614 + 1.0 * 6.370953559875488
Epoch 580, val loss: 0.989447832107544
Epoch 590, training loss: 6.626242637634277 = 0.25122976303100586 + 1.0 * 6.3750128746032715
Epoch 590, val loss: 0.9969285726547241
Epoch 600, training loss: 6.61481237411499 = 0.2367546409368515 + 1.0 * 6.378057956695557
Epoch 600, val loss: 1.0051729679107666
Epoch 610, training loss: 6.59151029586792 = 0.2232457399368286 + 1.0 * 6.368264675140381
Epoch 610, val loss: 1.0143764019012451
Epoch 620, training loss: 6.574146270751953 = 0.2105570286512375 + 1.0 * 6.363589286804199
Epoch 620, val loss: 1.0241835117340088
Epoch 630, training loss: 6.561614990234375 = 0.1986100971698761 + 1.0 * 6.363004684448242
Epoch 630, val loss: 1.034685492515564
Epoch 640, training loss: 6.5529022216796875 = 0.18739387392997742 + 1.0 * 6.365508556365967
Epoch 640, val loss: 1.0455459356307983
Epoch 650, training loss: 6.544015884399414 = 0.1769365668296814 + 1.0 * 6.367079257965088
Epoch 650, val loss: 1.057195782661438
Epoch 660, training loss: 6.5282979011535645 = 0.16718648374080658 + 1.0 * 6.361111640930176
Epoch 660, val loss: 1.0690510272979736
Epoch 670, training loss: 6.514496326446533 = 0.15803760290145874 + 1.0 * 6.35645866394043
Epoch 670, val loss: 1.081389307975769
Epoch 680, training loss: 6.505724906921387 = 0.1494370400905609 + 1.0 * 6.356287956237793
Epoch 680, val loss: 1.0940312147140503
Epoch 690, training loss: 6.495872497558594 = 0.14138318598270416 + 1.0 * 6.354489326477051
Epoch 690, val loss: 1.1067482233047485
Epoch 700, training loss: 6.487688064575195 = 0.1338466852903366 + 1.0 * 6.353841304779053
Epoch 700, val loss: 1.1200230121612549
Epoch 710, training loss: 6.478087425231934 = 0.126755490899086 + 1.0 * 6.35133171081543
Epoch 710, val loss: 1.1333776712417603
Epoch 720, training loss: 6.4796881675720215 = 0.12008801102638245 + 1.0 * 6.359600067138672
Epoch 720, val loss: 1.1467126607894897
Epoch 730, training loss: 6.4647040367126465 = 0.11383330076932907 + 1.0 * 6.350870609283447
Epoch 730, val loss: 1.160257339477539
Epoch 740, training loss: 6.4564948081970215 = 0.10795664042234421 + 1.0 * 6.348538398742676
Epoch 740, val loss: 1.174008846282959
Epoch 750, training loss: 6.451836109161377 = 0.10243039578199387 + 1.0 * 6.349405765533447
Epoch 750, val loss: 1.1874973773956299
Epoch 760, training loss: 6.443077564239502 = 0.09724482893943787 + 1.0 * 6.345832824707031
Epoch 760, val loss: 1.201265573501587
Epoch 770, training loss: 6.437719821929932 = 0.09238959848880768 + 1.0 * 6.345330238342285
Epoch 770, val loss: 1.2146925926208496
Epoch 780, training loss: 6.431386470794678 = 0.08784787356853485 + 1.0 * 6.343538761138916
Epoch 780, val loss: 1.2282155752182007
Epoch 790, training loss: 6.424776077270508 = 0.08358225971460342 + 1.0 * 6.341193675994873
Epoch 790, val loss: 1.2418694496154785
Epoch 800, training loss: 6.422400951385498 = 0.07955817133188248 + 1.0 * 6.3428425788879395
Epoch 800, val loss: 1.2552330493927002
Epoch 810, training loss: 6.414036750793457 = 0.07578454166650772 + 1.0 * 6.338252067565918
Epoch 810, val loss: 1.2685264348983765
Epoch 820, training loss: 6.4100422859191895 = 0.0722474455833435 + 1.0 * 6.337794780731201
Epoch 820, val loss: 1.2819921970367432
Epoch 830, training loss: 6.409852981567383 = 0.06892243772745132 + 1.0 * 6.340930461883545
Epoch 830, val loss: 1.2950356006622314
Epoch 840, training loss: 6.406832695007324 = 0.06580209732055664 + 1.0 * 6.341030597686768
Epoch 840, val loss: 1.3080161809921265
Epoch 850, training loss: 6.397451400756836 = 0.06289003044366837 + 1.0 * 6.334561347961426
Epoch 850, val loss: 1.3212260007858276
Epoch 860, training loss: 6.3934149742126465 = 0.060142066329717636 + 1.0 * 6.333272933959961
Epoch 860, val loss: 1.3338066339492798
Epoch 870, training loss: 6.401453018188477 = 0.057553209364414215 + 1.0 * 6.343899726867676
Epoch 870, val loss: 1.346644401550293
Epoch 880, training loss: 6.391178607940674 = 0.05512017011642456 + 1.0 * 6.336058616638184
Epoch 880, val loss: 1.3592277765274048
Epoch 890, training loss: 6.38334846496582 = 0.052833810448646545 + 1.0 * 6.330514430999756
Epoch 890, val loss: 1.3716703653335571
Epoch 900, training loss: 6.381584167480469 = 0.05067378282546997 + 1.0 * 6.3309102058410645
Epoch 900, val loss: 1.3840341567993164
Epoch 910, training loss: 6.377763748168945 = 0.04863632470369339 + 1.0 * 6.329127311706543
Epoch 910, val loss: 1.3960108757019043
Epoch 920, training loss: 6.373723030090332 = 0.046716075390577316 + 1.0 * 6.327006816864014
Epoch 920, val loss: 1.4082521200180054
Epoch 930, training loss: 6.371687412261963 = 0.04490337148308754 + 1.0 * 6.326784133911133
Epoch 930, val loss: 1.4200726747512817
Epoch 940, training loss: 6.3776984214782715 = 0.04318627715110779 + 1.0 * 6.334512233734131
Epoch 940, val loss: 1.4317141771316528
Epoch 950, training loss: 6.367610931396484 = 0.04156770184636116 + 1.0 * 6.326043128967285
Epoch 950, val loss: 1.4434329271316528
Epoch 960, training loss: 6.364936351776123 = 0.04003654420375824 + 1.0 * 6.324899673461914
Epoch 960, val loss: 1.454970359802246
Epoch 970, training loss: 6.363806247711182 = 0.038583602756261826 + 1.0 * 6.325222492218018
Epoch 970, val loss: 1.4660097360610962
Epoch 980, training loss: 6.360126972198486 = 0.037211064249277115 + 1.0 * 6.322916030883789
Epoch 980, val loss: 1.4775413274765015
Epoch 990, training loss: 6.357086658477783 = 0.0359070710837841 + 1.0 * 6.321179389953613
Epoch 990, val loss: 1.4884777069091797
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 10.560493469238281 = 1.9636523723602295 + 1.0 * 8.596840858459473
Epoch 0, val loss: 1.9565938711166382
Epoch 10, training loss: 10.54985237121582 = 1.953235149383545 + 1.0 * 8.596617698669434
Epoch 10, val loss: 1.9469233751296997
Epoch 20, training loss: 10.535591125488281 = 1.9406030178070068 + 1.0 * 8.594987869262695
Epoch 20, val loss: 1.935012936592102
Epoch 30, training loss: 10.506126403808594 = 1.9230923652648926 + 1.0 * 8.583033561706543
Epoch 30, val loss: 1.9184163808822632
Epoch 40, training loss: 10.417083740234375 = 1.8985810279846191 + 1.0 * 8.518503189086914
Epoch 40, val loss: 1.8958972692489624
Epoch 50, training loss: 10.10460090637207 = 1.871058702468872 + 1.0 * 8.233542442321777
Epoch 50, val loss: 1.8714344501495361
Epoch 60, training loss: 9.70823860168457 = 1.8444716930389404 + 1.0 * 7.863766670227051
Epoch 60, val loss: 1.849194049835205
Epoch 70, training loss: 9.220288276672363 = 1.8258311748504639 + 1.0 * 7.3944573402404785
Epoch 70, val loss: 1.8333455324172974
Epoch 80, training loss: 8.969061851501465 = 1.8104215860366821 + 1.0 * 7.158639907836914
Epoch 80, val loss: 1.8201414346694946
Epoch 90, training loss: 8.814569473266602 = 1.7926145792007446 + 1.0 * 7.0219550132751465
Epoch 90, val loss: 1.8054927587509155
Epoch 100, training loss: 8.713756561279297 = 1.7733670473098755 + 1.0 * 6.940389156341553
Epoch 100, val loss: 1.7900227308273315
Epoch 110, training loss: 8.645666122436523 = 1.754263162612915 + 1.0 * 6.8914031982421875
Epoch 110, val loss: 1.7733558416366577
Epoch 120, training loss: 8.584359169006348 = 1.735026478767395 + 1.0 * 6.849332332611084
Epoch 120, val loss: 1.755045771598816
Epoch 130, training loss: 8.523494720458984 = 1.7146825790405273 + 1.0 * 6.808811664581299
Epoch 130, val loss: 1.7354519367218018
Epoch 140, training loss: 8.459966659545898 = 1.6921535730361938 + 1.0 * 6.767813205718994
Epoch 140, val loss: 1.7151097059249878
Epoch 150, training loss: 8.394221305847168 = 1.666267991065979 + 1.0 * 6.72795295715332
Epoch 150, val loss: 1.6930880546569824
Epoch 160, training loss: 8.325678825378418 = 1.6370567083358765 + 1.0 * 6.688621997833252
Epoch 160, val loss: 1.66877281665802
Epoch 170, training loss: 8.258439064025879 = 1.6042276620864868 + 1.0 * 6.654211044311523
Epoch 170, val loss: 1.6415307521820068
Epoch 180, training loss: 8.193345069885254 = 1.5678398609161377 + 1.0 * 6.625504970550537
Epoch 180, val loss: 1.6112616062164307
Epoch 190, training loss: 8.128549575805664 = 1.5283796787261963 + 1.0 * 6.600170135498047
Epoch 190, val loss: 1.5788297653198242
Epoch 200, training loss: 8.065231323242188 = 1.4861903190612793 + 1.0 * 6.579041481018066
Epoch 200, val loss: 1.5447593927383423
Epoch 210, training loss: 8.00411319732666 = 1.4425346851348877 + 1.0 * 6.561578750610352
Epoch 210, val loss: 1.5100449323654175
Epoch 220, training loss: 7.946548938751221 = 1.3986068964004517 + 1.0 * 6.547942161560059
Epoch 220, val loss: 1.4759081602096558
Epoch 230, training loss: 7.889481544494629 = 1.3548303842544556 + 1.0 * 6.534651279449463
Epoch 230, val loss: 1.4423359632492065
Epoch 240, training loss: 7.834184646606445 = 1.3109499216079712 + 1.0 * 6.523234844207764
Epoch 240, val loss: 1.4091417789459229
Epoch 250, training loss: 7.782774925231934 = 1.26711905002594 + 1.0 * 6.515655994415283
Epoch 250, val loss: 1.3768190145492554
Epoch 260, training loss: 7.730323314666748 = 1.223939061164856 + 1.0 * 6.506384372711182
Epoch 260, val loss: 1.3457260131835938
Epoch 270, training loss: 7.679733753204346 = 1.1813937425613403 + 1.0 * 6.498340129852295
Epoch 270, val loss: 1.3157602548599243
Epoch 280, training loss: 7.630414009094238 = 1.1396358013153076 + 1.0 * 6.490777969360352
Epoch 280, val loss: 1.2866785526275635
Epoch 290, training loss: 7.582642555236816 = 1.0984711647033691 + 1.0 * 6.484171390533447
Epoch 290, val loss: 1.2585996389389038
Epoch 300, training loss: 7.542995452880859 = 1.058489203453064 + 1.0 * 6.484506130218506
Epoch 300, val loss: 1.2319220304489136
Epoch 310, training loss: 7.49198055267334 = 1.0199534893035889 + 1.0 * 6.47202730178833
Epoch 310, val loss: 1.206708312034607
Epoch 320, training loss: 7.448570251464844 = 0.9824162125587463 + 1.0 * 6.466154098510742
Epoch 320, val loss: 1.1828629970550537
Epoch 330, training loss: 7.407268524169922 = 0.9456129670143127 + 1.0 * 6.461655616760254
Epoch 330, val loss: 1.1597003936767578
Epoch 340, training loss: 7.371280193328857 = 0.9095697999000549 + 1.0 * 6.461710453033447
Epoch 340, val loss: 1.1374746561050415
Epoch 350, training loss: 7.326241970062256 = 0.8745266199111938 + 1.0 * 6.451715469360352
Epoch 350, val loss: 1.1164649724960327
Epoch 360, training loss: 7.28729772567749 = 0.839859127998352 + 1.0 * 6.447438716888428
Epoch 360, val loss: 1.0958770513534546
Epoch 370, training loss: 7.255213260650635 = 0.8053106665611267 + 1.0 * 6.449902534484863
Epoch 370, val loss: 1.0755780935287476
Epoch 380, training loss: 7.213253974914551 = 0.7712428569793701 + 1.0 * 6.44201135635376
Epoch 380, val loss: 1.0558664798736572
Epoch 390, training loss: 7.174900531768799 = 0.7376875877380371 + 1.0 * 6.437212944030762
Epoch 390, val loss: 1.0369130373001099
Epoch 400, training loss: 7.137528419494629 = 0.7043917775154114 + 1.0 * 6.433136463165283
Epoch 400, val loss: 1.0180989503860474
Epoch 410, training loss: 7.101219177246094 = 0.6713459491729736 + 1.0 * 6.429873466491699
Epoch 410, val loss: 0.9997521042823792
Epoch 420, training loss: 7.070925235748291 = 0.6387379765510559 + 1.0 * 6.432187080383301
Epoch 420, val loss: 0.9821667671203613
Epoch 430, training loss: 7.031860828399658 = 0.6069198846817017 + 1.0 * 6.424941062927246
Epoch 430, val loss: 0.9651620388031006
Epoch 440, training loss: 6.997764587402344 = 0.5760877132415771 + 1.0 * 6.4216766357421875
Epoch 440, val loss: 0.9496331214904785
Epoch 450, training loss: 6.96729850769043 = 0.5460913777351379 + 1.0 * 6.421206951141357
Epoch 450, val loss: 0.9351821541786194
Epoch 460, training loss: 6.9369096755981445 = 0.5170607566833496 + 1.0 * 6.419848918914795
Epoch 460, val loss: 0.9218332171440125
Epoch 470, training loss: 6.903461456298828 = 0.48893728852272034 + 1.0 * 6.414524078369141
Epoch 470, val loss: 0.910020649433136
Epoch 480, training loss: 6.878591060638428 = 0.4615617096424103 + 1.0 * 6.41702938079834
Epoch 480, val loss: 0.899046778678894
Epoch 490, training loss: 6.845901012420654 = 0.4349610507488251 + 1.0 * 6.410940170288086
Epoch 490, val loss: 0.8894150853157043
Epoch 500, training loss: 6.816314697265625 = 0.4088733494281769 + 1.0 * 6.407441139221191
Epoch 500, val loss: 0.8805433511734009
Epoch 510, training loss: 6.792728900909424 = 0.383344441652298 + 1.0 * 6.409384250640869
Epoch 510, val loss: 0.87222820520401
Epoch 520, training loss: 6.761842727661133 = 0.3584982752799988 + 1.0 * 6.403344631195068
Epoch 520, val loss: 0.8646479249000549
Epoch 530, training loss: 6.735487461090088 = 0.334340363740921 + 1.0 * 6.40114688873291
Epoch 530, val loss: 0.8575709462165833
Epoch 540, training loss: 6.713856220245361 = 0.311018168926239 + 1.0 * 6.402838230133057
Epoch 540, val loss: 0.851045548915863
Epoch 550, training loss: 6.687726020812988 = 0.2887391746044159 + 1.0 * 6.39898681640625
Epoch 550, val loss: 0.8453574180603027
Epoch 560, training loss: 6.6633806228637695 = 0.26754987239837646 + 1.0 * 6.3958306312561035
Epoch 560, val loss: 0.8404785990715027
Epoch 570, training loss: 6.645955562591553 = 0.24756860733032227 + 1.0 * 6.3983869552612305
Epoch 570, val loss: 0.8362988829612732
Epoch 580, training loss: 6.62852144241333 = 0.22900983691215515 + 1.0 * 6.399511814117432
Epoch 580, val loss: 0.8331809639930725
Epoch 590, training loss: 6.601614475250244 = 0.2118796408176422 + 1.0 * 6.389734745025635
Epoch 590, val loss: 0.8311376571655273
Epoch 600, training loss: 6.583917617797852 = 0.19608847796916962 + 1.0 * 6.387829303741455
Epoch 600, val loss: 0.8298093676567078
Epoch 610, training loss: 6.566816806793213 = 0.18156367540359497 + 1.0 * 6.385252952575684
Epoch 610, val loss: 0.8294922113418579
Epoch 620, training loss: 6.561734199523926 = 0.1682569682598114 + 1.0 * 6.393477439880371
Epoch 620, val loss: 0.830028235912323
Epoch 630, training loss: 6.540311336517334 = 0.15617313981056213 + 1.0 * 6.384138107299805
Epoch 630, val loss: 0.8315175175666809
Epoch 640, training loss: 6.527909278869629 = 0.14519810676574707 + 1.0 * 6.382711410522461
Epoch 640, val loss: 0.8339297771453857
Epoch 650, training loss: 6.514631748199463 = 0.13519743084907532 + 1.0 * 6.379434108734131
Epoch 650, val loss: 0.8369197249412537
Epoch 660, training loss: 6.503331184387207 = 0.12603989243507385 + 1.0 * 6.377291202545166
Epoch 660, val loss: 0.8406878709793091
Epoch 670, training loss: 6.499093532562256 = 0.11763022840023041 + 1.0 * 6.381463527679443
Epoch 670, val loss: 0.8447285294532776
Epoch 680, training loss: 6.487153053283691 = 0.10993082076311111 + 1.0 * 6.377222061157227
Epoch 680, val loss: 0.8492993116378784
Epoch 690, training loss: 6.481567859649658 = 0.10290998965501785 + 1.0 * 6.378657817840576
Epoch 690, val loss: 0.8544133901596069
Epoch 700, training loss: 6.468848705291748 = 0.09645729511976242 + 1.0 * 6.372391223907471
Epoch 700, val loss: 0.8595678210258484
Epoch 710, training loss: 6.459352493286133 = 0.09053253382444382 + 1.0 * 6.3688201904296875
Epoch 710, val loss: 0.8651564121246338
Epoch 720, training loss: 6.452085018157959 = 0.08506365120410919 + 1.0 * 6.367021560668945
Epoch 720, val loss: 0.8709240555763245
Epoch 730, training loss: 6.457631587982178 = 0.0800221636891365 + 1.0 * 6.3776092529296875
Epoch 730, val loss: 0.8765090703964233
Epoch 740, training loss: 6.445271015167236 = 0.07541415095329285 + 1.0 * 6.369856834411621
Epoch 740, val loss: 0.8826392292976379
Epoch 750, training loss: 6.435492515563965 = 0.0711810365319252 + 1.0 * 6.364311695098877
Epoch 750, val loss: 0.8889589905738831
Epoch 760, training loss: 6.428520202636719 = 0.06727194786071777 + 1.0 * 6.361248016357422
Epoch 760, val loss: 0.8950954675674438
Epoch 770, training loss: 6.42389440536499 = 0.06365131586790085 + 1.0 * 6.360243320465088
Epoch 770, val loss: 0.9012895822525024
Epoch 780, training loss: 6.4262542724609375 = 0.06029947102069855 + 1.0 * 6.365954875946045
Epoch 780, val loss: 0.9073195457458496
Epoch 790, training loss: 6.415506839752197 = 0.05720534175634384 + 1.0 * 6.358301639556885
Epoch 790, val loss: 0.9133893847465515
Epoch 800, training loss: 6.414219856262207 = 0.05434727668762207 + 1.0 * 6.359872341156006
Epoch 800, val loss: 0.919533371925354
Epoch 810, training loss: 6.406992435455322 = 0.05170271545648575 + 1.0 * 6.355289936065674
Epoch 810, val loss: 0.9258267283439636
Epoch 820, training loss: 6.404975414276123 = 0.04923730716109276 + 1.0 * 6.355738162994385
Epoch 820, val loss: 0.9319058656692505
Epoch 830, training loss: 6.399232864379883 = 0.04694654047489166 + 1.0 * 6.352286338806152
Epoch 830, val loss: 0.9375050067901611
Epoch 840, training loss: 6.396327972412109 = 0.04481471702456474 + 1.0 * 6.351513385772705
Epoch 840, val loss: 0.9438206553459167
Epoch 850, training loss: 6.401566028594971 = 0.04282684996724129 + 1.0 * 6.358739376068115
Epoch 850, val loss: 0.9495399594306946
Epoch 860, training loss: 6.392933368682861 = 0.04098300263285637 + 1.0 * 6.351950168609619
Epoch 860, val loss: 0.9554912447929382
Epoch 870, training loss: 6.3875956535339355 = 0.03924724459648132 + 1.0 * 6.348348617553711
Epoch 870, val loss: 0.9613502621650696
Epoch 880, training loss: 6.387147903442383 = 0.037628430873155594 + 1.0 * 6.3495192527771
Epoch 880, val loss: 0.9670055508613586
Epoch 890, training loss: 6.380807399749756 = 0.03610417991876602 + 1.0 * 6.344703197479248
Epoch 890, val loss: 0.9725908041000366
Epoch 900, training loss: 6.3823442459106445 = 0.03467608243227005 + 1.0 * 6.347668170928955
Epoch 900, val loss: 0.9781500697135925
Epoch 910, training loss: 6.375579833984375 = 0.033330194652080536 + 1.0 * 6.342249870300293
Epoch 910, val loss: 0.9836869835853577
Epoch 920, training loss: 6.373166561126709 = 0.03206643834710121 + 1.0 * 6.341100215911865
Epoch 920, val loss: 0.9892250299453735
Epoch 930, training loss: 6.389735698699951 = 0.03087652288377285 + 1.0 * 6.358859062194824
Epoch 930, val loss: 0.9945787191390991
Epoch 940, training loss: 6.370683193206787 = 0.029756108298897743 + 1.0 * 6.3409271240234375
Epoch 940, val loss: 0.9996591210365295
Epoch 950, training loss: 6.367201805114746 = 0.028704199939966202 + 1.0 * 6.338497638702393
Epoch 950, val loss: 1.005265712738037
Epoch 960, training loss: 6.364442825317383 = 0.02770780213177204 + 1.0 * 6.336735248565674
Epoch 960, val loss: 1.0103508234024048
Epoch 970, training loss: 6.377191066741943 = 0.026765435934066772 + 1.0 * 6.350425720214844
Epoch 970, val loss: 1.0152605772018433
Epoch 980, training loss: 6.361957550048828 = 0.025865929201245308 + 1.0 * 6.3360915184021
Epoch 980, val loss: 1.0203304290771484
Epoch 990, training loss: 6.358616828918457 = 0.025020657107234 + 1.0 * 6.333596229553223
Epoch 990, val loss: 1.0255014896392822
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 10.539804458618164 = 1.9429439306259155 + 1.0 * 8.596860885620117
Epoch 0, val loss: 1.946867823600769
Epoch 10, training loss: 10.529829025268555 = 1.933163046836853 + 1.0 * 8.59666633605957
Epoch 10, val loss: 1.937479853630066
Epoch 20, training loss: 10.515694618225098 = 1.9205286502838135 + 1.0 * 8.595166206359863
Epoch 20, val loss: 1.924890160560608
Epoch 30, training loss: 10.48494815826416 = 1.9023654460906982 + 1.0 * 8.582582473754883
Epoch 30, val loss: 1.9065054655075073
Epoch 40, training loss: 10.389265060424805 = 1.8771967887878418 + 1.0 * 8.512067794799805
Epoch 40, val loss: 1.8816498517990112
Epoch 50, training loss: 10.097721099853516 = 1.8494154214859009 + 1.0 * 8.248305320739746
Epoch 50, val loss: 1.8553762435913086
Epoch 60, training loss: 9.842334747314453 = 1.826027512550354 + 1.0 * 8.01630687713623
Epoch 60, val loss: 1.8343942165374756
Epoch 70, training loss: 9.32071304321289 = 1.8087142705917358 + 1.0 * 7.511998653411865
Epoch 70, val loss: 1.8176573514938354
Epoch 80, training loss: 8.953340530395508 = 1.7951083183288574 + 1.0 * 7.158231735229492
Epoch 80, val loss: 1.8045330047607422
Epoch 90, training loss: 8.753097534179688 = 1.7793018817901611 + 1.0 * 6.9737958908081055
Epoch 90, val loss: 1.790163278579712
Epoch 100, training loss: 8.611345291137695 = 1.761661410331726 + 1.0 * 6.849684238433838
Epoch 100, val loss: 1.7751177549362183
Epoch 110, training loss: 8.519275665283203 = 1.7433730363845825 + 1.0 * 6.77590274810791
Epoch 110, val loss: 1.7591356039047241
Epoch 120, training loss: 8.447746276855469 = 1.7236467599868774 + 1.0 * 6.724099159240723
Epoch 120, val loss: 1.741241455078125
Epoch 130, training loss: 8.383307456970215 = 1.701326608657837 + 1.0 * 6.681981086730957
Epoch 130, val loss: 1.7211604118347168
Epoch 140, training loss: 8.329253196716309 = 1.6756958961486816 + 1.0 * 6.653557300567627
Epoch 140, val loss: 1.698881983757019
Epoch 150, training loss: 8.268549919128418 = 1.646977186203003 + 1.0 * 6.621572494506836
Epoch 150, val loss: 1.6743758916854858
Epoch 160, training loss: 8.212821960449219 = 1.6145886182785034 + 1.0 * 6.598233222961426
Epoch 160, val loss: 1.646836757659912
Epoch 170, training loss: 8.156050682067871 = 1.578064203262329 + 1.0 * 6.577986240386963
Epoch 170, val loss: 1.6158031225204468
Epoch 180, training loss: 8.107818603515625 = 1.537605881690979 + 1.0 * 6.5702128410339355
Epoch 180, val loss: 1.5816463232040405
Epoch 190, training loss: 8.044607162475586 = 1.4943523406982422 + 1.0 * 6.550254821777344
Epoch 190, val loss: 1.5452519655227661
Epoch 200, training loss: 7.983194351196289 = 1.4481236934661865 + 1.0 * 6.535070896148682
Epoch 200, val loss: 1.5063954591751099
Epoch 210, training loss: 7.92163610458374 = 1.3990527391433716 + 1.0 * 6.522583484649658
Epoch 210, val loss: 1.4656150341033936
Epoch 220, training loss: 7.859134674072266 = 1.3478877544403076 + 1.0 * 6.511247158050537
Epoch 220, val loss: 1.42319655418396
Epoch 230, training loss: 7.801264762878418 = 1.2955830097198486 + 1.0 * 6.505681991577148
Epoch 230, val loss: 1.3806840181350708
Epoch 240, training loss: 7.740467071533203 = 1.2442072629928589 + 1.0 * 6.496259689331055
Epoch 240, val loss: 1.3395179510116577
Epoch 250, training loss: 7.679054260253906 = 1.193623661994934 + 1.0 * 6.485430717468262
Epoch 250, val loss: 1.2996189594268799
Epoch 260, training loss: 7.621927261352539 = 1.144197702407837 + 1.0 * 6.477729320526123
Epoch 260, val loss: 1.2614103555679321
Epoch 270, training loss: 7.5714263916015625 = 1.0965962409973145 + 1.0 * 6.474830150604248
Epoch 270, val loss: 1.2255537509918213
Epoch 280, training loss: 7.519333362579346 = 1.0517592430114746 + 1.0 * 6.467574119567871
Epoch 280, val loss: 1.192530870437622
Epoch 290, training loss: 7.469615936279297 = 1.0092442035675049 + 1.0 * 6.460371971130371
Epoch 290, val loss: 1.1619880199432373
Epoch 300, training loss: 7.422662258148193 = 0.9686068296432495 + 1.0 * 6.454055309295654
Epoch 300, val loss: 1.133366346359253
Epoch 310, training loss: 7.378788471221924 = 0.9297012090682983 + 1.0 * 6.449087142944336
Epoch 310, val loss: 1.106299877166748
Epoch 320, training loss: 7.347495079040527 = 0.893031656742096 + 1.0 * 6.454463481903076
Epoch 320, val loss: 1.0812863111495972
Epoch 330, training loss: 7.2997283935546875 = 0.8588709831237793 + 1.0 * 6.440857410430908
Epoch 330, val loss: 1.0584758520126343
Epoch 340, training loss: 7.262062072753906 = 0.8266583681106567 + 1.0 * 6.435403823852539
Epoch 340, val loss: 1.0377757549285889
Epoch 350, training loss: 7.227395057678223 = 0.7961990833282471 + 1.0 * 6.431196212768555
Epoch 350, val loss: 1.0184520483016968
Epoch 360, training loss: 7.205385208129883 = 0.7672752141952515 + 1.0 * 6.438109874725342
Epoch 360, val loss: 1.0008474588394165
Epoch 370, training loss: 7.16664457321167 = 0.7402662038803101 + 1.0 * 6.42637825012207
Epoch 370, val loss: 0.9850690960884094
Epoch 380, training loss: 7.13618803024292 = 0.7146435976028442 + 1.0 * 6.421544551849365
Epoch 380, val loss: 0.9709495306015015
Epoch 390, training loss: 7.106213569641113 = 0.690028965473175 + 1.0 * 6.416184425354004
Epoch 390, val loss: 0.9578740000724792
Epoch 400, training loss: 7.079278945922852 = 0.6661275625228882 + 1.0 * 6.413151264190674
Epoch 400, val loss: 0.945679247379303
Epoch 410, training loss: 7.053886413574219 = 0.6427986025810242 + 1.0 * 6.411087989807129
Epoch 410, val loss: 0.9342604875564575
Epoch 420, training loss: 7.026350498199463 = 0.6200271844863892 + 1.0 * 6.406323432922363
Epoch 420, val loss: 0.9235854744911194
Epoch 430, training loss: 7.000582695007324 = 0.5974913835525513 + 1.0 * 6.4030914306640625
Epoch 430, val loss: 0.9134829044342041
Epoch 440, training loss: 6.985909461975098 = 0.5750775337219238 + 1.0 * 6.410831928253174
Epoch 440, val loss: 0.9036940932273865
Epoch 450, training loss: 6.953537464141846 = 0.5528241395950317 + 1.0 * 6.4007134437561035
Epoch 450, val loss: 0.8944472670555115
Epoch 460, training loss: 6.925879001617432 = 0.5308858752250671 + 1.0 * 6.394993305206299
Epoch 460, val loss: 0.8861852288246155
Epoch 470, training loss: 6.9018073081970215 = 0.5091897249221802 + 1.0 * 6.392617702484131
Epoch 470, val loss: 0.8782910108566284
Epoch 480, training loss: 6.8841552734375 = 0.4878760576248169 + 1.0 * 6.396279335021973
Epoch 480, val loss: 0.8712635040283203
Epoch 490, training loss: 6.861173629760742 = 0.4671807289123535 + 1.0 * 6.393992900848389
Epoch 490, val loss: 0.8654536008834839
Epoch 500, training loss: 6.848939418792725 = 0.44727233052253723 + 1.0 * 6.40166711807251
Epoch 500, val loss: 0.8607677817344666
Epoch 510, training loss: 6.812939167022705 = 0.4281349182128906 + 1.0 * 6.3848042488098145
Epoch 510, val loss: 0.8571921586990356
Epoch 520, training loss: 6.792148113250732 = 0.40971091389656067 + 1.0 * 6.382437229156494
Epoch 520, val loss: 0.8547197580337524
Epoch 530, training loss: 6.771265983581543 = 0.3918968141078949 + 1.0 * 6.379369258880615
Epoch 530, val loss: 0.8530723452568054
Epoch 540, training loss: 6.751823902130127 = 0.37459319829940796 + 1.0 * 6.377230644226074
Epoch 540, val loss: 0.8522573113441467
Epoch 550, training loss: 6.73805046081543 = 0.3577873110771179 + 1.0 * 6.380263328552246
Epoch 550, val loss: 0.852183997631073
Epoch 560, training loss: 6.717658042907715 = 0.3414292633533478 + 1.0 * 6.3762288093566895
Epoch 560, val loss: 0.8526877164840698
Epoch 570, training loss: 6.697943687438965 = 0.32540735602378845 + 1.0 * 6.3725361824035645
Epoch 570, val loss: 0.8537597060203552
Epoch 580, training loss: 6.68162727355957 = 0.30959969758987427 + 1.0 * 6.372027397155762
Epoch 580, val loss: 0.8550673127174377
Epoch 590, training loss: 6.663757801055908 = 0.2940102219581604 + 1.0 * 6.369747638702393
Epoch 590, val loss: 0.8567593097686768
Epoch 600, training loss: 6.653062343597412 = 0.27868571877479553 + 1.0 * 6.3743767738342285
Epoch 600, val loss: 0.8589110374450684
Epoch 610, training loss: 6.632051467895508 = 0.26361846923828125 + 1.0 * 6.368432998657227
Epoch 610, val loss: 0.8610581159591675
Epoch 620, training loss: 6.611907958984375 = 0.24894553422927856 + 1.0 * 6.362962245941162
Epoch 620, val loss: 0.8638558983802795
Epoch 630, training loss: 6.608361721038818 = 0.23470793664455414 + 1.0 * 6.373653888702393
Epoch 630, val loss: 0.8669816255569458
Epoch 640, training loss: 6.5827317237854 = 0.22101640701293945 + 1.0 * 6.361715316772461
Epoch 640, val loss: 0.870405375957489
Epoch 650, training loss: 6.564936637878418 = 0.20799537003040314 + 1.0 * 6.356941223144531
Epoch 650, val loss: 0.8745299577713013
Epoch 660, training loss: 6.552297592163086 = 0.19561022520065308 + 1.0 * 6.356687545776367
Epoch 660, val loss: 0.8789869546890259
Epoch 670, training loss: 6.545711994171143 = 0.18395015597343445 + 1.0 * 6.361762046813965
Epoch 670, val loss: 0.8838217258453369
Epoch 680, training loss: 6.528789043426514 = 0.1730882078409195 + 1.0 * 6.355700969696045
Epoch 680, val loss: 0.8893012404441833
Epoch 690, training loss: 6.518334865570068 = 0.1629456877708435 + 1.0 * 6.35538911819458
Epoch 690, val loss: 0.8951439261436462
Epoch 700, training loss: 6.506950855255127 = 0.15350458025932312 + 1.0 * 6.3534464836120605
Epoch 700, val loss: 0.9014053344726562
Epoch 710, training loss: 6.493206977844238 = 0.14473140239715576 + 1.0 * 6.348475456237793
Epoch 710, val loss: 0.9081946611404419
Epoch 720, training loss: 6.489062309265137 = 0.13654643297195435 + 1.0 * 6.352515697479248
Epoch 720, val loss: 0.915253221988678
Epoch 730, training loss: 6.477086067199707 = 0.12891347706317902 + 1.0 * 6.348172664642334
Epoch 730, val loss: 0.9224894642829895
Epoch 740, training loss: 6.468852996826172 = 0.12180715054273605 + 1.0 * 6.3470458984375
Epoch 740, val loss: 0.9302070736885071
Epoch 750, training loss: 6.4588541984558105 = 0.11518804728984833 + 1.0 * 6.343666076660156
Epoch 750, val loss: 0.937953770160675
Epoch 760, training loss: 6.454394340515137 = 0.10900432616472244 + 1.0 * 6.3453898429870605
Epoch 760, val loss: 0.9459145665168762
Epoch 770, training loss: 6.447142124176025 = 0.10325942933559418 + 1.0 * 6.3438825607299805
Epoch 770, val loss: 0.9540705680847168
Epoch 780, training loss: 6.439562797546387 = 0.09791745245456696 + 1.0 * 6.341645240783691
Epoch 780, val loss: 0.9625477194786072
Epoch 790, training loss: 6.429800510406494 = 0.09293128550052643 + 1.0 * 6.336869239807129
Epoch 790, val loss: 0.9708724617958069
Epoch 800, training loss: 6.431314945220947 = 0.08826442807912827 + 1.0 * 6.343050479888916
Epoch 800, val loss: 0.9794639348983765
Epoch 810, training loss: 6.425154209136963 = 0.08389559388160706 + 1.0 * 6.341258525848389
Epoch 810, val loss: 0.9876046776771545
Epoch 820, training loss: 6.414400577545166 = 0.07984033972024918 + 1.0 * 6.334560394287109
Epoch 820, val loss: 0.9964657425880432
Epoch 830, training loss: 6.408834457397461 = 0.07602868974208832 + 1.0 * 6.332805633544922
Epoch 830, val loss: 1.0048675537109375
Epoch 840, training loss: 6.406797409057617 = 0.07244525104761124 + 1.0 * 6.334352016448975
Epoch 840, val loss: 1.0134278535842896
Epoch 850, training loss: 6.400906085968018 = 0.06908253580331802 + 1.0 * 6.331823348999023
Epoch 850, val loss: 1.0218086242675781
Epoch 860, training loss: 6.396968364715576 = 0.06593861430883408 + 1.0 * 6.331029891967773
Epoch 860, val loss: 1.0306391716003418
Epoch 870, training loss: 6.3919782638549805 = 0.06298205256462097 + 1.0 * 6.328996181488037
Epoch 870, val loss: 1.038970947265625
Epoch 880, training loss: 6.398467540740967 = 0.060195423662662506 + 1.0 * 6.3382720947265625
Epoch 880, val loss: 1.047257661819458
Epoch 890, training loss: 6.3899641036987305 = 0.05759477615356445 + 1.0 * 6.332369327545166
Epoch 890, val loss: 1.0557984113693237
Epoch 900, training loss: 6.3812150955200195 = 0.05514969676733017 + 1.0 * 6.326065540313721
Epoch 900, val loss: 1.064183235168457
Epoch 910, training loss: 6.377384185791016 = 0.05284253507852554 + 1.0 * 6.3245415687561035
Epoch 910, val loss: 1.0723472833633423
Epoch 920, training loss: 6.374990463256836 = 0.050660621374845505 + 1.0 * 6.324329853057861
Epoch 920, val loss: 1.0805197954177856
Epoch 930, training loss: 6.37932014465332 = 0.04860023781657219 + 1.0 * 6.330719947814941
Epoch 930, val loss: 1.0885810852050781
Epoch 940, training loss: 6.370553970336914 = 0.04667133092880249 + 1.0 * 6.323882579803467
Epoch 940, val loss: 1.0969152450561523
Epoch 950, training loss: 6.365028381347656 = 0.04484345391392708 + 1.0 * 6.320184707641602
Epoch 950, val loss: 1.1048049926757812
Epoch 960, training loss: 6.3682379722595215 = 0.04311169683933258 + 1.0 * 6.3251261711120605
Epoch 960, val loss: 1.1125526428222656
Epoch 970, training loss: 6.3639912605285645 = 0.041480764746665955 + 1.0 * 6.322510719299316
Epoch 970, val loss: 1.1204206943511963
Epoch 980, training loss: 6.3603315353393555 = 0.039938345551490784 + 1.0 * 6.320393085479736
Epoch 980, val loss: 1.128175973892212
Epoch 990, training loss: 6.355357646942139 = 0.03848421201109886 + 1.0 * 6.316873550415039
Epoch 990, val loss: 1.1358779668807983
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8081180811808119
The final CL Acc:0.73580, 0.01222, The final GNN Acc:0.80812, 0.00086
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13140])
remove edge: torch.Size([2, 8010])
updated graph: torch.Size([2, 10594])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.5454683303833 = 1.9486382007598877 + 1.0 * 8.596830368041992
Epoch 0, val loss: 1.9493677616119385
Epoch 10, training loss: 10.535282135009766 = 1.9387331008911133 + 1.0 * 8.596549034118652
Epoch 10, val loss: 1.94009268283844
Epoch 20, training loss: 10.520768165588379 = 1.9265060424804688 + 1.0 * 8.59426212310791
Epoch 20, val loss: 1.928238034248352
Epoch 30, training loss: 10.486775398254395 = 1.9094051122665405 + 1.0 * 8.577370643615723
Epoch 30, val loss: 1.9114307165145874
Epoch 40, training loss: 10.379327774047852 = 1.8864688873291016 + 1.0 * 8.49285888671875
Epoch 40, val loss: 1.8898059129714966
Epoch 50, training loss: 10.054327011108398 = 1.861154556274414 + 1.0 * 8.193172454833984
Epoch 50, val loss: 1.8665465116500854
Epoch 60, training loss: 9.793052673339844 = 1.8350255489349365 + 1.0 * 7.958027362823486
Epoch 60, val loss: 1.8422359228134155
Epoch 70, training loss: 9.387602806091309 = 1.8093039989471436 + 1.0 * 7.578298568725586
Epoch 70, val loss: 1.8186966180801392
Epoch 80, training loss: 9.116988182067871 = 1.7861995697021484 + 1.0 * 7.330788612365723
Epoch 80, val loss: 1.79645574092865
Epoch 90, training loss: 8.93137264251709 = 1.7614542245864868 + 1.0 * 7.169918060302734
Epoch 90, val loss: 1.7731716632843018
Epoch 100, training loss: 8.735603332519531 = 1.7403572797775269 + 1.0 * 6.995245933532715
Epoch 100, val loss: 1.7544859647750854
Epoch 110, training loss: 8.611308097839355 = 1.7193634510040283 + 1.0 * 6.891944885253906
Epoch 110, val loss: 1.7353423833847046
Epoch 120, training loss: 8.517223358154297 = 1.6914043426513672 + 1.0 * 6.8258185386657715
Epoch 120, val loss: 1.7104804515838623
Epoch 130, training loss: 8.432153701782227 = 1.6609563827514648 + 1.0 * 6.771197319030762
Epoch 130, val loss: 1.6826775074005127
Epoch 140, training loss: 8.352039337158203 = 1.6279727220535278 + 1.0 * 6.724066257476807
Epoch 140, val loss: 1.6528302431106567
Epoch 150, training loss: 8.276619911193848 = 1.5903044939041138 + 1.0 * 6.686315536499023
Epoch 150, val loss: 1.620305061340332
Epoch 160, training loss: 8.198817253112793 = 1.5476850271224976 + 1.0 * 6.651132583618164
Epoch 160, val loss: 1.5837892293930054
Epoch 170, training loss: 8.123319625854492 = 1.500521183013916 + 1.0 * 6.622797966003418
Epoch 170, val loss: 1.5432335138320923
Epoch 180, training loss: 8.052696228027344 = 1.4502216577529907 + 1.0 * 6.602474212646484
Epoch 180, val loss: 1.499998688697815
Epoch 190, training loss: 7.9776458740234375 = 1.3977705240249634 + 1.0 * 6.579875469207764
Epoch 190, val loss: 1.4550355672836304
Epoch 200, training loss: 7.906467914581299 = 1.3437443971633911 + 1.0 * 6.562723636627197
Epoch 200, val loss: 1.4089454412460327
Epoch 210, training loss: 7.836026668548584 = 1.288740634918213 + 1.0 * 6.547286033630371
Epoch 210, val loss: 1.3626235723495483
Epoch 220, training loss: 7.771390438079834 = 1.2340952157974243 + 1.0 * 6.537295341491699
Epoch 220, val loss: 1.3171570301055908
Epoch 230, training loss: 7.704902172088623 = 1.181670069694519 + 1.0 * 6.5232319831848145
Epoch 230, val loss: 1.2738134860992432
Epoch 240, training loss: 7.642181873321533 = 1.1315199136734009 + 1.0 * 6.510662078857422
Epoch 240, val loss: 1.2329261302947998
Epoch 250, training loss: 7.584489822387695 = 1.0836957693099976 + 1.0 * 6.500793933868408
Epoch 250, val loss: 1.1941994428634644
Epoch 260, training loss: 7.532132148742676 = 1.0389469861984253 + 1.0 * 6.493185043334961
Epoch 260, val loss: 1.1581335067749023
Epoch 270, training loss: 7.480470180511475 = 0.99730384349823 + 1.0 * 6.483166217803955
Epoch 270, val loss: 1.1246843338012695
Epoch 280, training loss: 7.433283805847168 = 0.958185613155365 + 1.0 * 6.475098133087158
Epoch 280, val loss: 1.093358039855957
Epoch 290, training loss: 7.396474361419678 = 0.9213961958885193 + 1.0 * 6.475078105926514
Epoch 290, val loss: 1.0637844800949097
Epoch 300, training loss: 7.351693630218506 = 0.8872284293174744 + 1.0 * 6.464465141296387
Epoch 300, val loss: 1.0363953113555908
Epoch 310, training loss: 7.310445785522461 = 0.8549069762229919 + 1.0 * 6.455538749694824
Epoch 310, val loss: 1.0107722282409668
Epoch 320, training loss: 7.274749755859375 = 0.8238604068756104 + 1.0 * 6.4508891105651855
Epoch 320, val loss: 0.9862963557243347
Epoch 330, training loss: 7.238081455230713 = 0.7940996289253235 + 1.0 * 6.443981647491455
Epoch 330, val loss: 0.9631465077400208
Epoch 340, training loss: 7.205417633056641 = 0.7654539942741394 + 1.0 * 6.4399638175964355
Epoch 340, val loss: 0.9413526058197021
Epoch 350, training loss: 7.169761657714844 = 0.7372815608978271 + 1.0 * 6.4324798583984375
Epoch 350, val loss: 0.9204550385475159
Epoch 360, training loss: 7.138514041900635 = 0.7093862891197205 + 1.0 * 6.4291276931762695
Epoch 360, val loss: 0.9003486633300781
Epoch 370, training loss: 7.107461929321289 = 0.6819995641708374 + 1.0 * 6.425462245941162
Epoch 370, val loss: 0.881253719329834
Epoch 380, training loss: 7.077722549438477 = 0.6551413536071777 + 1.0 * 6.422581195831299
Epoch 380, val loss: 0.863263726234436
Epoch 390, training loss: 7.043997764587402 = 0.6288388967514038 + 1.0 * 6.415158748626709
Epoch 390, val loss: 0.846504807472229
Epoch 400, training loss: 7.015666961669922 = 0.6030462384223938 + 1.0 * 6.412620544433594
Epoch 400, val loss: 0.831039547920227
Epoch 410, training loss: 6.98732852935791 = 0.5779798030853271 + 1.0 * 6.409348964691162
Epoch 410, val loss: 0.8167968988418579
Epoch 420, training loss: 6.957859992980957 = 0.5537359714508057 + 1.0 * 6.4041242599487305
Epoch 420, val loss: 0.8040276169776917
Epoch 430, training loss: 6.929476737976074 = 0.5301318764686584 + 1.0 * 6.3993449211120605
Epoch 430, val loss: 0.7925592064857483
Epoch 440, training loss: 6.915609359741211 = 0.5071864724159241 + 1.0 * 6.408422946929932
Epoch 440, val loss: 0.7822231650352478
Epoch 450, training loss: 6.881483554840088 = 0.4852674603462219 + 1.0 * 6.396215915679932
Epoch 450, val loss: 0.7731809616088867
Epoch 460, training loss: 6.856314659118652 = 0.46425116062164307 + 1.0 * 6.392063617706299
Epoch 460, val loss: 0.7653831243515015
Epoch 470, training loss: 6.83114767074585 = 0.4441315829753876 + 1.0 * 6.387016296386719
Epoch 470, val loss: 0.7586572766304016
Epoch 480, training loss: 6.811154842376709 = 0.42510414123535156 + 1.0 * 6.386050701141357
Epoch 480, val loss: 0.7530579566955566
Epoch 490, training loss: 6.789103984832764 = 0.4069557189941406 + 1.0 * 6.382148265838623
Epoch 490, val loss: 0.7484610676765442
Epoch 500, training loss: 6.768754005432129 = 0.3896271288394928 + 1.0 * 6.379127025604248
Epoch 500, val loss: 0.7448184490203857
Epoch 510, training loss: 6.764072895050049 = 0.37314119935035706 + 1.0 * 6.390931606292725
Epoch 510, val loss: 0.7421103715896606
Epoch 520, training loss: 6.734187126159668 = 0.3576613664627075 + 1.0 * 6.37652587890625
Epoch 520, val loss: 0.7403253316879272
Epoch 530, training loss: 6.717142105102539 = 0.3430624306201935 + 1.0 * 6.374079704284668
Epoch 530, val loss: 0.7394551634788513
Epoch 540, training loss: 6.702234745025635 = 0.32921984791755676 + 1.0 * 6.3730149269104
Epoch 540, val loss: 0.7394072413444519
Epoch 550, training loss: 6.686049461364746 = 0.3161458671092987 + 1.0 * 6.369903564453125
Epoch 550, val loss: 0.7401212453842163
Epoch 560, training loss: 6.673577785491943 = 0.3037990927696228 + 1.0 * 6.369778633117676
Epoch 560, val loss: 0.7415384650230408
Epoch 570, training loss: 6.657404899597168 = 0.29210400581359863 + 1.0 * 6.365301132202148
Epoch 570, val loss: 0.7435979843139648
Epoch 580, training loss: 6.642897605895996 = 0.28096267580986023 + 1.0 * 6.361935138702393
Epoch 580, val loss: 0.7462356686592102
Epoch 590, training loss: 6.632959842681885 = 0.2702818512916565 + 1.0 * 6.362678050994873
Epoch 590, val loss: 0.7493874430656433
Epoch 600, training loss: 6.623852252960205 = 0.2600221335887909 + 1.0 * 6.363830089569092
Epoch 600, val loss: 0.75290846824646
Epoch 610, training loss: 6.606645584106445 = 0.25009503960609436 + 1.0 * 6.356550693511963
Epoch 610, val loss: 0.7567018270492554
Epoch 620, training loss: 6.599006175994873 = 0.2402791976928711 + 1.0 * 6.358726978302002
Epoch 620, val loss: 0.7606949806213379
Epoch 630, training loss: 6.584137916564941 = 0.23044469952583313 + 1.0 * 6.353693008422852
Epoch 630, val loss: 0.7647505402565002
Epoch 640, training loss: 6.57167911529541 = 0.22043991088867188 + 1.0 * 6.351239204406738
Epoch 640, val loss: 0.768792986869812
Epoch 650, training loss: 6.562841892242432 = 0.21013377606868744 + 1.0 * 6.352708339691162
Epoch 650, val loss: 0.7726816534996033
Epoch 660, training loss: 6.551712989807129 = 0.19951844215393066 + 1.0 * 6.352194786071777
Epoch 660, val loss: 0.7763437628746033
Epoch 670, training loss: 6.53829288482666 = 0.18865129351615906 + 1.0 * 6.349641799926758
Epoch 670, val loss: 0.7798156142234802
Epoch 680, training loss: 6.524961948394775 = 0.17765340209007263 + 1.0 * 6.34730863571167
Epoch 680, val loss: 0.7831779718399048
Epoch 690, training loss: 6.511321544647217 = 0.1667456030845642 + 1.0 * 6.344575881958008
Epoch 690, val loss: 0.7865976691246033
Epoch 700, training loss: 6.506146430969238 = 0.15619656443595886 + 1.0 * 6.349949836730957
Epoch 700, val loss: 0.7901853322982788
Epoch 710, training loss: 6.495151519775391 = 0.14625538885593414 + 1.0 * 6.348896026611328
Epoch 710, val loss: 0.7939600944519043
Epoch 720, training loss: 6.479091644287109 = 0.13705064356327057 + 1.0 * 6.342041015625
Epoch 720, val loss: 0.797981858253479
Epoch 730, training loss: 6.467973232269287 = 0.12856826186180115 + 1.0 * 6.339405059814453
Epoch 730, val loss: 0.8024090528488159
Epoch 740, training loss: 6.459322929382324 = 0.12079140543937683 + 1.0 * 6.338531494140625
Epoch 740, val loss: 0.807194709777832
Epoch 750, training loss: 6.456619739532471 = 0.11368001997470856 + 1.0 * 6.342939853668213
Epoch 750, val loss: 0.8123303651809692
Epoch 760, training loss: 6.446100234985352 = 0.10719455778598785 + 1.0 * 6.3389058113098145
Epoch 760, val loss: 0.817729115486145
Epoch 770, training loss: 6.438876152038574 = 0.10125825554132462 + 1.0 * 6.337617874145508
Epoch 770, val loss: 0.8234524726867676
Epoch 780, training loss: 6.4300689697265625 = 0.09582677483558655 + 1.0 * 6.334242343902588
Epoch 780, val loss: 0.8293850421905518
Epoch 790, training loss: 6.422479629516602 = 0.09080249071121216 + 1.0 * 6.331676959991455
Epoch 790, val loss: 0.8355780839920044
Epoch 800, training loss: 6.416651725769043 = 0.08614732325077057 + 1.0 * 6.330504417419434
Epoch 800, val loss: 0.8420383334159851
Epoch 810, training loss: 6.421051025390625 = 0.08182772994041443 + 1.0 * 6.339223384857178
Epoch 810, val loss: 0.8486072421073914
Epoch 820, training loss: 6.408592700958252 = 0.07781810313463211 + 1.0 * 6.330774784088135
Epoch 820, val loss: 0.8553414940834045
Epoch 830, training loss: 6.407102108001709 = 0.07408417761325836 + 1.0 * 6.333017826080322
Epoch 830, val loss: 0.8621516227722168
Epoch 840, training loss: 6.398441791534424 = 0.07060395926237106 + 1.0 * 6.327837944030762
Epoch 840, val loss: 0.8690317869186401
Epoch 850, training loss: 6.3933186531066895 = 0.06734515726566315 + 1.0 * 6.3259735107421875
Epoch 850, val loss: 0.8760395050048828
Epoch 860, training loss: 6.392361164093018 = 0.06429088115692139 + 1.0 * 6.328070163726807
Epoch 860, val loss: 0.8830045461654663
Epoch 870, training loss: 6.38707160949707 = 0.06143263354897499 + 1.0 * 6.325638771057129
Epoch 870, val loss: 0.8899955153465271
Epoch 880, training loss: 6.381465911865234 = 0.058746587485075 + 1.0 * 6.322719097137451
Epoch 880, val loss: 0.8970147967338562
Epoch 890, training loss: 6.376730918884277 = 0.05621960759162903 + 1.0 * 6.320511341094971
Epoch 890, val loss: 0.904059886932373
Epoch 900, training loss: 6.377260684967041 = 0.05383665859699249 + 1.0 * 6.323423862457275
Epoch 900, val loss: 0.9111247062683105
Epoch 910, training loss: 6.370502471923828 = 0.051597993820905685 + 1.0 * 6.318904399871826
Epoch 910, val loss: 0.9180774688720703
Epoch 920, training loss: 6.368729114532471 = 0.04948410764336586 + 1.0 * 6.319244861602783
Epoch 920, val loss: 0.9250910878181458
Epoch 930, training loss: 6.36815071105957 = 0.0474749319255352 + 1.0 * 6.320675849914551
Epoch 930, val loss: 0.9320144653320312
Epoch 940, training loss: 6.3618621826171875 = 0.045573845505714417 + 1.0 * 6.316288471221924
Epoch 940, val loss: 0.9389616250991821
Epoch 950, training loss: 6.359396457672119 = 0.04377690702676773 + 1.0 * 6.315619468688965
Epoch 950, val loss: 0.9458609819412231
Epoch 960, training loss: 6.357774257659912 = 0.04207053408026695 + 1.0 * 6.315703868865967
Epoch 960, val loss: 0.9527366161346436
Epoch 970, training loss: 6.35511589050293 = 0.04045446589589119 + 1.0 * 6.314661502838135
Epoch 970, val loss: 0.9595555067062378
Epoch 980, training loss: 6.353329181671143 = 0.03891931101679802 + 1.0 * 6.3144097328186035
Epoch 980, val loss: 0.9663804769515991
Epoch 990, training loss: 6.350515842437744 = 0.03746947646141052 + 1.0 * 6.313046455383301
Epoch 990, val loss: 0.9731083512306213
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 10.547070503234863 = 1.9502284526824951 + 1.0 * 8.596841812133789
Epoch 0, val loss: 1.9409364461898804
Epoch 10, training loss: 10.536017417907715 = 1.9394079446792603 + 1.0 * 8.596609115600586
Epoch 10, val loss: 1.9307634830474854
Epoch 20, training loss: 10.5209379196167 = 1.9262136220932007 + 1.0 * 8.594724655151367
Epoch 20, val loss: 1.9179352521896362
Epoch 30, training loss: 10.48666000366211 = 1.908042073249817 + 1.0 * 8.578618049621582
Epoch 30, val loss: 1.8999143838882446
Epoch 40, training loss: 10.35200023651123 = 1.8840411901474 + 1.0 * 8.4679594039917
Epoch 40, val loss: 1.8768713474273682
Epoch 50, training loss: 9.893638610839844 = 1.8595737218856812 + 1.0 * 8.034065246582031
Epoch 50, val loss: 1.8546634912490845
Epoch 60, training loss: 9.467613220214844 = 1.8395583629608154 + 1.0 * 7.628055095672607
Epoch 60, val loss: 1.836756944656372
Epoch 70, training loss: 9.150824546813965 = 1.8232311010360718 + 1.0 * 7.327593803405762
Epoch 70, val loss: 1.821642279624939
Epoch 80, training loss: 8.886770248413086 = 1.806632399559021 + 1.0 * 7.080137729644775
Epoch 80, val loss: 1.8061128854751587
Epoch 90, training loss: 8.691770553588867 = 1.788305640220642 + 1.0 * 6.903465270996094
Epoch 90, val loss: 1.790043592453003
Epoch 100, training loss: 8.578259468078613 = 1.7688075304031372 + 1.0 * 6.809452056884766
Epoch 100, val loss: 1.773067593574524
Epoch 110, training loss: 8.493307113647461 = 1.7493109703063965 + 1.0 * 6.7439961433410645
Epoch 110, val loss: 1.7560869455337524
Epoch 120, training loss: 8.42972183227539 = 1.7296046018600464 + 1.0 * 6.700117111206055
Epoch 120, val loss: 1.738524317741394
Epoch 130, training loss: 8.370997428894043 = 1.7083923816680908 + 1.0 * 6.662605285644531
Epoch 130, val loss: 1.719560980796814
Epoch 140, training loss: 8.31675910949707 = 1.684586524963379 + 1.0 * 6.632172107696533
Epoch 140, val loss: 1.6986966133117676
Epoch 150, training loss: 8.267216682434082 = 1.6577622890472412 + 1.0 * 6.609454154968262
Epoch 150, val loss: 1.6758090257644653
Epoch 160, training loss: 8.21415901184082 = 1.6278129816055298 + 1.0 * 6.586345672607422
Epoch 160, val loss: 1.6504569053649902
Epoch 170, training loss: 8.162220001220703 = 1.5940961837768555 + 1.0 * 6.568124294281006
Epoch 170, val loss: 1.6220906972885132
Epoch 180, training loss: 8.112865447998047 = 1.556395173072815 + 1.0 * 6.5564703941345215
Epoch 180, val loss: 1.5905802249908447
Epoch 190, training loss: 8.054839134216309 = 1.516026258468628 + 1.0 * 6.538812637329102
Epoch 190, val loss: 1.5569920539855957
Epoch 200, training loss: 7.9976677894592285 = 1.4734255075454712 + 1.0 * 6.524242401123047
Epoch 200, val loss: 1.5220086574554443
Epoch 210, training loss: 7.941021919250488 = 1.4291977882385254 + 1.0 * 6.511824131011963
Epoch 210, val loss: 1.4860992431640625
Epoch 220, training loss: 7.886621952056885 = 1.3841193914413452 + 1.0 * 6.50250244140625
Epoch 220, val loss: 1.4500863552093506
Epoch 230, training loss: 7.830445289611816 = 1.3389188051223755 + 1.0 * 6.4915266036987305
Epoch 230, val loss: 1.4143918752670288
Epoch 240, training loss: 7.774703025817871 = 1.29293692111969 + 1.0 * 6.481766223907471
Epoch 240, val loss: 1.3785912990570068
Epoch 250, training loss: 7.720256805419922 = 1.2463209629058838 + 1.0 * 6.473935604095459
Epoch 250, val loss: 1.3426843881607056
Epoch 260, training loss: 7.664612770080566 = 1.1989511251449585 + 1.0 * 6.465661525726318
Epoch 260, val loss: 1.306444764137268
Epoch 270, training loss: 7.609807968139648 = 1.15058434009552 + 1.0 * 6.459223747253418
Epoch 270, val loss: 1.2695270776748657
Epoch 280, training loss: 7.555883407592773 = 1.1014196872711182 + 1.0 * 6.454463481903076
Epoch 280, val loss: 1.2318401336669922
Epoch 290, training loss: 7.497934341430664 = 1.0517529249191284 + 1.0 * 6.446181297302246
Epoch 290, val loss: 1.1938350200653076
Epoch 300, training loss: 7.445830821990967 = 1.0018510818481445 + 1.0 * 6.443979740142822
Epoch 300, val loss: 1.155470848083496
Epoch 310, training loss: 7.3929548263549805 = 0.9525569677352905 + 1.0 * 6.4403977394104
Epoch 310, val loss: 1.1172759532928467
Epoch 320, training loss: 7.335898399353027 = 0.9045209884643555 + 1.0 * 6.431377410888672
Epoch 320, val loss: 1.080173373222351
Epoch 330, training loss: 7.2842793464660645 = 0.8581838607788086 + 1.0 * 6.426095485687256
Epoch 330, val loss: 1.044299840927124
Epoch 340, training loss: 7.2354536056518555 = 0.8139169812202454 + 1.0 * 6.421536445617676
Epoch 340, val loss: 1.0101197957992554
Epoch 350, training loss: 7.191709995269775 = 0.7723898887634277 + 1.0 * 6.419320106506348
Epoch 350, val loss: 0.978550910949707
Epoch 360, training loss: 7.146990776062012 = 0.7340261340141296 + 1.0 * 6.412964820861816
Epoch 360, val loss: 0.9496105313301086
Epoch 370, training loss: 7.1085205078125 = 0.698441207408905 + 1.0 * 6.410079479217529
Epoch 370, val loss: 0.923413097858429
Epoch 380, training loss: 7.071606159210205 = 0.6655713319778442 + 1.0 * 6.40603494644165
Epoch 380, val loss: 0.8998924493789673
Epoch 390, training loss: 7.0363287925720215 = 0.6347569823265076 + 1.0 * 6.401571750640869
Epoch 390, val loss: 0.878608226776123
Epoch 400, training loss: 7.0025482177734375 = 0.605673611164093 + 1.0 * 6.39687442779541
Epoch 400, val loss: 0.859343945980072
Epoch 410, training loss: 6.973128795623779 = 0.5781429409980774 + 1.0 * 6.394985675811768
Epoch 410, val loss: 0.8417628407478333
Epoch 420, training loss: 6.942010402679443 = 0.551724374294281 + 1.0 * 6.390285968780518
Epoch 420, val loss: 0.8256723880767822
Epoch 430, training loss: 6.931358814239502 = 0.5262718200683594 + 1.0 * 6.405086994171143
Epoch 430, val loss: 0.8108232617378235
Epoch 440, training loss: 6.891641616821289 = 0.5020166039466858 + 1.0 * 6.389625072479248
Epoch 440, val loss: 0.7972860932350159
Epoch 450, training loss: 6.861147880554199 = 0.4785749614238739 + 1.0 * 6.382573127746582
Epoch 450, val loss: 0.7848595976829529
Epoch 460, training loss: 6.8348798751831055 = 0.455766499042511 + 1.0 * 6.37911319732666
Epoch 460, val loss: 0.7733758091926575
Epoch 470, training loss: 6.817134380340576 = 0.43350493907928467 + 1.0 * 6.383629322052002
Epoch 470, val loss: 0.7627339959144592
Epoch 480, training loss: 6.785424709320068 = 0.4117791950702667 + 1.0 * 6.373645305633545
Epoch 480, val loss: 0.752991259098053
Epoch 490, training loss: 6.763505458831787 = 0.3905653953552246 + 1.0 * 6.3729400634765625
Epoch 490, val loss: 0.7440431714057922
Epoch 500, training loss: 6.739871978759766 = 0.3699279725551605 + 1.0 * 6.369944095611572
Epoch 500, val loss: 0.7358747720718384
Epoch 510, training loss: 6.723020553588867 = 0.34994086623191833 + 1.0 * 6.373079776763916
Epoch 510, val loss: 0.72850501537323
Epoch 520, training loss: 6.695753574371338 = 0.3306838274002075 + 1.0 * 6.36506986618042
Epoch 520, val loss: 0.7219066619873047
Epoch 530, training loss: 6.6759467124938965 = 0.31202661991119385 + 1.0 * 6.363920211791992
Epoch 530, val loss: 0.7160650491714478
Epoch 540, training loss: 6.658135414123535 = 0.2941092252731323 + 1.0 * 6.364026069641113
Epoch 540, val loss: 0.7108857035636902
Epoch 550, training loss: 6.637885570526123 = 0.27705255150794983 + 1.0 * 6.360833168029785
Epoch 550, val loss: 0.7063589096069336
Epoch 560, training loss: 6.620151996612549 = 0.26075538992881775 + 1.0 * 6.359396457672119
Epoch 560, val loss: 0.7025210857391357
Epoch 570, training loss: 6.600324630737305 = 0.245266392827034 + 1.0 * 6.355058193206787
Epoch 570, val loss: 0.6992874145507812
Epoch 580, training loss: 6.584980487823486 = 0.23047958314418793 + 1.0 * 6.354500770568848
Epoch 580, val loss: 0.6966462731361389
Epoch 590, training loss: 6.573602676391602 = 0.2163836658000946 + 1.0 * 6.357219219207764
Epoch 590, val loss: 0.6945866346359253
Epoch 600, training loss: 6.555538654327393 = 0.20313389599323273 + 1.0 * 6.352404594421387
Epoch 600, val loss: 0.6930771470069885
Epoch 610, training loss: 6.537755489349365 = 0.19056326150894165 + 1.0 * 6.347192287445068
Epoch 610, val loss: 0.6920948028564453
Epoch 620, training loss: 6.525984764099121 = 0.1786535233259201 + 1.0 * 6.3473310470581055
Epoch 620, val loss: 0.6916185617446899
Epoch 630, training loss: 6.51182222366333 = 0.16745053231716156 + 1.0 * 6.344371795654297
Epoch 630, val loss: 0.6916684508323669
Epoch 640, training loss: 6.501102447509766 = 0.15693974494934082 + 1.0 * 6.344162464141846
Epoch 640, val loss: 0.6921886205673218
Epoch 650, training loss: 6.489014148712158 = 0.14713655412197113 + 1.0 * 6.341877460479736
Epoch 650, val loss: 0.6931887865066528
Epoch 660, training loss: 6.4839606285095215 = 0.13800568878650665 + 1.0 * 6.345954895019531
Epoch 660, val loss: 0.6945982575416565
Epoch 670, training loss: 6.471292018890381 = 0.12948501110076904 + 1.0 * 6.341806888580322
Epoch 670, val loss: 0.6963816285133362
Epoch 680, training loss: 6.460906028747559 = 0.12161082774400711 + 1.0 * 6.339295387268066
Epoch 680, val loss: 0.6985698938369751
Epoch 690, training loss: 6.451077461242676 = 0.11429814249277115 + 1.0 * 6.3367791175842285
Epoch 690, val loss: 0.701052188873291
Epoch 700, training loss: 6.443770408630371 = 0.1075483188033104 + 1.0 * 6.336222171783447
Epoch 700, val loss: 0.7038743495941162
Epoch 710, training loss: 6.434401512145996 = 0.10127422958612442 + 1.0 * 6.333127498626709
Epoch 710, val loss: 0.7070260047912598
Epoch 720, training loss: 6.429986476898193 = 0.09547992795705795 + 1.0 * 6.334506511688232
Epoch 720, val loss: 0.7104639410972595
Epoch 730, training loss: 6.421488285064697 = 0.09013069421052933 + 1.0 * 6.331357479095459
Epoch 730, val loss: 0.7141157388687134
Epoch 740, training loss: 6.418196201324463 = 0.08517640829086304 + 1.0 * 6.333019733428955
Epoch 740, val loss: 0.7179792523384094
Epoch 750, training loss: 6.40717077255249 = 0.08058909326791763 + 1.0 * 6.3265814781188965
Epoch 750, val loss: 0.7220421433448792
Epoch 760, training loss: 6.401985168457031 = 0.07632502168416977 + 1.0 * 6.325660228729248
Epoch 760, val loss: 0.7262392640113831
Epoch 770, training loss: 6.403326988220215 = 0.07236382365226746 + 1.0 * 6.330963134765625
Epoch 770, val loss: 0.7306140661239624
Epoch 780, training loss: 6.399627685546875 = 0.06868959963321686 + 1.0 * 6.33093786239624
Epoch 780, val loss: 0.7351683974266052
Epoch 790, training loss: 6.387538433074951 = 0.06529378145933151 + 1.0 * 6.322244644165039
Epoch 790, val loss: 0.7397018671035767
Epoch 800, training loss: 6.386466979980469 = 0.06213383749127388 + 1.0 * 6.324333190917969
Epoch 800, val loss: 0.7444025874137878
Epoch 810, training loss: 6.3795599937438965 = 0.05917825177311897 + 1.0 * 6.3203816413879395
Epoch 810, val loss: 0.7491779923439026
Epoch 820, training loss: 6.379547119140625 = 0.05642009153962135 + 1.0 * 6.323126792907715
Epoch 820, val loss: 0.7540017366409302
Epoch 830, training loss: 6.372452735900879 = 0.053838204592466354 + 1.0 * 6.318614482879639
Epoch 830, val loss: 0.7588596940040588
Epoch 840, training loss: 6.3709516525268555 = 0.05142257735133171 + 1.0 * 6.319529056549072
Epoch 840, val loss: 0.7638164758682251
Epoch 850, training loss: 6.368890285491943 = 0.04916160926222801 + 1.0 * 6.319728851318359
Epoch 850, val loss: 0.76875901222229
Epoch 860, training loss: 6.363083839416504 = 0.047042034566402435 + 1.0 * 6.316041946411133
Epoch 860, val loss: 0.7736302614212036
Epoch 870, training loss: 6.359103679656982 = 0.04505246505141258 + 1.0 * 6.314051151275635
Epoch 870, val loss: 0.7784968614578247
Epoch 880, training loss: 6.355291366577148 = 0.043172359466552734 + 1.0 * 6.312119007110596
Epoch 880, val loss: 0.7834210395812988
Epoch 890, training loss: 6.364387035369873 = 0.04139452055096626 + 1.0 * 6.322992324829102
Epoch 890, val loss: 0.7883056402206421
Epoch 900, training loss: 6.353616714477539 = 0.03973628580570221 + 1.0 * 6.313880443572998
Epoch 900, val loss: 0.7932486534118652
Epoch 910, training loss: 6.3508782386779785 = 0.03816669434309006 + 1.0 * 6.312711715698242
Epoch 910, val loss: 0.7980253100395203
Epoch 920, training loss: 6.351008415222168 = 0.036688610911369324 + 1.0 * 6.314319610595703
Epoch 920, val loss: 0.8028351068496704
Epoch 930, training loss: 6.344180107116699 = 0.03529340773820877 + 1.0 * 6.308886528015137
Epoch 930, val loss: 0.8076205253601074
Epoch 940, training loss: 6.340017795562744 = 0.03396616131067276 + 1.0 * 6.306051731109619
Epoch 940, val loss: 0.8123618364334106
Epoch 950, training loss: 6.346414089202881 = 0.032705217599868774 + 1.0 * 6.313708782196045
Epoch 950, val loss: 0.8170716166496277
Epoch 960, training loss: 6.342753887176514 = 0.031523093581199646 + 1.0 * 6.311230659484863
Epoch 960, val loss: 0.8217943906784058
Epoch 970, training loss: 6.3356451988220215 = 0.030396632850170135 + 1.0 * 6.305248737335205
Epoch 970, val loss: 0.8263601660728455
Epoch 980, training loss: 6.335482597351074 = 0.029331214725971222 + 1.0 * 6.306151390075684
Epoch 980, val loss: 0.8308854103088379
Epoch 990, training loss: 6.330749034881592 = 0.028316712006926537 + 1.0 * 6.302432537078857
Epoch 990, val loss: 0.8354242444038391
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.52027416229248 = 1.923439621925354 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9188532829284668
Epoch 10, training loss: 10.51097297668457 = 1.9144104719161987 + 1.0 * 8.596562385559082
Epoch 10, val loss: 1.9095085859298706
Epoch 20, training loss: 10.497529029846191 = 1.9032917022705078 + 1.0 * 8.594237327575684
Epoch 20, val loss: 1.8976263999938965
Epoch 30, training loss: 10.462372779846191 = 1.8877241611480713 + 1.0 * 8.5746488571167
Epoch 30, val loss: 1.88082754611969
Epoch 40, training loss: 10.297659873962402 = 1.867347002029419 + 1.0 * 8.430313110351562
Epoch 40, val loss: 1.8597955703735352
Epoch 50, training loss: 9.75840950012207 = 1.845823049545288 + 1.0 * 7.912586688995361
Epoch 50, val loss: 1.8390790224075317
Epoch 60, training loss: 9.250517845153809 = 1.8301297426223755 + 1.0 * 7.4203877449035645
Epoch 60, val loss: 1.8248411417007446
Epoch 70, training loss: 8.958398818969727 = 1.816982626914978 + 1.0 * 7.141416549682617
Epoch 70, val loss: 1.8126473426818848
Epoch 80, training loss: 8.802824974060059 = 1.8028841018676758 + 1.0 * 6.999940872192383
Epoch 80, val loss: 1.7991453409194946
Epoch 90, training loss: 8.692017555236816 = 1.786932110786438 + 1.0 * 6.90508508682251
Epoch 90, val loss: 1.7842379808425903
Epoch 100, training loss: 8.59463119506836 = 1.7702910900115967 + 1.0 * 6.824339866638184
Epoch 100, val loss: 1.7691105604171753
Epoch 110, training loss: 8.519499778747559 = 1.7529245615005493 + 1.0 * 6.766575336456299
Epoch 110, val loss: 1.7533067464828491
Epoch 120, training loss: 8.456124305725098 = 1.7341222763061523 + 1.0 * 6.722002029418945
Epoch 120, val loss: 1.7362512350082397
Epoch 130, training loss: 8.397099494934082 = 1.7130135297775269 + 1.0 * 6.684086322784424
Epoch 130, val loss: 1.717326045036316
Epoch 140, training loss: 8.34239387512207 = 1.6887495517730713 + 1.0 * 6.65364408493042
Epoch 140, val loss: 1.6958364248275757
Epoch 150, training loss: 8.28534984588623 = 1.6610792875289917 + 1.0 * 6.624270915985107
Epoch 150, val loss: 1.6715075969696045
Epoch 160, training loss: 8.229154586791992 = 1.6290102005004883 + 1.0 * 6.600144386291504
Epoch 160, val loss: 1.643722414970398
Epoch 170, training loss: 8.172136306762695 = 1.5916401147842407 + 1.0 * 6.580496311187744
Epoch 170, val loss: 1.6115021705627441
Epoch 180, training loss: 8.110583305358887 = 1.5486738681793213 + 1.0 * 6.5619096755981445
Epoch 180, val loss: 1.5748900175094604
Epoch 190, training loss: 8.046963691711426 = 1.5004228353500366 + 1.0 * 6.5465407371521
Epoch 190, val loss: 1.5340322256088257
Epoch 200, training loss: 7.9802021980285645 = 1.4473319053649902 + 1.0 * 6.532870292663574
Epoch 200, val loss: 1.4893548488616943
Epoch 210, training loss: 7.909528732299805 = 1.3893022537231445 + 1.0 * 6.52022647857666
Epoch 210, val loss: 1.4408841133117676
Epoch 220, training loss: 7.837347030639648 = 1.327129602432251 + 1.0 * 6.510217189788818
Epoch 220, val loss: 1.3893271684646606
Epoch 230, training loss: 7.764719009399414 = 1.26272451877594 + 1.0 * 6.501994609832764
Epoch 230, val loss: 1.3365987539291382
Epoch 240, training loss: 7.692413806915283 = 1.1987190246582031 + 1.0 * 6.49369478225708
Epoch 240, val loss: 1.2846843004226685
Epoch 250, training loss: 7.619792938232422 = 1.1362520456314087 + 1.0 * 6.483541011810303
Epoch 250, val loss: 1.2344903945922852
Epoch 260, training loss: 7.552475929260254 = 1.0756924152374268 + 1.0 * 6.476783275604248
Epoch 260, val loss: 1.1864687204360962
Epoch 270, training loss: 7.490108013153076 = 1.0180200338363647 + 1.0 * 6.472087860107422
Epoch 270, val loss: 1.1414663791656494
Epoch 280, training loss: 7.430119037628174 = 0.9640563726425171 + 1.0 * 6.466062545776367
Epoch 280, val loss: 1.099882960319519
Epoch 290, training loss: 7.376818656921387 = 0.9126542806625366 + 1.0 * 6.4641642570495605
Epoch 290, val loss: 1.060899257659912
Epoch 300, training loss: 7.318796157836914 = 0.8639501333236694 + 1.0 * 6.454845905303955
Epoch 300, val loss: 1.0241340398788452
Epoch 310, training loss: 7.267728328704834 = 0.8172568678855896 + 1.0 * 6.4504714012146
Epoch 310, val loss: 0.989202082157135
Epoch 320, training loss: 7.21960973739624 = 0.7720978856086731 + 1.0 * 6.447511672973633
Epoch 320, val loss: 0.955563485622406
Epoch 330, training loss: 7.172975063323975 = 0.728789746761322 + 1.0 * 6.444185256958008
Epoch 330, val loss: 0.9233824014663696
Epoch 340, training loss: 7.126917362213135 = 0.6872919201850891 + 1.0 * 6.439625263214111
Epoch 340, val loss: 0.8929443955421448
Epoch 350, training loss: 7.087048530578613 = 0.6480485796928406 + 1.0 * 6.439000129699707
Epoch 350, val loss: 0.8647003173828125
Epoch 360, training loss: 7.042500972747803 = 0.6110551953315735 + 1.0 * 6.431445598602295
Epoch 360, val loss: 0.8386990427970886
Epoch 370, training loss: 7.002934455871582 = 0.575975239276886 + 1.0 * 6.426959037780762
Epoch 370, val loss: 0.8148813843727112
Epoch 380, training loss: 6.968313217163086 = 0.542811930179596 + 1.0 * 6.425501346588135
Epoch 380, val loss: 0.7933036088943481
Epoch 390, training loss: 6.933429718017578 = 0.5116090774536133 + 1.0 * 6.421820640563965
Epoch 390, val loss: 0.7741316556930542
Epoch 400, training loss: 6.899977207183838 = 0.48190250992774963 + 1.0 * 6.418074607849121
Epoch 400, val loss: 0.7569950222969055
Epoch 410, training loss: 6.865782737731934 = 0.45351848006248474 + 1.0 * 6.412264347076416
Epoch 410, val loss: 0.7416002750396729
Epoch 420, training loss: 6.843123912811279 = 0.42632368206977844 + 1.0 * 6.416800022125244
Epoch 420, val loss: 0.7278478145599365
Epoch 430, training loss: 6.8091816902160645 = 0.4005531072616577 + 1.0 * 6.408628463745117
Epoch 430, val loss: 0.7156940698623657
Epoch 440, training loss: 6.782674789428711 = 0.37622183561325073 + 1.0 * 6.4064531326293945
Epoch 440, val loss: 0.7050492763519287
Epoch 450, training loss: 6.756133079528809 = 0.353142648935318 + 1.0 * 6.402990341186523
Epoch 450, val loss: 0.6957371234893799
Epoch 460, training loss: 6.730053424835205 = 0.33133190870285034 + 1.0 * 6.398721694946289
Epoch 460, val loss: 0.687553346157074
Epoch 470, training loss: 6.707651138305664 = 0.31076571345329285 + 1.0 * 6.396885395050049
Epoch 470, val loss: 0.6804311275482178
Epoch 480, training loss: 6.691137790679932 = 0.2914310693740845 + 1.0 * 6.399706840515137
Epoch 480, val loss: 0.6743532419204712
Epoch 490, training loss: 6.666279315948486 = 0.2734121084213257 + 1.0 * 6.392867088317871
Epoch 490, val loss: 0.669190526008606
Epoch 500, training loss: 6.644229412078857 = 0.2564849257469177 + 1.0 * 6.387744426727295
Epoch 500, val loss: 0.6649297475814819
Epoch 510, training loss: 6.630332946777344 = 0.2406417429447174 + 1.0 * 6.389691352844238
Epoch 510, val loss: 0.6614241003990173
Epoch 520, training loss: 6.618166446685791 = 0.22599516808986664 + 1.0 * 6.392171382904053
Epoch 520, val loss: 0.6585720777511597
Epoch 530, training loss: 6.596341133117676 = 0.2124195545911789 + 1.0 * 6.3839216232299805
Epoch 530, val loss: 0.6564098000526428
Epoch 540, training loss: 6.578583717346191 = 0.19976729154586792 + 1.0 * 6.378816604614258
Epoch 540, val loss: 0.6548925042152405
Epoch 550, training loss: 6.564040660858154 = 0.18793447315692902 + 1.0 * 6.376106262207031
Epoch 550, val loss: 0.653965175151825
Epoch 560, training loss: 6.565244674682617 = 0.1768747717142105 + 1.0 * 6.388370037078857
Epoch 560, val loss: 0.6535722613334656
Epoch 570, training loss: 6.544929504394531 = 0.16664233803749084 + 1.0 * 6.378287315368652
Epoch 570, val loss: 0.6536591649055481
Epoch 580, training loss: 6.529237747192383 = 0.1571740210056305 + 1.0 * 6.372063636779785
Epoch 580, val loss: 0.6541591882705688
Epoch 590, training loss: 6.517226219177246 = 0.14832577109336853 + 1.0 * 6.368900299072266
Epoch 590, val loss: 0.6550825238227844
Epoch 600, training loss: 6.50826358795166 = 0.14004424214363098 + 1.0 * 6.368219375610352
Epoch 600, val loss: 0.6563341617584229
Epoch 610, training loss: 6.499069690704346 = 0.13231375813484192 + 1.0 * 6.366755962371826
Epoch 610, val loss: 0.6579094529151917
Epoch 620, training loss: 6.490962028503418 = 0.12511198222637177 + 1.0 * 6.36584997177124
Epoch 620, val loss: 0.6598061323165894
Epoch 630, training loss: 6.490532875061035 = 0.11839769780635834 + 1.0 * 6.372135162353516
Epoch 630, val loss: 0.6620157957077026
Epoch 640, training loss: 6.47617769241333 = 0.11215075105428696 + 1.0 * 6.36402702331543
Epoch 640, val loss: 0.6642739176750183
Epoch 650, training loss: 6.4648942947387695 = 0.1063229963183403 + 1.0 * 6.358571529388428
Epoch 650, val loss: 0.6669137477874756
Epoch 660, training loss: 6.457398891448975 = 0.10084665566682816 + 1.0 * 6.3565521240234375
Epoch 660, val loss: 0.6697702407836914
Epoch 670, training loss: 6.450867176055908 = 0.09569702297449112 + 1.0 * 6.355170249938965
Epoch 670, val loss: 0.6728368997573853
Epoch 680, training loss: 6.465466022491455 = 0.09087266772985458 + 1.0 * 6.374593257904053
Epoch 680, val loss: 0.6760704517364502
Epoch 690, training loss: 6.442221164703369 = 0.08635970205068588 + 1.0 * 6.355861663818359
Epoch 690, val loss: 0.6793307662010193
Epoch 700, training loss: 6.434000492095947 = 0.08214227855205536 + 1.0 * 6.351858139038086
Epoch 700, val loss: 0.682817280292511
Epoch 710, training loss: 6.428954601287842 = 0.07818014174699783 + 1.0 * 6.35077428817749
Epoch 710, val loss: 0.686440110206604
Epoch 720, training loss: 6.428352355957031 = 0.07445244491100311 + 1.0 * 6.353899955749512
Epoch 720, val loss: 0.6901376247406006
Epoch 730, training loss: 6.419677257537842 = 0.07094970345497131 + 1.0 * 6.348727703094482
Epoch 730, val loss: 0.6939790844917297
Epoch 740, training loss: 6.41567850112915 = 0.06765322387218475 + 1.0 * 6.348025321960449
Epoch 740, val loss: 0.6979324817657471
Epoch 750, training loss: 6.416832447052002 = 0.06455901265144348 + 1.0 * 6.352273464202881
Epoch 750, val loss: 0.7018893361091614
Epoch 760, training loss: 6.406951904296875 = 0.061667218804359436 + 1.0 * 6.345284461975098
Epoch 760, val loss: 0.7057953476905823
Epoch 770, training loss: 6.400973796844482 = 0.058941904455423355 + 1.0 * 6.342031955718994
Epoch 770, val loss: 0.7098937034606934
Epoch 780, training loss: 6.3959479331970215 = 0.05636139586567879 + 1.0 * 6.3395867347717285
Epoch 780, val loss: 0.7140451073646545
Epoch 790, training loss: 6.4133405685424805 = 0.053922947496175766 + 1.0 * 6.35941743850708
Epoch 790, val loss: 0.7182426452636719
Epoch 800, training loss: 6.390398979187012 = 0.05163219943642616 + 1.0 * 6.338766574859619
Epoch 800, val loss: 0.7222755551338196
Epoch 810, training loss: 6.388226509094238 = 0.04947776347398758 + 1.0 * 6.338748931884766
Epoch 810, val loss: 0.7264276146888733
Epoch 820, training loss: 6.382569789886475 = 0.04743807762861252 + 1.0 * 6.335131645202637
Epoch 820, val loss: 0.7306748032569885
Epoch 830, training loss: 6.382540702819824 = 0.04550936445593834 + 1.0 * 6.337031364440918
Epoch 830, val loss: 0.7348200082778931
Epoch 840, training loss: 6.3812174797058105 = 0.043694738298654556 + 1.0 * 6.337522506713867
Epoch 840, val loss: 0.738941490650177
Epoch 850, training loss: 6.37465238571167 = 0.04197799414396286 + 1.0 * 6.332674503326416
Epoch 850, val loss: 0.7431623935699463
Epoch 860, training loss: 6.372766971588135 = 0.04034552723169327 + 1.0 * 6.33242130279541
Epoch 860, val loss: 0.7473469376564026
Epoch 870, training loss: 6.368056297302246 = 0.038805533200502396 + 1.0 * 6.329250812530518
Epoch 870, val loss: 0.7515132427215576
Epoch 880, training loss: 6.368972301483154 = 0.03734786435961723 + 1.0 * 6.331624507904053
Epoch 880, val loss: 0.7555765509605408
Epoch 890, training loss: 6.363765239715576 = 0.03597068041563034 + 1.0 * 6.327794551849365
Epoch 890, val loss: 0.7597945332527161
Epoch 900, training loss: 6.360612869262695 = 0.034653645008802414 + 1.0 * 6.325959205627441
Epoch 900, val loss: 0.7639289498329163
Epoch 910, training loss: 6.375092506408691 = 0.03340206667780876 + 1.0 * 6.341690540313721
Epoch 910, val loss: 0.7680943012237549
Epoch 920, training loss: 6.357897758483887 = 0.03222107142210007 + 1.0 * 6.325676918029785
Epoch 920, val loss: 0.7720775604248047
Epoch 930, training loss: 6.356631278991699 = 0.031099820509552956 + 1.0 * 6.325531482696533
Epoch 930, val loss: 0.7760701775550842
Epoch 940, training loss: 6.351968765258789 = 0.030034972354769707 + 1.0 * 6.321933746337891
Epoch 940, val loss: 0.7801560163497925
Epoch 950, training loss: 6.350978851318359 = 0.029016274958848953 + 1.0 * 6.321962356567383
Epoch 950, val loss: 0.7842123508453369
Epoch 960, training loss: 6.3628668785095215 = 0.02804603986442089 + 1.0 * 6.334820747375488
Epoch 960, val loss: 0.7881954908370972
Epoch 970, training loss: 6.346524715423584 = 0.027126315981149673 + 1.0 * 6.319398403167725
Epoch 970, val loss: 0.7920276522636414
Epoch 980, training loss: 6.3459601402282715 = 0.02625296264886856 + 1.0 * 6.31970739364624
Epoch 980, val loss: 0.7958792448043823
Epoch 990, training loss: 6.3437628746032715 = 0.025419604033231735 + 1.0 * 6.318343162536621
Epoch 990, val loss: 0.7998439073562622
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8408012651555088
The final CL Acc:0.80864, 0.01666, The final GNN Acc:0.83834, 0.00217
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11546])
remove edge: torch.Size([2, 9436])
updated graph: torch.Size([2, 10426])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.537269592285156 = 1.9403976202011108 + 1.0 * 8.596872329711914
Epoch 0, val loss: 1.9368195533752441
Epoch 10, training loss: 10.527446746826172 = 1.9307529926300049 + 1.0 * 8.596693992614746
Epoch 10, val loss: 1.9274771213531494
Epoch 20, training loss: 10.513923645019531 = 1.9187731742858887 + 1.0 * 8.595149993896484
Epoch 20, val loss: 1.9153941869735718
Epoch 30, training loss: 10.483327865600586 = 1.9022778272628784 + 1.0 * 8.581049919128418
Epoch 30, val loss: 1.8983129262924194
Epoch 40, training loss: 10.37060832977295 = 1.8797581195831299 + 1.0 * 8.490850448608398
Epoch 40, val loss: 1.875311017036438
Epoch 50, training loss: 9.9880952835083 = 1.8541122674942017 + 1.0 * 8.13398265838623
Epoch 50, val loss: 1.8503944873809814
Epoch 60, training loss: 9.737251281738281 = 1.8319401741027832 + 1.0 * 7.90531063079834
Epoch 60, val loss: 1.8296453952789307
Epoch 70, training loss: 9.29167366027832 = 1.8160123825073242 + 1.0 * 7.475661754608154
Epoch 70, val loss: 1.8150497674942017
Epoch 80, training loss: 8.933744430541992 = 1.8042585849761963 + 1.0 * 7.129486083984375
Epoch 80, val loss: 1.804019808769226
Epoch 90, training loss: 8.735103607177734 = 1.79132878780365 + 1.0 * 6.943774700164795
Epoch 90, val loss: 1.7916077375411987
Epoch 100, training loss: 8.639759063720703 = 1.7766876220703125 + 1.0 * 6.863071441650391
Epoch 100, val loss: 1.7785707712173462
Epoch 110, training loss: 8.565581321716309 = 1.7626217603683472 + 1.0 * 6.802959442138672
Epoch 110, val loss: 1.7662547826766968
Epoch 120, training loss: 8.500732421875 = 1.7483580112457275 + 1.0 * 6.752374172210693
Epoch 120, val loss: 1.75342857837677
Epoch 130, training loss: 8.437612533569336 = 1.7330164909362793 + 1.0 * 6.704596042633057
Epoch 130, val loss: 1.7393701076507568
Epoch 140, training loss: 8.379231452941895 = 1.7159279584884644 + 1.0 * 6.663303852081299
Epoch 140, val loss: 1.7239402532577515
Epoch 150, training loss: 8.325800895690918 = 1.6962804794311523 + 1.0 * 6.629520416259766
Epoch 150, val loss: 1.7065614461898804
Epoch 160, training loss: 8.271781921386719 = 1.6739115715026855 + 1.0 * 6.597870826721191
Epoch 160, val loss: 1.6870750188827515
Epoch 170, training loss: 8.2179594039917 = 1.6484092473983765 + 1.0 * 6.569550514221191
Epoch 170, val loss: 1.6649848222732544
Epoch 180, training loss: 8.171717643737793 = 1.6192082166671753 + 1.0 * 6.552509784698486
Epoch 180, val loss: 1.6398460865020752
Epoch 190, training loss: 8.114262580871582 = 1.5863357782363892 + 1.0 * 6.527926445007324
Epoch 190, val loss: 1.6117379665374756
Epoch 200, training loss: 8.061493873596191 = 1.549737811088562 + 1.0 * 6.51175594329834
Epoch 200, val loss: 1.5807113647460938
Epoch 210, training loss: 8.007649421691895 = 1.5093730688095093 + 1.0 * 6.498276233673096
Epoch 210, val loss: 1.5471405982971191
Epoch 220, training loss: 7.964052200317383 = 1.4656929969787598 + 1.0 * 6.498359203338623
Epoch 220, val loss: 1.5114493370056152
Epoch 230, training loss: 7.899426460266113 = 1.4206528663635254 + 1.0 * 6.478773593902588
Epoch 230, val loss: 1.4756903648376465
Epoch 240, training loss: 7.844163417816162 = 1.3745125532150269 + 1.0 * 6.469650745391846
Epoch 240, val loss: 1.439842700958252
Epoch 250, training loss: 7.789071559906006 = 1.3275622129440308 + 1.0 * 6.4615092277526855
Epoch 250, val loss: 1.4044114351272583
Epoch 260, training loss: 7.743258953094482 = 1.2802976369857788 + 1.0 * 6.462961196899414
Epoch 260, val loss: 1.369761347770691
Epoch 270, training loss: 7.684291362762451 = 1.2339481115341187 + 1.0 * 6.450343132019043
Epoch 270, val loss: 1.3364062309265137
Epoch 280, training loss: 7.630825042724609 = 1.187990427017212 + 1.0 * 6.442834854125977
Epoch 280, val loss: 1.3040454387664795
Epoch 290, training loss: 7.580724716186523 = 1.1425265073776245 + 1.0 * 6.438198089599609
Epoch 290, val loss: 1.2724381685256958
Epoch 300, training loss: 7.528982639312744 = 1.0976896286010742 + 1.0 * 6.43129301071167
Epoch 300, val loss: 1.2415772676467896
Epoch 310, training loss: 7.480588912963867 = 1.053189992904663 + 1.0 * 6.427398681640625
Epoch 310, val loss: 1.2112494707107544
Epoch 320, training loss: 7.440568923950195 = 1.0092079639434814 + 1.0 * 6.431360721588135
Epoch 320, val loss: 1.1814805269241333
Epoch 330, training loss: 7.386966228485107 = 0.9662628173828125 + 1.0 * 6.420703411102295
Epoch 330, val loss: 1.152590274810791
Epoch 340, training loss: 7.340569496154785 = 0.9241412878036499 + 1.0 * 6.416428089141846
Epoch 340, val loss: 1.124430775642395
Epoch 350, training loss: 7.295825004577637 = 0.8831314444541931 + 1.0 * 6.412693500518799
Epoch 350, val loss: 1.0973111391067505
Epoch 360, training loss: 7.252246379852295 = 0.8433249592781067 + 1.0 * 6.408921241760254
Epoch 360, val loss: 1.0712933540344238
Epoch 370, training loss: 7.211567401885986 = 0.8045714497566223 + 1.0 * 6.40699577331543
Epoch 370, val loss: 1.0461208820343018
Epoch 380, training loss: 7.171327114105225 = 0.7670578956604004 + 1.0 * 6.404269218444824
Epoch 380, val loss: 1.0223991870880127
Epoch 390, training loss: 7.129422664642334 = 0.7308455109596252 + 1.0 * 6.3985772132873535
Epoch 390, val loss: 0.9998926520347595
Epoch 400, training loss: 7.09663724899292 = 0.6957401633262634 + 1.0 * 6.400897026062012
Epoch 400, val loss: 0.9786553382873535
Epoch 410, training loss: 7.0585103034973145 = 0.6620564460754395 + 1.0 * 6.396453857421875
Epoch 410, val loss: 0.9589572548866272
Epoch 420, training loss: 7.020757675170898 = 0.6296665668487549 + 1.0 * 6.3910908699035645
Epoch 420, val loss: 0.9406723976135254
Epoch 430, training loss: 6.986355304718018 = 0.5982237458229065 + 1.0 * 6.388131618499756
Epoch 430, val loss: 0.9237148761749268
Epoch 440, training loss: 6.958287715911865 = 0.5676932334899902 + 1.0 * 6.390594482421875
Epoch 440, val loss: 0.9078492522239685
Epoch 450, training loss: 6.925021648406982 = 0.5381385684013367 + 1.0 * 6.38688325881958
Epoch 450, val loss: 0.8932127952575684
Epoch 460, training loss: 6.889385223388672 = 0.5094187259674072 + 1.0 * 6.3799662590026855
Epoch 460, val loss: 0.8796337842941284
Epoch 470, training loss: 6.858745098114014 = 0.4812484383583069 + 1.0 * 6.377496719360352
Epoch 470, val loss: 0.8671623468399048
Epoch 480, training loss: 6.839663982391357 = 0.453596293926239 + 1.0 * 6.386067867279053
Epoch 480, val loss: 0.8554070591926575
Epoch 490, training loss: 6.801425457000732 = 0.4267828166484833 + 1.0 * 6.374642848968506
Epoch 490, val loss: 0.844573438167572
Epoch 500, training loss: 6.7717814445495605 = 0.4005650281906128 + 1.0 * 6.371216297149658
Epoch 500, val loss: 0.8347442746162415
Epoch 510, training loss: 6.744551658630371 = 0.37496355175971985 + 1.0 * 6.3695878982543945
Epoch 510, val loss: 0.8256424069404602
Epoch 520, training loss: 6.7224602699279785 = 0.3501797914505005 + 1.0 * 6.372280597686768
Epoch 520, val loss: 0.8171064853668213
Epoch 530, training loss: 6.691769599914551 = 0.32651442289352417 + 1.0 * 6.365255355834961
Epoch 530, val loss: 0.8096147179603577
Epoch 540, training loss: 6.666229248046875 = 0.30384403467178345 + 1.0 * 6.362385272979736
Epoch 540, val loss: 0.8030456304550171
Epoch 550, training loss: 6.646923542022705 = 0.2822463810443878 + 1.0 * 6.3646769523620605
Epoch 550, val loss: 0.7973790764808655
Epoch 560, training loss: 6.626809597015381 = 0.26198622584342957 + 1.0 * 6.364823341369629
Epoch 560, val loss: 0.7926049828529358
Epoch 570, training loss: 6.601505279541016 = 0.24306707084178925 + 1.0 * 6.358438014984131
Epoch 570, val loss: 0.7888404726982117
Epoch 580, training loss: 6.585328578948975 = 0.22550831735134125 + 1.0 * 6.359820365905762
Epoch 580, val loss: 0.7862008810043335
Epoch 590, training loss: 6.5643486976623535 = 0.20930038392543793 + 1.0 * 6.355048179626465
Epoch 590, val loss: 0.784645140171051
Epoch 600, training loss: 6.545941352844238 = 0.19431500136852264 + 1.0 * 6.351626396179199
Epoch 600, val loss: 0.7840947508811951
Epoch 610, training loss: 6.536694526672363 = 0.18050099909305573 + 1.0 * 6.356193542480469
Epoch 610, val loss: 0.7844958901405334
Epoch 620, training loss: 6.5230841636657715 = 0.16785773634910583 + 1.0 * 6.355226516723633
Epoch 620, val loss: 0.7857738733291626
Epoch 630, training loss: 6.507969379425049 = 0.15633629262447357 + 1.0 * 6.351633071899414
Epoch 630, val loss: 0.7878928780555725
Epoch 640, training loss: 6.4920783042907715 = 0.145805224776268 + 1.0 * 6.346272945404053
Epoch 640, val loss: 0.7907552123069763
Epoch 650, training loss: 6.485937595367432 = 0.13614489138126373 + 1.0 * 6.34979248046875
Epoch 650, val loss: 0.7943060994148254
Epoch 660, training loss: 6.476193904876709 = 0.1272992342710495 + 1.0 * 6.3488945960998535
Epoch 660, val loss: 0.7984459400177002
Epoch 670, training loss: 6.461033344268799 = 0.11921663582324982 + 1.0 * 6.3418169021606445
Epoch 670, val loss: 0.8030033111572266
Epoch 680, training loss: 6.452176094055176 = 0.11176518350839615 + 1.0 * 6.3404107093811035
Epoch 680, val loss: 0.8081520199775696
Epoch 690, training loss: 6.452953338623047 = 0.1049095168709755 + 1.0 * 6.348043918609619
Epoch 690, val loss: 0.8135248422622681
Epoch 700, training loss: 6.435323238372803 = 0.09862998127937317 + 1.0 * 6.336693286895752
Epoch 700, val loss: 0.8193384408950806
Epoch 710, training loss: 6.4283857345581055 = 0.09283387660980225 + 1.0 * 6.335551738739014
Epoch 710, val loss: 0.8254353404045105
Epoch 720, training loss: 6.430767059326172 = 0.08747716248035431 + 1.0 * 6.343289852142334
Epoch 720, val loss: 0.8316054344177246
Epoch 730, training loss: 6.416077613830566 = 0.08255349844694138 + 1.0 * 6.333524227142334
Epoch 730, val loss: 0.8379877805709839
Epoch 740, training loss: 6.409427642822266 = 0.0779924988746643 + 1.0 * 6.331435203552246
Epoch 740, val loss: 0.8445379137992859
Epoch 750, training loss: 6.4095587730407715 = 0.07375907152891159 + 1.0 * 6.335799694061279
Epoch 750, val loss: 0.8511165976524353
Epoch 760, training loss: 6.401645183563232 = 0.06983895599842072 + 1.0 * 6.331806182861328
Epoch 760, val loss: 0.8577839136123657
Epoch 770, training loss: 6.395256042480469 = 0.06620520353317261 + 1.0 * 6.3290510177612305
Epoch 770, val loss: 0.8644944429397583
Epoch 780, training loss: 6.39472770690918 = 0.06282150000333786 + 1.0 * 6.331906318664551
Epoch 780, val loss: 0.8711838722229004
Epoch 790, training loss: 6.3869733810424805 = 0.05968300998210907 + 1.0 * 6.3272905349731445
Epoch 790, val loss: 0.8779251575469971
Epoch 800, training loss: 6.38041353225708 = 0.05675550177693367 + 1.0 * 6.323657989501953
Epoch 800, val loss: 0.884686291217804
Epoch 810, training loss: 6.378698825836182 = 0.05402012914419174 + 1.0 * 6.324678897857666
Epoch 810, val loss: 0.8913393020629883
Epoch 820, training loss: 6.3802809715271 = 0.051463548094034195 + 1.0 * 6.328817367553711
Epoch 820, val loss: 0.8980697393417358
Epoch 830, training loss: 6.3711371421813965 = 0.049081094563007355 + 1.0 * 6.322055816650391
Epoch 830, val loss: 0.904764711856842
Epoch 840, training loss: 6.366966247558594 = 0.04684912785887718 + 1.0 * 6.320116996765137
Epoch 840, val loss: 0.9113114476203918
Epoch 850, training loss: 6.363412857055664 = 0.044753339141607285 + 1.0 * 6.31865930557251
Epoch 850, val loss: 0.9179348349571228
Epoch 860, training loss: 6.368622779846191 = 0.04278424382209778 + 1.0 * 6.325838565826416
Epoch 860, val loss: 0.9244058728218079
Epoch 870, training loss: 6.3682708740234375 = 0.040937673300504684 + 1.0 * 6.327332973480225
Epoch 870, val loss: 0.9307746887207031
Epoch 880, training loss: 6.360666275024414 = 0.039225056767463684 + 1.0 * 6.321441173553467
Epoch 880, val loss: 0.9370505213737488
Epoch 890, training loss: 6.353744983673096 = 0.03761262819170952 + 1.0 * 6.316132545471191
Epoch 890, val loss: 0.9432663917541504
Epoch 900, training loss: 6.349998950958252 = 0.036094240844249725 + 1.0 * 6.313904762268066
Epoch 900, val loss: 0.9495413303375244
Epoch 910, training loss: 6.348644733428955 = 0.03465297073125839 + 1.0 * 6.313991546630859
Epoch 910, val loss: 0.9557274580001831
Epoch 920, training loss: 6.346973896026611 = 0.03329416736960411 + 1.0 * 6.3136796951293945
Epoch 920, val loss: 0.9617283344268799
Epoch 930, training loss: 6.342506408691406 = 0.03201700747013092 + 1.0 * 6.310489177703857
Epoch 930, val loss: 0.9678401350975037
Epoch 940, training loss: 6.342747688293457 = 0.030810263007879257 + 1.0 * 6.31193733215332
Epoch 940, val loss: 0.9738489389419556
Epoch 950, training loss: 6.341156482696533 = 0.029668230563402176 + 1.0 * 6.311488151550293
Epoch 950, val loss: 0.9795452952384949
Epoch 960, training loss: 6.3380255699157715 = 0.028591115027666092 + 1.0 * 6.309434413909912
Epoch 960, val loss: 0.9853875041007996
Epoch 970, training loss: 6.338099002838135 = 0.0275699645280838 + 1.0 * 6.3105292320251465
Epoch 970, val loss: 0.9910184741020203
Epoch 980, training loss: 6.335418224334717 = 0.026600558310747147 + 1.0 * 6.3088178634643555
Epoch 980, val loss: 0.9966040849685669
Epoch 990, training loss: 6.332696914672852 = 0.025677064433693886 + 1.0 * 6.3070197105407715
Epoch 990, val loss: 1.0021988153457642
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 10.515104293823242 = 1.9182356595993042 + 1.0 * 8.596868515014648
Epoch 0, val loss: 1.9167455434799194
Epoch 10, training loss: 10.506287574768066 = 1.9095830917358398 + 1.0 * 8.596704483032227
Epoch 10, val loss: 1.9084762334823608
Epoch 20, training loss: 10.494370460510254 = 1.898857831954956 + 1.0 * 8.595512390136719
Epoch 20, val loss: 1.8978241682052612
Epoch 30, training loss: 10.469252586364746 = 1.8838499784469604 + 1.0 * 8.585402488708496
Epoch 30, val loss: 1.8825453519821167
Epoch 40, training loss: 10.378984451293945 = 1.8632445335388184 + 1.0 * 8.515740394592285
Epoch 40, val loss: 1.861711859703064
Epoch 50, training loss: 9.967296600341797 = 1.8401259183883667 + 1.0 * 8.12717056274414
Epoch 50, val loss: 1.8390365839004517
Epoch 60, training loss: 9.513345718383789 = 1.8189387321472168 + 1.0 * 7.694406509399414
Epoch 60, val loss: 1.8190969228744507
Epoch 70, training loss: 9.062470436096191 = 1.8045698404312134 + 1.0 * 7.257900238037109
Epoch 70, val loss: 1.8052419424057007
Epoch 80, training loss: 8.813990592956543 = 1.7941306829452515 + 1.0 * 7.019859790802002
Epoch 80, val loss: 1.7943894863128662
Epoch 90, training loss: 8.696256637573242 = 1.7804152965545654 + 1.0 * 6.915841102600098
Epoch 90, val loss: 1.7807708978652954
Epoch 100, training loss: 8.598505973815918 = 1.7646297216415405 + 1.0 * 6.833876132965088
Epoch 100, val loss: 1.7661266326904297
Epoch 110, training loss: 8.525920867919922 = 1.7490590810775757 + 1.0 * 6.776861667633057
Epoch 110, val loss: 1.7517083883285522
Epoch 120, training loss: 8.467626571655273 = 1.732382893562317 + 1.0 * 6.735243320465088
Epoch 120, val loss: 1.7359650135040283
Epoch 130, training loss: 8.415681838989258 = 1.712766408920288 + 1.0 * 6.702915668487549
Epoch 130, val loss: 1.7178033590316772
Epoch 140, training loss: 8.363176345825195 = 1.6895463466644287 + 1.0 * 6.673630237579346
Epoch 140, val loss: 1.697013020515442
Epoch 150, training loss: 8.309715270996094 = 1.6624301671981812 + 1.0 * 6.647284984588623
Epoch 150, val loss: 1.6733444929122925
Epoch 160, training loss: 8.248170852661133 = 1.6315850019454956 + 1.0 * 6.616585731506348
Epoch 160, val loss: 1.646359920501709
Epoch 170, training loss: 8.18758773803711 = 1.595808982849121 + 1.0 * 6.5917792320251465
Epoch 170, val loss: 1.615181565284729
Epoch 180, training loss: 8.126569747924805 = 1.5544357299804688 + 1.0 * 6.572133541107178
Epoch 180, val loss: 1.57906973361969
Epoch 190, training loss: 8.061071395874023 = 1.5080336332321167 + 1.0 * 6.553038120269775
Epoch 190, val loss: 1.5391817092895508
Epoch 200, training loss: 7.995415687561035 = 1.456917405128479 + 1.0 * 6.538498401641846
Epoch 200, val loss: 1.4959696531295776
Epoch 210, training loss: 7.924187660217285 = 1.4032336473464966 + 1.0 * 6.520954132080078
Epoch 210, val loss: 1.4515233039855957
Epoch 220, training loss: 7.8549909591674805 = 1.3481212854385376 + 1.0 * 6.506869792938232
Epoch 220, val loss: 1.4072391986846924
Epoch 230, training loss: 7.791183948516846 = 1.2932568788528442 + 1.0 * 6.497927188873291
Epoch 230, val loss: 1.364581823348999
Epoch 240, training loss: 7.7256317138671875 = 1.2397899627685547 + 1.0 * 6.485841751098633
Epoch 240, val loss: 1.3249844312667847
Epoch 250, training loss: 7.6676716804504395 = 1.1877585649490356 + 1.0 * 6.479913234710693
Epoch 250, val loss: 1.2882232666015625
Epoch 260, training loss: 7.60752534866333 = 1.137752652168274 + 1.0 * 6.469772815704346
Epoch 260, val loss: 1.2542598247528076
Epoch 270, training loss: 7.551876068115234 = 1.0892850160598755 + 1.0 * 6.462591171264648
Epoch 270, val loss: 1.2229617834091187
Epoch 280, training loss: 7.50022029876709 = 1.0425525903701782 + 1.0 * 6.457667827606201
Epoch 280, val loss: 1.1938515901565552
Epoch 290, training loss: 7.449568271636963 = 0.9979181289672852 + 1.0 * 6.451650142669678
Epoch 290, val loss: 1.1671348810195923
Epoch 300, training loss: 7.3995208740234375 = 0.9544378519058228 + 1.0 * 6.445083141326904
Epoch 300, val loss: 1.141758680343628
Epoch 310, training loss: 7.354525089263916 = 0.911631166934967 + 1.0 * 6.442893981933594
Epoch 310, val loss: 1.1170493364334106
Epoch 320, training loss: 7.30504846572876 = 0.869875431060791 + 1.0 * 6.435173034667969
Epoch 320, val loss: 1.0930564403533936
Epoch 330, training loss: 7.259974002838135 = 0.8288396000862122 + 1.0 * 6.431134223937988
Epoch 330, val loss: 1.0697517395019531
Epoch 340, training loss: 7.21568489074707 = 0.7883186936378479 + 1.0 * 6.427366256713867
Epoch 340, val loss: 1.0469170808792114
Epoch 350, training loss: 7.169604301452637 = 0.7477626204490662 + 1.0 * 6.421841621398926
Epoch 350, val loss: 1.0242786407470703
Epoch 360, training loss: 7.133405685424805 = 0.707146167755127 + 1.0 * 6.426259517669678
Epoch 360, val loss: 1.0017719268798828
Epoch 370, training loss: 7.083532810211182 = 0.667023241519928 + 1.0 * 6.416509628295898
Epoch 370, val loss: 0.9799268841743469
Epoch 380, training loss: 7.043866157531738 = 0.6273011565208435 + 1.0 * 6.41656494140625
Epoch 380, val loss: 0.9587836265563965
Epoch 390, training loss: 6.997588157653809 = 0.5884093046188354 + 1.0 * 6.409178733825684
Epoch 390, val loss: 0.938475489616394
Epoch 400, training loss: 6.955406188964844 = 0.5501990914344788 + 1.0 * 6.40520715713501
Epoch 400, val loss: 0.9194245934486389
Epoch 410, training loss: 6.918500900268555 = 0.5128978490829468 + 1.0 * 6.405602931976318
Epoch 410, val loss: 0.9013858437538147
Epoch 420, training loss: 6.878119468688965 = 0.47701361775398254 + 1.0 * 6.401105880737305
Epoch 420, val loss: 0.885136604309082
Epoch 430, training loss: 6.839840888977051 = 0.442408949136734 + 1.0 * 6.39743185043335
Epoch 430, val loss: 0.8704740405082703
Epoch 440, training loss: 6.808053016662598 = 0.40938127040863037 + 1.0 * 6.398671627044678
Epoch 440, val loss: 0.8573184609413147
Epoch 450, training loss: 6.771872520446777 = 0.3784139156341553 + 1.0 * 6.393458843231201
Epoch 450, val loss: 0.8463889956474304
Epoch 460, training loss: 6.739015102386475 = 0.3493267595767975 + 1.0 * 6.389688491821289
Epoch 460, val loss: 0.8371381163597107
Epoch 470, training loss: 6.713158130645752 = 0.32209357619285583 + 1.0 * 6.391064643859863
Epoch 470, val loss: 0.8296347260475159
Epoch 480, training loss: 6.685220718383789 = 0.29693251848220825 + 1.0 * 6.3882880210876465
Epoch 480, val loss: 0.8238701224327087
Epoch 490, training loss: 6.67893648147583 = 0.27383318543434143 + 1.0 * 6.4051032066345215
Epoch 490, val loss: 0.8197493553161621
Epoch 500, training loss: 6.639697074890137 = 0.25296109914779663 + 1.0 * 6.386735916137695
Epoch 500, val loss: 0.817055881023407
Epoch 510, training loss: 6.613033771514893 = 0.2339402586221695 + 1.0 * 6.379093647003174
Epoch 510, val loss: 0.8158313035964966
Epoch 520, training loss: 6.592170238494873 = 0.21652533113956451 + 1.0 * 6.375644683837891
Epoch 520, val loss: 0.8156009912490845
Epoch 530, training loss: 6.573881149291992 = 0.20056377351284027 + 1.0 * 6.373317241668701
Epoch 530, val loss: 0.8164628148078918
Epoch 540, training loss: 6.5588765144348145 = 0.18600696325302124 + 1.0 * 6.372869491577148
Epoch 540, val loss: 0.8180860877037048
Epoch 550, training loss: 6.543967247009277 = 0.1728287935256958 + 1.0 * 6.371138572692871
Epoch 550, val loss: 0.8205977082252502
Epoch 560, training loss: 6.528781890869141 = 0.16079507768154144 + 1.0 * 6.367986679077148
Epoch 560, val loss: 0.8236653208732605
Epoch 570, training loss: 6.527716159820557 = 0.14980606734752655 + 1.0 * 6.377910137176514
Epoch 570, val loss: 0.8271969556808472
Epoch 580, training loss: 6.504870891571045 = 0.13982738554477692 + 1.0 * 6.365043640136719
Epoch 580, val loss: 0.8313371539115906
Epoch 590, training loss: 6.492798805236816 = 0.13069672882556915 + 1.0 * 6.362102031707764
Epoch 590, val loss: 0.8358412384986877
Epoch 600, training loss: 6.482574939727783 = 0.1223444938659668 + 1.0 * 6.360230445861816
Epoch 600, val loss: 0.8405304551124573
Epoch 610, training loss: 6.473639011383057 = 0.11470392346382141 + 1.0 * 6.3589348793029785
Epoch 610, val loss: 0.845711886882782
Epoch 620, training loss: 6.464125633239746 = 0.10766325891017914 + 1.0 * 6.356462478637695
Epoch 620, val loss: 0.8510366678237915
Epoch 630, training loss: 6.460655689239502 = 0.10117106884717941 + 1.0 * 6.359484672546387
Epoch 630, val loss: 0.8565280437469482
Epoch 640, training loss: 6.446959495544434 = 0.09518593549728394 + 1.0 * 6.351773738861084
Epoch 640, val loss: 0.8623515963554382
Epoch 650, training loss: 6.439336776733398 = 0.08964808285236359 + 1.0 * 6.349688529968262
Epoch 650, val loss: 0.8682895302772522
Epoch 660, training loss: 6.440674304962158 = 0.084516242146492 + 1.0 * 6.356158256530762
Epoch 660, val loss: 0.8743116855621338
Epoch 670, training loss: 6.428477764129639 = 0.07977700978517532 + 1.0 * 6.348700523376465
Epoch 670, val loss: 0.8805361986160278
Epoch 680, training loss: 6.419416904449463 = 0.07538348436355591 + 1.0 * 6.344033241271973
Epoch 680, val loss: 0.8868885636329651
Epoch 690, training loss: 6.418615341186523 = 0.07128886133432388 + 1.0 * 6.347326278686523
Epoch 690, val loss: 0.8932011127471924
Epoch 700, training loss: 6.420186519622803 = 0.06750330328941345 + 1.0 * 6.352683067321777
Epoch 700, val loss: 0.8995742201805115
Epoch 710, training loss: 6.402978420257568 = 0.06399969011545181 + 1.0 * 6.3389787673950195
Epoch 710, val loss: 0.9061310887336731
Epoch 720, training loss: 6.399263381958008 = 0.060729727149009705 + 1.0 * 6.338533878326416
Epoch 720, val loss: 0.9125938415527344
Epoch 730, training loss: 6.401505470275879 = 0.05767465755343437 + 1.0 * 6.343830585479736
Epoch 730, val loss: 0.918985903263092
Epoch 740, training loss: 6.394476413726807 = 0.05483219027519226 + 1.0 * 6.339644432067871
Epoch 740, val loss: 0.9255572557449341
Epoch 750, training loss: 6.3877387046813965 = 0.05217934399843216 + 1.0 * 6.335559368133545
Epoch 750, val loss: 0.9320539832115173
Epoch 760, training loss: 6.381807327270508 = 0.049698252230882645 + 1.0 * 6.332108974456787
Epoch 760, val loss: 0.9385724067687988
Epoch 770, training loss: 6.386236667633057 = 0.04737211763858795 + 1.0 * 6.338864326477051
Epoch 770, val loss: 0.9449893236160278
Epoch 780, training loss: 6.37679386138916 = 0.04520432651042938 + 1.0 * 6.331589698791504
Epoch 780, val loss: 0.9514212012290955
Epoch 790, training loss: 6.372178554534912 = 0.043174393475055695 + 1.0 * 6.329004287719727
Epoch 790, val loss: 0.957955002784729
Epoch 800, training loss: 6.377223491668701 = 0.041269343346357346 + 1.0 * 6.335954189300537
Epoch 800, val loss: 0.9642547369003296
Epoch 810, training loss: 6.370832443237305 = 0.03949142247438431 + 1.0 * 6.331340789794922
Epoch 810, val loss: 0.9705738425254822
Epoch 820, training loss: 6.362320423126221 = 0.037819135934114456 + 1.0 * 6.3245015144348145
Epoch 820, val loss: 0.976967453956604
Epoch 830, training loss: 6.359046936035156 = 0.03624332696199417 + 1.0 * 6.322803497314453
Epoch 830, val loss: 0.9832015633583069
Epoch 840, training loss: 6.366119861602783 = 0.03475525975227356 + 1.0 * 6.331364631652832
Epoch 840, val loss: 0.9893508553504944
Epoch 850, training loss: 6.36115026473999 = 0.03336549550294876 + 1.0 * 6.327784538269043
Epoch 850, val loss: 0.9954441785812378
Epoch 860, training loss: 6.355309009552002 = 0.032055240124464035 + 1.0 * 6.323253631591797
Epoch 860, val loss: 1.001572847366333
Epoch 870, training loss: 6.355596542358398 = 0.030822955071926117 + 1.0 * 6.324773788452148
Epoch 870, val loss: 1.0074951648712158
Epoch 880, training loss: 6.348486423492432 = 0.029663989320397377 + 1.0 * 6.318822383880615
Epoch 880, val loss: 1.0134614706039429
Epoch 890, training loss: 6.3462605476379395 = 0.028564024716615677 + 1.0 * 6.317696571350098
Epoch 890, val loss: 1.0193672180175781
Epoch 900, training loss: 6.348179817199707 = 0.027520636096596718 + 1.0 * 6.320659160614014
Epoch 900, val loss: 1.0250781774520874
Epoch 910, training loss: 6.345749855041504 = 0.026534445583820343 + 1.0 * 6.319215297698975
Epoch 910, val loss: 1.030848503112793
Epoch 920, training loss: 6.345484733581543 = 0.025602441281080246 + 1.0 * 6.319882392883301
Epoch 920, val loss: 1.0365267992019653
Epoch 930, training loss: 6.341337203979492 = 0.024720879271626472 + 1.0 * 6.316616535186768
Epoch 930, val loss: 1.0420900583267212
Epoch 940, training loss: 6.339815616607666 = 0.02388441190123558 + 1.0 * 6.31593132019043
Epoch 940, val loss: 1.0476018190383911
Epoch 950, training loss: 6.335028171539307 = 0.023090871050953865 + 1.0 * 6.31193733215332
Epoch 950, val loss: 1.0530732870101929
Epoch 960, training loss: 6.343015670776367 = 0.022335652261972427 + 1.0 * 6.320680141448975
Epoch 960, val loss: 1.0584050416946411
Epoch 970, training loss: 6.333805084228516 = 0.02161853387951851 + 1.0 * 6.3121867179870605
Epoch 970, val loss: 1.0636873245239258
Epoch 980, training loss: 6.3302178382873535 = 0.020938757807016373 + 1.0 * 6.309278964996338
Epoch 980, val loss: 1.0690975189208984
Epoch 990, training loss: 6.331555366516113 = 0.020286759361624718 + 1.0 * 6.3112688064575195
Epoch 990, val loss: 1.0742089748382568
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 10.524243354797363 = 1.9273970127105713 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9226654767990112
Epoch 10, training loss: 10.514388084411621 = 1.917816162109375 + 1.0 * 8.596571922302246
Epoch 10, val loss: 1.9127267599105835
Epoch 20, training loss: 10.499979019165039 = 1.9056087732315063 + 1.0 * 8.594369888305664
Epoch 20, val loss: 1.900054931640625
Epoch 30, training loss: 10.465835571289062 = 1.8883353471755981 + 1.0 * 8.577500343322754
Epoch 30, val loss: 1.8822121620178223
Epoch 40, training loss: 10.341153144836426 = 1.8657121658325195 + 1.0 * 8.475440979003906
Epoch 40, val loss: 1.859955072402954
Epoch 50, training loss: 9.93499755859375 = 1.8411788940429688 + 1.0 * 8.093818664550781
Epoch 50, val loss: 1.8364933729171753
Epoch 60, training loss: 9.698789596557617 = 1.819303274154663 + 1.0 * 7.879486560821533
Epoch 60, val loss: 1.8160561323165894
Epoch 70, training loss: 9.421979904174805 = 1.800965428352356 + 1.0 * 7.62101411819458
Epoch 70, val loss: 1.7992525100708008
Epoch 80, training loss: 9.071920394897461 = 1.7897833585739136 + 1.0 * 7.282136917114258
Epoch 80, val loss: 1.789674997329712
Epoch 90, training loss: 8.819229125976562 = 1.7818574905395508 + 1.0 * 7.037371635437012
Epoch 90, val loss: 1.7819067239761353
Epoch 100, training loss: 8.665237426757812 = 1.7695008516311646 + 1.0 * 6.895736217498779
Epoch 100, val loss: 1.7694973945617676
Epoch 110, training loss: 8.573698043823242 = 1.754516363143921 + 1.0 * 6.819181442260742
Epoch 110, val loss: 1.7553714513778687
Epoch 120, training loss: 8.498133659362793 = 1.7387148141860962 + 1.0 * 6.759418487548828
Epoch 120, val loss: 1.741326093673706
Epoch 130, training loss: 8.431258201599121 = 1.7217549085617065 + 1.0 * 6.709503650665283
Epoch 130, val loss: 1.726509690284729
Epoch 140, training loss: 8.368927001953125 = 1.7022747993469238 + 1.0 * 6.666652202606201
Epoch 140, val loss: 1.709834098815918
Epoch 150, training loss: 8.311866760253906 = 1.679766058921814 + 1.0 * 6.632101058959961
Epoch 150, val loss: 1.6910665035247803
Epoch 160, training loss: 8.2529935836792 = 1.6539710760116577 + 1.0 * 6.59902286529541
Epoch 160, val loss: 1.6698582172393799
Epoch 170, training loss: 8.197731971740723 = 1.624064564704895 + 1.0 * 6.573667526245117
Epoch 170, val loss: 1.645442247390747
Epoch 180, training loss: 8.142194747924805 = 1.5901883840560913 + 1.0 * 6.552006721496582
Epoch 180, val loss: 1.6180061101913452
Epoch 190, training loss: 8.086735725402832 = 1.552872657775879 + 1.0 * 6.533863067626953
Epoch 190, val loss: 1.5881868600845337
Epoch 200, training loss: 8.030326843261719 = 1.512116551399231 + 1.0 * 6.518210411071777
Epoch 200, val loss: 1.5563734769821167
Epoch 210, training loss: 7.975250720977783 = 1.4688981771469116 + 1.0 * 6.506352424621582
Epoch 210, val loss: 1.5238834619522095
Epoch 220, training loss: 7.9196014404296875 = 1.4245219230651855 + 1.0 * 6.495079517364502
Epoch 220, val loss: 1.4919300079345703
Epoch 230, training loss: 7.865040302276611 = 1.3793350458145142 + 1.0 * 6.485705375671387
Epoch 230, val loss: 1.4607415199279785
Epoch 240, training loss: 7.813130855560303 = 1.333707332611084 + 1.0 * 6.479423522949219
Epoch 240, val loss: 1.430732011795044
Epoch 250, training loss: 7.759992599487305 = 1.288835048675537 + 1.0 * 6.471157550811768
Epoch 250, val loss: 1.402740716934204
Epoch 260, training loss: 7.708764553070068 = 1.2448124885559082 + 1.0 * 6.46395206451416
Epoch 260, val loss: 1.3765326738357544
Epoch 270, training loss: 7.658896446228027 = 1.2011468410491943 + 1.0 * 6.457749366760254
Epoch 270, val loss: 1.3512611389160156
Epoch 280, training loss: 7.6109938621521 = 1.1577191352844238 + 1.0 * 6.453274726867676
Epoch 280, val loss: 1.3268142938613892
Epoch 290, training loss: 7.562239646911621 = 1.1145613193511963 + 1.0 * 6.447678565979004
Epoch 290, val loss: 1.3028665781021118
Epoch 300, training loss: 7.515958309173584 = 1.0715274810791016 + 1.0 * 6.444430828094482
Epoch 300, val loss: 1.2788220643997192
Epoch 310, training loss: 7.467249393463135 = 1.0287694931030273 + 1.0 * 6.438479900360107
Epoch 310, val loss: 1.2547624111175537
Epoch 320, training loss: 7.418619155883789 = 0.9858014583587646 + 1.0 * 6.432817459106445
Epoch 320, val loss: 1.2303472757339478
Epoch 330, training loss: 7.373457908630371 = 0.9427214860916138 + 1.0 * 6.430736541748047
Epoch 330, val loss: 1.2055416107177734
Epoch 340, training loss: 7.328839302062988 = 0.9002687931060791 + 1.0 * 6.428570747375488
Epoch 340, val loss: 1.1809561252593994
Epoch 350, training loss: 7.28010892868042 = 0.8588372468948364 + 1.0 * 6.421271800994873
Epoch 350, val loss: 1.1573234796524048
Epoch 360, training loss: 7.236581802368164 = 0.8182452321052551 + 1.0 * 6.418336391448975
Epoch 360, val loss: 1.1344174146652222
Epoch 370, training loss: 7.197332382202148 = 0.7788977026939392 + 1.0 * 6.4184346199035645
Epoch 370, val loss: 1.1128305196762085
Epoch 380, training loss: 7.1529860496521 = 0.7413482666015625 + 1.0 * 6.411637783050537
Epoch 380, val loss: 1.0934010744094849
Epoch 390, training loss: 7.113854885101318 = 0.7053057551383972 + 1.0 * 6.4085493087768555
Epoch 390, val loss: 1.0759551525115967
Epoch 400, training loss: 7.080535888671875 = 0.6707315444946289 + 1.0 * 6.409804344177246
Epoch 400, val loss: 1.0605796575546265
Epoch 410, training loss: 7.040503978729248 = 0.6379715800285339 + 1.0 * 6.402532577514648
Epoch 410, val loss: 1.047344446182251
Epoch 420, training loss: 7.006274700164795 = 0.606795608997345 + 1.0 * 6.399478912353516
Epoch 420, val loss: 1.0365240573883057
Epoch 430, training loss: 6.9748077392578125 = 0.5770133137702942 + 1.0 * 6.397794246673584
Epoch 430, val loss: 1.0275453329086304
Epoch 440, training loss: 6.942172527313232 = 0.5485683083534241 + 1.0 * 6.393604278564453
Epoch 440, val loss: 1.0204776525497437
Epoch 450, training loss: 6.914328098297119 = 0.5211585760116577 + 1.0 * 6.393169403076172
Epoch 450, val loss: 1.0150812864303589
Epoch 460, training loss: 6.888772010803223 = 0.49482741951942444 + 1.0 * 6.39394474029541
Epoch 460, val loss: 1.0111467838287354
Epoch 470, training loss: 6.855859756469727 = 0.46945589780807495 + 1.0 * 6.386404037475586
Epoch 470, val loss: 1.0085148811340332
Epoch 480, training loss: 6.82853889465332 = 0.4447958469390869 + 1.0 * 6.383742809295654
Epoch 480, val loss: 1.0069783926010132
Epoch 490, training loss: 6.806751251220703 = 0.42075833678245544 + 1.0 * 6.385993003845215
Epoch 490, val loss: 1.0064852237701416
Epoch 500, training loss: 6.778698921203613 = 0.39745351672172546 + 1.0 * 6.3812456130981445
Epoch 500, val loss: 1.006820559501648
Epoch 510, training loss: 6.752923965454102 = 0.3748977780342102 + 1.0 * 6.378026008605957
Epoch 510, val loss: 1.0081902742385864
Epoch 520, training loss: 6.731009006500244 = 0.35307008028030396 + 1.0 * 6.377938747406006
Epoch 520, val loss: 1.010276198387146
Epoch 530, training loss: 6.704878807067871 = 0.3320962190628052 + 1.0 * 6.3727827072143555
Epoch 530, val loss: 1.0132213830947876
Epoch 540, training loss: 6.692521572113037 = 0.31195327639579773 + 1.0 * 6.380568504333496
Epoch 540, val loss: 1.016867995262146
Epoch 550, training loss: 6.662235736846924 = 0.2928299307823181 + 1.0 * 6.369405746459961
Epoch 550, val loss: 1.0213334560394287
Epoch 560, training loss: 6.640906810760498 = 0.27461859583854675 + 1.0 * 6.366288185119629
Epoch 560, val loss: 1.0265297889709473
Epoch 570, training loss: 6.629522323608398 = 0.25731661915779114 + 1.0 * 6.37220573425293
Epoch 570, val loss: 1.032370686531067
Epoch 580, training loss: 6.60599946975708 = 0.24112914502620697 + 1.0 * 6.364870548248291
Epoch 580, val loss: 1.0385410785675049
Epoch 590, training loss: 6.587349891662598 = 0.22594954073429108 + 1.0 * 6.361400127410889
Epoch 590, val loss: 1.0455774068832397
Epoch 600, training loss: 6.570445537567139 = 0.21169190108776093 + 1.0 * 6.358753681182861
Epoch 600, val loss: 1.0532350540161133
Epoch 610, training loss: 6.562729358673096 = 0.19837965071201324 + 1.0 * 6.364349842071533
Epoch 610, val loss: 1.0612099170684814
Epoch 620, training loss: 6.545105934143066 = 0.18608011305332184 + 1.0 * 6.359025955200195
Epoch 620, val loss: 1.0695480108261108
Epoch 630, training loss: 6.528737545013428 = 0.17461661994457245 + 1.0 * 6.35412073135376
Epoch 630, val loss: 1.0784364938735962
Epoch 640, training loss: 6.5226359367370605 = 0.16391782462596893 + 1.0 * 6.358717918395996
Epoch 640, val loss: 1.0877424478530884
Epoch 650, training loss: 6.509910583496094 = 0.15398259460926056 + 1.0 * 6.35592794418335
Epoch 650, val loss: 1.0969940423965454
Epoch 660, training loss: 6.493480682373047 = 0.14476870000362396 + 1.0 * 6.348711967468262
Epoch 660, val loss: 1.106738805770874
Epoch 670, training loss: 6.487829208374023 = 0.13616934418678284 + 1.0 * 6.351659774780273
Epoch 670, val loss: 1.1167410612106323
Epoch 680, training loss: 6.477575778961182 = 0.1281755417585373 + 1.0 * 6.349400043487549
Epoch 680, val loss: 1.126668930053711
Epoch 690, training loss: 6.46568489074707 = 0.12074241787195206 + 1.0 * 6.344942569732666
Epoch 690, val loss: 1.1369260549545288
Epoch 700, training loss: 6.460518836975098 = 0.11380745470523834 + 1.0 * 6.346711158752441
Epoch 700, val loss: 1.1471284627914429
Epoch 710, training loss: 6.448462963104248 = 0.10735553503036499 + 1.0 * 6.341107368469238
Epoch 710, val loss: 1.1574009656906128
Epoch 720, training loss: 6.441516876220703 = 0.10132692009210587 + 1.0 * 6.3401899337768555
Epoch 720, val loss: 1.167876124382019
Epoch 730, training loss: 6.436437129974365 = 0.09569930285215378 + 1.0 * 6.340737819671631
Epoch 730, val loss: 1.1781951189041138
Epoch 740, training loss: 6.427649021148682 = 0.09046704322099686 + 1.0 * 6.33718204498291
Epoch 740, val loss: 1.188551664352417
Epoch 750, training loss: 6.42264986038208 = 0.08557150512933731 + 1.0 * 6.33707857131958
Epoch 750, val loss: 1.198952555656433
Epoch 760, training loss: 6.420097827911377 = 0.08099928498268127 + 1.0 * 6.3390984535217285
Epoch 760, val loss: 1.2092961072921753
Epoch 770, training loss: 6.415037631988525 = 0.07675151526927948 + 1.0 * 6.33828592300415
Epoch 770, val loss: 1.2191811800003052
Epoch 780, training loss: 6.405065059661865 = 0.0728067085146904 + 1.0 * 6.332258224487305
Epoch 780, val loss: 1.229396104812622
Epoch 790, training loss: 6.399140357971191 = 0.06910721957683563 + 1.0 * 6.330033302307129
Epoch 790, val loss: 1.239562749862671
Epoch 800, training loss: 6.3938703536987305 = 0.06563388556241989 + 1.0 * 6.3282365798950195
Epoch 800, val loss: 1.249635100364685
Epoch 810, training loss: 6.405661582946777 = 0.06237711012363434 + 1.0 * 6.343284606933594
Epoch 810, val loss: 1.2596925497055054
Epoch 820, training loss: 6.385955810546875 = 0.05932953581213951 + 1.0 * 6.326626300811768
Epoch 820, val loss: 1.2693315744400024
Epoch 830, training loss: 6.382259845733643 = 0.05648280307650566 + 1.0 * 6.325777053833008
Epoch 830, val loss: 1.279118537902832
Epoch 840, training loss: 6.377862930297852 = 0.05380944535136223 + 1.0 * 6.3240532875061035
Epoch 840, val loss: 1.2888425588607788
Epoch 850, training loss: 6.378686904907227 = 0.05130215361714363 + 1.0 * 6.327384948730469
Epoch 850, val loss: 1.298279881477356
Epoch 860, training loss: 6.37499475479126 = 0.04896611347794533 + 1.0 * 6.326028823852539
Epoch 860, val loss: 1.3076181411743164
Epoch 870, training loss: 6.366796016693115 = 0.04677361249923706 + 1.0 * 6.3200225830078125
Epoch 870, val loss: 1.3169708251953125
Epoch 880, training loss: 6.364534378051758 = 0.044708818197250366 + 1.0 * 6.319825649261475
Epoch 880, val loss: 1.326242208480835
Epoch 890, training loss: 6.372982978820801 = 0.04276854917407036 + 1.0 * 6.330214500427246
Epoch 890, val loss: 1.335332989692688
Epoch 900, training loss: 6.3583221435546875 = 0.04095160961151123 + 1.0 * 6.317370414733887
Epoch 900, val loss: 1.34412682056427
Epoch 910, training loss: 6.356960773468018 = 0.03924454748630524 + 1.0 * 6.317716121673584
Epoch 910, val loss: 1.3530654907226562
Epoch 920, training loss: 6.35326623916626 = 0.037633731961250305 + 1.0 * 6.315632343292236
Epoch 920, val loss: 1.3619263172149658
Epoch 930, training loss: 6.360767364501953 = 0.036113202571868896 + 1.0 * 6.3246541023254395
Epoch 930, val loss: 1.3705706596374512
Epoch 940, training loss: 6.352224826812744 = 0.03468162938952446 + 1.0 * 6.317543029785156
Epoch 940, val loss: 1.379162073135376
Epoch 950, training loss: 6.345492362976074 = 0.033334288746118546 + 1.0 * 6.312158107757568
Epoch 950, val loss: 1.3875596523284912
Epoch 960, training loss: 6.3449273109436035 = 0.03205970674753189 + 1.0 * 6.312867641448975
Epoch 960, val loss: 1.3959349393844604
Epoch 970, training loss: 6.346218109130859 = 0.030853215605020523 + 1.0 * 6.315364837646484
Epoch 970, val loss: 1.4042199850082397
Epoch 980, training loss: 6.34547233581543 = 0.029717108234763145 + 1.0 * 6.315755367279053
Epoch 980, val loss: 1.4123448133468628
Epoch 990, training loss: 6.337653636932373 = 0.028640221804380417 + 1.0 * 6.309013366699219
Epoch 990, val loss: 1.4202426671981812
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8133895624670533
The final CL Acc:0.75926, 0.01571, The final GNN Acc:0.81304, 0.00259
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13132])
remove edge: torch.Size([2, 7924])
updated graph: torch.Size([2, 10500])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.530400276184082 = 1.9335930347442627 + 1.0 * 8.596807479858398
Epoch 0, val loss: 1.9391615390777588
Epoch 10, training loss: 10.520438194274902 = 1.9240179061889648 + 1.0 * 8.596420288085938
Epoch 10, val loss: 1.92898428440094
Epoch 20, training loss: 10.505207061767578 = 1.9124295711517334 + 1.0 * 8.592777252197266
Epoch 20, val loss: 1.9165689945220947
Epoch 30, training loss: 10.459216117858887 = 1.8969579935073853 + 1.0 * 8.562257766723633
Epoch 30, val loss: 1.9000768661499023
Epoch 40, training loss: 10.23970890045166 = 1.8782422542572021 + 1.0 * 8.361466407775879
Epoch 40, val loss: 1.8809633255004883
Epoch 50, training loss: 9.653752326965332 = 1.8598345518112183 + 1.0 * 7.793917655944824
Epoch 50, val loss: 1.863572359085083
Epoch 60, training loss: 9.139137268066406 = 1.8460707664489746 + 1.0 * 7.293066024780273
Epoch 60, val loss: 1.8498010635375977
Epoch 70, training loss: 8.898353576660156 = 1.83257257938385 + 1.0 * 7.0657806396484375
Epoch 70, val loss: 1.8364346027374268
Epoch 80, training loss: 8.739655494689941 = 1.8179985284805298 + 1.0 * 6.921657085418701
Epoch 80, val loss: 1.8219550848007202
Epoch 90, training loss: 8.619297981262207 = 1.8023467063903809 + 1.0 * 6.816951274871826
Epoch 90, val loss: 1.8068742752075195
Epoch 100, training loss: 8.528701782226562 = 1.7863459587097168 + 1.0 * 6.7423553466796875
Epoch 100, val loss: 1.7910494804382324
Epoch 110, training loss: 8.453271865844727 = 1.7698858976364136 + 1.0 * 6.683385848999023
Epoch 110, val loss: 1.7750779390335083
Epoch 120, training loss: 8.388140678405762 = 1.7525429725646973 + 1.0 * 6.6355977058410645
Epoch 120, val loss: 1.758681058883667
Epoch 130, training loss: 8.336936950683594 = 1.7333171367645264 + 1.0 * 6.603619575500488
Epoch 130, val loss: 1.7407277822494507
Epoch 140, training loss: 8.29100227355957 = 1.7111620903015137 + 1.0 * 6.579840183258057
Epoch 140, val loss: 1.7206124067306519
Epoch 150, training loss: 8.243742942810059 = 1.6855792999267578 + 1.0 * 6.558163642883301
Epoch 150, val loss: 1.6977190971374512
Epoch 160, training loss: 8.195420265197754 = 1.655885100364685 + 1.0 * 6.5395355224609375
Epoch 160, val loss: 1.6715868711471558
Epoch 170, training loss: 8.142337799072266 = 1.6219185590744019 + 1.0 * 6.520419597625732
Epoch 170, val loss: 1.6418265104293823
Epoch 180, training loss: 8.085111618041992 = 1.5828194618225098 + 1.0 * 6.502292156219482
Epoch 180, val loss: 1.6076844930648804
Epoch 190, training loss: 8.02383804321289 = 1.5374295711517334 + 1.0 * 6.486408710479736
Epoch 190, val loss: 1.5682131052017212
Epoch 200, training loss: 7.959810256958008 = 1.4853061437606812 + 1.0 * 6.474503993988037
Epoch 200, val loss: 1.5232492685317993
Epoch 210, training loss: 7.8902740478515625 = 1.4266526699066162 + 1.0 * 6.463621616363525
Epoch 210, val loss: 1.4729368686676025
Epoch 220, training loss: 7.816795349121094 = 1.3614966869354248 + 1.0 * 6.455298900604248
Epoch 220, val loss: 1.4173132181167603
Epoch 230, training loss: 7.742178916931152 = 1.2912909984588623 + 1.0 * 6.450887680053711
Epoch 230, val loss: 1.3581292629241943
Epoch 240, training loss: 7.658996105194092 = 1.218051552772522 + 1.0 * 6.440944671630859
Epoch 240, val loss: 1.2967830896377563
Epoch 250, training loss: 7.576835632324219 = 1.1429530382156372 + 1.0 * 6.433882713317871
Epoch 250, val loss: 1.2346415519714355
Epoch 260, training loss: 7.497220039367676 = 1.0685049295425415 + 1.0 * 6.428715229034424
Epoch 260, val loss: 1.174020528793335
Epoch 270, training loss: 7.421677589416504 = 0.9981628656387329 + 1.0 * 6.4235148429870605
Epoch 270, val loss: 1.1174691915512085
Epoch 280, training loss: 7.351842403411865 = 0.9328886270523071 + 1.0 * 6.418953895568848
Epoch 280, val loss: 1.0659209489822388
Epoch 290, training loss: 7.292207717895508 = 0.8742751479148865 + 1.0 * 6.417932510375977
Epoch 290, val loss: 1.0206217765808105
Epoch 300, training loss: 7.232275009155273 = 0.8226395845413208 + 1.0 * 6.409635543823242
Epoch 300, val loss: 0.9817363619804382
Epoch 310, training loss: 7.1834845542907715 = 0.77684485912323 + 1.0 * 6.406639575958252
Epoch 310, val loss: 0.9483322501182556
Epoch 320, training loss: 7.138927459716797 = 0.7363075017929077 + 1.0 * 6.4026198387146
Epoch 320, val loss: 0.9199299216270447
Epoch 330, training loss: 7.095459461212158 = 0.6999735236167908 + 1.0 * 6.395485877990723
Epoch 330, val loss: 0.8952573537826538
Epoch 340, training loss: 7.064930438995361 = 0.6666447520256042 + 1.0 * 6.398285865783691
Epoch 340, val loss: 0.8735727071762085
Epoch 350, training loss: 7.024524688720703 = 0.6358411312103271 + 1.0 * 6.388683319091797
Epoch 350, val loss: 0.8543122410774231
Epoch 360, training loss: 6.991686820983887 = 0.6067321300506592 + 1.0 * 6.384954452514648
Epoch 360, val loss: 0.8369443416595459
Epoch 370, training loss: 6.960809230804443 = 0.5789847373962402 + 1.0 * 6.381824493408203
Epoch 370, val loss: 0.821075439453125
Epoch 380, training loss: 6.936576843261719 = 0.552452802658081 + 1.0 * 6.384123802185059
Epoch 380, val loss: 0.8066506385803223
Epoch 390, training loss: 6.901952743530273 = 0.5271102786064148 + 1.0 * 6.374842643737793
Epoch 390, val loss: 0.7935127019882202
Epoch 400, training loss: 6.876805305480957 = 0.5027177929878235 + 1.0 * 6.374087333679199
Epoch 400, val loss: 0.7816953063011169
Epoch 410, training loss: 6.852828025817871 = 0.47940391302108765 + 1.0 * 6.373424053192139
Epoch 410, val loss: 0.7712216377258301
Epoch 420, training loss: 6.824619770050049 = 0.45718881487846375 + 1.0 * 6.367431163787842
Epoch 420, val loss: 0.762168288230896
Epoch 430, training loss: 6.801259517669678 = 0.4359802007675171 + 1.0 * 6.365279197692871
Epoch 430, val loss: 0.7543949484825134
Epoch 440, training loss: 6.780461311340332 = 0.4158949851989746 + 1.0 * 6.364566326141357
Epoch 440, val loss: 0.7477282881736755
Epoch 450, training loss: 6.757504463195801 = 0.39690646529197693 + 1.0 * 6.360598087310791
Epoch 450, val loss: 0.7423036694526672
Epoch 460, training loss: 6.735256195068359 = 0.37889179587364197 + 1.0 * 6.3563642501831055
Epoch 460, val loss: 0.7377885580062866
Epoch 470, training loss: 6.716831207275391 = 0.3616674542427063 + 1.0 * 6.35516357421875
Epoch 470, val loss: 0.7340493202209473
Epoch 480, training loss: 6.705549716949463 = 0.3452128767967224 + 1.0 * 6.360336780548096
Epoch 480, val loss: 0.730969250202179
Epoch 490, training loss: 6.681177139282227 = 0.32952550053596497 + 1.0 * 6.351651668548584
Epoch 490, val loss: 0.7285154461860657
Epoch 500, training loss: 6.662101745605469 = 0.3144351541996002 + 1.0 * 6.3476667404174805
Epoch 500, val loss: 0.7265239953994751
Epoch 510, training loss: 6.6455559730529785 = 0.2997446060180664 + 1.0 * 6.345811367034912
Epoch 510, val loss: 0.7249129414558411
Epoch 520, training loss: 6.640628337860107 = 0.28542500734329224 + 1.0 * 6.355203151702881
Epoch 520, val loss: 0.7235211133956909
Epoch 530, training loss: 6.61771297454834 = 0.27150723338127136 + 1.0 * 6.346205711364746
Epoch 530, val loss: 0.7225058078765869
Epoch 540, training loss: 6.599109649658203 = 0.2579106092453003 + 1.0 * 6.341198921203613
Epoch 540, val loss: 0.721746027469635
Epoch 550, training loss: 6.588748931884766 = 0.24457955360412598 + 1.0 * 6.344169616699219
Epoch 550, val loss: 0.7212229371070862
Epoch 560, training loss: 6.5681538581848145 = 0.23162907361984253 + 1.0 * 6.336524963378906
Epoch 560, val loss: 0.7208777070045471
Epoch 570, training loss: 6.55333948135376 = 0.21904529631137848 + 1.0 * 6.334294319152832
Epoch 570, val loss: 0.7208678722381592
Epoch 580, training loss: 6.545494079589844 = 0.20691514015197754 + 1.0 * 6.338578701019287
Epoch 580, val loss: 0.721189022064209
Epoch 590, training loss: 6.543665885925293 = 0.19538596272468567 + 1.0 * 6.34827995300293
Epoch 590, val loss: 0.7218264937400818
Epoch 600, training loss: 6.518726348876953 = 0.18451285362243652 + 1.0 * 6.334213733673096
Epoch 600, val loss: 0.7228611707687378
Epoch 610, training loss: 6.50366735458374 = 0.17428980767726898 + 1.0 * 6.3293776512146
Epoch 610, val loss: 0.7242650389671326
Epoch 620, training loss: 6.491846084594727 = 0.16465255618095398 + 1.0 * 6.327193737030029
Epoch 620, val loss: 0.7260875105857849
Epoch 630, training loss: 6.483799934387207 = 0.1556311696767807 + 1.0 * 6.328168869018555
Epoch 630, val loss: 0.7282098531723022
Epoch 640, training loss: 6.475592613220215 = 0.14726482331752777 + 1.0 * 6.328327655792236
Epoch 640, val loss: 0.7306855320930481
Epoch 650, training loss: 6.4617133140563965 = 0.13946431875228882 + 1.0 * 6.322248935699463
Epoch 650, val loss: 0.7335191369056702
Epoch 660, training loss: 6.464097023010254 = 0.13217133283615112 + 1.0 * 6.331925868988037
Epoch 660, val loss: 0.736637532711029
Epoch 670, training loss: 6.450605392456055 = 0.12537437677383423 + 1.0 * 6.325231075286865
Epoch 670, val loss: 0.7399790287017822
Epoch 680, training loss: 6.440212249755859 = 0.11906197667121887 + 1.0 * 6.321150302886963
Epoch 680, val loss: 0.743597149848938
Epoch 690, training loss: 6.4308037757873535 = 0.11315721273422241 + 1.0 * 6.317646503448486
Epoch 690, val loss: 0.7474936246871948
Epoch 700, training loss: 6.427323818206787 = 0.10761160403490067 + 1.0 * 6.319712162017822
Epoch 700, val loss: 0.7515591382980347
Epoch 710, training loss: 6.419109344482422 = 0.10244366526603699 + 1.0 * 6.3166656494140625
Epoch 710, val loss: 0.7556893825531006
Epoch 720, training loss: 6.4151225090026855 = 0.09761836379766464 + 1.0 * 6.317503929138184
Epoch 720, val loss: 0.7600180506706238
Epoch 730, training loss: 6.409906387329102 = 0.09309206157922745 + 1.0 * 6.316814422607422
Epoch 730, val loss: 0.764465868473053
Epoch 740, training loss: 6.401719570159912 = 0.08883176743984222 + 1.0 * 6.312887668609619
Epoch 740, val loss: 0.7689657807350159
Epoch 750, training loss: 6.395037651062012 = 0.08482211083173752 + 1.0 * 6.310215473175049
Epoch 750, val loss: 0.7735748291015625
Epoch 760, training loss: 6.408854961395264 = 0.0810353234410286 + 1.0 * 6.32781982421875
Epoch 760, val loss: 0.7782631516456604
Epoch 770, training loss: 6.388306140899658 = 0.07751122862100601 + 1.0 * 6.310794830322266
Epoch 770, val loss: 0.7828482389450073
Epoch 780, training loss: 6.381607532501221 = 0.07418464869260788 + 1.0 * 6.307423114776611
Epoch 780, val loss: 0.7875625491142273
Epoch 790, training loss: 6.377979278564453 = 0.07103569060564041 + 1.0 * 6.306943416595459
Epoch 790, val loss: 0.7923797369003296
Epoch 800, training loss: 6.3801445960998535 = 0.06805526465177536 + 1.0 * 6.312089443206787
Epoch 800, val loss: 0.7971664071083069
Epoch 810, training loss: 6.3734636306762695 = 0.06523647904396057 + 1.0 * 6.308227062225342
Epoch 810, val loss: 0.8019692301750183
Epoch 820, training loss: 6.376197814941406 = 0.06257793307304382 + 1.0 * 6.313620090484619
Epoch 820, val loss: 0.806800127029419
Epoch 830, training loss: 6.365747928619385 = 0.0600556805729866 + 1.0 * 6.305692195892334
Epoch 830, val loss: 0.8116085529327393
Epoch 840, training loss: 6.359986305236816 = 0.057676639407873154 + 1.0 * 6.302309513092041
Epoch 840, val loss: 0.8163851499557495
Epoch 850, training loss: 6.355664253234863 = 0.05541399493813515 + 1.0 * 6.300250053405762
Epoch 850, val loss: 0.8212202787399292
Epoch 860, training loss: 6.360836505889893 = 0.053260281682014465 + 1.0 * 6.3075761795043945
Epoch 860, val loss: 0.826055645942688
Epoch 870, training loss: 6.359851360321045 = 0.05121489614248276 + 1.0 * 6.308636665344238
Epoch 870, val loss: 0.8307913541793823
Epoch 880, training loss: 6.348965644836426 = 0.04929018393158913 + 1.0 * 6.299675464630127
Epoch 880, val loss: 0.8355137705802917
Epoch 890, training loss: 6.345481872558594 = 0.04746106639504433 + 1.0 * 6.298020839691162
Epoch 890, val loss: 0.840214192867279
Epoch 900, training loss: 6.34641170501709 = 0.045716218650341034 + 1.0 * 6.300695419311523
Epoch 900, val loss: 0.8449025750160217
Epoch 910, training loss: 6.339354991912842 = 0.044052496552467346 + 1.0 * 6.295302391052246
Epoch 910, val loss: 0.8495582342147827
Epoch 920, training loss: 6.336977481842041 = 0.042474165558815 + 1.0 * 6.294503211975098
Epoch 920, val loss: 0.8541795611381531
Epoch 930, training loss: 6.336019515991211 = 0.040970947593450546 + 1.0 * 6.295048713684082
Epoch 930, val loss: 0.8588035106658936
Epoch 940, training loss: 6.345767498016357 = 0.03953215479850769 + 1.0 * 6.306235313415527
Epoch 940, val loss: 0.8633648753166199
Epoch 950, training loss: 6.3326287269592285 = 0.03817208111286163 + 1.0 * 6.294456481933594
Epoch 950, val loss: 0.8678656816482544
Epoch 960, training loss: 6.329700469970703 = 0.03687385097146034 + 1.0 * 6.2928266525268555
Epoch 960, val loss: 0.8723768591880798
Epoch 970, training loss: 6.330814361572266 = 0.03563312813639641 + 1.0 * 6.2951812744140625
Epoch 970, val loss: 0.8768838047981262
Epoch 980, training loss: 6.325930595397949 = 0.03444638103246689 + 1.0 * 6.291484355926514
Epoch 980, val loss: 0.8812240362167358
Epoch 990, training loss: 6.326540946960449 = 0.03332141414284706 + 1.0 * 6.293219566345215
Epoch 990, val loss: 0.8856277465820312
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8339483394833949
=== training gcn model ===
Epoch 0, training loss: 10.534375190734863 = 1.9375239610671997 + 1.0 * 8.596851348876953
Epoch 0, val loss: 1.9335930347442627
Epoch 10, training loss: 10.523846626281738 = 1.9272918701171875 + 1.0 * 8.59655475616455
Epoch 10, val loss: 1.9230401515960693
Epoch 20, training loss: 10.508209228515625 = 1.9142719507217407 + 1.0 * 8.593936920166016
Epoch 20, val loss: 1.9096758365631104
Epoch 30, training loss: 10.468605995178223 = 1.8959983587265015 + 1.0 * 8.57260799407959
Epoch 30, val loss: 1.8910090923309326
Epoch 40, training loss: 10.332368850708008 = 1.872218370437622 + 1.0 * 8.460150718688965
Epoch 40, val loss: 1.868011713027954
Epoch 50, training loss: 9.916458129882812 = 1.8474279642105103 + 1.0 * 8.069029808044434
Epoch 50, val loss: 1.8456522226333618
Epoch 60, training loss: 9.509858131408691 = 1.825566291809082 + 1.0 * 7.684291839599609
Epoch 60, val loss: 1.8266966342926025
Epoch 70, training loss: 9.038818359375 = 1.809444546699524 + 1.0 * 7.229373931884766
Epoch 70, val loss: 1.8124710321426392
Epoch 80, training loss: 8.802335739135742 = 1.7947921752929688 + 1.0 * 7.007544040679932
Epoch 80, val loss: 1.7998886108398438
Epoch 90, training loss: 8.656335830688477 = 1.7778509855270386 + 1.0 * 6.878484725952148
Epoch 90, val loss: 1.7856794595718384
Epoch 100, training loss: 8.563289642333984 = 1.7597163915634155 + 1.0 * 6.8035736083984375
Epoch 100, val loss: 1.7710299491882324
Epoch 110, training loss: 8.490644454956055 = 1.741146445274353 + 1.0 * 6.749497890472412
Epoch 110, val loss: 1.7559692859649658
Epoch 120, training loss: 8.42275619506836 = 1.721005916595459 + 1.0 * 6.7017502784729
Epoch 120, val loss: 1.739816665649414
Epoch 130, training loss: 8.36436653137207 = 1.6985526084899902 + 1.0 * 6.665814399719238
Epoch 130, val loss: 1.7215938568115234
Epoch 140, training loss: 8.309713363647461 = 1.672471046447754 + 1.0 * 6.637242794036865
Epoch 140, val loss: 1.7004965543746948
Epoch 150, training loss: 8.25532054901123 = 1.6415241956710815 + 1.0 * 6.613796234130859
Epoch 150, val loss: 1.6754053831100464
Epoch 160, training loss: 8.200594902038574 = 1.6053836345672607 + 1.0 * 6.595211029052734
Epoch 160, val loss: 1.6461044549942017
Epoch 170, training loss: 8.14406681060791 = 1.5641618967056274 + 1.0 * 6.579905033111572
Epoch 170, val loss: 1.612878441810608
Epoch 180, training loss: 8.082502365112305 = 1.5175631046295166 + 1.0 * 6.564939022064209
Epoch 180, val loss: 1.5755106210708618
Epoch 190, training loss: 8.016851425170898 = 1.4658710956573486 + 1.0 * 6.550980091094971
Epoch 190, val loss: 1.5342730283737183
Epoch 200, training loss: 7.948259353637695 = 1.4110372066497803 + 1.0 * 6.537222385406494
Epoch 200, val loss: 1.4906539916992188
Epoch 210, training loss: 7.877983093261719 = 1.3544949293136597 + 1.0 * 6.5234880447387695
Epoch 210, val loss: 1.4458332061767578
Epoch 220, training loss: 7.812901496887207 = 1.2968136072158813 + 1.0 * 6.516088008880615
Epoch 220, val loss: 1.4004974365234375
Epoch 230, training loss: 7.740368366241455 = 1.2405279874801636 + 1.0 * 6.499840259552002
Epoch 230, val loss: 1.3565130233764648
Epoch 240, training loss: 7.6736955642700195 = 1.1858800649642944 + 1.0 * 6.4878153800964355
Epoch 240, val loss: 1.3143532276153564
Epoch 250, training loss: 7.611613750457764 = 1.133506178855896 + 1.0 * 6.478107452392578
Epoch 250, val loss: 1.274616003036499
Epoch 260, training loss: 7.553119659423828 = 1.0845515727996826 + 1.0 * 6.468568325042725
Epoch 260, val loss: 1.238057017326355
Epoch 270, training loss: 7.498796463012695 = 1.039098858833313 + 1.0 * 6.459697723388672
Epoch 270, val loss: 1.204850196838379
Epoch 280, training loss: 7.456719398498535 = 0.9965231418609619 + 1.0 * 6.460196018218994
Epoch 280, val loss: 1.1746406555175781
Epoch 290, training loss: 7.4050397872924805 = 0.9570615887641907 + 1.0 * 6.4479780197143555
Epoch 290, val loss: 1.1469978094100952
Epoch 300, training loss: 7.359288215637207 = 0.9194574356079102 + 1.0 * 6.439830780029297
Epoch 300, val loss: 1.1212090253829956
Epoch 310, training loss: 7.31805419921875 = 0.8830657005310059 + 1.0 * 6.434988498687744
Epoch 310, val loss: 1.096559762954712
Epoch 320, training loss: 7.279446125030518 = 0.8476802110671997 + 1.0 * 6.431766033172607
Epoch 320, val loss: 1.072828769683838
Epoch 330, training loss: 7.2373809814453125 = 0.8130856156349182 + 1.0 * 6.424295425415039
Epoch 330, val loss: 1.04996919631958
Epoch 340, training loss: 7.197154998779297 = 0.7788640856742859 + 1.0 * 6.418291091918945
Epoch 340, val loss: 1.0273609161376953
Epoch 350, training loss: 7.165932655334473 = 0.7449002861976624 + 1.0 * 6.421032428741455
Epoch 350, val loss: 1.0049399137496948
Epoch 360, training loss: 7.123866558074951 = 0.7115569710731506 + 1.0 * 6.412309646606445
Epoch 360, val loss: 0.98334801197052
Epoch 370, training loss: 7.085033416748047 = 0.6790335178375244 + 1.0 * 6.406000137329102
Epoch 370, val loss: 0.9624849557876587
Epoch 380, training loss: 7.051113605499268 = 0.6471160650253296 + 1.0 * 6.403997421264648
Epoch 380, val loss: 0.9423611164093018
Epoch 390, training loss: 7.015178680419922 = 0.6159377098083496 + 1.0 * 6.399240970611572
Epoch 390, val loss: 0.9231706857681274
Epoch 400, training loss: 6.986573219299316 = 0.5855768322944641 + 1.0 * 6.400996208190918
Epoch 400, val loss: 0.9050553441047668
Epoch 410, training loss: 6.949735164642334 = 0.5561344027519226 + 1.0 * 6.393600940704346
Epoch 410, val loss: 0.8882275819778442
Epoch 420, training loss: 6.917032241821289 = 0.5276285409927368 + 1.0 * 6.389403820037842
Epoch 420, val loss: 0.8724399209022522
Epoch 430, training loss: 6.893440246582031 = 0.4999105930328369 + 1.0 * 6.393529415130615
Epoch 430, val loss: 0.8578642010688782
Epoch 440, training loss: 6.857071876525879 = 0.4734288454055786 + 1.0 * 6.38364315032959
Epoch 440, val loss: 0.8444460034370422
Epoch 450, training loss: 6.829139232635498 = 0.44793811440467834 + 1.0 * 6.381201267242432
Epoch 450, val loss: 0.8322898745536804
Epoch 460, training loss: 6.802120685577393 = 0.42323675751686096 + 1.0 * 6.3788838386535645
Epoch 460, val loss: 0.8211064338684082
Epoch 470, training loss: 6.780189514160156 = 0.39945685863494873 + 1.0 * 6.380732536315918
Epoch 470, val loss: 0.8108854293823242
Epoch 480, training loss: 6.751532077789307 = 0.37661436200141907 + 1.0 * 6.374917507171631
Epoch 480, val loss: 0.8017526268959045
Epoch 490, training loss: 6.7257795333862305 = 0.3546926975250244 + 1.0 * 6.371086597442627
Epoch 490, val loss: 0.7935711741447449
Epoch 500, training loss: 6.706634044647217 = 0.3335207402706146 + 1.0 * 6.37311315536499
Epoch 500, val loss: 0.7862266898155212
Epoch 510, training loss: 6.686105251312256 = 0.3134220838546753 + 1.0 * 6.372683048248291
Epoch 510, val loss: 0.7796339392662048
Epoch 520, training loss: 6.659786701202393 = 0.29418960213661194 + 1.0 * 6.365597248077393
Epoch 520, val loss: 0.7741679549217224
Epoch 530, training loss: 6.63812780380249 = 0.27581900358200073 + 1.0 * 6.362308979034424
Epoch 530, val loss: 0.7694470286369324
Epoch 540, training loss: 6.630587100982666 = 0.25828996300697327 + 1.0 * 6.372297286987305
Epoch 540, val loss: 0.7655651569366455
Epoch 550, training loss: 6.600220203399658 = 0.24182040989398956 + 1.0 * 6.358399868011475
Epoch 550, val loss: 0.76261967420578
Epoch 560, training loss: 6.582794189453125 = 0.22622506320476532 + 1.0 * 6.356569290161133
Epoch 560, val loss: 0.760621190071106
Epoch 570, training loss: 6.570589542388916 = 0.21153965592384338 + 1.0 * 6.3590497970581055
Epoch 570, val loss: 0.759357750415802
Epoch 580, training loss: 6.556041240692139 = 0.1978520154953003 + 1.0 * 6.358189105987549
Epoch 580, val loss: 0.7590381503105164
Epoch 590, training loss: 6.536933898925781 = 0.18514570593833923 + 1.0 * 6.35178804397583
Epoch 590, val loss: 0.7595953941345215
Epoch 600, training loss: 6.524334907531738 = 0.17330259084701538 + 1.0 * 6.351032257080078
Epoch 600, val loss: 0.7608493566513062
Epoch 610, training loss: 6.515404224395752 = 0.1623149812221527 + 1.0 * 6.353089332580566
Epoch 610, val loss: 0.7627650499343872
Epoch 620, training loss: 6.49933385848999 = 0.1522315889596939 + 1.0 * 6.347102165222168
Epoch 620, val loss: 0.7654356956481934
Epoch 630, training loss: 6.487490177154541 = 0.14288881421089172 + 1.0 * 6.344601154327393
Epoch 630, val loss: 0.7687416672706604
Epoch 640, training loss: 6.475741863250732 = 0.13420698046684265 + 1.0 * 6.3415350914001465
Epoch 640, val loss: 0.7724921107292175
Epoch 650, training loss: 6.471592426300049 = 0.12614354491233826 + 1.0 * 6.345448970794678
Epoch 650, val loss: 0.7767513990402222
Epoch 660, training loss: 6.467414855957031 = 0.11873141676187515 + 1.0 * 6.3486833572387695
Epoch 660, val loss: 0.7813366651535034
Epoch 670, training loss: 6.452179908752441 = 0.11189866065979004 + 1.0 * 6.3402814865112305
Epoch 670, val loss: 0.7864712476730347
Epoch 680, training loss: 6.442330360412598 = 0.10557449609041214 + 1.0 * 6.336755752563477
Epoch 680, val loss: 0.7918305993080139
Epoch 690, training loss: 6.437232494354248 = 0.09970424324274063 + 1.0 * 6.337528228759766
Epoch 690, val loss: 0.7973859906196594
Epoch 700, training loss: 6.428370475769043 = 0.09427231550216675 + 1.0 * 6.3340983390808105
Epoch 700, val loss: 0.8032460808753967
Epoch 710, training loss: 6.423019886016846 = 0.08920728415250778 + 1.0 * 6.333812713623047
Epoch 710, val loss: 0.8093430399894714
Epoch 720, training loss: 6.421507835388184 = 0.08452892303466797 + 1.0 * 6.336978912353516
Epoch 720, val loss: 0.8154730796813965
Epoch 730, training loss: 6.4098124504089355 = 0.08020104467868805 + 1.0 * 6.329611301422119
Epoch 730, val loss: 0.821847677230835
Epoch 740, training loss: 6.402986526489258 = 0.07617149502038956 + 1.0 * 6.326815128326416
Epoch 740, val loss: 0.82827228307724
Epoch 750, training loss: 6.397762298583984 = 0.07239401340484619 + 1.0 * 6.325368404388428
Epoch 750, val loss: 0.8347157835960388
Epoch 760, training loss: 6.404782295227051 = 0.06885770708322525 + 1.0 * 6.3359246253967285
Epoch 760, val loss: 0.8412520885467529
Epoch 770, training loss: 6.392932891845703 = 0.06556671857833862 + 1.0 * 6.327366352081299
Epoch 770, val loss: 0.847821831703186
Epoch 780, training loss: 6.384519100189209 = 0.06248699873685837 + 1.0 * 6.3220319747924805
Epoch 780, val loss: 0.8545359969139099
Epoch 790, training loss: 6.399901866912842 = 0.05959789827466011 + 1.0 * 6.340303897857666
Epoch 790, val loss: 0.8610982298851013
Epoch 800, training loss: 6.377881050109863 = 0.056925103068351746 + 1.0 * 6.320955753326416
Epoch 800, val loss: 0.8676356673240662
Epoch 810, training loss: 6.372916221618652 = 0.05440586060285568 + 1.0 * 6.31851053237915
Epoch 810, val loss: 0.8743327260017395
Epoch 820, training loss: 6.3700971603393555 = 0.052030399441719055 + 1.0 * 6.318066596984863
Epoch 820, val loss: 0.8809593319892883
Epoch 830, training loss: 6.37257194519043 = 0.049795642495155334 + 1.0 * 6.3227763175964355
Epoch 830, val loss: 0.8874743580818176
Epoch 840, training loss: 6.3636088371276855 = 0.047705940902233124 + 1.0 * 6.3159027099609375
Epoch 840, val loss: 0.8940589427947998
Epoch 850, training loss: 6.359549045562744 = 0.04573220759630203 + 1.0 * 6.313817024230957
Epoch 850, val loss: 0.9006637930870056
Epoch 860, training loss: 6.356634140014648 = 0.04386506974697113 + 1.0 * 6.312768936157227
Epoch 860, val loss: 0.9071943163871765
Epoch 870, training loss: 6.370123386383057 = 0.04210805147886276 + 1.0 * 6.328015327453613
Epoch 870, val loss: 0.91365647315979
Epoch 880, training loss: 6.357329368591309 = 0.04044441878795624 + 1.0 * 6.316884994506836
Epoch 880, val loss: 0.9199843406677246
Epoch 890, training loss: 6.351180076599121 = 0.03888501599431038 + 1.0 * 6.312294960021973
Epoch 890, val loss: 0.9263997673988342
Epoch 900, training loss: 6.346446514129639 = 0.03741069883108139 + 1.0 * 6.309035778045654
Epoch 900, val loss: 0.932765007019043
Epoch 910, training loss: 6.346142292022705 = 0.036007434129714966 + 1.0 * 6.3101348876953125
Epoch 910, val loss: 0.9390110969543457
Epoch 920, training loss: 6.343836307525635 = 0.034679725766181946 + 1.0 * 6.30915641784668
Epoch 920, val loss: 0.9451074600219727
Epoch 930, training loss: 6.341850757598877 = 0.03342447057366371 + 1.0 * 6.308426380157471
Epoch 930, val loss: 0.9513458013534546
Epoch 940, training loss: 6.348353862762451 = 0.03223448619246483 + 1.0 * 6.316119194030762
Epoch 940, val loss: 0.9574294686317444
Epoch 950, training loss: 6.338189125061035 = 0.03111516311764717 + 1.0 * 6.307074069976807
Epoch 950, val loss: 0.9633728861808777
Epoch 960, training loss: 6.334164142608643 = 0.030042927712202072 + 1.0 * 6.304121017456055
Epoch 960, val loss: 0.969388484954834
Epoch 970, training loss: 6.332945823669434 = 0.02902177907526493 + 1.0 * 6.303924083709717
Epoch 970, val loss: 0.9753256440162659
Epoch 980, training loss: 6.335136413574219 = 0.028049658983945847 + 1.0 * 6.307086944580078
Epoch 980, val loss: 0.9811705350875854
Epoch 990, training loss: 6.337973594665527 = 0.02712971344590187 + 1.0 * 6.3108439445495605
Epoch 990, val loss: 0.9869625568389893
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.554875373840332 = 1.958022952079773 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.9568743705749512
Epoch 10, training loss: 10.543988227844238 = 1.947424054145813 + 1.0 * 8.596564292907715
Epoch 10, val loss: 1.946548581123352
Epoch 20, training loss: 10.528403282165527 = 1.9343608617782593 + 1.0 * 8.594042778015137
Epoch 20, val loss: 1.9333531856536865
Epoch 30, training loss: 10.489163398742676 = 1.9163906574249268 + 1.0 * 8.572772979736328
Epoch 30, val loss: 1.9145876169204712
Epoch 40, training loss: 10.332509994506836 = 1.8925365209579468 + 1.0 * 8.439973831176758
Epoch 40, val loss: 1.8903006315231323
Epoch 50, training loss: 9.890682220458984 = 1.8659030199050903 + 1.0 * 8.024779319763184
Epoch 50, val loss: 1.8644886016845703
Epoch 60, training loss: 9.584410667419434 = 1.841650366783142 + 1.0 * 7.74276065826416
Epoch 60, val loss: 1.8424957990646362
Epoch 70, training loss: 9.14714527130127 = 1.8209503889083862 + 1.0 * 7.326194763183594
Epoch 70, val loss: 1.8236665725708008
Epoch 80, training loss: 8.890267372131348 = 1.8045674562454224 + 1.0 * 7.085699558258057
Epoch 80, val loss: 1.8089098930358887
Epoch 90, training loss: 8.729035377502441 = 1.7862894535064697 + 1.0 * 6.942746162414551
Epoch 90, val loss: 1.7923706769943237
Epoch 100, training loss: 8.595431327819824 = 1.7669636011123657 + 1.0 * 6.82846736907959
Epoch 100, val loss: 1.7750352621078491
Epoch 110, training loss: 8.509204864501953 = 1.7478364706039429 + 1.0 * 6.761368274688721
Epoch 110, val loss: 1.757838487625122
Epoch 120, training loss: 8.433423042297363 = 1.7275705337524414 + 1.0 * 6.705852508544922
Epoch 120, val loss: 1.73997163772583
Epoch 130, training loss: 8.370054244995117 = 1.705844521522522 + 1.0 * 6.664209365844727
Epoch 130, val loss: 1.7213993072509766
Epoch 140, training loss: 8.315058708190918 = 1.6816927194595337 + 1.0 * 6.633366107940674
Epoch 140, val loss: 1.7010267972946167
Epoch 150, training loss: 8.262308120727539 = 1.653656005859375 + 1.0 * 6.608652114868164
Epoch 150, val loss: 1.6776554584503174
Epoch 160, training loss: 8.205450057983398 = 1.621688961982727 + 1.0 * 6.583760738372803
Epoch 160, val loss: 1.6511558294296265
Epoch 170, training loss: 8.14689826965332 = 1.58537757396698 + 1.0 * 6.561520576477051
Epoch 170, val loss: 1.6210640668869019
Epoch 180, training loss: 8.085125923156738 = 1.5435888767242432 + 1.0 * 6.541536808013916
Epoch 180, val loss: 1.5864837169647217
Epoch 190, training loss: 8.025336265563965 = 1.4967387914657593 + 1.0 * 6.528597354888916
Epoch 190, val loss: 1.5478968620300293
Epoch 200, training loss: 7.959975242614746 = 1.4459601640701294 + 1.0 * 6.514015197753906
Epoch 200, val loss: 1.5059826374053955
Epoch 210, training loss: 7.892533779144287 = 1.3911190032958984 + 1.0 * 6.501414775848389
Epoch 210, val loss: 1.4609119892120361
Epoch 220, training loss: 7.825606346130371 = 1.333021640777588 + 1.0 * 6.492584705352783
Epoch 220, val loss: 1.4135104417800903
Epoch 230, training loss: 7.756945610046387 = 1.2748255729675293 + 1.0 * 6.482120037078857
Epoch 230, val loss: 1.3662029504776
Epoch 240, training loss: 7.690248489379883 = 1.217849612236023 + 1.0 * 6.47239875793457
Epoch 240, val loss: 1.3200397491455078
Epoch 250, training loss: 7.625538349151611 = 1.162392497062683 + 1.0 * 6.463145732879639
Epoch 250, val loss: 1.275034785270691
Epoch 260, training loss: 7.5686116218566895 = 1.1092681884765625 + 1.0 * 6.459343433380127
Epoch 260, val loss: 1.2317646741867065
Epoch 270, training loss: 7.512267112731934 = 1.060518503189087 + 1.0 * 6.451748847961426
Epoch 270, val loss: 1.1922751665115356
Epoch 280, training loss: 7.45720100402832 = 1.0155612230300903 + 1.0 * 6.4416399002075195
Epoch 280, val loss: 1.1560789346694946
Epoch 290, training loss: 7.407584190368652 = 0.9730427861213684 + 1.0 * 6.43454122543335
Epoch 290, val loss: 1.122328281402588
Epoch 300, training loss: 7.359875679016113 = 0.9322748184204102 + 1.0 * 6.427600860595703
Epoch 300, val loss: 1.0902460813522339
Epoch 310, training loss: 7.329918384552002 = 0.8926982283592224 + 1.0 * 6.437220096588135
Epoch 310, val loss: 1.0595719814300537
Epoch 320, training loss: 7.2732768058776855 = 0.854688823223114 + 1.0 * 6.418588161468506
Epoch 320, val loss: 1.0305194854736328
Epoch 330, training loss: 7.230166435241699 = 0.817300021648407 + 1.0 * 6.412866592407227
Epoch 330, val loss: 1.0025408267974854
Epoch 340, training loss: 7.190987586975098 = 0.7801780104637146 + 1.0 * 6.410809516906738
Epoch 340, val loss: 0.9749835133552551
Epoch 350, training loss: 7.147531032562256 = 0.7435107231140137 + 1.0 * 6.404020309448242
Epoch 350, val loss: 0.9481272101402283
Epoch 360, training loss: 7.105770587921143 = 0.7068470120429993 + 1.0 * 6.398923397064209
Epoch 360, val loss: 0.921906054019928
Epoch 370, training loss: 7.07227897644043 = 0.669999897480011 + 1.0 * 6.402278900146484
Epoch 370, val loss: 0.8958536982536316
Epoch 380, training loss: 7.027249336242676 = 0.6334283351898193 + 1.0 * 6.393820762634277
Epoch 380, val loss: 0.8703153133392334
Epoch 390, training loss: 6.986242294311523 = 0.597191333770752 + 1.0 * 6.3890509605407715
Epoch 390, val loss: 0.8457160592079163
Epoch 400, training loss: 6.9481120109558105 = 0.5614166259765625 + 1.0 * 6.386695384979248
Epoch 400, val loss: 0.8219304084777832
Epoch 410, training loss: 6.909605503082275 = 0.5263354182243347 + 1.0 * 6.383270263671875
Epoch 410, val loss: 0.799149215221405
Epoch 420, training loss: 6.8816962242126465 = 0.4921702444553375 + 1.0 * 6.389525890350342
Epoch 420, val loss: 0.7776703238487244
Epoch 430, training loss: 6.83854866027832 = 0.45965203642845154 + 1.0 * 6.378896713256836
Epoch 430, val loss: 0.7577406764030457
Epoch 440, training loss: 6.803984642028809 = 0.4288058876991272 + 1.0 * 6.375178813934326
Epoch 440, val loss: 0.7395926713943481
Epoch 450, training loss: 6.779069423675537 = 0.39960986375808716 + 1.0 * 6.379459381103516
Epoch 450, val loss: 0.7230132222175598
Epoch 460, training loss: 6.746940612792969 = 0.37234023213386536 + 1.0 * 6.374600410461426
Epoch 460, val loss: 0.708130419254303
Epoch 470, training loss: 6.716920852661133 = 0.3468879759311676 + 1.0 * 6.370032787322998
Epoch 470, val loss: 0.6951287984848022
Epoch 480, training loss: 6.695967197418213 = 0.32312726974487305 + 1.0 * 6.37283992767334
Epoch 480, val loss: 0.6837276816368103
Epoch 490, training loss: 6.66691255569458 = 0.3011806309223175 + 1.0 * 6.365731716156006
Epoch 490, val loss: 0.6738044023513794
Epoch 500, training loss: 6.641888618469238 = 0.28073304891586304 + 1.0 * 6.3611555099487305
Epoch 500, val loss: 0.6655046343803406
Epoch 510, training loss: 6.62091064453125 = 0.2615286409854889 + 1.0 * 6.359382152557373
Epoch 510, val loss: 0.6583002805709839
Epoch 520, training loss: 6.605132579803467 = 0.24359898269176483 + 1.0 * 6.3615336418151855
Epoch 520, val loss: 0.6519944667816162
Epoch 530, training loss: 6.585671424865723 = 0.22703222930431366 + 1.0 * 6.358639240264893
Epoch 530, val loss: 0.647026777267456
Epoch 540, training loss: 6.565351486206055 = 0.21155713498592377 + 1.0 * 6.353794574737549
Epoch 540, val loss: 0.6430709958076477
Epoch 550, training loss: 6.547515392303467 = 0.19708415865898132 + 1.0 * 6.350431442260742
Epoch 550, val loss: 0.6398624777793884
Epoch 560, training loss: 6.534793376922607 = 0.18358954787254333 + 1.0 * 6.351203918457031
Epoch 560, val loss: 0.6375251412391663
Epoch 570, training loss: 6.530569553375244 = 0.17118041217327118 + 1.0 * 6.359389305114746
Epoch 570, val loss: 0.6359816789627075
Epoch 580, training loss: 6.507656097412109 = 0.1598280519247055 + 1.0 * 6.347827911376953
Epoch 580, val loss: 0.6354005932807922
Epoch 590, training loss: 6.493147373199463 = 0.14933907985687256 + 1.0 * 6.343808174133301
Epoch 590, val loss: 0.6355153918266296
Epoch 600, training loss: 6.497314453125 = 0.13965308666229248 + 1.0 * 6.357661247253418
Epoch 600, val loss: 0.6361335515975952
Epoch 610, training loss: 6.472254276275635 = 0.1307949721813202 + 1.0 * 6.341459274291992
Epoch 610, val loss: 0.6373918056488037
Epoch 620, training loss: 6.460688591003418 = 0.12267613410949707 + 1.0 * 6.338012218475342
Epoch 620, val loss: 0.6393743753433228
Epoch 630, training loss: 6.454469680786133 = 0.11518791317939758 + 1.0 * 6.3392815589904785
Epoch 630, val loss: 0.6418063640594482
Epoch 640, training loss: 6.447195529937744 = 0.10830826312303543 + 1.0 * 6.3388872146606445
Epoch 640, val loss: 0.6444784998893738
Epoch 650, training loss: 6.4374308586120605 = 0.10200099647045135 + 1.0 * 6.335429668426514
Epoch 650, val loss: 0.6477797627449036
Epoch 660, training loss: 6.428719997406006 = 0.0961793065071106 + 1.0 * 6.332540512084961
Epoch 660, val loss: 0.6514010429382324
Epoch 670, training loss: 6.427803039550781 = 0.09078224748373032 + 1.0 * 6.3370208740234375
Epoch 670, val loss: 0.6552647352218628
Epoch 680, training loss: 6.419151782989502 = 0.08579353243112564 + 1.0 * 6.333358287811279
Epoch 680, val loss: 0.659267246723175
Epoch 690, training loss: 6.415870189666748 = 0.08120621740818024 + 1.0 * 6.3346638679504395
Epoch 690, val loss: 0.6636915802955627
Epoch 700, training loss: 6.403740882873535 = 0.07694140076637268 + 1.0 * 6.326799392700195
Epoch 700, val loss: 0.6682642698287964
Epoch 710, training loss: 6.398855209350586 = 0.07297556102275848 + 1.0 * 6.3258795738220215
Epoch 710, val loss: 0.6729536652565002
Epoch 720, training loss: 6.394402980804443 = 0.06928588449954987 + 1.0 * 6.325117111206055
Epoch 720, val loss: 0.6777690649032593
Epoch 730, training loss: 6.390673637390137 = 0.06585213541984558 + 1.0 * 6.324821472167969
Epoch 730, val loss: 0.6826294660568237
Epoch 740, training loss: 6.388449192047119 = 0.06266835331916809 + 1.0 * 6.325780868530273
Epoch 740, val loss: 0.6876715421676636
Epoch 750, training loss: 6.384911060333252 = 0.05969855934381485 + 1.0 * 6.325212478637695
Epoch 750, val loss: 0.6928247809410095
Epoch 760, training loss: 6.375965595245361 = 0.056932058185338974 + 1.0 * 6.319033622741699
Epoch 760, val loss: 0.6979149580001831
Epoch 770, training loss: 6.371124744415283 = 0.05434349179267883 + 1.0 * 6.316781044006348
Epoch 770, val loss: 0.703130304813385
Epoch 780, training loss: 6.368485927581787 = 0.05191193148493767 + 1.0 * 6.3165740966796875
Epoch 780, val loss: 0.7083490490913391
Epoch 790, training loss: 6.38229513168335 = 0.0496264323592186 + 1.0 * 6.332668781280518
Epoch 790, val loss: 0.7134577035903931
Epoch 800, training loss: 6.3616132736206055 = 0.04750774800777435 + 1.0 * 6.31410551071167
Epoch 800, val loss: 0.718669056892395
Epoch 810, training loss: 6.3599371910095215 = 0.04551323130726814 + 1.0 * 6.31442403793335
Epoch 810, val loss: 0.724031388759613
Epoch 820, training loss: 6.355404853820801 = 0.04362911358475685 + 1.0 * 6.3117756843566895
Epoch 820, val loss: 0.7292803525924683
Epoch 830, training loss: 6.363362789154053 = 0.04185593128204346 + 1.0 * 6.321506977081299
Epoch 830, val loss: 0.7344204783439636
Epoch 840, training loss: 6.350627422332764 = 0.04018445685505867 + 1.0 * 6.310442924499512
Epoch 840, val loss: 0.7395486831665039
Epoch 850, training loss: 6.3479533195495605 = 0.03861108049750328 + 1.0 * 6.309342384338379
Epoch 850, val loss: 0.7448152303695679
Epoch 860, training loss: 6.34857702255249 = 0.037128716707229614 + 1.0 * 6.311448097229004
Epoch 860, val loss: 0.7498970031738281
Epoch 870, training loss: 6.343299388885498 = 0.035727933049201965 + 1.0 * 6.3075714111328125
Epoch 870, val loss: 0.7549441456794739
Epoch 880, training loss: 6.34035587310791 = 0.034403104335069656 + 1.0 * 6.305952548980713
Epoch 880, val loss: 0.7600335478782654
Epoch 890, training loss: 6.342372894287109 = 0.033142637461423874 + 1.0 * 6.309230327606201
Epoch 890, val loss: 0.7650548219680786
Epoch 900, training loss: 6.344905853271484 = 0.031955406069755554 + 1.0 * 6.312950611114502
Epoch 900, val loss: 0.7699217796325684
Epoch 910, training loss: 6.335763931274414 = 0.03082980588078499 + 1.0 * 6.304934024810791
Epoch 910, val loss: 0.7747477293014526
Epoch 920, training loss: 6.332253456115723 = 0.029765959829092026 + 1.0 * 6.302487373352051
Epoch 920, val loss: 0.7797269225120544
Epoch 930, training loss: 6.330557346343994 = 0.028747305274009705 + 1.0 * 6.301810264587402
Epoch 930, val loss: 0.7845500707626343
Epoch 940, training loss: 6.334752082824707 = 0.027779726311564445 + 1.0 * 6.306972503662109
Epoch 940, val loss: 0.7892462015151978
Epoch 950, training loss: 6.334485054016113 = 0.026857763528823853 + 1.0 * 6.307627201080322
Epoch 950, val loss: 0.7938041687011719
Epoch 960, training loss: 6.326211929321289 = 0.025989308953285217 + 1.0 * 6.300222396850586
Epoch 960, val loss: 0.7985050678253174
Epoch 970, training loss: 6.324049949645996 = 0.025157125666737556 + 1.0 * 6.298892974853516
Epoch 970, val loss: 0.8031278252601624
Epoch 980, training loss: 6.320796966552734 = 0.0243619903922081 + 1.0 * 6.2964348793029785
Epoch 980, val loss: 0.8076828718185425
Epoch 990, training loss: 6.331052303314209 = 0.023602426052093506 + 1.0 * 6.307449817657471
Epoch 990, val loss: 0.8121325969696045
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8371112282551397
The final CL Acc:0.82099, 0.01222, The final GNN Acc:0.83658, 0.00197
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11550])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10526])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.529544830322266 = 1.9326871633529663 + 1.0 * 8.596858024597168
Epoch 0, val loss: 1.926527500152588
Epoch 10, training loss: 10.5194673538208 = 1.9227813482284546 + 1.0 * 8.596686363220215
Epoch 10, val loss: 1.9174201488494873
Epoch 20, training loss: 10.505882263183594 = 1.9104878902435303 + 1.0 * 8.595394134521484
Epoch 20, val loss: 1.9057902097702026
Epoch 30, training loss: 10.478231430053711 = 1.893476128578186 + 1.0 * 8.584754943847656
Epoch 30, val loss: 1.889657735824585
Epoch 40, training loss: 10.394658088684082 = 1.8703397512435913 + 1.0 * 8.52431869506836
Epoch 40, val loss: 1.868775725364685
Epoch 50, training loss: 10.047688484191895 = 1.8451907634735107 + 1.0 * 8.202497482299805
Epoch 50, val loss: 1.8474876880645752
Epoch 60, training loss: 9.794681549072266 = 1.8212862014770508 + 1.0 * 7.973395347595215
Epoch 60, val loss: 1.828307032585144
Epoch 70, training loss: 9.357656478881836 = 1.804521918296814 + 1.0 * 7.553134441375732
Epoch 70, val loss: 1.8133243322372437
Epoch 80, training loss: 8.9320068359375 = 1.7936187982559204 + 1.0 * 7.138387680053711
Epoch 80, val loss: 1.8023651838302612
Epoch 90, training loss: 8.760744094848633 = 1.7830694913864136 + 1.0 * 6.97767448425293
Epoch 90, val loss: 1.7912888526916504
Epoch 100, training loss: 8.621540069580078 = 1.767229437828064 + 1.0 * 6.854310512542725
Epoch 100, val loss: 1.777279257774353
Epoch 110, training loss: 8.530326843261719 = 1.7512812614440918 + 1.0 * 6.779045581817627
Epoch 110, val loss: 1.7636364698410034
Epoch 120, training loss: 8.456311225891113 = 1.7349460124969482 + 1.0 * 6.721364974975586
Epoch 120, val loss: 1.7489995956420898
Epoch 130, training loss: 8.394746780395508 = 1.7156805992126465 + 1.0 * 6.679065704345703
Epoch 130, val loss: 1.7315574884414673
Epoch 140, training loss: 8.336304664611816 = 1.692844271659851 + 1.0 * 6.643460750579834
Epoch 140, val loss: 1.7114886045455933
Epoch 150, training loss: 8.278852462768555 = 1.6663216352462769 + 1.0 * 6.6125311851501465
Epoch 150, val loss: 1.6887933015823364
Epoch 160, training loss: 8.219419479370117 = 1.6358888149261475 + 1.0 * 6.583530426025391
Epoch 160, val loss: 1.6631447076797485
Epoch 170, training loss: 8.157033920288086 = 1.6005723476409912 + 1.0 * 6.556461811065674
Epoch 170, val loss: 1.6333143711090088
Epoch 180, training loss: 8.09931755065918 = 1.5601774454116821 + 1.0 * 6.539140224456787
Epoch 180, val loss: 1.5994473695755005
Epoch 190, training loss: 8.035568237304688 = 1.5160425901412964 + 1.0 * 6.519525527954102
Epoch 190, val loss: 1.562638521194458
Epoch 200, training loss: 7.972652912139893 = 1.46829354763031 + 1.0 * 6.504359245300293
Epoch 200, val loss: 1.5232256650924683
Epoch 210, training loss: 7.909736156463623 = 1.4178056716918945 + 1.0 * 6.4919304847717285
Epoch 210, val loss: 1.4822174310684204
Epoch 220, training loss: 7.8526787757873535 = 1.3667415380477905 + 1.0 * 6.485937118530273
Epoch 220, val loss: 1.441633939743042
Epoch 230, training loss: 7.789583683013916 = 1.3165335655212402 + 1.0 * 6.473050117492676
Epoch 230, val loss: 1.4025236368179321
Epoch 240, training loss: 7.7306671142578125 = 1.2670867443084717 + 1.0 * 6.463580131530762
Epoch 240, val loss: 1.364708662033081
Epoch 250, training loss: 7.67941951751709 = 1.218851923942566 + 1.0 * 6.460567474365234
Epoch 250, val loss: 1.3289263248443604
Epoch 260, training loss: 7.621179580688477 = 1.1722211837768555 + 1.0 * 6.448958396911621
Epoch 260, val loss: 1.2951823472976685
Epoch 270, training loss: 7.567126750946045 = 1.1263970136642456 + 1.0 * 6.44072961807251
Epoch 270, val loss: 1.2626006603240967
Epoch 280, training loss: 7.515580654144287 = 1.0809229612350464 + 1.0 * 6.434657573699951
Epoch 280, val loss: 1.2308459281921387
Epoch 290, training loss: 7.465789794921875 = 1.0365195274353027 + 1.0 * 6.429270267486572
Epoch 290, val loss: 1.2001253366470337
Epoch 300, training loss: 7.415903568267822 = 0.9935091733932495 + 1.0 * 6.422394275665283
Epoch 300, val loss: 1.1706328392028809
Epoch 310, training loss: 7.368244647979736 = 0.9511834979057312 + 1.0 * 6.4170613288879395
Epoch 310, val loss: 1.1420202255249023
Epoch 320, training loss: 7.321383476257324 = 0.9092806577682495 + 1.0 * 6.412102699279785
Epoch 320, val loss: 1.1138801574707031
Epoch 330, training loss: 7.275530815124512 = 0.8677583932876587 + 1.0 * 6.407772541046143
Epoch 330, val loss: 1.0862547159194946
Epoch 340, training loss: 7.2361273765563965 = 0.8270437121391296 + 1.0 * 6.409083843231201
Epoch 340, val loss: 1.0594241619110107
Epoch 350, training loss: 7.18851900100708 = 0.787591814994812 + 1.0 * 6.4009270668029785
Epoch 350, val loss: 1.0338250398635864
Epoch 360, training loss: 7.145625591278076 = 0.7489228844642639 + 1.0 * 6.396702766418457
Epoch 360, val loss: 1.0092281103134155
Epoch 370, training loss: 7.10974645614624 = 0.7111989259719849 + 1.0 * 6.398547649383545
Epoch 370, val loss: 0.9856618046760559
Epoch 380, training loss: 7.066142559051514 = 0.6749432682991028 + 1.0 * 6.391199111938477
Epoch 380, val loss: 0.9636906385421753
Epoch 390, training loss: 7.027335166931152 = 0.6397785544395447 + 1.0 * 6.387556552886963
Epoch 390, val loss: 0.9430414438247681
Epoch 400, training loss: 6.988701343536377 = 0.6053566336631775 + 1.0 * 6.383344650268555
Epoch 400, val loss: 0.9235635995864868
Epoch 410, training loss: 6.954070568084717 = 0.5716549158096313 + 1.0 * 6.382415771484375
Epoch 410, val loss: 0.9052441120147705
Epoch 420, training loss: 6.924902439117432 = 0.5390898585319519 + 1.0 * 6.385812759399414
Epoch 420, val loss: 0.8884438872337341
Epoch 430, training loss: 6.885018825531006 = 0.5079385042190552 + 1.0 * 6.37708044052124
Epoch 430, val loss: 0.8732497096061707
Epoch 440, training loss: 6.850608825683594 = 0.4778384268283844 + 1.0 * 6.372770309448242
Epoch 440, val loss: 0.8594573736190796
Epoch 450, training loss: 6.821672439575195 = 0.44877010583877563 + 1.0 * 6.3729023933410645
Epoch 450, val loss: 0.8471058011054993
Epoch 460, training loss: 6.798165798187256 = 0.42113789916038513 + 1.0 * 6.377027988433838
Epoch 460, val loss: 0.8364286422729492
Epoch 470, training loss: 6.763657569885254 = 0.3950687646865845 + 1.0 * 6.368588924407959
Epoch 470, val loss: 0.8274127244949341
Epoch 480, training loss: 6.734560966491699 = 0.37036919593811035 + 1.0 * 6.36419153213501
Epoch 480, val loss: 0.8198820948600769
Epoch 490, training loss: 6.7090864181518555 = 0.34694185853004456 + 1.0 * 6.362144470214844
Epoch 490, val loss: 0.8137933015823364
Epoch 500, training loss: 6.68756628036499 = 0.32485073804855347 + 1.0 * 6.362715721130371
Epoch 500, val loss: 0.809074878692627
Epoch 510, training loss: 6.662662029266357 = 0.3042438328266144 + 1.0 * 6.358417987823486
Epoch 510, val loss: 0.8056289553642273
Epoch 520, training loss: 6.640834808349609 = 0.2848452031612396 + 1.0 * 6.355989456176758
Epoch 520, val loss: 0.8033055067062378
Epoch 530, training loss: 6.626861572265625 = 0.26655298471450806 + 1.0 * 6.360308647155762
Epoch 530, val loss: 0.8020763993263245
Epoch 540, training loss: 6.603024959564209 = 0.24939492344856262 + 1.0 * 6.353630065917969
Epoch 540, val loss: 0.8019081950187683
Epoch 550, training loss: 6.584560871124268 = 0.23334859311580658 + 1.0 * 6.351212501525879
Epoch 550, val loss: 0.8025854229927063
Epoch 560, training loss: 6.565623760223389 = 0.21824394166469574 + 1.0 * 6.347379684448242
Epoch 560, val loss: 0.8040233254432678
Epoch 570, training loss: 6.559632778167725 = 0.20407219231128693 + 1.0 * 6.355560779571533
Epoch 570, val loss: 0.8061991930007935
Epoch 580, training loss: 6.5364556312561035 = 0.19085238873958588 + 1.0 * 6.3456034660339355
Epoch 580, val loss: 0.8091009259223938
Epoch 590, training loss: 6.5203657150268555 = 0.17852644622325897 + 1.0 * 6.34183931350708
Epoch 590, val loss: 0.8124465346336365
Epoch 600, training loss: 6.513969898223877 = 0.16696789860725403 + 1.0 * 6.347002029418945
Epoch 600, val loss: 0.8164068460464478
Epoch 610, training loss: 6.500143527984619 = 0.1562255322933197 + 1.0 * 6.3439178466796875
Epoch 610, val loss: 0.8208396434783936
Epoch 620, training loss: 6.483736515045166 = 0.14626239240169525 + 1.0 * 6.337474346160889
Epoch 620, val loss: 0.825588583946228
Epoch 630, training loss: 6.476334571838379 = 0.13698147237300873 + 1.0 * 6.339353084564209
Epoch 630, val loss: 0.830731213092804
Epoch 640, training loss: 6.463986873626709 = 0.12835858762264252 + 1.0 * 6.335628509521484
Epoch 640, val loss: 0.8362748026847839
Epoch 650, training loss: 6.452993869781494 = 0.12037142366170883 + 1.0 * 6.332622528076172
Epoch 650, val loss: 0.842039167881012
Epoch 660, training loss: 6.444807529449463 = 0.1129593625664711 + 1.0 * 6.33184814453125
Epoch 660, val loss: 0.848027765750885
Epoch 670, training loss: 6.448609828948975 = 0.10609331727027893 + 1.0 * 6.3425164222717285
Epoch 670, val loss: 0.8543052673339844
Epoch 680, training loss: 6.429842472076416 = 0.09977277368307114 + 1.0 * 6.330069541931152
Epoch 680, val loss: 0.8606792688369751
Epoch 690, training loss: 6.420704364776611 = 0.09392099827528 + 1.0 * 6.326783180236816
Epoch 690, val loss: 0.8670545220375061
Epoch 700, training loss: 6.414816856384277 = 0.08847600966691971 + 1.0 * 6.326340675354004
Epoch 700, val loss: 0.8736848831176758
Epoch 710, training loss: 6.410782337188721 = 0.08342794328927994 + 1.0 * 6.327354431152344
Epoch 710, val loss: 0.8805323243141174
Epoch 720, training loss: 6.4039387702941895 = 0.07879649102687836 + 1.0 * 6.3251423835754395
Epoch 720, val loss: 0.8871987462043762
Epoch 730, training loss: 6.396799564361572 = 0.07449508458375931 + 1.0 * 6.3223042488098145
Epoch 730, val loss: 0.8938254117965698
Epoch 740, training loss: 6.395798683166504 = 0.07048024237155914 + 1.0 * 6.325318336486816
Epoch 740, val loss: 0.900696337223053
Epoch 750, training loss: 6.390212059020996 = 0.06675426661968231 + 1.0 * 6.323457717895508
Epoch 750, val loss: 0.9076663255691528
Epoch 760, training loss: 6.384994983673096 = 0.06330360472202301 + 1.0 * 6.321691513061523
Epoch 760, val loss: 0.9143689870834351
Epoch 770, training loss: 6.37968111038208 = 0.06009021773934364 + 1.0 * 6.319591045379639
Epoch 770, val loss: 0.9211311340332031
Epoch 780, training loss: 6.375480651855469 = 0.057093359529972076 + 1.0 * 6.318387508392334
Epoch 780, val loss: 0.9279349446296692
Epoch 790, training loss: 6.371655464172363 = 0.054296571761369705 + 1.0 * 6.31735897064209
Epoch 790, val loss: 0.9346558451652527
Epoch 800, training loss: 6.366580486297607 = 0.05168507993221283 + 1.0 * 6.3148956298828125
Epoch 800, val loss: 0.9413588643074036
Epoch 810, training loss: 6.369935512542725 = 0.04924551770091057 + 1.0 * 6.320690155029297
Epoch 810, val loss: 0.9480225443840027
Epoch 820, training loss: 6.360078811645508 = 0.046973321586847305 + 1.0 * 6.313105583190918
Epoch 820, val loss: 0.9545530080795288
Epoch 830, training loss: 6.3595290184021 = 0.04484657570719719 + 1.0 * 6.314682483673096
Epoch 830, val loss: 0.9610479474067688
Epoch 840, training loss: 6.355432033538818 = 0.042852818965911865 + 1.0 * 6.312579154968262
Epoch 840, val loss: 0.967523992061615
Epoch 850, training loss: 6.352895736694336 = 0.04098891466856003 + 1.0 * 6.311906814575195
Epoch 850, val loss: 0.9738450646400452
Epoch 860, training loss: 6.351449966430664 = 0.03923538699746132 + 1.0 * 6.312214374542236
Epoch 860, val loss: 0.9801123738288879
Epoch 870, training loss: 6.346864700317383 = 0.037592481821775436 + 1.0 * 6.309272289276123
Epoch 870, val loss: 0.9863393306732178
Epoch 880, training loss: 6.345259666442871 = 0.03604675084352493 + 1.0 * 6.309212684631348
Epoch 880, val loss: 0.9924558401107788
Epoch 890, training loss: 6.342823505401611 = 0.03459392860531807 + 1.0 * 6.308229446411133
Epoch 890, val loss: 0.9984877705574036
Epoch 900, training loss: 6.338911056518555 = 0.033229708671569824 + 1.0 * 6.305681228637695
Epoch 900, val loss: 1.0043917894363403
Epoch 910, training loss: 6.33896017074585 = 0.03193205967545509 + 1.0 * 6.307028293609619
Epoch 910, val loss: 1.0102511644363403
Epoch 920, training loss: 6.334803104400635 = 0.03071197308599949 + 1.0 * 6.304090976715088
Epoch 920, val loss: 1.0162428617477417
Epoch 930, training loss: 6.333127498626709 = 0.029562773182988167 + 1.0 * 6.303564548492432
Epoch 930, val loss: 1.021856665611267
Epoch 940, training loss: 6.336309432983398 = 0.028477489948272705 + 1.0 * 6.307831764221191
Epoch 940, val loss: 1.027571678161621
Epoch 950, training loss: 6.328149795532227 = 0.02745998091995716 + 1.0 * 6.300689697265625
Epoch 950, val loss: 1.033057451248169
Epoch 960, training loss: 6.325804710388184 = 0.026488052681088448 + 1.0 * 6.299316883087158
Epoch 960, val loss: 1.038342833518982
Epoch 970, training loss: 6.325737476348877 = 0.02556387335062027 + 1.0 * 6.300173759460449
Epoch 970, val loss: 1.0438169240951538
Epoch 980, training loss: 6.323450565338135 = 0.02468808926641941 + 1.0 * 6.298762321472168
Epoch 980, val loss: 1.0492966175079346
Epoch 990, training loss: 6.321595191955566 = 0.02386043779551983 + 1.0 * 6.29773473739624
Epoch 990, val loss: 1.0544663667678833
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 10.545540809631348 = 1.9486958980560303 + 1.0 * 8.596844673156738
Epoch 0, val loss: 1.949766993522644
Epoch 10, training loss: 10.535685539245605 = 1.939088225364685 + 1.0 * 8.596597671508789
Epoch 10, val loss: 1.940069556236267
Epoch 20, training loss: 10.521699905395508 = 1.9270195960998535 + 1.0 * 8.594679832458496
Epoch 20, val loss: 1.927877426147461
Epoch 30, training loss: 10.488545417785645 = 1.9100033044815063 + 1.0 * 8.57854175567627
Epoch 30, val loss: 1.9109535217285156
Epoch 40, training loss: 10.37350082397461 = 1.887006163597107 + 1.0 * 8.486495018005371
Epoch 40, val loss: 1.8889390230178833
Epoch 50, training loss: 9.962668418884277 = 1.8616851568222046 + 1.0 * 8.100983619689941
Epoch 50, val loss: 1.86538565158844
Epoch 60, training loss: 9.613565444946289 = 1.8403048515319824 + 1.0 * 7.773260593414307
Epoch 60, val loss: 1.8466604948043823
Epoch 70, training loss: 9.143318176269531 = 1.8255696296691895 + 1.0 * 7.3177490234375
Epoch 70, val loss: 1.8337100744247437
Epoch 80, training loss: 8.899805068969727 = 1.8130943775177002 + 1.0 * 7.086710453033447
Epoch 80, val loss: 1.8217170238494873
Epoch 90, training loss: 8.739665985107422 = 1.7998126745224 + 1.0 * 6.939853191375732
Epoch 90, val loss: 1.809279441833496
Epoch 100, training loss: 8.610366821289062 = 1.7865757942199707 + 1.0 * 6.82379150390625
Epoch 100, val loss: 1.7974520921707153
Epoch 110, training loss: 8.507286071777344 = 1.7737836837768555 + 1.0 * 6.733502388000488
Epoch 110, val loss: 1.7859522104263306
Epoch 120, training loss: 8.433449745178223 = 1.760710597038269 + 1.0 * 6.672739028930664
Epoch 120, val loss: 1.7741624116897583
Epoch 130, training loss: 8.377605438232422 = 1.7468187808990479 + 1.0 * 6.630786895751953
Epoch 130, val loss: 1.7618602514266968
Epoch 140, training loss: 8.32889461517334 = 1.7314103841781616 + 1.0 * 6.597484588623047
Epoch 140, val loss: 1.748627781867981
Epoch 150, training loss: 8.283199310302734 = 1.7136156558990479 + 1.0 * 6.569583892822266
Epoch 150, val loss: 1.7339166402816772
Epoch 160, training loss: 8.240392684936523 = 1.6926627159118652 + 1.0 * 6.547730445861816
Epoch 160, val loss: 1.7170546054840088
Epoch 170, training loss: 8.196900367736816 = 1.6682324409484863 + 1.0 * 6.52866792678833
Epoch 170, val loss: 1.697569727897644
Epoch 180, training loss: 8.15274715423584 = 1.639965295791626 + 1.0 * 6.512781620025635
Epoch 180, val loss: 1.674967646598816
Epoch 190, training loss: 8.10532283782959 = 1.6068708896636963 + 1.0 * 6.498452186584473
Epoch 190, val loss: 1.648444414138794
Epoch 200, training loss: 8.056005477905273 = 1.5689564943313599 + 1.0 * 6.487049102783203
Epoch 200, val loss: 1.6181776523590088
Epoch 210, training loss: 8.004171371459961 = 1.5272866487503052 + 1.0 * 6.476884841918945
Epoch 210, val loss: 1.5848344564437866
Epoch 220, training loss: 7.9485931396484375 = 1.4814316034317017 + 1.0 * 6.467161655426025
Epoch 220, val loss: 1.548068642616272
Epoch 230, training loss: 7.888839244842529 = 1.4314645528793335 + 1.0 * 6.457374572753906
Epoch 230, val loss: 1.5081509351730347
Epoch 240, training loss: 7.829606056213379 = 1.378749132156372 + 1.0 * 6.450856685638428
Epoch 240, val loss: 1.4663704633712769
Epoch 250, training loss: 7.769756317138672 = 1.3258733749389648 + 1.0 * 6.443882942199707
Epoch 250, val loss: 1.4253277778625488
Epoch 260, training loss: 7.707986354827881 = 1.2738460302352905 + 1.0 * 6.434140205383301
Epoch 260, val loss: 1.3852449655532837
Epoch 270, training loss: 7.652770042419434 = 1.2229312658309937 + 1.0 * 6.42983865737915
Epoch 270, val loss: 1.3464518785476685
Epoch 280, training loss: 7.598163604736328 = 1.1742808818817139 + 1.0 * 6.423882961273193
Epoch 280, val loss: 1.3098162412643433
Epoch 290, training loss: 7.5469970703125 = 1.1285382509231567 + 1.0 * 6.418458938598633
Epoch 290, val loss: 1.2758835554122925
Epoch 300, training loss: 7.496656894683838 = 1.0856881141662598 + 1.0 * 6.410968780517578
Epoch 300, val loss: 1.2446274757385254
Epoch 310, training loss: 7.453550338745117 = 1.0455435514450073 + 1.0 * 6.40800666809082
Epoch 310, val loss: 1.2159231901168823
Epoch 320, training loss: 7.410130977630615 = 1.008103370666504 + 1.0 * 6.402027606964111
Epoch 320, val loss: 1.1896517276763916
Epoch 330, training loss: 7.371800422668457 = 0.9729404449462891 + 1.0 * 6.398859977722168
Epoch 330, val loss: 1.1654231548309326
Epoch 340, training loss: 7.331029891967773 = 0.9394264817237854 + 1.0 * 6.391603469848633
Epoch 340, val loss: 1.1429928541183472
Epoch 350, training loss: 7.293539047241211 = 0.9066171050071716 + 1.0 * 6.3869218826293945
Epoch 350, val loss: 1.1215252876281738
Epoch 360, training loss: 7.267329692840576 = 0.8740113377571106 + 1.0 * 6.393318176269531
Epoch 360, val loss: 1.100656509399414
Epoch 370, training loss: 7.225722789764404 = 0.8420724868774414 + 1.0 * 6.383650302886963
Epoch 370, val loss: 1.0805408954620361
Epoch 380, training loss: 7.188106536865234 = 0.8104243874549866 + 1.0 * 6.377682209014893
Epoch 380, val loss: 1.0614451169967651
Epoch 390, training loss: 7.152246475219727 = 0.778785228729248 + 1.0 * 6.3734612464904785
Epoch 390, val loss: 1.0425955057144165
Epoch 400, training loss: 7.118804931640625 = 0.747230052947998 + 1.0 * 6.371574878692627
Epoch 400, val loss: 1.0242568254470825
Epoch 410, training loss: 7.090052604675293 = 0.7162845134735107 + 1.0 * 6.373767852783203
Epoch 410, val loss: 1.0068480968475342
Epoch 420, training loss: 7.05194616317749 = 0.6862793564796448 + 1.0 * 6.36566686630249
Epoch 420, val loss: 0.9908869862556458
Epoch 430, training loss: 7.0220046043396 = 0.6570170521736145 + 1.0 * 6.364987373352051
Epoch 430, val loss: 0.9759387969970703
Epoch 440, training loss: 6.992980003356934 = 0.6288509368896484 + 1.0 * 6.364129066467285
Epoch 440, val loss: 0.9622079133987427
Epoch 450, training loss: 6.959583759307861 = 0.6017680764198303 + 1.0 * 6.357815742492676
Epoch 450, val loss: 0.9500342607498169
Epoch 460, training loss: 6.936923980712891 = 0.5756177306175232 + 1.0 * 6.361306190490723
Epoch 460, val loss: 0.9391249418258667
Epoch 470, training loss: 6.909033298492432 = 0.5505107045173645 + 1.0 * 6.358522415161133
Epoch 470, val loss: 0.929618239402771
Epoch 480, training loss: 6.878334999084473 = 0.5264119505882263 + 1.0 * 6.351922988891602
Epoch 480, val loss: 0.9216241240501404
Epoch 490, training loss: 6.851717948913574 = 0.5030331611633301 + 1.0 * 6.348684787750244
Epoch 490, val loss: 0.914682149887085
Epoch 500, training loss: 6.834950923919678 = 0.48038825392723083 + 1.0 * 6.354562759399414
Epoch 500, val loss: 0.9088379740715027
Epoch 510, training loss: 6.804860591888428 = 0.4587464928627014 + 1.0 * 6.346114158630371
Epoch 510, val loss: 0.9043237566947937
Epoch 520, training loss: 6.780960559844971 = 0.43779629468917847 + 1.0 * 6.343164443969727
Epoch 520, val loss: 0.9007647037506104
Epoch 530, training loss: 6.758090019226074 = 0.41745683550834656 + 1.0 * 6.340633392333984
Epoch 530, val loss: 0.8981170058250427
Epoch 540, training loss: 6.7378435134887695 = 0.39770856499671936 + 1.0 * 6.340135097503662
Epoch 540, val loss: 0.8964458703994751
Epoch 550, training loss: 6.724897861480713 = 0.37874463200569153 + 1.0 * 6.346153259277344
Epoch 550, val loss: 0.8953905701637268
Epoch 560, training loss: 6.700207710266113 = 0.3607040047645569 + 1.0 * 6.339503765106201
Epoch 560, val loss: 0.895504355430603
Epoch 570, training loss: 6.678676605224609 = 0.3433935344219208 + 1.0 * 6.335283279418945
Epoch 570, val loss: 0.8962059020996094
Epoch 580, training loss: 6.664096832275391 = 0.32674574851989746 + 1.0 * 6.337350845336914
Epoch 580, val loss: 0.8975785374641418
Epoch 590, training loss: 6.650496482849121 = 0.3108751177787781 + 1.0 * 6.339621543884277
Epoch 590, val loss: 0.8994853496551514
Epoch 600, training loss: 6.627535820007324 = 0.29575568437576294 + 1.0 * 6.331779956817627
Epoch 600, val loss: 0.9020201563835144
Epoch 610, training loss: 6.612980842590332 = 0.28125059604644775 + 1.0 * 6.331730365753174
Epoch 610, val loss: 0.9049677848815918
Epoch 620, training loss: 6.595179080963135 = 0.26737910509109497 + 1.0 * 6.3277997970581055
Epoch 620, val loss: 0.9083412885665894
Epoch 630, training loss: 6.584374904632568 = 0.25411751866340637 + 1.0 * 6.330257415771484
Epoch 630, val loss: 0.9123296737670898
Epoch 640, training loss: 6.5661940574646 = 0.24143587052822113 + 1.0 * 6.324758052825928
Epoch 640, val loss: 0.9166420102119446
Epoch 650, training loss: 6.553002834320068 = 0.22929179668426514 + 1.0 * 6.323710918426514
Epoch 650, val loss: 0.9214000701904297
Epoch 660, training loss: 6.542453289031982 = 0.2176402062177658 + 1.0 * 6.324812889099121
Epoch 660, val loss: 0.9264513850212097
Epoch 670, training loss: 6.532360553741455 = 0.20648859441280365 + 1.0 * 6.32587194442749
Epoch 670, val loss: 0.9318034648895264
Epoch 680, training loss: 6.519330978393555 = 0.19586360454559326 + 1.0 * 6.323467254638672
Epoch 680, val loss: 0.937505304813385
Epoch 690, training loss: 6.506106376647949 = 0.18572752177715302 + 1.0 * 6.32037878036499
Epoch 690, val loss: 0.9434201121330261
Epoch 700, training loss: 6.494769096374512 = 0.1760426014661789 + 1.0 * 6.318726539611816
Epoch 700, val loss: 0.9497191309928894
Epoch 710, training loss: 6.489718914031982 = 0.1667976975440979 + 1.0 * 6.322921276092529
Epoch 710, val loss: 0.956244707107544
Epoch 720, training loss: 6.477512836456299 = 0.15802694857120514 + 1.0 * 6.319485664367676
Epoch 720, val loss: 0.9627405405044556
Epoch 730, training loss: 6.465171813964844 = 0.14974066615104675 + 1.0 * 6.315431118011475
Epoch 730, val loss: 0.9696688055992126
Epoch 740, training loss: 6.456326961517334 = 0.14186351001262665 + 1.0 * 6.3144636154174805
Epoch 740, val loss: 0.9767132997512817
Epoch 750, training loss: 6.450983047485352 = 0.1343797743320465 + 1.0 * 6.316603183746338
Epoch 750, val loss: 0.9840103983879089
Epoch 760, training loss: 6.443236351013184 = 0.1273265928030014 + 1.0 * 6.3159098625183105
Epoch 760, val loss: 0.9912208914756775
Epoch 770, training loss: 6.434671878814697 = 0.12067893892526627 + 1.0 * 6.313992977142334
Epoch 770, val loss: 0.9988007545471191
Epoch 780, training loss: 6.4265055656433105 = 0.11440595239400864 + 1.0 * 6.312099456787109
Epoch 780, val loss: 1.0063267946243286
Epoch 790, training loss: 6.419248580932617 = 0.10849305987358093 + 1.0 * 6.310755729675293
Epoch 790, val loss: 1.0141105651855469
Epoch 800, training loss: 6.412919521331787 = 0.10292600095272064 + 1.0 * 6.309993743896484
Epoch 800, val loss: 1.0219312906265259
Epoch 810, training loss: 6.411993503570557 = 0.09769197553396225 + 1.0 * 6.314301490783691
Epoch 810, val loss: 1.0298168659210205
Epoch 820, training loss: 6.400982856750488 = 0.09278015792369843 + 1.0 * 6.308202743530273
Epoch 820, val loss: 1.037856936454773
Epoch 830, training loss: 6.395130157470703 = 0.08815640956163406 + 1.0 * 6.306973934173584
Epoch 830, val loss: 1.0458908081054688
Epoch 840, training loss: 6.397343158721924 = 0.08382030576467514 + 1.0 * 6.313522815704346
Epoch 840, val loss: 1.0538827180862427
Epoch 850, training loss: 6.385653495788574 = 0.07975304871797562 + 1.0 * 6.305900573730469
Epoch 850, val loss: 1.0619322061538696
Epoch 860, training loss: 6.38301944732666 = 0.07594693452119827 + 1.0 * 6.307072639465332
Epoch 860, val loss: 1.06986665725708
Epoch 870, training loss: 6.375107765197754 = 0.07238128036260605 + 1.0 * 6.3027262687683105
Epoch 870, val loss: 1.0779043436050415
Epoch 880, training loss: 6.3698344230651855 = 0.06902096420526505 + 1.0 * 6.300813674926758
Epoch 880, val loss: 1.085949182510376
Epoch 890, training loss: 6.373144149780273 = 0.06585849076509476 + 1.0 * 6.307285785675049
Epoch 890, val loss: 1.0938597917556763
Epoch 900, training loss: 6.367964744567871 = 0.06289117783308029 + 1.0 * 6.3050737380981445
Epoch 900, val loss: 1.1017438173294067
Epoch 910, training loss: 6.363349914550781 = 0.060108572244644165 + 1.0 * 6.30324125289917
Epoch 910, val loss: 1.1095585823059082
Epoch 920, training loss: 6.356024265289307 = 0.05749121308326721 + 1.0 * 6.298532962799072
Epoch 920, val loss: 1.1173131465911865
Epoch 930, training loss: 6.352517604827881 = 0.05502753332257271 + 1.0 * 6.297490119934082
Epoch 930, val loss: 1.1250947713851929
Epoch 940, training loss: 6.356724739074707 = 0.05270243436098099 + 1.0 * 6.304022312164307
Epoch 940, val loss: 1.1327027082443237
Epoch 950, training loss: 6.350964069366455 = 0.050510551780462265 + 1.0 * 6.3004536628723145
Epoch 950, val loss: 1.1400718688964844
Epoch 960, training loss: 6.344465255737305 = 0.0484529584646225 + 1.0 * 6.2960124015808105
Epoch 960, val loss: 1.1476030349731445
Epoch 970, training loss: 6.340807914733887 = 0.04650493711233139 + 1.0 * 6.294302940368652
Epoch 970, val loss: 1.1550081968307495
Epoch 980, training loss: 6.347103595733643 = 0.04466279596090317 + 1.0 * 6.302440643310547
Epoch 980, val loss: 1.162363886833191
Epoch 990, training loss: 6.342091083526611 = 0.04292553290724754 + 1.0 * 6.299165725708008
Epoch 990, val loss: 1.1694281101226807
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 10.530821800231934 = 1.933991551399231 + 1.0 * 8.596830368041992
Epoch 0, val loss: 1.924256443977356
Epoch 10, training loss: 10.521136283874512 = 1.924576997756958 + 1.0 * 8.596559524536133
Epoch 10, val loss: 1.9154645204544067
Epoch 20, training loss: 10.507068634033203 = 1.912704348564148 + 1.0 * 8.594364166259766
Epoch 20, val loss: 1.9041359424591064
Epoch 30, training loss: 10.474344253540039 = 1.8957722187042236 + 1.0 * 8.578572273254395
Epoch 30, val loss: 1.8879013061523438
Epoch 40, training loss: 10.368867874145508 = 1.872796893119812 + 1.0 * 8.496070861816406
Epoch 40, val loss: 1.8668007850646973
Epoch 50, training loss: 9.967809677124023 = 1.847520351409912 + 1.0 * 8.120288848876953
Epoch 50, val loss: 1.8446640968322754
Epoch 60, training loss: 9.633464813232422 = 1.8242117166519165 + 1.0 * 7.809252738952637
Epoch 60, val loss: 1.8250713348388672
Epoch 70, training loss: 9.210896492004395 = 1.8067737817764282 + 1.0 * 7.404122352600098
Epoch 70, val loss: 1.8109874725341797
Epoch 80, training loss: 8.930326461791992 = 1.7946889400482178 + 1.0 * 7.135637283325195
Epoch 80, val loss: 1.8014872074127197
Epoch 90, training loss: 8.779121398925781 = 1.7822129726409912 + 1.0 * 6.996908664703369
Epoch 90, val loss: 1.7907065153121948
Epoch 100, training loss: 8.662250518798828 = 1.7675058841705322 + 1.0 * 6.894744396209717
Epoch 100, val loss: 1.77809476852417
Epoch 110, training loss: 8.557458877563477 = 1.7523813247680664 + 1.0 * 6.80507755279541
Epoch 110, val loss: 1.765429973602295
Epoch 120, training loss: 8.470252990722656 = 1.7361350059509277 + 1.0 * 6.7341179847717285
Epoch 120, val loss: 1.7515525817871094
Epoch 130, training loss: 8.396430015563965 = 1.717236042022705 + 1.0 * 6.67919397354126
Epoch 130, val loss: 1.7352088689804077
Epoch 140, training loss: 8.334144592285156 = 1.695414662361145 + 1.0 * 6.638730049133301
Epoch 140, val loss: 1.716529369354248
Epoch 150, training loss: 8.279350280761719 = 1.6698757410049438 + 1.0 * 6.609474182128906
Epoch 150, val loss: 1.695006251335144
Epoch 160, training loss: 8.22278881072998 = 1.6403629779815674 + 1.0 * 6.582425594329834
Epoch 160, val loss: 1.6703567504882812
Epoch 170, training loss: 8.166054725646973 = 1.606985092163086 + 1.0 * 6.559069633483887
Epoch 170, val loss: 1.6425637006759644
Epoch 180, training loss: 8.108325958251953 = 1.5699410438537598 + 1.0 * 6.538384914398193
Epoch 180, val loss: 1.6118773221969604
Epoch 190, training loss: 8.050623893737793 = 1.5294091701507568 + 1.0 * 6.521214485168457
Epoch 190, val loss: 1.578460693359375
Epoch 200, training loss: 7.993785381317139 = 1.4857125282287598 + 1.0 * 6.508072853088379
Epoch 200, val loss: 1.5427387952804565
Epoch 210, training loss: 7.932015895843506 = 1.4402238130569458 + 1.0 * 6.49179220199585
Epoch 210, val loss: 1.5058127641677856
Epoch 220, training loss: 7.872687339782715 = 1.3933871984481812 + 1.0 * 6.479300022125244
Epoch 220, val loss: 1.468225359916687
Epoch 230, training loss: 7.82919979095459 = 1.3458375930786133 + 1.0 * 6.483362197875977
Epoch 230, val loss: 1.430791974067688
Epoch 240, training loss: 7.761967182159424 = 1.2996710538864136 + 1.0 * 6.462296009063721
Epoch 240, val loss: 1.3948050737380981
Epoch 250, training loss: 7.703970432281494 = 1.2540985345840454 + 1.0 * 6.449872016906738
Epoch 250, val loss: 1.3600424528121948
Epoch 260, training loss: 7.65054988861084 = 1.2085736989974976 + 1.0 * 6.441976070404053
Epoch 260, val loss: 1.3259199857711792
Epoch 270, training loss: 7.597614765167236 = 1.162979006767273 + 1.0 * 6.434635639190674
Epoch 270, val loss: 1.2922011613845825
Epoch 280, training loss: 7.553044319152832 = 1.117439866065979 + 1.0 * 6.435604572296143
Epoch 280, val loss: 1.25920832157135
Epoch 290, training loss: 7.496650218963623 = 1.0731077194213867 + 1.0 * 6.423542499542236
Epoch 290, val loss: 1.2275243997573853
Epoch 300, training loss: 7.446847915649414 = 1.02962327003479 + 1.0 * 6.417224884033203
Epoch 300, val loss: 1.1969764232635498
Epoch 310, training loss: 7.3992791175842285 = 0.9869670271873474 + 1.0 * 6.412312030792236
Epoch 310, val loss: 1.1674619913101196
Epoch 320, training loss: 7.352917671203613 = 0.9452225565910339 + 1.0 * 6.407695293426514
Epoch 320, val loss: 1.138905644416809
Epoch 330, training loss: 7.308129787445068 = 0.9044618010520935 + 1.0 * 6.40366792678833
Epoch 330, val loss: 1.1114848852157593
Epoch 340, training loss: 7.266664505004883 = 0.8649337291717529 + 1.0 * 6.401730537414551
Epoch 340, val loss: 1.0853739976882935
Epoch 350, training loss: 7.2223992347717285 = 0.8270347714424133 + 1.0 * 6.395364284515381
Epoch 350, val loss: 1.060786485671997
Epoch 360, training loss: 7.181159019470215 = 0.7903773784637451 + 1.0 * 6.390781402587891
Epoch 360, val loss: 1.0375550985336304
Epoch 370, training loss: 7.144711971282959 = 0.7548351883888245 + 1.0 * 6.389876842498779
Epoch 370, val loss: 1.015466570854187
Epoch 380, training loss: 7.109129905700684 = 0.7208144664764404 + 1.0 * 6.388315677642822
Epoch 380, val loss: 0.9947504997253418
Epoch 390, training loss: 7.069863319396973 = 0.6881059408187866 + 1.0 * 6.3817572593688965
Epoch 390, val loss: 0.9754623770713806
Epoch 400, training loss: 7.035751819610596 = 0.6564874053001404 + 1.0 * 6.3792643547058105
Epoch 400, val loss: 0.9573431015014648
Epoch 410, training loss: 7.005692481994629 = 0.626075267791748 + 1.0 * 6.379617214202881
Epoch 410, val loss: 0.9404371380805969
Epoch 420, training loss: 6.969560146331787 = 0.5968533158302307 + 1.0 * 6.372706890106201
Epoch 420, val loss: 0.9249044060707092
Epoch 430, training loss: 6.937882423400879 = 0.5685109496116638 + 1.0 * 6.36937141418457
Epoch 430, val loss: 0.9104301929473877
Epoch 440, training loss: 6.9183573722839355 = 0.5409902930259705 + 1.0 * 6.37736701965332
Epoch 440, val loss: 0.896933376789093
Epoch 450, training loss: 6.88214635848999 = 0.5145043134689331 + 1.0 * 6.367641925811768
Epoch 450, val loss: 0.8844994306564331
Epoch 460, training loss: 6.851919651031494 = 0.4888201355934143 + 1.0 * 6.363099575042725
Epoch 460, val loss: 0.8731119632720947
Epoch 470, training loss: 6.823919773101807 = 0.46365219354629517 + 1.0 * 6.360267639160156
Epoch 470, val loss: 0.8624733090400696
Epoch 480, training loss: 6.807102203369141 = 0.4389367401599884 + 1.0 * 6.368165493011475
Epoch 480, val loss: 0.852616012096405
Epoch 490, training loss: 6.775063991546631 = 0.41489532589912415 + 1.0 * 6.36016845703125
Epoch 490, val loss: 0.8435486555099487
Epoch 500, training loss: 6.746691703796387 = 0.39146187901496887 + 1.0 * 6.35522985458374
Epoch 500, val loss: 0.8351949453353882
Epoch 510, training loss: 6.721141815185547 = 0.3686091899871826 + 1.0 * 6.352532386779785
Epoch 510, val loss: 0.8277663588523865
Epoch 520, training loss: 6.700237274169922 = 0.3463088870048523 + 1.0 * 6.353928565979004
Epoch 520, val loss: 0.8212459683418274
Epoch 530, training loss: 6.674895286560059 = 0.3247133195400238 + 1.0 * 6.350182056427002
Epoch 530, val loss: 0.8155457973480225
Epoch 540, training loss: 6.653590679168701 = 0.3039827048778534 + 1.0 * 6.349607944488525
Epoch 540, val loss: 0.8110947012901306
Epoch 550, training loss: 6.636430263519287 = 0.28408464789390564 + 1.0 * 6.3523454666137695
Epoch 550, val loss: 0.8075754642486572
Epoch 560, training loss: 6.611870765686035 = 0.2652457654476166 + 1.0 * 6.346624851226807
Epoch 560, val loss: 0.8052138090133667
Epoch 570, training loss: 6.590496063232422 = 0.24736623466014862 + 1.0 * 6.343129634857178
Epoch 570, val loss: 0.8039386868476868
Epoch 580, training loss: 6.5801849365234375 = 0.23056139051914215 + 1.0 * 6.349623680114746
Epoch 580, val loss: 0.8036171793937683
Epoch 590, training loss: 6.557692050933838 = 0.21496903896331787 + 1.0 * 6.3427228927612305
Epoch 590, val loss: 0.804367184638977
Epoch 600, training loss: 6.547499179840088 = 0.2003999650478363 + 1.0 * 6.347099304199219
Epoch 600, val loss: 0.8059158325195312
Epoch 610, training loss: 6.526550769805908 = 0.18693046271800995 + 1.0 * 6.339620113372803
Epoch 610, val loss: 0.8082926869392395
Epoch 620, training loss: 6.510568141937256 = 0.17438854277133942 + 1.0 * 6.336179733276367
Epoch 620, val loss: 0.8113456964492798
Epoch 630, training loss: 6.500619888305664 = 0.16275694966316223 + 1.0 * 6.337862968444824
Epoch 630, val loss: 0.8149579167366028
Epoch 640, training loss: 6.491754055023193 = 0.1520463228225708 + 1.0 * 6.339707851409912
Epoch 640, val loss: 0.8190975189208984
Epoch 650, training loss: 6.485940456390381 = 0.14218752086162567 + 1.0 * 6.343752861022949
Epoch 650, val loss: 0.8235176801681519
Epoch 660, training loss: 6.467736721038818 = 0.13320580124855042 + 1.0 * 6.334530830383301
Epoch 660, val loss: 0.8282570838928223
Epoch 670, training loss: 6.454755783081055 = 0.12488465756177902 + 1.0 * 6.32987117767334
Epoch 670, val loss: 0.8333280086517334
Epoch 680, training loss: 6.445353984832764 = 0.1171988770365715 + 1.0 * 6.328155040740967
Epoch 680, val loss: 0.8388009667396545
Epoch 690, training loss: 6.437504768371582 = 0.11008048802614212 + 1.0 * 6.327424049377441
Epoch 690, val loss: 0.8445571660995483
Epoch 700, training loss: 6.437970161437988 = 0.10351553559303284 + 1.0 * 6.334454536437988
Epoch 700, val loss: 0.8504356145858765
Epoch 710, training loss: 6.4326863288879395 = 0.09748748689889908 + 1.0 * 6.335198879241943
Epoch 710, val loss: 0.8562525510787964
Epoch 720, training loss: 6.416421890258789 = 0.09197105467319489 + 1.0 * 6.324450969696045
Epoch 720, val loss: 0.8622033596038818
Epoch 730, training loss: 6.408919334411621 = 0.086847685277462 + 1.0 * 6.322071552276611
Epoch 730, val loss: 0.868321418762207
Epoch 740, training loss: 6.403312683105469 = 0.08207540214061737 + 1.0 * 6.321237087249756
Epoch 740, val loss: 0.8745452165603638
Epoch 750, training loss: 6.404732704162598 = 0.0776377022266388 + 1.0 * 6.327095031738281
Epoch 750, val loss: 0.8808292150497437
Epoch 760, training loss: 6.4000630378723145 = 0.07354797422885895 + 1.0 * 6.326515197753906
Epoch 760, val loss: 0.88714200258255
Epoch 770, training loss: 6.389013290405273 = 0.06974068284034729 + 1.0 * 6.319272518157959
Epoch 770, val loss: 0.8934897184371948
Epoch 780, training loss: 6.382401943206787 = 0.066195547580719 + 1.0 * 6.316206455230713
Epoch 780, val loss: 0.8998813629150391
Epoch 790, training loss: 6.392606258392334 = 0.06287124752998352 + 1.0 * 6.329734802246094
Epoch 790, val loss: 0.9062802791595459
Epoch 800, training loss: 6.378973484039307 = 0.059814538806676865 + 1.0 * 6.319159030914307
Epoch 800, val loss: 0.9126532077789307
Epoch 810, training loss: 6.371407508850098 = 0.0569499172270298 + 1.0 * 6.314457416534424
Epoch 810, val loss: 0.9190720319747925
Epoch 820, training loss: 6.365674018859863 = 0.054258570075035095 + 1.0 * 6.311415672302246
Epoch 820, val loss: 0.9254932403564453
Epoch 830, training loss: 6.3636393547058105 = 0.05172830447554588 + 1.0 * 6.311911106109619
Epoch 830, val loss: 0.9319437742233276
Epoch 840, training loss: 6.3597025871276855 = 0.049362849444150925 + 1.0 * 6.31033992767334
Epoch 840, val loss: 0.938321053981781
Epoch 850, training loss: 6.358158111572266 = 0.047159839421510696 + 1.0 * 6.310998439788818
Epoch 850, val loss: 0.9446966052055359
Epoch 860, training loss: 6.3542890548706055 = 0.04508379474282265 + 1.0 * 6.309205055236816
Epoch 860, val loss: 0.9510538578033447
Epoch 870, training loss: 6.349823474884033 = 0.04312468692660332 + 1.0 * 6.306698799133301
Epoch 870, val loss: 0.9573981165885925
Epoch 880, training loss: 6.366233825683594 = 0.04127500206232071 + 1.0 * 6.324958801269531
Epoch 880, val loss: 0.9637554883956909
Epoch 890, training loss: 6.346622943878174 = 0.0395488478243351 + 1.0 * 6.307074069976807
Epoch 890, val loss: 0.9698565602302551
Epoch 900, training loss: 6.3436055183410645 = 0.03793103247880936 + 1.0 * 6.3056745529174805
Epoch 900, val loss: 0.9760530591011047
Epoch 910, training loss: 6.339955806732178 = 0.036399196833372116 + 1.0 * 6.303556442260742
Epoch 910, val loss: 0.9822040796279907
Epoch 920, training loss: 6.337000370025635 = 0.034945618361234665 + 1.0 * 6.3020548820495605
Epoch 920, val loss: 0.9883351922035217
Epoch 930, training loss: 6.3646440505981445 = 0.033577680587768555 + 1.0 * 6.331066608428955
Epoch 930, val loss: 0.9944009184837341
Epoch 940, training loss: 6.337474822998047 = 0.032292820513248444 + 1.0 * 6.305181980133057
Epoch 940, val loss: 1.000146508216858
Epoch 950, training loss: 6.334553241729736 = 0.031088022515177727 + 1.0 * 6.303465366363525
Epoch 950, val loss: 1.005993366241455
Epoch 960, training loss: 6.33112907409668 = 0.029946258291602135 + 1.0 * 6.301182746887207
Epoch 960, val loss: 1.0117485523223877
Epoch 970, training loss: 6.327157497406006 = 0.028860338032245636 + 1.0 * 6.298296928405762
Epoch 970, val loss: 1.0174667835235596
Epoch 980, training loss: 6.326364040374756 = 0.02782672643661499 + 1.0 * 6.298537254333496
Epoch 980, val loss: 1.0232270956039429
Epoch 990, training loss: 6.3265910148620605 = 0.02684670127928257 + 1.0 * 6.299744129180908
Epoch 990, val loss: 1.0287925004959106
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8133895624670533
The final CL Acc:0.76420, 0.02710, The final GNN Acc:0.81321, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13086])
remove edge: torch.Size([2, 7826])
updated graph: torch.Size([2, 10356])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.53974723815918 = 1.942923903465271 + 1.0 * 8.596823692321777
Epoch 0, val loss: 1.9427083730697632
Epoch 10, training loss: 10.529512405395508 = 1.9329497814178467 + 1.0 * 8.596562385559082
Epoch 10, val loss: 1.9333457946777344
Epoch 20, training loss: 10.51484203338623 = 1.9204996824264526 + 1.0 * 8.594342231750488
Epoch 20, val loss: 1.9211980104446411
Epoch 30, training loss: 10.478558540344238 = 1.902837872505188 + 1.0 * 8.57572078704834
Epoch 30, val loss: 1.9036849737167358
Epoch 40, training loss: 10.332467079162598 = 1.8786200284957886 + 1.0 * 8.45384693145752
Epoch 40, val loss: 1.880261778831482
Epoch 50, training loss: 9.775533676147461 = 1.8516758680343628 + 1.0 * 7.923857688903809
Epoch 50, val loss: 1.8550150394439697
Epoch 60, training loss: 9.382716178894043 = 1.8296457529067993 + 1.0 * 7.553070545196533
Epoch 60, val loss: 1.8350720405578613
Epoch 70, training loss: 9.049029350280762 = 1.813748836517334 + 1.0 * 7.235280513763428
Epoch 70, val loss: 1.820169448852539
Epoch 80, training loss: 8.837688446044922 = 1.7994515895843506 + 1.0 * 7.038236618041992
Epoch 80, val loss: 1.8064630031585693
Epoch 90, training loss: 8.688158988952637 = 1.7860573530197144 + 1.0 * 6.902101993560791
Epoch 90, val loss: 1.7936527729034424
Epoch 100, training loss: 8.59974193572998 = 1.771741271018982 + 1.0 * 6.828001022338867
Epoch 100, val loss: 1.7800147533416748
Epoch 110, training loss: 8.52135944366455 = 1.756260633468628 + 1.0 * 6.765098571777344
Epoch 110, val loss: 1.7654839754104614
Epoch 120, training loss: 8.461173057556152 = 1.740280032157898 + 1.0 * 6.720893383026123
Epoch 120, val loss: 1.7501907348632812
Epoch 130, training loss: 8.40497875213623 = 1.7226004600524902 + 1.0 * 6.68237829208374
Epoch 130, val loss: 1.7337968349456787
Epoch 140, training loss: 8.350038528442383 = 1.701778531074524 + 1.0 * 6.648260116577148
Epoch 140, val loss: 1.7154414653778076
Epoch 150, training loss: 8.288230895996094 = 1.6771332025527954 + 1.0 * 6.61109733581543
Epoch 150, val loss: 1.6943354606628418
Epoch 160, training loss: 8.227927207946777 = 1.6481248140335083 + 1.0 * 6.579802513122559
Epoch 160, val loss: 1.6694443225860596
Epoch 170, training loss: 8.168364524841309 = 1.6132949590682983 + 1.0 * 6.555069446563721
Epoch 170, val loss: 1.6393028497695923
Epoch 180, training loss: 8.107156753540039 = 1.571846842765808 + 1.0 * 6.5353102684021
Epoch 180, val loss: 1.6031609773635864
Epoch 190, training loss: 8.04542350769043 = 1.5238312482833862 + 1.0 * 6.521592617034912
Epoch 190, val loss: 1.5614210367202759
Epoch 200, training loss: 7.9763641357421875 = 1.471481204032898 + 1.0 * 6.5048828125
Epoch 200, val loss: 1.5164594650268555
Epoch 210, training loss: 7.90720272064209 = 1.4160534143447876 + 1.0 * 6.491149425506592
Epoch 210, val loss: 1.4690779447555542
Epoch 220, training loss: 7.836856842041016 = 1.3587064743041992 + 1.0 * 6.478150367736816
Epoch 220, val loss: 1.420469880104065
Epoch 230, training loss: 7.769415855407715 = 1.300802230834961 + 1.0 * 6.468613624572754
Epoch 230, val loss: 1.3718560934066772
Epoch 240, training loss: 7.704272747039795 = 1.2434463500976562 + 1.0 * 6.460826396942139
Epoch 240, val loss: 1.3244144916534424
Epoch 250, training loss: 7.638608932495117 = 1.1870508193969727 + 1.0 * 6.4515581130981445
Epoch 250, val loss: 1.2783740758895874
Epoch 260, training loss: 7.576381683349609 = 1.1313989162445068 + 1.0 * 6.444982528686523
Epoch 260, val loss: 1.2335244417190552
Epoch 270, training loss: 7.513452529907227 = 1.076749563217163 + 1.0 * 6.436702728271484
Epoch 270, val loss: 1.1899837255477905
Epoch 280, training loss: 7.4543046951293945 = 1.022312045097351 + 1.0 * 6.431992530822754
Epoch 280, val loss: 1.1471058130264282
Epoch 290, training loss: 7.394144535064697 = 0.9686359167098999 + 1.0 * 6.425508499145508
Epoch 290, val loss: 1.1050803661346436
Epoch 300, training loss: 7.334216117858887 = 0.9158598780632019 + 1.0 * 6.418356418609619
Epoch 300, val loss: 1.0641069412231445
Epoch 310, training loss: 7.281620979309082 = 0.8646247386932373 + 1.0 * 6.416996479034424
Epoch 310, val loss: 1.0245537757873535
Epoch 320, training loss: 7.225405693054199 = 0.8161035776138306 + 1.0 * 6.409302234649658
Epoch 320, val loss: 0.9876680970191956
Epoch 330, training loss: 7.175552845001221 = 0.7703577876091003 + 1.0 * 6.405195236206055
Epoch 330, val loss: 0.9536080956459045
Epoch 340, training loss: 7.126716136932373 = 0.7281308174133301 + 1.0 * 6.398585319519043
Epoch 340, val loss: 0.9229274988174438
Epoch 350, training loss: 7.083985328674316 = 0.6894051432609558 + 1.0 * 6.394580364227295
Epoch 350, val loss: 0.8960436582565308
Epoch 360, training loss: 7.045838356018066 = 0.6542549729347229 + 1.0 * 6.391583442687988
Epoch 360, val loss: 0.8728482127189636
Epoch 370, training loss: 7.006566524505615 = 0.6226009726524353 + 1.0 * 6.383965492248535
Epoch 370, val loss: 0.85334312915802
Epoch 380, training loss: 6.981406211853027 = 0.5938612222671509 + 1.0 * 6.387545108795166
Epoch 380, val loss: 0.8370489478111267
Epoch 390, training loss: 6.947731018066406 = 0.5677131414413452 + 1.0 * 6.3800177574157715
Epoch 390, val loss: 0.8235704302787781
Epoch 400, training loss: 6.917496204376221 = 0.5437555909156799 + 1.0 * 6.3737406730651855
Epoch 400, val loss: 0.8124884963035583
Epoch 410, training loss: 6.890508651733398 = 0.5214717388153076 + 1.0 * 6.369036674499512
Epoch 410, val loss: 0.803269624710083
Epoch 420, training loss: 6.867175579071045 = 0.5003546476364136 + 1.0 * 6.366820812225342
Epoch 420, val loss: 0.7954176664352417
Epoch 430, training loss: 6.844632148742676 = 0.4801517426967621 + 1.0 * 6.364480495452881
Epoch 430, val loss: 0.7885991930961609
Epoch 440, training loss: 6.821069240570068 = 0.4606642425060272 + 1.0 * 6.360404968261719
Epoch 440, val loss: 0.7825323939323425
Epoch 450, training loss: 6.800693988800049 = 0.44166329503059387 + 1.0 * 6.359030723571777
Epoch 450, val loss: 0.7770982980728149
Epoch 460, training loss: 6.77777624130249 = 0.42299550771713257 + 1.0 * 6.354780673980713
Epoch 460, val loss: 0.7719024419784546
Epoch 470, training loss: 6.7566399574279785 = 0.4044720232486725 + 1.0 * 6.352168083190918
Epoch 470, val loss: 0.7669844031333923
Epoch 480, training loss: 6.748612403869629 = 0.3860851228237152 + 1.0 * 6.362527370452881
Epoch 480, val loss: 0.7623680830001831
Epoch 490, training loss: 6.716551780700684 = 0.36806750297546387 + 1.0 * 6.348484039306641
Epoch 490, val loss: 0.7581952214241028
Epoch 500, training loss: 6.696188926696777 = 0.3504396975040436 + 1.0 * 6.345749378204346
Epoch 500, val loss: 0.7545645236968994
Epoch 510, training loss: 6.678544044494629 = 0.3332594633102417 + 1.0 * 6.345284461975098
Epoch 510, val loss: 0.7515094876289368
Epoch 520, training loss: 6.662702560424805 = 0.31677091121673584 + 1.0 * 6.345931529998779
Epoch 520, val loss: 0.7491327524185181
Epoch 530, training loss: 6.640660285949707 = 0.30103522539138794 + 1.0 * 6.339624881744385
Epoch 530, val loss: 0.7473556399345398
Epoch 540, training loss: 6.624405860900879 = 0.28603217005729675 + 1.0 * 6.33837366104126
Epoch 540, val loss: 0.7462206482887268
Epoch 550, training loss: 6.60883092880249 = 0.271839439868927 + 1.0 * 6.336991310119629
Epoch 550, val loss: 0.745731770992279
Epoch 560, training loss: 6.595272064208984 = 0.2584749162197113 + 1.0 * 6.33679723739624
Epoch 560, val loss: 0.7456744313240051
Epoch 570, training loss: 6.580153942108154 = 0.24577003717422485 + 1.0 * 6.334383964538574
Epoch 570, val loss: 0.7461546659469604
Epoch 580, training loss: 6.5785136222839355 = 0.23365440964698792 + 1.0 * 6.3448591232299805
Epoch 580, val loss: 0.7470647692680359
Epoch 590, training loss: 6.553672790527344 = 0.22217606008052826 + 1.0 * 6.331496715545654
Epoch 590, val loss: 0.7482381463050842
Epoch 600, training loss: 6.540287971496582 = 0.21118612587451935 + 1.0 * 6.329102039337158
Epoch 600, val loss: 0.7497089505195618
Epoch 610, training loss: 6.530603408813477 = 0.20062623918056488 + 1.0 * 6.329977035522461
Epoch 610, val loss: 0.7514641880989075
Epoch 620, training loss: 6.520453929901123 = 0.1905149519443512 + 1.0 * 6.329938888549805
Epoch 620, val loss: 0.7534024715423584
Epoch 630, training loss: 6.506655693054199 = 0.1808415800333023 + 1.0 * 6.325814247131348
Epoch 630, val loss: 0.7555112242698669
Epoch 640, training loss: 6.496579647064209 = 0.17156563699245453 + 1.0 * 6.325014114379883
Epoch 640, val loss: 0.7578533887863159
Epoch 650, training loss: 6.488267421722412 = 0.1626921147108078 + 1.0 * 6.325575351715088
Epoch 650, val loss: 0.7603710889816284
Epoch 660, training loss: 6.483294486999512 = 0.15423992276191711 + 1.0 * 6.329054355621338
Epoch 660, val loss: 0.7629109621047974
Epoch 670, training loss: 6.468393802642822 = 0.14620546996593475 + 1.0 * 6.322188377380371
Epoch 670, val loss: 0.7656787633895874
Epoch 680, training loss: 6.458228588104248 = 0.13857156038284302 + 1.0 * 6.319656848907471
Epoch 680, val loss: 0.7685192823410034
Epoch 690, training loss: 6.449058532714844 = 0.13131144642829895 + 1.0 * 6.317747116088867
Epoch 690, val loss: 0.7716265916824341
Epoch 700, training loss: 6.45566987991333 = 0.12443354725837708 + 1.0 * 6.331236362457275
Epoch 700, val loss: 0.7748578190803528
Epoch 710, training loss: 6.435740947723389 = 0.11796285957098007 + 1.0 * 6.31777811050415
Epoch 710, val loss: 0.7781621813774109
Epoch 720, training loss: 6.428654670715332 = 0.11188922077417374 + 1.0 * 6.316765308380127
Epoch 720, val loss: 0.7815086245536804
Epoch 730, training loss: 6.423593521118164 = 0.10615605115890503 + 1.0 * 6.317437648773193
Epoch 730, val loss: 0.7850696444511414
Epoch 740, training loss: 6.415617942810059 = 0.10076888650655746 + 1.0 * 6.314848899841309
Epoch 740, val loss: 0.7887842059135437
Epoch 750, training loss: 6.4090895652771 = 0.09571114927530289 + 1.0 * 6.31337833404541
Epoch 750, val loss: 0.7925653457641602
Epoch 760, training loss: 6.40153169631958 = 0.09095200896263123 + 1.0 * 6.310579776763916
Epoch 760, val loss: 0.7965143322944641
Epoch 770, training loss: 6.401992321014404 = 0.08646969497203827 + 1.0 * 6.31552267074585
Epoch 770, val loss: 0.8005478978157043
Epoch 780, training loss: 6.3932294845581055 = 0.08226301521062851 + 1.0 * 6.310966491699219
Epoch 780, val loss: 0.8047327995300293
Epoch 790, training loss: 6.388525485992432 = 0.07832492142915726 + 1.0 * 6.3102006912231445
Epoch 790, val loss: 0.8088974356651306
Epoch 800, training loss: 6.383722305297852 = 0.07462954521179199 + 1.0 * 6.3090925216674805
Epoch 800, val loss: 0.8132362961769104
Epoch 810, training loss: 6.3780059814453125 = 0.07116981595754623 + 1.0 * 6.306836128234863
Epoch 810, val loss: 0.817521333694458
Epoch 820, training loss: 6.372187614440918 = 0.06791354715824127 + 1.0 * 6.304274082183838
Epoch 820, val loss: 0.8218998908996582
Epoch 830, training loss: 6.376042366027832 = 0.06485024839639664 + 1.0 * 6.311192035675049
Epoch 830, val loss: 0.8264259099960327
Epoch 840, training loss: 6.36941385269165 = 0.061973825097084045 + 1.0 * 6.307439804077148
Epoch 840, val loss: 0.8308337330818176
Epoch 850, training loss: 6.365722179412842 = 0.059278231114149094 + 1.0 * 6.30644416809082
Epoch 850, val loss: 0.8352930545806885
Epoch 860, training loss: 6.357983589172363 = 0.056740760803222656 + 1.0 * 6.301242828369141
Epoch 860, val loss: 0.839912474155426
Epoch 870, training loss: 6.354187488555908 = 0.054352886974811554 + 1.0 * 6.299834728240967
Epoch 870, val loss: 0.8443945050239563
Epoch 880, training loss: 6.350814342498779 = 0.0521002896130085 + 1.0 * 6.2987141609191895
Epoch 880, val loss: 0.8489646315574646
Epoch 890, training loss: 6.368936538696289 = 0.049971554428339005 + 1.0 * 6.318964958190918
Epoch 890, val loss: 0.8536191582679749
Epoch 900, training loss: 6.345489501953125 = 0.047974180430173874 + 1.0 * 6.297515392303467
Epoch 900, val loss: 0.8582114577293396
Epoch 910, training loss: 6.343801498413086 = 0.04609055817127228 + 1.0 * 6.29771089553833
Epoch 910, val loss: 0.8626518249511719
Epoch 920, training loss: 6.342378616333008 = 0.04430639371275902 + 1.0 * 6.298072338104248
Epoch 920, val loss: 0.8672196269035339
Epoch 930, training loss: 6.338046073913574 = 0.04261941835284233 + 1.0 * 6.295426845550537
Epoch 930, val loss: 0.8718209862709045
Epoch 940, training loss: 6.334956645965576 = 0.04102392494678497 + 1.0 * 6.293932914733887
Epoch 940, val loss: 0.8762643337249756
Epoch 950, training loss: 6.332945346832275 = 0.0395108237862587 + 1.0 * 6.2934346199035645
Epoch 950, val loss: 0.8807527422904968
Epoch 960, training loss: 6.338010787963867 = 0.038071949034929276 + 1.0 * 6.299938678741455
Epoch 960, val loss: 0.8853121995925903
Epoch 970, training loss: 6.335486888885498 = 0.036714401096105576 + 1.0 * 6.29877233505249
Epoch 970, val loss: 0.8896628618240356
Epoch 980, training loss: 6.327682971954346 = 0.03542693704366684 + 1.0 * 6.292255878448486
Epoch 980, val loss: 0.8940344452857971
Epoch 990, training loss: 6.328552722930908 = 0.034202881157398224 + 1.0 * 6.294349670410156
Epoch 990, val loss: 0.8983411192893982
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 10.546586990356445 = 1.9497618675231934 + 1.0 * 8.59682559967041
Epoch 0, val loss: 1.9420828819274902
Epoch 10, training loss: 10.53543472290039 = 1.9389491081237793 + 1.0 * 8.596485137939453
Epoch 10, val loss: 1.9309455156326294
Epoch 20, training loss: 10.519030570983887 = 1.9251964092254639 + 1.0 * 8.593833923339844
Epoch 20, val loss: 1.9166946411132812
Epoch 30, training loss: 10.480233192443848 = 1.9056470394134521 + 1.0 * 8.574585914611816
Epoch 30, val loss: 1.8966796398162842
Epoch 40, training loss: 10.358297348022461 = 1.879824161529541 + 1.0 * 8.478472709655762
Epoch 40, val loss: 1.871504545211792
Epoch 50, training loss: 9.937037467956543 = 1.8512370586395264 + 1.0 * 8.085800170898438
Epoch 50, val loss: 1.8444222211837769
Epoch 60, training loss: 9.58520793914795 = 1.8245984315872192 + 1.0 * 7.760609149932861
Epoch 60, val loss: 1.8211482763290405
Epoch 70, training loss: 9.179754257202148 = 1.8065667152404785 + 1.0 * 7.373188018798828
Epoch 70, val loss: 1.8058443069458008
Epoch 80, training loss: 8.885618209838867 = 1.7924362421035767 + 1.0 * 7.093181610107422
Epoch 80, val loss: 1.7942798137664795
Epoch 90, training loss: 8.700695991516113 = 1.7775206565856934 + 1.0 * 6.92317533493042
Epoch 90, val loss: 1.781706690788269
Epoch 100, training loss: 8.58390998840332 = 1.7607229948043823 + 1.0 * 6.823187351226807
Epoch 100, val loss: 1.767645239830017
Epoch 110, training loss: 8.486425399780273 = 1.7439024448394775 + 1.0 * 6.742522716522217
Epoch 110, val loss: 1.7531224489212036
Epoch 120, training loss: 8.423598289489746 = 1.7255381345748901 + 1.0 * 6.698060512542725
Epoch 120, val loss: 1.7368584871292114
Epoch 130, training loss: 8.371818542480469 = 1.7036638259887695 + 1.0 * 6.668154716491699
Epoch 130, val loss: 1.7176177501678467
Epoch 140, training loss: 8.320677757263184 = 1.6778806447982788 + 1.0 * 6.642797470092773
Epoch 140, val loss: 1.6951050758361816
Epoch 150, training loss: 8.267620086669922 = 1.6475181579589844 + 1.0 * 6.6201019287109375
Epoch 150, val loss: 1.6689945459365845
Epoch 160, training loss: 8.209037780761719 = 1.6124759912490845 + 1.0 * 6.596561431884766
Epoch 160, val loss: 1.6391901969909668
Epoch 170, training loss: 8.144248962402344 = 1.5719059705734253 + 1.0 * 6.572342872619629
Epoch 170, val loss: 1.604801893234253
Epoch 180, training loss: 8.07859992980957 = 1.5249302387237549 + 1.0 * 6.5536699295043945
Epoch 180, val loss: 1.5651891231536865
Epoch 190, training loss: 8.006447792053223 = 1.4727355241775513 + 1.0 * 6.533712387084961
Epoch 190, val loss: 1.5217068195343018
Epoch 200, training loss: 7.93590784072876 = 1.416124701499939 + 1.0 * 6.519783020019531
Epoch 200, val loss: 1.474979281425476
Epoch 210, training loss: 7.867112159729004 = 1.3570420742034912 + 1.0 * 6.510069847106934
Epoch 210, val loss: 1.4269052743911743
Epoch 220, training loss: 7.796945571899414 = 1.298089623451233 + 1.0 * 6.498856067657471
Epoch 220, val loss: 1.3795613050460815
Epoch 230, training loss: 7.729063034057617 = 1.2400037050247192 + 1.0 * 6.4890594482421875
Epoch 230, val loss: 1.3335521221160889
Epoch 240, training loss: 7.668277740478516 = 1.1840838193893433 + 1.0 * 6.484193801879883
Epoch 240, val loss: 1.2899192571640015
Epoch 250, training loss: 7.604768753051758 = 1.1326243877410889 + 1.0 * 6.47214412689209
Epoch 250, val loss: 1.2502318620681763
Epoch 260, training loss: 7.549042701721191 = 1.0851194858551025 + 1.0 * 6.463923454284668
Epoch 260, val loss: 1.2141220569610596
Epoch 270, training loss: 7.498636245727539 = 1.0410068035125732 + 1.0 * 6.457629680633545
Epoch 270, val loss: 1.1810318231582642
Epoch 280, training loss: 7.44986629486084 = 1.0004194974899292 + 1.0 * 6.449446678161621
Epoch 280, val loss: 1.1511834859848022
Epoch 290, training loss: 7.410799503326416 = 0.9626965522766113 + 1.0 * 6.448102951049805
Epoch 290, val loss: 1.1239176988601685
Epoch 300, training loss: 7.3644795417785645 = 0.9273819923400879 + 1.0 * 6.437097549438477
Epoch 300, val loss: 1.0987834930419922
Epoch 310, training loss: 7.32285213470459 = 0.8933668732643127 + 1.0 * 6.429485321044922
Epoch 310, val loss: 1.0749820470809937
Epoch 320, training loss: 7.294672966003418 = 0.8599810004234314 + 1.0 * 6.434691905975342
Epoch 320, val loss: 1.051895022392273
Epoch 330, training loss: 7.247237205505371 = 0.8277064561843872 + 1.0 * 6.419530868530273
Epoch 330, val loss: 1.0298223495483398
Epoch 340, training loss: 7.210975646972656 = 0.7957276105880737 + 1.0 * 6.415247917175293
Epoch 340, val loss: 1.0084339380264282
Epoch 350, training loss: 7.173630714416504 = 0.7639634609222412 + 1.0 * 6.409667015075684
Epoch 350, val loss: 0.98725426197052
Epoch 360, training loss: 7.138274669647217 = 0.7326423525810242 + 1.0 * 6.405632495880127
Epoch 360, val loss: 0.9667571187019348
Epoch 370, training loss: 7.103565692901611 = 0.7015071511268616 + 1.0 * 6.4020586013793945
Epoch 370, val loss: 0.9466204643249512
Epoch 380, training loss: 7.071521759033203 = 0.6709026098251343 + 1.0 * 6.400619029998779
Epoch 380, val loss: 0.9270135760307312
Epoch 390, training loss: 7.034825801849365 = 0.6411174535751343 + 1.0 * 6.393708229064941
Epoch 390, val loss: 0.9083766341209412
Epoch 400, training loss: 7.007711887359619 = 0.6120719313621521 + 1.0 * 6.395639896392822
Epoch 400, val loss: 0.8906446695327759
Epoch 410, training loss: 6.970954418182373 = 0.5842662453651428 + 1.0 * 6.386688232421875
Epoch 410, val loss: 0.8742700815200806
Epoch 420, training loss: 6.942230224609375 = 0.5577274560928345 + 1.0 * 6.38450288772583
Epoch 420, val loss: 0.8594903945922852
Epoch 430, training loss: 6.913018226623535 = 0.5322471261024475 + 1.0 * 6.380771160125732
Epoch 430, val loss: 0.8460744023323059
Epoch 440, training loss: 6.888437747955322 = 0.5080257654190063 + 1.0 * 6.3804121017456055
Epoch 440, val loss: 0.8341072797775269
Epoch 450, training loss: 6.8623528480529785 = 0.48515480756759644 + 1.0 * 6.377198219299316
Epoch 450, val loss: 0.8239098191261292
Epoch 460, training loss: 6.835482597351074 = 0.46332335472106934 + 1.0 * 6.372159481048584
Epoch 460, val loss: 0.8150767683982849
Epoch 470, training loss: 6.8197808265686035 = 0.4426209330558777 + 1.0 * 6.37716007232666
Epoch 470, val loss: 0.807475209236145
Epoch 480, training loss: 6.791823863983154 = 0.4230203926563263 + 1.0 * 6.36880350112915
Epoch 480, val loss: 0.8011505007743835
Epoch 490, training loss: 6.769278049468994 = 0.4042268395423889 + 1.0 * 6.36505126953125
Epoch 490, val loss: 0.7958316802978516
Epoch 500, training loss: 6.749786853790283 = 0.3862592279911041 + 1.0 * 6.363527774810791
Epoch 500, val loss: 0.7914151549339294
Epoch 510, training loss: 6.728084564208984 = 0.3690626621246338 + 1.0 * 6.3590216636657715
Epoch 510, val loss: 0.787929892539978
Epoch 520, training loss: 6.709262847900391 = 0.35246723890304565 + 1.0 * 6.356795787811279
Epoch 520, val loss: 0.785222589969635
Epoch 530, training loss: 6.697214603424072 = 0.3364604711532593 + 1.0 * 6.360754013061523
Epoch 530, val loss: 0.7832095623016357
Epoch 540, training loss: 6.676465034484863 = 0.32111456990242004 + 1.0 * 6.355350494384766
Epoch 540, val loss: 0.7818288207054138
Epoch 550, training loss: 6.664107799530029 = 0.30627796053886414 + 1.0 * 6.357830047607422
Epoch 550, val loss: 0.7810736894607544
Epoch 560, training loss: 6.643181324005127 = 0.29204607009887695 + 1.0 * 6.35113525390625
Epoch 560, val loss: 0.7808745503425598
Epoch 570, training loss: 6.626169204711914 = 0.2782577872276306 + 1.0 * 6.347911357879639
Epoch 570, val loss: 0.7813042402267456
Epoch 580, training loss: 6.612325668334961 = 0.2649226188659668 + 1.0 * 6.347403049468994
Epoch 580, val loss: 0.7821975946426392
Epoch 590, training loss: 6.598877906799316 = 0.252107709646225 + 1.0 * 6.346770286560059
Epoch 590, val loss: 0.7834674715995789
Epoch 600, training loss: 6.584390163421631 = 0.23979516327381134 + 1.0 * 6.344594955444336
Epoch 600, val loss: 0.785294234752655
Epoch 610, training loss: 6.575827121734619 = 0.22799576818943024 + 1.0 * 6.3478312492370605
Epoch 610, val loss: 0.7875046133995056
Epoch 620, training loss: 6.5573248863220215 = 0.21669423580169678 + 1.0 * 6.340630531311035
Epoch 620, val loss: 0.7900646924972534
Epoch 630, training loss: 6.546724319458008 = 0.2058994323015213 + 1.0 * 6.340825080871582
Epoch 630, val loss: 0.7931382656097412
Epoch 640, training loss: 6.531430244445801 = 0.19559748470783234 + 1.0 * 6.335832595825195
Epoch 640, val loss: 0.7965365648269653
Epoch 650, training loss: 6.525197982788086 = 0.1857791393995285 + 1.0 * 6.339418888092041
Epoch 650, val loss: 0.8003074526786804
Epoch 660, training loss: 6.512351036071777 = 0.1764315366744995 + 1.0 * 6.335919380187988
Epoch 660, val loss: 0.8043455481529236
Epoch 670, training loss: 6.500152587890625 = 0.16760168969631195 + 1.0 * 6.332551002502441
Epoch 670, val loss: 0.8087376356124878
Epoch 680, training loss: 6.492178916931152 = 0.15919487178325653 + 1.0 * 6.33298397064209
Epoch 680, val loss: 0.8134127855300903
Epoch 690, training loss: 6.482434272766113 = 0.15124602615833282 + 1.0 * 6.331188201904297
Epoch 690, val loss: 0.8182694911956787
Epoch 700, training loss: 6.472535610198975 = 0.14375746250152588 + 1.0 * 6.328778266906738
Epoch 700, val loss: 0.823361337184906
Epoch 710, training loss: 6.464460372924805 = 0.13663698732852936 + 1.0 * 6.327823162078857
Epoch 710, val loss: 0.8287034034729004
Epoch 720, training loss: 6.454535484313965 = 0.12991437315940857 + 1.0 * 6.324621200561523
Epoch 720, val loss: 0.8341971039772034
Epoch 730, training loss: 6.447198390960693 = 0.12355396896600723 + 1.0 * 6.323644638061523
Epoch 730, val loss: 0.8398071527481079
Epoch 740, training loss: 6.446904182434082 = 0.11752067506313324 + 1.0 * 6.329383373260498
Epoch 740, val loss: 0.8456608653068542
Epoch 750, training loss: 6.434372425079346 = 0.1118226870894432 + 1.0 * 6.322549819946289
Epoch 750, val loss: 0.8516188859939575
Epoch 760, training loss: 6.427795886993408 = 0.10641214996576309 + 1.0 * 6.321383953094482
Epoch 760, val loss: 0.8576855063438416
Epoch 770, training loss: 6.4206671714782715 = 0.10129021108150482 + 1.0 * 6.3193769454956055
Epoch 770, val loss: 0.8639055490493774
Epoch 780, training loss: 6.413434028625488 = 0.09643936902284622 + 1.0 * 6.316994667053223
Epoch 780, val loss: 0.870231032371521
Epoch 790, training loss: 6.410276889801025 = 0.09182696044445038 + 1.0 * 6.318449974060059
Epoch 790, val loss: 0.8766700029373169
Epoch 800, training loss: 6.405137062072754 = 0.08746154606342316 + 1.0 * 6.317675590515137
Epoch 800, val loss: 0.8831301927566528
Epoch 810, training loss: 6.3986663818359375 = 0.08335628360509872 + 1.0 * 6.315310001373291
Epoch 810, val loss: 0.8895806074142456
Epoch 820, training loss: 6.3903679847717285 = 0.07946504652500153 + 1.0 * 6.310903072357178
Epoch 820, val loss: 0.8962189555168152
Epoch 830, training loss: 6.400479793548584 = 0.07577815651893616 + 1.0 * 6.32470178604126
Epoch 830, val loss: 0.9029507040977478
Epoch 840, training loss: 6.3874993324279785 = 0.07229481637477875 + 1.0 * 6.315204620361328
Epoch 840, val loss: 0.9096446633338928
Epoch 850, training loss: 6.377100467681885 = 0.06901334226131439 + 1.0 * 6.308087348937988
Epoch 850, val loss: 0.9163801670074463
Epoch 860, training loss: 6.375907897949219 = 0.06589818000793457 + 1.0 * 6.310009956359863
Epoch 860, val loss: 0.9232223629951477
Epoch 870, training loss: 6.368484973907471 = 0.06295830011367798 + 1.0 * 6.3055267333984375
Epoch 870, val loss: 0.9301270246505737
Epoch 880, training loss: 6.3686957359313965 = 0.0601801723241806 + 1.0 * 6.308515548706055
Epoch 880, val loss: 0.9369094967842102
Epoch 890, training loss: 6.361434459686279 = 0.057555727660655975 + 1.0 * 6.3038787841796875
Epoch 890, val loss: 0.9438172578811646
Epoch 900, training loss: 6.357564926147461 = 0.05506119504570961 + 1.0 * 6.30250358581543
Epoch 900, val loss: 0.950729250907898
Epoch 910, training loss: 6.363264560699463 = 0.05269652232527733 + 1.0 * 6.310567855834961
Epoch 910, val loss: 0.9576341509819031
Epoch 920, training loss: 6.365764141082764 = 0.05046913027763367 + 1.0 * 6.315295219421387
Epoch 920, val loss: 0.9644094705581665
Epoch 930, training loss: 6.351333141326904 = 0.0483720526099205 + 1.0 * 6.3029608726501465
Epoch 930, val loss: 0.9710661768913269
Epoch 940, training loss: 6.3459320068359375 = 0.0463905856013298 + 1.0 * 6.299541473388672
Epoch 940, val loss: 0.9777973294258118
Epoch 950, training loss: 6.346796989440918 = 0.044509947299957275 + 1.0 * 6.3022871017456055
Epoch 950, val loss: 0.9845407009124756
Epoch 960, training loss: 6.342673301696777 = 0.04272724315524101 + 1.0 * 6.299945831298828
Epoch 960, val loss: 0.9911998510360718
Epoch 970, training loss: 6.339779376983643 = 0.04104301705956459 + 1.0 * 6.298736572265625
Epoch 970, val loss: 0.9978008270263672
Epoch 980, training loss: 6.336892127990723 = 0.039444323629140854 + 1.0 * 6.297447681427002
Epoch 980, val loss: 1.0043739080429077
Epoch 990, training loss: 6.338252067565918 = 0.03792540729045868 + 1.0 * 6.300326824188232
Epoch 990, val loss: 1.0108613967895508
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 10.538272857666016 = 1.9414116144180298 + 1.0 * 8.596860885620117
Epoch 0, val loss: 1.94572913646698
Epoch 10, training loss: 10.528491973876953 = 1.931774616241455 + 1.0 * 8.59671688079834
Epoch 10, val loss: 1.935928225517273
Epoch 20, training loss: 10.515570640563965 = 1.9199734926223755 + 1.0 * 8.595597267150879
Epoch 20, val loss: 1.9237905740737915
Epoch 30, training loss: 10.488990783691406 = 1.9035025835037231 + 1.0 * 8.585488319396973
Epoch 30, val loss: 1.9067118167877197
Epoch 40, training loss: 10.392914772033691 = 1.8802539110183716 + 1.0 * 8.51266098022461
Epoch 40, val loss: 1.8830174207687378
Epoch 50, training loss: 9.907615661621094 = 1.8529218435287476 + 1.0 * 8.054694175720215
Epoch 50, val loss: 1.8563028573989868
Epoch 60, training loss: 9.424123764038086 = 1.8285361528396606 + 1.0 * 7.595587730407715
Epoch 60, val loss: 1.8334323167800903
Epoch 70, training loss: 8.974841117858887 = 1.8112610578536987 + 1.0 * 7.163579940795898
Epoch 70, val loss: 1.8168833255767822
Epoch 80, training loss: 8.747325897216797 = 1.797545313835144 + 1.0 * 6.9497809410095215
Epoch 80, val loss: 1.8026834726333618
Epoch 90, training loss: 8.619904518127441 = 1.7821279764175415 + 1.0 * 6.8377766609191895
Epoch 90, val loss: 1.7875926494598389
Epoch 100, training loss: 8.528562545776367 = 1.7649959325790405 + 1.0 * 6.763566970825195
Epoch 100, val loss: 1.7719032764434814
Epoch 110, training loss: 8.447855949401855 = 1.7479302883148193 + 1.0 * 6.699925422668457
Epoch 110, val loss: 1.7563738822937012
Epoch 120, training loss: 8.381644248962402 = 1.7301726341247559 + 1.0 * 6.6514716148376465
Epoch 120, val loss: 1.7402570247650146
Epoch 130, training loss: 8.326679229736328 = 1.7095866203308105 + 1.0 * 6.617092132568359
Epoch 130, val loss: 1.7220897674560547
Epoch 140, training loss: 8.275965690612793 = 1.6850086450576782 + 1.0 * 6.590957164764404
Epoch 140, val loss: 1.7009532451629639
Epoch 150, training loss: 8.225282669067383 = 1.655900239944458 + 1.0 * 6.569382190704346
Epoch 150, val loss: 1.6760358810424805
Epoch 160, training loss: 8.175326347351074 = 1.621458888053894 + 1.0 * 6.553867816925049
Epoch 160, val loss: 1.6465262174606323
Epoch 170, training loss: 8.11871337890625 = 1.5814597606658936 + 1.0 * 6.537253379821777
Epoch 170, val loss: 1.6123754978179932
Epoch 180, training loss: 8.05842399597168 = 1.5354342460632324 + 1.0 * 6.522989273071289
Epoch 180, val loss: 1.5732673406600952
Epoch 190, training loss: 7.994970321655273 = 1.4832514524459839 + 1.0 * 6.51171875
Epoch 190, val loss: 1.5292015075683594
Epoch 200, training loss: 7.925381660461426 = 1.4257696866989136 + 1.0 * 6.499611854553223
Epoch 200, val loss: 1.4807298183441162
Epoch 210, training loss: 7.855730056762695 = 1.3641643524169922 + 1.0 * 6.491565704345703
Epoch 210, val loss: 1.4289700984954834
Epoch 220, training loss: 7.782565593719482 = 1.3017724752426147 + 1.0 * 6.480792999267578
Epoch 220, val loss: 1.3771650791168213
Epoch 230, training loss: 7.712140083312988 = 1.2400647401809692 + 1.0 * 6.472075462341309
Epoch 230, val loss: 1.3264302015304565
Epoch 240, training loss: 7.646120071411133 = 1.1808054447174072 + 1.0 * 6.4653143882751465
Epoch 240, val loss: 1.2784088850021362
Epoch 250, training loss: 7.583041667938232 = 1.1254979372024536 + 1.0 * 6.457543849945068
Epoch 250, val loss: 1.2342720031738281
Epoch 260, training loss: 7.525235176086426 = 1.0738224983215332 + 1.0 * 6.451412677764893
Epoch 260, val loss: 1.1936841011047363
Epoch 270, training loss: 7.472570896148682 = 1.0258737802505493 + 1.0 * 6.446697235107422
Epoch 270, val loss: 1.1567342281341553
Epoch 280, training loss: 7.419683456420898 = 0.981346845626831 + 1.0 * 6.438336372375488
Epoch 280, val loss: 1.122836947441101
Epoch 290, training loss: 7.373758792877197 = 0.93944251537323 + 1.0 * 6.434316158294678
Epoch 290, val loss: 1.091310977935791
Epoch 300, training loss: 7.334318161010742 = 0.8997379541397095 + 1.0 * 6.434580326080322
Epoch 300, val loss: 1.0618494749069214
Epoch 310, training loss: 7.2880120277404785 = 0.8623389005661011 + 1.0 * 6.425673007965088
Epoch 310, val loss: 1.0343126058578491
Epoch 320, training loss: 7.245774269104004 = 0.8263069987297058 + 1.0 * 6.419467449188232
Epoch 320, val loss: 1.0081160068511963
Epoch 330, training loss: 7.2110915184021 = 0.7911666631698608 + 1.0 * 6.419924736022949
Epoch 330, val loss: 0.9826985597610474
Epoch 340, training loss: 7.16896915435791 = 0.7573405504226685 + 1.0 * 6.411628723144531
Epoch 340, val loss: 0.9583927989006042
Epoch 350, training loss: 7.131999492645264 = 0.7246483564376831 + 1.0 * 6.407351016998291
Epoch 350, val loss: 0.9353243708610535
Epoch 360, training loss: 7.103533744812012 = 0.6927778720855713 + 1.0 * 6.4107561111450195
Epoch 360, val loss: 0.9131002426147461
Epoch 370, training loss: 7.061318397521973 = 0.6619464159011841 + 1.0 * 6.399372100830078
Epoch 370, val loss: 0.8919824957847595
Epoch 380, training loss: 7.026628017425537 = 0.6319713592529297 + 1.0 * 6.394656658172607
Epoch 380, val loss: 0.8719887137413025
Epoch 390, training loss: 7.001230239868164 = 0.60257488489151 + 1.0 * 6.398655414581299
Epoch 390, val loss: 0.8529097437858582
Epoch 400, training loss: 6.961186408996582 = 0.5738001465797424 + 1.0 * 6.387386322021484
Epoch 400, val loss: 0.8348516821861267
Epoch 410, training loss: 6.941880226135254 = 0.5454519391059875 + 1.0 * 6.396428108215332
Epoch 410, val loss: 0.8177736401557922
Epoch 420, training loss: 6.902291297912598 = 0.5178334712982178 + 1.0 * 6.384458065032959
Epoch 420, val loss: 0.8016200065612793
Epoch 430, training loss: 6.867818355560303 = 0.490657776594162 + 1.0 * 6.377160549163818
Epoch 430, val loss: 0.7864231467247009
Epoch 440, training loss: 6.836850166320801 = 0.46382832527160645 + 1.0 * 6.373021602630615
Epoch 440, val loss: 0.7719551920890808
Epoch 450, training loss: 6.82584285736084 = 0.4375213384628296 + 1.0 * 6.388321399688721
Epoch 450, val loss: 0.7584272623062134
Epoch 460, training loss: 6.78214693069458 = 0.41237008571624756 + 1.0 * 6.369776725769043
Epoch 460, val loss: 0.7460691332817078
Epoch 470, training loss: 6.753316402435303 = 0.3882042169570923 + 1.0 * 6.3651123046875
Epoch 470, val loss: 0.7349205017089844
Epoch 480, training loss: 6.737690448760986 = 0.36504891514778137 + 1.0 * 6.372641563415527
Epoch 480, val loss: 0.7247442007064819
Epoch 490, training loss: 6.705249786376953 = 0.34318917989730835 + 1.0 * 6.362060546875
Epoch 490, val loss: 0.7157495617866516
Epoch 500, training loss: 6.681497097015381 = 0.32262298464775085 + 1.0 * 6.358874320983887
Epoch 500, val loss: 0.707976758480072
Epoch 510, training loss: 6.659997940063477 = 0.3033176362514496 + 1.0 * 6.356680393218994
Epoch 510, val loss: 0.7013010382652283
Epoch 520, training loss: 6.6408491134643555 = 0.2851625680923462 + 1.0 * 6.355686664581299
Epoch 520, val loss: 0.69565349817276
Epoch 530, training loss: 6.620652198791504 = 0.26815634965896606 + 1.0 * 6.3524956703186035
Epoch 530, val loss: 0.6909847855567932
Epoch 540, training loss: 6.605782985687256 = 0.25232070684432983 + 1.0 * 6.353462219238281
Epoch 540, val loss: 0.6871351003646851
Epoch 550, training loss: 6.58590841293335 = 0.2375713586807251 + 1.0 * 6.348337173461914
Epoch 550, val loss: 0.684202253818512
Epoch 560, training loss: 6.569987773895264 = 0.22382725775241852 + 1.0 * 6.346160411834717
Epoch 560, val loss: 0.6819360852241516
Epoch 570, training loss: 6.554616928100586 = 0.21100856363773346 + 1.0 * 6.343608379364014
Epoch 570, val loss: 0.6803569793701172
Epoch 580, training loss: 6.544476509094238 = 0.19899539649486542 + 1.0 * 6.345480918884277
Epoch 580, val loss: 0.6793839335441589
Epoch 590, training loss: 6.532062530517578 = 0.18777680397033691 + 1.0 * 6.34428596496582
Epoch 590, val loss: 0.6788236498832703
Epoch 600, training loss: 6.515769004821777 = 0.17731301486492157 + 1.0 * 6.338456153869629
Epoch 600, val loss: 0.6789064407348633
Epoch 610, training loss: 6.509450912475586 = 0.1675027459859848 + 1.0 * 6.34194803237915
Epoch 610, val loss: 0.6794038414955139
Epoch 620, training loss: 6.496812343597412 = 0.15833602845668793 + 1.0 * 6.338476181030273
Epoch 620, val loss: 0.6802144646644592
Epoch 630, training loss: 6.484385967254639 = 0.1497783064842224 + 1.0 * 6.3346076011657715
Epoch 630, val loss: 0.6814858317375183
Epoch 640, training loss: 6.483163833618164 = 0.14174991846084595 + 1.0 * 6.341413974761963
Epoch 640, val loss: 0.6831177473068237
Epoch 650, training loss: 6.468287467956543 = 0.13423122465610504 + 1.0 * 6.334056377410889
Epoch 650, val loss: 0.6849194765090942
Epoch 660, training loss: 6.455306529998779 = 0.12720394134521484 + 1.0 * 6.3281025886535645
Epoch 660, val loss: 0.6871150732040405
Epoch 670, training loss: 6.463533878326416 = 0.12060333788394928 + 1.0 * 6.342930316925049
Epoch 670, val loss: 0.6895615458488464
Epoch 680, training loss: 6.441810607910156 = 0.11442363262176514 + 1.0 * 6.327386856079102
Epoch 680, val loss: 0.692047655582428
Epoch 690, training loss: 6.4341607093811035 = 0.10865975171327591 + 1.0 * 6.325500965118408
Epoch 690, val loss: 0.6949224472045898
Epoch 700, training loss: 6.427234172821045 = 0.10323454439640045 + 1.0 * 6.323999404907227
Epoch 700, val loss: 0.6979489326477051
Epoch 710, training loss: 6.430912971496582 = 0.09812604635953903 + 1.0 * 6.332787036895752
Epoch 710, val loss: 0.7010666728019714
Epoch 720, training loss: 6.414376258850098 = 0.09333255887031555 + 1.0 * 6.321043491363525
Epoch 720, val loss: 0.7043299674987793
Epoch 730, training loss: 6.4083380699157715 = 0.08882636576890945 + 1.0 * 6.319511890411377
Epoch 730, val loss: 0.7077847123146057
Epoch 740, training loss: 6.41237735748291 = 0.08457159996032715 + 1.0 * 6.327805995941162
Epoch 740, val loss: 0.7112892270088196
Epoch 750, training loss: 6.400912284851074 = 0.08057749271392822 + 1.0 * 6.3203349113464355
Epoch 750, val loss: 0.7148247957229614
Epoch 760, training loss: 6.3995442390441895 = 0.07681693136692047 + 1.0 * 6.322727203369141
Epoch 760, val loss: 0.7185327410697937
Epoch 770, training loss: 6.3948259353637695 = 0.07329091429710388 + 1.0 * 6.321535110473633
Epoch 770, val loss: 0.7222284078598022
Epoch 780, training loss: 6.3831257820129395 = 0.06997842341661453 + 1.0 * 6.31314754486084
Epoch 780, val loss: 0.7260664105415344
Epoch 790, training loss: 6.380898952484131 = 0.0668497309088707 + 1.0 * 6.314049243927002
Epoch 790, val loss: 0.7299813032150269
Epoch 800, training loss: 6.379612445831299 = 0.06389838457107544 + 1.0 * 6.315713882446289
Epoch 800, val loss: 0.733891487121582
Epoch 810, training loss: 6.372018337249756 = 0.061120834201574326 + 1.0 * 6.310897350311279
Epoch 810, val loss: 0.7378392219543457
Epoch 820, training loss: 6.36799430847168 = 0.05849951505661011 + 1.0 * 6.309494972229004
Epoch 820, val loss: 0.7418752312660217
Epoch 830, training loss: 6.379922389984131 = 0.056019872426986694 + 1.0 * 6.323902606964111
Epoch 830, val loss: 0.7458640336990356
Epoch 840, training loss: 6.361573696136475 = 0.05370122194290161 + 1.0 * 6.307872295379639
Epoch 840, val loss: 0.749851405620575
Epoch 850, training loss: 6.359360218048096 = 0.051511723548173904 + 1.0 * 6.3078484535217285
Epoch 850, val loss: 0.7539463043212891
Epoch 860, training loss: 6.36090612411499 = 0.04943449795246124 + 1.0 * 6.311471462249756
Epoch 860, val loss: 0.7580500245094299
Epoch 870, training loss: 6.3562188148498535 = 0.047469403594732285 + 1.0 * 6.308749198913574
Epoch 870, val loss: 0.7620329260826111
Epoch 880, training loss: 6.351045608520508 = 0.04562081769108772 + 1.0 * 6.305424690246582
Epoch 880, val loss: 0.7661081552505493
Epoch 890, training loss: 6.346152305603027 = 0.04386398568749428 + 1.0 * 6.30228853225708
Epoch 890, val loss: 0.7702376842498779
Epoch 900, training loss: 6.354255199432373 = 0.04219402000308037 + 1.0 * 6.312061309814453
Epoch 900, val loss: 0.7743104696273804
Epoch 910, training loss: 6.354581356048584 = 0.040623389184474945 + 1.0 * 6.313958168029785
Epoch 910, val loss: 0.7782245874404907
Epoch 920, training loss: 6.343810081481934 = 0.03913722187280655 + 1.0 * 6.304672718048096
Epoch 920, val loss: 0.7821632623672485
Epoch 930, training loss: 6.338094711303711 = 0.03773067891597748 + 1.0 * 6.300364017486572
Epoch 930, val loss: 0.7861942052841187
Epoch 940, training loss: 6.334233283996582 = 0.036387305706739426 + 1.0 * 6.297845840454102
Epoch 940, val loss: 0.790229856967926
Epoch 950, training loss: 6.334007263183594 = 0.03510582074522972 + 1.0 * 6.298901557922363
Epoch 950, val loss: 0.7942318320274353
Epoch 960, training loss: 6.337863922119141 = 0.03388523310422897 + 1.0 * 6.30397891998291
Epoch 960, val loss: 0.7981345653533936
Epoch 970, training loss: 6.331101417541504 = 0.03273343667387962 + 1.0 * 6.298367977142334
Epoch 970, val loss: 0.8020182251930237
Epoch 980, training loss: 6.329559803009033 = 0.03163633495569229 + 1.0 * 6.297923564910889
Epoch 980, val loss: 0.8058979511260986
Epoch 990, training loss: 6.33001184463501 = 0.030592722818255424 + 1.0 * 6.299418926239014
Epoch 990, val loss: 0.8097830414772034
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8350026357406432
The final CL Acc:0.79506, 0.00630, The final GNN Acc:0.83834, 0.00237
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9534])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.52265453338623 = 1.9257805347442627 + 1.0 * 8.596874237060547
Epoch 0, val loss: 1.9187092781066895
Epoch 10, training loss: 10.512977600097656 = 1.9162410497665405 + 1.0 * 8.596736907958984
Epoch 10, val loss: 1.9097061157226562
Epoch 20, training loss: 10.500325202941895 = 1.9047249555587769 + 1.0 * 8.595600128173828
Epoch 20, val loss: 1.898341178894043
Epoch 30, training loss: 10.47348690032959 = 1.8888194561004639 + 1.0 * 8.584667205810547
Epoch 30, val loss: 1.882326364517212
Epoch 40, training loss: 10.383197784423828 = 1.867208480834961 + 1.0 * 8.515989303588867
Epoch 40, val loss: 1.8611985445022583
Epoch 50, training loss: 10.050713539123535 = 1.8440067768096924 + 1.0 * 8.206707000732422
Epoch 50, val loss: 1.8399569988250732
Epoch 60, training loss: 9.804174423217773 = 1.822813630104065 + 1.0 * 7.981360912322998
Epoch 60, val loss: 1.822101354598999
Epoch 70, training loss: 9.28686237335205 = 1.8072031736373901 + 1.0 * 7.479659557342529
Epoch 70, val loss: 1.8090615272521973
Epoch 80, training loss: 8.889577865600586 = 1.7968755960464478 + 1.0 * 7.092702388763428
Epoch 80, val loss: 1.7999541759490967
Epoch 90, training loss: 8.739323616027832 = 1.7848716974258423 + 1.0 * 6.954452037811279
Epoch 90, val loss: 1.788846492767334
Epoch 100, training loss: 8.636119842529297 = 1.7696013450622559 + 1.0 * 6.866518020629883
Epoch 100, val loss: 1.7753759622573853
Epoch 110, training loss: 8.562737464904785 = 1.7541447877883911 + 1.0 * 6.808592319488525
Epoch 110, val loss: 1.7621265649795532
Epoch 120, training loss: 8.494915962219238 = 1.7387871742248535 + 1.0 * 6.756128787994385
Epoch 120, val loss: 1.7488782405853271
Epoch 130, training loss: 8.430342674255371 = 1.722206473350525 + 1.0 * 6.708136081695557
Epoch 130, val loss: 1.734378695487976
Epoch 140, training loss: 8.372037887573242 = 1.7030277252197266 + 1.0 * 6.669010162353516
Epoch 140, val loss: 1.7177603244781494
Epoch 150, training loss: 8.31718635559082 = 1.6805591583251953 + 1.0 * 6.636627674102783
Epoch 150, val loss: 1.6984996795654297
Epoch 160, training loss: 8.26490306854248 = 1.6544090509414673 + 1.0 * 6.610494136810303
Epoch 160, val loss: 1.676277756690979
Epoch 170, training loss: 8.2106351852417 = 1.6242421865463257 + 1.0 * 6.586393356323242
Epoch 170, val loss: 1.650696873664856
Epoch 180, training loss: 8.156832695007324 = 1.589473843574524 + 1.0 * 6.56735897064209
Epoch 180, val loss: 1.621308445930481
Epoch 190, training loss: 8.099517822265625 = 1.5503991842269897 + 1.0 * 6.549118518829346
Epoch 190, val loss: 1.58843994140625
Epoch 200, training loss: 8.041515350341797 = 1.5069445371627808 + 1.0 * 6.534570693969727
Epoch 200, val loss: 1.552051067352295
Epoch 210, training loss: 7.981231212615967 = 1.4592808485031128 + 1.0 * 6.5219502449035645
Epoch 210, val loss: 1.5124825239181519
Epoch 220, training loss: 7.919678211212158 = 1.407965064048767 + 1.0 * 6.511713027954102
Epoch 220, val loss: 1.4705091714859009
Epoch 230, training loss: 7.8576178550720215 = 1.355299949645996 + 1.0 * 6.502317905426025
Epoch 230, val loss: 1.4282469749450684
Epoch 240, training loss: 7.797004222869873 = 1.302664875984192 + 1.0 * 6.494339466094971
Epoch 240, val loss: 1.3868918418884277
Epoch 250, training loss: 7.735146522521973 = 1.2499449253082275 + 1.0 * 6.485201358795166
Epoch 250, val loss: 1.3462557792663574
Epoch 260, training loss: 7.673858165740967 = 1.1974235773086548 + 1.0 * 6.476434707641602
Epoch 260, val loss: 1.3065117597579956
Epoch 270, training loss: 7.63021993637085 = 1.1454418897628784 + 1.0 * 6.484777927398682
Epoch 270, val loss: 1.267844319343567
Epoch 280, training loss: 7.559700965881348 = 1.095747470855713 + 1.0 * 6.463953495025635
Epoch 280, val loss: 1.2312246561050415
Epoch 290, training loss: 7.504243850708008 = 1.0472880601882935 + 1.0 * 6.456955909729004
Epoch 290, val loss: 1.1961464881896973
Epoch 300, training loss: 7.450534343719482 = 0.999723494052887 + 1.0 * 6.45081090927124
Epoch 300, val loss: 1.1619477272033691
Epoch 310, training loss: 7.405702114105225 = 0.9529343247413635 + 1.0 * 6.452767848968506
Epoch 310, val loss: 1.1287007331848145
Epoch 320, training loss: 7.351278781890869 = 0.9077457785606384 + 1.0 * 6.443532943725586
Epoch 320, val loss: 1.0969046354293823
Epoch 330, training loss: 7.300814151763916 = 0.8639711737632751 + 1.0 * 6.436842918395996
Epoch 330, val loss: 1.0667248964309692
Epoch 340, training loss: 7.254663467407227 = 0.8214704990386963 + 1.0 * 6.433193206787109
Epoch 340, val loss: 1.037723422050476
Epoch 350, training loss: 7.212195873260498 = 0.7805410027503967 + 1.0 * 6.431654930114746
Epoch 350, val loss: 1.010206699371338
Epoch 360, training loss: 7.16627311706543 = 0.7413285970687866 + 1.0 * 6.4249444007873535
Epoch 360, val loss: 0.9846805334091187
Epoch 370, training loss: 7.1299333572387695 = 0.7036176919937134 + 1.0 * 6.426315784454346
Epoch 370, val loss: 0.9607818722724915
Epoch 380, training loss: 7.087148189544678 = 0.6677276492118835 + 1.0 * 6.4194207191467285
Epoch 380, val loss: 0.9385337829589844
Epoch 390, training loss: 7.047686576843262 = 0.6334945559501648 + 1.0 * 6.414192199707031
Epoch 390, val loss: 0.9181955456733704
Epoch 400, training loss: 7.011106014251709 = 0.6006454229354858 + 1.0 * 6.410460472106934
Epoch 400, val loss: 0.8993551135063171
Epoch 410, training loss: 6.977923393249512 = 0.5690967440605164 + 1.0 * 6.40882682800293
Epoch 410, val loss: 0.8818455338478088
Epoch 420, training loss: 6.9490766525268555 = 0.5388851761817932 + 1.0 * 6.410191535949707
Epoch 420, val loss: 0.8657749891281128
Epoch 430, training loss: 6.911415100097656 = 0.5099799633026123 + 1.0 * 6.401435375213623
Epoch 430, val loss: 0.8510406613349915
Epoch 440, training loss: 6.88116979598999 = 0.48217344284057617 + 1.0 * 6.398996353149414
Epoch 440, val loss: 0.8375729322433472
Epoch 450, training loss: 6.851712226867676 = 0.45553258061408997 + 1.0 * 6.396179676055908
Epoch 450, val loss: 0.8252401947975159
Epoch 460, training loss: 6.8235249519348145 = 0.4300418496131897 + 1.0 * 6.3934831619262695
Epoch 460, val loss: 0.8141759634017944
Epoch 470, training loss: 6.795455455780029 = 0.4055452346801758 + 1.0 * 6.3899102210998535
Epoch 470, val loss: 0.8042082786560059
Epoch 480, training loss: 6.778002738952637 = 0.3820420503616333 + 1.0 * 6.395960807800293
Epoch 480, val loss: 0.7951722145080566
Epoch 490, training loss: 6.745955467224121 = 0.3597765862941742 + 1.0 * 6.386178970336914
Epoch 490, val loss: 0.7873510122299194
Epoch 500, training loss: 6.721063613891602 = 0.33848726749420166 + 1.0 * 6.3825764656066895
Epoch 500, val loss: 0.7806152105331421
Epoch 510, training loss: 6.703182697296143 = 0.31812283396720886 + 1.0 * 6.385059833526611
Epoch 510, val loss: 0.7747505307197571
Epoch 520, training loss: 6.679964065551758 = 0.29877302050590515 + 1.0 * 6.381191253662109
Epoch 520, val loss: 0.769759476184845
Epoch 530, training loss: 6.660740375518799 = 0.2803393006324768 + 1.0 * 6.380401134490967
Epoch 530, val loss: 0.7657448053359985
Epoch 540, training loss: 6.640475749969482 = 0.26298627257347107 + 1.0 * 6.3774895668029785
Epoch 540, val loss: 0.7625672817230225
Epoch 550, training loss: 6.619792461395264 = 0.24659043550491333 + 1.0 * 6.373201847076416
Epoch 550, val loss: 0.7603538036346436
Epoch 560, training loss: 6.602624893188477 = 0.23110632598400116 + 1.0 * 6.371518611907959
Epoch 560, val loss: 0.758989691734314
Epoch 570, training loss: 6.602036476135254 = 0.21654438972473145 + 1.0 * 6.385492324829102
Epoch 570, val loss: 0.758346676826477
Epoch 580, training loss: 6.574875354766846 = 0.2029840648174286 + 1.0 * 6.371891498565674
Epoch 580, val loss: 0.7584149837493896
Epoch 590, training loss: 6.557710647583008 = 0.19028368592262268 + 1.0 * 6.367426872253418
Epoch 590, val loss: 0.7592787742614746
Epoch 600, training loss: 6.542840480804443 = 0.17836648225784302 + 1.0 * 6.364473819732666
Epoch 600, val loss: 0.7607739567756653
Epoch 610, training loss: 6.546702861785889 = 0.1672123521566391 + 1.0 * 6.379490375518799
Epoch 610, val loss: 0.7628626227378845
Epoch 620, training loss: 6.518960952758789 = 0.15689420700073242 + 1.0 * 6.362066745758057
Epoch 620, val loss: 0.7654218077659607
Epoch 630, training loss: 6.507107257843018 = 0.14731290936470032 + 1.0 * 6.3597941398620605
Epoch 630, val loss: 0.7685641646385193
Epoch 640, training loss: 6.499471664428711 = 0.138377383351326 + 1.0 * 6.3610944747924805
Epoch 640, val loss: 0.7721307873725891
Epoch 650, training loss: 6.492337226867676 = 0.13006900250911713 + 1.0 * 6.362268447875977
Epoch 650, val loss: 0.7760228514671326
Epoch 660, training loss: 6.480576992034912 = 0.12237381190061569 + 1.0 * 6.358203411102295
Epoch 660, val loss: 0.7803289890289307
Epoch 670, training loss: 6.469113826751709 = 0.11520449072122574 + 1.0 * 6.353909492492676
Epoch 670, val loss: 0.7849487662315369
Epoch 680, training loss: 6.463678359985352 = 0.10851636528968811 + 1.0 * 6.355162143707275
Epoch 680, val loss: 0.7898697257041931
Epoch 690, training loss: 6.4576334953308105 = 0.10231644660234451 + 1.0 * 6.355317115783691
Epoch 690, val loss: 0.7948976755142212
Epoch 700, training loss: 6.4490156173706055 = 0.09656919538974762 + 1.0 * 6.352446556091309
Epoch 700, val loss: 0.8002105355262756
Epoch 710, training loss: 6.443810939788818 = 0.09121672809123993 + 1.0 * 6.352594375610352
Epoch 710, val loss: 0.8056666851043701
Epoch 720, training loss: 6.434793472290039 = 0.08622698485851288 + 1.0 * 6.34856653213501
Epoch 720, val loss: 0.8112695217132568
Epoch 730, training loss: 6.436802864074707 = 0.08158165216445923 + 1.0 * 6.355221271514893
Epoch 730, val loss: 0.8169935345649719
Epoch 740, training loss: 6.422410011291504 = 0.0772683396935463 + 1.0 * 6.345141887664795
Epoch 740, val loss: 0.8227697014808655
Epoch 750, training loss: 6.418816566467285 = 0.07324393093585968 + 1.0 * 6.345572471618652
Epoch 750, val loss: 0.8286589980125427
Epoch 760, training loss: 6.417452812194824 = 0.0694800466299057 + 1.0 * 6.347972869873047
Epoch 760, val loss: 0.8346223831176758
Epoch 770, training loss: 6.407997131347656 = 0.06597216427326202 + 1.0 * 6.342024803161621
Epoch 770, val loss: 0.8406148552894592
Epoch 780, training loss: 6.4056806564331055 = 0.06268630176782608 + 1.0 * 6.342994213104248
Epoch 780, val loss: 0.8466544151306152
Epoch 790, training loss: 6.401777744293213 = 0.05961766839027405 + 1.0 * 6.342160224914551
Epoch 790, val loss: 0.8527224659919739
Epoch 800, training loss: 6.395781517028809 = 0.05675259977579117 + 1.0 * 6.339028835296631
Epoch 800, val loss: 0.8587217926979065
Epoch 810, training loss: 6.402710914611816 = 0.0540810264647007 + 1.0 * 6.348629951477051
Epoch 810, val loss: 0.8647487163543701
Epoch 820, training loss: 6.38875675201416 = 0.05159493163228035 + 1.0 * 6.337162017822266
Epoch 820, val loss: 0.8707835078239441
Epoch 830, training loss: 6.385555744171143 = 0.04926038160920143 + 1.0 * 6.336295127868652
Epoch 830, val loss: 0.876762330532074
Epoch 840, training loss: 6.383490562438965 = 0.04706643521785736 + 1.0 * 6.336424350738525
Epoch 840, val loss: 0.882782518863678
Epoch 850, training loss: 6.380148887634277 = 0.04500725492835045 + 1.0 * 6.335141658782959
Epoch 850, val loss: 0.8887680172920227
Epoch 860, training loss: 6.376814365386963 = 0.043075334280729294 + 1.0 * 6.333738803863525
Epoch 860, val loss: 0.8946588635444641
Epoch 870, training loss: 6.374298095703125 = 0.0412544384598732 + 1.0 * 6.333043575286865
Epoch 870, val loss: 0.900545060634613
Epoch 880, training loss: 6.390270709991455 = 0.039549242705106735 + 1.0 * 6.35072135925293
Epoch 880, val loss: 0.9063531756401062
Epoch 890, training loss: 6.372355937957764 = 0.037958599627017975 + 1.0 * 6.334397315979004
Epoch 890, val loss: 0.9120950102806091
Epoch 900, training loss: 6.364065170288086 = 0.03645945340394974 + 1.0 * 6.327605724334717
Epoch 900, val loss: 0.9176033139228821
Epoch 910, training loss: 6.36128568649292 = 0.03504021093249321 + 1.0 * 6.326245307922363
Epoch 910, val loss: 0.9231851696968079
Epoch 920, training loss: 6.359609127044678 = 0.033692747354507446 + 1.0 * 6.325916290283203
Epoch 920, val loss: 0.9287685751914978
Epoch 930, training loss: 6.372995853424072 = 0.03241948038339615 + 1.0 * 6.340576171875
Epoch 930, val loss: 0.934291660785675
Epoch 940, training loss: 6.3565263748168945 = 0.0312183890491724 + 1.0 * 6.325307846069336
Epoch 940, val loss: 0.939755916595459
Epoch 950, training loss: 6.355441570281982 = 0.03008575551211834 + 1.0 * 6.3253560066223145
Epoch 950, val loss: 0.9450771808624268
Epoch 960, training loss: 6.353194236755371 = 0.029013413935899734 + 1.0 * 6.324180603027344
Epoch 960, val loss: 0.9503163695335388
Epoch 970, training loss: 6.348532199859619 = 0.027997346594929695 + 1.0 * 6.320534706115723
Epoch 970, val loss: 0.9555013179779053
Epoch 980, training loss: 6.347104549407959 = 0.027031462639570236 + 1.0 * 6.320073127746582
Epoch 980, val loss: 0.9606434106826782
Epoch 990, training loss: 6.356470108032227 = 0.02611040137708187 + 1.0 * 6.330359935760498
Epoch 990, val loss: 0.9657094478607178
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 10.544060707092285 = 1.9472182989120483 + 1.0 * 8.596842765808105
Epoch 0, val loss: 1.9475849866867065
Epoch 10, training loss: 10.533361434936523 = 1.9368497133255005 + 1.0 * 8.596511840820312
Epoch 10, val loss: 1.9369251728057861
Epoch 20, training loss: 10.517547607421875 = 1.9238734245300293 + 1.0 * 8.593674659729004
Epoch 20, val loss: 1.923315167427063
Epoch 30, training loss: 10.478069305419922 = 1.9056050777435303 + 1.0 * 8.572463989257812
Epoch 30, val loss: 1.9043375253677368
Epoch 40, training loss: 10.32852554321289 = 1.8820443153381348 + 1.0 * 8.446480751037598
Epoch 40, val loss: 1.8812605142593384
Epoch 50, training loss: 9.96708869934082 = 1.856889247894287 + 1.0 * 8.110199928283691
Epoch 50, val loss: 1.8574663400650024
Epoch 60, training loss: 9.71108627319336 = 1.834753155708313 + 1.0 * 7.876332759857178
Epoch 60, val loss: 1.8370306491851807
Epoch 70, training loss: 9.324557304382324 = 1.8171544075012207 + 1.0 * 7.5074028968811035
Epoch 70, val loss: 1.8207944631576538
Epoch 80, training loss: 8.98538875579834 = 1.805842638015747 + 1.0 * 7.179545879364014
Epoch 80, val loss: 1.8114902973175049
Epoch 90, training loss: 8.794013977050781 = 1.7929526567459106 + 1.0 * 7.001060962677002
Epoch 90, val loss: 1.80036199092865
Epoch 100, training loss: 8.642359733581543 = 1.7777992486953735 + 1.0 * 6.864560604095459
Epoch 100, val loss: 1.7873121500015259
Epoch 110, training loss: 8.545280456542969 = 1.763287901878357 + 1.0 * 6.7819929122924805
Epoch 110, val loss: 1.7747002840042114
Epoch 120, training loss: 8.4772310256958 = 1.7478370666503906 + 1.0 * 6.72939395904541
Epoch 120, val loss: 1.7614028453826904
Epoch 130, training loss: 8.416704177856445 = 1.730138897895813 + 1.0 * 6.686564922332764
Epoch 130, val loss: 1.7467504739761353
Epoch 140, training loss: 8.356687545776367 = 1.7100768089294434 + 1.0 * 6.646611213684082
Epoch 140, val loss: 1.7303776741027832
Epoch 150, training loss: 8.30215072631836 = 1.6872692108154297 + 1.0 * 6.6148810386657715
Epoch 150, val loss: 1.7118408679962158
Epoch 160, training loss: 8.252352714538574 = 1.660780906677246 + 1.0 * 6.591571807861328
Epoch 160, val loss: 1.6903069019317627
Epoch 170, training loss: 8.200699806213379 = 1.6298911571502686 + 1.0 * 6.570808410644531
Epoch 170, val loss: 1.6650599241256714
Epoch 180, training loss: 8.149588584899902 = 1.5947215557098389 + 1.0 * 6.554866790771484
Epoch 180, val loss: 1.6362898349761963
Epoch 190, training loss: 8.091886520385742 = 1.5557786226272583 + 1.0 * 6.536108016967773
Epoch 190, val loss: 1.604331135749817
Epoch 200, training loss: 8.034125328063965 = 1.5127941370010376 + 1.0 * 6.521330833435059
Epoch 200, val loss: 1.5690110921859741
Epoch 210, training loss: 7.975381374359131 = 1.4665822982788086 + 1.0 * 6.508799076080322
Epoch 210, val loss: 1.531243085861206
Epoch 220, training loss: 7.915951728820801 = 1.4186726808547974 + 1.0 * 6.497279167175293
Epoch 220, val loss: 1.4922568798065186
Epoch 230, training loss: 7.854923725128174 = 1.369160771369934 + 1.0 * 6.485763072967529
Epoch 230, val loss: 1.4521087408065796
Epoch 240, training loss: 7.794519424438477 = 1.3182871341705322 + 1.0 * 6.476232051849365
Epoch 240, val loss: 1.4112536907196045
Epoch 250, training loss: 7.742247581481934 = 1.2677756547927856 + 1.0 * 6.4744720458984375
Epoch 250, val loss: 1.3716075420379639
Epoch 260, training loss: 7.680943012237549 = 1.2187880277633667 + 1.0 * 6.462154865264893
Epoch 260, val loss: 1.333607792854309
Epoch 270, training loss: 7.624245643615723 = 1.170461654663086 + 1.0 * 6.453783988952637
Epoch 270, val loss: 1.2969272136688232
Epoch 280, training loss: 7.571461200714111 = 1.1228727102279663 + 1.0 * 6.4485883712768555
Epoch 280, val loss: 1.261303186416626
Epoch 290, training loss: 7.523932456970215 = 1.076555609703064 + 1.0 * 6.447376728057861
Epoch 290, val loss: 1.227400779724121
Epoch 300, training loss: 7.469326972961426 = 1.0318048000335693 + 1.0 * 6.4375224113464355
Epoch 300, val loss: 1.1951439380645752
Epoch 310, training loss: 7.4250311851501465 = 0.9882898330688477 + 1.0 * 6.436741352081299
Epoch 310, val loss: 1.1642298698425293
Epoch 320, training loss: 7.376377105712891 = 0.9461789131164551 + 1.0 * 6.4301981925964355
Epoch 320, val loss: 1.1347438097000122
Epoch 330, training loss: 7.33045768737793 = 0.9053323268890381 + 1.0 * 6.425125598907471
Epoch 330, val loss: 1.106320858001709
Epoch 340, training loss: 7.290595054626465 = 0.865455687046051 + 1.0 * 6.425139427185059
Epoch 340, val loss: 1.0788609981536865
Epoch 350, training loss: 7.243789196014404 = 0.8266685605049133 + 1.0 * 6.417120456695557
Epoch 350, val loss: 1.0525133609771729
Epoch 360, training loss: 7.201817512512207 = 0.7889178395271301 + 1.0 * 6.412899494171143
Epoch 360, val loss: 1.0271697044372559
Epoch 370, training loss: 7.16208028793335 = 0.7523249387741089 + 1.0 * 6.409755229949951
Epoch 370, val loss: 1.0030344724655151
Epoch 380, training loss: 7.123334884643555 = 0.7172667384147644 + 1.0 * 6.406068325042725
Epoch 380, val loss: 0.9802975058555603
Epoch 390, training loss: 7.08575439453125 = 0.6834034323692322 + 1.0 * 6.402350902557373
Epoch 390, val loss: 0.9590520858764648
Epoch 400, training loss: 7.051828861236572 = 0.6508326530456543 + 1.0 * 6.400996208190918
Epoch 400, val loss: 0.9392028450965881
Epoch 410, training loss: 7.022414207458496 = 0.6197631359100342 + 1.0 * 6.402650833129883
Epoch 410, val loss: 0.9210581183433533
Epoch 420, training loss: 6.985406875610352 = 0.590164840221405 + 1.0 * 6.395242214202881
Epoch 420, val loss: 0.9046406745910645
Epoch 430, training loss: 6.953161239624023 = 0.5618655681610107 + 1.0 * 6.391295433044434
Epoch 430, val loss: 0.8897088766098022
Epoch 440, training loss: 6.925782680511475 = 0.5347296595573425 + 1.0 * 6.391053199768066
Epoch 440, val loss: 0.8761974573135376
Epoch 450, training loss: 6.894480228424072 = 0.5087050795555115 + 1.0 * 6.385775089263916
Epoch 450, val loss: 0.8640723824501038
Epoch 460, training loss: 6.866293430328369 = 0.4836581349372864 + 1.0 * 6.382635116577148
Epoch 460, val loss: 0.8532902002334595
Epoch 470, training loss: 6.844999313354492 = 0.45942386984825134 + 1.0 * 6.385575294494629
Epoch 470, val loss: 0.8435935378074646
Epoch 480, training loss: 6.81520414352417 = 0.4362486004829407 + 1.0 * 6.378955364227295
Epoch 480, val loss: 0.8351837396621704
Epoch 490, training loss: 6.789468765258789 = 0.41388270258903503 + 1.0 * 6.375586032867432
Epoch 490, val loss: 0.8278566002845764
Epoch 500, training loss: 6.764667987823486 = 0.392171174287796 + 1.0 * 6.372496604919434
Epoch 500, val loss: 0.8214988708496094
Epoch 510, training loss: 6.748260021209717 = 0.37108054757118225 + 1.0 * 6.3771796226501465
Epoch 510, val loss: 0.8159867525100708
Epoch 520, training loss: 6.720961570739746 = 0.3507683277130127 + 1.0 * 6.370193004608154
Epoch 520, val loss: 0.8113530874252319
Epoch 530, training loss: 6.698509693145752 = 0.3311491310596466 + 1.0 * 6.367360591888428
Epoch 530, val loss: 0.8076035380363464
Epoch 540, training loss: 6.676724910736084 = 0.3122655153274536 + 1.0 * 6.36445951461792
Epoch 540, val loss: 0.8046694397926331
Epoch 550, training loss: 6.660183906555176 = 0.29418784379959106 + 1.0 * 6.36599588394165
Epoch 550, val loss: 0.8025748133659363
Epoch 560, training loss: 6.638683319091797 = 0.27696874737739563 + 1.0 * 6.3617143630981445
Epoch 560, val loss: 0.8013787269592285
Epoch 570, training loss: 6.621225357055664 = 0.26058948040008545 + 1.0 * 6.360635757446289
Epoch 570, val loss: 0.8009615540504456
Epoch 580, training loss: 6.6048054695129395 = 0.24509289860725403 + 1.0 * 6.359712600708008
Epoch 580, val loss: 0.8011327981948853
Epoch 590, training loss: 6.586294651031494 = 0.23059521615505219 + 1.0 * 6.35569953918457
Epoch 590, val loss: 0.8020854592323303
Epoch 600, training loss: 6.569324493408203 = 0.21688401699066162 + 1.0 * 6.352440357208252
Epoch 600, val loss: 0.8036561012268066
Epoch 610, training loss: 6.557088851928711 = 0.20401886105537415 + 1.0 * 6.35306978225708
Epoch 610, val loss: 0.8057790398597717
Epoch 620, training loss: 6.550180912017822 = 0.19202321767807007 + 1.0 * 6.358157634735107
Epoch 620, val loss: 0.8084802031517029
Epoch 630, training loss: 6.528899192810059 = 0.1808176338672638 + 1.0 * 6.348081588745117
Epoch 630, val loss: 0.8116292357444763
Epoch 640, training loss: 6.516787052154541 = 0.17036716639995575 + 1.0 * 6.346419811248779
Epoch 640, val loss: 0.8151775598526001
Epoch 650, training loss: 6.512662887573242 = 0.16059520840644836 + 1.0 * 6.352067470550537
Epoch 650, val loss: 0.8191010355949402
Epoch 660, training loss: 6.493757724761963 = 0.1515035778284073 + 1.0 * 6.342254161834717
Epoch 660, val loss: 0.8232890963554382
Epoch 670, training loss: 6.484158515930176 = 0.14298182725906372 + 1.0 * 6.341176509857178
Epoch 670, val loss: 0.8278344869613647
Epoch 680, training loss: 6.487739562988281 = 0.13501587510108948 + 1.0 * 6.352723598480225
Epoch 680, val loss: 0.8326027393341064
Epoch 690, training loss: 6.468745231628418 = 0.12758886814117432 + 1.0 * 6.341156482696533
Epoch 690, val loss: 0.8375807404518127
Epoch 700, training loss: 6.459323406219482 = 0.1206597313284874 + 1.0 * 6.338663578033447
Epoch 700, val loss: 0.842945396900177
Epoch 710, training loss: 6.449141502380371 = 0.11415888369083405 + 1.0 * 6.334982395172119
Epoch 710, val loss: 0.8483566641807556
Epoch 720, training loss: 6.4491143226623535 = 0.10806899517774582 + 1.0 * 6.341045379638672
Epoch 720, val loss: 0.8540188670158386
Epoch 730, training loss: 6.437666416168213 = 0.10240853577852249 + 1.0 * 6.3352580070495605
Epoch 730, val loss: 0.8599926829338074
Epoch 740, training loss: 6.429330825805664 = 0.09711214154958725 + 1.0 * 6.332218647003174
Epoch 740, val loss: 0.8660672903060913
Epoch 750, training loss: 6.424228668212891 = 0.09214777499437332 + 1.0 * 6.332080841064453
Epoch 750, val loss: 0.8722468018531799
Epoch 760, training loss: 6.419073581695557 = 0.08749405294656754 + 1.0 * 6.331579685211182
Epoch 760, val loss: 0.8785143494606018
Epoch 770, training loss: 6.4118876457214355 = 0.08314891159534454 + 1.0 * 6.328738689422607
Epoch 770, val loss: 0.8848499655723572
Epoch 780, training loss: 6.407371997833252 = 0.07907955348491669 + 1.0 * 6.328292369842529
Epoch 780, val loss: 0.8911898136138916
Epoch 790, training loss: 6.40438985824585 = 0.07526818662881851 + 1.0 * 6.3291215896606445
Epoch 790, val loss: 0.8975306749343872
Epoch 800, training loss: 6.395688533782959 = 0.07170997560024261 + 1.0 * 6.323978424072266
Epoch 800, val loss: 0.9040745496749878
Epoch 810, training loss: 6.391214847564697 = 0.0683695375919342 + 1.0 * 6.322845458984375
Epoch 810, val loss: 0.9105340838432312
Epoch 820, training loss: 6.394159317016602 = 0.06523251533508301 + 1.0 * 6.328927040100098
Epoch 820, val loss: 0.9168663024902344
Epoch 830, training loss: 6.387497425079346 = 0.06228945776820183 + 1.0 * 6.3252081871032715
Epoch 830, val loss: 0.9232475161552429
Epoch 840, training loss: 6.378490447998047 = 0.059539683163166046 + 1.0 * 6.318950653076172
Epoch 840, val loss: 0.9296314120292664
Epoch 850, training loss: 6.375818252563477 = 0.056944768875837326 + 1.0 * 6.318873405456543
Epoch 850, val loss: 0.9360241293907166
Epoch 860, training loss: 6.38518762588501 = 0.05450108274817467 + 1.0 * 6.330686569213867
Epoch 860, val loss: 0.9422122240066528
Epoch 870, training loss: 6.37321138381958 = 0.05220210552215576 + 1.0 * 6.321009159088135
Epoch 870, val loss: 0.9483660459518433
Epoch 880, training loss: 6.370055198669434 = 0.05004473030567169 + 1.0 * 6.320010662078857
Epoch 880, val loss: 0.9546182155609131
Epoch 890, training loss: 6.363272666931152 = 0.048018988221883774 + 1.0 * 6.315253734588623
Epoch 890, val loss: 0.9607462286949158
Epoch 900, training loss: 6.359884738922119 = 0.04609531909227371 + 1.0 * 6.313789367675781
Epoch 900, val loss: 0.9667559862136841
Epoch 910, training loss: 6.35601282119751 = 0.044276464730501175 + 1.0 * 6.311736583709717
Epoch 910, val loss: 0.9728000164031982
Epoch 920, training loss: 6.371350288391113 = 0.042551927268505096 + 1.0 * 6.328798294067383
Epoch 920, val loss: 0.9786712527275085
Epoch 930, training loss: 6.351701736450195 = 0.04093710333108902 + 1.0 * 6.310764789581299
Epoch 930, val loss: 0.9844998717308044
Epoch 940, training loss: 6.3505778312683105 = 0.039406295865774155 + 1.0 * 6.311171531677246
Epoch 940, val loss: 0.990479052066803
Epoch 950, training loss: 6.351613521575928 = 0.03795218840241432 + 1.0 * 6.313661098480225
Epoch 950, val loss: 0.9961612224578857
Epoch 960, training loss: 6.345583915710449 = 0.036574747413396835 + 1.0 * 6.309009075164795
Epoch 960, val loss: 1.001603126525879
Epoch 970, training loss: 6.34588623046875 = 0.035268113017082214 + 1.0 * 6.310617923736572
Epoch 970, val loss: 1.0073541402816772
Epoch 980, training loss: 6.340354919433594 = 0.03403221815824509 + 1.0 * 6.3063225746154785
Epoch 980, val loss: 1.0128167867660522
Epoch 990, training loss: 6.340662002563477 = 0.03285619989037514 + 1.0 * 6.307806015014648
Epoch 990, val loss: 1.0184929370880127
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 10.548105239868164 = 1.9512436389923096 + 1.0 * 8.596861839294434
Epoch 0, val loss: 1.9536571502685547
Epoch 10, training loss: 10.537827491760254 = 1.9411914348602295 + 1.0 * 8.596635818481445
Epoch 10, val loss: 1.9428986310958862
Epoch 20, training loss: 10.523569107055664 = 1.9288357496261597 + 1.0 * 8.594733238220215
Epoch 20, val loss: 1.9292941093444824
Epoch 30, training loss: 10.489900588989258 = 1.9117717742919922 + 1.0 * 8.578128814697266
Epoch 30, val loss: 1.9102602005004883
Epoch 40, training loss: 10.342596054077148 = 1.8893332481384277 + 1.0 * 8.453262329101562
Epoch 40, val loss: 1.8859338760375977
Epoch 50, training loss: 9.797994613647461 = 1.8659659624099731 + 1.0 * 7.932028293609619
Epoch 50, val loss: 1.862096905708313
Epoch 60, training loss: 9.305720329284668 = 1.8489913940429688 + 1.0 * 7.456728935241699
Epoch 60, val loss: 1.8461254835128784
Epoch 70, training loss: 9.000225067138672 = 1.8333885669708252 + 1.0 * 7.166836261749268
Epoch 70, val loss: 1.8314542770385742
Epoch 80, training loss: 8.8676176071167 = 1.817604660987854 + 1.0 * 7.050013065338135
Epoch 80, val loss: 1.8168666362762451
Epoch 90, training loss: 8.6992769241333 = 1.8020488023757935 + 1.0 * 6.897227764129639
Epoch 90, val loss: 1.802821397781372
Epoch 100, training loss: 8.58448600769043 = 1.7872620820999146 + 1.0 * 6.797224044799805
Epoch 100, val loss: 1.7896140813827515
Epoch 110, training loss: 8.495943069458008 = 1.7727299928665161 + 1.0 * 6.723212718963623
Epoch 110, val loss: 1.7764042615890503
Epoch 120, training loss: 8.42349624633789 = 1.7572228908538818 + 1.0 * 6.666273593902588
Epoch 120, val loss: 1.762495756149292
Epoch 130, training loss: 8.372492790222168 = 1.7400574684143066 + 1.0 * 6.632435321807861
Epoch 130, val loss: 1.7477675676345825
Epoch 140, training loss: 8.319605827331543 = 1.720934510231018 + 1.0 * 6.5986714363098145
Epoch 140, val loss: 1.731707215309143
Epoch 150, training loss: 8.271333694458008 = 1.699086308479309 + 1.0 * 6.572247505187988
Epoch 150, val loss: 1.7134846448898315
Epoch 160, training loss: 8.22545051574707 = 1.673986554145813 + 1.0 * 6.551464080810547
Epoch 160, val loss: 1.692649483680725
Epoch 170, training loss: 8.179190635681152 = 1.6450790166854858 + 1.0 * 6.534111976623535
Epoch 170, val loss: 1.6686488389968872
Epoch 180, training loss: 8.139457702636719 = 1.6118378639221191 + 1.0 * 6.527619361877441
Epoch 180, val loss: 1.6411478519439697
Epoch 190, training loss: 8.082925796508789 = 1.5752129554748535 + 1.0 * 6.5077128410339355
Epoch 190, val loss: 1.6111066341400146
Epoch 200, training loss: 8.028432846069336 = 1.5352122783660889 + 1.0 * 6.493220806121826
Epoch 200, val loss: 1.5784586668014526
Epoch 210, training loss: 7.972107887268066 = 1.491753101348877 + 1.0 * 6.4803547859191895
Epoch 210, val loss: 1.5433464050292969
Epoch 220, training loss: 7.915103912353516 = 1.4449408054351807 + 1.0 * 6.470163345336914
Epoch 220, val loss: 1.5059984922409058
Epoch 230, training loss: 7.859076499938965 = 1.396510362625122 + 1.0 * 6.462565898895264
Epoch 230, val loss: 1.467799425125122
Epoch 240, training loss: 7.800936698913574 = 1.3471529483795166 + 1.0 * 6.453783988952637
Epoch 240, val loss: 1.4293973445892334
Epoch 250, training loss: 7.741670608520508 = 1.2967631816864014 + 1.0 * 6.4449076652526855
Epoch 250, val loss: 1.3905342817306519
Epoch 260, training loss: 7.685347557067871 = 1.2459179162979126 + 1.0 * 6.439429759979248
Epoch 260, val loss: 1.351568341255188
Epoch 270, training loss: 7.631526470184326 = 1.1957441568374634 + 1.0 * 6.435782432556152
Epoch 270, val loss: 1.3134734630584717
Epoch 280, training loss: 7.574617385864258 = 1.1456884145736694 + 1.0 * 6.428928852081299
Epoch 280, val loss: 1.275706171989441
Epoch 290, training loss: 7.524043560028076 = 1.0955172777175903 + 1.0 * 6.428526401519775
Epoch 290, val loss: 1.237615704536438
Epoch 300, training loss: 7.466469764709473 = 1.0457576513290405 + 1.0 * 6.420711994171143
Epoch 300, val loss: 1.1998738050460815
Epoch 310, training loss: 7.411832332611084 = 0.9964017271995544 + 1.0 * 6.415430545806885
Epoch 310, val loss: 1.1624374389648438
Epoch 320, training loss: 7.358651161193848 = 0.9473150372505188 + 1.0 * 6.4113359451293945
Epoch 320, val loss: 1.12510085105896
Epoch 330, training loss: 7.308643341064453 = 0.8987881541252136 + 1.0 * 6.409855365753174
Epoch 330, val loss: 1.08805513381958
Epoch 340, training loss: 7.260457992553711 = 0.8517354726791382 + 1.0 * 6.408722400665283
Epoch 340, val loss: 1.052125096321106
Epoch 350, training loss: 7.209433078765869 = 0.806799590587616 + 1.0 * 6.4026336669921875
Epoch 350, val loss: 1.0182676315307617
Epoch 360, training loss: 7.162411689758301 = 0.764055073261261 + 1.0 * 6.3983564376831055
Epoch 360, val loss: 0.986534833908081
Epoch 370, training loss: 7.12392520904541 = 0.723991870880127 + 1.0 * 6.399933338165283
Epoch 370, val loss: 0.9571726322174072
Epoch 380, training loss: 7.080863952636719 = 0.6871334910392761 + 1.0 * 6.393730640411377
Epoch 380, val loss: 0.9310564994812012
Epoch 390, training loss: 7.042400360107422 = 0.6528880596160889 + 1.0 * 6.389512538909912
Epoch 390, val loss: 0.9077890515327454
Epoch 400, training loss: 7.012582302093506 = 0.6209734678268433 + 1.0 * 6.391608715057373
Epoch 400, val loss: 0.8868582248687744
Epoch 410, training loss: 6.978358745574951 = 0.5912680625915527 + 1.0 * 6.387090682983398
Epoch 410, val loss: 0.8682950735092163
Epoch 420, training loss: 6.943987846374512 = 0.5636346340179443 + 1.0 * 6.3803534507751465
Epoch 420, val loss: 0.8519513010978699
Epoch 430, training loss: 6.914849281311035 = 0.5375556945800781 + 1.0 * 6.377293586730957
Epoch 430, val loss: 0.8373708128929138
Epoch 440, training loss: 6.903485298156738 = 0.5129329562187195 + 1.0 * 6.390552520751953
Epoch 440, val loss: 0.8243035078048706
Epoch 450, training loss: 6.865417957305908 = 0.4898321330547333 + 1.0 * 6.375586032867432
Epoch 450, val loss: 0.812910258769989
Epoch 460, training loss: 6.837160110473633 = 0.4679141044616699 + 1.0 * 6.369246006011963
Epoch 460, val loss: 0.8028326034545898
Epoch 470, training loss: 6.8177809715271 = 0.4468824565410614 + 1.0 * 6.370898723602295
Epoch 470, val loss: 0.7936584949493408
Epoch 480, training loss: 6.796557903289795 = 0.42679569125175476 + 1.0 * 6.369762420654297
Epoch 480, val loss: 0.7854042053222656
Epoch 490, training loss: 6.769619464874268 = 0.4074578881263733 + 1.0 * 6.362161636352539
Epoch 490, val loss: 0.77803635597229
Epoch 500, training loss: 6.748872756958008 = 0.38876205682754517 + 1.0 * 6.360110759735107
Epoch 500, val loss: 0.7714149355888367
Epoch 510, training loss: 6.729307174682617 = 0.3706090450286865 + 1.0 * 6.35869836807251
Epoch 510, val loss: 0.765396773815155
Epoch 520, training loss: 6.715002536773682 = 0.35304272174835205 + 1.0 * 6.361959934234619
Epoch 520, val loss: 0.7599495053291321
Epoch 530, training loss: 6.691164016723633 = 0.33607351779937744 + 1.0 * 6.355090618133545
Epoch 530, val loss: 0.7551923394203186
Epoch 540, training loss: 6.676381587982178 = 0.3195886015892029 + 1.0 * 6.35679292678833
Epoch 540, val loss: 0.7509492635726929
Epoch 550, training loss: 6.670216083526611 = 0.3036779761314392 + 1.0 * 6.366538047790527
Epoch 550, val loss: 0.7469300627708435
Epoch 560, training loss: 6.640048503875732 = 0.2883464992046356 + 1.0 * 6.3517022132873535
Epoch 560, val loss: 0.7434954643249512
Epoch 570, training loss: 6.620142936706543 = 0.2735520303249359 + 1.0 * 6.346590995788574
Epoch 570, val loss: 0.7405692934989929
Epoch 580, training loss: 6.604755401611328 = 0.2592295706272125 + 1.0 * 6.345525741577148
Epoch 580, val loss: 0.7379176020622253
Epoch 590, training loss: 6.598817348480225 = 0.24541977047920227 + 1.0 * 6.353397369384766
Epoch 590, val loss: 0.7356156706809998
Epoch 600, training loss: 6.575292587280273 = 0.23216593265533447 + 1.0 * 6.3431267738342285
Epoch 600, val loss: 0.7335776090621948
Epoch 610, training loss: 6.559323787689209 = 0.2195035070180893 + 1.0 * 6.339820384979248
Epoch 610, val loss: 0.7320386171340942
Epoch 620, training loss: 6.545670032501221 = 0.20737974345684052 + 1.0 * 6.338290214538574
Epoch 620, val loss: 0.7308209538459778
Epoch 630, training loss: 6.535711288452148 = 0.1957983821630478 + 1.0 * 6.3399128913879395
Epoch 630, val loss: 0.7298436760902405
Epoch 640, training loss: 6.527219295501709 = 0.18483126163482666 + 1.0 * 6.342388153076172
Epoch 640, val loss: 0.7293049097061157
Epoch 650, training loss: 6.512674331665039 = 0.17455287277698517 + 1.0 * 6.33812141418457
Epoch 650, val loss: 0.7289829254150391
Epoch 660, training loss: 6.4993367195129395 = 0.1648710072040558 + 1.0 * 6.334465503692627
Epoch 660, val loss: 0.7291677594184875
Epoch 670, training loss: 6.48737907409668 = 0.15571896731853485 + 1.0 * 6.331660270690918
Epoch 670, val loss: 0.7296180725097656
Epoch 680, training loss: 6.478206634521484 = 0.14709118008613586 + 1.0 * 6.331115245819092
Epoch 680, val loss: 0.7303129434585571
Epoch 690, training loss: 6.468924045562744 = 0.13898758590221405 + 1.0 * 6.329936504364014
Epoch 690, val loss: 0.7312889695167542
Epoch 700, training loss: 6.462639331817627 = 0.1314390003681183 + 1.0 * 6.331200122833252
Epoch 700, val loss: 0.7325121760368347
Epoch 710, training loss: 6.450717449188232 = 0.12436536699533463 + 1.0 * 6.326352119445801
Epoch 710, val loss: 0.7340273261070251
Epoch 720, training loss: 6.444149971008301 = 0.11772719025611877 + 1.0 * 6.326422691345215
Epoch 720, val loss: 0.7357215881347656
Epoch 730, training loss: 6.4391326904296875 = 0.11152221262454987 + 1.0 * 6.327610492706299
Epoch 730, val loss: 0.7376300692558289
Epoch 740, training loss: 6.4327497482299805 = 0.10575741529464722 + 1.0 * 6.326992511749268
Epoch 740, val loss: 0.7396662831306458
Epoch 750, training loss: 6.422675609588623 = 0.10037901997566223 + 1.0 * 6.322296619415283
Epoch 750, val loss: 0.7419605255126953
Epoch 760, training loss: 6.417203903198242 = 0.09533023834228516 + 1.0 * 6.321873664855957
Epoch 760, val loss: 0.7444252371788025
Epoch 770, training loss: 6.414991855621338 = 0.09059988707304001 + 1.0 * 6.324391841888428
Epoch 770, val loss: 0.747037410736084
Epoch 780, training loss: 6.410333633422852 = 0.08618073910474777 + 1.0 * 6.324152946472168
Epoch 780, val loss: 0.7497201561927795
Epoch 790, training loss: 6.400572776794434 = 0.08205931633710861 + 1.0 * 6.3185133934021
Epoch 790, val loss: 0.752530574798584
Epoch 800, training loss: 6.395105361938477 = 0.07819519191980362 + 1.0 * 6.316910266876221
Epoch 800, val loss: 0.7554399967193604
Epoch 810, training loss: 6.400421619415283 = 0.0745629370212555 + 1.0 * 6.3258585929870605
Epoch 810, val loss: 0.7584500908851624
Epoch 820, training loss: 6.387430191040039 = 0.07116703689098358 + 1.0 * 6.316263198852539
Epoch 820, val loss: 0.761589765548706
Epoch 830, training loss: 6.381033420562744 = 0.06797267496585846 + 1.0 * 6.313060760498047
Epoch 830, val loss: 0.7647393941879272
Epoch 840, training loss: 6.381198883056641 = 0.06496644020080566 + 1.0 * 6.316232204437256
Epoch 840, val loss: 0.7680082321166992
Epoch 850, training loss: 6.3802995681762695 = 0.06214524060487747 + 1.0 * 6.318154335021973
Epoch 850, val loss: 0.771380603313446
Epoch 860, training loss: 6.371474742889404 = 0.05949638783931732 + 1.0 * 6.311978340148926
Epoch 860, val loss: 0.7746410965919495
Epoch 870, training loss: 6.367648124694824 = 0.05700504779815674 + 1.0 * 6.310643196105957
Epoch 870, val loss: 0.778046190738678
Epoch 880, training loss: 6.37118673324585 = 0.05464936047792435 + 1.0 * 6.316537380218506
Epoch 880, val loss: 0.7814847230911255
Epoch 890, training loss: 6.363815784454346 = 0.05242551490664482 + 1.0 * 6.311390399932861
Epoch 890, val loss: 0.785020112991333
Epoch 900, training loss: 6.360004901885986 = 0.05034157633781433 + 1.0 * 6.30966329574585
Epoch 900, val loss: 0.7884225845336914
Epoch 910, training loss: 6.354533672332764 = 0.04836440831422806 + 1.0 * 6.306169033050537
Epoch 910, val loss: 0.7919729948043823
Epoch 920, training loss: 6.354818344116211 = 0.04648921638727188 + 1.0 * 6.308329105377197
Epoch 920, val loss: 0.7955531477928162
Epoch 930, training loss: 6.349225044250488 = 0.044710587710142136 + 1.0 * 6.304514408111572
Epoch 930, val loss: 0.7991379499435425
Epoch 940, training loss: 6.347769260406494 = 0.04303387552499771 + 1.0 * 6.30473518371582
Epoch 940, val loss: 0.8026903867721558
Epoch 950, training loss: 6.3514084815979 = 0.04144376888871193 + 1.0 * 6.309964656829834
Epoch 950, val loss: 0.8063136339187622
Epoch 960, training loss: 6.343620300292969 = 0.039931364357471466 + 1.0 * 6.303689002990723
Epoch 960, val loss: 0.8098797798156738
Epoch 970, training loss: 6.35412073135376 = 0.03850507363677025 + 1.0 * 6.315615653991699
Epoch 970, val loss: 0.8135056495666504
Epoch 980, training loss: 6.341006755828857 = 0.03714456409215927 + 1.0 * 6.30386209487915
Epoch 980, val loss: 0.8170127868652344
Epoch 990, training loss: 6.335349082946777 = 0.03586627542972565 + 1.0 * 6.299482822418213
Epoch 990, val loss: 0.8205536007881165
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.808645229309436
The final CL Acc:0.78519, 0.01318, The final GNN Acc:0.80812, 0.00155
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13174])
remove edge: torch.Size([2, 7918])
updated graph: torch.Size([2, 10536])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.54755973815918 = 1.9506962299346924 + 1.0 * 8.596863746643066
Epoch 0, val loss: 1.9532350301742554
Epoch 10, training loss: 10.536678314208984 = 1.9400705099105835 + 1.0 * 8.59660816192627
Epoch 10, val loss: 1.9426357746124268
Epoch 20, training loss: 10.520377159118652 = 1.9262189865112305 + 1.0 * 8.594158172607422
Epoch 20, val loss: 1.9287782907485962
Epoch 30, training loss: 10.480704307556152 = 1.906266212463379 + 1.0 * 8.574438095092773
Epoch 30, val loss: 1.9087063074111938
Epoch 40, training loss: 10.339310646057129 = 1.8792954683303833 + 1.0 * 8.460015296936035
Epoch 40, val loss: 1.8824000358581543
Epoch 50, training loss: 9.952900886535645 = 1.847374677658081 + 1.0 * 8.105525970458984
Epoch 50, val loss: 1.851755976676941
Epoch 60, training loss: 9.730961799621582 = 1.8173505067825317 + 1.0 * 7.913610935211182
Epoch 60, val loss: 1.824026107788086
Epoch 70, training loss: 9.344612121582031 = 1.79153573513031 + 1.0 * 7.553076267242432
Epoch 70, val loss: 1.8000705242156982
Epoch 80, training loss: 9.030669212341309 = 1.771238088607788 + 1.0 * 7.259430885314941
Epoch 80, val loss: 1.7818015813827515
Epoch 90, training loss: 8.930269241333008 = 1.7494267225265503 + 1.0 * 7.180842876434326
Epoch 90, val loss: 1.7606055736541748
Epoch 100, training loss: 8.834442138671875 = 1.7235337495803833 + 1.0 * 7.110908031463623
Epoch 100, val loss: 1.7364128828048706
Epoch 110, training loss: 8.718728065490723 = 1.697880506515503 + 1.0 * 7.020847320556641
Epoch 110, val loss: 1.7130247354507446
Epoch 120, training loss: 8.602851867675781 = 1.6704328060150146 + 1.0 * 6.932419300079346
Epoch 120, val loss: 1.687617301940918
Epoch 130, training loss: 8.503997802734375 = 1.639215111732483 + 1.0 * 6.864782810211182
Epoch 130, val loss: 1.6596400737762451
Epoch 140, training loss: 8.405169486999512 = 1.6037418842315674 + 1.0 * 6.801427364349365
Epoch 140, val loss: 1.6290665864944458
Epoch 150, training loss: 8.314658164978027 = 1.5649235248565674 + 1.0 * 6.749734401702881
Epoch 150, val loss: 1.5958799123764038
Epoch 160, training loss: 8.221485137939453 = 1.5236272811889648 + 1.0 * 6.6978583335876465
Epoch 160, val loss: 1.5603077411651611
Epoch 170, training loss: 8.139927864074707 = 1.4793788194656372 + 1.0 * 6.660549163818359
Epoch 170, val loss: 1.5225343704223633
Epoch 180, training loss: 8.060355186462402 = 1.4326444864273071 + 1.0 * 6.627710342407227
Epoch 180, val loss: 1.4829298257827759
Epoch 190, training loss: 7.98678731918335 = 1.3845432996749878 + 1.0 * 6.602243900299072
Epoch 190, val loss: 1.4423404932022095
Epoch 200, training loss: 7.916402339935303 = 1.336230754852295 + 1.0 * 6.580171585083008
Epoch 200, val loss: 1.401994228363037
Epoch 210, training loss: 7.851054668426514 = 1.2874078750610352 + 1.0 * 6.5636467933654785
Epoch 210, val loss: 1.3615765571594238
Epoch 220, training loss: 7.789210796356201 = 1.238888144493103 + 1.0 * 6.550322532653809
Epoch 220, val loss: 1.3217722177505493
Epoch 230, training loss: 7.729663848876953 = 1.191218376159668 + 1.0 * 6.538445472717285
Epoch 230, val loss: 1.2830437421798706
Epoch 240, training loss: 7.670487880706787 = 1.1438913345336914 + 1.0 * 6.526596546173096
Epoch 240, val loss: 1.2450753450393677
Epoch 250, training loss: 7.613731384277344 = 1.0971295833587646 + 1.0 * 6.5166015625
Epoch 250, val loss: 1.2079813480377197
Epoch 260, training loss: 7.569000244140625 = 1.0512573719024658 + 1.0 * 6.51774263381958
Epoch 260, val loss: 1.1721655130386353
Epoch 270, training loss: 7.506157398223877 = 1.0067750215530396 + 1.0 * 6.499382495880127
Epoch 270, val loss: 1.1380401849746704
Epoch 280, training loss: 7.454531192779541 = 0.9633468389511108 + 1.0 * 6.491184234619141
Epoch 280, val loss: 1.1049727201461792
Epoch 290, training loss: 7.40526008605957 = 0.920680582523346 + 1.0 * 6.484579563140869
Epoch 290, val loss: 1.0727487802505493
Epoch 300, training loss: 7.357182502746582 = 0.8792078495025635 + 1.0 * 6.4779744148254395
Epoch 300, val loss: 1.0418472290039062
Epoch 310, training loss: 7.308910846710205 = 0.8391542434692383 + 1.0 * 6.469756603240967
Epoch 310, val loss: 1.0122880935668945
Epoch 320, training loss: 7.26676607131958 = 0.8002756834030151 + 1.0 * 6.466490268707275
Epoch 320, val loss: 0.9840165972709656
Epoch 330, training loss: 7.224527835845947 = 0.7629185914993286 + 1.0 * 6.461609363555908
Epoch 330, val loss: 0.9574234485626221
Epoch 340, training loss: 7.179257392883301 = 0.7272819876670837 + 1.0 * 6.451975345611572
Epoch 340, val loss: 0.9326462149620056
Epoch 350, training loss: 7.140399932861328 = 0.6931576132774353 + 1.0 * 6.447242259979248
Epoch 350, val loss: 0.9095855355262756
Epoch 360, training loss: 7.106463432312012 = 0.6607487201690674 + 1.0 * 6.445714950561523
Epoch 360, val loss: 0.8886736631393433
Epoch 370, training loss: 7.068437576293945 = 0.6303159594535828 + 1.0 * 6.438121795654297
Epoch 370, val loss: 0.8698995113372803
Epoch 380, training loss: 7.035924911499023 = 0.6013967394828796 + 1.0 * 6.434528350830078
Epoch 380, val loss: 0.8531250953674316
Epoch 390, training loss: 7.001616477966309 = 0.5739498734474182 + 1.0 * 6.427666664123535
Epoch 390, val loss: 0.8382091522216797
Epoch 400, training loss: 6.974795341491699 = 0.5478131771087646 + 1.0 * 6.4269819259643555
Epoch 400, val loss: 0.8250616788864136
Epoch 410, training loss: 6.944018840789795 = 0.5229695439338684 + 1.0 * 6.421049118041992
Epoch 410, val loss: 0.8135215640068054
Epoch 420, training loss: 6.916931629180908 = 0.49913591146469116 + 1.0 * 6.417795658111572
Epoch 420, val loss: 0.8034430742263794
Epoch 430, training loss: 6.894654273986816 = 0.4763554632663727 + 1.0 * 6.418298721313477
Epoch 430, val loss: 0.7946513295173645
Epoch 440, training loss: 6.865841865539551 = 0.4545624554157257 + 1.0 * 6.411279201507568
Epoch 440, val loss: 0.7871655225753784
Epoch 450, training loss: 6.840006351470947 = 0.43358734250068665 + 1.0 * 6.406418800354004
Epoch 450, val loss: 0.7807012796401978
Epoch 460, training loss: 6.814657688140869 = 0.41328006982803345 + 1.0 * 6.4013776779174805
Epoch 460, val loss: 0.7751230597496033
Epoch 470, training loss: 6.810370922088623 = 0.39353758096694946 + 1.0 * 6.416833400726318
Epoch 470, val loss: 0.7702606916427612
Epoch 480, training loss: 6.773069858551025 = 0.37457606196403503 + 1.0 * 6.398493766784668
Epoch 480, val loss: 0.766140341758728
Epoch 490, training loss: 6.749411106109619 = 0.3561682403087616 + 1.0 * 6.393242835998535
Epoch 490, val loss: 0.7626389861106873
Epoch 500, training loss: 6.733381748199463 = 0.3382633626461029 + 1.0 * 6.395118236541748
Epoch 500, val loss: 0.7597165107727051
Epoch 510, training loss: 6.708769798278809 = 0.3209012448787689 + 1.0 * 6.387868404388428
Epoch 510, val loss: 0.7574067711830139
Epoch 520, training loss: 6.690630912780762 = 0.30403321981430054 + 1.0 * 6.386597633361816
Epoch 520, val loss: 0.7556854486465454
Epoch 530, training loss: 6.68743896484375 = 0.2877447307109833 + 1.0 * 6.399694442749023
Epoch 530, val loss: 0.7545570731163025
Epoch 540, training loss: 6.653355121612549 = 0.27227774262428284 + 1.0 * 6.381077289581299
Epoch 540, val loss: 0.7538914680480957
Epoch 550, training loss: 6.634556770324707 = 0.25748389959335327 + 1.0 * 6.377072811126709
Epoch 550, val loss: 0.7538173794746399
Epoch 560, training loss: 6.624406814575195 = 0.24335885047912598 + 1.0 * 6.381048202514648
Epoch 560, val loss: 0.7544084191322327
Epoch 570, training loss: 6.603535175323486 = 0.22994877398014069 + 1.0 * 6.373586177825928
Epoch 570, val loss: 0.7555503845214844
Epoch 580, training loss: 6.585746765136719 = 0.21726681292057037 + 1.0 * 6.3684797286987305
Epoch 580, val loss: 0.7573094367980957
Epoch 590, training loss: 6.578405857086182 = 0.20528185367584229 + 1.0 * 6.373124122619629
Epoch 590, val loss: 0.7596828937530518
Epoch 600, training loss: 6.562260150909424 = 0.19407472014427185 + 1.0 * 6.368185520172119
Epoch 600, val loss: 0.7624815106391907
Epoch 610, training loss: 6.545467376708984 = 0.18355165421962738 + 1.0 * 6.361915588378906
Epoch 610, val loss: 0.7658796310424805
Epoch 620, training loss: 6.540219783782959 = 0.17367950081825256 + 1.0 * 6.366540431976318
Epoch 620, val loss: 0.769838809967041
Epoch 630, training loss: 6.525939464569092 = 0.1644672453403473 + 1.0 * 6.361472129821777
Epoch 630, val loss: 0.7741477489471436
Epoch 640, training loss: 6.512084007263184 = 0.1558779627084732 + 1.0 * 6.356205940246582
Epoch 640, val loss: 0.7788357138633728
Epoch 650, training loss: 6.50801420211792 = 0.14782579243183136 + 1.0 * 6.3601884841918945
Epoch 650, val loss: 0.7840240001678467
Epoch 660, training loss: 6.493497848510742 = 0.1402967870235443 + 1.0 * 6.353200912475586
Epoch 660, val loss: 0.7894691824913025
Epoch 670, training loss: 6.485372066497803 = 0.133216992020607 + 1.0 * 6.3521552085876465
Epoch 670, val loss: 0.7953052520751953
Epoch 680, training loss: 6.475069999694824 = 0.12656858563423157 + 1.0 * 6.348501205444336
Epoch 680, val loss: 0.8014068603515625
Epoch 690, training loss: 6.4693427085876465 = 0.12033554911613464 + 1.0 * 6.3490071296691895
Epoch 690, val loss: 0.8077411651611328
Epoch 700, training loss: 6.467297077178955 = 0.11450131982564926 + 1.0 * 6.352795600891113
Epoch 700, val loss: 0.81413334608078
Epoch 710, training loss: 6.453948974609375 = 0.10903243720531464 + 1.0 * 6.344916343688965
Epoch 710, val loss: 0.8207154870033264
Epoch 720, training loss: 6.445455074310303 = 0.10388921946287155 + 1.0 * 6.34156608581543
Epoch 720, val loss: 0.8274363279342651
Epoch 730, training loss: 6.452312469482422 = 0.09904388338327408 + 1.0 * 6.353268623352051
Epoch 730, val loss: 0.8343099355697632
Epoch 740, training loss: 6.434896469116211 = 0.09448692947626114 + 1.0 * 6.340409755706787
Epoch 740, val loss: 0.8411401510238647
Epoch 750, training loss: 6.426565170288086 = 0.09019511938095093 + 1.0 * 6.33636999130249
Epoch 750, val loss: 0.8480634689331055
Epoch 760, training loss: 6.44069242477417 = 0.08613987267017365 + 1.0 * 6.354552745819092
Epoch 760, val loss: 0.8550847768783569
Epoch 770, training loss: 6.421700954437256 = 0.0823216587305069 + 1.0 * 6.33937931060791
Epoch 770, val loss: 0.862035870552063
Epoch 780, training loss: 6.4120049476623535 = 0.07873237878084183 + 1.0 * 6.333272457122803
Epoch 780, val loss: 0.8688819408416748
Epoch 790, training loss: 6.405840873718262 = 0.07532798498868942 + 1.0 * 6.330513000488281
Epoch 790, val loss: 0.87593013048172
Epoch 800, training loss: 6.409511566162109 = 0.07209470868110657 + 1.0 * 6.337416648864746
Epoch 800, val loss: 0.8830308318138123
Epoch 810, training loss: 6.4062581062316895 = 0.06904847174882889 + 1.0 * 6.337209701538086
Epoch 810, val loss: 0.889965832233429
Epoch 820, training loss: 6.393353462219238 = 0.06616558134555817 + 1.0 * 6.327188014984131
Epoch 820, val loss: 0.8968014121055603
Epoch 830, training loss: 6.402480602264404 = 0.06343583762645721 + 1.0 * 6.339044570922852
Epoch 830, val loss: 0.9037438631057739
Epoch 840, training loss: 6.390973091125488 = 0.060847289860248566 + 1.0 * 6.33012580871582
Epoch 840, val loss: 0.9105578064918518
Epoch 850, training loss: 6.385805130004883 = 0.058401551097631454 + 1.0 * 6.327403545379639
Epoch 850, val loss: 0.9172747731208801
Epoch 860, training loss: 6.379773139953613 = 0.05608060210943222 + 1.0 * 6.323692321777344
Epoch 860, val loss: 0.9240511059761047
Epoch 870, training loss: 6.376152992248535 = 0.05387754365801811 + 1.0 * 6.322275638580322
Epoch 870, val loss: 0.930650532245636
Epoch 880, training loss: 6.378439903259277 = 0.05178683623671532 + 1.0 * 6.326653003692627
Epoch 880, val loss: 0.9372761845588684
Epoch 890, training loss: 6.370872497558594 = 0.04980010539293289 + 1.0 * 6.321072578430176
Epoch 890, val loss: 0.943871021270752
Epoch 900, training loss: 6.3740553855896 = 0.047916751354932785 + 1.0 * 6.326138496398926
Epoch 900, val loss: 0.9502608776092529
Epoch 910, training loss: 6.364283561706543 = 0.046129535883665085 + 1.0 * 6.3181538581848145
Epoch 910, val loss: 0.9566529393196106
Epoch 920, training loss: 6.3611955642700195 = 0.044429901987314224 + 1.0 * 6.316765785217285
Epoch 920, val loss: 0.9629216194152832
Epoch 930, training loss: 6.359963893890381 = 0.04280873388051987 + 1.0 * 6.317155361175537
Epoch 930, val loss: 0.9692438244819641
Epoch 940, training loss: 6.358682632446289 = 0.04126552864909172 + 1.0 * 6.317417144775391
Epoch 940, val loss: 0.9755135774612427
Epoch 950, training loss: 6.3546271324157715 = 0.03980344906449318 + 1.0 * 6.314823627471924
Epoch 950, val loss: 0.9814997911453247
Epoch 960, training loss: 6.354103088378906 = 0.03841026872396469 + 1.0 * 6.315692901611328
Epoch 960, val loss: 0.9875255227088928
Epoch 970, training loss: 6.355302810668945 = 0.03708099573850632 + 1.0 * 6.3182220458984375
Epoch 970, val loss: 0.9935206770896912
Epoch 980, training loss: 6.349564552307129 = 0.035817358642816544 + 1.0 * 6.313747406005859
Epoch 980, val loss: 0.9993513226509094
Epoch 990, training loss: 6.354189395904541 = 0.034613482654094696 + 1.0 * 6.319575786590576
Epoch 990, val loss: 1.0051707029342651
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 10.540366172790527 = 1.9435261487960815 + 1.0 * 8.596839904785156
Epoch 0, val loss: 1.93453848361969
Epoch 10, training loss: 10.529203414916992 = 1.9326859712600708 + 1.0 * 8.596517562866211
Epoch 10, val loss: 1.924098253250122
Epoch 20, training loss: 10.512219429016113 = 1.918823003768921 + 1.0 * 8.593396186828613
Epoch 20, val loss: 1.9102444648742676
Epoch 30, training loss: 10.467774391174316 = 1.8992540836334229 + 1.0 * 8.568520545959473
Epoch 30, val loss: 1.8904647827148438
Epoch 40, training loss: 10.299818992614746 = 1.8738653659820557 + 1.0 * 8.42595386505127
Epoch 40, val loss: 1.8658138513565063
Epoch 50, training loss: 9.918306350708008 = 1.8448495864868164 + 1.0 * 8.073456764221191
Epoch 50, val loss: 1.8385844230651855
Epoch 60, training loss: 9.631208419799805 = 1.8186768293380737 + 1.0 * 7.812531471252441
Epoch 60, val loss: 1.8148940801620483
Epoch 70, training loss: 9.253859519958496 = 1.7972553968429565 + 1.0 * 7.45660400390625
Epoch 70, val loss: 1.7966500520706177
Epoch 80, training loss: 8.90701961517334 = 1.7831183671951294 + 1.0 * 7.123900890350342
Epoch 80, val loss: 1.7859079837799072
Epoch 90, training loss: 8.762125015258789 = 1.76628577709198 + 1.0 * 6.995839595794678
Epoch 90, val loss: 1.7713465690612793
Epoch 100, training loss: 8.637975692749023 = 1.7443065643310547 + 1.0 * 6.8936686515808105
Epoch 100, val loss: 1.75309157371521
Epoch 110, training loss: 8.538742065429688 = 1.7231305837631226 + 1.0 * 6.815611839294434
Epoch 110, val loss: 1.7357826232910156
Epoch 120, training loss: 8.448766708374023 = 1.7002322673797607 + 1.0 * 6.748534202575684
Epoch 120, val loss: 1.716495394706726
Epoch 130, training loss: 8.37120246887207 = 1.6732778549194336 + 1.0 * 6.6979241371154785
Epoch 130, val loss: 1.6941087245941162
Epoch 140, training loss: 8.296646118164062 = 1.642323613166809 + 1.0 * 6.654322147369385
Epoch 140, val loss: 1.6687062978744507
Epoch 150, training loss: 8.229799270629883 = 1.6070479154586792 + 1.0 * 6.622751235961914
Epoch 150, val loss: 1.6399188041687012
Epoch 160, training loss: 8.162516593933105 = 1.5675517320632935 + 1.0 * 6.594964504241943
Epoch 160, val loss: 1.6077535152435303
Epoch 170, training loss: 8.095643043518066 = 1.5244845151901245 + 1.0 * 6.5711588859558105
Epoch 170, val loss: 1.5728840827941895
Epoch 180, training loss: 8.030722618103027 = 1.4786676168441772 + 1.0 * 6.5520548820495605
Epoch 180, val loss: 1.5361179113388062
Epoch 190, training loss: 7.969539165496826 = 1.4312801361083984 + 1.0 * 6.538259029388428
Epoch 190, val loss: 1.4987826347351074
Epoch 200, training loss: 7.907866477966309 = 1.3841893672943115 + 1.0 * 6.523676872253418
Epoch 200, val loss: 1.4620736837387085
Epoch 210, training loss: 7.849199295043945 = 1.3371878862380981 + 1.0 * 6.512011528015137
Epoch 210, val loss: 1.4261277914047241
Epoch 220, training loss: 7.794768333435059 = 1.2905350923538208 + 1.0 * 6.504233360290527
Epoch 220, val loss: 1.3911597728729248
Epoch 230, training loss: 7.740196704864502 = 1.245678424835205 + 1.0 * 6.494518280029297
Epoch 230, val loss: 1.3582595586776733
Epoch 240, training loss: 7.6855549812316895 = 1.2024911642074585 + 1.0 * 6.483063697814941
Epoch 240, val loss: 1.3270256519317627
Epoch 250, training loss: 7.642813205718994 = 1.1602869033813477 + 1.0 * 6.4825263023376465
Epoch 250, val loss: 1.2969210147857666
Epoch 260, training loss: 7.590018272399902 = 1.1196722984313965 + 1.0 * 6.470345973968506
Epoch 260, val loss: 1.2681770324707031
Epoch 270, training loss: 7.539529323577881 = 1.0800055265426636 + 1.0 * 6.459523677825928
Epoch 270, val loss: 1.2402393817901611
Epoch 280, training loss: 7.4941582679748535 = 1.0408836603164673 + 1.0 * 6.453274726867676
Epoch 280, val loss: 1.212831974029541
Epoch 290, training loss: 7.454321384429932 = 1.00258207321167 + 1.0 * 6.451739311218262
Epoch 290, val loss: 1.1860719919204712
Epoch 300, training loss: 7.406704425811768 = 0.9653157591819763 + 1.0 * 6.4413886070251465
Epoch 300, val loss: 1.1599537134170532
Epoch 310, training loss: 7.363149642944336 = 0.9287478923797607 + 1.0 * 6.434401512145996
Epoch 310, val loss: 1.1343811750411987
Epoch 320, training loss: 7.324131965637207 = 0.8928595185279846 + 1.0 * 6.431272506713867
Epoch 320, val loss: 1.1094187498092651
Epoch 330, training loss: 7.283037185668945 = 0.8578637838363647 + 1.0 * 6.425173282623291
Epoch 330, val loss: 1.0852538347244263
Epoch 340, training loss: 7.242782115936279 = 0.8235568404197693 + 1.0 * 6.419225215911865
Epoch 340, val loss: 1.0617905855178833
Epoch 350, training loss: 7.207698822021484 = 0.7898614406585693 + 1.0 * 6.417837142944336
Epoch 350, val loss: 1.0391812324523926
Epoch 360, training loss: 7.167651653289795 = 0.7571016550064087 + 1.0 * 6.410550117492676
Epoch 360, val loss: 1.0175968408584595
Epoch 370, training loss: 7.1321234703063965 = 0.7250534296035767 + 1.0 * 6.407070159912109
Epoch 370, val loss: 0.9968675374984741
Epoch 380, training loss: 7.097524642944336 = 0.6935696005821228 + 1.0 * 6.403954982757568
Epoch 380, val loss: 0.9770371317863464
Epoch 390, training loss: 7.0648393630981445 = 0.662827730178833 + 1.0 * 6.402011871337891
Epoch 390, val loss: 0.958206295967102
Epoch 400, training loss: 7.028881072998047 = 0.6330183148384094 + 1.0 * 6.395862579345703
Epoch 400, val loss: 0.9405329823493958
Epoch 410, training loss: 6.999705791473389 = 0.6040075421333313 + 1.0 * 6.395698070526123
Epoch 410, val loss: 0.923887312412262
Epoch 420, training loss: 6.967034816741943 = 0.5758903622627258 + 1.0 * 6.391144275665283
Epoch 420, val loss: 0.9083882570266724
Epoch 430, training loss: 6.934735298156738 = 0.5485762357711792 + 1.0 * 6.3861589431762695
Epoch 430, val loss: 0.8940191268920898
Epoch 440, training loss: 6.910568714141846 = 0.5219300985336304 + 1.0 * 6.388638496398926
Epoch 440, val loss: 0.8807633519172668
Epoch 450, training loss: 6.880011081695557 = 0.49617305397987366 + 1.0 * 6.383838176727295
Epoch 450, val loss: 0.868933379650116
Epoch 460, training loss: 6.850803375244141 = 0.4713440537452698 + 1.0 * 6.379459381103516
Epoch 460, val loss: 0.8585951924324036
Epoch 470, training loss: 6.822501182556152 = 0.4473418593406677 + 1.0 * 6.37515926361084
Epoch 470, val loss: 0.8498561382293701
Epoch 480, training loss: 6.796153545379639 = 0.4241941273212433 + 1.0 * 6.371959209442139
Epoch 480, val loss: 0.8429024815559387
Epoch 490, training loss: 6.780869483947754 = 0.4020574986934662 + 1.0 * 6.378811836242676
Epoch 490, val loss: 0.8378303050994873
Epoch 500, training loss: 6.751359462738037 = 0.38121718168258667 + 1.0 * 6.370142459869385
Epoch 500, val loss: 0.8347563147544861
Epoch 510, training loss: 6.7275190353393555 = 0.36172378063201904 + 1.0 * 6.365795135498047
Epoch 510, val loss: 0.8335129618644714
Epoch 520, training loss: 6.714825630187988 = 0.3436163365840912 + 1.0 * 6.371209144592285
Epoch 520, val loss: 0.8338647484779358
Epoch 530, training loss: 6.690704822540283 = 0.32695716619491577 + 1.0 * 6.363747596740723
Epoch 530, val loss: 0.8356844782829285
Epoch 540, training loss: 6.671462059020996 = 0.31155911087989807 + 1.0 * 6.359902858734131
Epoch 540, val loss: 0.8386328816413879
Epoch 550, training loss: 6.655611038208008 = 0.2972065508365631 + 1.0 * 6.358404636383057
Epoch 550, val loss: 0.842444658279419
Epoch 560, training loss: 6.642627239227295 = 0.28377994894981384 + 1.0 * 6.358847141265869
Epoch 560, val loss: 0.8469218015670776
Epoch 570, training loss: 6.627203464508057 = 0.27112680673599243 + 1.0 * 6.356076717376709
Epoch 570, val loss: 0.8520224690437317
Epoch 580, training loss: 6.6102728843688965 = 0.2589898109436035 + 1.0 * 6.351283073425293
Epoch 580, val loss: 0.8574296236038208
Epoch 590, training loss: 6.60373067855835 = 0.24718505144119263 + 1.0 * 6.356545448303223
Epoch 590, val loss: 0.8630058765411377
Epoch 600, training loss: 6.589079856872559 = 0.23566097021102905 + 1.0 * 6.353418827056885
Epoch 600, val loss: 0.8686833381652832
Epoch 610, training loss: 6.572209358215332 = 0.2243116796016693 + 1.0 * 6.347897529602051
Epoch 610, val loss: 0.8744423985481262
Epoch 620, training loss: 6.557776927947998 = 0.21302630007266998 + 1.0 * 6.34475040435791
Epoch 620, val loss: 0.8800656199455261
Epoch 630, training loss: 6.5466389656066895 = 0.2018052637577057 + 1.0 * 6.344833850860596
Epoch 630, val loss: 0.8856470584869385
Epoch 640, training loss: 6.536073684692383 = 0.19074028730392456 + 1.0 * 6.345333576202393
Epoch 640, val loss: 0.891238808631897
Epoch 650, training loss: 6.519547462463379 = 0.17996640503406525 + 1.0 * 6.33958101272583
Epoch 650, val loss: 0.8968325257301331
Epoch 660, training loss: 6.508381366729736 = 0.16955223679542542 + 1.0 * 6.338829040527344
Epoch 660, val loss: 0.9025209546089172
Epoch 670, training loss: 6.505150318145752 = 0.15960216522216797 + 1.0 * 6.345548152923584
Epoch 670, val loss: 0.9082773327827454
Epoch 680, training loss: 6.490619659423828 = 0.15021784603595734 + 1.0 * 6.340401649475098
Epoch 680, val loss: 0.9141756892204285
Epoch 690, training loss: 6.479062080383301 = 0.14145971834659576 + 1.0 * 6.337602138519287
Epoch 690, val loss: 0.9203115105628967
Epoch 700, training loss: 6.466476917266846 = 0.13329865038394928 + 1.0 * 6.3331780433654785
Epoch 700, val loss: 0.926558256149292
Epoch 710, training loss: 6.456849098205566 = 0.1256941705942154 + 1.0 * 6.331154823303223
Epoch 710, val loss: 0.9330155253410339
Epoch 720, training loss: 6.452114582061768 = 0.11861724406480789 + 1.0 * 6.333497524261475
Epoch 720, val loss: 0.9396619200706482
Epoch 730, training loss: 6.4532790184021 = 0.11208675056695938 + 1.0 * 6.341192245483398
Epoch 730, val loss: 0.9464770555496216
Epoch 740, training loss: 6.4368109703063965 = 0.10606420785188675 + 1.0 * 6.330746650695801
Epoch 740, val loss: 0.9535202383995056
Epoch 750, training loss: 6.427191734313965 = 0.10046052932739258 + 1.0 * 6.326731204986572
Epoch 750, val loss: 0.9606627225875854
Epoch 760, training loss: 6.419964790344238 = 0.09522365033626556 + 1.0 * 6.324741363525391
Epoch 760, val loss: 0.9680155515670776
Epoch 770, training loss: 6.416173458099365 = 0.0903235375881195 + 1.0 * 6.325850009918213
Epoch 770, val loss: 0.9755951762199402
Epoch 780, training loss: 6.409042835235596 = 0.08575894683599472 + 1.0 * 6.323283672332764
Epoch 780, val loss: 0.9832767248153687
Epoch 790, training loss: 6.409268379211426 = 0.08152206987142563 + 1.0 * 6.327746391296387
Epoch 790, val loss: 0.9911086559295654
Epoch 800, training loss: 6.398393154144287 = 0.07756254822015762 + 1.0 * 6.320830821990967
Epoch 800, val loss: 0.9990370273590088
Epoch 810, training loss: 6.393590927124023 = 0.07385042309761047 + 1.0 * 6.319740295410156
Epoch 810, val loss: 1.0070390701293945
Epoch 820, training loss: 6.395727634429932 = 0.0703619122505188 + 1.0 * 6.3253655433654785
Epoch 820, val loss: 1.0151336193084717
Epoch 830, training loss: 6.389965057373047 = 0.06710252910852432 + 1.0 * 6.32286262512207
Epoch 830, val loss: 1.0232306718826294
Epoch 840, training loss: 6.382087707519531 = 0.06405190378427505 + 1.0 * 6.31803560256958
Epoch 840, val loss: 1.0314408540725708
Epoch 850, training loss: 6.378446578979492 = 0.061186283826828 + 1.0 * 6.317260265350342
Epoch 850, val loss: 1.0395995378494263
Epoch 860, training loss: 6.374571323394775 = 0.058490023016929626 + 1.0 * 6.316081523895264
Epoch 860, val loss: 1.0477814674377441
Epoch 870, training loss: 6.3693695068359375 = 0.05595216527581215 + 1.0 * 6.313417434692383
Epoch 870, val loss: 1.0560171604156494
Epoch 880, training loss: 6.374060153961182 = 0.05355774238705635 + 1.0 * 6.320502281188965
Epoch 880, val loss: 1.064239501953125
Epoch 890, training loss: 6.365365982055664 = 0.0513068363070488 + 1.0 * 6.314059257507324
Epoch 890, val loss: 1.0724146366119385
Epoch 900, training loss: 6.360538959503174 = 0.04918394982814789 + 1.0 * 6.311355113983154
Epoch 900, val loss: 1.0806171894073486
Epoch 910, training loss: 6.356902122497559 = 0.04717615991830826 + 1.0 * 6.309725761413574
Epoch 910, val loss: 1.0887593030929565
Epoch 920, training loss: 6.361286163330078 = 0.04528219625353813 + 1.0 * 6.316003799438477
Epoch 920, val loss: 1.0968286991119385
Epoch 930, training loss: 6.353362083435059 = 0.04350563883781433 + 1.0 * 6.309856414794922
Epoch 930, val loss: 1.1048696041107178
Epoch 940, training loss: 6.349100112915039 = 0.04182504862546921 + 1.0 * 6.307275295257568
Epoch 940, val loss: 1.11283540725708
Epoch 950, training loss: 6.346639633178711 = 0.04022974148392677 + 1.0 * 6.30640983581543
Epoch 950, val loss: 1.1207698583602905
Epoch 960, training loss: 6.353585720062256 = 0.03871709480881691 + 1.0 * 6.314868450164795
Epoch 960, val loss: 1.1286982297897339
Epoch 970, training loss: 6.3437933921813965 = 0.037288568913936615 + 1.0 * 6.306504726409912
Epoch 970, val loss: 1.1365149021148682
Epoch 980, training loss: 6.34043550491333 = 0.035932473838329315 + 1.0 * 6.304502964019775
Epoch 980, val loss: 1.1442784070968628
Epoch 990, training loss: 6.3423614501953125 = 0.034645192325115204 + 1.0 * 6.307716369628906
Epoch 990, val loss: 1.1519044637680054
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8292040063257776
=== training gcn model ===
Epoch 0, training loss: 10.525224685668945 = 1.928386926651001 + 1.0 * 8.596837997436523
Epoch 0, val loss: 1.9214050769805908
Epoch 10, training loss: 10.515392303466797 = 1.9188498258590698 + 1.0 * 8.596542358398438
Epoch 10, val loss: 1.9125635623931885
Epoch 20, training loss: 10.500971794128418 = 1.9072245359420776 + 1.0 * 8.59374713897705
Epoch 20, val loss: 1.9012788534164429
Epoch 30, training loss: 10.462153434753418 = 1.891359567642212 + 1.0 * 8.570794105529785
Epoch 30, val loss: 1.8855069875717163
Epoch 40, training loss: 10.314376831054688 = 1.870794415473938 + 1.0 * 8.443582534790039
Epoch 40, val loss: 1.8656736612319946
Epoch 50, training loss: 9.882150650024414 = 1.848456621170044 + 1.0 * 8.03369426727295
Epoch 50, val loss: 1.844846248626709
Epoch 60, training loss: 9.450926780700684 = 1.8279459476470947 + 1.0 * 7.622981071472168
Epoch 60, val loss: 1.8263047933578491
Epoch 70, training loss: 8.997400283813477 = 1.8118562698364258 + 1.0 * 7.185544490814209
Epoch 70, val loss: 1.8112112283706665
Epoch 80, training loss: 8.770827293395996 = 1.7963768243789673 + 1.0 * 6.974450588226318
Epoch 80, val loss: 1.7963892221450806
Epoch 90, training loss: 8.656291961669922 = 1.7794432640075684 + 1.0 * 6.876849174499512
Epoch 90, val loss: 1.781057357788086
Epoch 100, training loss: 8.567488670349121 = 1.7607948780059814 + 1.0 * 6.806694030761719
Epoch 100, val loss: 1.7648078203201294
Epoch 110, training loss: 8.500442504882812 = 1.740570068359375 + 1.0 * 6.7598724365234375
Epoch 110, val loss: 1.7471179962158203
Epoch 120, training loss: 8.430365562438965 = 1.7192989587783813 + 1.0 * 6.711066246032715
Epoch 120, val loss: 1.7280193567276
Epoch 130, training loss: 8.363978385925293 = 1.696317195892334 + 1.0 * 6.667661190032959
Epoch 130, val loss: 1.7072970867156982
Epoch 140, training loss: 8.303102493286133 = 1.669999122619629 + 1.0 * 6.633102893829346
Epoch 140, val loss: 1.6839210987091064
Epoch 150, training loss: 8.246566772460938 = 1.638975977897644 + 1.0 * 6.607590675354004
Epoch 150, val loss: 1.6565402746200562
Epoch 160, training loss: 8.189043045043945 = 1.6021616458892822 + 1.0 * 6.586881637573242
Epoch 160, val loss: 1.6240535974502563
Epoch 170, training loss: 8.128942489624023 = 1.5592029094696045 + 1.0 * 6.569739818572998
Epoch 170, val loss: 1.5862032175064087
Epoch 180, training loss: 8.063200950622559 = 1.5107014179229736 + 1.0 * 6.552499294281006
Epoch 180, val loss: 1.543946385383606
Epoch 190, training loss: 7.995278358459473 = 1.4568476676940918 + 1.0 * 6.538430690765381
Epoch 190, val loss: 1.4975085258483887
Epoch 200, training loss: 7.9256815910339355 = 1.3984179496765137 + 1.0 * 6.527263641357422
Epoch 200, val loss: 1.447733998298645
Epoch 210, training loss: 7.853366374969482 = 1.3380041122436523 + 1.0 * 6.51536226272583
Epoch 210, val loss: 1.396799921989441
Epoch 220, training loss: 7.782427787780762 = 1.2764198780059814 + 1.0 * 6.506007671356201
Epoch 220, val loss: 1.3453857898712158
Epoch 230, training loss: 7.716061115264893 = 1.2156764268875122 + 1.0 * 6.50038480758667
Epoch 230, val loss: 1.2951794862747192
Epoch 240, training loss: 7.646312713623047 = 1.1571500301361084 + 1.0 * 6.489162445068359
Epoch 240, val loss: 1.247362494468689
Epoch 250, training loss: 7.585582733154297 = 1.1003873348236084 + 1.0 * 6.485195636749268
Epoch 250, val loss: 1.20146906375885
Epoch 260, training loss: 7.519753456115723 = 1.0461262464523315 + 1.0 * 6.473627090454102
Epoch 260, val loss: 1.1578384637832642
Epoch 270, training loss: 7.46267032623291 = 0.9939218759536743 + 1.0 * 6.468748569488525
Epoch 270, val loss: 1.1159940958023071
Epoch 280, training loss: 7.404489994049072 = 0.9440142512321472 + 1.0 * 6.460475921630859
Epoch 280, val loss: 1.0760880708694458
Epoch 290, training loss: 7.352860927581787 = 0.8965429663658142 + 1.0 * 6.456317901611328
Epoch 290, val loss: 1.0382636785507202
Epoch 300, training loss: 7.298236846923828 = 0.8516046404838562 + 1.0 * 6.446632385253906
Epoch 300, val loss: 1.0028420686721802
Epoch 310, training loss: 7.2544331550598145 = 0.8090165853500366 + 1.0 * 6.445416450500488
Epoch 310, val loss: 0.9696657657623291
Epoch 320, training loss: 7.20613431930542 = 0.769450843334198 + 1.0 * 6.436683654785156
Epoch 320, val loss: 0.9393417239189148
Epoch 330, training loss: 7.162982940673828 = 0.7325773239135742 + 1.0 * 6.430405616760254
Epoch 330, val loss: 0.9118961095809937
Epoch 340, training loss: 7.1221137046813965 = 0.698032557964325 + 1.0 * 6.424081325531006
Epoch 340, val loss: 0.8870275020599365
Epoch 350, training loss: 7.08709716796875 = 0.6658817529678345 + 1.0 * 6.421215534210205
Epoch 350, val loss: 0.8648912310600281
Epoch 360, training loss: 7.052898406982422 = 0.6366233825683594 + 1.0 * 6.4162750244140625
Epoch 360, val loss: 0.8458437919616699
Epoch 370, training loss: 7.021857261657715 = 0.6094046831130981 + 1.0 * 6.412452697753906
Epoch 370, val loss: 0.8291594982147217
Epoch 380, training loss: 6.990792274475098 = 0.5837792158126831 + 1.0 * 6.407012939453125
Epoch 380, val loss: 0.8145620822906494
Epoch 390, training loss: 6.984736919403076 = 0.5596335530281067 + 1.0 * 6.425103187561035
Epoch 390, val loss: 0.8017765879631042
Epoch 400, training loss: 6.941695213317871 = 0.5370176434516907 + 1.0 * 6.404677391052246
Epoch 400, val loss: 0.7907580733299255
Epoch 410, training loss: 6.9119086265563965 = 0.5155423879623413 + 1.0 * 6.396366119384766
Epoch 410, val loss: 0.7811196446418762
Epoch 420, training loss: 6.887985706329346 = 0.49491751194000244 + 1.0 * 6.393068313598633
Epoch 420, val loss: 0.7725710868835449
Epoch 430, training loss: 6.868663787841797 = 0.4750480651855469 + 1.0 * 6.39361572265625
Epoch 430, val loss: 0.7649607062339783
Epoch 440, training loss: 6.844781398773193 = 0.455929160118103 + 1.0 * 6.388852119445801
Epoch 440, val loss: 0.7581832408905029
Epoch 450, training loss: 6.8223724365234375 = 0.43732547760009766 + 1.0 * 6.38504695892334
Epoch 450, val loss: 0.7521193623542786
Epoch 460, training loss: 6.801462650299072 = 0.41914069652557373 + 1.0 * 6.382321834564209
Epoch 460, val loss: 0.7467042207717896
Epoch 470, training loss: 6.781521320343018 = 0.40142062306404114 + 1.0 * 6.380100727081299
Epoch 470, val loss: 0.7418645024299622
Epoch 480, training loss: 6.769975662231445 = 0.3842347264289856 + 1.0 * 6.385740756988525
Epoch 480, val loss: 0.7377051115036011
Epoch 490, training loss: 6.7433624267578125 = 0.36763906478881836 + 1.0 * 6.375723361968994
Epoch 490, val loss: 0.7341189980506897
Epoch 500, training loss: 6.7240190505981445 = 0.35143911838531494 + 1.0 * 6.372580051422119
Epoch 500, val loss: 0.7310810685157776
Epoch 510, training loss: 6.705346584320068 = 0.33560529351234436 + 1.0 * 6.369741439819336
Epoch 510, val loss: 0.7285466194152832
Epoch 520, training loss: 6.701876640319824 = 0.3201160728931427 + 1.0 * 6.381760597229004
Epoch 520, val loss: 0.7265350818634033
Epoch 530, training loss: 6.674306392669678 = 0.30515000224113464 + 1.0 * 6.369156360626221
Epoch 530, val loss: 0.724989652633667
Epoch 540, training loss: 6.6553263664245605 = 0.29055655002593994 + 1.0 * 6.36476993560791
Epoch 540, val loss: 0.7239376306533813
Epoch 550, training loss: 6.638334274291992 = 0.2762718200683594 + 1.0 * 6.362062454223633
Epoch 550, val loss: 0.7233338952064514
Epoch 560, training loss: 6.627027988433838 = 0.26225635409355164 + 1.0 * 6.364771842956543
Epoch 560, val loss: 0.7232106924057007
Epoch 570, training loss: 6.618900299072266 = 0.24863894283771515 + 1.0 * 6.370261192321777
Epoch 570, val loss: 0.7236621975898743
Epoch 580, training loss: 6.595239639282227 = 0.2354622334241867 + 1.0 * 6.359777450561523
Epoch 580, val loss: 0.7246205806732178
Epoch 590, training loss: 6.578713893890381 = 0.22264374792575836 + 1.0 * 6.356070041656494
Epoch 590, val loss: 0.7261617183685303
Epoch 600, training loss: 6.571267604827881 = 0.21024556457996368 + 1.0 * 6.361021995544434
Epoch 600, val loss: 0.7282923460006714
Epoch 610, training loss: 6.554046630859375 = 0.19839072227478027 + 1.0 * 6.355656147003174
Epoch 610, val loss: 0.7309578061103821
Epoch 620, training loss: 6.541752815246582 = 0.18703526258468628 + 1.0 * 6.35471773147583
Epoch 620, val loss: 0.7342027425765991
Epoch 630, training loss: 6.526854991912842 = 0.17624886333942413 + 1.0 * 6.3506059646606445
Epoch 630, val loss: 0.7380038499832153
Epoch 640, training loss: 6.515478610992432 = 0.16600219905376434 + 1.0 * 6.349476337432861
Epoch 640, val loss: 0.7422997951507568
Epoch 650, training loss: 6.508213520050049 = 0.156324565410614 + 1.0 * 6.351889133453369
Epoch 650, val loss: 0.7471466064453125
Epoch 660, training loss: 6.497759819030762 = 0.14726567268371582 + 1.0 * 6.350494384765625
Epoch 660, val loss: 0.7523851990699768
Epoch 670, training loss: 6.483741283416748 = 0.1387522965669632 + 1.0 * 6.344988822937012
Epoch 670, val loss: 0.7581245303153992
Epoch 680, training loss: 6.475706100463867 = 0.13075107336044312 + 1.0 * 6.344954967498779
Epoch 680, val loss: 0.76422518491745
Epoch 690, training loss: 6.469646453857422 = 0.12325244396924973 + 1.0 * 6.346394062042236
Epoch 690, val loss: 0.7706564664840698
Epoch 700, training loss: 6.456902980804443 = 0.11623850464820862 + 1.0 * 6.340664386749268
Epoch 700, val loss: 0.7774726152420044
Epoch 710, training loss: 6.449841022491455 = 0.10968142747879028 + 1.0 * 6.3401594161987305
Epoch 710, val loss: 0.7845484018325806
Epoch 720, training loss: 6.445202827453613 = 0.10355333238840103 + 1.0 * 6.341649532318115
Epoch 720, val loss: 0.7918359041213989
Epoch 730, training loss: 6.436033725738525 = 0.09783757477998734 + 1.0 * 6.338196277618408
Epoch 730, val loss: 0.7992680668830872
Epoch 740, training loss: 6.429448127746582 = 0.09250820428133011 + 1.0 * 6.336939811706543
Epoch 740, val loss: 0.8067857623100281
Epoch 750, training loss: 6.422203063964844 = 0.08752276748418808 + 1.0 * 6.334680080413818
Epoch 750, val loss: 0.8145501613616943
Epoch 760, training loss: 6.422029495239258 = 0.08286095410585403 + 1.0 * 6.339168548583984
Epoch 760, val loss: 0.822319746017456
Epoch 770, training loss: 6.413257598876953 = 0.0785231962800026 + 1.0 * 6.3347344398498535
Epoch 770, val loss: 0.8303030133247375
Epoch 780, training loss: 6.413479328155518 = 0.07446248829364777 + 1.0 * 6.339016914367676
Epoch 780, val loss: 0.8382027745246887
Epoch 790, training loss: 6.402735710144043 = 0.0706842765212059 + 1.0 * 6.3320512771606445
Epoch 790, val loss: 0.8462213277816772
Epoch 800, training loss: 6.397808074951172 = 0.06714717298746109 + 1.0 * 6.330660820007324
Epoch 800, val loss: 0.8541839122772217
Epoch 810, training loss: 6.397703647613525 = 0.06384800374507904 + 1.0 * 6.333855628967285
Epoch 810, val loss: 0.8621858358383179
Epoch 820, training loss: 6.388358116149902 = 0.06076325476169586 + 1.0 * 6.327594757080078
Epoch 820, val loss: 0.8701048493385315
Epoch 830, training loss: 6.38421106338501 = 0.057879164814949036 + 1.0 * 6.326332092285156
Epoch 830, val loss: 0.8780074119567871
Epoch 840, training loss: 6.386249542236328 = 0.05517079681158066 + 1.0 * 6.33107852935791
Epoch 840, val loss: 0.8859280347824097
Epoch 850, training loss: 6.377861022949219 = 0.05264272168278694 + 1.0 * 6.325218200683594
Epoch 850, val loss: 0.8936799168586731
Epoch 860, training loss: 6.3736982345581055 = 0.05027099326252937 + 1.0 * 6.323427200317383
Epoch 860, val loss: 0.9014735221862793
Epoch 870, training loss: 6.373347759246826 = 0.04804294928908348 + 1.0 * 6.325304985046387
Epoch 870, val loss: 0.9091165661811829
Epoch 880, training loss: 6.367642402648926 = 0.0459551177918911 + 1.0 * 6.3216872215271
Epoch 880, val loss: 0.9166732430458069
Epoch 890, training loss: 6.366142749786377 = 0.04399782791733742 + 1.0 * 6.322144985198975
Epoch 890, val loss: 0.9241567850112915
Epoch 900, training loss: 6.361888408660889 = 0.042152997106313705 + 1.0 * 6.319735527038574
Epoch 900, val loss: 0.9315952062606812
Epoch 910, training loss: 6.372213840484619 = 0.04041033983230591 + 1.0 * 6.331803321838379
Epoch 910, val loss: 0.9388440847396851
Epoch 920, training loss: 6.362353324890137 = 0.038784243166446686 + 1.0 * 6.323569297790527
Epoch 920, val loss: 0.9461826086044312
Epoch 930, training loss: 6.354852199554443 = 0.037241075187921524 + 1.0 * 6.317611217498779
Epoch 930, val loss: 0.9533703923225403
Epoch 940, training loss: 6.352122783660889 = 0.03578823804855347 + 1.0 * 6.3163347244262695
Epoch 940, val loss: 0.960404098033905
Epoch 950, training loss: 6.363346576690674 = 0.034412600100040436 + 1.0 * 6.328934192657471
Epoch 950, val loss: 0.9673328399658203
Epoch 960, training loss: 6.349546909332275 = 0.03311482444405556 + 1.0 * 6.316431999206543
Epoch 960, val loss: 0.9744029641151428
Epoch 970, training loss: 6.345844745635986 = 0.03188953548669815 + 1.0 * 6.313955307006836
Epoch 970, val loss: 0.9811903238296509
Epoch 980, training loss: 6.344428062438965 = 0.03072621300816536 + 1.0 * 6.313701629638672
Epoch 980, val loss: 0.9879480600357056
Epoch 990, training loss: 6.347591400146484 = 0.029624100774526596 + 1.0 * 6.317967414855957
Epoch 990, val loss: 0.9945480823516846
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8402741170268846
The final CL Acc:0.79012, 0.02444, The final GNN Acc:0.83465, 0.00452
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11540])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10496])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.546934127807617 = 1.9500784873962402 + 1.0 * 8.596855163574219
Epoch 0, val loss: 1.944452166557312
Epoch 10, training loss: 10.536994934082031 = 1.9403328895568848 + 1.0 * 8.596662521362305
Epoch 10, val loss: 1.934759497642517
Epoch 20, training loss: 10.523460388183594 = 1.9282087087631226 + 1.0 * 8.59525203704834
Epoch 20, val loss: 1.9221925735473633
Epoch 30, training loss: 10.49441146850586 = 1.911110281944275 + 1.0 * 8.583301544189453
Epoch 30, val loss: 1.9039909839630127
Epoch 40, training loss: 10.390563011169434 = 1.887427568435669 + 1.0 * 8.503135681152344
Epoch 40, val loss: 1.8792535066604614
Epoch 50, training loss: 9.969139099121094 = 1.8610796928405762 + 1.0 * 8.10805892944336
Epoch 50, val loss: 1.8533958196640015
Epoch 60, training loss: 9.484088897705078 = 1.839404582977295 + 1.0 * 7.644683837890625
Epoch 60, val loss: 1.8344120979309082
Epoch 70, training loss: 9.024551391601562 = 1.8240416049957275 + 1.0 * 7.200509548187256
Epoch 70, val loss: 1.8211920261383057
Epoch 80, training loss: 8.787942886352539 = 1.809666633605957 + 1.0 * 6.97827672958374
Epoch 80, val loss: 1.8082853555679321
Epoch 90, training loss: 8.662341117858887 = 1.7930562496185303 + 1.0 * 6.869284629821777
Epoch 90, val loss: 1.7933350801467896
Epoch 100, training loss: 8.559349060058594 = 1.776604413986206 + 1.0 * 6.782744884490967
Epoch 100, val loss: 1.7794839143753052
Epoch 110, training loss: 8.492384910583496 = 1.7617247104644775 + 1.0 * 6.730660438537598
Epoch 110, val loss: 1.767168402671814
Epoch 120, training loss: 8.430458068847656 = 1.7469407320022583 + 1.0 * 6.683516979217529
Epoch 120, val loss: 1.7548134326934814
Epoch 130, training loss: 8.378623962402344 = 1.7307413816452026 + 1.0 * 6.64788293838501
Epoch 130, val loss: 1.7411532402038574
Epoch 140, training loss: 8.3291015625 = 1.7123829126358032 + 1.0 * 6.616718769073486
Epoch 140, val loss: 1.725712776184082
Epoch 150, training loss: 8.282564163208008 = 1.6910990476608276 + 1.0 * 6.591464996337891
Epoch 150, val loss: 1.7081494331359863
Epoch 160, training loss: 8.237605094909668 = 1.6663172245025635 + 1.0 * 6.571287631988525
Epoch 160, val loss: 1.6879173517227173
Epoch 170, training loss: 8.189706802368164 = 1.6376863718032837 + 1.0 * 6.552020072937012
Epoch 170, val loss: 1.664552092552185
Epoch 180, training loss: 8.141715049743652 = 1.604737639427185 + 1.0 * 6.536977767944336
Epoch 180, val loss: 1.6377291679382324
Epoch 190, training loss: 8.091511726379395 = 1.5676677227020264 + 1.0 * 6.523844242095947
Epoch 190, val loss: 1.6076596975326538
Epoch 200, training loss: 8.036598205566406 = 1.5261904001235962 + 1.0 * 6.5104079246521
Epoch 200, val loss: 1.5743439197540283
Epoch 210, training loss: 7.9799346923828125 = 1.4804288148880005 + 1.0 * 6.499505996704102
Epoch 210, val loss: 1.5378106832504272
Epoch 220, training loss: 7.926983833312988 = 1.431291103363037 + 1.0 * 6.495692729949951
Epoch 220, val loss: 1.4990726709365845
Epoch 230, training loss: 7.862623691558838 = 1.380473256111145 + 1.0 * 6.482150554656982
Epoch 230, val loss: 1.4594018459320068
Epoch 240, training loss: 7.801331996917725 = 1.3276692628860474 + 1.0 * 6.473662853240967
Epoch 240, val loss: 1.418722152709961
Epoch 250, training loss: 7.739738464355469 = 1.273422360420227 + 1.0 * 6.466316223144531
Epoch 250, val loss: 1.3772943019866943
Epoch 260, training loss: 7.683750152587891 = 1.2185924053192139 + 1.0 * 6.465157985687256
Epoch 260, val loss: 1.3359798192977905
Epoch 270, training loss: 7.622012615203857 = 1.164955973625183 + 1.0 * 6.457056522369385
Epoch 270, val loss: 1.2960014343261719
Epoch 280, training loss: 7.562930583953857 = 1.1132129430770874 + 1.0 * 6.4497175216674805
Epoch 280, val loss: 1.2577369213104248
Epoch 290, training loss: 7.507260322570801 = 1.0631352663040161 + 1.0 * 6.444125175476074
Epoch 290, val loss: 1.2209875583648682
Epoch 300, training loss: 7.454227447509766 = 1.0150387287139893 + 1.0 * 6.4391889572143555
Epoch 300, val loss: 1.1858420372009277
Epoch 310, training loss: 7.407843589782715 = 0.969527006149292 + 1.0 * 6.438316345214844
Epoch 310, val loss: 1.1528962850570679
Epoch 320, training loss: 7.356520175933838 = 0.9265618920326233 + 1.0 * 6.429958343505859
Epoch 320, val loss: 1.1219360828399658
Epoch 330, training loss: 7.309517860412598 = 0.8854591250419617 + 1.0 * 6.42405891418457
Epoch 330, val loss: 1.0926233530044556
Epoch 340, training loss: 7.269124984741211 = 0.8460419178009033 + 1.0 * 6.4230828285217285
Epoch 340, val loss: 1.0646597146987915
Epoch 350, training loss: 7.230832576751709 = 0.8092764019966125 + 1.0 * 6.421555995941162
Epoch 350, val loss: 1.038898229598999
Epoch 360, training loss: 7.188875198364258 = 0.7749106884002686 + 1.0 * 6.41396427154541
Epoch 360, val loss: 1.0155144929885864
Epoch 370, training loss: 7.151304244995117 = 0.7424308657646179 + 1.0 * 6.408873558044434
Epoch 370, val loss: 0.9935795664787292
Epoch 380, training loss: 7.1155805587768555 = 0.7115115523338318 + 1.0 * 6.404068946838379
Epoch 380, val loss: 0.9732356071472168
Epoch 390, training loss: 7.094145774841309 = 0.6820883750915527 + 1.0 * 6.412057399749756
Epoch 390, val loss: 0.9543788433074951
Epoch 400, training loss: 7.061179161071777 = 0.6543150544166565 + 1.0 * 6.406864166259766
Epoch 400, val loss: 0.9370323419570923
Epoch 410, training loss: 7.0235514640808105 = 0.6279847025871277 + 1.0 * 6.395566940307617
Epoch 410, val loss: 0.9211792349815369
Epoch 420, training loss: 6.9938859939575195 = 0.6027116775512695 + 1.0 * 6.39117431640625
Epoch 420, val loss: 0.9064987301826477
Epoch 430, training loss: 6.973720550537109 = 0.5781769156455994 + 1.0 * 6.395543575286865
Epoch 430, val loss: 0.8926902413368225
Epoch 440, training loss: 6.943264484405518 = 0.5544491410255432 + 1.0 * 6.388815402984619
Epoch 440, val loss: 0.8798374533653259
Epoch 450, training loss: 6.915095806121826 = 0.5313968062400818 + 1.0 * 6.3836989402771
Epoch 450, val loss: 0.8678417801856995
Epoch 460, training loss: 6.88856840133667 = 0.5086928606033325 + 1.0 * 6.379875659942627
Epoch 460, val loss: 0.8563436269760132
Epoch 470, training loss: 6.8744025230407715 = 0.4862126410007477 + 1.0 * 6.388189792633057
Epoch 470, val loss: 0.8453071117401123
Epoch 480, training loss: 6.839854717254639 = 0.46412378549575806 + 1.0 * 6.375730991363525
Epoch 480, val loss: 0.8348700404167175
Epoch 490, training loss: 6.814943313598633 = 0.4422574043273926 + 1.0 * 6.37268590927124
Epoch 490, val loss: 0.8249848484992981
Epoch 500, training loss: 6.805671215057373 = 0.42057904601097107 + 1.0 * 6.385092258453369
Epoch 500, val loss: 0.8155404925346375
Epoch 510, training loss: 6.7705583572387695 = 0.39925044775009155 + 1.0 * 6.371307849884033
Epoch 510, val loss: 0.8066736459732056
Epoch 520, training loss: 6.745375633239746 = 0.3782157897949219 + 1.0 * 6.367159843444824
Epoch 520, val loss: 0.7983213663101196
Epoch 530, training loss: 6.722316741943359 = 0.3574144244194031 + 1.0 * 6.364902496337891
Epoch 530, val loss: 0.7904665470123291
Epoch 540, training loss: 6.704228401184082 = 0.33696362376213074 + 1.0 * 6.367264747619629
Epoch 540, val loss: 0.783138632774353
Epoch 550, training loss: 6.6808366775512695 = 0.3171452283859253 + 1.0 * 6.363691329956055
Epoch 550, val loss: 0.7764042615890503
Epoch 560, training loss: 6.657879829406738 = 0.29785215854644775 + 1.0 * 6.36002779006958
Epoch 560, val loss: 0.7703034281730652
Epoch 570, training loss: 6.6391777992248535 = 0.2791840732097626 + 1.0 * 6.359993934631348
Epoch 570, val loss: 0.7648223638534546
Epoch 580, training loss: 6.618263244628906 = 0.2613096237182617 + 1.0 * 6.3569536209106445
Epoch 580, val loss: 0.7600701451301575
Epoch 590, training loss: 6.604647636413574 = 0.24437980353832245 + 1.0 * 6.360267639160156
Epoch 590, val loss: 0.7560698390007019
Epoch 600, training loss: 6.581076622009277 = 0.22840051352977753 + 1.0 * 6.352675914764404
Epoch 600, val loss: 0.7528546452522278
Epoch 610, training loss: 6.5667901039123535 = 0.21339429914951324 + 1.0 * 6.353395938873291
Epoch 610, val loss: 0.7504405975341797
Epoch 620, training loss: 6.557218074798584 = 0.19941338896751404 + 1.0 * 6.357804775238037
Epoch 620, val loss: 0.7487915754318237
Epoch 630, training loss: 6.536380767822266 = 0.18647170066833496 + 1.0 * 6.34990930557251
Epoch 630, val loss: 0.748020350933075
Epoch 640, training loss: 6.5217719078063965 = 0.17443613708019257 + 1.0 * 6.3473358154296875
Epoch 640, val loss: 0.7479125261306763
Epoch 650, training loss: 6.51772928237915 = 0.16324284672737122 + 1.0 * 6.354486465454102
Epoch 650, val loss: 0.7485204935073853
Epoch 660, training loss: 6.500291347503662 = 0.1529316008090973 + 1.0 * 6.347359657287598
Epoch 660, val loss: 0.7498013973236084
Epoch 670, training loss: 6.485054969787598 = 0.14334775507450104 + 1.0 * 6.341707229614258
Epoch 670, val loss: 0.7516579031944275
Epoch 680, training loss: 6.480532169342041 = 0.13446585834026337 + 1.0 * 6.346066474914551
Epoch 680, val loss: 0.7540717124938965
Epoch 690, training loss: 6.471377372741699 = 0.12627476453781128 + 1.0 * 6.345102787017822
Epoch 690, val loss: 0.7569331526756287
Epoch 700, training loss: 6.459317684173584 = 0.11871808767318726 + 1.0 * 6.340599536895752
Epoch 700, val loss: 0.7602019309997559
Epoch 710, training loss: 6.450222015380859 = 0.11173049360513687 + 1.0 * 6.338491439819336
Epoch 710, val loss: 0.7639045715332031
Epoch 720, training loss: 6.441432476043701 = 0.10524573177099228 + 1.0 * 6.33618688583374
Epoch 720, val loss: 0.7679013013839722
Epoch 730, training loss: 6.436288833618164 = 0.09921935945749283 + 1.0 * 6.337069511413574
Epoch 730, val loss: 0.7721731066703796
Epoch 740, training loss: 6.435614585876465 = 0.09363467246294022 + 1.0 * 6.34197998046875
Epoch 740, val loss: 0.7766715884208679
Epoch 750, training loss: 6.422920227050781 = 0.08847854286432266 + 1.0 * 6.334441661834717
Epoch 750, val loss: 0.7813352346420288
Epoch 760, training loss: 6.418928146362305 = 0.08367889374494553 + 1.0 * 6.335249423980713
Epoch 760, val loss: 0.7862204313278198
Epoch 770, training loss: 6.409994602203369 = 0.07922926545143127 + 1.0 * 6.330765247344971
Epoch 770, val loss: 0.7912596464157104
Epoch 780, training loss: 6.404648780822754 = 0.0750814750790596 + 1.0 * 6.3295674324035645
Epoch 780, val loss: 0.7963936924934387
Epoch 790, training loss: 6.398247241973877 = 0.07120136171579361 + 1.0 * 6.327045917510986
Epoch 790, val loss: 0.8016988635063171
Epoch 800, training loss: 6.393754482269287 = 0.06756094843149185 + 1.0 * 6.326193332672119
Epoch 800, val loss: 0.807027280330658
Epoch 810, training loss: 6.40327787399292 = 0.06415964663028717 + 1.0 * 6.339118003845215
Epoch 810, val loss: 0.8123924732208252
Epoch 820, training loss: 6.3880534172058105 = 0.06100977212190628 + 1.0 * 6.327043533325195
Epoch 820, val loss: 0.8178372383117676
Epoch 830, training loss: 6.381107330322266 = 0.058062050491571426 + 1.0 * 6.323045253753662
Epoch 830, val loss: 0.8233875632286072
Epoch 840, training loss: 6.392755031585693 = 0.05529740825295448 + 1.0 * 6.337457656860352
Epoch 840, val loss: 0.8288794159889221
Epoch 850, training loss: 6.376732349395752 = 0.05271648243069649 + 1.0 * 6.324016094207764
Epoch 850, val loss: 0.8343468904495239
Epoch 860, training loss: 6.372453212738037 = 0.050303924828767776 + 1.0 * 6.322149276733398
Epoch 860, val loss: 0.8398697376251221
Epoch 870, training loss: 6.371938705444336 = 0.04804062470793724 + 1.0 * 6.3238983154296875
Epoch 870, val loss: 0.8453418612480164
Epoch 880, training loss: 6.367195129394531 = 0.04592093080282211 + 1.0 * 6.321274280548096
Epoch 880, val loss: 0.8509010672569275
Epoch 890, training loss: 6.362035751342773 = 0.04392603784799576 + 1.0 * 6.318109512329102
Epoch 890, val loss: 0.8563876748085022
Epoch 900, training loss: 6.364488124847412 = 0.04205087944865227 + 1.0 * 6.322437286376953
Epoch 900, val loss: 0.861842155456543
Epoch 910, training loss: 6.358642578125 = 0.040295716375112534 + 1.0 * 6.318346977233887
Epoch 910, val loss: 0.8673578500747681
Epoch 920, training loss: 6.354715347290039 = 0.038640476763248444 + 1.0 * 6.316074848175049
Epoch 920, val loss: 0.872740626335144
Epoch 930, training loss: 6.352056503295898 = 0.0370854027569294 + 1.0 * 6.314970970153809
Epoch 930, val loss: 0.8781367540359497
Epoch 940, training loss: 6.3501505851745605 = 0.035623110830783844 + 1.0 * 6.31452751159668
Epoch 940, val loss: 0.8835411071777344
Epoch 950, training loss: 6.35638427734375 = 0.034242674708366394 + 1.0 * 6.322141647338867
Epoch 950, val loss: 0.8888475298881531
Epoch 960, training loss: 6.350947380065918 = 0.03294796869158745 + 1.0 * 6.317999362945557
Epoch 960, val loss: 0.8940559029579163
Epoch 970, training loss: 6.341882705688477 = 0.03172659873962402 + 1.0 * 6.310155868530273
Epoch 970, val loss: 0.899267315864563
Epoch 980, training loss: 6.340419292449951 = 0.03057003952562809 + 1.0 * 6.309849262237549
Epoch 980, val loss: 0.9044947624206543
Epoch 990, training loss: 6.343111991882324 = 0.029472367838025093 + 1.0 * 6.3136396408081055
Epoch 990, val loss: 0.9095878601074219
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.7970479704797049
=== training gcn model ===
Epoch 0, training loss: 10.526162147521973 = 1.9293237924575806 + 1.0 * 8.596837997436523
Epoch 0, val loss: 1.9360008239746094
Epoch 10, training loss: 10.51645278930664 = 1.9198830127716064 + 1.0 * 8.596570014953613
Epoch 10, val loss: 1.925776481628418
Epoch 20, training loss: 10.502676963806152 = 1.908023715019226 + 1.0 * 8.594653129577637
Epoch 20, val loss: 1.9127764701843262
Epoch 30, training loss: 10.472050666809082 = 1.891554832458496 + 1.0 * 8.580495834350586
Epoch 30, val loss: 1.8947556018829346
Epoch 40, training loss: 10.361607551574707 = 1.86955726146698 + 1.0 * 8.492050170898438
Epoch 40, val loss: 1.871545433998108
Epoch 50, training loss: 9.872722625732422 = 1.845689296722412 + 1.0 * 8.027033805847168
Epoch 50, val loss: 1.8477102518081665
Epoch 60, training loss: 9.366100311279297 = 1.826778769493103 + 1.0 * 7.5393218994140625
Epoch 60, val loss: 1.8303344249725342
Epoch 70, training loss: 8.984341621398926 = 1.8148925304412842 + 1.0 * 7.169449329376221
Epoch 70, val loss: 1.819374680519104
Epoch 80, training loss: 8.818885803222656 = 1.8035945892333984 + 1.0 * 7.0152907371521
Epoch 80, val loss: 1.8083000183105469
Epoch 90, training loss: 8.724912643432617 = 1.7914873361587524 + 1.0 * 6.933425426483154
Epoch 90, val loss: 1.7960731983184814
Epoch 100, training loss: 8.626235961914062 = 1.7785388231277466 + 1.0 * 6.8476972579956055
Epoch 100, val loss: 1.783565878868103
Epoch 110, training loss: 8.545775413513184 = 1.7664453983306885 + 1.0 * 6.779329776763916
Epoch 110, val loss: 1.7723459005355835
Epoch 120, training loss: 8.483367919921875 = 1.7537052631378174 + 1.0 * 6.7296624183654785
Epoch 120, val loss: 1.7610316276550293
Epoch 130, training loss: 8.426148414611816 = 1.738627314567566 + 1.0 * 6.687521457672119
Epoch 130, val loss: 1.7483052015304565
Epoch 140, training loss: 8.369430541992188 = 1.720955491065979 + 1.0 * 6.64847469329834
Epoch 140, val loss: 1.7338764667510986
Epoch 150, training loss: 8.316643714904785 = 1.7003225088119507 + 1.0 * 6.616321086883545
Epoch 150, val loss: 1.7172907590866089
Epoch 160, training loss: 8.267572402954102 = 1.6755571365356445 + 1.0 * 6.592014789581299
Epoch 160, val loss: 1.6972767114639282
Epoch 170, training loss: 8.217392921447754 = 1.6458817720413208 + 1.0 * 6.571511268615723
Epoch 170, val loss: 1.6733169555664062
Epoch 180, training loss: 8.164957046508789 = 1.6104875802993774 + 1.0 * 6.554469585418701
Epoch 180, val loss: 1.6447187662124634
Epoch 190, training loss: 8.10740852355957 = 1.5682859420776367 + 1.0 * 6.539123058319092
Epoch 190, val loss: 1.6104822158813477
Epoch 200, training loss: 8.050494194030762 = 1.5183919668197632 + 1.0 * 6.532102584838867
Epoch 200, val loss: 1.5702598094940186
Epoch 210, training loss: 7.977111339569092 = 1.462949275970459 + 1.0 * 6.514162063598633
Epoch 210, val loss: 1.525769829750061
Epoch 220, training loss: 7.905264854431152 = 1.4021447896957397 + 1.0 * 6.503119945526123
Epoch 220, val loss: 1.4772101640701294
Epoch 230, training loss: 7.829383373260498 = 1.3365683555603027 + 1.0 * 6.492815017700195
Epoch 230, val loss: 1.4251089096069336
Epoch 240, training loss: 7.757411003112793 = 1.2677218914031982 + 1.0 * 6.489688873291016
Epoch 240, val loss: 1.3709497451782227
Epoch 250, training loss: 7.677796840667725 = 1.1993780136108398 + 1.0 * 6.478418827056885
Epoch 250, val loss: 1.3175987005233765
Epoch 260, training loss: 7.601383209228516 = 1.1323996782302856 + 1.0 * 6.4689836502075195
Epoch 260, val loss: 1.2658741474151611
Epoch 270, training loss: 7.533411026000977 = 1.0675508975982666 + 1.0 * 6.465859889984131
Epoch 270, val loss: 1.2165427207946777
Epoch 280, training loss: 7.463651180267334 = 1.0069702863693237 + 1.0 * 6.456680774688721
Epoch 280, val loss: 1.1710339784622192
Epoch 290, training loss: 7.403287887573242 = 0.9506967663764954 + 1.0 * 6.4525909423828125
Epoch 290, val loss: 1.1297166347503662
Epoch 300, training loss: 7.346502304077148 = 0.8991283774375916 + 1.0 * 6.447373867034912
Epoch 300, val loss: 1.0927324295043945
Epoch 310, training loss: 7.290700912475586 = 0.8517937064170837 + 1.0 * 6.438907146453857
Epoch 310, val loss: 1.0596072673797607
Epoch 320, training loss: 7.2523322105407715 = 0.8082820177078247 + 1.0 * 6.444050312042236
Epoch 320, val loss: 1.030102252960205
Epoch 330, training loss: 7.200243949890137 = 0.7689813375473022 + 1.0 * 6.431262493133545
Epoch 330, val loss: 1.0044052600860596
Epoch 340, training loss: 7.1571526527404785 = 0.7326758503913879 + 1.0 * 6.424476623535156
Epoch 340, val loss: 0.9815782308578491
Epoch 350, training loss: 7.122548580169678 = 0.6986187696456909 + 1.0 * 6.423929691314697
Epoch 350, val loss: 0.9610146880149841
Epoch 360, training loss: 7.091775894165039 = 0.666778028011322 + 1.0 * 6.424997806549072
Epoch 360, val loss: 0.9427503347396851
Epoch 370, training loss: 7.050225257873535 = 0.6370081901550293 + 1.0 * 6.413217067718506
Epoch 370, val loss: 0.9266369938850403
Epoch 380, training loss: 7.017110824584961 = 0.6086806058883667 + 1.0 * 6.408430099487305
Epoch 380, val loss: 0.9122132658958435
Epoch 390, training loss: 6.986271858215332 = 0.5813981294631958 + 1.0 * 6.404873847961426
Epoch 390, val loss: 0.8992067575454712
Epoch 400, training loss: 6.957356929779053 = 0.55519038438797 + 1.0 * 6.402166366577148
Epoch 400, val loss: 0.8875895142555237
Epoch 410, training loss: 6.929934978485107 = 0.5302616357803345 + 1.0 * 6.3996734619140625
Epoch 410, val loss: 0.8775487542152405
Epoch 420, training loss: 6.9026265144348145 = 0.5061505436897278 + 1.0 * 6.396475791931152
Epoch 420, val loss: 0.8686610460281372
Epoch 430, training loss: 6.878357887268066 = 0.48271334171295166 + 1.0 * 6.395644664764404
Epoch 430, val loss: 0.8608589768409729
Epoch 440, training loss: 6.856624126434326 = 0.4602164626121521 + 1.0 * 6.396407604217529
Epoch 440, val loss: 0.8542412519454956
Epoch 450, training loss: 6.826951026916504 = 0.4386298358440399 + 1.0 * 6.388321399688721
Epoch 450, val loss: 0.8488832116127014
Epoch 460, training loss: 6.803378582000732 = 0.41772034764289856 + 1.0 * 6.385658264160156
Epoch 460, val loss: 0.8445765972137451
Epoch 470, training loss: 6.787147521972656 = 0.3973965346813202 + 1.0 * 6.389750957489014
Epoch 470, val loss: 0.8412834405899048
Epoch 480, training loss: 6.760871410369873 = 0.3777851164340973 + 1.0 * 6.383086204528809
Epoch 480, val loss: 0.838989794254303
Epoch 490, training loss: 6.736153602600098 = 0.358871728181839 + 1.0 * 6.377281665802002
Epoch 490, val loss: 0.8377341628074646
Epoch 500, training loss: 6.720664024353027 = 0.3405603766441345 + 1.0 * 6.380103588104248
Epoch 500, val loss: 0.8373497128486633
Epoch 510, training loss: 6.705357551574707 = 0.3230891525745392 + 1.0 * 6.38226842880249
Epoch 510, val loss: 0.8378587365150452
Epoch 520, training loss: 6.683101177215576 = 0.30655762553215027 + 1.0 * 6.3765435218811035
Epoch 520, val loss: 0.8393662571907043
Epoch 530, training loss: 6.661423206329346 = 0.29075297713279724 + 1.0 * 6.370670318603516
Epoch 530, val loss: 0.8416312336921692
Epoch 540, training loss: 6.644965648651123 = 0.2756492495536804 + 1.0 * 6.369316577911377
Epoch 540, val loss: 0.8446107506752014
Epoch 550, training loss: 6.630457878112793 = 0.26128414273262024 + 1.0 * 6.369173526763916
Epoch 550, val loss: 0.848358690738678
Epoch 560, training loss: 6.626794338226318 = 0.2476964294910431 + 1.0 * 6.379097938537598
Epoch 560, val loss: 0.8528008460998535
Epoch 570, training loss: 6.60076379776001 = 0.23498773574829102 + 1.0 * 6.365776062011719
Epoch 570, val loss: 0.8579427003860474
Epoch 580, training loss: 6.585098743438721 = 0.22300641238689423 + 1.0 * 6.3620924949646
Epoch 580, val loss: 0.8637451529502869
Epoch 590, training loss: 6.571005821228027 = 0.21167443692684174 + 1.0 * 6.3593316078186035
Epoch 590, val loss: 0.8700623512268066
Epoch 600, training loss: 6.57082462310791 = 0.20096619427204132 + 1.0 * 6.369858264923096
Epoch 600, val loss: 0.8769438862800598
Epoch 610, training loss: 6.548940658569336 = 0.19098544120788574 + 1.0 * 6.357955455780029
Epoch 610, val loss: 0.8843021988868713
Epoch 620, training loss: 6.536888599395752 = 0.18162378668785095 + 1.0 * 6.355264663696289
Epoch 620, val loss: 0.8922768235206604
Epoch 630, training loss: 6.526278972625732 = 0.17278973758220673 + 1.0 * 6.353489398956299
Epoch 630, val loss: 0.9006222486495972
Epoch 640, training loss: 6.5379180908203125 = 0.16446277499198914 + 1.0 * 6.37345552444458
Epoch 640, val loss: 0.9093373417854309
Epoch 650, training loss: 6.507895469665527 = 0.15671022236347198 + 1.0 * 6.351185321807861
Epoch 650, val loss: 0.9184054136276245
Epoch 660, training loss: 6.500761985778809 = 0.149418905377388 + 1.0 * 6.351343154907227
Epoch 660, val loss: 0.927892804145813
Epoch 670, training loss: 6.491799354553223 = 0.14252407848834991 + 1.0 * 6.3492751121521
Epoch 670, val loss: 0.9376498460769653
Epoch 680, training loss: 6.487267971038818 = 0.1360374242067337 + 1.0 * 6.351230621337891
Epoch 680, val loss: 0.9475284814834595
Epoch 690, training loss: 6.474835395812988 = 0.12995925545692444 + 1.0 * 6.344876289367676
Epoch 690, val loss: 0.9577744007110596
Epoch 700, training loss: 6.469966888427734 = 0.12421046197414398 + 1.0 * 6.345756530761719
Epoch 700, val loss: 0.9681907296180725
Epoch 710, training loss: 6.463820457458496 = 0.11877831816673279 + 1.0 * 6.3450422286987305
Epoch 710, val loss: 0.9787250757217407
Epoch 720, training loss: 6.459385871887207 = 0.1136525422334671 + 1.0 * 6.345733165740967
Epoch 720, val loss: 0.9895051121711731
Epoch 730, training loss: 6.451490879058838 = 0.108815498650074 + 1.0 * 6.34267520904541
Epoch 730, val loss: 1.0002343654632568
Epoch 740, training loss: 6.444370746612549 = 0.10424940288066864 + 1.0 * 6.340121269226074
Epoch 740, val loss: 1.0112546682357788
Epoch 750, training loss: 6.441745758056641 = 0.09990585595369339 + 1.0 * 6.341839790344238
Epoch 750, val loss: 1.0222632884979248
Epoch 760, training loss: 6.4367499351501465 = 0.09579296410083771 + 1.0 * 6.340957164764404
Epoch 760, val loss: 1.0333248376846313
Epoch 770, training loss: 6.429015636444092 = 0.09189970046281815 + 1.0 * 6.33711576461792
Epoch 770, val loss: 1.0444780588150024
Epoch 780, training loss: 6.424423694610596 = 0.08819612860679626 + 1.0 * 6.3362274169921875
Epoch 780, val loss: 1.055673599243164
Epoch 790, training loss: 6.425037860870361 = 0.0846707746386528 + 1.0 * 6.340367317199707
Epoch 790, val loss: 1.0668482780456543
Epoch 800, training loss: 6.414624214172363 = 0.08130485564470291 + 1.0 * 6.333319187164307
Epoch 800, val loss: 1.0780928134918213
Epoch 810, training loss: 6.411240577697754 = 0.0780905932188034 + 1.0 * 6.3331499099731445
Epoch 810, val loss: 1.0893611907958984
Epoch 820, training loss: 6.413491725921631 = 0.07502403855323792 + 1.0 * 6.338467597961426
Epoch 820, val loss: 1.1005816459655762
Epoch 830, training loss: 6.405522346496582 = 0.07211312651634216 + 1.0 * 6.333409309387207
Epoch 830, val loss: 1.1117812395095825
Epoch 840, training loss: 6.397469520568848 = 0.06933602690696716 + 1.0 * 6.328133583068848
Epoch 840, val loss: 1.1229363679885864
Epoch 850, training loss: 6.399042129516602 = 0.06667850911617279 + 1.0 * 6.332363605499268
Epoch 850, val loss: 1.1341060400009155
Epoch 860, training loss: 6.3914570808410645 = 0.06414125114679337 + 1.0 * 6.327315807342529
Epoch 860, val loss: 1.1452170610427856
Epoch 870, training loss: 6.393980026245117 = 0.06172334775328636 + 1.0 * 6.33225679397583
Epoch 870, val loss: 1.1563256978988647
Epoch 880, training loss: 6.385226249694824 = 0.059409212321043015 + 1.0 * 6.325817108154297
Epoch 880, val loss: 1.167245626449585
Epoch 890, training loss: 6.384278774261475 = 0.057191792875528336 + 1.0 * 6.327086925506592
Epoch 890, val loss: 1.1783026456832886
Epoch 900, training loss: 6.379281044006348 = 0.05505947396159172 + 1.0 * 6.324221611022949
Epoch 900, val loss: 1.1891390085220337
Epoch 910, training loss: 6.377213954925537 = 0.05301772430539131 + 1.0 * 6.3241963386535645
Epoch 910, val loss: 1.200059175491333
Epoch 920, training loss: 6.376916408538818 = 0.051057908684015274 + 1.0 * 6.3258585929870605
Epoch 920, val loss: 1.2108330726623535
Epoch 930, training loss: 6.3710222244262695 = 0.049182992428541183 + 1.0 * 6.321839332580566
Epoch 930, val loss: 1.2215088605880737
Epoch 940, training loss: 6.368328094482422 = 0.047395288944244385 + 1.0 * 6.320932865142822
Epoch 940, val loss: 1.2321553230285645
Epoch 950, training loss: 6.364112854003906 = 0.04568587988615036 + 1.0 * 6.318427085876465
Epoch 950, val loss: 1.2427313327789307
Epoch 960, training loss: 6.365573883056641 = 0.04405399411916733 + 1.0 * 6.32151985168457
Epoch 960, val loss: 1.253196358680725
Epoch 970, training loss: 6.37076473236084 = 0.042499009519815445 + 1.0 * 6.32826566696167
Epoch 970, val loss: 1.263307809829712
Epoch 980, training loss: 6.361772537231445 = 0.041028942912817 + 1.0 * 6.320743560791016
Epoch 980, val loss: 1.2735487222671509
Epoch 990, training loss: 6.356193542480469 = 0.03962893784046173 + 1.0 * 6.316564559936523
Epoch 990, val loss: 1.2836275100708008
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.7996837111228255
=== training gcn model ===
Epoch 0, training loss: 10.549162864685059 = 1.9523197412490845 + 1.0 * 8.596842765808105
Epoch 0, val loss: 1.95376455783844
Epoch 10, training loss: 10.538338661193848 = 1.9417825937271118 + 1.0 * 8.596555709838867
Epoch 10, val loss: 1.943166732788086
Epoch 20, training loss: 10.521733283996582 = 1.9278115034103394 + 1.0 * 8.593921661376953
Epoch 20, val loss: 1.9286186695098877
Epoch 30, training loss: 10.480738639831543 = 1.9072622060775757 + 1.0 * 8.573476791381836
Epoch 30, val loss: 1.90703284740448
Epoch 40, training loss: 10.328469276428223 = 1.8808097839355469 + 1.0 * 8.447659492492676
Epoch 40, val loss: 1.8808448314666748
Epoch 50, training loss: 9.93359375 = 1.8541343212127686 + 1.0 * 8.079459190368652
Epoch 50, val loss: 1.8556562662124634
Epoch 60, training loss: 9.538202285766602 = 1.8340116739273071 + 1.0 * 7.704190731048584
Epoch 60, val loss: 1.8381365537643433
Epoch 70, training loss: 9.18453311920166 = 1.8195974826812744 + 1.0 * 7.364935398101807
Epoch 70, val loss: 1.8251550197601318
Epoch 80, training loss: 8.951417922973633 = 1.8067620992660522 + 1.0 * 7.144655704498291
Epoch 80, val loss: 1.8126667737960815
Epoch 90, training loss: 8.77575397491455 = 1.7945061922073364 + 1.0 * 6.981247425079346
Epoch 90, val loss: 1.8011157512664795
Epoch 100, training loss: 8.637604713439941 = 1.7834011316299438 + 1.0 * 6.854203701019287
Epoch 100, val loss: 1.7911912202835083
Epoch 110, training loss: 8.53866195678711 = 1.7717074155807495 + 1.0 * 6.7669548988342285
Epoch 110, val loss: 1.7809711694717407
Epoch 120, training loss: 8.463350296020508 = 1.758942723274231 + 1.0 * 6.704407215118408
Epoch 120, val loss: 1.7697199583053589
Epoch 130, training loss: 8.401175498962402 = 1.7446790933609009 + 1.0 * 6.656496524810791
Epoch 130, val loss: 1.7571941614151
Epoch 140, training loss: 8.351455688476562 = 1.7283222675323486 + 1.0 * 6.623133182525635
Epoch 140, val loss: 1.7431092262268066
Epoch 150, training loss: 8.301708221435547 = 1.709559440612793 + 1.0 * 6.592148780822754
Epoch 150, val loss: 1.727142572402954
Epoch 160, training loss: 8.257113456726074 = 1.6878759860992432 + 1.0 * 6.56923770904541
Epoch 160, val loss: 1.7088556289672852
Epoch 170, training loss: 8.212596893310547 = 1.662680745124817 + 1.0 * 6.549915790557861
Epoch 170, val loss: 1.687712550163269
Epoch 180, training loss: 8.1666898727417 = 1.633701205253601 + 1.0 * 6.532988548278809
Epoch 180, val loss: 1.6635117530822754
Epoch 190, training loss: 8.119501113891602 = 1.6004557609558105 + 1.0 * 6.519045829772949
Epoch 190, val loss: 1.6357327699661255
Epoch 200, training loss: 8.068909645080566 = 1.5629642009735107 + 1.0 * 6.505945682525635
Epoch 200, val loss: 1.6046133041381836
Epoch 210, training loss: 8.015767097473145 = 1.5222218036651611 + 1.0 * 6.4935455322265625
Epoch 210, val loss: 1.5708986520767212
Epoch 220, training loss: 7.961703300476074 = 1.4780137538909912 + 1.0 * 6.483689785003662
Epoch 220, val loss: 1.5343878269195557
Epoch 230, training loss: 7.905662536621094 = 1.4318468570709229 + 1.0 * 6.47381591796875
Epoch 230, val loss: 1.4964661598205566
Epoch 240, training loss: 7.850381851196289 = 1.3845646381378174 + 1.0 * 6.465817451477051
Epoch 240, val loss: 1.4579945802688599
Epoch 250, training loss: 7.7953877449035645 = 1.3369640111923218 + 1.0 * 6.458423614501953
Epoch 250, val loss: 1.4196504354476929
Epoch 260, training loss: 7.743504524230957 = 1.2899134159088135 + 1.0 * 6.4535908699035645
Epoch 260, val loss: 1.3825191259384155
Epoch 270, training loss: 7.691775321960449 = 1.2442067861557007 + 1.0 * 6.447568416595459
Epoch 270, val loss: 1.3469467163085938
Epoch 280, training loss: 7.637460231781006 = 1.199580192565918 + 1.0 * 6.437880039215088
Epoch 280, val loss: 1.3127835988998413
Epoch 290, training loss: 7.588351249694824 = 1.1555887460708618 + 1.0 * 6.432762622833252
Epoch 290, val loss: 1.2796976566314697
Epoch 300, training loss: 7.541049957275391 = 1.1123205423355103 + 1.0 * 6.42872953414917
Epoch 300, val loss: 1.2475990056991577
Epoch 310, training loss: 7.490889549255371 = 1.0696810483932495 + 1.0 * 6.421208381652832
Epoch 310, val loss: 1.2165392637252808
Epoch 320, training loss: 7.446775913238525 = 1.0273605585098267 + 1.0 * 6.419415473937988
Epoch 320, val loss: 1.186034083366394
Epoch 330, training loss: 7.401218414306641 = 0.9858273267745972 + 1.0 * 6.415390968322754
Epoch 330, val loss: 1.1562292575836182
Epoch 340, training loss: 7.353991985321045 = 0.9451156258583069 + 1.0 * 6.408876419067383
Epoch 340, val loss: 1.1272813081741333
Epoch 350, training loss: 7.31169319152832 = 0.9050936102867126 + 1.0 * 6.406599521636963
Epoch 350, val loss: 1.09898042678833
Epoch 360, training loss: 7.271500587463379 = 0.8661777377128601 + 1.0 * 6.405323028564453
Epoch 360, val loss: 1.0717122554779053
Epoch 370, training loss: 7.226467132568359 = 0.8285827040672302 + 1.0 * 6.397884368896484
Epoch 370, val loss: 1.0455753803253174
Epoch 380, training loss: 7.1893534660339355 = 0.7922778725624084 + 1.0 * 6.397075653076172
Epoch 380, val loss: 1.0204966068267822
Epoch 390, training loss: 7.150182723999023 = 0.757443904876709 + 1.0 * 6.3927388191223145
Epoch 390, val loss: 0.9967270493507385
Epoch 400, training loss: 7.111962795257568 = 0.724281370639801 + 1.0 * 6.387681484222412
Epoch 400, val loss: 0.9744225144386292
Epoch 410, training loss: 7.0784454345703125 = 0.6927694082260132 + 1.0 * 6.38567590713501
Epoch 410, val loss: 0.9536777138710022
Epoch 420, training loss: 7.044799327850342 = 0.6626831293106079 + 1.0 * 6.382116317749023
Epoch 420, val loss: 0.9343557953834534
Epoch 430, training loss: 7.013849258422852 = 0.6340434551239014 + 1.0 * 6.379805564880371
Epoch 430, val loss: 0.9164595007896423
Epoch 440, training loss: 6.983044147491455 = 0.6068434119224548 + 1.0 * 6.3762006759643555
Epoch 440, val loss: 0.9001939296722412
Epoch 450, training loss: 6.964203834533691 = 0.5809847712516785 + 1.0 * 6.383219242095947
Epoch 450, val loss: 0.8853148221969604
Epoch 460, training loss: 6.930300235748291 = 0.5564984679222107 + 1.0 * 6.3738017082214355
Epoch 460, val loss: 0.8720331192016602
Epoch 470, training loss: 6.903203010559082 = 0.5332474112510681 + 1.0 * 6.369955539703369
Epoch 470, val loss: 0.8601658940315247
Epoch 480, training loss: 6.87764310836792 = 0.5109850764274597 + 1.0 * 6.3666582107543945
Epoch 480, val loss: 0.8494648933410645
Epoch 490, training loss: 6.856469631195068 = 0.489664763212204 + 1.0 * 6.366805076599121
Epoch 490, val loss: 0.8400113582611084
Epoch 500, training loss: 6.833498477935791 = 0.46935051679611206 + 1.0 * 6.364148139953613
Epoch 500, val loss: 0.8317931890487671
Epoch 510, training loss: 6.810708045959473 = 0.44984889030456543 + 1.0 * 6.360859394073486
Epoch 510, val loss: 0.8245435953140259
Epoch 520, training loss: 6.79227876663208 = 0.4311278760433197 + 1.0 * 6.361150741577148
Epoch 520, val loss: 0.8183701038360596
Epoch 530, training loss: 6.771138668060303 = 0.4132636785507202 + 1.0 * 6.357874870300293
Epoch 530, val loss: 0.8130084276199341
Epoch 540, training loss: 6.750398635864258 = 0.3960047662258148 + 1.0 * 6.35439395904541
Epoch 540, val loss: 0.8083943128585815
Epoch 550, training loss: 6.742995262145996 = 0.37928614020347595 + 1.0 * 6.363708972930908
Epoch 550, val loss: 0.8044388890266418
Epoch 560, training loss: 6.714842796325684 = 0.36322712898254395 + 1.0 * 6.351615905761719
Epoch 560, val loss: 0.8011136054992676
Epoch 570, training loss: 6.69718074798584 = 0.3476361036300659 + 1.0 * 6.349544525146484
Epoch 570, val loss: 0.7982829213142395
Epoch 580, training loss: 6.681162357330322 = 0.3324319124221802 + 1.0 * 6.348730564117432
Epoch 580, val loss: 0.7959284782409668
Epoch 590, training loss: 6.666533470153809 = 0.31762877106666565 + 1.0 * 6.348904609680176
Epoch 590, val loss: 0.7940645217895508
Epoch 600, training loss: 6.647256851196289 = 0.30322080850601196 + 1.0 * 6.344036102294922
Epoch 600, val loss: 0.7925090789794922
Epoch 610, training loss: 6.634711265563965 = 0.2890806198120117 + 1.0 * 6.345630645751953
Epoch 610, val loss: 0.7912478446960449
Epoch 620, training loss: 6.626784801483154 = 0.27519527077674866 + 1.0 * 6.351589679718018
Epoch 620, val loss: 0.7904022932052612
Epoch 630, training loss: 6.602093696594238 = 0.26168572902679443 + 1.0 * 6.340407848358154
Epoch 630, val loss: 0.78996741771698
Epoch 640, training loss: 6.585972309112549 = 0.24835707247257233 + 1.0 * 6.337615013122559
Epoch 640, val loss: 0.7900003790855408
Epoch 650, training loss: 6.57219123840332 = 0.23523230850696564 + 1.0 * 6.336958885192871
Epoch 650, val loss: 0.7905764579772949
Epoch 660, training loss: 6.565414905548096 = 0.2223660796880722 + 1.0 * 6.343049049377441
Epoch 660, val loss: 0.7917913794517517
Epoch 670, training loss: 6.549640655517578 = 0.20994199812412262 + 1.0 * 6.339698791503906
Epoch 670, val loss: 0.7934422492980957
Epoch 680, training loss: 6.531172275543213 = 0.19803205132484436 + 1.0 * 6.3331403732299805
Epoch 680, val loss: 0.795821487903595
Epoch 690, training loss: 6.518052577972412 = 0.18659836053848267 + 1.0 * 6.331454277038574
Epoch 690, val loss: 0.7988427877426147
Epoch 700, training loss: 6.521689414978027 = 0.1757018268108368 + 1.0 * 6.345987796783447
Epoch 700, val loss: 0.8024995923042297
Epoch 710, training loss: 6.49579381942749 = 0.16548055410385132 + 1.0 * 6.330313205718994
Epoch 710, val loss: 0.8068578243255615
Epoch 720, training loss: 6.485541343688965 = 0.1558806300163269 + 1.0 * 6.329660892486572
Epoch 720, val loss: 0.8118300437927246
Epoch 730, training loss: 6.475897789001465 = 0.14689843356609344 + 1.0 * 6.3289995193481445
Epoch 730, val loss: 0.8173556327819824
Epoch 740, training loss: 6.467650413513184 = 0.1385483592748642 + 1.0 * 6.329102039337158
Epoch 740, val loss: 0.8232370615005493
Epoch 750, training loss: 6.456057071685791 = 0.13083584606647491 + 1.0 * 6.325221061706543
Epoch 750, val loss: 0.8295764327049255
Epoch 760, training loss: 6.449016571044922 = 0.12371573597192764 + 1.0 * 6.325300693511963
Epoch 760, val loss: 0.8362816572189331
Epoch 770, training loss: 6.444512367248535 = 0.11714471876621246 + 1.0 * 6.327367782592773
Epoch 770, val loss: 0.8431946635246277
Epoch 780, training loss: 6.434103488922119 = 0.11110970377922058 + 1.0 * 6.322993755340576
Epoch 780, val loss: 0.8504366874694824
Epoch 790, training loss: 6.4309234619140625 = 0.10550186038017273 + 1.0 * 6.3254218101501465
Epoch 790, val loss: 0.8578126430511475
Epoch 800, training loss: 6.422480583190918 = 0.1003275066614151 + 1.0 * 6.322153091430664
Epoch 800, val loss: 0.8652178645133972
Epoch 810, training loss: 6.415887355804443 = 0.09552973508834839 + 1.0 * 6.320357799530029
Epoch 810, val loss: 0.8727787733078003
Epoch 820, training loss: 6.411721706390381 = 0.09106322377920151 + 1.0 * 6.3206586837768555
Epoch 820, val loss: 0.8804165124893188
Epoch 830, training loss: 6.4065704345703125 = 0.08689365535974503 + 1.0 * 6.319676876068115
Epoch 830, val loss: 0.8880457878112793
Epoch 840, training loss: 6.401093482971191 = 0.08302828669548035 + 1.0 * 6.318065166473389
Epoch 840, val loss: 0.8958150744438171
Epoch 850, training loss: 6.395484924316406 = 0.07938522845506668 + 1.0 * 6.316099643707275
Epoch 850, val loss: 0.9036107659339905
Epoch 860, training loss: 6.394126892089844 = 0.0759584903717041 + 1.0 * 6.3181681632995605
Epoch 860, val loss: 0.9113109111785889
Epoch 870, training loss: 6.390257835388184 = 0.07273118942975998 + 1.0 * 6.317526817321777
Epoch 870, val loss: 0.9190842509269714
Epoch 880, training loss: 6.382965564727783 = 0.06968420743942261 + 1.0 * 6.313281536102295
Epoch 880, val loss: 0.9269272685050964
Epoch 890, training loss: 6.39080810546875 = 0.06680280715227127 + 1.0 * 6.324005126953125
Epoch 890, val loss: 0.9346486926078796
Epoch 900, training loss: 6.37690544128418 = 0.06406503170728683 + 1.0 * 6.312840461730957
Epoch 900, val loss: 0.9422456622123718
Epoch 910, training loss: 6.371479034423828 = 0.061476610600948334 + 1.0 * 6.310002326965332
Epoch 910, val loss: 0.9500360488891602
Epoch 920, training loss: 6.366724014282227 = 0.05899869650602341 + 1.0 * 6.307725429534912
Epoch 920, val loss: 0.9577168822288513
Epoch 930, training loss: 6.36599588394165 = 0.05663648620247841 + 1.0 * 6.309359550476074
Epoch 930, val loss: 0.9655374884605408
Epoch 940, training loss: 6.367664337158203 = 0.054389722645282745 + 1.0 * 6.313274383544922
Epoch 940, val loss: 0.9731456637382507
Epoch 950, training loss: 6.361877918243408 = 0.05225849524140358 + 1.0 * 6.309619426727295
Epoch 950, val loss: 0.9807432889938354
Epoch 960, training loss: 6.354501724243164 = 0.0502372570335865 + 1.0 * 6.304264545440674
Epoch 960, val loss: 0.9884471893310547
Epoch 970, training loss: 6.353392601013184 = 0.048299968242645264 + 1.0 * 6.305092811584473
Epoch 970, val loss: 0.9960676431655884
Epoch 980, training loss: 6.353030204772949 = 0.04644960165023804 + 1.0 * 6.306580543518066
Epoch 980, val loss: 1.0035604238510132
Epoch 990, training loss: 6.349067211151123 = 0.044694684445858 + 1.0 * 6.304372310638428
Epoch 990, val loss: 1.011292576789856
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.7981022667369532
The final CL Acc:0.76914, 0.01145, The final GNN Acc:0.79828, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13210])
remove edge: torch.Size([2, 7882])
updated graph: torch.Size([2, 10536])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.54006290435791 = 1.9432274103164673 + 1.0 * 8.596835136413574
Epoch 0, val loss: 1.9441105127334595
Epoch 10, training loss: 10.529864311218262 = 1.933322787284851 + 1.0 * 8.596541404724121
Epoch 10, val loss: 1.934815764427185
Epoch 20, training loss: 10.514656066894531 = 1.9205294847488403 + 1.0 * 8.59412670135498
Epoch 20, val loss: 1.9224157333374023
Epoch 30, training loss: 10.475457191467285 = 1.9022548198699951 + 1.0 * 8.573202133178711
Epoch 30, val loss: 1.9045530557632446
Epoch 40, training loss: 10.318873405456543 = 1.87770676612854 + 1.0 * 8.441166877746582
Epoch 40, val loss: 1.8814529180526733
Epoch 50, training loss: 9.81498908996582 = 1.8508483171463013 + 1.0 * 7.964140892028809
Epoch 50, val loss: 1.856963038444519
Epoch 60, training loss: 9.403664588928223 = 1.8297239542007446 + 1.0 * 7.573940277099609
Epoch 60, val loss: 1.8378022909164429
Epoch 70, training loss: 9.05452823638916 = 1.8153942823410034 + 1.0 * 7.239133834838867
Epoch 70, val loss: 1.8241647481918335
Epoch 80, training loss: 8.799612045288086 = 1.8023607730865479 + 1.0 * 6.997251033782959
Epoch 80, val loss: 1.8121237754821777
Epoch 90, training loss: 8.684419631958008 = 1.786016583442688 + 1.0 * 6.898402690887451
Epoch 90, val loss: 1.7972146272659302
Epoch 100, training loss: 8.572264671325684 = 1.7674537897109985 + 1.0 * 6.804810523986816
Epoch 100, val loss: 1.7804687023162842
Epoch 110, training loss: 8.48430061340332 = 1.749971866607666 + 1.0 * 6.734328746795654
Epoch 110, val loss: 1.7646558284759521
Epoch 120, training loss: 8.406981468200684 = 1.7319105863571167 + 1.0 * 6.6750712394714355
Epoch 120, val loss: 1.7482599020004272
Epoch 130, training loss: 8.341519355773926 = 1.7110786437988281 + 1.0 * 6.630440711975098
Epoch 130, val loss: 1.7298061847686768
Epoch 140, training loss: 8.283602714538574 = 1.6864084005355835 + 1.0 * 6.597194671630859
Epoch 140, val loss: 1.7083051204681396
Epoch 150, training loss: 8.229022979736328 = 1.6575496196746826 + 1.0 * 6.571473598480225
Epoch 150, val loss: 1.6833937168121338
Epoch 160, training loss: 8.174216270446777 = 1.6241644620895386 + 1.0 * 6.550052165985107
Epoch 160, val loss: 1.6548302173614502
Epoch 170, training loss: 8.121024131774902 = 1.585878849029541 + 1.0 * 6.535145282745361
Epoch 170, val loss: 1.6224005222320557
Epoch 180, training loss: 8.061942100524902 = 1.5433257818222046 + 1.0 * 6.518616676330566
Epoch 180, val loss: 1.5867366790771484
Epoch 190, training loss: 8.00169563293457 = 1.496984601020813 + 1.0 * 6.504711151123047
Epoch 190, val loss: 1.5480306148529053
Epoch 200, training loss: 7.94570255279541 = 1.4476622343063354 + 1.0 * 6.498040199279785
Epoch 200, val loss: 1.5071488618850708
Epoch 210, training loss: 7.883156776428223 = 1.3972423076629639 + 1.0 * 6.48591423034668
Epoch 210, val loss: 1.4658812284469604
Epoch 220, training loss: 7.823451995849609 = 1.3471029996871948 + 1.0 * 6.476348876953125
Epoch 220, val loss: 1.4253679513931274
Epoch 230, training loss: 7.764474391937256 = 1.2975492477416992 + 1.0 * 6.466925144195557
Epoch 230, val loss: 1.3858174085617065
Epoch 240, training loss: 7.708200931549072 = 1.2489887475967407 + 1.0 * 6.459212303161621
Epoch 240, val loss: 1.3476123809814453
Epoch 250, training loss: 7.6560869216918945 = 1.2022091150283813 + 1.0 * 6.453877925872803
Epoch 250, val loss: 1.311435580253601
Epoch 260, training loss: 7.6041131019592285 = 1.1580137014389038 + 1.0 * 6.446099281311035
Epoch 260, val loss: 1.2780542373657227
Epoch 270, training loss: 7.557834625244141 = 1.1161385774612427 + 1.0 * 6.4416961669921875
Epoch 270, val loss: 1.2471450567245483
Epoch 280, training loss: 7.515883445739746 = 1.0770708322525024 + 1.0 * 6.438812732696533
Epoch 280, val loss: 1.218932032585144
Epoch 290, training loss: 7.4705424308776855 = 1.0407614707946777 + 1.0 * 6.429780960083008
Epoch 290, val loss: 1.1932843923568726
Epoch 300, training loss: 7.430843353271484 = 1.0065813064575195 + 1.0 * 6.424262046813965
Epoch 300, val loss: 1.1695791482925415
Epoch 310, training loss: 7.3960137367248535 = 0.9740666151046753 + 1.0 * 6.421947002410889
Epoch 310, val loss: 1.147176742553711
Epoch 320, training loss: 7.360963344573975 = 0.943120002746582 + 1.0 * 6.417843341827393
Epoch 320, val loss: 1.1258751153945923
Epoch 330, training loss: 7.3245320320129395 = 0.9133427739143372 + 1.0 * 6.411189079284668
Epoch 330, val loss: 1.1052871942520142
Epoch 340, training loss: 7.294814109802246 = 0.8840510845184326 + 1.0 * 6.410763263702393
Epoch 340, val loss: 1.0848468542099
Epoch 350, training loss: 7.258932590484619 = 0.8549589514732361 + 1.0 * 6.403973579406738
Epoch 350, val loss: 1.0641499757766724
Epoch 360, training loss: 7.2249860763549805 = 0.8257067203521729 + 1.0 * 6.399279594421387
Epoch 360, val loss: 1.043092131614685
Epoch 370, training loss: 7.191632270812988 = 0.7961156368255615 + 1.0 * 6.395516395568848
Epoch 370, val loss: 1.0214033126831055
Epoch 380, training loss: 7.158542633056641 = 0.7662630081176758 + 1.0 * 6.392279624938965
Epoch 380, val loss: 0.999077558517456
Epoch 390, training loss: 7.130346298217773 = 0.7360431551933289 + 1.0 * 6.394303321838379
Epoch 390, val loss: 0.9762740731239319
Epoch 400, training loss: 7.0916829109191895 = 0.7057642340660095 + 1.0 * 6.385918617248535
Epoch 400, val loss: 0.95318603515625
Epoch 410, training loss: 7.057794094085693 = 0.6752882599830627 + 1.0 * 6.382505893707275
Epoch 410, val loss: 0.929955780506134
Epoch 420, training loss: 7.029695987701416 = 0.6448078751564026 + 1.0 * 6.384888172149658
Epoch 420, val loss: 0.9069455862045288
Epoch 430, training loss: 6.9921369552612305 = 0.6147194504737854 + 1.0 * 6.37741756439209
Epoch 430, val loss: 0.8844608664512634
Epoch 440, training loss: 6.959399223327637 = 0.5849869847297668 + 1.0 * 6.3744120597839355
Epoch 440, val loss: 0.8626042604446411
Epoch 450, training loss: 6.938845634460449 = 0.5557430386543274 + 1.0 * 6.3831024169921875
Epoch 450, val loss: 0.841734766960144
Epoch 460, training loss: 6.899813175201416 = 0.5274922251701355 + 1.0 * 6.372321128845215
Epoch 460, val loss: 0.822256326675415
Epoch 470, training loss: 6.867642879486084 = 0.5001019835472107 + 1.0 * 6.3675408363342285
Epoch 470, val loss: 0.8041960000991821
Epoch 480, training loss: 6.8474860191345215 = 0.4735378623008728 + 1.0 * 6.373948097229004
Epoch 480, val loss: 0.7876349687576294
Epoch 490, training loss: 6.814023971557617 = 0.44806987047195435 + 1.0 * 6.3659539222717285
Epoch 490, val loss: 0.7727741003036499
Epoch 500, training loss: 6.785946369171143 = 0.42368656396865845 + 1.0 * 6.362259864807129
Epoch 500, val loss: 0.7594283223152161
Epoch 510, training loss: 6.762994289398193 = 0.4002152681350708 + 1.0 * 6.362779140472412
Epoch 510, val loss: 0.7475855350494385
Epoch 520, training loss: 6.744409084320068 = 0.3777080178260803 + 1.0 * 6.366701126098633
Epoch 520, val loss: 0.7372283935546875
Epoch 530, training loss: 6.712924480438232 = 0.3563464283943176 + 1.0 * 6.3565778732299805
Epoch 530, val loss: 0.7282087206840515
Epoch 540, training loss: 6.690883159637451 = 0.3358502686023712 + 1.0 * 6.355032920837402
Epoch 540, val loss: 0.7204779386520386
Epoch 550, training loss: 6.669662952423096 = 0.31616342067718506 + 1.0 * 6.353499412536621
Epoch 550, val loss: 0.7139315009117126
Epoch 560, training loss: 6.65946626663208 = 0.2973824441432953 + 1.0 * 6.362083911895752
Epoch 560, val loss: 0.7086005806922913
Epoch 570, training loss: 6.633153915405273 = 0.2795989513397217 + 1.0 * 6.353555202484131
Epoch 570, val loss: 0.7044125199317932
Epoch 580, training loss: 6.611424446105957 = 0.2628064751625061 + 1.0 * 6.348618030548096
Epoch 580, val loss: 0.7014219760894775
Epoch 590, training loss: 6.605659008026123 = 0.24693651497364044 + 1.0 * 6.358722686767578
Epoch 590, val loss: 0.6995012760162354
Epoch 600, training loss: 6.57816219329834 = 0.23202818632125854 + 1.0 * 6.346134185791016
Epoch 600, val loss: 0.6986753940582275
Epoch 610, training loss: 6.562613487243652 = 0.218078151345253 + 1.0 * 6.3445353507995605
Epoch 610, val loss: 0.6987974047660828
Epoch 620, training loss: 6.549929618835449 = 0.20500430464744568 + 1.0 * 6.344925403594971
Epoch 620, val loss: 0.6998148560523987
Epoch 630, training loss: 6.533901214599609 = 0.1927953064441681 + 1.0 * 6.341105937957764
Epoch 630, val loss: 0.7016843557357788
Epoch 640, training loss: 6.534092426300049 = 0.18143673241138458 + 1.0 * 6.35265588760376
Epoch 640, val loss: 0.7042531967163086
Epoch 650, training loss: 6.511808395385742 = 0.17090477049350739 + 1.0 * 6.3409037590026855
Epoch 650, val loss: 0.7074524760246277
Epoch 660, training loss: 6.498918056488037 = 0.1611155867576599 + 1.0 * 6.337802410125732
Epoch 660, val loss: 0.7112641930580139
Epoch 670, training loss: 6.491559028625488 = 0.1519468128681183 + 1.0 * 6.339612007141113
Epoch 670, val loss: 0.7155376076698303
Epoch 680, training loss: 6.481276035308838 = 0.14341013133525848 + 1.0 * 6.337865829467773
Epoch 680, val loss: 0.7202138900756836
Epoch 690, training loss: 6.469476699829102 = 0.13545548915863037 + 1.0 * 6.334021091461182
Epoch 690, val loss: 0.7253134846687317
Epoch 700, training loss: 6.4637451171875 = 0.12801723182201385 + 1.0 * 6.335727691650391
Epoch 700, val loss: 0.7307196259498596
Epoch 710, training loss: 6.452605724334717 = 0.12107645720243454 + 1.0 * 6.331529140472412
Epoch 710, val loss: 0.7363817691802979
Epoch 720, training loss: 6.448829174041748 = 0.11458317190408707 + 1.0 * 6.3342461585998535
Epoch 720, val loss: 0.7422447204589844
Epoch 730, training loss: 6.437623500823975 = 0.10851702094078064 + 1.0 * 6.329106330871582
Epoch 730, val loss: 0.7483349442481995
Epoch 740, training loss: 6.4299139976501465 = 0.10282465070486069 + 1.0 * 6.327089309692383
Epoch 740, val loss: 0.7545954585075378
Epoch 750, training loss: 6.42986536026001 = 0.0974816232919693 + 1.0 * 6.332383632659912
Epoch 750, val loss: 0.760986864566803
Epoch 760, training loss: 6.4189276695251465 = 0.09246163815259933 + 1.0 * 6.326466083526611
Epoch 760, val loss: 0.7675083875656128
Epoch 770, training loss: 6.411597728729248 = 0.08774320036172867 + 1.0 * 6.323854446411133
Epoch 770, val loss: 0.774176836013794
Epoch 780, training loss: 6.414265155792236 = 0.08329427242279053 + 1.0 * 6.330970764160156
Epoch 780, val loss: 0.7809159159660339
Epoch 790, training loss: 6.41049337387085 = 0.07915002852678299 + 1.0 * 6.331343173980713
Epoch 790, val loss: 0.7875933647155762
Epoch 800, training loss: 6.395438194274902 = 0.07525079697370529 + 1.0 * 6.320187568664551
Epoch 800, val loss: 0.7944180965423584
Epoch 810, training loss: 6.390852451324463 = 0.07159112393856049 + 1.0 * 6.31926155090332
Epoch 810, val loss: 0.8013252019882202
Epoch 820, training loss: 6.385815620422363 = 0.06814364343881607 + 1.0 * 6.317671775817871
Epoch 820, val loss: 0.8082691431045532
Epoch 830, training loss: 6.391119480133057 = 0.06489113718271255 + 1.0 * 6.326228141784668
Epoch 830, val loss: 0.8152863383293152
Epoch 840, training loss: 6.387125015258789 = 0.06185470148921013 + 1.0 * 6.325270175933838
Epoch 840, val loss: 0.8221461176872253
Epoch 850, training loss: 6.376849174499512 = 0.05902158468961716 + 1.0 * 6.3178277015686035
Epoch 850, val loss: 0.829108715057373
Epoch 860, training loss: 6.369735240936279 = 0.05634965002536774 + 1.0 * 6.313385486602783
Epoch 860, val loss: 0.8361087441444397
Epoch 870, training loss: 6.371318817138672 = 0.053835585713386536 + 1.0 * 6.317483425140381
Epoch 870, val loss: 0.8430783748626709
Epoch 880, training loss: 6.364405155181885 = 0.05147120729088783 + 1.0 * 6.312933921813965
Epoch 880, val loss: 0.8500248789787292
Epoch 890, training loss: 6.360990047454834 = 0.04925062507390976 + 1.0 * 6.311739444732666
Epoch 890, val loss: 0.8570116758346558
Epoch 900, training loss: 6.3639702796936035 = 0.04715719446539879 + 1.0 * 6.316812992095947
Epoch 900, val loss: 0.8639277815818787
Epoch 910, training loss: 6.357566833496094 = 0.045187823474407196 + 1.0 * 6.312378883361816
Epoch 910, val loss: 0.8707841038703918
Epoch 920, training loss: 6.359665393829346 = 0.04334086924791336 + 1.0 * 6.316324710845947
Epoch 920, val loss: 0.8776702880859375
Epoch 930, training loss: 6.3495097160339355 = 0.041594117879867554 + 1.0 * 6.307915687561035
Epoch 930, val loss: 0.8844236731529236
Epoch 940, training loss: 6.346621036529541 = 0.03995198756456375 + 1.0 * 6.306669235229492
Epoch 940, val loss: 0.8912871479988098
Epoch 950, training loss: 6.344405651092529 = 0.03839821740984917 + 1.0 * 6.306007385253906
Epoch 950, val loss: 0.8980963230133057
Epoch 960, training loss: 6.348474025726318 = 0.036928918212652206 + 1.0 * 6.311544895172119
Epoch 960, val loss: 0.9048397541046143
Epoch 970, training loss: 6.3447418212890625 = 0.035534463822841644 + 1.0 * 6.309207439422607
Epoch 970, val loss: 0.91138756275177
Epoch 980, training loss: 6.343134880065918 = 0.03422833979129791 + 1.0 * 6.308906555175781
Epoch 980, val loss: 0.9181177020072937
Epoch 990, training loss: 6.339687824249268 = 0.032987672835588455 + 1.0 * 6.306700229644775
Epoch 990, val loss: 0.9245578050613403
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 10.534109115600586 = 1.937282919883728 + 1.0 * 8.596826553344727
Epoch 0, val loss: 1.9304317235946655
Epoch 10, training loss: 10.523759841918945 = 1.9272640943527222 + 1.0 * 8.596495628356934
Epoch 10, val loss: 1.92097008228302
Epoch 20, training loss: 10.508118629455566 = 1.914236068725586 + 1.0 * 8.59388256072998
Epoch 20, val loss: 1.9082568883895874
Epoch 30, training loss: 10.469701766967773 = 1.8952479362487793 + 1.0 * 8.574453353881836
Epoch 30, val loss: 1.8895186185836792
Epoch 40, training loss: 10.340270042419434 = 1.8699685335159302 + 1.0 * 8.470301628112793
Epoch 40, val loss: 1.8652368783950806
Epoch 50, training loss: 9.924476623535156 = 1.842745304107666 + 1.0 * 8.081730842590332
Epoch 50, val loss: 1.8392122983932495
Epoch 60, training loss: 9.596220970153809 = 1.8169864416122437 + 1.0 * 7.779234886169434
Epoch 60, val loss: 1.8156988620758057
Epoch 70, training loss: 9.16522216796875 = 1.7995927333831787 + 1.0 * 7.36562967300415
Epoch 70, val loss: 1.8003438711166382
Epoch 80, training loss: 8.909505844116211 = 1.788286566734314 + 1.0 * 7.121219158172607
Epoch 80, val loss: 1.7902647256851196
Epoch 90, training loss: 8.765338897705078 = 1.772206425666809 + 1.0 * 6.9931321144104
Epoch 90, val loss: 1.7757285833358765
Epoch 100, training loss: 8.66737174987793 = 1.752510666847229 + 1.0 * 6.914860725402832
Epoch 100, val loss: 1.7582978010177612
Epoch 110, training loss: 8.561600685119629 = 1.733222246170044 + 1.0 * 6.828378677368164
Epoch 110, val loss: 1.740995168685913
Epoch 120, training loss: 8.484312057495117 = 1.7134380340576172 + 1.0 * 6.7708740234375
Epoch 120, val loss: 1.7227435111999512
Epoch 130, training loss: 8.409014701843262 = 1.6899038553237915 + 1.0 * 6.719110488891602
Epoch 130, val loss: 1.701704978942871
Epoch 140, training loss: 8.333292961120605 = 1.66298508644104 + 1.0 * 6.6703081130981445
Epoch 140, val loss: 1.6780939102172852
Epoch 150, training loss: 8.263248443603516 = 1.6312618255615234 + 1.0 * 6.631986618041992
Epoch 150, val loss: 1.6504662036895752
Epoch 160, training loss: 8.19513988494873 = 1.5939991474151611 + 1.0 * 6.60114049911499
Epoch 160, val loss: 1.618483543395996
Epoch 170, training loss: 8.128093719482422 = 1.551509141921997 + 1.0 * 6.576584339141846
Epoch 170, val loss: 1.582152009010315
Epoch 180, training loss: 8.061979293823242 = 1.5043696165084839 + 1.0 * 6.557610034942627
Epoch 180, val loss: 1.54218590259552
Epoch 190, training loss: 7.995392799377441 = 1.453658938407898 + 1.0 * 6.541733741760254
Epoch 190, val loss: 1.4997345209121704
Epoch 200, training loss: 7.929278373718262 = 1.4001684188842773 + 1.0 * 6.529109954833984
Epoch 200, val loss: 1.4554647207260132
Epoch 210, training loss: 7.863090515136719 = 1.346269130706787 + 1.0 * 6.516821384429932
Epoch 210, val loss: 1.4116705656051636
Epoch 220, training loss: 7.799062252044678 = 1.2930124998092651 + 1.0 * 6.506049633026123
Epoch 220, val loss: 1.369097352027893
Epoch 230, training loss: 7.740320682525635 = 1.2404230833053589 + 1.0 * 6.499897480010986
Epoch 230, val loss: 1.3276443481445312
Epoch 240, training loss: 7.676865100860596 = 1.1898540258407593 + 1.0 * 6.487010955810547
Epoch 240, val loss: 1.2884432077407837
Epoch 250, training loss: 7.617290496826172 = 1.1409187316894531 + 1.0 * 6.476371765136719
Epoch 250, val loss: 1.2510316371917725
Epoch 260, training loss: 7.564845561981201 = 1.093348503112793 + 1.0 * 6.471497058868408
Epoch 260, val loss: 1.2149484157562256
Epoch 270, training loss: 7.507984638214111 = 1.0475263595581055 + 1.0 * 6.460458278656006
Epoch 270, val loss: 1.1806241273880005
Epoch 280, training loss: 7.455767631530762 = 1.003326177597046 + 1.0 * 6.452441215515137
Epoch 280, val loss: 1.1477282047271729
Epoch 290, training loss: 7.407202243804932 = 0.9601655006408691 + 1.0 * 6.4470367431640625
Epoch 290, val loss: 1.1157618761062622
Epoch 300, training loss: 7.358456134796143 = 0.9183008074760437 + 1.0 * 6.440155506134033
Epoch 300, val loss: 1.0846607685089111
Epoch 310, training loss: 7.316967964172363 = 0.8775479793548584 + 1.0 * 6.439419746398926
Epoch 310, val loss: 1.0545547008514404
Epoch 320, training loss: 7.267716884613037 = 0.8378071188926697 + 1.0 * 6.429909706115723
Epoch 320, val loss: 1.02530837059021
Epoch 330, training loss: 7.222184181213379 = 0.7987657189369202 + 1.0 * 6.4234185218811035
Epoch 330, val loss: 0.9964706897735596
Epoch 340, training loss: 7.1818647384643555 = 0.760337233543396 + 1.0 * 6.42152738571167
Epoch 340, val loss: 0.9681286811828613
Epoch 350, training loss: 7.139512538909912 = 0.7227903604507446 + 1.0 * 6.416722297668457
Epoch 350, val loss: 0.9405553936958313
Epoch 360, training loss: 7.098931312561035 = 0.6857143640518188 + 1.0 * 6.413217067718506
Epoch 360, val loss: 0.9134863615036011
Epoch 370, training loss: 7.060537815093994 = 0.64955073595047 + 1.0 * 6.41098690032959
Epoch 370, val loss: 0.8872817158699036
Epoch 380, training loss: 7.018684387207031 = 0.6142131090164185 + 1.0 * 6.404471397399902
Epoch 380, val loss: 0.8623796105384827
Epoch 390, training loss: 6.983168125152588 = 0.5796718597412109 + 1.0 * 6.403496265411377
Epoch 390, val loss: 0.8387050628662109
Epoch 400, training loss: 6.947885990142822 = 0.5464426875114441 + 1.0 * 6.4014434814453125
Epoch 400, val loss: 0.8167862296104431
Epoch 410, training loss: 6.909884452819824 = 0.5145792961120605 + 1.0 * 6.395305156707764
Epoch 410, val loss: 0.7968363761901855
Epoch 420, training loss: 6.875729560852051 = 0.48392045497894287 + 1.0 * 6.391808986663818
Epoch 420, val loss: 0.7787001132965088
Epoch 430, training loss: 6.850071430206299 = 0.45439302921295166 + 1.0 * 6.395678520202637
Epoch 430, val loss: 0.7623503804206848
Epoch 440, training loss: 6.813232421875 = 0.4262722432613373 + 1.0 * 6.386960029602051
Epoch 440, val loss: 0.7477800250053406
Epoch 450, training loss: 6.783319473266602 = 0.39946553111076355 + 1.0 * 6.383853912353516
Epoch 450, val loss: 0.7349144220352173
Epoch 460, training loss: 6.754191875457764 = 0.3738267123699188 + 1.0 * 6.380365371704102
Epoch 460, val loss: 0.7234146595001221
Epoch 470, training loss: 6.729084014892578 = 0.3494298756122589 + 1.0 * 6.3796539306640625
Epoch 470, val loss: 0.713221549987793
Epoch 480, training loss: 6.7060370445251465 = 0.3263510465621948 + 1.0 * 6.379685878753662
Epoch 480, val loss: 0.7042690515518188
Epoch 490, training loss: 6.678084850311279 = 0.3045625686645508 + 1.0 * 6.3735222816467285
Epoch 490, val loss: 0.6964476108551025
Epoch 500, training loss: 6.661634922027588 = 0.2840318977832794 + 1.0 * 6.377603054046631
Epoch 500, val loss: 0.6897065043449402
Epoch 510, training loss: 6.634946346282959 = 0.2648049592971802 + 1.0 * 6.370141506195068
Epoch 510, val loss: 0.6839110255241394
Epoch 520, training loss: 6.618710994720459 = 0.24681542813777924 + 1.0 * 6.371895790100098
Epoch 520, val loss: 0.679114580154419
Epoch 530, training loss: 6.595061779022217 = 0.2300383448600769 + 1.0 * 6.365023612976074
Epoch 530, val loss: 0.6752910017967224
Epoch 540, training loss: 6.576464653015137 = 0.21443970501422882 + 1.0 * 6.362024784088135
Epoch 540, val loss: 0.6723760366439819
Epoch 550, training loss: 6.572309970855713 = 0.1999576985836029 + 1.0 * 6.372352123260498
Epoch 550, val loss: 0.6702667474746704
Epoch 560, training loss: 6.54644775390625 = 0.1866508424282074 + 1.0 * 6.35979700088501
Epoch 560, val loss: 0.6689453125
Epoch 570, training loss: 6.534367561340332 = 0.17440015077590942 + 1.0 * 6.359967231750488
Epoch 570, val loss: 0.6684082746505737
Epoch 580, training loss: 6.518678188323975 = 0.16310910880565643 + 1.0 * 6.355568885803223
Epoch 580, val loss: 0.6685536503791809
Epoch 590, training loss: 6.508320331573486 = 0.1527119129896164 + 1.0 * 6.3556084632873535
Epoch 590, val loss: 0.6693927645683289
Epoch 600, training loss: 6.494460105895996 = 0.1431291699409485 + 1.0 * 6.351330757141113
Epoch 600, val loss: 0.6707801818847656
Epoch 610, training loss: 6.4922943115234375 = 0.13427181541919708 + 1.0 * 6.358022689819336
Epoch 610, val loss: 0.672791600227356
Epoch 620, training loss: 6.476460933685303 = 0.12614065408706665 + 1.0 * 6.350320339202881
Epoch 620, val loss: 0.6751910448074341
Epoch 630, training loss: 6.465514659881592 = 0.11861380189657211 + 1.0 * 6.346900939941406
Epoch 630, val loss: 0.6781105399131775
Epoch 640, training loss: 6.4605841636657715 = 0.11161822080612183 + 1.0 * 6.348966121673584
Epoch 640, val loss: 0.681394100189209
Epoch 650, training loss: 6.455236911773682 = 0.10518337041139603 + 1.0 * 6.350053310394287
Epoch 650, val loss: 0.6849502325057983
Epoch 660, training loss: 6.440496444702148 = 0.0991973876953125 + 1.0 * 6.341299057006836
Epoch 660, val loss: 0.6888937950134277
Epoch 670, training loss: 6.434423923492432 = 0.0936257392168045 + 1.0 * 6.340798377990723
Epoch 670, val loss: 0.6930773258209229
Epoch 680, training loss: 6.436922073364258 = 0.0884467214345932 + 1.0 * 6.348475456237793
Epoch 680, val loss: 0.6974515914916992
Epoch 690, training loss: 6.421061038970947 = 0.08364828675985336 + 1.0 * 6.3374128341674805
Epoch 690, val loss: 0.7020444273948669
Epoch 700, training loss: 6.416277885437012 = 0.07917921990156174 + 1.0 * 6.337098598480225
Epoch 700, val loss: 0.706836998462677
Epoch 710, training loss: 6.412498950958252 = 0.07500497996807098 + 1.0 * 6.337493896484375
Epoch 710, val loss: 0.7117176055908203
Epoch 720, training loss: 6.411831378936768 = 0.07113342732191086 + 1.0 * 6.340697765350342
Epoch 720, val loss: 0.716747522354126
Epoch 730, training loss: 6.399989128112793 = 0.06754546612501144 + 1.0 * 6.332443714141846
Epoch 730, val loss: 0.7219111323356628
Epoch 740, training loss: 6.395277976989746 = 0.0641888976097107 + 1.0 * 6.331089019775391
Epoch 740, val loss: 0.7271590828895569
Epoch 750, training loss: 6.399989128112793 = 0.0610509067773819 + 1.0 * 6.338938236236572
Epoch 750, val loss: 0.7324754595756531
Epoch 760, training loss: 6.390621662139893 = 0.05813741311430931 + 1.0 * 6.332484245300293
Epoch 760, val loss: 0.7378104329109192
Epoch 770, training loss: 6.391351699829102 = 0.05541231483221054 + 1.0 * 6.335939407348633
Epoch 770, val loss: 0.7432780265808105
Epoch 780, training loss: 6.383533000946045 = 0.05286769941449165 + 1.0 * 6.330665111541748
Epoch 780, val loss: 0.7486119866371155
Epoch 790, training loss: 6.376811981201172 = 0.050494484603405 + 1.0 * 6.326317310333252
Epoch 790, val loss: 0.7540866732597351
Epoch 800, training loss: 6.372839450836182 = 0.0482604093849659 + 1.0 * 6.324579238891602
Epoch 800, val loss: 0.7595549821853638
Epoch 810, training loss: 6.377995491027832 = 0.046164240688085556 + 1.0 * 6.331831455230713
Epoch 810, val loss: 0.7649704217910767
Epoch 820, training loss: 6.369589328765869 = 0.044205013662576675 + 1.0 * 6.325384140014648
Epoch 820, val loss: 0.7703759074211121
Epoch 830, training loss: 6.363960266113281 = 0.04236597940325737 + 1.0 * 6.32159423828125
Epoch 830, val loss: 0.7757852077484131
Epoch 840, training loss: 6.370213985443115 = 0.04063904657959938 + 1.0 * 6.329575061798096
Epoch 840, val loss: 0.7811263203620911
Epoch 850, training loss: 6.360574722290039 = 0.0390188954770565 + 1.0 * 6.3215556144714355
Epoch 850, val loss: 0.786403238773346
Epoch 860, training loss: 6.3569536209106445 = 0.037491898983716965 + 1.0 * 6.319461822509766
Epoch 860, val loss: 0.7917160391807556
Epoch 870, training loss: 6.356807708740234 = 0.03605123236775398 + 1.0 * 6.320756435394287
Epoch 870, val loss: 0.7969663739204407
Epoch 880, training loss: 6.353446006774902 = 0.03468983620405197 + 1.0 * 6.318756103515625
Epoch 880, val loss: 0.8021458387374878
Epoch 890, training loss: 6.349557876586914 = 0.033404503017663956 + 1.0 * 6.316153526306152
Epoch 890, val loss: 0.8072994947433472
Epoch 900, training loss: 6.347756862640381 = 0.03218947723507881 + 1.0 * 6.315567493438721
Epoch 900, val loss: 0.8124380111694336
Epoch 910, training loss: 6.346694469451904 = 0.031038062646985054 + 1.0 * 6.3156561851501465
Epoch 910, val loss: 0.8175317645072937
Epoch 920, training loss: 6.347949028015137 = 0.029947107657790184 + 1.0 * 6.318001747131348
Epoch 920, val loss: 0.8224906921386719
Epoch 930, training loss: 6.347509860992432 = 0.028918057680130005 + 1.0 * 6.318591594696045
Epoch 930, val loss: 0.827422559261322
Epoch 940, training loss: 6.340766429901123 = 0.027945004403591156 + 1.0 * 6.312821388244629
Epoch 940, val loss: 0.8323257565498352
Epoch 950, training loss: 6.340495586395264 = 0.02702191099524498 + 1.0 * 6.313473701477051
Epoch 950, val loss: 0.8371995091438293
Epoch 960, training loss: 6.337851524353027 = 0.026143480092287064 + 1.0 * 6.311707973480225
Epoch 960, val loss: 0.8419449329376221
Epoch 970, training loss: 6.33571195602417 = 0.02530973218381405 + 1.0 * 6.3104023933410645
Epoch 970, val loss: 0.8467453718185425
Epoch 980, training loss: 6.334949016571045 = 0.02451189048588276 + 1.0 * 6.310437202453613
Epoch 980, val loss: 0.8514313101768494
Epoch 990, training loss: 6.333583831787109 = 0.023752862587571144 + 1.0 * 6.309831142425537
Epoch 990, val loss: 0.8559890985488892
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 10.540678977966309 = 1.9438130855560303 + 1.0 * 8.5968656539917
Epoch 0, val loss: 1.9423061609268188
Epoch 10, training loss: 10.530607223510742 = 1.9339098930358887 + 1.0 * 8.596697807312012
Epoch 10, val loss: 1.9322457313537598
Epoch 20, training loss: 10.516977310180664 = 1.9216712713241577 + 1.0 * 8.595306396484375
Epoch 20, val loss: 1.9199342727661133
Epoch 30, training loss: 10.48848819732666 = 1.904746174812317 + 1.0 * 8.583742141723633
Epoch 30, val loss: 1.902969241142273
Epoch 40, training loss: 10.391965866088867 = 1.8812626600265503 + 1.0 * 8.510703086853027
Epoch 40, val loss: 1.879995584487915
Epoch 50, training loss: 10.009552001953125 = 1.8538531064987183 + 1.0 * 8.155698776245117
Epoch 50, val loss: 1.8543559312820435
Epoch 60, training loss: 9.6066255569458 = 1.8286360502243042 + 1.0 * 7.777989864349365
Epoch 60, val loss: 1.832162618637085
Epoch 70, training loss: 9.170297622680664 = 1.811025619506836 + 1.0 * 7.35927152633667
Epoch 70, val loss: 1.816263198852539
Epoch 80, training loss: 8.893745422363281 = 1.796140193939209 + 1.0 * 7.097604751586914
Epoch 80, val loss: 1.8021217584609985
Epoch 90, training loss: 8.72840404510498 = 1.778239369392395 + 1.0 * 6.950164318084717
Epoch 90, val loss: 1.7864758968353271
Epoch 100, training loss: 8.626102447509766 = 1.7587149143218994 + 1.0 * 6.867387294769287
Epoch 100, val loss: 1.7699875831604004
Epoch 110, training loss: 8.542530059814453 = 1.7397152185440063 + 1.0 * 6.802814960479736
Epoch 110, val loss: 1.7530936002731323
Epoch 120, training loss: 8.4747896194458 = 1.719539999961853 + 1.0 * 6.755249977111816
Epoch 120, val loss: 1.734491229057312
Epoch 130, training loss: 8.413117408752441 = 1.6966181993484497 + 1.0 * 6.716498851776123
Epoch 130, val loss: 1.7135272026062012
Epoch 140, training loss: 8.349806785583496 = 1.6701730489730835 + 1.0 * 6.679634094238281
Epoch 140, val loss: 1.689934492111206
Epoch 150, training loss: 8.283160209655762 = 1.6395663022994995 + 1.0 * 6.643594264984131
Epoch 150, val loss: 1.662893295288086
Epoch 160, training loss: 8.218714714050293 = 1.6042912006378174 + 1.0 * 6.614423751831055
Epoch 160, val loss: 1.631956934928894
Epoch 170, training loss: 8.15463638305664 = 1.5639933347702026 + 1.0 * 6.590643405914307
Epoch 170, val loss: 1.5965765714645386
Epoch 180, training loss: 8.089324951171875 = 1.5186238288879395 + 1.0 * 6.5707011222839355
Epoch 180, val loss: 1.5569864511489868
Epoch 190, training loss: 8.021719932556152 = 1.468855857849121 + 1.0 * 6.552864074707031
Epoch 190, val loss: 1.513871431350708
Epoch 200, training loss: 7.956639289855957 = 1.416161298751831 + 1.0 * 6.540478229522705
Epoch 200, val loss: 1.4688125848770142
Epoch 210, training loss: 7.893061637878418 = 1.3639979362487793 + 1.0 * 6.529063701629639
Epoch 210, val loss: 1.4252026081085205
Epoch 220, training loss: 7.829066276550293 = 1.3132860660552979 + 1.0 * 6.515780448913574
Epoch 220, val loss: 1.3834002017974854
Epoch 230, training loss: 7.769467830657959 = 1.2639460563659668 + 1.0 * 6.505521774291992
Epoch 230, val loss: 1.343382716178894
Epoch 240, training loss: 7.713273048400879 = 1.2160093784332275 + 1.0 * 6.497263431549072
Epoch 240, val loss: 1.305309534072876
Epoch 250, training loss: 7.6594343185424805 = 1.1695654392242432 + 1.0 * 6.489869117736816
Epoch 250, val loss: 1.2691597938537598
Epoch 260, training loss: 7.6143035888671875 = 1.1249333620071411 + 1.0 * 6.489370346069336
Epoch 260, val loss: 1.2350093126296997
Epoch 270, training loss: 7.5596208572387695 = 1.0821068286895752 + 1.0 * 6.477513790130615
Epoch 270, val loss: 1.2027326822280884
Epoch 280, training loss: 7.510624885559082 = 1.0403083562850952 + 1.0 * 6.470316410064697
Epoch 280, val loss: 1.1715563535690308
Epoch 290, training loss: 7.469298362731934 = 0.9991844296455383 + 1.0 * 6.470113754272461
Epoch 290, val loss: 1.1411300897598267
Epoch 300, training loss: 7.420049667358398 = 0.9593127965927124 + 1.0 * 6.4607367515563965
Epoch 300, val loss: 1.1115649938583374
Epoch 310, training loss: 7.375622272491455 = 0.9205124974250793 + 1.0 * 6.455109596252441
Epoch 310, val loss: 1.0828453302383423
Epoch 320, training loss: 7.332601547241211 = 0.8825380802154541 + 1.0 * 6.450063705444336
Epoch 320, val loss: 1.0547614097595215
Epoch 330, training loss: 7.294064044952393 = 0.8459692001342773 + 1.0 * 6.448094844818115
Epoch 330, val loss: 1.0277433395385742
Epoch 340, training loss: 7.251152515411377 = 0.8109071850776672 + 1.0 * 6.440245151519775
Epoch 340, val loss: 1.0019832849502563
Epoch 350, training loss: 7.212015151977539 = 0.7768930196762085 + 1.0 * 6.435122013092041
Epoch 350, val loss: 0.9771884679794312
Epoch 360, training loss: 7.173003196716309 = 0.7437713742256165 + 1.0 * 6.429231643676758
Epoch 360, val loss: 0.9533464312553406
Epoch 370, training loss: 7.148380756378174 = 0.711581826210022 + 1.0 * 6.436799049377441
Epoch 370, val loss: 0.9305818676948547
Epoch 380, training loss: 7.103667259216309 = 0.6806716918945312 + 1.0 * 6.422995567321777
Epoch 380, val loss: 0.9092500805854797
Epoch 390, training loss: 7.068033218383789 = 0.6509182453155518 + 1.0 * 6.417115211486816
Epoch 390, val loss: 0.8893185257911682
Epoch 400, training loss: 7.040411472320557 = 0.6221908330917358 + 1.0 * 6.418220520019531
Epoch 400, val loss: 0.8706969618797302
Epoch 410, training loss: 7.004531383514404 = 0.5946882367134094 + 1.0 * 6.4098429679870605
Epoch 410, val loss: 0.8535555005073547
Epoch 420, training loss: 6.973326683044434 = 0.5680944323539734 + 1.0 * 6.4052324295043945
Epoch 420, val loss: 0.8377164602279663
Epoch 430, training loss: 6.943095684051514 = 0.542305588722229 + 1.0 * 6.400790214538574
Epoch 430, val loss: 0.8230341672897339
Epoch 440, training loss: 6.915193557739258 = 0.5173198580741882 + 1.0 * 6.397873878479004
Epoch 440, val loss: 0.8094841241836548
Epoch 450, training loss: 6.893576622009277 = 0.49299201369285583 + 1.0 * 6.400584697723389
Epoch 450, val loss: 0.7968757748603821
Epoch 460, training loss: 6.862971305847168 = 0.46932917833328247 + 1.0 * 6.393641948699951
Epoch 460, val loss: 0.7852271795272827
Epoch 470, training loss: 6.8360137939453125 = 0.44617655873298645 + 1.0 * 6.389837265014648
Epoch 470, val loss: 0.77439945936203
Epoch 480, training loss: 6.8216729164123535 = 0.42345598340034485 + 1.0 * 6.398216724395752
Epoch 480, val loss: 0.7642903923988342
Epoch 490, training loss: 6.787445068359375 = 0.4014833867549896 + 1.0 * 6.385961532592773
Epoch 490, val loss: 0.755094051361084
Epoch 500, training loss: 6.762364864349365 = 0.38007184863090515 + 1.0 * 6.382293224334717
Epoch 500, val loss: 0.7467185258865356
Epoch 510, training loss: 6.738911151885986 = 0.3593038022518158 + 1.0 * 6.379607200622559
Epoch 510, val loss: 0.739168107509613
Epoch 520, training loss: 6.719864368438721 = 0.33929648995399475 + 1.0 * 6.380568027496338
Epoch 520, val loss: 0.7325164675712585
Epoch 530, training loss: 6.701026916503906 = 0.32023173570632935 + 1.0 * 6.380795001983643
Epoch 530, val loss: 0.7267987132072449
Epoch 540, training loss: 6.677377223968506 = 0.30211639404296875 + 1.0 * 6.375260829925537
Epoch 540, val loss: 0.721937358379364
Epoch 550, training loss: 6.656809329986572 = 0.28483134508132935 + 1.0 * 6.371977806091309
Epoch 550, val loss: 0.7178449630737305
Epoch 560, training loss: 6.645965576171875 = 0.26838046312332153 + 1.0 * 6.377584934234619
Epoch 560, val loss: 0.7143826484680176
Epoch 570, training loss: 6.62577486038208 = 0.2528362274169922 + 1.0 * 6.372938632965088
Epoch 570, val loss: 0.7115746140480042
Epoch 580, training loss: 6.605197906494141 = 0.2380421906709671 + 1.0 * 6.3671555519104
Epoch 580, val loss: 0.7093383073806763
Epoch 590, training loss: 6.594541072845459 = 0.22392362356185913 + 1.0 * 6.370617389678955
Epoch 590, val loss: 0.7076143622398376
Epoch 600, training loss: 6.577386379241943 = 0.2105599343776703 + 1.0 * 6.36682653427124
Epoch 600, val loss: 0.7063724994659424
Epoch 610, training loss: 6.559366703033447 = 0.19789887964725494 + 1.0 * 6.3614678382873535
Epoch 610, val loss: 0.7055783867835999
Epoch 620, training loss: 6.557482719421387 = 0.1859075278043747 + 1.0 * 6.371575355529785
Epoch 620, val loss: 0.7052015066146851
Epoch 630, training loss: 6.533712387084961 = 0.17467883229255676 + 1.0 * 6.359033584594727
Epoch 630, val loss: 0.7052369713783264
Epoch 640, training loss: 6.520913124084473 = 0.16413310170173645 + 1.0 * 6.356780052185059
Epoch 640, val loss: 0.7056879997253418
Epoch 650, training loss: 6.50889253616333 = 0.15425358712673187 + 1.0 * 6.354639053344727
Epoch 650, val loss: 0.7065358757972717
Epoch 660, training loss: 6.508815765380859 = 0.14502345025539398 + 1.0 * 6.363792419433594
Epoch 660, val loss: 0.7077315449714661
Epoch 670, training loss: 6.489536762237549 = 0.13648715615272522 + 1.0 * 6.3530497550964355
Epoch 670, val loss: 0.7092813849449158
Epoch 680, training loss: 6.478375434875488 = 0.12855568528175354 + 1.0 * 6.349819660186768
Epoch 680, val loss: 0.711151659488678
Epoch 690, training loss: 6.4701409339904785 = 0.12117427587509155 + 1.0 * 6.348966598510742
Epoch 690, val loss: 0.7133216857910156
Epoch 700, training loss: 6.4698662757873535 = 0.11433790624141693 + 1.0 * 6.355528354644775
Epoch 700, val loss: 0.7157552242279053
Epoch 710, training loss: 6.456214904785156 = 0.10801348090171814 + 1.0 * 6.348201274871826
Epoch 710, val loss: 0.7183436155319214
Epoch 720, training loss: 6.4476542472839355 = 0.10216342657804489 + 1.0 * 6.3454909324646
Epoch 720, val loss: 0.7211607694625854
Epoch 730, training loss: 6.438626766204834 = 0.09672276675701141 + 1.0 * 6.341904163360596
Epoch 730, val loss: 0.7242222428321838
Epoch 740, training loss: 6.442236423492432 = 0.09164481610059738 + 1.0 * 6.350591659545898
Epoch 740, val loss: 0.7274535894393921
Epoch 750, training loss: 6.43040657043457 = 0.08694669604301453 + 1.0 * 6.3434600830078125
Epoch 750, val loss: 0.7308283448219299
Epoch 760, training loss: 6.421984672546387 = 0.08255329728126526 + 1.0 * 6.339431285858154
Epoch 760, val loss: 0.7342860102653503
Epoch 770, training loss: 6.416688919067383 = 0.07845333963632584 + 1.0 * 6.338235378265381
Epoch 770, val loss: 0.7379258871078491
Epoch 780, training loss: 6.410382270812988 = 0.07461220771074295 + 1.0 * 6.335770130157471
Epoch 780, val loss: 0.741642415523529
Epoch 790, training loss: 6.407203197479248 = 0.07103512436151505 + 1.0 * 6.33616828918457
Epoch 790, val loss: 0.7454058527946472
Epoch 800, training loss: 6.405664920806885 = 0.06767792254686356 + 1.0 * 6.337986946105957
Epoch 800, val loss: 0.7492567896842957
Epoch 810, training loss: 6.395538806915283 = 0.06453317403793335 + 1.0 * 6.331005573272705
Epoch 810, val loss: 0.7531346082687378
Epoch 820, training loss: 6.393789291381836 = 0.061578597873449326 + 1.0 * 6.332210540771484
Epoch 820, val loss: 0.7570602297782898
Epoch 830, training loss: 6.3927226066589355 = 0.058800164610147476 + 1.0 * 6.333922386169434
Epoch 830, val loss: 0.7610352635383606
Epoch 840, training loss: 6.3858561515808105 = 0.056205637753009796 + 1.0 * 6.329650402069092
Epoch 840, val loss: 0.7650004625320435
Epoch 850, training loss: 6.380407810211182 = 0.05375576391816139 + 1.0 * 6.3266520500183105
Epoch 850, val loss: 0.7690073251724243
Epoch 860, training loss: 6.379894256591797 = 0.05145067721605301 + 1.0 * 6.32844352722168
Epoch 860, val loss: 0.7730280160903931
Epoch 870, training loss: 6.379174709320068 = 0.04927555099129677 + 1.0 * 6.329899311065674
Epoch 870, val loss: 0.7770565748214722
Epoch 880, training loss: 6.374226093292236 = 0.047234006226062775 + 1.0 * 6.326992034912109
Epoch 880, val loss: 0.7810693979263306
Epoch 890, training loss: 6.371275901794434 = 0.04530904442071915 + 1.0 * 6.325966835021973
Epoch 890, val loss: 0.7850218415260315
Epoch 900, training loss: 6.364317893981934 = 0.04349159076809883 + 1.0 * 6.320826530456543
Epoch 900, val loss: 0.7889658808708191
Epoch 910, training loss: 6.363038539886475 = 0.041775722056627274 + 1.0 * 6.321262836456299
Epoch 910, val loss: 0.792938232421875
Epoch 920, training loss: 6.365498065948486 = 0.04014851525425911 + 1.0 * 6.3253493309021
Epoch 920, val loss: 0.7968910336494446
Epoch 930, training loss: 6.359051704406738 = 0.03861954063177109 + 1.0 * 6.320432186126709
Epoch 930, val loss: 0.8008466362953186
Epoch 940, training loss: 6.35361385345459 = 0.0371674969792366 + 1.0 * 6.316446304321289
Epoch 940, val loss: 0.8047692179679871
Epoch 950, training loss: 6.3528900146484375 = 0.035792719572782516 + 1.0 * 6.317097187042236
Epoch 950, val loss: 0.8087037801742554
Epoch 960, training loss: 6.354661464691162 = 0.03448937460780144 + 1.0 * 6.320172309875488
Epoch 960, val loss: 0.8126463294029236
Epoch 970, training loss: 6.348678112030029 = 0.03325781226158142 + 1.0 * 6.315420150756836
Epoch 970, val loss: 0.8164334297180176
Epoch 980, training loss: 6.345296382904053 = 0.03208860382437706 + 1.0 * 6.313207626342773
Epoch 980, val loss: 0.8202874064445496
Epoch 990, training loss: 6.344136714935303 = 0.0309767909348011 + 1.0 * 6.313159942626953
Epoch 990, val loss: 0.8240699768066406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8434370057986295
The final CL Acc:0.80123, 0.00698, The final GNN Acc:0.83957, 0.00376
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10478])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.553617477416992 = 1.9568078517913818 + 1.0 * 8.596809387207031
Epoch 0, val loss: 1.955911636352539
Epoch 10, training loss: 10.542320251464844 = 1.9458791017532349 + 1.0 * 8.596441268920898
Epoch 10, val loss: 1.9446792602539062
Epoch 20, training loss: 10.52580451965332 = 1.9322329759597778 + 1.0 * 8.593571662902832
Epoch 20, val loss: 1.9305193424224854
Epoch 30, training loss: 10.486428260803223 = 1.9132213592529297 + 1.0 * 8.573206901550293
Epoch 30, val loss: 1.9106967449188232
Epoch 40, training loss: 10.347192764282227 = 1.8888869285583496 + 1.0 * 8.458306312561035
Epoch 40, val loss: 1.8864197731018066
Epoch 50, training loss: 9.989460945129395 = 1.862152099609375 + 1.0 * 8.12730884552002
Epoch 50, val loss: 1.860352873802185
Epoch 60, training loss: 9.645820617675781 = 1.8395347595214844 + 1.0 * 7.806285381317139
Epoch 60, val loss: 1.8397012948989868
Epoch 70, training loss: 9.1021728515625 = 1.8249683380126953 + 1.0 * 7.277204990386963
Epoch 70, val loss: 1.8265832662582397
Epoch 80, training loss: 8.795848846435547 = 1.814892292022705 + 1.0 * 6.980956077575684
Epoch 80, val loss: 1.8171920776367188
Epoch 90, training loss: 8.661782264709473 = 1.801919937133789 + 1.0 * 6.859862327575684
Epoch 90, val loss: 1.8041305541992188
Epoch 100, training loss: 8.56683349609375 = 1.7872662544250488 + 1.0 * 6.779567241668701
Epoch 100, val loss: 1.7897111177444458
Epoch 110, training loss: 8.490703582763672 = 1.7740002870559692 + 1.0 * 6.716702938079834
Epoch 110, val loss: 1.776380181312561
Epoch 120, training loss: 8.428335189819336 = 1.7616605758666992 + 1.0 * 6.666674613952637
Epoch 120, val loss: 1.7638682126998901
Epoch 130, training loss: 8.374692916870117 = 1.7484153509140015 + 1.0 * 6.626277923583984
Epoch 130, val loss: 1.7509492635726929
Epoch 140, training loss: 8.327658653259277 = 1.733046054840088 + 1.0 * 6.5946125984191895
Epoch 140, val loss: 1.7364435195922852
Epoch 150, training loss: 8.283342361450195 = 1.7151256799697876 + 1.0 * 6.568216323852539
Epoch 150, val loss: 1.7199618816375732
Epoch 160, training loss: 8.24023723602295 = 1.6942689418792725 + 1.0 * 6.545968055725098
Epoch 160, val loss: 1.7010719776153564
Epoch 170, training loss: 8.197369575500488 = 1.6700085401535034 + 1.0 * 6.527360916137695
Epoch 170, val loss: 1.6792596578598022
Epoch 180, training loss: 8.154318809509277 = 1.6419588327407837 + 1.0 * 6.512359619140625
Epoch 180, val loss: 1.6540929079055786
Epoch 190, training loss: 8.106489181518555 = 1.6093666553497314 + 1.0 * 6.497122287750244
Epoch 190, val loss: 1.6249967813491821
Epoch 200, training loss: 8.05831527709961 = 1.5718194246292114 + 1.0 * 6.486495494842529
Epoch 200, val loss: 1.5916976928710938
Epoch 210, training loss: 8.006175994873047 = 1.5302256345748901 + 1.0 * 6.475950717926025
Epoch 210, val loss: 1.5555088520050049
Epoch 220, training loss: 7.949190139770508 = 1.485294222831726 + 1.0 * 6.463895797729492
Epoch 220, val loss: 1.5170083045959473
Epoch 230, training loss: 7.891483306884766 = 1.4373196363449097 + 1.0 * 6.454163551330566
Epoch 230, val loss: 1.4764806032180786
Epoch 240, training loss: 7.839903831481934 = 1.3872475624084473 + 1.0 * 6.452656269073486
Epoch 240, val loss: 1.4347906112670898
Epoch 250, training loss: 7.782602787017822 = 1.3382092714309692 + 1.0 * 6.444393634796143
Epoch 250, val loss: 1.3949108123779297
Epoch 260, training loss: 7.724786758422852 = 1.2904285192489624 + 1.0 * 6.4343581199646
Epoch 260, val loss: 1.3565794229507446
Epoch 270, training loss: 7.671394348144531 = 1.2436628341674805 + 1.0 * 6.427731513977051
Epoch 270, val loss: 1.3194513320922852
Epoch 280, training loss: 7.620120048522949 = 1.1977733373641968 + 1.0 * 6.422346591949463
Epoch 280, val loss: 1.2833349704742432
Epoch 290, training loss: 7.571938514709473 = 1.1529369354248047 + 1.0 * 6.419001579284668
Epoch 290, val loss: 1.248328447341919
Epoch 300, training loss: 7.52260160446167 = 1.1093859672546387 + 1.0 * 6.413215637207031
Epoch 300, val loss: 1.2145415544509888
Epoch 310, training loss: 7.475657939910889 = 1.0663948059082031 + 1.0 * 6.4092631340026855
Epoch 310, val loss: 1.1811920404434204
Epoch 320, training loss: 7.433176040649414 = 1.0239157676696777 + 1.0 * 6.409260272979736
Epoch 320, val loss: 1.1482467651367188
Epoch 330, training loss: 7.385025978088379 = 0.9823833107948303 + 1.0 * 6.402642726898193
Epoch 330, val loss: 1.1161534786224365
Epoch 340, training loss: 7.339391231536865 = 0.9415979385375977 + 1.0 * 6.397793292999268
Epoch 340, val loss: 1.0846449136734009
Epoch 350, training loss: 7.296141147613525 = 0.9016245603561401 + 1.0 * 6.394516468048096
Epoch 350, val loss: 1.0538067817687988
Epoch 360, training loss: 7.257518291473389 = 0.8630685806274414 + 1.0 * 6.394449710845947
Epoch 360, val loss: 1.0242995023727417
Epoch 370, training loss: 7.215819358825684 = 0.8263412117958069 + 1.0 * 6.3894782066345215
Epoch 370, val loss: 0.9965808987617493
Epoch 380, training loss: 7.176309108734131 = 0.7913170456886292 + 1.0 * 6.3849921226501465
Epoch 380, val loss: 0.9705055356025696
Epoch 390, training loss: 7.146347999572754 = 0.7579659223556519 + 1.0 * 6.3883819580078125
Epoch 390, val loss: 0.9461974501609802
Epoch 400, training loss: 7.105817794799805 = 0.7264926433563232 + 1.0 * 6.3793253898620605
Epoch 400, val loss: 0.9239404797554016
Epoch 410, training loss: 7.072334289550781 = 0.6965378522872925 + 1.0 * 6.375796318054199
Epoch 410, val loss: 0.9034233093261719
Epoch 420, training loss: 7.039787292480469 = 0.6675773859024048 + 1.0 * 6.3722100257873535
Epoch 420, val loss: 0.8843116760253906
Epoch 430, training loss: 7.01403284072876 = 0.6393351554870605 + 1.0 * 6.374697685241699
Epoch 430, val loss: 0.8664785027503967
Epoch 440, training loss: 6.981687068939209 = 0.6119437217712402 + 1.0 * 6.369743347167969
Epoch 440, val loss: 0.8499715328216553
Epoch 450, training loss: 6.952402114868164 = 0.5852276682853699 + 1.0 * 6.3671746253967285
Epoch 450, val loss: 0.834761381149292
Epoch 460, training loss: 6.921052932739258 = 0.5587493777275085 + 1.0 * 6.362303733825684
Epoch 460, val loss: 0.8203169703483582
Epoch 470, training loss: 6.897821426391602 = 0.5323164463043213 + 1.0 * 6.365505218505859
Epoch 470, val loss: 0.8065248727798462
Epoch 480, training loss: 6.868178367614746 = 0.5060223340988159 + 1.0 * 6.362155914306641
Epoch 480, val loss: 0.7934641242027283
Epoch 490, training loss: 6.837228298187256 = 0.480141282081604 + 1.0 * 6.357087135314941
Epoch 490, val loss: 0.7810742259025574
Epoch 500, training loss: 6.808836936950684 = 0.45459654927253723 + 1.0 * 6.354240417480469
Epoch 500, val loss: 0.7695149779319763
Epoch 510, training loss: 6.781436443328857 = 0.42936480045318604 + 1.0 * 6.352071762084961
Epoch 510, val loss: 0.7586444616317749
Epoch 520, training loss: 6.765506267547607 = 0.40457862615585327 + 1.0 * 6.360927581787109
Epoch 520, val loss: 0.7484357953071594
Epoch 530, training loss: 6.730897426605225 = 0.380582720041275 + 1.0 * 6.350314617156982
Epoch 530, val loss: 0.739159107208252
Epoch 540, training loss: 6.704765796661377 = 0.3572796583175659 + 1.0 * 6.3474860191345215
Epoch 540, val loss: 0.7307682633399963
Epoch 550, training loss: 6.6957783699035645 = 0.3348456919193268 + 1.0 * 6.36093282699585
Epoch 550, val loss: 0.7231011390686035
Epoch 560, training loss: 6.662245750427246 = 0.31358206272125244 + 1.0 * 6.348663806915283
Epoch 560, val loss: 0.716488778591156
Epoch 570, training loss: 6.636662006378174 = 0.2933926284313202 + 1.0 * 6.343269348144531
Epoch 570, val loss: 0.710938036441803
Epoch 580, training loss: 6.619583606719971 = 0.2742491662502289 + 1.0 * 6.345334529876709
Epoch 580, val loss: 0.7061870694160461
Epoch 590, training loss: 6.603694438934326 = 0.2563927471637726 + 1.0 * 6.347301483154297
Epoch 590, val loss: 0.702340304851532
Epoch 600, training loss: 6.580348491668701 = 0.2397160828113556 + 1.0 * 6.340632438659668
Epoch 600, val loss: 0.6994198560714722
Epoch 610, training loss: 6.562110424041748 = 0.2242198884487152 + 1.0 * 6.337890625
Epoch 610, val loss: 0.6973037123680115
Epoch 620, training loss: 6.5466694831848145 = 0.20984357595443726 + 1.0 * 6.336825847625732
Epoch 620, val loss: 0.6958784461021423
Epoch 630, training loss: 6.533297061920166 = 0.1965259462594986 + 1.0 * 6.336771011352539
Epoch 630, val loss: 0.6951566934585571
Epoch 640, training loss: 6.516112327575684 = 0.18423579633235931 + 1.0 * 6.331876754760742
Epoch 640, val loss: 0.6950194239616394
Epoch 650, training loss: 6.507065773010254 = 0.17285485565662384 + 1.0 * 6.3342108726501465
Epoch 650, val loss: 0.6955013275146484
Epoch 660, training loss: 6.497483253479004 = 0.1623363494873047 + 1.0 * 6.335146903991699
Epoch 660, val loss: 0.6962982416152954
Epoch 670, training loss: 6.480949401855469 = 0.15265266597270966 + 1.0 * 6.328296661376953
Epoch 670, val loss: 0.6976574659347534
Epoch 680, training loss: 6.470479488372803 = 0.14364412426948547 + 1.0 * 6.3268351554870605
Epoch 680, val loss: 0.6993563771247864
Epoch 690, training loss: 6.477477073669434 = 0.13527929782867432 + 1.0 * 6.342197895050049
Epoch 690, val loss: 0.7012466788291931
Epoch 700, training loss: 6.453344821929932 = 0.1275387704372406 + 1.0 * 6.325806140899658
Epoch 700, val loss: 0.7033838629722595
Epoch 710, training loss: 6.446274280548096 = 0.12035777419805527 + 1.0 * 6.325916290283203
Epoch 710, val loss: 0.7059442400932312
Epoch 720, training loss: 6.436239242553711 = 0.11366064101457596 + 1.0 * 6.322578430175781
Epoch 720, val loss: 0.7085817456245422
Epoch 730, training loss: 6.437053203582764 = 0.10739967972040176 + 1.0 * 6.329653739929199
Epoch 730, val loss: 0.711353063583374
Epoch 740, training loss: 6.425511360168457 = 0.10157544910907745 + 1.0 * 6.3239359855651855
Epoch 740, val loss: 0.7143997550010681
Epoch 750, training loss: 6.419102668762207 = 0.09612049162387848 + 1.0 * 6.322982311248779
Epoch 750, val loss: 0.7175542116165161
Epoch 760, training loss: 6.413880825042725 = 0.09102793037891388 + 1.0 * 6.322853088378906
Epoch 760, val loss: 0.7207747101783752
Epoch 770, training loss: 6.404079914093018 = 0.08627869188785553 + 1.0 * 6.317800998687744
Epoch 770, val loss: 0.7241784930229187
Epoch 780, training loss: 6.3973588943481445 = 0.08180955797433853 + 1.0 * 6.315549373626709
Epoch 780, val loss: 0.727691113948822
Epoch 790, training loss: 6.395029067993164 = 0.07761605083942413 + 1.0 * 6.317412853240967
Epoch 790, val loss: 0.7312463521957397
Epoch 800, training loss: 6.389901638031006 = 0.07368750125169754 + 1.0 * 6.316214084625244
Epoch 800, val loss: 0.7347390651702881
Epoch 810, training loss: 6.39079475402832 = 0.07001489400863647 + 1.0 * 6.320779800415039
Epoch 810, val loss: 0.7384347915649414
Epoch 820, training loss: 6.378755569458008 = 0.06658878922462463 + 1.0 * 6.312166690826416
Epoch 820, val loss: 0.7422361373901367
Epoch 830, training loss: 6.3753437995910645 = 0.06336093693971634 + 1.0 * 6.31198263168335
Epoch 830, val loss: 0.7461168169975281
Epoch 840, training loss: 6.376613140106201 = 0.06032333895564079 + 1.0 * 6.316289901733398
Epoch 840, val loss: 0.7498340606689453
Epoch 850, training loss: 6.368185520172119 = 0.05748451501131058 + 1.0 * 6.3107008934021
Epoch 850, val loss: 0.7537835836410522
Epoch 860, training loss: 6.3670525550842285 = 0.05481221526861191 + 1.0 * 6.312240123748779
Epoch 860, val loss: 0.7577146887779236
Epoch 870, training loss: 6.359105587005615 = 0.052307236939668655 + 1.0 * 6.306798458099365
Epoch 870, val loss: 0.7615858316421509
Epoch 880, training loss: 6.3584184646606445 = 0.04995109885931015 + 1.0 * 6.308467388153076
Epoch 880, val loss: 0.7656497359275818
Epoch 890, training loss: 6.355844497680664 = 0.04773446545004845 + 1.0 * 6.308110237121582
Epoch 890, val loss: 0.7694768905639648
Epoch 900, training loss: 6.3515119552612305 = 0.045657627284526825 + 1.0 * 6.305854320526123
Epoch 900, val loss: 0.7735422849655151
Epoch 910, training loss: 6.351665496826172 = 0.043700698763132095 + 1.0 * 6.30796480178833
Epoch 910, val loss: 0.7775284647941589
Epoch 920, training loss: 6.34720516204834 = 0.04185432568192482 + 1.0 * 6.3053507804870605
Epoch 920, val loss: 0.781369149684906
Epoch 930, training loss: 6.345530986785889 = 0.040126629173755646 + 1.0 * 6.305404186248779
Epoch 930, val loss: 0.7854111194610596
Epoch 940, training loss: 6.340402603149414 = 0.03849637508392334 + 1.0 * 6.301906108856201
Epoch 940, val loss: 0.7891795635223389
Epoch 950, training loss: 6.337431907653809 = 0.03696679323911667 + 1.0 * 6.300465106964111
Epoch 950, val loss: 0.7931968569755554
Epoch 960, training loss: 6.337765693664551 = 0.03551642969250679 + 1.0 * 6.302249431610107
Epoch 960, val loss: 0.7971830368041992
Epoch 970, training loss: 6.332680702209473 = 0.03414774686098099 + 1.0 * 6.298532962799072
Epoch 970, val loss: 0.800774335861206
Epoch 980, training loss: 6.335159778594971 = 0.032861314713954926 + 1.0 * 6.302298545837402
Epoch 980, val loss: 0.8046720623970032
Epoch 990, training loss: 6.330709457397461 = 0.031646255403757095 + 1.0 * 6.299063205718994
Epoch 990, val loss: 0.8084038496017456
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 10.54076099395752 = 1.9439523220062256 + 1.0 * 8.596808433532715
Epoch 0, val loss: 1.945688247680664
Epoch 10, training loss: 10.530920028686523 = 1.9344487190246582 + 1.0 * 8.596471786499023
Epoch 10, val loss: 1.935713291168213
Epoch 20, training loss: 10.516866683959961 = 1.922958493232727 + 1.0 * 8.593908309936523
Epoch 20, val loss: 1.923467993736267
Epoch 30, training loss: 10.48110580444336 = 1.9070839881896973 + 1.0 * 8.574021339416504
Epoch 30, val loss: 1.9066613912582397
Epoch 40, training loss: 10.331816673278809 = 1.886486291885376 + 1.0 * 8.445330619812012
Epoch 40, val loss: 1.885764718055725
Epoch 50, training loss: 9.893020629882812 = 1.8633919954299927 + 1.0 * 8.02962875366211
Epoch 50, val loss: 1.8629939556121826
Epoch 60, training loss: 9.428667068481445 = 1.8442883491516113 + 1.0 * 7.584378242492676
Epoch 60, val loss: 1.8449112176895142
Epoch 70, training loss: 9.049664497375488 = 1.8306225538253784 + 1.0 * 7.21904182434082
Epoch 70, val loss: 1.8322646617889404
Epoch 80, training loss: 8.866496086120605 = 1.8184691667556763 + 1.0 * 7.0480265617370605
Epoch 80, val loss: 1.8207557201385498
Epoch 90, training loss: 8.705076217651367 = 1.8053042888641357 + 1.0 * 6.8997721672058105
Epoch 90, val loss: 1.8082387447357178
Epoch 100, training loss: 8.582578659057617 = 1.793785810470581 + 1.0 * 6.788792610168457
Epoch 100, val loss: 1.7972025871276855
Epoch 110, training loss: 8.498978614807129 = 1.7834014892578125 + 1.0 * 6.715577125549316
Epoch 110, val loss: 1.787019968032837
Epoch 120, training loss: 8.437201499938965 = 1.772261619567871 + 1.0 * 6.664939880371094
Epoch 120, val loss: 1.7763222455978394
Epoch 130, training loss: 8.3873872756958 = 1.7601382732391357 + 1.0 * 6.627248764038086
Epoch 130, val loss: 1.7650671005249023
Epoch 140, training loss: 8.343822479248047 = 1.7468959093093872 + 1.0 * 6.596926212310791
Epoch 140, val loss: 1.753288745880127
Epoch 150, training loss: 8.301383018493652 = 1.7320261001586914 + 1.0 * 6.569356918334961
Epoch 150, val loss: 1.7405143976211548
Epoch 160, training loss: 8.2616548538208 = 1.7150059938430786 + 1.0 * 6.5466485023498535
Epoch 160, val loss: 1.7262009382247925
Epoch 170, training loss: 8.220685005187988 = 1.6953771114349365 + 1.0 * 6.525308132171631
Epoch 170, val loss: 1.709891676902771
Epoch 180, training loss: 8.179962158203125 = 1.6723216772079468 + 1.0 * 6.507640838623047
Epoch 180, val loss: 1.6908295154571533
Epoch 190, training loss: 8.139358520507812 = 1.6450287103652954 + 1.0 * 6.494329452514648
Epoch 190, val loss: 1.6682868003845215
Epoch 200, training loss: 8.095198631286621 = 1.6131001710891724 + 1.0 * 6.482098579406738
Epoch 200, val loss: 1.6419459581375122
Epoch 210, training loss: 8.049994468688965 = 1.5758339166641235 + 1.0 * 6.474160671234131
Epoch 210, val loss: 1.6111747026443481
Epoch 220, training loss: 7.996670722961426 = 1.533103108406067 + 1.0 * 6.463567733764648
Epoch 220, val loss: 1.5759273767471313
Epoch 230, training loss: 7.9404826164245605 = 1.4847946166992188 + 1.0 * 6.455687999725342
Epoch 230, val loss: 1.5362836122512817
Epoch 240, training loss: 7.880208969116211 = 1.4313652515411377 + 1.0 * 6.448843479156494
Epoch 240, val loss: 1.4929348230361938
Epoch 250, training loss: 7.820338726043701 = 1.3744697570800781 + 1.0 * 6.445868968963623
Epoch 250, val loss: 1.4473143815994263
Epoch 260, training loss: 7.75745153427124 = 1.3170303106307983 + 1.0 * 6.440421104431152
Epoch 260, val loss: 1.4016144275665283
Epoch 270, training loss: 7.692569255828857 = 1.259334921836853 + 1.0 * 6.433234214782715
Epoch 270, val loss: 1.356135606765747
Epoch 280, training loss: 7.630580425262451 = 1.2024755477905273 + 1.0 * 6.428104877471924
Epoch 280, val loss: 1.3116923570632935
Epoch 290, training loss: 7.574548244476318 = 1.1484274864196777 + 1.0 * 6.426120758056641
Epoch 290, val loss: 1.2697827816009521
Epoch 300, training loss: 7.516384601593018 = 1.0977681875228882 + 1.0 * 6.41861629486084
Epoch 300, val loss: 1.231257677078247
Epoch 310, training loss: 7.469189643859863 = 1.0504379272460938 + 1.0 * 6.4187517166137695
Epoch 310, val loss: 1.1957095861434937
Epoch 320, training loss: 7.415952682495117 = 1.0063589811325073 + 1.0 * 6.40959358215332
Epoch 320, val loss: 1.1635112762451172
Epoch 330, training loss: 7.370060920715332 = 0.9647819399833679 + 1.0 * 6.405279159545898
Epoch 330, val loss: 1.1339695453643799
Epoch 340, training loss: 7.331264495849609 = 0.9253663420677185 + 1.0 * 6.405898094177246
Epoch 340, val loss: 1.106756567955017
Epoch 350, training loss: 7.288027763366699 = 0.8878677487373352 + 1.0 * 6.40015983581543
Epoch 350, val loss: 1.0817009210586548
Epoch 360, training loss: 7.246668338775635 = 0.851313591003418 + 1.0 * 6.395354747772217
Epoch 360, val loss: 1.0579537153244019
Epoch 370, training loss: 7.206839561462402 = 0.8152828812599182 + 1.0 * 6.391556739807129
Epoch 370, val loss: 1.0350624322891235
Epoch 380, training loss: 7.184737205505371 = 0.7796867489814758 + 1.0 * 6.405050277709961
Epoch 380, val loss: 1.013055682182312
Epoch 390, training loss: 7.136087417602539 = 0.7455171346664429 + 1.0 * 6.390570163726807
Epoch 390, val loss: 0.9926071763038635
Epoch 400, training loss: 7.095865726470947 = 0.712508499622345 + 1.0 * 6.383357048034668
Epoch 400, val loss: 0.9735657572746277
Epoch 410, training loss: 7.067413330078125 = 0.6805706024169922 + 1.0 * 6.386842727661133
Epoch 410, val loss: 0.9558164477348328
Epoch 420, training loss: 7.028957843780518 = 0.6499896049499512 + 1.0 * 6.378968238830566
Epoch 420, val loss: 0.9398893117904663
Epoch 430, training loss: 6.999361038208008 = 0.620924711227417 + 1.0 * 6.378436088562012
Epoch 430, val loss: 0.9257015585899353
Epoch 440, training loss: 6.96977424621582 = 0.5934796333312988 + 1.0 * 6.3762946128845215
Epoch 440, val loss: 0.9132606983184814
Epoch 450, training loss: 6.938770771026611 = 0.5675715804100037 + 1.0 * 6.371199131011963
Epoch 450, val loss: 0.9025975465774536
Epoch 460, training loss: 6.916565418243408 = 0.5429162979125977 + 1.0 * 6.3736491203308105
Epoch 460, val loss: 0.8932226300239563
Epoch 470, training loss: 6.888362407684326 = 0.5196561813354492 + 1.0 * 6.368706226348877
Epoch 470, val loss: 0.8851889371871948
Epoch 480, training loss: 6.861147403717041 = 0.4974295198917389 + 1.0 * 6.363718032836914
Epoch 480, val loss: 0.8784500956535339
Epoch 490, training loss: 6.838961124420166 = 0.4759783148765564 + 1.0 * 6.362982749938965
Epoch 490, val loss: 0.8725181818008423
Epoch 500, training loss: 6.8207573890686035 = 0.4552792012691498 + 1.0 * 6.365478038787842
Epoch 500, val loss: 0.8673449754714966
Epoch 510, training loss: 6.793259620666504 = 0.4353676736354828 + 1.0 * 6.357892036437988
Epoch 510, val loss: 0.8630427122116089
Epoch 520, training loss: 6.770595073699951 = 0.41596734523773193 + 1.0 * 6.35462760925293
Epoch 520, val loss: 0.859325647354126
Epoch 530, training loss: 6.750400543212891 = 0.397014856338501 + 1.0 * 6.3533854484558105
Epoch 530, val loss: 0.856199324131012
Epoch 540, training loss: 6.73128080368042 = 0.3786294162273407 + 1.0 * 6.352651596069336
Epoch 540, val loss: 0.8536068201065063
Epoch 550, training loss: 6.713984489440918 = 0.3609940707683563 + 1.0 * 6.352990627288818
Epoch 550, val loss: 0.8518751263618469
Epoch 560, training loss: 6.69320821762085 = 0.3440704941749573 + 1.0 * 6.349137783050537
Epoch 560, val loss: 0.85084068775177
Epoch 570, training loss: 6.674389839172363 = 0.3277131915092468 + 1.0 * 6.346676826477051
Epoch 570, val loss: 0.8504781723022461
Epoch 580, training loss: 6.662133693695068 = 0.31199920177459717 + 1.0 * 6.350134372711182
Epoch 580, val loss: 0.850807785987854
Epoch 590, training loss: 6.641387462615967 = 0.29700416326522827 + 1.0 * 6.344383239746094
Epoch 590, val loss: 0.8517212271690369
Epoch 600, training loss: 6.625833034515381 = 0.2826891243457794 + 1.0 * 6.343143939971924
Epoch 600, val loss: 0.8533541560173035
Epoch 610, training loss: 6.61210823059082 = 0.2688939571380615 + 1.0 * 6.343214511871338
Epoch 610, val loss: 0.8555194735527039
Epoch 620, training loss: 6.599221706390381 = 0.2557414174079895 + 1.0 * 6.343480110168457
Epoch 620, val loss: 0.8579750061035156
Epoch 630, training loss: 6.580310821533203 = 0.24316251277923584 + 1.0 * 6.337148189544678
Epoch 630, val loss: 0.8610734343528748
Epoch 640, training loss: 6.566390037536621 = 0.2310558706521988 + 1.0 * 6.335334300994873
Epoch 640, val loss: 0.8644676208496094
Epoch 650, training loss: 6.571018218994141 = 0.21940243244171143 + 1.0 * 6.351615905761719
Epoch 650, val loss: 0.8681066632270813
Epoch 660, training loss: 6.541193962097168 = 0.20836932957172394 + 1.0 * 6.33282470703125
Epoch 660, val loss: 0.8721207976341248
Epoch 670, training loss: 6.530111312866211 = 0.1978076994419098 + 1.0 * 6.332303524017334
Epoch 670, val loss: 0.8764742016792297
Epoch 680, training loss: 6.5211501121521 = 0.18766069412231445 + 1.0 * 6.333489418029785
Epoch 680, val loss: 0.8809822797775269
Epoch 690, training loss: 6.506149768829346 = 0.17797154188156128 + 1.0 * 6.328178405761719
Epoch 690, val loss: 0.8855932354927063
Epoch 700, training loss: 6.496243000030518 = 0.16871997714042664 + 1.0 * 6.327523231506348
Epoch 700, val loss: 0.8905327916145325
Epoch 710, training loss: 6.500208854675293 = 0.15991371870040894 + 1.0 * 6.340295314788818
Epoch 710, val loss: 0.8955804109573364
Epoch 720, training loss: 6.478051662445068 = 0.1516014188528061 + 1.0 * 6.326450347900391
Epoch 720, val loss: 0.9006413221359253
Epoch 730, training loss: 6.468414783477783 = 0.14374026656150818 + 1.0 * 6.324674606323242
Epoch 730, val loss: 0.9058356285095215
Epoch 740, training loss: 6.459098815917969 = 0.1362476348876953 + 1.0 * 6.322851181030273
Epoch 740, val loss: 0.9110338091850281
Epoch 750, training loss: 6.465091228485107 = 0.1291380226612091 + 1.0 * 6.335953235626221
Epoch 750, val loss: 0.916264533996582
Epoch 760, training loss: 6.445173263549805 = 0.12248923629522324 + 1.0 * 6.322683811187744
Epoch 760, val loss: 0.9215993881225586
Epoch 770, training loss: 6.436950206756592 = 0.11619963496923447 + 1.0 * 6.320750713348389
Epoch 770, val loss: 0.9269937872886658
Epoch 780, training loss: 6.430134296417236 = 0.110260508954525 + 1.0 * 6.319873809814453
Epoch 780, val loss: 0.9322885870933533
Epoch 790, training loss: 6.426210403442383 = 0.10466038435697556 + 1.0 * 6.321549892425537
Epoch 790, val loss: 0.9376574754714966
Epoch 800, training loss: 6.415949821472168 = 0.09942403435707092 + 1.0 * 6.316525936126709
Epoch 800, val loss: 0.9430195689201355
Epoch 810, training loss: 6.411259174346924 = 0.0944962352514267 + 1.0 * 6.316762924194336
Epoch 810, val loss: 0.9484593272209167
Epoch 820, training loss: 6.404330253601074 = 0.08985009789466858 + 1.0 * 6.314480304718018
Epoch 820, val loss: 0.95374995470047
Epoch 830, training loss: 6.4067559242248535 = 0.08546972274780273 + 1.0 * 6.321286201477051
Epoch 830, val loss: 0.9591060876846313
Epoch 840, training loss: 6.399147033691406 = 0.08136755973100662 + 1.0 * 6.317779541015625
Epoch 840, val loss: 0.9643898010253906
Epoch 850, training loss: 6.3892645835876465 = 0.07751484215259552 + 1.0 * 6.3117499351501465
Epoch 850, val loss: 0.9697298407554626
Epoch 860, training loss: 6.383768558502197 = 0.0738811269402504 + 1.0 * 6.309887409210205
Epoch 860, val loss: 0.9749420285224915
Epoch 870, training loss: 6.3954291343688965 = 0.07046979665756226 + 1.0 * 6.3249592781066895
Epoch 870, val loss: 0.9801597595214844
Epoch 880, training loss: 6.381384372711182 = 0.0672660768032074 + 1.0 * 6.314118385314941
Epoch 880, val loss: 0.9851108193397522
Epoch 890, training loss: 6.371431827545166 = 0.06426350772380829 + 1.0 * 6.307168483734131
Epoch 890, val loss: 0.9902125597000122
Epoch 900, training loss: 6.369157791137695 = 0.06142806261777878 + 1.0 * 6.307729721069336
Epoch 900, val loss: 0.995165228843689
Epoch 910, training loss: 6.371804714202881 = 0.05875544995069504 + 1.0 * 6.31304931640625
Epoch 910, val loss: 1.0000425577163696
Epoch 920, training loss: 6.363349437713623 = 0.05625738203525543 + 1.0 * 6.307092189788818
Epoch 920, val loss: 1.0049757957458496
Epoch 930, training loss: 6.358551979064941 = 0.0538896843791008 + 1.0 * 6.304662227630615
Epoch 930, val loss: 1.0098410844802856
Epoch 940, training loss: 6.369078636169434 = 0.051658693701028824 + 1.0 * 6.31742000579834
Epoch 940, val loss: 1.0146127939224243
Epoch 950, training loss: 6.355225563049316 = 0.04957618564367294 + 1.0 * 6.305649280548096
Epoch 950, val loss: 1.019222378730774
Epoch 960, training loss: 6.350015163421631 = 0.047606952488422394 + 1.0 * 6.302408218383789
Epoch 960, val loss: 1.0239773988723755
Epoch 970, training loss: 6.345762252807617 = 0.04574200138449669 + 1.0 * 6.300020217895508
Epoch 970, val loss: 1.0285228490829468
Epoch 980, training loss: 6.348756313323975 = 0.043977268040180206 + 1.0 * 6.304779052734375
Epoch 980, val loss: 1.033017635345459
Epoch 990, training loss: 6.348778247833252 = 0.04231739044189453 + 1.0 * 6.306460857391357
Epoch 990, val loss: 1.0374131202697754
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 10.5387544631958 = 1.9419374465942383 + 1.0 * 8.596817016601562
Epoch 0, val loss: 1.944798231124878
Epoch 10, training loss: 10.52820873260498 = 1.931715965270996 + 1.0 * 8.596492767333984
Epoch 10, val loss: 1.9336005449295044
Epoch 20, training loss: 10.512798309326172 = 1.9187713861465454 + 1.0 * 8.594026565551758
Epoch 20, val loss: 1.919037938117981
Epoch 30, training loss: 10.476335525512695 = 1.9005483388900757 + 1.0 * 8.575787544250488
Epoch 30, val loss: 1.8981486558914185
Epoch 40, training loss: 10.355253219604492 = 1.8769497871398926 + 1.0 * 8.478303909301758
Epoch 40, val loss: 1.8719576597213745
Epoch 50, training loss: 9.919519424438477 = 1.8531335592269897 + 1.0 * 8.066386222839355
Epoch 50, val loss: 1.8469256162643433
Epoch 60, training loss: 9.483110427856445 = 1.8358373641967773 + 1.0 * 7.647273063659668
Epoch 60, val loss: 1.8308264017105103
Epoch 70, training loss: 9.113308906555176 = 1.8234965801239014 + 1.0 * 7.2898125648498535
Epoch 70, val loss: 1.8190133571624756
Epoch 80, training loss: 8.843207359313965 = 1.812705159187317 + 1.0 * 7.030501842498779
Epoch 80, val loss: 1.8089733123779297
Epoch 90, training loss: 8.672074317932129 = 1.8020668029785156 + 1.0 * 6.870007514953613
Epoch 90, val loss: 1.7992682456970215
Epoch 100, training loss: 8.568965911865234 = 1.7907626628875732 + 1.0 * 6.77820348739624
Epoch 100, val loss: 1.7887388467788696
Epoch 110, training loss: 8.490484237670898 = 1.7793917655944824 + 1.0 * 6.711092948913574
Epoch 110, val loss: 1.778027892112732
Epoch 120, training loss: 8.42495346069336 = 1.7682669162750244 + 1.0 * 6.656686782836914
Epoch 120, val loss: 1.767951250076294
Epoch 130, training loss: 8.371535301208496 = 1.7564018964767456 + 1.0 * 6.615133285522461
Epoch 130, val loss: 1.7574855089187622
Epoch 140, training loss: 8.327310562133789 = 1.7427952289581299 + 1.0 * 6.58451509475708
Epoch 140, val loss: 1.7457047700881958
Epoch 150, training loss: 8.286354064941406 = 1.7273344993591309 + 1.0 * 6.559019088745117
Epoch 150, val loss: 1.732506513595581
Epoch 160, training loss: 8.247722625732422 = 1.7096126079559326 + 1.0 * 6.538110256195068
Epoch 160, val loss: 1.71746826171875
Epoch 170, training loss: 8.210589408874512 = 1.688827395439148 + 1.0 * 6.521761894226074
Epoch 170, val loss: 1.6997824907302856
Epoch 180, training loss: 8.171191215515137 = 1.664617657661438 + 1.0 * 6.50657320022583
Epoch 180, val loss: 1.679215669631958
Epoch 190, training loss: 8.129093170166016 = 1.6363635063171387 + 1.0 * 6.492729663848877
Epoch 190, val loss: 1.6551603078842163
Epoch 200, training loss: 8.085333824157715 = 1.6034572124481201 + 1.0 * 6.481876373291016
Epoch 200, val loss: 1.6272170543670654
Epoch 210, training loss: 8.037381172180176 = 1.5658513307571411 + 1.0 * 6.471529483795166
Epoch 210, val loss: 1.595492959022522
Epoch 220, training loss: 7.98459529876709 = 1.5229204893112183 + 1.0 * 6.461674690246582
Epoch 220, val loss: 1.5593708753585815
Epoch 230, training loss: 7.9275383949279785 = 1.4743133783340454 + 1.0 * 6.453225135803223
Epoch 230, val loss: 1.5187584161758423
Epoch 240, training loss: 7.867038249969482 = 1.420298457145691 + 1.0 * 6.446739673614502
Epoch 240, val loss: 1.4741851091384888
Epoch 250, training loss: 7.805180549621582 = 1.3628144264221191 + 1.0 * 6.442366123199463
Epoch 250, val loss: 1.4274306297302246
Epoch 260, training loss: 7.737220287322998 = 1.3025288581848145 + 1.0 * 6.434691429138184
Epoch 260, val loss: 1.3793108463287354
Epoch 270, training loss: 7.669422149658203 = 1.2396414279937744 + 1.0 * 6.42978048324585
Epoch 270, val loss: 1.3298428058624268
Epoch 280, training loss: 7.6022233963012695 = 1.1758465766906738 + 1.0 * 6.426376819610596
Epoch 280, val loss: 1.2806018590927124
Epoch 290, training loss: 7.53348445892334 = 1.112431287765503 + 1.0 * 6.421053409576416
Epoch 290, val loss: 1.231688380241394
Epoch 300, training loss: 7.465322494506836 = 1.0491231679916382 + 1.0 * 6.416199207305908
Epoch 300, val loss: 1.1829990148544312
Epoch 310, training loss: 7.398148059844971 = 0.9865564703941345 + 1.0 * 6.411591529846191
Epoch 310, val loss: 1.134660243988037
Epoch 320, training loss: 7.33941650390625 = 0.9262992739677429 + 1.0 * 6.413117408752441
Epoch 320, val loss: 1.0881233215332031
Epoch 330, training loss: 7.276149272918701 = 0.8701532483100891 + 1.0 * 6.405995845794678
Epoch 330, val loss: 1.0446054935455322
Epoch 340, training loss: 7.2187604904174805 = 0.8176264762878418 + 1.0 * 6.401134014129639
Epoch 340, val loss: 1.0041582584381104
Epoch 350, training loss: 7.166696548461914 = 0.768664538860321 + 1.0 * 6.398032188415527
Epoch 350, val loss: 0.9668592810630798
Epoch 360, training loss: 7.119089603424072 = 0.7229456305503845 + 1.0 * 6.396143913269043
Epoch 360, val loss: 0.9327616095542908
Epoch 370, training loss: 7.073996543884277 = 0.6803941130638123 + 1.0 * 6.39360237121582
Epoch 370, val loss: 0.9021120667457581
Epoch 380, training loss: 7.030615329742432 = 0.6409656405448914 + 1.0 * 6.389649868011475
Epoch 380, val loss: 0.8747962117195129
Epoch 390, training loss: 6.98835563659668 = 0.6038429141044617 + 1.0 * 6.384512901306152
Epoch 390, val loss: 0.8502551317214966
Epoch 400, training loss: 6.953741550445557 = 0.5687164664268494 + 1.0 * 6.3850250244140625
Epoch 400, val loss: 0.8282298445701599
Epoch 410, training loss: 6.914662837982178 = 0.5357293486595154 + 1.0 * 6.378933429718018
Epoch 410, val loss: 0.8087422251701355
Epoch 420, training loss: 6.880494117736816 = 0.5044199228286743 + 1.0 * 6.376074314117432
Epoch 420, val loss: 0.7915322184562683
Epoch 430, training loss: 6.849595069885254 = 0.474774032831192 + 1.0 * 6.374821186065674
Epoch 430, val loss: 0.7762898206710815
Epoch 440, training loss: 6.816741466522217 = 0.4468383193016052 + 1.0 * 6.369903087615967
Epoch 440, val loss: 0.763070285320282
Epoch 450, training loss: 6.788067817687988 = 0.42034661769866943 + 1.0 * 6.367721080780029
Epoch 450, val loss: 0.7515055537223816
Epoch 460, training loss: 6.7661871910095215 = 0.3951731324195862 + 1.0 * 6.37101411819458
Epoch 460, val loss: 0.74140465259552
Epoch 470, training loss: 6.737832546234131 = 0.371381014585495 + 1.0 * 6.366451740264893
Epoch 470, val loss: 0.7329213619232178
Epoch 480, training loss: 6.7099385261535645 = 0.3489496409893036 + 1.0 * 6.360989093780518
Epoch 480, val loss: 0.7258017659187317
Epoch 490, training loss: 6.688098907470703 = 0.32775557041168213 + 1.0 * 6.3603434562683105
Epoch 490, val loss: 0.7198883295059204
Epoch 500, training loss: 6.663996696472168 = 0.3077864944934845 + 1.0 * 6.356210231781006
Epoch 500, val loss: 0.7151793837547302
Epoch 510, training loss: 6.642279148101807 = 0.288840115070343 + 1.0 * 6.353438854217529
Epoch 510, val loss: 0.7114714980125427
Epoch 520, training loss: 6.625246524810791 = 0.2708931267261505 + 1.0 * 6.354353427886963
Epoch 520, val loss: 0.7087193131446838
Epoch 530, training loss: 6.605185031890869 = 0.25401297211647034 + 1.0 * 6.351171970367432
Epoch 530, val loss: 0.7069426774978638
Epoch 540, training loss: 6.585676193237305 = 0.2380635142326355 + 1.0 * 6.3476128578186035
Epoch 540, val loss: 0.7060160040855408
Epoch 550, training loss: 6.583786487579346 = 0.2231137454509735 + 1.0 * 6.360672950744629
Epoch 550, val loss: 0.7058109641075134
Epoch 560, training loss: 6.556358814239502 = 0.20930151641368866 + 1.0 * 6.347057342529297
Epoch 560, val loss: 0.7064717411994934
Epoch 570, training loss: 6.537951946258545 = 0.19636733829975128 + 1.0 * 6.3415846824646
Epoch 570, val loss: 0.7078126072883606
Epoch 580, training loss: 6.523487091064453 = 0.18420034646987915 + 1.0 * 6.339286804199219
Epoch 580, val loss: 0.7096966505050659
Epoch 590, training loss: 6.515059947967529 = 0.17276033759117126 + 1.0 * 6.342299461364746
Epoch 590, val loss: 0.7121788859367371
Epoch 600, training loss: 6.506414890289307 = 0.16216476261615753 + 1.0 * 6.344250202178955
Epoch 600, val loss: 0.7152504324913025
Epoch 610, training loss: 6.48905611038208 = 0.15231099724769592 + 1.0 * 6.336745262145996
Epoch 610, val loss: 0.7188419699668884
Epoch 620, training loss: 6.476395130157471 = 0.14311018586158752 + 1.0 * 6.333284854888916
Epoch 620, val loss: 0.7228082418441772
Epoch 630, training loss: 6.465323448181152 = 0.13450603187084198 + 1.0 * 6.330817222595215
Epoch 630, val loss: 0.7271409034729004
Epoch 640, training loss: 6.4585747718811035 = 0.1264656037092209 + 1.0 * 6.332108974456787
Epoch 640, val loss: 0.7318560481071472
Epoch 650, training loss: 6.453735828399658 = 0.11904609948396683 + 1.0 * 6.334689617156982
Epoch 650, val loss: 0.7368149757385254
Epoch 660, training loss: 6.4396071434021 = 0.11219349503517151 + 1.0 * 6.327413558959961
Epoch 660, val loss: 0.7421281933784485
Epoch 670, training loss: 6.431236267089844 = 0.10581894963979721 + 1.0 * 6.325417518615723
Epoch 670, val loss: 0.7475734949111938
Epoch 680, training loss: 6.42985725402832 = 0.09987767785787582 + 1.0 * 6.329979419708252
Epoch 680, val loss: 0.7531290054321289
Epoch 690, training loss: 6.420843124389648 = 0.09436989575624466 + 1.0 * 6.326473236083984
Epoch 690, val loss: 0.7588937282562256
Epoch 700, training loss: 6.413936138153076 = 0.0892631933093071 + 1.0 * 6.324673175811768
Epoch 700, val loss: 0.7648362517356873
Epoch 710, training loss: 6.4065937995910645 = 0.08452043682336807 + 1.0 * 6.322073459625244
Epoch 710, val loss: 0.7708026766777039
Epoch 720, training loss: 6.40261697769165 = 0.08010479807853699 + 1.0 * 6.322512149810791
Epoch 720, val loss: 0.7769157886505127
Epoch 730, training loss: 6.394408702850342 = 0.07599322497844696 + 1.0 * 6.318415641784668
Epoch 730, val loss: 0.783031165599823
Epoch 740, training loss: 6.391014575958252 = 0.0721597969532013 + 1.0 * 6.318854808807373
Epoch 740, val loss: 0.7891863584518433
Epoch 750, training loss: 6.387953281402588 = 0.06858722865581512 + 1.0 * 6.319365978240967
Epoch 750, val loss: 0.7953534126281738
Epoch 760, training loss: 6.382752418518066 = 0.0652640089392662 + 1.0 * 6.317488193511963
Epoch 760, val loss: 0.8015567660331726
Epoch 770, training loss: 6.37564754486084 = 0.0621597096323967 + 1.0 * 6.313488006591797
Epoch 770, val loss: 0.8077484369277954
Epoch 780, training loss: 6.371679306030273 = 0.05925208330154419 + 1.0 * 6.312427043914795
Epoch 780, val loss: 0.8138988018035889
Epoch 790, training loss: 6.372419834136963 = 0.056525520980358124 + 1.0 * 6.31589412689209
Epoch 790, val loss: 0.8200061917304993
Epoch 800, training loss: 6.370826721191406 = 0.05398264154791832 + 1.0 * 6.3168439865112305
Epoch 800, val loss: 0.8261149525642395
Epoch 810, training loss: 6.363755226135254 = 0.05160876736044884 + 1.0 * 6.3121466636657715
Epoch 810, val loss: 0.8322046995162964
Epoch 820, training loss: 6.3596954345703125 = 0.04937806352972984 + 1.0 * 6.310317516326904
Epoch 820, val loss: 0.8381839394569397
Epoch 830, training loss: 6.355890274047852 = 0.0472867377102375 + 1.0 * 6.308603763580322
Epoch 830, val loss: 0.8440971374511719
Epoch 840, training loss: 6.351649761199951 = 0.04532252997159958 + 1.0 * 6.3063273429870605
Epoch 840, val loss: 0.8500441312789917
Epoch 850, training loss: 6.348227024078369 = 0.04346733167767525 + 1.0 * 6.304759502410889
Epoch 850, val loss: 0.8558802008628845
Epoch 860, training loss: 6.353718280792236 = 0.04171314090490341 + 1.0 * 6.312005043029785
Epoch 860, val loss: 0.8616973757743835
Epoch 870, training loss: 6.3526225090026855 = 0.04007777199149132 + 1.0 * 6.312544822692871
Epoch 870, val loss: 0.867431104183197
Epoch 880, training loss: 6.341025352478027 = 0.03853796049952507 + 1.0 * 6.302487373352051
Epoch 880, val loss: 0.8732371926307678
Epoch 890, training loss: 6.338996887207031 = 0.03707962855696678 + 1.0 * 6.30191707611084
Epoch 890, val loss: 0.8788468241691589
Epoch 900, training loss: 6.337453365325928 = 0.03569560870528221 + 1.0 * 6.3017578125
Epoch 900, val loss: 0.8843868970870972
Epoch 910, training loss: 6.336275577545166 = 0.034385427832603455 + 1.0 * 6.3018903732299805
Epoch 910, val loss: 0.8899054527282715
Epoch 920, training loss: 6.335209846496582 = 0.03314802795648575 + 1.0 * 6.302062034606934
Epoch 920, val loss: 0.8954015374183655
Epoch 930, training loss: 6.330594539642334 = 0.03197740390896797 + 1.0 * 6.298617362976074
Epoch 930, val loss: 0.9007763862609863
Epoch 940, training loss: 6.329548358917236 = 0.03086644969880581 + 1.0 * 6.298681735992432
Epoch 940, val loss: 0.9060809016227722
Epoch 950, training loss: 6.325939655303955 = 0.029810314998030663 + 1.0 * 6.29612922668457
Epoch 950, val loss: 0.9113333225250244
Epoch 960, training loss: 6.332019329071045 = 0.02880539745092392 + 1.0 * 6.303214073181152
Epoch 960, val loss: 0.9165576100349426
Epoch 970, training loss: 6.3262939453125 = 0.027855031192302704 + 1.0 * 6.298439025878906
Epoch 970, val loss: 0.9217196702957153
Epoch 980, training loss: 6.323253631591797 = 0.026957010850310326 + 1.0 * 6.2962965965271
Epoch 980, val loss: 0.926840603351593
Epoch 990, training loss: 6.322659015655518 = 0.026100749149918556 + 1.0 * 6.296558380126953
Epoch 990, val loss: 0.9318111538887024
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8096995255666843
The final CL Acc:0.76049, 0.03205, The final GNN Acc:0.80724, 0.00245
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13238])
remove edge: torch.Size([2, 7900])
updated graph: torch.Size([2, 10582])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.539315223693848 = 1.9424915313720703 + 1.0 * 8.596823692321777
Epoch 0, val loss: 1.9479836225509644
Epoch 10, training loss: 10.529346466064453 = 1.9328231811523438 + 1.0 * 8.59652328491211
Epoch 10, val loss: 1.9377708435058594
Epoch 20, training loss: 10.514575004577637 = 1.9203262329101562 + 1.0 * 8.59424877166748
Epoch 20, val loss: 1.9245151281356812
Epoch 30, training loss: 10.480278015136719 = 1.9022841453552246 + 1.0 * 8.577993392944336
Epoch 30, val loss: 1.905534267425537
Epoch 40, training loss: 10.37542724609375 = 1.8776155710220337 + 1.0 * 8.497811317443848
Epoch 40, val loss: 1.8807820081710815
Epoch 50, training loss: 10.034431457519531 = 1.8496007919311523 + 1.0 * 8.184830665588379
Epoch 50, val loss: 1.853814959526062
Epoch 60, training loss: 9.710760116577148 = 1.822570562362671 + 1.0 * 7.888189315795898
Epoch 60, val loss: 1.8293834924697876
Epoch 70, training loss: 9.2971830368042 = 1.8020011186599731 + 1.0 * 7.495182037353516
Epoch 70, val loss: 1.811078667640686
Epoch 80, training loss: 8.98983383178711 = 1.7884290218353271 + 1.0 * 7.201404571533203
Epoch 80, val loss: 1.7989511489868164
Epoch 90, training loss: 8.7833251953125 = 1.7724062204360962 + 1.0 * 7.010918617248535
Epoch 90, val loss: 1.784271001815796
Epoch 100, training loss: 8.649831771850586 = 1.7559458017349243 + 1.0 * 6.893885612487793
Epoch 100, val loss: 1.7697018384933472
Epoch 110, training loss: 8.539175987243652 = 1.7394379377365112 + 1.0 * 6.79973840713501
Epoch 110, val loss: 1.7543673515319824
Epoch 120, training loss: 8.455439567565918 = 1.7206590175628662 + 1.0 * 6.734780788421631
Epoch 120, val loss: 1.7366256713867188
Epoch 130, training loss: 8.388522148132324 = 1.6992837190628052 + 1.0 * 6.68923807144165
Epoch 130, val loss: 1.716957449913025
Epoch 140, training loss: 8.329760551452637 = 1.6745811700820923 + 1.0 * 6.655179500579834
Epoch 140, val loss: 1.6948562860488892
Epoch 150, training loss: 8.276239395141602 = 1.645247459411621 + 1.0 * 6.6309919357299805
Epoch 150, val loss: 1.6690325736999512
Epoch 160, training loss: 8.221675872802734 = 1.611054539680481 + 1.0 * 6.610620975494385
Epoch 160, val loss: 1.6393358707427979
Epoch 170, training loss: 8.164116859436035 = 1.5720789432525635 + 1.0 * 6.592038154602051
Epoch 170, val loss: 1.6058305501937866
Epoch 180, training loss: 8.105512619018555 = 1.5286188125610352 + 1.0 * 6.5768938064575195
Epoch 180, val loss: 1.569037914276123
Epoch 190, training loss: 8.042534828186035 = 1.4825923442840576 + 1.0 * 6.559942245483398
Epoch 190, val loss: 1.5307074785232544
Epoch 200, training loss: 7.977347373962402 = 1.4351131916046143 + 1.0 * 6.542234420776367
Epoch 200, val loss: 1.4918323755264282
Epoch 210, training loss: 7.9138360023498535 = 1.3865607976913452 + 1.0 * 6.527275085449219
Epoch 210, val loss: 1.452613115310669
Epoch 220, training loss: 7.857986927032471 = 1.3389745950698853 + 1.0 * 6.519012451171875
Epoch 220, val loss: 1.415335774421692
Epoch 230, training loss: 7.797247886657715 = 1.2932852506637573 + 1.0 * 6.503962516784668
Epoch 230, val loss: 1.3801536560058594
Epoch 240, training loss: 7.74105978012085 = 1.2481755018234253 + 1.0 * 6.492884159088135
Epoch 240, val loss: 1.346096158027649
Epoch 250, training loss: 7.686234474182129 = 1.2032617330551147 + 1.0 * 6.482972621917725
Epoch 250, val loss: 1.312706708908081
Epoch 260, training loss: 7.633200645446777 = 1.158320665359497 + 1.0 * 6.474880218505859
Epoch 260, val loss: 1.2799006700515747
Epoch 270, training loss: 7.580426216125488 = 1.114120602607727 + 1.0 * 6.466305732727051
Epoch 270, val loss: 1.2480052709579468
Epoch 280, training loss: 7.529333591461182 = 1.0706710815429688 + 1.0 * 6.458662509918213
Epoch 280, val loss: 1.2169955968856812
Epoch 290, training loss: 7.479311466217041 = 1.0273159742355347 + 1.0 * 6.451995372772217
Epoch 290, val loss: 1.1864548921585083
Epoch 300, training loss: 7.435297012329102 = 0.9839884638786316 + 1.0 * 6.451308727264404
Epoch 300, val loss: 1.1560500860214233
Epoch 310, training loss: 7.383053302764893 = 0.9418887495994568 + 1.0 * 6.441164493560791
Epoch 310, val loss: 1.1262534856796265
Epoch 320, training loss: 7.333582401275635 = 0.9002623558044434 + 1.0 * 6.433320045471191
Epoch 320, val loss: 1.0972005128860474
Epoch 330, training loss: 7.287186622619629 = 0.8591043949127197 + 1.0 * 6.42808198928833
Epoch 330, val loss: 1.0684500932693481
Epoch 340, training loss: 7.246531009674072 = 0.818756639957428 + 1.0 * 6.427774429321289
Epoch 340, val loss: 1.0405009984970093
Epoch 350, training loss: 7.2017998695373535 = 0.7803160548210144 + 1.0 * 6.421483993530273
Epoch 350, val loss: 1.0140198469161987
Epoch 360, training loss: 7.158409595489502 = 0.7433704137802124 + 1.0 * 6.4150390625
Epoch 360, val loss: 0.9888782501220703
Epoch 370, training loss: 7.124012470245361 = 0.7078418135643005 + 1.0 * 6.416170597076416
Epoch 370, val loss: 0.9651389718055725
Epoch 380, training loss: 7.080827236175537 = 0.6741445660591125 + 1.0 * 6.40668249130249
Epoch 380, val loss: 0.9432773590087891
Epoch 390, training loss: 7.045061111450195 = 0.642224133014679 + 1.0 * 6.402836799621582
Epoch 390, val loss: 0.9232434034347534
Epoch 400, training loss: 7.010997772216797 = 0.6117918491363525 + 1.0 * 6.399205684661865
Epoch 400, val loss: 0.9049947261810303
Epoch 410, training loss: 6.983707427978516 = 0.5828778147697449 + 1.0 * 6.400829792022705
Epoch 410, val loss: 0.888673722743988
Epoch 420, training loss: 6.94873046875 = 0.5558253526687622 + 1.0 * 6.392905235290527
Epoch 420, val loss: 0.8744622468948364
Epoch 430, training loss: 6.9227142333984375 = 0.5300557017326355 + 1.0 * 6.392658710479736
Epoch 430, val loss: 0.8621668219566345
Epoch 440, training loss: 6.891430854797363 = 0.5054522752761841 + 1.0 * 6.385978698730469
Epoch 440, val loss: 0.8516374826431274
Epoch 450, training loss: 6.870211601257324 = 0.4818999767303467 + 1.0 * 6.388311862945557
Epoch 450, val loss: 0.8427814841270447
Epoch 460, training loss: 6.8465046882629395 = 0.45948976278305054 + 1.0 * 6.387014865875244
Epoch 460, val loss: 0.8354842066764832
Epoch 470, training loss: 6.818997859954834 = 0.43837201595306396 + 1.0 * 6.3806257247924805
Epoch 470, val loss: 0.829690158367157
Epoch 480, training loss: 6.793784141540527 = 0.4183241128921509 + 1.0 * 6.375460147857666
Epoch 480, val loss: 0.8253769278526306
Epoch 490, training loss: 6.7720818519592285 = 0.3991587460041046 + 1.0 * 6.372922897338867
Epoch 490, val loss: 0.8221338391304016
Epoch 500, training loss: 6.750210762023926 = 0.3808034658432007 + 1.0 * 6.3694071769714355
Epoch 500, val loss: 0.8199294805526733
Epoch 510, training loss: 6.731715679168701 = 0.36318957805633545 + 1.0 * 6.368525981903076
Epoch 510, val loss: 0.818757951259613
Epoch 520, training loss: 6.71984338760376 = 0.3465304672718048 + 1.0 * 6.373312950134277
Epoch 520, val loss: 0.8182973861694336
Epoch 530, training loss: 6.692819595336914 = 0.3306744694709778 + 1.0 * 6.362144947052002
Epoch 530, val loss: 0.8187680244445801
Epoch 540, training loss: 6.676676273345947 = 0.31552788615226746 + 1.0 * 6.361148357391357
Epoch 540, val loss: 0.8198466300964355
Epoch 550, training loss: 6.665435791015625 = 0.301052063703537 + 1.0 * 6.364383697509766
Epoch 550, val loss: 0.8214502930641174
Epoch 560, training loss: 6.646265506744385 = 0.28725025057792664 + 1.0 * 6.359015464782715
Epoch 560, val loss: 0.8237356543540955
Epoch 570, training loss: 6.62856912612915 = 0.27395716309547424 + 1.0 * 6.354611873626709
Epoch 570, val loss: 0.8265309929847717
Epoch 580, training loss: 6.615362644195557 = 0.2610938847064972 + 1.0 * 6.354268550872803
Epoch 580, val loss: 0.8297750353813171
Epoch 590, training loss: 6.6024980545043945 = 0.24870704114437103 + 1.0 * 6.353791236877441
Epoch 590, val loss: 0.8334600329399109
Epoch 600, training loss: 6.586196422576904 = 0.2367837131023407 + 1.0 * 6.34941291809082
Epoch 600, val loss: 0.8376021981239319
Epoch 610, training loss: 6.574337959289551 = 0.22526848316192627 + 1.0 * 6.349069595336914
Epoch 610, val loss: 0.8419718146324158
Epoch 620, training loss: 6.559388160705566 = 0.21415521204471588 + 1.0 * 6.345232963562012
Epoch 620, val loss: 0.846767008304596
Epoch 630, training loss: 6.546836853027344 = 0.2034008502960205 + 1.0 * 6.343436241149902
Epoch 630, val loss: 0.8518807888031006
Epoch 640, training loss: 6.536832809448242 = 0.19302603602409363 + 1.0 * 6.343806743621826
Epoch 640, val loss: 0.8572100400924683
Epoch 650, training loss: 6.532153606414795 = 0.18318906426429749 + 1.0 * 6.348964691162109
Epoch 650, val loss: 0.8626146912574768
Epoch 660, training loss: 6.515024662017822 = 0.17381826043128967 + 1.0 * 6.3412065505981445
Epoch 660, val loss: 0.868364155292511
Epoch 670, training loss: 6.5019989013671875 = 0.16490338742733002 + 1.0 * 6.337095737457275
Epoch 670, val loss: 0.874280571937561
Epoch 680, training loss: 6.49129056930542 = 0.15642106533050537 + 1.0 * 6.334869384765625
Epoch 680, val loss: 0.8804328441619873
Epoch 690, training loss: 6.525165557861328 = 0.14838699996471405 + 1.0 * 6.376778602600098
Epoch 690, val loss: 0.8866685032844543
Epoch 700, training loss: 6.479135036468506 = 0.14088112115859985 + 1.0 * 6.338253974914551
Epoch 700, val loss: 0.892812192440033
Epoch 710, training loss: 6.465691566467285 = 0.13382771611213684 + 1.0 * 6.331863880157471
Epoch 710, val loss: 0.8992952704429626
Epoch 720, training loss: 6.457175254821777 = 0.1271696835756302 + 1.0 * 6.330005645751953
Epoch 720, val loss: 0.9057449698448181
Epoch 730, training loss: 6.448196887969971 = 0.12087826430797577 + 1.0 * 6.3273186683654785
Epoch 730, val loss: 0.9123876094818115
Epoch 740, training loss: 6.442504405975342 = 0.11493198573589325 + 1.0 * 6.327572345733643
Epoch 740, val loss: 0.9191571474075317
Epoch 750, training loss: 6.435133457183838 = 0.1093553900718689 + 1.0 * 6.325778007507324
Epoch 750, val loss: 0.9257943630218506
Epoch 760, training loss: 6.430574893951416 = 0.10412456840276718 + 1.0 * 6.326450347900391
Epoch 760, val loss: 0.9325984716415405
Epoch 770, training loss: 6.426049709320068 = 0.09922011196613312 + 1.0 * 6.326829433441162
Epoch 770, val loss: 0.9393279552459717
Epoch 780, training loss: 6.418295383453369 = 0.09459394216537476 + 1.0 * 6.32370138168335
Epoch 780, val loss: 0.9460921287536621
Epoch 790, training loss: 6.410275459289551 = 0.09023787826299667 + 1.0 * 6.320037364959717
Epoch 790, val loss: 0.9529662132263184
Epoch 800, training loss: 6.409046649932861 = 0.08611676841974258 + 1.0 * 6.322929859161377
Epoch 800, val loss: 0.9598155617713928
Epoch 810, training loss: 6.400399208068848 = 0.08222819864749908 + 1.0 * 6.31817102432251
Epoch 810, val loss: 0.966567873954773
Epoch 820, training loss: 6.39866828918457 = 0.07856138795614243 + 1.0 * 6.3201069831848145
Epoch 820, val loss: 0.9733190536499023
Epoch 830, training loss: 6.392593860626221 = 0.07511473447084427 + 1.0 * 6.317479133605957
Epoch 830, val loss: 0.9800699949264526
Epoch 840, training loss: 6.3876190185546875 = 0.0718383640050888 + 1.0 * 6.3157806396484375
Epoch 840, val loss: 0.98674076795578
Epoch 850, training loss: 6.390458106994629 = 0.06874953955411911 + 1.0 * 6.321708679199219
Epoch 850, val loss: 0.9933872818946838
Epoch 860, training loss: 6.382269382476807 = 0.06584581732749939 + 1.0 * 6.316423416137695
Epoch 860, val loss: 0.9998900294303894
Epoch 870, training loss: 6.375938415527344 = 0.06309594959020615 + 1.0 * 6.31284236907959
Epoch 870, val loss: 1.0063937902450562
Epoch 880, training loss: 6.3723273277282715 = 0.06049305945634842 + 1.0 * 6.311834335327148
Epoch 880, val loss: 1.0128580331802368
Epoch 890, training loss: 6.372927665710449 = 0.05803195759654045 + 1.0 * 6.3148956298828125
Epoch 890, val loss: 1.0192219018936157
Epoch 900, training loss: 6.368068218231201 = 0.05570259690284729 + 1.0 * 6.312365531921387
Epoch 900, val loss: 1.025550127029419
Epoch 910, training loss: 6.364138603210449 = 0.05349263921380043 + 1.0 * 6.310646057128906
Epoch 910, val loss: 1.0317678451538086
Epoch 920, training loss: 6.35792875289917 = 0.051400624215602875 + 1.0 * 6.306528091430664
Epoch 920, val loss: 1.0379464626312256
Epoch 930, training loss: 6.358597278594971 = 0.04941388592123985 + 1.0 * 6.309183597564697
Epoch 930, val loss: 1.044103980064392
Epoch 940, training loss: 6.354861736297607 = 0.047532517462968826 + 1.0 * 6.307329177856445
Epoch 940, val loss: 1.0500613451004028
Epoch 950, training loss: 6.351367950439453 = 0.04574821516871452 + 1.0 * 6.305619716644287
Epoch 950, val loss: 1.0560252666473389
Epoch 960, training loss: 6.355733871459961 = 0.044055093079805374 + 1.0 * 6.311678886413574
Epoch 960, val loss: 1.0618723630905151
Epoch 970, training loss: 6.350337505340576 = 0.042460013180971146 + 1.0 * 6.307877540588379
Epoch 970, val loss: 1.0676324367523193
Epoch 980, training loss: 6.343641757965088 = 0.04093361273407936 + 1.0 * 6.302708148956299
Epoch 980, val loss: 1.0733541250228882
Epoch 990, training loss: 6.339803218841553 = 0.039488062262535095 + 1.0 * 6.3003153800964355
Epoch 990, val loss: 1.0790445804595947
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 10.540212631225586 = 1.9433722496032715 + 1.0 * 8.596839904785156
Epoch 0, val loss: 1.9467450380325317
Epoch 10, training loss: 10.529829978942871 = 1.9332181215286255 + 1.0 * 8.596611976623535
Epoch 10, val loss: 1.9365605115890503
Epoch 20, training loss: 10.515789985656738 = 1.9209247827529907 + 1.0 * 8.594864845275879
Epoch 20, val loss: 1.924275517463684
Epoch 30, training loss: 10.484764099121094 = 1.904232382774353 + 1.0 * 8.58053207397461
Epoch 30, val loss: 1.9075324535369873
Epoch 40, training loss: 10.377389907836914 = 1.8817099332809448 + 1.0 * 8.49567985534668
Epoch 40, val loss: 1.8854674100875854
Epoch 50, training loss: 9.924179077148438 = 1.8566603660583496 + 1.0 * 8.06751823425293
Epoch 50, val loss: 1.861759066581726
Epoch 60, training loss: 9.391048431396484 = 1.8354403972625732 + 1.0 * 7.555607795715332
Epoch 60, val loss: 1.842087745666504
Epoch 70, training loss: 9.036693572998047 = 1.8181966543197632 + 1.0 * 7.218497276306152
Epoch 70, val loss: 1.8261216878890991
Epoch 80, training loss: 8.843062400817871 = 1.8022516965866089 + 1.0 * 7.040810585021973
Epoch 80, val loss: 1.8116923570632935
Epoch 90, training loss: 8.69123649597168 = 1.785217046737671 + 1.0 * 6.906019687652588
Epoch 90, val loss: 1.796461820602417
Epoch 100, training loss: 8.580904006958008 = 1.7676067352294922 + 1.0 * 6.813296794891357
Epoch 100, val loss: 1.780614972114563
Epoch 110, training loss: 8.494121551513672 = 1.7491649389266968 + 1.0 * 6.744956970214844
Epoch 110, val loss: 1.7639445066452026
Epoch 120, training loss: 8.410663604736328 = 1.729658842086792 + 1.0 * 6.681004524230957
Epoch 120, val loss: 1.7468115091323853
Epoch 130, training loss: 8.342918395996094 = 1.7078384160995483 + 1.0 * 6.635079860687256
Epoch 130, val loss: 1.7275056838989258
Epoch 140, training loss: 8.283711433410645 = 1.6816645860671997 + 1.0 * 6.602046966552734
Epoch 140, val loss: 1.7042549848556519
Epoch 150, training loss: 8.227119445800781 = 1.6503759622573853 + 1.0 * 6.576743125915527
Epoch 150, val loss: 1.676583170890808
Epoch 160, training loss: 8.16966438293457 = 1.6138495206832886 + 1.0 * 6.55581521987915
Epoch 160, val loss: 1.6446961164474487
Epoch 170, training loss: 8.108911514282227 = 1.5729963779449463 + 1.0 * 6.535915374755859
Epoch 170, val loss: 1.6091265678405762
Epoch 180, training loss: 8.044880867004395 = 1.5275038480758667 + 1.0 * 6.5173773765563965
Epoch 180, val loss: 1.569645643234253
Epoch 190, training loss: 7.979996204376221 = 1.4781566858291626 + 1.0 * 6.501839637756348
Epoch 190, val loss: 1.5273085832595825
Epoch 200, training loss: 7.915393829345703 = 1.42718505859375 + 1.0 * 6.488208770751953
Epoch 200, val loss: 1.4836663007736206
Epoch 210, training loss: 7.8521952629089355 = 1.375380516052246 + 1.0 * 6.4768147468566895
Epoch 210, val loss: 1.4396213293075562
Epoch 220, training loss: 7.79191780090332 = 1.3239061832427979 + 1.0 * 6.468011379241943
Epoch 220, val loss: 1.3968193531036377
Epoch 230, training loss: 7.732188701629639 = 1.2740033864974976 + 1.0 * 6.458185195922852
Epoch 230, val loss: 1.3556863069534302
Epoch 240, training loss: 7.675504207611084 = 1.2254747152328491 + 1.0 * 6.450029373168945
Epoch 240, val loss: 1.3162131309509277
Epoch 250, training loss: 7.624233245849609 = 1.1780153512954712 + 1.0 * 6.446218013763428
Epoch 250, val loss: 1.278326153755188
Epoch 260, training loss: 7.569091320037842 = 1.1320821046829224 + 1.0 * 6.437009334564209
Epoch 260, val loss: 1.2423593997955322
Epoch 270, training loss: 7.517212867736816 = 1.087599515914917 + 1.0 * 6.42961311340332
Epoch 270, val loss: 1.208047866821289
Epoch 280, training loss: 7.470392227172852 = 1.0442278385162354 + 1.0 * 6.426164627075195
Epoch 280, val loss: 1.1752090454101562
Epoch 290, training loss: 7.423066139221191 = 1.0026589632034302 + 1.0 * 6.420407295227051
Epoch 290, val loss: 1.1440131664276123
Epoch 300, training loss: 7.377659320831299 = 0.9624879956245422 + 1.0 * 6.415171146392822
Epoch 300, val loss: 1.1145356893539429
Epoch 310, training loss: 7.333511829376221 = 0.9234408736228943 + 1.0 * 6.410070896148682
Epoch 310, val loss: 1.0862631797790527
Epoch 320, training loss: 7.299030303955078 = 0.8852403163909912 + 1.0 * 6.413790225982666
Epoch 320, val loss: 1.0590568780899048
Epoch 330, training loss: 7.2557783126831055 = 0.848258912563324 + 1.0 * 6.407519340515137
Epoch 330, val loss: 1.0332247018814087
Epoch 340, training loss: 7.212912559509277 = 0.8123219013214111 + 1.0 * 6.400590896606445
Epoch 340, val loss: 1.008589506149292
Epoch 350, training loss: 7.173152446746826 = 0.7769293189048767 + 1.0 * 6.396223068237305
Epoch 350, val loss: 0.9849005937576294
Epoch 360, training loss: 7.137320518493652 = 0.7419418096542358 + 1.0 * 6.395378589630127
Epoch 360, val loss: 0.9619207382202148
Epoch 370, training loss: 7.102847099304199 = 0.7077282071113586 + 1.0 * 6.395118713378906
Epoch 370, val loss: 0.9401202201843262
Epoch 380, training loss: 7.063406467437744 = 0.6747652292251587 + 1.0 * 6.388641357421875
Epoch 380, val loss: 0.9196994304656982
Epoch 390, training loss: 7.030118942260742 = 0.6429437398910522 + 1.0 * 6.3871750831604
Epoch 390, val loss: 0.9007144570350647
Epoch 400, training loss: 6.994789123535156 = 0.6126625537872314 + 1.0 * 6.382126331329346
Epoch 400, val loss: 0.8835599422454834
Epoch 410, training loss: 6.970440864562988 = 0.5842114090919495 + 1.0 * 6.386229515075684
Epoch 410, val loss: 0.8684454560279846
Epoch 420, training loss: 6.9375481605529785 = 0.5580635666847229 + 1.0 * 6.3794846534729
Epoch 420, val loss: 0.8555591106414795
Epoch 430, training loss: 6.90806770324707 = 0.5337213277816772 + 1.0 * 6.3743462562561035
Epoch 430, val loss: 0.8447244763374329
Epoch 440, training loss: 6.882635116577148 = 0.5109716653823853 + 1.0 * 6.371663570404053
Epoch 440, val loss: 0.8356238603591919
Epoch 450, training loss: 6.870481967926025 = 0.4896388649940491 + 1.0 * 6.380843162536621
Epoch 450, val loss: 0.8281784653663635
Epoch 460, training loss: 6.837749481201172 = 0.4696372151374817 + 1.0 * 6.368112087249756
Epoch 460, val loss: 0.8221852779388428
Epoch 470, training loss: 6.815535545349121 = 0.4506796896457672 + 1.0 * 6.364855766296387
Epoch 470, val loss: 0.8172782063484192
Epoch 480, training loss: 6.795984745025635 = 0.43232592940330505 + 1.0 * 6.363658905029297
Epoch 480, val loss: 0.8132022619247437
Epoch 490, training loss: 6.779074192047119 = 0.414478600025177 + 1.0 * 6.364595413208008
Epoch 490, val loss: 0.8097792863845825
Epoch 500, training loss: 6.755682945251465 = 0.3969323933124542 + 1.0 * 6.358750343322754
Epoch 500, val loss: 0.8070610165596008
Epoch 510, training loss: 6.737305164337158 = 0.37950414419174194 + 1.0 * 6.3578009605407715
Epoch 510, val loss: 0.8048210144042969
Epoch 520, training loss: 6.719372749328613 = 0.36220115423202515 + 1.0 * 6.357171535491943
Epoch 520, val loss: 0.8029782176017761
Epoch 530, training loss: 6.699324607849121 = 0.3450793921947479 + 1.0 * 6.354245185852051
Epoch 530, val loss: 0.8017763495445251
Epoch 540, training loss: 6.682613372802734 = 0.3281474709510803 + 1.0 * 6.354465961456299
Epoch 540, val loss: 0.8011493682861328
Epoch 550, training loss: 6.662057876586914 = 0.3114553689956665 + 1.0 * 6.350602626800537
Epoch 550, val loss: 0.8011251091957092
Epoch 560, training loss: 6.642938613891602 = 0.2951670289039612 + 1.0 * 6.347771644592285
Epoch 560, val loss: 0.8018134832382202
Epoch 570, training loss: 6.62846565246582 = 0.2793136239051819 + 1.0 * 6.349152088165283
Epoch 570, val loss: 0.8030463457107544
Epoch 580, training loss: 6.609464168548584 = 0.26396360993385315 + 1.0 * 6.345500469207764
Epoch 580, val loss: 0.8048489093780518
Epoch 590, training loss: 6.594055652618408 = 0.24915261566638947 + 1.0 * 6.344902992248535
Epoch 590, val loss: 0.8071428537368774
Epoch 600, training loss: 6.577191352844238 = 0.23488353192806244 + 1.0 * 6.342308044433594
Epoch 600, val loss: 0.8098670840263367
Epoch 610, training loss: 6.565657138824463 = 0.2211623340845108 + 1.0 * 6.344494819641113
Epoch 610, val loss: 0.8129522204399109
Epoch 620, training loss: 6.5476789474487305 = 0.20803005993366241 + 1.0 * 6.339648723602295
Epoch 620, val loss: 0.8162949681282043
Epoch 630, training loss: 6.533593654632568 = 0.19544418156147003 + 1.0 * 6.338149547576904
Epoch 630, val loss: 0.819923996925354
Epoch 640, training loss: 6.526278018951416 = 0.183433398604393 + 1.0 * 6.342844486236572
Epoch 640, val loss: 0.8236634135246277
Epoch 650, training loss: 6.510780334472656 = 0.172061026096344 + 1.0 * 6.338719367980957
Epoch 650, val loss: 0.8276185393333435
Epoch 660, training loss: 6.496229648590088 = 0.16131320595741272 + 1.0 * 6.334916591644287
Epoch 660, val loss: 0.8317975401878357
Epoch 670, training loss: 6.4837493896484375 = 0.15119050443172455 + 1.0 * 6.332559108734131
Epoch 670, val loss: 0.8360560536384583
Epoch 680, training loss: 6.471623420715332 = 0.14167411625385284 + 1.0 * 6.329949378967285
Epoch 680, val loss: 0.8406479954719543
Epoch 690, training loss: 6.469691276550293 = 0.13276846706867218 + 1.0 * 6.336922645568848
Epoch 690, val loss: 0.8454355597496033
Epoch 700, training loss: 6.457020282745361 = 0.12446390837430954 + 1.0 * 6.332556247711182
Epoch 700, val loss: 0.8503037095069885
Epoch 710, training loss: 6.4486870765686035 = 0.11681332439184189 + 1.0 * 6.331873893737793
Epoch 710, val loss: 0.8554221391677856
Epoch 720, training loss: 6.437864780426025 = 0.10971260070800781 + 1.0 * 6.328152179718018
Epoch 720, val loss: 0.8606783747673035
Epoch 730, training loss: 6.427298069000244 = 0.10316874086856842 + 1.0 * 6.324129104614258
Epoch 730, val loss: 0.8661918044090271
Epoch 740, training loss: 6.424135208129883 = 0.09711866825819016 + 1.0 * 6.327016353607178
Epoch 740, val loss: 0.871860921382904
Epoch 750, training loss: 6.4158430099487305 = 0.09154579043388367 + 1.0 * 6.3242974281311035
Epoch 750, val loss: 0.8776247501373291
Epoch 760, training loss: 6.407927513122559 = 0.08642666786909103 + 1.0 * 6.321500778198242
Epoch 760, val loss: 0.8836408853530884
Epoch 770, training loss: 6.403067111968994 = 0.08169303834438324 + 1.0 * 6.32137393951416
Epoch 770, val loss: 0.8896865248680115
Epoch 780, training loss: 6.39824104309082 = 0.07731615751981735 + 1.0 * 6.320924758911133
Epoch 780, val loss: 0.8958717584609985
Epoch 790, training loss: 6.393974781036377 = 0.07327475398778915 + 1.0 * 6.320700168609619
Epoch 790, val loss: 0.9020864963531494
Epoch 800, training loss: 6.3875579833984375 = 0.06954914331436157 + 1.0 * 6.318008899688721
Epoch 800, val loss: 0.9084534645080566
Epoch 810, training loss: 6.381112098693848 = 0.06609483063220978 + 1.0 * 6.315017223358154
Epoch 810, val loss: 0.9148787260055542
Epoch 820, training loss: 6.379001617431641 = 0.06287889182567596 + 1.0 * 6.316122531890869
Epoch 820, val loss: 0.9213297367095947
Epoch 830, training loss: 6.375790119171143 = 0.059878718107938766 + 1.0 * 6.315911293029785
Epoch 830, val loss: 0.9276712536811829
Epoch 840, training loss: 6.372875213623047 = 0.05710224434733391 + 1.0 * 6.315773010253906
Epoch 840, val loss: 0.9341446757316589
Epoch 850, training loss: 6.365978240966797 = 0.054511137306690216 + 1.0 * 6.311467170715332
Epoch 850, val loss: 0.9405773878097534
Epoch 860, training loss: 6.363212585449219 = 0.052083805203437805 + 1.0 * 6.311128616333008
Epoch 860, val loss: 0.9469860792160034
Epoch 870, training loss: 6.3627214431762695 = 0.04980894550681114 + 1.0 * 6.312912464141846
Epoch 870, val loss: 0.9533798694610596
Epoch 880, training loss: 6.360326290130615 = 0.04766944423317909 + 1.0 * 6.312656879425049
Epoch 880, val loss: 0.9596771001815796
Epoch 890, training loss: 6.355374336242676 = 0.04567648470401764 + 1.0 * 6.30969762802124
Epoch 890, val loss: 0.9659435153007507
Epoch 900, training loss: 6.352227210998535 = 0.043792180716991425 + 1.0 * 6.308434963226318
Epoch 900, val loss: 0.9721694588661194
Epoch 910, training loss: 6.348204612731934 = 0.042027056217193604 + 1.0 * 6.306177616119385
Epoch 910, val loss: 0.9783992171287537
Epoch 920, training loss: 6.346202850341797 = 0.040355682373046875 + 1.0 * 6.30584716796875
Epoch 920, val loss: 0.9845741391181946
Epoch 930, training loss: 6.350454807281494 = 0.03878021612763405 + 1.0 * 6.31167459487915
Epoch 930, val loss: 0.9906286001205444
Epoch 940, training loss: 6.342810153961182 = 0.037300705909729004 + 1.0 * 6.305509567260742
Epoch 940, val loss: 0.9966370463371277
Epoch 950, training loss: 6.33906888961792 = 0.03589782863855362 + 1.0 * 6.303171157836914
Epoch 950, val loss: 1.0027203559875488
Epoch 960, training loss: 6.3461456298828125 = 0.03457452356815338 + 1.0 * 6.31157112121582
Epoch 960, val loss: 1.0085817575454712
Epoch 970, training loss: 6.33518123626709 = 0.03332258015871048 + 1.0 * 6.301858425140381
Epoch 970, val loss: 1.0144134759902954
Epoch 980, training loss: 6.331653594970703 = 0.03213447332382202 + 1.0 * 6.299519062042236
Epoch 980, val loss: 1.0203454494476318
Epoch 990, training loss: 6.330952167510986 = 0.031009187921881676 + 1.0 * 6.299942970275879
Epoch 990, val loss: 1.0261189937591553
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 10.538726806640625 = 1.941890001296997 + 1.0 * 8.596837043762207
Epoch 0, val loss: 1.9483258724212646
Epoch 10, training loss: 10.528361320495605 = 1.9317846298217773 + 1.0 * 8.596576690673828
Epoch 10, val loss: 1.937546968460083
Epoch 20, training loss: 10.514080047607422 = 1.9193496704101562 + 1.0 * 8.594730377197266
Epoch 20, val loss: 1.9240062236785889
Epoch 30, training loss: 10.482491493225098 = 1.9022982120513916 + 1.0 * 8.580193519592285
Epoch 30, val loss: 1.9052855968475342
Epoch 40, training loss: 10.3668794631958 = 1.8797314167022705 + 1.0 * 8.48714828491211
Epoch 40, val loss: 1.8811259269714355
Epoch 50, training loss: 9.82557487487793 = 1.8559234142303467 + 1.0 * 7.969651222229004
Epoch 50, val loss: 1.8569488525390625
Epoch 60, training loss: 9.3438138961792 = 1.8348561525344849 + 1.0 * 7.508957862854004
Epoch 60, val loss: 1.837302327156067
Epoch 70, training loss: 8.958060264587402 = 1.818176507949829 + 1.0 * 7.139883518218994
Epoch 70, val loss: 1.821384072303772
Epoch 80, training loss: 8.743840217590332 = 1.8013123273849487 + 1.0 * 6.942528247833252
Epoch 80, val loss: 1.805185079574585
Epoch 90, training loss: 8.641889572143555 = 1.7842488288879395 + 1.0 * 6.857641220092773
Epoch 90, val loss: 1.7888578176498413
Epoch 100, training loss: 8.569450378417969 = 1.7657557725906372 + 1.0 * 6.803694248199463
Epoch 100, val loss: 1.7713532447814941
Epoch 110, training loss: 8.504850387573242 = 1.7469217777252197 + 1.0 * 6.757928371429443
Epoch 110, val loss: 1.7537901401519775
Epoch 120, training loss: 8.44852066040039 = 1.7273130416870117 + 1.0 * 6.721208095550537
Epoch 120, val loss: 1.7358548641204834
Epoch 130, training loss: 8.396860122680664 = 1.705477237701416 + 1.0 * 6.691382884979248
Epoch 130, val loss: 1.716254472732544
Epoch 140, training loss: 8.343201637268066 = 1.6803910732269287 + 1.0 * 6.662810325622559
Epoch 140, val loss: 1.6941629648208618
Epoch 150, training loss: 8.286770820617676 = 1.6510134935379028 + 1.0 * 6.635756969451904
Epoch 150, val loss: 1.6686725616455078
Epoch 160, training loss: 8.229429244995117 = 1.61630117893219 + 1.0 * 6.613128185272217
Epoch 160, val loss: 1.6387553215026855
Epoch 170, training loss: 8.169108390808105 = 1.5757209062576294 + 1.0 * 6.593387603759766
Epoch 170, val loss: 1.604164719581604
Epoch 180, training loss: 8.106451988220215 = 1.528316855430603 + 1.0 * 6.5781354904174805
Epoch 180, val loss: 1.5637239217758179
Epoch 190, training loss: 8.039438247680664 = 1.473503828048706 + 1.0 * 6.565934658050537
Epoch 190, val loss: 1.5170176029205322
Epoch 200, training loss: 7.9696855545043945 = 1.412543773651123 + 1.0 * 6.5571417808532715
Epoch 200, val loss: 1.4654276371002197
Epoch 210, training loss: 7.893521308898926 = 1.3476403951644897 + 1.0 * 6.5458807945251465
Epoch 210, val loss: 1.410791277885437
Epoch 220, training loss: 7.81626558303833 = 1.2798649072647095 + 1.0 * 6.53640079498291
Epoch 220, val loss: 1.3541786670684814
Epoch 230, training loss: 7.739622116088867 = 1.211984395980835 + 1.0 * 6.527637958526611
Epoch 230, val loss: 1.2980780601501465
Epoch 240, training loss: 7.663193702697754 = 1.1459647417068481 + 1.0 * 6.517229080200195
Epoch 240, val loss: 1.2438409328460693
Epoch 250, training loss: 7.5903730392456055 = 1.0823874473571777 + 1.0 * 6.507985591888428
Epoch 250, val loss: 1.1922500133514404
Epoch 260, training loss: 7.520482540130615 = 1.0223031044006348 + 1.0 * 6.4981794357299805
Epoch 260, val loss: 1.1441692113876343
Epoch 270, training loss: 7.458357334136963 = 0.9649063348770142 + 1.0 * 6.493451118469238
Epoch 270, val loss: 1.0988082885742188
Epoch 280, training loss: 7.398308277130127 = 0.9105594754219055 + 1.0 * 6.487748622894287
Epoch 280, val loss: 1.0564160346984863
Epoch 290, training loss: 7.336483955383301 = 0.8591585755348206 + 1.0 * 6.477325439453125
Epoch 290, val loss: 1.0168299674987793
Epoch 300, training loss: 7.279356479644775 = 0.8096502423286438 + 1.0 * 6.469706058502197
Epoch 300, val loss: 0.9790655970573425
Epoch 310, training loss: 7.232168674468994 = 0.7614557147026062 + 1.0 * 6.470713138580322
Epoch 310, val loss: 0.9427785277366638
Epoch 320, training loss: 7.175589561462402 = 0.7161242961883545 + 1.0 * 6.459465503692627
Epoch 320, val loss: 0.90861576795578
Epoch 330, training loss: 7.1240339279174805 = 0.6732848882675171 + 1.0 * 6.450748920440674
Epoch 330, val loss: 0.8770940899848938
Epoch 340, training loss: 7.077332973480225 = 0.6325268149375916 + 1.0 * 6.444806098937988
Epoch 340, val loss: 0.8474419713020325
Epoch 350, training loss: 7.0378875732421875 = 0.5942202210426331 + 1.0 * 6.443667411804199
Epoch 350, val loss: 0.8200116753578186
Epoch 360, training loss: 6.995603084564209 = 0.5589020848274231 + 1.0 * 6.436700820922852
Epoch 360, val loss: 0.7955770492553711
Epoch 370, training loss: 6.957307815551758 = 0.5260100364685059 + 1.0 * 6.431297779083252
Epoch 370, val loss: 0.773671567440033
Epoch 380, training loss: 6.924515724182129 = 0.4954298734664917 + 1.0 * 6.429085731506348
Epoch 380, val loss: 0.7540556192398071
Epoch 390, training loss: 6.891284942626953 = 0.4671582579612732 + 1.0 * 6.424126625061035
Epoch 390, val loss: 0.7367627024650574
Epoch 400, training loss: 6.860149383544922 = 0.4407253861427307 + 1.0 * 6.419424057006836
Epoch 400, val loss: 0.721422016620636
Epoch 410, training loss: 6.837037086486816 = 0.4161776304244995 + 1.0 * 6.420859336853027
Epoch 410, val loss: 0.7077947854995728
Epoch 420, training loss: 6.804184913635254 = 0.3932390809059143 + 1.0 * 6.410945892333984
Epoch 420, val loss: 0.6958954930305481
Epoch 430, training loss: 6.7798309326171875 = 0.37159767746925354 + 1.0 * 6.408233165740967
Epoch 430, val loss: 0.6851986646652222
Epoch 440, training loss: 6.756278038024902 = 0.3511142432689667 + 1.0 * 6.405163764953613
Epoch 440, val loss: 0.6755048632621765
Epoch 450, training loss: 6.733949661254883 = 0.33178994059562683 + 1.0 * 6.402159690856934
Epoch 450, val loss: 0.666832685470581
Epoch 460, training loss: 6.711889743804932 = 0.31331145763397217 + 1.0 * 6.39857816696167
Epoch 460, val loss: 0.6590157747268677
Epoch 470, training loss: 6.702856540679932 = 0.2956423759460449 + 1.0 * 6.407214164733887
Epoch 470, val loss: 0.6519541144371033
Epoch 480, training loss: 6.674289703369141 = 0.2788791358470917 + 1.0 * 6.395410537719727
Epoch 480, val loss: 0.6457998156547546
Epoch 490, training loss: 6.653100490570068 = 0.2628452479839325 + 1.0 * 6.390255451202393
Epoch 490, val loss: 0.640409529209137
Epoch 500, training loss: 6.6486310958862305 = 0.2474963217973709 + 1.0 * 6.401134967803955
Epoch 500, val loss: 0.6357084512710571
Epoch 510, training loss: 6.6197967529296875 = 0.23293688893318176 + 1.0 * 6.386859893798828
Epoch 510, val loss: 0.6318064332008362
Epoch 520, training loss: 6.6004719734191895 = 0.21906030178070068 + 1.0 * 6.381411552429199
Epoch 520, val loss: 0.6285849809646606
Epoch 530, training loss: 6.586024761199951 = 0.20582778751850128 + 1.0 * 6.380197048187256
Epoch 530, val loss: 0.6259657740592957
Epoch 540, training loss: 6.57973051071167 = 0.19332846999168396 + 1.0 * 6.386402130126953
Epoch 540, val loss: 0.6240532398223877
Epoch 550, training loss: 6.556650638580322 = 0.18163637816905975 + 1.0 * 6.375014305114746
Epoch 550, val loss: 0.6226731538772583
Epoch 560, training loss: 6.5427021980285645 = 0.17064352333545685 + 1.0 * 6.372058868408203
Epoch 560, val loss: 0.6218206882476807
Epoch 570, training loss: 6.535984992980957 = 0.1603459119796753 + 1.0 * 6.375638961791992
Epoch 570, val loss: 0.621466875076294
Epoch 580, training loss: 6.520354747772217 = 0.15077725052833557 + 1.0 * 6.369577407836914
Epoch 580, val loss: 0.6215803027153015
Epoch 590, training loss: 6.506826400756836 = 0.1418580710887909 + 1.0 * 6.364968299865723
Epoch 590, val loss: 0.6220294833183289
Epoch 600, training loss: 6.503947734832764 = 0.1335388571023941 + 1.0 * 6.37040901184082
Epoch 600, val loss: 0.6228832602500916
Epoch 610, training loss: 6.487377166748047 = 0.12581197917461395 + 1.0 * 6.361565113067627
Epoch 610, val loss: 0.6240620017051697
Epoch 620, training loss: 6.480538845062256 = 0.11861804872751236 + 1.0 * 6.3619208335876465
Epoch 620, val loss: 0.6255068778991699
Epoch 630, training loss: 6.474468231201172 = 0.11192432790994644 + 1.0 * 6.362544059753418
Epoch 630, val loss: 0.62730473279953
Epoch 640, training loss: 6.461560249328613 = 0.10574670881032944 + 1.0 * 6.355813503265381
Epoch 640, val loss: 0.6292240619659424
Epoch 650, training loss: 6.45472526550293 = 0.09997979551553726 + 1.0 * 6.354745388031006
Epoch 650, val loss: 0.631373941898346
Epoch 660, training loss: 6.447487831115723 = 0.09461862593889236 + 1.0 * 6.352869033813477
Epoch 660, val loss: 0.6338390707969666
Epoch 670, training loss: 6.440150260925293 = 0.08962792158126831 + 1.0 * 6.350522518157959
Epoch 670, val loss: 0.636335551738739
Epoch 680, training loss: 6.433499813079834 = 0.08496078848838806 + 1.0 * 6.348538875579834
Epoch 680, val loss: 0.6390048265457153
Epoch 690, training loss: 6.430156230926514 = 0.08062361925840378 + 1.0 * 6.349532604217529
Epoch 690, val loss: 0.6419567465782166
Epoch 700, training loss: 6.422253131866455 = 0.07659466564655304 + 1.0 * 6.345658302307129
Epoch 700, val loss: 0.6448951363563538
Epoch 710, training loss: 6.414466857910156 = 0.07282978296279907 + 1.0 * 6.341637134552002
Epoch 710, val loss: 0.6479109525680542
Epoch 720, training loss: 6.40940523147583 = 0.06928906589746475 + 1.0 * 6.340116024017334
Epoch 720, val loss: 0.651188313961029
Epoch 730, training loss: 6.4268388748168945 = 0.06597806513309479 + 1.0 * 6.360860824584961
Epoch 730, val loss: 0.654596745967865
Epoch 740, training loss: 6.40515661239624 = 0.06289549916982651 + 1.0 * 6.34226131439209
Epoch 740, val loss: 0.6580957770347595
Epoch 750, training loss: 6.396386623382568 = 0.06002418324351311 + 1.0 * 6.336362361907959
Epoch 750, val loss: 0.6614037156105042
Epoch 760, training loss: 6.393336772918701 = 0.05732144042849541 + 1.0 * 6.336015224456787
Epoch 760, val loss: 0.664912223815918
Epoch 770, training loss: 6.388524055480957 = 0.0547836571931839 + 1.0 * 6.333740234375
Epoch 770, val loss: 0.6685918569564819
Epoch 780, training loss: 6.384873867034912 = 0.05240603908896446 + 1.0 * 6.332468032836914
Epoch 780, val loss: 0.6722175478935242
Epoch 790, training loss: 6.392545223236084 = 0.05016728490591049 + 1.0 * 6.34237813949585
Epoch 790, val loss: 0.6758720278739929
Epoch 800, training loss: 6.383877754211426 = 0.04808051511645317 + 1.0 * 6.335797309875488
Epoch 800, val loss: 0.679614782333374
Epoch 810, training loss: 6.3752264976501465 = 0.046106692403554916 + 1.0 * 6.329119682312012
Epoch 810, val loss: 0.6832365989685059
Epoch 820, training loss: 6.371266841888428 = 0.044249121099710464 + 1.0 * 6.327017784118652
Epoch 820, val loss: 0.6869069337844849
Epoch 830, training loss: 6.369290828704834 = 0.04248446226119995 + 1.0 * 6.326806545257568
Epoch 830, val loss: 0.690691351890564
Epoch 840, training loss: 6.367166042327881 = 0.04082048684358597 + 1.0 * 6.326345443725586
Epoch 840, val loss: 0.6945878863334656
Epoch 850, training loss: 6.36707067489624 = 0.039265889674425125 + 1.0 * 6.3278045654296875
Epoch 850, val loss: 0.6982819437980652
Epoch 860, training loss: 6.360959053039551 = 0.03779241070151329 + 1.0 * 6.323166847229004
Epoch 860, val loss: 0.7018997669219971
Epoch 870, training loss: 6.364250659942627 = 0.036389779299497604 + 1.0 * 6.3278608322143555
Epoch 870, val loss: 0.7056659460067749
Epoch 880, training loss: 6.363222599029541 = 0.03506820648908615 + 1.0 * 6.328154563903809
Epoch 880, val loss: 0.7095166444778442
Epoch 890, training loss: 6.356197357177734 = 0.03381935879588127 + 1.0 * 6.322378158569336
Epoch 890, val loss: 0.713165819644928
Epoch 900, training loss: 6.351790904998779 = 0.03263472765684128 + 1.0 * 6.319156169891357
Epoch 900, val loss: 0.7167807221412659
Epoch 910, training loss: 6.362588882446289 = 0.03150187432765961 + 1.0 * 6.331087112426758
Epoch 910, val loss: 0.7205477952957153
Epoch 920, training loss: 6.352334022521973 = 0.030434204265475273 + 1.0 * 6.321899890899658
Epoch 920, val loss: 0.7242624759674072
Epoch 930, training loss: 6.345810413360596 = 0.029418744146823883 + 1.0 * 6.316391468048096
Epoch 930, val loss: 0.7277944684028625
Epoch 940, training loss: 6.3457794189453125 = 0.02845143899321556 + 1.0 * 6.317327976226807
Epoch 940, val loss: 0.731353223323822
Epoch 950, training loss: 6.34621000289917 = 0.02752634882926941 + 1.0 * 6.318683624267578
Epoch 950, val loss: 0.7350128293037415
Epoch 960, training loss: 6.342550754547119 = 0.02664560079574585 + 1.0 * 6.3159050941467285
Epoch 960, val loss: 0.7385662794113159
Epoch 970, training loss: 6.349473476409912 = 0.025808060541749 + 1.0 * 6.323665618896484
Epoch 970, val loss: 0.7421021461486816
Epoch 980, training loss: 6.342479705810547 = 0.02500893734395504 + 1.0 * 6.317470550537109
Epoch 980, val loss: 0.7456323504447937
Epoch 990, training loss: 6.335216045379639 = 0.024250928312540054 + 1.0 * 6.310965061187744
Epoch 990, val loss: 0.7489969730377197
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8418555614127571
The final CL Acc:0.79136, 0.03205, The final GNN Acc:0.83940, 0.00174
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11676])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10618])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.554859161376953 = 1.9579893350601196 + 1.0 * 8.596869468688965
Epoch 0, val loss: 1.962035894393921
Epoch 10, training loss: 10.544084548950195 = 1.9473811388015747 + 1.0 * 8.59670352935791
Epoch 10, val loss: 1.9514386653900146
Epoch 20, training loss: 10.529823303222656 = 1.9344274997711182 + 1.0 * 8.595396041870117
Epoch 20, val loss: 1.9380135536193848
Epoch 30, training loss: 10.501253128051758 = 1.91653573513031 + 1.0 * 8.584717750549316
Epoch 30, val loss: 1.918950080871582
Epoch 40, training loss: 10.419900894165039 = 1.891968846321106 + 1.0 * 8.527932167053223
Epoch 40, val loss: 1.8931201696395874
Epoch 50, training loss: 10.120299339294434 = 1.8645490407943726 + 1.0 * 8.25575065612793
Epoch 50, val loss: 1.8659549951553345
Epoch 60, training loss: 9.909516334533691 = 1.838492751121521 + 1.0 * 8.071023941040039
Epoch 60, val loss: 1.8426928520202637
Epoch 70, training loss: 9.56255054473877 = 1.816802740097046 + 1.0 * 7.745748043060303
Epoch 70, val loss: 1.8235241174697876
Epoch 80, training loss: 9.128608703613281 = 1.800525188446045 + 1.0 * 7.328083038330078
Epoch 80, val loss: 1.8091129064559937
Epoch 90, training loss: 8.9164457321167 = 1.7865139245986938 + 1.0 * 7.129931926727295
Epoch 90, val loss: 1.796135425567627
Epoch 100, training loss: 8.747642517089844 = 1.7713162899017334 + 1.0 * 6.976325988769531
Epoch 100, val loss: 1.781815528869629
Epoch 110, training loss: 8.624910354614258 = 1.7571039199829102 + 1.0 * 6.8678059577941895
Epoch 110, val loss: 1.7688870429992676
Epoch 120, training loss: 8.542906761169434 = 1.7424861192703247 + 1.0 * 6.80042028427124
Epoch 120, val loss: 1.7562510967254639
Epoch 130, training loss: 8.476451873779297 = 1.7257232666015625 + 1.0 * 6.750728130340576
Epoch 130, val loss: 1.7421661615371704
Epoch 140, training loss: 8.419454574584961 = 1.7064095735549927 + 1.0 * 6.7130446434021
Epoch 140, val loss: 1.7261722087860107
Epoch 150, training loss: 8.367268562316895 = 1.6842435598373413 + 1.0 * 6.683025360107422
Epoch 150, val loss: 1.7079172134399414
Epoch 160, training loss: 8.319818496704102 = 1.6586267948150635 + 1.0 * 6.661191940307617
Epoch 160, val loss: 1.6869913339614868
Epoch 170, training loss: 8.266335487365723 = 1.6292508840560913 + 1.0 * 6.6370849609375
Epoch 170, val loss: 1.6628789901733398
Epoch 180, training loss: 8.213737487792969 = 1.595554232597351 + 1.0 * 6.618183612823486
Epoch 180, val loss: 1.6351675987243652
Epoch 190, training loss: 8.159734725952148 = 1.55708646774292 + 1.0 * 6.602648735046387
Epoch 190, val loss: 1.603855013847351
Epoch 200, training loss: 8.100667953491211 = 1.5140786170959473 + 1.0 * 6.586589336395264
Epoch 200, val loss: 1.5689069032669067
Epoch 210, training loss: 8.038894653320312 = 1.4665495157241821 + 1.0 * 6.572344779968262
Epoch 210, val loss: 1.5305083990097046
Epoch 220, training loss: 7.975154876708984 = 1.4148792028427124 + 1.0 * 6.560275554656982
Epoch 220, val loss: 1.4891433715820312
Epoch 230, training loss: 7.910525798797607 = 1.3606926202774048 + 1.0 * 6.549833297729492
Epoch 230, val loss: 1.4461913108825684
Epoch 240, training loss: 7.843455791473389 = 1.3058648109436035 + 1.0 * 6.537590980529785
Epoch 240, val loss: 1.4030567407608032
Epoch 250, training loss: 7.775968551635742 = 1.2507827281951904 + 1.0 * 6.525185585021973
Epoch 250, val loss: 1.3603298664093018
Epoch 260, training loss: 7.710827350616455 = 1.1960102319717407 + 1.0 * 6.514817237854004
Epoch 260, val loss: 1.3186718225479126
Epoch 270, training loss: 7.652997970581055 = 1.1427202224731445 + 1.0 * 6.51027774810791
Epoch 270, val loss: 1.2790952920913696
Epoch 280, training loss: 7.591307640075684 = 1.0923088788986206 + 1.0 * 6.498998641967773
Epoch 280, val loss: 1.2426981925964355
Epoch 290, training loss: 7.535212516784668 = 1.0445468425750732 + 1.0 * 6.490665435791016
Epoch 290, val loss: 1.2093076705932617
Epoch 300, training loss: 7.485300064086914 = 0.998896062374115 + 1.0 * 6.486403942108154
Epoch 300, val loss: 1.178360104560852
Epoch 310, training loss: 7.436369895935059 = 0.955670177936554 + 1.0 * 6.48069953918457
Epoch 310, val loss: 1.1499581336975098
Epoch 320, training loss: 7.385498523712158 = 0.9146823883056641 + 1.0 * 6.470816135406494
Epoch 320, val loss: 1.1238549947738647
Epoch 330, training loss: 7.340933799743652 = 0.8754546642303467 + 1.0 * 6.465478897094727
Epoch 330, val loss: 1.099474310874939
Epoch 340, training loss: 7.300506114959717 = 0.8380290269851685 + 1.0 * 6.462477207183838
Epoch 340, val loss: 1.0768704414367676
Epoch 350, training loss: 7.259027481079102 = 0.8026865124702454 + 1.0 * 6.456340789794922
Epoch 350, val loss: 1.0562877655029297
Epoch 360, training loss: 7.220748424530029 = 0.7691076397895813 + 1.0 * 6.451640605926514
Epoch 360, val loss: 1.0373029708862305
Epoch 370, training loss: 7.190765380859375 = 0.7371400594711304 + 1.0 * 6.453625202178955
Epoch 370, val loss: 1.0199918746948242
Epoch 380, training loss: 7.14959716796875 = 0.7070485353469849 + 1.0 * 6.442548751831055
Epoch 380, val loss: 1.004413366317749
Epoch 390, training loss: 7.116180419921875 = 0.6783999800682068 + 1.0 * 6.437780380249023
Epoch 390, val loss: 0.9903231859207153
Epoch 400, training loss: 7.087357521057129 = 0.6509814262390137 + 1.0 * 6.436376094818115
Epoch 400, val loss: 0.9776386618614197
Epoch 410, training loss: 7.05987548828125 = 0.6250065565109253 + 1.0 * 6.434868812561035
Epoch 410, val loss: 0.9664364457130432
Epoch 420, training loss: 7.027851581573486 = 0.6004044413566589 + 1.0 * 6.427447319030762
Epoch 420, val loss: 0.956568717956543
Epoch 430, training loss: 6.999103546142578 = 0.5768867135047913 + 1.0 * 6.422216892242432
Epoch 430, val loss: 0.9479093551635742
Epoch 440, training loss: 6.986316680908203 = 0.5543539524078369 + 1.0 * 6.431962966918945
Epoch 440, val loss: 0.9404125213623047
Epoch 450, training loss: 6.95245361328125 = 0.5330952405929565 + 1.0 * 6.419358253479004
Epoch 450, val loss: 0.934071958065033
Epoch 460, training loss: 6.925037384033203 = 0.5127952098846436 + 1.0 * 6.4122419357299805
Epoch 460, val loss: 0.9287405610084534
Epoch 470, training loss: 6.901322841644287 = 0.4932718575000763 + 1.0 * 6.408051013946533
Epoch 470, val loss: 0.9242677092552185
Epoch 480, training loss: 6.8994574546813965 = 0.4744510352611542 + 1.0 * 6.42500638961792
Epoch 480, val loss: 0.9206113219261169
Epoch 490, training loss: 6.861229419708252 = 0.4565414488315582 + 1.0 * 6.404687881469727
Epoch 490, val loss: 0.9177361130714417
Epoch 500, training loss: 6.839356422424316 = 0.43933674693107605 + 1.0 * 6.400019645690918
Epoch 500, val loss: 0.9154701232910156
Epoch 510, training loss: 6.819828033447266 = 0.4226806163787842 + 1.0 * 6.3971476554870605
Epoch 510, val loss: 0.9138553142547607
Epoch 520, training loss: 6.8141045570373535 = 0.4065459966659546 + 1.0 * 6.407558441162109
Epoch 520, val loss: 0.9127863645553589
Epoch 530, training loss: 6.785567760467529 = 0.39105889201164246 + 1.0 * 6.3945088386535645
Epoch 530, val loss: 0.9122185707092285
Epoch 540, training loss: 6.767211437225342 = 0.37602874636650085 + 1.0 * 6.391182899475098
Epoch 540, val loss: 0.9120450019836426
Epoch 550, training loss: 6.7486066818237305 = 0.36138176918029785 + 1.0 * 6.387225151062012
Epoch 550, val loss: 0.9123170375823975
Epoch 560, training loss: 6.741112232208252 = 0.34716230630874634 + 1.0 * 6.39394998550415
Epoch 560, val loss: 0.9130908846855164
Epoch 570, training loss: 6.720470428466797 = 0.33347612619400024 + 1.0 * 6.386994361877441
Epoch 570, val loss: 0.9142027497291565
Epoch 580, training loss: 6.702695369720459 = 0.32022586464881897 + 1.0 * 6.382469654083252
Epoch 580, val loss: 0.9156672954559326
Epoch 590, training loss: 6.6871795654296875 = 0.30732735991477966 + 1.0 * 6.379852294921875
Epoch 590, val loss: 0.9175103902816772
Epoch 600, training loss: 6.683040618896484 = 0.2947975695133209 + 1.0 * 6.388243198394775
Epoch 600, val loss: 0.9197653532028198
Epoch 610, training loss: 6.6613264083862305 = 0.28264087438583374 + 1.0 * 6.378685474395752
Epoch 610, val loss: 0.9223693013191223
Epoch 620, training loss: 6.646125316619873 = 0.2708454430103302 + 1.0 * 6.375279903411865
Epoch 620, val loss: 0.9253829121589661
Epoch 630, training loss: 6.632833957672119 = 0.25935518741607666 + 1.0 * 6.373478889465332
Epoch 630, val loss: 0.9288008809089661
Epoch 640, training loss: 6.628032684326172 = 0.24822112917900085 + 1.0 * 6.379811763763428
Epoch 640, val loss: 0.9326396584510803
Epoch 650, training loss: 6.606997489929199 = 0.23751290142536163 + 1.0 * 6.3694844245910645
Epoch 650, val loss: 0.9367570877075195
Epoch 660, training loss: 6.595544338226318 = 0.2271207571029663 + 1.0 * 6.3684234619140625
Epoch 660, val loss: 0.9412816762924194
Epoch 670, training loss: 6.5867180824279785 = 0.2170540690422058 + 1.0 * 6.369664192199707
Epoch 670, val loss: 0.9461525678634644
Epoch 680, training loss: 6.573973178863525 = 0.20731966197490692 + 1.0 * 6.3666534423828125
Epoch 680, val loss: 0.9513707756996155
Epoch 690, training loss: 6.569279193878174 = 0.1978982836008072 + 1.0 * 6.371380805969238
Epoch 690, val loss: 0.9570063948631287
Epoch 700, training loss: 6.550815582275391 = 0.18884435296058655 + 1.0 * 6.361971378326416
Epoch 700, val loss: 0.9629177451133728
Epoch 710, training loss: 6.538655757904053 = 0.1800721138715744 + 1.0 * 6.358583450317383
Epoch 710, val loss: 0.9692486524581909
Epoch 720, training loss: 6.536032199859619 = 0.17158536612987518 + 1.0 * 6.364446640014648
Epoch 720, val loss: 0.9759050011634827
Epoch 730, training loss: 6.52846622467041 = 0.16348741948604584 + 1.0 * 6.364978790283203
Epoch 730, val loss: 0.9829021096229553
Epoch 740, training loss: 6.511736869812012 = 0.15570251643657684 + 1.0 * 6.356034278869629
Epoch 740, val loss: 0.9901840686798096
Epoch 750, training loss: 6.501591205596924 = 0.14819815754890442 + 1.0 * 6.353393077850342
Epoch 750, val loss: 0.99775230884552
Epoch 760, training loss: 6.492558479309082 = 0.14097464084625244 + 1.0 * 6.351583957672119
Epoch 760, val loss: 1.0057212114334106
Epoch 770, training loss: 6.51047945022583 = 0.13404500484466553 + 1.0 * 6.376434326171875
Epoch 770, val loss: 1.0139943361282349
Epoch 780, training loss: 6.484663009643555 = 0.12746132910251617 + 1.0 * 6.35720157623291
Epoch 780, val loss: 1.0225334167480469
Epoch 790, training loss: 6.471884727478027 = 0.12118873000144958 + 1.0 * 6.350696086883545
Epoch 790, val loss: 1.0312368869781494
Epoch 800, training loss: 6.4657673835754395 = 0.11518989503383636 + 1.0 * 6.350577354431152
Epoch 800, val loss: 1.0401387214660645
Epoch 810, training loss: 6.4557719230651855 = 0.1094985380768776 + 1.0 * 6.346273422241211
Epoch 810, val loss: 1.049250602722168
Epoch 820, training loss: 6.451653480529785 = 0.10410531610250473 + 1.0 * 6.347548007965088
Epoch 820, val loss: 1.0585763454437256
Epoch 830, training loss: 6.443778038024902 = 0.09897924214601517 + 1.0 * 6.344798564910889
Epoch 830, val loss: 1.0679963827133179
Epoch 840, training loss: 6.4382781982421875 = 0.09411296248435974 + 1.0 * 6.344165325164795
Epoch 840, val loss: 1.0775482654571533
Epoch 850, training loss: 6.438196659088135 = 0.08950169384479523 + 1.0 * 6.348694801330566
Epoch 850, val loss: 1.0871808528900146
Epoch 860, training loss: 6.427223205566406 = 0.08517415821552277 + 1.0 * 6.3420491218566895
Epoch 860, val loss: 1.0968886613845825
Epoch 870, training loss: 6.423390865325928 = 0.08107487112283707 + 1.0 * 6.342316150665283
Epoch 870, val loss: 1.1066126823425293
Epoch 880, training loss: 6.420087814331055 = 0.07720724493265152 + 1.0 * 6.342880725860596
Epoch 880, val loss: 1.1163078546524048
Epoch 890, training loss: 6.411294937133789 = 0.07355353981256485 + 1.0 * 6.337741374969482
Epoch 890, val loss: 1.1260323524475098
Epoch 900, training loss: 6.407790660858154 = 0.07009916007518768 + 1.0 * 6.337691307067871
Epoch 900, val loss: 1.1357899904251099
Epoch 910, training loss: 6.410581588745117 = 0.06684986501932144 + 1.0 * 6.343731880187988
Epoch 910, val loss: 1.1454720497131348
Epoch 920, training loss: 6.399215221405029 = 0.06379145383834839 + 1.0 * 6.335423946380615
Epoch 920, val loss: 1.1549599170684814
Epoch 930, training loss: 6.397754192352295 = 0.060931526124477386 + 1.0 * 6.336822509765625
Epoch 930, val loss: 1.1644504070281982
Epoch 940, training loss: 6.39672327041626 = 0.0582258477807045 + 1.0 * 6.338497638702393
Epoch 940, val loss: 1.1737574338912964
Epoch 950, training loss: 6.388955116271973 = 0.05567812919616699 + 1.0 * 6.333277225494385
Epoch 950, val loss: 1.1830121278762817
Epoch 960, training loss: 6.384243965148926 = 0.05327214300632477 + 1.0 * 6.330971717834473
Epoch 960, val loss: 1.192254662513733
Epoch 970, training loss: 6.383746147155762 = 0.05100109428167343 + 1.0 * 6.33274507522583
Epoch 970, val loss: 1.201480746269226
Epoch 980, training loss: 6.38215970993042 = 0.04885924980044365 + 1.0 * 6.333300590515137
Epoch 980, val loss: 1.2104432582855225
Epoch 990, training loss: 6.379439830780029 = 0.0468386746942997 + 1.0 * 6.332601070404053
Epoch 990, val loss: 1.219291090965271
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 10.546960830688477 = 1.9501116275787354 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.9514122009277344
Epoch 10, training loss: 10.537399291992188 = 1.94075345993042 + 1.0 * 8.59664535522461
Epoch 10, val loss: 1.9425550699234009
Epoch 20, training loss: 10.524381637573242 = 1.9292727708816528 + 1.0 * 8.595108985900879
Epoch 20, val loss: 1.931440830230713
Epoch 30, training loss: 10.495591163635254 = 1.913185954093933 + 1.0 * 8.582405090332031
Epoch 30, val loss: 1.9157425165176392
Epoch 40, training loss: 10.396693229675293 = 1.8911857604980469 + 1.0 * 8.505507469177246
Epoch 40, val loss: 1.894931674003601
Epoch 50, training loss: 10.012471199035645 = 1.867319941520691 + 1.0 * 8.145151138305664
Epoch 50, val loss: 1.8733861446380615
Epoch 60, training loss: 9.648730278015137 = 1.8470308780670166 + 1.0 * 7.801699161529541
Epoch 60, val loss: 1.8551201820373535
Epoch 70, training loss: 9.141694068908691 = 1.831554889678955 + 1.0 * 7.310139179229736
Epoch 70, val loss: 1.8406753540039062
Epoch 80, training loss: 8.865340232849121 = 1.8197201490402222 + 1.0 * 7.045619964599609
Epoch 80, val loss: 1.8290761709213257
Epoch 90, training loss: 8.729229927062988 = 1.8053301572799683 + 1.0 * 6.923900127410889
Epoch 90, val loss: 1.815557599067688
Epoch 100, training loss: 8.622127532958984 = 1.7890033721923828 + 1.0 * 6.83312463760376
Epoch 100, val loss: 1.8010939359664917
Epoch 110, training loss: 8.540262222290039 = 1.7735846042633057 + 1.0 * 6.7666778564453125
Epoch 110, val loss: 1.7875268459320068
Epoch 120, training loss: 8.47262954711914 = 1.758583426475525 + 1.0 * 6.714046001434326
Epoch 120, val loss: 1.7741767168045044
Epoch 130, training loss: 8.413482666015625 = 1.7428898811340332 + 1.0 * 6.670592308044434
Epoch 130, val loss: 1.760612964630127
Epoch 140, training loss: 8.360596656799316 = 1.7252552509307861 + 1.0 * 6.635341644287109
Epoch 140, val loss: 1.7459394931793213
Epoch 150, training loss: 8.313789367675781 = 1.7050262689590454 + 1.0 * 6.608762741088867
Epoch 150, val loss: 1.7295840978622437
Epoch 160, training loss: 8.267086029052734 = 1.6819791793823242 + 1.0 * 6.585107326507568
Epoch 160, val loss: 1.7111401557922363
Epoch 170, training loss: 8.221223831176758 = 1.6554265022277832 + 1.0 * 6.565797328948975
Epoch 170, val loss: 1.689895749092102
Epoch 180, training loss: 8.175862312316895 = 1.6251777410507202 + 1.0 * 6.550684452056885
Epoch 180, val loss: 1.6656885147094727
Epoch 190, training loss: 8.125898361206055 = 1.5912021398544312 + 1.0 * 6.534696102142334
Epoch 190, val loss: 1.6383750438690186
Epoch 200, training loss: 8.074028015136719 = 1.5530476570129395 + 1.0 * 6.520979881286621
Epoch 200, val loss: 1.6077046394348145
Epoch 210, training loss: 8.020000457763672 = 1.5112195014953613 + 1.0 * 6.5087809562683105
Epoch 210, val loss: 1.574101448059082
Epoch 220, training loss: 7.964852333068848 = 1.4669880867004395 + 1.0 * 6.497864246368408
Epoch 220, val loss: 1.5385220050811768
Epoch 230, training loss: 7.912406921386719 = 1.4207470417022705 + 1.0 * 6.491660118103027
Epoch 230, val loss: 1.5015207529067993
Epoch 240, training loss: 7.853204250335693 = 1.3740581274032593 + 1.0 * 6.4791460037231445
Epoch 240, val loss: 1.4642882347106934
Epoch 250, training loss: 7.798098564147949 = 1.3271417617797852 + 1.0 * 6.470956802368164
Epoch 250, val loss: 1.427123785018921
Epoch 260, training loss: 7.750251293182373 = 1.2803460359573364 + 1.0 * 6.469905376434326
Epoch 260, val loss: 1.3907380104064941
Epoch 270, training loss: 7.692712306976318 = 1.2344541549682617 + 1.0 * 6.458258152008057
Epoch 270, val loss: 1.3554644584655762
Epoch 280, training loss: 7.644646644592285 = 1.1890645027160645 + 1.0 * 6.455582141876221
Epoch 280, val loss: 1.3212918043136597
Epoch 290, training loss: 7.589639663696289 = 1.1444027423858643 + 1.0 * 6.445236682891846
Epoch 290, val loss: 1.288429856300354
Epoch 300, training loss: 7.53945255279541 = 1.0999201536178589 + 1.0 * 6.439532279968262
Epoch 300, val loss: 1.2562165260314941
Epoch 310, training loss: 7.503762245178223 = 1.0554563999176025 + 1.0 * 6.448306083679199
Epoch 310, val loss: 1.2246214151382446
Epoch 320, training loss: 7.444789886474609 = 1.0119221210479736 + 1.0 * 6.432868003845215
Epoch 320, val loss: 1.1941596269607544
Epoch 330, training loss: 7.3960442543029785 = 0.9690276384353638 + 1.0 * 6.427016735076904
Epoch 330, val loss: 1.1646857261657715
Epoch 340, training loss: 7.3480610847473145 = 0.9265729188919067 + 1.0 * 6.421488285064697
Epoch 340, val loss: 1.1358985900878906
Epoch 350, training loss: 7.3020710945129395 = 0.8847156167030334 + 1.0 * 6.417355537414551
Epoch 350, val loss: 1.1079461574554443
Epoch 360, training loss: 7.26767110824585 = 0.8437363505363464 + 1.0 * 6.4239349365234375
Epoch 360, val loss: 1.0811114311218262
Epoch 370, training loss: 7.221157550811768 = 0.8046205639839172 + 1.0 * 6.416536808013916
Epoch 370, val loss: 1.0557799339294434
Epoch 380, training loss: 7.174996376037598 = 0.7672996520996094 + 1.0 * 6.407696723937988
Epoch 380, val loss: 1.0322622060775757
Epoch 390, training loss: 7.135250568389893 = 0.7315378785133362 + 1.0 * 6.403712749481201
Epoch 390, val loss: 1.010195016860962
Epoch 400, training loss: 7.098617076873779 = 0.697228729724884 + 1.0 * 6.401388168334961
Epoch 400, val loss: 0.9894884824752808
Epoch 410, training loss: 7.065337181091309 = 0.664556622505188 + 1.0 * 6.40078067779541
Epoch 410, val loss: 0.9702413082122803
Epoch 420, training loss: 7.029225826263428 = 0.6335347890853882 + 1.0 * 6.39569091796875
Epoch 420, val loss: 0.9525361061096191
Epoch 430, training loss: 6.995871067047119 = 0.6038946509361267 + 1.0 * 6.391976356506348
Epoch 430, val loss: 0.936127781867981
Epoch 440, training loss: 6.9686689376831055 = 0.5754417777061462 + 1.0 * 6.3932271003723145
Epoch 440, val loss: 0.9207366108894348
Epoch 450, training loss: 6.93769645690918 = 0.5481203198432922 + 1.0 * 6.389575958251953
Epoch 450, val loss: 0.9064626097679138
Epoch 460, training loss: 6.910006046295166 = 0.5218192338943481 + 1.0 * 6.388186931610107
Epoch 460, val loss: 0.8931549787521362
Epoch 470, training loss: 6.880801677703857 = 0.49629929661750793 + 1.0 * 6.384502410888672
Epoch 470, val loss: 0.8807112574577332
Epoch 480, training loss: 6.85386848449707 = 0.4713849723339081 + 1.0 * 6.38248348236084
Epoch 480, val loss: 0.8689211010932922
Epoch 490, training loss: 6.825900077819824 = 0.44696131348609924 + 1.0 * 6.378938674926758
Epoch 490, val loss: 0.8577205538749695
Epoch 500, training loss: 6.800388813018799 = 0.4229510724544525 + 1.0 * 6.377437591552734
Epoch 500, val loss: 0.8471306562423706
Epoch 510, training loss: 6.773305892944336 = 0.3992707431316376 + 1.0 * 6.374035358428955
Epoch 510, val loss: 0.8371213674545288
Epoch 520, training loss: 6.7521772384643555 = 0.37592971324920654 + 1.0 * 6.376247406005859
Epoch 520, val loss: 0.8275970220565796
Epoch 530, training loss: 6.728001594543457 = 0.3531113862991333 + 1.0 * 6.374890327453613
Epoch 530, val loss: 0.818694531917572
Epoch 540, training loss: 6.698751449584961 = 0.33089661598205566 + 1.0 * 6.367855072021484
Epoch 540, val loss: 0.8105581998825073
Epoch 550, training loss: 6.67539119720459 = 0.309249609708786 + 1.0 * 6.3661417961120605
Epoch 550, val loss: 0.8030358552932739
Epoch 560, training loss: 6.662512302398682 = 0.28825387358665466 + 1.0 * 6.374258518218994
Epoch 560, val loss: 0.7961888909339905
Epoch 570, training loss: 6.632122993469238 = 0.26820704340934753 + 1.0 * 6.363915920257568
Epoch 570, val loss: 0.789961576461792
Epoch 580, training loss: 6.612390041351318 = 0.2491157501935959 + 1.0 * 6.363274097442627
Epoch 580, val loss: 0.7846176028251648
Epoch 590, training loss: 6.596444129943848 = 0.2310195118188858 + 1.0 * 6.365424633026123
Epoch 590, val loss: 0.7800420522689819
Epoch 600, training loss: 6.572639465332031 = 0.21406956017017365 + 1.0 * 6.358570098876953
Epoch 600, val loss: 0.7762583494186401
Epoch 610, training loss: 6.555586338043213 = 0.19825027883052826 + 1.0 * 6.357336044311523
Epoch 610, val loss: 0.7733297348022461
Epoch 620, training loss: 6.548949241638184 = 0.18354152143001556 + 1.0 * 6.365407943725586
Epoch 620, val loss: 0.7712189555168152
Epoch 630, training loss: 6.527608871459961 = 0.17003732919692993 + 1.0 * 6.357571601867676
Epoch 630, val loss: 0.7696920037269592
Epoch 640, training loss: 6.51149845123291 = 0.1576167196035385 + 1.0 * 6.3538818359375
Epoch 640, val loss: 0.7691696286201477
Epoch 650, training loss: 6.497375965118408 = 0.14625504612922668 + 1.0 * 6.351120948791504
Epoch 650, val loss: 0.7693083882331848
Epoch 660, training loss: 6.485723495483398 = 0.13586346805095673 + 1.0 * 6.349860191345215
Epoch 660, val loss: 0.7700698375701904
Epoch 670, training loss: 6.479397296905518 = 0.1263328194618225 + 1.0 * 6.35306453704834
Epoch 670, val loss: 0.7715602517127991
Epoch 680, training loss: 6.468985080718994 = 0.11766470968723297 + 1.0 * 6.351320266723633
Epoch 680, val loss: 0.7734108567237854
Epoch 690, training loss: 6.45509672164917 = 0.1097453162074089 + 1.0 * 6.345351219177246
Epoch 690, val loss: 0.7758512496948242
Epoch 700, training loss: 6.449268341064453 = 0.1025024950504303 + 1.0 * 6.346765995025635
Epoch 700, val loss: 0.7787245512008667
Epoch 710, training loss: 6.441181659698486 = 0.09588242322206497 + 1.0 * 6.345299243927002
Epoch 710, val loss: 0.781895637512207
Epoch 720, training loss: 6.433224678039551 = 0.08984291553497314 + 1.0 * 6.343381881713867
Epoch 720, val loss: 0.7854095697402954
Epoch 730, training loss: 6.423980712890625 = 0.08430838584899902 + 1.0 * 6.339672088623047
Epoch 730, val loss: 0.789191722869873
Epoch 740, training loss: 6.41802978515625 = 0.07923934608697891 + 1.0 * 6.338790416717529
Epoch 740, val loss: 0.793272852897644
Epoch 750, training loss: 6.415831565856934 = 0.07458457350730896 + 1.0 * 6.341247081756592
Epoch 750, val loss: 0.7974666357040405
Epoch 760, training loss: 6.405484676361084 = 0.07032083719968796 + 1.0 * 6.3351640701293945
Epoch 760, val loss: 0.8018349409103394
Epoch 770, training loss: 6.400045871734619 = 0.06638772040605545 + 1.0 * 6.333658218383789
Epoch 770, val loss: 0.8063268065452576
Epoch 780, training loss: 6.419535160064697 = 0.06276686489582062 + 1.0 * 6.3567681312561035
Epoch 780, val loss: 0.810783326625824
Epoch 790, training loss: 6.39263916015625 = 0.0594453290104866 + 1.0 * 6.333193778991699
Epoch 790, val loss: 0.8154579997062683
Epoch 800, training loss: 6.386751174926758 = 0.05638734996318817 + 1.0 * 6.330363750457764
Epoch 800, val loss: 0.820244312286377
Epoch 810, training loss: 6.382720470428467 = 0.053548239171504974 + 1.0 * 6.329172134399414
Epoch 810, val loss: 0.8250504732131958
Epoch 820, training loss: 6.3789143562316895 = 0.05090765282511711 + 1.0 * 6.328006744384766
Epoch 820, val loss: 0.8298611640930176
Epoch 830, training loss: 6.389997959136963 = 0.048451077193021774 + 1.0 * 6.341547012329102
Epoch 830, val loss: 0.8346307277679443
Epoch 840, training loss: 6.372084140777588 = 0.04618207365274429 + 1.0 * 6.325901985168457
Epoch 840, val loss: 0.8393948674201965
Epoch 850, training loss: 6.3701090812683105 = 0.04407748952507973 + 1.0 * 6.326031684875488
Epoch 850, val loss: 0.8442674279212952
Epoch 860, training loss: 6.365725994110107 = 0.04210764914751053 + 1.0 * 6.323618412017822
Epoch 860, val loss: 0.8491516709327698
Epoch 870, training loss: 6.364129543304443 = 0.040261417627334595 + 1.0 * 6.323868274688721
Epoch 870, val loss: 0.8539586663246155
Epoch 880, training loss: 6.362368106842041 = 0.038534872233867645 + 1.0 * 6.323833465576172
Epoch 880, val loss: 0.8587455749511719
Epoch 890, training loss: 6.3618083000183105 = 0.03692489117383957 + 1.0 * 6.324883460998535
Epoch 890, val loss: 0.8635097146034241
Epoch 900, training loss: 6.361718654632568 = 0.03541706129908562 + 1.0 * 6.326301574707031
Epoch 900, val loss: 0.8681919574737549
Epoch 910, training loss: 6.355097770690918 = 0.0340082161128521 + 1.0 * 6.321089744567871
Epoch 910, val loss: 0.8728652596473694
Epoch 920, training loss: 6.351402759552002 = 0.03268450126051903 + 1.0 * 6.318718433380127
Epoch 920, val loss: 0.8775937557220459
Epoch 930, training loss: 6.34831428527832 = 0.03143341839313507 + 1.0 * 6.316880702972412
Epoch 930, val loss: 0.882222056388855
Epoch 940, training loss: 6.349534511566162 = 0.03024909272789955 + 1.0 * 6.3192853927612305
Epoch 940, val loss: 0.8868193626403809
Epoch 950, training loss: 6.344188690185547 = 0.02913159877061844 + 1.0 * 6.315057277679443
Epoch 950, val loss: 0.891375720500946
Epoch 960, training loss: 6.3489813804626465 = 0.028080258518457413 + 1.0 * 6.320900917053223
Epoch 960, val loss: 0.8959802985191345
Epoch 970, training loss: 6.345346927642822 = 0.02709142118692398 + 1.0 * 6.318255424499512
Epoch 970, val loss: 0.9003610610961914
Epoch 980, training loss: 6.340385913848877 = 0.02615630067884922 + 1.0 * 6.314229488372803
Epoch 980, val loss: 0.9047381281852722
Epoch 990, training loss: 6.3374924659729 = 0.025271745398640633 + 1.0 * 6.312220573425293
Epoch 990, val loss: 0.9090951681137085
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 10.559714317321777 = 1.9628875255584717 + 1.0 * 8.596826553344727
Epoch 0, val loss: 1.9718080759048462
Epoch 10, training loss: 10.549055099487305 = 1.9525012969970703 + 1.0 * 8.596553802490234
Epoch 10, val loss: 1.961119294166565
Epoch 20, training loss: 10.534441947937012 = 1.940057635307312 + 1.0 * 8.59438419342041
Epoch 20, val loss: 1.9481037855148315
Epoch 30, training loss: 10.498923301696777 = 1.9228790998458862 + 1.0 * 8.576044082641602
Epoch 30, val loss: 1.9299477338790894
Epoch 40, training loss: 10.35110855102539 = 1.9001814126968384 + 1.0 * 8.450926780700684
Epoch 40, val loss: 1.9069173336029053
Epoch 50, training loss: 9.867897033691406 = 1.8755091428756714 + 1.0 * 7.992387771606445
Epoch 50, val loss: 1.8829072713851929
Epoch 60, training loss: 9.434066772460938 = 1.8571045398712158 + 1.0 * 7.576961994171143
Epoch 60, val loss: 1.865670084953308
Epoch 70, training loss: 9.117304801940918 = 1.843068242073059 + 1.0 * 7.274236679077148
Epoch 70, val loss: 1.8521782159805298
Epoch 80, training loss: 8.898256301879883 = 1.8281593322753906 + 1.0 * 7.070096492767334
Epoch 80, val loss: 1.8383982181549072
Epoch 90, training loss: 8.749958038330078 = 1.8112730979919434 + 1.0 * 6.938685417175293
Epoch 90, val loss: 1.823585033416748
Epoch 100, training loss: 8.652901649475098 = 1.794337272644043 + 1.0 * 6.858564376831055
Epoch 100, val loss: 1.8087975978851318
Epoch 110, training loss: 8.582584381103516 = 1.778429627418518 + 1.0 * 6.804154872894287
Epoch 110, val loss: 1.7948192358016968
Epoch 120, training loss: 8.519987106323242 = 1.7630072832107544 + 1.0 * 6.756979942321777
Epoch 120, val loss: 1.7812392711639404
Epoch 130, training loss: 8.461933135986328 = 1.7467833757400513 + 1.0 * 6.715149402618408
Epoch 130, val loss: 1.7673213481903076
Epoch 140, training loss: 8.408720016479492 = 1.7288899421691895 + 1.0 * 6.679830074310303
Epoch 140, val loss: 1.752454400062561
Epoch 150, training loss: 8.351713180541992 = 1.7089030742645264 + 1.0 * 6.642810344696045
Epoch 150, val loss: 1.73626708984375
Epoch 160, training loss: 8.300963401794434 = 1.685922384262085 + 1.0 * 6.6150407791137695
Epoch 160, val loss: 1.7177845239639282
Epoch 170, training loss: 8.250358581542969 = 1.6593512296676636 + 1.0 * 6.591007709503174
Epoch 170, val loss: 1.6964884996414185
Epoch 180, training loss: 8.198322296142578 = 1.6287169456481934 + 1.0 * 6.569605350494385
Epoch 180, val loss: 1.6719900369644165
Epoch 190, training loss: 8.146208763122559 = 1.5937273502349854 + 1.0 * 6.552481651306152
Epoch 190, val loss: 1.6439753770828247
Epoch 200, training loss: 8.089770317077637 = 1.5546181201934814 + 1.0 * 6.535152435302734
Epoch 200, val loss: 1.6127201318740845
Epoch 210, training loss: 8.032215118408203 = 1.511614203453064 + 1.0 * 6.520601272583008
Epoch 210, val loss: 1.5782665014266968
Epoch 220, training loss: 7.973660945892334 = 1.4654635190963745 + 1.0 * 6.50819730758667
Epoch 220, val loss: 1.5416247844696045
Epoch 230, training loss: 7.9148077964782715 = 1.417277455329895 + 1.0 * 6.497530460357666
Epoch 230, val loss: 1.5035640001296997
Epoch 240, training loss: 7.855871200561523 = 1.368283987045288 + 1.0 * 6.4875874519348145
Epoch 240, val loss: 1.4653339385986328
Epoch 250, training loss: 7.801300525665283 = 1.319090723991394 + 1.0 * 6.4822096824646
Epoch 250, val loss: 1.4274232387542725
Epoch 260, training loss: 7.743023872375488 = 1.270371675491333 + 1.0 * 6.472651958465576
Epoch 260, val loss: 1.3902959823608398
Epoch 270, training loss: 7.687808036804199 = 1.2220464944839478 + 1.0 * 6.465761661529541
Epoch 270, val loss: 1.3539174795150757
Epoch 280, training loss: 7.637380123138428 = 1.1748028993606567 + 1.0 * 6.4625773429870605
Epoch 280, val loss: 1.319053053855896
Epoch 290, training loss: 7.5835065841674805 = 1.1291953325271606 + 1.0 * 6.454311370849609
Epoch 290, val loss: 1.2860004901885986
Epoch 300, training loss: 7.5337815284729 = 1.084721565246582 + 1.0 * 6.449059963226318
Epoch 300, val loss: 1.2542849779129028
Epoch 310, training loss: 7.486143112182617 = 1.041573405265808 + 1.0 * 6.4445695877075195
Epoch 310, val loss: 1.224079966545105
Epoch 320, training loss: 7.440282821655273 = 0.999843180179596 + 1.0 * 6.440439701080322
Epoch 320, val loss: 1.1955372095108032
Epoch 330, training loss: 7.392571449279785 = 0.9595801830291748 + 1.0 * 6.4329915046691895
Epoch 330, val loss: 1.168397068977356
Epoch 340, training loss: 7.348815441131592 = 0.9205648303031921 + 1.0 * 6.428250789642334
Epoch 340, val loss: 1.14264976978302
Epoch 350, training loss: 7.310484409332275 = 0.8828240036964417 + 1.0 * 6.4276604652404785
Epoch 350, val loss: 1.1181362867355347
Epoch 360, training loss: 7.268777847290039 = 0.8465678691864014 + 1.0 * 6.422209739685059
Epoch 360, val loss: 1.0951660871505737
Epoch 370, training loss: 7.226892948150635 = 0.8116217255592346 + 1.0 * 6.415271282196045
Epoch 370, val loss: 1.073482632637024
Epoch 380, training loss: 7.190634727478027 = 0.7777145504951477 + 1.0 * 6.412919998168945
Epoch 380, val loss: 1.0530060529708862
Epoch 390, training loss: 7.155989170074463 = 0.7450585961341858 + 1.0 * 6.410930633544922
Epoch 390, val loss: 1.0337201356887817
Epoch 400, training loss: 7.118924617767334 = 0.7136227488517761 + 1.0 * 6.405302047729492
Epoch 400, val loss: 1.0158485174179077
Epoch 410, training loss: 7.085811138153076 = 0.6831561326980591 + 1.0 * 6.402655124664307
Epoch 410, val loss: 0.9991539716720581
Epoch 420, training loss: 7.056299686431885 = 0.6539300680160522 + 1.0 * 6.402369499206543
Epoch 420, val loss: 0.98367840051651
Epoch 430, training loss: 7.02023983001709 = 0.6259362697601318 + 1.0 * 6.394303321838379
Epoch 430, val loss: 0.9695364236831665
Epoch 440, training loss: 6.990324020385742 = 0.5988829135894775 + 1.0 * 6.391441345214844
Epoch 440, val loss: 0.9564539194107056
Epoch 450, training loss: 6.96500301361084 = 0.5729126334190369 + 1.0 * 6.392090320587158
Epoch 450, val loss: 0.9445177912712097
Epoch 460, training loss: 6.934510231018066 = 0.5480460524559021 + 1.0 * 6.3864641189575195
Epoch 460, val loss: 0.933718740940094
Epoch 470, training loss: 6.906728744506836 = 0.5240593552589417 + 1.0 * 6.382669448852539
Epoch 470, val loss: 0.9238030314445496
Epoch 480, training loss: 6.884993553161621 = 0.5008424520492554 + 1.0 * 6.384150981903076
Epoch 480, val loss: 0.9146941304206848
Epoch 490, training loss: 6.859216690063477 = 0.4787063002586365 + 1.0 * 6.380510330200195
Epoch 490, val loss: 0.9065114855766296
Epoch 500, training loss: 6.83442497253418 = 0.4574955105781555 + 1.0 * 6.37692928314209
Epoch 500, val loss: 0.8991279602050781
Epoch 510, training loss: 6.811319351196289 = 0.4370550811290741 + 1.0 * 6.374264240264893
Epoch 510, val loss: 0.8923425078392029
Epoch 520, training loss: 6.79110050201416 = 0.4172987937927246 + 1.0 * 6.3738017082214355
Epoch 520, val loss: 0.8861907124519348
Epoch 530, training loss: 6.768716812133789 = 0.39820370078086853 + 1.0 * 6.370512962341309
Epoch 530, val loss: 0.8805844187736511
Epoch 540, training loss: 6.755982875823975 = 0.3797627389431 + 1.0 * 6.376220226287842
Epoch 540, val loss: 0.8755368590354919
Epoch 550, training loss: 6.728351593017578 = 0.36204439401626587 + 1.0 * 6.366307258605957
Epoch 550, val loss: 0.870936393737793
Epoch 560, training loss: 6.708073616027832 = 0.34488359093666077 + 1.0 * 6.363190174102783
Epoch 560, val loss: 0.8667430281639099
Epoch 570, training loss: 6.699146747589111 = 0.328227698802948 + 1.0 * 6.370919227600098
Epoch 570, val loss: 0.8629845976829529
Epoch 580, training loss: 6.673479080200195 = 0.3121744990348816 + 1.0 * 6.361304759979248
Epoch 580, val loss: 0.859635055065155
Epoch 590, training loss: 6.6583380699157715 = 0.29665666818618774 + 1.0 * 6.3616814613342285
Epoch 590, val loss: 0.8566465973854065
Epoch 600, training loss: 6.639585971832275 = 0.2816702425479889 + 1.0 * 6.357915878295898
Epoch 600, val loss: 0.8541145920753479
Epoch 610, training loss: 6.622249603271484 = 0.2671928405761719 + 1.0 * 6.3550567626953125
Epoch 610, val loss: 0.8519384264945984
Epoch 620, training loss: 6.609965801239014 = 0.25314411520957947 + 1.0 * 6.356821537017822
Epoch 620, val loss: 0.8501929640769958
Epoch 630, training loss: 6.5936126708984375 = 0.2396070659160614 + 1.0 * 6.354005813598633
Epoch 630, val loss: 0.8489062190055847
Epoch 640, training loss: 6.582780361175537 = 0.22656461596488953 + 1.0 * 6.356215953826904
Epoch 640, val loss: 0.8479804992675781
Epoch 650, training loss: 6.563061714172363 = 0.2140749841928482 + 1.0 * 6.348986625671387
Epoch 650, val loss: 0.8474395275115967
Epoch 660, training loss: 6.55009651184082 = 0.20206891000270844 + 1.0 * 6.34802770614624
Epoch 660, val loss: 0.8473461270332336
Epoch 670, training loss: 6.542916774749756 = 0.1905478984117508 + 1.0 * 6.3523688316345215
Epoch 670, val loss: 0.8476911783218384
Epoch 680, training loss: 6.528718948364258 = 0.17956134676933289 + 1.0 * 6.349157810211182
Epoch 680, val loss: 0.8485617637634277
Epoch 690, training loss: 6.515034198760986 = 0.16913370788097382 + 1.0 * 6.345900535583496
Epoch 690, val loss: 0.8497709631919861
Epoch 700, training loss: 6.50203275680542 = 0.159272238612175 + 1.0 * 6.3427605628967285
Epoch 700, val loss: 0.851298451423645
Epoch 710, training loss: 6.489933013916016 = 0.14992274343967438 + 1.0 * 6.340010166168213
Epoch 710, val loss: 0.8532367944717407
Epoch 720, training loss: 6.490103244781494 = 0.14110156893730164 + 1.0 * 6.349001884460449
Epoch 720, val loss: 0.8555741310119629
Epoch 730, training loss: 6.475892543792725 = 0.1328320950269699 + 1.0 * 6.343060493469238
Epoch 730, val loss: 0.8583470582962036
Epoch 740, training loss: 6.462730407714844 = 0.12509849667549133 + 1.0 * 6.337631702423096
Epoch 740, val loss: 0.8611584901809692
Epoch 750, training loss: 6.460490703582764 = 0.11783690005540848 + 1.0 * 6.342653751373291
Epoch 750, val loss: 0.8643050789833069
Epoch 760, training loss: 6.448881149291992 = 0.11108886450529099 + 1.0 * 6.33779239654541
Epoch 760, val loss: 0.867643415927887
Epoch 770, training loss: 6.442723751068115 = 0.10476226359605789 + 1.0 * 6.337961673736572
Epoch 770, val loss: 0.8712198734283447
Epoch 780, training loss: 6.43295955657959 = 0.09885117411613464 + 1.0 * 6.334108352661133
Epoch 780, val loss: 0.8750230669975281
Epoch 790, training loss: 6.425943374633789 = 0.09332935512065887 + 1.0 * 6.332613945007324
Epoch 790, val loss: 0.8789578676223755
Epoch 800, training loss: 6.423638820648193 = 0.08817923814058304 + 1.0 * 6.3354597091674805
Epoch 800, val loss: 0.8831300735473633
Epoch 810, training loss: 6.415891647338867 = 0.0833750069141388 + 1.0 * 6.332516670227051
Epoch 810, val loss: 0.8873931765556335
Epoch 820, training loss: 6.412840843200684 = 0.07889247685670853 + 1.0 * 6.333948135375977
Epoch 820, val loss: 0.8918150067329407
Epoch 830, training loss: 6.406347274780273 = 0.07470549643039703 + 1.0 * 6.331641674041748
Epoch 830, val loss: 0.8963764309883118
Epoch 840, training loss: 6.3964619636535645 = 0.07082139700651169 + 1.0 * 6.325640678405762
Epoch 840, val loss: 0.9008086323738098
Epoch 850, training loss: 6.3927507400512695 = 0.0671837329864502 + 1.0 * 6.325567245483398
Epoch 850, val loss: 0.905393123626709
Epoch 860, training loss: 6.392308712005615 = 0.06378225982189178 + 1.0 * 6.328526496887207
Epoch 860, val loss: 0.9100814461708069
Epoch 870, training loss: 6.389591693878174 = 0.06060707941651344 + 1.0 * 6.32898473739624
Epoch 870, val loss: 0.914897084236145
Epoch 880, training loss: 6.3834757804870605 = 0.057663481682538986 + 1.0 * 6.325812339782715
Epoch 880, val loss: 0.9195166826248169
Epoch 890, training loss: 6.375539779663086 = 0.054897241294384 + 1.0 * 6.320642471313477
Epoch 890, val loss: 0.92429119348526
Epoch 900, training loss: 6.373891830444336 = 0.052311286330223083 + 1.0 * 6.321580410003662
Epoch 900, val loss: 0.9290722012519836
Epoch 910, training loss: 6.3684234619140625 = 0.049891091883182526 + 1.0 * 6.318532466888428
Epoch 910, val loss: 0.9339281320571899
Epoch 920, training loss: 6.364684104919434 = 0.047618795186281204 + 1.0 * 6.317065238952637
Epoch 920, val loss: 0.9386623501777649
Epoch 930, training loss: 6.368743896484375 = 0.045489612966775894 + 1.0 * 6.323254108428955
Epoch 930, val loss: 0.9434369206428528
Epoch 940, training loss: 6.365983486175537 = 0.04349113628268242 + 1.0 * 6.3224921226501465
Epoch 940, val loss: 0.948203980922699
Epoch 950, training loss: 6.358763694763184 = 0.0416269451379776 + 1.0 * 6.317136764526367
Epoch 950, val loss: 0.9529662132263184
Epoch 960, training loss: 6.354266166687012 = 0.039870504289865494 + 1.0 * 6.314395427703857
Epoch 960, val loss: 0.9574958086013794
Epoch 970, training loss: 6.356262683868408 = 0.03821997344493866 + 1.0 * 6.318042755126953
Epoch 970, val loss: 0.9621413350105286
Epoch 980, training loss: 6.350678443908691 = 0.0366695336997509 + 1.0 * 6.314008712768555
Epoch 980, val loss: 0.9667156934738159
Epoch 990, training loss: 6.348831653594971 = 0.03520452231168747 + 1.0 * 6.313627243041992
Epoch 990, val loss: 0.9712543487548828
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8149710068529257
The final CL Acc:0.77778, 0.00605, The final GNN Acc:0.81444, 0.00240
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13200])
remove edge: torch.Size([2, 7996])
updated graph: torch.Size([2, 10640])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.555953025817871 = 1.9590948820114136 + 1.0 * 8.596858024597168
Epoch 0, val loss: 1.9474833011627197
Epoch 10, training loss: 10.544866561889648 = 1.948174238204956 + 1.0 * 8.596692085266113
Epoch 10, val loss: 1.937280535697937
Epoch 20, training loss: 10.529748916625977 = 1.9343855381011963 + 1.0 * 8.59536361694336
Epoch 20, val loss: 1.9240248203277588
Epoch 30, training loss: 10.4989595413208 = 1.9145002365112305 + 1.0 * 8.58445930480957
Epoch 30, val loss: 1.9046355485916138
Epoch 40, training loss: 10.415017127990723 = 1.8862653970718384 + 1.0 * 8.528751373291016
Epoch 40, val loss: 1.8780361413955688
Epoch 50, training loss: 10.121907234191895 = 1.855049967765808 + 1.0 * 8.266857147216797
Epoch 50, val loss: 1.8504365682601929
Epoch 60, training loss: 9.731573104858398 = 1.826742172241211 + 1.0 * 7.904830455780029
Epoch 60, val loss: 1.827143907546997
Epoch 70, training loss: 9.230029106140137 = 1.8068244457244873 + 1.0 * 7.4232048988342285
Epoch 70, val loss: 1.8087905645370483
Epoch 80, training loss: 8.94096851348877 = 1.787400484085083 + 1.0 * 7.153567790985107
Epoch 80, val loss: 1.790906548500061
Epoch 90, training loss: 8.74079704284668 = 1.7678717374801636 + 1.0 * 6.972925662994385
Epoch 90, val loss: 1.7736456394195557
Epoch 100, training loss: 8.642383575439453 = 1.7476718425750732 + 1.0 * 6.894711494445801
Epoch 100, val loss: 1.7566616535186768
Epoch 110, training loss: 8.570391654968262 = 1.7258747816085815 + 1.0 * 6.844517230987549
Epoch 110, val loss: 1.7377729415893555
Epoch 120, training loss: 8.50297737121582 = 1.701570987701416 + 1.0 * 6.801405906677246
Epoch 120, val loss: 1.7159759998321533
Epoch 130, training loss: 8.43446159362793 = 1.6747492551803589 + 1.0 * 6.7597126960754395
Epoch 130, val loss: 1.6919384002685547
Epoch 140, training loss: 8.359755516052246 = 1.645095705986023 + 1.0 * 6.714659690856934
Epoch 140, val loss: 1.6662652492523193
Epoch 150, training loss: 8.28963565826416 = 1.6110060214996338 + 1.0 * 6.678629398345947
Epoch 150, val loss: 1.637334942817688
Epoch 160, training loss: 8.219581604003906 = 1.571882963180542 + 1.0 * 6.647698879241943
Epoch 160, val loss: 1.6039663553237915
Epoch 170, training loss: 8.15226936340332 = 1.529098629951477 + 1.0 * 6.623170852661133
Epoch 170, val loss: 1.567617416381836
Epoch 180, training loss: 8.084761619567871 = 1.4843608140945435 + 1.0 * 6.600400447845459
Epoch 180, val loss: 1.5299795866012573
Epoch 190, training loss: 8.018545150756836 = 1.4378330707550049 + 1.0 * 6.580711841583252
Epoch 190, val loss: 1.4909675121307373
Epoch 200, training loss: 7.9532976150512695 = 1.3900858163833618 + 1.0 * 6.563211917877197
Epoch 200, val loss: 1.4511867761611938
Epoch 210, training loss: 7.895684719085693 = 1.342443823814392 + 1.0 * 6.553240776062012
Epoch 210, val loss: 1.412219524383545
Epoch 220, training loss: 7.831051349639893 = 1.2968168258666992 + 1.0 * 6.534234523773193
Epoch 220, val loss: 1.3752036094665527
Epoch 230, training loss: 7.775230884552002 = 1.2525376081466675 + 1.0 * 6.522693157196045
Epoch 230, val loss: 1.339663028717041
Epoch 240, training loss: 7.7195916175842285 = 1.2093616724014282 + 1.0 * 6.51023006439209
Epoch 240, val loss: 1.3053829669952393
Epoch 250, training loss: 7.669940948486328 = 1.1671974658966064 + 1.0 * 6.502743244171143
Epoch 250, val loss: 1.2724155187606812
Epoch 260, training loss: 7.618750095367432 = 1.1265608072280884 + 1.0 * 6.492189407348633
Epoch 260, val loss: 1.2406854629516602
Epoch 270, training loss: 7.569185256958008 = 1.0867059230804443 + 1.0 * 6.482479572296143
Epoch 270, val loss: 1.2097809314727783
Epoch 280, training loss: 7.522324085235596 = 1.0471018552780151 + 1.0 * 6.475222110748291
Epoch 280, val loss: 1.179286241531372
Epoch 290, training loss: 7.48060417175293 = 1.0082061290740967 + 1.0 * 6.472397804260254
Epoch 290, val loss: 1.149388313293457
Epoch 300, training loss: 7.434686660766602 = 0.9704024791717529 + 1.0 * 6.4642839431762695
Epoch 300, val loss: 1.120658278465271
Epoch 310, training loss: 7.389297008514404 = 0.9331367611885071 + 1.0 * 6.456160068511963
Epoch 310, val loss: 1.0923629999160767
Epoch 320, training loss: 7.345842361450195 = 0.896212637424469 + 1.0 * 6.449629783630371
Epoch 320, val loss: 1.0644347667694092
Epoch 330, training loss: 7.314050674438477 = 0.859565794467926 + 1.0 * 6.454484939575195
Epoch 330, val loss: 1.0370638370513916
Epoch 340, training loss: 7.266217231750488 = 0.8239779472351074 + 1.0 * 6.442239284515381
Epoch 340, val loss: 1.0104281902313232
Epoch 350, training loss: 7.225620746612549 = 0.7891284823417664 + 1.0 * 6.436492443084717
Epoch 350, val loss: 0.9847162365913391
Epoch 360, training loss: 7.185563087463379 = 0.7549638748168945 + 1.0 * 6.430599212646484
Epoch 360, val loss: 0.959678053855896
Epoch 370, training loss: 7.165282249450684 = 0.7215766310691833 + 1.0 * 6.4437055587768555
Epoch 370, val loss: 0.935577929019928
Epoch 380, training loss: 7.1135101318359375 = 0.6895662546157837 + 1.0 * 6.423943996429443
Epoch 380, val loss: 0.913088858127594
Epoch 390, training loss: 7.078739643096924 = 0.6589058041572571 + 1.0 * 6.419833660125732
Epoch 390, val loss: 0.8920921683311462
Epoch 400, training loss: 7.0444231033325195 = 0.6295087337493896 + 1.0 * 6.414914131164551
Epoch 400, val loss: 0.8725047707557678
Epoch 410, training loss: 7.017849922180176 = 0.6012660264968872 + 1.0 * 6.416584014892578
Epoch 410, val loss: 0.8544032573699951
Epoch 420, training loss: 6.984181880950928 = 0.5745839476585388 + 1.0 * 6.409597873687744
Epoch 420, val loss: 0.8379245400428772
Epoch 430, training loss: 6.955610752105713 = 0.5492801666259766 + 1.0 * 6.406330585479736
Epoch 430, val loss: 0.8232466578483582
Epoch 440, training loss: 6.933531284332275 = 0.52521812915802 + 1.0 * 6.408313274383545
Epoch 440, val loss: 0.8100028038024902
Epoch 450, training loss: 6.907283782958984 = 0.5024093389511108 + 1.0 * 6.404874324798584
Epoch 450, val loss: 0.7983331680297852
Epoch 460, training loss: 6.878501892089844 = 0.48088404536247253 + 1.0 * 6.397617816925049
Epoch 460, val loss: 0.788167417049408
Epoch 470, training loss: 6.854024887084961 = 0.460359126329422 + 1.0 * 6.393665790557861
Epoch 470, val loss: 0.77916020154953
Epoch 480, training loss: 6.8324384689331055 = 0.4407547414302826 + 1.0 * 6.391683578491211
Epoch 480, val loss: 0.7713608145713806
Epoch 490, training loss: 6.822362899780273 = 0.42210862040519714 + 1.0 * 6.400254249572754
Epoch 490, val loss: 0.7646678686141968
Epoch 500, training loss: 6.796531677246094 = 0.4046474099159241 + 1.0 * 6.3918843269348145
Epoch 500, val loss: 0.7589659094810486
Epoch 510, training loss: 6.772467613220215 = 0.38798192143440247 + 1.0 * 6.384485721588135
Epoch 510, val loss: 0.7542670369148254
Epoch 520, training loss: 6.756494045257568 = 0.37196335196495056 + 1.0 * 6.384530544281006
Epoch 520, val loss: 0.750248908996582
Epoch 530, training loss: 6.736763000488281 = 0.35653090476989746 + 1.0 * 6.380231857299805
Epoch 530, val loss: 0.746857762336731
Epoch 540, training loss: 6.71973180770874 = 0.34152352809906006 + 1.0 * 6.378208160400391
Epoch 540, val loss: 0.7441680431365967
Epoch 550, training loss: 6.7033305168151855 = 0.32672223448753357 + 1.0 * 6.376608371734619
Epoch 550, val loss: 0.7419136762619019
Epoch 560, training loss: 6.686835765838623 = 0.3120478093624115 + 1.0 * 6.3747878074646
Epoch 560, val loss: 0.7399554252624512
Epoch 570, training loss: 6.676815032958984 = 0.2974550724029541 + 1.0 * 6.379359722137451
Epoch 570, val loss: 0.7383331656455994
Epoch 580, training loss: 6.654223918914795 = 0.28293168544769287 + 1.0 * 6.3712921142578125
Epoch 580, val loss: 0.7370993494987488
Epoch 590, training loss: 6.637823581695557 = 0.26833978295326233 + 1.0 * 6.369483947753906
Epoch 590, val loss: 0.7360575795173645
Epoch 600, training loss: 6.623888969421387 = 0.2538245916366577 + 1.0 * 6.3700642585754395
Epoch 600, val loss: 0.7352422475814819
Epoch 610, training loss: 6.6088972091674805 = 0.23956406116485596 + 1.0 * 6.369333267211914
Epoch 610, val loss: 0.7348617911338806
Epoch 620, training loss: 6.589785099029541 = 0.2257237434387207 + 1.0 * 6.36406135559082
Epoch 620, val loss: 0.7348554134368896
Epoch 630, training loss: 6.5748162269592285 = 0.21243655681610107 + 1.0 * 6.362379550933838
Epoch 630, val loss: 0.7353098392486572
Epoch 640, training loss: 6.560333251953125 = 0.19984093308448792 + 1.0 * 6.36049222946167
Epoch 640, val loss: 0.7362828254699707
Epoch 650, training loss: 6.553341388702393 = 0.18806429207324982 + 1.0 * 6.365277290344238
Epoch 650, val loss: 0.7375218272209167
Epoch 660, training loss: 6.538649559020996 = 0.17723940312862396 + 1.0 * 6.361410140991211
Epoch 660, val loss: 0.739617109298706
Epoch 670, training loss: 6.523085594177246 = 0.16722372174263 + 1.0 * 6.355861663818359
Epoch 670, val loss: 0.7421396970748901
Epoch 680, training loss: 6.51131010055542 = 0.15791262686252594 + 1.0 * 6.353397369384766
Epoch 680, val loss: 0.7449992299079895
Epoch 690, training loss: 6.501712799072266 = 0.14925886690616608 + 1.0 * 6.352453708648682
Epoch 690, val loss: 0.7484464049339294
Epoch 700, training loss: 6.50122594833374 = 0.1412304788827896 + 1.0 * 6.359995365142822
Epoch 700, val loss: 0.7521008849143982
Epoch 710, training loss: 6.487856388092041 = 0.13385231792926788 + 1.0 * 6.35400390625
Epoch 710, val loss: 0.7562651634216309
Epoch 720, training loss: 6.475193977355957 = 0.1269768625497818 + 1.0 * 6.348217010498047
Epoch 720, val loss: 0.760774552822113
Epoch 730, training loss: 6.468257427215576 = 0.12053722888231277 + 1.0 * 6.347720146179199
Epoch 730, val loss: 0.7654781937599182
Epoch 740, training loss: 6.469717025756836 = 0.11452379822731018 + 1.0 * 6.355193138122559
Epoch 740, val loss: 0.7703889608383179
Epoch 750, training loss: 6.456072807312012 = 0.10894825309515 + 1.0 * 6.3471245765686035
Epoch 750, val loss: 0.775627076625824
Epoch 760, training loss: 6.446911811828613 = 0.10372865200042725 + 1.0 * 6.3431830406188965
Epoch 760, val loss: 0.7810983657836914
Epoch 770, training loss: 6.445237636566162 = 0.09881553798913956 + 1.0 * 6.34642219543457
Epoch 770, val loss: 0.786608099937439
Epoch 780, training loss: 6.435197830200195 = 0.09419551491737366 + 1.0 * 6.341002464294434
Epoch 780, val loss: 0.7922669649124146
Epoch 790, training loss: 6.432126522064209 = 0.08985516428947449 + 1.0 * 6.342271327972412
Epoch 790, val loss: 0.7981432676315308
Epoch 800, training loss: 6.425936698913574 = 0.08577611297369003 + 1.0 * 6.340160369873047
Epoch 800, val loss: 0.8040363788604736
Epoch 810, training loss: 6.423250198364258 = 0.0819341391324997 + 1.0 * 6.341316223144531
Epoch 810, val loss: 0.8100120425224304
Epoch 820, training loss: 6.414794921875 = 0.07830371707677841 + 1.0 * 6.336491107940674
Epoch 820, val loss: 0.8160938024520874
Epoch 830, training loss: 6.413383483886719 = 0.0748843103647232 + 1.0 * 6.338499069213867
Epoch 830, val loss: 0.8222259879112244
Epoch 840, training loss: 6.411180019378662 = 0.0716567412018776 + 1.0 * 6.3395233154296875
Epoch 840, val loss: 0.8282745480537415
Epoch 850, training loss: 6.404709339141846 = 0.06862194091081619 + 1.0 * 6.336087226867676
Epoch 850, val loss: 0.834444522857666
Epoch 860, training loss: 6.399040222167969 = 0.06575798243284225 + 1.0 * 6.333282470703125
Epoch 860, val loss: 0.8406438827514648
Epoch 870, training loss: 6.394401550292969 = 0.06304959207773209 + 1.0 * 6.3313517570495605
Epoch 870, val loss: 0.8468957543373108
Epoch 880, training loss: 6.393245697021484 = 0.060484956949949265 + 1.0 * 6.332760810852051
Epoch 880, val loss: 0.8531899452209473
Epoch 890, training loss: 6.388998031616211 = 0.058054447174072266 + 1.0 * 6.330943584442139
Epoch 890, val loss: 0.8592123985290527
Epoch 900, training loss: 6.383681297302246 = 0.05575869604945183 + 1.0 * 6.327922821044922
Epoch 900, val loss: 0.8655602931976318
Epoch 910, training loss: 6.380615711212158 = 0.05358569324016571 + 1.0 * 6.327030181884766
Epoch 910, val loss: 0.8717912435531616
Epoch 920, training loss: 6.3789496421813965 = 0.05151423439383507 + 1.0 * 6.327435493469238
Epoch 920, val loss: 0.8779589533805847
Epoch 930, training loss: 6.379867076873779 = 0.04955669492483139 + 1.0 * 6.330310344696045
Epoch 930, val loss: 0.8839445114135742
Epoch 940, training loss: 6.374976634979248 = 0.04771290719509125 + 1.0 * 6.327263832092285
Epoch 940, val loss: 0.8901429772377014
Epoch 950, training loss: 6.369598865509033 = 0.04596283286809921 + 1.0 * 6.323636054992676
Epoch 950, val loss: 0.8962683081626892
Epoch 960, training loss: 6.366326808929443 = 0.04429156705737114 + 1.0 * 6.322035312652588
Epoch 960, val loss: 0.9022694826126099
Epoch 970, training loss: 6.369522571563721 = 0.042700186371803284 + 1.0 * 6.326822280883789
Epoch 970, val loss: 0.9083649516105652
Epoch 980, training loss: 6.370425701141357 = 0.04118746146559715 + 1.0 * 6.329238414764404
Epoch 980, val loss: 0.9140160083770752
Epoch 990, training loss: 6.361074924468994 = 0.0397615060210228 + 1.0 * 6.321313381195068
Epoch 990, val loss: 0.9200823903083801
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 10.546538352966309 = 1.9497039318084717 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9499369859695435
Epoch 10, training loss: 10.5361909866333 = 1.9396528005599976 + 1.0 * 8.596538543701172
Epoch 10, val loss: 1.9397149085998535
Epoch 20, training loss: 10.520959854125977 = 1.9272847175598145 + 1.0 * 8.59367561340332
Epoch 20, val loss: 1.9266818761825562
Epoch 30, training loss: 10.479077339172363 = 1.9105290174484253 + 1.0 * 8.568548202514648
Epoch 30, val loss: 1.908767580986023
Epoch 40, training loss: 10.298948287963867 = 1.889841914176941 + 1.0 * 8.409106254577637
Epoch 40, val loss: 1.8877224922180176
Epoch 50, training loss: 9.947136878967285 = 1.8690680265426636 + 1.0 * 8.078068733215332
Epoch 50, val loss: 1.8674054145812988
Epoch 60, training loss: 9.420132637023926 = 1.8519220352172852 + 1.0 * 7.568210601806641
Epoch 60, val loss: 1.8512804508209229
Epoch 70, training loss: 9.005542755126953 = 1.8377565145492554 + 1.0 * 7.167786121368408
Epoch 70, val loss: 1.8374981880187988
Epoch 80, training loss: 8.761874198913574 = 1.824054479598999 + 1.0 * 6.937819480895996
Epoch 80, val loss: 1.823393702507019
Epoch 90, training loss: 8.644084930419922 = 1.8075406551361084 + 1.0 * 6.836544513702393
Epoch 90, val loss: 1.8073716163635254
Epoch 100, training loss: 8.548357009887695 = 1.7892518043518066 + 1.0 * 6.7591047286987305
Epoch 100, val loss: 1.7901253700256348
Epoch 110, training loss: 8.469278335571289 = 1.7710176706314087 + 1.0 * 6.698260307312012
Epoch 110, val loss: 1.7730962038040161
Epoch 120, training loss: 8.402857780456543 = 1.7525999546051025 + 1.0 * 6.650257587432861
Epoch 120, val loss: 1.7557796239852905
Epoch 130, training loss: 8.34661865234375 = 1.73261559009552 + 1.0 * 6.6140031814575195
Epoch 130, val loss: 1.7370848655700684
Epoch 140, training loss: 8.29811954498291 = 1.710087776184082 + 1.0 * 6.588031768798828
Epoch 140, val loss: 1.7162079811096191
Epoch 150, training loss: 8.247066497802734 = 1.6845107078552246 + 1.0 * 6.562556266784668
Epoch 150, val loss: 1.69297456741333
Epoch 160, training loss: 8.197721481323242 = 1.6553454399108887 + 1.0 * 6.542375564575195
Epoch 160, val loss: 1.6665563583374023
Epoch 170, training loss: 8.148460388183594 = 1.6217403411865234 + 1.0 * 6.526719570159912
Epoch 170, val loss: 1.6364059448242188
Epoch 180, training loss: 8.101540565490723 = 1.5833641290664673 + 1.0 * 6.518176555633545
Epoch 180, val loss: 1.6023601293563843
Epoch 190, training loss: 8.040674209594727 = 1.540697693824768 + 1.0 * 6.499976634979248
Epoch 190, val loss: 1.5645779371261597
Epoch 200, training loss: 7.9820685386657715 = 1.4930706024169922 + 1.0 * 6.488997936248779
Epoch 200, val loss: 1.5228207111358643
Epoch 210, training loss: 7.919135093688965 = 1.4403108358383179 + 1.0 * 6.478824138641357
Epoch 210, val loss: 1.4771660566329956
Epoch 220, training loss: 7.853177547454834 = 1.3828331232070923 + 1.0 * 6.470344543457031
Epoch 220, val loss: 1.42800772190094
Epoch 230, training loss: 7.80008602142334 = 1.321463704109192 + 1.0 * 6.4786224365234375
Epoch 230, val loss: 1.376579761505127
Epoch 240, training loss: 7.720684051513672 = 1.2597877979278564 + 1.0 * 6.4608964920043945
Epoch 240, val loss: 1.3253819942474365
Epoch 250, training loss: 7.650679588317871 = 1.1977232694625854 + 1.0 * 6.452956199645996
Epoch 250, val loss: 1.2743453979492188
Epoch 260, training loss: 7.58155632019043 = 1.1359310150146484 + 1.0 * 6.445625305175781
Epoch 260, val loss: 1.2239539623260498
Epoch 270, training loss: 7.5149030685424805 = 1.075326681137085 + 1.0 * 6.439576148986816
Epoch 270, val loss: 1.174828052520752
Epoch 280, training loss: 7.456999778747559 = 1.0170605182647705 + 1.0 * 6.439939022064209
Epoch 280, val loss: 1.1280063390731812
Epoch 290, training loss: 7.392698764801025 = 0.9633545279502869 + 1.0 * 6.429344177246094
Epoch 290, val loss: 1.0849206447601318
Epoch 300, training loss: 7.339453220367432 = 0.9140180349349976 + 1.0 * 6.4254350662231445
Epoch 300, val loss: 1.0457608699798584
Epoch 310, training loss: 7.28891134262085 = 0.8692498207092285 + 1.0 * 6.419661521911621
Epoch 310, val loss: 1.010468602180481
Epoch 320, training loss: 7.2438554763793945 = 0.828998327255249 + 1.0 * 6.414857387542725
Epoch 320, val loss: 0.9791098237037659
Epoch 330, training loss: 7.210357189178467 = 0.7927210927009583 + 1.0 * 6.417635917663574
Epoch 330, val loss: 0.9512644410133362
Epoch 340, training loss: 7.166848659515381 = 0.7601396441459656 + 1.0 * 6.40670919418335
Epoch 340, val loss: 0.9268683195114136
Epoch 350, training loss: 7.130622863769531 = 0.7306191325187683 + 1.0 * 6.400003910064697
Epoch 350, val loss: 0.9050299525260925
Epoch 360, training loss: 7.103283405303955 = 0.7033063173294067 + 1.0 * 6.399977207183838
Epoch 360, val loss: 0.8852431774139404
Epoch 370, training loss: 7.0721211433410645 = 0.6778180003166199 + 1.0 * 6.394303321838379
Epoch 370, val loss: 0.8668915033340454
Epoch 380, training loss: 7.04397439956665 = 0.6536266207695007 + 1.0 * 6.390347957611084
Epoch 380, val loss: 0.8497695326805115
Epoch 390, training loss: 7.016721248626709 = 0.6302948594093323 + 1.0 * 6.3864264488220215
Epoch 390, val loss: 0.8334456086158752
Epoch 400, training loss: 6.9928669929504395 = 0.6074783205986023 + 1.0 * 6.3853888511657715
Epoch 400, val loss: 0.8175646662712097
Epoch 410, training loss: 6.964664459228516 = 0.5850613117218018 + 1.0 * 6.379603385925293
Epoch 410, val loss: 0.802055299282074
Epoch 420, training loss: 6.93828821182251 = 0.5628331303596497 + 1.0 * 6.375454902648926
Epoch 420, val loss: 0.7869102358818054
Epoch 430, training loss: 6.914070129394531 = 0.5406404137611389 + 1.0 * 6.373429775238037
Epoch 430, val loss: 0.771908164024353
Epoch 440, training loss: 6.8905158042907715 = 0.5186041593551636 + 1.0 * 6.371911525726318
Epoch 440, val loss: 0.7571986317634583
Epoch 450, training loss: 6.864534378051758 = 0.4967813193798065 + 1.0 * 6.367753028869629
Epoch 450, val loss: 0.7428686618804932
Epoch 460, training loss: 6.838920593261719 = 0.4750118553638458 + 1.0 * 6.363908767700195
Epoch 460, val loss: 0.7289869785308838
Epoch 470, training loss: 6.823543071746826 = 0.45333364605903625 + 1.0 * 6.370209217071533
Epoch 470, val loss: 0.715564489364624
Epoch 480, training loss: 6.791206359863281 = 0.4318239986896515 + 1.0 * 6.359382152557373
Epoch 480, val loss: 0.7027570605278015
Epoch 490, training loss: 6.772242546081543 = 0.41052207350730896 + 1.0 * 6.361720561981201
Epoch 490, val loss: 0.6905673742294312
Epoch 500, training loss: 6.745357990264893 = 0.38946136832237244 + 1.0 * 6.355896472930908
Epoch 500, val loss: 0.6790904998779297
Epoch 510, training loss: 6.731926918029785 = 0.3687282204627991 + 1.0 * 6.363198757171631
Epoch 510, val loss: 0.6683329343795776
Epoch 520, training loss: 6.698652744293213 = 0.3484024107456207 + 1.0 * 6.350250244140625
Epoch 520, val loss: 0.6584533452987671
Epoch 530, training loss: 6.677599906921387 = 0.3285315930843353 + 1.0 * 6.3490681648254395
Epoch 530, val loss: 0.6493792533874512
Epoch 540, training loss: 6.662755966186523 = 0.3092274069786072 + 1.0 * 6.3535284996032715
Epoch 540, val loss: 0.6410300731658936
Epoch 550, training loss: 6.641317367553711 = 0.2905712127685547 + 1.0 * 6.350746154785156
Epoch 550, val loss: 0.6336290836334229
Epoch 560, training loss: 6.616856575012207 = 0.27275165915489197 + 1.0 * 6.344104766845703
Epoch 560, val loss: 0.6270209550857544
Epoch 570, training loss: 6.602994918823242 = 0.2557070255279541 + 1.0 * 6.347288131713867
Epoch 570, val loss: 0.6212512254714966
Epoch 580, training loss: 6.581418037414551 = 0.2395363599061966 + 1.0 * 6.34188175201416
Epoch 580, val loss: 0.6163102984428406
Epoch 590, training loss: 6.563818454742432 = 0.2242022156715393 + 1.0 * 6.339616298675537
Epoch 590, val loss: 0.6122074127197266
Epoch 600, training loss: 6.554898738861084 = 0.20985811948776245 + 1.0 * 6.345040798187256
Epoch 600, val loss: 0.6088535785675049
Epoch 610, training loss: 6.535073757171631 = 0.19651255011558533 + 1.0 * 6.338561058044434
Epoch 610, val loss: 0.6063642501831055
Epoch 620, training loss: 6.518820285797119 = 0.18405869603157043 + 1.0 * 6.334761619567871
Epoch 620, val loss: 0.6045671701431274
Epoch 630, training loss: 6.503915309906006 = 0.17242255806922913 + 1.0 * 6.331492900848389
Epoch 630, val loss: 0.6035898923873901
Epoch 640, training loss: 6.492615222930908 = 0.16155855357646942 + 1.0 * 6.331056594848633
Epoch 640, val loss: 0.6032737493515015
Epoch 650, training loss: 6.48159646987915 = 0.15148717164993286 + 1.0 * 6.330109119415283
Epoch 650, val loss: 0.6036620736122131
Epoch 660, training loss: 6.474958896636963 = 0.14217866957187653 + 1.0 * 6.332780361175537
Epoch 660, val loss: 0.6045575737953186
Epoch 670, training loss: 6.4609832763671875 = 0.1335623413324356 + 1.0 * 6.327420711517334
Epoch 670, val loss: 0.6059723496437073
Epoch 680, training loss: 6.450567722320557 = 0.12555630505084991 + 1.0 * 6.325011253356934
Epoch 680, val loss: 0.6079522371292114
Epoch 690, training loss: 6.444800853729248 = 0.11811193078756332 + 1.0 * 6.326688766479492
Epoch 690, val loss: 0.6104004979133606
Epoch 700, training loss: 6.436695098876953 = 0.11121001839637756 + 1.0 * 6.3254852294921875
Epoch 700, val loss: 0.6132596731185913
Epoch 710, training loss: 6.433469295501709 = 0.10483215749263763 + 1.0 * 6.32863712310791
Epoch 710, val loss: 0.616416871547699
Epoch 720, training loss: 6.419227600097656 = 0.0989404171705246 + 1.0 * 6.320287227630615
Epoch 720, val loss: 0.6198016405105591
Epoch 730, training loss: 6.41370964050293 = 0.09345845878124237 + 1.0 * 6.320250988006592
Epoch 730, val loss: 0.6235180497169495
Epoch 740, training loss: 6.407674312591553 = 0.08833838999271393 + 1.0 * 6.3193359375
Epoch 740, val loss: 0.6275247931480408
Epoch 750, training loss: 6.403773784637451 = 0.08357064425945282 + 1.0 * 6.3202033042907715
Epoch 750, val loss: 0.6317564249038696
Epoch 760, training loss: 6.394983291625977 = 0.07911431789398193 + 1.0 * 6.315868854522705
Epoch 760, val loss: 0.6361513733863831
Epoch 770, training loss: 6.39345121383667 = 0.07496634870767593 + 1.0 * 6.318484783172607
Epoch 770, val loss: 0.6406844258308411
Epoch 780, training loss: 6.390221118927002 = 0.07110010087490082 + 1.0 * 6.31912088394165
Epoch 780, val loss: 0.6453486680984497
Epoch 790, training loss: 6.38357400894165 = 0.06749281287193298 + 1.0 * 6.3160810470581055
Epoch 790, val loss: 0.6501327753067017
Epoch 800, training loss: 6.374790191650391 = 0.06414405256509781 + 1.0 * 6.310646057128906
Epoch 800, val loss: 0.6549803018569946
Epoch 810, training loss: 6.372310638427734 = 0.06102016568183899 + 1.0 * 6.311290264129639
Epoch 810, val loss: 0.659884512424469
Epoch 820, training loss: 6.367866516113281 = 0.05808037891983986 + 1.0 * 6.309786319732666
Epoch 820, val loss: 0.6649055480957031
Epoch 830, training loss: 6.368025779724121 = 0.0553356409072876 + 1.0 * 6.312690258026123
Epoch 830, val loss: 0.6700179576873779
Epoch 840, training loss: 6.360543727874756 = 0.05276188626885414 + 1.0 * 6.30778169631958
Epoch 840, val loss: 0.6751464009284973
Epoch 850, training loss: 6.361201286315918 = 0.0503493994474411 + 1.0 * 6.31085205078125
Epoch 850, val loss: 0.680297315120697
Epoch 860, training loss: 6.353734016418457 = 0.04810541123151779 + 1.0 * 6.305628776550293
Epoch 860, val loss: 0.6854304075241089
Epoch 870, training loss: 6.349071025848389 = 0.04598778858780861 + 1.0 * 6.303083419799805
Epoch 870, val loss: 0.6905847787857056
Epoch 880, training loss: 6.347183704376221 = 0.0439961813390255 + 1.0 * 6.303187370300293
Epoch 880, val loss: 0.6958009600639343
Epoch 890, training loss: 6.356322765350342 = 0.04212499037384987 + 1.0 * 6.314197540283203
Epoch 890, val loss: 0.7009950280189514
Epoch 900, training loss: 6.347485065460205 = 0.040364645421504974 + 1.0 * 6.307120323181152
Epoch 900, val loss: 0.7062423825263977
Epoch 910, training loss: 6.339428424835205 = 0.03871781751513481 + 1.0 * 6.300710678100586
Epoch 910, val loss: 0.7113296389579773
Epoch 920, training loss: 6.345872402191162 = 0.03716558218002319 + 1.0 * 6.308706760406494
Epoch 920, val loss: 0.7164930701255798
Epoch 930, training loss: 6.33628511428833 = 0.03569870814681053 + 1.0 * 6.300586223602295
Epoch 930, val loss: 0.7216241955757141
Epoch 940, training loss: 6.3337249755859375 = 0.034318044781684875 + 1.0 * 6.299407005310059
Epoch 940, val loss: 0.7266947031021118
Epoch 950, training loss: 6.34052038192749 = 0.033012449741363525 + 1.0 * 6.3075079917907715
Epoch 950, val loss: 0.7317970395088196
Epoch 960, training loss: 6.3314642906188965 = 0.03177144005894661 + 1.0 * 6.299692630767822
Epoch 960, val loss: 0.7369033098220825
Epoch 970, training loss: 6.32749605178833 = 0.03060910291969776 + 1.0 * 6.296886920928955
Epoch 970, val loss: 0.7418679594993591
Epoch 980, training loss: 6.327607154846191 = 0.02950206771492958 + 1.0 * 6.298105239868164
Epoch 980, val loss: 0.7469086647033691
Epoch 990, training loss: 6.324015140533447 = 0.028453756123781204 + 1.0 * 6.29556131362915
Epoch 990, val loss: 0.7518948912620544
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 10.557561874389648 = 1.9607007503509521 + 1.0 * 8.596860885620117
Epoch 0, val loss: 1.9611526727676392
Epoch 10, training loss: 10.546789169311523 = 1.9501194953918457 + 1.0 * 8.59666919708252
Epoch 10, val loss: 1.949905514717102
Epoch 20, training loss: 10.532188415527344 = 1.9370675086975098 + 1.0 * 8.595121383666992
Epoch 20, val loss: 1.9358859062194824
Epoch 30, training loss: 10.50082015991211 = 1.9189798831939697 + 1.0 * 8.581840515136719
Epoch 30, val loss: 1.9165061712265015
Epoch 40, training loss: 10.383354187011719 = 1.8944480419158936 + 1.0 * 8.488905906677246
Epoch 40, val loss: 1.8911292552947998
Epoch 50, training loss: 9.822802543640137 = 1.8686436414718628 + 1.0 * 7.954159259796143
Epoch 50, val loss: 1.8660188913345337
Epoch 60, training loss: 9.41025161743164 = 1.8483043909072876 + 1.0 * 7.561946868896484
Epoch 60, val loss: 1.8474609851837158
Epoch 70, training loss: 9.148215293884277 = 1.8299992084503174 + 1.0 * 7.318216323852539
Epoch 70, val loss: 1.8302115201950073
Epoch 80, training loss: 8.874650001525879 = 1.8117800951004028 + 1.0 * 7.062870025634766
Epoch 80, val loss: 1.8128063678741455
Epoch 90, training loss: 8.710094451904297 = 1.7927322387695312 + 1.0 * 6.917362689971924
Epoch 90, val loss: 1.7943007946014404
Epoch 100, training loss: 8.600557327270508 = 1.7716522216796875 + 1.0 * 6.82890510559082
Epoch 100, val loss: 1.774487018585205
Epoch 110, training loss: 8.510293960571289 = 1.7505693435668945 + 1.0 * 6.759725093841553
Epoch 110, val loss: 1.7549107074737549
Epoch 120, training loss: 8.430312156677246 = 1.7298519611358643 + 1.0 * 6.700459957122803
Epoch 120, val loss: 1.7356451749801636
Epoch 130, training loss: 8.356230735778809 = 1.7081540822982788 + 1.0 * 6.64807653427124
Epoch 130, val loss: 1.7153605222702026
Epoch 140, training loss: 8.294036865234375 = 1.6840081214904785 + 1.0 * 6.610029220581055
Epoch 140, val loss: 1.6928826570510864
Epoch 150, training loss: 8.23771858215332 = 1.6561373472213745 + 1.0 * 6.581581115722656
Epoch 150, val loss: 1.667301893234253
Epoch 160, training loss: 8.18354320526123 = 1.6237871646881104 + 1.0 * 6.559756278991699
Epoch 160, val loss: 1.6378939151763916
Epoch 170, training loss: 8.128640174865723 = 1.5863553285598755 + 1.0 * 6.542284965515137
Epoch 170, val loss: 1.6042757034301758
Epoch 180, training loss: 8.072883605957031 = 1.543566346168518 + 1.0 * 6.5293169021606445
Epoch 180, val loss: 1.566344141960144
Epoch 190, training loss: 8.01176929473877 = 1.4957876205444336 + 1.0 * 6.515981674194336
Epoch 190, val loss: 1.5247182846069336
Epoch 200, training loss: 7.948085784912109 = 1.4433834552764893 + 1.0 * 6.504702568054199
Epoch 200, val loss: 1.4800468683242798
Epoch 210, training loss: 7.8822760581970215 = 1.3870092630386353 + 1.0 * 6.495266914367676
Epoch 210, val loss: 1.4326730966567993
Epoch 220, training loss: 7.814961910247803 = 1.3271080255508423 + 1.0 * 6.48785400390625
Epoch 220, val loss: 1.3830825090408325
Epoch 230, training loss: 7.745997905731201 = 1.2653766870498657 + 1.0 * 6.480621337890625
Epoch 230, val loss: 1.3328391313552856
Epoch 240, training loss: 7.676972389221191 = 1.2034461498260498 + 1.0 * 6.4735260009765625
Epoch 240, val loss: 1.2828863859176636
Epoch 250, training loss: 7.609624862670898 = 1.142594337463379 + 1.0 * 6.4670305252075195
Epoch 250, val loss: 1.2345576286315918
Epoch 260, training loss: 7.545532703399658 = 1.0845881700515747 + 1.0 * 6.460944652557373
Epoch 260, val loss: 1.188976764678955
Epoch 270, training loss: 7.4864044189453125 = 1.0296813249588013 + 1.0 * 6.456723213195801
Epoch 270, val loss: 1.1461235284805298
Epoch 280, training loss: 7.4338459968566895 = 0.9788250923156738 + 1.0 * 6.455020904541016
Epoch 280, val loss: 1.10673987865448
Epoch 290, training loss: 7.379091739654541 = 0.9320594072341919 + 1.0 * 6.447032451629639
Epoch 290, val loss: 1.070817470550537
Epoch 300, training loss: 7.330624103546143 = 0.8887267112731934 + 1.0 * 6.441897392272949
Epoch 300, val loss: 1.0377576351165771
Epoch 310, training loss: 7.284544467926025 = 0.8485388159751892 + 1.0 * 6.436005592346191
Epoch 310, val loss: 1.0074355602264404
Epoch 320, training loss: 7.245070457458496 = 0.811124861240387 + 1.0 * 6.433945655822754
Epoch 320, val loss: 0.9795611500740051
Epoch 330, training loss: 7.206393241882324 = 0.776263415813446 + 1.0 * 6.4301300048828125
Epoch 330, val loss: 0.9538463354110718
Epoch 340, training loss: 7.167370319366455 = 0.7436012625694275 + 1.0 * 6.423768997192383
Epoch 340, val loss: 0.9302536845207214
Epoch 350, training loss: 7.136757850646973 = 0.712780237197876 + 1.0 * 6.423977851867676
Epoch 350, val loss: 0.9082870483398438
Epoch 360, training loss: 7.101863384246826 = 0.6836119890213013 + 1.0 * 6.4182515144348145
Epoch 360, val loss: 0.8879486918449402
Epoch 370, training loss: 7.069352149963379 = 0.6558243036270142 + 1.0 * 6.413527965545654
Epoch 370, val loss: 0.8689119219779968
Epoch 380, training loss: 7.03628396987915 = 0.6289361119270325 + 1.0 * 6.407347679138184
Epoch 380, val loss: 0.8509002923965454
Epoch 390, training loss: 7.0084452629089355 = 0.6025647521018982 + 1.0 * 6.405880451202393
Epoch 390, val loss: 0.8335390090942383
Epoch 400, training loss: 6.9804840087890625 = 0.5766605734825134 + 1.0 * 6.403823375701904
Epoch 400, val loss: 0.8169167041778564
Epoch 410, training loss: 6.947445392608643 = 0.5511955618858337 + 1.0 * 6.396249771118164
Epoch 410, val loss: 0.8008548021316528
Epoch 420, training loss: 6.919095039367676 = 0.5260034799575806 + 1.0 * 6.393091678619385
Epoch 420, val loss: 0.7854397892951965
Epoch 430, training loss: 6.8924102783203125 = 0.5009252429008484 + 1.0 * 6.391485214233398
Epoch 430, val loss: 0.770534873008728
Epoch 440, training loss: 6.864457130432129 = 0.4760955274105072 + 1.0 * 6.38836145401001
Epoch 440, val loss: 0.7562957406044006
Epoch 450, training loss: 6.840052604675293 = 0.4515713155269623 + 1.0 * 6.388481140136719
Epoch 450, val loss: 0.7427874803543091
Epoch 460, training loss: 6.809370994567871 = 0.4274449646472931 + 1.0 * 6.3819260597229
Epoch 460, val loss: 0.7301062941551208
Epoch 470, training loss: 6.782389163970947 = 0.4037933647632599 + 1.0 * 6.37859582901001
Epoch 470, val loss: 0.7183074951171875
Epoch 480, training loss: 6.769189834594727 = 0.3808225989341736 + 1.0 * 6.388367176055908
Epoch 480, val loss: 0.7075249552726746
Epoch 490, training loss: 6.7334723472595215 = 0.3589462339878082 + 1.0 * 6.374526023864746
Epoch 490, val loss: 0.6979720592498779
Epoch 500, training loss: 6.709690570831299 = 0.33801931142807007 + 1.0 * 6.371671199798584
Epoch 500, val loss: 0.6896471381187439
Epoch 510, training loss: 6.686737537384033 = 0.3180428743362427 + 1.0 * 6.36869478225708
Epoch 510, val loss: 0.682439386844635
Epoch 520, training loss: 6.671034336090088 = 0.2990567982196808 + 1.0 * 6.37197732925415
Epoch 520, val loss: 0.6763960719108582
Epoch 530, training loss: 6.6570892333984375 = 0.28123217821121216 + 1.0 * 6.375856876373291
Epoch 530, val loss: 0.6714810729026794
Epoch 540, training loss: 6.629443645477295 = 0.2645729184150696 + 1.0 * 6.364870548248291
Epoch 540, val loss: 0.6677603721618652
Epoch 550, training loss: 6.609816074371338 = 0.24890057742595673 + 1.0 * 6.360915660858154
Epoch 550, val loss: 0.664958119392395
Epoch 560, training loss: 6.594403266906738 = 0.23412078619003296 + 1.0 * 6.3602824211120605
Epoch 560, val loss: 0.6629942655563354
Epoch 570, training loss: 6.579941749572754 = 0.2202494591474533 + 1.0 * 6.359692096710205
Epoch 570, val loss: 0.6617958545684814
Epoch 580, training loss: 6.56585168838501 = 0.2073081135749817 + 1.0 * 6.358543395996094
Epoch 580, val loss: 0.6612958312034607
Epoch 590, training loss: 6.551196575164795 = 0.1952044814825058 + 1.0 * 6.355992317199707
Epoch 590, val loss: 0.6614589691162109
Epoch 600, training loss: 6.535412788391113 = 0.18382008373737335 + 1.0 * 6.351592540740967
Epoch 600, val loss: 0.6621281504631042
Epoch 610, training loss: 6.541851043701172 = 0.1731324940919876 + 1.0 * 6.36871862411499
Epoch 610, val loss: 0.6632739305496216
Epoch 620, training loss: 6.513471603393555 = 0.1631624400615692 + 1.0 * 6.350309371948242
Epoch 620, val loss: 0.6648585796356201
Epoch 630, training loss: 6.501305103302002 = 0.15383043885231018 + 1.0 * 6.347474575042725
Epoch 630, val loss: 0.6669031381607056
Epoch 640, training loss: 6.497105598449707 = 0.14506986737251282 + 1.0 * 6.3520355224609375
Epoch 640, val loss: 0.6693624258041382
Epoch 650, training loss: 6.486846923828125 = 0.13689137995243073 + 1.0 * 6.3499555587768555
Epoch 650, val loss: 0.6719123125076294
Epoch 660, training loss: 6.471726894378662 = 0.12924745678901672 + 1.0 * 6.342479228973389
Epoch 660, val loss: 0.6749481558799744
Epoch 670, training loss: 6.471255779266357 = 0.12208262085914612 + 1.0 * 6.349173069000244
Epoch 670, val loss: 0.678199291229248
Epoch 680, training loss: 6.457696914672852 = 0.11540543287992477 + 1.0 * 6.342291355133057
Epoch 680, val loss: 0.6816158294677734
Epoch 690, training loss: 6.4506072998046875 = 0.1091553270816803 + 1.0 * 6.341452121734619
Epoch 690, val loss: 0.6852487921714783
Epoch 700, training loss: 6.4409661293029785 = 0.10331638902425766 + 1.0 * 6.337649822235107
Epoch 700, val loss: 0.6890599131584167
Epoch 710, training loss: 6.434868335723877 = 0.09786251187324524 + 1.0 * 6.337005615234375
Epoch 710, val loss: 0.692996084690094
Epoch 720, training loss: 6.42682409286499 = 0.09275051951408386 + 1.0 * 6.334073543548584
Epoch 720, val loss: 0.6970775127410889
Epoch 730, training loss: 6.423060894012451 = 0.08795151114463806 + 1.0 * 6.335109233856201
Epoch 730, val loss: 0.7012191414833069
Epoch 740, training loss: 6.4185662269592285 = 0.08346697688102722 + 1.0 * 6.335099220275879
Epoch 740, val loss: 0.7055028080940247
Epoch 750, training loss: 6.413660526275635 = 0.07929002493619919 + 1.0 * 6.3343706130981445
Epoch 750, val loss: 0.7097437977790833
Epoch 760, training loss: 6.404538631439209 = 0.07537200301885605 + 1.0 * 6.329166412353516
Epoch 760, val loss: 0.7141658067703247
Epoch 770, training loss: 6.412876129150391 = 0.07169153541326523 + 1.0 * 6.341184616088867
Epoch 770, val loss: 0.7186132669448853
Epoch 780, training loss: 6.397627353668213 = 0.06825333088636398 + 1.0 * 6.329373836517334
Epoch 780, val loss: 0.7230648994445801
Epoch 790, training loss: 6.391181468963623 = 0.0650225356221199 + 1.0 * 6.3261590003967285
Epoch 790, val loss: 0.727557897567749
Epoch 800, training loss: 6.39522123336792 = 0.061982687562704086 + 1.0 * 6.33323860168457
Epoch 800, val loss: 0.7320672869682312
Epoch 810, training loss: 6.387077808380127 = 0.059137336909770966 + 1.0 * 6.327940464019775
Epoch 810, val loss: 0.7366518378257751
Epoch 820, training loss: 6.38193941116333 = 0.05645821988582611 + 1.0 * 6.325481414794922
Epoch 820, val loss: 0.7411319017410278
Epoch 830, training loss: 6.381033420562744 = 0.05393984913825989 + 1.0 * 6.327093601226807
Epoch 830, val loss: 0.7456981539726257
Epoch 840, training loss: 6.374324798583984 = 0.05156778544187546 + 1.0 * 6.322757244110107
Epoch 840, val loss: 0.7503042221069336
Epoch 850, training loss: 6.370769500732422 = 0.049332231283187866 + 1.0 * 6.321437358856201
Epoch 850, val loss: 0.7548579573631287
Epoch 860, training loss: 6.371372699737549 = 0.047224633395671844 + 1.0 * 6.324148178100586
Epoch 860, val loss: 0.759388267993927
Epoch 870, training loss: 6.3631134033203125 = 0.04524349048733711 + 1.0 * 6.317870140075684
Epoch 870, val loss: 0.7639154195785522
Epoch 880, training loss: 6.361968040466309 = 0.04337161034345627 + 1.0 * 6.318596363067627
Epoch 880, val loss: 0.7685123682022095
Epoch 890, training loss: 6.360516548156738 = 0.041602492332458496 + 1.0 * 6.31891393661499
Epoch 890, val loss: 0.7730053663253784
Epoch 900, training loss: 6.355223178863525 = 0.03993656113743782 + 1.0 * 6.315286636352539
Epoch 900, val loss: 0.7775229811668396
Epoch 910, training loss: 6.36061429977417 = 0.038361500948667526 + 1.0 * 6.3222527503967285
Epoch 910, val loss: 0.7820940613746643
Epoch 920, training loss: 6.352046966552734 = 0.03687340021133423 + 1.0 * 6.315173625946045
Epoch 920, val loss: 0.7864773869514465
Epoch 930, training loss: 6.347366809844971 = 0.03546306490898132 + 1.0 * 6.311903953552246
Epoch 930, val loss: 0.7910211086273193
Epoch 940, training loss: 6.347871780395508 = 0.03412340208888054 + 1.0 * 6.313748359680176
Epoch 940, val loss: 0.7955100536346436
Epoch 950, training loss: 6.3450188636779785 = 0.03285755589604378 + 1.0 * 6.312161445617676
Epoch 950, val loss: 0.7998929023742676
Epoch 960, training loss: 6.346309185028076 = 0.03166315704584122 + 1.0 * 6.314646244049072
Epoch 960, val loss: 0.8043397068977356
Epoch 970, training loss: 6.340812683105469 = 0.030530456453561783 + 1.0 * 6.310282230377197
Epoch 970, val loss: 0.8086957931518555
Epoch 980, training loss: 6.34284782409668 = 0.02945576049387455 + 1.0 * 6.313392162322998
Epoch 980, val loss: 0.8130519390106201
Epoch 990, training loss: 6.336316108703613 = 0.02843666821718216 + 1.0 * 6.307879447937012
Epoch 990, val loss: 0.8173169493675232
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8402741170268846
The final CL Acc:0.82222, 0.00800, The final GNN Acc:0.83992, 0.00431
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9554])
updated graph: torch.Size([2, 10592])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.563404083251953 = 1.9665520191192627 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.9761625528335571
Epoch 10, training loss: 10.553220748901367 = 1.9566212892532349 + 1.0 * 8.596599578857422
Epoch 10, val loss: 1.9661370515823364
Epoch 20, training loss: 10.538568496704102 = 1.944252848625183 + 1.0 * 8.594315528869629
Epoch 20, val loss: 1.9529222249984741
Epoch 30, training loss: 10.504196166992188 = 1.927059531211853 + 1.0 * 8.577136993408203
Epoch 30, val loss: 1.934030532836914
Epoch 40, training loss: 10.390800476074219 = 1.9049656391143799 + 1.0 * 8.485835075378418
Epoch 40, val loss: 1.9108177423477173
Epoch 50, training loss: 10.092232704162598 = 1.882656216621399 + 1.0 * 8.209576606750488
Epoch 50, val loss: 1.8887957334518433
Epoch 60, training loss: 9.823285102844238 = 1.8617364168167114 + 1.0 * 7.961548805236816
Epoch 60, val loss: 1.8682645559310913
Epoch 70, training loss: 9.361543655395508 = 1.8450345993041992 + 1.0 * 7.516509056091309
Epoch 70, val loss: 1.852156639099121
Epoch 80, training loss: 8.984539031982422 = 1.8317794799804688 + 1.0 * 7.152759075164795
Epoch 80, val loss: 1.8394505977630615
Epoch 90, training loss: 8.81246566772461 = 1.8147425651550293 + 1.0 * 6.997723579406738
Epoch 90, val loss: 1.8220893144607544
Epoch 100, training loss: 8.669276237487793 = 1.7957837581634521 + 1.0 * 6.873492240905762
Epoch 100, val loss: 1.8041645288467407
Epoch 110, training loss: 8.564523696899414 = 1.7802703380584717 + 1.0 * 6.784253120422363
Epoch 110, val loss: 1.7894428968429565
Epoch 120, training loss: 8.491445541381836 = 1.766074299812317 + 1.0 * 6.72537088394165
Epoch 120, val loss: 1.7756463289260864
Epoch 130, training loss: 8.432221412658691 = 1.7514374256134033 + 1.0 * 6.680783748626709
Epoch 130, val loss: 1.7614316940307617
Epoch 140, training loss: 8.382588386535645 = 1.7356561422348022 + 1.0 * 6.646932125091553
Epoch 140, val loss: 1.7466570138931274
Epoch 150, training loss: 8.33753490447998 = 1.7180935144424438 + 1.0 * 6.619441032409668
Epoch 150, val loss: 1.7309091091156006
Epoch 160, training loss: 8.292360305786133 = 1.6984010934829712 + 1.0 * 6.593958854675293
Epoch 160, val loss: 1.7136309146881104
Epoch 170, training loss: 8.2485990524292 = 1.6760419607162476 + 1.0 * 6.57255744934082
Epoch 170, val loss: 1.6941778659820557
Epoch 180, training loss: 8.205103874206543 = 1.6500892639160156 + 1.0 * 6.555014610290527
Epoch 180, val loss: 1.6715342998504639
Epoch 190, training loss: 8.162306785583496 = 1.6198161840438843 + 1.0 * 6.542490482330322
Epoch 190, val loss: 1.6451054811477661
Epoch 200, training loss: 8.114079475402832 = 1.5854549407958984 + 1.0 * 6.528624534606934
Epoch 200, val loss: 1.615301489830017
Epoch 210, training loss: 8.0633544921875 = 1.5469257831573486 + 1.0 * 6.516428470611572
Epoch 210, val loss: 1.5819144248962402
Epoch 220, training loss: 8.011221885681152 = 1.503945231437683 + 1.0 * 6.50727653503418
Epoch 220, val loss: 1.5447503328323364
Epoch 230, training loss: 7.957673072814941 = 1.457263708114624 + 1.0 * 6.5004096031188965
Epoch 230, val loss: 1.5049421787261963
Epoch 240, training loss: 7.897397994995117 = 1.4077911376953125 + 1.0 * 6.489606857299805
Epoch 240, val loss: 1.4632470607757568
Epoch 250, training loss: 7.837070941925049 = 1.3557348251342773 + 1.0 * 6.4813361167907715
Epoch 250, val loss: 1.4198179244995117
Epoch 260, training loss: 7.781601428985596 = 1.3015780448913574 + 1.0 * 6.480023384094238
Epoch 260, val loss: 1.3754174709320068
Epoch 270, training loss: 7.716960430145264 = 1.2464936971664429 + 1.0 * 6.470466613769531
Epoch 270, val loss: 1.3313456773757935
Epoch 280, training loss: 7.653243064880371 = 1.1912193298339844 + 1.0 * 6.462023735046387
Epoch 280, val loss: 1.2876781225204468
Epoch 290, training loss: 7.595803260803223 = 1.1359081268310547 + 1.0 * 6.459895133972168
Epoch 290, val loss: 1.2445547580718994
Epoch 300, training loss: 7.536701202392578 = 1.0815595388412476 + 1.0 * 6.455141544342041
Epoch 300, val loss: 1.2028530836105347
Epoch 310, training loss: 7.476130485534668 = 1.0288126468658447 + 1.0 * 6.447318077087402
Epoch 310, val loss: 1.1629467010498047
Epoch 320, training loss: 7.4227190017700195 = 0.9777860045433044 + 1.0 * 6.44493293762207
Epoch 320, val loss: 1.124939203262329
Epoch 330, training loss: 7.369050979614258 = 0.929261326789856 + 1.0 * 6.439789772033691
Epoch 330, val loss: 1.089133858680725
Epoch 340, training loss: 7.316218852996826 = 0.8830075860023499 + 1.0 * 6.433211326599121
Epoch 340, val loss: 1.0558809041976929
Epoch 350, training loss: 7.274424076080322 = 0.8391212821006775 + 1.0 * 6.435302734375
Epoch 350, val loss: 1.0247383117675781
Epoch 360, training loss: 7.2229156494140625 = 0.7979517579078674 + 1.0 * 6.42496395111084
Epoch 360, val loss: 0.9966447353363037
Epoch 370, training loss: 7.182487964630127 = 0.7594247460365295 + 1.0 * 6.423063278198242
Epoch 370, val loss: 0.971310555934906
Epoch 380, training loss: 7.142615795135498 = 0.7232913374900818 + 1.0 * 6.4193243980407715
Epoch 380, val loss: 0.9483458399772644
Epoch 390, training loss: 7.1044697761535645 = 0.6894048452377319 + 1.0 * 6.415064811706543
Epoch 390, val loss: 0.9279524087905884
Epoch 400, training loss: 7.075123310089111 = 0.657483696937561 + 1.0 * 6.41763973236084
Epoch 400, val loss: 0.9097620844841003
Epoch 410, training loss: 7.036056995391846 = 0.6277186274528503 + 1.0 * 6.40833854675293
Epoch 410, val loss: 0.8936386108398438
Epoch 420, training loss: 7.004673957824707 = 0.599554181098938 + 1.0 * 6.405119895935059
Epoch 420, val loss: 0.8795782327651978
Epoch 430, training loss: 6.981837272644043 = 0.5728376507759094 + 1.0 * 6.408999443054199
Epoch 430, val loss: 0.8668581247329712
Epoch 440, training loss: 6.945873737335205 = 0.5475756525993347 + 1.0 * 6.398298263549805
Epoch 440, val loss: 0.8557320833206177
Epoch 450, training loss: 6.9188337326049805 = 0.5235719680786133 + 1.0 * 6.395261764526367
Epoch 450, val loss: 0.8460637927055359
Epoch 460, training loss: 6.899109363555908 = 0.5005940198898315 + 1.0 * 6.398515224456787
Epoch 460, val loss: 0.8372146487236023
Epoch 470, training loss: 6.868379592895508 = 0.4786407947540283 + 1.0 * 6.3897385597229
Epoch 470, val loss: 0.8294681906700134
Epoch 480, training loss: 6.8456339836120605 = 0.4575299918651581 + 1.0 * 6.38810396194458
Epoch 480, val loss: 0.822726309299469
Epoch 490, training loss: 6.826279163360596 = 0.43716973066329956 + 1.0 * 6.3891096115112305
Epoch 490, val loss: 0.8166772723197937
Epoch 500, training loss: 6.800662994384766 = 0.4176810681819916 + 1.0 * 6.382981777191162
Epoch 500, val loss: 0.8115703463554382
Epoch 510, training loss: 6.781691551208496 = 0.39879709482192993 + 1.0 * 6.382894515991211
Epoch 510, val loss: 0.8072320222854614
Epoch 520, training loss: 6.764182090759277 = 0.3807421624660492 + 1.0 * 6.383440017700195
Epoch 520, val loss: 0.8033850789070129
Epoch 530, training loss: 6.740597724914551 = 0.36341214179992676 + 1.0 * 6.377185344696045
Epoch 530, val loss: 0.8005306124687195
Epoch 540, training loss: 6.722440719604492 = 0.3467382788658142 + 1.0 * 6.375702381134033
Epoch 540, val loss: 0.798221230506897
Epoch 550, training loss: 6.706076622009277 = 0.3308003842830658 + 1.0 * 6.3752760887146
Epoch 550, val loss: 0.7964208126068115
Epoch 560, training loss: 6.684802532196045 = 0.31557971239089966 + 1.0 * 6.369222640991211
Epoch 560, val loss: 0.7953031659126282
Epoch 570, training loss: 6.673754692077637 = 0.3009684085845947 + 1.0 * 6.372786521911621
Epoch 570, val loss: 0.7944686412811279
Epoch 580, training loss: 6.654068946838379 = 0.2870389521121979 + 1.0 * 6.367030143737793
Epoch 580, val loss: 0.7941527366638184
Epoch 590, training loss: 6.638790607452393 = 0.2736176550388336 + 1.0 * 6.365172863006592
Epoch 590, val loss: 0.7941388487815857
Epoch 600, training loss: 6.6287102699279785 = 0.2606430649757385 + 1.0 * 6.368067264556885
Epoch 600, val loss: 0.7943031191825867
Epoch 610, training loss: 6.60911750793457 = 0.24809972941875458 + 1.0 * 6.36101770401001
Epoch 610, val loss: 0.7948038578033447
Epoch 620, training loss: 6.594517230987549 = 0.23596954345703125 + 1.0 * 6.358547687530518
Epoch 620, val loss: 0.7955451607704163
Epoch 630, training loss: 6.588560104370117 = 0.2241738885641098 + 1.0 * 6.364386081695557
Epoch 630, val loss: 0.7964645028114319
Epoch 640, training loss: 6.572967052459717 = 0.21273104846477509 + 1.0 * 6.360236167907715
Epoch 640, val loss: 0.7975274920463562
Epoch 650, training loss: 6.555866241455078 = 0.2017168253660202 + 1.0 * 6.354149341583252
Epoch 650, val loss: 0.7989345788955688
Epoch 660, training loss: 6.5456342697143555 = 0.1910010278224945 + 1.0 * 6.354633331298828
Epoch 660, val loss: 0.8005237579345703
Epoch 670, training loss: 6.5345458984375 = 0.18062856793403625 + 1.0 * 6.353917121887207
Epoch 670, val loss: 0.802204430103302
Epoch 680, training loss: 6.52235221862793 = 0.17069081962108612 + 1.0 * 6.351661205291748
Epoch 680, val loss: 0.8042160868644714
Epoch 690, training loss: 6.510770320892334 = 0.1611396074295044 + 1.0 * 6.349630832672119
Epoch 690, val loss: 0.8064843416213989
Epoch 700, training loss: 6.5030035972595215 = 0.15197211503982544 + 1.0 * 6.351031303405762
Epoch 700, val loss: 0.8089316487312317
Epoch 710, training loss: 6.489958763122559 = 0.14322294294834137 + 1.0 * 6.346735954284668
Epoch 710, val loss: 0.8116997480392456
Epoch 720, training loss: 6.486260890960693 = 0.1349276304244995 + 1.0 * 6.351333141326904
Epoch 720, val loss: 0.8146187663078308
Epoch 730, training loss: 6.472114086151123 = 0.1271001547574997 + 1.0 * 6.3450140953063965
Epoch 730, val loss: 0.8178010582923889
Epoch 740, training loss: 6.4628143310546875 = 0.11971384286880493 + 1.0 * 6.343100547790527
Epoch 740, val loss: 0.8212817907333374
Epoch 750, training loss: 6.461170196533203 = 0.11274243146181107 + 1.0 * 6.348427772521973
Epoch 750, val loss: 0.8249762058258057
Epoch 760, training loss: 6.451779365539551 = 0.10624632984399796 + 1.0 * 6.3455328941345215
Epoch 760, val loss: 0.8288865089416504
Epoch 770, training loss: 6.439992904663086 = 0.10014712065458298 + 1.0 * 6.339845657348633
Epoch 770, val loss: 0.8329823613166809
Epoch 780, training loss: 6.433314323425293 = 0.0944359228014946 + 1.0 * 6.338878631591797
Epoch 780, val loss: 0.8372930288314819
Epoch 790, training loss: 6.430553436279297 = 0.0890846699476242 + 1.0 * 6.341468811035156
Epoch 790, val loss: 0.8417912125587463
Epoch 800, training loss: 6.423307418823242 = 0.08409129828214645 + 1.0 * 6.339216232299805
Epoch 800, val loss: 0.846405029296875
Epoch 810, training loss: 6.416325569152832 = 0.07942920923233032 + 1.0 * 6.3368964195251465
Epoch 810, val loss: 0.8511781692504883
Epoch 820, training loss: 6.409708499908447 = 0.075075164437294 + 1.0 * 6.3346333503723145
Epoch 820, val loss: 0.8560749292373657
Epoch 830, training loss: 6.404272556304932 = 0.07101801782846451 + 1.0 * 6.333254337310791
Epoch 830, val loss: 0.8610663414001465
Epoch 840, training loss: 6.400490760803223 = 0.06723357737064362 + 1.0 * 6.33325719833374
Epoch 840, val loss: 0.8661321997642517
Epoch 850, training loss: 6.398338794708252 = 0.06370964646339417 + 1.0 * 6.334629058837891
Epoch 850, val loss: 0.8712727427482605
Epoch 860, training loss: 6.392251014709473 = 0.06044275686144829 + 1.0 * 6.331808090209961
Epoch 860, val loss: 0.8764966130256653
Epoch 870, training loss: 6.389418601989746 = 0.057391803711652756 + 1.0 * 6.332026958465576
Epoch 870, val loss: 0.8816990256309509
Epoch 880, training loss: 6.3834686279296875 = 0.0545475035905838 + 1.0 * 6.328921318054199
Epoch 880, val loss: 0.8869386911392212
Epoch 890, training loss: 6.379509449005127 = 0.05189663916826248 + 1.0 * 6.32761287689209
Epoch 890, val loss: 0.8922533392906189
Epoch 900, training loss: 6.376596927642822 = 0.04941841587424278 + 1.0 * 6.327178478240967
Epoch 900, val loss: 0.8975334763526917
Epoch 910, training loss: 6.375072002410889 = 0.04709785059094429 + 1.0 * 6.327974319458008
Epoch 910, val loss: 0.9028578400611877
Epoch 920, training loss: 6.371908187866211 = 0.044940199702978134 + 1.0 * 6.326968193054199
Epoch 920, val loss: 0.9080620408058167
Epoch 930, training loss: 6.366386413574219 = 0.042923346161842346 + 1.0 * 6.323462963104248
Epoch 930, val loss: 0.9133317470550537
Epoch 940, training loss: 6.362664699554443 = 0.041033897548913956 + 1.0 * 6.321630954742432
Epoch 940, val loss: 0.9186298251152039
Epoch 950, training loss: 6.362078666687012 = 0.03925710916519165 + 1.0 * 6.322821617126465
Epoch 950, val loss: 0.9238362908363342
Epoch 960, training loss: 6.3579583168029785 = 0.037604451179504395 + 1.0 * 6.320353984832764
Epoch 960, val loss: 0.9290251731872559
Epoch 970, training loss: 6.359395503997803 = 0.03605074808001518 + 1.0 * 6.323344707489014
Epoch 970, val loss: 0.9341612458229065
Epoch 980, training loss: 6.358351230621338 = 0.034601371735334396 + 1.0 * 6.323750019073486
Epoch 980, val loss: 0.9392392635345459
Epoch 990, training loss: 6.351412773132324 = 0.03324287384748459 + 1.0 * 6.318170070648193
Epoch 990, val loss: 0.9442319273948669
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 10.552045822143555 = 1.9551931619644165 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.9590038061141968
Epoch 10, training loss: 10.541858673095703 = 1.9451980590820312 + 1.0 * 8.596660614013672
Epoch 10, val loss: 1.9490200281143188
Epoch 20, training loss: 10.527918815612793 = 1.9327701330184937 + 1.0 * 8.595149040222168
Epoch 20, val loss: 1.9366651773452759
Epoch 30, training loss: 10.49753189086914 = 1.9154942035675049 + 1.0 * 8.582037925720215
Epoch 30, val loss: 1.9195775985717773
Epoch 40, training loss: 10.385580062866211 = 1.892412781715393 + 1.0 * 8.49316692352295
Epoch 40, val loss: 1.8975560665130615
Epoch 50, training loss: 9.958770751953125 = 1.8678545951843262 + 1.0 * 8.090916633605957
Epoch 50, val loss: 1.8746531009674072
Epoch 60, training loss: 9.479186058044434 = 1.8474310636520386 + 1.0 * 7.6317548751831055
Epoch 60, val loss: 1.8555641174316406
Epoch 70, training loss: 9.12696647644043 = 1.8328278064727783 + 1.0 * 7.294138431549072
Epoch 70, val loss: 1.8408467769622803
Epoch 80, training loss: 8.899452209472656 = 1.8177258968353271 + 1.0 * 7.081726551055908
Epoch 80, val loss: 1.82668936252594
Epoch 90, training loss: 8.73099136352539 = 1.8018590211868286 + 1.0 * 6.929131984710693
Epoch 90, val loss: 1.8127762079238892
Epoch 100, training loss: 8.615235328674316 = 1.7874786853790283 + 1.0 * 6.827756881713867
Epoch 100, val loss: 1.80009925365448
Epoch 110, training loss: 8.530097961425781 = 1.774570107460022 + 1.0 * 6.755527496337891
Epoch 110, val loss: 1.7878594398498535
Epoch 120, training loss: 8.461385726928711 = 1.7617549896240234 + 1.0 * 6.699631214141846
Epoch 120, val loss: 1.7752145528793335
Epoch 130, training loss: 8.406671524047852 = 1.7480053901672363 + 1.0 * 6.658666133880615
Epoch 130, val loss: 1.7618811130523682
Epoch 140, training loss: 8.360204696655273 = 1.732744812965393 + 1.0 * 6.627459526062012
Epoch 140, val loss: 1.7476774454116821
Epoch 150, training loss: 8.316494941711426 = 1.715646505355835 + 1.0 * 6.600848197937012
Epoch 150, val loss: 1.7324119806289673
Epoch 160, training loss: 8.278789520263672 = 1.6962990760803223 + 1.0 * 6.582489967346191
Epoch 160, val loss: 1.7155383825302124
Epoch 170, training loss: 8.236557006835938 = 1.6743375062942505 + 1.0 * 6.562219142913818
Epoch 170, val loss: 1.6967217922210693
Epoch 180, training loss: 8.194522857666016 = 1.6492480039596558 + 1.0 * 6.5452752113342285
Epoch 180, val loss: 1.6754882335662842
Epoch 190, training loss: 8.151434898376465 = 1.6203311681747437 + 1.0 * 6.531103610992432
Epoch 190, val loss: 1.6511837244033813
Epoch 200, training loss: 8.108131408691406 = 1.5872926712036133 + 1.0 * 6.520839214324951
Epoch 200, val loss: 1.6235785484313965
Epoch 210, training loss: 8.05959701538086 = 1.5501708984375 + 1.0 * 6.509426593780518
Epoch 210, val loss: 1.5926823616027832
Epoch 220, training loss: 8.006974220275879 = 1.5085021257400513 + 1.0 * 6.498472213745117
Epoch 220, val loss: 1.5581004619598389
Epoch 230, training loss: 7.951908111572266 = 1.4620740413665771 + 1.0 * 6.489833831787109
Epoch 230, val loss: 1.5196211338043213
Epoch 240, training loss: 7.895413875579834 = 1.4111528396606445 + 1.0 * 6.4842610359191895
Epoch 240, val loss: 1.477432370185852
Epoch 250, training loss: 7.833110809326172 = 1.356549859046936 + 1.0 * 6.476561069488525
Epoch 250, val loss: 1.4322532415390015
Epoch 260, training loss: 7.769321918487549 = 1.2991929054260254 + 1.0 * 6.470129013061523
Epoch 260, val loss: 1.38467276096344
Epoch 270, training loss: 7.703522682189941 = 1.2400134801864624 + 1.0 * 6.4635090827941895
Epoch 270, val loss: 1.3354579210281372
Epoch 280, training loss: 7.637565612792969 = 1.1793930530548096 + 1.0 * 6.45817232131958
Epoch 280, val loss: 1.2851910591125488
Epoch 290, training loss: 7.573374271392822 = 1.1189683675765991 + 1.0 * 6.454405784606934
Epoch 290, val loss: 1.235229730606079
Epoch 300, training loss: 7.508388996124268 = 1.059806227684021 + 1.0 * 6.448582649230957
Epoch 300, val loss: 1.1863898038864136
Epoch 310, training loss: 7.455211639404297 = 1.0023186206817627 + 1.0 * 6.452892780303955
Epoch 310, val loss: 1.1391657590866089
Epoch 320, training loss: 7.389742374420166 = 0.9481180906295776 + 1.0 * 6.441624164581299
Epoch 320, val loss: 1.0950006246566772
Epoch 330, training loss: 7.331527233123779 = 0.897368848323822 + 1.0 * 6.4341583251953125
Epoch 330, val loss: 1.0543535947799683
Epoch 340, training loss: 7.292633056640625 = 0.8502810001373291 + 1.0 * 6.442351818084717
Epoch 340, val loss: 1.017418384552002
Epoch 350, training loss: 7.23567533493042 = 0.8078917264938354 + 1.0 * 6.427783489227295
Epoch 350, val loss: 0.9849282503128052
Epoch 360, training loss: 7.191196441650391 = 0.7695494890213013 + 1.0 * 6.421647071838379
Epoch 360, val loss: 0.9565075635910034
Epoch 370, training loss: 7.165599822998047 = 0.7347356677055359 + 1.0 * 6.430864334106445
Epoch 370, val loss: 0.9317247271537781
Epoch 380, training loss: 7.120193958282471 = 0.7034252285957336 + 1.0 * 6.416768550872803
Epoch 380, val loss: 0.9103424549102783
Epoch 390, training loss: 7.085906982421875 = 0.6748330593109131 + 1.0 * 6.411073684692383
Epoch 390, val loss: 0.8918375372886658
Epoch 400, training loss: 7.054152011871338 = 0.6482439041137695 + 1.0 * 6.405908107757568
Epoch 400, val loss: 0.8754562139511108
Epoch 410, training loss: 7.042723178863525 = 0.623259961605072 + 1.0 * 6.419463157653809
Epoch 410, val loss: 0.8606730103492737
Epoch 420, training loss: 7.003439426422119 = 0.5998826026916504 + 1.0 * 6.403556823730469
Epoch 420, val loss: 0.8475264310836792
Epoch 430, training loss: 6.976295471191406 = 0.5776128172874451 + 1.0 * 6.398682594299316
Epoch 430, val loss: 0.8357515335083008
Epoch 440, training loss: 6.951330184936523 = 0.5561910271644592 + 1.0 * 6.395139217376709
Epoch 440, val loss: 0.8247917890548706
Epoch 450, training loss: 6.927914142608643 = 0.5354465842247009 + 1.0 * 6.392467498779297
Epoch 450, val loss: 0.8146041631698608
Epoch 460, training loss: 6.903632640838623 = 0.5152164697647095 + 1.0 * 6.388416290283203
Epoch 460, val loss: 0.805231511592865
Epoch 470, training loss: 6.887101650238037 = 0.4953741431236267 + 1.0 * 6.391727447509766
Epoch 470, val loss: 0.7965742945671082
Epoch 480, training loss: 6.866263389587402 = 0.47598695755004883 + 1.0 * 6.3902764320373535
Epoch 480, val loss: 0.7885932922363281
Epoch 490, training loss: 6.839728832244873 = 0.45696842670440674 + 1.0 * 6.382760524749756
Epoch 490, val loss: 0.7813509702682495
Epoch 500, training loss: 6.816656589508057 = 0.43813958764076233 + 1.0 * 6.378517150878906
Epoch 500, val loss: 0.7747481465339661
Epoch 510, training loss: 6.796056270599365 = 0.41937026381492615 + 1.0 * 6.376686096191406
Epoch 510, val loss: 0.7686642408370972
Epoch 520, training loss: 6.777401924133301 = 0.4007086455821991 + 1.0 * 6.376693248748779
Epoch 520, val loss: 0.7631639838218689
Epoch 530, training loss: 6.760496139526367 = 0.3821972906589508 + 1.0 * 6.378298759460449
Epoch 530, val loss: 0.7583863139152527
Epoch 540, training loss: 6.737618446350098 = 0.3637860417366028 + 1.0 * 6.3738322257995605
Epoch 540, val loss: 0.7542675733566284
Epoch 550, training loss: 6.7134294509887695 = 0.34553974866867065 + 1.0 * 6.367889881134033
Epoch 550, val loss: 0.7508643865585327
Epoch 560, training loss: 6.694671154022217 = 0.3274902403354645 + 1.0 * 6.367180824279785
Epoch 560, val loss: 0.748170793056488
Epoch 570, training loss: 6.678168296813965 = 0.3098010718822479 + 1.0 * 6.3683671951293945
Epoch 570, val loss: 0.7462318539619446
Epoch 580, training loss: 6.6588311195373535 = 0.2926238775253296 + 1.0 * 6.366207122802734
Epoch 580, val loss: 0.7451274394989014
Epoch 590, training loss: 6.638808250427246 = 0.2761547267436981 + 1.0 * 6.362653732299805
Epoch 590, val loss: 0.744853675365448
Epoch 600, training loss: 6.620392322540283 = 0.26040026545524597 + 1.0 * 6.359992027282715
Epoch 600, val loss: 0.745387852191925
Epoch 610, training loss: 6.602841854095459 = 0.2453552931547165 + 1.0 * 6.357486724853516
Epoch 610, val loss: 0.7467412352561951
Epoch 620, training loss: 6.60319185256958 = 0.2310352474451065 + 1.0 * 6.372156620025635
Epoch 620, val loss: 0.7489006519317627
Epoch 630, training loss: 6.578945159912109 = 0.21761950850486755 + 1.0 * 6.361325740814209
Epoch 630, val loss: 0.7516734600067139
Epoch 640, training loss: 6.561262130737305 = 0.2050192654132843 + 1.0 * 6.356242656707764
Epoch 640, val loss: 0.755198061466217
Epoch 650, training loss: 6.54578161239624 = 0.19314779341220856 + 1.0 * 6.352633953094482
Epoch 650, val loss: 0.7593706250190735
Epoch 660, training loss: 6.532809257507324 = 0.18194478750228882 + 1.0 * 6.350864410400391
Epoch 660, val loss: 0.7641152143478394
Epoch 670, training loss: 6.523997783660889 = 0.17143791913986206 + 1.0 * 6.352560043334961
Epoch 670, val loss: 0.769330620765686
Epoch 680, training loss: 6.511594772338867 = 0.1616504192352295 + 1.0 * 6.349944114685059
Epoch 680, val loss: 0.7750134468078613
Epoch 690, training loss: 6.501121997833252 = 0.15250977873802185 + 1.0 * 6.348612308502197
Epoch 690, val loss: 0.7811751961708069
Epoch 700, training loss: 6.494265556335449 = 0.14396822452545166 + 1.0 * 6.350297451019287
Epoch 700, val loss: 0.7876796126365662
Epoch 710, training loss: 6.47988224029541 = 0.13600674271583557 + 1.0 * 6.343875408172607
Epoch 710, val loss: 0.7944813370704651
Epoch 720, training loss: 6.471615791320801 = 0.12856093049049377 + 1.0 * 6.34305477142334
Epoch 720, val loss: 0.8016197681427002
Epoch 730, training loss: 6.467678070068359 = 0.12159593403339386 + 1.0 * 6.3460822105407715
Epoch 730, val loss: 0.8090118765830994
Epoch 740, training loss: 6.463104724884033 = 0.11510930210351944 + 1.0 * 6.347995281219482
Epoch 740, val loss: 0.8166302442550659
Epoch 750, training loss: 6.449919700622559 = 0.10906054079532623 + 1.0 * 6.3408589363098145
Epoch 750, val loss: 0.8243252038955688
Epoch 760, training loss: 6.439947128295898 = 0.10340002924203873 + 1.0 * 6.336546897888184
Epoch 760, val loss: 0.8322508931159973
Epoch 770, training loss: 6.435269832611084 = 0.09807994961738586 + 1.0 * 6.337189674377441
Epoch 770, val loss: 0.8403275609016418
Epoch 780, training loss: 6.4307332038879395 = 0.09309528023004532 + 1.0 * 6.337637901306152
Epoch 780, val loss: 0.8484556078910828
Epoch 790, training loss: 6.428018569946289 = 0.08844051510095596 + 1.0 * 6.339578151702881
Epoch 790, val loss: 0.8565759658813477
Epoch 800, training loss: 6.418086528778076 = 0.08408224582672119 + 1.0 * 6.3340044021606445
Epoch 800, val loss: 0.8647366166114807
Epoch 810, training loss: 6.412472248077393 = 0.0799851343035698 + 1.0 * 6.332487106323242
Epoch 810, val loss: 0.872994065284729
Epoch 820, training loss: 6.417966842651367 = 0.07613147050142288 + 1.0 * 6.3418354988098145
Epoch 820, val loss: 0.8812447786331177
Epoch 830, training loss: 6.403293132781982 = 0.07251828163862228 + 1.0 * 6.330774784088135
Epoch 830, val loss: 0.8893460035324097
Epoch 840, training loss: 6.398525238037109 = 0.06912007927894592 + 1.0 * 6.329405307769775
Epoch 840, val loss: 0.8974888920783997
Epoch 850, training loss: 6.404913425445557 = 0.0659232959151268 + 1.0 * 6.338990211486816
Epoch 850, val loss: 0.9055753946304321
Epoch 860, training loss: 6.391116619110107 = 0.06291941553354263 + 1.0 * 6.328197002410889
Epoch 860, val loss: 0.9135624170303345
Epoch 870, training loss: 6.386668682098389 = 0.06008884310722351 + 1.0 * 6.326580047607422
Epoch 870, val loss: 0.9215289950370789
Epoch 880, training loss: 6.392933368682861 = 0.05741893872618675 + 1.0 * 6.335514545440674
Epoch 880, val loss: 0.9293970465660095
Epoch 890, training loss: 6.3819475173950195 = 0.054913248866796494 + 1.0 * 6.3270344734191895
Epoch 890, val loss: 0.9371758699417114
Epoch 900, training loss: 6.375761032104492 = 0.05254635587334633 + 1.0 * 6.323214530944824
Epoch 900, val loss: 0.9449104070663452
Epoch 910, training loss: 6.381163597106934 = 0.05030840262770653 + 1.0 * 6.330855369567871
Epoch 910, val loss: 0.9525856375694275
Epoch 920, training loss: 6.371786117553711 = 0.04820568114519119 + 1.0 * 6.323580265045166
Epoch 920, val loss: 0.9601423740386963
Epoch 930, training loss: 6.368631839752197 = 0.046216681599617004 + 1.0 * 6.322415351867676
Epoch 930, val loss: 0.9676199555397034
Epoch 940, training loss: 6.368802070617676 = 0.04434095695614815 + 1.0 * 6.324460983276367
Epoch 940, val loss: 0.9750535488128662
Epoch 950, training loss: 6.36161470413208 = 0.04256998747587204 + 1.0 * 6.319044589996338
Epoch 950, val loss: 0.9823402762413025
Epoch 960, training loss: 6.359061241149902 = 0.04089175537228584 + 1.0 * 6.318169593811035
Epoch 960, val loss: 0.9895864129066467
Epoch 970, training loss: 6.364917278289795 = 0.03930424898862839 + 1.0 * 6.325613021850586
Epoch 970, val loss: 0.9967125654220581
Epoch 980, training loss: 6.358811855316162 = 0.037809889763593674 + 1.0 * 6.321002006530762
Epoch 980, val loss: 1.0036295652389526
Epoch 990, training loss: 6.352736473083496 = 0.03639628365635872 + 1.0 * 6.31633996963501
Epoch 990, val loss: 1.0104854106903076
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 10.568584442138672 = 1.9717376232147217 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9774913787841797
Epoch 10, training loss: 10.556326866149902 = 1.9597362279891968 + 1.0 * 8.596590995788574
Epoch 10, val loss: 1.9648884534835815
Epoch 20, training loss: 10.539043426513672 = 1.9445644617080688 + 1.0 * 8.594478607177734
Epoch 20, val loss: 1.9488005638122559
Epoch 30, training loss: 10.50158405303955 = 1.92312753200531 + 1.0 * 8.57845687866211
Epoch 30, val loss: 1.9260306358337402
Epoch 40, training loss: 10.392902374267578 = 1.8947198390960693 + 1.0 * 8.49818229675293
Epoch 40, val loss: 1.8970863819122314
Epoch 50, training loss: 10.081202507019043 = 1.8652743101119995 + 1.0 * 8.215928077697754
Epoch 50, val loss: 1.8680040836334229
Epoch 60, training loss: 9.757688522338867 = 1.840706706047058 + 1.0 * 7.9169816970825195
Epoch 60, val loss: 1.8449541330337524
Epoch 70, training loss: 9.261666297912598 = 1.823647379875183 + 1.0 * 7.438019275665283
Epoch 70, val loss: 1.8297163248062134
Epoch 80, training loss: 8.992456436157227 = 1.8111252784729004 + 1.0 * 7.181331634521484
Epoch 80, val loss: 1.818245530128479
Epoch 90, training loss: 8.824014663696289 = 1.7952675819396973 + 1.0 * 7.028747081756592
Epoch 90, val loss: 1.8034740686416626
Epoch 100, training loss: 8.694756507873535 = 1.7781120538711548 + 1.0 * 6.91664457321167
Epoch 100, val loss: 1.7883549928665161
Epoch 110, training loss: 8.591636657714844 = 1.763102412223816 + 1.0 * 6.8285346031188965
Epoch 110, val loss: 1.7752962112426758
Epoch 120, training loss: 8.506674766540527 = 1.7489248514175415 + 1.0 * 6.757749557495117
Epoch 120, val loss: 1.762412667274475
Epoch 130, training loss: 8.430340766906738 = 1.7333476543426514 + 1.0 * 6.696992874145508
Epoch 130, val loss: 1.7482621669769287
Epoch 140, training loss: 8.372477531433105 = 1.7154655456542969 + 1.0 * 6.657011985778809
Epoch 140, val loss: 1.7326078414916992
Epoch 150, training loss: 8.314908027648926 = 1.6951919794082642 + 1.0 * 6.619716167449951
Epoch 150, val loss: 1.7151832580566406
Epoch 160, training loss: 8.263689041137695 = 1.6720086336135864 + 1.0 * 6.59168004989624
Epoch 160, val loss: 1.6954421997070312
Epoch 170, training loss: 8.216512680053711 = 1.6454682350158691 + 1.0 * 6.571044445037842
Epoch 170, val loss: 1.6728930473327637
Epoch 180, training loss: 8.165718078613281 = 1.6156692504882812 + 1.0 * 6.550048351287842
Epoch 180, val loss: 1.6476508378982544
Epoch 190, training loss: 8.115729331970215 = 1.5823479890823364 + 1.0 * 6.53338098526001
Epoch 190, val loss: 1.6195088624954224
Epoch 200, training loss: 8.0658597946167 = 1.5458284616470337 + 1.0 * 6.520030975341797
Epoch 200, val loss: 1.588921070098877
Epoch 210, training loss: 8.013975143432617 = 1.5072416067123413 + 1.0 * 6.506733417510986
Epoch 210, val loss: 1.556875228881836
Epoch 220, training loss: 7.962324142456055 = 1.466846227645874 + 1.0 * 6.495477676391602
Epoch 220, val loss: 1.5237524509429932
Epoch 230, training loss: 7.913768768310547 = 1.4251731634140015 + 1.0 * 6.488595485687256
Epoch 230, val loss: 1.490193247795105
Epoch 240, training loss: 7.862217903137207 = 1.3834831714630127 + 1.0 * 6.478734970092773
Epoch 240, val loss: 1.4573004245758057
Epoch 250, training loss: 7.813129901885986 = 1.3422646522521973 + 1.0 * 6.470865249633789
Epoch 250, val loss: 1.4256893396377563
Epoch 260, training loss: 7.763652324676514 = 1.301474690437317 + 1.0 * 6.462177753448486
Epoch 260, val loss: 1.3949573040008545
Epoch 270, training loss: 7.715791702270508 = 1.2605502605438232 + 1.0 * 6.455241680145264
Epoch 270, val loss: 1.3647006750106812
Epoch 280, training loss: 7.673539161682129 = 1.21974778175354 + 1.0 * 6.45379114151001
Epoch 280, val loss: 1.3350952863693237
Epoch 290, training loss: 7.623568058013916 = 1.1797019243240356 + 1.0 * 6.44386625289917
Epoch 290, val loss: 1.306675672531128
Epoch 300, training loss: 7.578289031982422 = 1.1400094032287598 + 1.0 * 6.438279628753662
Epoch 300, val loss: 1.2790073156356812
Epoch 310, training loss: 7.5339460372924805 = 1.100388765335083 + 1.0 * 6.433557033538818
Epoch 310, val loss: 1.2518625259399414
Epoch 320, training loss: 7.490874290466309 = 1.0610487461090088 + 1.0 * 6.429825782775879
Epoch 320, val loss: 1.2254469394683838
Epoch 330, training loss: 7.445468425750732 = 1.0221467018127441 + 1.0 * 6.423321723937988
Epoch 330, val loss: 1.199811577796936
Epoch 340, training loss: 7.405032157897949 = 0.9834315180778503 + 1.0 * 6.421600818634033
Epoch 340, val loss: 1.174520492553711
Epoch 350, training loss: 7.363474369049072 = 0.9450984597206116 + 1.0 * 6.4183759689331055
Epoch 350, val loss: 1.1497026681900024
Epoch 360, training loss: 7.320786476135254 = 0.9071260094642639 + 1.0 * 6.413660526275635
Epoch 360, val loss: 1.1254323720932007
Epoch 370, training loss: 7.283627510070801 = 0.8696746826171875 + 1.0 * 6.413952827453613
Epoch 370, val loss: 1.101356029510498
Epoch 380, training loss: 7.240367889404297 = 0.8332960605621338 + 1.0 * 6.407072067260742
Epoch 380, val loss: 1.0782554149627686
Epoch 390, training loss: 7.201102256774902 = 0.7980787754058838 + 1.0 * 6.4030232429504395
Epoch 390, val loss: 1.0556440353393555
Epoch 400, training loss: 7.163947105407715 = 0.7641433477401733 + 1.0 * 6.399803638458252
Epoch 400, val loss: 1.0339123010635376
Epoch 410, training loss: 7.13029146194458 = 0.7318935990333557 + 1.0 * 6.398397922515869
Epoch 410, val loss: 1.0133341550827026
Epoch 420, training loss: 7.096236705780029 = 0.7015421986579895 + 1.0 * 6.3946943283081055
Epoch 420, val loss: 0.9941727519035339
Epoch 430, training loss: 7.064659595489502 = 0.6729341745376587 + 1.0 * 6.391725540161133
Epoch 430, val loss: 0.9760961532592773
Epoch 440, training loss: 7.035048961639404 = 0.6458126902580261 + 1.0 * 6.3892364501953125
Epoch 440, val loss: 0.9592392444610596
Epoch 450, training loss: 7.0048909187316895 = 0.6196103096008301 + 1.0 * 6.385280609130859
Epoch 450, val loss: 0.9432035684585571
Epoch 460, training loss: 6.983465194702148 = 0.5940104126930237 + 1.0 * 6.3894548416137695
Epoch 460, val loss: 0.9276169538497925
Epoch 470, training loss: 6.952080249786377 = 0.5688611268997192 + 1.0 * 6.383219242095947
Epoch 470, val loss: 0.9124634861946106
Epoch 480, training loss: 6.923855781555176 = 0.5437815189361572 + 1.0 * 6.380074501037598
Epoch 480, val loss: 0.8975766897201538
Epoch 490, training loss: 6.899127006530762 = 0.5187033414840698 + 1.0 * 6.380423545837402
Epoch 490, val loss: 0.8827647566795349
Epoch 500, training loss: 6.867265701293945 = 0.49364349246025085 + 1.0 * 6.373622417449951
Epoch 500, val loss: 0.8684055209159851
Epoch 510, training loss: 6.841863632202148 = 0.4685060381889343 + 1.0 * 6.373357772827148
Epoch 510, val loss: 0.8544502258300781
Epoch 520, training loss: 6.815091609954834 = 0.4435882866382599 + 1.0 * 6.3715033531188965
Epoch 520, val loss: 0.8410478830337524
Epoch 530, training loss: 6.798586368560791 = 0.4191208481788635 + 1.0 * 6.379465579986572
Epoch 530, val loss: 0.828655481338501
Epoch 540, training loss: 6.761650085449219 = 0.3954375684261322 + 1.0 * 6.366212368011475
Epoch 540, val loss: 0.8174216151237488
Epoch 550, training loss: 6.7371439933776855 = 0.3725513815879822 + 1.0 * 6.364592552185059
Epoch 550, val loss: 0.8074160814285278
Epoch 560, training loss: 6.713460922241211 = 0.3505476415157318 + 1.0 * 6.362913131713867
Epoch 560, val loss: 0.7988046407699585
Epoch 570, training loss: 6.695952415466309 = 0.32953739166259766 + 1.0 * 6.366415023803711
Epoch 570, val loss: 0.7916419506072998
Epoch 580, training loss: 6.668946743011475 = 0.3096660375595093 + 1.0 * 6.359280586242676
Epoch 580, val loss: 0.7858426570892334
Epoch 590, training loss: 6.65386438369751 = 0.2908075153827667 + 1.0 * 6.363056659698486
Epoch 590, val loss: 0.78130042552948
Epoch 600, training loss: 6.6349263191223145 = 0.2731037139892578 + 1.0 * 6.361822605133057
Epoch 600, val loss: 0.777809739112854
Epoch 610, training loss: 6.610832214355469 = 0.2564438283443451 + 1.0 * 6.354388236999512
Epoch 610, val loss: 0.7754210829734802
Epoch 620, training loss: 6.594248294830322 = 0.24072910845279694 + 1.0 * 6.353518962860107
Epoch 620, val loss: 0.7740135788917542
Epoch 630, training loss: 6.578121185302734 = 0.22597765922546387 + 1.0 * 6.35214376449585
Epoch 630, val loss: 0.7734540104866028
Epoch 640, training loss: 6.5690388679504395 = 0.21219402551651 + 1.0 * 6.356844902038574
Epoch 640, val loss: 0.7736815810203552
Epoch 650, training loss: 6.549041748046875 = 0.1992911398410797 + 1.0 * 6.349750518798828
Epoch 650, val loss: 0.7746371626853943
Epoch 660, training loss: 6.533629894256592 = 0.1871820092201233 + 1.0 * 6.346447944641113
Epoch 660, val loss: 0.7763281464576721
Epoch 670, training loss: 6.523723602294922 = 0.17582817375659943 + 1.0 * 6.347895622253418
Epoch 670, val loss: 0.7786715030670166
Epoch 680, training loss: 6.513896942138672 = 0.16525694727897644 + 1.0 * 6.348639965057373
Epoch 680, val loss: 0.7815179228782654
Epoch 690, training loss: 6.497737407684326 = 0.155422180891037 + 1.0 * 6.342315196990967
Epoch 690, val loss: 0.7847709655761719
Epoch 700, training loss: 6.487853050231934 = 0.14622235298156738 + 1.0 * 6.341630935668945
Epoch 700, val loss: 0.7885481119155884
Epoch 710, training loss: 6.479739665985107 = 0.1376064568758011 + 1.0 * 6.342133045196533
Epoch 710, val loss: 0.7928153872489929
Epoch 720, training loss: 6.472292900085449 = 0.12958158552646637 + 1.0 * 6.342711448669434
Epoch 720, val loss: 0.7973746061325073
Epoch 730, training loss: 6.462395668029785 = 0.12211563438177109 + 1.0 * 6.340280055999756
Epoch 730, val loss: 0.8022788166999817
Epoch 740, training loss: 6.454792022705078 = 0.11515312641859055 + 1.0 * 6.339638710021973
Epoch 740, val loss: 0.8075726628303528
Epoch 750, training loss: 6.442779064178467 = 0.10865551978349686 + 1.0 * 6.334123611450195
Epoch 750, val loss: 0.8131712675094604
Epoch 760, training loss: 6.437742233276367 = 0.102572500705719 + 1.0 * 6.335169792175293
Epoch 760, val loss: 0.8190926313400269
Epoch 770, training loss: 6.436929225921631 = 0.09689448773860931 + 1.0 * 6.3400349617004395
Epoch 770, val loss: 0.8252658247947693
Epoch 780, training loss: 6.424761772155762 = 0.0916171744465828 + 1.0 * 6.333144664764404
Epoch 780, val loss: 0.8315845131874084
Epoch 790, training loss: 6.416851997375488 = 0.08668062835931778 + 1.0 * 6.330171585083008
Epoch 790, val loss: 0.8380785584449768
Epoch 800, training loss: 6.412606239318848 = 0.08206062763929367 + 1.0 * 6.330545425415039
Epoch 800, val loss: 0.8448308110237122
Epoch 810, training loss: 6.408205032348633 = 0.07775505632162094 + 1.0 * 6.330450057983398
Epoch 810, val loss: 0.8517126441001892
Epoch 820, training loss: 6.402799129486084 = 0.07374077290296555 + 1.0 * 6.3290581703186035
Epoch 820, val loss: 0.8585777878761292
Epoch 830, training loss: 6.396821975708008 = 0.06999387592077255 + 1.0 * 6.3268280029296875
Epoch 830, val loss: 0.8656132221221924
Epoch 840, training loss: 6.389711380004883 = 0.06648200005292892 + 1.0 * 6.3232293128967285
Epoch 840, val loss: 0.8727808594703674
Epoch 850, training loss: 6.400657653808594 = 0.06319443136453629 + 1.0 * 6.33746337890625
Epoch 850, val loss: 0.8799716234207153
Epoch 860, training loss: 6.384831428527832 = 0.06012953445315361 + 1.0 * 6.32470178604126
Epoch 860, val loss: 0.8871934413909912
Epoch 870, training loss: 6.379022121429443 = 0.057262636721134186 + 1.0 * 6.3217597007751465
Epoch 870, val loss: 0.8943319320678711
Epoch 880, training loss: 6.385111331939697 = 0.054574329406023026 + 1.0 * 6.330536842346191
Epoch 880, val loss: 0.9015974402427673
Epoch 890, training loss: 6.374075889587402 = 0.0520644374191761 + 1.0 * 6.322011470794678
Epoch 890, val loss: 0.9087561964988708
Epoch 900, training loss: 6.368523597717285 = 0.049702659249305725 + 1.0 * 6.318820953369141
Epoch 900, val loss: 0.915937602519989
Epoch 910, training loss: 6.371535301208496 = 0.047485318034887314 + 1.0 * 6.324049949645996
Epoch 910, val loss: 0.9232178926467896
Epoch 920, training loss: 6.362601280212402 = 0.045400235801935196 + 1.0 * 6.317201137542725
Epoch 920, val loss: 0.9303680658340454
Epoch 930, training loss: 6.364852428436279 = 0.04343939200043678 + 1.0 * 6.321413040161133
Epoch 930, val loss: 0.9374808669090271
Epoch 940, training loss: 6.357329845428467 = 0.041606079787015915 + 1.0 * 6.315723896026611
Epoch 940, val loss: 0.9446071982383728
Epoch 950, training loss: 6.353950500488281 = 0.039875857532024384 + 1.0 * 6.314074516296387
Epoch 950, val loss: 0.9515388607978821
Epoch 960, training loss: 6.350601673126221 = 0.0382460281252861 + 1.0 * 6.3123555183410645
Epoch 960, val loss: 0.9585996270179749
Epoch 970, training loss: 6.363407611846924 = 0.036710720509290695 + 1.0 * 6.326696872711182
Epoch 970, val loss: 0.9655643105506897
Epoch 980, training loss: 6.348349094390869 = 0.035260289907455444 + 1.0 * 6.313088893890381
Epoch 980, val loss: 0.9723914265632629
Epoch 990, training loss: 6.344132423400879 = 0.03389887139201164 + 1.0 * 6.3102335929870605
Epoch 990, val loss: 0.9790728688240051
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8075909330521878
The final CL Acc:0.76543, 0.00630, The final GNN Acc:0.80654, 0.00114
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13136])
remove edge: torch.Size([2, 7938])
updated graph: torch.Size([2, 10518])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.556198120117188 = 1.959365963935852 + 1.0 * 8.596832275390625
Epoch 0, val loss: 1.9558372497558594
Epoch 10, training loss: 10.545001029968262 = 1.9483999013900757 + 1.0 * 8.596601486206055
Epoch 10, val loss: 1.9446797370910645
Epoch 20, training loss: 10.529439926147461 = 1.9346423149108887 + 1.0 * 8.59479808807373
Epoch 20, val loss: 1.930405855178833
Epoch 30, training loss: 10.495333671569824 = 1.915070652961731 + 1.0 * 8.580263137817383
Epoch 30, val loss: 1.9100611209869385
Epoch 40, training loss: 10.382562637329102 = 1.8882288932800293 + 1.0 * 8.49433422088623
Epoch 40, val loss: 1.8832985162734985
Epoch 50, training loss: 10.01546573638916 = 1.8589199781417847 + 1.0 * 8.156545639038086
Epoch 50, val loss: 1.8555943965911865
Epoch 60, training loss: 9.617640495300293 = 1.8354734182357788 + 1.0 * 7.782167434692383
Epoch 60, val loss: 1.834505558013916
Epoch 70, training loss: 9.071922302246094 = 1.8187732696533203 + 1.0 * 7.253149032592773
Epoch 70, val loss: 1.8192522525787354
Epoch 80, training loss: 8.81692886352539 = 1.8042901754379272 + 1.0 * 7.012639045715332
Epoch 80, val loss: 1.805503487586975
Epoch 90, training loss: 8.693117141723633 = 1.7859141826629639 + 1.0 * 6.907203197479248
Epoch 90, val loss: 1.7884541749954224
Epoch 100, training loss: 8.581915855407715 = 1.766236662864685 + 1.0 * 6.81567907333374
Epoch 100, val loss: 1.7715559005737305
Epoch 110, training loss: 8.49666690826416 = 1.7485159635543823 + 1.0 * 6.748150825500488
Epoch 110, val loss: 1.7563368082046509
Epoch 120, training loss: 8.427103996276855 = 1.7301353216171265 + 1.0 * 6.6969685554504395
Epoch 120, val loss: 1.7397013902664185
Epoch 130, training loss: 8.36499309539795 = 1.7091692686080933 + 1.0 * 6.655824184417725
Epoch 130, val loss: 1.7205255031585693
Epoch 140, training loss: 8.308125495910645 = 1.684827446937561 + 1.0 * 6.623297691345215
Epoch 140, val loss: 1.6985743045806885
Epoch 150, training loss: 8.25224781036377 = 1.6568429470062256 + 1.0 * 6.595404624938965
Epoch 150, val loss: 1.6736948490142822
Epoch 160, training loss: 8.194353103637695 = 1.6246033906936646 + 1.0 * 6.56974983215332
Epoch 160, val loss: 1.6452264785766602
Epoch 170, training loss: 8.136720657348633 = 1.5876364707946777 + 1.0 * 6.549084663391113
Epoch 170, val loss: 1.6125822067260742
Epoch 180, training loss: 8.075551986694336 = 1.546146035194397 + 1.0 * 6.52940559387207
Epoch 180, val loss: 1.576227068901062
Epoch 190, training loss: 8.012948989868164 = 1.5007975101470947 + 1.0 * 6.51215124130249
Epoch 190, val loss: 1.5367074012756348
Epoch 200, training loss: 7.950544357299805 = 1.4517700672149658 + 1.0 * 6.498774528503418
Epoch 200, val loss: 1.494418978691101
Epoch 210, training loss: 7.885990142822266 = 1.4001524448394775 + 1.0 * 6.485837459564209
Epoch 210, val loss: 1.4504268169403076
Epoch 220, training loss: 7.822542667388916 = 1.3466538190841675 + 1.0 * 6.475888729095459
Epoch 220, val loss: 1.4055402278900146
Epoch 230, training loss: 7.762197971343994 = 1.2926257848739624 + 1.0 * 6.469572067260742
Epoch 230, val loss: 1.3609395027160645
Epoch 240, training loss: 7.697134494781494 = 1.238850712776184 + 1.0 * 6.4582839012146
Epoch 240, val loss: 1.3171586990356445
Epoch 250, training loss: 7.635512828826904 = 1.1849614381790161 + 1.0 * 6.450551509857178
Epoch 250, val loss: 1.2737778425216675
Epoch 260, training loss: 7.575666427612305 = 1.1313371658325195 + 1.0 * 6.444329261779785
Epoch 260, val loss: 1.2311508655548096
Epoch 270, training loss: 7.519733428955078 = 1.0790992975234985 + 1.0 * 6.440634250640869
Epoch 270, val loss: 1.19001042842865
Epoch 280, training loss: 7.459837436676025 = 1.0276575088500977 + 1.0 * 6.432179927825928
Epoch 280, val loss: 1.150068759918213
Epoch 290, training loss: 7.404479026794434 = 0.9771595597267151 + 1.0 * 6.427319526672363
Epoch 290, val loss: 1.111228585243225
Epoch 300, training loss: 7.356846809387207 = 0.9284390211105347 + 1.0 * 6.428407669067383
Epoch 300, val loss: 1.0742789506912231
Epoch 310, training loss: 7.300355911254883 = 0.8823224902153015 + 1.0 * 6.418033599853516
Epoch 310, val loss: 1.0399587154388428
Epoch 320, training loss: 7.2523064613342285 = 0.8384600281715393 + 1.0 * 6.413846492767334
Epoch 320, val loss: 1.007908821105957
Epoch 330, training loss: 7.213679313659668 = 0.7971031069755554 + 1.0 * 6.416576385498047
Epoch 330, val loss: 0.9784916639328003
Epoch 340, training loss: 7.165287971496582 = 0.7588449716567993 + 1.0 * 6.406443119049072
Epoch 340, val loss: 0.952303409576416
Epoch 350, training loss: 7.12457275390625 = 0.7231376767158508 + 1.0 * 6.401434898376465
Epoch 350, val loss: 0.9289196133613586
Epoch 360, training loss: 7.08830451965332 = 0.6897459030151367 + 1.0 * 6.398558616638184
Epoch 360, val loss: 0.9081627726554871
Epoch 370, training loss: 7.054012298583984 = 0.6587157249450684 + 1.0 * 6.395296573638916
Epoch 370, val loss: 0.8899661898612976
Epoch 380, training loss: 7.020163059234619 = 0.6294403672218323 + 1.0 * 6.390722751617432
Epoch 380, val loss: 0.8738600611686707
Epoch 390, training loss: 6.99246072769165 = 0.6016346216201782 + 1.0 * 6.390826225280762
Epoch 390, val loss: 0.8596504926681519
Epoch 400, training loss: 6.963762283325195 = 0.5753559470176697 + 1.0 * 6.388406276702881
Epoch 400, val loss: 0.8473502993583679
Epoch 410, training loss: 6.931765556335449 = 0.5505806803703308 + 1.0 * 6.381185054779053
Epoch 410, val loss: 0.8365880250930786
Epoch 420, training loss: 6.907506942749023 = 0.526774525642395 + 1.0 * 6.380732536315918
Epoch 420, val loss: 0.827122151851654
Epoch 430, training loss: 6.88264274597168 = 0.5039164423942566 + 1.0 * 6.378726482391357
Epoch 430, val loss: 0.8189266920089722
Epoch 440, training loss: 6.855103969573975 = 0.4819950759410858 + 1.0 * 6.373108863830566
Epoch 440, val loss: 0.8116078972816467
Epoch 450, training loss: 6.831271648406982 = 0.46072056889533997 + 1.0 * 6.370551109313965
Epoch 450, val loss: 0.8051659464836121
Epoch 460, training loss: 6.822172164916992 = 0.4400925636291504 + 1.0 * 6.382079601287842
Epoch 460, val loss: 0.7995510101318359
Epoch 470, training loss: 6.788029193878174 = 0.420400470495224 + 1.0 * 6.367628574371338
Epoch 470, val loss: 0.7947208285331726
Epoch 480, training loss: 6.765555381774902 = 0.4014210104942322 + 1.0 * 6.364134311676025
Epoch 480, val loss: 0.7905017137527466
Epoch 490, training loss: 6.744533538818359 = 0.382980078458786 + 1.0 * 6.36155366897583
Epoch 490, val loss: 0.7870263457298279
Epoch 500, training loss: 6.72671365737915 = 0.3652430772781372 + 1.0 * 6.361470699310303
Epoch 500, val loss: 0.7842221260070801
Epoch 510, training loss: 6.704998016357422 = 0.348259836435318 + 1.0 * 6.356738090515137
Epoch 510, val loss: 0.7820493578910828
Epoch 520, training loss: 6.686246871948242 = 0.33193913102149963 + 1.0 * 6.354307651519775
Epoch 520, val loss: 0.7805140018463135
Epoch 530, training loss: 6.6711039543151855 = 0.3162330389022827 + 1.0 * 6.354870796203613
Epoch 530, val loss: 0.7797207832336426
Epoch 540, training loss: 6.652266025543213 = 0.30120036005973816 + 1.0 * 6.351065635681152
Epoch 540, val loss: 0.7795647978782654
Epoch 550, training loss: 6.638214111328125 = 0.2868664264678955 + 1.0 * 6.35134744644165
Epoch 550, val loss: 0.779918909072876
Epoch 560, training loss: 6.622893810272217 = 0.2731531858444214 + 1.0 * 6.349740505218506
Epoch 560, val loss: 0.7808735966682434
Epoch 570, training loss: 6.607812881469727 = 0.2600575089454651 + 1.0 * 6.347755432128906
Epoch 570, val loss: 0.7823654413223267
Epoch 580, training loss: 6.5941572189331055 = 0.2475486397743225 + 1.0 * 6.346608638763428
Epoch 580, val loss: 0.7843193411827087
Epoch 590, training loss: 6.579195976257324 = 0.23559464514255524 + 1.0 * 6.343601226806641
Epoch 590, val loss: 0.7866734266281128
Epoch 600, training loss: 6.56799840927124 = 0.224156454205513 + 1.0 * 6.343842029571533
Epoch 600, val loss: 0.7894989252090454
Epoch 610, training loss: 6.552656650543213 = 0.2132359892129898 + 1.0 * 6.339420795440674
Epoch 610, val loss: 0.7927086353302002
Epoch 620, training loss: 6.540726184844971 = 0.20275914669036865 + 1.0 * 6.3379669189453125
Epoch 620, val loss: 0.7963034510612488
Epoch 630, training loss: 6.537986755371094 = 0.1926986426115036 + 1.0 * 6.345288276672363
Epoch 630, val loss: 0.8003005385398865
Epoch 640, training loss: 6.525223731994629 = 0.18311305344104767 + 1.0 * 6.342110633850098
Epoch 640, val loss: 0.8046028017997742
Epoch 650, training loss: 6.5081257820129395 = 0.1739964783191681 + 1.0 * 6.334129333496094
Epoch 650, val loss: 0.809153139591217
Epoch 660, training loss: 6.496339797973633 = 0.16528211534023285 + 1.0 * 6.331057548522949
Epoch 660, val loss: 0.8140684366226196
Epoch 670, training loss: 6.487335205078125 = 0.15693411231040955 + 1.0 * 6.3304009437561035
Epoch 670, val loss: 0.8193345069885254
Epoch 680, training loss: 6.489619731903076 = 0.14897269010543823 + 1.0 * 6.340647220611572
Epoch 680, val loss: 0.8248282074928284
Epoch 690, training loss: 6.471993446350098 = 0.14140447974205017 + 1.0 * 6.3305888175964355
Epoch 690, val loss: 0.8304691910743713
Epoch 700, training loss: 6.462594985961914 = 0.13424012064933777 + 1.0 * 6.328354835510254
Epoch 700, val loss: 0.8362926244735718
Epoch 710, training loss: 6.453896522521973 = 0.12744635343551636 + 1.0 * 6.326450347900391
Epoch 710, val loss: 0.8423560261726379
Epoch 720, training loss: 6.448671340942383 = 0.1210007295012474 + 1.0 * 6.327670574188232
Epoch 720, val loss: 0.8486049175262451
Epoch 730, training loss: 6.442093372344971 = 0.11491209268569946 + 1.0 * 6.327181339263916
Epoch 730, val loss: 0.8549291491508484
Epoch 740, training loss: 6.434210777282715 = 0.10915905982255936 + 1.0 * 6.325051784515381
Epoch 740, val loss: 0.8613375425338745
Epoch 750, training loss: 6.4263787269592285 = 0.10373402386903763 + 1.0 * 6.3226447105407715
Epoch 750, val loss: 0.8678059577941895
Epoch 760, training loss: 6.419105529785156 = 0.09862419962882996 + 1.0 * 6.320481300354004
Epoch 760, val loss: 0.8743093609809875
Epoch 770, training loss: 6.413218021392822 = 0.09380270540714264 + 1.0 * 6.319415092468262
Epoch 770, val loss: 0.880892813205719
Epoch 780, training loss: 6.413593769073486 = 0.08925802260637283 + 1.0 * 6.32433557510376
Epoch 780, val loss: 0.8875113129615784
Epoch 790, training loss: 6.4016032218933105 = 0.08500294387340546 + 1.0 * 6.316600322723389
Epoch 790, val loss: 0.8940662741661072
Epoch 800, training loss: 6.396933078765869 = 0.08098961412906647 + 1.0 * 6.315943241119385
Epoch 800, val loss: 0.9006403088569641
Epoch 810, training loss: 6.402363300323486 = 0.07720133662223816 + 1.0 * 6.325161933898926
Epoch 810, val loss: 0.9072489142417908
Epoch 820, training loss: 6.391550540924072 = 0.07363026589155197 + 1.0 * 6.317920207977295
Epoch 820, val loss: 0.9137529134750366
Epoch 830, training loss: 6.382319450378418 = 0.07028533518314362 + 1.0 * 6.3120341300964355
Epoch 830, val loss: 0.9202055931091309
Epoch 840, training loss: 6.380484580993652 = 0.06712139397859573 + 1.0 * 6.313363075256348
Epoch 840, val loss: 0.9266673922538757
Epoch 850, training loss: 6.375393390655518 = 0.0641389712691307 + 1.0 * 6.311254501342773
Epoch 850, val loss: 0.9331654906272888
Epoch 860, training loss: 6.373174667358398 = 0.0613354817032814 + 1.0 * 6.3118391036987305
Epoch 860, val loss: 0.9395342469215393
Epoch 870, training loss: 6.368213653564453 = 0.05869342386722565 + 1.0 * 6.309520244598389
Epoch 870, val loss: 0.9458951950073242
Epoch 880, training loss: 6.367396354675293 = 0.05619179829955101 + 1.0 * 6.311204433441162
Epoch 880, val loss: 0.9522494673728943
Epoch 890, training loss: 6.365180969238281 = 0.053830768913030624 + 1.0 * 6.311350345611572
Epoch 890, val loss: 0.9585651755332947
Epoch 900, training loss: 6.357457637786865 = 0.0516059584915638 + 1.0 * 6.305851459503174
Epoch 900, val loss: 0.9648128747940063
Epoch 910, training loss: 6.354893207550049 = 0.04950033500790596 + 1.0 * 6.305392742156982
Epoch 910, val loss: 0.9710042476654053
Epoch 920, training loss: 6.361089706420898 = 0.047506436705589294 + 1.0 * 6.3135833740234375
Epoch 920, val loss: 0.9771719574928284
Epoch 930, training loss: 6.350545883178711 = 0.04563620686531067 + 1.0 * 6.304909706115723
Epoch 930, val loss: 0.9832121729850769
Epoch 940, training loss: 6.346635341644287 = 0.0438607856631279 + 1.0 * 6.302774429321289
Epoch 940, val loss: 0.9891589283943176
Epoch 950, training loss: 6.346175193786621 = 0.04217591881752014 + 1.0 * 6.303999423980713
Epoch 950, val loss: 0.9951214790344238
Epoch 960, training loss: 6.34328031539917 = 0.04058102145791054 + 1.0 * 6.302699089050293
Epoch 960, val loss: 1.001051902770996
Epoch 970, training loss: 6.342214584350586 = 0.03907181695103645 + 1.0 * 6.303142547607422
Epoch 970, val loss: 1.0068787336349487
Epoch 980, training loss: 6.341404438018799 = 0.03764010965824127 + 1.0 * 6.303764343261719
Epoch 980, val loss: 1.0126335620880127
Epoch 990, training loss: 6.337629318237305 = 0.03628253936767578 + 1.0 * 6.301346778869629
Epoch 990, val loss: 1.018316626548767
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 10.528796195983887 = 1.9319790601730347 + 1.0 * 8.596817016601562
Epoch 0, val loss: 1.9331893920898438
Epoch 10, training loss: 10.5192289352417 = 1.9227466583251953 + 1.0 * 8.596482276916504
Epoch 10, val loss: 1.9246026277542114
Epoch 20, training loss: 10.50500202178955 = 1.9109488725662231 + 1.0 * 8.594053268432617
Epoch 20, val loss: 1.9132912158966064
Epoch 30, training loss: 10.469147682189941 = 1.8943151235580444 + 1.0 * 8.574832916259766
Epoch 30, val loss: 1.897125005722046
Epoch 40, training loss: 10.289350509643555 = 1.8725026845932007 + 1.0 * 8.416848182678223
Epoch 40, val loss: 1.8765227794647217
Epoch 50, training loss: 9.617005348205566 = 1.8495382070541382 + 1.0 * 7.767467498779297
Epoch 50, val loss: 1.8552093505859375
Epoch 60, training loss: 9.152069091796875 = 1.8321213722229004 + 1.0 * 7.319948196411133
Epoch 60, val loss: 1.8389217853546143
Epoch 70, training loss: 8.860390663146973 = 1.8192890882492065 + 1.0 * 7.041101932525635
Epoch 70, val loss: 1.8264247179031372
Epoch 80, training loss: 8.717528343200684 = 1.8054481744766235 + 1.0 * 6.912079811096191
Epoch 80, val loss: 1.8131204843521118
Epoch 90, training loss: 8.596678733825684 = 1.7885167598724365 + 1.0 * 6.808161735534668
Epoch 90, val loss: 1.7978850603103638
Epoch 100, training loss: 8.511223793029785 = 1.771416425704956 + 1.0 * 6.73980712890625
Epoch 100, val loss: 1.782509207725525
Epoch 110, training loss: 8.44327449798584 = 1.7540602684020996 + 1.0 * 6.68921422958374
Epoch 110, val loss: 1.7663639783859253
Epoch 120, training loss: 8.380885124206543 = 1.7351422309875488 + 1.0 * 6.645742893218994
Epoch 120, val loss: 1.7487852573394775
Epoch 130, training loss: 8.325243949890137 = 1.7135093212127686 + 1.0 * 6.611734867095947
Epoch 130, val loss: 1.7291510105133057
Epoch 140, training loss: 8.272554397583008 = 1.6883854866027832 + 1.0 * 6.584168910980225
Epoch 140, val loss: 1.7068276405334473
Epoch 150, training loss: 8.221641540527344 = 1.6590770483016968 + 1.0 * 6.562564849853516
Epoch 150, val loss: 1.680971384048462
Epoch 160, training loss: 8.169295310974121 = 1.6245386600494385 + 1.0 * 6.5447564125061035
Epoch 160, val loss: 1.6507545709609985
Epoch 170, training loss: 8.11628246307373 = 1.5846985578536987 + 1.0 * 6.531583786010742
Epoch 170, val loss: 1.6162261962890625
Epoch 180, training loss: 8.056171417236328 = 1.5397297143936157 + 1.0 * 6.516441345214844
Epoch 180, val loss: 1.5774152278900146
Epoch 190, training loss: 7.993617057800293 = 1.489791750907898 + 1.0 * 6.5038251876831055
Epoch 190, val loss: 1.5348645448684692
Epoch 200, training loss: 7.932667255401611 = 1.4359345436096191 + 1.0 * 6.496732711791992
Epoch 200, val loss: 1.489582896232605
Epoch 210, training loss: 7.864253044128418 = 1.3808823823928833 + 1.0 * 6.483370780944824
Epoch 210, val loss: 1.4438005685806274
Epoch 220, training loss: 7.798012733459473 = 1.3254187107086182 + 1.0 * 6.472594261169434
Epoch 220, val loss: 1.398073434829712
Epoch 230, training loss: 7.73446798324585 = 1.270776391029358 + 1.0 * 6.463691711425781
Epoch 230, val loss: 1.3538721799850464
Epoch 240, training loss: 7.674074172973633 = 1.2180798053741455 + 1.0 * 6.455994606018066
Epoch 240, val loss: 1.311830997467041
Epoch 250, training loss: 7.617858409881592 = 1.1669621467590332 + 1.0 * 6.450896263122559
Epoch 250, val loss: 1.27165949344635
Epoch 260, training loss: 7.561756134033203 = 1.1173784732818604 + 1.0 * 6.444377422332764
Epoch 260, val loss: 1.233288288116455
Epoch 270, training loss: 7.508034706115723 = 1.0694471597671509 + 1.0 * 6.438587665557861
Epoch 270, val loss: 1.1965776681900024
Epoch 280, training loss: 7.452502250671387 = 1.0223616361618042 + 1.0 * 6.430140495300293
Epoch 280, val loss: 1.1606919765472412
Epoch 290, training loss: 7.4013471603393555 = 0.9758557081222534 + 1.0 * 6.4254913330078125
Epoch 290, val loss: 1.125401258468628
Epoch 300, training loss: 7.35542106628418 = 0.930622935295105 + 1.0 * 6.424798011779785
Epoch 300, val loss: 1.091102957725525
Epoch 310, training loss: 7.301885604858398 = 0.8868154883384705 + 1.0 * 6.415070056915283
Epoch 310, val loss: 1.058107614517212
Epoch 320, training loss: 7.2571821212768555 = 0.844325602054596 + 1.0 * 6.412856578826904
Epoch 320, val loss: 1.0260611772537231
Epoch 330, training loss: 7.214051723480225 = 0.8038712739944458 + 1.0 * 6.410180568695068
Epoch 330, val loss: 0.9955137968063354
Epoch 340, training loss: 7.167956352233887 = 0.7651777863502502 + 1.0 * 6.402778625488281
Epoch 340, val loss: 0.9666808843612671
Epoch 350, training loss: 7.135239601135254 = 0.7279720306396484 + 1.0 * 6.4072675704956055
Epoch 350, val loss: 0.9392727017402649
Epoch 360, training loss: 7.0896100997924805 = 0.6927602291107178 + 1.0 * 6.396850109100342
Epoch 360, val loss: 0.9137765169143677
Epoch 370, training loss: 7.050434112548828 = 0.6589757800102234 + 1.0 * 6.391458511352539
Epoch 370, val loss: 0.8899754285812378
Epoch 380, training loss: 7.016432285308838 = 0.6262570023536682 + 1.0 * 6.3901753425598145
Epoch 380, val loss: 0.8673821687698364
Epoch 390, training loss: 6.981482982635498 = 0.5947499871253967 + 1.0 * 6.386733055114746
Epoch 390, val loss: 0.8461591601371765
Epoch 400, training loss: 6.946835994720459 = 0.5642860531806946 + 1.0 * 6.38254976272583
Epoch 400, val loss: 0.826271116733551
Epoch 410, training loss: 6.914156913757324 = 0.5344908237457275 + 1.0 * 6.379666328430176
Epoch 410, val loss: 0.8073886632919312
Epoch 420, training loss: 6.888779640197754 = 0.5053831338882446 + 1.0 * 6.383396625518799
Epoch 420, val loss: 0.7894772291183472
Epoch 430, training loss: 6.855797290802002 = 0.4771832227706909 + 1.0 * 6.3786139488220215
Epoch 430, val loss: 0.7726932168006897
Epoch 440, training loss: 6.829228401184082 = 0.4498502016067505 + 1.0 * 6.379378318786621
Epoch 440, val loss: 0.7569563984870911
Epoch 450, training loss: 6.794219970703125 = 0.42337197065353394 + 1.0 * 6.370848178863525
Epoch 450, val loss: 0.7422669529914856
Epoch 460, training loss: 6.765054702758789 = 0.3976511061191559 + 1.0 * 6.367403507232666
Epoch 460, val loss: 0.7286215424537659
Epoch 470, training loss: 6.745789527893066 = 0.3727719783782959 + 1.0 * 6.37301778793335
Epoch 470, val loss: 0.7160012722015381
Epoch 480, training loss: 6.7130208015441895 = 0.34899768233299255 + 1.0 * 6.364023208618164
Epoch 480, val loss: 0.7046063542366028
Epoch 490, training loss: 6.689111709594727 = 0.3262534439563751 + 1.0 * 6.362858295440674
Epoch 490, val loss: 0.6942869424819946
Epoch 500, training loss: 6.664893627166748 = 0.3047138750553131 + 1.0 * 6.360179901123047
Epoch 500, val loss: 0.6852209568023682
Epoch 510, training loss: 6.642958641052246 = 0.2843690812587738 + 1.0 * 6.3585896492004395
Epoch 510, val loss: 0.6774101853370667
Epoch 520, training loss: 6.622636318206787 = 0.26525264978408813 + 1.0 * 6.357383728027344
Epoch 520, val loss: 0.6707650423049927
Epoch 530, training loss: 6.6016411781311035 = 0.2473290115594864 + 1.0 * 6.354311943054199
Epoch 530, val loss: 0.6652295589447021
Epoch 540, training loss: 6.582943439483643 = 0.2305530309677124 + 1.0 * 6.352390289306641
Epoch 540, val loss: 0.6608395576477051
Epoch 550, training loss: 6.566849708557129 = 0.21493256092071533 + 1.0 * 6.351917266845703
Epoch 550, val loss: 0.6574837565422058
Epoch 560, training loss: 6.550665378570557 = 0.20053383708000183 + 1.0 * 6.350131511688232
Epoch 560, val loss: 0.6551671624183655
Epoch 570, training loss: 6.533609390258789 = 0.18716563284397125 + 1.0 * 6.3464436531066895
Epoch 570, val loss: 0.6536990404129028
Epoch 580, training loss: 6.527760028839111 = 0.1747661828994751 + 1.0 * 6.352993965148926
Epoch 580, val loss: 0.6531496047973633
Epoch 590, training loss: 6.511397361755371 = 0.16336922347545624 + 1.0 * 6.348028182983398
Epoch 590, val loss: 0.6533687710762024
Epoch 600, training loss: 6.496335983276367 = 0.15287460386753082 + 1.0 * 6.343461513519287
Epoch 600, val loss: 0.6542458534240723
Epoch 610, training loss: 6.491793632507324 = 0.1431645005941391 + 1.0 * 6.348628997802734
Epoch 610, val loss: 0.6557862162590027
Epoch 620, training loss: 6.474030494689941 = 0.13425301015377045 + 1.0 * 6.33977746963501
Epoch 620, val loss: 0.6580219268798828
Epoch 630, training loss: 6.464648723602295 = 0.12600958347320557 + 1.0 * 6.338639259338379
Epoch 630, val loss: 0.6606997847557068
Epoch 640, training loss: 6.456663131713867 = 0.1183946505188942 + 1.0 * 6.338268280029297
Epoch 640, val loss: 0.6638265252113342
Epoch 650, training loss: 6.446484565734863 = 0.11136423051357269 + 1.0 * 6.33512020111084
Epoch 650, val loss: 0.6673973202705383
Epoch 660, training loss: 6.437568187713623 = 0.10484621673822403 + 1.0 * 6.332722187042236
Epoch 660, val loss: 0.6713463664054871
Epoch 670, training loss: 6.444356441497803 = 0.09880644828081131 + 1.0 * 6.345550060272217
Epoch 670, val loss: 0.675621747970581
Epoch 680, training loss: 6.426174640655518 = 0.09326936304569244 + 1.0 * 6.332905292510986
Epoch 680, val loss: 0.6801877021789551
Epoch 690, training loss: 6.417318344116211 = 0.08813483268022537 + 1.0 * 6.329183578491211
Epoch 690, val loss: 0.6849105954170227
Epoch 700, training loss: 6.410210609436035 = 0.08335063606500626 + 1.0 * 6.326859951019287
Epoch 700, val loss: 0.6898481249809265
Epoch 710, training loss: 6.416101932525635 = 0.07888240367174149 + 1.0 * 6.337219715118408
Epoch 710, val loss: 0.6949201226234436
Epoch 720, training loss: 6.405680179595947 = 0.07476441562175751 + 1.0 * 6.330915927886963
Epoch 720, val loss: 0.7002155184745789
Epoch 730, training loss: 6.395566940307617 = 0.07091834396123886 + 1.0 * 6.324648380279541
Epoch 730, val loss: 0.7055522799491882
Epoch 740, training loss: 6.390150547027588 = 0.06733253598213196 + 1.0 * 6.322817802429199
Epoch 740, val loss: 0.7109335064888
Epoch 750, training loss: 6.387217044830322 = 0.06397917121648788 + 1.0 * 6.323237895965576
Epoch 750, val loss: 0.7164020538330078
Epoch 760, training loss: 6.3836517333984375 = 0.06084684655070305 + 1.0 * 6.322804927825928
Epoch 760, val loss: 0.7219527363777161
Epoch 770, training loss: 6.376340866088867 = 0.057930838316679 + 1.0 * 6.3184099197387695
Epoch 770, val loss: 0.72750324010849
Epoch 780, training loss: 6.3733344078063965 = 0.05520164966583252 + 1.0 * 6.3181328773498535
Epoch 780, val loss: 0.733059287071228
Epoch 790, training loss: 6.373626232147217 = 0.05264231562614441 + 1.0 * 6.32098388671875
Epoch 790, val loss: 0.7386233806610107
Epoch 800, training loss: 6.369357585906982 = 0.050247956067323685 + 1.0 * 6.3191094398498535
Epoch 800, val loss: 0.7442145943641663
Epoch 810, training loss: 6.366800308227539 = 0.04800998419523239 + 1.0 * 6.318790435791016
Epoch 810, val loss: 0.7496618032455444
Epoch 820, training loss: 6.359856128692627 = 0.045916598290205 + 1.0 * 6.313939571380615
Epoch 820, val loss: 0.755183756351471
Epoch 830, training loss: 6.356978893280029 = 0.043942272663116455 + 1.0 * 6.3130364418029785
Epoch 830, val loss: 0.7606700658798218
Epoch 840, training loss: 6.355013847351074 = 0.04207833856344223 + 1.0 * 6.3129353523254395
Epoch 840, val loss: 0.7661377191543579
Epoch 850, training loss: 6.359766006469727 = 0.04032757878303528 + 1.0 * 6.319438457489014
Epoch 850, val loss: 0.7715628147125244
Epoch 860, training loss: 6.350322723388672 = 0.03868236392736435 + 1.0 * 6.31164026260376
Epoch 860, val loss: 0.7768985033035278
Epoch 870, training loss: 6.345952033996582 = 0.03713831305503845 + 1.0 * 6.308813571929932
Epoch 870, val loss: 0.7821879982948303
Epoch 880, training loss: 6.344552040100098 = 0.03567643463611603 + 1.0 * 6.308875560760498
Epoch 880, val loss: 0.7874637246131897
Epoch 890, training loss: 6.348843097686768 = 0.034294918179512024 + 1.0 * 6.314548015594482
Epoch 890, val loss: 0.7926635146141052
Epoch 900, training loss: 6.34564733505249 = 0.0329974927008152 + 1.0 * 6.312649726867676
Epoch 900, val loss: 0.7978509664535522
Epoch 910, training loss: 6.337726593017578 = 0.0317712239921093 + 1.0 * 6.305955410003662
Epoch 910, val loss: 0.8029652833938599
Epoch 920, training loss: 6.335378646850586 = 0.030610576272010803 + 1.0 * 6.304768085479736
Epoch 920, val loss: 0.808040976524353
Epoch 930, training loss: 6.333699703216553 = 0.029504436999559402 + 1.0 * 6.304195404052734
Epoch 930, val loss: 0.8129737973213196
Epoch 940, training loss: 6.339978218078613 = 0.02845483087003231 + 1.0 * 6.3115234375
Epoch 940, val loss: 0.8179337978363037
Epoch 950, training loss: 6.334046840667725 = 0.027463292703032494 + 1.0 * 6.306583404541016
Epoch 950, val loss: 0.82282555103302
Epoch 960, training loss: 6.335508346557617 = 0.026523690670728683 + 1.0 * 6.308984756469727
Epoch 960, val loss: 0.8275967240333557
Epoch 970, training loss: 6.328307151794434 = 0.02563641406595707 + 1.0 * 6.302670955657959
Epoch 970, val loss: 0.832328200340271
Epoch 980, training loss: 6.324151515960693 = 0.024787116795778275 + 1.0 * 6.2993645668029785
Epoch 980, val loss: 0.8370062112808228
Epoch 990, training loss: 6.326173782348633 = 0.023976193740963936 + 1.0 * 6.302197456359863
Epoch 990, val loss: 0.841633141040802
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 10.523724555969238 = 1.9269412755966187 + 1.0 * 8.596783638000488
Epoch 0, val loss: 1.9239360094070435
Epoch 10, training loss: 10.513497352600098 = 1.9171785116195679 + 1.0 * 8.596319198608398
Epoch 10, val loss: 1.9135732650756836
Epoch 20, training loss: 10.497476577758789 = 1.9047930240631104 + 1.0 * 8.592683792114258
Epoch 20, val loss: 1.9003043174743652
Epoch 30, training loss: 10.451990127563477 = 1.8874292373657227 + 1.0 * 8.564560890197754
Epoch 30, val loss: 1.8819053173065186
Epoch 40, training loss: 10.234403610229492 = 1.8657828569412231 + 1.0 * 8.368620872497559
Epoch 40, val loss: 1.8603754043579102
Epoch 50, training loss: 9.711888313293457 = 1.8432585000991821 + 1.0 * 7.8686299324035645
Epoch 50, val loss: 1.8395122289657593
Epoch 60, training loss: 9.231233596801758 = 1.8270131349563599 + 1.0 * 7.404220104217529
Epoch 60, val loss: 1.825714111328125
Epoch 70, training loss: 8.953385353088379 = 1.815050721168518 + 1.0 * 7.138334274291992
Epoch 70, val loss: 1.8147504329681396
Epoch 80, training loss: 8.807042121887207 = 1.8007993698120117 + 1.0 * 7.006242752075195
Epoch 80, val loss: 1.8015427589416504
Epoch 90, training loss: 8.682160377502441 = 1.7844065427780151 + 1.0 * 6.897753715515137
Epoch 90, val loss: 1.7871918678283691
Epoch 100, training loss: 8.578804016113281 = 1.7685452699661255 + 1.0 * 6.810258388519287
Epoch 100, val loss: 1.7735254764556885
Epoch 110, training loss: 8.488618850708008 = 1.7530823945999146 + 1.0 * 6.735536098480225
Epoch 110, val loss: 1.759806513786316
Epoch 120, training loss: 8.412680625915527 = 1.7359000444412231 + 1.0 * 6.676780700683594
Epoch 120, val loss: 1.7446708679199219
Epoch 130, training loss: 8.351157188415527 = 1.7156398296356201 + 1.0 * 6.635517597198486
Epoch 130, val loss: 1.7274383306503296
Epoch 140, training loss: 8.295246124267578 = 1.6920373439788818 + 1.0 * 6.603209018707275
Epoch 140, val loss: 1.7076787948608398
Epoch 150, training loss: 8.240656852722168 = 1.6646901369094849 + 1.0 * 6.575966835021973
Epoch 150, val loss: 1.6847585439682007
Epoch 160, training loss: 8.186984062194824 = 1.633174180984497 + 1.0 * 6.553810119628906
Epoch 160, val loss: 1.6581766605377197
Epoch 170, training loss: 8.131370544433594 = 1.59698486328125 + 1.0 * 6.534385681152344
Epoch 170, val loss: 1.627650499343872
Epoch 180, training loss: 8.075997352600098 = 1.5555849075317383 + 1.0 * 6.520412445068359
Epoch 180, val loss: 1.5927680730819702
Epoch 190, training loss: 8.01602554321289 = 1.509759783744812 + 1.0 * 6.506266117095947
Epoch 190, val loss: 1.5541563034057617
Epoch 200, training loss: 7.956984996795654 = 1.4602116346359253 + 1.0 * 6.4967732429504395
Epoch 200, val loss: 1.5124173164367676
Epoch 210, training loss: 7.893752098083496 = 1.4083807468414307 + 1.0 * 6.4853715896606445
Epoch 210, val loss: 1.4689451456069946
Epoch 220, training loss: 7.831708908081055 = 1.3554103374481201 + 1.0 * 6.4762983322143555
Epoch 220, val loss: 1.4246470928192139
Epoch 230, training loss: 7.769588470458984 = 1.3019037246704102 + 1.0 * 6.467684745788574
Epoch 230, val loss: 1.3800063133239746
Epoch 240, training loss: 7.7130584716796875 = 1.2482082843780518 + 1.0 * 6.464849948883057
Epoch 240, val loss: 1.3351678848266602
Epoch 250, training loss: 7.6494340896606445 = 1.195372223854065 + 1.0 * 6.454061985015869
Epoch 250, val loss: 1.291256070137024
Epoch 260, training loss: 7.589163780212402 = 1.1430563926696777 + 1.0 * 6.446107387542725
Epoch 260, val loss: 1.2475385665893555
Epoch 270, training loss: 7.532257556915283 = 1.0908454656600952 + 1.0 * 6.441411972045898
Epoch 270, val loss: 1.203843116760254
Epoch 280, training loss: 7.47572135925293 = 1.0393553972244263 + 1.0 * 6.436366081237793
Epoch 280, val loss: 1.1606957912445068
Epoch 290, training loss: 7.418046951293945 = 0.9887647032737732 + 1.0 * 6.429282188415527
Epoch 290, val loss: 1.1182295083999634
Epoch 300, training loss: 7.364382743835449 = 0.9388514757156372 + 1.0 * 6.425531387329102
Epoch 300, val loss: 1.0762132406234741
Epoch 310, training loss: 7.313878536224365 = 0.890223503112793 + 1.0 * 6.423655033111572
Epoch 310, val loss: 1.0353626012802124
Epoch 320, training loss: 7.261960506439209 = 0.8434988260269165 + 1.0 * 6.418461799621582
Epoch 320, val loss: 0.9965657591819763
Epoch 330, training loss: 7.211322784423828 = 0.7989048361778259 + 1.0 * 6.412417888641357
Epoch 330, val loss: 0.9600726366043091
Epoch 340, training loss: 7.165483474731445 = 0.7566031217575073 + 1.0 * 6.408880233764648
Epoch 340, val loss: 0.925866961479187
Epoch 350, training loss: 7.121147155761719 = 0.7167219519615173 + 1.0 * 6.404425144195557
Epoch 350, val loss: 0.8944211006164551
Epoch 360, training loss: 7.080051422119141 = 0.6794393658638 + 1.0 * 6.400611877441406
Epoch 360, val loss: 0.865690290927887
Epoch 370, training loss: 7.042736053466797 = 0.6446435451507568 + 1.0 * 6.398092746734619
Epoch 370, val loss: 0.8398460745811462
Epoch 380, training loss: 7.004650115966797 = 0.6119741797447205 + 1.0 * 6.392675876617432
Epoch 380, val loss: 0.8163172602653503
Epoch 390, training loss: 6.975534915924072 = 0.5810932517051697 + 1.0 * 6.394441604614258
Epoch 390, val loss: 0.7947894334793091
Epoch 400, training loss: 6.937833309173584 = 0.5519279837608337 + 1.0 * 6.3859052658081055
Epoch 400, val loss: 0.7752708196640015
Epoch 410, training loss: 6.91630220413208 = 0.5242950320243835 + 1.0 * 6.392007350921631
Epoch 410, val loss: 0.7573853731155396
Epoch 420, training loss: 6.880361080169678 = 0.4981832802295685 + 1.0 * 6.382177829742432
Epoch 420, val loss: 0.7410315871238708
Epoch 430, training loss: 6.8506011962890625 = 0.47330209612846375 + 1.0 * 6.3772993087768555
Epoch 430, val loss: 0.7261345982551575
Epoch 440, training loss: 6.826396465301514 = 0.44931212067604065 + 1.0 * 6.377084255218506
Epoch 440, val loss: 0.7123419642448425
Epoch 450, training loss: 6.80135440826416 = 0.4261610805988312 + 1.0 * 6.375193119049072
Epoch 450, val loss: 0.6994673013687134
Epoch 460, training loss: 6.77451753616333 = 0.4036705791950226 + 1.0 * 6.370846748352051
Epoch 460, val loss: 0.6875810623168945
Epoch 470, training loss: 6.750574588775635 = 0.3816574215888977 + 1.0 * 6.368916988372803
Epoch 470, val loss: 0.6764706373214722
Epoch 480, training loss: 6.726265907287598 = 0.36002105474472046 + 1.0 * 6.366244792938232
Epoch 480, val loss: 0.666019082069397
Epoch 490, training loss: 6.703808784484863 = 0.33876651525497437 + 1.0 * 6.365042209625244
Epoch 490, val loss: 0.6563112735748291
Epoch 500, training loss: 6.679121971130371 = 0.3179030120372772 + 1.0 * 6.3612189292907715
Epoch 500, val loss: 0.6472876071929932
Epoch 510, training loss: 6.656736373901367 = 0.29735299944877625 + 1.0 * 6.359383583068848
Epoch 510, val loss: 0.638877809047699
Epoch 520, training loss: 6.639800071716309 = 0.27724748849868774 + 1.0 * 6.362552642822266
Epoch 520, val loss: 0.6311265826225281
Epoch 530, training loss: 6.612837791442871 = 0.257797509431839 + 1.0 * 6.355040073394775
Epoch 530, val loss: 0.6241785287857056
Epoch 540, training loss: 6.604226589202881 = 0.23910197615623474 + 1.0 * 6.365124702453613
Epoch 540, val loss: 0.6179964542388916
Epoch 550, training loss: 6.572713851928711 = 0.22141115367412567 + 1.0 * 6.351302623748779
Epoch 550, val loss: 0.6127182841300964
Epoch 560, training loss: 6.564034461975098 = 0.20480825006961823 + 1.0 * 6.359226226806641
Epoch 560, val loss: 0.6083201169967651
Epoch 570, training loss: 6.539400100708008 = 0.18941760063171387 + 1.0 * 6.349982738494873
Epoch 570, val loss: 0.6048517823219299
Epoch 580, training loss: 6.521528244018555 = 0.17525966465473175 + 1.0 * 6.346268653869629
Epoch 580, val loss: 0.6022970080375671
Epoch 590, training loss: 6.506397247314453 = 0.16225171089172363 + 1.0 * 6.34414529800415
Epoch 590, val loss: 0.6005938649177551
Epoch 600, training loss: 6.496343612670898 = 0.15043042600154877 + 1.0 * 6.345913410186768
Epoch 600, val loss: 0.5996822714805603
Epoch 610, training loss: 6.480472087860107 = 0.13978837430477142 + 1.0 * 6.340683937072754
Epoch 610, val loss: 0.5996436476707458
Epoch 620, training loss: 6.468617916107178 = 0.13013847172260284 + 1.0 * 6.338479518890381
Epoch 620, val loss: 0.6003637909889221
Epoch 630, training loss: 6.458520412445068 = 0.12135531008243561 + 1.0 * 6.337164878845215
Epoch 630, val loss: 0.6016820669174194
Epoch 640, training loss: 6.455939292907715 = 0.11336354911327362 + 1.0 * 6.342575550079346
Epoch 640, val loss: 0.6035772562026978
Epoch 650, training loss: 6.446021556854248 = 0.10613029450178146 + 1.0 * 6.33989143371582
Epoch 650, val loss: 0.6059477925300598
Epoch 660, training loss: 6.4347052574157715 = 0.09955988824367523 + 1.0 * 6.335145473480225
Epoch 660, val loss: 0.6086829304695129
Epoch 670, training loss: 6.424755096435547 = 0.09353812038898468 + 1.0 * 6.331216812133789
Epoch 670, val loss: 0.6119009256362915
Epoch 680, training loss: 6.418968677520752 = 0.08799145370721817 + 1.0 * 6.330977439880371
Epoch 680, val loss: 0.6154626607894897
Epoch 690, training loss: 6.413397789001465 = 0.08290151506662369 + 1.0 * 6.330496311187744
Epoch 690, val loss: 0.6192783117294312
Epoch 700, training loss: 6.404993057250977 = 0.07822742313146591 + 1.0 * 6.326765537261963
Epoch 700, val loss: 0.6233273148536682
Epoch 710, training loss: 6.399686336517334 = 0.07390406727790833 + 1.0 * 6.325782299041748
Epoch 710, val loss: 0.6276102066040039
Epoch 720, training loss: 6.404623508453369 = 0.06989173591136932 + 1.0 * 6.334731578826904
Epoch 720, val loss: 0.6321407556533813
Epoch 730, training loss: 6.391604423522949 = 0.06619333475828171 + 1.0 * 6.325411319732666
Epoch 730, val loss: 0.6366433501243591
Epoch 740, training loss: 6.394960403442383 = 0.0627475157380104 + 1.0 * 6.332212924957275
Epoch 740, val loss: 0.6413872838020325
Epoch 750, training loss: 6.382396697998047 = 0.059562452137470245 + 1.0 * 6.322834014892578
Epoch 750, val loss: 0.6462387442588806
Epoch 760, training loss: 6.376111030578613 = 0.056588832288980484 + 1.0 * 6.319522380828857
Epoch 760, val loss: 0.6512045860290527
Epoch 770, training loss: 6.380593299865723 = 0.053807374089956284 + 1.0 * 6.326786041259766
Epoch 770, val loss: 0.6562790870666504
Epoch 780, training loss: 6.3702192306518555 = 0.05122322961688042 + 1.0 * 6.318995952606201
Epoch 780, val loss: 0.6613168120384216
Epoch 790, training loss: 6.363991737365723 = 0.04880004748702049 + 1.0 * 6.315191745758057
Epoch 790, val loss: 0.6665040254592896
Epoch 800, training loss: 6.362003803253174 = 0.046531371772289276 + 1.0 * 6.315472602844238
Epoch 800, val loss: 0.6716709733009338
Epoch 810, training loss: 6.358229160308838 = 0.04440655559301376 + 1.0 * 6.3138227462768555
Epoch 810, val loss: 0.6768608093261719
Epoch 820, training loss: 6.357416152954102 = 0.042425189167261124 + 1.0 * 6.314990997314453
Epoch 820, val loss: 0.6820344924926758
Epoch 830, training loss: 6.351306915283203 = 0.04055977985262871 + 1.0 * 6.310747146606445
Epoch 830, val loss: 0.6872771382331848
Epoch 840, training loss: 6.351039886474609 = 0.0388060100376606 + 1.0 * 6.312233924865723
Epoch 840, val loss: 0.6925239562988281
Epoch 850, training loss: 6.349674224853516 = 0.037158261984586716 + 1.0 * 6.312515735626221
Epoch 850, val loss: 0.6976449489593506
Epoch 860, training loss: 6.348245143890381 = 0.035619087517261505 + 1.0 * 6.312625885009766
Epoch 860, val loss: 0.7028197050094604
Epoch 870, training loss: 6.342968940734863 = 0.03417518362402916 + 1.0 * 6.308793544769287
Epoch 870, val loss: 0.7079066038131714
Epoch 880, training loss: 6.339444160461426 = 0.03280945494771004 + 1.0 * 6.306634902954102
Epoch 880, val loss: 0.7130510807037354
Epoch 890, training loss: 6.337864398956299 = 0.031516753137111664 + 1.0 * 6.306347846984863
Epoch 890, val loss: 0.7181304097175598
Epoch 900, training loss: 6.3382134437561035 = 0.030297672376036644 + 1.0 * 6.307915687561035
Epoch 900, val loss: 0.7231824398040771
Epoch 910, training loss: 6.335244655609131 = 0.02915160357952118 + 1.0 * 6.306093215942383
Epoch 910, val loss: 0.7281246781349182
Epoch 920, training loss: 6.332890510559082 = 0.028070852160453796 + 1.0 * 6.304819583892822
Epoch 920, val loss: 0.7331312894821167
Epoch 930, training loss: 6.333038806915283 = 0.027049051597714424 + 1.0 * 6.305989742279053
Epoch 930, val loss: 0.7380119562149048
Epoch 940, training loss: 6.329559803009033 = 0.026082398369908333 + 1.0 * 6.3034772872924805
Epoch 940, val loss: 0.7427908182144165
Epoch 950, training loss: 6.3258562088012695 = 0.02516614831984043 + 1.0 * 6.300690174102783
Epoch 950, val loss: 0.7476009726524353
Epoch 960, training loss: 6.327467918395996 = 0.02429652214050293 + 1.0 * 6.303171157836914
Epoch 960, val loss: 0.7523375749588013
Epoch 970, training loss: 6.329612731933594 = 0.02347145415842533 + 1.0 * 6.306141376495361
Epoch 970, val loss: 0.7570122480392456
Epoch 980, training loss: 6.323827743530273 = 0.022693049162626266 + 1.0 * 6.3011345863342285
Epoch 980, val loss: 0.7615828514099121
Epoch 990, training loss: 6.319892406463623 = 0.02195444330573082 + 1.0 * 6.297937870025635
Epoch 990, val loss: 0.7661269903182983
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8444444444444444
0.8350026357406432
The final CL Acc:0.82099, 0.02145, The final GNN Acc:0.83781, 0.00199
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10576])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.54140853881836 = 1.9445644617080688 + 1.0 * 8.596843719482422
Epoch 0, val loss: 1.9398213624954224
Epoch 10, training loss: 10.531538009643555 = 1.9349168539047241 + 1.0 * 8.5966215133667
Epoch 10, val loss: 1.9296174049377441
Epoch 20, training loss: 10.517960548400879 = 1.923121690750122 + 1.0 * 8.594839096069336
Epoch 20, val loss: 1.9170403480529785
Epoch 30, training loss: 10.486122131347656 = 1.906801700592041 + 1.0 * 8.579320907592773
Epoch 30, val loss: 1.8995587825775146
Epoch 40, training loss: 10.34941577911377 = 1.8852307796478271 + 1.0 * 8.464184761047363
Epoch 40, val loss: 1.8774324655532837
Epoch 50, training loss: 9.864296913146973 = 1.8643027544021606 + 1.0 * 7.999994277954102
Epoch 50, val loss: 1.8576483726501465
Epoch 60, training loss: 9.295605659484863 = 1.8489201068878174 + 1.0 * 7.446685791015625
Epoch 60, val loss: 1.843363642692566
Epoch 70, training loss: 8.857266426086426 = 1.8365522623062134 + 1.0 * 7.020713806152344
Epoch 70, val loss: 1.8316930532455444
Epoch 80, training loss: 8.694210052490234 = 1.8249849081039429 + 1.0 * 6.86922550201416
Epoch 80, val loss: 1.8204885721206665
Epoch 90, training loss: 8.599984169006348 = 1.8113932609558105 + 1.0 * 6.788590908050537
Epoch 90, val loss: 1.8077850341796875
Epoch 100, training loss: 8.517782211303711 = 1.7978107929229736 + 1.0 * 6.719971656799316
Epoch 100, val loss: 1.7955266237258911
Epoch 110, training loss: 8.457939147949219 = 1.7853864431381226 + 1.0 * 6.672553062438965
Epoch 110, val loss: 1.7844209671020508
Epoch 120, training loss: 8.408374786376953 = 1.7732590436935425 + 1.0 * 6.635116100311279
Epoch 120, val loss: 1.7736951112747192
Epoch 130, training loss: 8.364701271057129 = 1.760460376739502 + 1.0 * 6.604240894317627
Epoch 130, val loss: 1.762520432472229
Epoch 140, training loss: 8.324562072753906 = 1.746254324913025 + 1.0 * 6.57830810546875
Epoch 140, val loss: 1.7503013610839844
Epoch 150, training loss: 8.286930084228516 = 1.7303367853164673 + 1.0 * 6.556593418121338
Epoch 150, val loss: 1.7367099523544312
Epoch 160, training loss: 8.249521255493164 = 1.712209701538086 + 1.0 * 6.53731107711792
Epoch 160, val loss: 1.7213246822357178
Epoch 170, training loss: 8.211664199829102 = 1.6912519931793213 + 1.0 * 6.520411968231201
Epoch 170, val loss: 1.7036104202270508
Epoch 180, training loss: 8.174137115478516 = 1.6668018102645874 + 1.0 * 6.507335662841797
Epoch 180, val loss: 1.6830564737319946
Epoch 190, training loss: 8.133136749267578 = 1.6384999752044678 + 1.0 * 6.494636535644531
Epoch 190, val loss: 1.659386396408081
Epoch 200, training loss: 8.089014053344727 = 1.6056838035583496 + 1.0 * 6.483329772949219
Epoch 200, val loss: 1.6319905519485474
Epoch 210, training loss: 8.044160842895508 = 1.5678215026855469 + 1.0 * 6.476339340209961
Epoch 210, val loss: 1.600506067276001
Epoch 220, training loss: 7.9918622970581055 = 1.5252759456634521 + 1.0 * 6.466586112976074
Epoch 220, val loss: 1.5656510591506958
Epoch 230, training loss: 7.935868263244629 = 1.4781677722930908 + 1.0 * 6.457700252532959
Epoch 230, val loss: 1.5275810956954956
Epoch 240, training loss: 7.8792266845703125 = 1.4268670082092285 + 1.0 * 6.452359676361084
Epoch 240, val loss: 1.4865188598632812
Epoch 250, training loss: 7.819933891296387 = 1.373154878616333 + 1.0 * 6.446779251098633
Epoch 250, val loss: 1.444225549697876
Epoch 260, training loss: 7.75674295425415 = 1.3176345825195312 + 1.0 * 6.439108371734619
Epoch 260, val loss: 1.4013382196426392
Epoch 270, training loss: 7.704178810119629 = 1.2611916065216064 + 1.0 * 6.442986965179443
Epoch 270, val loss: 1.3584260940551758
Epoch 280, training loss: 7.636314392089844 = 1.205642819404602 + 1.0 * 6.430671691894531
Epoch 280, val loss: 1.3168407678604126
Epoch 290, training loss: 7.575222969055176 = 1.1507093906402588 + 1.0 * 6.424513339996338
Epoch 290, val loss: 1.2763535976409912
Epoch 300, training loss: 7.516331672668457 = 1.0964038372039795 + 1.0 * 6.419927597045898
Epoch 300, val loss: 1.2366222143173218
Epoch 310, training loss: 7.477308750152588 = 1.0432754755020142 + 1.0 * 6.434033393859863
Epoch 310, val loss: 1.1980255842208862
Epoch 320, training loss: 7.408090591430664 = 0.9932176470756531 + 1.0 * 6.414873123168945
Epoch 320, val loss: 1.1620391607284546
Epoch 330, training loss: 7.354375839233398 = 0.9458444118499756 + 1.0 * 6.408531188964844
Epoch 330, val loss: 1.1282577514648438
Epoch 340, training loss: 7.305706977844238 = 0.9010206460952759 + 1.0 * 6.404686450958252
Epoch 340, val loss: 1.0968153476715088
Epoch 350, training loss: 7.262120723724365 = 0.8592334389686584 + 1.0 * 6.402887344360352
Epoch 350, val loss: 1.0682318210601807
Epoch 360, training loss: 7.221002578735352 = 0.8212137818336487 + 1.0 * 6.399788856506348
Epoch 360, val loss: 1.0429126024246216
Epoch 370, training loss: 7.18176794052124 = 0.7865042090415955 + 1.0 * 6.395263671875
Epoch 370, val loss: 1.0206968784332275
Epoch 380, training loss: 7.146448135375977 = 0.7545156478881836 + 1.0 * 6.391932487487793
Epoch 380, val loss: 1.0011571645736694
Epoch 390, training loss: 7.113815784454346 = 0.7249172925949097 + 1.0 * 6.3888983726501465
Epoch 390, val loss: 0.984084963798523
Epoch 400, training loss: 7.08924674987793 = 0.697303295135498 + 1.0 * 6.391943454742432
Epoch 400, val loss: 0.9691309928894043
Epoch 410, training loss: 7.056784629821777 = 0.6717193126678467 + 1.0 * 6.38506555557251
Epoch 410, val loss: 0.9560055136680603
Epoch 420, training loss: 7.027421474456787 = 0.6474879384040833 + 1.0 * 6.3799333572387695
Epoch 420, val loss: 0.9443031549453735
Epoch 430, training loss: 7.000515937805176 = 0.6240996718406677 + 1.0 * 6.376416206359863
Epoch 430, val loss: 0.9335054159164429
Epoch 440, training loss: 6.982650279998779 = 0.6012685894966125 + 1.0 * 6.381381511688232
Epoch 440, val loss: 0.923481822013855
Epoch 450, training loss: 6.9516682624816895 = 0.5791935324668884 + 1.0 * 6.372474670410156
Epoch 450, val loss: 0.9140855073928833
Epoch 460, training loss: 6.927656173706055 = 0.5576545000076294 + 1.0 * 6.370001792907715
Epoch 460, val loss: 0.9052663445472717
Epoch 470, training loss: 6.903903484344482 = 0.5364271402359009 + 1.0 * 6.367476463317871
Epoch 470, val loss: 0.896862268447876
Epoch 480, training loss: 6.897037506103516 = 0.5155060291290283 + 1.0 * 6.381531238555908
Epoch 480, val loss: 0.8888856172561646
Epoch 490, training loss: 6.863000392913818 = 0.4951321482658386 + 1.0 * 6.367868423461914
Epoch 490, val loss: 0.881403923034668
Epoch 500, training loss: 6.837738990783691 = 0.47517940402030945 + 1.0 * 6.362559795379639
Epoch 500, val loss: 0.8743123412132263
Epoch 510, training loss: 6.814299583435059 = 0.4554692804813385 + 1.0 * 6.358830451965332
Epoch 510, val loss: 0.8675668835639954
Epoch 520, training loss: 6.798317909240723 = 0.4359271228313446 + 1.0 * 6.362390995025635
Epoch 520, val loss: 0.8610830307006836
Epoch 530, training loss: 6.776523590087891 = 0.41668233275413513 + 1.0 * 6.359841346740723
Epoch 530, val loss: 0.8548702001571655
Epoch 540, training loss: 6.753158092498779 = 0.3976432681083679 + 1.0 * 6.355515003204346
Epoch 540, val loss: 0.8488768935203552
Epoch 550, training loss: 6.740739345550537 = 0.37871190905570984 + 1.0 * 6.362027645111084
Epoch 550, val loss: 0.8430005311965942
Epoch 560, training loss: 6.714884281158447 = 0.35992199182510376 + 1.0 * 6.354962348937988
Epoch 560, val loss: 0.8372617959976196
Epoch 570, training loss: 6.692042350769043 = 0.34130796790122986 + 1.0 * 6.350734233856201
Epoch 570, val loss: 0.831642210483551
Epoch 580, training loss: 6.673330307006836 = 0.32289186120033264 + 1.0 * 6.350438594818115
Epoch 580, val loss: 0.8263041973114014
Epoch 590, training loss: 6.652951240539551 = 0.3047853410243988 + 1.0 * 6.348165988922119
Epoch 590, val loss: 0.8212077021598816
Epoch 600, training loss: 6.632906913757324 = 0.2870008051395416 + 1.0 * 6.3459062576293945
Epoch 600, val loss: 0.8164965510368347
Epoch 610, training loss: 6.624396800994873 = 0.2696690857410431 + 1.0 * 6.354727745056152
Epoch 610, val loss: 0.8121791481971741
Epoch 620, training loss: 6.597168445587158 = 0.25304654240608215 + 1.0 * 6.344121932983398
Epoch 620, val loss: 0.8083545565605164
Epoch 630, training loss: 6.579411506652832 = 0.23707403242588043 + 1.0 * 6.342337608337402
Epoch 630, val loss: 0.8050989508628845
Epoch 640, training loss: 6.563453674316406 = 0.22175201773643494 + 1.0 * 6.341701507568359
Epoch 640, val loss: 0.8024236559867859
Epoch 650, training loss: 6.547117233276367 = 0.20725655555725098 + 1.0 * 6.339860439300537
Epoch 650, val loss: 0.8002777695655823
Epoch 660, training loss: 6.535222053527832 = 0.1936458796262741 + 1.0 * 6.341576099395752
Epoch 660, val loss: 0.7988320589065552
Epoch 670, training loss: 6.520174026489258 = 0.1808841973543167 + 1.0 * 6.339289665222168
Epoch 670, val loss: 0.7980029582977295
Epoch 680, training loss: 6.506577968597412 = 0.16892769932746887 + 1.0 * 6.337650299072266
Epoch 680, val loss: 0.7978147864341736
Epoch 690, training loss: 6.496893882751465 = 0.15773557126522064 + 1.0 * 6.339158535003662
Epoch 690, val loss: 0.7982222437858582
Epoch 700, training loss: 6.483752250671387 = 0.14736972749233246 + 1.0 * 6.3363823890686035
Epoch 700, val loss: 0.7990720868110657
Epoch 710, training loss: 6.472438335418701 = 0.13775406777858734 + 1.0 * 6.334684371948242
Epoch 710, val loss: 0.8005368709564209
Epoch 720, training loss: 6.465875148773193 = 0.12884554266929626 + 1.0 * 6.337029457092285
Epoch 720, val loss: 0.8023959398269653
Epoch 730, training loss: 6.4531989097595215 = 0.12060711532831192 + 1.0 * 6.332592010498047
Epoch 730, val loss: 0.8047209978103638
Epoch 740, training loss: 6.44338321685791 = 0.112993523478508 + 1.0 * 6.330389499664307
Epoch 740, val loss: 0.8074225187301636
Epoch 750, training loss: 6.439168930053711 = 0.1059737354516983 + 1.0 * 6.333195209503174
Epoch 750, val loss: 0.8104525804519653
Epoch 760, training loss: 6.430074214935303 = 0.09951725602149963 + 1.0 * 6.330556869506836
Epoch 760, val loss: 0.8137773275375366
Epoch 770, training loss: 6.419572830200195 = 0.09359175711870193 + 1.0 * 6.325981140136719
Epoch 770, val loss: 0.8174537420272827
Epoch 780, training loss: 6.41340970993042 = 0.08811868727207184 + 1.0 * 6.325291156768799
Epoch 780, val loss: 0.8213924169540405
Epoch 790, training loss: 6.409363269805908 = 0.08306007832288742 + 1.0 * 6.326303005218506
Epoch 790, val loss: 0.8255134224891663
Epoch 800, training loss: 6.403746604919434 = 0.07839222997426987 + 1.0 * 6.32535457611084
Epoch 800, val loss: 0.8298695683479309
Epoch 810, training loss: 6.400216102600098 = 0.07408900558948517 + 1.0 * 6.326127052307129
Epoch 810, val loss: 0.8342570066452026
Epoch 820, training loss: 6.391353130340576 = 0.07012477517127991 + 1.0 * 6.321228504180908
Epoch 820, val loss: 0.8388895392417908
Epoch 830, training loss: 6.393509864807129 = 0.06644225120544434 + 1.0 * 6.3270673751831055
Epoch 830, val loss: 0.8436304926872253
Epoch 840, training loss: 6.382586479187012 = 0.06303875893354416 + 1.0 * 6.319547653198242
Epoch 840, val loss: 0.8483325839042664
Epoch 850, training loss: 6.377476215362549 = 0.05988012254238129 + 1.0 * 6.317595958709717
Epoch 850, val loss: 0.8532931208610535
Epoch 860, training loss: 6.376322269439697 = 0.05694761127233505 + 1.0 * 6.3193745613098145
Epoch 860, val loss: 0.8580872416496277
Epoch 870, training loss: 6.372616291046143 = 0.054237548261880875 + 1.0 * 6.318378925323486
Epoch 870, val loss: 0.8630353212356567
Epoch 880, training loss: 6.36583948135376 = 0.05170593783259392 + 1.0 * 6.314133644104004
Epoch 880, val loss: 0.8680798411369324
Epoch 890, training loss: 6.361772060394287 = 0.04933275282382965 + 1.0 * 6.312439441680908
Epoch 890, val loss: 0.8731018900871277
Epoch 900, training loss: 6.35858154296875 = 0.04710545018315315 + 1.0 * 6.311476230621338
Epoch 900, val loss: 0.8781756162643433
Epoch 910, training loss: 6.368986129760742 = 0.045030124485492706 + 1.0 * 6.32395601272583
Epoch 910, val loss: 0.8830981850624084
Epoch 920, training loss: 6.360996246337891 = 0.04310297966003418 + 1.0 * 6.3178935050964355
Epoch 920, val loss: 0.8880486488342285
Epoch 930, training loss: 6.352899551391602 = 0.04129766672849655 + 1.0 * 6.3116021156311035
Epoch 930, val loss: 0.8931718468666077
Epoch 940, training loss: 6.349794864654541 = 0.039595723152160645 + 1.0 * 6.31019926071167
Epoch 940, val loss: 0.8981268405914307
Epoch 950, training loss: 6.347405433654785 = 0.03799661248922348 + 1.0 * 6.309408664703369
Epoch 950, val loss: 0.9029877185821533
Epoch 960, training loss: 6.342565536499023 = 0.03649623319506645 + 1.0 * 6.306069374084473
Epoch 960, val loss: 0.9079737663269043
Epoch 970, training loss: 6.340663433074951 = 0.03507909178733826 + 1.0 * 6.30558443069458
Epoch 970, val loss: 0.912969708442688
Epoch 980, training loss: 6.345029830932617 = 0.03373837098479271 + 1.0 * 6.311291694641113
Epoch 980, val loss: 0.9178500771522522
Epoch 990, training loss: 6.340534687042236 = 0.03247915580868721 + 1.0 * 6.308055400848389
Epoch 990, val loss: 0.9226449728012085
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 10.533865928649902 = 1.9370187520980835 + 1.0 * 8.596847534179688
Epoch 0, val loss: 1.9381194114685059
Epoch 10, training loss: 10.5239839553833 = 1.927369475364685 + 1.0 * 8.596614837646484
Epoch 10, val loss: 1.9284007549285889
Epoch 20, training loss: 10.510469436645508 = 1.915532112121582 + 1.0 * 8.594937324523926
Epoch 20, val loss: 1.9163087606430054
Epoch 30, training loss: 10.480183601379395 = 1.8992141485214233 + 1.0 * 8.58096981048584
Epoch 30, val loss: 1.899628758430481
Epoch 40, training loss: 10.362187385559082 = 1.8775304555892944 + 1.0 * 8.484657287597656
Epoch 40, val loss: 1.8783537149429321
Epoch 50, training loss: 9.857633590698242 = 1.8554645776748657 + 1.0 * 8.002168655395508
Epoch 50, val loss: 1.8579294681549072
Epoch 60, training loss: 9.280251502990723 = 1.8398633003234863 + 1.0 * 7.440388202667236
Epoch 60, val loss: 1.843403697013855
Epoch 70, training loss: 8.86859130859375 = 1.8297111988067627 + 1.0 * 7.038879871368408
Epoch 70, val loss: 1.8332178592681885
Epoch 80, training loss: 8.700719833374023 = 1.8197848796844482 + 1.0 * 6.880935192108154
Epoch 80, val loss: 1.823527455329895
Epoch 90, training loss: 8.574771881103516 = 1.807841420173645 + 1.0 * 6.766930103302002
Epoch 90, val loss: 1.8122484683990479
Epoch 100, training loss: 8.489107131958008 = 1.7959192991256714 + 1.0 * 6.693188190460205
Epoch 100, val loss: 1.8010627031326294
Epoch 110, training loss: 8.42982006072998 = 1.7848464250564575 + 1.0 * 6.644973278045654
Epoch 110, val loss: 1.7904143333435059
Epoch 120, training loss: 8.380341529846191 = 1.7737007141113281 + 1.0 * 6.606640815734863
Epoch 120, val loss: 1.7798062562942505
Epoch 130, training loss: 8.343318939208984 = 1.7616493701934814 + 1.0 * 6.581669807434082
Epoch 130, val loss: 1.7688636779785156
Epoch 140, training loss: 8.302937507629395 = 1.7482424974441528 + 1.0 * 6.554694652557373
Epoch 140, val loss: 1.7573540210723877
Epoch 150, training loss: 8.26774787902832 = 1.732900619506836 + 1.0 * 6.534846782684326
Epoch 150, val loss: 1.744553804397583
Epoch 160, training loss: 8.233357429504395 = 1.7147847414016724 + 1.0 * 6.518572807312012
Epoch 160, val loss: 1.7297260761260986
Epoch 170, training loss: 8.201864242553711 = 1.6931674480438232 + 1.0 * 6.508696556091309
Epoch 170, val loss: 1.7121480703353882
Epoch 180, training loss: 8.159882545471191 = 1.6680561304092407 + 1.0 * 6.49182653427124
Epoch 180, val loss: 1.691762924194336
Epoch 190, training loss: 8.119109153747559 = 1.638543963432312 + 1.0 * 6.480565071105957
Epoch 190, val loss: 1.6678285598754883
Epoch 200, training loss: 8.073949813842773 = 1.604013442993164 + 1.0 * 6.469936847686768
Epoch 200, val loss: 1.6399832963943481
Epoch 210, training loss: 8.031129837036133 = 1.5640943050384521 + 1.0 * 6.467035293579102
Epoch 210, val loss: 1.6079983711242676
Epoch 220, training loss: 7.974868297576904 = 1.5205389261245728 + 1.0 * 6.454329490661621
Epoch 220, val loss: 1.5736058950424194
Epoch 230, training loss: 7.919946670532227 = 1.473702073097229 + 1.0 * 6.446244716644287
Epoch 230, val loss: 1.5370582342147827
Epoch 240, training loss: 7.863544464111328 = 1.4243671894073486 + 1.0 * 6.439177513122559
Epoch 240, val loss: 1.4989396333694458
Epoch 250, training loss: 7.807893753051758 = 1.3738481998443604 + 1.0 * 6.434045314788818
Epoch 250, val loss: 1.460426688194275
Epoch 260, training loss: 7.751522064208984 = 1.3238465785980225 + 1.0 * 6.427675724029541
Epoch 260, val loss: 1.422990322113037
Epoch 270, training loss: 7.696855068206787 = 1.2746467590332031 + 1.0 * 6.422208309173584
Epoch 270, val loss: 1.386678695678711
Epoch 280, training loss: 7.650215148925781 = 1.226387858390808 + 1.0 * 6.423827171325684
Epoch 280, val loss: 1.3515021800994873
Epoch 290, training loss: 7.598447799682617 = 1.1803662776947021 + 1.0 * 6.418081760406494
Epoch 290, val loss: 1.318385124206543
Epoch 300, training loss: 7.5452704429626465 = 1.1357909440994263 + 1.0 * 6.40947961807251
Epoch 300, val loss: 1.2866510152816772
Epoch 310, training loss: 7.497241973876953 = 1.091965913772583 + 1.0 * 6.405276298522949
Epoch 310, val loss: 1.255517601966858
Epoch 320, training loss: 7.450474739074707 = 1.0489000082015991 + 1.0 * 6.401574611663818
Epoch 320, val loss: 1.2249892950057983
Epoch 330, training loss: 7.404649257659912 = 1.006506085395813 + 1.0 * 6.398143291473389
Epoch 330, val loss: 1.195037841796875
Epoch 340, training loss: 7.365674018859863 = 0.9645833373069763 + 1.0 * 6.401090621948242
Epoch 340, val loss: 1.1654993295669556
Epoch 350, training loss: 7.31540060043335 = 0.9236749410629272 + 1.0 * 6.391725540161133
Epoch 350, val loss: 1.1367651224136353
Epoch 360, training loss: 7.271399974822998 = 0.8835563659667969 + 1.0 * 6.387843608856201
Epoch 360, val loss: 1.108718752861023
Epoch 370, training loss: 7.23032283782959 = 0.8440214395523071 + 1.0 * 6.386301517486572
Epoch 370, val loss: 1.0813871622085571
Epoch 380, training loss: 7.200717926025391 = 0.8055981397628784 + 1.0 * 6.395119667053223
Epoch 380, val loss: 1.0552135705947876
Epoch 390, training loss: 7.1505327224731445 = 0.7691482305526733 + 1.0 * 6.381384372711182
Epoch 390, val loss: 1.0306509733200073
Epoch 400, training loss: 7.110678195953369 = 0.7339184880256653 + 1.0 * 6.3767595291137695
Epoch 400, val loss: 1.007598876953125
Epoch 410, training loss: 7.074265480041504 = 0.6999577283859253 + 1.0 * 6.374307632446289
Epoch 410, val loss: 0.9862117171287537
Epoch 420, training loss: 7.044711589813232 = 0.6674743294715881 + 1.0 * 6.377237319946289
Epoch 420, val loss: 0.9667264819145203
Epoch 430, training loss: 7.009344577789307 = 0.6367320418357849 + 1.0 * 6.372612476348877
Epoch 430, val loss: 0.9494234323501587
Epoch 440, training loss: 6.97658634185791 = 0.6073465347290039 + 1.0 * 6.369239807128906
Epoch 440, val loss: 0.9339940547943115
Epoch 450, training loss: 6.949998378753662 = 0.5792180895805359 + 1.0 * 6.3707804679870605
Epoch 450, val loss: 0.9202362895011902
Epoch 460, training loss: 6.9190545082092285 = 0.5523650050163269 + 1.0 * 6.366689682006836
Epoch 460, val loss: 0.9082603454589844
Epoch 470, training loss: 6.8872389793396 = 0.5265278816223145 + 1.0 * 6.360711097717285
Epoch 470, val loss: 0.8975857496261597
Epoch 480, training loss: 6.861239433288574 = 0.5014995336532593 + 1.0 * 6.359739780426025
Epoch 480, val loss: 0.8881357908248901
Epoch 490, training loss: 6.8386454582214355 = 0.47735482454299927 + 1.0 * 6.361290454864502
Epoch 490, val loss: 0.8799506425857544
Epoch 500, training loss: 6.811920166015625 = 0.45432204008102417 + 1.0 * 6.357598304748535
Epoch 500, val loss: 0.8726518154144287
Epoch 510, training loss: 6.785522937774658 = 0.43232354521751404 + 1.0 * 6.353199481964111
Epoch 510, val loss: 0.8664576411247253
Epoch 520, training loss: 6.761932849884033 = 0.411225289106369 + 1.0 * 6.350707530975342
Epoch 520, val loss: 0.8611615300178528
Epoch 530, training loss: 6.740421772003174 = 0.3909893333911896 + 1.0 * 6.349432468414307
Epoch 530, val loss: 0.8566385507583618
Epoch 540, training loss: 6.728923797607422 = 0.3717502951622009 + 1.0 * 6.357173442840576
Epoch 540, val loss: 0.8529059290885925
Epoch 550, training loss: 6.706615447998047 = 0.35373902320861816 + 1.0 * 6.352876663208008
Epoch 550, val loss: 0.8500500917434692
Epoch 560, training loss: 6.684407711029053 = 0.33692118525505066 + 1.0 * 6.34748649597168
Epoch 560, val loss: 0.8479376435279846
Epoch 570, training loss: 6.663970947265625 = 0.3210623860359192 + 1.0 * 6.3429083824157715
Epoch 570, val loss: 0.8465362191200256
Epoch 580, training loss: 6.649712562561035 = 0.30608466267585754 + 1.0 * 6.3436279296875
Epoch 580, val loss: 0.8457316756248474
Epoch 590, training loss: 6.6352219581604 = 0.29199540615081787 + 1.0 * 6.343226432800293
Epoch 590, val loss: 0.8454170227050781
Epoch 600, training loss: 6.618917465209961 = 0.27878499031066895 + 1.0 * 6.340132236480713
Epoch 600, val loss: 0.8456739187240601
Epoch 610, training loss: 6.607930660247803 = 0.266299307346344 + 1.0 * 6.3416314125061035
Epoch 610, val loss: 0.8464011549949646
Epoch 620, training loss: 6.590574264526367 = 0.2544843554496765 + 1.0 * 6.336090087890625
Epoch 620, val loss: 0.847411036491394
Epoch 630, training loss: 6.579037666320801 = 0.24320408701896667 + 1.0 * 6.335833549499512
Epoch 630, val loss: 0.8487699627876282
Epoch 640, training loss: 6.566873073577881 = 0.23238249123096466 + 1.0 * 6.334490776062012
Epoch 640, val loss: 0.850453794002533
Epoch 650, training loss: 6.5552520751953125 = 0.22198760509490967 + 1.0 * 6.333264350891113
Epoch 650, val loss: 0.8524503111839294
Epoch 660, training loss: 6.545551776885986 = 0.21197548508644104 + 1.0 * 6.333576202392578
Epoch 660, val loss: 0.8544699549674988
Epoch 670, training loss: 6.5325398445129395 = 0.2022634893655777 + 1.0 * 6.3302764892578125
Epoch 670, val loss: 0.8566151857376099
Epoch 680, training loss: 6.523619651794434 = 0.19276413321495056 + 1.0 * 6.330855369567871
Epoch 680, val loss: 0.8590365052223206
Epoch 690, training loss: 6.511669635772705 = 0.18346968293190002 + 1.0 * 6.328199863433838
Epoch 690, val loss: 0.8616816401481628
Epoch 700, training loss: 6.507187843322754 = 0.17438416182994843 + 1.0 * 6.332803726196289
Epoch 700, val loss: 0.864411473274231
Epoch 710, training loss: 6.494636535644531 = 0.16554972529411316 + 1.0 * 6.329086780548096
Epoch 710, val loss: 0.8675360083580017
Epoch 720, training loss: 6.48156213760376 = 0.1569584459066391 + 1.0 * 6.32460355758667
Epoch 720, val loss: 0.870742678642273
Epoch 730, training loss: 6.471795558929443 = 0.14862088859081268 + 1.0 * 6.323174476623535
Epoch 730, val loss: 0.8742772340774536
Epoch 740, training loss: 6.4658379554748535 = 0.14060042798519135 + 1.0 * 6.32523775100708
Epoch 740, val loss: 0.8780264854431152
Epoch 750, training loss: 6.456517696380615 = 0.1329670250415802 + 1.0 * 6.323550701141357
Epoch 750, val loss: 0.8819618821144104
Epoch 760, training loss: 6.444643020629883 = 0.12570902705192566 + 1.0 * 6.318933963775635
Epoch 760, val loss: 0.8861751556396484
Epoch 770, training loss: 6.437527179718018 = 0.11881612986326218 + 1.0 * 6.318711280822754
Epoch 770, val loss: 0.8907762765884399
Epoch 780, training loss: 6.439120292663574 = 0.11229098588228226 + 1.0 * 6.326829433441162
Epoch 780, val loss: 0.8956178426742554
Epoch 790, training loss: 6.43398380279541 = 0.1061849519610405 + 1.0 * 6.327798843383789
Epoch 790, val loss: 0.9005773663520813
Epoch 800, training loss: 6.418282508850098 = 0.10047877579927444 + 1.0 * 6.317803859710693
Epoch 800, val loss: 0.9057409763336182
Epoch 810, training loss: 6.410637855529785 = 0.09513382613658905 + 1.0 * 6.31550407409668
Epoch 810, val loss: 0.9110586047172546
Epoch 820, training loss: 6.415018081665039 = 0.09012923389673233 + 1.0 * 6.324888706207275
Epoch 820, val loss: 0.9165998697280884
Epoch 830, training loss: 6.4028639793396 = 0.08546675741672516 + 1.0 * 6.317397117614746
Epoch 830, val loss: 0.9220839142799377
Epoch 840, training loss: 6.394134044647217 = 0.08110073208808899 + 1.0 * 6.313033103942871
Epoch 840, val loss: 0.9277702569961548
Epoch 850, training loss: 6.388157367706299 = 0.07700670510530472 + 1.0 * 6.311150550842285
Epoch 850, val loss: 0.9335666298866272
Epoch 860, training loss: 6.3933610916137695 = 0.07317134737968445 + 1.0 * 6.320189952850342
Epoch 860, val loss: 0.9394633173942566
Epoch 870, training loss: 6.38551664352417 = 0.06959690153598785 + 1.0 * 6.315919876098633
Epoch 870, val loss: 0.9452989101409912
Epoch 880, training loss: 6.3794636726379395 = 0.06626418977975845 + 1.0 * 6.313199520111084
Epoch 880, val loss: 0.9510911703109741
Epoch 890, training loss: 6.373033046722412 = 0.06314997375011444 + 1.0 * 6.309883117675781
Epoch 890, val loss: 0.9571016430854797
Epoch 900, training loss: 6.367406845092773 = 0.06023993715643883 + 1.0 * 6.307167053222656
Epoch 900, val loss: 0.962984561920166
Epoch 910, training loss: 6.364627838134766 = 0.057509664446115494 + 1.0 * 6.307117938995361
Epoch 910, val loss: 0.9688827991485596
Epoch 920, training loss: 6.362271785736084 = 0.05493902042508125 + 1.0 * 6.307332992553711
Epoch 920, val loss: 0.9749115109443665
Epoch 930, training loss: 6.36078405380249 = 0.052524812519550323 + 1.0 * 6.308259010314941
Epoch 930, val loss: 0.9808431267738342
Epoch 940, training loss: 6.3565216064453125 = 0.0502748042345047 + 1.0 * 6.306246757507324
Epoch 940, val loss: 0.9866753220558167
Epoch 950, training loss: 6.352221965789795 = 0.048159871250391006 + 1.0 * 6.3040618896484375
Epoch 950, val loss: 0.9924702048301697
Epoch 960, training loss: 6.357223987579346 = 0.04616475850343704 + 1.0 * 6.31105899810791
Epoch 960, val loss: 0.9982593655586243
Epoch 970, training loss: 6.350693225860596 = 0.044287629425525665 + 1.0 * 6.306405544281006
Epoch 970, val loss: 1.004206657409668
Epoch 980, training loss: 6.347503662109375 = 0.04252643883228302 + 1.0 * 6.3049774169921875
Epoch 980, val loss: 1.0097631216049194
Epoch 990, training loss: 6.343730449676514 = 0.04086387902498245 + 1.0 * 6.302866458892822
Epoch 990, val loss: 1.0154740810394287
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 10.558977127075195 = 1.9621471166610718 + 1.0 * 8.596830368041992
Epoch 0, val loss: 1.9605462551116943
Epoch 10, training loss: 10.547721862792969 = 1.9511687755584717 + 1.0 * 8.596552848815918
Epoch 10, val loss: 1.9499287605285645
Epoch 20, training loss: 10.532257080078125 = 1.9377628564834595 + 1.0 * 8.594493865966797
Epoch 20, val loss: 1.936369776725769
Epoch 30, training loss: 10.499130249023438 = 1.9192579984664917 + 1.0 * 8.579872131347656
Epoch 30, val loss: 1.9171481132507324
Epoch 40, training loss: 10.400373458862305 = 1.8950549364089966 + 1.0 * 8.505318641662598
Epoch 40, val loss: 1.8930696249008179
Epoch 50, training loss: 10.057046890258789 = 1.869957447052002 + 1.0 * 8.187089920043945
Epoch 50, val loss: 1.8691669702529907
Epoch 60, training loss: 9.744270324707031 = 1.848265290260315 + 1.0 * 7.896005153656006
Epoch 60, val loss: 1.850060224533081
Epoch 70, training loss: 9.247577667236328 = 1.8315274715423584 + 1.0 * 7.416049957275391
Epoch 70, val loss: 1.8355926275253296
Epoch 80, training loss: 8.85958480834961 = 1.8194996118545532 + 1.0 * 7.040085315704346
Epoch 80, val loss: 1.8247828483581543
Epoch 90, training loss: 8.709946632385254 = 1.8059971332550049 + 1.0 * 6.90394926071167
Epoch 90, val loss: 1.8122069835662842
Epoch 100, training loss: 8.607198715209961 = 1.7903540134429932 + 1.0 * 6.816844463348389
Epoch 100, val loss: 1.7980283498764038
Epoch 110, training loss: 8.526227951049805 = 1.7760462760925293 + 1.0 * 6.750181674957275
Epoch 110, val loss: 1.7850892543792725
Epoch 120, training loss: 8.460583686828613 = 1.7632249593734741 + 1.0 * 6.69735860824585
Epoch 120, val loss: 1.7732689380645752
Epoch 130, training loss: 8.4046630859375 = 1.7499830722808838 + 1.0 * 6.654680252075195
Epoch 130, val loss: 1.7613484859466553
Epoch 140, training loss: 8.358925819396973 = 1.7352056503295898 + 1.0 * 6.623720169067383
Epoch 140, val loss: 1.7485302686691284
Epoch 150, training loss: 8.312929153442383 = 1.718642234802246 + 1.0 * 6.5942864418029785
Epoch 150, val loss: 1.7345091104507446
Epoch 160, training loss: 8.27176570892334 = 1.6998459100723267 + 1.0 * 6.5719194412231445
Epoch 160, val loss: 1.7188395261764526
Epoch 170, training loss: 8.231361389160156 = 1.6782974004745483 + 1.0 * 6.553063869476318
Epoch 170, val loss: 1.7009516954421997
Epoch 180, training loss: 8.193495750427246 = 1.6532440185546875 + 1.0 * 6.540251731872559
Epoch 180, val loss: 1.6802303791046143
Epoch 190, training loss: 8.150750160217285 = 1.6244105100631714 + 1.0 * 6.526339530944824
Epoch 190, val loss: 1.6563702821731567
Epoch 200, training loss: 8.10523796081543 = 1.591336727142334 + 1.0 * 6.513901710510254
Epoch 200, val loss: 1.628976821899414
Epoch 210, training loss: 8.057809829711914 = 1.5536893606185913 + 1.0 * 6.504120826721191
Epoch 210, val loss: 1.5978058576583862
Epoch 220, training loss: 8.009297370910645 = 1.511358380317688 + 1.0 * 6.497939109802246
Epoch 220, val loss: 1.5628172159194946
Epoch 230, training loss: 7.952771186828613 = 1.4652621746063232 + 1.0 * 6.487508773803711
Epoch 230, val loss: 1.524855375289917
Epoch 240, training loss: 7.89573335647583 = 1.4160566329956055 + 1.0 * 6.479676723480225
Epoch 240, val loss: 1.4846454858779907
Epoch 250, training loss: 7.844424724578857 = 1.3644380569458008 + 1.0 * 6.479986667633057
Epoch 250, val loss: 1.4428433179855347
Epoch 260, training loss: 7.780864715576172 = 1.3120890855789185 + 1.0 * 6.468775749206543
Epoch 260, val loss: 1.4010425806045532
Epoch 270, training loss: 7.720907688140869 = 1.259156584739685 + 1.0 * 6.4617509841918945
Epoch 270, val loss: 1.359119176864624
Epoch 280, training loss: 7.663765907287598 = 1.205741047859192 + 1.0 * 6.458024978637695
Epoch 280, val loss: 1.3170720338821411
Epoch 290, training loss: 7.6045403480529785 = 1.153204083442688 + 1.0 * 6.45133638381958
Epoch 290, val loss: 1.2763007879257202
Epoch 300, training loss: 7.549276351928711 = 1.1016924381256104 + 1.0 * 6.44758415222168
Epoch 300, val loss: 1.236440896987915
Epoch 310, training loss: 7.492704391479492 = 1.0506761074066162 + 1.0 * 6.442028045654297
Epoch 310, val loss: 1.1971429586410522
Epoch 320, training loss: 7.441620349884033 = 1.0008888244628906 + 1.0 * 6.440731525421143
Epoch 320, val loss: 1.1588891744613647
Epoch 330, training loss: 7.386420249938965 = 0.952838122844696 + 1.0 * 6.433582305908203
Epoch 330, val loss: 1.1220186948776245
Epoch 340, training loss: 7.339155197143555 = 0.9064126014709473 + 1.0 * 6.432742595672607
Epoch 340, val loss: 1.086567759513855
Epoch 350, training loss: 7.286947250366211 = 0.8623025417327881 + 1.0 * 6.424644470214844
Epoch 350, val loss: 1.0533089637756348
Epoch 360, training loss: 7.2411603927612305 = 0.8208087682723999 + 1.0 * 6.420351505279541
Epoch 360, val loss: 1.0226794481277466
Epoch 370, training loss: 7.207973480224609 = 0.7818214893341064 + 1.0 * 6.426151752471924
Epoch 370, val loss: 0.994545578956604
Epoch 380, training loss: 7.160679817199707 = 0.7457534074783325 + 1.0 * 6.414926528930664
Epoch 380, val loss: 0.9693012237548828
Epoch 390, training loss: 7.1206207275390625 = 0.7119518518447876 + 1.0 * 6.4086689949035645
Epoch 390, val loss: 0.9468814134597778
Epoch 400, training loss: 7.091367244720459 = 0.6801234483718872 + 1.0 * 6.411243915557861
Epoch 400, val loss: 0.9268420934677124
Epoch 410, training loss: 7.052648067474365 = 0.6501407623291016 + 1.0 * 6.402507305145264
Epoch 410, val loss: 0.9092046618461609
Epoch 420, training loss: 7.022828102111816 = 0.6217063069343567 + 1.0 * 6.401121616363525
Epoch 420, val loss: 0.893683671951294
Epoch 430, training loss: 6.991131782531738 = 0.5945552587509155 + 1.0 * 6.396576404571533
Epoch 430, val loss: 0.8800942897796631
Epoch 440, training loss: 6.9617533683776855 = 0.5683401226997375 + 1.0 * 6.393413066864014
Epoch 440, val loss: 0.8680574297904968
Epoch 450, training loss: 6.936291217803955 = 0.5429804921150208 + 1.0 * 6.393310546875
Epoch 450, val loss: 0.857235848903656
Epoch 460, training loss: 6.909300327301025 = 0.5186019539833069 + 1.0 * 6.390698432922363
Epoch 460, val loss: 0.8476375937461853
Epoch 470, training loss: 6.881580829620361 = 0.4950772523880005 + 1.0 * 6.38650369644165
Epoch 470, val loss: 0.8390798568725586
Epoch 480, training loss: 6.854151725769043 = 0.4721410870552063 + 1.0 * 6.382010459899902
Epoch 480, val loss: 0.8312100768089294
Epoch 490, training loss: 6.828622817993164 = 0.44977447390556335 + 1.0 * 6.378848552703857
Epoch 490, val loss: 0.8240175843238831
Epoch 500, training loss: 6.818648338317871 = 0.42814427614212036 + 1.0 * 6.390503883361816
Epoch 500, val loss: 0.8174540996551514
Epoch 510, training loss: 6.782019138336182 = 0.40760937333106995 + 1.0 * 6.3744096755981445
Epoch 510, val loss: 0.8117360472679138
Epoch 520, training loss: 6.762868881225586 = 0.38802170753479004 + 1.0 * 6.374846935272217
Epoch 520, val loss: 0.8068355917930603
Epoch 530, training loss: 6.74044132232666 = 0.3693249225616455 + 1.0 * 6.371116638183594
Epoch 530, val loss: 0.8026105165481567
Epoch 540, training loss: 6.724587917327881 = 0.3516462743282318 + 1.0 * 6.372941493988037
Epoch 540, val loss: 0.7991998195648193
Epoch 550, training loss: 6.703540802001953 = 0.3350616693496704 + 1.0 * 6.368479251861572
Epoch 550, val loss: 0.7966083288192749
Epoch 560, training loss: 6.684727191925049 = 0.31935015320777893 + 1.0 * 6.365376949310303
Epoch 560, val loss: 0.7946718335151672
Epoch 570, training loss: 6.670450210571289 = 0.3043993413448334 + 1.0 * 6.366050720214844
Epoch 570, val loss: 0.7933191657066345
Epoch 580, training loss: 6.6558732986450195 = 0.29022520780563354 + 1.0 * 6.36564826965332
Epoch 580, val loss: 0.7924822568893433
Epoch 590, training loss: 6.640276908874512 = 0.2767150402069092 + 1.0 * 6.363561630249023
Epoch 590, val loss: 0.7920603156089783
Epoch 600, training loss: 6.624190330505371 = 0.26376765966415405 + 1.0 * 6.360422611236572
Epoch 600, val loss: 0.7919835448265076
Epoch 610, training loss: 6.607605457305908 = 0.25128281116485596 + 1.0 * 6.356322765350342
Epoch 610, val loss: 0.7922148704528809
Epoch 620, training loss: 6.593789577484131 = 0.2391071766614914 + 1.0 * 6.354682445526123
Epoch 620, val loss: 0.7926821708679199
Epoch 630, training loss: 6.5871357917785645 = 0.22720132768154144 + 1.0 * 6.359934329986572
Epoch 630, val loss: 0.7933827638626099
Epoch 640, training loss: 6.569805145263672 = 0.21558737754821777 + 1.0 * 6.354218006134033
Epoch 640, val loss: 0.794277548789978
Epoch 650, training loss: 6.55477237701416 = 0.2042001336812973 + 1.0 * 6.350572109222412
Epoch 650, val loss: 0.7954034805297852
Epoch 660, training loss: 6.559283256530762 = 0.19303792715072632 + 1.0 * 6.366245269775391
Epoch 660, val loss: 0.7967517375946045
Epoch 670, training loss: 6.53078556060791 = 0.1822935789823532 + 1.0 * 6.34849214553833
Epoch 670, val loss: 0.7983376383781433
Epoch 680, training loss: 6.520541191101074 = 0.17192456126213074 + 1.0 * 6.348616600036621
Epoch 680, val loss: 0.8003262281417847
Epoch 690, training loss: 6.50728702545166 = 0.161965012550354 + 1.0 * 6.345322132110596
Epoch 690, val loss: 0.8027258515357971
Epoch 700, training loss: 6.49733829498291 = 0.15244551002979279 + 1.0 * 6.344892978668213
Epoch 700, val loss: 0.8055208325386047
Epoch 710, training loss: 6.48668098449707 = 0.1434401571750641 + 1.0 * 6.343240737915039
Epoch 710, val loss: 0.8086759448051453
Epoch 720, training loss: 6.479256629943848 = 0.13497163355350494 + 1.0 * 6.344285011291504
Epoch 720, val loss: 0.8122022747993469
Epoch 730, training loss: 6.467965126037598 = 0.127044215798378 + 1.0 * 6.340920925140381
Epoch 730, val loss: 0.8160781264305115
Epoch 740, training loss: 6.45951509475708 = 0.11962763965129852 + 1.0 * 6.339887619018555
Epoch 740, val loss: 0.8202850222587585
Epoch 750, training loss: 6.452122688293457 = 0.11269926279783249 + 1.0 * 6.339423656463623
Epoch 750, val loss: 0.8248598575592041
Epoch 760, training loss: 6.448807716369629 = 0.10623964667320251 + 1.0 * 6.3425679206848145
Epoch 760, val loss: 0.8296698927879333
Epoch 770, training loss: 6.439195156097412 = 0.10028276592493057 + 1.0 * 6.338912487030029
Epoch 770, val loss: 0.8346554636955261
Epoch 780, training loss: 6.429409027099609 = 0.09473377466201782 + 1.0 * 6.334675312042236
Epoch 780, val loss: 0.8399059772491455
Epoch 790, training loss: 6.427645683288574 = 0.08956388384103775 + 1.0 * 6.3380818367004395
Epoch 790, val loss: 0.8453683257102966
Epoch 800, training loss: 6.417135715484619 = 0.0847521498799324 + 1.0 * 6.332383632659912
Epoch 800, val loss: 0.8510244488716125
Epoch 810, training loss: 6.415071487426758 = 0.08026913553476334 + 1.0 * 6.334802150726318
Epoch 810, val loss: 0.8568177223205566
Epoch 820, training loss: 6.408339500427246 = 0.07609998434782028 + 1.0 * 6.332239627838135
Epoch 820, val loss: 0.8626556396484375
Epoch 830, training loss: 6.402344226837158 = 0.07220428436994553 + 1.0 * 6.330140113830566
Epoch 830, val loss: 0.8686755895614624
Epoch 840, training loss: 6.397988796234131 = 0.06857449561357498 + 1.0 * 6.329414367675781
Epoch 840, val loss: 0.874780535697937
Epoch 850, training loss: 6.393044948577881 = 0.06518704444169998 + 1.0 * 6.327857971191406
Epoch 850, val loss: 0.880929172039032
Epoch 860, training loss: 6.392385482788086 = 0.06204213201999664 + 1.0 * 6.330343246459961
Epoch 860, val loss: 0.8870681524276733
Epoch 870, training loss: 6.387608051300049 = 0.05909988656640053 + 1.0 * 6.328508377075195
Epoch 870, val loss: 0.8932793736457825
Epoch 880, training loss: 6.381814956665039 = 0.05635436251759529 + 1.0 * 6.325460433959961
Epoch 880, val loss: 0.8995433449745178
Epoch 890, training loss: 6.379698276519775 = 0.05378180742263794 + 1.0 * 6.325916290283203
Epoch 890, val loss: 0.9058021306991577
Epoch 900, training loss: 6.377166271209717 = 0.05137386918067932 + 1.0 * 6.32579231262207
Epoch 900, val loss: 0.9121265411376953
Epoch 910, training loss: 6.372917652130127 = 0.04912763461470604 + 1.0 * 6.323790073394775
Epoch 910, val loss: 0.9182730317115784
Epoch 920, training loss: 6.367856502532959 = 0.04701181873679161 + 1.0 * 6.320844650268555
Epoch 920, val loss: 0.9245539903640747
Epoch 930, training loss: 6.365190505981445 = 0.04501970112323761 + 1.0 * 6.320170879364014
Epoch 930, val loss: 0.930819034576416
Epoch 940, training loss: 6.374221324920654 = 0.043148238211870193 + 1.0 * 6.33107328414917
Epoch 940, val loss: 0.937072217464447
Epoch 950, training loss: 6.3618998527526855 = 0.04139259085059166 + 1.0 * 6.320507049560547
Epoch 950, val loss: 0.9432169198989868
Epoch 960, training loss: 6.3570051193237305 = 0.039747778326272964 + 1.0 * 6.317257404327393
Epoch 960, val loss: 0.9492748379707336
Epoch 970, training loss: 6.360504627227783 = 0.03819432854652405 + 1.0 * 6.322310447692871
Epoch 970, val loss: 0.9553652405738831
Epoch 980, training loss: 6.355644702911377 = 0.036727551370859146 + 1.0 * 6.318917274475098
Epoch 980, val loss: 0.9613913297653198
Epoch 990, training loss: 6.3506760597229 = 0.03534723445773125 + 1.0 * 6.315328598022461
Epoch 990, val loss: 0.9673203229904175
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8144438587243016
The final CL Acc:0.79506, 0.00972, The final GNN Acc:0.81093, 0.00263
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13242])
remove edge: torch.Size([2, 7910])
updated graph: torch.Size([2, 10596])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.537243843078613 = 1.9404367208480835 + 1.0 * 8.596807479858398
Epoch 0, val loss: 1.9382573366165161
Epoch 10, training loss: 10.526430130004883 = 1.9298851490020752 + 1.0 * 8.596545219421387
Epoch 10, val loss: 1.9281537532806396
Epoch 20, training loss: 10.511520385742188 = 1.9166864156723022 + 1.0 * 8.594834327697754
Epoch 20, val loss: 1.9150196313858032
Epoch 30, training loss: 10.479259490966797 = 1.8981726169586182 + 1.0 * 8.581087112426758
Epoch 30, val loss: 1.8960652351379395
Epoch 40, training loss: 10.361676216125488 = 1.8731542825698853 + 1.0 * 8.488521575927734
Epoch 40, val loss: 1.8713083267211914
Epoch 50, training loss: 9.798763275146484 = 1.8456830978393555 + 1.0 * 7.953080177307129
Epoch 50, val loss: 1.8454481363296509
Epoch 60, training loss: 9.318799018859863 = 1.8232365846633911 + 1.0 * 7.4955620765686035
Epoch 60, val loss: 1.824428915977478
Epoch 70, training loss: 8.914453506469727 = 1.808425784111023 + 1.0 * 7.106028079986572
Epoch 70, val loss: 1.8096427917480469
Epoch 80, training loss: 8.712059020996094 = 1.7953901290893555 + 1.0 * 6.916668891906738
Epoch 80, val loss: 1.7966042757034302
Epoch 90, training loss: 8.59345817565918 = 1.7802650928497314 + 1.0 * 6.813192844390869
Epoch 90, val loss: 1.7823160886764526
Epoch 100, training loss: 8.52501392364502 = 1.7634061574935913 + 1.0 * 6.761607646942139
Epoch 100, val loss: 1.7666995525360107
Epoch 110, training loss: 8.46497917175293 = 1.7459686994552612 + 1.0 * 6.719010829925537
Epoch 110, val loss: 1.750444769859314
Epoch 120, training loss: 8.40822982788086 = 1.7277743816375732 + 1.0 * 6.680455684661865
Epoch 120, val loss: 1.733860731124878
Epoch 130, training loss: 8.350459098815918 = 1.7074681520462036 + 1.0 * 6.642990589141846
Epoch 130, val loss: 1.7159427404403687
Epoch 140, training loss: 8.297185897827148 = 1.6834927797317505 + 1.0 * 6.613692760467529
Epoch 140, val loss: 1.6953847408294678
Epoch 150, training loss: 8.24359130859375 = 1.6550679206848145 + 1.0 * 6.588523864746094
Epoch 150, val loss: 1.671330451965332
Epoch 160, training loss: 8.1886625289917 = 1.6209081411361694 + 1.0 * 6.56775426864624
Epoch 160, val loss: 1.642386794090271
Epoch 170, training loss: 8.130240440368652 = 1.5799458026885986 + 1.0 * 6.550294876098633
Epoch 170, val loss: 1.6077253818511963
Epoch 180, training loss: 8.066184043884277 = 1.5329142808914185 + 1.0 * 6.53326940536499
Epoch 180, val loss: 1.5681273937225342
Epoch 190, training loss: 7.997293472290039 = 1.4797117710113525 + 1.0 * 6.517581462860107
Epoch 190, val loss: 1.5234384536743164
Epoch 200, training loss: 7.927605152130127 = 1.4204694032669067 + 1.0 * 6.50713586807251
Epoch 200, val loss: 1.474086046218872
Epoch 210, training loss: 7.852957725524902 = 1.3582689762115479 + 1.0 * 6.494688510894775
Epoch 210, val loss: 1.423324465751648
Epoch 220, training loss: 7.778256893157959 = 1.294620394706726 + 1.0 * 6.483636379241943
Epoch 220, val loss: 1.3715908527374268
Epoch 230, training loss: 7.7067131996154785 = 1.2311140298843384 + 1.0 * 6.47559928894043
Epoch 230, val loss: 1.320636510848999
Epoch 240, training loss: 7.637984275817871 = 1.1693224906921387 + 1.0 * 6.468661785125732
Epoch 240, val loss: 1.271521806716919
Epoch 250, training loss: 7.570494174957275 = 1.1107369661331177 + 1.0 * 6.459757328033447
Epoch 250, val loss: 1.2254304885864258
Epoch 260, training loss: 7.507122993469238 = 1.0550471544265747 + 1.0 * 6.452075958251953
Epoch 260, val loss: 1.1819664239883423
Epoch 270, training loss: 7.445669174194336 = 1.0018413066864014 + 1.0 * 6.443828105926514
Epoch 270, val loss: 1.1406611204147339
Epoch 280, training loss: 7.393811225891113 = 0.9510524272918701 + 1.0 * 6.442759037017822
Epoch 280, val loss: 1.101488471031189
Epoch 290, training loss: 7.3359198570251465 = 0.9037123322486877 + 1.0 * 6.4322075843811035
Epoch 290, val loss: 1.0651893615722656
Epoch 300, training loss: 7.284150123596191 = 0.8587188720703125 + 1.0 * 6.425431251525879
Epoch 300, val loss: 1.0308079719543457
Epoch 310, training loss: 7.2366509437561035 = 0.8154255151748657 + 1.0 * 6.421225547790527
Epoch 310, val loss: 0.9979915022850037
Epoch 320, training loss: 7.191030502319336 = 0.7743194699287415 + 1.0 * 6.41671085357666
Epoch 320, val loss: 0.9671642184257507
Epoch 330, training loss: 7.144947528839111 = 0.7352442741394043 + 1.0 * 6.409703254699707
Epoch 330, val loss: 0.9382986426353455
Epoch 340, training loss: 7.103421688079834 = 0.6978697180747986 + 1.0 * 6.405551910400391
Epoch 340, val loss: 0.9113057851791382
Epoch 350, training loss: 7.064485549926758 = 0.6628745794296265 + 1.0 * 6.401610851287842
Epoch 350, val loss: 0.8868505954742432
Epoch 360, training loss: 7.028522491455078 = 0.6302970051765442 + 1.0 * 6.3982253074646
Epoch 360, val loss: 0.8647081851959229
Epoch 370, training loss: 6.992897033691406 = 0.5993098020553589 + 1.0 * 6.393587112426758
Epoch 370, val loss: 0.8443588018417358
Epoch 380, training loss: 6.962886810302734 = 0.5696741342544556 + 1.0 * 6.393212795257568
Epoch 380, val loss: 0.8256822228431702
Epoch 390, training loss: 6.931103229522705 = 0.5415019989013672 + 1.0 * 6.389601230621338
Epoch 390, val loss: 0.8088217377662659
Epoch 400, training loss: 6.898926258087158 = 0.5146721005439758 + 1.0 * 6.384253978729248
Epoch 400, val loss: 0.7934369444847107
Epoch 410, training loss: 6.869040489196777 = 0.48882418870925903 + 1.0 * 6.380216121673584
Epoch 410, val loss: 0.7793995141983032
Epoch 420, training loss: 6.843442916870117 = 0.4639354348182678 + 1.0 * 6.379507541656494
Epoch 420, val loss: 0.7667863368988037
Epoch 430, training loss: 6.815529823303223 = 0.4399457573890686 + 1.0 * 6.375584125518799
Epoch 430, val loss: 0.7553653120994568
Epoch 440, training loss: 6.7883501052856445 = 0.41659608483314514 + 1.0 * 6.371754169464111
Epoch 440, val loss: 0.745088517665863
Epoch 450, training loss: 6.764018535614014 = 0.3940223455429077 + 1.0 * 6.369996070861816
Epoch 450, val loss: 0.736063539981842
Epoch 460, training loss: 6.738820552825928 = 0.372256338596344 + 1.0 * 6.3665642738342285
Epoch 460, val loss: 0.728108286857605
Epoch 470, training loss: 6.726565837860107 = 0.3511914312839508 + 1.0 * 6.3753743171691895
Epoch 470, val loss: 0.7212785482406616
Epoch 480, training loss: 6.695340156555176 = 0.3310360014438629 + 1.0 * 6.364304065704346
Epoch 480, val loss: 0.7156734466552734
Epoch 490, training loss: 6.671595096588135 = 0.31164589524269104 + 1.0 * 6.359949111938477
Epoch 490, val loss: 0.7109271287918091
Epoch 500, training loss: 6.659648418426514 = 0.29302483797073364 + 1.0 * 6.366623401641846
Epoch 500, val loss: 0.70731520652771
Epoch 510, training loss: 6.631966590881348 = 0.27536851167678833 + 1.0 * 6.356597900390625
Epoch 510, val loss: 0.7045761346817017
Epoch 520, training loss: 6.624979019165039 = 0.25853729248046875 + 1.0 * 6.36644172668457
Epoch 520, val loss: 0.7027016282081604
Epoch 530, training loss: 6.5961174964904785 = 0.24271030724048615 + 1.0 * 6.353407382965088
Epoch 530, val loss: 0.7017640471458435
Epoch 540, training loss: 6.577868461608887 = 0.22771567106246948 + 1.0 * 6.350152969360352
Epoch 540, val loss: 0.7015643119812012
Epoch 550, training loss: 6.567784786224365 = 0.2135290503501892 + 1.0 * 6.354255676269531
Epoch 550, val loss: 0.7022568583488464
Epoch 560, training loss: 6.554636001586914 = 0.20023933053016663 + 1.0 * 6.354396820068359
Epoch 560, val loss: 0.7038125395774841
Epoch 570, training loss: 6.537571430206299 = 0.1878550797700882 + 1.0 * 6.3497161865234375
Epoch 570, val loss: 0.7058142423629761
Epoch 580, training loss: 6.52051305770874 = 0.1762615293264389 + 1.0 * 6.34425163269043
Epoch 580, val loss: 0.7084768414497375
Epoch 590, training loss: 6.512685775756836 = 0.16540950536727905 + 1.0 * 6.347276210784912
Epoch 590, val loss: 0.7117615938186646
Epoch 600, training loss: 6.501108646392822 = 0.15532228350639343 + 1.0 * 6.3457865715026855
Epoch 600, val loss: 0.7154993414878845
Epoch 610, training loss: 6.489870071411133 = 0.1459837555885315 + 1.0 * 6.343886375427246
Epoch 610, val loss: 0.7197195887565613
Epoch 620, training loss: 6.475549697875977 = 0.1373538076877594 + 1.0 * 6.33819580078125
Epoch 620, val loss: 0.7241565585136414
Epoch 630, training loss: 6.465804576873779 = 0.12931662797927856 + 1.0 * 6.336487770080566
Epoch 630, val loss: 0.7289969325065613
Epoch 640, training loss: 6.457993984222412 = 0.12182226777076721 + 1.0 * 6.336171627044678
Epoch 640, val loss: 0.7342485785484314
Epoch 650, training loss: 6.450024127960205 = 0.1148967444896698 + 1.0 * 6.335127353668213
Epoch 650, val loss: 0.7398059368133545
Epoch 660, training loss: 6.4440531730651855 = 0.1084853932261467 + 1.0 * 6.335567951202393
Epoch 660, val loss: 0.7454420328140259
Epoch 670, training loss: 6.434686183929443 = 0.10257567465305328 + 1.0 * 6.332110404968262
Epoch 670, val loss: 0.7512953877449036
Epoch 680, training loss: 6.425757884979248 = 0.09708932042121887 + 1.0 * 6.328668594360352
Epoch 680, val loss: 0.757255494594574
Epoch 690, training loss: 6.4206013679504395 = 0.09197468310594559 + 1.0 * 6.32862663269043
Epoch 690, val loss: 0.7633894681930542
Epoch 700, training loss: 6.4151997566223145 = 0.08722522854804993 + 1.0 * 6.327974319458008
Epoch 700, val loss: 0.7697031497955322
Epoch 710, training loss: 6.410463333129883 = 0.08282898366451263 + 1.0 * 6.327634334564209
Epoch 710, val loss: 0.7759829759597778
Epoch 720, training loss: 6.404174327850342 = 0.07872430980205536 + 1.0 * 6.3254499435424805
Epoch 720, val loss: 0.7823376059532166
Epoch 730, training loss: 6.402023792266846 = 0.07489456981420517 + 1.0 * 6.327129364013672
Epoch 730, val loss: 0.7888485789299011
Epoch 740, training loss: 6.392642974853516 = 0.07133223116397858 + 1.0 * 6.321310520172119
Epoch 740, val loss: 0.7952728867530823
Epoch 750, training loss: 6.390792369842529 = 0.06800027936697006 + 1.0 * 6.322792053222656
Epoch 750, val loss: 0.8016498684883118
Epoch 760, training loss: 6.385016441345215 = 0.06489056348800659 + 1.0 * 6.320126056671143
Epoch 760, val loss: 0.8081689476966858
Epoch 770, training loss: 6.380842685699463 = 0.061986152082681656 + 1.0 * 6.318856716156006
Epoch 770, val loss: 0.8145633935928345
Epoch 780, training loss: 6.37667179107666 = 0.05925045534968376 + 1.0 * 6.3174214363098145
Epoch 780, val loss: 0.8209723830223083
Epoch 790, training loss: 6.380627155303955 = 0.05668533965945244 + 1.0 * 6.323941707611084
Epoch 790, val loss: 0.8274552226066589
Epoch 800, training loss: 6.375030994415283 = 0.05427591875195503 + 1.0 * 6.3207550048828125
Epoch 800, val loss: 0.8338122367858887
Epoch 810, training loss: 6.36756706237793 = 0.052027519792318344 + 1.0 * 6.315539360046387
Epoch 810, val loss: 0.8401076793670654
Epoch 820, training loss: 6.363653659820557 = 0.04990460351109505 + 1.0 * 6.313748836517334
Epoch 820, val loss: 0.8463582992553711
Epoch 830, training loss: 6.362837791442871 = 0.047900181263685226 + 1.0 * 6.314937591552734
Epoch 830, val loss: 0.8525077700614929
Epoch 840, training loss: 6.358196258544922 = 0.0460122749209404 + 1.0 * 6.312183856964111
Epoch 840, val loss: 0.8587712049484253
Epoch 850, training loss: 6.358835697174072 = 0.044230394065380096 + 1.0 * 6.314605236053467
Epoch 850, val loss: 0.8648127913475037
Epoch 860, training loss: 6.3512091636657715 = 0.04255465790629387 + 1.0 * 6.308654308319092
Epoch 860, val loss: 0.87083500623703
Epoch 870, training loss: 6.348808765411377 = 0.04096626490354538 + 1.0 * 6.30784273147583
Epoch 870, val loss: 0.8768013715744019
Epoch 880, training loss: 6.351214408874512 = 0.039456941187381744 + 1.0 * 6.311757564544678
Epoch 880, val loss: 0.8827406764030457
Epoch 890, training loss: 6.350939750671387 = 0.0380343459546566 + 1.0 * 6.312905311584473
Epoch 890, val loss: 0.8886364698410034
Epoch 900, training loss: 6.342757701873779 = 0.03669844940304756 + 1.0 * 6.30605936050415
Epoch 900, val loss: 0.8943288922309875
Epoch 910, training loss: 6.340952396392822 = 0.035422809422016144 + 1.0 * 6.305529594421387
Epoch 910, val loss: 0.8999800086021423
Epoch 920, training loss: 6.341853618621826 = 0.03420835733413696 + 1.0 * 6.307645320892334
Epoch 920, val loss: 0.9056893587112427
Epoch 930, training loss: 6.335748672485352 = 0.03305335342884064 + 1.0 * 6.302695274353027
Epoch 930, val loss: 0.9112389087677002
Epoch 940, training loss: 6.3479413986206055 = 0.031955424696207047 + 1.0 * 6.315986156463623
Epoch 940, val loss: 0.9167353510856628
Epoch 950, training loss: 6.333489894866943 = 0.03092287667095661 + 1.0 * 6.302567005157471
Epoch 950, val loss: 0.9221476316452026
Epoch 960, training loss: 6.330722332000732 = 0.02993367239832878 + 1.0 * 6.300788879394531
Epoch 960, val loss: 0.9274513125419617
Epoch 970, training loss: 6.340699672698975 = 0.02898862212896347 + 1.0 * 6.311710834503174
Epoch 970, val loss: 0.9327591061592102
Epoch 980, training loss: 6.329349517822266 = 0.028092246502637863 + 1.0 * 6.301257133483887
Epoch 980, val loss: 0.9379493594169617
Epoch 990, training loss: 6.3260884284973145 = 0.027234625071287155 + 1.0 * 6.298853874206543
Epoch 990, val loss: 0.9429963231086731
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 10.544525146484375 = 1.9477150440216064 + 1.0 * 8.596810340881348
Epoch 0, val loss: 1.9487665891647339
Epoch 10, training loss: 10.533731460571289 = 1.937250018119812 + 1.0 * 8.596481323242188
Epoch 10, val loss: 1.9389582872390747
Epoch 20, training loss: 10.518614768981934 = 1.9243773221969604 + 1.0 * 8.594237327575684
Epoch 20, val loss: 1.9265224933624268
Epoch 30, training loss: 10.485052108764648 = 1.9065203666687012 + 1.0 * 8.578531265258789
Epoch 30, val loss: 1.909153938293457
Epoch 40, training loss: 10.370965003967285 = 1.882529377937317 + 1.0 * 8.488435745239258
Epoch 40, val loss: 1.8864214420318604
Epoch 50, training loss: 9.884066581726074 = 1.8564163446426392 + 1.0 * 8.027649879455566
Epoch 50, val loss: 1.8617205619812012
Epoch 60, training loss: 9.438460350036621 = 1.8325802087783813 + 1.0 * 7.605879783630371
Epoch 60, val loss: 1.8394556045532227
Epoch 70, training loss: 9.09833812713623 = 1.8162935972213745 + 1.0 * 7.282044887542725
Epoch 70, val loss: 1.8242100477218628
Epoch 80, training loss: 8.914769172668457 = 1.801459789276123 + 1.0 * 7.113309383392334
Epoch 80, val loss: 1.8104561567306519
Epoch 90, training loss: 8.747709274291992 = 1.7845892906188965 + 1.0 * 6.963120460510254
Epoch 90, val loss: 1.794541597366333
Epoch 100, training loss: 8.646417617797852 = 1.7664090394973755 + 1.0 * 6.880008220672607
Epoch 100, val loss: 1.7767877578735352
Epoch 110, training loss: 8.574682235717773 = 1.7475827932357788 + 1.0 * 6.827099323272705
Epoch 110, val loss: 1.7583502531051636
Epoch 120, training loss: 8.506329536437988 = 1.7282794713974 + 1.0 * 6.778050422668457
Epoch 120, val loss: 1.7397373914718628
Epoch 130, training loss: 8.443840026855469 = 1.7067046165466309 + 1.0 * 6.737135887145996
Epoch 130, val loss: 1.719439148902893
Epoch 140, training loss: 8.388080596923828 = 1.6804288625717163 + 1.0 * 6.707651615142822
Epoch 140, val loss: 1.6956682205200195
Epoch 150, training loss: 8.3281831741333 = 1.6488425731658936 + 1.0 * 6.679340362548828
Epoch 150, val loss: 1.667946457862854
Epoch 160, training loss: 8.266338348388672 = 1.6121970415115356 + 1.0 * 6.654140949249268
Epoch 160, val loss: 1.6364521980285645
Epoch 170, training loss: 8.200607299804688 = 1.570068597793579 + 1.0 * 6.630538463592529
Epoch 170, val loss: 1.6004424095153809
Epoch 180, training loss: 8.132482528686523 = 1.5221983194351196 + 1.0 * 6.610284328460693
Epoch 180, val loss: 1.55971360206604
Epoch 190, training loss: 8.060710906982422 = 1.469451904296875 + 1.0 * 6.591259479522705
Epoch 190, val loss: 1.5153861045837402
Epoch 200, training loss: 7.988125324249268 = 1.4150041341781616 + 1.0 * 6.573121070861816
Epoch 200, val loss: 1.470180869102478
Epoch 210, training loss: 7.9146223068237305 = 1.360100507736206 + 1.0 * 6.5545220375061035
Epoch 210, val loss: 1.4247854948043823
Epoch 220, training loss: 7.847224235534668 = 1.3046786785125732 + 1.0 * 6.542545795440674
Epoch 220, val loss: 1.3790521621704102
Epoch 230, training loss: 7.777701377868652 = 1.2500983476638794 + 1.0 * 6.5276031494140625
Epoch 230, val loss: 1.3343451023101807
Epoch 240, training loss: 7.7108845710754395 = 1.196154236793518 + 1.0 * 6.514730453491211
Epoch 240, val loss: 1.2904757261276245
Epoch 250, training loss: 7.6493964195251465 = 1.1430482864379883 + 1.0 * 6.506348133087158
Epoch 250, val loss: 1.2479369640350342
Epoch 260, training loss: 7.5871052742004395 = 1.0920270681381226 + 1.0 * 6.495078086853027
Epoch 260, val loss: 1.2075064182281494
Epoch 270, training loss: 7.526978492736816 = 1.0423259735107422 + 1.0 * 6.484652519226074
Epoch 270, val loss: 1.1683906316757202
Epoch 280, training loss: 7.473618507385254 = 0.994179904460907 + 1.0 * 6.479438781738281
Epoch 280, val loss: 1.1308460235595703
Epoch 290, training loss: 7.4161176681518555 = 0.9476912021636963 + 1.0 * 6.46842622756958
Epoch 290, val loss: 1.0948584079742432
Epoch 300, training loss: 7.368191242218018 = 0.9024863243103027 + 1.0 * 6.465704917907715
Epoch 300, val loss: 1.0600146055221558
Epoch 310, training loss: 7.313504219055176 = 0.8588970303535461 + 1.0 * 6.454607009887695
Epoch 310, val loss: 1.0264511108398438
Epoch 320, training loss: 7.264958381652832 = 0.8166272044181824 + 1.0 * 6.448331356048584
Epoch 320, val loss: 0.994056761264801
Epoch 330, training loss: 7.223396301269531 = 0.775757372379303 + 1.0 * 6.447638988494873
Epoch 330, val loss: 0.9629757404327393
Epoch 340, training loss: 7.176285743713379 = 0.7370721697807312 + 1.0 * 6.439213752746582
Epoch 340, val loss: 0.9339166879653931
Epoch 350, training loss: 7.132061958312988 = 0.7002960443496704 + 1.0 * 6.431766033172607
Epoch 350, val loss: 0.9068182110786438
Epoch 360, training loss: 7.092045783996582 = 0.6651425361633301 + 1.0 * 6.426903247833252
Epoch 360, val loss: 0.8817981481552124
Epoch 370, training loss: 7.058703422546387 = 0.6319370269775391 + 1.0 * 6.426766395568848
Epoch 370, val loss: 0.8590993285179138
Epoch 380, training loss: 7.019633769989014 = 0.6007278561592102 + 1.0 * 6.418905735015869
Epoch 380, val loss: 0.8389058709144592
Epoch 390, training loss: 6.98617696762085 = 0.5711491703987122 + 1.0 * 6.415027618408203
Epoch 390, val loss: 0.820892333984375
Epoch 400, training loss: 6.9585371017456055 = 0.5433164834976196 + 1.0 * 6.415220737457275
Epoch 400, val loss: 0.8051359057426453
Epoch 410, training loss: 6.923557758331299 = 0.5171318054199219 + 1.0 * 6.406425952911377
Epoch 410, val loss: 0.7913727164268494
Epoch 420, training loss: 6.899957180023193 = 0.4922981262207031 + 1.0 * 6.40765905380249
Epoch 420, val loss: 0.7794314622879028
Epoch 430, training loss: 6.868940353393555 = 0.46901988983154297 + 1.0 * 6.399920463562012
Epoch 430, val loss: 0.7691715955734253
Epoch 440, training loss: 6.843298435211182 = 0.44709816575050354 + 1.0 * 6.396200180053711
Epoch 440, val loss: 0.7604114413261414
Epoch 450, training loss: 6.824296951293945 = 0.4264150559902191 + 1.0 * 6.397881984710693
Epoch 450, val loss: 0.7530131340026855
Epoch 460, training loss: 6.7957587242126465 = 0.406974196434021 + 1.0 * 6.388784408569336
Epoch 460, val loss: 0.7467445135116577
Epoch 470, training loss: 6.774194717407227 = 0.3885916471481323 + 1.0 * 6.385602951049805
Epoch 470, val loss: 0.7414621710777283
Epoch 480, training loss: 6.7722954750061035 = 0.3712330758571625 + 1.0 * 6.401062488555908
Epoch 480, val loss: 0.7370983362197876
Epoch 490, training loss: 6.739680767059326 = 0.35498669743537903 + 1.0 * 6.3846940994262695
Epoch 490, val loss: 0.7335057258605957
Epoch 500, training loss: 6.7167487144470215 = 0.3396095931529999 + 1.0 * 6.377139091491699
Epoch 500, val loss: 0.7305850982666016
Epoch 510, training loss: 6.699039459228516 = 0.3249569237232208 + 1.0 * 6.374082565307617
Epoch 510, val loss: 0.7284239530563354
Epoch 520, training loss: 6.682116508483887 = 0.3110142648220062 + 1.0 * 6.371102333068848
Epoch 520, val loss: 0.7268266677856445
Epoch 530, training loss: 6.667211055755615 = 0.2977692484855652 + 1.0 * 6.369441986083984
Epoch 530, val loss: 0.7256728410720825
Epoch 540, training loss: 6.658113479614258 = 0.28506457805633545 + 1.0 * 6.373048782348633
Epoch 540, val loss: 0.7250261306762695
Epoch 550, training loss: 6.638883590698242 = 0.27286359667778015 + 1.0 * 6.366020202636719
Epoch 550, val loss: 0.7247483134269714
Epoch 560, training loss: 6.625888824462891 = 0.2610470652580261 + 1.0 * 6.364841938018799
Epoch 560, val loss: 0.7248377203941345
Epoch 570, training loss: 6.611244201660156 = 0.24955560266971588 + 1.0 * 6.361688613891602
Epoch 570, val loss: 0.7252047061920166
Epoch 580, training loss: 6.597141265869141 = 0.23829533159732819 + 1.0 * 6.3588457107543945
Epoch 580, val loss: 0.725900411605835
Epoch 590, training loss: 6.58620548248291 = 0.22725041210651398 + 1.0 * 6.358954906463623
Epoch 590, val loss: 0.7268750071525574
Epoch 600, training loss: 6.574531555175781 = 0.21644216775894165 + 1.0 * 6.358089447021484
Epoch 600, val loss: 0.7280081510543823
Epoch 610, training loss: 6.561160087585449 = 0.2058180272579193 + 1.0 * 6.355341911315918
Epoch 610, val loss: 0.7294440269470215
Epoch 620, training loss: 6.546655654907227 = 0.1954052448272705 + 1.0 * 6.351250171661377
Epoch 620, val loss: 0.7311627268791199
Epoch 630, training loss: 6.539036750793457 = 0.18521007895469666 + 1.0 * 6.353826522827148
Epoch 630, val loss: 0.7331857085227966
Epoch 640, training loss: 6.526660919189453 = 0.17532287538051605 + 1.0 * 6.351337909698486
Epoch 640, val loss: 0.7355349659919739
Epoch 650, training loss: 6.516530513763428 = 0.1658094823360443 + 1.0 * 6.3507208824157715
Epoch 650, val loss: 0.738185465335846
Epoch 660, training loss: 6.500692367553711 = 0.15672160685062408 + 1.0 * 6.343970775604248
Epoch 660, val loss: 0.7412348389625549
Epoch 670, training loss: 6.491946697235107 = 0.14809352159500122 + 1.0 * 6.343852996826172
Epoch 670, val loss: 0.7445716857910156
Epoch 680, training loss: 6.481446266174316 = 0.1398935317993164 + 1.0 * 6.341552734375
Epoch 680, val loss: 0.7483627796173096
Epoch 690, training loss: 6.477157115936279 = 0.13215652108192444 + 1.0 * 6.345000743865967
Epoch 690, val loss: 0.7524955868721008
Epoch 700, training loss: 6.467283725738525 = 0.12490827590227127 + 1.0 * 6.3423752784729
Epoch 700, val loss: 0.7567547559738159
Epoch 710, training loss: 6.456957817077637 = 0.11809796094894409 + 1.0 * 6.338860034942627
Epoch 710, val loss: 0.7613073587417603
Epoch 720, training loss: 6.4494948387146 = 0.11169996857643127 + 1.0 * 6.337794780731201
Epoch 720, val loss: 0.7661237716674805
Epoch 730, training loss: 6.442580699920654 = 0.10569840669631958 + 1.0 * 6.3368821144104
Epoch 730, val loss: 0.771054208278656
Epoch 740, training loss: 6.438620567321777 = 0.10008376091718674 + 1.0 * 6.338536739349365
Epoch 740, val loss: 0.7761130928993225
Epoch 750, training loss: 6.427431106567383 = 0.09483257681131363 + 1.0 * 6.332598686218262
Epoch 750, val loss: 0.7812514305114746
Epoch 760, training loss: 6.424843788146973 = 0.08990529924631119 + 1.0 * 6.3349385261535645
Epoch 760, val loss: 0.7865754961967468
Epoch 770, training loss: 6.415821552276611 = 0.08528756350278854 + 1.0 * 6.330533981323242
Epoch 770, val loss: 0.7919281721115112
Epoch 780, training loss: 6.4113545417785645 = 0.08094769716262817 + 1.0 * 6.330406665802002
Epoch 780, val loss: 0.7973936796188354
Epoch 790, training loss: 6.407254219055176 = 0.07687335461378098 + 1.0 * 6.330380916595459
Epoch 790, val loss: 0.8029108047485352
Epoch 800, training loss: 6.408151149749756 = 0.07307177782058716 + 1.0 * 6.335079193115234
Epoch 800, val loss: 0.8084864020347595
Epoch 810, training loss: 6.3961052894592285 = 0.06953075528144836 + 1.0 * 6.326574325561523
Epoch 810, val loss: 0.8137673139572144
Epoch 820, training loss: 6.39089822769165 = 0.06620287150144577 + 1.0 * 6.324695587158203
Epoch 820, val loss: 0.8192838430404663
Epoch 830, training loss: 6.386995315551758 = 0.06306949257850647 + 1.0 * 6.323925971984863
Epoch 830, val loss: 0.8249173760414124
Epoch 840, training loss: 6.389691352844238 = 0.06012554466724396 + 1.0 * 6.32956600189209
Epoch 840, val loss: 0.8304691314697266
Epoch 850, training loss: 6.378378391265869 = 0.057376183569431305 + 1.0 * 6.321002006530762
Epoch 850, val loss: 0.8358842134475708
Epoch 860, training loss: 6.374446868896484 = 0.05478283017873764 + 1.0 * 6.319664001464844
Epoch 860, val loss: 0.8413528800010681
Epoch 870, training loss: 6.378349304199219 = 0.052341900765895844 + 1.0 * 6.32600736618042
Epoch 870, val loss: 0.8468466997146606
Epoch 880, training loss: 6.374424934387207 = 0.0500534288585186 + 1.0 * 6.324371337890625
Epoch 880, val loss: 0.8523377180099487
Epoch 890, training loss: 6.364811897277832 = 0.04790274426341057 + 1.0 * 6.316909313201904
Epoch 890, val loss: 0.857594907283783
Epoch 900, training loss: 6.36305046081543 = 0.045872461050748825 + 1.0 * 6.317177772521973
Epoch 900, val loss: 0.8629871606826782
Epoch 910, training loss: 6.365530490875244 = 0.043955545872449875 + 1.0 * 6.321575164794922
Epoch 910, val loss: 0.8683774471282959
Epoch 920, training loss: 6.356550216674805 = 0.04215462878346443 + 1.0 * 6.314395427703857
Epoch 920, val loss: 0.8736111521720886
Epoch 930, training loss: 6.355540752410889 = 0.04045142978429794 + 1.0 * 6.315089225769043
Epoch 930, val loss: 0.8788530826568604
Epoch 940, training loss: 6.357409954071045 = 0.03884017840027809 + 1.0 * 6.318569660186768
Epoch 940, val loss: 0.8841242790222168
Epoch 950, training loss: 6.349071979522705 = 0.03732604905962944 + 1.0 * 6.311746120452881
Epoch 950, val loss: 0.8892032504081726
Epoch 960, training loss: 6.347048282623291 = 0.03589294105768204 + 1.0 * 6.311155319213867
Epoch 960, val loss: 0.8942983746528625
Epoch 970, training loss: 6.348273277282715 = 0.0345333032310009 + 1.0 * 6.313739776611328
Epoch 970, val loss: 0.89943927526474
Epoch 980, training loss: 6.343830108642578 = 0.033246103674173355 + 1.0 * 6.31058406829834
Epoch 980, val loss: 0.9044632315635681
Epoch 990, training loss: 6.350029468536377 = 0.03202946484088898 + 1.0 * 6.317999839782715
Epoch 990, val loss: 0.9094353318214417
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 10.558773040771484 = 1.9619592428207397 + 1.0 * 8.596814155578613
Epoch 0, val loss: 1.94955313205719
Epoch 10, training loss: 10.54783821105957 = 1.951364517211914 + 1.0 * 8.596473693847656
Epoch 10, val loss: 1.939825415611267
Epoch 20, training loss: 10.53178882598877 = 1.9379253387451172 + 1.0 * 8.593863487243652
Epoch 20, val loss: 1.9271444082260132
Epoch 30, training loss: 10.492280006408691 = 1.9188015460968018 + 1.0 * 8.573478698730469
Epoch 30, val loss: 1.9088525772094727
Epoch 40, training loss: 10.332263946533203 = 1.8933281898498535 + 1.0 * 8.438935279846191
Epoch 40, val loss: 1.8849742412567139
Epoch 50, training loss: 9.862319946289062 = 1.8651803731918335 + 1.0 * 7.997139930725098
Epoch 50, val loss: 1.8590195178985596
Epoch 60, training loss: 9.41093635559082 = 1.8421049118041992 + 1.0 * 7.568830966949463
Epoch 60, val loss: 1.838221549987793
Epoch 70, training loss: 8.992534637451172 = 1.827515721321106 + 1.0 * 7.165018558502197
Epoch 70, val loss: 1.824437141418457
Epoch 80, training loss: 8.788591384887695 = 1.8118517398834229 + 1.0 * 6.976739883422852
Epoch 80, val loss: 1.8100093603134155
Epoch 90, training loss: 8.67979907989502 = 1.7915136814117432 + 1.0 * 6.888285160064697
Epoch 90, val loss: 1.7924914360046387
Epoch 100, training loss: 8.60527229309082 = 1.7701036930084229 + 1.0 * 6.835168361663818
Epoch 100, val loss: 1.7743005752563477
Epoch 110, training loss: 8.531546592712402 = 1.751470923423767 + 1.0 * 6.780075550079346
Epoch 110, val loss: 1.757898211479187
Epoch 120, training loss: 8.461406707763672 = 1.734818696975708 + 1.0 * 6.726587772369385
Epoch 120, val loss: 1.7424994707107544
Epoch 130, training loss: 8.399046897888184 = 1.7165799140930176 + 1.0 * 6.682466983795166
Epoch 130, val loss: 1.7260949611663818
Epoch 140, training loss: 8.341894149780273 = 1.6948705911636353 + 1.0 * 6.6470232009887695
Epoch 140, val loss: 1.7072176933288574
Epoch 150, training loss: 8.290331840515137 = 1.6692543029785156 + 1.0 * 6.621077537536621
Epoch 150, val loss: 1.6852372884750366
Epoch 160, training loss: 8.232412338256836 = 1.639617681503296 + 1.0 * 6.592794895172119
Epoch 160, val loss: 1.65988028049469
Epoch 170, training loss: 8.174997329711914 = 1.6052848100662231 + 1.0 * 6.569712162017822
Epoch 170, val loss: 1.630625605583191
Epoch 180, training loss: 8.115679740905762 = 1.566116213798523 + 1.0 * 6.549563884735107
Epoch 180, val loss: 1.5977026224136353
Epoch 190, training loss: 8.056112289428711 = 1.5229203701019287 + 1.0 * 6.533192157745361
Epoch 190, val loss: 1.5617939233779907
Epoch 200, training loss: 7.993432998657227 = 1.4757221937179565 + 1.0 * 6.5177106857299805
Epoch 200, val loss: 1.522894263267517
Epoch 210, training loss: 7.931347846984863 = 1.4259085655212402 + 1.0 * 6.505439281463623
Epoch 210, val loss: 1.4826184511184692
Epoch 220, training loss: 7.870129585266113 = 1.3751333951950073 + 1.0 * 6.494996070861816
Epoch 220, val loss: 1.4420735836029053
Epoch 230, training loss: 7.808469295501709 = 1.324282169342041 + 1.0 * 6.484187126159668
Epoch 230, val loss: 1.4017435312271118
Epoch 240, training loss: 7.749014854431152 = 1.2741684913635254 + 1.0 * 6.474846363067627
Epoch 240, val loss: 1.362449049949646
Epoch 250, training loss: 7.69050931930542 = 1.2256571054458618 + 1.0 * 6.464852333068848
Epoch 250, val loss: 1.3248683214187622
Epoch 260, training loss: 7.635041236877441 = 1.1785911321640015 + 1.0 * 6.45644998550415
Epoch 260, val loss: 1.2886919975280762
Epoch 270, training loss: 7.584138870239258 = 1.1334996223449707 + 1.0 * 6.450639247894287
Epoch 270, val loss: 1.2543338537216187
Epoch 280, training loss: 7.532283782958984 = 1.0909587144851685 + 1.0 * 6.4413251876831055
Epoch 280, val loss: 1.222419261932373
Epoch 290, training loss: 7.4866228103637695 = 1.0508882999420166 + 1.0 * 6.435734272003174
Epoch 290, val loss: 1.1928200721740723
Epoch 300, training loss: 7.4426045417785645 = 1.0134657621383667 + 1.0 * 6.429138660430908
Epoch 300, val loss: 1.1654716730117798
Epoch 310, training loss: 7.4010539054870605 = 0.9781505465507507 + 1.0 * 6.422903537750244
Epoch 310, val loss: 1.1401243209838867
Epoch 320, training loss: 7.364964962005615 = 0.9447202086448669 + 1.0 * 6.4202446937561035
Epoch 320, val loss: 1.116441011428833
Epoch 330, training loss: 7.327221393585205 = 0.9130659103393555 + 1.0 * 6.41415548324585
Epoch 330, val loss: 1.0943628549575806
Epoch 340, training loss: 7.291928291320801 = 0.8825527429580688 + 1.0 * 6.4093756675720215
Epoch 340, val loss: 1.0734713077545166
Epoch 350, training loss: 7.25771427154541 = 0.8528373837471008 + 1.0 * 6.404876708984375
Epoch 350, val loss: 1.0535106658935547
Epoch 360, training loss: 7.231044769287109 = 0.8238725066184998 + 1.0 * 6.407172203063965
Epoch 360, val loss: 1.0344499349594116
Epoch 370, training loss: 7.1942219734191895 = 0.7956545948982239 + 1.0 * 6.398567199707031
Epoch 370, val loss: 1.0162454843521118
Epoch 380, training loss: 7.161337852478027 = 0.7678936719894409 + 1.0 * 6.393444061279297
Epoch 380, val loss: 0.998672604560852
Epoch 390, training loss: 7.130118370056152 = 0.7403432726860046 + 1.0 * 6.389775276184082
Epoch 390, val loss: 0.9814508557319641
Epoch 400, training loss: 7.100078582763672 = 0.7130511403083801 + 1.0 * 6.387027263641357
Epoch 400, val loss: 0.964497447013855
Epoch 410, training loss: 7.070544719696045 = 0.6861534714698792 + 1.0 * 6.3843913078308105
Epoch 410, val loss: 0.9480106234550476
Epoch 420, training loss: 7.03957986831665 = 0.6594079732894897 + 1.0 * 6.380171775817871
Epoch 420, val loss: 0.9316684007644653
Epoch 430, training loss: 7.016540050506592 = 0.6328940391540527 + 1.0 * 6.383646011352539
Epoch 430, val loss: 0.9155322313308716
Epoch 440, training loss: 6.983913898468018 = 0.6068629622459412 + 1.0 * 6.377050876617432
Epoch 440, val loss: 0.8998743295669556
Epoch 450, training loss: 6.955973148345947 = 0.5811947584152222 + 1.0 * 6.3747782707214355
Epoch 450, val loss: 0.8845683932304382
Epoch 460, training loss: 6.926392555236816 = 0.5558626651763916 + 1.0 * 6.370529651641846
Epoch 460, val loss: 0.8696035742759705
Epoch 470, training loss: 6.899120330810547 = 0.5309123992919922 + 1.0 * 6.368207931518555
Epoch 470, val loss: 0.8551739454269409
Epoch 480, training loss: 6.8753838539123535 = 0.5063480138778687 + 1.0 * 6.369035720825195
Epoch 480, val loss: 0.8412056565284729
Epoch 490, training loss: 6.849677562713623 = 0.4822924733161926 + 1.0 * 6.367384910583496
Epoch 490, val loss: 0.8279136419296265
Epoch 500, training loss: 6.821999549865723 = 0.458856999874115 + 1.0 * 6.363142490386963
Epoch 500, val loss: 0.8152636885643005
Epoch 510, training loss: 6.7954559326171875 = 0.4359862506389618 + 1.0 * 6.359469890594482
Epoch 510, val loss: 0.8034511804580688
Epoch 520, training loss: 6.771860599517822 = 0.41361239552497864 + 1.0 * 6.358248233795166
Epoch 520, val loss: 0.7923371195793152
Epoch 530, training loss: 6.752945423126221 = 0.3918702304363251 + 1.0 * 6.361075401306152
Epoch 530, val loss: 0.7820581197738647
Epoch 540, training loss: 6.726071834564209 = 0.3709671199321747 + 1.0 * 6.355104923248291
Epoch 540, val loss: 0.7727916836738586
Epoch 550, training loss: 6.702176094055176 = 0.35073596239089966 + 1.0 * 6.351439952850342
Epoch 550, val loss: 0.7643641829490662
Epoch 560, training loss: 6.6897664070129395 = 0.33120620250701904 + 1.0 * 6.358560085296631
Epoch 560, val loss: 0.756772518157959
Epoch 570, training loss: 6.6623687744140625 = 0.3125157952308655 + 1.0 * 6.349853038787842
Epoch 570, val loss: 0.7502073645591736
Epoch 580, training loss: 6.642660140991211 = 0.29464173316955566 + 1.0 * 6.348018169403076
Epoch 580, val loss: 0.7444848418235779
Epoch 590, training loss: 6.631771564483643 = 0.27756640315055847 + 1.0 * 6.354205131530762
Epoch 590, val loss: 0.7395521402359009
Epoch 600, training loss: 6.607212066650391 = 0.2613372206687927 + 1.0 * 6.345874786376953
Epoch 600, val loss: 0.7354210615158081
Epoch 610, training loss: 6.590480327606201 = 0.24595363438129425 + 1.0 * 6.344526767730713
Epoch 610, val loss: 0.7320629358291626
Epoch 620, training loss: 6.572564601898193 = 0.23136106133460999 + 1.0 * 6.341203689575195
Epoch 620, val loss: 0.729394257068634
Epoch 630, training loss: 6.561859607696533 = 0.21751105785369873 + 1.0 * 6.344348430633545
Epoch 630, val loss: 0.7273961901664734
Epoch 640, training loss: 6.547677993774414 = 0.2044639140367508 + 1.0 * 6.34321403503418
Epoch 640, val loss: 0.7260255813598633
Epoch 650, training loss: 6.535799980163574 = 0.19223783910274506 + 1.0 * 6.343562126159668
Epoch 650, val loss: 0.7253100275993347
Epoch 660, training loss: 6.517001152038574 = 0.18074384331703186 + 1.0 * 6.336257457733154
Epoch 660, val loss: 0.7250557541847229
Epoch 670, training loss: 6.504425525665283 = 0.16996510326862335 + 1.0 * 6.334460258483887
Epoch 670, val loss: 0.725350022315979
Epoch 680, training loss: 6.49273157119751 = 0.15983906388282776 + 1.0 * 6.332892417907715
Epoch 680, val loss: 0.7261605262756348
Epoch 690, training loss: 6.489887237548828 = 0.1503625214099884 + 1.0 * 6.339524745941162
Epoch 690, val loss: 0.7274020910263062
Epoch 700, training loss: 6.4760050773620605 = 0.14154398441314697 + 1.0 * 6.334461212158203
Epoch 700, val loss: 0.7289567589759827
Epoch 710, training loss: 6.466654300689697 = 0.13332834839820862 + 1.0 * 6.3333258628845215
Epoch 710, val loss: 0.7308268547058105
Epoch 720, training loss: 6.452960968017578 = 0.1256788820028305 + 1.0 * 6.327281951904297
Epoch 720, val loss: 0.733153760433197
Epoch 730, training loss: 6.445765972137451 = 0.11852588504552841 + 1.0 * 6.327239990234375
Epoch 730, val loss: 0.7357252240180969
Epoch 740, training loss: 6.438382625579834 = 0.11186889559030533 + 1.0 * 6.326513767242432
Epoch 740, val loss: 0.7385475039482117
Epoch 750, training loss: 6.432956218719482 = 0.10567659139633179 + 1.0 * 6.327279567718506
Epoch 750, val loss: 0.741468608379364
Epoch 760, training loss: 6.423595905303955 = 0.09990433603525162 + 1.0 * 6.323691368103027
Epoch 760, val loss: 0.7446153163909912
Epoch 770, training loss: 6.42310905456543 = 0.09451667964458466 + 1.0 * 6.328592300415039
Epoch 770, val loss: 0.7481062412261963
Epoch 780, training loss: 6.413362503051758 = 0.08949141949415207 + 1.0 * 6.32387113571167
Epoch 780, val loss: 0.7516219615936279
Epoch 790, training loss: 6.409603595733643 = 0.08480080962181091 + 1.0 * 6.324802875518799
Epoch 790, val loss: 0.7553393244743347
Epoch 800, training loss: 6.400249481201172 = 0.08042047172784805 + 1.0 * 6.319828987121582
Epoch 800, val loss: 0.7591046094894409
Epoch 810, training loss: 6.398080825805664 = 0.0763353630900383 + 1.0 * 6.3217453956604
Epoch 810, val loss: 0.7630918622016907
Epoch 820, training loss: 6.391702651977539 = 0.07252035290002823 + 1.0 * 6.319182395935059
Epoch 820, val loss: 0.7671490907669067
Epoch 830, training loss: 6.384346961975098 = 0.06896355003118515 + 1.0 * 6.315383434295654
Epoch 830, val loss: 0.7712398767471313
Epoch 840, training loss: 6.379547595977783 = 0.06562688201665878 + 1.0 * 6.313920497894287
Epoch 840, val loss: 0.775510847568512
Epoch 850, training loss: 6.381291389465332 = 0.06249518319964409 + 1.0 * 6.318796157836914
Epoch 850, val loss: 0.7798584699630737
Epoch 860, training loss: 6.379223823547363 = 0.059573978185653687 + 1.0 * 6.319649696350098
Epoch 860, val loss: 0.7843209505081177
Epoch 870, training loss: 6.367848873138428 = 0.0568440817296505 + 1.0 * 6.311004638671875
Epoch 870, val loss: 0.7885953783988953
Epoch 880, training loss: 6.365457534790039 = 0.05428176000714302 + 1.0 * 6.31117582321167
Epoch 880, val loss: 0.7930306196212769
Epoch 890, training loss: 6.364801406860352 = 0.05186884477734566 + 1.0 * 6.31293249130249
Epoch 890, val loss: 0.7976418733596802
Epoch 900, training loss: 6.367252826690674 = 0.04960077628493309 + 1.0 * 6.317652225494385
Epoch 900, val loss: 0.8021032214164734
Epoch 910, training loss: 6.359957218170166 = 0.047485530376434326 + 1.0 * 6.312471866607666
Epoch 910, val loss: 0.8066509962081909
Epoch 920, training loss: 6.352432727813721 = 0.045490775257349014 + 1.0 * 6.306941986083984
Epoch 920, val loss: 0.8110224008560181
Epoch 930, training loss: 6.349174976348877 = 0.04361039772629738 + 1.0 * 6.3055644035339355
Epoch 930, val loss: 0.8156516551971436
Epoch 940, training loss: 6.3476338386535645 = 0.041829999536275864 + 1.0 * 6.3058037757873535
Epoch 940, val loss: 0.820332944393158
Epoch 950, training loss: 6.349968433380127 = 0.04015028104186058 + 1.0 * 6.309818267822266
Epoch 950, val loss: 0.8250482678413391
Epoch 960, training loss: 6.3473052978515625 = 0.03857026621699333 + 1.0 * 6.308734893798828
Epoch 960, val loss: 0.8295877575874329
Epoch 970, training loss: 6.342123508453369 = 0.03708391264081001 + 1.0 * 6.305039405822754
Epoch 970, val loss: 0.8339701890945435
Epoch 980, training loss: 6.338583469390869 = 0.03568192571401596 + 1.0 * 6.302901744842529
Epoch 980, val loss: 0.8384454250335693
Epoch 990, training loss: 6.340634822845459 = 0.034354228526353836 + 1.0 * 6.306280612945557
Epoch 990, val loss: 0.8430671691894531
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8386926726410122
The final CL Acc:0.80370, 0.01210, The final GNN Acc:0.83992, 0.00252
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9444])
updated graph: torch.Size([2, 10506])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.543405532836914 = 1.9465365409851074 + 1.0 * 8.596868515014648
Epoch 0, val loss: 1.9487203359603882
Epoch 10, training loss: 10.533586502075195 = 1.9368815422058105 + 1.0 * 8.596705436706543
Epoch 10, val loss: 1.9392772912979126
Epoch 20, training loss: 10.520524024963379 = 1.925171971321106 + 1.0 * 8.595352172851562
Epoch 20, val loss: 1.927260160446167
Epoch 30, training loss: 10.492664337158203 = 1.9091172218322754 + 1.0 * 8.583547592163086
Epoch 30, val loss: 1.91026771068573
Epoch 40, training loss: 10.390776634216309 = 1.8875054121017456 + 1.0 * 8.503271102905273
Epoch 40, val loss: 1.8878636360168457
Epoch 50, training loss: 10.101025581359863 = 1.8626823425292969 + 1.0 * 8.238343238830566
Epoch 50, val loss: 1.8638259172439575
Epoch 60, training loss: 9.868240356445312 = 1.8427330255508423 + 1.0 * 8.025506973266602
Epoch 60, val loss: 1.8460299968719482
Epoch 70, training loss: 9.2615327835083 = 1.8282434940338135 + 1.0 * 7.433289051055908
Epoch 70, val loss: 1.833356261253357
Epoch 80, training loss: 8.92455005645752 = 1.8159695863723755 + 1.0 * 7.108580589294434
Epoch 80, val loss: 1.8217390775680542
Epoch 90, training loss: 8.761897087097168 = 1.801164984703064 + 1.0 * 6.960732460021973
Epoch 90, val loss: 1.8075271844863892
Epoch 100, training loss: 8.65212345123291 = 1.7856937646865845 + 1.0 * 6.866429328918457
Epoch 100, val loss: 1.7927985191345215
Epoch 110, training loss: 8.573399543762207 = 1.769795298576355 + 1.0 * 6.803604602813721
Epoch 110, val loss: 1.7780871391296387
Epoch 120, training loss: 8.506340980529785 = 1.7530806064605713 + 1.0 * 6.753260612487793
Epoch 120, val loss: 1.7630021572113037
Epoch 130, training loss: 8.445581436157227 = 1.7350417375564575 + 1.0 * 6.7105393409729
Epoch 130, val loss: 1.746893048286438
Epoch 140, training loss: 8.389537811279297 = 1.7149803638458252 + 1.0 * 6.674557685852051
Epoch 140, val loss: 1.7292276620864868
Epoch 150, training loss: 8.333525657653809 = 1.692443609237671 + 1.0 * 6.641082286834717
Epoch 150, val loss: 1.7095614671707153
Epoch 160, training loss: 8.28067398071289 = 1.6667654514312744 + 1.0 * 6.613908767700195
Epoch 160, val loss: 1.6873353719711304
Epoch 170, training loss: 8.23690414428711 = 1.6375560760498047 + 1.0 * 6.599348068237305
Epoch 170, val loss: 1.6623194217681885
Epoch 180, training loss: 8.180563926696777 = 1.6048468351364136 + 1.0 * 6.575717449188232
Epoch 180, val loss: 1.6344177722930908
Epoch 190, training loss: 8.127589225769043 = 1.568116307258606 + 1.0 * 6.559473037719727
Epoch 190, val loss: 1.6033002138137817
Epoch 200, training loss: 8.072704315185547 = 1.5270681381225586 + 1.0 * 6.54563570022583
Epoch 200, val loss: 1.568690538406372
Epoch 210, training loss: 8.017141342163086 = 1.481694221496582 + 1.0 * 6.535447597503662
Epoch 210, val loss: 1.5306533575057983
Epoch 220, training loss: 7.9571027755737305 = 1.4332314729690552 + 1.0 * 6.523871421813965
Epoch 220, val loss: 1.490463376045227
Epoch 230, training loss: 7.898529052734375 = 1.382347822189331 + 1.0 * 6.516181468963623
Epoch 230, val loss: 1.4487078189849854
Epoch 240, training loss: 7.835768699645996 = 1.3292957544326782 + 1.0 * 6.506473064422607
Epoch 240, val loss: 1.4056392908096313
Epoch 250, training loss: 7.779686450958252 = 1.2751156091690063 + 1.0 * 6.504570960998535
Epoch 250, val loss: 1.3622199296951294
Epoch 260, training loss: 7.715705871582031 = 1.2213091850280762 + 1.0 * 6.494396686553955
Epoch 260, val loss: 1.3195642232894897
Epoch 270, training loss: 7.653431415557861 = 1.167905330657959 + 1.0 * 6.485526084899902
Epoch 270, val loss: 1.2778185606002808
Epoch 280, training loss: 7.595715522766113 = 1.1151974201202393 + 1.0 * 6.480517864227295
Epoch 280, val loss: 1.2372525930404663
Epoch 290, training loss: 7.541021823883057 = 1.0639675855636597 + 1.0 * 6.477054119110107
Epoch 290, val loss: 1.1986417770385742
Epoch 300, training loss: 7.484079837799072 = 1.0147780179977417 + 1.0 * 6.469301700592041
Epoch 300, val loss: 1.162329912185669
Epoch 310, training loss: 7.430186748504639 = 0.967494010925293 + 1.0 * 6.462692737579346
Epoch 310, val loss: 1.1283445358276367
Epoch 320, training loss: 7.399541854858398 = 0.9222871661186218 + 1.0 * 6.477254867553711
Epoch 320, val loss: 1.0969195365905762
Epoch 330, training loss: 7.335455894470215 = 0.8805282115936279 + 1.0 * 6.454927921295166
Epoch 330, val loss: 1.0686109066009521
Epoch 340, training loss: 7.289746284484863 = 0.8414586782455444 + 1.0 * 6.448287487030029
Epoch 340, val loss: 1.043342113494873
Epoch 350, training loss: 7.249073505401611 = 0.8046785593032837 + 1.0 * 6.444395065307617
Epoch 350, val loss: 1.0205873250961304
Epoch 360, training loss: 7.209202289581299 = 0.7700318098068237 + 1.0 * 6.4391703605651855
Epoch 360, val loss: 1.0002282857894897
Epoch 370, training loss: 7.174908638000488 = 0.7373123168945312 + 1.0 * 6.437596321105957
Epoch 370, val loss: 0.9821183085441589
Epoch 380, training loss: 7.151079177856445 = 0.7067175507545471 + 1.0 * 6.444361686706543
Epoch 380, val loss: 0.9658993482589722
Epoch 390, training loss: 7.108694076538086 = 0.6781137585639954 + 1.0 * 6.430580139160156
Epoch 390, val loss: 0.9520413875579834
Epoch 400, training loss: 7.075237274169922 = 0.6509697437286377 + 1.0 * 6.424267768859863
Epoch 400, val loss: 0.9396786093711853
Epoch 410, training loss: 7.0456695556640625 = 0.6249763369560242 + 1.0 * 6.420693397521973
Epoch 410, val loss: 0.9286512732505798
Epoch 420, training loss: 7.019561767578125 = 0.6000826358795166 + 1.0 * 6.419478893280029
Epoch 420, val loss: 0.9188210964202881
Epoch 430, training loss: 6.991577625274658 = 0.5763148069381714 + 1.0 * 6.415262699127197
Epoch 430, val loss: 0.9103398323059082
Epoch 440, training loss: 6.9645256996154785 = 0.5534148812294006 + 1.0 * 6.411110877990723
Epoch 440, val loss: 0.9029396176338196
Epoch 450, training loss: 6.940642833709717 = 0.5311949849128723 + 1.0 * 6.40944766998291
Epoch 450, val loss: 0.8963232636451721
Epoch 460, training loss: 6.922791481018066 = 0.5096247792243958 + 1.0 * 6.413166522979736
Epoch 460, val loss: 0.8904027938842773
Epoch 470, training loss: 6.892428874969482 = 0.48872822523117065 + 1.0 * 6.403700828552246
Epoch 470, val loss: 0.8853451013565063
Epoch 480, training loss: 6.867745399475098 = 0.4683091342449188 + 1.0 * 6.3994364738464355
Epoch 480, val loss: 0.8807499408721924
Epoch 490, training loss: 6.853850364685059 = 0.44828128814697266 + 1.0 * 6.405569076538086
Epoch 490, val loss: 0.8765719532966614
Epoch 500, training loss: 6.825753688812256 = 0.42871028184890747 + 1.0 * 6.397043228149414
Epoch 500, val loss: 0.8728164434432983
Epoch 510, training loss: 6.805639266967773 = 0.4095157980918884 + 1.0 * 6.39612340927124
Epoch 510, val loss: 0.8694996237754822
Epoch 520, training loss: 6.780310153961182 = 0.3907136619091034 + 1.0 * 6.389596462249756
Epoch 520, val loss: 0.8664215207099915
Epoch 530, training loss: 6.760889530181885 = 0.37232717871665955 + 1.0 * 6.388562202453613
Epoch 530, val loss: 0.8637473583221436
Epoch 540, training loss: 6.741339683532715 = 0.3543394207954407 + 1.0 * 6.38700008392334
Epoch 540, val loss: 0.8613036274909973
Epoch 550, training loss: 6.720678329467773 = 0.3367784023284912 + 1.0 * 6.383900165557861
Epoch 550, val loss: 0.8592424392700195
Epoch 560, training loss: 6.7031121253967285 = 0.31967461109161377 + 1.0 * 6.383437633514404
Epoch 560, val loss: 0.8573983311653137
Epoch 570, training loss: 6.686086177825928 = 0.3031154274940491 + 1.0 * 6.382970809936523
Epoch 570, val loss: 0.8559213280677795
Epoch 580, training loss: 6.666325092315674 = 0.2871706783771515 + 1.0 * 6.379154205322266
Epoch 580, val loss: 0.8547954559326172
Epoch 590, training loss: 6.647371292114258 = 0.2718085050582886 + 1.0 * 6.37556266784668
Epoch 590, val loss: 0.8540588617324829
Epoch 600, training loss: 6.6355061531066895 = 0.25703126192092896 + 1.0 * 6.378474712371826
Epoch 600, val loss: 0.8534919619560242
Epoch 610, training loss: 6.626319885253906 = 0.24291186034679413 + 1.0 * 6.383408069610596
Epoch 610, val loss: 0.853215754032135
Epoch 620, training loss: 6.602659225463867 = 0.22953647375106812 + 1.0 * 6.373122692108154
Epoch 620, val loss: 0.8534872531890869
Epoch 630, training loss: 6.585594177246094 = 0.21680811047554016 + 1.0 * 6.368785858154297
Epoch 630, val loss: 0.8539699912071228
Epoch 640, training loss: 6.571684837341309 = 0.20468030869960785 + 1.0 * 6.36700439453125
Epoch 640, val loss: 0.8547650575637817
Epoch 650, training loss: 6.573153972625732 = 0.1931498646736145 + 1.0 * 6.380003929138184
Epoch 650, val loss: 0.8558651208877563
Epoch 660, training loss: 6.549107074737549 = 0.18230514228343964 + 1.0 * 6.366801738739014
Epoch 660, val loss: 0.8573209047317505
Epoch 670, training loss: 6.538573265075684 = 0.17206549644470215 + 1.0 * 6.3665080070495605
Epoch 670, val loss: 0.8591277599334717
Epoch 680, training loss: 6.523318767547607 = 0.1624252200126648 + 1.0 * 6.360893726348877
Epoch 680, val loss: 0.8612738847732544
Epoch 690, training loss: 6.514599323272705 = 0.15335126221179962 + 1.0 * 6.361248016357422
Epoch 690, val loss: 0.8637889623641968
Epoch 700, training loss: 6.505333423614502 = 0.14480456709861755 + 1.0 * 6.360528945922852
Epoch 700, val loss: 0.8666219115257263
Epoch 710, training loss: 6.495484828948975 = 0.13678137958049774 + 1.0 * 6.35870361328125
Epoch 710, val loss: 0.8698291182518005
Epoch 720, training loss: 6.487948417663574 = 0.12925101816654205 + 1.0 * 6.358697414398193
Epoch 720, val loss: 0.8733494877815247
Epoch 730, training loss: 6.479066848754883 = 0.12219046801328659 + 1.0 * 6.356876373291016
Epoch 730, val loss: 0.8771155476570129
Epoch 740, training loss: 6.468995094299316 = 0.11558478325605392 + 1.0 * 6.353410243988037
Epoch 740, val loss: 0.8811151385307312
Epoch 750, training loss: 6.4642863273620605 = 0.1093824952840805 + 1.0 * 6.354903697967529
Epoch 750, val loss: 0.8855742812156677
Epoch 760, training loss: 6.45835018157959 = 0.10358106344938278 + 1.0 * 6.354769229888916
Epoch 760, val loss: 0.8899633884429932
Epoch 770, training loss: 6.44771671295166 = 0.09816643595695496 + 1.0 * 6.349550247192383
Epoch 770, val loss: 0.8946414589881897
Epoch 780, training loss: 6.4406280517578125 = 0.09309940040111542 + 1.0 * 6.347528457641602
Epoch 780, val loss: 0.8996707201004028
Epoch 790, training loss: 6.433166980743408 = 0.08834023773670197 + 1.0 * 6.344826698303223
Epoch 790, val loss: 0.9047700762748718
Epoch 800, training loss: 6.428417682647705 = 0.08385675400495529 + 1.0 * 6.3445611000061035
Epoch 800, val loss: 0.9100154042243958
Epoch 810, training loss: 6.430964469909668 = 0.07964432239532471 + 1.0 * 6.351320266723633
Epoch 810, val loss: 0.9154030680656433
Epoch 820, training loss: 6.424099922180176 = 0.07570098340511322 + 1.0 * 6.3483991622924805
Epoch 820, val loss: 0.9207131266593933
Epoch 830, training loss: 6.413171768188477 = 0.0720137283205986 + 1.0 * 6.341157913208008
Epoch 830, val loss: 0.9262591600418091
Epoch 840, training loss: 6.408329486846924 = 0.06855325400829315 + 1.0 * 6.339776039123535
Epoch 840, val loss: 0.9318820834159851
Epoch 850, training loss: 6.40655517578125 = 0.06529784202575684 + 1.0 * 6.341257095336914
Epoch 850, val loss: 0.937474250793457
Epoch 860, training loss: 6.399409770965576 = 0.06223180890083313 + 1.0 * 6.337177753448486
Epoch 860, val loss: 0.943177342414856
Epoch 870, training loss: 6.3988423347473145 = 0.059350285679101944 + 1.0 * 6.339491844177246
Epoch 870, val loss: 0.9489385485649109
Epoch 880, training loss: 6.4009857177734375 = 0.056641463190317154 + 1.0 * 6.344344139099121
Epoch 880, val loss: 0.9545192122459412
Epoch 890, training loss: 6.389017581939697 = 0.05409499257802963 + 1.0 * 6.334922790527344
Epoch 890, val loss: 0.9603291749954224
Epoch 900, training loss: 6.385045051574707 = 0.05171138048171997 + 1.0 * 6.333333492279053
Epoch 900, val loss: 0.9661189317703247
Epoch 910, training loss: 6.381144046783447 = 0.04946054145693779 + 1.0 * 6.33168363571167
Epoch 910, val loss: 0.9718136191368103
Epoch 920, training loss: 6.378475666046143 = 0.04733183979988098 + 1.0 * 6.331143856048584
Epoch 920, val loss: 0.9774763584136963
Epoch 930, training loss: 6.3810715675354 = 0.045320700854063034 + 1.0 * 6.335751056671143
Epoch 930, val loss: 0.9832207560539246
Epoch 940, training loss: 6.374143123626709 = 0.04342728480696678 + 1.0 * 6.330715656280518
Epoch 940, val loss: 0.9886957406997681
Epoch 950, training loss: 6.36994743347168 = 0.04164665937423706 + 1.0 * 6.328300952911377
Epoch 950, val loss: 0.9944279193878174
Epoch 960, training loss: 6.367977142333984 = 0.039964642375707626 + 1.0 * 6.328012466430664
Epoch 960, val loss: 1.0000134706497192
Epoch 970, training loss: 6.374001979827881 = 0.03837352246046066 + 1.0 * 6.335628509521484
Epoch 970, val loss: 1.0054141283035278
Epoch 980, training loss: 6.364094257354736 = 0.036866698414087296 + 1.0 * 6.327227592468262
Epoch 980, val loss: 1.0109933614730835
Epoch 990, training loss: 6.360912322998047 = 0.03545481711626053 + 1.0 * 6.325457572937012
Epoch 990, val loss: 1.0164633989334106
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 10.552452087402344 = 1.9556174278259277 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9652336835861206
Epoch 10, training loss: 10.542442321777344 = 1.945871353149414 + 1.0 * 8.59657096862793
Epoch 10, val loss: 1.955613613128662
Epoch 20, training loss: 10.528192520141602 = 1.9337385892868042 + 1.0 * 8.594453811645508
Epoch 20, val loss: 1.9430193901062012
Epoch 30, training loss: 10.4928560256958 = 1.9164197444915771 + 1.0 * 8.576436042785645
Epoch 30, val loss: 1.9245786666870117
Epoch 40, training loss: 10.343832015991211 = 1.8933221101760864 + 1.0 * 8.450510025024414
Epoch 40, val loss: 1.90081787109375
Epoch 50, training loss: 9.865221977233887 = 1.8706363439559937 + 1.0 * 7.994585990905762
Epoch 50, val loss: 1.8782312870025635
Epoch 60, training loss: 9.360278129577637 = 1.8533051013946533 + 1.0 * 7.506972789764404
Epoch 60, val loss: 1.8612616062164307
Epoch 70, training loss: 8.999260902404785 = 1.8387339115142822 + 1.0 * 7.160526752471924
Epoch 70, val loss: 1.847194790840149
Epoch 80, training loss: 8.804912567138672 = 1.8229846954345703 + 1.0 * 6.981927871704102
Epoch 80, val loss: 1.831862449645996
Epoch 90, training loss: 8.660797119140625 = 1.806273102760315 + 1.0 * 6.854523658752441
Epoch 90, val loss: 1.8160358667373657
Epoch 100, training loss: 8.567300796508789 = 1.7902439832687378 + 1.0 * 6.777056694030762
Epoch 100, val loss: 1.8014341592788696
Epoch 110, training loss: 8.49508285522461 = 1.7749148607254028 + 1.0 * 6.720167636871338
Epoch 110, val loss: 1.7875877618789673
Epoch 120, training loss: 8.434829711914062 = 1.7593872547149658 + 1.0 * 6.675442218780518
Epoch 120, val loss: 1.773638129234314
Epoch 130, training loss: 8.383842468261719 = 1.7429454326629639 + 1.0 * 6.640897274017334
Epoch 130, val loss: 1.7589361667633057
Epoch 140, training loss: 8.33788776397705 = 1.724832534790039 + 1.0 * 6.613055229187012
Epoch 140, val loss: 1.7432584762573242
Epoch 150, training loss: 8.292484283447266 = 1.704542636871338 + 1.0 * 6.587941646575928
Epoch 150, val loss: 1.7259676456451416
Epoch 160, training loss: 8.248828887939453 = 1.6814864873886108 + 1.0 * 6.567342281341553
Epoch 160, val loss: 1.7064558267593384
Epoch 170, training loss: 8.204996109008789 = 1.6549361944198608 + 1.0 * 6.550059795379639
Epoch 170, val loss: 1.6840113401412964
Epoch 180, training loss: 8.160863876342773 = 1.6244943141937256 + 1.0 * 6.536369323730469
Epoch 180, val loss: 1.6583360433578491
Epoch 190, training loss: 8.113614082336426 = 1.5900077819824219 + 1.0 * 6.523606300354004
Epoch 190, val loss: 1.6291624307632446
Epoch 200, training loss: 8.064448356628418 = 1.5509124994277954 + 1.0 * 6.513535976409912
Epoch 200, val loss: 1.596072793006897
Epoch 210, training loss: 8.009209632873535 = 1.507480502128601 + 1.0 * 6.501729488372803
Epoch 210, val loss: 1.5593085289001465
Epoch 220, training loss: 7.95374870300293 = 1.4598405361175537 + 1.0 * 6.493908405303955
Epoch 220, val loss: 1.5190950632095337
Epoch 230, training loss: 7.895185947418213 = 1.409094214439392 + 1.0 * 6.486091613769531
Epoch 230, val loss: 1.476739764213562
Epoch 240, training loss: 7.833742141723633 = 1.3565165996551514 + 1.0 * 6.477225303649902
Epoch 240, val loss: 1.433266043663025
Epoch 250, training loss: 7.772052764892578 = 1.3027684688568115 + 1.0 * 6.469284534454346
Epoch 250, val loss: 1.3893780708312988
Epoch 260, training loss: 7.716822147369385 = 1.2495903968811035 + 1.0 * 6.467231750488281
Epoch 260, val loss: 1.3469706773757935
Epoch 270, training loss: 7.65562105178833 = 1.1984491348266602 + 1.0 * 6.45717191696167
Epoch 270, val loss: 1.3069325685501099
Epoch 280, training loss: 7.602597236633301 = 1.1493035554885864 + 1.0 * 6.453293800354004
Epoch 280, val loss: 1.2693108320236206
Epoch 290, training loss: 7.551229476928711 = 1.1027870178222656 + 1.0 * 6.448442459106445
Epoch 290, val loss: 1.2347532510757446
Epoch 300, training loss: 7.500632286071777 = 1.0588496923446655 + 1.0 * 6.441782474517822
Epoch 300, val loss: 1.2029783725738525
Epoch 310, training loss: 7.456003189086914 = 1.0169705152511597 + 1.0 * 6.439032554626465
Epoch 310, val loss: 1.1734684705734253
Epoch 320, training loss: 7.409374237060547 = 0.977520763874054 + 1.0 * 6.431853294372559
Epoch 320, val loss: 1.1463168859481812
Epoch 330, training loss: 7.370500087738037 = 0.9402859807014465 + 1.0 * 6.430213928222656
Epoch 330, val loss: 1.1212503910064697
Epoch 340, training loss: 7.3277506828308105 = 0.9043965935707092 + 1.0 * 6.423354148864746
Epoch 340, val loss: 1.0975291728973389
Epoch 350, training loss: 7.289188861846924 = 0.869442880153656 + 1.0 * 6.419745922088623
Epoch 350, val loss: 1.074628233909607
Epoch 360, training loss: 7.252902984619141 = 0.8351913094520569 + 1.0 * 6.4177117347717285
Epoch 360, val loss: 1.0525586605072021
Epoch 370, training loss: 7.2155280113220215 = 0.8020772337913513 + 1.0 * 6.413450717926025
Epoch 370, val loss: 1.0315027236938477
Epoch 380, training loss: 7.182253360748291 = 0.7701069712638855 + 1.0 * 6.41214656829834
Epoch 380, val loss: 1.0115388631820679
Epoch 390, training loss: 7.145994663238525 = 0.7390032410621643 + 1.0 * 6.406991481781006
Epoch 390, val loss: 0.9925921559333801
Epoch 400, training loss: 7.1140642166137695 = 0.708810567855835 + 1.0 * 6.405253887176514
Epoch 400, val loss: 0.9746900796890259
Epoch 410, training loss: 7.080838203430176 = 0.6798226833343506 + 1.0 * 6.401015281677246
Epoch 410, val loss: 0.9580662250518799
Epoch 420, training loss: 7.051135540008545 = 0.6517520546913147 + 1.0 * 6.399383544921875
Epoch 420, val loss: 0.9427129030227661
Epoch 430, training loss: 7.02034330368042 = 0.6245607137680054 + 1.0 * 6.395782470703125
Epoch 430, val loss: 0.9285516738891602
Epoch 440, training loss: 6.991949081420898 = 0.5982277989387512 + 1.0 * 6.393721103668213
Epoch 440, val loss: 0.91562420129776
Epoch 450, training loss: 6.9638752937316895 = 0.5727102160453796 + 1.0 * 6.391165256500244
Epoch 450, val loss: 0.9038720726966858
Epoch 460, training loss: 6.938361167907715 = 0.5480082631111145 + 1.0 * 6.390352725982666
Epoch 460, val loss: 0.8933150172233582
Epoch 470, training loss: 6.911209583282471 = 0.5239687561988831 + 1.0 * 6.387240886688232
Epoch 470, val loss: 0.8839341998100281
Epoch 480, training loss: 6.885998725891113 = 0.5005761981010437 + 1.0 * 6.385422706604004
Epoch 480, val loss: 0.8756900429725647
Epoch 490, training loss: 6.861652851104736 = 0.4778062105178833 + 1.0 * 6.383846759796143
Epoch 490, val loss: 0.8685351610183716
Epoch 500, training loss: 6.839380264282227 = 0.45569664239883423 + 1.0 * 6.383683681488037
Epoch 500, val loss: 0.8625070452690125
Epoch 510, training loss: 6.81484317779541 = 0.4342872202396393 + 1.0 * 6.380556106567383
Epoch 510, val loss: 0.8576464653015137
Epoch 520, training loss: 6.790347099304199 = 0.4136867821216583 + 1.0 * 6.376660346984863
Epoch 520, val loss: 0.8539548516273499
Epoch 530, training loss: 6.771191120147705 = 0.39395400881767273 + 1.0 * 6.377237319946289
Epoch 530, val loss: 0.8514142036437988
Epoch 540, training loss: 6.748237609863281 = 0.375153511762619 + 1.0 * 6.37308406829834
Epoch 540, val loss: 0.8499306440353394
Epoch 550, training loss: 6.734766960144043 = 0.3573268949985504 + 1.0 * 6.377439975738525
Epoch 550, val loss: 0.8494783043861389
Epoch 560, training loss: 6.710835933685303 = 0.3404728174209595 + 1.0 * 6.370363235473633
Epoch 560, val loss: 0.8499987721443176
Epoch 570, training loss: 6.691384315490723 = 0.32454439997673035 + 1.0 * 6.36683988571167
Epoch 570, val loss: 0.8514374494552612
Epoch 580, training loss: 6.676855564117432 = 0.3093896210193634 + 1.0 * 6.367465972900391
Epoch 580, val loss: 0.8535827994346619
Epoch 590, training loss: 6.6587677001953125 = 0.29499194025993347 + 1.0 * 6.363775730133057
Epoch 590, val loss: 0.8562618494033813
Epoch 600, training loss: 6.644502639770508 = 0.2812478542327881 + 1.0 * 6.363255023956299
Epoch 600, val loss: 0.8595940470695496
Epoch 610, training loss: 6.62809419631958 = 0.2680269479751587 + 1.0 * 6.360067367553711
Epoch 610, val loss: 0.8632594347000122
Epoch 620, training loss: 6.6152567863464355 = 0.2552318871021271 + 1.0 * 6.360024929046631
Epoch 620, val loss: 0.8671374917030334
Epoch 630, training loss: 6.606266498565674 = 0.2428532987833023 + 1.0 * 6.363413333892822
Epoch 630, val loss: 0.8713370561599731
Epoch 640, training loss: 6.587655544281006 = 0.2308253049850464 + 1.0 * 6.35683012008667
Epoch 640, val loss: 0.8757208585739136
Epoch 650, training loss: 6.572483539581299 = 0.2191130667924881 + 1.0 * 6.353370666503906
Epoch 650, val loss: 0.880271315574646
Epoch 660, training loss: 6.561862468719482 = 0.20768383145332336 + 1.0 * 6.354178428649902
Epoch 660, val loss: 0.885158121585846
Epoch 670, training loss: 6.551156997680664 = 0.19658422470092773 + 1.0 * 6.354572772979736
Epoch 670, val loss: 0.8899135589599609
Epoch 680, training loss: 6.5370635986328125 = 0.18593591451644897 + 1.0 * 6.351127624511719
Epoch 680, val loss: 0.8951031565666199
Epoch 690, training loss: 6.525514125823975 = 0.17570731043815613 + 1.0 * 6.349806785583496
Epoch 690, val loss: 0.9005833268165588
Epoch 700, training loss: 6.515902519226074 = 0.16590145230293274 + 1.0 * 6.350000858306885
Epoch 700, val loss: 0.9061222672462463
Epoch 710, training loss: 6.510119915008545 = 0.1565757691860199 + 1.0 * 6.353544235229492
Epoch 710, val loss: 0.9118298292160034
Epoch 720, training loss: 6.49701452255249 = 0.14779335260391235 + 1.0 * 6.349221229553223
Epoch 720, val loss: 0.9180164337158203
Epoch 730, training loss: 6.484323501586914 = 0.13950276374816895 + 1.0 * 6.344820976257324
Epoch 730, val loss: 0.9245420694351196
Epoch 740, training loss: 6.473911762237549 = 0.13166199624538422 + 1.0 * 6.342249870300293
Epoch 740, val loss: 0.9313264489173889
Epoch 750, training loss: 6.477284908294678 = 0.1242571547627449 + 1.0 * 6.353027820587158
Epoch 750, val loss: 0.9382752180099487
Epoch 760, training loss: 6.4596757888793945 = 0.11733267456293106 + 1.0 * 6.342343330383301
Epoch 760, val loss: 0.9455703496932983
Epoch 770, training loss: 6.449390888214111 = 0.11081893742084503 + 1.0 * 6.338572025299072
Epoch 770, val loss: 0.9531008005142212
Epoch 780, training loss: 6.449355125427246 = 0.1047094464302063 + 1.0 * 6.3446455001831055
Epoch 780, val loss: 0.9606817364692688
Epoch 790, training loss: 6.442076683044434 = 0.09900625795125961 + 1.0 * 6.3430705070495605
Epoch 790, val loss: 0.9684443473815918
Epoch 800, training loss: 6.4293060302734375 = 0.09368621557950974 + 1.0 * 6.335619926452637
Epoch 800, val loss: 0.9762818217277527
Epoch 810, training loss: 6.423327922821045 = 0.08873144537210464 + 1.0 * 6.334596633911133
Epoch 810, val loss: 0.9843116998672485
Epoch 820, training loss: 6.420144557952881 = 0.08410320430994034 + 1.0 * 6.336041450500488
Epoch 820, val loss: 0.9923999905586243
Epoch 830, training loss: 6.4157586097717285 = 0.07978818565607071 + 1.0 * 6.335970401763916
Epoch 830, val loss: 1.000409483909607
Epoch 840, training loss: 6.409931182861328 = 0.07577205449342728 + 1.0 * 6.334158897399902
Epoch 840, val loss: 1.0085257291793823
Epoch 850, training loss: 6.406107425689697 = 0.07203511148691177 + 1.0 * 6.334072113037109
Epoch 850, val loss: 1.0165716409683228
Epoch 860, training loss: 6.398566246032715 = 0.06854601949453354 + 1.0 * 6.330020427703857
Epoch 860, val loss: 1.02469801902771
Epoch 870, training loss: 6.393238067626953 = 0.0652875304222107 + 1.0 * 6.327950477600098
Epoch 870, val loss: 1.0328295230865479
Epoch 880, training loss: 6.399129867553711 = 0.062230516225099564 + 1.0 * 6.336899280548096
Epoch 880, val loss: 1.0408310890197754
Epoch 890, training loss: 6.389331817626953 = 0.05937620624899864 + 1.0 * 6.329955577850342
Epoch 890, val loss: 1.048784613609314
Epoch 900, training loss: 6.383637428283691 = 0.05671215429902077 + 1.0 * 6.326925277709961
Epoch 900, val loss: 1.056832194328308
Epoch 910, training loss: 6.382342338562012 = 0.054203230887651443 + 1.0 * 6.328139305114746
Epoch 910, val loss: 1.0646480321884155
Epoch 920, training loss: 6.376184940338135 = 0.05185296759009361 + 1.0 * 6.324331760406494
Epoch 920, val loss: 1.0724635124206543
Epoch 930, training loss: 6.373939514160156 = 0.04964141547679901 + 1.0 * 6.324297904968262
Epoch 930, val loss: 1.0802459716796875
Epoch 940, training loss: 6.374537467956543 = 0.04756150394678116 + 1.0 * 6.3269758224487305
Epoch 940, val loss: 1.0879356861114502
Epoch 950, training loss: 6.368897914886475 = 0.04561096057295799 + 1.0 * 6.323287010192871
Epoch 950, val loss: 1.0955662727355957
Epoch 960, training loss: 6.3631439208984375 = 0.043769191950559616 + 1.0 * 6.3193745613098145
Epoch 960, val loss: 1.1031434535980225
Epoch 970, training loss: 6.361290454864502 = 0.04203297197818756 + 1.0 * 6.3192572593688965
Epoch 970, val loss: 1.1106826066970825
Epoch 980, training loss: 6.36430025100708 = 0.0403948538005352 + 1.0 * 6.3239054679870605
Epoch 980, val loss: 1.1180851459503174
Epoch 990, training loss: 6.3623948097229 = 0.03884713351726532 + 1.0 * 6.323547840118408
Epoch 990, val loss: 1.1253467798233032
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 10.549848556518555 = 1.9529852867126465 + 1.0 * 8.59686279296875
Epoch 0, val loss: 1.9485609531402588
Epoch 10, training loss: 10.53911304473877 = 1.9424229860305786 + 1.0 * 8.59669017791748
Epoch 10, val loss: 1.9380879402160645
Epoch 20, training loss: 10.524796485900879 = 1.929490089416504 + 1.0 * 8.595306396484375
Epoch 20, val loss: 1.9252787828445435
Epoch 30, training loss: 10.494956016540527 = 1.9117668867111206 + 1.0 * 8.583189010620117
Epoch 30, val loss: 1.9080297946929932
Epoch 40, training loss: 10.387104034423828 = 1.8880316019058228 + 1.0 * 8.499072074890137
Epoch 40, val loss: 1.8859344720840454
Epoch 50, training loss: 10.06637191772461 = 1.8612661361694336 + 1.0 * 8.205105781555176
Epoch 50, val loss: 1.8620606660842896
Epoch 60, training loss: 9.789261817932129 = 1.8390568494796753 + 1.0 * 7.950205326080322
Epoch 60, val loss: 1.8421707153320312
Epoch 70, training loss: 9.322272300720215 = 1.824425220489502 + 1.0 * 7.497847080230713
Epoch 70, val loss: 1.829037070274353
Epoch 80, training loss: 8.960137367248535 = 1.8156074285507202 + 1.0 * 7.144529819488525
Epoch 80, val loss: 1.8214753866195679
Epoch 90, training loss: 8.793733596801758 = 1.8006548881530762 + 1.0 * 6.993078708648682
Epoch 90, val loss: 1.808891773223877
Epoch 100, training loss: 8.667540550231934 = 1.7841652631759644 + 1.0 * 6.88337516784668
Epoch 100, val loss: 1.795660138130188
Epoch 110, training loss: 8.575638771057129 = 1.769300103187561 + 1.0 * 6.806338787078857
Epoch 110, val loss: 1.7833340167999268
Epoch 120, training loss: 8.505202293395996 = 1.754494309425354 + 1.0 * 6.750708103179932
Epoch 120, val loss: 1.7703137397766113
Epoch 130, training loss: 8.443095207214355 = 1.7380069494247437 + 1.0 * 6.7050886154174805
Epoch 130, val loss: 1.755821704864502
Epoch 140, training loss: 8.394807815551758 = 1.7192127704620361 + 1.0 * 6.675594806671143
Epoch 140, val loss: 1.7396119832992554
Epoch 150, training loss: 8.335193634033203 = 1.6983463764190674 + 1.0 * 6.636847496032715
Epoch 150, val loss: 1.7218190431594849
Epoch 160, training loss: 8.284040451049805 = 1.6746848821640015 + 1.0 * 6.609355926513672
Epoch 160, val loss: 1.7017886638641357
Epoch 170, training loss: 8.232799530029297 = 1.6473709344863892 + 1.0 * 6.585428237915039
Epoch 170, val loss: 1.6787621974945068
Epoch 180, training loss: 8.187202453613281 = 1.6163438558578491 + 1.0 * 6.570858955383301
Epoch 180, val loss: 1.6525925397872925
Epoch 190, training loss: 8.133881568908691 = 1.5826590061187744 + 1.0 * 6.551222324371338
Epoch 190, val loss: 1.6242175102233887
Epoch 200, training loss: 8.079883575439453 = 1.546109676361084 + 1.0 * 6.533773422241211
Epoch 200, val loss: 1.5936129093170166
Epoch 210, training loss: 8.026985168457031 = 1.5070242881774902 + 1.0 * 6.519960880279541
Epoch 210, val loss: 1.560981273651123
Epoch 220, training loss: 7.9848713874816895 = 1.4660744667053223 + 1.0 * 6.518796920776367
Epoch 220, val loss: 1.5272250175476074
Epoch 230, training loss: 7.922952651977539 = 1.4248632192611694 + 1.0 * 6.49808931350708
Epoch 230, val loss: 1.493709921836853
Epoch 240, training loss: 7.872123718261719 = 1.3832933902740479 + 1.0 * 6.48883056640625
Epoch 240, val loss: 1.4602181911468506
Epoch 250, training loss: 7.823834419250488 = 1.3413794040679932 + 1.0 * 6.482454776763916
Epoch 250, val loss: 1.4267228841781616
Epoch 260, training loss: 7.772140979766846 = 1.2994699478149414 + 1.0 * 6.472671031951904
Epoch 260, val loss: 1.3936593532562256
Epoch 270, training loss: 7.722874641418457 = 1.2574267387390137 + 1.0 * 6.465447902679443
Epoch 270, val loss: 1.3610455989837646
Epoch 280, training loss: 7.6799540519714355 = 1.2152791023254395 + 1.0 * 6.464674949645996
Epoch 280, val loss: 1.3287203311920166
Epoch 290, training loss: 7.6281561851501465 = 1.1732158660888672 + 1.0 * 6.454940319061279
Epoch 290, val loss: 1.2969245910644531
Epoch 300, training loss: 7.579799652099609 = 1.131256103515625 + 1.0 * 6.448543548583984
Epoch 300, val loss: 1.2656461000442505
Epoch 310, training loss: 7.532728672027588 = 1.0894184112548828 + 1.0 * 6.443310260772705
Epoch 310, val loss: 1.2348718643188477
Epoch 320, training loss: 7.490409851074219 = 1.0476778745651245 + 1.0 * 6.442731857299805
Epoch 320, val loss: 1.2044987678527832
Epoch 330, training loss: 7.442941665649414 = 1.0065586566925049 + 1.0 * 6.43638277053833
Epoch 330, val loss: 1.1749138832092285
Epoch 340, training loss: 7.395575523376465 = 0.9659242033958435 + 1.0 * 6.429651260375977
Epoch 340, val loss: 1.1462793350219727
Epoch 350, training loss: 7.351509094238281 = 0.9259748458862305 + 1.0 * 6.425534248352051
Epoch 350, val loss: 1.1186054944992065
Epoch 360, training loss: 7.311975479125977 = 0.8868624567985535 + 1.0 * 6.425113201141357
Epoch 360, val loss: 1.0919744968414307
Epoch 370, training loss: 7.269386291503906 = 0.8489797711372375 + 1.0 * 6.420406341552734
Epoch 370, val loss: 1.0668343305587769
Epoch 380, training loss: 7.228036403656006 = 0.8124063611030579 + 1.0 * 6.415629863739014
Epoch 380, val loss: 1.0430870056152344
Epoch 390, training loss: 7.201285362243652 = 0.7771612405776978 + 1.0 * 6.424124240875244
Epoch 390, val loss: 1.0207493305206299
Epoch 400, training loss: 7.155364036560059 = 0.7436406016349792 + 1.0 * 6.411723613739014
Epoch 400, val loss: 1.0001063346862793
Epoch 410, training loss: 7.11806583404541 = 0.7116305828094482 + 1.0 * 6.406435012817383
Epoch 410, val loss: 0.9812375903129578
Epoch 420, training loss: 7.083465576171875 = 0.6809228658676147 + 1.0 * 6.402542591094971
Epoch 420, val loss: 0.9638248085975647
Epoch 430, training loss: 7.051904201507568 = 0.651435911655426 + 1.0 * 6.400468349456787
Epoch 430, val loss: 0.9479172229766846
Epoch 440, training loss: 7.022305488586426 = 0.62321537733078 + 1.0 * 6.39909029006958
Epoch 440, val loss: 0.9333547353744507
Epoch 450, training loss: 6.991606712341309 = 0.5964013338088989 + 1.0 * 6.395205497741699
Epoch 450, val loss: 0.9204880595207214
Epoch 460, training loss: 6.965366840362549 = 0.5708355903625488 + 1.0 * 6.39453125
Epoch 460, val loss: 0.9091317653656006
Epoch 470, training loss: 6.936636924743652 = 0.5463622808456421 + 1.0 * 6.390274524688721
Epoch 470, val loss: 0.8990915417671204
Epoch 480, training loss: 6.911292552947998 = 0.5228177309036255 + 1.0 * 6.388474941253662
Epoch 480, val loss: 0.8904269337654114
Epoch 490, training loss: 6.8854875564575195 = 0.5001224875450134 + 1.0 * 6.385365009307861
Epoch 490, val loss: 0.8829361200332642
Epoch 500, training loss: 6.860307216644287 = 0.4781700074672699 + 1.0 * 6.382137298583984
Epoch 500, val loss: 0.8765454292297363
Epoch 510, training loss: 6.848974704742432 = 0.45679572224617004 + 1.0 * 6.392179012298584
Epoch 510, val loss: 0.8711346983909607
Epoch 520, training loss: 6.816502094268799 = 0.4360206723213196 + 1.0 * 6.380481243133545
Epoch 520, val loss: 0.8665474057197571
Epoch 530, training loss: 6.791021823883057 = 0.41569891571998596 + 1.0 * 6.3753228187561035
Epoch 530, val loss: 0.8629319071769714
Epoch 540, training loss: 6.784018039703369 = 0.39568957686424255 + 1.0 * 6.388328552246094
Epoch 540, val loss: 0.8598946332931519
Epoch 550, training loss: 6.7475762367248535 = 0.37619659304618835 + 1.0 * 6.371379852294922
Epoch 550, val loss: 0.85764479637146
Epoch 560, training loss: 6.727630615234375 = 0.35711678862571716 + 1.0 * 6.370513916015625
Epoch 560, val loss: 0.8560653328895569
Epoch 570, training loss: 6.706664562225342 = 0.3384286165237427 + 1.0 * 6.368236064910889
Epoch 570, val loss: 0.8551335334777832
Epoch 580, training loss: 6.688814640045166 = 0.3202884793281555 + 1.0 * 6.368525981903076
Epoch 580, val loss: 0.8547560572624207
Epoch 590, training loss: 6.667611122131348 = 0.30277499556541443 + 1.0 * 6.3648362159729
Epoch 590, val loss: 0.8552324771881104
Epoch 600, training loss: 6.657583713531494 = 0.2858748435974121 + 1.0 * 6.371708869934082
Epoch 600, val loss: 0.8562573790550232
Epoch 610, training loss: 6.63221549987793 = 0.2696268558502197 + 1.0 * 6.362588882446289
Epoch 610, val loss: 0.8578531742095947
Epoch 620, training loss: 6.612421035766602 = 0.25403741002082825 + 1.0 * 6.358383655548096
Epoch 620, val loss: 0.8601278066635132
Epoch 630, training loss: 6.605093479156494 = 0.23907890915870667 + 1.0 * 6.36601448059082
Epoch 630, val loss: 0.8629644513130188
Epoch 640, training loss: 6.585546016693115 = 0.2248716503381729 + 1.0 * 6.3606743812561035
Epoch 640, val loss: 0.8663068413734436
Epoch 650, training loss: 6.566363334655762 = 0.21137738227844238 + 1.0 * 6.354986190795898
Epoch 650, val loss: 0.8702635765075684
Epoch 660, training loss: 6.552368640899658 = 0.19856330752372742 + 1.0 * 6.3538055419921875
Epoch 660, val loss: 0.8747946619987488
Epoch 670, training loss: 6.538800239562988 = 0.186444491147995 + 1.0 * 6.35235595703125
Epoch 670, val loss: 0.8798357248306274
Epoch 680, training loss: 6.528302192687988 = 0.1750466227531433 + 1.0 * 6.353255748748779
Epoch 680, val loss: 0.8854085206985474
Epoch 690, training loss: 6.5140790939331055 = 0.16432124376296997 + 1.0 * 6.349757671356201
Epoch 690, val loss: 0.8914082050323486
Epoch 700, training loss: 6.501819133758545 = 0.15431030094623566 + 1.0 * 6.347508907318115
Epoch 700, val loss: 0.8978923559188843
Epoch 710, training loss: 6.494361400604248 = 0.14491233229637146 + 1.0 * 6.349449157714844
Epoch 710, val loss: 0.9047754406929016
Epoch 720, training loss: 6.482421875 = 0.13618884980678558 + 1.0 * 6.346232891082764
Epoch 720, val loss: 0.9119303822517395
Epoch 730, training loss: 6.470786094665527 = 0.12805335223674774 + 1.0 * 6.342732906341553
Epoch 730, val loss: 0.9194466471672058
Epoch 740, training loss: 6.460001468658447 = 0.12046791613101959 + 1.0 * 6.33953332901001
Epoch 740, val loss: 0.9271474480628967
Epoch 750, training loss: 6.463169574737549 = 0.1133858859539032 + 1.0 * 6.349783897399902
Epoch 750, val loss: 0.9349717497825623
Epoch 760, training loss: 6.449748516082764 = 0.10681477189064026 + 1.0 * 6.342933654785156
Epoch 760, val loss: 0.9430214762687683
Epoch 770, training loss: 6.440577030181885 = 0.10071307420730591 + 1.0 * 6.3398637771606445
Epoch 770, val loss: 0.9511930346488953
Epoch 780, training loss: 6.43134069442749 = 0.09503120183944702 + 1.0 * 6.336309432983398
Epoch 780, val loss: 0.9593951106071472
Epoch 790, training loss: 6.427183151245117 = 0.08973280340433121 + 1.0 * 6.3374505043029785
Epoch 790, val loss: 0.9677545428276062
Epoch 800, training loss: 6.4181365966796875 = 0.08479385077953339 + 1.0 * 6.333342552185059
Epoch 800, val loss: 0.9761870503425598
Epoch 810, training loss: 6.41200590133667 = 0.08019420504570007 + 1.0 * 6.331811904907227
Epoch 810, val loss: 0.9846199154853821
Epoch 820, training loss: 6.408438682556152 = 0.0759008377790451 + 1.0 * 6.332537651062012
Epoch 820, val loss: 0.993084728717804
Epoch 830, training loss: 6.402584552764893 = 0.07189725339412689 + 1.0 * 6.330687522888184
Epoch 830, val loss: 1.0014289617538452
Epoch 840, training loss: 6.401004791259766 = 0.0681920126080513 + 1.0 * 6.332812786102295
Epoch 840, val loss: 1.0098731517791748
Epoch 850, training loss: 6.390719890594482 = 0.06473463028669357 + 1.0 * 6.325985431671143
Epoch 850, val loss: 1.0183244943618774
Epoch 860, training loss: 6.3886823654174805 = 0.061497095972299576 + 1.0 * 6.327185153961182
Epoch 860, val loss: 1.0267447233200073
Epoch 870, training loss: 6.3821234703063965 = 0.05846036970615387 + 1.0 * 6.323663234710693
Epoch 870, val loss: 1.0350149869918823
Epoch 880, training loss: 6.380480766296387 = 0.05562544986605644 + 1.0 * 6.324855327606201
Epoch 880, val loss: 1.0433433055877686
Epoch 890, training loss: 6.381583213806152 = 0.05297255888581276 + 1.0 * 6.328610420227051
Epoch 890, val loss: 1.0515908002853394
Epoch 900, training loss: 6.376351356506348 = 0.05049855262041092 + 1.0 * 6.325852870941162
Epoch 900, val loss: 1.0597789287567139
Epoch 910, training loss: 6.370866298675537 = 0.04817228019237518 + 1.0 * 6.322693824768066
Epoch 910, val loss: 1.0678844451904297
Epoch 920, training loss: 6.365541458129883 = 0.04599393159151077 + 1.0 * 6.319547653198242
Epoch 920, val loss: 1.0759788751602173
Epoch 930, training loss: 6.369550704956055 = 0.043942827731370926 + 1.0 * 6.325607776641846
Epoch 930, val loss: 1.0838632583618164
Epoch 940, training loss: 6.360087871551514 = 0.04202544316649437 + 1.0 * 6.3180623054504395
Epoch 940, val loss: 1.0917110443115234
Epoch 950, training loss: 6.356674671173096 = 0.04022318124771118 + 1.0 * 6.316451549530029
Epoch 950, val loss: 1.0995995998382568
Epoch 960, training loss: 6.360935211181641 = 0.03852687031030655 + 1.0 * 6.322408199310303
Epoch 960, val loss: 1.1072814464569092
Epoch 970, training loss: 6.351975917816162 = 0.03693312779068947 + 1.0 * 6.315042972564697
Epoch 970, val loss: 1.1149805784225464
Epoch 980, training loss: 6.3593597412109375 = 0.0354338213801384 + 1.0 * 6.323925971984863
Epoch 980, val loss: 1.1225768327713013
Epoch 990, training loss: 6.348301887512207 = 0.034018561244010925 + 1.0 * 6.31428337097168
Epoch 990, val loss: 1.1299303770065308
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8165524512387982
The final CL Acc:0.75556, 0.00800, The final GNN Acc:0.81690, 0.00174
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13248])
remove edge: torch.Size([2, 7924])
updated graph: torch.Size([2, 10616])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.558430671691895 = 1.9615880250930786 + 1.0 * 8.596842765808105
Epoch 0, val loss: 1.9627559185028076
Epoch 10, training loss: 10.54776668548584 = 1.9511901140213013 + 1.0 * 8.596576690673828
Epoch 10, val loss: 1.952789068222046
Epoch 20, training loss: 10.532672882080078 = 1.9385855197906494 + 1.0 * 8.594087600708008
Epoch 20, val loss: 1.9400243759155273
Epoch 30, training loss: 10.49543571472168 = 1.9213289022445679 + 1.0 * 8.57410717010498
Epoch 30, val loss: 1.922160267829895
Epoch 40, training loss: 10.346131324768066 = 1.8990097045898438 + 1.0 * 8.447121620178223
Epoch 40, val loss: 1.8996984958648682
Epoch 50, training loss: 9.949012756347656 = 1.8751184940338135 + 1.0 * 8.073894500732422
Epoch 50, val loss: 1.8763487339019775
Epoch 60, training loss: 9.533818244934082 = 1.8544045686721802 + 1.0 * 7.679413795471191
Epoch 60, val loss: 1.8559844493865967
Epoch 70, training loss: 9.148603439331055 = 1.8394876718521118 + 1.0 * 7.309115886688232
Epoch 70, val loss: 1.841426968574524
Epoch 80, training loss: 8.934073448181152 = 1.8231682777404785 + 1.0 * 7.110905170440674
Epoch 80, val loss: 1.8257098197937012
Epoch 90, training loss: 8.757433891296387 = 1.8025490045547485 + 1.0 * 6.9548845291137695
Epoch 90, val loss: 1.8067790269851685
Epoch 100, training loss: 8.635862350463867 = 1.7826427221298218 + 1.0 * 6.853219985961914
Epoch 100, val loss: 1.7889289855957031
Epoch 110, training loss: 8.545638084411621 = 1.7635129690170288 + 1.0 * 6.782125473022461
Epoch 110, val loss: 1.7717937231063843
Epoch 120, training loss: 8.468417167663574 = 1.7443732023239136 + 1.0 * 6.724043846130371
Epoch 120, val loss: 1.7549002170562744
Epoch 130, training loss: 8.402327537536621 = 1.7243707180023193 + 1.0 * 6.677957057952881
Epoch 130, val loss: 1.7373902797698975
Epoch 140, training loss: 8.343059539794922 = 1.7023488283157349 + 1.0 * 6.640710830688477
Epoch 140, val loss: 1.7183257341384888
Epoch 150, training loss: 8.289529800415039 = 1.6775270700454712 + 1.0 * 6.612002849578857
Epoch 150, val loss: 1.6970221996307373
Epoch 160, training loss: 8.236063957214355 = 1.649505615234375 + 1.0 * 6.5865583419799805
Epoch 160, val loss: 1.6731793880462646
Epoch 170, training loss: 8.184037208557129 = 1.6177105903625488 + 1.0 * 6.56632661819458
Epoch 170, val loss: 1.6462370157241821
Epoch 180, training loss: 8.131006240844727 = 1.5816655158996582 + 1.0 * 6.549340724945068
Epoch 180, val loss: 1.6157246828079224
Epoch 190, training loss: 8.07720947265625 = 1.541111946105957 + 1.0 * 6.536098003387451
Epoch 190, val loss: 1.5815622806549072
Epoch 200, training loss: 8.020430564880371 = 1.4967141151428223 + 1.0 * 6.523716449737549
Epoch 200, val loss: 1.5444608926773071
Epoch 210, training loss: 7.9601030349731445 = 1.4495642185211182 + 1.0 * 6.510538578033447
Epoch 210, val loss: 1.5053248405456543
Epoch 220, training loss: 7.899862289428711 = 1.399580717086792 + 1.0 * 6.500281810760498
Epoch 220, val loss: 1.4642177820205688
Epoch 230, training loss: 7.838015556335449 = 1.347257375717163 + 1.0 * 6.490757942199707
Epoch 230, val loss: 1.4216253757476807
Epoch 240, training loss: 7.78120231628418 = 1.2935216426849365 + 1.0 * 6.487680912017822
Epoch 240, val loss: 1.3784974813461304
Epoch 250, training loss: 7.719285011291504 = 1.239809274673462 + 1.0 * 6.479475498199463
Epoch 250, val loss: 1.3362208604812622
Epoch 260, training loss: 7.6574811935424805 = 1.1870827674865723 + 1.0 * 6.470398426055908
Epoch 260, val loss: 1.2953267097473145
Epoch 270, training loss: 7.59869384765625 = 1.1351921558380127 + 1.0 * 6.463501453399658
Epoch 270, val loss: 1.2557748556137085
Epoch 280, training loss: 7.543069362640381 = 1.0844354629516602 + 1.0 * 6.458633899688721
Epoch 280, val loss: 1.2177175283432007
Epoch 290, training loss: 7.488691329956055 = 1.0354336500167847 + 1.0 * 6.4532575607299805
Epoch 290, val loss: 1.1817315816879272
Epoch 300, training loss: 7.436817646026611 = 0.988430380821228 + 1.0 * 6.448387145996094
Epoch 300, val loss: 1.1478372812271118
Epoch 310, training loss: 7.385865688323975 = 0.943358838558197 + 1.0 * 6.442506790161133
Epoch 310, val loss: 1.1157981157302856
Epoch 320, training loss: 7.337434768676758 = 0.9001122713088989 + 1.0 * 6.437322616577148
Epoch 320, val loss: 1.0852619409561157
Epoch 330, training loss: 7.292145252227783 = 0.8582834601402283 + 1.0 * 6.43386173248291
Epoch 330, val loss: 1.055907130241394
Epoch 340, training loss: 7.248120307922363 = 0.8179781436920166 + 1.0 * 6.430141925811768
Epoch 340, val loss: 1.0276626348495483
Epoch 350, training loss: 7.205530643463135 = 0.7792807221412659 + 1.0 * 6.426249980926514
Epoch 350, val loss: 1.0005563497543335
Epoch 360, training loss: 7.16664981842041 = 0.7419403791427612 + 1.0 * 6.424709320068359
Epoch 360, val loss: 0.974357008934021
Epoch 370, training loss: 7.124556064605713 = 0.7060336470603943 + 1.0 * 6.418522357940674
Epoch 370, val loss: 0.949051558971405
Epoch 380, training loss: 7.097354412078857 = 0.6713465452194214 + 1.0 * 6.4260077476501465
Epoch 380, val loss: 0.924730658531189
Epoch 390, training loss: 7.050375461578369 = 0.6381715536117554 + 1.0 * 6.412203788757324
Epoch 390, val loss: 0.9015633463859558
Epoch 400, training loss: 7.013892650604248 = 0.6062405705451965 + 1.0 * 6.407651901245117
Epoch 400, val loss: 0.8795177340507507
Epoch 410, training loss: 6.984764099121094 = 0.5754011869430542 + 1.0 * 6.40936279296875
Epoch 410, val loss: 0.8585096597671509
Epoch 420, training loss: 6.951798439025879 = 0.5457621216773987 + 1.0 * 6.406036376953125
Epoch 420, val loss: 0.8387838006019592
Epoch 430, training loss: 6.917263984680176 = 0.5173993706703186 + 1.0 * 6.399864673614502
Epoch 430, val loss: 0.8202781677246094
Epoch 440, training loss: 6.8866095542907715 = 0.4899407625198364 + 1.0 * 6.396668910980225
Epoch 440, val loss: 0.802802562713623
Epoch 450, training loss: 6.859120845794678 = 0.46329736709594727 + 1.0 * 6.3958234786987305
Epoch 450, val loss: 0.7863560318946838
Epoch 460, training loss: 6.837418556213379 = 0.4376395642757416 + 1.0 * 6.399778842926025
Epoch 460, val loss: 0.7710304260253906
Epoch 470, training loss: 6.806736469268799 = 0.41298040747642517 + 1.0 * 6.393755912780762
Epoch 470, val loss: 0.7567879557609558
Epoch 480, training loss: 6.776671409606934 = 0.38917285203933716 + 1.0 * 6.387498378753662
Epoch 480, val loss: 0.743527889251709
Epoch 490, training loss: 6.751049518585205 = 0.36614304780960083 + 1.0 * 6.38490629196167
Epoch 490, val loss: 0.7310996651649475
Epoch 500, training loss: 6.7285895347595215 = 0.34385454654693604 + 1.0 * 6.384735107421875
Epoch 500, val loss: 0.7194939851760864
Epoch 510, training loss: 6.708069324493408 = 0.3225308060646057 + 1.0 * 6.385538578033447
Epoch 510, val loss: 0.7088429927825928
Epoch 520, training loss: 6.683977127075195 = 0.30234262347221375 + 1.0 * 6.381634712219238
Epoch 520, val loss: 0.6990930438041687
Epoch 530, training loss: 6.659981727600098 = 0.2830745577812195 + 1.0 * 6.3769073486328125
Epoch 530, val loss: 0.6901817321777344
Epoch 540, training loss: 6.6390767097473145 = 0.2647259533405304 + 1.0 * 6.374350547790527
Epoch 540, val loss: 0.6820439100265503
Epoch 550, training loss: 6.619428634643555 = 0.2473323941230774 + 1.0 * 6.372096061706543
Epoch 550, val loss: 0.6747214198112488
Epoch 560, training loss: 6.629333019256592 = 0.23096026480197906 + 1.0 * 6.398372650146484
Epoch 560, val loss: 0.6682609915733337
Epoch 570, training loss: 6.590986728668213 = 0.2158769816160202 + 1.0 * 6.375109672546387
Epoch 570, val loss: 0.6627263426780701
Epoch 580, training loss: 6.570784091949463 = 0.20182305574417114 + 1.0 * 6.368960857391357
Epoch 580, val loss: 0.6580183506011963
Epoch 590, training loss: 6.55507755279541 = 0.18876579403877258 + 1.0 * 6.366311550140381
Epoch 590, val loss: 0.6541047096252441
Epoch 600, training loss: 6.539797306060791 = 0.17660154402256012 + 1.0 * 6.363195896148682
Epoch 600, val loss: 0.6509427428245544
Epoch 610, training loss: 6.538087844848633 = 0.16532070934772491 + 1.0 * 6.372766971588135
Epoch 610, val loss: 0.6485147476196289
Epoch 620, training loss: 6.5226216316223145 = 0.15488609671592712 + 1.0 * 6.367735385894775
Epoch 620, val loss: 0.6467094421386719
Epoch 630, training loss: 6.507355213165283 = 0.1452915370464325 + 1.0 * 6.362063884735107
Epoch 630, val loss: 0.6455304622650146
Epoch 640, training loss: 6.493923664093018 = 0.13639454543590546 + 1.0 * 6.357529163360596
Epoch 640, val loss: 0.6449394226074219
Epoch 650, training loss: 6.483262538909912 = 0.12810692191123962 + 1.0 * 6.3551554679870605
Epoch 650, val loss: 0.6448254585266113
Epoch 660, training loss: 6.4748053550720215 = 0.12038600444793701 + 1.0 * 6.354419231414795
Epoch 660, val loss: 0.6451936960220337
Epoch 670, training loss: 6.4795379638671875 = 0.11324085295200348 + 1.0 * 6.366297245025635
Epoch 670, val loss: 0.6459895968437195
Epoch 680, training loss: 6.462907791137695 = 0.10668706893920898 + 1.0 * 6.356220722198486
Epoch 680, val loss: 0.6470915079116821
Epoch 690, training loss: 6.451667785644531 = 0.10059797018766403 + 1.0 * 6.351069927215576
Epoch 690, val loss: 0.6486178040504456
Epoch 700, training loss: 6.443015098571777 = 0.09492050856351852 + 1.0 * 6.348094463348389
Epoch 700, val loss: 0.6504497528076172
Epoch 710, training loss: 6.435914516448975 = 0.08962176740169525 + 1.0 * 6.346292972564697
Epoch 710, val loss: 0.6525225639343262
Epoch 720, training loss: 6.435410022735596 = 0.08467750251293182 + 1.0 * 6.350732326507568
Epoch 720, val loss: 0.6548454761505127
Epoch 730, training loss: 6.435826778411865 = 0.0800984725356102 + 1.0 * 6.3557281494140625
Epoch 730, val loss: 0.6574450731277466
Epoch 740, training loss: 6.418704509735107 = 0.07587014138698578 + 1.0 * 6.34283447265625
Epoch 740, val loss: 0.6601353883743286
Epoch 750, training loss: 6.41327428817749 = 0.07192995399236679 + 1.0 * 6.341344356536865
Epoch 750, val loss: 0.6630740165710449
Epoch 760, training loss: 6.408304214477539 = 0.06823974847793579 + 1.0 * 6.340064525604248
Epoch 760, val loss: 0.6661141514778137
Epoch 770, training loss: 6.409627437591553 = 0.06477846205234528 + 1.0 * 6.344849109649658
Epoch 770, val loss: 0.6692880988121033
Epoch 780, training loss: 6.412199974060059 = 0.061581045389175415 + 1.0 * 6.350618839263916
Epoch 780, val loss: 0.6725600957870483
Epoch 790, training loss: 6.3973894119262695 = 0.058594997972249985 + 1.0 * 6.338794231414795
Epoch 790, val loss: 0.6760047674179077
Epoch 800, training loss: 6.3906636238098145 = 0.05581217631697655 + 1.0 * 6.334851264953613
Epoch 800, val loss: 0.6794856786727905
Epoch 810, training loss: 6.3885979652404785 = 0.053203437477350235 + 1.0 * 6.335394382476807
Epoch 810, val loss: 0.6830176711082458
Epoch 820, training loss: 6.387948513031006 = 0.050764910876750946 + 1.0 * 6.337183475494385
Epoch 820, val loss: 0.686623215675354
Epoch 830, training loss: 6.379796504974365 = 0.048482585698366165 + 1.0 * 6.3313140869140625
Epoch 830, val loss: 0.6902207136154175
Epoch 840, training loss: 6.377869606018066 = 0.04634151980280876 + 1.0 * 6.331528186798096
Epoch 840, val loss: 0.6938862204551697
Epoch 850, training loss: 6.383551120758057 = 0.044333022087812424 + 1.0 * 6.3392181396484375
Epoch 850, val loss: 0.6976162195205688
Epoch 860, training loss: 6.374515056610107 = 0.042464978992938995 + 1.0 * 6.33204984664917
Epoch 860, val loss: 0.7012911438941956
Epoch 870, training loss: 6.368724822998047 = 0.040698181837797165 + 1.0 * 6.32802677154541
Epoch 870, val loss: 0.7049270868301392
Epoch 880, training loss: 6.365694999694824 = 0.03904194384813309 + 1.0 * 6.326653003692627
Epoch 880, val loss: 0.7086836099624634
Epoch 890, training loss: 6.367238998413086 = 0.037477221339941025 + 1.0 * 6.329761981964111
Epoch 890, val loss: 0.712352991104126
Epoch 900, training loss: 6.361629962921143 = 0.03600131347775459 + 1.0 * 6.325628757476807
Epoch 900, val loss: 0.7160496711730957
Epoch 910, training loss: 6.358208179473877 = 0.03461356833577156 + 1.0 * 6.323594570159912
Epoch 910, val loss: 0.7197638750076294
Epoch 920, training loss: 6.356423854827881 = 0.03330131247639656 + 1.0 * 6.323122501373291
Epoch 920, val loss: 0.7234833240509033
Epoch 930, training loss: 6.3677496910095215 = 0.032063402235507965 + 1.0 * 6.335686206817627
Epoch 930, val loss: 0.7270561456680298
Epoch 940, training loss: 6.353556156158447 = 0.03090856410562992 + 1.0 * 6.322647571563721
Epoch 940, val loss: 0.7306964993476868
Epoch 950, training loss: 6.349675178527832 = 0.02981271967291832 + 1.0 * 6.319862365722656
Epoch 950, val loss: 0.7343083024024963
Epoch 960, training loss: 6.346797943115234 = 0.028770411387085915 + 1.0 * 6.318027496337891
Epoch 960, val loss: 0.7378677129745483
Epoch 970, training loss: 6.3471550941467285 = 0.027779027819633484 + 1.0 * 6.319375991821289
Epoch 970, val loss: 0.7414443492889404
Epoch 980, training loss: 6.342864513397217 = 0.02684084326028824 + 1.0 * 6.316023826599121
Epoch 980, val loss: 0.7449088096618652
Epoch 990, training loss: 6.343737602233887 = 0.02595093473792076 + 1.0 * 6.317786693572998
Epoch 990, val loss: 0.7483958005905151
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 10.547097206115723 = 1.9503060579299927 + 1.0 * 8.59679126739502
Epoch 0, val loss: 1.9443243741989136
Epoch 10, training loss: 10.535500526428223 = 1.9391225576400757 + 1.0 * 8.596378326416016
Epoch 10, val loss: 1.9330494403839111
Epoch 20, training loss: 10.518428802490234 = 1.9251898527145386 + 1.0 * 8.593238830566406
Epoch 20, val loss: 1.9191402196884155
Epoch 30, training loss: 10.476832389831543 = 1.9060171842575073 + 1.0 * 8.570815086364746
Epoch 30, val loss: 1.9003404378890991
Epoch 40, training loss: 10.308916091918945 = 1.8822157382965088 + 1.0 * 8.426700592041016
Epoch 40, val loss: 1.877903938293457
Epoch 50, training loss: 9.897758483886719 = 1.8568018674850464 + 1.0 * 8.040956497192383
Epoch 50, val loss: 1.8542734384536743
Epoch 60, training loss: 9.578720092773438 = 1.8349153995513916 + 1.0 * 7.743804931640625
Epoch 60, val loss: 1.8343935012817383
Epoch 70, training loss: 9.14549446105957 = 1.8168472051620483 + 1.0 * 7.328647136688232
Epoch 70, val loss: 1.818769097328186
Epoch 80, training loss: 8.871721267700195 = 1.8038569688796997 + 1.0 * 7.067864418029785
Epoch 80, val loss: 1.8073174953460693
Epoch 90, training loss: 8.695197105407715 = 1.7873332500457764 + 1.0 * 6.907864093780518
Epoch 90, val loss: 1.792776346206665
Epoch 100, training loss: 8.581266403198242 = 1.7696093320846558 + 1.0 * 6.811657428741455
Epoch 100, val loss: 1.7781963348388672
Epoch 110, training loss: 8.50333309173584 = 1.752096176147461 + 1.0 * 6.751236915588379
Epoch 110, val loss: 1.7636611461639404
Epoch 120, training loss: 8.4378662109375 = 1.7331736087799072 + 1.0 * 6.704692840576172
Epoch 120, val loss: 1.7471107244491577
Epoch 130, training loss: 8.38019847869873 = 1.712038278579712 + 1.0 * 6.6681599617004395
Epoch 130, val loss: 1.7284444570541382
Epoch 140, training loss: 8.323368072509766 = 1.6883422136306763 + 1.0 * 6.635025978088379
Epoch 140, val loss: 1.7078403234481812
Epoch 150, training loss: 8.26662826538086 = 1.6604688167572021 + 1.0 * 6.606159687042236
Epoch 150, val loss: 1.6842397451400757
Epoch 160, training loss: 8.20822525024414 = 1.6282031536102295 + 1.0 * 6.58002233505249
Epoch 160, val loss: 1.6570100784301758
Epoch 170, training loss: 8.152484893798828 = 1.591962456703186 + 1.0 * 6.560522079467773
Epoch 170, val loss: 1.6265544891357422
Epoch 180, training loss: 8.090415000915527 = 1.5525834560394287 + 1.0 * 6.537831783294678
Epoch 180, val loss: 1.5932921171188354
Epoch 190, training loss: 8.02918529510498 = 1.5100843906402588 + 1.0 * 6.519101142883301
Epoch 190, val loss: 1.5574722290039062
Epoch 200, training loss: 7.9687113761901855 = 1.4650893211364746 + 1.0 * 6.503622055053711
Epoch 200, val loss: 1.5196611881256104
Epoch 210, training loss: 7.913877010345459 = 1.4192253351211548 + 1.0 * 6.494651794433594
Epoch 210, val loss: 1.4812877178192139
Epoch 220, training loss: 7.853318214416504 = 1.373794436454773 + 1.0 * 6.479523658752441
Epoch 220, val loss: 1.4435088634490967
Epoch 230, training loss: 7.799737453460693 = 1.3285207748413086 + 1.0 * 6.471216678619385
Epoch 230, val loss: 1.4063400030136108
Epoch 240, training loss: 7.746283054351807 = 1.2837748527526855 + 1.0 * 6.462508201599121
Epoch 240, val loss: 1.369818091392517
Epoch 250, training loss: 7.693656921386719 = 1.2391517162322998 + 1.0 * 6.454505443572998
Epoch 250, val loss: 1.3337488174438477
Epoch 260, training loss: 7.640288829803467 = 1.1936663389205933 + 1.0 * 6.446622371673584
Epoch 260, val loss: 1.2973053455352783
Epoch 270, training loss: 7.588886260986328 = 1.1468020677566528 + 1.0 * 6.442084312438965
Epoch 270, val loss: 1.2599906921386719
Epoch 280, training loss: 7.534688472747803 = 1.0988069772720337 + 1.0 * 6.435881614685059
Epoch 280, val loss: 1.2221473455429077
Epoch 290, training loss: 7.4790425300598145 = 1.049585223197937 + 1.0 * 6.429457187652588
Epoch 290, val loss: 1.1836650371551514
Epoch 300, training loss: 7.428755283355713 = 0.9991341233253479 + 1.0 * 6.42962121963501
Epoch 300, val loss: 1.1443507671356201
Epoch 310, training loss: 7.369436264038086 = 0.9484322667121887 + 1.0 * 6.421003818511963
Epoch 310, val loss: 1.1050670146942139
Epoch 320, training loss: 7.315364837646484 = 0.8980150818824768 + 1.0 * 6.417349815368652
Epoch 320, val loss: 1.0663788318634033
Epoch 330, training loss: 7.262957572937012 = 0.8483831286430359 + 1.0 * 6.41457462310791
Epoch 330, val loss: 1.0285778045654297
Epoch 340, training loss: 7.210489273071289 = 0.8007797598838806 + 1.0 * 6.409709453582764
Epoch 340, val loss: 0.9928454160690308
Epoch 350, training loss: 7.160882472991943 = 0.7558075189590454 + 1.0 * 6.4050750732421875
Epoch 350, val loss: 0.9597292542457581
Epoch 360, training loss: 7.115108966827393 = 0.7133970260620117 + 1.0 * 6.401711940765381
Epoch 360, val loss: 0.9292759895324707
Epoch 370, training loss: 7.07418966293335 = 0.6738638281822205 + 1.0 * 6.400325775146484
Epoch 370, val loss: 0.9017472267150879
Epoch 380, training loss: 7.033123970031738 = 0.6375243067741394 + 1.0 * 6.395599842071533
Epoch 380, val loss: 0.877384603023529
Epoch 390, training loss: 6.9955902099609375 = 0.6037647724151611 + 1.0 * 6.391825199127197
Epoch 390, val loss: 0.8557432889938354
Epoch 400, training loss: 6.961493015289307 = 0.5723296999931335 + 1.0 * 6.389163494110107
Epoch 400, val loss: 0.8365071415901184
Epoch 410, training loss: 6.92978048324585 = 0.5431799292564392 + 1.0 * 6.386600494384766
Epoch 410, val loss: 0.8197246789932251
Epoch 420, training loss: 6.896754264831543 = 0.515775203704834 + 1.0 * 6.380979061126709
Epoch 420, val loss: 0.8049265146255493
Epoch 430, training loss: 6.869363307952881 = 0.48989230394363403 + 1.0 * 6.3794708251953125
Epoch 430, val loss: 0.7917929887771606
Epoch 440, training loss: 6.848985195159912 = 0.4655531346797943 + 1.0 * 6.383431911468506
Epoch 440, val loss: 0.7801780700683594
Epoch 450, training loss: 6.816547393798828 = 0.442497193813324 + 1.0 * 6.374050140380859
Epoch 450, val loss: 0.7700682282447815
Epoch 460, training loss: 6.792684078216553 = 0.4203845262527466 + 1.0 * 6.372299671173096
Epoch 460, val loss: 0.7609964609146118
Epoch 470, training loss: 6.7692389488220215 = 0.39912715554237366 + 1.0 * 6.37011194229126
Epoch 470, val loss: 0.7527880668640137
Epoch 480, training loss: 6.748109340667725 = 0.37856292724609375 + 1.0 * 6.369546413421631
Epoch 480, val loss: 0.7454267740249634
Epoch 490, training loss: 6.723304271697998 = 0.35866081714630127 + 1.0 * 6.364643573760986
Epoch 490, val loss: 0.7387531995773315
Epoch 500, training loss: 6.729492664337158 = 0.33928629755973816 + 1.0 * 6.390206336975098
Epoch 500, val loss: 0.7326134443283081
Epoch 510, training loss: 6.6874470710754395 = 0.32062461972236633 + 1.0 * 6.366822242736816
Epoch 510, val loss: 0.7270470857620239
Epoch 520, training loss: 6.659159183502197 = 0.30249592661857605 + 1.0 * 6.356663227081299
Epoch 520, val loss: 0.7221384048461914
Epoch 530, training loss: 6.640326499938965 = 0.2848661243915558 + 1.0 * 6.355460166931152
Epoch 530, val loss: 0.7176774144172668
Epoch 540, training loss: 6.620792865753174 = 0.2677784562110901 + 1.0 * 6.3530144691467285
Epoch 540, val loss: 0.7137786746025085
Epoch 550, training loss: 6.6022796630859375 = 0.2512698173522949 + 1.0 * 6.351009845733643
Epoch 550, val loss: 0.7104412913322449
Epoch 560, training loss: 6.587449073791504 = 0.2355031967163086 + 1.0 * 6.351945877075195
Epoch 560, val loss: 0.7076194286346436
Epoch 570, training loss: 6.570420265197754 = 0.2207079976797104 + 1.0 * 6.349712371826172
Epoch 570, val loss: 0.7054811120033264
Epoch 580, training loss: 6.554107666015625 = 0.2067161202430725 + 1.0 * 6.347391605377197
Epoch 580, val loss: 0.7040333151817322
Epoch 590, training loss: 6.539119243621826 = 0.19354254007339478 + 1.0 * 6.345576763153076
Epoch 590, val loss: 0.7032418251037598
Epoch 600, training loss: 6.524827003479004 = 0.18121886253356934 + 1.0 * 6.343608379364014
Epoch 600, val loss: 0.7030664682388306
Epoch 610, training loss: 6.513174533843994 = 0.16975067555904388 + 1.0 * 6.343423843383789
Epoch 610, val loss: 0.7035625576972961
Epoch 620, training loss: 6.498593330383301 = 0.15905854105949402 + 1.0 * 6.339534759521484
Epoch 620, val loss: 0.7046763300895691
Epoch 630, training loss: 6.497262001037598 = 0.14909222722053528 + 1.0 * 6.348169803619385
Epoch 630, val loss: 0.7062522768974304
Epoch 640, training loss: 6.4767584800720215 = 0.13992294669151306 + 1.0 * 6.3368353843688965
Epoch 640, val loss: 0.7083264589309692
Epoch 650, training loss: 6.466483116149902 = 0.13139967620372772 + 1.0 * 6.335083484649658
Epoch 650, val loss: 0.7109005451202393
Epoch 660, training loss: 6.46936559677124 = 0.12348519265651703 + 1.0 * 6.345880508422852
Epoch 660, val loss: 0.7137962579727173
Epoch 670, training loss: 6.450166702270508 = 0.11619378626346588 + 1.0 * 6.333972930908203
Epoch 670, val loss: 0.7169707417488098
Epoch 680, training loss: 6.440672874450684 = 0.10943960398435593 + 1.0 * 6.331233501434326
Epoch 680, val loss: 0.7205058336257935
Epoch 690, training loss: 6.434700012207031 = 0.10316069424152374 + 1.0 * 6.331539154052734
Epoch 690, val loss: 0.7243329882621765
Epoch 700, training loss: 6.426071643829346 = 0.09733407199382782 + 1.0 * 6.328737735748291
Epoch 700, val loss: 0.7283622622489929
Epoch 710, training loss: 6.425288677215576 = 0.09193456918001175 + 1.0 * 6.3333539962768555
Epoch 710, val loss: 0.7325862050056458
Epoch 720, training loss: 6.412339687347412 = 0.08693020045757294 + 1.0 * 6.325409412384033
Epoch 720, val loss: 0.7368915677070618
Epoch 730, training loss: 6.408163070678711 = 0.08227342367172241 + 1.0 * 6.325889587402344
Epoch 730, val loss: 0.7413870096206665
Epoch 740, training loss: 6.404743194580078 = 0.07794824987649918 + 1.0 * 6.3267951011657715
Epoch 740, val loss: 0.7459951639175415
Epoch 750, training loss: 6.395009517669678 = 0.07392682135105133 + 1.0 * 6.321082592010498
Epoch 750, val loss: 0.75063556432724
Epoch 760, training loss: 6.390592098236084 = 0.07018199563026428 + 1.0 * 6.320410251617432
Epoch 760, val loss: 0.7553901672363281
Epoch 770, training loss: 6.39084529876709 = 0.06667499989271164 + 1.0 * 6.324170112609863
Epoch 770, val loss: 0.7601909637451172
Epoch 780, training loss: 6.38298225402832 = 0.06342795491218567 + 1.0 * 6.319554328918457
Epoch 780, val loss: 0.7650019526481628
Epoch 790, training loss: 6.381130218505859 = 0.06039086729288101 + 1.0 * 6.320739269256592
Epoch 790, val loss: 0.7697904109954834
Epoch 800, training loss: 6.373744010925293 = 0.05756077542901039 + 1.0 * 6.316183090209961
Epoch 800, val loss: 0.7746239900588989
Epoch 810, training loss: 6.369630336761475 = 0.054904792457818985 + 1.0 * 6.314725399017334
Epoch 810, val loss: 0.7794409394264221
Epoch 820, training loss: 6.372851848602295 = 0.052416276186704636 + 1.0 * 6.320435523986816
Epoch 820, val loss: 0.7842704653739929
Epoch 830, training loss: 6.368077754974365 = 0.05008364096283913 + 1.0 * 6.317994117736816
Epoch 830, val loss: 0.7889519333839417
Epoch 840, training loss: 6.359487533569336 = 0.047896381467580795 + 1.0 * 6.311591148376465
Epoch 840, val loss: 0.793637216091156
Epoch 850, training loss: 6.356320858001709 = 0.04584028571844101 + 1.0 * 6.31048059463501
Epoch 850, val loss: 0.7983527183532715
Epoch 860, training loss: 6.354229927062988 = 0.04389509931206703 + 1.0 * 6.3103346824646
Epoch 860, val loss: 0.803016722202301
Epoch 870, training loss: 6.355169773101807 = 0.042064402252435684 + 1.0 * 6.313105583190918
Epoch 870, val loss: 0.8075926303863525
Epoch 880, training loss: 6.349784851074219 = 0.04035484418272972 + 1.0 * 6.309430122375488
Epoch 880, val loss: 0.8121134042739868
Epoch 890, training loss: 6.346068382263184 = 0.038735050708055496 + 1.0 * 6.307333469390869
Epoch 890, val loss: 0.816618800163269
Epoch 900, training loss: 6.348127841949463 = 0.03720493242144585 + 1.0 * 6.310923099517822
Epoch 900, val loss: 0.821113109588623
Epoch 910, training loss: 6.341719627380371 = 0.03575563803315163 + 1.0 * 6.30596399307251
Epoch 910, val loss: 0.8254423141479492
Epoch 920, training loss: 6.339687824249268 = 0.034389592707157135 + 1.0 * 6.305298328399658
Epoch 920, val loss: 0.8298229575157166
Epoch 930, training loss: 6.335437774658203 = 0.03309226408600807 + 1.0 * 6.302345275878906
Epoch 930, val loss: 0.8341009616851807
Epoch 940, training loss: 6.3333940505981445 = 0.03186001628637314 + 1.0 * 6.301534175872803
Epoch 940, val loss: 0.8384000658988953
Epoch 950, training loss: 6.344483852386475 = 0.030689097940921783 + 1.0 * 6.3137946128845215
Epoch 950, val loss: 0.8426072597503662
Epoch 960, training loss: 6.3293070793151855 = 0.029589248821139336 + 1.0 * 6.299717903137207
Epoch 960, val loss: 0.8467090725898743
Epoch 970, training loss: 6.331111907958984 = 0.028540829196572304 + 1.0 * 6.3025712966918945
Epoch 970, val loss: 0.8507969975471497
Epoch 980, training loss: 6.328253746032715 = 0.027545316144824028 + 1.0 * 6.300708293914795
Epoch 980, val loss: 0.8548445701599121
Epoch 990, training loss: 6.325689315795898 = 0.026603057980537415 + 1.0 * 6.299086093902588
Epoch 990, val loss: 0.8588101267814636
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.536100387573242 = 1.9392549991607666 + 1.0 * 8.596845626831055
Epoch 0, val loss: 1.9359869956970215
Epoch 10, training loss: 10.525150299072266 = 1.9286189079284668 + 1.0 * 8.59653091430664
Epoch 10, val loss: 1.9248815774917603
Epoch 20, training loss: 10.509086608886719 = 1.9150638580322266 + 1.0 * 8.594022750854492
Epoch 20, val loss: 1.910675048828125
Epoch 30, training loss: 10.470704078674316 = 1.8960579633712769 + 1.0 * 8.57464599609375
Epoch 30, val loss: 1.8910571336746216
Epoch 40, training loss: 10.324130058288574 = 1.8710215091705322 + 1.0 * 8.453108787536621
Epoch 40, val loss: 1.866441011428833
Epoch 50, training loss: 9.943180084228516 = 1.844688892364502 + 1.0 * 8.098490715026855
Epoch 50, val loss: 1.8419122695922852
Epoch 60, training loss: 9.555161476135254 = 1.8232399225234985 + 1.0 * 7.731921195983887
Epoch 60, val loss: 1.8216984272003174
Epoch 70, training loss: 9.097363471984863 = 1.8066610097885132 + 1.0 * 7.2907023429870605
Epoch 70, val loss: 1.8064557313919067
Epoch 80, training loss: 8.84087085723877 = 1.7913957834243774 + 1.0 * 7.049474716186523
Epoch 80, val loss: 1.793034315109253
Epoch 90, training loss: 8.669066429138184 = 1.7731739282608032 + 1.0 * 6.89589262008667
Epoch 90, val loss: 1.7768384218215942
Epoch 100, training loss: 8.570409774780273 = 1.7529425621032715 + 1.0 * 6.81746768951416
Epoch 100, val loss: 1.7586710453033447
Epoch 110, training loss: 8.488041877746582 = 1.7311139106750488 + 1.0 * 6.756927967071533
Epoch 110, val loss: 1.738741397857666
Epoch 120, training loss: 8.415234565734863 = 1.7072960138320923 + 1.0 * 6.7079386711120605
Epoch 120, val loss: 1.7168385982513428
Epoch 130, training loss: 8.346053123474121 = 1.6800354719161987 + 1.0 * 6.666017532348633
Epoch 130, val loss: 1.6923108100891113
Epoch 140, training loss: 8.284059524536133 = 1.648659348487854 + 1.0 * 6.635400295257568
Epoch 140, val loss: 1.6646478176116943
Epoch 150, training loss: 8.218600273132324 = 1.6131170988082886 + 1.0 * 6.605483055114746
Epoch 150, val loss: 1.6338900327682495
Epoch 160, training loss: 8.154354095458984 = 1.5731916427612305 + 1.0 * 6.581161975860596
Epoch 160, val loss: 1.5995819568634033
Epoch 170, training loss: 8.091536521911621 = 1.5292060375213623 + 1.0 * 6.56233024597168
Epoch 170, val loss: 1.5620542764663696
Epoch 180, training loss: 8.028789520263672 = 1.4831602573394775 + 1.0 * 6.545629501342773
Epoch 180, val loss: 1.5236889123916626
Epoch 190, training loss: 7.963494300842285 = 1.4361190795898438 + 1.0 * 6.527375221252441
Epoch 190, val loss: 1.4850478172302246
Epoch 200, training loss: 7.909993648529053 = 1.3885804414749146 + 1.0 * 6.521413326263428
Epoch 200, val loss: 1.4468334913253784
Epoch 210, training loss: 7.844352722167969 = 1.342005968093872 + 1.0 * 6.502346992492676
Epoch 210, val loss: 1.4102058410644531
Epoch 220, training loss: 7.787785053253174 = 1.2962408065795898 + 1.0 * 6.491544246673584
Epoch 220, val loss: 1.3747360706329346
Epoch 230, training loss: 7.736439228057861 = 1.2513214349746704 + 1.0 * 6.4851179122924805
Epoch 230, val loss: 1.3404496908187866
Epoch 240, training loss: 7.679225921630859 = 1.207235336303711 + 1.0 * 6.471990585327148
Epoch 240, val loss: 1.3072038888931274
Epoch 250, training loss: 7.626611232757568 = 1.1638764142990112 + 1.0 * 6.462734699249268
Epoch 250, val loss: 1.2748106718063354
Epoch 260, training loss: 7.583120346069336 = 1.121053695678711 + 1.0 * 6.462066650390625
Epoch 260, val loss: 1.2430095672607422
Epoch 270, training loss: 7.527787685394287 = 1.0790232419967651 + 1.0 * 6.448764324188232
Epoch 270, val loss: 1.2116949558258057
Epoch 280, training loss: 7.48190975189209 = 1.0373891592025757 + 1.0 * 6.444520473480225
Epoch 280, val loss: 1.1807540655136108
Epoch 290, training loss: 7.433017730712891 = 0.9964424967765808 + 1.0 * 6.436575412750244
Epoch 290, val loss: 1.1499592065811157
Epoch 300, training loss: 7.385748863220215 = 0.9558433890342712 + 1.0 * 6.429905414581299
Epoch 300, val loss: 1.119541883468628
Epoch 310, training loss: 7.342122554779053 = 0.9157286882400513 + 1.0 * 6.426393985748291
Epoch 310, val loss: 1.0893535614013672
Epoch 320, training loss: 7.30121374130249 = 0.8766551613807678 + 1.0 * 6.424558639526367
Epoch 320, val loss: 1.0597912073135376
Epoch 330, training loss: 7.255240440368652 = 0.8389122486114502 + 1.0 * 6.416327953338623
Epoch 330, val loss: 1.031288981437683
Epoch 340, training loss: 7.213547706604004 = 0.8024297952651978 + 1.0 * 6.411118030548096
Epoch 340, val loss: 1.0037579536437988
Epoch 350, training loss: 7.178200721740723 = 0.7672775983810425 + 1.0 * 6.410923004150391
Epoch 350, val loss: 0.9775357842445374
Epoch 360, training loss: 7.141040325164795 = 0.7338235974311829 + 1.0 * 6.407216548919678
Epoch 360, val loss: 0.9530978798866272
Epoch 370, training loss: 7.1028618812561035 = 0.7023264169692993 + 1.0 * 6.400535583496094
Epoch 370, val loss: 0.9304320812225342
Epoch 380, training loss: 7.068707466125488 = 0.6722328662872314 + 1.0 * 6.396474838256836
Epoch 380, val loss: 0.9095892310142517
Epoch 390, training loss: 7.037514686584473 = 0.6434386968612671 + 1.0 * 6.394075870513916
Epoch 390, val loss: 0.8903859257698059
Epoch 400, training loss: 7.020841121673584 = 0.6159147024154663 + 1.0 * 6.404926300048828
Epoch 400, val loss: 0.8727446794509888
Epoch 410, training loss: 6.978759765625 = 0.5896784067153931 + 1.0 * 6.3890814781188965
Epoch 410, val loss: 0.8568490743637085
Epoch 420, training loss: 6.949990749359131 = 0.5645184516906738 + 1.0 * 6.385472297668457
Epoch 420, val loss: 0.842586874961853
Epoch 430, training loss: 6.921176433563232 = 0.5401463508605957 + 1.0 * 6.381030082702637
Epoch 430, val loss: 0.8296746015548706
Epoch 440, training loss: 6.893881797790527 = 0.5164167881011963 + 1.0 * 6.37746524810791
Epoch 440, val loss: 0.8180249333381653
Epoch 450, training loss: 6.871582508087158 = 0.4932440519332886 + 1.0 * 6.37833833694458
Epoch 450, val loss: 0.8075364828109741
Epoch 460, training loss: 6.850399971008301 = 0.4709240198135376 + 1.0 * 6.379476070404053
Epoch 460, val loss: 0.798173189163208
Epoch 470, training loss: 6.8215556144714355 = 0.449285089969635 + 1.0 * 6.372270584106445
Epoch 470, val loss: 0.7898557186126709
Epoch 480, training loss: 6.798984527587891 = 0.42827513813972473 + 1.0 * 6.370709419250488
Epoch 480, val loss: 0.782437801361084
Epoch 490, training loss: 6.776906490325928 = 0.4078092575073242 + 1.0 * 6.3690972328186035
Epoch 490, val loss: 0.7757627367973328
Epoch 500, training loss: 6.756134986877441 = 0.3879125118255615 + 1.0 * 6.368222236633301
Epoch 500, val loss: 0.7698245644569397
Epoch 510, training loss: 6.732985973358154 = 0.3685746192932129 + 1.0 * 6.364411354064941
Epoch 510, val loss: 0.7645214796066284
Epoch 520, training loss: 6.712324619293213 = 0.34976765513420105 + 1.0 * 6.3625569343566895
Epoch 520, val loss: 0.7598786354064941
Epoch 530, training loss: 6.691965103149414 = 0.3315412104129791 + 1.0 * 6.360424041748047
Epoch 530, val loss: 0.7558647394180298
Epoch 540, training loss: 6.672557830810547 = 0.31393298506736755 + 1.0 * 6.3586249351501465
Epoch 540, val loss: 0.7524749040603638
Epoch 550, training loss: 6.652457237243652 = 0.29692205786705017 + 1.0 * 6.35553503036499
Epoch 550, val loss: 0.7498096227645874
Epoch 560, training loss: 6.638450622558594 = 0.28050675988197327 + 1.0 * 6.357944011688232
Epoch 560, val loss: 0.7478689551353455
Epoch 570, training loss: 6.624195575714111 = 0.26485976576805115 + 1.0 * 6.359335899353027
Epoch 570, val loss: 0.7466046810150146
Epoch 580, training loss: 6.6017045974731445 = 0.24996566772460938 + 1.0 * 6.351738929748535
Epoch 580, val loss: 0.7461322546005249
Epoch 590, training loss: 6.584275245666504 = 0.23578403890132904 + 1.0 * 6.348491191864014
Epoch 590, val loss: 0.7465242147445679
Epoch 600, training loss: 6.580008029937744 = 0.22235620021820068 + 1.0 * 6.357651710510254
Epoch 600, val loss: 0.7477404475212097
Epoch 610, training loss: 6.557119846343994 = 0.2097269743680954 + 1.0 * 6.347393035888672
Epoch 610, val loss: 0.7496166229248047
Epoch 620, training loss: 6.558083534240723 = 0.19793710112571716 + 1.0 * 6.360146522521973
Epoch 620, val loss: 0.7521995902061462
Epoch 630, training loss: 6.531311511993408 = 0.18697822093963623 + 1.0 * 6.344333171844482
Epoch 630, val loss: 0.7552823424339294
Epoch 640, training loss: 6.5193562507629395 = 0.1767514944076538 + 1.0 * 6.342604637145996
Epoch 640, val loss: 0.7590045928955078
Epoch 650, training loss: 6.507287979125977 = 0.16719046235084534 + 1.0 * 6.340097427368164
Epoch 650, val loss: 0.763274073600769
Epoch 660, training loss: 6.496733665466309 = 0.15822653472423553 + 1.0 * 6.338507175445557
Epoch 660, val loss: 0.76804119348526
Epoch 670, training loss: 6.508357048034668 = 0.14983905851840973 + 1.0 * 6.358518123626709
Epoch 670, val loss: 0.7732211351394653
Epoch 680, training loss: 6.480986595153809 = 0.14206893742084503 + 1.0 * 6.3389177322387695
Epoch 680, val loss: 0.7785848379135132
Epoch 690, training loss: 6.470320701599121 = 0.13481022417545319 + 1.0 * 6.33551025390625
Epoch 690, val loss: 0.7843495011329651
Epoch 700, training loss: 6.461660385131836 = 0.12799620628356934 + 1.0 * 6.3336639404296875
Epoch 700, val loss: 0.7905072569847107
Epoch 710, training loss: 6.468916893005371 = 0.12160130590200424 + 1.0 * 6.347315788269043
Epoch 710, val loss: 0.7968701720237732
Epoch 720, training loss: 6.447324275970459 = 0.11564156413078308 + 1.0 * 6.3316826820373535
Epoch 720, val loss: 0.8032887578010559
Epoch 730, training loss: 6.4409379959106445 = 0.11004453897476196 + 1.0 * 6.330893516540527
Epoch 730, val loss: 0.8099870681762695
Epoch 740, training loss: 6.436196804046631 = 0.10477320104837418 + 1.0 * 6.331423759460449
Epoch 740, val loss: 0.8169204592704773
Epoch 750, training loss: 6.430581092834473 = 0.09981048107147217 + 1.0 * 6.330770492553711
Epoch 750, val loss: 0.8239754438400269
Epoch 760, training loss: 6.422926902770996 = 0.09514199942350388 + 1.0 * 6.327785015106201
Epoch 760, val loss: 0.8310806751251221
Epoch 770, training loss: 6.416722774505615 = 0.0907362550497055 + 1.0 * 6.325986385345459
Epoch 770, val loss: 0.8383654952049255
Epoch 780, training loss: 6.420872211456299 = 0.0865694209933281 + 1.0 * 6.33430290222168
Epoch 780, val loss: 0.8457860946655273
Epoch 790, training loss: 6.411918640136719 = 0.08266305923461914 + 1.0 * 6.3292555809021
Epoch 790, val loss: 0.8531999588012695
Epoch 800, training loss: 6.402227878570557 = 0.07896739989519119 + 1.0 * 6.323260307312012
Epoch 800, val loss: 0.8605929613113403
Epoch 810, training loss: 6.397365093231201 = 0.07547476887702942 + 1.0 * 6.321890354156494
Epoch 810, val loss: 0.8681943416595459
Epoch 820, training loss: 6.405170917510986 = 0.07216844707727432 + 1.0 * 6.33300256729126
Epoch 820, val loss: 0.8758145570755005
Epoch 830, training loss: 6.392477035522461 = 0.06904234737157822 + 1.0 * 6.323434829711914
Epoch 830, val loss: 0.883301317691803
Epoch 840, training loss: 6.385463237762451 = 0.06609169393777847 + 1.0 * 6.319371700286865
Epoch 840, val loss: 0.8908731341362
Epoch 850, training loss: 6.381472110748291 = 0.06328978389501572 + 1.0 * 6.318182468414307
Epoch 850, val loss: 0.8985319137573242
Epoch 860, training loss: 6.385091781616211 = 0.060636334121227264 + 1.0 * 6.324455261230469
Epoch 860, val loss: 0.9061513543128967
Epoch 870, training loss: 6.379695415496826 = 0.05811519920825958 + 1.0 * 6.321580410003662
Epoch 870, val loss: 0.9135876297950745
Epoch 880, training loss: 6.373079776763916 = 0.05575058236718178 + 1.0 * 6.317329406738281
Epoch 880, val loss: 0.920981228351593
Epoch 890, training loss: 6.370543956756592 = 0.05351110175251961 + 1.0 * 6.317032814025879
Epoch 890, val loss: 0.9283059239387512
Epoch 900, training loss: 6.364784240722656 = 0.05138735845685005 + 1.0 * 6.31339693069458
Epoch 900, val loss: 0.9356781244277954
Epoch 910, training loss: 6.368866443634033 = 0.049368929117918015 + 1.0 * 6.319497585296631
Epoch 910, val loss: 0.9429965615272522
Epoch 920, training loss: 6.358656883239746 = 0.04745594039559364 + 1.0 * 6.311201095581055
Epoch 920, val loss: 0.9502308964729309
Epoch 930, training loss: 6.356830596923828 = 0.045637574046850204 + 1.0 * 6.311192989349365
Epoch 930, val loss: 0.9574474096298218
Epoch 940, training loss: 6.357892990112305 = 0.04390650615096092 + 1.0 * 6.313986301422119
Epoch 940, val loss: 0.9646196365356445
Epoch 950, training loss: 6.3597893714904785 = 0.04226964712142944 + 1.0 * 6.317519664764404
Epoch 950, val loss: 0.9717062711715698
Epoch 960, training loss: 6.350776672363281 = 0.0407162681221962 + 1.0 * 6.310060501098633
Epoch 960, val loss: 0.9784528017044067
Epoch 970, training loss: 6.346434116363525 = 0.039246153086423874 + 1.0 * 6.307188034057617
Epoch 970, val loss: 0.9853968024253845
Epoch 980, training loss: 6.3448591232299805 = 0.037842072546482086 + 1.0 * 6.307016849517822
Epoch 980, val loss: 0.9923756718635559
Epoch 990, training loss: 6.357544422149658 = 0.03650251775979996 + 1.0 * 6.321042060852051
Epoch 990, val loss: 0.9992455244064331
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8355297838692674
The final CL Acc:0.80617, 0.01720, The final GNN Acc:0.83746, 0.00138
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9442])
updated graph: torch.Size([2, 10528])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.54687213897705 = 1.9501055479049683 + 1.0 * 8.596766471862793
Epoch 0, val loss: 1.9409295320510864
Epoch 10, training loss: 10.536218643188477 = 1.9398964643478394 + 1.0 * 8.596322059631348
Epoch 10, val loss: 1.931901216506958
Epoch 20, training loss: 10.520563125610352 = 1.9274353981018066 + 1.0 * 8.593127250671387
Epoch 20, val loss: 1.920624852180481
Epoch 30, training loss: 10.48336410522461 = 1.910255789756775 + 1.0 * 8.573108673095703
Epoch 30, val loss: 1.9049150943756104
Epoch 40, training loss: 10.365214347839355 = 1.8883569240570068 + 1.0 * 8.47685718536377
Epoch 40, val loss: 1.8854886293411255
Epoch 50, training loss: 10.00551986694336 = 1.8638545274734497 + 1.0 * 8.1416654586792
Epoch 50, val loss: 1.8635812997817993
Epoch 60, training loss: 9.636039733886719 = 1.839430570602417 + 1.0 * 7.796609401702881
Epoch 60, val loss: 1.8420685529708862
Epoch 70, training loss: 9.216391563415527 = 1.821388840675354 + 1.0 * 7.395002841949463
Epoch 70, val loss: 1.8258962631225586
Epoch 80, training loss: 8.963394165039062 = 1.8067255020141602 + 1.0 * 7.156668186187744
Epoch 80, val loss: 1.8125522136688232
Epoch 90, training loss: 8.802777290344238 = 1.791449785232544 + 1.0 * 7.011327743530273
Epoch 90, val loss: 1.7996268272399902
Epoch 100, training loss: 8.688084602355957 = 1.775183916091919 + 1.0 * 6.912900447845459
Epoch 100, val loss: 1.7862828969955444
Epoch 110, training loss: 8.599084854125977 = 1.7594215869903564 + 1.0 * 6.839663505554199
Epoch 110, val loss: 1.7725204229354858
Epoch 120, training loss: 8.530359268188477 = 1.7432630062103271 + 1.0 * 6.78709602355957
Epoch 120, val loss: 1.7576117515563965
Epoch 130, training loss: 8.4669771194458 = 1.725372076034546 + 1.0 * 6.741604804992676
Epoch 130, val loss: 1.7412198781967163
Epoch 140, training loss: 8.40803337097168 = 1.7049427032470703 + 1.0 * 6.703090190887451
Epoch 140, val loss: 1.723225474357605
Epoch 150, training loss: 8.352254867553711 = 1.6811509132385254 + 1.0 * 6.671104431152344
Epoch 150, val loss: 1.7030041217803955
Epoch 160, training loss: 8.29545783996582 = 1.6533904075622559 + 1.0 * 6.6420674324035645
Epoch 160, val loss: 1.679807186126709
Epoch 170, training loss: 8.23763370513916 = 1.6211744546890259 + 1.0 * 6.616458892822266
Epoch 170, val loss: 1.6527529954910278
Epoch 180, training loss: 8.17928409576416 = 1.5845056772232056 + 1.0 * 6.594778537750244
Epoch 180, val loss: 1.621931552886963
Epoch 190, training loss: 8.118279457092285 = 1.544066309928894 + 1.0 * 6.57421350479126
Epoch 190, val loss: 1.5879770517349243
Epoch 200, training loss: 8.05728816986084 = 1.5002562999725342 + 1.0 * 6.557031631469727
Epoch 200, val loss: 1.5513951778411865
Epoch 210, training loss: 7.997100353240967 = 1.454344391822815 + 1.0 * 6.542756080627441
Epoch 210, val loss: 1.5137282609939575
Epoch 220, training loss: 7.939716339111328 = 1.4080199003219604 + 1.0 * 6.531696319580078
Epoch 220, val loss: 1.4763381481170654
Epoch 230, training loss: 7.881743431091309 = 1.3626410961151123 + 1.0 * 6.519102096557617
Epoch 230, val loss: 1.4406527280807495
Epoch 240, training loss: 7.827375411987305 = 1.3184410333633423 + 1.0 * 6.508934497833252
Epoch 240, val loss: 1.406632423400879
Epoch 250, training loss: 7.774876594543457 = 1.275240182876587 + 1.0 * 6.499636173248291
Epoch 250, val loss: 1.3744860887527466
Epoch 260, training loss: 7.732420921325684 = 1.2328896522521973 + 1.0 * 6.499531269073486
Epoch 260, val loss: 1.3441686630249023
Epoch 270, training loss: 7.678476810455322 = 1.1921807527542114 + 1.0 * 6.4862961769104
Epoch 270, val loss: 1.3159362077713013
Epoch 280, training loss: 7.629326820373535 = 1.1525137424468994 + 1.0 * 6.476812839508057
Epoch 280, val loss: 1.2893017530441284
Epoch 290, training loss: 7.583740711212158 = 1.1134328842163086 + 1.0 * 6.47030782699585
Epoch 290, val loss: 1.263606071472168
Epoch 300, training loss: 7.544061660766602 = 1.0752623081207275 + 1.0 * 6.468799114227295
Epoch 300, val loss: 1.239090085029602
Epoch 310, training loss: 7.496591567993164 = 1.0383669137954712 + 1.0 * 6.458224773406982
Epoch 310, val loss: 1.2159361839294434
Epoch 320, training loss: 7.454438209533691 = 1.0025609731674194 + 1.0 * 6.451877117156982
Epoch 320, val loss: 1.1938809156417847
Epoch 330, training loss: 7.414021015167236 = 0.9677419662475586 + 1.0 * 6.446279048919678
Epoch 330, val loss: 1.1728010177612305
Epoch 340, training loss: 7.382492542266846 = 0.9340530633926392 + 1.0 * 6.448439598083496
Epoch 340, val loss: 1.1528170108795166
Epoch 350, training loss: 7.33851432800293 = 0.9016844034194946 + 1.0 * 6.436830043792725
Epoch 350, val loss: 1.1341047286987305
Epoch 360, training loss: 7.302900314331055 = 0.8701000809669495 + 1.0 * 6.43280029296875
Epoch 360, val loss: 1.1164147853851318
Epoch 370, training loss: 7.266725540161133 = 0.8389320373535156 + 1.0 * 6.427793502807617
Epoch 370, val loss: 1.0994728803634644
Epoch 380, training loss: 7.237319469451904 = 0.8081411123275757 + 1.0 * 6.429178237915039
Epoch 380, val loss: 1.0832765102386475
Epoch 390, training loss: 7.1998982429504395 = 0.7779610753059387 + 1.0 * 6.421936988830566
Epoch 390, val loss: 1.0681581497192383
Epoch 400, training loss: 7.165680408477783 = 0.7479281425476074 + 1.0 * 6.417752265930176
Epoch 400, val loss: 1.0538438558578491
Epoch 410, training loss: 7.131313323974609 = 0.717900812625885 + 1.0 * 6.413412570953369
Epoch 410, val loss: 1.0402145385742188
Epoch 420, training loss: 7.102238178253174 = 0.6878678202629089 + 1.0 * 6.414370536804199
Epoch 420, val loss: 1.0273709297180176
Epoch 430, training loss: 7.067291259765625 = 0.6580771803855896 + 1.0 * 6.409214019775391
Epoch 430, val loss: 1.015386939048767
Epoch 440, training loss: 7.034672737121582 = 0.6286236047744751 + 1.0 * 6.4060492515563965
Epoch 440, val loss: 1.004286766052246
Epoch 450, training loss: 7.002381801605225 = 0.5996832847595215 + 1.0 * 6.402698516845703
Epoch 450, val loss: 0.9942973852157593
Epoch 460, training loss: 6.970160484313965 = 0.5711880922317505 + 1.0 * 6.398972511291504
Epoch 460, val loss: 0.9854549765586853
Epoch 470, training loss: 6.948118209838867 = 0.5431667566299438 + 1.0 * 6.404951572418213
Epoch 470, val loss: 0.9777253270149231
Epoch 480, training loss: 6.912492752075195 = 0.5160562992095947 + 1.0 * 6.39643669128418
Epoch 480, val loss: 0.9712629318237305
Epoch 490, training loss: 6.88189172744751 = 0.4898920953273773 + 1.0 * 6.3919997215271
Epoch 490, val loss: 0.9661900401115417
Epoch 500, training loss: 6.854414939880371 = 0.4644317924976349 + 1.0 * 6.389983177185059
Epoch 500, val loss: 0.962310791015625
Epoch 510, training loss: 6.8316731452941895 = 0.43970945477485657 + 1.0 * 6.391963481903076
Epoch 510, val loss: 0.9597097635269165
Epoch 520, training loss: 6.804697036743164 = 0.41598010063171387 + 1.0 * 6.388717174530029
Epoch 520, val loss: 0.9582107067108154
Epoch 530, training loss: 6.777597904205322 = 0.39307957887649536 + 1.0 * 6.384518146514893
Epoch 530, val loss: 0.957778811454773
Epoch 540, training loss: 6.755499839782715 = 0.3711221516132355 + 1.0 * 6.384377479553223
Epoch 540, val loss: 0.9582094550132751
Epoch 550, training loss: 6.730558395385742 = 0.35013630986213684 + 1.0 * 6.380422115325928
Epoch 550, val loss: 0.959613025188446
Epoch 560, training loss: 6.707423210144043 = 0.32998400926589966 + 1.0 * 6.377439022064209
Epoch 560, val loss: 0.9617632627487183
Epoch 570, training loss: 6.6908769607543945 = 0.3106248378753662 + 1.0 * 6.380251884460449
Epoch 570, val loss: 0.9644655585289001
Epoch 580, training loss: 6.667750835418701 = 0.29220712184906006 + 1.0 * 6.375543594360352
Epoch 580, val loss: 0.9678657650947571
Epoch 590, training loss: 6.646625995635986 = 0.2745704650878906 + 1.0 * 6.372055530548096
Epoch 590, val loss: 0.9717053174972534
Epoch 600, training loss: 6.631564140319824 = 0.2577340602874756 + 1.0 * 6.3738298416137695
Epoch 600, val loss: 0.9758892059326172
Epoch 610, training loss: 6.612769603729248 = 0.24184906482696533 + 1.0 * 6.370920658111572
Epoch 610, val loss: 0.9805790781974792
Epoch 620, training loss: 6.594598293304443 = 0.22676633298397064 + 1.0 * 6.367832183837891
Epoch 620, val loss: 0.985501766204834
Epoch 630, training loss: 6.578573703765869 = 0.21244505047798157 + 1.0 * 6.366128444671631
Epoch 630, val loss: 0.9909473657608032
Epoch 640, training loss: 6.563129901885986 = 0.19894728064537048 + 1.0 * 6.364182472229004
Epoch 640, val loss: 0.9965980648994446
Epoch 650, training loss: 6.5563225746154785 = 0.18632470071315765 + 1.0 * 6.369997978210449
Epoch 650, val loss: 1.0025608539581299
Epoch 660, training loss: 6.536358833312988 = 0.17452429234981537 + 1.0 * 6.361834526062012
Epoch 660, val loss: 1.0087467432022095
Epoch 670, training loss: 6.5229973793029785 = 0.16349543631076813 + 1.0 * 6.359501838684082
Epoch 670, val loss: 1.0152733325958252
Epoch 680, training loss: 6.513708591461182 = 0.15318651497364044 + 1.0 * 6.360522270202637
Epoch 680, val loss: 1.0220696926116943
Epoch 690, training loss: 6.501369476318359 = 0.14361800253391266 + 1.0 * 6.357751369476318
Epoch 690, val loss: 1.0288313627243042
Epoch 700, training loss: 6.500134468078613 = 0.13476184010505676 + 1.0 * 6.365372657775879
Epoch 700, val loss: 1.0358963012695312
Epoch 710, training loss: 6.482059478759766 = 0.1266188621520996 + 1.0 * 6.355440616607666
Epoch 710, val loss: 1.043163537979126
Epoch 720, training loss: 6.471604347229004 = 0.11906248331069946 + 1.0 * 6.352541923522949
Epoch 720, val loss: 1.050571084022522
Epoch 730, training loss: 6.46774959564209 = 0.11206401139497757 + 1.0 * 6.355685710906982
Epoch 730, val loss: 1.0581797361373901
Epoch 740, training loss: 6.459074974060059 = 0.105618417263031 + 1.0 * 6.353456497192383
Epoch 740, val loss: 1.0658587217330933
Epoch 750, training loss: 6.4472432136535645 = 0.09966003149747849 + 1.0 * 6.347583293914795
Epoch 750, val loss: 1.0736980438232422
Epoch 760, training loss: 6.441638946533203 = 0.09412994980812073 + 1.0 * 6.347508907318115
Epoch 760, val loss: 1.081568956375122
Epoch 770, training loss: 6.436748027801514 = 0.08902229368686676 + 1.0 * 6.347725868225098
Epoch 770, val loss: 1.089453101158142
Epoch 780, training loss: 6.431894779205322 = 0.08431541919708252 + 1.0 * 6.347579479217529
Epoch 780, val loss: 1.097412347793579
Epoch 790, training loss: 6.423423767089844 = 0.07994663715362549 + 1.0 * 6.343477249145508
Epoch 790, val loss: 1.1053088903427124
Epoch 800, training loss: 6.41923713684082 = 0.07589113712310791 + 1.0 * 6.343346118927002
Epoch 800, val loss: 1.1133337020874023
Epoch 810, training loss: 6.417741298675537 = 0.0721263512969017 + 1.0 * 6.345614910125732
Epoch 810, val loss: 1.1212867498397827
Epoch 820, training loss: 6.411194324493408 = 0.06862760335206985 + 1.0 * 6.34256649017334
Epoch 820, val loss: 1.129064679145813
Epoch 830, training loss: 6.4038920402526855 = 0.06538574397563934 + 1.0 * 6.33850622177124
Epoch 830, val loss: 1.1370069980621338
Epoch 840, training loss: 6.399033069610596 = 0.06234654411673546 + 1.0 * 6.336686611175537
Epoch 840, val loss: 1.1448336839675903
Epoch 850, training loss: 6.4023613929748535 = 0.059507325291633606 + 1.0 * 6.342854022979736
Epoch 850, val loss: 1.152624249458313
Epoch 860, training loss: 6.393462181091309 = 0.05686292424798012 + 1.0 * 6.336599349975586
Epoch 860, val loss: 1.1603928804397583
Epoch 870, training loss: 6.38821268081665 = 0.05438705161213875 + 1.0 * 6.333825588226318
Epoch 870, val loss: 1.1681092977523804
Epoch 880, training loss: 6.389646530151367 = 0.052057795226573944 + 1.0 * 6.337588787078857
Epoch 880, val loss: 1.1757193803787231
Epoch 890, training loss: 6.3815813064575195 = 0.04988199844956398 + 1.0 * 6.331699371337891
Epoch 890, val loss: 1.1832658052444458
Epoch 900, training loss: 6.38199520111084 = 0.047833479940891266 + 1.0 * 6.334161758422852
Epoch 900, val loss: 1.1907334327697754
Epoch 910, training loss: 6.377272605895996 = 0.04591074585914612 + 1.0 * 6.331361770629883
Epoch 910, val loss: 1.1981019973754883
Epoch 920, training loss: 6.372310638427734 = 0.04410228878259659 + 1.0 * 6.3282084465026855
Epoch 920, val loss: 1.20551335811615
Epoch 930, training loss: 6.3718485832214355 = 0.04239347577095032 + 1.0 * 6.3294548988342285
Epoch 930, val loss: 1.2128393650054932
Epoch 940, training loss: 6.368229389190674 = 0.04077547416090965 + 1.0 * 6.327454090118408
Epoch 940, val loss: 1.2199008464813232
Epoch 950, training loss: 6.3645339012146 = 0.03925377503037453 + 1.0 * 6.32528018951416
Epoch 950, val loss: 1.2271509170532227
Epoch 960, training loss: 6.369936943054199 = 0.03780941665172577 + 1.0 * 6.332127571105957
Epoch 960, val loss: 1.2341114282608032
Epoch 970, training loss: 6.361931324005127 = 0.0364505760371685 + 1.0 * 6.325480937957764
Epoch 970, val loss: 1.2410310506820679
Epoch 980, training loss: 6.3574538230896 = 0.03515847772359848 + 1.0 * 6.322295188903809
Epoch 980, val loss: 1.2479408979415894
Epoch 990, training loss: 6.360929012298584 = 0.03393225744366646 + 1.0 * 6.326996803283691
Epoch 990, val loss: 1.2547316551208496
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 10.550503730773926 = 1.95366632938385 + 1.0 * 8.596837043762207
Epoch 0, val loss: 1.957861304283142
Epoch 10, training loss: 10.540444374084473 = 1.9438469409942627 + 1.0 * 8.596597671508789
Epoch 10, val loss: 1.9482171535491943
Epoch 20, training loss: 10.526846885681152 = 1.9317823648452759 + 1.0 * 8.595064163208008
Epoch 20, val loss: 1.9362326860427856
Epoch 30, training loss: 10.498395919799805 = 1.9149761199951172 + 1.0 * 8.583419799804688
Epoch 30, val loss: 1.9194585084915161
Epoch 40, training loss: 10.388727188110352 = 1.8921432495117188 + 1.0 * 8.496583938598633
Epoch 40, val loss: 1.8971673250198364
Epoch 50, training loss: 9.813231468200684 = 1.8675298690795898 + 1.0 * 7.945701599121094
Epoch 50, val loss: 1.87344491481781
Epoch 60, training loss: 9.424081802368164 = 1.8477671146392822 + 1.0 * 7.576314449310303
Epoch 60, val loss: 1.8548424243927002
Epoch 70, training loss: 9.07827377319336 = 1.8333600759506226 + 1.0 * 7.244913578033447
Epoch 70, val loss: 1.8412665128707886
Epoch 80, training loss: 8.882542610168457 = 1.818827748298645 + 1.0 * 7.063714504241943
Epoch 80, val loss: 1.8269983530044556
Epoch 90, training loss: 8.754227638244629 = 1.8015775680541992 + 1.0 * 6.95265007019043
Epoch 90, val loss: 1.8110953569412231
Epoch 100, training loss: 8.643811225891113 = 1.7847216129302979 + 1.0 * 6.859089374542236
Epoch 100, val loss: 1.7962234020233154
Epoch 110, training loss: 8.564542770385742 = 1.769089937210083 + 1.0 * 6.79545259475708
Epoch 110, val loss: 1.7818353176116943
Epoch 120, training loss: 8.498284339904785 = 1.7525787353515625 + 1.0 * 6.745705604553223
Epoch 120, val loss: 1.7661851644515991
Epoch 130, training loss: 8.434289932250977 = 1.7343273162841797 + 1.0 * 6.699963092803955
Epoch 130, val loss: 1.7492883205413818
Epoch 140, training loss: 8.378369331359863 = 1.7136125564575195 + 1.0 * 6.664756774902344
Epoch 140, val loss: 1.730852484703064
Epoch 150, training loss: 8.32790756225586 = 1.6896932125091553 + 1.0 * 6.638214588165283
Epoch 150, val loss: 1.7102092504501343
Epoch 160, training loss: 8.276826858520508 = 1.6622233390808105 + 1.0 * 6.6146039962768555
Epoch 160, val loss: 1.686674952507019
Epoch 170, training loss: 8.22669792175293 = 1.631149411201477 + 1.0 * 6.595548629760742
Epoch 170, val loss: 1.6600900888442993
Epoch 180, training loss: 8.173564910888672 = 1.5964425802230835 + 1.0 * 6.577122211456299
Epoch 180, val loss: 1.6304653882980347
Epoch 190, training loss: 8.119152069091797 = 1.558072805404663 + 1.0 * 6.561079025268555
Epoch 190, val loss: 1.5978820323944092
Epoch 200, training loss: 8.066617965698242 = 1.5169668197631836 + 1.0 * 6.549651622772217
Epoch 200, val loss: 1.563178300857544
Epoch 210, training loss: 8.011186599731445 = 1.4743980169296265 + 1.0 * 6.5367889404296875
Epoch 210, val loss: 1.5277316570281982
Epoch 220, training loss: 7.95595121383667 = 1.4311259984970093 + 1.0 * 6.524825096130371
Epoch 220, val loss: 1.49211585521698
Epoch 230, training loss: 7.904104232788086 = 1.387755274772644 + 1.0 * 6.516348838806152
Epoch 230, val loss: 1.4573287963867188
Epoch 240, training loss: 7.849648952484131 = 1.3452824354171753 + 1.0 * 6.504366397857666
Epoch 240, val loss: 1.424057126045227
Epoch 250, training loss: 7.799708366394043 = 1.3034875392913818 + 1.0 * 6.496220588684082
Epoch 250, val loss: 1.392438292503357
Epoch 260, training loss: 7.75010347366333 = 1.2622452974319458 + 1.0 * 6.487858295440674
Epoch 260, val loss: 1.3622965812683105
Epoch 270, training loss: 7.706384181976318 = 1.2216275930404663 + 1.0 * 6.4847564697265625
Epoch 270, val loss: 1.3336610794067383
Epoch 280, training loss: 7.657680511474609 = 1.1822983026504517 + 1.0 * 6.475382328033447
Epoch 280, val loss: 1.3068897724151611
Epoch 290, training loss: 7.612676620483398 = 1.144026279449463 + 1.0 * 6.4686503410339355
Epoch 290, val loss: 1.2813116312026978
Epoch 300, training loss: 7.568548202514648 = 1.1060075759887695 + 1.0 * 6.462540626525879
Epoch 300, val loss: 1.2563031911849976
Epoch 310, training loss: 7.5243144035339355 = 1.0681277513504028 + 1.0 * 6.456186771392822
Epoch 310, val loss: 1.2316194772720337
Epoch 320, training loss: 7.491886138916016 = 1.030428171157837 + 1.0 * 6.461458206176758
Epoch 320, val loss: 1.2071670293807983
Epoch 330, training loss: 7.444606781005859 = 0.9939332604408264 + 1.0 * 6.450673580169678
Epoch 330, val loss: 1.183527946472168
Epoch 340, training loss: 7.401273250579834 = 0.9584949612617493 + 1.0 * 6.44277811050415
Epoch 340, val loss: 1.160634994506836
Epoch 350, training loss: 7.361073017120361 = 0.9239063858985901 + 1.0 * 6.437166690826416
Epoch 350, val loss: 1.138290524482727
Epoch 360, training loss: 7.323235511779785 = 0.890150785446167 + 1.0 * 6.433084964752197
Epoch 360, val loss: 1.1165789365768433
Epoch 370, training loss: 7.289633274078369 = 0.8574463725090027 + 1.0 * 6.432187080383301
Epoch 370, val loss: 1.0958170890808105
Epoch 380, training loss: 7.253377437591553 = 0.8261222839355469 + 1.0 * 6.427255153656006
Epoch 380, val loss: 1.07588791847229
Epoch 390, training loss: 7.217593669891357 = 0.7957422137260437 + 1.0 * 6.421851634979248
Epoch 390, val loss: 1.0566511154174805
Epoch 400, training loss: 7.1931843757629395 = 0.7661973237991333 + 1.0 * 6.426987171173096
Epoch 400, val loss: 1.0381503105163574
Epoch 410, training loss: 7.15426778793335 = 0.7377071380615234 + 1.0 * 6.416560649871826
Epoch 410, val loss: 1.0204217433929443
Epoch 420, training loss: 7.121786594390869 = 0.709993839263916 + 1.0 * 6.411792755126953
Epoch 420, val loss: 1.0033291578292847
Epoch 430, training loss: 7.089846611022949 = 0.68284010887146 + 1.0 * 6.40700626373291
Epoch 430, val loss: 0.9867383241653442
Epoch 440, training loss: 7.061953544616699 = 0.6560152769088745 + 1.0 * 6.405938148498535
Epoch 440, val loss: 0.9705206751823425
Epoch 450, training loss: 7.039889335632324 = 0.6296249628067017 + 1.0 * 6.410264492034912
Epoch 450, val loss: 0.9548053741455078
Epoch 460, training loss: 7.002296447753906 = 0.6036874651908875 + 1.0 * 6.398609161376953
Epoch 460, val loss: 0.9395735859870911
Epoch 470, training loss: 6.9740705490112305 = 0.5778646469116211 + 1.0 * 6.396205902099609
Epoch 470, val loss: 0.9246671795845032
Epoch 480, training loss: 6.948977470397949 = 0.5519399642944336 + 1.0 * 6.397037506103516
Epoch 480, val loss: 0.9101477265357971
Epoch 490, training loss: 6.923174858093262 = 0.5260037183761597 + 1.0 * 6.3971710205078125
Epoch 490, val loss: 0.8961381912231445
Epoch 500, training loss: 6.889055252075195 = 0.5001552104949951 + 1.0 * 6.388899803161621
Epoch 500, val loss: 0.8826777935028076
Epoch 510, training loss: 6.864603042602539 = 0.474255234003067 + 1.0 * 6.390347957611084
Epoch 510, val loss: 0.8698744773864746
Epoch 520, training loss: 6.83804988861084 = 0.44861388206481934 + 1.0 * 6.389435768127441
Epoch 520, val loss: 0.8580148220062256
Epoch 530, training loss: 6.809019088745117 = 0.4234144687652588 + 1.0 * 6.385604381561279
Epoch 530, val loss: 0.8472535014152527
Epoch 540, training loss: 6.779637813568115 = 0.39878126978874207 + 1.0 * 6.380856513977051
Epoch 540, val loss: 0.8377118110656738
Epoch 550, training loss: 6.753657817840576 = 0.37490174174308777 + 1.0 * 6.378756046295166
Epoch 550, val loss: 0.8295463919639587
Epoch 560, training loss: 6.736691951751709 = 0.3520313501358032 + 1.0 * 6.384660720825195
Epoch 560, val loss: 0.8229352831840515
Epoch 570, training loss: 6.704644203186035 = 0.3304641842842102 + 1.0 * 6.374179840087891
Epoch 570, val loss: 0.8179490566253662
Epoch 580, training loss: 6.690774917602539 = 0.31021714210510254 + 1.0 * 6.380557537078857
Epoch 580, val loss: 0.8145279884338379
Epoch 590, training loss: 6.665543556213379 = 0.29128792881965637 + 1.0 * 6.374255657196045
Epoch 590, val loss: 0.8125072121620178
Epoch 600, training loss: 6.643084526062012 = 0.27362561225891113 + 1.0 * 6.36945915222168
Epoch 600, val loss: 0.8118254542350769
Epoch 610, training loss: 6.626176834106445 = 0.25706952810287476 + 1.0 * 6.369107246398926
Epoch 610, val loss: 0.8122705817222595
Epoch 620, training loss: 6.608822345733643 = 0.24170398712158203 + 1.0 * 6.3671183586120605
Epoch 620, val loss: 0.8136841654777527
Epoch 630, training loss: 6.592422008514404 = 0.22744935750961304 + 1.0 * 6.3649725914001465
Epoch 630, val loss: 0.8160266876220703
Epoch 640, training loss: 6.576997756958008 = 0.21415875852108002 + 1.0 * 6.362839221954346
Epoch 640, val loss: 0.8190907835960388
Epoch 650, training loss: 6.565972328186035 = 0.2017364203929901 + 1.0 * 6.364235877990723
Epoch 650, val loss: 0.8228188753128052
Epoch 660, training loss: 6.5564727783203125 = 0.19018852710723877 + 1.0 * 6.366284370422363
Epoch 660, val loss: 0.8269460201263428
Epoch 670, training loss: 6.537655830383301 = 0.1795029193162918 + 1.0 * 6.358152866363525
Epoch 670, val loss: 0.8316702246665955
Epoch 680, training loss: 6.52511739730835 = 0.16952593624591827 + 1.0 * 6.355591297149658
Epoch 680, val loss: 0.8367610573768616
Epoch 690, training loss: 6.52777624130249 = 0.16020677983760834 + 1.0 * 6.367569446563721
Epoch 690, val loss: 0.8421396017074585
Epoch 700, training loss: 6.506287097930908 = 0.15150339901447296 + 1.0 * 6.354783535003662
Epoch 700, val loss: 0.8478249907493591
Epoch 710, training loss: 6.496340274810791 = 0.14337316155433655 + 1.0 * 6.352967262268066
Epoch 710, val loss: 0.8536899089813232
Epoch 720, training loss: 6.488656044006348 = 0.13577520847320557 + 1.0 * 6.352880954742432
Epoch 720, val loss: 0.8596376180648804
Epoch 730, training loss: 6.4768781661987305 = 0.1286565065383911 + 1.0 * 6.348221778869629
Epoch 730, val loss: 0.865825355052948
Epoch 740, training loss: 6.470086574554443 = 0.1219739243388176 + 1.0 * 6.3481125831604
Epoch 740, val loss: 0.8721348643302917
Epoch 750, training loss: 6.468592166900635 = 0.11570069938898087 + 1.0 * 6.352891445159912
Epoch 750, val loss: 0.8784404397010803
Epoch 760, training loss: 6.454817771911621 = 0.10984492301940918 + 1.0 * 6.344973087310791
Epoch 760, val loss: 0.8848524689674377
Epoch 770, training loss: 6.449336051940918 = 0.10434872657060623 + 1.0 * 6.344987392425537
Epoch 770, val loss: 0.8913373947143555
Epoch 780, training loss: 6.447655200958252 = 0.09917449206113815 + 1.0 * 6.348480701446533
Epoch 780, val loss: 0.897803008556366
Epoch 790, training loss: 6.438670635223389 = 0.09431379288434982 + 1.0 * 6.344357013702393
Epoch 790, val loss: 0.9042565822601318
Epoch 800, training loss: 6.431757926940918 = 0.08975076675415039 + 1.0 * 6.342007160186768
Epoch 800, val loss: 0.9107090830802917
Epoch 810, training loss: 6.428430557250977 = 0.08545427769422531 + 1.0 * 6.342976093292236
Epoch 810, val loss: 0.9171711206436157
Epoch 820, training loss: 6.420778751373291 = 0.08142870664596558 + 1.0 * 6.33935022354126
Epoch 820, val loss: 0.9235304594039917
Epoch 830, training loss: 6.416860580444336 = 0.07764457911252975 + 1.0 * 6.339216232299805
Epoch 830, val loss: 0.9298619031906128
Epoch 840, training loss: 6.409437656402588 = 0.07408420741558075 + 1.0 * 6.335353374481201
Epoch 840, val loss: 0.9362526535987854
Epoch 850, training loss: 6.409736156463623 = 0.07071945071220398 + 1.0 * 6.339016914367676
Epoch 850, val loss: 0.9425415396690369
Epoch 860, training loss: 6.4008660316467285 = 0.06754810363054276 + 1.0 * 6.333317756652832
Epoch 860, val loss: 0.9488204121589661
Epoch 870, training loss: 6.399091720581055 = 0.06455934792757034 + 1.0 * 6.334532260894775
Epoch 870, val loss: 0.955016553401947
Epoch 880, training loss: 6.395058631896973 = 0.0617467425763607 + 1.0 * 6.333312034606934
Epoch 880, val loss: 0.9611628651618958
Epoch 890, training loss: 6.390239238739014 = 0.059090953320264816 + 1.0 * 6.331148147583008
Epoch 890, val loss: 0.9672166705131531
Epoch 900, training loss: 6.387068748474121 = 0.05658508464694023 + 1.0 * 6.330483436584473
Epoch 900, val loss: 0.9733160138130188
Epoch 910, training loss: 6.387002944946289 = 0.05421862751245499 + 1.0 * 6.332784175872803
Epoch 910, val loss: 0.9792398810386658
Epoch 920, training loss: 6.3804097175598145 = 0.051988646388053894 + 1.0 * 6.328421115875244
Epoch 920, val loss: 0.9850895404815674
Epoch 930, training loss: 6.3776679039001465 = 0.0498848594725132 + 1.0 * 6.327783107757568
Epoch 930, val loss: 0.9910003542900085
Epoch 940, training loss: 6.374032020568848 = 0.0478924885392189 + 1.0 * 6.326139450073242
Epoch 940, val loss: 0.9967734813690186
Epoch 950, training loss: 6.374455451965332 = 0.04600641503930092 + 1.0 * 6.328449249267578
Epoch 950, val loss: 1.0024882555007935
Epoch 960, training loss: 6.368685722351074 = 0.04422341287136078 + 1.0 * 6.324462413787842
Epoch 960, val loss: 1.008177638053894
Epoch 970, training loss: 6.374288082122803 = 0.042535457760095596 + 1.0 * 6.331752777099609
Epoch 970, val loss: 1.0137425661087036
Epoch 980, training loss: 6.366946220397949 = 0.04094334691762924 + 1.0 * 6.326003074645996
Epoch 980, val loss: 1.019160270690918
Epoch 990, training loss: 6.361656665802002 = 0.03944125026464462 + 1.0 * 6.322215557098389
Epoch 990, val loss: 1.0245552062988281
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 10.547996520996094 = 1.9511524438858032 + 1.0 * 8.596843719482422
Epoch 0, val loss: 1.9599690437316895
Epoch 10, training loss: 10.53754711151123 = 1.940892219543457 + 1.0 * 8.596654891967773
Epoch 10, val loss: 1.949141263961792
Epoch 20, training loss: 10.523604393005371 = 1.9282971620559692 + 1.0 * 8.595307350158691
Epoch 20, val loss: 1.9357905387878418
Epoch 30, training loss: 10.49616813659668 = 1.9106287956237793 + 1.0 * 8.585538864135742
Epoch 30, val loss: 1.9169689416885376
Epoch 40, training loss: 10.424155235290527 = 1.8860151767730713 + 1.0 * 8.538140296936035
Epoch 40, val loss: 1.8913995027542114
Epoch 50, training loss: 10.155350685119629 = 1.8589943647384644 + 1.0 * 8.296356201171875
Epoch 50, val loss: 1.864855170249939
Epoch 60, training loss: 9.838292121887207 = 1.833999752998352 + 1.0 * 8.004292488098145
Epoch 60, val loss: 1.8413782119750977
Epoch 70, training loss: 9.424057006835938 = 1.8146216869354248 + 1.0 * 7.609435558319092
Epoch 70, val loss: 1.8226548433303833
Epoch 80, training loss: 9.07219409942627 = 1.7999567985534668 + 1.0 * 7.272237300872803
Epoch 80, val loss: 1.8081810474395752
Epoch 90, training loss: 8.858643531799316 = 1.7844269275665283 + 1.0 * 7.074216365814209
Epoch 90, val loss: 1.7931816577911377
Epoch 100, training loss: 8.685783386230469 = 1.767982006072998 + 1.0 * 6.917801380157471
Epoch 100, val loss: 1.7774910926818848
Epoch 110, training loss: 8.584677696228027 = 1.751651406288147 + 1.0 * 6.83302640914917
Epoch 110, val loss: 1.761676549911499
Epoch 120, training loss: 8.50707721710205 = 1.7340290546417236 + 1.0 * 6.773047924041748
Epoch 120, val loss: 1.7449058294296265
Epoch 130, training loss: 8.435065269470215 = 1.7150458097457886 + 1.0 * 6.720019817352295
Epoch 130, val loss: 1.7275731563568115
Epoch 140, training loss: 8.374835014343262 = 1.6939440965652466 + 1.0 * 6.680891036987305
Epoch 140, val loss: 1.7089693546295166
Epoch 150, training loss: 8.32069206237793 = 1.669698715209961 + 1.0 * 6.650993347167969
Epoch 150, val loss: 1.6879299879074097
Epoch 160, training loss: 8.264243125915527 = 1.641998529434204 + 1.0 * 6.622244834899902
Epoch 160, val loss: 1.6640726327896118
Epoch 170, training loss: 8.210416793823242 = 1.6102498769760132 + 1.0 * 6.6001667976379395
Epoch 170, val loss: 1.636752963066101
Epoch 180, training loss: 8.16075611114502 = 1.5738683938980103 + 1.0 * 6.586887359619141
Epoch 180, val loss: 1.605567216873169
Epoch 190, training loss: 8.10114574432373 = 1.53325355052948 + 1.0 * 6.567892551422119
Epoch 190, val loss: 1.5708528757095337
Epoch 200, training loss: 8.040970802307129 = 1.4887417554855347 + 1.0 * 6.552229404449463
Epoch 200, val loss: 1.532956600189209
Epoch 210, training loss: 7.983035087585449 = 1.440510630607605 + 1.0 * 6.542524337768555
Epoch 210, val loss: 1.492356538772583
Epoch 220, training loss: 7.919219493865967 = 1.3902039527893066 + 1.0 * 6.52901554107666
Epoch 220, val loss: 1.4504948854446411
Epoch 230, training loss: 7.857277870178223 = 1.3386857509613037 + 1.0 * 6.51859188079834
Epoch 230, val loss: 1.408104658126831
Epoch 240, training loss: 7.79762077331543 = 1.2866852283477783 + 1.0 * 6.510935306549072
Epoch 240, val loss: 1.365943193435669
Epoch 250, training loss: 7.737487316131592 = 1.2359676361083984 + 1.0 * 6.501519680023193
Epoch 250, val loss: 1.3257508277893066
Epoch 260, training loss: 7.6792893409729 = 1.1874758005142212 + 1.0 * 6.491813659667969
Epoch 260, val loss: 1.287851095199585
Epoch 270, training loss: 7.6252899169921875 = 1.1409695148468018 + 1.0 * 6.484320640563965
Epoch 270, val loss: 1.2524501085281372
Epoch 280, training loss: 7.57489013671875 = 1.0968161821365356 + 1.0 * 6.478074073791504
Epoch 280, val loss: 1.2196099758148193
Epoch 290, training loss: 7.52787971496582 = 1.0557985305786133 + 1.0 * 6.472081184387207
Epoch 290, val loss: 1.1895242929458618
Epoch 300, training loss: 7.480832099914551 = 1.017408013343811 + 1.0 * 6.463424205780029
Epoch 300, val loss: 1.1621290445327759
Epoch 310, training loss: 7.44188117980957 = 0.9812390804290771 + 1.0 * 6.460641860961914
Epoch 310, val loss: 1.13667893409729
Epoch 320, training loss: 7.398408889770508 = 0.9469704627990723 + 1.0 * 6.4514384269714355
Epoch 320, val loss: 1.1128013134002686
Epoch 330, training loss: 7.363158702850342 = 0.9141436815261841 + 1.0 * 6.449015140533447
Epoch 330, val loss: 1.0898679494857788
Epoch 340, training loss: 7.323723793029785 = 0.882408857345581 + 1.0 * 6.441314697265625
Epoch 340, val loss: 1.0675334930419922
Epoch 350, training loss: 7.287529945373535 = 0.8513075113296509 + 1.0 * 6.436222553253174
Epoch 350, val loss: 1.045519471168518
Epoch 360, training loss: 7.2609477043151855 = 0.8207339644432068 + 1.0 * 6.440213680267334
Epoch 360, val loss: 1.0236514806747437
Epoch 370, training loss: 7.220353126525879 = 0.790675699710846 + 1.0 * 6.429677486419678
Epoch 370, val loss: 1.0022581815719604
Epoch 380, training loss: 7.186753749847412 = 0.7611196637153625 + 1.0 * 6.425633907318115
Epoch 380, val loss: 0.9811668395996094
Epoch 390, training loss: 7.152708530426025 = 0.7320253252983093 + 1.0 * 6.42068338394165
Epoch 390, val loss: 0.9605658650398254
Epoch 400, training loss: 7.12906551361084 = 0.7033160328865051 + 1.0 * 6.4257493019104
Epoch 400, val loss: 0.9406629204750061
Epoch 410, training loss: 7.091681957244873 = 0.6752371788024902 + 1.0 * 6.416444778442383
Epoch 410, val loss: 0.9217239022254944
Epoch 420, training loss: 7.059276580810547 = 0.6476354002952576 + 1.0 * 6.4116411209106445
Epoch 420, val loss: 0.9037992358207703
Epoch 430, training loss: 7.0345540046691895 = 0.6203436255455017 + 1.0 * 6.414210319519043
Epoch 430, val loss: 0.8869916796684265
Epoch 440, training loss: 7.002079486846924 = 0.5937156081199646 + 1.0 * 6.4083638191223145
Epoch 440, val loss: 0.8713257312774658
Epoch 450, training loss: 6.971588134765625 = 0.5674291253089905 + 1.0 * 6.404159069061279
Epoch 450, val loss: 0.8569375872612
Epoch 460, training loss: 6.942245960235596 = 0.5414850115776062 + 1.0 * 6.400761127471924
Epoch 460, val loss: 0.8436405062675476
Epoch 470, training loss: 6.915277481079102 = 0.5159825682640076 + 1.0 * 6.399294853210449
Epoch 470, val loss: 0.83159339427948
Epoch 480, training loss: 6.887266635894775 = 0.4911538064479828 + 1.0 * 6.39611291885376
Epoch 480, val loss: 0.8205059170722961
Epoch 490, training loss: 6.860363006591797 = 0.4668659269809723 + 1.0 * 6.393496990203857
Epoch 490, val loss: 0.8105639815330505
Epoch 500, training loss: 6.849404335021973 = 0.4431397020816803 + 1.0 * 6.406264781951904
Epoch 500, val loss: 0.8016341328620911
Epoch 510, training loss: 6.8102006912231445 = 0.4203985929489136 + 1.0 * 6.389801979064941
Epoch 510, val loss: 0.7936457991600037
Epoch 520, training loss: 6.786087512969971 = 0.3984052240848541 + 1.0 * 6.3876824378967285
Epoch 520, val loss: 0.786634624004364
Epoch 530, training loss: 6.771815776824951 = 0.3771744668483734 + 1.0 * 6.394641399383545
Epoch 530, val loss: 0.7806031107902527
Epoch 540, training loss: 6.74098014831543 = 0.35692599415779114 + 1.0 * 6.384054183959961
Epoch 540, val loss: 0.7754900455474854
Epoch 550, training loss: 6.719443321228027 = 0.337552547454834 + 1.0 * 6.381890773773193
Epoch 550, val loss: 0.7712487578392029
Epoch 560, training loss: 6.6980156898498535 = 0.3190307319164276 + 1.0 * 6.3789849281311035
Epoch 560, val loss: 0.7678946256637573
Epoch 570, training loss: 6.690542221069336 = 0.30140435695648193 + 1.0 * 6.3891377449035645
Epoch 570, val loss: 0.7653015851974487
Epoch 580, training loss: 6.66434907913208 = 0.28477951884269714 + 1.0 * 6.3795695304870605
Epoch 580, val loss: 0.7633544206619263
Epoch 590, training loss: 6.643627166748047 = 0.26905274391174316 + 1.0 * 6.374574184417725
Epoch 590, val loss: 0.7621899843215942
Epoch 600, training loss: 6.625884056091309 = 0.254164457321167 + 1.0 * 6.371719837188721
Epoch 600, val loss: 0.7618176341056824
Epoch 610, training loss: 6.616130828857422 = 0.24001765251159668 + 1.0 * 6.376113414764404
Epoch 610, val loss: 0.7620799541473389
Epoch 620, training loss: 6.604631423950195 = 0.2267971634864807 + 1.0 * 6.377834320068359
Epoch 620, val loss: 0.763026237487793
Epoch 630, training loss: 6.5841498374938965 = 0.21430149674415588 + 1.0 * 6.369848251342773
Epoch 630, val loss: 0.7642525434494019
Epoch 640, training loss: 6.569149494171143 = 0.2025046944618225 + 1.0 * 6.366644859313965
Epoch 640, val loss: 0.7662160992622375
Epoch 650, training loss: 6.557239055633545 = 0.19139616191387177 + 1.0 * 6.365842819213867
Epoch 650, val loss: 0.7685893774032593
Epoch 660, training loss: 6.545325756072998 = 0.18095681071281433 + 1.0 * 6.364368915557861
Epoch 660, val loss: 0.7712321877479553
Epoch 670, training loss: 6.537524700164795 = 0.17112474143505096 + 1.0 * 6.366399765014648
Epoch 670, val loss: 0.7743538022041321
Epoch 680, training loss: 6.522850513458252 = 0.16188019514083862 + 1.0 * 6.360970497131348
Epoch 680, val loss: 0.7777491211891174
Epoch 690, training loss: 6.511497974395752 = 0.15319183468818665 + 1.0 * 6.358305931091309
Epoch 690, val loss: 0.7813913226127625
Epoch 700, training loss: 6.505472660064697 = 0.14500296115875244 + 1.0 * 6.360469818115234
Epoch 700, val loss: 0.7853572368621826
Epoch 710, training loss: 6.494717121124268 = 0.1373189091682434 + 1.0 * 6.35739803314209
Epoch 710, val loss: 0.7894608378410339
Epoch 720, training loss: 6.48455810546875 = 0.13009916245937347 + 1.0 * 6.354458808898926
Epoch 720, val loss: 0.7936878204345703
Epoch 730, training loss: 6.479228496551514 = 0.12330900877714157 + 1.0 * 6.355919361114502
Epoch 730, val loss: 0.7981318831443787
Epoch 740, training loss: 6.469058990478516 = 0.11692040413618088 + 1.0 * 6.352138519287109
Epoch 740, val loss: 0.8027071952819824
Epoch 750, training loss: 6.471896648406982 = 0.11095095425844193 + 1.0 * 6.360945701599121
Epoch 750, val loss: 0.8072695136070251
Epoch 760, training loss: 6.459933280944824 = 0.10534065961837769 + 1.0 * 6.354592800140381
Epoch 760, val loss: 0.8118346929550171
Epoch 770, training loss: 6.449063301086426 = 0.1001063734292984 + 1.0 * 6.348957061767578
Epoch 770, val loss: 0.8164992928504944
Epoch 780, training loss: 6.440955638885498 = 0.09517460316419601 + 1.0 * 6.345780849456787
Epoch 780, val loss: 0.8213160037994385
Epoch 790, training loss: 6.435617923736572 = 0.09051863849163055 + 1.0 * 6.345099449157715
Epoch 790, val loss: 0.8261353969573975
Epoch 800, training loss: 6.44234037399292 = 0.08613569289445877 + 1.0 * 6.356204509735107
Epoch 800, val loss: 0.8310519456863403
Epoch 810, training loss: 6.424953460693359 = 0.08204083889722824 + 1.0 * 6.342912673950195
Epoch 810, val loss: 0.8358553051948547
Epoch 820, training loss: 6.423280715942383 = 0.07818913459777832 + 1.0 * 6.345091342926025
Epoch 820, val loss: 0.8407291173934937
Epoch 830, training loss: 6.415262699127197 = 0.0745636597275734 + 1.0 * 6.340699195861816
Epoch 830, val loss: 0.8456461429595947
Epoch 840, training loss: 6.411931991577148 = 0.07114355266094208 + 1.0 * 6.3407883644104
Epoch 840, val loss: 0.8505281805992126
Epoch 850, training loss: 6.419128894805908 = 0.06792788207530975 + 1.0 * 6.351201057434082
Epoch 850, val loss: 0.8554915189743042
Epoch 860, training loss: 6.403042793273926 = 0.06492579728364944 + 1.0 * 6.3381171226501465
Epoch 860, val loss: 0.8602141737937927
Epoch 870, training loss: 6.399242877960205 = 0.06209743395447731 + 1.0 * 6.3371453285217285
Epoch 870, val loss: 0.8650932312011719
Epoch 880, training loss: 6.394057273864746 = 0.05941839516162872 + 1.0 * 6.334639072418213
Epoch 880, val loss: 0.8699432015419006
Epoch 890, training loss: 6.410661697387695 = 0.05688675865530968 + 1.0 * 6.3537750244140625
Epoch 890, val loss: 0.8747106194496155
Epoch 900, training loss: 6.387919902801514 = 0.054518040269613266 + 1.0 * 6.333401679992676
Epoch 900, val loss: 0.8794964551925659
Epoch 910, training loss: 6.386097431182861 = 0.05228188633918762 + 1.0 * 6.333815574645996
Epoch 910, val loss: 0.8842265009880066
Epoch 920, training loss: 6.3803019523620605 = 0.05016450583934784 + 1.0 * 6.330137252807617
Epoch 920, val loss: 0.8889431357383728
Epoch 930, training loss: 6.378709316253662 = 0.048156555742025375 + 1.0 * 6.330552577972412
Epoch 930, val loss: 0.89359450340271
Epoch 940, training loss: 6.380816459655762 = 0.04626241698861122 + 1.0 * 6.334554195404053
Epoch 940, val loss: 0.8982815742492676
Epoch 950, training loss: 6.376108646392822 = 0.04447968676686287 + 1.0 * 6.331628799438477
Epoch 950, val loss: 0.9027462601661682
Epoch 960, training loss: 6.371322154998779 = 0.0427873432636261 + 1.0 * 6.3285346031188965
Epoch 960, val loss: 0.9072222709655762
Epoch 970, training loss: 6.367867469787598 = 0.04118472337722778 + 1.0 * 6.3266825675964355
Epoch 970, val loss: 0.9117301106452942
Epoch 980, training loss: 6.369966983795166 = 0.03966372087597847 + 1.0 * 6.330303192138672
Epoch 980, val loss: 0.9161689877510071
Epoch 990, training loss: 6.366305828094482 = 0.03822118416428566 + 1.0 * 6.328084468841553
Epoch 990, val loss: 0.920563280582428
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8112809699525567
The final CL Acc:0.76667, 0.04000, The final GNN Acc:0.80935, 0.00174
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 7862])
updated graph: torch.Size([2, 10458])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.535330772399902 = 1.9385185241699219 + 1.0 * 8.59681224822998
Epoch 0, val loss: 1.9432297945022583
Epoch 10, training loss: 10.52504825592041 = 1.928497314453125 + 1.0 * 8.596550941467285
Epoch 10, val loss: 1.93305504322052
Epoch 20, training loss: 10.5111722946167 = 1.9166315793991089 + 1.0 * 8.5945405960083
Epoch 20, val loss: 1.920837640762329
Epoch 30, training loss: 10.479035377502441 = 1.90053391456604 + 1.0 * 8.57850170135498
Epoch 30, val loss: 1.9043246507644653
Epoch 40, training loss: 10.336603164672852 = 1.8791841268539429 + 1.0 * 8.457419395446777
Epoch 40, val loss: 1.8829529285430908
Epoch 50, training loss: 9.781210899353027 = 1.8554887771606445 + 1.0 * 7.925722122192383
Epoch 50, val loss: 1.8597910404205322
Epoch 60, training loss: 9.337509155273438 = 1.8370121717453003 + 1.0 * 7.500497341156006
Epoch 60, val loss: 1.8416692018508911
Epoch 70, training loss: 8.979342460632324 = 1.8258259296417236 + 1.0 * 7.1535162925720215
Epoch 70, val loss: 1.8301193714141846
Epoch 80, training loss: 8.779668807983398 = 1.812648057937622 + 1.0 * 6.9670209884643555
Epoch 80, val loss: 1.8172639608383179
Epoch 90, training loss: 8.666109085083008 = 1.7977194786071777 + 1.0 * 6.868389129638672
Epoch 90, val loss: 1.802994728088379
Epoch 100, training loss: 8.571617126464844 = 1.7816678285598755 + 1.0 * 6.7899489402771
Epoch 100, val loss: 1.7879592180252075
Epoch 110, training loss: 8.495840072631836 = 1.7658857107162476 + 1.0 * 6.729954242706299
Epoch 110, val loss: 1.7730820178985596
Epoch 120, training loss: 8.432528495788574 = 1.7492707967758179 + 1.0 * 6.683257579803467
Epoch 120, val loss: 1.7573610544204712
Epoch 130, training loss: 8.376676559448242 = 1.7304538488388062 + 1.0 * 6.646223068237305
Epoch 130, val loss: 1.7400038242340088
Epoch 140, training loss: 8.32447624206543 = 1.70859694480896 + 1.0 * 6.615879535675049
Epoch 140, val loss: 1.7202857732772827
Epoch 150, training loss: 8.27278995513916 = 1.683058261871338 + 1.0 * 6.589731693267822
Epoch 150, val loss: 1.6978646516799927
Epoch 160, training loss: 8.22048568725586 = 1.6528215408325195 + 1.0 * 6.56766414642334
Epoch 160, val loss: 1.671507477760315
Epoch 170, training loss: 8.1672945022583 = 1.616615653038025 + 1.0 * 6.550678730010986
Epoch 170, val loss: 1.6401078701019287
Epoch 180, training loss: 8.108335494995117 = 1.574204683303833 + 1.0 * 6.534130573272705
Epoch 180, val loss: 1.6035001277923584
Epoch 190, training loss: 8.046321868896484 = 1.5255440473556519 + 1.0 * 6.520778179168701
Epoch 190, val loss: 1.561876654624939
Epoch 200, training loss: 7.979551792144775 = 1.4710556268692017 + 1.0 * 6.508496284484863
Epoch 200, val loss: 1.5156826972961426
Epoch 210, training loss: 7.91121768951416 = 1.4121055603027344 + 1.0 * 6.499112129211426
Epoch 210, val loss: 1.4664808511734009
Epoch 220, training loss: 7.839690685272217 = 1.3512911796569824 + 1.0 * 6.488399505615234
Epoch 220, val loss: 1.4168615341186523
Epoch 230, training loss: 7.770266532897949 = 1.2902183532714844 + 1.0 * 6.480048179626465
Epoch 230, val loss: 1.3681105375289917
Epoch 240, training loss: 7.70433235168457 = 1.231180191040039 + 1.0 * 6.473152160644531
Epoch 240, val loss: 1.3220301866531372
Epoch 250, training loss: 7.641108989715576 = 1.1755543947219849 + 1.0 * 6.465554714202881
Epoch 250, val loss: 1.279733657836914
Epoch 260, training loss: 7.582686424255371 = 1.1240265369415283 + 1.0 * 6.458659648895264
Epoch 260, val loss: 1.2413216829299927
Epoch 270, training loss: 7.529349327087402 = 1.0770939588546753 + 1.0 * 6.4522552490234375
Epoch 270, val loss: 1.2068206071853638
Epoch 280, training loss: 7.4809136390686035 = 1.0342799425125122 + 1.0 * 6.446633815765381
Epoch 280, val loss: 1.1757115125656128
Epoch 290, training loss: 7.435929775238037 = 0.9950369596481323 + 1.0 * 6.440892696380615
Epoch 290, val loss: 1.1474714279174805
Epoch 300, training loss: 7.393871307373047 = 0.9590111374855042 + 1.0 * 6.4348602294921875
Epoch 300, val loss: 1.1218042373657227
Epoch 310, training loss: 7.35518217086792 = 0.9256464242935181 + 1.0 * 6.429535865783691
Epoch 310, val loss: 1.0981565713882446
Epoch 320, training loss: 7.3284711837768555 = 0.8941619992256165 + 1.0 * 6.434309005737305
Epoch 320, val loss: 1.0760912895202637
Epoch 330, training loss: 7.285815238952637 = 0.8643097281455994 + 1.0 * 6.421505451202393
Epoch 330, val loss: 1.0553925037384033
Epoch 340, training loss: 7.252120018005371 = 0.8353306651115417 + 1.0 * 6.416789531707764
Epoch 340, val loss: 1.035482406616211
Epoch 350, training loss: 7.220087051391602 = 0.8067500591278076 + 1.0 * 6.413336753845215
Epoch 350, val loss: 1.0159660577774048
Epoch 360, training loss: 7.186953544616699 = 0.7782732248306274 + 1.0 * 6.408680438995361
Epoch 360, val loss: 0.996778130531311
Epoch 370, training loss: 7.15520715713501 = 0.7497535943984985 + 1.0 * 6.405453681945801
Epoch 370, val loss: 0.9778180122375488
Epoch 380, training loss: 7.123342037200928 = 0.7212694883346558 + 1.0 * 6.402072429656982
Epoch 380, val loss: 0.9590895771980286
Epoch 390, training loss: 7.094188690185547 = 0.6927692890167236 + 1.0 * 6.401419162750244
Epoch 390, val loss: 0.9407462477684021
Epoch 400, training loss: 7.065367221832275 = 0.6645370125770569 + 1.0 * 6.400830268859863
Epoch 400, val loss: 0.9229856133460999
Epoch 410, training loss: 7.0285162925720215 = 0.6368846893310547 + 1.0 * 6.391631603240967
Epoch 410, val loss: 0.905984103679657
Epoch 420, training loss: 6.998096942901611 = 0.6097065210342407 + 1.0 * 6.38839054107666
Epoch 420, val loss: 0.889816403388977
Epoch 430, training loss: 6.978236198425293 = 0.5830199122428894 + 1.0 * 6.395216464996338
Epoch 430, val loss: 0.8745832443237305
Epoch 440, training loss: 6.941810607910156 = 0.5572238564491272 + 1.0 * 6.384586811065674
Epoch 440, val loss: 0.8603415489196777
Epoch 450, training loss: 6.913535118103027 = 0.5321769118309021 + 1.0 * 6.3813581466674805
Epoch 450, val loss: 0.8471511006355286
Epoch 460, training loss: 6.886320114135742 = 0.5077313184738159 + 1.0 * 6.378588676452637
Epoch 460, val loss: 0.8348671197891235
Epoch 470, training loss: 6.864271640777588 = 0.4838796555995941 + 1.0 * 6.380392074584961
Epoch 470, val loss: 0.823464035987854
Epoch 480, training loss: 6.836352825164795 = 0.46077871322631836 + 1.0 * 6.375574111938477
Epoch 480, val loss: 0.8129496574401855
Epoch 490, training loss: 6.810245037078857 = 0.43820083141326904 + 1.0 * 6.372044086456299
Epoch 490, val loss: 0.8032034635543823
Epoch 500, training loss: 6.788132190704346 = 0.41601255536079407 + 1.0 * 6.372119426727295
Epoch 500, val loss: 0.7940930128097534
Epoch 510, training loss: 6.766880512237549 = 0.39430367946624756 + 1.0 * 6.372576713562012
Epoch 510, val loss: 0.7856292128562927
Epoch 520, training loss: 6.739131927490234 = 0.37308675050735474 + 1.0 * 6.366044998168945
Epoch 520, val loss: 0.7777987718582153
Epoch 530, training loss: 6.716594219207764 = 0.3522239327430725 + 1.0 * 6.364370346069336
Epoch 530, val loss: 0.7705318927764893
Epoch 540, training loss: 6.698162078857422 = 0.33184579014778137 + 1.0 * 6.366316318511963
Epoch 540, val loss: 0.7638434171676636
Epoch 550, training loss: 6.674415588378906 = 0.31209084391593933 + 1.0 * 6.3623247146606445
Epoch 550, val loss: 0.7577627301216125
Epoch 560, training loss: 6.6513495445251465 = 0.2929627001285553 + 1.0 * 6.358386993408203
Epoch 560, val loss: 0.7523631453514099
Epoch 570, training loss: 6.630539894104004 = 0.2745070159435272 + 1.0 * 6.356032848358154
Epoch 570, val loss: 0.7475880980491638
Epoch 580, training loss: 6.612588405609131 = 0.2567986249923706 + 1.0 * 6.355789661407471
Epoch 580, val loss: 0.7434983253479004
Epoch 590, training loss: 6.593782424926758 = 0.24000611901283264 + 1.0 * 6.353776454925537
Epoch 590, val loss: 0.7401361465454102
Epoch 600, training loss: 6.575352668762207 = 0.2241167426109314 + 1.0 * 6.351235866546631
Epoch 600, val loss: 0.7374995946884155
Epoch 610, training loss: 6.565937519073486 = 0.2091219127178192 + 1.0 * 6.356815814971924
Epoch 610, val loss: 0.7356067895889282
Epoch 620, training loss: 6.546713829040527 = 0.19516298174858093 + 1.0 * 6.351551055908203
Epoch 620, val loss: 0.7344505190849304
Epoch 630, training loss: 6.529238700866699 = 0.18220791220664978 + 1.0 * 6.3470306396484375
Epoch 630, val loss: 0.7339944839477539
Epoch 640, training loss: 6.513777256011963 = 0.1701381802558899 + 1.0 * 6.343638896942139
Epoch 640, val loss: 0.734161376953125
Epoch 650, training loss: 6.50303316116333 = 0.1588919311761856 + 1.0 * 6.344141006469727
Epoch 650, val loss: 0.7350216507911682
Epoch 660, training loss: 6.4899582862854 = 0.14849720895290375 + 1.0 * 6.341461181640625
Epoch 660, val loss: 0.7364290356636047
Epoch 670, training loss: 6.479687690734863 = 0.1389250010251999 + 1.0 * 6.340762615203857
Epoch 670, val loss: 0.7384074330329895
Epoch 680, training loss: 6.467563629150391 = 0.1300596296787262 + 1.0 * 6.337503910064697
Epoch 680, val loss: 0.7407925724983215
Epoch 690, training loss: 6.469894886016846 = 0.1218390166759491 + 1.0 * 6.348055839538574
Epoch 690, val loss: 0.7436040639877319
Epoch 700, training loss: 6.4519453048706055 = 0.11432900279760361 + 1.0 * 6.337616443634033
Epoch 700, val loss: 0.7467482089996338
Epoch 710, training loss: 6.4417853355407715 = 0.10741080343723297 + 1.0 * 6.33437442779541
Epoch 710, val loss: 0.7501768469810486
Epoch 720, training loss: 6.435053825378418 = 0.1010190099477768 + 1.0 * 6.3340349197387695
Epoch 720, val loss: 0.7538662552833557
Epoch 730, training loss: 6.427945137023926 = 0.09511737525463104 + 1.0 * 6.332827568054199
Epoch 730, val loss: 0.7578012347221375
Epoch 740, training loss: 6.4217119216918945 = 0.08965910226106644 + 1.0 * 6.332052707672119
Epoch 740, val loss: 0.7618817090988159
Epoch 750, training loss: 6.412883758544922 = 0.08462042361497879 + 1.0 * 6.328263282775879
Epoch 750, val loss: 0.7661423087120056
Epoch 760, training loss: 6.416513442993164 = 0.07994548976421356 + 1.0 * 6.3365678787231445
Epoch 760, val loss: 0.7705885171890259
Epoch 770, training loss: 6.403409004211426 = 0.07563523948192596 + 1.0 * 6.327773571014404
Epoch 770, val loss: 0.7750750780105591
Epoch 780, training loss: 6.396754264831543 = 0.07164141535758972 + 1.0 * 6.325112819671631
Epoch 780, val loss: 0.7797070145606995
Epoch 790, training loss: 6.393547058105469 = 0.06792804598808289 + 1.0 * 6.325619220733643
Epoch 790, val loss: 0.7844280004501343
Epoch 800, training loss: 6.389576435089111 = 0.06447792798280716 + 1.0 * 6.325098514556885
Epoch 800, val loss: 0.7892009615898132
Epoch 810, training loss: 6.380918502807617 = 0.061269693076610565 + 1.0 * 6.319648742675781
Epoch 810, val loss: 0.7939897775650024
Epoch 820, training loss: 6.379605770111084 = 0.058279674500226974 + 1.0 * 6.32132625579834
Epoch 820, val loss: 0.7988227605819702
Epoch 830, training loss: 6.374483585357666 = 0.055493392050266266 + 1.0 * 6.318990230560303
Epoch 830, val loss: 0.8036935925483704
Epoch 840, training loss: 6.372561454772949 = 0.05290410667657852 + 1.0 * 6.319657325744629
Epoch 840, val loss: 0.8085498809814453
Epoch 850, training loss: 6.365324974060059 = 0.05047951266169548 + 1.0 * 6.314845561981201
Epoch 850, val loss: 0.8133853673934937
Epoch 860, training loss: 6.36641263961792 = 0.048206280916929245 + 1.0 * 6.318206310272217
Epoch 860, val loss: 0.8181954622268677
Epoch 870, training loss: 6.360733509063721 = 0.04607861489057541 + 1.0 * 6.31465482711792
Epoch 870, val loss: 0.8230161070823669
Epoch 880, training loss: 6.361349582672119 = 0.044096749275922775 + 1.0 * 6.3172526359558105
Epoch 880, val loss: 0.8277919888496399
Epoch 890, training loss: 6.353435039520264 = 0.04223378002643585 + 1.0 * 6.311201095581055
Epoch 890, val loss: 0.8325462341308594
Epoch 900, training loss: 6.351868629455566 = 0.040484294295310974 + 1.0 * 6.311384201049805
Epoch 900, val loss: 0.8372535109519958
Epoch 910, training loss: 6.351529121398926 = 0.0388374887406826 + 1.0 * 6.312691688537598
Epoch 910, val loss: 0.841846227645874
Epoch 920, training loss: 6.347292900085449 = 0.03728768974542618 + 1.0 * 6.310005187988281
Epoch 920, val loss: 0.8464905619621277
Epoch 930, training loss: 6.3456196784973145 = 0.03583158925175667 + 1.0 * 6.309788227081299
Epoch 930, val loss: 0.8510224223136902
Epoch 940, training loss: 6.342116832733154 = 0.0344594344496727 + 1.0 * 6.307657241821289
Epoch 940, val loss: 0.8555223345756531
Epoch 950, training loss: 6.3438639640808105 = 0.03316020965576172 + 1.0 * 6.310703754425049
Epoch 950, val loss: 0.8599866032600403
Epoch 960, training loss: 6.336703300476074 = 0.03194243460893631 + 1.0 * 6.304760932922363
Epoch 960, val loss: 0.8643592000007629
Epoch 970, training loss: 6.334717750549316 = 0.030784638598561287 + 1.0 * 6.303933143615723
Epoch 970, val loss: 0.8687276840209961
Epoch 980, training loss: 6.33648157119751 = 0.02968600206077099 + 1.0 * 6.306795597076416
Epoch 980, val loss: 0.8729755878448486
Epoch 990, training loss: 6.331796646118164 = 0.028643883764743805 + 1.0 * 6.303152561187744
Epoch 990, val loss: 0.8772899508476257
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 10.540148735046387 = 1.9433454275131226 + 1.0 * 8.596803665161133
Epoch 0, val loss: 1.9423977136611938
Epoch 10, training loss: 10.52937126159668 = 1.932904839515686 + 1.0 * 8.596466064453125
Epoch 10, val loss: 1.9314197301864624
Epoch 20, training loss: 10.514089584350586 = 1.9202542304992676 + 1.0 * 8.59383487701416
Epoch 20, val loss: 1.9178807735443115
Epoch 30, training loss: 10.477495193481445 = 1.902937412261963 + 1.0 * 8.574557304382324
Epoch 30, val loss: 1.8994581699371338
Epoch 40, training loss: 10.34293270111084 = 1.8808133602142334 + 1.0 * 8.462119102478027
Epoch 40, val loss: 1.8771485090255737
Epoch 50, training loss: 9.932252883911133 = 1.857346773147583 + 1.0 * 8.074906349182129
Epoch 50, val loss: 1.8550302982330322
Epoch 60, training loss: 9.36269760131836 = 1.8394442796707153 + 1.0 * 7.523253440856934
Epoch 60, val loss: 1.8392084836959839
Epoch 70, training loss: 8.957208633422852 = 1.8275684118270874 + 1.0 * 7.129640102386475
Epoch 70, val loss: 1.8277465105056763
Epoch 80, training loss: 8.779897689819336 = 1.8137110471725464 + 1.0 * 6.9661865234375
Epoch 80, val loss: 1.8141981363296509
Epoch 90, training loss: 8.65029239654541 = 1.7971351146697998 + 1.0 * 6.8531575202941895
Epoch 90, val loss: 1.7988513708114624
Epoch 100, training loss: 8.556784629821777 = 1.7805570363998413 + 1.0 * 6.776227951049805
Epoch 100, val loss: 1.7838653326034546
Epoch 110, training loss: 8.480545043945312 = 1.764906644821167 + 1.0 * 6.715638637542725
Epoch 110, val loss: 1.7696462869644165
Epoch 120, training loss: 8.420589447021484 = 1.749090313911438 + 1.0 * 6.671499252319336
Epoch 120, val loss: 1.7549124956130981
Epoch 130, training loss: 8.365388870239258 = 1.7316747903823853 + 1.0 * 6.633713722229004
Epoch 130, val loss: 1.7386736869812012
Epoch 140, training loss: 8.31917953491211 = 1.7119076251983643 + 1.0 * 6.607271671295166
Epoch 140, val loss: 1.7205510139465332
Epoch 150, training loss: 8.268622398376465 = 1.689466118812561 + 1.0 * 6.579156398773193
Epoch 150, val loss: 1.7004507780075073
Epoch 160, training loss: 8.222784042358398 = 1.663671612739563 + 1.0 * 6.559112071990967
Epoch 160, val loss: 1.6775230169296265
Epoch 170, training loss: 8.175044059753418 = 1.6338876485824585 + 1.0 * 6.541156768798828
Epoch 170, val loss: 1.6511306762695312
Epoch 180, training loss: 8.127142906188965 = 1.5997917652130127 + 1.0 * 6.527350902557373
Epoch 180, val loss: 1.6212273836135864
Epoch 190, training loss: 8.075922012329102 = 1.5615941286087036 + 1.0 * 6.514327526092529
Epoch 190, val loss: 1.5878654718399048
Epoch 200, training loss: 8.021819114685059 = 1.5188117027282715 + 1.0 * 6.503007411956787
Epoch 200, val loss: 1.55068039894104
Epoch 210, training loss: 7.965481758117676 = 1.4718502759933472 + 1.0 * 6.493631362915039
Epoch 210, val loss: 1.5102217197418213
Epoch 220, training loss: 7.904593467712402 = 1.421321153640747 + 1.0 * 6.483272075653076
Epoch 220, val loss: 1.4671698808670044
Epoch 230, training loss: 7.845624923706055 = 1.3673992156982422 + 1.0 * 6.4782257080078125
Epoch 230, val loss: 1.4218204021453857
Epoch 240, training loss: 7.779498100280762 = 1.3113882541656494 + 1.0 * 6.468109607696533
Epoch 240, val loss: 1.375184178352356
Epoch 250, training loss: 7.714442729949951 = 1.2532600164413452 + 1.0 * 6.461182594299316
Epoch 250, val loss: 1.3271582126617432
Epoch 260, training loss: 7.654304504394531 = 1.193353533744812 + 1.0 * 6.46095085144043
Epoch 260, val loss: 1.2779595851898193
Epoch 270, training loss: 7.585783958435059 = 1.1329810619354248 + 1.0 * 6.452803134918213
Epoch 270, val loss: 1.2290109395980835
Epoch 280, training loss: 7.517818927764893 = 1.073012351989746 + 1.0 * 6.4448065757751465
Epoch 280, val loss: 1.1803803443908691
Epoch 290, training loss: 7.454274654388428 = 1.0141798257827759 + 1.0 * 6.440094947814941
Epoch 290, val loss: 1.132702112197876
Epoch 300, training loss: 7.395166397094727 = 0.9579788446426392 + 1.0 * 6.437187671661377
Epoch 300, val loss: 1.0872933864593506
Epoch 310, training loss: 7.335829257965088 = 0.905644416809082 + 1.0 * 6.430184841156006
Epoch 310, val loss: 1.0451792478561401
Epoch 320, training loss: 7.284557342529297 = 0.8573266267776489 + 1.0 * 6.4272308349609375
Epoch 320, val loss: 1.006556749343872
Epoch 330, training loss: 7.23599910736084 = 0.8134137392044067 + 1.0 * 6.422585487365723
Epoch 330, val loss: 0.9719775319099426
Epoch 340, training loss: 7.191145896911621 = 0.7736808061599731 + 1.0 * 6.4174652099609375
Epoch 340, val loss: 0.9414045810699463
Epoch 350, training loss: 7.155831813812256 = 0.737580418586731 + 1.0 * 6.4182515144348145
Epoch 350, val loss: 0.9145033359527588
Epoch 360, training loss: 7.115236282348633 = 0.7048428654670715 + 1.0 * 6.410393238067627
Epoch 360, val loss: 0.8910255432128906
Epoch 370, training loss: 7.080153465270996 = 0.6748281717300415 + 1.0 * 6.405325412750244
Epoch 370, val loss: 0.8704749345779419
Epoch 380, training loss: 7.051100730895996 = 0.6467924118041992 + 1.0 * 6.404308319091797
Epoch 380, val loss: 0.8521588444709778
Epoch 390, training loss: 7.028570175170898 = 0.6204719543457031 + 1.0 * 6.408098220825195
Epoch 390, val loss: 0.8359696269035339
Epoch 400, training loss: 6.993622303009033 = 0.5958852767944336 + 1.0 * 6.3977370262146
Epoch 400, val loss: 0.8217464685440063
Epoch 410, training loss: 6.964515686035156 = 0.572437047958374 + 1.0 * 6.392078876495361
Epoch 410, val loss: 0.8089931011199951
Epoch 420, training loss: 6.940287113189697 = 0.5498126745223999 + 1.0 * 6.390474319458008
Epoch 420, val loss: 0.7974581718444824
Epoch 430, training loss: 6.916699409484863 = 0.5279150605201721 + 1.0 * 6.388784408569336
Epoch 430, val loss: 0.7870607972145081
Epoch 440, training loss: 6.891570091247559 = 0.5067157745361328 + 1.0 * 6.384854316711426
Epoch 440, val loss: 0.7775474786758423
Epoch 450, training loss: 6.867326259613037 = 0.48598232865333557 + 1.0 * 6.381343841552734
Epoch 450, val loss: 0.7688618898391724
Epoch 460, training loss: 6.844634532928467 = 0.46557295322418213 + 1.0 * 6.379061698913574
Epoch 460, val loss: 0.7608221173286438
Epoch 470, training loss: 6.828455924987793 = 0.4455145001411438 + 1.0 * 6.382941246032715
Epoch 470, val loss: 0.7533100843429565
Epoch 480, training loss: 6.802918910980225 = 0.425870805978775 + 1.0 * 6.377048015594482
Epoch 480, val loss: 0.7464636564254761
Epoch 490, training loss: 6.778320789337158 = 0.40650179982185364 + 1.0 * 6.371819019317627
Epoch 490, val loss: 0.7400464415550232
Epoch 500, training loss: 6.771907329559326 = 0.38735684752464294 + 1.0 * 6.38455057144165
Epoch 500, val loss: 0.7340304851531982
Epoch 510, training loss: 6.73911714553833 = 0.3686787188053131 + 1.0 * 6.370438575744629
Epoch 510, val loss: 0.728486180305481
Epoch 520, training loss: 6.717053413391113 = 0.3503774106502533 + 1.0 * 6.366675853729248
Epoch 520, val loss: 0.7234053015708923
Epoch 530, training loss: 6.69675874710083 = 0.3324461579322815 + 1.0 * 6.364312648773193
Epoch 530, val loss: 0.7188201546669006
Epoch 540, training loss: 6.681362628936768 = 0.31493380665779114 + 1.0 * 6.366428852081299
Epoch 540, val loss: 0.7147821187973022
Epoch 550, training loss: 6.66012716293335 = 0.2979835867881775 + 1.0 * 6.362143516540527
Epoch 550, val loss: 0.7113410234451294
Epoch 560, training loss: 6.641722202301025 = 0.2815926671028137 + 1.0 * 6.360129356384277
Epoch 560, val loss: 0.7084286212921143
Epoch 570, training loss: 6.622588634490967 = 0.2658137381076813 + 1.0 * 6.356774806976318
Epoch 570, val loss: 0.7061500549316406
Epoch 580, training loss: 6.60691499710083 = 0.25072968006134033 + 1.0 * 6.356185436248779
Epoch 580, val loss: 0.7044439315795898
Epoch 590, training loss: 6.589898109436035 = 0.23633475601673126 + 1.0 * 6.35356330871582
Epoch 590, val loss: 0.7032699584960938
Epoch 600, training loss: 6.577999591827393 = 0.22262972593307495 + 1.0 * 6.355370044708252
Epoch 600, val loss: 0.7027384042739868
Epoch 610, training loss: 6.561979293823242 = 0.20977944135665894 + 1.0 * 6.352200031280518
Epoch 610, val loss: 0.7026930451393127
Epoch 620, training loss: 6.546473026275635 = 0.19767990708351135 + 1.0 * 6.348793029785156
Epoch 620, val loss: 0.703200101852417
Epoch 630, training loss: 6.53742790222168 = 0.18623992800712585 + 1.0 * 6.3511881828308105
Epoch 630, val loss: 0.7042721509933472
Epoch 640, training loss: 6.523543834686279 = 0.17550146579742432 + 1.0 * 6.3480424880981445
Epoch 640, val loss: 0.7057650089263916
Epoch 650, training loss: 6.511415481567383 = 0.1654413789510727 + 1.0 * 6.345973968505859
Epoch 650, val loss: 0.7078307271003723
Epoch 660, training loss: 6.499201774597168 = 0.15601609647274017 + 1.0 * 6.343185901641846
Epoch 660, val loss: 0.7102266550064087
Epoch 670, training loss: 6.490375995635986 = 0.14719168841838837 + 1.0 * 6.343184471130371
Epoch 670, val loss: 0.7131403088569641
Epoch 680, training loss: 6.480953216552734 = 0.13895626366138458 + 1.0 * 6.341997146606445
Epoch 680, val loss: 0.7164757251739502
Epoch 690, training loss: 6.4717559814453125 = 0.13128775358200073 + 1.0 * 6.340468406677246
Epoch 690, val loss: 0.7200310826301575
Epoch 700, training loss: 6.461606979370117 = 0.12410151958465576 + 1.0 * 6.337505340576172
Epoch 700, val loss: 0.7239858508110046
Epoch 710, training loss: 6.468674659729004 = 0.11739040911197662 + 1.0 * 6.351284027099609
Epoch 710, val loss: 0.7283028960227966
Epoch 720, training loss: 6.449544906616211 = 0.11114741861820221 + 1.0 * 6.33839750289917
Epoch 720, val loss: 0.7328372001647949
Epoch 730, training loss: 6.439658164978027 = 0.10533709079027176 + 1.0 * 6.334321022033691
Epoch 730, val loss: 0.7374392151832581
Epoch 740, training loss: 6.432024955749512 = 0.09988550841808319 + 1.0 * 6.332139492034912
Epoch 740, val loss: 0.7423070669174194
Epoch 750, training loss: 6.429903507232666 = 0.0947624072432518 + 1.0 * 6.335141181945801
Epoch 750, val loss: 0.747390627861023
Epoch 760, training loss: 6.420251846313477 = 0.08996177464723587 + 1.0 * 6.330289840698242
Epoch 760, val loss: 0.7527152895927429
Epoch 770, training loss: 6.4210333824157715 = 0.08548373728990555 + 1.0 * 6.335549831390381
Epoch 770, val loss: 0.7580951452255249
Epoch 780, training loss: 6.411282539367676 = 0.08127714693546295 + 1.0 * 6.330005168914795
Epoch 780, val loss: 0.7635514736175537
Epoch 790, training loss: 6.404269218444824 = 0.07734113931655884 + 1.0 * 6.32692813873291
Epoch 790, val loss: 0.7691126465797424
Epoch 800, training loss: 6.404030799865723 = 0.0736209899187088 + 1.0 * 6.330410003662109
Epoch 800, val loss: 0.7748965620994568
Epoch 810, training loss: 6.397776126861572 = 0.07013019174337387 + 1.0 * 6.327645778656006
Epoch 810, val loss: 0.7806776165962219
Epoch 820, training loss: 6.392416954040527 = 0.06685540825128555 + 1.0 * 6.3255615234375
Epoch 820, val loss: 0.7863836288452148
Epoch 830, training loss: 6.387668609619141 = 0.06376588344573975 + 1.0 * 6.323902606964111
Epoch 830, val loss: 0.7922133803367615
Epoch 840, training loss: 6.384898662567139 = 0.06085740402340889 + 1.0 * 6.324041366577148
Epoch 840, val loss: 0.7981637120246887
Epoch 850, training loss: 6.379762649536133 = 0.058121174573898315 + 1.0 * 6.321641445159912
Epoch 850, val loss: 0.8038597106933594
Epoch 860, training loss: 6.376770973205566 = 0.05553232505917549 + 1.0 * 6.3212385177612305
Epoch 860, val loss: 0.8096593618392944
Epoch 870, training loss: 6.374960899353027 = 0.05308891832828522 + 1.0 * 6.321871757507324
Epoch 870, val loss: 0.815589964389801
Epoch 880, training loss: 6.371038436889648 = 0.05079088360071182 + 1.0 * 6.320247650146484
Epoch 880, val loss: 0.8211411237716675
Epoch 890, training loss: 6.371448993682861 = 0.048617973923683167 + 1.0 * 6.322831153869629
Epoch 890, val loss: 0.8268207311630249
Epoch 900, training loss: 6.363816261291504 = 0.04657631739974022 + 1.0 * 6.317239761352539
Epoch 900, val loss: 0.8324186205863953
Epoch 910, training loss: 6.358983039855957 = 0.04463213309645653 + 1.0 * 6.3143510818481445
Epoch 910, val loss: 0.8379202485084534
Epoch 920, training loss: 6.361010551452637 = 0.042795587331056595 + 1.0 * 6.3182148933410645
Epoch 920, val loss: 0.8434610366821289
Epoch 930, training loss: 6.359847545623779 = 0.04106118530035019 + 1.0 * 6.318786144256592
Epoch 930, val loss: 0.8489669561386108
Epoch 940, training loss: 6.351154804229736 = 0.03942660614848137 + 1.0 * 6.311728000640869
Epoch 940, val loss: 0.8542989492416382
Epoch 950, training loss: 6.348950386047363 = 0.03787405043840408 + 1.0 * 6.3110761642456055
Epoch 950, val loss: 0.8596263527870178
Epoch 960, training loss: 6.3590474128723145 = 0.036407265812158585 + 1.0 * 6.3226399421691895
Epoch 960, val loss: 0.8650110960006714
Epoch 970, training loss: 6.343325138092041 = 0.0350240059196949 + 1.0 * 6.308300971984863
Epoch 970, val loss: 0.8701838254928589
Epoch 980, training loss: 6.342901229858398 = 0.03371667116880417 + 1.0 * 6.309184551239014
Epoch 980, val loss: 0.8750909566879272
Epoch 990, training loss: 6.339261054992676 = 0.03246762230992317 + 1.0 * 6.306793212890625
Epoch 990, val loss: 0.8802230954170227
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 10.552279472351074 = 1.955466389656067 + 1.0 * 8.596813201904297
Epoch 0, val loss: 1.9563257694244385
Epoch 10, training loss: 10.541773796081543 = 1.945279836654663 + 1.0 * 8.5964937210083
Epoch 10, val loss: 1.9465044736862183
Epoch 20, training loss: 10.526690483093262 = 1.9327318668365479 + 1.0 * 8.593958854675293
Epoch 20, val loss: 1.933752417564392
Epoch 30, training loss: 10.489962577819824 = 1.9151421785354614 + 1.0 * 8.574820518493652
Epoch 30, val loss: 1.9153566360473633
Epoch 40, training loss: 10.34676456451416 = 1.8924154043197632 + 1.0 * 8.454349517822266
Epoch 40, val loss: 1.8925660848617554
Epoch 50, training loss: 9.947446823120117 = 1.8686004877090454 + 1.0 * 8.078845977783203
Epoch 50, val loss: 1.8694584369659424
Epoch 60, training loss: 9.411731719970703 = 1.8493510484695435 + 1.0 * 7.562380790710449
Epoch 60, val loss: 1.8519858121871948
Epoch 70, training loss: 9.017333030700684 = 1.8344101905822754 + 1.0 * 7.182922840118408
Epoch 70, val loss: 1.8379364013671875
Epoch 80, training loss: 8.795536994934082 = 1.819941759109497 + 1.0 * 6.975594997406006
Epoch 80, val loss: 1.8235572576522827
Epoch 90, training loss: 8.654520034790039 = 1.8037282228469849 + 1.0 * 6.850791931152344
Epoch 90, val loss: 1.8079878091812134
Epoch 100, training loss: 8.557845115661621 = 1.7867305278778076 + 1.0 * 6.771114349365234
Epoch 100, val loss: 1.7924989461898804
Epoch 110, training loss: 8.484184265136719 = 1.7702407836914062 + 1.0 * 6.7139434814453125
Epoch 110, val loss: 1.7777001857757568
Epoch 120, training loss: 8.423989295959473 = 1.753889799118042 + 1.0 * 6.67009973526001
Epoch 120, val loss: 1.7627365589141846
Epoch 130, training loss: 8.373614311218262 = 1.7365094423294067 + 1.0 * 6.637104511260986
Epoch 130, val loss: 1.747099757194519
Epoch 140, training loss: 8.328285217285156 = 1.7172380685806274 + 1.0 * 6.611047267913818
Epoch 140, val loss: 1.73019540309906
Epoch 150, training loss: 8.281978607177734 = 1.695320963859558 + 1.0 * 6.586658000946045
Epoch 150, val loss: 1.711359977722168
Epoch 160, training loss: 8.240428924560547 = 1.670072317123413 + 1.0 * 6.570356845855713
Epoch 160, val loss: 1.6897257566452026
Epoch 170, training loss: 8.191431045532227 = 1.6409226655960083 + 1.0 * 6.550508499145508
Epoch 170, val loss: 1.664961576461792
Epoch 180, training loss: 8.141514778137207 = 1.6071516275405884 + 1.0 * 6.53436279296875
Epoch 180, val loss: 1.6363049745559692
Epoch 190, training loss: 8.089075088500977 = 1.5679808855056763 + 1.0 * 6.521093845367432
Epoch 190, val loss: 1.6030017137527466
Epoch 200, training loss: 8.032855987548828 = 1.522735357284546 + 1.0 * 6.510120391845703
Epoch 200, val loss: 1.5644582509994507
Epoch 210, training loss: 7.974300861358643 = 1.4716873168945312 + 1.0 * 6.502613544464111
Epoch 210, val loss: 1.5210987329483032
Epoch 220, training loss: 7.906524181365967 = 1.415561556816101 + 1.0 * 6.490962505340576
Epoch 220, val loss: 1.4734022617340088
Epoch 230, training loss: 7.84197998046875 = 1.3547580242156982 + 1.0 * 6.487222194671631
Epoch 230, val loss: 1.4216665029525757
Epoch 240, training loss: 7.767147064208984 = 1.2915540933609009 + 1.0 * 6.475593090057373
Epoch 240, val loss: 1.3676573038101196
Epoch 250, training loss: 7.69542121887207 = 1.2267284393310547 + 1.0 * 6.468692779541016
Epoch 250, val loss: 1.312639832496643
Epoch 260, training loss: 7.625044822692871 = 1.1617724895477295 + 1.0 * 6.463272571563721
Epoch 260, val loss: 1.2575371265411377
Epoch 270, training loss: 7.556197643280029 = 1.0983586311340332 + 1.0 * 6.457839012145996
Epoch 270, val loss: 1.2041693925857544
Epoch 280, training loss: 7.488799095153809 = 1.0377492904663086 + 1.0 * 6.4510498046875
Epoch 280, val loss: 1.153106689453125
Epoch 290, training loss: 7.425730228424072 = 0.9796845316886902 + 1.0 * 6.446045875549316
Epoch 290, val loss: 1.104435920715332
Epoch 300, training loss: 7.371667385101318 = 0.925175130367279 + 1.0 * 6.4464921951293945
Epoch 300, val loss: 1.058951735496521
Epoch 310, training loss: 7.30999755859375 = 0.8748646974563599 + 1.0 * 6.43513298034668
Epoch 310, val loss: 1.0175559520721436
Epoch 320, training loss: 7.25893497467041 = 0.8282864093780518 + 1.0 * 6.4306488037109375
Epoch 320, val loss: 0.979775071144104
Epoch 330, training loss: 7.2150678634643555 = 0.7852845191955566 + 1.0 * 6.429783344268799
Epoch 330, val loss: 0.9453431963920593
Epoch 340, training loss: 7.170376777648926 = 0.7460262775421143 + 1.0 * 6.424350261688232
Epoch 340, val loss: 0.9147787690162659
Epoch 350, training loss: 7.128525733947754 = 0.7101132273674011 + 1.0 * 6.418412685394287
Epoch 350, val loss: 0.8877586722373962
Epoch 360, training loss: 7.093277931213379 = 0.6769246459007263 + 1.0 * 6.416353225708008
Epoch 360, val loss: 0.8638248443603516
Epoch 370, training loss: 7.056917667388916 = 0.6461840867996216 + 1.0 * 6.410733699798584
Epoch 370, val loss: 0.8428453803062439
Epoch 380, training loss: 7.024357318878174 = 0.6173200011253357 + 1.0 * 6.407037258148193
Epoch 380, val loss: 0.8242811560630798
Epoch 390, training loss: 6.993300437927246 = 0.5902053117752075 + 1.0 * 6.403095245361328
Epoch 390, val loss: 0.807680606842041
Epoch 400, training loss: 6.963584899902344 = 0.5644999742507935 + 1.0 * 6.39908504486084
Epoch 400, val loss: 0.7931151986122131
Epoch 410, training loss: 6.936506748199463 = 0.5400585532188416 + 1.0 * 6.396448135375977
Epoch 410, val loss: 0.7800438404083252
Epoch 420, training loss: 6.913896560668945 = 0.5167339444160461 + 1.0 * 6.397162437438965
Epoch 420, val loss: 0.7684909105300903
Epoch 430, training loss: 6.886936664581299 = 0.4946916997432709 + 1.0 * 6.392244815826416
Epoch 430, val loss: 0.7583526372909546
Epoch 440, training loss: 6.86229133605957 = 0.47367843985557556 + 1.0 * 6.388612747192383
Epoch 440, val loss: 0.749485433101654
Epoch 450, training loss: 6.839306831359863 = 0.4535648822784424 + 1.0 * 6.3857421875
Epoch 450, val loss: 0.7418180108070374
Epoch 460, training loss: 6.816537857055664 = 0.43435823917388916 + 1.0 * 6.3821797370910645
Epoch 460, val loss: 0.7351378202438354
Epoch 470, training loss: 6.797171592712402 = 0.4159553349018097 + 1.0 * 6.381216049194336
Epoch 470, val loss: 0.7294571995735168
Epoch 480, training loss: 6.7756428718566895 = 0.3982856869697571 + 1.0 * 6.377357006072998
Epoch 480, val loss: 0.7246754169464111
Epoch 490, training loss: 6.782167911529541 = 0.3812926709651947 + 1.0 * 6.400875091552734
Epoch 490, val loss: 0.7207866907119751
Epoch 500, training loss: 6.741486549377441 = 0.36529070138931274 + 1.0 * 6.376195907592773
Epoch 500, val loss: 0.7176802754402161
Epoch 510, training loss: 6.721075057983398 = 0.3499176502227783 + 1.0 * 6.371157646179199
Epoch 510, val loss: 0.7154845595359802
Epoch 520, training loss: 6.703850746154785 = 0.33502352237701416 + 1.0 * 6.3688273429870605
Epoch 520, val loss: 0.7138425707817078
Epoch 530, training loss: 6.691180229187012 = 0.32061296701431274 + 1.0 * 6.370567321777344
Epoch 530, val loss: 0.7127928137779236
Epoch 540, training loss: 6.670956134796143 = 0.306768536567688 + 1.0 * 6.364187717437744
Epoch 540, val loss: 0.712440013885498
Epoch 550, training loss: 6.655156135559082 = 0.2933189570903778 + 1.0 * 6.361837387084961
Epoch 550, val loss: 0.7126424908638
Epoch 560, training loss: 6.6437177658081055 = 0.28030386567115784 + 1.0 * 6.3634138107299805
Epoch 560, val loss: 0.7133583426475525
Epoch 570, training loss: 6.6417036056518555 = 0.2676854133605957 + 1.0 * 6.37401819229126
Epoch 570, val loss: 0.7145896553993225
Epoch 580, training loss: 6.614446640014648 = 0.2556689977645874 + 1.0 * 6.3587775230407715
Epoch 580, val loss: 0.7163744568824768
Epoch 590, training loss: 6.599491119384766 = 0.2440498024225235 + 1.0 * 6.355441093444824
Epoch 590, val loss: 0.7186064720153809
Epoch 600, training loss: 6.5874762535095215 = 0.23281504213809967 + 1.0 * 6.354660987854004
Epoch 600, val loss: 0.7212660312652588
Epoch 610, training loss: 6.574652194976807 = 0.22201603651046753 + 1.0 * 6.352636337280273
Epoch 610, val loss: 0.724273145198822
Epoch 620, training loss: 6.5638861656188965 = 0.21169884502887726 + 1.0 * 6.352187156677246
Epoch 620, val loss: 0.727860689163208
Epoch 630, training loss: 6.5524678230285645 = 0.20178969204425812 + 1.0 * 6.350677967071533
Epoch 630, val loss: 0.7317982912063599
Epoch 640, training loss: 6.5569071769714355 = 0.19228307902812958 + 1.0 * 6.3646240234375
Epoch 640, val loss: 0.7360647916793823
Epoch 650, training loss: 6.532018184661865 = 0.18318206071853638 + 1.0 * 6.3488359451293945
Epoch 650, val loss: 0.7406948804855347
Epoch 660, training loss: 6.520729064941406 = 0.17451365292072296 + 1.0 * 6.34621524810791
Epoch 660, val loss: 0.7456732988357544
Epoch 670, training loss: 6.511416912078857 = 0.16619649529457092 + 1.0 * 6.345220565795898
Epoch 670, val loss: 0.7509467005729675
Epoch 680, training loss: 6.512375831604004 = 0.15826767683029175 + 1.0 * 6.3541083335876465
Epoch 680, val loss: 0.7565335035324097
Epoch 690, training loss: 6.493475914001465 = 0.15070344507694244 + 1.0 * 6.342772483825684
Epoch 690, val loss: 0.7622987031936646
Epoch 700, training loss: 6.484664440155029 = 0.1435122787952423 + 1.0 * 6.341152191162109
Epoch 700, val loss: 0.76844322681427
Epoch 710, training loss: 6.480750560760498 = 0.13667190074920654 + 1.0 * 6.344078540802002
Epoch 710, val loss: 0.7746449708938599
Epoch 720, training loss: 6.476700305938721 = 0.13015791773796082 + 1.0 * 6.3465423583984375
Epoch 720, val loss: 0.7808859348297119
Epoch 730, training loss: 6.460773468017578 = 0.12403226643800735 + 1.0 * 6.336740970611572
Epoch 730, val loss: 0.7875514626502991
Epoch 740, training loss: 6.4545369148254395 = 0.11819545924663544 + 1.0 * 6.336341381072998
Epoch 740, val loss: 0.7941548824310303
Epoch 750, training loss: 6.447057247161865 = 0.11263788491487503 + 1.0 * 6.334419250488281
Epoch 750, val loss: 0.8009347915649414
Epoch 760, training loss: 6.454855918884277 = 0.10735978186130524 + 1.0 * 6.347496032714844
Epoch 760, val loss: 0.8077871203422546
Epoch 770, training loss: 6.442170143127441 = 0.10240180045366287 + 1.0 * 6.339768409729004
Epoch 770, val loss: 0.8147751092910767
Epoch 780, training loss: 6.431447982788086 = 0.09770005196332932 + 1.0 * 6.333747863769531
Epoch 780, val loss: 0.8217557668685913
Epoch 790, training loss: 6.423465251922607 = 0.09325964003801346 + 1.0 * 6.33020544052124
Epoch 790, val loss: 0.8288335204124451
Epoch 800, training loss: 6.418493747711182 = 0.08905191719532013 + 1.0 * 6.329442024230957
Epoch 800, val loss: 0.8359428644180298
Epoch 810, training loss: 6.423216342926025 = 0.08507052063941956 + 1.0 * 6.338145732879639
Epoch 810, val loss: 0.8430871963500977
Epoch 820, training loss: 6.413591384887695 = 0.0813009962439537 + 1.0 * 6.332290172576904
Epoch 820, val loss: 0.8502289652824402
Epoch 830, training loss: 6.405589580535889 = 0.07775293290615082 + 1.0 * 6.327836513519287
Epoch 830, val loss: 0.8572500944137573
Epoch 840, training loss: 6.400200843811035 = 0.07440333068370819 + 1.0 * 6.3257975578308105
Epoch 840, val loss: 0.8643251061439514
Epoch 850, training loss: 6.396113395690918 = 0.07122700661420822 + 1.0 * 6.324886322021484
Epoch 850, val loss: 0.8712940216064453
Epoch 860, training loss: 6.402646064758301 = 0.06821610033512115 + 1.0 * 6.334429740905762
Epoch 860, val loss: 0.878230094909668
Epoch 870, training loss: 6.392146587371826 = 0.06537631154060364 + 1.0 * 6.326770305633545
Epoch 870, val loss: 0.8851256966590881
Epoch 880, training loss: 6.385167598724365 = 0.06264816969633102 + 1.0 * 6.322519302368164
Epoch 880, val loss: 0.8921024203300476
Epoch 890, training loss: 6.382179260253906 = 0.06006106734275818 + 1.0 * 6.322118282318115
Epoch 890, val loss: 0.8988797664642334
Epoch 900, training loss: 6.382316589355469 = 0.057603124529123306 + 1.0 * 6.324713230133057
Epoch 900, val loss: 0.9055859446525574
Epoch 910, training loss: 6.375464916229248 = 0.055257976055145264 + 1.0 * 6.320207118988037
Epoch 910, val loss: 0.9123408794403076
Epoch 920, training loss: 6.376052379608154 = 0.05304364860057831 + 1.0 * 6.3230085372924805
Epoch 920, val loss: 0.9189218282699585
Epoch 930, training loss: 6.3710408210754395 = 0.050963450223207474 + 1.0 * 6.320077419281006
Epoch 930, val loss: 0.9254412651062012
Epoch 940, training loss: 6.367347240447998 = 0.04898461326956749 + 1.0 * 6.318362712860107
Epoch 940, val loss: 0.9319292306900024
Epoch 950, training loss: 6.369614601135254 = 0.047113701701164246 + 1.0 * 6.322500705718994
Epoch 950, val loss: 0.9382561445236206
Epoch 960, training loss: 6.362270832061768 = 0.045346327126026154 + 1.0 * 6.316924571990967
Epoch 960, val loss: 0.9446202516555786
Epoch 970, training loss: 6.360019683837891 = 0.04365910589694977 + 1.0 * 6.3163604736328125
Epoch 970, val loss: 0.9509449005126953
Epoch 980, training loss: 6.360514163970947 = 0.04206160455942154 + 1.0 * 6.31845235824585
Epoch 980, val loss: 0.956947922706604
Epoch 990, training loss: 6.353945732116699 = 0.040532760322093964 + 1.0 * 6.313413143157959
Epoch 990, val loss: 0.9630638957023621
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8413284132841329
The final CL Acc:0.80000, 0.00605, The final GNN Acc:0.83869, 0.00197
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10534])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.564245223999023 = 1.967461347579956 + 1.0 * 8.596783638000488
Epoch 0, val loss: 1.9699056148529053
Epoch 10, training loss: 10.552441596984863 = 1.956022024154663 + 1.0 * 8.596419334411621
Epoch 10, val loss: 1.9588943719863892
Epoch 20, training loss: 10.535614967346191 = 1.9418823719024658 + 1.0 * 8.593732833862305
Epoch 20, val loss: 1.9448832273483276
Epoch 30, training loss: 10.498029708862305 = 1.9224380254745483 + 1.0 * 8.575592041015625
Epoch 30, val loss: 1.9253811836242676
Epoch 40, training loss: 10.370841026306152 = 1.8976552486419678 + 1.0 * 8.473185539245605
Epoch 40, val loss: 1.9016072750091553
Epoch 50, training loss: 9.862364768981934 = 1.8712798357009888 + 1.0 * 7.991085052490234
Epoch 50, val loss: 1.8766306638717651
Epoch 60, training loss: 9.342199325561523 = 1.8517951965332031 + 1.0 * 7.4904046058654785
Epoch 60, val loss: 1.8587716817855835
Epoch 70, training loss: 9.012166976928711 = 1.8375701904296875 + 1.0 * 7.174596786499023
Epoch 70, val loss: 1.8454045057296753
Epoch 80, training loss: 8.834885597229004 = 1.8230139017105103 + 1.0 * 7.011871337890625
Epoch 80, val loss: 1.831922173500061
Epoch 90, training loss: 8.724085807800293 = 1.8071213960647583 + 1.0 * 6.916964530944824
Epoch 90, val loss: 1.8170262575149536
Epoch 100, training loss: 8.637103080749512 = 1.7913191318511963 + 1.0 * 6.845783710479736
Epoch 100, val loss: 1.8018805980682373
Epoch 110, training loss: 8.575315475463867 = 1.776918888092041 + 1.0 * 6.798396587371826
Epoch 110, val loss: 1.7877483367919922
Epoch 120, training loss: 8.52409553527832 = 1.763472557067871 + 1.0 * 6.760622501373291
Epoch 120, val loss: 1.7745699882507324
Epoch 130, training loss: 8.473824501037598 = 1.7493699789047241 + 1.0 * 6.724454402923584
Epoch 130, val loss: 1.761014699935913
Epoch 140, training loss: 8.42868709564209 = 1.733864188194275 + 1.0 * 6.694823265075684
Epoch 140, val loss: 1.7466143369674683
Epoch 150, training loss: 8.376577377319336 = 1.7163678407669067 + 1.0 * 6.660209655761719
Epoch 150, val loss: 1.7310054302215576
Epoch 160, training loss: 8.329376220703125 = 1.6961331367492676 + 1.0 * 6.633243560791016
Epoch 160, val loss: 1.7134592533111572
Epoch 170, training loss: 8.283239364624023 = 1.6721696853637695 + 1.0 * 6.611069679260254
Epoch 170, val loss: 1.6926963329315186
Epoch 180, training loss: 8.231962203979492 = 1.643880844116211 + 1.0 * 6.5880818367004395
Epoch 180, val loss: 1.6682096719741821
Epoch 190, training loss: 8.179122924804688 = 1.6104824542999268 + 1.0 * 6.56864070892334
Epoch 190, val loss: 1.6392403841018677
Epoch 200, training loss: 8.124458312988281 = 1.5713109970092773 + 1.0 * 6.553147315979004
Epoch 200, val loss: 1.6054913997650146
Epoch 210, training loss: 8.065455436706543 = 1.5270602703094482 + 1.0 * 6.538395404815674
Epoch 210, val loss: 1.5679395198822021
Epoch 220, training loss: 8.006695747375488 = 1.4793736934661865 + 1.0 * 6.527321815490723
Epoch 220, val loss: 1.5282363891601562
Epoch 230, training loss: 7.943342208862305 = 1.4294124841690063 + 1.0 * 6.513929843902588
Epoch 230, val loss: 1.4876227378845215
Epoch 240, training loss: 7.880981922149658 = 1.3782309293746948 + 1.0 * 6.502750873565674
Epoch 240, val loss: 1.4468064308166504
Epoch 250, training loss: 7.821517467498779 = 1.3276554346084595 + 1.0 * 6.493862152099609
Epoch 250, val loss: 1.4075927734375
Epoch 260, training loss: 7.763761043548584 = 1.2790874242782593 + 1.0 * 6.484673500061035
Epoch 260, val loss: 1.3706835508346558
Epoch 270, training loss: 7.71002197265625 = 1.2322770357131958 + 1.0 * 6.477745056152344
Epoch 270, val loss: 1.3359601497650146
Epoch 280, training loss: 7.658236980438232 = 1.187384009361267 + 1.0 * 6.470852851867676
Epoch 280, val loss: 1.3034272193908691
Epoch 290, training loss: 7.605289459228516 = 1.1438989639282227 + 1.0 * 6.461390495300293
Epoch 290, val loss: 1.2722430229187012
Epoch 300, training loss: 7.554819107055664 = 1.1008650064468384 + 1.0 * 6.453954219818115
Epoch 300, val loss: 1.2417770624160767
Epoch 310, training loss: 7.508458614349365 = 1.0582503080368042 + 1.0 * 6.4502081871032715
Epoch 310, val loss: 1.2120929956436157
Epoch 320, training loss: 7.46141242980957 = 1.0164445638656616 + 1.0 * 6.444967746734619
Epoch 320, val loss: 1.1830592155456543
Epoch 330, training loss: 7.411920547485352 = 0.9748954772949219 + 1.0 * 6.43702507019043
Epoch 330, val loss: 1.1543216705322266
Epoch 340, training loss: 7.365626335144043 = 0.9337269067764282 + 1.0 * 6.431899547576904
Epoch 340, val loss: 1.125916600227356
Epoch 350, training loss: 7.319864273071289 = 0.8932454586029053 + 1.0 * 6.426619052886963
Epoch 350, val loss: 1.098387598991394
Epoch 360, training loss: 7.286310195922852 = 0.8535699844360352 + 1.0 * 6.432740211486816
Epoch 360, val loss: 1.0716336965560913
Epoch 370, training loss: 7.238638401031494 = 0.815758228302002 + 1.0 * 6.422880172729492
Epoch 370, val loss: 1.046582579612732
Epoch 380, training loss: 7.193442344665527 = 0.7795394659042358 + 1.0 * 6.413902759552002
Epoch 380, val loss: 1.0231947898864746
Epoch 390, training loss: 7.154228210449219 = 0.7444344758987427 + 1.0 * 6.409793853759766
Epoch 390, val loss: 1.0009734630584717
Epoch 400, training loss: 7.116234302520752 = 0.7103189826011658 + 1.0 * 6.405915260314941
Epoch 400, val loss: 0.9801657199859619
Epoch 410, training loss: 7.088291168212891 = 0.6773591041564941 + 1.0 * 6.4109320640563965
Epoch 410, val loss: 0.9607996344566345
Epoch 420, training loss: 7.045354843139648 = 0.6457824110984802 + 1.0 * 6.399572372436523
Epoch 420, val loss: 0.9432792663574219
Epoch 430, training loss: 7.013153076171875 = 0.614963948726654 + 1.0 * 6.398189067840576
Epoch 430, val loss: 0.9270923137664795
Epoch 440, training loss: 6.983241081237793 = 0.5850961804389954 + 1.0 * 6.398144721984863
Epoch 440, val loss: 0.9123098850250244
Epoch 450, training loss: 6.9477338790893555 = 0.5560908317565918 + 1.0 * 6.391643047332764
Epoch 450, val loss: 0.8992430567741394
Epoch 460, training loss: 6.914936065673828 = 0.5277261137962341 + 1.0 * 6.387209892272949
Epoch 460, val loss: 0.8874177932739258
Epoch 470, training loss: 6.894712924957275 = 0.4999912679195404 + 1.0 * 6.394721508026123
Epoch 470, val loss: 0.8771610856056213
Epoch 480, training loss: 6.861202239990234 = 0.4734414219856262 + 1.0 * 6.387760639190674
Epoch 480, val loss: 0.8684388399124146
Epoch 490, training loss: 6.829527378082275 = 0.4480316936969757 + 1.0 * 6.381495475769043
Epoch 490, val loss: 0.8613892197608948
Epoch 500, training loss: 6.801912307739258 = 0.42358383536338806 + 1.0 * 6.378328323364258
Epoch 500, val loss: 0.8556650876998901
Epoch 510, training loss: 6.776139736175537 = 0.40005940198898315 + 1.0 * 6.376080513000488
Epoch 510, val loss: 0.8513657450675964
Epoch 520, training loss: 6.75242805480957 = 0.37764471769332886 + 1.0 * 6.374783515930176
Epoch 520, val loss: 0.8483273386955261
Epoch 530, training loss: 6.73002290725708 = 0.35645750164985657 + 1.0 * 6.373565196990967
Epoch 530, val loss: 0.8467586040496826
Epoch 540, training loss: 6.705963134765625 = 0.33636194467544556 + 1.0 * 6.369601249694824
Epoch 540, val loss: 0.8462646007537842
Epoch 550, training loss: 6.69125509262085 = 0.3173748254776001 + 1.0 * 6.373880386352539
Epoch 550, val loss: 0.8469021320343018
Epoch 560, training loss: 6.6706862449646 = 0.29968127608299255 + 1.0 * 6.371005058288574
Epoch 560, val loss: 0.8484457731246948
Epoch 570, training loss: 6.64811372756958 = 0.28299131989479065 + 1.0 * 6.365122318267822
Epoch 570, val loss: 0.8509438037872314
Epoch 580, training loss: 6.628705978393555 = 0.26720020174980164 + 1.0 * 6.36150598526001
Epoch 580, val loss: 0.8540894389152527
Epoch 590, training loss: 6.621771812438965 = 0.25221869349479675 + 1.0 * 6.369553089141846
Epoch 590, val loss: 0.8579315543174744
Epoch 600, training loss: 6.599234104156494 = 0.2381325215101242 + 1.0 * 6.3611016273498535
Epoch 600, val loss: 0.8622775673866272
Epoch 610, training loss: 6.581234455108643 = 0.22476613521575928 + 1.0 * 6.356468200683594
Epoch 610, val loss: 0.8670044541358948
Epoch 620, training loss: 6.5732011795043945 = 0.21207360923290253 + 1.0 * 6.3611273765563965
Epoch 620, val loss: 0.8721454739570618
Epoch 630, training loss: 6.559000015258789 = 0.20007376372814178 + 1.0 * 6.358926296234131
Epoch 630, val loss: 0.877749502658844
Epoch 640, training loss: 6.5428619384765625 = 0.1887580156326294 + 1.0 * 6.354104042053223
Epoch 640, val loss: 0.8835617899894714
Epoch 650, training loss: 6.528771877288818 = 0.17802296578884125 + 1.0 * 6.3507490158081055
Epoch 650, val loss: 0.8897416591644287
Epoch 660, training loss: 6.520646572113037 = 0.16785800457000732 + 1.0 * 6.35278844833374
Epoch 660, val loss: 0.8962121605873108
Epoch 670, training loss: 6.514125347137451 = 0.15826250612735748 + 1.0 * 6.355862617492676
Epoch 670, val loss: 0.902942419052124
Epoch 680, training loss: 6.4968743324279785 = 0.1492471843957901 + 1.0 * 6.34762716293335
Epoch 680, val loss: 0.9098881483078003
Epoch 690, training loss: 6.485232353210449 = 0.14074501395225525 + 1.0 * 6.344487190246582
Epoch 690, val loss: 0.9170003533363342
Epoch 700, training loss: 6.476446628570557 = 0.1327228993177414 + 1.0 * 6.343723773956299
Epoch 700, val loss: 0.9242812991142273
Epoch 710, training loss: 6.468530654907227 = 0.12519773840904236 + 1.0 * 6.343332767486572
Epoch 710, val loss: 0.9317998886108398
Epoch 720, training loss: 6.464926719665527 = 0.11818066984415054 + 1.0 * 6.34674596786499
Epoch 720, val loss: 0.9393377304077148
Epoch 730, training loss: 6.452012538909912 = 0.11163583397865295 + 1.0 * 6.340376853942871
Epoch 730, val loss: 0.9469233155250549
Epoch 740, training loss: 6.446572303771973 = 0.10548233985900879 + 1.0 * 6.341089725494385
Epoch 740, val loss: 0.9546874165534973
Epoch 750, training loss: 6.438238143920898 = 0.0997491180896759 + 1.0 * 6.338489055633545
Epoch 750, val loss: 0.9626019597053528
Epoch 760, training loss: 6.430997848510742 = 0.09439174085855484 + 1.0 * 6.336606025695801
Epoch 760, val loss: 0.9705103039741516
Epoch 770, training loss: 6.424120903015137 = 0.08937768638134003 + 1.0 * 6.334743022918701
Epoch 770, val loss: 0.9785043001174927
Epoch 780, training loss: 6.4267048835754395 = 0.08468682318925858 + 1.0 * 6.342018127441406
Epoch 780, val loss: 0.9865971207618713
Epoch 790, training loss: 6.414139747619629 = 0.0803464949131012 + 1.0 * 6.3337931632995605
Epoch 790, val loss: 0.9946081042289734
Epoch 800, training loss: 6.407872676849365 = 0.07628452032804489 + 1.0 * 6.331588268280029
Epoch 800, val loss: 1.0026135444641113
Epoch 810, training loss: 6.401607990264893 = 0.07248210906982422 + 1.0 * 6.329125881195068
Epoch 810, val loss: 1.010664701461792
Epoch 820, training loss: 6.397096633911133 = 0.06891337782144547 + 1.0 * 6.328183174133301
Epoch 820, val loss: 1.0187451839447021
Epoch 830, training loss: 6.402533531188965 = 0.06558071076869965 + 1.0 * 6.3369526863098145
Epoch 830, val loss: 1.0267815589904785
Epoch 840, training loss: 6.391666412353516 = 0.062485720962285995 + 1.0 * 6.329180717468262
Epoch 840, val loss: 1.0348331928253174
Epoch 850, training loss: 6.384163856506348 = 0.05958334356546402 + 1.0 * 6.324580669403076
Epoch 850, val loss: 1.0427255630493164
Epoch 860, training loss: 6.382842063903809 = 0.05685576796531677 + 1.0 * 6.325986385345459
Epoch 860, val loss: 1.050585150718689
Epoch 870, training loss: 6.390450477600098 = 0.05430001765489578 + 1.0 * 6.336150646209717
Epoch 870, val loss: 1.0584330558776855
Epoch 880, training loss: 6.37980842590332 = 0.051936112344264984 + 1.0 * 6.327872276306152
Epoch 880, val loss: 1.0661860704421997
Epoch 890, training loss: 6.3704514503479 = 0.04970636963844299 + 1.0 * 6.32074499130249
Epoch 890, val loss: 1.0738325119018555
Epoch 900, training loss: 6.367520809173584 = 0.0476052425801754 + 1.0 * 6.319915771484375
Epoch 900, val loss: 1.081429123878479
Epoch 910, training loss: 6.364693641662598 = 0.045619502663612366 + 1.0 * 6.3190741539001465
Epoch 910, val loss: 1.0890167951583862
Epoch 920, training loss: 6.374518394470215 = 0.04375182092189789 + 1.0 * 6.330766677856445
Epoch 920, val loss: 1.0965209007263184
Epoch 930, training loss: 6.359555244445801 = 0.042007870972156525 + 1.0 * 6.31754732131958
Epoch 930, val loss: 1.104002594947815
Epoch 940, training loss: 6.357183933258057 = 0.040359463542699814 + 1.0 * 6.316824436187744
Epoch 940, val loss: 1.1113609075546265
Epoch 950, training loss: 6.355484962463379 = 0.03880036994814873 + 1.0 * 6.316684722900391
Epoch 950, val loss: 1.1186681985855103
Epoch 960, training loss: 6.353731632232666 = 0.037331994622945786 + 1.0 * 6.316399574279785
Epoch 960, val loss: 1.1259396076202393
Epoch 970, training loss: 6.3527631759643555 = 0.035953789949417114 + 1.0 * 6.316809177398682
Epoch 970, val loss: 1.1330536603927612
Epoch 980, training loss: 6.347540855407715 = 0.034647028893232346 + 1.0 * 6.312893867492676
Epoch 980, val loss: 1.1400694847106934
Epoch 990, training loss: 6.355076313018799 = 0.03340953588485718 + 1.0 * 6.321666717529297
Epoch 990, val loss: 1.1469916105270386
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 10.544290542602539 = 1.9475066661834717 + 1.0 * 8.596783638000488
Epoch 0, val loss: 1.9504296779632568
Epoch 10, training loss: 10.53386116027832 = 1.9374797344207764 + 1.0 * 8.596381187438965
Epoch 10, val loss: 1.9399371147155762
Epoch 20, training loss: 10.518122673034668 = 1.9249500036239624 + 1.0 * 8.593173027038574
Epoch 20, val loss: 1.9265434741973877
Epoch 30, training loss: 10.476251602172852 = 1.907433032989502 + 1.0 * 8.568818092346191
Epoch 30, val loss: 1.9078553915023804
Epoch 40, training loss: 10.270862579345703 = 1.885762095451355 + 1.0 * 8.385100364685059
Epoch 40, val loss: 1.8860852718353271
Epoch 50, training loss: 9.70022964477539 = 1.863122582435608 + 1.0 * 7.837106704711914
Epoch 50, val loss: 1.8648475408554077
Epoch 60, training loss: 9.219610214233398 = 1.8483781814575195 + 1.0 * 7.371232032775879
Epoch 60, val loss: 1.8516528606414795
Epoch 70, training loss: 8.909360885620117 = 1.8370087146759033 + 1.0 * 7.072351932525635
Epoch 70, val loss: 1.8406277894973755
Epoch 80, training loss: 8.77417278289795 = 1.824347734451294 + 1.0 * 6.949825286865234
Epoch 80, val loss: 1.8286937475204468
Epoch 90, training loss: 8.679676055908203 = 1.8105307817459106 + 1.0 * 6.869144916534424
Epoch 90, val loss: 1.8161391019821167
Epoch 100, training loss: 8.599358558654785 = 1.7979286909103394 + 1.0 * 6.8014302253723145
Epoch 100, val loss: 1.804948091506958
Epoch 110, training loss: 8.523746490478516 = 1.7872436046600342 + 1.0 * 6.7365031242370605
Epoch 110, val loss: 1.7951017618179321
Epoch 120, training loss: 8.459331512451172 = 1.777024269104004 + 1.0 * 6.68230676651001
Epoch 120, val loss: 1.78565514087677
Epoch 130, training loss: 8.403918266296387 = 1.7660950422286987 + 1.0 * 6.637823581695557
Epoch 130, val loss: 1.775956630706787
Epoch 140, training loss: 8.357962608337402 = 1.7538686990737915 + 1.0 * 6.6040940284729
Epoch 140, val loss: 1.7656408548355103
Epoch 150, training loss: 8.3160400390625 = 1.739938735961914 + 1.0 * 6.576100826263428
Epoch 150, val loss: 1.7541723251342773
Epoch 160, training loss: 8.277324676513672 = 1.723641037940979 + 1.0 * 6.553683280944824
Epoch 160, val loss: 1.7410194873809814
Epoch 170, training loss: 8.2391939163208 = 1.704639196395874 + 1.0 * 6.534554481506348
Epoch 170, val loss: 1.725934624671936
Epoch 180, training loss: 8.20090103149414 = 1.6825546026229858 + 1.0 * 6.518346786499023
Epoch 180, val loss: 1.7084828615188599
Epoch 190, training loss: 8.161218643188477 = 1.6566193103790283 + 1.0 * 6.504599094390869
Epoch 190, val loss: 1.6880581378936768
Epoch 200, training loss: 8.118789672851562 = 1.6263166666030884 + 1.0 * 6.492473125457764
Epoch 200, val loss: 1.664176344871521
Epoch 210, training loss: 8.072811126708984 = 1.591398000717163 + 1.0 * 6.4814133644104
Epoch 210, val loss: 1.636645793914795
Epoch 220, training loss: 8.023433685302734 = 1.5515141487121582 + 1.0 * 6.471919059753418
Epoch 220, val loss: 1.605306625366211
Epoch 230, training loss: 7.971193790435791 = 1.5070010423660278 + 1.0 * 6.464192867279053
Epoch 230, val loss: 1.5703704357147217
Epoch 240, training loss: 7.914030075073242 = 1.45881986618042 + 1.0 * 6.455210208892822
Epoch 240, val loss: 1.5327907800674438
Epoch 250, training loss: 7.858049392700195 = 1.4079011678695679 + 1.0 * 6.450148105621338
Epoch 250, val loss: 1.493341326713562
Epoch 260, training loss: 7.798916816711426 = 1.3557738065719604 + 1.0 * 6.443142890930176
Epoch 260, val loss: 1.4535075426101685
Epoch 270, training loss: 7.740517616271973 = 1.3036253452301025 + 1.0 * 6.436892509460449
Epoch 270, val loss: 1.4138528108596802
Epoch 280, training loss: 7.686505317687988 = 1.2521334886550903 + 1.0 * 6.4343719482421875
Epoch 280, val loss: 1.3750848770141602
Epoch 290, training loss: 7.629972457885742 = 1.2027853727340698 + 1.0 * 6.427186965942383
Epoch 290, val loss: 1.3383092880249023
Epoch 300, training loss: 7.575611591339111 = 1.155213713645935 + 1.0 * 6.420397758483887
Epoch 300, val loss: 1.3032517433166504
Epoch 310, training loss: 7.529369831085205 = 1.1095069646835327 + 1.0 * 6.419862747192383
Epoch 310, val loss: 1.2697378396987915
Epoch 320, training loss: 7.483098030090332 = 1.0661869049072266 + 1.0 * 6.4169111251831055
Epoch 320, val loss: 1.2382084131240845
Epoch 330, training loss: 7.433855056762695 = 1.0254695415496826 + 1.0 * 6.408385276794434
Epoch 330, val loss: 1.2087368965148926
Epoch 340, training loss: 7.390137195587158 = 0.9866970181465149 + 1.0 * 6.403439998626709
Epoch 340, val loss: 1.1807897090911865
Epoch 350, training loss: 7.35106897354126 = 0.949557900428772 + 1.0 * 6.401511192321777
Epoch 350, val loss: 1.1539944410324097
Epoch 360, training loss: 7.313085556030273 = 0.9140288829803467 + 1.0 * 6.399056911468506
Epoch 360, val loss: 1.1284804344177246
Epoch 370, training loss: 7.272315502166748 = 0.8799047470092773 + 1.0 * 6.392410755157471
Epoch 370, val loss: 1.1038458347320557
Epoch 380, training loss: 7.238762855529785 = 0.8466507792472839 + 1.0 * 6.3921122550964355
Epoch 380, val loss: 1.0798804759979248
Epoch 390, training loss: 7.2041850090026855 = 0.8141645789146423 + 1.0 * 6.390020370483398
Epoch 390, val loss: 1.0566051006317139
Epoch 400, training loss: 7.16689395904541 = 0.7825743556022644 + 1.0 * 6.38431978225708
Epoch 400, val loss: 1.0339295864105225
Epoch 410, training loss: 7.131546974182129 = 0.7516462802886963 + 1.0 * 6.379900932312012
Epoch 410, val loss: 1.0120582580566406
Epoch 420, training loss: 7.101202964782715 = 0.7212451696395874 + 1.0 * 6.379957675933838
Epoch 420, val loss: 0.9906578063964844
Epoch 430, training loss: 7.073448181152344 = 0.6914910674095154 + 1.0 * 6.381957054138184
Epoch 430, val loss: 0.9700976610183716
Epoch 440, training loss: 7.036747455596924 = 0.6626008749008179 + 1.0 * 6.374146461486816
Epoch 440, val loss: 0.950688898563385
Epoch 450, training loss: 7.004100322723389 = 0.6341513991355896 + 1.0 * 6.369948863983154
Epoch 450, val loss: 0.9320811629295349
Epoch 460, training loss: 6.978724956512451 = 0.606086254119873 + 1.0 * 6.372638702392578
Epoch 460, val loss: 0.9144106507301331
Epoch 470, training loss: 6.943914413452148 = 0.5786187648773193 + 1.0 * 6.36529541015625
Epoch 470, val loss: 0.8979711532592773
Epoch 480, training loss: 6.91563081741333 = 0.5515036582946777 + 1.0 * 6.364127159118652
Epoch 480, val loss: 0.8826937675476074
Epoch 490, training loss: 6.886340141296387 = 0.5248289108276367 + 1.0 * 6.36151123046875
Epoch 490, val loss: 0.8685587644577026
Epoch 500, training loss: 6.858755111694336 = 0.498767614364624 + 1.0 * 6.359987258911133
Epoch 500, val loss: 0.8560450077056885
Epoch 510, training loss: 6.833588123321533 = 0.47331517934799194 + 1.0 * 6.3602728843688965
Epoch 510, val loss: 0.8448230624198914
Epoch 520, training loss: 6.805160045623779 = 0.448556512594223 + 1.0 * 6.356603622436523
Epoch 520, val loss: 0.8351135849952698
Epoch 530, training loss: 6.780692100524902 = 0.42439717054367065 + 1.0 * 6.356295108795166
Epoch 530, val loss: 0.8267636895179749
Epoch 540, training loss: 6.76121711730957 = 0.40094101428985596 + 1.0 * 6.360276222229004
Epoch 540, val loss: 0.8199183344841003
Epoch 550, training loss: 6.728656768798828 = 0.3784305155277252 + 1.0 * 6.350226402282715
Epoch 550, val loss: 0.8146826028823853
Epoch 560, training loss: 6.706394195556641 = 0.3567645847797394 + 1.0 * 6.3496294021606445
Epoch 560, val loss: 0.810740053653717
Epoch 570, training loss: 6.685983180999756 = 0.33596429228782654 + 1.0 * 6.3500189781188965
Epoch 570, val loss: 0.8080520629882812
Epoch 580, training loss: 6.6621904373168945 = 0.3162651062011719 + 1.0 * 6.345925331115723
Epoch 580, val loss: 0.8067263960838318
Epoch 590, training loss: 6.642525672912598 = 0.2975641191005707 + 1.0 * 6.344961643218994
Epoch 590, val loss: 0.8064695000648499
Epoch 600, training loss: 6.6293816566467285 = 0.2798806428909302 + 1.0 * 6.349501132965088
Epoch 600, val loss: 0.8071854710578918
Epoch 610, training loss: 6.606359004974365 = 0.2634009122848511 + 1.0 * 6.342957973480225
Epoch 610, val loss: 0.8089314699172974
Epoch 620, training loss: 6.587405681610107 = 0.24786023795604706 + 1.0 * 6.339545249938965
Epoch 620, val loss: 0.8115205764770508
Epoch 630, training loss: 6.572447776794434 = 0.23324871063232422 + 1.0 * 6.339199066162109
Epoch 630, val loss: 0.8148801326751709
Epoch 640, training loss: 6.561025619506836 = 0.21958498656749725 + 1.0 * 6.341440677642822
Epoch 640, val loss: 0.8189038038253784
Epoch 650, training loss: 6.540383338928223 = 0.20681780576705933 + 1.0 * 6.333565711975098
Epoch 650, val loss: 0.8236742615699768
Epoch 660, training loss: 6.527577877044678 = 0.19479914009571075 + 1.0 * 6.3327789306640625
Epoch 660, val loss: 0.8288153409957886
Epoch 670, training loss: 6.539068222045898 = 0.18347911536693573 + 1.0 * 6.355588912963867
Epoch 670, val loss: 0.8344473242759705
Epoch 680, training loss: 6.510706901550293 = 0.17307065427303314 + 1.0 * 6.337636470794678
Epoch 680, val loss: 0.8405542373657227
Epoch 690, training loss: 6.495621204376221 = 0.16335928440093994 + 1.0 * 6.33226203918457
Epoch 690, val loss: 0.8469784259796143
Epoch 700, training loss: 6.482316017150879 = 0.15422417223453522 + 1.0 * 6.328091621398926
Epoch 700, val loss: 0.8536546230316162
Epoch 710, training loss: 6.472131252288818 = 0.14562684297561646 + 1.0 * 6.326504230499268
Epoch 710, val loss: 0.8607531189918518
Epoch 720, training loss: 6.464289665222168 = 0.13758641481399536 + 1.0 * 6.326703071594238
Epoch 720, val loss: 0.8680270314216614
Epoch 730, training loss: 6.457774639129639 = 0.13011418282985687 + 1.0 * 6.32766056060791
Epoch 730, val loss: 0.8757591247558594
Epoch 740, training loss: 6.447718143463135 = 0.1231081634759903 + 1.0 * 6.324609756469727
Epoch 740, val loss: 0.8835691213607788
Epoch 750, training loss: 6.443755626678467 = 0.11653497815132141 + 1.0 * 6.327220439910889
Epoch 750, val loss: 0.8915130496025085
Epoch 760, training loss: 6.432244300842285 = 0.11041341722011566 + 1.0 * 6.321830749511719
Epoch 760, val loss: 0.8997762799263
Epoch 770, training loss: 6.425041675567627 = 0.10465416312217712 + 1.0 * 6.320387363433838
Epoch 770, val loss: 0.9081699848175049
Epoch 780, training loss: 6.4218549728393555 = 0.09925033897161484 + 1.0 * 6.322604656219482
Epoch 780, val loss: 0.9167395830154419
Epoch 790, training loss: 6.4130706787109375 = 0.09420204162597656 + 1.0 * 6.318868637084961
Epoch 790, val loss: 0.9255118370056152
Epoch 800, training loss: 6.405901908874512 = 0.08946294337511063 + 1.0 * 6.316439151763916
Epoch 800, val loss: 0.9342401623725891
Epoch 810, training loss: 6.4021196365356445 = 0.08501972258090973 + 1.0 * 6.3171000480651855
Epoch 810, val loss: 0.943126916885376
Epoch 820, training loss: 6.399868965148926 = 0.08086060732603073 + 1.0 * 6.3190083503723145
Epoch 820, val loss: 0.952034592628479
Epoch 830, training loss: 6.390039443969727 = 0.076962411403656 + 1.0 * 6.313076972961426
Epoch 830, val loss: 0.9609314799308777
Epoch 840, training loss: 6.386932849884033 = 0.07331091165542603 + 1.0 * 6.313621997833252
Epoch 840, val loss: 0.9699702262878418
Epoch 850, training loss: 6.38162088394165 = 0.0698750838637352 + 1.0 * 6.311745643615723
Epoch 850, val loss: 0.9789281487464905
Epoch 860, training loss: 6.387783050537109 = 0.06664162129163742 + 1.0 * 6.321141242980957
Epoch 860, val loss: 0.9879145622253418
Epoch 870, training loss: 6.376556873321533 = 0.06361785531044006 + 1.0 * 6.312939167022705
Epoch 870, val loss: 0.9968275427818298
Epoch 880, training loss: 6.378746509552002 = 0.06078607961535454 + 1.0 * 6.317960262298584
Epoch 880, val loss: 1.0057076215744019
Epoch 890, training loss: 6.3702592849731445 = 0.05811882019042969 + 1.0 * 6.312140464782715
Epoch 890, val loss: 1.0145542621612549
Epoch 900, training loss: 6.365823268890381 = 0.05562333017587662 + 1.0 * 6.310199737548828
Epoch 900, val loss: 1.0233041048049927
Epoch 910, training loss: 6.359824180603027 = 0.053266238421201706 + 1.0 * 6.306558132171631
Epoch 910, val loss: 1.032030463218689
Epoch 920, training loss: 6.356400966644287 = 0.051037997007369995 + 1.0 * 6.305363178253174
Epoch 920, val loss: 1.0406618118286133
Epoch 930, training loss: 6.360061168670654 = 0.04893241077661514 + 1.0 * 6.311128616333008
Epoch 930, val loss: 1.0491565465927124
Epoch 940, training loss: 6.355767726898193 = 0.04695601388812065 + 1.0 * 6.308811664581299
Epoch 940, val loss: 1.0576609373092651
Epoch 950, training loss: 6.3521342277526855 = 0.04509365186095238 + 1.0 * 6.307040691375732
Epoch 950, val loss: 1.065943956375122
Epoch 960, training loss: 6.346446990966797 = 0.04334473982453346 + 1.0 * 6.303102016448975
Epoch 960, val loss: 1.0741965770721436
Epoch 970, training loss: 6.343729019165039 = 0.041686996817588806 + 1.0 * 6.302042007446289
Epoch 970, val loss: 1.0824097394943237
Epoch 980, training loss: 6.340710639953613 = 0.04011258855462074 + 1.0 * 6.30059814453125
Epoch 980, val loss: 1.0904332399368286
Epoch 990, training loss: 6.3484787940979 = 0.03862273693084717 + 1.0 * 6.309855937957764
Epoch 990, val loss: 1.0983655452728271
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 10.553275108337402 = 1.9564555883407593 + 1.0 * 8.596819877624512
Epoch 0, val loss: 1.961038589477539
Epoch 10, training loss: 10.542459487915039 = 1.945924162864685 + 1.0 * 8.596535682678223
Epoch 10, val loss: 1.9503623247146606
Epoch 20, training loss: 10.526772499084473 = 1.9323292970657349 + 1.0 * 8.594443321228027
Epoch 20, val loss: 1.9360742568969727
Epoch 30, training loss: 10.491893768310547 = 1.91292142868042 + 1.0 * 8.578971862792969
Epoch 30, val loss: 1.915094256401062
Epoch 40, training loss: 10.381539344787598 = 1.887056589126587 + 1.0 * 8.49448299407959
Epoch 40, val loss: 1.8882133960723877
Epoch 50, training loss: 9.99828815460205 = 1.859700083732605 + 1.0 * 8.138587951660156
Epoch 50, val loss: 1.8605775833129883
Epoch 60, training loss: 9.661393165588379 = 1.8345907926559448 + 1.0 * 7.826802730560303
Epoch 60, val loss: 1.836217999458313
Epoch 70, training loss: 9.235255241394043 = 1.8166489601135254 + 1.0 * 7.418606281280518
Epoch 70, val loss: 1.8199716806411743
Epoch 80, training loss: 8.953518867492676 = 1.8037956953048706 + 1.0 * 7.149723529815674
Epoch 80, val loss: 1.8082029819488525
Epoch 90, training loss: 8.805249214172363 = 1.788851022720337 + 1.0 * 7.016397953033447
Epoch 90, val loss: 1.7945328950881958
Epoch 100, training loss: 8.692811965942383 = 1.7732304334640503 + 1.0 * 6.919581413269043
Epoch 100, val loss: 1.7807574272155762
Epoch 110, training loss: 8.587642669677734 = 1.7608147859573364 + 1.0 * 6.8268280029296875
Epoch 110, val loss: 1.7696481943130493
Epoch 120, training loss: 8.503482818603516 = 1.7491710186004639 + 1.0 * 6.754311561584473
Epoch 120, val loss: 1.7588216066360474
Epoch 130, training loss: 8.437477111816406 = 1.735520601272583 + 1.0 * 6.701956272125244
Epoch 130, val loss: 1.746313452720642
Epoch 140, training loss: 8.37862777709961 = 1.7194666862487793 + 1.0 * 6.659160614013672
Epoch 140, val loss: 1.7323315143585205
Epoch 150, training loss: 8.326456069946289 = 1.7007379531860352 + 1.0 * 6.625718116760254
Epoch 150, val loss: 1.7164722681045532
Epoch 160, training loss: 8.278307914733887 = 1.6787235736846924 + 1.0 * 6.599584102630615
Epoch 160, val loss: 1.6979851722717285
Epoch 170, training loss: 8.229900360107422 = 1.6531291007995605 + 1.0 * 6.576771259307861
Epoch 170, val loss: 1.6766657829284668
Epoch 180, training loss: 8.181483268737793 = 1.6235114336013794 + 1.0 * 6.557971954345703
Epoch 180, val loss: 1.6520514488220215
Epoch 190, training loss: 8.130875587463379 = 1.5893748998641968 + 1.0 * 6.541501045227051
Epoch 190, val loss: 1.6238641738891602
Epoch 200, training loss: 8.078359603881836 = 1.550788402557373 + 1.0 * 6.527571678161621
Epoch 200, val loss: 1.592353343963623
Epoch 210, training loss: 8.02208423614502 = 1.5085747241973877 + 1.0 * 6.513509273529053
Epoch 210, val loss: 1.5585048198699951
Epoch 220, training loss: 7.966113090515137 = 1.46293306350708 + 1.0 * 6.503180027008057
Epoch 220, val loss: 1.5226918458938599
Epoch 230, training loss: 7.905112266540527 = 1.4147136211395264 + 1.0 * 6.49039888381958
Epoch 230, val loss: 1.4855999946594238
Epoch 240, training loss: 7.845254898071289 = 1.364281177520752 + 1.0 * 6.480973720550537
Epoch 240, val loss: 1.4475767612457275
Epoch 250, training loss: 7.785139083862305 = 1.3126370906829834 + 1.0 * 6.472501754760742
Epoch 250, val loss: 1.4091476202011108
Epoch 260, training loss: 7.725530624389648 = 1.2607102394104004 + 1.0 * 6.464820384979248
Epoch 260, val loss: 1.3708350658416748
Epoch 270, training loss: 7.665341854095459 = 1.208943486213684 + 1.0 * 6.4563984870910645
Epoch 270, val loss: 1.3330057859420776
Epoch 280, training loss: 7.611883163452148 = 1.1576985120773315 + 1.0 * 6.454184532165527
Epoch 280, val loss: 1.2956501245498657
Epoch 290, training loss: 7.552121639251709 = 1.1085399389266968 + 1.0 * 6.443581581115723
Epoch 290, val loss: 1.2597154378890991
Epoch 300, training loss: 7.499773979187012 = 1.061486840248108 + 1.0 * 6.438287258148193
Epoch 300, val loss: 1.225627064704895
Epoch 310, training loss: 7.458254814147949 = 1.0164215564727783 + 1.0 * 6.441833019256592
Epoch 310, val loss: 1.1931540966033936
Epoch 320, training loss: 7.402630805969238 = 0.9743096232414246 + 1.0 * 6.428321361541748
Epoch 320, val loss: 1.1629886627197266
Epoch 330, training loss: 7.35771369934082 = 0.9342779517173767 + 1.0 * 6.423435688018799
Epoch 330, val loss: 1.134849190711975
Epoch 340, training loss: 7.314432621002197 = 0.8957119584083557 + 1.0 * 6.418720722198486
Epoch 340, val loss: 1.1081417798995972
Epoch 350, training loss: 7.274946212768555 = 0.8585737347602844 + 1.0 * 6.416372299194336
Epoch 350, val loss: 1.0828584432601929
Epoch 360, training loss: 7.2332305908203125 = 0.8230925798416138 + 1.0 * 6.410138130187988
Epoch 360, val loss: 1.059356451034546
Epoch 370, training loss: 7.19518518447876 = 0.7886579632759094 + 1.0 * 6.406527042388916
Epoch 370, val loss: 1.037036657333374
Epoch 380, training loss: 7.163150787353516 = 0.7551894783973694 + 1.0 * 6.407961368560791
Epoch 380, val loss: 1.0157554149627686
Epoch 390, training loss: 7.122133255004883 = 0.7228624820709229 + 1.0 * 6.399270534515381
Epoch 390, val loss: 0.9958685040473938
Epoch 400, training loss: 7.088729381561279 = 0.6914809346199036 + 1.0 * 6.397248268127441
Epoch 400, val loss: 0.9774340391159058
Epoch 410, training loss: 7.05908203125 = 0.6608826518058777 + 1.0 * 6.398199558258057
Epoch 410, val loss: 0.9600083827972412
Epoch 420, training loss: 7.02385139465332 = 0.6313830018043518 + 1.0 * 6.392468452453613
Epoch 420, val loss: 0.9439430832862854
Epoch 430, training loss: 6.991823673248291 = 0.6027113199234009 + 1.0 * 6.38911247253418
Epoch 430, val loss: 0.9293100237846375
Epoch 440, training loss: 6.961307048797607 = 0.5749455690383911 + 1.0 * 6.386361598968506
Epoch 440, val loss: 0.915756344795227
Epoch 450, training loss: 6.932852745056152 = 0.5481517314910889 + 1.0 * 6.384701251983643
Epoch 450, val loss: 0.9036014676094055
Epoch 460, training loss: 6.911787986755371 = 0.5222012996673584 + 1.0 * 6.389586448669434
Epoch 460, val loss: 0.8926181793212891
Epoch 470, training loss: 6.876497745513916 = 0.4973997473716736 + 1.0 * 6.379097938537598
Epoch 470, val loss: 0.8829135894775391
Epoch 480, training loss: 6.854487895965576 = 0.47354650497436523 + 1.0 * 6.380941390991211
Epoch 480, val loss: 0.8746753334999084
Epoch 490, training loss: 6.825954437255859 = 0.45063790678977966 + 1.0 * 6.375316619873047
Epoch 490, val loss: 0.8673962354660034
Epoch 500, training loss: 6.802017688751221 = 0.4285367429256439 + 1.0 * 6.373480796813965
Epoch 500, val loss: 0.861528754234314
Epoch 510, training loss: 6.783563137054443 = 0.4073185622692108 + 1.0 * 6.37624454498291
Epoch 510, val loss: 0.8564097881317139
Epoch 520, training loss: 6.755046367645264 = 0.3869515657424927 + 1.0 * 6.3680949211120605
Epoch 520, val loss: 0.8523913621902466
Epoch 530, training loss: 6.734483242034912 = 0.36733773350715637 + 1.0 * 6.367145538330078
Epoch 530, val loss: 0.8492477536201477
Epoch 540, training loss: 6.715885639190674 = 0.3483586311340332 + 1.0 * 6.367527008056641
Epoch 540, val loss: 0.8466757535934448
Epoch 550, training loss: 6.6938652992248535 = 0.33003315329551697 + 1.0 * 6.363831996917725
Epoch 550, val loss: 0.8450341820716858
Epoch 560, training loss: 6.673582077026367 = 0.3122624158859253 + 1.0 * 6.361319541931152
Epoch 560, val loss: 0.8439081311225891
Epoch 570, training loss: 6.658855438232422 = 0.29508405923843384 + 1.0 * 6.363771438598633
Epoch 570, val loss: 0.8430860042572021
Epoch 580, training loss: 6.637547016143799 = 0.27859780192375183 + 1.0 * 6.358949184417725
Epoch 580, val loss: 0.8431366086006165
Epoch 590, training loss: 6.618769645690918 = 0.26266422867774963 + 1.0 * 6.356105327606201
Epoch 590, val loss: 0.8434665203094482
Epoch 600, training loss: 6.601736068725586 = 0.24723398685455322 + 1.0 * 6.354502201080322
Epoch 600, val loss: 0.8442009687423706
Epoch 610, training loss: 6.585385322570801 = 0.2325001358985901 + 1.0 * 6.3528852462768555
Epoch 610, val loss: 0.8452049493789673
Epoch 620, training loss: 6.57016134262085 = 0.21853229403495789 + 1.0 * 6.351629257202148
Epoch 620, val loss: 0.8471209406852722
Epoch 630, training loss: 6.555224895477295 = 0.20523837208747864 + 1.0 * 6.349986553192139
Epoch 630, val loss: 0.8491916060447693
Epoch 640, training loss: 6.553312301635742 = 0.19264107942581177 + 1.0 * 6.360671043395996
Epoch 640, val loss: 0.8514711260795593
Epoch 650, training loss: 6.529542922973633 = 0.1808701455593109 + 1.0 * 6.348672866821289
Epoch 650, val loss: 0.8546358942985535
Epoch 660, training loss: 6.515695571899414 = 0.16978991031646729 + 1.0 * 6.345905780792236
Epoch 660, val loss: 0.8581058979034424
Epoch 670, training loss: 6.504098415374756 = 0.15937618911266327 + 1.0 * 6.344722270965576
Epoch 670, val loss: 0.8617978096008301
Epoch 680, training loss: 6.4998555183410645 = 0.14964750409126282 + 1.0 * 6.350207805633545
Epoch 680, val loss: 0.865768551826477
Epoch 690, training loss: 6.484058856964111 = 0.1406436264514923 + 1.0 * 6.343415260314941
Epoch 690, val loss: 0.8703531622886658
Epoch 700, training loss: 6.473113536834717 = 0.13227039575576782 + 1.0 * 6.340843200683594
Epoch 700, val loss: 0.8751503229141235
Epoch 710, training loss: 6.46334171295166 = 0.12444182485342026 + 1.0 * 6.338900089263916
Epoch 710, val loss: 0.8800752758979797
Epoch 720, training loss: 6.468176364898682 = 0.11717384308576584 + 1.0 * 6.3510026931762695
Epoch 720, val loss: 0.8852412104606628
Epoch 730, training loss: 6.448999881744385 = 0.1104746162891388 + 1.0 * 6.338525295257568
Epoch 730, val loss: 0.8906862735748291
Epoch 740, training loss: 6.442224025726318 = 0.10427732765674591 + 1.0 * 6.337946891784668
Epoch 740, val loss: 0.8963038325309753
Epoch 750, training loss: 6.4336934089660645 = 0.09852978587150574 + 1.0 * 6.335163593292236
Epoch 750, val loss: 0.9021151661872864
Epoch 760, training loss: 6.427918434143066 = 0.09317638725042343 + 1.0 * 6.334742069244385
Epoch 760, val loss: 0.9079985022544861
Epoch 770, training loss: 6.42241907119751 = 0.08820270746946335 + 1.0 * 6.334216594696045
Epoch 770, val loss: 0.9140104651451111
Epoch 780, training loss: 6.42132568359375 = 0.08359009027481079 + 1.0 * 6.337735652923584
Epoch 780, val loss: 0.9202154278755188
Epoch 790, training loss: 6.410632610321045 = 0.07931694388389587 + 1.0 * 6.331315517425537
Epoch 790, val loss: 0.9266160130500793
Epoch 800, training loss: 6.404286861419678 = 0.07532001286745071 + 1.0 * 6.3289666175842285
Epoch 800, val loss: 0.9329477548599243
Epoch 810, training loss: 6.40049409866333 = 0.0715818926692009 + 1.0 * 6.328912258148193
Epoch 810, val loss: 0.9393057823181152
Epoch 820, training loss: 6.399808406829834 = 0.06810186058282852 + 1.0 * 6.331706523895264
Epoch 820, val loss: 0.9456397294998169
Epoch 830, training loss: 6.3933491706848145 = 0.06485425680875778 + 1.0 * 6.328495025634766
Epoch 830, val loss: 0.9521384835243225
Epoch 840, training loss: 6.387697219848633 = 0.06183132901787758 + 1.0 * 6.325865745544434
Epoch 840, val loss: 0.9585562348365784
Epoch 850, training loss: 6.382899761199951 = 0.058993399143218994 + 1.0 * 6.323906421661377
Epoch 850, val loss: 0.9649184346199036
Epoch 860, training loss: 6.386911869049072 = 0.0563286691904068 + 1.0 * 6.330583095550537
Epoch 860, val loss: 0.9712756276130676
Epoch 870, training loss: 6.385168552398682 = 0.05384599789977074 + 1.0 * 6.33132266998291
Epoch 870, val loss: 0.9774582386016846
Epoch 880, training loss: 6.372908592224121 = 0.05152773857116699 + 1.0 * 6.321380615234375
Epoch 880, val loss: 0.9839290976524353
Epoch 890, training loss: 6.369625568389893 = 0.049344874918460846 + 1.0 * 6.3202805519104
Epoch 890, val loss: 0.9902781248092651
Epoch 900, training loss: 6.365825176239014 = 0.047281067818403244 + 1.0 * 6.318543910980225
Epoch 900, val loss: 0.9964494705200195
Epoch 910, training loss: 6.376228332519531 = 0.045334592461586 + 1.0 * 6.330893516540527
Epoch 910, val loss: 1.0024986267089844
Epoch 920, training loss: 6.362210750579834 = 0.04352754354476929 + 1.0 * 6.31868314743042
Epoch 920, val loss: 1.008752703666687
Epoch 930, training loss: 6.3585333824157715 = 0.04181591048836708 + 1.0 * 6.316717624664307
Epoch 930, val loss: 1.0150424242019653
Epoch 940, training loss: 6.356249809265137 = 0.04019239544868469 + 1.0 * 6.316057205200195
Epoch 940, val loss: 1.0209894180297852
Epoch 950, training loss: 6.359187602996826 = 0.03866402059793472 + 1.0 * 6.320523738861084
Epoch 950, val loss: 1.0269237756729126
Epoch 960, training loss: 6.351899147033691 = 0.03721653297543526 + 1.0 * 6.314682483673096
Epoch 960, val loss: 1.0329723358154297
Epoch 970, training loss: 6.349247932434082 = 0.035850003361701965 + 1.0 * 6.3133978843688965
Epoch 970, val loss: 1.0389240980148315
Epoch 980, training loss: 6.348502159118652 = 0.03455530107021332 + 1.0 * 6.313946723937988
Epoch 980, val loss: 1.0446010828018188
Epoch 990, training loss: 6.347192764282227 = 0.03333914652466774 + 1.0 * 6.313853740692139
Epoch 990, val loss: 1.0505613088607788
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8060094886663153
The final CL Acc:0.76049, 0.00924, The final GNN Acc:0.80601, 0.00000
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13192])
remove edge: torch.Size([2, 7956])
updated graph: torch.Size([2, 10592])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.549193382263184 = 1.9523894786834717 + 1.0 * 8.596803665161133
Epoch 0, val loss: 1.947108507156372
Epoch 10, training loss: 10.538788795471191 = 1.9423259496688843 + 1.0 * 8.596463203430176
Epoch 10, val loss: 1.9374523162841797
Epoch 20, training loss: 10.523590087890625 = 1.929564356803894 + 1.0 * 8.594025611877441
Epoch 20, val loss: 1.924926519393921
Epoch 30, training loss: 10.486495018005371 = 1.9113309383392334 + 1.0 * 8.575163841247559
Epoch 30, val loss: 1.9067877531051636
Epoch 40, training loss: 10.343301773071289 = 1.8870362043380737 + 1.0 * 8.456265449523926
Epoch 40, val loss: 1.88321053981781
Epoch 50, training loss: 9.879483222961426 = 1.8594311475753784 + 1.0 * 8.020051956176758
Epoch 50, val loss: 1.856565237045288
Epoch 60, training loss: 9.433777809143066 = 1.8364145755767822 + 1.0 * 7.597363471984863
Epoch 60, val loss: 1.835446834564209
Epoch 70, training loss: 9.011200904846191 = 1.821146845817566 + 1.0 * 7.190054416656494
Epoch 70, val loss: 1.8212120532989502
Epoch 80, training loss: 8.856392860412598 = 1.8052901029586792 + 1.0 * 7.051102638244629
Epoch 80, val loss: 1.8065203428268433
Epoch 90, training loss: 8.761579513549805 = 1.7856584787368774 + 1.0 * 6.975921154022217
Epoch 90, val loss: 1.788762092590332
Epoch 100, training loss: 8.670421600341797 = 1.7666598558425903 + 1.0 * 6.903761386871338
Epoch 100, val loss: 1.7721405029296875
Epoch 110, training loss: 8.572553634643555 = 1.7503225803375244 + 1.0 * 6.822230815887451
Epoch 110, val loss: 1.7579190731048584
Epoch 120, training loss: 8.487424850463867 = 1.7337852716445923 + 1.0 * 6.753639221191406
Epoch 120, val loss: 1.7428425550460815
Epoch 130, training loss: 8.420022010803223 = 1.7146564722061157 + 1.0 * 6.705365180969238
Epoch 130, val loss: 1.7249560356140137
Epoch 140, training loss: 8.353734016418457 = 1.692232608795166 + 1.0 * 6.661501407623291
Epoch 140, val loss: 1.7042860984802246
Epoch 150, training loss: 8.292160987854004 = 1.6664882898330688 + 1.0 * 6.625672817230225
Epoch 150, val loss: 1.6811635494232178
Epoch 160, training loss: 8.231795310974121 = 1.6364699602127075 + 1.0 * 6.595324993133545
Epoch 160, val loss: 1.6545783281326294
Epoch 170, training loss: 8.17342758178711 = 1.600883960723877 + 1.0 * 6.572544097900391
Epoch 170, val loss: 1.6233460903167725
Epoch 180, training loss: 8.110313415527344 = 1.5599613189697266 + 1.0 * 6.550352573394775
Epoch 180, val loss: 1.5876657962799072
Epoch 190, training loss: 8.04655933380127 = 1.513378620147705 + 1.0 * 6.5331807136535645
Epoch 190, val loss: 1.547327995300293
Epoch 200, training loss: 7.980802059173584 = 1.4622300863265991 + 1.0 * 6.518571853637695
Epoch 200, val loss: 1.5035256147384644
Epoch 210, training loss: 7.9132561683654785 = 1.4077826738357544 + 1.0 * 6.505473613739014
Epoch 210, val loss: 1.45766019821167
Epoch 220, training loss: 7.847747802734375 = 1.3517606258392334 + 1.0 * 6.495987415313721
Epoch 220, val loss: 1.4109807014465332
Epoch 230, training loss: 7.77998685836792 = 1.2957552671432495 + 1.0 * 6.484231472015381
Epoch 230, val loss: 1.3647632598876953
Epoch 240, training loss: 7.714964866638184 = 1.239958643913269 + 1.0 * 6.475006103515625
Epoch 240, val loss: 1.3189904689788818
Epoch 250, training loss: 7.656397819519043 = 1.1859239339828491 + 1.0 * 6.470473766326904
Epoch 250, val loss: 1.2750093936920166
Epoch 260, training loss: 7.5952467918396 = 1.1346540451049805 + 1.0 * 6.460592746734619
Epoch 260, val loss: 1.23337984085083
Epoch 270, training loss: 7.53858757019043 = 1.0857555866241455 + 1.0 * 6.452832221984863
Epoch 270, val loss: 1.194007158279419
Epoch 280, training loss: 7.4866461753845215 = 1.0394073724746704 + 1.0 * 6.447238922119141
Epoch 280, val loss: 1.157138705253601
Epoch 290, training loss: 7.436380863189697 = 0.9960118532180786 + 1.0 * 6.440369129180908
Epoch 290, val loss: 1.1226228475570679
Epoch 300, training loss: 7.388902187347412 = 0.9546338319778442 + 1.0 * 6.434268474578857
Epoch 300, val loss: 1.0900427103042603
Epoch 310, training loss: 7.350067138671875 = 0.9145522117614746 + 1.0 * 6.4355149269104
Epoch 310, val loss: 1.0586752891540527
Epoch 320, training loss: 7.304278373718262 = 0.8760536313056946 + 1.0 * 6.428224563598633
Epoch 320, val loss: 1.0286283493041992
Epoch 330, training loss: 7.257962226867676 = 0.8382189273834229 + 1.0 * 6.419743537902832
Epoch 330, val loss: 0.9990193843841553
Epoch 340, training loss: 7.214991569519043 = 0.8002226948738098 + 1.0 * 6.414768695831299
Epoch 340, val loss: 0.9693142175674438
Epoch 350, training loss: 7.1762213706970215 = 0.7622411847114563 + 1.0 * 6.413980007171631
Epoch 350, val loss: 0.939588725566864
Epoch 360, training loss: 7.132935047149658 = 0.7250118851661682 + 1.0 * 6.407923221588135
Epoch 360, val loss: 0.9106061458587646
Epoch 370, training loss: 7.092832088470459 = 0.6883619427680969 + 1.0 * 6.404469966888428
Epoch 370, val loss: 0.8823674917221069
Epoch 380, training loss: 7.064868927001953 = 0.6525650024414062 + 1.0 * 6.412303924560547
Epoch 380, val loss: 0.8551052212715149
Epoch 390, training loss: 7.019773483276367 = 0.6185405850410461 + 1.0 * 6.401232719421387
Epoch 390, val loss: 0.8298549652099609
Epoch 400, training loss: 6.980623245239258 = 0.5862323641777039 + 1.0 * 6.394391059875488
Epoch 400, val loss: 0.8067877888679504
Epoch 410, training loss: 6.947173595428467 = 0.555604100227356 + 1.0 * 6.3915696144104
Epoch 410, val loss: 0.7858311533927917
Epoch 420, training loss: 6.915444850921631 = 0.5268669724464417 + 1.0 * 6.388577938079834
Epoch 420, val loss: 0.7672112584114075
Epoch 430, training loss: 6.8874053955078125 = 0.5001106262207031 + 1.0 * 6.387294769287109
Epoch 430, val loss: 0.75093674659729
Epoch 440, training loss: 6.85899543762207 = 0.47515687346458435 + 1.0 * 6.383838653564453
Epoch 440, val loss: 0.7367424368858337
Epoch 450, training loss: 6.832676887512207 = 0.4519549012184143 + 1.0 * 6.3807220458984375
Epoch 450, val loss: 0.7244567275047302
Epoch 460, training loss: 6.808034420013428 = 0.4300450086593628 + 1.0 * 6.377989292144775
Epoch 460, val loss: 0.7136133909225464
Epoch 470, training loss: 6.796074390411377 = 0.40919095277786255 + 1.0 * 6.38688325881958
Epoch 470, val loss: 0.7038557529449463
Epoch 480, training loss: 6.7626261711120605 = 0.3893038034439087 + 1.0 * 6.373322486877441
Epoch 480, val loss: 0.6950385570526123
Epoch 490, training loss: 6.741211414337158 = 0.37007230520248413 + 1.0 * 6.371139049530029
Epoch 490, val loss: 0.6869028806686401
Epoch 500, training loss: 6.72720193862915 = 0.3512193560600281 + 1.0 * 6.375982761383057
Epoch 500, val loss: 0.6792413592338562
Epoch 510, training loss: 6.7010416984558105 = 0.33287718892097473 + 1.0 * 6.368164539337158
Epoch 510, val loss: 0.6719621419906616
Epoch 520, training loss: 6.682730674743652 = 0.3149607181549072 + 1.0 * 6.367770195007324
Epoch 520, val loss: 0.6651532649993896
Epoch 530, training loss: 6.661458492279053 = 0.29734715819358826 + 1.0 * 6.364111423492432
Epoch 530, val loss: 0.6586527824401855
Epoch 540, training loss: 6.645141124725342 = 0.2801455557346344 + 1.0 * 6.36499547958374
Epoch 540, val loss: 0.6525423526763916
Epoch 550, training loss: 6.626870155334473 = 0.26356932520866394 + 1.0 * 6.363300800323486
Epoch 550, val loss: 0.6470656991004944
Epoch 560, training loss: 6.610591888427734 = 0.24777723848819733 + 1.0 * 6.362814426422119
Epoch 560, val loss: 0.6422069072723389
Epoch 570, training loss: 6.591855525970459 = 0.23280876874923706 + 1.0 * 6.359046936035156
Epoch 570, val loss: 0.6380406618118286
Epoch 580, training loss: 6.574442386627197 = 0.21870450675487518 + 1.0 * 6.355737686157227
Epoch 580, val loss: 0.6346819400787354
Epoch 590, training loss: 6.569456577301025 = 0.20550085604190826 + 1.0 * 6.363955497741699
Epoch 590, val loss: 0.6320085525512695
Epoch 600, training loss: 6.547285079956055 = 0.1933051347732544 + 1.0 * 6.35398006439209
Epoch 600, val loss: 0.630021333694458
Epoch 610, training loss: 6.532683372497559 = 0.1819634884595871 + 1.0 * 6.350719928741455
Epoch 610, val loss: 0.6287032961845398
Epoch 620, training loss: 6.524686336517334 = 0.1713848114013672 + 1.0 * 6.353301525115967
Epoch 620, val loss: 0.6279357075691223
Epoch 630, training loss: 6.516002655029297 = 0.16159790754318237 + 1.0 * 6.354404926300049
Epoch 630, val loss: 0.6277469992637634
Epoch 640, training loss: 6.499396324157715 = 0.15251941978931427 + 1.0 * 6.346877098083496
Epoch 640, val loss: 0.6280047297477722
Epoch 650, training loss: 6.4903364181518555 = 0.14404639601707458 + 1.0 * 6.346290111541748
Epoch 650, val loss: 0.6287083029747009
Epoch 660, training loss: 6.486434459686279 = 0.13614264130592346 + 1.0 * 6.350291728973389
Epoch 660, val loss: 0.6297873258590698
Epoch 670, training loss: 6.473923683166504 = 0.12878842651844025 + 1.0 * 6.34513521194458
Epoch 670, val loss: 0.631209671497345
Epoch 680, training loss: 6.466468811035156 = 0.12191958725452423 + 1.0 * 6.344549179077148
Epoch 680, val loss: 0.6329081058502197
Epoch 690, training loss: 6.463779926300049 = 0.115519680082798 + 1.0 * 6.348260402679443
Epoch 690, val loss: 0.6349160075187683
Epoch 700, training loss: 6.450103282928467 = 0.1095849946141243 + 1.0 * 6.340518474578857
Epoch 700, val loss: 0.6370593905448914
Epoch 710, training loss: 6.441466808319092 = 0.10400910675525665 + 1.0 * 6.337457656860352
Epoch 710, val loss: 0.6394270658493042
Epoch 720, training loss: 6.438322067260742 = 0.0987708643078804 + 1.0 * 6.339550971984863
Epoch 720, val loss: 0.6419531106948853
Epoch 730, training loss: 6.434445381164551 = 0.09387952834367752 + 1.0 * 6.3405656814575195
Epoch 730, val loss: 0.6446558237075806
Epoch 740, training loss: 6.422176361083984 = 0.0893019437789917 + 1.0 * 6.332874298095703
Epoch 740, val loss: 0.6474420428276062
Epoch 750, training loss: 6.417663097381592 = 0.08498848229646683 + 1.0 * 6.332674503326416
Epoch 750, val loss: 0.650375247001648
Epoch 760, training loss: 6.411072254180908 = 0.0809263288974762 + 1.0 * 6.330145835876465
Epoch 760, val loss: 0.6534436941146851
Epoch 770, training loss: 6.421718120574951 = 0.07709847390651703 + 1.0 * 6.3446197509765625
Epoch 770, val loss: 0.6566593647003174
Epoch 780, training loss: 6.404273986816406 = 0.0735231265425682 + 1.0 * 6.330750942230225
Epoch 780, val loss: 0.6598691344261169
Epoch 790, training loss: 6.39808988571167 = 0.07018068432807922 + 1.0 * 6.327908992767334
Epoch 790, val loss: 0.6631411910057068
Epoch 800, training loss: 6.393267631530762 = 0.06702129542827606 + 1.0 * 6.32624626159668
Epoch 800, val loss: 0.6664904356002808
Epoch 810, training loss: 6.3951334953308105 = 0.06404002755880356 + 1.0 * 6.3310933113098145
Epoch 810, val loss: 0.6698886156082153
Epoch 820, training loss: 6.389472961425781 = 0.061237696558237076 + 1.0 * 6.328235149383545
Epoch 820, val loss: 0.673369824886322
Epoch 830, training loss: 6.3821940422058105 = 0.05859849229454994 + 1.0 * 6.3235955238342285
Epoch 830, val loss: 0.6768530011177063
Epoch 840, training loss: 6.377251148223877 = 0.05609935149550438 + 1.0 * 6.3211517333984375
Epoch 840, val loss: 0.6804277896881104
Epoch 850, training loss: 6.3772501945495605 = 0.05373580381274223 + 1.0 * 6.323514461517334
Epoch 850, val loss: 0.6840404868125916
Epoch 860, training loss: 6.3744964599609375 = 0.05150686576962471 + 1.0 * 6.322989463806152
Epoch 860, val loss: 0.6876435279846191
Epoch 870, training loss: 6.369103908538818 = 0.04941476136445999 + 1.0 * 6.3196892738342285
Epoch 870, val loss: 0.6912574768066406
Epoch 880, training loss: 6.366364002227783 = 0.047427840530872345 + 1.0 * 6.318936347961426
Epoch 880, val loss: 0.6948942542076111
Epoch 890, training loss: 6.369392395019531 = 0.04554873704910278 + 1.0 * 6.323843479156494
Epoch 890, val loss: 0.6985544562339783
Epoch 900, training loss: 6.3630051612854 = 0.04378136992454529 + 1.0 * 6.319223880767822
Epoch 900, val loss: 0.7021449208259583
Epoch 910, training loss: 6.3595805168151855 = 0.042102061212062836 + 1.0 * 6.317478656768799
Epoch 910, val loss: 0.7057971954345703
Epoch 920, training loss: 6.354679107666016 = 0.0405145138502121 + 1.0 * 6.314164638519287
Epoch 920, val loss: 0.709457278251648
Epoch 930, training loss: 6.3540143966674805 = 0.0390048623085022 + 1.0 * 6.315009593963623
Epoch 930, val loss: 0.7131248712539673
Epoch 940, training loss: 6.3524980545043945 = 0.037573352456092834 + 1.0 * 6.314924716949463
Epoch 940, val loss: 0.7167608737945557
Epoch 950, training loss: 6.348987579345703 = 0.03621659800410271 + 1.0 * 6.312770843505859
Epoch 950, val loss: 0.7203902006149292
Epoch 960, training loss: 6.347130298614502 = 0.03493371605873108 + 1.0 * 6.312196731567383
Epoch 960, val loss: 0.7239606380462646
Epoch 970, training loss: 6.34810733795166 = 0.03371356427669525 + 1.0 * 6.314393997192383
Epoch 970, val loss: 0.7275356650352478
Epoch 980, training loss: 6.342007637023926 = 0.032556917518377304 + 1.0 * 6.309450626373291
Epoch 980, val loss: 0.7311146855354309
Epoch 990, training loss: 6.3416290283203125 = 0.031451500952243805 + 1.0 * 6.310177326202393
Epoch 990, val loss: 0.7346628308296204
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8334211913547708
=== training gcn model ===
Epoch 0, training loss: 10.535821914672852 = 1.939039945602417 + 1.0 * 8.596781730651855
Epoch 0, val loss: 1.9465992450714111
Epoch 10, training loss: 10.52504825592041 = 1.928722620010376 + 1.0 * 8.596325874328613
Epoch 10, val loss: 1.9359244108200073
Epoch 20, training loss: 10.508061408996582 = 1.914969563484192 + 1.0 * 8.59309196472168
Epoch 20, val loss: 1.9210399389266968
Epoch 30, training loss: 10.466559410095215 = 1.894876480102539 + 1.0 * 8.571682929992676
Epoch 30, val loss: 1.898762822151184
Epoch 40, training loss: 10.316483497619629 = 1.8692067861557007 + 1.0 * 8.447277069091797
Epoch 40, val loss: 1.8717529773712158
Epoch 50, training loss: 9.844735145568848 = 1.8416626453399658 + 1.0 * 8.003072738647461
Epoch 50, val loss: 1.8437669277191162
Epoch 60, training loss: 9.3921537399292 = 1.820442795753479 + 1.0 * 7.57171106338501
Epoch 60, val loss: 1.824857234954834
Epoch 70, training loss: 8.95376205444336 = 1.8078287839889526 + 1.0 * 7.145933151245117
Epoch 70, val loss: 1.813256859779358
Epoch 80, training loss: 8.734025001525879 = 1.7960124015808105 + 1.0 * 6.938012599945068
Epoch 80, val loss: 1.8011040687561035
Epoch 90, training loss: 8.620473861694336 = 1.7795782089233398 + 1.0 * 6.840895175933838
Epoch 90, val loss: 1.7847124338150024
Epoch 100, training loss: 8.545777320861816 = 1.762100338935852 + 1.0 * 6.783676624298096
Epoch 100, val loss: 1.767849326133728
Epoch 110, training loss: 8.483562469482422 = 1.7459449768066406 + 1.0 * 6.737617492675781
Epoch 110, val loss: 1.752598524093628
Epoch 120, training loss: 8.423364639282227 = 1.729562759399414 + 1.0 * 6.6938018798828125
Epoch 120, val loss: 1.737803339958191
Epoch 130, training loss: 8.36832046508789 = 1.7113062143325806 + 1.0 * 6.6570143699646
Epoch 130, val loss: 1.7221919298171997
Epoch 140, training loss: 8.308462142944336 = 1.690675139427185 + 1.0 * 6.6177873611450195
Epoch 140, val loss: 1.705005168914795
Epoch 150, training loss: 8.255603790283203 = 1.6665747165679932 + 1.0 * 6.589028835296631
Epoch 150, val loss: 1.6850166320800781
Epoch 160, training loss: 8.203949928283691 = 1.6381680965423584 + 1.0 * 6.565781593322754
Epoch 160, val loss: 1.6613008975982666
Epoch 170, training loss: 8.15240478515625 = 1.605381727218628 + 1.0 * 6.547022819519043
Epoch 170, val loss: 1.6337594985961914
Epoch 180, training loss: 8.097723960876465 = 1.5674335956573486 + 1.0 * 6.530290603637695
Epoch 180, val loss: 1.6019313335418701
Epoch 190, training loss: 8.039541244506836 = 1.5237765312194824 + 1.0 * 6.5157647132873535
Epoch 190, val loss: 1.565333366394043
Epoch 200, training loss: 7.977367401123047 = 1.4738489389419556 + 1.0 * 6.503518581390381
Epoch 200, val loss: 1.5235645771026611
Epoch 210, training loss: 7.9155120849609375 = 1.4189372062683105 + 1.0 * 6.496574878692627
Epoch 210, val loss: 1.4780669212341309
Epoch 220, training loss: 7.845252990722656 = 1.3607943058013916 + 1.0 * 6.4844584465026855
Epoch 220, val loss: 1.43008553981781
Epoch 230, training loss: 7.776405334472656 = 1.2999294996261597 + 1.0 * 6.476475715637207
Epoch 230, val loss: 1.380393385887146
Epoch 240, training loss: 7.705723762512207 = 1.2378246784210205 + 1.0 * 6.467898845672607
Epoch 240, val loss: 1.3299036026000977
Epoch 250, training loss: 7.637185096740723 = 1.1755096912384033 + 1.0 * 6.46167516708374
Epoch 250, val loss: 1.279633641242981
Epoch 260, training loss: 7.570673942565918 = 1.1153351068496704 + 1.0 * 6.455338954925537
Epoch 260, val loss: 1.2316607236862183
Epoch 270, training loss: 7.506661415100098 = 1.0582696199417114 + 1.0 * 6.448391914367676
Epoch 270, val loss: 1.1866505146026611
Epoch 280, training loss: 7.445499420166016 = 1.0042531490325928 + 1.0 * 6.441246509552002
Epoch 280, val loss: 1.144657850265503
Epoch 290, training loss: 7.393963813781738 = 0.9531866908073425 + 1.0 * 6.44077730178833
Epoch 290, val loss: 1.1055930852890015
Epoch 300, training loss: 7.337367057800293 = 0.9055546522140503 + 1.0 * 6.431812286376953
Epoch 300, val loss: 1.0696039199829102
Epoch 310, training loss: 7.290639877319336 = 0.8605408072471619 + 1.0 * 6.430099010467529
Epoch 310, val loss: 1.0361790657043457
Epoch 320, training loss: 7.239145278930664 = 0.8182038068771362 + 1.0 * 6.420941352844238
Epoch 320, val loss: 1.0050612688064575
Epoch 330, training loss: 7.196468353271484 = 0.7781622409820557 + 1.0 * 6.418306350708008
Epoch 330, val loss: 0.976183295249939
Epoch 340, training loss: 7.153256893157959 = 0.7404754757881165 + 1.0 * 6.412781238555908
Epoch 340, val loss: 0.9495019912719727
Epoch 350, training loss: 7.112679958343506 = 0.7049716711044312 + 1.0 * 6.407708168029785
Epoch 350, val loss: 0.9248897433280945
Epoch 360, training loss: 7.077378273010254 = 0.6717139482498169 + 1.0 * 6.405664443969727
Epoch 360, val loss: 0.9023767113685608
Epoch 370, training loss: 7.0420966148376465 = 0.6408361196517944 + 1.0 * 6.4012603759765625
Epoch 370, val loss: 0.8819650411605835
Epoch 380, training loss: 7.012625217437744 = 0.6117499470710754 + 1.0 * 6.400875091552734
Epoch 380, val loss: 0.863456666469574
Epoch 390, training loss: 6.978745937347412 = 0.5844155550003052 + 1.0 * 6.3943305015563965
Epoch 390, val loss: 0.8465551733970642
Epoch 400, training loss: 6.948627471923828 = 0.5584161877632141 + 1.0 * 6.39021110534668
Epoch 400, val loss: 0.8312733769416809
Epoch 410, training loss: 6.924651145935059 = 0.533545732498169 + 1.0 * 6.3911051750183105
Epoch 410, val loss: 0.8173109889030457
Epoch 420, training loss: 6.89821720123291 = 0.5098950862884521 + 1.0 * 6.388321876525879
Epoch 420, val loss: 0.804487943649292
Epoch 430, training loss: 6.86964225769043 = 0.4871954917907715 + 1.0 * 6.382446765899658
Epoch 430, val loss: 0.7928898930549622
Epoch 440, training loss: 6.845217704772949 = 0.46515539288520813 + 1.0 * 6.380062103271484
Epoch 440, val loss: 0.7822102904319763
Epoch 450, training loss: 6.820906162261963 = 0.4437035620212555 + 1.0 * 6.37720251083374
Epoch 450, val loss: 0.772338330745697
Epoch 460, training loss: 6.799871444702148 = 0.42280659079551697 + 1.0 * 6.3770647048950195
Epoch 460, val loss: 0.7631815671920776
Epoch 470, training loss: 6.778953552246094 = 0.40241965651512146 + 1.0 * 6.3765339851379395
Epoch 470, val loss: 0.7547715902328491
Epoch 480, training loss: 6.75284481048584 = 0.3824796676635742 + 1.0 * 6.370365142822266
Epoch 480, val loss: 0.746832013130188
Epoch 490, training loss: 6.729611873626709 = 0.3629125654697418 + 1.0 * 6.36669921875
Epoch 490, val loss: 0.739508330821991
Epoch 500, training loss: 6.716194152832031 = 0.3437316417694092 + 1.0 * 6.372462749481201
Epoch 500, val loss: 0.7327430248260498
Epoch 510, training loss: 6.694766521453857 = 0.32531431317329407 + 1.0 * 6.369451999664307
Epoch 510, val loss: 0.7265156507492065
Epoch 520, training loss: 6.672091484069824 = 0.30769315361976624 + 1.0 * 6.36439847946167
Epoch 520, val loss: 0.7209222316741943
Epoch 530, training loss: 6.649435520172119 = 0.29070207476615906 + 1.0 * 6.358733654022217
Epoch 530, val loss: 0.7159973978996277
Epoch 540, training loss: 6.637462139129639 = 0.2743230164051056 + 1.0 * 6.3631391525268555
Epoch 540, val loss: 0.7116066813468933
Epoch 550, training loss: 6.619569778442383 = 0.25875505805015564 + 1.0 * 6.360814571380615
Epoch 550, val loss: 0.7077881693840027
Epoch 560, training loss: 6.601312637329102 = 0.2439691126346588 + 1.0 * 6.357343673706055
Epoch 560, val loss: 0.7047091722488403
Epoch 570, training loss: 6.586844444274902 = 0.22996392846107483 + 1.0 * 6.3568806648254395
Epoch 570, val loss: 0.7022684216499329
Epoch 580, training loss: 6.569936752319336 = 0.21681194007396698 + 1.0 * 6.353124618530273
Epoch 580, val loss: 0.7003813982009888
Epoch 590, training loss: 6.554428577423096 = 0.2044345885515213 + 1.0 * 6.34999418258667
Epoch 590, val loss: 0.6992964744567871
Epoch 600, training loss: 6.542323112487793 = 0.1927785575389862 + 1.0 * 6.349544525146484
Epoch 600, val loss: 0.6988755464553833
Epoch 610, training loss: 6.534791469573975 = 0.18188898265361786 + 1.0 * 6.352902412414551
Epoch 610, val loss: 0.6989702582359314
Epoch 620, training loss: 6.517723083496094 = 0.17179349064826965 + 1.0 * 6.3459296226501465
Epoch 620, val loss: 0.6997281908988953
Epoch 630, training loss: 6.505253314971924 = 0.16232338547706604 + 1.0 * 6.342929840087891
Epoch 630, val loss: 0.7011017203330994
Epoch 640, training loss: 6.508697032928467 = 0.15348954498767853 + 1.0 * 6.355207443237305
Epoch 640, val loss: 0.7029350996017456
Epoch 650, training loss: 6.486516952514648 = 0.14531920850276947 + 1.0 * 6.341197967529297
Epoch 650, val loss: 0.7051851153373718
Epoch 660, training loss: 6.4768967628479 = 0.137690469622612 + 1.0 * 6.339206218719482
Epoch 660, val loss: 0.7080291509628296
Epoch 670, training loss: 6.4671220779418945 = 0.1305527687072754 + 1.0 * 6.336569309234619
Epoch 670, val loss: 0.7112776041030884
Epoch 680, training loss: 6.474495887756348 = 0.12388571351766586 + 1.0 * 6.350610256195068
Epoch 680, val loss: 0.7147430181503296
Epoch 690, training loss: 6.457881927490234 = 0.11772870272397995 + 1.0 * 6.340153217315674
Epoch 690, val loss: 0.71851646900177
Epoch 700, training loss: 6.446151256561279 = 0.11195521056652069 + 1.0 * 6.334196090698242
Epoch 700, val loss: 0.7227495908737183
Epoch 710, training loss: 6.4403486251831055 = 0.10652560740709305 + 1.0 * 6.333823204040527
Epoch 710, val loss: 0.7271542549133301
Epoch 720, training loss: 6.434065341949463 = 0.10144995152950287 + 1.0 * 6.332615375518799
Epoch 720, val loss: 0.7315912842750549
Epoch 730, training loss: 6.427920818328857 = 0.09670440107584 + 1.0 * 6.331216335296631
Epoch 730, val loss: 0.7363977432250977
Epoch 740, training loss: 6.419569492340088 = 0.09222661703824997 + 1.0 * 6.327342987060547
Epoch 740, val loss: 0.7413932085037231
Epoch 750, training loss: 6.419196128845215 = 0.08800450712442398 + 1.0 * 6.331191539764404
Epoch 750, val loss: 0.746426522731781
Epoch 760, training loss: 6.408979415893555 = 0.0840311199426651 + 1.0 * 6.324948310852051
Epoch 760, val loss: 0.7515895366668701
Epoch 770, training loss: 6.411905765533447 = 0.08028130978345871 + 1.0 * 6.331624507904053
Epoch 770, val loss: 0.7568950057029724
Epoch 780, training loss: 6.402307987213135 = 0.07675577700138092 + 1.0 * 6.325551986694336
Epoch 780, val loss: 0.7620708346366882
Epoch 790, training loss: 6.401095867156982 = 0.07343321293592453 + 1.0 * 6.327662467956543
Epoch 790, val loss: 0.7675120234489441
Epoch 800, training loss: 6.395564079284668 = 0.07030095160007477 + 1.0 * 6.325263023376465
Epoch 800, val loss: 0.7726934552192688
Epoch 810, training loss: 6.386425495147705 = 0.06734643876552582 + 1.0 * 6.3190789222717285
Epoch 810, val loss: 0.7781199216842651
Epoch 820, training loss: 6.382600784301758 = 0.06453227251768112 + 1.0 * 6.318068504333496
Epoch 820, val loss: 0.7836065292358398
Epoch 830, training loss: 6.3785858154296875 = 0.06185556948184967 + 1.0 * 6.31673002243042
Epoch 830, val loss: 0.7890172600746155
Epoch 840, training loss: 6.379948139190674 = 0.0593140572309494 + 1.0 * 6.320633888244629
Epoch 840, val loss: 0.7944428324699402
Epoch 850, training loss: 6.3812689781188965 = 0.056914690881967545 + 1.0 * 6.32435417175293
Epoch 850, val loss: 0.7997256517410278
Epoch 860, training loss: 6.371150016784668 = 0.05465741083025932 + 1.0 * 6.316492557525635
Epoch 860, val loss: 0.80495685338974
Epoch 870, training loss: 6.3650712966918945 = 0.05251795053482056 + 1.0 * 6.312553405761719
Epoch 870, val loss: 0.8103572726249695
Epoch 880, training loss: 6.3622727394104 = 0.05047868192195892 + 1.0 * 6.311794281005859
Epoch 880, val loss: 0.8156846761703491
Epoch 890, training loss: 6.359846591949463 = 0.048532649874687195 + 1.0 * 6.311314105987549
Epoch 890, val loss: 0.820957362651825
Epoch 900, training loss: 6.367644786834717 = 0.046681828796863556 + 1.0 * 6.320962905883789
Epoch 900, val loss: 0.82608962059021
Epoch 910, training loss: 6.355124473571777 = 0.04493996873497963 + 1.0 * 6.310184478759766
Epoch 910, val loss: 0.8312260508537292
Epoch 920, training loss: 6.356096267700195 = 0.043283261358737946 + 1.0 * 6.312812805175781
Epoch 920, val loss: 0.836467981338501
Epoch 930, training loss: 6.3500823974609375 = 0.04170938581228256 + 1.0 * 6.308372974395752
Epoch 930, val loss: 0.8414641618728638
Epoch 940, training loss: 6.346782684326172 = 0.04021588712930679 + 1.0 * 6.3065667152404785
Epoch 940, val loss: 0.8465563654899597
Epoch 950, training loss: 6.344653129577637 = 0.03878745809197426 + 1.0 * 6.30586576461792
Epoch 950, val loss: 0.851703405380249
Epoch 960, training loss: 6.35152006149292 = 0.03742174804210663 + 1.0 * 6.314098358154297
Epoch 960, val loss: 0.8566941022872925
Epoch 970, training loss: 6.3458099365234375 = 0.03612717613577843 + 1.0 * 6.309682846069336
Epoch 970, val loss: 0.8615544438362122
Epoch 980, training loss: 6.339056491851807 = 0.03489358350634575 + 1.0 * 6.304162979125977
Epoch 980, val loss: 0.866578996181488
Epoch 990, training loss: 6.336243629455566 = 0.03371310606598854 + 1.0 * 6.302530288696289
Epoch 990, val loss: 0.8715529441833496
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.5369234085083 = 1.9401018619537354 + 1.0 * 8.596821784973145
Epoch 0, val loss: 1.9423093795776367
Epoch 10, training loss: 10.527400970458984 = 1.9308713674545288 + 1.0 * 8.596529960632324
Epoch 10, val loss: 1.9335523843765259
Epoch 20, training loss: 10.514161109924316 = 1.9197217226028442 + 1.0 * 8.594439506530762
Epoch 20, val loss: 1.922545313835144
Epoch 30, training loss: 10.483007431030273 = 1.9044747352600098 + 1.0 * 8.578533172607422
Epoch 30, val loss: 1.907058835029602
Epoch 40, training loss: 10.357666969299316 = 1.8845336437225342 + 1.0 * 8.473133087158203
Epoch 40, val loss: 1.8873810768127441
Epoch 50, training loss: 9.86213493347168 = 1.8625495433807373 + 1.0 * 7.9995856285095215
Epoch 50, val loss: 1.8661997318267822
Epoch 60, training loss: 9.351812362670898 = 1.844378113746643 + 1.0 * 7.507433891296387
Epoch 60, val loss: 1.8491729497909546
Epoch 70, training loss: 8.920082092285156 = 1.8322278261184692 + 1.0 * 7.087853908538818
Epoch 70, val loss: 1.8371500968933105
Epoch 80, training loss: 8.754828453063965 = 1.8195414543151855 + 1.0 * 6.935286998748779
Epoch 80, val loss: 1.8240810632705688
Epoch 90, training loss: 8.671783447265625 = 1.8027904033660889 + 1.0 * 6.868993282318115
Epoch 90, val loss: 1.8073164224624634
Epoch 100, training loss: 8.58885383605957 = 1.7854763269424438 + 1.0 * 6.803377628326416
Epoch 100, val loss: 1.7912341356277466
Epoch 110, training loss: 8.515731811523438 = 1.7704094648361206 + 1.0 * 6.7453227043151855
Epoch 110, val loss: 1.7773334980010986
Epoch 120, training loss: 8.452010154724121 = 1.7558234930038452 + 1.0 * 6.696186542510986
Epoch 120, val loss: 1.7632352113723755
Epoch 130, training loss: 8.397673606872559 = 1.7396655082702637 + 1.0 * 6.658008098602295
Epoch 130, val loss: 1.747794270515442
Epoch 140, training loss: 8.346931457519531 = 1.721036672592163 + 1.0 * 6.625895023345947
Epoch 140, val loss: 1.730501413345337
Epoch 150, training loss: 8.2968111038208 = 1.6992278099060059 + 1.0 * 6.597583293914795
Epoch 150, val loss: 1.710782766342163
Epoch 160, training loss: 8.25010871887207 = 1.673350214958191 + 1.0 * 6.576758861541748
Epoch 160, val loss: 1.6877413988113403
Epoch 170, training loss: 8.199437141418457 = 1.643242359161377 + 1.0 * 6.55619478225708
Epoch 170, val loss: 1.6609675884246826
Epoch 180, training loss: 8.147994995117188 = 1.6078652143478394 + 1.0 * 6.540129661560059
Epoch 180, val loss: 1.6296725273132324
Epoch 190, training loss: 8.092144966125488 = 1.5660814046859741 + 1.0 * 6.526063442230225
Epoch 190, val loss: 1.5928523540496826
Epoch 200, training loss: 8.031142234802246 = 1.5176489353179932 + 1.0 * 6.513493537902832
Epoch 200, val loss: 1.5504299402236938
Epoch 210, training loss: 7.968070030212402 = 1.4637356996536255 + 1.0 * 6.504334449768066
Epoch 210, val loss: 1.5037072896957397
Epoch 220, training loss: 7.898416996002197 = 1.4056181907653809 + 1.0 * 6.492798805236816
Epoch 220, val loss: 1.4541219472885132
Epoch 230, training loss: 7.830885887145996 = 1.3442565202713013 + 1.0 * 6.486629486083984
Epoch 230, val loss: 1.4024949073791504
Epoch 240, training loss: 7.758919715881348 = 1.282250165939331 + 1.0 * 6.4766693115234375
Epoch 240, val loss: 1.351172685623169
Epoch 250, training loss: 7.689975738525391 = 1.2207908630371094 + 1.0 * 6.469184875488281
Epoch 250, val loss: 1.3011727333068848
Epoch 260, training loss: 7.623961925506592 = 1.1608033180236816 + 1.0 * 6.46315860748291
Epoch 260, val loss: 1.2532912492752075
Epoch 270, training loss: 7.564483642578125 = 1.1045067310333252 + 1.0 * 6.459976673126221
Epoch 270, val loss: 1.2092126607894897
Epoch 280, training loss: 7.504160404205322 = 1.0528669357299805 + 1.0 * 6.451293468475342
Epoch 280, val loss: 1.1697187423706055
Epoch 290, training loss: 7.448709011077881 = 1.005006194114685 + 1.0 * 6.443702697753906
Epoch 290, val loss: 1.133763313293457
Epoch 300, training loss: 7.409738063812256 = 0.9604483246803284 + 1.0 * 6.449289798736572
Epoch 300, val loss: 1.1007570028305054
Epoch 310, training loss: 7.355018615722656 = 0.9197421669960022 + 1.0 * 6.435276508331299
Epoch 310, val loss: 1.0708304643630981
Epoch 320, training loss: 7.309131622314453 = 0.8816454410552979 + 1.0 * 6.427485942840576
Epoch 320, val loss: 1.0429738759994507
Epoch 330, training loss: 7.27099084854126 = 0.8454772233963013 + 1.0 * 6.425513744354248
Epoch 330, val loss: 1.0166593790054321
Epoch 340, training loss: 7.230978012084961 = 0.8110729455947876 + 1.0 * 6.419905185699463
Epoch 340, val loss: 0.9918068051338196
Epoch 350, training loss: 7.193026065826416 = 0.7779248356819153 + 1.0 * 6.415101051330566
Epoch 350, val loss: 0.9680748581886292
Epoch 360, training loss: 7.156097412109375 = 0.7458848357200623 + 1.0 * 6.410212516784668
Epoch 360, val loss: 0.9451915621757507
Epoch 370, training loss: 7.1215925216674805 = 0.7149606347084045 + 1.0 * 6.406631946563721
Epoch 370, val loss: 0.9233814477920532
Epoch 380, training loss: 7.096057891845703 = 0.6850846409797668 + 1.0 * 6.410973072052002
Epoch 380, val loss: 0.9025765061378479
Epoch 390, training loss: 7.055460453033447 = 0.6564897894859314 + 1.0 * 6.398970603942871
Epoch 390, val loss: 0.8831313252449036
Epoch 400, training loss: 7.034718990325928 = 0.6290878653526306 + 1.0 * 6.405631065368652
Epoch 400, val loss: 0.8650293350219727
Epoch 410, training loss: 6.996047496795654 = 0.6031205058097839 + 1.0 * 6.392927169799805
Epoch 410, val loss: 0.8484147787094116
Epoch 420, training loss: 6.967670440673828 = 0.578217089176178 + 1.0 * 6.389453411102295
Epoch 420, val loss: 0.8330698013305664
Epoch 430, training loss: 6.941590309143066 = 0.5542929172515869 + 1.0 * 6.3872971534729
Epoch 430, val loss: 0.8188450932502747
Epoch 440, training loss: 6.914524555206299 = 0.5313940048217773 + 1.0 * 6.3831305503845215
Epoch 440, val loss: 0.8058847784996033
Epoch 450, training loss: 6.889407157897949 = 0.5092536807060242 + 1.0 * 6.380153656005859
Epoch 450, val loss: 0.7939677834510803
Epoch 460, training loss: 6.87107515335083 = 0.4877484142780304 + 1.0 * 6.383326530456543
Epoch 460, val loss: 0.782925009727478
Epoch 470, training loss: 6.8459553718566895 = 0.46709108352661133 + 1.0 * 6.378864288330078
Epoch 470, val loss: 0.7728980779647827
Epoch 480, training loss: 6.821493148803711 = 0.447264164686203 + 1.0 * 6.3742289543151855
Epoch 480, val loss: 0.7638197541236877
Epoch 490, training loss: 6.797671794891357 = 0.427961140871048 + 1.0 * 6.369710445404053
Epoch 490, val loss: 0.7554661631584167
Epoch 500, training loss: 6.778861999511719 = 0.40906521677970886 + 1.0 * 6.3697967529296875
Epoch 500, val loss: 0.747775673866272
Epoch 510, training loss: 6.758751392364502 = 0.3906639814376831 + 1.0 * 6.368087291717529
Epoch 510, val loss: 0.740705668926239
Epoch 520, training loss: 6.737781524658203 = 0.3727779984474182 + 1.0 * 6.36500358581543
Epoch 520, val loss: 0.7342393398284912
Epoch 530, training loss: 6.7187042236328125 = 0.35527947545051575 + 1.0 * 6.363424777984619
Epoch 530, val loss: 0.7282751202583313
Epoch 540, training loss: 6.698131084442139 = 0.33806946873664856 + 1.0 * 6.3600616455078125
Epoch 540, val loss: 0.7227994799613953
Epoch 550, training loss: 6.686143398284912 = 0.3211759030818939 + 1.0 * 6.364967346191406
Epoch 550, val loss: 0.7178144454956055
Epoch 560, training loss: 6.661798477172852 = 0.3047487735748291 + 1.0 * 6.357049465179443
Epoch 560, val loss: 0.7132911682128906
Epoch 570, training loss: 6.6444411277771 = 0.28877630829811096 + 1.0 * 6.3556647300720215
Epoch 570, val loss: 0.7093250751495361
Epoch 580, training loss: 6.628174304962158 = 0.2732318937778473 + 1.0 * 6.354942321777344
Epoch 580, val loss: 0.7059100270271301
Epoch 590, training loss: 6.612556457519531 = 0.25829631090164185 + 1.0 * 6.354259967803955
Epoch 590, val loss: 0.7029973864555359
Epoch 600, training loss: 6.594371795654297 = 0.24401655793190002 + 1.0 * 6.35035514831543
Epoch 600, val loss: 0.7007283568382263
Epoch 610, training loss: 6.579413414001465 = 0.23038353025913239 + 1.0 * 6.349030017852783
Epoch 610, val loss: 0.699093759059906
Epoch 620, training loss: 6.567548751831055 = 0.21743151545524597 + 1.0 * 6.350117206573486
Epoch 620, val loss: 0.6980386972427368
Epoch 630, training loss: 6.559876918792725 = 0.20521289110183716 + 1.0 * 6.354663848876953
Epoch 630, val loss: 0.6975305080413818
Epoch 640, training loss: 6.539900302886963 = 0.1937706172466278 + 1.0 * 6.346129894256592
Epoch 640, val loss: 0.6976523995399475
Epoch 650, training loss: 6.526337146759033 = 0.1830199956893921 + 1.0 * 6.343317031860352
Epoch 650, val loss: 0.6983503103256226
Epoch 660, training loss: 6.513688087463379 = 0.17290988564491272 + 1.0 * 6.340778350830078
Epoch 660, val loss: 0.6996359825134277
Epoch 670, training loss: 6.5044426918029785 = 0.16340801119804382 + 1.0 * 6.341034889221191
Epoch 670, val loss: 0.7014313340187073
Epoch 680, training loss: 6.496031761169434 = 0.15453481674194336 + 1.0 * 6.34149694442749
Epoch 680, val loss: 0.7036632895469666
Epoch 690, training loss: 6.483683109283447 = 0.14628666639328003 + 1.0 * 6.337396621704102
Epoch 690, val loss: 0.706195056438446
Epoch 700, training loss: 6.475053310394287 = 0.1385694444179535 + 1.0 * 6.336483955383301
Epoch 700, val loss: 0.7091922760009766
Epoch 710, training loss: 6.470401287078857 = 0.13133962452411652 + 1.0 * 6.339061737060547
Epoch 710, val loss: 0.7126086354255676
Epoch 720, training loss: 6.468809604644775 = 0.12458902597427368 + 1.0 * 6.3442206382751465
Epoch 720, val loss: 0.7162677645683289
Epoch 730, training loss: 6.45505428314209 = 0.1182905063033104 + 1.0 * 6.336763858795166
Epoch 730, val loss: 0.7201847434043884
Epoch 740, training loss: 6.44325065612793 = 0.11239427328109741 + 1.0 * 6.3308563232421875
Epoch 740, val loss: 0.7243844270706177
Epoch 750, training loss: 6.4372124671936035 = 0.10684739053249359 + 1.0 * 6.330365180969238
Epoch 750, val loss: 0.7288825511932373
Epoch 760, training loss: 6.4322075843811035 = 0.10164979845285416 + 1.0 * 6.330557823181152
Epoch 760, val loss: 0.7335785031318665
Epoch 770, training loss: 6.426057815551758 = 0.09678258001804352 + 1.0 * 6.329275131225586
Epoch 770, val loss: 0.7383508086204529
Epoch 780, training loss: 6.42163610458374 = 0.09220481663942337 + 1.0 * 6.329431056976318
Epoch 780, val loss: 0.7433575987815857
Epoch 790, training loss: 6.413524150848389 = 0.08790948987007141 + 1.0 * 6.3256144523620605
Epoch 790, val loss: 0.7485242486000061
Epoch 800, training loss: 6.408259868621826 = 0.08386670053005219 + 1.0 * 6.324393272399902
Epoch 800, val loss: 0.753791093826294
Epoch 810, training loss: 6.40408182144165 = 0.0800536721944809 + 1.0 * 6.324028015136719
Epoch 810, val loss: 0.7592511177062988
Epoch 820, training loss: 6.409857749938965 = 0.07645978778600693 + 1.0 * 6.33339786529541
Epoch 820, val loss: 0.7647891640663147
Epoch 830, training loss: 6.39661169052124 = 0.07309487462043762 + 1.0 * 6.323516845703125
Epoch 830, val loss: 0.7702997922897339
Epoch 840, training loss: 6.390057563781738 = 0.06991267204284668 + 1.0 * 6.320145130157471
Epoch 840, val loss: 0.7758874297142029
Epoch 850, training loss: 6.386673450469971 = 0.06690522283315659 + 1.0 * 6.31976842880249
Epoch 850, val loss: 0.7816111445426941
Epoch 860, training loss: 6.391507625579834 = 0.06407210975885391 + 1.0 * 6.327435493469238
Epoch 860, val loss: 0.787370502948761
Epoch 870, training loss: 6.37908411026001 = 0.061408381909132004 + 1.0 * 6.317675590515137
Epoch 870, val loss: 0.7930261492729187
Epoch 880, training loss: 6.380266189575195 = 0.058895666152238846 + 1.0 * 6.321370601654053
Epoch 880, val loss: 0.7987455129623413
Epoch 890, training loss: 6.37184476852417 = 0.056521326303482056 + 1.0 * 6.315323352813721
Epoch 890, val loss: 0.8045152425765991
Epoch 900, training loss: 6.37039041519165 = 0.05427270755171776 + 1.0 * 6.316117763519287
Epoch 900, val loss: 0.8102832436561584
Epoch 910, training loss: 6.368433952331543 = 0.05214151740074158 + 1.0 * 6.3162922859191895
Epoch 910, val loss: 0.8160907030105591
Epoch 920, training loss: 6.364915370941162 = 0.05012881010770798 + 1.0 * 6.314786434173584
Epoch 920, val loss: 0.821807324886322
Epoch 930, training loss: 6.363166332244873 = 0.04823051020503044 + 1.0 * 6.314935684204102
Epoch 930, val loss: 0.8274314999580383
Epoch 940, training loss: 6.3610734939575195 = 0.04643024876713753 + 1.0 * 6.314643383026123
Epoch 940, val loss: 0.8330662846565247
Epoch 950, training loss: 6.356412887573242 = 0.044724948704242706 + 1.0 * 6.31168794631958
Epoch 950, val loss: 0.8386878371238708
Epoch 960, training loss: 6.354247093200684 = 0.04310805723071098 + 1.0 * 6.311139106750488
Epoch 960, val loss: 0.8442603349685669
Epoch 970, training loss: 6.353166103363037 = 0.041569165885448456 + 1.0 * 6.311596870422363
Epoch 970, val loss: 0.8498457670211792
Epoch 980, training loss: 6.350407123565674 = 0.04011024907231331 + 1.0 * 6.310297012329102
Epoch 980, val loss: 0.8553751111030579
Epoch 990, training loss: 6.350273609161377 = 0.038721565157175064 + 1.0 * 6.311552047729492
Epoch 990, val loss: 0.8608314990997314
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8334211913547708
The final CL Acc:0.81481, 0.01210, The final GNN Acc:0.83518, 0.00249
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10572])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.54804801940918 = 1.951225757598877 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.9576655626296997
Epoch 10, training loss: 10.537879943847656 = 1.9412604570388794 + 1.0 * 8.596619606018066
Epoch 10, val loss: 1.9470754861831665
Epoch 20, training loss: 10.524542808532715 = 1.9295769929885864 + 1.0 * 8.594965934753418
Epoch 20, val loss: 1.9345450401306152
Epoch 30, training loss: 10.495598793029785 = 1.91368567943573 + 1.0 * 8.581912994384766
Epoch 30, val loss: 1.9175397157669067
Epoch 40, training loss: 10.396236419677734 = 1.8925015926361084 + 1.0 * 8.503734588623047
Epoch 40, val loss: 1.8956434726715088
Epoch 50, training loss: 10.070104598999023 = 1.8692653179168701 + 1.0 * 8.200839042663574
Epoch 50, val loss: 1.8727631568908691
Epoch 60, training loss: 9.600971221923828 = 1.8499099016189575 + 1.0 * 7.751060962677002
Epoch 60, val loss: 1.854884386062622
Epoch 70, training loss: 9.113875389099121 = 1.8371583223342896 + 1.0 * 7.276716709136963
Epoch 70, val loss: 1.8434045314788818
Epoch 80, training loss: 8.890931129455566 = 1.824696660041809 + 1.0 * 7.066234588623047
Epoch 80, val loss: 1.8314049243927002
Epoch 90, training loss: 8.744771957397461 = 1.807110071182251 + 1.0 * 6.937661647796631
Epoch 90, val loss: 1.8145993947982788
Epoch 100, training loss: 8.633150100708008 = 1.789762020111084 + 1.0 * 6.843388557434082
Epoch 100, val loss: 1.7985678911209106
Epoch 110, training loss: 8.55256175994873 = 1.774463415145874 + 1.0 * 6.7780985832214355
Epoch 110, val loss: 1.7843077182769775
Epoch 120, training loss: 8.489726066589355 = 1.7591781616210938 + 1.0 * 6.730547904968262
Epoch 120, val loss: 1.769990086555481
Epoch 130, training loss: 8.438909530639648 = 1.7427247762680054 + 1.0 * 6.696185111999512
Epoch 130, val loss: 1.7550102472305298
Epoch 140, training loss: 8.39477825164795 = 1.724658727645874 + 1.0 * 6.670119285583496
Epoch 140, val loss: 1.739105463027954
Epoch 150, training loss: 8.351519584655762 = 1.7041398286819458 + 1.0 * 6.6473798751831055
Epoch 150, val loss: 1.7216393947601318
Epoch 160, training loss: 8.309038162231445 = 1.680564045906067 + 1.0 * 6.628474235534668
Epoch 160, val loss: 1.7018307447433472
Epoch 170, training loss: 8.263228416442871 = 1.6538536548614502 + 1.0 * 6.609375
Epoch 170, val loss: 1.679545283317566
Epoch 180, training loss: 8.214920043945312 = 1.6232072114944458 + 1.0 * 6.591712951660156
Epoch 180, val loss: 1.6539572477340698
Epoch 190, training loss: 8.164320945739746 = 1.5879968404769897 + 1.0 * 6.576324462890625
Epoch 190, val loss: 1.6246328353881836
Epoch 200, training loss: 8.114178657531738 = 1.548526644706726 + 1.0 * 6.565652370452881
Epoch 200, val loss: 1.59200918674469
Epoch 210, training loss: 8.056299209594727 = 1.5056394338607788 + 1.0 * 6.550660133361816
Epoch 210, val loss: 1.5567346811294556
Epoch 220, training loss: 7.996094703674316 = 1.4597604274749756 + 1.0 * 6.53633451461792
Epoch 220, val loss: 1.5195472240447998
Epoch 230, training loss: 7.935834884643555 = 1.4115163087844849 + 1.0 * 6.524318695068359
Epoch 230, val loss: 1.4810978174209595
Epoch 240, training loss: 7.88124418258667 = 1.3624110221862793 + 1.0 * 6.518833160400391
Epoch 240, val loss: 1.4428224563598633
Epoch 250, training loss: 7.8186564445495605 = 1.3145629167556763 + 1.0 * 6.504093647003174
Epoch 250, val loss: 1.4060461521148682
Epoch 260, training loss: 7.762467384338379 = 1.267628788948059 + 1.0 * 6.494838714599609
Epoch 260, val loss: 1.3705929517745972
Epoch 270, training loss: 7.713468074798584 = 1.221601128578186 + 1.0 * 6.4918670654296875
Epoch 270, val loss: 1.3365671634674072
Epoch 280, training loss: 7.655834197998047 = 1.1769382953643799 + 1.0 * 6.478896141052246
Epoch 280, val loss: 1.3039871454238892
Epoch 290, training loss: 7.6052703857421875 = 1.1329025030136108 + 1.0 * 6.472367763519287
Epoch 290, val loss: 1.2720575332641602
Epoch 300, training loss: 7.57274055480957 = 1.0889829397201538 + 1.0 * 6.483757495880127
Epoch 300, val loss: 1.2403990030288696
Epoch 310, training loss: 7.509572982788086 = 1.0457463264465332 + 1.0 * 6.463826656341553
Epoch 310, val loss: 1.2092775106430054
Epoch 320, training loss: 7.458888053894043 = 1.0028839111328125 + 1.0 * 6.4560041427612305
Epoch 320, val loss: 1.178527593612671
Epoch 330, training loss: 7.409621715545654 = 0.9602165818214417 + 1.0 * 6.449405193328857
Epoch 330, val loss: 1.1480116844177246
Epoch 340, training loss: 7.362315654754639 = 0.9179764986038208 + 1.0 * 6.444339275360107
Epoch 340, val loss: 1.1177321672439575
Epoch 350, training loss: 7.317000389099121 = 0.8765577673912048 + 1.0 * 6.4404425621032715
Epoch 350, val loss: 1.0882824659347534
Epoch 360, training loss: 7.281434059143066 = 0.8368386030197144 + 1.0 * 6.4445953369140625
Epoch 360, val loss: 1.0603089332580566
Epoch 370, training loss: 7.2313666343688965 = 0.7994977235794067 + 1.0 * 6.431869029998779
Epoch 370, val loss: 1.0345418453216553
Epoch 380, training loss: 7.192200183868408 = 0.7641291618347168 + 1.0 * 6.428071022033691
Epoch 380, val loss: 1.0106741189956665
Epoch 390, training loss: 7.15606164932251 = 0.7306279540061951 + 1.0 * 6.42543363571167
Epoch 390, val loss: 0.9888043403625488
Epoch 400, training loss: 7.122674942016602 = 0.6990993022918701 + 1.0 * 6.423575401306152
Epoch 400, val loss: 0.9689115285873413
Epoch 410, training loss: 7.091442584991455 = 0.6694362759590149 + 1.0 * 6.422006130218506
Epoch 410, val loss: 0.9512197971343994
Epoch 420, training loss: 7.056342124938965 = 0.6413954496383667 + 1.0 * 6.414946556091309
Epoch 420, val loss: 0.9353588819503784
Epoch 430, training loss: 7.024256706237793 = 0.6147605180740356 + 1.0 * 6.409496307373047
Epoch 430, val loss: 0.9213753938674927
Epoch 440, training loss: 6.996793746948242 = 0.5892179012298584 + 1.0 * 6.407575607299805
Epoch 440, val loss: 0.9088149666786194
Epoch 450, training loss: 6.976938247680664 = 0.5647739171981812 + 1.0 * 6.412164211273193
Epoch 450, val loss: 0.8976572751998901
Epoch 460, training loss: 6.945067882537842 = 0.5414853692054749 + 1.0 * 6.403582572937012
Epoch 460, val loss: 0.8878604173660278
Epoch 470, training loss: 6.916666030883789 = 0.5190512537956238 + 1.0 * 6.3976149559021
Epoch 470, val loss: 0.8790938854217529
Epoch 480, training loss: 6.892312049865723 = 0.49733057618141174 + 1.0 * 6.394981384277344
Epoch 480, val loss: 0.8713271021842957
Epoch 490, training loss: 6.874610900878906 = 0.4762875735759735 + 1.0 * 6.3983235359191895
Epoch 490, val loss: 0.8644010424613953
Epoch 500, training loss: 6.846697807312012 = 0.4559642970561981 + 1.0 * 6.39073371887207
Epoch 500, val loss: 0.8582342267036438
Epoch 510, training loss: 6.826747417449951 = 0.4362078905105591 + 1.0 * 6.390539646148682
Epoch 510, val loss: 0.8527488708496094
Epoch 520, training loss: 6.808561325073242 = 0.41698166728019714 + 1.0 * 6.391579627990723
Epoch 520, val loss: 0.847728431224823
Epoch 530, training loss: 6.782556533813477 = 0.3982582688331604 + 1.0 * 6.384298324584961
Epoch 530, val loss: 0.8433377742767334
Epoch 540, training loss: 6.763012409210205 = 0.37985512614250183 + 1.0 * 6.383157253265381
Epoch 540, val loss: 0.8393440246582031
Epoch 550, training loss: 6.744207382202148 = 0.3617241084575653 + 1.0 * 6.38248348236084
Epoch 550, val loss: 0.8358030915260315
Epoch 560, training loss: 6.724461555480957 = 0.34390223026275635 + 1.0 * 6.38055944442749
Epoch 560, val loss: 0.832636296749115
Epoch 570, training loss: 6.703197002410889 = 0.3262690007686615 + 1.0 * 6.376927852630615
Epoch 570, val loss: 0.8297987580299377
Epoch 580, training loss: 6.684965133666992 = 0.3089081943035126 + 1.0 * 6.376057147979736
Epoch 580, val loss: 0.8272344470024109
Epoch 590, training loss: 6.66829776763916 = 0.29194217920303345 + 1.0 * 6.3763556480407715
Epoch 590, val loss: 0.8251073956489563
Epoch 600, training loss: 6.647976398468018 = 0.27555495500564575 + 1.0 * 6.3724212646484375
Epoch 600, val loss: 0.8233466744422913
Epoch 610, training loss: 6.628036975860596 = 0.25968679785728455 + 1.0 * 6.368350028991699
Epoch 610, val loss: 0.8219901919364929
Epoch 620, training loss: 6.625431060791016 = 0.24440564215183258 + 1.0 * 6.381025314331055
Epoch 620, val loss: 0.8210617899894714
Epoch 630, training loss: 6.597778797149658 = 0.229949489235878 + 1.0 * 6.367829322814941
Epoch 630, val loss: 0.8205474615097046
Epoch 640, training loss: 6.581156253814697 = 0.21616587042808533 + 1.0 * 6.364990234375
Epoch 640, val loss: 0.8205829858779907
Epoch 650, training loss: 6.571244716644287 = 0.20313499867916107 + 1.0 * 6.368109703063965
Epoch 650, val loss: 0.8211424946784973
Epoch 660, training loss: 6.552135467529297 = 0.19086885452270508 + 1.0 * 6.361266613006592
Epoch 660, val loss: 0.8222489356994629
Epoch 670, training loss: 6.538583278656006 = 0.17932230234146118 + 1.0 * 6.3592610359191895
Epoch 670, val loss: 0.8239454030990601
Epoch 680, training loss: 6.527304172515869 = 0.16842296719551086 + 1.0 * 6.358880996704102
Epoch 680, val loss: 0.826208770275116
Epoch 690, training loss: 6.519320487976074 = 0.158189058303833 + 1.0 * 6.361131191253662
Epoch 690, val loss: 0.828949511051178
Epoch 700, training loss: 6.5043044090271 = 0.1486484706401825 + 1.0 * 6.355656147003174
Epoch 700, val loss: 0.8321694731712341
Epoch 710, training loss: 6.494889736175537 = 0.13968674838542938 + 1.0 * 6.355203151702881
Epoch 710, val loss: 0.8358738422393799
Epoch 720, training loss: 6.492613315582275 = 0.13128662109375 + 1.0 * 6.361326694488525
Epoch 720, val loss: 0.8400089740753174
Epoch 730, training loss: 6.476243495941162 = 0.12342406809329987 + 1.0 * 6.352819442749023
Epoch 730, val loss: 0.8445245623588562
Epoch 740, training loss: 6.4671101570129395 = 0.11609160155057907 + 1.0 * 6.35101842880249
Epoch 740, val loss: 0.8494123220443726
Epoch 750, training loss: 6.46466588973999 = 0.10923323035240173 + 1.0 * 6.355432510375977
Epoch 750, val loss: 0.8546417355537415
Epoch 760, training loss: 6.453283309936523 = 0.10287916660308838 + 1.0 * 6.350404262542725
Epoch 760, val loss: 0.8600640296936035
Epoch 770, training loss: 6.444167613983154 = 0.09694736450910568 + 1.0 * 6.347220420837402
Epoch 770, val loss: 0.8657515048980713
Epoch 780, training loss: 6.4373273849487305 = 0.09142955392599106 + 1.0 * 6.345897674560547
Epoch 780, val loss: 0.8715335130691528
Epoch 790, training loss: 6.437556743621826 = 0.08628413081169128 + 1.0 * 6.3512725830078125
Epoch 790, val loss: 0.8775641322135925
Epoch 800, training loss: 6.425269603729248 = 0.08148650079965591 + 1.0 * 6.343782901763916
Epoch 800, val loss: 0.8836798667907715
Epoch 810, training loss: 6.419412136077881 = 0.07702384889125824 + 1.0 * 6.342388153076172
Epoch 810, val loss: 0.889953076839447
Epoch 820, training loss: 6.4203925132751465 = 0.0728568285703659 + 1.0 * 6.347535610198975
Epoch 820, val loss: 0.8963682055473328
Epoch 830, training loss: 6.412703990936279 = 0.06900385022163391 + 1.0 * 6.343699932098389
Epoch 830, val loss: 0.9027373194694519
Epoch 840, training loss: 6.406590461730957 = 0.06540393829345703 + 1.0 * 6.3411865234375
Epoch 840, val loss: 0.9091897010803223
Epoch 850, training loss: 6.399179458618164 = 0.06204412505030632 + 1.0 * 6.337135314941406
Epoch 850, val loss: 0.9156745076179504
Epoch 860, training loss: 6.397109508514404 = 0.058907851576805115 + 1.0 * 6.338201522827148
Epoch 860, val loss: 0.9222261905670166
Epoch 870, training loss: 6.391841411590576 = 0.05598556250333786 + 1.0 * 6.335855960845947
Epoch 870, val loss: 0.9287458658218384
Epoch 880, training loss: 6.391354084014893 = 0.05326766148209572 + 1.0 * 6.3380866050720215
Epoch 880, val loss: 0.9351602792739868
Epoch 890, training loss: 6.385218620300293 = 0.05071492865681648 + 1.0 * 6.334503650665283
Epoch 890, val loss: 0.9416037797927856
Epoch 900, training loss: 6.380781650543213 = 0.048335980623960495 + 1.0 * 6.3324456214904785
Epoch 900, val loss: 0.9479632377624512
Epoch 910, training loss: 6.377814769744873 = 0.046102989464998245 + 1.0 * 6.331711769104004
Epoch 910, val loss: 0.9543548822402954
Epoch 920, training loss: 6.378237247467041 = 0.04402346536517143 + 1.0 * 6.334213733673096
Epoch 920, val loss: 0.9606785774230957
Epoch 930, training loss: 6.3727593421936035 = 0.04206833243370056 + 1.0 * 6.330690860748291
Epoch 930, val loss: 0.9668546319007874
Epoch 940, training loss: 6.367458820343018 = 0.04024285450577736 + 1.0 * 6.327216148376465
Epoch 940, val loss: 0.9730360507965088
Epoch 950, training loss: 6.367303848266602 = 0.03852609544992447 + 1.0 * 6.32877779006958
Epoch 950, val loss: 0.9791620373725891
Epoch 960, training loss: 6.36422872543335 = 0.03691098466515541 + 1.0 * 6.327317714691162
Epoch 960, val loss: 0.9852893948554993
Epoch 970, training loss: 6.360428810119629 = 0.03539620712399483 + 1.0 * 6.325032711029053
Epoch 970, val loss: 0.9912676811218262
Epoch 980, training loss: 6.361627578735352 = 0.03397376090288162 + 1.0 * 6.327653884887695
Epoch 980, val loss: 0.997176468372345
Epoch 990, training loss: 6.354519367218018 = 0.03262777626514435 + 1.0 * 6.321891784667969
Epoch 990, val loss: 1.0030150413513184
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.7965208223510807
=== training gcn model ===
Epoch 0, training loss: 10.549609184265137 = 1.9527946710586548 + 1.0 * 8.596814155578613
Epoch 0, val loss: 1.955183744430542
Epoch 10, training loss: 10.53784465789795 = 1.9413931369781494 + 1.0 * 8.596451759338379
Epoch 10, val loss: 1.943634033203125
Epoch 20, training loss: 10.52050495147705 = 1.9268518686294556 + 1.0 * 8.593652725219727
Epoch 20, val loss: 1.928883671760559
Epoch 30, training loss: 10.480587005615234 = 1.9063091278076172 + 1.0 * 8.574277877807617
Epoch 30, val loss: 1.9084237813949585
Epoch 40, training loss: 10.349913597106934 = 1.8798305988311768 + 1.0 * 8.470083236694336
Epoch 40, val loss: 1.883239984512329
Epoch 50, training loss: 10.0376558303833 = 1.851478934288025 + 1.0 * 8.186177253723145
Epoch 50, val loss: 1.8571946620941162
Epoch 60, training loss: 9.6544771194458 = 1.8290870189666748 + 1.0 * 7.825389862060547
Epoch 60, val loss: 1.837218165397644
Epoch 70, training loss: 9.24807071685791 = 1.8138841390609741 + 1.0 * 7.4341864585876465
Epoch 70, val loss: 1.8230798244476318
Epoch 80, training loss: 8.981657981872559 = 1.8003644943237305 + 1.0 * 7.181293487548828
Epoch 80, val loss: 1.8109660148620605
Epoch 90, training loss: 8.783209800720215 = 1.7856624126434326 + 1.0 * 6.997547149658203
Epoch 90, val loss: 1.7986633777618408
Epoch 100, training loss: 8.655465126037598 = 1.771277904510498 + 1.0 * 6.8841872215271
Epoch 100, val loss: 1.7865078449249268
Epoch 110, training loss: 8.56041145324707 = 1.7572864294052124 + 1.0 * 6.803125381469727
Epoch 110, val loss: 1.7739120721817017
Epoch 120, training loss: 8.484733581542969 = 1.7423964738845825 + 1.0 * 6.742337226867676
Epoch 120, val loss: 1.7604994773864746
Epoch 130, training loss: 8.424222946166992 = 1.7257325649261475 + 1.0 * 6.698490142822266
Epoch 130, val loss: 1.7462189197540283
Epoch 140, training loss: 8.372695922851562 = 1.7066901922225952 + 1.0 * 6.666005611419678
Epoch 140, val loss: 1.7305113077163696
Epoch 150, training loss: 8.32201099395752 = 1.6846812963485718 + 1.0 * 6.637329578399658
Epoch 150, val loss: 1.7126948833465576
Epoch 160, training loss: 8.273012161254883 = 1.6594102382659912 + 1.0 * 6.613602161407471
Epoch 160, val loss: 1.6921991109848022
Epoch 170, training loss: 8.224359512329102 = 1.6304235458374023 + 1.0 * 6.593935966491699
Epoch 170, val loss: 1.668613314628601
Epoch 180, training loss: 8.174203872680664 = 1.5979228019714355 + 1.0 * 6.57628059387207
Epoch 180, val loss: 1.6423726081848145
Epoch 190, training loss: 8.121414184570312 = 1.5624825954437256 + 1.0 * 6.558931350708008
Epoch 190, val loss: 1.6140397787094116
Epoch 200, training loss: 8.068230628967285 = 1.5239754915237427 + 1.0 * 6.544255256652832
Epoch 200, val loss: 1.5835859775543213
Epoch 210, training loss: 8.02074909210205 = 1.483221411705017 + 1.0 * 6.537527561187744
Epoch 210, val loss: 1.5519541501998901
Epoch 220, training loss: 7.962779998779297 = 1.4421635866165161 + 1.0 * 6.52061653137207
Epoch 220, val loss: 1.5208848714828491
Epoch 230, training loss: 7.909945011138916 = 1.400945782661438 + 1.0 * 6.508999347686768
Epoch 230, val loss: 1.4903476238250732
Epoch 240, training loss: 7.860634803771973 = 1.3597304821014404 + 1.0 * 6.500904083251953
Epoch 240, val loss: 1.4604403972625732
Epoch 250, training loss: 7.811793327331543 = 1.3192709684371948 + 1.0 * 6.492522239685059
Epoch 250, val loss: 1.4320083856582642
Epoch 260, training loss: 7.762664318084717 = 1.2797104120254517 + 1.0 * 6.482954025268555
Epoch 260, val loss: 1.4048960208892822
Epoch 270, training loss: 7.719522476196289 = 1.2407886981964111 + 1.0 * 6.478733539581299
Epoch 270, val loss: 1.3786078691482544
Epoch 280, training loss: 7.673374652862549 = 1.2028136253356934 + 1.0 * 6.4705610275268555
Epoch 280, val loss: 1.353695034980774
Epoch 290, training loss: 7.6299309730529785 = 1.1657453775405884 + 1.0 * 6.46418571472168
Epoch 290, val loss: 1.3298991918563843
Epoch 300, training loss: 7.590251922607422 = 1.1293593645095825 + 1.0 * 6.460892677307129
Epoch 300, val loss: 1.3068475723266602
Epoch 310, training loss: 7.549014091491699 = 1.0939781665802002 + 1.0 * 6.455036163330078
Epoch 310, val loss: 1.2847692966461182
Epoch 320, training loss: 7.508835792541504 = 1.059768557548523 + 1.0 * 6.449067115783691
Epoch 320, val loss: 1.263770580291748
Epoch 330, training loss: 7.471025466918945 = 1.0267677307128906 + 1.0 * 6.444257736206055
Epoch 330, val loss: 1.243821144104004
Epoch 340, training loss: 7.437445163726807 = 0.9949299097061157 + 1.0 * 6.4425153732299805
Epoch 340, val loss: 1.2248578071594238
Epoch 350, training loss: 7.401558876037598 = 0.9644556641578674 + 1.0 * 6.437103271484375
Epoch 350, val loss: 1.2071419954299927
Epoch 360, training loss: 7.367301940917969 = 0.9352738857269287 + 1.0 * 6.432028293609619
Epoch 360, val loss: 1.190595030784607
Epoch 370, training loss: 7.340011119842529 = 0.9070576429367065 + 1.0 * 6.432953357696533
Epoch 370, val loss: 1.1749099493026733
Epoch 380, training loss: 7.306093215942383 = 0.8795689940452576 + 1.0 * 6.4265241622924805
Epoch 380, val loss: 1.1601648330688477
Epoch 390, training loss: 7.2737531661987305 = 0.8527019619941711 + 1.0 * 6.421051025390625
Epoch 390, val loss: 1.1461325883865356
Epoch 400, training loss: 7.247054100036621 = 0.826213002204895 + 1.0 * 6.420841217041016
Epoch 400, val loss: 1.1327656507492065
Epoch 410, training loss: 7.215080261230469 = 0.7999879717826843 + 1.0 * 6.415092468261719
Epoch 410, val loss: 1.1199859380722046
Epoch 420, training loss: 7.193016052246094 = 0.7738717198371887 + 1.0 * 6.419144153594971
Epoch 420, val loss: 1.1077077388763428
Epoch 430, training loss: 7.157102108001709 = 0.7477869987487793 + 1.0 * 6.40931510925293
Epoch 430, val loss: 1.0957458019256592
Epoch 440, training loss: 7.126012325286865 = 0.7217360138893127 + 1.0 * 6.404276371002197
Epoch 440, val loss: 1.0842474699020386
Epoch 450, training loss: 7.10402774810791 = 0.6954896450042725 + 1.0 * 6.408537864685059
Epoch 450, val loss: 1.073094367980957
Epoch 460, training loss: 7.069075584411621 = 0.669314980506897 + 1.0 * 6.399760723114014
Epoch 460, val loss: 1.062338948249817
Epoch 470, training loss: 7.040172576904297 = 0.6431682705879211 + 1.0 * 6.397004127502441
Epoch 470, val loss: 1.0521020889282227
Epoch 480, training loss: 7.014477729797363 = 0.6172459125518799 + 1.0 * 6.397231578826904
Epoch 480, val loss: 1.0426548719406128
Epoch 490, training loss: 6.989340305328369 = 0.5918088555335999 + 1.0 * 6.397531509399414
Epoch 490, val loss: 1.0341272354125977
Epoch 500, training loss: 6.956725120544434 = 0.5669034719467163 + 1.0 * 6.389821529388428
Epoch 500, val loss: 1.026615023612976
Epoch 510, training loss: 6.93040657043457 = 0.5426755547523499 + 1.0 * 6.387731075286865
Epoch 510, val loss: 1.0203378200531006
Epoch 520, training loss: 6.905909538269043 = 0.5190947651863098 + 1.0 * 6.386814594268799
Epoch 520, val loss: 1.0151361227035522
Epoch 530, training loss: 6.880632400512695 = 0.49631667137145996 + 1.0 * 6.384315490722656
Epoch 530, val loss: 1.0111013650894165
Epoch 540, training loss: 6.855615139007568 = 0.47417980432510376 + 1.0 * 6.381435394287109
Epoch 540, val loss: 1.008044958114624
Epoch 550, training loss: 6.831986904144287 = 0.4526105523109436 + 1.0 * 6.379376411437988
Epoch 550, val loss: 1.0060268640518188
Epoch 560, training loss: 6.809343338012695 = 0.43155112862586975 + 1.0 * 6.3777923583984375
Epoch 560, val loss: 1.0050108432769775
Epoch 570, training loss: 6.787383556365967 = 0.4109984040260315 + 1.0 * 6.37638521194458
Epoch 570, val loss: 1.0048414468765259
Epoch 580, training loss: 6.768316745758057 = 0.3909367322921753 + 1.0 * 6.377379894256592
Epoch 580, val loss: 1.0054516792297363
Epoch 590, training loss: 6.741664409637451 = 0.3713977038860321 + 1.0 * 6.370266914367676
Epoch 590, val loss: 1.0070382356643677
Epoch 600, training loss: 6.720875263214111 = 0.3523131012916565 + 1.0 * 6.3685622215271
Epoch 600, val loss: 1.00941801071167
Epoch 610, training loss: 6.709521770477295 = 0.3337055742740631 + 1.0 * 6.375816345214844
Epoch 610, val loss: 1.0124539136886597
Epoch 620, training loss: 6.6842756271362305 = 0.3158261477947235 + 1.0 * 6.368449687957764
Epoch 620, val loss: 1.0163874626159668
Epoch 630, training loss: 6.66432523727417 = 0.2985936403274536 + 1.0 * 6.365731716156006
Epoch 630, val loss: 1.0211806297302246
Epoch 640, training loss: 6.643054008483887 = 0.2819664478302002 + 1.0 * 6.361087799072266
Epoch 640, val loss: 1.0266995429992676
Epoch 650, training loss: 6.625980854034424 = 0.26595422625541687 + 1.0 * 6.360026836395264
Epoch 650, val loss: 1.033032774925232
Epoch 660, training loss: 6.6130781173706055 = 0.25064370036125183 + 1.0 * 6.362434387207031
Epoch 660, val loss: 1.0400538444519043
Epoch 670, training loss: 6.595086097717285 = 0.2361222356557846 + 1.0 * 6.358963966369629
Epoch 670, val loss: 1.0479179620742798
Epoch 680, training loss: 6.601474761962891 = 0.2223307192325592 + 1.0 * 6.379144191741943
Epoch 680, val loss: 1.0561808347702026
Epoch 690, training loss: 6.568978786468506 = 0.20949715375900269 + 1.0 * 6.3594818115234375
Epoch 690, val loss: 1.0650699138641357
Epoch 700, training loss: 6.5513176918029785 = 0.19739660620689392 + 1.0 * 6.353920936584473
Epoch 700, val loss: 1.07435142993927
Epoch 710, training loss: 6.536426544189453 = 0.1859913170337677 + 1.0 * 6.350435256958008
Epoch 710, val loss: 1.0842223167419434
Epoch 720, training loss: 6.524430751800537 = 0.17523735761642456 + 1.0 * 6.349193572998047
Epoch 720, val loss: 1.0945754051208496
Epoch 730, training loss: 6.520437240600586 = 0.16510796546936035 + 1.0 * 6.355329513549805
Epoch 730, val loss: 1.1050571203231812
Epoch 740, training loss: 6.514148712158203 = 0.15568631887435913 + 1.0 * 6.358462333679199
Epoch 740, val loss: 1.115805983543396
Epoch 750, training loss: 6.494758129119873 = 0.14686676859855652 + 1.0 * 6.347891330718994
Epoch 750, val loss: 1.1267290115356445
Epoch 760, training loss: 6.482583999633789 = 0.13860861957073212 + 1.0 * 6.34397554397583
Epoch 760, val loss: 1.1378556489944458
Epoch 770, training loss: 6.4737725257873535 = 0.1308622509241104 + 1.0 * 6.342910289764404
Epoch 770, val loss: 1.1490205526351929
Epoch 780, training loss: 6.466367244720459 = 0.12358757853507996 + 1.0 * 6.342779636383057
Epoch 780, val loss: 1.1603426933288574
Epoch 790, training loss: 6.459751129150391 = 0.11678232252597809 + 1.0 * 6.342968940734863
Epoch 790, val loss: 1.171600580215454
Epoch 800, training loss: 6.460498809814453 = 0.11043767631053925 + 1.0 * 6.350060939788818
Epoch 800, val loss: 1.1830044984817505
Epoch 810, training loss: 6.446382522583008 = 0.1045009046792984 + 1.0 * 6.34188175201416
Epoch 810, val loss: 1.1941285133361816
Epoch 820, training loss: 6.435616493225098 = 0.09894946217536926 + 1.0 * 6.336667060852051
Epoch 820, val loss: 1.2053415775299072
Epoch 830, training loss: 6.430534362792969 = 0.09374725818634033 + 1.0 * 6.336787223815918
Epoch 830, val loss: 1.2165038585662842
Epoch 840, training loss: 6.426337718963623 = 0.08887141197919846 + 1.0 * 6.337466239929199
Epoch 840, val loss: 1.2274112701416016
Epoch 850, training loss: 6.417516708374023 = 0.08431211858987808 + 1.0 * 6.333204746246338
Epoch 850, val loss: 1.2385034561157227
Epoch 860, training loss: 6.4131598472595215 = 0.08003970235586166 + 1.0 * 6.333120346069336
Epoch 860, val loss: 1.2494215965270996
Epoch 870, training loss: 6.4091949462890625 = 0.07603438198566437 + 1.0 * 6.333160400390625
Epoch 870, val loss: 1.2599642276763916
Epoch 880, training loss: 6.405021667480469 = 0.07229100167751312 + 1.0 * 6.332730770111084
Epoch 880, val loss: 1.2707428932189941
Epoch 890, training loss: 6.400473117828369 = 0.06878245621919632 + 1.0 * 6.331690788269043
Epoch 890, val loss: 1.2811764478683472
Epoch 900, training loss: 6.393948078155518 = 0.06548763066530228 + 1.0 * 6.328460216522217
Epoch 900, val loss: 1.291542410850525
Epoch 910, training loss: 6.389058589935303 = 0.06239810958504677 + 1.0 * 6.326660633087158
Epoch 910, val loss: 1.3019455671310425
Epoch 920, training loss: 6.384986877441406 = 0.05949840322136879 + 1.0 * 6.325488567352295
Epoch 920, val loss: 1.3120932579040527
Epoch 930, training loss: 6.383984565734863 = 0.05677122622728348 + 1.0 * 6.327213287353516
Epoch 930, val loss: 1.3223422765731812
Epoch 940, training loss: 6.380244731903076 = 0.054205138236284256 + 1.0 * 6.326039791107178
Epoch 940, val loss: 1.3320876359939575
Epoch 950, training loss: 6.382500648498535 = 0.05180371552705765 + 1.0 * 6.330697059631348
Epoch 950, val loss: 1.3419283628463745
Epoch 960, training loss: 6.3721466064453125 = 0.04956093430519104 + 1.0 * 6.322585582733154
Epoch 960, val loss: 1.3516936302185059
Epoch 970, training loss: 6.368448734283447 = 0.047443937510252 + 1.0 * 6.321004867553711
Epoch 970, val loss: 1.3610340356826782
Epoch 980, training loss: 6.365216255187988 = 0.045449454337358475 + 1.0 * 6.319766998291016
Epoch 980, val loss: 1.370398759841919
Epoch 990, training loss: 6.380334377288818 = 0.04357238858938217 + 1.0 * 6.336761951446533
Epoch 990, val loss: 1.3798043727874756
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.79388508170796
=== training gcn model ===
Epoch 0, training loss: 10.538537979125977 = 1.9416718482971191 + 1.0 * 8.596866607666016
Epoch 0, val loss: 1.9406460523605347
Epoch 10, training loss: 10.52846622467041 = 1.9317548274993896 + 1.0 * 8.596711158752441
Epoch 10, val loss: 1.9306559562683105
Epoch 20, training loss: 10.51513671875 = 1.9196385145187378 + 1.0 * 8.595498085021973
Epoch 20, val loss: 1.9184669256210327
Epoch 30, training loss: 10.488183975219727 = 1.9026991128921509 + 1.0 * 8.585484504699707
Epoch 30, val loss: 1.9016004800796509
Epoch 40, training loss: 10.412432670593262 = 1.878947377204895 + 1.0 * 8.533485412597656
Epoch 40, val loss: 1.8790299892425537
Epoch 50, training loss: 10.174442291259766 = 1.8513599634170532 + 1.0 * 8.323081970214844
Epoch 50, val loss: 1.8543899059295654
Epoch 60, training loss: 9.972521781921387 = 1.8245528936386108 + 1.0 * 8.147969245910645
Epoch 60, val loss: 1.8319863080978394
Epoch 70, training loss: 9.658954620361328 = 1.8030813932418823 + 1.0 * 7.855873107910156
Epoch 70, val loss: 1.8136321306228638
Epoch 80, training loss: 9.192904472351074 = 1.7874351739883423 + 1.0 * 7.4054694175720215
Epoch 80, val loss: 1.8007558584213257
Epoch 90, training loss: 8.924837112426758 = 1.7748044729232788 + 1.0 * 7.1500325202941895
Epoch 90, val loss: 1.7891994714736938
Epoch 100, training loss: 8.78270435333252 = 1.7587090730667114 + 1.0 * 7.0239949226379395
Epoch 100, val loss: 1.7744320631027222
Epoch 110, training loss: 8.65884780883789 = 1.741231918334961 + 1.0 * 6.9176154136657715
Epoch 110, val loss: 1.758392572402954
Epoch 120, training loss: 8.560600280761719 = 1.7234454154968262 + 1.0 * 6.837155342102051
Epoch 120, val loss: 1.742038369178772
Epoch 130, training loss: 8.477795600891113 = 1.7035481929779053 + 1.0 * 6.774247169494629
Epoch 130, val loss: 1.7244540452957153
Epoch 140, training loss: 8.409543991088867 = 1.679948329925537 + 1.0 * 6.729596138000488
Epoch 140, val loss: 1.7045402526855469
Epoch 150, training loss: 8.346040725708008 = 1.6527941226959229 + 1.0 * 6.693246841430664
Epoch 150, val loss: 1.6821938753128052
Epoch 160, training loss: 8.283477783203125 = 1.6224688291549683 + 1.0 * 6.661008834838867
Epoch 160, val loss: 1.6571440696716309
Epoch 170, training loss: 8.224078178405762 = 1.5883015394210815 + 1.0 * 6.635776519775391
Epoch 170, val loss: 1.628661036491394
Epoch 180, training loss: 8.16744327545166 = 1.5501348972320557 + 1.0 * 6.617308616638184
Epoch 180, val loss: 1.597019076347351
Epoch 190, training loss: 8.106485366821289 = 1.5091118812561035 + 1.0 * 6.597373962402344
Epoch 190, val loss: 1.563272476196289
Epoch 200, training loss: 8.046265602111816 = 1.4654593467712402 + 1.0 * 6.580806255340576
Epoch 200, val loss: 1.5274707078933716
Epoch 210, training loss: 7.986021041870117 = 1.4194749593734741 + 1.0 * 6.5665459632873535
Epoch 210, val loss: 1.4901788234710693
Epoch 220, training loss: 7.927623748779297 = 1.3721364736557007 + 1.0 * 6.555487155914307
Epoch 220, val loss: 1.4524115324020386
Epoch 230, training loss: 7.868339538574219 = 1.3244794607162476 + 1.0 * 6.543859958648682
Epoch 230, val loss: 1.4149185419082642
Epoch 240, training loss: 7.810137748718262 = 1.2765684127807617 + 1.0 * 6.5335693359375
Epoch 240, val loss: 1.3778094053268433
Epoch 250, training loss: 7.752286434173584 = 1.2285054922103882 + 1.0 * 6.523780822753906
Epoch 250, val loss: 1.341238260269165
Epoch 260, training loss: 7.6999640464782715 = 1.1809028387069702 + 1.0 * 6.519061088562012
Epoch 260, val loss: 1.3058136701583862
Epoch 270, training loss: 7.645177841186523 = 1.1349176168441772 + 1.0 * 6.510260105133057
Epoch 270, val loss: 1.2721970081329346
Epoch 280, training loss: 7.591623783111572 = 1.0900096893310547 + 1.0 * 6.501614093780518
Epoch 280, val loss: 1.2400115728378296
Epoch 290, training loss: 7.540304183959961 = 1.0460094213485718 + 1.0 * 6.4942946434021
Epoch 290, val loss: 1.2089368104934692
Epoch 300, training loss: 7.491616249084473 = 1.0032322406768799 + 1.0 * 6.488383769989014
Epoch 300, val loss: 1.1792553663253784
Epoch 310, training loss: 7.4434027671813965 = 0.9618809819221497 + 1.0 * 6.4815216064453125
Epoch 310, val loss: 1.1510417461395264
Epoch 320, training loss: 7.398202419281006 = 0.9216228723526001 + 1.0 * 6.476579666137695
Epoch 320, val loss: 1.1239173412322998
Epoch 330, training loss: 7.35772180557251 = 0.88249671459198 + 1.0 * 6.47522497177124
Epoch 330, val loss: 1.097790241241455
Epoch 340, training loss: 7.313139915466309 = 0.8449986577033997 + 1.0 * 6.468141078948975
Epoch 340, val loss: 1.073050618171692
Epoch 350, training loss: 7.270366191864014 = 0.8088160157203674 + 1.0 * 6.461550235748291
Epoch 350, val loss: 1.0497210025787354
Epoch 360, training loss: 7.230013370513916 = 0.7738696932792664 + 1.0 * 6.456143856048584
Epoch 360, val loss: 1.0276504755020142
Epoch 370, training loss: 7.1982197761535645 = 0.7403832674026489 + 1.0 * 6.457836627960205
Epoch 370, val loss: 1.0070514678955078
Epoch 380, training loss: 7.159470558166504 = 0.7087711095809937 + 1.0 * 6.450699329376221
Epoch 380, val loss: 0.988347589969635
Epoch 390, training loss: 7.124061584472656 = 0.6787911653518677 + 1.0 * 6.445270538330078
Epoch 390, val loss: 0.9715008735656738
Epoch 400, training loss: 7.090210914611816 = 0.6501376628875732 + 1.0 * 6.440073490142822
Epoch 400, val loss: 0.9563243985176086
Epoch 410, training loss: 7.06186056137085 = 0.6226916313171387 + 1.0 * 6.439168930053711
Epoch 410, val loss: 0.9428636431694031
Epoch 420, training loss: 7.033871650695801 = 0.5967296361923218 + 1.0 * 6.4371418952941895
Epoch 420, val loss: 0.9310292601585388
Epoch 430, training loss: 7.005820274353027 = 0.5721727609634399 + 1.0 * 6.433647632598877
Epoch 430, val loss: 0.9210122227668762
Epoch 440, training loss: 6.976059436798096 = 0.5487411618232727 + 1.0 * 6.427318096160889
Epoch 440, val loss: 0.9124174118041992
Epoch 450, training loss: 6.954503059387207 = 0.5261916518211365 + 1.0 * 6.428311347961426
Epoch 450, val loss: 0.9051403403282166
Epoch 460, training loss: 6.932064533233643 = 0.504536509513855 + 1.0 * 6.427527904510498
Epoch 460, val loss: 0.8990468978881836
Epoch 470, training loss: 6.902223587036133 = 0.48389291763305664 + 1.0 * 6.418330669403076
Epoch 470, val loss: 0.8940765857696533
Epoch 480, training loss: 6.8799872398376465 = 0.4639948904514313 + 1.0 * 6.415992259979248
Epoch 480, val loss: 0.8901177048683167
Epoch 490, training loss: 6.86393404006958 = 0.4447593688964844 + 1.0 * 6.419174671173096
Epoch 490, val loss: 0.8871911764144897
Epoch 500, training loss: 6.841381072998047 = 0.42627596855163574 + 1.0 * 6.41510534286499
Epoch 500, val loss: 0.8850386142730713
Epoch 510, training loss: 6.816579341888428 = 0.40851646661758423 + 1.0 * 6.408062934875488
Epoch 510, val loss: 0.8838359713554382
Epoch 520, training loss: 6.80709171295166 = 0.39134469628334045 + 1.0 * 6.415747165679932
Epoch 520, val loss: 0.8832839131355286
Epoch 530, training loss: 6.778046131134033 = 0.3749139606952667 + 1.0 * 6.40313196182251
Epoch 530, val loss: 0.8834359049797058
Epoch 540, training loss: 6.7593913078308105 = 0.3590907156467438 + 1.0 * 6.4003005027771
Epoch 540, val loss: 0.8843541145324707
Epoch 550, training loss: 6.743402481079102 = 0.3437940776348114 + 1.0 * 6.399608612060547
Epoch 550, val loss: 0.8858688473701477
Epoch 560, training loss: 6.732579231262207 = 0.3290309011936188 + 1.0 * 6.403548240661621
Epoch 560, val loss: 0.8878465294837952
Epoch 570, training loss: 6.71205472946167 = 0.3149379789829254 + 1.0 * 6.397116661071777
Epoch 570, val loss: 0.8904964327812195
Epoch 580, training loss: 6.694858551025391 = 0.3013203740119934 + 1.0 * 6.393537998199463
Epoch 580, val loss: 0.8936440348625183
Epoch 590, training loss: 6.686883926391602 = 0.28816479444503784 + 1.0 * 6.398719310760498
Epoch 590, val loss: 0.8972779512405396
Epoch 600, training loss: 6.666595935821533 = 0.2755236029624939 + 1.0 * 6.3910722732543945
Epoch 600, val loss: 0.9014652371406555
Epoch 610, training loss: 6.655252456665039 = 0.26328158378601074 + 1.0 * 6.391970634460449
Epoch 610, val loss: 0.9060584306716919
Epoch 620, training loss: 6.636959075927734 = 0.2514702379703522 + 1.0 * 6.385488986968994
Epoch 620, val loss: 0.9110285639762878
Epoch 630, training loss: 6.622993469238281 = 0.24006807804107666 + 1.0 * 6.382925510406494
Epoch 630, val loss: 0.9165741801261902
Epoch 640, training loss: 6.616637706756592 = 0.22904756665229797 + 1.0 * 6.387589931488037
Epoch 640, val loss: 0.9223748445510864
Epoch 650, training loss: 6.602063179016113 = 0.21844084560871124 + 1.0 * 6.383622169494629
Epoch 650, val loss: 0.9286057353019714
Epoch 660, training loss: 6.584183692932129 = 0.20820081233978271 + 1.0 * 6.375982761383057
Epoch 660, val loss: 0.9351170063018799
Epoch 670, training loss: 6.577115058898926 = 0.19830180704593658 + 1.0 * 6.37881326675415
Epoch 670, val loss: 0.9418841004371643
Epoch 680, training loss: 6.56504487991333 = 0.18879809975624084 + 1.0 * 6.376246929168701
Epoch 680, val loss: 0.9488672614097595
Epoch 690, training loss: 6.554059982299805 = 0.17968474328517914 + 1.0 * 6.374375343322754
Epoch 690, val loss: 0.9561555981636047
Epoch 700, training loss: 6.541672229766846 = 0.1709466129541397 + 1.0 * 6.370725631713867
Epoch 700, val loss: 0.9637241959571838
Epoch 710, training loss: 6.544730186462402 = 0.16256113350391388 + 1.0 * 6.382169246673584
Epoch 710, val loss: 0.9715461730957031
Epoch 720, training loss: 6.526602745056152 = 0.15457598865032196 + 1.0 * 6.3720269203186035
Epoch 720, val loss: 0.97938472032547
Epoch 730, training loss: 6.513908386230469 = 0.14698243141174316 + 1.0 * 6.3669257164001465
Epoch 730, val loss: 0.9875248670578003
Epoch 740, training loss: 6.5042500495910645 = 0.13974006474018097 + 1.0 * 6.3645100593566895
Epoch 740, val loss: 0.9958521127700806
Epoch 750, training loss: 6.502998352050781 = 0.13284412026405334 + 1.0 * 6.37015438079834
Epoch 750, val loss: 1.0043245553970337
Epoch 760, training loss: 6.490377902984619 = 0.12632277607917786 + 1.0 * 6.364055156707764
Epoch 760, val loss: 1.013070821762085
Epoch 770, training loss: 6.486201763153076 = 0.12011635303497314 + 1.0 * 6.366085529327393
Epoch 770, val loss: 1.0216574668884277
Epoch 780, training loss: 6.475013732910156 = 0.11426109075546265 + 1.0 * 6.360752582550049
Epoch 780, val loss: 1.0304315090179443
Epoch 790, training loss: 6.46933650970459 = 0.1087312251329422 + 1.0 * 6.360605239868164
Epoch 790, val loss: 1.0393753051757812
Epoch 800, training loss: 6.459170818328857 = 0.10349876433610916 + 1.0 * 6.3556718826293945
Epoch 800, val loss: 1.0481913089752197
Epoch 810, training loss: 6.459159851074219 = 0.09856195002794266 + 1.0 * 6.360598087310791
Epoch 810, val loss: 1.0572341680526733
Epoch 820, training loss: 6.449100494384766 = 0.09392458200454712 + 1.0 * 6.355175971984863
Epoch 820, val loss: 1.0660903453826904
Epoch 830, training loss: 6.441841125488281 = 0.0895468145608902 + 1.0 * 6.352294445037842
Epoch 830, val loss: 1.0751755237579346
Epoch 840, training loss: 6.450598239898682 = 0.08541150391101837 + 1.0 * 6.36518669128418
Epoch 840, val loss: 1.0840301513671875
Epoch 850, training loss: 6.433279991149902 = 0.08154106140136719 + 1.0 * 6.351738929748535
Epoch 850, val loss: 1.093115210533142
Epoch 860, training loss: 6.426916122436523 = 0.07788790017366409 + 1.0 * 6.34902811050415
Epoch 860, val loss: 1.1021145582199097
Epoch 870, training loss: 6.421942710876465 = 0.07443221658468246 + 1.0 * 6.34751033782959
Epoch 870, val loss: 1.1110162734985352
Epoch 880, training loss: 6.428483009338379 = 0.07116512954235077 + 1.0 * 6.357317924499512
Epoch 880, val loss: 1.1198570728302002
Epoch 890, training loss: 6.414438724517822 = 0.06810818612575531 + 1.0 * 6.346330642700195
Epoch 890, val loss: 1.1286243200302124
Epoch 900, training loss: 6.410194396972656 = 0.0652238205075264 + 1.0 * 6.344970703125
Epoch 900, val loss: 1.1373615264892578
Epoch 910, training loss: 6.406732559204102 = 0.06249701604247093 + 1.0 * 6.344235420227051
Epoch 910, val loss: 1.1460579633712769
Epoch 920, training loss: 6.406379222869873 = 0.05991443246603012 + 1.0 * 6.34646463394165
Epoch 920, val loss: 1.1546064615249634
Epoch 930, training loss: 6.4028425216674805 = 0.05747212469577789 + 1.0 * 6.345370292663574
Epoch 930, val loss: 1.1631232500076294
Epoch 940, training loss: 6.399258613586426 = 0.055169738829135895 + 1.0 * 6.344089031219482
Epoch 940, val loss: 1.1714365482330322
Epoch 950, training loss: 6.396885395050049 = 0.052985839545726776 + 1.0 * 6.343899726867676
Epoch 950, val loss: 1.1795796155929565
Epoch 960, training loss: 6.388890743255615 = 0.05092668533325195 + 1.0 * 6.337964057922363
Epoch 960, val loss: 1.18782377243042
Epoch 970, training loss: 6.400337219238281 = 0.04897289723157883 + 1.0 * 6.3513641357421875
Epoch 970, val loss: 1.1957896947860718
Epoch 980, training loss: 6.385980606079102 = 0.04711885750293732 + 1.0 * 6.33886194229126
Epoch 980, val loss: 1.2036365270614624
Epoch 990, training loss: 6.380897045135498 = 0.045368462800979614 + 1.0 * 6.335528373718262
Epoch 990, val loss: 1.2114969491958618
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8017923036373221
The final CL Acc:0.74444, 0.01318, The final GNN Acc:0.79740, 0.00329
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13234])
remove edge: torch.Size([2, 7944])
updated graph: torch.Size([2, 10622])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.565847396850586 = 1.969009280204773 + 1.0 * 8.596837997436523
Epoch 0, val loss: 1.9652348756790161
Epoch 10, training loss: 10.555534362792969 = 1.9588569402694702 + 1.0 * 8.596677780151367
Epoch 10, val loss: 1.9557154178619385
Epoch 20, training loss: 10.542409896850586 = 1.9468713998794556 + 1.0 * 8.595538139343262
Epoch 20, val loss: 1.943873643875122
Epoch 30, training loss: 10.516704559326172 = 1.9305812120437622 + 1.0 * 8.5861234664917
Epoch 30, val loss: 1.927201509475708
Epoch 40, training loss: 10.433910369873047 = 1.9075005054473877 + 1.0 * 8.526410102844238
Epoch 40, val loss: 1.9038277864456177
Epoch 50, training loss: 10.079086303710938 = 1.8807778358459473 + 1.0 * 8.198307991027832
Epoch 50, val loss: 1.8778349161148071
Epoch 60, training loss: 9.622591018676758 = 1.858194351196289 + 1.0 * 7.764397144317627
Epoch 60, val loss: 1.8570897579193115
Epoch 70, training loss: 9.057652473449707 = 1.841099500656128 + 1.0 * 7.216552734375
Epoch 70, val loss: 1.8416774272918701
Epoch 80, training loss: 8.818403244018555 = 1.824304461479187 + 1.0 * 6.994098663330078
Epoch 80, val loss: 1.8257009983062744
Epoch 90, training loss: 8.691218376159668 = 1.8024828433990479 + 1.0 * 6.888735294342041
Epoch 90, val loss: 1.8051462173461914
Epoch 100, training loss: 8.590649604797363 = 1.7800627946853638 + 1.0 * 6.810586452484131
Epoch 100, val loss: 1.7846765518188477
Epoch 110, training loss: 8.513493537902832 = 1.7593969106674194 + 1.0 * 6.754096508026123
Epoch 110, val loss: 1.7658191919326782
Epoch 120, training loss: 8.450357437133789 = 1.7388744354248047 + 1.0 * 6.711482524871826
Epoch 120, val loss: 1.746740698814392
Epoch 130, training loss: 8.392118453979492 = 1.7170056104660034 + 1.0 * 6.675112724304199
Epoch 130, val loss: 1.72654128074646
Epoch 140, training loss: 8.340238571166992 = 1.6925114393234253 + 1.0 * 6.647727012634277
Epoch 140, val loss: 1.704140067100525
Epoch 150, training loss: 8.288687705993652 = 1.664562463760376 + 1.0 * 6.624125003814697
Epoch 150, val loss: 1.6790225505828857
Epoch 160, training loss: 8.236567497253418 = 1.632433533668518 + 1.0 * 6.604133605957031
Epoch 160, val loss: 1.6503236293792725
Epoch 170, training loss: 8.182003021240234 = 1.5954638719558716 + 1.0 * 6.586538791656494
Epoch 170, val loss: 1.6173577308654785
Epoch 180, training loss: 8.125813484191895 = 1.5534279346466064 + 1.0 * 6.572385311126709
Epoch 180, val loss: 1.5800329446792603
Epoch 190, training loss: 8.064846992492676 = 1.5069776773452759 + 1.0 * 6.5578694343566895
Epoch 190, val loss: 1.5392078161239624
Epoch 200, training loss: 8.002645492553711 = 1.456592082977295 + 1.0 * 6.546053409576416
Epoch 200, val loss: 1.4955084323883057
Epoch 210, training loss: 7.939148426055908 = 1.403225064277649 + 1.0 * 6.535923480987549
Epoch 210, val loss: 1.449874758720398
Epoch 220, training loss: 7.874659538269043 = 1.3487918376922607 + 1.0 * 6.525867462158203
Epoch 220, val loss: 1.4042474031448364
Epoch 230, training loss: 7.810901165008545 = 1.2937947511672974 + 1.0 * 6.517106533050537
Epoch 230, val loss: 1.3593153953552246
Epoch 240, training loss: 7.7498555183410645 = 1.239611029624939 + 1.0 * 6.510244369506836
Epoch 240, val loss: 1.3158820867538452
Epoch 250, training loss: 7.686043739318848 = 1.186623454093933 + 1.0 * 6.499420166015625
Epoch 250, val loss: 1.274276614189148
Epoch 260, training loss: 7.625672340393066 = 1.1349316835403442 + 1.0 * 6.490740776062012
Epoch 260, val loss: 1.234144687652588
Epoch 270, training loss: 7.569201469421387 = 1.0851281881332397 + 1.0 * 6.484073162078857
Epoch 270, val loss: 1.1960062980651855
Epoch 280, training loss: 7.515727996826172 = 1.0380043983459473 + 1.0 * 6.477723598480225
Epoch 280, val loss: 1.160213828086853
Epoch 290, training loss: 7.464272499084473 = 0.9934542179107666 + 1.0 * 6.470818519592285
Epoch 290, val loss: 1.126275897026062
Epoch 300, training loss: 7.415161609649658 = 0.9513993263244629 + 1.0 * 6.463762283325195
Epoch 300, val loss: 1.0945149660110474
Epoch 310, training loss: 7.368710994720459 = 0.9113303422927856 + 1.0 * 6.457380771636963
Epoch 310, val loss: 1.0642540454864502
Epoch 320, training loss: 7.328761577606201 = 0.8728871941566467 + 1.0 * 6.455874443054199
Epoch 320, val loss: 1.035119652748108
Epoch 330, training loss: 7.283370494842529 = 0.8362215757369995 + 1.0 * 6.44714879989624
Epoch 330, val loss: 1.0075315237045288
Epoch 340, training loss: 7.244446754455566 = 0.8011422157287598 + 1.0 * 6.443304538726807
Epoch 340, val loss: 0.9813929796218872
Epoch 350, training loss: 7.213142395019531 = 0.7671681642532349 + 1.0 * 6.445974349975586
Epoch 350, val loss: 0.9563791751861572
Epoch 360, training loss: 7.169572830200195 = 0.734486997127533 + 1.0 * 6.435085773468018
Epoch 360, val loss: 0.9329793453216553
Epoch 370, training loss: 7.133748531341553 = 0.7028957009315491 + 1.0 * 6.430852890014648
Epoch 370, val loss: 0.911069929599762
Epoch 380, training loss: 7.097651958465576 = 0.6723266243934631 + 1.0 * 6.425325393676758
Epoch 380, val loss: 0.8905550241470337
Epoch 390, training loss: 7.07095193862915 = 0.6426507234573364 + 1.0 * 6.4283013343811035
Epoch 390, val loss: 0.871565580368042
Epoch 400, training loss: 7.036534786224365 = 0.6144781112670898 + 1.0 * 6.422056674957275
Epoch 400, val loss: 0.8540160059928894
Epoch 410, training loss: 7.003355979919434 = 0.587582528591156 + 1.0 * 6.415773391723633
Epoch 410, val loss: 0.838299572467804
Epoch 420, training loss: 6.974475860595703 = 0.5617495179176331 + 1.0 * 6.412726402282715
Epoch 420, val loss: 0.8238801956176758
Epoch 430, training loss: 6.9453535079956055 = 0.5368186831474304 + 1.0 * 6.408535003662109
Epoch 430, val loss: 0.8106870055198669
Epoch 440, training loss: 6.923677444458008 = 0.5129526853561401 + 1.0 * 6.410724639892578
Epoch 440, val loss: 0.79866623878479
Epoch 450, training loss: 6.89554500579834 = 0.490268737077713 + 1.0 * 6.405276298522949
Epoch 450, val loss: 0.7879301905632019
Epoch 460, training loss: 6.8679938316345215 = 0.4684702455997467 + 1.0 * 6.399523735046387
Epoch 460, val loss: 0.7781830430030823
Epoch 470, training loss: 6.850640296936035 = 0.4473833739757538 + 1.0 * 6.403256893157959
Epoch 470, val loss: 0.7693228721618652
Epoch 480, training loss: 6.834160327911377 = 0.4273194968700409 + 1.0 * 6.406840801239014
Epoch 480, val loss: 0.7612431049346924
Epoch 490, training loss: 6.806818008422852 = 0.4081081449985504 + 1.0 * 6.398709774017334
Epoch 490, val loss: 0.7541011571884155
Epoch 500, training loss: 6.780106067657471 = 0.38967907428741455 + 1.0 * 6.390427112579346
Epoch 500, val loss: 0.747730553150177
Epoch 510, training loss: 6.758940696716309 = 0.3717675507068634 + 1.0 * 6.387173175811768
Epoch 510, val loss: 0.7419907450675964
Epoch 520, training loss: 6.740304470062256 = 0.3543549180030823 + 1.0 * 6.385949611663818
Epoch 520, val loss: 0.7368535399436951
Epoch 530, training loss: 6.720577239990234 = 0.3374536633491516 + 1.0 * 6.383123397827148
Epoch 530, val loss: 0.73222416639328
Epoch 540, training loss: 6.704906940460205 = 0.3211008906364441 + 1.0 * 6.383806228637695
Epoch 540, val loss: 0.7281733155250549
Epoch 550, training loss: 6.683968544006348 = 0.3051561415195465 + 1.0 * 6.378812313079834
Epoch 550, val loss: 0.7246044278144836
Epoch 560, training loss: 6.674858093261719 = 0.28967270255088806 + 1.0 * 6.385185241699219
Epoch 560, val loss: 0.7214604616165161
Epoch 570, training loss: 6.653983116149902 = 0.27470555901527405 + 1.0 * 6.37927770614624
Epoch 570, val loss: 0.7188000679016113
Epoch 580, training loss: 6.635272026062012 = 0.26027214527130127 + 1.0 * 6.375
Epoch 580, val loss: 0.7166142463684082
Epoch 590, training loss: 6.617194175720215 = 0.24633406102657318 + 1.0 * 6.3708600997924805
Epoch 590, val loss: 0.7149203419685364
Epoch 600, training loss: 6.622853755950928 = 0.23287947475910187 + 1.0 * 6.389974117279053
Epoch 600, val loss: 0.7136989235877991
Epoch 610, training loss: 6.589969635009766 = 0.22018049657344818 + 1.0 * 6.369789123535156
Epoch 610, val loss: 0.7128598093986511
Epoch 620, training loss: 6.574929714202881 = 0.2080518901348114 + 1.0 * 6.366878032684326
Epoch 620, val loss: 0.7125003337860107
Epoch 630, training loss: 6.561074256896973 = 0.1964896023273468 + 1.0 * 6.364584445953369
Epoch 630, val loss: 0.7126889824867249
Epoch 640, training loss: 6.567265033721924 = 0.18548521399497986 + 1.0 * 6.381779670715332
Epoch 640, val loss: 0.7133002877235413
Epoch 650, training loss: 6.539239883422852 = 0.17516762018203735 + 1.0 * 6.364072322845459
Epoch 650, val loss: 0.7142449617385864
Epoch 660, training loss: 6.526853561401367 = 0.1654292345046997 + 1.0 * 6.361424446105957
Epoch 660, val loss: 0.7156445980072021
Epoch 670, training loss: 6.5155181884765625 = 0.15622694790363312 + 1.0 * 6.359291076660156
Epoch 670, val loss: 0.7174967527389526
Epoch 680, training loss: 6.517491817474365 = 0.14757826924324036 + 1.0 * 6.369913578033447
Epoch 680, val loss: 0.7196539640426636
Epoch 690, training loss: 6.497869491577148 = 0.1394621580839157 + 1.0 * 6.358407497406006
Epoch 690, val loss: 0.7220972776412964
Epoch 700, training loss: 6.487481594085693 = 0.13183821737766266 + 1.0 * 6.355643272399902
Epoch 700, val loss: 0.7248587012290955
Epoch 710, training loss: 6.4782257080078125 = 0.12468414753675461 + 1.0 * 6.353541374206543
Epoch 710, val loss: 0.7279568910598755
Epoch 720, training loss: 6.480498313903809 = 0.11795704811811447 + 1.0 * 6.362541198730469
Epoch 720, val loss: 0.7313328385353088
Epoch 730, training loss: 6.4645843505859375 = 0.11164978891611099 + 1.0 * 6.35293436050415
Epoch 730, val loss: 0.7349706292152405
Epoch 740, training loss: 6.456174850463867 = 0.10574541240930557 + 1.0 * 6.350429534912109
Epoch 740, val loss: 0.7387843728065491
Epoch 750, training loss: 6.455592632293701 = 0.10020072013139725 + 1.0 * 6.355391979217529
Epoch 750, val loss: 0.7428408265113831
Epoch 760, training loss: 6.444614410400391 = 0.0950286015868187 + 1.0 * 6.349586009979248
Epoch 760, val loss: 0.7469760179519653
Epoch 770, training loss: 6.43687105178833 = 0.0901683047413826 + 1.0 * 6.346702575683594
Epoch 770, val loss: 0.7513532638549805
Epoch 780, training loss: 6.431528091430664 = 0.08561158925294876 + 1.0 * 6.345916271209717
Epoch 780, val loss: 0.7558619379997253
Epoch 790, training loss: 6.428826808929443 = 0.08134382218122482 + 1.0 * 6.347483158111572
Epoch 790, val loss: 0.7604812383651733
Epoch 800, training loss: 6.42086935043335 = 0.07734298706054688 + 1.0 * 6.343526363372803
Epoch 800, val loss: 0.7651275992393494
Epoch 810, training loss: 6.416314125061035 = 0.07357829064130783 + 1.0 * 6.342735767364502
Epoch 810, val loss: 0.7699295878410339
Epoch 820, training loss: 6.415953636169434 = 0.07005609571933746 + 1.0 * 6.345897674560547
Epoch 820, val loss: 0.7748181223869324
Epoch 830, training loss: 6.409097194671631 = 0.06674239784479141 + 1.0 * 6.342354774475098
Epoch 830, val loss: 0.7797900438308716
Epoch 840, training loss: 6.404610633850098 = 0.06363961100578308 + 1.0 * 6.340970993041992
Epoch 840, val loss: 0.784784197807312
Epoch 850, training loss: 6.399296760559082 = 0.060716208070516586 + 1.0 * 6.33858060836792
Epoch 850, val loss: 0.7898756265640259
Epoch 860, training loss: 6.396320343017578 = 0.05796945095062256 + 1.0 * 6.338350772857666
Epoch 860, val loss: 0.7949934005737305
Epoch 870, training loss: 6.391328811645508 = 0.05537539720535278 + 1.0 * 6.335953235626221
Epoch 870, val loss: 0.800141453742981
Epoch 880, training loss: 6.394324779510498 = 0.052937012165784836 + 1.0 * 6.341387748718262
Epoch 880, val loss: 0.8052941560745239
Epoch 890, training loss: 6.3851752281188965 = 0.05064789205789566 + 1.0 * 6.334527492523193
Epoch 890, val loss: 0.8103775382041931
Epoch 900, training loss: 6.384496212005615 = 0.048492368310689926 + 1.0 * 6.33600378036499
Epoch 900, val loss: 0.8154842257499695
Epoch 910, training loss: 6.378757953643799 = 0.04645217955112457 + 1.0 * 6.332305908203125
Epoch 910, val loss: 0.8206068277359009
Epoch 920, training loss: 6.377982139587402 = 0.04452875256538391 + 1.0 * 6.333453178405762
Epoch 920, val loss: 0.8257377743721008
Epoch 930, training loss: 6.381221771240234 = 0.04270927608013153 + 1.0 * 6.338512420654297
Epoch 930, val loss: 0.8308332562446594
Epoch 940, training loss: 6.373271942138672 = 0.040999386459589005 + 1.0 * 6.332272529602051
Epoch 940, val loss: 0.835900604724884
Epoch 950, training loss: 6.368777275085449 = 0.03938226401805878 + 1.0 * 6.329394817352295
Epoch 950, val loss: 0.8409437537193298
Epoch 960, training loss: 6.3703508377075195 = 0.03784964233636856 + 1.0 * 6.332501411437988
Epoch 960, val loss: 0.8459799885749817
Epoch 970, training loss: 6.366738319396973 = 0.03640555962920189 + 1.0 * 6.3303327560424805
Epoch 970, val loss: 0.8509111404418945
Epoch 980, training loss: 6.360801696777344 = 0.035037796944379807 + 1.0 * 6.325763702392578
Epoch 980, val loss: 0.8558409214019775
Epoch 990, training loss: 6.358138561248779 = 0.033743876963853836 + 1.0 * 6.324394702911377
Epoch 990, val loss: 0.8607448935508728
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 10.535603523254395 = 1.938751220703125 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.9335304498672485
Epoch 10, training loss: 10.526232719421387 = 1.92953360080719 + 1.0 * 8.596698760986328
Epoch 10, val loss: 1.9248480796813965
Epoch 20, training loss: 10.513580322265625 = 1.91808021068573 + 1.0 * 8.595499992370605
Epoch 20, val loss: 1.9136244058609009
Epoch 30, training loss: 10.487083435058594 = 1.9018417596817017 + 1.0 * 8.585241317749023
Epoch 30, val loss: 1.8976103067398071
Epoch 40, training loss: 10.405647277832031 = 1.878916621208191 + 1.0 * 8.52673053741455
Epoch 40, val loss: 1.875730276107788
Epoch 50, training loss: 10.072184562683105 = 1.8538386821746826 + 1.0 * 8.218345642089844
Epoch 50, val loss: 1.8531123399734497
Epoch 60, training loss: 9.611016273498535 = 1.8294066190719604 + 1.0 * 7.781609535217285
Epoch 60, val loss: 1.8322792053222656
Epoch 70, training loss: 9.14212417602539 = 1.8098167181015015 + 1.0 * 7.3323073387146
Epoch 70, val loss: 1.8146612644195557
Epoch 80, training loss: 8.95185375213623 = 1.7908049821853638 + 1.0 * 7.161048412322998
Epoch 80, val loss: 1.7977098226547241
Epoch 90, training loss: 8.804835319519043 = 1.7681137323379517 + 1.0 * 7.036721229553223
Epoch 90, val loss: 1.7785203456878662
Epoch 100, training loss: 8.682366371154785 = 1.7452558279037476 + 1.0 * 6.937110424041748
Epoch 100, val loss: 1.7597864866256714
Epoch 110, training loss: 8.584563255310059 = 1.7224304676055908 + 1.0 * 6.862133026123047
Epoch 110, val loss: 1.741045355796814
Epoch 120, training loss: 8.496856689453125 = 1.6974083185195923 + 1.0 * 6.799448490142822
Epoch 120, val loss: 1.7203257083892822
Epoch 130, training loss: 8.42198657989502 = 1.6692970991134644 + 1.0 * 6.752689361572266
Epoch 130, val loss: 1.6967610120773315
Epoch 140, training loss: 8.352005004882812 = 1.6367307901382446 + 1.0 * 6.715274333953857
Epoch 140, val loss: 1.669287919998169
Epoch 150, training loss: 8.284048080444336 = 1.598641037940979 + 1.0 * 6.685406684875488
Epoch 150, val loss: 1.6375007629394531
Epoch 160, training loss: 8.213425636291504 = 1.5551377534866333 + 1.0 * 6.658287525177002
Epoch 160, val loss: 1.601523756980896
Epoch 170, training loss: 8.139707565307617 = 1.507539987564087 + 1.0 * 6.632167339324951
Epoch 170, val loss: 1.5625495910644531
Epoch 180, training loss: 8.070630073547363 = 1.4565714597702026 + 1.0 * 6.614058494567871
Epoch 180, val loss: 1.5210708379745483
Epoch 190, training loss: 7.992094993591309 = 1.4039580821990967 + 1.0 * 6.588136672973633
Epoch 190, val loss: 1.4789516925811768
Epoch 200, training loss: 7.919241905212402 = 1.350533366203308 + 1.0 * 6.568708419799805
Epoch 200, val loss: 1.436457872390747
Epoch 210, training loss: 7.849585056304932 = 1.2970064878463745 + 1.0 * 6.552578449249268
Epoch 210, val loss: 1.3942253589630127
Epoch 220, training loss: 7.784460067749023 = 1.2443689107894897 + 1.0 * 6.540091037750244
Epoch 220, val loss: 1.353121042251587
Epoch 230, training loss: 7.721787452697754 = 1.1938341856002808 + 1.0 * 6.527953147888184
Epoch 230, val loss: 1.314340591430664
Epoch 240, training loss: 7.661477565765381 = 1.1452827453613281 + 1.0 * 6.516194820404053
Epoch 240, val loss: 1.2772870063781738
Epoch 250, training loss: 7.609833717346191 = 1.0988233089447021 + 1.0 * 6.51101016998291
Epoch 250, val loss: 1.2424439191818237
Epoch 260, training loss: 7.5540771484375 = 1.0551645755767822 + 1.0 * 6.498912334442139
Epoch 260, val loss: 1.210229754447937
Epoch 270, training loss: 7.504476547241211 = 1.0138349533081055 + 1.0 * 6.4906415939331055
Epoch 270, val loss: 1.1798882484436035
Epoch 280, training loss: 7.456502914428711 = 0.9741763472557068 + 1.0 * 6.482326507568359
Epoch 280, val loss: 1.1510239839553833
Epoch 290, training loss: 7.411337852478027 = 0.9358851909637451 + 1.0 * 6.475452899932861
Epoch 290, val loss: 1.123130202293396
Epoch 300, training loss: 7.370257377624512 = 0.8989933729171753 + 1.0 * 6.471263885498047
Epoch 300, val loss: 1.0961501598358154
Epoch 310, training loss: 7.325780868530273 = 0.8626827001571655 + 1.0 * 6.463098049163818
Epoch 310, val loss: 1.0693418979644775
Epoch 320, training loss: 7.28387975692749 = 0.8266517519950867 + 1.0 * 6.457228183746338
Epoch 320, val loss: 1.0425766706466675
Epoch 330, training loss: 7.249441623687744 = 0.7910284399986267 + 1.0 * 6.458413124084473
Epoch 330, val loss: 1.016076922416687
Epoch 340, training loss: 7.20457649230957 = 0.7568490505218506 + 1.0 * 6.447727203369141
Epoch 340, val loss: 0.990555465221405
Epoch 350, training loss: 7.166913032531738 = 0.7235310673713684 + 1.0 * 6.4433817863464355
Epoch 350, val loss: 0.9658939838409424
Epoch 360, training loss: 7.129366874694824 = 0.6909730434417725 + 1.0 * 6.438394069671631
Epoch 360, val loss: 0.9422317743301392
Epoch 370, training loss: 7.092713832855225 = 0.6592223048210144 + 1.0 * 6.4334917068481445
Epoch 370, val loss: 0.9197208881378174
Epoch 380, training loss: 7.058714389801025 = 0.6284599900245667 + 1.0 * 6.4302544593811035
Epoch 380, val loss: 0.8986722826957703
Epoch 390, training loss: 7.0259318351745605 = 0.5989331603050232 + 1.0 * 6.426998615264893
Epoch 390, val loss: 0.8793756365776062
Epoch 400, training loss: 6.994818210601807 = 0.5705299973487854 + 1.0 * 6.424288272857666
Epoch 400, val loss: 0.8617639541625977
Epoch 410, training loss: 6.965458393096924 = 0.5434520840644836 + 1.0 * 6.422006130218506
Epoch 410, val loss: 0.8458060622215271
Epoch 420, training loss: 6.93247127532959 = 0.5174815058708191 + 1.0 * 6.414989948272705
Epoch 420, val loss: 0.8317887187004089
Epoch 430, training loss: 6.905045509338379 = 0.49259161949157715 + 1.0 * 6.412454128265381
Epoch 430, val loss: 0.8194552659988403
Epoch 440, training loss: 6.8800740242004395 = 0.468791663646698 + 1.0 * 6.411282539367676
Epoch 440, val loss: 0.8087152242660522
Epoch 450, training loss: 6.852273941040039 = 0.4461783766746521 + 1.0 * 6.406095504760742
Epoch 450, val loss: 0.799551248550415
Epoch 460, training loss: 6.8274054527282715 = 0.4244830906391144 + 1.0 * 6.4029221534729
Epoch 460, val loss: 0.7918127179145813
Epoch 470, training loss: 6.808445453643799 = 0.40359655022621155 + 1.0 * 6.404849052429199
Epoch 470, val loss: 0.7853424549102783
Epoch 480, training loss: 6.781449794769287 = 0.38358160853385925 + 1.0 * 6.3978681564331055
Epoch 480, val loss: 0.7799522280693054
Epoch 490, training loss: 6.762113094329834 = 0.3642663061618805 + 1.0 * 6.397846698760986
Epoch 490, val loss: 0.775605320930481
Epoch 500, training loss: 6.738529205322266 = 0.34561172127723694 + 1.0 * 6.392917633056641
Epoch 500, val loss: 0.7722110152244568
Epoch 510, training loss: 6.719642639160156 = 0.32758957147598267 + 1.0 * 6.392053127288818
Epoch 510, val loss: 0.7697420120239258
Epoch 520, training loss: 6.700133800506592 = 0.31023168563842773 + 1.0 * 6.389902114868164
Epoch 520, val loss: 0.7681050896644592
Epoch 530, training loss: 6.684563159942627 = 0.29353296756744385 + 1.0 * 6.391030311584473
Epoch 530, val loss: 0.7671926021575928
Epoch 540, training loss: 6.662811756134033 = 0.2775801718235016 + 1.0 * 6.3852314949035645
Epoch 540, val loss: 0.7671456336975098
Epoch 550, training loss: 6.652333736419678 = 0.2623630464076996 + 1.0 * 6.389970779418945
Epoch 550, val loss: 0.7678083777427673
Epoch 560, training loss: 6.630547523498535 = 0.2478487640619278 + 1.0 * 6.3826985359191895
Epoch 560, val loss: 0.7692292928695679
Epoch 570, training loss: 6.612525939941406 = 0.2341700792312622 + 1.0 * 6.378355979919434
Epoch 570, val loss: 0.7713102102279663
Epoch 580, training loss: 6.597115516662598 = 0.2212023138999939 + 1.0 * 6.375913143157959
Epoch 580, val loss: 0.7739840745925903
Epoch 590, training loss: 6.589369773864746 = 0.20897623896598816 + 1.0 * 6.3803935050964355
Epoch 590, val loss: 0.7772321105003357
Epoch 600, training loss: 6.574341297149658 = 0.1974571943283081 + 1.0 * 6.3768839836120605
Epoch 600, val loss: 0.7809505462646484
Epoch 610, training loss: 6.558330059051514 = 0.18667572736740112 + 1.0 * 6.371654510498047
Epoch 610, val loss: 0.7851199507713318
Epoch 620, training loss: 6.546077728271484 = 0.17654992640018463 + 1.0 * 6.369527816772461
Epoch 620, val loss: 0.7896736860275269
Epoch 630, training loss: 6.550453186035156 = 0.16704978048801422 + 1.0 * 6.383403301239014
Epoch 630, val loss: 0.7944796681404114
Epoch 640, training loss: 6.527133464813232 = 0.15815293788909912 + 1.0 * 6.368980407714844
Epoch 640, val loss: 0.7995312809944153
Epoch 650, training loss: 6.51381778717041 = 0.14983920753002167 + 1.0 * 6.363978385925293
Epoch 650, val loss: 0.8049082159996033
Epoch 660, training loss: 6.5040507316589355 = 0.14201512932777405 + 1.0 * 6.362035751342773
Epoch 660, val loss: 0.8104225397109985
Epoch 670, training loss: 6.4951348304748535 = 0.1346437633037567 + 1.0 * 6.3604912757873535
Epoch 670, val loss: 0.8162053227424622
Epoch 680, training loss: 6.50102424621582 = 0.12770944833755493 + 1.0 * 6.37331485748291
Epoch 680, val loss: 0.8221403956413269
Epoch 690, training loss: 6.482560157775879 = 0.12121742963790894 + 1.0 * 6.361342906951904
Epoch 690, val loss: 0.8281919956207275
Epoch 700, training loss: 6.47234582901001 = 0.11513370275497437 + 1.0 * 6.357212066650391
Epoch 700, val loss: 0.834424614906311
Epoch 710, training loss: 6.465911865234375 = 0.10939392447471619 + 1.0 * 6.356517791748047
Epoch 710, val loss: 0.8407001495361328
Epoch 720, training loss: 6.459517002105713 = 0.10400151461362839 + 1.0 * 6.355515480041504
Epoch 720, val loss: 0.8470160961151123
Epoch 730, training loss: 6.453718662261963 = 0.09895855188369751 + 1.0 * 6.35476016998291
Epoch 730, val loss: 0.8534822463989258
Epoch 740, training loss: 6.444437503814697 = 0.09419796615839005 + 1.0 * 6.3502397537231445
Epoch 740, val loss: 0.8599733710289001
Epoch 750, training loss: 6.438833236694336 = 0.08968377113342285 + 1.0 * 6.349149227142334
Epoch 750, val loss: 0.8665323853492737
Epoch 760, training loss: 6.446179389953613 = 0.08540888130664825 + 1.0 * 6.3607707023620605
Epoch 760, val loss: 0.8730839490890503
Epoch 770, training loss: 6.4308037757873535 = 0.08141953498125076 + 1.0 * 6.349384307861328
Epoch 770, val loss: 0.8796566128730774
Epoch 780, training loss: 6.424365520477295 = 0.07765649259090424 + 1.0 * 6.346709251403809
Epoch 780, val loss: 0.8861467838287354
Epoch 790, training loss: 6.4184136390686035 = 0.0740961879491806 + 1.0 * 6.344317436218262
Epoch 790, val loss: 0.8926735520362854
Epoch 800, training loss: 6.421091079711914 = 0.07074972987174988 + 1.0 * 6.350341320037842
Epoch 800, val loss: 0.8991115093231201
Epoch 810, training loss: 6.412403106689453 = 0.06759316474199295 + 1.0 * 6.3448100090026855
Epoch 810, val loss: 0.9055161476135254
Epoch 820, training loss: 6.411184310913086 = 0.06463144719600677 + 1.0 * 6.346552848815918
Epoch 820, val loss: 0.9119046330451965
Epoch 830, training loss: 6.405318737030029 = 0.06183280050754547 + 1.0 * 6.3434858322143555
Epoch 830, val loss: 0.9181900024414062
Epoch 840, training loss: 6.397741794586182 = 0.05920620262622833 + 1.0 * 6.338535785675049
Epoch 840, val loss: 0.924563467502594
Epoch 850, training loss: 6.394545555114746 = 0.056717969477176666 + 1.0 * 6.337827682495117
Epoch 850, val loss: 0.9308328628540039
Epoch 860, training loss: 6.399330139160156 = 0.05436687543988228 + 1.0 * 6.344963073730469
Epoch 860, val loss: 0.937034547328949
Epoch 870, training loss: 6.390421390533447 = 0.052133407443761826 + 1.0 * 6.338287830352783
Epoch 870, val loss: 0.9431383609771729
Epoch 880, training loss: 6.384792804718018 = 0.05003265291452408 + 1.0 * 6.3347601890563965
Epoch 880, val loss: 0.9493574500083923
Epoch 890, training loss: 6.382786750793457 = 0.048035070300102234 + 1.0 * 6.334751605987549
Epoch 890, val loss: 0.9554262757301331
Epoch 900, training loss: 6.383144378662109 = 0.046147461980581284 + 1.0 * 6.336997032165527
Epoch 900, val loss: 0.961371660232544
Epoch 910, training loss: 6.379000186920166 = 0.04437118023633957 + 1.0 * 6.334629058837891
Epoch 910, val loss: 0.967328667640686
Epoch 920, training loss: 6.37450647354126 = 0.04268481954932213 + 1.0 * 6.331821441650391
Epoch 920, val loss: 0.9731336832046509
Epoch 930, training loss: 6.370638370513916 = 0.04108954966068268 + 1.0 * 6.3295488357543945
Epoch 930, val loss: 0.9789248108863831
Epoch 940, training loss: 6.367300033569336 = 0.03957873210310936 + 1.0 * 6.327721118927002
Epoch 940, val loss: 0.9846432209014893
Epoch 950, training loss: 6.364952564239502 = 0.03813721612095833 + 1.0 * 6.326815128326416
Epoch 950, val loss: 0.9903287291526794
Epoch 960, training loss: 6.373650074005127 = 0.0367649607360363 + 1.0 * 6.33688497543335
Epoch 960, val loss: 0.9958727955818176
Epoch 970, training loss: 6.361964702606201 = 0.03546978533267975 + 1.0 * 6.3264946937561035
Epoch 970, val loss: 1.0013766288757324
Epoch 980, training loss: 6.359342575073242 = 0.034245871007442474 + 1.0 * 6.325096607208252
Epoch 980, val loss: 1.0068787336349487
Epoch 990, training loss: 6.358087062835693 = 0.03307357057929039 + 1.0 * 6.325013637542725
Epoch 990, val loss: 1.0122778415679932
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 10.535897254943848 = 1.9390348196029663 + 1.0 * 8.59686279296875
Epoch 0, val loss: 1.9372684955596924
Epoch 10, training loss: 10.525577545166016 = 1.9288928508758545 + 1.0 * 8.596684455871582
Epoch 10, val loss: 1.927061915397644
Epoch 20, training loss: 10.511368751525879 = 1.916020154953003 + 1.0 * 8.595348358154297
Epoch 20, val loss: 1.9137825965881348
Epoch 30, training loss: 10.48213005065918 = 1.8977980613708496 + 1.0 * 8.584332466125488
Epoch 30, val loss: 1.8947690725326538
Epoch 40, training loss: 10.393218994140625 = 1.8726526498794556 + 1.0 * 8.5205659866333
Epoch 40, val loss: 1.8693655729293823
Epoch 50, training loss: 10.09354019165039 = 1.8444414138793945 + 1.0 * 8.249098777770996
Epoch 50, val loss: 1.842934250831604
Epoch 60, training loss: 9.82879638671875 = 1.820218563079834 + 1.0 * 8.008577346801758
Epoch 60, val loss: 1.821179747581482
Epoch 70, training loss: 9.292853355407715 = 1.8018015623092651 + 1.0 * 7.491052150726318
Epoch 70, val loss: 1.8043122291564941
Epoch 80, training loss: 8.893148422241211 = 1.7895604372024536 + 1.0 * 7.103588104248047
Epoch 80, val loss: 1.7931190729141235
Epoch 90, training loss: 8.734745025634766 = 1.7748266458511353 + 1.0 * 6.959918022155762
Epoch 90, val loss: 1.7795175313949585
Epoch 100, training loss: 8.607406616210938 = 1.7560126781463623 + 1.0 * 6.851393699645996
Epoch 100, val loss: 1.7631361484527588
Epoch 110, training loss: 8.521590232849121 = 1.7365062236785889 + 1.0 * 6.785083770751953
Epoch 110, val loss: 1.7462732791900635
Epoch 120, training loss: 8.451936721801758 = 1.7157279253005981 + 1.0 * 6.736208915710449
Epoch 120, val loss: 1.7280339002609253
Epoch 130, training loss: 8.383522033691406 = 1.692363977432251 + 1.0 * 6.691157817840576
Epoch 130, val loss: 1.7075364589691162
Epoch 140, training loss: 8.319098472595215 = 1.6651971340179443 + 1.0 * 6.65390157699585
Epoch 140, val loss: 1.6838229894638062
Epoch 150, training loss: 8.259241104125977 = 1.6334079504013062 + 1.0 * 6.625833034515381
Epoch 150, val loss: 1.6561505794525146
Epoch 160, training loss: 8.19723892211914 = 1.5967975854873657 + 1.0 * 6.6004414558410645
Epoch 160, val loss: 1.624089002609253
Epoch 170, training loss: 8.134821891784668 = 1.554768681526184 + 1.0 * 6.580053329467773
Epoch 170, val loss: 1.5872262716293335
Epoch 180, training loss: 8.074012756347656 = 1.5079610347747803 + 1.0 * 6.566051483154297
Epoch 180, val loss: 1.5464624166488647
Epoch 190, training loss: 8.006134986877441 = 1.4579923152923584 + 1.0 * 6.548142910003662
Epoch 190, val loss: 1.5031063556671143
Epoch 200, training loss: 7.943008899688721 = 1.405313491821289 + 1.0 * 6.537695407867432
Epoch 200, val loss: 1.457673192024231
Epoch 210, training loss: 7.876141548156738 = 1.3516902923583984 + 1.0 * 6.52445125579834
Epoch 210, val loss: 1.411904215812683
Epoch 220, training loss: 7.8106865882873535 = 1.2975283861160278 + 1.0 * 6.513158321380615
Epoch 220, val loss: 1.366141438484192
Epoch 230, training loss: 7.750312328338623 = 1.2430247068405151 + 1.0 * 6.507287502288818
Epoch 230, val loss: 1.3206580877304077
Epoch 240, training loss: 7.685479164123535 = 1.1892613172531128 + 1.0 * 6.496217727661133
Epoch 240, val loss: 1.2766215801239014
Epoch 250, training loss: 7.6249213218688965 = 1.1362686157226562 + 1.0 * 6.48865270614624
Epoch 250, val loss: 1.2339363098144531
Epoch 260, training loss: 7.572310447692871 = 1.0846610069274902 + 1.0 * 6.487649440765381
Epoch 260, val loss: 1.1932601928710938
Epoch 270, training loss: 7.510530948638916 = 1.035080075263977 + 1.0 * 6.4754509925842285
Epoch 270, val loss: 1.1546838283538818
Epoch 280, training loss: 7.455088138580322 = 0.9868280291557312 + 1.0 * 6.468260288238525
Epoch 280, val loss: 1.1178646087646484
Epoch 290, training loss: 7.401939868927002 = 0.939731240272522 + 1.0 * 6.4622087478637695
Epoch 290, val loss: 1.0823026895523071
Epoch 300, training loss: 7.3621015548706055 = 0.893962025642395 + 1.0 * 6.4681396484375
Epoch 300, val loss: 1.0482268333435059
Epoch 310, training loss: 7.30595064163208 = 0.8506487011909485 + 1.0 * 6.455301761627197
Epoch 310, val loss: 1.0165166854858398
Epoch 320, training loss: 7.257809638977051 = 0.8096268177032471 + 1.0 * 6.448183059692383
Epoch 320, val loss: 0.9868968725204468
Epoch 330, training loss: 7.218348026275635 = 0.7707318067550659 + 1.0 * 6.447616100311279
Epoch 330, val loss: 0.9594157934188843
Epoch 340, training loss: 7.172580242156982 = 0.7342513203620911 + 1.0 * 6.438328742980957
Epoch 340, val loss: 0.9341413974761963
Epoch 350, training loss: 7.133235931396484 = 0.699971079826355 + 1.0 * 6.43326473236084
Epoch 350, val loss: 0.9111348390579224
Epoch 360, training loss: 7.105068206787109 = 0.6680353283882141 + 1.0 * 6.437032699584961
Epoch 360, val loss: 0.8905145525932312
Epoch 370, training loss: 7.065671920776367 = 0.6388713121414185 + 1.0 * 6.426800727844238
Epoch 370, val loss: 0.8725264668464661
Epoch 380, training loss: 7.034131050109863 = 0.6117902994155884 + 1.0 * 6.4223408699035645
Epoch 380, val loss: 0.8569619059562683
Epoch 390, training loss: 7.0049824714660645 = 0.5863633751869202 + 1.0 * 6.418619155883789
Epoch 390, val loss: 0.8432999849319458
Epoch 400, training loss: 6.976977348327637 = 0.5623219609260559 + 1.0 * 6.4146552085876465
Epoch 400, val loss: 0.8313921689987183
Epoch 410, training loss: 6.959246635437012 = 0.539504885673523 + 1.0 * 6.419741630554199
Epoch 410, val loss: 0.8210301399230957
Epoch 420, training loss: 6.93086576461792 = 0.5179932713508606 + 1.0 * 6.412872314453125
Epoch 420, val loss: 0.8119384050369263
Epoch 430, training loss: 6.903836727142334 = 0.4975302815437317 + 1.0 * 6.406306266784668
Epoch 430, val loss: 0.8039968609809875
Epoch 440, training loss: 6.881220817565918 = 0.47764822840690613 + 1.0 * 6.4035725593566895
Epoch 440, val loss: 0.796880304813385
Epoch 450, training loss: 6.863337993621826 = 0.45816147327423096 + 1.0 * 6.405176639556885
Epoch 450, val loss: 0.7903743386268616
Epoch 460, training loss: 6.837592124938965 = 0.43916261196136475 + 1.0 * 6.3984293937683105
Epoch 460, val loss: 0.7843765616416931
Epoch 470, training loss: 6.816382884979248 = 0.4205564558506012 + 1.0 * 6.39582633972168
Epoch 470, val loss: 0.778847336769104
Epoch 480, training loss: 6.795372486114502 = 0.4022004008293152 + 1.0 * 6.393172264099121
Epoch 480, val loss: 0.7737116813659668
Epoch 490, training loss: 6.781732082366943 = 0.3841271996498108 + 1.0 * 6.397604942321777
Epoch 490, val loss: 0.7689434289932251
Epoch 500, training loss: 6.754003524780273 = 0.3664838671684265 + 1.0 * 6.387519836425781
Epoch 500, val loss: 0.7646374106407166
Epoch 510, training loss: 6.734939098358154 = 0.3492218554019928 + 1.0 * 6.385717391967773
Epoch 510, val loss: 0.7607941031455994
Epoch 520, training loss: 6.7206854820251465 = 0.3323030471801758 + 1.0 * 6.388382434844971
Epoch 520, val loss: 0.7572855353355408
Epoch 530, training loss: 6.704158306121826 = 0.31596341729164124 + 1.0 * 6.388195037841797
Epoch 530, val loss: 0.7542986273765564
Epoch 540, training loss: 6.680352210998535 = 0.3002172112464905 + 1.0 * 6.3801350593566895
Epoch 540, val loss: 0.7517184019088745
Epoch 550, training loss: 6.661925315856934 = 0.28493767976760864 + 1.0 * 6.376987457275391
Epoch 550, val loss: 0.7495758533477783
Epoch 560, training loss: 6.662173748016357 = 0.2701781690120697 + 1.0 * 6.391995429992676
Epoch 560, val loss: 0.7478559017181396
Epoch 570, training loss: 6.631179332733154 = 0.2561112344264984 + 1.0 * 6.375068187713623
Epoch 570, val loss: 0.7466015815734863
Epoch 580, training loss: 6.613703727722168 = 0.24265600740909576 + 1.0 * 6.371047496795654
Epoch 580, val loss: 0.7457974553108215
Epoch 590, training loss: 6.599550724029541 = 0.2297278642654419 + 1.0 * 6.369822978973389
Epoch 590, val loss: 0.7454397678375244
Epoch 600, training loss: 6.589313507080078 = 0.2173793762922287 + 1.0 * 6.371933937072754
Epoch 600, val loss: 0.7455166578292847
Epoch 610, training loss: 6.580198287963867 = 0.20570842921733856 + 1.0 * 6.374489784240723
Epoch 610, val loss: 0.7459537982940674
Epoch 620, training loss: 6.560547828674316 = 0.194692462682724 + 1.0 * 6.3658552169799805
Epoch 620, val loss: 0.7468239068984985
Epoch 630, training loss: 6.54753303527832 = 0.18429236114025116 + 1.0 * 6.363240718841553
Epoch 630, val loss: 0.7481027841567993
Epoch 640, training loss: 6.538254737854004 = 0.17444269359111786 + 1.0 * 6.36381196975708
Epoch 640, val loss: 0.749757707118988
Epoch 650, training loss: 6.526325702667236 = 0.1651802510023117 + 1.0 * 6.361145496368408
Epoch 650, val loss: 0.7517905831336975
Epoch 660, training loss: 6.517147064208984 = 0.1564963310956955 + 1.0 * 6.360650539398193
Epoch 660, val loss: 0.7541117072105408
Epoch 670, training loss: 6.51964807510376 = 0.14833325147628784 + 1.0 * 6.371315002441406
Epoch 670, val loss: 0.7567522525787354
Epoch 680, training loss: 6.499655246734619 = 0.14070665836334229 + 1.0 * 6.358948707580566
Epoch 680, val loss: 0.7596692442893982
Epoch 690, training loss: 6.4890313148498535 = 0.13355641067028046 + 1.0 * 6.355474948883057
Epoch 690, val loss: 0.7629095911979675
Epoch 700, training loss: 6.4799933433532715 = 0.1268312633037567 + 1.0 * 6.3531622886657715
Epoch 700, val loss: 0.7664093375205994
Epoch 710, training loss: 6.480936050415039 = 0.12050110846757889 + 1.0 * 6.3604350090026855
Epoch 710, val loss: 0.7701424956321716
Epoch 720, training loss: 6.469547748565674 = 0.11457090079784393 + 1.0 * 6.354976654052734
Epoch 720, val loss: 0.7740557789802551
Epoch 730, training loss: 6.458527088165283 = 0.10902103036642075 + 1.0 * 6.34950590133667
Epoch 730, val loss: 0.7781084179878235
Epoch 740, training loss: 6.451925277709961 = 0.10378449410200119 + 1.0 * 6.348140716552734
Epoch 740, val loss: 0.7823410034179688
Epoch 750, training loss: 6.449323654174805 = 0.09884428977966309 + 1.0 * 6.3504791259765625
Epoch 750, val loss: 0.786747932434082
Epoch 760, training loss: 6.443569183349609 = 0.09421694278717041 + 1.0 * 6.3493523597717285
Epoch 760, val loss: 0.7911306023597717
Epoch 770, training loss: 6.4373369216918945 = 0.08986728638410568 + 1.0 * 6.347469806671143
Epoch 770, val loss: 0.7956472039222717
Epoch 780, training loss: 6.429698467254639 = 0.08576637506484985 + 1.0 * 6.343932151794434
Epoch 780, val loss: 0.8002345561981201
Epoch 790, training loss: 6.436631202697754 = 0.08190242201089859 + 1.0 * 6.354728698730469
Epoch 790, val loss: 0.8048692941665649
Epoch 800, training loss: 6.422034740447998 = 0.07826043665409088 + 1.0 * 6.343774318695068
Epoch 800, val loss: 0.8095254898071289
Epoch 810, training loss: 6.416958808898926 = 0.07482445240020752 + 1.0 * 6.342134475708008
Epoch 810, val loss: 0.814243495464325
Epoch 820, training loss: 6.412053108215332 = 0.07158876955509186 + 1.0 * 6.340464115142822
Epoch 820, val loss: 0.8189800977706909
Epoch 830, training loss: 6.406267166137695 = 0.06853252649307251 + 1.0 * 6.337734699249268
Epoch 830, val loss: 0.8237473964691162
Epoch 840, training loss: 6.402127265930176 = 0.0656391978263855 + 1.0 * 6.336488246917725
Epoch 840, val loss: 0.8285624384880066
Epoch 850, training loss: 6.407346725463867 = 0.06289386004209518 + 1.0 * 6.344452857971191
Epoch 850, val loss: 0.833415687084198
Epoch 860, training loss: 6.39825439453125 = 0.06031052768230438 + 1.0 * 6.337944030761719
Epoch 860, val loss: 0.8380911350250244
Epoch 870, training loss: 6.392518997192383 = 0.05786481499671936 + 1.0 * 6.334654331207275
Epoch 870, val loss: 0.8428285121917725
Epoch 880, training loss: 6.390660762786865 = 0.055542897433042526 + 1.0 * 6.335117816925049
Epoch 880, val loss: 0.8475273251533508
Epoch 890, training loss: 6.386013507843018 = 0.05334314703941345 + 1.0 * 6.332670211791992
Epoch 890, val loss: 0.852256178855896
Epoch 900, training loss: 6.382157325744629 = 0.05126240849494934 + 1.0 * 6.330894947052002
Epoch 900, val loss: 0.8569318652153015
Epoch 910, training loss: 6.385059356689453 = 0.04928427189588547 + 1.0 * 6.335774898529053
Epoch 910, val loss: 0.8615899682044983
Epoch 920, training loss: 6.384783744812012 = 0.047408103942871094 + 1.0 * 6.337375640869141
Epoch 920, val loss: 0.8661109209060669
Epoch 930, training loss: 6.3739333152771 = 0.04564397782087326 + 1.0 * 6.32828950881958
Epoch 930, val loss: 0.8705572485923767
Epoch 940, training loss: 6.371152877807617 = 0.04395974427461624 + 1.0 * 6.327193260192871
Epoch 940, val loss: 0.8750485777854919
Epoch 950, training loss: 6.367724418640137 = 0.04235110431909561 + 1.0 * 6.32537317276001
Epoch 950, val loss: 0.8796190619468689
Epoch 960, training loss: 6.365551471710205 = 0.04082011803984642 + 1.0 * 6.324731349945068
Epoch 960, val loss: 0.8841357231140137
Epoch 970, training loss: 6.379172325134277 = 0.039363522082567215 + 1.0 * 6.339808940887451
Epoch 970, val loss: 0.8885453939437866
Epoch 980, training loss: 6.361932754516602 = 0.03797813504934311 + 1.0 * 6.3239545822143555
Epoch 980, val loss: 0.8929405212402344
Epoch 990, training loss: 6.358972072601318 = 0.03666369989514351 + 1.0 * 6.322308540344238
Epoch 990, val loss: 0.8972646594047546
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8344754876120191
The final CL Acc:0.81481, 0.01983, The final GNN Acc:0.83694, 0.00245
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9486])
updated graph: torch.Size([2, 10504])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.54868221282959 = 1.951830506324768 + 1.0 * 8.596851348876953
Epoch 0, val loss: 1.9496679306030273
Epoch 10, training loss: 10.538320541381836 = 1.9416420459747314 + 1.0 * 8.596678733825684
Epoch 10, val loss: 1.9392361640930176
Epoch 20, training loss: 10.524620056152344 = 1.9292705059051514 + 1.0 * 8.595349311828613
Epoch 20, val loss: 1.9262205362319946
Epoch 30, training loss: 10.496589660644531 = 1.9116883277893066 + 1.0 * 8.584900856018066
Epoch 30, val loss: 1.9076275825500488
Epoch 40, training loss: 10.410260200500488 = 1.8868076801300049 + 1.0 * 8.523452758789062
Epoch 40, val loss: 1.8821911811828613
Epoch 50, training loss: 10.01271915435791 = 1.8584672212600708 + 1.0 * 8.154252052307129
Epoch 50, val loss: 1.8552167415618896
Epoch 60, training loss: 9.483448028564453 = 1.835299015045166 + 1.0 * 7.648149013519287
Epoch 60, val loss: 1.8334693908691406
Epoch 70, training loss: 9.075398445129395 = 1.8203411102294922 + 1.0 * 7.255057334899902
Epoch 70, val loss: 1.8185404539108276
Epoch 80, training loss: 8.907758712768555 = 1.8053022623062134 + 1.0 * 7.102456092834473
Epoch 80, val loss: 1.8042163848876953
Epoch 90, training loss: 8.760822296142578 = 1.7873457670211792 + 1.0 * 6.973476886749268
Epoch 90, val loss: 1.7889297008514404
Epoch 100, training loss: 8.653950691223145 = 1.771315574645996 + 1.0 * 6.882635116577148
Epoch 100, val loss: 1.7757928371429443
Epoch 110, training loss: 8.560232162475586 = 1.7577693462371826 + 1.0 * 6.802463054656982
Epoch 110, val loss: 1.7635170221328735
Epoch 120, training loss: 8.487746238708496 = 1.743518352508545 + 1.0 * 6.744227886199951
Epoch 120, val loss: 1.7502896785736084
Epoch 130, training loss: 8.42788314819336 = 1.7270461320877075 + 1.0 * 6.700837135314941
Epoch 130, val loss: 1.7357045412063599
Epoch 140, training loss: 8.370882034301758 = 1.7080901861190796 + 1.0 * 6.662791728973389
Epoch 140, val loss: 1.7195980548858643
Epoch 150, training loss: 8.318836212158203 = 1.6862037181854248 + 1.0 * 6.632632255554199
Epoch 150, val loss: 1.7012202739715576
Epoch 160, training loss: 8.266060829162598 = 1.6609026193618774 + 1.0 * 6.60515832901001
Epoch 160, val loss: 1.6798006296157837
Epoch 170, training loss: 8.212564468383789 = 1.6312518119812012 + 1.0 * 6.58131217956543
Epoch 170, val loss: 1.6546517610549927
Epoch 180, training loss: 8.163511276245117 = 1.5965725183486938 + 1.0 * 6.566938400268555
Epoch 180, val loss: 1.6251370906829834
Epoch 190, training loss: 8.103740692138672 = 1.557383418083191 + 1.0 * 6.54635763168335
Epoch 190, val loss: 1.5921963453292847
Epoch 200, training loss: 8.045525550842285 = 1.5134910345077515 + 1.0 * 6.532034397125244
Epoch 200, val loss: 1.5554543733596802
Epoch 210, training loss: 7.986749172210693 = 1.465246558189392 + 1.0 * 6.521502494812012
Epoch 210, val loss: 1.5155223608016968
Epoch 220, training loss: 7.92324161529541 = 1.413910150527954 + 1.0 * 6.509331226348877
Epoch 220, val loss: 1.4735883474349976
Epoch 230, training loss: 7.862058639526367 = 1.3604602813720703 + 1.0 * 6.501598358154297
Epoch 230, val loss: 1.4305918216705322
Epoch 240, training loss: 7.7971696853637695 = 1.306656837463379 + 1.0 * 6.490512847900391
Epoch 240, val loss: 1.3879698514938354
Epoch 250, training loss: 7.7355804443359375 = 1.2527308464050293 + 1.0 * 6.482849597930908
Epoch 250, val loss: 1.3458226919174194
Epoch 260, training loss: 7.677984237670898 = 1.1999578475952148 + 1.0 * 6.478026390075684
Epoch 260, val loss: 1.305258870124817
Epoch 270, training loss: 7.618438243865967 = 1.1489351987838745 + 1.0 * 6.469502925872803
Epoch 270, val loss: 1.266593337059021
Epoch 280, training loss: 7.563085556030273 = 1.0997580289840698 + 1.0 * 6.463327407836914
Epoch 280, val loss: 1.2298493385314941
Epoch 290, training loss: 7.510159969329834 = 1.05266273021698 + 1.0 * 6.4574971199035645
Epoch 290, val loss: 1.1949388980865479
Epoch 300, training loss: 7.464138507843018 = 1.0075024366378784 + 1.0 * 6.45663595199585
Epoch 300, val loss: 1.1618404388427734
Epoch 310, training loss: 7.410696983337402 = 0.9646007418632507 + 1.0 * 6.446096420288086
Epoch 310, val loss: 1.1304755210876465
Epoch 320, training loss: 7.3646931648254395 = 0.9237961769104004 + 1.0 * 6.440896987915039
Epoch 320, val loss: 1.1009515523910522
Epoch 330, training loss: 7.322986602783203 = 0.8850914835929871 + 1.0 * 6.43789529800415
Epoch 330, val loss: 1.073001742362976
Epoch 340, training loss: 7.282546520233154 = 0.8485631942749023 + 1.0 * 6.433983325958252
Epoch 340, val loss: 1.0469022989273071
Epoch 350, training loss: 7.241927146911621 = 0.8138777613639832 + 1.0 * 6.428049564361572
Epoch 350, val loss: 1.0224649906158447
Epoch 360, training loss: 7.204721450805664 = 0.7808690667152405 + 1.0 * 6.423852443695068
Epoch 360, val loss: 0.9995075464248657
Epoch 370, training loss: 7.175695419311523 = 0.7491030097007751 + 1.0 * 6.4265923500061035
Epoch 370, val loss: 0.9779902696609497
Epoch 380, training loss: 7.135278224945068 = 0.7186442017555237 + 1.0 * 6.4166340827941895
Epoch 380, val loss: 0.9578424692153931
Epoch 390, training loss: 7.10452938079834 = 0.6890196800231934 + 1.0 * 6.4155097007751465
Epoch 390, val loss: 0.938805341720581
Epoch 400, training loss: 7.07581901550293 = 0.6601397395133972 + 1.0 * 6.415679454803467
Epoch 400, val loss: 0.9208720326423645
Epoch 410, training loss: 7.039226531982422 = 0.6320274472236633 + 1.0 * 6.407198905944824
Epoch 410, val loss: 0.9041334986686707
Epoch 420, training loss: 7.006587028503418 = 0.6043441295623779 + 1.0 * 6.402242660522461
Epoch 420, val loss: 0.8884525299072266
Epoch 430, training loss: 6.977272033691406 = 0.5771727561950684 + 1.0 * 6.400099277496338
Epoch 430, val loss: 0.8737443685531616
Epoch 440, training loss: 6.955511093139648 = 0.5506287813186646 + 1.0 * 6.404882431030273
Epoch 440, val loss: 0.860284686088562
Epoch 450, training loss: 6.920658111572266 = 0.5246629118919373 + 1.0 * 6.395995140075684
Epoch 450, val loss: 0.8479759097099304
Epoch 460, training loss: 6.891424179077148 = 0.4992213249206543 + 1.0 * 6.392202854156494
Epoch 460, val loss: 0.8367174863815308
Epoch 470, training loss: 6.864484786987305 = 0.4743775427341461 + 1.0 * 6.390107154846191
Epoch 470, val loss: 0.8264850378036499
Epoch 480, training loss: 6.841838836669922 = 0.4503169059753418 + 1.0 * 6.39152193069458
Epoch 480, val loss: 0.817352831363678
Epoch 490, training loss: 6.812345027923584 = 0.42732733488082886 + 1.0 * 6.3850178718566895
Epoch 490, val loss: 0.8094630837440491
Epoch 500, training loss: 6.787503242492676 = 0.40512776374816895 + 1.0 * 6.382375240325928
Epoch 500, val loss: 0.8026114106178284
Epoch 510, training loss: 6.764074802398682 = 0.38369882106781006 + 1.0 * 6.380375862121582
Epoch 510, val loss: 0.7964434623718262
Epoch 520, training loss: 6.753632545471191 = 0.3631448447704315 + 1.0 * 6.3904876708984375
Epoch 520, val loss: 0.7911750674247742
Epoch 530, training loss: 6.722247123718262 = 0.3436707556247711 + 1.0 * 6.378576278686523
Epoch 530, val loss: 0.7867364287376404
Epoch 540, training loss: 6.699863910675049 = 0.32513824105262756 + 1.0 * 6.374725818634033
Epoch 540, val loss: 0.7832553386688232
Epoch 550, training loss: 6.681758880615234 = 0.30746299028396606 + 1.0 * 6.374295711517334
Epoch 550, val loss: 0.780357301235199
Epoch 560, training loss: 6.666133403778076 = 0.29068809747695923 + 1.0 * 6.375445365905762
Epoch 560, val loss: 0.7780824899673462
Epoch 570, training loss: 6.667667388916016 = 0.27485257387161255 + 1.0 * 6.392814636230469
Epoch 570, val loss: 0.7764472365379333
Epoch 580, training loss: 6.632424831390381 = 0.26002761721611023 + 1.0 * 6.372397422790527
Epoch 580, val loss: 0.7753946781158447
Epoch 590, training loss: 6.6146416664123535 = 0.2460147738456726 + 1.0 * 6.368627071380615
Epoch 590, val loss: 0.7750462889671326
Epoch 600, training loss: 6.598134994506836 = 0.23272326588630676 + 1.0 * 6.365411758422852
Epoch 600, val loss: 0.7750366926193237
Epoch 610, training loss: 6.583052635192871 = 0.2201380729675293 + 1.0 * 6.362914562225342
Epoch 610, val loss: 0.7755383253097534
Epoch 620, training loss: 6.572730541229248 = 0.2082262486219406 + 1.0 * 6.364504337310791
Epoch 620, val loss: 0.7765849828720093
Epoch 630, training loss: 6.568476676940918 = 0.19706186652183533 + 1.0 * 6.371414661407471
Epoch 630, val loss: 0.7778092622756958
Epoch 640, training loss: 6.546443939208984 = 0.18661434948444366 + 1.0 * 6.359829425811768
Epoch 640, val loss: 0.7797955870628357
Epoch 650, training loss: 6.534124374389648 = 0.1767808496952057 + 1.0 * 6.357343673706055
Epoch 650, val loss: 0.7820442318916321
Epoch 660, training loss: 6.534325122833252 = 0.16753005981445312 + 1.0 * 6.366795063018799
Epoch 660, val loss: 0.7845041751861572
Epoch 670, training loss: 6.515175819396973 = 0.15886077284812927 + 1.0 * 6.3563151359558105
Epoch 670, val loss: 0.7874581813812256
Epoch 680, training loss: 6.50335168838501 = 0.1507001519203186 + 1.0 * 6.352651596069336
Epoch 680, val loss: 0.7907394170761108
Epoch 690, training loss: 6.496553897857666 = 0.1430160254240036 + 1.0 * 6.3535380363464355
Epoch 690, val loss: 0.7942506074905396
Epoch 700, training loss: 6.486618518829346 = 0.1358104646205902 + 1.0 * 6.350808143615723
Epoch 700, val loss: 0.7978749871253967
Epoch 710, training loss: 6.482467174530029 = 0.12906411290168762 + 1.0 * 6.353403091430664
Epoch 710, val loss: 0.8019298911094666
Epoch 720, training loss: 6.470097541809082 = 0.12272395938634872 + 1.0 * 6.3473734855651855
Epoch 720, val loss: 0.8062039017677307
Epoch 730, training loss: 6.464828968048096 = 0.11675084382295609 + 1.0 * 6.34807825088501
Epoch 730, val loss: 0.8105862140655518
Epoch 740, training loss: 6.45646333694458 = 0.1111258864402771 + 1.0 * 6.345337390899658
Epoch 740, val loss: 0.8150954246520996
Epoch 750, training loss: 6.452071666717529 = 0.10585234314203262 + 1.0 * 6.346219539642334
Epoch 750, val loss: 0.820012629032135
Epoch 760, training loss: 6.443425178527832 = 0.10087540000677109 + 1.0 * 6.342549800872803
Epoch 760, val loss: 0.8248931765556335
Epoch 770, training loss: 6.446550369262695 = 0.09617659449577332 + 1.0 * 6.3503737449646
Epoch 770, val loss: 0.8298954367637634
Epoch 780, training loss: 6.436792373657227 = 0.0917573794722557 + 1.0 * 6.345035076141357
Epoch 780, val loss: 0.8350355625152588
Epoch 790, training loss: 6.428151607513428 = 0.08758828788995743 + 1.0 * 6.3405632972717285
Epoch 790, val loss: 0.840456485748291
Epoch 800, training loss: 6.4275221824646 = 0.08364961296319962 + 1.0 * 6.343872547149658
Epoch 800, val loss: 0.8457647562026978
Epoch 810, training loss: 6.421818256378174 = 0.07992928475141525 + 1.0 * 6.341888904571533
Epoch 810, val loss: 0.8511040210723877
Epoch 820, training loss: 6.4140825271606445 = 0.07642795145511627 + 1.0 * 6.3376545906066895
Epoch 820, val loss: 0.8567054271697998
Epoch 830, training loss: 6.410881519317627 = 0.0731157660484314 + 1.0 * 6.337765693664551
Epoch 830, val loss: 0.8623332977294922
Epoch 840, training loss: 6.407685279846191 = 0.06997928023338318 + 1.0 * 6.337706089019775
Epoch 840, val loss: 0.8678408265113831
Epoch 850, training loss: 6.39987850189209 = 0.06701503694057465 + 1.0 * 6.3328633308410645
Epoch 850, val loss: 0.873525857925415
Epoch 860, training loss: 6.39785623550415 = 0.06420806795358658 + 1.0 * 6.333648204803467
Epoch 860, val loss: 0.879331648349762
Epoch 870, training loss: 6.393362045288086 = 0.06154877319931984 + 1.0 * 6.331813335418701
Epoch 870, val loss: 0.8850302696228027
Epoch 880, training loss: 6.391473770141602 = 0.05902274325489998 + 1.0 * 6.332450866699219
Epoch 880, val loss: 0.890769898891449
Epoch 890, training loss: 6.388329029083252 = 0.05663113668560982 + 1.0 * 6.331697940826416
Epoch 890, val loss: 0.8966529369354248
Epoch 900, training loss: 6.386147975921631 = 0.05436208099126816 + 1.0 * 6.331785678863525
Epoch 900, val loss: 0.9024237394332886
Epoch 910, training loss: 6.378670692443848 = 0.05221299082040787 + 1.0 * 6.326457500457764
Epoch 910, val loss: 0.9082208275794983
Epoch 920, training loss: 6.37702751159668 = 0.050178367644548416 + 1.0 * 6.326848983764648
Epoch 920, val loss: 0.9141843318939209
Epoch 930, training loss: 6.3756327629089355 = 0.048243291676044464 + 1.0 * 6.327389240264893
Epoch 930, val loss: 0.919883131980896
Epoch 940, training loss: 6.373476028442383 = 0.04641006514430046 + 1.0 * 6.327065944671631
Epoch 940, val loss: 0.9257616996765137
Epoch 950, training loss: 6.367064952850342 = 0.04466414079070091 + 1.0 * 6.32240104675293
Epoch 950, val loss: 0.9316193461418152
Epoch 960, training loss: 6.369749546051025 = 0.043007742613554 + 1.0 * 6.326741695404053
Epoch 960, val loss: 0.9375026822090149
Epoch 970, training loss: 6.363682746887207 = 0.04143012687563896 + 1.0 * 6.3222527503967285
Epoch 970, val loss: 0.9431660175323486
Epoch 980, training loss: 6.359963893890381 = 0.0399339534342289 + 1.0 * 6.3200297355651855
Epoch 980, val loss: 0.9490910172462463
Epoch 990, training loss: 6.371318340301514 = 0.038508325815200806 + 1.0 * 6.332809925079346
Epoch 990, val loss: 0.9548109173774719
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 10.53049373626709 = 1.9336984157562256 + 1.0 * 8.596795082092285
Epoch 0, val loss: 1.931706428527832
Epoch 10, training loss: 10.520651817321777 = 1.9242931604385376 + 1.0 * 8.596358299255371
Epoch 10, val loss: 1.922606348991394
Epoch 20, training loss: 10.505350112915039 = 1.9123024940490723 + 1.0 * 8.593047142028809
Epoch 20, val loss: 1.910710096359253
Epoch 30, training loss: 10.464812278747559 = 1.8953516483306885 + 1.0 * 8.56946086883545
Epoch 30, val loss: 1.8935060501098633
Epoch 40, training loss: 10.295114517211914 = 1.873857021331787 + 1.0 * 8.421257019042969
Epoch 40, val loss: 1.8720996379852295
Epoch 50, training loss: 9.74083137512207 = 1.8519972562789917 + 1.0 * 7.888833999633789
Epoch 50, val loss: 1.850462555885315
Epoch 60, training loss: 9.425947189331055 = 1.835092306137085 + 1.0 * 7.590854644775391
Epoch 60, val loss: 1.8350107669830322
Epoch 70, training loss: 9.173595428466797 = 1.8201113939285278 + 1.0 * 7.3534836769104
Epoch 70, val loss: 1.8208692073822021
Epoch 80, training loss: 8.924339294433594 = 1.8050185441970825 + 1.0 * 7.119320869445801
Epoch 80, val loss: 1.8067926168441772
Epoch 90, training loss: 8.759495735168457 = 1.7927337884902954 + 1.0 * 6.966762065887451
Epoch 90, val loss: 1.7956510782241821
Epoch 100, training loss: 8.649628639221191 = 1.7800755500793457 + 1.0 * 6.869553089141846
Epoch 100, val loss: 1.7839033603668213
Epoch 110, training loss: 8.550247192382812 = 1.7665133476257324 + 1.0 * 6.78373384475708
Epoch 110, val loss: 1.7712618112564087
Epoch 120, training loss: 8.473845481872559 = 1.7525107860565186 + 1.0 * 6.721334934234619
Epoch 120, val loss: 1.7587575912475586
Epoch 130, training loss: 8.410646438598633 = 1.73702871799469 + 1.0 * 6.673617362976074
Epoch 130, val loss: 1.745463252067566
Epoch 140, training loss: 8.35349178314209 = 1.719065546989441 + 1.0 * 6.634426593780518
Epoch 140, val loss: 1.7303807735443115
Epoch 150, training loss: 8.30101490020752 = 1.698235273361206 + 1.0 * 6.602779865264893
Epoch 150, val loss: 1.713004231452942
Epoch 160, training loss: 8.255247116088867 = 1.673851728439331 + 1.0 * 6.581395149230957
Epoch 160, val loss: 1.69259774684906
Epoch 170, training loss: 8.205360412597656 = 1.6455893516540527 + 1.0 * 6.559770584106445
Epoch 170, val loss: 1.6689878702163696
Epoch 180, training loss: 8.155126571655273 = 1.6128171682357788 + 1.0 * 6.542309284210205
Epoch 180, val loss: 1.6416367292404175
Epoch 190, training loss: 8.103845596313477 = 1.5752209424972534 + 1.0 * 6.528624534606934
Epoch 190, val loss: 1.6104451417922974
Epoch 200, training loss: 8.048157691955566 = 1.5337297916412354 + 1.0 * 6.51442813873291
Epoch 200, val loss: 1.5761152505874634
Epoch 210, training loss: 7.989748001098633 = 1.4879827499389648 + 1.0 * 6.501765251159668
Epoch 210, val loss: 1.5384492874145508
Epoch 220, training loss: 7.928628921508789 = 1.4382517337799072 + 1.0 * 6.490376949310303
Epoch 220, val loss: 1.4977346658706665
Epoch 230, training loss: 7.873559951782227 = 1.3855154514312744 + 1.0 * 6.488044738769531
Epoch 230, val loss: 1.4549072980880737
Epoch 240, training loss: 7.806739807128906 = 1.3319389820098877 + 1.0 * 6.474801063537598
Epoch 240, val loss: 1.4121140241622925
Epoch 250, training loss: 7.742776393890381 = 1.278480887413025 + 1.0 * 6.464295387268066
Epoch 250, val loss: 1.3699593544006348
Epoch 260, training loss: 7.683141708374023 = 1.2265822887420654 + 1.0 * 6.456559658050537
Epoch 260, val loss: 1.3298813104629517
Epoch 270, training loss: 7.627586364746094 = 1.1777589321136475 + 1.0 * 6.449827194213867
Epoch 270, val loss: 1.2931355237960815
Epoch 280, training loss: 7.5748090744018555 = 1.1318529844284058 + 1.0 * 6.44295597076416
Epoch 280, val loss: 1.2597219944000244
Epoch 290, training loss: 7.529128551483154 = 1.088990330696106 + 1.0 * 6.440138339996338
Epoch 290, val loss: 1.2294811010360718
Epoch 300, training loss: 7.483898639678955 = 1.0496282577514648 + 1.0 * 6.43427038192749
Epoch 300, val loss: 1.2029122114181519
Epoch 310, training loss: 7.439651966094971 = 1.01348876953125 + 1.0 * 6.426163196563721
Epoch 310, val loss: 1.1794277429580688
Epoch 320, training loss: 7.398762226104736 = 0.9797326326370239 + 1.0 * 6.419029712677002
Epoch 320, val loss: 1.1582832336425781
Epoch 330, training loss: 7.376123428344727 = 0.9478248953819275 + 1.0 * 6.428298473358154
Epoch 330, val loss: 1.1389095783233643
Epoch 340, training loss: 7.333317279815674 = 0.9181298017501831 + 1.0 * 6.415187358856201
Epoch 340, val loss: 1.1214159727096558
Epoch 350, training loss: 7.296260833740234 = 0.8900237083435059 + 1.0 * 6.4062371253967285
Epoch 350, val loss: 1.1053842306137085
Epoch 360, training loss: 7.263879299163818 = 0.8628636002540588 + 1.0 * 6.401015758514404
Epoch 360, val loss: 1.0903079509735107
Epoch 370, training loss: 7.236052513122559 = 0.8363427519798279 + 1.0 * 6.399709701538086
Epoch 370, val loss: 1.0760408639907837
Epoch 380, training loss: 7.2112202644348145 = 0.81074059009552 + 1.0 * 6.400479793548584
Epoch 380, val loss: 1.0625998973846436
Epoch 390, training loss: 7.176493167877197 = 0.7857820391654968 + 1.0 * 6.390711307525635
Epoch 390, val loss: 1.0501067638397217
Epoch 400, training loss: 7.147918701171875 = 0.7610532641410828 + 1.0 * 6.386865615844727
Epoch 400, val loss: 1.0381656885147095
Epoch 410, training loss: 7.119662761688232 = 0.7363338470458984 + 1.0 * 6.383328914642334
Epoch 410, val loss: 1.0266859531402588
Epoch 420, training loss: 7.09335994720459 = 0.7116409540176392 + 1.0 * 6.38171911239624
Epoch 420, val loss: 1.0157780647277832
Epoch 430, training loss: 7.064382553100586 = 0.6870545744895935 + 1.0 * 6.377327919006348
Epoch 430, val loss: 1.0054066181182861
Epoch 440, training loss: 7.037809371948242 = 0.6622658967971802 + 1.0 * 6.375543594360352
Epoch 440, val loss: 0.9954387545585632
Epoch 450, training loss: 7.015500068664551 = 0.6372687816619873 + 1.0 * 6.378231048583984
Epoch 450, val loss: 0.9858616590499878
Epoch 460, training loss: 6.985177040100098 = 0.6122033596038818 + 1.0 * 6.372973918914795
Epoch 460, val loss: 0.9766437411308289
Epoch 470, training loss: 6.955295562744141 = 0.5868661999702454 + 1.0 * 6.368429183959961
Epoch 470, val loss: 0.9678133130073547
Epoch 480, training loss: 6.928311347961426 = 0.5611916184425354 + 1.0 * 6.367119789123535
Epoch 480, val loss: 0.959208071231842
Epoch 490, training loss: 6.901015758514404 = 0.5354838371276855 + 1.0 * 6.365531921386719
Epoch 490, val loss: 0.9510747790336609
Epoch 500, training loss: 6.871967792510986 = 0.510008692741394 + 1.0 * 6.361958980560303
Epoch 500, val loss: 0.9435234665870667
Epoch 510, training loss: 6.8446831703186035 = 0.48466646671295166 + 1.0 * 6.360016822814941
Epoch 510, val loss: 0.9364821314811707
Epoch 520, training loss: 6.82087516784668 = 0.4595833718776703 + 1.0 * 6.361291885375977
Epoch 520, val loss: 0.930108904838562
Epoch 530, training loss: 6.796586990356445 = 0.4351404905319214 + 1.0 * 6.361446380615234
Epoch 530, val loss: 0.9243665933609009
Epoch 540, training loss: 6.7666096687316895 = 0.41144463419914246 + 1.0 * 6.355165004730225
Epoch 540, val loss: 0.91947340965271
Epoch 550, training loss: 6.744529724121094 = 0.388553261756897 + 1.0 * 6.355976581573486
Epoch 550, val loss: 0.9153238534927368
Epoch 560, training loss: 6.718047142028809 = 0.36663639545440674 + 1.0 * 6.351410865783691
Epoch 560, val loss: 0.9120224714279175
Epoch 570, training loss: 6.695772647857666 = 0.3457116484642029 + 1.0 * 6.350060939788818
Epoch 570, val loss: 0.909517228603363
Epoch 580, training loss: 6.6764817237854 = 0.3257559537887573 + 1.0 * 6.3507256507873535
Epoch 580, val loss: 0.9077423214912415
Epoch 590, training loss: 6.658543586730957 = 0.3068489730358124 + 1.0 * 6.351694583892822
Epoch 590, val loss: 0.9067439436912537
Epoch 600, training loss: 6.639957904815674 = 0.28904178738594055 + 1.0 * 6.350915908813477
Epoch 600, val loss: 0.9063817262649536
Epoch 610, training loss: 6.6183390617370605 = 0.27228787541389465 + 1.0 * 6.346051216125488
Epoch 610, val loss: 0.9067555665969849
Epoch 620, training loss: 6.599775314331055 = 0.2564813196659088 + 1.0 * 6.343294143676758
Epoch 620, val loss: 0.9077232480049133
Epoch 630, training loss: 6.588534355163574 = 0.24155756831169128 + 1.0 * 6.3469767570495605
Epoch 630, val loss: 0.90934157371521
Epoch 640, training loss: 6.5709381103515625 = 0.22760827839374542 + 1.0 * 6.343329906463623
Epoch 640, val loss: 0.9115399122238159
Epoch 650, training loss: 6.556783199310303 = 0.21453765034675598 + 1.0 * 6.342245578765869
Epoch 650, val loss: 0.9143319129943848
Epoch 660, training loss: 6.543262958526611 = 0.20230019092559814 + 1.0 * 6.340962886810303
Epoch 660, val loss: 0.9175989627838135
Epoch 670, training loss: 6.529355049133301 = 0.19085435569286346 + 1.0 * 6.338500499725342
Epoch 670, val loss: 0.9213748574256897
Epoch 680, training loss: 6.5177764892578125 = 0.1801208257675171 + 1.0 * 6.337655544281006
Epoch 680, val loss: 0.9256092309951782
Epoch 690, training loss: 6.505215167999268 = 0.17007678747177124 + 1.0 * 6.335138320922852
Epoch 690, val loss: 0.930219829082489
Epoch 700, training loss: 6.499866008758545 = 0.16068358719348907 + 1.0 * 6.339182376861572
Epoch 700, val loss: 0.9351803064346313
Epoch 710, training loss: 6.484381675720215 = 0.1519012600183487 + 1.0 * 6.332480430603027
Epoch 710, val loss: 0.9404831528663635
Epoch 720, training loss: 6.474300861358643 = 0.14370477199554443 + 1.0 * 6.330595970153809
Epoch 720, val loss: 0.9461603164672852
Epoch 730, training loss: 6.486520290374756 = 0.1360296756029129 + 1.0 * 6.350490570068359
Epoch 730, val loss: 0.952025830745697
Epoch 740, training loss: 6.462760925292969 = 0.12895211577415466 + 1.0 * 6.333808898925781
Epoch 740, val loss: 0.9581663012504578
Epoch 750, training loss: 6.450037002563477 = 0.12233372777700424 + 1.0 * 6.327703475952148
Epoch 750, val loss: 0.964560866355896
Epoch 760, training loss: 6.441718101501465 = 0.11612045764923096 + 1.0 * 6.325597763061523
Epoch 760, val loss: 0.9712075591087341
Epoch 770, training loss: 6.434972763061523 = 0.11028795689344406 + 1.0 * 6.3246846199035645
Epoch 770, val loss: 0.9780904054641724
Epoch 780, training loss: 6.443219184875488 = 0.10481488704681396 + 1.0 * 6.338404178619385
Epoch 780, val loss: 0.9851190447807312
Epoch 790, training loss: 6.422005653381348 = 0.09970463812351227 + 1.0 * 6.322300910949707
Epoch 790, val loss: 0.9922906756401062
Epoch 800, training loss: 6.417552471160889 = 0.09490407258272171 + 1.0 * 6.322648525238037
Epoch 800, val loss: 0.9995929598808289
Epoch 810, training loss: 6.4150238037109375 = 0.09038666635751724 + 1.0 * 6.324636936187744
Epoch 810, val loss: 1.007051706314087
Epoch 820, training loss: 6.407071590423584 = 0.08614779263734818 + 1.0 * 6.320923805236816
Epoch 820, val loss: 1.0145881175994873
Epoch 830, training loss: 6.402596473693848 = 0.0821739062666893 + 1.0 * 6.320422649383545
Epoch 830, val loss: 1.022096872329712
Epoch 840, training loss: 6.396540641784668 = 0.07842431962490082 + 1.0 * 6.318116188049316
Epoch 840, val loss: 1.029763102531433
Epoch 850, training loss: 6.398451805114746 = 0.07488688081502914 + 1.0 * 6.3235650062561035
Epoch 850, val loss: 1.0374644994735718
Epoch 860, training loss: 6.389747142791748 = 0.07156781107187271 + 1.0 * 6.318179130554199
Epoch 860, val loss: 1.045257568359375
Epoch 870, training loss: 6.3894243240356445 = 0.06844046711921692 + 1.0 * 6.32098388671875
Epoch 870, val loss: 1.0529481172561646
Epoch 880, training loss: 6.379788875579834 = 0.06549306958913803 + 1.0 * 6.314295768737793
Epoch 880, val loss: 1.060701608657837
Epoch 890, training loss: 6.375984191894531 = 0.06271140277385712 + 1.0 * 6.313272953033447
Epoch 890, val loss: 1.0684587955474854
Epoch 900, training loss: 6.380777359008789 = 0.060078829526901245 + 1.0 * 6.3206987380981445
Epoch 900, val loss: 1.0762450695037842
Epoch 910, training loss: 6.373505592346191 = 0.05760154873132706 + 1.0 * 6.315904140472412
Epoch 910, val loss: 1.0839899778366089
Epoch 920, training loss: 6.365854263305664 = 0.055265676230192184 + 1.0 * 6.310588359832764
Epoch 920, val loss: 1.0916696786880493
Epoch 930, training loss: 6.367538928985596 = 0.053050071001052856 + 1.0 * 6.314488887786865
Epoch 930, val loss: 1.0993804931640625
Epoch 940, training loss: 6.365142345428467 = 0.050973355770111084 + 1.0 * 6.314168930053711
Epoch 940, val loss: 1.1069860458374023
Epoch 950, training loss: 6.358399391174316 = 0.049007564783096313 + 1.0 * 6.309391975402832
Epoch 950, val loss: 1.114444613456726
Epoch 960, training loss: 6.355368137359619 = 0.04714531823992729 + 1.0 * 6.308222770690918
Epoch 960, val loss: 1.121955394744873
Epoch 970, training loss: 6.351461410522461 = 0.045375365763902664 + 1.0 * 6.30608606338501
Epoch 970, val loss: 1.1294718980789185
Epoch 980, training loss: 6.352350234985352 = 0.04369042441248894 + 1.0 * 6.30866003036499
Epoch 980, val loss: 1.1369975805282593
Epoch 990, training loss: 6.348294734954834 = 0.04209311679005623 + 1.0 * 6.306201457977295
Epoch 990, val loss: 1.1443257331848145
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 10.553504943847656 = 1.9566503763198853 + 1.0 * 8.596854209899902
Epoch 0, val loss: 1.9587712287902832
Epoch 10, training loss: 10.543190002441406 = 1.946527361869812 + 1.0 * 8.596662521362305
Epoch 10, val loss: 1.9479871988296509
Epoch 20, training loss: 10.529229164123535 = 1.9341524839401245 + 1.0 * 8.595076560974121
Epoch 20, val loss: 1.9347952604293823
Epoch 30, training loss: 10.498163223266602 = 1.917130708694458 + 1.0 * 8.581032752990723
Epoch 30, val loss: 1.916839838027954
Epoch 40, training loss: 10.380176544189453 = 1.8942418098449707 + 1.0 * 8.485934257507324
Epoch 40, val loss: 1.8934695720672607
Epoch 50, training loss: 10.013406753540039 = 1.8697350025177002 + 1.0 * 8.143671989440918
Epoch 50, val loss: 1.869590401649475
Epoch 60, training loss: 9.586118698120117 = 1.84918212890625 + 1.0 * 7.736936092376709
Epoch 60, val loss: 1.8506898880004883
Epoch 70, training loss: 9.080549240112305 = 1.8344513177871704 + 1.0 * 7.246098041534424
Epoch 70, val loss: 1.8374160528182983
Epoch 80, training loss: 8.822208404541016 = 1.8208765983581543 + 1.0 * 7.001331806182861
Epoch 80, val loss: 1.8253129720687866
Epoch 90, training loss: 8.700054168701172 = 1.8041263818740845 + 1.0 * 6.895927429199219
Epoch 90, val loss: 1.8100274801254272
Epoch 100, training loss: 8.605985641479492 = 1.7869925498962402 + 1.0 * 6.818992614746094
Epoch 100, val loss: 1.7945271730422974
Epoch 110, training loss: 8.532005310058594 = 1.7722411155700684 + 1.0 * 6.759764671325684
Epoch 110, val loss: 1.781495213508606
Epoch 120, training loss: 8.466721534729004 = 1.758901596069336 + 1.0 * 6.707819938659668
Epoch 120, val loss: 1.7695690393447876
Epoch 130, training loss: 8.413945198059082 = 1.7448227405548096 + 1.0 * 6.669122695922852
Epoch 130, val loss: 1.7570230960845947
Epoch 140, training loss: 8.36357593536377 = 1.7290912866592407 + 1.0 * 6.63448429107666
Epoch 140, val loss: 1.7433265447616577
Epoch 150, training loss: 8.31694507598877 = 1.7112932205200195 + 1.0 * 6.60565185546875
Epoch 150, val loss: 1.728223443031311
Epoch 160, training loss: 8.274677276611328 = 1.6911404132843018 + 1.0 * 6.5835371017456055
Epoch 160, val loss: 1.7114132642745972
Epoch 170, training loss: 8.230798721313477 = 1.6682260036468506 + 1.0 * 6.562572479248047
Epoch 170, val loss: 1.6924470663070679
Epoch 180, training loss: 8.186145782470703 = 1.642026424407959 + 1.0 * 6.544118881225586
Epoch 180, val loss: 1.6708132028579712
Epoch 190, training loss: 8.142842292785645 = 1.6123497486114502 + 1.0 * 6.530492782592773
Epoch 190, val loss: 1.646329641342163
Epoch 200, training loss: 8.094703674316406 = 1.5793191194534302 + 1.0 * 6.515384197235107
Epoch 200, val loss: 1.6190720796585083
Epoch 210, training loss: 8.043680191040039 = 1.5424370765686035 + 1.0 * 6.5012431144714355
Epoch 210, val loss: 1.5886523723602295
Epoch 220, training loss: 7.991700172424316 = 1.5015437602996826 + 1.0 * 6.490156173706055
Epoch 220, val loss: 1.5549066066741943
Epoch 230, training loss: 7.941817283630371 = 1.4570413827896118 + 1.0 * 6.484776020050049
Epoch 230, val loss: 1.5184364318847656
Epoch 240, training loss: 7.882139205932617 = 1.4103041887283325 + 1.0 * 6.471835136413574
Epoch 240, val loss: 1.4799152612686157
Epoch 250, training loss: 7.825830936431885 = 1.3615256547927856 + 1.0 * 6.464305400848389
Epoch 250, val loss: 1.440035104751587
Epoch 260, training loss: 7.7780303955078125 = 1.3113386631011963 + 1.0 * 6.466691493988037
Epoch 260, val loss: 1.3993494510650635
Epoch 270, training loss: 7.714119911193848 = 1.261716365814209 + 1.0 * 6.452403545379639
Epoch 270, val loss: 1.3593558073043823
Epoch 280, training loss: 7.657112121582031 = 1.2127964496612549 + 1.0 * 6.444315433502197
Epoch 280, val loss: 1.3204128742218018
Epoch 290, training loss: 7.603187561035156 = 1.164576768875122 + 1.0 * 6.438610553741455
Epoch 290, val loss: 1.2824742794036865
Epoch 300, training loss: 7.5625319480896 = 1.1170753240585327 + 1.0 * 6.445456504821777
Epoch 300, val loss: 1.245534062385559
Epoch 310, training loss: 7.503192901611328 = 1.071460485458374 + 1.0 * 6.431732177734375
Epoch 310, val loss: 1.210635781288147
Epoch 320, training loss: 7.452853679656982 = 1.0273165702819824 + 1.0 * 6.425537109375
Epoch 320, val loss: 1.177619457244873
Epoch 330, training loss: 7.406435489654541 = 0.9842409491539001 + 1.0 * 6.422194480895996
Epoch 330, val loss: 1.1458475589752197
Epoch 340, training loss: 7.358219146728516 = 0.9420234560966492 + 1.0 * 6.416195869445801
Epoch 340, val loss: 1.1151570081710815
Epoch 350, training loss: 7.312564849853516 = 0.9004684090614319 + 1.0 * 6.4120965003967285
Epoch 350, val loss: 1.0854159593582153
Epoch 360, training loss: 7.276320934295654 = 0.8597503900527954 + 1.0 * 6.416570663452148
Epoch 360, val loss: 1.0567113161087036
Epoch 370, training loss: 7.227349281311035 = 0.8203839659690857 + 1.0 * 6.406965255737305
Epoch 370, val loss: 1.0294803380966187
Epoch 380, training loss: 7.1832685470581055 = 0.7820742130279541 + 1.0 * 6.4011945724487305
Epoch 380, val loss: 1.0033512115478516
Epoch 390, training loss: 7.150328636169434 = 0.744790256023407 + 1.0 * 6.405538558959961
Epoch 390, val loss: 0.9784383177757263
Epoch 400, training loss: 7.102583408355713 = 0.7089667320251465 + 1.0 * 6.393616676330566
Epoch 400, val loss: 0.9550201296806335
Epoch 410, training loss: 7.067529678344727 = 0.6745463013648987 + 1.0 * 6.392983436584473
Epoch 410, val loss: 0.9330843687057495
Epoch 420, training loss: 7.030542373657227 = 0.6416521668434143 + 1.0 * 6.388890266418457
Epoch 420, val loss: 0.9128044843673706
Epoch 430, training loss: 6.996800899505615 = 0.6104088425636292 + 1.0 * 6.386392116546631
Epoch 430, val loss: 0.8943734169006348
Epoch 440, training loss: 6.963647842407227 = 0.5807863473892212 + 1.0 * 6.382861614227295
Epoch 440, val loss: 0.8777609467506409
Epoch 450, training loss: 6.932888031005859 = 0.5525379776954651 + 1.0 * 6.380350112915039
Epoch 450, val loss: 0.8627927303314209
Epoch 460, training loss: 6.907325267791748 = 0.5257086753845215 + 1.0 * 6.381616592407227
Epoch 460, val loss: 0.8494399189949036
Epoch 470, training loss: 6.876338005065918 = 0.5002825260162354 + 1.0 * 6.376055717468262
Epoch 470, val loss: 0.8377947807312012
Epoch 480, training loss: 6.84782075881958 = 0.47605234384536743 + 1.0 * 6.371768474578857
Epoch 480, val loss: 0.8275154232978821
Epoch 490, training loss: 6.826468467712402 = 0.45278382301330566 + 1.0 * 6.373684406280518
Epoch 490, val loss: 0.8184979557991028
Epoch 500, training loss: 6.802286624908447 = 0.4305323660373688 + 1.0 * 6.371754169464111
Epoch 500, val loss: 0.8107888698577881
Epoch 510, training loss: 6.775834083557129 = 0.4093196392059326 + 1.0 * 6.366514682769775
Epoch 510, val loss: 0.804192066192627
Epoch 520, training loss: 6.752120494842529 = 0.3888968229293823 + 1.0 * 6.363223552703857
Epoch 520, val loss: 0.7985649108886719
Epoch 530, training loss: 6.745178699493408 = 0.36921218037605286 + 1.0 * 6.375966548919678
Epoch 530, val loss: 0.7938697934150696
Epoch 540, training loss: 6.714591979980469 = 0.3504539728164673 + 1.0 * 6.364138126373291
Epoch 540, val loss: 0.7899489402770996
Epoch 550, training loss: 6.69046688079834 = 0.3323846757411957 + 1.0 * 6.358082294464111
Epoch 550, val loss: 0.7867013812065125
Epoch 560, training loss: 6.669723987579346 = 0.31489118933677673 + 1.0 * 6.354832649230957
Epoch 560, val loss: 0.7840957045555115
Epoch 570, training loss: 6.661470890045166 = 0.29794982075691223 + 1.0 * 6.363521099090576
Epoch 570, val loss: 0.7820647358894348
Epoch 580, training loss: 6.634352207183838 = 0.28175267577171326 + 1.0 * 6.352599620819092
Epoch 580, val loss: 0.7804622054100037
Epoch 590, training loss: 6.615354537963867 = 0.26616376638412476 + 1.0 * 6.349190711975098
Epoch 590, val loss: 0.7792352437973022
Epoch 600, training loss: 6.598076820373535 = 0.2510931193828583 + 1.0 * 6.346983909606934
Epoch 600, val loss: 0.7785702347755432
Epoch 610, training loss: 6.589494705200195 = 0.23651331663131714 + 1.0 * 6.3529815673828125
Epoch 610, val loss: 0.7784138917922974
Epoch 620, training loss: 6.570769786834717 = 0.22253894805908203 + 1.0 * 6.348230838775635
Epoch 620, val loss: 0.7786702513694763
Epoch 630, training loss: 6.552710056304932 = 0.20914438366889954 + 1.0 * 6.343565464019775
Epoch 630, val loss: 0.7793691158294678
Epoch 640, training loss: 6.544562339782715 = 0.19639471173286438 + 1.0 * 6.348167419433594
Epoch 640, val loss: 0.7807602882385254
Epoch 650, training loss: 6.524561882019043 = 0.18437190353870392 + 1.0 * 6.3401899337768555
Epoch 650, val loss: 0.7826449871063232
Epoch 660, training loss: 6.512275695800781 = 0.17302216589450836 + 1.0 * 6.3392534255981445
Epoch 660, val loss: 0.7851059436798096
Epoch 670, training loss: 6.499848365783691 = 0.16239233314990997 + 1.0 * 6.337456226348877
Epoch 670, val loss: 0.7882412672042847
Epoch 680, training loss: 6.489460468292236 = 0.15248887240886688 + 1.0 * 6.336971759796143
Epoch 680, val loss: 0.7918524146080017
Epoch 690, training loss: 6.477464199066162 = 0.14324206113815308 + 1.0 * 6.334222316741943
Epoch 690, val loss: 0.7959796190261841
Epoch 700, training loss: 6.467746257781982 = 0.1346103399991989 + 1.0 * 6.333136081695557
Epoch 700, val loss: 0.8007153272628784
Epoch 710, training loss: 6.458506107330322 = 0.12658166885375977 + 1.0 * 6.3319244384765625
Epoch 710, val loss: 0.8059626221656799
Epoch 720, training loss: 6.452458381652832 = 0.11914203315973282 + 1.0 * 6.333316326141357
Epoch 720, val loss: 0.8113365173339844
Epoch 730, training loss: 6.440039157867432 = 0.11222033947706223 + 1.0 * 6.327818870544434
Epoch 730, val loss: 0.8171297907829285
Epoch 740, training loss: 6.433218955993652 = 0.10576213896274567 + 1.0 * 6.327456951141357
Epoch 740, val loss: 0.823318600654602
Epoch 750, training loss: 6.430586338043213 = 0.09974374622106552 + 1.0 * 6.3308424949646
Epoch 750, val loss: 0.829827606678009
Epoch 760, training loss: 6.42481803894043 = 0.09416278451681137 + 1.0 * 6.330655097961426
Epoch 760, val loss: 0.8364426493644714
Epoch 770, training loss: 6.414907932281494 = 0.08898748457431793 + 1.0 * 6.325920581817627
Epoch 770, val loss: 0.843101978302002
Epoch 780, training loss: 6.4069061279296875 = 0.08414952456951141 + 1.0 * 6.322756767272949
Epoch 780, val loss: 0.8499780893325806
Epoch 790, training loss: 6.401203155517578 = 0.07962661236524582 + 1.0 * 6.3215765953063965
Epoch 790, val loss: 0.8570605516433716
Epoch 800, training loss: 6.401268482208252 = 0.07539770752191544 + 1.0 * 6.325870990753174
Epoch 800, val loss: 0.8642971515655518
Epoch 810, training loss: 6.391505241394043 = 0.07145170122385025 + 1.0 * 6.320053577423096
Epoch 810, val loss: 0.8716087341308594
Epoch 820, training loss: 6.388949394226074 = 0.06776309013366699 + 1.0 * 6.321186542510986
Epoch 820, val loss: 0.8788954615592957
Epoch 830, training loss: 6.385331153869629 = 0.06433219462633133 + 1.0 * 6.3209991455078125
Epoch 830, val loss: 0.8863468170166016
Epoch 840, training loss: 6.378746509552002 = 0.06113866716623306 + 1.0 * 6.317607879638672
Epoch 840, val loss: 0.8936716914176941
Epoch 850, training loss: 6.372030258178711 = 0.05815556272864342 + 1.0 * 6.3138747215271
Epoch 850, val loss: 0.9009960293769836
Epoch 860, training loss: 6.368224620819092 = 0.05535850673913956 + 1.0 * 6.3128662109375
Epoch 860, val loss: 0.9084481000900269
Epoch 870, training loss: 6.381497859954834 = 0.052745334804058075 + 1.0 * 6.328752517700195
Epoch 870, val loss: 0.9160110354423523
Epoch 880, training loss: 6.364081382751465 = 0.05031052604317665 + 1.0 * 6.313770771026611
Epoch 880, val loss: 0.9231970906257629
Epoch 890, training loss: 6.359592914581299 = 0.04803936928510666 + 1.0 * 6.311553478240967
Epoch 890, val loss: 0.9302312731742859
Epoch 900, training loss: 6.358959674835205 = 0.04591098800301552 + 1.0 * 6.313048839569092
Epoch 900, val loss: 0.937477707862854
Epoch 910, training loss: 6.352445602416992 = 0.04391653463244438 + 1.0 * 6.308528900146484
Epoch 910, val loss: 0.9447607398033142
Epoch 920, training loss: 6.352322578430176 = 0.04204393923282623 + 1.0 * 6.310278415679932
Epoch 920, val loss: 0.9517413377761841
Epoch 930, training loss: 6.347341537475586 = 0.04029082879424095 + 1.0 * 6.307050704956055
Epoch 930, val loss: 0.9588313102722168
Epoch 940, training loss: 6.344311714172363 = 0.03864625468850136 + 1.0 * 6.305665493011475
Epoch 940, val loss: 0.9656593799591064
Epoch 950, training loss: 6.345043182373047 = 0.037095826119184494 + 1.0 * 6.307947158813477
Epoch 950, val loss: 0.9725201725959778
Epoch 960, training loss: 6.343608856201172 = 0.03563980758190155 + 1.0 * 6.307969093322754
Epoch 960, val loss: 0.9795577526092529
Epoch 970, training loss: 6.338949203491211 = 0.03427440673112869 + 1.0 * 6.3046746253967285
Epoch 970, val loss: 0.9860675930976868
Epoch 980, training loss: 6.335380554199219 = 0.032986246049404144 + 1.0 * 6.302394390106201
Epoch 980, val loss: 0.9925878643989563
Epoch 990, training loss: 6.334170818328857 = 0.03176410123705864 + 1.0 * 6.3024067878723145
Epoch 990, val loss: 0.9992845058441162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8181338956246705
The final CL Acc:0.76173, 0.00698, The final GNN Acc:0.81480, 0.00280
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13224])
remove edge: torch.Size([2, 7900])
updated graph: torch.Size([2, 10568])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.536417007446289 = 1.939563274383545 + 1.0 * 8.596854209899902
Epoch 0, val loss: 1.9365804195404053
Epoch 10, training loss: 10.526008605957031 = 1.9293369054794312 + 1.0 * 8.596672058105469
Epoch 10, val loss: 1.92691969871521
Epoch 20, training loss: 10.511919021606445 = 1.9167300462722778 + 1.0 * 8.595189094543457
Epoch 20, val loss: 1.9148269891738892
Epoch 30, training loss: 10.480443954467773 = 1.8992480039596558 + 1.0 * 8.581195831298828
Epoch 30, val loss: 1.8979071378707886
Epoch 40, training loss: 10.361391067504883 = 1.8756476640701294 + 1.0 * 8.485743522644043
Epoch 40, val loss: 1.875641942024231
Epoch 50, training loss: 9.899831771850586 = 1.8512684106826782 + 1.0 * 8.048563003540039
Epoch 50, val loss: 1.8534045219421387
Epoch 60, training loss: 9.434531211853027 = 1.8315342664718628 + 1.0 * 7.602996826171875
Epoch 60, val loss: 1.8357391357421875
Epoch 70, training loss: 9.012019157409668 = 1.8181530237197876 + 1.0 * 7.19386625289917
Epoch 70, val loss: 1.823534607887268
Epoch 80, training loss: 8.79818058013916 = 1.8059605360031128 + 1.0 * 6.992220401763916
Epoch 80, val loss: 1.8121517896652222
Epoch 90, training loss: 8.661251068115234 = 1.7906780242919922 + 1.0 * 6.870573043823242
Epoch 90, val loss: 1.7981963157653809
Epoch 100, training loss: 8.564005851745605 = 1.7746038436889648 + 1.0 * 6.789402008056641
Epoch 100, val loss: 1.7836889028549194
Epoch 110, training loss: 8.48415756225586 = 1.7589564323425293 + 1.0 * 6.725200653076172
Epoch 110, val loss: 1.7695711851119995
Epoch 120, training loss: 8.421674728393555 = 1.7429062128067017 + 1.0 * 6.678768634796143
Epoch 120, val loss: 1.7548487186431885
Epoch 130, training loss: 8.370182037353516 = 1.7252469062805176 + 1.0 * 6.644935607910156
Epoch 130, val loss: 1.7390527725219727
Epoch 140, training loss: 8.319378852844238 = 1.7055206298828125 + 1.0 * 6.613858222961426
Epoch 140, val loss: 1.7217258214950562
Epoch 150, training loss: 8.27350902557373 = 1.6827998161315918 + 1.0 * 6.590709209442139
Epoch 150, val loss: 1.7021541595458984
Epoch 160, training loss: 8.230708122253418 = 1.6561487913131714 + 1.0 * 6.574559211730957
Epoch 160, val loss: 1.6794590950012207
Epoch 170, training loss: 8.181249618530273 = 1.6253876686096191 + 1.0 * 6.555861473083496
Epoch 170, val loss: 1.6534944772720337
Epoch 180, training loss: 8.130449295043945 = 1.5898714065551758 + 1.0 * 6.540578365325928
Epoch 180, val loss: 1.6236768960952759
Epoch 190, training loss: 8.076059341430664 = 1.549178123474121 + 1.0 * 6.526880741119385
Epoch 190, val loss: 1.5895464420318604
Epoch 200, training loss: 8.020381927490234 = 1.5036777257919312 + 1.0 * 6.516704559326172
Epoch 200, val loss: 1.5516998767852783
Epoch 210, training loss: 7.959975242614746 = 1.4550093412399292 + 1.0 * 6.504965782165527
Epoch 210, val loss: 1.511433482170105
Epoch 220, training loss: 7.896161079406738 = 1.4028104543685913 + 1.0 * 6.493350505828857
Epoch 220, val loss: 1.4685004949569702
Epoch 230, training loss: 7.835453033447266 = 1.3475542068481445 + 1.0 * 6.487898826599121
Epoch 230, val loss: 1.423309326171875
Epoch 240, training loss: 7.7676897048950195 = 1.2918418645858765 + 1.0 * 6.4758477210998535
Epoch 240, val loss: 1.3780748844146729
Epoch 250, training loss: 7.704331874847412 = 1.2366091012954712 + 1.0 * 6.4677228927612305
Epoch 250, val loss: 1.3336997032165527
Epoch 260, training loss: 7.64285135269165 = 1.1828413009643555 + 1.0 * 6.460010051727295
Epoch 260, val loss: 1.2908375263214111
Epoch 270, training loss: 7.585041046142578 = 1.1309436559677124 + 1.0 * 6.454097270965576
Epoch 270, val loss: 1.249819278717041
Epoch 280, training loss: 7.529041767120361 = 1.0821391344070435 + 1.0 * 6.446902751922607
Epoch 280, val loss: 1.2116010189056396
Epoch 290, training loss: 7.4755401611328125 = 1.0365647077560425 + 1.0 * 6.4389753341674805
Epoch 290, val loss: 1.1762783527374268
Epoch 300, training loss: 7.438201427459717 = 0.9938785433769226 + 1.0 * 6.4443230628967285
Epoch 300, val loss: 1.1434731483459473
Epoch 310, training loss: 7.384453296661377 = 0.9543056488037109 + 1.0 * 6.430147647857666
Epoch 310, val loss: 1.1132208108901978
Epoch 320, training loss: 7.33998966217041 = 0.916972279548645 + 1.0 * 6.423017501831055
Epoch 320, val loss: 1.0847452878952026
Epoch 330, training loss: 7.29981803894043 = 0.881325900554657 + 1.0 * 6.418492317199707
Epoch 330, val loss: 1.0575371980667114
Epoch 340, training loss: 7.263908386230469 = 0.8471555113792419 + 1.0 * 6.416752815246582
Epoch 340, val loss: 1.031365156173706
Epoch 350, training loss: 7.2253570556640625 = 0.8147047162055969 + 1.0 * 6.410652160644531
Epoch 350, val loss: 1.0064802169799805
Epoch 360, training loss: 7.190046787261963 = 0.7836726307868958 + 1.0 * 6.406373977661133
Epoch 360, val loss: 0.9827666282653809
Epoch 370, training loss: 7.172304153442383 = 0.7539185285568237 + 1.0 * 6.4183855056762695
Epoch 370, val loss: 0.9602290391921997
Epoch 380, training loss: 7.130451202392578 = 0.726064920425415 + 1.0 * 6.404386043548584
Epoch 380, val loss: 0.9391891360282898
Epoch 390, training loss: 7.09702205657959 = 0.6996228098869324 + 1.0 * 6.397399425506592
Epoch 390, val loss: 0.9197040796279907
Epoch 400, training loss: 7.067624568939209 = 0.6743131279945374 + 1.0 * 6.393311500549316
Epoch 400, val loss: 0.901520311832428
Epoch 410, training loss: 7.045784950256348 = 0.6499420404434204 + 1.0 * 6.395843029022217
Epoch 410, val loss: 0.8845406770706177
Epoch 420, training loss: 7.016051769256592 = 0.6264697313308716 + 1.0 * 6.38958215713501
Epoch 420, val loss: 0.8688104152679443
Epoch 430, training loss: 6.9894843101501465 = 0.6034265160560608 + 1.0 * 6.3860578536987305
Epoch 430, val loss: 0.8540458679199219
Epoch 440, training loss: 6.9657111167907715 = 0.580575168132782 + 1.0 * 6.385136127471924
Epoch 440, val loss: 0.8399908542633057
Epoch 450, training loss: 6.938712120056152 = 0.5577719211578369 + 1.0 * 6.380939960479736
Epoch 450, val loss: 0.8265478610992432
Epoch 460, training loss: 6.917636394500732 = 0.5349711775779724 + 1.0 * 6.382665157318115
Epoch 460, val loss: 0.8136047720909119
Epoch 470, training loss: 6.887484550476074 = 0.5121943950653076 + 1.0 * 6.375290393829346
Epoch 470, val loss: 0.8012293577194214
Epoch 480, training loss: 6.868905544281006 = 0.48937469720840454 + 1.0 * 6.379530906677246
Epoch 480, val loss: 0.7892776727676392
Epoch 490, training loss: 6.84174919128418 = 0.46671193838119507 + 1.0 * 6.37503719329834
Epoch 490, val loss: 0.7780547142028809
Epoch 500, training loss: 6.8150482177734375 = 0.44431057572364807 + 1.0 * 6.370737552642822
Epoch 500, val loss: 0.7674163579940796
Epoch 510, training loss: 6.789873123168945 = 0.4220927953720093 + 1.0 * 6.3677802085876465
Epoch 510, val loss: 0.7574498653411865
Epoch 520, training loss: 6.770008563995361 = 0.4002375304698944 + 1.0 * 6.3697710037231445
Epoch 520, val loss: 0.7481640577316284
Epoch 530, training loss: 6.747186183929443 = 0.37897923588752747 + 1.0 * 6.368206977844238
Epoch 530, val loss: 0.739650547504425
Epoch 540, training loss: 6.721895217895508 = 0.35836362838745117 + 1.0 * 6.363531589508057
Epoch 540, val loss: 0.7320199608802795
Epoch 550, training loss: 6.702388286590576 = 0.338430255651474 + 1.0 * 6.36395788192749
Epoch 550, val loss: 0.7251953482627869
Epoch 560, training loss: 6.685576438903809 = 0.3193396329879761 + 1.0 * 6.366236686706543
Epoch 560, val loss: 0.7193026542663574
Epoch 570, training loss: 6.659060001373291 = 0.3011707365512848 + 1.0 * 6.357889175415039
Epoch 570, val loss: 0.7143328189849854
Epoch 580, training loss: 6.645186901092529 = 0.28382742404937744 + 1.0 * 6.361359596252441
Epoch 580, val loss: 0.7101771235466003
Epoch 590, training loss: 6.625185966491699 = 0.26732468605041504 + 1.0 * 6.357861518859863
Epoch 590, val loss: 0.7068224549293518
Epoch 600, training loss: 6.605014801025391 = 0.25160154700279236 + 1.0 * 6.353413105010986
Epoch 600, val loss: 0.7041128873825073
Epoch 610, training loss: 6.589150428771973 = 0.23659445345401764 + 1.0 * 6.352555751800537
Epoch 610, val loss: 0.7020523548126221
Epoch 620, training loss: 6.576841354370117 = 0.22236968576908112 + 1.0 * 6.354471683502197
Epoch 620, val loss: 0.7007418870925903
Epoch 630, training loss: 6.559682846069336 = 0.20894837379455566 + 1.0 * 6.350734233856201
Epoch 630, val loss: 0.699945330619812
Epoch 640, training loss: 6.543914318084717 = 0.19620880484580994 + 1.0 * 6.347705364227295
Epoch 640, val loss: 0.6997427344322205
Epoch 650, training loss: 6.530034065246582 = 0.18410895764827728 + 1.0 * 6.345925331115723
Epoch 650, val loss: 0.7002296447753906
Epoch 660, training loss: 6.519518852233887 = 0.17265017330646515 + 1.0 * 6.346868515014648
Epoch 660, val loss: 0.70133376121521
Epoch 670, training loss: 6.508670806884766 = 0.16192416846752167 + 1.0 * 6.346746444702148
Epoch 670, val loss: 0.7029807567596436
Epoch 680, training loss: 6.494839191436768 = 0.15187706053256989 + 1.0 * 6.342962265014648
Epoch 680, val loss: 0.7050693035125732
Epoch 690, training loss: 6.495112895965576 = 0.14248774945735931 + 1.0 * 6.352625370025635
Epoch 690, val loss: 0.707690417766571
Epoch 700, training loss: 6.477743625640869 = 0.1337544322013855 + 1.0 * 6.343989372253418
Epoch 700, val loss: 0.7108698487281799
Epoch 710, training loss: 6.465020656585693 = 0.1256212741136551 + 1.0 * 6.339399337768555
Epoch 710, val loss: 0.7143937945365906
Epoch 720, training loss: 6.466720104217529 = 0.1180407777428627 + 1.0 * 6.348679542541504
Epoch 720, val loss: 0.7183597087860107
Epoch 730, training loss: 6.452415943145752 = 0.11104048043489456 + 1.0 * 6.341375350952148
Epoch 730, val loss: 0.7227650880813599
Epoch 740, training loss: 6.445235729217529 = 0.10453610867261887 + 1.0 * 6.340699672698975
Epoch 740, val loss: 0.727333128452301
Epoch 750, training loss: 6.433085918426514 = 0.0985214039683342 + 1.0 * 6.334564685821533
Epoch 750, val loss: 0.732207715511322
Epoch 760, training loss: 6.427319049835205 = 0.09293033927679062 + 1.0 * 6.334388732910156
Epoch 760, val loss: 0.7373136281967163
Epoch 770, training loss: 6.419839382171631 = 0.08772334456443787 + 1.0 * 6.33211612701416
Epoch 770, val loss: 0.7426503300666809
Epoch 780, training loss: 6.424166679382324 = 0.08288197219371796 + 1.0 * 6.34128475189209
Epoch 780, val loss: 0.7482072710990906
Epoch 790, training loss: 6.412588596343994 = 0.07839285582304001 + 1.0 * 6.334195613861084
Epoch 790, val loss: 0.7539550065994263
Epoch 800, training loss: 6.405660152435303 = 0.07422769069671631 + 1.0 * 6.331432342529297
Epoch 800, val loss: 0.7596277594566345
Epoch 810, training loss: 6.398736953735352 = 0.07035671919584274 + 1.0 * 6.328380107879639
Epoch 810, val loss: 0.76566082239151
Epoch 820, training loss: 6.394980430603027 = 0.0667690634727478 + 1.0 * 6.328211307525635
Epoch 820, val loss: 0.7715445160865784
Epoch 830, training loss: 6.390069484710693 = 0.06341458112001419 + 1.0 * 6.32665491104126
Epoch 830, val loss: 0.7775219678878784
Epoch 840, training loss: 6.397844314575195 = 0.060281455516815186 + 1.0 * 6.3375630378723145
Epoch 840, val loss: 0.7837268114089966
Epoch 850, training loss: 6.387025833129883 = 0.05736759305000305 + 1.0 * 6.329658031463623
Epoch 850, val loss: 0.7899727821350098
Epoch 860, training loss: 6.3766984939575195 = 0.054643988609313965 + 1.0 * 6.322054386138916
Epoch 860, val loss: 0.7959796190261841
Epoch 870, training loss: 6.375873565673828 = 0.05209197849035263 + 1.0 * 6.323781490325928
Epoch 870, val loss: 0.8021040558815002
Epoch 880, training loss: 6.372013568878174 = 0.04970511421561241 + 1.0 * 6.322308540344238
Epoch 880, val loss: 0.8083674907684326
Epoch 890, training loss: 6.372625350952148 = 0.04747360199689865 + 1.0 * 6.3251519203186035
Epoch 890, val loss: 0.8144901990890503
Epoch 900, training loss: 6.364114761352539 = 0.04538336396217346 + 1.0 * 6.318731307983398
Epoch 900, val loss: 0.8205530047416687
Epoch 910, training loss: 6.362793922424316 = 0.043419402092695236 + 1.0 * 6.3193745613098145
Epoch 910, val loss: 0.8266183733940125
Epoch 920, training loss: 6.362644195556641 = 0.04157429188489914 + 1.0 * 6.321069717407227
Epoch 920, val loss: 0.8327195048332214
Epoch 930, training loss: 6.3582072257995605 = 0.039844609797000885 + 1.0 * 6.318362712860107
Epoch 930, val loss: 0.8388034105300903
Epoch 940, training loss: 6.353207111358643 = 0.03821735084056854 + 1.0 * 6.3149895668029785
Epoch 940, val loss: 0.8446905016899109
Epoch 950, training loss: 6.350418567657471 = 0.03668241575360298 + 1.0 * 6.3137359619140625
Epoch 950, val loss: 0.8505300283432007
Epoch 960, training loss: 6.357244968414307 = 0.03523194044828415 + 1.0 * 6.322012901306152
Epoch 960, val loss: 0.8564469218254089
Epoch 970, training loss: 6.350192546844482 = 0.033872876316308975 + 1.0 * 6.316319465637207
Epoch 970, val loss: 0.8623508214950562
Epoch 980, training loss: 6.346247673034668 = 0.032587453722953796 + 1.0 * 6.313660144805908
Epoch 980, val loss: 0.8679339289665222
Epoch 990, training loss: 6.347324848175049 = 0.03137301653623581 + 1.0 * 6.315951824188232
Epoch 990, val loss: 0.8735696077346802
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 10.541412353515625 = 1.9445778131484985 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9375689029693604
Epoch 10, training loss: 10.531205177307129 = 1.9346822500228882 + 1.0 * 8.59652328491211
Epoch 10, val loss: 1.92806875705719
Epoch 20, training loss: 10.51663589477539 = 1.9226309061050415 + 1.0 * 8.59400463104248
Epoch 20, val loss: 1.915878176689148
Epoch 30, training loss: 10.481167793273926 = 1.9062145948410034 + 1.0 * 8.574953079223633
Epoch 30, val loss: 1.89874267578125
Epoch 40, training loss: 10.34205150604248 = 1.8851162195205688 + 1.0 * 8.456934928894043
Epoch 40, val loss: 1.8770455121994019
Epoch 50, training loss: 9.842032432556152 = 1.8621498346328735 + 1.0 * 7.979882717132568
Epoch 50, val loss: 1.8538367748260498
Epoch 60, training loss: 9.315351486206055 = 1.842693567276001 + 1.0 * 7.472657680511475
Epoch 60, val loss: 1.8349194526672363
Epoch 70, training loss: 8.996506690979004 = 1.8277696371078491 + 1.0 * 7.168736934661865
Epoch 70, val loss: 1.820352554321289
Epoch 80, training loss: 8.831893920898438 = 1.811100959777832 + 1.0 * 7.020792484283447
Epoch 80, val loss: 1.8043543100357056
Epoch 90, training loss: 8.712071418762207 = 1.7933348417282104 + 1.0 * 6.918736934661865
Epoch 90, val loss: 1.7874377965927124
Epoch 100, training loss: 8.614842414855957 = 1.776455044746399 + 1.0 * 6.838387489318848
Epoch 100, val loss: 1.7722424268722534
Epoch 110, training loss: 8.530985832214355 = 1.7606478929519653 + 1.0 * 6.7703375816345215
Epoch 110, val loss: 1.7588670253753662
Epoch 120, training loss: 8.469686508178711 = 1.743756890296936 + 1.0 * 6.7259297370910645
Epoch 120, val loss: 1.744490146636963
Epoch 130, training loss: 8.409197807312012 = 1.7243348360061646 + 1.0 * 6.684863090515137
Epoch 130, val loss: 1.727758526802063
Epoch 140, training loss: 8.352261543273926 = 1.7023192644119263 + 1.0 * 6.649941921234131
Epoch 140, val loss: 1.708825945854187
Epoch 150, training loss: 8.297019958496094 = 1.6769044399261475 + 1.0 * 6.620115756988525
Epoch 150, val loss: 1.6871846914291382
Epoch 160, training loss: 8.241777420043945 = 1.6469913721084595 + 1.0 * 6.594785690307617
Epoch 160, val loss: 1.6619012355804443
Epoch 170, training loss: 8.185293197631836 = 1.6125389337539673 + 1.0 * 6.572754383087158
Epoch 170, val loss: 1.6326875686645508
Epoch 180, training loss: 8.126928329467773 = 1.5725699663162231 + 1.0 * 6.55435848236084
Epoch 180, val loss: 1.5988109111785889
Epoch 190, training loss: 8.065689086914062 = 1.5266896486282349 + 1.0 * 6.538999080657959
Epoch 190, val loss: 1.5599273443222046
Epoch 200, training loss: 8.000187873840332 = 1.4754019975662231 + 1.0 * 6.524785995483398
Epoch 200, val loss: 1.5166890621185303
Epoch 210, training loss: 7.929699897766113 = 1.4187772274017334 + 1.0 * 6.510922431945801
Epoch 210, val loss: 1.4692847728729248
Epoch 220, training loss: 7.858936309814453 = 1.3579965829849243 + 1.0 * 6.500939846038818
Epoch 220, val loss: 1.4189034700393677
Epoch 230, training loss: 7.786630630493164 = 1.2947152853012085 + 1.0 * 6.491915225982666
Epoch 230, val loss: 1.3669816255569458
Epoch 240, training loss: 7.711682319641113 = 1.2297508716583252 + 1.0 * 6.481931209564209
Epoch 240, val loss: 1.3144328594207764
Epoch 250, training loss: 7.640896797180176 = 1.1648091077804565 + 1.0 * 6.47608757019043
Epoch 250, val loss: 1.2627612352371216
Epoch 260, training loss: 7.571136474609375 = 1.1023449897766113 + 1.0 * 6.468791484832764
Epoch 260, val loss: 1.2135543823242188
Epoch 270, training loss: 7.509112358093262 = 1.0428811311721802 + 1.0 * 6.466231346130371
Epoch 270, val loss: 1.1673952341079712
Epoch 280, training loss: 7.447258472442627 = 0.9884269833564758 + 1.0 * 6.458831310272217
Epoch 280, val loss: 1.1258291006088257
Epoch 290, training loss: 7.389256477355957 = 0.9386454820632935 + 1.0 * 6.450611114501953
Epoch 290, val loss: 1.0883549451828003
Epoch 300, training loss: 7.336889266967773 = 0.8926020264625549 + 1.0 * 6.444287300109863
Epoch 300, val loss: 1.0543581247329712
Epoch 310, training loss: 7.298062324523926 = 0.8500521779060364 + 1.0 * 6.448009967803955
Epoch 310, val loss: 1.0236501693725586
Epoch 320, training loss: 7.248917579650879 = 0.8114525079727173 + 1.0 * 6.437465190887451
Epoch 320, val loss: 0.9965161085128784
Epoch 330, training loss: 7.205285549163818 = 0.7760844230651855 + 1.0 * 6.429201126098633
Epoch 330, val loss: 0.9725992679595947
Epoch 340, training loss: 7.175583839416504 = 0.7433116436004639 + 1.0 * 6.432272434234619
Epoch 340, val loss: 0.9512447714805603
Epoch 350, training loss: 7.134189128875732 = 0.7132084965705872 + 1.0 * 6.420980453491211
Epoch 350, val loss: 0.9323395490646362
Epoch 360, training loss: 7.1036553382873535 = 0.6850247383117676 + 1.0 * 6.418630599975586
Epoch 360, val loss: 0.9153491854667664
Epoch 370, training loss: 7.071986675262451 = 0.6582801938056946 + 1.0 * 6.413706302642822
Epoch 370, val loss: 0.8999409079551697
Epoch 380, training loss: 7.047932147979736 = 0.6326481699943542 + 1.0 * 6.415284156799316
Epoch 380, val loss: 0.8857342004776001
Epoch 390, training loss: 7.014247894287109 = 0.607827365398407 + 1.0 * 6.406420707702637
Epoch 390, val loss: 0.8724737763404846
Epoch 400, training loss: 6.985843658447266 = 0.5832615494728088 + 1.0 * 6.402582168579102
Epoch 400, val loss: 0.8597784638404846
Epoch 410, training loss: 6.965071201324463 = 0.5586995482444763 + 1.0 * 6.406371593475342
Epoch 410, val loss: 0.84754878282547
Epoch 420, training loss: 6.934257984161377 = 0.5342974066734314 + 1.0 * 6.399960517883301
Epoch 420, val loss: 0.8358173370361328
Epoch 430, training loss: 6.906374454498291 = 0.5099077820777893 + 1.0 * 6.3964667320251465
Epoch 430, val loss: 0.8244436383247375
Epoch 440, training loss: 6.877020835876465 = 0.4853258430957794 + 1.0 * 6.391695022583008
Epoch 440, val loss: 0.8133562207221985
Epoch 450, training loss: 6.8510966300964355 = 0.4606533944606781 + 1.0 * 6.390443325042725
Epoch 450, val loss: 0.8025743961334229
Epoch 460, training loss: 6.824370384216309 = 0.43614012002944946 + 1.0 * 6.388230323791504
Epoch 460, val loss: 0.792206346988678
Epoch 470, training loss: 6.79653787612915 = 0.41171175241470337 + 1.0 * 6.384826183319092
Epoch 470, val loss: 0.7822977304458618
Epoch 480, training loss: 6.784068584442139 = 0.38747450709342957 + 1.0 * 6.396594047546387
Epoch 480, val loss: 0.7730290293693542
Epoch 490, training loss: 6.748029708862305 = 0.3640615940093994 + 1.0 * 6.383968353271484
Epoch 490, val loss: 0.7646613717079163
Epoch 500, training loss: 6.719232559204102 = 0.34132492542266846 + 1.0 * 6.377907752990723
Epoch 500, val loss: 0.7572484016418457
Epoch 510, training loss: 6.694288730621338 = 0.31939366459846497 + 1.0 * 6.374895095825195
Epoch 510, val loss: 0.7510142922401428
Epoch 520, training loss: 6.678791522979736 = 0.29840850830078125 + 1.0 * 6.380383014678955
Epoch 520, val loss: 0.7460451722145081
Epoch 530, training loss: 6.653167247772217 = 0.2785886526107788 + 1.0 * 6.374578475952148
Epoch 530, val loss: 0.7423912286758423
Epoch 540, training loss: 6.630377292633057 = 0.26004403829574585 + 1.0 * 6.370333194732666
Epoch 540, val loss: 0.7400322556495667
Epoch 550, training loss: 6.611987590789795 = 0.24276761710643768 + 1.0 * 6.369219779968262
Epoch 550, val loss: 0.7389864325523376
Epoch 560, training loss: 6.592763423919678 = 0.22672000527381897 + 1.0 * 6.366043567657471
Epoch 560, val loss: 0.7390824556350708
Epoch 570, training loss: 6.575198650360107 = 0.21184006333351135 + 1.0 * 6.363358497619629
Epoch 570, val loss: 0.7402288913726807
Epoch 580, training loss: 6.563818454742432 = 0.19805778563022614 + 1.0 * 6.365760803222656
Epoch 580, val loss: 0.7423973679542542
Epoch 590, training loss: 6.551263332366943 = 0.18536436557769775 + 1.0 * 6.365899085998535
Epoch 590, val loss: 0.7453117370605469
Epoch 600, training loss: 6.531829833984375 = 0.17364273965358734 + 1.0 * 6.358187198638916
Epoch 600, val loss: 0.7488833069801331
Epoch 610, training loss: 6.519802093505859 = 0.16280589997768402 + 1.0 * 6.356996059417725
Epoch 610, val loss: 0.7530601024627686
Epoch 620, training loss: 6.510471820831299 = 0.15276865661144257 + 1.0 * 6.35770320892334
Epoch 620, val loss: 0.7577770948410034
Epoch 630, training loss: 6.5072431564331055 = 0.1435072422027588 + 1.0 * 6.363736152648926
Epoch 630, val loss: 0.7628633379936218
Epoch 640, training loss: 6.48861026763916 = 0.1350085437297821 + 1.0 * 6.353601932525635
Epoch 640, val loss: 0.7681617736816406
Epoch 650, training loss: 6.476867198944092 = 0.12712083756923676 + 1.0 * 6.349746227264404
Epoch 650, val loss: 0.7737216949462891
Epoch 660, training loss: 6.466794967651367 = 0.11979115754365921 + 1.0 * 6.347003936767578
Epoch 660, val loss: 0.7795952558517456
Epoch 670, training loss: 6.460905075073242 = 0.11296339333057404 + 1.0 * 6.347941875457764
Epoch 670, val loss: 0.7857210636138916
Epoch 680, training loss: 6.466092109680176 = 0.10666084289550781 + 1.0 * 6.359431266784668
Epoch 680, val loss: 0.791973888874054
Epoch 690, training loss: 6.448206901550293 = 0.10084584355354309 + 1.0 * 6.347361087799072
Epoch 690, val loss: 0.7980888485908508
Epoch 700, training loss: 6.438629150390625 = 0.09543463587760925 + 1.0 * 6.343194484710693
Epoch 700, val loss: 0.8043318390846252
Epoch 710, training loss: 6.432248592376709 = 0.0903836190700531 + 1.0 * 6.341865062713623
Epoch 710, val loss: 0.8107948303222656
Epoch 720, training loss: 6.429687976837158 = 0.08568694442510605 + 1.0 * 6.344000816345215
Epoch 720, val loss: 0.8173143863677979
Epoch 730, training loss: 6.41889762878418 = 0.08134738355875015 + 1.0 * 6.337550163269043
Epoch 730, val loss: 0.823638916015625
Epoch 740, training loss: 6.413517951965332 = 0.07729755342006683 + 1.0 * 6.3362202644348145
Epoch 740, val loss: 0.8299781680107117
Epoch 750, training loss: 6.408424377441406 = 0.07349665462970734 + 1.0 * 6.334927558898926
Epoch 750, val loss: 0.8364879488945007
Epoch 760, training loss: 6.407214164733887 = 0.06993108242750168 + 1.0 * 6.337283134460449
Epoch 760, val loss: 0.8430061340332031
Epoch 770, training loss: 6.404814720153809 = 0.06659702956676483 + 1.0 * 6.338217735290527
Epoch 770, val loss: 0.8495751023292542
Epoch 780, training loss: 6.395281791687012 = 0.06350040435791016 + 1.0 * 6.331781387329102
Epoch 780, val loss: 0.8557673692703247
Epoch 790, training loss: 6.39229679107666 = 0.06058796867728233 + 1.0 * 6.331708908081055
Epoch 790, val loss: 0.8620357513427734
Epoch 800, training loss: 6.395042419433594 = 0.05784538760781288 + 1.0 * 6.3371968269348145
Epoch 800, val loss: 0.8684382438659668
Epoch 810, training loss: 6.396728515625 = 0.05528223514556885 + 1.0 * 6.341446399688721
Epoch 810, val loss: 0.8747745752334595
Epoch 820, training loss: 6.380885601043701 = 0.05287887901067734 + 1.0 * 6.328006744384766
Epoch 820, val loss: 0.8809731006622314
Epoch 830, training loss: 6.37717866897583 = 0.05062073469161987 + 1.0 * 6.3265581130981445
Epoch 830, val loss: 0.8871080875396729
Epoch 840, training loss: 6.373822212219238 = 0.04848577454686165 + 1.0 * 6.325336456298828
Epoch 840, val loss: 0.8933437466621399
Epoch 850, training loss: 6.375498294830322 = 0.04647186025977135 + 1.0 * 6.329026222229004
Epoch 850, val loss: 0.8995585441589355
Epoch 860, training loss: 6.375083923339844 = 0.044569309800863266 + 1.0 * 6.330514430999756
Epoch 860, val loss: 0.9057484865188599
Epoch 870, training loss: 6.368861675262451 = 0.04279356822371483 + 1.0 * 6.326067924499512
Epoch 870, val loss: 0.9116939306259155
Epoch 880, training loss: 6.362836837768555 = 0.0411079116165638 + 1.0 * 6.321728706359863
Epoch 880, val loss: 0.9176278114318848
Epoch 890, training loss: 6.361294269561768 = 0.039516571909189224 + 1.0 * 6.321777820587158
Epoch 890, val loss: 0.9235659837722778
Epoch 900, training loss: 6.361477375030518 = 0.03800951689481735 + 1.0 * 6.32346773147583
Epoch 900, val loss: 0.9295265674591064
Epoch 910, training loss: 6.357089519500732 = 0.03659128025174141 + 1.0 * 6.320498466491699
Epoch 910, val loss: 0.9353029131889343
Epoch 920, training loss: 6.352747917175293 = 0.03524663671851158 + 1.0 * 6.317501068115234
Epoch 920, val loss: 0.9410036206245422
Epoch 930, training loss: 6.356184959411621 = 0.03396688029170036 + 1.0 * 6.32221794128418
Epoch 930, val loss: 0.94679856300354
Epoch 940, training loss: 6.35126256942749 = 0.03276040032505989 + 1.0 * 6.318501949310303
Epoch 940, val loss: 0.9526026844978333
Epoch 950, training loss: 6.355910778045654 = 0.03161568194627762 + 1.0 * 6.3242950439453125
Epoch 950, val loss: 0.958104133605957
Epoch 960, training loss: 6.346967697143555 = 0.03053305111825466 + 1.0 * 6.316434860229492
Epoch 960, val loss: 0.9636043906211853
Epoch 970, training loss: 6.343331813812256 = 0.029504070058465004 + 1.0 * 6.3138275146484375
Epoch 970, val loss: 0.9690617322921753
Epoch 980, training loss: 6.343637466430664 = 0.028521636500954628 + 1.0 * 6.315115928649902
Epoch 980, val loss: 0.9745078682899475
Epoch 990, training loss: 6.341261386871338 = 0.027585873380303383 + 1.0 * 6.313675403594971
Epoch 990, val loss: 0.979957640171051
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 10.55974006652832 = 1.962917447090149 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.9626376628875732
Epoch 10, training loss: 10.548919677734375 = 1.9524016380310059 + 1.0 * 8.596517562866211
Epoch 10, val loss: 1.9526909589767456
Epoch 20, training loss: 10.533285140991211 = 1.9394454956054688 + 1.0 * 8.593839645385742
Epoch 20, val loss: 1.939981460571289
Epoch 30, training loss: 10.493973731994629 = 1.9213058948516846 + 1.0 * 8.572668075561523
Epoch 30, val loss: 1.9217476844787598
Epoch 40, training loss: 10.340956687927246 = 1.8977118730545044 + 1.0 * 8.443244934082031
Epoch 40, val loss: 1.8986082077026367
Epoch 50, training loss: 9.922473907470703 = 1.8723526000976562 + 1.0 * 8.050121307373047
Epoch 50, val loss: 1.8747174739837646
Epoch 60, training loss: 9.44864273071289 = 1.850417137145996 + 1.0 * 7.598226070404053
Epoch 60, val loss: 1.8538848161697388
Epoch 70, training loss: 9.02275276184082 = 1.8336396217346191 + 1.0 * 7.189113140106201
Epoch 70, val loss: 1.837721347808838
Epoch 80, training loss: 8.85198974609375 = 1.81479811668396 + 1.0 * 7.037191390991211
Epoch 80, val loss: 1.8197922706604004
Epoch 90, training loss: 8.748034477233887 = 1.7914687395095825 + 1.0 * 6.9565653800964355
Epoch 90, val loss: 1.798994779586792
Epoch 100, training loss: 8.640848159790039 = 1.770136833190918 + 1.0 * 6.870711326599121
Epoch 100, val loss: 1.7805334329605103
Epoch 110, training loss: 8.545284271240234 = 1.7521499395370483 + 1.0 * 6.793134689331055
Epoch 110, val loss: 1.764724612236023
Epoch 120, training loss: 8.472744941711426 = 1.734357237815857 + 1.0 * 6.7383880615234375
Epoch 120, val loss: 1.748610019683838
Epoch 130, training loss: 8.40564250946045 = 1.7141820192337036 + 1.0 * 6.691460132598877
Epoch 130, val loss: 1.730948805809021
Epoch 140, training loss: 8.347834587097168 = 1.6911171674728394 + 1.0 * 6.656717300415039
Epoch 140, val loss: 1.7115975618362427
Epoch 150, training loss: 8.289322853088379 = 1.6649177074432373 + 1.0 * 6.624405384063721
Epoch 150, val loss: 1.6898571252822876
Epoch 160, training loss: 8.234429359436035 = 1.6343467235565186 + 1.0 * 6.600082874298096
Epoch 160, val loss: 1.6646151542663574
Epoch 170, training loss: 8.179096221923828 = 1.5984195470809937 + 1.0 * 6.580676555633545
Epoch 170, val loss: 1.6350785493850708
Epoch 180, training loss: 8.117778778076172 = 1.5573519468307495 + 1.0 * 6.560427188873291
Epoch 180, val loss: 1.6013582944869995
Epoch 190, training loss: 8.055408477783203 = 1.5111345052719116 + 1.0 * 6.544273853302002
Epoch 190, val loss: 1.563387155532837
Epoch 200, training loss: 7.9907050132751465 = 1.4597463607788086 + 1.0 * 6.530958652496338
Epoch 200, val loss: 1.5211693048477173
Epoch 210, training loss: 7.9223456382751465 = 1.4043058156967163 + 1.0 * 6.518039703369141
Epoch 210, val loss: 1.4757686853408813
Epoch 220, training loss: 7.854727268218994 = 1.3460536003112793 + 1.0 * 6.508673667907715
Epoch 220, val loss: 1.4283835887908936
Epoch 230, training loss: 7.785195350646973 = 1.2875566482543945 + 1.0 * 6.497638702392578
Epoch 230, val loss: 1.381234884262085
Epoch 240, training loss: 7.719179153442383 = 1.2300559282302856 + 1.0 * 6.489123344421387
Epoch 240, val loss: 1.3353698253631592
Epoch 250, training loss: 7.657073974609375 = 1.1750173568725586 + 1.0 * 6.482056617736816
Epoch 250, val loss: 1.2920218706130981
Epoch 260, training loss: 7.596056938171387 = 1.1231515407562256 + 1.0 * 6.47290563583374
Epoch 260, val loss: 1.2519383430480957
Epoch 270, training loss: 7.5445685386657715 = 1.0745993852615356 + 1.0 * 6.469969272613525
Epoch 270, val loss: 1.2153637409210205
Epoch 280, training loss: 7.490926742553711 = 1.029968023300171 + 1.0 * 6.460958957672119
Epoch 280, val loss: 1.182592749595642
Epoch 290, training loss: 7.442444801330566 = 0.9887230396270752 + 1.0 * 6.45372200012207
Epoch 290, val loss: 1.153273344039917
Epoch 300, training loss: 7.399327278137207 = 0.9501078724861145 + 1.0 * 6.449219226837158
Epoch 300, val loss: 1.1267393827438354
Epoch 310, training loss: 7.356342315673828 = 0.9138413071632385 + 1.0 * 6.442501068115234
Epoch 310, val loss: 1.102365493774414
Epoch 320, training loss: 7.315616607666016 = 0.8790662288665771 + 1.0 * 6.436550617218018
Epoch 320, val loss: 1.0797189474105835
Epoch 330, training loss: 7.275193691253662 = 0.8454020619392395 + 1.0 * 6.429791450500488
Epoch 330, val loss: 1.0581517219543457
Epoch 340, training loss: 7.238759994506836 = 0.8124902248382568 + 1.0 * 6.426270008087158
Epoch 340, val loss: 1.0374270677566528
Epoch 350, training loss: 7.20817232131958 = 0.7797648310661316 + 1.0 * 6.428407669067383
Epoch 350, val loss: 1.017068862915039
Epoch 360, training loss: 7.1651458740234375 = 0.74733567237854 + 1.0 * 6.417810440063477
Epoch 360, val loss: 0.9969700574874878
Epoch 370, training loss: 7.130658149719238 = 0.7151287794113159 + 1.0 * 6.415529251098633
Epoch 370, val loss: 0.9774295091629028
Epoch 380, training loss: 7.09438419342041 = 0.6834792494773865 + 1.0 * 6.410904884338379
Epoch 380, val loss: 0.9585053324699402
Epoch 390, training loss: 7.058581352233887 = 0.6522014141082764 + 1.0 * 6.406379699707031
Epoch 390, val loss: 0.9402292966842651
Epoch 400, training loss: 7.023903846740723 = 0.6213647723197937 + 1.0 * 6.402539253234863
Epoch 400, val loss: 0.9227097034454346
Epoch 410, training loss: 6.995748996734619 = 0.5915083885192871 + 1.0 * 6.404240608215332
Epoch 410, val loss: 0.9064217209815979
Epoch 420, training loss: 6.963024139404297 = 0.5632480978965759 + 1.0 * 6.399775981903076
Epoch 420, val loss: 0.8918455839157104
Epoch 430, training loss: 6.930778503417969 = 0.5363858938217163 + 1.0 * 6.394392490386963
Epoch 430, val loss: 0.8789770007133484
Epoch 440, training loss: 6.907856464385986 = 0.5111268758773804 + 1.0 * 6.396729469299316
Epoch 440, val loss: 0.8676512837409973
Epoch 450, training loss: 6.8822479248046875 = 0.4875166118144989 + 1.0 * 6.394731521606445
Epoch 450, val loss: 0.8581407070159912
Epoch 460, training loss: 6.852897644042969 = 0.46551966667175293 + 1.0 * 6.387377738952637
Epoch 460, val loss: 0.8502089381217957
Epoch 470, training loss: 6.83033561706543 = 0.44473525881767273 + 1.0 * 6.385600566864014
Epoch 470, val loss: 0.8435105085372925
Epoch 480, training loss: 6.810851573944092 = 0.4252112805843353 + 1.0 * 6.3856401443481445
Epoch 480, val loss: 0.8378003239631653
Epoch 490, training loss: 6.78470516204834 = 0.4067317247390747 + 1.0 * 6.377973556518555
Epoch 490, val loss: 0.8331060409545898
Epoch 500, training loss: 6.777197360992432 = 0.3890925645828247 + 1.0 * 6.3881049156188965
Epoch 500, val loss: 0.829075813293457
Epoch 510, training loss: 6.748985767364502 = 0.3721705973148346 + 1.0 * 6.376815319061279
Epoch 510, val loss: 0.8257652521133423
Epoch 520, training loss: 6.72764778137207 = 0.3559434115886688 + 1.0 * 6.371704578399658
Epoch 520, val loss: 0.8231574892997742
Epoch 530, training loss: 6.709172248840332 = 0.3401804566383362 + 1.0 * 6.368991851806641
Epoch 530, val loss: 0.8209230303764343
Epoch 540, training loss: 6.695463180541992 = 0.32483938336372375 + 1.0 * 6.370623588562012
Epoch 540, val loss: 0.8191297650337219
Epoch 550, training loss: 6.678158283233643 = 0.310034841299057 + 1.0 * 6.368123531341553
Epoch 550, val loss: 0.8179119825363159
Epoch 560, training loss: 6.658304691314697 = 0.295560359954834 + 1.0 * 6.362744331359863
Epoch 560, val loss: 0.8172079920768738
Epoch 570, training loss: 6.661324501037598 = 0.281391978263855 + 1.0 * 6.379932403564453
Epoch 570, val loss: 0.816785991191864
Epoch 580, training loss: 6.627038955688477 = 0.2677762508392334 + 1.0 * 6.359262943267822
Epoch 580, val loss: 0.8169317245483398
Epoch 590, training loss: 6.61383056640625 = 0.2545281648635864 + 1.0 * 6.359302520751953
Epoch 590, val loss: 0.8177457451820374
Epoch 600, training loss: 6.5981621742248535 = 0.24162746965885162 + 1.0 * 6.356534481048584
Epoch 600, val loss: 0.818862795829773
Epoch 610, training loss: 6.587245464324951 = 0.2291874885559082 + 1.0 * 6.358057975769043
Epoch 610, val loss: 0.8203813433647156
Epoch 620, training loss: 6.574642181396484 = 0.2173575907945633 + 1.0 * 6.3572845458984375
Epoch 620, val loss: 0.8225536346435547
Epoch 630, training loss: 6.557624340057373 = 0.2060573846101761 + 1.0 * 6.351566791534424
Epoch 630, val loss: 0.8252102732658386
Epoch 640, training loss: 6.544601917266846 = 0.19527079164981842 + 1.0 * 6.349330902099609
Epoch 640, val loss: 0.8282193541526794
Epoch 650, training loss: 6.545602798461914 = 0.18501274287700653 + 1.0 * 6.360589981079102
Epoch 650, val loss: 0.8315924406051636
Epoch 660, training loss: 6.527906894683838 = 0.17532165348529816 + 1.0 * 6.352585315704346
Epoch 660, val loss: 0.8352762460708618
Epoch 670, training loss: 6.51289176940918 = 0.1662270724773407 + 1.0 * 6.346664905548096
Epoch 670, val loss: 0.8394477367401123
Epoch 680, training loss: 6.502076625823975 = 0.15761302411556244 + 1.0 * 6.34446382522583
Epoch 680, val loss: 0.8439167141914368
Epoch 690, training loss: 6.49699592590332 = 0.14946970343589783 + 1.0 * 6.3475260734558105
Epoch 690, val loss: 0.8486093282699585
Epoch 700, training loss: 6.48604154586792 = 0.1418272852897644 + 1.0 * 6.34421443939209
Epoch 700, val loss: 0.85364830493927
Epoch 710, training loss: 6.4770827293396 = 0.13462737202644348 + 1.0 * 6.3424553871154785
Epoch 710, val loss: 0.8588122129440308
Epoch 720, training loss: 6.468297004699707 = 0.12787136435508728 + 1.0 * 6.340425491333008
Epoch 720, val loss: 0.8643468022346497
Epoch 730, training loss: 6.459242343902588 = 0.12149316072463989 + 1.0 * 6.337749004364014
Epoch 730, val loss: 0.8701097369194031
Epoch 740, training loss: 6.464997291564941 = 0.11547347158193588 + 1.0 * 6.349524021148682
Epoch 740, val loss: 0.8759962320327759
Epoch 750, training loss: 6.447665691375732 = 0.10986778140068054 + 1.0 * 6.337798118591309
Epoch 750, val loss: 0.8819618821144104
Epoch 760, training loss: 6.438150882720947 = 0.10455732047557831 + 1.0 * 6.333593368530273
Epoch 760, val loss: 0.8882310390472412
Epoch 770, training loss: 6.4368767738342285 = 0.09955433011054993 + 1.0 * 6.337322235107422
Epoch 770, val loss: 0.8945478796958923
Epoch 780, training loss: 6.43187141418457 = 0.09487386047840118 + 1.0 * 6.3369975090026855
Epoch 780, val loss: 0.9008838534355164
Epoch 790, training loss: 6.419953346252441 = 0.0904800146818161 + 1.0 * 6.329473495483398
Epoch 790, val loss: 0.9074500799179077
Epoch 800, training loss: 6.415873050689697 = 0.08632895350456238 + 1.0 * 6.3295440673828125
Epoch 800, val loss: 0.9140729904174805
Epoch 810, training loss: 6.412161350250244 = 0.08241212368011475 + 1.0 * 6.32974910736084
Epoch 810, val loss: 0.9207390546798706
Epoch 820, training loss: 6.4080328941345215 = 0.07874659448862076 + 1.0 * 6.329286098480225
Epoch 820, val loss: 0.9273059964179993
Epoch 830, training loss: 6.403618812561035 = 0.07529912143945694 + 1.0 * 6.328319549560547
Epoch 830, val loss: 0.9340581297874451
Epoch 840, training loss: 6.396538734436035 = 0.07205553352832794 + 1.0 * 6.324483394622803
Epoch 840, val loss: 0.9408214688301086
Epoch 850, training loss: 6.4011149406433105 = 0.06898456066846848 + 1.0 * 6.332130432128906
Epoch 850, val loss: 0.9475418329238892
Epoch 860, training loss: 6.3910932540893555 = 0.0661027580499649 + 1.0 * 6.324990272521973
Epoch 860, val loss: 0.954123854637146
Epoch 870, training loss: 6.386963367462158 = 0.06336905807256699 + 1.0 * 6.323594093322754
Epoch 870, val loss: 0.9609293341636658
Epoch 880, training loss: 6.392122268676758 = 0.06079740449786186 + 1.0 * 6.331325054168701
Epoch 880, val loss: 0.9675104022026062
Epoch 890, training loss: 6.380015850067139 = 0.05835491791367531 + 1.0 * 6.321660995483398
Epoch 890, val loss: 0.9741808772087097
Epoch 900, training loss: 6.37468957901001 = 0.056051865220069885 + 1.0 * 6.318637847900391
Epoch 900, val loss: 0.9808390140533447
Epoch 910, training loss: 6.371967315673828 = 0.05385801941156387 + 1.0 * 6.318109512329102
Epoch 910, val loss: 0.9874480366706848
Epoch 920, training loss: 6.382872581481934 = 0.05178249627351761 + 1.0 * 6.331089973449707
Epoch 920, val loss: 0.9939526319503784
Epoch 930, training loss: 6.3675432205200195 = 0.04981805011630058 + 1.0 * 6.31772518157959
Epoch 930, val loss: 1.0003764629364014
Epoch 940, training loss: 6.3674139976501465 = 0.0479595847427845 + 1.0 * 6.319454193115234
Epoch 940, val loss: 1.0068473815917969
Epoch 950, training loss: 6.360625743865967 = 0.04619864746928215 + 1.0 * 6.314426898956299
Epoch 950, val loss: 1.0131878852844238
Epoch 960, training loss: 6.360313415527344 = 0.044524550437927246 + 1.0 * 6.315788745880127
Epoch 960, val loss: 1.0195287466049194
Epoch 970, training loss: 6.360174179077148 = 0.0429290309548378 + 1.0 * 6.317245006561279
Epoch 970, val loss: 1.025760531425476
Epoch 980, training loss: 6.355332851409912 = 0.04141859710216522 + 1.0 * 6.3139142990112305
Epoch 980, val loss: 1.0318653583526611
Epoch 990, training loss: 6.353172779083252 = 0.03998468071222305 + 1.0 * 6.313188076019287
Epoch 990, val loss: 1.0380182266235352
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8350026357406432
The final CL Acc:0.80494, 0.01666, The final GNN Acc:0.83711, 0.00172
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10546])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.551141738891602 = 1.9542983770370483 + 1.0 * 8.596843719482422
Epoch 0, val loss: 1.9594192504882812
Epoch 10, training loss: 10.54127311706543 = 1.9446712732315063 + 1.0 * 8.596601486206055
Epoch 10, val loss: 1.9501245021820068
Epoch 20, training loss: 10.527081489562988 = 1.9324878454208374 + 1.0 * 8.59459400177002
Epoch 20, val loss: 1.9378730058670044
Epoch 30, training loss: 10.494461059570312 = 1.9150097370147705 + 1.0 * 8.579451560974121
Epoch 30, val loss: 1.9199175834655762
Epoch 40, training loss: 10.39348316192627 = 1.8911620378494263 + 1.0 * 8.502321243286133
Epoch 40, val loss: 1.8960838317871094
Epoch 50, training loss: 10.038135528564453 = 1.864506483078003 + 1.0 * 8.173628807067871
Epoch 50, val loss: 1.8698251247406006
Epoch 60, training loss: 9.673213958740234 = 1.8397666215896606 + 1.0 * 7.833446979522705
Epoch 60, val loss: 1.8467391729354858
Epoch 70, training loss: 9.215877532958984 = 1.8224036693572998 + 1.0 * 7.3934736251831055
Epoch 70, val loss: 1.831027626991272
Epoch 80, training loss: 8.98695182800293 = 1.8084869384765625 + 1.0 * 7.178464889526367
Epoch 80, val loss: 1.8177348375320435
Epoch 90, training loss: 8.806394577026367 = 1.7939674854278564 + 1.0 * 7.012426853179932
Epoch 90, val loss: 1.8036394119262695
Epoch 100, training loss: 8.685009002685547 = 1.780409336090088 + 1.0 * 6.904600143432617
Epoch 100, val loss: 1.7901192903518677
Epoch 110, training loss: 8.595512390136719 = 1.7672394514083862 + 1.0 * 6.828273296356201
Epoch 110, val loss: 1.7768466472625732
Epoch 120, training loss: 8.517220497131348 = 1.7539863586425781 + 1.0 * 6.7632341384887695
Epoch 120, val loss: 1.7638055086135864
Epoch 130, training loss: 8.449661254882812 = 1.7394840717315674 + 1.0 * 6.710176944732666
Epoch 130, val loss: 1.7502574920654297
Epoch 140, training loss: 8.387886047363281 = 1.7228889465332031 + 1.0 * 6.664997100830078
Epoch 140, val loss: 1.7356441020965576
Epoch 150, training loss: 8.336250305175781 = 1.7033482789993286 + 1.0 * 6.632901668548584
Epoch 150, val loss: 1.7189555168151855
Epoch 160, training loss: 8.291152954101562 = 1.6804903745651245 + 1.0 * 6.610662937164307
Epoch 160, val loss: 1.6998082399368286
Epoch 170, training loss: 8.241406440734863 = 1.6545504331588745 + 1.0 * 6.586856365203857
Epoch 170, val loss: 1.6781283617019653
Epoch 180, training loss: 8.191164016723633 = 1.6247442960739136 + 1.0 * 6.56641960144043
Epoch 180, val loss: 1.653366208076477
Epoch 190, training loss: 8.139607429504395 = 1.590331792831421 + 1.0 * 6.5492753982543945
Epoch 190, val loss: 1.6248942613601685
Epoch 200, training loss: 8.09038257598877 = 1.5511597394943237 + 1.0 * 6.5392231941223145
Epoch 200, val loss: 1.5927081108093262
Epoch 210, training loss: 8.031970977783203 = 1.5094643831253052 + 1.0 * 6.522506237030029
Epoch 210, val loss: 1.5592378377914429
Epoch 220, training loss: 7.974976539611816 = 1.4660000801086426 + 1.0 * 6.508976459503174
Epoch 220, val loss: 1.5249576568603516
Epoch 230, training loss: 7.919511318206787 = 1.4214414358139038 + 1.0 * 6.498069763183594
Epoch 230, val loss: 1.4907139539718628
Epoch 240, training loss: 7.865987777709961 = 1.3773071765899658 + 1.0 * 6.488680839538574
Epoch 240, val loss: 1.4579370021820068
Epoch 250, training loss: 7.813007354736328 = 1.3343405723571777 + 1.0 * 6.47866678237915
Epoch 250, val loss: 1.4267420768737793
Epoch 260, training loss: 7.762389183044434 = 1.2926585674285889 + 1.0 * 6.469730854034424
Epoch 260, val loss: 1.3975484371185303
Epoch 270, training loss: 7.714752197265625 = 1.2527450323104858 + 1.0 * 6.46200704574585
Epoch 270, val loss: 1.37024986743927
Epoch 280, training loss: 7.668854713439941 = 1.2140129804611206 + 1.0 * 6.454841613769531
Epoch 280, val loss: 1.3443657159805298
Epoch 290, training loss: 7.625175476074219 = 1.1763980388641357 + 1.0 * 6.448777198791504
Epoch 290, val loss: 1.3195754289627075
Epoch 300, training loss: 7.582080364227295 = 1.1401381492614746 + 1.0 * 6.44194221496582
Epoch 300, val loss: 1.295825719833374
Epoch 310, training loss: 7.545829772949219 = 1.104722261428833 + 1.0 * 6.441107749938965
Epoch 310, val loss: 1.2726315259933472
Epoch 320, training loss: 7.501481056213379 = 1.0702482461929321 + 1.0 * 6.431232929229736
Epoch 320, val loss: 1.2499138116836548
Epoch 330, training loss: 7.462833881378174 = 1.0366007089614868 + 1.0 * 6.426233291625977
Epoch 330, val loss: 1.2276030778884888
Epoch 340, training loss: 7.427951812744141 = 1.0035474300384521 + 1.0 * 6.424404144287109
Epoch 340, val loss: 1.2054387331008911
Epoch 350, training loss: 7.390581130981445 = 0.9711894989013672 + 1.0 * 6.419391632080078
Epoch 350, val loss: 1.1835049390792847
Epoch 360, training loss: 7.3531951904296875 = 0.9394711256027222 + 1.0 * 6.413723945617676
Epoch 360, val loss: 1.161616325378418
Epoch 370, training loss: 7.319554328918457 = 0.9080246090888977 + 1.0 * 6.411529541015625
Epoch 370, val loss: 1.1395926475524902
Epoch 380, training loss: 7.2849297523498535 = 0.8768139481544495 + 1.0 * 6.408115863800049
Epoch 380, val loss: 1.1174063682556152
Epoch 390, training loss: 7.250309467315674 = 0.8457658886909485 + 1.0 * 6.404543399810791
Epoch 390, val loss: 1.0951465368270874
Epoch 400, training loss: 7.214910984039307 = 0.8144936561584473 + 1.0 * 6.400417327880859
Epoch 400, val loss: 1.0724250078201294
Epoch 410, training loss: 7.189760208129883 = 0.7829116582870483 + 1.0 * 6.406848430633545
Epoch 410, val loss: 1.0493378639221191
Epoch 420, training loss: 7.148664951324463 = 0.7517479658126831 + 1.0 * 6.39691686630249
Epoch 420, val loss: 1.0263638496398926
Epoch 430, training loss: 7.1139068603515625 = 0.7207639813423157 + 1.0 * 6.3931427001953125
Epoch 430, val loss: 1.0038894414901733
Epoch 440, training loss: 7.079651832580566 = 0.6901864409446716 + 1.0 * 6.38946533203125
Epoch 440, val loss: 0.9818949103355408
Epoch 450, training loss: 7.062575340270996 = 0.6603288054466248 + 1.0 * 6.402246475219727
Epoch 450, val loss: 0.960995614528656
Epoch 460, training loss: 7.02129602432251 = 0.6318963170051575 + 1.0 * 6.389399528503418
Epoch 460, val loss: 0.9417738318443298
Epoch 470, training loss: 6.9934282302856445 = 0.6048253774642944 + 1.0 * 6.3886027336120605
Epoch 470, val loss: 0.9244107007980347
Epoch 480, training loss: 6.9606499671936035 = 0.5793639421463013 + 1.0 * 6.381286144256592
Epoch 480, val loss: 0.9089189171791077
Epoch 490, training loss: 6.932991981506348 = 0.555208146572113 + 1.0 * 6.37778377532959
Epoch 490, val loss: 0.895439088344574
Epoch 500, training loss: 6.911834239959717 = 0.5323075652122498 + 1.0 * 6.379526615142822
Epoch 500, val loss: 0.8836875557899475
Epoch 510, training loss: 6.895415782928467 = 0.5107037425041199 + 1.0 * 6.384712219238281
Epoch 510, val loss: 0.8736420273780823
Epoch 520, training loss: 6.861711502075195 = 0.49022191762924194 + 1.0 * 6.371489524841309
Epoch 520, val loss: 0.8651503324508667
Epoch 530, training loss: 6.8393988609313965 = 0.4707070589065552 + 1.0 * 6.368691921234131
Epoch 530, val loss: 0.8580073118209839
Epoch 540, training loss: 6.82169771194458 = 0.4518575370311737 + 1.0 * 6.369840145111084
Epoch 540, val loss: 0.8518573045730591
Epoch 550, training loss: 6.799073696136475 = 0.43358567357063293 + 1.0 * 6.365488052368164
Epoch 550, val loss: 0.846550464630127
Epoch 560, training loss: 6.781979084014893 = 0.4158494770526886 + 1.0 * 6.366129398345947
Epoch 560, val loss: 0.8421075940132141
Epoch 570, training loss: 6.7613444328308105 = 0.3984554708003998 + 1.0 * 6.362888813018799
Epoch 570, val loss: 0.8382190465927124
Epoch 580, training loss: 6.74646520614624 = 0.3813667893409729 + 1.0 * 6.365098476409912
Epoch 580, val loss: 0.834814727306366
Epoch 590, training loss: 6.723902702331543 = 0.36448612809181213 + 1.0 * 6.359416484832764
Epoch 590, val loss: 0.8318266272544861
Epoch 600, training loss: 6.705796718597412 = 0.34778910875320435 + 1.0 * 6.358007431030273
Epoch 600, val loss: 0.829169511795044
Epoch 610, training loss: 6.6984758377075195 = 0.3312584459781647 + 1.0 * 6.367217540740967
Epoch 610, val loss: 0.8268154859542847
Epoch 620, training loss: 6.669715404510498 = 0.31505557894706726 + 1.0 * 6.3546600341796875
Epoch 620, val loss: 0.8248492479324341
Epoch 630, training loss: 6.659736156463623 = 0.29907742142677307 + 1.0 * 6.360658645629883
Epoch 630, val loss: 0.8232173919677734
Epoch 640, training loss: 6.6348958015441895 = 0.2834542393684387 + 1.0 * 6.351441383361816
Epoch 640, val loss: 0.8218711018562317
Epoch 650, training loss: 6.617172718048096 = 0.26813584566116333 + 1.0 * 6.349036693572998
Epoch 650, val loss: 0.8208626508712769
Epoch 660, training loss: 6.60331916809082 = 0.25320151448249817 + 1.0 * 6.3501176834106445
Epoch 660, val loss: 0.8202725648880005
Epoch 670, training loss: 6.59061336517334 = 0.23878620564937592 + 1.0 * 6.351827144622803
Epoch 670, val loss: 0.8201372623443604
Epoch 680, training loss: 6.571822166442871 = 0.22495810687541962 + 1.0 * 6.346864223480225
Epoch 680, val loss: 0.8204970955848694
Epoch 690, training loss: 6.562051773071289 = 0.21174876391887665 + 1.0 * 6.3503031730651855
Epoch 690, val loss: 0.8213688135147095
Epoch 700, training loss: 6.54307222366333 = 0.19921056926250458 + 1.0 * 6.3438615798950195
Epoch 700, val loss: 0.822657585144043
Epoch 710, training loss: 6.527991771697998 = 0.1873341053724289 + 1.0 * 6.340657711029053
Epoch 710, val loss: 0.8245234489440918
Epoch 720, training loss: 6.517261981964111 = 0.1760810911655426 + 1.0 * 6.341180801391602
Epoch 720, val loss: 0.8268514275550842
Epoch 730, training loss: 6.51041316986084 = 0.16550475358963013 + 1.0 * 6.344908237457275
Epoch 730, val loss: 0.8295131921768188
Epoch 740, training loss: 6.4941487312316895 = 0.15564580261707306 + 1.0 * 6.338502883911133
Epoch 740, val loss: 0.8326228857040405
Epoch 750, training loss: 6.482464790344238 = 0.14641988277435303 + 1.0 * 6.336044788360596
Epoch 750, val loss: 0.836168646812439
Epoch 760, training loss: 6.4730000495910645 = 0.13776472210884094 + 1.0 * 6.335235118865967
Epoch 760, val loss: 0.8400029540061951
Epoch 770, training loss: 6.473968982696533 = 0.12967516481876373 + 1.0 * 6.344293594360352
Epoch 770, val loss: 0.8441885709762573
Epoch 780, training loss: 6.456363677978516 = 0.12212419509887695 + 1.0 * 6.334239482879639
Epoch 780, val loss: 0.8485736846923828
Epoch 790, training loss: 6.447048664093018 = 0.11510448157787323 + 1.0 * 6.331943988800049
Epoch 790, val loss: 0.8532937169075012
Epoch 800, training loss: 6.443158149719238 = 0.10853411257266998 + 1.0 * 6.33462381362915
Epoch 800, val loss: 0.8581923246383667
Epoch 810, training loss: 6.435731410980225 = 0.1024056151509285 + 1.0 * 6.3333258628845215
Epoch 810, val loss: 0.8632287979125977
Epoch 820, training loss: 6.4266037940979 = 0.09672562032938004 + 1.0 * 6.329878330230713
Epoch 820, val loss: 0.8684948682785034
Epoch 830, training loss: 6.419111251831055 = 0.09140939265489578 + 1.0 * 6.327702045440674
Epoch 830, val loss: 0.8738369345664978
Epoch 840, training loss: 6.424192428588867 = 0.08644504845142365 + 1.0 * 6.337747573852539
Epoch 840, val loss: 0.8791703581809998
Epoch 850, training loss: 6.4069976806640625 = 0.08185158669948578 + 1.0 * 6.325146198272705
Epoch 850, val loss: 0.8847099542617798
Epoch 860, training loss: 6.401309013366699 = 0.07755309343338013 + 1.0 * 6.323755741119385
Epoch 860, val loss: 0.8903290629386902
Epoch 870, training loss: 6.396598815917969 = 0.07352519035339355 + 1.0 * 6.323073863983154
Epoch 870, val loss: 0.8959338068962097
Epoch 880, training loss: 6.401196479797363 = 0.06975900381803513 + 1.0 * 6.331437587738037
Epoch 880, val loss: 0.9015989303588867
Epoch 890, training loss: 6.390045166015625 = 0.0662463903427124 + 1.0 * 6.323798656463623
Epoch 890, val loss: 0.9071693420410156
Epoch 900, training loss: 6.385181427001953 = 0.06297511607408524 + 1.0 * 6.322206497192383
Epoch 900, val loss: 0.9127640128135681
Epoch 910, training loss: 6.379012107849121 = 0.059914346784353256 + 1.0 * 6.319097995758057
Epoch 910, val loss: 0.918382465839386
Epoch 920, training loss: 6.382011890411377 = 0.05704745650291443 + 1.0 * 6.32496452331543
Epoch 920, val loss: 0.9239674806594849
Epoch 930, training loss: 6.373138904571533 = 0.054365478456020355 + 1.0 * 6.31877326965332
Epoch 930, val loss: 0.9294434785842896
Epoch 940, training loss: 6.377015590667725 = 0.05185654014348984 + 1.0 * 6.325159072875977
Epoch 940, val loss: 0.9350521564483643
Epoch 950, training loss: 6.371274471282959 = 0.04950143024325371 + 1.0 * 6.321773052215576
Epoch 950, val loss: 0.940308153629303
Epoch 960, training loss: 6.362433910369873 = 0.04731893166899681 + 1.0 * 6.315114974975586
Epoch 960, val loss: 0.9458242058753967
Epoch 970, training loss: 6.358154773712158 = 0.04525941610336304 + 1.0 * 6.31289529800415
Epoch 970, val loss: 0.9512007236480713
Epoch 980, training loss: 6.355995178222656 = 0.04332493245601654 + 1.0 * 6.3126702308654785
Epoch 980, val loss: 0.9565266966819763
Epoch 990, training loss: 6.361300468444824 = 0.04150461405515671 + 1.0 * 6.319796085357666
Epoch 990, val loss: 0.9617983102798462
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 10.532465934753418 = 1.9356105327606201 + 1.0 * 8.596855163574219
Epoch 0, val loss: 1.927256464958191
Epoch 10, training loss: 10.522991180419922 = 1.9262930154800415 + 1.0 * 8.596697807312012
Epoch 10, val loss: 1.9185584783554077
Epoch 20, training loss: 10.510372161865234 = 1.9150390625 + 1.0 * 8.595333099365234
Epoch 20, val loss: 1.9076275825500488
Epoch 30, training loss: 10.481599807739258 = 1.8993746042251587 + 1.0 * 8.58222484588623
Epoch 30, val loss: 1.8920691013336182
Epoch 40, training loss: 10.363090515136719 = 1.8783944845199585 + 1.0 * 8.484696388244629
Epoch 40, val loss: 1.8715628385543823
Epoch 50, training loss: 9.939273834228516 = 1.8558214902877808 + 1.0 * 8.083452224731445
Epoch 50, val loss: 1.8502788543701172
Epoch 60, training loss: 9.50114917755127 = 1.8381195068359375 + 1.0 * 7.663029670715332
Epoch 60, val loss: 1.8346961736679077
Epoch 70, training loss: 9.077400207519531 = 1.8265420198440552 + 1.0 * 7.250858306884766
Epoch 70, val loss: 1.8241153955459595
Epoch 80, training loss: 8.864347457885742 = 1.8159916400909424 + 1.0 * 7.048356056213379
Epoch 80, val loss: 1.8144549131393433
Epoch 90, training loss: 8.699186325073242 = 1.802475929260254 + 1.0 * 6.89670991897583
Epoch 90, val loss: 1.8034826517105103
Epoch 100, training loss: 8.599973678588867 = 1.7895054817199707 + 1.0 * 6.8104681968688965
Epoch 100, val loss: 1.7928293943405151
Epoch 110, training loss: 8.522418975830078 = 1.7771002054214478 + 1.0 * 6.745318412780762
Epoch 110, val loss: 1.7819068431854248
Epoch 120, training loss: 8.457695960998535 = 1.7648735046386719 + 1.0 * 6.692822456359863
Epoch 120, val loss: 1.770891547203064
Epoch 130, training loss: 8.403790473937988 = 1.7516311407089233 + 1.0 * 6.652159214019775
Epoch 130, val loss: 1.7591546773910522
Epoch 140, training loss: 8.355326652526855 = 1.7365072965621948 + 1.0 * 6.618819713592529
Epoch 140, val loss: 1.746200442314148
Epoch 150, training loss: 8.31036376953125 = 1.719239592552185 + 1.0 * 6.591124534606934
Epoch 150, val loss: 1.7316360473632812
Epoch 160, training loss: 8.267422676086426 = 1.699293613433838 + 1.0 * 6.568129062652588
Epoch 160, val loss: 1.7149380445480347
Epoch 170, training loss: 8.223928451538086 = 1.6760740280151367 + 1.0 * 6.547854423522949
Epoch 170, val loss: 1.6956181526184082
Epoch 180, training loss: 8.179779052734375 = 1.6490286588668823 + 1.0 * 6.530750274658203
Epoch 180, val loss: 1.6730839014053345
Epoch 190, training loss: 8.133488655090332 = 1.6177021265029907 + 1.0 * 6.515786647796631
Epoch 190, val loss: 1.646889328956604
Epoch 200, training loss: 8.083719253540039 = 1.5817734003067017 + 1.0 * 6.501945972442627
Epoch 200, val loss: 1.6169064044952393
Epoch 210, training loss: 8.035040855407715 = 1.5415880680084229 + 1.0 * 6.493453025817871
Epoch 210, val loss: 1.5834728479385376
Epoch 220, training loss: 7.979881763458252 = 1.4981077909469604 + 1.0 * 6.481773853302002
Epoch 220, val loss: 1.5473889112472534
Epoch 230, training loss: 7.923492431640625 = 1.4519189596176147 + 1.0 * 6.471573352813721
Epoch 230, val loss: 1.5093307495117188
Epoch 240, training loss: 7.867305278778076 = 1.4037278890609741 + 1.0 * 6.4635772705078125
Epoch 240, val loss: 1.4699827432632446
Epoch 250, training loss: 7.822901725769043 = 1.3555278778076172 + 1.0 * 6.467373847961426
Epoch 250, val loss: 1.4312117099761963
Epoch 260, training loss: 7.759590148925781 = 1.3090169429779053 + 1.0 * 6.450572967529297
Epoch 260, val loss: 1.3944772481918335
Epoch 270, training loss: 7.706820487976074 = 1.2641656398773193 + 1.0 * 6.442654609680176
Epoch 270, val loss: 1.3597134351730347
Epoch 280, training loss: 7.658196449279785 = 1.2209153175354004 + 1.0 * 6.437281131744385
Epoch 280, val loss: 1.3269063234329224
Epoch 290, training loss: 7.6162261962890625 = 1.1794548034667969 + 1.0 * 6.436771392822266
Epoch 290, val loss: 1.2962678670883179
Epoch 300, training loss: 7.569180488586426 = 1.1403155326843262 + 1.0 * 6.4288649559021
Epoch 300, val loss: 1.2679657936096191
Epoch 310, training loss: 7.5257039070129395 = 1.1030651330947876 + 1.0 * 6.422638893127441
Epoch 310, val loss: 1.2416380643844604
Epoch 320, training loss: 7.4893798828125 = 1.0673401355743408 + 1.0 * 6.42203950881958
Epoch 320, val loss: 1.2167439460754395
Epoch 330, training loss: 7.447188854217529 = 1.0332807302474976 + 1.0 * 6.413908004760742
Epoch 330, val loss: 1.1935003995895386
Epoch 340, training loss: 7.409553527832031 = 1.00058913230896 + 1.0 * 6.40896463394165
Epoch 340, val loss: 1.1716217994689941
Epoch 350, training loss: 7.385357856750488 = 0.9691097736358643 + 1.0 * 6.416247844696045
Epoch 350, val loss: 1.1508581638336182
Epoch 360, training loss: 7.342369079589844 = 0.93912273645401 + 1.0 * 6.4032464027404785
Epoch 360, val loss: 1.1313438415527344
Epoch 370, training loss: 7.308128833770752 = 0.9102231860160828 + 1.0 * 6.3979058265686035
Epoch 370, val loss: 1.113010048866272
Epoch 380, training loss: 7.275389194488525 = 0.8821345567703247 + 1.0 * 6.39325475692749
Epoch 380, val loss: 1.09548819065094
Epoch 390, training loss: 7.26279354095459 = 0.8547234535217285 + 1.0 * 6.408070087432861
Epoch 390, val loss: 1.0786610841751099
Epoch 400, training loss: 7.220864772796631 = 0.8283329010009766 + 1.0 * 6.392531871795654
Epoch 400, val loss: 1.0629351139068604
Epoch 410, training loss: 7.187919616699219 = 0.8026934266090393 + 1.0 * 6.385226249694824
Epoch 410, val loss: 1.0480467081069946
Epoch 420, training loss: 7.159623146057129 = 0.7775307297706604 + 1.0 * 6.382092475891113
Epoch 420, val loss: 1.0336918830871582
Epoch 430, training loss: 7.137250900268555 = 0.7527807950973511 + 1.0 * 6.384469985961914
Epoch 430, val loss: 1.0198742151260376
Epoch 440, training loss: 7.104634761810303 = 0.7284794449806213 + 1.0 * 6.376155376434326
Epoch 440, val loss: 1.0066859722137451
Epoch 450, training loss: 7.085774898529053 = 0.7044903635978699 + 1.0 * 6.381284713745117
Epoch 450, val loss: 0.9939061999320984
Epoch 460, training loss: 7.055626392364502 = 0.6807621121406555 + 1.0 * 6.374864101409912
Epoch 460, val loss: 0.9817132949829102
Epoch 470, training loss: 7.029433250427246 = 0.657334566116333 + 1.0 * 6.372098445892334
Epoch 470, val loss: 0.9699443578720093
Epoch 480, training loss: 7.001453399658203 = 0.6340231895446777 + 1.0 * 6.367430210113525
Epoch 480, val loss: 0.9584997296333313
Epoch 490, training loss: 6.9784016609191895 = 0.6107439994812012 + 1.0 * 6.367657661437988
Epoch 490, val loss: 0.947404146194458
Epoch 500, training loss: 6.954897880554199 = 0.5875622630119324 + 1.0 * 6.367335796356201
Epoch 500, val loss: 0.9365498423576355
Epoch 510, training loss: 6.931543350219727 = 0.5645515322685242 + 1.0 * 6.366991996765137
Epoch 510, val loss: 0.9261305332183838
Epoch 520, training loss: 6.904891490936279 = 0.5416548848152161 + 1.0 * 6.363236427307129
Epoch 520, val loss: 0.9158569574356079
Epoch 530, training loss: 6.876459121704102 = 0.5188398361206055 + 1.0 * 6.357619285583496
Epoch 530, val loss: 0.9058891534805298
Epoch 540, training loss: 6.8553385734558105 = 0.4960154592990875 + 1.0 * 6.359323024749756
Epoch 540, val loss: 0.8961331844329834
Epoch 550, training loss: 6.8319268226623535 = 0.4733183979988098 + 1.0 * 6.358608245849609
Epoch 550, val loss: 0.8866510391235352
Epoch 560, training loss: 6.802845478057861 = 0.4507187306880951 + 1.0 * 6.352126598358154
Epoch 560, val loss: 0.8774976134300232
Epoch 570, training loss: 6.788317680358887 = 0.42816975712776184 + 1.0 * 6.360147953033447
Epoch 570, val loss: 0.8686793446540833
Epoch 580, training loss: 6.756987571716309 = 0.40588462352752686 + 1.0 * 6.351102828979492
Epoch 580, val loss: 0.8602470755577087
Epoch 590, training loss: 6.736642360687256 = 0.38374409079551697 + 1.0 * 6.352898120880127
Epoch 590, val loss: 0.8523139953613281
Epoch 600, training loss: 6.708501815795898 = 0.3619089126586914 + 1.0 * 6.346592903137207
Epoch 600, val loss: 0.844862163066864
Epoch 610, training loss: 6.689497470855713 = 0.3403850793838501 + 1.0 * 6.349112510681152
Epoch 610, val loss: 0.8380455374717712
Epoch 620, training loss: 6.672628402709961 = 0.3194939196109772 + 1.0 * 6.353134632110596
Epoch 620, val loss: 0.8319011330604553
Epoch 630, training loss: 6.642496109008789 = 0.2993942201137543 + 1.0 * 6.343101978302002
Epoch 630, val loss: 0.8266556262969971
Epoch 640, training loss: 6.619821071624756 = 0.2801544964313507 + 1.0 * 6.339666366577148
Epoch 640, val loss: 0.8223010301589966
Epoch 650, training loss: 6.621347427368164 = 0.2618720233440399 + 1.0 * 6.359475612640381
Epoch 650, val loss: 0.8188820481300354
Epoch 660, training loss: 6.582018852233887 = 0.24490340054035187 + 1.0 * 6.337115287780762
Epoch 660, val loss: 0.8164569139480591
Epoch 670, training loss: 6.566503524780273 = 0.22909222543239594 + 1.0 * 6.337411403656006
Epoch 670, val loss: 0.8151293992996216
Epoch 680, training loss: 6.554740905761719 = 0.21440406143665314 + 1.0 * 6.340336799621582
Epoch 680, val loss: 0.8146982789039612
Epoch 690, training loss: 6.541759490966797 = 0.2008955031633377 + 1.0 * 6.340864181518555
Epoch 690, val loss: 0.8149721026420593
Epoch 700, training loss: 6.519647598266602 = 0.18852165341377258 + 1.0 * 6.331125736236572
Epoch 700, val loss: 0.8164302706718445
Epoch 710, training loss: 6.508272647857666 = 0.1771245002746582 + 1.0 * 6.331148147583008
Epoch 710, val loss: 0.8185483813285828
Epoch 720, training loss: 6.506314754486084 = 0.16659662127494812 + 1.0 * 6.339718341827393
Epoch 720, val loss: 0.821314811706543
Epoch 730, training loss: 6.493473052978516 = 0.15699945390224457 + 1.0 * 6.33647346496582
Epoch 730, val loss: 0.8246728181838989
Epoch 740, training loss: 6.477627754211426 = 0.1481483429670334 + 1.0 * 6.329479217529297
Epoch 740, val loss: 0.8287150263786316
Epoch 750, training loss: 6.4655680656433105 = 0.13996738195419312 + 1.0 * 6.325600624084473
Epoch 750, val loss: 0.833172619342804
Epoch 760, training loss: 6.4641571044921875 = 0.1323842853307724 + 1.0 * 6.331772804260254
Epoch 760, val loss: 0.8379256129264832
Epoch 770, training loss: 6.457476615905762 = 0.1253826916217804 + 1.0 * 6.332093715667725
Epoch 770, val loss: 0.843010663986206
Epoch 780, training loss: 6.442383766174316 = 0.11890875548124313 + 1.0 * 6.323474884033203
Epoch 780, val loss: 0.8483914136886597
Epoch 790, training loss: 6.433615684509277 = 0.11290236562490463 + 1.0 * 6.320713520050049
Epoch 790, val loss: 0.8540772199630737
Epoch 800, training loss: 6.428402900695801 = 0.10729570686817169 + 1.0 * 6.321107387542725
Epoch 800, val loss: 0.8599332571029663
Epoch 810, training loss: 6.425961971282959 = 0.10205984115600586 + 1.0 * 6.323902130126953
Epoch 810, val loss: 0.8658142685890198
Epoch 820, training loss: 6.4163031578063965 = 0.0971917137503624 + 1.0 * 6.319111347198486
Epoch 820, val loss: 0.8718518614768982
Epoch 830, training loss: 6.410778522491455 = 0.09264524281024933 + 1.0 * 6.318133354187012
Epoch 830, val loss: 0.8781616687774658
Epoch 840, training loss: 6.4063568115234375 = 0.08838694542646408 + 1.0 * 6.317969799041748
Epoch 840, val loss: 0.8843235969543457
Epoch 850, training loss: 6.400431156158447 = 0.08437660336494446 + 1.0 * 6.316054344177246
Epoch 850, val loss: 0.8905876874923706
Epoch 860, training loss: 6.393759250640869 = 0.08060882985591888 + 1.0 * 6.313150405883789
Epoch 860, val loss: 0.8969876766204834
Epoch 870, training loss: 6.397520542144775 = 0.07704616338014603 + 1.0 * 6.320474147796631
Epoch 870, val loss: 0.9032341241836548
Epoch 880, training loss: 6.3916916847229 = 0.07369889318943024 + 1.0 * 6.317992687225342
Epoch 880, val loss: 0.9093655347824097
Epoch 890, training loss: 6.382261276245117 = 0.07056299597024918 + 1.0 * 6.3116984367370605
Epoch 890, val loss: 0.9159104824066162
Epoch 900, training loss: 6.378582954406738 = 0.06758426874876022 + 1.0 * 6.310998916625977
Epoch 900, val loss: 0.9222777485847473
Epoch 910, training loss: 6.376862049102783 = 0.06476743519306183 + 1.0 * 6.312094688415527
Epoch 910, val loss: 0.9284756183624268
Epoch 920, training loss: 6.3709187507629395 = 0.062102220952510834 + 1.0 * 6.308816432952881
Epoch 920, val loss: 0.9348405599594116
Epoch 930, training loss: 6.366451263427734 = 0.05956258624792099 + 1.0 * 6.306888580322266
Epoch 930, val loss: 0.9411899447441101
Epoch 940, training loss: 6.365833759307861 = 0.05713309720158577 + 1.0 * 6.3087005615234375
Epoch 940, val loss: 0.9474805593490601
Epoch 950, training loss: 6.363457202911377 = 0.05482063814997673 + 1.0 * 6.308636665344238
Epoch 950, val loss: 0.9535038471221924
Epoch 960, training loss: 6.361286640167236 = 0.05263351649045944 + 1.0 * 6.308653354644775
Epoch 960, val loss: 0.9597094655036926
Epoch 970, training loss: 6.353912353515625 = 0.05055317282676697 + 1.0 * 6.303359031677246
Epoch 970, val loss: 0.9659812450408936
Epoch 980, training loss: 6.36028528213501 = 0.04856587201356888 + 1.0 * 6.3117194175720215
Epoch 980, val loss: 0.9720883965492249
Epoch 990, training loss: 6.3516364097595215 = 0.04667198657989502 + 1.0 * 6.304964542388916
Epoch 990, val loss: 0.9780247211456299
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 10.534587860107422 = 1.9377378225326538 + 1.0 * 8.596850395202637
Epoch 0, val loss: 1.9388177394866943
Epoch 10, training loss: 10.524825096130371 = 1.928179383277893 + 1.0 * 8.59664535522461
Epoch 10, val loss: 1.9288297891616821
Epoch 20, training loss: 10.511393547058105 = 1.916349172592163 + 1.0 * 8.595044136047363
Epoch 20, val loss: 1.916316270828247
Epoch 30, training loss: 10.48119831085205 = 1.899875283241272 + 1.0 * 8.58132266998291
Epoch 30, val loss: 1.8989232778549194
Epoch 40, training loss: 10.363126754760742 = 1.877842903137207 + 1.0 * 8.485283851623535
Epoch 40, val loss: 1.8763238191604614
Epoch 50, training loss: 9.920909881591797 = 1.8542587757110596 + 1.0 * 8.066651344299316
Epoch 50, val loss: 1.8528554439544678
Epoch 60, training loss: 9.610376358032227 = 1.8325672149658203 + 1.0 * 7.777808666229248
Epoch 60, val loss: 1.8312941789627075
Epoch 70, training loss: 9.238611221313477 = 1.8171910047531128 + 1.0 * 7.421420574188232
Epoch 70, val loss: 1.8166064023971558
Epoch 80, training loss: 8.994068145751953 = 1.8061561584472656 + 1.0 * 7.187912464141846
Epoch 80, val loss: 1.805274248123169
Epoch 90, training loss: 8.823026657104492 = 1.7911792993545532 + 1.0 * 7.03184700012207
Epoch 90, val loss: 1.7902952432632446
Epoch 100, training loss: 8.698639869689941 = 1.7757437229156494 + 1.0 * 6.922895908355713
Epoch 100, val loss: 1.775805115699768
Epoch 110, training loss: 8.600789070129395 = 1.7621244192123413 + 1.0 * 6.838664531707764
Epoch 110, val loss: 1.7630040645599365
Epoch 120, training loss: 8.519970893859863 = 1.748479962348938 + 1.0 * 6.771490573883057
Epoch 120, val loss: 1.749692678451538
Epoch 130, training loss: 8.458465576171875 = 1.7332661151885986 + 1.0 * 6.725199222564697
Epoch 130, val loss: 1.7351261377334595
Epoch 140, training loss: 8.388832092285156 = 1.71651291847229 + 1.0 * 6.672318935394287
Epoch 140, val loss: 1.7197972536087036
Epoch 150, training loss: 8.33191967010498 = 1.6977226734161377 + 1.0 * 6.634197235107422
Epoch 150, val loss: 1.703132152557373
Epoch 160, training loss: 8.279353141784668 = 1.6758525371551514 + 1.0 * 6.603500843048096
Epoch 160, val loss: 1.6840596199035645
Epoch 170, training loss: 8.232792854309082 = 1.6499212980270386 + 1.0 * 6.582871913909912
Epoch 170, val loss: 1.6615045070648193
Epoch 180, training loss: 8.178621292114258 = 1.6199638843536377 + 1.0 * 6.558657169342041
Epoch 180, val loss: 1.635488510131836
Epoch 190, training loss: 8.125996589660645 = 1.5856910943984985 + 1.0 * 6.540305137634277
Epoch 190, val loss: 1.6056790351867676
Epoch 200, training loss: 8.073537826538086 = 1.5467493534088135 + 1.0 * 6.526788234710693
Epoch 200, val loss: 1.5719647407531738
Epoch 210, training loss: 8.015793800354004 = 1.503743290901184 + 1.0 * 6.512050151824951
Epoch 210, val loss: 1.5349903106689453
Epoch 220, training loss: 7.958675384521484 = 1.4572932720184326 + 1.0 * 6.501381874084473
Epoch 220, val loss: 1.4954614639282227
Epoch 230, training loss: 7.902290344238281 = 1.4090218544006348 + 1.0 * 6.4932684898376465
Epoch 230, val loss: 1.4551119804382324
Epoch 240, training loss: 7.841197967529297 = 1.360573649406433 + 1.0 * 6.480624198913574
Epoch 240, val loss: 1.4155282974243164
Epoch 250, training loss: 7.7848405838012695 = 1.3121554851531982 + 1.0 * 6.472684860229492
Epoch 250, val loss: 1.3769934177398682
Epoch 260, training loss: 7.728661060333252 = 1.2646037340164185 + 1.0 * 6.464057445526123
Epoch 260, val loss: 1.3401734828948975
Epoch 270, training loss: 7.675145149230957 = 1.2177536487579346 + 1.0 * 6.457391738891602
Epoch 270, val loss: 1.3049813508987427
Epoch 280, training loss: 7.625487804412842 = 1.1717729568481445 + 1.0 * 6.453714847564697
Epoch 280, val loss: 1.2714658975601196
Epoch 290, training loss: 7.5722880363464355 = 1.1268359422683716 + 1.0 * 6.4454522132873535
Epoch 290, val loss: 1.2394444942474365
Epoch 300, training loss: 7.521200656890869 = 1.082190990447998 + 1.0 * 6.439009666442871
Epoch 300, val loss: 1.208121657371521
Epoch 310, training loss: 7.4859514236450195 = 1.037724256515503 + 1.0 * 6.448227405548096
Epoch 310, val loss: 1.177319049835205
Epoch 320, training loss: 7.427654266357422 = 0.9942598938941956 + 1.0 * 6.433394432067871
Epoch 320, val loss: 1.1476556062698364
Epoch 330, training loss: 7.377860069274902 = 0.9515944123268127 + 1.0 * 6.426265716552734
Epoch 330, val loss: 1.118622899055481
Epoch 340, training loss: 7.332479476928711 = 0.9096971154212952 + 1.0 * 6.4227824211120605
Epoch 340, val loss: 1.0903171300888062
Epoch 350, training loss: 7.292293071746826 = 0.8691233992576599 + 1.0 * 6.4231696128845215
Epoch 350, val loss: 1.0633487701416016
Epoch 360, training loss: 7.2473464012146 = 0.8303695917129517 + 1.0 * 6.4169769287109375
Epoch 360, val loss: 1.0380160808563232
Epoch 370, training loss: 7.206362247467041 = 0.7932152152061462 + 1.0 * 6.41314697265625
Epoch 370, val loss: 1.0143463611602783
Epoch 380, training loss: 7.169061183929443 = 0.7578024864196777 + 1.0 * 6.411258697509766
Epoch 380, val loss: 0.992482602596283
Epoch 390, training loss: 7.131503105163574 = 0.7242652773857117 + 1.0 * 6.407238006591797
Epoch 390, val loss: 0.9724962115287781
Epoch 400, training loss: 7.095804214477539 = 0.6925084590911865 + 1.0 * 6.403295516967773
Epoch 400, val loss: 0.9543165564537048
Epoch 410, training loss: 7.061971664428711 = 0.6621986627578735 + 1.0 * 6.399773120880127
Epoch 410, val loss: 0.937749445438385
Epoch 420, training loss: 7.037864685058594 = 0.6332315802574158 + 1.0 * 6.404633045196533
Epoch 420, val loss: 0.9226142764091492
Epoch 430, training loss: 7.001495838165283 = 0.6058648824691772 + 1.0 * 6.395630836486816
Epoch 430, val loss: 0.9089442491531372
Epoch 440, training loss: 6.970929145812988 = 0.5795695185661316 + 1.0 * 6.391359806060791
Epoch 440, val loss: 0.8965139985084534
Epoch 450, training loss: 6.9421162605285645 = 0.5540719628334045 + 1.0 * 6.388044357299805
Epoch 450, val loss: 0.8849416375160217
Epoch 460, training loss: 6.9372053146362305 = 0.529274582862854 + 1.0 * 6.407930850982666
Epoch 460, val loss: 0.8740655779838562
Epoch 470, training loss: 6.896100997924805 = 0.505469024181366 + 1.0 * 6.390632152557373
Epoch 470, val loss: 0.8639645576477051
Epoch 480, training loss: 6.863770484924316 = 0.4824081063270569 + 1.0 * 6.381362438201904
Epoch 480, val loss: 0.8546980023384094
Epoch 490, training loss: 6.8401079177856445 = 0.45991823077201843 + 1.0 * 6.380189895629883
Epoch 490, val loss: 0.846039354801178
Epoch 500, training loss: 6.818915843963623 = 0.4380398392677307 + 1.0 * 6.380876064300537
Epoch 500, val loss: 0.8381102681159973
Epoch 510, training loss: 6.791303634643555 = 0.4169082045555115 + 1.0 * 6.374395370483398
Epoch 510, val loss: 0.8310549259185791
Epoch 520, training loss: 6.768980979919434 = 0.3964651823043823 + 1.0 * 6.372515678405762
Epoch 520, val loss: 0.8247962594032288
Epoch 530, training loss: 6.756625175476074 = 0.37681353092193604 + 1.0 * 6.379811763763428
Epoch 530, val loss: 0.8194167613983154
Epoch 540, training loss: 6.730522632598877 = 0.3580862283706665 + 1.0 * 6.3724365234375
Epoch 540, val loss: 0.8149686455726624
Epoch 550, training loss: 6.70814323425293 = 0.34022000432014465 + 1.0 * 6.367923259735107
Epoch 550, val loss: 0.8114351630210876
Epoch 560, training loss: 6.693253517150879 = 0.3232056200504303 + 1.0 * 6.3700480461120605
Epoch 560, val loss: 0.8086287379264832
Epoch 570, training loss: 6.671231746673584 = 0.3070654273033142 + 1.0 * 6.364166259765625
Epoch 570, val loss: 0.8067076802253723
Epoch 580, training loss: 6.6646881103515625 = 0.2916664481163025 + 1.0 * 6.373021602630615
Epoch 580, val loss: 0.8053566813468933
Epoch 590, training loss: 6.639215469360352 = 0.2770550549030304 + 1.0 * 6.3621602058410645
Epoch 590, val loss: 0.8045837879180908
Epoch 600, training loss: 6.621706485748291 = 0.26298803091049194 + 1.0 * 6.358718395233154
Epoch 600, val loss: 0.804386556148529
Epoch 610, training loss: 6.612486362457275 = 0.24937114119529724 + 1.0 * 6.363115310668945
Epoch 610, val loss: 0.8044472932815552
Epoch 620, training loss: 6.597030162811279 = 0.23622973263263702 + 1.0 * 6.360800266265869
Epoch 620, val loss: 0.8048350214958191
Epoch 630, training loss: 6.578115463256836 = 0.22341462969779968 + 1.0 * 6.354701042175293
Epoch 630, val loss: 0.8055832386016846
Epoch 640, training loss: 6.576814651489258 = 0.2108764946460724 + 1.0 * 6.365938186645508
Epoch 640, val loss: 0.8063298463821411
Epoch 650, training loss: 6.549708366394043 = 0.19871985912322998 + 1.0 * 6.350988388061523
Epoch 650, val loss: 0.8074028491973877
Epoch 660, training loss: 6.53694486618042 = 0.18686112761497498 + 1.0 * 6.350083827972412
Epoch 660, val loss: 0.8086471557617188
Epoch 670, training loss: 6.53961181640625 = 0.17533232271671295 + 1.0 * 6.364279270172119
Epoch 670, val loss: 0.8099120855331421
Epoch 680, training loss: 6.511846542358398 = 0.16432055830955505 + 1.0 * 6.3475260734558105
Epoch 680, val loss: 0.8114333748817444
Epoch 690, training loss: 6.500253200531006 = 0.15377865731716156 + 1.0 * 6.346474647521973
Epoch 690, val loss: 0.8131811618804932
Epoch 700, training loss: 6.4886674880981445 = 0.14376048743724823 + 1.0 * 6.344906806945801
Epoch 700, val loss: 0.8150225877761841
Epoch 710, training loss: 6.47825288772583 = 0.13433684408664703 + 1.0 * 6.343915939331055
Epoch 710, val loss: 0.8170300126075745
Epoch 720, training loss: 6.468379497528076 = 0.12555108964443207 + 1.0 * 6.342828273773193
Epoch 720, val loss: 0.8193786144256592
Epoch 730, training loss: 6.458086013793945 = 0.11735953390598297 + 1.0 * 6.340726375579834
Epoch 730, val loss: 0.8219680190086365
Epoch 740, training loss: 6.4612274169921875 = 0.10975035279989243 + 1.0 * 6.351477146148682
Epoch 740, val loss: 0.8246976733207703
Epoch 750, training loss: 6.444674968719482 = 0.10275142639875412 + 1.0 * 6.341923713684082
Epoch 750, val loss: 0.8276718258857727
Epoch 760, training loss: 6.4356207847595215 = 0.09630655497312546 + 1.0 * 6.3393144607543945
Epoch 760, val loss: 0.8309113383293152
Epoch 770, training loss: 6.426375389099121 = 0.09039787948131561 + 1.0 * 6.335977554321289
Epoch 770, val loss: 0.8342033624649048
Epoch 780, training loss: 6.419093608856201 = 0.08497701585292816 + 1.0 * 6.334116458892822
Epoch 780, val loss: 0.8378400802612305
Epoch 790, training loss: 6.423699378967285 = 0.07997854053974152 + 1.0 * 6.34372091293335
Epoch 790, val loss: 0.841484010219574
Epoch 800, training loss: 6.411862850189209 = 0.07541874796152115 + 1.0 * 6.336443901062012
Epoch 800, val loss: 0.8452608585357666
Epoch 810, training loss: 6.4014787673950195 = 0.07121166586875916 + 1.0 * 6.330266952514648
Epoch 810, val loss: 0.8493406176567078
Epoch 820, training loss: 6.397716045379639 = 0.06731799989938736 + 1.0 * 6.330398082733154
Epoch 820, val loss: 0.8533163070678711
Epoch 830, training loss: 6.399437427520752 = 0.06372661888599396 + 1.0 * 6.3357110023498535
Epoch 830, val loss: 0.8573862314224243
Epoch 840, training loss: 6.387800216674805 = 0.06041252985596657 + 1.0 * 6.327387809753418
Epoch 840, val loss: 0.8616259694099426
Epoch 850, training loss: 6.387946605682373 = 0.057345062494277954 + 1.0 * 6.330601692199707
Epoch 850, val loss: 0.8658776879310608
Epoch 860, training loss: 6.380980014801025 = 0.0545082613825798 + 1.0 * 6.32647180557251
Epoch 860, val loss: 0.869958221912384
Epoch 870, training loss: 6.376916885375977 = 0.05187799036502838 + 1.0 * 6.325038909912109
Epoch 870, val loss: 0.8744100332260132
Epoch 880, training loss: 6.371809959411621 = 0.04942326992750168 + 1.0 * 6.322386741638184
Epoch 880, val loss: 0.8787164092063904
Epoch 890, training loss: 6.374884605407715 = 0.04712928086519241 + 1.0 * 6.327755451202393
Epoch 890, val loss: 0.8830607533454895
Epoch 900, training loss: 6.369540691375732 = 0.045002423226833344 + 1.0 * 6.324538230895996
Epoch 900, val loss: 0.8873357772827148
Epoch 910, training loss: 6.370992660522461 = 0.04301680251955986 + 1.0 * 6.327975749969482
Epoch 910, val loss: 0.8917340040206909
Epoch 920, training loss: 6.361217498779297 = 0.04116552323102951 + 1.0 * 6.320052146911621
Epoch 920, val loss: 0.8960208296775818
Epoch 930, training loss: 6.357229709625244 = 0.039433885365724564 + 1.0 * 6.317795753479004
Epoch 930, val loss: 0.9003541469573975
Epoch 940, training loss: 6.361983299255371 = 0.03780360892415047 + 1.0 * 6.324179649353027
Epoch 940, val loss: 0.9046044945716858
Epoch 950, training loss: 6.356064796447754 = 0.036271657794713974 + 1.0 * 6.319793224334717
Epoch 950, val loss: 0.908858060836792
Epoch 960, training loss: 6.351764678955078 = 0.03484494984149933 + 1.0 * 6.316919803619385
Epoch 960, val loss: 0.9131618738174438
Epoch 970, training loss: 6.35227108001709 = 0.033496495336294174 + 1.0 * 6.318774700164795
Epoch 970, val loss: 0.9173476696014404
Epoch 980, training loss: 6.346459865570068 = 0.0322301872074604 + 1.0 * 6.314229488372803
Epoch 980, val loss: 0.9215376377105713
Epoch 990, training loss: 6.348636150360107 = 0.031037474051117897 + 1.0 * 6.317598819732666
Epoch 990, val loss: 0.9256547689437866
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8112809699525567
The final CL Acc:0.78395, 0.01848, The final GNN Acc:0.81269, 0.00317
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13188])
remove edge: torch.Size([2, 7866])
updated graph: torch.Size([2, 10498])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.529533386230469 = 1.9327081441879272 + 1.0 * 8.59682559967041
Epoch 0, val loss: 1.9323893785476685
Epoch 10, training loss: 10.519147872924805 = 1.9226317405700684 + 1.0 * 8.596516609191895
Epoch 10, val loss: 1.9228332042694092
Epoch 20, training loss: 10.503924369812012 = 1.9098694324493408 + 1.0 * 8.59405517578125
Epoch 20, val loss: 1.9102859497070312
Epoch 30, training loss: 10.467692375183105 = 1.8918812274932861 + 1.0 * 8.575811386108398
Epoch 30, val loss: 1.89231276512146
Epoch 40, training loss: 10.346956253051758 = 1.8679977655410767 + 1.0 * 8.478958129882812
Epoch 40, val loss: 1.8693773746490479
Epoch 50, training loss: 9.978628158569336 = 1.8414825201034546 + 1.0 * 8.13714599609375
Epoch 50, val loss: 1.8444489240646362
Epoch 60, training loss: 9.724142074584961 = 1.816758394241333 + 1.0 * 7.907383441925049
Epoch 60, val loss: 1.8216394186019897
Epoch 70, training loss: 9.441986083984375 = 1.795135498046875 + 1.0 * 7.6468505859375
Epoch 70, val loss: 1.802553653717041
Epoch 80, training loss: 9.06715202331543 = 1.7798542976379395 + 1.0 * 7.287298202514648
Epoch 80, val loss: 1.790176510810852
Epoch 90, training loss: 8.878082275390625 = 1.7677847146987915 + 1.0 * 7.110297203063965
Epoch 90, val loss: 1.7789607048034668
Epoch 100, training loss: 8.764699935913086 = 1.7504132986068726 + 1.0 * 7.014286994934082
Epoch 100, val loss: 1.7627265453338623
Epoch 110, training loss: 8.681644439697266 = 1.7322286367416382 + 1.0 * 6.949416160583496
Epoch 110, val loss: 1.7463048696517944
Epoch 120, training loss: 8.587957382202148 = 1.712980031967163 + 1.0 * 6.8749775886535645
Epoch 120, val loss: 1.7287977933883667
Epoch 130, training loss: 8.501642227172852 = 1.691190242767334 + 1.0 * 6.810451507568359
Epoch 130, val loss: 1.709185004234314
Epoch 140, training loss: 8.419979095458984 = 1.6654605865478516 + 1.0 * 6.754518985748291
Epoch 140, val loss: 1.6870707273483276
Epoch 150, training loss: 8.33416748046875 = 1.636217713356018 + 1.0 * 6.697949409484863
Epoch 150, val loss: 1.6624265909194946
Epoch 160, training loss: 8.2555570602417 = 1.6024444103240967 + 1.0 * 6.653112411499023
Epoch 160, val loss: 1.6338251829147339
Epoch 170, training loss: 8.180645942687988 = 1.5638926029205322 + 1.0 * 6.616753101348877
Epoch 170, val loss: 1.6012409925460815
Epoch 180, training loss: 8.113152503967285 = 1.5212589502334595 + 1.0 * 6.591893196105957
Epoch 180, val loss: 1.5654665231704712
Epoch 190, training loss: 8.040987014770508 = 1.476497769355774 + 1.0 * 6.564488887786865
Epoch 190, val loss: 1.5282789468765259
Epoch 200, training loss: 7.975593566894531 = 1.430277705192566 + 1.0 * 6.545315742492676
Epoch 200, val loss: 1.4902251958847046
Epoch 210, training loss: 7.915707111358643 = 1.3838027715682983 + 1.0 * 6.531904220581055
Epoch 210, val loss: 1.4526915550231934
Epoch 220, training loss: 7.8566975593566895 = 1.3390345573425293 + 1.0 * 6.51766300201416
Epoch 220, val loss: 1.4166041612625122
Epoch 230, training loss: 7.798830986022949 = 1.2949867248535156 + 1.0 * 6.503844261169434
Epoch 230, val loss: 1.3817781209945679
Epoch 240, training loss: 7.74453067779541 = 1.2516149282455444 + 1.0 * 6.492915630340576
Epoch 240, val loss: 1.3477678298950195
Epoch 250, training loss: 7.698831558227539 = 1.2089738845825195 + 1.0 * 6.4898576736450195
Epoch 250, val loss: 1.3148093223571777
Epoch 260, training loss: 7.645072937011719 = 1.1681021451950073 + 1.0 * 6.476970672607422
Epoch 260, val loss: 1.2837034463882446
Epoch 270, training loss: 7.5956525802612305 = 1.1287193298339844 + 1.0 * 6.466933250427246
Epoch 270, val loss: 1.2543851137161255
Epoch 280, training loss: 7.549180030822754 = 1.0904276371002197 + 1.0 * 6.458752632141113
Epoch 280, val loss: 1.2263705730438232
Epoch 290, training loss: 7.50931978225708 = 1.0536259412765503 + 1.0 * 6.45569372177124
Epoch 290, val loss: 1.19996178150177
Epoch 300, training loss: 7.463644981384277 = 1.0186560153961182 + 1.0 * 6.444989204406738
Epoch 300, val loss: 1.1756788492202759
Epoch 310, training loss: 7.423219680786133 = 0.9849878549575806 + 1.0 * 6.438231945037842
Epoch 310, val loss: 1.1528968811035156
Epoch 320, training loss: 7.385167121887207 = 0.952231228351593 + 1.0 * 6.43293571472168
Epoch 320, val loss: 1.1310232877731323
Epoch 330, training loss: 7.350194454193115 = 0.9205452799797058 + 1.0 * 6.429649353027344
Epoch 330, val loss: 1.110242486000061
Epoch 340, training loss: 7.312580108642578 = 0.8900402188301086 + 1.0 * 6.422539710998535
Epoch 340, val loss: 1.090648889541626
Epoch 350, training loss: 7.281882286071777 = 0.8601596355438232 + 1.0 * 6.421722888946533
Epoch 350, val loss: 1.0716371536254883
Epoch 360, training loss: 7.244734764099121 = 0.8311548829078674 + 1.0 * 6.413579940795898
Epoch 360, val loss: 1.0533032417297363
Epoch 370, training loss: 7.212072372436523 = 0.8029725551605225 + 1.0 * 6.40910005569458
Epoch 370, val loss: 1.0359326601028442
Epoch 380, training loss: 7.180130481719971 = 0.7754195928573608 + 1.0 * 6.40471076965332
Epoch 380, val loss: 1.0190678834915161
Epoch 390, training loss: 7.15510368347168 = 0.7484846115112305 + 1.0 * 6.406619071960449
Epoch 390, val loss: 1.0028804540634155
Epoch 400, training loss: 7.122321605682373 = 0.722335934638977 + 1.0 * 6.3999857902526855
Epoch 400, val loss: 0.9876200556755066
Epoch 410, training loss: 7.091578483581543 = 0.696793794631958 + 1.0 * 6.394784927368164
Epoch 410, val loss: 0.9731006622314453
Epoch 420, training loss: 7.068072319030762 = 0.6716212034225464 + 1.0 * 6.396450996398926
Epoch 420, val loss: 0.9591644406318665
Epoch 430, training loss: 7.038172245025635 = 0.6469729542732239 + 1.0 * 6.391199111938477
Epoch 430, val loss: 0.9457014203071594
Epoch 440, training loss: 7.009857654571533 = 0.6226083040237427 + 1.0 * 6.38724946975708
Epoch 440, val loss: 0.9328349232673645
Epoch 450, training loss: 6.984795093536377 = 0.5985590815544128 + 1.0 * 6.386236190795898
Epoch 450, val loss: 0.92021244764328
Epoch 460, training loss: 6.957021713256836 = 0.5748251080513 + 1.0 * 6.382196426391602
Epoch 460, val loss: 0.907873809337616
Epoch 470, training loss: 6.936870098114014 = 0.55133455991745 + 1.0 * 6.385535717010498
Epoch 470, val loss: 0.8957337141036987
Epoch 480, training loss: 6.905930519104004 = 0.528200089931488 + 1.0 * 6.377730369567871
Epoch 480, val loss: 0.8838103413581848
Epoch 490, training loss: 6.879321575164795 = 0.5052920579910278 + 1.0 * 6.374029636383057
Epoch 490, val loss: 0.8720476031303406
Epoch 500, training loss: 6.853463172912598 = 0.4824357032775879 + 1.0 * 6.37102746963501
Epoch 500, val loss: 0.8602911829948425
Epoch 510, training loss: 6.835306167602539 = 0.45953619480133057 + 1.0 * 6.375770092010498
Epoch 510, val loss: 0.8486169576644897
Epoch 520, training loss: 6.818744659423828 = 0.43685269355773926 + 1.0 * 6.381892204284668
Epoch 520, val loss: 0.8369386196136475
Epoch 530, training loss: 6.784438133239746 = 0.4144922196865082 + 1.0 * 6.369946002960205
Epoch 530, val loss: 0.8258067965507507
Epoch 540, training loss: 6.756534576416016 = 0.39225953817367554 + 1.0 * 6.364274978637695
Epoch 540, val loss: 0.8151087164878845
Epoch 550, training loss: 6.731561660766602 = 0.370169460773468 + 1.0 * 6.361392021179199
Epoch 550, val loss: 0.8049454689025879
Epoch 560, training loss: 6.712432861328125 = 0.3483944535255432 + 1.0 * 6.364038467407227
Epoch 560, val loss: 0.795612633228302
Epoch 570, training loss: 6.696534633636475 = 0.32736262679100037 + 1.0 * 6.369172096252441
Epoch 570, val loss: 0.7871991395950317
Epoch 580, training loss: 6.668240547180176 = 0.30728834867477417 + 1.0 * 6.360952377319336
Epoch 580, val loss: 0.7800512313842773
Epoch 590, training loss: 6.644224166870117 = 0.28830674290657043 + 1.0 * 6.355917453765869
Epoch 590, val loss: 0.7742199301719666
Epoch 600, training loss: 6.624308109283447 = 0.27041175961494446 + 1.0 * 6.353896141052246
Epoch 600, val loss: 0.7696385979652405
Epoch 610, training loss: 6.605839252471924 = 0.2536364495754242 + 1.0 * 6.352202892303467
Epoch 610, val loss: 0.7664786577224731
Epoch 620, training loss: 6.590991973876953 = 0.2380557805299759 + 1.0 * 6.352936267852783
Epoch 620, val loss: 0.7643746137619019
Epoch 630, training loss: 6.575081825256348 = 0.22370050847530365 + 1.0 * 6.351381301879883
Epoch 630, val loss: 0.7636058926582336
Epoch 640, training loss: 6.558974742889404 = 0.21044301986694336 + 1.0 * 6.348531723022461
Epoch 640, val loss: 0.7639052867889404
Epoch 650, training loss: 6.548724174499512 = 0.19814907014369965 + 1.0 * 6.350574970245361
Epoch 650, val loss: 0.7650794982910156
Epoch 660, training loss: 6.536571979522705 = 0.18679510056972504 + 1.0 * 6.349776744842529
Epoch 660, val loss: 0.7669445872306824
Epoch 670, training loss: 6.52230978012085 = 0.17633506655693054 + 1.0 * 6.345974922180176
Epoch 670, val loss: 0.7697976231575012
Epoch 680, training loss: 6.509647369384766 = 0.16660729050636292 + 1.0 * 6.3430399894714355
Epoch 680, val loss: 0.7731991410255432
Epoch 690, training loss: 6.506268501281738 = 0.1575549691915512 + 1.0 * 6.348713397979736
Epoch 690, val loss: 0.7772012948989868
Epoch 700, training loss: 6.493105411529541 = 0.14913922548294067 + 1.0 * 6.343966007232666
Epoch 700, val loss: 0.7817124724388123
Epoch 710, training loss: 6.4805402755737305 = 0.14130324125289917 + 1.0 * 6.339237213134766
Epoch 710, val loss: 0.7867186665534973
Epoch 720, training loss: 6.476445198059082 = 0.13395728170871735 + 1.0 * 6.342487812042236
Epoch 720, val loss: 0.7920956015586853
Epoch 730, training loss: 6.467921257019043 = 0.12713560461997986 + 1.0 * 6.340785503387451
Epoch 730, val loss: 0.7977203726768494
Epoch 740, training loss: 6.456101417541504 = 0.12076149880886078 + 1.0 * 6.3353400230407715
Epoch 740, val loss: 0.8038474917411804
Epoch 750, training loss: 6.448496341705322 = 0.11479347944259644 + 1.0 * 6.33370304107666
Epoch 750, val loss: 0.8101344704627991
Epoch 760, training loss: 6.441004753112793 = 0.10918339341878891 + 1.0 * 6.331821441650391
Epoch 760, val loss: 0.8167226314544678
Epoch 770, training loss: 6.468449115753174 = 0.10391128808259964 + 1.0 * 6.364537715911865
Epoch 770, val loss: 0.8233945965766907
Epoch 780, training loss: 6.438266277313232 = 0.09901982545852661 + 1.0 * 6.3392462730407715
Epoch 780, val loss: 0.8301327228546143
Epoch 790, training loss: 6.424937725067139 = 0.09445419162511826 + 1.0 * 6.330483436584473
Epoch 790, val loss: 0.8372216820716858
Epoch 800, training loss: 6.41687536239624 = 0.09015199542045593 + 1.0 * 6.326723575592041
Epoch 800, val loss: 0.844285786151886
Epoch 810, training loss: 6.412400722503662 = 0.08609365671873093 + 1.0 * 6.32630729675293
Epoch 810, val loss: 0.8515529036521912
Epoch 820, training loss: 6.408374786376953 = 0.08228675276041031 + 1.0 * 6.326087951660156
Epoch 820, val loss: 0.8586792349815369
Epoch 830, training loss: 6.406848430633545 = 0.078724704682827 + 1.0 * 6.328123569488525
Epoch 830, val loss: 0.8660595417022705
Epoch 840, training loss: 6.40002965927124 = 0.07537707686424255 + 1.0 * 6.324652671813965
Epoch 840, val loss: 0.8733919262886047
Epoch 850, training loss: 6.397479057312012 = 0.07220864295959473 + 1.0 * 6.325270175933838
Epoch 850, val loss: 0.8807644248008728
Epoch 860, training loss: 6.3900556564331055 = 0.06922656297683716 + 1.0 * 6.320828914642334
Epoch 860, val loss: 0.8880645632743835
Epoch 870, training loss: 6.385765075683594 = 0.06641031056642532 + 1.0 * 6.31935453414917
Epoch 870, val loss: 0.8955075740814209
Epoch 880, training loss: 6.385680675506592 = 0.06374508887529373 + 1.0 * 6.321935653686523
Epoch 880, val loss: 0.9028363823890686
Epoch 890, training loss: 6.3787736892700195 = 0.06122720241546631 + 1.0 * 6.317546367645264
Epoch 890, val loss: 0.9101096391677856
Epoch 900, training loss: 6.375813961029053 = 0.05885249376296997 + 1.0 * 6.316961288452148
Epoch 900, val loss: 0.9174678325653076
Epoch 910, training loss: 6.393511772155762 = 0.05660470202565193 + 1.0 * 6.336906909942627
Epoch 910, val loss: 0.9246019124984741
Epoch 920, training loss: 6.369499206542969 = 0.05448028817772865 + 1.0 * 6.315019130706787
Epoch 920, val loss: 0.9316982626914978
Epoch 930, training loss: 6.368121147155762 = 0.05247539281845093 + 1.0 * 6.315645694732666
Epoch 930, val loss: 0.938834011554718
Epoch 940, training loss: 6.363473415374756 = 0.0505654513835907 + 1.0 * 6.312908172607422
Epoch 940, val loss: 0.9458868503570557
Epoch 950, training loss: 6.375104904174805 = 0.048751238733530045 + 1.0 * 6.326353549957275
Epoch 950, val loss: 0.9528936147689819
Epoch 960, training loss: 6.364207744598389 = 0.047021832317113876 + 1.0 * 6.317185878753662
Epoch 960, val loss: 0.95968097448349
Epoch 970, training loss: 6.3571672439575195 = 0.04539287090301514 + 1.0 * 6.311774253845215
Epoch 970, val loss: 0.9666582942008972
Epoch 980, training loss: 6.356766223907471 = 0.04383217170834541 + 1.0 * 6.312933921813965
Epoch 980, val loss: 0.9734410047531128
Epoch 990, training loss: 6.352657318115234 = 0.04234960675239563 + 1.0 * 6.310307502746582
Epoch 990, val loss: 0.980026364326477
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 10.551795959472656 = 1.9549709558486938 + 1.0 * 8.596824645996094
Epoch 0, val loss: 1.950459361076355
Epoch 10, training loss: 10.541448593139648 = 1.9448846578598022 + 1.0 * 8.596564292907715
Epoch 10, val loss: 1.940786361694336
Epoch 20, training loss: 10.52736759185791 = 1.9325636625289917 + 1.0 * 8.594803810119629
Epoch 20, val loss: 1.9285106658935547
Epoch 30, training loss: 10.49677848815918 = 1.9153683185577393 + 1.0 * 8.58141040802002
Epoch 30, val loss: 1.9109675884246826
Epoch 40, training loss: 10.376718521118164 = 1.891512155532837 + 1.0 * 8.485206604003906
Epoch 40, val loss: 1.8870136737823486
Epoch 50, training loss: 9.743622779846191 = 1.864559531211853 + 1.0 * 7.879063129425049
Epoch 50, val loss: 1.861304521560669
Epoch 60, training loss: 9.28079891204834 = 1.8442195653915405 + 1.0 * 7.436579704284668
Epoch 60, val loss: 1.8435029983520508
Epoch 70, training loss: 8.913606643676758 = 1.829854130744934 + 1.0 * 7.083752155303955
Epoch 70, val loss: 1.8309537172317505
Epoch 80, training loss: 8.728750228881836 = 1.8164849281311035 + 1.0 * 6.912264823913574
Epoch 80, val loss: 1.819287657737732
Epoch 90, training loss: 8.631423950195312 = 1.8001455068588257 + 1.0 * 6.831278324127197
Epoch 90, val loss: 1.804801106452942
Epoch 100, training loss: 8.556450843811035 = 1.7828909158706665 + 1.0 * 6.773560047149658
Epoch 100, val loss: 1.7895429134368896
Epoch 110, training loss: 8.484864234924316 = 1.7671571969985962 + 1.0 * 6.71770715713501
Epoch 110, val loss: 1.775773286819458
Epoch 120, training loss: 8.420425415039062 = 1.7525972127914429 + 1.0 * 6.667828559875488
Epoch 120, val loss: 1.7630865573883057
Epoch 130, training loss: 8.362577438354492 = 1.736778736114502 + 1.0 * 6.625799179077148
Epoch 130, val loss: 1.749228835105896
Epoch 140, training loss: 8.31433391571045 = 1.7183784246444702 + 1.0 * 6.5959553718566895
Epoch 140, val loss: 1.7332239151000977
Epoch 150, training loss: 8.268709182739258 = 1.696983814239502 + 1.0 * 6.571725845336914
Epoch 150, val loss: 1.7146780490875244
Epoch 160, training loss: 8.223024368286133 = 1.6716009378433228 + 1.0 * 6.5514235496521
Epoch 160, val loss: 1.692798376083374
Epoch 170, training loss: 8.176878929138184 = 1.641392707824707 + 1.0 * 6.535486221313477
Epoch 170, val loss: 1.6668275594711304
Epoch 180, training loss: 8.126545906066895 = 1.6060365438461304 + 1.0 * 6.520509243011475
Epoch 180, val loss: 1.636443018913269
Epoch 190, training loss: 8.073283195495605 = 1.5656999349594116 + 1.0 * 6.5075836181640625
Epoch 190, val loss: 1.6019494533538818
Epoch 200, training loss: 8.015538215637207 = 1.520575761795044 + 1.0 * 6.494962692260742
Epoch 200, val loss: 1.563554048538208
Epoch 210, training loss: 7.957178115844727 = 1.4713799953460693 + 1.0 * 6.485797882080078
Epoch 210, val loss: 1.5222084522247314
Epoch 220, training loss: 7.89417839050293 = 1.420545220375061 + 1.0 * 6.473633289337158
Epoch 220, val loss: 1.4801899194717407
Epoch 230, training loss: 7.833028793334961 = 1.3691251277923584 + 1.0 * 6.463903903961182
Epoch 230, val loss: 1.4384087324142456
Epoch 240, training loss: 7.775115966796875 = 1.3185182809829712 + 1.0 * 6.456597805023193
Epoch 240, val loss: 1.398118495941162
Epoch 250, training loss: 7.716146469116211 = 1.269710659980774 + 1.0 * 6.446435928344727
Epoch 250, val loss: 1.3601233959197998
Epoch 260, training loss: 7.6599931716918945 = 1.2224068641662598 + 1.0 * 6.437586307525635
Epoch 260, val loss: 1.3239887952804565
Epoch 270, training loss: 7.610161781311035 = 1.176774263381958 + 1.0 * 6.433387279510498
Epoch 270, val loss: 1.2898436784744263
Epoch 280, training loss: 7.560061454772949 = 1.1335748434066772 + 1.0 * 6.426486492156982
Epoch 280, val loss: 1.2580302953720093
Epoch 290, training loss: 7.509657382965088 = 1.0914191007614136 + 1.0 * 6.418238162994385
Epoch 290, val loss: 1.2273133993148804
Epoch 300, training loss: 7.464792728424072 = 1.0497108697891235 + 1.0 * 6.415081977844238
Epoch 300, val loss: 1.1968629360198975
Epoch 310, training loss: 7.418786525726318 = 1.008682370185852 + 1.0 * 6.410104274749756
Epoch 310, val loss: 1.1668280363082886
Epoch 320, training loss: 7.372101783752441 = 0.9681737422943115 + 1.0 * 6.403928279876709
Epoch 320, val loss: 1.1372160911560059
Epoch 330, training loss: 7.328982830047607 = 0.9279837608337402 + 1.0 * 6.400999069213867
Epoch 330, val loss: 1.1076346635818481
Epoch 340, training loss: 7.286236763000488 = 0.8884420990943909 + 1.0 * 6.397794723510742
Epoch 340, val loss: 1.0784262418746948
Epoch 350, training loss: 7.241700172424316 = 0.8496383428573608 + 1.0 * 6.392061710357666
Epoch 350, val loss: 1.049700379371643
Epoch 360, training loss: 7.200170516967773 = 0.8115065097808838 + 1.0 * 6.3886637687683105
Epoch 360, val loss: 1.0214215517044067
Epoch 370, training loss: 7.159773826599121 = 0.7741648554801941 + 1.0 * 6.385609149932861
Epoch 370, val loss: 0.9938395023345947
Epoch 380, training loss: 7.125363349914551 = 0.7379783391952515 + 1.0 * 6.38738489151001
Epoch 380, val loss: 0.9673787355422974
Epoch 390, training loss: 7.0835371017456055 = 0.7030858397483826 + 1.0 * 6.380451202392578
Epoch 390, val loss: 0.9421842694282532
Epoch 400, training loss: 7.0461883544921875 = 0.6694374084472656 + 1.0 * 6.376750946044922
Epoch 400, val loss: 0.9182515740394592
Epoch 410, training loss: 7.022101879119873 = 0.6370589137077332 + 1.0 * 6.385043144226074
Epoch 410, val loss: 0.8956069350242615
Epoch 420, training loss: 6.978603839874268 = 0.6062979698181152 + 1.0 * 6.372305870056152
Epoch 420, val loss: 0.8746464252471924
Epoch 430, training loss: 6.946151256561279 = 0.576967716217041 + 1.0 * 6.369183540344238
Epoch 430, val loss: 0.8553919196128845
Epoch 440, training loss: 6.9222540855407715 = 0.5488665699958801 + 1.0 * 6.373387336730957
Epoch 440, val loss: 0.8375391364097595
Epoch 450, training loss: 6.88891077041626 = 0.5221419334411621 + 1.0 * 6.366768836975098
Epoch 450, val loss: 0.8210797309875488
Epoch 460, training loss: 6.861241340637207 = 0.4964319169521332 + 1.0 * 6.364809513092041
Epoch 460, val loss: 0.8059619665145874
Epoch 470, training loss: 6.832278728485107 = 0.47143444418907166 + 1.0 * 6.360844135284424
Epoch 470, val loss: 0.7919110059738159
Epoch 480, training loss: 6.811334133148193 = 0.44690483808517456 + 1.0 * 6.364429473876953
Epoch 480, val loss: 0.7786741852760315
Epoch 490, training loss: 6.781952857971191 = 0.42297911643981934 + 1.0 * 6.358973979949951
Epoch 490, val loss: 0.7662938833236694
Epoch 500, training loss: 6.756298065185547 = 0.39939233660697937 + 1.0 * 6.356905937194824
Epoch 500, val loss: 0.7546964883804321
Epoch 510, training loss: 6.729218482971191 = 0.37603455781936646 + 1.0 * 6.353183746337891
Epoch 510, val loss: 0.7436481714248657
Epoch 520, training loss: 6.704326152801514 = 0.3528979420661926 + 1.0 * 6.351428031921387
Epoch 520, val loss: 0.7331891655921936
Epoch 530, training loss: 6.683630466461182 = 0.33026638627052307 + 1.0 * 6.353363990783691
Epoch 530, val loss: 0.7232818603515625
Epoch 540, training loss: 6.660360813140869 = 0.3084586262702942 + 1.0 * 6.351902008056641
Epoch 540, val loss: 0.7144355773925781
Epoch 550, training loss: 6.635359764099121 = 0.2874917685985565 + 1.0 * 6.347867965698242
Epoch 550, val loss: 0.7064036130905151
Epoch 560, training loss: 6.622433662414551 = 0.26751768589019775 + 1.0 * 6.354916095733643
Epoch 560, val loss: 0.6991446018218994
Epoch 570, training loss: 6.596480369567871 = 0.24883262813091278 + 1.0 * 6.347647666931152
Epoch 570, val loss: 0.6930249333381653
Epoch 580, training loss: 6.574219226837158 = 0.2313551902770996 + 1.0 * 6.342864036560059
Epoch 580, val loss: 0.6877867579460144
Epoch 590, training loss: 6.5622687339782715 = 0.215076744556427 + 1.0 * 6.34719181060791
Epoch 590, val loss: 0.6832641363143921
Epoch 600, training loss: 6.5465521812438965 = 0.2002720683813095 + 1.0 * 6.346280097961426
Epoch 600, val loss: 0.6796939373016357
Epoch 610, training loss: 6.526113033294678 = 0.18669119477272034 + 1.0 * 6.33942174911499
Epoch 610, val loss: 0.67706698179245
Epoch 620, training loss: 6.510859966278076 = 0.17422989010810852 + 1.0 * 6.336629867553711
Epoch 620, val loss: 0.6751058101654053
Epoch 630, training loss: 6.510542392730713 = 0.16280119121074677 + 1.0 * 6.34774112701416
Epoch 630, val loss: 0.6738376617431641
Epoch 640, training loss: 6.487331867218018 = 0.1524084210395813 + 1.0 * 6.334923267364502
Epoch 640, val loss: 0.6733077168464661
Epoch 650, training loss: 6.475727081298828 = 0.14289934933185577 + 1.0 * 6.332827568054199
Epoch 650, val loss: 0.6734047532081604
Epoch 660, training loss: 6.467083930969238 = 0.13416075706481934 + 1.0 * 6.332923412322998
Epoch 660, val loss: 0.6739519834518433
Epoch 670, training loss: 6.457667350769043 = 0.12615348398685455 + 1.0 * 6.33151388168335
Epoch 670, val loss: 0.6748184561729431
Epoch 680, training loss: 6.44815731048584 = 0.1188148707151413 + 1.0 * 6.329342365264893
Epoch 680, val loss: 0.676299512386322
Epoch 690, training loss: 6.439764499664307 = 0.11203417181968689 + 1.0 * 6.327730178833008
Epoch 690, val loss: 0.6780431866645813
Epoch 700, training loss: 6.441155433654785 = 0.10576976835727692 + 1.0 * 6.335385799407959
Epoch 700, val loss: 0.6799430847167969
Epoch 710, training loss: 6.424793243408203 = 0.10001707822084427 + 1.0 * 6.3247761726379395
Epoch 710, val loss: 0.6823179721832275
Epoch 720, training loss: 6.419015884399414 = 0.09467387944459915 + 1.0 * 6.324341773986816
Epoch 720, val loss: 0.6849768757820129
Epoch 730, training loss: 6.415928363800049 = 0.08968999981880188 + 1.0 * 6.32623815536499
Epoch 730, val loss: 0.6877113580703735
Epoch 740, training loss: 6.4102067947387695 = 0.08508153259754181 + 1.0 * 6.325125217437744
Epoch 740, val loss: 0.6906000375747681
Epoch 750, training loss: 6.402490139007568 = 0.08079873025417328 + 1.0 * 6.321691513061523
Epoch 750, val loss: 0.6937828063964844
Epoch 760, training loss: 6.396687984466553 = 0.07680334150791168 + 1.0 * 6.319884777069092
Epoch 760, val loss: 0.697063148021698
Epoch 770, training loss: 6.400356292724609 = 0.07305985689163208 + 1.0 * 6.327296257019043
Epoch 770, val loss: 0.7003689408302307
Epoch 780, training loss: 6.390156269073486 = 0.0695856511592865 + 1.0 * 6.320570468902588
Epoch 780, val loss: 0.7039713263511658
Epoch 790, training loss: 6.384893417358398 = 0.06631962209939957 + 1.0 * 6.318573951721191
Epoch 790, val loss: 0.7075849175453186
Epoch 800, training loss: 6.378450393676758 = 0.06326630711555481 + 1.0 * 6.315184116363525
Epoch 800, val loss: 0.7112881541252136
Epoch 810, training loss: 6.377206802368164 = 0.06039175018668175 + 1.0 * 6.31681489944458
Epoch 810, val loss: 0.7150914072990417
Epoch 820, training loss: 6.373534202575684 = 0.057692963629961014 + 1.0 * 6.315841197967529
Epoch 820, val loss: 0.718909740447998
Epoch 830, training loss: 6.370226860046387 = 0.055176734924316406 + 1.0 * 6.31505012512207
Epoch 830, val loss: 0.7228293418884277
Epoch 840, training loss: 6.365782260894775 = 0.05280674621462822 + 1.0 * 6.3129754066467285
Epoch 840, val loss: 0.7269117832183838
Epoch 850, training loss: 6.364190578460693 = 0.05057452246546745 + 1.0 * 6.3136162757873535
Epoch 850, val loss: 0.7309117913246155
Epoch 860, training loss: 6.3614912033081055 = 0.048467494547367096 + 1.0 * 6.313023567199707
Epoch 860, val loss: 0.734840989112854
Epoch 870, training loss: 6.355105876922607 = 0.04648906737565994 + 1.0 * 6.308616638183594
Epoch 870, val loss: 0.7389723062515259
Epoch 880, training loss: 6.359506130218506 = 0.04462117329239845 + 1.0 * 6.314885139465332
Epoch 880, val loss: 0.7430269122123718
Epoch 890, training loss: 6.352228164672852 = 0.042868681252002716 + 1.0 * 6.309359550476074
Epoch 890, val loss: 0.7471229434013367
Epoch 900, training loss: 6.348110198974609 = 0.04120743274688721 + 1.0 * 6.306902885437012
Epoch 900, val loss: 0.751236081123352
Epoch 910, training loss: 6.344863414764404 = 0.03963537514209747 + 1.0 * 6.305228233337402
Epoch 910, val loss: 0.755462646484375
Epoch 920, training loss: 6.351125717163086 = 0.03813982382416725 + 1.0 * 6.312985897064209
Epoch 920, val loss: 0.7595078945159912
Epoch 930, training loss: 6.353665351867676 = 0.036739934235811234 + 1.0 * 6.316925525665283
Epoch 930, val loss: 0.7635054588317871
Epoch 940, training loss: 6.341995716094971 = 0.035413578152656555 + 1.0 * 6.306581974029541
Epoch 940, val loss: 0.7676039338111877
Epoch 950, training loss: 6.33635950088501 = 0.03416213020682335 + 1.0 * 6.302197456359863
Epoch 950, val loss: 0.7717640995979309
Epoch 960, training loss: 6.333927154541016 = 0.03296569362282753 + 1.0 * 6.300961494445801
Epoch 960, val loss: 0.775722324848175
Epoch 970, training loss: 6.3362274169921875 = 0.03182495757937431 + 1.0 * 6.3044023513793945
Epoch 970, val loss: 0.7796926498413086
Epoch 980, training loss: 6.330536842346191 = 0.0307431910187006 + 1.0 * 6.299793720245361
Epoch 980, val loss: 0.7836677432060242
Epoch 990, training loss: 6.33328914642334 = 0.029715677723288536 + 1.0 * 6.3035736083984375
Epoch 990, val loss: 0.7876483201980591
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 10.547357559204102 = 1.9505468606948853 + 1.0 * 8.596810340881348
Epoch 0, val loss: 1.9492616653442383
Epoch 10, training loss: 10.53612232208252 = 1.9397069215774536 + 1.0 * 8.596415519714355
Epoch 10, val loss: 1.9378538131713867
Epoch 20, training loss: 10.51947021484375 = 1.9263367652893066 + 1.0 * 8.593133926391602
Epoch 20, val loss: 1.9236265420913696
Epoch 30, training loss: 10.474471092224121 = 1.90786874294281 + 1.0 * 8.56660270690918
Epoch 30, val loss: 1.904038906097412
Epoch 40, training loss: 10.261537551879883 = 1.8846840858459473 + 1.0 * 8.376852989196777
Epoch 40, val loss: 1.8804543018341064
Epoch 50, training loss: 9.740314483642578 = 1.8609727621078491 + 1.0 * 7.8793416023254395
Epoch 50, val loss: 1.8577256202697754
Epoch 60, training loss: 9.305500030517578 = 1.8433866500854492 + 1.0 * 7.462112903594971
Epoch 60, val loss: 1.8419636487960815
Epoch 70, training loss: 8.945764541625977 = 1.8317227363586426 + 1.0 * 7.114041328430176
Epoch 70, val loss: 1.831671953201294
Epoch 80, training loss: 8.753908157348633 = 1.8190104961395264 + 1.0 * 6.934897422790527
Epoch 80, val loss: 1.8199418783187866
Epoch 90, training loss: 8.61866569519043 = 1.8028666973114014 + 1.0 * 6.815798759460449
Epoch 90, val loss: 1.8053085803985596
Epoch 100, training loss: 8.535859107971191 = 1.7874126434326172 + 1.0 * 6.748446464538574
Epoch 100, val loss: 1.7918723821640015
Epoch 110, training loss: 8.475885391235352 = 1.7736866474151611 + 1.0 * 6.7021989822387695
Epoch 110, val loss: 1.7804479598999023
Epoch 120, training loss: 8.424623489379883 = 1.7600820064544678 + 1.0 * 6.664541244506836
Epoch 120, val loss: 1.7689651250839233
Epoch 130, training loss: 8.381025314331055 = 1.7452449798583984 + 1.0 * 6.635780334472656
Epoch 130, val loss: 1.7560657262802124
Epoch 140, training loss: 8.338966369628906 = 1.728794813156128 + 1.0 * 6.610171318054199
Epoch 140, val loss: 1.7415696382522583
Epoch 150, training loss: 8.296211242675781 = 1.7100534439086914 + 1.0 * 6.586157321929932
Epoch 150, val loss: 1.725180983543396
Epoch 160, training loss: 8.255300521850586 = 1.6884689331054688 + 1.0 * 6.566831588745117
Epoch 160, val loss: 1.7065876722335815
Epoch 170, training loss: 8.208086967468262 = 1.6638797521591187 + 1.0 * 6.544207572937012
Epoch 170, val loss: 1.685505747795105
Epoch 180, training loss: 8.16062068939209 = 1.6356323957443237 + 1.0 * 6.524988174438477
Epoch 180, val loss: 1.6613930463790894
Epoch 190, training loss: 8.11147403717041 = 1.6030229330062866 + 1.0 * 6.508451461791992
Epoch 190, val loss: 1.6334706544876099
Epoch 200, training loss: 8.061615943908691 = 1.566115140914917 + 1.0 * 6.495500564575195
Epoch 200, val loss: 1.6021087169647217
Epoch 210, training loss: 8.006631851196289 = 1.525267481803894 + 1.0 * 6.4813642501831055
Epoch 210, val loss: 1.5674715042114258
Epoch 220, training loss: 7.9501953125 = 1.480324625968933 + 1.0 * 6.469870567321777
Epoch 220, val loss: 1.5295878648757935
Epoch 230, training loss: 7.892243385314941 = 1.4320305585861206 + 1.0 * 6.460212707519531
Epoch 230, val loss: 1.489259958267212
Epoch 240, training loss: 7.837267875671387 = 1.3823283910751343 + 1.0 * 6.454939365386963
Epoch 240, val loss: 1.4480170011520386
Epoch 250, training loss: 7.776713848114014 = 1.330939769744873 + 1.0 * 6.445774078369141
Epoch 250, val loss: 1.4056217670440674
Epoch 260, training loss: 7.716242790222168 = 1.2779884338378906 + 1.0 * 6.438254356384277
Epoch 260, val loss: 1.3624831438064575
Epoch 270, training loss: 7.657246112823486 = 1.2242757081985474 + 1.0 * 6.4329705238342285
Epoch 270, val loss: 1.319245457649231
Epoch 280, training loss: 7.599119186401367 = 1.1709848642349243 + 1.0 * 6.428134441375732
Epoch 280, val loss: 1.2767431735992432
Epoch 290, training loss: 7.544424533843994 = 1.1183924674987793 + 1.0 * 6.426032066345215
Epoch 290, val loss: 1.2354788780212402
Epoch 300, training loss: 7.484862804412842 = 1.0675145387649536 + 1.0 * 6.417348384857178
Epoch 300, val loss: 1.196130394935608
Epoch 310, training loss: 7.43093729019165 = 1.0186152458190918 + 1.0 * 6.412322044372559
Epoch 310, val loss: 1.1589094400405884
Epoch 320, training loss: 7.390883922576904 = 0.9719029664993286 + 1.0 * 6.418981075286865
Epoch 320, val loss: 1.1238616704940796
Epoch 330, training loss: 7.333604335784912 = 0.9283438324928284 + 1.0 * 6.4052605628967285
Epoch 330, val loss: 1.0914386510849
Epoch 340, training loss: 7.288715362548828 = 0.8873760104179382 + 1.0 * 6.401339530944824
Epoch 340, val loss: 1.0614376068115234
Epoch 350, training loss: 7.2479023933410645 = 0.8490874171257019 + 1.0 * 6.398815155029297
Epoch 350, val loss: 1.0336942672729492
Epoch 360, training loss: 7.203662872314453 = 0.8132715821266174 + 1.0 * 6.3903913497924805
Epoch 360, val loss: 1.008122444152832
Epoch 370, training loss: 7.165795803070068 = 0.779393196105957 + 1.0 * 6.386402606964111
Epoch 370, val loss: 0.9843500256538391
Epoch 380, training loss: 7.144800662994385 = 0.747378945350647 + 1.0 * 6.397421836853027
Epoch 380, val loss: 0.9623292088508606
Epoch 390, training loss: 7.0994977951049805 = 0.7178056240081787 + 1.0 * 6.381692409515381
Epoch 390, val loss: 0.9424061179161072
Epoch 400, training loss: 7.066893577575684 = 0.6897732615470886 + 1.0 * 6.377120494842529
Epoch 400, val loss: 0.9240792989730835
Epoch 410, training loss: 7.044098854064941 = 0.6629422307014465 + 1.0 * 6.3811564445495605
Epoch 410, val loss: 0.9070112705230713
Epoch 420, training loss: 7.007897853851318 = 0.6373695731163025 + 1.0 * 6.370528221130371
Epoch 420, val loss: 0.8911157846450806
Epoch 430, training loss: 6.980376720428467 = 0.6126713156700134 + 1.0 * 6.367705345153809
Epoch 430, val loss: 0.8763526082038879
Epoch 440, training loss: 6.961664199829102 = 0.5887384414672852 + 1.0 * 6.372925758361816
Epoch 440, val loss: 0.8625743389129639
Epoch 450, training loss: 6.931090354919434 = 0.5658290386199951 + 1.0 * 6.365261554718018
Epoch 450, val loss: 0.8498152494430542
Epoch 460, training loss: 6.904115200042725 = 0.5435920357704163 + 1.0 * 6.360523223876953
Epoch 460, val loss: 0.838121771812439
Epoch 470, training loss: 6.88651704788208 = 0.5219593048095703 + 1.0 * 6.36455774307251
Epoch 470, val loss: 0.8272799253463745
Epoch 480, training loss: 6.858953952789307 = 0.5010424256324768 + 1.0 * 6.357911586761475
Epoch 480, val loss: 0.817419171333313
Epoch 490, training loss: 6.836357116699219 = 0.48069697618484497 + 1.0 * 6.3556599617004395
Epoch 490, val loss: 0.8083890080451965
Epoch 500, training loss: 6.81809139251709 = 0.4609467387199402 + 1.0 * 6.357144832611084
Epoch 500, val loss: 0.8002129197120667
Epoch 510, training loss: 6.79203987121582 = 0.4418725371360779 + 1.0 * 6.350167274475098
Epoch 510, val loss: 0.7928820848464966
Epoch 520, training loss: 6.781103134155273 = 0.42335402965545654 + 1.0 * 6.357748985290527
Epoch 520, val loss: 0.7864686250686646
Epoch 530, training loss: 6.752908229827881 = 0.4054655432701111 + 1.0 * 6.347442626953125
Epoch 530, val loss: 0.7806463241577148
Epoch 540, training loss: 6.732419967651367 = 0.38803935050964355 + 1.0 * 6.344380855560303
Epoch 540, val loss: 0.7755774855613708
Epoch 550, training loss: 6.71348762512207 = 0.37101781368255615 + 1.0 * 6.342469692230225
Epoch 550, val loss: 0.7709972262382507
Epoch 560, training loss: 6.705467700958252 = 0.35448941588401794 + 1.0 * 6.350978374481201
Epoch 560, val loss: 0.7669440507888794
Epoch 570, training loss: 6.680647373199463 = 0.3385619819164276 + 1.0 * 6.342085361480713
Epoch 570, val loss: 0.7634714245796204
Epoch 580, training loss: 6.6609039306640625 = 0.32304683327674866 + 1.0 * 6.337857246398926
Epoch 580, val loss: 0.7604929208755493
Epoch 590, training loss: 6.656298637390137 = 0.3079538643360138 + 1.0 * 6.348344802856445
Epoch 590, val loss: 0.757832407951355
Epoch 600, training loss: 6.628631591796875 = 0.29337918758392334 + 1.0 * 6.335252285003662
Epoch 600, val loss: 0.7557325959205627
Epoch 610, training loss: 6.61147928237915 = 0.2792791426181793 + 1.0 * 6.332200050354004
Epoch 610, val loss: 0.7541245222091675
Epoch 620, training loss: 6.59922981262207 = 0.2656320333480835 + 1.0 * 6.333597660064697
Epoch 620, val loss: 0.7528607845306396
Epoch 630, training loss: 6.592741012573242 = 0.252514123916626 + 1.0 * 6.340226650238037
Epoch 630, val loss: 0.7519209980964661
Epoch 640, training loss: 6.57078742980957 = 0.24003905057907104 + 1.0 * 6.330748558044434
Epoch 640, val loss: 0.7515934109687805
Epoch 650, training loss: 6.555376052856445 = 0.22808051109313965 + 1.0 * 6.327295303344727
Epoch 650, val loss: 0.7516818642616272
Epoch 660, training loss: 6.554977893829346 = 0.21662844717502594 + 1.0 * 6.338349342346191
Epoch 660, val loss: 0.7521193623542786
Epoch 670, training loss: 6.534398078918457 = 0.2057626098394394 + 1.0 * 6.3286356925964355
Epoch 670, val loss: 0.7528612017631531
Epoch 680, training loss: 6.519561290740967 = 0.19540593028068542 + 1.0 * 6.324155330657959
Epoch 680, val loss: 0.7542049288749695
Epoch 690, training loss: 6.518776893615723 = 0.18555596470832825 + 1.0 * 6.333220958709717
Epoch 690, val loss: 0.7557685375213623
Epoch 700, training loss: 6.501833438873291 = 0.17623017728328705 + 1.0 * 6.325603485107422
Epoch 700, val loss: 0.7577098608016968
Epoch 710, training loss: 6.4879069328308105 = 0.16739556193351746 + 1.0 * 6.320511341094971
Epoch 710, val loss: 0.7601098418235779
Epoch 720, training loss: 6.489233493804932 = 0.15900853276252747 + 1.0 * 6.330224990844727
Epoch 720, val loss: 0.7628035545349121
Epoch 730, training loss: 6.4761481285095215 = 0.15115077793598175 + 1.0 * 6.324997425079346
Epoch 730, val loss: 0.7656757831573486
Epoch 740, training loss: 6.4636359214782715 = 0.14376208186149597 + 1.0 * 6.319873809814453
Epoch 740, val loss: 0.7689945697784424
Epoch 750, training loss: 6.451712608337402 = 0.13675430417060852 + 1.0 * 6.314958095550537
Epoch 750, val loss: 0.772537887096405
Epoch 760, training loss: 6.444772720336914 = 0.1301184892654419 + 1.0 * 6.314654350280762
Epoch 760, val loss: 0.7762766480445862
Epoch 770, training loss: 6.440708637237549 = 0.12385330349206924 + 1.0 * 6.316855430603027
Epoch 770, val loss: 0.7802143692970276
Epoch 780, training loss: 6.434906959533691 = 0.11797573417425156 + 1.0 * 6.316931247711182
Epoch 780, val loss: 0.7843170762062073
Epoch 790, training loss: 6.4231672286987305 = 0.11245083808898926 + 1.0 * 6.310716152191162
Epoch 790, val loss: 0.7887028455734253
Epoch 800, training loss: 6.417626857757568 = 0.10722709447145462 + 1.0 * 6.310399532318115
Epoch 800, val loss: 0.7932308912277222
Epoch 810, training loss: 6.420274257659912 = 0.10228049010038376 + 1.0 * 6.317993640899658
Epoch 810, val loss: 0.7977946996688843
Epoch 820, training loss: 6.410462379455566 = 0.09763984382152557 + 1.0 * 6.312822341918945
Epoch 820, val loss: 0.8024429082870483
Epoch 830, training loss: 6.403406143188477 = 0.0932447612285614 + 1.0 * 6.310161590576172
Epoch 830, val loss: 0.8073281645774841
Epoch 840, training loss: 6.398383617401123 = 0.0891069695353508 + 1.0 * 6.309276580810547
Epoch 840, val loss: 0.8122801780700684
Epoch 850, training loss: 6.393533706665039 = 0.08518990129232407 + 1.0 * 6.308343887329102
Epoch 850, val loss: 0.8172873258590698
Epoch 860, training loss: 6.39402961730957 = 0.0814981535077095 + 1.0 * 6.312531471252441
Epoch 860, val loss: 0.8224090337753296
Epoch 870, training loss: 6.384300231933594 = 0.07800807803869247 + 1.0 * 6.3062920570373535
Epoch 870, val loss: 0.8275753855705261
Epoch 880, training loss: 6.383016109466553 = 0.07471595704555511 + 1.0 * 6.308300018310547
Epoch 880, val loss: 0.8327568173408508
Epoch 890, training loss: 6.374367713928223 = 0.07160452753305435 + 1.0 * 6.302762985229492
Epoch 890, val loss: 0.8379713296890259
Epoch 900, training loss: 6.369972229003906 = 0.06865746527910233 + 1.0 * 6.301314830780029
Epoch 900, val loss: 0.8433035016059875
Epoch 910, training loss: 6.374044895172119 = 0.06586632132530212 + 1.0 * 6.308178424835205
Epoch 910, val loss: 0.8485797047615051
Epoch 920, training loss: 6.370187759399414 = 0.06323038786649704 + 1.0 * 6.306957244873047
Epoch 920, val loss: 0.8537245392799377
Epoch 930, training loss: 6.359269142150879 = 0.060742903500795364 + 1.0 * 6.298526287078857
Epoch 930, val loss: 0.8591158390045166
Epoch 940, training loss: 6.355676651000977 = 0.05837763473391533 + 1.0 * 6.297298908233643
Epoch 940, val loss: 0.8644015192985535
Epoch 950, training loss: 6.3623151779174805 = 0.05613591521978378 + 1.0 * 6.306179046630859
Epoch 950, val loss: 0.8696205019950867
Epoch 960, training loss: 6.3571577072143555 = 0.05402454733848572 + 1.0 * 6.303133010864258
Epoch 960, val loss: 0.874846339225769
Epoch 970, training loss: 6.3504204750061035 = 0.05201132595539093 + 1.0 * 6.2984089851379395
Epoch 970, val loss: 0.8801178336143494
Epoch 980, training loss: 6.348405838012695 = 0.05011332035064697 + 1.0 * 6.298292636871338
Epoch 980, val loss: 0.8852431774139404
Epoch 990, training loss: 6.3423542976379395 = 0.04830968752503395 + 1.0 * 6.294044494628906
Epoch 990, val loss: 0.8904154300689697
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8392198207696363
The final CL Acc:0.81235, 0.00924, The final GNN Acc:0.83746, 0.00151
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11562])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10480])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.517407417297363 = 1.9205410480499268 + 1.0 * 8.596866607666016
Epoch 0, val loss: 1.9165568351745605
Epoch 10, training loss: 10.508673667907715 = 1.911965012550354 + 1.0 * 8.596708297729492
Epoch 10, val loss: 1.9085862636566162
Epoch 20, training loss: 10.49695873260498 = 1.901394009590149 + 1.0 * 8.595564842224121
Epoch 20, val loss: 1.8983991146087646
Epoch 30, training loss: 10.473142623901367 = 1.886866807937622 + 1.0 * 8.586276054382324
Epoch 30, val loss: 1.88425612449646
Epoch 40, training loss: 10.3925199508667 = 1.8670417070388794 + 1.0 * 8.52547836303711
Epoch 40, val loss: 1.8653548955917358
Epoch 50, training loss: 10.003364562988281 = 1.8442975282669067 + 1.0 * 8.159067153930664
Epoch 50, val loss: 1.844220519065857
Epoch 60, training loss: 9.521079063415527 = 1.8240466117858887 + 1.0 * 7.697032451629639
Epoch 60, val loss: 1.8265928030014038
Epoch 70, training loss: 9.135367393493652 = 1.8104031085968018 + 1.0 * 7.32496452331543
Epoch 70, val loss: 1.8138350248336792
Epoch 80, training loss: 8.922405242919922 = 1.7966372966766357 + 1.0 * 7.125768184661865
Epoch 80, val loss: 1.80111825466156
Epoch 90, training loss: 8.802621841430664 = 1.7808631658554077 + 1.0 * 7.021759033203125
Epoch 90, val loss: 1.7874552011489868
Epoch 100, training loss: 8.702205657958984 = 1.7641581296920776 + 1.0 * 6.938047885894775
Epoch 100, val loss: 1.7734779119491577
Epoch 110, training loss: 8.606703758239746 = 1.7474539279937744 + 1.0 * 6.859249591827393
Epoch 110, val loss: 1.759644865989685
Epoch 120, training loss: 8.534395217895508 = 1.7288763523101807 + 1.0 * 6.805519104003906
Epoch 120, val loss: 1.7442206144332886
Epoch 130, training loss: 8.463967323303223 = 1.7070848941802979 + 1.0 * 6.756882190704346
Epoch 130, val loss: 1.726113200187683
Epoch 140, training loss: 8.403215408325195 = 1.6816701889038086 + 1.0 * 6.721545219421387
Epoch 140, val loss: 1.705222249031067
Epoch 150, training loss: 8.343912124633789 = 1.6517428159713745 + 1.0 * 6.692169189453125
Epoch 150, val loss: 1.6808078289031982
Epoch 160, training loss: 8.281558990478516 = 1.6169023513793945 + 1.0 * 6.664656639099121
Epoch 160, val loss: 1.6529160737991333
Epoch 170, training loss: 8.217745780944824 = 1.5771543979644775 + 1.0 * 6.640591621398926
Epoch 170, val loss: 1.621208906173706
Epoch 180, training loss: 8.153271675109863 = 1.5330913066864014 + 1.0 * 6.620180606842041
Epoch 180, val loss: 1.5863481760025024
Epoch 190, training loss: 8.08669662475586 = 1.4858169555664062 + 1.0 * 6.600880146026611
Epoch 190, val loss: 1.5493345260620117
Epoch 200, training loss: 8.020323753356934 = 1.4357788562774658 + 1.0 * 6.584545135498047
Epoch 200, val loss: 1.5102980136871338
Epoch 210, training loss: 7.956768989562988 = 1.383939504623413 + 1.0 * 6.572829723358154
Epoch 210, val loss: 1.4702876806259155
Epoch 220, training loss: 7.892085552215576 = 1.3323248624801636 + 1.0 * 6.559760570526123
Epoch 220, val loss: 1.430978775024414
Epoch 230, training loss: 7.8296380043029785 = 1.281006932258606 + 1.0 * 6.548631191253662
Epoch 230, val loss: 1.3925553560256958
Epoch 240, training loss: 7.768128395080566 = 1.2303167581558228 + 1.0 * 6.537811756134033
Epoch 240, val loss: 1.3553740978240967
Epoch 250, training loss: 7.718916893005371 = 1.1809709072113037 + 1.0 * 6.537945747375488
Epoch 250, val loss: 1.3199037313461304
Epoch 260, training loss: 7.6551923751831055 = 1.1344002485275269 + 1.0 * 6.520792007446289
Epoch 260, val loss: 1.2869433164596558
Epoch 270, training loss: 7.602063179016113 = 1.0896912813186646 + 1.0 * 6.512372016906738
Epoch 270, val loss: 1.2556501626968384
Epoch 280, training loss: 7.551248073577881 = 1.046174168586731 + 1.0 * 6.5050740242004395
Epoch 280, val loss: 1.2254886627197266
Epoch 290, training loss: 7.508677959442139 = 1.0039957761764526 + 1.0 * 6.5046820640563965
Epoch 290, val loss: 1.196486473083496
Epoch 300, training loss: 7.454677581787109 = 0.9632817506790161 + 1.0 * 6.491395950317383
Epoch 300, val loss: 1.1687343120574951
Epoch 310, training loss: 7.4087324142456055 = 0.9233028888702393 + 1.0 * 6.485429286956787
Epoch 310, val loss: 1.1416704654693604
Epoch 320, training loss: 7.362417697906494 = 0.8837456107139587 + 1.0 * 6.478672027587891
Epoch 320, val loss: 1.1152279376983643
Epoch 330, training loss: 7.329115867614746 = 0.8446648120880127 + 1.0 * 6.4844512939453125
Epoch 330, val loss: 1.0895428657531738
Epoch 340, training loss: 7.279158115386963 = 0.8069857358932495 + 1.0 * 6.472172260284424
Epoch 340, val loss: 1.0652985572814941
Epoch 350, training loss: 7.23206901550293 = 0.7704582214355469 + 1.0 * 6.461610794067383
Epoch 350, val loss: 1.0426987409591675
Epoch 360, training loss: 7.195584774017334 = 0.734948992729187 + 1.0 * 6.460635662078857
Epoch 360, val loss: 1.0215723514556885
Epoch 370, training loss: 7.155160903930664 = 0.7010225057601929 + 1.0 * 6.454138278961182
Epoch 370, val loss: 1.0024772882461548
Epoch 380, training loss: 7.116797924041748 = 0.6686612367630005 + 1.0 * 6.448136806488037
Epoch 380, val loss: 0.9854220747947693
Epoch 390, training loss: 7.080455780029297 = 0.637328565120697 + 1.0 * 6.443127155303955
Epoch 390, val loss: 0.9701064229011536
Epoch 400, training loss: 7.04572057723999 = 0.6068387627601624 + 1.0 * 6.438881874084473
Epoch 400, val loss: 0.9564681649208069
Epoch 410, training loss: 7.024721145629883 = 0.5771769881248474 + 1.0 * 6.447544097900391
Epoch 410, val loss: 0.9445323944091797
Epoch 420, training loss: 6.9820942878723145 = 0.5487518906593323 + 1.0 * 6.433342456817627
Epoch 420, val loss: 0.9344043731689453
Epoch 430, training loss: 6.948858261108398 = 0.5212206244468689 + 1.0 * 6.427637577056885
Epoch 430, val loss: 0.925993800163269
Epoch 440, training loss: 6.918749809265137 = 0.4944509267807007 + 1.0 * 6.4242987632751465
Epoch 440, val loss: 0.9190763235092163
Epoch 450, training loss: 6.890666961669922 = 0.4684586524963379 + 1.0 * 6.422208309173584
Epoch 450, val loss: 0.9137170910835266
Epoch 460, training loss: 6.8730597496032715 = 0.44354042410850525 + 1.0 * 6.429519176483154
Epoch 460, val loss: 0.909879207611084
Epoch 470, training loss: 6.836024761199951 = 0.4199204742908478 + 1.0 * 6.416104316711426
Epoch 470, val loss: 0.907731294631958
Epoch 480, training loss: 6.809454441070557 = 0.3972296714782715 + 1.0 * 6.412224769592285
Epoch 480, val loss: 0.9070252180099487
Epoch 490, training loss: 6.784080982208252 = 0.375378280878067 + 1.0 * 6.408702850341797
Epoch 490, val loss: 0.9076786041259766
Epoch 500, training loss: 6.765136241912842 = 0.3543890714645386 + 1.0 * 6.410747051239014
Epoch 500, val loss: 0.9096647500991821
Epoch 510, training loss: 6.738803863525391 = 0.33438295125961304 + 1.0 * 6.404420852661133
Epoch 510, val loss: 0.9127013683319092
Epoch 520, training loss: 6.716217041015625 = 0.31537744402885437 + 1.0 * 6.400839805603027
Epoch 520, val loss: 0.9168968200683594
Epoch 530, training loss: 6.696183681488037 = 0.2972298264503479 + 1.0 * 6.398953914642334
Epoch 530, val loss: 0.9220731258392334
Epoch 540, training loss: 6.682129383087158 = 0.27991563081741333 + 1.0 * 6.4022135734558105
Epoch 540, val loss: 0.9280830025672913
Epoch 550, training loss: 6.663799285888672 = 0.26350823044776917 + 1.0 * 6.4002909660339355
Epoch 550, val loss: 0.9349443912506104
Epoch 560, training loss: 6.641605377197266 = 0.2480001449584961 + 1.0 * 6.3936052322387695
Epoch 560, val loss: 0.9423942565917969
Epoch 570, training loss: 6.6250762939453125 = 0.2334086000919342 + 1.0 * 6.39166784286499
Epoch 570, val loss: 0.9506428837776184
Epoch 580, training loss: 6.60740327835083 = 0.2195950597524643 + 1.0 * 6.387808322906494
Epoch 580, val loss: 0.9594001173973083
Epoch 590, training loss: 6.598941802978516 = 0.20655879378318787 + 1.0 * 6.392383098602295
Epoch 590, val loss: 0.9686577916145325
Epoch 600, training loss: 6.582154273986816 = 0.19434380531311035 + 1.0 * 6.387810707092285
Epoch 600, val loss: 0.9782742261886597
Epoch 610, training loss: 6.566685676574707 = 0.18288742005825043 + 1.0 * 6.383798122406006
Epoch 610, val loss: 0.9882344603538513
Epoch 620, training loss: 6.550898551940918 = 0.17214836180210114 + 1.0 * 6.378750324249268
Epoch 620, val loss: 0.9985405802726746
Epoch 630, training loss: 6.540879726409912 = 0.1620660126209259 + 1.0 * 6.378813743591309
Epoch 630, val loss: 1.0090477466583252
Epoch 640, training loss: 6.537881851196289 = 0.15265057981014252 + 1.0 * 6.3852314949035645
Epoch 640, val loss: 1.0197070837020874
Epoch 650, training loss: 6.518472194671631 = 0.14393317699432373 + 1.0 * 6.374538898468018
Epoch 650, val loss: 1.0304549932479858
Epoch 660, training loss: 6.5081939697265625 = 0.13579954206943512 + 1.0 * 6.372394561767578
Epoch 660, val loss: 1.0414495468139648
Epoch 670, training loss: 6.4987473487854 = 0.12819500267505646 + 1.0 * 6.3705525398254395
Epoch 670, val loss: 1.0525627136230469
Epoch 680, training loss: 6.497081756591797 = 0.1210850328207016 + 1.0 * 6.3759965896606445
Epoch 680, val loss: 1.063683032989502
Epoch 690, training loss: 6.481900691986084 = 0.11447376012802124 + 1.0 * 6.367426872253418
Epoch 690, val loss: 1.074771523475647
Epoch 700, training loss: 6.474745273590088 = 0.10832404345273972 + 1.0 * 6.366421222686768
Epoch 700, val loss: 1.085971713066101
Epoch 710, training loss: 6.467345714569092 = 0.1025744155049324 + 1.0 * 6.364771366119385
Epoch 710, val loss: 1.0971617698669434
Epoch 720, training loss: 6.462045669555664 = 0.09720783680677414 + 1.0 * 6.364837646484375
Epoch 720, val loss: 1.1081442832946777
Epoch 730, training loss: 6.452205181121826 = 0.09221144020557404 + 1.0 * 6.359993934631348
Epoch 730, val loss: 1.1190576553344727
Epoch 740, training loss: 6.445813179016113 = 0.08754348754882812 + 1.0 * 6.358269691467285
Epoch 740, val loss: 1.1299409866333008
Epoch 750, training loss: 6.447081089019775 = 0.08317386358976364 + 1.0 * 6.363907337188721
Epoch 750, val loss: 1.1407568454742432
Epoch 760, training loss: 6.439695835113525 = 0.07908926159143448 + 1.0 * 6.360606670379639
Epoch 760, val loss: 1.151389718055725
Epoch 770, training loss: 6.431671619415283 = 0.07527507096529007 + 1.0 * 6.356396675109863
Epoch 770, val loss: 1.1619497537612915
Epoch 780, training loss: 6.424604415893555 = 0.07170097529888153 + 1.0 * 6.352903366088867
Epoch 780, val loss: 1.1724207401275635
Epoch 790, training loss: 6.4209794998168945 = 0.0683540478348732 + 1.0 * 6.352625370025635
Epoch 790, val loss: 1.1827737092971802
Epoch 800, training loss: 6.424105167388916 = 0.06521802395582199 + 1.0 * 6.358887195587158
Epoch 800, val loss: 1.192881464958191
Epoch 810, training loss: 6.412343502044678 = 0.06227831169962883 + 1.0 * 6.350065231323242
Epoch 810, val loss: 1.2028015851974487
Epoch 820, training loss: 6.4073896408081055 = 0.05952734872698784 + 1.0 * 6.347862243652344
Epoch 820, val loss: 1.2127267122268677
Epoch 830, training loss: 6.402554035186768 = 0.056936200708150864 + 1.0 * 6.345617771148682
Epoch 830, val loss: 1.2225701808929443
Epoch 840, training loss: 6.406783103942871 = 0.054494261741638184 + 1.0 * 6.352288722991943
Epoch 840, val loss: 1.2322309017181396
Epoch 850, training loss: 6.396994590759277 = 0.052187565714120865 + 1.0 * 6.344807147979736
Epoch 850, val loss: 1.2415324449539185
Epoch 860, training loss: 6.395692348480225 = 0.05002395808696747 + 1.0 * 6.345668315887451
Epoch 860, val loss: 1.2509410381317139
Epoch 870, training loss: 6.3884196281433105 = 0.04798059165477753 + 1.0 * 6.3404388427734375
Epoch 870, val loss: 1.2600313425064087
Epoch 880, training loss: 6.387197494506836 = 0.04605882987380028 + 1.0 * 6.34113883972168
Epoch 880, val loss: 1.2690647840499878
Epoch 890, training loss: 6.38330602645874 = 0.04424139857292175 + 1.0 * 6.339064598083496
Epoch 890, val loss: 1.27789306640625
Epoch 900, training loss: 6.384683609008789 = 0.04252500459551811 + 1.0 * 6.342158794403076
Epoch 900, val loss: 1.286543369293213
Epoch 910, training loss: 6.3772196769714355 = 0.04090448468923569 + 1.0 * 6.336315155029297
Epoch 910, val loss: 1.2950923442840576
Epoch 920, training loss: 6.374862194061279 = 0.039371684193611145 + 1.0 * 6.335490703582764
Epoch 920, val loss: 1.3035025596618652
Epoch 930, training loss: 6.376594543457031 = 0.037919651716947556 + 1.0 * 6.338675022125244
Epoch 930, val loss: 1.3119244575500488
Epoch 940, training loss: 6.373657703399658 = 0.036540742963552475 + 1.0 * 6.3371171951293945
Epoch 940, val loss: 1.3197972774505615
Epoch 950, training loss: 6.368447780609131 = 0.03523975610733032 + 1.0 * 6.333208084106445
Epoch 950, val loss: 1.3278330564498901
Epoch 960, training loss: 6.371934413909912 = 0.03400709852576256 + 1.0 * 6.337927341461182
Epoch 960, val loss: 1.335753321647644
Epoch 970, training loss: 6.363682746887207 = 0.0328315794467926 + 1.0 * 6.330851078033447
Epoch 970, val loss: 1.3433350324630737
Epoch 980, training loss: 6.3605637550354 = 0.03171798959374428 + 1.0 * 6.328845977783203
Epoch 980, val loss: 1.3509721755981445
Epoch 990, training loss: 6.362527370452881 = 0.030657628551125526 + 1.0 * 6.331869602203369
Epoch 990, val loss: 1.3585237264633179
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 10.535991668701172 = 1.9391162395477295 + 1.0 * 8.596875190734863
Epoch 0, val loss: 1.9294261932373047
Epoch 10, training loss: 10.525330543518066 = 1.9286202192306519 + 1.0 * 8.596710205078125
Epoch 10, val loss: 1.9198529720306396
Epoch 20, training loss: 10.51078987121582 = 1.9155217409133911 + 1.0 * 8.595268249511719
Epoch 20, val loss: 1.9073874950408936
Epoch 30, training loss: 10.480831146240234 = 1.8971811532974243 + 1.0 * 8.583649635314941
Epoch 30, val loss: 1.8895772695541382
Epoch 40, training loss: 10.388622283935547 = 1.87222421169281 + 1.0 * 8.516398429870605
Epoch 40, val loss: 1.8656741380691528
Epoch 50, training loss: 10.07004451751709 = 1.8435910940170288 + 1.0 * 8.22645378112793
Epoch 50, val loss: 1.8391188383102417
Epoch 60, training loss: 9.86410903930664 = 1.8176536560058594 + 1.0 * 8.046455383300781
Epoch 60, val loss: 1.8163478374481201
Epoch 70, training loss: 9.591360092163086 = 1.7955480813980103 + 1.0 * 7.795811653137207
Epoch 70, val loss: 1.796256184577942
Epoch 80, training loss: 9.21761703491211 = 1.7802428007125854 + 1.0 * 7.437374114990234
Epoch 80, val loss: 1.782809853553772
Epoch 90, training loss: 8.946840286254883 = 1.7677013874053955 + 1.0 * 7.179138660430908
Epoch 90, val loss: 1.7715175151824951
Epoch 100, training loss: 8.774188995361328 = 1.752408504486084 + 1.0 * 7.021780014038086
Epoch 100, val loss: 1.7574695348739624
Epoch 110, training loss: 8.63547420501709 = 1.7347896099090576 + 1.0 * 6.900684833526611
Epoch 110, val loss: 1.741573452949524
Epoch 120, training loss: 8.535598754882812 = 1.7157502174377441 + 1.0 * 6.819848537445068
Epoch 120, val loss: 1.7245246171951294
Epoch 130, training loss: 8.453458786010742 = 1.6945596933364868 + 1.0 * 6.758899211883545
Epoch 130, val loss: 1.7055736780166626
Epoch 140, training loss: 8.386610984802246 = 1.6695573329925537 + 1.0 * 6.717053413391113
Epoch 140, val loss: 1.6835601329803467
Epoch 150, training loss: 8.326157569885254 = 1.6404131650924683 + 1.0 * 6.685744285583496
Epoch 150, val loss: 1.6583328247070312
Epoch 160, training loss: 8.267953872680664 = 1.6070071458816528 + 1.0 * 6.660946369171143
Epoch 160, val loss: 1.6294585466384888
Epoch 170, training loss: 8.212370872497559 = 1.5687566995620728 + 1.0 * 6.643613815307617
Epoch 170, val loss: 1.5965261459350586
Epoch 180, training loss: 8.149762153625488 = 1.525997281074524 + 1.0 * 6.623764991760254
Epoch 180, val loss: 1.5600225925445557
Epoch 190, training loss: 8.087574005126953 = 1.478787899017334 + 1.0 * 6.608785629272461
Epoch 190, val loss: 1.5202724933624268
Epoch 200, training loss: 8.030852317810059 = 1.4278428554534912 + 1.0 * 6.603009223937988
Epoch 200, val loss: 1.4779717922210693
Epoch 210, training loss: 7.959017276763916 = 1.3753422498703003 + 1.0 * 6.583674907684326
Epoch 210, val loss: 1.4355015754699707
Epoch 220, training loss: 7.890201568603516 = 1.3216640949249268 + 1.0 * 6.56853723526001
Epoch 220, val loss: 1.3929914236068726
Epoch 230, training loss: 7.821845531463623 = 1.267083764076233 + 1.0 * 6.55476188659668
Epoch 230, val loss: 1.3506830930709839
Epoch 240, training loss: 7.7604289054870605 = 1.212611198425293 + 1.0 * 6.547817707061768
Epoch 240, val loss: 1.3096195459365845
Epoch 250, training loss: 7.693177223205566 = 1.1601145267486572 + 1.0 * 6.533062934875488
Epoch 250, val loss: 1.2710140943527222
Epoch 260, training loss: 7.6318464279174805 = 1.1091573238372803 + 1.0 * 6.522688865661621
Epoch 260, val loss: 1.2344785928726196
Epoch 270, training loss: 7.57839822769165 = 1.059372901916504 + 1.0 * 6.5190253257751465
Epoch 270, val loss: 1.199682593345642
Epoch 280, training loss: 7.519429683685303 = 1.0117267370224 + 1.0 * 6.507702827453613
Epoch 280, val loss: 1.1669327020645142
Epoch 290, training loss: 7.4646124839782715 = 0.9656945466995239 + 1.0 * 6.498918056488037
Epoch 290, val loss: 1.1359875202178955
Epoch 300, training loss: 7.4111738204956055 = 0.9207295179367065 + 1.0 * 6.490444183349609
Epoch 300, val loss: 1.1063616275787354
Epoch 310, training loss: 7.366199970245361 = 0.8767378926277161 + 1.0 * 6.489461898803711
Epoch 310, val loss: 1.0778933763504028
Epoch 320, training loss: 7.313694000244141 = 0.8343762159347534 + 1.0 * 6.479317665100098
Epoch 320, val loss: 1.050742506980896
Epoch 330, training loss: 7.26435661315918 = 0.7936220169067383 + 1.0 * 6.470734596252441
Epoch 330, val loss: 1.0253790616989136
Epoch 340, training loss: 7.22175931930542 = 0.7545515894889832 + 1.0 * 6.467207908630371
Epoch 340, val loss: 1.0016974210739136
Epoch 350, training loss: 7.180784225463867 = 0.7175600528717041 + 1.0 * 6.463223934173584
Epoch 350, val loss: 0.9798830151557922
Epoch 360, training loss: 7.144651412963867 = 0.6831295490264893 + 1.0 * 6.461521625518799
Epoch 360, val loss: 0.9607592821121216
Epoch 370, training loss: 7.103286266326904 = 0.6512208580970764 + 1.0 * 6.452065467834473
Epoch 370, val loss: 0.9438526034355164
Epoch 380, training loss: 7.066133975982666 = 0.6211689710617065 + 1.0 * 6.44496488571167
Epoch 380, val loss: 0.9291078448295593
Epoch 390, training loss: 7.032543182373047 = 0.5927507281303406 + 1.0 * 6.439792633056641
Epoch 390, val loss: 0.916293740272522
Epoch 400, training loss: 7.003340244293213 = 0.5658050775527954 + 1.0 * 6.437535285949707
Epoch 400, val loss: 0.9053716659545898
Epoch 410, training loss: 6.984766483306885 = 0.5404111742973328 + 1.0 * 6.444355487823486
Epoch 410, val loss: 0.896183967590332
Epoch 420, training loss: 6.951190948486328 = 0.5166352391242981 + 1.0 * 6.434555530548096
Epoch 420, val loss: 0.8887728452682495
Epoch 430, training loss: 6.919341564178467 = 0.4942793846130371 + 1.0 * 6.42506217956543
Epoch 430, val loss: 0.8830494284629822
Epoch 440, training loss: 6.894857406616211 = 0.4730806052684784 + 1.0 * 6.42177677154541
Epoch 440, val loss: 0.8787187337875366
Epoch 450, training loss: 6.873960018157959 = 0.4530574381351471 + 1.0 * 6.420902729034424
Epoch 450, val loss: 0.8756586909294128
Epoch 460, training loss: 6.852463722229004 = 0.4343213737010956 + 1.0 * 6.418142318725586
Epoch 460, val loss: 0.874039888381958
Epoch 470, training loss: 6.82946252822876 = 0.41658782958984375 + 1.0 * 6.412874698638916
Epoch 470, val loss: 0.8736305236816406
Epoch 480, training loss: 6.808413505554199 = 0.3997279107570648 + 1.0 * 6.408685684204102
Epoch 480, val loss: 0.8743402361869812
Epoch 490, training loss: 6.803415298461914 = 0.3836839199066162 + 1.0 * 6.419731140136719
Epoch 490, val loss: 0.8762255311012268
Epoch 500, training loss: 6.775002956390381 = 0.3686622381210327 + 1.0 * 6.406340599060059
Epoch 500, val loss: 0.8790029287338257
Epoch 510, training loss: 6.758823871612549 = 0.3545113801956177 + 1.0 * 6.404312610626221
Epoch 510, val loss: 0.8829328417778015
Epoch 520, training loss: 6.738710880279541 = 0.34104689955711365 + 1.0 * 6.3976640701293945
Epoch 520, val loss: 0.8876581192016602
Epoch 530, training loss: 6.726561546325684 = 0.3282092809677124 + 1.0 * 6.398352146148682
Epoch 530, val loss: 0.8931876420974731
Epoch 540, training loss: 6.7130537033081055 = 0.3160285949707031 + 1.0 * 6.397025108337402
Epoch 540, val loss: 0.8995634317398071
Epoch 550, training loss: 6.696530818939209 = 0.30450525879859924 + 1.0 * 6.392025470733643
Epoch 550, val loss: 0.9064574241638184
Epoch 560, training loss: 6.683406829833984 = 0.29349735379219055 + 1.0 * 6.389909267425537
Epoch 560, val loss: 0.9139345288276672
Epoch 570, training loss: 6.679696083068848 = 0.2829512655735016 + 1.0 * 6.396744728088379
Epoch 570, val loss: 0.9220051169395447
Epoch 580, training loss: 6.659063339233398 = 0.2728884816169739 + 1.0 * 6.38617467880249
Epoch 580, val loss: 0.9304804801940918
Epoch 590, training loss: 6.647035598754883 = 0.26322346925735474 + 1.0 * 6.383811950683594
Epoch 590, val loss: 0.9393444657325745
Epoch 600, training loss: 6.64844274520874 = 0.2538875341415405 + 1.0 * 6.39455509185791
Epoch 600, val loss: 0.9483982920646667
Epoch 610, training loss: 6.62796688079834 = 0.24488618969917297 + 1.0 * 6.38308048248291
Epoch 610, val loss: 0.9578307271003723
Epoch 620, training loss: 6.6149067878723145 = 0.23614363372325897 + 1.0 * 6.378763198852539
Epoch 620, val loss: 0.9673463106155396
Epoch 630, training loss: 6.602626800537109 = 0.22756575047969818 + 1.0 * 6.37506103515625
Epoch 630, val loss: 0.9770241975784302
Epoch 640, training loss: 6.598209857940674 = 0.2191047966480255 + 1.0 * 6.379105091094971
Epoch 640, val loss: 0.9868178963661194
Epoch 650, training loss: 6.591452121734619 = 0.21076436340808868 + 1.0 * 6.380687713623047
Epoch 650, val loss: 0.9965085387229919
Epoch 660, training loss: 6.574273109436035 = 0.20255187153816223 + 1.0 * 6.371721267700195
Epoch 660, val loss: 1.006264567375183
Epoch 670, training loss: 6.564103603363037 = 0.19439594447612762 + 1.0 * 6.3697075843811035
Epoch 670, val loss: 1.0161548852920532
Epoch 680, training loss: 6.563249111175537 = 0.18628104031085968 + 1.0 * 6.376967906951904
Epoch 680, val loss: 1.0259361267089844
Epoch 690, training loss: 6.54752779006958 = 0.17826439440250397 + 1.0 * 6.369263172149658
Epoch 690, val loss: 1.0356570482254028
Epoch 700, training loss: 6.535750865936279 = 0.17033350467681885 + 1.0 * 6.36541748046875
Epoch 700, val loss: 1.0455278158187866
Epoch 710, training loss: 6.539011478424072 = 0.16255362331867218 + 1.0 * 6.376457691192627
Epoch 710, val loss: 1.0553677082061768
Epoch 720, training loss: 6.515824794769287 = 0.15500591695308685 + 1.0 * 6.360818862915039
Epoch 720, val loss: 1.0651254653930664
Epoch 730, training loss: 6.508542060852051 = 0.14769747853279114 + 1.0 * 6.360844612121582
Epoch 730, val loss: 1.0750017166137695
Epoch 740, training loss: 6.513927936553955 = 0.14065447449684143 + 1.0 * 6.3732733726501465
Epoch 740, val loss: 1.0851564407348633
Epoch 750, training loss: 6.4976959228515625 = 0.13394322991371155 + 1.0 * 6.363752841949463
Epoch 750, val loss: 1.0950380563735962
Epoch 760, training loss: 6.486876010894775 = 0.12757334113121033 + 1.0 * 6.359302520751953
Epoch 760, val loss: 1.105268120765686
Epoch 770, training loss: 6.476140022277832 = 0.12150882929563522 + 1.0 * 6.354631423950195
Epoch 770, val loss: 1.1157331466674805
Epoch 780, training loss: 6.468947410583496 = 0.11574084311723709 + 1.0 * 6.353206634521484
Epoch 780, val loss: 1.1265465021133423
Epoch 790, training loss: 6.47932767868042 = 0.11026788502931595 + 1.0 * 6.3690595626831055
Epoch 790, val loss: 1.1373441219329834
Epoch 800, training loss: 6.4620866775512695 = 0.10513342171907425 + 1.0 * 6.356953144073486
Epoch 800, val loss: 1.148106336593628
Epoch 810, training loss: 6.450465679168701 = 0.10027780383825302 + 1.0 * 6.3501877784729
Epoch 810, val loss: 1.1591960191726685
Epoch 820, training loss: 6.4435625076293945 = 0.09567704051733017 + 1.0 * 6.347885608673096
Epoch 820, val loss: 1.170387625694275
Epoch 830, training loss: 6.439624786376953 = 0.09131961315870285 + 1.0 * 6.3483052253723145
Epoch 830, val loss: 1.1817989349365234
Epoch 840, training loss: 6.443666458129883 = 0.0871930867433548 + 1.0 * 6.356473445892334
Epoch 840, val loss: 1.193082332611084
Epoch 850, training loss: 6.432570457458496 = 0.083302803337574 + 1.0 * 6.349267482757568
Epoch 850, val loss: 1.2045800685882568
Epoch 860, training loss: 6.428477764129639 = 0.07962242513895035 + 1.0 * 6.348855495452881
Epoch 860, val loss: 1.2158945798873901
Epoch 870, training loss: 6.420928478240967 = 0.07614294439554214 + 1.0 * 6.344785690307617
Epoch 870, val loss: 1.227396845817566
Epoch 880, training loss: 6.418424606323242 = 0.07285325974225998 + 1.0 * 6.345571517944336
Epoch 880, val loss: 1.2386568784713745
Epoch 890, training loss: 6.4138922691345215 = 0.06974434107542038 + 1.0 * 6.3441481590271
Epoch 890, val loss: 1.249969720840454
Epoch 900, training loss: 6.408439636230469 = 0.06680942326784134 + 1.0 * 6.341629981994629
Epoch 900, val loss: 1.2611726522445679
Epoch 910, training loss: 6.40844202041626 = 0.06403319537639618 + 1.0 * 6.344408988952637
Epoch 910, val loss: 1.2721350193023682
Epoch 920, training loss: 6.401396751403809 = 0.061412014067173004 + 1.0 * 6.339984893798828
Epoch 920, val loss: 1.2832423448562622
Epoch 930, training loss: 6.395139217376709 = 0.05892853066325188 + 1.0 * 6.33621072769165
Epoch 930, val loss: 1.2941879034042358
Epoch 940, training loss: 6.397738456726074 = 0.056573208421468735 + 1.0 * 6.341165065765381
Epoch 940, val loss: 1.3050538301467896
Epoch 950, training loss: 6.394501686096191 = 0.054343510419130325 + 1.0 * 6.340157985687256
Epoch 950, val loss: 1.315788745880127
Epoch 960, training loss: 6.389873504638672 = 0.05223461613059044 + 1.0 * 6.337638854980469
Epoch 960, val loss: 1.326434850692749
Epoch 970, training loss: 6.383734226226807 = 0.05023632198572159 + 1.0 * 6.333498001098633
Epoch 970, val loss: 1.3369344472885132
Epoch 980, training loss: 6.385290145874023 = 0.04833928495645523 + 1.0 * 6.336950778961182
Epoch 980, val loss: 1.3473588228225708
Epoch 990, training loss: 6.397380828857422 = 0.04654010012745857 + 1.0 * 6.3508405685424805
Epoch 990, val loss: 1.357503890991211
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.797575118608329
=== training gcn model ===
Epoch 0, training loss: 10.536393165588379 = 1.9395173788070679 + 1.0 * 8.59687614440918
Epoch 0, val loss: 1.9270907640457153
Epoch 10, training loss: 10.52656364440918 = 1.9298226833343506 + 1.0 * 8.59674072265625
Epoch 10, val loss: 1.9175503253936768
Epoch 20, training loss: 10.513296127319336 = 1.9177076816558838 + 1.0 * 8.595588684082031
Epoch 20, val loss: 1.9051445722579956
Epoch 30, training loss: 10.48559284210205 = 1.9007654190063477 + 1.0 * 8.584827423095703
Epoch 30, val loss: 1.8872442245483398
Epoch 40, training loss: 10.38392448425293 = 1.8775466680526733 + 1.0 * 8.506378173828125
Epoch 40, val loss: 1.8629403114318848
Epoch 50, training loss: 10.048699378967285 = 1.8519647121429443 + 1.0 * 8.196734428405762
Epoch 50, val loss: 1.8376826047897339
Epoch 60, training loss: 9.661456108093262 = 1.8315818309783936 + 1.0 * 7.829874515533447
Epoch 60, val loss: 1.819973349571228
Epoch 70, training loss: 9.12059211730957 = 1.8174715042114258 + 1.0 * 7.3031206130981445
Epoch 70, val loss: 1.8078711032867432
Epoch 80, training loss: 8.879384994506836 = 1.8056607246398926 + 1.0 * 7.073723793029785
Epoch 80, val loss: 1.7970248460769653
Epoch 90, training loss: 8.728017807006836 = 1.7897374629974365 + 1.0 * 6.93828010559082
Epoch 90, val loss: 1.7824596166610718
Epoch 100, training loss: 8.620098114013672 = 1.7724980115890503 + 1.0 * 6.847599983215332
Epoch 100, val loss: 1.767349123954773
Epoch 110, training loss: 8.536630630493164 = 1.755852222442627 + 1.0 * 6.780778408050537
Epoch 110, val loss: 1.7532093524932861
Epoch 120, training loss: 8.471256256103516 = 1.7385454177856445 + 1.0 * 6.732710361480713
Epoch 120, val loss: 1.7386126518249512
Epoch 130, training loss: 8.413360595703125 = 1.7192717790603638 + 1.0 * 6.694088459014893
Epoch 130, val loss: 1.7224633693695068
Epoch 140, training loss: 8.360779762268066 = 1.6976525783538818 + 1.0 * 6.663127422332764
Epoch 140, val loss: 1.7045063972473145
Epoch 150, training loss: 8.306596755981445 = 1.6731314659118652 + 1.0 * 6.633465766906738
Epoch 150, val loss: 1.6842197179794312
Epoch 160, training loss: 8.253097534179688 = 1.6446691751480103 + 1.0 * 6.608428478240967
Epoch 160, val loss: 1.660875678062439
Epoch 170, training loss: 8.199949264526367 = 1.612229824066162 + 1.0 * 6.587718963623047
Epoch 170, val loss: 1.6344821453094482
Epoch 180, training loss: 8.145517349243164 = 1.5757540464401245 + 1.0 * 6.569763660430908
Epoch 180, val loss: 1.6048576831817627
Epoch 190, training loss: 8.08797836303711 = 1.5342564582824707 + 1.0 * 6.5537214279174805
Epoch 190, val loss: 1.5714788436889648
Epoch 200, training loss: 8.027054786682129 = 1.4872938394546509 + 1.0 * 6.539760589599609
Epoch 200, val loss: 1.5340121984481812
Epoch 210, training loss: 7.963500499725342 = 1.4353896379470825 + 1.0 * 6.528110980987549
Epoch 210, val loss: 1.4931120872497559
Epoch 220, training loss: 7.898748397827148 = 1.3800048828125 + 1.0 * 6.518743515014648
Epoch 220, val loss: 1.449874997138977
Epoch 230, training loss: 7.831524848937988 = 1.3220469951629639 + 1.0 * 6.509477615356445
Epoch 230, val loss: 1.4052890539169312
Epoch 240, training loss: 7.762889862060547 = 1.2624057531356812 + 1.0 * 6.500483989715576
Epoch 240, val loss: 1.3598952293395996
Epoch 250, training loss: 7.701530456542969 = 1.2024494409561157 + 1.0 * 6.499081134796143
Epoch 250, val loss: 1.3149876594543457
Epoch 260, training loss: 7.629999160766602 = 1.1436632871627808 + 1.0 * 6.486335754394531
Epoch 260, val loss: 1.271798014640808
Epoch 270, training loss: 7.565068244934082 = 1.0861995220184326 + 1.0 * 6.4788689613342285
Epoch 270, val loss: 1.2306016683578491
Epoch 280, training loss: 7.50692081451416 = 1.0310206413269043 + 1.0 * 6.475900173187256
Epoch 280, val loss: 1.1922352313995361
Epoch 290, training loss: 7.44579553604126 = 0.9788914322853088 + 1.0 * 6.466904163360596
Epoch 290, val loss: 1.1571476459503174
Epoch 300, training loss: 7.390643119812012 = 0.9300205707550049 + 1.0 * 6.460622787475586
Epoch 300, val loss: 1.1255172491073608
Epoch 310, training loss: 7.340939044952393 = 0.8845942616462708 + 1.0 * 6.4563446044921875
Epoch 310, val loss: 1.0974324941635132
Epoch 320, training loss: 7.29519510269165 = 0.8425555229187012 + 1.0 * 6.452639579772949
Epoch 320, val loss: 1.0726176500320435
Epoch 330, training loss: 7.247953414916992 = 0.8037805557250977 + 1.0 * 6.4441728591918945
Epoch 330, val loss: 1.0510201454162598
Epoch 340, training loss: 7.212225914001465 = 0.7678560018539429 + 1.0 * 6.444369792938232
Epoch 340, val loss: 1.0320614576339722
Epoch 350, training loss: 7.170158863067627 = 0.7347437739372253 + 1.0 * 6.435415267944336
Epoch 350, val loss: 1.0155850648880005
Epoch 360, training loss: 7.134274005889893 = 0.7038393020629883 + 1.0 * 6.430434703826904
Epoch 360, val loss: 1.0011574029922485
Epoch 370, training loss: 7.102315902709961 = 0.6746119260787964 + 1.0 * 6.427703857421875
Epoch 370, val loss: 0.9883815050125122
Epoch 380, training loss: 7.06866455078125 = 0.6467205882072449 + 1.0 * 6.4219441413879395
Epoch 380, val loss: 0.9768584966659546
Epoch 390, training loss: 7.050078392028809 = 0.6197997331619263 + 1.0 * 6.430278778076172
Epoch 390, val loss: 0.9663199186325073
Epoch 400, training loss: 7.010163307189941 = 0.593944787979126 + 1.0 * 6.416218280792236
Epoch 400, val loss: 0.9566569924354553
Epoch 410, training loss: 6.980298042297363 = 0.5687243342399597 + 1.0 * 6.411573886871338
Epoch 410, val loss: 0.9478244781494141
Epoch 420, training loss: 6.951816082000732 = 0.5439478158950806 + 1.0 * 6.407868385314941
Epoch 420, val loss: 0.9395981431007385
Epoch 430, training loss: 6.934277057647705 = 0.5195041298866272 + 1.0 * 6.414772987365723
Epoch 430, val loss: 0.9320087432861328
Epoch 440, training loss: 6.900250434875488 = 0.49568021297454834 + 1.0 * 6.40457010269165
Epoch 440, val loss: 0.9249365925788879
Epoch 450, training loss: 6.878212928771973 = 0.4723827838897705 + 1.0 * 6.405829906463623
Epoch 450, val loss: 0.9185488224029541
Epoch 460, training loss: 6.847535133361816 = 0.4495351314544678 + 1.0 * 6.398000240325928
Epoch 460, val loss: 0.9127467274665833
Epoch 470, training loss: 6.8206467628479 = 0.4270641505718231 + 1.0 * 6.393582820892334
Epoch 470, val loss: 0.9076371788978577
Epoch 480, training loss: 6.80844259262085 = 0.40497273206710815 + 1.0 * 6.403470039367676
Epoch 480, val loss: 0.9030721187591553
Epoch 490, training loss: 6.778163909912109 = 0.3835918605327606 + 1.0 * 6.3945722579956055
Epoch 490, val loss: 0.8991512060165405
Epoch 500, training loss: 6.7516374588012695 = 0.36281445622444153 + 1.0 * 6.38882303237915
Epoch 500, val loss: 0.8959865570068359
Epoch 510, training loss: 6.727605819702148 = 0.3426167964935303 + 1.0 * 6.384989261627197
Epoch 510, val loss: 0.8935433626174927
Epoch 520, training loss: 6.715828895568848 = 0.32305052876472473 + 1.0 * 6.392778396606445
Epoch 520, val loss: 0.8918747901916504
Epoch 530, training loss: 6.689608573913574 = 0.3043210208415985 + 1.0 * 6.385287761688232
Epoch 530, val loss: 0.8910279273986816
Epoch 540, training loss: 6.667751789093018 = 0.28651168942451477 + 1.0 * 6.381239891052246
Epoch 540, val loss: 0.891057014465332
Epoch 550, training loss: 6.647647857666016 = 0.26960277557373047 + 1.0 * 6.378045082092285
Epoch 550, val loss: 0.8919459581375122
Epoch 560, training loss: 6.62916898727417 = 0.25353574752807617 + 1.0 * 6.375633239746094
Epoch 560, val loss: 0.8936967253684998
Epoch 570, training loss: 6.612210273742676 = 0.23829929530620575 + 1.0 * 6.373910903930664
Epoch 570, val loss: 0.8962078094482422
Epoch 580, training loss: 6.596808433532715 = 0.22391724586486816 + 1.0 * 6.372890949249268
Epoch 580, val loss: 0.8994067907333374
Epoch 590, training loss: 6.581091403961182 = 0.2104225605726242 + 1.0 * 6.370668888092041
Epoch 590, val loss: 0.9032129645347595
Epoch 600, training loss: 6.568656921386719 = 0.19774147868156433 + 1.0 * 6.370915412902832
Epoch 600, val loss: 0.9076444506645203
Epoch 610, training loss: 6.554563045501709 = 0.18584665656089783 + 1.0 * 6.368716239929199
Epoch 610, val loss: 0.9125164747238159
Epoch 620, training loss: 6.539968967437744 = 0.17471008002758026 + 1.0 * 6.365258693695068
Epoch 620, val loss: 0.9178701043128967
Epoch 630, training loss: 6.529310703277588 = 0.1643153578042984 + 1.0 * 6.36499547958374
Epoch 630, val loss: 0.9236624240875244
Epoch 640, training loss: 6.515788555145264 = 0.1545817106962204 + 1.0 * 6.361207008361816
Epoch 640, val loss: 0.9299145340919495
Epoch 650, training loss: 6.508340358734131 = 0.1454644650220871 + 1.0 * 6.362875938415527
Epoch 650, val loss: 0.9364827871322632
Epoch 660, training loss: 6.4983086585998535 = 0.13694599270820618 + 1.0 * 6.361362457275391
Epoch 660, val loss: 0.9431813955307007
Epoch 670, training loss: 6.490417003631592 = 0.12901630997657776 + 1.0 * 6.361400604248047
Epoch 670, val loss: 0.950134813785553
Epoch 680, training loss: 6.4778523445129395 = 0.12160924822092056 + 1.0 * 6.356243133544922
Epoch 680, val loss: 0.9572944641113281
Epoch 690, training loss: 6.469254016876221 = 0.11467570811510086 + 1.0 * 6.354578495025635
Epoch 690, val loss: 0.9646664261817932
Epoch 700, training loss: 6.468699932098389 = 0.10818655043840408 + 1.0 * 6.360513210296631
Epoch 700, val loss: 0.9721338152885437
Epoch 710, training loss: 6.45944356918335 = 0.10214243084192276 + 1.0 * 6.357301235198975
Epoch 710, val loss: 0.9796132445335388
Epoch 720, training loss: 6.4496026039123535 = 0.09648358076810837 + 1.0 * 6.353118896484375
Epoch 720, val loss: 0.9872194528579712
Epoch 730, training loss: 6.443397045135498 = 0.09121907502412796 + 1.0 * 6.35217809677124
Epoch 730, val loss: 0.9947219491004944
Epoch 740, training loss: 6.435806751251221 = 0.08632500469684601 + 1.0 * 6.349481582641602
Epoch 740, val loss: 1.002220630645752
Epoch 750, training loss: 6.428189754486084 = 0.08175988495349884 + 1.0 * 6.346429824829102
Epoch 750, val loss: 1.009830117225647
Epoch 760, training loss: 6.422452449798584 = 0.07747923582792282 + 1.0 * 6.344973087310791
Epoch 760, val loss: 1.0173834562301636
Epoch 770, training loss: 6.416842937469482 = 0.07346093654632568 + 1.0 * 6.343381881713867
Epoch 770, val loss: 1.0249398946762085
Epoch 780, training loss: 6.418698310852051 = 0.06969623267650604 + 1.0 * 6.349001884460449
Epoch 780, val loss: 1.0324822664260864
Epoch 790, training loss: 6.416934967041016 = 0.06617678701877594 + 1.0 * 6.350758075714111
Epoch 790, val loss: 1.0398004055023193
Epoch 800, training loss: 6.403275966644287 = 0.06289730221033096 + 1.0 * 6.340378761291504
Epoch 800, val loss: 1.0471233129501343
Epoch 810, training loss: 6.399434566497803 = 0.059827469289302826 + 1.0 * 6.339607238769531
Epoch 810, val loss: 1.0544309616088867
Epoch 820, training loss: 6.39798641204834 = 0.05694354325532913 + 1.0 * 6.341042995452881
Epoch 820, val loss: 1.0616368055343628
Epoch 830, training loss: 6.392990589141846 = 0.05424578860402107 + 1.0 * 6.338744640350342
Epoch 830, val loss: 1.0687084197998047
Epoch 840, training loss: 6.389833927154541 = 0.05172411724925041 + 1.0 * 6.338109970092773
Epoch 840, val loss: 1.0757354497909546
Epoch 850, training loss: 6.384958267211914 = 0.04935970902442932 + 1.0 * 6.335598468780518
Epoch 850, val loss: 1.0827516317367554
Epoch 860, training loss: 6.389469623565674 = 0.04713335633277893 + 1.0 * 6.342336177825928
Epoch 860, val loss: 1.089574933052063
Epoch 870, training loss: 6.388573169708252 = 0.045048050582408905 + 1.0 * 6.343524932861328
Epoch 870, val loss: 1.096208095550537
Epoch 880, training loss: 6.375615119934082 = 0.04309794679284096 + 1.0 * 6.332517147064209
Epoch 880, val loss: 1.1028625965118408
Epoch 890, training loss: 6.372873306274414 = 0.04126337543129921 + 1.0 * 6.331609725952148
Epoch 890, val loss: 1.1094459295272827
Epoch 900, training loss: 6.373379707336426 = 0.039534229785203934 + 1.0 * 6.333845615386963
Epoch 900, val loss: 1.1159145832061768
Epoch 910, training loss: 6.370319366455078 = 0.03790450096130371 + 1.0 * 6.332414627075195
Epoch 910, val loss: 1.1222262382507324
Epoch 920, training loss: 6.363935470581055 = 0.03636883571743965 + 1.0 * 6.327566623687744
Epoch 920, val loss: 1.1284515857696533
Epoch 930, training loss: 6.362290382385254 = 0.034921035170555115 + 1.0 * 6.327369213104248
Epoch 930, val loss: 1.1346272230148315
Epoch 940, training loss: 6.367613792419434 = 0.03355318680405617 + 1.0 * 6.3340606689453125
Epoch 940, val loss: 1.1406651735305786
Epoch 950, training loss: 6.362247467041016 = 0.032267920672893524 + 1.0 * 6.329979419708252
Epoch 950, val loss: 1.1465791463851929
Epoch 960, training loss: 6.360780239105225 = 0.031052323058247566 + 1.0 * 6.329728126525879
Epoch 960, val loss: 1.1523561477661133
Epoch 970, training loss: 6.353445529937744 = 0.029903214424848557 + 1.0 * 6.32354211807251
Epoch 970, val loss: 1.158049464225769
Epoch 980, training loss: 6.351845741271973 = 0.028818553313612938 + 1.0 * 6.32302713394165
Epoch 980, val loss: 1.1637310981750488
Epoch 990, training loss: 6.3546576499938965 = 0.027786316350102425 + 1.0 * 6.326871395111084
Epoch 990, val loss: 1.1692534685134888
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8107538218239325
The final CL Acc:0.75926, 0.02181, The final GNN Acc:0.80566, 0.00578
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13218])
remove edge: torch.Size([2, 7924])
updated graph: torch.Size([2, 10586])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.547842025756836 = 1.9509795904159546 + 1.0 * 8.59686279296875
Epoch 0, val loss: 1.9475868940353394
Epoch 10, training loss: 10.536534309387207 = 1.9399174451828003 + 1.0 * 8.596616744995117
Epoch 10, val loss: 1.93645179271698
Epoch 20, training loss: 10.520198822021484 = 1.925838828086853 + 1.0 * 8.5943603515625
Epoch 20, val loss: 1.9223623275756836
Epoch 30, training loss: 10.481523513793945 = 1.9061003923416138 + 1.0 * 8.575423240661621
Epoch 30, val loss: 1.9028211832046509
Epoch 40, training loss: 10.32498550415039 = 1.8802485466003418 + 1.0 * 8.44473648071289
Epoch 40, val loss: 1.8783763647079468
Epoch 50, training loss: 9.992396354675293 = 1.8521701097488403 + 1.0 * 8.140226364135742
Epoch 50, val loss: 1.8534170389175415
Epoch 60, training loss: 9.588672637939453 = 1.831039309501648 + 1.0 * 7.757633686065674
Epoch 60, val loss: 1.8349708318710327
Epoch 70, training loss: 9.048526763916016 = 1.815665602684021 + 1.0 * 7.232861518859863
Epoch 70, val loss: 1.820590853691101
Epoch 80, training loss: 8.799789428710938 = 1.8002476692199707 + 1.0 * 6.999541282653809
Epoch 80, val loss: 1.8066693544387817
Epoch 90, training loss: 8.628082275390625 = 1.781266689300537 + 1.0 * 6.84681510925293
Epoch 90, val loss: 1.791128158569336
Epoch 100, training loss: 8.523333549499512 = 1.7618154287338257 + 1.0 * 6.7615180015563965
Epoch 100, val loss: 1.7751388549804688
Epoch 110, training loss: 8.446468353271484 = 1.7422165870666504 + 1.0 * 6.704251289367676
Epoch 110, val loss: 1.7580903768539429
Epoch 120, training loss: 8.384298324584961 = 1.720900058746338 + 1.0 * 6.663398265838623
Epoch 120, val loss: 1.7391855716705322
Epoch 130, training loss: 8.32879638671875 = 1.6971684694290161 + 1.0 * 6.631628036499023
Epoch 130, val loss: 1.7184042930603027
Epoch 140, training loss: 8.274155616760254 = 1.6707745790481567 + 1.0 * 6.6033806800842285
Epoch 140, val loss: 1.695487141609192
Epoch 150, training loss: 8.219029426574707 = 1.6411559581756592 + 1.0 * 6.577873229980469
Epoch 150, val loss: 1.6700912714004517
Epoch 160, training loss: 8.168774604797363 = 1.6081641912460327 + 1.0 * 6.560610294342041
Epoch 160, val loss: 1.642030119895935
Epoch 170, training loss: 8.113787651062012 = 1.5728514194488525 + 1.0 * 6.54093599319458
Epoch 170, val loss: 1.6119451522827148
Epoch 180, training loss: 8.060476303100586 = 1.5352897644042969 + 1.0 * 6.525187015533447
Epoch 180, val loss: 1.5802185535430908
Epoch 190, training loss: 8.006329536437988 = 1.4956403970718384 + 1.0 * 6.510688781738281
Epoch 190, val loss: 1.5467065572738647
Epoch 200, training loss: 7.954761981964111 = 1.4544044733047485 + 1.0 * 6.500357627868652
Epoch 200, val loss: 1.5121194124221802
Epoch 210, training loss: 7.902927398681641 = 1.4133994579315186 + 1.0 * 6.489528179168701
Epoch 210, val loss: 1.47802734375
Epoch 220, training loss: 7.852784156799316 = 1.373309850692749 + 1.0 * 6.4794745445251465
Epoch 220, val loss: 1.4451216459274292
Epoch 230, training loss: 7.804230213165283 = 1.333975911140442 + 1.0 * 6.470254421234131
Epoch 230, val loss: 1.413295865058899
Epoch 240, training loss: 7.760378837585449 = 1.2958616018295288 + 1.0 * 6.464517116546631
Epoch 240, val loss: 1.3827584981918335
Epoch 250, training loss: 7.715679168701172 = 1.2595916986465454 + 1.0 * 6.456087589263916
Epoch 250, val loss: 1.3540650606155396
Epoch 260, training loss: 7.673913478851318 = 1.2241989374160767 + 1.0 * 6.449714660644531
Epoch 260, val loss: 1.3264148235321045
Epoch 270, training loss: 7.635965824127197 = 1.1890825033187866 + 1.0 * 6.446883201599121
Epoch 270, val loss: 1.2992717027664185
Epoch 280, training loss: 7.5950703620910645 = 1.154427409172058 + 1.0 * 6.440642833709717
Epoch 280, val loss: 1.2725552320480347
Epoch 290, training loss: 7.553491115570068 = 1.1195354461669922 + 1.0 * 6.433955669403076
Epoch 290, val loss: 1.2457716464996338
Epoch 300, training loss: 7.512995719909668 = 1.0836963653564453 + 1.0 * 6.429299354553223
Epoch 300, val loss: 1.2182592153549194
Epoch 310, training loss: 7.4710283279418945 = 1.0464779138565063 + 1.0 * 6.424550533294678
Epoch 310, val loss: 1.1895233392715454
Epoch 320, training loss: 7.429917335510254 = 1.0078998804092407 + 1.0 * 6.422017574310303
Epoch 320, val loss: 1.1594529151916504
Epoch 330, training loss: 7.386975288391113 = 0.9684600830078125 + 1.0 * 6.418515205383301
Epoch 330, val loss: 1.1285626888275146
Epoch 340, training loss: 7.344043254852295 = 0.9282032251358032 + 1.0 * 6.415840148925781
Epoch 340, val loss: 1.097047209739685
Epoch 350, training loss: 7.30108642578125 = 0.8874800205230713 + 1.0 * 6.413606643676758
Epoch 350, val loss: 1.0650453567504883
Epoch 360, training loss: 7.2552385330200195 = 0.8468421101570129 + 1.0 * 6.408396244049072
Epoch 360, val loss: 1.0332417488098145
Epoch 370, training loss: 7.214737892150879 = 0.8067536354064941 + 1.0 * 6.407984256744385
Epoch 370, val loss: 1.0022767782211304
Epoch 380, training loss: 7.179745674133301 = 0.7680180072784424 + 1.0 * 6.4117279052734375
Epoch 380, val loss: 0.9726752638816833
Epoch 390, training loss: 7.133233547210693 = 0.7311541438102722 + 1.0 * 6.4020795822143555
Epoch 390, val loss: 0.9451606869697571
Epoch 400, training loss: 7.093632698059082 = 0.6961998343467712 + 1.0 * 6.397432804107666
Epoch 400, val loss: 0.9198811650276184
Epoch 410, training loss: 7.058967590332031 = 0.6632301807403564 + 1.0 * 6.395737171173096
Epoch 410, val loss: 0.8969330191612244
Epoch 420, training loss: 7.031925678253174 = 0.632472813129425 + 1.0 * 6.3994526863098145
Epoch 420, val loss: 0.8764986395835876
Epoch 430, training loss: 6.993010520935059 = 0.6039609909057617 + 1.0 * 6.389049530029297
Epoch 430, val loss: 0.8586166501045227
Epoch 440, training loss: 6.964751720428467 = 0.5772855877876282 + 1.0 * 6.387465953826904
Epoch 440, val loss: 0.842998206615448
Epoch 450, training loss: 6.942157745361328 = 0.5522864460945129 + 1.0 * 6.389871120452881
Epoch 450, val loss: 0.8293380737304688
Epoch 460, training loss: 6.911209583282471 = 0.5288932919502258 + 1.0 * 6.3823161125183105
Epoch 460, val loss: 0.8176174163818359
Epoch 470, training loss: 6.8864521980285645 = 0.5067658424377441 + 1.0 * 6.37968635559082
Epoch 470, val loss: 0.8074233531951904
Epoch 480, training loss: 6.866613864898682 = 0.485658198595047 + 1.0 * 6.380955696105957
Epoch 480, val loss: 0.7986161112785339
Epoch 490, training loss: 6.844357490539551 = 0.4654057025909424 + 1.0 * 6.378951549530029
Epoch 490, val loss: 0.790986180305481
Epoch 500, training loss: 6.819626331329346 = 0.4458720088005066 + 1.0 * 6.373754501342773
Epoch 500, val loss: 0.7841838598251343
Epoch 510, training loss: 6.798966884613037 = 0.4267958104610443 + 1.0 * 6.372170925140381
Epoch 510, val loss: 0.7781200408935547
Epoch 520, training loss: 6.783215522766113 = 0.4081250727176666 + 1.0 * 6.375090599060059
Epoch 520, val loss: 0.7726156711578369
Epoch 530, training loss: 6.7576375007629395 = 0.3899332284927368 + 1.0 * 6.367704391479492
Epoch 530, val loss: 0.7674809694290161
Epoch 540, training loss: 6.736043930053711 = 0.372041791677475 + 1.0 * 6.364002227783203
Epoch 540, val loss: 0.7626761794090271
Epoch 550, training loss: 6.725063323974609 = 0.3544366657733917 + 1.0 * 6.370626449584961
Epoch 550, val loss: 0.7581146955490112
Epoch 560, training loss: 6.7028727531433105 = 0.33723172545433044 + 1.0 * 6.365641117095947
Epoch 560, val loss: 0.753741443157196
Epoch 570, training loss: 6.679935932159424 = 0.32039594650268555 + 1.0 * 6.359539985656738
Epoch 570, val loss: 0.749523401260376
Epoch 580, training loss: 6.666713237762451 = 0.30390337109565735 + 1.0 * 6.362809658050537
Epoch 580, val loss: 0.7454805970191956
Epoch 590, training loss: 6.645010948181152 = 0.28790968656539917 + 1.0 * 6.3571014404296875
Epoch 590, val loss: 0.7415804862976074
Epoch 600, training loss: 6.626296043395996 = 0.2724444568157196 + 1.0 * 6.353851795196533
Epoch 600, val loss: 0.7380180358886719
Epoch 610, training loss: 6.609725475311279 = 0.25750693678855896 + 1.0 * 6.3522186279296875
Epoch 610, val loss: 0.7348043918609619
Epoch 620, training loss: 6.604159355163574 = 0.2432006448507309 + 1.0 * 6.360958576202393
Epoch 620, val loss: 0.7320149540901184
Epoch 630, training loss: 6.580136299133301 = 0.22960950434207916 + 1.0 * 6.350526809692383
Epoch 630, val loss: 0.7297223806381226
Epoch 640, training loss: 6.575427055358887 = 0.2167627215385437 + 1.0 * 6.358664512634277
Epoch 640, val loss: 0.7278963923454285
Epoch 650, training loss: 6.553140163421631 = 0.2046726495027542 + 1.0 * 6.3484673500061035
Epoch 650, val loss: 0.726625919342041
Epoch 660, training loss: 6.538078308105469 = 0.1933012157678604 + 1.0 * 6.3447771072387695
Epoch 660, val loss: 0.7259284853935242
Epoch 670, training loss: 6.528136253356934 = 0.18262101709842682 + 1.0 * 6.345515251159668
Epoch 670, val loss: 0.7258307933807373
Epoch 680, training loss: 6.521066665649414 = 0.1726648062467575 + 1.0 * 6.34840202331543
Epoch 680, val loss: 0.7261695861816406
Epoch 690, training loss: 6.504893779754639 = 0.16345147788524628 + 1.0 * 6.341442108154297
Epoch 690, val loss: 0.726959228515625
Epoch 700, training loss: 6.4946088790893555 = 0.15485325455665588 + 1.0 * 6.339755535125732
Epoch 700, val loss: 0.7282710075378418
Epoch 710, training loss: 6.485297679901123 = 0.14680476486682892 + 1.0 * 6.3384928703308105
Epoch 710, val loss: 0.7300825119018555
Epoch 720, training loss: 6.476253986358643 = 0.13926486670970917 + 1.0 * 6.336988925933838
Epoch 720, val loss: 0.732307493686676
Epoch 730, training loss: 6.4846343994140625 = 0.13221874833106995 + 1.0 * 6.352415561676025
Epoch 730, val loss: 0.7348158359527588
Epoch 740, training loss: 6.466932773590088 = 0.1256631463766098 + 1.0 * 6.341269493103027
Epoch 740, val loss: 0.7375729084014893
Epoch 750, training loss: 6.454615592956543 = 0.11953460425138474 + 1.0 * 6.335081100463867
Epoch 750, val loss: 0.7406192421913147
Epoch 760, training loss: 6.452587604522705 = 0.1137794777750969 + 1.0 * 6.338808059692383
Epoch 760, val loss: 0.7440054416656494
Epoch 770, training loss: 6.44364595413208 = 0.1083909422159195 + 1.0 * 6.335255146026611
Epoch 770, val loss: 0.7475632429122925
Epoch 780, training loss: 6.4360151290893555 = 0.10333533585071564 + 1.0 * 6.332679748535156
Epoch 780, val loss: 0.7513306736946106
Epoch 790, training loss: 6.428105354309082 = 0.0985700935125351 + 1.0 * 6.329535484313965
Epoch 790, val loss: 0.7553517818450928
Epoch 800, training loss: 6.428591251373291 = 0.09407360851764679 + 1.0 * 6.334517478942871
Epoch 800, val loss: 0.7595775127410889
Epoch 810, training loss: 6.418710708618164 = 0.08984700590372086 + 1.0 * 6.328863620758057
Epoch 810, val loss: 0.7638728022575378
Epoch 820, training loss: 6.415549278259277 = 0.08586663752794266 + 1.0 * 6.32968282699585
Epoch 820, val loss: 0.768279492855072
Epoch 830, training loss: 6.4118332862854 = 0.08211148530244827 + 1.0 * 6.329721927642822
Epoch 830, val loss: 0.7728831171989441
Epoch 840, training loss: 6.405736923217773 = 0.07855992764234543 + 1.0 * 6.327177047729492
Epoch 840, val loss: 0.7776157259941101
Epoch 850, training loss: 6.400726318359375 = 0.07520100474357605 + 1.0 * 6.325525283813477
Epoch 850, val loss: 0.7824663519859314
Epoch 860, training loss: 6.403263092041016 = 0.0720234215259552 + 1.0 * 6.331239700317383
Epoch 860, val loss: 0.7873747944831848
Epoch 870, training loss: 6.394238471984863 = 0.06901764869689941 + 1.0 * 6.325221061706543
Epoch 870, val loss: 0.7922670841217041
Epoch 880, training loss: 6.3887834548950195 = 0.06617451459169388 + 1.0 * 6.322608947753906
Epoch 880, val loss: 0.7972854971885681
Epoch 890, training loss: 6.387875080108643 = 0.06348060816526413 + 1.0 * 6.324394702911377
Epoch 890, val loss: 0.8023339509963989
Epoch 900, training loss: 6.383261680603027 = 0.06092938035726547 + 1.0 * 6.322332382202148
Epoch 900, val loss: 0.8073605298995972
Epoch 910, training loss: 6.37747859954834 = 0.058508530259132385 + 1.0 * 6.318970203399658
Epoch 910, val loss: 0.8124401569366455
Epoch 920, training loss: 6.3777337074279785 = 0.05621243268251419 + 1.0 * 6.321521282196045
Epoch 920, val loss: 0.8175532221794128
Epoch 930, training loss: 6.373530387878418 = 0.05403262749314308 + 1.0 * 6.319497585296631
Epoch 930, val loss: 0.8226760625839233
Epoch 940, training loss: 6.376875400543213 = 0.05196592956781387 + 1.0 * 6.324909687042236
Epoch 940, val loss: 0.8278042078018188
Epoch 950, training loss: 6.368507385253906 = 0.05000314489006996 + 1.0 * 6.318504333496094
Epoch 950, val loss: 0.8328210115432739
Epoch 960, training loss: 6.364434242248535 = 0.04814775288105011 + 1.0 * 6.316286563873291
Epoch 960, val loss: 0.8378947377204895
Epoch 970, training loss: 6.360835075378418 = 0.04637879133224487 + 1.0 * 6.314456462860107
Epoch 970, val loss: 0.8429970741271973
Epoch 980, training loss: 6.36224889755249 = 0.04469377174973488 + 1.0 * 6.317554950714111
Epoch 980, val loss: 0.8481515645980835
Epoch 990, training loss: 6.357969284057617 = 0.043090593069791794 + 1.0 * 6.314878463745117
Epoch 990, val loss: 0.8531950116157532
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.538021087646484 = 1.9411544799804688 + 1.0 * 8.596866607666016
Epoch 0, val loss: 1.93563711643219
Epoch 10, training loss: 10.5281400680542 = 1.9314510822296143 + 1.0 * 8.596689224243164
Epoch 10, val loss: 1.926132082939148
Epoch 20, training loss: 10.514609336853027 = 1.9193871021270752 + 1.0 * 8.595222473144531
Epoch 20, val loss: 1.9137588739395142
Epoch 30, training loss: 10.484935760498047 = 1.9024322032928467 + 1.0 * 8.582503318786621
Epoch 30, val loss: 1.8960028886795044
Epoch 40, training loss: 10.375997543334961 = 1.8792344331741333 + 1.0 * 8.496763229370117
Epoch 40, val loss: 1.8723233938217163
Epoch 50, training loss: 9.962465286254883 = 1.852571964263916 + 1.0 * 8.109892845153809
Epoch 50, val loss: 1.8466311693191528
Epoch 60, training loss: 9.579075813293457 = 1.8292113542556763 + 1.0 * 7.749864101409912
Epoch 60, val loss: 1.8252816200256348
Epoch 70, training loss: 9.177884101867676 = 1.8095818758010864 + 1.0 * 7.368302345275879
Epoch 70, val loss: 1.8074297904968262
Epoch 80, training loss: 8.969346046447754 = 1.7917534112930298 + 1.0 * 7.177592754364014
Epoch 80, val loss: 1.7905199527740479
Epoch 90, training loss: 8.815017700195312 = 1.771561622619629 + 1.0 * 7.043456077575684
Epoch 90, val loss: 1.7717971801757812
Epoch 100, training loss: 8.66993236541748 = 1.7523167133331299 + 1.0 * 6.9176154136657715
Epoch 100, val loss: 1.7550135850906372
Epoch 110, training loss: 8.565709114074707 = 1.7331514358520508 + 1.0 * 6.832557678222656
Epoch 110, val loss: 1.7381118535995483
Epoch 120, training loss: 8.481303215026855 = 1.7111966609954834 + 1.0 * 6.770106792449951
Epoch 120, val loss: 1.7188103199005127
Epoch 130, training loss: 8.412978172302246 = 1.685861349105835 + 1.0 * 6.72711706161499
Epoch 130, val loss: 1.6969629526138306
Epoch 140, training loss: 8.348527908325195 = 1.6565546989440918 + 1.0 * 6.691973686218262
Epoch 140, val loss: 1.6718766689300537
Epoch 150, training loss: 8.280914306640625 = 1.6230202913284302 + 1.0 * 6.657894134521484
Epoch 150, val loss: 1.6432263851165771
Epoch 160, training loss: 8.212940216064453 = 1.584572434425354 + 1.0 * 6.6283674240112305
Epoch 160, val loss: 1.6103129386901855
Epoch 170, training loss: 8.145279884338379 = 1.5405104160308838 + 1.0 * 6.604769706726074
Epoch 170, val loss: 1.5727225542068481
Epoch 180, training loss: 8.075477600097656 = 1.4918454885482788 + 1.0 * 6.583632469177246
Epoch 180, val loss: 1.531503438949585
Epoch 190, training loss: 8.004233360290527 = 1.4389621019363403 + 1.0 * 6.565270900726318
Epoch 190, val loss: 1.4867929220199585
Epoch 200, training loss: 7.931262016296387 = 1.3819055557250977 + 1.0 * 6.549356460571289
Epoch 200, val loss: 1.4389326572418213
Epoch 210, training loss: 7.859280586242676 = 1.3214504718780518 + 1.0 * 6.537830352783203
Epoch 210, val loss: 1.388562560081482
Epoch 220, training loss: 7.784468650817871 = 1.2603445053100586 + 1.0 * 6.5241241455078125
Epoch 220, val loss: 1.338471531867981
Epoch 230, training loss: 7.7104997634887695 = 1.1995038986206055 + 1.0 * 6.510995864868164
Epoch 230, val loss: 1.2891968488693237
Epoch 240, training loss: 7.6412739753723145 = 1.139140009880066 + 1.0 * 6.502133846282959
Epoch 240, val loss: 1.240906834602356
Epoch 250, training loss: 7.573944568634033 = 1.0808496475219727 + 1.0 * 6.4930949211120605
Epoch 250, val loss: 1.1947457790374756
Epoch 260, training loss: 7.508565902709961 = 1.0250074863433838 + 1.0 * 6.483558654785156
Epoch 260, val loss: 1.1510428190231323
Epoch 270, training loss: 7.45107364654541 = 0.9713389873504639 + 1.0 * 6.479734420776367
Epoch 270, val loss: 1.1095529794692993
Epoch 280, training loss: 7.388888359069824 = 0.9207112789154053 + 1.0 * 6.468177318572998
Epoch 280, val loss: 1.0708256959915161
Epoch 290, training loss: 7.334362983703613 = 0.87248295545578 + 1.0 * 6.461880207061768
Epoch 290, val loss: 1.034384846687317
Epoch 300, training loss: 7.290013313293457 = 0.8268240094184875 + 1.0 * 6.463189125061035
Epoch 300, val loss: 1.000213623046875
Epoch 310, training loss: 7.236977577209473 = 0.7842712998390198 + 1.0 * 6.452706336975098
Epoch 310, val loss: 0.9689857959747314
Epoch 320, training loss: 7.189026832580566 = 0.7440085411071777 + 1.0 * 6.445018291473389
Epoch 320, val loss: 0.9401103854179382
Epoch 330, training loss: 7.144826889038086 = 0.7056905627250671 + 1.0 * 6.439136505126953
Epoch 330, val loss: 0.9133388996124268
Epoch 340, training loss: 7.106653690338135 = 0.6692113280296326 + 1.0 * 6.437442302703857
Epoch 340, val loss: 0.8886674642562866
Epoch 350, training loss: 7.064947605133057 = 0.635123610496521 + 1.0 * 6.429823875427246
Epoch 350, val loss: 0.8663714528083801
Epoch 360, training loss: 7.030852794647217 = 0.6030677556991577 + 1.0 * 6.4277849197387695
Epoch 360, val loss: 0.8462435603141785
Epoch 370, training loss: 6.993618011474609 = 0.5726217031478882 + 1.0 * 6.420996189117432
Epoch 370, val loss: 0.827911913394928
Epoch 380, training loss: 6.973072528839111 = 0.5435507893562317 + 1.0 * 6.429521560668945
Epoch 380, val loss: 0.8111632466316223
Epoch 390, training loss: 6.937447547912598 = 0.5162703990936279 + 1.0 * 6.421176910400391
Epoch 390, val loss: 0.7960753440856934
Epoch 400, training loss: 6.9022016525268555 = 0.49048033356666565 + 1.0 * 6.411721229553223
Epoch 400, val loss: 0.7824738621711731
Epoch 410, training loss: 6.8726396560668945 = 0.4657706022262573 + 1.0 * 6.406868934631348
Epoch 410, val loss: 0.7702174186706543
Epoch 420, training loss: 6.848613739013672 = 0.4420187771320343 + 1.0 * 6.406594753265381
Epoch 420, val loss: 0.7591331005096436
Epoch 430, training loss: 6.826930522918701 = 0.41936782002449036 + 1.0 * 6.407562732696533
Epoch 430, val loss: 0.7492794394493103
Epoch 440, training loss: 6.797917366027832 = 0.3976811170578003 + 1.0 * 6.400236129760742
Epoch 440, val loss: 0.7406231760978699
Epoch 450, training loss: 6.773519515991211 = 0.376810222864151 + 1.0 * 6.396709442138672
Epoch 450, val loss: 0.7330881357192993
Epoch 460, training loss: 6.7576093673706055 = 0.3568839132785797 + 1.0 * 6.400725364685059
Epoch 460, val loss: 0.7265619039535522
Epoch 470, training loss: 6.731853485107422 = 0.33784428238868713 + 1.0 * 6.394009113311768
Epoch 470, val loss: 0.7211148738861084
Epoch 480, training loss: 6.708622455596924 = 0.3195236921310425 + 1.0 * 6.389098644256592
Epoch 480, val loss: 0.7165404558181763
Epoch 490, training loss: 6.687838077545166 = 0.30186519026756287 + 1.0 * 6.38597297668457
Epoch 490, val loss: 0.7128234505653381
Epoch 500, training loss: 6.683298110961914 = 0.28488099575042725 + 1.0 * 6.398416996002197
Epoch 500, val loss: 0.7099570035934448
Epoch 510, training loss: 6.650526523590088 = 0.2687617242336273 + 1.0 * 6.381764888763428
Epoch 510, val loss: 0.7078567743301392
Epoch 520, training loss: 6.63297176361084 = 0.25341954827308655 + 1.0 * 6.379552364349365
Epoch 520, val loss: 0.7064896821975708
Epoch 530, training loss: 6.61562442779541 = 0.2387731671333313 + 1.0 * 6.3768510818481445
Epoch 530, val loss: 0.7057892680168152
Epoch 540, training loss: 6.618873596191406 = 0.22484524548053741 + 1.0 * 6.394028186798096
Epoch 540, val loss: 0.7057299017906189
Epoch 550, training loss: 6.588983535766602 = 0.2117561399936676 + 1.0 * 6.377227306365967
Epoch 550, val loss: 0.7063043117523193
Epoch 560, training loss: 6.570991516113281 = 0.1994258612394333 + 1.0 * 6.371565818786621
Epoch 560, val loss: 0.7074809074401855
Epoch 570, training loss: 6.557835102081299 = 0.1877727508544922 + 1.0 * 6.370062351226807
Epoch 570, val loss: 0.709227979183197
Epoch 580, training loss: 6.549262523651123 = 0.17680799961090088 + 1.0 * 6.372454643249512
Epoch 580, val loss: 0.7115201950073242
Epoch 590, training loss: 6.534006595611572 = 0.1665615290403366 + 1.0 * 6.36744499206543
Epoch 590, val loss: 0.7143384218215942
Epoch 600, training loss: 6.5206403732299805 = 0.15694962441921234 + 1.0 * 6.3636908531188965
Epoch 600, val loss: 0.7176004648208618
Epoch 610, training loss: 6.509880542755127 = 0.14792189002037048 + 1.0 * 6.3619585037231445
Epoch 610, val loss: 0.7213020324707031
Epoch 620, training loss: 6.5042572021484375 = 0.13947995007038116 + 1.0 * 6.364777088165283
Epoch 620, val loss: 0.7253859043121338
Epoch 630, training loss: 6.490328788757324 = 0.13162052631378174 + 1.0 * 6.358708381652832
Epoch 630, val loss: 0.7298230528831482
Epoch 640, training loss: 6.48495626449585 = 0.12428490817546844 + 1.0 * 6.360671520233154
Epoch 640, val loss: 0.7345494627952576
Epoch 650, training loss: 6.475093364715576 = 0.11744193732738495 + 1.0 * 6.357651233673096
Epoch 650, val loss: 0.7395660281181335
Epoch 660, training loss: 6.465973854064941 = 0.11105059832334518 + 1.0 * 6.354923248291016
Epoch 660, val loss: 0.744805634021759
Epoch 670, training loss: 6.455655097961426 = 0.10508976876735687 + 1.0 * 6.350565433502197
Epoch 670, val loss: 0.7502478957176208
Epoch 680, training loss: 6.450225830078125 = 0.09950893372297287 + 1.0 * 6.350717067718506
Epoch 680, val loss: 0.7558993697166443
Epoch 690, training loss: 6.452995300292969 = 0.0942980945110321 + 1.0 * 6.358697414398193
Epoch 690, val loss: 0.7616796493530273
Epoch 700, training loss: 6.439167499542236 = 0.08944990485906601 + 1.0 * 6.349717617034912
Epoch 700, val loss: 0.7675783038139343
Epoch 710, training loss: 6.430082321166992 = 0.08493459224700928 + 1.0 * 6.345147609710693
Epoch 710, val loss: 0.7736043334007263
Epoch 720, training loss: 6.430257797241211 = 0.08070128411054611 + 1.0 * 6.3495564460754395
Epoch 720, val loss: 0.7797157168388367
Epoch 730, training loss: 6.420309543609619 = 0.0767451599240303 + 1.0 * 6.343564510345459
Epoch 730, val loss: 0.7859225273132324
Epoch 740, training loss: 6.413105010986328 = 0.0730363205075264 + 1.0 * 6.340068817138672
Epoch 740, val loss: 0.7922531962394714
Epoch 750, training loss: 6.410691261291504 = 0.06954243779182434 + 1.0 * 6.341148853302002
Epoch 750, val loss: 0.7986372113227844
Epoch 760, training loss: 6.414432048797607 = 0.06627406924962997 + 1.0 * 6.34815788269043
Epoch 760, val loss: 0.8051028847694397
Epoch 770, training loss: 6.399218559265137 = 0.06322750449180603 + 1.0 * 6.335990905761719
Epoch 770, val loss: 0.8115159869194031
Epoch 780, training loss: 6.396320819854736 = 0.060364820063114166 + 1.0 * 6.33595609664917
Epoch 780, val loss: 0.8179873824119568
Epoch 790, training loss: 6.391030311584473 = 0.057663384824991226 + 1.0 * 6.333366870880127
Epoch 790, val loss: 0.8245133757591248
Epoch 800, training loss: 6.388665676116943 = 0.05511314794421196 + 1.0 * 6.333552360534668
Epoch 800, val loss: 0.831170916557312
Epoch 810, training loss: 6.3847527503967285 = 0.05271559953689575 + 1.0 * 6.332036972045898
Epoch 810, val loss: 0.837760865688324
Epoch 820, training loss: 6.390317916870117 = 0.050475817173719406 + 1.0 * 6.339842319488525
Epoch 820, val loss: 0.8442047238349915
Epoch 830, training loss: 6.37762975692749 = 0.04837290197610855 + 1.0 * 6.329257011413574
Epoch 830, val loss: 0.8506202697753906
Epoch 840, training loss: 6.37355899810791 = 0.0463893860578537 + 1.0 * 6.327169418334961
Epoch 840, val loss: 0.8570424914360046
Epoch 850, training loss: 6.370758533477783 = 0.044509369879961014 + 1.0 * 6.326249122619629
Epoch 850, val loss: 0.8635484576225281
Epoch 860, training loss: 6.378195285797119 = 0.042731642723083496 + 1.0 * 6.335463523864746
Epoch 860, val loss: 0.8700554370880127
Epoch 870, training loss: 6.369573593139648 = 0.041053056716918945 + 1.0 * 6.32852029800415
Epoch 870, val loss: 0.8763623237609863
Epoch 880, training loss: 6.364325046539307 = 0.0394701287150383 + 1.0 * 6.324854850769043
Epoch 880, val loss: 0.8827006816864014
Epoch 890, training loss: 6.3608527183532715 = 0.03796631097793579 + 1.0 * 6.3228864669799805
Epoch 890, val loss: 0.8890026807785034
Epoch 900, training loss: 6.375734806060791 = 0.03654250502586365 + 1.0 * 6.3391923904418945
Epoch 900, val loss: 0.8953141570091248
Epoch 910, training loss: 6.357761859893799 = 0.035200782120227814 + 1.0 * 6.322561264038086
Epoch 910, val loss: 0.9014230370521545
Epoch 920, training loss: 6.354632377624512 = 0.03393222391605377 + 1.0 * 6.320700168609619
Epoch 920, val loss: 0.9075409173965454
Epoch 930, training loss: 6.35131311416626 = 0.03272558003664017 + 1.0 * 6.318587303161621
Epoch 930, val loss: 0.9136472344398499
Epoch 940, training loss: 6.36271858215332 = 0.031578823924064636 + 1.0 * 6.33113956451416
Epoch 940, val loss: 0.91975998878479
Epoch 950, training loss: 6.349593639373779 = 0.030489768832921982 + 1.0 * 6.319103717803955
Epoch 950, val loss: 0.9256851077079773
Epoch 960, training loss: 6.345839977264404 = 0.02945716120302677 + 1.0 * 6.316382884979248
Epoch 960, val loss: 0.9316195249557495
Epoch 970, training loss: 6.351621150970459 = 0.028473131358623505 + 1.0 * 6.323148250579834
Epoch 970, val loss: 0.9374873638153076
Epoch 980, training loss: 6.345475196838379 = 0.02754034474492073 + 1.0 * 6.317934989929199
Epoch 980, val loss: 0.9432785511016846
Epoch 990, training loss: 6.340821743011475 = 0.026649752631783485 + 1.0 * 6.31417179107666
Epoch 990, val loss: 0.9491015672683716
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 10.536837577819824 = 1.939983606338501 + 1.0 * 8.596854209899902
Epoch 0, val loss: 1.9224125146865845
Epoch 10, training loss: 10.526259422302246 = 1.9296432733535767 + 1.0 * 8.5966157913208
Epoch 10, val loss: 1.9125088453292847
Epoch 20, training loss: 10.51120662689209 = 1.916508436203003 + 1.0 * 8.594697952270508
Epoch 20, val loss: 1.8994826078414917
Epoch 30, training loss: 10.477532386779785 = 1.8980103731155396 + 1.0 * 8.579522132873535
Epoch 30, val loss: 1.88084876537323
Epoch 40, training loss: 10.36140251159668 = 1.872711420059204 + 1.0 * 8.488691329956055
Epoch 40, val loss: 1.8562312126159668
Epoch 50, training loss: 9.955066680908203 = 1.8439714908599854 + 1.0 * 8.111095428466797
Epoch 50, val loss: 1.829609751701355
Epoch 60, training loss: 9.515970230102539 = 1.8189948797225952 + 1.0 * 7.6969757080078125
Epoch 60, val loss: 1.8074601888656616
Epoch 70, training loss: 9.08449935913086 = 1.8005176782608032 + 1.0 * 7.283981800079346
Epoch 70, val loss: 1.7912788391113281
Epoch 80, training loss: 8.868224143981934 = 1.7836720943450928 + 1.0 * 7.084551811218262
Epoch 80, val loss: 1.7767689228057861
Epoch 90, training loss: 8.760193824768066 = 1.7647230625152588 + 1.0 * 6.9954705238342285
Epoch 90, val loss: 1.7606251239776611
Epoch 100, training loss: 8.65744686126709 = 1.7440705299377441 + 1.0 * 6.913376331329346
Epoch 100, val loss: 1.743245244026184
Epoch 110, training loss: 8.557245254516602 = 1.7238225936889648 + 1.0 * 6.833423137664795
Epoch 110, val loss: 1.7265022993087769
Epoch 120, training loss: 8.470972061157227 = 1.7028512954711914 + 1.0 * 6.768121242523193
Epoch 120, val loss: 1.7088958024978638
Epoch 130, training loss: 8.394623756408691 = 1.6785500049591064 + 1.0 * 6.716073513031006
Epoch 130, val loss: 1.6881611347198486
Epoch 140, training loss: 8.327096939086914 = 1.6502032279968262 + 1.0 * 6.67689323425293
Epoch 140, val loss: 1.6638214588165283
Epoch 150, training loss: 8.257795333862305 = 1.6179198026657104 + 1.0 * 6.639875411987305
Epoch 150, val loss: 1.6360663175582886
Epoch 160, training loss: 8.193039894104004 = 1.5808628797531128 + 1.0 * 6.612176895141602
Epoch 160, val loss: 1.6040679216384888
Epoch 170, training loss: 8.128616333007812 = 1.5391939878463745 + 1.0 * 6.589422225952148
Epoch 170, val loss: 1.568360686302185
Epoch 180, training loss: 8.065192222595215 = 1.4931925535202026 + 1.0 * 6.572000026702881
Epoch 180, val loss: 1.5294649600982666
Epoch 190, training loss: 7.998807907104492 = 1.4438835382461548 + 1.0 * 6.554924488067627
Epoch 190, val loss: 1.48824942111969
Epoch 200, training loss: 7.933125019073486 = 1.3923683166503906 + 1.0 * 6.540756702423096
Epoch 200, val loss: 1.4459441900253296
Epoch 210, training loss: 7.8668389320373535 = 1.3396583795547485 + 1.0 * 6.5271806716918945
Epoch 210, val loss: 1.4037026166915894
Epoch 220, training loss: 7.799571990966797 = 1.2866400480270386 + 1.0 * 6.512931823730469
Epoch 220, val loss: 1.3621562719345093
Epoch 230, training loss: 7.736270904541016 = 1.2338645458221436 + 1.0 * 6.502406120300293
Epoch 230, val loss: 1.3217287063598633
Epoch 240, training loss: 7.673071384429932 = 1.1819716691970825 + 1.0 * 6.491099834442139
Epoch 240, val loss: 1.2827343940734863
Epoch 250, training loss: 7.613262176513672 = 1.130936861038208 + 1.0 * 6.482325077056885
Epoch 250, val loss: 1.2448872327804565
Epoch 260, training loss: 7.556387901306152 = 1.0810186862945557 + 1.0 * 6.475369453430176
Epoch 260, val loss: 1.208128809928894
Epoch 270, training loss: 7.49821138381958 = 1.0324488878250122 + 1.0 * 6.465762615203857
Epoch 270, val loss: 1.1728538274765015
Epoch 280, training loss: 7.445521831512451 = 0.9852339029312134 + 1.0 * 6.460288047790527
Epoch 280, val loss: 1.1389210224151611
Epoch 290, training loss: 7.3951334953308105 = 0.9401440620422363 + 1.0 * 6.454989433288574
Epoch 290, val loss: 1.1069061756134033
Epoch 300, training loss: 7.346097946166992 = 0.8976624608039856 + 1.0 * 6.448435306549072
Epoch 300, val loss: 1.0772013664245605
Epoch 310, training loss: 7.299796104431152 = 0.8577378988265991 + 1.0 * 6.442058086395264
Epoch 310, val loss: 1.0496636629104614
Epoch 320, training loss: 7.262706279754639 = 0.8205422163009644 + 1.0 * 6.442163944244385
Epoch 320, val loss: 1.0246059894561768
Epoch 330, training loss: 7.22212028503418 = 0.7866413593292236 + 1.0 * 6.435479164123535
Epoch 330, val loss: 1.002419352531433
Epoch 340, training loss: 7.183606147766113 = 0.7557690143585205 + 1.0 * 6.427836894989014
Epoch 340, val loss: 0.9831854701042175
Epoch 350, training loss: 7.154404640197754 = 0.7274172306060791 + 1.0 * 6.426987171173096
Epoch 350, val loss: 0.9662531018257141
Epoch 360, training loss: 7.122228145599365 = 0.7013002634048462 + 1.0 * 6.420928001403809
Epoch 360, val loss: 0.9514018893241882
Epoch 370, training loss: 7.091670036315918 = 0.6767893433570862 + 1.0 * 6.414880752563477
Epoch 370, val loss: 0.9381677508354187
Epoch 380, training loss: 7.0741868019104 = 0.6534035801887512 + 1.0 * 6.420783042907715
Epoch 380, val loss: 0.9259687662124634
Epoch 390, training loss: 7.040831089019775 = 0.6308581233024597 + 1.0 * 6.40997314453125
Epoch 390, val loss: 0.9146007299423218
Epoch 400, training loss: 7.013162612915039 = 0.608588457107544 + 1.0 * 6.404574394226074
Epoch 400, val loss: 0.9036537408828735
Epoch 410, training loss: 6.989084243774414 = 0.5862184762954712 + 1.0 * 6.402865886688232
Epoch 410, val loss: 0.8927075862884521
Epoch 420, training loss: 6.962929725646973 = 0.5636432766914368 + 1.0 * 6.399286270141602
Epoch 420, val loss: 0.8817803263664246
Epoch 430, training loss: 6.93536901473999 = 0.5405608415603638 + 1.0 * 6.394808292388916
Epoch 430, val loss: 0.8707041144371033
Epoch 440, training loss: 6.915613174438477 = 0.5168683528900146 + 1.0 * 6.398744583129883
Epoch 440, val loss: 0.8593783974647522
Epoch 450, training loss: 6.881887912750244 = 0.49274301528930664 + 1.0 * 6.3891448974609375
Epoch 450, val loss: 0.848091721534729
Epoch 460, training loss: 6.855484962463379 = 0.4684046506881714 + 1.0 * 6.387080192565918
Epoch 460, val loss: 0.8371046185493469
Epoch 470, training loss: 6.828121662139893 = 0.4440617859363556 + 1.0 * 6.384059906005859
Epoch 470, val loss: 0.8265783786773682
Epoch 480, training loss: 6.803070068359375 = 0.4201669991016388 + 1.0 * 6.382903099060059
Epoch 480, val loss: 0.8167937397956848
Epoch 490, training loss: 6.7782206535339355 = 0.3972432315349579 + 1.0 * 6.380977630615234
Epoch 490, val loss: 0.8082138895988464
Epoch 500, training loss: 6.753769397735596 = 0.37544313073158264 + 1.0 * 6.378326416015625
Epoch 500, val loss: 0.8009176254272461
Epoch 510, training loss: 6.733433723449707 = 0.3548718988895416 + 1.0 * 6.378561973571777
Epoch 510, val loss: 0.7948728799819946
Epoch 520, training loss: 6.709036350250244 = 0.33566421270370483 + 1.0 * 6.3733720779418945
Epoch 520, val loss: 0.7901163101196289
Epoch 530, training loss: 6.6958160400390625 = 0.31780198216438293 + 1.0 * 6.378014087677002
Epoch 530, val loss: 0.786484956741333
Epoch 540, training loss: 6.671023368835449 = 0.3013153374195099 + 1.0 * 6.369708061218262
Epoch 540, val loss: 0.7838674783706665
Epoch 550, training loss: 6.6538543701171875 = 0.28599488735198975 + 1.0 * 6.367859363555908
Epoch 550, val loss: 0.782263457775116
Epoch 560, training loss: 6.640199661254883 = 0.2716197371482849 + 1.0 * 6.368579864501953
Epoch 560, val loss: 0.7812546491622925
Epoch 570, training loss: 6.624881267547607 = 0.2580174207687378 + 1.0 * 6.36686372756958
Epoch 570, val loss: 0.7807900905609131
Epoch 580, training loss: 6.606359004974365 = 0.24493174254894257 + 1.0 * 6.361427307128906
Epoch 580, val loss: 0.7806810736656189
Epoch 590, training loss: 6.592155456542969 = 0.23215731978416443 + 1.0 * 6.3599982261657715
Epoch 590, val loss: 0.7807683944702148
Epoch 600, training loss: 6.583037853240967 = 0.21949566900730133 + 1.0 * 6.363542079925537
Epoch 600, val loss: 0.7808579206466675
Epoch 610, training loss: 6.56334114074707 = 0.20685844123363495 + 1.0 * 6.35648250579834
Epoch 610, val loss: 0.7810967564582825
Epoch 620, training loss: 6.548992156982422 = 0.19413413107395172 + 1.0 * 6.354857921600342
Epoch 620, val loss: 0.7812604308128357
Epoch 630, training loss: 6.536945343017578 = 0.18143366277217865 + 1.0 * 6.355511665344238
Epoch 630, val loss: 0.7812508344650269
Epoch 640, training loss: 6.52079439163208 = 0.1689811497926712 + 1.0 * 6.351813316345215
Epoch 640, val loss: 0.7816008925437927
Epoch 650, training loss: 6.506523132324219 = 0.15696091949939728 + 1.0 * 6.349562168121338
Epoch 650, val loss: 0.782240629196167
Epoch 660, training loss: 6.5018181800842285 = 0.14556366205215454 + 1.0 * 6.356254577636719
Epoch 660, val loss: 0.7832862138748169
Epoch 670, training loss: 6.487813472747803 = 0.13501201570034027 + 1.0 * 6.352801322937012
Epoch 670, val loss: 0.7848283052444458
Epoch 680, training loss: 6.472203254699707 = 0.12544013559818268 + 1.0 * 6.3467631340026855
Epoch 680, val loss: 0.7872844934463501
Epoch 690, training loss: 6.461184024810791 = 0.11674164235591888 + 1.0 * 6.344442367553711
Epoch 690, val loss: 0.7902663946151733
Epoch 700, training loss: 6.458006381988525 = 0.1088891550898552 + 1.0 * 6.349117279052734
Epoch 700, val loss: 0.7937333583831787
Epoch 710, training loss: 6.445307731628418 = 0.10185350477695465 + 1.0 * 6.343454360961914
Epoch 710, val loss: 0.7978132963180542
Epoch 720, training loss: 6.435310363769531 = 0.095512755215168 + 1.0 * 6.339797496795654
Epoch 720, val loss: 0.8022052645683289
Epoch 730, training loss: 6.42848014831543 = 0.08977189660072327 + 1.0 * 6.338708400726318
Epoch 730, val loss: 0.8067337870597839
Epoch 740, training loss: 6.424076080322266 = 0.0845460593700409 + 1.0 * 6.339529991149902
Epoch 740, val loss: 0.8114844560623169
Epoch 750, training loss: 6.417133808135986 = 0.07979343086481094 + 1.0 * 6.337340354919434
Epoch 750, val loss: 0.8163813948631287
Epoch 760, training loss: 6.413620948791504 = 0.07545007765293121 + 1.0 * 6.338171005249023
Epoch 760, val loss: 0.8212729096412659
Epoch 770, training loss: 6.405245780944824 = 0.07147055119276047 + 1.0 * 6.333775043487549
Epoch 770, val loss: 0.8262551426887512
Epoch 780, training loss: 6.3996901512146 = 0.06779340654611588 + 1.0 * 6.331896781921387
Epoch 780, val loss: 0.8312170505523682
Epoch 790, training loss: 6.405438423156738 = 0.06436466425657272 + 1.0 * 6.341073989868164
Epoch 790, val loss: 0.8360404372215271
Epoch 800, training loss: 6.396152019500732 = 0.06118679791688919 + 1.0 * 6.334965229034424
Epoch 800, val loss: 0.8406476974487305
Epoch 810, training loss: 6.385416030883789 = 0.05825426056981087 + 1.0 * 6.32716178894043
Epoch 810, val loss: 0.845558762550354
Epoch 820, training loss: 6.382750511169434 = 0.05549811199307442 + 1.0 * 6.327252388000488
Epoch 820, val loss: 0.8502209186553955
Epoch 830, training loss: 6.392398834228516 = 0.052918847650289536 + 1.0 * 6.339479923248291
Epoch 830, val loss: 0.8547593951225281
Epoch 840, training loss: 6.375164031982422 = 0.05051711946725845 + 1.0 * 6.324646949768066
Epoch 840, val loss: 0.8594071269035339
Epoch 850, training loss: 6.372781753540039 = 0.04827869310975075 + 1.0 * 6.324502944946289
Epoch 850, val loss: 0.8641236424446106
Epoch 860, training loss: 6.370693683624268 = 0.04617201164364815 + 1.0 * 6.324521541595459
Epoch 860, val loss: 0.8686647415161133
Epoch 870, training loss: 6.3655900955200195 = 0.04419265314936638 + 1.0 * 6.321397304534912
Epoch 870, val loss: 0.8732674717903137
Epoch 880, training loss: 6.364013195037842 = 0.04233318194746971 + 1.0 * 6.321680068969727
Epoch 880, val loss: 0.878015398979187
Epoch 890, training loss: 6.370973110198975 = 0.04058918356895447 + 1.0 * 6.330383777618408
Epoch 890, val loss: 0.8826674818992615
Epoch 900, training loss: 6.359123229980469 = 0.038949403911828995 + 1.0 * 6.320173740386963
Epoch 900, val loss: 0.8871709108352661
Epoch 910, training loss: 6.356167316436768 = 0.03741266578435898 + 1.0 * 6.31875467300415
Epoch 910, val loss: 0.8919312357902527
Epoch 920, training loss: 6.354462623596191 = 0.03596062213182449 + 1.0 * 6.318501949310303
Epoch 920, val loss: 0.8965075612068176
Epoch 930, training loss: 6.350735664367676 = 0.03459053859114647 + 1.0 * 6.316144943237305
Epoch 930, val loss: 0.9011432528495789
Epoch 940, training loss: 6.351138591766357 = 0.03329692780971527 + 1.0 * 6.317841529846191
Epoch 940, val loss: 0.9058231115341187
Epoch 950, training loss: 6.351202011108398 = 0.03207575902342796 + 1.0 * 6.319126129150391
Epoch 950, val loss: 0.9105820655822754
Epoch 960, training loss: 6.347196578979492 = 0.030923334881663322 + 1.0 * 6.316273212432861
Epoch 960, val loss: 0.9151795506477356
Epoch 970, training loss: 6.341724872589111 = 0.029830848798155785 + 1.0 * 6.311893939971924
Epoch 970, val loss: 0.919934868812561
Epoch 980, training loss: 6.34428071975708 = 0.02879326231777668 + 1.0 * 6.315487384796143
Epoch 980, val loss: 0.9245158433914185
Epoch 990, training loss: 6.340988636016846 = 0.027812883257865906 + 1.0 * 6.313175678253174
Epoch 990, val loss: 0.9290934205055237
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8365840801265156
The final CL Acc:0.79630, 0.01210, The final GNN Acc:0.83781, 0.00090
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10552])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.529791831970215 = 1.9329692125320435 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.9396283626556396
Epoch 10, training loss: 10.520042419433594 = 1.9234944581985474 + 1.0 * 8.596548080444336
Epoch 10, val loss: 1.9297865629196167
Epoch 20, training loss: 10.50601577758789 = 1.9115118980407715 + 1.0 * 8.594504356384277
Epoch 20, val loss: 1.9170575141906738
Epoch 30, training loss: 10.473223686218262 = 1.8946925401687622 + 1.0 * 8.578531265258789
Epoch 30, val loss: 1.8991124629974365
Epoch 40, training loss: 10.354574203491211 = 1.8730605840682983 + 1.0 * 8.481513977050781
Epoch 40, val loss: 1.8768081665039062
Epoch 50, training loss: 9.99526596069336 = 1.850730538368225 + 1.0 * 8.144535064697266
Epoch 50, val loss: 1.8543274402618408
Epoch 60, training loss: 9.581033706665039 = 1.8312276601791382 + 1.0 * 7.749805927276611
Epoch 60, val loss: 1.8352622985839844
Epoch 70, training loss: 9.064352989196777 = 1.8185583353042603 + 1.0 * 7.245794296264648
Epoch 70, val loss: 1.82266104221344
Epoch 80, training loss: 8.82463264465332 = 1.8082287311553955 + 1.0 * 7.016403675079346
Epoch 80, val loss: 1.8116168975830078
Epoch 90, training loss: 8.690868377685547 = 1.793880581855774 + 1.0 * 6.896987438201904
Epoch 90, val loss: 1.7972445487976074
Epoch 100, training loss: 8.598132133483887 = 1.779402256011963 + 1.0 * 6.818729877471924
Epoch 100, val loss: 1.7831279039382935
Epoch 110, training loss: 8.532712936401367 = 1.7655621767044067 + 1.0 * 6.767150402069092
Epoch 110, val loss: 1.7694206237792969
Epoch 120, training loss: 8.47400951385498 = 1.7509468793869019 + 1.0 * 6.723062992095947
Epoch 120, val loss: 1.755383014678955
Epoch 130, training loss: 8.418722152709961 = 1.7346642017364502 + 1.0 * 6.68405818939209
Epoch 130, val loss: 1.7406022548675537
Epoch 140, training loss: 8.366820335388184 = 1.7163532972335815 + 1.0 * 6.6504669189453125
Epoch 140, val loss: 1.72458815574646
Epoch 150, training loss: 8.317828178405762 = 1.6953998804092407 + 1.0 * 6.6224284172058105
Epoch 150, val loss: 1.70661199092865
Epoch 160, training loss: 8.26864242553711 = 1.6712771654129028 + 1.0 * 6.597365379333496
Epoch 160, val loss: 1.6860244274139404
Epoch 170, training loss: 8.220219612121582 = 1.6434675455093384 + 1.0 * 6.576751708984375
Epoch 170, val loss: 1.6624351739883423
Epoch 180, training loss: 8.169609069824219 = 1.6118388175964355 + 1.0 * 6.557769775390625
Epoch 180, val loss: 1.6357998847961426
Epoch 190, training loss: 8.117334365844727 = 1.5761706829071045 + 1.0 * 6.541163921356201
Epoch 190, val loss: 1.606054663658142
Epoch 200, training loss: 8.070321083068848 = 1.5364562273025513 + 1.0 * 6.533864974975586
Epoch 200, val loss: 1.5734647512435913
Epoch 210, training loss: 8.009330749511719 = 1.4941097497940063 + 1.0 * 6.515221118927002
Epoch 210, val loss: 1.5392388105392456
Epoch 220, training loss: 7.953405857086182 = 1.4491418600082397 + 1.0 * 6.504263877868652
Epoch 220, val loss: 1.5034505128860474
Epoch 230, training loss: 7.89572286605835 = 1.4015599489212036 + 1.0 * 6.4941630363464355
Epoch 230, val loss: 1.4662531614303589
Epoch 240, training loss: 7.844922065734863 = 1.352787971496582 + 1.0 * 6.492134094238281
Epoch 240, val loss: 1.4290770292282104
Epoch 250, training loss: 7.784151554107666 = 1.3041157722473145 + 1.0 * 6.480035781860352
Epoch 250, val loss: 1.392429232597351
Epoch 260, training loss: 7.727989673614502 = 1.2552909851074219 + 1.0 * 6.47269868850708
Epoch 260, val loss: 1.356319785118103
Epoch 270, training loss: 7.673127174377441 = 1.2068297863006592 + 1.0 * 6.466297626495361
Epoch 270, val loss: 1.3208286762237549
Epoch 280, training loss: 7.636610984802246 = 1.1594617366790771 + 1.0 * 6.477149486541748
Epoch 280, val loss: 1.286586046218872
Epoch 290, training loss: 7.575828552246094 = 1.1151949167251587 + 1.0 * 6.460633754730225
Epoch 290, val loss: 1.254832148551941
Epoch 300, training loss: 7.52540922164917 = 1.0732841491699219 + 1.0 * 6.452125072479248
Epoch 300, val loss: 1.2251036167144775
Epoch 310, training loss: 7.480552673339844 = 1.0336259603500366 + 1.0 * 6.446926593780518
Epoch 310, val loss: 1.197224736213684
Epoch 320, training loss: 7.438206672668457 = 0.9958685636520386 + 1.0 * 6.442337989807129
Epoch 320, val loss: 1.170868158340454
Epoch 330, training loss: 7.400176048278809 = 0.9599357843399048 + 1.0 * 6.440240383148193
Epoch 330, val loss: 1.146040678024292
Epoch 340, training loss: 7.362526893615723 = 0.9257836937904358 + 1.0 * 6.436743259429932
Epoch 340, val loss: 1.1223924160003662
Epoch 350, training loss: 7.32323694229126 = 0.8926343321800232 + 1.0 * 6.430602550506592
Epoch 350, val loss: 1.0995054244995117
Epoch 360, training loss: 7.287588119506836 = 0.8600084781646729 + 1.0 * 6.427579402923584
Epoch 360, val loss: 1.0768591165542603
Epoch 370, training loss: 7.255626201629639 = 0.8278917074203491 + 1.0 * 6.427734375
Epoch 370, val loss: 1.054442286491394
Epoch 380, training loss: 7.2180938720703125 = 0.7963491082191467 + 1.0 * 6.4217448234558105
Epoch 380, val loss: 1.0322712659835815
Epoch 390, training loss: 7.183435916900635 = 0.7650924921035767 + 1.0 * 6.418343544006348
Epoch 390, val loss: 1.010209083557129
Epoch 400, training loss: 7.148914813995361 = 0.7343149185180664 + 1.0 * 6.414599895477295
Epoch 400, val loss: 0.9884803891181946
Epoch 410, training loss: 7.115843772888184 = 0.704103946685791 + 1.0 * 6.411739826202393
Epoch 410, val loss: 0.9675077199935913
Epoch 420, training loss: 7.082681655883789 = 0.6744973659515381 + 1.0 * 6.408184051513672
Epoch 420, val loss: 0.9474430680274963
Epoch 430, training loss: 7.060636520385742 = 0.6454984545707703 + 1.0 * 6.415138244628906
Epoch 430, val loss: 0.9286155700683594
Epoch 440, training loss: 7.022360324859619 = 0.6174143552780151 + 1.0 * 6.4049458503723145
Epoch 440, val loss: 0.9112359881401062
Epoch 450, training loss: 6.99018669128418 = 0.5900323987007141 + 1.0 * 6.400154113769531
Epoch 450, val loss: 0.8954113721847534
Epoch 460, training loss: 6.959972381591797 = 0.5633135437965393 + 1.0 * 6.396658897399902
Epoch 460, val loss: 0.8811326622962952
Epoch 470, training loss: 6.939620018005371 = 0.5373473167419434 + 1.0 * 6.402272701263428
Epoch 470, val loss: 0.8684161901473999
Epoch 480, training loss: 6.906482219696045 = 0.5126005411148071 + 1.0 * 6.393881797790527
Epoch 480, val loss: 0.8574017286300659
Epoch 490, training loss: 6.878559112548828 = 0.4888160228729248 + 1.0 * 6.389743328094482
Epoch 490, val loss: 0.8479368090629578
Epoch 500, training loss: 6.852715969085693 = 0.4657723605632782 + 1.0 * 6.386943817138672
Epoch 500, val loss: 0.8398292064666748
Epoch 510, training loss: 6.827571868896484 = 0.4433516561985016 + 1.0 * 6.384220123291016
Epoch 510, val loss: 0.8327602744102478
Epoch 520, training loss: 6.820433616638184 = 0.4214740991592407 + 1.0 * 6.398959636688232
Epoch 520, val loss: 0.8266339898109436
Epoch 530, training loss: 6.7851433753967285 = 0.40030986070632935 + 1.0 * 6.384833335876465
Epoch 530, val loss: 0.8212927579879761
Epoch 540, training loss: 6.759072780609131 = 0.37958672642707825 + 1.0 * 6.379486083984375
Epoch 540, val loss: 0.8165998458862305
Epoch 550, training loss: 6.7362961769104 = 0.3591431975364685 + 1.0 * 6.377152919769287
Epoch 550, val loss: 0.8125016093254089
Epoch 560, training loss: 6.715891361236572 = 0.33899185061454773 + 1.0 * 6.376899719238281
Epoch 560, val loss: 0.8087989687919617
Epoch 570, training loss: 6.694153308868408 = 0.31929826736450195 + 1.0 * 6.374855041503906
Epoch 570, val loss: 0.8053826689720154
Epoch 580, training loss: 6.672166347503662 = 0.2999747097492218 + 1.0 * 6.372191429138184
Epoch 580, val loss: 0.8025398254394531
Epoch 590, training loss: 6.653679847717285 = 0.28104108572006226 + 1.0 * 6.372638702392578
Epoch 590, val loss: 0.8002632856369019
Epoch 600, training loss: 6.6300530433654785 = 0.26263704895973206 + 1.0 * 6.367415904998779
Epoch 600, val loss: 0.7984959483146667
Epoch 610, training loss: 6.6157145500183105 = 0.2449398636817932 + 1.0 * 6.370774745941162
Epoch 610, val loss: 0.7973263263702393
Epoch 620, training loss: 6.596975803375244 = 0.22808845341205597 + 1.0 * 6.368887424468994
Epoch 620, val loss: 0.7969984412193298
Epoch 630, training loss: 6.575648784637451 = 0.2121671587228775 + 1.0 * 6.363481521606445
Epoch 630, val loss: 0.7973728179931641
Epoch 640, training loss: 6.560000419616699 = 0.19717919826507568 + 1.0 * 6.362821102142334
Epoch 640, val loss: 0.7987185120582581
Epoch 650, training loss: 6.546713829040527 = 0.18323147296905518 + 1.0 * 6.363482475280762
Epoch 650, val loss: 0.8009101152420044
Epoch 660, training loss: 6.529942035675049 = 0.1703646034002304 + 1.0 * 6.359577655792236
Epoch 660, val loss: 0.8037428855895996
Epoch 670, training loss: 6.51771879196167 = 0.15850257873535156 + 1.0 * 6.359216213226318
Epoch 670, val loss: 0.8075146079063416
Epoch 680, training loss: 6.507217884063721 = 0.14760200679302216 + 1.0 * 6.359615802764893
Epoch 680, val loss: 0.8120912313461304
Epoch 690, training loss: 6.493565559387207 = 0.13765890896320343 + 1.0 * 6.3559064865112305
Epoch 690, val loss: 0.8171265721321106
Epoch 700, training loss: 6.491837978363037 = 0.1285524070262909 + 1.0 * 6.363285541534424
Epoch 700, val loss: 0.8228869438171387
Epoch 710, training loss: 6.472749710083008 = 0.12026674300432205 + 1.0 * 6.352482795715332
Epoch 710, val loss: 0.8290812969207764
Epoch 720, training loss: 6.4632344245910645 = 0.11266089230775833 + 1.0 * 6.350573539733887
Epoch 720, val loss: 0.8357855081558228
Epoch 730, training loss: 6.45626163482666 = 0.10565973818302155 + 1.0 * 6.350601673126221
Epoch 730, val loss: 0.8429834246635437
Epoch 740, training loss: 6.451329708099365 = 0.09923645108938217 + 1.0 * 6.35209321975708
Epoch 740, val loss: 0.8504135608673096
Epoch 750, training loss: 6.44105339050293 = 0.09335769712924957 + 1.0 * 6.347695827484131
Epoch 750, val loss: 0.8578779697418213
Epoch 760, training loss: 6.43449592590332 = 0.08792174607515335 + 1.0 * 6.346574306488037
Epoch 760, val loss: 0.8657035231590271
Epoch 770, training loss: 6.430881023406982 = 0.08288595825433731 + 1.0 * 6.347995281219482
Epoch 770, val loss: 0.8737108707427979
Epoch 780, training loss: 6.423425674438477 = 0.07823149859905243 + 1.0 * 6.345194339752197
Epoch 780, val loss: 0.8817352652549744
Epoch 790, training loss: 6.4185791015625 = 0.07394368946552277 + 1.0 * 6.344635486602783
Epoch 790, val loss: 0.8898769617080688
Epoch 800, training loss: 6.4122514724731445 = 0.06996653228998184 + 1.0 * 6.34228515625
Epoch 800, val loss: 0.8979726433753967
Epoch 810, training loss: 6.405194282531738 = 0.06627927720546722 + 1.0 * 6.33891487121582
Epoch 810, val loss: 0.9061656594276428
Epoch 820, training loss: 6.406353950500488 = 0.06283780932426453 + 1.0 * 6.3435163497924805
Epoch 820, val loss: 0.9144026041030884
Epoch 830, training loss: 6.4052019119262695 = 0.05963373929262161 + 1.0 * 6.3455681800842285
Epoch 830, val loss: 0.9225419759750366
Epoch 840, training loss: 6.3945841789245605 = 0.05666483938694 + 1.0 * 6.337919235229492
Epoch 840, val loss: 0.9304565191268921
Epoch 850, training loss: 6.388522624969482 = 0.05389789491891861 + 1.0 * 6.334624767303467
Epoch 850, val loss: 0.9383360743522644
Epoch 860, training loss: 6.3859381675720215 = 0.051303453743457794 + 1.0 * 6.334634780883789
Epoch 860, val loss: 0.946314811706543
Epoch 870, training loss: 6.3853983879089355 = 0.04887259751558304 + 1.0 * 6.336525917053223
Epoch 870, val loss: 0.9541959166526794
Epoch 880, training loss: 6.38299036026001 = 0.046606674790382385 + 1.0 * 6.336383819580078
Epoch 880, val loss: 0.9619429111480713
Epoch 890, training loss: 6.377584934234619 = 0.044494908303022385 + 1.0 * 6.333089828491211
Epoch 890, val loss: 0.9695137143135071
Epoch 900, training loss: 6.373580455780029 = 0.04251426085829735 + 1.0 * 6.331066131591797
Epoch 900, val loss: 0.9770500063896179
Epoch 910, training loss: 6.375412940979004 = 0.04065424203872681 + 1.0 * 6.334758758544922
Epoch 910, val loss: 0.9846010804176331
Epoch 920, training loss: 6.368361473083496 = 0.038915012031793594 + 1.0 * 6.329446315765381
Epoch 920, val loss: 0.9918580055236816
Epoch 930, training loss: 6.3640336990356445 = 0.037282153964042664 + 1.0 * 6.326751708984375
Epoch 930, val loss: 0.9989781379699707
Epoch 940, training loss: 6.362159729003906 = 0.03574724122881889 + 1.0 * 6.326412677764893
Epoch 940, val loss: 1.006160855293274
Epoch 950, training loss: 6.371747016906738 = 0.03429928421974182 + 1.0 * 6.337447643280029
Epoch 950, val loss: 1.013250470161438
Epoch 960, training loss: 6.360890865325928 = 0.03293919190764427 + 1.0 * 6.327951908111572
Epoch 960, val loss: 1.0199964046478271
Epoch 970, training loss: 6.3561553955078125 = 0.031665898859500885 + 1.0 * 6.324489593505859
Epoch 970, val loss: 1.02668035030365
Epoch 980, training loss: 6.356994152069092 = 0.030459066852927208 + 1.0 * 6.326535224914551
Epoch 980, val loss: 1.0334123373031616
Epoch 990, training loss: 6.351663112640381 = 0.02932426705956459 + 1.0 * 6.322339057922363
Epoch 990, val loss: 1.0398679971694946
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 10.530863761901855 = 1.9340513944625854 + 1.0 * 8.59681224822998
Epoch 0, val loss: 1.9370636940002441
Epoch 10, training loss: 10.52084732055664 = 1.9243667125701904 + 1.0 * 8.596480369567871
Epoch 10, val loss: 1.9268637895584106
Epoch 20, training loss: 10.506464958190918 = 1.9124490022659302 + 1.0 * 8.594016075134277
Epoch 20, val loss: 1.9141565561294556
Epoch 30, training loss: 10.471850395202637 = 1.896000623703003 + 1.0 * 8.575849533081055
Epoch 30, val loss: 1.8966665267944336
Epoch 40, training loss: 10.329280853271484 = 1.8749011754989624 + 1.0 * 8.45438003540039
Epoch 40, val loss: 1.8752906322479248
Epoch 50, training loss: 9.82005786895752 = 1.853176474571228 + 1.0 * 7.96688175201416
Epoch 50, val loss: 1.854082465171814
Epoch 60, training loss: 9.298707962036133 = 1.8372613191604614 + 1.0 * 7.461446762084961
Epoch 60, val loss: 1.840074062347412
Epoch 70, training loss: 9.003107070922852 = 1.825358271598816 + 1.0 * 7.177748680114746
Epoch 70, val loss: 1.829283595085144
Epoch 80, training loss: 8.833612442016602 = 1.8124184608459473 + 1.0 * 7.021193504333496
Epoch 80, val loss: 1.8172755241394043
Epoch 90, training loss: 8.709850311279297 = 1.7977927923202515 + 1.0 * 6.912057399749756
Epoch 90, val loss: 1.803961157798767
Epoch 100, training loss: 8.597801208496094 = 1.7839988470077515 + 1.0 * 6.813802242279053
Epoch 100, val loss: 1.7917697429656982
Epoch 110, training loss: 8.517861366271973 = 1.770869493484497 + 1.0 * 6.746992111206055
Epoch 110, val loss: 1.7803354263305664
Epoch 120, training loss: 8.455530166625977 = 1.7568800449371338 + 1.0 * 6.698650360107422
Epoch 120, val loss: 1.768194556236267
Epoch 130, training loss: 8.3976469039917 = 1.741376519203186 + 1.0 * 6.6562700271606445
Epoch 130, val loss: 1.755027174949646
Epoch 140, training loss: 8.346158981323242 = 1.7238969802856445 + 1.0 * 6.622262001037598
Epoch 140, val loss: 1.7403132915496826
Epoch 150, training loss: 8.300491333007812 = 1.7036198377609253 + 1.0 * 6.596871376037598
Epoch 150, val loss: 1.7234529256820679
Epoch 160, training loss: 8.254674911499023 = 1.680013656616211 + 1.0 * 6.5746612548828125
Epoch 160, val loss: 1.7040103673934937
Epoch 170, training loss: 8.209354400634766 = 1.6523292064666748 + 1.0 * 6.55702543258667
Epoch 170, val loss: 1.681434988975525
Epoch 180, training loss: 8.16128158569336 = 1.6199393272399902 + 1.0 * 6.541342258453369
Epoch 180, val loss: 1.6551756858825684
Epoch 190, training loss: 8.114923477172852 = 1.5825986862182617 + 1.0 * 6.532325267791748
Epoch 190, val loss: 1.6249699592590332
Epoch 200, training loss: 8.058658599853516 = 1.5411436557769775 + 1.0 * 6.517514705657959
Epoch 200, val loss: 1.5917154550552368
Epoch 210, training loss: 8.001545906066895 = 1.4955251216888428 + 1.0 * 6.506021022796631
Epoch 210, val loss: 1.5552600622177124
Epoch 220, training loss: 7.943027973175049 = 1.4459642171859741 + 1.0 * 6.497063636779785
Epoch 220, val loss: 1.5159083604812622
Epoch 230, training loss: 7.883721828460693 = 1.3934170007705688 + 1.0 * 6.490304946899414
Epoch 230, val loss: 1.474635124206543
Epoch 240, training loss: 7.821552276611328 = 1.3400094509124756 + 1.0 * 6.481543064117432
Epoch 240, val loss: 1.4334644079208374
Epoch 250, training loss: 7.761927127838135 = 1.2866946458816528 + 1.0 * 6.4752326011657715
Epoch 250, val loss: 1.3928550481796265
Epoch 260, training loss: 7.702244758605957 = 1.233729600906372 + 1.0 * 6.468515396118164
Epoch 260, val loss: 1.3532354831695557
Epoch 270, training loss: 7.648318290710449 = 1.1824748516082764 + 1.0 * 6.465843200683594
Epoch 270, val loss: 1.315617561340332
Epoch 280, training loss: 7.591668605804443 = 1.133897304534912 + 1.0 * 6.457771301269531
Epoch 280, val loss: 1.2808094024658203
Epoch 290, training loss: 7.538158416748047 = 1.0876328945159912 + 1.0 * 6.450525283813477
Epoch 290, val loss: 1.2481924295425415
Epoch 300, training loss: 7.497241497039795 = 1.0436266660690308 + 1.0 * 6.453614711761475
Epoch 300, val loss: 1.21799635887146
Epoch 310, training loss: 7.447532653808594 = 1.0026966333389282 + 1.0 * 6.444836139678955
Epoch 310, val loss: 1.1903554201126099
Epoch 320, training loss: 7.3997321128845215 = 0.9640889763832092 + 1.0 * 6.435643196105957
Epoch 320, val loss: 1.165041446685791
Epoch 330, training loss: 7.357898712158203 = 0.927106499671936 + 1.0 * 6.430792331695557
Epoch 330, val loss: 1.141247272491455
Epoch 340, training loss: 7.319129943847656 = 0.891305148601532 + 1.0 * 6.427824974060059
Epoch 340, val loss: 1.1187036037445068
Epoch 350, training loss: 7.280824184417725 = 0.8571173548698425 + 1.0 * 6.423707008361816
Epoch 350, val loss: 1.0973753929138184
Epoch 360, training loss: 7.2425007820129395 = 0.8243871331214905 + 1.0 * 6.418113708496094
Epoch 360, val loss: 1.0774599313735962
Epoch 370, training loss: 7.206216812133789 = 0.7926061153411865 + 1.0 * 6.413610935211182
Epoch 370, val loss: 1.058316707611084
Epoch 380, training loss: 7.171984672546387 = 0.7616480588912964 + 1.0 * 6.410336494445801
Epoch 380, val loss: 1.0400536060333252
Epoch 390, training loss: 7.152309894561768 = 0.7318445444107056 + 1.0 * 6.420465469360352
Epoch 390, val loss: 1.0227867364883423
Epoch 400, training loss: 7.109544277191162 = 0.7035725712776184 + 1.0 * 6.405971527099609
Epoch 400, val loss: 1.0070723295211792
Epoch 410, training loss: 7.077170372009277 = 0.676342248916626 + 1.0 * 6.4008283615112305
Epoch 410, val loss: 0.9925917983055115
Epoch 420, training loss: 7.046650409698486 = 0.6498391032218933 + 1.0 * 6.396811485290527
Epoch 420, val loss: 0.9790240526199341
Epoch 430, training loss: 7.017503261566162 = 0.6237841248512268 + 1.0 * 6.39371919631958
Epoch 430, val loss: 0.9664016366004944
Epoch 440, training loss: 6.990095138549805 = 0.5979540944099426 + 1.0 * 6.392140865325928
Epoch 440, val loss: 0.9545533061027527
Epoch 450, training loss: 6.967702388763428 = 0.57248455286026 + 1.0 * 6.3952178955078125
Epoch 450, val loss: 0.9432797431945801
Epoch 460, training loss: 6.935247421264648 = 0.5472161173820496 + 1.0 * 6.388031482696533
Epoch 460, val loss: 0.9330316185951233
Epoch 470, training loss: 6.905917167663574 = 0.5219659805297852 + 1.0 * 6.383951187133789
Epoch 470, val loss: 0.9234423637390137
Epoch 480, training loss: 6.885946273803711 = 0.49671515822410583 + 1.0 * 6.389231204986572
Epoch 480, val loss: 0.9145862460136414
Epoch 490, training loss: 6.850885391235352 = 0.47186383605003357 + 1.0 * 6.379021644592285
Epoch 490, val loss: 0.906735360622406
Epoch 500, training loss: 6.831277370452881 = 0.4474674463272095 + 1.0 * 6.383810043334961
Epoch 500, val loss: 0.8998101949691772
Epoch 510, training loss: 6.798989295959473 = 0.423933207988739 + 1.0 * 6.375056266784668
Epoch 510, val loss: 0.8939363956451416
Epoch 520, training loss: 6.773065567016602 = 0.4013126492500305 + 1.0 * 6.371752738952637
Epoch 520, val loss: 0.8894099593162537
Epoch 530, training loss: 6.762665748596191 = 0.3798619210720062 + 1.0 * 6.382803916931152
Epoch 530, val loss: 0.8860117197036743
Epoch 540, training loss: 6.731783866882324 = 0.35984286665916443 + 1.0 * 6.371941089630127
Epoch 540, val loss: 0.8839414715766907
Epoch 550, training loss: 6.708497047424316 = 0.3411710560321808 + 1.0 * 6.367325782775879
Epoch 550, val loss: 0.8831732273101807
Epoch 560, training loss: 6.696124076843262 = 0.32380175590515137 + 1.0 * 6.3723225593566895
Epoch 560, val loss: 0.8835090398788452
Epoch 570, training loss: 6.672953128814697 = 0.3077735900878906 + 1.0 * 6.365179538726807
Epoch 570, val loss: 0.884902834892273
Epoch 580, training loss: 6.654081344604492 = 0.2928231954574585 + 1.0 * 6.361258029937744
Epoch 580, val loss: 0.8872063755989075
Epoch 590, training loss: 6.639792442321777 = 0.2788900136947632 + 1.0 * 6.360902309417725
Epoch 590, val loss: 0.8903135657310486
Epoch 600, training loss: 6.6311845779418945 = 0.26586440205574036 + 1.0 * 6.365320205688477
Epoch 600, val loss: 0.8940086960792542
Epoch 610, training loss: 6.611059665679932 = 0.25369101762771606 + 1.0 * 6.357368469238281
Epoch 610, val loss: 0.8984405398368835
Epoch 620, training loss: 6.596117973327637 = 0.24212488532066345 + 1.0 * 6.353992938995361
Epoch 620, val loss: 0.9033765196800232
Epoch 630, training loss: 6.58306360244751 = 0.2310236692428589 + 1.0 * 6.352039813995361
Epoch 630, val loss: 0.9087603688240051
Epoch 640, training loss: 6.589595794677734 = 0.22028659284114838 + 1.0 * 6.369309425354004
Epoch 640, val loss: 0.9145355820655823
Epoch 650, training loss: 6.560656547546387 = 0.20995888113975525 + 1.0 * 6.3506975173950195
Epoch 650, val loss: 0.9206773638725281
Epoch 660, training loss: 6.546891212463379 = 0.19986660778522491 + 1.0 * 6.347024440765381
Epoch 660, val loss: 0.9271860122680664
Epoch 670, training loss: 6.5362958908081055 = 0.18994104862213135 + 1.0 * 6.346354961395264
Epoch 670, val loss: 0.9340387582778931
Epoch 680, training loss: 6.537271499633789 = 0.1802177131175995 + 1.0 * 6.357053756713867
Epoch 680, val loss: 0.9411752820014954
Epoch 690, training loss: 6.5180745124816895 = 0.17075857520103455 + 1.0 * 6.347315788269043
Epoch 690, val loss: 0.9486737251281738
Epoch 700, training loss: 6.504589557647705 = 0.16156595945358276 + 1.0 * 6.343023777008057
Epoch 700, val loss: 0.9566043615341187
Epoch 710, training loss: 6.49343204498291 = 0.15265247225761414 + 1.0 * 6.340779781341553
Epoch 710, val loss: 0.9648774266242981
Epoch 720, training loss: 6.489895820617676 = 0.14409036934375763 + 1.0 * 6.345805644989014
Epoch 720, val loss: 0.9734904170036316
Epoch 730, training loss: 6.477847576141357 = 0.135975643992424 + 1.0 * 6.341871738433838
Epoch 730, val loss: 0.9824206233024597
Epoch 740, training loss: 6.466348648071289 = 0.12828056514263153 + 1.0 * 6.338068008422852
Epoch 740, val loss: 0.9916419386863708
Epoch 750, training loss: 6.459068775177002 = 0.12100207060575485 + 1.0 * 6.338066577911377
Epoch 750, val loss: 1.0010513067245483
Epoch 760, training loss: 6.4497480392456055 = 0.1141800582408905 + 1.0 * 6.335567951202393
Epoch 760, val loss: 1.0106232166290283
Epoch 770, training loss: 6.443016529083252 = 0.10780897736549377 + 1.0 * 6.335207462310791
Epoch 770, val loss: 1.0204588174819946
Epoch 780, training loss: 6.4333953857421875 = 0.10183805227279663 + 1.0 * 6.331557273864746
Epoch 780, val loss: 1.0303760766983032
Epoch 790, training loss: 6.430617809295654 = 0.09623043239116669 + 1.0 * 6.334387302398682
Epoch 790, val loss: 1.0403728485107422
Epoch 800, training loss: 6.426774024963379 = 0.09104439616203308 + 1.0 * 6.335729598999023
Epoch 800, val loss: 1.0504196882247925
Epoch 810, training loss: 6.415216445922852 = 0.08619234710931778 + 1.0 * 6.329024314880371
Epoch 810, val loss: 1.0605757236480713
Epoch 820, training loss: 6.409308433532715 = 0.08165721595287323 + 1.0 * 6.327651023864746
Epoch 820, val loss: 1.0706870555877686
Epoch 830, training loss: 6.403306007385254 = 0.07741503417491913 + 1.0 * 6.325891017913818
Epoch 830, val loss: 1.0809074640274048
Epoch 840, training loss: 6.425340175628662 = 0.07344915717840195 + 1.0 * 6.351891040802002
Epoch 840, val loss: 1.0910894870758057
Epoch 850, training loss: 6.3950581550598145 = 0.06976534426212311 + 1.0 * 6.325292587280273
Epoch 850, val loss: 1.1011302471160889
Epoch 860, training loss: 6.389845371246338 = 0.06633398681879044 + 1.0 * 6.323511600494385
Epoch 860, val loss: 1.1113170385360718
Epoch 870, training loss: 6.385559558868408 = 0.06312073022127151 + 1.0 * 6.322438716888428
Epoch 870, val loss: 1.1213456392288208
Epoch 880, training loss: 6.381989479064941 = 0.0601048581302166 + 1.0 * 6.321884632110596
Epoch 880, val loss: 1.1314150094985962
Epoch 890, training loss: 6.382462978363037 = 0.0572805255651474 + 1.0 * 6.3251824378967285
Epoch 890, val loss: 1.1413252353668213
Epoch 900, training loss: 6.37805700302124 = 0.0546460822224617 + 1.0 * 6.323410987854004
Epoch 900, val loss: 1.151138424873352
Epoch 910, training loss: 6.371237277984619 = 0.05218121036887169 + 1.0 * 6.319056034088135
Epoch 910, val loss: 1.1608386039733887
Epoch 920, training loss: 6.370027542114258 = 0.04986973851919174 + 1.0 * 6.320158004760742
Epoch 920, val loss: 1.1704156398773193
Epoch 930, training loss: 6.3682732582092285 = 0.04769850894808769 + 1.0 * 6.320574760437012
Epoch 930, val loss: 1.179856300354004
Epoch 940, training loss: 6.363490104675293 = 0.04566885903477669 + 1.0 * 6.317821025848389
Epoch 940, val loss: 1.1891381740570068
Epoch 950, training loss: 6.358580589294434 = 0.04375828430056572 + 1.0 * 6.314822196960449
Epoch 950, val loss: 1.1983823776245117
Epoch 960, training loss: 6.356014251708984 = 0.0419594943523407 + 1.0 * 6.3140549659729
Epoch 960, val loss: 1.207483172416687
Epoch 970, training loss: 6.366526126861572 = 0.04026829078793526 + 1.0 * 6.326257705688477
Epoch 970, val loss: 1.2164429426193237
Epoch 980, training loss: 6.354542255401611 = 0.03866905719041824 + 1.0 * 6.315873146057129
Epoch 980, val loss: 1.2250275611877441
Epoch 990, training loss: 6.348568916320801 = 0.03717231750488281 + 1.0 * 6.311396598815918
Epoch 990, val loss: 1.2338467836380005
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 10.544882774353027 = 1.9480412006378174 + 1.0 * 8.596841812133789
Epoch 0, val loss: 1.9486461877822876
Epoch 10, training loss: 10.534124374389648 = 1.9375323057174683 + 1.0 * 8.59659194946289
Epoch 10, val loss: 1.9379476308822632
Epoch 20, training loss: 10.518915176391602 = 1.9243186712265015 + 1.0 * 8.594596862792969
Epoch 20, val loss: 1.9244569540023804
Epoch 30, training loss: 10.483692169189453 = 1.9060895442962646 + 1.0 * 8.57760238647461
Epoch 30, val loss: 1.905656337738037
Epoch 40, training loss: 10.342981338500977 = 1.882853627204895 + 1.0 * 8.460127830505371
Epoch 40, val loss: 1.8827933073043823
Epoch 50, training loss: 9.959306716918945 = 1.8600014448165894 + 1.0 * 8.099305152893066
Epoch 50, val loss: 1.8617026805877686
Epoch 60, training loss: 9.338410377502441 = 1.8451660871505737 + 1.0 * 7.493244647979736
Epoch 60, val loss: 1.8484857082366943
Epoch 70, training loss: 8.87186050415039 = 1.8345133066177368 + 1.0 * 7.037347316741943
Epoch 70, val loss: 1.8384486436843872
Epoch 80, training loss: 8.727020263671875 = 1.822930097579956 + 1.0 * 6.90408992767334
Epoch 80, val loss: 1.8271849155426025
Epoch 90, training loss: 8.6148042678833 = 1.8091832399368286 + 1.0 * 6.805621147155762
Epoch 90, val loss: 1.8143733739852905
Epoch 100, training loss: 8.535774230957031 = 1.795964002609253 + 1.0 * 6.739809989929199
Epoch 100, val loss: 1.802390456199646
Epoch 110, training loss: 8.472455024719238 = 1.783759355545044 + 1.0 * 6.688695907592773
Epoch 110, val loss: 1.7912381887435913
Epoch 120, training loss: 8.422872543334961 = 1.7717658281326294 + 1.0 * 6.651106834411621
Epoch 120, val loss: 1.7802215814590454
Epoch 130, training loss: 8.37942123413086 = 1.7592709064483643 + 1.0 * 6.620150089263916
Epoch 130, val loss: 1.768885612487793
Epoch 140, training loss: 8.338712692260742 = 1.7456332445144653 + 1.0 * 6.593079090118408
Epoch 140, val loss: 1.7569515705108643
Epoch 150, training loss: 8.300166130065918 = 1.7304065227508545 + 1.0 * 6.569759845733643
Epoch 150, val loss: 1.743957281112671
Epoch 160, training loss: 8.26468276977539 = 1.7132129669189453 + 1.0 * 6.551469326019287
Epoch 160, val loss: 1.7294806241989136
Epoch 170, training loss: 8.228452682495117 = 1.6937521696090698 + 1.0 * 6.534700870513916
Epoch 170, val loss: 1.7133132219314575
Epoch 180, training loss: 8.190412521362305 = 1.6714578866958618 + 1.0 * 6.518954277038574
Epoch 180, val loss: 1.6949232816696167
Epoch 190, training loss: 8.151710510253906 = 1.6457936763763428 + 1.0 * 6.505917072296143
Epoch 190, val loss: 1.6737563610076904
Epoch 200, training loss: 8.11722469329834 = 1.6161657571792603 + 1.0 * 6.501058578491211
Epoch 200, val loss: 1.6493691205978394
Epoch 210, training loss: 8.07258415222168 = 1.5830204486846924 + 1.0 * 6.489563465118408
Epoch 210, val loss: 1.622054100036621
Epoch 220, training loss: 8.0245943069458 = 1.5462604761123657 + 1.0 * 6.478333950042725
Epoch 220, val loss: 1.5917071104049683
Epoch 230, training loss: 7.974684238433838 = 1.5053893327713013 + 1.0 * 6.469295024871826
Epoch 230, val loss: 1.5580382347106934
Epoch 240, training loss: 7.922431945800781 = 1.4604400396347046 + 1.0 * 6.461991786956787
Epoch 240, val loss: 1.5209671258926392
Epoch 250, training loss: 7.87034273147583 = 1.411645531654358 + 1.0 * 6.458697319030762
Epoch 250, val loss: 1.4808926582336426
Epoch 260, training loss: 7.811568737030029 = 1.360045075416565 + 1.0 * 6.451523780822754
Epoch 260, val loss: 1.4388753175735474
Epoch 270, training loss: 7.751893997192383 = 1.3062098026275635 + 1.0 * 6.44568395614624
Epoch 270, val loss: 1.3952215909957886
Epoch 280, training loss: 7.696618556976318 = 1.2504535913467407 + 1.0 * 6.446165084838867
Epoch 280, val loss: 1.3504879474639893
Epoch 290, training loss: 7.631501197814941 = 1.1944596767425537 + 1.0 * 6.437041282653809
Epoch 290, val loss: 1.3059656620025635
Epoch 300, training loss: 7.5710625648498535 = 1.1386055946350098 + 1.0 * 6.432456970214844
Epoch 300, val loss: 1.2620506286621094
Epoch 310, training loss: 7.516892910003662 = 1.0831894874572754 + 1.0 * 6.433703422546387
Epoch 310, val loss: 1.218725562095642
Epoch 320, training loss: 7.455047130584717 = 1.0291190147399902 + 1.0 * 6.425928115844727
Epoch 320, val loss: 1.1770488023757935
Epoch 330, training loss: 7.397644519805908 = 0.9771004319190979 + 1.0 * 6.420544147491455
Epoch 330, val loss: 1.1370277404785156
Epoch 340, training loss: 7.343904495239258 = 0.9270641207695007 + 1.0 * 6.416840553283691
Epoch 340, val loss: 1.098844289779663
Epoch 350, training loss: 7.294928073883057 = 0.8795876502990723 + 1.0 * 6.415340423583984
Epoch 350, val loss: 1.0629338026046753
Epoch 360, training loss: 7.246794700622559 = 0.8351425528526306 + 1.0 * 6.411652088165283
Epoch 360, val loss: 1.0297513008117676
Epoch 370, training loss: 7.20460844039917 = 0.7935697436332703 + 1.0 * 6.411038875579834
Epoch 370, val loss: 0.9991382956504822
Epoch 380, training loss: 7.162006378173828 = 0.7547857761383057 + 1.0 * 6.407220840454102
Epoch 380, val loss: 0.9714491367340088
Epoch 390, training loss: 7.118616104125977 = 0.7188327312469482 + 1.0 * 6.399783134460449
Epoch 390, val loss: 0.9463863372802734
Epoch 400, training loss: 7.082777500152588 = 0.6849956512451172 + 1.0 * 6.397781848907471
Epoch 400, val loss: 0.9236482977867126
Epoch 410, training loss: 7.050642967224121 = 0.6530890464782715 + 1.0 * 6.39755392074585
Epoch 410, val loss: 0.902977705001831
Epoch 420, training loss: 7.013291835784912 = 0.6229805946350098 + 1.0 * 6.390311241149902
Epoch 420, val loss: 0.8843976855278015
Epoch 430, training loss: 6.982454299926758 = 0.5942214727401733 + 1.0 * 6.388232707977295
Epoch 430, val loss: 0.8675624132156372
Epoch 440, training loss: 6.951603889465332 = 0.5667403936386108 + 1.0 * 6.384863376617432
Epoch 440, val loss: 0.8523610830307007
Epoch 450, training loss: 6.923593044281006 = 0.5405303239822388 + 1.0 * 6.383062839508057
Epoch 450, val loss: 0.838630735874176
Epoch 460, training loss: 6.896092414855957 = 0.5152908563613892 + 1.0 * 6.380801677703857
Epoch 460, val loss: 0.8262848258018494
Epoch 470, training loss: 6.873007774353027 = 0.49107956886291504 + 1.0 * 6.381927967071533
Epoch 470, val loss: 0.8152627944946289
Epoch 480, training loss: 6.842679023742676 = 0.4679373502731323 + 1.0 * 6.374741554260254
Epoch 480, val loss: 0.8054273724555969
Epoch 490, training loss: 6.823617935180664 = 0.44565269351005554 + 1.0 * 6.377965450286865
Epoch 490, val loss: 0.7967584729194641
Epoch 500, training loss: 6.798646926879883 = 0.42436590790748596 + 1.0 * 6.37428092956543
Epoch 500, val loss: 0.7892403602600098
Epoch 510, training loss: 6.773309707641602 = 0.40395301580429077 + 1.0 * 6.369356632232666
Epoch 510, val loss: 0.782752513885498
Epoch 520, training loss: 6.7485504150390625 = 0.38440775871276855 + 1.0 * 6.364142417907715
Epoch 520, val loss: 0.7772876024246216
Epoch 530, training loss: 6.738830089569092 = 0.3656761944293976 + 1.0 * 6.3731536865234375
Epoch 530, val loss: 0.7727925777435303
Epoch 540, training loss: 6.7149152755737305 = 0.34775403141975403 + 1.0 * 6.367161273956299
Epoch 540, val loss: 0.7692053318023682
Epoch 550, training loss: 6.689891815185547 = 0.3307478129863739 + 1.0 * 6.35914421081543
Epoch 550, val loss: 0.7663533091545105
Epoch 560, training loss: 6.672694206237793 = 0.3143942356109619 + 1.0 * 6.358299732208252
Epoch 560, val loss: 0.7642208933830261
Epoch 570, training loss: 6.656752586364746 = 0.2987324595451355 + 1.0 * 6.358020305633545
Epoch 570, val loss: 0.7628660798072815
Epoch 580, training loss: 6.640713691711426 = 0.2837529480457306 + 1.0 * 6.356960773468018
Epoch 580, val loss: 0.7620306015014648
Epoch 590, training loss: 6.62129545211792 = 0.2693060636520386 + 1.0 * 6.351989269256592
Epoch 590, val loss: 0.7617068886756897
Epoch 600, training loss: 6.608702659606934 = 0.2552838921546936 + 1.0 * 6.353418827056885
Epoch 600, val loss: 0.7618281841278076
Epoch 610, training loss: 6.596304893493652 = 0.24178332090377808 + 1.0 * 6.354521751403809
Epoch 610, val loss: 0.7624916434288025
Epoch 620, training loss: 6.575273036956787 = 0.22866113483905792 + 1.0 * 6.346611976623535
Epoch 620, val loss: 0.7632396221160889
Epoch 630, training loss: 6.561949729919434 = 0.21589702367782593 + 1.0 * 6.346052646636963
Epoch 630, val loss: 0.7643442749977112
Epoch 640, training loss: 6.55812406539917 = 0.20348498225212097 + 1.0 * 6.354639053344727
Epoch 640, val loss: 0.7658654451370239
Epoch 650, training loss: 6.539636135101318 = 0.19150960445404053 + 1.0 * 6.348126411437988
Epoch 650, val loss: 0.7676681280136108
Epoch 660, training loss: 6.5209221839904785 = 0.1799791306257248 + 1.0 * 6.340942859649658
Epoch 660, val loss: 0.7697206139564514
Epoch 670, training loss: 6.508944034576416 = 0.1689329296350479 + 1.0 * 6.340011119842529
Epoch 670, val loss: 0.7721211910247803
Epoch 680, training loss: 6.503490447998047 = 0.15840353071689606 + 1.0 * 6.345087051391602
Epoch 680, val loss: 0.7749421000480652
Epoch 690, training loss: 6.497859954833984 = 0.1485287994146347 + 1.0 * 6.349331378936768
Epoch 690, val loss: 0.7781147360801697
Epoch 700, training loss: 6.476407051086426 = 0.13921427726745605 + 1.0 * 6.337192535400391
Epoch 700, val loss: 0.7814610004425049
Epoch 710, training loss: 6.465130805969238 = 0.13055211305618286 + 1.0 * 6.334578514099121
Epoch 710, val loss: 0.7851479649543762
Epoch 720, training loss: 6.457124710083008 = 0.12246351689100266 + 1.0 * 6.33466100692749
Epoch 720, val loss: 0.7892243266105652
Epoch 730, training loss: 6.455899238586426 = 0.11494268476963043 + 1.0 * 6.340956687927246
Epoch 730, val loss: 0.793648898601532
Epoch 740, training loss: 6.441962242126465 = 0.10801248997449875 + 1.0 * 6.333949565887451
Epoch 740, val loss: 0.7981888055801392
Epoch 750, training loss: 6.432640552520752 = 0.1016322523355484 + 1.0 * 6.331008434295654
Epoch 750, val loss: 0.8028818368911743
Epoch 760, training loss: 6.425497055053711 = 0.09573434293270111 + 1.0 * 6.329762935638428
Epoch 760, val loss: 0.8078990578651428
Epoch 770, training loss: 6.424386501312256 = 0.09028946608304977 + 1.0 * 6.334096908569336
Epoch 770, val loss: 0.8131444454193115
Epoch 780, training loss: 6.413711071014404 = 0.08528981357812881 + 1.0 * 6.328421115875244
Epoch 780, val loss: 0.8185907006263733
Epoch 790, training loss: 6.405887126922607 = 0.08069048076868057 + 1.0 * 6.325196743011475
Epoch 790, val loss: 0.8238823413848877
Epoch 800, training loss: 6.399994850158691 = 0.07643697410821915 + 1.0 * 6.3235578536987305
Epoch 800, val loss: 0.8294336199760437
Epoch 810, training loss: 6.394457817077637 = 0.07249152660369873 + 1.0 * 6.321966171264648
Epoch 810, val loss: 0.8351088762283325
Epoch 820, training loss: 6.399910926818848 = 0.0688217282295227 + 1.0 * 6.331089019775391
Epoch 820, val loss: 0.8407983779907227
Epoch 830, training loss: 6.391488075256348 = 0.0654539167881012 + 1.0 * 6.326034069061279
Epoch 830, val loss: 0.8466253280639648
Epoch 840, training loss: 6.386011123657227 = 0.062332697212696075 + 1.0 * 6.323678493499756
Epoch 840, val loss: 0.8522287011146545
Epoch 850, training loss: 6.3798017501831055 = 0.05943155288696289 + 1.0 * 6.320370197296143
Epoch 850, val loss: 0.8580273985862732
Epoch 860, training loss: 6.375000953674316 = 0.05673275887966156 + 1.0 * 6.318268299102783
Epoch 860, val loss: 0.8636568784713745
Epoch 870, training loss: 6.371675491333008 = 0.05421660467982292 + 1.0 * 6.3174591064453125
Epoch 870, val loss: 0.869363009929657
Epoch 880, training loss: 6.372401237487793 = 0.05185817927122116 + 1.0 * 6.32054328918457
Epoch 880, val loss: 0.8751444220542908
Epoch 890, training loss: 6.366562366485596 = 0.0496600978076458 + 1.0 * 6.316902160644531
Epoch 890, val loss: 0.8808009028434753
Epoch 900, training loss: 6.367025375366211 = 0.04760761559009552 + 1.0 * 6.319417953491211
Epoch 900, val loss: 0.8863853812217712
Epoch 910, training loss: 6.359617710113525 = 0.04567375034093857 + 1.0 * 6.313943862915039
Epoch 910, val loss: 0.892090380191803
Epoch 920, training loss: 6.355801105499268 = 0.043873656541109085 + 1.0 * 6.311927318572998
Epoch 920, val loss: 0.8975000977516174
Epoch 930, training loss: 6.352588176727295 = 0.042166490107774734 + 1.0 * 6.310421466827393
Epoch 930, val loss: 0.9029998183250427
Epoch 940, training loss: 6.352952003479004 = 0.04055025428533554 + 1.0 * 6.31240177154541
Epoch 940, val loss: 0.9085119962692261
Epoch 950, training loss: 6.3529181480407715 = 0.03903916850686073 + 1.0 * 6.313879013061523
Epoch 950, val loss: 0.9141345024108887
Epoch 960, training loss: 6.34840202331543 = 0.03761454299092293 + 1.0 * 6.310787677764893
Epoch 960, val loss: 0.9193652272224426
Epoch 970, training loss: 6.34480094909668 = 0.036276139318943024 + 1.0 * 6.3085246086120605
Epoch 970, val loss: 0.924598753452301
Epoch 980, training loss: 6.341843128204346 = 0.034997183829545975 + 1.0 * 6.3068461418151855
Epoch 980, val loss: 0.9299355149269104
Epoch 990, training loss: 6.3562798500061035 = 0.03378286212682724 + 1.0 * 6.3224968910217285
Epoch 990, val loss: 0.9352760314941406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8017923036373221
The final CL Acc:0.75432, 0.00972, The final GNN Acc:0.80636, 0.00538
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13218])
remove edge: torch.Size([2, 7832])
updated graph: torch.Size([2, 10494])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.52707576751709 = 1.9302561283111572 + 1.0 * 8.596819877624512
Epoch 0, val loss: 1.9257451295852661
Epoch 10, training loss: 10.516885757446289 = 1.9203858375549316 + 1.0 * 8.5964994430542
Epoch 10, val loss: 1.915435552597046
Epoch 20, training loss: 10.50184440612793 = 1.907646894454956 + 1.0 * 8.594197273254395
Epoch 20, val loss: 1.9020476341247559
Epoch 30, training loss: 10.465194702148438 = 1.889275074005127 + 1.0 * 8.575919151306152
Epoch 30, val loss: 1.882741928100586
Epoch 40, training loss: 10.31478214263916 = 1.8650985956192017 + 1.0 * 8.44968318939209
Epoch 40, val loss: 1.8583464622497559
Epoch 50, training loss: 9.874541282653809 = 1.8388025760650635 + 1.0 * 8.035738945007324
Epoch 50, val loss: 1.8325799703598022
Epoch 60, training loss: 9.381499290466309 = 1.8174399137496948 + 1.0 * 7.564059734344482
Epoch 60, val loss: 1.8133680820465088
Epoch 70, training loss: 8.955472946166992 = 1.8057239055633545 + 1.0 * 7.149748802185059
Epoch 70, val loss: 1.8037663698196411
Epoch 80, training loss: 8.746726989746094 = 1.7935577630996704 + 1.0 * 6.953169345855713
Epoch 80, val loss: 1.7930208444595337
Epoch 90, training loss: 8.642354965209961 = 1.7774720191955566 + 1.0 * 6.864882469177246
Epoch 90, val loss: 1.779335618019104
Epoch 100, training loss: 8.553361892700195 = 1.7605657577514648 + 1.0 * 6.7927961349487305
Epoch 100, val loss: 1.7654579877853394
Epoch 110, training loss: 8.477120399475098 = 1.744134783744812 + 1.0 * 6.732985973358154
Epoch 110, val loss: 1.7515997886657715
Epoch 120, training loss: 8.40895938873291 = 1.7263067960739136 + 1.0 * 6.682652950286865
Epoch 120, val loss: 1.7363831996917725
Epoch 130, training loss: 8.351195335388184 = 1.7049809694290161 + 1.0 * 6.646214485168457
Epoch 130, val loss: 1.7185231447219849
Epoch 140, training loss: 8.296210289001465 = 1.679394245147705 + 1.0 * 6.61681604385376
Epoch 140, val loss: 1.697547197341919
Epoch 150, training loss: 8.241957664489746 = 1.6491655111312866 + 1.0 * 6.59279203414917
Epoch 150, val loss: 1.672942042350769
Epoch 160, training loss: 8.185407638549805 = 1.6144335269927979 + 1.0 * 6.570974349975586
Epoch 160, val loss: 1.644720196723938
Epoch 170, training loss: 8.126481056213379 = 1.5750023126602173 + 1.0 * 6.551478385925293
Epoch 170, val loss: 1.612610936164856
Epoch 180, training loss: 8.066123008728027 = 1.5318632125854492 + 1.0 * 6.534259796142578
Epoch 180, val loss: 1.5779166221618652
Epoch 190, training loss: 8.0055570602417 = 1.4860795736312866 + 1.0 * 6.519477844238281
Epoch 190, val loss: 1.5416263341903687
Epoch 200, training loss: 7.943784713745117 = 1.438230037689209 + 1.0 * 6.505554676055908
Epoch 200, val loss: 1.5043854713439941
Epoch 210, training loss: 7.884338855743408 = 1.3897600173950195 + 1.0 * 6.494578838348389
Epoch 210, val loss: 1.4675825834274292
Epoch 220, training loss: 7.827234268188477 = 1.342448115348816 + 1.0 * 6.484786033630371
Epoch 220, val loss: 1.4325803518295288
Epoch 230, training loss: 7.770217418670654 = 1.2964433431625366 + 1.0 * 6.473773956298828
Epoch 230, val loss: 1.3994652032852173
Epoch 240, training loss: 7.729137420654297 = 1.252096176147461 + 1.0 * 6.477041244506836
Epoch 240, val loss: 1.3682398796081543
Epoch 250, training loss: 7.6700263023376465 = 1.210166573524475 + 1.0 * 6.459859848022461
Epoch 250, val loss: 1.339231252670288
Epoch 260, training loss: 7.620481014251709 = 1.169791579246521 + 1.0 * 6.450689315795898
Epoch 260, val loss: 1.3115580081939697
Epoch 270, training loss: 7.572744846343994 = 1.1301153898239136 + 1.0 * 6.442629337310791
Epoch 270, val loss: 1.2843891382217407
Epoch 280, training loss: 7.527431488037109 = 1.091062307357788 + 1.0 * 6.436368942260742
Epoch 280, val loss: 1.257567286491394
Epoch 290, training loss: 7.485738754272461 = 1.0528937578201294 + 1.0 * 6.432845115661621
Epoch 290, val loss: 1.2313498258590698
Epoch 300, training loss: 7.440684795379639 = 1.0150033235549927 + 1.0 * 6.4256815910339355
Epoch 300, val loss: 1.2050528526306152
Epoch 310, training loss: 7.396161079406738 = 0.9768991470336914 + 1.0 * 6.419261932373047
Epoch 310, val loss: 1.1784427165985107
Epoch 320, training loss: 7.35296630859375 = 0.9384307861328125 + 1.0 * 6.4145355224609375
Epoch 320, val loss: 1.1513898372650146
Epoch 330, training loss: 7.317534446716309 = 0.9001592397689819 + 1.0 * 6.417375087738037
Epoch 330, val loss: 1.1242692470550537
Epoch 340, training loss: 7.268587112426758 = 0.8621816039085388 + 1.0 * 6.406405448913574
Epoch 340, val loss: 1.0973707437515259
Epoch 350, training loss: 7.229477882385254 = 0.8240503668785095 + 1.0 * 6.4054274559021
Epoch 350, val loss: 1.070249319076538
Epoch 360, training loss: 7.185463905334473 = 0.7859303951263428 + 1.0 * 6.399533748626709
Epoch 360, val loss: 1.0431475639343262
Epoch 370, training loss: 7.156782627105713 = 0.7478430867195129 + 1.0 * 6.408939361572266
Epoch 370, val loss: 1.0162243843078613
Epoch 380, training loss: 7.105153560638428 = 0.7107234001159668 + 1.0 * 6.394430160522461
Epoch 380, val loss: 0.989921510219574
Epoch 390, training loss: 7.063392639160156 = 0.6742550134658813 + 1.0 * 6.3891377449035645
Epoch 390, val loss: 0.9644206762313843
Epoch 400, training loss: 7.023908615112305 = 0.6385753154754639 + 1.0 * 6.385333061218262
Epoch 400, val loss: 0.9396293759346008
Epoch 410, training loss: 6.986447811126709 = 0.603833019733429 + 1.0 * 6.382614612579346
Epoch 410, val loss: 0.9158253073692322
Epoch 420, training loss: 6.9583001136779785 = 0.5702543258666992 + 1.0 * 6.388045787811279
Epoch 420, val loss: 0.8931599259376526
Epoch 430, training loss: 6.916572093963623 = 0.5383254289627075 + 1.0 * 6.378246784210205
Epoch 430, val loss: 0.8719400763511658
Epoch 440, training loss: 6.891648769378662 = 0.5079032778739929 + 1.0 * 6.3837456703186035
Epoch 440, val loss: 0.852388858795166
Epoch 450, training loss: 6.855053901672363 = 0.47908151149749756 + 1.0 * 6.375972270965576
Epoch 450, val loss: 0.8343526124954224
Epoch 460, training loss: 6.82208776473999 = 0.45135805010795593 + 1.0 * 6.370729923248291
Epoch 460, val loss: 0.8178845643997192
Epoch 470, training loss: 6.794040679931641 = 0.4247027337551117 + 1.0 * 6.369338035583496
Epoch 470, val loss: 0.8027974963188171
Epoch 480, training loss: 6.778721809387207 = 0.39910635352134705 + 1.0 * 6.379615306854248
Epoch 480, val loss: 0.788993775844574
Epoch 490, training loss: 6.7422590255737305 = 0.37489569187164307 + 1.0 * 6.367363452911377
Epoch 490, val loss: 0.7768080830574036
Epoch 500, training loss: 6.713500499725342 = 0.3517543077468872 + 1.0 * 6.361746311187744
Epoch 500, val loss: 0.7661015391349792
Epoch 510, training loss: 6.6892313957214355 = 0.3296315371990204 + 1.0 * 6.359600067138672
Epoch 510, val loss: 0.7565619945526123
Epoch 520, training loss: 6.668182849884033 = 0.3085222840309143 + 1.0 * 6.359660625457764
Epoch 520, val loss: 0.7482350468635559
Epoch 530, training loss: 6.655073642730713 = 0.2886889576911926 + 1.0 * 6.366384506225586
Epoch 530, val loss: 0.7409716844558716
Epoch 540, training loss: 6.628327369689941 = 0.27012279629707336 + 1.0 * 6.358204364776611
Epoch 540, val loss: 0.7350455522537231
Epoch 550, training loss: 6.606174945831299 = 0.25268909335136414 + 1.0 * 6.353486061096191
Epoch 550, val loss: 0.7301352620124817
Epoch 560, training loss: 6.588883876800537 = 0.23628801107406616 + 1.0 * 6.352595806121826
Epoch 560, val loss: 0.7260585427284241
Epoch 570, training loss: 6.5743231773376465 = 0.2209847867488861 + 1.0 * 6.353338241577148
Epoch 570, val loss: 0.7228025197982788
Epoch 580, training loss: 6.554498672485352 = 0.20677772164344788 + 1.0 * 6.347721099853516
Epoch 580, val loss: 0.7204991579055786
Epoch 590, training loss: 6.539856433868408 = 0.19352836906909943 + 1.0 * 6.346328258514404
Epoch 590, val loss: 0.7189247608184814
Epoch 600, training loss: 6.5482988357543945 = 0.18121123313903809 + 1.0 * 6.3670878410339355
Epoch 600, val loss: 0.7179073095321655
Epoch 610, training loss: 6.51959228515625 = 0.16987022757530212 + 1.0 * 6.349721908569336
Epoch 610, val loss: 0.7175366282463074
Epoch 620, training loss: 6.5035080909729 = 0.15938200056552887 + 1.0 * 6.344126224517822
Epoch 620, val loss: 0.7179573774337769
Epoch 630, training loss: 6.495497226715088 = 0.14964185655117035 + 1.0 * 6.345855236053467
Epoch 630, val loss: 0.7187418341636658
Epoch 640, training loss: 6.479124546051025 = 0.14063440263271332 + 1.0 * 6.338490009307861
Epoch 640, val loss: 0.7200102806091309
Epoch 650, training loss: 6.470488548278809 = 0.1322574019432068 + 1.0 * 6.338231086730957
Epoch 650, val loss: 0.7218166589736938
Epoch 660, training loss: 6.4598002433776855 = 0.12445516884326935 + 1.0 * 6.335345268249512
Epoch 660, val loss: 0.7240278720855713
Epoch 670, training loss: 6.457993030548096 = 0.11720307916402817 + 1.0 * 6.340789794921875
Epoch 670, val loss: 0.7265141010284424
Epoch 680, training loss: 6.445040702819824 = 0.1104908138513565 + 1.0 * 6.334549903869629
Epoch 680, val loss: 0.7294067740440369
Epoch 690, training loss: 6.439539909362793 = 0.10427774488925934 + 1.0 * 6.335262298583984
Epoch 690, val loss: 0.7325233221054077
Epoch 700, training loss: 6.429011344909668 = 0.09852936863899231 + 1.0 * 6.330482006072998
Epoch 700, val loss: 0.7360159158706665
Epoch 710, training loss: 6.421910285949707 = 0.09316740185022354 + 1.0 * 6.328742980957031
Epoch 710, val loss: 0.7397869229316711
Epoch 720, training loss: 6.415796279907227 = 0.08814946562051773 + 1.0 * 6.327646732330322
Epoch 720, val loss: 0.7436839938163757
Epoch 730, training loss: 6.422807216644287 = 0.08346173167228699 + 1.0 * 6.339345455169678
Epoch 730, val loss: 0.7477246522903442
Epoch 740, training loss: 6.404246807098389 = 0.07912197709083557 + 1.0 * 6.325124740600586
Epoch 740, val loss: 0.7520201206207275
Epoch 750, training loss: 6.400203704833984 = 0.07506769150495529 + 1.0 * 6.325136184692383
Epoch 750, val loss: 0.7565892934799194
Epoch 760, training loss: 6.396996974945068 = 0.07126973569393158 + 1.0 * 6.325727462768555
Epoch 760, val loss: 0.7611849308013916
Epoch 770, training loss: 6.3916754722595215 = 0.06772225350141525 + 1.0 * 6.323953151702881
Epoch 770, val loss: 0.7657677531242371
Epoch 780, training loss: 6.387084007263184 = 0.06441712379455566 + 1.0 * 6.322666645050049
Epoch 780, val loss: 0.7707377076148987
Epoch 790, training loss: 6.383607387542725 = 0.061323776841163635 + 1.0 * 6.322283744812012
Epoch 790, val loss: 0.7757120132446289
Epoch 800, training loss: 6.378504276275635 = 0.05842115730047226 + 1.0 * 6.320083141326904
Epoch 800, val loss: 0.7805869579315186
Epoch 810, training loss: 6.374778747558594 = 0.05569816753268242 + 1.0 * 6.319080352783203
Epoch 810, val loss: 0.7856401801109314
Epoch 820, training loss: 6.374056339263916 = 0.053142648190259933 + 1.0 * 6.320913791656494
Epoch 820, val loss: 0.7906724810600281
Epoch 830, training loss: 6.366781234741211 = 0.05074885115027428 + 1.0 * 6.316032409667969
Epoch 830, val loss: 0.7957300543785095
Epoch 840, training loss: 6.363131999969482 = 0.048504527658224106 + 1.0 * 6.314627647399902
Epoch 840, val loss: 0.800848662853241
Epoch 850, training loss: 6.366347789764404 = 0.04638737440109253 + 1.0 * 6.319960594177246
Epoch 850, val loss: 0.805959939956665
Epoch 860, training loss: 6.3595991134643555 = 0.04440043494105339 + 1.0 * 6.31519889831543
Epoch 860, val loss: 0.8110013604164124
Epoch 870, training loss: 6.357724666595459 = 0.04252737760543823 + 1.0 * 6.315197467803955
Epoch 870, val loss: 0.8162217736244202
Epoch 880, training loss: 6.351954936981201 = 0.040771711617708206 + 1.0 * 6.311183452606201
Epoch 880, val loss: 0.8212636709213257
Epoch 890, training loss: 6.348916053771973 = 0.0391191802918911 + 1.0 * 6.3097968101501465
Epoch 890, val loss: 0.8263677954673767
Epoch 900, training loss: 6.352025985717773 = 0.03755868226289749 + 1.0 * 6.314467430114746
Epoch 900, val loss: 0.8314275145530701
Epoch 910, training loss: 6.345736980438232 = 0.03608643636107445 + 1.0 * 6.309650421142578
Epoch 910, val loss: 0.8364554643630981
Epoch 920, training loss: 6.341053009033203 = 0.034696437418460846 + 1.0 * 6.306356430053711
Epoch 920, val loss: 0.841548502445221
Epoch 930, training loss: 6.3514509201049805 = 0.03338444232940674 + 1.0 * 6.318066596984863
Epoch 930, val loss: 0.8464871644973755
Epoch 940, training loss: 6.34233283996582 = 0.032145582139492035 + 1.0 * 6.310187339782715
Epoch 940, val loss: 0.8512644171714783
Epoch 950, training loss: 6.3358564376831055 = 0.030977103859186172 + 1.0 * 6.304879188537598
Epoch 950, val loss: 0.8563445210456848
Epoch 960, training loss: 6.335269927978516 = 0.029869886115193367 + 1.0 * 6.3053998947143555
Epoch 960, val loss: 0.8611730337142944
Epoch 970, training loss: 6.33574104309082 = 0.02881455048918724 + 1.0 * 6.306926727294922
Epoch 970, val loss: 0.865907609462738
Epoch 980, training loss: 6.3301873207092285 = 0.027816617861390114 + 1.0 * 6.302370548248291
Epoch 980, val loss: 0.8707093596458435
Epoch 990, training loss: 6.332275390625 = 0.02687007375061512 + 1.0 * 6.305405139923096
Epoch 990, val loss: 0.87545245885849
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 10.54903793334961 = 1.9522042274475098 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9443544149398804
Epoch 10, training loss: 10.538514137268066 = 1.9419087171554565 + 1.0 * 8.59660530090332
Epoch 10, val loss: 1.9342414140701294
Epoch 20, training loss: 10.524462699890137 = 1.9296215772628784 + 1.0 * 8.594841003417969
Epoch 20, val loss: 1.9222357273101807
Epoch 30, training loss: 10.491976737976074 = 1.9126996994018555 + 1.0 * 8.579277038574219
Epoch 30, val loss: 1.905915379524231
Epoch 40, training loss: 10.363242149353027 = 1.8899881839752197 + 1.0 * 8.473254203796387
Epoch 40, val loss: 1.8846882581710815
Epoch 50, training loss: 9.84721851348877 = 1.8662837743759155 + 1.0 * 7.9809346199035645
Epoch 50, val loss: 1.8625398874282837
Epoch 60, training loss: 9.329121589660645 = 1.8470427989959717 + 1.0 * 7.482078552246094
Epoch 60, val loss: 1.844922423362732
Epoch 70, training loss: 9.007003784179688 = 1.8311914205551147 + 1.0 * 7.175812244415283
Epoch 70, val loss: 1.8296098709106445
Epoch 80, training loss: 8.81082534790039 = 1.8136721849441528 + 1.0 * 6.997153282165527
Epoch 80, val loss: 1.8135472536087036
Epoch 90, training loss: 8.673832893371582 = 1.7942579984664917 + 1.0 * 6.879574775695801
Epoch 90, val loss: 1.7968333959579468
Epoch 100, training loss: 8.564390182495117 = 1.7755842208862305 + 1.0 * 6.7888054847717285
Epoch 100, val loss: 1.7811717987060547
Epoch 110, training loss: 8.479880332946777 = 1.7579113245010376 + 1.0 * 6.721968650817871
Epoch 110, val loss: 1.7660977840423584
Epoch 120, training loss: 8.410784721374512 = 1.7399648427963257 + 1.0 * 6.6708197593688965
Epoch 120, val loss: 1.7501201629638672
Epoch 130, training loss: 8.355483055114746 = 1.7202261686325073 + 1.0 * 6.635257244110107
Epoch 130, val loss: 1.7325401306152344
Epoch 140, training loss: 8.304749488830566 = 1.6978310346603394 + 1.0 * 6.6069183349609375
Epoch 140, val loss: 1.712639570236206
Epoch 150, training loss: 8.256621360778809 = 1.6720272302627563 + 1.0 * 6.584594249725342
Epoch 150, val loss: 1.689902424812317
Epoch 160, training loss: 8.212430953979492 = 1.6421128511428833 + 1.0 * 6.570318222045898
Epoch 160, val loss: 1.6636450290679932
Epoch 170, training loss: 8.158158302307129 = 1.6082713603973389 + 1.0 * 6.549886703491211
Epoch 170, val loss: 1.6340879201889038
Epoch 180, training loss: 8.105813980102539 = 1.5699971914291382 + 1.0 * 6.535816669464111
Epoch 180, val loss: 1.6006702184677124
Epoch 190, training loss: 8.050987243652344 = 1.5270559787750244 + 1.0 * 6.523931503295898
Epoch 190, val loss: 1.5630862712860107
Epoch 200, training loss: 7.9936723709106445 = 1.4804635047912598 + 1.0 * 6.513208866119385
Epoch 200, val loss: 1.522596001625061
Epoch 210, training loss: 7.932032585144043 = 1.4312430620193481 + 1.0 * 6.500789642333984
Epoch 210, val loss: 1.4800829887390137
Epoch 220, training loss: 7.8727827072143555 = 1.3799617290496826 + 1.0 * 6.492820739746094
Epoch 220, val loss: 1.436324954032898
Epoch 230, training loss: 7.8105854988098145 = 1.3280194997787476 + 1.0 * 6.482565879821777
Epoch 230, val loss: 1.3927080631256104
Epoch 240, training loss: 7.75018310546875 = 1.2761282920837402 + 1.0 * 6.47405481338501
Epoch 240, val loss: 1.3497978448867798
Epoch 250, training loss: 7.692090034484863 = 1.2252269983291626 + 1.0 * 6.46686315536499
Epoch 250, val loss: 1.3085474967956543
Epoch 260, training loss: 7.637331008911133 = 1.176363229751587 + 1.0 * 6.460967540740967
Epoch 260, val loss: 1.2695083618164062
Epoch 270, training loss: 7.581583023071289 = 1.129332184791565 + 1.0 * 6.452250957489014
Epoch 270, val loss: 1.2323590517044067
Epoch 280, training loss: 7.538449764251709 = 1.0842279195785522 + 1.0 * 6.454221725463867
Epoch 280, val loss: 1.1971529722213745
Epoch 290, training loss: 7.483805179595947 = 1.0415641069412231 + 1.0 * 6.442241191864014
Epoch 290, val loss: 1.1642383337020874
Epoch 300, training loss: 7.435309410095215 = 1.0009725093841553 + 1.0 * 6.4343366622924805
Epoch 300, val loss: 1.133007526397705
Epoch 310, training loss: 7.391400337219238 = 0.9621412754058838 + 1.0 * 6.429258823394775
Epoch 310, val loss: 1.1030741930007935
Epoch 320, training loss: 7.352293968200684 = 0.9250390529632568 + 1.0 * 6.427255153656006
Epoch 320, val loss: 1.0745179653167725
Epoch 330, training loss: 7.310812473297119 = 0.8896889686584473 + 1.0 * 6.421123504638672
Epoch 330, val loss: 1.047225832939148
Epoch 340, training loss: 7.268167972564697 = 0.8556475639343262 + 1.0 * 6.412520408630371
Epoch 340, val loss: 1.020605206489563
Epoch 350, training loss: 7.230906009674072 = 0.8222863078117371 + 1.0 * 6.4086198806762695
Epoch 350, val loss: 0.9942582845687866
Epoch 360, training loss: 7.1951375007629395 = 0.7895514369010925 + 1.0 * 6.405586242675781
Epoch 360, val loss: 0.9682713150978088
Epoch 370, training loss: 7.159493446350098 = 0.757416844367981 + 1.0 * 6.402076721191406
Epoch 370, val loss: 0.9426754713058472
Epoch 380, training loss: 7.124544620513916 = 0.7257621884346008 + 1.0 * 6.398782253265381
Epoch 380, val loss: 0.9174227118492126
Epoch 390, training loss: 7.090548515319824 = 0.6946052312850952 + 1.0 * 6.3959431648254395
Epoch 390, val loss: 0.892833411693573
Epoch 400, training loss: 7.05734920501709 = 0.6641019582748413 + 1.0 * 6.393247127532959
Epoch 400, val loss: 0.8689199090003967
Epoch 410, training loss: 7.02217435836792 = 0.6341860890388489 + 1.0 * 6.387988090515137
Epoch 410, val loss: 0.8459545969963074
Epoch 420, training loss: 6.991306304931641 = 0.6049096584320068 + 1.0 * 6.386396884918213
Epoch 420, val loss: 0.8240228891372681
Epoch 430, training loss: 6.960301876068115 = 0.5763052701950073 + 1.0 * 6.383996486663818
Epoch 430, val loss: 0.8033987879753113
Epoch 440, training loss: 6.927250862121582 = 0.5485637187957764 + 1.0 * 6.378686904907227
Epoch 440, val loss: 0.7841457724571228
Epoch 450, training loss: 6.898214817047119 = 0.5216787457466125 + 1.0 * 6.376535892486572
Epoch 450, val loss: 0.7663569450378418
Epoch 460, training loss: 6.869632244110107 = 0.49571946263313293 + 1.0 * 6.373912811279297
Epoch 460, val loss: 0.7501094341278076
Epoch 470, training loss: 6.8415303230285645 = 0.47078409790992737 + 1.0 * 6.37074613571167
Epoch 470, val loss: 0.735196053981781
Epoch 480, training loss: 6.815186977386475 = 0.446686327457428 + 1.0 * 6.368500709533691
Epoch 480, val loss: 0.7215341925621033
Epoch 490, training loss: 6.789675235748291 = 0.42352163791656494 + 1.0 * 6.366153717041016
Epoch 490, val loss: 0.708987295627594
Epoch 500, training loss: 6.767031192779541 = 0.4013324975967407 + 1.0 * 6.36569881439209
Epoch 500, val loss: 0.6975618004798889
Epoch 510, training loss: 6.746855735778809 = 0.3800174295902252 + 1.0 * 6.366838455200195
Epoch 510, val loss: 0.6870577335357666
Epoch 520, training loss: 6.720284938812256 = 0.35951435565948486 + 1.0 * 6.3607707023620605
Epoch 520, val loss: 0.6773601174354553
Epoch 530, training loss: 6.6970534324646 = 0.33971258997917175 + 1.0 * 6.3573408126831055
Epoch 530, val loss: 0.6683847904205322
Epoch 540, training loss: 6.6821417808532715 = 0.32058727741241455 + 1.0 * 6.3615546226501465
Epoch 540, val loss: 0.6600861549377441
Epoch 550, training loss: 6.661402225494385 = 0.30218306183815 + 1.0 * 6.359219074249268
Epoch 550, val loss: 0.6524586081504822
Epoch 560, training loss: 6.637462139129639 = 0.2844108045101166 + 1.0 * 6.35305118560791
Epoch 560, val loss: 0.6454680562019348
Epoch 570, training loss: 6.625259876251221 = 0.26721474528312683 + 1.0 * 6.3580451011657715
Epoch 570, val loss: 0.6390323042869568
Epoch 580, training loss: 6.601648807525635 = 0.25074437260627747 + 1.0 * 6.35090446472168
Epoch 580, val loss: 0.6332365870475769
Epoch 590, training loss: 6.581918239593506 = 0.23493444919586182 + 1.0 * 6.346983909606934
Epoch 590, val loss: 0.6281571984291077
Epoch 600, training loss: 6.567696571350098 = 0.21984334290027618 + 1.0 * 6.347853183746338
Epoch 600, val loss: 0.6237789988517761
Epoch 610, training loss: 6.553831577301025 = 0.20562733709812164 + 1.0 * 6.348204135894775
Epoch 610, val loss: 0.6201217174530029
Epoch 620, training loss: 6.5366411209106445 = 0.19230832159519196 + 1.0 * 6.344332695007324
Epoch 620, val loss: 0.6173617839813232
Epoch 630, training loss: 6.520568370819092 = 0.17989125847816467 + 1.0 * 6.340677261352539
Epoch 630, val loss: 0.6153475046157837
Epoch 640, training loss: 6.518725395202637 = 0.1683414727449417 + 1.0 * 6.350383758544922
Epoch 640, val loss: 0.6141065955162048
Epoch 650, training loss: 6.500805854797363 = 0.15770398080348969 + 1.0 * 6.343101978302002
Epoch 650, val loss: 0.6136330962181091
Epoch 660, training loss: 6.485590934753418 = 0.14790239930152893 + 1.0 * 6.337688446044922
Epoch 660, val loss: 0.6139114499092102
Epoch 670, training loss: 6.481573581695557 = 0.13887004554271698 + 1.0 * 6.342703342437744
Epoch 670, val loss: 0.6147760152816772
Epoch 680, training loss: 6.470075607299805 = 0.13056538999080658 + 1.0 * 6.339510440826416
Epoch 680, val loss: 0.6161936521530151
Epoch 690, training loss: 6.457613468170166 = 0.1229248046875 + 1.0 * 6.334688663482666
Epoch 690, val loss: 0.6181439161300659
Epoch 700, training loss: 6.449007511138916 = 0.11586334556341171 + 1.0 * 6.333144187927246
Epoch 700, val loss: 0.6204876899719238
Epoch 710, training loss: 6.443656921386719 = 0.10935258120298386 + 1.0 * 6.334304332733154
Epoch 710, val loss: 0.6232113838195801
Epoch 720, training loss: 6.434534549713135 = 0.10332203656435013 + 1.0 * 6.331212520599365
Epoch 720, val loss: 0.6262965202331543
Epoch 730, training loss: 6.4299445152282715 = 0.09776055812835693 + 1.0 * 6.332183837890625
Epoch 730, val loss: 0.6296215653419495
Epoch 740, training loss: 6.422428131103516 = 0.09260137379169464 + 1.0 * 6.329826831817627
Epoch 740, val loss: 0.6331970691680908
Epoch 750, training loss: 6.417809963226318 = 0.08782166987657547 + 1.0 * 6.329988479614258
Epoch 750, val loss: 0.6369708776473999
Epoch 760, training loss: 6.4074249267578125 = 0.08338785916566849 + 1.0 * 6.324037075042725
Epoch 760, val loss: 0.6409334540367126
Epoch 770, training loss: 6.415226459503174 = 0.0792517438530922 + 1.0 * 6.33597469329834
Epoch 770, val loss: 0.64503413438797
Epoch 780, training loss: 6.399543285369873 = 0.07542276382446289 + 1.0 * 6.32412052154541
Epoch 780, val loss: 0.6492429971694946
Epoch 790, training loss: 6.392249584197998 = 0.07183914631605148 + 1.0 * 6.320410251617432
Epoch 790, val loss: 0.6535740494728088
Epoch 800, training loss: 6.388794422149658 = 0.06848461925983429 + 1.0 * 6.320309638977051
Epoch 800, val loss: 0.6579778790473938
Epoch 810, training loss: 6.3871965408325195 = 0.06534481793642044 + 1.0 * 6.32185173034668
Epoch 810, val loss: 0.6624512672424316
Epoch 820, training loss: 6.380411624908447 = 0.062417417764663696 + 1.0 * 6.317994117736816
Epoch 820, val loss: 0.6669561266899109
Epoch 830, training loss: 6.3765411376953125 = 0.05966910719871521 + 1.0 * 6.3168721199035645
Epoch 830, val loss: 0.6715151071548462
Epoch 840, training loss: 6.377068519592285 = 0.05708219110965729 + 1.0 * 6.319986343383789
Epoch 840, val loss: 0.6760985255241394
Epoch 850, training loss: 6.370180606842041 = 0.05465321987867355 + 1.0 * 6.315527439117432
Epoch 850, val loss: 0.6806899309158325
Epoch 860, training loss: 6.370983600616455 = 0.05237635225057602 + 1.0 * 6.318607330322266
Epoch 860, val loss: 0.685300350189209
Epoch 870, training loss: 6.3652520179748535 = 0.05023282766342163 + 1.0 * 6.315019130706787
Epoch 870, val loss: 0.6898990869522095
Epoch 880, training loss: 6.359993934631348 = 0.048221055418252945 + 1.0 * 6.31177282333374
Epoch 880, val loss: 0.6944870352745056
Epoch 890, training loss: 6.356212139129639 = 0.0463130809366703 + 1.0 * 6.309898853302002
Epoch 890, val loss: 0.6990765333175659
Epoch 900, training loss: 6.365357875823975 = 0.044511858373880386 + 1.0 * 6.320846080780029
Epoch 900, val loss: 0.7036564946174622
Epoch 910, training loss: 6.357914447784424 = 0.04281225800514221 + 1.0 * 6.3151021003723145
Epoch 910, val loss: 0.7082409858703613
Epoch 920, training loss: 6.348882675170898 = 0.041213732212781906 + 1.0 * 6.307669162750244
Epoch 920, val loss: 0.7127583622932434
Epoch 930, training loss: 6.345820903778076 = 0.03969546779990196 + 1.0 * 6.306125640869141
Epoch 930, val loss: 0.7172569036483765
Epoch 940, training loss: 6.353540420532227 = 0.03825148940086365 + 1.0 * 6.31528902053833
Epoch 940, val loss: 0.7217177152633667
Epoch 950, training loss: 6.345922470092773 = 0.03688741847872734 + 1.0 * 6.309034824371338
Epoch 950, val loss: 0.7262098789215088
Epoch 960, training loss: 6.341145038604736 = 0.0355948843061924 + 1.0 * 6.3055500984191895
Epoch 960, val loss: 0.7305983304977417
Epoch 970, training loss: 6.349305152893066 = 0.03436606749892235 + 1.0 * 6.314939022064209
Epoch 970, val loss: 0.7349977493286133
Epoch 980, training loss: 6.342920303344727 = 0.03320881351828575 + 1.0 * 6.309711456298828
Epoch 980, val loss: 0.7393096089363098
Epoch 990, training loss: 6.335200786590576 = 0.032103393226861954 + 1.0 * 6.303097248077393
Epoch 990, val loss: 0.743613064289093
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 10.532774925231934 = 1.935944676399231 + 1.0 * 8.596830368041992
Epoch 0, val loss: 1.9317818880081177
Epoch 10, training loss: 10.521942138671875 = 1.9253993034362793 + 1.0 * 8.596543312072754
Epoch 10, val loss: 1.921971321105957
Epoch 20, training loss: 10.506250381469727 = 1.9119462966918945 + 1.0 * 8.594304084777832
Epoch 20, val loss: 1.9091864824295044
Epoch 30, training loss: 10.469095230102539 = 1.8927828073501587 + 1.0 * 8.576312065124512
Epoch 30, val loss: 1.8909095525741577
Epoch 40, training loss: 10.335809707641602 = 1.8674839735031128 + 1.0 * 8.4683256149292
Epoch 40, val loss: 1.8679299354553223
Epoch 50, training loss: 9.969040870666504 = 1.8404338359832764 + 1.0 * 8.128606796264648
Epoch 50, val loss: 1.8444159030914307
Epoch 60, training loss: 9.604297637939453 = 1.8170772790908813 + 1.0 * 7.787220478057861
Epoch 60, val loss: 1.8242367506027222
Epoch 70, training loss: 9.083785057067871 = 1.8010224103927612 + 1.0 * 7.2827630043029785
Epoch 70, val loss: 1.8102761507034302
Epoch 80, training loss: 8.785147666931152 = 1.789091944694519 + 1.0 * 6.996056079864502
Epoch 80, val loss: 1.7990742921829224
Epoch 90, training loss: 8.62732219696045 = 1.771291732788086 + 1.0 * 6.856030464172363
Epoch 90, val loss: 1.7834627628326416
Epoch 100, training loss: 8.50217342376709 = 1.7516918182373047 + 1.0 * 6.750481605529785
Epoch 100, val loss: 1.766608715057373
Epoch 110, training loss: 8.414087295532227 = 1.7322545051574707 + 1.0 * 6.681832313537598
Epoch 110, val loss: 1.7486827373504639
Epoch 120, training loss: 8.347346305847168 = 1.7107648849487305 + 1.0 * 6.6365814208984375
Epoch 120, val loss: 1.7286261320114136
Epoch 130, training loss: 8.287996292114258 = 1.6857831478118896 + 1.0 * 6.602212905883789
Epoch 130, val loss: 1.7063442468643188
Epoch 140, training loss: 8.233454704284668 = 1.6568108797073364 + 1.0 * 6.576643466949463
Epoch 140, val loss: 1.681435465812683
Epoch 150, training loss: 8.179434776306152 = 1.6236283779144287 + 1.0 * 6.5558061599731445
Epoch 150, val loss: 1.653380274772644
Epoch 160, training loss: 8.12882137298584 = 1.5866793394088745 + 1.0 * 6.542141914367676
Epoch 160, val loss: 1.6224029064178467
Epoch 170, training loss: 8.071038246154785 = 1.5468538999557495 + 1.0 * 6.524184703826904
Epoch 170, val loss: 1.5892387628555298
Epoch 180, training loss: 8.01424789428711 = 1.5047012567520142 + 1.0 * 6.509546279907227
Epoch 180, val loss: 1.554321050643921
Epoch 190, training loss: 7.9585161209106445 = 1.4608609676361084 + 1.0 * 6.497654914855957
Epoch 190, val loss: 1.5183203220367432
Epoch 200, training loss: 7.904326438903809 = 1.4172849655151367 + 1.0 * 6.487041473388672
Epoch 200, val loss: 1.4829920530319214
Epoch 210, training loss: 7.851695537567139 = 1.3748425245285034 + 1.0 * 6.476852893829346
Epoch 210, val loss: 1.4488780498504639
Epoch 220, training loss: 7.8012518882751465 = 1.333306908607483 + 1.0 * 6.467945098876953
Epoch 220, val loss: 1.4158810377120972
Epoch 230, training loss: 7.753720760345459 = 1.2927404642105103 + 1.0 * 6.460980415344238
Epoch 230, val loss: 1.383913278579712
Epoch 240, training loss: 7.706913471221924 = 1.253739356994629 + 1.0 * 6.453174114227295
Epoch 240, val loss: 1.3535120487213135
Epoch 250, training loss: 7.663003444671631 = 1.216130256652832 + 1.0 * 6.446873188018799
Epoch 250, val loss: 1.3242958784103394
Epoch 260, training loss: 7.618750095367432 = 1.1790056228637695 + 1.0 * 6.439744472503662
Epoch 260, val loss: 1.2955352067947388
Epoch 270, training loss: 7.577444553375244 = 1.1423449516296387 + 1.0 * 6.4350996017456055
Epoch 270, val loss: 1.2671458721160889
Epoch 280, training loss: 7.534628391265869 = 1.1063331365585327 + 1.0 * 6.428295135498047
Epoch 280, val loss: 1.2392643690109253
Epoch 290, training loss: 7.492938041687012 = 1.0706043243408203 + 1.0 * 6.422333717346191
Epoch 290, val loss: 1.2117319107055664
Epoch 300, training loss: 7.452484130859375 = 1.035137414932251 + 1.0 * 6.417346954345703
Epoch 300, val loss: 1.1843785047531128
Epoch 310, training loss: 7.416333198547363 = 1.0001779794692993 + 1.0 * 6.4161553382873535
Epoch 310, val loss: 1.1574240922927856
Epoch 320, training loss: 7.375720977783203 = 0.9661924839019775 + 1.0 * 6.409528732299805
Epoch 320, val loss: 1.1312445402145386
Epoch 330, training loss: 7.3375372886657715 = 0.9326730966567993 + 1.0 * 6.404864311218262
Epoch 330, val loss: 1.1054847240447998
Epoch 340, training loss: 7.304905414581299 = 0.8992600440979004 + 1.0 * 6.405645370483398
Epoch 340, val loss: 1.0796551704406738
Epoch 350, training loss: 7.265823841094971 = 0.8659252524375916 + 1.0 * 6.399898529052734
Epoch 350, val loss: 1.0536826848983765
Epoch 360, training loss: 7.227164268493652 = 0.8323749899864197 + 1.0 * 6.394789218902588
Epoch 360, val loss: 1.0274556875228882
Epoch 370, training loss: 7.1878204345703125 = 0.7984055280685425 + 1.0 * 6.3894147872924805
Epoch 370, val loss: 1.0006790161132812
Epoch 380, training loss: 7.1572370529174805 = 0.7640116214752197 + 1.0 * 6.39322566986084
Epoch 380, val loss: 0.9734667539596558
Epoch 390, training loss: 7.115972995758057 = 0.7297989130020142 + 1.0 * 6.386174201965332
Epoch 390, val loss: 0.9463380575180054
Epoch 400, training loss: 7.078019618988037 = 0.6959186792373657 + 1.0 * 6.382101058959961
Epoch 400, val loss: 0.9196411967277527
Epoch 410, training loss: 7.040994644165039 = 0.6625639796257019 + 1.0 * 6.3784308433532715
Epoch 410, val loss: 0.8936676979064941
Epoch 420, training loss: 7.009341239929199 = 0.6302171945571899 + 1.0 * 6.379124164581299
Epoch 420, val loss: 0.8690782785415649
Epoch 430, training loss: 6.977991104125977 = 0.5993362665176392 + 1.0 * 6.378654956817627
Epoch 430, val loss: 0.8465167284011841
Epoch 440, training loss: 6.940072059631348 = 0.5698930025100708 + 1.0 * 6.370179176330566
Epoch 440, val loss: 0.8260417580604553
Epoch 450, training loss: 6.917102813720703 = 0.5417652130126953 + 1.0 * 6.375337600708008
Epoch 450, val loss: 0.8076518774032593
Epoch 460, training loss: 6.886776924133301 = 0.5152028203010559 + 1.0 * 6.3715739250183105
Epoch 460, val loss: 0.7915351390838623
Epoch 470, training loss: 6.85382604598999 = 0.49013468623161316 + 1.0 * 6.363691329956055
Epoch 470, val loss: 0.7775710225105286
Epoch 480, training loss: 6.8290605545043945 = 0.46641087532043457 + 1.0 * 6.362649917602539
Epoch 480, val loss: 0.7654714584350586
Epoch 490, training loss: 6.807555198669434 = 0.44408729672431946 + 1.0 * 6.363467693328857
Epoch 490, val loss: 0.7551164031028748
Epoch 500, training loss: 6.7846832275390625 = 0.4232497811317444 + 1.0 * 6.361433506011963
Epoch 500, val loss: 0.7463975548744202
Epoch 510, training loss: 6.759521007537842 = 0.40379273891448975 + 1.0 * 6.3557281494140625
Epoch 510, val loss: 0.7391024827957153
Epoch 520, training loss: 6.74699592590332 = 0.3855988681316376 + 1.0 * 6.3613972663879395
Epoch 520, val loss: 0.733066737651825
Epoch 530, training loss: 6.721283435821533 = 0.36870047450065613 + 1.0 * 6.352582931518555
Epoch 530, val loss: 0.7281439304351807
Epoch 540, training loss: 6.702791213989258 = 0.35287484526634216 + 1.0 * 6.349916458129883
Epoch 540, val loss: 0.7242121696472168
Epoch 550, training loss: 6.691709995269775 = 0.3380170464515686 + 1.0 * 6.353693008422852
Epoch 550, val loss: 0.7211011648178101
Epoch 560, training loss: 6.678348541259766 = 0.3241078853607178 + 1.0 * 6.354240894317627
Epoch 560, val loss: 0.718718945980072
Epoch 570, training loss: 6.655891418457031 = 0.3110080063343048 + 1.0 * 6.344883441925049
Epoch 570, val loss: 0.7170149087905884
Epoch 580, training loss: 6.643919944763184 = 0.29855582118034363 + 1.0 * 6.345364093780518
Epoch 580, val loss: 0.7158188223838806
Epoch 590, training loss: 6.630302906036377 = 0.2866935729980469 + 1.0 * 6.34360933303833
Epoch 590, val loss: 0.7150914669036865
Epoch 600, training loss: 6.618642807006836 = 0.27533817291259766 + 1.0 * 6.343304634094238
Epoch 600, val loss: 0.7147567272186279
Epoch 610, training loss: 6.602440357208252 = 0.26437661051750183 + 1.0 * 6.338063716888428
Epoch 610, val loss: 0.7147554159164429
Epoch 620, training loss: 6.589807510375977 = 0.2536241412162781 + 1.0 * 6.336183547973633
Epoch 620, val loss: 0.7149454951286316
Epoch 630, training loss: 6.577353000640869 = 0.24295850098133087 + 1.0 * 6.334394454956055
Epoch 630, val loss: 0.7152726054191589
Epoch 640, training loss: 6.568297863006592 = 0.23231413960456848 + 1.0 * 6.335983753204346
Epoch 640, val loss: 0.7156313061714172
Epoch 650, training loss: 6.555863380432129 = 0.221654012799263 + 1.0 * 6.334209442138672
Epoch 650, val loss: 0.7160252928733826
Epoch 660, training loss: 6.541479110717773 = 0.21090705692768097 + 1.0 * 6.330572128295898
Epoch 660, val loss: 0.7163882851600647
Epoch 670, training loss: 6.528733253479004 = 0.20007917284965515 + 1.0 * 6.3286542892456055
Epoch 670, val loss: 0.7167768478393555
Epoch 680, training loss: 6.521032333374023 = 0.18927954137325287 + 1.0 * 6.331752777099609
Epoch 680, val loss: 0.717276394367218
Epoch 690, training loss: 6.50842809677124 = 0.17867456376552582 + 1.0 * 6.329753398895264
Epoch 690, val loss: 0.7179661393165588
Epoch 700, training loss: 6.494288921356201 = 0.16833646595478058 + 1.0 * 6.325952529907227
Epoch 700, val loss: 0.7189186811447144
Epoch 710, training loss: 6.48187780380249 = 0.15833857655525208 + 1.0 * 6.3235392570495605
Epoch 710, val loss: 0.720182478427887
Epoch 720, training loss: 6.496050834655762 = 0.1487896740436554 + 1.0 * 6.34726095199585
Epoch 720, val loss: 0.7218482494354248
Epoch 730, training loss: 6.46199369430542 = 0.13986679911613464 + 1.0 * 6.322126865386963
Epoch 730, val loss: 0.7237895131111145
Epoch 740, training loss: 6.451385498046875 = 0.13151665031909943 + 1.0 * 6.319869041442871
Epoch 740, val loss: 0.726071834564209
Epoch 750, training loss: 6.442380428314209 = 0.12372399866580963 + 1.0 * 6.3186564445495605
Epoch 750, val loss: 0.7286596298217773
Epoch 760, training loss: 6.435702323913574 = 0.11647142469882965 + 1.0 * 6.319231033325195
Epoch 760, val loss: 0.7315696477890015
Epoch 770, training loss: 6.429922103881836 = 0.10977988690137863 + 1.0 * 6.3201422691345215
Epoch 770, val loss: 0.734765887260437
Epoch 780, training loss: 6.422605514526367 = 0.1036173477768898 + 1.0 * 6.31898832321167
Epoch 780, val loss: 0.7380852103233337
Epoch 790, training loss: 6.412593841552734 = 0.09792296588420868 + 1.0 * 6.314671039581299
Epoch 790, val loss: 0.7416577339172363
Epoch 800, training loss: 6.407040596008301 = 0.09264634549617767 + 1.0 * 6.314394474029541
Epoch 800, val loss: 0.7454330325126648
Epoch 810, training loss: 6.403634071350098 = 0.08775384724140167 + 1.0 * 6.315880298614502
Epoch 810, val loss: 0.7493716478347778
Epoch 820, training loss: 6.397438049316406 = 0.08322007209062576 + 1.0 * 6.314218044281006
Epoch 820, val loss: 0.7534224390983582
Epoch 830, training loss: 6.3931732177734375 = 0.0790107324719429 + 1.0 * 6.314162254333496
Epoch 830, val loss: 0.7576268911361694
Epoch 840, training loss: 6.383623123168945 = 0.07509499788284302 + 1.0 * 6.308527946472168
Epoch 840, val loss: 0.7619096636772156
Epoch 850, training loss: 6.379603862762451 = 0.07144336402416229 + 1.0 * 6.308160305023193
Epoch 850, val loss: 0.7663239240646362
Epoch 860, training loss: 6.383638858795166 = 0.06803114712238312 + 1.0 * 6.31560754776001
Epoch 860, val loss: 0.7708033323287964
Epoch 870, training loss: 6.375131130218506 = 0.06484542787075043 + 1.0 * 6.310285568237305
Epoch 870, val loss: 0.7753912210464478
Epoch 880, training loss: 6.366954326629639 = 0.061870016157627106 + 1.0 * 6.305084228515625
Epoch 880, val loss: 0.7799428701400757
Epoch 890, training loss: 6.3649821281433105 = 0.05907557159662247 + 1.0 * 6.305906772613525
Epoch 890, val loss: 0.7845621705055237
Epoch 900, training loss: 6.36702823638916 = 0.05645573139190674 + 1.0 * 6.310572624206543
Epoch 900, val loss: 0.7893116474151611
Epoch 910, training loss: 6.358600616455078 = 0.05401016026735306 + 1.0 * 6.304590225219727
Epoch 910, val loss: 0.7938616871833801
Epoch 920, training loss: 6.354123115539551 = 0.05171047896146774 + 1.0 * 6.302412509918213
Epoch 920, val loss: 0.7984547019004822
Epoch 930, training loss: 6.350155830383301 = 0.04953426495194435 + 1.0 * 6.300621509552002
Epoch 930, val loss: 0.8031453490257263
Epoch 940, training loss: 6.35365104675293 = 0.04747791960835457 + 1.0 * 6.306173324584961
Epoch 940, val loss: 0.8078353404998779
Epoch 950, training loss: 6.349748134613037 = 0.04554248973727226 + 1.0 * 6.304205417633057
Epoch 950, val loss: 0.8124561905860901
Epoch 960, training loss: 6.343829154968262 = 0.04371502250432968 + 1.0 * 6.300114154815674
Epoch 960, val loss: 0.8170461058616638
Epoch 970, training loss: 6.3411545753479 = 0.041987158358097076 + 1.0 * 6.299167633056641
Epoch 970, val loss: 0.8216201663017273
Epoch 980, training loss: 6.341427803039551 = 0.04035196453332901 + 1.0 * 6.3010759353637695
Epoch 980, val loss: 0.8262003660202026
Epoch 990, training loss: 6.3423686027526855 = 0.03880643472075462 + 1.0 * 6.303562164306641
Epoch 990, val loss: 0.8306825757026672
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8350026357406432
The final CL Acc:0.81728, 0.02794, The final GNN Acc:0.83764, 0.00336
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9438])
updated graph: torch.Size([2, 10532])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.538595199584961 = 1.9417672157287598 + 1.0 * 8.59682846069336
Epoch 0, val loss: 1.9456497430801392
Epoch 10, training loss: 10.528682708740234 = 1.932112216949463 + 1.0 * 8.596570014953613
Epoch 10, val loss: 1.9355438947677612
Epoch 20, training loss: 10.514749526977539 = 1.920004963874817 + 1.0 * 8.594744682312012
Epoch 20, val loss: 1.9228066205978394
Epoch 30, training loss: 10.483409881591797 = 1.9028414487838745 + 1.0 * 8.580568313598633
Epoch 30, val loss: 1.9049339294433594
Epoch 40, training loss: 10.368504524230957 = 1.8800872564315796 + 1.0 * 8.488417625427246
Epoch 40, val loss: 1.8823267221450806
Epoch 50, training loss: 9.943100929260254 = 1.8565489053726196 + 1.0 * 8.086551666259766
Epoch 50, val loss: 1.8598291873931885
Epoch 60, training loss: 9.469045639038086 = 1.8375520706176758 + 1.0 * 7.631494045257568
Epoch 60, val loss: 1.8425277471542358
Epoch 70, training loss: 9.057313919067383 = 1.8254235982894897 + 1.0 * 7.2318902015686035
Epoch 70, val loss: 1.831214189529419
Epoch 80, training loss: 8.8440580368042 = 1.8127363920211792 + 1.0 * 7.031322002410889
Epoch 80, val loss: 1.8193124532699585
Epoch 90, training loss: 8.706488609313965 = 1.7982280254364014 + 1.0 * 6.908260822296143
Epoch 90, val loss: 1.8058894872665405
Epoch 100, training loss: 8.595205307006836 = 1.785276174545288 + 1.0 * 6.809929370880127
Epoch 100, val loss: 1.794037103652954
Epoch 110, training loss: 8.516180038452148 = 1.7735233306884766 + 1.0 * 6.742656230926514
Epoch 110, val loss: 1.7832342386245728
Epoch 120, training loss: 8.454094886779785 = 1.7606703042984009 + 1.0 * 6.693424224853516
Epoch 120, val loss: 1.7714985609054565
Epoch 130, training loss: 8.401556968688965 = 1.74602472782135 + 1.0 * 6.655532360076904
Epoch 130, val loss: 1.7585490942001343
Epoch 140, training loss: 8.351936340332031 = 1.7297165393829346 + 1.0 * 6.622219562530518
Epoch 140, val loss: 1.7442044019699097
Epoch 150, training loss: 8.305887222290039 = 1.711098074913025 + 1.0 * 6.594789028167725
Epoch 150, val loss: 1.728072166442871
Epoch 160, training loss: 8.262446403503418 = 1.6892704963684082 + 1.0 * 6.57317590713501
Epoch 160, val loss: 1.7093415260314941
Epoch 170, training loss: 8.216163635253906 = 1.6636395454406738 + 1.0 * 6.552523612976074
Epoch 170, val loss: 1.6874569654464722
Epoch 180, training loss: 8.171858787536621 = 1.6334941387176514 + 1.0 * 6.538364887237549
Epoch 180, val loss: 1.6617463827133179
Epoch 190, training loss: 8.120553970336914 = 1.5984644889831543 + 1.0 * 6.522089958190918
Epoch 190, val loss: 1.6318252086639404
Epoch 200, training loss: 8.068032264709473 = 1.558500051498413 + 1.0 * 6.509532451629639
Epoch 200, val loss: 1.597861647605896
Epoch 210, training loss: 8.011154174804688 = 1.5133105516433716 + 1.0 * 6.497843265533447
Epoch 210, val loss: 1.5596452951431274
Epoch 220, training loss: 7.957655906677246 = 1.463889241218567 + 1.0 * 6.493766784667969
Epoch 220, val loss: 1.5185521841049194
Epoch 230, training loss: 7.892910003662109 = 1.4118068218231201 + 1.0 * 6.481103420257568
Epoch 230, val loss: 1.4757943153381348
Epoch 240, training loss: 7.828023910522461 = 1.3568146228790283 + 1.0 * 6.471209526062012
Epoch 240, val loss: 1.4311659336090088
Epoch 250, training loss: 7.762490272521973 = 1.299332857131958 + 1.0 * 6.4631571769714355
Epoch 250, val loss: 1.3853241205215454
Epoch 260, training loss: 7.704470157623291 = 1.2401243448257446 + 1.0 * 6.464345932006836
Epoch 260, val loss: 1.3389304876327515
Epoch 270, training loss: 7.633909225463867 = 1.1814072132110596 + 1.0 * 6.4525017738342285
Epoch 270, val loss: 1.2935949563980103
Epoch 280, training loss: 7.568398475646973 = 1.1232057809829712 + 1.0 * 6.445192813873291
Epoch 280, val loss: 1.249394416809082
Epoch 290, training loss: 7.506490230560303 = 1.0665656328201294 + 1.0 * 6.439924716949463
Epoch 290, val loss: 1.207228422164917
Epoch 300, training loss: 7.453365325927734 = 1.0128203630447388 + 1.0 * 6.440545082092285
Epoch 300, val loss: 1.1679807901382446
Epoch 310, training loss: 7.393813610076904 = 0.9622376561164856 + 1.0 * 6.431575775146484
Epoch 310, val loss: 1.1318413019180298
Epoch 320, training loss: 7.340761661529541 = 0.9149659872055054 + 1.0 * 6.425795555114746
Epoch 320, val loss: 1.0986543893814087
Epoch 330, training loss: 7.297207832336426 = 0.8709427714347839 + 1.0 * 6.426265239715576
Epoch 330, val loss: 1.0685570240020752
Epoch 340, training loss: 7.250558853149414 = 0.8307192325592041 + 1.0 * 6.419839382171631
Epoch 340, val loss: 1.0418115854263306
Epoch 350, training loss: 7.207571029663086 = 0.7939287424087524 + 1.0 * 6.413642406463623
Epoch 350, val loss: 1.0181316137313843
Epoch 360, training loss: 7.170866966247559 = 0.7599523067474365 + 1.0 * 6.410914897918701
Epoch 360, val loss: 0.9970501661300659
Epoch 370, training loss: 7.137284278869629 = 0.7287941575050354 + 1.0 * 6.408490180969238
Epoch 370, val loss: 0.9784832000732422
Epoch 380, training loss: 7.104517459869385 = 0.7001810669898987 + 1.0 * 6.404336452484131
Epoch 380, val loss: 0.9623251557350159
Epoch 390, training loss: 7.070614337921143 = 0.67305988073349 + 1.0 * 6.397554397583008
Epoch 390, val loss: 0.9476845264434814
Epoch 400, training loss: 7.043479919433594 = 0.6469927430152893 + 1.0 * 6.396487236022949
Epoch 400, val loss: 0.9342591166496277
Epoch 410, training loss: 7.020188331604004 = 0.6219322085380554 + 1.0 * 6.398256301879883
Epoch 410, val loss: 0.9220905900001526
Epoch 420, training loss: 6.986944198608398 = 0.5977602601051331 + 1.0 * 6.38918399810791
Epoch 420, val loss: 0.911125898361206
Epoch 430, training loss: 6.962145805358887 = 0.5741751194000244 + 1.0 * 6.387970924377441
Epoch 430, val loss: 0.901099443435669
Epoch 440, training loss: 6.940342426300049 = 0.5512884259223938 + 1.0 * 6.389053821563721
Epoch 440, val loss: 0.8919423818588257
Epoch 450, training loss: 6.9105424880981445 = 0.5292139053344727 + 1.0 * 6.381328582763672
Epoch 450, val loss: 0.8842288255691528
Epoch 460, training loss: 6.885321617126465 = 0.5077364444732666 + 1.0 * 6.377584934234619
Epoch 460, val loss: 0.8775064945220947
Epoch 470, training loss: 6.865272045135498 = 0.48693397641181946 + 1.0 * 6.378337860107422
Epoch 470, val loss: 0.8717904686927795
Epoch 480, training loss: 6.840902805328369 = 0.4670293629169464 + 1.0 * 6.373873233795166
Epoch 480, val loss: 0.867218554019928
Epoch 490, training loss: 6.8194780349731445 = 0.4480198919773102 + 1.0 * 6.371458053588867
Epoch 490, val loss: 0.8636977672576904
Epoch 500, training loss: 6.7983174324035645 = 0.42982351779937744 + 1.0 * 6.368494033813477
Epoch 500, val loss: 0.8611634373664856
Epoch 510, training loss: 6.786789894104004 = 0.41252991557121277 + 1.0 * 6.374259948730469
Epoch 510, val loss: 0.8593186736106873
Epoch 520, training loss: 6.760577201843262 = 0.39620640873908997 + 1.0 * 6.364370822906494
Epoch 520, val loss: 0.8584269285202026
Epoch 530, training loss: 6.742448329925537 = 0.38076937198638916 + 1.0 * 6.3616790771484375
Epoch 530, val loss: 0.8582794666290283
Epoch 540, training loss: 6.725697040557861 = 0.3660740852355957 + 1.0 * 6.359622955322266
Epoch 540, val loss: 0.8587250709533691
Epoch 550, training loss: 6.721949100494385 = 0.35212013125419617 + 1.0 * 6.369829177856445
Epoch 550, val loss: 0.8597402572631836
Epoch 560, training loss: 6.699099063873291 = 0.3389582335948944 + 1.0 * 6.360140800476074
Epoch 560, val loss: 0.8612031936645508
Epoch 570, training loss: 6.6812944412231445 = 0.326495885848999 + 1.0 * 6.354798793792725
Epoch 570, val loss: 0.8633663654327393
Epoch 580, training loss: 6.66798210144043 = 0.3145447373390198 + 1.0 * 6.353437423706055
Epoch 580, val loss: 0.8658950328826904
Epoch 590, training loss: 6.658715724945068 = 0.30305731296539307 + 1.0 * 6.355658531188965
Epoch 590, val loss: 0.8687326908111572
Epoch 600, training loss: 6.6458282470703125 = 0.29208019375801086 + 1.0 * 6.353747844696045
Epoch 600, val loss: 0.8720130920410156
Epoch 610, training loss: 6.628126621246338 = 0.2814702093601227 + 1.0 * 6.346656322479248
Epoch 610, val loss: 0.8757035136222839
Epoch 620, training loss: 6.616293430328369 = 0.271098256111145 + 1.0 * 6.345195293426514
Epoch 620, val loss: 0.8797589540481567
Epoch 630, training loss: 6.606765270233154 = 0.2608945965766907 + 1.0 * 6.345870494842529
Epoch 630, val loss: 0.8841816782951355
Epoch 640, training loss: 6.593482971191406 = 0.2508428990840912 + 1.0 * 6.342639923095703
Epoch 640, val loss: 0.8888770341873169
Epoch 650, training loss: 6.595682144165039 = 0.2409263253211975 + 1.0 * 6.354755878448486
Epoch 650, val loss: 0.8940037488937378
Epoch 660, training loss: 6.573763370513916 = 0.23117396235466003 + 1.0 * 6.342589378356934
Epoch 660, val loss: 0.8995752930641174
Epoch 670, training loss: 6.559980392456055 = 0.2214575558900833 + 1.0 * 6.338522911071777
Epoch 670, val loss: 0.905609667301178
Epoch 680, training loss: 6.549093246459961 = 0.2118159830570221 + 1.0 * 6.337277412414551
Epoch 680, val loss: 0.9121590256690979
Epoch 690, training loss: 6.539279937744141 = 0.20230185985565186 + 1.0 * 6.336977958679199
Epoch 690, val loss: 0.9190301895141602
Epoch 700, training loss: 6.52795934677124 = 0.1930219829082489 + 1.0 * 6.334937572479248
Epoch 700, val loss: 0.9264853596687317
Epoch 710, training loss: 6.5193257331848145 = 0.18394815921783447 + 1.0 * 6.3353776931762695
Epoch 710, val loss: 0.9344874024391174
Epoch 720, training loss: 6.510849475860596 = 0.17510855197906494 + 1.0 * 6.33574104309082
Epoch 720, val loss: 0.9428039789199829
Epoch 730, training loss: 6.498888969421387 = 0.16656909883022308 + 1.0 * 6.332319736480713
Epoch 730, val loss: 0.9515188932418823
Epoch 740, training loss: 6.487742900848389 = 0.1583344042301178 + 1.0 * 6.329408645629883
Epoch 740, val loss: 0.9606834053993225
Epoch 750, training loss: 6.480196952819824 = 0.15038469433784485 + 1.0 * 6.329812049865723
Epoch 750, val loss: 0.9703009724617004
Epoch 760, training loss: 6.47463846206665 = 0.14274616539478302 + 1.0 * 6.331892490386963
Epoch 760, val loss: 0.9801760315895081
Epoch 770, training loss: 6.462851047515869 = 0.13547468185424805 + 1.0 * 6.327376365661621
Epoch 770, val loss: 0.9903236627578735
Epoch 780, training loss: 6.454286098480225 = 0.12854844331741333 + 1.0 * 6.325737476348877
Epoch 780, val loss: 1.0008392333984375
Epoch 790, training loss: 6.448113441467285 = 0.1219443753361702 + 1.0 * 6.326169013977051
Epoch 790, val loss: 1.0117262601852417
Epoch 800, training loss: 6.443341255187988 = 0.11566511541604996 + 1.0 * 6.327676296234131
Epoch 800, val loss: 1.0227888822555542
Epoch 810, training loss: 6.4333672523498535 = 0.10974327474832535 + 1.0 * 6.323624134063721
Epoch 810, val loss: 1.0339996814727783
Epoch 820, training loss: 6.428080081939697 = 0.10414671152830124 + 1.0 * 6.3239336013793945
Epoch 820, val loss: 1.0453932285308838
Epoch 830, training loss: 6.423727512359619 = 0.09887289255857468 + 1.0 * 6.324854850769043
Epoch 830, val loss: 1.0569449663162231
Epoch 840, training loss: 6.414052963256836 = 0.09392813593149185 + 1.0 * 6.320124626159668
Epoch 840, val loss: 1.0686222314834595
Epoch 850, training loss: 6.410128116607666 = 0.08926645666360855 + 1.0 * 6.32086181640625
Epoch 850, val loss: 1.0804399251937866
Epoch 860, training loss: 6.409326553344727 = 0.08488965779542923 + 1.0 * 6.324436664581299
Epoch 860, val loss: 1.0922858715057373
Epoch 870, training loss: 6.400307655334473 = 0.08079022169113159 + 1.0 * 6.319517612457275
Epoch 870, val loss: 1.1041399240493774
Epoch 880, training loss: 6.3929266929626465 = 0.07695619016885757 + 1.0 * 6.315970420837402
Epoch 880, val loss: 1.1161261796951294
Epoch 890, training loss: 6.391289234161377 = 0.073355533182621 + 1.0 * 6.317933559417725
Epoch 890, val loss: 1.1281105279922485
Epoch 900, training loss: 6.38739013671875 = 0.0699891671538353 + 1.0 * 6.317400932312012
Epoch 900, val loss: 1.1399911642074585
Epoch 910, training loss: 6.385242462158203 = 0.06684570759534836 + 1.0 * 6.31839656829834
Epoch 910, val loss: 1.1518696546554565
Epoch 920, training loss: 6.377137184143066 = 0.06389275938272476 + 1.0 * 6.313244342803955
Epoch 920, val loss: 1.1636338233947754
Epoch 930, training loss: 6.372421741485596 = 0.06111915782094002 + 1.0 * 6.311302661895752
Epoch 930, val loss: 1.175466537475586
Epoch 940, training loss: 6.377026557922363 = 0.05850512161850929 + 1.0 * 6.318521499633789
Epoch 940, val loss: 1.1871886253356934
Epoch 950, training loss: 6.372356414794922 = 0.05606834962964058 + 1.0 * 6.316287994384766
Epoch 950, val loss: 1.1987029314041138
Epoch 960, training loss: 6.364716053009033 = 0.05377258360385895 + 1.0 * 6.310943603515625
Epoch 960, val loss: 1.2101340293884277
Epoch 970, training loss: 6.3587260246276855 = 0.051609303802251816 + 1.0 * 6.307116508483887
Epoch 970, val loss: 1.221549153327942
Epoch 980, training loss: 6.356622219085693 = 0.04956072196364403 + 1.0 * 6.307061672210693
Epoch 980, val loss: 1.232879638671875
Epoch 990, training loss: 6.359786510467529 = 0.04762430489063263 + 1.0 * 6.312162399291992
Epoch 990, val loss: 1.2440760135650635
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.797575118608329
=== training gcn model ===
Epoch 0, training loss: 10.528030395507812 = 1.931216835975647 + 1.0 * 8.596813201904297
Epoch 0, val loss: 1.9240137338638306
Epoch 10, training loss: 10.518010139465332 = 1.9214848279953003 + 1.0 * 8.596525192260742
Epoch 10, val loss: 1.9142554998397827
Epoch 20, training loss: 10.50402545928955 = 1.9094312191009521 + 1.0 * 8.59459400177002
Epoch 20, val loss: 1.9021728038787842
Epoch 30, training loss: 10.473404884338379 = 1.8925913572311401 + 1.0 * 8.58081340789795
Epoch 30, val loss: 1.8852359056472778
Epoch 40, training loss: 10.367508888244629 = 1.8705217838287354 + 1.0 * 8.496987342834473
Epoch 40, val loss: 1.8639376163482666
Epoch 50, training loss: 9.954109191894531 = 1.8480830192565918 + 1.0 * 8.106026649475098
Epoch 50, val loss: 1.8432326316833496
Epoch 60, training loss: 9.492107391357422 = 1.829125165939331 + 1.0 * 7.662981986999512
Epoch 60, val loss: 1.8259713649749756
Epoch 70, training loss: 9.024116516113281 = 1.8155826330184937 + 1.0 * 7.208534240722656
Epoch 70, val loss: 1.8136956691741943
Epoch 80, training loss: 8.851476669311523 = 1.8037419319152832 + 1.0 * 7.047735214233398
Epoch 80, val loss: 1.8028119802474976
Epoch 90, training loss: 8.73546028137207 = 1.7889716625213623 + 1.0 * 6.946488857269287
Epoch 90, val loss: 1.7903649806976318
Epoch 100, training loss: 8.622184753417969 = 1.775825023651123 + 1.0 * 6.8463592529296875
Epoch 100, val loss: 1.7799044847488403
Epoch 110, training loss: 8.532493591308594 = 1.7646799087524414 + 1.0 * 6.767813205718994
Epoch 110, val loss: 1.7706893682479858
Epoch 120, training loss: 8.463769912719727 = 1.7525137662887573 + 1.0 * 6.71125602722168
Epoch 120, val loss: 1.7603729963302612
Epoch 130, training loss: 8.406953811645508 = 1.7383019924163818 + 1.0 * 6.668651580810547
Epoch 130, val loss: 1.748513102531433
Epoch 140, training loss: 8.353826522827148 = 1.7220869064331055 + 1.0 * 6.631739139556885
Epoch 140, val loss: 1.7351229190826416
Epoch 150, training loss: 8.305688858032227 = 1.7033156156539917 + 1.0 * 6.6023736000061035
Epoch 150, val loss: 1.7197843790054321
Epoch 160, training loss: 8.259032249450684 = 1.6812762022018433 + 1.0 * 6.577756404876709
Epoch 160, val loss: 1.7017982006072998
Epoch 170, training loss: 8.211276054382324 = 1.6550493240356445 + 1.0 * 6.55622673034668
Epoch 170, val loss: 1.6803677082061768
Epoch 180, training loss: 8.16434097290039 = 1.6239603757858276 + 1.0 * 6.540380954742432
Epoch 180, val loss: 1.6548863649368286
Epoch 190, training loss: 8.112737655639648 = 1.5882469415664673 + 1.0 * 6.5244903564453125
Epoch 190, val loss: 1.6254509687423706
Epoch 200, training loss: 8.058623313903809 = 1.5470999479293823 + 1.0 * 6.511523246765137
Epoch 200, val loss: 1.5915262699127197
Epoch 210, training loss: 8.000114440917969 = 1.5003072023391724 + 1.0 * 6.499807357788086
Epoch 210, val loss: 1.5530120134353638
Epoch 220, training loss: 7.940102577209473 = 1.4486963748931885 + 1.0 * 6.491406440734863
Epoch 220, val loss: 1.51102614402771
Epoch 230, training loss: 7.876153469085693 = 1.3942629098892212 + 1.0 * 6.481890678405762
Epoch 230, val loss: 1.4673128128051758
Epoch 240, training loss: 7.811197757720947 = 1.3377957344055176 + 1.0 * 6.47340202331543
Epoch 240, val loss: 1.4228873252868652
Epoch 250, training loss: 7.750964164733887 = 1.281862497329712 + 1.0 * 6.469101905822754
Epoch 250, val loss: 1.3801653385162354
Epoch 260, training loss: 7.687488555908203 = 1.227817416191101 + 1.0 * 6.4596710205078125
Epoch 260, val loss: 1.3401607275009155
Epoch 270, training loss: 7.628959655761719 = 1.1759634017944336 + 1.0 * 6.452996253967285
Epoch 270, val loss: 1.3030577898025513
Epoch 280, training loss: 7.574693202972412 = 1.1269570589065552 + 1.0 * 6.4477362632751465
Epoch 280, val loss: 1.269204020500183
Epoch 290, training loss: 7.523166656494141 = 1.0817971229553223 + 1.0 * 6.441369533538818
Epoch 290, val loss: 1.239317536354065
Epoch 300, training loss: 7.480002403259277 = 1.0401288270950317 + 1.0 * 6.439873695373535
Epoch 300, val loss: 1.2130686044692993
Epoch 310, training loss: 7.431922912597656 = 1.0019595623016357 + 1.0 * 6.429963111877441
Epoch 310, val loss: 1.1900652647018433
Epoch 320, training loss: 7.392413139343262 = 0.966762363910675 + 1.0 * 6.425650596618652
Epoch 320, val loss: 1.1698490381240845
Epoch 330, training loss: 7.354028701782227 = 0.9339222311973572 + 1.0 * 6.420106410980225
Epoch 330, val loss: 1.1517781019210815
Epoch 340, training loss: 7.325863838195801 = 0.9031313061714172 + 1.0 * 6.422732353210449
Epoch 340, val loss: 1.1353564262390137
Epoch 350, training loss: 7.286282539367676 = 0.8742740154266357 + 1.0 * 6.412008762359619
Epoch 350, val loss: 1.1205412149429321
Epoch 360, training loss: 7.253634929656982 = 0.8463408350944519 + 1.0 * 6.407294273376465
Epoch 360, val loss: 1.106596827507019
Epoch 370, training loss: 7.223366737365723 = 0.8190532326698303 + 1.0 * 6.404313564300537
Epoch 370, val loss: 1.0930936336517334
Epoch 380, training loss: 7.192570686340332 = 0.7923315763473511 + 1.0 * 6.400238990783691
Epoch 380, val loss: 1.0802403688430786
Epoch 390, training loss: 7.162518501281738 = 0.7658240795135498 + 1.0 * 6.396694183349609
Epoch 390, val loss: 1.0677801370620728
Epoch 400, training loss: 7.133312225341797 = 0.7395296096801758 + 1.0 * 6.393782615661621
Epoch 400, val loss: 1.0554739236831665
Epoch 410, training loss: 7.1046366691589355 = 0.7136120200157166 + 1.0 * 6.391024589538574
Epoch 410, val loss: 1.043646216392517
Epoch 420, training loss: 7.074958801269531 = 0.6877413988113403 + 1.0 * 6.3872175216674805
Epoch 420, val loss: 1.031983494758606
Epoch 430, training loss: 7.047855377197266 = 0.6617905497550964 + 1.0 * 6.3860650062561035
Epoch 430, val loss: 1.0203851461410522
Epoch 440, training loss: 7.024846076965332 = 0.6360445618629456 + 1.0 * 6.388801574707031
Epoch 440, val loss: 1.009055256843567
Epoch 450, training loss: 6.9915852546691895 = 0.6103836297988892 + 1.0 * 6.38120174407959
Epoch 450, val loss: 0.9982196688652039
Epoch 460, training loss: 6.962739944458008 = 0.5847154855728149 + 1.0 * 6.378024578094482
Epoch 460, val loss: 0.9874700307846069
Epoch 470, training loss: 6.936135768890381 = 0.5591077208518982 + 1.0 * 6.377027988433838
Epoch 470, val loss: 0.9770032167434692
Epoch 480, training loss: 6.907145977020264 = 0.5337094068527222 + 1.0 * 6.373436450958252
Epoch 480, val loss: 0.9670506119728088
Epoch 490, training loss: 6.87824821472168 = 0.5083056092262268 + 1.0 * 6.369942665100098
Epoch 490, val loss: 0.9574578404426575
Epoch 500, training loss: 6.860435485839844 = 0.48294997215270996 + 1.0 * 6.377485275268555
Epoch 500, val loss: 0.9483194947242737
Epoch 510, training loss: 6.824904441833496 = 0.45783641934394836 + 1.0 * 6.367067813873291
Epoch 510, val loss: 0.9397873878479004
Epoch 520, training loss: 6.813755989074707 = 0.43295004963874817 + 1.0 * 6.380805969238281
Epoch 520, val loss: 0.9319555759429932
Epoch 530, training loss: 6.772712230682373 = 0.40852177143096924 + 1.0 * 6.364190578460693
Epoch 530, val loss: 0.9250392317771912
Epoch 540, training loss: 6.744860649108887 = 0.38444727659225464 + 1.0 * 6.360413551330566
Epoch 540, val loss: 0.9192584753036499
Epoch 550, training loss: 6.720283508300781 = 0.36076322197914124 + 1.0 * 6.359520435333252
Epoch 550, val loss: 0.9143821001052856
Epoch 560, training loss: 6.696636199951172 = 0.33769139647483826 + 1.0 * 6.358944892883301
Epoch 560, val loss: 0.9105404615402222
Epoch 570, training loss: 6.672422409057617 = 0.31556034088134766 + 1.0 * 6.3568620681762695
Epoch 570, val loss: 0.9083411693572998
Epoch 580, training loss: 6.653352737426758 = 0.29431667923927307 + 1.0 * 6.359035968780518
Epoch 580, val loss: 0.9072433710098267
Epoch 590, training loss: 6.630083084106445 = 0.27417877316474915 + 1.0 * 6.3559041023254395
Epoch 590, val loss: 0.9072631001472473
Epoch 600, training loss: 6.607331275939941 = 0.25519198179244995 + 1.0 * 6.352139472961426
Epoch 600, val loss: 0.908597469329834
Epoch 610, training loss: 6.588837623596191 = 0.23730714619159698 + 1.0 * 6.3515305519104
Epoch 610, val loss: 0.9109706282615662
Epoch 620, training loss: 6.573115825653076 = 0.22072315216064453 + 1.0 * 6.352392673492432
Epoch 620, val loss: 0.9140640497207642
Epoch 630, training loss: 6.553772926330566 = 0.20542150735855103 + 1.0 * 6.34835147857666
Epoch 630, val loss: 0.9184837341308594
Epoch 640, training loss: 6.536229610443115 = 0.1912645399570465 + 1.0 * 6.344964981079102
Epoch 640, val loss: 0.9236010313034058
Epoch 650, training loss: 6.5230817794799805 = 0.17813991010189056 + 1.0 * 6.344942092895508
Epoch 650, val loss: 0.9294820427894592
Epoch 660, training loss: 6.511743068695068 = 0.16604432463645935 + 1.0 * 6.345698833465576
Epoch 660, val loss: 0.935883641242981
Epoch 670, training loss: 6.498691082000732 = 0.15500575304031372 + 1.0 * 6.343685150146484
Epoch 670, val loss: 0.9432013630867004
Epoch 680, training loss: 6.485373497009277 = 0.14481443166732788 + 1.0 * 6.340559005737305
Epoch 680, val loss: 0.9509575963020325
Epoch 690, training loss: 6.478141784667969 = 0.13540324568748474 + 1.0 * 6.342738628387451
Epoch 690, val loss: 0.9590499401092529
Epoch 700, training loss: 6.468942642211914 = 0.1267879605293274 + 1.0 * 6.342154502868652
Epoch 700, val loss: 0.9674692749977112
Epoch 710, training loss: 6.458345413208008 = 0.11887279152870178 + 1.0 * 6.339472770690918
Epoch 710, val loss: 0.9762942790985107
Epoch 720, training loss: 6.4469194412231445 = 0.11159928888082504 + 1.0 * 6.335319995880127
Epoch 720, val loss: 0.9853881597518921
Epoch 730, training loss: 6.44420862197876 = 0.1048712208867073 + 1.0 * 6.339337348937988
Epoch 730, val loss: 0.9946251511573792
Epoch 740, training loss: 6.4336442947387695 = 0.0986509844660759 + 1.0 * 6.334993362426758
Epoch 740, val loss: 1.0040230751037598
Epoch 750, training loss: 6.430351257324219 = 0.09292932599782944 + 1.0 * 6.337421894073486
Epoch 750, val loss: 1.0135787725448608
Epoch 760, training loss: 6.419950008392334 = 0.0876370295882225 + 1.0 * 6.332313060760498
Epoch 760, val loss: 1.0231964588165283
Epoch 770, training loss: 6.412452220916748 = 0.08274861425161362 + 1.0 * 6.3297038078308105
Epoch 770, val loss: 1.03300142288208
Epoch 780, training loss: 6.411107063293457 = 0.07819298654794693 + 1.0 * 6.332913875579834
Epoch 780, val loss: 1.042725920677185
Epoch 790, training loss: 6.4035797119140625 = 0.0739942416548729 + 1.0 * 6.329585552215576
Epoch 790, val loss: 1.052390694618225
Epoch 800, training loss: 6.398019313812256 = 0.07009489834308624 + 1.0 * 6.3279242515563965
Epoch 800, val loss: 1.062252163887024
Epoch 810, training loss: 6.391746997833252 = 0.06647150218486786 + 1.0 * 6.325275421142578
Epoch 810, val loss: 1.0720266103744507
Epoch 820, training loss: 6.3967695236206055 = 0.06308529525995255 + 1.0 * 6.33368444442749
Epoch 820, val loss: 1.0817031860351562
Epoch 830, training loss: 6.387338638305664 = 0.059962198138237 + 1.0 * 6.327376365661621
Epoch 830, val loss: 1.0911707878112793
Epoch 840, training loss: 6.38080358505249 = 0.05706828087568283 + 1.0 * 6.323735237121582
Epoch 840, val loss: 1.100869059562683
Epoch 850, training loss: 6.375118732452393 = 0.05436433106660843 + 1.0 * 6.320754528045654
Epoch 850, val loss: 1.1103720664978027
Epoch 860, training loss: 6.374229907989502 = 0.05183181166648865 + 1.0 * 6.3223981857299805
Epoch 860, val loss: 1.1197773218154907
Epoch 870, training loss: 6.368642807006836 = 0.049464087933301926 + 1.0 * 6.319178581237793
Epoch 870, val loss: 1.128841757774353
Epoch 880, training loss: 6.366367340087891 = 0.0472603477537632 + 1.0 * 6.3191070556640625
Epoch 880, val loss: 1.1381860971450806
Epoch 890, training loss: 6.3618855476379395 = 0.045187611132860184 + 1.0 * 6.31669807434082
Epoch 890, val loss: 1.147206425666809
Epoch 900, training loss: 6.373863220214844 = 0.04324740543961525 + 1.0 * 6.330615997314453
Epoch 900, val loss: 1.1561369895935059
Epoch 910, training loss: 6.360743522644043 = 0.04143242537975311 + 1.0 * 6.319311141967773
Epoch 910, val loss: 1.1646512746810913
Epoch 920, training loss: 6.355233669281006 = 0.03974306955933571 + 1.0 * 6.31549072265625
Epoch 920, val loss: 1.1734979152679443
Epoch 930, training loss: 6.3511061668396 = 0.03814589977264404 + 1.0 * 6.312960147857666
Epoch 930, val loss: 1.1820619106292725
Epoch 940, training loss: 6.348512172698975 = 0.036637719720602036 + 1.0 * 6.3118743896484375
Epoch 940, val loss: 1.1904925107955933
Epoch 950, training loss: 6.364615440368652 = 0.03521521016955376 + 1.0 * 6.329400062561035
Epoch 950, val loss: 1.1987460851669312
Epoch 960, training loss: 6.346374034881592 = 0.0338820144534111 + 1.0 * 6.3124918937683105
Epoch 960, val loss: 1.2068150043487549
Epoch 970, training loss: 6.343472480773926 = 0.032626718282699585 + 1.0 * 6.310845851898193
Epoch 970, val loss: 1.215029239654541
Epoch 980, training loss: 6.344698905944824 = 0.031437549740076065 + 1.0 * 6.31326150894165
Epoch 980, val loss: 1.222899317741394
Epoch 990, training loss: 6.338025093078613 = 0.03031599521636963 + 1.0 * 6.307709217071533
Epoch 990, val loss: 1.2305998802185059
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 10.563294410705566 = 1.9664843082427979 + 1.0 * 8.596810340881348
Epoch 0, val loss: 1.9638696908950806
Epoch 10, training loss: 10.552541732788086 = 1.9560513496398926 + 1.0 * 8.596490859985352
Epoch 10, val loss: 1.9530258178710938
Epoch 20, training loss: 10.537349700927734 = 1.9429069757461548 + 1.0 * 8.594442367553711
Epoch 20, val loss: 1.9391834735870361
Epoch 30, training loss: 10.504254341125488 = 1.9241218566894531 + 1.0 * 8.580132484436035
Epoch 30, val loss: 1.9193540811538696
Epoch 40, training loss: 10.398367881774902 = 1.8979817628860474 + 1.0 * 8.500386238098145
Epoch 40, val loss: 1.8927171230316162
Epoch 50, training loss: 9.868435859680176 = 1.8680751323699951 + 1.0 * 8.000360488891602
Epoch 50, val loss: 1.8635168075561523
Epoch 60, training loss: 9.31831169128418 = 1.8445336818695068 + 1.0 * 7.473777770996094
Epoch 60, val loss: 1.8425939083099365
Epoch 70, training loss: 9.026208877563477 = 1.8284049034118652 + 1.0 * 7.197803497314453
Epoch 70, val loss: 1.8269821405410767
Epoch 80, training loss: 8.8441743850708 = 1.811824083328247 + 1.0 * 7.032350540161133
Epoch 80, val loss: 1.811244249343872
Epoch 90, training loss: 8.706357955932617 = 1.7953633069992065 + 1.0 * 6.910995006561279
Epoch 90, val loss: 1.7959433794021606
Epoch 100, training loss: 8.612707138061523 = 1.7793364524841309 + 1.0 * 6.833371162414551
Epoch 100, val loss: 1.7814610004425049
Epoch 110, training loss: 8.536134719848633 = 1.7646064758300781 + 1.0 * 6.771528244018555
Epoch 110, val loss: 1.7682431936264038
Epoch 120, training loss: 8.473275184631348 = 1.7504874467849731 + 1.0 * 6.722787380218506
Epoch 120, val loss: 1.7554844617843628
Epoch 130, training loss: 8.422317504882812 = 1.7351144552230835 + 1.0 * 6.6872029304504395
Epoch 130, val loss: 1.7416611909866333
Epoch 140, training loss: 8.375425338745117 = 1.7174856662750244 + 1.0 * 6.657939910888672
Epoch 140, val loss: 1.7261457443237305
Epoch 150, training loss: 8.329520225524902 = 1.6974010467529297 + 1.0 * 6.632119178771973
Epoch 150, val loss: 1.7087410688400269
Epoch 160, training loss: 8.285395622253418 = 1.6740471124649048 + 1.0 * 6.6113481521606445
Epoch 160, val loss: 1.6884760856628418
Epoch 170, training loss: 8.236462593078613 = 1.6467880010604858 + 1.0 * 6.589674949645996
Epoch 170, val loss: 1.6648210287094116
Epoch 180, training loss: 8.186685562133789 = 1.6149601936340332 + 1.0 * 6.571725368499756
Epoch 180, val loss: 1.6372331380844116
Epoch 190, training loss: 8.136979103088379 = 1.5782501697540283 + 1.0 * 6.55872917175293
Epoch 190, val loss: 1.6055563688278198
Epoch 200, training loss: 8.079239845275879 = 1.5378246307373047 + 1.0 * 6.541415214538574
Epoch 200, val loss: 1.5709264278411865
Epoch 210, training loss: 8.022128105163574 = 1.4940000772476196 + 1.0 * 6.528128147125244
Epoch 210, val loss: 1.5339703559875488
Epoch 220, training loss: 7.963549613952637 = 1.4476358890533447 + 1.0 * 6.515913963317871
Epoch 220, val loss: 1.4958345890045166
Epoch 230, training loss: 7.905760288238525 = 1.4001537561416626 + 1.0 * 6.505606651306152
Epoch 230, val loss: 1.4579010009765625
Epoch 240, training loss: 7.847574234008789 = 1.3523694276809692 + 1.0 * 6.495204925537109
Epoch 240, val loss: 1.4208792448043823
Epoch 250, training loss: 7.791356086730957 = 1.3047839403152466 + 1.0 * 6.486572265625
Epoch 250, val loss: 1.3849376440048218
Epoch 260, training loss: 7.735281944274902 = 1.2574599981307983 + 1.0 * 6.4778218269348145
Epoch 260, val loss: 1.3500983715057373
Epoch 270, training loss: 7.680030822753906 = 1.2107181549072266 + 1.0 * 6.46931266784668
Epoch 270, val loss: 1.3167157173156738
Epoch 280, training loss: 7.628207683563232 = 1.1644645929336548 + 1.0 * 6.463743209838867
Epoch 280, val loss: 1.284356713294983
Epoch 290, training loss: 7.574466228485107 = 1.1195932626724243 + 1.0 * 6.454873085021973
Epoch 290, val loss: 1.2536616325378418
Epoch 300, training loss: 7.526423454284668 = 1.0761733055114746 + 1.0 * 6.450250148773193
Epoch 300, val loss: 1.2246801853179932
Epoch 310, training loss: 7.4771928787231445 = 1.033947229385376 + 1.0 * 6.443245887756348
Epoch 310, val loss: 1.1969023942947388
Epoch 320, training loss: 7.437407970428467 = 0.9930945634841919 + 1.0 * 6.4443135261535645
Epoch 320, val loss: 1.1705296039581299
Epoch 330, training loss: 7.387588977813721 = 0.9539216160774231 + 1.0 * 6.433667182922363
Epoch 330, val loss: 1.145477056503296
Epoch 340, training loss: 7.344451427459717 = 0.9159531593322754 + 1.0 * 6.428498268127441
Epoch 340, val loss: 1.1215848922729492
Epoch 350, training loss: 7.3077545166015625 = 0.8790435791015625 + 1.0 * 6.4287109375
Epoch 350, val loss: 1.098669171333313
Epoch 360, training loss: 7.26499605178833 = 0.8438020348548889 + 1.0 * 6.421194076538086
Epoch 360, val loss: 1.0770658254623413
Epoch 370, training loss: 7.226603984832764 = 0.8100612759590149 + 1.0 * 6.4165425300598145
Epoch 370, val loss: 1.0568441152572632
Epoch 380, training loss: 7.190661907196045 = 0.7774252891540527 + 1.0 * 6.413236618041992
Epoch 380, val loss: 1.037449836730957
Epoch 390, training loss: 7.156364917755127 = 0.7463416457176208 + 1.0 * 6.410023212432861
Epoch 390, val loss: 1.0196892023086548
Epoch 400, training loss: 7.123705863952637 = 0.7167152762413025 + 1.0 * 6.4069905281066895
Epoch 400, val loss: 1.0035496950149536
Epoch 410, training loss: 7.094942092895508 = 0.6884497404098511 + 1.0 * 6.406492233276367
Epoch 410, val loss: 0.9889211058616638
Epoch 420, training loss: 7.061498641967773 = 0.661542534828186 + 1.0 * 6.399956226348877
Epoch 420, val loss: 0.9758942723274231
Epoch 430, training loss: 7.031752586364746 = 0.6358873248100281 + 1.0 * 6.395865440368652
Epoch 430, val loss: 0.9643921852111816
Epoch 440, training loss: 7.006046295166016 = 0.6111716032028198 + 1.0 * 6.394874572753906
Epoch 440, val loss: 0.9542251229286194
Epoch 450, training loss: 6.984241485595703 = 0.5875006318092346 + 1.0 * 6.396740913391113
Epoch 450, val loss: 0.9455074071884155
Epoch 460, training loss: 6.952138900756836 = 0.5648483633995056 + 1.0 * 6.3872904777526855
Epoch 460, val loss: 0.9381581544876099
Epoch 470, training loss: 6.927413463592529 = 0.542938768863678 + 1.0 * 6.384474754333496
Epoch 470, val loss: 0.932004451751709
Epoch 480, training loss: 6.915436744689941 = 0.5218102335929871 + 1.0 * 6.393626689910889
Epoch 480, val loss: 0.9268144369125366
Epoch 490, training loss: 6.884696006774902 = 0.5017510056495667 + 1.0 * 6.3829450607299805
Epoch 490, val loss: 0.9231367707252502
Epoch 500, training loss: 6.859276294708252 = 0.4825049042701721 + 1.0 * 6.376771450042725
Epoch 500, val loss: 0.9205852746963501
Epoch 510, training loss: 6.8408732414245605 = 0.46391960978507996 + 1.0 * 6.376953601837158
Epoch 510, val loss: 0.9189852476119995
Epoch 520, training loss: 6.823066234588623 = 0.44608789682388306 + 1.0 * 6.376978397369385
Epoch 520, val loss: 0.918389618396759
Epoch 530, training loss: 6.800372123718262 = 0.42899781465530396 + 1.0 * 6.371374130249023
Epoch 530, val loss: 0.9189107418060303
Epoch 540, training loss: 6.7803263664245605 = 0.41252246499061584 + 1.0 * 6.367804050445557
Epoch 540, val loss: 0.9202259182929993
Epoch 550, training loss: 6.763330459594727 = 0.3965286612510681 + 1.0 * 6.366801738739014
Epoch 550, val loss: 0.9221516251564026
Epoch 560, training loss: 6.7519989013671875 = 0.38107234239578247 + 1.0 * 6.370926380157471
Epoch 560, val loss: 0.9247326850891113
Epoch 570, training loss: 6.728440284729004 = 0.366118460893631 + 1.0 * 6.362321853637695
Epoch 570, val loss: 0.9281280040740967
Epoch 580, training loss: 6.710319995880127 = 0.35147225856781006 + 1.0 * 6.358847618103027
Epoch 580, val loss: 0.9318069219589233
Epoch 590, training loss: 6.70528507232666 = 0.3370715081691742 + 1.0 * 6.368213653564453
Epoch 590, val loss: 0.9358598589897156
Epoch 600, training loss: 6.681081295013428 = 0.3230404555797577 + 1.0 * 6.358040809631348
Epoch 600, val loss: 0.9403837323188782
Epoch 610, training loss: 6.664853096008301 = 0.309295117855072 + 1.0 * 6.355557918548584
Epoch 610, val loss: 0.9454764127731323
Epoch 620, training loss: 6.648919582366943 = 0.295678049325943 + 1.0 * 6.353241443634033
Epoch 620, val loss: 0.9506321549415588
Epoch 630, training loss: 6.641048908233643 = 0.2821914553642273 + 1.0 * 6.35885763168335
Epoch 630, val loss: 0.9562001824378967
Epoch 640, training loss: 6.62147855758667 = 0.2688964903354645 + 1.0 * 6.352581977844238
Epoch 640, val loss: 0.9623328447341919
Epoch 650, training loss: 6.604753494262695 = 0.2558109760284424 + 1.0 * 6.348942279815674
Epoch 650, val loss: 0.9688225388526917
Epoch 660, training loss: 6.592859268188477 = 0.2429918795824051 + 1.0 * 6.349867343902588
Epoch 660, val loss: 0.9755929708480835
Epoch 670, training loss: 6.576659679412842 = 0.23050597310066223 + 1.0 * 6.346153736114502
Epoch 670, val loss: 0.9830338358879089
Epoch 680, training loss: 6.578380584716797 = 0.2183917611837387 + 1.0 * 6.359988689422607
Epoch 680, val loss: 0.9906781315803528
Epoch 690, training loss: 6.5495476722717285 = 0.20683306455612183 + 1.0 * 6.342714786529541
Epoch 690, val loss: 0.998992919921875
Epoch 700, training loss: 6.537929534912109 = 0.19573527574539185 + 1.0 * 6.342194080352783
Epoch 700, val loss: 1.0077499151229858
Epoch 710, training loss: 6.526641845703125 = 0.18510198593139648 + 1.0 * 6.3415398597717285
Epoch 710, val loss: 1.0168569087982178
Epoch 720, training loss: 6.514834403991699 = 0.17501895129680634 + 1.0 * 6.339815616607666
Epoch 720, val loss: 1.0264102220535278
Epoch 730, training loss: 6.504909515380859 = 0.1655299961566925 + 1.0 * 6.33937931060791
Epoch 730, val loss: 1.0367271900177002
Epoch 740, training loss: 6.494213581085205 = 0.15655206143856049 + 1.0 * 6.3376617431640625
Epoch 740, val loss: 1.0472861528396606
Epoch 750, training loss: 6.487635612487793 = 0.1480593979358673 + 1.0 * 6.339576244354248
Epoch 750, val loss: 1.0583324432373047
Epoch 760, training loss: 6.477019786834717 = 0.14010266959667206 + 1.0 * 6.336916923522949
Epoch 760, val loss: 1.0697518587112427
Epoch 770, training loss: 6.4686126708984375 = 0.13264063000679016 + 1.0 * 6.335971832275391
Epoch 770, val loss: 1.081616997718811
Epoch 780, training loss: 6.470713138580322 = 0.1256304532289505 + 1.0 * 6.345082759857178
Epoch 780, val loss: 1.0936094522476196
Epoch 790, training loss: 6.452822208404541 = 0.11909080296754837 + 1.0 * 6.333731174468994
Epoch 790, val loss: 1.1058229207992554
Epoch 800, training loss: 6.445171356201172 = 0.11295291036367416 + 1.0 * 6.332218647003174
Epoch 800, val loss: 1.1183816194534302
Epoch 810, training loss: 6.436323165893555 = 0.10716963559389114 + 1.0 * 6.329153537750244
Epoch 810, val loss: 1.131044626235962
Epoch 820, training loss: 6.436654090881348 = 0.10171983391046524 + 1.0 * 6.334934234619141
Epoch 820, val loss: 1.1439341306686401
Epoch 830, training loss: 6.429455280303955 = 0.09661457687616348 + 1.0 * 6.332840919494629
Epoch 830, val loss: 1.1570364236831665
Epoch 840, training loss: 6.4188103675842285 = 0.09182747453451157 + 1.0 * 6.3269829750061035
Epoch 840, val loss: 1.1702511310577393
Epoch 850, training loss: 6.413036346435547 = 0.08732302486896515 + 1.0 * 6.325713157653809
Epoch 850, val loss: 1.18352472782135
Epoch 860, training loss: 6.417893409729004 = 0.08308973908424377 + 1.0 * 6.334803581237793
Epoch 860, val loss: 1.196948766708374
Epoch 870, training loss: 6.406824588775635 = 0.07912398874759674 + 1.0 * 6.327700614929199
Epoch 870, val loss: 1.2102298736572266
Epoch 880, training loss: 6.4001264572143555 = 0.07540900260210037 + 1.0 * 6.3247175216674805
Epoch 880, val loss: 1.2236884832382202
Epoch 890, training loss: 6.400932788848877 = 0.07192663103342056 + 1.0 * 6.329006195068359
Epoch 890, val loss: 1.2369059324264526
Epoch 900, training loss: 6.390522480010986 = 0.06865768879652023 + 1.0 * 6.321864604949951
Epoch 900, val loss: 1.2503151893615723
Epoch 910, training loss: 6.385655403137207 = 0.06557922810316086 + 1.0 * 6.320075988769531
Epoch 910, val loss: 1.2635782957077026
Epoch 920, training loss: 6.383311748504639 = 0.06267707794904709 + 1.0 * 6.320634841918945
Epoch 920, val loss: 1.2767854928970337
Epoch 930, training loss: 6.3798604011535645 = 0.05995338410139084 + 1.0 * 6.319907188415527
Epoch 930, val loss: 1.289925217628479
Epoch 940, training loss: 6.3787994384765625 = 0.05740012228488922 + 1.0 * 6.321399211883545
Epoch 940, val loss: 1.303079605102539
Epoch 950, training loss: 6.371631622314453 = 0.054996099323034286 + 1.0 * 6.316635608673096
Epoch 950, val loss: 1.316016435623169
Epoch 960, training loss: 6.368072509765625 = 0.05272310599684715 + 1.0 * 6.315349578857422
Epoch 960, val loss: 1.3288365602493286
Epoch 970, training loss: 6.372570037841797 = 0.050574932247400284 + 1.0 * 6.321995258331299
Epoch 970, val loss: 1.3416876792907715
Epoch 980, training loss: 6.367129802703857 = 0.04855061322450638 + 1.0 * 6.318579196929932
Epoch 980, val loss: 1.3542990684509277
Epoch 990, training loss: 6.362674713134766 = 0.04664992168545723 + 1.0 * 6.3160247802734375
Epoch 990, val loss: 1.366899013519287
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8065366367949395
The final CL Acc:0.75185, 0.01048, The final GNN Acc:0.80390, 0.00449
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13090])
remove edge: torch.Size([2, 7852])
updated graph: torch.Size([2, 10386])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.545818328857422 = 1.9490160942077637 + 1.0 * 8.596802711486816
Epoch 0, val loss: 1.9371095895767212
Epoch 10, training loss: 10.535103797912598 = 1.9386006593704224 + 1.0 * 8.596503257751465
Epoch 10, val loss: 1.927415132522583
Epoch 20, training loss: 10.519749641418457 = 1.9255255460739136 + 1.0 * 8.594223976135254
Epoch 20, val loss: 1.914927363395691
Epoch 30, training loss: 10.483107566833496 = 1.9071552753448486 + 1.0 * 8.575952529907227
Epoch 30, val loss: 1.8972952365875244
Epoch 40, training loss: 10.321993827819824 = 1.8828881978988647 + 1.0 * 8.439105987548828
Epoch 40, val loss: 1.8746916055679321
Epoch 50, training loss: 9.577898025512695 = 1.857487440109253 + 1.0 * 7.7204108238220215
Epoch 50, val loss: 1.851656436920166
Epoch 60, training loss: 9.22097110748291 = 1.839012622833252 + 1.0 * 7.381958484649658
Epoch 60, val loss: 1.8345500230789185
Epoch 70, training loss: 8.946457862854004 = 1.8224217891693115 + 1.0 * 7.1240363121032715
Epoch 70, val loss: 1.818916916847229
Epoch 80, training loss: 8.768465995788574 = 1.805869221687317 + 1.0 * 6.962596893310547
Epoch 80, val loss: 1.8035367727279663
Epoch 90, training loss: 8.65705394744873 = 1.788277268409729 + 1.0 * 6.868776798248291
Epoch 90, val loss: 1.788065791130066
Epoch 100, training loss: 8.572493553161621 = 1.7709184885025024 + 1.0 * 6.80157470703125
Epoch 100, val loss: 1.7731928825378418
Epoch 110, training loss: 8.497803688049316 = 1.7546721696853638 + 1.0 * 6.743131637573242
Epoch 110, val loss: 1.758801817893982
Epoch 120, training loss: 8.43887710571289 = 1.7384942770004272 + 1.0 * 6.700383186340332
Epoch 120, val loss: 1.7440811395645142
Epoch 130, training loss: 8.385404586791992 = 1.7204875946044922 + 1.0 * 6.664916515350342
Epoch 130, val loss: 1.727898120880127
Epoch 140, training loss: 8.334636688232422 = 1.6995257139205933 + 1.0 * 6.635110855102539
Epoch 140, val loss: 1.7094542980194092
Epoch 150, training loss: 8.282159805297852 = 1.675226092338562 + 1.0 * 6.60693359375
Epoch 150, val loss: 1.6881959438323975
Epoch 160, training loss: 8.230653762817383 = 1.6465890407562256 + 1.0 * 6.584064960479736
Epoch 160, val loss: 1.6631416082382202
Epoch 170, training loss: 8.181127548217773 = 1.6131114959716797 + 1.0 * 6.568016052246094
Epoch 170, val loss: 1.6340711116790771
Epoch 180, training loss: 8.123167991638184 = 1.575093150138855 + 1.0 * 6.548075199127197
Epoch 180, val loss: 1.6013994216918945
Epoch 190, training loss: 8.063882827758789 = 1.5320947170257568 + 1.0 * 6.531787872314453
Epoch 190, val loss: 1.5643765926361084
Epoch 200, training loss: 8.001477241516113 = 1.484087347984314 + 1.0 * 6.51738977432251
Epoch 200, val loss: 1.5233685970306396
Epoch 210, training loss: 7.942997932434082 = 1.4327257871627808 + 1.0 * 6.510272026062012
Epoch 210, val loss: 1.4803067445755005
Epoch 220, training loss: 7.875024318695068 = 1.3803991079330444 + 1.0 * 6.494625091552734
Epoch 220, val loss: 1.436772346496582
Epoch 230, training loss: 7.810196876525879 = 1.3268693685531616 + 1.0 * 6.483327388763428
Epoch 230, val loss: 1.3926526308059692
Epoch 240, training loss: 7.747345447540283 = 1.2728105783462524 + 1.0 * 6.47453498840332
Epoch 240, val loss: 1.3487846851348877
Epoch 250, training loss: 7.6884894371032715 = 1.2200599908828735 + 1.0 * 6.4684295654296875
Epoch 250, val loss: 1.306610345840454
Epoch 260, training loss: 7.628325462341309 = 1.169323205947876 + 1.0 * 6.459002494812012
Epoch 260, val loss: 1.2666230201721191
Epoch 270, training loss: 7.573218822479248 = 1.1203924417495728 + 1.0 * 6.452826499938965
Epoch 270, val loss: 1.2285298109054565
Epoch 280, training loss: 7.521002292633057 = 1.0739864110946655 + 1.0 * 6.447015762329102
Epoch 280, val loss: 1.1928941011428833
Epoch 290, training loss: 7.469432353973389 = 1.0299452543258667 + 1.0 * 6.439486980438232
Epoch 290, val loss: 1.1594713926315308
Epoch 300, training loss: 7.423524379730225 = 0.9876676201820374 + 1.0 * 6.435856819152832
Epoch 300, val loss: 1.1276299953460693
Epoch 310, training loss: 7.378612041473389 = 0.9478643536567688 + 1.0 * 6.4307475090026855
Epoch 310, val loss: 1.0978760719299316
Epoch 320, training loss: 7.333296298980713 = 0.9098362922668457 + 1.0 * 6.423460006713867
Epoch 320, val loss: 1.0695325136184692
Epoch 330, training loss: 7.2917094230651855 = 0.8726403117179871 + 1.0 * 6.419069290161133
Epoch 330, val loss: 1.0417835712432861
Epoch 340, training loss: 7.259671211242676 = 0.8358751535415649 + 1.0 * 6.4237961769104
Epoch 340, val loss: 1.014517068862915
Epoch 350, training loss: 7.212238311767578 = 0.7999637126922607 + 1.0 * 6.412274360656738
Epoch 350, val loss: 0.987906813621521
Epoch 360, training loss: 7.1725335121154785 = 0.7646054029464722 + 1.0 * 6.407927989959717
Epoch 360, val loss: 0.9619114398956299
Epoch 370, training loss: 7.138726234436035 = 0.7298610806465149 + 1.0 * 6.408864974975586
Epoch 370, val loss: 0.9366232752799988
Epoch 380, training loss: 7.0966973304748535 = 0.6961732506752014 + 1.0 * 6.400524139404297
Epoch 380, val loss: 0.9124726057052612
Epoch 390, training loss: 7.060574054718018 = 0.6633873581886292 + 1.0 * 6.397186756134033
Epoch 390, val loss: 0.8894315958023071
Epoch 400, training loss: 7.036603927612305 = 0.6316009163856506 + 1.0 * 6.405003070831299
Epoch 400, val loss: 0.8676486015319824
Epoch 410, training loss: 6.995506763458252 = 0.6013455986976624 + 1.0 * 6.394161224365234
Epoch 410, val loss: 0.8475872874259949
Epoch 420, training loss: 6.96162223815918 = 0.5723130106925964 + 1.0 * 6.389309406280518
Epoch 420, val loss: 0.8291118144989014
Epoch 430, training loss: 6.930441856384277 = 0.5442692637443542 + 1.0 * 6.386172771453857
Epoch 430, val loss: 0.8120006322860718
Epoch 440, training loss: 6.900887966156006 = 0.5170660018920898 + 1.0 * 6.383821964263916
Epoch 440, val loss: 0.7961137294769287
Epoch 450, training loss: 6.876955509185791 = 0.4906947910785675 + 1.0 * 6.386260509490967
Epoch 450, val loss: 0.7814133763313293
Epoch 460, training loss: 6.845293045043945 = 0.4650709927082062 + 1.0 * 6.380221843719482
Epoch 460, val loss: 0.7678974270820618
Epoch 470, training loss: 6.815537929534912 = 0.4399973154067993 + 1.0 * 6.375540733337402
Epoch 470, val loss: 0.755257785320282
Epoch 480, training loss: 6.8026275634765625 = 0.41525766253471375 + 1.0 * 6.3873701095581055
Epoch 480, val loss: 0.7433778047561646
Epoch 490, training loss: 6.764126777648926 = 0.39120060205459595 + 1.0 * 6.372926235198975
Epoch 490, val loss: 0.7323564887046814
Epoch 500, training loss: 6.7373199462890625 = 0.3677273690700531 + 1.0 * 6.369592666625977
Epoch 500, val loss: 0.7221792340278625
Epoch 510, training loss: 6.716185569763184 = 0.34481045603752136 + 1.0 * 6.37137508392334
Epoch 510, val loss: 0.7127739191055298
Epoch 520, training loss: 6.690553665161133 = 0.32268089056015015 + 1.0 * 6.367872714996338
Epoch 520, val loss: 0.7043011784553528
Epoch 530, training loss: 6.667456150054932 = 0.3015640377998352 + 1.0 * 6.365891933441162
Epoch 530, val loss: 0.6967555284500122
Epoch 540, training loss: 6.645392894744873 = 0.28161486983299255 + 1.0 * 6.363778114318848
Epoch 540, val loss: 0.6902416348457336
Epoch 550, training loss: 6.622241497039795 = 0.26275351643562317 + 1.0 * 6.359488010406494
Epoch 550, val loss: 0.6847419142723083
Epoch 560, training loss: 6.604405879974365 = 0.2450122982263565 + 1.0 * 6.35939359664917
Epoch 560, val loss: 0.6802729964256287
Epoch 570, training loss: 6.594410419464111 = 0.22865410149097443 + 1.0 * 6.365756511688232
Epoch 570, val loss: 0.676877498626709
Epoch 580, training loss: 6.566963195800781 = 0.21361489593982697 + 1.0 * 6.353348255157471
Epoch 580, val loss: 0.6745179295539856
Epoch 590, training loss: 6.5522966384887695 = 0.1996857076883316 + 1.0 * 6.352611064910889
Epoch 590, val loss: 0.6731002926826477
Epoch 600, training loss: 6.540569305419922 = 0.1867702752351761 + 1.0 * 6.353798866271973
Epoch 600, val loss: 0.6725066900253296
Epoch 610, training loss: 6.531016826629639 = 0.17492322623729706 + 1.0 * 6.356093406677246
Epoch 610, val loss: 0.6727756261825562
Epoch 620, training loss: 6.511415958404541 = 0.16402587294578552 + 1.0 * 6.347390174865723
Epoch 620, val loss: 0.6737878322601318
Epoch 630, training loss: 6.510530471801758 = 0.15395106375217438 + 1.0 * 6.356579303741455
Epoch 630, val loss: 0.6754623651504517
Epoch 640, training loss: 6.491811752319336 = 0.14474257826805115 + 1.0 * 6.347069263458252
Epoch 640, val loss: 0.6777510046958923
Epoch 650, training loss: 6.478457450866699 = 0.13622812926769257 + 1.0 * 6.34222936630249
Epoch 650, val loss: 0.6805514097213745
Epoch 660, training loss: 6.468298435211182 = 0.12832927703857422 + 1.0 * 6.339969158172607
Epoch 660, val loss: 0.6838283538818359
Epoch 670, training loss: 6.461496353149414 = 0.12099091708660126 + 1.0 * 6.340505599975586
Epoch 670, val loss: 0.6875698566436768
Epoch 680, training loss: 6.457690715789795 = 0.11421194672584534 + 1.0 * 6.343478679656982
Epoch 680, val loss: 0.6915809512138367
Epoch 690, training loss: 6.443691253662109 = 0.10797004401683807 + 1.0 * 6.335721015930176
Epoch 690, val loss: 0.6958896517753601
Epoch 700, training loss: 6.436634540557861 = 0.10216855257749557 + 1.0 * 6.334465980529785
Epoch 700, val loss: 0.7004641890525818
Epoch 710, training loss: 6.43581485748291 = 0.09676454216241837 + 1.0 * 6.33905029296875
Epoch 710, val loss: 0.7052778601646423
Epoch 720, training loss: 6.426992416381836 = 0.09174436330795288 + 1.0 * 6.335247993469238
Epoch 720, val loss: 0.7102730870246887
Epoch 730, training loss: 6.418809413909912 = 0.08708158880472183 + 1.0 * 6.331727981567383
Epoch 730, val loss: 0.7154194712638855
Epoch 740, training loss: 6.417780876159668 = 0.08276361227035522 + 1.0 * 6.335017204284668
Epoch 740, val loss: 0.7206756472587585
Epoch 750, training loss: 6.409049034118652 = 0.07875316590070724 + 1.0 * 6.330296039581299
Epoch 750, val loss: 0.7260414361953735
Epoch 760, training loss: 6.401456356048584 = 0.07498618960380554 + 1.0 * 6.326470375061035
Epoch 760, val loss: 0.7315871715545654
Epoch 770, training loss: 6.396827697753906 = 0.07144498825073242 + 1.0 * 6.325382709503174
Epoch 770, val loss: 0.7372768521308899
Epoch 780, training loss: 6.400806903839111 = 0.0681152269244194 + 1.0 * 6.332691669464111
Epoch 780, val loss: 0.7430540919303894
Epoch 790, training loss: 6.390136241912842 = 0.06501494348049164 + 1.0 * 6.3251214027404785
Epoch 790, val loss: 0.7489444613456726
Epoch 800, training loss: 6.383986473083496 = 0.06210043653845787 + 1.0 * 6.32188606262207
Epoch 800, val loss: 0.7548661231994629
Epoch 810, training loss: 6.396485328674316 = 0.05936289578676224 + 1.0 * 6.337122440338135
Epoch 810, val loss: 0.7608873248100281
Epoch 820, training loss: 6.378920555114746 = 0.05680808052420616 + 1.0 * 6.322112560272217
Epoch 820, val loss: 0.7668572068214417
Epoch 830, training loss: 6.374655723571777 = 0.054423075169324875 + 1.0 * 6.32023286819458
Epoch 830, val loss: 0.7728226184844971
Epoch 840, training loss: 6.371039390563965 = 0.05216628685593605 + 1.0 * 6.318872928619385
Epoch 840, val loss: 0.7788614630699158
Epoch 850, training loss: 6.3719353675842285 = 0.05003516003489494 + 1.0 * 6.321900367736816
Epoch 850, val loss: 0.7849230766296387
Epoch 860, training loss: 6.363613128662109 = 0.04802865907549858 + 1.0 * 6.315584659576416
Epoch 860, val loss: 0.7909262180328369
Epoch 870, training loss: 6.360083103179932 = 0.04613508656620979 + 1.0 * 6.313948154449463
Epoch 870, val loss: 0.7969867587089539
Epoch 880, training loss: 6.362155437469482 = 0.04433436319231987 + 1.0 * 6.317821025848389
Epoch 880, val loss: 0.803044855594635
Epoch 890, training loss: 6.358055114746094 = 0.04264186695218086 + 1.0 * 6.315413475036621
Epoch 890, val loss: 0.8090823888778687
Epoch 900, training loss: 6.3551788330078125 = 0.04104474186897278 + 1.0 * 6.314134120941162
Epoch 900, val loss: 0.8149886727333069
Epoch 910, training loss: 6.356407165527344 = 0.03953484818339348 + 1.0 * 6.3168721199035645
Epoch 910, val loss: 0.8209179043769836
Epoch 920, training loss: 6.349729537963867 = 0.0381072461605072 + 1.0 * 6.311622142791748
Epoch 920, val loss: 0.8268178105354309
Epoch 930, training loss: 6.346572399139404 = 0.03674693405628204 + 1.0 * 6.309825420379639
Epoch 930, val loss: 0.8326476216316223
Epoch 940, training loss: 6.346848487854004 = 0.03545690327882767 + 1.0 * 6.311391353607178
Epoch 940, val loss: 0.8384948372840881
Epoch 950, training loss: 6.341440200805664 = 0.034240853041410446 + 1.0 * 6.307199478149414
Epoch 950, val loss: 0.8441758751869202
Epoch 960, training loss: 6.33974552154541 = 0.03308267891407013 + 1.0 * 6.3066630363464355
Epoch 960, val loss: 0.8498784899711609
Epoch 970, training loss: 6.355203628540039 = 0.03197641298174858 + 1.0 * 6.323227405548096
Epoch 970, val loss: 0.8555870652198792
Epoch 980, training loss: 6.335877418518066 = 0.030932657420635223 + 1.0 * 6.30494499206543
Epoch 980, val loss: 0.8610923290252686
Epoch 990, training loss: 6.334639072418213 = 0.02994026429951191 + 1.0 * 6.304698944091797
Epoch 990, val loss: 0.8665786385536194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 10.536720275878906 = 1.939963936805725 + 1.0 * 8.596755981445312
Epoch 0, val loss: 1.941606044769287
Epoch 10, training loss: 10.525406837463379 = 1.9292734861373901 + 1.0 * 8.5961332321167
Epoch 10, val loss: 1.9302772283554077
Epoch 20, training loss: 10.506806373596191 = 1.915028691291809 + 1.0 * 8.591777801513672
Epoch 20, val loss: 1.9151023626327515
Epoch 30, training loss: 10.458410263061523 = 1.8944551944732666 + 1.0 * 8.563955307006836
Epoch 30, val loss: 1.8932361602783203
Epoch 40, training loss: 10.26213264465332 = 1.8700807094573975 + 1.0 * 8.392051696777344
Epoch 40, val loss: 1.8693071603775024
Epoch 50, training loss: 9.722612380981445 = 1.8444511890411377 + 1.0 * 7.8781609535217285
Epoch 50, val loss: 1.8445212841033936
Epoch 60, training loss: 9.29470443725586 = 1.824665904045105 + 1.0 * 7.470038890838623
Epoch 60, val loss: 1.8257765769958496
Epoch 70, training loss: 8.991707801818848 = 1.8103500604629517 + 1.0 * 7.1813578605651855
Epoch 70, val loss: 1.8118157386779785
Epoch 80, training loss: 8.767919540405273 = 1.7967978715896606 + 1.0 * 6.971121311187744
Epoch 80, val loss: 1.7990590333938599
Epoch 90, training loss: 8.633248329162598 = 1.7830125093460083 + 1.0 * 6.850235462188721
Epoch 90, val loss: 1.7857764959335327
Epoch 100, training loss: 8.532094955444336 = 1.7679544687271118 + 1.0 * 6.764140605926514
Epoch 100, val loss: 1.7711009979248047
Epoch 110, training loss: 8.453176498413086 = 1.752932071685791 + 1.0 * 6.700244426727295
Epoch 110, val loss: 1.7570730447769165
Epoch 120, training loss: 8.391376495361328 = 1.7371596097946167 + 1.0 * 6.654216766357422
Epoch 120, val loss: 1.743045449256897
Epoch 130, training loss: 8.340617179870605 = 1.7190688848495483 + 1.0 * 6.621548175811768
Epoch 130, val loss: 1.727382779121399
Epoch 140, training loss: 8.291946411132812 = 1.6980113983154297 + 1.0 * 6.593934535980225
Epoch 140, val loss: 1.7093828916549683
Epoch 150, training loss: 8.242438316345215 = 1.6732256412506104 + 1.0 * 6.569212436676025
Epoch 150, val loss: 1.6883200407028198
Epoch 160, training loss: 8.192233085632324 = 1.6443134546279907 + 1.0 * 6.547919273376465
Epoch 160, val loss: 1.6639339923858643
Epoch 170, training loss: 8.139201164245605 = 1.6110122203826904 + 1.0 * 6.528188705444336
Epoch 170, val loss: 1.6358609199523926
Epoch 180, training loss: 8.084565162658691 = 1.5730222463607788 + 1.0 * 6.511542797088623
Epoch 180, val loss: 1.6038397550582886
Epoch 190, training loss: 8.027936935424805 = 1.5308319330215454 + 1.0 * 6.497104644775391
Epoch 190, val loss: 1.5684971809387207
Epoch 200, training loss: 7.969106674194336 = 1.484986424446106 + 1.0 * 6.4841203689575195
Epoch 200, val loss: 1.5303065776824951
Epoch 210, training loss: 7.911365032196045 = 1.4363865852355957 + 1.0 * 6.474978446960449
Epoch 210, val loss: 1.4903008937835693
Epoch 220, training loss: 7.849699020385742 = 1.38601815700531 + 1.0 * 6.463680744171143
Epoch 220, val loss: 1.4493350982666016
Epoch 230, training loss: 7.7895612716674805 = 1.334242582321167 + 1.0 * 6.455318927764893
Epoch 230, val loss: 1.4076913595199585
Epoch 240, training loss: 7.729339599609375 = 1.281572937965393 + 1.0 * 6.4477667808532715
Epoch 240, val loss: 1.365899920463562
Epoch 250, training loss: 7.669677734375 = 1.2287980318069458 + 1.0 * 6.440879821777344
Epoch 250, val loss: 1.3248368501663208
Epoch 260, training loss: 7.6117072105407715 = 1.1765727996826172 + 1.0 * 6.435134410858154
Epoch 260, val loss: 1.2843399047851562
Epoch 270, training loss: 7.55519962310791 = 1.1252120733261108 + 1.0 * 6.42998743057251
Epoch 270, val loss: 1.2444902658462524
Epoch 280, training loss: 7.499107837677002 = 1.0754157304763794 + 1.0 * 6.423692226409912
Epoch 280, val loss: 1.2061551809310913
Epoch 290, training loss: 7.445345878601074 = 1.0272318124771118 + 1.0 * 6.418114185333252
Epoch 290, val loss: 1.1690967082977295
Epoch 300, training loss: 7.401710510253906 = 0.9813059568405151 + 1.0 * 6.420404434204102
Epoch 300, val loss: 1.1339191198349
Epoch 310, training loss: 7.348324775695801 = 0.9385292530059814 + 1.0 * 6.40979528427124
Epoch 310, val loss: 1.1012150049209595
Epoch 320, training loss: 7.303009986877441 = 0.8979976177215576 + 1.0 * 6.405012130737305
Epoch 320, val loss: 1.0702378749847412
Epoch 330, training loss: 7.266935348510742 = 0.8593806028366089 + 1.0 * 6.407554626464844
Epoch 330, val loss: 1.0404664278030396
Epoch 340, training loss: 7.219202995300293 = 0.8230348229408264 + 1.0 * 6.396168231964111
Epoch 340, val loss: 1.0124307870864868
Epoch 350, training loss: 7.180879592895508 = 0.7882630228996277 + 1.0 * 6.3926167488098145
Epoch 350, val loss: 0.9855453968048096
Epoch 360, training loss: 7.143442153930664 = 0.7549418807029724 + 1.0 * 6.388500213623047
Epoch 360, val loss: 0.9596203565597534
Epoch 370, training loss: 7.108367443084717 = 0.7229666113853455 + 1.0 * 6.385400772094727
Epoch 370, val loss: 0.9349851012229919
Epoch 380, training loss: 7.07352876663208 = 0.6919439435005188 + 1.0 * 6.381584644317627
Epoch 380, val loss: 0.9111770987510681
Epoch 390, training loss: 7.047752380371094 = 0.6619700193405151 + 1.0 * 6.385782241821289
Epoch 390, val loss: 0.888417661190033
Epoch 400, training loss: 7.009091854095459 = 0.6330850720405579 + 1.0 * 6.376006603240967
Epoch 400, val loss: 0.8670201897621155
Epoch 410, training loss: 6.9773712158203125 = 0.6046913266181946 + 1.0 * 6.372679710388184
Epoch 410, val loss: 0.8464547395706177
Epoch 420, training loss: 6.947444915771484 = 0.5765000581741333 + 1.0 * 6.370944976806641
Epoch 420, val loss: 0.8263649344444275
Epoch 430, training loss: 6.921772003173828 = 0.5488249659538269 + 1.0 * 6.3729472160339355
Epoch 430, val loss: 0.8071715235710144
Epoch 440, training loss: 6.8872857093811035 = 0.5216517448425293 + 1.0 * 6.365633964538574
Epoch 440, val loss: 0.7889366149902344
Epoch 450, training loss: 6.857145309448242 = 0.4946964383125305 + 1.0 * 6.362448692321777
Epoch 450, val loss: 0.771450400352478
Epoch 460, training loss: 6.829151630401611 = 0.46791157126426697 + 1.0 * 6.361239910125732
Epoch 460, val loss: 0.7547170519828796
Epoch 470, training loss: 6.8047709465026855 = 0.44183549284935 + 1.0 * 6.362935543060303
Epoch 470, val loss: 0.7390289902687073
Epoch 480, training loss: 6.776243686676025 = 0.416552871465683 + 1.0 * 6.3596906661987305
Epoch 480, val loss: 0.7246472239494324
Epoch 490, training loss: 6.745096683502197 = 0.39191025495529175 + 1.0 * 6.35318660736084
Epoch 490, val loss: 0.7112012505531311
Epoch 500, training loss: 6.722945213317871 = 0.3678966760635376 + 1.0 * 6.355048656463623
Epoch 500, val loss: 0.6987805962562561
Epoch 510, training loss: 6.69969367980957 = 0.34481197595596313 + 1.0 * 6.354881763458252
Epoch 510, val loss: 0.687622606754303
Epoch 520, training loss: 6.672391891479492 = 0.3228330910205841 + 1.0 * 6.3495588302612305
Epoch 520, val loss: 0.6777124404907227
Epoch 530, training loss: 6.649526119232178 = 0.30187228322029114 + 1.0 * 6.347653865814209
Epoch 530, val loss: 0.6688419580459595
Epoch 540, training loss: 6.628645420074463 = 0.28199321031570435 + 1.0 * 6.346652030944824
Epoch 540, val loss: 0.6610450744628906
Epoch 550, training loss: 6.608133792877197 = 0.26333385705947876 + 1.0 * 6.344799995422363
Epoch 550, val loss: 0.654288649559021
Epoch 560, training loss: 6.588591575622559 = 0.24587580561637878 + 1.0 * 6.342715740203857
Epoch 560, val loss: 0.6486652493476868
Epoch 570, training loss: 6.569477558135986 = 0.2294810712337494 + 1.0 * 6.339996337890625
Epoch 570, val loss: 0.6439393162727356
Epoch 580, training loss: 6.558317184448242 = 0.21413016319274902 + 1.0 * 6.344186782836914
Epoch 580, val loss: 0.6400324106216431
Epoch 590, training loss: 6.537099361419678 = 0.19984857738018036 + 1.0 * 6.337250709533691
Epoch 590, val loss: 0.6369413137435913
Epoch 600, training loss: 6.527548789978027 = 0.18655350804328918 + 1.0 * 6.3409953117370605
Epoch 600, val loss: 0.634671688079834
Epoch 610, training loss: 6.511210918426514 = 0.1742638796567917 + 1.0 * 6.336946964263916
Epoch 610, val loss: 0.6332190036773682
Epoch 620, training loss: 6.494063854217529 = 0.16288749873638153 + 1.0 * 6.331176280975342
Epoch 620, val loss: 0.6324135065078735
Epoch 630, training loss: 6.484193801879883 = 0.15233135223388672 + 1.0 * 6.331862449645996
Epoch 630, val loss: 0.6322296261787415
Epoch 640, training loss: 6.473357677459717 = 0.1425924003124237 + 1.0 * 6.330765247344971
Epoch 640, val loss: 0.6326643228530884
Epoch 650, training loss: 6.4639129638671875 = 0.1336437612771988 + 1.0 * 6.3302693367004395
Epoch 650, val loss: 0.6336191892623901
Epoch 660, training loss: 6.4532790184021 = 0.12536486983299255 + 1.0 * 6.327914237976074
Epoch 660, val loss: 0.6350575685501099
Epoch 670, training loss: 6.445449352264404 = 0.11773405969142914 + 1.0 * 6.3277153968811035
Epoch 670, val loss: 0.6370205879211426
Epoch 680, training loss: 6.435579776763916 = 0.11071070283651352 + 1.0 * 6.324869155883789
Epoch 680, val loss: 0.6393139958381653
Epoch 690, training loss: 6.4334821701049805 = 0.10421161353588104 + 1.0 * 6.329270362854004
Epoch 690, val loss: 0.6420228481292725
Epoch 700, training loss: 6.421613693237305 = 0.0982188954949379 + 1.0 * 6.323394775390625
Epoch 700, val loss: 0.6450318694114685
Epoch 710, training loss: 6.414966106414795 = 0.0926661491394043 + 1.0 * 6.322299957275391
Epoch 710, val loss: 0.6483056545257568
Epoch 720, training loss: 6.407992839813232 = 0.0875263661146164 + 1.0 * 6.3204665184021
Epoch 720, val loss: 0.651856005191803
Epoch 730, training loss: 6.399293422698975 = 0.08276474475860596 + 1.0 * 6.316528797149658
Epoch 730, val loss: 0.6556125283241272
Epoch 740, training loss: 6.39796257019043 = 0.07833395898342133 + 1.0 * 6.319628715515137
Epoch 740, val loss: 0.6595611572265625
Epoch 750, training loss: 6.391037940979004 = 0.07423370331525803 + 1.0 * 6.3168044090271
Epoch 750, val loss: 0.663666307926178
Epoch 760, training loss: 6.385261058807373 = 0.07043097913265228 + 1.0 * 6.314830303192139
Epoch 760, val loss: 0.6679121255874634
Epoch 770, training loss: 6.381103038787842 = 0.06689215451478958 + 1.0 * 6.314210891723633
Epoch 770, val loss: 0.6722212433815002
Epoch 780, training loss: 6.377814769744873 = 0.06358978152275085 + 1.0 * 6.314225196838379
Epoch 780, val loss: 0.6766494512557983
Epoch 790, training loss: 6.370919704437256 = 0.060514792799949646 + 1.0 * 6.3104047775268555
Epoch 790, val loss: 0.6811150312423706
Epoch 800, training loss: 6.367805480957031 = 0.05764954909682274 + 1.0 * 6.310155868530273
Epoch 800, val loss: 0.6856337785720825
Epoch 810, training loss: 6.367514610290527 = 0.05495983362197876 + 1.0 * 6.312554836273193
Epoch 810, val loss: 0.6901870369911194
Epoch 820, training loss: 6.360287189483643 = 0.052455976605415344 + 1.0 * 6.307831287384033
Epoch 820, val loss: 0.6947978734970093
Epoch 830, training loss: 6.358698844909668 = 0.050114668905735016 + 1.0 * 6.308584213256836
Epoch 830, val loss: 0.6993796825408936
Epoch 840, training loss: 6.358999729156494 = 0.04791697487235069 + 1.0 * 6.31108283996582
Epoch 840, val loss: 0.7040349841117859
Epoch 850, training loss: 6.351378440856934 = 0.04585897922515869 + 1.0 * 6.3055195808410645
Epoch 850, val loss: 0.708685040473938
Epoch 860, training loss: 6.348677158355713 = 0.04392065107822418 + 1.0 * 6.3047566413879395
Epoch 860, val loss: 0.7133210897445679
Epoch 870, training loss: 6.348081588745117 = 0.04209766164422035 + 1.0 * 6.305984020233154
Epoch 870, val loss: 0.7179657816886902
Epoch 880, training loss: 6.344229698181152 = 0.040385980159044266 + 1.0 * 6.3038434982299805
Epoch 880, val loss: 0.7225792407989502
Epoch 890, training loss: 6.3421220779418945 = 0.038773637264966965 + 1.0 * 6.303348541259766
Epoch 890, val loss: 0.7271741628646851
Epoch 900, training loss: 6.337167263031006 = 0.03725309669971466 + 1.0 * 6.299914360046387
Epoch 900, val loss: 0.7317295670509338
Epoch 910, training loss: 6.336525917053223 = 0.035812512040138245 + 1.0 * 6.300713539123535
Epoch 910, val loss: 0.736258864402771
Epoch 920, training loss: 6.339399337768555 = 0.03445582091808319 + 1.0 * 6.304943561553955
Epoch 920, val loss: 0.7407807111740112
Epoch 930, training loss: 6.3313984870910645 = 0.03318090736865997 + 1.0 * 6.2982177734375
Epoch 930, val loss: 0.7452288866043091
Epoch 940, training loss: 6.331259727478027 = 0.0319756343960762 + 1.0 * 6.299283981323242
Epoch 940, val loss: 0.7496742606163025
Epoch 950, training loss: 6.328847408294678 = 0.030831292271614075 + 1.0 * 6.29801607131958
Epoch 950, val loss: 0.7540456056594849
Epoch 960, training loss: 6.325992107391357 = 0.029746906831860542 + 1.0 * 6.296245098114014
Epoch 960, val loss: 0.7584342956542969
Epoch 970, training loss: 6.326661109924316 = 0.028716102242469788 + 1.0 * 6.297945022583008
Epoch 970, val loss: 0.7627617716789246
Epoch 980, training loss: 6.322964668273926 = 0.02773880958557129 + 1.0 * 6.295226097106934
Epoch 980, val loss: 0.7670634984970093
Epoch 990, training loss: 6.321000576019287 = 0.02681325189769268 + 1.0 * 6.294187545776367
Epoch 990, val loss: 0.7713325023651123
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 10.552162170410156 = 1.955355167388916 + 1.0 * 8.596807479858398
Epoch 0, val loss: 1.955540657043457
Epoch 10, training loss: 10.541803359985352 = 1.9453125 + 1.0 * 8.596490859985352
Epoch 10, val loss: 1.9457465410232544
Epoch 20, training loss: 10.526862144470215 = 1.932717204093933 + 1.0 * 8.594144821166992
Epoch 20, val loss: 1.932973027229309
Epoch 30, training loss: 10.490592002868652 = 1.914642572402954 + 1.0 * 8.575949668884277
Epoch 30, val loss: 1.914358377456665
Epoch 40, training loss: 10.341087341308594 = 1.890491247177124 + 1.0 * 8.45059585571289
Epoch 40, val loss: 1.890152096748352
Epoch 50, training loss: 9.857264518737793 = 1.8655016422271729 + 1.0 * 7.991763114929199
Epoch 50, val loss: 1.8659021854400635
Epoch 60, training loss: 9.318338394165039 = 1.8457934856414795 + 1.0 * 7.472545146942139
Epoch 60, val loss: 1.8472816944122314
Epoch 70, training loss: 8.902709007263184 = 1.8333733081817627 + 1.0 * 7.0693359375
Epoch 70, val loss: 1.8351318836212158
Epoch 80, training loss: 8.721944808959961 = 1.8200352191925049 + 1.0 * 6.901909351348877
Epoch 80, val loss: 1.8219032287597656
Epoch 90, training loss: 8.616156578063965 = 1.802182674407959 + 1.0 * 6.813973903656006
Epoch 90, val loss: 1.8045908212661743
Epoch 100, training loss: 8.526556015014648 = 1.783551812171936 + 1.0 * 6.743004322052002
Epoch 100, val loss: 1.7869529724121094
Epoch 110, training loss: 8.454110145568848 = 1.766417384147644 + 1.0 * 6.687693119049072
Epoch 110, val loss: 1.770668387413025
Epoch 120, training loss: 8.390228271484375 = 1.7495965957641602 + 1.0 * 6.640632152557373
Epoch 120, val loss: 1.7545477151870728
Epoch 130, training loss: 8.338630676269531 = 1.7315431833267212 + 1.0 * 6.607087135314941
Epoch 130, val loss: 1.73757004737854
Epoch 140, training loss: 8.287078857421875 = 1.711517333984375 + 1.0 * 6.5755615234375
Epoch 140, val loss: 1.7191253900527954
Epoch 150, training loss: 8.240370750427246 = 1.6886721849441528 + 1.0 * 6.551698684692383
Epoch 150, val loss: 1.6986355781555176
Epoch 160, training loss: 8.193107604980469 = 1.6624531745910645 + 1.0 * 6.5306549072265625
Epoch 160, val loss: 1.67557954788208
Epoch 170, training loss: 8.14488697052002 = 1.6321169137954712 + 1.0 * 6.512770175933838
Epoch 170, val loss: 1.649260401725769
Epoch 180, training loss: 8.095244407653809 = 1.5973759889602661 + 1.0 * 6.497868061065674
Epoch 180, val loss: 1.6195505857467651
Epoch 190, training loss: 8.044513702392578 = 1.5585161447525024 + 1.0 * 6.485997676849365
Epoch 190, val loss: 1.586814284324646
Epoch 200, training loss: 7.990033149719238 = 1.5157794952392578 + 1.0 * 6.4742536544799805
Epoch 200, val loss: 1.5514590740203857
Epoch 210, training loss: 7.933860778808594 = 1.469428300857544 + 1.0 * 6.464432716369629
Epoch 210, val loss: 1.5138603448867798
Epoch 220, training loss: 7.8782877922058105 = 1.4206624031066895 + 1.0 * 6.457625389099121
Epoch 220, val loss: 1.4750385284423828
Epoch 230, training loss: 7.818390846252441 = 1.3704602718353271 + 1.0 * 6.447930335998535
Epoch 230, val loss: 1.4359720945358276
Epoch 240, training loss: 7.76275634765625 = 1.3192994594573975 + 1.0 * 6.443456649780273
Epoch 240, val loss: 1.396932601928711
Epoch 250, training loss: 7.70330810546875 = 1.268622636795044 + 1.0 * 6.434685707092285
Epoch 250, val loss: 1.3589146137237549
Epoch 260, training loss: 7.6464056968688965 = 1.2187212705612183 + 1.0 * 6.427684307098389
Epoch 260, val loss: 1.3217995166778564
Epoch 270, training loss: 7.594808578491211 = 1.1699519157409668 + 1.0 * 6.424856662750244
Epoch 270, val loss: 1.285646677017212
Epoch 280, training loss: 7.540037631988525 = 1.1228564977645874 + 1.0 * 6.417181015014648
Epoch 280, val loss: 1.2507096529006958
Epoch 290, training loss: 7.492218017578125 = 1.0777161121368408 + 1.0 * 6.414501667022705
Epoch 290, val loss: 1.2172644138336182
Epoch 300, training loss: 7.444092273712158 = 1.0349534749984741 + 1.0 * 6.4091386795043945
Epoch 300, val loss: 1.1852573156356812
Epoch 310, training loss: 7.397008895874023 = 0.9941596984863281 + 1.0 * 6.402849197387695
Epoch 310, val loss: 1.154725432395935
Epoch 320, training loss: 7.355992794036865 = 0.9549938440322876 + 1.0 * 6.400999069213867
Epoch 320, val loss: 1.1253694295883179
Epoch 330, training loss: 7.314762115478516 = 0.9175484776496887 + 1.0 * 6.397213459014893
Epoch 330, val loss: 1.0973960161209106
Epoch 340, training loss: 7.275142669677734 = 0.8817790746688843 + 1.0 * 6.3933634757995605
Epoch 340, val loss: 1.070852518081665
Epoch 350, training loss: 7.236445903778076 = 0.8477262258529663 + 1.0 * 6.38871955871582
Epoch 350, val loss: 1.0457894802093506
Epoch 360, training loss: 7.198966979980469 = 0.8151074647903442 + 1.0 * 6.383859634399414
Epoch 360, val loss: 1.0222809314727783
Epoch 370, training loss: 7.16447639465332 = 0.7837485671043396 + 1.0 * 6.380727767944336
Epoch 370, val loss: 1.000124454498291
Epoch 380, training loss: 7.13446569442749 = 0.7538592219352722 + 1.0 * 6.380606651306152
Epoch 380, val loss: 0.979378342628479
Epoch 390, training loss: 7.100026607513428 = 0.7254255414009094 + 1.0 * 6.374600887298584
Epoch 390, val loss: 0.9604654908180237
Epoch 400, training loss: 7.0687432289123535 = 0.6981768608093262 + 1.0 * 6.370566368103027
Epoch 400, val loss: 0.9429433345794678
Epoch 410, training loss: 7.0562663078308105 = 0.6720079779624939 + 1.0 * 6.384258270263672
Epoch 410, val loss: 0.9267097115516663
Epoch 420, training loss: 7.015961170196533 = 0.6471962928771973 + 1.0 * 6.368764877319336
Epoch 420, val loss: 0.9119904041290283
Epoch 430, training loss: 6.986257553100586 = 0.6233280897140503 + 1.0 * 6.362929344177246
Epoch 430, val loss: 0.8984723091125488
Epoch 440, training loss: 6.9622297286987305 = 0.6001600027084351 + 1.0 * 6.362069606781006
Epoch 440, val loss: 0.8858786821365356
Epoch 450, training loss: 6.941003322601318 = 0.5777796506881714 + 1.0 * 6.363223552703857
Epoch 450, val loss: 0.8742836117744446
Epoch 460, training loss: 6.912927150726318 = 0.5562563538551331 + 1.0 * 6.35667085647583
Epoch 460, val loss: 0.8637920022010803
Epoch 470, training loss: 6.889749050140381 = 0.5353708863258362 + 1.0 * 6.3543782234191895
Epoch 470, val loss: 0.8540704846382141
Epoch 480, training loss: 6.873536109924316 = 0.5151164531707764 + 1.0 * 6.358419895172119
Epoch 480, val loss: 0.8451657891273499
Epoch 490, training loss: 6.848066329956055 = 0.49558883905410767 + 1.0 * 6.352477550506592
Epoch 490, val loss: 0.8372281789779663
Epoch 500, training loss: 6.82475471496582 = 0.4767099618911743 + 1.0 * 6.3480448722839355
Epoch 500, val loss: 0.8300732374191284
Epoch 510, training loss: 6.80425500869751 = 0.4583401083946228 + 1.0 * 6.345914840698242
Epoch 510, val loss: 0.8235326409339905
Epoch 520, training loss: 6.7846503257751465 = 0.4405454397201538 + 1.0 * 6.344104766845703
Epoch 520, val loss: 0.8175588250160217
Epoch 530, training loss: 6.765573501586914 = 0.4233868718147278 + 1.0 * 6.342186450958252
Epoch 530, val loss: 0.8123884201049805
Epoch 540, training loss: 6.74727201461792 = 0.4066566824913025 + 1.0 * 6.340615272521973
Epoch 540, val loss: 0.8076255917549133
Epoch 550, training loss: 6.734459400177002 = 0.39034387469291687 + 1.0 * 6.344115734100342
Epoch 550, val loss: 0.8032471537590027
Epoch 560, training loss: 6.714359283447266 = 0.37450385093688965 + 1.0 * 6.339855194091797
Epoch 560, val loss: 0.7993671894073486
Epoch 570, training loss: 6.6956024169921875 = 0.35904431343078613 + 1.0 * 6.336557865142822
Epoch 570, val loss: 0.7958750128746033
Epoch 580, training loss: 6.676810264587402 = 0.3438820242881775 + 1.0 * 6.33292818069458
Epoch 580, val loss: 0.7926934361457825
Epoch 590, training loss: 6.665316104888916 = 0.32896187901496887 + 1.0 * 6.3363542556762695
Epoch 590, val loss: 0.7898486256599426
Epoch 600, training loss: 6.650667667388916 = 0.3143719732761383 + 1.0 * 6.3362956047058105
Epoch 600, val loss: 0.7872671484947205
Epoch 610, training loss: 6.635366916656494 = 0.3001191318035126 + 1.0 * 6.335247993469238
Epoch 610, val loss: 0.7850528359413147
Epoch 620, training loss: 6.6149492263793945 = 0.2862285375595093 + 1.0 * 6.328720569610596
Epoch 620, val loss: 0.7830384373664856
Epoch 630, training loss: 6.599087715148926 = 0.2726094424724579 + 1.0 * 6.326478481292725
Epoch 630, val loss: 0.7813453674316406
Epoch 640, training loss: 6.585521697998047 = 0.2592456638813019 + 1.0 * 6.326275825500488
Epoch 640, val loss: 0.779994547367096
Epoch 650, training loss: 6.581499099731445 = 0.24623830616474152 + 1.0 * 6.33526086807251
Epoch 650, val loss: 0.7789178490638733
Epoch 660, training loss: 6.557217121124268 = 0.23375920951366425 + 1.0 * 6.323457717895508
Epoch 660, val loss: 0.7780876159667969
Epoch 670, training loss: 6.5435791015625 = 0.22165270149707794 + 1.0 * 6.321926593780518
Epoch 670, val loss: 0.7776214480400085
Epoch 680, training loss: 6.530627727508545 = 0.20991390943527222 + 1.0 * 6.320713996887207
Epoch 680, val loss: 0.777556300163269
Epoch 690, training loss: 6.527379512786865 = 0.19857507944107056 + 1.0 * 6.3288044929504395
Epoch 690, val loss: 0.7779524326324463
Epoch 700, training loss: 6.512351989746094 = 0.1877596527338028 + 1.0 * 6.324592113494873
Epoch 700, val loss: 0.7786732912063599
Epoch 710, training loss: 6.494765758514404 = 0.17741772532463074 + 1.0 * 6.317348003387451
Epoch 710, val loss: 0.7798886299133301
Epoch 720, training loss: 6.4849019050598145 = 0.16754673421382904 + 1.0 * 6.317355155944824
Epoch 720, val loss: 0.7815645933151245
Epoch 730, training loss: 6.483246326446533 = 0.15820850431919098 + 1.0 * 6.325037956237793
Epoch 730, val loss: 0.7836341857910156
Epoch 740, training loss: 6.465496063232422 = 0.14942744374275208 + 1.0 * 6.316068649291992
Epoch 740, val loss: 0.7862097024917603
Epoch 750, training loss: 6.454653263092041 = 0.1411384493112564 + 1.0 * 6.313514709472656
Epoch 750, val loss: 0.7891910672187805
Epoch 760, training loss: 6.445981979370117 = 0.13330323994159698 + 1.0 * 6.312678813934326
Epoch 760, val loss: 0.7926530838012695
Epoch 770, training loss: 6.4403181076049805 = 0.12591570615768433 + 1.0 * 6.3144025802612305
Epoch 770, val loss: 0.7965273261070251
Epoch 780, training loss: 6.433445930480957 = 0.11900809407234192 + 1.0 * 6.3144378662109375
Epoch 780, val loss: 0.8007113337516785
Epoch 790, training loss: 6.424519062042236 = 0.1125572919845581 + 1.0 * 6.311961650848389
Epoch 790, val loss: 0.8052367568016052
Epoch 800, training loss: 6.416604042053223 = 0.10649850964546204 + 1.0 * 6.310105323791504
Epoch 800, val loss: 0.810124933719635
Epoch 810, training loss: 6.417483329772949 = 0.10082033276557922 + 1.0 * 6.316662788391113
Epoch 810, val loss: 0.8152591586112976
Epoch 820, training loss: 6.403827667236328 = 0.09551370143890381 + 1.0 * 6.308313846588135
Epoch 820, val loss: 0.8206140995025635
Epoch 830, training loss: 6.396701812744141 = 0.09053602814674377 + 1.0 * 6.30616569519043
Epoch 830, val loss: 0.8262050151824951
Epoch 840, training loss: 6.391966342926025 = 0.08585671335458755 + 1.0 * 6.306109428405762
Epoch 840, val loss: 0.8320072889328003
Epoch 850, training loss: 6.396038055419922 = 0.08146116882562637 + 1.0 * 6.314577102661133
Epoch 850, val loss: 0.8379979729652405
Epoch 860, training loss: 6.3852081298828125 = 0.07736582309007645 + 1.0 * 6.307842254638672
Epoch 860, val loss: 0.8440837264060974
Epoch 870, training loss: 6.376204490661621 = 0.07352625578641891 + 1.0 * 6.302678108215332
Epoch 870, val loss: 0.8502519130706787
Epoch 880, training loss: 6.378507614135742 = 0.0699276477098465 + 1.0 * 6.308579921722412
Epoch 880, val loss: 0.8565323352813721
Epoch 890, training loss: 6.371859073638916 = 0.06656498461961746 + 1.0 * 6.305294036865234
Epoch 890, val loss: 0.8628751635551453
Epoch 900, training loss: 6.368132591247559 = 0.0634167492389679 + 1.0 * 6.304715633392334
Epoch 900, val loss: 0.8692196607589722
Epoch 910, training loss: 6.361333847045898 = 0.06046626716852188 + 1.0 * 6.300867557525635
Epoch 910, val loss: 0.8756684064865112
Epoch 920, training loss: 6.356377124786377 = 0.057695288211107254 + 1.0 * 6.298681735992432
Epoch 920, val loss: 0.8821392059326172
Epoch 930, training loss: 6.3640971183776855 = 0.05509485676884651 + 1.0 * 6.30900239944458
Epoch 930, val loss: 0.888654887676239
Epoch 940, training loss: 6.351207256317139 = 0.052658505737781525 + 1.0 * 6.298548698425293
Epoch 940, val loss: 0.8950978517532349
Epoch 950, training loss: 6.347841262817383 = 0.05036760866641998 + 1.0 * 6.297473430633545
Epoch 950, val loss: 0.9015917181968689
Epoch 960, training loss: 6.346429347991943 = 0.04821441322565079 + 1.0 * 6.298214912414551
Epoch 960, val loss: 0.9080883860588074
Epoch 970, training loss: 6.342262268066406 = 0.04618895426392555 + 1.0 * 6.2960734367370605
Epoch 970, val loss: 0.9145408272743225
Epoch 980, training loss: 6.33962869644165 = 0.04428064450621605 + 1.0 * 6.295348167419434
Epoch 980, val loss: 0.9210250973701477
Epoch 990, training loss: 6.343879222869873 = 0.04248076677322388 + 1.0 * 6.301398277282715
Epoch 990, val loss: 0.9274570941925049
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8365840801265156
The final CL Acc:0.81235, 0.01429, The final GNN Acc:0.83658, 0.00086
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9560])
updated graph: torch.Size([2, 10612])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.543484687805176 = 1.946608543395996 + 1.0 * 8.59687614440918
Epoch 0, val loss: 1.9460333585739136
Epoch 10, training loss: 10.533227920532227 = 1.9364961385726929 + 1.0 * 8.596732139587402
Epoch 10, val loss: 1.9356542825698853
Epoch 20, training loss: 10.520024299621582 = 1.9243297576904297 + 1.0 * 8.595694541931152
Epoch 20, val loss: 1.922937273979187
Epoch 30, training loss: 10.49415111541748 = 1.9076951742172241 + 1.0 * 8.586456298828125
Epoch 30, val loss: 1.905436396598816
Epoch 40, training loss: 10.398189544677734 = 1.8853068351745605 + 1.0 * 8.512882232666016
Epoch 40, val loss: 1.8825794458389282
Epoch 50, training loss: 9.946454048156738 = 1.8614494800567627 + 1.0 * 8.085004806518555
Epoch 50, val loss: 1.859288215637207
Epoch 60, training loss: 9.438058853149414 = 1.844760537147522 + 1.0 * 7.593298435211182
Epoch 60, val loss: 1.8440226316452026
Epoch 70, training loss: 9.130644798278809 = 1.830800175666809 + 1.0 * 7.299844264984131
Epoch 70, val loss: 1.830585241317749
Epoch 80, training loss: 8.967358589172363 = 1.81484854221344 + 1.0 * 7.152509689331055
Epoch 80, val loss: 1.8154226541519165
Epoch 90, training loss: 8.806236267089844 = 1.7987017631530762 + 1.0 * 7.007534503936768
Epoch 90, val loss: 1.8014172315597534
Epoch 100, training loss: 8.694814682006836 = 1.784058690071106 + 1.0 * 6.9107561111450195
Epoch 100, val loss: 1.7891114950180054
Epoch 110, training loss: 8.607596397399902 = 1.7695375680923462 + 1.0 * 6.8380584716796875
Epoch 110, val loss: 1.7764540910720825
Epoch 120, training loss: 8.536474227905273 = 1.7541757822036743 + 1.0 * 6.782298564910889
Epoch 120, val loss: 1.7628493309020996
Epoch 130, training loss: 8.472293853759766 = 1.7375380992889404 + 1.0 * 6.734755516052246
Epoch 130, val loss: 1.7481029033660889
Epoch 140, training loss: 8.411243438720703 = 1.718875527381897 + 1.0 * 6.692368030548096
Epoch 140, val loss: 1.7316515445709229
Epoch 150, training loss: 8.356073379516602 = 1.69765305519104 + 1.0 * 6.658420085906982
Epoch 150, val loss: 1.7131088972091675
Epoch 160, training loss: 8.301101684570312 = 1.673387885093689 + 1.0 * 6.627713680267334
Epoch 160, val loss: 1.691877007484436
Epoch 170, training loss: 8.251042366027832 = 1.6454073190689087 + 1.0 * 6.605634689331055
Epoch 170, val loss: 1.6675670146942139
Epoch 180, training loss: 8.199503898620605 = 1.6138782501220703 + 1.0 * 6.585625648498535
Epoch 180, val loss: 1.6401143074035645
Epoch 190, training loss: 8.14558219909668 = 1.578355312347412 + 1.0 * 6.567226886749268
Epoch 190, val loss: 1.6091588735580444
Epoch 200, training loss: 8.090843200683594 = 1.5384759902954102 + 1.0 * 6.552367687225342
Epoch 200, val loss: 1.5743860006332397
Epoch 210, training loss: 8.043216705322266 = 1.494486927986145 + 1.0 * 6.54872989654541
Epoch 210, val loss: 1.5360677242279053
Epoch 220, training loss: 7.977724075317383 = 1.4477137327194214 + 1.0 * 6.530010223388672
Epoch 220, val loss: 1.4956790208816528
Epoch 230, training loss: 7.919434547424316 = 1.3984466791152954 + 1.0 * 6.5209879875183105
Epoch 230, val loss: 1.4532939195632935
Epoch 240, training loss: 7.859196186065674 = 1.3470878601074219 + 1.0 * 6.512108325958252
Epoch 240, val loss: 1.409423589706421
Epoch 250, training loss: 7.7996826171875 = 1.2944633960723877 + 1.0 * 6.505218982696533
Epoch 250, val loss: 1.364825963973999
Epoch 260, training loss: 7.739936351776123 = 1.241761326789856 + 1.0 * 6.498175144195557
Epoch 260, val loss: 1.3207569122314453
Epoch 270, training loss: 7.680831432342529 = 1.189234733581543 + 1.0 * 6.491596698760986
Epoch 270, val loss: 1.2772971391677856
Epoch 280, training loss: 7.627591133117676 = 1.1372990608215332 + 1.0 * 6.490292072296143
Epoch 280, val loss: 1.2347116470336914
Epoch 290, training loss: 7.568653583526611 = 1.0869179964065552 + 1.0 * 6.481735706329346
Epoch 290, val loss: 1.1939184665679932
Epoch 300, training loss: 7.512946128845215 = 1.0382969379425049 + 1.0 * 6.474649429321289
Epoch 300, val loss: 1.1550220251083374
Epoch 310, training loss: 7.462162971496582 = 0.9915018081665039 + 1.0 * 6.470661163330078
Epoch 310, val loss: 1.1178507804870605
Epoch 320, training loss: 7.412409782409668 = 0.9467872977256775 + 1.0 * 6.465622425079346
Epoch 320, val loss: 1.0828356742858887
Epoch 330, training loss: 7.366794109344482 = 0.9044269919395447 + 1.0 * 6.462367057800293
Epoch 330, val loss: 1.0498369932174683
Epoch 340, training loss: 7.317735195159912 = 0.8645253777503967 + 1.0 * 6.45320987701416
Epoch 340, val loss: 1.0191899538040161
Epoch 350, training loss: 7.276034832000732 = 0.8269055485725403 + 1.0 * 6.449129104614258
Epoch 350, val loss: 0.9905520677566528
Epoch 360, training loss: 7.238916873931885 = 0.7915835380554199 + 1.0 * 6.447333335876465
Epoch 360, val loss: 0.963973343372345
Epoch 370, training loss: 7.199122428894043 = 0.7585391402244568 + 1.0 * 6.440583229064941
Epoch 370, val loss: 0.9396339654922485
Epoch 380, training loss: 7.1675825119018555 = 0.7273996472358704 + 1.0 * 6.440182685852051
Epoch 380, val loss: 0.9171311259269714
Epoch 390, training loss: 7.131377696990967 = 0.6980218887329102 + 1.0 * 6.433355808258057
Epoch 390, val loss: 0.8963717818260193
Epoch 400, training loss: 7.097314834594727 = 0.6700459122657776 + 1.0 * 6.427268981933594
Epoch 400, val loss: 0.8771433234214783
Epoch 410, training loss: 7.072058200836182 = 0.6430262327194214 + 1.0 * 6.429031848907471
Epoch 410, val loss: 0.859108030796051
Epoch 420, training loss: 7.037735939025879 = 0.6168994307518005 + 1.0 * 6.420836448669434
Epoch 420, val loss: 0.8423120379447937
Epoch 430, training loss: 7.012795925140381 = 0.5914368033409119 + 1.0 * 6.421359062194824
Epoch 430, val loss: 0.8264950513839722
Epoch 440, training loss: 6.978952884674072 = 0.5665234923362732 + 1.0 * 6.412429332733154
Epoch 440, val loss: 0.8118178248405457
Epoch 450, training loss: 6.951318264007568 = 0.5420499444007874 + 1.0 * 6.409268379211426
Epoch 450, val loss: 0.7979903817176819
Epoch 460, training loss: 6.936069011688232 = 0.5179497003555298 + 1.0 * 6.418119430541992
Epoch 460, val loss: 0.7851353287696838
Epoch 470, training loss: 6.89865779876709 = 0.4944590628147125 + 1.0 * 6.40419864654541
Epoch 470, val loss: 0.7733502388000488
Epoch 480, training loss: 6.875857353210449 = 0.47142839431762695 + 1.0 * 6.404428958892822
Epoch 480, val loss: 0.7626028656959534
Epoch 490, training loss: 6.851072311401367 = 0.4489758312702179 + 1.0 * 6.402096271514893
Epoch 490, val loss: 0.7528840899467468
Epoch 500, training loss: 6.822634220123291 = 0.4270937144756317 + 1.0 * 6.395540714263916
Epoch 500, val loss: 0.7443277835845947
Epoch 510, training loss: 6.798195838928223 = 0.405684232711792 + 1.0 * 6.39251184463501
Epoch 510, val loss: 0.7367146611213684
Epoch 520, training loss: 6.789392471313477 = 0.38472533226013184 + 1.0 * 6.404667377471924
Epoch 520, val loss: 0.7300527691841125
Epoch 530, training loss: 6.7524261474609375 = 0.36430859565734863 + 1.0 * 6.388117790222168
Epoch 530, val loss: 0.7245025038719177
Epoch 540, training loss: 6.73303747177124 = 0.34446650743484497 + 1.0 * 6.388570785522461
Epoch 540, val loss: 0.7198412418365479
Epoch 550, training loss: 6.709691047668457 = 0.32519659399986267 + 1.0 * 6.384494304656982
Epoch 550, val loss: 0.7160694599151611
Epoch 560, training loss: 6.68916130065918 = 0.3065752387046814 + 1.0 * 6.3825860023498535
Epoch 560, val loss: 0.713228166103363
Epoch 570, training loss: 6.670889854431152 = 0.28856444358825684 + 1.0 * 6.382325649261475
Epoch 570, val loss: 0.7112746834754944
Epoch 580, training loss: 6.65659761428833 = 0.2712712287902832 + 1.0 * 6.385326385498047
Epoch 580, val loss: 0.7102678418159485
Epoch 590, training loss: 6.631220817565918 = 0.2548601031303406 + 1.0 * 6.376360893249512
Epoch 590, val loss: 0.7101567387580872
Epoch 600, training loss: 6.6145243644714355 = 0.23924027383327484 + 1.0 * 6.375284194946289
Epoch 600, val loss: 0.7108909487724304
Epoch 610, training loss: 6.600268363952637 = 0.2244279384613037 + 1.0 * 6.375840663909912
Epoch 610, val loss: 0.7125365138053894
Epoch 620, training loss: 6.581330299377441 = 0.21050524711608887 + 1.0 * 6.370825290679932
Epoch 620, val loss: 0.7149956226348877
Epoch 630, training loss: 6.566596508026123 = 0.1974399834871292 + 1.0 * 6.369156360626221
Epoch 630, val loss: 0.7183029055595398
Epoch 640, training loss: 6.555567264556885 = 0.1851881891489029 + 1.0 * 6.3703789710998535
Epoch 640, val loss: 0.7222630977630615
Epoch 650, training loss: 6.542175769805908 = 0.17371836304664612 + 1.0 * 6.368457317352295
Epoch 650, val loss: 0.7269559502601624
Epoch 660, training loss: 6.532137870788574 = 0.16310210525989532 + 1.0 * 6.369035720825195
Epoch 660, val loss: 0.7321808338165283
Epoch 670, training loss: 6.516127586364746 = 0.15321913361549377 + 1.0 * 6.362908363342285
Epoch 670, val loss: 0.7379553914070129
Epoch 680, training loss: 6.5051960945129395 = 0.14400677382946014 + 1.0 * 6.361189365386963
Epoch 680, val loss: 0.7441917061805725
Epoch 690, training loss: 6.5010480880737305 = 0.13542893528938293 + 1.0 * 6.36561918258667
Epoch 690, val loss: 0.7508312463760376
Epoch 700, training loss: 6.486738204956055 = 0.12747658789157867 + 1.0 * 6.359261512756348
Epoch 700, val loss: 0.7577837109565735
Epoch 710, training loss: 6.4790472984313965 = 0.12007475644350052 + 1.0 * 6.358972549438477
Epoch 710, val loss: 0.7650706768035889
Epoch 720, training loss: 6.471441268920898 = 0.1132066622376442 + 1.0 * 6.358234405517578
Epoch 720, val loss: 0.7725670337677002
Epoch 730, training loss: 6.460132598876953 = 0.10684456676244736 + 1.0 * 6.353288173675537
Epoch 730, val loss: 0.7802467942237854
Epoch 740, training loss: 6.45302152633667 = 0.1009237989783287 + 1.0 * 6.352097511291504
Epoch 740, val loss: 0.7880924344062805
Epoch 750, training loss: 6.44951057434082 = 0.09539789706468582 + 1.0 * 6.35411262512207
Epoch 750, val loss: 0.7960994243621826
Epoch 760, training loss: 6.439103603363037 = 0.09026742726564407 + 1.0 * 6.3488359451293945
Epoch 760, val loss: 0.8042446374893188
Epoch 770, training loss: 6.442630767822266 = 0.08548462390899658 + 1.0 * 6.357146263122559
Epoch 770, val loss: 0.8124417066574097
Epoch 780, training loss: 6.431506156921387 = 0.08104012906551361 + 1.0 * 6.350466251373291
Epoch 780, val loss: 0.8207057118415833
Epoch 790, training loss: 6.422322750091553 = 0.07690625637769699 + 1.0 * 6.34541654586792
Epoch 790, val loss: 0.8289734721183777
Epoch 800, training loss: 6.417033672332764 = 0.07303981482982635 + 1.0 * 6.343993663787842
Epoch 800, val loss: 0.8373257517814636
Epoch 810, training loss: 6.418841361999512 = 0.06942111998796463 + 1.0 * 6.349420070648193
Epoch 810, val loss: 0.8456606268882751
Epoch 820, training loss: 6.408637046813965 = 0.06605620682239532 + 1.0 * 6.342580795288086
Epoch 820, val loss: 0.8539573550224304
Epoch 830, training loss: 6.404990196228027 = 0.06290719658136368 + 1.0 * 6.342082977294922
Epoch 830, val loss: 0.8622775673866272
Epoch 840, training loss: 6.398162841796875 = 0.059954334050416946 + 1.0 * 6.3382086753845215
Epoch 840, val loss: 0.8705602288246155
Epoch 850, training loss: 6.399734973907471 = 0.057185348123311996 + 1.0 * 6.342549800872803
Epoch 850, val loss: 0.8788340091705322
Epoch 860, training loss: 6.392287731170654 = 0.05458644777536392 + 1.0 * 6.337701320648193
Epoch 860, val loss: 0.8870289325714111
Epoch 870, training loss: 6.38946008682251 = 0.05215972661972046 + 1.0 * 6.3373003005981445
Epoch 870, val loss: 0.8951640129089355
Epoch 880, training loss: 6.399529933929443 = 0.049886368215084076 + 1.0 * 6.349643707275391
Epoch 880, val loss: 0.9032291173934937
Epoch 890, training loss: 6.3818464279174805 = 0.04775254428386688 + 1.0 * 6.334094047546387
Epoch 890, val loss: 0.911130964756012
Epoch 900, training loss: 6.376804828643799 = 0.04575883224606514 + 1.0 * 6.331046104431152
Epoch 900, val loss: 0.9190037250518799
Epoch 910, training loss: 6.373786926269531 = 0.04387576878070831 + 1.0 * 6.329911231994629
Epoch 910, val loss: 0.9268035888671875
Epoch 920, training loss: 6.373522758483887 = 0.042097847908735275 + 1.0 * 6.331424713134766
Epoch 920, val loss: 0.9345397353172302
Epoch 930, training loss: 6.372509956359863 = 0.04042084515094757 + 1.0 * 6.332088947296143
Epoch 930, val loss: 0.9420939087867737
Epoch 940, training loss: 6.369574069976807 = 0.03885572776198387 + 1.0 * 6.330718517303467
Epoch 940, val loss: 0.9495617747306824
Epoch 950, training loss: 6.3647356033325195 = 0.037379831075668335 + 1.0 * 6.327355861663818
Epoch 950, val loss: 0.9569522142410278
Epoch 960, training loss: 6.36329460144043 = 0.035982947796583176 + 1.0 * 6.3273115158081055
Epoch 960, val loss: 0.9642292261123657
Epoch 970, training loss: 6.362752437591553 = 0.03466217964887619 + 1.0 * 6.328090190887451
Epoch 970, val loss: 0.9713993668556213
Epoch 980, training loss: 6.356948375701904 = 0.033419169485569 + 1.0 * 6.323529243469238
Epoch 980, val loss: 0.9784258604049683
Epoch 990, training loss: 6.354095935821533 = 0.03224413841962814 + 1.0 * 6.32185173034668
Epoch 990, val loss: 0.9853757619857788
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 10.53522777557373 = 1.938363790512085 + 1.0 * 8.596863746643066
Epoch 0, val loss: 1.9306819438934326
Epoch 10, training loss: 10.525360107421875 = 1.9286677837371826 + 1.0 * 8.596692085266113
Epoch 10, val loss: 1.9221030473709106
Epoch 20, training loss: 10.512323379516602 = 1.916885256767273 + 1.0 * 8.595438003540039
Epoch 20, val loss: 1.9114198684692383
Epoch 30, training loss: 10.485051155090332 = 1.9005849361419678 + 1.0 * 8.584465980529785
Epoch 30, val loss: 1.8966374397277832
Epoch 40, training loss: 10.394266128540039 = 1.8784887790679932 + 1.0 * 8.515777587890625
Epoch 40, val loss: 1.8770456314086914
Epoch 50, training loss: 10.068443298339844 = 1.8548800945281982 + 1.0 * 8.213562965393066
Epoch 50, val loss: 1.8563004732131958
Epoch 60, training loss: 9.593388557434082 = 1.8359959125518799 + 1.0 * 7.757392406463623
Epoch 60, val loss: 1.8389712572097778
Epoch 70, training loss: 9.09780502319336 = 1.8203877210617065 + 1.0 * 7.2774176597595215
Epoch 70, val loss: 1.8238581418991089
Epoch 80, training loss: 8.879096984863281 = 1.806409239768982 + 1.0 * 7.072688102722168
Epoch 80, val loss: 1.8109146356582642
Epoch 90, training loss: 8.71956729888916 = 1.7921112775802612 + 1.0 * 6.927455902099609
Epoch 90, val loss: 1.7984352111816406
Epoch 100, training loss: 8.608563423156738 = 1.7770113945007324 + 1.0 * 6.831552028656006
Epoch 100, val loss: 1.7858521938323975
Epoch 110, training loss: 8.526671409606934 = 1.761462688446045 + 1.0 * 6.765208721160889
Epoch 110, val loss: 1.7725530862808228
Epoch 120, training loss: 8.454151153564453 = 1.7450425624847412 + 1.0 * 6.709108352661133
Epoch 120, val loss: 1.7580726146697998
Epoch 130, training loss: 8.394099235534668 = 1.7268060445785522 + 1.0 * 6.667293548583984
Epoch 130, val loss: 1.741947889328003
Epoch 140, training loss: 8.343274116516113 = 1.705954670906067 + 1.0 * 6.637319564819336
Epoch 140, val loss: 1.7239538431167603
Epoch 150, training loss: 8.293391227722168 = 1.6817853450775146 + 1.0 * 6.611605644226074
Epoch 150, val loss: 1.703347086906433
Epoch 160, training loss: 8.24499225616455 = 1.653559684753418 + 1.0 * 6.591432571411133
Epoch 160, val loss: 1.679295539855957
Epoch 170, training loss: 8.196671485900879 = 1.620887041091919 + 1.0 * 6.575784206390381
Epoch 170, val loss: 1.651598334312439
Epoch 180, training loss: 8.144402503967285 = 1.5837301015853882 + 1.0 * 6.560672283172607
Epoch 180, val loss: 1.620115876197815
Epoch 190, training loss: 8.088606834411621 = 1.5416158437728882 + 1.0 * 6.546990871429443
Epoch 190, val loss: 1.5843368768692017
Epoch 200, training loss: 8.033690452575684 = 1.4951965808868408 + 1.0 * 6.538494110107422
Epoch 200, val loss: 1.5448623895645142
Epoch 210, training loss: 7.971050262451172 = 1.4462440013885498 + 1.0 * 6.524806499481201
Epoch 210, val loss: 1.5036282539367676
Epoch 220, training loss: 7.910994529724121 = 1.395662546157837 + 1.0 * 6.515332221984863
Epoch 220, val loss: 1.4612666368484497
Epoch 230, training loss: 7.855715274810791 = 1.3448162078857422 + 1.0 * 6.510899066925049
Epoch 230, val loss: 1.4191648960113525
Epoch 240, training loss: 7.794442176818848 = 1.295637845993042 + 1.0 * 6.498804569244385
Epoch 240, val loss: 1.3792375326156616
Epoch 250, training loss: 7.738666534423828 = 1.2482445240020752 + 1.0 * 6.490421772003174
Epoch 250, val loss: 1.3417414426803589
Epoch 260, training loss: 7.685166358947754 = 1.2027403116226196 + 1.0 * 6.482426166534424
Epoch 260, val loss: 1.3067046403884888
Epoch 270, training loss: 7.6485395431518555 = 1.1597521305084229 + 1.0 * 6.488787651062012
Epoch 270, val loss: 1.2747979164123535
Epoch 280, training loss: 7.591098785400391 = 1.1201403141021729 + 1.0 * 6.470958232879639
Epoch 280, val loss: 1.2464125156402588
Epoch 290, training loss: 7.547779083251953 = 1.0828492641448975 + 1.0 * 6.464929580688477
Epoch 290, val loss: 1.2205250263214111
Epoch 300, training loss: 7.504703044891357 = 1.0470905303955078 + 1.0 * 6.45761251449585
Epoch 300, val loss: 1.1963446140289307
Epoch 310, training loss: 7.464409351348877 = 1.0124659538269043 + 1.0 * 6.451943397521973
Epoch 310, val loss: 1.1734859943389893
Epoch 320, training loss: 7.428967475891113 = 0.9789599180221558 + 1.0 * 6.450007438659668
Epoch 320, val loss: 1.15178382396698
Epoch 330, training loss: 7.39028787612915 = 0.9466270804405212 + 1.0 * 6.443660736083984
Epoch 330, val loss: 1.1310728788375854
Epoch 340, training loss: 7.352121829986572 = 0.9149608612060547 + 1.0 * 6.437160968780518
Epoch 340, val loss: 1.1109193563461304
Epoch 350, training loss: 7.316431045532227 = 0.8835978507995605 + 1.0 * 6.432833194732666
Epoch 350, val loss: 1.0910677909851074
Epoch 360, training loss: 7.284979820251465 = 0.8523876667022705 + 1.0 * 6.432591915130615
Epoch 360, val loss: 1.0713276863098145
Epoch 370, training loss: 7.248213768005371 = 0.8216767907142639 + 1.0 * 6.426537036895752
Epoch 370, val loss: 1.0520323514938354
Epoch 380, training loss: 7.212747573852539 = 0.7914764285087585 + 1.0 * 6.421271324157715
Epoch 380, val loss: 1.0331319570541382
Epoch 390, training loss: 7.179509162902832 = 0.7615118622779846 + 1.0 * 6.417997360229492
Epoch 390, val loss: 1.014399766921997
Epoch 400, training loss: 7.147102355957031 = 0.7316573858261108 + 1.0 * 6.415444850921631
Epoch 400, val loss: 0.996055543422699
Epoch 410, training loss: 7.124335289001465 = 0.7025628089904785 + 1.0 * 6.421772480010986
Epoch 410, val loss: 0.9783013463020325
Epoch 420, training loss: 7.08306360244751 = 0.6742580533027649 + 1.0 * 6.4088053703308105
Epoch 420, val loss: 0.9616116881370544
Epoch 430, training loss: 7.053228855133057 = 0.6466901898384094 + 1.0 * 6.406538486480713
Epoch 430, val loss: 0.9458657503128052
Epoch 440, training loss: 7.021717548370361 = 0.6197582483291626 + 1.0 * 6.401959419250488
Epoch 440, val loss: 0.9311565160751343
Epoch 450, training loss: 6.992623329162598 = 0.593528687953949 + 1.0 * 6.399094581604004
Epoch 450, val loss: 0.9176883101463318
Epoch 460, training loss: 6.9832539558410645 = 0.5681171417236328 + 1.0 * 6.415136814117432
Epoch 460, val loss: 0.9055841565132141
Epoch 470, training loss: 6.945197582244873 = 0.5439300537109375 + 1.0 * 6.4012675285339355
Epoch 470, val loss: 0.8952167630195618
Epoch 480, training loss: 6.9122748374938965 = 0.5207210183143616 + 1.0 * 6.39155387878418
Epoch 480, val loss: 0.886501669883728
Epoch 490, training loss: 6.888490200042725 = 0.49835020303726196 + 1.0 * 6.390140056610107
Epoch 490, val loss: 0.8791961073875427
Epoch 500, training loss: 6.86448860168457 = 0.47685953974723816 + 1.0 * 6.38762903213501
Epoch 500, val loss: 0.873313844203949
Epoch 510, training loss: 6.842593669891357 = 0.45619967579841614 + 1.0 * 6.386394023895264
Epoch 510, val loss: 0.8688821196556091
Epoch 520, training loss: 6.82162618637085 = 0.4362286627292633 + 1.0 * 6.385397434234619
Epoch 520, val loss: 0.8655895590782166
Epoch 530, training loss: 6.797745704650879 = 0.4169166684150696 + 1.0 * 6.380828857421875
Epoch 530, val loss: 0.8633989095687866
Epoch 540, training loss: 6.786297798156738 = 0.39813899993896484 + 1.0 * 6.388158798217773
Epoch 540, val loss: 0.8621955513954163
Epoch 550, training loss: 6.755374431610107 = 0.3801036775112152 + 1.0 * 6.375270843505859
Epoch 550, val loss: 0.8617711067199707
Epoch 560, training loss: 6.736934661865234 = 0.36270105838775635 + 1.0 * 6.374233722686768
Epoch 560, val loss: 0.862198531627655
Epoch 570, training loss: 6.717689514160156 = 0.3458355665206909 + 1.0 * 6.371853828430176
Epoch 570, val loss: 0.8632034659385681
Epoch 580, training loss: 6.6987714767456055 = 0.32943934202194214 + 1.0 * 6.369332313537598
Epoch 580, val loss: 0.8647928833961487
Epoch 590, training loss: 6.680996894836426 = 0.3135080635547638 + 1.0 * 6.367488861083984
Epoch 590, val loss: 0.8669096827507019
Epoch 600, training loss: 6.671575546264648 = 0.2981276512145996 + 1.0 * 6.373447895050049
Epoch 600, val loss: 0.8693867325782776
Epoch 610, training loss: 6.651118755340576 = 0.28348100185394287 + 1.0 * 6.367637634277344
Epoch 610, val loss: 0.8723459243774414
Epoch 620, training loss: 6.632468223571777 = 0.26941192150115967 + 1.0 * 6.363056182861328
Epoch 620, val loss: 0.8756106495857239
Epoch 630, training loss: 6.616459369659424 = 0.25586703419685364 + 1.0 * 6.360592365264893
Epoch 630, val loss: 0.8791124224662781
Epoch 640, training loss: 6.6030402183532715 = 0.2428390085697174 + 1.0 * 6.360201358795166
Epoch 640, val loss: 0.8829383850097656
Epoch 650, training loss: 6.587514877319336 = 0.23035928606987 + 1.0 * 6.357155799865723
Epoch 650, val loss: 0.8869481086730957
Epoch 660, training loss: 6.576211452484131 = 0.21846552193164825 + 1.0 * 6.357746124267578
Epoch 660, val loss: 0.8911670446395874
Epoch 670, training loss: 6.5666022300720215 = 0.20711450278759003 + 1.0 * 6.359487533569336
Epoch 670, val loss: 0.8955172896385193
Epoch 680, training loss: 6.552358627319336 = 0.19631965458393097 + 1.0 * 6.356039047241211
Epoch 680, val loss: 0.9000674486160278
Epoch 690, training loss: 6.539229393005371 = 0.18606840074062347 + 1.0 * 6.353160858154297
Epoch 690, val loss: 0.9047860503196716
Epoch 700, training loss: 6.529531955718994 = 0.17633283138275146 + 1.0 * 6.353199005126953
Epoch 700, val loss: 0.9096909761428833
Epoch 710, training loss: 6.516205310821533 = 0.16709938645362854 + 1.0 * 6.3491058349609375
Epoch 710, val loss: 0.9147965312004089
Epoch 720, training loss: 6.517420768737793 = 0.1583605259656906 + 1.0 * 6.359060287475586
Epoch 720, val loss: 0.9201023578643799
Epoch 730, training loss: 6.500341892242432 = 0.15013530850410461 + 1.0 * 6.35020637512207
Epoch 730, val loss: 0.925275444984436
Epoch 740, training loss: 6.486830234527588 = 0.14238061010837555 + 1.0 * 6.344449520111084
Epoch 740, val loss: 0.9308267831802368
Epoch 750, training loss: 6.478890419006348 = 0.1350589245557785 + 1.0 * 6.343831539154053
Epoch 750, val loss: 0.9364385604858398
Epoch 760, training loss: 6.472046852111816 = 0.12814269959926605 + 1.0 * 6.3439040184021
Epoch 760, val loss: 0.942099928855896
Epoch 770, training loss: 6.466732501983643 = 0.12163542211055756 + 1.0 * 6.345097064971924
Epoch 770, val loss: 0.9478217363357544
Epoch 780, training loss: 6.455715179443359 = 0.11554169654846191 + 1.0 * 6.340173244476318
Epoch 780, val loss: 0.9537725448608398
Epoch 790, training loss: 6.449070930480957 = 0.10981065034866333 + 1.0 * 6.339260101318359
Epoch 790, val loss: 0.9598021507263184
Epoch 800, training loss: 6.440978050231934 = 0.10440424084663391 + 1.0 * 6.336573600769043
Epoch 800, val loss: 0.9659958481788635
Epoch 810, training loss: 6.441192150115967 = 0.09931043535470963 + 1.0 * 6.34188175201416
Epoch 810, val loss: 0.9723494052886963
Epoch 820, training loss: 6.4383368492126465 = 0.09452428668737411 + 1.0 * 6.343812465667725
Epoch 820, val loss: 0.9786251783370972
Epoch 830, training loss: 6.424664497375488 = 0.09003401547670364 + 1.0 * 6.334630489349365
Epoch 830, val loss: 0.9850015044212341
Epoch 840, training loss: 6.418282985687256 = 0.08581274747848511 + 1.0 * 6.332470417022705
Epoch 840, val loss: 0.9914919137954712
Epoch 850, training loss: 6.4147443771362305 = 0.08183184266090393 + 1.0 * 6.332912445068359
Epoch 850, val loss: 0.998021125793457
Epoch 860, training loss: 6.409589767456055 = 0.07808147370815277 + 1.0 * 6.331508159637451
Epoch 860, val loss: 1.0045671463012695
Epoch 870, training loss: 6.406721591949463 = 0.07455645501613617 + 1.0 * 6.332165241241455
Epoch 870, val loss: 1.011297583580017
Epoch 880, training loss: 6.405994415283203 = 0.07123616337776184 + 1.0 * 6.334758281707764
Epoch 880, val loss: 1.017892837524414
Epoch 890, training loss: 6.3957695960998535 = 0.06811459362506866 + 1.0 * 6.327654838562012
Epoch 890, val loss: 1.024641990661621
Epoch 900, training loss: 6.391412258148193 = 0.06517257541418076 + 1.0 * 6.326239585876465
Epoch 900, val loss: 1.0313676595687866
Epoch 910, training loss: 6.386800765991211 = 0.06239171326160431 + 1.0 * 6.324409008026123
Epoch 910, val loss: 1.0381840467453003
Epoch 920, training loss: 6.3863654136657715 = 0.05975816398859024 + 1.0 * 6.3266072273254395
Epoch 920, val loss: 1.045023798942566
Epoch 930, training loss: 6.379443645477295 = 0.05728060007095337 + 1.0 * 6.322163105010986
Epoch 930, val loss: 1.051841139793396
Epoch 940, training loss: 6.378992080688477 = 0.05493909493088722 + 1.0 * 6.324052810668945
Epoch 940, val loss: 1.0588173866271973
Epoch 950, training loss: 6.374607563018799 = 0.05272947624325752 + 1.0 * 6.321877956390381
Epoch 950, val loss: 1.065751314163208
Epoch 960, training loss: 6.377780437469482 = 0.05063336342573166 + 1.0 * 6.327147006988525
Epoch 960, val loss: 1.0726124048233032
Epoch 970, training loss: 6.373597621917725 = 0.04864957183599472 + 1.0 * 6.324947834014893
Epoch 970, val loss: 1.0794636011123657
Epoch 980, training loss: 6.36670446395874 = 0.04677871614694595 + 1.0 * 6.319925785064697
Epoch 980, val loss: 1.0863858461380005
Epoch 990, training loss: 6.362058639526367 = 0.045000314712524414 + 1.0 * 6.317058086395264
Epoch 990, val loss: 1.0932567119598389
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.7996837111228255
=== training gcn model ===
Epoch 0, training loss: 10.52884578704834 = 1.931965947151184 + 1.0 * 8.596879959106445
Epoch 0, val loss: 1.9326748847961426
Epoch 10, training loss: 10.519713401794434 = 1.9229410886764526 + 1.0 * 8.596772193908691
Epoch 10, val loss: 1.9241111278533936
Epoch 20, training loss: 10.508007049560547 = 1.9120277166366577 + 1.0 * 8.595979690551758
Epoch 20, val loss: 1.9131741523742676
Epoch 30, training loss: 10.486007690429688 = 1.8969228267669678 + 1.0 * 8.58908462524414
Epoch 30, val loss: 1.8977371454238892
Epoch 40, training loss: 10.418444633483887 = 1.8757917881011963 + 1.0 * 8.54265308380127
Epoch 40, val loss: 1.8763984441757202
Epoch 50, training loss: 10.125 = 1.851829171180725 + 1.0 * 8.273170471191406
Epoch 50, val loss: 1.8532732725143433
Epoch 60, training loss: 9.643508911132812 = 1.8325626850128174 + 1.0 * 7.810946464538574
Epoch 60, val loss: 1.8347580432891846
Epoch 70, training loss: 9.096553802490234 = 1.8186085224151611 + 1.0 * 7.277945041656494
Epoch 70, val loss: 1.8203932046890259
Epoch 80, training loss: 8.87365436553955 = 1.805611491203308 + 1.0 * 7.068042755126953
Epoch 80, val loss: 1.8066895008087158
Epoch 90, training loss: 8.746997833251953 = 1.7896865606307983 + 1.0 * 6.957311630249023
Epoch 90, val loss: 1.790888786315918
Epoch 100, training loss: 8.645155906677246 = 1.773474097251892 + 1.0 * 6.871682167053223
Epoch 100, val loss: 1.7753885984420776
Epoch 110, training loss: 8.56801700592041 = 1.7580679655075073 + 1.0 * 6.809948921203613
Epoch 110, val loss: 1.7606480121612549
Epoch 120, training loss: 8.503214836120605 = 1.7419865131378174 + 1.0 * 6.761228084564209
Epoch 120, val loss: 1.7452926635742188
Epoch 130, training loss: 8.447501182556152 = 1.724084496498108 + 1.0 * 6.723416805267334
Epoch 130, val loss: 1.7286053895950317
Epoch 140, training loss: 8.389036178588867 = 1.7039170265197754 + 1.0 * 6.685119152069092
Epoch 140, val loss: 1.7102043628692627
Epoch 150, training loss: 8.33304214477539 = 1.6805535554885864 + 1.0 * 6.652488708496094
Epoch 150, val loss: 1.6894127130508423
Epoch 160, training loss: 8.282451629638672 = 1.6535128355026245 + 1.0 * 6.628938674926758
Epoch 160, val loss: 1.6656352281570435
Epoch 170, training loss: 8.225431442260742 = 1.622505784034729 + 1.0 * 6.6029253005981445
Epoch 170, val loss: 1.6385180950164795
Epoch 180, training loss: 8.1694917678833 = 1.5869344472885132 + 1.0 * 6.582557678222656
Epoch 180, val loss: 1.6075410842895508
Epoch 190, training loss: 8.111955642700195 = 1.5462284088134766 + 1.0 * 6.5657267570495605
Epoch 190, val loss: 1.5721322298049927
Epoch 200, training loss: 8.05700969696045 = 1.5006946325302124 + 1.0 * 6.5563154220581055
Epoch 200, val loss: 1.5329838991165161
Epoch 210, training loss: 7.989569187164307 = 1.4510083198547363 + 1.0 * 6.53856086730957
Epoch 210, val loss: 1.4901906251907349
Epoch 220, training loss: 7.923797130584717 = 1.396967887878418 + 1.0 * 6.526829242706299
Epoch 220, val loss: 1.44410240650177
Epoch 230, training loss: 7.85525369644165 = 1.3390893936157227 + 1.0 * 6.516164302825928
Epoch 230, val loss: 1.395105004310608
Epoch 240, training loss: 7.79152250289917 = 1.2791460752487183 + 1.0 * 6.512376308441162
Epoch 240, val loss: 1.3449323177337646
Epoch 250, training loss: 7.720631122589111 = 1.2199345827102661 + 1.0 * 6.500696659088135
Epoch 250, val loss: 1.295924425125122
Epoch 260, training loss: 7.6535749435424805 = 1.1614303588867188 + 1.0 * 6.492144584655762
Epoch 260, val loss: 1.2481679916381836
Epoch 270, training loss: 7.588245868682861 = 1.1040139198303223 + 1.0 * 6.484231948852539
Epoch 270, val loss: 1.201835036277771
Epoch 280, training loss: 7.526589393615723 = 1.0488052368164062 + 1.0 * 6.477784156799316
Epoch 280, val loss: 1.1580537557601929
Epoch 290, training loss: 7.471409797668457 = 0.9970139861106873 + 1.0 * 6.474395751953125
Epoch 290, val loss: 1.1177787780761719
Epoch 300, training loss: 7.414271354675293 = 0.9478946924209595 + 1.0 * 6.466376781463623
Epoch 300, val loss: 1.0805447101593018
Epoch 310, training loss: 7.36212682723999 = 0.9012019038200378 + 1.0 * 6.460925102233887
Epoch 310, val loss: 1.0459742546081543
Epoch 320, training loss: 7.321974277496338 = 0.8573152422904968 + 1.0 * 6.464659214019775
Epoch 320, val loss: 1.0143940448760986
Epoch 330, training loss: 7.269035816192627 = 0.8164328336715698 + 1.0 * 6.452602863311768
Epoch 330, val loss: 0.9859843850135803
Epoch 340, training loss: 7.223835468292236 = 0.7778304815292358 + 1.0 * 6.446004867553711
Epoch 340, val loss: 0.9601122736930847
Epoch 350, training loss: 7.182525157928467 = 0.741136372089386 + 1.0 * 6.4413886070251465
Epoch 350, val loss: 0.9365577697753906
Epoch 360, training loss: 7.149508476257324 = 0.7063267230987549 + 1.0 * 6.443181991577148
Epoch 360, val loss: 0.9152944087982178
Epoch 370, training loss: 7.10988712310791 = 0.673528254032135 + 1.0 * 6.43635892868042
Epoch 370, val loss: 0.8962882161140442
Epoch 380, training loss: 7.072481155395508 = 0.642265260219574 + 1.0 * 6.430215835571289
Epoch 380, val loss: 0.8792306780815125
Epoch 390, training loss: 7.038003444671631 = 0.6122384667396545 + 1.0 * 6.425765037536621
Epoch 390, val loss: 0.8638603687286377
Epoch 400, training loss: 7.008413791656494 = 0.5834470987319946 + 1.0 * 6.424966812133789
Epoch 400, val loss: 0.8500908017158508
Epoch 410, training loss: 6.977863788604736 = 0.5560097694396973 + 1.0 * 6.421854019165039
Epoch 410, val loss: 0.8378612399101257
Epoch 420, training loss: 6.946057319641113 = 0.5295600295066833 + 1.0 * 6.416497230529785
Epoch 420, val loss: 0.8269842863082886
Epoch 430, training loss: 6.931821346282959 = 0.5039877891540527 + 1.0 * 6.427833557128906
Epoch 430, val loss: 0.8173624277114868
Epoch 440, training loss: 6.893621921539307 = 0.479449599981308 + 1.0 * 6.414172172546387
Epoch 440, val loss: 0.8089793920516968
Epoch 450, training loss: 6.864652633666992 = 0.45582345128059387 + 1.0 * 6.408829212188721
Epoch 450, val loss: 0.801771879196167
Epoch 460, training loss: 6.8375067710876465 = 0.43294352293014526 + 1.0 * 6.4045634269714355
Epoch 460, val loss: 0.7956039309501648
Epoch 470, training loss: 6.823415756225586 = 0.4108250141143799 + 1.0 * 6.412590980529785
Epoch 470, val loss: 0.7904322743415833
Epoch 480, training loss: 6.794736862182617 = 0.38959962129592896 + 1.0 * 6.405137062072754
Epoch 480, val loss: 0.7862334847450256
Epoch 490, training loss: 6.766863822937012 = 0.3690562844276428 + 1.0 * 6.397807598114014
Epoch 490, val loss: 0.7830324172973633
Epoch 500, training loss: 6.744545936584473 = 0.3490985333919525 + 1.0 * 6.395447254180908
Epoch 500, val loss: 0.7807773351669312
Epoch 510, training loss: 6.72493839263916 = 0.32980582118034363 + 1.0 * 6.395132541656494
Epoch 510, val loss: 0.7793170213699341
Epoch 520, training loss: 6.702235698699951 = 0.3112345039844513 + 1.0 * 6.391001224517822
Epoch 520, val loss: 0.7785817980766296
Epoch 530, training loss: 6.683394432067871 = 0.29333943128585815 + 1.0 * 6.390055179595947
Epoch 530, val loss: 0.778605580329895
Epoch 540, training loss: 6.662949085235596 = 0.2759990394115448 + 1.0 * 6.3869500160217285
Epoch 540, val loss: 0.7793508172035217
Epoch 550, training loss: 6.647674560546875 = 0.25920969247817993 + 1.0 * 6.38846492767334
Epoch 550, val loss: 0.7807970643043518
Epoch 560, training loss: 6.635140895843506 = 0.24310271441936493 + 1.0 * 6.392038345336914
Epoch 560, val loss: 0.7827858924865723
Epoch 570, training loss: 6.608854293823242 = 0.22773997485637665 + 1.0 * 6.381114482879639
Epoch 570, val loss: 0.7853547930717468
Epoch 580, training loss: 6.59224796295166 = 0.21306952834129333 + 1.0 * 6.379178524017334
Epoch 580, val loss: 0.7885286211967468
Epoch 590, training loss: 6.5936970710754395 = 0.19914610683918 + 1.0 * 6.394550800323486
Epoch 590, val loss: 0.7922646403312683
Epoch 600, training loss: 6.5611701011657715 = 0.1860721856355667 + 1.0 * 6.375097751617432
Epoch 600, val loss: 0.7965449094772339
Epoch 610, training loss: 6.555281162261963 = 0.17384441196918488 + 1.0 * 6.381436824798584
Epoch 610, val loss: 0.8014041185379028
Epoch 620, training loss: 6.537490367889404 = 0.16247090697288513 + 1.0 * 6.375019550323486
Epoch 620, val loss: 0.8066938519477844
Epoch 630, training loss: 6.523886203765869 = 0.1519351750612259 + 1.0 * 6.371951103210449
Epoch 630, val loss: 0.812491774559021
Epoch 640, training loss: 6.510663032531738 = 0.14214442670345306 + 1.0 * 6.368518829345703
Epoch 640, val loss: 0.8187768459320068
Epoch 650, training loss: 6.505334854125977 = 0.1330825835466385 + 1.0 * 6.372252464294434
Epoch 650, val loss: 0.8254624009132385
Epoch 660, training loss: 6.491696834564209 = 0.12474998086690903 + 1.0 * 6.366946697235107
Epoch 660, val loss: 0.8324848413467407
Epoch 670, training loss: 6.484652996063232 = 0.11711947619915009 + 1.0 * 6.3675336837768555
Epoch 670, val loss: 0.8397318124771118
Epoch 680, training loss: 6.481298923492432 = 0.11005841195583344 + 1.0 * 6.371240615844727
Epoch 680, val loss: 0.8473271131515503
Epoch 690, training loss: 6.471114635467529 = 0.10357459634542465 + 1.0 * 6.367539882659912
Epoch 690, val loss: 0.8551112413406372
Epoch 700, training loss: 6.456912517547607 = 0.09759166091680527 + 1.0 * 6.359320640563965
Epoch 700, val loss: 0.862973153591156
Epoch 710, training loss: 6.4509663581848145 = 0.09205153584480286 + 1.0 * 6.358914852142334
Epoch 710, val loss: 0.871090829372406
Epoch 720, training loss: 6.454555034637451 = 0.08691012859344482 + 1.0 * 6.367644786834717
Epoch 720, val loss: 0.8793015480041504
Epoch 730, training loss: 6.444156169891357 = 0.0821874663233757 + 1.0 * 6.361968517303467
Epoch 730, val loss: 0.8875415921211243
Epoch 740, training loss: 6.435450077056885 = 0.07783668488264084 + 1.0 * 6.357613563537598
Epoch 740, val loss: 0.8956494331359863
Epoch 750, training loss: 6.427278518676758 = 0.07380332797765732 + 1.0 * 6.353475093841553
Epoch 750, val loss: 0.9039952158927917
Epoch 760, training loss: 6.423255920410156 = 0.07003563642501831 + 1.0 * 6.353220462799072
Epoch 760, val loss: 0.9124947190284729
Epoch 770, training loss: 6.4213175773620605 = 0.06653100252151489 + 1.0 * 6.354786396026611
Epoch 770, val loss: 0.9209373593330383
Epoch 780, training loss: 6.412185192108154 = 0.06328991055488586 + 1.0 * 6.348895072937012
Epoch 780, val loss: 0.9291718006134033
Epoch 790, training loss: 6.407582759857178 = 0.06026826798915863 + 1.0 * 6.347314357757568
Epoch 790, val loss: 0.9375067353248596
Epoch 800, training loss: 6.412899494171143 = 0.05744178593158722 + 1.0 * 6.355457782745361
Epoch 800, val loss: 0.9459750652313232
Epoch 810, training loss: 6.403600692749023 = 0.05481112003326416 + 1.0 * 6.348789691925049
Epoch 810, val loss: 0.9542239904403687
Epoch 820, training loss: 6.4011335372924805 = 0.05236244946718216 + 1.0 * 6.348771095275879
Epoch 820, val loss: 0.9624057412147522
Epoch 830, training loss: 6.394145965576172 = 0.05007027089595795 + 1.0 * 6.344075679779053
Epoch 830, val loss: 0.970538318157196
Epoch 840, training loss: 6.394360065460205 = 0.04792078211903572 + 1.0 * 6.346439361572266
Epoch 840, val loss: 0.9786607623100281
Epoch 850, training loss: 6.393763065338135 = 0.04590395465493202 + 1.0 * 6.347858905792236
Epoch 850, val loss: 0.9868816137313843
Epoch 860, training loss: 6.384221076965332 = 0.04402235150337219 + 1.0 * 6.340198516845703
Epoch 860, val loss: 0.9947351813316345
Epoch 870, training loss: 6.387370586395264 = 0.042254526168107986 + 1.0 * 6.345116138458252
Epoch 870, val loss: 1.0026967525482178
Epoch 880, training loss: 6.381382942199707 = 0.04059017077088356 + 1.0 * 6.340792655944824
Epoch 880, val loss: 1.0105072259902954
Epoch 890, training loss: 6.37785005569458 = 0.03902791440486908 + 1.0 * 6.338822364807129
Epoch 890, val loss: 1.0182713270187378
Epoch 900, training loss: 6.374274253845215 = 0.03755077347159386 + 1.0 * 6.336723327636719
Epoch 900, val loss: 1.0259606838226318
Epoch 910, training loss: 6.379439830780029 = 0.03615225851535797 + 1.0 * 6.343287467956543
Epoch 910, val loss: 1.0337419509887695
Epoch 920, training loss: 6.378287315368652 = 0.0348411425948143 + 1.0 * 6.343446254730225
Epoch 920, val loss: 1.0412108898162842
Epoch 930, training loss: 6.368091583251953 = 0.03361278399825096 + 1.0 * 6.334478855133057
Epoch 930, val loss: 1.0484992265701294
Epoch 940, training loss: 6.364419460296631 = 0.032445818185806274 + 1.0 * 6.331973552703857
Epoch 940, val loss: 1.0558197498321533
Epoch 950, training loss: 6.363736629486084 = 0.03133472055196762 + 1.0 * 6.332401752471924
Epoch 950, val loss: 1.0632824897766113
Epoch 960, training loss: 6.365076065063477 = 0.030283978208899498 + 1.0 * 6.334792137145996
Epoch 960, val loss: 1.0705194473266602
Epoch 970, training loss: 6.3612775802612305 = 0.029294196516275406 + 1.0 * 6.33198356628418
Epoch 970, val loss: 1.0775517225265503
Epoch 980, training loss: 6.356773376464844 = 0.028355693444609642 + 1.0 * 6.328417778015137
Epoch 980, val loss: 1.0844939947128296
Epoch 990, training loss: 6.357573509216309 = 0.02745828963816166 + 1.0 * 6.33011531829834
Epoch 990, val loss: 1.0915764570236206
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8096995255666843
The final CL Acc:0.75062, 0.02444, The final GNN Acc:0.80425, 0.00414
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13142])
remove edge: torch.Size([2, 7854])
updated graph: torch.Size([2, 10440])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.557829856872559 = 1.9609918594360352 + 1.0 * 8.596837997436523
Epoch 0, val loss: 1.969508171081543
Epoch 10, training loss: 10.5469388961792 = 1.950416922569275 + 1.0 * 8.596522331237793
Epoch 10, val loss: 1.9589396715164185
Epoch 20, training loss: 10.531064987182617 = 1.937477707862854 + 1.0 * 8.593586921691895
Epoch 20, val loss: 1.945374608039856
Epoch 30, training loss: 10.490707397460938 = 1.9196494817733765 + 1.0 * 8.57105827331543
Epoch 30, val loss: 1.926086187362671
Epoch 40, training loss: 10.32856273651123 = 1.8972364664077759 + 1.0 * 8.431325912475586
Epoch 40, val loss: 1.9024183750152588
Epoch 50, training loss: 9.950860977172852 = 1.8744609355926514 + 1.0 * 8.076399803161621
Epoch 50, val loss: 1.8795866966247559
Epoch 60, training loss: 9.335911750793457 = 1.85863196849823 + 1.0 * 7.477280139923096
Epoch 60, val loss: 1.8646680116653442
Epoch 70, training loss: 8.932577133178711 = 1.8449126482009888 + 1.0 * 7.0876641273498535
Epoch 70, val loss: 1.8512531518936157
Epoch 80, training loss: 8.7557954788208 = 1.8282520771026611 + 1.0 * 6.927543640136719
Epoch 80, val loss: 1.834455132484436
Epoch 90, training loss: 8.63923454284668 = 1.8086519241333008 + 1.0 * 6.830582618713379
Epoch 90, val loss: 1.8148242235183716
Epoch 100, training loss: 8.553838729858398 = 1.789729118347168 + 1.0 * 6.764110088348389
Epoch 100, val loss: 1.7961488962173462
Epoch 110, training loss: 8.486952781677246 = 1.77216374874115 + 1.0 * 6.714788913726807
Epoch 110, val loss: 1.7788488864898682
Epoch 120, training loss: 8.430731773376465 = 1.7544909715652466 + 1.0 * 6.67624044418335
Epoch 120, val loss: 1.7618157863616943
Epoch 130, training loss: 8.383810043334961 = 1.7358622550964355 + 1.0 * 6.647947788238525
Epoch 130, val loss: 1.7444887161254883
Epoch 140, training loss: 8.334175109863281 = 1.7157655954360962 + 1.0 * 6.618409156799316
Epoch 140, val loss: 1.7263519763946533
Epoch 150, training loss: 8.285666465759277 = 1.6934243440628052 + 1.0 * 6.592242240905762
Epoch 150, val loss: 1.7066465616226196
Epoch 160, training loss: 8.240310668945312 = 1.668342113494873 + 1.0 * 6.5719685554504395
Epoch 160, val loss: 1.6848394870758057
Epoch 170, training loss: 8.192224502563477 = 1.6401700973510742 + 1.0 * 6.5520548820495605
Epoch 170, val loss: 1.660543441772461
Epoch 180, training loss: 8.14522933959961 = 1.6084060668945312 + 1.0 * 6.53682279586792
Epoch 180, val loss: 1.6333308219909668
Epoch 190, training loss: 8.096172332763672 = 1.5727261304855347 + 1.0 * 6.523446083068848
Epoch 190, val loss: 1.60294771194458
Epoch 200, training loss: 8.046333312988281 = 1.5333616733551025 + 1.0 * 6.5129714012146
Epoch 200, val loss: 1.56977379322052
Epoch 210, training loss: 7.994081497192383 = 1.491056203842163 + 1.0 * 6.503025531768799
Epoch 210, val loss: 1.5345357656478882
Epoch 220, training loss: 7.937553405761719 = 1.4458401203155518 + 1.0 * 6.491713523864746
Epoch 220, val loss: 1.4973950386047363
Epoch 230, training loss: 7.880999565124512 = 1.3980066776275635 + 1.0 * 6.482993125915527
Epoch 230, val loss: 1.4587324857711792
Epoch 240, training loss: 7.833428382873535 = 1.3487756252288818 + 1.0 * 6.484652519226074
Epoch 240, val loss: 1.419826865196228
Epoch 250, training loss: 7.771003246307373 = 1.2994459867477417 + 1.0 * 6.471557140350342
Epoch 250, val loss: 1.3812355995178223
Epoch 260, training loss: 7.711845397949219 = 1.2493906021118164 + 1.0 * 6.462454795837402
Epoch 260, val loss: 1.3425910472869873
Epoch 270, training loss: 7.654247760772705 = 1.1989850997924805 + 1.0 * 6.455262660980225
Epoch 270, val loss: 1.3038548231124878
Epoch 280, training loss: 7.5978593826293945 = 1.1485826969146729 + 1.0 * 6.449276924133301
Epoch 280, val loss: 1.2653305530548096
Epoch 290, training loss: 7.553827285766602 = 1.0988333225250244 + 1.0 * 6.454994201660156
Epoch 290, val loss: 1.2274113893508911
Epoch 300, training loss: 7.492730140686035 = 1.051261305809021 + 1.0 * 6.441468715667725
Epoch 300, val loss: 1.191153883934021
Epoch 310, training loss: 7.441793918609619 = 1.005621075630188 + 1.0 * 6.436172962188721
Epoch 310, val loss: 1.1565583944320679
Epoch 320, training loss: 7.3921098709106445 = 0.9614527225494385 + 1.0 * 6.430656909942627
Epoch 320, val loss: 1.1231846809387207
Epoch 330, training loss: 7.35587739944458 = 0.9185461401939392 + 1.0 * 6.437331199645996
Epoch 330, val loss: 1.0907903909683228
Epoch 340, training loss: 7.303130149841309 = 0.8772813081741333 + 1.0 * 6.425848960876465
Epoch 340, val loss: 1.0597436428070068
Epoch 350, training loss: 7.2578511238098145 = 0.8372566103935242 + 1.0 * 6.420594692230225
Epoch 350, val loss: 1.0297406911849976
Epoch 360, training loss: 7.226678848266602 = 0.7981600165367126 + 1.0 * 6.428518772125244
Epoch 360, val loss: 1.0005722045898438
Epoch 370, training loss: 7.175312042236328 = 0.7607247829437256 + 1.0 * 6.414587497711182
Epoch 370, val loss: 0.9724340438842773
Epoch 380, training loss: 7.134847640991211 = 0.724560558795929 + 1.0 * 6.410286903381348
Epoch 380, val loss: 0.9455008506774902
Epoch 390, training loss: 7.096258163452148 = 0.6896721124649048 + 1.0 * 6.406586170196533
Epoch 390, val loss: 0.9195297360420227
Epoch 400, training loss: 7.0598297119140625 = 0.6563639044761658 + 1.0 * 6.403465747833252
Epoch 400, val loss: 0.8947938084602356
Epoch 410, training loss: 7.026101589202881 = 0.624920666217804 + 1.0 * 6.401180744171143
Epoch 410, val loss: 0.8716705441474915
Epoch 420, training loss: 6.992046356201172 = 0.5950934290885925 + 1.0 * 6.396953105926514
Epoch 420, val loss: 0.8500484824180603
Epoch 430, training loss: 6.9622883796691895 = 0.5667464137077332 + 1.0 * 6.395542144775391
Epoch 430, val loss: 0.8299432992935181
Epoch 440, training loss: 6.939539432525635 = 0.5400844216346741 + 1.0 * 6.3994550704956055
Epoch 440, val loss: 0.8113187551498413
Epoch 450, training loss: 6.9063920974731445 = 0.5149767398834229 + 1.0 * 6.391415119171143
Epoch 450, val loss: 0.7944923639297485
Epoch 460, training loss: 6.877275466918945 = 0.49105602502822876 + 1.0 * 6.386219501495361
Epoch 460, val loss: 0.7790406346321106
Epoch 470, training loss: 6.860623836517334 = 0.46807751059532166 + 1.0 * 6.3925461769104
Epoch 470, val loss: 0.7648090124130249
Epoch 480, training loss: 6.829067230224609 = 0.44609856605529785 + 1.0 * 6.382968902587891
Epoch 480, val loss: 0.7515521049499512
Epoch 490, training loss: 6.805012226104736 = 0.4247298538684845 + 1.0 * 6.380282402038574
Epoch 490, val loss: 0.7393322587013245
Epoch 500, training loss: 6.781172752380371 = 0.4038287103176117 + 1.0 * 6.377344131469727
Epoch 500, val loss: 0.7277244329452515
Epoch 510, training loss: 6.758930206298828 = 0.38325235247612 + 1.0 * 6.375678062438965
Epoch 510, val loss: 0.716827929019928
Epoch 520, training loss: 6.735218524932861 = 0.36305055022239685 + 1.0 * 6.372168064117432
Epoch 520, val loss: 0.7064123153686523
Epoch 530, training loss: 6.714538097381592 = 0.34304726123809814 + 1.0 * 6.371490955352783
Epoch 530, val loss: 0.6965459585189819
Epoch 540, training loss: 6.693586349487305 = 0.3233153223991394 + 1.0 * 6.3702712059021
Epoch 540, val loss: 0.6874116063117981
Epoch 550, training loss: 6.669844150543213 = 0.30395784974098206 + 1.0 * 6.365886211395264
Epoch 550, val loss: 0.6788983941078186
Epoch 560, training loss: 6.649171352386475 = 0.2850341200828552 + 1.0 * 6.364137172698975
Epoch 560, val loss: 0.6711781620979309
Epoch 570, training loss: 6.632393836975098 = 0.26666855812072754 + 1.0 * 6.365725517272949
Epoch 570, val loss: 0.6642470955848694
Epoch 580, training loss: 6.617389678955078 = 0.24915385246276855 + 1.0 * 6.368236064910889
Epoch 580, val loss: 0.6580738425254822
Epoch 590, training loss: 6.591890335083008 = 0.2325962781906128 + 1.0 * 6.3592939376831055
Epoch 590, val loss: 0.6528559923171997
Epoch 600, training loss: 6.574859619140625 = 0.21702620387077332 + 1.0 * 6.357833385467529
Epoch 600, val loss: 0.6485102772712708
Epoch 610, training loss: 6.578680992126465 = 0.2025241255760193 + 1.0 * 6.376156806945801
Epoch 610, val loss: 0.6450422406196594
Epoch 620, training loss: 6.547540664672852 = 0.18910783529281616 + 1.0 * 6.358432769775391
Epoch 620, val loss: 0.6426824927330017
Epoch 630, training loss: 6.532275676727295 = 0.17676560580730438 + 1.0 * 6.355510234832764
Epoch 630, val loss: 0.640886127948761
Epoch 640, training loss: 6.5169501304626465 = 0.1653488129377365 + 1.0 * 6.3516011238098145
Epoch 640, val loss: 0.6400223970413208
Epoch 650, training loss: 6.5050530433654785 = 0.15478938817977905 + 1.0 * 6.350263595581055
Epoch 650, val loss: 0.6398874521255493
Epoch 660, training loss: 6.5012125968933105 = 0.14505000412464142 + 1.0 * 6.3561625480651855
Epoch 660, val loss: 0.6404057145118713
Epoch 670, training loss: 6.483453750610352 = 0.1361381560564041 + 1.0 * 6.347315788269043
Epoch 670, val loss: 0.6413439512252808
Epoch 680, training loss: 6.477176189422607 = 0.1278974562883377 + 1.0 * 6.349278926849365
Epoch 680, val loss: 0.6428782343864441
Epoch 690, training loss: 6.467566013336182 = 0.12032684683799744 + 1.0 * 6.347239017486572
Epoch 690, val loss: 0.6448748707771301
Epoch 700, training loss: 6.4562506675720215 = 0.11333325505256653 + 1.0 * 6.342917442321777
Epoch 700, val loss: 0.6472933888435364
Epoch 710, training loss: 6.448001861572266 = 0.10685750842094421 + 1.0 * 6.341144561767578
Epoch 710, val loss: 0.6500701904296875
Epoch 720, training loss: 6.457553386688232 = 0.1008516252040863 + 1.0 * 6.356701850891113
Epoch 720, val loss: 0.6532579660415649
Epoch 730, training loss: 6.437331199645996 = 0.09533969312906265 + 1.0 * 6.341991424560547
Epoch 730, val loss: 0.656469464302063
Epoch 740, training loss: 6.428214073181152 = 0.09022403508424759 + 1.0 * 6.337989807128906
Epoch 740, val loss: 0.6599514484405518
Epoch 750, training loss: 6.423340320587158 = 0.0854688286781311 + 1.0 * 6.337871551513672
Epoch 750, val loss: 0.6636403799057007
Epoch 760, training loss: 6.417051315307617 = 0.08104050159454346 + 1.0 * 6.336010932922363
Epoch 760, val loss: 0.667526364326477
Epoch 770, training loss: 6.41677188873291 = 0.0769050344824791 + 1.0 * 6.339866638183594
Epoch 770, val loss: 0.6716179847717285
Epoch 780, training loss: 6.407142162322998 = 0.07306739687919617 + 1.0 * 6.334074974060059
Epoch 780, val loss: 0.6756939888000488
Epoch 790, training loss: 6.408752918243408 = 0.06947337090969086 + 1.0 * 6.339279651641846
Epoch 790, val loss: 0.6800128221511841
Epoch 800, training loss: 6.400107383728027 = 0.0661354809999466 + 1.0 * 6.333971977233887
Epoch 800, val loss: 0.6843418478965759
Epoch 810, training loss: 6.3928751945495605 = 0.06301803141832352 + 1.0 * 6.329857349395752
Epoch 810, val loss: 0.6887580752372742
Epoch 820, training loss: 6.3881731033325195 = 0.060091663151979446 + 1.0 * 6.3280816078186035
Epoch 820, val loss: 0.6932450532913208
Epoch 830, training loss: 6.388331890106201 = 0.05734187737107277 + 1.0 * 6.330989837646484
Epoch 830, val loss: 0.6978358626365662
Epoch 840, training loss: 6.38320255279541 = 0.054781585931777954 + 1.0 * 6.328421115875244
Epoch 840, val loss: 0.7023720145225525
Epoch 850, training loss: 6.3810296058654785 = 0.05237123370170593 + 1.0 * 6.328658580780029
Epoch 850, val loss: 0.7069525122642517
Epoch 860, training loss: 6.373972415924072 = 0.05011800304055214 + 1.0 * 6.323854446411133
Epoch 860, val loss: 0.711546778678894
Epoch 870, training loss: 6.372638702392578 = 0.04799514263868332 + 1.0 * 6.324643611907959
Epoch 870, val loss: 0.7161766886711121
Epoch 880, training loss: 6.371688365936279 = 0.04599718376994133 + 1.0 * 6.325691223144531
Epoch 880, val loss: 0.720763623714447
Epoch 890, training loss: 6.367501258850098 = 0.04412633553147316 + 1.0 * 6.3233747482299805
Epoch 890, val loss: 0.7253645062446594
Epoch 900, training loss: 6.362974643707275 = 0.04236358404159546 + 1.0 * 6.320611000061035
Epoch 900, val loss: 0.7298629283905029
Epoch 910, training loss: 6.361493110656738 = 0.04069633409380913 + 1.0 * 6.320796966552734
Epoch 910, val loss: 0.7343977093696594
Epoch 920, training loss: 6.361089706420898 = 0.03911975398659706 + 1.0 * 6.321969985961914
Epoch 920, val loss: 0.7389878034591675
Epoch 930, training loss: 6.356364727020264 = 0.03762638196349144 + 1.0 * 6.3187384605407715
Epoch 930, val loss: 0.7434846758842468
Epoch 940, training loss: 6.3556342124938965 = 0.036219239234924316 + 1.0 * 6.319415092468262
Epoch 940, val loss: 0.7479168772697449
Epoch 950, training loss: 6.351858139038086 = 0.034887585788965225 + 1.0 * 6.316970348358154
Epoch 950, val loss: 0.7523458003997803
Epoch 960, training loss: 6.350461483001709 = 0.03362707048654556 + 1.0 * 6.316834449768066
Epoch 960, val loss: 0.7566651701927185
Epoch 970, training loss: 6.356585502624512 = 0.03243260458111763 + 1.0 * 6.324152946472168
Epoch 970, val loss: 0.761070191860199
Epoch 980, training loss: 6.346105098724365 = 0.03130098059773445 + 1.0 * 6.3148040771484375
Epoch 980, val loss: 0.7653390169143677
Epoch 990, training loss: 6.3433661460876465 = 0.03022959642112255 + 1.0 * 6.313136577606201
Epoch 990, val loss: 0.7695682048797607
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 10.556846618652344 = 1.9600001573562622 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9625056982040405
Epoch 10, training loss: 10.546347618103027 = 1.9496686458587646 + 1.0 * 8.596678733825684
Epoch 10, val loss: 1.952722191810608
Epoch 20, training loss: 10.532587051391602 = 1.9372462034225464 + 1.0 * 8.595340728759766
Epoch 20, val loss: 1.940542459487915
Epoch 30, training loss: 10.503969192504883 = 1.9201788902282715 + 1.0 * 8.583789825439453
Epoch 30, val loss: 1.9235360622406006
Epoch 40, training loss: 10.405174255371094 = 1.8967878818511963 + 1.0 * 8.508386611938477
Epoch 40, val loss: 1.9006229639053345
Epoch 50, training loss: 10.04537582397461 = 1.8710346221923828 + 1.0 * 8.174341201782227
Epoch 50, val loss: 1.8762415647506714
Epoch 60, training loss: 9.653836250305176 = 1.847998857498169 + 1.0 * 7.805837154388428
Epoch 60, val loss: 1.8544440269470215
Epoch 70, training loss: 9.212528228759766 = 1.8286223411560059 + 1.0 * 7.38390588760376
Epoch 70, val loss: 1.8355581760406494
Epoch 80, training loss: 8.962327003479004 = 1.811692476272583 + 1.0 * 7.150634288787842
Epoch 80, val loss: 1.8189736604690552
Epoch 90, training loss: 8.795548439025879 = 1.7925890684127808 + 1.0 * 7.002959728240967
Epoch 90, val loss: 1.8017373085021973
Epoch 100, training loss: 8.672836303710938 = 1.7728182077407837 + 1.0 * 6.900018215179443
Epoch 100, val loss: 1.7843307256698608
Epoch 110, training loss: 8.572375297546387 = 1.7537174224853516 + 1.0 * 6.818657875061035
Epoch 110, val loss: 1.767134428024292
Epoch 120, training loss: 8.490320205688477 = 1.7347116470336914 + 1.0 * 6.755608081817627
Epoch 120, val loss: 1.7496834993362427
Epoch 130, training loss: 8.419273376464844 = 1.7142242193222046 + 1.0 * 6.70504903793335
Epoch 130, val loss: 1.731268286705017
Epoch 140, training loss: 8.359939575195312 = 1.6912530660629272 + 1.0 * 6.668686389923096
Epoch 140, val loss: 1.711329698562622
Epoch 150, training loss: 8.300808906555176 = 1.6651853322982788 + 1.0 * 6.635623931884766
Epoch 150, val loss: 1.6892467737197876
Epoch 160, training loss: 8.245793342590332 = 1.6350772380828857 + 1.0 * 6.610716342926025
Epoch 160, val loss: 1.6639209985733032
Epoch 170, training loss: 8.190078735351562 = 1.6001994609832764 + 1.0 * 6.589879035949707
Epoch 170, val loss: 1.6346341371536255
Epoch 180, training loss: 8.132940292358398 = 1.5601842403411865 + 1.0 * 6.572755813598633
Epoch 180, val loss: 1.6010313034057617
Epoch 190, training loss: 8.075379371643066 = 1.5151878595352173 + 1.0 * 6.5601911544799805
Epoch 190, val loss: 1.5634130239486694
Epoch 200, training loss: 8.010076522827148 = 1.4658608436584473 + 1.0 * 6.544216156005859
Epoch 200, val loss: 1.5220680236816406
Epoch 210, training loss: 7.944655895233154 = 1.412399411201477 + 1.0 * 6.532256603240967
Epoch 210, val loss: 1.4775315523147583
Epoch 220, training loss: 7.883213043212891 = 1.355733036994934 + 1.0 * 6.527480125427246
Epoch 220, val loss: 1.430701732635498
Epoch 230, training loss: 7.8112263679504395 = 1.2982326745986938 + 1.0 * 6.512993812561035
Epoch 230, val loss: 1.3834598064422607
Epoch 240, training loss: 7.743346691131592 = 1.2404955625534058 + 1.0 * 6.5028510093688965
Epoch 240, val loss: 1.336561679840088
Epoch 250, training loss: 7.687521934509277 = 1.1833785772323608 + 1.0 * 6.504143238067627
Epoch 250, val loss: 1.2910151481628418
Epoch 260, training loss: 7.618940353393555 = 1.129109263420105 + 1.0 * 6.48983097076416
Epoch 260, val loss: 1.2481478452682495
Epoch 270, training loss: 7.557466506958008 = 1.0768276453018188 + 1.0 * 6.4806389808654785
Epoch 270, val loss: 1.207737922668457
Epoch 280, training loss: 7.49957275390625 = 1.026317834854126 + 1.0 * 6.473254680633545
Epoch 280, val loss: 1.1691969633102417
Epoch 290, training loss: 7.459741592407227 = 0.9777022004127502 + 1.0 * 6.482039451599121
Epoch 290, val loss: 1.1324557065963745
Epoch 300, training loss: 7.394913673400879 = 0.931628942489624 + 1.0 * 6.463284492492676
Epoch 300, val loss: 1.0982000827789307
Epoch 310, training loss: 7.342714309692383 = 0.887603759765625 + 1.0 * 6.455110549926758
Epoch 310, val loss: 1.0658315420150757
Epoch 320, training loss: 7.293956756591797 = 0.845069408416748 + 1.0 * 6.448887348175049
Epoch 320, val loss: 1.034864902496338
Epoch 330, training loss: 7.249330043792725 = 0.8042349815368652 + 1.0 * 6.445095062255859
Epoch 330, val loss: 1.0054130554199219
Epoch 340, training loss: 7.208069801330566 = 0.7657167911529541 + 1.0 * 6.442353248596191
Epoch 340, val loss: 0.9781704545021057
Epoch 350, training loss: 7.162874221801758 = 0.7290100455284119 + 1.0 * 6.433864116668701
Epoch 350, val loss: 0.9528214931488037
Epoch 360, training loss: 7.134511470794678 = 0.6938945651054382 + 1.0 * 6.440617084503174
Epoch 360, val loss: 0.9291329979896545
Epoch 370, training loss: 7.090016841888428 = 0.6608949303627014 + 1.0 * 6.429121971130371
Epoch 370, val loss: 0.9075055718421936
Epoch 380, training loss: 7.051586151123047 = 0.62979656457901 + 1.0 * 6.421789646148682
Epoch 380, val loss: 0.8881377577781677
Epoch 390, training loss: 7.017495632171631 = 0.6003044843673706 + 1.0 * 6.417191028594971
Epoch 390, val loss: 0.8706309199333191
Epoch 400, training loss: 6.985276699066162 = 0.5721510052680969 + 1.0 * 6.413125514984131
Epoch 400, val loss: 0.8548391461372375
Epoch 410, training loss: 6.962887763977051 = 0.5454733371734619 + 1.0 * 6.41741418838501
Epoch 410, val loss: 0.8407281637191772
Epoch 420, training loss: 6.929863929748535 = 0.5205094814300537 + 1.0 * 6.409354209899902
Epoch 420, val loss: 0.8285664916038513
Epoch 430, training loss: 6.901749134063721 = 0.4966943562030792 + 1.0 * 6.405054569244385
Epoch 430, val loss: 0.8178309798240662
Epoch 440, training loss: 6.873051643371582 = 0.47383594512939453 + 1.0 * 6.3992156982421875
Epoch 440, val loss: 0.8081455230712891
Epoch 450, training loss: 6.848074913024902 = 0.4517587721347809 + 1.0 * 6.396316051483154
Epoch 450, val loss: 0.7995134592056274
Epoch 460, training loss: 6.839994430541992 = 0.4304888844490051 + 1.0 * 6.409505367279053
Epoch 460, val loss: 0.7917771935462952
Epoch 470, training loss: 6.804563999176025 = 0.410165011882782 + 1.0 * 6.394399166107178
Epoch 470, val loss: 0.7849522233009338
Epoch 480, training loss: 6.780396938323975 = 0.3906075060367584 + 1.0 * 6.389789581298828
Epoch 480, val loss: 0.7788793444633484
Epoch 490, training loss: 6.757866859436035 = 0.37169575691223145 + 1.0 * 6.386171340942383
Epoch 490, val loss: 0.7734092473983765
Epoch 500, training loss: 6.738821029663086 = 0.3534487187862396 + 1.0 * 6.385372161865234
Epoch 500, val loss: 0.7685014605522156
Epoch 510, training loss: 6.717182159423828 = 0.335711270570755 + 1.0 * 6.381470680236816
Epoch 510, val loss: 0.7641521096229553
Epoch 520, training loss: 6.699723720550537 = 0.31852957606315613 + 1.0 * 6.381194114685059
Epoch 520, val loss: 0.7602125406265259
Epoch 530, training loss: 6.6796159744262695 = 0.30190354585647583 + 1.0 * 6.377712249755859
Epoch 530, val loss: 0.7567691802978516
Epoch 540, training loss: 6.662734031677246 = 0.2858439087867737 + 1.0 * 6.376890182495117
Epoch 540, val loss: 0.7537681460380554
Epoch 550, training loss: 6.653149604797363 = 0.2703917920589447 + 1.0 * 6.382757663726807
Epoch 550, val loss: 0.7512310147285461
Epoch 560, training loss: 6.63116455078125 = 0.25559201836586 + 1.0 * 6.375572681427002
Epoch 560, val loss: 0.74922776222229
Epoch 570, training loss: 6.610610008239746 = 0.24146509170532227 + 1.0 * 6.369144916534424
Epoch 570, val loss: 0.7477242946624756
Epoch 580, training loss: 6.594169616699219 = 0.22789902985095978 + 1.0 * 6.366270542144775
Epoch 580, val loss: 0.7467005848884583
Epoch 590, training loss: 6.581084728240967 = 0.21490269899368286 + 1.0 * 6.36618185043335
Epoch 590, val loss: 0.7462329864501953
Epoch 600, training loss: 6.568050861358643 = 0.20258519053459167 + 1.0 * 6.3654656410217285
Epoch 600, val loss: 0.7463395595550537
Epoch 610, training loss: 6.559197425842285 = 0.1909504383802414 + 1.0 * 6.368247032165527
Epoch 610, val loss: 0.7470377683639526
Epoch 620, training loss: 6.543750286102295 = 0.18006476759910583 + 1.0 * 6.363685607910156
Epoch 620, val loss: 0.7482399344444275
Epoch 630, training loss: 6.529792785644531 = 0.1697961837053299 + 1.0 * 6.359996795654297
Epoch 630, val loss: 0.7500708103179932
Epoch 640, training loss: 6.517725467681885 = 0.1601610779762268 + 1.0 * 6.357564449310303
Epoch 640, val loss: 0.7524497509002686
Epoch 650, training loss: 6.513574123382568 = 0.1511302888393402 + 1.0 * 6.362443923950195
Epoch 650, val loss: 0.7554031014442444
Epoch 660, training loss: 6.497929096221924 = 0.1427481472492218 + 1.0 * 6.355180740356445
Epoch 660, val loss: 0.7588034868240356
Epoch 670, training loss: 6.48835563659668 = 0.13489770889282227 + 1.0 * 6.353457927703857
Epoch 670, val loss: 0.762752115726471
Epoch 680, training loss: 6.486200332641602 = 0.12757675349712372 + 1.0 * 6.358623504638672
Epoch 680, val loss: 0.7671026587486267
Epoch 690, training loss: 6.470696926116943 = 0.12073074281215668 + 1.0 * 6.349966049194336
Epoch 690, val loss: 0.7718507647514343
Epoch 700, training loss: 6.462547302246094 = 0.11435645073652267 + 1.0 * 6.348190784454346
Epoch 700, val loss: 0.7769643068313599
Epoch 710, training loss: 6.458868503570557 = 0.10839951783418655 + 1.0 * 6.35046911239624
Epoch 710, val loss: 0.7823821306228638
Epoch 720, training loss: 6.448843955993652 = 0.10287000238895416 + 1.0 * 6.345973968505859
Epoch 720, val loss: 0.7880464792251587
Epoch 730, training loss: 6.448422431945801 = 0.09770821779966354 + 1.0 * 6.350714206695557
Epoch 730, val loss: 0.7939916849136353
Epoch 740, training loss: 6.435338497161865 = 0.09288535267114639 + 1.0 * 6.3424530029296875
Epoch 740, val loss: 0.8000973463058472
Epoch 750, training loss: 6.428736686706543 = 0.08837448805570602 + 1.0 * 6.340362071990967
Epoch 750, val loss: 0.8064092397689819
Epoch 760, training loss: 6.439203262329102 = 0.0841311663389206 + 1.0 * 6.355072021484375
Epoch 760, val loss: 0.8128554821014404
Epoch 770, training loss: 6.422361850738525 = 0.08020355552434921 + 1.0 * 6.342158317565918
Epoch 770, val loss: 0.8193431496620178
Epoch 780, training loss: 6.416110515594482 = 0.07651571184396744 + 1.0 * 6.339594841003418
Epoch 780, val loss: 0.8259021043777466
Epoch 790, training loss: 6.410567283630371 = 0.07304787635803223 + 1.0 * 6.337519645690918
Epoch 790, val loss: 0.8325293660163879
Epoch 800, training loss: 6.405333518981934 = 0.06978678703308105 + 1.0 * 6.335546493530273
Epoch 800, val loss: 0.8391962051391602
Epoch 810, training loss: 6.408905982971191 = 0.06671902537345886 + 1.0 * 6.34218692779541
Epoch 810, val loss: 0.8459133505821228
Epoch 820, training loss: 6.40004825592041 = 0.06383362412452698 + 1.0 * 6.336214542388916
Epoch 820, val loss: 0.8525334596633911
Epoch 830, training loss: 6.393564701080322 = 0.06113196164369583 + 1.0 * 6.332432746887207
Epoch 830, val loss: 0.8591969609260559
Epoch 840, training loss: 6.3893046379089355 = 0.05856745317578316 + 1.0 * 6.330737113952637
Epoch 840, val loss: 0.8658679723739624
Epoch 850, training loss: 6.388854026794434 = 0.056149426847696304 + 1.0 * 6.332704544067383
Epoch 850, val loss: 0.8724822402000427
Epoch 860, training loss: 6.383112907409668 = 0.05387001484632492 + 1.0 * 6.329242706298828
Epoch 860, val loss: 0.8790757060050964
Epoch 870, training loss: 6.381906032562256 = 0.05171080306172371 + 1.0 * 6.330195426940918
Epoch 870, val loss: 0.8856216073036194
Epoch 880, training loss: 6.376267433166504 = 0.04966802895069122 + 1.0 * 6.326599597930908
Epoch 880, val loss: 0.8920984864234924
Epoch 890, training loss: 6.376096725463867 = 0.04773347079753876 + 1.0 * 6.328363418579102
Epoch 890, val loss: 0.8985356092453003
Epoch 900, training loss: 6.370999336242676 = 0.04591655358672142 + 1.0 * 6.325082778930664
Epoch 900, val loss: 0.9048541188240051
Epoch 910, training loss: 6.3673319816589355 = 0.044190991669893265 + 1.0 * 6.323141098022461
Epoch 910, val loss: 0.9111456871032715
Epoch 920, training loss: 6.363734722137451 = 0.042556267231702805 + 1.0 * 6.321178436279297
Epoch 920, val loss: 0.9174086451530457
Epoch 930, training loss: 6.362419605255127 = 0.04099796712398529 + 1.0 * 6.3214216232299805
Epoch 930, val loss: 0.9235965013504028
Epoch 940, training loss: 6.3646745681762695 = 0.03951824828982353 + 1.0 * 6.325156211853027
Epoch 940, val loss: 0.9297534823417664
Epoch 950, training loss: 6.357356071472168 = 0.03811478614807129 + 1.0 * 6.319241523742676
Epoch 950, val loss: 0.9357450604438782
Epoch 960, training loss: 6.358104705810547 = 0.03678396716713905 + 1.0 * 6.321320533752441
Epoch 960, val loss: 0.9417110085487366
Epoch 970, training loss: 6.352602958679199 = 0.03551240265369415 + 1.0 * 6.3170905113220215
Epoch 970, val loss: 0.9475910663604736
Epoch 980, training loss: 6.360129356384277 = 0.034303393214941025 + 1.0 * 6.325826168060303
Epoch 980, val loss: 0.9534426331520081
Epoch 990, training loss: 6.351282596588135 = 0.03315967321395874 + 1.0 * 6.318122863769531
Epoch 990, val loss: 0.9591783285140991
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 10.54388427734375 = 1.9470264911651611 + 1.0 * 8.596858024597168
Epoch 0, val loss: 1.9433010816574097
Epoch 10, training loss: 10.533279418945312 = 1.936618447303772 + 1.0 * 8.596660614013672
Epoch 10, val loss: 1.9330371618270874
Epoch 20, training loss: 10.518669128417969 = 1.9234950542449951 + 1.0 * 8.595173835754395
Epoch 20, val loss: 1.9195516109466553
Epoch 30, training loss: 10.487918853759766 = 1.905007243156433 + 1.0 * 8.582911491394043
Epoch 30, val loss: 1.900038480758667
Epoch 40, training loss: 10.388620376586914 = 1.8800816535949707 + 1.0 * 8.508539199829102
Epoch 40, val loss: 1.8743590116500854
Epoch 50, training loss: 9.97877025604248 = 1.8536741733551025 + 1.0 * 8.125096321105957
Epoch 50, val loss: 1.8487962484359741
Epoch 60, training loss: 9.453474044799805 = 1.8334662914276123 + 1.0 * 7.620007514953613
Epoch 60, val loss: 1.8300597667694092
Epoch 70, training loss: 9.0037841796875 = 1.8179436922073364 + 1.0 * 7.185840129852295
Epoch 70, val loss: 1.8150711059570312
Epoch 80, training loss: 8.77092170715332 = 1.8026010990142822 + 1.0 * 6.968320846557617
Epoch 80, val loss: 1.7995414733886719
Epoch 90, training loss: 8.639984130859375 = 1.785762071609497 + 1.0 * 6.854222297668457
Epoch 90, val loss: 1.7829134464263916
Epoch 100, training loss: 8.555556297302246 = 1.768193006515503 + 1.0 * 6.787363052368164
Epoch 100, val loss: 1.7670921087265015
Epoch 110, training loss: 8.486344337463379 = 1.7507826089859009 + 1.0 * 6.735561847686768
Epoch 110, val loss: 1.7520546913146973
Epoch 120, training loss: 8.421224594116211 = 1.7329555749893188 + 1.0 * 6.688268661499023
Epoch 120, val loss: 1.7366238832473755
Epoch 130, training loss: 8.365793228149414 = 1.713456630706787 + 1.0 * 6.652336597442627
Epoch 130, val loss: 1.7196109294891357
Epoch 140, training loss: 8.308417320251465 = 1.6914294958114624 + 1.0 * 6.616988182067871
Epoch 140, val loss: 1.7006274461746216
Epoch 150, training loss: 8.255309104919434 = 1.6662213802337646 + 1.0 * 6.589087963104248
Epoch 150, val loss: 1.6790902614593506
Epoch 160, training loss: 8.204439163208008 = 1.6372034549713135 + 1.0 * 6.567235469818115
Epoch 160, val loss: 1.6544599533081055
Epoch 170, training loss: 8.154378890991211 = 1.6040780544281006 + 1.0 * 6.550300598144531
Epoch 170, val loss: 1.6265230178833008
Epoch 180, training loss: 8.1021728515625 = 1.5673366785049438 + 1.0 * 6.5348358154296875
Epoch 180, val loss: 1.5956474542617798
Epoch 190, training loss: 8.048025131225586 = 1.5265939235687256 + 1.0 * 6.5214314460754395
Epoch 190, val loss: 1.5616835355758667
Epoch 200, training loss: 7.991560935974121 = 1.4815659523010254 + 1.0 * 6.509994983673096
Epoch 200, val loss: 1.5243606567382812
Epoch 210, training loss: 7.9347243309021 = 1.4328233003616333 + 1.0 * 6.501901149749756
Epoch 210, val loss: 1.4846532344818115
Epoch 220, training loss: 7.872875690460205 = 1.3812521696090698 + 1.0 * 6.491623401641846
Epoch 220, val loss: 1.4429889917373657
Epoch 230, training loss: 7.809479713439941 = 1.3268721103668213 + 1.0 * 6.482607841491699
Epoch 230, val loss: 1.3995940685272217
Epoch 240, training loss: 7.749569892883301 = 1.2702884674072266 + 1.0 * 6.479281425476074
Epoch 240, val loss: 1.3551582098007202
Epoch 250, training loss: 7.682262420654297 = 1.2136585712432861 + 1.0 * 6.468603610992432
Epoch 250, val loss: 1.3116575479507446
Epoch 260, training loss: 7.6198811531066895 = 1.1576122045516968 + 1.0 * 6.462268829345703
Epoch 260, val loss: 1.269129753112793
Epoch 270, training loss: 7.559453964233398 = 1.1026055812835693 + 1.0 * 6.45684814453125
Epoch 270, val loss: 1.228030800819397
Epoch 280, training loss: 7.50148868560791 = 1.04950749874115 + 1.0 * 6.451981067657471
Epoch 280, val loss: 1.1889033317565918
Epoch 290, training loss: 7.448273181915283 = 0.9991797208786011 + 1.0 * 6.449093341827393
Epoch 290, val loss: 1.1521271467208862
Epoch 300, training loss: 7.394182205200195 = 0.9515446424484253 + 1.0 * 6.4426374435424805
Epoch 300, val loss: 1.1176142692565918
Epoch 310, training loss: 7.342916488647461 = 0.9060888290405273 + 1.0 * 6.436827659606934
Epoch 310, val loss: 1.084938883781433
Epoch 320, training loss: 7.296478271484375 = 0.8627510666847229 + 1.0 * 6.433727264404297
Epoch 320, val loss: 1.0538216829299927
Epoch 330, training loss: 7.251682758331299 = 0.8218337297439575 + 1.0 * 6.429849147796631
Epoch 330, val loss: 1.0244369506835938
Epoch 340, training loss: 7.208199977874756 = 0.7833065390586853 + 1.0 * 6.424893379211426
Epoch 340, val loss: 0.9969492554664612
Epoch 350, training loss: 7.16713809967041 = 0.7467873096466064 + 1.0 * 6.420351028442383
Epoch 350, val loss: 0.9712055325508118
Epoch 360, training loss: 7.128063678741455 = 0.7120396494865417 + 1.0 * 6.416024208068848
Epoch 360, val loss: 0.9470351338386536
Epoch 370, training loss: 7.102421283721924 = 0.6790484189987183 + 1.0 * 6.423372745513916
Epoch 370, val loss: 0.9244486689567566
Epoch 380, training loss: 7.058415412902832 = 0.6480833888053894 + 1.0 * 6.410332202911377
Epoch 380, val loss: 0.903972864151001
Epoch 390, training loss: 7.0287065505981445 = 0.6188202500343323 + 1.0 * 6.409886360168457
Epoch 390, val loss: 0.8853062987327576
Epoch 400, training loss: 6.996284008026123 = 0.5909989476203918 + 1.0 * 6.405284881591797
Epoch 400, val loss: 0.8682806491851807
Epoch 410, training loss: 6.96552848815918 = 0.5645294189453125 + 1.0 * 6.400999069213867
Epoch 410, val loss: 0.8526296615600586
Epoch 420, training loss: 6.943416118621826 = 0.5390982627868652 + 1.0 * 6.404317855834961
Epoch 420, val loss: 0.8384195566177368
Epoch 430, training loss: 6.913026809692383 = 0.514994740486145 + 1.0 * 6.398032188415527
Epoch 430, val loss: 0.8254590034484863
Epoch 440, training loss: 6.88489294052124 = 0.4918980598449707 + 1.0 * 6.3929948806762695
Epoch 440, val loss: 0.8136847019195557
Epoch 450, training loss: 6.858131408691406 = 0.4696413576602936 + 1.0 * 6.388490200042725
Epoch 450, val loss: 0.8030825853347778
Epoch 460, training loss: 6.846536159515381 = 0.4482429623603821 + 1.0 * 6.3982930183410645
Epoch 460, val loss: 0.7934243083000183
Epoch 470, training loss: 6.812929630279541 = 0.42779049277305603 + 1.0 * 6.385138988494873
Epoch 470, val loss: 0.7847496271133423
Epoch 480, training loss: 6.790998458862305 = 0.40817272663116455 + 1.0 * 6.38282585144043
Epoch 480, val loss: 0.7770295143127441
Epoch 490, training loss: 6.781685829162598 = 0.38924792408943176 + 1.0 * 6.392437934875488
Epoch 490, val loss: 0.7701249122619629
Epoch 500, training loss: 6.751271724700928 = 0.37113508582115173 + 1.0 * 6.380136489868164
Epoch 500, val loss: 0.763929009437561
Epoch 510, training loss: 6.728567123413086 = 0.3536173701286316 + 1.0 * 6.374949932098389
Epoch 510, val loss: 0.7584139108657837
Epoch 520, training loss: 6.709952354431152 = 0.33661672472953796 + 1.0 * 6.373335838317871
Epoch 520, val loss: 0.7534641623497009
Epoch 530, training loss: 6.702450275421143 = 0.3201034367084503 + 1.0 * 6.3823466300964355
Epoch 530, val loss: 0.7490363717079163
Epoch 540, training loss: 6.674792289733887 = 0.30415236949920654 + 1.0 * 6.370639801025391
Epoch 540, val loss: 0.7450927495956421
Epoch 550, training loss: 6.6561713218688965 = 0.2887374460697174 + 1.0 * 6.367434024810791
Epoch 550, val loss: 0.7416854500770569
Epoch 560, training loss: 6.640212059020996 = 0.27379414439201355 + 1.0 * 6.36641788482666
Epoch 560, val loss: 0.7387778162956238
Epoch 570, training loss: 6.630599021911621 = 0.2593417167663574 + 1.0 * 6.371257305145264
Epoch 570, val loss: 0.7363559603691101
Epoch 580, training loss: 6.622933864593506 = 0.24551354348659515 + 1.0 * 6.377420425415039
Epoch 580, val loss: 0.7344834804534912
Epoch 590, training loss: 6.598482608795166 = 0.23243632912635803 + 1.0 * 6.36604642868042
Epoch 590, val loss: 0.7332150936126709
Epoch 600, training loss: 6.5796990394592285 = 0.21993431448936462 + 1.0 * 6.359764575958252
Epoch 600, val loss: 0.7325265407562256
Epoch 610, training loss: 6.565521240234375 = 0.2080431878566742 + 1.0 * 6.357478141784668
Epoch 610, val loss: 0.7324091792106628
Epoch 620, training loss: 6.554585933685303 = 0.19676169753074646 + 1.0 * 6.357824325561523
Epoch 620, val loss: 0.7328662276268005
Epoch 630, training loss: 6.547885894775391 = 0.18613509833812714 + 1.0 * 6.361750602722168
Epoch 630, val loss: 0.7338578701019287
Epoch 640, training loss: 6.530299663543701 = 0.17616212368011475 + 1.0 * 6.354137420654297
Epoch 640, val loss: 0.7354373931884766
Epoch 650, training loss: 6.521147727966309 = 0.1668154001235962 + 1.0 * 6.354332447052002
Epoch 650, val loss: 0.7375331521034241
Epoch 660, training loss: 6.508959770202637 = 0.15802867710590363 + 1.0 * 6.350931167602539
Epoch 660, val loss: 0.7401248216629028
Epoch 670, training loss: 6.502188205718994 = 0.149759903550148 + 1.0 * 6.352428436279297
Epoch 670, val loss: 0.743122398853302
Epoch 680, training loss: 6.491680145263672 = 0.14204131066799164 + 1.0 * 6.349638938903809
Epoch 680, val loss: 0.7464926838874817
Epoch 690, training loss: 6.482578754425049 = 0.13478480279445648 + 1.0 * 6.347794055938721
Epoch 690, val loss: 0.7502613067626953
Epoch 700, training loss: 6.476071357727051 = 0.12798982858657837 + 1.0 * 6.348081588745117
Epoch 700, val loss: 0.7543466091156006
Epoch 710, training loss: 6.4689788818359375 = 0.12159106880426407 + 1.0 * 6.347387790679932
Epoch 710, val loss: 0.7586504817008972
Epoch 720, training loss: 6.45814847946167 = 0.11561020463705063 + 1.0 * 6.342538356781006
Epoch 720, val loss: 0.7633716464042664
Epoch 730, training loss: 6.452971458435059 = 0.10998766124248505 + 1.0 * 6.342983722686768
Epoch 730, val loss: 0.7682811617851257
Epoch 740, training loss: 6.44800329208374 = 0.10468621551990509 + 1.0 * 6.343317031860352
Epoch 740, val loss: 0.7733099460601807
Epoch 750, training loss: 6.449862003326416 = 0.09970179945230484 + 1.0 * 6.350160121917725
Epoch 750, val loss: 0.7784091234207153
Epoch 760, training loss: 6.437216281890869 = 0.09503867477178574 + 1.0 * 6.342177391052246
Epoch 760, val loss: 0.783815860748291
Epoch 770, training loss: 6.42631196975708 = 0.09062986820936203 + 1.0 * 6.335681915283203
Epoch 770, val loss: 0.7892690896987915
Epoch 780, training loss: 6.420947551727295 = 0.08645124733448029 + 1.0 * 6.33449649810791
Epoch 780, val loss: 0.7948525547981262
Epoch 790, training loss: 6.431096076965332 = 0.08252034336328506 + 1.0 * 6.348575592041016
Epoch 790, val loss: 0.8005664944648743
Epoch 800, training loss: 6.413257598876953 = 0.07879187166690826 + 1.0 * 6.334465503692627
Epoch 800, val loss: 0.8061040639877319
Epoch 810, training loss: 6.407027244567871 = 0.07529450953006744 + 1.0 * 6.331732749938965
Epoch 810, val loss: 0.8119433522224426
Epoch 820, training loss: 6.409344673156738 = 0.07199504971504211 + 1.0 * 6.3373494148254395
Epoch 820, val loss: 0.817750871181488
Epoch 830, training loss: 6.400908470153809 = 0.06887008994817734 + 1.0 * 6.332038402557373
Epoch 830, val loss: 0.8235057592391968
Epoch 840, training loss: 6.395040512084961 = 0.06592324376106262 + 1.0 * 6.329117298126221
Epoch 840, val loss: 0.829431414604187
Epoch 850, training loss: 6.399840354919434 = 0.06313682347536087 + 1.0 * 6.336703300476074
Epoch 850, val loss: 0.8352324962615967
Epoch 860, training loss: 6.388118267059326 = 0.06050210818648338 + 1.0 * 6.327616214752197
Epoch 860, val loss: 0.8409744501113892
Epoch 870, training loss: 6.3846516609191895 = 0.058014869689941406 + 1.0 * 6.326636791229248
Epoch 870, val loss: 0.8468851447105408
Epoch 880, training loss: 6.381430625915527 = 0.05565381050109863 + 1.0 * 6.32577657699585
Epoch 880, val loss: 0.8526579737663269
Epoch 890, training loss: 6.378566741943359 = 0.05342179536819458 + 1.0 * 6.3251447677612305
Epoch 890, val loss: 0.8584815263748169
Epoch 900, training loss: 6.376271724700928 = 0.05130251497030258 + 1.0 * 6.324969291687012
Epoch 900, val loss: 0.8642658591270447
Epoch 910, training loss: 6.373073101043701 = 0.04929716885089874 + 1.0 * 6.323775768280029
Epoch 910, val loss: 0.8700047731399536
Epoch 920, training loss: 6.367532730102539 = 0.04739869758486748 + 1.0 * 6.320134162902832
Epoch 920, val loss: 0.8756160140037537
Epoch 930, training loss: 6.365322589874268 = 0.04560061916708946 + 1.0 * 6.3197221755981445
Epoch 930, val loss: 0.8813142776489258
Epoch 940, training loss: 6.362257480621338 = 0.04388467222452164 + 1.0 * 6.31837272644043
Epoch 940, val loss: 0.8869489431381226
Epoch 950, training loss: 6.36824893951416 = 0.04225675389170647 + 1.0 * 6.325992107391357
Epoch 950, val loss: 0.8924655914306641
Epoch 960, training loss: 6.361635684967041 = 0.04072103649377823 + 1.0 * 6.3209147453308105
Epoch 960, val loss: 0.8981353640556335
Epoch 970, training loss: 6.356884479522705 = 0.0392582081258297 + 1.0 * 6.317626476287842
Epoch 970, val loss: 0.9035664796829224
Epoch 980, training loss: 6.3552093505859375 = 0.037872202694416046 + 1.0 * 6.3173370361328125
Epoch 980, val loss: 0.9091145992279053
Epoch 990, training loss: 6.349476337432861 = 0.036546673625707626 + 1.0 * 6.312929630279541
Epoch 990, val loss: 0.9144871234893799
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8397469688982605
The final CL Acc:0.79630, 0.01318, The final GNN Acc:0.83922, 0.00114
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9430])
updated graph: torch.Size([2, 10486])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.540403366088867 = 1.9436391592025757 + 1.0 * 8.59676456451416
Epoch 0, val loss: 1.9386652708053589
Epoch 10, training loss: 10.529922485351562 = 1.933599591255188 + 1.0 * 8.596323013305664
Epoch 10, val loss: 1.9292658567428589
Epoch 20, training loss: 10.513800621032715 = 1.921005129814148 + 1.0 * 8.592795372009277
Epoch 20, val loss: 1.9171732664108276
Epoch 30, training loss: 10.46854305267334 = 1.90317702293396 + 1.0 * 8.5653657913208
Epoch 30, val loss: 1.8999797105789185
Epoch 40, training loss: 10.262575149536133 = 1.8805506229400635 + 1.0 * 8.382024765014648
Epoch 40, val loss: 1.8791828155517578
Epoch 50, training loss: 9.81876277923584 = 1.8570077419281006 + 1.0 * 7.961755275726318
Epoch 50, val loss: 1.8580951690673828
Epoch 60, training loss: 9.214746475219727 = 1.842228889465332 + 1.0 * 7.372518062591553
Epoch 60, val loss: 1.844478726387024
Epoch 70, training loss: 8.87828254699707 = 1.8324915170669556 + 1.0 * 7.045790672302246
Epoch 70, val loss: 1.8347498178482056
Epoch 80, training loss: 8.705493927001953 = 1.8202372789382935 + 1.0 * 6.885256767272949
Epoch 80, val loss: 1.8232754468917847
Epoch 90, training loss: 8.5908784866333 = 1.8064018487930298 + 1.0 * 6.7844767570495605
Epoch 90, val loss: 1.8108314275741577
Epoch 100, training loss: 8.514054298400879 = 1.7931959629058838 + 1.0 * 6.720858573913574
Epoch 100, val loss: 1.7987538576126099
Epoch 110, training loss: 8.453300476074219 = 1.7807137966156006 + 1.0 * 6.672586917877197
Epoch 110, val loss: 1.7869017124176025
Epoch 120, training loss: 8.402949333190918 = 1.767886996269226 + 1.0 * 6.6350626945495605
Epoch 120, val loss: 1.7748762369155884
Epoch 130, training loss: 8.362173080444336 = 1.7539803981781006 + 1.0 * 6.6081929206848145
Epoch 130, val loss: 1.7622233629226685
Epoch 140, training loss: 8.321443557739258 = 1.7384042739868164 + 1.0 * 6.583038806915283
Epoch 140, val loss: 1.7486357688903809
Epoch 150, training loss: 8.283597946166992 = 1.7205740213394165 + 1.0 * 6.563024044036865
Epoch 150, val loss: 1.7333616018295288
Epoch 160, training loss: 8.245635032653809 = 1.6997768878936768 + 1.0 * 6.545857906341553
Epoch 160, val loss: 1.7158786058425903
Epoch 170, training loss: 8.206877708435059 = 1.67545485496521 + 1.0 * 6.531423091888428
Epoch 170, val loss: 1.6955596208572388
Epoch 180, training loss: 8.166229248046875 = 1.6472253799438477 + 1.0 * 6.519003391265869
Epoch 180, val loss: 1.6719502210617065
Epoch 190, training loss: 8.120818138122559 = 1.6143778562545776 + 1.0 * 6.50644063949585
Epoch 190, val loss: 1.644554615020752
Epoch 200, training loss: 8.074371337890625 = 1.5768535137176514 + 1.0 * 6.4975175857543945
Epoch 200, val loss: 1.6134059429168701
Epoch 210, training loss: 8.020919799804688 = 1.5354390144348145 + 1.0 * 6.485480308532715
Epoch 210, val loss: 1.5788408517837524
Epoch 220, training loss: 7.966131210327148 = 1.4902716875076294 + 1.0 * 6.475859642028809
Epoch 220, val loss: 1.5411359071731567
Epoch 230, training loss: 7.914987564086914 = 1.4427093267440796 + 1.0 * 6.472278118133545
Epoch 230, val loss: 1.5015588998794556
Epoch 240, training loss: 7.854760646820068 = 1.3943439722061157 + 1.0 * 6.460416793823242
Epoch 240, val loss: 1.4617459774017334
Epoch 250, training loss: 7.7995829582214355 = 1.3457037210464478 + 1.0 * 6.453879356384277
Epoch 250, val loss: 1.4219577312469482
Epoch 260, training loss: 7.745765686035156 = 1.2974469661712646 + 1.0 * 6.4483184814453125
Epoch 260, val loss: 1.3830119371414185
Epoch 270, training loss: 7.693617343902588 = 1.2509665489196777 + 1.0 * 6.44265079498291
Epoch 270, val loss: 1.346375584602356
Epoch 280, training loss: 7.644235610961914 = 1.206822395324707 + 1.0 * 6.437413215637207
Epoch 280, val loss: 1.3121836185455322
Epoch 290, training loss: 7.595643043518066 = 1.1645548343658447 + 1.0 * 6.431087970733643
Epoch 290, val loss: 1.2803122997283936
Epoch 300, training loss: 7.558507919311523 = 1.1241427659988403 + 1.0 * 6.434365272521973
Epoch 300, val loss: 1.2504452466964722
Epoch 310, training loss: 7.512889385223389 = 1.0863337516784668 + 1.0 * 6.426555633544922
Epoch 310, val loss: 1.223035454750061
Epoch 320, training loss: 7.468900680541992 = 1.050519585609436 + 1.0 * 6.418381214141846
Epoch 320, val loss: 1.1975200176239014
Epoch 330, training loss: 7.429811477661133 = 1.0161330699920654 + 1.0 * 6.4136786460876465
Epoch 330, val loss: 1.173162817955017
Epoch 340, training loss: 7.398507118225098 = 0.9828271269798279 + 1.0 * 6.415679931640625
Epoch 340, val loss: 1.1496520042419434
Epoch 350, training loss: 7.358278751373291 = 0.950515627861023 + 1.0 * 6.4077630043029785
Epoch 350, val loss: 1.127017617225647
Epoch 360, training loss: 7.32203483581543 = 0.9188153743743896 + 1.0 * 6.403219223022461
Epoch 360, val loss: 1.1048465967178345
Epoch 370, training loss: 7.286725997924805 = 0.8871046900749207 + 1.0 * 6.399621486663818
Epoch 370, val loss: 1.082586646080017
Epoch 380, training loss: 7.252681255340576 = 0.8551197648048401 + 1.0 * 6.397561550140381
Epoch 380, val loss: 1.0600520372390747
Epoch 390, training loss: 7.227025985717773 = 0.8228130340576172 + 1.0 * 6.404212951660156
Epoch 390, val loss: 1.0373038053512573
Epoch 400, training loss: 7.184389591217041 = 0.7905262112617493 + 1.0 * 6.393863201141357
Epoch 400, val loss: 1.0144981145858765
Epoch 410, training loss: 7.146803855895996 = 0.7579629421234131 + 1.0 * 6.388840675354004
Epoch 410, val loss: 0.9916583895683289
Epoch 420, training loss: 7.111022472381592 = 0.7249658703804016 + 1.0 * 6.386056423187256
Epoch 420, val loss: 0.9688217639923096
Epoch 430, training loss: 7.074966907501221 = 0.6918123364448547 + 1.0 * 6.383154392242432
Epoch 430, val loss: 0.946217954158783
Epoch 440, training loss: 7.040703296661377 = 0.6588559746742249 + 1.0 * 6.381847381591797
Epoch 440, val loss: 0.924243152141571
Epoch 450, training loss: 7.005099296569824 = 0.6260221004486084 + 1.0 * 6.379076957702637
Epoch 450, val loss: 0.9028284549713135
Epoch 460, training loss: 6.974120140075684 = 0.5933437347412109 + 1.0 * 6.380776405334473
Epoch 460, val loss: 0.8822488188743591
Epoch 470, training loss: 6.941451549530029 = 0.5613349676132202 + 1.0 * 6.3801164627075195
Epoch 470, val loss: 0.862629234790802
Epoch 480, training loss: 6.905223846435547 = 0.5299959778785706 + 1.0 * 6.375227928161621
Epoch 480, val loss: 0.8443952798843384
Epoch 490, training loss: 6.871391296386719 = 0.49950653314590454 + 1.0 * 6.371884822845459
Epoch 490, val loss: 0.8273829817771912
Epoch 500, training loss: 6.839537143707275 = 0.4699019193649292 + 1.0 * 6.369635105133057
Epoch 500, val loss: 0.8118409514427185
Epoch 510, training loss: 6.812715530395508 = 0.4413779377937317 + 1.0 * 6.371337413787842
Epoch 510, val loss: 0.7978367805480957
Epoch 520, training loss: 6.782007694244385 = 0.4142606556415558 + 1.0 * 6.367746829986572
Epoch 520, val loss: 0.7856385111808777
Epoch 530, training loss: 6.753173828125 = 0.38847631216049194 + 1.0 * 6.364697456359863
Epoch 530, val loss: 0.7750630974769592
Epoch 540, training loss: 6.73084831237793 = 0.3640996217727661 + 1.0 * 6.366748809814453
Epoch 540, val loss: 0.7660084962844849
Epoch 550, training loss: 6.705661296844482 = 0.34118419885635376 + 1.0 * 6.364477157592773
Epoch 550, val loss: 0.7585955858230591
Epoch 560, training loss: 6.680352687835693 = 0.3196723759174347 + 1.0 * 6.360680103302002
Epoch 560, val loss: 0.752441942691803
Epoch 570, training loss: 6.657120227813721 = 0.29949951171875 + 1.0 * 6.357620716094971
Epoch 570, val loss: 0.747508704662323
Epoch 580, training loss: 6.637589931488037 = 0.2805836796760559 + 1.0 * 6.357006072998047
Epoch 580, val loss: 0.7437041401863098
Epoch 590, training loss: 6.618406772613525 = 0.2628971338272095 + 1.0 * 6.3555097579956055
Epoch 590, val loss: 0.7408169507980347
Epoch 600, training loss: 6.601099491119385 = 0.24637924134731293 + 1.0 * 6.354720115661621
Epoch 600, val loss: 0.738966703414917
Epoch 610, training loss: 6.583629608154297 = 0.23093436658382416 + 1.0 * 6.352695465087891
Epoch 610, val loss: 0.737869143486023
Epoch 620, training loss: 6.571042537689209 = 0.21648651361465454 + 1.0 * 6.354556083679199
Epoch 620, val loss: 0.7374480366706848
Epoch 630, training loss: 6.552794933319092 = 0.20301926136016846 + 1.0 * 6.349775791168213
Epoch 630, val loss: 0.7376985549926758
Epoch 640, training loss: 6.536833763122559 = 0.19038060307502747 + 1.0 * 6.3464531898498535
Epoch 640, val loss: 0.7386782765388489
Epoch 650, training loss: 6.527474880218506 = 0.17856095731258392 + 1.0 * 6.34891414642334
Epoch 650, val loss: 0.7400885820388794
Epoch 660, training loss: 6.511848449707031 = 0.16751009225845337 + 1.0 * 6.344338417053223
Epoch 660, val loss: 0.7419076561927795
Epoch 670, training loss: 6.516587257385254 = 0.15722106397151947 + 1.0 * 6.359366416931152
Epoch 670, val loss: 0.7441890239715576
Epoch 680, training loss: 6.491000175476074 = 0.1476835161447525 + 1.0 * 6.343316555023193
Epoch 680, val loss: 0.7469106316566467
Epoch 690, training loss: 6.477909088134766 = 0.1387840062379837 + 1.0 * 6.339125156402588
Epoch 690, val loss: 0.7499939203262329
Epoch 700, training loss: 6.466828346252441 = 0.1304583102464676 + 1.0 * 6.33636999130249
Epoch 700, val loss: 0.753370463848114
Epoch 710, training loss: 6.459757328033447 = 0.12267293781042099 + 1.0 * 6.3370842933654785
Epoch 710, val loss: 0.7570911645889282
Epoch 720, training loss: 6.454233169555664 = 0.11543750762939453 + 1.0 * 6.3387956619262695
Epoch 720, val loss: 0.7609751224517822
Epoch 730, training loss: 6.444655895233154 = 0.1087474450469017 + 1.0 * 6.33590841293335
Epoch 730, val loss: 0.7652518153190613
Epoch 740, training loss: 6.433832168579102 = 0.10253684967756271 + 1.0 * 6.331295490264893
Epoch 740, val loss: 0.7697659730911255
Epoch 750, training loss: 6.427374839782715 = 0.09674757719039917 + 1.0 * 6.33062744140625
Epoch 750, val loss: 0.7744870185852051
Epoch 760, training loss: 6.421253681182861 = 0.0913473591208458 + 1.0 * 6.329906463623047
Epoch 760, val loss: 0.7794008255004883
Epoch 770, training loss: 6.422292709350586 = 0.08631055802106857 + 1.0 * 6.335982322692871
Epoch 770, val loss: 0.7844766974449158
Epoch 780, training loss: 6.411554336547852 = 0.08164054900407791 + 1.0 * 6.32991361618042
Epoch 780, val loss: 0.7896308302879333
Epoch 790, training loss: 6.404171466827393 = 0.07729560136795044 + 1.0 * 6.326875686645508
Epoch 790, val loss: 0.7949776649475098
Epoch 800, training loss: 6.398794174194336 = 0.07324719429016113 + 1.0 * 6.325547218322754
Epoch 800, val loss: 0.8005074262619019
Epoch 810, training loss: 6.3928117752075195 = 0.06946907192468643 + 1.0 * 6.323342800140381
Epoch 810, val loss: 0.806044340133667
Epoch 820, training loss: 6.395086765289307 = 0.06593551486730576 + 1.0 * 6.329151153564453
Epoch 820, val loss: 0.8116798400878906
Epoch 830, training loss: 6.392274379730225 = 0.06264365464448929 + 1.0 * 6.3296308517456055
Epoch 830, val loss: 0.81741863489151
Epoch 840, training loss: 6.380419731140137 = 0.059602994471788406 + 1.0 * 6.320816516876221
Epoch 840, val loss: 0.8230879902839661
Epoch 850, training loss: 6.376763343811035 = 0.056754786521196365 + 1.0 * 6.320008754730225
Epoch 850, val loss: 0.828916072845459
Epoch 860, training loss: 6.3724751472473145 = 0.05408144369721413 + 1.0 * 6.318393707275391
Epoch 860, val loss: 0.834648072719574
Epoch 870, training loss: 6.372560977935791 = 0.051582202315330505 + 1.0 * 6.32097864151001
Epoch 870, val loss: 0.8403565287590027
Epoch 880, training loss: 6.369513034820557 = 0.049239784479141235 + 1.0 * 6.320273399353027
Epoch 880, val loss: 0.8462001085281372
Epoch 890, training loss: 6.3631272315979 = 0.04705798253417015 + 1.0 * 6.31606912612915
Epoch 890, val loss: 0.8518604636192322
Epoch 900, training loss: 6.36056661605835 = 0.045014459639787674 + 1.0 * 6.315552234649658
Epoch 900, val loss: 0.8576319813728333
Epoch 910, training loss: 6.357177257537842 = 0.04309508576989174 + 1.0 * 6.314082145690918
Epoch 910, val loss: 0.863318145275116
Epoch 920, training loss: 6.355934143066406 = 0.04128926992416382 + 1.0 * 6.314644813537598
Epoch 920, val loss: 0.8689461350440979
Epoch 930, training loss: 6.354430675506592 = 0.039595767855644226 + 1.0 * 6.314835071563721
Epoch 930, val loss: 0.874562680721283
Epoch 940, training loss: 6.348466873168945 = 0.03800148516893387 + 1.0 * 6.310465335845947
Epoch 940, val loss: 0.8802858591079712
Epoch 950, training loss: 6.347195148468018 = 0.03649688512086868 + 1.0 * 6.31069803237915
Epoch 950, val loss: 0.8858886957168579
Epoch 960, training loss: 6.345527172088623 = 0.035075459629297256 + 1.0 * 6.310451507568359
Epoch 960, val loss: 0.8914312124252319
Epoch 970, training loss: 6.344820022583008 = 0.03373660519719124 + 1.0 * 6.3110833168029785
Epoch 970, val loss: 0.8968430757522583
Epoch 980, training loss: 6.343822002410889 = 0.032472725957632065 + 1.0 * 6.311349391937256
Epoch 980, val loss: 0.9022672176361084
Epoch 990, training loss: 6.339914798736572 = 0.031285300850868225 + 1.0 * 6.308629512786865
Epoch 990, val loss: 0.9076389074325562
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 10.548905372619629 = 1.9520905017852783 + 1.0 * 8.59681510925293
Epoch 0, val loss: 1.9469859600067139
Epoch 10, training loss: 10.537919998168945 = 1.9414403438568115 + 1.0 * 8.596479415893555
Epoch 10, val loss: 1.93667733669281
Epoch 20, training loss: 10.522076606750488 = 1.928132176399231 + 1.0 * 8.593944549560547
Epoch 20, val loss: 1.92332923412323
Epoch 30, training loss: 10.484820365905762 = 1.9094852209091187 + 1.0 * 8.575335502624512
Epoch 30, val loss: 1.904195785522461
Epoch 40, training loss: 10.340397834777832 = 1.8854434490203857 + 1.0 * 8.454954147338867
Epoch 40, val loss: 1.8806843757629395
Epoch 50, training loss: 9.943745613098145 = 1.8597261905670166 + 1.0 * 8.084019660949707
Epoch 50, val loss: 1.8562489748001099
Epoch 60, training loss: 9.434684753417969 = 1.8409186601638794 + 1.0 * 7.593765735626221
Epoch 60, val loss: 1.8399677276611328
Epoch 70, training loss: 9.009635925292969 = 1.8286212682724 + 1.0 * 7.1810150146484375
Epoch 70, val loss: 1.8287495374679565
Epoch 80, training loss: 8.828582763671875 = 1.8170101642608643 + 1.0 * 7.011572360992432
Epoch 80, val loss: 1.8180328607559204
Epoch 90, training loss: 8.698877334594727 = 1.8029781579971313 + 1.0 * 6.895899295806885
Epoch 90, val loss: 1.805783748626709
Epoch 100, training loss: 8.602201461791992 = 1.7897586822509766 + 1.0 * 6.812442302703857
Epoch 100, val loss: 1.794475793838501
Epoch 110, training loss: 8.533366203308105 = 1.7779752016067505 + 1.0 * 6.755390644073486
Epoch 110, val loss: 1.784181833267212
Epoch 120, training loss: 8.479608535766602 = 1.7660576105117798 + 1.0 * 6.713551044464111
Epoch 120, val loss: 1.7738087177276611
Epoch 130, training loss: 8.432123184204102 = 1.7529888153076172 + 1.0 * 6.679134368896484
Epoch 130, val loss: 1.7628546953201294
Epoch 140, training loss: 8.385637283325195 = 1.7384166717529297 + 1.0 * 6.647220134735107
Epoch 140, val loss: 1.7508281469345093
Epoch 150, training loss: 8.34199047088623 = 1.7218724489212036 + 1.0 * 6.620117664337158
Epoch 150, val loss: 1.7373926639556885
Epoch 160, training loss: 8.297764778137207 = 1.703080177307129 + 1.0 * 6.594684600830078
Epoch 160, val loss: 1.7222561836242676
Epoch 170, training loss: 8.259472846984863 = 1.6813205480575562 + 1.0 * 6.578152179718018
Epoch 170, val loss: 1.7048022747039795
Epoch 180, training loss: 8.209588050842285 = 1.6564034223556519 + 1.0 * 6.553184509277344
Epoch 180, val loss: 1.6848902702331543
Epoch 190, training loss: 8.163948059082031 = 1.6279486417770386 + 1.0 * 6.535999298095703
Epoch 190, val loss: 1.6620819568634033
Epoch 200, training loss: 8.115911483764648 = 1.595335841178894 + 1.0 * 6.520575523376465
Epoch 200, val loss: 1.635872721672058
Epoch 210, training loss: 8.067123413085938 = 1.5590906143188477 + 1.0 * 6.508033275604248
Epoch 210, val loss: 1.6068882942199707
Epoch 220, training loss: 8.015730857849121 = 1.5197608470916748 + 1.0 * 6.495969772338867
Epoch 220, val loss: 1.5752140283584595
Epoch 230, training loss: 7.963083267211914 = 1.4773061275482178 + 1.0 * 6.485776901245117
Epoch 230, val loss: 1.5412973165512085
Epoch 240, training loss: 7.909664630889893 = 1.4332507848739624 + 1.0 * 6.476413726806641
Epoch 240, val loss: 1.5063271522521973
Epoch 250, training loss: 7.85561990737915 = 1.388534426689148 + 1.0 * 6.467085361480713
Epoch 250, val loss: 1.4711689949035645
Epoch 260, training loss: 7.809682846069336 = 1.3433505296707153 + 1.0 * 6.46633243560791
Epoch 260, val loss: 1.4361287355422974
Epoch 270, training loss: 7.753879070281982 = 1.2991098165512085 + 1.0 * 6.454769134521484
Epoch 270, val loss: 1.4023447036743164
Epoch 280, training loss: 7.7020368576049805 = 1.255573034286499 + 1.0 * 6.446463584899902
Epoch 280, val loss: 1.3694267272949219
Epoch 290, training loss: 7.6599345207214355 = 1.212754726409912 + 1.0 * 6.447179794311523
Epoch 290, val loss: 1.3373295068740845
Epoch 300, training loss: 7.607755661010742 = 1.1714471578598022 + 1.0 * 6.43630838394165
Epoch 300, val loss: 1.3066591024398804
Epoch 310, training loss: 7.561365127563477 = 1.1310102939605713 + 1.0 * 6.430355072021484
Epoch 310, val loss: 1.2765932083129883
Epoch 320, training loss: 7.517779350280762 = 1.0912829637527466 + 1.0 * 6.426496505737305
Epoch 320, val loss: 1.2469977140426636
Epoch 330, training loss: 7.47877311706543 = 1.0526585578918457 + 1.0 * 6.426114559173584
Epoch 330, val loss: 1.2179495096206665
Epoch 340, training loss: 7.433234214782715 = 1.01493239402771 + 1.0 * 6.418302059173584
Epoch 340, val loss: 1.1895015239715576
Epoch 350, training loss: 7.391140460968018 = 0.9776853919029236 + 1.0 * 6.413455009460449
Epoch 350, val loss: 1.1610699892044067
Epoch 360, training loss: 7.3509721755981445 = 0.9408407807350159 + 1.0 * 6.410131454467773
Epoch 360, val loss: 1.1325761079788208
Epoch 370, training loss: 7.31179666519165 = 0.9044488668441772 + 1.0 * 6.407347679138184
Epoch 370, val loss: 1.1043047904968262
Epoch 380, training loss: 7.274532318115234 = 0.8682408332824707 + 1.0 * 6.406291484832764
Epoch 380, val loss: 1.075910210609436
Epoch 390, training loss: 7.234101295471191 = 0.8322655558586121 + 1.0 * 6.401835918426514
Epoch 390, val loss: 1.0476449728012085
Epoch 400, training loss: 7.193422317504883 = 0.7961598038673401 + 1.0 * 6.3972625732421875
Epoch 400, val loss: 1.0192887783050537
Epoch 410, training loss: 7.1629767417907715 = 0.7601892352104187 + 1.0 * 6.402787685394287
Epoch 410, val loss: 0.9911120533943176
Epoch 420, training loss: 7.118877410888672 = 0.7249577045440674 + 1.0 * 6.393919467926025
Epoch 420, val loss: 0.9638873338699341
Epoch 430, training loss: 7.081789970397949 = 0.6904900670051575 + 1.0 * 6.391299724578857
Epoch 430, val loss: 0.937768280506134
Epoch 440, training loss: 7.04533576965332 = 0.657137393951416 + 1.0 * 6.388198375701904
Epoch 440, val loss: 0.9131520986557007
Epoch 450, training loss: 7.010455131530762 = 0.6251223087310791 + 1.0 * 6.3853325843811035
Epoch 450, val loss: 0.8903382420539856
Epoch 460, training loss: 6.979796409606934 = 0.5943871140480042 + 1.0 * 6.385409355163574
Epoch 460, val loss: 0.8693457245826721
Epoch 470, training loss: 6.946882724761963 = 0.5652265548706055 + 1.0 * 6.381656169891357
Epoch 470, val loss: 0.8503325581550598
Epoch 480, training loss: 6.915067195892334 = 0.5374594330787659 + 1.0 * 6.377607822418213
Epoch 480, val loss: 0.8334123492240906
Epoch 490, training loss: 6.886041164398193 = 0.5110573768615723 + 1.0 * 6.374983787536621
Epoch 490, val loss: 0.8183207511901855
Epoch 500, training loss: 6.8584489822387695 = 0.48569798469543457 + 1.0 * 6.372751235961914
Epoch 500, val loss: 0.8046589493751526
Epoch 510, training loss: 6.8394317626953125 = 0.4612858295440674 + 1.0 * 6.378145694732666
Epoch 510, val loss: 0.7923780679702759
Epoch 520, training loss: 6.80989408493042 = 0.43796518445014954 + 1.0 * 6.371928691864014
Epoch 520, val loss: 0.7816038131713867
Epoch 530, training loss: 6.78261661529541 = 0.4156556725502014 + 1.0 * 6.3669610023498535
Epoch 530, val loss: 0.7721609473228455
Epoch 540, training loss: 6.758755683898926 = 0.3940969407558441 + 1.0 * 6.364658832550049
Epoch 540, val loss: 0.7637314796447754
Epoch 550, training loss: 6.746290683746338 = 0.3733334541320801 + 1.0 * 6.372957229614258
Epoch 550, val loss: 0.7561469078063965
Epoch 560, training loss: 6.714519500732422 = 0.3534639775753021 + 1.0 * 6.361055374145508
Epoch 560, val loss: 0.7496721148490906
Epoch 570, training loss: 6.693859577178955 = 0.33447885513305664 + 1.0 * 6.359380722045898
Epoch 570, val loss: 0.7442228198051453
Epoch 580, training loss: 6.680415630340576 = 0.31640610098838806 + 1.0 * 6.364009380340576
Epoch 580, val loss: 0.7397440671920776
Epoch 590, training loss: 6.660693168640137 = 0.2992728352546692 + 1.0 * 6.361420154571533
Epoch 590, val loss: 0.7361600399017334
Epoch 600, training loss: 6.640360355377197 = 0.28312018513679504 + 1.0 * 6.357240200042725
Epoch 600, val loss: 0.7337061762809753
Epoch 610, training loss: 6.622121334075928 = 0.26777347922325134 + 1.0 * 6.3543477058410645
Epoch 610, val loss: 0.7320665717124939
Epoch 620, training loss: 6.603390693664551 = 0.25311678647994995 + 1.0 * 6.350274085998535
Epoch 620, val loss: 0.7312331795692444
Epoch 630, training loss: 6.594663143157959 = 0.2390490472316742 + 1.0 * 6.355614185333252
Epoch 630, val loss: 0.7310981154441833
Epoch 640, training loss: 6.578829765319824 = 0.22557252645492554 + 1.0 * 6.353257179260254
Epoch 640, val loss: 0.7315759658813477
Epoch 650, training loss: 6.5595574378967285 = 0.21263708174228668 + 1.0 * 6.346920490264893
Epoch 650, val loss: 0.7327024936676025
Epoch 660, training loss: 6.545553684234619 = 0.20021070539951324 + 1.0 * 6.345343112945557
Epoch 660, val loss: 0.7344697117805481
Epoch 670, training loss: 6.5332207679748535 = 0.18826764822006226 + 1.0 * 6.3449530601501465
Epoch 670, val loss: 0.7366911172866821
Epoch 680, training loss: 6.5210700035095215 = 0.17685401439666748 + 1.0 * 6.3442158699035645
Epoch 680, val loss: 0.7393483519554138
Epoch 690, training loss: 6.512485980987549 = 0.16605046391487122 + 1.0 * 6.346435546875
Epoch 690, val loss: 0.742682933807373
Epoch 700, training loss: 6.495942115783691 = 0.15588517487049103 + 1.0 * 6.340056896209717
Epoch 700, val loss: 0.7461980581283569
Epoch 710, training loss: 6.484918117523193 = 0.14635105431079865 + 1.0 * 6.33856725692749
Epoch 710, val loss: 0.7503198981285095
Epoch 720, training loss: 6.474135398864746 = 0.13737580180168152 + 1.0 * 6.336759567260742
Epoch 720, val loss: 0.7546185851097107
Epoch 730, training loss: 6.475608825683594 = 0.12899544835090637 + 1.0 * 6.34661340713501
Epoch 730, val loss: 0.7592032551765442
Epoch 740, training loss: 6.458831310272217 = 0.12125148624181747 + 1.0 * 6.337579727172852
Epoch 740, val loss: 0.7639341950416565
Epoch 750, training loss: 6.4467973709106445 = 0.11407957971096039 + 1.0 * 6.3327178955078125
Epoch 750, val loss: 0.7688995599746704
Epoch 760, training loss: 6.440340042114258 = 0.10743461549282074 + 1.0 * 6.332905292510986
Epoch 760, val loss: 0.7738310694694519
Epoch 770, training loss: 6.433708667755127 = 0.10127461701631546 + 1.0 * 6.332434177398682
Epoch 770, val loss: 0.7788003087043762
Epoch 780, training loss: 6.431519508361816 = 0.09558774530887604 + 1.0 * 6.335931777954102
Epoch 780, val loss: 0.7838000655174255
Epoch 790, training loss: 6.4249162673950195 = 0.09035312384366989 + 1.0 * 6.334563255310059
Epoch 790, val loss: 0.7887766361236572
Epoch 800, training loss: 6.412417888641357 = 0.08547590672969818 + 1.0 * 6.326941967010498
Epoch 800, val loss: 0.7939011454582214
Epoch 810, training loss: 6.4064202308654785 = 0.08092135190963745 + 1.0 * 6.325499057769775
Epoch 810, val loss: 0.7989637851715088
Epoch 820, training loss: 6.406561851501465 = 0.07662668079137802 + 1.0 * 6.329935073852539
Epoch 820, val loss: 0.8040706515312195
Epoch 830, training loss: 6.403146266937256 = 0.07258438318967819 + 1.0 * 6.330562114715576
Epoch 830, val loss: 0.8090689182281494
Epoch 840, training loss: 6.392982006072998 = 0.06878005713224411 + 1.0 * 6.324202060699463
Epoch 840, val loss: 0.8141851425170898
Epoch 850, training loss: 6.388084888458252 = 0.06518440693616867 + 1.0 * 6.322900295257568
Epoch 850, val loss: 0.8193317651748657
Epoch 860, training loss: 6.384572982788086 = 0.06180896982550621 + 1.0 * 6.322763919830322
Epoch 860, val loss: 0.824431300163269
Epoch 870, training loss: 6.377820014953613 = 0.058644216507673264 + 1.0 * 6.319175720214844
Epoch 870, val loss: 0.8296675086021423
Epoch 880, training loss: 6.374048709869385 = 0.05569188669323921 + 1.0 * 6.318356990814209
Epoch 880, val loss: 0.834923267364502
Epoch 890, training loss: 6.37827205657959 = 0.05294191464781761 + 1.0 * 6.3253302574157715
Epoch 890, val loss: 0.8402090072631836
Epoch 900, training loss: 6.369765281677246 = 0.05041778087615967 + 1.0 * 6.319347381591797
Epoch 900, val loss: 0.8454527258872986
Epoch 910, training loss: 6.374424934387207 = 0.048071108758449554 + 1.0 * 6.326354026794434
Epoch 910, val loss: 0.8508532047271729
Epoch 920, training loss: 6.362239360809326 = 0.045884981751441956 + 1.0 * 6.316354274749756
Epoch 920, val loss: 0.8561186194419861
Epoch 930, training loss: 6.358068943023682 = 0.043849699199199677 + 1.0 * 6.3142194747924805
Epoch 930, val loss: 0.8614507913589478
Epoch 940, training loss: 6.354619979858398 = 0.041931286454200745 + 1.0 * 6.312688827514648
Epoch 940, val loss: 0.8667161464691162
Epoch 950, training loss: 6.362026691436768 = 0.04012762010097504 + 1.0 * 6.321898937225342
Epoch 950, val loss: 0.8718706369400024
Epoch 960, training loss: 6.3522443771362305 = 0.03845658898353577 + 1.0 * 6.313787937164307
Epoch 960, val loss: 0.8771216869354248
Epoch 970, training loss: 6.353516101837158 = 0.03687319532036781 + 1.0 * 6.316642761230469
Epoch 970, val loss: 0.8822062015533447
Epoch 980, training loss: 6.345595359802246 = 0.03539025038480759 + 1.0 * 6.310204982757568
Epoch 980, val loss: 0.8871006369590759
Epoch 990, training loss: 6.342237949371338 = 0.03399268165230751 + 1.0 * 6.3082451820373535
Epoch 990, val loss: 0.8920871019363403
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 10.534830093383789 = 1.9379801750183105 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.9416779279708862
Epoch 10, training loss: 10.525553703308105 = 1.9289368391036987 + 1.0 * 8.596616744995117
Epoch 10, val loss: 1.9324134588241577
Epoch 20, training loss: 10.512503623962402 = 1.9174015522003174 + 1.0 * 8.595102310180664
Epoch 20, val loss: 1.9205814599990845
Epoch 30, training loss: 10.484200477600098 = 1.9009501934051514 + 1.0 * 8.583250045776367
Epoch 30, val loss: 1.9038881063461304
Epoch 40, training loss: 10.365443229675293 = 1.8783495426177979 + 1.0 * 8.487093925476074
Epoch 40, val loss: 1.8819540739059448
Epoch 50, training loss: 9.718843460083008 = 1.8559893369674683 + 1.0 * 7.86285400390625
Epoch 50, val loss: 1.8615503311157227
Epoch 60, training loss: 9.38350772857666 = 1.8383824825286865 + 1.0 * 7.5451250076293945
Epoch 60, val loss: 1.845055341720581
Epoch 70, training loss: 9.025163650512695 = 1.8255454301834106 + 1.0 * 7.199618339538574
Epoch 70, val loss: 1.8327043056488037
Epoch 80, training loss: 8.785406112670898 = 1.8131592273712158 + 1.0 * 6.972247123718262
Epoch 80, val loss: 1.821891188621521
Epoch 90, training loss: 8.67094612121582 = 1.7995550632476807 + 1.0 * 6.8713908195495605
Epoch 90, val loss: 1.8098233938217163
Epoch 100, training loss: 8.584390640258789 = 1.784996509552002 + 1.0 * 6.799394130706787
Epoch 100, val loss: 1.7969334125518799
Epoch 110, training loss: 8.516563415527344 = 1.7703795433044434 + 1.0 * 6.746184349060059
Epoch 110, val loss: 1.784132480621338
Epoch 120, training loss: 8.458062171936035 = 1.7556278705596924 + 1.0 * 6.702434539794922
Epoch 120, val loss: 1.771156907081604
Epoch 130, training loss: 8.405391693115234 = 1.739946961402893 + 1.0 * 6.665444374084473
Epoch 130, val loss: 1.7574740648269653
Epoch 140, training loss: 8.357065200805664 = 1.7220276594161987 + 1.0 * 6.635037899017334
Epoch 140, val loss: 1.7421339750289917
Epoch 150, training loss: 8.311758995056152 = 1.700847864151001 + 1.0 * 6.610910892486572
Epoch 150, val loss: 1.7243421077728271
Epoch 160, training loss: 8.26760482788086 = 1.6759226322174072 + 1.0 * 6.591681957244873
Epoch 160, val loss: 1.7035913467407227
Epoch 170, training loss: 8.220738410949707 = 1.6467589139938354 + 1.0 * 6.573979377746582
Epoch 170, val loss: 1.6793615818023682
Epoch 180, training loss: 8.170883178710938 = 1.6124954223632812 + 1.0 * 6.558387279510498
Epoch 180, val loss: 1.650708556175232
Epoch 190, training loss: 8.119416236877441 = 1.5724999904632568 + 1.0 * 6.546916484832764
Epoch 190, val loss: 1.6172913312911987
Epoch 200, training loss: 8.062243461608887 = 1.5278186798095703 + 1.0 * 6.534424781799316
Epoch 200, val loss: 1.580146074295044
Epoch 210, training loss: 8.000805854797363 = 1.4791401624679565 + 1.0 * 6.521666049957275
Epoch 210, val loss: 1.539635419845581
Epoch 220, training loss: 7.939316272735596 = 1.4271163940429688 + 1.0 * 6.512199878692627
Epoch 220, val loss: 1.4965449571609497
Epoch 230, training loss: 7.880420684814453 = 1.3738443851470947 + 1.0 * 6.506576061248779
Epoch 230, val loss: 1.4527746438980103
Epoch 240, training loss: 7.816768646240234 = 1.321529746055603 + 1.0 * 6.495238780975342
Epoch 240, val loss: 1.4103702306747437
Epoch 250, training loss: 7.756321907043457 = 1.2704828977584839 + 1.0 * 6.485838890075684
Epoch 250, val loss: 1.3696008920669556
Epoch 260, training loss: 7.701530933380127 = 1.2211098670959473 + 1.0 * 6.48042106628418
Epoch 260, val loss: 1.3307985067367554
Epoch 270, training loss: 7.648820400238037 = 1.1742067337036133 + 1.0 * 6.474613666534424
Epoch 270, val loss: 1.2946901321411133
Epoch 280, training loss: 7.595179557800293 = 1.1296041011810303 + 1.0 * 6.465575695037842
Epoch 280, val loss: 1.2608455419540405
Epoch 290, training loss: 7.550639629364014 = 1.0866748094558716 + 1.0 * 6.463964939117432
Epoch 290, val loss: 1.228829026222229
Epoch 300, training loss: 7.500842571258545 = 1.045765995979309 + 1.0 * 6.455076694488525
Epoch 300, val loss: 1.1987725496292114
Epoch 310, training loss: 7.456244468688965 = 1.0064160823822021 + 1.0 * 6.449828624725342
Epoch 310, val loss: 1.1703541278839111
Epoch 320, training loss: 7.412467956542969 = 0.9682216644287109 + 1.0 * 6.444246292114258
Epoch 320, val loss: 1.1430085897445679
Epoch 330, training loss: 7.376133918762207 = 0.931159257888794 + 1.0 * 6.444974899291992
Epoch 330, val loss: 1.1168278455734253
Epoch 340, training loss: 7.33168888092041 = 0.8957545757293701 + 1.0 * 6.435934543609619
Epoch 340, val loss: 1.0922410488128662
Epoch 350, training loss: 7.293231964111328 = 0.8615179061889648 + 1.0 * 6.431714057922363
Epoch 350, val loss: 1.068729043006897
Epoch 360, training loss: 7.2571187019348145 = 0.8281396627426147 + 1.0 * 6.42897891998291
Epoch 360, val loss: 1.0462509393692017
Epoch 370, training loss: 7.224851131439209 = 0.7958640456199646 + 1.0 * 6.4289870262146
Epoch 370, val loss: 1.0250165462493896
Epoch 380, training loss: 7.185836315155029 = 0.7648220658302307 + 1.0 * 6.421014308929443
Epoch 380, val loss: 1.0049653053283691
Epoch 390, training loss: 7.151191234588623 = 0.7346393465995789 + 1.0 * 6.4165520668029785
Epoch 390, val loss: 0.9859698414802551
Epoch 400, training loss: 7.127314567565918 = 0.7055463194847107 + 1.0 * 6.4217681884765625
Epoch 400, val loss: 0.9682744741439819
Epoch 410, training loss: 7.091965675354004 = 0.6778832674026489 + 1.0 * 6.4140825271606445
Epoch 410, val loss: 0.9521486759185791
Epoch 420, training loss: 7.057966709136963 = 0.6513213515281677 + 1.0 * 6.40664529800415
Epoch 420, val loss: 0.9372074007987976
Epoch 430, training loss: 7.029366493225098 = 0.6256524324417114 + 1.0 * 6.403714179992676
Epoch 430, val loss: 0.92348313331604
Epoch 440, training loss: 7.0038676261901855 = 0.6008055806159973 + 1.0 * 6.403061866760254
Epoch 440, val loss: 0.9109358787536621
Epoch 450, training loss: 6.983191967010498 = 0.5770784020423889 + 1.0 * 6.406113624572754
Epoch 450, val loss: 0.8997241258621216
Epoch 460, training loss: 6.953658103942871 = 0.5544028878211975 + 1.0 * 6.399255275726318
Epoch 460, val loss: 0.8898066282272339
Epoch 470, training loss: 6.929998397827148 = 0.5325807929039001 + 1.0 * 6.3974175453186035
Epoch 470, val loss: 0.8809788823127747
Epoch 480, training loss: 6.901884078979492 = 0.5115348696708679 + 1.0 * 6.390349388122559
Epoch 480, val loss: 0.8732741475105286
Epoch 490, training loss: 6.88720703125 = 0.4911555349826813 + 1.0 * 6.396051406860352
Epoch 490, val loss: 0.8665242195129395
Epoch 500, training loss: 6.861680507659912 = 0.4715868830680847 + 1.0 * 6.390093803405762
Epoch 500, val loss: 0.8607436418533325
Epoch 510, training loss: 6.838202476501465 = 0.45275720953941345 + 1.0 * 6.3854451179504395
Epoch 510, val loss: 0.8557361960411072
Epoch 520, training loss: 6.816103935241699 = 0.4345124065876007 + 1.0 * 6.381591320037842
Epoch 520, val loss: 0.8515597581863403
Epoch 530, training loss: 6.799902439117432 = 0.41678979992866516 + 1.0 * 6.38311243057251
Epoch 530, val loss: 0.8480873107910156
Epoch 540, training loss: 6.783178329467773 = 0.39960047602653503 + 1.0 * 6.383577823638916
Epoch 540, val loss: 0.8453415632247925
Epoch 550, training loss: 6.763940334320068 = 0.38305243849754333 + 1.0 * 6.380887985229492
Epoch 550, val loss: 0.8430881500244141
Epoch 560, training loss: 6.740929126739502 = 0.3670211434364319 + 1.0 * 6.373908042907715
Epoch 560, val loss: 0.8414626121520996
Epoch 570, training loss: 6.728140354156494 = 0.35139358043670654 + 1.0 * 6.376746654510498
Epoch 570, val loss: 0.840428352355957
Epoch 580, training loss: 6.707063674926758 = 0.3363002836704254 + 1.0 * 6.370763301849365
Epoch 580, val loss: 0.8399953842163086
Epoch 590, training loss: 6.692890167236328 = 0.32164809107780457 + 1.0 * 6.371242046356201
Epoch 590, val loss: 0.8401186466217041
Epoch 600, training loss: 6.67666482925415 = 0.30748698115348816 + 1.0 * 6.36917781829834
Epoch 600, val loss: 0.8407880663871765
Epoch 610, training loss: 6.66123628616333 = 0.29380103945732117 + 1.0 * 6.367435455322266
Epoch 610, val loss: 0.8420829772949219
Epoch 620, training loss: 6.6468000411987305 = 0.2805745601654053 + 1.0 * 6.366225242614746
Epoch 620, val loss: 0.8438903093338013
Epoch 630, training loss: 6.636190891265869 = 0.267825186252594 + 1.0 * 6.36836576461792
Epoch 630, val loss: 0.8462995290756226
Epoch 640, training loss: 6.621070861816406 = 0.255620539188385 + 1.0 * 6.365450382232666
Epoch 640, val loss: 0.8492259979248047
Epoch 650, training loss: 6.603077411651611 = 0.24388504028320312 + 1.0 * 6.359192371368408
Epoch 650, val loss: 0.8525460362434387
Epoch 660, training loss: 6.589749336242676 = 0.23260749876499176 + 1.0 * 6.357141971588135
Epoch 660, val loss: 0.8564220070838928
Epoch 670, training loss: 6.5974273681640625 = 0.22177281975746155 + 1.0 * 6.375654697418213
Epoch 670, val loss: 0.8607712984085083
Epoch 680, training loss: 6.567081928253174 = 0.21145769953727722 + 1.0 * 6.355624198913574
Epoch 680, val loss: 0.865500807762146
Epoch 690, training loss: 6.554267883300781 = 0.20158804953098297 + 1.0 * 6.35267972946167
Epoch 690, val loss: 0.8705943822860718
Epoch 700, training loss: 6.548660755157471 = 0.19211824238300323 + 1.0 * 6.356542587280273
Epoch 700, val loss: 0.8760862350463867
Epoch 710, training loss: 6.53984260559082 = 0.1831308901309967 + 1.0 * 6.3567118644714355
Epoch 710, val loss: 0.8819132447242737
Epoch 720, training loss: 6.525318145751953 = 0.17457659542560577 + 1.0 * 6.350741386413574
Epoch 720, val loss: 0.8879336714744568
Epoch 730, training loss: 6.51411247253418 = 0.16641104221343994 + 1.0 * 6.347701549530029
Epoch 730, val loss: 0.8943660259246826
Epoch 740, training loss: 6.511470794677734 = 0.15861305594444275 + 1.0 * 6.35285758972168
Epoch 740, val loss: 0.9010582566261292
Epoch 750, training loss: 6.497179985046387 = 0.15122033655643463 + 1.0 * 6.345959663391113
Epoch 750, val loss: 0.9078635573387146
Epoch 760, training loss: 6.490338325500488 = 0.1441873013973236 + 1.0 * 6.346150875091553
Epoch 760, val loss: 0.9149659872055054
Epoch 770, training loss: 6.482860088348389 = 0.1375143826007843 + 1.0 * 6.345345497131348
Epoch 770, val loss: 0.9222243428230286
Epoch 780, training loss: 6.47321081161499 = 0.13119304180145264 + 1.0 * 6.342017650604248
Epoch 780, val loss: 0.9296830296516418
Epoch 790, training loss: 6.467893123626709 = 0.12517815828323364 + 1.0 * 6.342714786529541
Epoch 790, val loss: 0.9372895956039429
Epoch 800, training loss: 6.469048023223877 = 0.11946636438369751 + 1.0 * 6.349581718444824
Epoch 800, val loss: 0.9449634552001953
Epoch 810, training loss: 6.455924987792969 = 0.11408931761980057 + 1.0 * 6.3418354988098145
Epoch 810, val loss: 0.9527696371078491
Epoch 820, training loss: 6.450473785400391 = 0.10899229347705841 + 1.0 * 6.341481685638428
Epoch 820, val loss: 0.9607170820236206
Epoch 830, training loss: 6.4412078857421875 = 0.10414409637451172 + 1.0 * 6.337063789367676
Epoch 830, val loss: 0.9687201380729675
Epoch 840, training loss: 6.440798282623291 = 0.09954597055912018 + 1.0 * 6.341252326965332
Epoch 840, val loss: 0.9768567085266113
Epoch 850, training loss: 6.436947345733643 = 0.09520096331834793 + 1.0 * 6.3417463302612305
Epoch 850, val loss: 0.9849748611450195
Epoch 860, training loss: 6.426896095275879 = 0.09107846766710281 + 1.0 * 6.335817813873291
Epoch 860, val loss: 0.9930893778800964
Epoch 870, training loss: 6.421440601348877 = 0.08716640621423721 + 1.0 * 6.3342742919921875
Epoch 870, val loss: 1.0013803243637085
Epoch 880, training loss: 6.421605587005615 = 0.08345134556293488 + 1.0 * 6.338154315948486
Epoch 880, val loss: 1.00962233543396
Epoch 890, training loss: 6.415199279785156 = 0.07994961738586426 + 1.0 * 6.335249423980713
Epoch 890, val loss: 1.0177888870239258
Epoch 900, training loss: 6.4054741859436035 = 0.07661177963018417 + 1.0 * 6.328862190246582
Epoch 900, val loss: 1.0261108875274658
Epoch 910, training loss: 6.403974533081055 = 0.07343326508998871 + 1.0 * 6.330541133880615
Epoch 910, val loss: 1.034496545791626
Epoch 920, training loss: 6.402435302734375 = 0.07041428238153458 + 1.0 * 6.332021236419678
Epoch 920, val loss: 1.0427531003952026
Epoch 930, training loss: 6.394711494445801 = 0.06755729764699936 + 1.0 * 6.327154159545898
Epoch 930, val loss: 1.0509909391403198
Epoch 940, training loss: 6.390170574188232 = 0.06483127921819687 + 1.0 * 6.325339317321777
Epoch 940, val loss: 1.0593242645263672
Epoch 950, training loss: 6.389519691467285 = 0.0622318871319294 + 1.0 * 6.327287673950195
Epoch 950, val loss: 1.0676430463790894
Epoch 960, training loss: 6.386250972747803 = 0.05976192280650139 + 1.0 * 6.326488971710205
Epoch 960, val loss: 1.0758460760116577
Epoch 970, training loss: 6.380767345428467 = 0.05741824582219124 + 1.0 * 6.3233489990234375
Epoch 970, val loss: 1.0839636325836182
Epoch 980, training loss: 6.3911004066467285 = 0.05518554523587227 + 1.0 * 6.3359150886535645
Epoch 980, val loss: 1.0921496152877808
Epoch 990, training loss: 6.382277965545654 = 0.053066376596689224 + 1.0 * 6.329211711883545
Epoch 990, val loss: 1.1001296043395996
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8107538218239325
The final CL Acc:0.75062, 0.01062, The final GNN Acc:0.80935, 0.00131
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13128])
remove edge: torch.Size([2, 7828])
updated graph: torch.Size([2, 10400])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.546600341796875 = 1.9497636556625366 + 1.0 * 8.596837043762207
Epoch 0, val loss: 1.9457447528839111
Epoch 10, training loss: 10.536114692687988 = 1.9395110607147217 + 1.0 * 8.596603393554688
Epoch 10, val loss: 1.935440182685852
Epoch 20, training loss: 10.521727561950684 = 1.9269996881484985 + 1.0 * 8.594727516174316
Epoch 20, val loss: 1.922470211982727
Epoch 30, training loss: 10.489052772521973 = 1.9096859693527222 + 1.0 * 8.579366683959961
Epoch 30, val loss: 1.9041392803192139
Epoch 40, training loss: 10.357690811157227 = 1.8869216442108154 + 1.0 * 8.470768928527832
Epoch 40, val loss: 1.880727767944336
Epoch 50, training loss: 9.998722076416016 = 1.8628792762756348 + 1.0 * 8.135843276977539
Epoch 50, val loss: 1.8573566675186157
Epoch 60, training loss: 9.387908935546875 = 1.845611333847046 + 1.0 * 7.542297840118408
Epoch 60, val loss: 1.8425651788711548
Epoch 70, training loss: 8.875802040100098 = 1.8323967456817627 + 1.0 * 7.043405532836914
Epoch 70, val loss: 1.8308660984039307
Epoch 80, training loss: 8.72873592376709 = 1.818535566329956 + 1.0 * 6.910200595855713
Epoch 80, val loss: 1.817632794380188
Epoch 90, training loss: 8.608681678771973 = 1.8015210628509521 + 1.0 * 6.807160377502441
Epoch 90, val loss: 1.8016804456710815
Epoch 100, training loss: 8.518533706665039 = 1.7847354412078857 + 1.0 * 6.733798027038574
Epoch 100, val loss: 1.7864958047866821
Epoch 110, training loss: 8.450163841247559 = 1.7685737609863281 + 1.0 * 6.6815900802612305
Epoch 110, val loss: 1.7721320390701294
Epoch 120, training loss: 8.393815040588379 = 1.75191330909729 + 1.0 * 6.64190149307251
Epoch 120, val loss: 1.757371187210083
Epoch 130, training loss: 8.345073699951172 = 1.7335675954818726 + 1.0 * 6.61150598526001
Epoch 130, val loss: 1.740998387336731
Epoch 140, training loss: 8.298219680786133 = 1.7129364013671875 + 1.0 * 6.5852837562561035
Epoch 140, val loss: 1.7224602699279785
Epoch 150, training loss: 8.2529296875 = 1.6892329454421997 + 1.0 * 6.563696384429932
Epoch 150, val loss: 1.7011866569519043
Epoch 160, training loss: 8.207361221313477 = 1.6618995666503906 + 1.0 * 6.545461177825928
Epoch 160, val loss: 1.6768152713775635
Epoch 170, training loss: 8.159409523010254 = 1.6305311918258667 + 1.0 * 6.528878211975098
Epoch 170, val loss: 1.6489156484603882
Epoch 180, training loss: 8.109650611877441 = 1.594202995300293 + 1.0 * 6.515447616577148
Epoch 180, val loss: 1.616763949394226
Epoch 190, training loss: 8.055489540100098 = 1.5522286891937256 + 1.0 * 6.503261089324951
Epoch 190, val loss: 1.579800009727478
Epoch 200, training loss: 8.00861644744873 = 1.5044113397598267 + 1.0 * 6.504204750061035
Epoch 200, val loss: 1.5382492542266846
Epoch 210, training loss: 7.939855575561523 = 1.4529881477355957 + 1.0 * 6.486867427825928
Epoch 210, val loss: 1.4936846494674683
Epoch 220, training loss: 7.875790596008301 = 1.3977726697921753 + 1.0 * 6.478017807006836
Epoch 220, val loss: 1.446320652961731
Epoch 230, training loss: 7.8094162940979 = 1.339516520500183 + 1.0 * 6.469899654388428
Epoch 230, val loss: 1.3966166973114014
Epoch 240, training loss: 7.741649150848389 = 1.2791239023208618 + 1.0 * 6.462525367736816
Epoch 240, val loss: 1.345583200454712
Epoch 250, training loss: 7.686110973358154 = 1.2178922891616821 + 1.0 * 6.468218803405762
Epoch 250, val loss: 1.2941551208496094
Epoch 260, training loss: 7.609136581420898 = 1.1582192182540894 + 1.0 * 6.4509172439575195
Epoch 260, val loss: 1.2444630861282349
Epoch 270, training loss: 7.545976161956787 = 1.1000123023986816 + 1.0 * 6.4459638595581055
Epoch 270, val loss: 1.196319580078125
Epoch 280, training loss: 7.492814540863037 = 1.0437288284301758 + 1.0 * 6.449085712432861
Epoch 280, val loss: 1.1500399112701416
Epoch 290, training loss: 7.427212238311768 = 0.9905604124069214 + 1.0 * 6.436651706695557
Epoch 290, val loss: 1.1063287258148193
Epoch 300, training loss: 7.369880676269531 = 0.9398987889289856 + 1.0 * 6.429981708526611
Epoch 300, val loss: 1.0648880004882812
Epoch 310, training loss: 7.320127010345459 = 0.8917282223701477 + 1.0 * 6.428398609161377
Epoch 310, val loss: 1.025748610496521
Epoch 320, training loss: 7.268054008483887 = 0.8471570611000061 + 1.0 * 6.420897006988525
Epoch 320, val loss: 0.9897869825363159
Epoch 330, training loss: 7.221452236175537 = 0.8058826327323914 + 1.0 * 6.41556978225708
Epoch 330, val loss: 0.9569920301437378
Epoch 340, training loss: 7.190547943115234 = 0.7675118446350098 + 1.0 * 6.423036098480225
Epoch 340, val loss: 0.9268548488616943
Epoch 350, training loss: 7.141925811767578 = 0.7321662306785583 + 1.0 * 6.409759521484375
Epoch 350, val loss: 0.8998520970344543
Epoch 360, training loss: 7.102897644042969 = 0.6992561221122742 + 1.0 * 6.403641700744629
Epoch 360, val loss: 0.8753497004508972
Epoch 370, training loss: 7.069097995758057 = 0.6681864857673645 + 1.0 * 6.400911331176758
Epoch 370, val loss: 0.852918803691864
Epoch 380, training loss: 7.043150424957275 = 0.6389039754867554 + 1.0 * 6.4042463302612305
Epoch 380, val loss: 0.8326835036277771
Epoch 390, training loss: 7.005505561828613 = 0.6111851930618286 + 1.0 * 6.394320487976074
Epoch 390, val loss: 0.8143931031227112
Epoch 400, training loss: 6.974267959594727 = 0.5845274329185486 + 1.0 * 6.389740467071533
Epoch 400, val loss: 0.7976545691490173
Epoch 410, training loss: 6.945584774017334 = 0.5586124062538147 + 1.0 * 6.386972427368164
Epoch 410, val loss: 0.7821750640869141
Epoch 420, training loss: 6.925416469573975 = 0.533720076084137 + 1.0 * 6.391696453094482
Epoch 420, val loss: 0.7680061459541321
Epoch 430, training loss: 6.895103454589844 = 0.5098547339439392 + 1.0 * 6.38524866104126
Epoch 430, val loss: 0.7553406357765198
Epoch 440, training loss: 6.864926815032959 = 0.4867812693119049 + 1.0 * 6.378145694732666
Epoch 440, val loss: 0.7437533736228943
Epoch 450, training loss: 6.847080707550049 = 0.4645150303840637 + 1.0 * 6.382565498352051
Epoch 450, val loss: 0.7331947088241577
Epoch 460, training loss: 6.8192458152771 = 0.44330456852912903 + 1.0 * 6.375941276550293
Epoch 460, val loss: 0.7237012982368469
Epoch 470, training loss: 6.795296669006348 = 0.4230647087097168 + 1.0 * 6.372231960296631
Epoch 470, val loss: 0.7153014540672302
Epoch 480, training loss: 6.771597385406494 = 0.4036845266819 + 1.0 * 6.367912769317627
Epoch 480, val loss: 0.7079440355300903
Epoch 490, training loss: 6.765857219696045 = 0.38515937328338623 + 1.0 * 6.380697727203369
Epoch 490, val loss: 0.7014992833137512
Epoch 500, training loss: 6.735312461853027 = 0.36769524216651917 + 1.0 * 6.367617130279541
Epoch 500, val loss: 0.6960159540176392
Epoch 510, training loss: 6.713084697723389 = 0.35106974840164185 + 1.0 * 6.3620147705078125
Epoch 510, val loss: 0.6914170384407043
Epoch 520, training loss: 6.693951606750488 = 0.335137277841568 + 1.0 * 6.358814239501953
Epoch 520, val loss: 0.6875626444816589
Epoch 530, training loss: 6.6766886711120605 = 0.31979620456695557 + 1.0 * 6.3568925857543945
Epoch 530, val loss: 0.6843684315681458
Epoch 540, training loss: 6.665042877197266 = 0.3050413429737091 + 1.0 * 6.360001564025879
Epoch 540, val loss: 0.6817034482955933
Epoch 550, training loss: 6.647619724273682 = 0.2908574342727661 + 1.0 * 6.356762409210205
Epoch 550, val loss: 0.6795473694801331
Epoch 560, training loss: 6.628143310546875 = 0.2770172655582428 + 1.0 * 6.351126194000244
Epoch 560, val loss: 0.6778178811073303
Epoch 570, training loss: 6.612553119659424 = 0.26340487599372864 + 1.0 * 6.349148273468018
Epoch 570, val loss: 0.6764214634895325
Epoch 580, training loss: 6.606255054473877 = 0.25004297494888306 + 1.0 * 6.356212139129639
Epoch 580, val loss: 0.6752576231956482
Epoch 590, training loss: 6.585301399230957 = 0.23702184855937958 + 1.0 * 6.3482794761657715
Epoch 590, val loss: 0.6743282079696655
Epoch 600, training loss: 6.56923246383667 = 0.22427351772785187 + 1.0 * 6.344958782196045
Epoch 600, val loss: 0.67367023229599
Epoch 610, training loss: 6.554296016693115 = 0.2118052989244461 + 1.0 * 6.3424906730651855
Epoch 610, val loss: 0.6733517646789551
Epoch 620, training loss: 6.540883541107178 = 0.1997050642967224 + 1.0 * 6.3411784172058105
Epoch 620, val loss: 0.6734276413917542
Epoch 630, training loss: 6.528048992156982 = 0.188119038939476 + 1.0 * 6.339930057525635
Epoch 630, val loss: 0.6739208698272705
Epoch 640, training loss: 6.517702579498291 = 0.17716087400913239 + 1.0 * 6.340541839599609
Epoch 640, val loss: 0.6748161911964417
Epoch 650, training loss: 6.50520133972168 = 0.16679996252059937 + 1.0 * 6.3384013175964355
Epoch 650, val loss: 0.6762003302574158
Epoch 660, training loss: 6.498180389404297 = 0.15705029666423798 + 1.0 * 6.341130256652832
Epoch 660, val loss: 0.6781116724014282
Epoch 670, training loss: 6.483079433441162 = 0.14795057475566864 + 1.0 * 6.3351287841796875
Epoch 670, val loss: 0.6804441809654236
Epoch 680, training loss: 6.473659515380859 = 0.1394466757774353 + 1.0 * 6.334212779998779
Epoch 680, val loss: 0.6832299828529358
Epoch 690, training loss: 6.470158100128174 = 0.13152122497558594 + 1.0 * 6.338636875152588
Epoch 690, val loss: 0.6864455342292786
Epoch 700, training loss: 6.4567389488220215 = 0.12413746863603592 + 1.0 * 6.332601547241211
Epoch 700, val loss: 0.689975917339325
Epoch 710, training loss: 6.445837020874023 = 0.11727394908666611 + 1.0 * 6.328563213348389
Epoch 710, val loss: 0.6938109397888184
Epoch 720, training loss: 6.442826271057129 = 0.11087162792682648 + 1.0 * 6.331954479217529
Epoch 720, val loss: 0.6979656219482422
Epoch 730, training loss: 6.436388969421387 = 0.10491015762090683 + 1.0 * 6.331478595733643
Epoch 730, val loss: 0.7023664116859436
Epoch 740, training loss: 6.427379608154297 = 0.09938537329435349 + 1.0 * 6.327994346618652
Epoch 740, val loss: 0.7069104909896851
Epoch 750, training loss: 6.418056011199951 = 0.09421543031930923 + 1.0 * 6.323840618133545
Epoch 750, val loss: 0.7117259502410889
Epoch 760, training loss: 6.414309978485107 = 0.08937902748584747 + 1.0 * 6.3249311447143555
Epoch 760, val loss: 0.7167137265205383
Epoch 770, training loss: 6.405757427215576 = 0.08486238121986389 + 1.0 * 6.320895195007324
Epoch 770, val loss: 0.7218500971794128
Epoch 780, training loss: 6.4017205238342285 = 0.08063922822475433 + 1.0 * 6.321081161499023
Epoch 780, val loss: 0.7270817756652832
Epoch 790, training loss: 6.400079250335693 = 0.07669322937726974 + 1.0 * 6.323386192321777
Epoch 790, val loss: 0.7324397563934326
Epoch 800, training loss: 6.391630172729492 = 0.07299356162548065 + 1.0 * 6.318636417388916
Epoch 800, val loss: 0.7378638982772827
Epoch 810, training loss: 6.398448944091797 = 0.06952496618032455 + 1.0 * 6.328924179077148
Epoch 810, val loss: 0.7433673143386841
Epoch 820, training loss: 6.387871742248535 = 0.06629779934883118 + 1.0 * 6.321573734283447
Epoch 820, val loss: 0.7489429712295532
Epoch 830, training loss: 6.378549575805664 = 0.06325554847717285 + 1.0 * 6.315293788909912
Epoch 830, val loss: 0.7545222640037537
Epoch 840, training loss: 6.373659610748291 = 0.06039904057979584 + 1.0 * 6.313260555267334
Epoch 840, val loss: 0.7602343559265137
Epoch 850, training loss: 6.372860908508301 = 0.05770270153880119 + 1.0 * 6.315158367156982
Epoch 850, val loss: 0.7659921646118164
Epoch 860, training loss: 6.37117338180542 = 0.05517468601465225 + 1.0 * 6.315998554229736
Epoch 860, val loss: 0.7716841101646423
Epoch 870, training loss: 6.365822792053223 = 0.05280504375696182 + 1.0 * 6.313017845153809
Epoch 870, val loss: 0.7774074673652649
Epoch 880, training loss: 6.3599443435668945 = 0.050567399710416794 + 1.0 * 6.3093767166137695
Epoch 880, val loss: 0.7831698656082153
Epoch 890, training loss: 6.360472202301025 = 0.04845885932445526 + 1.0 * 6.312013149261475
Epoch 890, val loss: 0.7889289855957031
Epoch 900, training loss: 6.356597423553467 = 0.046473607420921326 + 1.0 * 6.310123920440674
Epoch 900, val loss: 0.7946544289588928
Epoch 910, training loss: 6.355952262878418 = 0.044599179178476334 + 1.0 * 6.3113532066345215
Epoch 910, val loss: 0.8003772497177124
Epoch 920, training loss: 6.351760387420654 = 0.04284002259373665 + 1.0 * 6.308920383453369
Epoch 920, val loss: 0.8060432076454163
Epoch 930, training loss: 6.347223281860352 = 0.041176073253154755 + 1.0 * 6.306047439575195
Epoch 930, val loss: 0.8117098212242126
Epoch 940, training loss: 6.343497276306152 = 0.03960021585226059 + 1.0 * 6.303896903991699
Epoch 940, val loss: 0.8173664212226868
Epoch 950, training loss: 6.341013431549072 = 0.03810454532504082 + 1.0 * 6.302908897399902
Epoch 950, val loss: 0.8230211138725281
Epoch 960, training loss: 6.353694915771484 = 0.03669152781367302 + 1.0 * 6.31700325012207
Epoch 960, val loss: 0.828625500202179
Epoch 970, training loss: 6.345020294189453 = 0.03536209091544151 + 1.0 * 6.309658050537109
Epoch 970, val loss: 0.8340852856636047
Epoch 980, training loss: 6.3370361328125 = 0.03409723564982414 + 1.0 * 6.302938938140869
Epoch 980, val loss: 0.8394726514816284
Epoch 990, training loss: 6.332877159118652 = 0.03290221840143204 + 1.0 * 6.2999749183654785
Epoch 990, val loss: 0.8448762893676758
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 10.544746398925781 = 1.9479154348373413 + 1.0 * 8.596831321716309
Epoch 0, val loss: 1.9431947469711304
Epoch 10, training loss: 10.533469200134277 = 1.9369632005691528 + 1.0 * 8.596506118774414
Epoch 10, val loss: 1.9329065084457397
Epoch 20, training loss: 10.517290115356445 = 1.9233391284942627 + 1.0 * 8.593951225280762
Epoch 20, val loss: 1.9197213649749756
Epoch 30, training loss: 10.479103088378906 = 1.9043145179748535 + 1.0 * 8.574789047241211
Epoch 30, val loss: 1.9011200666427612
Epoch 40, training loss: 10.330687522888184 = 1.879791021347046 + 1.0 * 8.450896263122559
Epoch 40, val loss: 1.8780754804611206
Epoch 50, training loss: 9.949398040771484 = 1.8542646169662476 + 1.0 * 8.095133781433105
Epoch 50, val loss: 1.8544572591781616
Epoch 60, training loss: 9.505521774291992 = 1.833162546157837 + 1.0 * 7.672359466552734
Epoch 60, val loss: 1.8355865478515625
Epoch 70, training loss: 9.13621711730957 = 1.8176846504211426 + 1.0 * 7.318532943725586
Epoch 70, val loss: 1.821661353111267
Epoch 80, training loss: 8.95175552368164 = 1.8012865781784058 + 1.0 * 7.1504693031311035
Epoch 80, val loss: 1.806654930114746
Epoch 90, training loss: 8.76424503326416 = 1.78303861618042 + 1.0 * 6.98120641708374
Epoch 90, val loss: 1.7906923294067383
Epoch 100, training loss: 8.61801528930664 = 1.7671160697937012 + 1.0 * 6.850898742675781
Epoch 100, val loss: 1.7769099473953247
Epoch 110, training loss: 8.524078369140625 = 1.7517436742782593 + 1.0 * 6.772334575653076
Epoch 110, val loss: 1.7628031969070435
Epoch 120, training loss: 8.449974060058594 = 1.7342357635498047 + 1.0 * 6.715737819671631
Epoch 120, val loss: 1.7465336322784424
Epoch 130, training loss: 8.389811515808105 = 1.714690089225769 + 1.0 * 6.675121784210205
Epoch 130, val loss: 1.7286344766616821
Epoch 140, training loss: 8.328534126281738 = 1.693171739578247 + 1.0 * 6.635362148284912
Epoch 140, val loss: 1.709202527999878
Epoch 150, training loss: 8.274996757507324 = 1.668660044670105 + 1.0 * 6.606337070465088
Epoch 150, val loss: 1.687389612197876
Epoch 160, training loss: 8.221663475036621 = 1.6405136585235596 + 1.0 * 6.581149578094482
Epoch 160, val loss: 1.662561058998108
Epoch 170, training loss: 8.168768882751465 = 1.6083117723464966 + 1.0 * 6.5604567527771
Epoch 170, val loss: 1.6341712474822998
Epoch 180, training loss: 8.11849308013916 = 1.5724185705184937 + 1.0 * 6.546074390411377
Epoch 180, val loss: 1.6026127338409424
Epoch 190, training loss: 8.061352729797363 = 1.533652424812317 + 1.0 * 6.527699947357178
Epoch 190, val loss: 1.5688222646713257
Epoch 200, training loss: 8.005955696105957 = 1.4923089742660522 + 1.0 * 6.513647079467773
Epoch 200, val loss: 1.5330054759979248
Epoch 210, training loss: 7.955169677734375 = 1.4490611553192139 + 1.0 * 6.50610876083374
Epoch 210, val loss: 1.4960706233978271
Epoch 220, training loss: 7.896056175231934 = 1.4051673412322998 + 1.0 * 6.490889072418213
Epoch 220, val loss: 1.4592021703720093
Epoch 230, training loss: 7.84152889251709 = 1.360730767250061 + 1.0 * 6.480798244476318
Epoch 230, val loss: 1.42229163646698
Epoch 240, training loss: 7.791109561920166 = 1.3167282342910767 + 1.0 * 6.474381446838379
Epoch 240, val loss: 1.3865454196929932
Epoch 250, training loss: 7.737091064453125 = 1.2733877897262573 + 1.0 * 6.463703155517578
Epoch 250, val loss: 1.3516287803649902
Epoch 260, training loss: 7.685863494873047 = 1.229922890663147 + 1.0 * 6.4559407234191895
Epoch 260, val loss: 1.3171366453170776
Epoch 270, training loss: 7.641253471374512 = 1.1860514879226685 + 1.0 * 6.455202102661133
Epoch 270, val loss: 1.282614827156067
Epoch 280, training loss: 7.587466716766357 = 1.142124056816101 + 1.0 * 6.445342540740967
Epoch 280, val loss: 1.2481753826141357
Epoch 290, training loss: 7.535473346710205 = 1.0977153778076172 + 1.0 * 6.437757968902588
Epoch 290, val loss: 1.2135480642318726
Epoch 300, training loss: 7.487351417541504 = 1.0524511337280273 + 1.0 * 6.434900283813477
Epoch 300, val loss: 1.1783099174499512
Epoch 310, training loss: 7.437077045440674 = 1.0067838430404663 + 1.0 * 6.430293083190918
Epoch 310, val loss: 1.1429983377456665
Epoch 320, training loss: 7.384291172027588 = 0.9612564444541931 + 1.0 * 6.42303466796875
Epoch 320, val loss: 1.1078022718429565
Epoch 330, training loss: 7.334753036499023 = 0.9160197973251343 + 1.0 * 6.4187331199646
Epoch 330, val loss: 1.073107361793518
Epoch 340, training loss: 7.28891658782959 = 0.8717801570892334 + 1.0 * 6.417136192321777
Epoch 340, val loss: 1.0395020246505737
Epoch 350, training loss: 7.240847110748291 = 0.8293970823287964 + 1.0 * 6.411449909210205
Epoch 350, val loss: 1.007694959640503
Epoch 360, training loss: 7.197152137756348 = 0.7888287901878357 + 1.0 * 6.408323287963867
Epoch 360, val loss: 0.9777182340621948
Epoch 370, training loss: 7.1550612449646 = 0.750402569770813 + 1.0 * 6.404658794403076
Epoch 370, val loss: 0.949785590171814
Epoch 380, training loss: 7.119692802429199 = 0.7143615484237671 + 1.0 * 6.405331134796143
Epoch 380, val loss: 0.9242653846740723
Epoch 390, training loss: 7.078935623168945 = 0.681001603603363 + 1.0 * 6.3979339599609375
Epoch 390, val loss: 0.9011939167976379
Epoch 400, training loss: 7.043633937835693 = 0.6496031880378723 + 1.0 * 6.394030570983887
Epoch 400, val loss: 0.8802034854888916
Epoch 410, training loss: 7.010488510131836 = 0.6199120879173279 + 1.0 * 6.390576362609863
Epoch 410, val loss: 0.8610856533050537
Epoch 420, training loss: 6.983407020568848 = 0.5917947292327881 + 1.0 * 6.3916120529174805
Epoch 420, val loss: 0.8436949849128723
Epoch 430, training loss: 6.953512191772461 = 0.5655601620674133 + 1.0 * 6.387951850891113
Epoch 430, val loss: 0.8281916379928589
Epoch 440, training loss: 6.926632404327393 = 0.5411407351493835 + 1.0 * 6.385491847991943
Epoch 440, val loss: 0.8145010471343994
Epoch 450, training loss: 6.898406505584717 = 0.5180233716964722 + 1.0 * 6.380383014678955
Epoch 450, val loss: 0.8023849725723267
Epoch 460, training loss: 6.874646186828613 = 0.4960862994194031 + 1.0 * 6.3785600662231445
Epoch 460, val loss: 0.7915587425231934
Epoch 470, training loss: 6.858945846557617 = 0.4753372371196747 + 1.0 * 6.383608818054199
Epoch 470, val loss: 0.7820316553115845
Epoch 480, training loss: 6.833732604980469 = 0.4557950794696808 + 1.0 * 6.377937316894531
Epoch 480, val loss: 0.7737763524055481
Epoch 490, training loss: 6.8093342781066895 = 0.4373231828212738 + 1.0 * 6.372011184692383
Epoch 490, val loss: 0.7665916085243225
Epoch 500, training loss: 6.789278984069824 = 0.4196908473968506 + 1.0 * 6.369588375091553
Epoch 500, val loss: 0.7603276371955872
Epoch 510, training loss: 6.771602153778076 = 0.4029003083705902 + 1.0 * 6.368701934814453
Epoch 510, val loss: 0.754936158657074
Epoch 520, training loss: 6.754083633422852 = 0.38693273067474365 + 1.0 * 6.367150783538818
Epoch 520, val loss: 0.7503295540809631
Epoch 530, training loss: 6.742671966552734 = 0.37167346477508545 + 1.0 * 6.370998382568359
Epoch 530, val loss: 0.7464473247528076
Epoch 540, training loss: 6.7191314697265625 = 0.3570921719074249 + 1.0 * 6.362039089202881
Epoch 540, val loss: 0.7431772947311401
Epoch 550, training loss: 6.703711032867432 = 0.3430752754211426 + 1.0 * 6.360635757446289
Epoch 550, val loss: 0.7405173182487488
Epoch 560, training loss: 6.689733982086182 = 0.32953545451164246 + 1.0 * 6.360198497772217
Epoch 560, val loss: 0.7383290529251099
Epoch 570, training loss: 6.673632621765137 = 0.31647753715515137 + 1.0 * 6.357154846191406
Epoch 570, val loss: 0.7366849780082703
Epoch 580, training loss: 6.658778667449951 = 0.3037656545639038 + 1.0 * 6.355012893676758
Epoch 580, val loss: 0.7355057597160339
Epoch 590, training loss: 6.651351451873779 = 0.29137536883354187 + 1.0 * 6.359976291656494
Epoch 590, val loss: 0.7347191572189331
Epoch 600, training loss: 6.6338396072387695 = 0.2792342007160187 + 1.0 * 6.354605197906494
Epoch 600, val loss: 0.7343229651451111
Epoch 610, training loss: 6.620131492614746 = 0.26739075779914856 + 1.0 * 6.35274076461792
Epoch 610, val loss: 0.734174907207489
Epoch 620, training loss: 6.605331897735596 = 0.25573408603668213 + 1.0 * 6.349597930908203
Epoch 620, val loss: 0.7344057559967041
Epoch 630, training loss: 6.592402458190918 = 0.24422800540924072 + 1.0 * 6.348174571990967
Epoch 630, val loss: 0.7350229024887085
Epoch 640, training loss: 6.5818939208984375 = 0.2328736037015915 + 1.0 * 6.349020481109619
Epoch 640, val loss: 0.7358382344245911
Epoch 650, training loss: 6.56631326675415 = 0.22174276411533356 + 1.0 * 6.344570636749268
Epoch 650, val loss: 0.7369998097419739
Epoch 660, training loss: 6.555870056152344 = 0.210836261510849 + 1.0 * 6.345033645629883
Epoch 660, val loss: 0.7384743690490723
Epoch 670, training loss: 6.553923606872559 = 0.20016704499721527 + 1.0 * 6.353756427764893
Epoch 670, val loss: 0.7402220368385315
Epoch 680, training loss: 6.531219005584717 = 0.1898762583732605 + 1.0 * 6.341342926025391
Epoch 680, val loss: 0.7422223091125488
Epoch 690, training loss: 6.520538806915283 = 0.17989429831504822 + 1.0 * 6.340644359588623
Epoch 690, val loss: 0.7446273565292358
Epoch 700, training loss: 6.508439064025879 = 0.1702992022037506 + 1.0 * 6.33814001083374
Epoch 700, val loss: 0.7472933530807495
Epoch 710, training loss: 6.506185531616211 = 0.16113027930259705 + 1.0 * 6.345055103302002
Epoch 710, val loss: 0.7502626180648804
Epoch 720, training loss: 6.490795135498047 = 0.1524287909269333 + 1.0 * 6.338366508483887
Epoch 720, val loss: 0.7534632086753845
Epoch 730, training loss: 6.481079578399658 = 0.14420191943645477 + 1.0 * 6.336877822875977
Epoch 730, val loss: 0.7569846510887146
Epoch 740, training loss: 6.473005294799805 = 0.1364428848028183 + 1.0 * 6.336562633514404
Epoch 740, val loss: 0.7606986165046692
Epoch 750, training loss: 6.465731143951416 = 0.12914630770683289 + 1.0 * 6.33658504486084
Epoch 750, val loss: 0.7647601366043091
Epoch 760, training loss: 6.454492092132568 = 0.12227768450975418 + 1.0 * 6.33221435546875
Epoch 760, val loss: 0.7687938809394836
Epoch 770, training loss: 6.447934150695801 = 0.11583065986633301 + 1.0 * 6.332103729248047
Epoch 770, val loss: 0.773196280002594
Epoch 780, training loss: 6.443437099456787 = 0.10978257656097412 + 1.0 * 6.333654403686523
Epoch 780, val loss: 0.7776292562484741
Epoch 790, training loss: 6.434508323669434 = 0.10412158817052841 + 1.0 * 6.330386638641357
Epoch 790, val loss: 0.7822192311286926
Epoch 800, training loss: 6.427440166473389 = 0.09881219267845154 + 1.0 * 6.328628063201904
Epoch 800, val loss: 0.7869357466697693
Epoch 810, training loss: 6.4254608154296875 = 0.09383286535739899 + 1.0 * 6.33162784576416
Epoch 810, val loss: 0.791826069355011
Epoch 820, training loss: 6.419017791748047 = 0.0891818031668663 + 1.0 * 6.329835891723633
Epoch 820, val loss: 0.7966273427009583
Epoch 830, training loss: 6.41195011138916 = 0.08481811732053757 + 1.0 * 6.327132225036621
Epoch 830, val loss: 0.801640510559082
Epoch 840, training loss: 6.405381679534912 = 0.08073177188634872 + 1.0 * 6.324649810791016
Epoch 840, val loss: 0.8066790699958801
Epoch 850, training loss: 6.40073823928833 = 0.07689934968948364 + 1.0 * 6.323838710784912
Epoch 850, val loss: 0.8118352890014648
Epoch 860, training loss: 6.398525238037109 = 0.07329843938350677 + 1.0 * 6.325226783752441
Epoch 860, val loss: 0.8170011639595032
Epoch 870, training loss: 6.396164417266846 = 0.0699150413274765 + 1.0 * 6.326249599456787
Epoch 870, val loss: 0.8221823573112488
Epoch 880, training loss: 6.38706636428833 = 0.06674256920814514 + 1.0 * 6.320323944091797
Epoch 880, val loss: 0.8273828029632568
Epoch 890, training loss: 6.382867813110352 = 0.06375565379858017 + 1.0 * 6.319112300872803
Epoch 890, val loss: 0.8326218128204346
Epoch 900, training loss: 6.380487442016602 = 0.060942523181438446 + 1.0 * 6.319544792175293
Epoch 900, val loss: 0.8378972411155701
Epoch 910, training loss: 6.381845474243164 = 0.0582905076444149 + 1.0 * 6.323554992675781
Epoch 910, val loss: 0.8431780338287354
Epoch 920, training loss: 6.375696659088135 = 0.05580567568540573 + 1.0 * 6.319890975952148
Epoch 920, val loss: 0.8484660983085632
Epoch 930, training loss: 6.369079113006592 = 0.05346210300922394 + 1.0 * 6.315617084503174
Epoch 930, val loss: 0.8537483811378479
Epoch 940, training loss: 6.365530967712402 = 0.05125059559941292 + 1.0 * 6.3142805099487305
Epoch 940, val loss: 0.8590973615646362
Epoch 950, training loss: 6.382083415985107 = 0.049158137291669846 + 1.0 * 6.332925319671631
Epoch 950, val loss: 0.864166259765625
Epoch 960, training loss: 6.359820365905762 = 0.0472036711871624 + 1.0 * 6.31261682510376
Epoch 960, val loss: 0.8694197535514832
Epoch 970, training loss: 6.357919692993164 = 0.04535188525915146 + 1.0 * 6.312567710876465
Epoch 970, val loss: 0.8747883439064026
Epoch 980, training loss: 6.354548454284668 = 0.043594714254140854 + 1.0 * 6.310953617095947
Epoch 980, val loss: 0.8800281882286072
Epoch 990, training loss: 6.355221271514893 = 0.04192464426159859 + 1.0 * 6.313296794891357
Epoch 990, val loss: 0.8851200938224792
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.544408798217773 = 1.9476311206817627 + 1.0 * 8.59677791595459
Epoch 0, val loss: 1.948380947113037
Epoch 10, training loss: 10.533876419067383 = 1.937554121017456 + 1.0 * 8.596322059631348
Epoch 10, val loss: 1.9381978511810303
Epoch 20, training loss: 10.518113136291504 = 1.9249037504196167 + 1.0 * 8.593209266662598
Epoch 20, val loss: 1.925252079963684
Epoch 30, training loss: 10.477622985839844 = 1.9072856903076172 + 1.0 * 8.570337295532227
Epoch 30, val loss: 1.9074369668960571
Epoch 40, training loss: 10.308112144470215 = 1.8848506212234497 + 1.0 * 8.423261642456055
Epoch 40, val loss: 1.8854715824127197
Epoch 50, training loss: 9.81250286102295 = 1.8599308729171753 + 1.0 * 7.952572345733643
Epoch 50, val loss: 1.8611394166946411
Epoch 60, training loss: 9.381153106689453 = 1.8389203548431396 + 1.0 * 7.542232990264893
Epoch 60, val loss: 1.8414267301559448
Epoch 70, training loss: 8.972522735595703 = 1.8234632015228271 + 1.0 * 7.149059295654297
Epoch 70, val loss: 1.826505422592163
Epoch 80, training loss: 8.785673141479492 = 1.8086222410202026 + 1.0 * 6.977051258087158
Epoch 80, val loss: 1.8125008344650269
Epoch 90, training loss: 8.624149322509766 = 1.7908917665481567 + 1.0 * 6.83325719833374
Epoch 90, val loss: 1.796905279159546
Epoch 100, training loss: 8.52797794342041 = 1.7732255458831787 + 1.0 * 6.7547526359558105
Epoch 100, val loss: 1.7813324928283691
Epoch 110, training loss: 8.453399658203125 = 1.7554205656051636 + 1.0 * 6.697978973388672
Epoch 110, val loss: 1.764670491218567
Epoch 120, training loss: 8.392204284667969 = 1.736411213874817 + 1.0 * 6.655793190002441
Epoch 120, val loss: 1.7466175556182861
Epoch 130, training loss: 8.33873462677002 = 1.7153427600860596 + 1.0 * 6.623392105102539
Epoch 130, val loss: 1.7270596027374268
Epoch 140, training loss: 8.287907600402832 = 1.6912038326263428 + 1.0 * 6.596704006195068
Epoch 140, val loss: 1.7053520679473877
Epoch 150, training loss: 8.237913131713867 = 1.6632384061813354 + 1.0 * 6.5746750831604
Epoch 150, val loss: 1.6808209419250488
Epoch 160, training loss: 8.187590599060059 = 1.6310991048812866 + 1.0 * 6.556491851806641
Epoch 160, val loss: 1.6526931524276733
Epoch 170, training loss: 8.132662773132324 = 1.5941027402877808 + 1.0 * 6.538560390472412
Epoch 170, val loss: 1.620466947555542
Epoch 180, training loss: 8.0756254196167 = 1.5514862537384033 + 1.0 * 6.524138927459717
Epoch 180, val loss: 1.5834769010543823
Epoch 190, training loss: 8.016268730163574 = 1.5034047365188599 + 1.0 * 6.512864112854004
Epoch 190, val loss: 1.5420162677764893
Epoch 200, training loss: 7.952414512634277 = 1.4515231847763062 + 1.0 * 6.500891208648682
Epoch 200, val loss: 1.497437596321106
Epoch 210, training loss: 7.886646270751953 = 1.396251916885376 + 1.0 * 6.490394592285156
Epoch 210, val loss: 1.450403094291687
Epoch 220, training loss: 7.821094036102295 = 1.338436484336853 + 1.0 * 6.482657432556152
Epoch 220, val loss: 1.401689887046814
Epoch 230, training loss: 7.7556843757629395 = 1.2807894945144653 + 1.0 * 6.474895000457764
Epoch 230, val loss: 1.3536347150802612
Epoch 240, training loss: 7.689700603485107 = 1.2241414785385132 + 1.0 * 6.465559005737305
Epoch 240, val loss: 1.3068984746932983
Epoch 250, training loss: 7.626846790313721 = 1.168558955192566 + 1.0 * 6.458287715911865
Epoch 250, val loss: 1.2616130113601685
Epoch 260, training loss: 7.567716598510742 = 1.1150476932525635 + 1.0 * 6.452669143676758
Epoch 260, val loss: 1.218570590019226
Epoch 270, training loss: 7.511261940002441 = 1.0646544694900513 + 1.0 * 6.44660758972168
Epoch 270, val loss: 1.1784906387329102
Epoch 280, training loss: 7.456521034240723 = 1.0165956020355225 + 1.0 * 6.439925193786621
Epoch 280, val loss: 1.1406643390655518
Epoch 290, training loss: 7.404841899871826 = 0.9709225296974182 + 1.0 * 6.433919429779053
Epoch 290, val loss: 1.105043888092041
Epoch 300, training loss: 7.358668804168701 = 0.9278708100318909 + 1.0 * 6.430798053741455
Epoch 300, val loss: 1.0717997550964355
Epoch 310, training loss: 7.311239242553711 = 0.8870248794555664 + 1.0 * 6.4242143630981445
Epoch 310, val loss: 1.0404309034347534
Epoch 320, training loss: 7.266595363616943 = 0.8477815985679626 + 1.0 * 6.418813705444336
Epoch 320, val loss: 1.0106377601623535
Epoch 330, training loss: 7.22460412979126 = 0.8100842237472534 + 1.0 * 6.414519786834717
Epoch 330, val loss: 0.9822309613227844
Epoch 340, training loss: 7.187669277191162 = 0.7740513682365417 + 1.0 * 6.413618087768555
Epoch 340, val loss: 0.9554557800292969
Epoch 350, training loss: 7.145937919616699 = 0.739569902420044 + 1.0 * 6.406368255615234
Epoch 350, val loss: 0.9302946925163269
Epoch 360, training loss: 7.1088337898254395 = 0.7067036628723145 + 1.0 * 6.402130126953125
Epoch 360, val loss: 0.906806468963623
Epoch 370, training loss: 7.073617458343506 = 0.675407350063324 + 1.0 * 6.398210048675537
Epoch 370, val loss: 0.8849810361862183
Epoch 380, training loss: 7.041723251342773 = 0.6454201340675354 + 1.0 * 6.396303176879883
Epoch 380, val loss: 0.8647580146789551
Epoch 390, training loss: 7.016698837280273 = 0.6170297265052795 + 1.0 * 6.399669170379639
Epoch 390, val loss: 0.8462068438529968
Epoch 400, training loss: 6.9786295890808105 = 0.5901542901992798 + 1.0 * 6.38847541809082
Epoch 400, val loss: 0.8295096158981323
Epoch 410, training loss: 6.947847843170166 = 0.5645354390144348 + 1.0 * 6.383312225341797
Epoch 410, val loss: 0.8142924904823303
Epoch 420, training loss: 6.9245805740356445 = 0.540016233921051 + 1.0 * 6.384564399719238
Epoch 420, val loss: 0.8003903031349182
Epoch 430, training loss: 6.900195121765137 = 0.5167046189308167 + 1.0 * 6.383490562438965
Epoch 430, val loss: 0.7878217697143555
Epoch 440, training loss: 6.871650695800781 = 0.4944814443588257 + 1.0 * 6.377169132232666
Epoch 440, val loss: 0.7764602899551392
Epoch 450, training loss: 6.845671653747559 = 0.47303882241249084 + 1.0 * 6.37263298034668
Epoch 450, val loss: 0.7660303711891174
Epoch 460, training loss: 6.83115816116333 = 0.4522477090358734 + 1.0 * 6.378910541534424
Epoch 460, val loss: 0.7564101219177246
Epoch 470, training loss: 6.7995924949646 = 0.4321354627609253 + 1.0 * 6.367456912994385
Epoch 470, val loss: 0.7475348114967346
Epoch 480, training loss: 6.77802848815918 = 0.41249680519104004 + 1.0 * 6.3655314445495605
Epoch 480, val loss: 0.7391212582588196
Epoch 490, training loss: 6.762531280517578 = 0.3931891918182373 + 1.0 * 6.369341850280762
Epoch 490, val loss: 0.731100857257843
Epoch 500, training loss: 6.736835479736328 = 0.37440207600593567 + 1.0 * 6.362433433532715
Epoch 500, val loss: 0.7234329581260681
Epoch 510, training loss: 6.715776443481445 = 0.3559902012348175 + 1.0 * 6.359786033630371
Epoch 510, val loss: 0.7158932685852051
Epoch 520, training loss: 6.694482803344727 = 0.3378332257270813 + 1.0 * 6.356649398803711
Epoch 520, val loss: 0.7085586786270142
Epoch 530, training loss: 6.674389362335205 = 0.31989672780036926 + 1.0 * 6.354492664337158
Epoch 530, val loss: 0.701447606086731
Epoch 540, training loss: 6.665099143981934 = 0.30228087306022644 + 1.0 * 6.362818241119385
Epoch 540, val loss: 0.694579005241394
Epoch 550, training loss: 6.641094207763672 = 0.28517821431159973 + 1.0 * 6.3559160232543945
Epoch 550, val loss: 0.688015341758728
Epoch 560, training loss: 6.61919641494751 = 0.26859185099601746 + 1.0 * 6.35060453414917
Epoch 560, val loss: 0.6818119287490845
Epoch 570, training loss: 6.600278377532959 = 0.25254175066947937 + 1.0 * 6.347736835479736
Epoch 570, val loss: 0.6761992573738098
Epoch 580, training loss: 6.592597484588623 = 0.2371651828289032 + 1.0 * 6.355432510375977
Epoch 580, val loss: 0.6712262034416199
Epoch 590, training loss: 6.567285537719727 = 0.2226279228925705 + 1.0 * 6.3446574211120605
Epoch 590, val loss: 0.6669402122497559
Epoch 600, training loss: 6.553796291351318 = 0.20894008874893188 + 1.0 * 6.344856262207031
Epoch 600, val loss: 0.6633878350257874
Epoch 610, training loss: 6.540503025054932 = 0.19615738093852997 + 1.0 * 6.344345569610596
Epoch 610, val loss: 0.6606062650680542
Epoch 620, training loss: 6.52444314956665 = 0.1842852681875229 + 1.0 * 6.340157985687256
Epoch 620, val loss: 0.6585821509361267
Epoch 630, training loss: 6.513628959655762 = 0.17326101660728455 + 1.0 * 6.340367794036865
Epoch 630, val loss: 0.6572867631912231
Epoch 640, training loss: 6.5012335777282715 = 0.16306743025779724 + 1.0 * 6.338166236877441
Epoch 640, val loss: 0.6566891670227051
Epoch 650, training loss: 6.494210243225098 = 0.15362578630447388 + 1.0 * 6.3405842781066895
Epoch 650, val loss: 0.6566922068595886
Epoch 660, training loss: 6.485919952392578 = 0.14494188129901886 + 1.0 * 6.340978145599365
Epoch 660, val loss: 0.6572204232215881
Epoch 670, training loss: 6.469944477081299 = 0.13690641522407532 + 1.0 * 6.333037853240967
Epoch 670, val loss: 0.6581602096557617
Epoch 680, training loss: 6.462002754211426 = 0.1294349730014801 + 1.0 * 6.3325676918029785
Epoch 680, val loss: 0.6595917344093323
Epoch 690, training loss: 6.455801486968994 = 0.1224735677242279 + 1.0 * 6.333327770233154
Epoch 690, val loss: 0.6614722609519958
Epoch 700, training loss: 6.4497270584106445 = 0.1159985288977623 + 1.0 * 6.333728313446045
Epoch 700, val loss: 0.6636636853218079
Epoch 710, training loss: 6.439979553222656 = 0.10998746752738953 + 1.0 * 6.329992294311523
Epoch 710, val loss: 0.6660969257354736
Epoch 720, training loss: 6.432431697845459 = 0.10436566174030304 + 1.0 * 6.328065872192383
Epoch 720, val loss: 0.6687821745872498
Epoch 730, training loss: 6.427913665771484 = 0.09909866005182266 + 1.0 * 6.32881498336792
Epoch 730, val loss: 0.6717730760574341
Epoch 740, training loss: 6.423913955688477 = 0.0941678062081337 + 1.0 * 6.329746246337891
Epoch 740, val loss: 0.6749908328056335
Epoch 750, training loss: 6.416257381439209 = 0.08957751840353012 + 1.0 * 6.326679706573486
Epoch 750, val loss: 0.6783038973808289
Epoch 760, training loss: 6.4092817306518555 = 0.08526241779327393 + 1.0 * 6.324019432067871
Epoch 760, val loss: 0.6817713975906372
Epoch 770, training loss: 6.402966499328613 = 0.08120816946029663 + 1.0 * 6.321758270263672
Epoch 770, val loss: 0.6854230761528015
Epoch 780, training loss: 6.400053024291992 = 0.07738196849822998 + 1.0 * 6.322670936584473
Epoch 780, val loss: 0.6892449855804443
Epoch 790, training loss: 6.3996262550354 = 0.07378792017698288 + 1.0 * 6.325838565826416
Epoch 790, val loss: 0.6931301355361938
Epoch 800, training loss: 6.3942365646362305 = 0.07043039798736572 + 1.0 * 6.323806285858154
Epoch 800, val loss: 0.6970219016075134
Epoch 810, training loss: 6.384999752044678 = 0.0672694593667984 + 1.0 * 6.31773042678833
Epoch 810, val loss: 0.7009841799736023
Epoch 820, training loss: 6.380797386169434 = 0.06428144872188568 + 1.0 * 6.316515922546387
Epoch 820, val loss: 0.7051283121109009
Epoch 830, training loss: 6.380280017852783 = 0.06145310774445534 + 1.0 * 6.318826675415039
Epoch 830, val loss: 0.7093738913536072
Epoch 840, training loss: 6.374161720275879 = 0.05879122018814087 + 1.0 * 6.315370559692383
Epoch 840, val loss: 0.7136614918708801
Epoch 850, training loss: 6.370562553405762 = 0.056278422474861145 + 1.0 * 6.314284324645996
Epoch 850, val loss: 0.7178848385810852
Epoch 860, training loss: 6.369034290313721 = 0.053904611617326736 + 1.0 * 6.31512975692749
Epoch 860, val loss: 0.7222075462341309
Epoch 870, training loss: 6.364414215087891 = 0.05165476351976395 + 1.0 * 6.3127593994140625
Epoch 870, val loss: 0.7265642881393433
Epoch 880, training loss: 6.371125221252441 = 0.04952783137559891 + 1.0 * 6.321597576141357
Epoch 880, val loss: 0.7309549450874329
Epoch 890, training loss: 6.362486362457275 = 0.04753045365214348 + 1.0 * 6.314955711364746
Epoch 890, val loss: 0.7353016138076782
Epoch 900, training loss: 6.3546929359436035 = 0.04563460126519203 + 1.0 * 6.30905818939209
Epoch 900, val loss: 0.7396639585494995
Epoch 910, training loss: 6.352161884307861 = 0.04384050518274307 + 1.0 * 6.308321475982666
Epoch 910, val loss: 0.7441070079803467
Epoch 920, training loss: 6.355554103851318 = 0.042133864015340805 + 1.0 * 6.313420295715332
Epoch 920, val loss: 0.7486234307289124
Epoch 930, training loss: 6.350868225097656 = 0.04052220284938812 + 1.0 * 6.3103461265563965
Epoch 930, val loss: 0.7530853152275085
Epoch 940, training loss: 6.347675323486328 = 0.03900092840194702 + 1.0 * 6.308674335479736
Epoch 940, val loss: 0.7574583888053894
Epoch 950, training loss: 6.3484673500061035 = 0.03755764290690422 + 1.0 * 6.310909748077393
Epoch 950, val loss: 0.7618874907493591
Epoch 960, training loss: 6.340779781341553 = 0.03618746995925903 + 1.0 * 6.304592132568359
Epoch 960, val loss: 0.7663188576698303
Epoch 970, training loss: 6.338632106781006 = 0.03488573059439659 + 1.0 * 6.303746223449707
Epoch 970, val loss: 0.7707532048225403
Epoch 980, training loss: 6.343260765075684 = 0.03364677354693413 + 1.0 * 6.309614181518555
Epoch 980, val loss: 0.7752161026000977
Epoch 990, training loss: 6.339410781860352 = 0.032471779733896255 + 1.0 * 6.306939125061035
Epoch 990, val loss: 0.7796352505683899
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8323668950975225
The final CL Acc:0.80988, 0.00630, The final GNN Acc:0.83553, 0.00258
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11558])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10516])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.557039260864258 = 1.9602136611938477 + 1.0 * 8.59682559967041
Epoch 0, val loss: 1.9623125791549683
Epoch 10, training loss: 10.546734809875488 = 1.9501816034317017 + 1.0 * 8.596552848815918
Epoch 10, val loss: 1.952540397644043
Epoch 20, training loss: 10.532505989074707 = 1.9378608465194702 + 1.0 * 8.594645500183105
Epoch 20, val loss: 1.9399529695510864
Epoch 30, training loss: 10.501046180725098 = 1.9209158420562744 + 1.0 * 8.580130577087402
Epoch 30, val loss: 1.921998143196106
Epoch 40, training loss: 10.380325317382812 = 1.8985092639923096 + 1.0 * 8.481816291809082
Epoch 40, val loss: 1.8987689018249512
Epoch 50, training loss: 9.93852710723877 = 1.8750755786895752 + 1.0 * 8.063451766967773
Epoch 50, val loss: 1.8760188817977905
Epoch 60, training loss: 9.330808639526367 = 1.8587714433670044 + 1.0 * 7.472037315368652
Epoch 60, val loss: 1.8619273900985718
Epoch 70, training loss: 8.972620010375977 = 1.846495509147644 + 1.0 * 7.126124858856201
Epoch 70, val loss: 1.8506767749786377
Epoch 80, training loss: 8.779276847839355 = 1.8333733081817627 + 1.0 * 6.945903778076172
Epoch 80, val loss: 1.8379114866256714
Epoch 90, training loss: 8.663004875183105 = 1.8179537057876587 + 1.0 * 6.845050811767578
Epoch 90, val loss: 1.8232468366622925
Epoch 100, training loss: 8.574811935424805 = 1.8032841682434082 + 1.0 * 6.771528244018555
Epoch 100, val loss: 1.8096781969070435
Epoch 110, training loss: 8.509001731872559 = 1.7900148630142212 + 1.0 * 6.718986988067627
Epoch 110, val loss: 1.7973750829696655
Epoch 120, training loss: 8.452751159667969 = 1.7774723768234253 + 1.0 * 6.675279140472412
Epoch 120, val loss: 1.7856831550598145
Epoch 130, training loss: 8.405600547790527 = 1.7647087574005127 + 1.0 * 6.6408915519714355
Epoch 130, val loss: 1.7737576961517334
Epoch 140, training loss: 8.363479614257812 = 1.7508257627487183 + 1.0 * 6.612653732299805
Epoch 140, val loss: 1.7609869241714478
Epoch 150, training loss: 8.324209213256836 = 1.7352319955825806 + 1.0 * 6.588977336883545
Epoch 150, val loss: 1.7470158338546753
Epoch 160, training loss: 8.290325164794922 = 1.717515468597412 + 1.0 * 6.572810173034668
Epoch 160, val loss: 1.7315375804901123
Epoch 170, training loss: 8.24964427947998 = 1.6974295377731323 + 1.0 * 6.552215099334717
Epoch 170, val loss: 1.714237928390503
Epoch 180, training loss: 8.210681915283203 = 1.6744014024734497 + 1.0 * 6.536280632019043
Epoch 180, val loss: 1.694602608680725
Epoch 190, training loss: 8.1702241897583 = 1.6478359699249268 + 1.0 * 6.522388458251953
Epoch 190, val loss: 1.6721409559249878
Epoch 200, training loss: 8.127567291259766 = 1.6173464059829712 + 1.0 * 6.510220527648926
Epoch 200, val loss: 1.6466302871704102
Epoch 210, training loss: 8.083829879760742 = 1.5831371545791626 + 1.0 * 6.500692367553711
Epoch 210, val loss: 1.6182336807250977
Epoch 220, training loss: 8.034628868103027 = 1.5456154346466064 + 1.0 * 6.489013195037842
Epoch 220, val loss: 1.5875111818313599
Epoch 230, training loss: 7.984451770782471 = 1.5048632621765137 + 1.0 * 6.479588508605957
Epoch 230, val loss: 1.5546003580093384
Epoch 240, training loss: 7.933813571929932 = 1.4612284898757935 + 1.0 * 6.472585201263428
Epoch 240, val loss: 1.5198767185211182
Epoch 250, training loss: 7.885546684265137 = 1.4158426523208618 + 1.0 * 6.4697041511535645
Epoch 250, val loss: 1.4845662117004395
Epoch 260, training loss: 7.827118396759033 = 1.3699668645858765 + 1.0 * 6.457151412963867
Epoch 260, val loss: 1.449411392211914
Epoch 270, training loss: 7.772858619689941 = 1.3233850002288818 + 1.0 * 6.449473857879639
Epoch 270, val loss: 1.4143328666687012
Epoch 280, training loss: 7.725761413574219 = 1.277505874633789 + 1.0 * 6.44825553894043
Epoch 280, val loss: 1.3805394172668457
Epoch 290, training loss: 7.671125411987305 = 1.2332955598831177 + 1.0 * 6.437829971313477
Epoch 290, val loss: 1.3484821319580078
Epoch 300, training loss: 7.623558044433594 = 1.1904834508895874 + 1.0 * 6.433074474334717
Epoch 300, val loss: 1.3178962469100952
Epoch 310, training loss: 7.576431751251221 = 1.149587631225586 + 1.0 * 6.426844120025635
Epoch 310, val loss: 1.2891857624053955
Epoch 320, training loss: 7.532195568084717 = 1.1106224060058594 + 1.0 * 6.421573162078857
Epoch 320, val loss: 1.2621883153915405
Epoch 330, training loss: 7.492158889770508 = 1.0738025903701782 + 1.0 * 6.418356418609619
Epoch 330, val loss: 1.2370811700820923
Epoch 340, training loss: 7.450995445251465 = 1.0392639636993408 + 1.0 * 6.411731243133545
Epoch 340, val loss: 1.2138704061508179
Epoch 350, training loss: 7.414336204528809 = 1.00648033618927 + 1.0 * 6.407855987548828
Epoch 350, val loss: 1.1920475959777832
Epoch 360, training loss: 7.3871965408325195 = 0.9752453565597534 + 1.0 * 6.411951065063477
Epoch 360, val loss: 1.1714684963226318
Epoch 370, training loss: 7.347739219665527 = 0.9457158446311951 + 1.0 * 6.4020233154296875
Epoch 370, val loss: 1.1521269083023071
Epoch 380, training loss: 7.315457344055176 = 0.917146623134613 + 1.0 * 6.398310661315918
Epoch 380, val loss: 1.1334474086761475
Epoch 390, training loss: 7.287306308746338 = 0.8889432549476624 + 1.0 * 6.39836311340332
Epoch 390, val loss: 1.1148462295532227
Epoch 400, training loss: 7.253395080566406 = 0.8609066009521484 + 1.0 * 6.392488479614258
Epoch 400, val loss: 1.0962953567504883
Epoch 410, training loss: 7.220700740814209 = 0.832522451877594 + 1.0 * 6.38817834854126
Epoch 410, val loss: 1.0776314735412598
Epoch 420, training loss: 7.195738315582275 = 0.8035399317741394 + 1.0 * 6.39219856262207
Epoch 420, val loss: 1.0585381984710693
Epoch 430, training loss: 7.160101890563965 = 0.774143636226654 + 1.0 * 6.385958194732666
Epoch 430, val loss: 1.0394328832626343
Epoch 440, training loss: 7.125969886779785 = 0.7441802620887756 + 1.0 * 6.381789684295654
Epoch 440, val loss: 1.0201915502548218
Epoch 450, training loss: 7.092099666595459 = 0.7135511636734009 + 1.0 * 6.378548622131348
Epoch 450, val loss: 1.000875473022461
Epoch 460, training loss: 7.066341400146484 = 0.6825745105743408 + 1.0 * 6.3837666511535645
Epoch 460, val loss: 0.9817968010902405
Epoch 470, training loss: 7.028636932373047 = 0.6518966555595398 + 1.0 * 6.376740455627441
Epoch 470, val loss: 0.9634439945220947
Epoch 480, training loss: 6.994115829467773 = 0.6214597225189209 + 1.0 * 6.372655868530273
Epoch 480, val loss: 0.9458552002906799
Epoch 490, training loss: 6.969090461730957 = 0.5914193391799927 + 1.0 * 6.377671241760254
Epoch 490, val loss: 0.9292827248573303
Epoch 500, training loss: 6.934536933898926 = 0.5623141527175903 + 1.0 * 6.372222900390625
Epoch 500, val loss: 0.9140452146530151
Epoch 510, training loss: 6.9016432762146 = 0.5341329574584961 + 1.0 * 6.3675103187561035
Epoch 510, val loss: 0.9002217054367065
Epoch 520, training loss: 6.877631664276123 = 0.5069190263748169 + 1.0 * 6.370712757110596
Epoch 520, val loss: 0.8877733945846558
Epoch 530, training loss: 6.845767974853516 = 0.4808136522769928 + 1.0 * 6.364954471588135
Epoch 530, val loss: 0.8768694400787354
Epoch 540, training loss: 6.818390369415283 = 0.4558066129684448 + 1.0 * 6.362583637237549
Epoch 540, val loss: 0.8674899339675903
Epoch 550, training loss: 6.797939300537109 = 0.43177321553230286 + 1.0 * 6.366166114807129
Epoch 550, val loss: 0.8593761920928955
Epoch 560, training loss: 6.769387722015381 = 0.4087560474872589 + 1.0 * 6.360631465911865
Epoch 560, val loss: 0.8525702953338623
Epoch 570, training loss: 6.748570442199707 = 0.38671594858169556 + 1.0 * 6.361854553222656
Epoch 570, val loss: 0.8468969464302063
Epoch 580, training loss: 6.728098392486572 = 0.3655860722064972 + 1.0 * 6.362512111663818
Epoch 580, val loss: 0.8421734571456909
Epoch 590, training loss: 6.700733184814453 = 0.3453483283519745 + 1.0 * 6.355384826660156
Epoch 590, val loss: 0.8383341431617737
Epoch 600, training loss: 6.677890777587891 = 0.3258719742298126 + 1.0 * 6.3520188331604
Epoch 600, val loss: 0.8352147936820984
Epoch 610, training loss: 6.658233642578125 = 0.3070591986179352 + 1.0 * 6.351174354553223
Epoch 610, val loss: 0.8327566981315613
Epoch 620, training loss: 6.6429033279418945 = 0.2890019416809082 + 1.0 * 6.353901386260986
Epoch 620, val loss: 0.8308402895927429
Epoch 630, training loss: 6.622734546661377 = 0.2717907726764679 + 1.0 * 6.350943565368652
Epoch 630, val loss: 0.8296952843666077
Epoch 640, training loss: 6.603813648223877 = 0.25543344020843506 + 1.0 * 6.348380088806152
Epoch 640, val loss: 0.829101026058197
Epoch 650, training loss: 6.586424350738525 = 0.23992513120174408 + 1.0 * 6.346499443054199
Epoch 650, val loss: 0.8292269110679626
Epoch 660, training loss: 6.568352222442627 = 0.22518335282802582 + 1.0 * 6.34316873550415
Epoch 660, val loss: 0.8299184441566467
Epoch 670, training loss: 6.5588154792785645 = 0.2112574279308319 + 1.0 * 6.34755802154541
Epoch 670, val loss: 0.8312182426452637
Epoch 680, training loss: 6.543079376220703 = 0.1982317417860031 + 1.0 * 6.344847679138184
Epoch 680, val loss: 0.8331857919692993
Epoch 690, training loss: 6.5244975090026855 = 0.18602818250656128 + 1.0 * 6.338469505310059
Epoch 690, val loss: 0.8358068466186523
Epoch 700, training loss: 6.512753963470459 = 0.1745910793542862 + 1.0 * 6.338162899017334
Epoch 700, val loss: 0.8389999270439148
Epoch 710, training loss: 6.511301040649414 = 0.1639397293329239 + 1.0 * 6.347361087799072
Epoch 710, val loss: 0.842647910118103
Epoch 720, training loss: 6.4944167137146 = 0.15414319932460785 + 1.0 * 6.340273380279541
Epoch 720, val loss: 0.8469562530517578
Epoch 730, training loss: 6.479893684387207 = 0.1450534164905548 + 1.0 * 6.334840297698975
Epoch 730, val loss: 0.8516960144042969
Epoch 740, training loss: 6.473777770996094 = 0.13661208748817444 + 1.0 * 6.337165832519531
Epoch 740, val loss: 0.8568038940429688
Epoch 750, training loss: 6.462335586547852 = 0.12880219519138336 + 1.0 * 6.33353328704834
Epoch 750, val loss: 0.862420380115509
Epoch 760, training loss: 6.45207405090332 = 0.12152821570634842 + 1.0 * 6.330545902252197
Epoch 760, val loss: 0.8683379888534546
Epoch 770, training loss: 6.446796417236328 = 0.1147674098610878 + 1.0 * 6.332028865814209
Epoch 770, val loss: 0.8745903372764587
Epoch 780, training loss: 6.439919948577881 = 0.10851060599088669 + 1.0 * 6.331409454345703
Epoch 780, val loss: 0.8810054659843445
Epoch 790, training loss: 6.430610656738281 = 0.10272163897752762 + 1.0 * 6.3278889656066895
Epoch 790, val loss: 0.8877434730529785
Epoch 800, training loss: 6.424714088439941 = 0.09733493626117706 + 1.0 * 6.32737922668457
Epoch 800, val loss: 0.8946974873542786
Epoch 810, training loss: 6.42008638381958 = 0.09232226759195328 + 1.0 * 6.32776403427124
Epoch 810, val loss: 0.901694118976593
Epoch 820, training loss: 6.413290977478027 = 0.08766799420118332 + 1.0 * 6.325623035430908
Epoch 820, val loss: 0.9089546799659729
Epoch 830, training loss: 6.407253265380859 = 0.08332552015781403 + 1.0 * 6.323927879333496
Epoch 830, val loss: 0.9162277579307556
Epoch 840, training loss: 6.40548849105835 = 0.07927032560110092 + 1.0 * 6.326218128204346
Epoch 840, val loss: 0.9235215783119202
Epoch 850, training loss: 6.397714614868164 = 0.07550499588251114 + 1.0 * 6.32220983505249
Epoch 850, val loss: 0.931006908416748
Epoch 860, training loss: 6.391330242156982 = 0.07197735458612442 + 1.0 * 6.319353103637695
Epoch 860, val loss: 0.9384990930557251
Epoch 870, training loss: 6.388216972351074 = 0.0686626210808754 + 1.0 * 6.319554328918457
Epoch 870, val loss: 0.9460744261741638
Epoch 880, training loss: 6.38554573059082 = 0.06555953621864319 + 1.0 * 6.319986343383789
Epoch 880, val loss: 0.9535811543464661
Epoch 890, training loss: 6.383916854858398 = 0.06266529113054276 + 1.0 * 6.321251392364502
Epoch 890, val loss: 0.9611340761184692
Epoch 900, training loss: 6.374907970428467 = 0.05995010957121849 + 1.0 * 6.314958095550537
Epoch 900, val loss: 0.9686871767044067
Epoch 910, training loss: 6.373414039611816 = 0.057390131056308746 + 1.0 * 6.316023826599121
Epoch 910, val loss: 0.9762881994247437
Epoch 920, training loss: 6.374083518981934 = 0.054982028901576996 + 1.0 * 6.319101333618164
Epoch 920, val loss: 0.9838516712188721
Epoch 930, training loss: 6.36632776260376 = 0.052717145532369614 + 1.0 * 6.313610553741455
Epoch 930, val loss: 0.9914445877075195
Epoch 940, training loss: 6.36329984664917 = 0.05058431252837181 + 1.0 * 6.312715530395508
Epoch 940, val loss: 0.9989928007125854
Epoch 950, training loss: 6.3671183586120605 = 0.04857179895043373 + 1.0 * 6.318546772003174
Epoch 950, val loss: 1.0064882040023804
Epoch 960, training loss: 6.358211517333984 = 0.046677447855472565 + 1.0 * 6.3115339279174805
Epoch 960, val loss: 1.0138390064239502
Epoch 970, training loss: 6.354430198669434 = 0.04488752782344818 + 1.0 * 6.309542655944824
Epoch 970, val loss: 1.0212647914886475
Epoch 980, training loss: 6.351409435272217 = 0.043192531913518906 + 1.0 * 6.3082170486450195
Epoch 980, val loss: 1.028651237487793
Epoch 990, training loss: 6.355823993682861 = 0.041583020240068436 + 1.0 * 6.3142409324646
Epoch 990, val loss: 1.035913348197937
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 10.54716968536377 = 1.9503848552703857 + 1.0 * 8.596784591674805
Epoch 0, val loss: 1.9506953954696655
Epoch 10, training loss: 10.536459922790527 = 1.9401170015335083 + 1.0 * 8.596343040466309
Epoch 10, val loss: 1.9401403665542603
Epoch 20, training loss: 10.520301818847656 = 1.9270782470703125 + 1.0 * 8.593223571777344
Epoch 20, val loss: 1.9266377687454224
Epoch 30, training loss: 10.481399536132812 = 1.908637523651123 + 1.0 * 8.572761535644531
Epoch 30, val loss: 1.9077085256576538
Epoch 40, training loss: 10.338107109069824 = 1.8856374025344849 + 1.0 * 8.452469825744629
Epoch 40, val loss: 1.885473608970642
Epoch 50, training loss: 9.888063430786133 = 1.8621180057525635 + 1.0 * 8.025945663452148
Epoch 50, val loss: 1.8635109663009644
Epoch 60, training loss: 9.394185066223145 = 1.84242844581604 + 1.0 * 7.551756858825684
Epoch 60, val loss: 1.8452757596969604
Epoch 70, training loss: 9.064409255981445 = 1.8279162645339966 + 1.0 * 7.23649263381958
Epoch 70, val loss: 1.830993413925171
Epoch 80, training loss: 8.862384796142578 = 1.8138083219528198 + 1.0 * 7.048576354980469
Epoch 80, val loss: 1.8174173831939697
Epoch 90, training loss: 8.713594436645508 = 1.7999247312545776 + 1.0 * 6.913670063018799
Epoch 90, val loss: 1.8048139810562134
Epoch 100, training loss: 8.62166976928711 = 1.7861957550048828 + 1.0 * 6.835474491119385
Epoch 100, val loss: 1.792136311531067
Epoch 110, training loss: 8.552263259887695 = 1.7730457782745361 + 1.0 * 6.77921724319458
Epoch 110, val loss: 1.7795137166976929
Epoch 120, training loss: 8.492677688598633 = 1.7601146697998047 + 1.0 * 6.732563018798828
Epoch 120, val loss: 1.7671178579330444
Epoch 130, training loss: 8.43981647491455 = 1.7463358640670776 + 1.0 * 6.693480491638184
Epoch 130, val loss: 1.75437593460083
Epoch 140, training loss: 8.389080047607422 = 1.7308998107910156 + 1.0 * 6.6581807136535645
Epoch 140, val loss: 1.7406466007232666
Epoch 150, training loss: 8.340019226074219 = 1.7131403684616089 + 1.0 * 6.6268792152404785
Epoch 150, val loss: 1.7253494262695312
Epoch 160, training loss: 8.294817924499512 = 1.6923768520355225 + 1.0 * 6.60244083404541
Epoch 160, val loss: 1.7078430652618408
Epoch 170, training loss: 8.246164321899414 = 1.6680845022201538 + 1.0 * 6.578080177307129
Epoch 170, val loss: 1.6876410245895386
Epoch 180, training loss: 8.19813346862793 = 1.6394987106323242 + 1.0 * 6.558635234832764
Epoch 180, val loss: 1.6639504432678223
Epoch 190, training loss: 8.151440620422363 = 1.6061737537384033 + 1.0 * 6.545266628265381
Epoch 190, val loss: 1.6364493370056152
Epoch 200, training loss: 8.097505569458008 = 1.5687484741210938 + 1.0 * 6.528757095336914
Epoch 200, val loss: 1.6057084798812866
Epoch 210, training loss: 8.0433988571167 = 1.5271885395050049 + 1.0 * 6.516210556030273
Epoch 210, val loss: 1.5717270374298096
Epoch 220, training loss: 7.987146377563477 = 1.481741189956665 + 1.0 * 6.505404949188232
Epoch 220, val loss: 1.5347001552581787
Epoch 230, training loss: 7.929343223571777 = 1.433229923248291 + 1.0 * 6.496113300323486
Epoch 230, val loss: 1.495338797569275
Epoch 240, training loss: 7.871297836303711 = 1.3835691213607788 + 1.0 * 6.487728595733643
Epoch 240, val loss: 1.4552916288375854
Epoch 250, training loss: 7.814964294433594 = 1.3327866792678833 + 1.0 * 6.482177734375
Epoch 250, val loss: 1.4147149324417114
Epoch 260, training loss: 7.755023956298828 = 1.2819175720214844 + 1.0 * 6.473106384277344
Epoch 260, val loss: 1.374114990234375
Epoch 270, training loss: 7.697240829467773 = 1.2310547828674316 + 1.0 * 6.466186046600342
Epoch 270, val loss: 1.3340240716934204
Epoch 280, training loss: 7.640347480773926 = 1.1801483631134033 + 1.0 * 6.460198879241943
Epoch 280, val loss: 1.2941762208938599
Epoch 290, training loss: 7.585219860076904 = 1.1302076578140259 + 1.0 * 6.455012321472168
Epoch 290, val loss: 1.2552363872528076
Epoch 300, training loss: 7.530618190765381 = 1.08161461353302 + 1.0 * 6.44900369644165
Epoch 300, val loss: 1.2177505493164062
Epoch 310, training loss: 7.4769816398620605 = 1.0341063737869263 + 1.0 * 6.442875385284424
Epoch 310, val loss: 1.1812334060668945
Epoch 320, training loss: 7.427832126617432 = 0.9880989193916321 + 1.0 * 6.439733028411865
Epoch 320, val loss: 1.1459145545959473
Epoch 330, training loss: 7.37851095199585 = 0.9444848895072937 + 1.0 * 6.43402624130249
Epoch 330, val loss: 1.112501859664917
Epoch 340, training loss: 7.332381725311279 = 0.9028018116950989 + 1.0 * 6.429579734802246
Epoch 340, val loss: 1.0805482864379883
Epoch 350, training loss: 7.287144660949707 = 0.8626772165298462 + 1.0 * 6.42446756362915
Epoch 350, val loss: 1.0496702194213867
Epoch 360, training loss: 7.249070167541504 = 0.8243910074234009 + 1.0 * 6.424679279327393
Epoch 360, val loss: 1.019936203956604
Epoch 370, training loss: 7.206380844116211 = 0.7879068851470947 + 1.0 * 6.418474197387695
Epoch 370, val loss: 0.991661012172699
Epoch 380, training loss: 7.166623592376709 = 0.7529394030570984 + 1.0 * 6.413684368133545
Epoch 380, val loss: 0.9647219181060791
Epoch 390, training loss: 7.129777908325195 = 0.7194103598594666 + 1.0 * 6.410367488861084
Epoch 390, val loss: 0.9389786720275879
Epoch 400, training loss: 7.096501350402832 = 0.6867682933807373 + 1.0 * 6.409732818603516
Epoch 400, val loss: 0.9142473936080933
Epoch 410, training loss: 7.061860084533691 = 0.6552379131317139 + 1.0 * 6.406621932983398
Epoch 410, val loss: 0.8905432820320129
Epoch 420, training loss: 7.025320053100586 = 0.6243221759796143 + 1.0 * 6.400997638702393
Epoch 420, val loss: 0.867992103099823
Epoch 430, training loss: 6.99016809463501 = 0.5936822891235352 + 1.0 * 6.396485805511475
Epoch 430, val loss: 0.8462951183319092
Epoch 440, training loss: 6.972633361816406 = 0.5632367730140686 + 1.0 * 6.409396648406982
Epoch 440, val loss: 0.8255480527877808
Epoch 450, training loss: 6.93381929397583 = 0.5332861542701721 + 1.0 * 6.400533199310303
Epoch 450, val loss: 0.8062145709991455
Epoch 460, training loss: 6.8947224617004395 = 0.5039128661155701 + 1.0 * 6.390809535980225
Epoch 460, val loss: 0.788145124912262
Epoch 470, training loss: 6.862009048461914 = 0.47489774227142334 + 1.0 * 6.387111186981201
Epoch 470, val loss: 0.7712122797966003
Epoch 480, training loss: 6.836318016052246 = 0.44643160700798035 + 1.0 * 6.389886379241943
Epoch 480, val loss: 0.7555035948753357
Epoch 490, training loss: 6.801999092102051 = 0.4189865291118622 + 1.0 * 6.383012771606445
Epoch 490, val loss: 0.7414594888687134
Epoch 500, training loss: 6.773769855499268 = 0.3927118182182312 + 1.0 * 6.381058216094971
Epoch 500, val loss: 0.7290139198303223
Epoch 510, training loss: 6.750792503356934 = 0.36745497584342957 + 1.0 * 6.383337497711182
Epoch 510, val loss: 0.7180184721946716
Epoch 520, training loss: 6.731260299682617 = 0.34352099895477295 + 1.0 * 6.387739181518555
Epoch 520, val loss: 0.7087315917015076
Epoch 530, training loss: 6.6974310874938965 = 0.3211718201637268 + 1.0 * 6.3762593269348145
Epoch 530, val loss: 0.7009533643722534
Epoch 540, training loss: 6.672323703765869 = 0.30001959204673767 + 1.0 * 6.3723039627075195
Epoch 540, val loss: 0.6945774555206299
Epoch 550, training loss: 6.64993143081665 = 0.2800237834453583 + 1.0 * 6.369907855987549
Epoch 550, val loss: 0.6895285248756409
Epoch 560, training loss: 6.642518043518066 = 0.26121747493743896 + 1.0 * 6.381300449371338
Epoch 560, val loss: 0.6857291460037231
Epoch 570, training loss: 6.610220432281494 = 0.243706613779068 + 1.0 * 6.366513729095459
Epoch 570, val loss: 0.6832568645477295
Epoch 580, training loss: 6.595155239105225 = 0.2273816466331482 + 1.0 * 6.367773532867432
Epoch 580, val loss: 0.6818560361862183
Epoch 590, training loss: 6.579594135284424 = 0.21227435767650604 + 1.0 * 6.367319583892822
Epoch 590, val loss: 0.6815476417541504
Epoch 600, training loss: 6.559995174407959 = 0.19825181365013123 + 1.0 * 6.361743450164795
Epoch 600, val loss: 0.6821273565292358
Epoch 610, training loss: 6.544483661651611 = 0.18518181145191193 + 1.0 * 6.359302043914795
Epoch 610, val loss: 0.6835152506828308
Epoch 620, training loss: 6.530758380889893 = 0.1729961633682251 + 1.0 * 6.357762336730957
Epoch 620, val loss: 0.6857091188430786
Epoch 630, training loss: 6.526535511016846 = 0.16173993051052094 + 1.0 * 6.364795684814453
Epoch 630, val loss: 0.6887187361717224
Epoch 640, training loss: 6.512088298797607 = 0.15144069492816925 + 1.0 * 6.360647678375244
Epoch 640, val loss: 0.692283034324646
Epoch 650, training loss: 6.4956464767456055 = 0.14193271100521088 + 1.0 * 6.3537139892578125
Epoch 650, val loss: 0.6962636113166809
Epoch 660, training loss: 6.4836506843566895 = 0.13306431472301483 + 1.0 * 6.350586414337158
Epoch 660, val loss: 0.7008593082427979
Epoch 670, training loss: 6.4753923416137695 = 0.12482824921607971 + 1.0 * 6.350564002990723
Epoch 670, val loss: 0.7059173583984375
Epoch 680, training loss: 6.46875524520874 = 0.11723210662603378 + 1.0 * 6.351522922515869
Epoch 680, val loss: 0.7115038633346558
Epoch 690, training loss: 6.460642337799072 = 0.11029861867427826 + 1.0 * 6.350343704223633
Epoch 690, val loss: 0.7172757983207703
Epoch 700, training loss: 6.449662685394287 = 0.10389469563961029 + 1.0 * 6.345767974853516
Epoch 700, val loss: 0.7232957482337952
Epoch 710, training loss: 6.442010879516602 = 0.09795108437538147 + 1.0 * 6.344059944152832
Epoch 710, val loss: 0.729573130607605
Epoch 720, training loss: 6.443817138671875 = 0.0924331545829773 + 1.0 * 6.351384162902832
Epoch 720, val loss: 0.7360873818397522
Epoch 730, training loss: 6.430373191833496 = 0.08736979216337204 + 1.0 * 6.343003273010254
Epoch 730, val loss: 0.7427439093589783
Epoch 740, training loss: 6.422337055206299 = 0.0826912671327591 + 1.0 * 6.339645862579346
Epoch 740, val loss: 0.749454140663147
Epoch 750, training loss: 6.415914535522461 = 0.07832393795251846 + 1.0 * 6.33759069442749
Epoch 750, val loss: 0.7563013434410095
Epoch 760, training loss: 6.413447856903076 = 0.07425658404827118 + 1.0 * 6.339191436767578
Epoch 760, val loss: 0.7632620334625244
Epoch 770, training loss: 6.408204078674316 = 0.07049793750047684 + 1.0 * 6.337706089019775
Epoch 770, val loss: 0.7702678442001343
Epoch 780, training loss: 6.401920795440674 = 0.06701778620481491 + 1.0 * 6.334903240203857
Epoch 780, val loss: 0.777194619178772
Epoch 790, training loss: 6.3971734046936035 = 0.0637701153755188 + 1.0 * 6.33340311050415
Epoch 790, val loss: 0.7841600179672241
Epoch 800, training loss: 6.396954536437988 = 0.060725629329681396 + 1.0 * 6.336228847503662
Epoch 800, val loss: 0.7911387085914612
Epoch 810, training loss: 6.397270679473877 = 0.057892799377441406 + 1.0 * 6.3393778800964355
Epoch 810, val loss: 0.7980977892875671
Epoch 820, training loss: 6.3869524002075195 = 0.05525880679488182 + 1.0 * 6.331693649291992
Epoch 820, val loss: 0.8049286603927612
Epoch 830, training loss: 6.38268518447876 = 0.05279473960399628 + 1.0 * 6.329890251159668
Epoch 830, val loss: 0.8117042779922485
Epoch 840, training loss: 6.377806663513184 = 0.0504801943898201 + 1.0 * 6.32732629776001
Epoch 840, val loss: 0.8184382915496826
Epoch 850, training loss: 6.378629207611084 = 0.04830463230609894 + 1.0 * 6.330324649810791
Epoch 850, val loss: 0.825154721736908
Epoch 860, training loss: 6.374972343444824 = 0.04627183824777603 + 1.0 * 6.328700542449951
Epoch 860, val loss: 0.8317803740501404
Epoch 870, training loss: 6.369000434875488 = 0.04436073824763298 + 1.0 * 6.324639797210693
Epoch 870, val loss: 0.8382998108863831
Epoch 880, training loss: 6.371761798858643 = 0.04256115108728409 + 1.0 * 6.329200744628906
Epoch 880, val loss: 0.8447664976119995
Epoch 890, training loss: 6.368173122406006 = 0.04087070748209953 + 1.0 * 6.3273024559021
Epoch 890, val loss: 0.8510615825653076
Epoch 900, training loss: 6.3648176193237305 = 0.039288442581892014 + 1.0 * 6.325529098510742
Epoch 900, val loss: 0.8573175072669983
Epoch 910, training loss: 6.359097957611084 = 0.037798892706632614 + 1.0 * 6.321299076080322
Epoch 910, val loss: 0.8634597659111023
Epoch 920, training loss: 6.3561859130859375 = 0.03639083355665207 + 1.0 * 6.31979513168335
Epoch 920, val loss: 0.869500458240509
Epoch 930, training loss: 6.352410316467285 = 0.03505208343267441 + 1.0 * 6.317358016967773
Epoch 930, val loss: 0.8755086660385132
Epoch 940, training loss: 6.356261253356934 = 0.03378129377961159 + 1.0 * 6.322479724884033
Epoch 940, val loss: 0.8814812302589417
Epoch 950, training loss: 6.354201316833496 = 0.03257867693901062 + 1.0 * 6.321622848510742
Epoch 950, val loss: 0.8873202800750732
Epoch 960, training loss: 6.3465704917907715 = 0.03145672753453255 + 1.0 * 6.315113544464111
Epoch 960, val loss: 0.8930128216743469
Epoch 970, training loss: 6.344898700714111 = 0.030380627140402794 + 1.0 * 6.314517974853516
Epoch 970, val loss: 0.8987293839454651
Epoch 980, training loss: 6.3541460037231445 = 0.02935386262834072 + 1.0 * 6.32479190826416
Epoch 980, val loss: 0.9043093919754028
Epoch 990, training loss: 6.343400001525879 = 0.028385978192090988 + 1.0 * 6.315013885498047
Epoch 990, val loss: 0.9097951054573059
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8028465998945704
=== training gcn model ===
Epoch 0, training loss: 10.568397521972656 = 1.9715795516967773 + 1.0 * 8.596817970275879
Epoch 0, val loss: 1.9746956825256348
Epoch 10, training loss: 10.556747436523438 = 1.960273265838623 + 1.0 * 8.596474647521973
Epoch 10, val loss: 1.962829828262329
Epoch 20, training loss: 10.54017448425293 = 1.9463107585906982 + 1.0 * 8.593863487243652
Epoch 20, val loss: 1.9478886127471924
Epoch 30, training loss: 10.50166130065918 = 1.9270373582839966 + 1.0 * 8.574624061584473
Epoch 30, val loss: 1.9271296262741089
Epoch 40, training loss: 10.355018615722656 = 1.9033632278442383 + 1.0 * 8.451655387878418
Epoch 40, val loss: 1.9026973247528076
Epoch 50, training loss: 9.820091247558594 = 1.8808435201644897 + 1.0 * 7.939248085021973
Epoch 50, val loss: 1.8796947002410889
Epoch 60, training loss: 9.322898864746094 = 1.8638956546783447 + 1.0 * 7.45900297164917
Epoch 60, val loss: 1.8635550737380981
Epoch 70, training loss: 9.009794235229492 = 1.8492447137832642 + 1.0 * 7.160549163818359
Epoch 70, val loss: 1.8490874767303467
Epoch 80, training loss: 8.80577564239502 = 1.834290623664856 + 1.0 * 6.971484661102295
Epoch 80, val loss: 1.834190845489502
Epoch 90, training loss: 8.662079811096191 = 1.8185404539108276 + 1.0 * 6.843539237976074
Epoch 90, val loss: 1.8186410665512085
Epoch 100, training loss: 8.572040557861328 = 1.8034429550170898 + 1.0 * 6.76859712600708
Epoch 100, val loss: 1.8038145303726196
Epoch 110, training loss: 8.502994537353516 = 1.7896193265914917 + 1.0 * 6.713375091552734
Epoch 110, val loss: 1.7902865409851074
Epoch 120, training loss: 8.446759223937988 = 1.7771574258804321 + 1.0 * 6.6696014404296875
Epoch 120, val loss: 1.7780226469039917
Epoch 130, training loss: 8.399324417114258 = 1.7649199962615967 + 1.0 * 6.63440465927124
Epoch 130, val loss: 1.7661195993423462
Epoch 140, training loss: 8.356181144714355 = 1.7517482042312622 + 1.0 * 6.604433059692383
Epoch 140, val loss: 1.7537215948104858
Epoch 150, training loss: 8.317639350891113 = 1.7370047569274902 + 1.0 * 6.580634593963623
Epoch 150, val loss: 1.7402820587158203
Epoch 160, training loss: 8.284643173217773 = 1.7202720642089844 + 1.0 * 6.564371585845947
Epoch 160, val loss: 1.7254043817520142
Epoch 170, training loss: 8.245689392089844 = 1.7013788223266602 + 1.0 * 6.544311046600342
Epoch 170, val loss: 1.7087417840957642
Epoch 180, training loss: 8.209046363830566 = 1.6796380281448364 + 1.0 * 6.529407978057861
Epoch 180, val loss: 1.6897914409637451
Epoch 190, training loss: 8.170733451843262 = 1.654420018196106 + 1.0 * 6.516313076019287
Epoch 190, val loss: 1.6679344177246094
Epoch 200, training loss: 8.1312255859375 = 1.6257390975952148 + 1.0 * 6.505486965179443
Epoch 200, val loss: 1.6432652473449707
Epoch 210, training loss: 8.085954666137695 = 1.59345543384552 + 1.0 * 6.492498874664307
Epoch 210, val loss: 1.6158066987991333
Epoch 220, training loss: 8.039595603942871 = 1.557200312614441 + 1.0 * 6.482395648956299
Epoch 220, val loss: 1.5851925611495972
Epoch 230, training loss: 7.991281509399414 = 1.517225980758667 + 1.0 * 6.474055290222168
Epoch 230, val loss: 1.5518686771392822
Epoch 240, training loss: 7.940423011779785 = 1.4744656085968018 + 1.0 * 6.465957164764404
Epoch 240, val loss: 1.5169912576675415
Epoch 250, training loss: 7.887308120727539 = 1.4288185834884644 + 1.0 * 6.458489418029785
Epoch 250, val loss: 1.4803014993667603
Epoch 260, training loss: 7.83220100402832 = 1.3803008794784546 + 1.0 * 6.451900005340576
Epoch 260, val loss: 1.442021131515503
Epoch 270, training loss: 7.776250839233398 = 1.32937490940094 + 1.0 * 6.446876049041748
Epoch 270, val loss: 1.4027336835861206
Epoch 280, training loss: 7.7221856117248535 = 1.2768731117248535 + 1.0 * 6.4453125
Epoch 280, val loss: 1.362898588180542
Epoch 290, training loss: 7.65883731842041 = 1.2231776714324951 + 1.0 * 6.435659885406494
Epoch 290, val loss: 1.322603464126587
Epoch 300, training loss: 7.598319053649902 = 1.1681034564971924 + 1.0 * 6.430215358734131
Epoch 300, val loss: 1.2816295623779297
Epoch 310, training loss: 7.538430690765381 = 1.1120010614395142 + 1.0 * 6.426429748535156
Epoch 310, val loss: 1.240063190460205
Epoch 320, training loss: 7.483226776123047 = 1.0562773942947388 + 1.0 * 6.426949501037598
Epoch 320, val loss: 1.1986889839172363
Epoch 330, training loss: 7.420970439910889 = 1.0025087594985962 + 1.0 * 6.418461799621582
Epoch 330, val loss: 1.1591824293136597
Epoch 340, training loss: 7.365086078643799 = 0.9513951539993286 + 1.0 * 6.41369104385376
Epoch 340, val loss: 1.1219793558120728
Epoch 350, training loss: 7.317991733551025 = 0.9035269618034363 + 1.0 * 6.414464950561523
Epoch 350, val loss: 1.0876730680465698
Epoch 360, training loss: 7.268000602722168 = 0.8598757386207581 + 1.0 * 6.408124923706055
Epoch 360, val loss: 1.0570876598358154
Epoch 370, training loss: 7.223684787750244 = 0.8205059766769409 + 1.0 * 6.403178691864014
Epoch 370, val loss: 1.0302796363830566
Epoch 380, training loss: 7.184001445770264 = 0.7847068905830383 + 1.0 * 6.399294376373291
Epoch 380, val loss: 1.0067744255065918
Epoch 390, training loss: 7.154444217681885 = 0.7519477605819702 + 1.0 * 6.402496337890625
Epoch 390, val loss: 0.9861955642700195
Epoch 400, training loss: 7.114938259124756 = 0.7221028208732605 + 1.0 * 6.39283561706543
Epoch 400, val loss: 0.9681718349456787
Epoch 410, training loss: 7.083472728729248 = 0.6943739652633667 + 1.0 * 6.389098644256592
Epoch 410, val loss: 0.9523695111274719
Epoch 420, training loss: 7.06247091293335 = 0.66820228099823 + 1.0 * 6.39426851272583
Epoch 420, val loss: 0.9382010102272034
Epoch 430, training loss: 7.02776575088501 = 0.6433262228965759 + 1.0 * 6.384439468383789
Epoch 430, val loss: 0.9254511594772339
Epoch 440, training loss: 7.000669002532959 = 0.6193223595619202 + 1.0 * 6.381346702575684
Epoch 440, val loss: 0.9138764142990112
Epoch 450, training loss: 6.974755764007568 = 0.5959548354148865 + 1.0 * 6.378800868988037
Epoch 450, val loss: 0.9032621383666992
Epoch 460, training loss: 6.951361179351807 = 0.573111355304718 + 1.0 * 6.378249645233154
Epoch 460, val loss: 0.8934290409088135
Epoch 470, training loss: 6.923574447631836 = 0.5505790114402771 + 1.0 * 6.372995376586914
Epoch 470, val loss: 0.8844783306121826
Epoch 480, training loss: 6.898785591125488 = 0.5283078551292419 + 1.0 * 6.370477676391602
Epoch 480, val loss: 0.8762784004211426
Epoch 490, training loss: 6.877205848693848 = 0.5063170790672302 + 1.0 * 6.370888710021973
Epoch 490, val loss: 0.8688903450965881
Epoch 500, training loss: 6.851933479309082 = 0.4847777783870697 + 1.0 * 6.3671555519104
Epoch 500, val loss: 0.8624420762062073
Epoch 510, training loss: 6.829697132110596 = 0.4635717570781708 + 1.0 * 6.366125583648682
Epoch 510, val loss: 0.8569867610931396
Epoch 520, training loss: 6.807982921600342 = 0.44276121258735657 + 1.0 * 6.3652215003967285
Epoch 520, val loss: 0.8525477647781372
Epoch 530, training loss: 6.786511421203613 = 0.42252469062805176 + 1.0 * 6.363986492156982
Epoch 530, val loss: 0.8491895198822021
Epoch 540, training loss: 6.762003421783447 = 0.40284788608551025 + 1.0 * 6.359155654907227
Epoch 540, val loss: 0.8468834757804871
Epoch 550, training loss: 6.744081497192383 = 0.38371336460113525 + 1.0 * 6.360368251800537
Epoch 550, val loss: 0.8455715775489807
Epoch 560, training loss: 6.725554466247559 = 0.3653329908847809 + 1.0 * 6.3602213859558105
Epoch 560, val loss: 0.8451193571090698
Epoch 570, training loss: 6.704328536987305 = 0.3476537764072418 + 1.0 * 6.356674671173096
Epoch 570, val loss: 0.8457251787185669
Epoch 580, training loss: 6.683101654052734 = 0.33076757192611694 + 1.0 * 6.352334022521973
Epoch 580, val loss: 0.8471199870109558
Epoch 590, training loss: 6.666524887084961 = 0.3146537244319916 + 1.0 * 6.351871013641357
Epoch 590, val loss: 0.8493838906288147
Epoch 600, training loss: 6.647858142852783 = 0.29921388626098633 + 1.0 * 6.348644256591797
Epoch 600, val loss: 0.852416455745697
Epoch 610, training loss: 6.638638019561768 = 0.2844413220882416 + 1.0 * 6.354196548461914
Epoch 610, val loss: 0.8561972975730896
Epoch 620, training loss: 6.621316909790039 = 0.2704397141933441 + 1.0 * 6.350877285003662
Epoch 620, val loss: 0.8604270219802856
Epoch 630, training loss: 6.603878974914551 = 0.25718796253204346 + 1.0 * 6.346691131591797
Epoch 630, val loss: 0.8653500080108643
Epoch 640, training loss: 6.594104766845703 = 0.244562566280365 + 1.0 * 6.349542140960693
Epoch 640, val loss: 0.8707858324050903
Epoch 650, training loss: 6.57553243637085 = 0.23258832097053528 + 1.0 * 6.342944145202637
Epoch 650, val loss: 0.876672625541687
Epoch 660, training loss: 6.56467342376709 = 0.22117774188518524 + 1.0 * 6.343495845794678
Epoch 660, val loss: 0.8830233812332153
Epoch 670, training loss: 6.5504841804504395 = 0.21037684381008148 + 1.0 * 6.340107440948486
Epoch 670, val loss: 0.8896755576133728
Epoch 680, training loss: 6.541401386260986 = 0.20012375712394714 + 1.0 * 6.341277599334717
Epoch 680, val loss: 0.8966681361198425
Epoch 690, training loss: 6.528323650360107 = 0.1904071420431137 + 1.0 * 6.337916374206543
Epoch 690, val loss: 0.9039707779884338
Epoch 700, training loss: 6.515960216522217 = 0.18116477131843567 + 1.0 * 6.3347954750061035
Epoch 700, val loss: 0.9114960432052612
Epoch 710, training loss: 6.514930725097656 = 0.17237533628940582 + 1.0 * 6.342555522918701
Epoch 710, val loss: 0.9192367196083069
Epoch 720, training loss: 6.500737190246582 = 0.16407564282417297 + 1.0 * 6.336661338806152
Epoch 720, val loss: 0.9270214438438416
Epoch 730, training loss: 6.48743200302124 = 0.15621010959148407 + 1.0 * 6.331222057342529
Epoch 730, val loss: 0.9349973797798157
Epoch 740, training loss: 6.493756294250488 = 0.148763045668602 + 1.0 * 6.3449931144714355
Epoch 740, val loss: 0.9430387020111084
Epoch 750, training loss: 6.471215724945068 = 0.14171868562698364 + 1.0 * 6.32949686050415
Epoch 750, val loss: 0.9510895013809204
Epoch 760, training loss: 6.463378429412842 = 0.13504323363304138 + 1.0 * 6.328335285186768
Epoch 760, val loss: 0.9592742919921875
Epoch 770, training loss: 6.457963943481445 = 0.12870672345161438 + 1.0 * 6.329257011413574
Epoch 770, val loss: 0.9674705266952515
Epoch 780, training loss: 6.4493489265441895 = 0.12271098792552948 + 1.0 * 6.3266377449035645
Epoch 780, val loss: 0.9756125211715698
Epoch 790, training loss: 6.4460768699646 = 0.11704942584037781 + 1.0 * 6.3290276527404785
Epoch 790, val loss: 0.983756422996521
Epoch 800, training loss: 6.437240123748779 = 0.11170576512813568 + 1.0 * 6.325534343719482
Epoch 800, val loss: 0.9919940233230591
Epoch 810, training loss: 6.431285381317139 = 0.10664280503988266 + 1.0 * 6.324642658233643
Epoch 810, val loss: 1.000070571899414
Epoch 820, training loss: 6.423908233642578 = 0.10184342414140701 + 1.0 * 6.3220648765563965
Epoch 820, val loss: 1.008132815361023
Epoch 830, training loss: 6.420008659362793 = 0.097295843064785 + 1.0 * 6.3227128982543945
Epoch 830, val loss: 1.016177773475647
Epoch 840, training loss: 6.416510581970215 = 0.09298299252986908 + 1.0 * 6.323527812957764
Epoch 840, val loss: 1.024021863937378
Epoch 850, training loss: 6.417207717895508 = 0.08891092240810394 + 1.0 * 6.328296661376953
Epoch 850, val loss: 1.0317984819412231
Epoch 860, training loss: 6.40702486038208 = 0.08507978171110153 + 1.0 * 6.3219451904296875
Epoch 860, val loss: 1.0394489765167236
Epoch 870, training loss: 6.398246765136719 = 0.08146020770072937 + 1.0 * 6.316786766052246
Epoch 870, val loss: 1.0471277236938477
Epoch 880, training loss: 6.3942766189575195 = 0.07802240550518036 + 1.0 * 6.316254138946533
Epoch 880, val loss: 1.0547515153884888
Epoch 890, training loss: 6.397627830505371 = 0.07475278526544571 + 1.0 * 6.322875022888184
Epoch 890, val loss: 1.0621939897537231
Epoch 900, training loss: 6.387314796447754 = 0.07164530456066132 + 1.0 * 6.315669536590576
Epoch 900, val loss: 1.069428563117981
Epoch 910, training loss: 6.396134853363037 = 0.06871232390403748 + 1.0 * 6.327422618865967
Epoch 910, val loss: 1.0765893459320068
Epoch 920, training loss: 6.379579544067383 = 0.06592927873134613 + 1.0 * 6.313650131225586
Epoch 920, val loss: 1.0836881399154663
Epoch 930, training loss: 6.37470817565918 = 0.06328880786895752 + 1.0 * 6.311419486999512
Epoch 930, val loss: 1.090742826461792
Epoch 940, training loss: 6.372875213623047 = 0.060782138258218765 + 1.0 * 6.312093257904053
Epoch 940, val loss: 1.09770667552948
Epoch 950, training loss: 6.371068000793457 = 0.058396048843860626 + 1.0 * 6.312672138214111
Epoch 950, val loss: 1.1045362949371338
Epoch 960, training loss: 6.366664886474609 = 0.056129228323698044 + 1.0 * 6.310535430908203
Epoch 960, val loss: 1.1112936735153198
Epoch 970, training loss: 6.368980884552002 = 0.05397728085517883 + 1.0 * 6.315003395080566
Epoch 970, val loss: 1.1179739236831665
Epoch 980, training loss: 6.36073637008667 = 0.05192345380783081 + 1.0 * 6.308813095092773
Epoch 980, val loss: 1.1244632005691528
Epoch 990, training loss: 6.3632307052612305 = 0.049976199865341187 + 1.0 * 6.313254356384277
Epoch 990, val loss: 1.1309059858322144
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.7923036373220875
The final CL Acc:0.76914, 0.00698, The final GNN Acc:0.79968, 0.00524
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13216])
remove edge: torch.Size([2, 7952])
updated graph: torch.Size([2, 10612])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.549688339233398 = 1.952897548675537 + 1.0 * 8.596790313720703
Epoch 0, val loss: 1.9603455066680908
Epoch 10, training loss: 10.538141250610352 = 1.9417505264282227 + 1.0 * 8.596390724182129
Epoch 10, val loss: 1.9486207962036133
Epoch 20, training loss: 10.521673202514648 = 1.9278991222381592 + 1.0 * 8.59377384185791
Epoch 20, val loss: 1.9339942932128906
Epoch 30, training loss: 10.485468864440918 = 1.9086300134658813 + 1.0 * 8.576838493347168
Epoch 30, val loss: 1.9136990308761597
Epoch 40, training loss: 10.364412307739258 = 1.884096622467041 + 1.0 * 8.480315208435059
Epoch 40, val loss: 1.8890048265457153
Epoch 50, training loss: 9.76899528503418 = 1.8594858646392822 + 1.0 * 7.909509181976318
Epoch 50, val loss: 1.8648563623428345
Epoch 60, training loss: 9.260602951049805 = 1.8389455080032349 + 1.0 * 7.421657085418701
Epoch 60, val loss: 1.8450647592544556
Epoch 70, training loss: 8.955167770385742 = 1.8207327127456665 + 1.0 * 7.134435176849365
Epoch 70, val loss: 1.8273968696594238
Epoch 80, training loss: 8.772124290466309 = 1.8039700984954834 + 1.0 * 6.968153953552246
Epoch 80, val loss: 1.81156587600708
Epoch 90, training loss: 8.649943351745605 = 1.7871235609054565 + 1.0 * 6.862819671630859
Epoch 90, val loss: 1.7965977191925049
Epoch 100, training loss: 8.561145782470703 = 1.7700914144515991 + 1.0 * 6.7910542488098145
Epoch 100, val loss: 1.781716227531433
Epoch 110, training loss: 8.487841606140137 = 1.7528432607650757 + 1.0 * 6.7349982261657715
Epoch 110, val loss: 1.7662150859832764
Epoch 120, training loss: 8.42969036102295 = 1.7343847751617432 + 1.0 * 6.695305824279785
Epoch 120, val loss: 1.7493009567260742
Epoch 130, training loss: 8.379446983337402 = 1.713590383529663 + 1.0 * 6.665856838226318
Epoch 130, val loss: 1.7304362058639526
Epoch 140, training loss: 8.33348274230957 = 1.6892621517181396 + 1.0 * 6.64422082901001
Epoch 140, val loss: 1.7088630199432373
Epoch 150, training loss: 8.28589916229248 = 1.6603349447250366 + 1.0 * 6.625564098358154
Epoch 150, val loss: 1.6838128566741943
Epoch 160, training loss: 8.234002113342285 = 1.6256603002548218 + 1.0 * 6.608342170715332
Epoch 160, val loss: 1.6540340185165405
Epoch 170, training loss: 8.175908088684082 = 1.5849635601043701 + 1.0 * 6.590944290161133
Epoch 170, val loss: 1.6193416118621826
Epoch 180, training loss: 8.111577033996582 = 1.5380347967147827 + 1.0 * 6.57354211807251
Epoch 180, val loss: 1.5791605710983276
Epoch 190, training loss: 8.040616989135742 = 1.484684944152832 + 1.0 * 6.55593204498291
Epoch 190, val loss: 1.5336618423461914
Epoch 200, training loss: 7.965385437011719 = 1.4260934591293335 + 1.0 * 6.539291858673096
Epoch 200, val loss: 1.484082818031311
Epoch 210, training loss: 7.890431880950928 = 1.3633480072021484 + 1.0 * 6.527083873748779
Epoch 210, val loss: 1.4313907623291016
Epoch 220, training loss: 7.814302921295166 = 1.300137996673584 + 1.0 * 6.514164924621582
Epoch 220, val loss: 1.378808856010437
Epoch 230, training loss: 7.738259792327881 = 1.2376022338867188 + 1.0 * 6.500657558441162
Epoch 230, val loss: 1.3274253606796265
Epoch 240, training loss: 7.6666765213012695 = 1.1765953302383423 + 1.0 * 6.490081310272217
Epoch 240, val loss: 1.2777000665664673
Epoch 250, training loss: 7.599606037139893 = 1.1185921430587769 + 1.0 * 6.481013774871826
Epoch 250, val loss: 1.230905294418335
Epoch 260, training loss: 7.538902759552002 = 1.0653318166732788 + 1.0 * 6.473570823669434
Epoch 260, val loss: 1.1886323690414429
Epoch 270, training loss: 7.482258319854736 = 1.015832781791687 + 1.0 * 6.46642541885376
Epoch 270, val loss: 1.1495661735534668
Epoch 280, training loss: 7.429703235626221 = 0.9698882102966309 + 1.0 * 6.45981502532959
Epoch 280, val loss: 1.1136912107467651
Epoch 290, training loss: 7.377762794494629 = 0.9269082546234131 + 1.0 * 6.450854301452637
Epoch 290, val loss: 1.0804868936538696
Epoch 300, training loss: 7.334611415863037 = 0.886150062084198 + 1.0 * 6.448461532592773
Epoch 300, val loss: 1.049088478088379
Epoch 310, training loss: 7.287115097045898 = 0.8475110530853271 + 1.0 * 6.43960428237915
Epoch 310, val loss: 1.0195033550262451
Epoch 320, training loss: 7.245452880859375 = 0.8104333281517029 + 1.0 * 6.435019493103027
Epoch 320, val loss: 0.9912282228469849
Epoch 330, training loss: 7.20319128036499 = 0.7747994065284729 + 1.0 * 6.428391933441162
Epoch 330, val loss: 0.9642353057861328
Epoch 340, training loss: 7.1655097007751465 = 0.7405597567558289 + 1.0 * 6.424950122833252
Epoch 340, val loss: 0.9386324286460876
Epoch 350, training loss: 7.128706932067871 = 0.7080050110816956 + 1.0 * 6.42070198059082
Epoch 350, val loss: 0.9146014451980591
Epoch 360, training loss: 7.092518329620361 = 0.6773127317428589 + 1.0 * 6.415205478668213
Epoch 360, val loss: 0.8925575017929077
Epoch 370, training loss: 7.057770252227783 = 0.647905170917511 + 1.0 * 6.409864902496338
Epoch 370, val loss: 0.8720689415931702
Epoch 380, training loss: 7.0317487716674805 = 0.6197441816329956 + 1.0 * 6.412004470825195
Epoch 380, val loss: 0.853215217590332
Epoch 390, training loss: 6.997107982635498 = 0.5929769277572632 + 1.0 * 6.404130935668945
Epoch 390, val loss: 0.8362248539924622
Epoch 400, training loss: 6.968308448791504 = 0.5674440860748291 + 1.0 * 6.400864124298096
Epoch 400, val loss: 0.820905864238739
Epoch 410, training loss: 6.938968658447266 = 0.5430773496627808 + 1.0 * 6.395891189575195
Epoch 410, val loss: 0.8071507215499878
Epoch 420, training loss: 6.913341522216797 = 0.5196384787559509 + 1.0 * 6.393702983856201
Epoch 420, val loss: 0.7948553562164307
Epoch 430, training loss: 6.892333507537842 = 0.4972352981567383 + 1.0 * 6.3950982093811035
Epoch 430, val loss: 0.7840204834938049
Epoch 440, training loss: 6.86354923248291 = 0.4759172797203064 + 1.0 * 6.387631893157959
Epoch 440, val loss: 0.7746367454528809
Epoch 450, training loss: 6.838319778442383 = 0.45544496178627014 + 1.0 * 6.382874965667725
Epoch 450, val loss: 0.7663959860801697
Epoch 460, training loss: 6.832907199859619 = 0.43574702739715576 + 1.0 * 6.397160053253174
Epoch 460, val loss: 0.7592183947563171
Epoch 470, training loss: 6.8023176193237305 = 0.41702425479888916 + 1.0 * 6.385293483734131
Epoch 470, val loss: 0.7533336281776428
Epoch 480, training loss: 6.775135517120361 = 0.39909079670906067 + 1.0 * 6.376044750213623
Epoch 480, val loss: 0.7485368847846985
Epoch 490, training loss: 6.754420757293701 = 0.38180121779441833 + 1.0 * 6.37261962890625
Epoch 490, val loss: 0.7446247935295105
Epoch 500, training loss: 6.746220588684082 = 0.3650992214679718 + 1.0 * 6.3811211585998535
Epoch 500, val loss: 0.7417062520980835
Epoch 510, training loss: 6.716926574707031 = 0.34911084175109863 + 1.0 * 6.3678154945373535
Epoch 510, val loss: 0.7397037148475647
Epoch 520, training loss: 6.699228286743164 = 0.33369821310043335 + 1.0 * 6.365530014038086
Epoch 520, val loss: 0.738676905632019
Epoch 530, training loss: 6.693979263305664 = 0.3187410533428192 + 1.0 * 6.375238418579102
Epoch 530, val loss: 0.7384533286094666
Epoch 540, training loss: 6.6662797927856445 = 0.30436018109321594 + 1.0 * 6.361919403076172
Epoch 540, val loss: 0.7391351461410522
Epoch 550, training loss: 6.65090274810791 = 0.29036515951156616 + 1.0 * 6.360537528991699
Epoch 550, val loss: 0.7406654357910156
Epoch 560, training loss: 6.647922039031982 = 0.2766994535923004 + 1.0 * 6.371222496032715
Epoch 560, val loss: 0.7429538369178772
Epoch 570, training loss: 6.6203203201293945 = 0.26342180371284485 + 1.0 * 6.356898307800293
Epoch 570, val loss: 0.7461034655570984
Epoch 580, training loss: 6.606563568115234 = 0.2504344880580902 + 1.0 * 6.356129169464111
Epoch 580, val loss: 0.7500148415565491
Epoch 590, training loss: 6.601737022399902 = 0.23770248889923096 + 1.0 * 6.364034652709961
Epoch 590, val loss: 0.7545962333679199
Epoch 600, training loss: 6.577298641204834 = 0.2253037393093109 + 1.0 * 6.35199499130249
Epoch 600, val loss: 0.7598773241043091
Epoch 610, training loss: 6.5647172927856445 = 0.2132008969783783 + 1.0 * 6.351516246795654
Epoch 610, val loss: 0.7658010125160217
Epoch 620, training loss: 6.554852485656738 = 0.20139580965042114 + 1.0 * 6.353456497192383
Epoch 620, val loss: 0.7723256945610046
Epoch 630, training loss: 6.540596008300781 = 0.19002479314804077 + 1.0 * 6.350571155548096
Epoch 630, val loss: 0.7794703245162964
Epoch 640, training loss: 6.530766010284424 = 0.17909610271453857 + 1.0 * 6.351669788360596
Epoch 640, val loss: 0.7871392369270325
Epoch 650, training loss: 6.516453742980957 = 0.16869895160198212 + 1.0 * 6.347754955291748
Epoch 650, val loss: 0.7952434420585632
Epoch 660, training loss: 6.503190040588379 = 0.15884502232074738 + 1.0 * 6.3443450927734375
Epoch 660, val loss: 0.8038631677627563
Epoch 670, training loss: 6.4915547370910645 = 0.1494927704334259 + 1.0 * 6.342061996459961
Epoch 670, val loss: 0.8128371834754944
Epoch 680, training loss: 6.4884185791015625 = 0.1406944841146469 + 1.0 * 6.347723960876465
Epoch 680, val loss: 0.8221898674964905
Epoch 690, training loss: 6.475108623504639 = 0.13248836994171143 + 1.0 * 6.342620372772217
Epoch 690, val loss: 0.8318593502044678
Epoch 700, training loss: 6.463982105255127 = 0.12482167780399323 + 1.0 * 6.339160442352295
Epoch 700, val loss: 0.8417173027992249
Epoch 710, training loss: 6.4651689529418945 = 0.11768433451652527 + 1.0 * 6.347484588623047
Epoch 710, val loss: 0.8517654538154602
Epoch 720, training loss: 6.449073314666748 = 0.11107439547777176 + 1.0 * 6.337998867034912
Epoch 720, val loss: 0.8619158267974854
Epoch 730, training loss: 6.439952373504639 = 0.1049249991774559 + 1.0 * 6.33502721786499
Epoch 730, val loss: 0.8721458315849304
Epoch 740, training loss: 6.434776306152344 = 0.09919481724500656 + 1.0 * 6.335581302642822
Epoch 740, val loss: 0.8824632167816162
Epoch 750, training loss: 6.4271721839904785 = 0.09387733042240143 + 1.0 * 6.333294868469238
Epoch 750, val loss: 0.892799973487854
Epoch 760, training loss: 6.421395778656006 = 0.08894788473844528 + 1.0 * 6.3324480056762695
Epoch 760, val loss: 0.9031338691711426
Epoch 770, training loss: 6.41750431060791 = 0.08435828238725662 + 1.0 * 6.333146095275879
Epoch 770, val loss: 0.9134597182273865
Epoch 780, training loss: 6.4093427658081055 = 0.08008432388305664 + 1.0 * 6.329258441925049
Epoch 780, val loss: 0.9236935377120972
Epoch 790, training loss: 6.405694007873535 = 0.07611402124166489 + 1.0 * 6.329579830169678
Epoch 790, val loss: 0.9339051246643066
Epoch 800, training loss: 6.401973247528076 = 0.07240024954080582 + 1.0 * 6.329573154449463
Epoch 800, val loss: 0.9440361857414246
Epoch 810, training loss: 6.395077228546143 = 0.06893441081047058 + 1.0 * 6.32614278793335
Epoch 810, val loss: 0.9540517330169678
Epoch 820, training loss: 6.395768165588379 = 0.06569568067789078 + 1.0 * 6.330072402954102
Epoch 820, val loss: 0.964012086391449
Epoch 830, training loss: 6.390743255615234 = 0.06268030405044556 + 1.0 * 6.328063011169434
Epoch 830, val loss: 0.9737839102745056
Epoch 840, training loss: 6.382434368133545 = 0.059857435524463654 + 1.0 * 6.322576999664307
Epoch 840, val loss: 0.9834613800048828
Epoch 850, training loss: 6.379313945770264 = 0.057209454476833344 + 1.0 * 6.322104454040527
Epoch 850, val loss: 0.9930441379547119
Epoch 860, training loss: 6.386765003204346 = 0.0547221377491951 + 1.0 * 6.332042694091797
Epoch 860, val loss: 1.0024759769439697
Epoch 870, training loss: 6.374337196350098 = 0.05239897593855858 + 1.0 * 6.3219380378723145
Epoch 870, val loss: 1.0117647647857666
Epoch 880, training loss: 6.371300220489502 = 0.05021335929632187 + 1.0 * 6.321086883544922
Epoch 880, val loss: 1.0209318399429321
Epoch 890, training loss: 6.366646766662598 = 0.048156313598155975 + 1.0 * 6.318490505218506
Epoch 890, val loss: 1.0299501419067383
Epoch 900, training loss: 6.363481521606445 = 0.04622524231672287 + 1.0 * 6.317256450653076
Epoch 900, val loss: 1.0388447046279907
Epoch 910, training loss: 6.361608028411865 = 0.044395145028829575 + 1.0 * 6.31721305847168
Epoch 910, val loss: 1.0476425886154175
Epoch 920, training loss: 6.360931396484375 = 0.04266917333006859 + 1.0 * 6.318262100219727
Epoch 920, val loss: 1.0562920570373535
Epoch 930, training loss: 6.357151508331299 = 0.04104889929294586 + 1.0 * 6.316102504730225
Epoch 930, val loss: 1.0648084878921509
Epoch 940, training loss: 6.35396146774292 = 0.039511196315288544 + 1.0 * 6.314450263977051
Epoch 940, val loss: 1.0732046365737915
Epoch 950, training loss: 6.361202716827393 = 0.03805612027645111 + 1.0 * 6.323146820068359
Epoch 950, val loss: 1.0814398527145386
Epoch 960, training loss: 6.351321697235107 = 0.03668111190199852 + 1.0 * 6.314640522003174
Epoch 960, val loss: 1.0895767211914062
Epoch 970, training loss: 6.346780776977539 = 0.035379808396101 + 1.0 * 6.311400890350342
Epoch 970, val loss: 1.0976111888885498
Epoch 980, training loss: 6.349673271179199 = 0.03413918614387512 + 1.0 * 6.3155341148376465
Epoch 980, val loss: 1.105535864830017
Epoch 990, training loss: 6.343125343322754 = 0.032964952290058136 + 1.0 * 6.310160160064697
Epoch 990, val loss: 1.1133772134780884
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 10.556730270385742 = 1.9599320888519287 + 1.0 * 8.596797943115234
Epoch 0, val loss: 1.9634873867034912
Epoch 10, training loss: 10.545781135559082 = 1.9492619037628174 + 1.0 * 8.596519470214844
Epoch 10, val loss: 1.9527812004089355
Epoch 20, training loss: 10.530791282653809 = 1.936401128768921 + 1.0 * 8.594389915466309
Epoch 20, val loss: 1.939849615097046
Epoch 30, training loss: 10.496366500854492 = 1.918351411819458 + 1.0 * 8.578015327453613
Epoch 30, val loss: 1.9217976331710815
Epoch 40, training loss: 10.378850936889648 = 1.894091010093689 + 1.0 * 8.484760284423828
Epoch 40, val loss: 1.8984354734420776
Epoch 50, training loss: 10.017204284667969 = 1.8681612014770508 + 1.0 * 8.149043083190918
Epoch 50, val loss: 1.8745208978652954
Epoch 60, training loss: 9.631311416625977 = 1.845323085784912 + 1.0 * 7.7859883308410645
Epoch 60, val loss: 1.853794813156128
Epoch 70, training loss: 9.110344886779785 = 1.8294047117233276 + 1.0 * 7.280940532684326
Epoch 70, val loss: 1.8393914699554443
Epoch 80, training loss: 8.845727920532227 = 1.8164458274841309 + 1.0 * 7.029282093048096
Epoch 80, val loss: 1.8266634941101074
Epoch 90, training loss: 8.671127319335938 = 1.7987561225891113 + 1.0 * 6.872371673583984
Epoch 90, val loss: 1.8099812269210815
Epoch 100, training loss: 8.563331604003906 = 1.779293179512024 + 1.0 * 6.784038066864014
Epoch 100, val loss: 1.7930158376693726
Epoch 110, training loss: 8.483588218688965 = 1.7613147497177124 + 1.0 * 6.722273349761963
Epoch 110, val loss: 1.777063012123108
Epoch 120, training loss: 8.421167373657227 = 1.7436738014221191 + 1.0 * 6.677493572235107
Epoch 120, val loss: 1.760562539100647
Epoch 130, training loss: 8.369980812072754 = 1.7247376441955566 + 1.0 * 6.645243167877197
Epoch 130, val loss: 1.7427738904953003
Epoch 140, training loss: 8.320672988891602 = 1.7035655975341797 + 1.0 * 6.617107391357422
Epoch 140, val loss: 1.7234517335891724
Epoch 150, training loss: 8.27100944519043 = 1.6791294813156128 + 1.0 * 6.5918803215026855
Epoch 150, val loss: 1.7020025253295898
Epoch 160, training loss: 8.219616889953613 = 1.6510714292526245 + 1.0 * 6.568545341491699
Epoch 160, val loss: 1.6779582500457764
Epoch 170, training loss: 8.167695999145508 = 1.6190341711044312 + 1.0 * 6.548662185668945
Epoch 170, val loss: 1.650665044784546
Epoch 180, training loss: 8.114449501037598 = 1.5827205181121826 + 1.0 * 6.531728744506836
Epoch 180, val loss: 1.6199537515640259
Epoch 190, training loss: 8.06071662902832 = 1.5429587364196777 + 1.0 * 6.517758369445801
Epoch 190, val loss: 1.5866076946258545
Epoch 200, training loss: 8.006062507629395 = 1.5010498762130737 + 1.0 * 6.505012512207031
Epoch 200, val loss: 1.551913857460022
Epoch 210, training loss: 7.95287561416626 = 1.4587560892105103 + 1.0 * 6.494119644165039
Epoch 210, val loss: 1.517500877380371
Epoch 220, training loss: 7.9023542404174805 = 1.416831135749817 + 1.0 * 6.485523223876953
Epoch 220, val loss: 1.4839218854904175
Epoch 230, training loss: 7.8517327308654785 = 1.376792311668396 + 1.0 * 6.474940299987793
Epoch 230, val loss: 1.4527391195297241
Epoch 240, training loss: 7.804440498352051 = 1.3386094570159912 + 1.0 * 6.4658308029174805
Epoch 240, val loss: 1.4234589338302612
Epoch 250, training loss: 7.759016513824463 = 1.3015799522399902 + 1.0 * 6.457436561584473
Epoch 250, val loss: 1.39535391330719
Epoch 260, training loss: 7.7271881103515625 = 1.2653535604476929 + 1.0 * 6.46183443069458
Epoch 260, val loss: 1.3679957389831543
Epoch 270, training loss: 7.676392555236816 = 1.2299836874008179 + 1.0 * 6.446408748626709
Epoch 270, val loss: 1.3416383266448975
Epoch 280, training loss: 7.632822513580322 = 1.1946563720703125 + 1.0 * 6.43816614151001
Epoch 280, val loss: 1.315123200416565
Epoch 290, training loss: 7.589827060699463 = 1.1580885648727417 + 1.0 * 6.431738376617432
Epoch 290, val loss: 1.2876261472702026
Epoch 300, training loss: 7.553413391113281 = 1.1198992729187012 + 1.0 * 6.43351411819458
Epoch 300, val loss: 1.2586368322372437
Epoch 310, training loss: 7.504362106323242 = 1.0802257061004639 + 1.0 * 6.424136638641357
Epoch 310, val loss: 1.228558897972107
Epoch 320, training loss: 7.456214904785156 = 1.0388367176055908 + 1.0 * 6.4173784255981445
Epoch 320, val loss: 1.1969679594039917
Epoch 330, training loss: 7.408104419708252 = 0.9955911636352539 + 1.0 * 6.412513256072998
Epoch 330, val loss: 1.1636840105056763
Epoch 340, training loss: 7.364626884460449 = 0.9513619542121887 + 1.0 * 6.413264751434326
Epoch 340, val loss: 1.1294060945510864
Epoch 350, training loss: 7.316208839416504 = 0.9079322218894958 + 1.0 * 6.408276557922363
Epoch 350, val loss: 1.095994472503662
Epoch 360, training loss: 7.267033576965332 = 0.8656290769577026 + 1.0 * 6.40140438079834
Epoch 360, val loss: 1.0634969472885132
Epoch 370, training loss: 7.223034381866455 = 0.824876606464386 + 1.0 * 6.398157596588135
Epoch 370, val loss: 1.0323671102523804
Epoch 380, training loss: 7.180932521820068 = 0.7863345742225647 + 1.0 * 6.394598007202148
Epoch 380, val loss: 1.003363847732544
Epoch 390, training loss: 7.142528533935547 = 0.7506157159805298 + 1.0 * 6.391912937164307
Epoch 390, val loss: 0.9772191047668457
Epoch 400, training loss: 7.105831146240234 = 0.7173802852630615 + 1.0 * 6.388451099395752
Epoch 400, val loss: 0.9537476897239685
Epoch 410, training loss: 7.076888084411621 = 0.6863613724708557 + 1.0 * 6.39052677154541
Epoch 410, val loss: 0.9326522946357727
Epoch 420, training loss: 7.041343688964844 = 0.6577756404876709 + 1.0 * 6.383568286895752
Epoch 420, val loss: 0.9142010807991028
Epoch 430, training loss: 7.012107849121094 = 0.631228506565094 + 1.0 * 6.3808794021606445
Epoch 430, val loss: 0.8980960845947266
Epoch 440, training loss: 6.982905387878418 = 0.6061757802963257 + 1.0 * 6.376729488372803
Epoch 440, val loss: 0.8837997317314148
Epoch 450, training loss: 6.963418960571289 = 0.5823301076889038 + 1.0 * 6.381088733673096
Epoch 450, val loss: 0.8711352944374084
Epoch 460, training loss: 6.932289123535156 = 0.5598036646842957 + 1.0 * 6.372485637664795
Epoch 460, val loss: 0.8599910736083984
Epoch 470, training loss: 6.908113479614258 = 0.5383684635162354 + 1.0 * 6.369744777679443
Epoch 470, val loss: 0.8502912521362305
Epoch 480, training loss: 6.884415149688721 = 0.5177142024040222 + 1.0 * 6.366701126098633
Epoch 480, val loss: 0.8416932821273804
Epoch 490, training loss: 6.878901481628418 = 0.4976508915424347 + 1.0 * 6.381250381469727
Epoch 490, val loss: 0.8340132832527161
Epoch 500, training loss: 6.844840049743652 = 0.47845908999443054 + 1.0 * 6.3663811683654785
Epoch 500, val loss: 0.8272826075553894
Epoch 510, training loss: 6.821967601776123 = 0.45982757210731506 + 1.0 * 6.36214017868042
Epoch 510, val loss: 0.8214223980903625
Epoch 520, training loss: 6.807878017425537 = 0.44162431359291077 + 1.0 * 6.366253852844238
Epoch 520, val loss: 0.8162218928337097
Epoch 530, training loss: 6.782209396362305 = 0.42395395040512085 + 1.0 * 6.358255386352539
Epoch 530, val loss: 0.8117886185646057
Epoch 540, training loss: 6.7619781494140625 = 0.4067513942718506 + 1.0 * 6.355226516723633
Epoch 540, val loss: 0.8081442713737488
Epoch 550, training loss: 6.746585369110107 = 0.3899799883365631 + 1.0 * 6.356605529785156
Epoch 550, val loss: 0.8052546381950378
Epoch 560, training loss: 6.729687213897705 = 0.37374985218048096 + 1.0 * 6.355937480926514
Epoch 560, val loss: 0.8030948042869568
Epoch 570, training loss: 6.709141731262207 = 0.3581029772758484 + 1.0 * 6.351038932800293
Epoch 570, val loss: 0.8017390370368958
Epoch 580, training loss: 6.69441032409668 = 0.34295567870140076 + 1.0 * 6.351454734802246
Epoch 580, val loss: 0.8011326193809509
Epoch 590, training loss: 6.675321578979492 = 0.32829099893569946 + 1.0 * 6.3470306396484375
Epoch 590, val loss: 0.8012418746948242
Epoch 600, training loss: 6.660679340362549 = 0.3140192925930023 + 1.0 * 6.346660137176514
Epoch 600, val loss: 0.8020418286323547
Epoch 610, training loss: 6.649641990661621 = 0.3001163601875305 + 1.0 * 6.349525451660156
Epoch 610, val loss: 0.803341805934906
Epoch 620, training loss: 6.629295825958252 = 0.2866365313529968 + 1.0 * 6.3426594734191895
Epoch 620, val loss: 0.8052507638931274
Epoch 630, training loss: 6.61467981338501 = 0.27341705560684204 + 1.0 * 6.3412628173828125
Epoch 630, val loss: 0.8076576590538025
Epoch 640, training loss: 6.6059465408325195 = 0.2604077160358429 + 1.0 * 6.34553861618042
Epoch 640, val loss: 0.8104786276817322
Epoch 650, training loss: 6.589919090270996 = 0.24775737524032593 + 1.0 * 6.342161655426025
Epoch 650, val loss: 0.8136793375015259
Epoch 660, training loss: 6.572273254394531 = 0.235337495803833 + 1.0 * 6.336935520172119
Epoch 660, val loss: 0.817297101020813
Epoch 670, training loss: 6.558879375457764 = 0.22317636013031006 + 1.0 * 6.335702896118164
Epoch 670, val loss: 0.8212865591049194
Epoch 680, training loss: 6.548185348510742 = 0.21135830879211426 + 1.0 * 6.336827278137207
Epoch 680, val loss: 0.8255547285079956
Epoch 690, training loss: 6.535597801208496 = 0.19996775686740875 + 1.0 * 6.335629940032959
Epoch 690, val loss: 0.8302531838417053
Epoch 700, training loss: 6.520596027374268 = 0.18898673355579376 + 1.0 * 6.33160924911499
Epoch 700, val loss: 0.8353776931762695
Epoch 710, training loss: 6.50894832611084 = 0.17842291295528412 + 1.0 * 6.3305253982543945
Epoch 710, val loss: 0.8408599495887756
Epoch 720, training loss: 6.50229549407959 = 0.16836555302143097 + 1.0 * 6.333930015563965
Epoch 720, val loss: 0.8466774821281433
Epoch 730, training loss: 6.4908976554870605 = 0.15889683365821838 + 1.0 * 6.332000732421875
Epoch 730, val loss: 0.8527763485908508
Epoch 740, training loss: 6.477997303009033 = 0.14994904398918152 + 1.0 * 6.328048229217529
Epoch 740, val loss: 0.8593529462814331
Epoch 750, training loss: 6.4672017097473145 = 0.14149537682533264 + 1.0 * 6.325706481933594
Epoch 750, val loss: 0.8662387132644653
Epoch 760, training loss: 6.462597370147705 = 0.13353781402111053 + 1.0 * 6.329059600830078
Epoch 760, val loss: 0.8734739422798157
Epoch 770, training loss: 6.454526424407959 = 0.12608064711093903 + 1.0 * 6.328445911407471
Epoch 770, val loss: 0.8807901740074158
Epoch 780, training loss: 6.44247579574585 = 0.11915156990289688 + 1.0 * 6.323324203491211
Epoch 780, val loss: 0.888400137424469
Epoch 790, training loss: 6.4332356452941895 = 0.11265277862548828 + 1.0 * 6.320582866668701
Epoch 790, val loss: 0.8962074518203735
Epoch 800, training loss: 6.4361467361450195 = 0.10657166689634323 + 1.0 * 6.329575061798096
Epoch 800, val loss: 0.9041445851325989
Epoch 810, training loss: 6.423987865447998 = 0.10092758387327194 + 1.0 * 6.323060512542725
Epoch 810, val loss: 0.9122298955917358
Epoch 820, training loss: 6.417663097381592 = 0.09565052390098572 + 1.0 * 6.322012424468994
Epoch 820, val loss: 0.9204379916191101
Epoch 830, training loss: 6.41001558303833 = 0.0907459557056427 + 1.0 * 6.31926965713501
Epoch 830, val loss: 0.928495466709137
Epoch 840, training loss: 6.401803493499756 = 0.08617765456438065 + 1.0 * 6.3156256675720215
Epoch 840, val loss: 0.9367867112159729
Epoch 850, training loss: 6.397294044494629 = 0.08190042525529861 + 1.0 * 6.315393447875977
Epoch 850, val loss: 0.9450343251228333
Epoch 860, training loss: 6.395992755889893 = 0.07790631800889969 + 1.0 * 6.318086624145508
Epoch 860, val loss: 0.9531986713409424
Epoch 870, training loss: 6.387513160705566 = 0.0741681307554245 + 1.0 * 6.313344955444336
Epoch 870, val loss: 0.9614031910896301
Epoch 880, training loss: 6.384531021118164 = 0.0706770047545433 + 1.0 * 6.313854217529297
Epoch 880, val loss: 0.9696083068847656
Epoch 890, training loss: 6.379624366760254 = 0.06741021573543549 + 1.0 * 6.312214374542236
Epoch 890, val loss: 0.9777185320854187
Epoch 900, training loss: 6.375726699829102 = 0.06434651464223862 + 1.0 * 6.311380386352539
Epoch 900, val loss: 0.985819399356842
Epoch 910, training loss: 6.378890514373779 = 0.061479706317186356 + 1.0 * 6.317410945892334
Epoch 910, val loss: 0.9937704801559448
Epoch 920, training loss: 6.3713579177856445 = 0.05879323184490204 + 1.0 * 6.312564849853516
Epoch 920, val loss: 1.0015442371368408
Epoch 930, training loss: 6.3640007972717285 = 0.05627543851733208 + 1.0 * 6.307725429534912
Epoch 930, val loss: 1.0093940496444702
Epoch 940, training loss: 6.361473083496094 = 0.05390048027038574 + 1.0 * 6.307572364807129
Epoch 940, val loss: 1.0171425342559814
Epoch 950, training loss: 6.360720157623291 = 0.05166769027709961 + 1.0 * 6.309052467346191
Epoch 950, val loss: 1.0247046947479248
Epoch 960, training loss: 6.357160568237305 = 0.049575332552194595 + 1.0 * 6.3075852394104
Epoch 960, val loss: 1.0322515964508057
Epoch 970, training loss: 6.3533525466918945 = 0.04760495573282242 + 1.0 * 6.3057475090026855
Epoch 970, val loss: 1.0397462844848633
Epoch 980, training loss: 6.3495354652404785 = 0.0457439124584198 + 1.0 * 6.303791522979736
Epoch 980, val loss: 1.0471125841140747
Epoch 990, training loss: 6.3546247482299805 = 0.04398878291249275 + 1.0 * 6.310636043548584
Epoch 990, val loss: 1.0543819665908813
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 10.538877487182617 = 1.9420486688613892 + 1.0 * 8.59682846069336
Epoch 0, val loss: 1.9427571296691895
Epoch 10, training loss: 10.528236389160156 = 1.9317069053649902 + 1.0 * 8.596529960632324
Epoch 10, val loss: 1.9328426122665405
Epoch 20, training loss: 10.512768745422363 = 1.9184949398040771 + 1.0 * 8.594273567199707
Epoch 20, val loss: 1.9197773933410645
Epoch 30, training loss: 10.476423263549805 = 1.899751901626587 + 1.0 * 8.576671600341797
Epoch 30, val loss: 1.9013770818710327
Epoch 40, training loss: 10.341699600219727 = 1.8752484321594238 + 1.0 * 8.466450691223145
Epoch 40, val loss: 1.878549337387085
Epoch 50, training loss: 9.973509788513184 = 1.8511571884155273 + 1.0 * 8.122352600097656
Epoch 50, val loss: 1.8568973541259766
Epoch 60, training loss: 9.449687957763672 = 1.83278489112854 + 1.0 * 7.616903305053711
Epoch 60, val loss: 1.8403770923614502
Epoch 70, training loss: 9.001439094543457 = 1.819558024406433 + 1.0 * 7.181881427764893
Epoch 70, val loss: 1.8280894756317139
Epoch 80, training loss: 8.782524108886719 = 1.806968331336975 + 1.0 * 6.975555419921875
Epoch 80, val loss: 1.8164600133895874
Epoch 90, training loss: 8.653306007385254 = 1.791877269744873 + 1.0 * 6.861428737640381
Epoch 90, val loss: 1.8035225868225098
Epoch 100, training loss: 8.555811882019043 = 1.776382565498352 + 1.0 * 6.7794294357299805
Epoch 100, val loss: 1.7905393838882446
Epoch 110, training loss: 8.476560592651367 = 1.7612645626068115 + 1.0 * 6.715295791625977
Epoch 110, val loss: 1.7775083780288696
Epoch 120, training loss: 8.412504196166992 = 1.7458711862564087 + 1.0 * 6.666632652282715
Epoch 120, val loss: 1.7637872695922852
Epoch 130, training loss: 8.358214378356934 = 1.7286310195922852 + 1.0 * 6.629583358764648
Epoch 130, val loss: 1.7484714984893799
Epoch 140, training loss: 8.307638168334961 = 1.7085423469543457 + 1.0 * 6.599095821380615
Epoch 140, val loss: 1.7311280965805054
Epoch 150, training loss: 8.261563301086426 = 1.6849901676177979 + 1.0 * 6.576572895050049
Epoch 150, val loss: 1.7110530138015747
Epoch 160, training loss: 8.21241283416748 = 1.6573342084884644 + 1.0 * 6.555078983306885
Epoch 160, val loss: 1.6875087022781372
Epoch 170, training loss: 8.163006782531738 = 1.6247926950454712 + 1.0 * 6.538214206695557
Epoch 170, val loss: 1.659725546836853
Epoch 180, training loss: 8.112942695617676 = 1.5871164798736572 + 1.0 * 6.5258259773254395
Epoch 180, val loss: 1.6275118589401245
Epoch 190, training loss: 8.055190086364746 = 1.544580101966858 + 1.0 * 6.510610103607178
Epoch 190, val loss: 1.5910974740982056
Epoch 200, training loss: 7.9983625411987305 = 1.4975603818893433 + 1.0 * 6.500802040100098
Epoch 200, val loss: 1.5509699583053589
Epoch 210, training loss: 7.936864376068115 = 1.4478259086608887 + 1.0 * 6.489038467407227
Epoch 210, val loss: 1.509016990661621
Epoch 220, training loss: 7.875491142272949 = 1.39676833152771 + 1.0 * 6.478723049163818
Epoch 220, val loss: 1.4659361839294434
Epoch 230, training loss: 7.823300361633301 = 1.3451564311981201 + 1.0 * 6.47814416885376
Epoch 230, val loss: 1.4226255416870117
Epoch 240, training loss: 7.758390426635742 = 1.294867753982544 + 1.0 * 6.463522434234619
Epoch 240, val loss: 1.380972146987915
Epoch 250, training loss: 7.7000861167907715 = 1.2456804513931274 + 1.0 * 6.454405784606934
Epoch 250, val loss: 1.3407255411148071
Epoch 260, training loss: 7.6508588790893555 = 1.197540044784546 + 1.0 * 6.453319072723389
Epoch 260, val loss: 1.3021068572998047
Epoch 270, training loss: 7.5937066078186035 = 1.1512179374694824 + 1.0 * 6.442488670349121
Epoch 270, val loss: 1.2657896280288696
Epoch 280, training loss: 7.541685581207275 = 1.106435775756836 + 1.0 * 6.4352498054504395
Epoch 280, val loss: 1.2313705682754517
Epoch 290, training loss: 7.49188232421875 = 1.0628430843353271 + 1.0 * 6.429039478302002
Epoch 290, val loss: 1.1986758708953857
Epoch 300, training loss: 7.450395584106445 = 1.0208525657653809 + 1.0 * 6.4295430183410645
Epoch 300, val loss: 1.1677738428115845
Epoch 310, training loss: 7.4013166427612305 = 0.981296956539154 + 1.0 * 6.420019626617432
Epoch 310, val loss: 1.139273762702942
Epoch 320, training loss: 7.358445644378662 = 0.9434514045715332 + 1.0 * 6.414994239807129
Epoch 320, val loss: 1.1125367879867554
Epoch 330, training loss: 7.3196282386779785 = 0.9072007536888123 + 1.0 * 6.4124274253845215
Epoch 330, val loss: 1.0872852802276611
Epoch 340, training loss: 7.279406547546387 = 0.8727843761444092 + 1.0 * 6.406621932983398
Epoch 340, val loss: 1.0637544393539429
Epoch 350, training loss: 7.243967056274414 = 0.8401543498039246 + 1.0 * 6.403812885284424
Epoch 350, val loss: 1.0418568849563599
Epoch 360, training loss: 7.206895351409912 = 0.80909663438797 + 1.0 * 6.397798538208008
Epoch 360, val loss: 1.0214754343032837
Epoch 370, training loss: 7.174798965454102 = 0.7793657183647156 + 1.0 * 6.39543342590332
Epoch 370, val loss: 1.0025607347488403
Epoch 380, training loss: 7.144840717315674 = 0.7508744597434998 + 1.0 * 6.393966197967529
Epoch 380, val loss: 0.9850356578826904
Epoch 390, training loss: 7.112790584564209 = 0.7236096262931824 + 1.0 * 6.389181137084961
Epoch 390, val loss: 0.9688050150871277
Epoch 400, training loss: 7.081562519073486 = 0.6973364949226379 + 1.0 * 6.384225845336914
Epoch 400, val loss: 0.9537737965583801
Epoch 410, training loss: 7.052264213562012 = 0.6716263294219971 + 1.0 * 6.3806376457214355
Epoch 410, val loss: 0.9396139979362488
Epoch 420, training loss: 7.029187202453613 = 0.6465333104133606 + 1.0 * 6.382653713226318
Epoch 420, val loss: 0.9261122345924377
Epoch 430, training loss: 6.998511791229248 = 0.6221619844436646 + 1.0 * 6.376349925994873
Epoch 430, val loss: 0.9134264588356018
Epoch 440, training loss: 6.971334457397461 = 0.5983021855354309 + 1.0 * 6.373032093048096
Epoch 440, val loss: 0.901290774345398
Epoch 450, training loss: 6.944753646850586 = 0.574809730052948 + 1.0 * 6.369944095611572
Epoch 450, val loss: 0.8895283937454224
Epoch 460, training loss: 6.924810409545898 = 0.5516953468322754 + 1.0 * 6.373115062713623
Epoch 460, val loss: 0.8781954050064087
Epoch 470, training loss: 6.899499416351318 = 0.5294293165206909 + 1.0 * 6.370069980621338
Epoch 470, val loss: 0.8674358129501343
Epoch 480, training loss: 6.871264457702637 = 0.5079947113990784 + 1.0 * 6.363269805908203
Epoch 480, val loss: 0.8575257658958435
Epoch 490, training loss: 6.848416805267334 = 0.4872545897960663 + 1.0 * 6.361162185668945
Epoch 490, val loss: 0.8481912612915039
Epoch 500, training loss: 6.82828426361084 = 0.46710115671157837 + 1.0 * 6.361183166503906
Epoch 500, val loss: 0.8395367860794067
Epoch 510, training loss: 6.8166632652282715 = 0.4477393329143524 + 1.0 * 6.368924140930176
Epoch 510, val loss: 0.8317800164222717
Epoch 520, training loss: 6.788440227508545 = 0.4292760193347931 + 1.0 * 6.359164237976074
Epoch 520, val loss: 0.824828565120697
Epoch 530, training loss: 6.7653608322143555 = 0.4114072620868683 + 1.0 * 6.3539533615112305
Epoch 530, val loss: 0.818672239780426
Epoch 540, training loss: 6.7458367347717285 = 0.39407017827033997 + 1.0 * 6.351766586303711
Epoch 540, val loss: 0.8131817579269409
Epoch 550, training loss: 6.733882427215576 = 0.37729373574256897 + 1.0 * 6.356588840484619
Epoch 550, val loss: 0.808478057384491
Epoch 560, training loss: 6.710526466369629 = 0.3612079322338104 + 1.0 * 6.349318504333496
Epoch 560, val loss: 0.8045402765274048
Epoch 570, training loss: 6.695353031158447 = 0.3456988036632538 + 1.0 * 6.349654197692871
Epoch 570, val loss: 0.8012441396713257
Epoch 580, training loss: 6.676257133483887 = 0.3307057321071625 + 1.0 * 6.345551490783691
Epoch 580, val loss: 0.7986420392990112
Epoch 590, training loss: 6.661027431488037 = 0.31617408990859985 + 1.0 * 6.344853401184082
Epoch 590, val loss: 0.7966201305389404
Epoch 600, training loss: 6.652024745941162 = 0.3021356761455536 + 1.0 * 6.349889278411865
Epoch 600, val loss: 0.7952243089675903
Epoch 610, training loss: 6.629630088806152 = 0.28870919346809387 + 1.0 * 6.340920925140381
Epoch 610, val loss: 0.7944722771644592
Epoch 620, training loss: 6.617184638977051 = 0.2757614552974701 + 1.0 * 6.341423034667969
Epoch 620, val loss: 0.7943110466003418
Epoch 630, training loss: 6.605375289916992 = 0.2632914185523987 + 1.0 * 6.342083930969238
Epoch 630, val loss: 0.7946502566337585
Epoch 640, training loss: 6.58868408203125 = 0.2513468265533447 + 1.0 * 6.337337493896484
Epoch 640, val loss: 0.7955654859542847
Epoch 650, training loss: 6.580650329589844 = 0.2398664951324463 + 1.0 * 6.340784072875977
Epoch 650, val loss: 0.7970019578933716
Epoch 660, training loss: 6.563668727874756 = 0.22887945175170898 + 1.0 * 6.334789276123047
Epoch 660, val loss: 0.7988429665565491
Epoch 670, training loss: 6.565464973449707 = 0.21842777729034424 + 1.0 * 6.347037315368652
Epoch 670, val loss: 0.8011896014213562
Epoch 680, training loss: 6.540384292602539 = 0.20850038528442383 + 1.0 * 6.331883907318115
Epoch 680, val loss: 0.803963303565979
Epoch 690, training loss: 6.530083656311035 = 0.19903472065925598 + 1.0 * 6.331048965454102
Epoch 690, val loss: 0.8071704506874084
Epoch 700, training loss: 6.518642425537109 = 0.1900007724761963 + 1.0 * 6.328641414642334
Epoch 700, val loss: 0.8107146620750427
Epoch 710, training loss: 6.5227508544921875 = 0.18141773343086243 + 1.0 * 6.341332912445068
Epoch 710, val loss: 0.8146033883094788
Epoch 720, training loss: 6.503963947296143 = 0.17331168055534363 + 1.0 * 6.330652236938477
Epoch 720, val loss: 0.8188515305519104
Epoch 730, training loss: 6.492665767669678 = 0.16565313935279846 + 1.0 * 6.327012538909912
Epoch 730, val loss: 0.8233557939529419
Epoch 740, training loss: 6.484033584594727 = 0.15836498141288757 + 1.0 * 6.325668811798096
Epoch 740, val loss: 0.8281092047691345
Epoch 750, training loss: 6.476241111755371 = 0.15141859650611877 + 1.0 * 6.324822425842285
Epoch 750, val loss: 0.8330998420715332
Epoch 760, training loss: 6.47296667098999 = 0.1448194831609726 + 1.0 * 6.3281474113464355
Epoch 760, val loss: 0.8382584452629089
Epoch 770, training loss: 6.461905002593994 = 0.13861201703548431 + 1.0 * 6.323293209075928
Epoch 770, val loss: 0.843628466129303
Epoch 780, training loss: 6.4536285400390625 = 0.1327023208141327 + 1.0 * 6.320926189422607
Epoch 780, val loss: 0.8491010665893555
Epoch 790, training loss: 6.454154014587402 = 0.12706032395362854 + 1.0 * 6.327093601226807
Epoch 790, val loss: 0.8546217083930969
Epoch 800, training loss: 6.445443630218506 = 0.12172701954841614 + 1.0 * 6.323716640472412
Epoch 800, val loss: 0.8603094220161438
Epoch 810, training loss: 6.435729503631592 = 0.11665822565555573 + 1.0 * 6.319071292877197
Epoch 810, val loss: 0.8660576343536377
Epoch 820, training loss: 6.433028221130371 = 0.11183499544858932 + 1.0 * 6.321193218231201
Epoch 820, val loss: 0.871813178062439
Epoch 830, training loss: 6.425692081451416 = 0.10724812000989914 + 1.0 * 6.318443775177002
Epoch 830, val loss: 0.877713680267334
Epoch 840, training loss: 6.4220967292785645 = 0.10288702696561813 + 1.0 * 6.319209575653076
Epoch 840, val loss: 0.8835647106170654
Epoch 850, training loss: 6.415311336517334 = 0.09873918443918228 + 1.0 * 6.316572189331055
Epoch 850, val loss: 0.8895449638366699
Epoch 860, training loss: 6.408234119415283 = 0.09477810561656952 + 1.0 * 6.313456058502197
Epoch 860, val loss: 0.8955399394035339
Epoch 870, training loss: 6.403578281402588 = 0.09098498523235321 + 1.0 * 6.312593460083008
Epoch 870, val loss: 0.9015804529190063
Epoch 880, training loss: 6.40854024887085 = 0.08736752718687057 + 1.0 * 6.321172714233398
Epoch 880, val loss: 0.907602071762085
Epoch 890, training loss: 6.399104595184326 = 0.08394170552492142 + 1.0 * 6.315162658691406
Epoch 890, val loss: 0.9137012958526611
Epoch 900, training loss: 6.389799118041992 = 0.08066603541374207 + 1.0 * 6.309133052825928
Epoch 900, val loss: 0.9197960495948792
Epoch 910, training loss: 6.386431694030762 = 0.07752864062786102 + 1.0 * 6.308903217315674
Epoch 910, val loss: 0.9258493781089783
Epoch 920, training loss: 6.398360729217529 = 0.07453303784132004 + 1.0 * 6.323827743530273
Epoch 920, val loss: 0.9319347143173218
Epoch 930, training loss: 6.385125160217285 = 0.07168221473693848 + 1.0 * 6.313442707061768
Epoch 930, val loss: 0.9379611015319824
Epoch 940, training loss: 6.374781608581543 = 0.06897220760583878 + 1.0 * 6.305809497833252
Epoch 940, val loss: 0.9440929293632507
Epoch 950, training loss: 6.374054431915283 = 0.0663754791021347 + 1.0 * 6.307679176330566
Epoch 950, val loss: 0.950103223323822
Epoch 960, training loss: 6.372469425201416 = 0.06389904022216797 + 1.0 * 6.308570384979248
Epoch 960, val loss: 0.9561244249343872
Epoch 970, training loss: 6.372574329376221 = 0.06154446303844452 + 1.0 * 6.31102991104126
Epoch 970, val loss: 0.9622076153755188
Epoch 980, training loss: 6.364328384399414 = 0.05929173529148102 + 1.0 * 6.305036544799805
Epoch 980, val loss: 0.9681708216667175
Epoch 990, training loss: 6.361251354217529 = 0.05714169889688492 + 1.0 * 6.304109573364258
Epoch 990, val loss: 0.9741709232330322
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8371112282551397
The final CL Acc:0.78765, 0.00972, The final GNN Acc:0.83975, 0.00240
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9566])
updated graph: torch.Size([2, 10618])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.531414985656738 = 1.9345664978027344 + 1.0 * 8.596848487854004
Epoch 0, val loss: 1.9282327890396118
Epoch 10, training loss: 10.522007942199707 = 1.9253594875335693 + 1.0 * 8.596648216247559
Epoch 10, val loss: 1.9197148084640503
Epoch 20, training loss: 10.508935928344727 = 1.9137673377990723 + 1.0 * 8.595169067382812
Epoch 20, val loss: 1.9084581136703491
Epoch 30, training loss: 10.48046588897705 = 1.897173285484314 + 1.0 * 8.583292961120605
Epoch 30, val loss: 1.8918577432632446
Epoch 40, training loss: 10.380361557006836 = 1.8744709491729736 + 1.0 * 8.505890846252441
Epoch 40, val loss: 1.869796633720398
Epoch 50, training loss: 9.988301277160645 = 1.8512916564941406 + 1.0 * 8.137009620666504
Epoch 50, val loss: 1.8482394218444824
Epoch 60, training loss: 9.44682502746582 = 1.8342561721801758 + 1.0 * 7.612568378448486
Epoch 60, val loss: 1.8328996896743774
Epoch 70, training loss: 8.994697570800781 = 1.8227607011795044 + 1.0 * 7.171936988830566
Epoch 70, val loss: 1.8215917348861694
Epoch 80, training loss: 8.814983367919922 = 1.8110605478286743 + 1.0 * 7.003922462463379
Epoch 80, val loss: 1.8099135160446167
Epoch 90, training loss: 8.685935974121094 = 1.796919822692871 + 1.0 * 6.889016628265381
Epoch 90, val loss: 1.7965728044509888
Epoch 100, training loss: 8.59552001953125 = 1.7827240228652954 + 1.0 * 6.812795639038086
Epoch 100, val loss: 1.783527135848999
Epoch 110, training loss: 8.526628494262695 = 1.7694194316864014 + 1.0 * 6.757209300994873
Epoch 110, val loss: 1.7713415622711182
Epoch 120, training loss: 8.467057228088379 = 1.7557820081710815 + 1.0 * 6.711275100708008
Epoch 120, val loss: 1.7589601278305054
Epoch 130, training loss: 8.414220809936523 = 1.7406959533691406 + 1.0 * 6.673524856567383
Epoch 130, val loss: 1.7455363273620605
Epoch 140, training loss: 8.371055603027344 = 1.723421335220337 + 1.0 * 6.647634506225586
Epoch 140, val loss: 1.7304859161376953
Epoch 150, training loss: 8.322142601013184 = 1.7037203311920166 + 1.0 * 6.618422031402588
Epoch 150, val loss: 1.7134544849395752
Epoch 160, training loss: 8.27678394317627 = 1.6810739040374756 + 1.0 * 6.595710277557373
Epoch 160, val loss: 1.6939433813095093
Epoch 170, training loss: 8.231554985046387 = 1.6548874378204346 + 1.0 * 6.576667308807373
Epoch 170, val loss: 1.6713776588439941
Epoch 180, training loss: 8.18407154083252 = 1.6244627237319946 + 1.0 * 6.5596089363098145
Epoch 180, val loss: 1.6451467275619507
Epoch 190, training loss: 8.137282371520996 = 1.5895432233810425 + 1.0 * 6.547739028930664
Epoch 190, val loss: 1.6154029369354248
Epoch 200, training loss: 8.084712028503418 = 1.5506198406219482 + 1.0 * 6.534092426300049
Epoch 200, val loss: 1.5821864604949951
Epoch 210, training loss: 8.030047416687012 = 1.5073217153549194 + 1.0 * 6.522726058959961
Epoch 210, val loss: 1.5454570055007935
Epoch 220, training loss: 7.975042343139648 = 1.4598654508590698 + 1.0 * 6.515176773071289
Epoch 220, val loss: 1.5056836605072021
Epoch 230, training loss: 7.9141645431518555 = 1.4092249870300293 + 1.0 * 6.504939556121826
Epoch 230, val loss: 1.4638844728469849
Epoch 240, training loss: 7.854169845581055 = 1.356109380722046 + 1.0 * 6.49806022644043
Epoch 240, val loss: 1.420658826828003
Epoch 250, training loss: 7.7924299240112305 = 1.301754355430603 + 1.0 * 6.490675449371338
Epoch 250, val loss: 1.3771743774414062
Epoch 260, training loss: 7.7312092781066895 = 1.2470279932022095 + 1.0 * 6.4841814041137695
Epoch 260, val loss: 1.3341352939605713
Epoch 270, training loss: 7.670294284820557 = 1.1921926736831665 + 1.0 * 6.47810173034668
Epoch 270, val loss: 1.2916862964630127
Epoch 280, training loss: 7.6121978759765625 = 1.1382087469100952 + 1.0 * 6.473989009857178
Epoch 280, val loss: 1.250550627708435
Epoch 290, training loss: 7.553925514221191 = 1.0857242345809937 + 1.0 * 6.468201160430908
Epoch 290, val loss: 1.2108832597732544
Epoch 300, training loss: 7.499879360198975 = 1.0346108675003052 + 1.0 * 6.465268611907959
Epoch 300, val loss: 1.1728923320770264
Epoch 310, training loss: 7.443203449249268 = 0.9854137301445007 + 1.0 * 6.457789897918701
Epoch 310, val loss: 1.136534333229065
Epoch 320, training loss: 7.3942999839782715 = 0.937671959400177 + 1.0 * 6.45662784576416
Epoch 320, val loss: 1.101913571357727
Epoch 330, training loss: 7.341403961181641 = 0.8918808698654175 + 1.0 * 6.449522972106934
Epoch 330, val loss: 1.0691500902175903
Epoch 340, training loss: 7.2926764488220215 = 0.8482121229171753 + 1.0 * 6.444464206695557
Epoch 340, val loss: 1.0383769273757935
Epoch 350, training loss: 7.246214389801025 = 0.806293785572052 + 1.0 * 6.439920425415039
Epoch 350, val loss: 1.009507656097412
Epoch 360, training loss: 7.2032647132873535 = 0.7659738659858704 + 1.0 * 6.437290668487549
Epoch 360, val loss: 0.9825097918510437
Epoch 370, training loss: 7.160916328430176 = 0.7275202870368958 + 1.0 * 6.433395862579346
Epoch 370, val loss: 0.9575910568237305
Epoch 380, training loss: 7.122825622558594 = 0.691091775894165 + 1.0 * 6.43173360824585
Epoch 380, val loss: 0.9349256753921509
Epoch 390, training loss: 7.083835601806641 = 0.6567100286483765 + 1.0 * 6.427125453948975
Epoch 390, val loss: 0.9145141243934631
Epoch 400, training loss: 7.048011302947998 = 0.6239539980888367 + 1.0 * 6.424057483673096
Epoch 400, val loss: 0.8962271213531494
Epoch 410, training loss: 7.015801429748535 = 0.5930337309837341 + 1.0 * 6.422767639160156
Epoch 410, val loss: 0.8799911141395569
Epoch 420, training loss: 6.982563018798828 = 0.5637651681900024 + 1.0 * 6.418797969818115
Epoch 420, val loss: 0.8657549619674683
Epoch 430, training loss: 6.9492621421813965 = 0.5359820127487183 + 1.0 * 6.413280010223389
Epoch 430, val loss: 0.8533472418785095
Epoch 440, training loss: 6.923836708068848 = 0.5095226168632507 + 1.0 * 6.414314270019531
Epoch 440, val loss: 0.8425677418708801
Epoch 450, training loss: 6.895203113555908 = 0.4845638573169708 + 1.0 * 6.41063928604126
Epoch 450, val loss: 0.8332269787788391
Epoch 460, training loss: 6.866489410400391 = 0.46093544363975525 + 1.0 * 6.405553817749023
Epoch 460, val loss: 0.8254515528678894
Epoch 470, training loss: 6.8401312828063965 = 0.4383460283279419 + 1.0 * 6.401785373687744
Epoch 470, val loss: 0.818880021572113
Epoch 480, training loss: 6.815125465393066 = 0.4166889190673828 + 1.0 * 6.398436546325684
Epoch 480, val loss: 0.8133112192153931
Epoch 490, training loss: 6.80069637298584 = 0.39589622616767883 + 1.0 * 6.404799938201904
Epoch 490, val loss: 0.8087650537490845
Epoch 500, training loss: 6.772210121154785 = 0.3761744797229767 + 1.0 * 6.396035671234131
Epoch 500, val loss: 0.8051305413246155
Epoch 510, training loss: 6.74935245513916 = 0.3574680685997009 + 1.0 * 6.3918843269348145
Epoch 510, val loss: 0.8024574518203735
Epoch 520, training loss: 6.728750228881836 = 0.3395956754684448 + 1.0 * 6.389154434204102
Epoch 520, val loss: 0.8005720973014832
Epoch 530, training loss: 6.710725784301758 = 0.32256168127059937 + 1.0 * 6.388164043426514
Epoch 530, val loss: 0.7993777990341187
Epoch 540, training loss: 6.696393966674805 = 0.30637383460998535 + 1.0 * 6.39001989364624
Epoch 540, val loss: 0.7988494038581848
Epoch 550, training loss: 6.682297706604004 = 0.2910180389881134 + 1.0 * 6.391279697418213
Epoch 550, val loss: 0.7989221215248108
Epoch 560, training loss: 6.658510208129883 = 0.27627456188201904 + 1.0 * 6.382235527038574
Epoch 560, val loss: 0.7995232343673706
Epoch 570, training loss: 6.64019250869751 = 0.2621157467365265 + 1.0 * 6.378076553344727
Epoch 570, val loss: 0.8005912899971008
Epoch 580, training loss: 6.6279096603393555 = 0.24836665391921997 + 1.0 * 6.379542827606201
Epoch 580, val loss: 0.8020668029785156
Epoch 590, training loss: 6.610943794250488 = 0.2350105345249176 + 1.0 * 6.3759331703186035
Epoch 590, val loss: 0.8038126230239868
Epoch 600, training loss: 6.601170539855957 = 0.22208987176418304 + 1.0 * 6.379080772399902
Epoch 600, val loss: 0.8060078024864197
Epoch 610, training loss: 6.580752372741699 = 0.20950068533420563 + 1.0 * 6.371251583099365
Epoch 610, val loss: 0.8085656762123108
Epoch 620, training loss: 6.567176342010498 = 0.19731935858726501 + 1.0 * 6.369856834411621
Epoch 620, val loss: 0.8114727735519409
Epoch 630, training loss: 6.554402828216553 = 0.18555791676044464 + 1.0 * 6.368844985961914
Epoch 630, val loss: 0.8147512078285217
Epoch 640, training loss: 6.5421953201293945 = 0.17432549595832825 + 1.0 * 6.367869853973389
Epoch 640, val loss: 0.8182976841926575
Epoch 650, training loss: 6.530156135559082 = 0.16368728876113892 + 1.0 * 6.366468906402588
Epoch 650, val loss: 0.8221661448478699
Epoch 660, training loss: 6.517256736755371 = 0.1536514312028885 + 1.0 * 6.363605499267578
Epoch 660, val loss: 0.8264608383178711
Epoch 670, training loss: 6.507457256317139 = 0.1441965103149414 + 1.0 * 6.363260746002197
Epoch 670, val loss: 0.8310769200325012
Epoch 680, training loss: 6.495343208312988 = 0.13535381853580475 + 1.0 * 6.359989166259766
Epoch 680, val loss: 0.8360010385513306
Epoch 690, training loss: 6.4878926277160645 = 0.12711268663406372 + 1.0 * 6.360779762268066
Epoch 690, val loss: 0.8413184285163879
Epoch 700, training loss: 6.477158546447754 = 0.11946559697389603 + 1.0 * 6.357692718505859
Epoch 700, val loss: 0.8469560742378235
Epoch 710, training loss: 6.468348979949951 = 0.1123722493648529 + 1.0 * 6.355976581573486
Epoch 710, val loss: 0.8529103398323059
Epoch 720, training loss: 6.4587883949279785 = 0.10577914118766785 + 1.0 * 6.353009223937988
Epoch 720, val loss: 0.8591540455818176
Epoch 730, training loss: 6.461192607879639 = 0.09965996444225311 + 1.0 * 6.361532688140869
Epoch 730, val loss: 0.8656688928604126
Epoch 740, training loss: 6.45122766494751 = 0.09395165741443634 + 1.0 * 6.35727596282959
Epoch 740, val loss: 0.8722257614135742
Epoch 750, training loss: 6.443118572235107 = 0.08872354030609131 + 1.0 * 6.354394912719727
Epoch 750, val loss: 0.8790699243545532
Epoch 760, training loss: 6.434013366699219 = 0.08387574553489685 + 1.0 * 6.350137710571289
Epoch 760, val loss: 0.8860349059104919
Epoch 770, training loss: 6.425804138183594 = 0.07935328036546707 + 1.0 * 6.3464508056640625
Epoch 770, val loss: 0.8931646943092346
Epoch 780, training loss: 6.427278995513916 = 0.07514534145593643 + 1.0 * 6.352133750915527
Epoch 780, val loss: 0.9003573060035706
Epoch 790, training loss: 6.418159484863281 = 0.07123364508152008 + 1.0 * 6.346925735473633
Epoch 790, val loss: 0.9075895547866821
Epoch 800, training loss: 6.412302017211914 = 0.06760755181312561 + 1.0 * 6.3446946144104
Epoch 800, val loss: 0.9149302840232849
Epoch 810, training loss: 6.408541202545166 = 0.06422365456819534 + 1.0 * 6.344317436218262
Epoch 810, val loss: 0.9223228096961975
Epoch 820, training loss: 6.4042487144470215 = 0.06106848642230034 + 1.0 * 6.343180179595947
Epoch 820, val loss: 0.9296665787696838
Epoch 830, training loss: 6.397796154022217 = 0.0581255704164505 + 1.0 * 6.339670658111572
Epoch 830, val loss: 0.9370530247688293
Epoch 840, training loss: 6.396100997924805 = 0.05536044016480446 + 1.0 * 6.34074068069458
Epoch 840, val loss: 0.9444770216941833
Epoch 850, training loss: 6.394314765930176 = 0.052786942571401596 + 1.0 * 6.341527938842773
Epoch 850, val loss: 0.9518246054649353
Epoch 860, training loss: 6.391806602478027 = 0.050372716039419174 + 1.0 * 6.341434001922607
Epoch 860, val loss: 0.9590630531311035
Epoch 870, training loss: 6.383140563964844 = 0.04812752828001976 + 1.0 * 6.335012912750244
Epoch 870, val loss: 0.9663913249969482
Epoch 880, training loss: 6.379753589630127 = 0.04600987210869789 + 1.0 * 6.333743572235107
Epoch 880, val loss: 0.9736730456352234
Epoch 890, training loss: 6.389688968658447 = 0.04402242600917816 + 1.0 * 6.345666408538818
Epoch 890, val loss: 0.9808826446533203
Epoch 900, training loss: 6.380194187164307 = 0.04214649647474289 + 1.0 * 6.338047504425049
Epoch 900, val loss: 0.9878812432289124
Epoch 910, training loss: 6.37259578704834 = 0.040406160056591034 + 1.0 * 6.332189559936523
Epoch 910, val loss: 0.9950067400932312
Epoch 920, training loss: 6.368498802185059 = 0.03875109925866127 + 1.0 * 6.329747676849365
Epoch 920, val loss: 1.002074956893921
Epoch 930, training loss: 6.373492240905762 = 0.0371914766728878 + 1.0 * 6.336300849914551
Epoch 930, val loss: 1.0090149641036987
Epoch 940, training loss: 6.369754791259766 = 0.03572568669915199 + 1.0 * 6.334029197692871
Epoch 940, val loss: 1.0158088207244873
Epoch 950, training loss: 6.362842082977295 = 0.03434089571237564 + 1.0 * 6.328501224517822
Epoch 950, val loss: 1.022688627243042
Epoch 960, training loss: 6.360076427459717 = 0.03303929790854454 + 1.0 * 6.327037334442139
Epoch 960, val loss: 1.0294537544250488
Epoch 970, training loss: 6.359671592712402 = 0.031800854951143265 + 1.0 * 6.327870845794678
Epoch 970, val loss: 1.0361686944961548
Epoch 980, training loss: 6.355227470397949 = 0.030628377571702003 + 1.0 * 6.324599266052246
Epoch 980, val loss: 1.0427206754684448
Epoch 990, training loss: 6.360931873321533 = 0.029520850628614426 + 1.0 * 6.331410884857178
Epoch 990, val loss: 1.0492280721664429
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.797575118608329
=== training gcn model ===
Epoch 0, training loss: 10.547876358032227 = 1.951045036315918 + 1.0 * 8.596831321716309
Epoch 0, val loss: 1.9428654909133911
Epoch 10, training loss: 10.537744522094727 = 1.9412007331848145 + 1.0 * 8.59654426574707
Epoch 10, val loss: 1.9338476657867432
Epoch 20, training loss: 10.523466110229492 = 1.9290761947631836 + 1.0 * 8.594389915466309
Epoch 20, val loss: 1.922613263130188
Epoch 30, training loss: 10.489931106567383 = 1.9123120307922363 + 1.0 * 8.577619552612305
Epoch 30, val loss: 1.9070472717285156
Epoch 40, training loss: 10.35504150390625 = 1.8910666704177856 + 1.0 * 8.463974952697754
Epoch 40, val loss: 1.8877429962158203
Epoch 50, training loss: 9.861306190490723 = 1.8697868585586548 + 1.0 * 7.991519451141357
Epoch 50, val loss: 1.8682916164398193
Epoch 60, training loss: 9.274659156799316 = 1.8539659976959229 + 1.0 * 7.420693397521973
Epoch 60, val loss: 1.853771686553955
Epoch 70, training loss: 8.973209381103516 = 1.839670181274414 + 1.0 * 7.133539199829102
Epoch 70, val loss: 1.84060800075531
Epoch 80, training loss: 8.850692749023438 = 1.8231126070022583 + 1.0 * 7.027580261230469
Epoch 80, val loss: 1.8258743286132812
Epoch 90, training loss: 8.723857879638672 = 1.8054434061050415 + 1.0 * 6.918414115905762
Epoch 90, val loss: 1.8110650777816772
Epoch 100, training loss: 8.627365112304688 = 1.7889882326126099 + 1.0 * 6.838376522064209
Epoch 100, val loss: 1.7975709438323975
Epoch 110, training loss: 8.554070472717285 = 1.7734096050262451 + 1.0 * 6.780661106109619
Epoch 110, val loss: 1.7845710515975952
Epoch 120, training loss: 8.493109703063965 = 1.7573068141937256 + 1.0 * 6.735803127288818
Epoch 120, val loss: 1.771080732345581
Epoch 130, training loss: 8.437024116516113 = 1.7398792505264282 + 1.0 * 6.697144985198975
Epoch 130, val loss: 1.7565622329711914
Epoch 140, training loss: 8.383529663085938 = 1.7204186916351318 + 1.0 * 6.663111209869385
Epoch 140, val loss: 1.740566611289978
Epoch 150, training loss: 8.33281135559082 = 1.6983001232147217 + 1.0 * 6.634511470794678
Epoch 150, val loss: 1.722798228263855
Epoch 160, training loss: 8.281527519226074 = 1.6728641986846924 + 1.0 * 6.608663558959961
Epoch 160, val loss: 1.7024503946304321
Epoch 170, training loss: 8.230737686157227 = 1.643355131149292 + 1.0 * 6.587382793426514
Epoch 170, val loss: 1.679024577140808
Epoch 180, training loss: 8.179837226867676 = 1.609641194343567 + 1.0 * 6.570196151733398
Epoch 180, val loss: 1.652389645576477
Epoch 190, training loss: 8.126625061035156 = 1.5719246864318848 + 1.0 * 6.5547003746032715
Epoch 190, val loss: 1.6226356029510498
Epoch 200, training loss: 8.071383476257324 = 1.5300945043563843 + 1.0 * 6.541289329528809
Epoch 200, val loss: 1.5896097421646118
Epoch 210, training loss: 8.014123916625977 = 1.4844088554382324 + 1.0 * 6.529715061187744
Epoch 210, val loss: 1.553645133972168
Epoch 220, training loss: 7.959850788116455 = 1.4363211393356323 + 1.0 * 6.523529529571533
Epoch 220, val loss: 1.5157582759857178
Epoch 230, training loss: 7.898343086242676 = 1.3876171112060547 + 1.0 * 6.510725975036621
Epoch 230, val loss: 1.4774974584579468
Epoch 240, training loss: 7.841957092285156 = 1.3384696245193481 + 1.0 * 6.503487586975098
Epoch 240, val loss: 1.4390376806259155
Epoch 250, training loss: 7.786005020141602 = 1.2895466089248657 + 1.0 * 6.496458530426025
Epoch 250, val loss: 1.4010694026947021
Epoch 260, training loss: 7.735175132751465 = 1.2416703701019287 + 1.0 * 6.493504524230957
Epoch 260, val loss: 1.3642034530639648
Epoch 270, training loss: 7.6788835525512695 = 1.1953437328338623 + 1.0 * 6.483539581298828
Epoch 270, val loss: 1.3288452625274658
Epoch 280, training loss: 7.626182556152344 = 1.1496837139129639 + 1.0 * 6.476499080657959
Epoch 280, val loss: 1.2943545579910278
Epoch 290, training loss: 7.584280014038086 = 1.104504942893982 + 1.0 * 6.4797749519348145
Epoch 290, val loss: 1.2605254650115967
Epoch 300, training loss: 7.528395652770996 = 1.060577630996704 + 1.0 * 6.467818260192871
Epoch 300, val loss: 1.22784423828125
Epoch 310, training loss: 7.478708267211914 = 1.017334222793579 + 1.0 * 6.461374282836914
Epoch 310, val loss: 1.196014404296875
Epoch 320, training loss: 7.430147647857666 = 0.9744995832443237 + 1.0 * 6.455647945404053
Epoch 320, val loss: 1.16472589969635
Epoch 330, training loss: 7.38591194152832 = 0.9322157502174377 + 1.0 * 6.453696250915527
Epoch 330, val loss: 1.1340270042419434
Epoch 340, training loss: 7.337653636932373 = 0.891167163848877 + 1.0 * 6.446486473083496
Epoch 340, val loss: 1.104487657546997
Epoch 350, training loss: 7.295665740966797 = 0.8512868881225586 + 1.0 * 6.444378852844238
Epoch 350, val loss: 1.0758013725280762
Epoch 360, training loss: 7.253353595733643 = 0.8126068115234375 + 1.0 * 6.440746784210205
Epoch 360, val loss: 1.0480680465698242
Epoch 370, training loss: 7.211172103881836 = 0.7754910588264465 + 1.0 * 6.435680866241455
Epoch 370, val loss: 1.021935224533081
Epoch 380, training loss: 7.173149585723877 = 0.7398607134819031 + 1.0 * 6.433289051055908
Epoch 380, val loss: 0.997031569480896
Epoch 390, training loss: 7.132444858551025 = 0.705927312374115 + 1.0 * 6.426517486572266
Epoch 390, val loss: 0.973823606967926
Epoch 400, training loss: 7.10111141204834 = 0.6733883619308472 + 1.0 * 6.427722930908203
Epoch 400, val loss: 0.9519866704940796
Epoch 410, training loss: 7.066493988037109 = 0.6424922943115234 + 1.0 * 6.424001693725586
Epoch 410, val loss: 0.9320014119148254
Epoch 420, training loss: 7.030407905578613 = 0.6130322813987732 + 1.0 * 6.417375564575195
Epoch 420, val loss: 0.9136978387832642
Epoch 430, training loss: 6.997300624847412 = 0.5847298502922058 + 1.0 * 6.412570953369141
Epoch 430, val loss: 0.8968439698219299
Epoch 440, training loss: 6.980872631072998 = 0.5574168562889099 + 1.0 * 6.423455715179443
Epoch 440, val loss: 0.8813171982765198
Epoch 450, training loss: 6.939713478088379 = 0.5316058993339539 + 1.0 * 6.408107757568359
Epoch 450, val loss: 0.867523193359375
Epoch 460, training loss: 6.911604404449463 = 0.5068742036819458 + 1.0 * 6.404730319976807
Epoch 460, val loss: 0.8550845384597778
Epoch 470, training loss: 6.884532928466797 = 0.48309874534606934 + 1.0 * 6.401433944702148
Epoch 470, val loss: 0.8437471389770508
Epoch 480, training loss: 6.863166809082031 = 0.4602506160736084 + 1.0 * 6.402916431427002
Epoch 480, val loss: 0.8334223031997681
Epoch 490, training loss: 6.835237979888916 = 0.4385061264038086 + 1.0 * 6.396731853485107
Epoch 490, val loss: 0.8241873979568481
Epoch 500, training loss: 6.810609340667725 = 0.4176715612411499 + 1.0 * 6.392937660217285
Epoch 500, val loss: 0.8158367872238159
Epoch 510, training loss: 6.792771339416504 = 0.3976314067840576 + 1.0 * 6.395139694213867
Epoch 510, val loss: 0.8082387447357178
Epoch 520, training loss: 6.768853664398193 = 0.3784365952014923 + 1.0 * 6.390417098999023
Epoch 520, val loss: 0.8013994693756104
Epoch 530, training loss: 6.751221656799316 = 0.36000147461891174 + 1.0 * 6.3912200927734375
Epoch 530, val loss: 0.7952775955200195
Epoch 540, training loss: 6.726435661315918 = 0.34242162108421326 + 1.0 * 6.384014129638672
Epoch 540, val loss: 0.789923906326294
Epoch 550, training loss: 6.708311080932617 = 0.32552123069763184 + 1.0 * 6.3827900886535645
Epoch 550, val loss: 0.7851533889770508
Epoch 560, training loss: 6.693027973175049 = 0.3093143403530121 + 1.0 * 6.383713722229004
Epoch 560, val loss: 0.7808451056480408
Epoch 570, training loss: 6.673479080200195 = 0.2938676178455353 + 1.0 * 6.379611492156982
Epoch 570, val loss: 0.7772877812385559
Epoch 580, training loss: 6.655026912689209 = 0.27907946705818176 + 1.0 * 6.37594747543335
Epoch 580, val loss: 0.7741703987121582
Epoch 590, training loss: 6.6403608322143555 = 0.2649036645889282 + 1.0 * 6.375457286834717
Epoch 590, val loss: 0.7714829444885254
Epoch 600, training loss: 6.6293134689331055 = 0.25132548809051514 + 1.0 * 6.377987861633301
Epoch 600, val loss: 0.7691729664802551
Epoch 610, training loss: 6.616861343383789 = 0.23852787911891937 + 1.0 * 6.378333568572998
Epoch 610, val loss: 0.7673696875572205
Epoch 620, training loss: 6.597064018249512 = 0.22631226480007172 + 1.0 * 6.370751857757568
Epoch 620, val loss: 0.7659575939178467
Epoch 630, training loss: 6.581882953643799 = 0.21467842161655426 + 1.0 * 6.367204666137695
Epoch 630, val loss: 0.7649602890014648
Epoch 640, training loss: 6.568330764770508 = 0.20353804528713226 + 1.0 * 6.364792823791504
Epoch 640, val loss: 0.7643595337867737
Epoch 650, training loss: 6.5565690994262695 = 0.19288554787635803 + 1.0 * 6.363683700561523
Epoch 650, val loss: 0.7641159892082214
Epoch 660, training loss: 6.546527862548828 = 0.1827203631401062 + 1.0 * 6.363807678222656
Epoch 660, val loss: 0.7640725374221802
Epoch 670, training loss: 6.5412092208862305 = 0.1730927973985672 + 1.0 * 6.36811637878418
Epoch 670, val loss: 0.7643414735794067
Epoch 680, training loss: 6.524799823760986 = 0.16400761902332306 + 1.0 * 6.36079216003418
Epoch 680, val loss: 0.7649601101875305
Epoch 690, training loss: 6.5129289627075195 = 0.1553729623556137 + 1.0 * 6.357555866241455
Epoch 690, val loss: 0.7658323049545288
Epoch 700, training loss: 6.505234718322754 = 0.14718247950077057 + 1.0 * 6.3580522537231445
Epoch 700, val loss: 0.7669793963432312
Epoch 710, training loss: 6.500249862670898 = 0.13943535089492798 + 1.0 * 6.360814571380615
Epoch 710, val loss: 0.7683221697807312
Epoch 720, training loss: 6.491888523101807 = 0.1321096569299698 + 1.0 * 6.359778881072998
Epoch 720, val loss: 0.7698884606361389
Epoch 730, training loss: 6.480961799621582 = 0.12522976100444794 + 1.0 * 6.355731964111328
Epoch 730, val loss: 0.7717235684394836
Epoch 740, training loss: 6.469913959503174 = 0.11871971189975739 + 1.0 * 6.351194381713867
Epoch 740, val loss: 0.773727536201477
Epoch 750, training loss: 6.462751865386963 = 0.1125749871134758 + 1.0 * 6.350176811218262
Epoch 750, val loss: 0.7759749889373779
Epoch 760, training loss: 6.466550350189209 = 0.10675949603319168 + 1.0 * 6.359790802001953
Epoch 760, val loss: 0.7783100008964539
Epoch 770, training loss: 6.452467918395996 = 0.101347416639328 + 1.0 * 6.351120471954346
Epoch 770, val loss: 0.7809591889381409
Epoch 780, training loss: 6.442800998687744 = 0.09622885286808014 + 1.0 * 6.346571922302246
Epoch 780, val loss: 0.7836982607841492
Epoch 790, training loss: 6.435336112976074 = 0.09141704440116882 + 1.0 * 6.343919277191162
Epoch 790, val loss: 0.7866795659065247
Epoch 800, training loss: 6.431725025177002 = 0.08687019348144531 + 1.0 * 6.344854831695557
Epoch 800, val loss: 0.78981614112854
Epoch 810, training loss: 6.425781726837158 = 0.08260003477334976 + 1.0 * 6.343181610107422
Epoch 810, val loss: 0.7929818630218506
Epoch 820, training loss: 6.424495697021484 = 0.07859279215335846 + 1.0 * 6.345902919769287
Epoch 820, val loss: 0.7962515354156494
Epoch 830, training loss: 6.414953231811523 = 0.07484070956707001 + 1.0 * 6.340112686157227
Epoch 830, val loss: 0.7997017502784729
Epoch 840, training loss: 6.41190767288208 = 0.07130397111177444 + 1.0 * 6.340603828430176
Epoch 840, val loss: 0.8033031225204468
Epoch 850, training loss: 6.409997940063477 = 0.06797147542238235 + 1.0 * 6.342026233673096
Epoch 850, val loss: 0.8068867921829224
Epoch 860, training loss: 6.4045209884643555 = 0.0648266077041626 + 1.0 * 6.339694499969482
Epoch 860, val loss: 0.8105159997940063
Epoch 870, training loss: 6.396783828735352 = 0.06188574060797691 + 1.0 * 6.334897994995117
Epoch 870, val loss: 0.8142535090446472
Epoch 880, training loss: 6.394082069396973 = 0.05911087244749069 + 1.0 * 6.3349714279174805
Epoch 880, val loss: 0.8181665539741516
Epoch 890, training loss: 6.395177841186523 = 0.05648614093661308 + 1.0 * 6.338691711425781
Epoch 890, val loss: 0.8220351338386536
Epoch 900, training loss: 6.393730640411377 = 0.05401689186692238 + 1.0 * 6.3397135734558105
Epoch 900, val loss: 0.8259384632110596
Epoch 910, training loss: 6.385497093200684 = 0.051701273769140244 + 1.0 * 6.33379602432251
Epoch 910, val loss: 0.829886257648468
Epoch 920, training loss: 6.381667613983154 = 0.049509309232234955 + 1.0 * 6.332158088684082
Epoch 920, val loss: 0.833938479423523
Epoch 930, training loss: 6.384721755981445 = 0.04743644967675209 + 1.0 * 6.33728551864624
Epoch 930, val loss: 0.8378743529319763
Epoch 940, training loss: 6.3756818771362305 = 0.04549359157681465 + 1.0 * 6.330188274383545
Epoch 940, val loss: 0.8420211672782898
Epoch 950, training loss: 6.371269226074219 = 0.04364929348230362 + 1.0 * 6.327620029449463
Epoch 950, val loss: 0.846119225025177
Epoch 960, training loss: 6.374981880187988 = 0.04190050810575485 + 1.0 * 6.333081245422363
Epoch 960, val loss: 0.8501959443092346
Epoch 970, training loss: 6.371285915374756 = 0.04025241732597351 + 1.0 * 6.331033706665039
Epoch 970, val loss: 0.8542575836181641
Epoch 980, training loss: 6.365041732788086 = 0.03869662806391716 + 1.0 * 6.326344966888428
Epoch 980, val loss: 0.8583250641822815
Epoch 990, training loss: 6.361584663391113 = 0.03722453862428665 + 1.0 * 6.324359893798828
Epoch 990, val loss: 0.8624570965766907
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.801265155508698
=== training gcn model ===
Epoch 0, training loss: 10.553213119506836 = 1.9563698768615723 + 1.0 * 8.596842765808105
Epoch 0, val loss: 1.9527393579483032
Epoch 10, training loss: 10.542865753173828 = 1.9462796449661255 + 1.0 * 8.596586227416992
Epoch 10, val loss: 1.9424304962158203
Epoch 20, training loss: 10.528626441955566 = 1.933943510055542 + 1.0 * 8.594682693481445
Epoch 20, val loss: 1.9299372434616089
Epoch 30, training loss: 10.496454238891602 = 1.9171264171600342 + 1.0 * 8.579327583312988
Epoch 30, val loss: 1.9129283428192139
Epoch 40, training loss: 10.378271102905273 = 1.895195722579956 + 1.0 * 8.483075141906738
Epoch 40, val loss: 1.8916929960250854
Epoch 50, training loss: 10.055909156799316 = 1.8732613325119019 + 1.0 * 8.182647705078125
Epoch 50, val loss: 1.871803641319275
Epoch 60, training loss: 9.499662399291992 = 1.8569526672363281 + 1.0 * 7.642709255218506
Epoch 60, val loss: 1.858027458190918
Epoch 70, training loss: 8.940481185913086 = 1.844828486442566 + 1.0 * 7.095653057098389
Epoch 70, val loss: 1.8471232652664185
Epoch 80, training loss: 8.763193130493164 = 1.8317670822143555 + 1.0 * 6.931426048278809
Epoch 80, val loss: 1.8349878787994385
Epoch 90, training loss: 8.647838592529297 = 1.8150954246520996 + 1.0 * 6.832743167877197
Epoch 90, val loss: 1.8203166723251343
Epoch 100, training loss: 8.55308723449707 = 1.7979124784469604 + 1.0 * 6.75517463684082
Epoch 100, val loss: 1.8058134317398071
Epoch 110, training loss: 8.485326766967773 = 1.7822016477584839 + 1.0 * 6.703125
Epoch 110, val loss: 1.7925326824188232
Epoch 120, training loss: 8.427473068237305 = 1.767077088356018 + 1.0 * 6.660395622253418
Epoch 120, val loss: 1.7794591188430786
Epoch 130, training loss: 8.377748489379883 = 1.7512614727020264 + 1.0 * 6.6264872550964355
Epoch 130, val loss: 1.7658461332321167
Epoch 140, training loss: 8.333418846130371 = 1.7342026233673096 + 1.0 * 6.599215984344482
Epoch 140, val loss: 1.751276969909668
Epoch 150, training loss: 8.29396915435791 = 1.7152870893478394 + 1.0 * 6.5786824226379395
Epoch 150, val loss: 1.735361933708191
Epoch 160, training loss: 8.25314998626709 = 1.6940354108810425 + 1.0 * 6.559114456176758
Epoch 160, val loss: 1.7177658081054688
Epoch 170, training loss: 8.212153434753418 = 1.6697534322738647 + 1.0 * 6.542400360107422
Epoch 170, val loss: 1.6978510618209839
Epoch 180, training loss: 8.171611785888672 = 1.6419477462768555 + 1.0 * 6.529664516448975
Epoch 180, val loss: 1.675228238105774
Epoch 190, training loss: 8.12746524810791 = 1.6105793714523315 + 1.0 * 6.516885757446289
Epoch 190, val loss: 1.6499606370925903
Epoch 200, training loss: 8.08131217956543 = 1.5756778717041016 + 1.0 * 6.505634307861328
Epoch 200, val loss: 1.622173547744751
Epoch 210, training loss: 8.036369323730469 = 1.537104845046997 + 1.0 * 6.499264717102051
Epoch 210, val loss: 1.5917127132415771
Epoch 220, training loss: 7.98309850692749 = 1.4957780838012695 + 1.0 * 6.487320423126221
Epoch 220, val loss: 1.5593503713607788
Epoch 230, training loss: 7.930540084838867 = 1.4516149759292603 + 1.0 * 6.4789252281188965
Epoch 230, val loss: 1.5251598358154297
Epoch 240, training loss: 7.876567363739014 = 1.4047999382019043 + 1.0 * 6.471767425537109
Epoch 240, val loss: 1.4892308712005615
Epoch 250, training loss: 7.825536251068115 = 1.3561677932739258 + 1.0 * 6.4693684577941895
Epoch 250, val loss: 1.45218825340271
Epoch 260, training loss: 7.7666239738464355 = 1.3065996170043945 + 1.0 * 6.460024356842041
Epoch 260, val loss: 1.414634346961975
Epoch 270, training loss: 7.709758758544922 = 1.255617380142212 + 1.0 * 6.454141139984131
Epoch 270, val loss: 1.3762367963790894
Epoch 280, training loss: 7.662094593048096 = 1.203731656074524 + 1.0 * 6.458363056182861
Epoch 280, val loss: 1.3372031450271606
Epoch 290, training loss: 7.599148750305176 = 1.152363896369934 + 1.0 * 6.446784973144531
Epoch 290, val loss: 1.2983431816101074
Epoch 300, training loss: 7.54111385345459 = 1.1009770631790161 + 1.0 * 6.440136909484863
Epoch 300, val loss: 1.259660243988037
Epoch 310, training loss: 7.485677242279053 = 1.0497379302978516 + 1.0 * 6.435939311981201
Epoch 310, val loss: 1.2211629152297974
Epoch 320, training loss: 7.440643787384033 = 0.9991708993911743 + 1.0 * 6.441473007202148
Epoch 320, val loss: 1.183188796043396
Epoch 330, training loss: 7.380753040313721 = 0.9499862790107727 + 1.0 * 6.430766582489014
Epoch 330, val loss: 1.1465201377868652
Epoch 340, training loss: 7.3276686668396 = 0.9023145437240601 + 1.0 * 6.42535400390625
Epoch 340, val loss: 1.1111493110656738
Epoch 350, training loss: 7.279036998748779 = 0.8562605977058411 + 1.0 * 6.422776222229004
Epoch 350, val loss: 1.077060580253601
Epoch 360, training loss: 7.239586353302002 = 0.812504231929779 + 1.0 * 6.427082061767578
Epoch 360, val loss: 1.0451140403747559
Epoch 370, training loss: 7.190128326416016 = 0.7719521522521973 + 1.0 * 6.418176174163818
Epoch 370, val loss: 1.0154516696929932
Epoch 380, training loss: 7.145402908325195 = 0.7334537506103516 + 1.0 * 6.411949157714844
Epoch 380, val loss: 0.9880364537239075
Epoch 390, training loss: 7.10513973236084 = 0.6967915296554565 + 1.0 * 6.408348083496094
Epoch 390, val loss: 0.9621390104293823
Epoch 400, training loss: 7.067037582397461 = 0.6616595983505249 + 1.0 * 6.4053778648376465
Epoch 400, val loss: 0.9377524852752686
Epoch 410, training loss: 7.033150672912598 = 0.6281177997589111 + 1.0 * 6.405032634735107
Epoch 410, val loss: 0.9148041009902954
Epoch 420, training loss: 7.0004682540893555 = 0.5964093208312988 + 1.0 * 6.404058933258057
Epoch 420, val loss: 0.8937200307846069
Epoch 430, training loss: 6.963857650756836 = 0.565857470035553 + 1.0 * 6.398000240325928
Epoch 430, val loss: 0.8741094470024109
Epoch 440, training loss: 6.93039608001709 = 0.5362922549247742 + 1.0 * 6.39410400390625
Epoch 440, val loss: 0.8555395007133484
Epoch 450, training loss: 6.904962539672852 = 0.5077044367790222 + 1.0 * 6.397258281707764
Epoch 450, val loss: 0.8381460309028625
Epoch 460, training loss: 6.874236583709717 = 0.48032721877098083 + 1.0 * 6.393909454345703
Epoch 460, val loss: 0.8219513297080994
Epoch 470, training loss: 6.843218803405762 = 0.4543170630931854 + 1.0 * 6.388901710510254
Epoch 470, val loss: 0.8072988986968994
Epoch 480, training loss: 6.814985275268555 = 0.42956069111824036 + 1.0 * 6.385424613952637
Epoch 480, val loss: 0.7940003871917725
Epoch 490, training loss: 6.792505264282227 = 0.40627428889274597 + 1.0 * 6.386230945587158
Epoch 490, val loss: 0.7819331288337708
Epoch 500, training loss: 6.766038417816162 = 0.38456490635871887 + 1.0 * 6.381473541259766
Epoch 500, val loss: 0.7715361714363098
Epoch 510, training loss: 6.742415428161621 = 0.3643445670604706 + 1.0 * 6.378070831298828
Epoch 510, val loss: 0.7625346779823303
Epoch 520, training loss: 6.740067481994629 = 0.3455997407436371 + 1.0 * 6.394467830657959
Epoch 520, val loss: 0.7547126412391663
Epoch 530, training loss: 6.704852104187012 = 0.32827842235565186 + 1.0 * 6.37657356262207
Epoch 530, val loss: 0.7482240796089172
Epoch 540, training loss: 6.685333251953125 = 0.31232380867004395 + 1.0 * 6.373009204864502
Epoch 540, val loss: 0.7429352402687073
Epoch 550, training loss: 6.673242568969727 = 0.2975402772426605 + 1.0 * 6.375702381134033
Epoch 550, val loss: 0.7385544776916504
Epoch 560, training loss: 6.652741432189941 = 0.2838776111602783 + 1.0 * 6.368864059448242
Epoch 560, val loss: 0.7349764704704285
Epoch 570, training loss: 6.6365251541137695 = 0.2710774838924408 + 1.0 * 6.365447521209717
Epoch 570, val loss: 0.7322099208831787
Epoch 580, training loss: 6.629963397979736 = 0.2590559720993042 + 1.0 * 6.370907306671143
Epoch 580, val loss: 0.730100154876709
Epoch 590, training loss: 6.612396240234375 = 0.24778203666210175 + 1.0 * 6.364614009857178
Epoch 590, val loss: 0.7283982038497925
Epoch 600, training loss: 6.599534034729004 = 0.23708724975585938 + 1.0 * 6.3624467849731445
Epoch 600, val loss: 0.7273387312889099
Epoch 610, training loss: 6.585503101348877 = 0.2268875241279602 + 1.0 * 6.358615398406982
Epoch 610, val loss: 0.7265102863311768
Epoch 620, training loss: 6.573474884033203 = 0.21711355447769165 + 1.0 * 6.356361389160156
Epoch 620, val loss: 0.7260457277297974
Epoch 630, training loss: 6.562516689300537 = 0.20767706632614136 + 1.0 * 6.35483980178833
Epoch 630, val loss: 0.7258004546165466
Epoch 640, training loss: 6.551074981689453 = 0.19850598275661469 + 1.0 * 6.352569103240967
Epoch 640, val loss: 0.7257146239280701
Epoch 650, training loss: 6.5554094314575195 = 0.1895868480205536 + 1.0 * 6.365822792053223
Epoch 650, val loss: 0.7257270812988281
Epoch 660, training loss: 6.531846046447754 = 0.18087545037269592 + 1.0 * 6.35097074508667
Epoch 660, val loss: 0.7258962988853455
Epoch 670, training loss: 6.5243449211120605 = 0.17241504788398743 + 1.0 * 6.351929664611816
Epoch 670, val loss: 0.7262378931045532
Epoch 680, training loss: 6.515718936920166 = 0.16418993473052979 + 1.0 * 6.351529121398926
Epoch 680, val loss: 0.7266157865524292
Epoch 690, training loss: 6.501233100891113 = 0.15616880357265472 + 1.0 * 6.345064163208008
Epoch 690, val loss: 0.7272107601165771
Epoch 700, training loss: 6.496951103210449 = 0.14835874736309052 + 1.0 * 6.348592281341553
Epoch 700, val loss: 0.7279132604598999
Epoch 710, training loss: 6.4863176345825195 = 0.1408214569091797 + 1.0 * 6.34549617767334
Epoch 710, val loss: 0.7286690473556519
Epoch 720, training loss: 6.477184295654297 = 0.13356652855873108 + 1.0 * 6.343617916107178
Epoch 720, val loss: 0.7297342419624329
Epoch 730, training loss: 6.46591329574585 = 0.1265971064567566 + 1.0 * 6.339316368103027
Epoch 730, val loss: 0.7309441566467285
Epoch 740, training loss: 6.461949825286865 = 0.119908906519413 + 1.0 * 6.342041015625
Epoch 740, val loss: 0.7322400808334351
Epoch 750, training loss: 6.455721378326416 = 0.11351726949214935 + 1.0 * 6.3422040939331055
Epoch 750, val loss: 0.7336612343788147
Epoch 760, training loss: 6.444840908050537 = 0.10748749226331711 + 1.0 * 6.337353229522705
Epoch 760, val loss: 0.7353878617286682
Epoch 770, training loss: 6.437103271484375 = 0.10174434632062912 + 1.0 * 6.3353590965271
Epoch 770, val loss: 0.7373106479644775
Epoch 780, training loss: 6.42993688583374 = 0.09629242867231369 + 1.0 * 6.333644390106201
Epoch 780, val loss: 0.7393727898597717
Epoch 790, training loss: 6.434952735900879 = 0.09113771468400955 + 1.0 * 6.343814849853516
Epoch 790, val loss: 0.741558849811554
Epoch 800, training loss: 6.418567180633545 = 0.08631107211112976 + 1.0 * 6.332256317138672
Epoch 800, val loss: 0.7439665198326111
Epoch 810, training loss: 6.418464183807373 = 0.08175501227378845 + 1.0 * 6.336709022521973
Epoch 810, val loss: 0.7465925812721252
Epoch 820, training loss: 6.407782077789307 = 0.07747544348239899 + 1.0 * 6.330306529998779
Epoch 820, val loss: 0.7493404746055603
Epoch 830, training loss: 6.401308536529541 = 0.07345569878816605 + 1.0 * 6.327852725982666
Epoch 830, val loss: 0.7522725462913513
Epoch 840, training loss: 6.401025772094727 = 0.0696697011590004 + 1.0 * 6.331356048583984
Epoch 840, val loss: 0.755338191986084
Epoch 850, training loss: 6.394134998321533 = 0.06614340096712112 + 1.0 * 6.327991485595703
Epoch 850, val loss: 0.7584100961685181
Epoch 860, training loss: 6.389291286468506 = 0.06284230947494507 + 1.0 * 6.326448917388916
Epoch 860, val loss: 0.7617715001106262
Epoch 870, training loss: 6.383885860443115 = 0.05975697189569473 + 1.0 * 6.324129104614258
Epoch 870, val loss: 0.7652437686920166
Epoch 880, training loss: 6.38054084777832 = 0.05685904994606972 + 1.0 * 6.323681831359863
Epoch 880, val loss: 0.7687792181968689
Epoch 890, training loss: 6.379833698272705 = 0.054150842130184174 + 1.0 * 6.325682640075684
Epoch 890, val loss: 0.7723430395126343
Epoch 900, training loss: 6.375320911407471 = 0.05162525177001953 + 1.0 * 6.323695659637451
Epoch 900, val loss: 0.7760140895843506
Epoch 910, training loss: 6.376726150512695 = 0.049260854721069336 + 1.0 * 6.327465534210205
Epoch 910, val loss: 0.7797805070877075
Epoch 920, training loss: 6.3666815757751465 = 0.04705532267689705 + 1.0 * 6.319626331329346
Epoch 920, val loss: 0.7835734486579895
Epoch 930, training loss: 6.363455772399902 = 0.044983524829149246 + 1.0 * 6.318472385406494
Epoch 930, val loss: 0.7874667048454285
Epoch 940, training loss: 6.367745876312256 = 0.04303162172436714 + 1.0 * 6.324714183807373
Epoch 940, val loss: 0.7913636565208435
Epoch 950, training loss: 6.359570026397705 = 0.041217729449272156 + 1.0 * 6.318352222442627
Epoch 950, val loss: 0.7952176928520203
Epoch 960, training loss: 6.3558807373046875 = 0.03950480371713638 + 1.0 * 6.316375732421875
Epoch 960, val loss: 0.7992230653762817
Epoch 970, training loss: 6.353206634521484 = 0.03789361193776131 + 1.0 * 6.31531286239624
Epoch 970, val loss: 0.8032113909721375
Epoch 980, training loss: 6.355556488037109 = 0.0363757461309433 + 1.0 * 6.319180965423584
Epoch 980, val loss: 0.8072004914283752
Epoch 990, training loss: 6.351114273071289 = 0.034944746643304825 + 1.0 * 6.316169738769531
Epoch 990, val loss: 0.8111459016799927
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8033737480231946
The final CL Acc:0.74938, 0.02444, The final GNN Acc:0.80074, 0.00240
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13212])
remove edge: torch.Size([2, 7862])
updated graph: torch.Size([2, 10518])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.549657821655273 = 1.952800989151001 + 1.0 * 8.596857070922852
Epoch 0, val loss: 1.9542073011398315
Epoch 10, training loss: 10.539081573486328 = 1.9424182176589966 + 1.0 * 8.596663475036621
Epoch 10, val loss: 1.9435207843780518
Epoch 20, training loss: 10.525086402893066 = 1.9299647808074951 + 1.0 * 8.595121383666992
Epoch 20, val loss: 1.9306204319000244
Epoch 30, training loss: 10.495004653930664 = 1.9127553701400757 + 1.0 * 8.582249641418457
Epoch 30, val loss: 1.9128601551055908
Epoch 40, training loss: 10.389493942260742 = 1.8892145156860352 + 1.0 * 8.500279426574707
Epoch 40, val loss: 1.8893210887908936
Epoch 50, training loss: 10.036273956298828 = 1.863458275794983 + 1.0 * 8.172815322875977
Epoch 50, val loss: 1.864329218864441
Epoch 60, training loss: 9.754761695861816 = 1.8391692638397217 + 1.0 * 7.915592193603516
Epoch 60, val loss: 1.840852975845337
Epoch 70, training loss: 9.4025239944458 = 1.819803237915039 + 1.0 * 7.582720756530762
Epoch 70, val loss: 1.8225793838500977
Epoch 80, training loss: 9.057974815368652 = 1.807953119277954 + 1.0 * 7.250021457672119
Epoch 80, val loss: 1.811545729637146
Epoch 90, training loss: 8.852469444274902 = 1.7899351119995117 + 1.0 * 7.062534332275391
Epoch 90, val loss: 1.7954163551330566
Epoch 100, training loss: 8.686305046081543 = 1.7702789306640625 + 1.0 * 6.9160261154174805
Epoch 100, val loss: 1.7789713144302368
Epoch 110, training loss: 8.579450607299805 = 1.752049446105957 + 1.0 * 6.8274006843566895
Epoch 110, val loss: 1.7631794214248657
Epoch 120, training loss: 8.490726470947266 = 1.7320574522018433 + 1.0 * 6.758669376373291
Epoch 120, val loss: 1.7452467679977417
Epoch 130, training loss: 8.41611099243164 = 1.7094752788543701 + 1.0 * 6.70663595199585
Epoch 130, val loss: 1.7251830101013184
Epoch 140, training loss: 8.3511962890625 = 1.684549331665039 + 1.0 * 6.666647434234619
Epoch 140, val loss: 1.7033030986785889
Epoch 150, training loss: 8.291193008422852 = 1.656434416770935 + 1.0 * 6.634758472442627
Epoch 150, val loss: 1.6790202856063843
Epoch 160, training loss: 8.235048294067383 = 1.6245648860931396 + 1.0 * 6.610483646392822
Epoch 160, val loss: 1.6517301797866821
Epoch 170, training loss: 8.178765296936035 = 1.5896518230438232 + 1.0 * 6.589113712310791
Epoch 170, val loss: 1.6222431659698486
Epoch 180, training loss: 8.118687629699707 = 1.5521434545516968 + 1.0 * 6.566544055938721
Epoch 180, val loss: 1.5907310247421265
Epoch 190, training loss: 8.0607328414917 = 1.512215495109558 + 1.0 * 6.54851770401001
Epoch 190, val loss: 1.5574018955230713
Epoch 200, training loss: 8.010374069213867 = 1.4705395698547363 + 1.0 * 6.539834022521973
Epoch 200, val loss: 1.5229674577713013
Epoch 210, training loss: 7.952073574066162 = 1.428784966468811 + 1.0 * 6.523288726806641
Epoch 210, val loss: 1.489137053489685
Epoch 220, training loss: 7.897176742553711 = 1.3872278928756714 + 1.0 * 6.50994873046875
Epoch 220, val loss: 1.4560327529907227
Epoch 230, training loss: 7.851469039916992 = 1.3460456132888794 + 1.0 * 6.505423545837402
Epoch 230, val loss: 1.423819661140442
Epoch 240, training loss: 7.800325393676758 = 1.3061647415161133 + 1.0 * 6.4941606521606445
Epoch 240, val loss: 1.3932712078094482
Epoch 250, training loss: 7.750967979431152 = 1.267320156097412 + 1.0 * 6.48364782333374
Epoch 250, val loss: 1.3640539646148682
Epoch 260, training loss: 7.705141544342041 = 1.22891104221344 + 1.0 * 6.476230621337891
Epoch 260, val loss: 1.335747241973877
Epoch 270, training loss: 7.660266876220703 = 1.1909443140029907 + 1.0 * 6.469322681427002
Epoch 270, val loss: 1.3082706928253174
Epoch 280, training loss: 7.616038799285889 = 1.1532832384109497 + 1.0 * 6.4627556800842285
Epoch 280, val loss: 1.2813072204589844
Epoch 290, training loss: 7.571890830993652 = 1.115452527999878 + 1.0 * 6.456438064575195
Epoch 290, val loss: 1.2545793056488037
Epoch 300, training loss: 7.53643274307251 = 1.0772358179092407 + 1.0 * 6.459197044372559
Epoch 300, val loss: 1.2279223203659058
Epoch 310, training loss: 7.487066268920898 = 1.039108395576477 + 1.0 * 6.447957992553711
Epoch 310, val loss: 1.201202392578125
Epoch 320, training loss: 7.442599773406982 = 1.000808596611023 + 1.0 * 6.44179105758667
Epoch 320, val loss: 1.1745043992996216
Epoch 330, training loss: 7.4073991775512695 = 0.9623865485191345 + 1.0 * 6.44501256942749
Epoch 330, val loss: 1.1477547883987427
Epoch 340, training loss: 7.357309341430664 = 0.9240715503692627 + 1.0 * 6.4332380294799805
Epoch 340, val loss: 1.121120572090149
Epoch 350, training loss: 7.316153526306152 = 0.8858426809310913 + 1.0 * 6.4303107261657715
Epoch 350, val loss: 1.0945144891738892
Epoch 360, training loss: 7.272747039794922 = 0.847556471824646 + 1.0 * 6.425190448760986
Epoch 360, val loss: 1.0678369998931885
Epoch 370, training loss: 7.232587814331055 = 0.8090713024139404 + 1.0 * 6.423516273498535
Epoch 370, val loss: 1.041062355041504
Epoch 380, training loss: 7.192344665527344 = 0.7706428170204163 + 1.0 * 6.421701908111572
Epoch 380, val loss: 1.014358639717102
Epoch 390, training loss: 7.149123191833496 = 0.7323215007781982 + 1.0 * 6.416801452636719
Epoch 390, val loss: 0.9879500865936279
Epoch 400, training loss: 7.109345436096191 = 0.694570779800415 + 1.0 * 6.414774417877197
Epoch 400, val loss: 0.9622222781181335
Epoch 410, training loss: 7.066370964050293 = 0.6577164530754089 + 1.0 * 6.408654689788818
Epoch 410, val loss: 0.9374581575393677
Epoch 420, training loss: 7.030118465423584 = 0.6219473481178284 + 1.0 * 6.4081711769104
Epoch 420, val loss: 0.9140587449073792
Epoch 430, training loss: 6.993381500244141 = 0.58774334192276 + 1.0 * 6.405638217926025
Epoch 430, val loss: 0.8923394680023193
Epoch 440, training loss: 6.956669807434082 = 0.5554358959197998 + 1.0 * 6.401234149932861
Epoch 440, val loss: 0.8726682066917419
Epoch 450, training loss: 6.923349857330322 = 0.5247618556022644 + 1.0 * 6.398588180541992
Epoch 450, val loss: 0.8549028038978577
Epoch 460, training loss: 6.894525527954102 = 0.495749831199646 + 1.0 * 6.398775577545166
Epoch 460, val loss: 0.8388175368309021
Epoch 470, training loss: 6.860804557800293 = 0.4683680534362793 + 1.0 * 6.392436504364014
Epoch 470, val loss: 0.8243780136108398
Epoch 480, training loss: 6.832391738891602 = 0.4420684576034546 + 1.0 * 6.390323162078857
Epoch 480, val loss: 0.8111206889152527
Epoch 490, training loss: 6.8131184577941895 = 0.41658762097358704 + 1.0 * 6.396530628204346
Epoch 490, val loss: 0.7988860607147217
Epoch 500, training loss: 6.779209136962891 = 0.3919849395751953 + 1.0 * 6.387224197387695
Epoch 500, val loss: 0.7874000668525696
Epoch 510, training loss: 6.752216815948486 = 0.36801794171333313 + 1.0 * 6.3841986656188965
Epoch 510, val loss: 0.7765541076660156
Epoch 520, training loss: 6.729158401489258 = 0.34466397762298584 + 1.0 * 6.384494304656982
Epoch 520, val loss: 0.7664452791213989
Epoch 530, training loss: 6.7069807052612305 = 0.32199957966804504 + 1.0 * 6.384981155395508
Epoch 530, val loss: 0.7569951415061951
Epoch 540, training loss: 6.677516460418701 = 0.3001675009727478 + 1.0 * 6.377348899841309
Epoch 540, val loss: 0.7483057379722595
Epoch 550, training loss: 6.654479503631592 = 0.2792865037918091 + 1.0 * 6.375193119049072
Epoch 550, val loss: 0.740509569644928
Epoch 560, training loss: 6.63930606842041 = 0.25952303409576416 + 1.0 * 6.3797831535339355
Epoch 560, val loss: 0.7336194515228271
Epoch 570, training loss: 6.612592697143555 = 0.24112127721309662 + 1.0 * 6.371471405029297
Epoch 570, val loss: 0.7276962399482727
Epoch 580, training loss: 6.592990875244141 = 0.22402067482471466 + 1.0 * 6.3689703941345215
Epoch 580, val loss: 0.7227485775947571
Epoch 590, training loss: 6.58290433883667 = 0.20824623107910156 + 1.0 * 6.374658107757568
Epoch 590, val loss: 0.7188366651535034
Epoch 600, training loss: 6.562107563018799 = 0.19384357333183289 + 1.0 * 6.368264198303223
Epoch 600, val loss: 0.7158450484275818
Epoch 610, training loss: 6.548627853393555 = 0.1806255280971527 + 1.0 * 6.368002414703369
Epoch 610, val loss: 0.7136498689651489
Epoch 620, training loss: 6.535566806793213 = 0.16861233115196228 + 1.0 * 6.366954326629639
Epoch 620, val loss: 0.7123261094093323
Epoch 630, training loss: 6.518125057220459 = 0.157631054520607 + 1.0 * 6.360494136810303
Epoch 630, val loss: 0.7115055918693542
Epoch 640, training loss: 6.505553722381592 = 0.14755477011203766 + 1.0 * 6.357998847961426
Epoch 640, val loss: 0.7113984823226929
Epoch 650, training loss: 6.49677848815918 = 0.13828223943710327 + 1.0 * 6.358496189117432
Epoch 650, val loss: 0.7118111848831177
Epoch 660, training loss: 6.4856648445129395 = 0.12976640462875366 + 1.0 * 6.355898380279541
Epoch 660, val loss: 0.7127070426940918
Epoch 670, training loss: 6.4818339347839355 = 0.1219409629702568 + 1.0 * 6.359892845153809
Epoch 670, val loss: 0.7139073014259338
Epoch 680, training loss: 6.471238136291504 = 0.11475826054811478 + 1.0 * 6.356479644775391
Epoch 680, val loss: 0.7155884504318237
Epoch 690, training loss: 6.458835601806641 = 0.1081174910068512 + 1.0 * 6.350718021392822
Epoch 690, val loss: 0.7174969911575317
Epoch 700, training loss: 6.4509687423706055 = 0.10196830332279205 + 1.0 * 6.349000453948975
Epoch 700, val loss: 0.7197823524475098
Epoch 710, training loss: 6.447164535522461 = 0.09625934809446335 + 1.0 * 6.350905418395996
Epoch 710, val loss: 0.7223778367042542
Epoch 720, training loss: 6.438266754150391 = 0.09096524864435196 + 1.0 * 6.347301483154297
Epoch 720, val loss: 0.7251766920089722
Epoch 730, training loss: 6.431760787963867 = 0.08605007827281952 + 1.0 * 6.345710754394531
Epoch 730, val loss: 0.7281712889671326
Epoch 740, training loss: 6.427400588989258 = 0.08147767186164856 + 1.0 * 6.345922946929932
Epoch 740, val loss: 0.7314181327819824
Epoch 750, training loss: 6.4203572273254395 = 0.0772259533405304 + 1.0 * 6.343131065368652
Epoch 750, val loss: 0.7348476052284241
Epoch 760, training loss: 6.414104461669922 = 0.07327137887477875 + 1.0 * 6.3408331871032715
Epoch 760, val loss: 0.7383356094360352
Epoch 770, training loss: 6.409088611602783 = 0.06958107650279999 + 1.0 * 6.339507579803467
Epoch 770, val loss: 0.7420323491096497
Epoch 780, training loss: 6.4171833992004395 = 0.06614009290933609 + 1.0 * 6.351043224334717
Epoch 780, val loss: 0.7459042072296143
Epoch 790, training loss: 6.4033122062683105 = 0.06291626393795013 + 1.0 * 6.340395927429199
Epoch 790, val loss: 0.7497380375862122
Epoch 800, training loss: 6.396562576293945 = 0.05992263928055763 + 1.0 * 6.336639881134033
Epoch 800, val loss: 0.753664493560791
Epoch 810, training loss: 6.391907215118408 = 0.05712044611573219 + 1.0 * 6.334786891937256
Epoch 810, val loss: 0.7577484250068665
Epoch 820, training loss: 6.394990921020508 = 0.05449367314577103 + 1.0 * 6.340497016906738
Epoch 820, val loss: 0.7618983387947083
Epoch 830, training loss: 6.387314319610596 = 0.05203067511320114 + 1.0 * 6.3352837562561035
Epoch 830, val loss: 0.7660759091377258
Epoch 840, training loss: 6.384030342102051 = 0.04972735419869423 + 1.0 * 6.33430290222168
Epoch 840, val loss: 0.7703517079353333
Epoch 850, training loss: 6.378049850463867 = 0.047566503286361694 + 1.0 * 6.330483436584473
Epoch 850, val loss: 0.774520754814148
Epoch 860, training loss: 6.3749613761901855 = 0.045537129044532776 + 1.0 * 6.3294243812561035
Epoch 860, val loss: 0.7788571119308472
Epoch 870, training loss: 6.383071422576904 = 0.04362265393137932 + 1.0 * 6.339448928833008
Epoch 870, val loss: 0.7831718325614929
Epoch 880, training loss: 6.374701023101807 = 0.041833363473415375 + 1.0 * 6.332867622375488
Epoch 880, val loss: 0.7875523567199707
Epoch 890, training loss: 6.366092205047607 = 0.040146224200725555 + 1.0 * 6.325945854187012
Epoch 890, val loss: 0.7917197346687317
Epoch 900, training loss: 6.363112926483154 = 0.03855530545115471 + 1.0 * 6.324557781219482
Epoch 900, val loss: 0.7960535287857056
Epoch 910, training loss: 6.362364292144775 = 0.037052251398563385 + 1.0 * 6.32531213760376
Epoch 910, val loss: 0.8003922700881958
Epoch 920, training loss: 6.362735748291016 = 0.03563358262181282 + 1.0 * 6.327102184295654
Epoch 920, val loss: 0.8047574758529663
Epoch 930, training loss: 6.357226371765137 = 0.03429638594388962 + 1.0 * 6.322929859161377
Epoch 930, val loss: 0.8088878989219666
Epoch 940, training loss: 6.354663848876953 = 0.03303418308496475 + 1.0 * 6.321629524230957
Epoch 940, val loss: 0.8131259083747864
Epoch 950, training loss: 6.351818084716797 = 0.03183765709400177 + 1.0 * 6.319980621337891
Epoch 950, val loss: 0.8174260258674622
Epoch 960, training loss: 6.35311222076416 = 0.03070167452096939 + 1.0 * 6.322410583496094
Epoch 960, val loss: 0.8216820359230042
Epoch 970, training loss: 6.349442481994629 = 0.029625210911035538 + 1.0 * 6.319817066192627
Epoch 970, val loss: 0.8259657025337219
Epoch 980, training loss: 6.34564208984375 = 0.028608419001102448 + 1.0 * 6.317033767700195
Epoch 980, val loss: 0.8301013112068176
Epoch 990, training loss: 6.344031810760498 = 0.027643151581287384 + 1.0 * 6.3163886070251465
Epoch 990, val loss: 0.8342345356941223
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 10.53436279296875 = 1.9375396966934204 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.943394660949707
Epoch 10, training loss: 10.52457046508789 = 1.9280351400375366 + 1.0 * 8.596535682678223
Epoch 10, val loss: 1.9337501525878906
Epoch 20, training loss: 10.510173797607422 = 1.9158434867858887 + 1.0 * 8.594330787658691
Epoch 20, val loss: 1.921499490737915
Epoch 30, training loss: 10.477335929870605 = 1.8983670473098755 + 1.0 * 8.57896900177002
Epoch 30, val loss: 1.903982162475586
Epoch 40, training loss: 10.373099327087402 = 1.8745251893997192 + 1.0 * 8.498574256896973
Epoch 40, val loss: 1.8806772232055664
Epoch 50, training loss: 10.03610897064209 = 1.8478660583496094 + 1.0 * 8.18824291229248
Epoch 50, val loss: 1.8555024862289429
Epoch 60, training loss: 9.745223999023438 = 1.8236767053604126 + 1.0 * 7.9215474128723145
Epoch 60, val loss: 1.8329726457595825
Epoch 70, training loss: 9.317760467529297 = 1.8037971258163452 + 1.0 * 7.513963222503662
Epoch 70, val loss: 1.813926100730896
Epoch 80, training loss: 8.99970817565918 = 1.7880245447158813 + 1.0 * 7.21168327331543
Epoch 80, val loss: 1.7987334728240967
Epoch 90, training loss: 8.823309898376465 = 1.769542932510376 + 1.0 * 7.053767204284668
Epoch 90, val loss: 1.780892252922058
Epoch 100, training loss: 8.688935279846191 = 1.7486456632614136 + 1.0 * 6.940289497375488
Epoch 100, val loss: 1.7611403465270996
Epoch 110, training loss: 8.578871726989746 = 1.7288103103637695 + 1.0 * 6.850061416625977
Epoch 110, val loss: 1.7422350645065308
Epoch 120, training loss: 8.493579864501953 = 1.7081881761550903 + 1.0 * 6.785391330718994
Epoch 120, val loss: 1.722982406616211
Epoch 130, training loss: 8.417193412780762 = 1.6839269399642944 + 1.0 * 6.733266830444336
Epoch 130, val loss: 1.701263189315796
Epoch 140, training loss: 8.345865249633789 = 1.6554675102233887 + 1.0 * 6.6903977394104
Epoch 140, val loss: 1.6764757633209229
Epoch 150, training loss: 8.281018257141113 = 1.622867465019226 + 1.0 * 6.658151149749756
Epoch 150, val loss: 1.648629903793335
Epoch 160, training loss: 8.212608337402344 = 1.5857335329055786 + 1.0 * 6.626874923706055
Epoch 160, val loss: 1.6173256635665894
Epoch 170, training loss: 8.145309448242188 = 1.5437850952148438 + 1.0 * 6.601524829864502
Epoch 170, val loss: 1.582202434539795
Epoch 180, training loss: 8.078838348388672 = 1.4971736669540405 + 1.0 * 6.581664562225342
Epoch 180, val loss: 1.5438024997711182
Epoch 190, training loss: 8.011035919189453 = 1.446696400642395 + 1.0 * 6.5643391609191895
Epoch 190, val loss: 1.5029537677764893
Epoch 200, training loss: 7.945178031921387 = 1.3931076526641846 + 1.0 * 6.552070617675781
Epoch 200, val loss: 1.4603303670883179
Epoch 210, training loss: 7.876880645751953 = 1.3378942012786865 + 1.0 * 6.538986682891846
Epoch 210, val loss: 1.4173287153244019
Epoch 220, training loss: 7.810542583465576 = 1.2819727659225464 + 1.0 * 6.52856969833374
Epoch 220, val loss: 1.3743078708648682
Epoch 230, training loss: 7.743747234344482 = 1.225570559501648 + 1.0 * 6.518176555633545
Epoch 230, val loss: 1.3312993049621582
Epoch 240, training loss: 7.67880916595459 = 1.1692301034927368 + 1.0 * 6.509579181671143
Epoch 240, val loss: 1.2886404991149902
Epoch 250, training loss: 7.618063926696777 = 1.114069938659668 + 1.0 * 6.503993988037109
Epoch 250, val loss: 1.247009515762329
Epoch 260, training loss: 7.557720184326172 = 1.0613899230957031 + 1.0 * 6.496330261230469
Epoch 260, val loss: 1.2073506116867065
Epoch 270, training loss: 7.498824119567871 = 1.0109251737594604 + 1.0 * 6.487898826599121
Epoch 270, val loss: 1.1692211627960205
Epoch 280, training loss: 7.446710586547852 = 0.9625543355941772 + 1.0 * 6.484156131744385
Epoch 280, val loss: 1.132341980934143
Epoch 290, training loss: 7.393721580505371 = 0.9165666103363037 + 1.0 * 6.4771552085876465
Epoch 290, val loss: 1.0972857475280762
Epoch 300, training loss: 7.342708110809326 = 0.8731867671012878 + 1.0 * 6.469521522521973
Epoch 300, val loss: 1.0638213157653809
Epoch 310, training loss: 7.296120643615723 = 0.8320271372795105 + 1.0 * 6.4640936851501465
Epoch 310, val loss: 1.031769037246704
Epoch 320, training loss: 7.254913330078125 = 0.7928048372268677 + 1.0 * 6.462108612060547
Epoch 320, val loss: 1.0010366439819336
Epoch 330, training loss: 7.2085185050964355 = 0.7557533383369446 + 1.0 * 6.452764987945557
Epoch 330, val loss: 0.9717215299606323
Epoch 340, training loss: 7.168455600738525 = 0.7207551598548889 + 1.0 * 6.447700500488281
Epoch 340, val loss: 0.9441127777099609
Epoch 350, training loss: 7.129402160644531 = 0.6876708269119263 + 1.0 * 6.4417314529418945
Epoch 350, val loss: 0.9181813597679138
Epoch 360, training loss: 7.094816207885742 = 0.656043529510498 + 1.0 * 6.438772678375244
Epoch 360, val loss: 0.8939142227172852
Epoch 370, training loss: 7.065524101257324 = 0.6257736682891846 + 1.0 * 6.439750671386719
Epoch 370, val loss: 0.8713423609733582
Epoch 380, training loss: 7.0244293212890625 = 0.5967568159103394 + 1.0 * 6.427672386169434
Epoch 380, val loss: 0.8505504727363586
Epoch 390, training loss: 6.991847515106201 = 0.5685076117515564 + 1.0 * 6.42333984375
Epoch 390, val loss: 0.8313132524490356
Epoch 400, training loss: 6.959719181060791 = 0.540850818157196 + 1.0 * 6.418868541717529
Epoch 400, val loss: 0.8134754300117493
Epoch 410, training loss: 6.937393665313721 = 0.5139665603637695 + 1.0 * 6.423427104949951
Epoch 410, val loss: 0.7972263097763062
Epoch 420, training loss: 6.899914264678955 = 0.4884156882762909 + 1.0 * 6.411498546600342
Epoch 420, val loss: 0.7829781770706177
Epoch 430, training loss: 6.873899459838867 = 0.4640848636627197 + 1.0 * 6.409814357757568
Epoch 430, val loss: 0.7706188559532166
Epoch 440, training loss: 6.845144271850586 = 0.44085749983787537 + 1.0 * 6.404286861419678
Epoch 440, val loss: 0.7598696947097778
Epoch 450, training loss: 6.829750061035156 = 0.41879263520240784 + 1.0 * 6.410957336425781
Epoch 450, val loss: 0.7506673336029053
Epoch 460, training loss: 6.796346187591553 = 0.39803752303123474 + 1.0 * 6.398308753967285
Epoch 460, val loss: 0.7429988384246826
Epoch 470, training loss: 6.779386520385742 = 0.37847208976745605 + 1.0 * 6.400914192199707
Epoch 470, val loss: 0.7366649508476257
Epoch 480, training loss: 6.756372451782227 = 0.35995423793792725 + 1.0 * 6.39641809463501
Epoch 480, val loss: 0.7314309477806091
Epoch 490, training loss: 6.737805366516113 = 0.3424452245235443 + 1.0 * 6.395359992980957
Epoch 490, val loss: 0.7271720170974731
Epoch 500, training loss: 6.715331554412842 = 0.32576698064804077 + 1.0 * 6.389564514160156
Epoch 500, val loss: 0.7236889600753784
Epoch 510, training loss: 6.695281028747559 = 0.30972206592559814 + 1.0 * 6.38555908203125
Epoch 510, val loss: 0.7208564877510071
Epoch 520, training loss: 6.677679538726807 = 0.29426705837249756 + 1.0 * 6.3834123611450195
Epoch 520, val loss: 0.7186010479927063
Epoch 530, training loss: 6.660762786865234 = 0.27931272983551025 + 1.0 * 6.381450176239014
Epoch 530, val loss: 0.7169086933135986
Epoch 540, training loss: 6.644474506378174 = 0.2647530138492584 + 1.0 * 6.379721641540527
Epoch 540, val loss: 0.7157351970672607
Epoch 550, training loss: 6.6299214363098145 = 0.2506006062030792 + 1.0 * 6.3793206214904785
Epoch 550, val loss: 0.715012788772583
Epoch 560, training loss: 6.611584663391113 = 0.236888587474823 + 1.0 * 6.374696254730225
Epoch 560, val loss: 0.7147611379623413
Epoch 570, training loss: 6.595004558563232 = 0.2235691249370575 + 1.0 * 6.371435642242432
Epoch 570, val loss: 0.7150154113769531
Epoch 580, training loss: 6.580643653869629 = 0.21066953241825104 + 1.0 * 6.369974136352539
Epoch 580, val loss: 0.7157392501831055
Epoch 590, training loss: 6.5748772621154785 = 0.1982731968164444 + 1.0 * 6.376604080200195
Epoch 590, val loss: 0.7169848680496216
Epoch 600, training loss: 6.555552005767822 = 0.1865588277578354 + 1.0 * 6.368993282318115
Epoch 600, val loss: 0.718757688999176
Epoch 610, training loss: 6.539351463317871 = 0.17545747756958008 + 1.0 * 6.363893985748291
Epoch 610, val loss: 0.7210341691970825
Epoch 620, training loss: 6.527603626251221 = 0.16493695974349976 + 1.0 * 6.362666606903076
Epoch 620, val loss: 0.723783552646637
Epoch 630, training loss: 6.5333991050720215 = 0.15500180423259735 + 1.0 * 6.378397464752197
Epoch 630, val loss: 0.7269825339317322
Epoch 640, training loss: 6.51125955581665 = 0.14576303958892822 + 1.0 * 6.365496635437012
Epoch 640, val loss: 0.7305470108985901
Epoch 650, training loss: 6.4970011711120605 = 0.13713014125823975 + 1.0 * 6.359870910644531
Epoch 650, val loss: 0.7344449758529663
Epoch 660, training loss: 6.4847917556762695 = 0.129033163189888 + 1.0 * 6.3557586669921875
Epoch 660, val loss: 0.7387109398841858
Epoch 670, training loss: 6.4754486083984375 = 0.12143148481845856 + 1.0 * 6.35401725769043
Epoch 670, val loss: 0.7432976365089417
Epoch 680, training loss: 6.487071990966797 = 0.11431463062763214 + 1.0 * 6.372757434844971
Epoch 680, val loss: 0.7481627464294434
Epoch 690, training loss: 6.462061405181885 = 0.10771090537309647 + 1.0 * 6.354350566864014
Epoch 690, val loss: 0.7532811760902405
Epoch 700, training loss: 6.451713562011719 = 0.10155360400676727 + 1.0 * 6.350160121917725
Epoch 700, val loss: 0.7586174011230469
Epoch 710, training loss: 6.443641662597656 = 0.0957942008972168 + 1.0 * 6.3478474617004395
Epoch 710, val loss: 0.7642245888710022
Epoch 720, training loss: 6.455075740814209 = 0.09041809290647507 + 1.0 * 6.364657878875732
Epoch 720, val loss: 0.7700518369674683
Epoch 730, training loss: 6.431682109832764 = 0.08541624993085861 + 1.0 * 6.34626579284668
Epoch 730, val loss: 0.7760447263717651
Epoch 740, training loss: 6.425257682800293 = 0.0807688981294632 + 1.0 * 6.344488620758057
Epoch 740, val loss: 0.7821398377418518
Epoch 750, training loss: 6.425507068634033 = 0.07643165439367294 + 1.0 * 6.3490753173828125
Epoch 750, val loss: 0.7884555459022522
Epoch 760, training loss: 6.41605806350708 = 0.07239097356796265 + 1.0 * 6.343667030334473
Epoch 760, val loss: 0.7949333786964417
Epoch 770, training loss: 6.4102630615234375 = 0.06862059980630875 + 1.0 * 6.341642379760742
Epoch 770, val loss: 0.8014148473739624
Epoch 780, training loss: 6.406678676605225 = 0.06509776413440704 + 1.0 * 6.341580867767334
Epoch 780, val loss: 0.8080876469612122
Epoch 790, training loss: 6.398825168609619 = 0.061805810779333115 + 1.0 * 6.337019443511963
Epoch 790, val loss: 0.8148342370986938
Epoch 800, training loss: 6.396936893463135 = 0.058726709336042404 + 1.0 * 6.338210105895996
Epoch 800, val loss: 0.8216305375099182
Epoch 810, training loss: 6.397319316864014 = 0.05584874376654625 + 1.0 * 6.341470718383789
Epoch 810, val loss: 0.8284875750541687
Epoch 820, training loss: 6.3907036781311035 = 0.05316920951008797 + 1.0 * 6.337534427642822
Epoch 820, val loss: 0.8354580998420715
Epoch 830, training loss: 6.386931896209717 = 0.05066649243235588 + 1.0 * 6.336265563964844
Epoch 830, val loss: 0.8423148393630981
Epoch 840, training loss: 6.380658149719238 = 0.048324260860681534 + 1.0 * 6.332334041595459
Epoch 840, val loss: 0.8492549061775208
Epoch 850, training loss: 6.382026672363281 = 0.046129900962114334 + 1.0 * 6.335896968841553
Epoch 850, val loss: 0.8562203049659729
Epoch 860, training loss: 6.375948905944824 = 0.044079188257455826 + 1.0 * 6.331869602203369
Epoch 860, val loss: 0.8632438778877258
Epoch 870, training loss: 6.371309757232666 = 0.04215040057897568 + 1.0 * 6.329159259796143
Epoch 870, val loss: 0.8701571226119995
Epoch 880, training loss: 6.370060443878174 = 0.04034165292978287 + 1.0 * 6.329718589782715
Epoch 880, val loss: 0.8771369457244873
Epoch 890, training loss: 6.364330291748047 = 0.03864028677344322 + 1.0 * 6.325689792633057
Epoch 890, val loss: 0.8840091824531555
Epoch 900, training loss: 6.367366790771484 = 0.03703805431723595 + 1.0 * 6.330328941345215
Epoch 900, val loss: 0.8908749222755432
Epoch 910, training loss: 6.361425399780273 = 0.03553922101855278 + 1.0 * 6.325886249542236
Epoch 910, val loss: 0.8977901935577393
Epoch 920, training loss: 6.364284992218018 = 0.034124672412872314 + 1.0 * 6.330160140991211
Epoch 920, val loss: 0.9045434594154358
Epoch 930, training loss: 6.356206893920898 = 0.03279603645205498 + 1.0 * 6.323410987854004
Epoch 930, val loss: 0.9112192988395691
Epoch 940, training loss: 6.352102756500244 = 0.03154299780726433 + 1.0 * 6.320559978485107
Epoch 940, val loss: 0.917874813079834
Epoch 950, training loss: 6.350637435913086 = 0.030352625995874405 + 1.0 * 6.320284843444824
Epoch 950, val loss: 0.9245238900184631
Epoch 960, training loss: 6.357572555541992 = 0.029231581836938858 + 1.0 * 6.328341007232666
Epoch 960, val loss: 0.9311435222625732
Epoch 970, training loss: 6.346935272216797 = 0.02817307412624359 + 1.0 * 6.318762302398682
Epoch 970, val loss: 0.9376370906829834
Epoch 980, training loss: 6.344738483428955 = 0.02717369608581066 + 1.0 * 6.317564964294434
Epoch 980, val loss: 0.943976640701294
Epoch 990, training loss: 6.350551128387451 = 0.026224184781312943 + 1.0 * 6.324326992034912
Epoch 990, val loss: 0.950404167175293
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 10.535721778869629 = 1.9388691186904907 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.933732271194458
Epoch 10, training loss: 10.526183128356934 = 1.9295308589935303 + 1.0 * 8.596652030944824
Epoch 10, val loss: 1.92410409450531
Epoch 20, training loss: 10.513442993164062 = 1.9182634353637695 + 1.0 * 8.595179557800293
Epoch 20, val loss: 1.9125603437423706
Epoch 30, training loss: 10.485245704650879 = 1.9029381275177002 + 1.0 * 8.582307815551758
Epoch 30, val loss: 1.897191047668457
Epoch 40, training loss: 10.364522933959961 = 1.882357120513916 + 1.0 * 8.482165336608887
Epoch 40, val loss: 1.8773162364959717
Epoch 50, training loss: 9.976995468139648 = 1.8591951131820679 + 1.0 * 8.11780071258545
Epoch 50, val loss: 1.8557456731796265
Epoch 60, training loss: 9.48222827911377 = 1.8394453525543213 + 1.0 * 7.642782688140869
Epoch 60, val loss: 1.8373054265975952
Epoch 70, training loss: 9.01870346069336 = 1.8242276906967163 + 1.0 * 7.1944756507873535
Epoch 70, val loss: 1.8224925994873047
Epoch 80, training loss: 8.793745994567871 = 1.808858871459961 + 1.0 * 6.98488712310791
Epoch 80, val loss: 1.8075976371765137
Epoch 90, training loss: 8.654284477233887 = 1.7907660007476807 + 1.0 * 6.863518238067627
Epoch 90, val loss: 1.7911627292633057
Epoch 100, training loss: 8.555532455444336 = 1.7720223665237427 + 1.0 * 6.783510208129883
Epoch 100, val loss: 1.7744463682174683
Epoch 110, training loss: 8.472360610961914 = 1.753844141960144 + 1.0 * 6.718516826629639
Epoch 110, val loss: 1.7579851150512695
Epoch 120, training loss: 8.40950870513916 = 1.7352911233901978 + 1.0 * 6.674217224121094
Epoch 120, val loss: 1.7408069372177124
Epoch 130, training loss: 8.352714538574219 = 1.71499502658844 + 1.0 * 6.63771915435791
Epoch 130, val loss: 1.722092628479004
Epoch 140, training loss: 8.301328659057617 = 1.6920459270477295 + 1.0 * 6.609282493591309
Epoch 140, val loss: 1.7013105154037476
Epoch 150, training loss: 8.252266883850098 = 1.665755033493042 + 1.0 * 6.586511611938477
Epoch 150, val loss: 1.677913784980774
Epoch 160, training loss: 8.204608917236328 = 1.6356942653656006 + 1.0 * 6.568914413452148
Epoch 160, val loss: 1.6513551473617554
Epoch 170, training loss: 8.151275634765625 = 1.6014318466186523 + 1.0 * 6.549844264984131
Epoch 170, val loss: 1.6212553977966309
Epoch 180, training loss: 8.099214553833008 = 1.5625370740890503 + 1.0 * 6.536677837371826
Epoch 180, val loss: 1.587340235710144
Epoch 190, training loss: 8.043264389038086 = 1.519227147102356 + 1.0 * 6.5240373611450195
Epoch 190, val loss: 1.5498405694961548
Epoch 200, training loss: 7.98282527923584 = 1.471863865852356 + 1.0 * 6.510961532592773
Epoch 200, val loss: 1.5090632438659668
Epoch 210, training loss: 7.920867443084717 = 1.4201523065567017 + 1.0 * 6.500715255737305
Epoch 210, val loss: 1.4647326469421387
Epoch 220, training loss: 7.858440399169922 = 1.3647470474243164 + 1.0 * 6.4936933517456055
Epoch 220, val loss: 1.4177125692367554
Epoch 230, training loss: 7.792730331420898 = 1.3070095777511597 + 1.0 * 6.485720634460449
Epoch 230, val loss: 1.3692525625228882
Epoch 240, training loss: 7.724846839904785 = 1.2474104166030884 + 1.0 * 6.477436542510986
Epoch 240, val loss: 1.3195208311080933
Epoch 250, training loss: 7.6568498611450195 = 1.186058759689331 + 1.0 * 6.470790863037109
Epoch 250, val loss: 1.268753170967102
Epoch 260, training loss: 7.591935157775879 = 1.1237132549285889 + 1.0 * 6.468222141265869
Epoch 260, val loss: 1.217544436454773
Epoch 270, training loss: 7.523579120635986 = 1.0622344017028809 + 1.0 * 6.4613447189331055
Epoch 270, val loss: 1.16725492477417
Epoch 280, training loss: 7.457544326782227 = 1.0023025274276733 + 1.0 * 6.455241680145264
Epoch 280, val loss: 1.1184651851654053
Epoch 290, training loss: 7.399266719818115 = 0.9442809820175171 + 1.0 * 6.454985618591309
Epoch 290, val loss: 1.0714118480682373
Epoch 300, training loss: 7.339372158050537 = 0.8903013467788696 + 1.0 * 6.449070930480957
Epoch 300, val loss: 1.027829885482788
Epoch 310, training loss: 7.2819647789001465 = 0.8405414819717407 + 1.0 * 6.441423416137695
Epoch 310, val loss: 0.9877788424491882
Epoch 320, training loss: 7.23126220703125 = 0.7945846319198608 + 1.0 * 6.4366774559021
Epoch 320, val loss: 0.9511235952377319
Epoch 330, training loss: 7.187564373016357 = 0.7525544166564941 + 1.0 * 6.435009956359863
Epoch 330, val loss: 0.9180744290351868
Epoch 340, training loss: 7.143378257751465 = 0.7146275043487549 + 1.0 * 6.428750514984131
Epoch 340, val loss: 0.8888285160064697
Epoch 350, training loss: 7.103863716125488 = 0.6799070835113525 + 1.0 * 6.423956394195557
Epoch 350, val loss: 0.8626455664634705
Epoch 360, training loss: 7.071094036102295 = 0.6477688550949097 + 1.0 * 6.423325061798096
Epoch 360, val loss: 0.839027464389801
Epoch 370, training loss: 7.037766933441162 = 0.6180295944213867 + 1.0 * 6.419737339019775
Epoch 370, val loss: 0.8178044557571411
Epoch 380, training loss: 7.005566120147705 = 0.5902690291404724 + 1.0 * 6.415297031402588
Epoch 380, val loss: 0.7985039353370667
Epoch 390, training loss: 6.978054523468018 = 0.564336359500885 + 1.0 * 6.413718223571777
Epoch 390, val loss: 0.7809696793556213
Epoch 400, training loss: 6.946950912475586 = 0.5400522351264954 + 1.0 * 6.406898498535156
Epoch 400, val loss: 0.7650157809257507
Epoch 410, training loss: 6.920682907104492 = 0.5169953107833862 + 1.0 * 6.403687477111816
Epoch 410, val loss: 0.7503751516342163
Epoch 420, training loss: 6.898373126983643 = 0.49502134323120117 + 1.0 * 6.403351783752441
Epoch 420, val loss: 0.736907958984375
Epoch 430, training loss: 6.876497745513916 = 0.47416239976882935 + 1.0 * 6.402335166931152
Epoch 430, val loss: 0.7245786190032959
Epoch 440, training loss: 6.8506903648376465 = 0.454286128282547 + 1.0 * 6.396404266357422
Epoch 440, val loss: 0.713434100151062
Epoch 450, training loss: 6.828309535980225 = 0.43524986505508423 + 1.0 * 6.393059730529785
Epoch 450, val loss: 0.703348696231842
Epoch 460, training loss: 6.808778762817383 = 0.41702473163604736 + 1.0 * 6.391754150390625
Epoch 460, val loss: 0.6942230463027954
Epoch 470, training loss: 6.787060260772705 = 0.3993604779243469 + 1.0 * 6.387699604034424
Epoch 470, val loss: 0.6859807372093201
Epoch 480, training loss: 6.770375728607178 = 0.3822287321090698 + 1.0 * 6.388146877288818
Epoch 480, val loss: 0.6784927248954773
Epoch 490, training loss: 6.750018119812012 = 0.3656013309955597 + 1.0 * 6.384416580200195
Epoch 490, val loss: 0.6716884970664978
Epoch 500, training loss: 6.729971885681152 = 0.3492828905582428 + 1.0 * 6.3806891441345215
Epoch 500, val loss: 0.6655557155609131
Epoch 510, training loss: 6.719239234924316 = 0.3332171142101288 + 1.0 * 6.386022090911865
Epoch 510, val loss: 0.6599709391593933
Epoch 520, training loss: 6.6939826011657715 = 0.31747546792030334 + 1.0 * 6.37650728225708
Epoch 520, val loss: 0.6548662781715393
Epoch 530, training loss: 6.676541805267334 = 0.30197349190711975 + 1.0 * 6.374568462371826
Epoch 530, val loss: 0.6502369046211243
Epoch 540, training loss: 6.668262958526611 = 0.2866918742656708 + 1.0 * 6.381571292877197
Epoch 540, val loss: 0.6460294723510742
Epoch 550, training loss: 6.645098686218262 = 0.27180150151252747 + 1.0 * 6.373297214508057
Epoch 550, val loss: 0.6422263979911804
Epoch 560, training loss: 6.627080917358398 = 0.2573305070400238 + 1.0 * 6.369750499725342
Epoch 560, val loss: 0.6388313174247742
Epoch 570, training loss: 6.609981536865234 = 0.24324721097946167 + 1.0 * 6.366734504699707
Epoch 570, val loss: 0.6359473466873169
Epoch 580, training loss: 6.607431888580322 = 0.22964362800121307 + 1.0 * 6.377788066864014
Epoch 580, val loss: 0.6335252523422241
Epoch 590, training loss: 6.581359386444092 = 0.21663133800029755 + 1.0 * 6.364727973937988
Epoch 590, val loss: 0.6316728591918945
Epoch 600, training loss: 6.5683088302612305 = 0.20423763990402222 + 1.0 * 6.364071369171143
Epoch 600, val loss: 0.6303410530090332
Epoch 610, training loss: 6.5522236824035645 = 0.19244766235351562 + 1.0 * 6.359776020050049
Epoch 610, val loss: 0.6295959949493408
Epoch 620, training loss: 6.545122146606445 = 0.18122872710227966 + 1.0 * 6.363893508911133
Epoch 620, val loss: 0.6293786764144897
Epoch 630, training loss: 6.532172203063965 = 0.1706704944372177 + 1.0 * 6.361501693725586
Epoch 630, val loss: 0.6296695470809937
Epoch 640, training loss: 6.518777847290039 = 0.16078777611255646 + 1.0 * 6.357990264892578
Epoch 640, val loss: 0.630351185798645
Epoch 650, training loss: 6.5061187744140625 = 0.15146507322788239 + 1.0 * 6.354653835296631
Epoch 650, val loss: 0.6316038966178894
Epoch 660, training loss: 6.500151634216309 = 0.1426720768213272 + 1.0 * 6.357479572296143
Epoch 660, val loss: 0.6333373188972473
Epoch 670, training loss: 6.48568868637085 = 0.13442422449588776 + 1.0 * 6.351264476776123
Epoch 670, val loss: 0.6354333162307739
Epoch 680, training loss: 6.475549221038818 = 0.1266641467809677 + 1.0 * 6.3488850593566895
Epoch 680, val loss: 0.6378352642059326
Epoch 690, training loss: 6.475970268249512 = 0.11938721686601639 + 1.0 * 6.356583118438721
Epoch 690, val loss: 0.6406415700912476
Epoch 700, training loss: 6.46126127243042 = 0.11261698603630066 + 1.0 * 6.348644256591797
Epoch 700, val loss: 0.6436178088188171
Epoch 710, training loss: 6.452843189239502 = 0.10630962997674942 + 1.0 * 6.34653377532959
Epoch 710, val loss: 0.6468801498413086
Epoch 720, training loss: 6.443410396575928 = 0.1004101112484932 + 1.0 * 6.343000411987305
Epoch 720, val loss: 0.6504306793212891
Epoch 730, training loss: 6.440822124481201 = 0.09489966928958893 + 1.0 * 6.345922470092773
Epoch 730, val loss: 0.6542133688926697
Epoch 740, training loss: 6.43704080581665 = 0.08978399634361267 + 1.0 * 6.347256660461426
Epoch 740, val loss: 0.6581422686576843
Epoch 750, training loss: 6.431098937988281 = 0.08502379804849625 + 1.0 * 6.346075057983398
Epoch 750, val loss: 0.6621754765510559
Epoch 760, training loss: 6.422495365142822 = 0.0805978924036026 + 1.0 * 6.341897487640381
Epoch 760, val loss: 0.6664308905601501
Epoch 770, training loss: 6.413430690765381 = 0.0764797106385231 + 1.0 * 6.336950778961182
Epoch 770, val loss: 0.6707792282104492
Epoch 780, training loss: 6.408941268920898 = 0.07263071835041046 + 1.0 * 6.336310386657715
Epoch 780, val loss: 0.6752973198890686
Epoch 790, training loss: 6.409061431884766 = 0.069034144282341 + 1.0 * 6.340027332305908
Epoch 790, val loss: 0.68000328540802
Epoch 800, training loss: 6.401114463806152 = 0.06569623947143555 + 1.0 * 6.335418224334717
Epoch 800, val loss: 0.6846626996994019
Epoch 810, training loss: 6.395696640014648 = 0.06257491558790207 + 1.0 * 6.3331217765808105
Epoch 810, val loss: 0.68940269947052
Epoch 820, training loss: 6.39158296585083 = 0.05965530872344971 + 1.0 * 6.33192777633667
Epoch 820, val loss: 0.6942538022994995
Epoch 830, training loss: 6.3914289474487305 = 0.056920912116765976 + 1.0 * 6.334507942199707
Epoch 830, val loss: 0.6991787552833557
Epoch 840, training loss: 6.384577751159668 = 0.05437587574124336 + 1.0 * 6.330202102661133
Epoch 840, val loss: 0.7039989829063416
Epoch 850, training loss: 6.380972385406494 = 0.051997844129800797 + 1.0 * 6.328974723815918
Epoch 850, val loss: 0.708852231502533
Epoch 860, training loss: 6.375718593597412 = 0.049755923449993134 + 1.0 * 6.325962543487549
Epoch 860, val loss: 0.7138504981994629
Epoch 870, training loss: 6.387077808380127 = 0.047649919986724854 + 1.0 * 6.339427947998047
Epoch 870, val loss: 0.7188695073127747
Epoch 880, training loss: 6.372598648071289 = 0.04567692056298256 + 1.0 * 6.3269219398498535
Epoch 880, val loss: 0.7237836122512817
Epoch 890, training loss: 6.366232395172119 = 0.04382278025150299 + 1.0 * 6.322409629821777
Epoch 890, val loss: 0.7287282943725586
Epoch 900, training loss: 6.364267349243164 = 0.04207071661949158 + 1.0 * 6.3221964836120605
Epoch 900, val loss: 0.7337661385536194
Epoch 910, training loss: 6.375260829925537 = 0.04041784629225731 + 1.0 * 6.334843158721924
Epoch 910, val loss: 0.7387483716011047
Epoch 920, training loss: 6.364951133728027 = 0.038866184651851654 + 1.0 * 6.326085090637207
Epoch 920, val loss: 0.7436118125915527
Epoch 930, training loss: 6.358609199523926 = 0.03740471974015236 + 1.0 * 6.321204662322998
Epoch 930, val loss: 0.7484879493713379
Epoch 940, training loss: 6.354685306549072 = 0.03602408617734909 + 1.0 * 6.318661212921143
Epoch 940, val loss: 0.7533499002456665
Epoch 950, training loss: 6.3537211418151855 = 0.03471437841653824 + 1.0 * 6.31900691986084
Epoch 950, val loss: 0.7582167983055115
Epoch 960, training loss: 6.353646278381348 = 0.033472660928964615 + 1.0 * 6.320173740386963
Epoch 960, val loss: 0.7631043195724487
Epoch 970, training loss: 6.349212169647217 = 0.03229808807373047 + 1.0 * 6.316914081573486
Epoch 970, val loss: 0.767815113067627
Epoch 980, training loss: 6.349612236022949 = 0.031185058876872063 + 1.0 * 6.318427085876465
Epoch 980, val loss: 0.7725728750228882
Epoch 990, training loss: 6.347064018249512 = 0.030128328129649162 + 1.0 * 6.3169355392456055
Epoch 990, val loss: 0.7773746848106384
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8397469688982605
The final CL Acc:0.80247, 0.00175, The final GNN Acc:0.83904, 0.00179
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10604])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.552512168884277 = 1.9556725025177002 + 1.0 * 8.596839904785156
Epoch 0, val loss: 1.9539930820465088
Epoch 10, training loss: 10.541969299316406 = 1.9453359842300415 + 1.0 * 8.596632957458496
Epoch 10, val loss: 1.94418203830719
Epoch 20, training loss: 10.52815055847168 = 1.9330077171325684 + 1.0 * 8.59514331817627
Epoch 20, val loss: 1.9320186376571655
Epoch 30, training loss: 10.499700546264648 = 1.9162774085998535 + 1.0 * 8.583423614501953
Epoch 30, val loss: 1.9151273965835571
Epoch 40, training loss: 10.398887634277344 = 1.894044280052185 + 1.0 * 8.504843711853027
Epoch 40, val loss: 1.8929916620254517
Epoch 50, training loss: 10.035961151123047 = 1.871383547782898 + 1.0 * 8.16457748413086
Epoch 50, val loss: 1.8712648153305054
Epoch 60, training loss: 9.569209098815918 = 1.8529196977615356 + 1.0 * 7.716289043426514
Epoch 60, val loss: 1.8548206090927124
Epoch 70, training loss: 9.119714736938477 = 1.8397135734558105 + 1.0 * 7.280000686645508
Epoch 70, val loss: 1.8425133228302002
Epoch 80, training loss: 8.869816780090332 = 1.8268874883651733 + 1.0 * 7.042929649353027
Epoch 80, val loss: 1.8302299976348877
Epoch 90, training loss: 8.713606834411621 = 1.8131060600280762 + 1.0 * 6.900500774383545
Epoch 90, val loss: 1.8176101446151733
Epoch 100, training loss: 8.621698379516602 = 1.7988145351409912 + 1.0 * 6.822883605957031
Epoch 100, val loss: 1.8051947355270386
Epoch 110, training loss: 8.552435874938965 = 1.7849127054214478 + 1.0 * 6.767523288726807
Epoch 110, val loss: 1.7932615280151367
Epoch 120, training loss: 8.487480163574219 = 1.771568775177002 + 1.0 * 6.715910911560059
Epoch 120, val loss: 1.7815821170806885
Epoch 130, training loss: 8.43419075012207 = 1.7578998804092407 + 1.0 * 6.676290512084961
Epoch 130, val loss: 1.7695246934890747
Epoch 140, training loss: 8.38602066040039 = 1.743208408355713 + 1.0 * 6.6428117752075195
Epoch 140, val loss: 1.756734848022461
Epoch 150, training loss: 8.342266082763672 = 1.7270393371582031 + 1.0 * 6.615227222442627
Epoch 150, val loss: 1.7430322170257568
Epoch 160, training loss: 8.299226760864258 = 1.709114909172058 + 1.0 * 6.59011173248291
Epoch 160, val loss: 1.7279866933822632
Epoch 170, training loss: 8.257604598999023 = 1.6887437105178833 + 1.0 * 6.5688605308532715
Epoch 170, val loss: 1.711081624031067
Epoch 180, training loss: 8.216190338134766 = 1.6652092933654785 + 1.0 * 6.550981521606445
Epoch 180, val loss: 1.6916873455047607
Epoch 190, training loss: 8.173853874206543 = 1.637910008430481 + 1.0 * 6.535943508148193
Epoch 190, val loss: 1.6692837476730347
Epoch 200, training loss: 8.131122589111328 = 1.6063934564590454 + 1.0 * 6.524729251861572
Epoch 200, val loss: 1.6435788869857788
Epoch 210, training loss: 8.082853317260742 = 1.5705300569534302 + 1.0 * 6.512322902679443
Epoch 210, val loss: 1.6144332885742188
Epoch 220, training loss: 8.031304359436035 = 1.5297900438308716 + 1.0 * 6.501514434814453
Epoch 220, val loss: 1.581429362297058
Epoch 230, training loss: 7.975897789001465 = 1.4839268922805786 + 1.0 * 6.491971015930176
Epoch 230, val loss: 1.5444096326828003
Epoch 240, training loss: 7.918590068817139 = 1.433638095855713 + 1.0 * 6.484951972961426
Epoch 240, val loss: 1.5040758848190308
Epoch 250, training loss: 7.858500003814697 = 1.3805640935897827 + 1.0 * 6.477935791015625
Epoch 250, val loss: 1.4617559909820557
Epoch 260, training loss: 7.79447603225708 = 1.3250466585159302 + 1.0 * 6.4694294929504395
Epoch 260, val loss: 1.4177734851837158
Epoch 270, training loss: 7.737417697906494 = 1.268284797668457 + 1.0 * 6.469132900238037
Epoch 270, val loss: 1.3731176853179932
Epoch 280, training loss: 7.672142505645752 = 1.212593913078308 + 1.0 * 6.459548473358154
Epoch 280, val loss: 1.329803228378296
Epoch 290, training loss: 7.610332012176514 = 1.158652424812317 + 1.0 * 6.451679706573486
Epoch 290, val loss: 1.2881677150726318
Epoch 300, training loss: 7.552805423736572 = 1.1066311597824097 + 1.0 * 6.446174144744873
Epoch 300, val loss: 1.2485988140106201
Epoch 310, training loss: 7.501500129699707 = 1.057142734527588 + 1.0 * 6.444357395172119
Epoch 310, val loss: 1.211591124534607
Epoch 320, training loss: 7.447661399841309 = 1.010716199874878 + 1.0 * 6.43694543838501
Epoch 320, val loss: 1.177443027496338
Epoch 330, training loss: 7.397923469543457 = 0.9662604331970215 + 1.0 * 6.4316630363464355
Epoch 330, val loss: 1.1454617977142334
Epoch 340, training loss: 7.3507795333862305 = 0.9231635928153992 + 1.0 * 6.427616119384766
Epoch 340, val loss: 1.1150091886520386
Epoch 350, training loss: 7.305660247802734 = 0.8811931014060974 + 1.0 * 6.424467086791992
Epoch 350, val loss: 1.0858968496322632
Epoch 360, training loss: 7.261000156402588 = 0.8402817845344543 + 1.0 * 6.420718193054199
Epoch 360, val loss: 1.0580742359161377
Epoch 370, training loss: 7.216463088989258 = 0.8002498745918274 + 1.0 * 6.416213035583496
Epoch 370, val loss: 1.0314593315124512
Epoch 380, training loss: 7.174110412597656 = 0.761189877986908 + 1.0 * 6.4129204750061035
Epoch 380, val loss: 1.0059159994125366
Epoch 390, training loss: 7.13848876953125 = 0.7229811549186707 + 1.0 * 6.415507793426514
Epoch 390, val loss: 0.9815130233764648
Epoch 400, training loss: 7.092436790466309 = 0.68595290184021 + 1.0 * 6.4064836502075195
Epoch 400, val loss: 0.9584365487098694
Epoch 410, training loss: 7.053390979766846 = 0.6499987840652466 + 1.0 * 6.403392314910889
Epoch 410, val loss: 0.9368312954902649
Epoch 420, training loss: 7.017828464508057 = 0.6153623461723328 + 1.0 * 6.402466297149658
Epoch 420, val loss: 0.9166897535324097
Epoch 430, training loss: 6.983538627624512 = 0.5823637843132019 + 1.0 * 6.401175022125244
Epoch 430, val loss: 0.8986460566520691
Epoch 440, training loss: 6.9458160400390625 = 0.5508410334587097 + 1.0 * 6.394975185394287
Epoch 440, val loss: 0.8824911117553711
Epoch 450, training loss: 6.914389610290527 = 0.5207095146179199 + 1.0 * 6.393680095672607
Epoch 450, val loss: 0.8680909276008606
Epoch 460, training loss: 6.8834943771362305 = 0.49209511280059814 + 1.0 * 6.391399383544922
Epoch 460, val loss: 0.8554663062095642
Epoch 470, training loss: 6.855644226074219 = 0.4651300013065338 + 1.0 * 6.390514373779297
Epoch 470, val loss: 0.8446259498596191
Epoch 480, training loss: 6.823953628540039 = 0.4398125410079956 + 1.0 * 6.384140968322754
Epoch 480, val loss: 0.8357192873954773
Epoch 490, training loss: 6.798020839691162 = 0.4159049689769745 + 1.0 * 6.382115840911865
Epoch 490, val loss: 0.8283705711364746
Epoch 500, training loss: 6.793003559112549 = 0.39338502287864685 + 1.0 * 6.399618625640869
Epoch 500, val loss: 0.8225571513175964
Epoch 510, training loss: 6.754312992095947 = 0.3725511431694031 + 1.0 * 6.3817620277404785
Epoch 510, val loss: 0.8181350827217102
Epoch 520, training loss: 6.729730129241943 = 0.353083074092865 + 1.0 * 6.376646995544434
Epoch 520, val loss: 0.815384566783905
Epoch 530, training loss: 6.707469940185547 = 0.33479613065719604 + 1.0 * 6.372673988342285
Epoch 530, val loss: 0.8138006925582886
Epoch 540, training loss: 6.688270568847656 = 0.31758198142051697 + 1.0 * 6.370688438415527
Epoch 540, val loss: 0.8132561445236206
Epoch 550, training loss: 6.672734260559082 = 0.30144041776657104 + 1.0 * 6.371294021606445
Epoch 550, val loss: 0.8136810064315796
Epoch 560, training loss: 6.65905237197876 = 0.28641054034233093 + 1.0 * 6.3726420402526855
Epoch 560, val loss: 0.815043568611145
Epoch 570, training loss: 6.638557434082031 = 0.27235791087150574 + 1.0 * 6.366199493408203
Epoch 570, val loss: 0.817202627658844
Epoch 580, training loss: 6.622830867767334 = 0.25905221700668335 + 1.0 * 6.363778591156006
Epoch 580, val loss: 0.819974422454834
Epoch 590, training loss: 6.607725620269775 = 0.24635890126228333 + 1.0 * 6.3613667488098145
Epoch 590, val loss: 0.8231036067008972
Epoch 600, training loss: 6.598233699798584 = 0.23423738777637482 + 1.0 * 6.363996505737305
Epoch 600, val loss: 0.826630711555481
Epoch 610, training loss: 6.580382823944092 = 0.22266939282417297 + 1.0 * 6.357713222503662
Epoch 610, val loss: 0.8305245637893677
Epoch 620, training loss: 6.56710147857666 = 0.211528941988945 + 1.0 * 6.355572700500488
Epoch 620, val loss: 0.8347464799880981
Epoch 630, training loss: 6.556621551513672 = 0.2007695883512497 + 1.0 * 6.355852127075195
Epoch 630, val loss: 0.8392356634140015
Epoch 640, training loss: 6.549854755401611 = 0.19044537842273712 + 1.0 * 6.359409332275391
Epoch 640, val loss: 0.8435810208320618
Epoch 650, training loss: 6.533944129943848 = 0.18056342005729675 + 1.0 * 6.3533806800842285
Epoch 650, val loss: 0.848353385925293
Epoch 660, training loss: 6.519684791564941 = 0.1710471212863922 + 1.0 * 6.348637580871582
Epoch 660, val loss: 0.8533116579055786
Epoch 670, training loss: 6.509033203125 = 0.16187387704849243 + 1.0 * 6.347159385681152
Epoch 670, val loss: 0.8583084940910339
Epoch 680, training loss: 6.511549949645996 = 0.1530781090259552 + 1.0 * 6.358471870422363
Epoch 680, val loss: 0.8633770942687988
Epoch 690, training loss: 6.491933345794678 = 0.14469049870967865 + 1.0 * 6.347242832183838
Epoch 690, val loss: 0.8685163855552673
Epoch 700, training loss: 6.479487419128418 = 0.13671842217445374 + 1.0 * 6.342769145965576
Epoch 700, val loss: 0.8739209771156311
Epoch 710, training loss: 6.470049858093262 = 0.1291271597146988 + 1.0 * 6.340922832489014
Epoch 710, val loss: 0.8794499039649963
Epoch 720, training loss: 6.464791774749756 = 0.12192635238170624 + 1.0 * 6.342865467071533
Epoch 720, val loss: 0.8850497603416443
Epoch 730, training loss: 6.466998100280762 = 0.11516394466161728 + 1.0 * 6.351834297180176
Epoch 730, val loss: 0.8906600475311279
Epoch 740, training loss: 6.4500732421875 = 0.10886215418577194 + 1.0 * 6.341211318969727
Epoch 740, val loss: 0.8964352607727051
Epoch 750, training loss: 6.440713882446289 = 0.102932408452034 + 1.0 * 6.3377814292907715
Epoch 750, val loss: 0.9024033546447754
Epoch 760, training loss: 6.43311071395874 = 0.09735440462827682 + 1.0 * 6.335756301879883
Epoch 760, val loss: 0.908420741558075
Epoch 770, training loss: 6.432880401611328 = 0.09211479127407074 + 1.0 * 6.340765476226807
Epoch 770, val loss: 0.9145634770393372
Epoch 780, training loss: 6.421166896820068 = 0.08720025420188904 + 1.0 * 6.3339667320251465
Epoch 780, val loss: 0.9207492470741272
Epoch 790, training loss: 6.420869827270508 = 0.08260311186313629 + 1.0 * 6.338266849517822
Epoch 790, val loss: 0.9271044731140137
Epoch 800, training loss: 6.410165309906006 = 0.07831309735774994 + 1.0 * 6.331852436065674
Epoch 800, val loss: 0.9335280656814575
Epoch 810, training loss: 6.403768539428711 = 0.07429343461990356 + 1.0 * 6.329474925994873
Epoch 810, val loss: 0.9401133060455322
Epoch 820, training loss: 6.399551868438721 = 0.07051947712898254 + 1.0 * 6.3290324211120605
Epoch 820, val loss: 0.9467580914497375
Epoch 830, training loss: 6.402857780456543 = 0.06699436157941818 + 1.0 * 6.3358635902404785
Epoch 830, val loss: 0.9534264206886292
Epoch 840, training loss: 6.391046047210693 = 0.06371182948350906 + 1.0 * 6.327334403991699
Epoch 840, val loss: 0.960152268409729
Epoch 850, training loss: 6.39422607421875 = 0.0606524683535099 + 1.0 * 6.333573818206787
Epoch 850, val loss: 0.9670634865760803
Epoch 860, training loss: 6.383788108825684 = 0.057790838181972504 + 1.0 * 6.325997352600098
Epoch 860, val loss: 0.9737855195999146
Epoch 870, training loss: 6.3787407875061035 = 0.05510854348540306 + 1.0 * 6.32363224029541
Epoch 870, val loss: 0.9806993007659912
Epoch 880, training loss: 6.3826584815979 = 0.0525905005633831 + 1.0 * 6.330068111419678
Epoch 880, val loss: 0.9875514507293701
Epoch 890, training loss: 6.376664638519287 = 0.05024192854762077 + 1.0 * 6.326422691345215
Epoch 890, val loss: 0.9943838715553284
Epoch 900, training loss: 6.369380950927734 = 0.04804109036922455 + 1.0 * 6.321340084075928
Epoch 900, val loss: 1.0012691020965576
Epoch 910, training loss: 6.365161418914795 = 0.04597194492816925 + 1.0 * 6.319189548492432
Epoch 910, val loss: 1.008177638053894
Epoch 920, training loss: 6.369192600250244 = 0.044023819267749786 + 1.0 * 6.325168609619141
Epoch 920, val loss: 1.014978051185608
Epoch 930, training loss: 6.363571643829346 = 0.04221217334270477 + 1.0 * 6.321359634399414
Epoch 930, val loss: 1.0217095613479614
Epoch 940, training loss: 6.358851432800293 = 0.04050569236278534 + 1.0 * 6.318345546722412
Epoch 940, val loss: 1.0284863710403442
Epoch 950, training loss: 6.353975772857666 = 0.038897447288036346 + 1.0 * 6.315078258514404
Epoch 950, val loss: 1.0352532863616943
Epoch 960, training loss: 6.3522467613220215 = 0.037375517189502716 + 1.0 * 6.314871311187744
Epoch 960, val loss: 1.0419706106185913
Epoch 970, training loss: 6.356421947479248 = 0.03594294562935829 + 1.0 * 6.320478916168213
Epoch 970, val loss: 1.0484822988510132
Epoch 980, training loss: 6.347733020782471 = 0.03460130840539932 + 1.0 * 6.313131809234619
Epoch 980, val loss: 1.0550287961959839
Epoch 990, training loss: 6.348158359527588 = 0.03333243355154991 + 1.0 * 6.314826011657715
Epoch 990, val loss: 1.061563491821289
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 10.553200721740723 = 1.9563978910446167 + 1.0 * 8.596802711486816
Epoch 0, val loss: 1.959721565246582
Epoch 10, training loss: 10.54307746887207 = 1.9466108083724976 + 1.0 * 8.596467018127441
Epoch 10, val loss: 1.9505748748779297
Epoch 20, training loss: 10.52885913848877 = 1.9347734451293945 + 1.0 * 8.594085693359375
Epoch 20, val loss: 1.939051628112793
Epoch 30, training loss: 10.495594024658203 = 1.9185163974761963 + 1.0 * 8.577077865600586
Epoch 30, val loss: 1.9226497411727905
Epoch 40, training loss: 10.368270874023438 = 1.897688388824463 + 1.0 * 8.470582962036133
Epoch 40, val loss: 1.9020706415176392
Epoch 50, training loss: 9.87108325958252 = 1.8756699562072754 + 1.0 * 7.995413303375244
Epoch 50, val loss: 1.8805968761444092
Epoch 60, training loss: 9.355249404907227 = 1.8586794137954712 + 1.0 * 7.496569633483887
Epoch 60, val loss: 1.8649719953536987
Epoch 70, training loss: 9.050603866577148 = 1.8463294506072998 + 1.0 * 7.204274654388428
Epoch 70, val loss: 1.8530513048171997
Epoch 80, training loss: 8.838907241821289 = 1.8318002223968506 + 1.0 * 7.007106781005859
Epoch 80, val loss: 1.8391996622085571
Epoch 90, training loss: 8.711528778076172 = 1.8164647817611694 + 1.0 * 6.895063877105713
Epoch 90, val loss: 1.824627161026001
Epoch 100, training loss: 8.617344856262207 = 1.8015480041503906 + 1.0 * 6.815796852111816
Epoch 100, val loss: 1.8103430271148682
Epoch 110, training loss: 8.542232513427734 = 1.7875829935073853 + 1.0 * 6.7546491622924805
Epoch 110, val loss: 1.7972220182418823
Epoch 120, training loss: 8.481789588928223 = 1.7746031284332275 + 1.0 * 6.707186698913574
Epoch 120, val loss: 1.7849476337432861
Epoch 130, training loss: 8.431648254394531 = 1.7613452672958374 + 1.0 * 6.6703033447265625
Epoch 130, val loss: 1.772889256477356
Epoch 140, training loss: 8.3898344039917 = 1.7470682859420776 + 1.0 * 6.642765998840332
Epoch 140, val loss: 1.7604748010635376
Epoch 150, training loss: 8.348721504211426 = 1.7312390804290771 + 1.0 * 6.6174821853637695
Epoch 150, val loss: 1.7473316192626953
Epoch 160, training loss: 8.309995651245117 = 1.713289737701416 + 1.0 * 6.596705436706543
Epoch 160, val loss: 1.7328611612319946
Epoch 170, training loss: 8.273361206054688 = 1.6926047801971436 + 1.0 * 6.580756187438965
Epoch 170, val loss: 1.7164673805236816
Epoch 180, training loss: 8.233394622802734 = 1.668805718421936 + 1.0 * 6.56458854675293
Epoch 180, val loss: 1.6977838277816772
Epoch 190, training loss: 8.191611289978027 = 1.6413562297821045 + 1.0 * 6.550254821777344
Epoch 190, val loss: 1.6762279272079468
Epoch 200, training loss: 8.14754581451416 = 1.6095763444900513 + 1.0 * 6.53796911239624
Epoch 200, val loss: 1.6513162851333618
Epoch 210, training loss: 8.101316452026367 = 1.5731662511825562 + 1.0 * 6.5281500816345215
Epoch 210, val loss: 1.622778058052063
Epoch 220, training loss: 8.049981117248535 = 1.5322462320327759 + 1.0 * 6.517735004425049
Epoch 220, val loss: 1.5907306671142578
Epoch 230, training loss: 7.99456262588501 = 1.486603856086731 + 1.0 * 6.507958889007568
Epoch 230, val loss: 1.555046558380127
Epoch 240, training loss: 7.938332557678223 = 1.4365031719207764 + 1.0 * 6.501829624176025
Epoch 240, val loss: 1.5162787437438965
Epoch 250, training loss: 7.875884056091309 = 1.3836742639541626 + 1.0 * 6.4922099113464355
Epoch 250, val loss: 1.4755003452301025
Epoch 260, training loss: 7.817442893981934 = 1.3294141292572021 + 1.0 * 6.488028526306152
Epoch 260, val loss: 1.434402585029602
Epoch 270, training loss: 7.758360862731934 = 1.2763113975524902 + 1.0 * 6.482049465179443
Epoch 270, val loss: 1.3938994407653809
Epoch 280, training loss: 7.69798469543457 = 1.2237187623977661 + 1.0 * 6.474266052246094
Epoch 280, val loss: 1.3544890880584717
Epoch 290, training loss: 7.64009952545166 = 1.1724295616149902 + 1.0 * 6.46766996383667
Epoch 290, val loss: 1.3163070678710938
Epoch 300, training loss: 7.584700584411621 = 1.1227880716323853 + 1.0 * 6.461912631988525
Epoch 300, val loss: 1.279768466949463
Epoch 310, training loss: 7.5329155921936035 = 1.0756983757019043 + 1.0 * 6.457217216491699
Epoch 310, val loss: 1.2458014488220215
Epoch 320, training loss: 7.486043930053711 = 1.0320205688476562 + 1.0 * 6.454023361206055
Epoch 320, val loss: 1.214587688446045
Epoch 330, training loss: 7.438536643981934 = 0.9906107187271118 + 1.0 * 6.447926044464111
Epoch 330, val loss: 1.1855478286743164
Epoch 340, training loss: 7.397592544555664 = 0.9506801962852478 + 1.0 * 6.4469122886657715
Epoch 340, val loss: 1.1578621864318848
Epoch 350, training loss: 7.3508687019348145 = 0.912011444568634 + 1.0 * 6.438857078552246
Epoch 350, val loss: 1.131260633468628
Epoch 360, training loss: 7.309901237487793 = 0.8739645481109619 + 1.0 * 6.435936450958252
Epoch 360, val loss: 1.1053904294967651
Epoch 370, training loss: 7.272328853607178 = 0.8366219401359558 + 1.0 * 6.435707092285156
Epoch 370, val loss: 1.080141305923462
Epoch 380, training loss: 7.228721618652344 = 0.7996991872787476 + 1.0 * 6.429022312164307
Epoch 380, val loss: 1.0555306673049927
Epoch 390, training loss: 7.187234401702881 = 0.7630602717399597 + 1.0 * 6.4241743087768555
Epoch 390, val loss: 1.031480312347412
Epoch 400, training loss: 7.151310443878174 = 0.7267899513244629 + 1.0 * 6.424520492553711
Epoch 400, val loss: 1.008143424987793
Epoch 410, training loss: 7.110264778137207 = 0.6916934847831726 + 1.0 * 6.418571472167969
Epoch 410, val loss: 0.9860501885414124
Epoch 420, training loss: 7.072964668273926 = 0.6579568386077881 + 1.0 * 6.415008068084717
Epoch 420, val loss: 0.9656287431716919
Epoch 430, training loss: 7.039214134216309 = 0.6256482601165771 + 1.0 * 6.4135661125183105
Epoch 430, val loss: 0.9469790458679199
Epoch 440, training loss: 7.010221481323242 = 0.5950381755828857 + 1.0 * 6.415183067321777
Epoch 440, val loss: 0.9304652810096741
Epoch 450, training loss: 6.974333763122559 = 0.5664422512054443 + 1.0 * 6.407891273498535
Epoch 450, val loss: 0.9160822033882141
Epoch 460, training loss: 6.943703651428223 = 0.5393854379653931 + 1.0 * 6.404318332672119
Epoch 460, val loss: 0.9035943150520325
Epoch 470, training loss: 6.916122913360596 = 0.5137691497802734 + 1.0 * 6.402353763580322
Epoch 470, val loss: 0.8927691578865051
Epoch 480, training loss: 6.8956146240234375 = 0.48954081535339355 + 1.0 * 6.406074047088623
Epoch 480, val loss: 0.8836123943328857
Epoch 490, training loss: 6.864013671875 = 0.4666917324066162 + 1.0 * 6.397322177886963
Epoch 490, val loss: 0.8758736252784729
Epoch 500, training loss: 6.837522983551025 = 0.4449149966239929 + 1.0 * 6.392608165740967
Epoch 500, val loss: 0.8693568110466003
Epoch 510, training loss: 6.814885139465332 = 0.4240216910839081 + 1.0 * 6.390863418579102
Epoch 510, val loss: 0.8638793230056763
Epoch 520, training loss: 6.796308994293213 = 0.404013454914093 + 1.0 * 6.3922953605651855
Epoch 520, val loss: 0.8594211935997009
Epoch 530, training loss: 6.773816108703613 = 0.38495704531669617 + 1.0 * 6.388859272003174
Epoch 530, val loss: 0.8559024930000305
Epoch 540, training loss: 6.751326560974121 = 0.36663535237312317 + 1.0 * 6.38469123840332
Epoch 540, val loss: 0.8532218933105469
Epoch 550, training loss: 6.732736110687256 = 0.34899619221687317 + 1.0 * 6.383739948272705
Epoch 550, val loss: 0.8512724041938782
Epoch 560, training loss: 6.714374542236328 = 0.33202308416366577 + 1.0 * 6.382351398468018
Epoch 560, val loss: 0.8500834107398987
Epoch 570, training loss: 6.695183753967285 = 0.3157622218132019 + 1.0 * 6.379421710968018
Epoch 570, val loss: 0.8495084643363953
Epoch 580, training loss: 6.676321029663086 = 0.30017009377479553 + 1.0 * 6.376151084899902
Epoch 580, val loss: 0.8496367335319519
Epoch 590, training loss: 6.659653663635254 = 0.28520652651786804 + 1.0 * 6.374447345733643
Epoch 590, val loss: 0.8503206968307495
Epoch 600, training loss: 6.655183792114258 = 0.27085399627685547 + 1.0 * 6.384329795837402
Epoch 600, val loss: 0.8515447378158569
Epoch 610, training loss: 6.627386093139648 = 0.25717389583587646 + 1.0 * 6.370212078094482
Epoch 610, val loss: 0.8532978892326355
Epoch 620, training loss: 6.613120079040527 = 0.24410852789878845 + 1.0 * 6.369011402130127
Epoch 620, val loss: 0.8555000424385071
Epoch 630, training loss: 6.598292827606201 = 0.23158466815948486 + 1.0 * 6.366708278656006
Epoch 630, val loss: 0.8581139445304871
Epoch 640, training loss: 6.605896472930908 = 0.2196405827999115 + 1.0 * 6.386255741119385
Epoch 640, val loss: 0.8611241579055786
Epoch 650, training loss: 6.575107574462891 = 0.2083110213279724 + 1.0 * 6.366796493530273
Epoch 650, val loss: 0.8645246624946594
Epoch 660, training loss: 6.5600690841674805 = 0.19752052426338196 + 1.0 * 6.362548351287842
Epoch 660, val loss: 0.8681938648223877
Epoch 670, training loss: 6.54733419418335 = 0.18719469010829926 + 1.0 * 6.3601393699646
Epoch 670, val loss: 0.8721029758453369
Epoch 680, training loss: 6.535892009735107 = 0.17731057107448578 + 1.0 * 6.35858154296875
Epoch 680, val loss: 0.8763355016708374
Epoch 690, training loss: 6.5279388427734375 = 0.16788417100906372 + 1.0 * 6.3600544929504395
Epoch 690, val loss: 0.8807625770568848
Epoch 700, training loss: 6.518714904785156 = 0.15896561741828918 + 1.0 * 6.3597493171691895
Epoch 700, val loss: 0.8853881359100342
Epoch 710, training loss: 6.5051069259643555 = 0.15052972733974457 + 1.0 * 6.35457706451416
Epoch 710, val loss: 0.8901537656784058
Epoch 720, training loss: 6.497102737426758 = 0.14255672693252563 + 1.0 * 6.354546070098877
Epoch 720, val loss: 0.8950530290603638
Epoch 730, training loss: 6.486501216888428 = 0.1349908709526062 + 1.0 * 6.351510524749756
Epoch 730, val loss: 0.900166928768158
Epoch 740, training loss: 6.477495193481445 = 0.1278219372034073 + 1.0 * 6.349673271179199
Epoch 740, val loss: 0.9054489731788635
Epoch 750, training loss: 6.473829746246338 = 0.12103825807571411 + 1.0 * 6.3527913093566895
Epoch 750, val loss: 0.9109175205230713
Epoch 760, training loss: 6.472044944763184 = 0.11469440907239914 + 1.0 * 6.3573503494262695
Epoch 760, val loss: 0.9164629578590393
Epoch 770, training loss: 6.456073760986328 = 0.10875469446182251 + 1.0 * 6.34731912612915
Epoch 770, val loss: 0.9221277236938477
Epoch 780, training loss: 6.447391510009766 = 0.10317689180374146 + 1.0 * 6.34421443939209
Epoch 780, val loss: 0.9278651475906372
Epoch 790, training loss: 6.441296100616455 = 0.09793554991483688 + 1.0 * 6.343360424041748
Epoch 790, val loss: 0.9337774515151978
Epoch 800, training loss: 6.436276912689209 = 0.09300920367240906 + 1.0 * 6.343267917633057
Epoch 800, val loss: 0.9398066401481628
Epoch 810, training loss: 6.4307122230529785 = 0.08840134739875793 + 1.0 * 6.342310905456543
Epoch 810, val loss: 0.945813775062561
Epoch 820, training loss: 6.4274001121521 = 0.08410459011793137 + 1.0 * 6.343295574188232
Epoch 820, val loss: 0.9518824815750122
Epoch 830, training loss: 6.419865131378174 = 0.08008883148431778 + 1.0 * 6.339776515960693
Epoch 830, val loss: 0.9579702019691467
Epoch 840, training loss: 6.413748264312744 = 0.07632086426019669 + 1.0 * 6.337427616119385
Epoch 840, val loss: 0.9641183614730835
Epoch 850, training loss: 6.409607887268066 = 0.0727747306227684 + 1.0 * 6.3368330001831055
Epoch 850, val loss: 0.9702813029289246
Epoch 860, training loss: 6.410504341125488 = 0.06944326311349869 + 1.0 * 6.341061115264893
Epoch 860, val loss: 0.9764605760574341
Epoch 870, training loss: 6.39970588684082 = 0.06633343547582626 + 1.0 * 6.333372592926025
Epoch 870, val loss: 0.9826523661613464
Epoch 880, training loss: 6.39726448059082 = 0.06341437995433807 + 1.0 * 6.333849906921387
Epoch 880, val loss: 0.9887938499450684
Epoch 890, training loss: 6.392104625701904 = 0.060658764094114304 + 1.0 * 6.331445693969727
Epoch 890, val loss: 0.9949706196784973
Epoch 900, training loss: 6.390482425689697 = 0.05806498974561691 + 1.0 * 6.3324174880981445
Epoch 900, val loss: 1.0011146068572998
Epoch 910, training loss: 6.38722038269043 = 0.05562388524413109 + 1.0 * 6.331596374511719
Epoch 910, val loss: 1.0072650909423828
Epoch 920, training loss: 6.387680530548096 = 0.053335536271333694 + 1.0 * 6.334344863891602
Epoch 920, val loss: 1.0132734775543213
Epoch 930, training loss: 6.38134241104126 = 0.05119195207953453 + 1.0 * 6.330150604248047
Epoch 930, val loss: 1.0192245244979858
Epoch 940, training loss: 6.376370906829834 = 0.04916344955563545 + 1.0 * 6.327207565307617
Epoch 940, val loss: 1.025159239768982
Epoch 950, training loss: 6.373427867889404 = 0.04724735766649246 + 1.0 * 6.326180458068848
Epoch 950, val loss: 1.031105399131775
Epoch 960, training loss: 6.375514507293701 = 0.045432012528181076 + 1.0 * 6.330082416534424
Epoch 960, val loss: 1.036985993385315
Epoch 970, training loss: 6.372233867645264 = 0.043716639280319214 + 1.0 * 6.328517436981201
Epoch 970, val loss: 1.0427627563476562
Epoch 980, training loss: 6.36624002456665 = 0.04210079461336136 + 1.0 * 6.32413911819458
Epoch 980, val loss: 1.0484787225723267
Epoch 990, training loss: 6.362323760986328 = 0.0405660942196846 + 1.0 * 6.321757793426514
Epoch 990, val loss: 1.0541852712631226
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 10.543231010437012 = 1.9464023113250732 + 1.0 * 8.59682846069336
Epoch 0, val loss: 1.9460550546646118
Epoch 10, training loss: 10.532305717468262 = 1.9357483386993408 + 1.0 * 8.5965576171875
Epoch 10, val loss: 1.9360073804855347
Epoch 20, training loss: 10.51723861694336 = 1.9225997924804688 + 1.0 * 8.59463882446289
Epoch 20, val loss: 1.9231257438659668
Epoch 30, training loss: 10.484124183654785 = 1.9042235612869263 + 1.0 * 8.579900741577148
Epoch 30, val loss: 1.9046576023101807
Epoch 40, training loss: 10.369110107421875 = 1.8801305294036865 + 1.0 * 8.48897933959961
Epoch 40, val loss: 1.8813928365707397
Epoch 50, training loss: 10.037665367126465 = 1.8556777238845825 + 1.0 * 8.181987762451172
Epoch 50, val loss: 1.8584389686584473
Epoch 60, training loss: 9.711712837219238 = 1.836073398590088 + 1.0 * 7.87563943862915
Epoch 60, val loss: 1.8400492668151855
Epoch 70, training loss: 9.31749439239502 = 1.8209465742111206 + 1.0 * 7.496548175811768
Epoch 70, val loss: 1.8270124197006226
Epoch 80, training loss: 9.051860809326172 = 1.8107576370239258 + 1.0 * 7.241102695465088
Epoch 80, val loss: 1.818323016166687
Epoch 90, training loss: 8.836564064025879 = 1.7968121767044067 + 1.0 * 7.039752006530762
Epoch 90, val loss: 1.8061609268188477
Epoch 100, training loss: 8.6702241897583 = 1.7852109670639038 + 1.0 * 6.885013580322266
Epoch 100, val loss: 1.7958101034164429
Epoch 110, training loss: 8.564590454101562 = 1.7750312089920044 + 1.0 * 6.789559364318848
Epoch 110, val loss: 1.7860640287399292
Epoch 120, training loss: 8.491019248962402 = 1.763351559638977 + 1.0 * 6.727667331695557
Epoch 120, val loss: 1.7749629020690918
Epoch 130, training loss: 8.430771827697754 = 1.7502018213272095 + 1.0 * 6.680570125579834
Epoch 130, val loss: 1.7630815505981445
Epoch 140, training loss: 8.383641242980957 = 1.7356916666030884 + 1.0 * 6.647949695587158
Epoch 140, val loss: 1.75054931640625
Epoch 150, training loss: 8.334675788879395 = 1.7193046808242798 + 1.0 * 6.615371227264404
Epoch 150, val loss: 1.736739158630371
Epoch 160, training loss: 8.291765213012695 = 1.7005774974822998 + 1.0 * 6.591187953948975
Epoch 160, val loss: 1.7212607860565186
Epoch 170, training loss: 8.251688957214355 = 1.678977131843567 + 1.0 * 6.572711944580078
Epoch 170, val loss: 1.7034331560134888
Epoch 180, training loss: 8.20715045928955 = 1.654228925704956 + 1.0 * 6.552921772003174
Epoch 180, val loss: 1.6830873489379883
Epoch 190, training loss: 8.164094924926758 = 1.6260771751403809 + 1.0 * 6.538018226623535
Epoch 190, val loss: 1.6599647998809814
Epoch 200, training loss: 8.118818283081055 = 1.594098448753357 + 1.0 * 6.524719715118408
Epoch 200, val loss: 1.6337615251541138
Epoch 210, training loss: 8.073532104492188 = 1.5585819482803345 + 1.0 * 6.514949798583984
Epoch 210, val loss: 1.6048310995101929
Epoch 220, training loss: 8.022418975830078 = 1.5202175378799438 + 1.0 * 6.502201080322266
Epoch 220, val loss: 1.5737882852554321
Epoch 230, training loss: 7.970797538757324 = 1.479128122329712 + 1.0 * 6.491669654846191
Epoch 230, val loss: 1.5407766103744507
Epoch 240, training loss: 7.921032905578613 = 1.4358083009719849 + 1.0 * 6.485224723815918
Epoch 240, val loss: 1.5063093900680542
Epoch 250, training loss: 7.8672637939453125 = 1.391826868057251 + 1.0 * 6.475437164306641
Epoch 250, val loss: 1.4718739986419678
Epoch 260, training loss: 7.816105842590332 = 1.3475494384765625 + 1.0 * 6.4685564041137695
Epoch 260, val loss: 1.4374974966049194
Epoch 270, training loss: 7.771051406860352 = 1.3031895160675049 + 1.0 * 6.467862129211426
Epoch 270, val loss: 1.4035053253173828
Epoch 280, training loss: 7.717430114746094 = 1.2597630023956299 + 1.0 * 6.457667350769043
Epoch 280, val loss: 1.3710606098175049
Epoch 290, training loss: 7.66672945022583 = 1.2172455787658691 + 1.0 * 6.449483871459961
Epoch 290, val loss: 1.3398160934448242
Epoch 300, training loss: 7.625446319580078 = 1.1757243871688843 + 1.0 * 6.449721813201904
Epoch 300, val loss: 1.309761643409729
Epoch 310, training loss: 7.574071884155273 = 1.1359174251556396 + 1.0 * 6.438154220581055
Epoch 310, val loss: 1.2814245223999023
Epoch 320, training loss: 7.531319618225098 = 1.0977580547332764 + 1.0 * 6.4335618019104
Epoch 320, val loss: 1.2545983791351318
Epoch 330, training loss: 7.495114803314209 = 1.0616110563278198 + 1.0 * 6.4335036277771
Epoch 330, val loss: 1.2295705080032349
Epoch 340, training loss: 7.452238082885742 = 1.0277973413467407 + 1.0 * 6.424440860748291
Epoch 340, val loss: 1.2062846422195435
Epoch 350, training loss: 7.414694786071777 = 0.9955509901046753 + 1.0 * 6.4191436767578125
Epoch 350, val loss: 1.1845407485961914
Epoch 360, training loss: 7.379580974578857 = 0.9644412994384766 + 1.0 * 6.415139675140381
Epoch 360, val loss: 1.1638147830963135
Epoch 370, training loss: 7.345727920532227 = 0.9342387914657593 + 1.0 * 6.411489009857178
Epoch 370, val loss: 1.1439357995986938
Epoch 380, training loss: 7.312195301055908 = 0.9048541188240051 + 1.0 * 6.407341003417969
Epoch 380, val loss: 1.1250568628311157
Epoch 390, training loss: 7.279826641082764 = 0.8755465745925903 + 1.0 * 6.404280185699463
Epoch 390, val loss: 1.106533408164978
Epoch 400, training loss: 7.250061511993408 = 0.8460951447486877 + 1.0 * 6.403966426849365
Epoch 400, val loss: 1.088127613067627
Epoch 410, training loss: 7.21388578414917 = 0.8164276480674744 + 1.0 * 6.397458076477051
Epoch 410, val loss: 1.069937825202942
Epoch 420, training loss: 7.180945873260498 = 0.7863274812698364 + 1.0 * 6.394618511199951
Epoch 420, val loss: 1.0518473386764526
Epoch 430, training loss: 7.157573699951172 = 0.7559263706207275 + 1.0 * 6.401647090911865
Epoch 430, val loss: 1.0337927341461182
Epoch 440, training loss: 7.117673873901367 = 0.7257890701293945 + 1.0 * 6.391884803771973
Epoch 440, val loss: 1.016528606414795
Epoch 450, training loss: 7.08465051651001 = 0.6958867311477661 + 1.0 * 6.388763904571533
Epoch 450, val loss: 0.9999330639839172
Epoch 460, training loss: 7.051347732543945 = 0.6663359999656677 + 1.0 * 6.385011672973633
Epoch 460, val loss: 0.9839200377464294
Epoch 470, training loss: 7.024991035461426 = 0.6374375820159912 + 1.0 * 6.387553691864014
Epoch 470, val loss: 0.9689033627510071
Epoch 480, training loss: 6.992241382598877 = 0.6094809174537659 + 1.0 * 6.382760524749756
Epoch 480, val loss: 0.9554217457771301
Epoch 490, training loss: 6.961119651794434 = 0.5824118852615356 + 1.0 * 6.3787078857421875
Epoch 490, val loss: 0.9431447386741638
Epoch 500, training loss: 6.935064315795898 = 0.5562731623649597 + 1.0 * 6.378791332244873
Epoch 500, val loss: 0.9321718215942383
Epoch 510, training loss: 6.909008979797363 = 0.5311450362205505 + 1.0 * 6.377863883972168
Epoch 510, val loss: 0.9226236939430237
Epoch 520, training loss: 6.880180358886719 = 0.5070953369140625 + 1.0 * 6.373085021972656
Epoch 520, val loss: 0.9143857359886169
Epoch 530, training loss: 6.858964920043945 = 0.4839954674243927 + 1.0 * 6.374969482421875
Epoch 530, val loss: 0.9075061082839966
Epoch 540, training loss: 6.831291675567627 = 0.4617255628108978 + 1.0 * 6.369565963745117
Epoch 540, val loss: 0.9017160534858704
Epoch 550, training loss: 6.806994438171387 = 0.44018131494522095 + 1.0 * 6.3668131828308105
Epoch 550, val loss: 0.8970181941986084
Epoch 560, training loss: 6.790325164794922 = 0.4192647337913513 + 1.0 * 6.371060371398926
Epoch 560, val loss: 0.8931570649147034
Epoch 570, training loss: 6.767122745513916 = 0.3990107774734497 + 1.0 * 6.368112087249756
Epoch 570, val loss: 0.8903945088386536
Epoch 580, training loss: 6.741329193115234 = 0.3794516324996948 + 1.0 * 6.36187744140625
Epoch 580, val loss: 0.8885393142700195
Epoch 590, training loss: 6.721238136291504 = 0.3604670763015747 + 1.0 * 6.360771179199219
Epoch 590, val loss: 0.8875471353530884
Epoch 600, training loss: 6.706511497497559 = 0.3420610725879669 + 1.0 * 6.364450454711914
Epoch 600, val loss: 0.8873813152313232
Epoch 610, training loss: 6.682383060455322 = 0.3243056535720825 + 1.0 * 6.358077526092529
Epoch 610, val loss: 0.8881957530975342
Epoch 620, training loss: 6.672175884246826 = 0.30716240406036377 + 1.0 * 6.365013599395752
Epoch 620, val loss: 0.8895835876464844
Epoch 630, training loss: 6.64580774307251 = 0.290688157081604 + 1.0 * 6.355119705200195
Epoch 630, val loss: 0.8917346596717834
Epoch 640, training loss: 6.629045009613037 = 0.27488061785697937 + 1.0 * 6.3541646003723145
Epoch 640, val loss: 0.8946962356567383
Epoch 650, training loss: 6.613094806671143 = 0.25973954796791077 + 1.0 * 6.353355407714844
Epoch 650, val loss: 0.8982588052749634
Epoch 660, training loss: 6.594764709472656 = 0.2452353686094284 + 1.0 * 6.349529266357422
Epoch 660, val loss: 0.9024534225463867
Epoch 670, training loss: 6.582874774932861 = 0.2313964068889618 + 1.0 * 6.351478576660156
Epoch 670, val loss: 0.9072147607803345
Epoch 680, training loss: 6.567673683166504 = 0.21826842427253723 + 1.0 * 6.349405288696289
Epoch 680, val loss: 0.9125785827636719
Epoch 690, training loss: 6.557214260101318 = 0.20582708716392517 + 1.0 * 6.351387023925781
Epoch 690, val loss: 0.918383002281189
Epoch 700, training loss: 6.5389227867126465 = 0.1940542459487915 + 1.0 * 6.3448686599731445
Epoch 700, val loss: 0.9245939254760742
Epoch 710, training loss: 6.526168346405029 = 0.18291500210762024 + 1.0 * 6.343253135681152
Epoch 710, val loss: 0.9312846064567566
Epoch 720, training loss: 6.518289566040039 = 0.17239615321159363 + 1.0 * 6.345893383026123
Epoch 720, val loss: 0.9382337331771851
Epoch 730, training loss: 6.504138946533203 = 0.16249702870845795 + 1.0 * 6.341641902923584
Epoch 730, val loss: 0.9454882144927979
Epoch 740, training loss: 6.491918087005615 = 0.15320412814617157 + 1.0 * 6.338714122772217
Epoch 740, val loss: 0.9531406164169312
Epoch 750, training loss: 6.4839186668396 = 0.14446893334388733 + 1.0 * 6.339449882507324
Epoch 750, val loss: 0.9609816670417786
Epoch 760, training loss: 6.473809242248535 = 0.13630437850952148 + 1.0 * 6.337504863739014
Epoch 760, val loss: 0.9688748121261597
Epoch 770, training loss: 6.466981887817383 = 0.12867756187915802 + 1.0 * 6.33830451965332
Epoch 770, val loss: 0.9770902395248413
Epoch 780, training loss: 6.456511974334717 = 0.1215592697262764 + 1.0 * 6.3349528312683105
Epoch 780, val loss: 0.9853904247283936
Epoch 790, training loss: 6.44895076751709 = 0.11490175873041153 + 1.0 * 6.334049224853516
Epoch 790, val loss: 0.9938331842422485
Epoch 800, training loss: 6.443918228149414 = 0.108683280646801 + 1.0 * 6.335235118865967
Epoch 800, val loss: 1.0022732019424438
Epoch 810, training loss: 6.434049606323242 = 0.10288646817207336 + 1.0 * 6.331162929534912
Epoch 810, val loss: 1.0109751224517822
Epoch 820, training loss: 6.432044506072998 = 0.09747961163520813 + 1.0 * 6.334564685821533
Epoch 820, val loss: 1.019637942314148
Epoch 830, training loss: 6.422537803649902 = 0.09243569523096085 + 1.0 * 6.33010196685791
Epoch 830, val loss: 1.0282260179519653
Epoch 840, training loss: 6.414404392242432 = 0.08774236589670181 + 1.0 * 6.326662063598633
Epoch 840, val loss: 1.0369642972946167
Epoch 850, training loss: 6.40899133682251 = 0.0833556279540062 + 1.0 * 6.32563591003418
Epoch 850, val loss: 1.0456910133361816
Epoch 860, training loss: 6.421043872833252 = 0.0792653039097786 + 1.0 * 6.341778755187988
Epoch 860, val loss: 1.0541776418685913
Epoch 870, training loss: 6.400270938873291 = 0.07545788586139679 + 1.0 * 6.324812889099121
Epoch 870, val loss: 1.062721610069275
Epoch 880, training loss: 6.395168304443359 = 0.07190090417861938 + 1.0 * 6.323267459869385
Epoch 880, val loss: 1.0712885856628418
Epoch 890, training loss: 6.389935493469238 = 0.06856562197208405 + 1.0 * 6.321369647979736
Epoch 890, val loss: 1.0797706842422485
Epoch 900, training loss: 6.397356033325195 = 0.06543868035078049 + 1.0 * 6.3319172859191895
Epoch 900, val loss: 1.0881268978118896
Epoch 910, training loss: 6.385091781616211 = 0.06250614672899246 + 1.0 * 6.322585582733154
Epoch 910, val loss: 1.096349835395813
Epoch 920, training loss: 6.379727363586426 = 0.05976095050573349 + 1.0 * 6.3199663162231445
Epoch 920, val loss: 1.1046303510665894
Epoch 930, training loss: 6.377096176147461 = 0.057179100811481476 + 1.0 * 6.31991720199585
Epoch 930, val loss: 1.112817406654358
Epoch 940, training loss: 6.372435092926025 = 0.05474792793393135 + 1.0 * 6.317687034606934
Epoch 940, val loss: 1.1208913326263428
Epoch 950, training loss: 6.3720550537109375 = 0.052457045763731 + 1.0 * 6.319598197937012
Epoch 950, val loss: 1.1288766860961914
Epoch 960, training loss: 6.369569778442383 = 0.05030153691768646 + 1.0 * 6.319268226623535
Epoch 960, val loss: 1.136876106262207
Epoch 970, training loss: 6.3651347160339355 = 0.04826696962118149 + 1.0 * 6.316867828369141
Epoch 970, val loss: 1.1445519924163818
Epoch 980, training loss: 6.364254951477051 = 0.04635268449783325 + 1.0 * 6.317902088165283
Epoch 980, val loss: 1.1523627042770386
Epoch 990, training loss: 6.356949329376221 = 0.04454219713807106 + 1.0 * 6.31240701675415
Epoch 990, val loss: 1.159946084022522
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8112809699525567
The final CL Acc:0.74321, 0.00972, The final GNN Acc:0.81093, 0.00090
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13250])
remove edge: torch.Size([2, 7838])
updated graph: torch.Size([2, 10532])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.5511474609375 = 1.9543161392211914 + 1.0 * 8.596831321716309
Epoch 0, val loss: 1.95657479763031
Epoch 10, training loss: 10.540364265441895 = 1.9437941312789917 + 1.0 * 8.596570014953613
Epoch 10, val loss: 1.9458597898483276
Epoch 20, training loss: 10.52475357055664 = 1.9299882650375366 + 1.0 * 8.594765663146973
Epoch 20, val loss: 1.9314825534820557
Epoch 30, training loss: 10.490914344787598 = 1.9098764657974243 + 1.0 * 8.581037521362305
Epoch 30, val loss: 1.9103777408599854
Epoch 40, training loss: 10.37552547454834 = 1.882293701171875 + 1.0 * 8.493231773376465
Epoch 40, val loss: 1.8827290534973145
Epoch 50, training loss: 9.920310020446777 = 1.8537365198135376 + 1.0 * 8.066573143005371
Epoch 50, val loss: 1.8559080362319946
Epoch 60, training loss: 9.50171947479248 = 1.8297046422958374 + 1.0 * 7.6720147132873535
Epoch 60, val loss: 1.8338121175765991
Epoch 70, training loss: 9.15821647644043 = 1.8129591941833496 + 1.0 * 7.345257759094238
Epoch 70, val loss: 1.8178900480270386
Epoch 80, training loss: 8.91454029083252 = 1.7948657274246216 + 1.0 * 7.119674205780029
Epoch 80, val loss: 1.8011384010314941
Epoch 90, training loss: 8.751458168029785 = 1.7756084203720093 + 1.0 * 6.975849628448486
Epoch 90, val loss: 1.7838457822799683
Epoch 100, training loss: 8.623425483703613 = 1.757063627243042 + 1.0 * 6.866361618041992
Epoch 100, val loss: 1.7671781778335571
Epoch 110, training loss: 8.518693923950195 = 1.7392746210098267 + 1.0 * 6.779419422149658
Epoch 110, val loss: 1.751093864440918
Epoch 120, training loss: 8.436288833618164 = 1.7205979824066162 + 1.0 * 6.715691089630127
Epoch 120, val loss: 1.7340221405029297
Epoch 130, training loss: 8.365167617797852 = 1.699491262435913 + 1.0 * 6.665676593780518
Epoch 130, val loss: 1.7153234481811523
Epoch 140, training loss: 8.30361557006836 = 1.6751761436462402 + 1.0 * 6.628438949584961
Epoch 140, val loss: 1.6941527128219604
Epoch 150, training loss: 8.247611999511719 = 1.6469857692718506 + 1.0 * 6.600625991821289
Epoch 150, val loss: 1.6699223518371582
Epoch 160, training loss: 8.190680503845215 = 1.6146680116653442 + 1.0 * 6.576012134552002
Epoch 160, val loss: 1.6420323848724365
Epoch 170, training loss: 8.136414527893066 = 1.5780047178268433 + 1.0 * 6.558409690856934
Epoch 170, val loss: 1.6103838682174683
Epoch 180, training loss: 8.076644897460938 = 1.5375187397003174 + 1.0 * 6.539125919342041
Epoch 180, val loss: 1.5757046937942505
Epoch 190, training loss: 8.017317771911621 = 1.4936765432357788 + 1.0 * 6.523641586303711
Epoch 190, val loss: 1.5382918119430542
Epoch 200, training loss: 7.962558269500732 = 1.4474239349365234 + 1.0 * 6.515134334564209
Epoch 200, val loss: 1.4991952180862427
Epoch 210, training loss: 7.901099681854248 = 1.4009160995483398 + 1.0 * 6.500183582305908
Epoch 210, val loss: 1.4605488777160645
Epoch 220, training loss: 7.844605922698975 = 1.3542968034744263 + 1.0 * 6.490309238433838
Epoch 220, val loss: 1.4221727848052979
Epoch 230, training loss: 7.788999557495117 = 1.307489275932312 + 1.0 * 6.481510162353516
Epoch 230, val loss: 1.3839681148529053
Epoch 240, training loss: 7.735240936279297 = 1.2616498470306396 + 1.0 * 6.473590850830078
Epoch 240, val loss: 1.3470875024795532
Epoch 250, training loss: 7.682729721069336 = 1.2166355848312378 + 1.0 * 6.466094017028809
Epoch 250, val loss: 1.3110060691833496
Epoch 260, training loss: 7.630366802215576 = 1.1714965105056763 + 1.0 * 6.4588704109191895
Epoch 260, val loss: 1.275104284286499
Epoch 270, training loss: 7.577648639678955 = 1.1255184412002563 + 1.0 * 6.452130317687988
Epoch 270, val loss: 1.2386233806610107
Epoch 280, training loss: 7.531032562255859 = 1.07893705368042 + 1.0 * 6.4520955085754395
Epoch 280, val loss: 1.201751708984375
Epoch 290, training loss: 7.472765922546387 = 1.0321433544158936 + 1.0 * 6.440622806549072
Epoch 290, val loss: 1.164758324623108
Epoch 300, training loss: 7.421825885772705 = 0.9847030639648438 + 1.0 * 6.437122821807861
Epoch 300, val loss: 1.1271471977233887
Epoch 310, training loss: 7.371655464172363 = 0.937298595905304 + 1.0 * 6.434356689453125
Epoch 310, val loss: 1.0895957946777344
Epoch 320, training loss: 7.317437648773193 = 0.8908594250679016 + 1.0 * 6.426578044891357
Epoch 320, val loss: 1.0526645183563232
Epoch 330, training loss: 7.270130157470703 = 0.8455967307090759 + 1.0 * 6.424533367156982
Epoch 330, val loss: 1.0167449712753296
Epoch 340, training loss: 7.221921920776367 = 0.8024051785469055 + 1.0 * 6.419516563415527
Epoch 340, val loss: 0.9826110601425171
Epoch 350, training loss: 7.177807807922363 = 0.7617116570472717 + 1.0 * 6.416096210479736
Epoch 350, val loss: 0.9508920311927795
Epoch 360, training loss: 7.133808612823486 = 0.7237510085105896 + 1.0 * 6.410057544708252
Epoch 360, val loss: 0.9218019247055054
Epoch 370, training loss: 7.09863805770874 = 0.6882061958312988 + 1.0 * 6.410431861877441
Epoch 370, val loss: 0.8952761888504028
Epoch 380, training loss: 7.059255123138428 = 0.6551917195320129 + 1.0 * 6.4040632247924805
Epoch 380, val loss: 0.8713821768760681
Epoch 390, training loss: 7.023125171661377 = 0.6242015957832336 + 1.0 * 6.398923397064209
Epoch 390, val loss: 0.8498456478118896
Epoch 400, training loss: 6.989889621734619 = 0.5945681929588318 + 1.0 * 6.395321369171143
Epoch 400, val loss: 0.8301517367362976
Epoch 410, training loss: 6.977261066436768 = 0.5659221410751343 + 1.0 * 6.411338806152344
Epoch 410, val loss: 0.8121351003646851
Epoch 420, training loss: 6.934047222137451 = 0.5386428236961365 + 1.0 * 6.39540433883667
Epoch 420, val loss: 0.7957132458686829
Epoch 430, training loss: 6.900985240936279 = 0.5123524069786072 + 1.0 * 6.388632774353027
Epoch 430, val loss: 0.7806580662727356
Epoch 440, training loss: 6.871456146240234 = 0.48655781149864197 + 1.0 * 6.3848981857299805
Epoch 440, val loss: 0.7666524648666382
Epoch 450, training loss: 6.8431220054626465 = 0.4613279700279236 + 1.0 * 6.381793975830078
Epoch 450, val loss: 0.7535756826400757
Epoch 460, training loss: 6.823388576507568 = 0.4366454780101776 + 1.0 * 6.386743068695068
Epoch 460, val loss: 0.7415313720703125
Epoch 470, training loss: 6.794196128845215 = 0.4129296839237213 + 1.0 * 6.3812665939331055
Epoch 470, val loss: 0.730475902557373
Epoch 480, training loss: 6.766253471374512 = 0.38992664217948914 + 1.0 * 6.376327037811279
Epoch 480, val loss: 0.7204053997993469
Epoch 490, training loss: 6.740241527557373 = 0.3675941824913025 + 1.0 * 6.372647285461426
Epoch 490, val loss: 0.711219072341919
Epoch 500, training loss: 6.720022201538086 = 0.345929354429245 + 1.0 * 6.374093055725098
Epoch 500, val loss: 0.7028776407241821
Epoch 510, training loss: 6.706180095672607 = 0.3250998854637146 + 1.0 * 6.381080150604248
Epoch 510, val loss: 0.6955861449241638
Epoch 520, training loss: 6.675013542175293 = 0.3053445816040039 + 1.0 * 6.369668960571289
Epoch 520, val loss: 0.6891419291496277
Epoch 530, training loss: 6.651553153991699 = 0.2864595651626587 + 1.0 * 6.36509370803833
Epoch 530, val loss: 0.6835814118385315
Epoch 540, training loss: 6.631463050842285 = 0.2684302031993866 + 1.0 * 6.363032817840576
Epoch 540, val loss: 0.6788978576660156
Epoch 550, training loss: 6.613512992858887 = 0.2513353228569031 + 1.0 * 6.362177848815918
Epoch 550, val loss: 0.6751063466072083
Epoch 560, training loss: 6.596441745758057 = 0.23526103794574738 + 1.0 * 6.361180782318115
Epoch 560, val loss: 0.6720784306526184
Epoch 570, training loss: 6.57808256149292 = 0.2201286107301712 + 1.0 * 6.357954025268555
Epoch 570, val loss: 0.6698892712593079
Epoch 580, training loss: 6.562415599822998 = 0.20596735179424286 + 1.0 * 6.356448173522949
Epoch 580, val loss: 0.6685605645179749
Epoch 590, training loss: 6.5527729988098145 = 0.19277043640613556 + 1.0 * 6.360002517700195
Epoch 590, val loss: 0.6679744720458984
Epoch 600, training loss: 6.533392906188965 = 0.18046903610229492 + 1.0 * 6.35292387008667
Epoch 600, val loss: 0.6681108474731445
Epoch 610, training loss: 6.519700527191162 = 0.1690058410167694 + 1.0 * 6.35069465637207
Epoch 610, val loss: 0.6689214706420898
Epoch 620, training loss: 6.509725570678711 = 0.15832343697547913 + 1.0 * 6.351402282714844
Epoch 620, val loss: 0.670410692691803
Epoch 630, training loss: 6.503066539764404 = 0.14843599498271942 + 1.0 * 6.354630470275879
Epoch 630, val loss: 0.6724464893341064
Epoch 640, training loss: 6.485095977783203 = 0.13927766680717468 + 1.0 * 6.345818519592285
Epoch 640, val loss: 0.6749744415283203
Epoch 650, training loss: 6.475628852844238 = 0.13081002235412598 + 1.0 * 6.344818592071533
Epoch 650, val loss: 0.6778936386108398
Epoch 660, training loss: 6.466300010681152 = 0.12292370200157166 + 1.0 * 6.343376159667969
Epoch 660, val loss: 0.6812831163406372
Epoch 670, training loss: 6.458631992340088 = 0.11558425426483154 + 1.0 * 6.343047618865967
Epoch 670, val loss: 0.6850351691246033
Epoch 680, training loss: 6.449829578399658 = 0.10877113789319992 + 1.0 * 6.341058254241943
Epoch 680, val loss: 0.6891397833824158
Epoch 690, training loss: 6.445503234863281 = 0.10247235000133514 + 1.0 * 6.34303092956543
Epoch 690, val loss: 0.6933941841125488
Epoch 700, training loss: 6.437466144561768 = 0.0966174528002739 + 1.0 * 6.340848922729492
Epoch 700, val loss: 0.6979106664657593
Epoch 710, training loss: 6.427600860595703 = 0.09118158370256424 + 1.0 * 6.336419105529785
Epoch 710, val loss: 0.7026193141937256
Epoch 720, training loss: 6.421780586242676 = 0.08612453937530518 + 1.0 * 6.33565616607666
Epoch 720, val loss: 0.7074496746063232
Epoch 730, training loss: 6.419527530670166 = 0.08141034096479416 + 1.0 * 6.3381171226501465
Epoch 730, val loss: 0.7124633193016052
Epoch 740, training loss: 6.414312362670898 = 0.07702776044607162 + 1.0 * 6.337284564971924
Epoch 740, val loss: 0.7175586819648743
Epoch 750, training loss: 6.40854024887085 = 0.07295415550470352 + 1.0 * 6.335586071014404
Epoch 750, val loss: 0.7227070927619934
Epoch 760, training loss: 6.400340557098389 = 0.06917206943035126 + 1.0 * 6.3311686515808105
Epoch 760, val loss: 0.727929413318634
Epoch 770, training loss: 6.393867492675781 = 0.06564771384000778 + 1.0 * 6.328219890594482
Epoch 770, val loss: 0.7332223057746887
Epoch 780, training loss: 6.3923468589782715 = 0.062348563224077225 + 1.0 * 6.32999849319458
Epoch 780, val loss: 0.7386225461959839
Epoch 790, training loss: 6.387209892272949 = 0.05927549675107002 + 1.0 * 6.327934265136719
Epoch 790, val loss: 0.7440406084060669
Epoch 800, training loss: 6.382326602935791 = 0.05642049014568329 + 1.0 * 6.325906276702881
Epoch 800, val loss: 0.7494434714317322
Epoch 810, training loss: 6.382072925567627 = 0.05374758690595627 + 1.0 * 6.328325271606445
Epoch 810, val loss: 0.7549018859863281
Epoch 820, training loss: 6.374894142150879 = 0.05124817043542862 + 1.0 * 6.323646068572998
Epoch 820, val loss: 0.7603674530982971
Epoch 830, training loss: 6.373883247375488 = 0.04891306161880493 + 1.0 * 6.324970245361328
Epoch 830, val loss: 0.765791654586792
Epoch 840, training loss: 6.370490074157715 = 0.04673431068658829 + 1.0 * 6.323755741119385
Epoch 840, val loss: 0.7712094187736511
Epoch 850, training loss: 6.36754846572876 = 0.044692203402519226 + 1.0 * 6.322856426239014
Epoch 850, val loss: 0.7765912413597107
Epoch 860, training loss: 6.361445903778076 = 0.042776402086019516 + 1.0 * 6.318669319152832
Epoch 860, val loss: 0.7819303870201111
Epoch 870, training loss: 6.358850955963135 = 0.04098064824938774 + 1.0 * 6.317870140075684
Epoch 870, val loss: 0.7873021960258484
Epoch 880, training loss: 6.358984470367432 = 0.03928714245557785 + 1.0 * 6.319697380065918
Epoch 880, val loss: 0.7926638722419739
Epoch 890, training loss: 6.357702732086182 = 0.03769013658165932 + 1.0 * 6.32001256942749
Epoch 890, val loss: 0.7979174852371216
Epoch 900, training loss: 6.353466987609863 = 0.03619888424873352 + 1.0 * 6.317267894744873
Epoch 900, val loss: 0.8031790852546692
Epoch 910, training loss: 6.351036071777344 = 0.03479308634996414 + 1.0 * 6.3162431716918945
Epoch 910, val loss: 0.8083570003509521
Epoch 920, training loss: 6.354423999786377 = 0.03346972540020943 + 1.0 * 6.320954322814941
Epoch 920, val loss: 0.8135148882865906
Epoch 930, training loss: 6.344993591308594 = 0.03222314268350601 + 1.0 * 6.312770366668701
Epoch 930, val loss: 0.818612277507782
Epoch 940, training loss: 6.341331958770752 = 0.031043386086821556 + 1.0 * 6.310288429260254
Epoch 940, val loss: 0.8236572742462158
Epoch 950, training loss: 6.33978271484375 = 0.029923388734459877 + 1.0 * 6.309859275817871
Epoch 950, val loss: 0.828726589679718
Epoch 960, training loss: 6.346012115478516 = 0.02886126935482025 + 1.0 * 6.317151069641113
Epoch 960, val loss: 0.8337076902389526
Epoch 970, training loss: 6.341917037963867 = 0.02786165289580822 + 1.0 * 6.314055442810059
Epoch 970, val loss: 0.8386761546134949
Epoch 980, training loss: 6.339196681976318 = 0.026913335546851158 + 1.0 * 6.312283515930176
Epoch 980, val loss: 0.843457043170929
Epoch 990, training loss: 6.336528301239014 = 0.026019329205155373 + 1.0 * 6.310509204864502
Epoch 990, val loss: 0.848239541053772
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 10.557217597961426 = 1.9604332447052002 + 1.0 * 8.596784591674805
Epoch 0, val loss: 1.9671396017074585
Epoch 10, training loss: 10.546548843383789 = 1.9501278400421143 + 1.0 * 8.596421241760254
Epoch 10, val loss: 1.9561141729354858
Epoch 20, training loss: 10.530901908874512 = 1.9371393918991089 + 1.0 * 8.593762397766113
Epoch 20, val loss: 1.9416223764419556
Epoch 30, training loss: 10.493570327758789 = 1.9185105562210083 + 1.0 * 8.57505989074707
Epoch 30, val loss: 1.920552134513855
Epoch 40, training loss: 10.350330352783203 = 1.8942155838012695 + 1.0 * 8.456114768981934
Epoch 40, val loss: 1.895025610923767
Epoch 50, training loss: 9.857964515686035 = 1.8698289394378662 + 1.0 * 7.98813533782959
Epoch 50, val loss: 1.8704897165298462
Epoch 60, training loss: 9.244478225708008 = 1.8511357307434082 + 1.0 * 7.3933424949646
Epoch 60, val loss: 1.8526886701583862
Epoch 70, training loss: 8.910314559936523 = 1.8370898962020874 + 1.0 * 7.0732245445251465
Epoch 70, val loss: 1.8386982679367065
Epoch 80, training loss: 8.775726318359375 = 1.8205503225326538 + 1.0 * 6.955175876617432
Epoch 80, val loss: 1.821726679801941
Epoch 90, training loss: 8.673393249511719 = 1.7992051839828491 + 1.0 * 6.87418794631958
Epoch 90, val loss: 1.8005163669586182
Epoch 100, training loss: 8.592584609985352 = 1.7782584428787231 + 1.0 * 6.814326286315918
Epoch 100, val loss: 1.7805202007293701
Epoch 110, training loss: 8.521772384643555 = 1.7600047588348389 + 1.0 * 6.761767864227295
Epoch 110, val loss: 1.7637375593185425
Epoch 120, training loss: 8.458101272583008 = 1.7429852485656738 + 1.0 * 6.715116024017334
Epoch 120, val loss: 1.7484723329544067
Epoch 130, training loss: 8.401382446289062 = 1.7246507406234741 + 1.0 * 6.676731586456299
Epoch 130, val loss: 1.7325180768966675
Epoch 140, training loss: 8.349992752075195 = 1.7034695148468018 + 1.0 * 6.6465229988098145
Epoch 140, val loss: 1.714288353919983
Epoch 150, training loss: 8.301936149597168 = 1.6788362264633179 + 1.0 * 6.623100280761719
Epoch 150, val loss: 1.6933778524398804
Epoch 160, training loss: 8.249811172485352 = 1.6506415605545044 + 1.0 * 6.5991692543029785
Epoch 160, val loss: 1.6697158813476562
Epoch 170, training loss: 8.1971435546875 = 1.6182138919830322 + 1.0 * 6.578929901123047
Epoch 170, val loss: 1.6426302194595337
Epoch 180, training loss: 8.145676612854004 = 1.5811355113983154 + 1.0 * 6.564540863037109
Epoch 180, val loss: 1.6119158267974854
Epoch 190, training loss: 8.086971282958984 = 1.5401641130447388 + 1.0 * 6.546807289123535
Epoch 190, val loss: 1.5780409574508667
Epoch 200, training loss: 8.027054786682129 = 1.4949843883514404 + 1.0 * 6.532070636749268
Epoch 200, val loss: 1.5407761335372925
Epoch 210, training loss: 7.965106010437012 = 1.445892572402954 + 1.0 * 6.5192131996154785
Epoch 210, val loss: 1.5003687143325806
Epoch 220, training loss: 7.908511638641357 = 1.3937233686447144 + 1.0 * 6.5147881507873535
Epoch 220, val loss: 1.4576598405838013
Epoch 230, training loss: 7.840368747711182 = 1.340260624885559 + 1.0 * 6.500108242034912
Epoch 230, val loss: 1.4140764474868774
Epoch 240, training loss: 7.774635314941406 = 1.2852137088775635 + 1.0 * 6.489421367645264
Epoch 240, val loss: 1.3694429397583008
Epoch 250, training loss: 7.7091450691223145 = 1.228303074836731 + 1.0 * 6.480842113494873
Epoch 250, val loss: 1.3232287168502808
Epoch 260, training loss: 7.645939826965332 = 1.171570062637329 + 1.0 * 6.474370002746582
Epoch 260, val loss: 1.2773810625076294
Epoch 270, training loss: 7.583881378173828 = 1.116363525390625 + 1.0 * 6.467517852783203
Epoch 270, val loss: 1.2325950860977173
Epoch 280, training loss: 7.526562690734863 = 1.063324213027954 + 1.0 * 6.46323823928833
Epoch 280, val loss: 1.1895314455032349
Epoch 290, training loss: 7.467917442321777 = 1.0137243270874023 + 1.0 * 6.454193115234375
Epoch 290, val loss: 1.1494817733764648
Epoch 300, training loss: 7.415699005126953 = 0.9674015641212463 + 1.0 * 6.448297500610352
Epoch 300, val loss: 1.1119604110717773
Epoch 310, training loss: 7.370813846588135 = 0.924006998538971 + 1.0 * 6.446806907653809
Epoch 310, val loss: 1.076798915863037
Epoch 320, training loss: 7.3224616050720215 = 0.8835850358009338 + 1.0 * 6.438876628875732
Epoch 320, val loss: 1.0441808700561523
Epoch 330, training loss: 7.277371883392334 = 0.8457277417182922 + 1.0 * 6.431643962860107
Epoch 330, val loss: 1.0136774778366089
Epoch 340, training loss: 7.241025447845459 = 0.8097219467163086 + 1.0 * 6.43130350112915
Epoch 340, val loss: 0.9850060343742371
Epoch 350, training loss: 7.198253631591797 = 0.7758826017379761 + 1.0 * 6.422370910644531
Epoch 350, val loss: 0.9581547379493713
Epoch 360, training loss: 7.162656784057617 = 0.7436214089393616 + 1.0 * 6.4190354347229
Epoch 360, val loss: 0.933191180229187
Epoch 370, training loss: 7.130193710327148 = 0.7127139568328857 + 1.0 * 6.417479515075684
Epoch 370, val loss: 0.9097160696983337
Epoch 380, training loss: 7.094263076782227 = 0.6832339763641357 + 1.0 * 6.411028861999512
Epoch 380, val loss: 0.8880801200866699
Epoch 390, training loss: 7.062583923339844 = 0.6547977924346924 + 1.0 * 6.407785892486572
Epoch 390, val loss: 0.8680320978164673
Epoch 400, training loss: 7.031335830688477 = 0.6274955868721008 + 1.0 * 6.403840065002441
Epoch 400, val loss: 0.849537193775177
Epoch 410, training loss: 6.999959945678711 = 0.6012185215950012 + 1.0 * 6.398741245269775
Epoch 410, val loss: 0.8326279520988464
Epoch 420, training loss: 6.987856864929199 = 0.5758545398712158 + 1.0 * 6.4120025634765625
Epoch 420, val loss: 0.8170268535614014
Epoch 430, training loss: 6.947406768798828 = 0.5515501499176025 + 1.0 * 6.395856857299805
Epoch 430, val loss: 0.8028841614723206
Epoch 440, training loss: 6.916799545288086 = 0.5281162261962891 + 1.0 * 6.388683319091797
Epoch 440, val loss: 0.7899677753448486
Epoch 450, training loss: 6.891061305999756 = 0.505287766456604 + 1.0 * 6.385773658752441
Epoch 450, val loss: 0.7780407667160034
Epoch 460, training loss: 6.8664350509643555 = 0.4829401671886444 + 1.0 * 6.383494853973389
Epoch 460, val loss: 0.767035722732544
Epoch 470, training loss: 6.847187042236328 = 0.4611712396144867 + 1.0 * 6.386015892028809
Epoch 470, val loss: 0.7569423317909241
Epoch 480, training loss: 6.818940162658691 = 0.4400915205478668 + 1.0 * 6.378848552703857
Epoch 480, val loss: 0.7478429675102234
Epoch 490, training loss: 6.798739433288574 = 0.41966012120246887 + 1.0 * 6.379079341888428
Epoch 490, val loss: 0.7395850419998169
Epoch 500, training loss: 6.776957035064697 = 0.3999292552471161 + 1.0 * 6.377027988433838
Epoch 500, val loss: 0.7320773601531982
Epoch 510, training loss: 6.752861499786377 = 0.3808649778366089 + 1.0 * 6.3719964027404785
Epoch 510, val loss: 0.7253750562667847
Epoch 520, training loss: 6.746006965637207 = 0.36242440342903137 + 1.0 * 6.383582592010498
Epoch 520, val loss: 0.7193819880485535
Epoch 530, training loss: 6.713109970092773 = 0.3449328541755676 + 1.0 * 6.3681769371032715
Epoch 530, val loss: 0.7141315340995789
Epoch 540, training loss: 6.6941986083984375 = 0.32810139656066895 + 1.0 * 6.3660969734191895
Epoch 540, val loss: 0.7095592617988586
Epoch 550, training loss: 6.674574375152588 = 0.3118450343608856 + 1.0 * 6.362729549407959
Epoch 550, val loss: 0.7055402398109436
Epoch 560, training loss: 6.658637046813965 = 0.2961333692073822 + 1.0 * 6.362503528594971
Epoch 560, val loss: 0.7020694017410278
Epoch 570, training loss: 6.649074077606201 = 0.280996710062027 + 1.0 * 6.368077278137207
Epoch 570, val loss: 0.6990122199058533
Epoch 580, training loss: 6.627474784851074 = 0.26652616262435913 + 1.0 * 6.36094856262207
Epoch 580, val loss: 0.6964529156684875
Epoch 590, training loss: 6.6088547706604 = 0.25249359011650085 + 1.0 * 6.356361389160156
Epoch 590, val loss: 0.6942669153213501
Epoch 600, training loss: 6.593769550323486 = 0.23882724344730377 + 1.0 * 6.354942321777344
Epoch 600, val loss: 0.692425012588501
Epoch 610, training loss: 6.580252170562744 = 0.22559335827827454 + 1.0 * 6.354658603668213
Epoch 610, val loss: 0.6908537745475769
Epoch 620, training loss: 6.566513538360596 = 0.2128443419933319 + 1.0 * 6.353669166564941
Epoch 620, val loss: 0.6896116733551025
Epoch 630, training loss: 6.549431800842285 = 0.20053090155124664 + 1.0 * 6.34890079498291
Epoch 630, val loss: 0.6886128783226013
Epoch 640, training loss: 6.538214206695557 = 0.18865519762039185 + 1.0 * 6.3495588302612305
Epoch 640, val loss: 0.6878787279129028
Epoch 650, training loss: 6.527703285217285 = 0.17732690274715424 + 1.0 * 6.350376605987549
Epoch 650, val loss: 0.6874462962150574
Epoch 660, training loss: 6.512327194213867 = 0.16658973693847656 + 1.0 * 6.345737457275391
Epoch 660, val loss: 0.6873399615287781
Epoch 670, training loss: 6.500375270843506 = 0.15641941130161285 + 1.0 * 6.343955993652344
Epoch 670, val loss: 0.6875596046447754
Epoch 680, training loss: 6.496833801269531 = 0.14679765701293945 + 1.0 * 6.350036144256592
Epoch 680, val loss: 0.6881007552146912
Epoch 690, training loss: 6.481762409210205 = 0.13781490921974182 + 1.0 * 6.343947410583496
Epoch 690, val loss: 0.6890668869018555
Epoch 700, training loss: 6.470607757568359 = 0.12938769161701202 + 1.0 * 6.341219902038574
Epoch 700, val loss: 0.690356969833374
Epoch 710, training loss: 6.4637532234191895 = 0.12153699994087219 + 1.0 * 6.3422160148620605
Epoch 710, val loss: 0.6920220851898193
Epoch 720, training loss: 6.452157020568848 = 0.11421449482440948 + 1.0 * 6.337942600250244
Epoch 720, val loss: 0.6940594911575317
Epoch 730, training loss: 6.450162887573242 = 0.1074199229478836 + 1.0 * 6.342742919921875
Epoch 730, val loss: 0.6963849067687988
Epoch 740, training loss: 6.439925670623779 = 0.10115252435207367 + 1.0 * 6.338773250579834
Epoch 740, val loss: 0.6990563273429871
Epoch 750, training loss: 6.429103374481201 = 0.09533681720495224 + 1.0 * 6.333766460418701
Epoch 750, val loss: 0.701960563659668
Epoch 760, training loss: 6.4219746589660645 = 0.08994708210229874 + 1.0 * 6.332027435302734
Epoch 760, val loss: 0.7051253318786621
Epoch 770, training loss: 6.421792507171631 = 0.08494199067354202 + 1.0 * 6.336850643157959
Epoch 770, val loss: 0.708541989326477
Epoch 780, training loss: 6.414191722869873 = 0.08030831813812256 + 1.0 * 6.333883285522461
Epoch 780, val loss: 0.712181031703949
Epoch 790, training loss: 6.408209323883057 = 0.07602166384458542 + 1.0 * 6.332187652587891
Epoch 790, val loss: 0.7159280180931091
Epoch 800, training loss: 6.401367664337158 = 0.07207167148590088 + 1.0 * 6.329296112060547
Epoch 800, val loss: 0.7198328375816345
Epoch 810, training loss: 6.394639492034912 = 0.06840244680643082 + 1.0 * 6.326237201690674
Epoch 810, val loss: 0.72383713722229
Epoch 820, training loss: 6.389481544494629 = 0.06498454511165619 + 1.0 * 6.324497222900391
Epoch 820, val loss: 0.7279337644577026
Epoch 830, training loss: 6.386757850646973 = 0.06179146096110344 + 1.0 * 6.3249664306640625
Epoch 830, val loss: 0.7320728898048401
Epoch 840, training loss: 6.38585901260376 = 0.05881528928875923 + 1.0 * 6.327043533325195
Epoch 840, val loss: 0.736267626285553
Epoch 850, training loss: 6.3775739669799805 = 0.0560506172478199 + 1.0 * 6.321523189544678
Epoch 850, val loss: 0.7405582070350647
Epoch 860, training loss: 6.373915195465088 = 0.05346864089369774 + 1.0 * 6.320446491241455
Epoch 860, val loss: 0.7448645830154419
Epoch 870, training loss: 6.377999782562256 = 0.051050785928964615 + 1.0 * 6.326949119567871
Epoch 870, val loss: 0.749198317527771
Epoch 880, training loss: 6.3690104484558105 = 0.04879532754421234 + 1.0 * 6.320215225219727
Epoch 880, val loss: 0.7535712718963623
Epoch 890, training loss: 6.364039421081543 = 0.046682775020599365 + 1.0 * 6.317356586456299
Epoch 890, val loss: 0.7579090595245361
Epoch 900, training loss: 6.363932132720947 = 0.044691573828458786 + 1.0 * 6.319240570068359
Epoch 900, val loss: 0.7622838616371155
Epoch 910, training loss: 6.363084316253662 = 0.04283676669001579 + 1.0 * 6.320247650146484
Epoch 910, val loss: 0.7666199803352356
Epoch 920, training loss: 6.355855464935303 = 0.041094180196523666 + 1.0 * 6.314761161804199
Epoch 920, val loss: 0.7710052728652954
Epoch 930, training loss: 6.3534111976623535 = 0.039452869445085526 + 1.0 * 6.313958168029785
Epoch 930, val loss: 0.775251567363739
Epoch 940, training loss: 6.350405693054199 = 0.03789985552430153 + 1.0 * 6.312505722045898
Epoch 940, val loss: 0.7794688940048218
Epoch 950, training loss: 6.355728626251221 = 0.036428820341825485 + 1.0 * 6.319299697875977
Epoch 950, val loss: 0.7837035059928894
Epoch 960, training loss: 6.347868919372559 = 0.03504616394639015 + 1.0 * 6.3128228187561035
Epoch 960, val loss: 0.7879165410995483
Epoch 970, training loss: 6.3449859619140625 = 0.033739518374204636 + 1.0 * 6.311246395111084
Epoch 970, val loss: 0.7920771241188049
Epoch 980, training loss: 6.343163967132568 = 0.03250463679432869 + 1.0 * 6.310659408569336
Epoch 980, val loss: 0.7961866855621338
Epoch 990, training loss: 6.340685844421387 = 0.03133651986718178 + 1.0 * 6.309349536895752
Epoch 990, val loss: 0.800257682800293
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 10.550978660583496 = 1.954161524772644 + 1.0 * 8.596817016601562
Epoch 0, val loss: 1.9580507278442383
Epoch 10, training loss: 10.540251731872559 = 1.9437309503555298 + 1.0 * 8.59652042388916
Epoch 10, val loss: 1.9475029706954956
Epoch 20, training loss: 10.525226593017578 = 1.9308630228042603 + 1.0 * 8.59436321258545
Epoch 20, val loss: 1.9345864057540894
Epoch 30, training loss: 10.490516662597656 = 1.912994384765625 + 1.0 * 8.577522277832031
Epoch 30, val loss: 1.9168330430984497
Epoch 40, training loss: 10.34638500213623 = 1.889733910560608 + 1.0 * 8.456650733947754
Epoch 40, val loss: 1.894310712814331
Epoch 50, training loss: 9.79732608795166 = 1.8658281564712524 + 1.0 * 7.931497573852539
Epoch 50, val loss: 1.8710078001022339
Epoch 60, training loss: 9.296952247619629 = 1.8474018573760986 + 1.0 * 7.449550151824951
Epoch 60, val loss: 1.8535839319229126
Epoch 70, training loss: 9.01008129119873 = 1.8318344354629517 + 1.0 * 7.17824649810791
Epoch 70, val loss: 1.838619351387024
Epoch 80, training loss: 8.820398330688477 = 1.8139897584915161 + 1.0 * 7.006408214569092
Epoch 80, val loss: 1.8220032453536987
Epoch 90, training loss: 8.696967124938965 = 1.7942177057266235 + 1.0 * 6.902749061584473
Epoch 90, val loss: 1.8041234016418457
Epoch 100, training loss: 8.597658157348633 = 1.774968147277832 + 1.0 * 6.822689533233643
Epoch 100, val loss: 1.7866120338439941
Epoch 110, training loss: 8.525837898254395 = 1.756186604499817 + 1.0 * 6.769651412963867
Epoch 110, val loss: 1.7691559791564941
Epoch 120, training loss: 8.463362693786621 = 1.736964464187622 + 1.0 * 6.72639799118042
Epoch 120, val loss: 1.7512359619140625
Epoch 130, training loss: 8.403780937194824 = 1.7165247201919556 + 1.0 * 6.687255859375
Epoch 130, val loss: 1.7326031923294067
Epoch 140, training loss: 8.346577644348145 = 1.6937118768692017 + 1.0 * 6.652865886688232
Epoch 140, val loss: 1.7124478816986084
Epoch 150, training loss: 8.292407989501953 = 1.6673917770385742 + 1.0 * 6.625015735626221
Epoch 150, val loss: 1.6898168325424194
Epoch 160, training loss: 8.233744621276855 = 1.6371241807937622 + 1.0 * 6.596620559692383
Epoch 160, val loss: 1.664061188697815
Epoch 170, training loss: 8.176528930664062 = 1.6023638248443604 + 1.0 * 6.574165344238281
Epoch 170, val loss: 1.634478211402893
Epoch 180, training loss: 8.119245529174805 = 1.562935709953308 + 1.0 * 6.556310176849365
Epoch 180, val loss: 1.6010582447052002
Epoch 190, training loss: 8.059465408325195 = 1.5194333791732788 + 1.0 * 6.540031909942627
Epoch 190, val loss: 1.5642796754837036
Epoch 200, training loss: 7.997928619384766 = 1.4723761081695557 + 1.0 * 6.525552272796631
Epoch 200, val loss: 1.5244572162628174
Epoch 210, training loss: 7.9388203620910645 = 1.4232631921768188 + 1.0 * 6.515557289123535
Epoch 210, val loss: 1.483098030090332
Epoch 220, training loss: 7.876016616821289 = 1.3735835552215576 + 1.0 * 6.502432823181152
Epoch 220, val loss: 1.4413410425186157
Epoch 230, training loss: 7.816449165344238 = 1.3235762119293213 + 1.0 * 6.492872714996338
Epoch 230, val loss: 1.399530053138733
Epoch 240, training loss: 7.758489608764648 = 1.2741761207580566 + 1.0 * 6.484313488006592
Epoch 240, val loss: 1.358390212059021
Epoch 250, training loss: 7.701544761657715 = 1.2261509895324707 + 1.0 * 6.475393772125244
Epoch 250, val loss: 1.3185679912567139
Epoch 260, training loss: 7.648355484008789 = 1.1796314716339111 + 1.0 * 6.468723773956299
Epoch 260, val loss: 1.2801258563995361
Epoch 270, training loss: 7.6005330085754395 = 1.135266900062561 + 1.0 * 6.465266227722168
Epoch 270, val loss: 1.2440528869628906
Epoch 280, training loss: 7.54930305480957 = 1.0938224792480469 + 1.0 * 6.455480575561523
Epoch 280, val loss: 1.2106012105941772
Epoch 290, training loss: 7.503535747528076 = 1.0548371076583862 + 1.0 * 6.4486985206604
Epoch 290, val loss: 1.1795544624328613
Epoch 300, training loss: 7.466300010681152 = 1.018019437789917 + 1.0 * 6.4482808113098145
Epoch 300, val loss: 1.1506685018539429
Epoch 310, training loss: 7.422353744506836 = 0.98349928855896 + 1.0 * 6.438854217529297
Epoch 310, val loss: 1.1238409280776978
Epoch 320, training loss: 7.384659767150879 = 0.9505020380020142 + 1.0 * 6.434157848358154
Epoch 320, val loss: 1.0985702276229858
Epoch 330, training loss: 7.349675178527832 = 0.918304979801178 + 1.0 * 6.431370258331299
Epoch 330, val loss: 1.0740468502044678
Epoch 340, training loss: 7.312551021575928 = 0.8864497542381287 + 1.0 * 6.426101207733154
Epoch 340, val loss: 1.0499415397644043
Epoch 350, training loss: 7.277724266052246 = 0.8544432520866394 + 1.0 * 6.423281192779541
Epoch 350, val loss: 1.0256887674331665
Epoch 360, training loss: 7.244032859802246 = 0.8221177458763123 + 1.0 * 6.421915054321289
Epoch 360, val loss: 1.0010685920715332
Epoch 370, training loss: 7.204740047454834 = 0.7894336581230164 + 1.0 * 6.415306568145752
Epoch 370, val loss: 0.9760614633560181
Epoch 380, training loss: 7.167447566986084 = 0.7561072707176208 + 1.0 * 6.411340236663818
Epoch 380, val loss: 0.9503809213638306
Epoch 390, training loss: 7.129875659942627 = 0.7222058176994324 + 1.0 * 6.407670021057129
Epoch 390, val loss: 0.9242482781410217
Epoch 400, training loss: 7.097815990447998 = 0.688043475151062 + 1.0 * 6.4097723960876465
Epoch 400, val loss: 0.8980729579925537
Epoch 410, training loss: 7.057376384735107 = 0.6545661091804504 + 1.0 * 6.402810096740723
Epoch 410, val loss: 0.8724261522293091
Epoch 420, training loss: 7.022446155548096 = 0.6219201683998108 + 1.0 * 6.40052604675293
Epoch 420, val loss: 0.8478885293006897
Epoch 430, training loss: 6.991173267364502 = 0.5902480483055115 + 1.0 * 6.400925159454346
Epoch 430, val loss: 0.8243932723999023
Epoch 440, training loss: 6.953924655914307 = 0.5596966743469238 + 1.0 * 6.394227981567383
Epoch 440, val loss: 0.8024833798408508
Epoch 450, training loss: 6.923027515411377 = 0.5301769375801086 + 1.0 * 6.392850399017334
Epoch 450, val loss: 0.7820547819137573
Epoch 460, training loss: 6.8923845291137695 = 0.501833438873291 + 1.0 * 6.3905510902404785
Epoch 460, val loss: 0.7631804347038269
Epoch 470, training loss: 6.864445686340332 = 0.47461292147636414 + 1.0 * 6.389832973480225
Epoch 470, val loss: 0.7458693981170654
Epoch 480, training loss: 6.833880424499512 = 0.44850897789001465 + 1.0 * 6.385371208190918
Epoch 480, val loss: 0.7299578189849854
Epoch 490, training loss: 6.805488586425781 = 0.4233853816986084 + 1.0 * 6.382103443145752
Epoch 490, val loss: 0.715241014957428
Epoch 500, training loss: 6.786633491516113 = 0.39919665455818176 + 1.0 * 6.387436866760254
Epoch 500, val loss: 0.7017408013343811
Epoch 510, training loss: 6.757586479187012 = 0.3761681020259857 + 1.0 * 6.381418228149414
Epoch 510, val loss: 0.689418613910675
Epoch 520, training loss: 6.730679512023926 = 0.3541173040866852 + 1.0 * 6.376562118530273
Epoch 520, val loss: 0.6781461834907532
Epoch 530, training loss: 6.70620584487915 = 0.33294788002967834 + 1.0 * 6.373258113861084
Epoch 530, val loss: 0.6676665544509888
Epoch 540, training loss: 6.691330432891846 = 0.3126336634159088 + 1.0 * 6.378696918487549
Epoch 540, val loss: 0.6579686999320984
Epoch 550, training loss: 6.664952278137207 = 0.2932831346988678 + 1.0 * 6.371669292449951
Epoch 550, val loss: 0.649138867855072
Epoch 560, training loss: 6.642369747161865 = 0.2748333513736725 + 1.0 * 6.367536544799805
Epoch 560, val loss: 0.6410653591156006
Epoch 570, training loss: 6.62290620803833 = 0.2572029232978821 + 1.0 * 6.365703105926514
Epoch 570, val loss: 0.6336925029754639
Epoch 580, training loss: 6.6159796714782715 = 0.24044331908226013 + 1.0 * 6.3755364418029785
Epoch 580, val loss: 0.6270959973335266
Epoch 590, training loss: 6.588184833526611 = 0.22466249763965607 + 1.0 * 6.363522529602051
Epoch 590, val loss: 0.6213210225105286
Epoch 600, training loss: 6.570161819458008 = 0.2097395807504654 + 1.0 * 6.360422134399414
Epoch 600, val loss: 0.616259753704071
Epoch 610, training loss: 6.553885459899902 = 0.195603609085083 + 1.0 * 6.358282089233398
Epoch 610, val loss: 0.6119393110275269
Epoch 620, training loss: 6.547874450683594 = 0.18226252496242523 + 1.0 * 6.365612030029297
Epoch 620, val loss: 0.6083419322967529
Epoch 630, training loss: 6.528451442718506 = 0.16989214718341827 + 1.0 * 6.3585591316223145
Epoch 630, val loss: 0.6055030822753906
Epoch 640, training loss: 6.512436389923096 = 0.1583828628063202 + 1.0 * 6.354053497314453
Epoch 640, val loss: 0.6033505201339722
Epoch 650, training loss: 6.499335765838623 = 0.14765943586826324 + 1.0 * 6.3516764640808105
Epoch 650, val loss: 0.601876437664032
Epoch 660, training loss: 6.487858295440674 = 0.1376873403787613 + 1.0 * 6.350171089172363
Epoch 660, val loss: 0.6010317802429199
Epoch 670, training loss: 6.484090805053711 = 0.128457710146904 + 1.0 * 6.35563325881958
Epoch 670, val loss: 0.6007713675498962
Epoch 680, training loss: 6.469598770141602 = 0.11998210102319717 + 1.0 * 6.349616527557373
Epoch 680, val loss: 0.6010535955429077
Epoch 690, training loss: 6.459181308746338 = 0.11220438778400421 + 1.0 * 6.3469767570495605
Epoch 690, val loss: 0.6018463969230652
Epoch 700, training loss: 6.451319694519043 = 0.10507452487945557 + 1.0 * 6.346245288848877
Epoch 700, val loss: 0.6030533313751221
Epoch 710, training loss: 6.442703723907471 = 0.09852822870016098 + 1.0 * 6.344175338745117
Epoch 710, val loss: 0.6047303676605225
Epoch 720, training loss: 6.435225009918213 = 0.09250663965940475 + 1.0 * 6.342718601226807
Epoch 720, val loss: 0.6068304777145386
Epoch 730, training loss: 6.43023157119751 = 0.08697150647640228 + 1.0 * 6.343260288238525
Epoch 730, val loss: 0.6092818975448608
Epoch 740, training loss: 6.424857139587402 = 0.08190056681632996 + 1.0 * 6.34295654296875
Epoch 740, val loss: 0.6120589971542358
Epoch 750, training loss: 6.41683292388916 = 0.07723630219697952 + 1.0 * 6.339596748352051
Epoch 750, val loss: 0.6150364875793457
Epoch 760, training loss: 6.4101104736328125 = 0.07294879853725433 + 1.0 * 6.337161540985107
Epoch 760, val loss: 0.6182673573493958
Epoch 770, training loss: 6.403327465057373 = 0.06899107247591019 + 1.0 * 6.334336280822754
Epoch 770, val loss: 0.6217073798179626
Epoch 780, training loss: 6.402163982391357 = 0.06532709300518036 + 1.0 * 6.336836814880371
Epoch 780, val loss: 0.6253346800804138
Epoch 790, training loss: 6.397995471954346 = 0.061942800879478455 + 1.0 * 6.336052894592285
Epoch 790, val loss: 0.629086971282959
Epoch 800, training loss: 6.3929219245910645 = 0.05882622301578522 + 1.0 * 6.334095478057861
Epoch 800, val loss: 0.6329282522201538
Epoch 810, training loss: 6.386890888214111 = 0.05593731999397278 + 1.0 * 6.330953598022461
Epoch 810, val loss: 0.636898934841156
Epoch 820, training loss: 6.3852338790893555 = 0.053255844861269 + 1.0 * 6.331977844238281
Epoch 820, val loss: 0.6409552097320557
Epoch 830, training loss: 6.380909442901611 = 0.050775084644556046 + 1.0 * 6.330134391784668
Epoch 830, val loss: 0.645054042339325
Epoch 840, training loss: 6.375419616699219 = 0.04846211522817612 + 1.0 * 6.326957702636719
Epoch 840, val loss: 0.6491642594337463
Epoch 850, training loss: 6.3748908042907715 = 0.046300675719976425 + 1.0 * 6.328589916229248
Epoch 850, val loss: 0.6533913016319275
Epoch 860, training loss: 6.369121551513672 = 0.0442836694419384 + 1.0 * 6.324837684631348
Epoch 860, val loss: 0.6576145887374878
Epoch 870, training loss: 6.368974685668945 = 0.0423971451818943 + 1.0 * 6.326577663421631
Epoch 870, val loss: 0.6618125438690186
Epoch 880, training loss: 6.366321563720703 = 0.04063080996274948 + 1.0 * 6.325690746307373
Epoch 880, val loss: 0.6660454869270325
Epoch 890, training loss: 6.363109588623047 = 0.0389818474650383 + 1.0 * 6.324127674102783
Epoch 890, val loss: 0.670266330242157
Epoch 900, training loss: 6.357773780822754 = 0.03743833303451538 + 1.0 * 6.320335388183594
Epoch 900, val loss: 0.6744619011878967
Epoch 910, training loss: 6.355709552764893 = 0.035981256514787674 + 1.0 * 6.319728374481201
Epoch 910, val loss: 0.6786827445030212
Epoch 920, training loss: 6.359195709228516 = 0.03460821136832237 + 1.0 * 6.324587345123291
Epoch 920, val loss: 0.6829528212547302
Epoch 930, training loss: 6.353331089019775 = 0.033315509557724 + 1.0 * 6.3200154304504395
Epoch 930, val loss: 0.6871041059494019
Epoch 940, training loss: 6.351722717285156 = 0.03210213780403137 + 1.0 * 6.319620609283447
Epoch 940, val loss: 0.6912543177604675
Epoch 950, training loss: 6.346893310546875 = 0.0309526976197958 + 1.0 * 6.3159403800964355
Epoch 950, val loss: 0.6954143047332764
Epoch 960, training loss: 6.354461193084717 = 0.029869766905903816 + 1.0 * 6.324591636657715
Epoch 960, val loss: 0.6995723247528076
Epoch 970, training loss: 6.345668315887451 = 0.028840016573667526 + 1.0 * 6.31682825088501
Epoch 970, val loss: 0.7036281228065491
Epoch 980, training loss: 6.340638160705566 = 0.02787063829600811 + 1.0 * 6.312767505645752
Epoch 980, val loss: 0.707688570022583
Epoch 990, training loss: 6.341887950897217 = 0.0269472673535347 + 1.0 * 6.314940452575684
Epoch 990, val loss: 0.711757481098175
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.838165524512388
The final CL Acc:0.82346, 0.00924, The final GNN Acc:0.83834, 0.00108
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10542])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.547611236572266 = 1.9508072137832642 + 1.0 * 8.596803665161133
Epoch 0, val loss: 1.9459269046783447
Epoch 10, training loss: 10.536965370178223 = 1.940494418144226 + 1.0 * 8.596470832824707
Epoch 10, val loss: 1.935165286064148
Epoch 20, training loss: 10.521844863891602 = 1.9276328086853027 + 1.0 * 8.59421157836914
Epoch 20, val loss: 1.9218263626098633
Epoch 30, training loss: 10.486686706542969 = 1.909339189529419 + 1.0 * 8.577347755432129
Epoch 30, val loss: 1.902833104133606
Epoch 40, training loss: 10.34073543548584 = 1.8850185871124268 + 1.0 * 8.455717086791992
Epoch 40, val loss: 1.8784462213516235
Epoch 50, training loss: 9.865063667297363 = 1.8593350648880005 + 1.0 * 8.005728721618652
Epoch 50, val loss: 1.8535597324371338
Epoch 60, training loss: 9.335262298583984 = 1.841766357421875 + 1.0 * 7.493495464324951
Epoch 60, val loss: 1.8376837968826294
Epoch 70, training loss: 8.921146392822266 = 1.8304967880249023 + 1.0 * 7.0906500816345215
Epoch 70, val loss: 1.8275243043899536
Epoch 80, training loss: 8.743648529052734 = 1.8192044496536255 + 1.0 * 6.92444372177124
Epoch 80, val loss: 1.8168458938598633
Epoch 90, training loss: 8.622865676879883 = 1.8053327798843384 + 1.0 * 6.817532539367676
Epoch 90, val loss: 1.8039100170135498
Epoch 100, training loss: 8.542391777038574 = 1.7924903631210327 + 1.0 * 6.74990177154541
Epoch 100, val loss: 1.7922189235687256
Epoch 110, training loss: 8.4811429977417 = 1.7815362215042114 + 1.0 * 6.699606418609619
Epoch 110, val loss: 1.7823141813278198
Epoch 120, training loss: 8.429887771606445 = 1.7711118459701538 + 1.0 * 6.65877628326416
Epoch 120, val loss: 1.7728534936904907
Epoch 130, training loss: 8.388177871704102 = 1.7598135471343994 + 1.0 * 6.628364562988281
Epoch 130, val loss: 1.7626971006393433
Epoch 140, training loss: 8.348485946655273 = 1.7472103834152222 + 1.0 * 6.601275444030762
Epoch 140, val loss: 1.751570701599121
Epoch 150, training loss: 8.312211990356445 = 1.7328141927719116 + 1.0 * 6.579397678375244
Epoch 150, val loss: 1.7389941215515137
Epoch 160, training loss: 8.276947021484375 = 1.7162576913833618 + 1.0 * 6.560689449310303
Epoch 160, val loss: 1.7246496677398682
Epoch 170, training loss: 8.240925788879395 = 1.6972116231918335 + 1.0 * 6.54371452331543
Epoch 170, val loss: 1.7082276344299316
Epoch 180, training loss: 8.206276893615723 = 1.6749674081802368 + 1.0 * 6.531309604644775
Epoch 180, val loss: 1.6890689134597778
Epoch 190, training loss: 8.164369583129883 = 1.6490521430969238 + 1.0 * 6.515316963195801
Epoch 190, val loss: 1.6669299602508545
Epoch 200, training loss: 8.123254776000977 = 1.6189064979553223 + 1.0 * 6.5043487548828125
Epoch 200, val loss: 1.6412628889083862
Epoch 210, training loss: 8.078929901123047 = 1.584557056427002 + 1.0 * 6.494373321533203
Epoch 210, val loss: 1.6122163534164429
Epoch 220, training loss: 8.029736518859863 = 1.545972228050232 + 1.0 * 6.483764171600342
Epoch 220, val loss: 1.5798704624176025
Epoch 230, training loss: 7.98328971862793 = 1.5034990310668945 + 1.0 * 6.479790687561035
Epoch 230, val loss: 1.5448341369628906
Epoch 240, training loss: 7.927682876586914 = 1.45891273021698 + 1.0 * 6.4687700271606445
Epoch 240, val loss: 1.508924961090088
Epoch 250, training loss: 7.8732500076293945 = 1.4127713441848755 + 1.0 * 6.460478782653809
Epoch 250, val loss: 1.4725921154022217
Epoch 260, training loss: 7.8206562995910645 = 1.3656684160232544 + 1.0 * 6.4549880027771
Epoch 260, val loss: 1.43637216091156
Epoch 270, training loss: 7.771039962768555 = 1.3187435865402222 + 1.0 * 6.452296257019043
Epoch 270, val loss: 1.4014954566955566
Epoch 280, training loss: 7.716482162475586 = 1.2725765705108643 + 1.0 * 6.443905830383301
Epoch 280, val loss: 1.3677738904953003
Epoch 290, training loss: 7.666476249694824 = 1.2267180681228638 + 1.0 * 6.43975830078125
Epoch 290, val loss: 1.334833025932312
Epoch 300, training loss: 7.617891311645508 = 1.1815464496612549 + 1.0 * 6.436344623565674
Epoch 300, val loss: 1.302838683128357
Epoch 310, training loss: 7.56764030456543 = 1.1372040510177612 + 1.0 * 6.430436134338379
Epoch 310, val loss: 1.2717092037200928
Epoch 320, training loss: 7.520605087280273 = 1.093483805656433 + 1.0 * 6.427121162414551
Epoch 320, val loss: 1.2411683797836304
Epoch 330, training loss: 7.477104663848877 = 1.0507739782333374 + 1.0 * 6.42633056640625
Epoch 330, val loss: 1.211549162864685
Epoch 340, training loss: 7.429754257202148 = 1.0093116760253906 + 1.0 * 6.420442581176758
Epoch 340, val loss: 1.1827555894851685
Epoch 350, training loss: 7.3849382400512695 = 0.9687004685401917 + 1.0 * 6.416237831115723
Epoch 350, val loss: 1.1543437242507935
Epoch 360, training loss: 7.343845844268799 = 0.9290289878845215 + 1.0 * 6.414816856384277
Epoch 360, val loss: 1.1265736818313599
Epoch 370, training loss: 7.301277160644531 = 0.8907082080841064 + 1.0 * 6.410568714141846
Epoch 370, val loss: 1.0995906591415405
Epoch 380, training loss: 7.260678768157959 = 0.8532845377922058 + 1.0 * 6.4073944091796875
Epoch 380, val loss: 1.0730270147323608
Epoch 390, training loss: 7.221897125244141 = 0.8168855905532837 + 1.0 * 6.4050116539001465
Epoch 390, val loss: 1.04701828956604
Epoch 400, training loss: 7.183786869049072 = 0.78168785572052 + 1.0 * 6.402099132537842
Epoch 400, val loss: 1.0217430591583252
Epoch 410, training loss: 7.146849155426025 = 0.7476248145103455 + 1.0 * 6.399224281311035
Epoch 410, val loss: 0.9972257018089294
Epoch 420, training loss: 7.1133341789245605 = 0.714788019657135 + 1.0 * 6.39854621887207
Epoch 420, val loss: 0.9737686514854431
Epoch 430, training loss: 7.077719211578369 = 0.6834368109703064 + 1.0 * 6.394282341003418
Epoch 430, val loss: 0.9515053033828735
Epoch 440, training loss: 7.045012474060059 = 0.6534669399261475 + 1.0 * 6.391545295715332
Epoch 440, val loss: 0.9305225014686584
Epoch 450, training loss: 7.0133562088012695 = 0.6247926950454712 + 1.0 * 6.388563632965088
Epoch 450, val loss: 0.9108641743659973
Epoch 460, training loss: 6.984172344207764 = 0.597192645072937 + 1.0 * 6.386979579925537
Epoch 460, val loss: 0.8925440907478333
Epoch 470, training loss: 6.964024543762207 = 0.5707612037658691 + 1.0 * 6.393263339996338
Epoch 470, val loss: 0.8755489587783813
Epoch 480, training loss: 6.927713394165039 = 0.545496940612793 + 1.0 * 6.382216453552246
Epoch 480, val loss: 0.8601131439208984
Epoch 490, training loss: 6.899580001831055 = 0.5211308002471924 + 1.0 * 6.378449440002441
Epoch 490, val loss: 0.8459203243255615
Epoch 500, training loss: 6.876613140106201 = 0.49739184975624084 + 1.0 * 6.379221439361572
Epoch 500, val loss: 0.8327985405921936
Epoch 510, training loss: 6.8501152992248535 = 0.4744170606136322 + 1.0 * 6.375698089599609
Epoch 510, val loss: 0.8207664489746094
Epoch 520, training loss: 6.83013391494751 = 0.45187249779701233 + 1.0 * 6.378261566162109
Epoch 520, val loss: 0.8098617196083069
Epoch 530, training loss: 6.802512168884277 = 0.43017399311065674 + 1.0 * 6.37233829498291
Epoch 530, val loss: 0.7997795939445496
Epoch 540, training loss: 6.776488304138184 = 0.40906810760498047 + 1.0 * 6.367420196533203
Epoch 540, val loss: 0.7906990051269531
Epoch 550, training loss: 6.753789901733398 = 0.3884323835372925 + 1.0 * 6.365357398986816
Epoch 550, val loss: 0.7822744250297546
Epoch 560, training loss: 6.73336124420166 = 0.3682744801044464 + 1.0 * 6.365086555480957
Epoch 560, val loss: 0.7745897173881531
Epoch 570, training loss: 6.722929000854492 = 0.3489011228084564 + 1.0 * 6.374027729034424
Epoch 570, val loss: 0.7675092816352844
Epoch 580, training loss: 6.692270755767822 = 0.3303866982460022 + 1.0 * 6.361884117126465
Epoch 580, val loss: 0.7612725496292114
Epoch 590, training loss: 6.671635627746582 = 0.3126503825187683 + 1.0 * 6.358985424041748
Epoch 590, val loss: 0.7556374669075012
Epoch 600, training loss: 6.652684211730957 = 0.2956831455230713 + 1.0 * 6.357001304626465
Epoch 600, val loss: 0.7506779432296753
Epoch 610, training loss: 6.647601127624512 = 0.27950185537338257 + 1.0 * 6.368099212646484
Epoch 610, val loss: 0.7463743686676025
Epoch 620, training loss: 6.618891716003418 = 0.26420801877975464 + 1.0 * 6.354683876037598
Epoch 620, val loss: 0.7426556348800659
Epoch 630, training loss: 6.604658126831055 = 0.24974125623703003 + 1.0 * 6.354917049407959
Epoch 630, val loss: 0.7395780086517334
Epoch 640, training loss: 6.586903095245361 = 0.23605769872665405 + 1.0 * 6.3508453369140625
Epoch 640, val loss: 0.7371323704719543
Epoch 650, training loss: 6.5723419189453125 = 0.22302259504795074 + 1.0 * 6.3493194580078125
Epoch 650, val loss: 0.7352596521377563
Epoch 660, training loss: 6.568731307983398 = 0.21065212786197662 + 1.0 * 6.358078956604004
Epoch 660, val loss: 0.7339954376220703
Epoch 670, training loss: 6.546223163604736 = 0.19894273579120636 + 1.0 * 6.347280502319336
Epoch 670, val loss: 0.733161985874176
Epoch 680, training loss: 6.532856464385986 = 0.18788501620292664 + 1.0 * 6.344971656799316
Epoch 680, val loss: 0.7329743504524231
Epoch 690, training loss: 6.524004936218262 = 0.17742253839969635 + 1.0 * 6.346582412719727
Epoch 690, val loss: 0.7334046363830566
Epoch 700, training loss: 6.516465663909912 = 0.16755244135856628 + 1.0 * 6.348913192749023
Epoch 700, val loss: 0.7342742085456848
Epoch 710, training loss: 6.498813629150391 = 0.15830866992473602 + 1.0 * 6.340505123138428
Epoch 710, val loss: 0.7356946468353271
Epoch 720, training loss: 6.489083290100098 = 0.1496095061302185 + 1.0 * 6.339473724365234
Epoch 720, val loss: 0.7375855445861816
Epoch 730, training loss: 6.487328052520752 = 0.1414259523153305 + 1.0 * 6.345901966094971
Epoch 730, val loss: 0.739933967590332
Epoch 740, training loss: 6.474302291870117 = 0.13377617299556732 + 1.0 * 6.340526103973389
Epoch 740, val loss: 0.7426276206970215
Epoch 750, training loss: 6.463010311126709 = 0.12663190066814423 + 1.0 * 6.336378574371338
Epoch 750, val loss: 0.7456682324409485
Epoch 760, training loss: 6.453709125518799 = 0.11993958055973053 + 1.0 * 6.33376932144165
Epoch 760, val loss: 0.7490790486335754
Epoch 770, training loss: 6.453502178192139 = 0.11364921182394028 + 1.0 * 6.339852809906006
Epoch 770, val loss: 0.7528740167617798
Epoch 780, training loss: 6.445621013641357 = 0.10777100175619125 + 1.0 * 6.337850093841553
Epoch 780, val loss: 0.7567879557609558
Epoch 790, training loss: 6.432620525360107 = 0.10226806998252869 + 1.0 * 6.330352306365967
Epoch 790, val loss: 0.7611141800880432
Epoch 800, training loss: 6.434971809387207 = 0.09710049629211426 + 1.0 * 6.337871551513672
Epoch 800, val loss: 0.7656726837158203
Epoch 810, training loss: 6.423240661621094 = 0.09228943288326263 + 1.0 * 6.33095121383667
Epoch 810, val loss: 0.7704415321350098
Epoch 820, training loss: 6.417788505554199 = 0.08774358779191971 + 1.0 * 6.330044746398926
Epoch 820, val loss: 0.7753525376319885
Epoch 830, training loss: 6.4117431640625 = 0.08349955081939697 + 1.0 * 6.328243732452393
Epoch 830, val loss: 0.7804309129714966
Epoch 840, training loss: 6.405978679656982 = 0.07950521260499954 + 1.0 * 6.326473236083984
Epoch 840, val loss: 0.785584032535553
Epoch 850, training loss: 6.405539512634277 = 0.07576737552881241 + 1.0 * 6.329771995544434
Epoch 850, val loss: 0.790928304195404
Epoch 860, training loss: 6.395753860473633 = 0.07225952297449112 + 1.0 * 6.3234944343566895
Epoch 860, val loss: 0.7963465452194214
Epoch 870, training loss: 6.3911333084106445 = 0.06895072013139725 + 1.0 * 6.322182655334473
Epoch 870, val loss: 0.8018350601196289
Epoch 880, training loss: 6.389883041381836 = 0.06583864986896515 + 1.0 * 6.324044227600098
Epoch 880, val loss: 0.8074597120285034
Epoch 890, training loss: 6.385378837585449 = 0.06290734559297562 + 1.0 * 6.322471618652344
Epoch 890, val loss: 0.8130999803543091
Epoch 900, training loss: 6.38062047958374 = 0.060148511081933975 + 1.0 * 6.32047176361084
Epoch 900, val loss: 0.8187705874443054
Epoch 910, training loss: 6.377438545227051 = 0.05754757300019264 + 1.0 * 6.319890975952148
Epoch 910, val loss: 0.824487566947937
Epoch 920, training loss: 6.375414848327637 = 0.055105045437812805 + 1.0 * 6.320309638977051
Epoch 920, val loss: 0.8302236795425415
Epoch 930, training loss: 6.369113445281982 = 0.05280536413192749 + 1.0 * 6.31630802154541
Epoch 930, val loss: 0.8359370231628418
Epoch 940, training loss: 6.368378639221191 = 0.050630878657102585 + 1.0 * 6.317747592926025
Epoch 940, val loss: 0.8416763544082642
Epoch 950, training loss: 6.3638386726379395 = 0.048582252115011215 + 1.0 * 6.315256595611572
Epoch 950, val loss: 0.8473712205886841
Epoch 960, training loss: 6.361508846282959 = 0.04665178805589676 + 1.0 * 6.314857006072998
Epoch 960, val loss: 0.8530530333518982
Epoch 970, training loss: 6.363068103790283 = 0.04482699930667877 + 1.0 * 6.318241119384766
Epoch 970, val loss: 0.8586536049842834
Epoch 980, training loss: 6.354748249053955 = 0.04310934618115425 + 1.0 * 6.311638832092285
Epoch 980, val loss: 0.8642995953559875
Epoch 990, training loss: 6.352841854095459 = 0.041475698351860046 + 1.0 * 6.311366081237793
Epoch 990, val loss: 0.8698927164077759
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 10.550806045532227 = 1.9539594650268555 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9511178731918335
Epoch 10, training loss: 10.540084838867188 = 1.9434692859649658 + 1.0 * 8.5966157913208
Epoch 10, val loss: 1.9412446022033691
Epoch 20, training loss: 10.52562141418457 = 1.9304955005645752 + 1.0 * 8.595126152038574
Epoch 20, val loss: 1.9286634922027588
Epoch 30, training loss: 10.496999740600586 = 1.912612795829773 + 1.0 * 8.584386825561523
Epoch 30, val loss: 1.910944938659668
Epoch 40, training loss: 10.408430099487305 = 1.8881182670593262 + 1.0 * 8.52031135559082
Epoch 40, val loss: 1.8871647119522095
Epoch 50, training loss: 10.07892894744873 = 1.8621200323104858 + 1.0 * 8.216809272766113
Epoch 50, val loss: 1.862895131111145
Epoch 60, training loss: 9.805818557739258 = 1.8401472568511963 + 1.0 * 7.965671539306641
Epoch 60, val loss: 1.8428372144699097
Epoch 70, training loss: 9.375789642333984 = 1.8233380317687988 + 1.0 * 7.5524516105651855
Epoch 70, val loss: 1.8274461030960083
Epoch 80, training loss: 8.940547943115234 = 1.8110673427581787 + 1.0 * 7.129480361938477
Epoch 80, val loss: 1.8165539503097534
Epoch 90, training loss: 8.786577224731445 = 1.7977557182312012 + 1.0 * 6.988821983337402
Epoch 90, val loss: 1.8048220872879028
Epoch 100, training loss: 8.658170700073242 = 1.7826296091079712 + 1.0 * 6.875540733337402
Epoch 100, val loss: 1.7919337749481201
Epoch 110, training loss: 8.57424259185791 = 1.7697714567184448 + 1.0 * 6.804471015930176
Epoch 110, val loss: 1.7803685665130615
Epoch 120, training loss: 8.506574630737305 = 1.758114218711853 + 1.0 * 6.74846076965332
Epoch 120, val loss: 1.769118309020996
Epoch 130, training loss: 8.448612213134766 = 1.745408535003662 + 1.0 * 6.7032036781311035
Epoch 130, val loss: 1.7571661472320557
Epoch 140, training loss: 8.394256591796875 = 1.7311581373214722 + 1.0 * 6.6630988121032715
Epoch 140, val loss: 1.7443917989730835
Epoch 150, training loss: 8.346580505371094 = 1.7152917385101318 + 1.0 * 6.631288528442383
Epoch 150, val loss: 1.7304044961929321
Epoch 160, training loss: 8.301957130432129 = 1.6971802711486816 + 1.0 * 6.604776859283447
Epoch 160, val loss: 1.7144917249679565
Epoch 170, training loss: 8.258695602416992 = 1.6763395071029663 + 1.0 * 6.5823564529418945
Epoch 170, val loss: 1.6962743997573853
Epoch 180, training loss: 8.216703414916992 = 1.6527652740478516 + 1.0 * 6.563938140869141
Epoch 180, val loss: 1.6757676601409912
Epoch 190, training loss: 8.175114631652832 = 1.6261497735977173 + 1.0 * 6.548964977264404
Epoch 190, val loss: 1.6526442766189575
Epoch 200, training loss: 8.129952430725098 = 1.5963842868804932 + 1.0 * 6.533567905426025
Epoch 200, val loss: 1.6267576217651367
Epoch 210, training loss: 8.084989547729492 = 1.5635241270065308 + 1.0 * 6.521465301513672
Epoch 210, val loss: 1.598250389099121
Epoch 220, training loss: 8.041913032531738 = 1.527870774269104 + 1.0 * 6.514041900634766
Epoch 220, val loss: 1.567527174949646
Epoch 230, training loss: 7.9927239418029785 = 1.4905256032943726 + 1.0 * 6.502198219299316
Epoch 230, val loss: 1.5357675552368164
Epoch 240, training loss: 7.944464683532715 = 1.4524664878845215 + 1.0 * 6.491998195648193
Epoch 240, val loss: 1.5038952827453613
Epoch 250, training loss: 7.895860195159912 = 1.4139519929885864 + 1.0 * 6.481908321380615
Epoch 250, val loss: 1.4722814559936523
Epoch 260, training loss: 7.849806785583496 = 1.3752434253692627 + 1.0 * 6.474563121795654
Epoch 260, val loss: 1.441216230392456
Epoch 270, training loss: 7.8105878829956055 = 1.336901307106018 + 1.0 * 6.473686695098877
Epoch 270, val loss: 1.4113125801086426
Epoch 280, training loss: 7.759035587310791 = 1.2992349863052368 + 1.0 * 6.459800720214844
Epoch 280, val loss: 1.382509708404541
Epoch 290, training loss: 7.714705467224121 = 1.2615480422973633 + 1.0 * 6.453157424926758
Epoch 290, val loss: 1.3542362451553345
Epoch 300, training loss: 7.6723504066467285 = 1.223443627357483 + 1.0 * 6.448906898498535
Epoch 300, val loss: 1.3262155055999756
Epoch 310, training loss: 7.628070831298828 = 1.1853983402252197 + 1.0 * 6.4426727294921875
Epoch 310, val loss: 1.2986263036727905
Epoch 320, training loss: 7.586831092834473 = 1.1475943326950073 + 1.0 * 6.439236640930176
Epoch 320, val loss: 1.2716410160064697
Epoch 330, training loss: 7.542156219482422 = 1.1098642349243164 + 1.0 * 6.4322919845581055
Epoch 330, val loss: 1.2448979616165161
Epoch 340, training loss: 7.506267547607422 = 1.0721657276153564 + 1.0 * 6.4341020584106445
Epoch 340, val loss: 1.2182118892669678
Epoch 350, training loss: 7.460850238800049 = 1.0351238250732422 + 1.0 * 6.425726413726807
Epoch 350, val loss: 1.1921484470367432
Epoch 360, training loss: 7.4187517166137695 = 0.9983727931976318 + 1.0 * 6.420379161834717
Epoch 360, val loss: 1.1663808822631836
Epoch 370, training loss: 7.379706382751465 = 0.9618680477142334 + 1.0 * 6.4178385734558105
Epoch 370, val loss: 1.1405715942382812
Epoch 380, training loss: 7.343381881713867 = 0.9258142113685608 + 1.0 * 6.417567729949951
Epoch 380, val loss: 1.1149725914001465
Epoch 390, training loss: 7.299701690673828 = 0.8904434442520142 + 1.0 * 6.4092583656311035
Epoch 390, val loss: 1.089788794517517
Epoch 400, training loss: 7.261746883392334 = 0.8554912805557251 + 1.0 * 6.406255722045898
Epoch 400, val loss: 1.064788818359375
Epoch 410, training loss: 7.223318099975586 = 0.8208323121070862 + 1.0 * 6.4024858474731445
Epoch 410, val loss: 1.0399916172027588
Epoch 420, training loss: 7.185889720916748 = 0.7867079973220825 + 1.0 * 6.399181842803955
Epoch 420, val loss: 1.0155754089355469
Epoch 430, training loss: 7.1507744789123535 = 0.7536268830299377 + 1.0 * 6.3971476554870605
Epoch 430, val loss: 0.9920077919960022
Epoch 440, training loss: 7.1154279708862305 = 0.7215057611465454 + 1.0 * 6.393922328948975
Epoch 440, val loss: 0.9694575667381287
Epoch 450, training loss: 7.087985038757324 = 0.6903995275497437 + 1.0 * 6.397585391998291
Epoch 450, val loss: 0.9478171467781067
Epoch 460, training loss: 7.0491943359375 = 0.660919189453125 + 1.0 * 6.388275146484375
Epoch 460, val loss: 0.9278080463409424
Epoch 470, training loss: 7.020003318786621 = 0.6330687403678894 + 1.0 * 6.386934757232666
Epoch 470, val loss: 0.909792959690094
Epoch 480, training loss: 6.98906946182251 = 0.6066479086875916 + 1.0 * 6.382421493530273
Epoch 480, val loss: 0.8933017253875732
Epoch 490, training loss: 6.976642608642578 = 0.5815864205360413 + 1.0 * 6.395056247711182
Epoch 490, val loss: 0.8783289790153503
Epoch 500, training loss: 6.935918807983398 = 0.5581774115562439 + 1.0 * 6.37774133682251
Epoch 500, val loss: 0.8651662468910217
Epoch 510, training loss: 6.910606861114502 = 0.536069929599762 + 1.0 * 6.374536991119385
Epoch 510, val loss: 0.8535079956054688
Epoch 520, training loss: 6.904199600219727 = 0.5151231288909912 + 1.0 * 6.389076232910156
Epoch 520, val loss: 0.8431274890899658
Epoch 530, training loss: 6.867133140563965 = 0.49524834752082825 + 1.0 * 6.371884822845459
Epoch 530, val loss: 0.8339945673942566
Epoch 540, training loss: 6.844393253326416 = 0.47626811265945435 + 1.0 * 6.368124961853027
Epoch 540, val loss: 0.8257476091384888
Epoch 550, training loss: 6.824179649353027 = 0.45794999599456787 + 1.0 * 6.36622953414917
Epoch 550, val loss: 0.8182569146156311
Epoch 560, training loss: 6.810494422912598 = 0.4401320815086365 + 1.0 * 6.370362281799316
Epoch 560, val loss: 0.811348021030426
Epoch 570, training loss: 6.78608512878418 = 0.42278963327407837 + 1.0 * 6.363295555114746
Epoch 570, val loss: 0.8050435781478882
Epoch 580, training loss: 6.766393661499023 = 0.40572792291641235 + 1.0 * 6.360665798187256
Epoch 580, val loss: 0.7990774512290955
Epoch 590, training loss: 6.751147270202637 = 0.3888591229915619 + 1.0 * 6.362287998199463
Epoch 590, val loss: 0.793327271938324
Epoch 600, training loss: 6.729727268218994 = 0.372189462184906 + 1.0 * 6.357537746429443
Epoch 600, val loss: 0.7879208326339722
Epoch 610, training loss: 6.710136413574219 = 0.35561996698379517 + 1.0 * 6.354516506195068
Epoch 610, val loss: 0.7827092409133911
Epoch 620, training loss: 6.696380138397217 = 0.3391622304916382 + 1.0 * 6.357217788696289
Epoch 620, val loss: 0.777698278427124
Epoch 630, training loss: 6.686269283294678 = 0.3229612708091736 + 1.0 * 6.363307952880859
Epoch 630, val loss: 0.7729010581970215
Epoch 640, training loss: 6.660285949707031 = 0.30722206830978394 + 1.0 * 6.353064060211182
Epoch 640, val loss: 0.7687222361564636
Epoch 650, training loss: 6.640909671783447 = 0.291864275932312 + 1.0 * 6.349045276641846
Epoch 650, val loss: 0.7648667693138123
Epoch 660, training loss: 6.626202583312988 = 0.27693653106689453 + 1.0 * 6.349266052246094
Epoch 660, val loss: 0.7615495920181274
Epoch 670, training loss: 6.610149383544922 = 0.2625858187675476 + 1.0 * 6.347563743591309
Epoch 670, val loss: 0.7585573792457581
Epoch 680, training loss: 6.5941877365112305 = 0.24887815117835999 + 1.0 * 6.345309734344482
Epoch 680, val loss: 0.7562974095344543
Epoch 690, training loss: 6.5776848793029785 = 0.23573164641857147 + 1.0 * 6.341953277587891
Epoch 690, val loss: 0.7545412182807922
Epoch 700, training loss: 6.581932067871094 = 0.22314590215682983 + 1.0 * 6.358786106109619
Epoch 700, val loss: 0.7532307505607605
Epoch 710, training loss: 6.550507545471191 = 0.21128001809120178 + 1.0 * 6.339227676391602
Epoch 710, val loss: 0.7524857521057129
Epoch 720, training loss: 6.539071559906006 = 0.19999393820762634 + 1.0 * 6.339077472686768
Epoch 720, val loss: 0.7523431777954102
Epoch 730, training loss: 6.536738395690918 = 0.18927143514156342 + 1.0 * 6.347466945648193
Epoch 730, val loss: 0.7525083422660828
Epoch 740, training loss: 6.519545555114746 = 0.17911425232887268 + 1.0 * 6.340431213378906
Epoch 740, val loss: 0.7531067728996277
Epoch 750, training loss: 6.505756855010986 = 0.16956838965415955 + 1.0 * 6.336188316345215
Epoch 750, val loss: 0.7542621493339539
Epoch 760, training loss: 6.499707221984863 = 0.16051936149597168 + 1.0 * 6.339188098907471
Epoch 760, val loss: 0.7556855082511902
Epoch 770, training loss: 6.487101078033447 = 0.15204238891601562 + 1.0 * 6.335058689117432
Epoch 770, val loss: 0.7575279474258423
Epoch 780, training loss: 6.4811811447143555 = 0.1440524458885193 + 1.0 * 6.337128639221191
Epoch 780, val loss: 0.759783148765564
Epoch 790, training loss: 6.469942092895508 = 0.13659259676933289 + 1.0 * 6.333349704742432
Epoch 790, val loss: 0.7621464133262634
Epoch 800, training loss: 6.458399295806885 = 0.12957754731178284 + 1.0 * 6.328821659088135
Epoch 800, val loss: 0.7650520205497742
Epoch 810, training loss: 6.451086044311523 = 0.12298593670129776 + 1.0 * 6.328100204467773
Epoch 810, val loss: 0.768091082572937
Epoch 820, training loss: 6.455434799194336 = 0.11679989099502563 + 1.0 * 6.338634967803955
Epoch 820, val loss: 0.7711984515190125
Epoch 830, training loss: 6.442084312438965 = 0.11102069914340973 + 1.0 * 6.331063747406006
Epoch 830, val loss: 0.7746046781539917
Epoch 840, training loss: 6.429996967315674 = 0.10561053454875946 + 1.0 * 6.3243865966796875
Epoch 840, val loss: 0.7783234715461731
Epoch 850, training loss: 6.423285007476807 = 0.10051091760396957 + 1.0 * 6.3227739334106445
Epoch 850, val loss: 0.7821188569068909
Epoch 860, training loss: 6.4228949546813965 = 0.09570372104644775 + 1.0 * 6.327191352844238
Epoch 860, val loss: 0.7860905528068542
Epoch 870, training loss: 6.412709712982178 = 0.09117944538593292 + 1.0 * 6.321530342102051
Epoch 870, val loss: 0.7900187373161316
Epoch 880, training loss: 6.411203861236572 = 0.08693905174732208 + 1.0 * 6.324265003204346
Epoch 880, val loss: 0.7941639423370361
Epoch 890, training loss: 6.403118133544922 = 0.08294162899255753 + 1.0 * 6.320176601409912
Epoch 890, val loss: 0.7983105778694153
Epoch 900, training loss: 6.398603916168213 = 0.07917725294828415 + 1.0 * 6.319426536560059
Epoch 900, val loss: 0.8026324510574341
Epoch 910, training loss: 6.392673015594482 = 0.07561910152435303 + 1.0 * 6.31705379486084
Epoch 910, val loss: 0.8069949746131897
Epoch 920, training loss: 6.404409885406494 = 0.07226330041885376 + 1.0 * 6.332146644592285
Epoch 920, val loss: 0.8113629817962646
Epoch 930, training loss: 6.3877949714660645 = 0.06910771131515503 + 1.0 * 6.318687438964844
Epoch 930, val loss: 0.815714418888092
Epoch 940, training loss: 6.382902145385742 = 0.06614670157432556 + 1.0 * 6.316755294799805
Epoch 940, val loss: 0.8203433156013489
Epoch 950, training loss: 6.380263328552246 = 0.06334763765335083 + 1.0 * 6.316915512084961
Epoch 950, val loss: 0.824916422367096
Epoch 960, training loss: 6.373723030090332 = 0.06069634482264519 + 1.0 * 6.3130269050598145
Epoch 960, val loss: 0.8294236063957214
Epoch 970, training loss: 6.375798225402832 = 0.05818462371826172 + 1.0 * 6.31761360168457
Epoch 970, val loss: 0.8340365886688232
Epoch 980, training loss: 6.367199897766113 = 0.055815439671278 + 1.0 * 6.311384677886963
Epoch 980, val loss: 0.8387534022331238
Epoch 990, training loss: 6.3641886711120605 = 0.05357056483626366 + 1.0 * 6.310617923736572
Epoch 990, val loss: 0.8434181809425354
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.819715340010543
=== training gcn model ===
Epoch 0, training loss: 10.546911239624023 = 1.9501184225082397 + 1.0 * 8.596793174743652
Epoch 0, val loss: 1.9530811309814453
Epoch 10, training loss: 10.536919593811035 = 1.9405361413955688 + 1.0 * 8.596383094787598
Epoch 10, val loss: 1.9436864852905273
Epoch 20, training loss: 10.522058486938477 = 1.928555965423584 + 1.0 * 8.59350299835205
Epoch 20, val loss: 1.9314943552017212
Epoch 30, training loss: 10.485607147216797 = 1.911747932434082 + 1.0 * 8.573859214782715
Epoch 30, val loss: 1.9138405323028564
Epoch 40, training loss: 10.33487319946289 = 1.8903852701187134 + 1.0 * 8.444487571716309
Epoch 40, val loss: 1.8922233581542969
Epoch 50, training loss: 9.816808700561523 = 1.868226408958435 + 1.0 * 7.948582649230957
Epoch 50, val loss: 1.8704537153244019
Epoch 60, training loss: 9.291296005249023 = 1.8533399105072021 + 1.0 * 7.437955856323242
Epoch 60, val loss: 1.8570787906646729
Epoch 70, training loss: 8.99567985534668 = 1.842423677444458 + 1.0 * 7.153255939483643
Epoch 70, val loss: 1.846618413925171
Epoch 80, training loss: 8.81443977355957 = 1.8282451629638672 + 1.0 * 6.986195087432861
Epoch 80, val loss: 1.8326665163040161
Epoch 90, training loss: 8.712483406066895 = 1.8113510608673096 + 1.0 * 6.901132583618164
Epoch 90, val loss: 1.8162840604782104
Epoch 100, training loss: 8.626664161682129 = 1.7954994440078735 + 1.0 * 6.831164836883545
Epoch 100, val loss: 1.8009881973266602
Epoch 110, training loss: 8.553605079650879 = 1.782196283340454 + 1.0 * 6.771408557891846
Epoch 110, val loss: 1.7881051301956177
Epoch 120, training loss: 8.493350982666016 = 1.7698017358779907 + 1.0 * 6.723548889160156
Epoch 120, val loss: 1.7762880325317383
Epoch 130, training loss: 8.438892364501953 = 1.7571333646774292 + 1.0 * 6.681758880615234
Epoch 130, val loss: 1.7644288539886475
Epoch 140, training loss: 8.389976501464844 = 1.7432658672332764 + 1.0 * 6.6467108726501465
Epoch 140, val loss: 1.751806616783142
Epoch 150, training loss: 8.346553802490234 = 1.7274627685546875 + 1.0 * 6.619091510772705
Epoch 150, val loss: 1.7376251220703125
Epoch 160, training loss: 8.302367210388184 = 1.7090758085250854 + 1.0 * 6.593291759490967
Epoch 160, val loss: 1.7214267253875732
Epoch 170, training loss: 8.258691787719727 = 1.6877403259277344 + 1.0 * 6.57095193862915
Epoch 170, val loss: 1.7028318643569946
Epoch 180, training loss: 8.218340873718262 = 1.6627284288406372 + 1.0 * 6.555612087249756
Epoch 180, val loss: 1.681257963180542
Epoch 190, training loss: 8.171863555908203 = 1.6339479684829712 + 1.0 * 6.5379157066345215
Epoch 190, val loss: 1.6566249132156372
Epoch 200, training loss: 8.124539375305176 = 1.6011019945144653 + 1.0 * 6.5234375
Epoch 200, val loss: 1.6286194324493408
Epoch 210, training loss: 8.080881118774414 = 1.563981533050537 + 1.0 * 6.516900062561035
Epoch 210, val loss: 1.5973244905471802
Epoch 220, training loss: 8.025566101074219 = 1.5236793756484985 + 1.0 * 6.50188684463501
Epoch 220, val loss: 1.5636826753616333
Epoch 230, training loss: 7.971188068389893 = 1.4802570343017578 + 1.0 * 6.490931034088135
Epoch 230, val loss: 1.5281226634979248
Epoch 240, training loss: 7.915860652923584 = 1.4340242147445679 + 1.0 * 6.481836318969727
Epoch 240, val loss: 1.4912687540054321
Epoch 250, training loss: 7.861252784729004 = 1.3863085508346558 + 1.0 * 6.474944114685059
Epoch 250, val loss: 1.4542295932769775
Epoch 260, training loss: 7.805322170257568 = 1.3384157419204712 + 1.0 * 6.466906547546387
Epoch 260, val loss: 1.4178202152252197
Epoch 270, training loss: 7.75021505355835 = 1.2902542352676392 + 1.0 * 6.4599609375
Epoch 270, val loss: 1.3820114135742188
Epoch 280, training loss: 7.695931434631348 = 1.24235999584198 + 1.0 * 6.453571319580078
Epoch 280, val loss: 1.346953272819519
Epoch 290, training loss: 7.642787456512451 = 1.1952401399612427 + 1.0 * 6.447547435760498
Epoch 290, val loss: 1.3128389120101929
Epoch 300, training loss: 7.5901899337768555 = 1.1491916179656982 + 1.0 * 6.440998077392578
Epoch 300, val loss: 1.280000925064087
Epoch 310, training loss: 7.538802146911621 = 1.1047453880310059 + 1.0 * 6.434056758880615
Epoch 310, val loss: 1.2486683130264282
Epoch 320, training loss: 7.4901628494262695 = 1.0620921850204468 + 1.0 * 6.428070545196533
Epoch 320, val loss: 1.2192171812057495
Epoch 330, training loss: 7.44838809967041 = 1.021465539932251 + 1.0 * 6.42692232131958
Epoch 330, val loss: 1.191923975944519
Epoch 340, training loss: 7.401678085327148 = 0.9833154082298279 + 1.0 * 6.418362617492676
Epoch 340, val loss: 1.1669220924377441
Epoch 350, training loss: 7.360804557800293 = 0.9469490647315979 + 1.0 * 6.41385555267334
Epoch 350, val loss: 1.1437652111053467
Epoch 360, training loss: 7.321988582611084 = 0.9119343757629395 + 1.0 * 6.4100542068481445
Epoch 360, val loss: 1.1220197677612305
Epoch 370, training loss: 7.284251689910889 = 0.8781517744064331 + 1.0 * 6.406099796295166
Epoch 370, val loss: 1.101752758026123
Epoch 380, training loss: 7.246984481811523 = 0.8451442122459412 + 1.0 * 6.4018402099609375
Epoch 380, val loss: 1.0823684930801392
Epoch 390, training loss: 7.216348648071289 = 0.8127648830413818 + 1.0 * 6.403584003448486
Epoch 390, val loss: 1.0638326406478882
Epoch 400, training loss: 7.175503253936768 = 0.7810126543045044 + 1.0 * 6.394490718841553
Epoch 400, val loss: 1.046148657798767
Epoch 410, training loss: 7.140181064605713 = 0.7493903040885925 + 1.0 * 6.390790939331055
Epoch 410, val loss: 1.0289928913116455
Epoch 420, training loss: 7.105785846710205 = 0.7175837159156799 + 1.0 * 6.38820219039917
Epoch 420, val loss: 1.0122218132019043
Epoch 430, training loss: 7.074512958526611 = 0.685725748538971 + 1.0 * 6.388787269592285
Epoch 430, val loss: 0.9959537982940674
Epoch 440, training loss: 7.039900779724121 = 0.6542060375213623 + 1.0 * 6.38569450378418
Epoch 440, val loss: 0.9804621934890747
Epoch 450, training loss: 7.003702640533447 = 0.6228753924369812 + 1.0 * 6.3808274269104
Epoch 450, val loss: 0.9656943082809448
Epoch 460, training loss: 6.98359489440918 = 0.591884970664978 + 1.0 * 6.391709804534912
Epoch 460, val loss: 0.951651930809021
Epoch 470, training loss: 6.938337802886963 = 0.5618149042129517 + 1.0 * 6.376523017883301
Epoch 470, val loss: 0.9387586116790771
Epoch 480, training loss: 6.906146049499512 = 0.5327553153038025 + 1.0 * 6.3733906745910645
Epoch 480, val loss: 0.9271552562713623
Epoch 490, training loss: 6.876105785369873 = 0.5046920776367188 + 1.0 * 6.371413707733154
Epoch 490, val loss: 0.916803240776062
Epoch 500, training loss: 6.847623825073242 = 0.4778066575527191 + 1.0 * 6.36981725692749
Epoch 500, val loss: 0.9077833890914917
Epoch 510, training loss: 6.821453094482422 = 0.45227083563804626 + 1.0 * 6.369182109832764
Epoch 510, val loss: 0.9002817869186401
Epoch 520, training loss: 6.794780731201172 = 0.4279191792011261 + 1.0 * 6.366861343383789
Epoch 520, val loss: 0.8941464424133301
Epoch 530, training loss: 6.771273612976074 = 0.40481311082839966 + 1.0 * 6.36646032333374
Epoch 530, val loss: 0.8893033266067505
Epoch 540, training loss: 6.745426654815674 = 0.3829667866230011 + 1.0 * 6.362459659576416
Epoch 540, val loss: 0.8858391046524048
Epoch 550, training loss: 6.723101615905762 = 0.362162709236145 + 1.0 * 6.360939025878906
Epoch 550, val loss: 0.8835837841033936
Epoch 560, training loss: 6.706427574157715 = 0.3423439860343933 + 1.0 * 6.364083766937256
Epoch 560, val loss: 0.8822528123855591
Epoch 570, training loss: 6.681352615356445 = 0.32352522015571594 + 1.0 * 6.357827186584473
Epoch 570, val loss: 0.8820955157279968
Epoch 580, training loss: 6.660646438598633 = 0.3055459260940552 + 1.0 * 6.355100631713867
Epoch 580, val loss: 0.8828279972076416
Epoch 590, training loss: 6.642287731170654 = 0.28834962844848633 + 1.0 * 6.353938102722168
Epoch 590, val loss: 0.8845552802085876
Epoch 600, training loss: 6.628186225891113 = 0.2719343304634094 + 1.0 * 6.3562517166137695
Epoch 600, val loss: 0.8870557546615601
Epoch 610, training loss: 6.6085991859436035 = 0.2563215494155884 + 1.0 * 6.352277755737305
Epoch 610, val loss: 0.8905001878738403
Epoch 620, training loss: 6.591421604156494 = 0.24147707223892212 + 1.0 * 6.349944591522217
Epoch 620, val loss: 0.8947142362594604
Epoch 630, training loss: 6.57545804977417 = 0.22740529477596283 + 1.0 * 6.348052978515625
Epoch 630, val loss: 0.899562656879425
Epoch 640, training loss: 6.565342426300049 = 0.21408751606941223 + 1.0 * 6.351254940032959
Epoch 640, val loss: 0.9050649404525757
Epoch 650, training loss: 6.548343181610107 = 0.20160019397735596 + 1.0 * 6.346743106842041
Epoch 650, val loss: 0.9113691449165344
Epoch 660, training loss: 6.532705307006836 = 0.1898120492696762 + 1.0 * 6.342893123626709
Epoch 660, val loss: 0.9181371331214905
Epoch 670, training loss: 6.523096561431885 = 0.17873376607894897 + 1.0 * 6.344362735748291
Epoch 670, val loss: 0.9254918098449707
Epoch 680, training loss: 6.509940147399902 = 0.16836123168468475 + 1.0 * 6.341578960418701
Epoch 680, val loss: 0.9331129789352417
Epoch 690, training loss: 6.502575397491455 = 0.1586734801530838 + 1.0 * 6.343902111053467
Epoch 690, val loss: 0.9412828087806702
Epoch 700, training loss: 6.488287448883057 = 0.14962886273860931 + 1.0 * 6.338658809661865
Epoch 700, val loss: 0.9494085311889648
Epoch 710, training loss: 6.478024959564209 = 0.14121314883232117 + 1.0 * 6.3368120193481445
Epoch 710, val loss: 0.9581112265586853
Epoch 720, training loss: 6.467369079589844 = 0.13332219421863556 + 1.0 * 6.334046840667725
Epoch 720, val loss: 0.9669404029846191
Epoch 730, training loss: 6.465188026428223 = 0.12594477832317352 + 1.0 * 6.339243412017822
Epoch 730, val loss: 0.9759945273399353
Epoch 740, training loss: 6.459921360015869 = 0.11909187585115433 + 1.0 * 6.340829372406006
Epoch 740, val loss: 0.98482745885849
Epoch 750, training loss: 6.446340084075928 = 0.11274250596761703 + 1.0 * 6.333597660064697
Epoch 750, val loss: 0.9941972494125366
Epoch 760, training loss: 6.435990333557129 = 0.10680245608091354 + 1.0 * 6.329187870025635
Epoch 760, val loss: 1.0034193992614746
Epoch 770, training loss: 6.429345607757568 = 0.1012473925948143 + 1.0 * 6.328098297119141
Epoch 770, val loss: 1.0127850770950317
Epoch 780, training loss: 6.430046081542969 = 0.09605295211076736 + 1.0 * 6.333992958068848
Epoch 780, val loss: 1.0219528675079346
Epoch 790, training loss: 6.417986869812012 = 0.09124527126550674 + 1.0 * 6.326741695404053
Epoch 790, val loss: 1.0315052270889282
Epoch 800, training loss: 6.411627769470215 = 0.08674144744873047 + 1.0 * 6.324886322021484
Epoch 800, val loss: 1.0409116744995117
Epoch 810, training loss: 6.407514572143555 = 0.08252038806676865 + 1.0 * 6.324994087219238
Epoch 810, val loss: 1.0502984523773193
Epoch 820, training loss: 6.403192520141602 = 0.07857154309749603 + 1.0 * 6.324621200561523
Epoch 820, val loss: 1.0594648122787476
Epoch 830, training loss: 6.399314880371094 = 0.07488854229450226 + 1.0 * 6.324426174163818
Epoch 830, val loss: 1.0687124729156494
Epoch 840, training loss: 6.393094539642334 = 0.07144772261381149 + 1.0 * 6.321646690368652
Epoch 840, val loss: 1.0778642892837524
Epoch 850, training loss: 6.388105869293213 = 0.06822759658098221 + 1.0 * 6.319878101348877
Epoch 850, val loss: 1.0871354341506958
Epoch 860, training loss: 6.387913703918457 = 0.06519361585378647 + 1.0 * 6.322720050811768
Epoch 860, val loss: 1.0961753129959106
Epoch 870, training loss: 6.379951477050781 = 0.06234479695558548 + 1.0 * 6.317606449127197
Epoch 870, val loss: 1.1051146984100342
Epoch 880, training loss: 6.377124309539795 = 0.059668175876140594 + 1.0 * 6.317456245422363
Epoch 880, val loss: 1.114140272140503
Epoch 890, training loss: 6.373079776763916 = 0.05715177208185196 + 1.0 * 6.315927982330322
Epoch 890, val loss: 1.1231077909469604
Epoch 900, training loss: 6.369271755218506 = 0.054780349135398865 + 1.0 * 6.314491271972656
Epoch 900, val loss: 1.1318840980529785
Epoch 910, training loss: 6.365475654602051 = 0.05254870280623436 + 1.0 * 6.312926769256592
Epoch 910, val loss: 1.14076828956604
Epoch 920, training loss: 6.371546268463135 = 0.050441674888134 + 1.0 * 6.321104526519775
Epoch 920, val loss: 1.1492769718170166
Epoch 930, training loss: 6.360247611999512 = 0.04847058653831482 + 1.0 * 6.311777114868164
Epoch 930, val loss: 1.1579604148864746
Epoch 940, training loss: 6.362935543060303 = 0.04660451039671898 + 1.0 * 6.316330909729004
Epoch 940, val loss: 1.1665136814117432
Epoch 950, training loss: 6.353682041168213 = 0.044844817370176315 + 1.0 * 6.308837413787842
Epoch 950, val loss: 1.1749402284622192
Epoch 960, training loss: 6.352057456970215 = 0.04317682608962059 + 1.0 * 6.308880805969238
Epoch 960, val loss: 1.1834137439727783
Epoch 970, training loss: 6.349369049072266 = 0.0415935292840004 + 1.0 * 6.307775497436523
Epoch 970, val loss: 1.1917957067489624
Epoch 980, training loss: 6.355597019195557 = 0.04008651152253151 + 1.0 * 6.315510272979736
Epoch 980, val loss: 1.199831485748291
Epoch 990, training loss: 6.347959995269775 = 0.03867680951952934 + 1.0 * 6.309283256530762
Epoch 990, val loss: 1.2079064846038818
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.816025303110174
The final CL Acc:0.75679, 0.02810, The final GNN Acc:0.81761, 0.00155
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13236])
remove edge: torch.Size([2, 7974])
updated graph: torch.Size([2, 10654])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.557884216308594 = 1.9611265659332275 + 1.0 * 8.596757888793945
Epoch 0, val loss: 1.9594148397445679
Epoch 10, training loss: 10.546817779541016 = 1.9506217241287231 + 1.0 * 8.596196174621582
Epoch 10, val loss: 1.948806881904602
Epoch 20, training loss: 10.529382705688477 = 1.9370468854904175 + 1.0 * 8.59233570098877
Epoch 20, val loss: 1.9345234632492065
Epoch 30, training loss: 10.485275268554688 = 1.917724847793579 + 1.0 * 8.567550659179688
Epoch 30, val loss: 1.9135621786117554
Epoch 40, training loss: 10.300856590270996 = 1.8937712907791138 + 1.0 * 8.407085418701172
Epoch 40, val loss: 1.8886798620224
Epoch 50, training loss: 9.619646072387695 = 1.8691589832305908 + 1.0 * 7.750486850738525
Epoch 50, val loss: 1.8636049032211304
Epoch 60, training loss: 9.15868091583252 = 1.8508704900741577 + 1.0 * 7.307810306549072
Epoch 60, val loss: 1.846778392791748
Epoch 70, training loss: 8.897174835205078 = 1.8337737321853638 + 1.0 * 7.063400745391846
Epoch 70, val loss: 1.8303179740905762
Epoch 80, training loss: 8.781171798706055 = 1.8148410320281982 + 1.0 * 6.9663310050964355
Epoch 80, val loss: 1.8123881816864014
Epoch 90, training loss: 8.700397491455078 = 1.7953637838363647 + 1.0 * 6.905034065246582
Epoch 90, val loss: 1.7940641641616821
Epoch 100, training loss: 8.633965492248535 = 1.7766979932785034 + 1.0 * 6.857267379760742
Epoch 100, val loss: 1.7775846719741821
Epoch 110, training loss: 8.56938648223877 = 1.7594021558761597 + 1.0 * 6.8099846839904785
Epoch 110, val loss: 1.7628551721572876
Epoch 120, training loss: 8.508088111877441 = 1.7423949241638184 + 1.0 * 6.765693187713623
Epoch 120, val loss: 1.7482556104660034
Epoch 130, training loss: 8.453298568725586 = 1.7240623235702515 + 1.0 * 6.729236602783203
Epoch 130, val loss: 1.7322081327438354
Epoch 140, training loss: 8.394916534423828 = 1.7033662796020508 + 1.0 * 6.691550254821777
Epoch 140, val loss: 1.7142075300216675
Epoch 150, training loss: 8.3333740234375 = 1.6797086000442505 + 1.0 * 6.653665542602539
Epoch 150, val loss: 1.693833827972412
Epoch 160, training loss: 8.271574020385742 = 1.6525087356567383 + 1.0 * 6.619065284729004
Epoch 160, val loss: 1.670518159866333
Epoch 170, training loss: 8.212244987487793 = 1.621021032333374 + 1.0 * 6.591224193572998
Epoch 170, val loss: 1.6434431076049805
Epoch 180, training loss: 8.152297019958496 = 1.5850436687469482 + 1.0 * 6.567253589630127
Epoch 180, val loss: 1.612703800201416
Epoch 190, training loss: 8.093355178833008 = 1.544579029083252 + 1.0 * 6.548776149749756
Epoch 190, val loss: 1.5781872272491455
Epoch 200, training loss: 8.035934448242188 = 1.4999057054519653 + 1.0 * 6.536028861999512
Epoch 200, val loss: 1.5403733253479004
Epoch 210, training loss: 7.972298622131348 = 1.4524253606796265 + 1.0 * 6.519873142242432
Epoch 210, val loss: 1.5004767179489136
Epoch 220, training loss: 7.910505771636963 = 1.4022417068481445 + 1.0 * 6.508264064788818
Epoch 220, val loss: 1.4587653875350952
Epoch 230, training loss: 7.847313404083252 = 1.3504739999771118 + 1.0 * 6.49683952331543
Epoch 230, val loss: 1.416487693786621
Epoch 240, training loss: 7.78510856628418 = 1.2974587678909302 + 1.0 * 6.487649917602539
Epoch 240, val loss: 1.373778223991394
Epoch 250, training loss: 7.723115921020508 = 1.2437610626220703 + 1.0 * 6.4793548583984375
Epoch 250, val loss: 1.3308405876159668
Epoch 260, training loss: 7.6596574783325195 = 1.1894752979278564 + 1.0 * 6.470182418823242
Epoch 260, val loss: 1.2878668308258057
Epoch 270, training loss: 7.599733829498291 = 1.135008454322815 + 1.0 * 6.464725494384766
Epoch 270, val loss: 1.2447513341903687
Epoch 280, training loss: 7.539085388183594 = 1.081451416015625 + 1.0 * 6.457633972167969
Epoch 280, val loss: 1.20234215259552
Epoch 290, training loss: 7.480077266693115 = 1.0292948484420776 + 1.0 * 6.450782299041748
Epoch 290, val loss: 1.1611956357955933
Epoch 300, training loss: 7.423748970031738 = 0.9785029888153076 + 1.0 * 6.44524621963501
Epoch 300, val loss: 1.1212669610977173
Epoch 310, training loss: 7.371391773223877 = 0.9299837350845337 + 1.0 * 6.441408157348633
Epoch 310, val loss: 1.0828512907028198
Epoch 320, training loss: 7.317144870758057 = 0.8836435675621033 + 1.0 * 6.433501243591309
Epoch 320, val loss: 1.0466080904006958
Epoch 330, training loss: 7.272348403930664 = 0.839340090751648 + 1.0 * 6.433008193969727
Epoch 330, val loss: 1.012098789215088
Epoch 340, training loss: 7.224733352661133 = 0.7976019978523254 + 1.0 * 6.427131175994873
Epoch 340, val loss: 0.9800935983657837
Epoch 350, training loss: 7.178316116333008 = 0.7585921287536621 + 1.0 * 6.419723987579346
Epoch 350, val loss: 0.9507951736450195
Epoch 360, training loss: 7.1370344161987305 = 0.7217727899551392 + 1.0 * 6.415261745452881
Epoch 360, val loss: 0.9238740801811218
Epoch 370, training loss: 7.098055839538574 = 0.6871165037155151 + 1.0 * 6.4109392166137695
Epoch 370, val loss: 0.8993538618087769
Epoch 380, training loss: 7.066876411437988 = 0.6548435688018799 + 1.0 * 6.412032604217529
Epoch 380, val loss: 0.8775582313537598
Epoch 390, training loss: 7.032598495483398 = 0.6252101063728333 + 1.0 * 6.407388210296631
Epoch 390, val loss: 0.858765184879303
Epoch 400, training loss: 6.99699592590332 = 0.5977789759635925 + 1.0 * 6.399217128753662
Epoch 400, val loss: 0.8426041603088379
Epoch 410, training loss: 6.973055839538574 = 0.572223961353302 + 1.0 * 6.400831699371338
Epoch 410, val loss: 0.828626811504364
Epoch 420, training loss: 6.945521354675293 = 0.5485609769821167 + 1.0 * 6.396960258483887
Epoch 420, val loss: 0.8166435360908508
Epoch 430, training loss: 6.918004035949707 = 0.5265201926231384 + 1.0 * 6.391483783721924
Epoch 430, val loss: 0.8064954280853271
Epoch 440, training loss: 6.892343044281006 = 0.5058841109275818 + 1.0 * 6.386458873748779
Epoch 440, val loss: 0.7978129386901855
Epoch 450, training loss: 6.878641605377197 = 0.48637229204177856 + 1.0 * 6.392269134521484
Epoch 450, val loss: 0.7902812361717224
Epoch 460, training loss: 6.849020004272461 = 0.4679236114025116 + 1.0 * 6.381096363067627
Epoch 460, val loss: 0.7837393283843994
Epoch 470, training loss: 6.82859992980957 = 0.45022645592689514 + 1.0 * 6.378373622894287
Epoch 470, val loss: 0.7779773473739624
Epoch 480, training loss: 6.811532497406006 = 0.4332192540168762 + 1.0 * 6.378313064575195
Epoch 480, val loss: 0.7728550434112549
Epoch 490, training loss: 6.794739246368408 = 0.41688916087150574 + 1.0 * 6.37785005569458
Epoch 490, val loss: 0.7682202458381653
Epoch 500, training loss: 6.77155876159668 = 0.4009193181991577 + 1.0 * 6.370639324188232
Epoch 500, val loss: 0.7640953660011292
Epoch 510, training loss: 6.752923011779785 = 0.38521313667297363 + 1.0 * 6.367709636688232
Epoch 510, val loss: 0.7603504657745361
Epoch 520, training loss: 6.751418113708496 = 0.3697431683540344 + 1.0 * 6.381674766540527
Epoch 520, val loss: 0.757013738155365
Epoch 530, training loss: 6.721255302429199 = 0.35468173027038574 + 1.0 * 6.366573810577393
Epoch 530, val loss: 0.7541773915290833
Epoch 540, training loss: 6.702389717102051 = 0.33994126319885254 + 1.0 * 6.362448692321777
Epoch 540, val loss: 0.7517781853675842
Epoch 550, training loss: 6.688904762268066 = 0.3255271017551422 + 1.0 * 6.363377571105957
Epoch 550, val loss: 0.7499366998672485
Epoch 560, training loss: 6.6726155281066895 = 0.31167104840278625 + 1.0 * 6.3609442710876465
Epoch 560, val loss: 0.7486656308174133
Epoch 570, training loss: 6.654757499694824 = 0.2983625829219818 + 1.0 * 6.3563947677612305
Epoch 570, val loss: 0.7478576302528381
Epoch 580, training loss: 6.648131370544434 = 0.2855629026889801 + 1.0 * 6.362568378448486
Epoch 580, val loss: 0.7475922107696533
Epoch 590, training loss: 6.626454830169678 = 0.2733360528945923 + 1.0 * 6.353118896484375
Epoch 590, val loss: 0.747791588306427
Epoch 600, training loss: 6.612345218658447 = 0.26156488060951233 + 1.0 * 6.350780487060547
Epoch 600, val loss: 0.7482806444168091
Epoch 610, training loss: 6.619687080383301 = 0.25022223591804504 + 1.0 * 6.369464874267578
Epoch 610, val loss: 0.7491455078125
Epoch 620, training loss: 6.589658260345459 = 0.2393452525138855 + 1.0 * 6.350313186645508
Epoch 620, val loss: 0.7502724528312683
Epoch 630, training loss: 6.5756964683532715 = 0.22880806028842926 + 1.0 * 6.346888542175293
Epoch 630, val loss: 0.7515377998352051
Epoch 640, training loss: 6.562725067138672 = 0.21852341294288635 + 1.0 * 6.344201564788818
Epoch 640, val loss: 0.7530744075775146
Epoch 650, training loss: 6.550679683685303 = 0.2084311991930008 + 1.0 * 6.342248439788818
Epoch 650, val loss: 0.7547618746757507
Epoch 660, training loss: 6.54670524597168 = 0.19854736328125 + 1.0 * 6.34815788269043
Epoch 660, val loss: 0.7566971182823181
Epoch 670, training loss: 6.5323028564453125 = 0.18894928693771362 + 1.0 * 6.343353748321533
Epoch 670, val loss: 0.7586609721183777
Epoch 680, training loss: 6.5186848640441895 = 0.17957846820354462 + 1.0 * 6.339106559753418
Epoch 680, val loss: 0.7607612609863281
Epoch 690, training loss: 6.507714748382568 = 0.1704392433166504 + 1.0 * 6.337275505065918
Epoch 690, val loss: 0.7631796598434448
Epoch 700, training loss: 6.5094499588012695 = 0.1615557223558426 + 1.0 * 6.347894191741943
Epoch 700, val loss: 0.7658776044845581
Epoch 710, training loss: 6.4874091148376465 = 0.1530294567346573 + 1.0 * 6.33437967300415
Epoch 710, val loss: 0.7686837315559387
Epoch 720, training loss: 6.478531360626221 = 0.1448536515235901 + 1.0 * 6.333677768707275
Epoch 720, val loss: 0.7715436220169067
Epoch 730, training loss: 6.468989372253418 = 0.1370057314634323 + 1.0 * 6.33198356628418
Epoch 730, val loss: 0.7747308015823364
Epoch 740, training loss: 6.4687604904174805 = 0.12948672473430634 + 1.0 * 6.339273929595947
Epoch 740, val loss: 0.7781667709350586
Epoch 750, training loss: 6.455852031707764 = 0.1224064975976944 + 1.0 * 6.3334455490112305
Epoch 750, val loss: 0.7818660736083984
Epoch 760, training loss: 6.442970275878906 = 0.11566760390996933 + 1.0 * 6.3273024559021
Epoch 760, val loss: 0.7855266332626343
Epoch 770, training loss: 6.4356207847595215 = 0.10928885638713837 + 1.0 * 6.326332092285156
Epoch 770, val loss: 0.7894273400306702
Epoch 780, training loss: 6.4294209480285645 = 0.1032571792602539 + 1.0 * 6.3261637687683105
Epoch 780, val loss: 0.7935526371002197
Epoch 790, training loss: 6.426723003387451 = 0.09759116917848587 + 1.0 * 6.329131603240967
Epoch 790, val loss: 0.7978803515434265
Epoch 800, training loss: 6.417601108551025 = 0.09230875223875046 + 1.0 * 6.325292587280273
Epoch 800, val loss: 0.8021065592765808
Epoch 810, training loss: 6.410787105560303 = 0.08734600245952606 + 1.0 * 6.323441028594971
Epoch 810, val loss: 0.8063914179801941
Epoch 820, training loss: 6.403995513916016 = 0.08268655091524124 + 1.0 * 6.3213090896606445
Epoch 820, val loss: 0.8109299540519714
Epoch 830, training loss: 6.405435562133789 = 0.07829593867063522 + 1.0 * 6.327139854431152
Epoch 830, val loss: 0.8155767321586609
Epoch 840, training loss: 6.395744323730469 = 0.07420304417610168 + 1.0 * 6.3215413093566895
Epoch 840, val loss: 0.8203256726264954
Epoch 850, training loss: 6.395650386810303 = 0.07036158442497253 + 1.0 * 6.325288772583008
Epoch 850, val loss: 0.8250376582145691
Epoch 860, training loss: 6.385385036468506 = 0.06678499281406403 + 1.0 * 6.318600177764893
Epoch 860, val loss: 0.8298467397689819
Epoch 870, training loss: 6.379698753356934 = 0.06342129409313202 + 1.0 * 6.316277503967285
Epoch 870, val loss: 0.8346415758132935
Epoch 880, training loss: 6.378633975982666 = 0.06027401611208916 + 1.0 * 6.318359851837158
Epoch 880, val loss: 0.8396151065826416
Epoch 890, training loss: 6.371644496917725 = 0.05732610449194908 + 1.0 * 6.3143181800842285
Epoch 890, val loss: 0.8446249961853027
Epoch 900, training loss: 6.368800640106201 = 0.05457121133804321 + 1.0 * 6.314229488372803
Epoch 900, val loss: 0.8495271801948547
Epoch 910, training loss: 6.37498664855957 = 0.05198533087968826 + 1.0 * 6.323001384735107
Epoch 910, val loss: 0.8545300364494324
Epoch 920, training loss: 6.364345550537109 = 0.04959414154291153 + 1.0 * 6.314751625061035
Epoch 920, val loss: 0.8595789074897766
Epoch 930, training loss: 6.3590898513793945 = 0.04733561351895332 + 1.0 * 6.31175422668457
Epoch 930, val loss: 0.8644996285438538
Epoch 940, training loss: 6.361847877502441 = 0.04522859677672386 + 1.0 * 6.316619396209717
Epoch 940, val loss: 0.8695520758628845
Epoch 950, training loss: 6.3550705909729 = 0.04325159639120102 + 1.0 * 6.311819076538086
Epoch 950, val loss: 0.8744921684265137
Epoch 960, training loss: 6.350247859954834 = 0.04139614850282669 + 1.0 * 6.308851718902588
Epoch 960, val loss: 0.8794423937797546
Epoch 970, training loss: 6.347350597381592 = 0.03965584933757782 + 1.0 * 6.307694911956787
Epoch 970, val loss: 0.8843361735343933
Epoch 980, training loss: 6.349137783050537 = 0.03801533207297325 + 1.0 * 6.311122417449951
Epoch 980, val loss: 0.8892701268196106
Epoch 990, training loss: 6.344442844390869 = 0.03648097813129425 + 1.0 * 6.307961940765381
Epoch 990, val loss: 0.8941617012023926
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 10.532807350158691 = 1.9359952211380005 + 1.0 * 8.59681224822998
Epoch 0, val loss: 1.9320136308670044
Epoch 10, training loss: 10.522682189941406 = 1.9261679649353027 + 1.0 * 8.596513748168945
Epoch 10, val loss: 1.9219826459884644
Epoch 20, training loss: 10.508357048034668 = 1.913946509361267 + 1.0 * 8.59441089630127
Epoch 20, val loss: 1.9095653295516968
Epoch 30, training loss: 10.474655151367188 = 1.8967244625091553 + 1.0 * 8.577930450439453
Epoch 30, val loss: 1.892449975013733
Epoch 40, training loss: 10.34150218963623 = 1.8738822937011719 + 1.0 * 8.467619895935059
Epoch 40, val loss: 1.870681643486023
Epoch 50, training loss: 9.862292289733887 = 1.8486366271972656 + 1.0 * 8.013655662536621
Epoch 50, val loss: 1.8469871282577515
Epoch 60, training loss: 9.425495147705078 = 1.826653003692627 + 1.0 * 7.598841667175293
Epoch 60, val loss: 1.8273046016693115
Epoch 70, training loss: 9.042362213134766 = 1.8137327432632446 + 1.0 * 7.2286295890808105
Epoch 70, val loss: 1.8153949975967407
Epoch 80, training loss: 8.840057373046875 = 1.8005576133728027 + 1.0 * 7.0395002365112305
Epoch 80, val loss: 1.8031888008117676
Epoch 90, training loss: 8.675177574157715 = 1.7851263284683228 + 1.0 * 6.890051364898682
Epoch 90, val loss: 1.7901955842971802
Epoch 100, training loss: 8.576919555664062 = 1.770009160041809 + 1.0 * 6.806910037994385
Epoch 100, val loss: 1.777632713317871
Epoch 110, training loss: 8.501357078552246 = 1.7551366090774536 + 1.0 * 6.746220588684082
Epoch 110, val loss: 1.7642959356307983
Epoch 120, training loss: 8.441943168640137 = 1.738877296447754 + 1.0 * 6.703065872192383
Epoch 120, val loss: 1.7493855953216553
Epoch 130, training loss: 8.389291763305664 = 1.7204595804214478 + 1.0 * 6.668831825256348
Epoch 130, val loss: 1.7328015565872192
Epoch 140, training loss: 8.33761978149414 = 1.69954514503479 + 1.0 * 6.6380743980407715
Epoch 140, val loss: 1.7145220041275024
Epoch 150, training loss: 8.287614822387695 = 1.6753056049346924 + 1.0 * 6.612308979034424
Epoch 150, val loss: 1.6935497522354126
Epoch 160, training loss: 8.236894607543945 = 1.6470831632614136 + 1.0 * 6.589811325073242
Epoch 160, val loss: 1.669309377670288
Epoch 170, training loss: 8.184825897216797 = 1.614306092262268 + 1.0 * 6.570519924163818
Epoch 170, val loss: 1.6411620378494263
Epoch 180, training loss: 8.12938117980957 = 1.5766534805297852 + 1.0 * 6.552727222442627
Epoch 180, val loss: 1.609061598777771
Epoch 190, training loss: 8.071982383728027 = 1.5342291593551636 + 1.0 * 6.537753105163574
Epoch 190, val loss: 1.5729917287826538
Epoch 200, training loss: 8.012876510620117 = 1.487562656402588 + 1.0 * 6.525313377380371
Epoch 200, val loss: 1.5336287021636963
Epoch 210, training loss: 7.951817035675049 = 1.437757968902588 + 1.0 * 6.514059066772461
Epoch 210, val loss: 1.4920058250427246
Epoch 220, training loss: 7.8902692794799805 = 1.3864625692367554 + 1.0 * 6.5038065910339355
Epoch 220, val loss: 1.449647068977356
Epoch 230, training loss: 7.829277515411377 = 1.3340697288513184 + 1.0 * 6.495207786560059
Epoch 230, val loss: 1.4066956043243408
Epoch 240, training loss: 7.76713752746582 = 1.281368613243103 + 1.0 * 6.485768795013428
Epoch 240, val loss: 1.364017128944397
Epoch 250, training loss: 7.70913028717041 = 1.228193998336792 + 1.0 * 6.480936050415039
Epoch 250, val loss: 1.3213666677474976
Epoch 260, training loss: 7.645990371704102 = 1.1756393909454346 + 1.0 * 6.470351219177246
Epoch 260, val loss: 1.2794194221496582
Epoch 270, training loss: 7.586310863494873 = 1.1236095428466797 + 1.0 * 6.462701320648193
Epoch 270, val loss: 1.2381174564361572
Epoch 280, training loss: 7.532276630401611 = 1.0716204643249512 + 1.0 * 6.46065616607666
Epoch 280, val loss: 1.1970750093460083
Epoch 290, training loss: 7.4775071144104 = 1.0211900472640991 + 1.0 * 6.456316947937012
Epoch 290, val loss: 1.1575182676315308
Epoch 300, training loss: 7.4179182052612305 = 0.972902774810791 + 1.0 * 6.4450154304504395
Epoch 300, val loss: 1.1194764375686646
Epoch 310, training loss: 7.365509033203125 = 0.926171064376831 + 1.0 * 6.439338207244873
Epoch 310, val loss: 1.0826443433761597
Epoch 320, training loss: 7.319539546966553 = 0.8809807896614075 + 1.0 * 6.438558578491211
Epoch 320, val loss: 1.0471144914627075
Epoch 330, training loss: 7.2686662673950195 = 0.8380663394927979 + 1.0 * 6.430599689483643
Epoch 330, val loss: 1.0131851434707642
Epoch 340, training loss: 7.221824645996094 = 0.7971618175506592 + 1.0 * 6.424663066864014
Epoch 340, val loss: 0.9809074401855469
Epoch 350, training loss: 7.1799540519714355 = 0.758354663848877 + 1.0 * 6.421599388122559
Epoch 350, val loss: 0.9503887891769409
Epoch 360, training loss: 7.138540267944336 = 0.722061812877655 + 1.0 * 6.416478633880615
Epoch 360, val loss: 0.9221842288970947
Epoch 370, training loss: 7.103479385375977 = 0.6880371570587158 + 1.0 * 6.415441989898682
Epoch 370, val loss: 0.8964292407035828
Epoch 380, training loss: 7.066645622253418 = 0.6562528014183044 + 1.0 * 6.410392761230469
Epoch 380, val loss: 0.8728782534599304
Epoch 390, training loss: 7.031254768371582 = 0.626213550567627 + 1.0 * 6.405041217803955
Epoch 390, val loss: 0.8513709306716919
Epoch 400, training loss: 7.004945755004883 = 0.5976741909980774 + 1.0 * 6.407271385192871
Epoch 400, val loss: 0.8316627740859985
Epoch 410, training loss: 6.971797943115234 = 0.5707263350486755 + 1.0 * 6.401071548461914
Epoch 410, val loss: 0.813848078250885
Epoch 420, training loss: 6.940080165863037 = 0.5448854565620422 + 1.0 * 6.3951945304870605
Epoch 420, val loss: 0.7975995540618896
Epoch 430, training loss: 6.914017200469971 = 0.5199232697486877 + 1.0 * 6.394093990325928
Epoch 430, val loss: 0.7825878262519836
Epoch 440, training loss: 6.886077404022217 = 0.4958247244358063 + 1.0 * 6.390252590179443
Epoch 440, val loss: 0.768836498260498
Epoch 450, training loss: 6.861156940460205 = 0.4722229540348053 + 1.0 * 6.388934135437012
Epoch 450, val loss: 0.7561058402061462
Epoch 460, training loss: 6.836202621459961 = 0.44918105006217957 + 1.0 * 6.387021541595459
Epoch 460, val loss: 0.7440686225891113
Epoch 470, training loss: 6.808184623718262 = 0.42652183771133423 + 1.0 * 6.381662845611572
Epoch 470, val loss: 0.7330111861228943
Epoch 480, training loss: 6.789590358734131 = 0.4042249023914337 + 1.0 * 6.3853654861450195
Epoch 480, val loss: 0.7225893139839172
Epoch 490, training loss: 6.759494304656982 = 0.38251885771751404 + 1.0 * 6.3769755363464355
Epoch 490, val loss: 0.712955117225647
Epoch 500, training loss: 6.735221862792969 = 0.3612714111804962 + 1.0 * 6.373950481414795
Epoch 500, val loss: 0.7040776014328003
Epoch 510, training loss: 6.712277412414551 = 0.3405202031135559 + 1.0 * 6.3717570304870605
Epoch 510, val loss: 0.695848822593689
Epoch 520, training loss: 6.6967973709106445 = 0.32046157121658325 + 1.0 * 6.376335620880127
Epoch 520, val loss: 0.6884022355079651
Epoch 530, training loss: 6.671524524688721 = 0.3014559745788574 + 1.0 * 6.370068550109863
Epoch 530, val loss: 0.6818031072616577
Epoch 540, training loss: 6.650737285614014 = 0.2834002673625946 + 1.0 * 6.367337226867676
Epoch 540, val loss: 0.675936222076416
Epoch 550, training loss: 6.63588809967041 = 0.26623982191085815 + 1.0 * 6.369648456573486
Epoch 550, val loss: 0.6709727644920349
Epoch 560, training loss: 6.61747932434082 = 0.2501392066478729 + 1.0 * 6.367340087890625
Epoch 560, val loss: 0.6668471693992615
Epoch 570, training loss: 6.597672939300537 = 0.23506703972816467 + 1.0 * 6.362606048583984
Epoch 570, val loss: 0.6635560989379883
Epoch 580, training loss: 6.5812907218933105 = 0.22098256647586823 + 1.0 * 6.3603081703186035
Epoch 580, val loss: 0.6610669493675232
Epoch 590, training loss: 6.5677289962768555 = 0.20781457424163818 + 1.0 * 6.359914302825928
Epoch 590, val loss: 0.6593846678733826
Epoch 600, training loss: 6.552656173706055 = 0.19556888937950134 + 1.0 * 6.357087135314941
Epoch 600, val loss: 0.6583860516548157
Epoch 610, training loss: 6.538684844970703 = 0.1841498762369156 + 1.0 * 6.354535102844238
Epoch 610, val loss: 0.6580271124839783
Epoch 620, training loss: 6.529689788818359 = 0.17349204421043396 + 1.0 * 6.356197834014893
Epoch 620, val loss: 0.6582678556442261
Epoch 630, training loss: 6.516166687011719 = 0.1636030673980713 + 1.0 * 6.352563381195068
Epoch 630, val loss: 0.6590225696563721
Epoch 640, training loss: 6.5054826736450195 = 0.15443170070648193 + 1.0 * 6.351050853729248
Epoch 640, val loss: 0.6602286100387573
Epoch 650, training loss: 6.499402046203613 = 0.14586782455444336 + 1.0 * 6.35353422164917
Epoch 650, val loss: 0.6618592143058777
Epoch 660, training loss: 6.4860310554504395 = 0.1379033923149109 + 1.0 * 6.348127841949463
Epoch 660, val loss: 0.6638176441192627
Epoch 670, training loss: 6.478460788726807 = 0.13045236468315125 + 1.0 * 6.348008632659912
Epoch 670, val loss: 0.6660279035568237
Epoch 680, training loss: 6.469063758850098 = 0.12349716573953629 + 1.0 * 6.345566749572754
Epoch 680, val loss: 0.6685678958892822
Epoch 690, training loss: 6.4613037109375 = 0.11698655784130096 + 1.0 * 6.3443169593811035
Epoch 690, val loss: 0.671287477016449
Epoch 700, training loss: 6.458503723144531 = 0.11089962720870972 + 1.0 * 6.347604274749756
Epoch 700, val loss: 0.6742232441902161
Epoch 710, training loss: 6.44461727142334 = 0.10522768646478653 + 1.0 * 6.339389801025391
Epoch 710, val loss: 0.677267849445343
Epoch 720, training loss: 6.438226699829102 = 0.09989961981773376 + 1.0 * 6.338326930999756
Epoch 720, val loss: 0.680494487285614
Epoch 730, training loss: 6.435405731201172 = 0.09489020705223083 + 1.0 * 6.340515613555908
Epoch 730, val loss: 0.6838488578796387
Epoch 740, training loss: 6.434035301208496 = 0.09020394086837769 + 1.0 * 6.343831539154053
Epoch 740, val loss: 0.6872798204421997
Epoch 750, training loss: 6.421788692474365 = 0.0858326405286789 + 1.0 * 6.33595609664917
Epoch 750, val loss: 0.6907393336296082
Epoch 760, training loss: 6.4153056144714355 = 0.08171530067920685 + 1.0 * 6.333590507507324
Epoch 760, val loss: 0.6943233013153076
Epoch 770, training loss: 6.413078784942627 = 0.07783780992031097 + 1.0 * 6.335240840911865
Epoch 770, val loss: 0.6979635953903198
Epoch 780, training loss: 6.410375595092773 = 0.07420719414949417 + 1.0 * 6.33616828918457
Epoch 780, val loss: 0.7016310095787048
Epoch 790, training loss: 6.400190353393555 = 0.07080762088298798 + 1.0 * 6.32938289642334
Epoch 790, val loss: 0.705345869064331
Epoch 800, training loss: 6.396884441375732 = 0.0675976574420929 + 1.0 * 6.329286575317383
Epoch 800, val loss: 0.7091331481933594
Epoch 810, training loss: 6.399771213531494 = 0.06456806510686874 + 1.0 * 6.335203170776367
Epoch 810, val loss: 0.7129738330841064
Epoch 820, training loss: 6.389886379241943 = 0.06172466278076172 + 1.0 * 6.328161716461182
Epoch 820, val loss: 0.7167137861251831
Epoch 830, training loss: 6.385526657104492 = 0.05904319882392883 + 1.0 * 6.326483249664307
Epoch 830, val loss: 0.7205144166946411
Epoch 840, training loss: 6.384154796600342 = 0.056513771414756775 + 1.0 * 6.327641010284424
Epoch 840, val loss: 0.7243478894233704
Epoch 850, training loss: 6.3795647621154785 = 0.05413299426436424 + 1.0 * 6.325431823730469
Epoch 850, val loss: 0.7281420230865479
Epoch 860, training loss: 6.3735671043396 = 0.05188818275928497 + 1.0 * 6.32167911529541
Epoch 860, val loss: 0.7320027947425842
Epoch 870, training loss: 6.385082244873047 = 0.04976607486605644 + 1.0 * 6.335316181182861
Epoch 870, val loss: 0.7358120679855347
Epoch 880, training loss: 6.368289947509766 = 0.04776446521282196 + 1.0 * 6.320525646209717
Epoch 880, val loss: 0.7396252751350403
Epoch 890, training loss: 6.365635395050049 = 0.04587540775537491 + 1.0 * 6.319759845733643
Epoch 890, val loss: 0.7434377670288086
Epoch 900, training loss: 6.364151954650879 = 0.04408670589327812 + 1.0 * 6.320065021514893
Epoch 900, val loss: 0.7472819685935974
Epoch 910, training loss: 6.360507488250732 = 0.042394887655973434 + 1.0 * 6.318112373352051
Epoch 910, val loss: 0.7510148882865906
Epoch 920, training loss: 6.359581470489502 = 0.040802452713251114 + 1.0 * 6.318778991699219
Epoch 920, val loss: 0.7547568678855896
Epoch 930, training loss: 6.3540778160095215 = 0.039288219064474106 + 1.0 * 6.314789772033691
Epoch 930, val loss: 0.7585026025772095
Epoch 940, training loss: 6.358132362365723 = 0.037848830223083496 + 1.0 * 6.32028341293335
Epoch 940, val loss: 0.7622372508049011
Epoch 950, training loss: 6.353353500366211 = 0.03648583963513374 + 1.0 * 6.316867828369141
Epoch 950, val loss: 0.7658752799034119
Epoch 960, training loss: 6.351554870605469 = 0.035197850316762924 + 1.0 * 6.316357135772705
Epoch 960, val loss: 0.7695286870002747
Epoch 970, training loss: 6.346495628356934 = 0.033974699676036835 + 1.0 * 6.312520980834961
Epoch 970, val loss: 0.7731456756591797
Epoch 980, training loss: 6.344748497009277 = 0.03281396999955177 + 1.0 * 6.311934471130371
Epoch 980, val loss: 0.7767336964607239
Epoch 990, training loss: 6.342953681945801 = 0.03170431777834892 + 1.0 * 6.311249256134033
Epoch 990, val loss: 0.7803220748901367
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.55192756652832 = 1.9550875425338745 + 1.0 * 8.596839904785156
Epoch 0, val loss: 1.9479899406433105
Epoch 10, training loss: 10.542327880859375 = 1.9457173347473145 + 1.0 * 8.596610069274902
Epoch 10, val loss: 1.9394137859344482
Epoch 20, training loss: 10.529430389404297 = 1.9343726634979248 + 1.0 * 8.595057487487793
Epoch 20, val loss: 1.9286813735961914
Epoch 30, training loss: 10.502143859863281 = 1.9188040494918823 + 1.0 * 8.58333969116211
Epoch 30, val loss: 1.9136877059936523
Epoch 40, training loss: 10.393942832946777 = 1.8975335359573364 + 1.0 * 8.49640941619873
Epoch 40, val loss: 1.8936030864715576
Epoch 50, training loss: 9.870185852050781 = 1.872841477394104 + 1.0 * 7.997344017028809
Epoch 50, val loss: 1.870414137840271
Epoch 60, training loss: 9.475130081176758 = 1.849576711654663 + 1.0 * 7.625553131103516
Epoch 60, val loss: 1.8493162393569946
Epoch 70, training loss: 9.083684921264648 = 1.8335672616958618 + 1.0 * 7.250117301940918
Epoch 70, val loss: 1.8346638679504395
Epoch 80, training loss: 8.863436698913574 = 1.8195652961730957 + 1.0 * 7.0438714027404785
Epoch 80, val loss: 1.8218879699707031
Epoch 90, training loss: 8.736261367797852 = 1.802426815032959 + 1.0 * 6.933835029602051
Epoch 90, val loss: 1.8066892623901367
Epoch 100, training loss: 8.622081756591797 = 1.7849762439727783 + 1.0 * 6.837105751037598
Epoch 100, val loss: 1.7921570539474487
Epoch 110, training loss: 8.5367431640625 = 1.7694730758666992 + 1.0 * 6.767270565032959
Epoch 110, val loss: 1.7793034315109253
Epoch 120, training loss: 8.466153144836426 = 1.754280924797058 + 1.0 * 6.711872577667236
Epoch 120, val loss: 1.7659642696380615
Epoch 130, training loss: 8.410453796386719 = 1.7372527122497559 + 1.0 * 6.673200607299805
Epoch 130, val loss: 1.7505943775177002
Epoch 140, training loss: 8.36184024810791 = 1.7178527116775513 + 1.0 * 6.64398717880249
Epoch 140, val loss: 1.7332683801651
Epoch 150, training loss: 8.309209823608398 = 1.6958516836166382 + 1.0 * 6.613358497619629
Epoch 150, val loss: 1.7142058610916138
Epoch 160, training loss: 8.259742736816406 = 1.6706199645996094 + 1.0 * 6.589122295379639
Epoch 160, val loss: 1.692697525024414
Epoch 170, training loss: 8.208849906921387 = 1.6411858797073364 + 1.0 * 6.56766414642334
Epoch 170, val loss: 1.6678366661071777
Epoch 180, training loss: 8.1563720703125 = 1.607126235961914 + 1.0 * 6.549245357513428
Epoch 180, val loss: 1.639269232749939
Epoch 190, training loss: 8.100890159606934 = 1.5687257051467896 + 1.0 * 6.532164573669434
Epoch 190, val loss: 1.6071885824203491
Epoch 200, training loss: 8.044604301452637 = 1.5265836715698242 + 1.0 * 6.5180206298828125
Epoch 200, val loss: 1.5722148418426514
Epoch 210, training loss: 7.988419532775879 = 1.4817698001861572 + 1.0 * 6.506649494171143
Epoch 210, val loss: 1.535194754600525
Epoch 220, training loss: 7.9292683601379395 = 1.4359718561172485 + 1.0 * 6.4932966232299805
Epoch 220, val loss: 1.4975327253341675
Epoch 230, training loss: 7.8728132247924805 = 1.3897831439971924 + 1.0 * 6.483030319213867
Epoch 230, val loss: 1.4598623514175415
Epoch 240, training loss: 7.818449974060059 = 1.343817949295044 + 1.0 * 6.474632263183594
Epoch 240, val loss: 1.4226672649383545
Epoch 250, training loss: 7.764978885650635 = 1.2988444566726685 + 1.0 * 6.466134548187256
Epoch 250, val loss: 1.3864307403564453
Epoch 260, training loss: 7.713318347930908 = 1.2543439865112305 + 1.0 * 6.458974361419678
Epoch 260, val loss: 1.3509483337402344
Epoch 270, training loss: 7.664331912994385 = 1.2107068300247192 + 1.0 * 6.453625202178955
Epoch 270, val loss: 1.3165942430496216
Epoch 280, training loss: 7.6138200759887695 = 1.1680771112442017 + 1.0 * 6.445743083953857
Epoch 280, val loss: 1.2834956645965576
Epoch 290, training loss: 7.568203926086426 = 1.1262454986572266 + 1.0 * 6.441958427429199
Epoch 290, val loss: 1.2513002157211304
Epoch 300, training loss: 7.51984167098999 = 1.085574984550476 + 1.0 * 6.434266567230225
Epoch 300, val loss: 1.2202290296554565
Epoch 310, training loss: 7.474217414855957 = 1.0460522174835205 + 1.0 * 6.428165435791016
Epoch 310, val loss: 1.1904922723770142
Epoch 320, training loss: 7.432875633239746 = 1.0077465772628784 + 1.0 * 6.425128936767578
Epoch 320, val loss: 1.1617953777313232
Epoch 330, training loss: 7.390283107757568 = 0.9709183573722839 + 1.0 * 6.419364929199219
Epoch 330, val loss: 1.1344977617263794
Epoch 340, training loss: 7.349663257598877 = 0.9349787831306458 + 1.0 * 6.414684295654297
Epoch 340, val loss: 1.1079922914505005
Epoch 350, training loss: 7.313595771789551 = 0.8999481797218323 + 1.0 * 6.413647651672363
Epoch 350, val loss: 1.082238793373108
Epoch 360, training loss: 7.27381706237793 = 0.8657540082931519 + 1.0 * 6.408062934875488
Epoch 360, val loss: 1.0573064088821411
Epoch 370, training loss: 7.236302852630615 = 0.8318414092063904 + 1.0 * 6.40446138381958
Epoch 370, val loss: 1.0325953960418701
Epoch 380, training loss: 7.208966255187988 = 0.798240065574646 + 1.0 * 6.410726070404053
Epoch 380, val loss: 1.0081501007080078
Epoch 390, training loss: 7.164789199829102 = 0.7653491497039795 + 1.0 * 6.399439811706543
Epoch 390, val loss: 0.9843283295631409
Epoch 400, training loss: 7.128103256225586 = 0.7328120470046997 + 1.0 * 6.395291328430176
Epoch 400, val loss: 0.9608014225959778
Epoch 410, training loss: 7.092126369476318 = 0.7005313038825989 + 1.0 * 6.391594886779785
Epoch 410, val loss: 0.9375419616699219
Epoch 420, training loss: 7.075825214385986 = 0.6686853766441345 + 1.0 * 6.407139778137207
Epoch 420, val loss: 0.9148205518722534
Epoch 430, training loss: 7.025172710418701 = 0.6381385922431946 + 1.0 * 6.387033939361572
Epoch 430, val loss: 0.8932523727416992
Epoch 440, training loss: 6.994001865386963 = 0.6084409356117249 + 1.0 * 6.385560989379883
Epoch 440, val loss: 0.8726626634597778
Epoch 450, training loss: 6.960816860198975 = 0.5794547200202942 + 1.0 * 6.381361961364746
Epoch 450, val loss: 0.8528413772583008
Epoch 460, training loss: 6.940003395080566 = 0.5511748790740967 + 1.0 * 6.388828277587891
Epoch 460, val loss: 0.834034264087677
Epoch 470, training loss: 6.903493404388428 = 0.5240320563316345 + 1.0 * 6.379461288452148
Epoch 470, val loss: 0.8164290189743042
Epoch 480, training loss: 6.87465763092041 = 0.497773677110672 + 1.0 * 6.3768839836120605
Epoch 480, val loss: 0.8000213503837585
Epoch 490, training loss: 6.847270965576172 = 0.4721667766571045 + 1.0 * 6.375103950500488
Epoch 490, val loss: 0.7845279574394226
Epoch 500, training loss: 6.8187665939331055 = 0.4472079277038574 + 1.0 * 6.371558666229248
Epoch 500, val loss: 0.7700629830360413
Epoch 510, training loss: 6.793374538421631 = 0.4228261411190033 + 1.0 * 6.370548248291016
Epoch 510, val loss: 0.7565677762031555
Epoch 520, training loss: 6.77280855178833 = 0.3989272713661194 + 1.0 * 6.3738813400268555
Epoch 520, val loss: 0.7440361380577087
Epoch 530, training loss: 6.741788864135742 = 0.3756200075149536 + 1.0 * 6.366168975830078
Epoch 530, val loss: 0.7326033115386963
Epoch 540, training loss: 6.72287130355835 = 0.35292673110961914 + 1.0 * 6.3699445724487305
Epoch 540, val loss: 0.7221940755844116
Epoch 550, training loss: 6.696375846862793 = 0.3310089707374573 + 1.0 * 6.3653669357299805
Epoch 550, val loss: 0.7129460573196411
Epoch 560, training loss: 6.67095422744751 = 0.3099794089794159 + 1.0 * 6.3609747886657715
Epoch 560, val loss: 0.7047985196113586
Epoch 570, training loss: 6.649392604827881 = 0.28982067108154297 + 1.0 * 6.359571933746338
Epoch 570, val loss: 0.6978351473808289
Epoch 580, training loss: 6.634335041046143 = 0.270739883184433 + 1.0 * 6.363595008850098
Epoch 580, val loss: 0.691964328289032
Epoch 590, training loss: 6.613319396972656 = 0.252821683883667 + 1.0 * 6.360497951507568
Epoch 590, val loss: 0.6872700452804565
Epoch 600, training loss: 6.590973854064941 = 0.23609529435634613 + 1.0 * 6.3548784255981445
Epoch 600, val loss: 0.6835658550262451
Epoch 610, training loss: 6.5747480392456055 = 0.22045543789863586 + 1.0 * 6.354292392730713
Epoch 610, val loss: 0.6808376908302307
Epoch 620, training loss: 6.558953285217285 = 0.20591950416564941 + 1.0 * 6.353033542633057
Epoch 620, val loss: 0.678902268409729
Epoch 630, training loss: 6.542868614196777 = 0.19245587289333344 + 1.0 * 6.350412845611572
Epoch 630, val loss: 0.6777763366699219
Epoch 640, training loss: 6.529267311096191 = 0.17992380261421204 + 1.0 * 6.349343299865723
Epoch 640, val loss: 0.6772570610046387
Epoch 650, training loss: 6.521109104156494 = 0.16833345592021942 + 1.0 * 6.352775573730469
Epoch 650, val loss: 0.6773420572280884
Epoch 660, training loss: 6.503485679626465 = 0.15762992203235626 + 1.0 * 6.345855712890625
Epoch 660, val loss: 0.6779212951660156
Epoch 670, training loss: 6.491555690765381 = 0.1477050483226776 + 1.0 * 6.343850612640381
Epoch 670, val loss: 0.67891526222229
Epoch 680, training loss: 6.481905937194824 = 0.1384761482477188 + 1.0 * 6.3434295654296875
Epoch 680, val loss: 0.680396556854248
Epoch 690, training loss: 6.470285892486572 = 0.12992264330387115 + 1.0 * 6.340363025665283
Epoch 690, val loss: 0.6822198629379272
Epoch 700, training loss: 6.4674506187438965 = 0.1220063716173172 + 1.0 * 6.345444202423096
Epoch 700, val loss: 0.6843223571777344
Epoch 710, training loss: 6.453118801116943 = 0.11470169574022293 + 1.0 * 6.338417053222656
Epoch 710, val loss: 0.6866936087608337
Epoch 720, training loss: 6.4432053565979 = 0.10790103673934937 + 1.0 * 6.335304260253906
Epoch 720, val loss: 0.6893942952156067
Epoch 730, training loss: 6.44197416305542 = 0.10158088058233261 + 1.0 * 6.34039306640625
Epoch 730, val loss: 0.6923789381980896
Epoch 740, training loss: 6.434606552124023 = 0.09574660658836365 + 1.0 * 6.338860034942627
Epoch 740, val loss: 0.6955301761627197
Epoch 750, training loss: 6.422738552093506 = 0.09033231437206268 + 1.0 * 6.332406044006348
Epoch 750, val loss: 0.6988474726676941
Epoch 760, training loss: 6.417655944824219 = 0.08531543612480164 + 1.0 * 6.332340717315674
Epoch 760, val loss: 0.7023245692253113
Epoch 770, training loss: 6.414905071258545 = 0.08066057413816452 + 1.0 * 6.334244728088379
Epoch 770, val loss: 0.7059460282325745
Epoch 780, training loss: 6.40642786026001 = 0.07633957266807556 + 1.0 * 6.330088138580322
Epoch 780, val loss: 0.7097275853157043
Epoch 790, training loss: 6.401440620422363 = 0.07232046872377396 + 1.0 * 6.32912015914917
Epoch 790, val loss: 0.7135075330734253
Epoch 800, training loss: 6.3973212242126465 = 0.06858991831541061 + 1.0 * 6.328731536865234
Epoch 800, val loss: 0.7174872159957886
Epoch 810, training loss: 6.391234397888184 = 0.06512133032083511 + 1.0 * 6.326113224029541
Epoch 810, val loss: 0.7214163541793823
Epoch 820, training loss: 6.387355804443359 = 0.06189805269241333 + 1.0 * 6.325457572937012
Epoch 820, val loss: 0.7254342436790466
Epoch 830, training loss: 6.380699634552002 = 0.058892957866191864 + 1.0 * 6.321806907653809
Epoch 830, val loss: 0.729464054107666
Epoch 840, training loss: 6.3797831535339355 = 0.05608532950282097 + 1.0 * 6.323698043823242
Epoch 840, val loss: 0.7335445284843445
Epoch 850, training loss: 6.373487949371338 = 0.05346086993813515 + 1.0 * 6.320026874542236
Epoch 850, val loss: 0.7377753257751465
Epoch 860, training loss: 6.369204998016357 = 0.05100972205400467 + 1.0 * 6.318195343017578
Epoch 860, val loss: 0.7418034076690674
Epoch 870, training loss: 6.3690056800842285 = 0.04870932921767235 + 1.0 * 6.320296287536621
Epoch 870, val loss: 0.7459180951118469
Epoch 880, training loss: 6.362971782684326 = 0.046554047614336014 + 1.0 * 6.316417694091797
Epoch 880, val loss: 0.7500827312469482
Epoch 890, training loss: 6.3634934425354 = 0.04453624039888382 + 1.0 * 6.318957328796387
Epoch 890, val loss: 0.7541823983192444
Epoch 900, training loss: 6.3590545654296875 = 0.042642127722501755 + 1.0 * 6.316412448883057
Epoch 900, val loss: 0.7582540512084961
Epoch 910, training loss: 6.356483459472656 = 0.04086931794881821 + 1.0 * 6.315614223480225
Epoch 910, val loss: 0.7623347043991089
Epoch 920, training loss: 6.351555824279785 = 0.039204373955726624 + 1.0 * 6.312351226806641
Epoch 920, val loss: 0.7663875818252563
Epoch 930, training loss: 6.348319053649902 = 0.03763081133365631 + 1.0 * 6.310688018798828
Epoch 930, val loss: 0.7704212069511414
Epoch 940, training loss: 6.354089260101318 = 0.03614334017038345 + 1.0 * 6.317945957183838
Epoch 940, val loss: 0.7745305895805359
Epoch 950, training loss: 6.350100517272949 = 0.03475389629602432 + 1.0 * 6.315346717834473
Epoch 950, val loss: 0.7785301208496094
Epoch 960, training loss: 6.342610836029053 = 0.03344229608774185 + 1.0 * 6.309168338775635
Epoch 960, val loss: 0.7823466658592224
Epoch 970, training loss: 6.341023921966553 = 0.032199129462242126 + 1.0 * 6.3088250160217285
Epoch 970, val loss: 0.7863032817840576
Epoch 980, training loss: 6.341141223907471 = 0.031023431569337845 + 1.0 * 6.310117721557617
Epoch 980, val loss: 0.7902873158454895
Epoch 990, training loss: 6.336528301239014 = 0.02991366758942604 + 1.0 * 6.306614398956299
Epoch 990, val loss: 0.794081449508667
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8407407407407408
0.8365840801265156
The final CL Acc:0.82099, 0.02058, The final GNN Acc:0.84010, 0.00358
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9576])
updated graph: torch.Size([2, 10618])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.531256675720215 = 1.9344103336334229 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9373449087142944
Epoch 10, training loss: 10.522249221801758 = 1.9255694150924683 + 1.0 * 8.5966796875
Epoch 10, val loss: 1.9282536506652832
Epoch 20, training loss: 10.510717391967773 = 1.9152915477752686 + 1.0 * 8.595425605773926
Epoch 20, val loss: 1.9178295135498047
Epoch 30, training loss: 10.486279487609863 = 1.9015605449676514 + 1.0 * 8.584718704223633
Epoch 30, val loss: 1.9038827419281006
Epoch 40, training loss: 10.38620662689209 = 1.883636713027954 + 1.0 * 8.502570152282715
Epoch 40, val loss: 1.8859610557556152
Epoch 50, training loss: 9.976916313171387 = 1.8641111850738525 + 1.0 * 8.112805366516113
Epoch 50, val loss: 1.866862416267395
Epoch 60, training loss: 9.443266868591309 = 1.8486806154251099 + 1.0 * 7.594586372375488
Epoch 60, val loss: 1.8526338338851929
Epoch 70, training loss: 9.090850830078125 = 1.8366258144378662 + 1.0 * 7.25422477722168
Epoch 70, val loss: 1.8406656980514526
Epoch 80, training loss: 8.872145652770996 = 1.8234747648239136 + 1.0 * 7.048670768737793
Epoch 80, val loss: 1.828296184539795
Epoch 90, training loss: 8.72727108001709 = 1.809100866317749 + 1.0 * 6.91817045211792
Epoch 90, val loss: 1.815733790397644
Epoch 100, training loss: 8.625692367553711 = 1.7949881553649902 + 1.0 * 6.830704212188721
Epoch 100, val loss: 1.8032417297363281
Epoch 110, training loss: 8.543893814086914 = 1.7804744243621826 + 1.0 * 6.763419151306152
Epoch 110, val loss: 1.7900606393814087
Epoch 120, training loss: 8.480461120605469 = 1.7648835182189941 + 1.0 * 6.715577602386475
Epoch 120, val loss: 1.7757980823516846
Epoch 130, training loss: 8.425697326660156 = 1.7477397918701172 + 1.0 * 6.677957057952881
Epoch 130, val loss: 1.7605489492416382
Epoch 140, training loss: 8.377076148986816 = 1.7285780906677246 + 1.0 * 6.648498058319092
Epoch 140, val loss: 1.7438883781433105
Epoch 150, training loss: 8.330101013183594 = 1.7066534757614136 + 1.0 * 6.623447895050049
Epoch 150, val loss: 1.7252168655395508
Epoch 160, training loss: 8.282516479492188 = 1.6811803579330444 + 1.0 * 6.6013360023498535
Epoch 160, val loss: 1.703740119934082
Epoch 170, training loss: 8.235517501831055 = 1.651758074760437 + 1.0 * 6.583759307861328
Epoch 170, val loss: 1.679124355316162
Epoch 180, training loss: 8.184364318847656 = 1.6179468631744385 + 1.0 * 6.566417694091797
Epoch 180, val loss: 1.650960922241211
Epoch 190, training loss: 8.130393028259277 = 1.5790711641311646 + 1.0 * 6.551321983337402
Epoch 190, val loss: 1.6185641288757324
Epoch 200, training loss: 8.074339866638184 = 1.5357047319412231 + 1.0 * 6.53863525390625
Epoch 200, val loss: 1.582770586013794
Epoch 210, training loss: 8.013321876525879 = 1.4884049892425537 + 1.0 * 6.524917125701904
Epoch 210, val loss: 1.5437523126602173
Epoch 220, training loss: 7.958039283752441 = 1.4374639987945557 + 1.0 * 6.520575523376465
Epoch 220, val loss: 1.5019075870513916
Epoch 230, training loss: 7.891082763671875 = 1.384305715560913 + 1.0 * 6.506777286529541
Epoch 230, val loss: 1.4582990407943726
Epoch 240, training loss: 7.827273845672607 = 1.3294973373413086 + 1.0 * 6.497776508331299
Epoch 240, val loss: 1.4138333797454834
Epoch 250, training loss: 7.764416694641113 = 1.2735925912857056 + 1.0 * 6.490824222564697
Epoch 250, val loss: 1.3692927360534668
Epoch 260, training loss: 7.703446388244629 = 1.218322515487671 + 1.0 * 6.485124111175537
Epoch 260, val loss: 1.3262792825698853
Epoch 270, training loss: 7.643381118774414 = 1.1646382808685303 + 1.0 * 6.478742599487305
Epoch 270, val loss: 1.2852745056152344
Epoch 280, training loss: 7.583673477172852 = 1.1119461059570312 + 1.0 * 6.47172737121582
Epoch 280, val loss: 1.246120810508728
Epoch 290, training loss: 7.52633810043335 = 1.060275673866272 + 1.0 * 6.466062545776367
Epoch 290, val loss: 1.208527684211731
Epoch 300, training loss: 7.483672142028809 = 1.0098825693130493 + 1.0 * 6.473789691925049
Epoch 300, val loss: 1.172675371170044
Epoch 310, training loss: 7.42270040512085 = 0.9621285200119019 + 1.0 * 6.460571765899658
Epoch 310, val loss: 1.1395196914672852
Epoch 320, training loss: 7.368864059448242 = 0.9166651368141174 + 1.0 * 6.4521989822387695
Epoch 320, val loss: 1.1086697578430176
Epoch 330, training loss: 7.320858955383301 = 0.8733317852020264 + 1.0 * 6.4475274085998535
Epoch 330, val loss: 1.079945683479309
Epoch 340, training loss: 7.281576156616211 = 0.8325782418251038 + 1.0 * 6.448997974395752
Epoch 340, val loss: 1.0537739992141724
Epoch 350, training loss: 7.23505163192749 = 0.7947368621826172 + 1.0 * 6.440314769744873
Epoch 350, val loss: 1.030293583869934
Epoch 360, training loss: 7.193580150604248 = 0.7591347098350525 + 1.0 * 6.434445381164551
Epoch 360, val loss: 1.0091934204101562
Epoch 370, training loss: 7.155961513519287 = 0.7254657745361328 + 1.0 * 6.430495738983154
Epoch 370, val loss: 0.9901371598243713
Epoch 380, training loss: 7.124361991882324 = 0.693755567073822 + 1.0 * 6.430606365203857
Epoch 380, val loss: 0.9732065796852112
Epoch 390, training loss: 7.088770389556885 = 0.663826584815979 + 1.0 * 6.424943923950195
Epoch 390, val loss: 0.9583994746208191
Epoch 400, training loss: 7.054590702056885 = 0.6351748704910278 + 1.0 * 6.4194159507751465
Epoch 400, val loss: 0.9451262950897217
Epoch 410, training loss: 7.025801658630371 = 0.6073977947235107 + 1.0 * 6.4184041023254395
Epoch 410, val loss: 0.9331433176994324
Epoch 420, training loss: 7.001384735107422 = 0.5804985165596008 + 1.0 * 6.420886039733887
Epoch 420, val loss: 0.9222436547279358
Epoch 430, training loss: 6.965654373168945 = 0.5544734597206116 + 1.0 * 6.4111809730529785
Epoch 430, val loss: 0.9125073552131653
Epoch 440, training loss: 6.93582010269165 = 0.5290769934654236 + 1.0 * 6.406743049621582
Epoch 440, val loss: 0.9037173390388489
Epoch 450, training loss: 6.912670612335205 = 0.5042157173156738 + 1.0 * 6.408454895019531
Epoch 450, val loss: 0.8957892656326294
Epoch 460, training loss: 6.8818678855896 = 0.479997843503952 + 1.0 * 6.401870250701904
Epoch 460, val loss: 0.8887132406234741
Epoch 470, training loss: 6.854855060577393 = 0.45655766129493713 + 1.0 * 6.398297309875488
Epoch 470, val loss: 0.8827255368232727
Epoch 480, training loss: 6.832958698272705 = 0.43389594554901123 + 1.0 * 6.399062633514404
Epoch 480, val loss: 0.8777103424072266
Epoch 490, training loss: 6.810194492340088 = 0.41227594017982483 + 1.0 * 6.397918701171875
Epoch 490, val loss: 0.8738749623298645
Epoch 500, training loss: 6.783834457397461 = 0.39158132672309875 + 1.0 * 6.3922529220581055
Epoch 500, val loss: 0.8711071610450745
Epoch 510, training loss: 6.763405799865723 = 0.37183380126953125 + 1.0 * 6.391571998596191
Epoch 510, val loss: 0.8694609999656677
Epoch 520, training loss: 6.7397141456604 = 0.3531050384044647 + 1.0 * 6.386609077453613
Epoch 520, val loss: 0.868816077709198
Epoch 530, training loss: 6.732595443725586 = 0.33534955978393555 + 1.0 * 6.39724588394165
Epoch 530, val loss: 0.8690778613090515
Epoch 540, training loss: 6.701516151428223 = 0.3185904920101166 + 1.0 * 6.382925510406494
Epoch 540, val loss: 0.8702455759048462
Epoch 550, training loss: 6.6831817626953125 = 0.3027583658695221 + 1.0 * 6.380423545837402
Epoch 550, val loss: 0.8722707629203796
Epoch 560, training loss: 6.665713310241699 = 0.287688672542572 + 1.0 * 6.378024578094482
Epoch 560, val loss: 0.875045657157898
Epoch 570, training loss: 6.655095100402832 = 0.27333009243011475 + 1.0 * 6.381764888763428
Epoch 570, val loss: 0.8784101009368896
Epoch 580, training loss: 6.64021635055542 = 0.2597607970237732 + 1.0 * 6.380455493927002
Epoch 580, val loss: 0.8823880553245544
Epoch 590, training loss: 6.618973255157471 = 0.24680525064468384 + 1.0 * 6.372168064117432
Epoch 590, val loss: 0.8868741393089294
Epoch 600, training loss: 6.6071648597717285 = 0.23437786102294922 + 1.0 * 6.372786998748779
Epoch 600, val loss: 0.8918386101722717
Epoch 610, training loss: 6.593470096588135 = 0.22253534197807312 + 1.0 * 6.370934963226318
Epoch 610, val loss: 0.8970920443534851
Epoch 620, training loss: 6.584188461303711 = 0.21125394105911255 + 1.0 * 6.372934341430664
Epoch 620, val loss: 0.9025756120681763
Epoch 630, training loss: 6.5712199211120605 = 0.20055034756660461 + 1.0 * 6.370669364929199
Epoch 630, val loss: 0.9082577228546143
Epoch 640, training loss: 6.554303169250488 = 0.19035008549690247 + 1.0 * 6.363953113555908
Epoch 640, val loss: 0.914101243019104
Epoch 650, training loss: 6.542541027069092 = 0.18057222664356232 + 1.0 * 6.361968994140625
Epoch 650, val loss: 0.9201383590698242
Epoch 660, training loss: 6.5315632820129395 = 0.171192929148674 + 1.0 * 6.36037015914917
Epoch 660, val loss: 0.926279604434967
Epoch 670, training loss: 6.52132511138916 = 0.16219443082809448 + 1.0 * 6.359130859375
Epoch 670, val loss: 0.9325333833694458
Epoch 680, training loss: 6.525426864624023 = 0.1535746157169342 + 1.0 * 6.371852397918701
Epoch 680, val loss: 0.9388657808303833
Epoch 690, training loss: 6.504868984222412 = 0.14538413286209106 + 1.0 * 6.359484672546387
Epoch 690, val loss: 0.9451905488967896
Epoch 700, training loss: 6.494204044342041 = 0.13761737942695618 + 1.0 * 6.356586456298828
Epoch 700, val loss: 0.9515223503112793
Epoch 710, training loss: 6.484286785125732 = 0.13023890554904938 + 1.0 * 6.354047775268555
Epoch 710, val loss: 0.9579636454582214
Epoch 720, training loss: 6.485572814941406 = 0.12324576079845428 + 1.0 * 6.3623270988464355
Epoch 720, val loss: 0.9644632935523987
Epoch 730, training loss: 6.471104145050049 = 0.11661643534898758 + 1.0 * 6.354487895965576
Epoch 730, val loss: 0.9709340333938599
Epoch 740, training loss: 6.461336135864258 = 0.11037260293960571 + 1.0 * 6.350963592529297
Epoch 740, val loss: 0.9774655103683472
Epoch 750, training loss: 6.459356307983398 = 0.10445991158485413 + 1.0 * 6.354896545410156
Epoch 750, val loss: 0.9840795397758484
Epoch 760, training loss: 6.447849273681641 = 0.09890806674957275 + 1.0 * 6.348941326141357
Epoch 760, val loss: 0.9906800389289856
Epoch 770, training loss: 6.442460060119629 = 0.09367794543504715 + 1.0 * 6.348782062530518
Epoch 770, val loss: 0.9973858594894409
Epoch 780, training loss: 6.43681526184082 = 0.08877597749233246 + 1.0 * 6.348039150238037
Epoch 780, val loss: 1.0040102005004883
Epoch 790, training loss: 6.429067611694336 = 0.08417344093322754 + 1.0 * 6.344893932342529
Epoch 790, val loss: 1.0106477737426758
Epoch 800, training loss: 6.422569751739502 = 0.07985402643680573 + 1.0 * 6.342715740203857
Epoch 800, val loss: 1.0173109769821167
Epoch 810, training loss: 6.417069911956787 = 0.07579231262207031 + 1.0 * 6.341277599334717
Epoch 810, val loss: 1.02398681640625
Epoch 820, training loss: 6.425830841064453 = 0.07198182493448257 + 1.0 * 6.353848934173584
Epoch 820, val loss: 1.0306344032287598
Epoch 830, training loss: 6.408982753753662 = 0.06840643286705017 + 1.0 * 6.340576171875
Epoch 830, val loss: 1.037161946296692
Epoch 840, training loss: 6.403520584106445 = 0.06508059799671173 + 1.0 * 6.33843994140625
Epoch 840, val loss: 1.0436468124389648
Epoch 850, training loss: 6.3989577293396 = 0.06195602938532829 + 1.0 * 6.337001800537109
Epoch 850, val loss: 1.0502022504806519
Epoch 860, training loss: 6.39772891998291 = 0.05901917815208435 + 1.0 * 6.338709831237793
Epoch 860, val loss: 1.0567331314086914
Epoch 870, training loss: 6.3930583000183105 = 0.056265369057655334 + 1.0 * 6.336792945861816
Epoch 870, val loss: 1.063170075416565
Epoch 880, training loss: 6.388123512268066 = 0.05368351563811302 + 1.0 * 6.334440231323242
Epoch 880, val loss: 1.0695439577102661
Epoch 890, training loss: 6.384815692901611 = 0.05125885829329491 + 1.0 * 6.333556652069092
Epoch 890, val loss: 1.0758880376815796
Epoch 900, training loss: 6.388887882232666 = 0.04898172244429588 + 1.0 * 6.339906215667725
Epoch 900, val loss: 1.0821806192398071
Epoch 910, training loss: 6.381557464599609 = 0.04683861508965492 + 1.0 * 6.334718704223633
Epoch 910, val loss: 1.0884331464767456
Epoch 920, training loss: 6.37475061416626 = 0.04482300579547882 + 1.0 * 6.329927444458008
Epoch 920, val loss: 1.0946418046951294
Epoch 930, training loss: 6.371150016784668 = 0.042924534529447556 + 1.0 * 6.328225612640381
Epoch 930, val loss: 1.100781798362732
Epoch 940, training loss: 6.376744747161865 = 0.04113716259598732 + 1.0 * 6.335607528686523
Epoch 940, val loss: 1.106879711151123
Epoch 950, training loss: 6.36848258972168 = 0.03944439813494682 + 1.0 * 6.329038143157959
Epoch 950, val loss: 1.1129449605941772
Epoch 960, training loss: 6.36992883682251 = 0.03785740211606026 + 1.0 * 6.332071304321289
Epoch 960, val loss: 1.1188956499099731
Epoch 970, training loss: 6.365069389343262 = 0.036358289420604706 + 1.0 * 6.328711032867432
Epoch 970, val loss: 1.1247432231903076
Epoch 980, training loss: 6.35901403427124 = 0.034942734986543655 + 1.0 * 6.324071407318115
Epoch 980, val loss: 1.1305617094039917
Epoch 990, training loss: 6.356375217437744 = 0.03360915929079056 + 1.0 * 6.322765827178955
Epoch 990, val loss: 1.1362557411193848
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 10.550623893737793 = 1.9537866115570068 + 1.0 * 8.596837043762207
Epoch 0, val loss: 1.9546558856964111
Epoch 10, training loss: 10.539772987365723 = 1.9431610107421875 + 1.0 * 8.596611976623535
Epoch 10, val loss: 1.9438118934631348
Epoch 20, training loss: 10.525084495544434 = 1.9299343824386597 + 1.0 * 8.595149993896484
Epoch 20, val loss: 1.9303455352783203
Epoch 30, training loss: 10.495567321777344 = 1.9112980365753174 + 1.0 * 8.584269523620605
Epoch 30, val loss: 1.911317229270935
Epoch 40, training loss: 10.398139953613281 = 1.8858661651611328 + 1.0 * 8.512273788452148
Epoch 40, val loss: 1.8863016366958618
Epoch 50, training loss: 10.038946151733398 = 1.8591973781585693 + 1.0 * 8.17974853515625
Epoch 50, val loss: 1.8614320755004883
Epoch 60, training loss: 9.6406831741333 = 1.8372210264205933 + 1.0 * 7.803462028503418
Epoch 60, val loss: 1.8412848711013794
Epoch 70, training loss: 9.227506637573242 = 1.8222880363464355 + 1.0 * 7.405218601226807
Epoch 70, val loss: 1.8281474113464355
Epoch 80, training loss: 9.026936531066895 = 1.8068090677261353 + 1.0 * 7.220127582550049
Epoch 80, val loss: 1.8144177198410034
Epoch 90, training loss: 8.827813148498535 = 1.7892674207687378 + 1.0 * 7.038546085357666
Epoch 90, val loss: 1.7998597621917725
Epoch 100, training loss: 8.70114803314209 = 1.7740765810012817 + 1.0 * 6.927071571350098
Epoch 100, val loss: 1.7872334718704224
Epoch 110, training loss: 8.603984832763672 = 1.759079933166504 + 1.0 * 6.84490442276001
Epoch 110, val loss: 1.7741796970367432
Epoch 120, training loss: 8.524998664855957 = 1.7426676750183105 + 1.0 * 6.7823309898376465
Epoch 120, val loss: 1.7598390579223633
Epoch 130, training loss: 8.449024200439453 = 1.724609613418579 + 1.0 * 6.724414348602295
Epoch 130, val loss: 1.7440954446792603
Epoch 140, training loss: 8.385662078857422 = 1.7040209770202637 + 1.0 * 6.681640625
Epoch 140, val loss: 1.726385235786438
Epoch 150, training loss: 8.32697868347168 = 1.680263876914978 + 1.0 * 6.64671516418457
Epoch 150, val loss: 1.7061306238174438
Epoch 160, training loss: 8.27193832397461 = 1.6532251834869385 + 1.0 * 6.61871337890625
Epoch 160, val loss: 1.6832337379455566
Epoch 170, training loss: 8.216822624206543 = 1.622851848602295 + 1.0 * 6.593970775604248
Epoch 170, val loss: 1.6575357913970947
Epoch 180, training loss: 8.16271686553955 = 1.5890957117080688 + 1.0 * 6.5736212730407715
Epoch 180, val loss: 1.62897527217865
Epoch 190, training loss: 8.109336853027344 = 1.5524754524230957 + 1.0 * 6.556861400604248
Epoch 190, val loss: 1.5980409383773804
Epoch 200, training loss: 8.055622100830078 = 1.5135250091552734 + 1.0 * 6.542097568511963
Epoch 200, val loss: 1.5654133558273315
Epoch 210, training loss: 8.001262664794922 = 1.4730803966522217 + 1.0 * 6.528182506561279
Epoch 210, val loss: 1.5316957235336304
Epoch 220, training loss: 7.948551177978516 = 1.431783676147461 + 1.0 * 6.516767501831055
Epoch 220, val loss: 1.4978256225585938
Epoch 230, training loss: 7.897097110748291 = 1.390511155128479 + 1.0 * 6.506586074829102
Epoch 230, val loss: 1.4644715785980225
Epoch 240, training loss: 7.844891548156738 = 1.3496431112289429 + 1.0 * 6.495248317718506
Epoch 240, val loss: 1.4320378303527832
Epoch 250, training loss: 7.797913551330566 = 1.309081792831421 + 1.0 * 6.488831520080566
Epoch 250, val loss: 1.400661826133728
Epoch 260, training loss: 7.746723175048828 = 1.2694838047027588 + 1.0 * 6.47723913192749
Epoch 260, val loss: 1.3709543943405151
Epoch 270, training loss: 7.70140266418457 = 1.2306288480758667 + 1.0 * 6.470773696899414
Epoch 270, val loss: 1.342576503753662
Epoch 280, training loss: 7.65666389465332 = 1.1922615766525269 + 1.0 * 6.464402198791504
Epoch 280, val loss: 1.315237045288086
Epoch 290, training loss: 7.614631652832031 = 1.1546648740768433 + 1.0 * 6.459966659545898
Epoch 290, val loss: 1.2889819145202637
Epoch 300, training loss: 7.569389343261719 = 1.1175483465194702 + 1.0 * 6.451840877532959
Epoch 300, val loss: 1.2635269165039062
Epoch 310, training loss: 7.525771141052246 = 1.0804429054260254 + 1.0 * 6.445328235626221
Epoch 310, val loss: 1.2383596897125244
Epoch 320, training loss: 7.487841606140137 = 1.043217420578003 + 1.0 * 6.444624423980713
Epoch 320, val loss: 1.2134093046188354
Epoch 330, training loss: 7.442869186401367 = 1.00628662109375 + 1.0 * 6.436582565307617
Epoch 330, val loss: 1.1886399984359741
Epoch 340, training loss: 7.40185546875 = 0.9696523547172546 + 1.0 * 6.43220329284668
Epoch 340, val loss: 1.1643648147583008
Epoch 350, training loss: 7.360636234283447 = 0.9331324696540833 + 1.0 * 6.42750358581543
Epoch 350, val loss: 1.1403143405914307
Epoch 360, training loss: 7.325411319732666 = 0.8970755338668823 + 1.0 * 6.428335666656494
Epoch 360, val loss: 1.116858720779419
Epoch 370, training loss: 7.281949520111084 = 0.8617132306098938 + 1.0 * 6.420236110687256
Epoch 370, val loss: 1.0942388772964478
Epoch 380, training loss: 7.245218753814697 = 0.8267250061035156 + 1.0 * 6.418493747711182
Epoch 380, val loss: 1.0720996856689453
Epoch 390, training loss: 7.208176612854004 = 0.792451024055481 + 1.0 * 6.4157257080078125
Epoch 390, val loss: 1.0508143901824951
Epoch 400, training loss: 7.168576717376709 = 0.7588929533958435 + 1.0 * 6.409683704376221
Epoch 400, val loss: 1.0305105447769165
Epoch 410, training loss: 7.132519721984863 = 0.7259871959686279 + 1.0 * 6.4065327644348145
Epoch 410, val loss: 1.0111660957336426
Epoch 420, training loss: 7.098405838012695 = 0.6938102841377258 + 1.0 * 6.404595375061035
Epoch 420, val loss: 0.9929454922676086
Epoch 430, training loss: 7.072465896606445 = 0.6626473069190979 + 1.0 * 6.409818649291992
Epoch 430, val loss: 0.975985586643219
Epoch 440, training loss: 7.035220146179199 = 0.6328617930412292 + 1.0 * 6.402358531951904
Epoch 440, val loss: 0.960541307926178
Epoch 450, training loss: 7.000654220581055 = 0.6041728258132935 + 1.0 * 6.396481513977051
Epoch 450, val loss: 0.9465187191963196
Epoch 460, training loss: 6.971969127655029 = 0.5765091776847839 + 1.0 * 6.39546012878418
Epoch 460, val loss: 0.933846116065979
Epoch 470, training loss: 6.943395614624023 = 0.5498495697975159 + 1.0 * 6.393546104431152
Epoch 470, val loss: 0.9225108027458191
Epoch 480, training loss: 6.917070388793945 = 0.5242244601249695 + 1.0 * 6.39284610748291
Epoch 480, val loss: 0.9125223755836487
Epoch 490, training loss: 6.888057231903076 = 0.499555766582489 + 1.0 * 6.3885016441345215
Epoch 490, val loss: 0.9038028717041016
Epoch 500, training loss: 6.862443923950195 = 0.47577372193336487 + 1.0 * 6.386670112609863
Epoch 500, val loss: 0.8962700366973877
Epoch 510, training loss: 6.8351335525512695 = 0.45276710391044617 + 1.0 * 6.38236665725708
Epoch 510, val loss: 0.8897972702980042
Epoch 520, training loss: 6.817086696624756 = 0.43042677640914917 + 1.0 * 6.386660099029541
Epoch 520, val loss: 0.884192943572998
Epoch 530, training loss: 6.792369842529297 = 0.40882179141044617 + 1.0 * 6.383548259735107
Epoch 530, val loss: 0.8795722126960754
Epoch 540, training loss: 6.764313220977783 = 0.3880068361759186 + 1.0 * 6.376306533813477
Epoch 540, val loss: 0.875857949256897
Epoch 550, training loss: 6.7430548667907715 = 0.3678213953971863 + 1.0 * 6.3752336502075195
Epoch 550, val loss: 0.8728350400924683
Epoch 560, training loss: 6.724983215332031 = 0.34834474325180054 + 1.0 * 6.376638412475586
Epoch 560, val loss: 0.8706197738647461
Epoch 570, training loss: 6.700372219085693 = 0.32961133122444153 + 1.0 * 6.370760917663574
Epoch 570, val loss: 0.8691766262054443
Epoch 580, training loss: 6.68265438079834 = 0.3116348385810852 + 1.0 * 6.37101936340332
Epoch 580, val loss: 0.8685215711593628
Epoch 590, training loss: 6.661841869354248 = 0.2944362461566925 + 1.0 * 6.367405414581299
Epoch 590, val loss: 0.868590772151947
Epoch 600, training loss: 6.6451640129089355 = 0.2780475318431854 + 1.0 * 6.367116451263428
Epoch 600, val loss: 0.8694692254066467
Epoch 610, training loss: 6.632046222686768 = 0.26244232058525085 + 1.0 * 6.369604110717773
Epoch 610, val loss: 0.8709136247634888
Epoch 620, training loss: 6.612633228302002 = 0.24767392873764038 + 1.0 * 6.364959239959717
Epoch 620, val loss: 0.8730763792991638
Epoch 630, training loss: 6.594564914703369 = 0.23373602330684662 + 1.0 * 6.360828876495361
Epoch 630, val loss: 0.8759944438934326
Epoch 640, training loss: 6.581569194793701 = 0.22055478394031525 + 1.0 * 6.361014366149902
Epoch 640, val loss: 0.8794237375259399
Epoch 650, training loss: 6.565004348754883 = 0.20814378559589386 + 1.0 * 6.356860637664795
Epoch 650, val loss: 0.8835024237632751
Epoch 660, training loss: 6.551730632781982 = 0.196466863155365 + 1.0 * 6.355263710021973
Epoch 660, val loss: 0.8880195617675781
Epoch 670, training loss: 6.549758434295654 = 0.1854919046163559 + 1.0 * 6.364266395568848
Epoch 670, val loss: 0.8929585218429565
Epoch 680, training loss: 6.527380466461182 = 0.17522676289081573 + 1.0 * 6.352153778076172
Epoch 680, val loss: 0.8983253240585327
Epoch 690, training loss: 6.517021656036377 = 0.16557994484901428 + 1.0 * 6.351441860198975
Epoch 690, val loss: 0.9040773510932922
Epoch 700, training loss: 6.514359951019287 = 0.15651321411132812 + 1.0 * 6.357846736907959
Epoch 700, val loss: 0.9101153612136841
Epoch 710, training loss: 6.498523712158203 = 0.14803016185760498 + 1.0 * 6.350493431091309
Epoch 710, val loss: 0.9165254235267639
Epoch 720, training loss: 6.48676872253418 = 0.14005476236343384 + 1.0 * 6.346714019775391
Epoch 720, val loss: 0.9231008291244507
Epoch 730, training loss: 6.479139804840088 = 0.1325511336326599 + 1.0 * 6.346588611602783
Epoch 730, val loss: 0.9299492835998535
Epoch 740, training loss: 6.473232269287109 = 0.12550237774848938 + 1.0 * 6.347729682922363
Epoch 740, val loss: 0.9369893074035645
Epoch 750, training loss: 6.464341640472412 = 0.11891404539346695 + 1.0 * 6.345427513122559
Epoch 750, val loss: 0.944295346736908
Epoch 760, training loss: 6.453218936920166 = 0.11271486431360245 + 1.0 * 6.340504169464111
Epoch 760, val loss: 0.9516831040382385
Epoch 770, training loss: 6.45444917678833 = 0.10689418017864227 + 1.0 * 6.347555160522461
Epoch 770, val loss: 0.959238588809967
Epoch 780, training loss: 6.439199924468994 = 0.10143604129552841 + 1.0 * 6.337763786315918
Epoch 780, val loss: 0.9668417572975159
Epoch 790, training loss: 6.433564186096191 = 0.09630697965621948 + 1.0 * 6.337257385253906
Epoch 790, val loss: 0.9746363759040833
Epoch 800, training loss: 6.432332515716553 = 0.09147945791482925 + 1.0 * 6.340853214263916
Epoch 800, val loss: 0.9825329780578613
Epoch 810, training loss: 6.428205490112305 = 0.08695874363183975 + 1.0 * 6.341246604919434
Epoch 810, val loss: 0.9905043840408325
Epoch 820, training loss: 6.419793128967285 = 0.08271609246730804 + 1.0 * 6.3370771408081055
Epoch 820, val loss: 0.9984722137451172
Epoch 830, training loss: 6.411840438842773 = 0.07873493432998657 + 1.0 * 6.333105564117432
Epoch 830, val loss: 1.0064760446548462
Epoch 840, training loss: 6.407559871673584 = 0.07498662173748016 + 1.0 * 6.332573413848877
Epoch 840, val loss: 1.0145071744918823
Epoch 850, training loss: 6.404412746429443 = 0.0714583769440651 + 1.0 * 6.332954406738281
Epoch 850, val loss: 1.0225670337677002
Epoch 860, training loss: 6.397102355957031 = 0.0681399330496788 + 1.0 * 6.328962326049805
Epoch 860, val loss: 1.0306646823883057
Epoch 870, training loss: 6.400226593017578 = 0.06501919776201248 + 1.0 * 6.335207462310791
Epoch 870, val loss: 1.038611650466919
Epoch 880, training loss: 6.393348693847656 = 0.06210680305957794 + 1.0 * 6.331242084503174
Epoch 880, val loss: 1.0465784072875977
Epoch 890, training loss: 6.384373188018799 = 0.059361781924963 + 1.0 * 6.325011253356934
Epoch 890, val loss: 1.0545403957366943
Epoch 900, training loss: 6.381274700164795 = 0.056775398552417755 + 1.0 * 6.324499130249023
Epoch 900, val loss: 1.0624948740005493
Epoch 910, training loss: 6.385162353515625 = 0.054332684725522995 + 1.0 * 6.330829620361328
Epoch 910, val loss: 1.070441722869873
Epoch 920, training loss: 6.38098669052124 = 0.05203381925821304 + 1.0 * 6.328952789306641
Epoch 920, val loss: 1.0781842470169067
Epoch 930, training loss: 6.375000476837158 = 0.04987305775284767 + 1.0 * 6.325127601623535
Epoch 930, val loss: 1.085903525352478
Epoch 940, training loss: 6.368535995483398 = 0.04783482477068901 + 1.0 * 6.3207011222839355
Epoch 940, val loss: 1.093583106994629
Epoch 950, training loss: 6.365755081176758 = 0.04590825363993645 + 1.0 * 6.3198466300964355
Epoch 950, val loss: 1.1011755466461182
Epoch 960, training loss: 6.368455410003662 = 0.04408687353134155 + 1.0 * 6.324368476867676
Epoch 960, val loss: 1.1086806058883667
Epoch 970, training loss: 6.364071846008301 = 0.042368341237306595 + 1.0 * 6.3217034339904785
Epoch 970, val loss: 1.1161997318267822
Epoch 980, training loss: 6.360987663269043 = 0.04074008762836456 + 1.0 * 6.320247650146484
Epoch 980, val loss: 1.1235301494598389
Epoch 990, training loss: 6.359776973724365 = 0.03920488432049751 + 1.0 * 6.3205718994140625
Epoch 990, val loss: 1.1307313442230225
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.7986294148655773
=== training gcn model ===
Epoch 0, training loss: 10.535724639892578 = 1.938869595527649 + 1.0 * 8.596855163574219
Epoch 0, val loss: 1.935597538948059
Epoch 10, training loss: 10.525773048400879 = 1.9291317462921143 + 1.0 * 8.596641540527344
Epoch 10, val loss: 1.9258508682250977
Epoch 20, training loss: 10.51182746887207 = 1.9168362617492676 + 1.0 * 8.594990730285645
Epoch 20, val loss: 1.9129749536514282
Epoch 30, training loss: 10.480255126953125 = 1.8995485305786133 + 1.0 * 8.580706596374512
Epoch 30, val loss: 1.8944815397262573
Epoch 40, training loss: 10.346080780029297 = 1.876761794090271 + 1.0 * 8.469319343566895
Epoch 40, val loss: 1.871087670326233
Epoch 50, training loss: 9.911248207092285 = 1.8551162481307983 + 1.0 * 8.056132316589355
Epoch 50, val loss: 1.8508158922195435
Epoch 60, training loss: 9.356714248657227 = 1.8416516780853271 + 1.0 * 7.51506233215332
Epoch 60, val loss: 1.8387399911880493
Epoch 70, training loss: 9.027557373046875 = 1.8303700685501099 + 1.0 * 7.197187423706055
Epoch 70, val loss: 1.8285036087036133
Epoch 80, training loss: 8.837920188903809 = 1.8165282011032104 + 1.0 * 7.021391868591309
Epoch 80, val loss: 1.8156342506408691
Epoch 90, training loss: 8.70808219909668 = 1.801924705505371 + 1.0 * 6.906157493591309
Epoch 90, val loss: 1.802491545677185
Epoch 100, training loss: 8.619725227355957 = 1.7885279655456543 + 1.0 * 6.831197261810303
Epoch 100, val loss: 1.7907681465148926
Epoch 110, training loss: 8.551502227783203 = 1.7756426334381104 + 1.0 * 6.775859355926514
Epoch 110, val loss: 1.7795637845993042
Epoch 120, training loss: 8.500033378601074 = 1.7620627880096436 + 1.0 * 6.73797082901001
Epoch 120, val loss: 1.7679812908172607
Epoch 130, training loss: 8.438883781433105 = 1.747251272201538 + 1.0 * 6.6916327476501465
Epoch 130, val loss: 1.7555148601531982
Epoch 140, training loss: 8.389537811279297 = 1.7306493520736694 + 1.0 * 6.658888339996338
Epoch 140, val loss: 1.7418094873428345
Epoch 150, training loss: 8.347173690795898 = 1.7116464376449585 + 1.0 * 6.63552713394165
Epoch 150, val loss: 1.726243019104004
Epoch 160, training loss: 8.299247741699219 = 1.6898601055145264 + 1.0 * 6.6093878746032715
Epoch 160, val loss: 1.708502173423767
Epoch 170, training loss: 8.253728866577148 = 1.664678931236267 + 1.0 * 6.58905029296875
Epoch 170, val loss: 1.6881871223449707
Epoch 180, training loss: 8.21055793762207 = 1.6356786489486694 + 1.0 * 6.574879169464111
Epoch 180, val loss: 1.6649655103683472
Epoch 190, training loss: 8.160006523132324 = 1.6027476787567139 + 1.0 * 6.557258605957031
Epoch 190, val loss: 1.6388468742370605
Epoch 200, training loss: 8.10879135131836 = 1.5656418800354004 + 1.0 * 6.543148994445801
Epoch 200, val loss: 1.609477162361145
Epoch 210, training loss: 8.056300163269043 = 1.5241243839263916 + 1.0 * 6.5321760177612305
Epoch 210, val loss: 1.576720118522644
Epoch 220, training loss: 8.002450942993164 = 1.479536771774292 + 1.0 * 6.522913932800293
Epoch 220, val loss: 1.5418506860733032
Epoch 230, training loss: 7.9446868896484375 = 1.4326708316802979 + 1.0 * 6.5120158195495605
Epoch 230, val loss: 1.505720853805542
Epoch 240, training loss: 7.886359214782715 = 1.383768081665039 + 1.0 * 6.502591133117676
Epoch 240, val loss: 1.4683527946472168
Epoch 250, training loss: 7.828614711761475 = 1.333145022392273 + 1.0 * 6.495469570159912
Epoch 250, val loss: 1.430381417274475
Epoch 260, training loss: 7.776853561401367 = 1.282015085220337 + 1.0 * 6.494838714599609
Epoch 260, val loss: 1.392748475074768
Epoch 270, training loss: 7.713805198669434 = 1.2317458391189575 + 1.0 * 6.482059478759766
Epoch 270, val loss: 1.3562456369400024
Epoch 280, training loss: 7.656772613525391 = 1.181840181350708 + 1.0 * 6.4749321937561035
Epoch 280, val loss: 1.3206489086151123
Epoch 290, training loss: 7.606273651123047 = 1.1326037645339966 + 1.0 * 6.47367000579834
Epoch 290, val loss: 1.28585946559906
Epoch 300, training loss: 7.5506463050842285 = 1.0848606824874878 + 1.0 * 6.465785503387451
Epoch 300, val loss: 1.2525792121887207
Epoch 310, training loss: 7.498029708862305 = 1.0390504598617554 + 1.0 * 6.45897912979126
Epoch 310, val loss: 1.2208186388015747
Epoch 320, training loss: 7.449049949645996 = 0.9947060942649841 + 1.0 * 6.454343795776367
Epoch 320, val loss: 1.1902525424957275
Epoch 330, training loss: 7.400923252105713 = 0.9517102241516113 + 1.0 * 6.449213027954102
Epoch 330, val loss: 1.1605749130249023
Epoch 340, training loss: 7.364857196807861 = 0.9101687073707581 + 1.0 * 6.454688549041748
Epoch 340, val loss: 1.1318331956863403
Epoch 350, training loss: 7.314425945281982 = 0.8708056807518005 + 1.0 * 6.443620204925537
Epoch 350, val loss: 1.104864239692688
Epoch 360, training loss: 7.2725043296813965 = 0.8336347937583923 + 1.0 * 6.438869476318359
Epoch 360, val loss: 1.0798043012619019
Epoch 370, training loss: 7.231784820556641 = 0.7983090877532959 + 1.0 * 6.433475971221924
Epoch 370, val loss: 1.056384801864624
Epoch 380, training loss: 7.195322513580322 = 0.7646579742431641 + 1.0 * 6.430664539337158
Epoch 380, val loss: 1.034525990486145
Epoch 390, training loss: 7.161136627197266 = 0.7327966690063477 + 1.0 * 6.428339958190918
Epoch 390, val loss: 1.0146074295043945
Epoch 400, training loss: 7.125084400177002 = 0.7027606964111328 + 1.0 * 6.422323703765869
Epoch 400, val loss: 0.9965717196464539
Epoch 410, training loss: 7.107780933380127 = 0.6741884350776672 + 1.0 * 6.433592319488525
Epoch 410, val loss: 0.98035728931427
Epoch 420, training loss: 7.065051555633545 = 0.6471996903419495 + 1.0 * 6.41785192489624
Epoch 420, val loss: 0.9657648205757141
Epoch 430, training loss: 7.035075664520264 = 0.621464192867279 + 1.0 * 6.41361141204834
Epoch 430, val loss: 0.9529147744178772
Epoch 440, training loss: 7.007720470428467 = 0.5966688394546509 + 1.0 * 6.4110517501831055
Epoch 440, val loss: 0.9414116144180298
Epoch 450, training loss: 6.983663082122803 = 0.5727936029434204 + 1.0 * 6.410869598388672
Epoch 450, val loss: 0.9310177564620972
Epoch 460, training loss: 6.956623077392578 = 0.5497413277626038 + 1.0 * 6.406881809234619
Epoch 460, val loss: 0.9219071865081787
Epoch 470, training loss: 6.930752754211426 = 0.5273221135139465 + 1.0 * 6.403430461883545
Epoch 470, val loss: 0.9136651158332825
Epoch 480, training loss: 6.910982131958008 = 0.505384624004364 + 1.0 * 6.405597686767578
Epoch 480, val loss: 0.9060881733894348
Epoch 490, training loss: 6.883349895477295 = 0.48392564058303833 + 1.0 * 6.399424076080322
Epoch 490, val loss: 0.8990751504898071
Epoch 500, training loss: 6.864398956298828 = 0.4628376364707947 + 1.0 * 6.401561260223389
Epoch 500, val loss: 0.8927388191223145
Epoch 510, training loss: 6.83607816696167 = 0.44211214780807495 + 1.0 * 6.393966197967529
Epoch 510, val loss: 0.8868472576141357
Epoch 520, training loss: 6.816989898681641 = 0.42176786065101624 + 1.0 * 6.395222187042236
Epoch 520, val loss: 0.8814588189125061
Epoch 530, training loss: 6.793137550354004 = 0.4018394351005554 + 1.0 * 6.391298294067383
Epoch 530, val loss: 0.876614511013031
Epoch 540, training loss: 6.769792556762695 = 0.3822803497314453 + 1.0 * 6.38751220703125
Epoch 540, val loss: 0.8723395466804504
Epoch 550, training loss: 6.753406047821045 = 0.3631420433521271 + 1.0 * 6.39026403427124
Epoch 550, val loss: 0.8685253262519836
Epoch 560, training loss: 6.7373480796813965 = 0.34450069069862366 + 1.0 * 6.392847537994385
Epoch 560, val loss: 0.865132749080658
Epoch 570, training loss: 6.709331035614014 = 0.3264957666397095 + 1.0 * 6.382835388183594
Epoch 570, val loss: 0.8624394536018372
Epoch 580, training loss: 6.689894676208496 = 0.30905354022979736 + 1.0 * 6.380841255187988
Epoch 580, val loss: 0.8603554964065552
Epoch 590, training loss: 6.669006824493408 = 0.2920762002468109 + 1.0 * 6.3769307136535645
Epoch 590, val loss: 0.8587226867675781
Epoch 600, training loss: 6.6586151123046875 = 0.2756010890007019 + 1.0 * 6.38301420211792
Epoch 600, val loss: 0.8575615286827087
Epoch 610, training loss: 6.640018939971924 = 0.25968441367149353 + 1.0 * 6.380334377288818
Epoch 610, val loss: 0.8567067384719849
Epoch 620, training loss: 6.616744518280029 = 0.24443042278289795 + 1.0 * 6.372313976287842
Epoch 620, val loss: 0.856532096862793
Epoch 630, training loss: 6.6005706787109375 = 0.2297668159008026 + 1.0 * 6.3708038330078125
Epoch 630, val loss: 0.8568772673606873
Epoch 640, training loss: 6.5876240730285645 = 0.21576187014579773 + 1.0 * 6.371862411499023
Epoch 640, val loss: 0.8575994372367859
Epoch 650, training loss: 6.574169635772705 = 0.20251789689064026 + 1.0 * 6.371651649475098
Epoch 650, val loss: 0.858888566493988
Epoch 660, training loss: 6.55918025970459 = 0.19001083076000214 + 1.0 * 6.369169235229492
Epoch 660, val loss: 0.8607968091964722
Epoch 670, training loss: 6.54376745223999 = 0.1782245934009552 + 1.0 * 6.365542888641357
Epoch 670, val loss: 0.8632449507713318
Epoch 680, training loss: 6.536561012268066 = 0.1671711802482605 + 1.0 * 6.36939001083374
Epoch 680, val loss: 0.866240918636322
Epoch 690, training loss: 6.5237298011779785 = 0.15686264634132385 + 1.0 * 6.3668670654296875
Epoch 690, val loss: 0.869684100151062
Epoch 700, training loss: 6.5100884437561035 = 0.14730358123779297 + 1.0 * 6.3627848625183105
Epoch 700, val loss: 0.873670220375061
Epoch 710, training loss: 6.497583389282227 = 0.13843362033367157 + 1.0 * 6.359149932861328
Epoch 710, val loss: 0.8782650828361511
Epoch 720, training loss: 6.491359710693359 = 0.1301933079957962 + 1.0 * 6.361166477203369
Epoch 720, val loss: 0.8832622766494751
Epoch 730, training loss: 6.47862434387207 = 0.12255854904651642 + 1.0 * 6.35606575012207
Epoch 730, val loss: 0.8887128829956055
Epoch 740, training loss: 6.477830410003662 = 0.11547532677650452 + 1.0 * 6.3623552322387695
Epoch 740, val loss: 0.89451003074646
Epoch 750, training loss: 6.466839790344238 = 0.10893219709396362 + 1.0 * 6.357907772064209
Epoch 750, val loss: 0.9007233381271362
Epoch 760, training loss: 6.456113815307617 = 0.10286378115415573 + 1.0 * 6.353250026702881
Epoch 760, val loss: 0.9072279334068298
Epoch 770, training loss: 6.459029674530029 = 0.09722273051738739 + 1.0 * 6.361806869506836
Epoch 770, val loss: 0.9139583110809326
Epoch 780, training loss: 6.4450764656066895 = 0.09198831766843796 + 1.0 * 6.35308837890625
Epoch 780, val loss: 0.9208084940910339
Epoch 790, training loss: 6.4358625411987305 = 0.08714254200458527 + 1.0 * 6.348720073699951
Epoch 790, val loss: 0.9280011057853699
Epoch 800, training loss: 6.431130886077881 = 0.0826234370470047 + 1.0 * 6.348507404327393
Epoch 800, val loss: 0.9353334307670593
Epoch 810, training loss: 6.430682182312012 = 0.07840065658092499 + 1.0 * 6.35228157043457
Epoch 810, val loss: 0.9427337646484375
Epoch 820, training loss: 6.4226226806640625 = 0.0744573324918747 + 1.0 * 6.348165512084961
Epoch 820, val loss: 0.9502130746841431
Epoch 830, training loss: 6.4161810874938965 = 0.07079405337572098 + 1.0 * 6.345386981964111
Epoch 830, val loss: 0.9579264521598816
Epoch 840, training loss: 6.417233943939209 = 0.06736213713884354 + 1.0 * 6.349871635437012
Epoch 840, val loss: 0.9655287265777588
Epoch 850, training loss: 6.407180309295654 = 0.06416554749011993 + 1.0 * 6.343014717102051
Epoch 850, val loss: 0.9732896685600281
Epoch 860, training loss: 6.400588512420654 = 0.06117340922355652 + 1.0 * 6.339415073394775
Epoch 860, val loss: 0.9810812473297119
Epoch 870, training loss: 6.400773048400879 = 0.05836620554327965 + 1.0 * 6.342406749725342
Epoch 870, val loss: 0.988980770111084
Epoch 880, training loss: 6.394672393798828 = 0.05573436990380287 + 1.0 * 6.338938236236572
Epoch 880, val loss: 0.9967555403709412
Epoch 890, training loss: 6.392021179199219 = 0.053268104791641235 + 1.0 * 6.3387532234191895
Epoch 890, val loss: 1.0047361850738525
Epoch 900, training loss: 6.3933539390563965 = 0.05095891281962395 + 1.0 * 6.342394828796387
Epoch 900, val loss: 1.0127111673355103
Epoch 910, training loss: 6.385650157928467 = 0.04878535494208336 + 1.0 * 6.336864948272705
Epoch 910, val loss: 1.0205373764038086
Epoch 920, training loss: 6.3805437088012695 = 0.04674564674496651 + 1.0 * 6.333797931671143
Epoch 920, val loss: 1.0284879207611084
Epoch 930, training loss: 6.378566741943359 = 0.04482164606451988 + 1.0 * 6.333745002746582
Epoch 930, val loss: 1.0363268852233887
Epoch 940, training loss: 6.377716541290283 = 0.043006040155887604 + 1.0 * 6.334710597991943
Epoch 940, val loss: 1.0442525148391724
Epoch 950, training loss: 6.378809928894043 = 0.041295796632766724 + 1.0 * 6.3375139236450195
Epoch 950, val loss: 1.0519967079162598
Epoch 960, training loss: 6.371257305145264 = 0.03968736529350281 + 1.0 * 6.331570148468018
Epoch 960, val loss: 1.0597000122070312
Epoch 970, training loss: 6.369027137756348 = 0.03817468881607056 + 1.0 * 6.330852508544922
Epoch 970, val loss: 1.0674467086791992
Epoch 980, training loss: 6.371780872344971 = 0.036740776151418686 + 1.0 * 6.335040092468262
Epoch 980, val loss: 1.0749950408935547
Epoch 990, training loss: 6.3641157150268555 = 0.03539000824093819 + 1.0 * 6.328725814819336
Epoch 990, val loss: 1.0825450420379639
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8075909330521878
The final CL Acc:0.75062, 0.01429, The final GNN Acc:0.80408, 0.00391
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13198])
remove edge: torch.Size([2, 7822])
updated graph: torch.Size([2, 10464])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.545373916625977 = 1.9485273361206055 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9455487728118896
Epoch 10, training loss: 10.535248756408691 = 1.9386074542999268 + 1.0 * 8.596641540527344
Epoch 10, val loss: 1.935973048210144
Epoch 20, training loss: 10.521354675292969 = 1.9265167713165283 + 1.0 * 8.59483814239502
Epoch 20, val loss: 1.923933506011963
Epoch 30, training loss: 10.48843765258789 = 1.9096648693084717 + 1.0 * 8.57877254486084
Epoch 30, val loss: 1.9069123268127441
Epoch 40, training loss: 10.340898513793945 = 1.8878291845321655 + 1.0 * 8.453069686889648
Epoch 40, val loss: 1.885437250137329
Epoch 50, training loss: 9.91654109954834 = 1.8660547733306885 + 1.0 * 8.05048656463623
Epoch 50, val loss: 1.8649542331695557
Epoch 60, training loss: 9.385665893554688 = 1.8501176834106445 + 1.0 * 7.535548686981201
Epoch 60, val loss: 1.8498327732086182
Epoch 70, training loss: 9.001975059509277 = 1.8369916677474976 + 1.0 * 7.16498327255249
Epoch 70, val loss: 1.8370693922042847
Epoch 80, training loss: 8.797117233276367 = 1.8230341672897339 + 1.0 * 6.974082946777344
Epoch 80, val loss: 1.8240423202514648
Epoch 90, training loss: 8.665079116821289 = 1.80622398853302 + 1.0 * 6.8588547706604
Epoch 90, val loss: 1.8090741634368896
Epoch 100, training loss: 8.57320785522461 = 1.7890454530715942 + 1.0 * 6.7841620445251465
Epoch 100, val loss: 1.7939964532852173
Epoch 110, training loss: 8.502449035644531 = 1.7724884748458862 + 1.0 * 6.729960918426514
Epoch 110, val loss: 1.7794617414474487
Epoch 120, training loss: 8.442203521728516 = 1.7558190822601318 + 1.0 * 6.686384677886963
Epoch 120, val loss: 1.7648446559906006
Epoch 130, training loss: 8.389450073242188 = 1.7380280494689941 + 1.0 * 6.651422500610352
Epoch 130, val loss: 1.74944007396698
Epoch 140, training loss: 8.33952522277832 = 1.7183336019515991 + 1.0 * 6.621191501617432
Epoch 140, val loss: 1.7325527667999268
Epoch 150, training loss: 8.29355239868164 = 1.6958707571029663 + 1.0 * 6.597681999206543
Epoch 150, val loss: 1.7134759426116943
Epoch 160, training loss: 8.248025894165039 = 1.6699923276901245 + 1.0 * 6.578033447265625
Epoch 160, val loss: 1.6916732788085938
Epoch 170, training loss: 8.198283195495605 = 1.640055775642395 + 1.0 * 6.558227062225342
Epoch 170, val loss: 1.6666172742843628
Epoch 180, training loss: 8.147624969482422 = 1.6054555177688599 + 1.0 * 6.542169570922852
Epoch 180, val loss: 1.637608289718628
Epoch 190, training loss: 8.096864700317383 = 1.5655970573425293 + 1.0 * 6.531267166137695
Epoch 190, val loss: 1.6042197942733765
Epoch 200, training loss: 8.039684295654297 = 1.5209076404571533 + 1.0 * 6.518776893615723
Epoch 200, val loss: 1.566896677017212
Epoch 210, training loss: 7.980412483215332 = 1.471748948097229 + 1.0 * 6.508663654327393
Epoch 210, val loss: 1.5258288383483887
Epoch 220, training loss: 7.9187726974487305 = 1.4181888103485107 + 1.0 * 6.500584125518799
Epoch 220, val loss: 1.4811924695968628
Epoch 230, training loss: 7.85493278503418 = 1.361668348312378 + 1.0 * 6.493264198303223
Epoch 230, val loss: 1.434449553489685
Epoch 240, training loss: 7.790243625640869 = 1.3035731315612793 + 1.0 * 6.48667049407959
Epoch 240, val loss: 1.3867627382278442
Epoch 250, training loss: 7.724224090576172 = 1.2455281019210815 + 1.0 * 6.478695869445801
Epoch 250, val loss: 1.3393607139587402
Epoch 260, training loss: 7.665032386779785 = 1.188410758972168 + 1.0 * 6.476621627807617
Epoch 260, val loss: 1.2931925058364868
Epoch 270, training loss: 7.600223541259766 = 1.1337374448776245 + 1.0 * 6.466485977172852
Epoch 270, val loss: 1.2496161460876465
Epoch 280, training loss: 7.542581081390381 = 1.081790804862976 + 1.0 * 6.460790157318115
Epoch 280, val loss: 1.208791971206665
Epoch 290, training loss: 7.491645812988281 = 1.033029556274414 + 1.0 * 6.458616256713867
Epoch 290, val loss: 1.1710065603256226
Epoch 300, training loss: 7.438274383544922 = 0.9873536229133606 + 1.0 * 6.450920581817627
Epoch 300, val loss: 1.136240005493164
Epoch 310, training loss: 7.392752647399902 = 0.9439572095870972 + 1.0 * 6.448795318603516
Epoch 310, val loss: 1.103622317314148
Epoch 320, training loss: 7.346359729766846 = 0.9024881720542908 + 1.0 * 6.44387149810791
Epoch 320, val loss: 1.0728994607925415
Epoch 330, training loss: 7.30248498916626 = 0.8628851175308228 + 1.0 * 6.439599990844727
Epoch 330, val loss: 1.043821096420288
Epoch 340, training loss: 7.256471157073975 = 0.8247514367103577 + 1.0 * 6.431719779968262
Epoch 340, val loss: 1.0163111686706543
Epoch 350, training loss: 7.216161727905273 = 0.7877328991889954 + 1.0 * 6.428428649902344
Epoch 350, val loss: 0.9899009466171265
Epoch 360, training loss: 7.1759467124938965 = 0.7520350217819214 + 1.0 * 6.4239115715026855
Epoch 360, val loss: 0.9647883772850037
Epoch 370, training loss: 7.139743804931641 = 0.7180039286613464 + 1.0 * 6.4217400550842285
Epoch 370, val loss: 0.9413284063339233
Epoch 380, training loss: 7.10568380355835 = 0.6856580376625061 + 1.0 * 6.420025825500488
Epoch 380, val loss: 0.9196891784667969
Epoch 390, training loss: 7.06974458694458 = 0.6551682353019714 + 1.0 * 6.414576530456543
Epoch 390, val loss: 0.9001016020774841
Epoch 400, training loss: 7.037590503692627 = 0.6265420317649841 + 1.0 * 6.411048412322998
Epoch 400, val loss: 0.8825035095214844
Epoch 410, training loss: 7.009042263031006 = 0.5997413992881775 + 1.0 * 6.409300804138184
Epoch 410, val loss: 0.8668730854988098
Epoch 420, training loss: 6.978966236114502 = 0.5748112797737122 + 1.0 * 6.4041547775268555
Epoch 420, val loss: 0.8532593250274658
Epoch 430, training loss: 6.954226493835449 = 0.551636278629303 + 1.0 * 6.402590274810791
Epoch 430, val loss: 0.8414835333824158
Epoch 440, training loss: 6.927320957183838 = 0.5300692915916443 + 1.0 * 6.397251605987549
Epoch 440, val loss: 0.8313789963722229
Epoch 450, training loss: 6.902862071990967 = 0.5097740292549133 + 1.0 * 6.393087863922119
Epoch 450, val loss: 0.8226293921470642
Epoch 460, training loss: 6.882462978363037 = 0.4905303418636322 + 1.0 * 6.391932487487793
Epoch 460, val loss: 0.8150194883346558
Epoch 470, training loss: 6.871834754943848 = 0.4723645746707916 + 1.0 * 6.399470329284668
Epoch 470, val loss: 0.8083133101463318
Epoch 480, training loss: 6.841071605682373 = 0.4551564157009125 + 1.0 * 6.385915279388428
Epoch 480, val loss: 0.8025540113449097
Epoch 490, training loss: 6.821293830871582 = 0.4386107921600342 + 1.0 * 6.382683277130127
Epoch 490, val loss: 0.7973198294639587
Epoch 500, training loss: 6.804144382476807 = 0.42253199219703674 + 1.0 * 6.381612300872803
Epoch 500, val loss: 0.7924495339393616
Epoch 510, training loss: 6.784971714019775 = 0.4068657457828522 + 1.0 * 6.378106117248535
Epoch 510, val loss: 0.7878777384757996
Epoch 520, training loss: 6.770360946655273 = 0.3915436267852783 + 1.0 * 6.378817558288574
Epoch 520, val loss: 0.7836388349533081
Epoch 530, training loss: 6.750888824462891 = 0.3765598237514496 + 1.0 * 6.374329090118408
Epoch 530, val loss: 0.7795876264572144
Epoch 540, training loss: 6.732604503631592 = 0.36176931858062744 + 1.0 * 6.370835304260254
Epoch 540, val loss: 0.7757682800292969
Epoch 550, training loss: 6.727943420410156 = 0.3472457528114319 + 1.0 * 6.380697727203369
Epoch 550, val loss: 0.7721850275993347
Epoch 560, training loss: 6.704167366027832 = 0.3331410884857178 + 1.0 * 6.371026039123535
Epoch 560, val loss: 0.7689358592033386
Epoch 570, training loss: 6.6844305992126465 = 0.31942838430404663 + 1.0 * 6.365002155303955
Epoch 570, val loss: 0.7660884857177734
Epoch 580, training loss: 6.671558380126953 = 0.3061124086380005 + 1.0 * 6.365446090698242
Epoch 580, val loss: 0.7635922431945801
Epoch 590, training loss: 6.655247688293457 = 0.2932550013065338 + 1.0 * 6.361992835998535
Epoch 590, val loss: 0.7614975571632385
Epoch 600, training loss: 6.643404006958008 = 0.28086304664611816 + 1.0 * 6.362541198730469
Epoch 600, val loss: 0.7598450779914856
Epoch 610, training loss: 6.627442359924316 = 0.2689533531665802 + 1.0 * 6.358489036560059
Epoch 610, val loss: 0.7586063146591187
Epoch 620, training loss: 6.615750789642334 = 0.25745874643325806 + 1.0 * 6.358292102813721
Epoch 620, val loss: 0.7577359080314636
Epoch 630, training loss: 6.601728439331055 = 0.2463875561952591 + 1.0 * 6.355340957641602
Epoch 630, val loss: 0.7572396993637085
Epoch 640, training loss: 6.5975165367126465 = 0.2356954663991928 + 1.0 * 6.361821174621582
Epoch 640, val loss: 0.7571374177932739
Epoch 650, training loss: 6.577730178833008 = 0.22537113726139069 + 1.0 * 6.352358818054199
Epoch 650, val loss: 0.7574889063835144
Epoch 660, training loss: 6.565566062927246 = 0.21532994508743286 + 1.0 * 6.350235939025879
Epoch 660, val loss: 0.758260190486908
Epoch 670, training loss: 6.5565714836120605 = 0.2055029720067978 + 1.0 * 6.351068496704102
Epoch 670, val loss: 0.7594020962715149
Epoch 680, training loss: 6.5436553955078125 = 0.19589225947856903 + 1.0 * 6.3477630615234375
Epoch 680, val loss: 0.7609106302261353
Epoch 690, training loss: 6.534846305847168 = 0.1864732950925827 + 1.0 * 6.348372936248779
Epoch 690, val loss: 0.7628851532936096
Epoch 700, training loss: 6.5272440910339355 = 0.17725870013237 + 1.0 * 6.349985599517822
Epoch 700, val loss: 0.7651737332344055
Epoch 710, training loss: 6.511979103088379 = 0.16825678944587708 + 1.0 * 6.343722343444824
Epoch 710, val loss: 0.767853856086731
Epoch 720, training loss: 6.500702857971191 = 0.15944592654705048 + 1.0 * 6.341257095336914
Epoch 720, val loss: 0.770890474319458
Epoch 730, training loss: 6.4937920570373535 = 0.1508440524339676 + 1.0 * 6.342947959899902
Epoch 730, val loss: 0.7742955684661865
Epoch 740, training loss: 6.488152503967285 = 0.14251276850700378 + 1.0 * 6.345639705657959
Epoch 740, val loss: 0.7779862284660339
Epoch 750, training loss: 6.471954822540283 = 0.1345243602991104 + 1.0 * 6.337430477142334
Epoch 750, val loss: 0.7820844650268555
Epoch 760, training loss: 6.463540554046631 = 0.12685127556324005 + 1.0 * 6.336689472198486
Epoch 760, val loss: 0.7864859104156494
Epoch 770, training loss: 6.4590067863464355 = 0.11951353400945663 + 1.0 * 6.339493274688721
Epoch 770, val loss: 0.7910932302474976
Epoch 780, training loss: 6.453493595123291 = 0.11258148401975632 + 1.0 * 6.340912342071533
Epoch 780, val loss: 0.7959516644477844
Epoch 790, training loss: 6.440004348754883 = 0.10605259239673615 + 1.0 * 6.333951950073242
Epoch 790, val loss: 0.8009760975837708
Epoch 800, training loss: 6.439934730529785 = 0.09993068128824234 + 1.0 * 6.340003967285156
Epoch 800, val loss: 0.8062048554420471
Epoch 810, training loss: 6.42840051651001 = 0.09422077238559723 + 1.0 * 6.334179878234863
Epoch 810, val loss: 0.8115296959877014
Epoch 820, training loss: 6.419745922088623 = 0.08889789879322052 + 1.0 * 6.330848217010498
Epoch 820, val loss: 0.817087709903717
Epoch 830, training loss: 6.413564682006836 = 0.08392685651779175 + 1.0 * 6.3296380043029785
Epoch 830, val loss: 0.8226948380470276
Epoch 840, training loss: 6.41121768951416 = 0.07931525260210037 + 1.0 * 6.331902503967285
Epoch 840, val loss: 0.8283158540725708
Epoch 850, training loss: 6.403923034667969 = 0.07504060119390488 + 1.0 * 6.328882217407227
Epoch 850, val loss: 0.8341450691223145
Epoch 860, training loss: 6.398217678070068 = 0.07107076048851013 + 1.0 * 6.327147006988525
Epoch 860, val loss: 0.8400011658668518
Epoch 870, training loss: 6.393757343292236 = 0.06738249212503433 + 1.0 * 6.3263750076293945
Epoch 870, val loss: 0.8458610773086548
Epoch 880, training loss: 6.389542579650879 = 0.06395646929740906 + 1.0 * 6.325586318969727
Epoch 880, val loss: 0.8518567681312561
Epoch 890, training loss: 6.383608818054199 = 0.0607721246778965 + 1.0 * 6.322836875915527
Epoch 890, val loss: 0.8577250838279724
Epoch 900, training loss: 6.378722190856934 = 0.05781140550971031 + 1.0 * 6.320910930633545
Epoch 900, val loss: 0.8638125061988831
Epoch 910, training loss: 6.375586986541748 = 0.055049121379852295 + 1.0 * 6.32053804397583
Epoch 910, val loss: 0.8698206543922424
Epoch 920, training loss: 6.379316806793213 = 0.0524764284491539 + 1.0 * 6.326840400695801
Epoch 920, val loss: 0.8757957220077515
Epoch 930, training loss: 6.3712029457092285 = 0.05007952079176903 + 1.0 * 6.3211236000061035
Epoch 930, val loss: 0.8818038105964661
Epoch 940, training loss: 6.366302967071533 = 0.04784277826547623 + 1.0 * 6.318459987640381
Epoch 940, val loss: 0.8879379630088806
Epoch 950, training loss: 6.363697528839111 = 0.04574509710073471 + 1.0 * 6.317952632904053
Epoch 950, val loss: 0.8939042091369629
Epoch 960, training loss: 6.365018367767334 = 0.043777815997600555 + 1.0 * 6.321240425109863
Epoch 960, val loss: 0.8998126983642578
Epoch 970, training loss: 6.364887237548828 = 0.04193900153040886 + 1.0 * 6.322948455810547
Epoch 970, val loss: 0.9057103991508484
Epoch 980, training loss: 6.356247901916504 = 0.04021165892481804 + 1.0 * 6.316036224365234
Epoch 980, val loss: 0.9116553664207458
Epoch 990, training loss: 6.355025291442871 = 0.038587428629398346 + 1.0 * 6.316437721252441
Epoch 990, val loss: 0.9175220727920532
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 10.56103229522705 = 1.9642060995101929 + 1.0 * 8.596826553344727
Epoch 0, val loss: 1.9548019170761108
Epoch 10, training loss: 10.5499849319458 = 1.9534146785736084 + 1.0 * 8.596570014953613
Epoch 10, val loss: 1.9448012113571167
Epoch 20, training loss: 10.53475570678711 = 1.9400341510772705 + 1.0 * 8.594721794128418
Epoch 20, val loss: 1.932160496711731
Epoch 30, training loss: 10.502706527709961 = 1.92115318775177 + 1.0 * 8.58155345916748
Epoch 30, val loss: 1.9143620729446411
Epoch 40, training loss: 10.407468795776367 = 1.8948802947998047 + 1.0 * 8.512588500976562
Epoch 40, val loss: 1.8902101516723633
Epoch 50, training loss: 10.032682418823242 = 1.8651554584503174 + 1.0 * 8.167527198791504
Epoch 50, val loss: 1.8633719682693481
Epoch 60, training loss: 9.591382026672363 = 1.8399732112884521 + 1.0 * 7.75140905380249
Epoch 60, val loss: 1.8412801027297974
Epoch 70, training loss: 9.238916397094727 = 1.8210101127624512 + 1.0 * 7.417906761169434
Epoch 70, val loss: 1.823250651359558
Epoch 80, training loss: 8.991092681884766 = 1.8000715970993042 + 1.0 * 7.191020965576172
Epoch 80, val loss: 1.803800106048584
Epoch 90, training loss: 8.811250686645508 = 1.7781405448913574 + 1.0 * 7.03311014175415
Epoch 90, val loss: 1.785368800163269
Epoch 100, training loss: 8.695919036865234 = 1.7563676834106445 + 1.0 * 6.939551830291748
Epoch 100, val loss: 1.767914056777954
Epoch 110, training loss: 8.617871284484863 = 1.7335914373397827 + 1.0 * 6.884280204772949
Epoch 110, val loss: 1.7484407424926758
Epoch 120, training loss: 8.546891212463379 = 1.7091602087020874 + 1.0 * 6.837730884552002
Epoch 120, val loss: 1.726232647895813
Epoch 130, training loss: 8.480173110961914 = 1.6819287538528442 + 1.0 * 6.798243999481201
Epoch 130, val loss: 1.701905369758606
Epoch 140, training loss: 8.410331726074219 = 1.650909185409546 + 1.0 * 6.759422302246094
Epoch 140, val loss: 1.675307273864746
Epoch 150, training loss: 8.3390531539917 = 1.615234613418579 + 1.0 * 6.723818778991699
Epoch 150, val loss: 1.644813895225525
Epoch 160, training loss: 8.265298843383789 = 1.5747278928756714 + 1.0 * 6.690570831298828
Epoch 160, val loss: 1.6104204654693604
Epoch 170, training loss: 8.192852020263672 = 1.5298857688903809 + 1.0 * 6.662966728210449
Epoch 170, val loss: 1.5725561380386353
Epoch 180, training loss: 8.11782455444336 = 1.4825478792190552 + 1.0 * 6.6352763175964355
Epoch 180, val loss: 1.5335166454315186
Epoch 190, training loss: 8.04422664642334 = 1.4336473941802979 + 1.0 * 6.610579490661621
Epoch 190, val loss: 1.4938464164733887
Epoch 200, training loss: 7.973895072937012 = 1.3845179080963135 + 1.0 * 6.589376926422119
Epoch 200, val loss: 1.4546222686767578
Epoch 210, training loss: 7.90916633605957 = 1.3372422456741333 + 1.0 * 6.571924209594727
Epoch 210, val loss: 1.417761206626892
Epoch 220, training loss: 7.846707344055176 = 1.2915525436401367 + 1.0 * 6.555154800415039
Epoch 220, val loss: 1.382746696472168
Epoch 230, training loss: 7.79268741607666 = 1.2474852800369263 + 1.0 * 6.545202255249023
Epoch 230, val loss: 1.3495863676071167
Epoch 240, training loss: 7.734759330749512 = 1.2055391073226929 + 1.0 * 6.529220104217529
Epoch 240, val loss: 1.318758249282837
Epoch 250, training loss: 7.680631637573242 = 1.165244698524475 + 1.0 * 6.515387058258057
Epoch 250, val loss: 1.2897413969039917
Epoch 260, training loss: 7.632166385650635 = 1.1261239051818848 + 1.0 * 6.50604248046875
Epoch 260, val loss: 1.2618849277496338
Epoch 270, training loss: 7.587055683135986 = 1.0880684852600098 + 1.0 * 6.498987197875977
Epoch 270, val loss: 1.2350224256515503
Epoch 280, training loss: 7.541640758514404 = 1.0514580011367798 + 1.0 * 6.490182876586914
Epoch 280, val loss: 1.2092530727386475
Epoch 290, training loss: 7.49495792388916 = 1.0162994861602783 + 1.0 * 6.478658199310303
Epoch 290, val loss: 1.1844168901443481
Epoch 300, training loss: 7.4537177085876465 = 0.9823251962661743 + 1.0 * 6.471392631530762
Epoch 300, val loss: 1.1604341268539429
Epoch 310, training loss: 7.414266586303711 = 0.9496622681617737 + 1.0 * 6.464604377746582
Epoch 310, val loss: 1.1374815702438354
Epoch 320, training loss: 7.378239631652832 = 0.9185254573822021 + 1.0 * 6.459713935852051
Epoch 320, val loss: 1.115639090538025
Epoch 330, training loss: 7.340576171875 = 0.8884626030921936 + 1.0 * 6.452113628387451
Epoch 330, val loss: 1.0945452451705933
Epoch 340, training loss: 7.310282230377197 = 0.8590903282165527 + 1.0 * 6.4511919021606445
Epoch 340, val loss: 1.073996901512146
Epoch 350, training loss: 7.271315574645996 = 0.8303199410438538 + 1.0 * 6.440995693206787
Epoch 350, val loss: 1.0540201663970947
Epoch 360, training loss: 7.238646030426025 = 0.8019905090332031 + 1.0 * 6.436655521392822
Epoch 360, val loss: 1.03446626663208
Epoch 370, training loss: 7.204339504241943 = 0.7735373377799988 + 1.0 * 6.430802345275879
Epoch 370, val loss: 1.0150784254074097
Epoch 380, training loss: 7.176496505737305 = 0.7446596622467041 + 1.0 * 6.4318366050720215
Epoch 380, val loss: 0.9957793354988098
Epoch 390, training loss: 7.138733863830566 = 0.7154869437217712 + 1.0 * 6.42324686050415
Epoch 390, val loss: 0.9765748381614685
Epoch 400, training loss: 7.107790470123291 = 0.6858506798744202 + 1.0 * 6.421939849853516
Epoch 400, val loss: 0.9575567245483398
Epoch 410, training loss: 7.070190906524658 = 0.6557359099388123 + 1.0 * 6.414454936981201
Epoch 410, val loss: 0.9387802481651306
Epoch 420, training loss: 7.037297248840332 = 0.6251030564308167 + 1.0 * 6.41219425201416
Epoch 420, val loss: 0.920365571975708
Epoch 430, training loss: 7.007438659667969 = 0.5940271019935608 + 1.0 * 6.413411617279053
Epoch 430, val loss: 0.9023077487945557
Epoch 440, training loss: 6.968254089355469 = 0.5628974437713623 + 1.0 * 6.405356407165527
Epoch 440, val loss: 0.884931743144989
Epoch 450, training loss: 6.935496807098389 = 0.5316175818443298 + 1.0 * 6.403879165649414
Epoch 450, val loss: 0.8682344555854797
Epoch 460, training loss: 6.899264812469482 = 0.5004624724388123 + 1.0 * 6.398802280426025
Epoch 460, val loss: 0.8524634838104248
Epoch 470, training loss: 6.869229316711426 = 0.46974319219589233 + 1.0 * 6.399486064910889
Epoch 470, val loss: 0.8377597332000732
Epoch 480, training loss: 6.835765838623047 = 0.43975111842155457 + 1.0 * 6.39601469039917
Epoch 480, val loss: 0.8242268562316895
Epoch 490, training loss: 6.802518844604492 = 0.4107113480567932 + 1.0 * 6.391807556152344
Epoch 490, val loss: 0.8119766116142273
Epoch 500, training loss: 6.771763801574707 = 0.3827112019062042 + 1.0 * 6.389052391052246
Epoch 500, val loss: 0.8010332584381104
Epoch 510, training loss: 6.751900672912598 = 0.3561086654663086 + 1.0 * 6.395792007446289
Epoch 510, val loss: 0.7914267778396606
Epoch 520, training loss: 6.720209121704102 = 0.3311597406864166 + 1.0 * 6.389049530029297
Epoch 520, val loss: 0.7832111716270447
Epoch 530, training loss: 6.690598487854004 = 0.30784955620765686 + 1.0 * 6.382749080657959
Epoch 530, val loss: 0.7764617800712585
Epoch 540, training loss: 6.666834354400635 = 0.28608769178390503 + 1.0 * 6.380746841430664
Epoch 540, val loss: 0.7710397839546204
Epoch 550, training loss: 6.6493072509765625 = 0.26594242453575134 + 1.0 * 6.383364677429199
Epoch 550, val loss: 0.7668150663375854
Epoch 560, training loss: 6.624128341674805 = 0.2474537342786789 + 1.0 * 6.376674652099609
Epoch 560, val loss: 0.7639287114143372
Epoch 570, training loss: 6.6049885749816895 = 0.23050454258918762 + 1.0 * 6.374484062194824
Epoch 570, val loss: 0.7622659206390381
Epoch 580, training loss: 6.587299346923828 = 0.21494439244270325 + 1.0 * 6.372354984283447
Epoch 580, val loss: 0.761736273765564
Epoch 590, training loss: 6.57924747467041 = 0.20069603621959686 + 1.0 * 6.378551483154297
Epoch 590, val loss: 0.7621073126792908
Epoch 600, training loss: 6.56064510345459 = 0.18776777386665344 + 1.0 * 6.37287712097168
Epoch 600, val loss: 0.7633600234985352
Epoch 610, training loss: 6.543034076690674 = 0.17598658800125122 + 1.0 * 6.367047309875488
Epoch 610, val loss: 0.7654753923416138
Epoch 620, training loss: 6.528625011444092 = 0.1651412397623062 + 1.0 * 6.363483905792236
Epoch 620, val loss: 0.768279492855072
Epoch 630, training loss: 6.520895957946777 = 0.15513961017131805 + 1.0 * 6.365756511688232
Epoch 630, val loss: 0.7717562317848206
Epoch 640, training loss: 6.511476993560791 = 0.14598073065280914 + 1.0 * 6.3654961585998535
Epoch 640, val loss: 0.7757324576377869
Epoch 650, training loss: 6.496415615081787 = 0.13755959272384644 + 1.0 * 6.358856201171875
Epoch 650, val loss: 0.7802655696868896
Epoch 660, training loss: 6.486985683441162 = 0.12979251146316528 + 1.0 * 6.3571929931640625
Epoch 660, val loss: 0.7852690815925598
Epoch 670, training loss: 6.479073524475098 = 0.12259814888238907 + 1.0 * 6.356475353240967
Epoch 670, val loss: 0.7906988263130188
Epoch 680, training loss: 6.474680423736572 = 0.11593741178512573 + 1.0 * 6.358743190765381
Epoch 680, val loss: 0.7963368892669678
Epoch 690, training loss: 6.463963031768799 = 0.1098199114203453 + 1.0 * 6.354143142700195
Epoch 690, val loss: 0.802308201789856
Epoch 700, training loss: 6.455082893371582 = 0.10414764285087585 + 1.0 * 6.350935459136963
Epoch 700, val loss: 0.8085507750511169
Epoch 710, training loss: 6.451572418212891 = 0.0988696739077568 + 1.0 * 6.352702617645264
Epoch 710, val loss: 0.8150319457054138
Epoch 720, training loss: 6.44692325592041 = 0.0939817950129509 + 1.0 * 6.352941513061523
Epoch 720, val loss: 0.8214530348777771
Epoch 730, training loss: 6.435966968536377 = 0.08944448828697205 + 1.0 * 6.346522331237793
Epoch 730, val loss: 0.8281329870223999
Epoch 740, training loss: 6.429935932159424 = 0.08520817011594772 + 1.0 * 6.344727993011475
Epoch 740, val loss: 0.8349018096923828
Epoch 750, training loss: 6.435918807983398 = 0.08124341070652008 + 1.0 * 6.35467529296875
Epoch 750, val loss: 0.8416754603385925
Epoch 760, training loss: 6.424333572387695 = 0.07755322754383087 + 1.0 * 6.346780300140381
Epoch 760, val loss: 0.8484192490577698
Epoch 770, training loss: 6.41517448425293 = 0.0740976557135582 + 1.0 * 6.341076850891113
Epoch 770, val loss: 0.8552568554878235
Epoch 780, training loss: 6.416475296020508 = 0.07085160166025162 + 1.0 * 6.34562349319458
Epoch 780, val loss: 0.8620396256446838
Epoch 790, training loss: 6.407966613769531 = 0.06781168282032013 + 1.0 * 6.340155124664307
Epoch 790, val loss: 0.8688514232635498
Epoch 800, training loss: 6.4097371101379395 = 0.06493847817182541 + 1.0 * 6.344798564910889
Epoch 800, val loss: 0.8755960464477539
Epoch 810, training loss: 6.399621486663818 = 0.06224982440471649 + 1.0 * 6.337371826171875
Epoch 810, val loss: 0.8822767734527588
Epoch 820, training loss: 6.393664836883545 = 0.059702374041080475 + 1.0 * 6.333962440490723
Epoch 820, val loss: 0.8890042304992676
Epoch 830, training loss: 6.389496803283691 = 0.05729244276881218 + 1.0 * 6.332204341888428
Epoch 830, val loss: 0.8957079648971558
Epoch 840, training loss: 6.390976905822754 = 0.05500813201069832 + 1.0 * 6.335968971252441
Epoch 840, val loss: 0.9024029970169067
Epoch 850, training loss: 6.390322685241699 = 0.052850592881441116 + 1.0 * 6.337471961975098
Epoch 850, val loss: 0.9087170958518982
Epoch 860, training loss: 6.3803181648254395 = 0.05082552507519722 + 1.0 * 6.329492568969727
Epoch 860, val loss: 0.9151932597160339
Epoch 870, training loss: 6.381993770599365 = 0.048905715346336365 + 1.0 * 6.333087921142578
Epoch 870, val loss: 0.9216070771217346
Epoch 880, training loss: 6.373743057250977 = 0.047080863267183304 + 1.0 * 6.326662063598633
Epoch 880, val loss: 0.9279062151908875
Epoch 890, training loss: 6.371510028839111 = 0.04534836485981941 + 1.0 * 6.326161861419678
Epoch 890, val loss: 0.9342382550239563
Epoch 900, training loss: 6.369132041931152 = 0.04370119050145149 + 1.0 * 6.325430870056152
Epoch 900, val loss: 0.9405155181884766
Epoch 910, training loss: 6.377490043640137 = 0.0421321801841259 + 1.0 * 6.335357666015625
Epoch 910, val loss: 0.9467288255691528
Epoch 920, training loss: 6.3655195236206055 = 0.04065157473087311 + 1.0 * 6.3248677253723145
Epoch 920, val loss: 0.9527034163475037
Epoch 930, training loss: 6.361944198608398 = 0.03924485296010971 + 1.0 * 6.322699546813965
Epoch 930, val loss: 0.9587551355361938
Epoch 940, training loss: 6.362051010131836 = 0.03790238872170448 + 1.0 * 6.324148654937744
Epoch 940, val loss: 0.964767336845398
Epoch 950, training loss: 6.357245445251465 = 0.03662717714905739 + 1.0 * 6.320618152618408
Epoch 950, val loss: 0.9706109166145325
Epoch 960, training loss: 6.3541131019592285 = 0.035413291305303574 + 1.0 * 6.318699836730957
Epoch 960, val loss: 0.9764951467514038
Epoch 970, training loss: 6.356714248657227 = 0.03425385057926178 + 1.0 * 6.322460174560547
Epoch 970, val loss: 0.9823178648948669
Epoch 980, training loss: 6.350913047790527 = 0.03315161541104317 + 1.0 * 6.317761421203613
Epoch 980, val loss: 0.9879368543624878
Epoch 990, training loss: 6.3488993644714355 = 0.03209919109940529 + 1.0 * 6.316800117492676
Epoch 990, val loss: 0.9935762882232666
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 10.543822288513184 = 1.9469982385635376 + 1.0 * 8.596823692321777
Epoch 0, val loss: 1.9399126768112183
Epoch 10, training loss: 10.533361434936523 = 1.936827540397644 + 1.0 * 8.59653377532959
Epoch 10, val loss: 1.9298430681228638
Epoch 20, training loss: 10.518770217895508 = 1.9243477582931519 + 1.0 * 8.594422340393066
Epoch 20, val loss: 1.9174662828445435
Epoch 30, training loss: 10.484867095947266 = 1.9067860841751099 + 1.0 * 8.578081130981445
Epoch 30, val loss: 1.9002337455749512
Epoch 40, training loss: 10.34648609161377 = 1.8835676908493042 + 1.0 * 8.462918281555176
Epoch 40, val loss: 1.8783334493637085
Epoch 50, training loss: 9.794281959533691 = 1.8583269119262695 + 1.0 * 7.935955047607422
Epoch 50, val loss: 1.854637622833252
Epoch 60, training loss: 9.312926292419434 = 1.8379510641098022 + 1.0 * 7.4749755859375
Epoch 60, val loss: 1.8353294134140015
Epoch 70, training loss: 8.95450210571289 = 1.8222856521606445 + 1.0 * 7.132215976715088
Epoch 70, val loss: 1.8199704885482788
Epoch 80, training loss: 8.80582332611084 = 1.805221676826477 + 1.0 * 7.000601291656494
Epoch 80, val loss: 1.803770899772644
Epoch 90, training loss: 8.69468879699707 = 1.78288733959198 + 1.0 * 6.911801815032959
Epoch 90, val loss: 1.7847295999526978
Epoch 100, training loss: 8.606931686401367 = 1.7611474990844727 + 1.0 * 6.845783710479736
Epoch 100, val loss: 1.7671247720718384
Epoch 110, training loss: 8.533341407775879 = 1.7413091659545898 + 1.0 * 6.792032241821289
Epoch 110, val loss: 1.7504701614379883
Epoch 120, training loss: 8.469449043273926 = 1.7202473878860474 + 1.0 * 6.74920129776001
Epoch 120, val loss: 1.7320071458816528
Epoch 130, training loss: 8.410106658935547 = 1.6959391832351685 + 1.0 * 6.714167594909668
Epoch 130, val loss: 1.710530161857605
Epoch 140, training loss: 8.354169845581055 = 1.6675125360488892 + 1.0 * 6.686656951904297
Epoch 140, val loss: 1.6854660511016846
Epoch 150, training loss: 8.29796028137207 = 1.6340179443359375 + 1.0 * 6.663941860198975
Epoch 150, val loss: 1.6561274528503418
Epoch 160, training loss: 8.239883422851562 = 1.5950206518173218 + 1.0 * 6.644862651824951
Epoch 160, val loss: 1.6219850778579712
Epoch 170, training loss: 8.176557540893555 = 1.5508779287338257 + 1.0 * 6.6256794929504395
Epoch 170, val loss: 1.5833150148391724
Epoch 180, training loss: 8.1099853515625 = 1.5019924640655518 + 1.0 * 6.607993125915527
Epoch 180, val loss: 1.5406243801116943
Epoch 190, training loss: 8.044554710388184 = 1.4502464532852173 + 1.0 * 6.594307899475098
Epoch 190, val loss: 1.496030569076538
Epoch 200, training loss: 7.974617004394531 = 1.3976621627807617 + 1.0 * 6.5769548416137695
Epoch 200, val loss: 1.451233148574829
Epoch 210, training loss: 7.908749580383301 = 1.3451040983200073 + 1.0 * 6.563645362854004
Epoch 210, val loss: 1.407008409500122
Epoch 220, training loss: 7.846372604370117 = 1.2937408685684204 + 1.0 * 6.552631855010986
Epoch 220, val loss: 1.3646270036697388
Epoch 230, training loss: 7.786062717437744 = 1.2445334196090698 + 1.0 * 6.541529178619385
Epoch 230, val loss: 1.3245738744735718
Epoch 240, training loss: 7.7292561531066895 = 1.197199821472168 + 1.0 * 6.5320563316345215
Epoch 240, val loss: 1.287007212638855
Epoch 250, training loss: 7.672468185424805 = 1.1509608030319214 + 1.0 * 6.521507263183594
Epoch 250, val loss: 1.2509770393371582
Epoch 260, training loss: 7.616807460784912 = 1.1046494245529175 + 1.0 * 6.512157917022705
Epoch 260, val loss: 1.2154961824417114
Epoch 270, training loss: 7.567637920379639 = 1.0577653646469116 + 1.0 * 6.5098724365234375
Epoch 270, val loss: 1.1801315546035767
Epoch 280, training loss: 7.506474494934082 = 1.0111643075942993 + 1.0 * 6.495310306549072
Epoch 280, val loss: 1.1453737020492554
Epoch 290, training loss: 7.452558994293213 = 0.9645093679428101 + 1.0 * 6.488049507141113
Epoch 290, val loss: 1.1107475757598877
Epoch 300, training loss: 7.4019575119018555 = 0.9180746674537659 + 1.0 * 6.483882904052734
Epoch 300, val loss: 1.076318383216858
Epoch 310, training loss: 7.347684860229492 = 0.8729540705680847 + 1.0 * 6.474730968475342
Epoch 310, val loss: 1.0432826280593872
Epoch 320, training loss: 7.2982306480407715 = 0.8293668627738953 + 1.0 * 6.4688639640808105
Epoch 320, val loss: 1.0115916728973389
Epoch 330, training loss: 7.254518032073975 = 0.7874833345413208 + 1.0 * 6.467034816741943
Epoch 330, val loss: 0.9813917279243469
Epoch 340, training loss: 7.20842981338501 = 0.7478805780410767 + 1.0 * 6.460549354553223
Epoch 340, val loss: 0.9532142877578735
Epoch 350, training loss: 7.165524005889893 = 0.710750937461853 + 1.0 * 6.45477294921875
Epoch 350, val loss: 0.9274054169654846
Epoch 360, training loss: 7.122998237609863 = 0.6758435368537903 + 1.0 * 6.447154521942139
Epoch 360, val loss: 0.9035568237304688
Epoch 370, training loss: 7.093278884887695 = 0.6429353356361389 + 1.0 * 6.450343608856201
Epoch 370, val loss: 0.8816457986831665
Epoch 380, training loss: 7.052184104919434 = 0.6123225092887878 + 1.0 * 6.43986177444458
Epoch 380, val loss: 0.8619967103004456
Epoch 390, training loss: 7.018208980560303 = 0.5836576819419861 + 1.0 * 6.434551239013672
Epoch 390, val loss: 0.8444470763206482
Epoch 400, training loss: 6.990784645080566 = 0.556609570980072 + 1.0 * 6.43417501449585
Epoch 400, val loss: 0.8285292983055115
Epoch 410, training loss: 6.9582061767578125 = 0.5311315059661865 + 1.0 * 6.427074909210205
Epoch 410, val loss: 0.8143130540847778
Epoch 420, training loss: 6.92997407913208 = 0.5069257616996765 + 1.0 * 6.423048496246338
Epoch 420, val loss: 0.8014419674873352
Epoch 430, training loss: 6.901824951171875 = 0.4837864935398102 + 1.0 * 6.418038368225098
Epoch 430, val loss: 0.7897271513938904
Epoch 440, training loss: 6.881025314331055 = 0.46138447523117065 + 1.0 * 6.419641017913818
Epoch 440, val loss: 0.7789766788482666
Epoch 450, training loss: 6.850278854370117 = 0.439694344997406 + 1.0 * 6.410584449768066
Epoch 450, val loss: 0.7691519260406494
Epoch 460, training loss: 6.824621200561523 = 0.4185935854911804 + 1.0 * 6.406027793884277
Epoch 460, val loss: 0.7600942254066467
Epoch 470, training loss: 6.802931308746338 = 0.3978675603866577 + 1.0 * 6.405063629150391
Epoch 470, val loss: 0.7516043186187744
Epoch 480, training loss: 6.784401893615723 = 0.3775661885738373 + 1.0 * 6.406835556030273
Epoch 480, val loss: 0.7438664436340332
Epoch 490, training loss: 6.756380081176758 = 0.35778695344924927 + 1.0 * 6.398592948913574
Epoch 490, val loss: 0.7368883490562439
Epoch 500, training loss: 6.736689567565918 = 0.33844122290611267 + 1.0 * 6.398248195648193
Epoch 500, val loss: 0.7305765151977539
Epoch 510, training loss: 6.712579727172852 = 0.3196311891078949 + 1.0 * 6.392948627471924
Epoch 510, val loss: 0.7250257134437561
Epoch 520, training loss: 6.69005012512207 = 0.3013690710067749 + 1.0 * 6.388680934906006
Epoch 520, val loss: 0.7203156352043152
Epoch 530, training loss: 6.676051616668701 = 0.28371143341064453 + 1.0 * 6.392340183258057
Epoch 530, val loss: 0.7163012623786926
Epoch 540, training loss: 6.656702518463135 = 0.26677289605140686 + 1.0 * 6.38992977142334
Epoch 540, val loss: 0.7130143046379089
Epoch 550, training loss: 6.637474060058594 = 0.2507157623767853 + 1.0 * 6.386758327484131
Epoch 550, val loss: 0.7104949951171875
Epoch 560, training loss: 6.615877628326416 = 0.2354513704776764 + 1.0 * 6.380426406860352
Epoch 560, val loss: 0.7087664008140564
Epoch 570, training loss: 6.608144283294678 = 0.22101852297782898 + 1.0 * 6.3871259689331055
Epoch 570, val loss: 0.7076758742332458
Epoch 580, training loss: 6.585176944732666 = 0.20744219422340393 + 1.0 * 6.377734661102295
Epoch 580, val loss: 0.7071772813796997
Epoch 590, training loss: 6.568794250488281 = 0.19471262395381927 + 1.0 * 6.374081611633301
Epoch 590, val loss: 0.7073301076889038
Epoch 600, training loss: 6.556608200073242 = 0.18277543783187866 + 1.0 * 6.373832702636719
Epoch 600, val loss: 0.7080639600753784
Epoch 610, training loss: 6.543436527252197 = 0.17163333296775818 + 1.0 * 6.371803283691406
Epoch 610, val loss: 0.7093335390090942
Epoch 620, training loss: 6.529446125030518 = 0.1612522155046463 + 1.0 * 6.368194103240967
Epoch 620, val loss: 0.7111300230026245
Epoch 630, training loss: 6.5186076164245605 = 0.15159618854522705 + 1.0 * 6.367011547088623
Epoch 630, val loss: 0.7134270668029785
Epoch 640, training loss: 6.517920017242432 = 0.14258050918579102 + 1.0 * 6.375339508056641
Epoch 640, val loss: 0.7161310315132141
Epoch 650, training loss: 6.498345375061035 = 0.13425227999687195 + 1.0 * 6.36409330368042
Epoch 650, val loss: 0.7191352248191833
Epoch 660, training loss: 6.488572597503662 = 0.12649451196193695 + 1.0 * 6.3620781898498535
Epoch 660, val loss: 0.7225106954574585
Epoch 670, training loss: 6.486019134521484 = 0.11926891654729843 + 1.0 * 6.366750240325928
Epoch 670, val loss: 0.7261614799499512
Epoch 680, training loss: 6.473331928253174 = 0.112544946372509 + 1.0 * 6.3607869148254395
Epoch 680, val loss: 0.7301039099693298
Epoch 690, training loss: 6.463011741638184 = 0.10629768669605255 + 1.0 * 6.356714248657227
Epoch 690, val loss: 0.7342066168785095
Epoch 700, training loss: 6.454740047454834 = 0.10045910626649857 + 1.0 * 6.354280948638916
Epoch 700, val loss: 0.7385035753250122
Epoch 710, training loss: 6.455345630645752 = 0.09501234441995621 + 1.0 * 6.360333442687988
Epoch 710, val loss: 0.7429812550544739
Epoch 720, training loss: 6.443249702453613 = 0.08994623273611069 + 1.0 * 6.3533034324646
Epoch 720, val loss: 0.7475257515907288
Epoch 730, training loss: 6.437584400177002 = 0.08524898439645767 + 1.0 * 6.352335453033447
Epoch 730, val loss: 0.7522711753845215
Epoch 740, training loss: 6.428989887237549 = 0.08084685355424881 + 1.0 * 6.348143100738525
Epoch 740, val loss: 0.7571098804473877
Epoch 750, training loss: 6.427403926849365 = 0.07672470808029175 + 1.0 * 6.350679397583008
Epoch 750, val loss: 0.7620347738265991
Epoch 760, training loss: 6.426255226135254 = 0.07288840413093567 + 1.0 * 6.353366851806641
Epoch 760, val loss: 0.7669812440872192
Epoch 770, training loss: 6.413396835327148 = 0.06931175291538239 + 1.0 * 6.344085216522217
Epoch 770, val loss: 0.7720038890838623
Epoch 780, training loss: 6.41032075881958 = 0.06596359610557556 + 1.0 * 6.344357013702393
Epoch 780, val loss: 0.7770807147026062
Epoch 790, training loss: 6.408553123474121 = 0.06282589584589005 + 1.0 * 6.345727443695068
Epoch 790, val loss: 0.7822033166885376
Epoch 800, training loss: 6.4043049812316895 = 0.05989295244216919 + 1.0 * 6.344411849975586
Epoch 800, val loss: 0.7873865365982056
Epoch 810, training loss: 6.397598743438721 = 0.057142216712236404 + 1.0 * 6.340456485748291
Epoch 810, val loss: 0.7926428318023682
Epoch 820, training loss: 6.394654750823975 = 0.05455435439944267 + 1.0 * 6.340100288391113
Epoch 820, val loss: 0.7978784441947937
Epoch 830, training loss: 6.393753528594971 = 0.052122097462415695 + 1.0 * 6.3416314125061035
Epoch 830, val loss: 0.8031162023544312
Epoch 840, training loss: 6.39146614074707 = 0.04984351247549057 + 1.0 * 6.341622829437256
Epoch 840, val loss: 0.8083200454711914
Epoch 850, training loss: 6.384245872497559 = 0.047704312950372696 + 1.0 * 6.336541652679443
Epoch 850, val loss: 0.8135476112365723
Epoch 860, training loss: 6.380634784698486 = 0.04568614438176155 + 1.0 * 6.334948539733887
Epoch 860, val loss: 0.8187471032142639
Epoch 870, training loss: 6.3792405128479 = 0.043786898255348206 + 1.0 * 6.335453510284424
Epoch 870, val loss: 0.8239003419876099
Epoch 880, training loss: 6.374996185302734 = 0.04200137034058571 + 1.0 * 6.3329949378967285
Epoch 880, val loss: 0.829071581363678
Epoch 890, training loss: 6.374002933502197 = 0.04031263291835785 + 1.0 * 6.333690166473389
Epoch 890, val loss: 0.8342196941375732
Epoch 900, training loss: 6.370693206787109 = 0.03872555494308472 + 1.0 * 6.331967830657959
Epoch 900, val loss: 0.839336097240448
Epoch 910, training loss: 6.367077827453613 = 0.037229023873806 + 1.0 * 6.329848766326904
Epoch 910, val loss: 0.844415545463562
Epoch 920, training loss: 6.366815090179443 = 0.03581923618912697 + 1.0 * 6.330996036529541
Epoch 920, val loss: 0.8494639992713928
Epoch 930, training loss: 6.363594055175781 = 0.034478168934583664 + 1.0 * 6.329115867614746
Epoch 930, val loss: 0.8544644117355347
Epoch 940, training loss: 6.365965366363525 = 0.03321327641606331 + 1.0 * 6.332752227783203
Epoch 940, val loss: 0.8594584465026855
Epoch 950, training loss: 6.360926628112793 = 0.03201862797141075 + 1.0 * 6.3289079666137695
Epoch 950, val loss: 0.8643940091133118
Epoch 960, training loss: 6.355396270751953 = 0.030884413048624992 + 1.0 * 6.324512004852295
Epoch 960, val loss: 0.869263231754303
Epoch 970, training loss: 6.35416316986084 = 0.029807476326823235 + 1.0 * 6.324355602264404
Epoch 970, val loss: 0.8741351366043091
Epoch 980, training loss: 6.356381893157959 = 0.028785964474081993 + 1.0 * 6.3275957107543945
Epoch 980, val loss: 0.8789397478103638
Epoch 990, training loss: 6.351789474487305 = 0.02781638503074646 + 1.0 * 6.323973178863525
Epoch 990, val loss: 0.8837105631828308
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8360569319978914
The final CL Acc:0.78765, 0.00972, The final GNN Acc:0.83711, 0.00149
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10548])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.541481971740723 = 1.9446243047714233 + 1.0 * 8.596858024597168
Epoch 0, val loss: 1.945051670074463
Epoch 10, training loss: 10.530963897705078 = 1.9343453645706177 + 1.0 * 8.59661865234375
Epoch 10, val loss: 1.9351930618286133
Epoch 20, training loss: 10.515901565551758 = 1.9215209484100342 + 1.0 * 8.594380378723145
Epoch 20, val loss: 1.9226770401000977
Epoch 30, training loss: 10.479589462280273 = 1.9036959409713745 + 1.0 * 8.57589340209961
Epoch 30, val loss: 1.9052363634109497
Epoch 40, training loss: 10.358665466308594 = 1.8798974752426147 + 1.0 * 8.478768348693848
Epoch 40, val loss: 1.8828630447387695
Epoch 50, training loss: 9.928603172302246 = 1.8534454107284546 + 1.0 * 8.07515811920166
Epoch 50, val loss: 1.8588813543319702
Epoch 60, training loss: 9.660785675048828 = 1.8291772603988647 + 1.0 * 7.831608295440674
Epoch 60, val loss: 1.8380094766616821
Epoch 70, training loss: 9.272773742675781 = 1.812362790107727 + 1.0 * 7.460411071777344
Epoch 70, val loss: 1.8228839635849
Epoch 80, training loss: 8.987909317016602 = 1.798560619354248 + 1.0 * 7.189349174499512
Epoch 80, val loss: 1.8099011182785034
Epoch 90, training loss: 8.800795555114746 = 1.7847973108291626 + 1.0 * 7.015998363494873
Epoch 90, val loss: 1.7966434955596924
Epoch 100, training loss: 8.705775260925293 = 1.769862413406372 + 1.0 * 6.935912609100342
Epoch 100, val loss: 1.7824128866195679
Epoch 110, training loss: 8.640314102172852 = 1.7533376216888428 + 1.0 * 6.886976718902588
Epoch 110, val loss: 1.7669954299926758
Epoch 120, training loss: 8.562430381774902 = 1.7349607944488525 + 1.0 * 6.827469348907471
Epoch 120, val loss: 1.7503019571304321
Epoch 130, training loss: 8.483354568481445 = 1.7145050764083862 + 1.0 * 6.7688493728637695
Epoch 130, val loss: 1.7319740056991577
Epoch 140, training loss: 8.413627624511719 = 1.6908015012741089 + 1.0 * 6.7228264808654785
Epoch 140, val loss: 1.7112051248550415
Epoch 150, training loss: 8.33978271484375 = 1.6634732484817505 + 1.0 * 6.676309108734131
Epoch 150, val loss: 1.6878877878189087
Epoch 160, training loss: 8.263134956359863 = 1.6322977542877197 + 1.0 * 6.6308369636535645
Epoch 160, val loss: 1.6610828638076782
Epoch 170, training loss: 8.198525428771973 = 1.5972158908843994 + 1.0 * 6.601309776306152
Epoch 170, val loss: 1.6309069395065308
Epoch 180, training loss: 8.134490966796875 = 1.5583081245422363 + 1.0 * 6.576183319091797
Epoch 180, val loss: 1.5972740650177002
Epoch 190, training loss: 8.075523376464844 = 1.515331506729126 + 1.0 * 6.560192108154297
Epoch 190, val loss: 1.5600556135177612
Epoch 200, training loss: 8.014410018920898 = 1.4685468673706055 + 1.0 * 6.545863628387451
Epoch 200, val loss: 1.519652247428894
Epoch 210, training loss: 7.954098701477051 = 1.4189021587371826 + 1.0 * 6.535196304321289
Epoch 210, val loss: 1.4772284030914307
Epoch 220, training loss: 7.890652179718018 = 1.36905038356781 + 1.0 * 6.521601676940918
Epoch 220, val loss: 1.4354493618011475
Epoch 230, training loss: 7.82740592956543 = 1.3194314241409302 + 1.0 * 6.507974624633789
Epoch 230, val loss: 1.3943688869476318
Epoch 240, training loss: 7.766770362854004 = 1.2695127725601196 + 1.0 * 6.497257709503174
Epoch 240, val loss: 1.3539520502090454
Epoch 250, training loss: 7.708231449127197 = 1.2206530570983887 + 1.0 * 6.487578392028809
Epoch 250, val loss: 1.315591812133789
Epoch 260, training loss: 7.649148941040039 = 1.1730706691741943 + 1.0 * 6.476078510284424
Epoch 260, val loss: 1.2792829275131226
Epoch 270, training loss: 7.5920281410217285 = 1.1260175704956055 + 1.0 * 6.466010570526123
Epoch 270, val loss: 1.2444947957992554
Epoch 280, training loss: 7.537186622619629 = 1.0793179273605347 + 1.0 * 6.457868576049805
Epoch 280, val loss: 1.210896611213684
Epoch 290, training loss: 7.4871296882629395 = 1.0338748693466187 + 1.0 * 6.453254699707031
Epoch 290, val loss: 1.1792528629302979
Epoch 300, training loss: 7.433942794799805 = 0.9900829792022705 + 1.0 * 6.443859577178955
Epoch 300, val loss: 1.1501750946044922
Epoch 310, training loss: 7.383151054382324 = 0.9474284052848816 + 1.0 * 6.435722827911377
Epoch 310, val loss: 1.1225684881210327
Epoch 320, training loss: 7.334612846374512 = 0.9056296348571777 + 1.0 * 6.428983211517334
Epoch 320, val loss: 1.0962350368499756
Epoch 330, training loss: 7.294008255004883 = 0.8651635646820068 + 1.0 * 6.428844928741455
Epoch 330, val loss: 1.0717064142227173
Epoch 340, training loss: 7.243627548217773 = 0.8261913061141968 + 1.0 * 6.417436122894287
Epoch 340, val loss: 1.0490825176239014
Epoch 350, training loss: 7.200808048248291 = 0.7880104184150696 + 1.0 * 6.412797451019287
Epoch 350, val loss: 1.0274300575256348
Epoch 360, training loss: 7.166549205780029 = 0.7503951787948608 + 1.0 * 6.416153907775879
Epoch 360, val loss: 1.0065429210662842
Epoch 370, training loss: 7.117099761962891 = 0.7136048078536987 + 1.0 * 6.403494834899902
Epoch 370, val loss: 0.9867554903030396
Epoch 380, training loss: 7.076518535614014 = 0.6772575974464417 + 1.0 * 6.399260997772217
Epoch 380, val loss: 0.9677439332008362
Epoch 390, training loss: 7.049434661865234 = 0.6412904858589172 + 1.0 * 6.408143997192383
Epoch 390, val loss: 0.9490305781364441
Epoch 400, training loss: 7.001651287078857 = 0.6062097549438477 + 1.0 * 6.39544153213501
Epoch 400, val loss: 0.9312986135482788
Epoch 410, training loss: 6.960921287536621 = 0.5717593431472778 + 1.0 * 6.389162063598633
Epoch 410, val loss: 0.9144895672798157
Epoch 420, training loss: 6.929565906524658 = 0.5379078984260559 + 1.0 * 6.391657829284668
Epoch 420, val loss: 0.8981168270111084
Epoch 430, training loss: 6.892268180847168 = 0.5051693320274353 + 1.0 * 6.387098789215088
Epoch 430, val loss: 0.8827825784683228
Epoch 440, training loss: 6.855257511138916 = 0.47359198331832886 + 1.0 * 6.3816657066345215
Epoch 440, val loss: 0.8687539100646973
Epoch 450, training loss: 6.821296215057373 = 0.4432441294193268 + 1.0 * 6.378052234649658
Epoch 450, val loss: 0.8558395504951477
Epoch 460, training loss: 6.7921271324157715 = 0.41447415947914124 + 1.0 * 6.377653121948242
Epoch 460, val loss: 0.8442252278327942
Epoch 470, training loss: 6.766249179840088 = 0.38765770196914673 + 1.0 * 6.378591537475586
Epoch 470, val loss: 0.8346119523048401
Epoch 480, training loss: 6.73436164855957 = 0.36271747946739197 + 1.0 * 6.371644020080566
Epoch 480, val loss: 0.8265870809555054
Epoch 490, training loss: 6.708268165588379 = 0.33948764204978943 + 1.0 * 6.368780612945557
Epoch 490, val loss: 0.8200428485870361
Epoch 500, training loss: 6.687351703643799 = 0.317916601896286 + 1.0 * 6.3694353103637695
Epoch 500, val loss: 0.814990758895874
Epoch 510, training loss: 6.665595054626465 = 0.2979981303215027 + 1.0 * 6.3675971031188965
Epoch 510, val loss: 0.8116565942764282
Epoch 520, training loss: 6.648609638214111 = 0.2795702815055847 + 1.0 * 6.369039535522461
Epoch 520, val loss: 0.8094574809074402
Epoch 530, training loss: 6.629699230194092 = 0.2626183331012726 + 1.0 * 6.3670806884765625
Epoch 530, val loss: 0.8083124160766602
Epoch 540, training loss: 6.606905460357666 = 0.24695755541324615 + 1.0 * 6.359947681427002
Epoch 540, val loss: 0.8084861040115356
Epoch 550, training loss: 6.5891876220703125 = 0.23229379951953888 + 1.0 * 6.356894016265869
Epoch 550, val loss: 0.8092615008354187
Epoch 560, training loss: 6.574063777923584 = 0.21851865947246552 + 1.0 * 6.3555450439453125
Epoch 560, val loss: 0.810721218585968
Epoch 570, training loss: 6.56084680557251 = 0.20562545955181122 + 1.0 * 6.355221271514893
Epoch 570, val loss: 0.8127759695053101
Epoch 580, training loss: 6.547232627868652 = 0.1936059147119522 + 1.0 * 6.353626728057861
Epoch 580, val loss: 0.8156272172927856
Epoch 590, training loss: 6.533010482788086 = 0.18232415616512299 + 1.0 * 6.350686550140381
Epoch 590, val loss: 0.8190221190452576
Epoch 600, training loss: 6.526934623718262 = 0.17175306379795074 + 1.0 * 6.355181694030762
Epoch 600, val loss: 0.8226785659790039
Epoch 610, training loss: 6.511229515075684 = 0.16191567480564117 + 1.0 * 6.349313735961914
Epoch 610, val loss: 0.8269146084785461
Epoch 620, training loss: 6.498056888580322 = 0.15267851948738098 + 1.0 * 6.345378398895264
Epoch 620, val loss: 0.8314808011054993
Epoch 630, training loss: 6.488914489746094 = 0.14398770034313202 + 1.0 * 6.344926834106445
Epoch 630, val loss: 0.8362396955490112
Epoch 640, training loss: 6.483481407165527 = 0.13589094579219818 + 1.0 * 6.347590446472168
Epoch 640, val loss: 0.8410508036613464
Epoch 650, training loss: 6.470001220703125 = 0.1283586472272873 + 1.0 * 6.341642379760742
Epoch 650, val loss: 0.8464955687522888
Epoch 660, training loss: 6.46076774597168 = 0.1212933287024498 + 1.0 * 6.339474201202393
Epoch 660, val loss: 0.8521493077278137
Epoch 670, training loss: 6.4603047370910645 = 0.11465220153331757 + 1.0 * 6.3456525802612305
Epoch 670, val loss: 0.8578356504440308
Epoch 680, training loss: 6.449744701385498 = 0.10844863206148148 + 1.0 * 6.341296195983887
Epoch 680, val loss: 0.8634229898452759
Epoch 690, training loss: 6.437019348144531 = 0.10265060514211655 + 1.0 * 6.334368705749512
Epoch 690, val loss: 0.8695459365844727
Epoch 700, training loss: 6.433681488037109 = 0.0972062423825264 + 1.0 * 6.336475372314453
Epoch 700, val loss: 0.8756228685379028
Epoch 710, training loss: 6.425704002380371 = 0.09211942553520203 + 1.0 * 6.333584785461426
Epoch 710, val loss: 0.8816607594490051
Epoch 720, training loss: 6.41986608505249 = 0.08736919611692429 + 1.0 * 6.3324971199035645
Epoch 720, val loss: 0.8879818916320801
Epoch 730, training loss: 6.419358730316162 = 0.08291727304458618 + 1.0 * 6.336441516876221
Epoch 730, val loss: 0.8942249417304993
Epoch 740, training loss: 6.408207416534424 = 0.07875945419073105 + 1.0 * 6.3294477462768555
Epoch 740, val loss: 0.9005861878395081
Epoch 750, training loss: 6.40305757522583 = 0.07485074549913406 + 1.0 * 6.328207015991211
Epoch 750, val loss: 0.9069157242774963
Epoch 760, training loss: 6.400923252105713 = 0.07118681818246841 + 1.0 * 6.329736232757568
Epoch 760, val loss: 0.9132545590400696
Epoch 770, training loss: 6.393881320953369 = 0.06777402758598328 + 1.0 * 6.326107501983643
Epoch 770, val loss: 0.9196173548698425
Epoch 780, training loss: 6.387340068817139 = 0.06456514447927475 + 1.0 * 6.322774887084961
Epoch 780, val loss: 0.9261457324028015
Epoch 790, training loss: 6.387908458709717 = 0.06154507026076317 + 1.0 * 6.326363563537598
Epoch 790, val loss: 0.9325089454650879
Epoch 800, training loss: 6.380093574523926 = 0.058708369731903076 + 1.0 * 6.321385383605957
Epoch 800, val loss: 0.9387354254722595
Epoch 810, training loss: 6.376676559448242 = 0.05604740232229233 + 1.0 * 6.320629119873047
Epoch 810, val loss: 0.9450024366378784
Epoch 820, training loss: 6.374999523162842 = 0.05354999005794525 + 1.0 * 6.3214497566223145
Epoch 820, val loss: 0.951291024684906
Epoch 830, training loss: 6.375240802764893 = 0.051202479749917984 + 1.0 * 6.324038505554199
Epoch 830, val loss: 0.9572584629058838
Epoch 840, training loss: 6.365510940551758 = 0.04900530353188515 + 1.0 * 6.316505432128906
Epoch 840, val loss: 0.9633771181106567
Epoch 850, training loss: 6.362070560455322 = 0.04693072661757469 + 1.0 * 6.3151397705078125
Epoch 850, val loss: 0.9695744514465332
Epoch 860, training loss: 6.365281581878662 = 0.0449690967798233 + 1.0 * 6.3203125
Epoch 860, val loss: 0.9755747318267822
Epoch 870, training loss: 6.358570098876953 = 0.04311920702457428 + 1.0 * 6.315450668334961
Epoch 870, val loss: 0.9814140200614929
Epoch 880, training loss: 6.353222846984863 = 0.04137951508164406 + 1.0 * 6.311843395233154
Epoch 880, val loss: 0.9873940944671631
Epoch 890, training loss: 6.34911584854126 = 0.039730239659547806 + 1.0 * 6.309385776519775
Epoch 890, val loss: 0.9932841062545776
Epoch 900, training loss: 6.353233337402344 = 0.03816622123122215 + 1.0 * 6.315067291259766
Epoch 900, val loss: 0.999003529548645
Epoch 910, training loss: 6.350253105163574 = 0.03669188171625137 + 1.0 * 6.31356143951416
Epoch 910, val loss: 1.0046902894973755
Epoch 920, training loss: 6.341337203979492 = 0.03530601039528847 + 1.0 * 6.306031227111816
Epoch 920, val loss: 1.0102646350860596
Epoch 930, training loss: 6.340821266174316 = 0.03399604931473732 + 1.0 * 6.306825160980225
Epoch 930, val loss: 1.015842080116272
Epoch 940, training loss: 6.346435070037842 = 0.032754573971033096 + 1.0 * 6.313680648803711
Epoch 940, val loss: 1.0213154554367065
Epoch 950, training loss: 6.337400913238525 = 0.031573981046676636 + 1.0 * 6.3058271408081055
Epoch 950, val loss: 1.0266247987747192
Epoch 960, training loss: 6.332913398742676 = 0.03045486845076084 + 1.0 * 6.302458763122559
Epoch 960, val loss: 1.0320837497711182
Epoch 970, training loss: 6.330152988433838 = 0.029384179040789604 + 1.0 * 6.300768852233887
Epoch 970, val loss: 1.037355899810791
Epoch 980, training loss: 6.33927583694458 = 0.02836441621184349 + 1.0 * 6.310911655426025
Epoch 980, val loss: 1.042536973953247
Epoch 990, training loss: 6.332043647766113 = 0.02739926427602768 + 1.0 * 6.304644584655762
Epoch 990, val loss: 1.0476027727127075
Epoch 1000, training loss: 6.326388359069824 = 0.026483146473765373 + 1.0 * 6.299905300140381
Epoch 1000, val loss: 1.0526623725891113
Epoch 1010, training loss: 6.330511569976807 = 0.025612618774175644 + 1.0 * 6.304898738861084
Epoch 1010, val loss: 1.0577315092086792
Epoch 1020, training loss: 6.323551654815674 = 0.024787090718746185 + 1.0 * 6.298764705657959
Epoch 1020, val loss: 1.0626819133758545
Epoch 1030, training loss: 6.324843883514404 = 0.024000873789191246 + 1.0 * 6.300843238830566
Epoch 1030, val loss: 1.0676345825195312
Epoch 1040, training loss: 6.318916320800781 = 0.02324962615966797 + 1.0 * 6.295666694641113
Epoch 1040, val loss: 1.0724232196807861
Epoch 1050, training loss: 6.31649112701416 = 0.022531725466251373 + 1.0 * 6.293959617614746
Epoch 1050, val loss: 1.0772849321365356
Epoch 1060, training loss: 6.315207004547119 = 0.02184261381626129 + 1.0 * 6.293364524841309
Epoch 1060, val loss: 1.0819711685180664
Epoch 1070, training loss: 6.3206095695495605 = 0.021182961761951447 + 1.0 * 6.299426555633545
Epoch 1070, val loss: 1.0865296125411987
Epoch 1080, training loss: 6.320103645324707 = 0.020557377487421036 + 1.0 * 6.299546241760254
Epoch 1080, val loss: 1.0910862684249878
Epoch 1090, training loss: 6.315584182739258 = 0.0199639480561018 + 1.0 * 6.295620441436768
Epoch 1090, val loss: 1.0954612493515015
Epoch 1100, training loss: 6.310066223144531 = 0.01939626969397068 + 1.0 * 6.290669918060303
Epoch 1100, val loss: 1.0999164581298828
Epoch 1110, training loss: 6.308681964874268 = 0.018850600346922874 + 1.0 * 6.289831161499023
Epoch 1110, val loss: 1.1043378114700317
Epoch 1120, training loss: 6.3140645027160645 = 0.01832491159439087 + 1.0 * 6.295739650726318
Epoch 1120, val loss: 1.1087074279785156
Epoch 1130, training loss: 6.306203365325928 = 0.0178218986839056 + 1.0 * 6.288381576538086
Epoch 1130, val loss: 1.1128405332565308
Epoch 1140, training loss: 6.30564022064209 = 0.017339570447802544 + 1.0 * 6.288300514221191
Epoch 1140, val loss: 1.1170129776000977
Epoch 1150, training loss: 6.311497211456299 = 0.01687518320977688 + 1.0 * 6.29462194442749
Epoch 1150, val loss: 1.1211315393447876
Epoch 1160, training loss: 6.303025245666504 = 0.01643380895256996 + 1.0 * 6.286591529846191
Epoch 1160, val loss: 1.1251583099365234
Epoch 1170, training loss: 6.301296710968018 = 0.01600952260196209 + 1.0 * 6.285287380218506
Epoch 1170, val loss: 1.1292392015457153
Epoch 1180, training loss: 6.302170276641846 = 0.015599404461681843 + 1.0 * 6.286571025848389
Epoch 1180, val loss: 1.1332638263702393
Epoch 1190, training loss: 6.302565574645996 = 0.015204022638499737 + 1.0 * 6.2873616218566895
Epoch 1190, val loss: 1.137169361114502
Epoch 1200, training loss: 6.300057888031006 = 0.014825386926531792 + 1.0 * 6.2852325439453125
Epoch 1200, val loss: 1.1409693956375122
Epoch 1210, training loss: 6.298152923583984 = 0.014462519437074661 + 1.0 * 6.283690452575684
Epoch 1210, val loss: 1.1448043584823608
Epoch 1220, training loss: 6.298589706420898 = 0.014111572876572609 + 1.0 * 6.284478187561035
Epoch 1220, val loss: 1.1484975814819336
Epoch 1230, training loss: 6.3008246421813965 = 0.013774230144917965 + 1.0 * 6.287050247192383
Epoch 1230, val loss: 1.1521815061569214
Epoch 1240, training loss: 6.29358434677124 = 0.013449550606310368 + 1.0 * 6.280134677886963
Epoch 1240, val loss: 1.1557914018630981
Epoch 1250, training loss: 6.292359828948975 = 0.013135924935340881 + 1.0 * 6.279223918914795
Epoch 1250, val loss: 1.1594573259353638
Epoch 1260, training loss: 6.296642303466797 = 0.01283204834908247 + 1.0 * 6.283810138702393
Epoch 1260, val loss: 1.1630584001541138
Epoch 1270, training loss: 6.29836893081665 = 0.012539613991975784 + 1.0 * 6.285829544067383
Epoch 1270, val loss: 1.1664546728134155
Epoch 1280, training loss: 6.291266918182373 = 0.012259858660399914 + 1.0 * 6.2790069580078125
Epoch 1280, val loss: 1.1698638200759888
Epoch 1290, training loss: 6.289444923400879 = 0.011990212835371494 + 1.0 * 6.277454853057861
Epoch 1290, val loss: 1.173294186592102
Epoch 1300, training loss: 6.2881879806518555 = 0.011727659031748772 + 1.0 * 6.27646017074585
Epoch 1300, val loss: 1.1767675876617432
Epoch 1310, training loss: 6.307251930236816 = 0.011473278515040874 + 1.0 * 6.295778751373291
Epoch 1310, val loss: 1.1801396608352661
Epoch 1320, training loss: 6.290173530578613 = 0.011228758841753006 + 1.0 * 6.278944969177246
Epoch 1320, val loss: 1.183227777481079
Epoch 1330, training loss: 6.287619113922119 = 0.010994368232786655 + 1.0 * 6.27662467956543
Epoch 1330, val loss: 1.1863397359848022
Epoch 1340, training loss: 6.2851457595825195 = 0.010767173022031784 + 1.0 * 6.274378776550293
Epoch 1340, val loss: 1.189610242843628
Epoch 1350, training loss: 6.287095069885254 = 0.010544780641794205 + 1.0 * 6.27655029296875
Epoch 1350, val loss: 1.1927857398986816
Epoch 1360, training loss: 6.2910475730896 = 0.01032919343560934 + 1.0 * 6.2807183265686035
Epoch 1360, val loss: 1.195887804031372
Epoch 1370, training loss: 6.283376693725586 = 0.010121681727468967 + 1.0 * 6.273254871368408
Epoch 1370, val loss: 1.1989245414733887
Epoch 1380, training loss: 6.282685279846191 = 0.009921474382281303 + 1.0 * 6.272763729095459
Epoch 1380, val loss: 1.2019912004470825
Epoch 1390, training loss: 6.30365514755249 = 0.009726565331220627 + 1.0 * 6.293928623199463
Epoch 1390, val loss: 1.2050292491912842
Epoch 1400, training loss: 6.282925605773926 = 0.009540023282170296 + 1.0 * 6.273385524749756
Epoch 1400, val loss: 1.2078057527542114
Epoch 1410, training loss: 6.280221462249756 = 0.009359071031212807 + 1.0 * 6.270862579345703
Epoch 1410, val loss: 1.2107685804367065
Epoch 1420, training loss: 6.278963088989258 = 0.009182414039969444 + 1.0 * 6.26978063583374
Epoch 1420, val loss: 1.2137113809585571
Epoch 1430, training loss: 6.278492450714111 = 0.009008950553834438 + 1.0 * 6.26948356628418
Epoch 1430, val loss: 1.2165813446044922
Epoch 1440, training loss: 6.293790817260742 = 0.008839845657348633 + 1.0 * 6.2849507331848145
Epoch 1440, val loss: 1.219294548034668
Epoch 1450, training loss: 6.280582427978516 = 0.008677411824464798 + 1.0 * 6.271904945373535
Epoch 1450, val loss: 1.2221574783325195
Epoch 1460, training loss: 6.2763237953186035 = 0.008520171977579594 + 1.0 * 6.26780366897583
Epoch 1460, val loss: 1.2248497009277344
Epoch 1470, training loss: 6.2783002853393555 = 0.00836657453328371 + 1.0 * 6.269933700561523
Epoch 1470, val loss: 1.2275723218917847
Epoch 1480, training loss: 6.279167652130127 = 0.0082173403352499 + 1.0 * 6.2709503173828125
Epoch 1480, val loss: 1.2302734851837158
Epoch 1490, training loss: 6.2777557373046875 = 0.008073876611888409 + 1.0 * 6.269681930541992
Epoch 1490, val loss: 1.2327771186828613
Epoch 1500, training loss: 6.278036117553711 = 0.00793470535427332 + 1.0 * 6.270101547241211
Epoch 1500, val loss: 1.2353761196136475
Epoch 1510, training loss: 6.274061679840088 = 0.007798805832862854 + 1.0 * 6.266263008117676
Epoch 1510, val loss: 1.2380276918411255
Epoch 1520, training loss: 6.2773756980896 = 0.007665712386369705 + 1.0 * 6.269710063934326
Epoch 1520, val loss: 1.2405890226364136
Epoch 1530, training loss: 6.2750630378723145 = 0.007535689510405064 + 1.0 * 6.2675275802612305
Epoch 1530, val loss: 1.2430933713912964
Epoch 1540, training loss: 6.273901462554932 = 0.007409506477415562 + 1.0 * 6.266491889953613
Epoch 1540, val loss: 1.2455655336380005
Epoch 1550, training loss: 6.273353576660156 = 0.007287329528480768 + 1.0 * 6.266066074371338
Epoch 1550, val loss: 1.2480754852294922
Epoch 1560, training loss: 6.275312423706055 = 0.007167330477386713 + 1.0 * 6.2681450843811035
Epoch 1560, val loss: 1.2504825592041016
Epoch 1570, training loss: 6.27537727355957 = 0.0070505873300135136 + 1.0 * 6.268326759338379
Epoch 1570, val loss: 1.252913236618042
Epoch 1580, training loss: 6.274514675140381 = 0.006937532685697079 + 1.0 * 6.267577171325684
Epoch 1580, val loss: 1.2552365064620972
Epoch 1590, training loss: 6.2716898918151855 = 0.006828224752098322 + 1.0 * 6.264861583709717
Epoch 1590, val loss: 1.257563829421997
Epoch 1600, training loss: 6.277118682861328 = 0.006721699144691229 + 1.0 * 6.270397186279297
Epoch 1600, val loss: 1.259812593460083
Epoch 1610, training loss: 6.270018100738525 = 0.006617381703108549 + 1.0 * 6.263400554656982
Epoch 1610, val loss: 1.262154459953308
Epoch 1620, training loss: 6.269010543823242 = 0.006516095716506243 + 1.0 * 6.2624945640563965
Epoch 1620, val loss: 1.2643702030181885
Epoch 1630, training loss: 6.267864227294922 = 0.006416860036551952 + 1.0 * 6.261447429656982
Epoch 1630, val loss: 1.266671895980835
Epoch 1640, training loss: 6.2684478759765625 = 0.006319139152765274 + 1.0 * 6.262128829956055
Epoch 1640, val loss: 1.2689759731292725
Epoch 1650, training loss: 6.274269104003906 = 0.006223391741514206 + 1.0 * 6.268045902252197
Epoch 1650, val loss: 1.2711962461471558
Epoch 1660, training loss: 6.270967483520508 = 0.006130553781986237 + 1.0 * 6.26483678817749
Epoch 1660, val loss: 1.2732281684875488
Epoch 1670, training loss: 6.268579483032227 = 0.006040746346116066 + 1.0 * 6.262538909912109
Epoch 1670, val loss: 1.2754175662994385
Epoch 1680, training loss: 6.269867897033691 = 0.005953084211796522 + 1.0 * 6.263914585113525
Epoch 1680, val loss: 1.277467966079712
Epoch 1690, training loss: 6.271738052368164 = 0.005867902655154467 + 1.0 * 6.265870094299316
Epoch 1690, val loss: 1.2795805931091309
Epoch 1700, training loss: 6.267053604125977 = 0.005784757900983095 + 1.0 * 6.261268615722656
Epoch 1700, val loss: 1.2816110849380493
Epoch 1710, training loss: 6.264906406402588 = 0.005703344009816647 + 1.0 * 6.25920295715332
Epoch 1710, val loss: 1.2837151288986206
Epoch 1720, training loss: 6.265740394592285 = 0.005622846074402332 + 1.0 * 6.260117530822754
Epoch 1720, val loss: 1.2857152223587036
Epoch 1730, training loss: 6.271910667419434 = 0.005544179119169712 + 1.0 * 6.266366481781006
Epoch 1730, val loss: 1.287727952003479
Epoch 1740, training loss: 6.265993595123291 = 0.005468069575726986 + 1.0 * 6.260525703430176
Epoch 1740, val loss: 1.2897168397903442
Epoch 1750, training loss: 6.263589382171631 = 0.005393909756094217 + 1.0 * 6.258195400238037
Epoch 1750, val loss: 1.2916463613510132
Epoch 1760, training loss: 6.262807369232178 = 0.005321120377629995 + 1.0 * 6.257486343383789
Epoch 1760, val loss: 1.293612003326416
Epoch 1770, training loss: 6.269656658172607 = 0.005249152425676584 + 1.0 * 6.264407634735107
Epoch 1770, val loss: 1.2955541610717773
Epoch 1780, training loss: 6.265452861785889 = 0.005179137922823429 + 1.0 * 6.2602739334106445
Epoch 1780, val loss: 1.2974909543991089
Epoch 1790, training loss: 6.266030788421631 = 0.005111122969537973 + 1.0 * 6.260919570922852
Epoch 1790, val loss: 1.2992569208145142
Epoch 1800, training loss: 6.266093730926514 = 0.005044659599661827 + 1.0 * 6.261049270629883
Epoch 1800, val loss: 1.3011258840560913
Epoch 1810, training loss: 6.263930797576904 = 0.004980013705790043 + 1.0 * 6.258950710296631
Epoch 1810, val loss: 1.3029615879058838
Epoch 1820, training loss: 6.261979103088379 = 0.004916435107588768 + 1.0 * 6.2570624351501465
Epoch 1820, val loss: 1.3047624826431274
Epoch 1830, training loss: 6.264204502105713 = 0.004853882826864719 + 1.0 * 6.259350776672363
Epoch 1830, val loss: 1.3065418004989624
Epoch 1840, training loss: 6.263360977172852 = 0.004792482126504183 + 1.0 * 6.258568286895752
Epoch 1840, val loss: 1.3083711862564087
Epoch 1850, training loss: 6.262115955352783 = 0.004732679575681686 + 1.0 * 6.257383346557617
Epoch 1850, val loss: 1.3101177215576172
Epoch 1860, training loss: 6.26253604888916 = 0.004674188327044249 + 1.0 * 6.257862091064453
Epoch 1860, val loss: 1.311822772026062
Epoch 1870, training loss: 6.264683723449707 = 0.004616590682417154 + 1.0 * 6.260066986083984
Epoch 1870, val loss: 1.3135666847229004
Epoch 1880, training loss: 6.260143280029297 = 0.004560772329568863 + 1.0 * 6.255582332611084
Epoch 1880, val loss: 1.315266728401184
Epoch 1890, training loss: 6.258787155151367 = 0.004506159573793411 + 1.0 * 6.254281044006348
Epoch 1890, val loss: 1.3169525861740112
Epoch 1900, training loss: 6.260457992553711 = 0.004452232737094164 + 1.0 * 6.256005764007568
Epoch 1900, val loss: 1.3186355829238892
Epoch 1910, training loss: 6.262713432312012 = 0.004399248864501715 + 1.0 * 6.25831413269043
Epoch 1910, val loss: 1.3203394412994385
Epoch 1920, training loss: 6.257991790771484 = 0.004347141366451979 + 1.0 * 6.2536444664001465
Epoch 1920, val loss: 1.321884036064148
Epoch 1930, training loss: 6.258684158325195 = 0.004296471364796162 + 1.0 * 6.254387855529785
Epoch 1930, val loss: 1.323470115661621
Epoch 1940, training loss: 6.265267372131348 = 0.00424679322168231 + 1.0 * 6.261020660400391
Epoch 1940, val loss: 1.3251069784164429
Epoch 1950, training loss: 6.260958671569824 = 0.004197881557047367 + 1.0 * 6.256760597229004
Epoch 1950, val loss: 1.3267664909362793
Epoch 1960, training loss: 6.26159143447876 = 0.004150453023612499 + 1.0 * 6.25744104385376
Epoch 1960, val loss: 1.3282278776168823
Epoch 1970, training loss: 6.262069225311279 = 0.0041039311327040195 + 1.0 * 6.257965087890625
Epoch 1970, val loss: 1.329843521118164
Epoch 1980, training loss: 6.256237030029297 = 0.004058454185724258 + 1.0 * 6.25217866897583
Epoch 1980, val loss: 1.3313426971435547
Epoch 1990, training loss: 6.254952430725098 = 0.0040134587325155735 + 1.0 * 6.250938892364502
Epoch 1990, val loss: 1.3329304456710815
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.819715340010543
=== training gcn model ===
Epoch 0, training loss: 10.552669525146484 = 1.9558231830596924 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9584710597991943
Epoch 10, training loss: 10.541400909423828 = 1.944808840751648 + 1.0 * 8.59659194946289
Epoch 10, val loss: 1.9475407600402832
Epoch 20, training loss: 10.5255126953125 = 1.9310681819915771 + 1.0 * 8.594444274902344
Epoch 20, val loss: 1.9337321519851685
Epoch 30, training loss: 10.487611770629883 = 1.911871314048767 + 1.0 * 8.575740814208984
Epoch 30, val loss: 1.9142794609069824
Epoch 40, training loss: 10.347440719604492 = 1.886073112487793 + 1.0 * 8.4613676071167
Epoch 40, val loss: 1.8887701034545898
Epoch 50, training loss: 9.87432861328125 = 1.856863021850586 + 1.0 * 8.017465591430664
Epoch 50, val loss: 1.8611648082733154
Epoch 60, training loss: 9.520904541015625 = 1.8314661979675293 + 1.0 * 7.689438343048096
Epoch 60, val loss: 1.838501214981079
Epoch 70, training loss: 9.07906723022461 = 1.8149465322494507 + 1.0 * 7.264120578765869
Epoch 70, val loss: 1.823007583618164
Epoch 80, training loss: 8.822328567504883 = 1.80215322971344 + 1.0 * 7.020174980163574
Epoch 80, val loss: 1.8106067180633545
Epoch 90, training loss: 8.700098991394043 = 1.7882260084152222 + 1.0 * 6.911872863769531
Epoch 90, val loss: 1.797606348991394
Epoch 100, training loss: 8.602389335632324 = 1.7709652185440063 + 1.0 * 6.831423759460449
Epoch 100, val loss: 1.7824583053588867
Epoch 110, training loss: 8.520768165588379 = 1.7533009052276611 + 1.0 * 6.767467021942139
Epoch 110, val loss: 1.7665760517120361
Epoch 120, training loss: 8.454684257507324 = 1.735779047012329 + 1.0 * 6.718905448913574
Epoch 120, val loss: 1.750456690788269
Epoch 130, training loss: 8.387800216674805 = 1.7170937061309814 + 1.0 * 6.670706272125244
Epoch 130, val loss: 1.7336703538894653
Epoch 140, training loss: 8.328574180603027 = 1.6956015825271606 + 1.0 * 6.632972717285156
Epoch 140, val loss: 1.7153071165084839
Epoch 150, training loss: 8.2693510055542 = 1.6702730655670166 + 1.0 * 6.5990777015686035
Epoch 150, val loss: 1.6942331790924072
Epoch 160, training loss: 8.212152481079102 = 1.6402206420898438 + 1.0 * 6.571932315826416
Epoch 160, val loss: 1.6691360473632812
Epoch 170, training loss: 8.1559476852417 = 1.6048314571380615 + 1.0 * 6.551115989685059
Epoch 170, val loss: 1.6393654346466064
Epoch 180, training loss: 8.096078872680664 = 1.5637601613998413 + 1.0 * 6.532318592071533
Epoch 180, val loss: 1.6048604249954224
Epoch 190, training loss: 8.033437728881836 = 1.5165311098098755 + 1.0 * 6.51690673828125
Epoch 190, val loss: 1.5655012130737305
Epoch 200, training loss: 7.967790126800537 = 1.4638158082962036 + 1.0 * 6.503974437713623
Epoch 200, val loss: 1.5220195055007935
Epoch 210, training loss: 7.906556606292725 = 1.4066821336746216 + 1.0 * 6.499874591827393
Epoch 210, val loss: 1.475615382194519
Epoch 220, training loss: 7.830974578857422 = 1.347678542137146 + 1.0 * 6.483295917510986
Epoch 220, val loss: 1.4286490678787231
Epoch 230, training loss: 7.76033878326416 = 1.2874280214309692 + 1.0 * 6.4729108810424805
Epoch 230, val loss: 1.3815398216247559
Epoch 240, training loss: 7.692466735839844 = 1.226431965827942 + 1.0 * 6.466034889221191
Epoch 240, val loss: 1.3348487615585327
Epoch 250, training loss: 7.624541759490967 = 1.1662081480026245 + 1.0 * 6.458333492279053
Epoch 250, val loss: 1.2897872924804688
Epoch 260, training loss: 7.5580596923828125 = 1.10727858543396 + 1.0 * 6.450780868530273
Epoch 260, val loss: 1.2463839054107666
Epoch 270, training loss: 7.496114730834961 = 1.0499025583267212 + 1.0 * 6.446212291717529
Epoch 270, val loss: 1.2049298286437988
Epoch 280, training loss: 7.432725429534912 = 0.9952546954154968 + 1.0 * 6.43747091293335
Epoch 280, val loss: 1.1663072109222412
Epoch 290, training loss: 7.380070686340332 = 0.9432738423347473 + 1.0 * 6.43679666519165
Epoch 290, val loss: 1.130531907081604
Epoch 300, training loss: 7.321468830108643 = 0.8949910998344421 + 1.0 * 6.426477909088135
Epoch 300, val loss: 1.0982671976089478
Epoch 310, training loss: 7.273501396179199 = 0.8505288362503052 + 1.0 * 6.422972679138184
Epoch 310, val loss: 1.0696806907653809
Epoch 320, training loss: 7.22493314743042 = 0.8098911046981812 + 1.0 * 6.415041923522949
Epoch 320, val loss: 1.0447272062301636
Epoch 330, training loss: 7.185547351837158 = 0.7729679346084595 + 1.0 * 6.412579536437988
Epoch 330, val loss: 1.023176908493042
Epoch 340, training loss: 7.144074440002441 = 0.7396025657653809 + 1.0 * 6.4044718742370605
Epoch 340, val loss: 1.0048916339874268
Epoch 350, training loss: 7.107950687408447 = 0.7089505791664124 + 1.0 * 6.39900016784668
Epoch 350, val loss: 0.9891902208328247
Epoch 360, training loss: 7.076961994171143 = 0.6805008053779602 + 1.0 * 6.396461009979248
Epoch 360, val loss: 0.9756218791007996
Epoch 370, training loss: 7.044428825378418 = 0.6540484428405762 + 1.0 * 6.390380382537842
Epoch 370, val loss: 0.9640041589736938
Epoch 380, training loss: 7.014096260070801 = 0.6289619207382202 + 1.0 * 6.385134220123291
Epoch 380, val loss: 0.9538595676422119
Epoch 390, training loss: 6.9918742179870605 = 0.6051496267318726 + 1.0 * 6.386724472045898
Epoch 390, val loss: 0.9450206756591797
Epoch 400, training loss: 6.961539268493652 = 0.5826042294502258 + 1.0 * 6.378934860229492
Epoch 400, val loss: 0.9375029802322388
Epoch 410, training loss: 6.9353485107421875 = 0.5608953833580017 + 1.0 * 6.374453067779541
Epoch 410, val loss: 0.931013286113739
Epoch 420, training loss: 6.909795761108398 = 0.5398034453392029 + 1.0 * 6.369992256164551
Epoch 420, val loss: 0.9254866242408752
Epoch 430, training loss: 6.898430824279785 = 0.519244909286499 + 1.0 * 6.379186153411865
Epoch 430, val loss: 0.9208618998527527
Epoch 440, training loss: 6.867705821990967 = 0.499554306268692 + 1.0 * 6.368151664733887
Epoch 440, val loss: 0.9172586798667908
Epoch 450, training loss: 6.844127178192139 = 0.4805389940738678 + 1.0 * 6.363588333129883
Epoch 450, val loss: 0.9146276116371155
Epoch 460, training loss: 6.81978702545166 = 0.4619651734828949 + 1.0 * 6.357821941375732
Epoch 460, val loss: 0.9127453565597534
Epoch 470, training loss: 6.799694061279297 = 0.44376900792121887 + 1.0 * 6.3559250831604
Epoch 470, val loss: 0.9116910696029663
Epoch 480, training loss: 6.77887487411499 = 0.4260328412055969 + 1.0 * 6.352841854095459
Epoch 480, val loss: 0.911455512046814
Epoch 490, training loss: 6.7648491859436035 = 0.40884289145469666 + 1.0 * 6.356006145477295
Epoch 490, val loss: 0.9120092391967773
Epoch 500, training loss: 6.746891975402832 = 0.392208069562912 + 1.0 * 6.354683876037598
Epoch 500, val loss: 0.9132434129714966
Epoch 510, training loss: 6.728743553161621 = 0.37610575556755066 + 1.0 * 6.352637767791748
Epoch 510, val loss: 0.915189266204834
Epoch 520, training loss: 6.7067999839782715 = 0.3604603707790375 + 1.0 * 6.346339702606201
Epoch 520, val loss: 0.9176754951477051
Epoch 530, training loss: 6.688554763793945 = 0.3452335298061371 + 1.0 * 6.343321323394775
Epoch 530, val loss: 0.9208111763000488
Epoch 540, training loss: 6.678348541259766 = 0.330354779958725 + 1.0 * 6.347993850708008
Epoch 540, val loss: 0.9244373440742493
Epoch 550, training loss: 6.657257080078125 = 0.3159825801849365 + 1.0 * 6.341274738311768
Epoch 550, val loss: 0.9286952018737793
Epoch 560, training loss: 6.641397953033447 = 0.30197012424468994 + 1.0 * 6.339427947998047
Epoch 560, val loss: 0.9334567189216614
Epoch 570, training loss: 6.6253814697265625 = 0.28829869627952576 + 1.0 * 6.337082862854004
Epoch 570, val loss: 0.9387865662574768
Epoch 580, training loss: 6.612586975097656 = 0.274996280670166 + 1.0 * 6.33759069442749
Epoch 580, val loss: 0.9445714354515076
Epoch 590, training loss: 6.594315528869629 = 0.26211708784103394 + 1.0 * 6.332198619842529
Epoch 590, val loss: 0.9511345624923706
Epoch 600, training loss: 6.580286979675293 = 0.24957667291164398 + 1.0 * 6.330710411071777
Epoch 600, val loss: 0.9581546187400818
Epoch 610, training loss: 6.573587894439697 = 0.23740527033805847 + 1.0 * 6.336182594299316
Epoch 610, val loss: 0.9657508134841919
Epoch 620, training loss: 6.553141117095947 = 0.2256549745798111 + 1.0 * 6.327486038208008
Epoch 620, val loss: 0.9740716814994812
Epoch 630, training loss: 6.550159454345703 = 0.2143050730228424 + 1.0 * 6.335854530334473
Epoch 630, val loss: 0.9829434156417847
Epoch 640, training loss: 6.531399726867676 = 0.20338444411754608 + 1.0 * 6.328015327453613
Epoch 640, val loss: 0.9922392964363098
Epoch 650, training loss: 6.517069339752197 = 0.1928853988647461 + 1.0 * 6.324183940887451
Epoch 650, val loss: 1.002281904220581
Epoch 660, training loss: 6.508235454559326 = 0.182761549949646 + 1.0 * 6.325473785400391
Epoch 660, val loss: 1.0127050876617432
Epoch 670, training loss: 6.4938507080078125 = 0.1730508655309677 + 1.0 * 6.320799827575684
Epoch 670, val loss: 1.0236543416976929
Epoch 680, training loss: 6.4836039543151855 = 0.16375455260276794 + 1.0 * 6.319849491119385
Epoch 680, val loss: 1.0351125001907349
Epoch 690, training loss: 6.4767279624938965 = 0.15484750270843506 + 1.0 * 6.321880340576172
Epoch 690, val loss: 1.0469506978988647
Epoch 700, training loss: 6.467239856719971 = 0.14635859429836273 + 1.0 * 6.320881366729736
Epoch 700, val loss: 1.0589442253112793
Epoch 710, training loss: 6.4566264152526855 = 0.13830308616161346 + 1.0 * 6.318323135375977
Epoch 710, val loss: 1.071524739265442
Epoch 720, training loss: 6.449655055999756 = 0.13065406680107117 + 1.0 * 6.319001197814941
Epoch 720, val loss: 1.0842069387435913
Epoch 730, training loss: 6.438758373260498 = 0.123409703373909 + 1.0 * 6.3153486251831055
Epoch 730, val loss: 1.09730863571167
Epoch 740, training loss: 6.431875705718994 = 0.11654135584831238 + 1.0 * 6.315334320068359
Epoch 740, val loss: 1.110481858253479
Epoch 750, training loss: 6.424748420715332 = 0.11005686223506927 + 1.0 * 6.314691543579102
Epoch 750, val loss: 1.1237934827804565
Epoch 760, training loss: 6.415280342102051 = 0.10395049303770065 + 1.0 * 6.3113298416137695
Epoch 760, val loss: 1.1372934579849243
Epoch 770, training loss: 6.408340930938721 = 0.09819943457841873 + 1.0 * 6.310141563415527
Epoch 770, val loss: 1.1509106159210205
Epoch 780, training loss: 6.413909912109375 = 0.09279085695743561 + 1.0 * 6.3211188316345215
Epoch 780, val loss: 1.164423942565918
Epoch 790, training loss: 6.395905017852783 = 0.08774401247501373 + 1.0 * 6.308160781860352
Epoch 790, val loss: 1.1781878471374512
Epoch 800, training loss: 6.390592098236084 = 0.08301305770874023 + 1.0 * 6.307579040527344
Epoch 800, val loss: 1.1919147968292236
Epoch 810, training loss: 6.39553689956665 = 0.07857172191143036 + 1.0 * 6.316965103149414
Epoch 810, val loss: 1.2054505348205566
Epoch 820, training loss: 6.3874077796936035 = 0.07443099468946457 + 1.0 * 6.312976837158203
Epoch 820, val loss: 1.2189886569976807
Epoch 830, training loss: 6.375478267669678 = 0.07056838274002075 + 1.0 * 6.304909706115723
Epoch 830, val loss: 1.2326793670654297
Epoch 840, training loss: 6.369960784912109 = 0.06694867461919785 + 1.0 * 6.303011894226074
Epoch 840, val loss: 1.2460861206054688
Epoch 850, training loss: 6.3739013671875 = 0.06356160342693329 + 1.0 * 6.31033992767334
Epoch 850, val loss: 1.2595335245132446
Epoch 860, training loss: 6.376930236816406 = 0.06040080264210701 + 1.0 * 6.316529273986816
Epoch 860, val loss: 1.2728798389434814
Epoch 870, training loss: 6.361037731170654 = 0.05745348706841469 + 1.0 * 6.303584098815918
Epoch 870, val loss: 1.2857424020767212
Epoch 880, training loss: 6.356207370758057 = 0.054703932255506516 + 1.0 * 6.301503658294678
Epoch 880, val loss: 1.2989821434020996
Epoch 890, training loss: 6.350451469421387 = 0.05212411284446716 + 1.0 * 6.298327445983887
Epoch 890, val loss: 1.3117890357971191
Epoch 900, training loss: 6.347006797790527 = 0.049704164266586304 + 1.0 * 6.297302722930908
Epoch 900, val loss: 1.3246634006500244
Epoch 910, training loss: 6.367715358734131 = 0.047438208013772964 + 1.0 * 6.320277214050293
Epoch 910, val loss: 1.3372254371643066
Epoch 920, training loss: 6.341912746429443 = 0.0453202947974205 + 1.0 * 6.2965922355651855
Epoch 920, val loss: 1.3496166467666626
Epoch 930, training loss: 6.339511871337891 = 0.043342411518096924 + 1.0 * 6.296169281005859
Epoch 930, val loss: 1.3619991540908813
Epoch 940, training loss: 6.337002277374268 = 0.04148244112730026 + 1.0 * 6.295519828796387
Epoch 940, val loss: 1.373996615409851
Epoch 950, training loss: 6.344275951385498 = 0.03973451256752014 + 1.0 * 6.30454158782959
Epoch 950, val loss: 1.385866641998291
Epoch 960, training loss: 6.3347039222717285 = 0.03809601068496704 + 1.0 * 6.296607971191406
Epoch 960, val loss: 1.3978404998779297
Epoch 970, training loss: 6.329927444458008 = 0.03655291348695755 + 1.0 * 6.293374538421631
Epoch 970, val loss: 1.409467101097107
Epoch 980, training loss: 6.333459854125977 = 0.03510009124875069 + 1.0 * 6.2983598709106445
Epoch 980, val loss: 1.4208585023880005
Epoch 990, training loss: 6.327205181121826 = 0.033736396580934525 + 1.0 * 6.293468952178955
Epoch 990, val loss: 1.4321478605270386
Epoch 1000, training loss: 6.323313236236572 = 0.03245067596435547 + 1.0 * 6.290862560272217
Epoch 1000, val loss: 1.443336009979248
Epoch 1010, training loss: 6.320140838623047 = 0.03123711794614792 + 1.0 * 6.288903713226318
Epoch 1010, val loss: 1.4543174505233765
Epoch 1020, training loss: 6.319558620452881 = 0.030089203268289566 + 1.0 * 6.289469242095947
Epoch 1020, val loss: 1.4651787281036377
Epoch 1030, training loss: 6.322815895080566 = 0.029002653434872627 + 1.0 * 6.293813228607178
Epoch 1030, val loss: 1.4757009744644165
Epoch 1040, training loss: 6.318478107452393 = 0.027977094054222107 + 1.0 * 6.290501117706299
Epoch 1040, val loss: 1.4862627983093262
Epoch 1050, training loss: 6.318539619445801 = 0.027007456868886948 + 1.0 * 6.291532039642334
Epoch 1050, val loss: 1.4965794086456299
Epoch 1060, training loss: 6.3150458335876465 = 0.02609114907681942 + 1.0 * 6.288954734802246
Epoch 1060, val loss: 1.5067195892333984
Epoch 1070, training loss: 6.314619064331055 = 0.025222886353731155 + 1.0 * 6.289396286010742
Epoch 1070, val loss: 1.5167423486709595
Epoch 1080, training loss: 6.309930801391602 = 0.024402763694524765 + 1.0 * 6.285528182983398
Epoch 1080, val loss: 1.5268018245697021
Epoch 1090, training loss: 6.3090386390686035 = 0.02362166903913021 + 1.0 * 6.285417079925537
Epoch 1090, val loss: 1.5366312265396118
Epoch 1100, training loss: 6.309107303619385 = 0.022877344861626625 + 1.0 * 6.286230087280273
Epoch 1100, val loss: 1.546233057975769
Epoch 1110, training loss: 6.30827522277832 = 0.022168414667248726 + 1.0 * 6.286106586456299
Epoch 1110, val loss: 1.5556570291519165
Epoch 1120, training loss: 6.307801246643066 = 0.02149394154548645 + 1.0 * 6.286307334899902
Epoch 1120, val loss: 1.564809799194336
Epoch 1130, training loss: 6.303595542907715 = 0.02085343934595585 + 1.0 * 6.282742023468018
Epoch 1130, val loss: 1.5741324424743652
Epoch 1140, training loss: 6.306321620941162 = 0.020241690799593925 + 1.0 * 6.2860798835754395
Epoch 1140, val loss: 1.5832008123397827
Epoch 1150, training loss: 6.30136251449585 = 0.019658168777823448 + 1.0 * 6.281704425811768
Epoch 1150, val loss: 1.591961145401001
Epoch 1160, training loss: 6.300512790679932 = 0.01910196989774704 + 1.0 * 6.2814106941223145
Epoch 1160, val loss: 1.60102117061615
Epoch 1170, training loss: 6.308581352233887 = 0.018569808453321457 + 1.0 * 6.290011405944824
Epoch 1170, val loss: 1.6095529794692993
Epoch 1180, training loss: 6.298673152923584 = 0.01806054264307022 + 1.0 * 6.280612468719482
Epoch 1180, val loss: 1.6180018186569214
Epoch 1190, training loss: 6.298219680786133 = 0.017575733363628387 + 1.0 * 6.280643939971924
Epoch 1190, val loss: 1.6266347169876099
Epoch 1200, training loss: 6.296522617340088 = 0.017110105603933334 + 1.0 * 6.279412746429443
Epoch 1200, val loss: 1.6348466873168945
Epoch 1210, training loss: 6.293856620788574 = 0.01666436158120632 + 1.0 * 6.277192115783691
Epoch 1210, val loss: 1.6433042287826538
Epoch 1220, training loss: 6.294360637664795 = 0.016235830262303352 + 1.0 * 6.278124809265137
Epoch 1220, val loss: 1.6513720750808716
Epoch 1230, training loss: 6.301069736480713 = 0.015824342146515846 + 1.0 * 6.285245418548584
Epoch 1230, val loss: 1.659080147743225
Epoch 1240, training loss: 6.2926764488220215 = 0.015433380380272865 + 1.0 * 6.277243137359619
Epoch 1240, val loss: 1.6671690940856934
Epoch 1250, training loss: 6.290635108947754 = 0.015057903714478016 + 1.0 * 6.275577068328857
Epoch 1250, val loss: 1.675222396850586
Epoch 1260, training loss: 6.288975238800049 = 0.014695445075631142 + 1.0 * 6.274279594421387
Epoch 1260, val loss: 1.6828209161758423
Epoch 1270, training loss: 6.29539680480957 = 0.014346785843372345 + 1.0 * 6.281050205230713
Epoch 1270, val loss: 1.69035804271698
Epoch 1280, training loss: 6.296872615814209 = 0.014009431935846806 + 1.0 * 6.282863140106201
Epoch 1280, val loss: 1.6977744102478027
Epoch 1290, training loss: 6.288133144378662 = 0.013688878156244755 + 1.0 * 6.274444103240967
Epoch 1290, val loss: 1.7053219079971313
Epoch 1300, training loss: 6.285893440246582 = 0.013378911651670933 + 1.0 * 6.272514343261719
Epoch 1300, val loss: 1.7128283977508545
Epoch 1310, training loss: 6.283952236175537 = 0.013079825788736343 + 1.0 * 6.270872592926025
Epoch 1310, val loss: 1.7200901508331299
Epoch 1320, training loss: 6.2857842445373535 = 0.012790209613740444 + 1.0 * 6.272994041442871
Epoch 1320, val loss: 1.7272391319274902
Epoch 1330, training loss: 6.288167953491211 = 0.012510665692389011 + 1.0 * 6.2756571769714355
Epoch 1330, val loss: 1.734108328819275
Epoch 1340, training loss: 6.284898281097412 = 0.012242593802511692 + 1.0 * 6.272655487060547
Epoch 1340, val loss: 1.741268515586853
Epoch 1350, training loss: 6.282637119293213 = 0.011983873322606087 + 1.0 * 6.270653247833252
Epoch 1350, val loss: 1.7483433485031128
Epoch 1360, training loss: 6.281976699829102 = 0.0117342509329319 + 1.0 * 6.270242214202881
Epoch 1360, val loss: 1.7551674842834473
Epoch 1370, training loss: 6.280409812927246 = 0.011493080295622349 + 1.0 * 6.268916606903076
Epoch 1370, val loss: 1.762008547782898
Epoch 1380, training loss: 6.281599521636963 = 0.011259126476943493 + 1.0 * 6.270340442657471
Epoch 1380, val loss: 1.7687546014785767
Epoch 1390, training loss: 6.284268856048584 = 0.011032378301024437 + 1.0 * 6.273236274719238
Epoch 1390, val loss: 1.7752008438110352
Epoch 1400, training loss: 6.2820563316345215 = 0.010812810622155666 + 1.0 * 6.271243572235107
Epoch 1400, val loss: 1.7815767526626587
Epoch 1410, training loss: 6.279557704925537 = 0.010602056980133057 + 1.0 * 6.268955707550049
Epoch 1410, val loss: 1.7883297204971313
Epoch 1420, training loss: 6.286449432373047 = 0.010397030971944332 + 1.0 * 6.276052474975586
Epoch 1420, val loss: 1.7946114540100098
Epoch 1430, training loss: 6.278848171234131 = 0.010199273936450481 + 1.0 * 6.268649101257324
Epoch 1430, val loss: 1.8006154298782349
Epoch 1440, training loss: 6.276514053344727 = 0.01000785268843174 + 1.0 * 6.266506195068359
Epoch 1440, val loss: 1.8071744441986084
Epoch 1450, training loss: 6.278843879699707 = 0.009821752086281776 + 1.0 * 6.269021987915039
Epoch 1450, val loss: 1.8132622241973877
Epoch 1460, training loss: 6.275943756103516 = 0.009640879929065704 + 1.0 * 6.266303062438965
Epoch 1460, val loss: 1.8192167282104492
Epoch 1470, training loss: 6.274923324584961 = 0.009466661140322685 + 1.0 * 6.265456676483154
Epoch 1470, val loss: 1.8255864381790161
Epoch 1480, training loss: 6.28116512298584 = 0.009296281263232231 + 1.0 * 6.271868705749512
Epoch 1480, val loss: 1.8315688371658325
Epoch 1490, training loss: 6.2757887840271 = 0.00913107581436634 + 1.0 * 6.266657829284668
Epoch 1490, val loss: 1.8371225595474243
Epoch 1500, training loss: 6.275336742401123 = 0.008971871808171272 + 1.0 * 6.266365051269531
Epoch 1500, val loss: 1.8433161973953247
Epoch 1510, training loss: 6.271901607513428 = 0.008816564455628395 + 1.0 * 6.263084888458252
Epoch 1510, val loss: 1.849077820777893
Epoch 1520, training loss: 6.274779319763184 = 0.008665519766509533 + 1.0 * 6.266113758087158
Epoch 1520, val loss: 1.8547861576080322
Epoch 1530, training loss: 6.271433353424072 = 0.008518547751009464 + 1.0 * 6.262914657592773
Epoch 1530, val loss: 1.8603118658065796
Epoch 1540, training loss: 6.270406723022461 = 0.008376644924283028 + 1.0 * 6.262030124664307
Epoch 1540, val loss: 1.8663066625595093
Epoch 1550, training loss: 6.2745361328125 = 0.008238047361373901 + 1.0 * 6.266298294067383
Epoch 1550, val loss: 1.871935486793518
Epoch 1560, training loss: 6.271002769470215 = 0.008102788589894772 + 1.0 * 6.262899875640869
Epoch 1560, val loss: 1.8771075010299683
Epoch 1570, training loss: 6.271287441253662 = 0.007971994578838348 + 1.0 * 6.263315677642822
Epoch 1570, val loss: 1.882725715637207
Epoch 1580, training loss: 6.275756359100342 = 0.007844766601920128 + 1.0 * 6.267911434173584
Epoch 1580, val loss: 1.888298749923706
Epoch 1590, training loss: 6.2701616287231445 = 0.007720411289483309 + 1.0 * 6.262441158294678
Epoch 1590, val loss: 1.8935273885726929
Epoch 1600, training loss: 6.269001007080078 = 0.007599882781505585 + 1.0 * 6.261401176452637
Epoch 1600, val loss: 1.89907968044281
Epoch 1610, training loss: 6.268327713012695 = 0.007481809239834547 + 1.0 * 6.260846138000488
Epoch 1610, val loss: 1.904270887374878
Epoch 1620, training loss: 6.273647785186768 = 0.007366726640611887 + 1.0 * 6.2662811279296875
Epoch 1620, val loss: 1.9094613790512085
Epoch 1630, training loss: 6.265133857727051 = 0.00725509924814105 + 1.0 * 6.25787878036499
Epoch 1630, val loss: 1.9145628213882446
Epoch 1640, training loss: 6.2646660804748535 = 0.0071464176289737225 + 1.0 * 6.257519721984863
Epoch 1640, val loss: 1.919938564300537
Epoch 1650, training loss: 6.266617298126221 = 0.007039732299745083 + 1.0 * 6.259577751159668
Epoch 1650, val loss: 1.9250303506851196
Epoch 1660, training loss: 6.271305561065674 = 0.006935579236596823 + 1.0 * 6.264369964599609
Epoch 1660, val loss: 1.929732084274292
Epoch 1670, training loss: 6.267545700073242 = 0.0068346732296049595 + 1.0 * 6.260711193084717
Epoch 1670, val loss: 1.9349722862243652
Epoch 1680, training loss: 6.267088413238525 = 0.006735947914421558 + 1.0 * 6.260352611541748
Epoch 1680, val loss: 1.939958095550537
Epoch 1690, training loss: 6.263166427612305 = 0.006639701779931784 + 1.0 * 6.256526947021484
Epoch 1690, val loss: 1.9449387788772583
Epoch 1700, training loss: 6.262699127197266 = 0.006545597221702337 + 1.0 * 6.256153583526611
Epoch 1700, val loss: 1.9499703645706177
Epoch 1710, training loss: 6.270292282104492 = 0.006453083362430334 + 1.0 * 6.263839244842529
Epoch 1710, val loss: 1.9548349380493164
Epoch 1720, training loss: 6.2716145515441895 = 0.006363373715430498 + 1.0 * 6.265251159667969
Epoch 1720, val loss: 1.9594509601593018
Epoch 1730, training loss: 6.265360355377197 = 0.006275888066738844 + 1.0 * 6.259084701538086
Epoch 1730, val loss: 1.9642802476882935
Epoch 1740, training loss: 6.262889385223389 = 0.006190887652337551 + 1.0 * 6.2566986083984375
Epoch 1740, val loss: 1.9692503213882446
Epoch 1750, training loss: 6.264764785766602 = 0.006107604131102562 + 1.0 * 6.258656978607178
Epoch 1750, val loss: 1.9738928079605103
Epoch 1760, training loss: 6.261045455932617 = 0.006026007700711489 + 1.0 * 6.255019664764404
Epoch 1760, val loss: 1.9783967733383179
Epoch 1770, training loss: 6.2641401290893555 = 0.0059463088400661945 + 1.0 * 6.2581939697265625
Epoch 1770, val loss: 1.983185887336731
Epoch 1780, training loss: 6.261013984680176 = 0.005867896601557732 + 1.0 * 6.255146026611328
Epoch 1780, val loss: 1.9877099990844727
Epoch 1790, training loss: 6.261964321136475 = 0.005791410803794861 + 1.0 * 6.256173133850098
Epoch 1790, val loss: 1.9923450946807861
Epoch 1800, training loss: 6.261114597320557 = 0.005716453306376934 + 1.0 * 6.255398273468018
Epoch 1800, val loss: 1.996795654296875
Epoch 1810, training loss: 6.263723850250244 = 0.005643214099109173 + 1.0 * 6.25808048248291
Epoch 1810, val loss: 2.001183271408081
Epoch 1820, training loss: 6.261294841766357 = 0.005571315530687571 + 1.0 * 6.255723476409912
Epoch 1820, val loss: 2.0055124759674072
Epoch 1830, training loss: 6.261359691619873 = 0.005501862615346909 + 1.0 * 6.255857944488525
Epoch 1830, val loss: 2.0100858211517334
Epoch 1840, training loss: 6.260776519775391 = 0.005433718208223581 + 1.0 * 6.255342960357666
Epoch 1840, val loss: 2.014493703842163
Epoch 1850, training loss: 6.258514881134033 = 0.0053672390058636665 + 1.0 * 6.253147602081299
Epoch 1850, val loss: 2.018798351287842
Epoch 1860, training loss: 6.256516456604004 = 0.005301845259964466 + 1.0 * 6.251214504241943
Epoch 1860, val loss: 2.023308515548706
Epoch 1870, training loss: 6.256741523742676 = 0.005237238947302103 + 1.0 * 6.251504421234131
Epoch 1870, val loss: 2.027615547180176
Epoch 1880, training loss: 6.264303207397461 = 0.0051735080778598785 + 1.0 * 6.259129524230957
Epoch 1880, val loss: 2.031712532043457
Epoch 1890, training loss: 6.2655863761901855 = 0.005111370701342821 + 1.0 * 6.260475158691406
Epoch 1890, val loss: 2.035633087158203
Epoch 1900, training loss: 6.2599921226501465 = 0.005051427520811558 + 1.0 * 6.254940509796143
Epoch 1900, val loss: 2.040036916732788
Epoch 1910, training loss: 6.255614757537842 = 0.004992888309061527 + 1.0 * 6.250621795654297
Epoch 1910, val loss: 2.044508218765259
Epoch 1920, training loss: 6.254906177520752 = 0.0049353535287082195 + 1.0 * 6.24997091293335
Epoch 1920, val loss: 2.048672676086426
Epoch 1930, training loss: 6.260284900665283 = 0.004878474399447441 + 1.0 * 6.255406379699707
Epoch 1930, val loss: 2.052673578262329
Epoch 1940, training loss: 6.254833698272705 = 0.004822151269763708 + 1.0 * 6.250011444091797
Epoch 1940, val loss: 2.056490421295166
Epoch 1950, training loss: 6.26038122177124 = 0.004767369478940964 + 1.0 * 6.255613803863525
Epoch 1950, val loss: 2.0606751441955566
Epoch 1960, training loss: 6.256083011627197 = 0.004713735543191433 + 1.0 * 6.251369476318359
Epoch 1960, val loss: 2.0647616386413574
Epoch 1970, training loss: 6.263851642608643 = 0.004661391489207745 + 1.0 * 6.259190082550049
Epoch 1970, val loss: 2.06884765625
Epoch 1980, training loss: 6.254746913909912 = 0.00460995314642787 + 1.0 * 6.250136852264404
Epoch 1980, val loss: 2.072392225265503
Epoch 1990, training loss: 6.2532124519348145 = 0.004559964407235384 + 1.0 * 6.248652458190918
Epoch 1990, val loss: 2.0766704082489014
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 10.52872085571289 = 1.931851863861084 + 1.0 * 8.596869468688965
Epoch 0, val loss: 1.9328176975250244
Epoch 10, training loss: 10.519088745117188 = 1.9223614931106567 + 1.0 * 8.59672737121582
Epoch 10, val loss: 1.9240713119506836
Epoch 20, training loss: 10.506329536437988 = 1.910957932472229 + 1.0 * 8.59537124633789
Epoch 20, val loss: 1.913092017173767
Epoch 30, training loss: 10.478857040405273 = 1.8952537775039673 + 1.0 * 8.583602905273438
Epoch 30, val loss: 1.8976187705993652
Epoch 40, training loss: 10.38999080657959 = 1.873774528503418 + 1.0 * 8.516216278076172
Epoch 40, val loss: 1.8768736124038696
Epoch 50, training loss: 10.054039001464844 = 1.8490780591964722 + 1.0 * 8.204960823059082
Epoch 50, val loss: 1.854128360748291
Epoch 60, training loss: 9.776374816894531 = 1.8246936798095703 + 1.0 * 7.951681137084961
Epoch 60, val loss: 1.8329044580459595
Epoch 70, training loss: 9.46275806427002 = 1.8056660890579224 + 1.0 * 7.657092094421387
Epoch 70, val loss: 1.8150489330291748
Epoch 80, training loss: 9.051939010620117 = 1.790243148803711 + 1.0 * 7.261695384979248
Epoch 80, val loss: 1.7999458312988281
Epoch 90, training loss: 8.798331260681152 = 1.7767316102981567 + 1.0 * 7.021599769592285
Epoch 90, val loss: 1.7868455648422241
Epoch 100, training loss: 8.640307426452637 = 1.7608673572540283 + 1.0 * 6.879439830780029
Epoch 100, val loss: 1.772684931755066
Epoch 110, training loss: 8.53149700164795 = 1.7428975105285645 + 1.0 * 6.788599491119385
Epoch 110, val loss: 1.7564290761947632
Epoch 120, training loss: 8.461864471435547 = 1.72244393825531 + 1.0 * 6.7394208908081055
Epoch 120, val loss: 1.7376840114593506
Epoch 130, training loss: 8.405964851379395 = 1.6993707418441772 + 1.0 * 6.706593990325928
Epoch 130, val loss: 1.7165952920913696
Epoch 140, training loss: 8.349780082702637 = 1.6726375818252563 + 1.0 * 6.677142143249512
Epoch 140, val loss: 1.6925076246261597
Epoch 150, training loss: 8.291889190673828 = 1.6416658163070679 + 1.0 * 6.650223731994629
Epoch 150, val loss: 1.6653367280960083
Epoch 160, training loss: 8.22961139678955 = 1.6060473918914795 + 1.0 * 6.623563766479492
Epoch 160, val loss: 1.6346187591552734
Epoch 170, training loss: 8.166828155517578 = 1.5653835535049438 + 1.0 * 6.601444721221924
Epoch 170, val loss: 1.5998777151107788
Epoch 180, training loss: 8.101659774780273 = 1.5198668241500854 + 1.0 * 6.581793308258057
Epoch 180, val loss: 1.5613876581192017
Epoch 190, training loss: 8.034714698791504 = 1.4695054292678833 + 1.0 * 6.565208911895752
Epoch 190, val loss: 1.5191172361373901
Epoch 200, training loss: 7.966188430786133 = 1.4152634143829346 + 1.0 * 6.550924777984619
Epoch 200, val loss: 1.4741034507751465
Epoch 210, training loss: 7.897042274475098 = 1.3592288494110107 + 1.0 * 6.537813186645508
Epoch 210, val loss: 1.4286397695541382
Epoch 220, training loss: 7.83013916015625 = 1.3036715984344482 + 1.0 * 6.526467800140381
Epoch 220, val loss: 1.3843936920166016
Epoch 230, training loss: 7.765990257263184 = 1.2491180896759033 + 1.0 * 6.516872406005859
Epoch 230, val loss: 1.3419831991195679
Epoch 240, training loss: 7.704770088195801 = 1.196975827217102 + 1.0 * 6.507794380187988
Epoch 240, val loss: 1.3023412227630615
Epoch 250, training loss: 7.645498752593994 = 1.146997332572937 + 1.0 * 6.498501300811768
Epoch 250, val loss: 1.26534903049469
Epoch 260, training loss: 7.595810890197754 = 1.0987581014633179 + 1.0 * 6.4970526695251465
Epoch 260, val loss: 1.2302844524383545
Epoch 270, training loss: 7.540248870849609 = 1.0527554750442505 + 1.0 * 6.487493515014648
Epoch 270, val loss: 1.1977070569992065
Epoch 280, training loss: 7.488143444061279 = 1.0085021257400513 + 1.0 * 6.479641437530518
Epoch 280, val loss: 1.1670012474060059
Epoch 290, training loss: 7.439427852630615 = 0.9653924107551575 + 1.0 * 6.474035263061523
Epoch 290, val loss: 1.137588620185852
Epoch 300, training loss: 7.394435882568359 = 0.9233416318893433 + 1.0 * 6.471094131469727
Epoch 300, val loss: 1.1096750497817993
Epoch 310, training loss: 7.345757007598877 = 0.8827272057533264 + 1.0 * 6.463029861450195
Epoch 310, val loss: 1.0832723379135132
Epoch 320, training loss: 7.302201747894287 = 0.8436347842216492 + 1.0 * 6.458567142486572
Epoch 320, val loss: 1.0582383871078491
Epoch 330, training loss: 7.258830547332764 = 0.8061742186546326 + 1.0 * 6.452656269073486
Epoch 330, val loss: 1.0350522994995117
Epoch 340, training loss: 7.2184882164001465 = 0.7701473236083984 + 1.0 * 6.448340892791748
Epoch 340, val loss: 1.0131762027740479
Epoch 350, training loss: 7.183136940002441 = 0.7355852127075195 + 1.0 * 6.447551727294922
Epoch 350, val loss: 0.9928049445152283
Epoch 360, training loss: 7.142716407775879 = 0.7027836441993713 + 1.0 * 6.439932823181152
Epoch 360, val loss: 0.9741658568382263
Epoch 370, training loss: 7.106141567230225 = 0.671552836894989 + 1.0 * 6.43458890914917
Epoch 370, val loss: 0.9570817947387695
Epoch 380, training loss: 7.080842018127441 = 0.6417359113693237 + 1.0 * 6.439105987548828
Epoch 380, val loss: 0.9415024518966675
Epoch 390, training loss: 7.045968532562256 = 0.6138172149658203 + 1.0 * 6.4321513175964355
Epoch 390, val loss: 0.9275662899017334
Epoch 400, training loss: 7.011711120605469 = 0.587405800819397 + 1.0 * 6.424305438995361
Epoch 400, val loss: 0.9151583909988403
Epoch 410, training loss: 6.981227874755859 = 0.5620009899139404 + 1.0 * 6.41922664642334
Epoch 410, val loss: 0.9036728143692017
Epoch 420, training loss: 6.953272819519043 = 0.5373439192771912 + 1.0 * 6.415928840637207
Epoch 420, val loss: 0.8932610750198364
Epoch 430, training loss: 6.925607681274414 = 0.5132532119750977 + 1.0 * 6.412354469299316
Epoch 430, val loss: 0.8836772441864014
Epoch 440, training loss: 6.902602672576904 = 0.4896754026412964 + 1.0 * 6.412927150726318
Epoch 440, val loss: 0.8749898672103882
Epoch 450, training loss: 6.877671241760254 = 0.4668594300746918 + 1.0 * 6.410811901092529
Epoch 450, val loss: 0.8672700524330139
Epoch 460, training loss: 6.8493971824646 = 0.4444512128829956 + 1.0 * 6.4049458503723145
Epoch 460, val loss: 0.860228955745697
Epoch 470, training loss: 6.823017120361328 = 0.4223209321498871 + 1.0 * 6.400696277618408
Epoch 470, val loss: 0.8540052175521851
Epoch 480, training loss: 6.8110222816467285 = 0.40059223771095276 + 1.0 * 6.410429954528809
Epoch 480, val loss: 0.8485874533653259
Epoch 490, training loss: 6.779075622558594 = 0.3794603943824768 + 1.0 * 6.399615287780762
Epoch 490, val loss: 0.8441656231880188
Epoch 500, training loss: 6.7536187171936035 = 0.3590126633644104 + 1.0 * 6.394606113433838
Epoch 500, val loss: 0.840709388256073
Epoch 510, training loss: 6.730847358703613 = 0.3392271101474762 + 1.0 * 6.39162015914917
Epoch 510, val loss: 0.8381627798080444
Epoch 520, training loss: 6.718038082122803 = 0.32026731967926025 + 1.0 * 6.397770881652832
Epoch 520, val loss: 0.8365132808685303
Epoch 530, training loss: 6.688493251800537 = 0.3023664653301239 + 1.0 * 6.38612699508667
Epoch 530, val loss: 0.8359237909317017
Epoch 540, training loss: 6.6703691482543945 = 0.28543752431869507 + 1.0 * 6.384931564331055
Epoch 540, val loss: 0.836209774017334
Epoch 550, training loss: 6.655365467071533 = 0.2694304883480072 + 1.0 * 6.385934829711914
Epoch 550, val loss: 0.8371939659118652
Epoch 560, training loss: 6.636859893798828 = 0.25446099042892456 + 1.0 * 6.382399082183838
Epoch 560, val loss: 0.838762104511261
Epoch 570, training loss: 6.617257118225098 = 0.2404341846704483 + 1.0 * 6.3768229484558105
Epoch 570, val loss: 0.8411320447921753
Epoch 580, training loss: 6.602125644683838 = 0.22722671926021576 + 1.0 * 6.374898910522461
Epoch 580, val loss: 0.8438072800636292
Epoch 590, training loss: 6.605319499969482 = 0.21480108797550201 + 1.0 * 6.3905181884765625
Epoch 590, val loss: 0.8467870354652405
Epoch 600, training loss: 6.577237129211426 = 0.2032630294561386 + 1.0 * 6.373974323272705
Epoch 600, val loss: 0.8503956198692322
Epoch 610, training loss: 6.561142444610596 = 0.192436084151268 + 1.0 * 6.368706226348877
Epoch 610, val loss: 0.8542143702507019
Epoch 620, training loss: 6.54885721206665 = 0.18221086263656616 + 1.0 * 6.3666462898254395
Epoch 620, val loss: 0.8581846356391907
Epoch 630, training loss: 6.536677360534668 = 0.17253661155700684 + 1.0 * 6.36414098739624
Epoch 630, val loss: 0.8625744581222534
Epoch 640, training loss: 6.526444911956787 = 0.16337864100933075 + 1.0 * 6.36306619644165
Epoch 640, val loss: 0.8671140074729919
Epoch 650, training loss: 6.531959056854248 = 0.15477094054222107 + 1.0 * 6.377188205718994
Epoch 650, val loss: 0.8714808225631714
Epoch 660, training loss: 6.505159854888916 = 0.14672335982322693 + 1.0 * 6.358436584472656
Epoch 660, val loss: 0.8765443563461304
Epoch 670, training loss: 6.497525215148926 = 0.13914135098457336 + 1.0 * 6.358383655548096
Epoch 670, val loss: 0.8813913464546204
Epoch 680, training loss: 6.486903667449951 = 0.13196782767772675 + 1.0 * 6.354935646057129
Epoch 680, val loss: 0.8864649534225464
Epoch 690, training loss: 6.490007400512695 = 0.1252031922340393 + 1.0 * 6.364804267883301
Epoch 690, val loss: 0.8914851546287537
Epoch 700, training loss: 6.47351598739624 = 0.11884432286024094 + 1.0 * 6.354671478271484
Epoch 700, val loss: 0.8967559337615967
Epoch 710, training loss: 6.464130401611328 = 0.11286633461713791 + 1.0 * 6.351263999938965
Epoch 710, val loss: 0.9022237658500671
Epoch 720, training loss: 6.456121444702148 = 0.10722634196281433 + 1.0 * 6.348895072937012
Epoch 720, val loss: 0.9076298475265503
Epoch 730, training loss: 6.462223529815674 = 0.10189696401357651 + 1.0 * 6.360326766967773
Epoch 730, val loss: 0.9130959510803223
Epoch 740, training loss: 6.445798873901367 = 0.09690790623426437 + 1.0 * 6.348890781402588
Epoch 740, val loss: 0.9186545014381409
Epoch 750, training loss: 6.436647415161133 = 0.0921921655535698 + 1.0 * 6.344455242156982
Epoch 750, val loss: 0.9244139790534973
Epoch 760, training loss: 6.430315971374512 = 0.08774769306182861 + 1.0 * 6.342568397521973
Epoch 760, val loss: 0.9300482869148254
Epoch 770, training loss: 6.431122779846191 = 0.08355166763067245 + 1.0 * 6.347570896148682
Epoch 770, val loss: 0.9357928037643433
Epoch 780, training loss: 6.4246320724487305 = 0.07959716022014618 + 1.0 * 6.345035076141357
Epoch 780, val loss: 0.9415224194526672
Epoch 790, training loss: 6.4182820320129395 = 0.07588687539100647 + 1.0 * 6.342395305633545
Epoch 790, val loss: 0.9473311305046082
Epoch 800, training loss: 6.410646438598633 = 0.07240808755159378 + 1.0 * 6.33823823928833
Epoch 800, val loss: 0.9532127380371094
Epoch 810, training loss: 6.405314922332764 = 0.0691196396946907 + 1.0 * 6.336195468902588
Epoch 810, val loss: 0.9590216875076294
Epoch 820, training loss: 6.4035162925720215 = 0.06601952761411667 + 1.0 * 6.337496757507324
Epoch 820, val loss: 0.9649149775505066
Epoch 830, training loss: 6.400684833526611 = 0.06309416890144348 + 1.0 * 6.33759069442749
Epoch 830, val loss: 0.9705386757850647
Epoch 840, training loss: 6.394683837890625 = 0.06034348905086517 + 1.0 * 6.334340572357178
Epoch 840, val loss: 0.976584792137146
Epoch 850, training loss: 6.3881707191467285 = 0.05775228515267372 + 1.0 * 6.330418586730957
Epoch 850, val loss: 0.9824889898300171
Epoch 860, training loss: 6.3962016105651855 = 0.055305611342191696 + 1.0 * 6.340896129608154
Epoch 860, val loss: 0.9881802201271057
Epoch 870, training loss: 6.385101318359375 = 0.05299217253923416 + 1.0 * 6.332108974456787
Epoch 870, val loss: 0.9938595294952393
Epoch 880, training loss: 6.379843235015869 = 0.050819769501686096 + 1.0 * 6.329023361206055
Epoch 880, val loss: 0.9998834729194641
Epoch 890, training loss: 6.379055023193359 = 0.048762235790491104 + 1.0 * 6.330292701721191
Epoch 890, val loss: 1.0053728818893433
Epoch 900, training loss: 6.370827674865723 = 0.046828389167785645 + 1.0 * 6.323999404907227
Epoch 900, val loss: 1.011112093925476
Epoch 910, training loss: 6.3691487312316895 = 0.04499147832393646 + 1.0 * 6.324157238006592
Epoch 910, val loss: 1.0168919563293457
Epoch 920, training loss: 6.374187469482422 = 0.04325367510318756 + 1.0 * 6.330933570861816
Epoch 920, val loss: 1.0223143100738525
Epoch 930, training loss: 6.36476469039917 = 0.04160071536898613 + 1.0 * 6.323163986206055
Epoch 930, val loss: 1.0280237197875977
Epoch 940, training loss: 6.358980178833008 = 0.04004248231649399 + 1.0 * 6.3189377784729
Epoch 940, val loss: 1.0337072610855103
Epoch 950, training loss: 6.358545303344727 = 0.038558781147003174 + 1.0 * 6.319986343383789
Epoch 950, val loss: 1.0391072034835815
Epoch 960, training loss: 6.3654584884643555 = 0.03714625537395477 + 1.0 * 6.328312397003174
Epoch 960, val loss: 1.0445033311843872
Epoch 970, training loss: 6.3552680015563965 = 0.035815272480249405 + 1.0 * 6.31945276260376
Epoch 970, val loss: 1.0498137474060059
Epoch 980, training loss: 6.349883079528809 = 0.0345504954457283 + 1.0 * 6.315332412719727
Epoch 980, val loss: 1.0553568601608276
Epoch 990, training loss: 6.347919940948486 = 0.033346906304359436 + 1.0 * 6.314572811126709
Epoch 990, val loss: 1.0607070922851562
Epoch 1000, training loss: 6.348079204559326 = 0.03220184147357941 + 1.0 * 6.315877437591553
Epoch 1000, val loss: 1.0658128261566162
Epoch 1010, training loss: 6.34519624710083 = 0.031109998002648354 + 1.0 * 6.314086437225342
Epoch 1010, val loss: 1.0709376335144043
Epoch 1020, training loss: 6.344156742095947 = 0.030074378475546837 + 1.0 * 6.314082145690918
Epoch 1020, val loss: 1.076216220855713
Epoch 1030, training loss: 6.338405609130859 = 0.02909095399081707 + 1.0 * 6.309314727783203
Epoch 1030, val loss: 1.0813448429107666
Epoch 1040, training loss: 6.336307525634766 = 0.02815079875290394 + 1.0 * 6.308156490325928
Epoch 1040, val loss: 1.0864685773849487
Epoch 1050, training loss: 6.344857215881348 = 0.02725054696202278 + 1.0 * 6.317606449127197
Epoch 1050, val loss: 1.091403841972351
Epoch 1060, training loss: 6.338368892669678 = 0.026394857093691826 + 1.0 * 6.311974048614502
Epoch 1060, val loss: 1.0958833694458008
Epoch 1070, training loss: 6.330753326416016 = 0.025581318885087967 + 1.0 * 6.305171966552734
Epoch 1070, val loss: 1.1011687517166138
Epoch 1080, training loss: 6.329158306121826 = 0.02480378933250904 + 1.0 * 6.304354667663574
Epoch 1080, val loss: 1.1061065196990967
Epoch 1090, training loss: 6.327813148498535 = 0.024057593196630478 + 1.0 * 6.303755760192871
Epoch 1090, val loss: 1.1108161211013794
Epoch 1100, training loss: 6.337596416473389 = 0.023340458050370216 + 1.0 * 6.314256191253662
Epoch 1100, val loss: 1.1153820753097534
Epoch 1110, training loss: 6.325464248657227 = 0.022660091519355774 + 1.0 * 6.302803993225098
Epoch 1110, val loss: 1.1198575496673584
Epoch 1120, training loss: 6.328569412231445 = 0.022008748725056648 + 1.0 * 6.306560516357422
Epoch 1120, val loss: 1.1248103380203247
Epoch 1130, training loss: 6.3221001625061035 = 0.021382508799433708 + 1.0 * 6.300717830657959
Epoch 1130, val loss: 1.1291605234146118
Epoch 1140, training loss: 6.320468425750732 = 0.020784329622983932 + 1.0 * 6.299684047698975
Epoch 1140, val loss: 1.1338170766830444
Epoch 1150, training loss: 6.321042537689209 = 0.020210523158311844 + 1.0 * 6.3008317947387695
Epoch 1150, val loss: 1.1382564306259155
Epoch 1160, training loss: 6.321141242980957 = 0.019657578319311142 + 1.0 * 6.301483631134033
Epoch 1160, val loss: 1.1424839496612549
Epoch 1170, training loss: 6.31843376159668 = 0.019128525629639626 + 1.0 * 6.299305438995361
Epoch 1170, val loss: 1.1468545198440552
Epoch 1180, training loss: 6.324090003967285 = 0.018622344359755516 + 1.0 * 6.30546760559082
Epoch 1180, val loss: 1.1511874198913574
Epoch 1190, training loss: 6.316712856292725 = 0.01813516765832901 + 1.0 * 6.298577785491943
Epoch 1190, val loss: 1.1552830934524536
Epoch 1200, training loss: 6.313304424285889 = 0.017667457461357117 + 1.0 * 6.295637130737305
Epoch 1200, val loss: 1.1597177982330322
Epoch 1210, training loss: 6.3152852058410645 = 0.017217637971043587 + 1.0 * 6.298067569732666
Epoch 1210, val loss: 1.1638039350509644
Epoch 1220, training loss: 6.310533046722412 = 0.016781674697995186 + 1.0 * 6.293751239776611
Epoch 1220, val loss: 1.167495608329773
Epoch 1230, training loss: 6.309455871582031 = 0.01636575162410736 + 1.0 * 6.293090343475342
Epoch 1230, val loss: 1.171693205833435
Epoch 1240, training loss: 6.308312892913818 = 0.01596578024327755 + 1.0 * 6.292346954345703
Epoch 1240, val loss: 1.176012396812439
Epoch 1250, training loss: 6.306760787963867 = 0.015579413622617722 + 1.0 * 6.291181564331055
Epoch 1250, val loss: 1.1799311637878418
Epoch 1260, training loss: 6.315740585327148 = 0.01520517747849226 + 1.0 * 6.300535202026367
Epoch 1260, val loss: 1.18377685546875
Epoch 1270, training loss: 6.310581684112549 = 0.014845889061689377 + 1.0 * 6.295735836029053
Epoch 1270, val loss: 1.1873074769973755
Epoch 1280, training loss: 6.307216167449951 = 0.014499930664896965 + 1.0 * 6.292716026306152
Epoch 1280, val loss: 1.191412329673767
Epoch 1290, training loss: 6.30638313293457 = 0.014166832901537418 + 1.0 * 6.2922163009643555
Epoch 1290, val loss: 1.1950230598449707
Epoch 1300, training loss: 6.303439617156982 = 0.013847428373992443 + 1.0 * 6.289592266082764
Epoch 1300, val loss: 1.198747992515564
Epoch 1310, training loss: 6.303712368011475 = 0.013537845574319363 + 1.0 * 6.29017448425293
Epoch 1310, val loss: 1.2025898694992065
Epoch 1320, training loss: 6.304333209991455 = 0.01323863584548235 + 1.0 * 6.291094779968262
Epoch 1320, val loss: 1.2061322927474976
Epoch 1330, training loss: 6.304955005645752 = 0.012948707677423954 + 1.0 * 6.292006492614746
Epoch 1330, val loss: 1.2095789909362793
Epoch 1340, training loss: 6.299442291259766 = 0.012668435461819172 + 1.0 * 6.286773681640625
Epoch 1340, val loss: 1.2132208347320557
Epoch 1350, training loss: 6.298158645629883 = 0.01239862572401762 + 1.0 * 6.285759925842285
Epoch 1350, val loss: 1.216863751411438
Epoch 1360, training loss: 6.302066326141357 = 0.0121369119733572 + 1.0 * 6.289929389953613
Epoch 1360, val loss: 1.220391035079956
Epoch 1370, training loss: 6.298522472381592 = 0.0118830231949687 + 1.0 * 6.286639213562012
Epoch 1370, val loss: 1.2235946655273438
Epoch 1380, training loss: 6.296578884124756 = 0.011638958938419819 + 1.0 * 6.284939765930176
Epoch 1380, val loss: 1.2270690202713013
Epoch 1390, training loss: 6.29600715637207 = 0.01140205841511488 + 1.0 * 6.284605026245117
Epoch 1390, val loss: 1.2306121587753296
Epoch 1400, training loss: 6.303633689880371 = 0.011171330697834492 + 1.0 * 6.292462348937988
Epoch 1400, val loss: 1.2334522008895874
Epoch 1410, training loss: 6.295279502868652 = 0.010951510630548 + 1.0 * 6.284327983856201
Epoch 1410, val loss: 1.2365269660949707
Epoch 1420, training loss: 6.29504919052124 = 0.010739585384726524 + 1.0 * 6.284309387207031
Epoch 1420, val loss: 1.2402527332305908
Epoch 1430, training loss: 6.292098522186279 = 0.010532638058066368 + 1.0 * 6.2815656661987305
Epoch 1430, val loss: 1.2436294555664062
Epoch 1440, training loss: 6.2913641929626465 = 0.010330883786082268 + 1.0 * 6.281033515930176
Epoch 1440, val loss: 1.2467315196990967
Epoch 1450, training loss: 6.29875373840332 = 0.010134860873222351 + 1.0 * 6.288619041442871
Epoch 1450, val loss: 1.249803900718689
Epoch 1460, training loss: 6.2932610511779785 = 0.00994274951517582 + 1.0 * 6.283318519592285
Epoch 1460, val loss: 1.252428650856018
Epoch 1470, training loss: 6.292550563812256 = 0.009759698994457722 + 1.0 * 6.282790660858154
Epoch 1470, val loss: 1.2557393312454224
Epoch 1480, training loss: 6.290978908538818 = 0.009580448269844055 + 1.0 * 6.281398296356201
Epoch 1480, val loss: 1.2590080499649048
Epoch 1490, training loss: 6.294734954833984 = 0.009406578727066517 + 1.0 * 6.285328388214111
Epoch 1490, val loss: 1.2618210315704346
Epoch 1500, training loss: 6.290093421936035 = 0.009236976504325867 + 1.0 * 6.280856609344482
Epoch 1500, val loss: 1.2648307085037231
Epoch 1510, training loss: 6.289183139801025 = 0.009072785265743732 + 1.0 * 6.2801103591918945
Epoch 1510, val loss: 1.2678568363189697
Epoch 1520, training loss: 6.298454761505127 = 0.008912088349461555 + 1.0 * 6.2895426750183105
Epoch 1520, val loss: 1.2706137895584106
Epoch 1530, training loss: 6.287588119506836 = 0.008758636191487312 + 1.0 * 6.278829574584961
Epoch 1530, val loss: 1.2731387615203857
Epoch 1540, training loss: 6.286747455596924 = 0.008608982898294926 + 1.0 * 6.278138637542725
Epoch 1540, val loss: 1.276323914527893
Epoch 1550, training loss: 6.285988807678223 = 0.008463424630463123 + 1.0 * 6.277525424957275
Epoch 1550, val loss: 1.2793182134628296
Epoch 1560, training loss: 6.290909767150879 = 0.008321234956383705 + 1.0 * 6.282588481903076
Epoch 1560, val loss: 1.2820525169372559
Epoch 1570, training loss: 6.286903381347656 = 0.008182156831026077 + 1.0 * 6.278721332550049
Epoch 1570, val loss: 1.2843589782714844
Epoch 1580, training loss: 6.283496856689453 = 0.008047114126384258 + 1.0 * 6.275449752807617
Epoch 1580, val loss: 1.287221074104309
Epoch 1590, training loss: 6.283348083496094 = 0.007915990427136421 + 1.0 * 6.275432109832764
Epoch 1590, val loss: 1.2901527881622314
Epoch 1600, training loss: 6.2869720458984375 = 0.007787786424160004 + 1.0 * 6.279184341430664
Epoch 1600, val loss: 1.2928799390792847
Epoch 1610, training loss: 6.287502288818359 = 0.007662440650165081 + 1.0 * 6.279839992523193
Epoch 1610, val loss: 1.2952454090118408
Epoch 1620, training loss: 6.282951831817627 = 0.007541187573224306 + 1.0 * 6.2754106521606445
Epoch 1620, val loss: 1.2975857257843018
Epoch 1630, training loss: 6.282855033874512 = 0.007424195762723684 + 1.0 * 6.275430679321289
Epoch 1630, val loss: 1.3004618883132935
Epoch 1640, training loss: 6.28438138961792 = 0.0073089092038571835 + 1.0 * 6.277072429656982
Epoch 1640, val loss: 1.3031452894210815
Epoch 1650, training loss: 6.2825751304626465 = 0.0071970089338719845 + 1.0 * 6.275378227233887
Epoch 1650, val loss: 1.305511713027954
Epoch 1660, training loss: 6.280917167663574 = 0.007088419049978256 + 1.0 * 6.273828983306885
Epoch 1660, val loss: 1.3080066442489624
Epoch 1670, training loss: 6.2795796394348145 = 0.006981717422604561 + 1.0 * 6.272597789764404
Epoch 1670, val loss: 1.310576319694519
Epoch 1680, training loss: 6.284029483795166 = 0.006876748986542225 + 1.0 * 6.2771525382995605
Epoch 1680, val loss: 1.3131204843521118
Epoch 1690, training loss: 6.283839225769043 = 0.006774097681045532 + 1.0 * 6.277065277099609
Epoch 1690, val loss: 1.3150527477264404
Epoch 1700, training loss: 6.281208515167236 = 0.006675986107438803 + 1.0 * 6.274532318115234
Epoch 1700, val loss: 1.3174495697021484
Epoch 1710, training loss: 6.280384540557861 = 0.0065794773399829865 + 1.0 * 6.273805141448975
Epoch 1710, val loss: 1.3200132846832275
Epoch 1720, training loss: 6.279482841491699 = 0.006485356483608484 + 1.0 * 6.2729973793029785
Epoch 1720, val loss: 1.3225129842758179
Epoch 1730, training loss: 6.27877950668335 = 0.006393123418092728 + 1.0 * 6.27238655090332
Epoch 1730, val loss: 1.324724555015564
Epoch 1740, training loss: 6.2789177894592285 = 0.0063028475269675255 + 1.0 * 6.2726149559021
Epoch 1740, val loss: 1.3269743919372559
Epoch 1750, training loss: 6.28069543838501 = 0.006215234752744436 + 1.0 * 6.27448034286499
Epoch 1750, val loss: 1.3291852474212646
Epoch 1760, training loss: 6.276800155639648 = 0.006129147484898567 + 1.0 * 6.2706708908081055
Epoch 1760, val loss: 1.3312759399414062
Epoch 1770, training loss: 6.275579452514648 = 0.006045229732990265 + 1.0 * 6.269534111022949
Epoch 1770, val loss: 1.3336623907089233
Epoch 1780, training loss: 6.279582500457764 = 0.005963156931102276 + 1.0 * 6.273619174957275
Epoch 1780, val loss: 1.3359326124191284
Epoch 1790, training loss: 6.278336524963379 = 0.005882554221898317 + 1.0 * 6.272453784942627
Epoch 1790, val loss: 1.338079571723938
Epoch 1800, training loss: 6.275424480438232 = 0.0058040800504386425 + 1.0 * 6.269620418548584
Epoch 1800, val loss: 1.3400063514709473
Epoch 1810, training loss: 6.273085594177246 = 0.005728475749492645 + 1.0 * 6.267357349395752
Epoch 1810, val loss: 1.342362880706787
Epoch 1820, training loss: 6.274432182312012 = 0.005653844214975834 + 1.0 * 6.268778324127197
Epoch 1820, val loss: 1.3446242809295654
Epoch 1830, training loss: 6.277729511260986 = 0.005580189172178507 + 1.0 * 6.272149085998535
Epoch 1830, val loss: 1.3465673923492432
Epoch 1840, training loss: 6.277932167053223 = 0.005507653579115868 + 1.0 * 6.272424697875977
Epoch 1840, val loss: 1.3485482931137085
Epoch 1850, training loss: 6.274253845214844 = 0.005437536630779505 + 1.0 * 6.2688164710998535
Epoch 1850, val loss: 1.350461721420288
Epoch 1860, training loss: 6.27239990234375 = 0.005368737503886223 + 1.0 * 6.267031192779541
Epoch 1860, val loss: 1.3526434898376465
Epoch 1870, training loss: 6.271095275878906 = 0.005301687866449356 + 1.0 * 6.265793800354004
Epoch 1870, val loss: 1.3548529148101807
Epoch 1880, training loss: 6.2735395431518555 = 0.005235398653894663 + 1.0 * 6.268304347991943
Epoch 1880, val loss: 1.3568965196609497
Epoch 1890, training loss: 6.274356365203857 = 0.005170084070414305 + 1.0 * 6.269186496734619
Epoch 1890, val loss: 1.358558177947998
Epoch 1900, training loss: 6.27752161026001 = 0.00510680815204978 + 1.0 * 6.272414684295654
Epoch 1900, val loss: 1.3603075742721558
Epoch 1910, training loss: 6.271327495574951 = 0.005045606754720211 + 1.0 * 6.266282081604004
Epoch 1910, val loss: 1.3624050617218018
Epoch 1920, training loss: 6.270523548126221 = 0.0049855392426252365 + 1.0 * 6.265538215637207
Epoch 1920, val loss: 1.3644969463348389
Epoch 1930, training loss: 6.2714972496032715 = 0.004926024936139584 + 1.0 * 6.266571044921875
Epoch 1930, val loss: 1.3663874864578247
Epoch 1940, training loss: 6.272611141204834 = 0.004867026582360268 + 1.0 * 6.267744064331055
Epoch 1940, val loss: 1.3681561946868896
Epoch 1950, training loss: 6.27073860168457 = 0.004809626378118992 + 1.0 * 6.265928745269775
Epoch 1950, val loss: 1.3698023557662964
Epoch 1960, training loss: 6.270519733428955 = 0.004754076711833477 + 1.0 * 6.26576566696167
Epoch 1960, val loss: 1.3715516328811646
Epoch 1970, training loss: 6.2677812576293945 = 0.004699911922216415 + 1.0 * 6.2630815505981445
Epoch 1970, val loss: 1.3735933303833008
Epoch 1980, training loss: 6.267347812652588 = 0.004646447487175465 + 1.0 * 6.262701511383057
Epoch 1980, val loss: 1.3755912780761719
Epoch 1990, training loss: 6.269169330596924 = 0.004593842662870884 + 1.0 * 6.264575481414795
Epoch 1990, val loss: 1.3774031400680542
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8202424881391671
The final CL Acc:0.77654, 0.01222, The final GNN Acc:0.81726, 0.00386
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13274])
remove edge: torch.Size([2, 7916])
updated graph: torch.Size([2, 10634])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.527292251586914 = 1.9304388761520386 + 1.0 * 8.596853256225586
Epoch 0, val loss: 1.9310182332992554
Epoch 10, training loss: 10.51741886138916 = 1.9208296537399292 + 1.0 * 8.596589088439941
Epoch 10, val loss: 1.9219855070114136
Epoch 20, training loss: 10.502711296081543 = 1.9087412357330322 + 1.0 * 8.59397029876709
Epoch 20, val loss: 1.910180926322937
Epoch 30, training loss: 10.464761734008789 = 1.8917948007583618 + 1.0 * 8.572966575622559
Epoch 30, val loss: 1.893393635749817
Epoch 40, training loss: 10.326415061950684 = 1.8696887493133545 + 1.0 * 8.45672607421875
Epoch 40, val loss: 1.8722528219223022
Epoch 50, training loss: 9.908682823181152 = 1.8446956872940063 + 1.0 * 8.063986778259277
Epoch 50, val loss: 1.8487753868103027
Epoch 60, training loss: 9.642279624938965 = 1.8213468790054321 + 1.0 * 7.820932388305664
Epoch 60, val loss: 1.8278430700302124
Epoch 70, training loss: 9.205278396606445 = 1.8034552335739136 + 1.0 * 7.401823043823242
Epoch 70, val loss: 1.8116109371185303
Epoch 80, training loss: 8.868294715881348 = 1.7901031970977783 + 1.0 * 7.078191757202148
Epoch 80, val loss: 1.7992584705352783
Epoch 90, training loss: 8.677687644958496 = 1.7761157751083374 + 1.0 * 6.901571750640869
Epoch 90, val loss: 1.7860164642333984
Epoch 100, training loss: 8.5490083694458 = 1.7586108446121216 + 1.0 * 6.7903971672058105
Epoch 100, val loss: 1.7704850435256958
Epoch 110, training loss: 8.465838432312012 = 1.739548921585083 + 1.0 * 6.726289749145508
Epoch 110, val loss: 1.7532659769058228
Epoch 120, training loss: 8.403205871582031 = 1.718150019645691 + 1.0 * 6.685055732727051
Epoch 120, val loss: 1.7330302000045776
Epoch 130, training loss: 8.344210624694824 = 1.6936180591583252 + 1.0 * 6.65059232711792
Epoch 130, val loss: 1.7099326848983765
Epoch 140, training loss: 8.282783508300781 = 1.6648085117340088 + 1.0 * 6.617974758148193
Epoch 140, val loss: 1.6834498643875122
Epoch 150, training loss: 8.21876335144043 = 1.6310442686080933 + 1.0 * 6.587718963623047
Epoch 150, val loss: 1.6531469821929932
Epoch 160, training loss: 8.157464981079102 = 1.5916022062301636 + 1.0 * 6.565863132476807
Epoch 160, val loss: 1.6181491613388062
Epoch 170, training loss: 8.094868659973145 = 1.5467571020126343 + 1.0 * 6.548111915588379
Epoch 170, val loss: 1.5787250995635986
Epoch 180, training loss: 8.029000282287598 = 1.4969947338104248 + 1.0 * 6.532005786895752
Epoch 180, val loss: 1.5352587699890137
Epoch 190, training loss: 7.9615068435668945 = 1.444018840789795 + 1.0 * 6.5174880027771
Epoch 190, val loss: 1.4898258447647095
Epoch 200, training loss: 7.896493911743164 = 1.3905879259109497 + 1.0 * 6.505906105041504
Epoch 200, val loss: 1.445050597190857
Epoch 210, training loss: 7.829647064208984 = 1.3379323482513428 + 1.0 * 6.491714954376221
Epoch 210, val loss: 1.4020029306411743
Epoch 220, training loss: 7.774184226989746 = 1.2865867614746094 + 1.0 * 6.487597465515137
Epoch 220, val loss: 1.3612898588180542
Epoch 230, training loss: 7.7083563804626465 = 1.237987995147705 + 1.0 * 6.470368385314941
Epoch 230, val loss: 1.3236373662948608
Epoch 240, training loss: 7.649759292602539 = 1.191150426864624 + 1.0 * 6.458609104156494
Epoch 240, val loss: 1.2882167100906372
Epoch 250, training loss: 7.598105430603027 = 1.1452662944793701 + 1.0 * 6.452839374542236
Epoch 250, val loss: 1.2541457414627075
Epoch 260, training loss: 7.543635845184326 = 1.1006358861923218 + 1.0 * 6.442999839782715
Epoch 260, val loss: 1.2212270498275757
Epoch 270, training loss: 7.487483978271484 = 1.0563567876815796 + 1.0 * 6.431127071380615
Epoch 270, val loss: 1.188679575920105
Epoch 280, training loss: 7.4363603591918945 = 1.011606216430664 + 1.0 * 6.4247541427612305
Epoch 280, val loss: 1.1558176279067993
Epoch 290, training loss: 7.390633583068848 = 0.9669135212898254 + 1.0 * 6.423719882965088
Epoch 290, val loss: 1.1230746507644653
Epoch 300, training loss: 7.335115432739258 = 0.9229581952095032 + 1.0 * 6.41215705871582
Epoch 300, val loss: 1.0909521579742432
Epoch 310, training loss: 7.285714626312256 = 0.879391074180603 + 1.0 * 6.406323432922363
Epoch 310, val loss: 1.059388279914856
Epoch 320, training loss: 7.244524002075195 = 0.8365625143051147 + 1.0 * 6.407961368560791
Epoch 320, val loss: 1.0286058187484741
Epoch 330, training loss: 7.194024562835693 = 0.7955279350280762 + 1.0 * 6.398496627807617
Epoch 330, val loss: 0.9994072914123535
Epoch 340, training loss: 7.15049934387207 = 0.7562816143035889 + 1.0 * 6.3942179679870605
Epoch 340, val loss: 0.9721440076828003
Epoch 350, training loss: 7.107855796813965 = 0.7187529802322388 + 1.0 * 6.389102935791016
Epoch 350, val loss: 0.9467123746871948
Epoch 360, training loss: 7.082261085510254 = 0.6830116510391235 + 1.0 * 6.39924955368042
Epoch 360, val loss: 0.9232128858566284
Epoch 370, training loss: 7.036637306213379 = 0.6497585773468018 + 1.0 * 6.386878967285156
Epoch 370, val loss: 0.9021866321563721
Epoch 380, training loss: 6.998294830322266 = 0.6185920238494873 + 1.0 * 6.379703044891357
Epoch 380, val loss: 0.8834536075592041
Epoch 390, training loss: 6.967811107635498 = 0.5892181396484375 + 1.0 * 6.3785929679870605
Epoch 390, val loss: 0.8666222095489502
Epoch 400, training loss: 6.941409587860107 = 0.561767041683197 + 1.0 * 6.379642486572266
Epoch 400, val loss: 0.8517283201217651
Epoch 410, training loss: 6.908208847045898 = 0.5360565781593323 + 1.0 * 6.372152328491211
Epoch 410, val loss: 0.8387851119041443
Epoch 420, training loss: 6.880126953125 = 0.5117602944374084 + 1.0 * 6.368366718292236
Epoch 420, val loss: 0.8273681998252869
Epoch 430, training loss: 6.864433288574219 = 0.4887044429779053 + 1.0 * 6.375729084014893
Epoch 430, val loss: 0.8174408078193665
Epoch 440, training loss: 6.830482006072998 = 0.46702614426612854 + 1.0 * 6.363455772399902
Epoch 440, val loss: 0.8089521527290344
Epoch 450, training loss: 6.80782413482666 = 0.44636070728302 + 1.0 * 6.36146354675293
Epoch 450, val loss: 0.8016397953033447
Epoch 460, training loss: 6.796517372131348 = 0.42661651968955994 + 1.0 * 6.369900703430176
Epoch 460, val loss: 0.7954167723655701
Epoch 470, training loss: 6.766323566436768 = 0.4078386723995209 + 1.0 * 6.358484745025635
Epoch 470, val loss: 0.790244460105896
Epoch 480, training loss: 6.744875431060791 = 0.38983261585235596 + 1.0 * 6.355042934417725
Epoch 480, val loss: 0.7859154343605042
Epoch 490, training loss: 6.728384494781494 = 0.37255674600601196 + 1.0 * 6.355827808380127
Epoch 490, val loss: 0.7824620604515076
Epoch 500, training loss: 6.708125114440918 = 0.35600486397743225 + 1.0 * 6.352120399475098
Epoch 500, val loss: 0.7797651290893555
Epoch 510, training loss: 6.689816474914551 = 0.3400629460811615 + 1.0 * 6.349753379821777
Epoch 510, val loss: 0.7777615189552307
Epoch 520, training loss: 6.676658630371094 = 0.3247174024581909 + 1.0 * 6.351941108703613
Epoch 520, val loss: 0.7765693068504333
Epoch 530, training loss: 6.658011436462402 = 0.31007319688796997 + 1.0 * 6.347938060760498
Epoch 530, val loss: 0.7759190797805786
Epoch 540, training loss: 6.640216827392578 = 0.2959948182106018 + 1.0 * 6.344222068786621
Epoch 540, val loss: 0.7758025527000427
Epoch 550, training loss: 6.629325866699219 = 0.2824212610721588 + 1.0 * 6.346904754638672
Epoch 550, val loss: 0.7764074802398682
Epoch 560, training loss: 6.610933303833008 = 0.2693975567817688 + 1.0 * 6.341535568237305
Epoch 560, val loss: 0.7776002287864685
Epoch 570, training loss: 6.5989813804626465 = 0.25687745213508606 + 1.0 * 6.342103958129883
Epoch 570, val loss: 0.7792862057685852
Epoch 580, training loss: 6.582749843597412 = 0.24488243460655212 + 1.0 * 6.337867259979248
Epoch 580, val loss: 0.781594455242157
Epoch 590, training loss: 6.571586608886719 = 0.23339331150054932 + 1.0 * 6.338193416595459
Epoch 590, val loss: 0.7843340635299683
Epoch 600, training loss: 6.5553879737854 = 0.22239290177822113 + 1.0 * 6.3329949378967285
Epoch 600, val loss: 0.7876216173171997
Epoch 610, training loss: 6.550014019012451 = 0.21184208989143372 + 1.0 * 6.33817195892334
Epoch 610, val loss: 0.7914626598358154
Epoch 620, training loss: 6.535894393920898 = 0.201838880777359 + 1.0 * 6.334055423736572
Epoch 620, val loss: 0.795809268951416
Epoch 630, training loss: 6.521506309509277 = 0.19236110150814056 + 1.0 * 6.329145431518555
Epoch 630, val loss: 0.8003202080726624
Epoch 640, training loss: 6.510313987731934 = 0.1833273470401764 + 1.0 * 6.326986789703369
Epoch 640, val loss: 0.805278480052948
Epoch 650, training loss: 6.4993767738342285 = 0.17468611896038055 + 1.0 * 6.324690818786621
Epoch 650, val loss: 0.8107401728630066
Epoch 660, training loss: 6.499090194702148 = 0.1664479523897171 + 1.0 * 6.332642078399658
Epoch 660, val loss: 0.8165588974952698
Epoch 670, training loss: 6.48629903793335 = 0.1586761474609375 + 1.0 * 6.327622890472412
Epoch 670, val loss: 0.8227135539054871
Epoch 680, training loss: 6.474226951599121 = 0.15130369365215302 + 1.0 * 6.322923183441162
Epoch 680, val loss: 0.8288754224777222
Epoch 690, training loss: 6.463889122009277 = 0.1443042904138565 + 1.0 * 6.319584846496582
Epoch 690, val loss: 0.8353984355926514
Epoch 700, training loss: 6.464676380157471 = 0.13765659928321838 + 1.0 * 6.327019691467285
Epoch 700, val loss: 0.8422962427139282
Epoch 710, training loss: 6.449029445648193 = 0.13139016926288605 + 1.0 * 6.317639350891113
Epoch 710, val loss: 0.849186897277832
Epoch 720, training loss: 6.442728519439697 = 0.12545236945152283 + 1.0 * 6.3172760009765625
Epoch 720, val loss: 0.8562398552894592
Epoch 730, training loss: 6.44163179397583 = 0.11980624496936798 + 1.0 * 6.3218255043029785
Epoch 730, val loss: 0.8635978698730469
Epoch 740, training loss: 6.430965423583984 = 0.11448011547327042 + 1.0 * 6.316485404968262
Epoch 740, val loss: 0.8710705637931824
Epoch 750, training loss: 6.423031806945801 = 0.10943123698234558 + 1.0 * 6.313600540161133
Epoch 750, val loss: 0.8785629868507385
Epoch 760, training loss: 6.416957378387451 = 0.10463748127222061 + 1.0 * 6.312319755554199
Epoch 760, val loss: 0.8862838745117188
Epoch 770, training loss: 6.413564682006836 = 0.10009384155273438 + 1.0 * 6.313470840454102
Epoch 770, val loss: 0.8941605091094971
Epoch 780, training loss: 6.406793117523193 = 0.09578053653240204 + 1.0 * 6.3110127449035645
Epoch 780, val loss: 0.9019421339035034
Epoch 790, training loss: 6.408549785614014 = 0.09170746058225632 + 1.0 * 6.316842555999756
Epoch 790, val loss: 0.9098984003067017
Epoch 800, training loss: 6.395310878753662 = 0.08785013109445572 + 1.0 * 6.307460784912109
Epoch 800, val loss: 0.9177438020706177
Epoch 810, training loss: 6.389429569244385 = 0.08418729901313782 + 1.0 * 6.30524206161499
Epoch 810, val loss: 0.9256064295768738
Epoch 820, training loss: 6.3853631019592285 = 0.08070044964551926 + 1.0 * 6.304662704467773
Epoch 820, val loss: 0.9336193799972534
Epoch 830, training loss: 6.393592834472656 = 0.07739593088626862 + 1.0 * 6.316196918487549
Epoch 830, val loss: 0.941739559173584
Epoch 840, training loss: 6.37637996673584 = 0.07427466660737991 + 1.0 * 6.30210542678833
Epoch 840, val loss: 0.9495818018913269
Epoch 850, training loss: 6.374142646789551 = 0.07131499797105789 + 1.0 * 6.302827835083008
Epoch 850, val loss: 0.9572657942771912
Epoch 860, training loss: 6.3731465339660645 = 0.0684993714094162 + 1.0 * 6.304646968841553
Epoch 860, val loss: 0.9651915431022644
Epoch 870, training loss: 6.366646766662598 = 0.06582384556531906 + 1.0 * 6.300822734832764
Epoch 870, val loss: 0.9731031656265259
Epoch 880, training loss: 6.361614227294922 = 0.0632895827293396 + 1.0 * 6.2983245849609375
Epoch 880, val loss: 0.980740487575531
Epoch 890, training loss: 6.358160972595215 = 0.060873113572597504 + 1.0 * 6.297287940979004
Epoch 890, val loss: 0.9884373545646667
Epoch 900, training loss: 6.369986057281494 = 0.05857185646891594 + 1.0 * 6.3114142417907715
Epoch 900, val loss: 0.9962369799613953
Epoch 910, training loss: 6.357213973999023 = 0.05639549344778061 + 1.0 * 6.30081844329834
Epoch 910, val loss: 1.0039048194885254
Epoch 920, training loss: 6.350561141967773 = 0.05433086305856705 + 1.0 * 6.296230316162109
Epoch 920, val loss: 1.0112009048461914
Epoch 930, training loss: 6.346022129058838 = 0.052359648048877716 + 1.0 * 6.2936625480651855
Epoch 930, val loss: 1.0186413526535034
Epoch 940, training loss: 6.344763278961182 = 0.05047732964158058 + 1.0 * 6.294285774230957
Epoch 940, val loss: 1.0261508226394653
Epoch 950, training loss: 6.342227935791016 = 0.04868771508336067 + 1.0 * 6.293540000915527
Epoch 950, val loss: 1.0337409973144531
Epoch 960, training loss: 6.348120212554932 = 0.04699009656906128 + 1.0 * 6.301130294799805
Epoch 960, val loss: 1.0410196781158447
Epoch 970, training loss: 6.337656497955322 = 0.04537288472056389 + 1.0 * 6.292283535003662
Epoch 970, val loss: 1.048142433166504
Epoch 980, training loss: 6.333759307861328 = 0.0438329353928566 + 1.0 * 6.289926528930664
Epoch 980, val loss: 1.055288553237915
Epoch 990, training loss: 6.337199687957764 = 0.0423574298620224 + 1.0 * 6.29484224319458
Epoch 990, val loss: 1.0624871253967285
Epoch 1000, training loss: 6.3343825340271 = 0.0409538559615612 + 1.0 * 6.293428897857666
Epoch 1000, val loss: 1.0696792602539062
Epoch 1010, training loss: 6.3293938636779785 = 0.03962179645895958 + 1.0 * 6.289772033691406
Epoch 1010, val loss: 1.0764344930648804
Epoch 1020, training loss: 6.325235843658447 = 0.03834577277302742 + 1.0 * 6.286890029907227
Epoch 1020, val loss: 1.0832244157791138
Epoch 1030, training loss: 6.338055610656738 = 0.037125930190086365 + 1.0 * 6.300929546356201
Epoch 1030, val loss: 1.0901473760604858
Epoch 1040, training loss: 6.327150344848633 = 0.035956934094429016 + 1.0 * 6.29119348526001
Epoch 1040, val loss: 1.0969539880752563
Epoch 1050, training loss: 6.320992946624756 = 0.034847427159547806 + 1.0 * 6.2861456871032715
Epoch 1050, val loss: 1.1034375429153442
Epoch 1060, training loss: 6.318308353424072 = 0.03377925604581833 + 1.0 * 6.284529209136963
Epoch 1060, val loss: 1.1100493669509888
Epoch 1070, training loss: 6.331707000732422 = 0.03275790065526962 + 1.0 * 6.298949241638184
Epoch 1070, val loss: 1.1168164014816284
Epoch 1080, training loss: 6.3184919357299805 = 0.0317780077457428 + 1.0 * 6.28671407699585
Epoch 1080, val loss: 1.1233183145523071
Epoch 1090, training loss: 6.312267780303955 = 0.03084631636738777 + 1.0 * 6.281421661376953
Epoch 1090, val loss: 1.129530429840088
Epoch 1100, training loss: 6.311625003814697 = 0.029948243871331215 + 1.0 * 6.281676769256592
Epoch 1100, val loss: 1.1358730792999268
Epoch 1110, training loss: 6.327842712402344 = 0.029086915776133537 + 1.0 * 6.298755645751953
Epoch 1110, val loss: 1.142287015914917
Epoch 1120, training loss: 6.313561916351318 = 0.02826155349612236 + 1.0 * 6.285300254821777
Epoch 1120, val loss: 1.1485552787780762
Epoch 1130, training loss: 6.307753086090088 = 0.027473436668515205 + 1.0 * 6.280279636383057
Epoch 1130, val loss: 1.1544950008392334
Epoch 1140, training loss: 6.308698654174805 = 0.026711681857705116 + 1.0 * 6.281987190246582
Epoch 1140, val loss: 1.1605737209320068
Epoch 1150, training loss: 6.305385589599609 = 0.025983478873968124 + 1.0 * 6.279402256011963
Epoch 1150, val loss: 1.1667959690093994
Epoch 1160, training loss: 6.302947998046875 = 0.02528444118797779 + 1.0 * 6.277663707733154
Epoch 1160, val loss: 1.1725956201553345
Epoch 1170, training loss: 6.301775932312012 = 0.02461191639304161 + 1.0 * 6.277163982391357
Epoch 1170, val loss: 1.1784175634384155
Epoch 1180, training loss: 6.302160739898682 = 0.023962752893567085 + 1.0 * 6.278197765350342
Epoch 1180, val loss: 1.1843003034591675
Epoch 1190, training loss: 6.302605152130127 = 0.02333899959921837 + 1.0 * 6.279266357421875
Epoch 1190, val loss: 1.1901980638504028
Epoch 1200, training loss: 6.298989772796631 = 0.02273886650800705 + 1.0 * 6.276250839233398
Epoch 1200, val loss: 1.1958327293395996
Epoch 1210, training loss: 6.302074909210205 = 0.022162508219480515 + 1.0 * 6.27991247177124
Epoch 1210, val loss: 1.2014038562774658
Epoch 1220, training loss: 6.299448490142822 = 0.021607453003525734 + 1.0 * 6.277841091156006
Epoch 1220, val loss: 1.2070958614349365
Epoch 1230, training loss: 6.295773983001709 = 0.021075263619422913 + 1.0 * 6.274698734283447
Epoch 1230, val loss: 1.2125192880630493
Epoch 1240, training loss: 6.295924186706543 = 0.020560018718242645 + 1.0 * 6.275364398956299
Epoch 1240, val loss: 1.2178982496261597
Epoch 1250, training loss: 6.294260025024414 = 0.02006405219435692 + 1.0 * 6.274196147918701
Epoch 1250, val loss: 1.2234214544296265
Epoch 1260, training loss: 6.29237174987793 = 0.019586652517318726 + 1.0 * 6.272785186767578
Epoch 1260, val loss: 1.228737235069275
Epoch 1270, training loss: 6.308931827545166 = 0.019123394042253494 + 1.0 * 6.28980827331543
Epoch 1270, val loss: 1.2340233325958252
Epoch 1280, training loss: 6.2969136238098145 = 0.01868291199207306 + 1.0 * 6.278230667114258
Epoch 1280, val loss: 1.2392888069152832
Epoch 1290, training loss: 6.290858268737793 = 0.018254965543746948 + 1.0 * 6.272603511810303
Epoch 1290, val loss: 1.2442342042922974
Epoch 1300, training loss: 6.288627624511719 = 0.017841355875134468 + 1.0 * 6.270786285400391
Epoch 1300, val loss: 1.249237060546875
Epoch 1310, training loss: 6.291470527648926 = 0.01744025945663452 + 1.0 * 6.2740302085876465
Epoch 1310, val loss: 1.2543624639511108
Epoch 1320, training loss: 6.2864532470703125 = 0.017053546383976936 + 1.0 * 6.269399642944336
Epoch 1320, val loss: 1.2594510316848755
Epoch 1330, training loss: 6.287482261657715 = 0.01667974703013897 + 1.0 * 6.2708024978637695
Epoch 1330, val loss: 1.2643730640411377
Epoch 1340, training loss: 6.286365509033203 = 0.016316233202815056 + 1.0 * 6.270049095153809
Epoch 1340, val loss: 1.2693513631820679
Epoch 1350, training loss: 6.289068222045898 = 0.015965476632118225 + 1.0 * 6.273102760314941
Epoch 1350, val loss: 1.2743422985076904
Epoch 1360, training loss: 6.2881083488464355 = 0.01562763750553131 + 1.0 * 6.272480487823486
Epoch 1360, val loss: 1.2792065143585205
Epoch 1370, training loss: 6.283430576324463 = 0.015300438739359379 + 1.0 * 6.268130302429199
Epoch 1370, val loss: 1.2838737964630127
Epoch 1380, training loss: 6.282168865203857 = 0.014982784166932106 + 1.0 * 6.267186164855957
Epoch 1380, val loss: 1.288564682006836
Epoch 1390, training loss: 6.299622535705566 = 0.01467414665967226 + 1.0 * 6.284948348999023
Epoch 1390, val loss: 1.2933242321014404
Epoch 1400, training loss: 6.28096342086792 = 0.014379280619323254 + 1.0 * 6.2665839195251465
Epoch 1400, val loss: 1.298095941543579
Epoch 1410, training loss: 6.279677867889404 = 0.01409245003014803 + 1.0 * 6.265585422515869
Epoch 1410, val loss: 1.302513837814331
Epoch 1420, training loss: 6.278985500335693 = 0.013812152668833733 + 1.0 * 6.265173435211182
Epoch 1420, val loss: 1.307016134262085
Epoch 1430, training loss: 6.294341087341309 = 0.013539835810661316 + 1.0 * 6.280801296234131
Epoch 1430, val loss: 1.3116835355758667
Epoch 1440, training loss: 6.282796382904053 = 0.013280190527439117 + 1.0 * 6.2695159912109375
Epoch 1440, val loss: 1.316312551498413
Epoch 1450, training loss: 6.278242588043213 = 0.01302514411509037 + 1.0 * 6.265217304229736
Epoch 1450, val loss: 1.3205358982086182
Epoch 1460, training loss: 6.275694847106934 = 0.01277824118733406 + 1.0 * 6.262916564941406
Epoch 1460, val loss: 1.324872612953186
Epoch 1470, training loss: 6.288817405700684 = 0.012537157163023949 + 1.0 * 6.276280403137207
Epoch 1470, val loss: 1.32936429977417
Epoch 1480, training loss: 6.283518314361572 = 0.012305148877203465 + 1.0 * 6.271213054656982
Epoch 1480, val loss: 1.3338675498962402
Epoch 1490, training loss: 6.274555683135986 = 0.012080470100045204 + 1.0 * 6.26247501373291
Epoch 1490, val loss: 1.3379696607589722
Epoch 1500, training loss: 6.273460865020752 = 0.01186167448759079 + 1.0 * 6.261599063873291
Epoch 1500, val loss: 1.3420268297195435
Epoch 1510, training loss: 6.278867244720459 = 0.011646812781691551 + 1.0 * 6.267220497131348
Epoch 1510, val loss: 1.3462821245193481
Epoch 1520, training loss: 6.271936416625977 = 0.011439923197031021 + 1.0 * 6.260496616363525
Epoch 1520, val loss: 1.350607991218567
Epoch 1530, training loss: 6.271087169647217 = 0.011237984523177147 + 1.0 * 6.2598490715026855
Epoch 1530, val loss: 1.3546665906906128
Epoch 1540, training loss: 6.27224063873291 = 0.01104123704135418 + 1.0 * 6.261199474334717
Epoch 1540, val loss: 1.3587473630905151
Epoch 1550, training loss: 6.2747907638549805 = 0.010849501937627792 + 1.0 * 6.263941287994385
Epoch 1550, val loss: 1.3629271984100342
Epoch 1560, training loss: 6.271792888641357 = 0.01066481601446867 + 1.0 * 6.261127948760986
Epoch 1560, val loss: 1.366912603378296
Epoch 1570, training loss: 6.272648334503174 = 0.010484234429895878 + 1.0 * 6.262164115905762
Epoch 1570, val loss: 1.3707181215286255
Epoch 1580, training loss: 6.270287036895752 = 0.01030875276774168 + 1.0 * 6.259978294372559
Epoch 1580, val loss: 1.3746485710144043
Epoch 1590, training loss: 6.268452167510986 = 0.010136866942048073 + 1.0 * 6.258315086364746
Epoch 1590, val loss: 1.378509283065796
Epoch 1600, training loss: 6.272284507751465 = 0.009969186037778854 + 1.0 * 6.262315273284912
Epoch 1600, val loss: 1.3824131488800049
Epoch 1610, training loss: 6.268765449523926 = 0.009805954992771149 + 1.0 * 6.2589592933654785
Epoch 1610, val loss: 1.3863736391067505
Epoch 1620, training loss: 6.273294448852539 = 0.009647556580603123 + 1.0 * 6.263647079467773
Epoch 1620, val loss: 1.390146255493164
Epoch 1630, training loss: 6.270414352416992 = 0.009493489749729633 + 1.0 * 6.260921001434326
Epoch 1630, val loss: 1.3938947916030884
Epoch 1640, training loss: 6.267584800720215 = 0.009344027377665043 + 1.0 * 6.258240699768066
Epoch 1640, val loss: 1.3975815773010254
Epoch 1650, training loss: 6.2674150466918945 = 0.009197456762194633 + 1.0 * 6.258217811584473
Epoch 1650, val loss: 1.401236653327942
Epoch 1660, training loss: 6.268470764160156 = 0.009053468704223633 + 1.0 * 6.2594170570373535
Epoch 1660, val loss: 1.4049595594406128
Epoch 1670, training loss: 6.2680487632751465 = 0.008913225494325161 + 1.0 * 6.259135723114014
Epoch 1670, val loss: 1.408685326576233
Epoch 1680, training loss: 6.264828681945801 = 0.008776841685175896 + 1.0 * 6.256052017211914
Epoch 1680, val loss: 1.4123282432556152
Epoch 1690, training loss: 6.266825199127197 = 0.008644193410873413 + 1.0 * 6.258181095123291
Epoch 1690, val loss: 1.4158282279968262
Epoch 1700, training loss: 6.268167495727539 = 0.008514043875038624 + 1.0 * 6.259653568267822
Epoch 1700, val loss: 1.4194351434707642
Epoch 1710, training loss: 6.264979362487793 = 0.008387618698179722 + 1.0 * 6.256591796875
Epoch 1710, val loss: 1.4230204820632935
Epoch 1720, training loss: 6.273440361022949 = 0.008264051750302315 + 1.0 * 6.265176296234131
Epoch 1720, val loss: 1.426451325416565
Epoch 1730, training loss: 6.263068675994873 = 0.00814432930201292 + 1.0 * 6.254924297332764
Epoch 1730, val loss: 1.4299485683441162
Epoch 1740, training loss: 6.261303424835205 = 0.008027767762541771 + 1.0 * 6.2532758712768555
Epoch 1740, val loss: 1.4332001209259033
Epoch 1750, training loss: 6.261446475982666 = 0.007912609726190567 + 1.0 * 6.253533840179443
Epoch 1750, val loss: 1.4364936351776123
Epoch 1760, training loss: 6.261404037475586 = 0.007799818646162748 + 1.0 * 6.253604412078857
Epoch 1760, val loss: 1.4399025440216064
Epoch 1770, training loss: 6.271989822387695 = 0.007689045742154121 + 1.0 * 6.26430082321167
Epoch 1770, val loss: 1.4434081315994263
Epoch 1780, training loss: 6.26308012008667 = 0.0075829606503248215 + 1.0 * 6.255496978759766
Epoch 1780, val loss: 1.4468263387680054
Epoch 1790, training loss: 6.260941505432129 = 0.007478157989680767 + 1.0 * 6.253463268280029
Epoch 1790, val loss: 1.4499431848526
Epoch 1800, training loss: 6.264403343200684 = 0.007375724613666534 + 1.0 * 6.257027626037598
Epoch 1800, val loss: 1.453155517578125
Epoch 1810, training loss: 6.260997772216797 = 0.007274779491126537 + 1.0 * 6.25372314453125
Epoch 1810, val loss: 1.4564968347549438
Epoch 1820, training loss: 6.262772083282471 = 0.007176999002695084 + 1.0 * 6.2555952072143555
Epoch 1820, val loss: 1.4596762657165527
Epoch 1830, training loss: 6.259337902069092 = 0.007081998512148857 + 1.0 * 6.252255916595459
Epoch 1830, val loss: 1.4627982378005981
Epoch 1840, training loss: 6.257917404174805 = 0.006987960077822208 + 1.0 * 6.250929355621338
Epoch 1840, val loss: 1.4658927917480469
Epoch 1850, training loss: 6.26090669631958 = 0.006896277889609337 + 1.0 * 6.254010200500488
Epoch 1850, val loss: 1.4690132141113281
Epoch 1860, training loss: 6.260077476501465 = 0.006806131452322006 + 1.0 * 6.253271579742432
Epoch 1860, val loss: 1.4722075462341309
Epoch 1870, training loss: 6.263278484344482 = 0.006718248128890991 + 1.0 * 6.256560325622559
Epoch 1870, val loss: 1.47527277469635
Epoch 1880, training loss: 6.257007122039795 = 0.006632358767092228 + 1.0 * 6.250374794006348
Epoch 1880, val loss: 1.4783375263214111
Epoch 1890, training loss: 6.260878086090088 = 0.00654813414439559 + 1.0 * 6.254330158233643
Epoch 1890, val loss: 1.4813283681869507
Epoch 1900, training loss: 6.258449077606201 = 0.006465740967541933 + 1.0 * 6.251983165740967
Epoch 1900, val loss: 1.484238624572754
Epoch 1910, training loss: 6.256927013397217 = 0.00638478621840477 + 1.0 * 6.250542163848877
Epoch 1910, val loss: 1.4872056245803833
Epoch 1920, training loss: 6.2555975914001465 = 0.006306149531155825 + 1.0 * 6.24929141998291
Epoch 1920, val loss: 1.4900593757629395
Epoch 1930, training loss: 6.25801420211792 = 0.006228286772966385 + 1.0 * 6.251785755157471
Epoch 1930, val loss: 1.4929473400115967
Epoch 1940, training loss: 6.258522033691406 = 0.0061522312462329865 + 1.0 * 6.2523698806762695
Epoch 1940, val loss: 1.496002197265625
Epoch 1950, training loss: 6.25609016418457 = 0.006078687962144613 + 1.0 * 6.250011444091797
Epoch 1950, val loss: 1.498809814453125
Epoch 1960, training loss: 6.257040500640869 = 0.00600563595071435 + 1.0 * 6.251034736633301
Epoch 1960, val loss: 1.5015392303466797
Epoch 1970, training loss: 6.258440971374512 = 0.0059341187588870525 + 1.0 * 6.252506732940674
Epoch 1970, val loss: 1.504387617111206
Epoch 1980, training loss: 6.25413179397583 = 0.0058641014620661736 + 1.0 * 6.248267650604248
Epoch 1980, val loss: 1.5071847438812256
Epoch 1990, training loss: 6.253490924835205 = 0.005795760080218315 + 1.0 * 6.247694969177246
Epoch 1990, val loss: 1.5098285675048828
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.534200668334961 = 1.9373513460159302 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.930786371231079
Epoch 10, training loss: 10.524110794067383 = 1.9275058507919312 + 1.0 * 8.59660530090332
Epoch 10, val loss: 1.921351671218872
Epoch 20, training loss: 10.509456634521484 = 1.9151532649993896 + 1.0 * 8.594303131103516
Epoch 20, val loss: 1.9090793132781982
Epoch 30, training loss: 10.47232723236084 = 1.8978382349014282 + 1.0 * 8.574488639831543
Epoch 30, val loss: 1.8915855884552002
Epoch 40, training loss: 10.32861042022705 = 1.8747087717056274 + 1.0 * 8.453901290893555
Epoch 40, val loss: 1.8689810037612915
Epoch 50, training loss: 9.836677551269531 = 1.848479151725769 + 1.0 * 7.988198280334473
Epoch 50, val loss: 1.8438559770584106
Epoch 60, training loss: 9.590737342834473 = 1.823136806488037 + 1.0 * 7.7676005363464355
Epoch 60, val loss: 1.8205350637435913
Epoch 70, training loss: 9.2849702835083 = 1.8037993907928467 + 1.0 * 7.481171131134033
Epoch 70, val loss: 1.803411602973938
Epoch 80, training loss: 8.996294975280762 = 1.7919819355010986 + 1.0 * 7.204313278198242
Epoch 80, val loss: 1.7937320470809937
Epoch 90, training loss: 8.739557266235352 = 1.7829790115356445 + 1.0 * 6.956578254699707
Epoch 90, val loss: 1.7854641675949097
Epoch 100, training loss: 8.601000785827637 = 1.769395112991333 + 1.0 * 6.831605434417725
Epoch 100, val loss: 1.773587942123413
Epoch 110, training loss: 8.507723808288574 = 1.7526086568832397 + 1.0 * 6.755115032196045
Epoch 110, val loss: 1.758940577507019
Epoch 120, training loss: 8.429369926452637 = 1.734442949295044 + 1.0 * 6.694927215576172
Epoch 120, val loss: 1.7433937788009644
Epoch 130, training loss: 8.366629600524902 = 1.7145793437957764 + 1.0 * 6.652050018310547
Epoch 130, val loss: 1.726348876953125
Epoch 140, training loss: 8.309883117675781 = 1.6915650367736816 + 1.0 * 6.6183180809021
Epoch 140, val loss: 1.7067140340805054
Epoch 150, training loss: 8.2539701461792 = 1.6643837690353394 + 1.0 * 6.5895867347717285
Epoch 150, val loss: 1.683801531791687
Epoch 160, training loss: 8.198335647583008 = 1.6320972442626953 + 1.0 * 6.566238880157471
Epoch 160, val loss: 1.6569558382034302
Epoch 170, training loss: 8.138641357421875 = 1.5950736999511719 + 1.0 * 6.543568134307861
Epoch 170, val loss: 1.626365065574646
Epoch 180, training loss: 8.075934410095215 = 1.5529959201812744 + 1.0 * 6.522938251495361
Epoch 180, val loss: 1.591489315032959
Epoch 190, training loss: 8.018994331359863 = 1.5058478116989136 + 1.0 * 6.513146877288818
Epoch 190, val loss: 1.5524145364761353
Epoch 200, training loss: 7.949751853942871 = 1.45596182346344 + 1.0 * 6.493790149688721
Epoch 200, val loss: 1.5114820003509521
Epoch 210, training loss: 7.884415626525879 = 1.4040957689285278 + 1.0 * 6.480319976806641
Epoch 210, val loss: 1.468815565109253
Epoch 220, training loss: 7.819980144500732 = 1.3509860038757324 + 1.0 * 6.468994140625
Epoch 220, val loss: 1.4251487255096436
Epoch 230, training loss: 7.762147426605225 = 1.297683596611023 + 1.0 * 6.464463710784912
Epoch 230, val loss: 1.3816293478012085
Epoch 240, training loss: 7.698896884918213 = 1.2468596696853638 + 1.0 * 6.452037334442139
Epoch 240, val loss: 1.3406763076782227
Epoch 250, training loss: 7.641060829162598 = 1.1978803873062134 + 1.0 * 6.443180561065674
Epoch 250, val loss: 1.301466703414917
Epoch 260, training loss: 7.586243152618408 = 1.150276780128479 + 1.0 * 6.435966491699219
Epoch 260, val loss: 1.2634910345077515
Epoch 270, training loss: 7.535465240478516 = 1.1039563417434692 + 1.0 * 6.431509017944336
Epoch 270, val loss: 1.2267121076583862
Epoch 280, training loss: 7.483193874359131 = 1.0600568056106567 + 1.0 * 6.423137187957764
Epoch 280, val loss: 1.1921147108078003
Epoch 290, training loss: 7.437385559082031 = 1.0187146663665771 + 1.0 * 6.418671131134033
Epoch 290, val loss: 1.1598163843154907
Epoch 300, training loss: 7.3907647132873535 = 0.9793880581855774 + 1.0 * 6.411376476287842
Epoch 300, val loss: 1.1292040348052979
Epoch 310, training loss: 7.35182523727417 = 0.9418554902076721 + 1.0 * 6.409969806671143
Epoch 310, val loss: 1.0999990701675415
Epoch 320, training loss: 7.311056613922119 = 0.9068838357925415 + 1.0 * 6.404172897338867
Epoch 320, val loss: 1.0727508068084717
Epoch 330, training loss: 7.269429683685303 = 0.8738874197006226 + 1.0 * 6.395542144775391
Epoch 330, val loss: 1.047234058380127
Epoch 340, training loss: 7.232819557189941 = 0.8419638276100159 + 1.0 * 6.39085578918457
Epoch 340, val loss: 1.0226378440856934
Epoch 350, training loss: 7.196524620056152 = 0.8107088208198547 + 1.0 * 6.385815620422363
Epoch 350, val loss: 0.9986260533332825
Epoch 360, training loss: 7.1639814376831055 = 0.7801739573478699 + 1.0 * 6.38380765914917
Epoch 360, val loss: 0.9753623604774475
Epoch 370, training loss: 7.130369186401367 = 0.750701367855072 + 1.0 * 6.37966775894165
Epoch 370, val loss: 0.9530623555183411
Epoch 380, training loss: 7.096680164337158 = 0.7219820022583008 + 1.0 * 6.374698162078857
Epoch 380, val loss: 0.9315415620803833
Epoch 390, training loss: 7.068366050720215 = 0.6939159035682678 + 1.0 * 6.374450206756592
Epoch 390, val loss: 0.9108732342720032
Epoch 400, training loss: 7.035526275634766 = 0.6668366193771362 + 1.0 * 6.36868953704834
Epoch 400, val loss: 0.8912350535392761
Epoch 410, training loss: 7.006007194519043 = 0.6407414674758911 + 1.0 * 6.365265846252441
Epoch 410, val loss: 0.8727851510047913
Epoch 420, training loss: 6.978083610534668 = 0.6157490015029907 + 1.0 * 6.362334728240967
Epoch 420, val loss: 0.8556676506996155
Epoch 430, training loss: 6.953693866729736 = 0.5921531915664673 + 1.0 * 6.361540794372559
Epoch 430, val loss: 0.840067446231842
Epoch 440, training loss: 6.925800800323486 = 0.5696320533752441 + 1.0 * 6.356168746948242
Epoch 440, val loss: 0.8257583975791931
Epoch 450, training loss: 6.901390075683594 = 0.5480038523674011 + 1.0 * 6.353386402130127
Epoch 450, val loss: 0.8126837611198425
Epoch 460, training loss: 6.881558895111084 = 0.5271773338317871 + 1.0 * 6.354381561279297
Epoch 460, val loss: 0.8007807731628418
Epoch 470, training loss: 6.857801914215088 = 0.5072554349899292 + 1.0 * 6.350546360015869
Epoch 470, val loss: 0.7899453043937683
Epoch 480, training loss: 6.834705352783203 = 0.4878353774547577 + 1.0 * 6.346869945526123
Epoch 480, val loss: 0.7800003290176392
Epoch 490, training loss: 6.820879936218262 = 0.4688010513782501 + 1.0 * 6.352078914642334
Epoch 490, val loss: 0.770733654499054
Epoch 500, training loss: 6.794147491455078 = 0.4501769244670868 + 1.0 * 6.343970775604248
Epoch 500, val loss: 0.7622795701026917
Epoch 510, training loss: 6.771570682525635 = 0.4319448471069336 + 1.0 * 6.339625835418701
Epoch 510, val loss: 0.7542728781700134
Epoch 520, training loss: 6.750865459442139 = 0.4137780964374542 + 1.0 * 6.337087154388428
Epoch 520, val loss: 0.7466904520988464
Epoch 530, training loss: 6.731470584869385 = 0.3956514894962311 + 1.0 * 6.335819244384766
Epoch 530, val loss: 0.7394351363182068
Epoch 540, training loss: 6.714718818664551 = 0.3776809275150299 + 1.0 * 6.337038040161133
Epoch 540, val loss: 0.7326083183288574
Epoch 550, training loss: 6.69447660446167 = 0.3600233793258667 + 1.0 * 6.334453105926514
Epoch 550, val loss: 0.7260682582855225
Epoch 560, training loss: 6.681509017944336 = 0.34252381324768066 + 1.0 * 6.338985443115234
Epoch 560, val loss: 0.7197835445404053
Epoch 570, training loss: 6.657358169555664 = 0.32514092326164246 + 1.0 * 6.332217216491699
Epoch 570, val loss: 0.7138777375221252
Epoch 580, training loss: 6.636592864990234 = 0.3079303503036499 + 1.0 * 6.328662395477295
Epoch 580, val loss: 0.7080753445625305
Epoch 590, training loss: 6.623400688171387 = 0.2907264232635498 + 1.0 * 6.332674503326416
Epoch 590, val loss: 0.7025753259658813
Epoch 600, training loss: 6.603274345397949 = 0.2737487554550171 + 1.0 * 6.329525470733643
Epoch 600, val loss: 0.697277843952179
Epoch 610, training loss: 6.5820393562316895 = 0.2569744288921356 + 1.0 * 6.3250651359558105
Epoch 610, val loss: 0.6923277974128723
Epoch 620, training loss: 6.56556510925293 = 0.2404894083738327 + 1.0 * 6.325075626373291
Epoch 620, val loss: 0.6877611875534058
Epoch 630, training loss: 6.545197486877441 = 0.22451069951057434 + 1.0 * 6.3206868171691895
Epoch 630, val loss: 0.6837306022644043
Epoch 640, training loss: 6.528217315673828 = 0.20920045673847198 + 1.0 * 6.319016933441162
Epoch 640, val loss: 0.6803857088088989
Epoch 650, training loss: 6.5133137702941895 = 0.19469775259494781 + 1.0 * 6.318615913391113
Epoch 650, val loss: 0.677880585193634
Epoch 660, training loss: 6.500177383422852 = 0.18122653663158417 + 1.0 * 6.318950653076172
Epoch 660, val loss: 0.6763054132461548
Epoch 670, training loss: 6.485574722290039 = 0.16892623901367188 + 1.0 * 6.316648483276367
Epoch 670, val loss: 0.6757163405418396
Epoch 680, training loss: 6.471130847930908 = 0.1576746106147766 + 1.0 * 6.313456058502197
Epoch 680, val loss: 0.6759421229362488
Epoch 690, training loss: 6.466916084289551 = 0.14741410315036774 + 1.0 * 6.319501876831055
Epoch 690, val loss: 0.6769406199455261
Epoch 700, training loss: 6.456622123718262 = 0.13812032341957092 + 1.0 * 6.318501949310303
Epoch 700, val loss: 0.6786684393882751
Epoch 710, training loss: 6.440809726715088 = 0.12974460422992706 + 1.0 * 6.311065196990967
Epoch 710, val loss: 0.6810636520385742
Epoch 720, training loss: 6.430975914001465 = 0.12207656353712082 + 1.0 * 6.308899402618408
Epoch 720, val loss: 0.6838952302932739
Epoch 730, training loss: 6.429260730743408 = 0.11502809822559357 + 1.0 * 6.31423282623291
Epoch 730, val loss: 0.6871122717857361
Epoch 740, training loss: 6.422972202301025 = 0.10855095833539963 + 1.0 * 6.3144211769104
Epoch 740, val loss: 0.6907994747161865
Epoch 750, training loss: 6.408607006072998 = 0.10264255851507187 + 1.0 * 6.305964469909668
Epoch 750, val loss: 0.6946879625320435
Epoch 760, training loss: 6.404057025909424 = 0.09716474264860153 + 1.0 * 6.306892395019531
Epoch 760, val loss: 0.6987625956535339
Epoch 770, training loss: 6.397634983062744 = 0.09206631034612656 + 1.0 * 6.305568695068359
Epoch 770, val loss: 0.7030574083328247
Epoch 780, training loss: 6.392584323883057 = 0.08732639998197556 + 1.0 * 6.305257797241211
Epoch 780, val loss: 0.7075821757316589
Epoch 790, training loss: 6.385722637176514 = 0.08290185034275055 + 1.0 * 6.302820682525635
Epoch 790, val loss: 0.7122840285301208
Epoch 800, training loss: 6.3832268714904785 = 0.07876663655042648 + 1.0 * 6.304460048675537
Epoch 800, val loss: 0.7171139121055603
Epoch 810, training loss: 6.379822731018066 = 0.07490981370210648 + 1.0 * 6.30491304397583
Epoch 810, val loss: 0.7220366597175598
Epoch 820, training loss: 6.3729939460754395 = 0.07132218033075333 + 1.0 * 6.301671981811523
Epoch 820, val loss: 0.727034330368042
Epoch 830, training loss: 6.366414546966553 = 0.06795487552881241 + 1.0 * 6.298459529876709
Epoch 830, val loss: 0.732149600982666
Epoch 840, training loss: 6.371580123901367 = 0.06478529423475266 + 1.0 * 6.3067946434021
Epoch 840, val loss: 0.7373042106628418
Epoch 850, training loss: 6.361879825592041 = 0.0618203766644001 + 1.0 * 6.3000593185424805
Epoch 850, val loss: 0.742486834526062
Epoch 860, training loss: 6.3676252365112305 = 0.05902852863073349 + 1.0 * 6.308596611022949
Epoch 860, val loss: 0.7477959394454956
Epoch 870, training loss: 6.356482028961182 = 0.0564284585416317 + 1.0 * 6.300053596496582
Epoch 870, val loss: 0.7530145645141602
Epoch 880, training loss: 6.34810209274292 = 0.05398733168840408 + 1.0 * 6.294114589691162
Epoch 880, val loss: 0.7582843899726868
Epoch 890, training loss: 6.344867706298828 = 0.05167693644762039 + 1.0 * 6.293190956115723
Epoch 890, val loss: 0.7635523676872253
Epoch 900, training loss: 6.3415141105651855 = 0.04949197173118591 + 1.0 * 6.292022228240967
Epoch 900, val loss: 0.7688160538673401
Epoch 910, training loss: 6.365296840667725 = 0.04743567481637001 + 1.0 * 6.317861080169678
Epoch 910, val loss: 0.7740341424942017
Epoch 920, training loss: 6.337144374847412 = 0.04549092799425125 + 1.0 * 6.291653633117676
Epoch 920, val loss: 0.7792856693267822
Epoch 930, training loss: 6.334150791168213 = 0.043675653636455536 + 1.0 * 6.290475368499756
Epoch 930, val loss: 0.784522294998169
Epoch 940, training loss: 6.331120491027832 = 0.04195127636194229 + 1.0 * 6.2891693115234375
Epoch 940, val loss: 0.7896459698677063
Epoch 950, training loss: 6.329443454742432 = 0.04031592234969139 + 1.0 * 6.289127349853516
Epoch 950, val loss: 0.7947609424591064
Epoch 960, training loss: 6.328590393066406 = 0.03876618295907974 + 1.0 * 6.28982400894165
Epoch 960, val loss: 0.7998673319816589
Epoch 970, training loss: 6.324787139892578 = 0.037305623292922974 + 1.0 * 6.287481307983398
Epoch 970, val loss: 0.8049936890602112
Epoch 980, training loss: 6.326959133148193 = 0.0359191969037056 + 1.0 * 6.291039943695068
Epoch 980, val loss: 0.810029923915863
Epoch 990, training loss: 6.322675704956055 = 0.03461332619190216 + 1.0 * 6.288062572479248
Epoch 990, val loss: 0.8149619102478027
Epoch 1000, training loss: 6.320579528808594 = 0.03337118402123451 + 1.0 * 6.287208557128906
Epoch 1000, val loss: 0.8198755383491516
Epoch 1010, training loss: 6.317415237426758 = 0.03219792619347572 + 1.0 * 6.28521728515625
Epoch 1010, val loss: 0.8247546553611755
Epoch 1020, training loss: 6.316910743713379 = 0.031078042462468147 + 1.0 * 6.28583288192749
Epoch 1020, val loss: 0.8295764923095703
Epoch 1030, training loss: 6.322166919708252 = 0.030019206926226616 + 1.0 * 6.292147636413574
Epoch 1030, val loss: 0.8344021439552307
Epoch 1040, training loss: 6.313369274139404 = 0.02900948002934456 + 1.0 * 6.284359931945801
Epoch 1040, val loss: 0.8391295075416565
Epoch 1050, training loss: 6.310929775238037 = 0.028054120019078255 + 1.0 * 6.2828755378723145
Epoch 1050, val loss: 0.8438866138458252
Epoch 1060, training loss: 6.3089680671691895 = 0.027142705395817757 + 1.0 * 6.281825542449951
Epoch 1060, val loss: 0.8485395908355713
Epoch 1070, training loss: 6.314201831817627 = 0.026269489899277687 + 1.0 * 6.287932395935059
Epoch 1070, val loss: 0.853168249130249
Epoch 1080, training loss: 6.307704925537109 = 0.025446809828281403 + 1.0 * 6.282258033752441
Epoch 1080, val loss: 0.8576998710632324
Epoch 1090, training loss: 6.306626319885254 = 0.024659547954797745 + 1.0 * 6.281966686248779
Epoch 1090, val loss: 0.8622746467590332
Epoch 1100, training loss: 6.303940773010254 = 0.023909039795398712 + 1.0 * 6.280031681060791
Epoch 1100, val loss: 0.86670982837677
Epoch 1110, training loss: 6.303752899169922 = 0.023192329332232475 + 1.0 * 6.280560493469238
Epoch 1110, val loss: 0.8711346387863159
Epoch 1120, training loss: 6.301208972930908 = 0.022506888955831528 + 1.0 * 6.278702259063721
Epoch 1120, val loss: 0.8755207061767578
Epoch 1130, training loss: 6.305359363555908 = 0.021853391081094742 + 1.0 * 6.283505916595459
Epoch 1130, val loss: 0.8798379898071289
Epoch 1140, training loss: 6.298276901245117 = 0.02123159170150757 + 1.0 * 6.277045249938965
Epoch 1140, val loss: 0.8841253519058228
Epoch 1150, training loss: 6.295687198638916 = 0.020632490515708923 + 1.0 * 6.275054931640625
Epoch 1150, val loss: 0.8883948922157288
Epoch 1160, training loss: 6.3077192306518555 = 0.020061185583472252 + 1.0 * 6.287658214569092
Epoch 1160, val loss: 0.8925327062606812
Epoch 1170, training loss: 6.298983573913574 = 0.019511964172124863 + 1.0 * 6.279471397399902
Epoch 1170, val loss: 0.8966697454452515
Epoch 1180, training loss: 6.293941020965576 = 0.018990319222211838 + 1.0 * 6.2749505043029785
Epoch 1180, val loss: 0.9007725119590759
Epoch 1190, training loss: 6.298406600952148 = 0.018489280715584755 + 1.0 * 6.279917240142822
Epoch 1190, val loss: 0.904795229434967
Epoch 1200, training loss: 6.290715217590332 = 0.018004996702075005 + 1.0 * 6.27271032333374
Epoch 1200, val loss: 0.9087801575660706
Epoch 1210, training loss: 6.294317722320557 = 0.017543744295835495 + 1.0 * 6.276773929595947
Epoch 1210, val loss: 0.9127820134162903
Epoch 1220, training loss: 6.29003381729126 = 0.017100060358643532 + 1.0 * 6.2729339599609375
Epoch 1220, val loss: 0.9166349172592163
Epoch 1230, training loss: 6.288050174713135 = 0.01667407527565956 + 1.0 * 6.271376132965088
Epoch 1230, val loss: 0.9205604195594788
Epoch 1240, training loss: 6.28634786605835 = 0.01626148447394371 + 1.0 * 6.270086288452148
Epoch 1240, val loss: 0.9244239330291748
Epoch 1250, training loss: 6.293055057525635 = 0.01586347259581089 + 1.0 * 6.277191638946533
Epoch 1250, val loss: 0.9282689690589905
Epoch 1260, training loss: 6.291301250457764 = 0.01548387948423624 + 1.0 * 6.275817394256592
Epoch 1260, val loss: 0.931978166103363
Epoch 1270, training loss: 6.2861328125 = 0.015120123513042927 + 1.0 * 6.271012783050537
Epoch 1270, val loss: 0.9357848763465881
Epoch 1280, training loss: 6.283405780792236 = 0.014769171364605427 + 1.0 * 6.268636703491211
Epoch 1280, val loss: 0.9394991993904114
Epoch 1290, training loss: 6.290225505828857 = 0.014429622329771519 + 1.0 * 6.275795936584473
Epoch 1290, val loss: 0.9431081414222717
Epoch 1300, training loss: 6.284262180328369 = 0.014100316911935806 + 1.0 * 6.2701616287231445
Epoch 1300, val loss: 0.9467484354972839
Epoch 1310, training loss: 6.283353328704834 = 0.013786527328193188 + 1.0 * 6.269567012786865
Epoch 1310, val loss: 0.950309693813324
Epoch 1320, training loss: 6.287907123565674 = 0.013482989743351936 + 1.0 * 6.274424076080322
Epoch 1320, val loss: 0.9538437724113464
Epoch 1330, training loss: 6.281410217285156 = 0.013191521167755127 + 1.0 * 6.268218517303467
Epoch 1330, val loss: 0.9573363065719604
Epoch 1340, training loss: 6.278865337371826 = 0.012909266166388988 + 1.0 * 6.265955924987793
Epoch 1340, val loss: 0.9608039259910583
Epoch 1350, training loss: 6.277883529663086 = 0.01263546571135521 + 1.0 * 6.2652482986450195
Epoch 1350, val loss: 0.9642397165298462
Epoch 1360, training loss: 6.29041862487793 = 0.012370462529361248 + 1.0 * 6.278048038482666
Epoch 1360, val loss: 0.9676024317741394
Epoch 1370, training loss: 6.2812347412109375 = 0.012111165560781956 + 1.0 * 6.269123554229736
Epoch 1370, val loss: 0.9709820747375488
Epoch 1380, training loss: 6.278130531311035 = 0.011866350658237934 + 1.0 * 6.266263961791992
Epoch 1380, val loss: 0.9742922186851501
Epoch 1390, training loss: 6.277109622955322 = 0.01162536721676588 + 1.0 * 6.26548433303833
Epoch 1390, val loss: 0.9775799512863159
Epoch 1400, training loss: 6.275516033172607 = 0.011393795721232891 + 1.0 * 6.264122009277344
Epoch 1400, val loss: 0.9808513522148132
Epoch 1410, training loss: 6.281746864318848 = 0.011167917400598526 + 1.0 * 6.270578861236572
Epoch 1410, val loss: 0.9840878248214722
Epoch 1420, training loss: 6.277168273925781 = 0.010951991192996502 + 1.0 * 6.266216278076172
Epoch 1420, val loss: 0.9872192144393921
Epoch 1430, training loss: 6.272816181182861 = 0.010741407051682472 + 1.0 * 6.262074947357178
Epoch 1430, val loss: 0.9904293417930603
Epoch 1440, training loss: 6.272691249847412 = 0.010537158697843552 + 1.0 * 6.2621541023254395
Epoch 1440, val loss: 0.9935340285301208
Epoch 1450, training loss: 6.276941776275635 = 0.010337491519749165 + 1.0 * 6.266604423522949
Epoch 1450, val loss: 0.996625542640686
Epoch 1460, training loss: 6.27229118347168 = 0.010145692154765129 + 1.0 * 6.262145519256592
Epoch 1460, val loss: 0.9996834993362427
Epoch 1470, training loss: 6.279263019561768 = 0.009958446957170963 + 1.0 * 6.2693047523498535
Epoch 1470, val loss: 1.0027028322219849
Epoch 1480, training loss: 6.2768659591674805 = 0.009779800660908222 + 1.0 * 6.267086029052734
Epoch 1480, val loss: 1.005725622177124
Epoch 1490, training loss: 6.26956844329834 = 0.00960451364517212 + 1.0 * 6.2599639892578125
Epoch 1490, val loss: 1.0086363554000854
Epoch 1500, training loss: 6.268918514251709 = 0.009435039013624191 + 1.0 * 6.259483337402344
Epoch 1500, val loss: 1.0115803480148315
Epoch 1510, training loss: 6.267766952514648 = 0.009270324371755123 + 1.0 * 6.2584967613220215
Epoch 1510, val loss: 1.01450514793396
Epoch 1520, training loss: 6.272160530090332 = 0.009108925238251686 + 1.0 * 6.263051509857178
Epoch 1520, val loss: 1.0173869132995605
Epoch 1530, training loss: 6.276034832000732 = 0.008952931500971317 + 1.0 * 6.2670817375183105
Epoch 1530, val loss: 1.0202332735061646
Epoch 1540, training loss: 6.268286228179932 = 0.008800524286925793 + 1.0 * 6.259485721588135
Epoch 1540, val loss: 1.0230960845947266
Epoch 1550, training loss: 6.266610622406006 = 0.008654058910906315 + 1.0 * 6.257956504821777
Epoch 1550, val loss: 1.0259218215942383
Epoch 1560, training loss: 6.26802396774292 = 0.008509978652000427 + 1.0 * 6.259513854980469
Epoch 1560, val loss: 1.0287339687347412
Epoch 1570, training loss: 6.266856670379639 = 0.008370056748390198 + 1.0 * 6.258486747741699
Epoch 1570, val loss: 1.0314618349075317
Epoch 1580, training loss: 6.271640777587891 = 0.008235887624323368 + 1.0 * 6.263404846191406
Epoch 1580, val loss: 1.0341533422470093
Epoch 1590, training loss: 6.263943672180176 = 0.008102082647383213 + 1.0 * 6.2558417320251465
Epoch 1590, val loss: 1.0368597507476807
Epoch 1600, training loss: 6.2630534172058105 = 0.007974721491336823 + 1.0 * 6.2550787925720215
Epoch 1600, val loss: 1.0395843982696533
Epoch 1610, training loss: 6.267324447631836 = 0.007849779911339283 + 1.0 * 6.259474754333496
Epoch 1610, val loss: 1.0422519445419312
Epoch 1620, training loss: 6.263901710510254 = 0.0077266572043299675 + 1.0 * 6.2561750411987305
Epoch 1620, val loss: 1.0448658466339111
Epoch 1630, training loss: 6.26388692855835 = 0.007607988081872463 + 1.0 * 6.256278991699219
Epoch 1630, val loss: 1.0474618673324585
Epoch 1640, training loss: 6.26472806930542 = 0.007492585573345423 + 1.0 * 6.257235527038574
Epoch 1640, val loss: 1.0500799417495728
Epoch 1650, training loss: 6.262944221496582 = 0.0073785800486803055 + 1.0 * 6.255565643310547
Epoch 1650, val loss: 1.0526330471038818
Epoch 1660, training loss: 6.260884761810303 = 0.007268103305250406 + 1.0 * 6.253616809844971
Epoch 1660, val loss: 1.0552234649658203
Epoch 1670, training loss: 6.269174575805664 = 0.007159741595387459 + 1.0 * 6.262014865875244
Epoch 1670, val loss: 1.0577064752578735
Epoch 1680, training loss: 6.262171268463135 = 0.007054644171148539 + 1.0 * 6.2551164627075195
Epoch 1680, val loss: 1.0602359771728516
Epoch 1690, training loss: 6.259613990783691 = 0.00695262523368001 + 1.0 * 6.252661228179932
Epoch 1690, val loss: 1.0627233982086182
Epoch 1700, training loss: 6.266096115112305 = 0.006852676626294851 + 1.0 * 6.259243488311768
Epoch 1700, val loss: 1.065161943435669
Epoch 1710, training loss: 6.2648444175720215 = 0.006754905451089144 + 1.0 * 6.258089542388916
Epoch 1710, val loss: 1.0676279067993164
Epoch 1720, training loss: 6.259623050689697 = 0.006659387610852718 + 1.0 * 6.252963542938232
Epoch 1720, val loss: 1.0699995756149292
Epoch 1730, training loss: 6.258152008056641 = 0.006567084230482578 + 1.0 * 6.251585006713867
Epoch 1730, val loss: 1.0724376440048218
Epoch 1740, training loss: 6.261295318603516 = 0.0064760176464915276 + 1.0 * 6.254819393157959
Epoch 1740, val loss: 1.0748037099838257
Epoch 1750, training loss: 6.25841760635376 = 0.006387052126228809 + 1.0 * 6.252030372619629
Epoch 1750, val loss: 1.0771392583847046
Epoch 1760, training loss: 6.258638858795166 = 0.006300898268818855 + 1.0 * 6.25233793258667
Epoch 1760, val loss: 1.0794737339019775
Epoch 1770, training loss: 6.255823135375977 = 0.006215421482920647 + 1.0 * 6.249607563018799
Epoch 1770, val loss: 1.0818090438842773
Epoch 1780, training loss: 6.256416320800781 = 0.006132118403911591 + 1.0 * 6.250284194946289
Epoch 1780, val loss: 1.0841140747070312
Epoch 1790, training loss: 6.260377407073975 = 0.006050019059330225 + 1.0 * 6.254327297210693
Epoch 1790, val loss: 1.086391568183899
Epoch 1800, training loss: 6.263922691345215 = 0.005971785634756088 + 1.0 * 6.257950782775879
Epoch 1800, val loss: 1.0886343717575073
Epoch 1810, training loss: 6.255134582519531 = 0.005893387366086245 + 1.0 * 6.249241352081299
Epoch 1810, val loss: 1.090934157371521
Epoch 1820, training loss: 6.254031181335449 = 0.005818295758217573 + 1.0 * 6.248212814331055
Epoch 1820, val loss: 1.0931241512298584
Epoch 1830, training loss: 6.2529072761535645 = 0.005744070280343294 + 1.0 * 6.24716329574585
Epoch 1830, val loss: 1.0953396558761597
Epoch 1840, training loss: 6.268603324890137 = 0.005671455524861813 + 1.0 * 6.262931823730469
Epoch 1840, val loss: 1.0975579023361206
Epoch 1850, training loss: 6.260071277618408 = 0.005599100608378649 + 1.0 * 6.254472255706787
Epoch 1850, val loss: 1.0996973514556885
Epoch 1860, training loss: 6.253279685974121 = 0.005530731752514839 + 1.0 * 6.247748851776123
Epoch 1860, val loss: 1.101854681968689
Epoch 1870, training loss: 6.251372814178467 = 0.005463546607643366 + 1.0 * 6.245909214019775
Epoch 1870, val loss: 1.1040029525756836
Epoch 1880, training loss: 6.255884647369385 = 0.005396055988967419 + 1.0 * 6.250488758087158
Epoch 1880, val loss: 1.1061357259750366
Epoch 1890, training loss: 6.252524375915527 = 0.005330073647201061 + 1.0 * 6.247194290161133
Epoch 1890, val loss: 1.1082266569137573
Epoch 1900, training loss: 6.250373840332031 = 0.005266647785902023 + 1.0 * 6.245107173919678
Epoch 1900, val loss: 1.1103055477142334
Epoch 1910, training loss: 6.250012397766113 = 0.0052044386975467205 + 1.0 * 6.244808197021484
Epoch 1910, val loss: 1.1124041080474854
Epoch 1920, training loss: 6.2520318031311035 = 0.005142692942172289 + 1.0 * 6.246889114379883
Epoch 1920, val loss: 1.1145137548446655
Epoch 1930, training loss: 6.257857322692871 = 0.005080879665911198 + 1.0 * 6.252776622772217
Epoch 1930, val loss: 1.1165767908096313
Epoch 1940, training loss: 6.250107765197754 = 0.005022636614739895 + 1.0 * 6.2450852394104
Epoch 1940, val loss: 1.1185564994812012
Epoch 1950, training loss: 6.249192237854004 = 0.004965534433722496 + 1.0 * 6.244226932525635
Epoch 1950, val loss: 1.1205404996871948
Epoch 1960, training loss: 6.2498040199279785 = 0.0049090455286204815 + 1.0 * 6.244894981384277
Epoch 1960, val loss: 1.1225380897521973
Epoch 1970, training loss: 6.252753734588623 = 0.004852949175983667 + 1.0 * 6.24790096282959
Epoch 1970, val loss: 1.1245183944702148
Epoch 1980, training loss: 6.252135276794434 = 0.004797425586730242 + 1.0 * 6.247337818145752
Epoch 1980, val loss: 1.1265095472335815
Epoch 1990, training loss: 6.249442100524902 = 0.004743748810142279 + 1.0 * 6.244698524475098
Epoch 1990, val loss: 1.1284825801849365
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 10.537779808044434 = 1.9409215450286865 + 1.0 * 8.596858024597168
Epoch 0, val loss: 1.9368427991867065
Epoch 10, training loss: 10.527974128723145 = 1.9313572645187378 + 1.0 * 8.596616744995117
Epoch 10, val loss: 1.9277994632720947
Epoch 20, training loss: 10.513375282287598 = 1.9187943935394287 + 1.0 * 8.59458065032959
Epoch 20, val loss: 1.9156347513198853
Epoch 30, training loss: 10.477314949035645 = 1.9003299474716187 + 1.0 * 8.576985359191895
Epoch 30, val loss: 1.8976430892944336
Epoch 40, training loss: 10.35263729095459 = 1.8741697072982788 + 1.0 * 8.47846794128418
Epoch 40, val loss: 1.8731348514556885
Epoch 50, training loss: 9.944704055786133 = 1.8434301614761353 + 1.0 * 8.101273536682129
Epoch 50, val loss: 1.8456898927688599
Epoch 60, training loss: 9.685161590576172 = 1.8124537467956543 + 1.0 * 7.872707843780518
Epoch 60, val loss: 1.8190696239471436
Epoch 70, training loss: 9.38946533203125 = 1.7882699966430664 + 1.0 * 7.601195812225342
Epoch 70, val loss: 1.7976508140563965
Epoch 80, training loss: 8.992897033691406 = 1.7732055187225342 + 1.0 * 7.219691753387451
Epoch 80, val loss: 1.7842364311218262
Epoch 90, training loss: 8.796305656433105 = 1.7600101232528687 + 1.0 * 7.0362958908081055
Epoch 90, val loss: 1.7709170579910278
Epoch 100, training loss: 8.656975746154785 = 1.7406319379806519 + 1.0 * 6.916343688964844
Epoch 100, val loss: 1.7528165578842163
Epoch 110, training loss: 8.543259620666504 = 1.720141053199768 + 1.0 * 6.823118686676025
Epoch 110, val loss: 1.7348326444625854
Epoch 120, training loss: 8.449124336242676 = 1.6973782777786255 + 1.0 * 6.75174617767334
Epoch 120, val loss: 1.7148914337158203
Epoch 130, training loss: 8.361368179321289 = 1.6715362071990967 + 1.0 * 6.689831733703613
Epoch 130, val loss: 1.6918741464614868
Epoch 140, training loss: 8.282792091369629 = 1.641196370124817 + 1.0 * 6.641595840454102
Epoch 140, val loss: 1.6656644344329834
Epoch 150, training loss: 8.213031768798828 = 1.6049290895462036 + 1.0 * 6.608102798461914
Epoch 150, val loss: 1.6352331638336182
Epoch 160, training loss: 8.139938354492188 = 1.5636847019195557 + 1.0 * 6.576253890991211
Epoch 160, val loss: 1.600975513458252
Epoch 170, training loss: 8.069818496704102 = 1.5175267457962036 + 1.0 * 6.552291393280029
Epoch 170, val loss: 1.5626003742218018
Epoch 180, training loss: 7.998971939086914 = 1.4669069051742554 + 1.0 * 6.532064914703369
Epoch 180, val loss: 1.520915150642395
Epoch 190, training loss: 7.930590629577637 = 1.4134583473205566 + 1.0 * 6.51713228225708
Epoch 190, val loss: 1.4776817560195923
Epoch 200, training loss: 7.85964822769165 = 1.3595367670059204 + 1.0 * 6.5001115798950195
Epoch 200, val loss: 1.4342254400253296
Epoch 210, training loss: 7.792008399963379 = 1.3053311109542847 + 1.0 * 6.486677169799805
Epoch 210, val loss: 1.3906584978103638
Epoch 220, training loss: 7.729172706604004 = 1.252135157585144 + 1.0 * 6.47703742980957
Epoch 220, val loss: 1.3483564853668213
Epoch 230, training loss: 7.664511203765869 = 1.2004510164260864 + 1.0 * 6.464060306549072
Epoch 230, val loss: 1.3076153993606567
Epoch 240, training loss: 7.607120513916016 = 1.151289463043213 + 1.0 * 6.455831050872803
Epoch 240, val loss: 1.2697532176971436
Epoch 250, training loss: 7.550718307495117 = 1.1051117181777954 + 1.0 * 6.445606708526611
Epoch 250, val loss: 1.2346175909042358
Epoch 260, training loss: 7.497638702392578 = 1.0614783763885498 + 1.0 * 6.436160564422607
Epoch 260, val loss: 1.2019257545471191
Epoch 270, training loss: 7.459324836730957 = 1.0200626850128174 + 1.0 * 6.4392619132995605
Epoch 270, val loss: 1.1715399026870728
Epoch 280, training loss: 7.404059886932373 = 0.9818442463874817 + 1.0 * 6.422215461730957
Epoch 280, val loss: 1.1436923742294312
Epoch 290, training loss: 7.362083435058594 = 0.9456028342247009 + 1.0 * 6.416480541229248
Epoch 290, val loss: 1.117788553237915
Epoch 300, training loss: 7.320058822631836 = 0.9103297591209412 + 1.0 * 6.40972900390625
Epoch 300, val loss: 1.0927809476852417
Epoch 310, training loss: 7.27916955947876 = 0.8752435445785522 + 1.0 * 6.403925895690918
Epoch 310, val loss: 1.0679935216903687
Epoch 320, training loss: 7.240877151489258 = 0.8402073383331299 + 1.0 * 6.400669574737549
Epoch 320, val loss: 1.0433827638626099
Epoch 330, training loss: 7.202635765075684 = 0.8053998351097107 + 1.0 * 6.397235870361328
Epoch 330, val loss: 1.019036054611206
Epoch 340, training loss: 7.161162376403809 = 0.7703138589859009 + 1.0 * 6.390848636627197
Epoch 340, val loss: 0.9947286248207092
Epoch 350, training loss: 7.125633239746094 = 0.7348898649215698 + 1.0 * 6.390743255615234
Epoch 350, val loss: 0.9703871011734009
Epoch 360, training loss: 7.084583759307861 = 0.6994861960411072 + 1.0 * 6.385097503662109
Epoch 360, val loss: 0.94663405418396
Epoch 370, training loss: 7.04651403427124 = 0.664404571056366 + 1.0 * 6.382109642028809
Epoch 370, val loss: 0.9237337112426758
Epoch 380, training loss: 7.007497787475586 = 0.6300190687179565 + 1.0 * 6.37747859954834
Epoch 380, val loss: 0.9021026492118835
Epoch 390, training loss: 6.972678184509277 = 0.5963094234466553 + 1.0 * 6.376368999481201
Epoch 390, val loss: 0.8818588852882385
Epoch 400, training loss: 6.9351301193237305 = 0.5637239813804626 + 1.0 * 6.371406078338623
Epoch 400, val loss: 0.8633705377578735
Epoch 410, training loss: 6.899003505706787 = 0.5322921872138977 + 1.0 * 6.366711139678955
Epoch 410, val loss: 0.8469695448875427
Epoch 420, training loss: 6.866296291351318 = 0.5018916726112366 + 1.0 * 6.364404678344727
Epoch 420, val loss: 0.832397997379303
Epoch 430, training loss: 6.839987754821777 = 0.4726749360561371 + 1.0 * 6.367312908172607
Epoch 430, val loss: 0.8197296261787415
Epoch 440, training loss: 6.803313255310059 = 0.4448283612728119 + 1.0 * 6.358484745025635
Epoch 440, val loss: 0.8090912699699402
Epoch 450, training loss: 6.772382736206055 = 0.41799238324165344 + 1.0 * 6.3543901443481445
Epoch 450, val loss: 0.8001300096511841
Epoch 460, training loss: 6.752924919128418 = 0.3921562731266022 + 1.0 * 6.360768795013428
Epoch 460, val loss: 0.7925984859466553
Epoch 470, training loss: 6.718206405639648 = 0.3675134479999542 + 1.0 * 6.3506927490234375
Epoch 470, val loss: 0.7866701483726501
Epoch 480, training loss: 6.692275047302246 = 0.3439023196697235 + 1.0 * 6.348372936248779
Epoch 480, val loss: 0.7821166515350342
Epoch 490, training loss: 6.673896312713623 = 0.3213193118572235 + 1.0 * 6.352577209472656
Epoch 490, val loss: 0.7785990238189697
Epoch 500, training loss: 6.6447625160217285 = 0.2999439835548401 + 1.0 * 6.344818592071533
Epoch 500, val loss: 0.7764146327972412
Epoch 510, training loss: 6.620785236358643 = 0.279642790555954 + 1.0 * 6.341142654418945
Epoch 510, val loss: 0.7754105925559998
Epoch 520, training loss: 6.599081993103027 = 0.2604030966758728 + 1.0 * 6.33867883682251
Epoch 520, val loss: 0.7754369378089905
Epoch 530, training loss: 6.5880045890808105 = 0.24231602251529694 + 1.0 * 6.345688343048096
Epoch 530, val loss: 0.776318371295929
Epoch 540, training loss: 6.561023235321045 = 0.22555693984031677 + 1.0 * 6.335466384887695
Epoch 540, val loss: 0.7782260775566101
Epoch 550, training loss: 6.543948650360107 = 0.20993182063102722 + 1.0 * 6.334016799926758
Epoch 550, val loss: 0.7811084985733032
Epoch 560, training loss: 6.52850341796875 = 0.19534030556678772 + 1.0 * 6.333163261413574
Epoch 560, val loss: 0.7847059369087219
Epoch 570, training loss: 6.523838520050049 = 0.18184633553028107 + 1.0 * 6.341992378234863
Epoch 570, val loss: 0.7889571189880371
Epoch 580, training loss: 6.502285480499268 = 0.1694348007440567 + 1.0 * 6.332850456237793
Epoch 580, val loss: 0.7941011190414429
Epoch 590, training loss: 6.485179424285889 = 0.1579522341489792 + 1.0 * 6.3272271156311035
Epoch 590, val loss: 0.7998398542404175
Epoch 600, training loss: 6.472188472747803 = 0.14732301235198975 + 1.0 * 6.324865341186523
Epoch 600, val loss: 0.8061188459396362
Epoch 610, training loss: 6.476962566375732 = 0.13750627636909485 + 1.0 * 6.339456081390381
Epoch 610, val loss: 0.8129681348800659
Epoch 620, training loss: 6.453054904937744 = 0.1285456269979477 + 1.0 * 6.324509143829346
Epoch 620, val loss: 0.8199997544288635
Epoch 630, training loss: 6.441537857055664 = 0.12031672149896622 + 1.0 * 6.321221351623535
Epoch 630, val loss: 0.8277424573898315
Epoch 640, training loss: 6.433543682098389 = 0.11273243278265 + 1.0 * 6.3208112716674805
Epoch 640, val loss: 0.8356064558029175
Epoch 650, training loss: 6.425629138946533 = 0.10575684159994125 + 1.0 * 6.3198723793029785
Epoch 650, val loss: 0.8435800671577454
Epoch 660, training loss: 6.417887210845947 = 0.09934590756893158 + 1.0 * 6.318541526794434
Epoch 660, val loss: 0.8520479798316956
Epoch 670, training loss: 6.411306381225586 = 0.09344364702701569 + 1.0 * 6.317862510681152
Epoch 670, val loss: 0.8605561256408691
Epoch 680, training loss: 6.405605316162109 = 0.08800139278173447 + 1.0 * 6.317604064941406
Epoch 680, val loss: 0.8692179322242737
Epoch 690, training loss: 6.397615432739258 = 0.08298337459564209 + 1.0 * 6.314631938934326
Epoch 690, val loss: 0.877990186214447
Epoch 700, training loss: 6.4048333168029785 = 0.07835593819618225 + 1.0 * 6.326477527618408
Epoch 700, val loss: 0.8866769671440125
Epoch 710, training loss: 6.384918212890625 = 0.07411486655473709 + 1.0 * 6.310803413391113
Epoch 710, val loss: 0.8953285217285156
Epoch 720, training loss: 6.380941867828369 = 0.07018604129552841 + 1.0 * 6.310755729675293
Epoch 720, val loss: 0.9041650295257568
Epoch 730, training loss: 6.374974250793457 = 0.06652665138244629 + 1.0 * 6.308447360992432
Epoch 730, val loss: 0.9129131436347961
Epoch 740, training loss: 6.370859146118164 = 0.06311798840761185 + 1.0 * 6.307741165161133
Epoch 740, val loss: 0.9216445684432983
Epoch 750, training loss: 6.374451160430908 = 0.05995028093457222 + 1.0 * 6.31450080871582
Epoch 750, val loss: 0.9301460385322571
Epoch 760, training loss: 6.36551570892334 = 0.0570339635014534 + 1.0 * 6.308481693267822
Epoch 760, val loss: 0.9386026263237
Epoch 770, training loss: 6.358879089355469 = 0.05432034283876419 + 1.0 * 6.304558753967285
Epoch 770, val loss: 0.9472246170043945
Epoch 780, training loss: 6.354722499847412 = 0.05177619308233261 + 1.0 * 6.302946090698242
Epoch 780, val loss: 0.9555547833442688
Epoch 790, training loss: 6.361694812774658 = 0.04939029738306999 + 1.0 * 6.312304496765137
Epoch 790, val loss: 0.9639113545417786
Epoch 800, training loss: 6.355014801025391 = 0.04716337099671364 + 1.0 * 6.307851314544678
Epoch 800, val loss: 0.971890926361084
Epoch 810, training loss: 6.3450026512146 = 0.04508671164512634 + 1.0 * 6.299915790557861
Epoch 810, val loss: 0.9801467061042786
Epoch 820, training loss: 6.3428239822387695 = 0.0431334488093853 + 1.0 * 6.2996907234191895
Epoch 820, val loss: 0.9882793426513672
Epoch 830, training loss: 6.351006507873535 = 0.041293688118457794 + 1.0 * 6.309712886810303
Epoch 830, val loss: 0.9961177110671997
Epoch 840, training loss: 6.340398788452148 = 0.03957737609744072 + 1.0 * 6.300821304321289
Epoch 840, val loss: 1.003818392753601
Epoch 850, training loss: 6.336723804473877 = 0.03796130791306496 + 1.0 * 6.298762321472168
Epoch 850, val loss: 1.011677861213684
Epoch 860, training loss: 6.331592559814453 = 0.03643854707479477 + 1.0 * 6.295154094696045
Epoch 860, val loss: 1.0192686319351196
Epoch 870, training loss: 6.332107067108154 = 0.03500118851661682 + 1.0 * 6.29710578918457
Epoch 870, val loss: 1.0268337726593018
Epoch 880, training loss: 6.33094596862793 = 0.03364270180463791 + 1.0 * 6.297303199768066
Epoch 880, val loss: 1.0342320203781128
Epoch 890, training loss: 6.3248291015625 = 0.03236691281199455 + 1.0 * 6.292462348937988
Epoch 890, val loss: 1.0415223836898804
Epoch 900, training loss: 6.33090877532959 = 0.03115997463464737 + 1.0 * 6.29974889755249
Epoch 900, val loss: 1.0488460063934326
Epoch 910, training loss: 6.324369430541992 = 0.030024424195289612 + 1.0 * 6.294344902038574
Epoch 910, val loss: 1.0557861328125
Epoch 920, training loss: 6.320322036743164 = 0.028945883736014366 + 1.0 * 6.291376113891602
Epoch 920, val loss: 1.0628912448883057
Epoch 930, training loss: 6.321170806884766 = 0.02792336605489254 + 1.0 * 6.293247222900391
Epoch 930, val loss: 1.0698323249816895
Epoch 940, training loss: 6.318686008453369 = 0.026953479275107384 + 1.0 * 6.291732311248779
Epoch 940, val loss: 1.0765138864517212
Epoch 950, training loss: 6.313143730163574 = 0.02603229321539402 + 1.0 * 6.287111282348633
Epoch 950, val loss: 1.0832979679107666
Epoch 960, training loss: 6.312754154205322 = 0.02515672892332077 + 1.0 * 6.28759765625
Epoch 960, val loss: 1.0899887084960938
Epoch 970, training loss: 6.317983150482178 = 0.024325957521796227 + 1.0 * 6.293657302856445
Epoch 970, val loss: 1.0962821245193481
Epoch 980, training loss: 6.310399055480957 = 0.02354305423796177 + 1.0 * 6.286856174468994
Epoch 980, val loss: 1.1026043891906738
Epoch 990, training loss: 6.307985782623291 = 0.022798027843236923 + 1.0 * 6.285187721252441
Epoch 990, val loss: 1.109091877937317
Epoch 1000, training loss: 6.306265354156494 = 0.0220824982970953 + 1.0 * 6.284183025360107
Epoch 1000, val loss: 1.1154214143753052
Epoch 1010, training loss: 6.312227725982666 = 0.02139996364712715 + 1.0 * 6.290827751159668
Epoch 1010, val loss: 1.1214172840118408
Epoch 1020, training loss: 6.304405212402344 = 0.02074764296412468 + 1.0 * 6.283657550811768
Epoch 1020, val loss: 1.1274694204330444
Epoch 1030, training loss: 6.307294845581055 = 0.020127257332205772 + 1.0 * 6.287167549133301
Epoch 1030, val loss: 1.1336241960525513
Epoch 1040, training loss: 6.312950134277344 = 0.01953899674117565 + 1.0 * 6.2934112548828125
Epoch 1040, val loss: 1.1393182277679443
Epoch 1050, training loss: 6.30029296875 = 0.018978122621774673 + 1.0 * 6.281314849853516
Epoch 1050, val loss: 1.1450873613357544
Epoch 1060, training loss: 6.298018455505371 = 0.01844171993434429 + 1.0 * 6.279576778411865
Epoch 1060, val loss: 1.1510190963745117
Epoch 1070, training loss: 6.296548843383789 = 0.017925545573234558 + 1.0 * 6.278623104095459
Epoch 1070, val loss: 1.1567813158035278
Epoch 1080, training loss: 6.297229766845703 = 0.017427921295166016 + 1.0 * 6.279801845550537
Epoch 1080, val loss: 1.1624119281768799
Epoch 1090, training loss: 6.296374797821045 = 0.01695244014263153 + 1.0 * 6.279422283172607
Epoch 1090, val loss: 1.1676944494247437
Epoch 1100, training loss: 6.297272682189941 = 0.016500670462846756 + 1.0 * 6.2807722091674805
Epoch 1100, val loss: 1.1731743812561035
Epoch 1110, training loss: 6.294272422790527 = 0.016066938638687134 + 1.0 * 6.278205394744873
Epoch 1110, val loss: 1.1787015199661255
Epoch 1120, training loss: 6.291652679443359 = 0.015649182721972466 + 1.0 * 6.276003360748291
Epoch 1120, val loss: 1.1840707063674927
Epoch 1130, training loss: 6.296907901763916 = 0.015246659517288208 + 1.0 * 6.281661033630371
Epoch 1130, val loss: 1.1893343925476074
Epoch 1140, training loss: 6.291778564453125 = 0.014860814437270164 + 1.0 * 6.276917934417725
Epoch 1140, val loss: 1.19437837600708
Epoch 1150, training loss: 6.288078308105469 = 0.01449087169021368 + 1.0 * 6.273587226867676
Epoch 1150, val loss: 1.1996254920959473
Epoch 1160, training loss: 6.29266357421875 = 0.014134598895907402 + 1.0 * 6.278529167175293
Epoch 1160, val loss: 1.2047244310379028
Epoch 1170, training loss: 6.291472911834717 = 0.013792308047413826 + 1.0 * 6.277680397033691
Epoch 1170, val loss: 1.209479570388794
Epoch 1180, training loss: 6.286294937133789 = 0.013464819639921188 + 1.0 * 6.272830009460449
Epoch 1180, val loss: 1.214393973350525
Epoch 1190, training loss: 6.285341262817383 = 0.013148773461580276 + 1.0 * 6.272192478179932
Epoch 1190, val loss: 1.2194702625274658
Epoch 1200, training loss: 6.292719841003418 = 0.012841805815696716 + 1.0 * 6.27987813949585
Epoch 1200, val loss: 1.224316120147705
Epoch 1210, training loss: 6.287275791168213 = 0.01254790835082531 + 1.0 * 6.274727821350098
Epoch 1210, val loss: 1.2287428379058838
Epoch 1220, training loss: 6.2880353927612305 = 0.012264451012015343 + 1.0 * 6.275771141052246
Epoch 1220, val loss: 1.2335151433944702
Epoch 1230, training loss: 6.282105445861816 = 0.011991412378847599 + 1.0 * 6.270113945007324
Epoch 1230, val loss: 1.2381185293197632
Epoch 1240, training loss: 6.282044410705566 = 0.011726414784789085 + 1.0 * 6.270318031311035
Epoch 1240, val loss: 1.2428611516952515
Epoch 1250, training loss: 6.283289909362793 = 0.011470151133835316 + 1.0 * 6.271819591522217
Epoch 1250, val loss: 1.2473849058151245
Epoch 1260, training loss: 6.285942554473877 = 0.01122298464179039 + 1.0 * 6.274719715118408
Epoch 1260, val loss: 1.2518165111541748
Epoch 1270, training loss: 6.280097484588623 = 0.010984781198203564 + 1.0 * 6.269112586975098
Epoch 1270, val loss: 1.2560800313949585
Epoch 1280, training loss: 6.282055377960205 = 0.010755538940429688 + 1.0 * 6.271299839019775
Epoch 1280, val loss: 1.2604938745498657
Epoch 1290, training loss: 6.27716064453125 = 0.010533218272030354 + 1.0 * 6.266627311706543
Epoch 1290, val loss: 1.264830470085144
Epoch 1300, training loss: 6.278530120849609 = 0.01031816191971302 + 1.0 * 6.268211841583252
Epoch 1300, val loss: 1.2691701650619507
Epoch 1310, training loss: 6.279638290405273 = 0.010109731927514076 + 1.0 * 6.269528388977051
Epoch 1310, val loss: 1.2732881307601929
Epoch 1320, training loss: 6.279123783111572 = 0.009907648898661137 + 1.0 * 6.269216060638428
Epoch 1320, val loss: 1.2773319482803345
Epoch 1330, training loss: 6.27467679977417 = 0.009712766855955124 + 1.0 * 6.2649641036987305
Epoch 1330, val loss: 1.2815191745758057
Epoch 1340, training loss: 6.274752140045166 = 0.009523245505988598 + 1.0 * 6.265228748321533
Epoch 1340, val loss: 1.2857744693756104
Epoch 1350, training loss: 6.280752658843994 = 0.00933864526450634 + 1.0 * 6.271413803100586
Epoch 1350, val loss: 1.2897201776504517
Epoch 1360, training loss: 6.277749061584473 = 0.009161545895040035 + 1.0 * 6.268587589263916
Epoch 1360, val loss: 1.2934876680374146
Epoch 1370, training loss: 6.273723602294922 = 0.008988899178802967 + 1.0 * 6.264734745025635
Epoch 1370, val loss: 1.297582745552063
Epoch 1380, training loss: 6.273383617401123 = 0.008822154253721237 + 1.0 * 6.264561653137207
Epoch 1380, val loss: 1.3016260862350464
Epoch 1390, training loss: 6.2761640548706055 = 0.008658742532134056 + 1.0 * 6.267505168914795
Epoch 1390, val loss: 1.3054553270339966
Epoch 1400, training loss: 6.278427600860596 = 0.008500729687511921 + 1.0 * 6.269927024841309
Epoch 1400, val loss: 1.308974266052246
Epoch 1410, training loss: 6.270453929901123 = 0.00834925938397646 + 1.0 * 6.262104511260986
Epoch 1410, val loss: 1.3127751350402832
Epoch 1420, training loss: 6.268943786621094 = 0.008201817981898785 + 1.0 * 6.2607421875
Epoch 1420, val loss: 1.3166104555130005
Epoch 1430, training loss: 6.26796293258667 = 0.008057947270572186 + 1.0 * 6.259904861450195
Epoch 1430, val loss: 1.320556640625
Epoch 1440, training loss: 6.270236492156982 = 0.007916552014648914 + 1.0 * 6.262320041656494
Epoch 1440, val loss: 1.3242650032043457
Epoch 1450, training loss: 6.26935338973999 = 0.007779391016811132 + 1.0 * 6.261573791503906
Epoch 1450, val loss: 1.327556848526001
Epoch 1460, training loss: 6.267125129699707 = 0.007647773250937462 + 1.0 * 6.259477138519287
Epoch 1460, val loss: 1.3311430215835571
Epoch 1470, training loss: 6.267781734466553 = 0.007519388105720282 + 1.0 * 6.260262489318848
Epoch 1470, val loss: 1.3349261283874512
Epoch 1480, training loss: 6.27692985534668 = 0.007393636740744114 + 1.0 * 6.269536018371582
Epoch 1480, val loss: 1.338491678237915
Epoch 1490, training loss: 6.26924991607666 = 0.007272202987223864 + 1.0 * 6.261977672576904
Epoch 1490, val loss: 1.3417587280273438
Epoch 1500, training loss: 6.266615867614746 = 0.007153888698667288 + 1.0 * 6.259461879730225
Epoch 1500, val loss: 1.345263123512268
Epoch 1510, training loss: 6.265538692474365 = 0.007038242649286985 + 1.0 * 6.258500576019287
Epoch 1510, val loss: 1.348907232284546
Epoch 1520, training loss: 6.273650169372559 = 0.00692483875900507 + 1.0 * 6.266725540161133
Epoch 1520, val loss: 1.3522392511367798
Epoch 1530, training loss: 6.265722751617432 = 0.00681547774001956 + 1.0 * 6.258907318115234
Epoch 1530, val loss: 1.3554673194885254
Epoch 1540, training loss: 6.263293743133545 = 0.006708330009132624 + 1.0 * 6.256585597991943
Epoch 1540, val loss: 1.3589485883712769
Epoch 1550, training loss: 6.271344184875488 = 0.006603538058698177 + 1.0 * 6.264740467071533
Epoch 1550, val loss: 1.362208604812622
Epoch 1560, training loss: 6.266205787658691 = 0.006501669995486736 + 1.0 * 6.259704113006592
Epoch 1560, val loss: 1.3653892278671265
Epoch 1570, training loss: 6.26499080657959 = 0.006402671802788973 + 1.0 * 6.2585883140563965
Epoch 1570, val loss: 1.3686738014221191
Epoch 1580, training loss: 6.260844707489014 = 0.006306210067123175 + 1.0 * 6.254538536071777
Epoch 1580, val loss: 1.372031331062317
Epoch 1590, training loss: 6.2666335105896 = 0.0062118717469275 + 1.0 * 6.2604217529296875
Epoch 1590, val loss: 1.3753541707992554
Epoch 1600, training loss: 6.2670087814331055 = 0.006119588855654001 + 1.0 * 6.260889053344727
Epoch 1600, val loss: 1.378109097480774
Epoch 1610, training loss: 6.261918544769287 = 0.006030596327036619 + 1.0 * 6.255887985229492
Epoch 1610, val loss: 1.3813748359680176
Epoch 1620, training loss: 6.26085901260376 = 0.005943434778600931 + 1.0 * 6.254915714263916
Epoch 1620, val loss: 1.3845298290252686
Epoch 1630, training loss: 6.2716898918151855 = 0.005858131684362888 + 1.0 * 6.26583194732666
Epoch 1630, val loss: 1.387625813484192
Epoch 1640, training loss: 6.261873245239258 = 0.005774718709290028 + 1.0 * 6.256098747253418
Epoch 1640, val loss: 1.3904356956481934
Epoch 1650, training loss: 6.2591471672058105 = 0.005693882238119841 + 1.0 * 6.253453254699707
Epoch 1650, val loss: 1.3935755491256714
Epoch 1660, training loss: 6.262876510620117 = 0.005613945424556732 + 1.0 * 6.257262706756592
Epoch 1660, val loss: 1.3967089653015137
Epoch 1670, training loss: 6.257857322692871 = 0.005536386743187904 + 1.0 * 6.252320766448975
Epoch 1670, val loss: 1.3995190858840942
Epoch 1680, training loss: 6.263391494750977 = 0.005460602231323719 + 1.0 * 6.257930755615234
Epoch 1680, val loss: 1.4025121927261353
Epoch 1690, training loss: 6.2567925453186035 = 0.00538620725274086 + 1.0 * 6.251406192779541
Epoch 1690, val loss: 1.405321478843689
Epoch 1700, training loss: 6.258720874786377 = 0.005314017180353403 + 1.0 * 6.253407001495361
Epoch 1700, val loss: 1.4084421396255493
Epoch 1710, training loss: 6.261549949645996 = 0.0052429321222007275 + 1.0 * 6.256307125091553
Epoch 1710, val loss: 1.4111846685409546
Epoch 1720, training loss: 6.257553577423096 = 0.005173729732632637 + 1.0 * 6.252379894256592
Epoch 1720, val loss: 1.4139736890792847
Epoch 1730, training loss: 6.256422996520996 = 0.005106016993522644 + 1.0 * 6.251317024230957
Epoch 1730, val loss: 1.4170169830322266
Epoch 1740, training loss: 6.264509201049805 = 0.005039235111325979 + 1.0 * 6.259469985961914
Epoch 1740, val loss: 1.4197903871536255
Epoch 1750, training loss: 6.25633430480957 = 0.004974467679858208 + 1.0 * 6.251359939575195
Epoch 1750, val loss: 1.4223415851593018
Epoch 1760, training loss: 6.257292747497559 = 0.004911229945719242 + 1.0 * 6.252381324768066
Epoch 1760, val loss: 1.4252221584320068
Epoch 1770, training loss: 6.256999492645264 = 0.004848896991461515 + 1.0 * 6.252150535583496
Epoch 1770, val loss: 1.4278777837753296
Epoch 1780, training loss: 6.255703926086426 = 0.0047879754565656185 + 1.0 * 6.250916004180908
Epoch 1780, val loss: 1.4306303262710571
Epoch 1790, training loss: 6.261499404907227 = 0.004728533793240786 + 1.0 * 6.256771087646484
Epoch 1790, val loss: 1.4333823919296265
Epoch 1800, training loss: 6.255959510803223 = 0.004670195747166872 + 1.0 * 6.251289367675781
Epoch 1800, val loss: 1.4358104467391968
Epoch 1810, training loss: 6.254262924194336 = 0.00461344700306654 + 1.0 * 6.249649524688721
Epoch 1810, val loss: 1.438494324684143
Epoch 1820, training loss: 6.2548980712890625 = 0.004557471722364426 + 1.0 * 6.250340461730957
Epoch 1820, val loss: 1.441199779510498
Epoch 1830, training loss: 6.254411220550537 = 0.0045024012215435505 + 1.0 * 6.249908924102783
Epoch 1830, val loss: 1.44378662109375
Epoch 1840, training loss: 6.25213623046875 = 0.004448730498552322 + 1.0 * 6.247687339782715
Epoch 1840, val loss: 1.4464131593704224
Epoch 1850, training loss: 6.25936222076416 = 0.004395912401378155 + 1.0 * 6.2549662590026855
Epoch 1850, val loss: 1.4489939212799072
Epoch 1860, training loss: 6.257910251617432 = 0.004344439599663019 + 1.0 * 6.253565788269043
Epoch 1860, val loss: 1.4511163234710693
Epoch 1870, training loss: 6.2530670166015625 = 0.00429470045492053 + 1.0 * 6.248772144317627
Epoch 1870, val loss: 1.453686237335205
Epoch 1880, training loss: 6.250824928283691 = 0.004245490301400423 + 1.0 * 6.246579647064209
Epoch 1880, val loss: 1.4563069343566895
Epoch 1890, training loss: 6.256049156188965 = 0.004196845460683107 + 1.0 * 6.251852512359619
Epoch 1890, val loss: 1.4587795734405518
Epoch 1900, training loss: 6.251438617706299 = 0.004149054642766714 + 1.0 * 6.247289657592773
Epoch 1900, val loss: 1.4610031843185425
Epoch 1910, training loss: 6.251559257507324 = 0.0041025299578905106 + 1.0 * 6.2474565505981445
Epoch 1910, val loss: 1.4634214639663696
Epoch 1920, training loss: 6.250367641448975 = 0.004056896083056927 + 1.0 * 6.246310710906982
Epoch 1920, val loss: 1.4659727811813354
Epoch 1930, training loss: 6.251527309417725 = 0.004011860117316246 + 1.0 * 6.247515678405762
Epoch 1930, val loss: 1.4683990478515625
Epoch 1940, training loss: 6.255674839019775 = 0.0039678774774074554 + 1.0 * 6.251707077026367
Epoch 1940, val loss: 1.470555305480957
Epoch 1950, training loss: 6.249902725219727 = 0.003924505319446325 + 1.0 * 6.245978355407715
Epoch 1950, val loss: 1.4728951454162598
Epoch 1960, training loss: 6.247655391693115 = 0.0038821538910269737 + 1.0 * 6.243773460388184
Epoch 1960, val loss: 1.4753379821777344
Epoch 1970, training loss: 6.250762462615967 = 0.003840173128992319 + 1.0 * 6.246922492980957
Epoch 1970, val loss: 1.4777307510375977
Epoch 1980, training loss: 6.249879837036133 = 0.0037989947013556957 + 1.0 * 6.2460808753967285
Epoch 1980, val loss: 1.4797720909118652
Epoch 1990, training loss: 6.248970985412598 = 0.0037589222192764282 + 1.0 * 6.245212078094482
Epoch 1990, val loss: 1.4819326400756836
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8360569319978914
The final CL Acc:0.80370, 0.01090, The final GNN Acc:0.83746, 0.00108
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10582])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.554186820983887 = 1.9573286771774292 + 1.0 * 8.596858024597168
Epoch 0, val loss: 1.9594950675964355
Epoch 10, training loss: 10.543267250061035 = 1.9466352462768555 + 1.0 * 8.59663200378418
Epoch 10, val loss: 1.948282241821289
Epoch 20, training loss: 10.528135299682617 = 1.933434247970581 + 1.0 * 8.594700813293457
Epoch 20, val loss: 1.9343445301055908
Epoch 30, training loss: 10.49470329284668 = 1.9148788452148438 + 1.0 * 8.579824447631836
Epoch 30, val loss: 1.9147388935089111
Epoch 40, training loss: 10.394146919250488 = 1.8896070718765259 + 1.0 * 8.504539489746094
Epoch 40, val loss: 1.8890106678009033
Epoch 50, training loss: 10.00908374786377 = 1.862669825553894 + 1.0 * 8.146413803100586
Epoch 50, val loss: 1.8625388145446777
Epoch 60, training loss: 9.788213729858398 = 1.8367702960968018 + 1.0 * 7.951443672180176
Epoch 60, val loss: 1.839228868484497
Epoch 70, training loss: 9.486794471740723 = 1.8157182931900024 + 1.0 * 7.671075820922852
Epoch 70, val loss: 1.8207772970199585
Epoch 80, training loss: 9.031278610229492 = 1.8018836975097656 + 1.0 * 7.229395389556885
Epoch 80, val loss: 1.8092504739761353
Epoch 90, training loss: 8.732330322265625 = 1.7934850454330444 + 1.0 * 6.938845634460449
Epoch 90, val loss: 1.8013856410980225
Epoch 100, training loss: 8.626983642578125 = 1.780976414680481 + 1.0 * 6.846007347106934
Epoch 100, val loss: 1.7892203330993652
Epoch 110, training loss: 8.543722152709961 = 1.7645583152770996 + 1.0 * 6.779163837432861
Epoch 110, val loss: 1.7743505239486694
Epoch 120, training loss: 8.474985122680664 = 1.748678207397461 + 1.0 * 6.726306438446045
Epoch 120, val loss: 1.760095238685608
Epoch 130, training loss: 8.405899047851562 = 1.733445644378662 + 1.0 * 6.672453880310059
Epoch 130, val loss: 1.7460358142852783
Epoch 140, training loss: 8.341134071350098 = 1.7172272205352783 + 1.0 * 6.623907089233398
Epoch 140, val loss: 1.7313827276229858
Epoch 150, training loss: 8.287030220031738 = 1.6977473497390747 + 1.0 * 6.589282512664795
Epoch 150, val loss: 1.7144593000411987
Epoch 160, training loss: 8.23869514465332 = 1.6738797426223755 + 1.0 * 6.564815044403076
Epoch 160, val loss: 1.6942309141159058
Epoch 170, training loss: 8.191288948059082 = 1.6454885005950928 + 1.0 * 6.54580020904541
Epoch 170, val loss: 1.6701043844223022
Epoch 180, training loss: 8.141538619995117 = 1.612690806388855 + 1.0 * 6.528848171234131
Epoch 180, val loss: 1.6418406963348389
Epoch 190, training loss: 8.08845043182373 = 1.5749661922454834 + 1.0 * 6.513484001159668
Epoch 190, val loss: 1.6089742183685303
Epoch 200, training loss: 8.036467552185059 = 1.531937837600708 + 1.0 * 6.5045294761657715
Epoch 200, val loss: 1.5714666843414307
Epoch 210, training loss: 7.974004745483398 = 1.484708547592163 + 1.0 * 6.4892964363098145
Epoch 210, val loss: 1.5305832624435425
Epoch 220, training loss: 7.9149169921875 = 1.4336010217666626 + 1.0 * 6.481316089630127
Epoch 220, val loss: 1.4863612651824951
Epoch 230, training loss: 7.85252571105957 = 1.3798424005508423 + 1.0 * 6.472683429718018
Epoch 230, val loss: 1.440054178237915
Epoch 240, training loss: 7.791523456573486 = 1.3248271942138672 + 1.0 * 6.466696262359619
Epoch 240, val loss: 1.3934110403060913
Epoch 250, training loss: 7.726617813110352 = 1.2696017026901245 + 1.0 * 6.4570159912109375
Epoch 250, val loss: 1.347407341003418
Epoch 260, training loss: 7.664029121398926 = 1.214434027671814 + 1.0 * 6.449594974517822
Epoch 260, val loss: 1.3023334741592407
Epoch 270, training loss: 7.603731632232666 = 1.1604446172714233 + 1.0 * 6.443286895751953
Epoch 270, val loss: 1.2592501640319824
Epoch 280, training loss: 7.542570114135742 = 1.1079912185668945 + 1.0 * 6.434578895568848
Epoch 280, val loss: 1.2184160947799683
Epoch 290, training loss: 7.4931817054748535 = 1.0568054914474487 + 1.0 * 6.436376094818115
Epoch 290, val loss: 1.1794489622116089
Epoch 300, training loss: 7.4327802658081055 = 1.008360743522644 + 1.0 * 6.424419403076172
Epoch 300, val loss: 1.1431546211242676
Epoch 310, training loss: 7.377198219299316 = 0.9619600772857666 + 1.0 * 6.415237903594971
Epoch 310, val loss: 1.1090645790100098
Epoch 320, training loss: 7.329424858093262 = 0.9172219038009644 + 1.0 * 6.412202835083008
Epoch 320, val loss: 1.076660394668579
Epoch 330, training loss: 7.281418323516846 = 0.8747841715812683 + 1.0 * 6.406634330749512
Epoch 330, val loss: 1.046495795249939
Epoch 340, training loss: 7.234920501708984 = 0.8346887826919556 + 1.0 * 6.400231838226318
Epoch 340, val loss: 1.018711805343628
Epoch 350, training loss: 7.191866397857666 = 0.7968059778213501 + 1.0 * 6.3950605392456055
Epoch 350, val loss: 0.9930948615074158
Epoch 360, training loss: 7.159843921661377 = 0.7616472840309143 + 1.0 * 6.398196697235107
Epoch 360, val loss: 0.9702346324920654
Epoch 370, training loss: 7.116498947143555 = 0.7294483184814453 + 1.0 * 6.387050628662109
Epoch 370, val loss: 0.950427234172821
Epoch 380, training loss: 7.085165023803711 = 0.6996028423309326 + 1.0 * 6.385562419891357
Epoch 380, val loss: 0.9332178831100464
Epoch 390, training loss: 7.055747032165527 = 0.6722657680511475 + 1.0 * 6.383481502532959
Epoch 390, val loss: 0.9185155630111694
Epoch 400, training loss: 7.022232532501221 = 0.647253155708313 + 1.0 * 6.374979496002197
Epoch 400, val loss: 0.9064556360244751
Epoch 410, training loss: 6.995006084442139 = 0.6240261197090149 + 1.0 * 6.3709797859191895
Epoch 410, val loss: 0.8964795470237732
Epoch 420, training loss: 6.973247051239014 = 0.602475106716156 + 1.0 * 6.370771884918213
Epoch 420, val loss: 0.8882690668106079
Epoch 430, training loss: 6.948181629180908 = 0.5825855135917664 + 1.0 * 6.365596294403076
Epoch 430, val loss: 0.8819319009780884
Epoch 440, training loss: 6.925821304321289 = 0.5640028119087219 + 1.0 * 6.361818313598633
Epoch 440, val loss: 0.8770859241485596
Epoch 450, training loss: 6.907646656036377 = 0.5465078353881836 + 1.0 * 6.361138820648193
Epoch 450, val loss: 0.8733314275741577
Epoch 460, training loss: 6.89365291595459 = 0.5300832390785217 + 1.0 * 6.363569736480713
Epoch 460, val loss: 0.8705151677131653
Epoch 470, training loss: 6.869536399841309 = 0.5145446062088013 + 1.0 * 6.354991912841797
Epoch 470, val loss: 0.868596613407135
Epoch 480, training loss: 6.850048065185547 = 0.49961674213409424 + 1.0 * 6.350431442260742
Epoch 480, val loss: 0.8672580122947693
Epoch 490, training loss: 6.83485746383667 = 0.48504772782325745 + 1.0 * 6.349809646606445
Epoch 490, val loss: 0.8662527203559875
Epoch 500, training loss: 6.8170552253723145 = 0.47071602940559387 + 1.0 * 6.346339225769043
Epoch 500, val loss: 0.8655042052268982
Epoch 510, training loss: 6.800500392913818 = 0.4564811587333679 + 1.0 * 6.344019412994385
Epoch 510, val loss: 0.8649370074272156
Epoch 520, training loss: 6.783602714538574 = 0.4421374797821045 + 1.0 * 6.341464996337891
Epoch 520, val loss: 0.8643753528594971
Epoch 530, training loss: 6.766190528869629 = 0.42740723490715027 + 1.0 * 6.338783264160156
Epoch 530, val loss: 0.8637213706970215
Epoch 540, training loss: 6.7516326904296875 = 0.41221383213996887 + 1.0 * 6.339418888092041
Epoch 540, val loss: 0.8627728819847107
Epoch 550, training loss: 6.73086404800415 = 0.396590918302536 + 1.0 * 6.334273338317871
Epoch 550, val loss: 0.8616238832473755
Epoch 560, training loss: 6.713094234466553 = 0.3803834021091461 + 1.0 * 6.3327107429504395
Epoch 560, val loss: 0.8602378368377686
Epoch 570, training loss: 6.694568634033203 = 0.3636048436164856 + 1.0 * 6.330963611602783
Epoch 570, val loss: 0.8586542010307312
Epoch 580, training loss: 6.678357124328613 = 0.34652966260910034 + 1.0 * 6.331827640533447
Epoch 580, val loss: 0.8569488525390625
Epoch 590, training loss: 6.659277439117432 = 0.3295542001724243 + 1.0 * 6.329723358154297
Epoch 590, val loss: 0.8553919792175293
Epoch 600, training loss: 6.63921594619751 = 0.31283795833587646 + 1.0 * 6.326377868652344
Epoch 600, val loss: 0.8540745377540588
Epoch 610, training loss: 6.6208367347717285 = 0.2964857816696167 + 1.0 * 6.324350833892822
Epoch 610, val loss: 0.8530816435813904
Epoch 620, training loss: 6.612231731414795 = 0.28069108724594116 + 1.0 * 6.331540584564209
Epoch 620, val loss: 0.8525590896606445
Epoch 630, training loss: 6.592012405395508 = 0.2656588852405548 + 1.0 * 6.326353549957275
Epoch 630, val loss: 0.8525581359863281
Epoch 640, training loss: 6.574520111083984 = 0.25135812163352966 + 1.0 * 6.323162078857422
Epoch 640, val loss: 0.8529970049858093
Epoch 650, training loss: 6.5565948486328125 = 0.2378690391778946 + 1.0 * 6.3187255859375
Epoch 650, val loss: 0.853999674320221
Epoch 660, training loss: 6.542882919311523 = 0.2251076102256775 + 1.0 * 6.317775249481201
Epoch 660, val loss: 0.8555643558502197
Epoch 670, training loss: 6.545000076293945 = 0.21307845413684845 + 1.0 * 6.331921577453613
Epoch 670, val loss: 0.8575744032859802
Epoch 680, training loss: 6.5199480056762695 = 0.20186986029148102 + 1.0 * 6.31807804107666
Epoch 680, val loss: 0.8599870204925537
Epoch 690, training loss: 6.505387306213379 = 0.19141480326652527 + 1.0 * 6.313972473144531
Epoch 690, val loss: 0.8631157279014587
Epoch 700, training loss: 6.502728462219238 = 0.18159538507461548 + 1.0 * 6.321133136749268
Epoch 700, val loss: 0.8664719462394714
Epoch 710, training loss: 6.486659526824951 = 0.17241282761096954 + 1.0 * 6.314246654510498
Epoch 710, val loss: 0.870205819606781
Epoch 720, training loss: 6.474968910217285 = 0.16382135450839996 + 1.0 * 6.311147689819336
Epoch 720, val loss: 0.8744331002235413
Epoch 730, training loss: 6.46571683883667 = 0.15572874248027802 + 1.0 * 6.309988021850586
Epoch 730, val loss: 0.8788256645202637
Epoch 740, training loss: 6.459306716918945 = 0.14813826978206635 + 1.0 * 6.311168670654297
Epoch 740, val loss: 0.8834307789802551
Epoch 750, training loss: 6.448940277099609 = 0.14102141559123993 + 1.0 * 6.307919025421143
Epoch 750, val loss: 0.8883716464042664
Epoch 760, training loss: 6.440488338470459 = 0.13432426750659943 + 1.0 * 6.306164264678955
Epoch 760, val loss: 0.8934856653213501
Epoch 770, training loss: 6.442656517028809 = 0.12802447378635406 + 1.0 * 6.314631938934326
Epoch 770, val loss: 0.8986293077468872
Epoch 780, training loss: 6.427173137664795 = 0.12211089581251144 + 1.0 * 6.305062294006348
Epoch 780, val loss: 0.9040095806121826
Epoch 790, training loss: 6.421997547149658 = 0.11654013395309448 + 1.0 * 6.305457592010498
Epoch 790, val loss: 0.9095355272293091
Epoch 800, training loss: 6.422394275665283 = 0.11127988994121552 + 1.0 * 6.311114311218262
Epoch 800, val loss: 0.9148889780044556
Epoch 810, training loss: 6.410294055938721 = 0.1063321977853775 + 1.0 * 6.303961753845215
Epoch 810, val loss: 0.920616626739502
Epoch 820, training loss: 6.401854991912842 = 0.10164643824100494 + 1.0 * 6.300208568572998
Epoch 820, val loss: 0.9263724088668823
Epoch 830, training loss: 6.39919376373291 = 0.09720022976398468 + 1.0 * 6.301993370056152
Epoch 830, val loss: 0.9321088194847107
Epoch 840, training loss: 6.393730640411377 = 0.09298418462276459 + 1.0 * 6.300746440887451
Epoch 840, val loss: 0.93788081407547
Epoch 850, training loss: 6.387034893035889 = 0.08899431675672531 + 1.0 * 6.298040390014648
Epoch 850, val loss: 0.9438695907592773
Epoch 860, training loss: 6.385524749755859 = 0.08520089834928513 + 1.0 * 6.300323963165283
Epoch 860, val loss: 0.9498493075370789
Epoch 870, training loss: 6.379878997802734 = 0.08159632980823517 + 1.0 * 6.298282623291016
Epoch 870, val loss: 0.9556450843811035
Epoch 880, training loss: 6.375236511230469 = 0.07818249613046646 + 1.0 * 6.297053813934326
Epoch 880, val loss: 0.9617846608161926
Epoch 890, training loss: 6.373129367828369 = 0.07493411004543304 + 1.0 * 6.2981953620910645
Epoch 890, val loss: 0.9677395224571228
Epoch 900, training loss: 6.366057395935059 = 0.07185008376836777 + 1.0 * 6.2942070960998535
Epoch 900, val loss: 0.9737681150436401
Epoch 910, training loss: 6.3701276779174805 = 0.06891229003667831 + 1.0 * 6.301215171813965
Epoch 910, val loss: 0.9798122644424438
Epoch 920, training loss: 6.36234188079834 = 0.06612341105937958 + 1.0 * 6.296218395233154
Epoch 920, val loss: 0.9857913255691528
Epoch 930, training loss: 6.359846591949463 = 0.06347385793924332 + 1.0 * 6.296372890472412
Epoch 930, val loss: 0.991950273513794
Epoch 940, training loss: 6.35216760635376 = 0.060950081795454025 + 1.0 * 6.29121732711792
Epoch 940, val loss: 0.9978100657463074
Epoch 950, training loss: 6.349399089813232 = 0.05855170637369156 + 1.0 * 6.290847301483154
Epoch 950, val loss: 1.0039653778076172
Epoch 960, training loss: 6.348808765411377 = 0.056266266852617264 + 1.0 * 6.292542457580566
Epoch 960, val loss: 1.0100260972976685
Epoch 970, training loss: 6.347257137298584 = 0.05408810079097748 + 1.0 * 6.293169021606445
Epoch 970, val loss: 1.0160001516342163
Epoch 980, training loss: 6.3425421714782715 = 0.05202136188745499 + 1.0 * 6.290520668029785
Epoch 980, val loss: 1.0221892595291138
Epoch 990, training loss: 6.341404438018799 = 0.050052739679813385 + 1.0 * 6.291351795196533
Epoch 990, val loss: 1.0281885862350464
Epoch 1000, training loss: 6.335960865020752 = 0.04817600175738335 + 1.0 * 6.287785053253174
Epoch 1000, val loss: 1.0341393947601318
Epoch 1010, training loss: 6.3381266593933105 = 0.046390432864427567 + 1.0 * 6.291736125946045
Epoch 1010, val loss: 1.040116548538208
Epoch 1020, training loss: 6.332638263702393 = 0.044690344482660294 + 1.0 * 6.287948131561279
Epoch 1020, val loss: 1.046045184135437
Epoch 1030, training loss: 6.3320770263671875 = 0.04307374730706215 + 1.0 * 6.289003372192383
Epoch 1030, val loss: 1.0519189834594727
Epoch 1040, training loss: 6.328390598297119 = 0.04153415188193321 + 1.0 * 6.286856651306152
Epoch 1040, val loss: 1.0577728748321533
Epoch 1050, training loss: 6.326102256774902 = 0.04006759449839592 + 1.0 * 6.28603458404541
Epoch 1050, val loss: 1.063559651374817
Epoch 1060, training loss: 6.323012351989746 = 0.03866752237081528 + 1.0 * 6.284344673156738
Epoch 1060, val loss: 1.0693540573120117
Epoch 1070, training loss: 6.3217363357543945 = 0.037331514060497284 + 1.0 * 6.284404754638672
Epoch 1070, val loss: 1.075170874595642
Epoch 1080, training loss: 6.322011470794678 = 0.03605572506785393 + 1.0 * 6.285955905914307
Epoch 1080, val loss: 1.0807818174362183
Epoch 1090, training loss: 6.325603485107422 = 0.03484497219324112 + 1.0 * 6.2907586097717285
Epoch 1090, val loss: 1.0863595008850098
Epoch 1100, training loss: 6.31718111038208 = 0.033687956631183624 + 1.0 * 6.2834930419921875
Epoch 1100, val loss: 1.091995358467102
Epoch 1110, training loss: 6.314472675323486 = 0.032587867230176926 + 1.0 * 6.281884670257568
Epoch 1110, val loss: 1.0976415872573853
Epoch 1120, training loss: 6.316765785217285 = 0.031534742563962936 + 1.0 * 6.285231113433838
Epoch 1120, val loss: 1.1029647588729858
Epoch 1130, training loss: 6.309765815734863 = 0.030529247596859932 + 1.0 * 6.279236793518066
Epoch 1130, val loss: 1.108363151550293
Epoch 1140, training loss: 6.309930801391602 = 0.02956695668399334 + 1.0 * 6.280364036560059
Epoch 1140, val loss: 1.1138226985931396
Epoch 1150, training loss: 6.31558895111084 = 0.028648152947425842 + 1.0 * 6.286940574645996
Epoch 1150, val loss: 1.1190975904464722
Epoch 1160, training loss: 6.307121753692627 = 0.027769623324275017 + 1.0 * 6.279352188110352
Epoch 1160, val loss: 1.1242161989212036
Epoch 1170, training loss: 6.3044514656066895 = 0.026931900531053543 + 1.0 * 6.277519702911377
Epoch 1170, val loss: 1.1296683549880981
Epoch 1180, training loss: 6.304596424102783 = 0.02612779289484024 + 1.0 * 6.278468608856201
Epoch 1180, val loss: 1.134751558303833
Epoch 1190, training loss: 6.309816360473633 = 0.02535836212337017 + 1.0 * 6.284458160400391
Epoch 1190, val loss: 1.1396604776382446
Epoch 1200, training loss: 6.300882816314697 = 0.024623386561870575 + 1.0 * 6.276259422302246
Epoch 1200, val loss: 1.1446268558502197
Epoch 1210, training loss: 6.298937797546387 = 0.023922912776470184 + 1.0 * 6.275014877319336
Epoch 1210, val loss: 1.1499255895614624
Epoch 1220, training loss: 6.29756498336792 = 0.02324758656322956 + 1.0 * 6.274317264556885
Epoch 1220, val loss: 1.1547520160675049
Epoch 1230, training loss: 6.299591064453125 = 0.022599155083298683 + 1.0 * 6.276991844177246
Epoch 1230, val loss: 1.1596556901931763
Epoch 1240, training loss: 6.297647476196289 = 0.021976931020617485 + 1.0 * 6.275670528411865
Epoch 1240, val loss: 1.163955569267273
Epoch 1250, training loss: 6.297389030456543 = 0.02138444222509861 + 1.0 * 6.276004791259766
Epoch 1250, val loss: 1.1690939664840698
Epoch 1260, training loss: 6.293910980224609 = 0.020815134048461914 + 1.0 * 6.273095607757568
Epoch 1260, val loss: 1.1739228963851929
Epoch 1270, training loss: 6.291787624359131 = 0.020265823230147362 + 1.0 * 6.27152156829834
Epoch 1270, val loss: 1.1784449815750122
Epoch 1280, training loss: 6.296051502227783 = 0.019735991954803467 + 1.0 * 6.276315689086914
Epoch 1280, val loss: 1.1830271482467651
Epoch 1290, training loss: 6.294655799865723 = 0.019229445606470108 + 1.0 * 6.275426387786865
Epoch 1290, val loss: 1.187169075012207
Epoch 1300, training loss: 6.292040824890137 = 0.018744707107543945 + 1.0 * 6.273295879364014
Epoch 1300, val loss: 1.1920477151870728
Epoch 1310, training loss: 6.288750171661377 = 0.01827695406973362 + 1.0 * 6.270473003387451
Epoch 1310, val loss: 1.1966050863265991
Epoch 1320, training loss: 6.298522472381592 = 0.017826927825808525 + 1.0 * 6.28069543838501
Epoch 1320, val loss: 1.2007083892822266
Epoch 1330, training loss: 6.289229393005371 = 0.017393209040164948 + 1.0 * 6.271836280822754
Epoch 1330, val loss: 1.2050634622573853
Epoch 1340, training loss: 6.286200046539307 = 0.01697624661028385 + 1.0 * 6.269223690032959
Epoch 1340, val loss: 1.20961594581604
Epoch 1350, training loss: 6.287631511688232 = 0.016572723165154457 + 1.0 * 6.271058559417725
Epoch 1350, val loss: 1.2137891054153442
Epoch 1360, training loss: 6.2839674949646 = 0.01618354395031929 + 1.0 * 6.267784118652344
Epoch 1360, val loss: 1.2177305221557617
Epoch 1370, training loss: 6.283074855804443 = 0.015810245648026466 + 1.0 * 6.2672648429870605
Epoch 1370, val loss: 1.22211754322052
Epoch 1380, training loss: 6.2922258377075195 = 0.015450228936970234 + 1.0 * 6.27677583694458
Epoch 1380, val loss: 1.2264481782913208
Epoch 1390, training loss: 6.286859512329102 = 0.015100263990461826 + 1.0 * 6.271759033203125
Epoch 1390, val loss: 1.2297937870025635
Epoch 1400, training loss: 6.283461570739746 = 0.014766941778361797 + 1.0 * 6.2686944007873535
Epoch 1400, val loss: 1.2342467308044434
Epoch 1410, training loss: 6.281108856201172 = 0.014442585408687592 + 1.0 * 6.266666412353516
Epoch 1410, val loss: 1.238250494003296
Epoch 1420, training loss: 6.28640079498291 = 0.014128094539046288 + 1.0 * 6.27227258682251
Epoch 1420, val loss: 1.2418550252914429
Epoch 1430, training loss: 6.2794013023376465 = 0.01382527593523264 + 1.0 * 6.265575885772705
Epoch 1430, val loss: 1.2457919120788574
Epoch 1440, training loss: 6.280606746673584 = 0.013531994074583054 + 1.0 * 6.2670745849609375
Epoch 1440, val loss: 1.2497682571411133
Epoch 1450, training loss: 6.2817277908325195 = 0.013247239403426647 + 1.0 * 6.2684807777404785
Epoch 1450, val loss: 1.2533979415893555
Epoch 1460, training loss: 6.283065319061279 = 0.012973550707101822 + 1.0 * 6.270091533660889
Epoch 1460, val loss: 1.2571676969528198
Epoch 1470, training loss: 6.277298450469971 = 0.012707837857306004 + 1.0 * 6.264590740203857
Epoch 1470, val loss: 1.260504961013794
Epoch 1480, training loss: 6.276750564575195 = 0.012452797032892704 + 1.0 * 6.264297962188721
Epoch 1480, val loss: 1.264465093612671
Epoch 1490, training loss: 6.275441646575928 = 0.012205472216010094 + 1.0 * 6.263236045837402
Epoch 1490, val loss: 1.2682346105575562
Epoch 1500, training loss: 6.274299621582031 = 0.011963828466832638 + 1.0 * 6.262335777282715
Epoch 1500, val loss: 1.2717020511627197
Epoch 1510, training loss: 6.275139808654785 = 0.011729303747415543 + 1.0 * 6.263410568237305
Epoch 1510, val loss: 1.2752702236175537
Epoch 1520, training loss: 6.281148433685303 = 0.011502576991915703 + 1.0 * 6.269645690917969
Epoch 1520, val loss: 1.2784582376480103
Epoch 1530, training loss: 6.274275302886963 = 0.011282694526016712 + 1.0 * 6.2629923820495605
Epoch 1530, val loss: 1.2819442749023438
Epoch 1540, training loss: 6.276604652404785 = 0.011070854030549526 + 1.0 * 6.265533924102783
Epoch 1540, val loss: 1.2855442762374878
Epoch 1550, training loss: 6.2731170654296875 = 0.010864860378205776 + 1.0 * 6.262252330780029
Epoch 1550, val loss: 1.2889142036437988
Epoch 1560, training loss: 6.279685974121094 = 0.01066505629569292 + 1.0 * 6.269021034240723
Epoch 1560, val loss: 1.292054533958435
Epoch 1570, training loss: 6.272773742675781 = 0.010470456443727016 + 1.0 * 6.262303352355957
Epoch 1570, val loss: 1.2952934503555298
Epoch 1580, training loss: 6.2710185050964355 = 0.0102818813174963 + 1.0 * 6.260736465454102
Epoch 1580, val loss: 1.2987810373306274
Epoch 1590, training loss: 6.277538776397705 = 0.010097982361912727 + 1.0 * 6.2674407958984375
Epoch 1590, val loss: 1.3017572164535522
Epoch 1600, training loss: 6.270910263061523 = 0.009920031763613224 + 1.0 * 6.260990142822266
Epoch 1600, val loss: 1.3048622608184814
Epoch 1610, training loss: 6.268827438354492 = 0.009747258387506008 + 1.0 * 6.259080410003662
Epoch 1610, val loss: 1.3083220720291138
Epoch 1620, training loss: 6.269680976867676 = 0.009578756988048553 + 1.0 * 6.260102272033691
Epoch 1620, val loss: 1.311501145362854
Epoch 1630, training loss: 6.274089336395264 = 0.009414320811629295 + 1.0 * 6.264675140380859
Epoch 1630, val loss: 1.3142342567443848
Epoch 1640, training loss: 6.269682884216309 = 0.009254463948309422 + 1.0 * 6.260428428649902
Epoch 1640, val loss: 1.3173489570617676
Epoch 1650, training loss: 6.267920970916748 = 0.009100334718823433 + 1.0 * 6.258820533752441
Epoch 1650, val loss: 1.3206671476364136
Epoch 1660, training loss: 6.272626876831055 = 0.008949236012995243 + 1.0 * 6.263677597045898
Epoch 1660, val loss: 1.3235379457473755
Epoch 1670, training loss: 6.270049571990967 = 0.008802586235105991 + 1.0 * 6.261247158050537
Epoch 1670, val loss: 1.3264230489730835
Epoch 1680, training loss: 6.2655930519104 = 0.008659346029162407 + 1.0 * 6.256933689117432
Epoch 1680, val loss: 1.3293037414550781
Epoch 1690, training loss: 6.267097473144531 = 0.008520481176674366 + 1.0 * 6.2585768699646
Epoch 1690, val loss: 1.3325227499008179
Epoch 1700, training loss: 6.27111291885376 = 0.008385257795453072 + 1.0 * 6.262727737426758
Epoch 1700, val loss: 1.335209608078003
Epoch 1710, training loss: 6.265260219573975 = 0.008252828381955624 + 1.0 * 6.257007598876953
Epoch 1710, val loss: 1.3379911184310913
Epoch 1720, training loss: 6.26391077041626 = 0.008124325424432755 + 1.0 * 6.255786418914795
Epoch 1720, val loss: 1.3411060571670532
Epoch 1730, training loss: 6.264533996582031 = 0.007998140528798103 + 1.0 * 6.25653600692749
Epoch 1730, val loss: 1.3438786268234253
Epoch 1740, training loss: 6.270540714263916 = 0.00787446554750204 + 1.0 * 6.26266622543335
Epoch 1740, val loss: 1.3463889360427856
Epoch 1750, training loss: 6.264068126678467 = 0.007755412720143795 + 1.0 * 6.256312847137451
Epoch 1750, val loss: 1.3491501808166504
Epoch 1760, training loss: 6.264426231384277 = 0.007639287505298853 + 1.0 * 6.256786823272705
Epoch 1760, val loss: 1.3522164821624756
Epoch 1770, training loss: 6.266669273376465 = 0.007525230757892132 + 1.0 * 6.259143829345703
Epoch 1770, val loss: 1.3548061847686768
Epoch 1780, training loss: 6.263358116149902 = 0.007413801737129688 + 1.0 * 6.25594425201416
Epoch 1780, val loss: 1.357496738433838
Epoch 1790, training loss: 6.269040107727051 = 0.007305541075766087 + 1.0 * 6.261734485626221
Epoch 1790, val loss: 1.3601741790771484
Epoch 1800, training loss: 6.262592315673828 = 0.007199924439191818 + 1.0 * 6.255392551422119
Epoch 1800, val loss: 1.3624236583709717
Epoch 1810, training loss: 6.261345863342285 = 0.007097340654581785 + 1.0 * 6.25424861907959
Epoch 1810, val loss: 1.3653512001037598
Epoch 1820, training loss: 6.268640518188477 = 0.006997210439294577 + 1.0 * 6.261643409729004
Epoch 1820, val loss: 1.367878794670105
Epoch 1830, training loss: 6.261236667633057 = 0.0068985591642558575 + 1.0 * 6.254338264465332
Epoch 1830, val loss: 1.3702505826950073
Epoch 1840, training loss: 6.2599711418151855 = 0.00680318009108305 + 1.0 * 6.253168106079102
Epoch 1840, val loss: 1.3731242418289185
Epoch 1850, training loss: 6.260401248931885 = 0.006708753295242786 + 1.0 * 6.253692626953125
Epoch 1850, val loss: 1.3756661415100098
Epoch 1860, training loss: 6.265129566192627 = 0.0066162217408418655 + 1.0 * 6.258513450622559
Epoch 1860, val loss: 1.3778448104858398
Epoch 1870, training loss: 6.260446071624756 = 0.006526280660182238 + 1.0 * 6.25391960144043
Epoch 1870, val loss: 1.3802238702774048
Epoch 1880, training loss: 6.260478973388672 = 0.0064385877922177315 + 1.0 * 6.254040241241455
Epoch 1880, val loss: 1.3831075429916382
Epoch 1890, training loss: 6.266425609588623 = 0.006352995056658983 + 1.0 * 6.260072708129883
Epoch 1890, val loss: 1.385388970375061
Epoch 1900, training loss: 6.25922155380249 = 0.006268912460654974 + 1.0 * 6.252952575683594
Epoch 1900, val loss: 1.387579083442688
Epoch 1910, training loss: 6.2571916580200195 = 0.006187141872942448 + 1.0 * 6.251004695892334
Epoch 1910, val loss: 1.3902424573898315
Epoch 1920, training loss: 6.258714199066162 = 0.006106573157012463 + 1.0 * 6.252607822418213
Epoch 1920, val loss: 1.392699956893921
Epoch 1930, training loss: 6.261651039123535 = 0.006027622614055872 + 1.0 * 6.2556233406066895
Epoch 1930, val loss: 1.3947244882583618
Epoch 1940, training loss: 6.258765697479248 = 0.005949875805526972 + 1.0 * 6.2528157234191895
Epoch 1940, val loss: 1.3969788551330566
Epoch 1950, training loss: 6.256285190582275 = 0.005874243099242449 + 1.0 * 6.250411033630371
Epoch 1950, val loss: 1.3995412588119507
Epoch 1960, training loss: 6.258275508880615 = 0.005800457671284676 + 1.0 * 6.252475261688232
Epoch 1960, val loss: 1.4018012285232544
Epoch 1970, training loss: 6.256685256958008 = 0.005727583542466164 + 1.0 * 6.250957489013672
Epoch 1970, val loss: 1.4038851261138916
Epoch 1980, training loss: 6.266742706298828 = 0.0056566460989415646 + 1.0 * 6.2610859870910645
Epoch 1980, val loss: 1.406014323234558
Epoch 1990, training loss: 6.257370471954346 = 0.005587802268564701 + 1.0 * 6.2517828941345215
Epoch 1990, val loss: 1.4081922769546509
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8186610437532947
=== training gcn model ===
Epoch 0, training loss: 10.555670738220215 = 1.958847165107727 + 1.0 * 8.596823692321777
Epoch 0, val loss: 1.9518533945083618
Epoch 10, training loss: 10.544669151306152 = 1.9481900930404663 + 1.0 * 8.596479415893555
Epoch 10, val loss: 1.9418063163757324
Epoch 20, training loss: 10.528367042541504 = 1.935281753540039 + 1.0 * 8.593085289001465
Epoch 20, val loss: 1.9293301105499268
Epoch 30, training loss: 10.484405517578125 = 1.917736530303955 + 1.0 * 8.566668510437012
Epoch 30, val loss: 1.9121116399765015
Epoch 40, training loss: 10.309370040893555 = 1.8952720165252686 + 1.0 * 8.414097785949707
Epoch 40, val loss: 1.890459418296814
Epoch 50, training loss: 9.849679946899414 = 1.8710033893585205 + 1.0 * 7.9786763191223145
Epoch 50, val loss: 1.867282748222351
Epoch 60, training loss: 9.512406349182129 = 1.8512948751449585 + 1.0 * 7.661111831665039
Epoch 60, val loss: 1.8496533632278442
Epoch 70, training loss: 9.0412015914917 = 1.835004448890686 + 1.0 * 7.206197261810303
Epoch 70, val loss: 1.8352421522140503
Epoch 80, training loss: 8.763381004333496 = 1.8210090398788452 + 1.0 * 6.942371845245361
Epoch 80, val loss: 1.8228780031204224
Epoch 90, training loss: 8.636229515075684 = 1.804975986480713 + 1.0 * 6.831253528594971
Epoch 90, val loss: 1.8081257343292236
Epoch 100, training loss: 8.55910587310791 = 1.7870116233825684 + 1.0 * 6.772094249725342
Epoch 100, val loss: 1.792100191116333
Epoch 110, training loss: 8.494129180908203 = 1.7716292142868042 + 1.0 * 6.722500324249268
Epoch 110, val loss: 1.779335379600525
Epoch 120, training loss: 8.429267883300781 = 1.7581117153167725 + 1.0 * 6.67115592956543
Epoch 120, val loss: 1.7681159973144531
Epoch 130, training loss: 8.372781753540039 = 1.7438836097717285 + 1.0 * 6.628897666931152
Epoch 130, val loss: 1.7560573816299438
Epoch 140, training loss: 8.320762634277344 = 1.7280523777008057 + 1.0 * 6.592710494995117
Epoch 140, val loss: 1.7426596879959106
Epoch 150, training loss: 8.270930290222168 = 1.7102336883544922 + 1.0 * 6.560696601867676
Epoch 150, val loss: 1.7280393838882446
Epoch 160, training loss: 8.223840713500977 = 1.6899179220199585 + 1.0 * 6.5339226722717285
Epoch 160, val loss: 1.7115991115570068
Epoch 170, training loss: 8.175952911376953 = 1.6663899421691895 + 1.0 * 6.509562969207764
Epoch 170, val loss: 1.6926852464675903
Epoch 180, training loss: 8.13089370727539 = 1.638747215270996 + 1.0 * 6.492146968841553
Epoch 180, val loss: 1.6702884435653687
Epoch 190, training loss: 8.082708358764648 = 1.6065704822540283 + 1.0 * 6.476138114929199
Epoch 190, val loss: 1.644060492515564
Epoch 200, training loss: 8.0357084274292 = 1.5696907043457031 + 1.0 * 6.466017723083496
Epoch 200, val loss: 1.6140415668487549
Epoch 210, training loss: 7.978716850280762 = 1.5285170078277588 + 1.0 * 6.450199604034424
Epoch 210, val loss: 1.5805586576461792
Epoch 220, training loss: 7.922313213348389 = 1.4830317497253418 + 1.0 * 6.439281463623047
Epoch 220, val loss: 1.5436211824417114
Epoch 230, training loss: 7.871181011199951 = 1.433717131614685 + 1.0 * 6.437463760375977
Epoch 230, val loss: 1.5038822889328003
Epoch 240, training loss: 7.807829856872559 = 1.383065938949585 + 1.0 * 6.424764156341553
Epoch 240, val loss: 1.463639497756958
Epoch 250, training loss: 7.748705863952637 = 1.3315232992172241 + 1.0 * 6.417182445526123
Epoch 250, val loss: 1.4229793548583984
Epoch 260, training loss: 7.68943977355957 = 1.279586672782898 + 1.0 * 6.409852981567383
Epoch 260, val loss: 1.3822771310806274
Epoch 270, training loss: 7.635263442993164 = 1.2279891967773438 + 1.0 * 6.40727424621582
Epoch 270, val loss: 1.342568278312683
Epoch 280, training loss: 7.580991268157959 = 1.1790634393692017 + 1.0 * 6.401927947998047
Epoch 280, val loss: 1.3058598041534424
Epoch 290, training loss: 7.5277204513549805 = 1.132836103439331 + 1.0 * 6.3948845863342285
Epoch 290, val loss: 1.2720141410827637
Epoch 300, training loss: 7.478728294372559 = 1.089178204536438 + 1.0 * 6.38955020904541
Epoch 300, val loss: 1.2409124374389648
Epoch 310, training loss: 7.447197914123535 = 1.0480163097381592 + 1.0 * 6.399181842803955
Epoch 310, val loss: 1.2125862836837769
Epoch 320, training loss: 7.392179012298584 = 1.0099023580551147 + 1.0 * 6.38227653503418
Epoch 320, val loss: 1.1874732971191406
Epoch 330, training loss: 7.351553440093994 = 0.9739180207252502 + 1.0 * 6.377635478973389
Epoch 330, val loss: 1.1649469137191772
Epoch 340, training loss: 7.322302341461182 = 0.9396036267280579 + 1.0 * 6.3826985359191895
Epoch 340, val loss: 1.1442347764968872
Epoch 350, training loss: 7.2782440185546875 = 0.9067457318305969 + 1.0 * 6.371498107910156
Epoch 350, val loss: 1.1253747940063477
Epoch 360, training loss: 7.240143299102783 = 0.8745861053466797 + 1.0 * 6.3655571937561035
Epoch 360, val loss: 1.107722282409668
Epoch 370, training loss: 7.204744815826416 = 0.842717170715332 + 1.0 * 6.362027645111084
Epoch 370, val loss: 1.0908113718032837
Epoch 380, training loss: 7.1805291175842285 = 0.810912549495697 + 1.0 * 6.369616508483887
Epoch 380, val loss: 1.074563980102539
Epoch 390, training loss: 7.138710975646973 = 0.7795594334602356 + 1.0 * 6.359151363372803
Epoch 390, val loss: 1.059072732925415
Epoch 400, training loss: 7.1035308837890625 = 0.7484229803085327 + 1.0 * 6.35510778427124
Epoch 400, val loss: 1.0444245338439941
Epoch 410, training loss: 7.074573040008545 = 0.7174081206321716 + 1.0 * 6.3571648597717285
Epoch 410, val loss: 1.030172348022461
Epoch 420, training loss: 7.038134574890137 = 0.6865650415420532 + 1.0 * 6.351569652557373
Epoch 420, val loss: 1.0163663625717163
Epoch 430, training loss: 7.004030227661133 = 0.6560885310173035 + 1.0 * 6.347941875457764
Epoch 430, val loss: 1.0031909942626953
Epoch 440, training loss: 6.972177505493164 = 0.6258224844932556 + 1.0 * 6.346354961395264
Epoch 440, val loss: 0.9901869297027588
Epoch 450, training loss: 6.9398884773254395 = 0.5960716009140015 + 1.0 * 6.343816757202148
Epoch 450, val loss: 0.9777773022651672
Epoch 460, training loss: 6.909107208251953 = 0.5669192671775818 + 1.0 * 6.342187881469727
Epoch 460, val loss: 0.9659838676452637
Epoch 470, training loss: 6.87904691696167 = 0.5383795499801636 + 1.0 * 6.340667247772217
Epoch 470, val loss: 0.9544501304626465
Epoch 480, training loss: 6.848289966583252 = 0.5106648802757263 + 1.0 * 6.337625026702881
Epoch 480, val loss: 0.9436743855476379
Epoch 490, training loss: 6.82070255279541 = 0.48380762338638306 + 1.0 * 6.336894989013672
Epoch 490, val loss: 0.9334501028060913
Epoch 500, training loss: 6.79173469543457 = 0.4579588770866394 + 1.0 * 6.333775997161865
Epoch 500, val loss: 0.9238861799240112
Epoch 510, training loss: 6.766048431396484 = 0.4331085681915283 + 1.0 * 6.332940101623535
Epoch 510, val loss: 0.9151302576065063
Epoch 520, training loss: 6.738206386566162 = 0.4092716872692108 + 1.0 * 6.328934669494629
Epoch 520, val loss: 0.9071363806724548
Epoch 530, training loss: 6.7125749588012695 = 0.3863210380077362 + 1.0 * 6.326253890991211
Epoch 530, val loss: 0.8997885584831238
Epoch 540, training loss: 6.697958469390869 = 0.3643580377101898 + 1.0 * 6.3336005210876465
Epoch 540, val loss: 0.8931545615196228
Epoch 550, training loss: 6.668519973754883 = 0.34363508224487305 + 1.0 * 6.32488489151001
Epoch 550, val loss: 0.8877867460250854
Epoch 560, training loss: 6.6461181640625 = 0.3238871693611145 + 1.0 * 6.322230815887451
Epoch 560, val loss: 0.88325035572052
Epoch 570, training loss: 6.625553607940674 = 0.3049973249435425 + 1.0 * 6.320556163787842
Epoch 570, val loss: 0.8793846964836121
Epoch 580, training loss: 6.6088032722473145 = 0.28693732619285583 + 1.0 * 6.321866035461426
Epoch 580, val loss: 0.8764768838882446
Epoch 590, training loss: 6.5934953689575195 = 0.26979899406433105 + 1.0 * 6.323696613311768
Epoch 590, val loss: 0.8741552233695984
Epoch 600, training loss: 6.570627212524414 = 0.25363481044769287 + 1.0 * 6.316992282867432
Epoch 600, val loss: 0.8730169534683228
Epoch 610, training loss: 6.552950382232666 = 0.23826435208320618 + 1.0 * 6.314685821533203
Epoch 610, val loss: 0.8724752068519592
Epoch 620, training loss: 6.540885925292969 = 0.22366595268249512 + 1.0 * 6.3172197341918945
Epoch 620, val loss: 0.8726519346237183
Epoch 630, training loss: 6.528512001037598 = 0.2099415510892868 + 1.0 * 6.318570613861084
Epoch 630, val loss: 0.8733375072479248
Epoch 640, training loss: 6.515573978424072 = 0.197055384516716 + 1.0 * 6.31851863861084
Epoch 640, val loss: 0.8748920559883118
Epoch 650, training loss: 6.4958014488220215 = 0.1849968284368515 + 1.0 * 6.310804843902588
Epoch 650, val loss: 0.8766867518424988
Epoch 660, training loss: 6.482135772705078 = 0.1737356185913086 + 1.0 * 6.3084001541137695
Epoch 660, val loss: 0.8793504238128662
Epoch 670, training loss: 6.471828460693359 = 0.16318544745445251 + 1.0 * 6.308642864227295
Epoch 670, val loss: 0.8823835849761963
Epoch 680, training loss: 6.461179256439209 = 0.15330994129180908 + 1.0 * 6.3078694343566895
Epoch 680, val loss: 0.8858384490013123
Epoch 690, training loss: 6.452280521392822 = 0.1441040337085724 + 1.0 * 6.308176517486572
Epoch 690, val loss: 0.8897331357002258
Epoch 700, training loss: 6.443641185760498 = 0.13555458188056946 + 1.0 * 6.308086395263672
Epoch 700, val loss: 0.8938528299331665
Epoch 710, training loss: 6.432907581329346 = 0.1276346892118454 + 1.0 * 6.305273056030273
Epoch 710, val loss: 0.8983413577079773
Epoch 720, training loss: 6.4230637550354 = 0.12026884406805038 + 1.0 * 6.302794933319092
Epoch 720, val loss: 0.9031122922897339
Epoch 730, training loss: 6.421258449554443 = 0.11340736597776413 + 1.0 * 6.307851314544678
Epoch 730, val loss: 0.9079496264457703
Epoch 740, training loss: 6.408990383148193 = 0.10705514252185822 + 1.0 * 6.301935195922852
Epoch 740, val loss: 0.913207471370697
Epoch 750, training loss: 6.402388095855713 = 0.10113686323165894 + 1.0 * 6.301251411437988
Epoch 750, val loss: 0.9186035990715027
Epoch 760, training loss: 6.393373966217041 = 0.09564053267240524 + 1.0 * 6.297733306884766
Epoch 760, val loss: 0.924079179763794
Epoch 770, training loss: 6.387857913970947 = 0.0905168205499649 + 1.0 * 6.2973408699035645
Epoch 770, val loss: 0.9297598600387573
Epoch 780, training loss: 6.400085926055908 = 0.08574943244457245 + 1.0 * 6.31433629989624
Epoch 780, val loss: 0.9355320930480957
Epoch 790, training loss: 6.378696918487549 = 0.08131806552410126 + 1.0 * 6.297379016876221
Epoch 790, val loss: 0.9412518739700317
Epoch 800, training loss: 6.372921466827393 = 0.07721295952796936 + 1.0 * 6.295708656311035
Epoch 800, val loss: 0.9473065137863159
Epoch 810, training loss: 6.367733478546143 = 0.07337019592523575 + 1.0 * 6.294363498687744
Epoch 810, val loss: 0.9532803893089294
Epoch 820, training loss: 6.363805770874023 = 0.06978162378072739 + 1.0 * 6.2940239906311035
Epoch 820, val loss: 0.9591965079307556
Epoch 830, training loss: 6.358565807342529 = 0.06643461436033249 + 1.0 * 6.292131423950195
Epoch 830, val loss: 0.9653597474098206
Epoch 840, training loss: 6.36133337020874 = 0.06330299377441406 + 1.0 * 6.298030376434326
Epoch 840, val loss: 0.9713712334632874
Epoch 850, training loss: 6.350232124328613 = 0.06038406118750572 + 1.0 * 6.2898478507995605
Epoch 850, val loss: 0.9775584936141968
Epoch 860, training loss: 6.347287654876709 = 0.05764460936188698 + 1.0 * 6.289642810821533
Epoch 860, val loss: 0.9837681651115417
Epoch 870, training loss: 6.348506450653076 = 0.05506731942296028 + 1.0 * 6.293438911437988
Epoch 870, val loss: 0.9898391366004944
Epoch 880, training loss: 6.3444600105285645 = 0.05265173316001892 + 1.0 * 6.291808128356934
Epoch 880, val loss: 0.9959378838539124
Epoch 890, training loss: 6.338281154632568 = 0.05039601027965546 + 1.0 * 6.2878851890563965
Epoch 890, val loss: 1.0021218061447144
Epoch 900, training loss: 6.334738254547119 = 0.04827023670077324 + 1.0 * 6.286468029022217
Epoch 900, val loss: 1.0083204507827759
Epoch 910, training loss: 6.347278118133545 = 0.046272631734609604 + 1.0 * 6.3010053634643555
Epoch 910, val loss: 1.0143845081329346
Epoch 920, training loss: 6.333557605743408 = 0.04438639059662819 + 1.0 * 6.28917121887207
Epoch 920, val loss: 1.0202122926712036
Epoch 930, training loss: 6.327400207519531 = 0.042625121772289276 + 1.0 * 6.284775257110596
Epoch 930, val loss: 1.0263868570327759
Epoch 940, training loss: 6.325197219848633 = 0.040949948132038116 + 1.0 * 6.284247398376465
Epoch 940, val loss: 1.0322182178497314
Epoch 950, training loss: 6.328319072723389 = 0.03936934098601341 + 1.0 * 6.288949966430664
Epoch 950, val loss: 1.0380053520202637
Epoch 960, training loss: 6.321442127227783 = 0.037877097725868225 + 1.0 * 6.283565044403076
Epoch 960, val loss: 1.0438989400863647
Epoch 970, training loss: 6.318799018859863 = 0.03646506369113922 + 1.0 * 6.282333850860596
Epoch 970, val loss: 1.0497294664382935
Epoch 980, training loss: 6.326610565185547 = 0.03512416034936905 + 1.0 * 6.2914862632751465
Epoch 980, val loss: 1.0552743673324585
Epoch 990, training loss: 6.314281463623047 = 0.033866941928863525 + 1.0 * 6.280414581298828
Epoch 990, val loss: 1.0609060525894165
Epoch 1000, training loss: 6.312840461730957 = 0.03267817571759224 + 1.0 * 6.280162334442139
Epoch 1000, val loss: 1.0667071342468262
Epoch 1010, training loss: 6.309476375579834 = 0.03154400736093521 + 1.0 * 6.277932167053223
Epoch 1010, val loss: 1.0722427368164062
Epoch 1020, training loss: 6.307384490966797 = 0.030462486669421196 + 1.0 * 6.276922225952148
Epoch 1020, val loss: 1.0776493549346924
Epoch 1030, training loss: 6.324713706970215 = 0.029430581256747246 + 1.0 * 6.295283317565918
Epoch 1030, val loss: 1.0828319787979126
Epoch 1040, training loss: 6.306591033935547 = 0.028460724279284477 + 1.0 * 6.278130531311035
Epoch 1040, val loss: 1.0881417989730835
Epoch 1050, training loss: 6.305409908294678 = 0.027542511001229286 + 1.0 * 6.277867317199707
Epoch 1050, val loss: 1.093762755393982
Epoch 1060, training loss: 6.301356315612793 = 0.026664646342396736 + 1.0 * 6.274691581726074
Epoch 1060, val loss: 1.0989811420440674
Epoch 1070, training loss: 6.302381992340088 = 0.025822926312685013 + 1.0 * 6.276558876037598
Epoch 1070, val loss: 1.1039843559265137
Epoch 1080, training loss: 6.304983615875244 = 0.025021275505423546 + 1.0 * 6.279962539672852
Epoch 1080, val loss: 1.1089247465133667
Epoch 1090, training loss: 6.2974724769592285 = 0.024256868287920952 + 1.0 * 6.2732157707214355
Epoch 1090, val loss: 1.1139322519302368
Epoch 1100, training loss: 6.295984268188477 = 0.023529943078756332 + 1.0 * 6.272454261779785
Epoch 1100, val loss: 1.1190214157104492
Epoch 1110, training loss: 6.295375347137451 = 0.022831670939922333 + 1.0 * 6.272543907165527
Epoch 1110, val loss: 1.1238324642181396
Epoch 1120, training loss: 6.3119378089904785 = 0.02216593362390995 + 1.0 * 6.289772033691406
Epoch 1120, val loss: 1.1285570859909058
Epoch 1130, training loss: 6.29502010345459 = 0.02152920328080654 + 1.0 * 6.273490905761719
Epoch 1130, val loss: 1.1330596208572388
Epoch 1140, training loss: 6.293063163757324 = 0.020927170291543007 + 1.0 * 6.272136211395264
Epoch 1140, val loss: 1.1381117105484009
Epoch 1150, training loss: 6.2899885177612305 = 0.020346395671367645 + 1.0 * 6.269642353057861
Epoch 1150, val loss: 1.1427110433578491
Epoch 1160, training loss: 6.289098739624023 = 0.01978493668138981 + 1.0 * 6.269313812255859
Epoch 1160, val loss: 1.1471514701843262
Epoch 1170, training loss: 6.304398059844971 = 0.019244736060500145 + 1.0 * 6.285153388977051
Epoch 1170, val loss: 1.1514825820922852
Epoch 1180, training loss: 6.2912187576293945 = 0.018732085824012756 + 1.0 * 6.272486686706543
Epoch 1180, val loss: 1.1557514667510986
Epoch 1190, training loss: 6.288846015930176 = 0.018245235085487366 + 1.0 * 6.27060079574585
Epoch 1190, val loss: 1.1604862213134766
Epoch 1200, training loss: 6.285327911376953 = 0.01777452789247036 + 1.0 * 6.267553329467773
Epoch 1200, val loss: 1.1648422479629517
Epoch 1210, training loss: 6.285812854766846 = 0.017318543046712875 + 1.0 * 6.268494129180908
Epoch 1210, val loss: 1.1689696311950684
Epoch 1220, training loss: 6.290390968322754 = 0.016878699883818626 + 1.0 * 6.273512363433838
Epoch 1220, val loss: 1.1730518341064453
Epoch 1230, training loss: 6.2819647789001465 = 0.0164590235799551 + 1.0 * 6.265505790710449
Epoch 1230, val loss: 1.1773614883422852
Epoch 1240, training loss: 6.285092353820801 = 0.016054421663284302 + 1.0 * 6.26903772354126
Epoch 1240, val loss: 1.1816010475158691
Epoch 1250, training loss: 6.2859907150268555 = 0.01566264033317566 + 1.0 * 6.270328044891357
Epoch 1250, val loss: 1.1855179071426392
Epoch 1260, training loss: 6.283614158630371 = 0.01528745424002409 + 1.0 * 6.268326759338379
Epoch 1260, val loss: 1.1893314123153687
Epoch 1270, training loss: 6.280388832092285 = 0.014929566532373428 + 1.0 * 6.265459060668945
Epoch 1270, val loss: 1.1935416460037231
Epoch 1280, training loss: 6.279230117797852 = 0.014582449570298195 + 1.0 * 6.264647483825684
Epoch 1280, val loss: 1.1974605321884155
Epoch 1290, training loss: 6.289705276489258 = 0.014246981590986252 + 1.0 * 6.275458335876465
Epoch 1290, val loss: 1.2012462615966797
Epoch 1300, training loss: 6.2823076248168945 = 0.013923152349889278 + 1.0 * 6.2683844566345215
Epoch 1300, val loss: 1.2048512697219849
Epoch 1310, training loss: 6.2771501541137695 = 0.013612967915832996 + 1.0 * 6.263537406921387
Epoch 1310, val loss: 1.2087841033935547
Epoch 1320, training loss: 6.280111789703369 = 0.01331298053264618 + 1.0 * 6.266798973083496
Epoch 1320, val loss: 1.2125449180603027
Epoch 1330, training loss: 6.28115177154541 = 0.013021574355661869 + 1.0 * 6.268130302429199
Epoch 1330, val loss: 1.2160035371780396
Epoch 1340, training loss: 6.2738118171691895 = 0.012741023674607277 + 1.0 * 6.261070728302002
Epoch 1340, val loss: 1.219552993774414
Epoch 1350, training loss: 6.274814128875732 = 0.012470620684325695 + 1.0 * 6.262343406677246
Epoch 1350, val loss: 1.2232508659362793
Epoch 1360, training loss: 6.276180267333984 = 0.012206947430968285 + 1.0 * 6.263973236083984
Epoch 1360, val loss: 1.2266747951507568
Epoch 1370, training loss: 6.276459693908691 = 0.0119526581838727 + 1.0 * 6.264506816864014
Epoch 1370, val loss: 1.2300714254379272
Epoch 1380, training loss: 6.276122093200684 = 0.011707266792654991 + 1.0 * 6.2644147872924805
Epoch 1380, val loss: 1.2335944175720215
Epoch 1390, training loss: 6.271200656890869 = 0.011470022611320019 + 1.0 * 6.259730815887451
Epoch 1390, val loss: 1.2369848489761353
Epoch 1400, training loss: 6.272253036499023 = 0.011240408755838871 + 1.0 * 6.261012554168701
Epoch 1400, val loss: 1.2403883934020996
Epoch 1410, training loss: 6.2764763832092285 = 0.011017519980669022 + 1.0 * 6.265459060668945
Epoch 1410, val loss: 1.2435814142227173
Epoch 1420, training loss: 6.272089004516602 = 0.01080191507935524 + 1.0 * 6.261287212371826
Epoch 1420, val loss: 1.2468597888946533
Epoch 1430, training loss: 6.275345802307129 = 0.010594646446406841 + 1.0 * 6.264750957489014
Epoch 1430, val loss: 1.2501225471496582
Epoch 1440, training loss: 6.268944263458252 = 0.01039271242916584 + 1.0 * 6.258551597595215
Epoch 1440, val loss: 1.2532322406768799
Epoch 1450, training loss: 6.268509387969971 = 0.01019696518778801 + 1.0 * 6.258312225341797
Epoch 1450, val loss: 1.256442904472351
Epoch 1460, training loss: 6.27107048034668 = 0.010005701333284378 + 1.0 * 6.2610650062561035
Epoch 1460, val loss: 1.2594748735427856
Epoch 1470, training loss: 6.267642021179199 = 0.0098203644156456 + 1.0 * 6.257821559906006
Epoch 1470, val loss: 1.2624857425689697
Epoch 1480, training loss: 6.269839286804199 = 0.009641045704483986 + 1.0 * 6.26019811630249
Epoch 1480, val loss: 1.2656129598617554
Epoch 1490, training loss: 6.271373748779297 = 0.009466404095292091 + 1.0 * 6.261907577514648
Epoch 1490, val loss: 1.2685456275939941
Epoch 1500, training loss: 6.269914627075195 = 0.009297758340835571 + 1.0 * 6.260616779327393
Epoch 1500, val loss: 1.2714284658432007
Epoch 1510, training loss: 6.26573371887207 = 0.009134206920862198 + 1.0 * 6.256599426269531
Epoch 1510, val loss: 1.2743641138076782
Epoch 1520, training loss: 6.264186382293701 = 0.008975448086857796 + 1.0 * 6.255210876464844
Epoch 1520, val loss: 1.277376413345337
Epoch 1530, training loss: 6.266690254211426 = 0.008819695562124252 + 1.0 * 6.257870674133301
Epoch 1530, val loss: 1.2801803350448608
Epoch 1540, training loss: 6.270638465881348 = 0.008668128401041031 + 1.0 * 6.261970520019531
Epoch 1540, val loss: 1.2829140424728394
Epoch 1550, training loss: 6.262518405914307 = 0.008521337062120438 + 1.0 * 6.253996849060059
Epoch 1550, val loss: 1.2853184938430786
Epoch 1560, training loss: 6.262165546417236 = 0.008380268700420856 + 1.0 * 6.253785133361816
Epoch 1560, val loss: 1.2883045673370361
Epoch 1570, training loss: 6.26185417175293 = 0.008243279531598091 + 1.0 * 6.253611087799072
Epoch 1570, val loss: 1.2912487983703613
Epoch 1580, training loss: 6.260268688201904 = 0.008106876164674759 + 1.0 * 6.252161979675293
Epoch 1580, val loss: 1.2938041687011719
Epoch 1590, training loss: 6.274059295654297 = 0.00797390565276146 + 1.0 * 6.266085624694824
Epoch 1590, val loss: 1.2963703870773315
Epoch 1600, training loss: 6.265964984893799 = 0.00784520898014307 + 1.0 * 6.258119583129883
Epoch 1600, val loss: 1.2986736297607422
Epoch 1610, training loss: 6.260272979736328 = 0.00772187439724803 + 1.0 * 6.252551078796387
Epoch 1610, val loss: 1.301459550857544
Epoch 1620, training loss: 6.2581939697265625 = 0.007601032964885235 + 1.0 * 6.2505927085876465
Epoch 1620, val loss: 1.3042060136795044
Epoch 1630, training loss: 6.259625434875488 = 0.007481323089450598 + 1.0 * 6.2521443367004395
Epoch 1630, val loss: 1.3066246509552002
Epoch 1640, training loss: 6.267647743225098 = 0.007364396005868912 + 1.0 * 6.260283470153809
Epoch 1640, val loss: 1.3087092638015747
Epoch 1650, training loss: 6.257933616638184 = 0.007253992836922407 + 1.0 * 6.2506794929504395
Epoch 1650, val loss: 1.3111927509307861
Epoch 1660, training loss: 6.256649494171143 = 0.007146046031266451 + 1.0 * 6.2495036125183105
Epoch 1660, val loss: 1.3140339851379395
Epoch 1670, training loss: 6.256155490875244 = 0.007039173506200314 + 1.0 * 6.24911642074585
Epoch 1670, val loss: 1.3164163827896118
Epoch 1680, training loss: 6.257406711578369 = 0.006933421827852726 + 1.0 * 6.250473499298096
Epoch 1680, val loss: 1.3186832666397095
Epoch 1690, training loss: 6.262334823608398 = 0.006830350961536169 + 1.0 * 6.255504608154297
Epoch 1690, val loss: 1.3208563327789307
Epoch 1700, training loss: 6.258081912994385 = 0.006730631925165653 + 1.0 * 6.251351356506348
Epoch 1700, val loss: 1.323149561882019
Epoch 1710, training loss: 6.258485794067383 = 0.006633988581597805 + 1.0 * 6.251852035522461
Epoch 1710, val loss: 1.3255765438079834
Epoch 1720, training loss: 6.261222839355469 = 0.006539507769048214 + 1.0 * 6.254683494567871
Epoch 1720, val loss: 1.3277665376663208
Epoch 1730, training loss: 6.254901885986328 = 0.006446843035519123 + 1.0 * 6.248455047607422
Epoch 1730, val loss: 1.3299447298049927
Epoch 1740, training loss: 6.254786491394043 = 0.006356554105877876 + 1.0 * 6.248429775238037
Epoch 1740, val loss: 1.3321974277496338
Epoch 1750, training loss: 6.260700225830078 = 0.006267260760068893 + 1.0 * 6.2544331550598145
Epoch 1750, val loss: 1.3343013525009155
Epoch 1760, training loss: 6.256054401397705 = 0.006180984899401665 + 1.0 * 6.249873638153076
Epoch 1760, val loss: 1.3362957239151
Epoch 1770, training loss: 6.256049633026123 = 0.006097331643104553 + 1.0 * 6.24995231628418
Epoch 1770, val loss: 1.3385450839996338
Epoch 1780, training loss: 6.253670692443848 = 0.006014915648847818 + 1.0 * 6.247655868530273
Epoch 1780, val loss: 1.3406935930252075
Epoch 1790, training loss: 6.255251884460449 = 0.005934237502515316 + 1.0 * 6.249317646026611
Epoch 1790, val loss: 1.3427544832229614
Epoch 1800, training loss: 6.253018379211426 = 0.0058550708927214146 + 1.0 * 6.24716329574585
Epoch 1800, val loss: 1.3447133302688599
Epoch 1810, training loss: 6.2552266120910645 = 0.005777732469141483 + 1.0 * 6.249448776245117
Epoch 1810, val loss: 1.346861720085144
Epoch 1820, training loss: 6.255788326263428 = 0.0057020531967282295 + 1.0 * 6.250086307525635
Epoch 1820, val loss: 1.348637342453003
Epoch 1830, training loss: 6.2524094581604 = 0.005628436338156462 + 1.0 * 6.246780872344971
Epoch 1830, val loss: 1.350643515586853
Epoch 1840, training loss: 6.253200054168701 = 0.005557156167924404 + 1.0 * 6.247642993927002
Epoch 1840, val loss: 1.3527899980545044
Epoch 1850, training loss: 6.255026340484619 = 0.005486640147864819 + 1.0 * 6.249539852142334
Epoch 1850, val loss: 1.354643702507019
Epoch 1860, training loss: 6.2499847412109375 = 0.005417188163846731 + 1.0 * 6.244567394256592
Epoch 1860, val loss: 1.3564373254776
Epoch 1870, training loss: 6.250865936279297 = 0.005349188111722469 + 1.0 * 6.245516777038574
Epoch 1870, val loss: 1.3584457635879517
Epoch 1880, training loss: 6.25391960144043 = 0.005282146856188774 + 1.0 * 6.248637676239014
Epoch 1880, val loss: 1.3602873086929321
Epoch 1890, training loss: 6.250550270080566 = 0.005216655787080526 + 1.0 * 6.245333671569824
Epoch 1890, val loss: 1.3620790243148804
Epoch 1900, training loss: 6.257104396820068 = 0.0051525915041565895 + 1.0 * 6.251951694488525
Epoch 1900, val loss: 1.3637762069702148
Epoch 1910, training loss: 6.252410411834717 = 0.005090770311653614 + 1.0 * 6.24731969833374
Epoch 1910, val loss: 1.3654850721359253
Epoch 1920, training loss: 6.248837471008301 = 0.005031202454119921 + 1.0 * 6.2438063621521
Epoch 1920, val loss: 1.3675110340118408
Epoch 1930, training loss: 6.249163627624512 = 0.004971958231180906 + 1.0 * 6.244191646575928
Epoch 1930, val loss: 1.369428038597107
Epoch 1940, training loss: 6.2549543380737305 = 0.004913115408271551 + 1.0 * 6.2500410079956055
Epoch 1940, val loss: 1.3710609674453735
Epoch 1950, training loss: 6.250118732452393 = 0.004855017643421888 + 1.0 * 6.245263576507568
Epoch 1950, val loss: 1.3726764917373657
Epoch 1960, training loss: 6.25214147567749 = 0.00479877507314086 + 1.0 * 6.247342586517334
Epoch 1960, val loss: 1.374449610710144
Epoch 1970, training loss: 6.249126434326172 = 0.004743481520563364 + 1.0 * 6.244382858276367
Epoch 1970, val loss: 1.3760578632354736
Epoch 1980, training loss: 6.248507022857666 = 0.004689909517765045 + 1.0 * 6.243817329406738
Epoch 1980, val loss: 1.3779276609420776
Epoch 1990, training loss: 6.247774124145508 = 0.004636488854885101 + 1.0 * 6.243137836456299
Epoch 1990, val loss: 1.379561185836792
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 10.546527862548828 = 1.9496824741363525 + 1.0 * 8.596845626831055
Epoch 0, val loss: 1.9538673162460327
Epoch 10, training loss: 10.536349296569824 = 1.9397658109664917 + 1.0 * 8.596583366394043
Epoch 10, val loss: 1.9434629678726196
Epoch 20, training loss: 10.521404266357422 = 1.927000880241394 + 1.0 * 8.594403266906738
Epoch 20, val loss: 1.9298516511917114
Epoch 30, training loss: 10.48583984375 = 1.9086029529571533 + 1.0 * 8.577237129211426
Epoch 30, val loss: 1.9102457761764526
Epoch 40, training loss: 10.363719940185547 = 1.8836063146591187 + 1.0 * 8.480113983154297
Epoch 40, val loss: 1.8846887350082397
Epoch 50, training loss: 9.939414978027344 = 1.8557050228118896 + 1.0 * 8.083709716796875
Epoch 50, val loss: 1.8572779893875122
Epoch 60, training loss: 9.660310745239258 = 1.8323124647140503 + 1.0 * 7.827998638153076
Epoch 60, val loss: 1.8364455699920654
Epoch 70, training loss: 9.189726829528809 = 1.8185409307479858 + 1.0 * 7.371185779571533
Epoch 70, val loss: 1.8242175579071045
Epoch 80, training loss: 8.810527801513672 = 1.8091009855270386 + 1.0 * 7.001426696777344
Epoch 80, val loss: 1.8152029514312744
Epoch 90, training loss: 8.647832870483398 = 1.7984509468078613 + 1.0 * 6.849381446838379
Epoch 90, val loss: 1.8038839101791382
Epoch 100, training loss: 8.546027183532715 = 1.7839815616607666 + 1.0 * 6.762045860290527
Epoch 100, val loss: 1.7898763418197632
Epoch 110, training loss: 8.469795227050781 = 1.7698622941970825 + 1.0 * 6.69993257522583
Epoch 110, val loss: 1.776690125465393
Epoch 120, training loss: 8.409378051757812 = 1.7561289072036743 + 1.0 * 6.653249263763428
Epoch 120, val loss: 1.7640072107315063
Epoch 130, training loss: 8.353095054626465 = 1.741125226020813 + 1.0 * 6.611969947814941
Epoch 130, val loss: 1.7503129243850708
Epoch 140, training loss: 8.300820350646973 = 1.7239571809768677 + 1.0 * 6.5768632888793945
Epoch 140, val loss: 1.7349573373794556
Epoch 150, training loss: 8.252421379089355 = 1.7042642831802368 + 1.0 * 6.548157215118408
Epoch 150, val loss: 1.7176992893218994
Epoch 160, training loss: 8.204072952270508 = 1.6815659999847412 + 1.0 * 6.522507190704346
Epoch 160, val loss: 1.6978912353515625
Epoch 170, training loss: 8.159430503845215 = 1.655290961265564 + 1.0 * 6.5041399002075195
Epoch 170, val loss: 1.6749868392944336
Epoch 180, training loss: 8.112234115600586 = 1.6252557039260864 + 1.0 * 6.486978054046631
Epoch 180, val loss: 1.6486955881118774
Epoch 190, training loss: 8.062895774841309 = 1.5911476612091064 + 1.0 * 6.471747875213623
Epoch 190, val loss: 1.6187306642532349
Epoch 200, training loss: 8.012758255004883 = 1.5525550842285156 + 1.0 * 6.460203647613525
Epoch 200, val loss: 1.5846812725067139
Epoch 210, training loss: 7.960407733917236 = 1.509933590888977 + 1.0 * 6.450474262237549
Epoch 210, val loss: 1.547200322151184
Epoch 220, training loss: 7.902021408081055 = 1.4639908075332642 + 1.0 * 6.43803071975708
Epoch 220, val loss: 1.506678581237793
Epoch 230, training loss: 7.844290733337402 = 1.414842128753662 + 1.0 * 6.42944860458374
Epoch 230, val loss: 1.463414192199707
Epoch 240, training loss: 7.787975311279297 = 1.3631713390350342 + 1.0 * 6.424803733825684
Epoch 240, val loss: 1.4181653261184692
Epoch 250, training loss: 7.728238582611084 = 1.3110699653625488 + 1.0 * 6.417168617248535
Epoch 250, val loss: 1.3729828596115112
Epoch 260, training loss: 7.669685363769531 = 1.2596417665481567 + 1.0 * 6.410043716430664
Epoch 260, val loss: 1.3289066553115845
Epoch 270, training loss: 7.612833023071289 = 1.209046483039856 + 1.0 * 6.403786659240723
Epoch 270, val loss: 1.2862515449523926
Epoch 280, training loss: 7.563164234161377 = 1.1600452661514282 + 1.0 * 6.403119087219238
Epoch 280, val loss: 1.245706558227539
Epoch 290, training loss: 7.510019302368164 = 1.1136960983276367 + 1.0 * 6.396323204040527
Epoch 290, val loss: 1.2082247734069824
Epoch 300, training loss: 7.459839820861816 = 1.0696736574172974 + 1.0 * 6.390166282653809
Epoch 300, val loss: 1.173471212387085
Epoch 310, training loss: 7.417490005493164 = 1.027496576309204 + 1.0 * 6.389993190765381
Epoch 310, val loss: 1.1410304307937622
Epoch 320, training loss: 7.371816158294678 = 0.9874009490013123 + 1.0 * 6.384415149688721
Epoch 320, val loss: 1.1111422777175903
Epoch 330, training loss: 7.327127933502197 = 0.9489604234695435 + 1.0 * 6.378167629241943
Epoch 330, val loss: 1.0832655429840088
Epoch 340, training loss: 7.287544250488281 = 0.9113425612449646 + 1.0 * 6.376201629638672
Epoch 340, val loss: 1.0566881895065308
Epoch 350, training loss: 7.250606060028076 = 0.8747581243515015 + 1.0 * 6.375847816467285
Epoch 350, val loss: 1.0314654111862183
Epoch 360, training loss: 7.207821846008301 = 0.838800311088562 + 1.0 * 6.369021415710449
Epoch 360, val loss: 1.007422924041748
Epoch 370, training loss: 7.168427467346191 = 0.8031144142150879 + 1.0 * 6.3653130531311035
Epoch 370, val loss: 0.9841742515563965
Epoch 380, training loss: 7.130279064178467 = 0.7679343819618225 + 1.0 * 6.362344741821289
Epoch 380, val loss: 0.9618472456932068
Epoch 390, training loss: 7.0929274559021 = 0.7336224913597107 + 1.0 * 6.359304904937744
Epoch 390, val loss: 0.9408637881278992
Epoch 400, training loss: 7.061948299407959 = 0.7001232504844666 + 1.0 * 6.361824989318848
Epoch 400, val loss: 0.9211229085922241
Epoch 410, training loss: 7.026747703552246 = 0.6679396033287048 + 1.0 * 6.3588080406188965
Epoch 410, val loss: 0.9029048681259155
Epoch 420, training loss: 6.990120887756348 = 0.6372352242469788 + 1.0 * 6.352885723114014
Epoch 420, val loss: 0.8864041566848755
Epoch 430, training loss: 6.958902359008789 = 0.6078606247901917 + 1.0 * 6.351041793823242
Epoch 430, val loss: 0.8714669942855835
Epoch 440, training loss: 6.929077625274658 = 0.5799875259399414 + 1.0 * 6.349090099334717
Epoch 440, val loss: 0.8581281900405884
Epoch 450, training loss: 6.898194789886475 = 0.5534946322441101 + 1.0 * 6.344700336456299
Epoch 450, val loss: 0.8463854789733887
Epoch 460, training loss: 6.877115249633789 = 0.5282965302467346 + 1.0 * 6.348818778991699
Epoch 460, val loss: 0.8359440565109253
Epoch 470, training loss: 6.846196174621582 = 0.5045557618141174 + 1.0 * 6.341640472412109
Epoch 470, val loss: 0.8268321752548218
Epoch 480, training loss: 6.8205060958862305 = 0.48175251483917236 + 1.0 * 6.338753700256348
Epoch 480, val loss: 0.8187353610992432
Epoch 490, training loss: 6.797072410583496 = 0.4597133696079254 + 1.0 * 6.3373589515686035
Epoch 490, val loss: 0.8114028573036194
Epoch 500, training loss: 6.783733367919922 = 0.43852823972702026 + 1.0 * 6.345205307006836
Epoch 500, val loss: 0.8048743605613708
Epoch 510, training loss: 6.756186008453369 = 0.41828063130378723 + 1.0 * 6.337905406951904
Epoch 510, val loss: 0.7991728186607361
Epoch 520, training loss: 6.7301177978515625 = 0.3985988199710846 + 1.0 * 6.33151912689209
Epoch 520, val loss: 0.793997585773468
Epoch 530, training loss: 6.708536148071289 = 0.3793782591819763 + 1.0 * 6.329157829284668
Epoch 530, val loss: 0.7893379926681519
Epoch 540, training loss: 6.690757751464844 = 0.36058178544044495 + 1.0 * 6.330175876617432
Epoch 540, val loss: 0.7851197123527527
Epoch 550, training loss: 6.682331085205078 = 0.342387318611145 + 1.0 * 6.339943885803223
Epoch 550, val loss: 0.7812166213989258
Epoch 560, training loss: 6.651874542236328 = 0.3248875141143799 + 1.0 * 6.326987266540527
Epoch 560, val loss: 0.7779306173324585
Epoch 570, training loss: 6.63112211227417 = 0.30784887075424194 + 1.0 * 6.323273181915283
Epoch 570, val loss: 0.7749897241592407
Epoch 580, training loss: 6.6296892166137695 = 0.2912764847278595 + 1.0 * 6.338412761688232
Epoch 580, val loss: 0.7723426818847656
Epoch 590, training loss: 6.600715160369873 = 0.2753511071205139 + 1.0 * 6.325364112854004
Epoch 590, val loss: 0.7701104283332825
Epoch 600, training loss: 6.580249309539795 = 0.2599368393421173 + 1.0 * 6.3203125
Epoch 600, val loss: 0.7682502865791321
Epoch 610, training loss: 6.562617778778076 = 0.24502035975456238 + 1.0 * 6.317597389221191
Epoch 610, val loss: 0.7667060494422913
Epoch 620, training loss: 6.564871788024902 = 0.23062363266944885 + 1.0 * 6.334248065948486
Epoch 620, val loss: 0.7655360698699951
Epoch 630, training loss: 6.531712532043457 = 0.21700306236743927 + 1.0 * 6.314709663391113
Epoch 630, val loss: 0.7647328972816467
Epoch 640, training loss: 6.517841815948486 = 0.20401185750961304 + 1.0 * 6.3138298988342285
Epoch 640, val loss: 0.7644526958465576
Epoch 650, training loss: 6.504723072052002 = 0.19161611795425415 + 1.0 * 6.313107013702393
Epoch 650, val loss: 0.7645875811576843
Epoch 660, training loss: 6.503232955932617 = 0.17985232174396515 + 1.0 * 6.323380470275879
Epoch 660, val loss: 0.7651404142379761
Epoch 670, training loss: 6.478409767150879 = 0.1688307374715805 + 1.0 * 6.309578895568848
Epoch 670, val loss: 0.7661644816398621
Epoch 680, training loss: 6.468088150024414 = 0.15843020379543304 + 1.0 * 6.309658050537109
Epoch 680, val loss: 0.7677056789398193
Epoch 690, training loss: 6.4590935707092285 = 0.14868980646133423 + 1.0 * 6.310403823852539
Epoch 690, val loss: 0.7696790099143982
Epoch 700, training loss: 6.446618556976318 = 0.13960310816764832 + 1.0 * 6.307015419006348
Epoch 700, val loss: 0.7720948457717896
Epoch 710, training loss: 6.447882175445557 = 0.13109983503818512 + 1.0 * 6.316782474517822
Epoch 710, val loss: 0.7749038338661194
Epoch 720, training loss: 6.430990219116211 = 0.12324213981628418 + 1.0 * 6.307748317718506
Epoch 720, val loss: 0.7780717015266418
Epoch 730, training loss: 6.418385028839111 = 0.11592557281255722 + 1.0 * 6.302459239959717
Epoch 730, val loss: 0.7817105054855347
Epoch 740, training loss: 6.414775371551514 = 0.10912290960550308 + 1.0 * 6.305652618408203
Epoch 740, val loss: 0.7856132388114929
Epoch 750, training loss: 6.408148288726807 = 0.10283010452985764 + 1.0 * 6.305318355560303
Epoch 750, val loss: 0.78973388671875
Epoch 760, training loss: 6.399057865142822 = 0.09700462967157364 + 1.0 * 6.302053451538086
Epoch 760, val loss: 0.7942543029785156
Epoch 770, training loss: 6.391298294067383 = 0.09159475564956665 + 1.0 * 6.299703598022461
Epoch 770, val loss: 0.7989789843559265
Epoch 780, training loss: 6.39120626449585 = 0.08657500892877579 + 1.0 * 6.304631233215332
Epoch 780, val loss: 0.8038147687911987
Epoch 790, training loss: 6.385443687438965 = 0.08194058388471603 + 1.0 * 6.303503036499023
Epoch 790, val loss: 0.8088935613632202
Epoch 800, training loss: 6.375224590301514 = 0.07762931287288666 + 1.0 * 6.297595500946045
Epoch 800, val loss: 0.8141113519668579
Epoch 810, training loss: 6.372209548950195 = 0.07363034039735794 + 1.0 * 6.298579216003418
Epoch 810, val loss: 0.819450855255127
Epoch 820, training loss: 6.366481781005859 = 0.06990987062454224 + 1.0 * 6.296571731567383
Epoch 820, val loss: 0.8247740864753723
Epoch 830, training loss: 6.359087944030762 = 0.0664575919508934 + 1.0 * 6.292630195617676
Epoch 830, val loss: 0.8303157091140747
Epoch 840, training loss: 6.35536003112793 = 0.06322692334651947 + 1.0 * 6.292133331298828
Epoch 840, val loss: 0.8358535766601562
Epoch 850, training loss: 6.359626770019531 = 0.0602063313126564 + 1.0 * 6.299420356750488
Epoch 850, val loss: 0.8414251208305359
Epoch 860, training loss: 6.354304790496826 = 0.05740329623222351 + 1.0 * 6.296901702880859
Epoch 860, val loss: 0.846897304058075
Epoch 870, training loss: 6.3443217277526855 = 0.05478845164179802 + 1.0 * 6.2895331382751465
Epoch 870, val loss: 0.8525940775871277
Epoch 880, training loss: 6.341667652130127 = 0.05233350768685341 + 1.0 * 6.289334297180176
Epoch 880, val loss: 0.858229398727417
Epoch 890, training loss: 6.34626579284668 = 0.050029922276735306 + 1.0 * 6.296236038208008
Epoch 890, val loss: 0.8637046813964844
Epoch 900, training loss: 6.334484577178955 = 0.047871578484773636 + 1.0 * 6.2866129875183105
Epoch 900, val loss: 0.8692916035652161
Epoch 910, training loss: 6.331428527832031 = 0.04584391787648201 + 1.0 * 6.285584449768066
Epoch 910, val loss: 0.8749180436134338
Epoch 920, training loss: 6.337848663330078 = 0.04392857849597931 + 1.0 * 6.293920040130615
Epoch 920, val loss: 0.8804041743278503
Epoch 930, training loss: 6.330031871795654 = 0.04214077070355415 + 1.0 * 6.287890911102295
Epoch 930, val loss: 0.8858197331428528
Epoch 940, training loss: 6.325649261474609 = 0.04045020788908005 + 1.0 * 6.285199165344238
Epoch 940, val loss: 0.8913740515708923
Epoch 950, training loss: 6.32588005065918 = 0.03885741904377937 + 1.0 * 6.287022590637207
Epoch 950, val loss: 0.8968273401260376
Epoch 960, training loss: 6.318830966949463 = 0.03734995797276497 + 1.0 * 6.28148078918457
Epoch 960, val loss: 0.9021477699279785
Epoch 970, training loss: 6.318749904632568 = 0.03592630475759506 + 1.0 * 6.28282356262207
Epoch 970, val loss: 0.9075666666030884
Epoch 980, training loss: 6.323230266571045 = 0.03458132967352867 + 1.0 * 6.288649082183838
Epoch 980, val loss: 0.9128490090370178
Epoch 990, training loss: 6.314987659454346 = 0.03330790251493454 + 1.0 * 6.281679630279541
Epoch 990, val loss: 0.9180498123168945
Epoch 1000, training loss: 6.317695617675781 = 0.03210447356104851 + 1.0 * 6.285591125488281
Epoch 1000, val loss: 0.9233100414276123
Epoch 1010, training loss: 6.310744285583496 = 0.03096589259803295 + 1.0 * 6.279778480529785
Epoch 1010, val loss: 0.9284337759017944
Epoch 1020, training loss: 6.30959939956665 = 0.02988452836871147 + 1.0 * 6.279715061187744
Epoch 1020, val loss: 0.9335547089576721
Epoch 1030, training loss: 6.306786060333252 = 0.02885681577026844 + 1.0 * 6.277929306030273
Epoch 1030, val loss: 0.9386411309242249
Epoch 1040, training loss: 6.307263374328613 = 0.027879390865564346 + 1.0 * 6.279384136199951
Epoch 1040, val loss: 0.9436696171760559
Epoch 1050, training loss: 6.304840087890625 = 0.026949146762490273 + 1.0 * 6.277891159057617
Epoch 1050, val loss: 0.9486056566238403
Epoch 1060, training loss: 6.3013105392456055 = 0.02606479823589325 + 1.0 * 6.275245666503906
Epoch 1060, val loss: 0.9535604119300842
Epoch 1070, training loss: 6.312875270843506 = 0.025223514065146446 + 1.0 * 6.287651538848877
Epoch 1070, val loss: 0.9583698511123657
Epoch 1080, training loss: 6.298460960388184 = 0.024425173178315163 + 1.0 * 6.274035930633545
Epoch 1080, val loss: 0.9630895256996155
Epoch 1090, training loss: 6.297542572021484 = 0.023666637018322945 + 1.0 * 6.273875713348389
Epoch 1090, val loss: 0.9679673910140991
Epoch 1100, training loss: 6.30087423324585 = 0.022940566763281822 + 1.0 * 6.277933597564697
Epoch 1100, val loss: 0.9726282358169556
Epoch 1110, training loss: 6.293361663818359 = 0.022247187793254852 + 1.0 * 6.271114349365234
Epoch 1110, val loss: 0.9771559238433838
Epoch 1120, training loss: 6.293343544006348 = 0.021584507077932358 + 1.0 * 6.271759033203125
Epoch 1120, val loss: 0.981829822063446
Epoch 1130, training loss: 6.302655220031738 = 0.02095085009932518 + 1.0 * 6.281704425811768
Epoch 1130, val loss: 0.9863962531089783
Epoch 1140, training loss: 6.293845176696777 = 0.020348070189356804 + 1.0 * 6.273497104644775
Epoch 1140, val loss: 0.9908173680305481
Epoch 1150, training loss: 6.290048599243164 = 0.019769692793488503 + 1.0 * 6.2702789306640625
Epoch 1150, val loss: 0.9953166842460632
Epoch 1160, training loss: 6.287965774536133 = 0.01921408250927925 + 1.0 * 6.268751621246338
Epoch 1160, val loss: 0.999774158000946
Epoch 1170, training loss: 6.306286334991455 = 0.0186796635389328 + 1.0 * 6.287606716156006
Epoch 1170, val loss: 1.004018783569336
Epoch 1180, training loss: 6.291057586669922 = 0.018173234537243843 + 1.0 * 6.272884368896484
Epoch 1180, val loss: 1.0082495212554932
Epoch 1190, training loss: 6.294215679168701 = 0.017688410356640816 + 1.0 * 6.276527404785156
Epoch 1190, val loss: 1.012542724609375
Epoch 1200, training loss: 6.284481525421143 = 0.01722303405404091 + 1.0 * 6.267258644104004
Epoch 1200, val loss: 1.0166566371917725
Epoch 1210, training loss: 6.282273292541504 = 0.01677704229950905 + 1.0 * 6.265496253967285
Epoch 1210, val loss: 1.0208555459976196
Epoch 1220, training loss: 6.2827606201171875 = 0.016346914693713188 + 1.0 * 6.266413688659668
Epoch 1220, val loss: 1.024996280670166
Epoch 1230, training loss: 6.289384365081787 = 0.015931202098727226 + 1.0 * 6.273453235626221
Epoch 1230, val loss: 1.0290063619613647
Epoch 1240, training loss: 6.2851715087890625 = 0.015533411875367165 + 1.0 * 6.2696380615234375
Epoch 1240, val loss: 1.0329761505126953
Epoch 1250, training loss: 6.280684947967529 = 0.015152310952544212 + 1.0 * 6.265532493591309
Epoch 1250, val loss: 1.0369675159454346
Epoch 1260, training loss: 6.283600807189941 = 0.014784503728151321 + 1.0 * 6.2688164710998535
Epoch 1260, val loss: 1.0409425497055054
Epoch 1270, training loss: 6.279792308807373 = 0.014430384151637554 + 1.0 * 6.265361785888672
Epoch 1270, val loss: 1.0447595119476318
Epoch 1280, training loss: 6.282607555389404 = 0.014089236967265606 + 1.0 * 6.268518447875977
Epoch 1280, val loss: 1.0485855340957642
Epoch 1290, training loss: 6.277342319488525 = 0.013762082904577255 + 1.0 * 6.263580322265625
Epoch 1290, val loss: 1.0523061752319336
Epoch 1300, training loss: 6.276463031768799 = 0.013447010889649391 + 1.0 * 6.263016223907471
Epoch 1300, val loss: 1.056149959564209
Epoch 1310, training loss: 6.277564525604248 = 0.013141007162630558 + 1.0 * 6.264423370361328
Epoch 1310, val loss: 1.0598020553588867
Epoch 1320, training loss: 6.277917861938477 = 0.012845675460994244 + 1.0 * 6.265072345733643
Epoch 1320, val loss: 1.0634424686431885
Epoch 1330, training loss: 6.273397445678711 = 0.012561779469251633 + 1.0 * 6.260835647583008
Epoch 1330, val loss: 1.0670267343521118
Epoch 1340, training loss: 6.273075103759766 = 0.012287730351090431 + 1.0 * 6.260787487030029
Epoch 1340, val loss: 1.0707119703292847
Epoch 1350, training loss: 6.280468463897705 = 0.012023413553833961 + 1.0 * 6.268445014953613
Epoch 1350, val loss: 1.0741827487945557
Epoch 1360, training loss: 6.276490211486816 = 0.01176975667476654 + 1.0 * 6.264720439910889
Epoch 1360, val loss: 1.077646255493164
Epoch 1370, training loss: 6.2706685066223145 = 0.01152260135859251 + 1.0 * 6.259145736694336
Epoch 1370, val loss: 1.0811601877212524
Epoch 1380, training loss: 6.271724224090576 = 0.011283773928880692 + 1.0 * 6.260440349578857
Epoch 1380, val loss: 1.0846823453903198
Epoch 1390, training loss: 6.274474620819092 = 0.011051703244447708 + 1.0 * 6.263422966003418
Epoch 1390, val loss: 1.0880091190338135
Epoch 1400, training loss: 6.273735046386719 = 0.010829446837306023 + 1.0 * 6.262905597686768
Epoch 1400, val loss: 1.0912889242172241
Epoch 1410, training loss: 6.26732873916626 = 0.010612333193421364 + 1.0 * 6.256716251373291
Epoch 1410, val loss: 1.094692587852478
Epoch 1420, training loss: 6.268728733062744 = 0.010402643121778965 + 1.0 * 6.258326053619385
Epoch 1420, val loss: 1.0980017185211182
Epoch 1430, training loss: 6.273064136505127 = 0.010199525393545628 + 1.0 * 6.262864589691162
Epoch 1430, val loss: 1.1011581420898438
Epoch 1440, training loss: 6.270116806030273 = 0.010003591887652874 + 1.0 * 6.26011323928833
Epoch 1440, val loss: 1.1042762994766235
Epoch 1450, training loss: 6.26719331741333 = 0.009814226999878883 + 1.0 * 6.257379055023193
Epoch 1450, val loss: 1.1075373888015747
Epoch 1460, training loss: 6.264097690582275 = 0.009630050510168076 + 1.0 * 6.254467487335205
Epoch 1460, val loss: 1.1107113361358643
Epoch 1470, training loss: 6.2660298347473145 = 0.009450194425880909 + 1.0 * 6.256579875946045
Epoch 1470, val loss: 1.1138532161712646
Epoch 1480, training loss: 6.2656755447387695 = 0.009275558404624462 + 1.0 * 6.256400108337402
Epoch 1480, val loss: 1.1168010234832764
Epoch 1490, training loss: 6.270941734313965 = 0.009107798337936401 + 1.0 * 6.261834144592285
Epoch 1490, val loss: 1.119814157485962
Epoch 1500, training loss: 6.262857913970947 = 0.008944617584347725 + 1.0 * 6.253913402557373
Epoch 1500, val loss: 1.1228373050689697
Epoch 1510, training loss: 6.2626776695251465 = 0.0087866997346282 + 1.0 * 6.2538909912109375
Epoch 1510, val loss: 1.125882625579834
Epoch 1520, training loss: 6.261157035827637 = 0.008632177487015724 + 1.0 * 6.2525248527526855
Epoch 1520, val loss: 1.1288381814956665
Epoch 1530, training loss: 6.274237632751465 = 0.008481684140861034 + 1.0 * 6.265756130218506
Epoch 1530, val loss: 1.1316896677017212
Epoch 1540, training loss: 6.263818740844727 = 0.008334757760167122 + 1.0 * 6.255484104156494
Epoch 1540, val loss: 1.134514570236206
Epoch 1550, training loss: 6.260100841522217 = 0.008194021880626678 + 1.0 * 6.251906871795654
Epoch 1550, val loss: 1.1374164819717407
Epoch 1560, training loss: 6.2608442306518555 = 0.008056296966969967 + 1.0 * 6.252788066864014
Epoch 1560, val loss: 1.1403526067733765
Epoch 1570, training loss: 6.265830993652344 = 0.007921998389065266 + 1.0 * 6.257908821105957
Epoch 1570, val loss: 1.142987608909607
Epoch 1580, training loss: 6.260776042938232 = 0.0077916099689900875 + 1.0 * 6.252984523773193
Epoch 1580, val loss: 1.1457539796829224
Epoch 1590, training loss: 6.258267879486084 = 0.007664969656616449 + 1.0 * 6.250602722167969
Epoch 1590, val loss: 1.1485445499420166
Epoch 1600, training loss: 6.261239528656006 = 0.007541042286902666 + 1.0 * 6.253698348999023
Epoch 1600, val loss: 1.1512854099273682
Epoch 1610, training loss: 6.257152080535889 = 0.00741976173594594 + 1.0 * 6.249732494354248
Epoch 1610, val loss: 1.1539195775985718
Epoch 1620, training loss: 6.259891033172607 = 0.00730188749730587 + 1.0 * 6.252589225769043
Epoch 1620, val loss: 1.1566470861434937
Epoch 1630, training loss: 6.258701801300049 = 0.007187352515757084 + 1.0 * 6.251514434814453
Epoch 1630, val loss: 1.1592262983322144
Epoch 1640, training loss: 6.25717830657959 = 0.007075706031173468 + 1.0 * 6.250102519989014
Epoch 1640, val loss: 1.1618669033050537
Epoch 1650, training loss: 6.264656066894531 = 0.006967333145439625 + 1.0 * 6.257688522338867
Epoch 1650, val loss: 1.164488673210144
Epoch 1660, training loss: 6.259582996368408 = 0.0068611521273851395 + 1.0 * 6.252721786499023
Epoch 1660, val loss: 1.1669760942459106
Epoch 1670, training loss: 6.255523204803467 = 0.006758523173630238 + 1.0 * 6.248764514923096
Epoch 1670, val loss: 1.1695767641067505
Epoch 1680, training loss: 6.255275726318359 = 0.006657620891928673 + 1.0 * 6.248618125915527
Epoch 1680, val loss: 1.1721508502960205
Epoch 1690, training loss: 6.262452602386475 = 0.006558879278600216 + 1.0 * 6.255893707275391
Epoch 1690, val loss: 1.174651026725769
Epoch 1700, training loss: 6.256702423095703 = 0.0064630224369466305 + 1.0 * 6.250239372253418
Epoch 1700, val loss: 1.1770803928375244
Epoch 1710, training loss: 6.2570648193359375 = 0.0063697574660182 + 1.0 * 6.25069522857666
Epoch 1710, val loss: 1.17953360080719
Epoch 1720, training loss: 6.253566741943359 = 0.006278503220528364 + 1.0 * 6.247288227081299
Epoch 1720, val loss: 1.1820160150527954
Epoch 1730, training loss: 6.252978324890137 = 0.006189194042235613 + 1.0 * 6.24678897857666
Epoch 1730, val loss: 1.1844429969787598
Epoch 1740, training loss: 6.255381107330322 = 0.006101424805819988 + 1.0 * 6.249279499053955
Epoch 1740, val loss: 1.186855673789978
Epoch 1750, training loss: 6.256355285644531 = 0.006015609484165907 + 1.0 * 6.250339508056641
Epoch 1750, val loss: 1.1891717910766602
Epoch 1760, training loss: 6.25678825378418 = 0.0059329611249268055 + 1.0 * 6.250855445861816
Epoch 1760, val loss: 1.1914719343185425
Epoch 1770, training loss: 6.2577595710754395 = 0.005852152593433857 + 1.0 * 6.2519073486328125
Epoch 1770, val loss: 1.1938316822052002
Epoch 1780, training loss: 6.252368450164795 = 0.0057732523418962955 + 1.0 * 6.24659538269043
Epoch 1780, val loss: 1.1961214542388916
Epoch 1790, training loss: 6.252756118774414 = 0.0056958431378006935 + 1.0 * 6.247060298919678
Epoch 1790, val loss: 1.198428988456726
Epoch 1800, training loss: 6.254080772399902 = 0.0056198411621153355 + 1.0 * 6.24846076965332
Epoch 1800, val loss: 1.200698971748352
Epoch 1810, training loss: 6.252404689788818 = 0.005545176099985838 + 1.0 * 6.246859550476074
Epoch 1810, val loss: 1.2029513120651245
Epoch 1820, training loss: 6.256837368011475 = 0.005472312215715647 + 1.0 * 6.2513651847839355
Epoch 1820, val loss: 1.2051953077316284
Epoch 1830, training loss: 6.250357627868652 = 0.0054020038805902 + 1.0 * 6.244955539703369
Epoch 1830, val loss: 1.2073404788970947
Epoch 1840, training loss: 6.2491230964660645 = 0.005332837346941233 + 1.0 * 6.243790149688721
Epoch 1840, val loss: 1.2095979452133179
Epoch 1850, training loss: 6.248186111450195 = 0.005264355801045895 + 1.0 * 6.242921829223633
Epoch 1850, val loss: 1.2118122577667236
Epoch 1860, training loss: 6.254710674285889 = 0.005197038408368826 + 1.0 * 6.249513626098633
Epoch 1860, val loss: 1.2140032052993774
Epoch 1870, training loss: 6.24949836730957 = 0.005131144542247057 + 1.0 * 6.2443671226501465
Epoch 1870, val loss: 1.2160450220108032
Epoch 1880, training loss: 6.249101161956787 = 0.005067534279078245 + 1.0 * 6.2440338134765625
Epoch 1880, val loss: 1.218208909034729
Epoch 1890, training loss: 6.2468061447143555 = 0.005005329381674528 + 1.0 * 6.241800785064697
Epoch 1890, val loss: 1.2203495502471924
Epoch 1900, training loss: 6.247365474700928 = 0.004944140091538429 + 1.0 * 6.2424211502075195
Epoch 1900, val loss: 1.2224795818328857
Epoch 1910, training loss: 6.260514259338379 = 0.004884016700088978 + 1.0 * 6.255630016326904
Epoch 1910, val loss: 1.2244739532470703
Epoch 1920, training loss: 6.249361038208008 = 0.004825327079743147 + 1.0 * 6.24453592300415
Epoch 1920, val loss: 1.2264429330825806
Epoch 1930, training loss: 6.245904445648193 = 0.004768732003867626 + 1.0 * 6.241135597229004
Epoch 1930, val loss: 1.2285171747207642
Epoch 1940, training loss: 6.246975421905518 = 0.004713147878646851 + 1.0 * 6.242262363433838
Epoch 1940, val loss: 1.2305808067321777
Epoch 1950, training loss: 6.251047611236572 = 0.0046579535119235516 + 1.0 * 6.246389865875244
Epoch 1950, val loss: 1.2325445413589478
Epoch 1960, training loss: 6.244256496429443 = 0.0046035717241466045 + 1.0 * 6.23965311050415
Epoch 1960, val loss: 1.2345479726791382
Epoch 1970, training loss: 6.247700214385986 = 0.004550008103251457 + 1.0 * 6.243150234222412
Epoch 1970, val loss: 1.2366174459457397
Epoch 1980, training loss: 6.246174335479736 = 0.004497688263654709 + 1.0 * 6.2416768074035645
Epoch 1980, val loss: 1.2385306358337402
Epoch 1990, training loss: 6.248859405517578 = 0.004446505568921566 + 1.0 * 6.244412899017334
Epoch 1990, val loss: 1.240425705909729
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8186610437532947
The final CL Acc:0.77654, 0.00924, The final GNN Acc:0.81673, 0.00273
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13132])
remove edge: torch.Size([2, 7922])
updated graph: torch.Size([2, 10498])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.545345306396484 = 1.948512315750122 + 1.0 * 8.596833229064941
Epoch 0, val loss: 1.9361387491226196
Epoch 10, training loss: 10.534890174865723 = 1.938331961631775 + 1.0 * 8.596558570861816
Epoch 10, val loss: 1.925750970840454
Epoch 20, training loss: 10.51978874206543 = 1.9258074760437012 + 1.0 * 8.593981742858887
Epoch 20, val loss: 1.9130065441131592
Epoch 30, training loss: 10.481616973876953 = 1.9085749387741089 + 1.0 * 8.573041915893555
Epoch 30, val loss: 1.8956222534179688
Epoch 40, training loss: 10.351564407348633 = 1.885650873184204 + 1.0 * 8.465913772583008
Epoch 40, val loss: 1.8735817670822144
Epoch 50, training loss: 9.8688325881958 = 1.8595367670059204 + 1.0 * 8.009295463562012
Epoch 50, val loss: 1.849403738975525
Epoch 60, training loss: 9.567501068115234 = 1.8345351219177246 + 1.0 * 7.732965469360352
Epoch 60, val loss: 1.8279051780700684
Epoch 70, training loss: 9.162131309509277 = 1.815645456314087 + 1.0 * 7.346485614776611
Epoch 70, val loss: 1.811726450920105
Epoch 80, training loss: 8.849984169006348 = 1.800749659538269 + 1.0 * 7.049234867095947
Epoch 80, val loss: 1.7988637685775757
Epoch 90, training loss: 8.695230484008789 = 1.7861287593841553 + 1.0 * 6.909101486206055
Epoch 90, val loss: 1.7857171297073364
Epoch 100, training loss: 8.570403099060059 = 1.7700908184051514 + 1.0 * 6.800312519073486
Epoch 100, val loss: 1.7717949151992798
Epoch 110, training loss: 8.474398612976074 = 1.7540384531021118 + 1.0 * 6.720359802246094
Epoch 110, val loss: 1.7582682371139526
Epoch 120, training loss: 8.405548095703125 = 1.7373629808425903 + 1.0 * 6.668184757232666
Epoch 120, val loss: 1.7442491054534912
Epoch 130, training loss: 8.34449577331543 = 1.7188135385513306 + 1.0 * 6.625682353973389
Epoch 130, val loss: 1.7284632921218872
Epoch 140, training loss: 8.291068077087402 = 1.697628140449524 + 1.0 * 6.59343957901001
Epoch 140, val loss: 1.7104815244674683
Epoch 150, training loss: 8.240731239318848 = 1.6731723546981812 + 1.0 * 6.567558765411377
Epoch 150, val loss: 1.689915418624878
Epoch 160, training loss: 8.186186790466309 = 1.6446502208709717 + 1.0 * 6.541536808013916
Epoch 160, val loss: 1.6662065982818604
Epoch 170, training loss: 8.129912376403809 = 1.6112855672836304 + 1.0 * 6.518626689910889
Epoch 170, val loss: 1.6386182308197021
Epoch 180, training loss: 8.070723533630371 = 1.572223424911499 + 1.0 * 6.498500347137451
Epoch 180, val loss: 1.6063599586486816
Epoch 190, training loss: 8.01038932800293 = 1.5269776582717896 + 1.0 * 6.4834113121032715
Epoch 190, val loss: 1.5691999197006226
Epoch 200, training loss: 7.947274208068848 = 1.4772311449050903 + 1.0 * 6.470043182373047
Epoch 200, val loss: 1.5284684896469116
Epoch 210, training loss: 7.881199359893799 = 1.422564148902893 + 1.0 * 6.458635330200195
Epoch 210, val loss: 1.4838249683380127
Epoch 220, training loss: 7.810831069946289 = 1.3631247282028198 + 1.0 * 6.44770622253418
Epoch 220, val loss: 1.4355087280273438
Epoch 230, training loss: 7.739850997924805 = 1.3002508878707886 + 1.0 * 6.439599990844727
Epoch 230, val loss: 1.384778618812561
Epoch 240, training loss: 7.666309833526611 = 1.235610008239746 + 1.0 * 6.430699825286865
Epoch 240, val loss: 1.3324404954910278
Epoch 250, training loss: 7.593134880065918 = 1.169945478439331 + 1.0 * 6.423189640045166
Epoch 250, val loss: 1.2794663906097412
Epoch 260, training loss: 7.522266387939453 = 1.1043957471847534 + 1.0 * 6.41787052154541
Epoch 260, val loss: 1.226931095123291
Epoch 270, training loss: 7.455153465270996 = 1.0421903133392334 + 1.0 * 6.412963390350342
Epoch 270, val loss: 1.1773287057876587
Epoch 280, training loss: 7.391728401184082 = 0.984106719493866 + 1.0 * 6.40762186050415
Epoch 280, val loss: 1.1312360763549805
Epoch 290, training loss: 7.3311614990234375 = 0.9292893409729004 + 1.0 * 6.401872158050537
Epoch 290, val loss: 1.0880038738250732
Epoch 300, training loss: 7.274204254150391 = 0.8774974346160889 + 1.0 * 6.396706581115723
Epoch 300, val loss: 1.0474541187286377
Epoch 310, training loss: 7.2220354080200195 = 0.8287661671638489 + 1.0 * 6.393269062042236
Epoch 310, val loss: 1.0095927715301514
Epoch 320, training loss: 7.174098491668701 = 0.7842915654182434 + 1.0 * 6.389806747436523
Epoch 320, val loss: 0.975374698638916
Epoch 330, training loss: 7.129246711730957 = 0.7437707185745239 + 1.0 * 6.385476112365723
Epoch 330, val loss: 0.9449164867401123
Epoch 340, training loss: 7.086973190307617 = 0.706188976764679 + 1.0 * 6.380784034729004
Epoch 340, val loss: 0.9169727563858032
Epoch 350, training loss: 7.0469560623168945 = 0.6709539890289307 + 1.0 * 6.376002311706543
Epoch 350, val loss: 0.8914216160774231
Epoch 360, training loss: 7.01155948638916 = 0.6380859613418579 + 1.0 * 6.373473644256592
Epoch 360, val loss: 0.8681672811508179
Epoch 370, training loss: 6.97940731048584 = 0.6079442501068115 + 1.0 * 6.371462821960449
Epoch 370, val loss: 0.8474254012107849
Epoch 380, training loss: 6.946256160736084 = 0.5795258283615112 + 1.0 * 6.366730213165283
Epoch 380, val loss: 0.8285857439041138
Epoch 390, training loss: 6.915255546569824 = 0.5524652600288391 + 1.0 * 6.362790107727051
Epoch 390, val loss: 0.8111920356750488
Epoch 400, training loss: 6.889886856079102 = 0.5264016389846802 + 1.0 * 6.363485336303711
Epoch 400, val loss: 0.7951827049255371
Epoch 410, training loss: 6.860072612762451 = 0.501708984375 + 1.0 * 6.358363628387451
Epoch 410, val loss: 0.7805203199386597
Epoch 420, training loss: 6.8315348625183105 = 0.4779486060142517 + 1.0 * 6.353586196899414
Epoch 420, val loss: 0.7670194506645203
Epoch 430, training loss: 6.805897235870361 = 0.45483526587486267 + 1.0 * 6.351061820983887
Epoch 430, val loss: 0.7543979287147522
Epoch 440, training loss: 6.780948162078857 = 0.43218693137168884 + 1.0 * 6.348761081695557
Epoch 440, val loss: 0.7425585985183716
Epoch 450, training loss: 6.7578840255737305 = 0.4100617468357086 + 1.0 * 6.347822189331055
Epoch 450, val loss: 0.7315223217010498
Epoch 460, training loss: 6.732209205627441 = 0.3885596990585327 + 1.0 * 6.343649387359619
Epoch 460, val loss: 0.7210890650749207
Epoch 470, training loss: 6.709078788757324 = 0.3674696087837219 + 1.0 * 6.341609001159668
Epoch 470, val loss: 0.7112108469009399
Epoch 480, training loss: 6.700564384460449 = 0.3468264639377594 + 1.0 * 6.353737831115723
Epoch 480, val loss: 0.7019293904304504
Epoch 490, training loss: 6.668746471405029 = 0.32697594165802 + 1.0 * 6.341770648956299
Epoch 490, val loss: 0.6933147311210632
Epoch 500, training loss: 6.6441450119018555 = 0.3077808916568756 + 1.0 * 6.336364269256592
Epoch 500, val loss: 0.685339629650116
Epoch 510, training loss: 6.623620986938477 = 0.2892264723777771 + 1.0 * 6.334394454956055
Epoch 510, val loss: 0.6781079769134521
Epoch 520, training loss: 6.6147637367248535 = 0.2715087831020355 + 1.0 * 6.343255043029785
Epoch 520, val loss: 0.6718026399612427
Epoch 530, training loss: 6.5894598960876465 = 0.25488656759262085 + 1.0 * 6.334573268890381
Epoch 530, val loss: 0.6661542057991028
Epoch 540, training loss: 6.567574501037598 = 0.23912538588047028 + 1.0 * 6.328449249267578
Epoch 540, val loss: 0.6613225340843201
Epoch 550, training loss: 6.550358772277832 = 0.22417831420898438 + 1.0 * 6.326180458068848
Epoch 550, val loss: 0.6573609113693237
Epoch 560, training loss: 6.53709602355957 = 0.21006709337234497 + 1.0 * 6.327028751373291
Epoch 560, val loss: 0.6541630029678345
Epoch 570, training loss: 6.528925895690918 = 0.19689904153347015 + 1.0 * 6.332026958465576
Epoch 570, val loss: 0.6518222689628601
Epoch 580, training loss: 6.511324882507324 = 0.18468277156352997 + 1.0 * 6.326642036437988
Epoch 580, val loss: 0.6500760912895203
Epoch 590, training loss: 6.493630409240723 = 0.1732979714870453 + 1.0 * 6.3203325271606445
Epoch 590, val loss: 0.6491557955741882
Epoch 600, training loss: 6.481385707855225 = 0.16267378628253937 + 1.0 * 6.318711757659912
Epoch 600, val loss: 0.6488693356513977
Epoch 610, training loss: 6.470528602600098 = 0.15274997055530548 + 1.0 * 6.317778587341309
Epoch 610, val loss: 0.6492127180099487
Epoch 620, training loss: 6.460339546203613 = 0.14352235198020935 + 1.0 * 6.316817283630371
Epoch 620, val loss: 0.6502680778503418
Epoch 630, training loss: 6.451898097991943 = 0.13497240841388702 + 1.0 * 6.316925525665283
Epoch 630, val loss: 0.651742696762085
Epoch 640, training loss: 6.442601680755615 = 0.12706659734249115 + 1.0 * 6.315535068511963
Epoch 640, val loss: 0.6538267731666565
Epoch 650, training loss: 6.435565948486328 = 0.11978885531425476 + 1.0 * 6.31577730178833
Epoch 650, val loss: 0.6561897993087769
Epoch 660, training loss: 6.424732208251953 = 0.11301887035369873 + 1.0 * 6.311713218688965
Epoch 660, val loss: 0.6589389443397522
Epoch 670, training loss: 6.415042877197266 = 0.1066933199763298 + 1.0 * 6.308349609375
Epoch 670, val loss: 0.6621869206428528
Epoch 680, training loss: 6.407721996307373 = 0.10077988356351852 + 1.0 * 6.306941986083984
Epoch 680, val loss: 0.665764331817627
Epoch 690, training loss: 6.413331985473633 = 0.0952618345618248 + 1.0 * 6.318069934844971
Epoch 690, val loss: 0.6696748733520508
Epoch 700, training loss: 6.39735221862793 = 0.09017234295606613 + 1.0 * 6.307179927825928
Epoch 700, val loss: 0.6739345788955688
Epoch 710, training loss: 6.392524242401123 = 0.08547117561101913 + 1.0 * 6.307053089141846
Epoch 710, val loss: 0.678151547908783
Epoch 720, training loss: 6.383852481842041 = 0.08107814192771912 + 1.0 * 6.302774429321289
Epoch 720, val loss: 0.6826533079147339
Epoch 730, training loss: 6.391763210296631 = 0.07697393745183945 + 1.0 * 6.314789295196533
Epoch 730, val loss: 0.687433123588562
Epoch 740, training loss: 6.373955726623535 = 0.07316174358129501 + 1.0 * 6.3007941246032715
Epoch 740, val loss: 0.6923157572746277
Epoch 750, training loss: 6.370514869689941 = 0.06962092220783234 + 1.0 * 6.300893783569336
Epoch 750, val loss: 0.6971988677978516
Epoch 760, training loss: 6.363651275634766 = 0.0662977546453476 + 1.0 * 6.297353744506836
Epoch 760, val loss: 0.7022960782051086
Epoch 770, training loss: 6.367599964141846 = 0.0631810650229454 + 1.0 * 6.304419040679932
Epoch 770, val loss: 0.7075037360191345
Epoch 780, training loss: 6.361023902893066 = 0.06027050316333771 + 1.0 * 6.300753593444824
Epoch 780, val loss: 0.712803065776825
Epoch 790, training loss: 6.3584418296813965 = 0.057565972208976746 + 1.0 * 6.300875663757324
Epoch 790, val loss: 0.7180017828941345
Epoch 800, training loss: 6.348565578460693 = 0.05503150075674057 + 1.0 * 6.293534278869629
Epoch 800, val loss: 0.7232322692871094
Epoch 810, training loss: 6.346432209014893 = 0.052644215524196625 + 1.0 * 6.293787956237793
Epoch 810, val loss: 0.7285441756248474
Epoch 820, training loss: 6.343565940856934 = 0.05039820820093155 + 1.0 * 6.293167591094971
Epoch 820, val loss: 0.7339252233505249
Epoch 830, training loss: 6.341981410980225 = 0.048283278942108154 + 1.0 * 6.293698310852051
Epoch 830, val loss: 0.7393311858177185
Epoch 840, training loss: 6.338959217071533 = 0.046299468725919724 + 1.0 * 6.292659759521484
Epoch 840, val loss: 0.7448015213012695
Epoch 850, training loss: 6.334102153778076 = 0.044439006596803665 + 1.0 * 6.289663314819336
Epoch 850, val loss: 0.7500886917114258
Epoch 860, training loss: 6.3311052322387695 = 0.04267607629299164 + 1.0 * 6.288429260253906
Epoch 860, val loss: 0.7554769515991211
Epoch 870, training loss: 6.33393669128418 = 0.04101403057575226 + 1.0 * 6.292922496795654
Epoch 870, val loss: 0.7609233856201172
Epoch 880, training loss: 6.329097747802734 = 0.03945108503103256 + 1.0 * 6.289646625518799
Epoch 880, val loss: 0.7662424445152283
Epoch 890, training loss: 6.322902679443359 = 0.03798140957951546 + 1.0 * 6.284921169281006
Epoch 890, val loss: 0.7714216113090515
Epoch 900, training loss: 6.320934772491455 = 0.03658219054341316 + 1.0 * 6.284352779388428
Epoch 900, val loss: 0.7767022848129272
Epoch 910, training loss: 6.329730033874512 = 0.035254720598459244 + 1.0 * 6.294475078582764
Epoch 910, val loss: 0.7819998264312744
Epoch 920, training loss: 6.31907844543457 = 0.034001901745796204 + 1.0 * 6.28507661819458
Epoch 920, val loss: 0.787278950214386
Epoch 930, training loss: 6.313418388366699 = 0.032817792147397995 + 1.0 * 6.280600547790527
Epoch 930, val loss: 0.792383074760437
Epoch 940, training loss: 6.311901092529297 = 0.0316866971552372 + 1.0 * 6.280214309692383
Epoch 940, val loss: 0.7975509762763977
Epoch 950, training loss: 6.325125694274902 = 0.030613619834184647 + 1.0 * 6.2945122718811035
Epoch 950, val loss: 0.802788496017456
Epoch 960, training loss: 6.3080244064331055 = 0.029599931091070175 + 1.0 * 6.278424263000488
Epoch 960, val loss: 0.807831883430481
Epoch 970, training loss: 6.307455539703369 = 0.028638003394007683 + 1.0 * 6.278817653656006
Epoch 970, val loss: 0.8127261400222778
Epoch 980, training loss: 6.305867671966553 = 0.027717458084225655 + 1.0 * 6.2781500816345215
Epoch 980, val loss: 0.8177258372306824
Epoch 990, training loss: 6.304088592529297 = 0.026838619261980057 + 1.0 * 6.277249813079834
Epoch 990, val loss: 0.8227440714836121
Epoch 1000, training loss: 6.302260398864746 = 0.02600410394370556 + 1.0 * 6.276256084442139
Epoch 1000, val loss: 0.8275709748268127
Epoch 1010, training loss: 6.307363986968994 = 0.025206735357642174 + 1.0 * 6.2821574211120605
Epoch 1010, val loss: 0.8323968052864075
Epoch 1020, training loss: 6.299491882324219 = 0.024448707699775696 + 1.0 * 6.27504301071167
Epoch 1020, val loss: 0.8372249007225037
Epoch 1030, training loss: 6.300015926361084 = 0.02372283674776554 + 1.0 * 6.2762932777404785
Epoch 1030, val loss: 0.8419001698493958
Epoch 1040, training loss: 6.2974395751953125 = 0.0230297539383173 + 1.0 * 6.274409770965576
Epoch 1040, val loss: 0.8467177748680115
Epoch 1050, training loss: 6.2950615882873535 = 0.022371016442775726 + 1.0 * 6.272690773010254
Epoch 1050, val loss: 0.8512528538703918
Epoch 1060, training loss: 6.293223857879639 = 0.021739572286605835 + 1.0 * 6.271484375
Epoch 1060, val loss: 0.8557896614074707
Epoch 1070, training loss: 6.29990291595459 = 0.02113194949924946 + 1.0 * 6.278770923614502
Epoch 1070, val loss: 0.8603859543800354
Epoch 1080, training loss: 6.291491508483887 = 0.02054980956017971 + 1.0 * 6.270941734313965
Epoch 1080, val loss: 0.8649798631668091
Epoch 1090, training loss: 6.289045333862305 = 0.01999513804912567 + 1.0 * 6.269050121307373
Epoch 1090, val loss: 0.8693196177482605
Epoch 1100, training loss: 6.28801965713501 = 0.019460298120975494 + 1.0 * 6.268559455871582
Epoch 1100, val loss: 0.8737239241600037
Epoch 1110, training loss: 6.296475887298584 = 0.018946152180433273 + 1.0 * 6.277529716491699
Epoch 1110, val loss: 0.8781667947769165
Epoch 1120, training loss: 6.28631591796875 = 0.018454376608133316 + 1.0 * 6.267861366271973
Epoch 1120, val loss: 0.8825768828392029
Epoch 1130, training loss: 6.2853007316589355 = 0.01798386685550213 + 1.0 * 6.267316818237305
Epoch 1130, val loss: 0.886701762676239
Epoch 1140, training loss: 6.286670684814453 = 0.017529167234897614 + 1.0 * 6.269141674041748
Epoch 1140, val loss: 0.8909276723861694
Epoch 1150, training loss: 6.287402153015137 = 0.017091339454054832 + 1.0 * 6.270310878753662
Epoch 1150, val loss: 0.895240843296051
Epoch 1160, training loss: 6.283809185028076 = 0.01667431928217411 + 1.0 * 6.267134666442871
Epoch 1160, val loss: 0.8993266820907593
Epoch 1170, training loss: 6.281322002410889 = 0.016270622611045837 + 1.0 * 6.265051364898682
Epoch 1170, val loss: 0.9033392071723938
Epoch 1180, training loss: 6.280303001403809 = 0.015880433842539787 + 1.0 * 6.264422416687012
Epoch 1180, val loss: 0.9074229001998901
Epoch 1190, training loss: 6.288306713104248 = 0.015503508038818836 + 1.0 * 6.27280330657959
Epoch 1190, val loss: 0.9115322828292847
Epoch 1200, training loss: 6.278999328613281 = 0.015142844058573246 + 1.0 * 6.263856410980225
Epoch 1200, val loss: 0.9154762029647827
Epoch 1210, training loss: 6.281725883483887 = 0.014795802533626556 + 1.0 * 6.266930103302002
Epoch 1210, val loss: 0.919299840927124
Epoch 1220, training loss: 6.28074836730957 = 0.014460010454058647 + 1.0 * 6.2662882804870605
Epoch 1220, val loss: 0.9232993125915527
Epoch 1230, training loss: 6.2760138511657715 = 0.014138367027044296 + 1.0 * 6.261875629425049
Epoch 1230, val loss: 0.9270758628845215
Epoch 1240, training loss: 6.2746076583862305 = 0.013827222399413586 + 1.0 * 6.260780334472656
Epoch 1240, val loss: 0.9307622313499451
Epoch 1250, training loss: 6.279067516326904 = 0.013524219393730164 + 1.0 * 6.265543460845947
Epoch 1250, val loss: 0.9345203042030334
Epoch 1260, training loss: 6.273748397827148 = 0.013231451623141766 + 1.0 * 6.260517120361328
Epoch 1260, val loss: 0.9383834004402161
Epoch 1270, training loss: 6.2723469734191895 = 0.012949291616678238 + 1.0 * 6.259397506713867
Epoch 1270, val loss: 0.941974937915802
Epoch 1280, training loss: 6.272394180297852 = 0.012676116079092026 + 1.0 * 6.25971794128418
Epoch 1280, val loss: 0.9455475807189941
Epoch 1290, training loss: 6.274834632873535 = 0.012409387156367302 + 1.0 * 6.262425422668457
Epoch 1290, val loss: 0.9492287039756775
Epoch 1300, training loss: 6.272335052490234 = 0.012152699753642082 + 1.0 * 6.2601823806762695
Epoch 1300, val loss: 0.9529021978378296
Epoch 1310, training loss: 6.277572154998779 = 0.011905724182724953 + 1.0 * 6.265666484832764
Epoch 1310, val loss: 0.9564007520675659
Epoch 1320, training loss: 6.270171642303467 = 0.011668439954519272 + 1.0 * 6.258503437042236
Epoch 1320, val loss: 0.9598537683486938
Epoch 1330, training loss: 6.2685699462890625 = 0.011437115259468555 + 1.0 * 6.2571330070495605
Epoch 1330, val loss: 0.9632362723350525
Epoch 1340, training loss: 6.268456935882568 = 0.011211510747671127 + 1.0 * 6.2572455406188965
Epoch 1340, val loss: 0.9666701555252075
Epoch 1350, training loss: 6.278182506561279 = 0.010992394760251045 + 1.0 * 6.267189979553223
Epoch 1350, val loss: 0.9701619744300842
Epoch 1360, training loss: 6.270513534545898 = 0.010780086740851402 + 1.0 * 6.2597336769104
Epoch 1360, val loss: 0.9735864996910095
Epoch 1370, training loss: 6.2687201499938965 = 0.010575927793979645 + 1.0 * 6.258144378662109
Epoch 1370, val loss: 0.9767762422561646
Epoch 1380, training loss: 6.272868633270264 = 0.010376823134720325 + 1.0 * 6.262491703033447
Epoch 1380, val loss: 0.9800683856010437
Epoch 1390, training loss: 6.266468524932861 = 0.0101837869733572 + 1.0 * 6.256284713745117
Epoch 1390, val loss: 0.9834073781967163
Epoch 1400, training loss: 6.2731242179870605 = 0.009996451437473297 + 1.0 * 6.26312780380249
Epoch 1400, val loss: 0.9866260886192322
Epoch 1410, training loss: 6.265871524810791 = 0.00981452688574791 + 1.0 * 6.256056785583496
Epoch 1410, val loss: 0.9898365139961243
Epoch 1420, training loss: 6.263704776763916 = 0.009638444520533085 + 1.0 * 6.254066467285156
Epoch 1420, val loss: 0.9929261207580566
Epoch 1430, training loss: 6.2628912925720215 = 0.009466135874390602 + 1.0 * 6.253425121307373
Epoch 1430, val loss: 0.996037483215332
Epoch 1440, training loss: 6.273436069488525 = 0.009298283606767654 + 1.0 * 6.2641377449035645
Epoch 1440, val loss: 0.9992066621780396
Epoch 1450, training loss: 6.2715325355529785 = 0.009135368280112743 + 1.0 * 6.262397289276123
Epoch 1450, val loss: 1.0023655891418457
Epoch 1460, training loss: 6.263834476470947 = 0.008979208767414093 + 1.0 * 6.254855155944824
Epoch 1460, val loss: 1.0053437948226929
Epoch 1470, training loss: 6.271193504333496 = 0.008827445097267628 + 1.0 * 6.26236629486084
Epoch 1470, val loss: 1.0082932710647583
Epoch 1480, training loss: 6.262368679046631 = 0.008678345009684563 + 1.0 * 6.253690242767334
Epoch 1480, val loss: 1.0113126039505005
Epoch 1490, training loss: 6.259265422821045 = 0.008534626103937626 + 1.0 * 6.250730991363525
Epoch 1490, val loss: 1.014180302619934
Epoch 1500, training loss: 6.259757041931152 = 0.008393197320401669 + 1.0 * 6.251363754272461
Epoch 1500, val loss: 1.0170966386795044
Epoch 1510, training loss: 6.2662835121154785 = 0.008254531770944595 + 1.0 * 6.258028984069824
Epoch 1510, val loss: 1.0200976133346558
Epoch 1520, training loss: 6.264372825622559 = 0.008119412697851658 + 1.0 * 6.256253242492676
Epoch 1520, val loss: 1.0230684280395508
Epoch 1530, training loss: 6.258642673492432 = 0.007989988662302494 + 1.0 * 6.25065279006958
Epoch 1530, val loss: 1.0258502960205078
Epoch 1540, training loss: 6.261434078216553 = 0.007863353937864304 + 1.0 * 6.253570556640625
Epoch 1540, val loss: 1.0285500288009644
Epoch 1550, training loss: 6.260701656341553 = 0.007739209104329348 + 1.0 * 6.252962589263916
Epoch 1550, val loss: 1.0314549207687378
Epoch 1560, training loss: 6.2574262619018555 = 0.007618609815835953 + 1.0 * 6.249807834625244
Epoch 1560, val loss: 1.0342156887054443
Epoch 1570, training loss: 6.255823612213135 = 0.0075011225417256355 + 1.0 * 6.248322486877441
Epoch 1570, val loss: 1.0368934869766235
Epoch 1580, training loss: 6.260470867156982 = 0.007385637611150742 + 1.0 * 6.253085136413574
Epoch 1580, val loss: 1.039650559425354
Epoch 1590, training loss: 6.25676155090332 = 0.007272242568433285 + 1.0 * 6.2494893074035645
Epoch 1590, val loss: 1.0424880981445312
Epoch 1600, training loss: 6.259467601776123 = 0.0071627614088356495 + 1.0 * 6.252305030822754
Epoch 1600, val loss: 1.0451666116714478
Epoch 1610, training loss: 6.25656795501709 = 0.007055893540382385 + 1.0 * 6.249512195587158
Epoch 1610, val loss: 1.0478460788726807
Epoch 1620, training loss: 6.255629062652588 = 0.006952994968742132 + 1.0 * 6.248676300048828
Epoch 1620, val loss: 1.0503745079040527
Epoch 1630, training loss: 6.253685474395752 = 0.00685126194730401 + 1.0 * 6.2468342781066895
Epoch 1630, val loss: 1.0528959035873413
Epoch 1640, training loss: 6.258988857269287 = 0.006751338951289654 + 1.0 * 6.252237319946289
Epoch 1640, val loss: 1.0555238723754883
Epoch 1650, training loss: 6.254389762878418 = 0.006653479300439358 + 1.0 * 6.24773645401001
Epoch 1650, val loss: 1.0582503080368042
Epoch 1660, training loss: 6.259859085083008 = 0.006558900699019432 + 1.0 * 6.253300189971924
Epoch 1660, val loss: 1.0607655048370361
Epoch 1670, training loss: 6.252438068389893 = 0.00646695401519537 + 1.0 * 6.245971202850342
Epoch 1670, val loss: 1.063177466392517
Epoch 1680, training loss: 6.25389289855957 = 0.006377123296260834 + 1.0 * 6.247515678405762
Epoch 1680, val loss: 1.065585970878601
Epoch 1690, training loss: 6.259366035461426 = 0.006288715172559023 + 1.0 * 6.253077507019043
Epoch 1690, val loss: 1.0681464672088623
Epoch 1700, training loss: 6.253437042236328 = 0.006202209275215864 + 1.0 * 6.24723482131958
Epoch 1700, val loss: 1.0705689191818237
Epoch 1710, training loss: 6.250533580780029 = 0.006118633784353733 + 1.0 * 6.244414806365967
Epoch 1710, val loss: 1.0728662014007568
Epoch 1720, training loss: 6.250621318817139 = 0.006035556551069021 + 1.0 * 6.244585990905762
Epoch 1720, val loss: 1.075269341468811
Epoch 1730, training loss: 6.255445957183838 = 0.005953952204436064 + 1.0 * 6.249492168426514
Epoch 1730, val loss: 1.0777311325073242
Epoch 1740, training loss: 6.254695415496826 = 0.0058740307576954365 + 1.0 * 6.248821258544922
Epoch 1740, val loss: 1.0802462100982666
Epoch 1750, training loss: 6.250748157501221 = 0.005797016900032759 + 1.0 * 6.244951248168945
Epoch 1750, val loss: 1.082554578781128
Epoch 1760, training loss: 6.249464511871338 = 0.005721920635551214 + 1.0 * 6.2437424659729
Epoch 1760, val loss: 1.0847134590148926
Epoch 1770, training loss: 6.250037670135498 = 0.005648154765367508 + 1.0 * 6.244389533996582
Epoch 1770, val loss: 1.0869382619857788
Epoch 1780, training loss: 6.253836631774902 = 0.005575274582952261 + 1.0 * 6.248261451721191
Epoch 1780, val loss: 1.0893114805221558
Epoch 1790, training loss: 6.2493791580200195 = 0.0055039613507688046 + 1.0 * 6.243875026702881
Epoch 1790, val loss: 1.091609001159668
Epoch 1800, training loss: 6.247697830200195 = 0.005434364080429077 + 1.0 * 6.242263317108154
Epoch 1800, val loss: 1.0937855243682861
Epoch 1810, training loss: 6.25647497177124 = 0.0053660329431295395 + 1.0 * 6.2511091232299805
Epoch 1810, val loss: 1.0960681438446045
Epoch 1820, training loss: 6.249604225158691 = 0.005299024283885956 + 1.0 * 6.24430513381958
Epoch 1820, val loss: 1.0983541011810303
Epoch 1830, training loss: 6.247003078460693 = 0.005234294105321169 + 1.0 * 6.241768836975098
Epoch 1830, val loss: 1.1004421710968018
Epoch 1840, training loss: 6.246184825897217 = 0.005170780699700117 + 1.0 * 6.241014003753662
Epoch 1840, val loss: 1.102489948272705
Epoch 1850, training loss: 6.251328945159912 = 0.0051078228279948235 + 1.0 * 6.24622106552124
Epoch 1850, val loss: 1.1046684980392456
Epoch 1860, training loss: 6.24500036239624 = 0.005045893136411905 + 1.0 * 6.239954471588135
Epoch 1860, val loss: 1.1069107055664062
Epoch 1870, training loss: 6.246456623077393 = 0.004985020961612463 + 1.0 * 6.241471767425537
Epoch 1870, val loss: 1.1089808940887451
Epoch 1880, training loss: 6.2614593505859375 = 0.0049253120087087154 + 1.0 * 6.256534099578857
Epoch 1880, val loss: 1.1112340688705444
Epoch 1890, training loss: 6.247811794281006 = 0.004868431016802788 + 1.0 * 6.242943286895752
Epoch 1890, val loss: 1.1133506298065186
Epoch 1900, training loss: 6.244658470153809 = 0.0048123374581336975 + 1.0 * 6.239846229553223
Epoch 1900, val loss: 1.11518394947052
Epoch 1910, training loss: 6.2433271408081055 = 0.004757218062877655 + 1.0 * 6.238569736480713
Epoch 1910, val loss: 1.1171276569366455
Epoch 1920, training loss: 6.243545055389404 = 0.004701984580606222 + 1.0 * 6.238842964172363
Epoch 1920, val loss: 1.1192398071289062
Epoch 1930, training loss: 6.255735874176025 = 0.004647496622055769 + 1.0 * 6.2510881423950195
Epoch 1930, val loss: 1.1214210987091064
Epoch 1940, training loss: 6.2478532791137695 = 0.004594237543642521 + 1.0 * 6.243258953094482
Epoch 1940, val loss: 1.123509168624878
Epoch 1950, training loss: 6.247451305389404 = 0.004543180577456951 + 1.0 * 6.242908000946045
Epoch 1950, val loss: 1.1253784894943237
Epoch 1960, training loss: 6.242834568023682 = 0.004492668434977531 + 1.0 * 6.238341808319092
Epoch 1960, val loss: 1.127346158027649
Epoch 1970, training loss: 6.243094444274902 = 0.004442788194864988 + 1.0 * 6.238651752471924
Epoch 1970, val loss: 1.1292709112167358
Epoch 1980, training loss: 6.242353916168213 = 0.004393712617456913 + 1.0 * 6.237960338592529
Epoch 1980, val loss: 1.1312575340270996
Epoch 1990, training loss: 6.24423885345459 = 0.004345059394836426 + 1.0 * 6.239893913269043
Epoch 1990, val loss: 1.1332423686981201
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 10.568524360656738 = 1.9716928005218506 + 1.0 * 8.596831321716309
Epoch 0, val loss: 1.9637295007705688
Epoch 10, training loss: 10.556059837341309 = 1.9595335721969604 + 1.0 * 8.596526145935059
Epoch 10, val loss: 1.9522221088409424
Epoch 20, training loss: 10.53805160522461 = 1.9441800117492676 + 1.0 * 8.5938720703125
Epoch 20, val loss: 1.9369471073150635
Epoch 30, training loss: 10.494335174560547 = 1.9223926067352295 + 1.0 * 8.571942329406738
Epoch 30, val loss: 1.9151098728179932
Epoch 40, training loss: 10.3364839553833 = 1.8933446407318115 + 1.0 * 8.44313907623291
Epoch 40, val loss: 1.8868868350982666
Epoch 50, training loss: 9.791255950927734 = 1.8622816801071167 + 1.0 * 7.928974151611328
Epoch 50, val loss: 1.857844352722168
Epoch 60, training loss: 9.452442169189453 = 1.8368582725524902 + 1.0 * 7.615583419799805
Epoch 60, val loss: 1.8350794315338135
Epoch 70, training loss: 9.080628395080566 = 1.8187724351882935 + 1.0 * 7.261855602264404
Epoch 70, val loss: 1.8184993267059326
Epoch 80, training loss: 8.857076644897461 = 1.8037266731262207 + 1.0 * 7.05334997177124
Epoch 80, val loss: 1.8048537969589233
Epoch 90, training loss: 8.718225479125977 = 1.7866393327713013 + 1.0 * 6.931586265563965
Epoch 90, val loss: 1.7898489236831665
Epoch 100, training loss: 8.601048469543457 = 1.7691383361816406 + 1.0 * 6.831910133361816
Epoch 100, val loss: 1.7748332023620605
Epoch 110, training loss: 8.514062881469727 = 1.7520751953125 + 1.0 * 6.761988162994385
Epoch 110, val loss: 1.7597469091415405
Epoch 120, training loss: 8.437036514282227 = 1.733517050743103 + 1.0 * 6.703519344329834
Epoch 120, val loss: 1.743477463722229
Epoch 130, training loss: 8.37083911895752 = 1.711950659751892 + 1.0 * 6.658888339996338
Epoch 130, val loss: 1.7250539064407349
Epoch 140, training loss: 8.308710098266602 = 1.6874992847442627 + 1.0 * 6.62121057510376
Epoch 140, val loss: 1.704286813735962
Epoch 150, training loss: 8.246257781982422 = 1.6595396995544434 + 1.0 * 6.58671760559082
Epoch 150, val loss: 1.6801882982254028
Epoch 160, training loss: 8.180035591125488 = 1.6269437074661255 + 1.0 * 6.553092002868652
Epoch 160, val loss: 1.6519055366516113
Epoch 170, training loss: 8.117230415344238 = 1.5898826122283936 + 1.0 * 6.527348041534424
Epoch 170, val loss: 1.619976282119751
Epoch 180, training loss: 8.056744575500488 = 1.548928141593933 + 1.0 * 6.507816791534424
Epoch 180, val loss: 1.5848222970962524
Epoch 190, training loss: 7.994805812835693 = 1.5038704872131348 + 1.0 * 6.490935325622559
Epoch 190, val loss: 1.546347737312317
Epoch 200, training loss: 7.93228006362915 = 1.4553924798965454 + 1.0 * 6.4768877029418945
Epoch 200, val loss: 1.5054388046264648
Epoch 210, training loss: 7.875121116638184 = 1.4049313068389893 + 1.0 * 6.470189571380615
Epoch 210, val loss: 1.4635790586471558
Epoch 220, training loss: 7.8100481033325195 = 1.3557918071746826 + 1.0 * 6.454256534576416
Epoch 220, val loss: 1.4239017963409424
Epoch 230, training loss: 7.752098083496094 = 1.3081809282302856 + 1.0 * 6.443917274475098
Epoch 230, val loss: 1.3858792781829834
Epoch 240, training loss: 7.697044372558594 = 1.2620803117752075 + 1.0 * 6.434964179992676
Epoch 240, val loss: 1.349604606628418
Epoch 250, training loss: 7.649998188018799 = 1.218170166015625 + 1.0 * 6.431828022003174
Epoch 250, val loss: 1.3155943155288696
Epoch 260, training loss: 7.598956108093262 = 1.176358699798584 + 1.0 * 6.422597408294678
Epoch 260, val loss: 1.2835745811462402
Epoch 270, training loss: 7.549280166625977 = 1.1351712942123413 + 1.0 * 6.414108753204346
Epoch 270, val loss: 1.2522004842758179
Epoch 280, training loss: 7.50177526473999 = 1.093947410583496 + 1.0 * 6.407827854156494
Epoch 280, val loss: 1.2210350036621094
Epoch 290, training loss: 7.45986270904541 = 1.052651047706604 + 1.0 * 6.407211780548096
Epoch 290, val loss: 1.1899458169937134
Epoch 300, training loss: 7.408843040466309 = 1.0114688873291016 + 1.0 * 6.397374153137207
Epoch 300, val loss: 1.1590642929077148
Epoch 310, training loss: 7.363117218017578 = 0.9700927734375 + 1.0 * 6.393024444580078
Epoch 310, val loss: 1.1282377243041992
Epoch 320, training loss: 7.324811935424805 = 0.9286767244338989 + 1.0 * 6.396135330200195
Epoch 320, val loss: 1.097428321838379
Epoch 330, training loss: 7.276963710784912 = 0.888367235660553 + 1.0 * 6.388596534729004
Epoch 330, val loss: 1.0675128698349
Epoch 340, training loss: 7.230747699737549 = 0.8492693305015564 + 1.0 * 6.381478309631348
Epoch 340, val loss: 1.0387369394302368
Epoch 350, training loss: 7.188148498535156 = 0.8113744258880615 + 1.0 * 6.376774311065674
Epoch 350, val loss: 1.011024832725525
Epoch 360, training loss: 7.162588596343994 = 0.7753138542175293 + 1.0 * 6.387274742126465
Epoch 360, val loss: 0.9850045442581177
Epoch 370, training loss: 7.11477518081665 = 0.7416804432868958 + 1.0 * 6.37309455871582
Epoch 370, val loss: 0.9611529111862183
Epoch 380, training loss: 7.077771186828613 = 0.7097659707069397 + 1.0 * 6.368005275726318
Epoch 380, val loss: 0.9392423033714294
Epoch 390, training loss: 7.044122219085693 = 0.6793379783630371 + 1.0 * 6.364784240722656
Epoch 390, val loss: 0.9189234375953674
Epoch 400, training loss: 7.013243675231934 = 0.6505024433135986 + 1.0 * 6.362741470336914
Epoch 400, val loss: 0.9004995226860046
Epoch 410, training loss: 6.982454299926758 = 0.6232956647872925 + 1.0 * 6.359158515930176
Epoch 410, val loss: 0.8839424848556519
Epoch 420, training loss: 6.953604698181152 = 0.5973322987556458 + 1.0 * 6.356272220611572
Epoch 420, val loss: 0.8689831495285034
Epoch 430, training loss: 6.926013469696045 = 0.5724085569381714 + 1.0 * 6.353604793548584
Epoch 430, val loss: 0.8554474115371704
Epoch 440, training loss: 6.901437282562256 = 0.5485559105873108 + 1.0 * 6.35288143157959
Epoch 440, val loss: 0.843319833278656
Epoch 450, training loss: 6.878087997436523 = 0.5258864760398865 + 1.0 * 6.352201461791992
Epoch 450, val loss: 0.8325158953666687
Epoch 460, training loss: 6.851158142089844 = 0.5040402412414551 + 1.0 * 6.347117900848389
Epoch 460, val loss: 0.8227211236953735
Epoch 470, training loss: 6.840320587158203 = 0.48291516304016113 + 1.0 * 6.357405185699463
Epoch 470, val loss: 0.8139172196388245
Epoch 480, training loss: 6.808340549468994 = 0.46269944310188293 + 1.0 * 6.345641136169434
Epoch 480, val loss: 0.8061150312423706
Epoch 490, training loss: 6.783935070037842 = 0.44305115938186646 + 1.0 * 6.340883731842041
Epoch 490, val loss: 0.7989925146102905
Epoch 500, training loss: 6.7636542320251465 = 0.42385509610176086 + 1.0 * 6.339798927307129
Epoch 500, val loss: 0.792485237121582
Epoch 510, training loss: 6.748267650604248 = 0.4052085280418396 + 1.0 * 6.343059062957764
Epoch 510, val loss: 0.7866095304489136
Epoch 520, training loss: 6.725322723388672 = 0.38717710971832275 + 1.0 * 6.338145732879639
Epoch 520, val loss: 0.7812777161598206
Epoch 530, training loss: 6.704065322875977 = 0.3695753812789917 + 1.0 * 6.334489822387695
Epoch 530, val loss: 0.7763568758964539
Epoch 540, training loss: 6.693337917327881 = 0.3523654639720917 + 1.0 * 6.340972423553467
Epoch 540, val loss: 0.7718778252601624
Epoch 550, training loss: 6.670007228851318 = 0.33567100763320923 + 1.0 * 6.334336280822754
Epoch 550, val loss: 0.7678853273391724
Epoch 560, training loss: 6.648456573486328 = 0.31938856840133667 + 1.0 * 6.329068183898926
Epoch 560, val loss: 0.7642163634300232
Epoch 570, training loss: 6.631499290466309 = 0.30348560214042664 + 1.0 * 6.328013896942139
Epoch 570, val loss: 0.7609989047050476
Epoch 580, training loss: 6.619150161743164 = 0.28799551725387573 + 1.0 * 6.331154823303223
Epoch 580, val loss: 0.7582411170005798
Epoch 590, training loss: 6.601630687713623 = 0.27297431230545044 + 1.0 * 6.328656196594238
Epoch 590, val loss: 0.7558412551879883
Epoch 600, training loss: 6.590752601623535 = 0.2583564519882202 + 1.0 * 6.332396030426025
Epoch 600, val loss: 0.7538184523582458
Epoch 610, training loss: 6.568049430847168 = 0.2441733479499817 + 1.0 * 6.323875904083252
Epoch 610, val loss: 0.7522327899932861
Epoch 620, training loss: 6.552668571472168 = 0.2303849458694458 + 1.0 * 6.322283744812012
Epoch 620, val loss: 0.7511698603630066
Epoch 630, training loss: 6.542328834533691 = 0.21705734729766846 + 1.0 * 6.3252716064453125
Epoch 630, val loss: 0.7506426572799683
Epoch 640, training loss: 6.524374485015869 = 0.2042936533689499 + 1.0 * 6.320080757141113
Epoch 640, val loss: 0.7506089210510254
Epoch 650, training loss: 6.509397506713867 = 0.1920132040977478 + 1.0 * 6.317384243011475
Epoch 650, val loss: 0.75118088722229
Epoch 660, training loss: 6.503844738006592 = 0.1802804321050644 + 1.0 * 6.323564529418945
Epoch 660, val loss: 0.7523577809333801
Epoch 670, training loss: 6.486741065979004 = 0.16919249296188354 + 1.0 * 6.317548751831055
Epoch 670, val loss: 0.754065215587616
Epoch 680, training loss: 6.476474761962891 = 0.15872201323509216 + 1.0 * 6.317752838134766
Epoch 680, val loss: 0.7563552260398865
Epoch 690, training loss: 6.462031364440918 = 0.1489032804965973 + 1.0 * 6.3131279945373535
Epoch 690, val loss: 0.7592731714248657
Epoch 700, training loss: 6.451615333557129 = 0.13973158597946167 + 1.0 * 6.311883926391602
Epoch 700, val loss: 0.7627176642417908
Epoch 710, training loss: 6.447532653808594 = 0.13120046257972717 + 1.0 * 6.3163323402404785
Epoch 710, val loss: 0.7667483687400818
Epoch 720, training loss: 6.432638168334961 = 0.12332109361886978 + 1.0 * 6.309317111968994
Epoch 720, val loss: 0.7712085843086243
Epoch 730, training loss: 6.424894332885742 = 0.1160285472869873 + 1.0 * 6.308866024017334
Epoch 730, val loss: 0.7760961055755615
Epoch 740, training loss: 6.418599605560303 = 0.1092904731631279 + 1.0 * 6.309309005737305
Epoch 740, val loss: 0.7813795208930969
Epoch 750, training loss: 6.409458160400391 = 0.10307289659976959 + 1.0 * 6.306385040283203
Epoch 750, val loss: 0.7868725061416626
Epoch 760, training loss: 6.406160831451416 = 0.09733327478170395 + 1.0 * 6.3088274002075195
Epoch 760, val loss: 0.7926523089408875
Epoch 770, training loss: 6.400459289550781 = 0.09203170984983444 + 1.0 * 6.308427810668945
Epoch 770, val loss: 0.7986602783203125
Epoch 780, training loss: 6.390454292297363 = 0.08713797479867935 + 1.0 * 6.303316116333008
Epoch 780, val loss: 0.8047189712524414
Epoch 790, training loss: 6.38445520401001 = 0.08259847015142441 + 1.0 * 6.301856517791748
Epoch 790, val loss: 0.8110178112983704
Epoch 800, training loss: 6.386975288391113 = 0.07839230448007584 + 1.0 * 6.308582782745361
Epoch 800, val loss: 0.8174086213111877
Epoch 810, training loss: 6.373655319213867 = 0.07450570911169052 + 1.0 * 6.299149513244629
Epoch 810, val loss: 0.8237305879592896
Epoch 820, training loss: 6.373056888580322 = 0.07088785618543625 + 1.0 * 6.302168846130371
Epoch 820, val loss: 0.8301569819450378
Epoch 830, training loss: 6.365665912628174 = 0.06752036511898041 + 1.0 * 6.298145771026611
Epoch 830, val loss: 0.836621105670929
Epoch 840, training loss: 6.364038944244385 = 0.0643758475780487 + 1.0 * 6.299663066864014
Epoch 840, val loss: 0.8430650234222412
Epoch 850, training loss: 6.361393928527832 = 0.061439745128154755 + 1.0 * 6.299954414367676
Epoch 850, val loss: 0.8495497703552246
Epoch 860, training loss: 6.353489398956299 = 0.05869777128100395 + 1.0 * 6.2947916984558105
Epoch 860, val loss: 0.8560097813606262
Epoch 870, training loss: 6.3532795906066895 = 0.05612615868449211 + 1.0 * 6.297153472900391
Epoch 870, val loss: 0.8624352216720581
Epoch 880, training loss: 6.348091125488281 = 0.053721826523542404 + 1.0 * 6.294369220733643
Epoch 880, val loss: 0.8688370585441589
Epoch 890, training loss: 6.349449634552002 = 0.05146683380007744 + 1.0 * 6.297982692718506
Epoch 890, val loss: 0.8750967979431152
Epoch 900, training loss: 6.342838764190674 = 0.049355942755937576 + 1.0 * 6.293482780456543
Epoch 900, val loss: 0.8814242482185364
Epoch 910, training loss: 6.3387322425842285 = 0.04737174138426781 + 1.0 * 6.291360378265381
Epoch 910, val loss: 0.887577474117279
Epoch 920, training loss: 6.334988117218018 = 0.04549581557512283 + 1.0 * 6.289492130279541
Epoch 920, val loss: 0.8938334584236145
Epoch 930, training loss: 6.339386940002441 = 0.04372343420982361 + 1.0 * 6.295663356781006
Epoch 930, val loss: 0.9000487327575684
Epoch 940, training loss: 6.335324287414551 = 0.042057786136865616 + 1.0 * 6.293266296386719
Epoch 940, val loss: 0.906160831451416
Epoch 950, training loss: 6.327713489532471 = 0.04048743471503258 + 1.0 * 6.28722620010376
Epoch 950, val loss: 0.9120957255363464
Epoch 960, training loss: 6.3249711990356445 = 0.038998767733573914 + 1.0 * 6.285972595214844
Epoch 960, val loss: 0.9180524349212646
Epoch 970, training loss: 6.325488567352295 = 0.037584248930215836 + 1.0 * 6.287904262542725
Epoch 970, val loss: 0.9240509867668152
Epoch 980, training loss: 6.321261405944824 = 0.036248333752155304 + 1.0 * 6.285013198852539
Epoch 980, val loss: 0.9299812316894531
Epoch 990, training loss: 6.320906639099121 = 0.03498451039195061 + 1.0 * 6.285922050476074
Epoch 990, val loss: 0.9356380105018616
Epoch 1000, training loss: 6.316991806030273 = 0.03378395363688469 + 1.0 * 6.283207893371582
Epoch 1000, val loss: 0.941377580165863
Epoch 1010, training loss: 6.336410045623779 = 0.032640863209962845 + 1.0 * 6.303769111633301
Epoch 1010, val loss: 0.9471478462219238
Epoch 1020, training loss: 6.317534923553467 = 0.031562238931655884 + 1.0 * 6.285972595214844
Epoch 1020, val loss: 0.9526568651199341
Epoch 1030, training loss: 6.3107500076293945 = 0.030535519123077393 + 1.0 * 6.280214309692383
Epoch 1030, val loss: 0.9580655694007874
Epoch 1040, training loss: 6.3093061447143555 = 0.02955322153866291 + 1.0 * 6.279752731323242
Epoch 1040, val loss: 0.9636362791061401
Epoch 1050, training loss: 6.312904357910156 = 0.02861352264881134 + 1.0 * 6.284290790557861
Epoch 1050, val loss: 0.969174325466156
Epoch 1060, training loss: 6.314045429229736 = 0.027725042775273323 + 1.0 * 6.286320209503174
Epoch 1060, val loss: 0.9745251536369324
Epoch 1070, training loss: 6.304795742034912 = 0.02687867544591427 + 1.0 * 6.27791690826416
Epoch 1070, val loss: 0.9797084331512451
Epoch 1080, training loss: 6.303013324737549 = 0.026067808270454407 + 1.0 * 6.2769455909729
Epoch 1080, val loss: 0.9849510192871094
Epoch 1090, training loss: 6.3052754402160645 = 0.02528924122452736 + 1.0 * 6.279986381530762
Epoch 1090, val loss: 0.9902273416519165
Epoch 1100, training loss: 6.301563262939453 = 0.024547608569264412 + 1.0 * 6.277015686035156
Epoch 1100, val loss: 0.995422899723053
Epoch 1110, training loss: 6.311827659606934 = 0.02384161949157715 + 1.0 * 6.2879862785339355
Epoch 1110, val loss: 1.000394582748413
Epoch 1120, training loss: 6.300063610076904 = 0.02316843904554844 + 1.0 * 6.276895046234131
Epoch 1120, val loss: 1.0052775144577026
Epoch 1130, training loss: 6.297659873962402 = 0.022521251812577248 + 1.0 * 6.275138854980469
Epoch 1130, val loss: 1.0102366209030151
Epoch 1140, training loss: 6.2973833084106445 = 0.02189781703054905 + 1.0 * 6.275485515594482
Epoch 1140, val loss: 1.0151782035827637
Epoch 1150, training loss: 6.299410343170166 = 0.021300949156284332 + 1.0 * 6.278109550476074
Epoch 1150, val loss: 1.0200735330581665
Epoch 1160, training loss: 6.293280124664307 = 0.020729929208755493 + 1.0 * 6.272550106048584
Epoch 1160, val loss: 1.0247858762741089
Epoch 1170, training loss: 6.292476177215576 = 0.020180849358439445 + 1.0 * 6.2722954750061035
Epoch 1170, val loss: 1.029491901397705
Epoch 1180, training loss: 6.298466205596924 = 0.019651489332318306 + 1.0 * 6.278814792633057
Epoch 1180, val loss: 1.0342453718185425
Epoch 1190, training loss: 6.289735317230225 = 0.019144099205732346 + 1.0 * 6.2705912590026855
Epoch 1190, val loss: 1.038867712020874
Epoch 1200, training loss: 6.292157173156738 = 0.018656382337212563 + 1.0 * 6.273500919342041
Epoch 1200, val loss: 1.0433921813964844
Epoch 1210, training loss: 6.291991233825684 = 0.018187478184700012 + 1.0 * 6.2738037109375
Epoch 1210, val loss: 1.0479730367660522
Epoch 1220, training loss: 6.28790283203125 = 0.01773831993341446 + 1.0 * 6.270164489746094
Epoch 1220, val loss: 1.0523467063903809
Epoch 1230, training loss: 6.2904229164123535 = 0.017304997891187668 + 1.0 * 6.273118019104004
Epoch 1230, val loss: 1.0567514896392822
Epoch 1240, training loss: 6.285898685455322 = 0.01688765175640583 + 1.0 * 6.2690110206604
Epoch 1240, val loss: 1.0611486434936523
Epoch 1250, training loss: 6.285175323486328 = 0.016485076397657394 + 1.0 * 6.26869010925293
Epoch 1250, val loss: 1.065421462059021
Epoch 1260, training loss: 6.285327911376953 = 0.016095371916890144 + 1.0 * 6.269232749938965
Epoch 1260, val loss: 1.0697383880615234
Epoch 1270, training loss: 6.287568092346191 = 0.015719328075647354 + 1.0 * 6.271848678588867
Epoch 1270, val loss: 1.0740394592285156
Epoch 1280, training loss: 6.285463809967041 = 0.0153593635186553 + 1.0 * 6.27010440826416
Epoch 1280, val loss: 1.0782188177108765
Epoch 1290, training loss: 6.283137798309326 = 0.015012436546385288 + 1.0 * 6.268125534057617
Epoch 1290, val loss: 1.0822325944900513
Epoch 1300, training loss: 6.280456066131592 = 0.014676262624561787 + 1.0 * 6.265779972076416
Epoch 1300, val loss: 1.0863114595413208
Epoch 1310, training loss: 6.284912586212158 = 0.014349832199513912 + 1.0 * 6.270562648773193
Epoch 1310, val loss: 1.0904407501220703
Epoch 1320, training loss: 6.283533573150635 = 0.014036919921636581 + 1.0 * 6.269496440887451
Epoch 1320, val loss: 1.0945278406143188
Epoch 1330, training loss: 6.284031391143799 = 0.013735655695199966 + 1.0 * 6.2702956199646
Epoch 1330, val loss: 1.0983335971832275
Epoch 1340, training loss: 6.27944803237915 = 0.013444013893604279 + 1.0 * 6.2660040855407715
Epoch 1340, val loss: 1.1022157669067383
Epoch 1350, training loss: 6.277004718780518 = 0.013160958886146545 + 1.0 * 6.263843536376953
Epoch 1350, val loss: 1.1061272621154785
Epoch 1360, training loss: 6.276973247528076 = 0.0128855025395751 + 1.0 * 6.264087677001953
Epoch 1360, val loss: 1.1100126504898071
Epoch 1370, training loss: 6.286776542663574 = 0.012619278393685818 + 1.0 * 6.2741570472717285
Epoch 1370, val loss: 1.1139307022094727
Epoch 1380, training loss: 6.279229164123535 = 0.01236241590231657 + 1.0 * 6.266866683959961
Epoch 1380, val loss: 1.1176891326904297
Epoch 1390, training loss: 6.278889179229736 = 0.01211394090205431 + 1.0 * 6.266775131225586
Epoch 1390, val loss: 1.1212953329086304
Epoch 1400, training loss: 6.275933265686035 = 0.011872542090713978 + 1.0 * 6.2640604972839355
Epoch 1400, val loss: 1.1250594854354858
Epoch 1410, training loss: 6.276245594024658 = 0.01163929421454668 + 1.0 * 6.264606475830078
Epoch 1410, val loss: 1.1287052631378174
Epoch 1420, training loss: 6.273977279663086 = 0.011412439867854118 + 1.0 * 6.262564659118652
Epoch 1420, val loss: 1.1323490142822266
Epoch 1430, training loss: 6.2744927406311035 = 0.011192040517926216 + 1.0 * 6.263300895690918
Epoch 1430, val loss: 1.1359786987304688
Epoch 1440, training loss: 6.278400421142578 = 0.01097847055643797 + 1.0 * 6.267421722412109
Epoch 1440, val loss: 1.1396355628967285
Epoch 1450, training loss: 6.273609161376953 = 0.010772567242383957 + 1.0 * 6.262836456298828
Epoch 1450, val loss: 1.1430611610412598
Epoch 1460, training loss: 6.270254611968994 = 0.01057193148881197 + 1.0 * 6.259682655334473
Epoch 1460, val loss: 1.146498441696167
Epoch 1470, training loss: 6.271596908569336 = 0.010375872254371643 + 1.0 * 6.261220932006836
Epoch 1470, val loss: 1.1500383615493774
Epoch 1480, training loss: 6.275552272796631 = 0.010185864754021168 + 1.0 * 6.265366554260254
Epoch 1480, val loss: 1.1535433530807495
Epoch 1490, training loss: 6.276518821716309 = 0.010002391412854195 + 1.0 * 6.266516208648682
Epoch 1490, val loss: 1.1569137573242188
Epoch 1500, training loss: 6.2694268226623535 = 0.009824184700846672 + 1.0 * 6.2596025466918945
Epoch 1500, val loss: 1.1601728200912476
Epoch 1510, training loss: 6.271231651306152 = 0.009650652296841145 + 1.0 * 6.261580944061279
Epoch 1510, val loss: 1.163486123085022
Epoch 1520, training loss: 6.27026891708374 = 0.009481669403612614 + 1.0 * 6.260787010192871
Epoch 1520, val loss: 1.166862964630127
Epoch 1530, training loss: 6.266613960266113 = 0.009317832067608833 + 1.0 * 6.257296085357666
Epoch 1530, val loss: 1.170077919960022
Epoch 1540, training loss: 6.266753673553467 = 0.009157789871096611 + 1.0 * 6.257596015930176
Epoch 1540, val loss: 1.1733167171478271
Epoch 1550, training loss: 6.2761125564575195 = 0.009001641534268856 + 1.0 * 6.267110824584961
Epoch 1550, val loss: 1.1766393184661865
Epoch 1560, training loss: 6.266343116760254 = 0.008851093240082264 + 1.0 * 6.2574920654296875
Epoch 1560, val loss: 1.1797845363616943
Epoch 1570, training loss: 6.266510963439941 = 0.008704079315066338 + 1.0 * 6.257806777954102
Epoch 1570, val loss: 1.1828802824020386
Epoch 1580, training loss: 6.2699198722839355 = 0.008560427464544773 + 1.0 * 6.261359214782715
Epoch 1580, val loss: 1.1860169172286987
Epoch 1590, training loss: 6.2655839920043945 = 0.008421347476541996 + 1.0 * 6.257162570953369
Epoch 1590, val loss: 1.1890714168548584
Epoch 1600, training loss: 6.264632701873779 = 0.008284720592200756 + 1.0 * 6.256348133087158
Epoch 1600, val loss: 1.1921674013137817
Epoch 1610, training loss: 6.268286228179932 = 0.008151797577738762 + 1.0 * 6.260134220123291
Epoch 1610, val loss: 1.1952790021896362
Epoch 1620, training loss: 6.2660393714904785 = 0.008022592402994633 + 1.0 * 6.258016586303711
Epoch 1620, val loss: 1.1982771158218384
Epoch 1630, training loss: 6.265017986297607 = 0.007896311581134796 + 1.0 * 6.257121562957764
Epoch 1630, val loss: 1.2012732028961182
Epoch 1640, training loss: 6.267077445983887 = 0.007773193065077066 + 1.0 * 6.259304046630859
Epoch 1640, val loss: 1.204232096672058
Epoch 1650, training loss: 6.2710347175598145 = 0.007654167246073484 + 1.0 * 6.263380527496338
Epoch 1650, val loss: 1.2071377038955688
Epoch 1660, training loss: 6.26210880279541 = 0.007537878584116697 + 1.0 * 6.254570960998535
Epoch 1660, val loss: 1.2099714279174805
Epoch 1670, training loss: 6.261697769165039 = 0.007424486801028252 + 1.0 * 6.254273414611816
Epoch 1670, val loss: 1.2127859592437744
Epoch 1680, training loss: 6.262388706207275 = 0.007312877103686333 + 1.0 * 6.255075931549072
Epoch 1680, val loss: 1.2156851291656494
Epoch 1690, training loss: 6.265506744384766 = 0.00720434682443738 + 1.0 * 6.258302211761475
Epoch 1690, val loss: 1.2185862064361572
Epoch 1700, training loss: 6.261081218719482 = 0.007098653819411993 + 1.0 * 6.2539825439453125
Epoch 1700, val loss: 1.2213385105133057
Epoch 1710, training loss: 6.259800910949707 = 0.006995284464210272 + 1.0 * 6.252805709838867
Epoch 1710, val loss: 1.2240289449691772
Epoch 1720, training loss: 6.264235019683838 = 0.00689359987154603 + 1.0 * 6.257341384887695
Epoch 1720, val loss: 1.226861596107483
Epoch 1730, training loss: 6.261325359344482 = 0.00679447315633297 + 1.0 * 6.254530906677246
Epoch 1730, val loss: 1.2296327352523804
Epoch 1740, training loss: 6.25790548324585 = 0.006698234472423792 + 1.0 * 6.25120735168457
Epoch 1740, val loss: 1.2323038578033447
Epoch 1750, training loss: 6.265557765960693 = 0.006604118272662163 + 1.0 * 6.25895357131958
Epoch 1750, val loss: 1.2349563837051392
Epoch 1760, training loss: 6.260438919067383 = 0.006512961816042662 + 1.0 * 6.253925800323486
Epoch 1760, val loss: 1.2375167608261108
Epoch 1770, training loss: 6.257193088531494 = 0.006423345301300287 + 1.0 * 6.25076961517334
Epoch 1770, val loss: 1.2400697469711304
Epoch 1780, training loss: 6.258248805999756 = 0.006335353944450617 + 1.0 * 6.251913547515869
Epoch 1780, val loss: 1.242705225944519
Epoch 1790, training loss: 6.26061487197876 = 0.0062492527067661285 + 1.0 * 6.25436544418335
Epoch 1790, val loss: 1.2453749179840088
Epoch 1800, training loss: 6.260605812072754 = 0.006164754740893841 + 1.0 * 6.254441261291504
Epoch 1800, val loss: 1.2479476928710938
Epoch 1810, training loss: 6.255902290344238 = 0.006082681007683277 + 1.0 * 6.249819755554199
Epoch 1810, val loss: 1.250417709350586
Epoch 1820, training loss: 6.256185054779053 = 0.006001855246722698 + 1.0 * 6.25018310546875
Epoch 1820, val loss: 1.2529164552688599
Epoch 1830, training loss: 6.2593889236450195 = 0.005922789219766855 + 1.0 * 6.2534661293029785
Epoch 1830, val loss: 1.2554844617843628
Epoch 1840, training loss: 6.256195068359375 = 0.0058457497507333755 + 1.0 * 6.250349521636963
Epoch 1840, val loss: 1.2579679489135742
Epoch 1850, training loss: 6.256499767303467 = 0.0057701230980455875 + 1.0 * 6.250729560852051
Epoch 1850, val loss: 1.2603726387023926
Epoch 1860, training loss: 6.263094425201416 = 0.00569600984454155 + 1.0 * 6.25739860534668
Epoch 1860, val loss: 1.2628320455551147
Epoch 1870, training loss: 6.255145072937012 = 0.005624193232506514 + 1.0 * 6.249520778656006
Epoch 1870, val loss: 1.2652783393859863
Epoch 1880, training loss: 6.252836227416992 = 0.005553974770009518 + 1.0 * 6.247282028198242
Epoch 1880, val loss: 1.267522931098938
Epoch 1890, training loss: 6.252621173858643 = 0.005484520923346281 + 1.0 * 6.24713659286499
Epoch 1890, val loss: 1.2698601484298706
Epoch 1900, training loss: 6.255471229553223 = 0.0054163094609975815 + 1.0 * 6.250054836273193
Epoch 1900, val loss: 1.2722868919372559
Epoch 1910, training loss: 6.257894992828369 = 0.005349433049559593 + 1.0 * 6.252545356750488
Epoch 1910, val loss: 1.274720311164856
Epoch 1920, training loss: 6.254001617431641 = 0.0052843112498521805 + 1.0 * 6.248717308044434
Epoch 1920, val loss: 1.2770272493362427
Epoch 1930, training loss: 6.25377082824707 = 0.0052206977270543575 + 1.0 * 6.248549938201904
Epoch 1930, val loss: 1.2792177200317383
Epoch 1940, training loss: 6.25067663192749 = 0.005158021580427885 + 1.0 * 6.245518684387207
Epoch 1940, val loss: 1.2814940214157104
Epoch 1950, training loss: 6.253079891204834 = 0.005096270237118006 + 1.0 * 6.247983455657959
Epoch 1950, val loss: 1.2838102579116821
Epoch 1960, training loss: 6.252186298370361 = 0.005035907495766878 + 1.0 * 6.247150421142578
Epoch 1960, val loss: 1.286146640777588
Epoch 1970, training loss: 6.251808166503906 = 0.004977133125066757 + 1.0 * 6.246830940246582
Epoch 1970, val loss: 1.2883390188217163
Epoch 1980, training loss: 6.252532005310059 = 0.004919331520795822 + 1.0 * 6.247612476348877
Epoch 1980, val loss: 1.2905207872390747
Epoch 1990, training loss: 6.25127649307251 = 0.00486267264932394 + 1.0 * 6.246413707733154
Epoch 1990, val loss: 1.2926665544509888
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 10.542168617248535 = 1.9453303813934326 + 1.0 * 8.596837997436523
Epoch 0, val loss: 1.946638822555542
Epoch 10, training loss: 10.532033920288086 = 1.9354392290115356 + 1.0 * 8.59659481048584
Epoch 10, val loss: 1.9362472295761108
Epoch 20, training loss: 10.517648696899414 = 1.9228624105453491 + 1.0 * 8.594786643981934
Epoch 20, val loss: 1.9228465557098389
Epoch 30, training loss: 10.484306335449219 = 1.9051278829574585 + 1.0 * 8.579178810119629
Epoch 30, val loss: 1.9037256240844727
Epoch 40, training loss: 10.366561889648438 = 1.881073236465454 + 1.0 * 8.485488891601562
Epoch 40, val loss: 1.878523826599121
Epoch 50, training loss: 9.889267921447754 = 1.8543583154678345 + 1.0 * 8.03490924835205
Epoch 50, val loss: 1.8519965410232544
Epoch 60, training loss: 9.56665325164795 = 1.8289892673492432 + 1.0 * 7.737663745880127
Epoch 60, val loss: 1.828181266784668
Epoch 70, training loss: 9.053550720214844 = 1.809166431427002 + 1.0 * 7.244384288787842
Epoch 70, val loss: 1.8098212480545044
Epoch 80, training loss: 8.715696334838867 = 1.7933868169784546 + 1.0 * 6.922309875488281
Epoch 80, val loss: 1.7953994274139404
Epoch 90, training loss: 8.595333099365234 = 1.7786496877670288 + 1.0 * 6.816683292388916
Epoch 90, val loss: 1.7811702489852905
Epoch 100, training loss: 8.49703598022461 = 1.7631311416625977 + 1.0 * 6.73390531539917
Epoch 100, val loss: 1.7664979696273804
Epoch 110, training loss: 8.410537719726562 = 1.7476750612258911 + 1.0 * 6.662862300872803
Epoch 110, val loss: 1.7523013353347778
Epoch 120, training loss: 8.346529006958008 = 1.7311660051345825 + 1.0 * 6.615362644195557
Epoch 120, val loss: 1.7377041578292847
Epoch 130, training loss: 8.29493522644043 = 1.7124569416046143 + 1.0 * 6.5824785232543945
Epoch 130, val loss: 1.721769094467163
Epoch 140, training loss: 8.245039939880371 = 1.6909500360488892 + 1.0 * 6.5540900230407715
Epoch 140, val loss: 1.7035900354385376
Epoch 150, training loss: 8.196459770202637 = 1.6658657789230347 + 1.0 * 6.530594348907471
Epoch 150, val loss: 1.6822322607040405
Epoch 160, training loss: 8.147438049316406 = 1.6361944675445557 + 1.0 * 6.5112433433532715
Epoch 160, val loss: 1.65679132938385
Epoch 170, training loss: 8.098658561706543 = 1.6009999513626099 + 1.0 * 6.497658729553223
Epoch 170, val loss: 1.6266666650772095
Epoch 180, training loss: 8.04336929321289 = 1.560456395149231 + 1.0 * 6.482913017272949
Epoch 180, val loss: 1.5922781229019165
Epoch 190, training loss: 7.9836320877075195 = 1.5141007900238037 + 1.0 * 6.469531536102295
Epoch 190, val loss: 1.5531384944915771
Epoch 200, training loss: 7.920519828796387 = 1.4617316722869873 + 1.0 * 6.45878791809082
Epoch 200, val loss: 1.509217381477356
Epoch 210, training loss: 7.856670379638672 = 1.403848648071289 + 1.0 * 6.452821731567383
Epoch 210, val loss: 1.4612160921096802
Epoch 220, training loss: 7.784981727600098 = 1.3425322771072388 + 1.0 * 6.442449569702148
Epoch 220, val loss: 1.4108692407608032
Epoch 230, training loss: 7.715388774871826 = 1.2787917852401733 + 1.0 * 6.436596870422363
Epoch 230, val loss: 1.3590816259384155
Epoch 240, training loss: 7.644121170043945 = 1.2144569158554077 + 1.0 * 6.429664134979248
Epoch 240, val loss: 1.307114839553833
Epoch 250, training loss: 7.574251174926758 = 1.150267481803894 + 1.0 * 6.423983573913574
Epoch 250, val loss: 1.2555807828903198
Epoch 260, training loss: 7.5077972412109375 = 1.087555170059204 + 1.0 * 6.420241832733154
Epoch 260, val loss: 1.2054654359817505
Epoch 270, training loss: 7.441985130310059 = 1.0278730392456055 + 1.0 * 6.414112091064453
Epoch 270, val loss: 1.1578452587127686
Epoch 280, training loss: 7.381006240844727 = 0.9712674617767334 + 1.0 * 6.409739017486572
Epoch 280, val loss: 1.1129331588745117
Epoch 290, training loss: 7.322038173675537 = 0.9180382490158081 + 1.0 * 6.4039998054504395
Epoch 290, val loss: 1.070905327796936
Epoch 300, training loss: 7.266256332397461 = 0.8677641749382019 + 1.0 * 6.398492336273193
Epoch 300, val loss: 1.0313774347305298
Epoch 310, training loss: 7.219087600708008 = 0.820199728012085 + 1.0 * 6.398887634277344
Epoch 310, val loss: 0.9944044351577759
Epoch 320, training loss: 7.167608737945557 = 0.7763894200325012 + 1.0 * 6.391219139099121
Epoch 320, val loss: 0.9606102705001831
Epoch 330, training loss: 7.120685577392578 = 0.735163688659668 + 1.0 * 6.38552188873291
Epoch 330, val loss: 0.9291678667068481
Epoch 340, training loss: 7.076704025268555 = 0.6959897875785828 + 1.0 * 6.380714416503906
Epoch 340, val loss: 0.899764895439148
Epoch 350, training loss: 7.041457653045654 = 0.6589569449424744 + 1.0 * 6.382500648498535
Epoch 350, val loss: 0.8725196123123169
Epoch 360, training loss: 7.000840187072754 = 0.6245341300964355 + 1.0 * 6.376306056976318
Epoch 360, val loss: 0.8476901054382324
Epoch 370, training loss: 6.963404178619385 = 0.591817319393158 + 1.0 * 6.371586799621582
Epoch 370, val loss: 0.8245044946670532
Epoch 380, training loss: 6.928618907928467 = 0.560187041759491 + 1.0 * 6.36843204498291
Epoch 380, val loss: 0.8027940392494202
Epoch 390, training loss: 6.901115894317627 = 0.5297711491584778 + 1.0 * 6.371344566345215
Epoch 390, val loss: 0.7824633717536926
Epoch 400, training loss: 6.865749835968018 = 0.5006278157234192 + 1.0 * 6.365121841430664
Epoch 400, val loss: 0.7636566162109375
Epoch 410, training loss: 6.831827640533447 = 0.4723914563655853 + 1.0 * 6.35943603515625
Epoch 410, val loss: 0.745928943157196
Epoch 420, training loss: 6.800426006317139 = 0.4447956383228302 + 1.0 * 6.355630397796631
Epoch 420, val loss: 0.7292637825012207
Epoch 430, training loss: 6.7877373695373535 = 0.41784197092056274 + 1.0 * 6.3698954582214355
Epoch 430, val loss: 0.7136726975440979
Epoch 440, training loss: 6.744997978210449 = 0.39218345284461975 + 1.0 * 6.352814674377441
Epoch 440, val loss: 0.6992698907852173
Epoch 450, training loss: 6.716498851776123 = 0.36747056245803833 + 1.0 * 6.34902811050415
Epoch 450, val loss: 0.6859060525894165
Epoch 460, training loss: 6.6911139488220215 = 0.34355756640434265 + 1.0 * 6.3475565910339355
Epoch 460, val loss: 0.6735799908638
Epoch 470, training loss: 6.669081211090088 = 0.3207743763923645 + 1.0 * 6.348306655883789
Epoch 470, val loss: 0.6622885465621948
Epoch 480, training loss: 6.6399641036987305 = 0.29913830757141113 + 1.0 * 6.340826034545898
Epoch 480, val loss: 0.6519154906272888
Epoch 490, training loss: 6.616344451904297 = 0.2785484492778778 + 1.0 * 6.337796211242676
Epoch 490, val loss: 0.6424333453178406
Epoch 500, training loss: 6.603940963745117 = 0.2591756582260132 + 1.0 * 6.3447651863098145
Epoch 500, val loss: 0.6339632868766785
Epoch 510, training loss: 6.579819202423096 = 0.2412247359752655 + 1.0 * 6.338594436645508
Epoch 510, val loss: 0.6265028715133667
Epoch 520, training loss: 6.555299282073975 = 0.224514901638031 + 1.0 * 6.330784320831299
Epoch 520, val loss: 0.6198842525482178
Epoch 530, training loss: 6.5380425453186035 = 0.208892822265625 + 1.0 * 6.3291497230529785
Epoch 530, val loss: 0.6142666935920715
Epoch 540, training loss: 6.526819229125977 = 0.19442154467105865 + 1.0 * 6.3323974609375
Epoch 540, val loss: 0.6095780730247498
Epoch 550, training loss: 6.506563663482666 = 0.18118524551391602 + 1.0 * 6.32537841796875
Epoch 550, val loss: 0.6057531833648682
Epoch 560, training loss: 6.492112159729004 = 0.16901935636997223 + 1.0 * 6.323092937469482
Epoch 560, val loss: 0.6027187705039978
Epoch 570, training loss: 6.494875907897949 = 0.15781660377979279 + 1.0 * 6.337059497833252
Epoch 570, val loss: 0.6004799604415894
Epoch 580, training loss: 6.469635009765625 = 0.14762257039546967 + 1.0 * 6.322012424468994
Epoch 580, val loss: 0.5989745259284973
Epoch 590, training loss: 6.45576810836792 = 0.1382848620414734 + 1.0 * 6.317483425140381
Epoch 590, val loss: 0.5980510711669922
Epoch 600, training loss: 6.444558143615723 = 0.12967142462730408 + 1.0 * 6.314886569976807
Epoch 600, val loss: 0.5977242588996887
Epoch 610, training loss: 6.442184925079346 = 0.1217484250664711 + 1.0 * 6.320436477661133
Epoch 610, val loss: 0.5979442596435547
Epoch 620, training loss: 6.427277565002441 = 0.11455976963043213 + 1.0 * 6.312717914581299
Epoch 620, val loss: 0.5985621809959412
Epoch 630, training loss: 6.418399333953857 = 0.10793513059616089 + 1.0 * 6.310464382171631
Epoch 630, val loss: 0.5995985269546509
Epoch 640, training loss: 6.410356521606445 = 0.1017903983592987 + 1.0 * 6.308566093444824
Epoch 640, val loss: 0.6010051369667053
Epoch 650, training loss: 6.412965297698975 = 0.09608542174100876 + 1.0 * 6.316879749298096
Epoch 650, val loss: 0.6027207374572754
Epoch 660, training loss: 6.40249490737915 = 0.09081950038671494 + 1.0 * 6.311675548553467
Epoch 660, val loss: 0.6047263145446777
Epoch 670, training loss: 6.3959574699401855 = 0.08595909178256989 + 1.0 * 6.309998512268066
Epoch 670, val loss: 0.6069634556770325
Epoch 680, training loss: 6.388030052185059 = 0.08146211504936218 + 1.0 * 6.306568145751953
Epoch 680, val loss: 0.6094281673431396
Epoch 690, training loss: 6.380194664001465 = 0.07728729397058487 + 1.0 * 6.302907466888428
Epoch 690, val loss: 0.612074077129364
Epoch 700, training loss: 6.374902725219727 = 0.073387011885643 + 1.0 * 6.301515579223633
Epoch 700, val loss: 0.6149229407310486
Epoch 710, training loss: 6.381829738616943 = 0.0697513148188591 + 1.0 * 6.312078475952148
Epoch 710, val loss: 0.6178873181343079
Epoch 720, training loss: 6.365026473999023 = 0.06637554615736008 + 1.0 * 6.298650741577148
Epoch 720, val loss: 0.6209810376167297
Epoch 730, training loss: 6.361276626586914 = 0.06322208046913147 + 1.0 * 6.2980546951293945
Epoch 730, val loss: 0.6242068409919739
Epoch 740, training loss: 6.357509613037109 = 0.06025531515479088 + 1.0 * 6.2972540855407715
Epoch 740, val loss: 0.627540111541748
Epoch 750, training loss: 6.358628273010254 = 0.0574781596660614 + 1.0 * 6.301150321960449
Epoch 750, val loss: 0.630929172039032
Epoch 760, training loss: 6.351423740386963 = 0.05490730330348015 + 1.0 * 6.296516418457031
Epoch 760, val loss: 0.6343463659286499
Epoch 770, training loss: 6.346501350402832 = 0.052489813417196274 + 1.0 * 6.29401159286499
Epoch 770, val loss: 0.6378426551818848
Epoch 780, training loss: 6.343108654022217 = 0.050205815583467484 + 1.0 * 6.292902946472168
Epoch 780, val loss: 0.6414204835891724
Epoch 790, training loss: 6.351022720336914 = 0.04805344343185425 + 1.0 * 6.302969455718994
Epoch 790, val loss: 0.6450551152229309
Epoch 800, training loss: 6.3405537605285645 = 0.046030085533857346 + 1.0 * 6.2945237159729
Epoch 800, val loss: 0.6486942172050476
Epoch 810, training loss: 6.334993362426758 = 0.04413098841905594 + 1.0 * 6.290862560272217
Epoch 810, val loss: 0.6523845791816711
Epoch 820, training loss: 6.342063903808594 = 0.042332012206315994 + 1.0 * 6.299731731414795
Epoch 820, val loss: 0.656096339225769
Epoch 830, training loss: 6.331068992614746 = 0.040643539279699326 + 1.0 * 6.2904253005981445
Epoch 830, val loss: 0.6598328351974487
Epoch 840, training loss: 6.327558994293213 = 0.039050616323947906 + 1.0 * 6.288508415222168
Epoch 840, val loss: 0.6635764837265015
Epoch 850, training loss: 6.327834606170654 = 0.037543486803770065 + 1.0 * 6.2902913093566895
Epoch 850, val loss: 0.6672808527946472
Epoch 860, training loss: 6.3222761154174805 = 0.036124490201473236 + 1.0 * 6.28615140914917
Epoch 860, val loss: 0.6710184812545776
Epoch 870, training loss: 6.321871757507324 = 0.03477725386619568 + 1.0 * 6.287094593048096
Epoch 870, val loss: 0.6747477650642395
Epoch 880, training loss: 6.32183313369751 = 0.033501997590065 + 1.0 * 6.288331031799316
Epoch 880, val loss: 0.6784363985061646
Epoch 890, training loss: 6.3162689208984375 = 0.032295916229486465 + 1.0 * 6.283973217010498
Epoch 890, val loss: 0.6821457147598267
Epoch 900, training loss: 6.314332008361816 = 0.03115062788128853 + 1.0 * 6.283181190490723
Epoch 900, val loss: 0.6858438849449158
Epoch 910, training loss: 6.3223557472229 = 0.03006080538034439 + 1.0 * 6.292294979095459
Epoch 910, val loss: 0.6895251870155334
Epoch 920, training loss: 6.312407493591309 = 0.029028136283159256 + 1.0 * 6.283379554748535
Epoch 920, val loss: 0.693173348903656
Epoch 930, training loss: 6.308847904205322 = 0.02804630808532238 + 1.0 * 6.280801773071289
Epoch 930, val loss: 0.6968263983726501
Epoch 940, training loss: 6.308929920196533 = 0.027108458802103996 + 1.0 * 6.281821250915527
Epoch 940, val loss: 0.7004808187484741
Epoch 950, training loss: 6.3108649253845215 = 0.026218079030513763 + 1.0 * 6.284646987915039
Epoch 950, val loss: 0.7040275931358337
Epoch 960, training loss: 6.306548118591309 = 0.025379419326782227 + 1.0 * 6.2811689376831055
Epoch 960, val loss: 0.7075462341308594
Epoch 970, training loss: 6.303284645080566 = 0.024579564109444618 + 1.0 * 6.27870512008667
Epoch 970, val loss: 0.7111015319824219
Epoch 980, training loss: 6.300939083099365 = 0.02381175570189953 + 1.0 * 6.277127265930176
Epoch 980, val loss: 0.7146314978599548
Epoch 990, training loss: 6.31071662902832 = 0.023076603189110756 + 1.0 * 6.28764009475708
Epoch 990, val loss: 0.7181092500686646
Epoch 1000, training loss: 6.301433086395264 = 0.02238020859658718 + 1.0 * 6.279052734375
Epoch 1000, val loss: 0.7215142250061035
Epoch 1010, training loss: 6.297458171844482 = 0.021715771406888962 + 1.0 * 6.275742530822754
Epoch 1010, val loss: 0.7249614596366882
Epoch 1020, training loss: 6.305169105529785 = 0.021079236641526222 + 1.0 * 6.284090042114258
Epoch 1020, val loss: 0.7283533811569214
Epoch 1030, training loss: 6.29693078994751 = 0.02047053910791874 + 1.0 * 6.27646017074585
Epoch 1030, val loss: 0.7317374348640442
Epoch 1040, training loss: 6.29399299621582 = 0.019887808710336685 + 1.0 * 6.274105072021484
Epoch 1040, val loss: 0.7351057529449463
Epoch 1050, training loss: 6.302238464355469 = 0.019327949732542038 + 1.0 * 6.282910346984863
Epoch 1050, val loss: 0.7384575605392456
Epoch 1060, training loss: 6.301051616668701 = 0.01879478245973587 + 1.0 * 6.282256603240967
Epoch 1060, val loss: 0.7416254878044128
Epoch 1070, training loss: 6.289919853210449 = 0.018289145082235336 + 1.0 * 6.271630764007568
Epoch 1070, val loss: 0.7448503971099854
Epoch 1080, training loss: 6.290057182312012 = 0.01780141331255436 + 1.0 * 6.272255897521973
Epoch 1080, val loss: 0.7480980753898621
Epoch 1090, training loss: 6.290798187255859 = 0.01732928305864334 + 1.0 * 6.273468971252441
Epoch 1090, val loss: 0.7512911558151245
Epoch 1100, training loss: 6.288017272949219 = 0.01687639392912388 + 1.0 * 6.271141052246094
Epoch 1100, val loss: 0.7544410228729248
Epoch 1110, training loss: 6.288468837738037 = 0.016442405059933662 + 1.0 * 6.272026538848877
Epoch 1110, val loss: 0.7575832605361938
Epoch 1120, training loss: 6.288302421569824 = 0.01602316088974476 + 1.0 * 6.272279262542725
Epoch 1120, val loss: 0.7606720924377441
Epoch 1130, training loss: 6.284115791320801 = 0.015621659345924854 + 1.0 * 6.268494129180908
Epoch 1130, val loss: 0.7637333273887634
Epoch 1140, training loss: 6.28594970703125 = 0.015235953964293003 + 1.0 * 6.270713806152344
Epoch 1140, val loss: 0.766798198223114
Epoch 1150, training loss: 6.2908830642700195 = 0.014864453114569187 + 1.0 * 6.2760186195373535
Epoch 1150, val loss: 0.7697620987892151
Epoch 1160, training loss: 6.282803058624268 = 0.014508350752294064 + 1.0 * 6.268294811248779
Epoch 1160, val loss: 0.7727138996124268
Epoch 1170, training loss: 6.287031650543213 = 0.014165496453642845 + 1.0 * 6.272866249084473
Epoch 1170, val loss: 0.7756757736206055
Epoch 1180, training loss: 6.280553817749023 = 0.013835164718329906 + 1.0 * 6.266718864440918
Epoch 1180, val loss: 0.7784967422485352
Epoch 1190, training loss: 6.280033111572266 = 0.013518226332962513 + 1.0 * 6.266514778137207
Epoch 1190, val loss: 0.7813943028450012
Epoch 1200, training loss: 6.277979373931885 = 0.013209681957960129 + 1.0 * 6.264769554138184
Epoch 1200, val loss: 0.7842432260513306
Epoch 1210, training loss: 6.2820892333984375 = 0.012909860350191593 + 1.0 * 6.269179344177246
Epoch 1210, val loss: 0.7870816588401794
Epoch 1220, training loss: 6.279396057128906 = 0.012623881921172142 + 1.0 * 6.266772270202637
Epoch 1220, val loss: 0.7898063659667969
Epoch 1230, training loss: 6.2783684730529785 = 0.012349809519946575 + 1.0 * 6.266018867492676
Epoch 1230, val loss: 0.7925071716308594
Epoch 1240, training loss: 6.274662494659424 = 0.01208362728357315 + 1.0 * 6.262578964233398
Epoch 1240, val loss: 0.7952675819396973
Epoch 1250, training loss: 6.278811454772949 = 0.011824368499219418 + 1.0 * 6.266987323760986
Epoch 1250, val loss: 0.7980300784111023
Epoch 1260, training loss: 6.274818420410156 = 0.01157368440181017 + 1.0 * 6.26324462890625
Epoch 1260, val loss: 0.8006110191345215
Epoch 1270, training loss: 6.273070812225342 = 0.011333669535815716 + 1.0 * 6.26173734664917
Epoch 1270, val loss: 0.8032715916633606
Epoch 1280, training loss: 6.272291660308838 = 0.011099194176495075 + 1.0 * 6.261192321777344
Epoch 1280, val loss: 0.8059009909629822
Epoch 1290, training loss: 6.282872200012207 = 0.010871311649680138 + 1.0 * 6.272000789642334
Epoch 1290, val loss: 0.808501660823822
Epoch 1300, training loss: 6.279909610748291 = 0.010653838515281677 + 1.0 * 6.269255638122559
Epoch 1300, val loss: 0.8110049962997437
Epoch 1310, training loss: 6.270726680755615 = 0.010443245060741901 + 1.0 * 6.260283470153809
Epoch 1310, val loss: 0.8135054111480713
Epoch 1320, training loss: 6.270176887512207 = 0.01023909356445074 + 1.0 * 6.259937763214111
Epoch 1320, val loss: 0.8160462379455566
Epoch 1330, training loss: 6.2711687088012695 = 0.010038942098617554 + 1.0 * 6.261129856109619
Epoch 1330, val loss: 0.8185596466064453
Epoch 1340, training loss: 6.270382404327393 = 0.009845123626291752 + 1.0 * 6.260537147521973
Epoch 1340, val loss: 0.8210499286651611
Epoch 1350, training loss: 6.274766445159912 = 0.009657428599894047 + 1.0 * 6.265109062194824
Epoch 1350, val loss: 0.8234806656837463
Epoch 1360, training loss: 6.267822265625 = 0.009476001374423504 + 1.0 * 6.258346080780029
Epoch 1360, val loss: 0.8258728384971619
Epoch 1370, training loss: 6.266772270202637 = 0.0093003511428833 + 1.0 * 6.257472038269043
Epoch 1370, val loss: 0.8282841444015503
Epoch 1380, training loss: 6.285595417022705 = 0.009129047393798828 + 1.0 * 6.276466369628906
Epoch 1380, val loss: 0.8306742310523987
Epoch 1390, training loss: 6.2687530517578125 = 0.008964781649410725 + 1.0 * 6.2597880363464355
Epoch 1390, val loss: 0.832944929599762
Epoch 1400, training loss: 6.26528787612915 = 0.008805299177765846 + 1.0 * 6.2564826011657715
Epoch 1400, val loss: 0.8352634310722351
Epoch 1410, training loss: 6.264031887054443 = 0.008648974820971489 + 1.0 * 6.255383014678955
Epoch 1410, val loss: 0.8376104235649109
Epoch 1420, training loss: 6.268624305725098 = 0.008495990186929703 + 1.0 * 6.260128498077393
Epoch 1420, val loss: 0.8399527072906494
Epoch 1430, training loss: 6.262824058532715 = 0.008347518742084503 + 1.0 * 6.254476547241211
Epoch 1430, val loss: 0.8421199321746826
Epoch 1440, training loss: 6.262763500213623 = 0.008205304853618145 + 1.0 * 6.254558086395264
Epoch 1440, val loss: 0.8443531394004822
Epoch 1450, training loss: 6.263038635253906 = 0.008065465837717056 + 1.0 * 6.2549729347229
Epoch 1450, val loss: 0.8465766310691833
Epoch 1460, training loss: 6.268950462341309 = 0.00792933814227581 + 1.0 * 6.261021137237549
Epoch 1460, val loss: 0.8488093614578247
Epoch 1470, training loss: 6.263146877288818 = 0.007796997204422951 + 1.0 * 6.255350112915039
Epoch 1470, val loss: 0.8509674072265625
Epoch 1480, training loss: 6.2678937911987305 = 0.007668104022741318 + 1.0 * 6.260225772857666
Epoch 1480, val loss: 0.8531241416931152
Epoch 1490, training loss: 6.261059761047363 = 0.007543137297034264 + 1.0 * 6.253516674041748
Epoch 1490, val loss: 0.8552036881446838
Epoch 1500, training loss: 6.260377883911133 = 0.007420792710036039 + 1.0 * 6.252956867218018
Epoch 1500, val loss: 0.8573428392410278
Epoch 1510, training loss: 6.264358043670654 = 0.007301243953406811 + 1.0 * 6.257056713104248
Epoch 1510, val loss: 0.8594561815261841
Epoch 1520, training loss: 6.259790420532227 = 0.007185562513768673 + 1.0 * 6.252604961395264
Epoch 1520, val loss: 0.8615216612815857
Epoch 1530, training loss: 6.257521629333496 = 0.00707269087433815 + 1.0 * 6.250448703765869
Epoch 1530, val loss: 0.8635644912719727
Epoch 1540, training loss: 6.260002613067627 = 0.006961904466152191 + 1.0 * 6.253040790557861
Epoch 1540, val loss: 0.8656476736068726
Epoch 1550, training loss: 6.263422012329102 = 0.006854444742202759 + 1.0 * 6.256567478179932
Epoch 1550, val loss: 0.8676389455795288
Epoch 1560, training loss: 6.2574663162231445 = 0.006750298198312521 + 1.0 * 6.250716209411621
Epoch 1560, val loss: 0.8696033954620361
Epoch 1570, training loss: 6.2557759284973145 = 0.006648516748100519 + 1.0 * 6.249127388000488
Epoch 1570, val loss: 0.8715904355049133
Epoch 1580, training loss: 6.259576797485352 = 0.006548203527927399 + 1.0 * 6.253028392791748
Epoch 1580, val loss: 0.8736066818237305
Epoch 1590, training loss: 6.256800651550293 = 0.006450938992202282 + 1.0 * 6.250349521636963
Epoch 1590, val loss: 0.8755537271499634
Epoch 1600, training loss: 6.255142688751221 = 0.006356736645102501 + 1.0 * 6.248785972595215
Epoch 1600, val loss: 0.8774644136428833
Epoch 1610, training loss: 6.256126880645752 = 0.006264189258217812 + 1.0 * 6.2498626708984375
Epoch 1610, val loss: 0.8793901205062866
Epoch 1620, training loss: 6.260756492614746 = 0.006173811852931976 + 1.0 * 6.25458288192749
Epoch 1620, val loss: 0.881264865398407
Epoch 1630, training loss: 6.254787445068359 = 0.0060859858058393 + 1.0 * 6.248701572418213
Epoch 1630, val loss: 0.8831263184547424
Epoch 1640, training loss: 6.253402233123779 = 0.006000013090670109 + 1.0 * 6.247402191162109
Epoch 1640, val loss: 0.8850084543228149
Epoch 1650, training loss: 6.261288166046143 = 0.0059151193127036095 + 1.0 * 6.255373001098633
Epoch 1650, val loss: 0.8869156837463379
Epoch 1660, training loss: 6.253758907318115 = 0.005832867231220007 + 1.0 * 6.247926235198975
Epoch 1660, val loss: 0.8887428045272827
Epoch 1670, training loss: 6.254254341125488 = 0.005752456374466419 + 1.0 * 6.248501777648926
Epoch 1670, val loss: 0.8905462026596069
Epoch 1680, training loss: 6.254171848297119 = 0.005673594307154417 + 1.0 * 6.248498439788818
Epoch 1680, val loss: 0.8923587799072266
Epoch 1690, training loss: 6.257610321044922 = 0.005596513859927654 + 1.0 * 6.252013683319092
Epoch 1690, val loss: 0.8941615223884583
Epoch 1700, training loss: 6.2517499923706055 = 0.005521486047655344 + 1.0 * 6.2462286949157715
Epoch 1700, val loss: 0.8959011435508728
Epoch 1710, training loss: 6.251479148864746 = 0.00544888386502862 + 1.0 * 6.246030330657959
Epoch 1710, val loss: 0.8976497650146484
Epoch 1720, training loss: 6.250138282775879 = 0.005376637447625399 + 1.0 * 6.2447614669799805
Epoch 1720, val loss: 0.8994393944740295
Epoch 1730, training loss: 6.249349117279053 = 0.005305349826812744 + 1.0 * 6.244043827056885
Epoch 1730, val loss: 0.9012436866760254
Epoch 1740, training loss: 6.257214069366455 = 0.0052351332269608974 + 1.0 * 6.251978874206543
Epoch 1740, val loss: 0.9030160307884216
Epoch 1750, training loss: 6.254475116729736 = 0.005167560186237097 + 1.0 * 6.249307632446289
Epoch 1750, val loss: 0.904649555683136
Epoch 1760, training loss: 6.252713203430176 = 0.005102190654724836 + 1.0 * 6.247611045837402
Epoch 1760, val loss: 0.9063274264335632
Epoch 1770, training loss: 6.250502586364746 = 0.00503781670704484 + 1.0 * 6.24546480178833
Epoch 1770, val loss: 0.9080314636230469
Epoch 1780, training loss: 6.252340316772461 = 0.004974369425326586 + 1.0 * 6.247365951538086
Epoch 1780, val loss: 0.9097514748573303
Epoch 1790, training loss: 6.251882076263428 = 0.004912673495709896 + 1.0 * 6.246969223022461
Epoch 1790, val loss: 0.911416232585907
Epoch 1800, training loss: 6.248475551605225 = 0.0048525077290833 + 1.0 * 6.24362325668335
Epoch 1800, val loss: 0.9130297899246216
Epoch 1810, training loss: 6.247159481048584 = 0.00479262787848711 + 1.0 * 6.242366790771484
Epoch 1810, val loss: 0.9147067070007324
Epoch 1820, training loss: 6.247040748596191 = 0.004733602050691843 + 1.0 * 6.242307186126709
Epoch 1820, val loss: 0.9164080023765564
Epoch 1830, training loss: 6.252536296844482 = 0.004675676114857197 + 1.0 * 6.247860431671143
Epoch 1830, val loss: 0.9180760979652405
Epoch 1840, training loss: 6.251519203186035 = 0.004619311075657606 + 1.0 * 6.2469000816345215
Epoch 1840, val loss: 0.9196549654006958
Epoch 1850, training loss: 6.248621463775635 = 0.0045649330131709576 + 1.0 * 6.244056701660156
Epoch 1850, val loss: 0.9212242364883423
Epoch 1860, training loss: 6.24935245513916 = 0.004511075560003519 + 1.0 * 6.244841575622559
Epoch 1860, val loss: 0.9228333830833435
Epoch 1870, training loss: 6.2485527992248535 = 0.004458296112716198 + 1.0 * 6.244094371795654
Epoch 1870, val loss: 0.9244101047515869
Epoch 1880, training loss: 6.250485897064209 = 0.00440671993419528 + 1.0 * 6.246078968048096
Epoch 1880, val loss: 0.925984263420105
Epoch 1890, training loss: 6.250572681427002 = 0.004356240853667259 + 1.0 * 6.246216297149658
Epoch 1890, val loss: 0.9275451302528381
Epoch 1900, training loss: 6.244457244873047 = 0.004306599963456392 + 1.0 * 6.240150451660156
Epoch 1900, val loss: 0.9290686249732971
Epoch 1910, training loss: 6.243780612945557 = 0.004257502034306526 + 1.0 * 6.239522933959961
Epoch 1910, val loss: 0.9306368231773376
Epoch 1920, training loss: 6.24971342086792 = 0.004209150560200214 + 1.0 * 6.245504379272461
Epoch 1920, val loss: 0.9322149157524109
Epoch 1930, training loss: 6.248154163360596 = 0.004162091761827469 + 1.0 * 6.243991851806641
Epoch 1930, val loss: 0.9336428046226501
Epoch 1940, training loss: 6.245368003845215 = 0.004116775933653116 + 1.0 * 6.241250991821289
Epoch 1940, val loss: 0.9350614547729492
Epoch 1950, training loss: 6.243646144866943 = 0.004071544855833054 + 1.0 * 6.239574432373047
Epoch 1950, val loss: 0.936561644077301
Epoch 1960, training loss: 6.244634628295898 = 0.004026809707283974 + 1.0 * 6.240607738494873
Epoch 1960, val loss: 0.9380965232849121
Epoch 1970, training loss: 6.247933387756348 = 0.003982663154602051 + 1.0 * 6.243950843811035
Epoch 1970, val loss: 0.9395828247070312
Epoch 1980, training loss: 6.245843887329102 = 0.003939913120120764 + 1.0 * 6.241903781890869
Epoch 1980, val loss: 0.9410391449928284
Epoch 1990, training loss: 6.244757175445557 = 0.0038978371303528547 + 1.0 * 6.240859508514404
Epoch 1990, val loss: 0.9424590468406677
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8355297838692674
The final CL Acc:0.82469, 0.00761, The final GNN Acc:0.83852, 0.00460
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9430])
updated graph: torch.Size([2, 10478])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.540566444396973 = 1.9437077045440674 + 1.0 * 8.596858978271484
Epoch 0, val loss: 1.9474706649780273
Epoch 10, training loss: 10.53123950958252 = 1.9345766305923462 + 1.0 * 8.596662521362305
Epoch 10, val loss: 1.9388768672943115
Epoch 20, training loss: 10.517993927001953 = 1.923232913017273 + 1.0 * 8.59476089477539
Epoch 20, val loss: 1.9277746677398682
Epoch 30, training loss: 10.483708381652832 = 1.9073373079299927 + 1.0 * 8.576371192932129
Epoch 30, val loss: 1.9119012355804443
Epoch 40, training loss: 10.337823867797852 = 1.8862272500991821 + 1.0 * 8.4515962600708
Epoch 40, val loss: 1.8915594816207886
Epoch 50, training loss: 9.8833646774292 = 1.8632752895355225 + 1.0 * 8.020089149475098
Epoch 50, val loss: 1.870676040649414
Epoch 60, training loss: 9.5842924118042 = 1.843965768814087 + 1.0 * 7.740326404571533
Epoch 60, val loss: 1.8540722131729126
Epoch 70, training loss: 9.07628345489502 = 1.8289138078689575 + 1.0 * 7.247369289398193
Epoch 70, val loss: 1.8398120403289795
Epoch 80, training loss: 8.780970573425293 = 1.8164317607879639 + 1.0 * 6.96453857421875
Epoch 80, val loss: 1.8270913362503052
Epoch 90, training loss: 8.684343338012695 = 1.8018018007278442 + 1.0 * 6.882541179656982
Epoch 90, val loss: 1.8128442764282227
Epoch 100, training loss: 8.582206726074219 = 1.7852699756622314 + 1.0 * 6.796936988830566
Epoch 100, val loss: 1.7977546453475952
Epoch 110, training loss: 8.495637893676758 = 1.7695364952087402 + 1.0 * 6.726100921630859
Epoch 110, val loss: 1.7833327054977417
Epoch 120, training loss: 8.423625946044922 = 1.7542263269424438 + 1.0 * 6.669399738311768
Epoch 120, val loss: 1.7691233158111572
Epoch 130, training loss: 8.35702896118164 = 1.7376772165298462 + 1.0 * 6.619351863861084
Epoch 130, val loss: 1.754038691520691
Epoch 140, training loss: 8.299554824829102 = 1.718948483467102 + 1.0 * 6.580605983734131
Epoch 140, val loss: 1.7376720905303955
Epoch 150, training loss: 8.246072769165039 = 1.6977170705795288 + 1.0 * 6.548355579376221
Epoch 150, val loss: 1.7195688486099243
Epoch 160, training loss: 8.198232650756836 = 1.6735332012176514 + 1.0 * 6.524699687957764
Epoch 160, val loss: 1.6993651390075684
Epoch 170, training loss: 8.151744842529297 = 1.6459189653396606 + 1.0 * 6.505825996398926
Epoch 170, val loss: 1.676348328590393
Epoch 180, training loss: 8.103950500488281 = 1.614373803138733 + 1.0 * 6.48957633972168
Epoch 180, val loss: 1.6501219272613525
Epoch 190, training loss: 8.054570198059082 = 1.5781930685043335 + 1.0 * 6.476377010345459
Epoch 190, val loss: 1.6200354099273682
Epoch 200, training loss: 8.009125709533691 = 1.5370111465454102 + 1.0 * 6.472114562988281
Epoch 200, val loss: 1.5857957601547241
Epoch 210, training loss: 7.949892520904541 = 1.4913411140441895 + 1.0 * 6.458551406860352
Epoch 210, val loss: 1.547900676727295
Epoch 220, training loss: 7.8907084465026855 = 1.4411190748214722 + 1.0 * 6.449589252471924
Epoch 220, val loss: 1.506377100944519
Epoch 230, training loss: 7.83152437210083 = 1.3866634368896484 + 1.0 * 6.444860935211182
Epoch 230, val loss: 1.461601972579956
Epoch 240, training loss: 7.766225814819336 = 1.3299317359924316 + 1.0 * 6.436294078826904
Epoch 240, val loss: 1.4152002334594727
Epoch 250, training loss: 7.7035231590271 = 1.2719160318374634 + 1.0 * 6.431607246398926
Epoch 250, val loss: 1.368047833442688
Epoch 260, training loss: 7.6389689445495605 = 1.2133370637893677 + 1.0 * 6.425631999969482
Epoch 260, val loss: 1.320822834968567
Epoch 270, training loss: 7.577424049377441 = 1.1556751728057861 + 1.0 * 6.421748638153076
Epoch 270, val loss: 1.2748620510101318
Epoch 280, training loss: 7.516037940979004 = 1.1002439260482788 + 1.0 * 6.4157938957214355
Epoch 280, val loss: 1.2311633825302124
Epoch 290, training loss: 7.457490921020508 = 1.046959638595581 + 1.0 * 6.410531044006348
Epoch 290, val loss: 1.1895990371704102
Epoch 300, training loss: 7.41315221786499 = 0.9959150552749634 + 1.0 * 6.417237281799316
Epoch 300, val loss: 1.1501786708831787
Epoch 310, training loss: 7.353917598724365 = 0.9481627345085144 + 1.0 * 6.405755043029785
Epoch 310, val loss: 1.1134212017059326
Epoch 320, training loss: 7.301060676574707 = 0.9030447602272034 + 1.0 * 6.398015975952148
Epoch 320, val loss: 1.0791622400283813
Epoch 330, training loss: 7.254520416259766 = 0.8600375652313232 + 1.0 * 6.394482612609863
Epoch 330, val loss: 1.0467338562011719
Epoch 340, training loss: 7.208869457244873 = 0.8188996911048889 + 1.0 * 6.389969825744629
Epoch 340, val loss: 1.015931248664856
Epoch 350, training loss: 7.173348903656006 = 0.7796729803085327 + 1.0 * 6.393675804138184
Epoch 350, val loss: 0.9869112372398376
Epoch 360, training loss: 7.129573345184326 = 0.7428660988807678 + 1.0 * 6.386707305908203
Epoch 360, val loss: 0.9599649906158447
Epoch 370, training loss: 7.088253021240234 = 0.7079823017120361 + 1.0 * 6.380270481109619
Epoch 370, val loss: 0.9348870515823364
Epoch 380, training loss: 7.05172872543335 = 0.6746274828910828 + 1.0 * 6.377101421356201
Epoch 380, val loss: 0.9110979437828064
Epoch 390, training loss: 7.0352067947387695 = 0.6425707936286926 + 1.0 * 6.392635822296143
Epoch 390, val loss: 0.8886234760284424
Epoch 400, training loss: 6.990162372589111 = 0.6121508479118347 + 1.0 * 6.378011703491211
Epoch 400, val loss: 0.8677768707275391
Epoch 410, training loss: 6.952749252319336 = 0.5828946828842163 + 1.0 * 6.36985445022583
Epoch 410, val loss: 0.8484513163566589
Epoch 420, training loss: 6.921270370483398 = 0.5544672012329102 + 1.0 * 6.366803169250488
Epoch 420, val loss: 0.8300715088844299
Epoch 430, training loss: 6.890620708465576 = 0.5267147421836853 + 1.0 * 6.363905906677246
Epoch 430, val loss: 0.8126369118690491
Epoch 440, training loss: 6.861598014831543 = 0.49953117966651917 + 1.0 * 6.362066745758057
Epoch 440, val loss: 0.7962741255760193
Epoch 450, training loss: 6.844943523406982 = 0.47311127185821533 + 1.0 * 6.371832370758057
Epoch 450, val loss: 0.7810439467430115
Epoch 460, training loss: 6.80548095703125 = 0.44751569628715515 + 1.0 * 6.357965469360352
Epoch 460, val loss: 0.766982913017273
Epoch 470, training loss: 6.778508186340332 = 0.4224640429019928 + 1.0 * 6.356044292449951
Epoch 470, val loss: 0.7539952397346497
Epoch 480, training loss: 6.751902103424072 = 0.3978615701198578 + 1.0 * 6.354040622711182
Epoch 480, val loss: 0.741863489151001
Epoch 490, training loss: 6.726979732513428 = 0.37387505173683167 + 1.0 * 6.353104591369629
Epoch 490, val loss: 0.7305635213851929
Epoch 500, training loss: 6.700730323791504 = 0.35062769055366516 + 1.0 * 6.350102424621582
Epoch 500, val loss: 0.7203751802444458
Epoch 510, training loss: 6.676182270050049 = 0.3280782699584961 + 1.0 * 6.348104000091553
Epoch 510, val loss: 0.7110787630081177
Epoch 520, training loss: 6.658046245574951 = 0.30631592869758606 + 1.0 * 6.3517303466796875
Epoch 520, val loss: 0.7027350664138794
Epoch 530, training loss: 6.638293743133545 = 0.28557026386260986 + 1.0 * 6.352723598480225
Epoch 530, val loss: 0.6953335404396057
Epoch 540, training loss: 6.6105146408081055 = 0.26596832275390625 + 1.0 * 6.344546318054199
Epoch 540, val loss: 0.689077615737915
Epoch 550, training loss: 6.588800430297852 = 0.24744416773319244 + 1.0 * 6.34135627746582
Epoch 550, val loss: 0.6838109493255615
Epoch 560, training loss: 6.569760322570801 = 0.2300175428390503 + 1.0 * 6.339742660522461
Epoch 560, val loss: 0.6795583367347717
Epoch 570, training loss: 6.563238620758057 = 0.21375055611133575 + 1.0 * 6.349488258361816
Epoch 570, val loss: 0.6763004064559937
Epoch 580, training loss: 6.537684917449951 = 0.1987805962562561 + 1.0 * 6.33890438079834
Epoch 580, val loss: 0.6739917397499084
Epoch 590, training loss: 6.520962715148926 = 0.1849660873413086 + 1.0 * 6.335996627807617
Epoch 590, val loss: 0.6727136969566345
Epoch 600, training loss: 6.5058369636535645 = 0.17224644124507904 + 1.0 * 6.333590507507324
Epoch 600, val loss: 0.6722968220710754
Epoch 610, training loss: 6.508711814880371 = 0.16054295003414154 + 1.0 * 6.348168849945068
Epoch 610, val loss: 0.6727433204650879
Epoch 620, training loss: 6.4818339347839355 = 0.149865984916687 + 1.0 * 6.331967830657959
Epoch 620, val loss: 0.6739345192909241
Epoch 630, training loss: 6.470107078552246 = 0.14011302590370178 + 1.0 * 6.329994201660156
Epoch 630, val loss: 0.6758496761322021
Epoch 640, training loss: 6.459255695343018 = 0.13114236295223236 + 1.0 * 6.328113555908203
Epoch 640, val loss: 0.6783838868141174
Epoch 650, training loss: 6.463101387023926 = 0.12291134893894196 + 1.0 * 6.3401899337768555
Epoch 650, val loss: 0.68138587474823
Epoch 660, training loss: 6.444141864776611 = 0.11538378894329071 + 1.0 * 6.328758239746094
Epoch 660, val loss: 0.6847871541976929
Epoch 670, training loss: 6.432987213134766 = 0.10846668481826782 + 1.0 * 6.324520587921143
Epoch 670, val loss: 0.6886180639266968
Epoch 680, training loss: 6.424959659576416 = 0.10207846760749817 + 1.0 * 6.32288122177124
Epoch 680, val loss: 0.6928023099899292
Epoch 690, training loss: 6.4260711669921875 = 0.09618731588125229 + 1.0 * 6.329884052276611
Epoch 690, val loss: 0.6972727179527283
Epoch 700, training loss: 6.413090705871582 = 0.09078486263751984 + 1.0 * 6.322305679321289
Epoch 700, val loss: 0.7019418478012085
Epoch 710, training loss: 6.4044671058654785 = 0.08579226583242416 + 1.0 * 6.3186750411987305
Epoch 710, val loss: 0.7068110704421997
Epoch 720, training loss: 6.399335861206055 = 0.08115935325622559 + 1.0 * 6.31817626953125
Epoch 720, val loss: 0.711877703666687
Epoch 730, training loss: 6.400461196899414 = 0.07686717808246613 + 1.0 * 6.323594093322754
Epoch 730, val loss: 0.7170369029045105
Epoch 740, training loss: 6.3926005363464355 = 0.07291393727064133 + 1.0 * 6.319686412811279
Epoch 740, val loss: 0.7222357392311096
Epoch 750, training loss: 6.383457660675049 = 0.06924473494291306 + 1.0 * 6.314212799072266
Epoch 750, val loss: 0.727552592754364
Epoch 760, training loss: 6.3788580894470215 = 0.06582166999578476 + 1.0 * 6.3130364418029785
Epoch 760, val loss: 0.7329497933387756
Epoch 770, training loss: 6.378032207489014 = 0.06262018531560898 + 1.0 * 6.3154120445251465
Epoch 770, val loss: 0.7383871674537659
Epoch 780, training loss: 6.373668670654297 = 0.05964013934135437 + 1.0 * 6.314028739929199
Epoch 780, val loss: 0.7437599301338196
Epoch 790, training loss: 6.368318557739258 = 0.0568745993077755 + 1.0 * 6.31144380569458
Epoch 790, val loss: 0.7491770386695862
Epoch 800, training loss: 6.366127967834473 = 0.054286133497953415 + 1.0 * 6.31184196472168
Epoch 800, val loss: 0.7546156644821167
Epoch 810, training loss: 6.360836029052734 = 0.051865823566913605 + 1.0 * 6.308969974517822
Epoch 810, val loss: 0.7600098252296448
Epoch 820, training loss: 6.358120918273926 = 0.049603912979364395 + 1.0 * 6.308516979217529
Epoch 820, val loss: 0.765404999256134
Epoch 830, training loss: 6.359828472137451 = 0.04748319834470749 + 1.0 * 6.312345504760742
Epoch 830, val loss: 0.7707661390304565
Epoch 840, training loss: 6.35158109664917 = 0.04549306631088257 + 1.0 * 6.306087970733643
Epoch 840, val loss: 0.7760648727416992
Epoch 850, training loss: 6.349792003631592 = 0.04362157732248306 + 1.0 * 6.306170463562012
Epoch 850, val loss: 0.7813388109207153
Epoch 860, training loss: 6.352741718292236 = 0.041863102465867996 + 1.0 * 6.310878753662109
Epoch 860, val loss: 0.7865849137306213
Epoch 870, training loss: 6.344851970672607 = 0.040202170610427856 + 1.0 * 6.304649829864502
Epoch 870, val loss: 0.7917816638946533
Epoch 880, training loss: 6.341074466705322 = 0.038640230894088745 + 1.0 * 6.30243444442749
Epoch 880, val loss: 0.7969861030578613
Epoch 890, training loss: 6.342848777770996 = 0.037161946296691895 + 1.0 * 6.305686950683594
Epoch 890, val loss: 0.8021378517150879
Epoch 900, training loss: 6.338834762573242 = 0.03576495870947838 + 1.0 * 6.303069591522217
Epoch 900, val loss: 0.807170569896698
Epoch 910, training loss: 6.335814476013184 = 0.0344536155462265 + 1.0 * 6.301361083984375
Epoch 910, val loss: 0.8121674656867981
Epoch 920, training loss: 6.3334784507751465 = 0.033212196081876755 + 1.0 * 6.300266265869141
Epoch 920, val loss: 0.8171883821487427
Epoch 930, training loss: 6.33417272567749 = 0.03203044831752777 + 1.0 * 6.302142143249512
Epoch 930, val loss: 0.8221648931503296
Epoch 940, training loss: 6.332223415374756 = 0.030911337584257126 + 1.0 * 6.30131196975708
Epoch 940, val loss: 0.8270435929298401
Epoch 950, training loss: 6.328094005584717 = 0.029856238514184952 + 1.0 * 6.2982378005981445
Epoch 950, val loss: 0.831866979598999
Epoch 960, training loss: 6.32568883895874 = 0.028852056711912155 + 1.0 * 6.296836853027344
Epoch 960, val loss: 0.8366676568984985
Epoch 970, training loss: 6.323890686035156 = 0.02789863757789135 + 1.0 * 6.295991897583008
Epoch 970, val loss: 0.8414787650108337
Epoch 980, training loss: 6.334218978881836 = 0.026989873498678207 + 1.0 * 6.307229042053223
Epoch 980, val loss: 0.8461777567863464
Epoch 990, training loss: 6.321325302124023 = 0.026124775409698486 + 1.0 * 6.295200347900391
Epoch 990, val loss: 0.8508156538009644
Epoch 1000, training loss: 6.319674015045166 = 0.025306643918156624 + 1.0 * 6.29436731338501
Epoch 1000, val loss: 0.8554481267929077
Epoch 1010, training loss: 6.324380874633789 = 0.02452288381755352 + 1.0 * 6.299858093261719
Epoch 1010, val loss: 0.8600025177001953
Epoch 1020, training loss: 6.319512367248535 = 0.023776907473802567 + 1.0 * 6.2957353591918945
Epoch 1020, val loss: 0.8644864559173584
Epoch 1030, training loss: 6.316908359527588 = 0.023068269714713097 + 1.0 * 6.293839931488037
Epoch 1030, val loss: 0.8689602613449097
Epoch 1040, training loss: 6.31473970413208 = 0.02238827757537365 + 1.0 * 6.292351245880127
Epoch 1040, val loss: 0.873351514339447
Epoch 1050, training loss: 6.315615177154541 = 0.021738974377512932 + 1.0 * 6.2938761711120605
Epoch 1050, val loss: 0.8777430057525635
Epoch 1060, training loss: 6.313406944274902 = 0.02111765742301941 + 1.0 * 6.2922892570495605
Epoch 1060, val loss: 0.8820547461509705
Epoch 1070, training loss: 6.323012351989746 = 0.02052636258304119 + 1.0 * 6.302485942840576
Epoch 1070, val loss: 0.8862974047660828
Epoch 1080, training loss: 6.313519477844238 = 0.019960714504122734 + 1.0 * 6.293558597564697
Epoch 1080, val loss: 0.8904241323471069
Epoch 1090, training loss: 6.3080596923828125 = 0.019420243799686432 + 1.0 * 6.288639545440674
Epoch 1090, val loss: 0.8945667743682861
Epoch 1100, training loss: 6.307022571563721 = 0.018902206793427467 + 1.0 * 6.288120269775391
Epoch 1100, val loss: 0.898719310760498
Epoch 1110, training loss: 6.313820838928223 = 0.018401674926280975 + 1.0 * 6.295419216156006
Epoch 1110, val loss: 0.9027552008628845
Epoch 1120, training loss: 6.306693077087402 = 0.017926208674907684 + 1.0 * 6.288766860961914
Epoch 1120, val loss: 0.906737744808197
Epoch 1130, training loss: 6.303585529327393 = 0.0174680408090353 + 1.0 * 6.2861175537109375
Epoch 1130, val loss: 0.9106863737106323
Epoch 1140, training loss: 6.303470134735107 = 0.017027510330080986 + 1.0 * 6.286442756652832
Epoch 1140, val loss: 0.9146266579627991
Epoch 1150, training loss: 6.310147762298584 = 0.016604702919721603 + 1.0 * 6.293542861938477
Epoch 1150, val loss: 0.918465256690979
Epoch 1160, training loss: 6.3041462898254395 = 0.016197513788938522 + 1.0 * 6.2879486083984375
Epoch 1160, val loss: 0.9222036004066467
Epoch 1170, training loss: 6.300890922546387 = 0.01581072062253952 + 1.0 * 6.285080432891846
Epoch 1170, val loss: 0.9259427785873413
Epoch 1180, training loss: 6.299252033233643 = 0.015437876805663109 + 1.0 * 6.283813953399658
Epoch 1180, val loss: 0.9296867847442627
Epoch 1190, training loss: 6.300343990325928 = 0.01507650502026081 + 1.0 * 6.285267353057861
Epoch 1190, val loss: 0.9333941340446472
Epoch 1200, training loss: 6.3001837730407715 = 0.014728311449289322 + 1.0 * 6.285455226898193
Epoch 1200, val loss: 0.9370371699333191
Epoch 1210, training loss: 6.299162864685059 = 0.014391916804015636 + 1.0 * 6.284770965576172
Epoch 1210, val loss: 0.9406176805496216
Epoch 1220, training loss: 6.2947797775268555 = 0.01406958606094122 + 1.0 * 6.280710220336914
Epoch 1220, val loss: 0.9441842436790466
Epoch 1230, training loss: 6.2943267822265625 = 0.01375871803611517 + 1.0 * 6.2805681228637695
Epoch 1230, val loss: 0.9477460384368896
Epoch 1240, training loss: 6.29681396484375 = 0.013456921093165874 + 1.0 * 6.2833571434021
Epoch 1240, val loss: 0.9512723684310913
Epoch 1250, training loss: 6.298018455505371 = 0.01316679734736681 + 1.0 * 6.284851551055908
Epoch 1250, val loss: 0.9547297954559326
Epoch 1260, training loss: 6.294079303741455 = 0.012885131873190403 + 1.0 * 6.28119421005249
Epoch 1260, val loss: 0.9580975770950317
Epoch 1270, training loss: 6.290588855743408 = 0.01261490024626255 + 1.0 * 6.2779741287231445
Epoch 1270, val loss: 0.9614856839179993
Epoch 1280, training loss: 6.289937496185303 = 0.012353948317468166 + 1.0 * 6.277583599090576
Epoch 1280, val loss: 0.9648932814598083
Epoch 1290, training loss: 6.291441917419434 = 0.01209899690002203 + 1.0 * 6.279343128204346
Epoch 1290, val loss: 0.9682675004005432
Epoch 1300, training loss: 6.291476726531982 = 0.011854124255478382 + 1.0 * 6.279622554779053
Epoch 1300, val loss: 0.9715345501899719
Epoch 1310, training loss: 6.287351608276367 = 0.011615979485213757 + 1.0 * 6.275735855102539
Epoch 1310, val loss: 0.9747832417488098
Epoch 1320, training loss: 6.291360855102539 = 0.011387110687792301 + 1.0 * 6.27997350692749
Epoch 1320, val loss: 0.9780496954917908
Epoch 1330, training loss: 6.288271903991699 = 0.011166105978190899 + 1.0 * 6.277105808258057
Epoch 1330, val loss: 0.9811948537826538
Epoch 1340, training loss: 6.2885541915893555 = 0.010952536948025227 + 1.0 * 6.277601718902588
Epoch 1340, val loss: 0.984295666217804
Epoch 1350, training loss: 6.285943984985352 = 0.010746229439973831 + 1.0 * 6.275197982788086
Epoch 1350, val loss: 0.9874687790870667
Epoch 1360, training loss: 6.286537170410156 = 0.010543925687670708 + 1.0 * 6.275993347167969
Epoch 1360, val loss: 0.9906236529350281
Epoch 1370, training loss: 6.286839008331299 = 0.010347465984523296 + 1.0 * 6.276491641998291
Epoch 1370, val loss: 0.9937084913253784
Epoch 1380, training loss: 6.287696838378906 = 0.010157771408557892 + 1.0 * 6.277539253234863
Epoch 1380, val loss: 0.9967464804649353
Epoch 1390, training loss: 6.283071041107178 = 0.009972784668207169 + 1.0 * 6.273098468780518
Epoch 1390, val loss: 0.9997538328170776
Epoch 1400, training loss: 6.285565376281738 = 0.009794182144105434 + 1.0 * 6.275771141052246
Epoch 1400, val loss: 1.0027793645858765
Epoch 1410, training loss: 6.282294273376465 = 0.00961997825652361 + 1.0 * 6.272674083709717
Epoch 1410, val loss: 1.0057671070098877
Epoch 1420, training loss: 6.284072399139404 = 0.009451000951230526 + 1.0 * 6.274621486663818
Epoch 1420, val loss: 1.0087273120880127
Epoch 1430, training loss: 6.2825798988342285 = 0.009286816231906414 + 1.0 * 6.2732930183410645
Epoch 1430, val loss: 1.0116249322891235
Epoch 1440, training loss: 6.282858848571777 = 0.009127028286457062 + 1.0 * 6.273731708526611
Epoch 1440, val loss: 1.014522671699524
Epoch 1450, training loss: 6.2793989181518555 = 0.00897216610610485 + 1.0 * 6.2704267501831055
Epoch 1450, val loss: 1.0173884630203247
Epoch 1460, training loss: 6.2790679931640625 = 0.008820624090731144 + 1.0 * 6.270247459411621
Epoch 1460, val loss: 1.020257830619812
Epoch 1470, training loss: 6.290009021759033 = 0.008672893047332764 + 1.0 * 6.281336307525635
Epoch 1470, val loss: 1.0230785608291626
Epoch 1480, training loss: 6.284675598144531 = 0.008530774153769016 + 1.0 * 6.276144981384277
Epoch 1480, val loss: 1.0258034467697144
Epoch 1490, training loss: 6.278199672698975 = 0.008392758667469025 + 1.0 * 6.269806861877441
Epoch 1490, val loss: 1.0285096168518066
Epoch 1500, training loss: 6.275705337524414 = 0.008258730173110962 + 1.0 * 6.267446517944336
Epoch 1500, val loss: 1.0312820672988892
Epoch 1510, training loss: 6.276364326477051 = 0.008127054199576378 + 1.0 * 6.268237113952637
Epoch 1510, val loss: 1.0340524911880493
Epoch 1520, training loss: 6.284653186798096 = 0.007999062538146973 + 1.0 * 6.276654243469238
Epoch 1520, val loss: 1.0367457866668701
Epoch 1530, training loss: 6.278453350067139 = 0.00787330511957407 + 1.0 * 6.270579814910889
Epoch 1530, val loss: 1.0393327474594116
Epoch 1540, training loss: 6.27532434463501 = 0.0077520934864878654 + 1.0 * 6.267572402954102
Epoch 1540, val loss: 1.041933298110962
Epoch 1550, training loss: 6.275925159454346 = 0.007634981069713831 + 1.0 * 6.268290042877197
Epoch 1550, val loss: 1.0445932149887085
Epoch 1560, training loss: 6.275128364562988 = 0.007518880069255829 + 1.0 * 6.267609596252441
Epoch 1560, val loss: 1.0472121238708496
Epoch 1570, training loss: 6.273647785186768 = 0.007405908778309822 + 1.0 * 6.266242027282715
Epoch 1570, val loss: 1.0498253107070923
Epoch 1580, training loss: 6.274782657623291 = 0.00729580782353878 + 1.0 * 6.267487049102783
Epoch 1580, val loss: 1.0524168014526367
Epoch 1590, training loss: 6.277278900146484 = 0.00718853110447526 + 1.0 * 6.270090579986572
Epoch 1590, val loss: 1.0549215078353882
Epoch 1600, training loss: 6.272250175476074 = 0.00708368793129921 + 1.0 * 6.265166282653809
Epoch 1600, val loss: 1.057379961013794
Epoch 1610, training loss: 6.2715044021606445 = 0.00698145991191268 + 1.0 * 6.264523029327393
Epoch 1610, val loss: 1.0599249601364136
Epoch 1620, training loss: 6.271717071533203 = 0.006881351582705975 + 1.0 * 6.264835834503174
Epoch 1620, val loss: 1.0624204874038696
Epoch 1630, training loss: 6.278997421264648 = 0.006783489603549242 + 1.0 * 6.272213935852051
Epoch 1630, val loss: 1.064873218536377
Epoch 1640, training loss: 6.272849082946777 = 0.006689481902867556 + 1.0 * 6.266159534454346
Epoch 1640, val loss: 1.0672731399536133
Epoch 1650, training loss: 6.271166801452637 = 0.006596695631742477 + 1.0 * 6.264570236206055
Epoch 1650, val loss: 1.0696510076522827
Epoch 1660, training loss: 6.281610012054443 = 0.006506352219730616 + 1.0 * 6.275103569030762
Epoch 1660, val loss: 1.072062611579895
Epoch 1670, training loss: 6.270559787750244 = 0.006418935488909483 + 1.0 * 6.264141082763672
Epoch 1670, val loss: 1.0742998123168945
Epoch 1680, training loss: 6.268329620361328 = 0.006333437282592058 + 1.0 * 6.261996269226074
Epoch 1680, val loss: 1.076649785041809
Epoch 1690, training loss: 6.267289161682129 = 0.006249540019780397 + 1.0 * 6.261039733886719
Epoch 1690, val loss: 1.0790271759033203
Epoch 1700, training loss: 6.267207622528076 = 0.006166708655655384 + 1.0 * 6.261040687561035
Epoch 1700, val loss: 1.0813874006271362
Epoch 1710, training loss: 6.274295330047607 = 0.006085514556616545 + 1.0 * 6.268209934234619
Epoch 1710, val loss: 1.0837098360061646
Epoch 1720, training loss: 6.268918514251709 = 0.006005486473441124 + 1.0 * 6.262913227081299
Epoch 1720, val loss: 1.085936188697815
Epoch 1730, training loss: 6.273582935333252 = 0.005928357131779194 + 1.0 * 6.2676544189453125
Epoch 1730, val loss: 1.0881744623184204
Epoch 1740, training loss: 6.267306804656982 = 0.005853802897036076 + 1.0 * 6.261453151702881
Epoch 1740, val loss: 1.0904189348220825
Epoch 1750, training loss: 6.265678882598877 = 0.005779720842838287 + 1.0 * 6.259899139404297
Epoch 1750, val loss: 1.0926544666290283
Epoch 1760, training loss: 6.265805721282959 = 0.0057068816386163235 + 1.0 * 6.260098934173584
Epoch 1760, val loss: 1.0949138402938843
Epoch 1770, training loss: 6.274737358093262 = 0.005635276902467012 + 1.0 * 6.269102096557617
Epoch 1770, val loss: 1.0970828533172607
Epoch 1780, training loss: 6.268609523773193 = 0.00556675111874938 + 1.0 * 6.26304292678833
Epoch 1780, val loss: 1.0991872549057007
Epoch 1790, training loss: 6.264252662658691 = 0.005498959217220545 + 1.0 * 6.258753776550293
Epoch 1790, val loss: 1.1013238430023193
Epoch 1800, training loss: 6.262744903564453 = 0.005432974547147751 + 1.0 * 6.257311820983887
Epoch 1800, val loss: 1.1035033464431763
Epoch 1810, training loss: 6.265007972717285 = 0.005367600359022617 + 1.0 * 6.259640216827393
Epoch 1810, val loss: 1.1056623458862305
Epoch 1820, training loss: 6.269394397735596 = 0.005303524900227785 + 1.0 * 6.2640910148620605
Epoch 1820, val loss: 1.107736587524414
Epoch 1830, training loss: 6.262609958648682 = 0.005240562837570906 + 1.0 * 6.257369518280029
Epoch 1830, val loss: 1.1097817420959473
Epoch 1840, training loss: 6.264286994934082 = 0.005179593805223703 + 1.0 * 6.25910758972168
Epoch 1840, val loss: 1.1118706464767456
Epoch 1850, training loss: 6.2657084465026855 = 0.005119886249303818 + 1.0 * 6.260588645935059
Epoch 1850, val loss: 1.113937258720398
Epoch 1860, training loss: 6.2634968757629395 = 0.005061286501586437 + 1.0 * 6.2584357261657715
Epoch 1860, val loss: 1.1159794330596924
Epoch 1870, training loss: 6.2632269859313965 = 0.005003378726541996 + 1.0 * 6.258223533630371
Epoch 1870, val loss: 1.1180225610733032
Epoch 1880, training loss: 6.261665344238281 = 0.00494645768776536 + 1.0 * 6.25671911239624
Epoch 1880, val loss: 1.1200273036956787
Epoch 1890, training loss: 6.26527738571167 = 0.004890682641416788 + 1.0 * 6.2603864669799805
Epoch 1890, val loss: 1.1220223903656006
Epoch 1900, training loss: 6.260512828826904 = 0.004836366977542639 + 1.0 * 6.25567626953125
Epoch 1900, val loss: 1.124024510383606
Epoch 1910, training loss: 6.264786720275879 = 0.004782991949468851 + 1.0 * 6.260003566741943
Epoch 1910, val loss: 1.1260219812393188
Epoch 1920, training loss: 6.259807586669922 = 0.004730409476906061 + 1.0 * 6.255077362060547
Epoch 1920, val loss: 1.1279155015945435
Epoch 1930, training loss: 6.259456634521484 = 0.0046792407520115376 + 1.0 * 6.254777431488037
Epoch 1930, val loss: 1.129899501800537
Epoch 1940, training loss: 6.263569355010986 = 0.0046289595775306225 + 1.0 * 6.25894021987915
Epoch 1940, val loss: 1.1318533420562744
Epoch 1950, training loss: 6.260879039764404 = 0.004579274915158749 + 1.0 * 6.25629997253418
Epoch 1950, val loss: 1.1337298154830933
Epoch 1960, training loss: 6.2603349685668945 = 0.004530557431280613 + 1.0 * 6.255804538726807
Epoch 1960, val loss: 1.1356414556503296
Epoch 1970, training loss: 6.263935565948486 = 0.004483216907829046 + 1.0 * 6.2594523429870605
Epoch 1970, val loss: 1.1375454664230347
Epoch 1980, training loss: 6.258505821228027 = 0.004436300601810217 + 1.0 * 6.2540693283081055
Epoch 1980, val loss: 1.139386773109436
Epoch 1990, training loss: 6.259718894958496 = 0.004390526097267866 + 1.0 * 6.255328178405762
Epoch 1990, val loss: 1.1412665843963623
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 10.541740417480469 = 1.944902777671814 + 1.0 * 8.596837997436523
Epoch 0, val loss: 1.944865345954895
Epoch 10, training loss: 10.530963897705078 = 1.9344295263290405 + 1.0 * 8.596534729003906
Epoch 10, val loss: 1.9344878196716309
Epoch 20, training loss: 10.515413284301758 = 1.9216065406799316 + 1.0 * 8.593807220458984
Epoch 20, val loss: 1.9211430549621582
Epoch 30, training loss: 10.476704597473145 = 1.9041815996170044 + 1.0 * 8.57252311706543
Epoch 30, val loss: 1.902578353881836
Epoch 40, training loss: 10.329336166381836 = 1.882062315940857 + 1.0 * 8.447274208068848
Epoch 40, val loss: 1.8797358274459839
Epoch 50, training loss: 9.869916915893555 = 1.8576176166534424 + 1.0 * 8.012299537658691
Epoch 50, val loss: 1.8551748991012573
Epoch 60, training loss: 9.463469505310059 = 1.8369991779327393 + 1.0 * 7.626470565795898
Epoch 60, val loss: 1.8362761735916138
Epoch 70, training loss: 9.06203842163086 = 1.8216068744659424 + 1.0 * 7.240431785583496
Epoch 70, val loss: 1.8217511177062988
Epoch 80, training loss: 8.86293888092041 = 1.8075460195541382 + 1.0 * 7.055392742156982
Epoch 80, val loss: 1.808106780052185
Epoch 90, training loss: 8.727652549743652 = 1.7921913862228394 + 1.0 * 6.935461521148682
Epoch 90, val loss: 1.7934201955795288
Epoch 100, training loss: 8.636251449584961 = 1.7748961448669434 + 1.0 * 6.861355304718018
Epoch 100, val loss: 1.7776774168014526
Epoch 110, training loss: 8.544018745422363 = 1.757269263267517 + 1.0 * 6.786749839782715
Epoch 110, val loss: 1.7618168592453003
Epoch 120, training loss: 8.473475456237793 = 1.7400881052017212 + 1.0 * 6.733387470245361
Epoch 120, val loss: 1.7461848258972168
Epoch 130, training loss: 8.405722618103027 = 1.721530795097351 + 1.0 * 6.684191703796387
Epoch 130, val loss: 1.7291920185089111
Epoch 140, training loss: 8.3467435836792 = 1.700240969657898 + 1.0 * 6.646502494812012
Epoch 140, val loss: 1.7099556922912598
Epoch 150, training loss: 8.295074462890625 = 1.6758861541748047 + 1.0 * 6.6191887855529785
Epoch 150, val loss: 1.6881247758865356
Epoch 160, training loss: 8.255699157714844 = 1.647843360900879 + 1.0 * 6.607855796813965
Epoch 160, val loss: 1.6630115509033203
Epoch 170, training loss: 8.193014144897461 = 1.6165050268173218 + 1.0 * 6.576509475708008
Epoch 170, val loss: 1.634971261024475
Epoch 180, training loss: 8.138517379760742 = 1.5815421342849731 + 1.0 * 6.556975364685059
Epoch 180, val loss: 1.6039115190505981
Epoch 190, training loss: 8.0806884765625 = 1.5423412322998047 + 1.0 * 6.5383477210998535
Epoch 190, val loss: 1.5693085193634033
Epoch 200, training loss: 8.01914119720459 = 1.4982391595840454 + 1.0 * 6.520901679992676
Epoch 200, val loss: 1.5305683612823486
Epoch 210, training loss: 7.955974578857422 = 1.4491115808486938 + 1.0 * 6.506863117218018
Epoch 210, val loss: 1.4877090454101562
Epoch 220, training loss: 7.893728256225586 = 1.3966506719589233 + 1.0 * 6.497077465057373
Epoch 220, val loss: 1.4426476955413818
Epoch 230, training loss: 7.827506065368652 = 1.3419206142425537 + 1.0 * 6.4855852127075195
Epoch 230, val loss: 1.3961091041564941
Epoch 240, training loss: 7.760774612426758 = 1.2849889993667603 + 1.0 * 6.475785732269287
Epoch 240, val loss: 1.348310947418213
Epoch 250, training loss: 7.693269729614258 = 1.2262756824493408 + 1.0 * 6.466994285583496
Epoch 250, val loss: 1.2996845245361328
Epoch 260, training loss: 7.631375789642334 = 1.1665021181106567 + 1.0 * 6.464873790740967
Epoch 260, val loss: 1.2507940530776978
Epoch 270, training loss: 7.5648393630981445 = 1.1077698469161987 + 1.0 * 6.457069396972656
Epoch 270, val loss: 1.2034891843795776
Epoch 280, training loss: 7.497710704803467 = 1.0503787994384766 + 1.0 * 6.44733190536499
Epoch 280, val loss: 1.1580897569656372
Epoch 290, training loss: 7.433792591094971 = 0.9943023920059204 + 1.0 * 6.43949031829834
Epoch 290, val loss: 1.1146131753921509
Epoch 300, training loss: 7.373963832855225 = 0.939813494682312 + 1.0 * 6.434150218963623
Epoch 300, val loss: 1.0732108354568481
Epoch 310, training loss: 7.323317527770996 = 0.8878505229949951 + 1.0 * 6.43546724319458
Epoch 310, val loss: 1.0346899032592773
Epoch 320, training loss: 7.262986183166504 = 0.8392173647880554 + 1.0 * 6.423768997192383
Epoch 320, val loss: 0.9998268485069275
Epoch 330, training loss: 7.21157693862915 = 0.7931576371192932 + 1.0 * 6.418419361114502
Epoch 330, val loss: 0.9677536487579346
Epoch 340, training loss: 7.162792205810547 = 0.74974524974823 + 1.0 * 6.413046836853027
Epoch 340, val loss: 0.9384675025939941
Epoch 350, training loss: 7.117687702178955 = 0.7089904546737671 + 1.0 * 6.408697128295898
Epoch 350, val loss: 0.9118217825889587
Epoch 360, training loss: 7.082376956939697 = 0.6704699397087097 + 1.0 * 6.411907196044922
Epoch 360, val loss: 0.8874133229255676
Epoch 370, training loss: 7.03619384765625 = 0.6342494487762451 + 1.0 * 6.401944160461426
Epoch 370, val loss: 0.8653320670127869
Epoch 380, training loss: 6.996474266052246 = 0.5998225808143616 + 1.0 * 6.396651744842529
Epoch 380, val loss: 0.8451257348060608
Epoch 390, training loss: 6.958498477935791 = 0.566855251789093 + 1.0 * 6.391643047332764
Epoch 390, val loss: 0.8264446258544922
Epoch 400, training loss: 6.926649570465088 = 0.5353617668151855 + 1.0 * 6.391287803649902
Epoch 400, val loss: 0.8093017339706421
Epoch 410, training loss: 6.889974117279053 = 0.5055660605430603 + 1.0 * 6.384407997131348
Epoch 410, val loss: 0.7938442826271057
Epoch 420, training loss: 6.857715606689453 = 0.477033793926239 + 1.0 * 6.380681991577148
Epoch 420, val loss: 0.7798082232475281
Epoch 430, training loss: 6.84595251083374 = 0.4496888816356659 + 1.0 * 6.396263599395752
Epoch 430, val loss: 0.7669864892959595
Epoch 440, training loss: 6.80088472366333 = 0.4237242043018341 + 1.0 * 6.377160549163818
Epoch 440, val loss: 0.7554015517234802
Epoch 450, training loss: 6.771846771240234 = 0.39891529083251953 + 1.0 * 6.372931480407715
Epoch 450, val loss: 0.7450157999992371
Epoch 460, training loss: 6.744023323059082 = 0.3749808073043823 + 1.0 * 6.36904239654541
Epoch 460, val loss: 0.7355666160583496
Epoch 470, training loss: 6.717751502990723 = 0.35184216499328613 + 1.0 * 6.365909099578857
Epoch 470, val loss: 0.7270730137825012
Epoch 480, training loss: 6.722288131713867 = 0.3295729160308838 + 1.0 * 6.392714977264404
Epoch 480, val loss: 0.7195219397544861
Epoch 490, training loss: 6.679741859436035 = 0.30844977498054504 + 1.0 * 6.3712921142578125
Epoch 490, val loss: 0.7128813862800598
Epoch 500, training loss: 6.648016929626465 = 0.2883754372596741 + 1.0 * 6.3596415519714355
Epoch 500, val loss: 0.7073430418968201
Epoch 510, training loss: 6.626603603363037 = 0.26923805475234985 + 1.0 * 6.357365608215332
Epoch 510, val loss: 0.7026805877685547
Epoch 520, training loss: 6.605904579162598 = 0.2510406970977783 + 1.0 * 6.354864120483398
Epoch 520, val loss: 0.6990050673484802
Epoch 530, training loss: 6.593466758728027 = 0.23384559154510498 + 1.0 * 6.359621047973633
Epoch 530, val loss: 0.6963027119636536
Epoch 540, training loss: 6.577086925506592 = 0.21784085035324097 + 1.0 * 6.359246253967285
Epoch 540, val loss: 0.6945520639419556
Epoch 550, training loss: 6.554764747619629 = 0.20299749076366425 + 1.0 * 6.351767063140869
Epoch 550, val loss: 0.6937773823738098
Epoch 560, training loss: 6.537899494171143 = 0.1892436146736145 + 1.0 * 6.348655700683594
Epoch 560, val loss: 0.6939961314201355
Epoch 570, training loss: 6.523068428039551 = 0.1764477640390396 + 1.0 * 6.346620559692383
Epoch 570, val loss: 0.6950350999832153
Epoch 580, training loss: 6.518701076507568 = 0.16460981965065002 + 1.0 * 6.354091167449951
Epoch 580, val loss: 0.6968902349472046
Epoch 590, training loss: 6.500923156738281 = 0.15367522835731506 + 1.0 * 6.347248077392578
Epoch 590, val loss: 0.6993522644042969
Epoch 600, training loss: 6.486501693725586 = 0.1436241865158081 + 1.0 * 6.342877388000488
Epoch 600, val loss: 0.7026132941246033
Epoch 610, training loss: 6.475776195526123 = 0.13431857526302338 + 1.0 * 6.341457843780518
Epoch 610, val loss: 0.7063486576080322
Epoch 620, training loss: 6.471001625061035 = 0.12575313448905945 + 1.0 * 6.345248699188232
Epoch 620, val loss: 0.7105932831764221
Epoch 630, training loss: 6.457922458648682 = 0.11790098994970322 + 1.0 * 6.34002161026001
Epoch 630, val loss: 0.7152072191238403
Epoch 640, training loss: 6.446334362030029 = 0.11065562069416046 + 1.0 * 6.335678577423096
Epoch 640, val loss: 0.7202206254005432
Epoch 650, training loss: 6.439185619354248 = 0.10393426567316055 + 1.0 * 6.335251331329346
Epoch 650, val loss: 0.7255427837371826
Epoch 660, training loss: 6.432459831237793 = 0.09772989153862 + 1.0 * 6.33473014831543
Epoch 660, val loss: 0.7310817241668701
Epoch 670, training loss: 6.424951076507568 = 0.092013418674469 + 1.0 * 6.332937717437744
Epoch 670, val loss: 0.7369084358215332
Epoch 680, training loss: 6.41497278213501 = 0.0867086872458458 + 1.0 * 6.328264236450195
Epoch 680, val loss: 0.7429180145263672
Epoch 690, training loss: 6.407719612121582 = 0.0817626565694809 + 1.0 * 6.32595682144165
Epoch 690, val loss: 0.7490955591201782
Epoch 700, training loss: 6.423752307891846 = 0.07715506106615067 + 1.0 * 6.346597194671631
Epoch 700, val loss: 0.7553696036338806
Epoch 710, training loss: 6.39889669418335 = 0.07294408977031708 + 1.0 * 6.325952529907227
Epoch 710, val loss: 0.7617834210395813
Epoch 720, training loss: 6.391802787780762 = 0.06902527064085007 + 1.0 * 6.32277774810791
Epoch 720, val loss: 0.7682805061340332
Epoch 730, training loss: 6.386598587036133 = 0.0653802827000618 + 1.0 * 6.321218490600586
Epoch 730, val loss: 0.7748739123344421
Epoch 740, training loss: 6.384223937988281 = 0.06197897344827652 + 1.0 * 6.322245121002197
Epoch 740, val loss: 0.781542181968689
Epoch 750, training loss: 6.379817485809326 = 0.05881239473819733 + 1.0 * 6.321004867553711
Epoch 750, val loss: 0.7880924940109253
Epoch 760, training loss: 6.374804973602295 = 0.05588369444012642 + 1.0 * 6.318921089172363
Epoch 760, val loss: 0.7947009801864624
Epoch 770, training loss: 6.375158309936523 = 0.053148336708545685 + 1.0 * 6.322010040283203
Epoch 770, val loss: 0.8012825846672058
Epoch 780, training loss: 6.365241050720215 = 0.050601791590452194 + 1.0 * 6.314639091491699
Epoch 780, val loss: 0.8078787922859192
Epoch 790, training loss: 6.365964889526367 = 0.048219434916973114 + 1.0 * 6.317745685577393
Epoch 790, val loss: 0.8144235610961914
Epoch 800, training loss: 6.36078405380249 = 0.04599035531282425 + 1.0 * 6.314793586730957
Epoch 800, val loss: 0.8208976984024048
Epoch 810, training loss: 6.361820697784424 = 0.04391445592045784 + 1.0 * 6.317906379699707
Epoch 810, val loss: 0.8274216651916504
Epoch 820, training loss: 6.352571487426758 = 0.041963450610637665 + 1.0 * 6.31060791015625
Epoch 820, val loss: 0.8336822986602783
Epoch 830, training loss: 6.349830627441406 = 0.04014437645673752 + 1.0 * 6.309686183929443
Epoch 830, val loss: 0.8400840759277344
Epoch 840, training loss: 6.346536636352539 = 0.0384308286011219 + 1.0 * 6.308105945587158
Epoch 840, val loss: 0.84636390209198
Epoch 850, training loss: 6.348030090332031 = 0.0368170402944088 + 1.0 * 6.31121301651001
Epoch 850, val loss: 0.8526238799095154
Epoch 860, training loss: 6.3514862060546875 = 0.03530893847346306 + 1.0 * 6.3161773681640625
Epoch 860, val loss: 0.858640730381012
Epoch 870, training loss: 6.339378833770752 = 0.033896010369062424 + 1.0 * 6.305482864379883
Epoch 870, val loss: 0.8647071719169617
Epoch 880, training loss: 6.336266994476318 = 0.03256422281265259 + 1.0 * 6.3037028312683105
Epoch 880, val loss: 0.8707374334335327
Epoch 890, training loss: 6.3346686363220215 = 0.031303953379392624 + 1.0 * 6.3033647537231445
Epoch 890, val loss: 0.8766928911209106
Epoch 900, training loss: 6.3411030769348145 = 0.030117768794298172 + 1.0 * 6.310985088348389
Epoch 900, val loss: 0.882531464099884
Epoch 910, training loss: 6.330555438995361 = 0.028998153284192085 + 1.0 * 6.3015570640563965
Epoch 910, val loss: 0.8882423043251038
Epoch 920, training loss: 6.329047679901123 = 0.027942389249801636 + 1.0 * 6.301105499267578
Epoch 920, val loss: 0.8939505219459534
Epoch 930, training loss: 6.326582431793213 = 0.02694065496325493 + 1.0 * 6.2996416091918945
Epoch 930, val loss: 0.8995987772941589
Epoch 940, training loss: 6.333403587341309 = 0.025989282876253128 + 1.0 * 6.307414531707764
Epoch 940, val loss: 0.9051333665847778
Epoch 950, training loss: 6.325832843780518 = 0.025096604600548744 + 1.0 * 6.300736427307129
Epoch 950, val loss: 0.9106528162956238
Epoch 960, training loss: 6.321805000305176 = 0.0242462120950222 + 1.0 * 6.297558784484863
Epoch 960, val loss: 0.916094183921814
Epoch 970, training loss: 6.321350574493408 = 0.02343868277966976 + 1.0 * 6.297912120819092
Epoch 970, val loss: 0.9215106964111328
Epoch 980, training loss: 6.32440185546875 = 0.02267063595354557 + 1.0 * 6.301731109619141
Epoch 980, val loss: 0.9267796277999878
Epoch 990, training loss: 6.319941997528076 = 0.02193852886557579 + 1.0 * 6.298003673553467
Epoch 990, val loss: 0.9319540858268738
Epoch 1000, training loss: 6.31605863571167 = 0.021246254444122314 + 1.0 * 6.294812202453613
Epoch 1000, val loss: 0.9371665716171265
Epoch 1010, training loss: 6.317912578582764 = 0.020584238693118095 + 1.0 * 6.297328472137451
Epoch 1010, val loss: 0.9422111511230469
Epoch 1020, training loss: 6.319497108459473 = 0.019957849755883217 + 1.0 * 6.299539089202881
Epoch 1020, val loss: 0.9472022652626038
Epoch 1030, training loss: 6.311705112457275 = 0.01936246082186699 + 1.0 * 6.292342662811279
Epoch 1030, val loss: 0.9521649479866028
Epoch 1040, training loss: 6.310129642486572 = 0.0187941063195467 + 1.0 * 6.291335582733154
Epoch 1040, val loss: 0.9570997357368469
Epoch 1050, training loss: 6.3090972900390625 = 0.01824776828289032 + 1.0 * 6.290849685668945
Epoch 1050, val loss: 0.9619070291519165
Epoch 1060, training loss: 6.316347599029541 = 0.017726216465234756 + 1.0 * 6.29862117767334
Epoch 1060, val loss: 0.966623842716217
Epoch 1070, training loss: 6.306553840637207 = 0.017231730744242668 + 1.0 * 6.2893218994140625
Epoch 1070, val loss: 0.9712819457054138
Epoch 1080, training loss: 6.304920673370361 = 0.01675991714000702 + 1.0 * 6.288160800933838
Epoch 1080, val loss: 0.9759588241577148
Epoch 1090, training loss: 6.304925918579102 = 0.016306566074490547 + 1.0 * 6.288619518280029
Epoch 1090, val loss: 0.9805607795715332
Epoch 1100, training loss: 6.310590744018555 = 0.015871481969952583 + 1.0 * 6.294719219207764
Epoch 1100, val loss: 0.9850696921348572
Epoch 1110, training loss: 6.306854248046875 = 0.015453862026333809 + 1.0 * 6.29140043258667
Epoch 1110, val loss: 0.9894461035728455
Epoch 1120, training loss: 6.30308723449707 = 0.015054604038596153 + 1.0 * 6.288032531738281
Epoch 1120, val loss: 0.9938764572143555
Epoch 1130, training loss: 6.302282810211182 = 0.014670604839920998 + 1.0 * 6.287612438201904
Epoch 1130, val loss: 0.9981842041015625
Epoch 1140, training loss: 6.306902885437012 = 0.014302629977464676 + 1.0 * 6.292600154876709
Epoch 1140, val loss: 1.0024807453155518
Epoch 1150, training loss: 6.300195693969727 = 0.013951950706541538 + 1.0 * 6.286243915557861
Epoch 1150, val loss: 1.0067110061645508
Epoch 1160, training loss: 6.296848297119141 = 0.013612222857773304 + 1.0 * 6.283236026763916
Epoch 1160, val loss: 1.0108925104141235
Epoch 1170, training loss: 6.298611164093018 = 0.013286232948303223 + 1.0 * 6.285325050354004
Epoch 1170, val loss: 1.0150593519210815
Epoch 1180, training loss: 6.3009562492370605 = 0.012973766773939133 + 1.0 * 6.28798246383667
Epoch 1180, val loss: 1.0190982818603516
Epoch 1190, training loss: 6.2952775955200195 = 0.012670737691223621 + 1.0 * 6.282607078552246
Epoch 1190, val loss: 1.023066759109497
Epoch 1200, training loss: 6.292570114135742 = 0.012380978092551231 + 1.0 * 6.280189037322998
Epoch 1200, val loss: 1.0270947217941284
Epoch 1210, training loss: 6.294460773468018 = 0.01210019364953041 + 1.0 * 6.282360553741455
Epoch 1210, val loss: 1.0310354232788086
Epoch 1220, training loss: 6.2954630851745605 = 0.011829053983092308 + 1.0 * 6.283634185791016
Epoch 1220, val loss: 1.0348682403564453
Epoch 1230, training loss: 6.297852993011475 = 0.011567485518753529 + 1.0 * 6.286285400390625
Epoch 1230, val loss: 1.0386396646499634
Epoch 1240, training loss: 6.292524814605713 = 0.01131836511194706 + 1.0 * 6.2812066078186035
Epoch 1240, val loss: 1.0424715280532837
Epoch 1250, training loss: 6.2902936935424805 = 0.011076666414737701 + 1.0 * 6.27921724319458
Epoch 1250, val loss: 1.0461950302124023
Epoch 1260, training loss: 6.291857719421387 = 0.01084288489073515 + 1.0 * 6.281014919281006
Epoch 1260, val loss: 1.0498647689819336
Epoch 1270, training loss: 6.2910590171813965 = 0.01061656791716814 + 1.0 * 6.280442237854004
Epoch 1270, val loss: 1.0534521341323853
Epoch 1280, training loss: 6.288135051727295 = 0.010398950427770615 + 1.0 * 6.277736186981201
Epoch 1280, val loss: 1.0570385456085205
Epoch 1290, training loss: 6.290254592895508 = 0.010187850333750248 + 1.0 * 6.280066967010498
Epoch 1290, val loss: 1.0605738162994385
Epoch 1300, training loss: 6.288651943206787 = 0.009985402226448059 + 1.0 * 6.2786664962768555
Epoch 1300, val loss: 1.0640363693237305
Epoch 1310, training loss: 6.286911487579346 = 0.009790698066353798 + 1.0 * 6.277120590209961
Epoch 1310, val loss: 1.0675870180130005
Epoch 1320, training loss: 6.283819675445557 = 0.00959936622530222 + 1.0 * 6.2742204666137695
Epoch 1320, val loss: 1.0709882974624634
Epoch 1330, training loss: 6.283992290496826 = 0.00941393245011568 + 1.0 * 6.27457857131958
Epoch 1330, val loss: 1.0744134187698364
Epoch 1340, training loss: 6.288101673126221 = 0.0092335045337677 + 1.0 * 6.278868198394775
Epoch 1340, val loss: 1.0777769088745117
Epoch 1350, training loss: 6.289640426635742 = 0.009060210548341274 + 1.0 * 6.280580043792725
Epoch 1350, val loss: 1.0810819864273071
Epoch 1360, training loss: 6.288389205932617 = 0.008893806487321854 + 1.0 * 6.2794952392578125
Epoch 1360, val loss: 1.084340214729309
Epoch 1370, training loss: 6.282046794891357 = 0.008733418770134449 + 1.0 * 6.273313522338867
Epoch 1370, val loss: 1.0876165628433228
Epoch 1380, training loss: 6.281845569610596 = 0.008576873689889908 + 1.0 * 6.273268699645996
Epoch 1380, val loss: 1.0908411741256714
Epoch 1390, training loss: 6.289707660675049 = 0.008424877189099789 + 1.0 * 6.281282901763916
Epoch 1390, val loss: 1.0940253734588623
Epoch 1400, training loss: 6.2800445556640625 = 0.008275738917291164 + 1.0 * 6.271769046783447
Epoch 1400, val loss: 1.0971003770828247
Epoch 1410, training loss: 6.279057502746582 = 0.008131316863000393 + 1.0 * 6.270925998687744
Epoch 1410, val loss: 1.1002553701400757
Epoch 1420, training loss: 6.279986381530762 = 0.007991008460521698 + 1.0 * 6.271995544433594
Epoch 1420, val loss: 1.1033648252487183
Epoch 1430, training loss: 6.281580924987793 = 0.00785432942211628 + 1.0 * 6.273726463317871
Epoch 1430, val loss: 1.1064320802688599
Epoch 1440, training loss: 6.277567386627197 = 0.007721861358731985 + 1.0 * 6.269845485687256
Epoch 1440, val loss: 1.1094996929168701
Epoch 1450, training loss: 6.282704830169678 = 0.007592877373099327 + 1.0 * 6.275112152099609
Epoch 1450, val loss: 1.1124885082244873
Epoch 1460, training loss: 6.282135963439941 = 0.007468275725841522 + 1.0 * 6.274667739868164
Epoch 1460, val loss: 1.1153857707977295
Epoch 1470, training loss: 6.2758469581604 = 0.007348330225795507 + 1.0 * 6.268498420715332
Epoch 1470, val loss: 1.1183513402938843
Epoch 1480, training loss: 6.275716781616211 = 0.007231348194181919 + 1.0 * 6.2684855461120605
Epoch 1480, val loss: 1.121293067932129
Epoch 1490, training loss: 6.276605606079102 = 0.007116276305168867 + 1.0 * 6.269489288330078
Epoch 1490, val loss: 1.12418794631958
Epoch 1500, training loss: 6.278839588165283 = 0.0070043024607002735 + 1.0 * 6.2718353271484375
Epoch 1500, val loss: 1.1270194053649902
Epoch 1510, training loss: 6.277785778045654 = 0.006894651334732771 + 1.0 * 6.270891189575195
Epoch 1510, val loss: 1.1297967433929443
Epoch 1520, training loss: 6.272866725921631 = 0.006788958329707384 + 1.0 * 6.266077995300293
Epoch 1520, val loss: 1.1325873136520386
Epoch 1530, training loss: 6.271925449371338 = 0.006685851141810417 + 1.0 * 6.265239715576172
Epoch 1530, val loss: 1.1353775262832642
Epoch 1540, training loss: 6.279124736785889 = 0.0065848869271576405 + 1.0 * 6.2725396156311035
Epoch 1540, val loss: 1.1380797624588013
Epoch 1550, training loss: 6.271692276000977 = 0.006487595848739147 + 1.0 * 6.265204906463623
Epoch 1550, val loss: 1.1407922506332397
Epoch 1560, training loss: 6.270995616912842 = 0.006392207928001881 + 1.0 * 6.264603614807129
Epoch 1560, val loss: 1.1435143947601318
Epoch 1570, training loss: 6.276210308074951 = 0.006298763211816549 + 1.0 * 6.269911766052246
Epoch 1570, val loss: 1.1461580991744995
Epoch 1580, training loss: 6.2710442543029785 = 0.006208612117916346 + 1.0 * 6.264835834503174
Epoch 1580, val loss: 1.1487308740615845
Epoch 1590, training loss: 6.268929958343506 = 0.006120773497968912 + 1.0 * 6.2628092765808105
Epoch 1590, val loss: 1.1513901948928833
Epoch 1600, training loss: 6.268820285797119 = 0.006034546997398138 + 1.0 * 6.262785911560059
Epoch 1600, val loss: 1.154010534286499
Epoch 1610, training loss: 6.2744598388671875 = 0.005950033199042082 + 1.0 * 6.268509864807129
Epoch 1610, val loss: 1.1565600633621216
Epoch 1620, training loss: 6.271979331970215 = 0.005867324769496918 + 1.0 * 6.266111850738525
Epoch 1620, val loss: 1.159101128578186
Epoch 1630, training loss: 6.272130012512207 = 0.005787059664726257 + 1.0 * 6.266343116760254
Epoch 1630, val loss: 1.1615986824035645
Epoch 1640, training loss: 6.268909931182861 = 0.005709355231374502 + 1.0 * 6.263200759887695
Epoch 1640, val loss: 1.164158821105957
Epoch 1650, training loss: 6.266177177429199 = 0.005632914137095213 + 1.0 * 6.260544300079346
Epoch 1650, val loss: 1.1666913032531738
Epoch 1660, training loss: 6.271335124969482 = 0.005557955242693424 + 1.0 * 6.265777111053467
Epoch 1660, val loss: 1.1692036390304565
Epoch 1670, training loss: 6.26818323135376 = 0.0054843672551214695 + 1.0 * 6.262698650360107
Epoch 1670, val loss: 1.1715972423553467
Epoch 1680, training loss: 6.266387462615967 = 0.0054129851050674915 + 1.0 * 6.260974407196045
Epoch 1680, val loss: 1.174045205116272
Epoch 1690, training loss: 6.266139030456543 = 0.00534350099042058 + 1.0 * 6.260795593261719
Epoch 1690, val loss: 1.176467776298523
Epoch 1700, training loss: 6.265013217926025 = 0.005275357980281115 + 1.0 * 6.259737968444824
Epoch 1700, val loss: 1.1788736581802368
Epoch 1710, training loss: 6.26646089553833 = 0.0052082170732319355 + 1.0 * 6.2612528800964355
Epoch 1710, val loss: 1.1812704801559448
Epoch 1720, training loss: 6.272546291351318 = 0.00514287268742919 + 1.0 * 6.267403602600098
Epoch 1720, val loss: 1.1836332082748413
Epoch 1730, training loss: 6.265024662017822 = 0.005078830290585756 + 1.0 * 6.259945869445801
Epoch 1730, val loss: 1.1858971118927002
Epoch 1740, training loss: 6.263472557067871 = 0.005016935057938099 + 1.0 * 6.258455753326416
Epoch 1740, val loss: 1.1882771253585815
Epoch 1750, training loss: 6.2703704833984375 = 0.0049563185311853886 + 1.0 * 6.265414237976074
Epoch 1750, val loss: 1.1906120777130127
Epoch 1760, training loss: 6.263091564178467 = 0.004896322265267372 + 1.0 * 6.258195400238037
Epoch 1760, val loss: 1.1927876472473145
Epoch 1770, training loss: 6.261067867279053 = 0.004838169552385807 + 1.0 * 6.256229877471924
Epoch 1770, val loss: 1.1951196193695068
Epoch 1780, training loss: 6.260975360870361 = 0.004781014751642942 + 1.0 * 6.256194114685059
Epoch 1780, val loss: 1.1974246501922607
Epoch 1790, training loss: 6.262424468994141 = 0.004724277649074793 + 1.0 * 6.257699966430664
Epoch 1790, val loss: 1.199696660041809
Epoch 1800, training loss: 6.268630504608154 = 0.004668697249144316 + 1.0 * 6.2639617919921875
Epoch 1800, val loss: 1.201904296875
Epoch 1810, training loss: 6.27310848236084 = 0.004614390432834625 + 1.0 * 6.268494129180908
Epoch 1810, val loss: 1.2040034532546997
Epoch 1820, training loss: 6.263331413269043 = 0.004563187714666128 + 1.0 * 6.258768081665039
Epoch 1820, val loss: 1.2062922716140747
Epoch 1830, training loss: 6.259835243225098 = 0.004511553794145584 + 1.0 * 6.255323886871338
Epoch 1830, val loss: 1.2084708213806152
Epoch 1840, training loss: 6.263084888458252 = 0.004461104981601238 + 1.0 * 6.2586236000061035
Epoch 1840, val loss: 1.2106924057006836
Epoch 1850, training loss: 6.262804985046387 = 0.004411345347762108 + 1.0 * 6.25839376449585
Epoch 1850, val loss: 1.2127926349639893
Epoch 1860, training loss: 6.259296894073486 = 0.004362125881016254 + 1.0 * 6.254934787750244
Epoch 1860, val loss: 1.2148901224136353
Epoch 1870, training loss: 6.259765625 = 0.004314532969146967 + 1.0 * 6.255451202392578
Epoch 1870, val loss: 1.2170815467834473
Epoch 1880, training loss: 6.263186454772949 = 0.004267708398401737 + 1.0 * 6.258918762207031
Epoch 1880, val loss: 1.2192001342773438
Epoch 1890, training loss: 6.260944843292236 = 0.004221640527248383 + 1.0 * 6.256723403930664
Epoch 1890, val loss: 1.2212971448898315
Epoch 1900, training loss: 6.257917404174805 = 0.004176427144557238 + 1.0 * 6.2537407875061035
Epoch 1900, val loss: 1.2233383655548096
Epoch 1910, training loss: 6.259101867675781 = 0.004131926689296961 + 1.0 * 6.254970073699951
Epoch 1910, val loss: 1.2254023551940918
Epoch 1920, training loss: 6.261147975921631 = 0.004088334273546934 + 1.0 * 6.257059574127197
Epoch 1920, val loss: 1.227454423904419
Epoch 1930, training loss: 6.259294509887695 = 0.004045638721436262 + 1.0 * 6.2552490234375
Epoch 1930, val loss: 1.2295054197311401
Epoch 1940, training loss: 6.2609028816223145 = 0.004003666341304779 + 1.0 * 6.256899356842041
Epoch 1940, val loss: 1.2314882278442383
Epoch 1950, training loss: 6.261186599731445 = 0.003963151015341282 + 1.0 * 6.257223606109619
Epoch 1950, val loss: 1.233460545539856
Epoch 1960, training loss: 6.257021427154541 = 0.003923140000551939 + 1.0 * 6.253098487854004
Epoch 1960, val loss: 1.2354369163513184
Epoch 1970, training loss: 6.255755424499512 = 0.003884025849401951 + 1.0 * 6.251871585845947
Epoch 1970, val loss: 1.2374582290649414
Epoch 1980, training loss: 6.255845069885254 = 0.003845097264274955 + 1.0 * 6.251999855041504
Epoch 1980, val loss: 1.2394376993179321
Epoch 1990, training loss: 6.26438570022583 = 0.0038064513355493546 + 1.0 * 6.2605791091918945
Epoch 1990, val loss: 1.241312861442566
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 10.534595489501953 = 1.937727451324463 + 1.0 * 8.596868515014648
Epoch 0, val loss: 1.9411896467208862
Epoch 10, training loss: 10.524833679199219 = 1.9281291961669922 + 1.0 * 8.596704483032227
Epoch 10, val loss: 1.931344747543335
Epoch 20, training loss: 10.511894226074219 = 1.9165618419647217 + 1.0 * 8.595332145690918
Epoch 20, val loss: 1.919281244277954
Epoch 30, training loss: 10.482786178588867 = 1.9007956981658936 + 1.0 * 8.581990242004395
Epoch 30, val loss: 1.9029114246368408
Epoch 40, training loss: 10.365439414978027 = 1.8794453144073486 + 1.0 * 8.485994338989258
Epoch 40, val loss: 1.8813128471374512
Epoch 50, training loss: 9.812796592712402 = 1.8554939031600952 + 1.0 * 7.957303047180176
Epoch 50, val loss: 1.8581706285476685
Epoch 60, training loss: 9.379556655883789 = 1.836370825767517 + 1.0 * 7.543185710906982
Epoch 60, val loss: 1.8402669429779053
Epoch 70, training loss: 9.013941764831543 = 1.8211995363235474 + 1.0 * 7.192741870880127
Epoch 70, val loss: 1.825075387954712
Epoch 80, training loss: 8.81301212310791 = 1.806779384613037 + 1.0 * 7.006232738494873
Epoch 80, val loss: 1.811169147491455
Epoch 90, training loss: 8.672525405883789 = 1.7917027473449707 + 1.0 * 6.880822658538818
Epoch 90, val loss: 1.7972166538238525
Epoch 100, training loss: 8.574472427368164 = 1.7771849632263184 + 1.0 * 6.797287464141846
Epoch 100, val loss: 1.7837538719177246
Epoch 110, training loss: 8.493293762207031 = 1.7630513906478882 + 1.0 * 6.7302422523498535
Epoch 110, val loss: 1.7701359987258911
Epoch 120, training loss: 8.429238319396973 = 1.7475959062576294 + 1.0 * 6.681642055511475
Epoch 120, val loss: 1.7555921077728271
Epoch 130, training loss: 8.375014305114746 = 1.7301610708236694 + 1.0 * 6.644853591918945
Epoch 130, val loss: 1.7398372888565063
Epoch 140, training loss: 8.32140064239502 = 1.7103317975997925 + 1.0 * 6.611069202423096
Epoch 140, val loss: 1.722395896911621
Epoch 150, training loss: 8.268939018249512 = 1.6875901222229004 + 1.0 * 6.581348896026611
Epoch 150, val loss: 1.7028412818908691
Epoch 160, training loss: 8.22146224975586 = 1.6610405445098877 + 1.0 * 6.560421943664551
Epoch 160, val loss: 1.680259108543396
Epoch 170, training loss: 8.168415069580078 = 1.630423665046692 + 1.0 * 6.537991523742676
Epoch 170, val loss: 1.6545886993408203
Epoch 180, training loss: 8.113219261169434 = 1.5950595140457153 + 1.0 * 6.51815938949585
Epoch 180, val loss: 1.6251667737960815
Epoch 190, training loss: 8.056294441223145 = 1.5544975996017456 + 1.0 * 6.501797199249268
Epoch 190, val loss: 1.5914299488067627
Epoch 200, training loss: 7.996761322021484 = 1.5085539817810059 + 1.0 * 6.4882073402404785
Epoch 200, val loss: 1.5534148216247559
Epoch 210, training loss: 7.935460567474365 = 1.457711100578308 + 1.0 * 6.477749347686768
Epoch 210, val loss: 1.5114954710006714
Epoch 220, training loss: 7.8695783615112305 = 1.4022338390350342 + 1.0 * 6.467344760894775
Epoch 220, val loss: 1.4660906791687012
Epoch 230, training loss: 7.801284313201904 = 1.3428397178649902 + 1.0 * 6.458444595336914
Epoch 230, val loss: 1.4179977178573608
Epoch 240, training loss: 7.739337921142578 = 1.2811017036437988 + 1.0 * 6.458236217498779
Epoch 240, val loss: 1.368922472000122
Epoch 250, training loss: 7.664795875549316 = 1.2198443412780762 + 1.0 * 6.44495153427124
Epoch 250, val loss: 1.3212004899978638
Epoch 260, training loss: 7.598029613494873 = 1.159590721130371 + 1.0 * 6.438438892364502
Epoch 260, val loss: 1.275331735610962
Epoch 270, training loss: 7.533046245574951 = 1.1006864309310913 + 1.0 * 6.43235969543457
Epoch 270, val loss: 1.2314696311950684
Epoch 280, training loss: 7.474116325378418 = 1.0435359477996826 + 1.0 * 6.430580139160156
Epoch 280, val loss: 1.189834475517273
Epoch 290, training loss: 7.414855480194092 = 0.9896098971366882 + 1.0 * 6.425245761871338
Epoch 290, val loss: 1.1514241695404053
Epoch 300, training loss: 7.357168674468994 = 0.938733696937561 + 1.0 * 6.418435096740723
Epoch 300, val loss: 1.1160972118377686
Epoch 310, training loss: 7.305202960968018 = 0.890604555606842 + 1.0 * 6.41459846496582
Epoch 310, val loss: 1.0833789110183716
Epoch 320, training loss: 7.262807369232178 = 0.845567524433136 + 1.0 * 6.417239665985107
Epoch 320, val loss: 1.053546667098999
Epoch 330, training loss: 7.213687896728516 = 0.8045727014541626 + 1.0 * 6.409115314483643
Epoch 330, val loss: 1.0273524522781372
Epoch 340, training loss: 7.171223163604736 = 0.7666627168655396 + 1.0 * 6.404560565948486
Epoch 340, val loss: 1.0043563842773438
Epoch 350, training loss: 7.131312370300293 = 0.7313244342803955 + 1.0 * 6.399988174438477
Epoch 350, val loss: 0.9838346242904663
Epoch 360, training loss: 7.094796657562256 = 0.6982503533363342 + 1.0 * 6.396546363830566
Epoch 360, val loss: 0.9657613039016724
Epoch 370, training loss: 7.060120582580566 = 0.667350709438324 + 1.0 * 6.392769813537598
Epoch 370, val loss: 0.9500008821487427
Epoch 380, training loss: 7.02827262878418 = 0.6385778188705444 + 1.0 * 6.389694690704346
Epoch 380, val loss: 0.9364737272262573
Epoch 390, training loss: 6.997740268707275 = 0.6112985610961914 + 1.0 * 6.386441707611084
Epoch 390, val loss: 0.9248558282852173
Epoch 400, training loss: 6.968900203704834 = 0.5851898193359375 + 1.0 * 6.3837103843688965
Epoch 400, val loss: 0.914713978767395
Epoch 410, training loss: 6.9566545486450195 = 0.5601212382316589 + 1.0 * 6.396533489227295
Epoch 410, val loss: 0.9058710336685181
Epoch 420, training loss: 6.916479110717773 = 0.5363413095474243 + 1.0 * 6.380137920379639
Epoch 420, val loss: 0.8983504772186279
Epoch 430, training loss: 6.890076637268066 = 0.5135586261749268 + 1.0 * 6.376518249511719
Epoch 430, val loss: 0.8921167850494385
Epoch 440, training loss: 6.865362644195557 = 0.4916040003299713 + 1.0 * 6.373758792877197
Epoch 440, val loss: 0.8869035840034485
Epoch 450, training loss: 6.841588020324707 = 0.4704640209674835 + 1.0 * 6.371123790740967
Epoch 450, val loss: 0.8826792240142822
Epoch 460, training loss: 6.818944454193115 = 0.4501243829727173 + 1.0 * 6.3688201904296875
Epoch 460, val loss: 0.8793050050735474
Epoch 470, training loss: 6.796474933624268 = 0.43051740527153015 + 1.0 * 6.365957736968994
Epoch 470, val loss: 0.8768371939659119
Epoch 480, training loss: 6.779820442199707 = 0.411482036113739 + 1.0 * 6.368338584899902
Epoch 480, val loss: 0.8750690817832947
Epoch 490, training loss: 6.754090785980225 = 0.3931366503238678 + 1.0 * 6.360954284667969
Epoch 490, val loss: 0.8738896250724792
Epoch 500, training loss: 6.73525333404541 = 0.3753127157688141 + 1.0 * 6.359940528869629
Epoch 500, val loss: 0.8733401894569397
Epoch 510, training loss: 6.721137523651123 = 0.358023077249527 + 1.0 * 6.363114356994629
Epoch 510, val loss: 0.8733624815940857
Epoch 520, training loss: 6.697302341461182 = 0.341324120759964 + 1.0 * 6.355978012084961
Epoch 520, val loss: 0.8738111257553101
Epoch 530, training loss: 6.676167964935303 = 0.32506507635116577 + 1.0 * 6.351102828979492
Epoch 530, val loss: 0.8747924566268921
Epoch 540, training loss: 6.661380767822266 = 0.3092253804206848 + 1.0 * 6.3521552085876465
Epoch 540, val loss: 0.8761773109436035
Epoch 550, training loss: 6.649451732635498 = 0.2939082086086273 + 1.0 * 6.355543613433838
Epoch 550, val loss: 0.8778466582298279
Epoch 560, training loss: 6.630610466003418 = 0.27917206287384033 + 1.0 * 6.351438522338867
Epoch 560, val loss: 0.8798897862434387
Epoch 570, training loss: 6.6120734214782715 = 0.26501479744911194 + 1.0 * 6.3470587730407715
Epoch 570, val loss: 0.8822212219238281
Epoch 580, training loss: 6.595824718475342 = 0.2514514625072479 + 1.0 * 6.3443732261657715
Epoch 580, val loss: 0.8848353028297424
Epoch 590, training loss: 6.578395843505859 = 0.23834346234798431 + 1.0 * 6.340052604675293
Epoch 590, val loss: 0.8878812193870544
Epoch 600, training loss: 6.563351154327393 = 0.22572344541549683 + 1.0 * 6.33762788772583
Epoch 600, val loss: 0.8911929130554199
Epoch 610, training loss: 6.5591535568237305 = 0.21360045671463013 + 1.0 * 6.345552921295166
Epoch 610, val loss: 0.8947839736938477
Epoch 620, training loss: 6.541723251342773 = 0.20208735764026642 + 1.0 * 6.339635848999023
Epoch 620, val loss: 0.8985490202903748
Epoch 630, training loss: 6.527364253997803 = 0.19120438396930695 + 1.0 * 6.336159706115723
Epoch 630, val loss: 0.9024417400360107
Epoch 640, training loss: 6.5181427001953125 = 0.18084825575351715 + 1.0 * 6.337294578552246
Epoch 640, val loss: 0.9066290259361267
Epoch 650, training loss: 6.5062456130981445 = 0.17108286917209625 + 1.0 * 6.33516263961792
Epoch 650, val loss: 0.9109042882919312
Epoch 660, training loss: 6.492074966430664 = 0.16184036433696747 + 1.0 * 6.330234527587891
Epoch 660, val loss: 0.9153687953948975
Epoch 670, training loss: 6.481600284576416 = 0.15308572351932526 + 1.0 * 6.328514575958252
Epoch 670, val loss: 0.9200814366340637
Epoch 680, training loss: 6.475895404815674 = 0.1448226273059845 + 1.0 * 6.331072807312012
Epoch 680, val loss: 0.9249569177627563
Epoch 690, training loss: 6.469175815582275 = 0.13704808056354523 + 1.0 * 6.332127571105957
Epoch 690, val loss: 0.9298830628395081
Epoch 700, training loss: 6.453591823577881 = 0.12972740828990936 + 1.0 * 6.323864459991455
Epoch 700, val loss: 0.9349672198295593
Epoch 710, training loss: 6.447723388671875 = 0.12282226979732513 + 1.0 * 6.324901103973389
Epoch 710, val loss: 0.9401609897613525
Epoch 720, training loss: 6.439756393432617 = 0.11630979180335999 + 1.0 * 6.323446750640869
Epoch 720, val loss: 0.9454495906829834
Epoch 730, training loss: 6.435214519500732 = 0.11016488820314407 + 1.0 * 6.32504940032959
Epoch 730, val loss: 0.9507765173912048
Epoch 740, training loss: 6.427215099334717 = 0.10439446568489075 + 1.0 * 6.322820663452148
Epoch 740, val loss: 0.9560975432395935
Epoch 750, training loss: 6.418793678283691 = 0.09896735846996307 + 1.0 * 6.319826126098633
Epoch 750, val loss: 0.9615586996078491
Epoch 760, training loss: 6.412319660186768 = 0.0938686653971672 + 1.0 * 6.318450927734375
Epoch 760, val loss: 0.9670255184173584
Epoch 770, training loss: 6.4056901931762695 = 0.08906038850545883 + 1.0 * 6.316629886627197
Epoch 770, val loss: 0.9726055860519409
Epoch 780, training loss: 6.4038166999816895 = 0.08453251421451569 + 1.0 * 6.319283962249756
Epoch 780, val loss: 0.9782878756523132
Epoch 790, training loss: 6.399078369140625 = 0.08029352128505707 + 1.0 * 6.318784713745117
Epoch 790, val loss: 0.9838528037071228
Epoch 800, training loss: 6.389097690582275 = 0.07632347196340561 + 1.0 * 6.312774181365967
Epoch 800, val loss: 0.9894948601722717
Epoch 810, training loss: 6.385324954986572 = 0.07258784770965576 + 1.0 * 6.312736988067627
Epoch 810, val loss: 0.9952524900436401
Epoch 820, training loss: 6.386165618896484 = 0.06907530874013901 + 1.0 * 6.3170905113220215
Epoch 820, val loss: 1.0010045766830444
Epoch 830, training loss: 6.376565933227539 = 0.06577986478805542 + 1.0 * 6.310786247253418
Epoch 830, val loss: 1.0067349672317505
Epoch 840, training loss: 6.372701644897461 = 0.06269168853759766 + 1.0 * 6.310009956359863
Epoch 840, val loss: 1.0124282836914062
Epoch 850, training loss: 6.367393970489502 = 0.05979454889893532 + 1.0 * 6.3075995445251465
Epoch 850, val loss: 1.018157720565796
Epoch 860, training loss: 6.364765644073486 = 0.057065170258283615 + 1.0 * 6.3077006340026855
Epoch 860, val loss: 1.0239242315292358
Epoch 870, training loss: 6.366081237792969 = 0.05450275540351868 + 1.0 * 6.311578273773193
Epoch 870, val loss: 1.0296813249588013
Epoch 880, training loss: 6.369211196899414 = 0.05209961533546448 + 1.0 * 6.317111492156982
Epoch 880, val loss: 1.035331130027771
Epoch 890, training loss: 6.354362487792969 = 0.04984645918011665 + 1.0 * 6.304515838623047
Epoch 890, val loss: 1.0409139394760132
Epoch 900, training loss: 6.350330352783203 = 0.04773174598813057 + 1.0 * 6.302598476409912
Epoch 900, val loss: 1.0464550256729126
Epoch 910, training loss: 6.35263204574585 = 0.04573773592710495 + 1.0 * 6.306894302368164
Epoch 910, val loss: 1.05208420753479
Epoch 920, training loss: 6.347609996795654 = 0.04385291039943695 + 1.0 * 6.303757190704346
Epoch 920, val loss: 1.0576518774032593
Epoch 930, training loss: 6.341305732727051 = 0.04208380728960037 + 1.0 * 6.299221992492676
Epoch 930, val loss: 1.0630854368209839
Epoch 940, training loss: 6.341010093688965 = 0.04041075333952904 + 1.0 * 6.300599575042725
Epoch 940, val loss: 1.068561315536499
Epoch 950, training loss: 6.341182708740234 = 0.038832567632198334 + 1.0 * 6.302350044250488
Epoch 950, val loss: 1.074022889137268
Epoch 960, training loss: 6.334751605987549 = 0.037344250828027725 + 1.0 * 6.297407150268555
Epoch 960, val loss: 1.0793051719665527
Epoch 970, training loss: 6.340586185455322 = 0.03593777120113373 + 1.0 * 6.304648399353027
Epoch 970, val loss: 1.0845845937728882
Epoch 980, training loss: 6.3299360275268555 = 0.0346061997115612 + 1.0 * 6.295330047607422
Epoch 980, val loss: 1.0898252725601196
Epoch 990, training loss: 6.327772617340088 = 0.03334752097725868 + 1.0 * 6.294425010681152
Epoch 990, val loss: 1.0949753522872925
Epoch 1000, training loss: 6.327239513397217 = 0.032151270657777786 + 1.0 * 6.295088291168213
Epoch 1000, val loss: 1.1001530885696411
Epoch 1010, training loss: 6.335414409637451 = 0.031015293672680855 + 1.0 * 6.304399013519287
Epoch 1010, val loss: 1.1053457260131836
Epoch 1020, training loss: 6.324197769165039 = 0.029945598915219307 + 1.0 * 6.294252395629883
Epoch 1020, val loss: 1.1103050708770752
Epoch 1030, training loss: 6.320766925811768 = 0.02892514318227768 + 1.0 * 6.291841983795166
Epoch 1030, val loss: 1.1152839660644531
Epoch 1040, training loss: 6.319041728973389 = 0.027953095734119415 + 1.0 * 6.291088581085205
Epoch 1040, val loss: 1.1202718019485474
Epoch 1050, training loss: 6.332060813903809 = 0.027028733864426613 + 1.0 * 6.305032253265381
Epoch 1050, val loss: 1.1252628564834595
Epoch 1060, training loss: 6.322703838348389 = 0.026151949539780617 + 1.0 * 6.296551704406738
Epoch 1060, val loss: 1.1300759315490723
Epoch 1070, training loss: 6.315997123718262 = 0.025318730622529984 + 1.0 * 6.29067850112915
Epoch 1070, val loss: 1.1347764730453491
Epoch 1080, training loss: 6.31856632232666 = 0.02452455833554268 + 1.0 * 6.294041633605957
Epoch 1080, val loss: 1.1394928693771362
Epoch 1090, training loss: 6.31160831451416 = 0.023766595870256424 + 1.0 * 6.287841796875
Epoch 1090, val loss: 1.1442241668701172
Epoch 1100, training loss: 6.311900615692139 = 0.023041877895593643 + 1.0 * 6.288858890533447
Epoch 1100, val loss: 1.1489065885543823
Epoch 1110, training loss: 6.32429838180542 = 0.02234889380633831 + 1.0 * 6.301949501037598
Epoch 1110, val loss: 1.1535658836364746
Epoch 1120, training loss: 6.313643932342529 = 0.021693658083677292 + 1.0 * 6.291950225830078
Epoch 1120, val loss: 1.1580532789230347
Epoch 1130, training loss: 6.308811187744141 = 0.021063892170786858 + 1.0 * 6.287747383117676
Epoch 1130, val loss: 1.1624847650527954
Epoch 1140, training loss: 6.305795669555664 = 0.02046220935881138 + 1.0 * 6.285333633422852
Epoch 1140, val loss: 1.1669507026672363
Epoch 1150, training loss: 6.309742450714111 = 0.01988360658288002 + 1.0 * 6.289858818054199
Epoch 1150, val loss: 1.1714224815368652
Epoch 1160, training loss: 6.306497573852539 = 0.019330447539687157 + 1.0 * 6.287167072296143
Epoch 1160, val loss: 1.1758103370666504
Epoch 1170, training loss: 6.307772636413574 = 0.01880292035639286 + 1.0 * 6.28896951675415
Epoch 1170, val loss: 1.18001127243042
Epoch 1180, training loss: 6.301984786987305 = 0.01829621009528637 + 1.0 * 6.283688545227051
Epoch 1180, val loss: 1.1842494010925293
Epoch 1190, training loss: 6.299805641174316 = 0.01780983991920948 + 1.0 * 6.28199577331543
Epoch 1190, val loss: 1.1884818077087402
Epoch 1200, training loss: 6.300180435180664 = 0.017341146245598793 + 1.0 * 6.282839298248291
Epoch 1200, val loss: 1.1926952600479126
Epoch 1210, training loss: 6.311124801635742 = 0.016890455037355423 + 1.0 * 6.294234275817871
Epoch 1210, val loss: 1.1968504190444946
Epoch 1220, training loss: 6.2985124588012695 = 0.01646282523870468 + 1.0 * 6.282049655914307
Epoch 1220, val loss: 1.2008525133132935
Epoch 1230, training loss: 6.296500205993652 = 0.016050582751631737 + 1.0 * 6.280449390411377
Epoch 1230, val loss: 1.2047770023345947
Epoch 1240, training loss: 6.298638820648193 = 0.015653744339942932 + 1.0 * 6.282985210418701
Epoch 1240, val loss: 1.2087606191635132
Epoch 1250, training loss: 6.296075820922852 = 0.015271703712642193 + 1.0 * 6.28080415725708
Epoch 1250, val loss: 1.212733268737793
Epoch 1260, training loss: 6.29788875579834 = 0.01490510068833828 + 1.0 * 6.282983779907227
Epoch 1260, val loss: 1.2166005373001099
Epoch 1270, training loss: 6.29596471786499 = 0.01455035712569952 + 1.0 * 6.28141450881958
Epoch 1270, val loss: 1.2204158306121826
Epoch 1280, training loss: 6.292145729064941 = 0.014209977351129055 + 1.0 * 6.277935981750488
Epoch 1280, val loss: 1.2242053747177124
Epoch 1290, training loss: 6.295462131500244 = 0.013880089856684208 + 1.0 * 6.281581878662109
Epoch 1290, val loss: 1.228002667427063
Epoch 1300, training loss: 6.291819095611572 = 0.013562042266130447 + 1.0 * 6.278256893157959
Epoch 1300, val loss: 1.2317715883255005
Epoch 1310, training loss: 6.292778491973877 = 0.013256769627332687 + 1.0 * 6.279521942138672
Epoch 1310, val loss: 1.2354323863983154
Epoch 1320, training loss: 6.295007705688477 = 0.012962579727172852 + 1.0 * 6.282044887542725
Epoch 1320, val loss: 1.2390111684799194
Epoch 1330, training loss: 6.288500785827637 = 0.012679332867264748 + 1.0 * 6.275821685791016
Epoch 1330, val loss: 1.2425928115844727
Epoch 1340, training loss: 6.286733627319336 = 0.01240482646971941 + 1.0 * 6.274328708648682
Epoch 1340, val loss: 1.2461278438568115
Epoch 1350, training loss: 6.288022041320801 = 0.012138440273702145 + 1.0 * 6.275883674621582
Epoch 1350, val loss: 1.249694585800171
Epoch 1360, training loss: 6.295052528381348 = 0.011881407350301743 + 1.0 * 6.2831711769104
Epoch 1360, val loss: 1.2532142400741577
Epoch 1370, training loss: 6.292153358459473 = 0.0116340983659029 + 1.0 * 6.280519485473633
Epoch 1370, val loss: 1.2566208839416504
Epoch 1380, training loss: 6.285301208496094 = 0.011394853703677654 + 1.0 * 6.273906230926514
Epoch 1380, val loss: 1.2599382400512695
Epoch 1390, training loss: 6.283802032470703 = 0.011163483373820782 + 1.0 * 6.272638320922852
Epoch 1390, val loss: 1.2632908821105957
Epoch 1400, training loss: 6.287261486053467 = 0.010937919840216637 + 1.0 * 6.2763237953186035
Epoch 1400, val loss: 1.2666860818862915
Epoch 1410, training loss: 6.285769462585449 = 0.010720171965658665 + 1.0 * 6.275049209594727
Epoch 1410, val loss: 1.2700153589248657
Epoch 1420, training loss: 6.286309242248535 = 0.01051037572324276 + 1.0 * 6.275798797607422
Epoch 1420, val loss: 1.2732789516448975
Epoch 1430, training loss: 6.2845540046691895 = 0.010305872187018394 + 1.0 * 6.274248123168945
Epoch 1430, val loss: 1.2765451669692993
Epoch 1440, training loss: 6.280343055725098 = 0.01010778546333313 + 1.0 * 6.270235061645508
Epoch 1440, val loss: 1.2797986268997192
Epoch 1450, training loss: 6.287174224853516 = 0.009915713220834732 + 1.0 * 6.277258396148682
Epoch 1450, val loss: 1.283023476600647
Epoch 1460, training loss: 6.284430503845215 = 0.009729521349072456 + 1.0 * 6.274701118469238
Epoch 1460, val loss: 1.2861757278442383
Epoch 1470, training loss: 6.286728858947754 = 0.009549824520945549 + 1.0 * 6.27717924118042
Epoch 1470, val loss: 1.2893154621124268
Epoch 1480, training loss: 6.281736850738525 = 0.009375356137752533 + 1.0 * 6.2723612785339355
Epoch 1480, val loss: 1.2923531532287598
Epoch 1490, training loss: 6.28049373626709 = 0.0092061972245574 + 1.0 * 6.271287441253662
Epoch 1490, val loss: 1.295413613319397
Epoch 1500, training loss: 6.2802629470825195 = 0.009041745215654373 + 1.0 * 6.271221160888672
Epoch 1500, val loss: 1.298448085784912
Epoch 1510, training loss: 6.278351783752441 = 0.00888131745159626 + 1.0 * 6.269470691680908
Epoch 1510, val loss: 1.3014723062515259
Epoch 1520, training loss: 6.283376216888428 = 0.008725672028958797 + 1.0 * 6.274650573730469
Epoch 1520, val loss: 1.3044589757919312
Epoch 1530, training loss: 6.281044960021973 = 0.008575311861932278 + 1.0 * 6.272469520568848
Epoch 1530, val loss: 1.3073965311050415
Epoch 1540, training loss: 6.2797064781188965 = 0.008428612723946571 + 1.0 * 6.271277904510498
Epoch 1540, val loss: 1.3102777004241943
Epoch 1550, training loss: 6.279677391052246 = 0.008286911062896252 + 1.0 * 6.271390438079834
Epoch 1550, val loss: 1.3131358623504639
Epoch 1560, training loss: 6.277990818023682 = 0.008148624561727047 + 1.0 * 6.269842147827148
Epoch 1560, val loss: 1.3159695863723755
Epoch 1570, training loss: 6.274936676025391 = 0.008014135994017124 + 1.0 * 6.266922473907471
Epoch 1570, val loss: 1.3187907934188843
Epoch 1580, training loss: 6.274038314819336 = 0.00788260716944933 + 1.0 * 6.26615571975708
Epoch 1580, val loss: 1.321617841720581
Epoch 1590, training loss: 6.2827467918396 = 0.007754306308925152 + 1.0 * 6.2749924659729
Epoch 1590, val loss: 1.3244543075561523
Epoch 1600, training loss: 6.275645732879639 = 0.007630047854036093 + 1.0 * 6.2680158615112305
Epoch 1600, val loss: 1.3271870613098145
Epoch 1610, training loss: 6.275866985321045 = 0.007509093265980482 + 1.0 * 6.268357753753662
Epoch 1610, val loss: 1.3298747539520264
Epoch 1620, training loss: 6.277599811553955 = 0.0073918611742556095 + 1.0 * 6.27020788192749
Epoch 1620, val loss: 1.3325344324111938
Epoch 1630, training loss: 6.27183198928833 = 0.007277762051671743 + 1.0 * 6.264554023742676
Epoch 1630, val loss: 1.3351693153381348
Epoch 1640, training loss: 6.273143768310547 = 0.0071661826223134995 + 1.0 * 6.265977382659912
Epoch 1640, val loss: 1.3378195762634277
Epoch 1650, training loss: 6.27506160736084 = 0.00705670565366745 + 1.0 * 6.268004894256592
Epoch 1650, val loss: 1.3405128717422485
Epoch 1660, training loss: 6.273981094360352 = 0.006949825212359428 + 1.0 * 6.267031192779541
Epoch 1660, val loss: 1.3431669473648071
Epoch 1670, training loss: 6.273184776306152 = 0.006846047006547451 + 1.0 * 6.26633882522583
Epoch 1670, val loss: 1.345726490020752
Epoch 1680, training loss: 6.274538040161133 = 0.0067453146912157536 + 1.0 * 6.267792701721191
Epoch 1680, val loss: 1.348265290260315
Epoch 1690, training loss: 6.2689032554626465 = 0.006646816153079271 + 1.0 * 6.262256622314453
Epoch 1690, val loss: 1.3507899045944214
Epoch 1700, training loss: 6.271526336669922 = 0.006550747435539961 + 1.0 * 6.264975547790527
Epoch 1700, val loss: 1.3532795906066895
Epoch 1710, training loss: 6.275086402893066 = 0.0064567322842776775 + 1.0 * 6.268629550933838
Epoch 1710, val loss: 1.3557826280593872
Epoch 1720, training loss: 6.270228862762451 = 0.006365201435983181 + 1.0 * 6.263863563537598
Epoch 1720, val loss: 1.3582321405410767
Epoch 1730, training loss: 6.271689414978027 = 0.006275785155594349 + 1.0 * 6.265413761138916
Epoch 1730, val loss: 1.3606590032577515
Epoch 1740, training loss: 6.267175674438477 = 0.006188619881868362 + 1.0 * 6.260987281799316
Epoch 1740, val loss: 1.3630708456039429
Epoch 1750, training loss: 6.274017810821533 = 0.006103576626628637 + 1.0 * 6.267914295196533
Epoch 1750, val loss: 1.365463376045227
Epoch 1760, training loss: 6.267123699188232 = 0.006019917782396078 + 1.0 * 6.261103630065918
Epoch 1760, val loss: 1.3678165674209595
Epoch 1770, training loss: 6.267331123352051 = 0.00593867152929306 + 1.0 * 6.261392593383789
Epoch 1770, val loss: 1.3701280355453491
Epoch 1780, training loss: 6.27403450012207 = 0.00585930934175849 + 1.0 * 6.26817512512207
Epoch 1780, val loss: 1.3724478483200073
Epoch 1790, training loss: 6.266795635223389 = 0.0057815806940197945 + 1.0 * 6.261013984680176
Epoch 1790, val loss: 1.3747202157974243
Epoch 1800, training loss: 6.265739440917969 = 0.005705780815333128 + 1.0 * 6.26003360748291
Epoch 1800, val loss: 1.3769547939300537
Epoch 1810, training loss: 6.265841960906982 = 0.005631425883620977 + 1.0 * 6.2602105140686035
Epoch 1810, val loss: 1.3791838884353638
Epoch 1820, training loss: 6.272276878356934 = 0.005558551289141178 + 1.0 * 6.26671838760376
Epoch 1820, val loss: 1.381414771080017
Epoch 1830, training loss: 6.26743745803833 = 0.005487293470650911 + 1.0 * 6.2619500160217285
Epoch 1830, val loss: 1.383617877960205
Epoch 1840, training loss: 6.266085624694824 = 0.005417695268988609 + 1.0 * 6.26066780090332
Epoch 1840, val loss: 1.3857581615447998
Epoch 1850, training loss: 6.267858505249023 = 0.005349841900169849 + 1.0 * 6.262508869171143
Epoch 1850, val loss: 1.387893557548523
Epoch 1860, training loss: 6.267945289611816 = 0.005283788312226534 + 1.0 * 6.262661457061768
Epoch 1860, val loss: 1.389991283416748
Epoch 1870, training loss: 6.265191555023193 = 0.005218588747084141 + 1.0 * 6.259973049163818
Epoch 1870, val loss: 1.3920607566833496
Epoch 1880, training loss: 6.265020847320557 = 0.005155143793672323 + 1.0 * 6.259865760803223
Epoch 1880, val loss: 1.3941110372543335
Epoch 1890, training loss: 6.268383502960205 = 0.005093013402074575 + 1.0 * 6.2632904052734375
Epoch 1890, val loss: 1.3961488008499146
Epoch 1900, training loss: 6.261710166931152 = 0.0050317952409386635 + 1.0 * 6.256678581237793
Epoch 1900, val loss: 1.3981692790985107
Epoch 1910, training loss: 6.2621541023254395 = 0.004971879534423351 + 1.0 * 6.2571821212768555
Epoch 1910, val loss: 1.400175929069519
Epoch 1920, training loss: 6.26329231262207 = 0.004912833217531443 + 1.0 * 6.2583794593811035
Epoch 1920, val loss: 1.4022037982940674
Epoch 1930, training loss: 6.269751071929932 = 0.004854999016970396 + 1.0 * 6.264895915985107
Epoch 1930, val loss: 1.4042271375656128
Epoch 1940, training loss: 6.263942718505859 = 0.004798987880349159 + 1.0 * 6.259143829345703
Epoch 1940, val loss: 1.4061558246612549
Epoch 1950, training loss: 6.260951519012451 = 0.0047439332120120525 + 1.0 * 6.256207466125488
Epoch 1950, val loss: 1.4080407619476318
Epoch 1960, training loss: 6.262965202331543 = 0.004689928609877825 + 1.0 * 6.258275508880615
Epoch 1960, val loss: 1.409934401512146
Epoch 1970, training loss: 6.265106678009033 = 0.004637024365365505 + 1.0 * 6.260469436645508
Epoch 1970, val loss: 1.4118276834487915
Epoch 1980, training loss: 6.261300086975098 = 0.004585362505167723 + 1.0 * 6.256714820861816
Epoch 1980, val loss: 1.4136981964111328
Epoch 1990, training loss: 6.259336948394775 = 0.004535065032541752 + 1.0 * 6.2548017501831055
Epoch 1990, val loss: 1.4154753684997559
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8255139694254086
The final CL Acc:0.75679, 0.01666, The final GNN Acc:0.81831, 0.00510
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13144])
remove edge: torch.Size([2, 7872])
updated graph: torch.Size([2, 10460])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.547719955444336 = 1.9508514404296875 + 1.0 * 8.596868515014648
Epoch 0, val loss: 1.9440736770629883
Epoch 10, training loss: 10.537668228149414 = 1.9409728050231934 + 1.0 * 8.596694946289062
Epoch 10, val loss: 1.9341731071472168
Epoch 20, training loss: 10.523892402648926 = 1.9287102222442627 + 1.0 * 8.595182418823242
Epoch 20, val loss: 1.9213708639144897
Epoch 30, training loss: 10.49337387084961 = 1.9114285707473755 + 1.0 * 8.581945419311523
Epoch 30, val loss: 1.9029366970062256
Epoch 40, training loss: 10.391107559204102 = 1.8870798349380493 + 1.0 * 8.504027366638184
Epoch 40, val loss: 1.8777409791946411
Epoch 50, training loss: 9.97918701171875 = 1.8592215776443481 + 1.0 * 8.119965553283691
Epoch 50, val loss: 1.8507702350616455
Epoch 60, training loss: 9.650666236877441 = 1.831679105758667 + 1.0 * 7.8189873695373535
Epoch 60, val loss: 1.8260577917099
Epoch 70, training loss: 9.262516975402832 = 1.809948205947876 + 1.0 * 7.452569007873535
Epoch 70, val loss: 1.8075703382492065
Epoch 80, training loss: 8.956369400024414 = 1.7911396026611328 + 1.0 * 7.165229320526123
Epoch 80, val loss: 1.791034460067749
Epoch 90, training loss: 8.754908561706543 = 1.7724826335906982 + 1.0 * 6.982426166534424
Epoch 90, val loss: 1.773916244506836
Epoch 100, training loss: 8.647212028503418 = 1.7520514726638794 + 1.0 * 6.895160675048828
Epoch 100, val loss: 1.7558172941207886
Epoch 110, training loss: 8.555291175842285 = 1.729561686515808 + 1.0 * 6.825729846954346
Epoch 110, val loss: 1.7374769449234009
Epoch 120, training loss: 8.455680847167969 = 1.7053635120391846 + 1.0 * 6.750317096710205
Epoch 120, val loss: 1.7182010412216187
Epoch 130, training loss: 8.373826026916504 = 1.6790028810501099 + 1.0 * 6.694822788238525
Epoch 130, val loss: 1.6962400674819946
Epoch 140, training loss: 8.303487777709961 = 1.6482595205307007 + 1.0 * 6.655228614807129
Epoch 140, val loss: 1.670044183731079
Epoch 150, training loss: 8.239219665527344 = 1.6116160154342651 + 1.0 * 6.627603530883789
Epoch 150, val loss: 1.6388933658599854
Epoch 160, training loss: 8.17540454864502 = 1.5696452856063843 + 1.0 * 6.605759143829346
Epoch 160, val loss: 1.6037591695785522
Epoch 170, training loss: 8.107597351074219 = 1.5226558446884155 + 1.0 * 6.584941387176514
Epoch 170, val loss: 1.5649163722991943
Epoch 180, training loss: 8.04039478302002 = 1.4710147380828857 + 1.0 * 6.569380283355713
Epoch 180, val loss: 1.523132562637329
Epoch 190, training loss: 7.964200019836426 = 1.4168040752410889 + 1.0 * 6.547395706176758
Epoch 190, val loss: 1.4802088737487793
Epoch 200, training loss: 7.890663146972656 = 1.360947847366333 + 1.0 * 6.529715061187744
Epoch 200, val loss: 1.4369369745254517
Epoch 210, training loss: 7.822257041931152 = 1.3053287267684937 + 1.0 * 6.516928195953369
Epoch 210, val loss: 1.3950750827789307
Epoch 220, training loss: 7.753442287445068 = 1.251076579093933 + 1.0 * 6.502365589141846
Epoch 220, val loss: 1.3549726009368896
Epoch 230, training loss: 7.687849044799805 = 1.1978914737701416 + 1.0 * 6.489957332611084
Epoch 230, val loss: 1.3164708614349365
Epoch 240, training loss: 7.631004333496094 = 1.146571397781372 + 1.0 * 6.484433174133301
Epoch 240, val loss: 1.280067801475525
Epoch 250, training loss: 7.567401885986328 = 1.0984654426574707 + 1.0 * 6.468936443328857
Epoch 250, val loss: 1.246356725692749
Epoch 260, training loss: 7.513576984405518 = 1.0528717041015625 + 1.0 * 6.460705280303955
Epoch 260, val loss: 1.2148818969726562
Epoch 270, training loss: 7.461025714874268 = 1.009717583656311 + 1.0 * 6.451308250427246
Epoch 270, val loss: 1.1851229667663574
Epoch 280, training loss: 7.419765472412109 = 0.9693050980567932 + 1.0 * 6.450460433959961
Epoch 280, val loss: 1.1571513414382935
Epoch 290, training loss: 7.370363712310791 = 0.9317390322685242 + 1.0 * 6.438624858856201
Epoch 290, val loss: 1.1307798624038696
Epoch 300, training loss: 7.327054023742676 = 0.8958704471588135 + 1.0 * 6.431183338165283
Epoch 300, val loss: 1.1053860187530518
Epoch 310, training loss: 7.285511493682861 = 0.8610976338386536 + 1.0 * 6.424413681030273
Epoch 310, val loss: 1.0803484916687012
Epoch 320, training loss: 7.250288486480713 = 0.8272119164466858 + 1.0 * 6.423076629638672
Epoch 320, val loss: 1.0555744171142578
Epoch 330, training loss: 7.209256172180176 = 0.7945678234100342 + 1.0 * 6.414688587188721
Epoch 330, val loss: 1.0314666032791138
Epoch 340, training loss: 7.173501968383789 = 0.7631193399429321 + 1.0 * 6.4103827476501465
Epoch 340, val loss: 1.0082334280014038
Epoch 350, training loss: 7.13721227645874 = 0.7325538992881775 + 1.0 * 6.404658317565918
Epoch 350, val loss: 0.9856499433517456
Epoch 360, training loss: 7.10411262512207 = 0.7028058767318726 + 1.0 * 6.401306629180908
Epoch 360, val loss: 0.9638132452964783
Epoch 370, training loss: 7.0776777267456055 = 0.6741316318511963 + 1.0 * 6.40354585647583
Epoch 370, val loss: 0.9429607391357422
Epoch 380, training loss: 7.040577411651611 = 0.6466188430786133 + 1.0 * 6.393958568572998
Epoch 380, val loss: 0.9233124852180481
Epoch 390, training loss: 7.01010274887085 = 0.6199812889099121 + 1.0 * 6.3901214599609375
Epoch 390, val loss: 0.9046915173530579
Epoch 400, training loss: 6.989431381225586 = 0.5942873954772949 + 1.0 * 6.395143985748291
Epoch 400, val loss: 0.8871859908103943
Epoch 410, training loss: 6.953813076019287 = 0.5696272253990173 + 1.0 * 6.384185791015625
Epoch 410, val loss: 0.8709186315536499
Epoch 420, training loss: 6.926170349121094 = 0.5457232594490051 + 1.0 * 6.380446910858154
Epoch 420, val loss: 0.8556790351867676
Epoch 430, training loss: 6.8996124267578125 = 0.5223838686943054 + 1.0 * 6.377228736877441
Epoch 430, val loss: 0.8413189053535461
Epoch 440, training loss: 6.88662576675415 = 0.4995425045490265 + 1.0 * 6.387083053588867
Epoch 440, val loss: 0.827834963798523
Epoch 450, training loss: 6.850039958953857 = 0.4774186909198761 + 1.0 * 6.372621059417725
Epoch 450, val loss: 0.8153125047683716
Epoch 460, training loss: 6.825732707977295 = 0.4556730389595032 + 1.0 * 6.370059490203857
Epoch 460, val loss: 0.8036851286888123
Epoch 470, training loss: 6.809916019439697 = 0.4341801702976227 + 1.0 * 6.375735759735107
Epoch 470, val loss: 0.792823314666748
Epoch 480, training loss: 6.778430938720703 = 0.41301220655441284 + 1.0 * 6.365418910980225
Epoch 480, val loss: 0.7826853394508362
Epoch 490, training loss: 6.754721641540527 = 0.3921011984348297 + 1.0 * 6.3626203536987305
Epoch 490, val loss: 0.7732998132705688
Epoch 500, training loss: 6.738707065582275 = 0.37140512466430664 + 1.0 * 6.367301940917969
Epoch 500, val loss: 0.7646666169166565
Epoch 510, training loss: 6.711564064025879 = 0.35121792554855347 + 1.0 * 6.36034631729126
Epoch 510, val loss: 0.7568438649177551
Epoch 520, training loss: 6.687722682952881 = 0.33160555362701416 + 1.0 * 6.356117248535156
Epoch 520, val loss: 0.749911367893219
Epoch 530, training loss: 6.665683269500732 = 0.31258153915405273 + 1.0 * 6.35310173034668
Epoch 530, val loss: 0.743935227394104
Epoch 540, training loss: 6.645543575286865 = 0.2942279279232025 + 1.0 * 6.351315498352051
Epoch 540, val loss: 0.7389419078826904
Epoch 550, training loss: 6.642829418182373 = 0.27673813700675964 + 1.0 * 6.366091251373291
Epoch 550, val loss: 0.7349350452423096
Epoch 560, training loss: 6.608332633972168 = 0.26033157110214233 + 1.0 * 6.348001003265381
Epoch 560, val loss: 0.7318886518478394
Epoch 570, training loss: 6.591076374053955 = 0.2449187934398651 + 1.0 * 6.346157550811768
Epoch 570, val loss: 0.7299095392227173
Epoch 580, training loss: 6.574428081512451 = 0.23042717576026917 + 1.0 * 6.344000816345215
Epoch 580, val loss: 0.7289486527442932
Epoch 590, training loss: 6.5586466789245605 = 0.21677067875862122 + 1.0 * 6.341876029968262
Epoch 590, val loss: 0.7288686633110046
Epoch 600, training loss: 6.547312259674072 = 0.2039821743965149 + 1.0 * 6.343329906463623
Epoch 600, val loss: 0.7295265197753906
Epoch 610, training loss: 6.533934116363525 = 0.1921117752790451 + 1.0 * 6.341822147369385
Epoch 610, val loss: 0.7309466600418091
Epoch 620, training loss: 6.51988410949707 = 0.18100544810295105 + 1.0 * 6.338878631591797
Epoch 620, val loss: 0.7330845594406128
Epoch 630, training loss: 6.514251232147217 = 0.17060540616512299 + 1.0 * 6.343646049499512
Epoch 630, val loss: 0.7358297109603882
Epoch 640, training loss: 6.494380474090576 = 0.16092410683631897 + 1.0 * 6.333456516265869
Epoch 640, val loss: 0.7389607429504395
Epoch 650, training loss: 6.484647750854492 = 0.15183356404304504 + 1.0 * 6.3328142166137695
Epoch 650, val loss: 0.7426629662513733
Epoch 660, training loss: 6.474274635314941 = 0.14328067004680634 + 1.0 * 6.330994129180908
Epoch 660, val loss: 0.7468631863594055
Epoch 670, training loss: 6.46601676940918 = 0.13524793088436127 + 1.0 * 6.330769062042236
Epoch 670, val loss: 0.7514196634292603
Epoch 680, training loss: 6.462453365325928 = 0.12773185968399048 + 1.0 * 6.334721565246582
Epoch 680, val loss: 0.7563035488128662
Epoch 690, training loss: 6.448655128479004 = 0.12070617079734802 + 1.0 * 6.327949047088623
Epoch 690, val loss: 0.7613763213157654
Epoch 700, training loss: 6.439575672149658 = 0.1141241043806076 + 1.0 * 6.325451374053955
Epoch 700, val loss: 0.7667812705039978
Epoch 710, training loss: 6.434248447418213 = 0.1079370304942131 + 1.0 * 6.3263115882873535
Epoch 710, val loss: 0.7724449634552002
Epoch 720, training loss: 6.429770469665527 = 0.10212968289852142 + 1.0 * 6.327641010284424
Epoch 720, val loss: 0.7781877517700195
Epoch 730, training loss: 6.420111656188965 = 0.09672609716653824 + 1.0 * 6.323385715484619
Epoch 730, val loss: 0.7840171456336975
Epoch 740, training loss: 6.410440921783447 = 0.09165046364068985 + 1.0 * 6.318790435791016
Epoch 740, val loss: 0.7900088429450989
Epoch 750, training loss: 6.405280590057373 = 0.086880162358284 + 1.0 * 6.3184003829956055
Epoch 750, val loss: 0.7961682677268982
Epoch 760, training loss: 6.406830310821533 = 0.0823933407664299 + 1.0 * 6.324437141418457
Epoch 760, val loss: 0.8024463057518005
Epoch 770, training loss: 6.398065567016602 = 0.07820028066635132 + 1.0 * 6.3198652267456055
Epoch 770, val loss: 0.8086618185043335
Epoch 780, training loss: 6.392475128173828 = 0.07427172362804413 + 1.0 * 6.318203449249268
Epoch 780, val loss: 0.8149440884590149
Epoch 790, training loss: 6.388158321380615 = 0.07059089839458466 + 1.0 * 6.317567348480225
Epoch 790, val loss: 0.8213082551956177
Epoch 800, training loss: 6.379937648773193 = 0.06714367121458054 + 1.0 * 6.312794208526611
Epoch 800, val loss: 0.8276122808456421
Epoch 810, training loss: 6.377151012420654 = 0.06389814615249634 + 1.0 * 6.313252925872803
Epoch 810, val loss: 0.8340447545051575
Epoch 820, training loss: 6.380918502807617 = 0.06084831431508064 + 1.0 * 6.320070266723633
Epoch 820, val loss: 0.8404548764228821
Epoch 830, training loss: 6.370760440826416 = 0.057984888553619385 + 1.0 * 6.312775611877441
Epoch 830, val loss: 0.8467046618461609
Epoch 840, training loss: 6.366052150726318 = 0.055299222469329834 + 1.0 * 6.310752868652344
Epoch 840, val loss: 0.8530551195144653
Epoch 850, training loss: 6.36898946762085 = 0.052763618528842926 + 1.0 * 6.316226005554199
Epoch 850, val loss: 0.8594244122505188
Epoch 860, training loss: 6.359346866607666 = 0.05038733780384064 + 1.0 * 6.308959484100342
Epoch 860, val loss: 0.8656640648841858
Epoch 870, training loss: 6.3557353019714355 = 0.048150189220905304 + 1.0 * 6.3075852394104
Epoch 870, val loss: 0.8718879222869873
Epoch 880, training loss: 6.3584208488464355 = 0.04604312777519226 + 1.0 * 6.3123779296875
Epoch 880, val loss: 0.8780806064605713
Epoch 890, training loss: 6.349422454833984 = 0.04406752064824104 + 1.0 * 6.305355072021484
Epoch 890, val loss: 0.8841005563735962
Epoch 900, training loss: 6.345537185668945 = 0.04220319911837578 + 1.0 * 6.303333759307861
Epoch 900, val loss: 0.8901754021644592
Epoch 910, training loss: 6.344138145446777 = 0.04043811187148094 + 1.0 * 6.303699970245361
Epoch 910, val loss: 0.8962722420692444
Epoch 920, training loss: 6.345375061035156 = 0.038772255182266235 + 1.0 * 6.306602954864502
Epoch 920, val loss: 0.902221143245697
Epoch 930, training loss: 6.34346866607666 = 0.037203099578619 + 1.0 * 6.306265354156494
Epoch 930, val loss: 0.9080324172973633
Epoch 940, training loss: 6.337081432342529 = 0.03572986274957657 + 1.0 * 6.301351547241211
Epoch 940, val loss: 0.9137004613876343
Epoch 950, training loss: 6.333902359008789 = 0.034337934106588364 + 1.0 * 6.299564361572266
Epoch 950, val loss: 0.9193515777587891
Epoch 960, training loss: 6.33184289932251 = 0.03301604464650154 + 1.0 * 6.298826694488525
Epoch 960, val loss: 0.9250564575195312
Epoch 970, training loss: 6.329875946044922 = 0.03176233544945717 + 1.0 * 6.298113822937012
Epoch 970, val loss: 0.9307368397712708
Epoch 980, training loss: 6.331662178039551 = 0.03057120181620121 + 1.0 * 6.301091194152832
Epoch 980, val loss: 0.936339795589447
Epoch 990, training loss: 6.331058025360107 = 0.029442476108670235 + 1.0 * 6.3016157150268555
Epoch 990, val loss: 0.9418467283248901
Epoch 1000, training loss: 6.323841094970703 = 0.028378291055560112 + 1.0 * 6.295462608337402
Epoch 1000, val loss: 0.9471638798713684
Epoch 1010, training loss: 6.323817253112793 = 0.027369603514671326 + 1.0 * 6.29644775390625
Epoch 1010, val loss: 0.9524454474449158
Epoch 1020, training loss: 6.320432662963867 = 0.0264072734862566 + 1.0 * 6.294025421142578
Epoch 1020, val loss: 0.957753598690033
Epoch 1030, training loss: 6.319494724273682 = 0.025491269305348396 + 1.0 * 6.294003486633301
Epoch 1030, val loss: 0.9630365371704102
Epoch 1040, training loss: 6.32598352432251 = 0.02461945451796055 + 1.0 * 6.301363945007324
Epoch 1040, val loss: 0.96822589635849
Epoch 1050, training loss: 6.327748775482178 = 0.02379429154098034 + 1.0 * 6.303954601287842
Epoch 1050, val loss: 0.9732431173324585
Epoch 1060, training loss: 6.319261074066162 = 0.02301649935543537 + 1.0 * 6.2962446212768555
Epoch 1060, val loss: 0.9780733585357666
Epoch 1070, training loss: 6.3141021728515625 = 0.022271588444709778 + 1.0 * 6.291830539703369
Epoch 1070, val loss: 0.9829192161560059
Epoch 1080, training loss: 6.312292098999023 = 0.02156071923673153 + 1.0 * 6.290731430053711
Epoch 1080, val loss: 0.9878242611885071
Epoch 1090, training loss: 6.316146373748779 = 0.020882101729512215 + 1.0 * 6.29526424407959
Epoch 1090, val loss: 0.9926587343215942
Epoch 1100, training loss: 6.316366672515869 = 0.020234761759638786 + 1.0 * 6.2961320877075195
Epoch 1100, val loss: 0.9974095225334167
Epoch 1110, training loss: 6.311867713928223 = 0.019622525200247765 + 1.0 * 6.292245388031006
Epoch 1110, val loss: 1.0018956661224365
Epoch 1120, training loss: 6.309327125549316 = 0.019034797325730324 + 1.0 * 6.290292263031006
Epoch 1120, val loss: 1.0064457654953003
Epoch 1130, training loss: 6.311153411865234 = 0.018473172560334206 + 1.0 * 6.292680263519287
Epoch 1130, val loss: 1.0110019445419312
Epoch 1140, training loss: 6.305881023406982 = 0.01793532259762287 + 1.0 * 6.287945747375488
Epoch 1140, val loss: 1.0154268741607666
Epoch 1150, training loss: 6.31408166885376 = 0.017420923337340355 + 1.0 * 6.296660900115967
Epoch 1150, val loss: 1.0198345184326172
Epoch 1160, training loss: 6.305550575256348 = 0.01693088747560978 + 1.0 * 6.288619518280029
Epoch 1160, val loss: 1.024143099784851
Epoch 1170, training loss: 6.303343772888184 = 0.016461044549942017 + 1.0 * 6.2868828773498535
Epoch 1170, val loss: 1.0283682346343994
Epoch 1180, training loss: 6.303868770599365 = 0.016009094193577766 + 1.0 * 6.2878594398498535
Epoch 1180, val loss: 1.0326215028762817
Epoch 1190, training loss: 6.303096294403076 = 0.015577085316181183 + 1.0 * 6.2875189781188965
Epoch 1190, val loss: 1.0368127822875977
Epoch 1200, training loss: 6.302099227905273 = 0.015163079835474491 + 1.0 * 6.286936283111572
Epoch 1200, val loss: 1.0408813953399658
Epoch 1210, training loss: 6.302755355834961 = 0.014766252599656582 + 1.0 * 6.287989139556885
Epoch 1210, val loss: 1.044908881187439
Epoch 1220, training loss: 6.303526878356934 = 0.01438436284661293 + 1.0 * 6.289142608642578
Epoch 1220, val loss: 1.0488865375518799
Epoch 1230, training loss: 6.296172142028809 = 0.014020023867487907 + 1.0 * 6.28215217590332
Epoch 1230, val loss: 1.0527794361114502
Epoch 1240, training loss: 6.295620441436768 = 0.013670220971107483 + 1.0 * 6.281949996948242
Epoch 1240, val loss: 1.056538462638855
Epoch 1250, training loss: 6.294809818267822 = 0.013332030735909939 + 1.0 * 6.281477928161621
Epoch 1250, val loss: 1.0603879690170288
Epoch 1260, training loss: 6.298305511474609 = 0.013006485998630524 + 1.0 * 6.285298824310303
Epoch 1260, val loss: 1.064243197441101
Epoch 1270, training loss: 6.2935380935668945 = 0.012693182565271854 + 1.0 * 6.280844688415527
Epoch 1270, val loss: 1.0680091381072998
Epoch 1280, training loss: 6.292943477630615 = 0.01239178515970707 + 1.0 * 6.280551910400391
Epoch 1280, val loss: 1.0716192722320557
Epoch 1290, training loss: 6.2915449142456055 = 0.01210111379623413 + 1.0 * 6.279443740844727
Epoch 1290, val loss: 1.0752654075622559
Epoch 1300, training loss: 6.296945571899414 = 0.011820152401924133 + 1.0 * 6.285125255584717
Epoch 1300, val loss: 1.0789295434951782
Epoch 1310, training loss: 6.2980637550354 = 0.011549222283065319 + 1.0 * 6.286514759063721
Epoch 1310, val loss: 1.0825532674789429
Epoch 1320, training loss: 6.290722846984863 = 0.01129207294434309 + 1.0 * 6.279430866241455
Epoch 1320, val loss: 1.0858436822891235
Epoch 1330, training loss: 6.288560390472412 = 0.011043194681406021 + 1.0 * 6.277517318725586
Epoch 1330, val loss: 1.0892047882080078
Epoch 1340, training loss: 6.2898640632629395 = 0.010800979100167751 + 1.0 * 6.2790632247924805
Epoch 1340, val loss: 1.0926735401153564
Epoch 1350, training loss: 6.29096794128418 = 0.010567888617515564 + 1.0 * 6.280400276184082
Epoch 1350, val loss: 1.0960893630981445
Epoch 1360, training loss: 6.288132667541504 = 0.010344787500798702 + 1.0 * 6.277787685394287
Epoch 1360, val loss: 1.0993068218231201
Epoch 1370, training loss: 6.285867691040039 = 0.010126740671694279 + 1.0 * 6.275741100311279
Epoch 1370, val loss: 1.1025534868240356
Epoch 1380, training loss: 6.284054279327393 = 0.009915591217577457 + 1.0 * 6.274138450622559
Epoch 1380, val loss: 1.105906367301941
Epoch 1390, training loss: 6.294120788574219 = 0.009711851365864277 + 1.0 * 6.284409046173096
Epoch 1390, val loss: 1.1092039346694946
Epoch 1400, training loss: 6.286818504333496 = 0.009515631012618542 + 1.0 * 6.2773027420043945
Epoch 1400, val loss: 1.1122909784317017
Epoch 1410, training loss: 6.288013458251953 = 0.009326058439910412 + 1.0 * 6.278687477111816
Epoch 1410, val loss: 1.115263819694519
Epoch 1420, training loss: 6.281276226043701 = 0.00914310198277235 + 1.0 * 6.2721333503723145
Epoch 1420, val loss: 1.1182622909545898
Epoch 1430, training loss: 6.282212257385254 = 0.008965522050857544 + 1.0 * 6.273246765136719
Epoch 1430, val loss: 1.1213586330413818
Epoch 1440, training loss: 6.289331436157227 = 0.008793042972683907 + 1.0 * 6.280538558959961
Epoch 1440, val loss: 1.1244683265686035
Epoch 1450, training loss: 6.281553268432617 = 0.00862490851432085 + 1.0 * 6.272928237915039
Epoch 1450, val loss: 1.1274206638336182
Epoch 1460, training loss: 6.2835516929626465 = 0.008462956175208092 + 1.0 * 6.275088787078857
Epoch 1460, val loss: 1.1303362846374512
Epoch 1470, training loss: 6.28126859664917 = 0.008305856958031654 + 1.0 * 6.27296257019043
Epoch 1470, val loss: 1.1332553625106812
Epoch 1480, training loss: 6.279129981994629 = 0.008154377341270447 + 1.0 * 6.270975589752197
Epoch 1480, val loss: 1.13609778881073
Epoch 1490, training loss: 6.276318550109863 = 0.008006012998521328 + 1.0 * 6.268312454223633
Epoch 1490, val loss: 1.1389437913894653
Epoch 1500, training loss: 6.283299446105957 = 0.007861148566007614 + 1.0 * 6.27543830871582
Epoch 1500, val loss: 1.141860842704773
Epoch 1510, training loss: 6.2798943519592285 = 0.007722110487520695 + 1.0 * 6.272172451019287
Epoch 1510, val loss: 1.1446731090545654
Epoch 1520, training loss: 6.277409553527832 = 0.00758697371929884 + 1.0 * 6.269822597503662
Epoch 1520, val loss: 1.147307276725769
Epoch 1530, training loss: 6.275768280029297 = 0.007455924525856972 + 1.0 * 6.268312454223633
Epoch 1530, val loss: 1.1499693393707275
Epoch 1540, training loss: 6.277510643005371 = 0.00732801528647542 + 1.0 * 6.2701826095581055
Epoch 1540, val loss: 1.152710199356079
Epoch 1550, training loss: 6.274848937988281 = 0.007203853223472834 + 1.0 * 6.267644882202148
Epoch 1550, val loss: 1.1553802490234375
Epoch 1560, training loss: 6.276473045349121 = 0.0070831929333508015 + 1.0 * 6.269389629364014
Epoch 1560, val loss: 1.157946228981018
Epoch 1570, training loss: 6.277480125427246 = 0.00696657644584775 + 1.0 * 6.270513534545898
Epoch 1570, val loss: 1.1605032682418823
Epoch 1580, training loss: 6.271905899047852 = 0.006852013990283012 + 1.0 * 6.265053749084473
Epoch 1580, val loss: 1.1630746126174927
Epoch 1590, training loss: 6.271755695343018 = 0.00674093421548605 + 1.0 * 6.2650146484375
Epoch 1590, val loss: 1.165635585784912
Epoch 1600, training loss: 6.2769951820373535 = 0.006632193457335234 + 1.0 * 6.270362854003906
Epoch 1600, val loss: 1.1682407855987549
Epoch 1610, training loss: 6.277995586395264 = 0.006527420599013567 + 1.0 * 6.271468162536621
Epoch 1610, val loss: 1.1707186698913574
Epoch 1620, training loss: 6.276576519012451 = 0.006424877792596817 + 1.0 * 6.270151615142822
Epoch 1620, val loss: 1.1730870008468628
Epoch 1630, training loss: 6.26917839050293 = 0.006325320806354284 + 1.0 * 6.262853145599365
Epoch 1630, val loss: 1.1754364967346191
Epoch 1640, training loss: 6.269504070281982 = 0.006228623911738396 + 1.0 * 6.263275623321533
Epoch 1640, val loss: 1.1777746677398682
Epoch 1650, training loss: 6.269672393798828 = 0.006133906077593565 + 1.0 * 6.263538360595703
Epoch 1650, val loss: 1.1801620721817017
Epoch 1660, training loss: 6.273446083068848 = 0.006041088607162237 + 1.0 * 6.267405033111572
Epoch 1660, val loss: 1.182597041130066
Epoch 1670, training loss: 6.271792888641357 = 0.005951304920017719 + 1.0 * 6.265841484069824
Epoch 1670, val loss: 1.184934377670288
Epoch 1680, training loss: 6.2725749015808105 = 0.005863617639988661 + 1.0 * 6.266711235046387
Epoch 1680, val loss: 1.1871740818023682
Epoch 1690, training loss: 6.265572547912598 = 0.005778293125331402 + 1.0 * 6.259794235229492
Epoch 1690, val loss: 1.189436912536621
Epoch 1700, training loss: 6.2668681144714355 = 0.005694731138646603 + 1.0 * 6.261173248291016
Epoch 1700, val loss: 1.1917215585708618
Epoch 1710, training loss: 6.27077579498291 = 0.005612500477582216 + 1.0 * 6.265163421630859
Epoch 1710, val loss: 1.1940211057662964
Epoch 1720, training loss: 6.2686662673950195 = 0.00553294038400054 + 1.0 * 6.263133525848389
Epoch 1720, val loss: 1.1962711811065674
Epoch 1730, training loss: 6.266355037689209 = 0.00545587670058012 + 1.0 * 6.260899066925049
Epoch 1730, val loss: 1.198370099067688
Epoch 1740, training loss: 6.264466285705566 = 0.005380342248827219 + 1.0 * 6.2590861320495605
Epoch 1740, val loss: 1.2004637718200684
Epoch 1750, training loss: 6.264553070068359 = 0.005306113976985216 + 1.0 * 6.259246826171875
Epoch 1750, val loss: 1.2026715278625488
Epoch 1760, training loss: 6.268459320068359 = 0.005233338102698326 + 1.0 * 6.26322603225708
Epoch 1760, val loss: 1.2048970460891724
Epoch 1770, training loss: 6.267380237579346 = 0.005163054447621107 + 1.0 * 6.262217044830322
Epoch 1770, val loss: 1.2070039510726929
Epoch 1780, training loss: 6.262664794921875 = 0.0050940257497131824 + 1.0 * 6.257570743560791
Epoch 1780, val loss: 1.2090492248535156
Epoch 1790, training loss: 6.274682521820068 = 0.0050271605141460896 + 1.0 * 6.269655227661133
Epoch 1790, val loss: 1.2111140489578247
Epoch 1800, training loss: 6.264669895172119 = 0.004960678517818451 + 1.0 * 6.259709358215332
Epoch 1800, val loss: 1.213180661201477
Epoch 1810, training loss: 6.263199806213379 = 0.004897027276456356 + 1.0 * 6.258302688598633
Epoch 1810, val loss: 1.2151213884353638
Epoch 1820, training loss: 6.26603364944458 = 0.004834005609154701 + 1.0 * 6.261199474334717
Epoch 1820, val loss: 1.2171505689620972
Epoch 1830, training loss: 6.260256767272949 = 0.00477258488535881 + 1.0 * 6.255484104156494
Epoch 1830, val loss: 1.219145655632019
Epoch 1840, training loss: 6.261732578277588 = 0.004712182097136974 + 1.0 * 6.257020473480225
Epoch 1840, val loss: 1.2211499214172363
Epoch 1850, training loss: 6.270697116851807 = 0.004652702249586582 + 1.0 * 6.266044616699219
Epoch 1850, val loss: 1.2231814861297607
Epoch 1860, training loss: 6.260257244110107 = 0.004595774691551924 + 1.0 * 6.255661487579346
Epoch 1860, val loss: 1.2250866889953613
Epoch 1870, training loss: 6.260340213775635 = 0.0045395223423838615 + 1.0 * 6.255800724029541
Epoch 1870, val loss: 1.2269207239151
Epoch 1880, training loss: 6.260654449462891 = 0.004484057426452637 + 1.0 * 6.256170272827148
Epoch 1880, val loss: 1.2288804054260254
Epoch 1890, training loss: 6.260623931884766 = 0.004429790191352367 + 1.0 * 6.256194114685059
Epoch 1890, val loss: 1.2308580875396729
Epoch 1900, training loss: 6.2630391120910645 = 0.004376720637083054 + 1.0 * 6.258662223815918
Epoch 1900, val loss: 1.232807993888855
Epoch 1910, training loss: 6.259253978729248 = 0.004324683453887701 + 1.0 * 6.254929065704346
Epoch 1910, val loss: 1.2346781492233276
Epoch 1920, training loss: 6.267143726348877 = 0.0042738886550068855 + 1.0 * 6.262869834899902
Epoch 1920, val loss: 1.2365041971206665
Epoch 1930, training loss: 6.260589599609375 = 0.004223864991217852 + 1.0 * 6.256365776062012
Epoch 1930, val loss: 1.238335132598877
Epoch 1940, training loss: 6.259446144104004 = 0.004175454843789339 + 1.0 * 6.255270481109619
Epoch 1940, val loss: 1.2400562763214111
Epoch 1950, training loss: 6.261449337005615 = 0.004127546213567257 + 1.0 * 6.257321834564209
Epoch 1950, val loss: 1.2418676614761353
Epoch 1960, training loss: 6.256123065948486 = 0.004080749116837978 + 1.0 * 6.252042293548584
Epoch 1960, val loss: 1.2436928749084473
Epoch 1970, training loss: 6.261297702789307 = 0.004035067744553089 + 1.0 * 6.257262706756592
Epoch 1970, val loss: 1.2454897165298462
Epoch 1980, training loss: 6.258126735687256 = 0.003989752847701311 + 1.0 * 6.25413703918457
Epoch 1980, val loss: 1.247274398803711
Epoch 1990, training loss: 6.255551338195801 = 0.003945635631680489 + 1.0 * 6.25160551071167
Epoch 1990, val loss: 1.2489386796951294
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 10.543656349182129 = 1.9467955827713013 + 1.0 * 8.596860885620117
Epoch 0, val loss: 1.9461007118225098
Epoch 10, training loss: 10.533995628356934 = 1.9373432397842407 + 1.0 * 8.596652030944824
Epoch 10, val loss: 1.9371317625045776
Epoch 20, training loss: 10.520247459411621 = 1.9258368015289307 + 1.0 * 8.59441089630127
Epoch 20, val loss: 1.9257317781448364
Epoch 30, training loss: 10.483227729797363 = 1.910097360610962 + 1.0 * 8.57313060760498
Epoch 30, val loss: 1.9096934795379639
Epoch 40, training loss: 10.319808959960938 = 1.8893804550170898 + 1.0 * 8.430428504943848
Epoch 40, val loss: 1.8890820741653442
Epoch 50, training loss: 9.879077911376953 = 1.8654214143753052 + 1.0 * 8.013656616210938
Epoch 50, val loss: 1.8666925430297852
Epoch 60, training loss: 9.58922290802002 = 1.8441020250320435 + 1.0 * 7.745120525360107
Epoch 60, val loss: 1.8479822874069214
Epoch 70, training loss: 9.153722763061523 = 1.8275638818740845 + 1.0 * 7.32615852355957
Epoch 70, val loss: 1.8334887027740479
Epoch 80, training loss: 8.838706970214844 = 1.8132166862487793 + 1.0 * 7.025490760803223
Epoch 80, val loss: 1.820681095123291
Epoch 90, training loss: 8.652200698852539 = 1.7977039813995361 + 1.0 * 6.854496955871582
Epoch 90, val loss: 1.8072221279144287
Epoch 100, training loss: 8.530113220214844 = 1.7816720008850098 + 1.0 * 6.748440742492676
Epoch 100, val loss: 1.7934015989303589
Epoch 110, training loss: 8.449825286865234 = 1.7655938863754272 + 1.0 * 6.684231281280518
Epoch 110, val loss: 1.779415249824524
Epoch 120, training loss: 8.38992977142334 = 1.7483850717544556 + 1.0 * 6.641544342041016
Epoch 120, val loss: 1.7641512155532837
Epoch 130, training loss: 8.336319923400879 = 1.729656457901001 + 1.0 * 6.606663703918457
Epoch 130, val loss: 1.747491478919983
Epoch 140, training loss: 8.287184715270996 = 1.7087963819503784 + 1.0 * 6.578388690948486
Epoch 140, val loss: 1.729277491569519
Epoch 150, training loss: 8.247991561889648 = 1.684861660003662 + 1.0 * 6.563129425048828
Epoch 150, val loss: 1.7086846828460693
Epoch 160, training loss: 8.195450782775879 = 1.6576603651046753 + 1.0 * 6.537790775299072
Epoch 160, val loss: 1.6854609251022339
Epoch 170, training loss: 8.14634895324707 = 1.6266579627990723 + 1.0 * 6.51969051361084
Epoch 170, val loss: 1.6593400239944458
Epoch 180, training loss: 8.095222473144531 = 1.5910189151763916 + 1.0 * 6.5042033195495605
Epoch 180, val loss: 1.6295477151870728
Epoch 190, training loss: 8.040559768676758 = 1.55012845993042 + 1.0 * 6.490431308746338
Epoch 190, val loss: 1.5953954458236694
Epoch 200, training loss: 7.986392498016357 = 1.5038284063339233 + 1.0 * 6.4825639724731445
Epoch 200, val loss: 1.5568881034851074
Epoch 210, training loss: 7.924349784851074 = 1.453513264656067 + 1.0 * 6.470836639404297
Epoch 210, val loss: 1.5150864124298096
Epoch 220, training loss: 7.8598222732543945 = 1.3993669748306274 + 1.0 * 6.460455417633057
Epoch 220, val loss: 1.47049880027771
Epoch 230, training loss: 7.7947821617126465 = 1.3421974182128906 + 1.0 * 6.452584743499756
Epoch 230, val loss: 1.4238249063491821
Epoch 240, training loss: 7.736234188079834 = 1.2832822799682617 + 1.0 * 6.452951908111572
Epoch 240, val loss: 1.376415729522705
Epoch 250, training loss: 7.666093349456787 = 1.2249194383621216 + 1.0 * 6.441174030303955
Epoch 250, val loss: 1.3301233053207397
Epoch 260, training loss: 7.60124397277832 = 1.168062448501587 + 1.0 * 6.433181285858154
Epoch 260, val loss: 1.2855113744735718
Epoch 270, training loss: 7.540256977081299 = 1.113145351409912 + 1.0 * 6.427111625671387
Epoch 270, val loss: 1.2428863048553467
Epoch 280, training loss: 7.485963821411133 = 1.0605692863464355 + 1.0 * 6.425394535064697
Epoch 280, val loss: 1.2024779319763184
Epoch 290, training loss: 7.428390026092529 = 1.0106319189071655 + 1.0 * 6.417757987976074
Epoch 290, val loss: 1.1645499467849731
Epoch 300, training loss: 7.374801158905029 = 0.963290810585022 + 1.0 * 6.411510467529297
Epoch 300, val loss: 1.128551959991455
Epoch 310, training loss: 7.325172424316406 = 0.9180281162261963 + 1.0 * 6.407144069671631
Epoch 310, val loss: 1.0942720174789429
Epoch 320, training loss: 7.278584957122803 = 0.8749979734420776 + 1.0 * 6.4035868644714355
Epoch 320, val loss: 1.0618711709976196
Epoch 330, training loss: 7.232048988342285 = 0.8342334032058716 + 1.0 * 6.397815704345703
Epoch 330, val loss: 1.0312104225158691
Epoch 340, training loss: 7.187661647796631 = 0.7949524521827698 + 1.0 * 6.392709255218506
Epoch 340, val loss: 1.001804232597351
Epoch 350, training loss: 7.147792816162109 = 0.7571216225624084 + 1.0 * 6.390671253204346
Epoch 350, val loss: 0.9736478328704834
Epoch 360, training loss: 7.105618953704834 = 0.7212541699409485 + 1.0 * 6.384364604949951
Epoch 360, val loss: 0.9472468495368958
Epoch 370, training loss: 7.0678815841674805 = 0.6869407892227173 + 1.0 * 6.380940914154053
Epoch 370, val loss: 0.9225659370422363
Epoch 380, training loss: 7.031652927398682 = 0.6539469957351685 + 1.0 * 6.377706050872803
Epoch 380, val loss: 0.8993673920631409
Epoch 390, training loss: 6.997722148895264 = 0.6223253607749939 + 1.0 * 6.375396728515625
Epoch 390, val loss: 0.8777951002120972
Epoch 400, training loss: 6.963343143463135 = 0.5921269059181213 + 1.0 * 6.371216297149658
Epoch 400, val loss: 0.857979953289032
Epoch 410, training loss: 6.93161153793335 = 0.5630984902381897 + 1.0 * 6.368513107299805
Epoch 410, val loss: 0.8396770358085632
Epoch 420, training loss: 6.904641151428223 = 0.5351609587669373 + 1.0 * 6.369480133056641
Epoch 420, val loss: 0.8227909207344055
Epoch 430, training loss: 6.872967720031738 = 0.5085182189941406 + 1.0 * 6.364449501037598
Epoch 430, val loss: 0.8074118494987488
Epoch 440, training loss: 6.843845367431641 = 0.482948899269104 + 1.0 * 6.360896587371826
Epoch 440, val loss: 0.7933459281921387
Epoch 450, training loss: 6.819161415100098 = 0.45843568444252014 + 1.0 * 6.3607258796691895
Epoch 450, val loss: 0.7804955840110779
Epoch 460, training loss: 6.790213108062744 = 0.4350619614124298 + 1.0 * 6.355151176452637
Epoch 460, val loss: 0.7689511775970459
Epoch 470, training loss: 6.766096591949463 = 0.41278916597366333 + 1.0 * 6.353307247161865
Epoch 470, val loss: 0.7586419582366943
Epoch 480, training loss: 6.7490692138671875 = 0.39161139726638794 + 1.0 * 6.357457637786865
Epoch 480, val loss: 0.7494419813156128
Epoch 490, training loss: 6.723631381988525 = 0.3716707229614258 + 1.0 * 6.3519606590271
Epoch 490, val loss: 0.7414097785949707
Epoch 500, training loss: 6.698594570159912 = 0.3528420627117157 + 1.0 * 6.345752716064453
Epoch 500, val loss: 0.7346382141113281
Epoch 510, training loss: 6.684626579284668 = 0.3350311815738678 + 1.0 * 6.349595546722412
Epoch 510, val loss: 0.7288426756858826
Epoch 520, training loss: 6.664362907409668 = 0.31823742389678955 + 1.0 * 6.346125602722168
Epoch 520, val loss: 0.7240431308746338
Epoch 530, training loss: 6.64177942276001 = 0.30247703194618225 + 1.0 * 6.3393025398254395
Epoch 530, val loss: 0.720299243927002
Epoch 540, training loss: 6.627839088439941 = 0.2875787615776062 + 1.0 * 6.3402605056762695
Epoch 540, val loss: 0.7174422144889832
Epoch 550, training loss: 6.610416412353516 = 0.27350524067878723 + 1.0 * 6.336911201477051
Epoch 550, val loss: 0.7154248356819153
Epoch 560, training loss: 6.595383644104004 = 0.26027750968933105 + 1.0 * 6.335106372833252
Epoch 560, val loss: 0.7142477631568909
Epoch 570, training loss: 6.582619667053223 = 0.24768783152103424 + 1.0 * 6.33493185043335
Epoch 570, val loss: 0.7138718962669373
Epoch 580, training loss: 6.569647789001465 = 0.23579280078411102 + 1.0 * 6.333855152130127
Epoch 580, val loss: 0.7140251994132996
Epoch 590, training loss: 6.553025245666504 = 0.2244679480791092 + 1.0 * 6.32855749130249
Epoch 590, val loss: 0.7149689793586731
Epoch 600, training loss: 6.540140151977539 = 0.21364261209964752 + 1.0 * 6.326497554779053
Epoch 600, val loss: 0.7164425849914551
Epoch 610, training loss: 6.537933349609375 = 0.2032538503408432 + 1.0 * 6.33467960357666
Epoch 610, val loss: 0.7184018492698669
Epoch 620, training loss: 6.5200934410095215 = 0.1933012753725052 + 1.0 * 6.326792240142822
Epoch 620, val loss: 0.7206992506980896
Epoch 630, training loss: 6.507481575012207 = 0.18377014994621277 + 1.0 * 6.323711395263672
Epoch 630, val loss: 0.7235972285270691
Epoch 640, training loss: 6.497162818908691 = 0.17461946606636047 + 1.0 * 6.322543144226074
Epoch 640, val loss: 0.726789653301239
Epoch 650, training loss: 6.4886088371276855 = 0.16582845151424408 + 1.0 * 6.322780609130859
Epoch 650, val loss: 0.7302273511886597
Epoch 660, training loss: 6.478028774261475 = 0.15736602246761322 + 1.0 * 6.320662975311279
Epoch 660, val loss: 0.7340781688690186
Epoch 670, training loss: 6.468297481536865 = 0.14927774667739868 + 1.0 * 6.319019794464111
Epoch 670, val loss: 0.7381709814071655
Epoch 680, training loss: 6.460082054138184 = 0.1415284276008606 + 1.0 * 6.318553447723389
Epoch 680, val loss: 0.7425170540809631
Epoch 690, training loss: 6.44924783706665 = 0.13414011895656586 + 1.0 * 6.315107822418213
Epoch 690, val loss: 0.7471253871917725
Epoch 700, training loss: 6.4411516189575195 = 0.12710830569267273 + 1.0 * 6.3140435218811035
Epoch 700, val loss: 0.7518802881240845
Epoch 710, training loss: 6.435362815856934 = 0.1204417422413826 + 1.0 * 6.314920902252197
Epoch 710, val loss: 0.7569113969802856
Epoch 720, training loss: 6.424680709838867 = 0.11411534994840622 + 1.0 * 6.31056547164917
Epoch 720, val loss: 0.7620055675506592
Epoch 730, training loss: 6.418034076690674 = 0.10813584923744202 + 1.0 * 6.309898376464844
Epoch 730, val loss: 0.7673321962356567
Epoch 740, training loss: 6.417497158050537 = 0.10250450670719147 + 1.0 * 6.314992427825928
Epoch 740, val loss: 0.7727028727531433
Epoch 750, training loss: 6.406484127044678 = 0.09718350321054459 + 1.0 * 6.309300422668457
Epoch 750, val loss: 0.7781171202659607
Epoch 760, training loss: 6.398810386657715 = 0.09218762069940567 + 1.0 * 6.3066229820251465
Epoch 760, val loss: 0.7836647033691406
Epoch 770, training loss: 6.392831802368164 = 0.08747832477092743 + 1.0 * 6.30535364151001
Epoch 770, val loss: 0.7893356680870056
Epoch 780, training loss: 6.39300537109375 = 0.08304749429225922 + 1.0 * 6.309957981109619
Epoch 780, val loss: 0.794931173324585
Epoch 790, training loss: 6.382513999938965 = 0.07888747751712799 + 1.0 * 6.303626537322998
Epoch 790, val loss: 0.8004454970359802
Epoch 800, training loss: 6.392580986022949 = 0.07496900111436844 + 1.0 * 6.317612171173096
Epoch 800, val loss: 0.8061461448669434
Epoch 810, training loss: 6.37769889831543 = 0.07134413719177246 + 1.0 * 6.306354999542236
Epoch 810, val loss: 0.8116050958633423
Epoch 820, training loss: 6.36956262588501 = 0.06791609525680542 + 1.0 * 6.301646709442139
Epoch 820, val loss: 0.8172791600227356
Epoch 830, training loss: 6.362934589385986 = 0.0646936446428299 + 1.0 * 6.298241138458252
Epoch 830, val loss: 0.8228864073753357
Epoch 840, training loss: 6.359364032745361 = 0.061652496457099915 + 1.0 * 6.297711372375488
Epoch 840, val loss: 0.8284043669700623
Epoch 850, training loss: 6.358445167541504 = 0.05878914147615433 + 1.0 * 6.299655914306641
Epoch 850, val loss: 0.8338941335678101
Epoch 860, training loss: 6.353195667266846 = 0.05609261617064476 + 1.0 * 6.297102928161621
Epoch 860, val loss: 0.8393457531929016
Epoch 870, training loss: 6.351686954498291 = 0.05355805531144142 + 1.0 * 6.298129081726074
Epoch 870, val loss: 0.8447420597076416
Epoch 880, training loss: 6.345802307128906 = 0.05117134004831314 + 1.0 * 6.294631004333496
Epoch 880, val loss: 0.8502198457717896
Epoch 890, training loss: 6.345847129821777 = 0.04892030730843544 + 1.0 * 6.296926975250244
Epoch 890, val loss: 0.8556028008460999
Epoch 900, training loss: 6.342325687408447 = 0.046799663454294205 + 1.0 * 6.295526027679443
Epoch 900, val loss: 0.8607364892959595
Epoch 910, training loss: 6.340920925140381 = 0.04480944201350212 + 1.0 * 6.296111583709717
Epoch 910, val loss: 0.8659921884536743
Epoch 920, training loss: 6.337348461151123 = 0.042935244739055634 + 1.0 * 6.294413089752197
Epoch 920, val loss: 0.8711576461791992
Epoch 930, training loss: 6.3322434425354 = 0.04116100072860718 + 1.0 * 6.291082382202148
Epoch 930, val loss: 0.8762756586074829
Epoch 940, training loss: 6.334007263183594 = 0.0394880585372448 + 1.0 * 6.294519424438477
Epoch 940, val loss: 0.8813480138778687
Epoch 950, training loss: 6.334292888641357 = 0.03790735825896263 + 1.0 * 6.296385765075684
Epoch 950, val loss: 0.8862124681472778
Epoch 960, training loss: 6.326393127441406 = 0.036406029015779495 + 1.0 * 6.289987087249756
Epoch 960, val loss: 0.8911536931991577
Epoch 970, training loss: 6.322537899017334 = 0.03499459847807884 + 1.0 * 6.287543296813965
Epoch 970, val loss: 0.8960814476013184
Epoch 980, training loss: 6.320584297180176 = 0.03365246206521988 + 1.0 * 6.286931991577148
Epoch 980, val loss: 0.9009144902229309
Epoch 990, training loss: 6.332176685333252 = 0.03237595409154892 + 1.0 * 6.299800872802734
Epoch 990, val loss: 0.9056063294410706
Epoch 1000, training loss: 6.319901466369629 = 0.03117799013853073 + 1.0 * 6.288723468780518
Epoch 1000, val loss: 0.910197377204895
Epoch 1010, training loss: 6.322388648986816 = 0.030041566118597984 + 1.0 * 6.292346954345703
Epoch 1010, val loss: 0.9148859977722168
Epoch 1020, training loss: 6.3127546310424805 = 0.02895740605890751 + 1.0 * 6.283797264099121
Epoch 1020, val loss: 0.9194190502166748
Epoch 1030, training loss: 6.312840938568115 = 0.02793058007955551 + 1.0 * 6.284910202026367
Epoch 1030, val loss: 0.9239638447761536
Epoch 1040, training loss: 6.3127970695495605 = 0.02695561572909355 + 1.0 * 6.285841464996338
Epoch 1040, val loss: 0.9284317493438721
Epoch 1050, training loss: 6.310062408447266 = 0.026026062667369843 + 1.0 * 6.284036159515381
Epoch 1050, val loss: 0.932772159576416
Epoch 1060, training loss: 6.3104119300842285 = 0.02514328807592392 + 1.0 * 6.285268783569336
Epoch 1060, val loss: 0.9371222257614136
Epoch 1070, training loss: 6.309213638305664 = 0.024307692423462868 + 1.0 * 6.284905910491943
Epoch 1070, val loss: 0.941387414932251
Epoch 1080, training loss: 6.3080244064331055 = 0.023511061444878578 + 1.0 * 6.284513473510742
Epoch 1080, val loss: 0.9456620216369629
Epoch 1090, training loss: 6.304286479949951 = 0.02274908870458603 + 1.0 * 6.2815375328063965
Epoch 1090, val loss: 0.9497411847114563
Epoch 1100, training loss: 6.30261754989624 = 0.022024648264050484 + 1.0 * 6.280592918395996
Epoch 1100, val loss: 0.9539120197296143
Epoch 1110, training loss: 6.302615642547607 = 0.02133253775537014 + 1.0 * 6.281282901763916
Epoch 1110, val loss: 0.957951545715332
Epoch 1120, training loss: 6.304200172424316 = 0.020670078694820404 + 1.0 * 6.283530235290527
Epoch 1120, val loss: 0.9618571996688843
Epoch 1130, training loss: 6.3029375076293945 = 0.02004251815378666 + 1.0 * 6.282895088195801
Epoch 1130, val loss: 0.9657580256462097
Epoch 1140, training loss: 6.2990641593933105 = 0.019441435113549232 + 1.0 * 6.279622554779053
Epoch 1140, val loss: 0.9696707129478455
Epoch 1150, training loss: 6.296405792236328 = 0.01886751875281334 + 1.0 * 6.277538299560547
Epoch 1150, val loss: 0.9736180305480957
Epoch 1160, training loss: 6.295445919036865 = 0.018318509683012962 + 1.0 * 6.277127265930176
Epoch 1160, val loss: 0.9773991107940674
Epoch 1170, training loss: 6.304666519165039 = 0.01778942160308361 + 1.0 * 6.286877155303955
Epoch 1170, val loss: 0.9810196757316589
Epoch 1180, training loss: 6.296560764312744 = 0.01728937029838562 + 1.0 * 6.279271602630615
Epoch 1180, val loss: 0.9847012162208557
Epoch 1190, training loss: 6.295359134674072 = 0.016806181520223618 + 1.0 * 6.278553009033203
Epoch 1190, val loss: 0.9883392453193665
Epoch 1200, training loss: 6.293403148651123 = 0.016346614807844162 + 1.0 * 6.277056694030762
Epoch 1200, val loss: 0.9919746518135071
Epoch 1210, training loss: 6.289807319641113 = 0.015903403982520103 + 1.0 * 6.273903846740723
Epoch 1210, val loss: 0.9955279231071472
Epoch 1220, training loss: 6.291222095489502 = 0.015479364432394505 + 1.0 * 6.275742530822754
Epoch 1220, val loss: 0.9990915060043335
Epoch 1230, training loss: 6.3025803565979 = 0.015072931535542011 + 1.0 * 6.2875075340271
Epoch 1230, val loss: 1.0023220777511597
Epoch 1240, training loss: 6.291532039642334 = 0.014678871259093285 + 1.0 * 6.276853084564209
Epoch 1240, val loss: 1.0055958032608032
Epoch 1250, training loss: 6.287414073944092 = 0.014308658428490162 + 1.0 * 6.273105621337891
Epoch 1250, val loss: 1.009055733680725
Epoch 1260, training loss: 6.284848213195801 = 0.01394960843026638 + 1.0 * 6.270898818969727
Epoch 1260, val loss: 1.0124727487564087
Epoch 1270, training loss: 6.284316062927246 = 0.0136027242988348 + 1.0 * 6.2707133293151855
Epoch 1270, val loss: 1.0157196521759033
Epoch 1280, training loss: 6.297423362731934 = 0.013266967609524727 + 1.0 * 6.284156322479248
Epoch 1280, val loss: 1.018802285194397
Epoch 1290, training loss: 6.287007808685303 = 0.01294662058353424 + 1.0 * 6.27406120300293
Epoch 1290, val loss: 1.0218948125839233
Epoch 1300, training loss: 6.282726764678955 = 0.012638932093977928 + 1.0 * 6.270087718963623
Epoch 1300, val loss: 1.0250924825668335
Epoch 1310, training loss: 6.286346912384033 = 0.012341183610260487 + 1.0 * 6.274005889892578
Epoch 1310, val loss: 1.0282384157180786
Epoch 1320, training loss: 6.281641960144043 = 0.012055222876369953 + 1.0 * 6.269586563110352
Epoch 1320, val loss: 1.0311577320098877
Epoch 1330, training loss: 6.279765605926514 = 0.011780695989727974 + 1.0 * 6.267984867095947
Epoch 1330, val loss: 1.0341265201568604
Epoch 1340, training loss: 6.279240131378174 = 0.011515503749251366 + 1.0 * 6.267724514007568
Epoch 1340, val loss: 1.0371755361557007
Epoch 1350, training loss: 6.284667015075684 = 0.011258994229137897 + 1.0 * 6.273407936096191
Epoch 1350, val loss: 1.040105938911438
Epoch 1360, training loss: 6.27934455871582 = 0.011009188368916512 + 1.0 * 6.268335342407227
Epoch 1360, val loss: 1.0429320335388184
Epoch 1370, training loss: 6.284381866455078 = 0.010769934393465519 + 1.0 * 6.273612022399902
Epoch 1370, val loss: 1.0456677675247192
Epoch 1380, training loss: 6.278438091278076 = 0.010541467927396297 + 1.0 * 6.26789665222168
Epoch 1380, val loss: 1.0484864711761475
Epoch 1390, training loss: 6.277438163757324 = 0.010320177301764488 + 1.0 * 6.267117977142334
Epoch 1390, val loss: 1.0513490438461304
Epoch 1400, training loss: 6.277477741241455 = 0.010104731656610966 + 1.0 * 6.267373085021973
Epoch 1400, val loss: 1.054139494895935
Epoch 1410, training loss: 6.280292987823486 = 0.009896144270896912 + 1.0 * 6.270396709442139
Epoch 1410, val loss: 1.0567506551742554
Epoch 1420, training loss: 6.276501655578613 = 0.009692754596471786 + 1.0 * 6.266808986663818
Epoch 1420, val loss: 1.0593223571777344
Epoch 1430, training loss: 6.279625415802002 = 0.009497164748609066 + 1.0 * 6.27012825012207
Epoch 1430, val loss: 1.061989426612854
Epoch 1440, training loss: 6.275140285491943 = 0.009309997782111168 + 1.0 * 6.2658305168151855
Epoch 1440, val loss: 1.064505696296692
Epoch 1450, training loss: 6.273734092712402 = 0.009128816425800323 + 1.0 * 6.2646050453186035
Epoch 1450, val loss: 1.0670368671417236
Epoch 1460, training loss: 6.272994041442871 = 0.008952014148235321 + 1.0 * 6.264041900634766
Epoch 1460, val loss: 1.0696463584899902
Epoch 1470, training loss: 6.27404260635376 = 0.008780326694250107 + 1.0 * 6.265262126922607
Epoch 1470, val loss: 1.0721162557601929
Epoch 1480, training loss: 6.276756286621094 = 0.0086135845631361 + 1.0 * 6.2681427001953125
Epoch 1480, val loss: 1.0744359493255615
Epoch 1490, training loss: 6.272945404052734 = 0.008453002199530602 + 1.0 * 6.264492511749268
Epoch 1490, val loss: 1.0768885612487793
Epoch 1500, training loss: 6.275126934051514 = 0.00829695537686348 + 1.0 * 6.266829967498779
Epoch 1500, val loss: 1.0792807340621948
Epoch 1510, training loss: 6.271462440490723 = 0.008146511390805244 + 1.0 * 6.2633161544799805
Epoch 1510, val loss: 1.0816140174865723
Epoch 1520, training loss: 6.271291255950928 = 0.008000903762876987 + 1.0 * 6.2632904052734375
Epoch 1520, val loss: 1.0839678049087524
Epoch 1530, training loss: 6.273437976837158 = 0.00785798393189907 + 1.0 * 6.265580177307129
Epoch 1530, val loss: 1.086215853691101
Epoch 1540, training loss: 6.273089408874512 = 0.007719056680798531 + 1.0 * 6.2653703689575195
Epoch 1540, val loss: 1.0884711742401123
Epoch 1550, training loss: 6.268700122833252 = 0.007585879880934954 + 1.0 * 6.261114120483398
Epoch 1550, val loss: 1.0906943082809448
Epoch 1560, training loss: 6.271442890167236 = 0.007455373648554087 + 1.0 * 6.2639875411987305
Epoch 1560, val loss: 1.0929580926895142
Epoch 1570, training loss: 6.268752098083496 = 0.007328911684453487 + 1.0 * 6.261423110961914
Epoch 1570, val loss: 1.0950446128845215
Epoch 1580, training loss: 6.269582271575928 = 0.007205919362604618 + 1.0 * 6.262376308441162
Epoch 1580, val loss: 1.0971215963363647
Epoch 1590, training loss: 6.273110389709473 = 0.007087255362421274 + 1.0 * 6.2660231590271
Epoch 1590, val loss: 1.099308967590332
Epoch 1600, training loss: 6.266392230987549 = 0.006971441674977541 + 1.0 * 6.259420871734619
Epoch 1600, val loss: 1.1014270782470703
Epoch 1610, training loss: 6.266858100891113 = 0.006858698558062315 + 1.0 * 6.2599992752075195
Epoch 1610, val loss: 1.1034419536590576
Epoch 1620, training loss: 6.271132469177246 = 0.006748368963599205 + 1.0 * 6.2643842697143555
Epoch 1620, val loss: 1.1054664850234985
Epoch 1630, training loss: 6.266351699829102 = 0.006642150226980448 + 1.0 * 6.259709358215332
Epoch 1630, val loss: 1.1075125932693481
Epoch 1640, training loss: 6.265030860900879 = 0.006538539659231901 + 1.0 * 6.258492469787598
Epoch 1640, val loss: 1.1094383001327515
Epoch 1650, training loss: 6.2667765617370605 = 0.006436933763325214 + 1.0 * 6.260339736938477
Epoch 1650, val loss: 1.1114091873168945
Epoch 1660, training loss: 6.2675371170043945 = 0.00633741170167923 + 1.0 * 6.261199474334717
Epoch 1660, val loss: 1.1134010553359985
Epoch 1670, training loss: 6.26677131652832 = 0.006241180934011936 + 1.0 * 6.2605299949646
Epoch 1670, val loss: 1.1152570247650146
Epoch 1680, training loss: 6.267050266265869 = 0.006147836335003376 + 1.0 * 6.260902404785156
Epoch 1680, val loss: 1.1171308755874634
Epoch 1690, training loss: 6.261881351470947 = 0.006057258229702711 + 1.0 * 6.255824089050293
Epoch 1690, val loss: 1.1189579963684082
Epoch 1700, training loss: 6.262533187866211 = 0.005968556273728609 + 1.0 * 6.256564617156982
Epoch 1700, val loss: 1.120874285697937
Epoch 1710, training loss: 6.265789985656738 = 0.0058817556127905846 + 1.0 * 6.259908199310303
Epoch 1710, val loss: 1.122781753540039
Epoch 1720, training loss: 6.263226509094238 = 0.005796913523226976 + 1.0 * 6.257429599761963
Epoch 1720, val loss: 1.124451756477356
Epoch 1730, training loss: 6.266392230987549 = 0.005714676342904568 + 1.0 * 6.260677337646484
Epoch 1730, val loss: 1.1262160539627075
Epoch 1740, training loss: 6.262796878814697 = 0.005634006578475237 + 1.0 * 6.257163047790527
Epoch 1740, val loss: 1.127956509590149
Epoch 1750, training loss: 6.261914253234863 = 0.005555040668696165 + 1.0 * 6.256359100341797
Epoch 1750, val loss: 1.129704236984253
Epoch 1760, training loss: 6.264190673828125 = 0.0054784067906439304 + 1.0 * 6.258712291717529
Epoch 1760, val loss: 1.1314001083374023
Epoch 1770, training loss: 6.260788440704346 = 0.005404616706073284 + 1.0 * 6.2553839683532715
Epoch 1770, val loss: 1.1330493688583374
Epoch 1780, training loss: 6.262490272521973 = 0.005331715103238821 + 1.0 * 6.2571587562561035
Epoch 1780, val loss: 1.1348285675048828
Epoch 1790, training loss: 6.261388778686523 = 0.005259918514639139 + 1.0 * 6.256128787994385
Epoch 1790, val loss: 1.1363705396652222
Epoch 1800, training loss: 6.260031223297119 = 0.0051896171644330025 + 1.0 * 6.2548418045043945
Epoch 1800, val loss: 1.1379683017730713
Epoch 1810, training loss: 6.260124683380127 = 0.005122095346450806 + 1.0 * 6.255002498626709
Epoch 1810, val loss: 1.1396706104278564
Epoch 1820, training loss: 6.262915134429932 = 0.0050561875104904175 + 1.0 * 6.257858753204346
Epoch 1820, val loss: 1.1411924362182617
Epoch 1830, training loss: 6.2561564445495605 = 0.0049902512691915035 + 1.0 * 6.251166343688965
Epoch 1830, val loss: 1.1427005529403687
Epoch 1840, training loss: 6.257453918457031 = 0.004926602356135845 + 1.0 * 6.252527236938477
Epoch 1840, val loss: 1.1443257331848145
Epoch 1850, training loss: 6.268153667449951 = 0.004864317364990711 + 1.0 * 6.263289451599121
Epoch 1850, val loss: 1.145809292793274
Epoch 1860, training loss: 6.258344650268555 = 0.004804106894880533 + 1.0 * 6.253540515899658
Epoch 1860, val loss: 1.1472135782241821
Epoch 1870, training loss: 6.25649881362915 = 0.004744796082377434 + 1.0 * 6.251753807067871
Epoch 1870, val loss: 1.1488062143325806
Epoch 1880, training loss: 6.261157035827637 = 0.004686771892011166 + 1.0 * 6.256470203399658
Epoch 1880, val loss: 1.1502892971038818
Epoch 1890, training loss: 6.254298210144043 = 0.004629961214959621 + 1.0 * 6.249668121337891
Epoch 1890, val loss: 1.1517102718353271
Epoch 1900, training loss: 6.255115032196045 = 0.004574345424771309 + 1.0 * 6.250540733337402
Epoch 1900, val loss: 1.1531749963760376
Epoch 1910, training loss: 6.257798194885254 = 0.004519416019320488 + 1.0 * 6.253278732299805
Epoch 1910, val loss: 1.1546125411987305
Epoch 1920, training loss: 6.259807586669922 = 0.0044655888341367245 + 1.0 * 6.25534200668335
Epoch 1920, val loss: 1.155902624130249
Epoch 1930, training loss: 6.253827095031738 = 0.004414043854922056 + 1.0 * 6.249413013458252
Epoch 1930, val loss: 1.1572306156158447
Epoch 1940, training loss: 6.253691673278809 = 0.004363877698779106 + 1.0 * 6.249327659606934
Epoch 1940, val loss: 1.158765196800232
Epoch 1950, training loss: 6.253011226654053 = 0.004313951823860407 + 1.0 * 6.248697280883789
Epoch 1950, val loss: 1.1602082252502441
Epoch 1960, training loss: 6.263763904571533 = 0.004265167284756899 + 1.0 * 6.259498596191406
Epoch 1960, val loss: 1.1614736318588257
Epoch 1970, training loss: 6.255468845367432 = 0.004215983673930168 + 1.0 * 6.2512526512146
Epoch 1970, val loss: 1.1627018451690674
Epoch 1980, training loss: 6.2527384757995605 = 0.004169359803199768 + 1.0 * 6.248569011688232
Epoch 1980, val loss: 1.1640220880508423
Epoch 1990, training loss: 6.258641242980957 = 0.004122837446630001 + 1.0 * 6.254518508911133
Epoch 1990, val loss: 1.1654385328292847
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8460727464417502
=== training gcn model ===
Epoch 0, training loss: 10.55751895904541 = 1.960678219795227 + 1.0 * 8.596840858459473
Epoch 0, val loss: 1.9526100158691406
Epoch 10, training loss: 10.546341896057129 = 1.9498103857040405 + 1.0 * 8.596531867980957
Epoch 10, val loss: 1.9414657354354858
Epoch 20, training loss: 10.529938697814941 = 1.9362596273422241 + 1.0 * 8.593679428100586
Epoch 20, val loss: 1.9274016618728638
Epoch 30, training loss: 10.486574172973633 = 1.917629599571228 + 1.0 * 8.568944931030273
Epoch 30, val loss: 1.9081287384033203
Epoch 40, training loss: 10.296805381774902 = 1.8934043645858765 + 1.0 * 8.403401374816895
Epoch 40, val loss: 1.8840363025665283
Epoch 50, training loss: 9.762928009033203 = 1.8671602010726929 + 1.0 * 7.895768165588379
Epoch 50, val loss: 1.8591173887252808
Epoch 60, training loss: 9.339133262634277 = 1.8439241647720337 + 1.0 * 7.495208740234375
Epoch 60, val loss: 1.8379122018814087
Epoch 70, training loss: 8.936042785644531 = 1.8214092254638672 + 1.0 * 7.114633560180664
Epoch 70, val loss: 1.8167200088500977
Epoch 80, training loss: 8.73440933227539 = 1.7995245456695557 + 1.0 * 6.934884548187256
Epoch 80, val loss: 1.7964171171188354
Epoch 90, training loss: 8.589609146118164 = 1.7769750356674194 + 1.0 * 6.812633991241455
Epoch 90, val loss: 1.7763038873672485
Epoch 100, training loss: 8.484884262084961 = 1.7538175582885742 + 1.0 * 6.731067180633545
Epoch 100, val loss: 1.7560452222824097
Epoch 110, training loss: 8.40117359161377 = 1.729002833366394 + 1.0 * 6.672171115875244
Epoch 110, val loss: 1.7343149185180664
Epoch 120, training loss: 8.327702522277832 = 1.7015639543533325 + 1.0 * 6.626138210296631
Epoch 120, val loss: 1.7101842164993286
Epoch 130, training loss: 8.262666702270508 = 1.6714078187942505 + 1.0 * 6.591258525848389
Epoch 130, val loss: 1.6838433742523193
Epoch 140, training loss: 8.201160430908203 = 1.638426423072815 + 1.0 * 6.5627336502075195
Epoch 140, val loss: 1.6548362970352173
Epoch 150, training loss: 8.13742446899414 = 1.601502537727356 + 1.0 * 6.535922050476074
Epoch 150, val loss: 1.6222460269927979
Epoch 160, training loss: 8.073787689208984 = 1.560340166091919 + 1.0 * 6.513447284698486
Epoch 160, val loss: 1.5857861042022705
Epoch 170, training loss: 8.010388374328613 = 1.5151275396347046 + 1.0 * 6.495261192321777
Epoch 170, val loss: 1.5459026098251343
Epoch 180, training loss: 7.945596218109131 = 1.4663413763046265 + 1.0 * 6.479254722595215
Epoch 180, val loss: 1.5030525922775269
Epoch 190, training loss: 7.884077548980713 = 1.414412498474121 + 1.0 * 6.469665050506592
Epoch 190, val loss: 1.4578971862792969
Epoch 200, training loss: 7.817502975463867 = 1.360675573348999 + 1.0 * 6.456827640533447
Epoch 200, val loss: 1.4117416143417358
Epoch 210, training loss: 7.755860328674316 = 1.305938959121704 + 1.0 * 6.449921131134033
Epoch 210, val loss: 1.3652552366256714
Epoch 220, training loss: 7.693634986877441 = 1.2511627674102783 + 1.0 * 6.442471981048584
Epoch 220, val loss: 1.3194268941879272
Epoch 230, training loss: 7.631575107574463 = 1.1970282793045044 + 1.0 * 6.434546947479248
Epoch 230, val loss: 1.2748721837997437
Epoch 240, training loss: 7.571476459503174 = 1.1436885595321655 + 1.0 * 6.427787780761719
Epoch 240, val loss: 1.23173189163208
Epoch 250, training loss: 7.519351482391357 = 1.0913991928100586 + 1.0 * 6.427952289581299
Epoch 250, val loss: 1.1902830600738525
Epoch 260, training loss: 7.460576057434082 = 1.0415476560592651 + 1.0 * 6.419028282165527
Epoch 260, val loss: 1.1514346599578857
Epoch 270, training loss: 7.404642105102539 = 0.9935052394866943 + 1.0 * 6.411137104034424
Epoch 270, val loss: 1.1147663593292236
Epoch 280, training loss: 7.3607258796691895 = 0.9475716948509216 + 1.0 * 6.413154125213623
Epoch 280, val loss: 1.0802996158599854
Epoch 290, training loss: 7.306460857391357 = 0.9043039679527283 + 1.0 * 6.402156829833984
Epoch 290, val loss: 1.0482892990112305
Epoch 300, training loss: 7.258984088897705 = 0.8630256652832031 + 1.0 * 6.395958423614502
Epoch 300, val loss: 1.0183053016662598
Epoch 310, training loss: 7.217692852020264 = 0.8236480951309204 + 1.0 * 6.394044876098633
Epoch 310, val loss: 0.9901270270347595
Epoch 320, training loss: 7.174882411956787 = 0.7867615818977356 + 1.0 * 6.388120651245117
Epoch 320, val loss: 0.964221179485321
Epoch 330, training loss: 7.136377334594727 = 0.752183198928833 + 1.0 * 6.384194374084473
Epoch 330, val loss: 0.9405152201652527
Epoch 340, training loss: 7.098004341125488 = 0.7193103432655334 + 1.0 * 6.3786940574646
Epoch 340, val loss: 0.9185135960578918
Epoch 350, training loss: 7.062620162963867 = 0.6879916787147522 + 1.0 * 6.37462854385376
Epoch 350, val loss: 0.8980489373207092
Epoch 360, training loss: 7.033360958099365 = 0.6581183075904846 + 1.0 * 6.375242710113525
Epoch 360, val loss: 0.8792110085487366
Epoch 370, training loss: 6.999862194061279 = 0.6300978660583496 + 1.0 * 6.36976432800293
Epoch 370, val loss: 0.8622272610664368
Epoch 380, training loss: 6.968916416168213 = 0.6032424569129944 + 1.0 * 6.365674018859863
Epoch 380, val loss: 0.8466475605964661
Epoch 390, training loss: 6.946834564208984 = 0.5774166584014893 + 1.0 * 6.369418144226074
Epoch 390, val loss: 0.8322741389274597
Epoch 400, training loss: 6.912059307098389 = 0.5526741743087769 + 1.0 * 6.359385013580322
Epoch 400, val loss: 0.8192052245140076
Epoch 410, training loss: 6.886138916015625 = 0.5288137793540955 + 1.0 * 6.357325077056885
Epoch 410, val loss: 0.8073417544364929
Epoch 420, training loss: 6.859574317932129 = 0.5057732462882996 + 1.0 * 6.353801250457764
Epoch 420, val loss: 0.796597421169281
Epoch 430, training loss: 6.83442497253418 = 0.4834928810596466 + 1.0 * 6.3509321212768555
Epoch 430, val loss: 0.7869840264320374
Epoch 440, training loss: 6.814499378204346 = 0.46194007992744446 + 1.0 * 6.3525590896606445
Epoch 440, val loss: 0.7783792614936829
Epoch 450, training loss: 6.788388729095459 = 0.44111400842666626 + 1.0 * 6.3472747802734375
Epoch 450, val loss: 0.7708399295806885
Epoch 460, training loss: 6.768060684204102 = 0.42105376720428467 + 1.0 * 6.347006797790527
Epoch 460, val loss: 0.7642073035240173
Epoch 470, training loss: 6.745229244232178 = 0.40163013339042664 + 1.0 * 6.343599319458008
Epoch 470, val loss: 0.7583907246589661
Epoch 480, training loss: 6.722590923309326 = 0.38289716839790344 + 1.0 * 6.339693546295166
Epoch 480, val loss: 0.7532923221588135
Epoch 490, training loss: 6.700999736785889 = 0.36469051241874695 + 1.0 * 6.336309432983398
Epoch 490, val loss: 0.7488790154457092
Epoch 500, training loss: 6.686103820800781 = 0.34701964259147644 + 1.0 * 6.339084148406982
Epoch 500, val loss: 0.7449890971183777
Epoch 510, training loss: 6.667318820953369 = 0.3299301862716675 + 1.0 * 6.337388515472412
Epoch 510, val loss: 0.7416714429855347
Epoch 520, training loss: 6.644077301025391 = 0.3134188652038574 + 1.0 * 6.330658435821533
Epoch 520, val loss: 0.7388824224472046
Epoch 530, training loss: 6.632521152496338 = 0.29741525650024414 + 1.0 * 6.335105895996094
Epoch 530, val loss: 0.7365655303001404
Epoch 540, training loss: 6.613282203674316 = 0.28197264671325684 + 1.0 * 6.331309795379639
Epoch 540, val loss: 0.7347829341888428
Epoch 550, training loss: 6.592385768890381 = 0.26714497804641724 + 1.0 * 6.325240612030029
Epoch 550, val loss: 0.7335361838340759
Epoch 560, training loss: 6.57701301574707 = 0.25287652015686035 + 1.0 * 6.324136257171631
Epoch 560, val loss: 0.7327439188957214
Epoch 570, training loss: 6.573522567749023 = 0.2392280548810959 + 1.0 * 6.334294319152832
Epoch 570, val loss: 0.7324907183647156
Epoch 580, training loss: 6.55169153213501 = 0.2263766974210739 + 1.0 * 6.325314998626709
Epoch 580, val loss: 0.7327906489372253
Epoch 590, training loss: 6.535278797149658 = 0.2142045944929123 + 1.0 * 6.32107400894165
Epoch 590, val loss: 0.7336626648902893
Epoch 600, training loss: 6.523323059082031 = 0.20267023146152496 + 1.0 * 6.320652961730957
Epoch 600, val loss: 0.7350577712059021
Epoch 610, training loss: 6.507734775543213 = 0.19178827106952667 + 1.0 * 6.315946578979492
Epoch 610, val loss: 0.7370595932006836
Epoch 620, training loss: 6.496257305145264 = 0.18154209852218628 + 1.0 * 6.314715385437012
Epoch 620, val loss: 0.7395972609519958
Epoch 630, training loss: 6.486556529998779 = 0.17189517617225647 + 1.0 * 6.314661502838135
Epoch 630, val loss: 0.7426307797431946
Epoch 640, training loss: 6.490562438964844 = 0.16281573474407196 + 1.0 * 6.327746868133545
Epoch 640, val loss: 0.7461717128753662
Epoch 650, training loss: 6.465743064880371 = 0.15433916449546814 + 1.0 * 6.311403751373291
Epoch 650, val loss: 0.7501525282859802
Epoch 660, training loss: 6.457816123962402 = 0.14638271927833557 + 1.0 * 6.3114333152771
Epoch 660, val loss: 0.7546000480651855
Epoch 670, training loss: 6.453711986541748 = 0.13890571892261505 + 1.0 * 6.3148064613342285
Epoch 670, val loss: 0.7594186067581177
Epoch 680, training loss: 6.443271160125732 = 0.13187943398952484 + 1.0 * 6.311391830444336
Epoch 680, val loss: 0.7646013498306274
Epoch 690, training loss: 6.433160305023193 = 0.12530238926410675 + 1.0 * 6.307857990264893
Epoch 690, val loss: 0.7700703740119934
Epoch 700, training loss: 6.426326751708984 = 0.11910469084978104 + 1.0 * 6.30722188949585
Epoch 700, val loss: 0.7758562564849854
Epoch 710, training loss: 6.421973705291748 = 0.11328381299972534 + 1.0 * 6.308690071105957
Epoch 710, val loss: 0.7818110585212708
Epoch 720, training loss: 6.414384841918945 = 0.10782160609960556 + 1.0 * 6.306563377380371
Epoch 720, val loss: 0.7879817485809326
Epoch 730, training loss: 6.406766414642334 = 0.10267502069473267 + 1.0 * 6.304091453552246
Epoch 730, val loss: 0.794331967830658
Epoch 740, training loss: 6.398974418640137 = 0.09781701117753983 + 1.0 * 6.301157474517822
Epoch 740, val loss: 0.800861656665802
Epoch 750, training loss: 6.401978492736816 = 0.09322789311408997 + 1.0 * 6.308750629425049
Epoch 750, val loss: 0.8075320720672607
Epoch 760, training loss: 6.395977973937988 = 0.08890185505151749 + 1.0 * 6.3070759773254395
Epoch 760, val loss: 0.8142822980880737
Epoch 770, training loss: 6.383534908294678 = 0.08484044671058655 + 1.0 * 6.298694610595703
Epoch 770, val loss: 0.821148157119751
Epoch 780, training loss: 6.382208347320557 = 0.08099454641342163 + 1.0 * 6.30121374130249
Epoch 780, val loss: 0.8280792832374573
Epoch 790, training loss: 6.374241352081299 = 0.0773664116859436 + 1.0 * 6.296875
Epoch 790, val loss: 0.834997832775116
Epoch 800, training loss: 6.377325534820557 = 0.07393843680620193 + 1.0 * 6.30338716506958
Epoch 800, val loss: 0.842013418674469
Epoch 810, training loss: 6.369389057159424 = 0.07070706784725189 + 1.0 * 6.29868221282959
Epoch 810, val loss: 0.8489803075790405
Epoch 820, training loss: 6.363764762878418 = 0.06764578074216843 + 1.0 * 6.296119213104248
Epoch 820, val loss: 0.8560346961021423
Epoch 830, training loss: 6.360841274261475 = 0.06474687159061432 + 1.0 * 6.2960944175720215
Epoch 830, val loss: 0.8630569577217102
Epoch 840, training loss: 6.354544162750244 = 0.06200181320309639 + 1.0 * 6.292542457580566
Epoch 840, val loss: 0.87007075548172
Epoch 850, training loss: 6.362020492553711 = 0.05940008908510208 + 1.0 * 6.3026204109191895
Epoch 850, val loss: 0.8770869970321655
Epoch 860, training loss: 6.3517255783081055 = 0.056933946907520294 + 1.0 * 6.2947916984558105
Epoch 860, val loss: 0.8840044736862183
Epoch 870, training loss: 6.3451642990112305 = 0.05459841713309288 + 1.0 * 6.2905659675598145
Epoch 870, val loss: 0.8909634947776794
Epoch 880, training loss: 6.3483567237854 = 0.052382174879312515 + 1.0 * 6.2959747314453125
Epoch 880, val loss: 0.8978587985038757
Epoch 890, training loss: 6.339446067810059 = 0.0502762496471405 + 1.0 * 6.289169788360596
Epoch 890, val loss: 0.9046877026557922
Epoch 900, training loss: 6.336977481842041 = 0.04828592389822006 + 1.0 * 6.288691520690918
Epoch 900, val loss: 0.911493718624115
Epoch 910, training loss: 6.341682434082031 = 0.04639267921447754 + 1.0 * 6.295289516448975
Epoch 910, val loss: 0.9182239174842834
Epoch 920, training loss: 6.332979202270508 = 0.04459906369447708 + 1.0 * 6.288380146026611
Epoch 920, val loss: 0.9249210357666016
Epoch 930, training loss: 6.328787803649902 = 0.04289159178733826 + 1.0 * 6.285896301269531
Epoch 930, val loss: 0.9315766096115112
Epoch 940, training loss: 6.33332633972168 = 0.041268955916166306 + 1.0 * 6.292057514190674
Epoch 940, val loss: 0.9381917119026184
Epoch 950, training loss: 6.327429294586182 = 0.03972286358475685 + 1.0 * 6.28770637512207
Epoch 950, val loss: 0.9446699619293213
Epoch 960, training loss: 6.3245697021484375 = 0.038260918110609055 + 1.0 * 6.286308765411377
Epoch 960, val loss: 0.9511136412620544
Epoch 970, training loss: 6.320033550262451 = 0.036866527050733566 + 1.0 * 6.283166885375977
Epoch 970, val loss: 0.9575013518333435
Epoch 980, training loss: 6.320005893707275 = 0.03553793951869011 + 1.0 * 6.284468173980713
Epoch 980, val loss: 0.9638552069664001
Epoch 990, training loss: 6.317645072937012 = 0.03427013382315636 + 1.0 * 6.283374786376953
Epoch 990, val loss: 0.9700843095779419
Epoch 1000, training loss: 6.318615913391113 = 0.03306717425584793 + 1.0 * 6.285548686981201
Epoch 1000, val loss: 0.9762676358222961
Epoch 1010, training loss: 6.314603805541992 = 0.03192700073122978 + 1.0 * 6.282676696777344
Epoch 1010, val loss: 0.9823501110076904
Epoch 1020, training loss: 6.3109893798828125 = 0.030837170779705048 + 1.0 * 6.280152320861816
Epoch 1020, val loss: 0.9883984327316284
Epoch 1030, training loss: 6.308743000030518 = 0.029794728383421898 + 1.0 * 6.2789483070373535
Epoch 1030, val loss: 0.9944028258323669
Epoch 1040, training loss: 6.318396091461182 = 0.028800196945667267 + 1.0 * 6.289596080780029
Epoch 1040, val loss: 1.0003185272216797
Epoch 1050, training loss: 6.3105878829956055 = 0.027853529900312424 + 1.0 * 6.282734394073486
Epoch 1050, val loss: 1.0061413049697876
Epoch 1060, training loss: 6.307700157165527 = 0.026951070874929428 + 1.0 * 6.280749320983887
Epoch 1060, val loss: 1.0119050741195679
Epoch 1070, training loss: 6.306665420532227 = 0.02609052136540413 + 1.0 * 6.280574798583984
Epoch 1070, val loss: 1.0175710916519165
Epoch 1080, training loss: 6.302173137664795 = 0.02527071349322796 + 1.0 * 6.276902198791504
Epoch 1080, val loss: 1.023193597793579
Epoch 1090, training loss: 6.300852298736572 = 0.024483762681484222 + 1.0 * 6.276368618011475
Epoch 1090, val loss: 1.0287672281265259
Epoch 1100, training loss: 6.302142143249512 = 0.023730171844363213 + 1.0 * 6.278411865234375
Epoch 1100, val loss: 1.0342856645584106
Epoch 1110, training loss: 6.302567005157471 = 0.023009169846773148 + 1.0 * 6.279557704925537
Epoch 1110, val loss: 1.0397154092788696
Epoch 1120, training loss: 6.2988996505737305 = 0.022319231182336807 + 1.0 * 6.276580333709717
Epoch 1120, val loss: 1.0450475215911865
Epoch 1130, training loss: 6.298947334289551 = 0.021662607789039612 + 1.0 * 6.277284622192383
Epoch 1130, val loss: 1.0503493547439575
Epoch 1140, training loss: 6.296470642089844 = 0.021031120792031288 + 1.0 * 6.275439739227295
Epoch 1140, val loss: 1.05556058883667
Epoch 1150, training loss: 6.29714822769165 = 0.020428597927093506 + 1.0 * 6.276719570159912
Epoch 1150, val loss: 1.0607355833053589
Epoch 1160, training loss: 6.294459819793701 = 0.019848741590976715 + 1.0 * 6.274610996246338
Epoch 1160, val loss: 1.0658209323883057
Epoch 1170, training loss: 6.291299343109131 = 0.01929713785648346 + 1.0 * 6.272002220153809
Epoch 1170, val loss: 1.0708463191986084
Epoch 1180, training loss: 6.290303707122803 = 0.018764307722449303 + 1.0 * 6.271539211273193
Epoch 1180, val loss: 1.075831413269043
Epoch 1190, training loss: 6.296167850494385 = 0.018252843990921974 + 1.0 * 6.277915000915527
Epoch 1190, val loss: 1.0807641744613647
Epoch 1200, training loss: 6.290950298309326 = 0.017762769013643265 + 1.0 * 6.273187637329102
Epoch 1200, val loss: 1.0856084823608398
Epoch 1210, training loss: 6.289279937744141 = 0.017292823642492294 + 1.0 * 6.271986961364746
Epoch 1210, val loss: 1.0903719663619995
Epoch 1220, training loss: 6.2934675216674805 = 0.01684260182082653 + 1.0 * 6.276625156402588
Epoch 1220, val loss: 1.095112919807434
Epoch 1230, training loss: 6.2871012687683105 = 0.016409041360020638 + 1.0 * 6.270692348480225
Epoch 1230, val loss: 1.0997575521469116
Epoch 1240, training loss: 6.283900260925293 = 0.015992311760783195 + 1.0 * 6.267908096313477
Epoch 1240, val loss: 1.1043834686279297
Epoch 1250, training loss: 6.28439474105835 = 0.015590466558933258 + 1.0 * 6.26880407333374
Epoch 1250, val loss: 1.1089789867401123
Epoch 1260, training loss: 6.293025970458984 = 0.015204863622784615 + 1.0 * 6.277821063995361
Epoch 1260, val loss: 1.1134980916976929
Epoch 1270, training loss: 6.284492015838623 = 0.014829493127763271 + 1.0 * 6.269662380218506
Epoch 1270, val loss: 1.1179548501968384
Epoch 1280, training loss: 6.286961555480957 = 0.014473224058747292 + 1.0 * 6.272488117218018
Epoch 1280, val loss: 1.1223596334457397
Epoch 1290, training loss: 6.282034397125244 = 0.014127535745501518 + 1.0 * 6.267906665802002
Epoch 1290, val loss: 1.1266839504241943
Epoch 1300, training loss: 6.282083988189697 = 0.013797609135508537 + 1.0 * 6.268286228179932
Epoch 1300, val loss: 1.130965232849121
Epoch 1310, training loss: 6.28516960144043 = 0.013476978987455368 + 1.0 * 6.271692752838135
Epoch 1310, val loss: 1.1351951360702515
Epoch 1320, training loss: 6.2790727615356445 = 0.013169246725738049 + 1.0 * 6.265903472900391
Epoch 1320, val loss: 1.1393691301345825
Epoch 1330, training loss: 6.277237892150879 = 0.012870736420154572 + 1.0 * 6.26436710357666
Epoch 1330, val loss: 1.1435327529907227
Epoch 1340, training loss: 6.278601169586182 = 0.012581952847540379 + 1.0 * 6.266019344329834
Epoch 1340, val loss: 1.1476556062698364
Epoch 1350, training loss: 6.278996467590332 = 0.012302948161959648 + 1.0 * 6.266693592071533
Epoch 1350, val loss: 1.1516988277435303
Epoch 1360, training loss: 6.277344226837158 = 0.012035352177917957 + 1.0 * 6.265308856964111
Epoch 1360, val loss: 1.1556930541992188
Epoch 1370, training loss: 6.2804388999938965 = 0.011776029132306576 + 1.0 * 6.268662929534912
Epoch 1370, val loss: 1.1596348285675049
Epoch 1380, training loss: 6.280828952789307 = 0.011526846326887608 + 1.0 * 6.269301891326904
Epoch 1380, val loss: 1.1635631322860718
Epoch 1390, training loss: 6.275437355041504 = 0.011285397224128246 + 1.0 * 6.2641520500183105
Epoch 1390, val loss: 1.1673802137374878
Epoch 1400, training loss: 6.2723164558410645 = 0.011052072048187256 + 1.0 * 6.261264324188232
Epoch 1400, val loss: 1.1711894273757935
Epoch 1410, training loss: 6.272387504577637 = 0.010825086385011673 + 1.0 * 6.261562347412109
Epoch 1410, val loss: 1.1749975681304932
Epoch 1420, training loss: 6.275603771209717 = 0.01060419250279665 + 1.0 * 6.2649993896484375
Epoch 1420, val loss: 1.1787678003311157
Epoch 1430, training loss: 6.275029182434082 = 0.010392188094556332 + 1.0 * 6.264636993408203
Epoch 1430, val loss: 1.1824541091918945
Epoch 1440, training loss: 6.2706122398376465 = 0.010187407955527306 + 1.0 * 6.260424613952637
Epoch 1440, val loss: 1.1861013174057007
Epoch 1450, training loss: 6.270153522491455 = 0.009989532642066479 + 1.0 * 6.2601637840271
Epoch 1450, val loss: 1.1897073984146118
Epoch 1460, training loss: 6.271903991699219 = 0.009795866906642914 + 1.0 * 6.262108325958252
Epoch 1460, val loss: 1.1933166980743408
Epoch 1470, training loss: 6.270602703094482 = 0.009608378633856773 + 1.0 * 6.2609944343566895
Epoch 1470, val loss: 1.1968693733215332
Epoch 1480, training loss: 6.272970199584961 = 0.00942614208906889 + 1.0 * 6.263544082641602
Epoch 1480, val loss: 1.2003732919692993
Epoch 1490, training loss: 6.272229194641113 = 0.009250673465430737 + 1.0 * 6.262978553771973
Epoch 1490, val loss: 1.2038376331329346
Epoch 1500, training loss: 6.267920970916748 = 0.009080161340534687 + 1.0 * 6.258841037750244
Epoch 1500, val loss: 1.2072771787643433
Epoch 1510, training loss: 6.267304420471191 = 0.008914624340832233 + 1.0 * 6.258389949798584
Epoch 1510, val loss: 1.2106984853744507
Epoch 1520, training loss: 6.274358749389648 = 0.008752945810556412 + 1.0 * 6.265605926513672
Epoch 1520, val loss: 1.2140949964523315
Epoch 1530, training loss: 6.26949405670166 = 0.008596346713602543 + 1.0 * 6.260897636413574
Epoch 1530, val loss: 1.2174415588378906
Epoch 1540, training loss: 6.266506195068359 = 0.008444945327937603 + 1.0 * 6.258061408996582
Epoch 1540, val loss: 1.2207508087158203
Epoch 1550, training loss: 6.2645182609558105 = 0.008297093212604523 + 1.0 * 6.256221294403076
Epoch 1550, val loss: 1.2240430116653442
Epoch 1560, training loss: 6.2650675773620605 = 0.00815268699079752 + 1.0 * 6.256915092468262
Epoch 1560, val loss: 1.2273370027542114
Epoch 1570, training loss: 6.2757720947265625 = 0.008011762984097004 + 1.0 * 6.267760276794434
Epoch 1570, val loss: 1.2305724620819092
Epoch 1580, training loss: 6.266294479370117 = 0.007877877913415432 + 1.0 * 6.258416652679443
Epoch 1580, val loss: 1.2337175607681274
Epoch 1590, training loss: 6.263510227203369 = 0.00774603895843029 + 1.0 * 6.255764007568359
Epoch 1590, val loss: 1.2368566989898682
Epoch 1600, training loss: 6.271559238433838 = 0.007618388626724482 + 1.0 * 6.263940811157227
Epoch 1600, val loss: 1.2399803400039673
Epoch 1610, training loss: 6.262862205505371 = 0.007493562996387482 + 1.0 * 6.255368709564209
Epoch 1610, val loss: 1.2430195808410645
Epoch 1620, training loss: 6.2615766525268555 = 0.007371887564659119 + 1.0 * 6.254204750061035
Epoch 1620, val loss: 1.2460740804672241
Epoch 1630, training loss: 6.26259183883667 = 0.007253261748701334 + 1.0 * 6.255338668823242
Epoch 1630, val loss: 1.2491226196289062
Epoch 1640, training loss: 6.267099380493164 = 0.007137896027415991 + 1.0 * 6.2599616050720215
Epoch 1640, val loss: 1.2521387338638306
Epoch 1650, training loss: 6.260911464691162 = 0.007026174571365118 + 1.0 * 6.253885269165039
Epoch 1650, val loss: 1.2550780773162842
Epoch 1660, training loss: 6.259843349456787 = 0.006917270831763744 + 1.0 * 6.252925872802734
Epoch 1660, val loss: 1.2580054998397827
Epoch 1670, training loss: 6.261241912841797 = 0.00681072473526001 + 1.0 * 6.254431247711182
Epoch 1670, val loss: 1.2609394788742065
Epoch 1680, training loss: 6.265670299530029 = 0.006706668063998222 + 1.0 * 6.258963584899902
Epoch 1680, val loss: 1.2638568878173828
Epoch 1690, training loss: 6.262657165527344 = 0.00660462211817503 + 1.0 * 6.256052494049072
Epoch 1690, val loss: 1.2667179107666016
Epoch 1700, training loss: 6.263408184051514 = 0.006506796460598707 + 1.0 * 6.256901264190674
Epoch 1700, val loss: 1.2695492506027222
Epoch 1710, training loss: 6.257643222808838 = 0.006410014349967241 + 1.0 * 6.251233100891113
Epoch 1710, val loss: 1.2723362445831299
Epoch 1720, training loss: 6.26012659072876 = 0.006315726786851883 + 1.0 * 6.253810882568359
Epoch 1720, val loss: 1.275113821029663
Epoch 1730, training loss: 6.260104179382324 = 0.00622425926849246 + 1.0 * 6.253880023956299
Epoch 1730, val loss: 1.2778860330581665
Epoch 1740, training loss: 6.262957572937012 = 0.0061346315778791904 + 1.0 * 6.2568230628967285
Epoch 1740, val loss: 1.2806330919265747
Epoch 1750, training loss: 6.259995937347412 = 0.006046807859092951 + 1.0 * 6.253949165344238
Epoch 1750, val loss: 1.2833118438720703
Epoch 1760, training loss: 6.256681442260742 = 0.005962098482996225 + 1.0 * 6.2507195472717285
Epoch 1760, val loss: 1.2859784364700317
Epoch 1770, training loss: 6.255832672119141 = 0.005878729280084372 + 1.0 * 6.249953746795654
Epoch 1770, val loss: 1.288640022277832
Epoch 1780, training loss: 6.25979471206665 = 0.005796788726001978 + 1.0 * 6.253997802734375
Epoch 1780, val loss: 1.2913024425506592
Epoch 1790, training loss: 6.259679794311523 = 0.005716875195503235 + 1.0 * 6.253962993621826
Epoch 1790, val loss: 1.2939027547836304
Epoch 1800, training loss: 6.256724834442139 = 0.005639876704663038 + 1.0 * 6.251084804534912
Epoch 1800, val loss: 1.2964534759521484
Epoch 1810, training loss: 6.256070613861084 = 0.0055640684440732 + 1.0 * 6.250506401062012
Epoch 1810, val loss: 1.2989656925201416
Epoch 1820, training loss: 6.263322830200195 = 0.00548957334831357 + 1.0 * 6.257833480834961
Epoch 1820, val loss: 1.3014864921569824
Epoch 1830, training loss: 6.258195877075195 = 0.005417655222117901 + 1.0 * 6.252778053283691
Epoch 1830, val loss: 1.3039684295654297
Epoch 1840, training loss: 6.254099369049072 = 0.005347284954041243 + 1.0 * 6.248752117156982
Epoch 1840, val loss: 1.3064229488372803
Epoch 1850, training loss: 6.25245475769043 = 0.005277868360280991 + 1.0 * 6.2471771240234375
Epoch 1850, val loss: 1.3088767528533936
Epoch 1860, training loss: 6.255496501922607 = 0.005209449678659439 + 1.0 * 6.250287055969238
Epoch 1860, val loss: 1.3113337755203247
Epoch 1870, training loss: 6.256460189819336 = 0.005142813082784414 + 1.0 * 6.251317501068115
Epoch 1870, val loss: 1.3137612342834473
Epoch 1880, training loss: 6.2548065185546875 = 0.005078054964542389 + 1.0 * 6.249728679656982
Epoch 1880, val loss: 1.3161519765853882
Epoch 1890, training loss: 6.252402305603027 = 0.005014719441533089 + 1.0 * 6.247387409210205
Epoch 1890, val loss: 1.3185096979141235
Epoch 1900, training loss: 6.254948616027832 = 0.00495240930467844 + 1.0 * 6.249996185302734
Epoch 1900, val loss: 1.3208414316177368
Epoch 1910, training loss: 6.2542219161987305 = 0.004891426768153906 + 1.0 * 6.249330520629883
Epoch 1910, val loss: 1.3231866359710693
Epoch 1920, training loss: 6.256167411804199 = 0.004831989295780659 + 1.0 * 6.251335620880127
Epoch 1920, val loss: 1.3254625797271729
Epoch 1930, training loss: 6.251107215881348 = 0.004773973487317562 + 1.0 * 6.246333122253418
Epoch 1930, val loss: 1.327746033668518
Epoch 1940, training loss: 6.24981164932251 = 0.004716907627880573 + 1.0 * 6.2450947761535645
Epoch 1940, val loss: 1.3300130367279053
Epoch 1950, training loss: 6.253129959106445 = 0.004660679493099451 + 1.0 * 6.248469352722168
Epoch 1950, val loss: 1.3322985172271729
Epoch 1960, training loss: 6.25457763671875 = 0.004605396185070276 + 1.0 * 6.249972343444824
Epoch 1960, val loss: 1.3345516920089722
Epoch 1970, training loss: 6.252053737640381 = 0.004551034886389971 + 1.0 * 6.24750280380249
Epoch 1970, val loss: 1.3367443084716797
Epoch 1980, training loss: 6.251597881317139 = 0.00449881237000227 + 1.0 * 6.247098922729492
Epoch 1980, val loss: 1.3389196395874023
Epoch 1990, training loss: 6.251730442047119 = 0.004446923732757568 + 1.0 * 6.247283458709717
Epoch 1990, val loss: 1.3410755395889282
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8360569319978914
The final CL Acc:0.81235, 0.00924, The final GNN Acc:0.84098, 0.00409
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10604])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.540704727172852 = 1.9439027309417725 + 1.0 * 8.5968017578125
Epoch 0, val loss: 1.948561191558838
Epoch 10, training loss: 10.530384063720703 = 1.933944821357727 + 1.0 * 8.596439361572266
Epoch 10, val loss: 1.9379796981811523
Epoch 20, training loss: 10.515143394470215 = 1.9216078519821167 + 1.0 * 8.593535423278809
Epoch 20, val loss: 1.9248203039169312
Epoch 30, training loss: 10.47800350189209 = 1.904444932937622 + 1.0 * 8.573558807373047
Epoch 30, val loss: 1.9064394235610962
Epoch 40, training loss: 10.358600616455078 = 1.88227117061615 + 1.0 * 8.476329803466797
Epoch 40, val loss: 1.8835086822509766
Epoch 50, training loss: 9.933110237121582 = 1.858953833580017 + 1.0 * 8.074156761169434
Epoch 50, val loss: 1.8605328798294067
Epoch 60, training loss: 9.483527183532715 = 1.8386770486831665 + 1.0 * 7.64484977722168
Epoch 60, val loss: 1.842215895652771
Epoch 70, training loss: 9.027713775634766 = 1.823344111442566 + 1.0 * 7.20436954498291
Epoch 70, val loss: 1.8280900716781616
Epoch 80, training loss: 8.811349868774414 = 1.8087964057922363 + 1.0 * 7.0025529861450195
Epoch 80, val loss: 1.8151642084121704
Epoch 90, training loss: 8.698019981384277 = 1.792077660560608 + 1.0 * 6.905941963195801
Epoch 90, val loss: 1.8002065420150757
Epoch 100, training loss: 8.600076675415039 = 1.7745729684829712 + 1.0 * 6.825503826141357
Epoch 100, val loss: 1.784752607345581
Epoch 110, training loss: 8.520075798034668 = 1.7578364610671997 + 1.0 * 6.762239456176758
Epoch 110, val loss: 1.7702833414077759
Epoch 120, training loss: 8.447294235229492 = 1.7405426502227783 + 1.0 * 6.706751346588135
Epoch 120, val loss: 1.7552999258041382
Epoch 130, training loss: 8.382333755493164 = 1.7209571599960327 + 1.0 * 6.661376476287842
Epoch 130, val loss: 1.7384370565414429
Epoch 140, training loss: 8.32719898223877 = 1.6981359720230103 + 1.0 * 6.629063129425049
Epoch 140, val loss: 1.719236135482788
Epoch 150, training loss: 8.273761749267578 = 1.6715248823165894 + 1.0 * 6.602237224578857
Epoch 150, val loss: 1.6973315477371216
Epoch 160, training loss: 8.217201232910156 = 1.640701174736023 + 1.0 * 6.576500415802002
Epoch 160, val loss: 1.6722126007080078
Epoch 170, training loss: 8.157965660095215 = 1.6056593656539917 + 1.0 * 6.552306652069092
Epoch 170, val loss: 1.6438837051391602
Epoch 180, training loss: 8.098448753356934 = 1.5661534070968628 + 1.0 * 6.5322957038879395
Epoch 180, val loss: 1.6117204427719116
Epoch 190, training loss: 8.03557014465332 = 1.521610975265503 + 1.0 * 6.513958930969238
Epoch 190, val loss: 1.575600504875183
Epoch 200, training loss: 7.976164817810059 = 1.4722132682800293 + 1.0 * 6.503951549530029
Epoch 200, val loss: 1.5358084440231323
Epoch 210, training loss: 7.910571098327637 = 1.420730710029602 + 1.0 * 6.489840507507324
Epoch 210, val loss: 1.4945999383926392
Epoch 220, training loss: 7.846194267272949 = 1.3679932355880737 + 1.0 * 6.478200912475586
Epoch 220, val loss: 1.4528319835662842
Epoch 230, training loss: 7.782802581787109 = 1.3145532608032227 + 1.0 * 6.468249320983887
Epoch 230, val loss: 1.4110828638076782
Epoch 240, training loss: 7.724881649017334 = 1.261704921722412 + 1.0 * 6.463176727294922
Epoch 240, val loss: 1.3707464933395386
Epoch 250, training loss: 7.663221836090088 = 1.2112064361572266 + 1.0 * 6.452015399932861
Epoch 250, val loss: 1.3334548473358154
Epoch 260, training loss: 7.60798454284668 = 1.1627413034439087 + 1.0 * 6.4452433586120605
Epoch 260, val loss: 1.2985728979110718
Epoch 270, training loss: 7.555441379547119 = 1.1163792610168457 + 1.0 * 6.439062118530273
Epoch 270, val loss: 1.2663053274154663
Epoch 280, training loss: 7.511317729949951 = 1.0727239847183228 + 1.0 * 6.438593864440918
Epoch 280, val loss: 1.2367867231369019
Epoch 290, training loss: 7.461267471313477 = 1.031758189201355 + 1.0 * 6.429509162902832
Epoch 290, val loss: 1.209969162940979
Epoch 300, training loss: 7.416816711425781 = 0.9927350282669067 + 1.0 * 6.424081802368164
Epoch 300, val loss: 1.185005784034729
Epoch 310, training loss: 7.37875509262085 = 0.9552949666976929 + 1.0 * 6.423460006713867
Epoch 310, val loss: 1.1613727807998657
Epoch 320, training loss: 7.33683443069458 = 0.9194594025611877 + 1.0 * 6.417375087738037
Epoch 320, val loss: 1.1390513181686401
Epoch 330, training loss: 7.298401832580566 = 0.8844950199127197 + 1.0 * 6.413906574249268
Epoch 330, val loss: 1.1174815893173218
Epoch 340, training loss: 7.259355545043945 = 0.8501303195953369 + 1.0 * 6.409224987030029
Epoch 340, val loss: 1.09642493724823
Epoch 350, training loss: 7.223783493041992 = 0.8160880208015442 + 1.0 * 6.407695293426514
Epoch 350, val loss: 1.0758851766586304
Epoch 360, training loss: 7.1916890144348145 = 0.7827762961387634 + 1.0 * 6.408912658691406
Epoch 360, val loss: 1.055803894996643
Epoch 370, training loss: 7.150016784667969 = 0.7502025365829468 + 1.0 * 6.399814128875732
Epoch 370, val loss: 1.0365318059921265
Epoch 380, training loss: 7.114838123321533 = 0.7182368040084839 + 1.0 * 6.39660120010376
Epoch 380, val loss: 1.0179790258407593
Epoch 390, training loss: 7.085285663604736 = 0.6870124936103821 + 1.0 * 6.39827299118042
Epoch 390, val loss: 1.0001550912857056
Epoch 400, training loss: 7.04912805557251 = 0.6567807197570801 + 1.0 * 6.39234733581543
Epoch 400, val loss: 0.98337322473526
Epoch 410, training loss: 7.018418312072754 = 0.6276749968528748 + 1.0 * 6.390743255615234
Epoch 410, val loss: 0.9678347110748291
Epoch 420, training loss: 6.992310047149658 = 0.5997496843338013 + 1.0 * 6.3925604820251465
Epoch 420, val loss: 0.9534203410148621
Epoch 430, training loss: 6.958949565887451 = 0.5729554295539856 + 1.0 * 6.385993957519531
Epoch 430, val loss: 0.9402818083763123
Epoch 440, training loss: 6.927729606628418 = 0.5472034215927124 + 1.0 * 6.380526065826416
Epoch 440, val loss: 0.9284670948982239
Epoch 450, training loss: 6.900694847106934 = 0.5222707986831665 + 1.0 * 6.378424167633057
Epoch 450, val loss: 0.9176405072212219
Epoch 460, training loss: 6.875236511230469 = 0.49813660979270935 + 1.0 * 6.377099990844727
Epoch 460, val loss: 0.9079988598823547
Epoch 470, training loss: 6.84753942489624 = 0.4748115539550781 + 1.0 * 6.372727870941162
Epoch 470, val loss: 0.8993445634841919
Epoch 480, training loss: 6.822871208190918 = 0.4520357549190521 + 1.0 * 6.370835304260254
Epoch 480, val loss: 0.8914690613746643
Epoch 490, training loss: 6.799668312072754 = 0.42980363965034485 + 1.0 * 6.369864463806152
Epoch 490, val loss: 0.8844067454338074
Epoch 500, training loss: 6.775899410247803 = 0.408148854970932 + 1.0 * 6.367750644683838
Epoch 500, val loss: 0.8780245184898376
Epoch 510, training loss: 6.750990390777588 = 0.38694316148757935 + 1.0 * 6.364047050476074
Epoch 510, val loss: 0.8723999857902527
Epoch 520, training loss: 6.735306739807129 = 0.36615410447120667 + 1.0 * 6.369152545928955
Epoch 520, val loss: 0.8672804832458496
Epoch 530, training loss: 6.709253787994385 = 0.3460373282432556 + 1.0 * 6.363216400146484
Epoch 530, val loss: 0.8627446293830872
Epoch 540, training loss: 6.684876441955566 = 0.3265721797943115 + 1.0 * 6.358304500579834
Epoch 540, val loss: 0.8589336276054382
Epoch 550, training loss: 6.6647210121154785 = 0.30771809816360474 + 1.0 * 6.3570027351379395
Epoch 550, val loss: 0.8556644916534424
Epoch 560, training loss: 6.647420406341553 = 0.28956377506256104 + 1.0 * 6.357856750488281
Epoch 560, val loss: 0.8529877662658691
Epoch 570, training loss: 6.631951808929443 = 0.272229939699173 + 1.0 * 6.359721660614014
Epoch 570, val loss: 0.8508731126785278
Epoch 580, training loss: 6.610165596008301 = 0.25580787658691406 + 1.0 * 6.354357719421387
Epoch 580, val loss: 0.8493471145629883
Epoch 590, training loss: 6.590272426605225 = 0.2402084767818451 + 1.0 * 6.350063800811768
Epoch 590, val loss: 0.8482815027236938
Epoch 600, training loss: 6.572750568389893 = 0.22542443871498108 + 1.0 * 6.347326278686523
Epoch 600, val loss: 0.8476983308792114
Epoch 610, training loss: 6.556649684906006 = 0.21141399443149567 + 1.0 * 6.345235824584961
Epoch 610, val loss: 0.8475713133811951
Epoch 620, training loss: 6.546614646911621 = 0.19821278750896454 + 1.0 * 6.34840202331543
Epoch 620, val loss: 0.8478553295135498
Epoch 630, training loss: 6.532251834869385 = 0.18588295578956604 + 1.0 * 6.346368789672852
Epoch 630, val loss: 0.8484343886375427
Epoch 640, training loss: 6.516455173492432 = 0.17432886362075806 + 1.0 * 6.342126369476318
Epoch 640, val loss: 0.8495092988014221
Epoch 650, training loss: 6.503790378570557 = 0.16346774995326996 + 1.0 * 6.340322494506836
Epoch 650, val loss: 0.8509286046028137
Epoch 660, training loss: 6.515629768371582 = 0.15326760709285736 + 1.0 * 6.362362384796143
Epoch 660, val loss: 0.8526067137718201
Epoch 670, training loss: 6.484832763671875 = 0.14383943378925323 + 1.0 * 6.340993404388428
Epoch 670, val loss: 0.8543887138366699
Epoch 680, training loss: 6.47232723236084 = 0.13504540920257568 + 1.0 * 6.337281703948975
Epoch 680, val loss: 0.8566981554031372
Epoch 690, training loss: 6.460867404937744 = 0.12682980298995972 + 1.0 * 6.334037780761719
Epoch 690, val loss: 0.8592610955238342
Epoch 700, training loss: 6.451021671295166 = 0.11913792788982391 + 1.0 * 6.331883907318115
Epoch 700, val loss: 0.8620951175689697
Epoch 710, training loss: 6.464004993438721 = 0.11196387559175491 + 1.0 * 6.352041244506836
Epoch 710, val loss: 0.8651410937309265
Epoch 720, training loss: 6.439244747161865 = 0.10532183945178986 + 1.0 * 6.333922863006592
Epoch 720, val loss: 0.8682947754859924
Epoch 730, training loss: 6.4284257888793945 = 0.0991639718413353 + 1.0 * 6.329261779785156
Epoch 730, val loss: 0.8718183040618896
Epoch 740, training loss: 6.420222282409668 = 0.093421071767807 + 1.0 * 6.326801300048828
Epoch 740, val loss: 0.8755416870117188
Epoch 750, training loss: 6.41731071472168 = 0.08806248754262924 + 1.0 * 6.329248428344727
Epoch 750, val loss: 0.8794529438018799
Epoch 760, training loss: 6.416145324707031 = 0.08310054987668991 + 1.0 * 6.33304500579834
Epoch 760, val loss: 0.8833271861076355
Epoch 770, training loss: 6.403366565704346 = 0.0785042867064476 + 1.0 * 6.324862480163574
Epoch 770, val loss: 0.8875579237937927
Epoch 780, training loss: 6.400260925292969 = 0.07422807067632675 + 1.0 * 6.326032638549805
Epoch 780, val loss: 0.8918836116790771
Epoch 790, training loss: 6.391260147094727 = 0.07024802267551422 + 1.0 * 6.321012020111084
Epoch 790, val loss: 0.8962312340736389
Epoch 800, training loss: 6.386466979980469 = 0.06654985249042511 + 1.0 * 6.31991720199585
Epoch 800, val loss: 0.9007561206817627
Epoch 810, training loss: 6.381697654724121 = 0.06309906393289566 + 1.0 * 6.318598747253418
Epoch 810, val loss: 0.9054141044616699
Epoch 820, training loss: 6.378662586212158 = 0.059873346239328384 + 1.0 * 6.318789005279541
Epoch 820, val loss: 0.9101194739341736
Epoch 830, training loss: 6.385186672210693 = 0.056867264211177826 + 1.0 * 6.328319549560547
Epoch 830, val loss: 0.9147617816925049
Epoch 840, training loss: 6.373439788818359 = 0.05407533049583435 + 1.0 * 6.319364547729492
Epoch 840, val loss: 0.9193785786628723
Epoch 850, training loss: 6.369338035583496 = 0.05147816985845566 + 1.0 * 6.317859649658203
Epoch 850, val loss: 0.9241447448730469
Epoch 860, training loss: 6.367648124694824 = 0.04904914274811745 + 1.0 * 6.318598747253418
Epoch 860, val loss: 0.9288524389266968
Epoch 870, training loss: 6.360682487487793 = 0.04677504301071167 + 1.0 * 6.313907623291016
Epoch 870, val loss: 0.933611273765564
Epoch 880, training loss: 6.359194278717041 = 0.044649336487054825 + 1.0 * 6.314545154571533
Epoch 880, val loss: 0.9383730292320251
Epoch 890, training loss: 6.358249664306641 = 0.042661458253860474 + 1.0 * 6.315587997436523
Epoch 890, val loss: 0.9430457353591919
Epoch 900, training loss: 6.353106498718262 = 0.04079713672399521 + 1.0 * 6.312309265136719
Epoch 900, val loss: 0.9477691650390625
Epoch 910, training loss: 6.3519158363342285 = 0.03905234858393669 + 1.0 * 6.312863349914551
Epoch 910, val loss: 0.9524956345558167
Epoch 920, training loss: 6.3466668128967285 = 0.03741248697042465 + 1.0 * 6.309254169464111
Epoch 920, val loss: 0.9571321606636047
Epoch 930, training loss: 6.3457183837890625 = 0.03586970642209053 + 1.0 * 6.309848785400391
Epoch 930, val loss: 0.9618406891822815
Epoch 940, training loss: 6.350471019744873 = 0.03441938757896423 + 1.0 * 6.316051483154297
Epoch 940, val loss: 0.9664167761802673
Epoch 950, training loss: 6.340810775756836 = 0.03306111320853233 + 1.0 * 6.3077497482299805
Epoch 950, val loss: 0.9709003567695618
Epoch 960, training loss: 6.336849212646484 = 0.031781360507011414 + 1.0 * 6.305068016052246
Epoch 960, val loss: 0.9755969047546387
Epoch 970, training loss: 6.337977409362793 = 0.03057003952562809 + 1.0 * 6.307407379150391
Epoch 970, val loss: 0.9801645874977112
Epoch 980, training loss: 6.3349127769470215 = 0.02943022921681404 + 1.0 * 6.305482387542725
Epoch 980, val loss: 0.9844619035720825
Epoch 990, training loss: 6.332972049713135 = 0.028356224298477173 + 1.0 * 6.3046159744262695
Epoch 990, val loss: 0.9888835549354553
Epoch 1000, training loss: 6.330142498016357 = 0.027342014014720917 + 1.0 * 6.30280065536499
Epoch 1000, val loss: 0.9933499097824097
Epoch 1010, training loss: 6.328502655029297 = 0.02637588232755661 + 1.0 * 6.302126884460449
Epoch 1010, val loss: 0.9977537989616394
Epoch 1020, training loss: 6.333532810211182 = 0.02546130120754242 + 1.0 * 6.308071613311768
Epoch 1020, val loss: 1.0019527673721313
Epoch 1030, training loss: 6.326318740844727 = 0.024595825001597404 + 1.0 * 6.301723003387451
Epoch 1030, val loss: 1.0061380863189697
Epoch 1040, training loss: 6.325910568237305 = 0.02377694845199585 + 1.0 * 6.302133560180664
Epoch 1040, val loss: 1.0104268789291382
Epoch 1050, training loss: 6.323766231536865 = 0.022998088970780373 + 1.0 * 6.3007683753967285
Epoch 1050, val loss: 1.0145506858825684
Epoch 1060, training loss: 6.321065902709961 = 0.022257044911384583 + 1.0 * 6.298809051513672
Epoch 1060, val loss: 1.0186823606491089
Epoch 1070, training loss: 6.320615291595459 = 0.021551616489887238 + 1.0 * 6.299063682556152
Epoch 1070, val loss: 1.0228266716003418
Epoch 1080, training loss: 6.325158596038818 = 0.020880376920104027 + 1.0 * 6.304278373718262
Epoch 1080, val loss: 1.0267555713653564
Epoch 1090, training loss: 6.319008827209473 = 0.020241446793079376 + 1.0 * 6.298767566680908
Epoch 1090, val loss: 1.0306005477905273
Epoch 1100, training loss: 6.316068649291992 = 0.019635435193777084 + 1.0 * 6.296433448791504
Epoch 1100, val loss: 1.0346239805221558
Epoch 1110, training loss: 6.314367771148682 = 0.019054735079407692 + 1.0 * 6.295312881469727
Epoch 1110, val loss: 1.0385514497756958
Epoch 1120, training loss: 6.319406986236572 = 0.018498986959457397 + 1.0 * 6.300908088684082
Epoch 1120, val loss: 1.0423822402954102
Epoch 1130, training loss: 6.317310333251953 = 0.01797023043036461 + 1.0 * 6.29934024810791
Epoch 1130, val loss: 1.046085238456726
Epoch 1140, training loss: 6.310599327087402 = 0.017465870827436447 + 1.0 * 6.29313325881958
Epoch 1140, val loss: 1.049831748008728
Epoch 1150, training loss: 6.3211212158203125 = 0.016984151676297188 + 1.0 * 6.304137229919434
Epoch 1150, val loss: 1.0535802841186523
Epoch 1160, training loss: 6.311598777770996 = 0.016525886952877045 + 1.0 * 6.29507303237915
Epoch 1160, val loss: 1.056998372077942
Epoch 1170, training loss: 6.307913303375244 = 0.016084957867860794 + 1.0 * 6.291828155517578
Epoch 1170, val loss: 1.060734748840332
Epoch 1180, training loss: 6.307393550872803 = 0.0156618170440197 + 1.0 * 6.291731834411621
Epoch 1180, val loss: 1.0643612146377563
Epoch 1190, training loss: 6.313654899597168 = 0.015254521742463112 + 1.0 * 6.298400402069092
Epoch 1190, val loss: 1.0678318738937378
Epoch 1200, training loss: 6.3057146072387695 = 0.014864739961922169 + 1.0 * 6.290849685668945
Epoch 1200, val loss: 1.0711860656738281
Epoch 1210, training loss: 6.303492546081543 = 0.01449188869446516 + 1.0 * 6.289000511169434
Epoch 1210, val loss: 1.074715495109558
Epoch 1220, training loss: 6.310796737670898 = 0.014132585376501083 + 1.0 * 6.296664237976074
Epoch 1220, val loss: 1.0781004428863525
Epoch 1230, training loss: 6.304168701171875 = 0.013786306604743004 + 1.0 * 6.290382385253906
Epoch 1230, val loss: 1.0812863111495972
Epoch 1240, training loss: 6.302135944366455 = 0.013454806990921497 + 1.0 * 6.2886810302734375
Epoch 1240, val loss: 1.0847221612930298
Epoch 1250, training loss: 6.305274486541748 = 0.013134625740349293 + 1.0 * 6.292140007019043
Epoch 1250, val loss: 1.088011622428894
Epoch 1260, training loss: 6.2995524406433105 = 0.01282668299973011 + 1.0 * 6.2867255210876465
Epoch 1260, val loss: 1.0911881923675537
Epoch 1270, training loss: 6.307769298553467 = 0.012530180625617504 + 1.0 * 6.295238971710205
Epoch 1270, val loss: 1.0944724082946777
Epoch 1280, training loss: 6.304312229156494 = 0.012246905826032162 + 1.0 * 6.292065143585205
Epoch 1280, val loss: 1.0973156690597534
Epoch 1290, training loss: 6.298094272613525 = 0.011974157765507698 + 1.0 * 6.2861199378967285
Epoch 1290, val loss: 1.1004606485366821
Epoch 1300, training loss: 6.296547889709473 = 0.01171201840043068 + 1.0 * 6.2848358154296875
Epoch 1300, val loss: 1.10362708568573
Epoch 1310, training loss: 6.296043395996094 = 0.011456843465566635 + 1.0 * 6.284586429595947
Epoch 1310, val loss: 1.1067183017730713
Epoch 1320, training loss: 6.300870895385742 = 0.011210021562874317 + 1.0 * 6.289660930633545
Epoch 1320, val loss: 1.109726071357727
Epoch 1330, training loss: 6.294600009918213 = 0.010971440933644772 + 1.0 * 6.283628463745117
Epoch 1330, val loss: 1.1126153469085693
Epoch 1340, training loss: 6.294344902038574 = 0.010741661302745342 + 1.0 * 6.283603191375732
Epoch 1340, val loss: 1.1155898571014404
Epoch 1350, training loss: 6.295470237731934 = 0.010519172996282578 + 1.0 * 6.284951210021973
Epoch 1350, val loss: 1.118524193763733
Epoch 1360, training loss: 6.29890251159668 = 0.010304232127964497 + 1.0 * 6.28859806060791
Epoch 1360, val loss: 1.1213020086288452
Epoch 1370, training loss: 6.292483329772949 = 0.010098529048264027 + 1.0 * 6.282384872436523
Epoch 1370, val loss: 1.1240577697753906
Epoch 1380, training loss: 6.292234420776367 = 0.009898990392684937 + 1.0 * 6.28233528137207
Epoch 1380, val loss: 1.1269912719726562
Epoch 1390, training loss: 6.300520896911621 = 0.009704815223813057 + 1.0 * 6.290816307067871
Epoch 1390, val loss: 1.1297239065170288
Epoch 1400, training loss: 6.291605472564697 = 0.00951786246150732 + 1.0 * 6.282087802886963
Epoch 1400, val loss: 1.132318377494812
Epoch 1410, training loss: 6.288944244384766 = 0.009336360730230808 + 1.0 * 6.279607772827148
Epoch 1410, val loss: 1.1351779699325562
Epoch 1420, training loss: 6.28888463973999 = 0.009159961715340614 + 1.0 * 6.279724597930908
Epoch 1420, val loss: 1.1379129886627197
Epoch 1430, training loss: 6.294919013977051 = 0.008988413028419018 + 1.0 * 6.285930633544922
Epoch 1430, val loss: 1.1405566930770874
Epoch 1440, training loss: 6.2944865226745605 = 0.008821962401270866 + 1.0 * 6.2856645584106445
Epoch 1440, val loss: 1.1430702209472656
Epoch 1450, training loss: 6.288854598999023 = 0.008662587031722069 + 1.0 * 6.280191898345947
Epoch 1450, val loss: 1.145649790763855
Epoch 1460, training loss: 6.286783218383789 = 0.008507588878273964 + 1.0 * 6.278275489807129
Epoch 1460, val loss: 1.1482199430465698
Epoch 1470, training loss: 6.2854485511779785 = 0.008356197737157345 + 1.0 * 6.277092456817627
Epoch 1470, val loss: 1.1508753299713135
Epoch 1480, training loss: 6.300485134124756 = 0.008208587765693665 + 1.0 * 6.292276382446289
Epoch 1480, val loss: 1.1533790826797485
Epoch 1490, training loss: 6.290651798248291 = 0.008067113347351551 + 1.0 * 6.2825846672058105
Epoch 1490, val loss: 1.1556364297866821
Epoch 1500, training loss: 6.282963275909424 = 0.007930287159979343 + 1.0 * 6.275032997131348
Epoch 1500, val loss: 1.1581116914749146
Epoch 1510, training loss: 6.283478260040283 = 0.0077970377169549465 + 1.0 * 6.275681018829346
Epoch 1510, val loss: 1.1606568098068237
Epoch 1520, training loss: 6.2865214347839355 = 0.007666200865060091 + 1.0 * 6.278855323791504
Epoch 1520, val loss: 1.1630592346191406
Epoch 1530, training loss: 6.285545825958252 = 0.007539184298366308 + 1.0 * 6.278006553649902
Epoch 1530, val loss: 1.1652863025665283
Epoch 1540, training loss: 6.282841682434082 = 0.0074165393598377705 + 1.0 * 6.275424957275391
Epoch 1540, val loss: 1.1675958633422852
Epoch 1550, training loss: 6.2813262939453125 = 0.007297365926206112 + 1.0 * 6.274028778076172
Epoch 1550, val loss: 1.1699923276901245
Epoch 1560, training loss: 6.281515121459961 = 0.007180606480687857 + 1.0 * 6.27433443069458
Epoch 1560, val loss: 1.172338843345642
Epoch 1570, training loss: 6.287752628326416 = 0.007066177669912577 + 1.0 * 6.280686378479004
Epoch 1570, val loss: 1.1745812892913818
Epoch 1580, training loss: 6.283360481262207 = 0.0069560399278998375 + 1.0 * 6.27640438079834
Epoch 1580, val loss: 1.17665433883667
Epoch 1590, training loss: 6.282769680023193 = 0.006848494987934828 + 1.0 * 6.27592134475708
Epoch 1590, val loss: 1.1789400577545166
Epoch 1600, training loss: 6.279419422149658 = 0.006744117476046085 + 1.0 * 6.272675514221191
Epoch 1600, val loss: 1.1811774969100952
Epoch 1610, training loss: 6.280479431152344 = 0.006641935091465712 + 1.0 * 6.273837566375732
Epoch 1610, val loss: 1.183417558670044
Epoch 1620, training loss: 6.282511234283447 = 0.0065421489998698235 + 1.0 * 6.2759690284729
Epoch 1620, val loss: 1.185576319694519
Epoch 1630, training loss: 6.281671524047852 = 0.006445250939577818 + 1.0 * 6.27522611618042
Epoch 1630, val loss: 1.1876287460327148
Epoch 1640, training loss: 6.277646064758301 = 0.006350784562528133 + 1.0 * 6.271295070648193
Epoch 1640, val loss: 1.1897928714752197
Epoch 1650, training loss: 6.278097152709961 = 0.006258789449930191 + 1.0 * 6.271838188171387
Epoch 1650, val loss: 1.1919573545455933
Epoch 1660, training loss: 6.285101890563965 = 0.006168989930301905 + 1.0 * 6.278933048248291
Epoch 1660, val loss: 1.193943738937378
Epoch 1670, training loss: 6.2792463302612305 = 0.0060813650488853455 + 1.0 * 6.273164749145508
Epoch 1670, val loss: 1.1958677768707275
Epoch 1680, training loss: 6.2771735191345215 = 0.005996679421514273 + 1.0 * 6.271176815032959
Epoch 1680, val loss: 1.1980369091033936
Epoch 1690, training loss: 6.279933452606201 = 0.005913223139941692 + 1.0 * 6.274020195007324
Epoch 1690, val loss: 1.2000329494476318
Epoch 1700, training loss: 6.275156497955322 = 0.005831594578921795 + 1.0 * 6.269324779510498
Epoch 1700, val loss: 1.2019679546356201
Epoch 1710, training loss: 6.280439376831055 = 0.005752203054726124 + 1.0 * 6.27468729019165
Epoch 1710, val loss: 1.20389723777771
Epoch 1720, training loss: 6.275085926055908 = 0.005674354732036591 + 1.0 * 6.269411563873291
Epoch 1720, val loss: 1.2059123516082764
Epoch 1730, training loss: 6.274136543273926 = 0.0055982740595936775 + 1.0 * 6.268538475036621
Epoch 1730, val loss: 1.207907795906067
Epoch 1740, training loss: 6.282634258270264 = 0.00552396522834897 + 1.0 * 6.2771100997924805
Epoch 1740, val loss: 1.2098615169525146
Epoch 1750, training loss: 6.278975486755371 = 0.005451700650155544 + 1.0 * 6.273523807525635
Epoch 1750, val loss: 1.2116202116012573
Epoch 1760, training loss: 6.273844242095947 = 0.005381266586482525 + 1.0 * 6.268463134765625
Epoch 1760, val loss: 1.2135252952575684
Epoch 1770, training loss: 6.2744526863098145 = 0.005312426947057247 + 1.0 * 6.269140243530273
Epoch 1770, val loss: 1.2154784202575684
Epoch 1780, training loss: 6.273632049560547 = 0.005244788248091936 + 1.0 * 6.268387317657471
Epoch 1780, val loss: 1.2172573804855347
Epoch 1790, training loss: 6.271487236022949 = 0.005178662948310375 + 1.0 * 6.266308784484863
Epoch 1790, val loss: 1.219079613685608
Epoch 1800, training loss: 6.277409553527832 = 0.005114011000841856 + 1.0 * 6.2722954750061035
Epoch 1800, val loss: 1.2209482192993164
Epoch 1810, training loss: 6.275346279144287 = 0.005050542298704386 + 1.0 * 6.2702956199646
Epoch 1810, val loss: 1.2226132154464722
Epoch 1820, training loss: 6.271040439605713 = 0.004989292938262224 + 1.0 * 6.266051292419434
Epoch 1820, val loss: 1.2243411540985107
Epoch 1830, training loss: 6.269782543182373 = 0.004929382354021072 + 1.0 * 6.264853000640869
Epoch 1830, val loss: 1.2262083292007446
Epoch 1840, training loss: 6.270256996154785 = 0.004869775380939245 + 1.0 * 6.265387058258057
Epoch 1840, val loss: 1.2279608249664307
Epoch 1850, training loss: 6.276784420013428 = 0.004811382386833429 + 1.0 * 6.271973133087158
Epoch 1850, val loss: 1.2296156883239746
Epoch 1860, training loss: 6.274095058441162 = 0.004754673223942518 + 1.0 * 6.269340515136719
Epoch 1860, val loss: 1.2312926054000854
Epoch 1870, training loss: 6.2706170082092285 = 0.004699329845607281 + 1.0 * 6.265917778015137
Epoch 1870, val loss: 1.2329566478729248
Epoch 1880, training loss: 6.2718825340271 = 0.004644983448088169 + 1.0 * 6.267237663269043
Epoch 1880, val loss: 1.2346224784851074
Epoch 1890, training loss: 6.273745059967041 = 0.004591902252286673 + 1.0 * 6.269153118133545
Epoch 1890, val loss: 1.2362539768218994
Epoch 1900, training loss: 6.268111705780029 = 0.004539641551673412 + 1.0 * 6.2635722160339355
Epoch 1900, val loss: 1.2379120588302612
Epoch 1910, training loss: 6.267259120941162 = 0.004488773178309202 + 1.0 * 6.262770175933838
Epoch 1910, val loss: 1.2395771741867065
Epoch 1920, training loss: 6.268260955810547 = 0.004438117612153292 + 1.0 * 6.26382303237915
Epoch 1920, val loss: 1.241278052330017
Epoch 1930, training loss: 6.2729716300964355 = 0.0043885293416678905 + 1.0 * 6.268583297729492
Epoch 1930, val loss: 1.2427749633789062
Epoch 1940, training loss: 6.266531944274902 = 0.00434096110984683 + 1.0 * 6.262190818786621
Epoch 1940, val loss: 1.2443509101867676
Epoch 1950, training loss: 6.2654242515563965 = 0.004294200800359249 + 1.0 * 6.261129856109619
Epoch 1950, val loss: 1.245980978012085
Epoch 1960, training loss: 6.268105506896973 = 0.004247767385095358 + 1.0 * 6.263857841491699
Epoch 1960, val loss: 1.2475690841674805
Epoch 1970, training loss: 6.269596099853516 = 0.004202107433229685 + 1.0 * 6.26539421081543
Epoch 1970, val loss: 1.249085783958435
Epoch 1980, training loss: 6.2666497230529785 = 0.004157287068665028 + 1.0 * 6.262492656707764
Epoch 1980, val loss: 1.2506353855133057
Epoch 1990, training loss: 6.266918659210205 = 0.0041137877851724625 + 1.0 * 6.262804985046387
Epoch 1990, val loss: 1.2521814107894897
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 10.5486478805542 = 1.9517955780029297 + 1.0 * 8.59685230255127
Epoch 0, val loss: 1.9493168592453003
Epoch 10, training loss: 10.53811264038086 = 1.9414620399475098 + 1.0 * 8.596651077270508
Epoch 10, val loss: 1.9386560916900635
Epoch 20, training loss: 10.52396011352539 = 1.9288661479949951 + 1.0 * 8.595093727111816
Epoch 20, val loss: 1.925767421722412
Epoch 30, training loss: 10.493861198425293 = 1.9114494323730469 + 1.0 * 8.582411766052246
Epoch 30, val loss: 1.9077582359313965
Epoch 40, training loss: 10.393301963806152 = 1.8876311779022217 + 1.0 * 8.505670547485352
Epoch 40, val loss: 1.8834969997406006
Epoch 50, training loss: 9.974926948547363 = 1.861999273300171 + 1.0 * 8.112927436828613
Epoch 50, val loss: 1.8584715127944946
Epoch 60, training loss: 9.527901649475098 = 1.8410521745681763 + 1.0 * 7.686849117279053
Epoch 60, val loss: 1.839647889137268
Epoch 70, training loss: 9.096891403198242 = 1.827484369277954 + 1.0 * 7.269407272338867
Epoch 70, val loss: 1.8263620138168335
Epoch 80, training loss: 8.89513111114502 = 1.8138569593429565 + 1.0 * 7.081274509429932
Epoch 80, val loss: 1.8125503063201904
Epoch 90, training loss: 8.730042457580566 = 1.7999584674835205 + 1.0 * 6.930084228515625
Epoch 90, val loss: 1.7988609075546265
Epoch 100, training loss: 8.608550071716309 = 1.7868808507919312 + 1.0 * 6.821669578552246
Epoch 100, val loss: 1.786403775215149
Epoch 110, training loss: 8.522141456604004 = 1.7744629383087158 + 1.0 * 6.747678756713867
Epoch 110, val loss: 1.7745829820632935
Epoch 120, training loss: 8.462668418884277 = 1.7611554861068726 + 1.0 * 6.701512813568115
Epoch 120, val loss: 1.761897325515747
Epoch 130, training loss: 8.415457725524902 = 1.745978593826294 + 1.0 * 6.669478893280029
Epoch 130, val loss: 1.7477933168411255
Epoch 140, training loss: 8.369579315185547 = 1.728569746017456 + 1.0 * 6.641009330749512
Epoch 140, val loss: 1.7322591543197632
Epoch 150, training loss: 8.326912879943848 = 1.7087345123291016 + 1.0 * 6.618178367614746
Epoch 150, val loss: 1.7149497270584106
Epoch 160, training loss: 8.27580738067627 = 1.6863919496536255 + 1.0 * 6.589415073394775
Epoch 160, val loss: 1.6954400539398193
Epoch 170, training loss: 8.22824478149414 = 1.6608407497406006 + 1.0 * 6.567404270172119
Epoch 170, val loss: 1.6733042001724243
Epoch 180, training loss: 8.180438995361328 = 1.631192684173584 + 1.0 * 6.549246311187744
Epoch 180, val loss: 1.6475920677185059
Epoch 190, training loss: 8.130104064941406 = 1.5970044136047363 + 1.0 * 6.533099174499512
Epoch 190, val loss: 1.6180627346038818
Epoch 200, training loss: 8.076637268066406 = 1.5575473308563232 + 1.0 * 6.519089698791504
Epoch 200, val loss: 1.5841070413589478
Epoch 210, training loss: 8.019291877746582 = 1.5130141973495483 + 1.0 * 6.506277561187744
Epoch 210, val loss: 1.5461241006851196
Epoch 220, training loss: 7.958099365234375 = 1.4637049436569214 + 1.0 * 6.494394302368164
Epoch 220, val loss: 1.5044958591461182
Epoch 230, training loss: 7.894587516784668 = 1.409684419631958 + 1.0 * 6.484902858734131
Epoch 230, val loss: 1.459553837776184
Epoch 240, training loss: 7.828390598297119 = 1.3517242670059204 + 1.0 * 6.476666450500488
Epoch 240, val loss: 1.4124733209609985
Epoch 250, training loss: 7.759671688079834 = 1.2917685508728027 + 1.0 * 6.467903137207031
Epoch 250, val loss: 1.364424228668213
Epoch 260, training loss: 7.68992805480957 = 1.2304189205169678 + 1.0 * 6.459509372711182
Epoch 260, val loss: 1.316058874130249
Epoch 270, training loss: 7.631389141082764 = 1.1689149141311646 + 1.0 * 6.462474346160889
Epoch 270, val loss: 1.2684179544448853
Epoch 280, training loss: 7.558883190155029 = 1.1097382307052612 + 1.0 * 6.4491448402404785
Epoch 280, val loss: 1.2231543064117432
Epoch 290, training loss: 7.494625091552734 = 1.0532314777374268 + 1.0 * 6.4413933753967285
Epoch 290, val loss: 1.180796504020691
Epoch 300, training loss: 7.438080787658691 = 0.9995934963226318 + 1.0 * 6.4384870529174805
Epoch 300, val loss: 1.141262412071228
Epoch 310, training loss: 7.380669116973877 = 0.9494885206222534 + 1.0 * 6.431180477142334
Epoch 310, val loss: 1.104780912399292
Epoch 320, training loss: 7.3305439949035645 = 0.902949869632721 + 1.0 * 6.427594184875488
Epoch 320, val loss: 1.071518898010254
Epoch 330, training loss: 7.280359745025635 = 0.8603268265724182 + 1.0 * 6.420032978057861
Epoch 330, val loss: 1.0415793657302856
Epoch 340, training loss: 7.2388129234313965 = 0.8211169838905334 + 1.0 * 6.417695999145508
Epoch 340, val loss: 1.0146526098251343
Epoch 350, training loss: 7.201431751251221 = 0.7854623794555664 + 1.0 * 6.415969371795654
Epoch 350, val loss: 0.9906806945800781
Epoch 360, training loss: 7.161437511444092 = 0.7529128193855286 + 1.0 * 6.408524513244629
Epoch 360, val loss: 0.969563901424408
Epoch 370, training loss: 7.12627649307251 = 0.7228264212608337 + 1.0 * 6.403450012207031
Epoch 370, val loss: 0.9506197571754456
Epoch 380, training loss: 7.094728469848633 = 0.6949050426483154 + 1.0 * 6.3998236656188965
Epoch 380, val loss: 0.933631420135498
Epoch 390, training loss: 7.066338539123535 = 0.66904217004776 + 1.0 * 6.39729642868042
Epoch 390, val loss: 0.918573796749115
Epoch 400, training loss: 7.03779411315918 = 0.6446303725242615 + 1.0 * 6.393163681030273
Epoch 400, val loss: 0.9050748348236084
Epoch 410, training loss: 7.015118598937988 = 0.6213186383247375 + 1.0 * 6.393799781799316
Epoch 410, val loss: 0.8926870822906494
Epoch 420, training loss: 6.9877729415893555 = 0.5990288257598877 + 1.0 * 6.388743877410889
Epoch 420, val loss: 0.8813655972480774
Epoch 430, training loss: 6.961477279663086 = 0.5776117444038391 + 1.0 * 6.3838653564453125
Epoch 430, val loss: 0.8711224794387817
Epoch 440, training loss: 6.937796592712402 = 0.5567243695259094 + 1.0 * 6.381072044372559
Epoch 440, val loss: 0.8617131114006042
Epoch 450, training loss: 6.920603275299072 = 0.5362722277641296 + 1.0 * 6.384331226348877
Epoch 450, val loss: 0.8529484272003174
Epoch 460, training loss: 6.89489221572876 = 0.5162699818611145 + 1.0 * 6.378622055053711
Epoch 460, val loss: 0.8448811769485474
Epoch 470, training loss: 6.870095729827881 = 0.4966317415237427 + 1.0 * 6.373464107513428
Epoch 470, val loss: 0.8375322222709656
Epoch 480, training loss: 6.850102424621582 = 0.477232426404953 + 1.0 * 6.372869968414307
Epoch 480, val loss: 0.8307180404663086
Epoch 490, training loss: 6.8305134773254395 = 0.4579998850822449 + 1.0 * 6.372513771057129
Epoch 490, val loss: 0.8244245648384094
Epoch 500, training loss: 6.808192253112793 = 0.43898406624794006 + 1.0 * 6.369208335876465
Epoch 500, val loss: 0.8185936808586121
Epoch 510, training loss: 6.785367488861084 = 0.4200892150402069 + 1.0 * 6.365278244018555
Epoch 510, val loss: 0.8133247494697571
Epoch 520, training loss: 6.767723083496094 = 0.40130606293678284 + 1.0 * 6.366416931152344
Epoch 520, val loss: 0.8084959387779236
Epoch 530, training loss: 6.745376110076904 = 0.3827309012413025 + 1.0 * 6.362645149230957
Epoch 530, val loss: 0.8042272329330444
Epoch 540, training loss: 6.726813793182373 = 0.3643637001514435 + 1.0 * 6.362450122833252
Epoch 540, val loss: 0.8004918694496155
Epoch 550, training loss: 6.7052412033081055 = 0.3463374376296997 + 1.0 * 6.358903884887695
Epoch 550, val loss: 0.7973847985267639
Epoch 560, training loss: 6.687821865081787 = 0.3287138342857361 + 1.0 * 6.359107971191406
Epoch 560, val loss: 0.7948299646377563
Epoch 570, training loss: 6.664697647094727 = 0.3115600049495697 + 1.0 * 6.353137493133545
Epoch 570, val loss: 0.7928328514099121
Epoch 580, training loss: 6.647337436676025 = 0.294834703207016 + 1.0 * 6.352502822875977
Epoch 580, val loss: 0.7914520502090454
Epoch 590, training loss: 6.629428386688232 = 0.27861467003822327 + 1.0 * 6.350813865661621
Epoch 590, val loss: 0.7906577587127686
Epoch 600, training loss: 6.614328861236572 = 0.2629812955856323 + 1.0 * 6.35134744644165
Epoch 600, val loss: 0.7904089689254761
Epoch 610, training loss: 6.599133014678955 = 0.24804431200027466 + 1.0 * 6.351088523864746
Epoch 610, val loss: 0.7907516956329346
Epoch 620, training loss: 6.579742431640625 = 0.23381979763507843 + 1.0 * 6.345922470092773
Epoch 620, val loss: 0.791689395904541
Epoch 630, training loss: 6.565195083618164 = 0.22027119994163513 + 1.0 * 6.344923973083496
Epoch 630, val loss: 0.7931950092315674
Epoch 640, training loss: 6.5537614822387695 = 0.20739100873470306 + 1.0 * 6.346370697021484
Epoch 640, val loss: 0.7952603101730347
Epoch 650, training loss: 6.535787582397461 = 0.19520097970962524 + 1.0 * 6.3405866622924805
Epoch 650, val loss: 0.7978727221488953
Epoch 660, training loss: 6.52233362197876 = 0.18368588387966156 + 1.0 * 6.338647842407227
Epoch 660, val loss: 0.8008602857589722
Epoch 670, training loss: 6.51300048828125 = 0.17281889915466309 + 1.0 * 6.340181827545166
Epoch 670, val loss: 0.8043201565742493
Epoch 680, training loss: 6.501828193664551 = 0.1626049429178238 + 1.0 * 6.339223384857178
Epoch 680, val loss: 0.8082178831100464
Epoch 690, training loss: 6.488531112670898 = 0.15304380655288696 + 1.0 * 6.335487365722656
Epoch 690, val loss: 0.8124585151672363
Epoch 700, training loss: 6.483492851257324 = 0.14406821131706238 + 1.0 * 6.3394246101379395
Epoch 700, val loss: 0.8171071410179138
Epoch 710, training loss: 6.471555709838867 = 0.1357065886259079 + 1.0 * 6.335849285125732
Epoch 710, val loss: 0.8219635486602783
Epoch 720, training loss: 6.458803176879883 = 0.12786948680877686 + 1.0 * 6.330933570861816
Epoch 720, val loss: 0.8271535634994507
Epoch 730, training loss: 6.449579238891602 = 0.12053846567869186 + 1.0 * 6.329041004180908
Epoch 730, val loss: 0.832726240158081
Epoch 740, training loss: 6.449165344238281 = 0.11366084963083267 + 1.0 * 6.335504531860352
Epoch 740, val loss: 0.8384891748428345
Epoch 750, training loss: 6.436639785766602 = 0.10727094858884811 + 1.0 * 6.329369068145752
Epoch 750, val loss: 0.8443198800086975
Epoch 760, training loss: 6.427406311035156 = 0.10131139308214188 + 1.0 * 6.326095104217529
Epoch 760, val loss: 0.8504718542098999
Epoch 770, training loss: 6.42002010345459 = 0.09574643522500992 + 1.0 * 6.324273586273193
Epoch 770, val loss: 0.8568080067634583
Epoch 780, training loss: 6.421991348266602 = 0.09056522697210312 + 1.0 * 6.33142614364624
Epoch 780, val loss: 0.8632702231407166
Epoch 790, training loss: 6.411144733428955 = 0.0857384130358696 + 1.0 * 6.325406551361084
Epoch 790, val loss: 0.8698577284812927
Epoch 800, training loss: 6.404687881469727 = 0.08125962316989899 + 1.0 * 6.323428153991699
Epoch 800, val loss: 0.8764384984970093
Epoch 810, training loss: 6.39716100692749 = 0.07708291709423065 + 1.0 * 6.320077896118164
Epoch 810, val loss: 0.8831607103347778
Epoch 820, training loss: 6.390993595123291 = 0.07318677008152008 + 1.0 * 6.317806720733643
Epoch 820, val loss: 0.8899443745613098
Epoch 830, training loss: 6.391551971435547 = 0.0695418044924736 + 1.0 * 6.322010040283203
Epoch 830, val loss: 0.89680415391922
Epoch 840, training loss: 6.382726192474365 = 0.06614422053098679 + 1.0 * 6.316582202911377
Epoch 840, val loss: 0.9036267995834351
Epoch 850, training loss: 6.390652656555176 = 0.0629817545413971 + 1.0 * 6.327671051025391
Epoch 850, val loss: 0.9104453921318054
Epoch 860, training loss: 6.37683629989624 = 0.06001167371869087 + 1.0 * 6.316824436187744
Epoch 860, val loss: 0.9174257516860962
Epoch 870, training loss: 6.3686699867248535 = 0.05725783854722977 + 1.0 * 6.311412334442139
Epoch 870, val loss: 0.9242409467697144
Epoch 880, training loss: 6.366206645965576 = 0.05466586723923683 + 1.0 * 6.311540603637695
Epoch 880, val loss: 0.9311332702636719
Epoch 890, training loss: 6.369330883026123 = 0.05223460495471954 + 1.0 * 6.31709623336792
Epoch 890, val loss: 0.9380913972854614
Epoch 900, training loss: 6.360393047332764 = 0.04996100440621376 + 1.0 * 6.310431957244873
Epoch 900, val loss: 0.9447402954101562
Epoch 910, training loss: 6.361035346984863 = 0.047825682908296585 + 1.0 * 6.313209533691406
Epoch 910, val loss: 0.9514968395233154
Epoch 920, training loss: 6.354656219482422 = 0.04582187160849571 + 1.0 * 6.308834552764893
Epoch 920, val loss: 0.9583901762962341
Epoch 930, training loss: 6.350952625274658 = 0.04393845051527023 + 1.0 * 6.307013988494873
Epoch 930, val loss: 0.9649932980537415
Epoch 940, training loss: 6.347413063049316 = 0.04216420277953148 + 1.0 * 6.305248737335205
Epoch 940, val loss: 0.9717376828193665
Epoch 950, training loss: 6.351809501647949 = 0.040488217025995255 + 1.0 * 6.311321258544922
Epoch 950, val loss: 0.9784056544303894
Epoch 960, training loss: 6.355067729949951 = 0.03890868276357651 + 1.0 * 6.316159248352051
Epoch 960, val loss: 0.9850786924362183
Epoch 970, training loss: 6.341052532196045 = 0.03743322938680649 + 1.0 * 6.303619384765625
Epoch 970, val loss: 0.9914237856864929
Epoch 980, training loss: 6.33770751953125 = 0.036037951707839966 + 1.0 * 6.301669597625732
Epoch 980, val loss: 0.997856080532074
Epoch 990, training loss: 6.336646556854248 = 0.03471526876091957 + 1.0 * 6.301931381225586
Epoch 990, val loss: 1.0043586492538452
Epoch 1000, training loss: 6.343320369720459 = 0.03346032649278641 + 1.0 * 6.3098602294921875
Epoch 1000, val loss: 1.0107837915420532
Epoch 1010, training loss: 6.332381248474121 = 0.03228093683719635 + 1.0 * 6.300100326538086
Epoch 1010, val loss: 1.0168566703796387
Epoch 1020, training loss: 6.329317092895508 = 0.031159766018390656 + 1.0 * 6.298157215118408
Epoch 1020, val loss: 1.0230510234832764
Epoch 1030, training loss: 6.330995082855225 = 0.0300942100584507 + 1.0 * 6.300900936126709
Epoch 1030, val loss: 1.0292483568191528
Epoch 1040, training loss: 6.327855110168457 = 0.0290831346064806 + 1.0 * 6.298771858215332
Epoch 1040, val loss: 1.0353984832763672
Epoch 1050, training loss: 6.324976921081543 = 0.02812272310256958 + 1.0 * 6.296854019165039
Epoch 1050, val loss: 1.041333556175232
Epoch 1060, training loss: 6.324307441711426 = 0.02721014805138111 + 1.0 * 6.297097206115723
Epoch 1060, val loss: 1.047342300415039
Epoch 1070, training loss: 6.326591968536377 = 0.026339996606111526 + 1.0 * 6.3002519607543945
Epoch 1070, val loss: 1.0533075332641602
Epoch 1080, training loss: 6.324127197265625 = 0.025515388697385788 + 1.0 * 6.298611640930176
Epoch 1080, val loss: 1.0591230392456055
Epoch 1090, training loss: 6.32142448425293 = 0.024728387594223022 + 1.0 * 6.296696186065674
Epoch 1090, val loss: 1.0647579431533813
Epoch 1100, training loss: 6.3181047439575195 = 0.02397795394062996 + 1.0 * 6.294126987457275
Epoch 1100, val loss: 1.070587158203125
Epoch 1110, training loss: 6.316093444824219 = 0.02326548658311367 + 1.0 * 6.29282808303833
Epoch 1110, val loss: 1.076039433479309
Epoch 1120, training loss: 6.313838481903076 = 0.022583365440368652 + 1.0 * 6.291254997253418
Epoch 1120, val loss: 1.081683874130249
Epoch 1130, training loss: 6.318353176116943 = 0.021931279450654984 + 1.0 * 6.296422004699707
Epoch 1130, val loss: 1.0872571468353271
Epoch 1140, training loss: 6.311570167541504 = 0.021307164803147316 + 1.0 * 6.2902631759643555
Epoch 1140, val loss: 1.0926755666732788
Epoch 1150, training loss: 6.310836315155029 = 0.020710714161396027 + 1.0 * 6.290125370025635
Epoch 1150, val loss: 1.098024845123291
Epoch 1160, training loss: 6.317149639129639 = 0.020138999447226524 + 1.0 * 6.29701042175293
Epoch 1160, val loss: 1.1034111976623535
Epoch 1170, training loss: 6.309815406799316 = 0.019594045355916023 + 1.0 * 6.290221214294434
Epoch 1170, val loss: 1.108611822128296
Epoch 1180, training loss: 6.308410167694092 = 0.019070785492658615 + 1.0 * 6.289339542388916
Epoch 1180, val loss: 1.1137348413467407
Epoch 1190, training loss: 6.309979438781738 = 0.018568197265267372 + 1.0 * 6.291411399841309
Epoch 1190, val loss: 1.1189017295837402
Epoch 1200, training loss: 6.30548620223999 = 0.018086401745676994 + 1.0 * 6.287399768829346
Epoch 1200, val loss: 1.1239901781082153
Epoch 1210, training loss: 6.305212020874023 = 0.017624149098992348 + 1.0 * 6.287587642669678
Epoch 1210, val loss: 1.1289349794387817
Epoch 1220, training loss: 6.304741859436035 = 0.017180385068058968 + 1.0 * 6.287561416625977
Epoch 1220, val loss: 1.1338393688201904
Epoch 1230, training loss: 6.304562568664551 = 0.01675364002585411 + 1.0 * 6.287808895111084
Epoch 1230, val loss: 1.1388028860092163
Epoch 1240, training loss: 6.30185079574585 = 0.016343504190444946 + 1.0 * 6.2855072021484375
Epoch 1240, val loss: 1.1436676979064941
Epoch 1250, training loss: 6.300000190734863 = 0.015949642285704613 + 1.0 * 6.284050464630127
Epoch 1250, val loss: 1.1484102010726929
Epoch 1260, training loss: 6.305807590484619 = 0.015569665469229221 + 1.0 * 6.290237903594971
Epoch 1260, val loss: 1.1532565355300903
Epoch 1270, training loss: 6.301636695861816 = 0.015205444768071175 + 1.0 * 6.286431312561035
Epoch 1270, val loss: 1.1579185724258423
Epoch 1280, training loss: 6.300811767578125 = 0.014855003915727139 + 1.0 * 6.285956859588623
Epoch 1280, val loss: 1.1622655391693115
Epoch 1290, training loss: 6.3000359535217285 = 0.01451684907078743 + 1.0 * 6.285519123077393
Epoch 1290, val loss: 1.166988730430603
Epoch 1300, training loss: 6.295302391052246 = 0.014189625158905983 + 1.0 * 6.2811126708984375
Epoch 1300, val loss: 1.1714993715286255
Epoch 1310, training loss: 6.297046184539795 = 0.01387368980795145 + 1.0 * 6.283172607421875
Epoch 1310, val loss: 1.1761077642440796
Epoch 1320, training loss: 6.298070430755615 = 0.013568108901381493 + 1.0 * 6.2845025062561035
Epoch 1320, val loss: 1.1806459426879883
Epoch 1330, training loss: 6.295499801635742 = 0.01327626034617424 + 1.0 * 6.282223701477051
Epoch 1330, val loss: 1.1848503351211548
Epoch 1340, training loss: 6.2929368019104 = 0.012992208823561668 + 1.0 * 6.27994441986084
Epoch 1340, val loss: 1.1890654563903809
Epoch 1350, training loss: 6.300386905670166 = 0.01271983701735735 + 1.0 * 6.287667274475098
Epoch 1350, val loss: 1.1933751106262207
Epoch 1360, training loss: 6.2931647300720215 = 0.012452958151698112 + 1.0 * 6.280711650848389
Epoch 1360, val loss: 1.1977674961090088
Epoch 1370, training loss: 6.291865825653076 = 0.012198329903185368 + 1.0 * 6.279667377471924
Epoch 1370, val loss: 1.201841950416565
Epoch 1380, training loss: 6.291512489318848 = 0.011950272135436535 + 1.0 * 6.279561996459961
Epoch 1380, val loss: 1.2060563564300537
Epoch 1390, training loss: 6.291539192199707 = 0.011710969731211662 + 1.0 * 6.279828071594238
Epoch 1390, val loss: 1.210126280784607
Epoch 1400, training loss: 6.293744087219238 = 0.011478908360004425 + 1.0 * 6.2822651863098145
Epoch 1400, val loss: 1.2141168117523193
Epoch 1410, training loss: 6.29141902923584 = 0.011255108751356602 + 1.0 * 6.280163764953613
Epoch 1410, val loss: 1.2182680368423462
Epoch 1420, training loss: 6.287561416625977 = 0.011037458665668964 + 1.0 * 6.276524066925049
Epoch 1420, val loss: 1.2220697402954102
Epoch 1430, training loss: 6.286377906799316 = 0.010827889665961266 + 1.0 * 6.27554988861084
Epoch 1430, val loss: 1.2259762287139893
Epoch 1440, training loss: 6.286662578582764 = 0.010623051784932613 + 1.0 * 6.2760396003723145
Epoch 1440, val loss: 1.2299354076385498
Epoch 1450, training loss: 6.291772842407227 = 0.010424952954053879 + 1.0 * 6.281347751617432
Epoch 1450, val loss: 1.2338037490844727
Epoch 1460, training loss: 6.28566837310791 = 0.010231317952275276 + 1.0 * 6.275436878204346
Epoch 1460, val loss: 1.2375982999801636
Epoch 1470, training loss: 6.285440921783447 = 0.01004565879702568 + 1.0 * 6.275395393371582
Epoch 1470, val loss: 1.241247534751892
Epoch 1480, training loss: 6.287364959716797 = 0.009864121675491333 + 1.0 * 6.277500629425049
Epoch 1480, val loss: 1.2450679540634155
Epoch 1490, training loss: 6.284806251525879 = 0.00968849565833807 + 1.0 * 6.275117874145508
Epoch 1490, val loss: 1.2487105131149292
Epoch 1500, training loss: 6.28841495513916 = 0.009519062004983425 + 1.0 * 6.278895854949951
Epoch 1500, val loss: 1.2523419857025146
Epoch 1510, training loss: 6.285501480102539 = 0.009351913817226887 + 1.0 * 6.276149749755859
Epoch 1510, val loss: 1.2560869455337524
Epoch 1520, training loss: 6.282292366027832 = 0.009191764518618584 + 1.0 * 6.27310037612915
Epoch 1520, val loss: 1.2594811916351318
Epoch 1530, training loss: 6.280133247375488 = 0.009035519324243069 + 1.0 * 6.271097660064697
Epoch 1530, val loss: 1.2629512548446655
Epoch 1540, training loss: 6.282434463500977 = 0.008882883004844189 + 1.0 * 6.2735514640808105
Epoch 1540, val loss: 1.2664449214935303
Epoch 1550, training loss: 6.284205913543701 = 0.008734708651900291 + 1.0 * 6.275471210479736
Epoch 1550, val loss: 1.2700202465057373
Epoch 1560, training loss: 6.281952857971191 = 0.008589319884777069 + 1.0 * 6.2733635902404785
Epoch 1560, val loss: 1.2735341787338257
Epoch 1570, training loss: 6.280826568603516 = 0.008449770510196686 + 1.0 * 6.272377014160156
Epoch 1570, val loss: 1.276714563369751
Epoch 1580, training loss: 6.280484676361084 = 0.008313320577144623 + 1.0 * 6.272171497344971
Epoch 1580, val loss: 1.2801792621612549
Epoch 1590, training loss: 6.280621528625488 = 0.008180334232747555 + 1.0 * 6.272441387176514
Epoch 1590, val loss: 1.283293604850769
Epoch 1600, training loss: 6.277581214904785 = 0.00805124081671238 + 1.0 * 6.269529819488525
Epoch 1600, val loss: 1.2866134643554688
Epoch 1610, training loss: 6.280660629272461 = 0.007925719954073429 + 1.0 * 6.272735118865967
Epoch 1610, val loss: 1.289810061454773
Epoch 1620, training loss: 6.277072906494141 = 0.007802823092788458 + 1.0 * 6.269269943237305
Epoch 1620, val loss: 1.2929962873458862
Epoch 1630, training loss: 6.276522159576416 = 0.007683010771870613 + 1.0 * 6.268839359283447
Epoch 1630, val loss: 1.296167016029358
Epoch 1640, training loss: 6.28243350982666 = 0.007566618733108044 + 1.0 * 6.274867057800293
Epoch 1640, val loss: 1.2994083166122437
Epoch 1650, training loss: 6.27700138092041 = 0.007453378289937973 + 1.0 * 6.269547939300537
Epoch 1650, val loss: 1.3022993803024292
Epoch 1660, training loss: 6.2751569747924805 = 0.007342254742980003 + 1.0 * 6.267814636230469
Epoch 1660, val loss: 1.3054271936416626
Epoch 1670, training loss: 6.278960704803467 = 0.0072340588085353374 + 1.0 * 6.271726608276367
Epoch 1670, val loss: 1.3084136247634888
Epoch 1680, training loss: 6.273732662200928 = 0.0071277194656431675 + 1.0 * 6.266604900360107
Epoch 1680, val loss: 1.3116008043289185
Epoch 1690, training loss: 6.276878356933594 = 0.007024404127150774 + 1.0 * 6.2698540687561035
Epoch 1690, val loss: 1.3145569562911987
Epoch 1700, training loss: 6.276638507843018 = 0.006924346089363098 + 1.0 * 6.26971435546875
Epoch 1700, val loss: 1.3174998760223389
Epoch 1710, training loss: 6.273099422454834 = 0.006826958619058132 + 1.0 * 6.26627254486084
Epoch 1710, val loss: 1.3201980590820312
Epoch 1720, training loss: 6.271854400634766 = 0.0067316084168851376 + 1.0 * 6.265122890472412
Epoch 1720, val loss: 1.3230535984039307
Epoch 1730, training loss: 6.271134853363037 = 0.006637834943830967 + 1.0 * 6.264496803283691
Epoch 1730, val loss: 1.3259539604187012
Epoch 1740, training loss: 6.277609348297119 = 0.006546423304826021 + 1.0 * 6.271062850952148
Epoch 1740, val loss: 1.3287779092788696
Epoch 1750, training loss: 6.277227401733398 = 0.006457107607275248 + 1.0 * 6.270770072937012
Epoch 1750, val loss: 1.3317359685897827
Epoch 1760, training loss: 6.272093772888184 = 0.0063694012351334095 + 1.0 * 6.265724182128906
Epoch 1760, val loss: 1.3343079090118408
Epoch 1770, training loss: 6.270303726196289 = 0.006284446455538273 + 1.0 * 6.26401948928833
Epoch 1770, val loss: 1.337022304534912
Epoch 1780, training loss: 6.2692484855651855 = 0.006201043725013733 + 1.0 * 6.263047218322754
Epoch 1780, val loss: 1.3398056030273438
Epoch 1790, training loss: 6.278412342071533 = 0.006119705270975828 + 1.0 * 6.272292613983154
Epoch 1790, val loss: 1.3425512313842773
Epoch 1800, training loss: 6.275015830993652 = 0.006039346102625132 + 1.0 * 6.26897668838501
Epoch 1800, val loss: 1.3452317714691162
Epoch 1810, training loss: 6.2699875831604 = 0.005961829796433449 + 1.0 * 6.264025688171387
Epoch 1810, val loss: 1.3476976156234741
Epoch 1820, training loss: 6.2699408531188965 = 0.005885608959943056 + 1.0 * 6.264055252075195
Epoch 1820, val loss: 1.3502665758132935
Epoch 1830, training loss: 6.269626140594482 = 0.00581175135448575 + 1.0 * 6.263814449310303
Epoch 1830, val loss: 1.352892279624939
Epoch 1840, training loss: 6.271191120147705 = 0.005738583393394947 + 1.0 * 6.2654523849487305
Epoch 1840, val loss: 1.355534553527832
Epoch 1850, training loss: 6.269340515136719 = 0.005665803328156471 + 1.0 * 6.263674736022949
Epoch 1850, val loss: 1.3582285642623901
Epoch 1860, training loss: 6.266997337341309 = 0.0055962298065423965 + 1.0 * 6.261401176452637
Epoch 1860, val loss: 1.360617756843567
Epoch 1870, training loss: 6.2701826095581055 = 0.0055282157845795155 + 1.0 * 6.264654159545898
Epoch 1870, val loss: 1.3630112409591675
Epoch 1880, training loss: 6.2683024406433105 = 0.005460523068904877 + 1.0 * 6.262841701507568
Epoch 1880, val loss: 1.3656355142593384
Epoch 1890, training loss: 6.267520904541016 = 0.005395056679844856 + 1.0 * 6.2621259689331055
Epoch 1890, val loss: 1.368019938468933
Epoch 1900, training loss: 6.2665114402771 = 0.005331052467226982 + 1.0 * 6.261180400848389
Epoch 1900, val loss: 1.370344638824463
Epoch 1910, training loss: 6.266737937927246 = 0.005267778877168894 + 1.0 * 6.261470317840576
Epoch 1910, val loss: 1.3729071617126465
Epoch 1920, training loss: 6.266544818878174 = 0.005205594003200531 + 1.0 * 6.26133918762207
Epoch 1920, val loss: 1.3753387928009033
Epoch 1930, training loss: 6.265880584716797 = 0.005145529750734568 + 1.0 * 6.260735034942627
Epoch 1930, val loss: 1.377573013305664
Epoch 1940, training loss: 6.268925189971924 = 0.005085707642138004 + 1.0 * 6.263839244842529
Epoch 1940, val loss: 1.3801062107086182
Epoch 1950, training loss: 6.266531467437744 = 0.005027608945965767 + 1.0 * 6.26150369644165
Epoch 1950, val loss: 1.382385015487671
Epoch 1960, training loss: 6.262946605682373 = 0.004970922134816647 + 1.0 * 6.2579755783081055
Epoch 1960, val loss: 1.3846471309661865
Epoch 1970, training loss: 6.264397144317627 = 0.004915013909339905 + 1.0 * 6.259481906890869
Epoch 1970, val loss: 1.386854648590088
Epoch 1980, training loss: 6.264276027679443 = 0.004860606975853443 + 1.0 * 6.259415626525879
Epoch 1980, val loss: 1.3892194032669067
Epoch 1990, training loss: 6.264400482177734 = 0.004806473385542631 + 1.0 * 6.259593963623047
Epoch 1990, val loss: 1.3914827108383179
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 10.54564094543457 = 1.94879949092865 + 1.0 * 8.596841812133789
Epoch 0, val loss: 1.941416621208191
Epoch 10, training loss: 10.534239768981934 = 1.9377042055130005 + 1.0 * 8.596535682678223
Epoch 10, val loss: 1.9309959411621094
Epoch 20, training loss: 10.517105102539062 = 1.923327922821045 + 1.0 * 8.59377670288086
Epoch 20, val loss: 1.9172358512878418
Epoch 30, training loss: 10.474844932556152 = 1.9029836654663086 + 1.0 * 8.571861267089844
Epoch 30, val loss: 1.897657871246338
Epoch 40, training loss: 10.315001487731934 = 1.8770272731781006 + 1.0 * 8.437973976135254
Epoch 40, val loss: 1.8735568523406982
Epoch 50, training loss: 9.952617645263672 = 1.8508615493774414 + 1.0 * 8.10175609588623
Epoch 50, val loss: 1.8500239849090576
Epoch 60, training loss: 9.649856567382812 = 1.8292700052261353 + 1.0 * 7.820586204528809
Epoch 60, val loss: 1.830803632736206
Epoch 70, training loss: 9.143034934997559 = 1.8135716915130615 + 1.0 * 7.329463481903076
Epoch 70, val loss: 1.8170040845870972
Epoch 80, training loss: 8.881178855895996 = 1.8016743659973145 + 1.0 * 7.079504489898682
Epoch 80, val loss: 1.807018756866455
Epoch 90, training loss: 8.705004692077637 = 1.7876994609832764 + 1.0 * 6.917304992675781
Epoch 90, val loss: 1.79534912109375
Epoch 100, training loss: 8.574529647827148 = 1.7739876508712769 + 1.0 * 6.800541877746582
Epoch 100, val loss: 1.7841769456863403
Epoch 110, training loss: 8.50368595123291 = 1.7602678537368774 + 1.0 * 6.743417739868164
Epoch 110, val loss: 1.772456169128418
Epoch 120, training loss: 8.443886756896973 = 1.744946002960205 + 1.0 * 6.698940753936768
Epoch 120, val loss: 1.7587984800338745
Epoch 130, training loss: 8.38720989227295 = 1.7277899980545044 + 1.0 * 6.659419536590576
Epoch 130, val loss: 1.7433671951293945
Epoch 140, training loss: 8.335427284240723 = 1.7087196111679077 + 1.0 * 6.626707553863525
Epoch 140, val loss: 1.7267354726791382
Epoch 150, training loss: 8.282547950744629 = 1.6870437860488892 + 1.0 * 6.595503807067871
Epoch 150, val loss: 1.7084460258483887
Epoch 160, training loss: 8.231217384338379 = 1.6621545553207397 + 1.0 * 6.569063186645508
Epoch 160, val loss: 1.687461256980896
Epoch 170, training loss: 8.180641174316406 = 1.633842945098877 + 1.0 * 6.546798229217529
Epoch 170, val loss: 1.663514256477356
Epoch 180, training loss: 8.128219604492188 = 1.6022040843963623 + 1.0 * 6.526015758514404
Epoch 180, val loss: 1.6367039680480957
Epoch 190, training loss: 8.074777603149414 = 1.5673812627792358 + 1.0 * 6.507396697998047
Epoch 190, val loss: 1.6073232889175415
Epoch 200, training loss: 8.024147033691406 = 1.5305033922195435 + 1.0 * 6.493643760681152
Epoch 200, val loss: 1.5765599012374878
Epoch 210, training loss: 7.969176769256592 = 1.4920591115951538 + 1.0 * 6.477117538452148
Epoch 210, val loss: 1.5446209907531738
Epoch 220, training loss: 7.917114734649658 = 1.4521454572677612 + 1.0 * 6.464969158172607
Epoch 220, val loss: 1.5117080211639404
Epoch 230, training loss: 7.865505218505859 = 1.410975456237793 + 1.0 * 6.454529762268066
Epoch 230, val loss: 1.4781512022018433
Epoch 240, training loss: 7.819939613342285 = 1.3692724704742432 + 1.0 * 6.450666904449463
Epoch 240, val loss: 1.4449256658554077
Epoch 250, training loss: 7.767641067504883 = 1.3281556367874146 + 1.0 * 6.439485549926758
Epoch 250, val loss: 1.4127073287963867
Epoch 260, training loss: 7.718862056732178 = 1.2871112823486328 + 1.0 * 6.431750774383545
Epoch 260, val loss: 1.381276249885559
Epoch 270, training loss: 7.678242206573486 = 1.2461485862731934 + 1.0 * 6.432093620300293
Epoch 270, val loss: 1.350670576095581
Epoch 280, training loss: 7.62588357925415 = 1.2058682441711426 + 1.0 * 6.420015335083008
Epoch 280, val loss: 1.3211979866027832
Epoch 290, training loss: 7.580787181854248 = 1.1659802198410034 + 1.0 * 6.414806842803955
Epoch 290, val loss: 1.2928030490875244
Epoch 300, training loss: 7.538305282592773 = 1.1266868114471436 + 1.0 * 6.411618232727051
Epoch 300, val loss: 1.265549898147583
Epoch 310, training loss: 7.49298095703125 = 1.0882210731506348 + 1.0 * 6.404759883880615
Epoch 310, val loss: 1.2394617795944214
Epoch 320, training loss: 7.452247619628906 = 1.0503065586090088 + 1.0 * 6.401941299438477
Epoch 320, val loss: 1.2143193483352661
Epoch 330, training loss: 7.411478042602539 = 1.0132670402526855 + 1.0 * 6.3982110023498535
Epoch 330, val loss: 1.1902291774749756
Epoch 340, training loss: 7.3715620040893555 = 0.977135419845581 + 1.0 * 6.394426345825195
Epoch 340, val loss: 1.1672738790512085
Epoch 350, training loss: 7.329233169555664 = 0.9418668746948242 + 1.0 * 6.38736629486084
Epoch 350, val loss: 1.1453620195388794
Epoch 360, training loss: 7.29268217086792 = 0.9073395729064941 + 1.0 * 6.385342597961426
Epoch 360, val loss: 1.1242302656173706
Epoch 370, training loss: 7.260867118835449 = 0.8738572001457214 + 1.0 * 6.387010097503662
Epoch 370, val loss: 1.1040873527526855
Epoch 380, training loss: 7.220761299133301 = 0.841490626335144 + 1.0 * 6.379270553588867
Epoch 380, val loss: 1.0852938890457153
Epoch 390, training loss: 7.183633327484131 = 0.8098822236061096 + 1.0 * 6.373751163482666
Epoch 390, val loss: 1.0673141479492188
Epoch 400, training loss: 7.149068355560303 = 0.7788370251655579 + 1.0 * 6.3702311515808105
Epoch 400, val loss: 1.0501432418823242
Epoch 410, training loss: 7.116489410400391 = 0.7484161853790283 + 1.0 * 6.368072986602783
Epoch 410, val loss: 1.0339432954788208
Epoch 420, training loss: 7.0865983963012695 = 0.718902587890625 + 1.0 * 6.3676958084106445
Epoch 420, val loss: 1.0189684629440308
Epoch 430, training loss: 7.05303430557251 = 0.6898754835128784 + 1.0 * 6.363158702850342
Epoch 430, val loss: 1.0049363374710083
Epoch 440, training loss: 7.023276329040527 = 0.6611288785934448 + 1.0 * 6.362147331237793
Epoch 440, val loss: 0.9916553497314453
Epoch 450, training loss: 6.9967451095581055 = 0.6327152252197266 + 1.0 * 6.364029884338379
Epoch 450, val loss: 0.9792852401733398
Epoch 460, training loss: 6.962212085723877 = 0.6048409342765808 + 1.0 * 6.3573713302612305
Epoch 460, val loss: 0.9680813550949097
Epoch 470, training loss: 6.931005954742432 = 0.5773642659187317 + 1.0 * 6.353641510009766
Epoch 470, val loss: 0.9579145908355713
Epoch 480, training loss: 6.903787136077881 = 0.5501996874809265 + 1.0 * 6.353587627410889
Epoch 480, val loss: 0.9488023519515991
Epoch 490, training loss: 6.874833583831787 = 0.5235878229141235 + 1.0 * 6.351245880126953
Epoch 490, val loss: 0.9410035014152527
Epoch 500, training loss: 6.852301597595215 = 0.4976543188095093 + 1.0 * 6.354647159576416
Epoch 500, val loss: 0.9345510601997375
Epoch 510, training loss: 6.81959342956543 = 0.47249293327331543 + 1.0 * 6.347100257873535
Epoch 510, val loss: 0.9296155571937561
Epoch 520, training loss: 6.793221950531006 = 0.4480379819869995 + 1.0 * 6.345183849334717
Epoch 520, val loss: 0.9262075424194336
Epoch 530, training loss: 6.773068904876709 = 0.42432746291160583 + 1.0 * 6.34874153137207
Epoch 530, val loss: 0.9241283535957336
Epoch 540, training loss: 6.747689247131348 = 0.4016025960445404 + 1.0 * 6.346086502075195
Epoch 540, val loss: 0.9235989451408386
Epoch 550, training loss: 6.719595432281494 = 0.37988191843032837 + 1.0 * 6.3397135734558105
Epoch 550, val loss: 0.9248799085617065
Epoch 560, training loss: 6.705597877502441 = 0.3590436577796936 + 1.0 * 6.346554279327393
Epoch 560, val loss: 0.9273386597633362
Epoch 570, training loss: 6.681149005889893 = 0.339173287153244 + 1.0 * 6.341975688934326
Epoch 570, val loss: 0.9310024976730347
Epoch 580, training loss: 6.657410144805908 = 0.3203031122684479 + 1.0 * 6.337107181549072
Epoch 580, val loss: 0.9361485838890076
Epoch 590, training loss: 6.637065887451172 = 0.3023574650287628 + 1.0 * 6.334708213806152
Epoch 590, val loss: 0.9423199892044067
Epoch 600, training loss: 6.619884014129639 = 0.28530699014663696 + 1.0 * 6.3345770835876465
Epoch 600, val loss: 0.9495433568954468
Epoch 610, training loss: 6.600879192352295 = 0.2691754400730133 + 1.0 * 6.3317036628723145
Epoch 610, val loss: 0.9575682282447815
Epoch 620, training loss: 6.584214687347412 = 0.253936231136322 + 1.0 * 6.330278396606445
Epoch 620, val loss: 0.9666611552238464
Epoch 630, training loss: 6.572357177734375 = 0.23958835005760193 + 1.0 * 6.33276891708374
Epoch 630, val loss: 0.9761766195297241
Epoch 640, training loss: 6.55740213394165 = 0.2261713743209839 + 1.0 * 6.331230640411377
Epoch 640, val loss: 0.9868301749229431
Epoch 650, training loss: 6.538323879241943 = 0.21355293691158295 + 1.0 * 6.324770927429199
Epoch 650, val loss: 0.9977552890777588
Epoch 660, training loss: 6.537879467010498 = 0.2016783505678177 + 1.0 * 6.336201190948486
Epoch 660, val loss: 1.0092655420303345
Epoch 670, training loss: 6.516645431518555 = 0.19060054421424866 + 1.0 * 6.326045036315918
Epoch 670, val loss: 1.0213069915771484
Epoch 680, training loss: 6.502967834472656 = 0.18022918701171875 + 1.0 * 6.3227386474609375
Epoch 680, val loss: 1.0338705778121948
Epoch 690, training loss: 6.4933576583862305 = 0.17047017812728882 + 1.0 * 6.322887420654297
Epoch 690, val loss: 1.0466086864471436
Epoch 700, training loss: 6.481133937835693 = 0.16131216287612915 + 1.0 * 6.319821834564209
Epoch 700, val loss: 1.0596603155136108
Epoch 710, training loss: 6.47161865234375 = 0.15272988379001617 + 1.0 * 6.3188886642456055
Epoch 710, val loss: 1.0731189250946045
Epoch 720, training loss: 6.469826698303223 = 0.1446717232465744 + 1.0 * 6.325154781341553
Epoch 720, val loss: 1.0866066217422485
Epoch 730, training loss: 6.4538092613220215 = 0.13710881769657135 + 1.0 * 6.316700458526611
Epoch 730, val loss: 1.1002099514007568
Epoch 740, training loss: 6.444550037384033 = 0.13001041114330292 + 1.0 * 6.314539432525635
Epoch 740, val loss: 1.11406409740448
Epoch 750, training loss: 6.441905498504639 = 0.12331957370042801 + 1.0 * 6.3185858726501465
Epoch 750, val loss: 1.1280003786087036
Epoch 760, training loss: 6.441689491271973 = 0.11706837266683578 + 1.0 * 6.324621200561523
Epoch 760, val loss: 1.1417510509490967
Epoch 770, training loss: 6.4237823486328125 = 0.1112157553434372 + 1.0 * 6.312566757202148
Epoch 770, val loss: 1.1557857990264893
Epoch 780, training loss: 6.4158806800842285 = 0.10571760684251785 + 1.0 * 6.3101630210876465
Epoch 780, val loss: 1.1696593761444092
Epoch 790, training loss: 6.4130730628967285 = 0.10054004192352295 + 1.0 * 6.312532901763916
Epoch 790, val loss: 1.183678150177002
Epoch 800, training loss: 6.404722690582275 = 0.09567384421825409 + 1.0 * 6.309048652648926
Epoch 800, val loss: 1.1975404024124146
Epoch 810, training loss: 6.4084882736206055 = 0.09110835939645767 + 1.0 * 6.317379951477051
Epoch 810, val loss: 1.211590051651001
Epoch 820, training loss: 6.3996734619140625 = 0.08681974560022354 + 1.0 * 6.312853813171387
Epoch 820, val loss: 1.2249643802642822
Epoch 830, training loss: 6.390275001525879 = 0.08280574530363083 + 1.0 * 6.307469367980957
Epoch 830, val loss: 1.2388701438903809
Epoch 840, training loss: 6.383487701416016 = 0.07901392877101898 + 1.0 * 6.304473876953125
Epoch 840, val loss: 1.2524375915527344
Epoch 850, training loss: 6.378793239593506 = 0.07543396204710007 + 1.0 * 6.303359508514404
Epoch 850, val loss: 1.2660983800888062
Epoch 860, training loss: 6.3840556144714355 = 0.07205364108085632 + 1.0 * 6.312002182006836
Epoch 860, val loss: 1.2796201705932617
Epoch 870, training loss: 6.380283832550049 = 0.0688670426607132 + 1.0 * 6.3114166259765625
Epoch 870, val loss: 1.2926921844482422
Epoch 880, training loss: 6.367833137512207 = 0.06588633358478546 + 1.0 * 6.301946640014648
Epoch 880, val loss: 1.3060139417648315
Epoch 890, training loss: 6.364035129547119 = 0.06306515634059906 + 1.0 * 6.300970077514648
Epoch 890, val loss: 1.3189420700073242
Epoch 900, training loss: 6.363776683807373 = 0.06039473041892052 + 1.0 * 6.30338191986084
Epoch 900, val loss: 1.3318835496902466
Epoch 910, training loss: 6.360279560089111 = 0.05787429213523865 + 1.0 * 6.30240535736084
Epoch 910, val loss: 1.3448513746261597
Epoch 920, training loss: 6.3573455810546875 = 0.0554957240819931 + 1.0 * 6.301849842071533
Epoch 920, val loss: 1.357243299484253
Epoch 930, training loss: 6.351892948150635 = 0.05325259268283844 + 1.0 * 6.298640251159668
Epoch 930, val loss: 1.3699814081192017
Epoch 940, training loss: 6.348890781402588 = 0.05112766847014427 + 1.0 * 6.297763347625732
Epoch 940, val loss: 1.3822544813156128
Epoch 950, training loss: 6.3463029861450195 = 0.04911072552204132 + 1.0 * 6.297192096710205
Epoch 950, val loss: 1.3945170640945435
Epoch 960, training loss: 6.348583698272705 = 0.04719676822423935 + 1.0 * 6.301386833190918
Epoch 960, val loss: 1.4064537286758423
Epoch 970, training loss: 6.343696117401123 = 0.04538961872458458 + 1.0 * 6.298306465148926
Epoch 970, val loss: 1.4184954166412354
Epoch 980, training loss: 6.340264797210693 = 0.043672654777765274 + 1.0 * 6.2965922355651855
Epoch 980, val loss: 1.4301425218582153
Epoch 990, training loss: 6.33547830581665 = 0.04204666614532471 + 1.0 * 6.293431758880615
Epoch 990, val loss: 1.4417994022369385
Epoch 1000, training loss: 6.339317321777344 = 0.040499038994312286 + 1.0 * 6.298818111419678
Epoch 1000, val loss: 1.4529441595077515
Epoch 1010, training loss: 6.3329973220825195 = 0.039036210626363754 + 1.0 * 6.293961048126221
Epoch 1010, val loss: 1.4643911123275757
Epoch 1020, training loss: 6.331542015075684 = 0.03764856979250908 + 1.0 * 6.293893337249756
Epoch 1020, val loss: 1.4753234386444092
Epoch 1030, training loss: 6.329952239990234 = 0.036331843584775925 + 1.0 * 6.293620586395264
Epoch 1030, val loss: 1.4863052368164062
Epoch 1040, training loss: 6.325981140136719 = 0.035077400505542755 + 1.0 * 6.290903568267822
Epoch 1040, val loss: 1.4970471858978271
Epoch 1050, training loss: 6.3285393714904785 = 0.03388584032654762 + 1.0 * 6.294653415679932
Epoch 1050, val loss: 1.5075560808181763
Epoch 1060, training loss: 6.325730800628662 = 0.03275027498602867 + 1.0 * 6.292980670928955
Epoch 1060, val loss: 1.5181560516357422
Epoch 1070, training loss: 6.322159290313721 = 0.03167475387454033 + 1.0 * 6.290484428405762
Epoch 1070, val loss: 1.5283650159835815
Epoch 1080, training loss: 6.31882905960083 = 0.03064803220331669 + 1.0 * 6.288180828094482
Epoch 1080, val loss: 1.5385411977767944
Epoch 1090, training loss: 6.316353797912598 = 0.02966887876391411 + 1.0 * 6.286684989929199
Epoch 1090, val loss: 1.5484302043914795
Epoch 1100, training loss: 6.319138526916504 = 0.02873319573700428 + 1.0 * 6.2904052734375
Epoch 1100, val loss: 1.5582385063171387
Epoch 1110, training loss: 6.315880298614502 = 0.027841396629810333 + 1.0 * 6.288038730621338
Epoch 1110, val loss: 1.5680903196334839
Epoch 1120, training loss: 6.3150634765625 = 0.026989953592419624 + 1.0 * 6.288073539733887
Epoch 1120, val loss: 1.577560544013977
Epoch 1130, training loss: 6.312164783477783 = 0.02617799863219261 + 1.0 * 6.28598690032959
Epoch 1130, val loss: 1.586775302886963
Epoch 1140, training loss: 6.309412956237793 = 0.02540723793208599 + 1.0 * 6.284005641937256
Epoch 1140, val loss: 1.5961809158325195
Epoch 1150, training loss: 6.306816577911377 = 0.024666791781783104 + 1.0 * 6.282149791717529
Epoch 1150, val loss: 1.605305552482605
Epoch 1160, training loss: 6.306429386138916 = 0.023956120014190674 + 1.0 * 6.282473087310791
Epoch 1160, val loss: 1.6143511533737183
Epoch 1170, training loss: 6.317760944366455 = 0.023275507614016533 + 1.0 * 6.294485569000244
Epoch 1170, val loss: 1.6231328248977661
Epoch 1180, training loss: 6.306876182556152 = 0.022626761347055435 + 1.0 * 6.284249305725098
Epoch 1180, val loss: 1.6317986249923706
Epoch 1190, training loss: 6.302801609039307 = 0.02200690656900406 + 1.0 * 6.280794620513916
Epoch 1190, val loss: 1.640432357788086
Epoch 1200, training loss: 6.302203178405762 = 0.021411260589957237 + 1.0 * 6.280791759490967
Epoch 1200, val loss: 1.6487681865692139
Epoch 1210, training loss: 6.3099446296691895 = 0.020839178934693336 + 1.0 * 6.289105415344238
Epoch 1210, val loss: 1.6570758819580078
Epoch 1220, training loss: 6.305232048034668 = 0.02029244229197502 + 1.0 * 6.284939765930176
Epoch 1220, val loss: 1.6654646396636963
Epoch 1230, training loss: 6.299882888793945 = 0.019767794758081436 + 1.0 * 6.280115127563477
Epoch 1230, val loss: 1.673425555229187
Epoch 1240, training loss: 6.297683238983154 = 0.019263621419668198 + 1.0 * 6.278419494628906
Epoch 1240, val loss: 1.6814714670181274
Epoch 1250, training loss: 6.3018646240234375 = 0.018776731565594673 + 1.0 * 6.283087730407715
Epoch 1250, val loss: 1.6893258094787598
Epoch 1260, training loss: 6.298649311065674 = 0.01830950379371643 + 1.0 * 6.28033971786499
Epoch 1260, val loss: 1.6969072818756104
Epoch 1270, training loss: 6.295524597167969 = 0.017862489446997643 + 1.0 * 6.27766227722168
Epoch 1270, val loss: 1.7047349214553833
Epoch 1280, training loss: 6.293586730957031 = 0.017432332038879395 + 1.0 * 6.276154518127441
Epoch 1280, val loss: 1.7122533321380615
Epoch 1290, training loss: 6.297550201416016 = 0.0170173067599535 + 1.0 * 6.2805328369140625
Epoch 1290, val loss: 1.719821810722351
Epoch 1300, training loss: 6.29268741607666 = 0.01661604270339012 + 1.0 * 6.276071548461914
Epoch 1300, val loss: 1.7268741130828857
Epoch 1310, training loss: 6.291470527648926 = 0.01623157039284706 + 1.0 * 6.275238990783691
Epoch 1310, val loss: 1.7343157529830933
Epoch 1320, training loss: 6.29614782333374 = 0.01586005836725235 + 1.0 * 6.280287742614746
Epoch 1320, val loss: 1.741203784942627
Epoch 1330, training loss: 6.290923595428467 = 0.015501970425248146 + 1.0 * 6.275421619415283
Epoch 1330, val loss: 1.7483104467391968
Epoch 1340, training loss: 6.290463447570801 = 0.01515878550708294 + 1.0 * 6.275304794311523
Epoch 1340, val loss: 1.7553831338882446
Epoch 1350, training loss: 6.299384593963623 = 0.01482649426907301 + 1.0 * 6.284558296203613
Epoch 1350, val loss: 1.7621647119522095
Epoch 1360, training loss: 6.2903289794921875 = 0.01450362429022789 + 1.0 * 6.275825500488281
Epoch 1360, val loss: 1.7686595916748047
Epoch 1370, training loss: 6.287374019622803 = 0.014196261763572693 + 1.0 * 6.273177623748779
Epoch 1370, val loss: 1.7755115032196045
Epoch 1380, training loss: 6.28533411026001 = 0.013895968906581402 + 1.0 * 6.271438121795654
Epoch 1380, val loss: 1.7819836139678955
Epoch 1390, training loss: 6.28593111038208 = 0.013604715466499329 + 1.0 * 6.272326469421387
Epoch 1390, val loss: 1.7885069847106934
Epoch 1400, training loss: 6.291684150695801 = 0.013322165235877037 + 1.0 * 6.278361797332764
Epoch 1400, val loss: 1.79469895362854
Epoch 1410, training loss: 6.285273551940918 = 0.013051427900791168 + 1.0 * 6.27222204208374
Epoch 1410, val loss: 1.8012797832489014
Epoch 1420, training loss: 6.286729335784912 = 0.012789169326424599 + 1.0 * 6.273940086364746
Epoch 1420, val loss: 1.807532787322998
Epoch 1430, training loss: 6.283206939697266 = 0.012534565292298794 + 1.0 * 6.27067232131958
Epoch 1430, val loss: 1.8133515119552612
Epoch 1440, training loss: 6.282407760620117 = 0.012289516627788544 + 1.0 * 6.270118236541748
Epoch 1440, val loss: 1.8196688890457153
Epoch 1450, training loss: 6.285900115966797 = 0.012051695957779884 + 1.0 * 6.273848533630371
Epoch 1450, val loss: 1.825585961341858
Epoch 1460, training loss: 6.282732009887695 = 0.011820287443697453 + 1.0 * 6.270911693572998
Epoch 1460, val loss: 1.8314285278320312
Epoch 1470, training loss: 6.288480281829834 = 0.011596542783081532 + 1.0 * 6.276883602142334
Epoch 1470, val loss: 1.8372913599014282
Epoch 1480, training loss: 6.280717849731445 = 0.01138066966086626 + 1.0 * 6.269337177276611
Epoch 1480, val loss: 1.842926263809204
Epoch 1490, training loss: 6.278419017791748 = 0.011170082725584507 + 1.0 * 6.26724910736084
Epoch 1490, val loss: 1.848650574684143
Epoch 1500, training loss: 6.281162261962891 = 0.010964821092784405 + 1.0 * 6.27019739151001
Epoch 1500, val loss: 1.8541566133499146
Epoch 1510, training loss: 6.2808756828308105 = 0.010765846818685532 + 1.0 * 6.2701096534729
Epoch 1510, val loss: 1.8596818447113037
Epoch 1520, training loss: 6.283996105194092 = 0.010573840700089931 + 1.0 * 6.2734222412109375
Epoch 1520, val loss: 1.865256905555725
Epoch 1530, training loss: 6.276854991912842 = 0.010386751964688301 + 1.0 * 6.266468048095703
Epoch 1530, val loss: 1.8706097602844238
Epoch 1540, training loss: 6.2766242027282715 = 0.010205855593085289 + 1.0 * 6.26641845703125
Epoch 1540, val loss: 1.8759995698928833
Epoch 1550, training loss: 6.279482364654541 = 0.010029356926679611 + 1.0 * 6.269453048706055
Epoch 1550, val loss: 1.8811707496643066
Epoch 1560, training loss: 6.275156497955322 = 0.009857675060629845 + 1.0 * 6.265298843383789
Epoch 1560, val loss: 1.8866323232650757
Epoch 1570, training loss: 6.276040554046631 = 0.00969032570719719 + 1.0 * 6.266350269317627
Epoch 1570, val loss: 1.8916858434677124
Epoch 1580, training loss: 6.2822651863098145 = 0.009526915848255157 + 1.0 * 6.272738456726074
Epoch 1580, val loss: 1.8964000940322876
Epoch 1590, training loss: 6.2767133712768555 = 0.009370503015816212 + 1.0 * 6.267343044281006
Epoch 1590, val loss: 1.9018328189849854
Epoch 1600, training loss: 6.273707389831543 = 0.009217503480613232 + 1.0 * 6.264489650726318
Epoch 1600, val loss: 1.906721591949463
Epoch 1610, training loss: 6.277400970458984 = 0.009068261831998825 + 1.0 * 6.268332481384277
Epoch 1610, val loss: 1.911600947380066
Epoch 1620, training loss: 6.272052764892578 = 0.008922314271330833 + 1.0 * 6.2631306648254395
Epoch 1620, val loss: 1.9163485765457153
Epoch 1630, training loss: 6.273457050323486 = 0.008781375363469124 + 1.0 * 6.264675617218018
Epoch 1630, val loss: 1.921254277229309
Epoch 1640, training loss: 6.273653030395508 = 0.00864289328455925 + 1.0 * 6.265010356903076
Epoch 1640, val loss: 1.9260172843933105
Epoch 1650, training loss: 6.2749176025390625 = 0.008508056402206421 + 1.0 * 6.266409397125244
Epoch 1650, val loss: 1.9305475950241089
Epoch 1660, training loss: 6.272956848144531 = 0.008377271704375744 + 1.0 * 6.264579772949219
Epoch 1660, val loss: 1.9351763725280762
Epoch 1670, training loss: 6.269634246826172 = 0.008250278420746326 + 1.0 * 6.261384010314941
Epoch 1670, val loss: 1.9398380517959595
Epoch 1680, training loss: 6.267760276794434 = 0.008126163855195045 + 1.0 * 6.259634017944336
Epoch 1680, val loss: 1.9443309307098389
Epoch 1690, training loss: 6.269389629364014 = 0.008004246279597282 + 1.0 * 6.261385440826416
Epoch 1690, val loss: 1.9488561153411865
Epoch 1700, training loss: 6.277304172515869 = 0.007885300554335117 + 1.0 * 6.269418716430664
Epoch 1700, val loss: 1.9532175064086914
Epoch 1710, training loss: 6.270603179931641 = 0.00776976253837347 + 1.0 * 6.262833595275879
Epoch 1710, val loss: 1.9575495719909668
Epoch 1720, training loss: 6.268678665161133 = 0.00765803549438715 + 1.0 * 6.261020660400391
Epoch 1720, val loss: 1.9620585441589355
Epoch 1730, training loss: 6.2745361328125 = 0.00754868658259511 + 1.0 * 6.266987323760986
Epoch 1730, val loss: 1.9662491083145142
Epoch 1740, training loss: 6.268767356872559 = 0.007440517656505108 + 1.0 * 6.261326789855957
Epoch 1740, val loss: 1.9702682495117188
Epoch 1750, training loss: 6.26560640335083 = 0.007336577400565147 + 1.0 * 6.258269786834717
Epoch 1750, val loss: 1.9746862649917603
Epoch 1760, training loss: 6.266138076782227 = 0.007233601063489914 + 1.0 * 6.258904457092285
Epoch 1760, val loss: 1.9786632061004639
Epoch 1770, training loss: 6.274058818817139 = 0.0071328808553516865 + 1.0 * 6.266925811767578
Epoch 1770, val loss: 1.9826359748840332
Epoch 1780, training loss: 6.2783966064453125 = 0.007036346942186356 + 1.0 * 6.271360397338867
Epoch 1780, val loss: 1.9869296550750732
Epoch 1790, training loss: 6.26599645614624 = 0.006941063795238733 + 1.0 * 6.2590556144714355
Epoch 1790, val loss: 1.9907644987106323
Epoch 1800, training loss: 6.263411998748779 = 0.006848778109997511 + 1.0 * 6.256563186645508
Epoch 1800, val loss: 1.9948632717132568
Epoch 1810, training loss: 6.263443470001221 = 0.006757591851055622 + 1.0 * 6.256685733795166
Epoch 1810, val loss: 1.9987003803253174
Epoch 1820, training loss: 6.2660040855407715 = 0.006667840760201216 + 1.0 * 6.259336471557617
Epoch 1820, val loss: 2.002553701400757
Epoch 1830, training loss: 6.2655348777771 = 0.006579720880836248 + 1.0 * 6.258955001831055
Epoch 1830, val loss: 2.006436347961426
Epoch 1840, training loss: 6.26746940612793 = 0.006494608242064714 + 1.0 * 6.260974884033203
Epoch 1840, val loss: 2.0104146003723145
Epoch 1850, training loss: 6.273280620574951 = 0.00641105929389596 + 1.0 * 6.26686954498291
Epoch 1850, val loss: 2.0140650272369385
Epoch 1860, training loss: 6.263937950134277 = 0.006330345291644335 + 1.0 * 6.257607460021973
Epoch 1860, val loss: 2.017645835876465
Epoch 1870, training loss: 6.26207160949707 = 0.006251574028283358 + 1.0 * 6.255819797515869
Epoch 1870, val loss: 2.021519184112549
Epoch 1880, training loss: 6.260720252990723 = 0.006173227913677692 + 1.0 * 6.254547119140625
Epoch 1880, val loss: 2.02512788772583
Epoch 1890, training loss: 6.261439323425293 = 0.006096352357417345 + 1.0 * 6.255342960357666
Epoch 1890, val loss: 2.0288350582122803
Epoch 1900, training loss: 6.267633438110352 = 0.006021267734467983 + 1.0 * 6.2616119384765625
Epoch 1900, val loss: 2.0324625968933105
Epoch 1910, training loss: 6.2642645835876465 = 0.0059471349231898785 + 1.0 * 6.258317470550537
Epoch 1910, val loss: 2.03584885597229
Epoch 1920, training loss: 6.26451301574707 = 0.005875698756426573 + 1.0 * 6.258637428283691
Epoch 1920, val loss: 2.039522409439087
Epoch 1930, training loss: 6.25993537902832 = 0.005805017426609993 + 1.0 * 6.2541303634643555
Epoch 1930, val loss: 2.04280948638916
Epoch 1940, training loss: 6.264161109924316 = 0.005736116785556078 + 1.0 * 6.258424758911133
Epoch 1940, val loss: 2.0463907718658447
Epoch 1950, training loss: 6.26103401184082 = 0.005668509751558304 + 1.0 * 6.255365371704102
Epoch 1950, val loss: 2.049696922302246
Epoch 1960, training loss: 6.259982585906982 = 0.005602776072919369 + 1.0 * 6.254379749298096
Epoch 1960, val loss: 2.0530757904052734
Epoch 1970, training loss: 6.260684490203857 = 0.005537840537726879 + 1.0 * 6.255146503448486
Epoch 1970, val loss: 2.0564379692077637
Epoch 1980, training loss: 6.263123512268066 = 0.005474185571074486 + 1.0 * 6.2576494216918945
Epoch 1980, val loss: 2.059608221054077
Epoch 1990, training loss: 6.261314868927002 = 0.005411475896835327 + 1.0 * 6.255903244018555
Epoch 1990, val loss: 2.0628082752227783
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8181338956246705
The final CL Acc:0.74321, 0.00698, The final GNN Acc:0.81796, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13108])
remove edge: torch.Size([2, 7922])
updated graph: torch.Size([2, 10474])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.542549133300781 = 1.9457542896270752 + 1.0 * 8.596795082092285
Epoch 0, val loss: 1.9542485475540161
Epoch 10, training loss: 10.53251838684082 = 1.936151385307312 + 1.0 * 8.596366882324219
Epoch 10, val loss: 1.944518804550171
Epoch 20, training loss: 10.516600608825684 = 1.9240795373916626 + 1.0 * 8.592520713806152
Epoch 20, val loss: 1.9316527843475342
Epoch 30, training loss: 10.469512939453125 = 1.9070004224777222 + 1.0 * 8.562512397766113
Epoch 30, val loss: 1.9126510620117188
Epoch 40, training loss: 10.257257461547852 = 1.8855643272399902 + 1.0 * 8.37169361114502
Epoch 40, val loss: 1.889559030532837
Epoch 50, training loss: 9.795453071594238 = 1.8620973825454712 + 1.0 * 7.933355808258057
Epoch 50, val loss: 1.8658639192581177
Epoch 60, training loss: 9.364509582519531 = 1.8431864976882935 + 1.0 * 7.521322727203369
Epoch 60, val loss: 1.8485273122787476
Epoch 70, training loss: 8.942451477050781 = 1.8288819789886475 + 1.0 * 7.113569736480713
Epoch 70, val loss: 1.8349577188491821
Epoch 80, training loss: 8.741975784301758 = 1.8168190717697144 + 1.0 * 6.925156593322754
Epoch 80, val loss: 1.82301926612854
Epoch 90, training loss: 8.619518280029297 = 1.8009787797927856 + 1.0 * 6.818539619445801
Epoch 90, val loss: 1.8077521324157715
Epoch 100, training loss: 8.524274826049805 = 1.7832164764404297 + 1.0 * 6.741058826446533
Epoch 100, val loss: 1.7908650636672974
Epoch 110, training loss: 8.466339111328125 = 1.7660048007965088 + 1.0 * 6.700334548950195
Epoch 110, val loss: 1.7746607065200806
Epoch 120, training loss: 8.412059783935547 = 1.7489033937454224 + 1.0 * 6.663156509399414
Epoch 120, val loss: 1.7589699029922485
Epoch 130, training loss: 8.359842300415039 = 1.7313685417175293 + 1.0 * 6.628473281860352
Epoch 130, val loss: 1.7431585788726807
Epoch 140, training loss: 8.302176475524902 = 1.7122339010238647 + 1.0 * 6.589942455291748
Epoch 140, val loss: 1.726402997970581
Epoch 150, training loss: 8.253355026245117 = 1.690368890762329 + 1.0 * 6.562985897064209
Epoch 150, val loss: 1.7077444791793823
Epoch 160, training loss: 8.195305824279785 = 1.6653903722763062 + 1.0 * 6.5299153327941895
Epoch 160, val loss: 1.6868900060653687
Epoch 170, training loss: 8.142437934875488 = 1.6367534399032593 + 1.0 * 6.505684852600098
Epoch 170, val loss: 1.6628319025039673
Epoch 180, training loss: 8.093551635742188 = 1.6034365892410278 + 1.0 * 6.490115165710449
Epoch 180, val loss: 1.635031819343567
Epoch 190, training loss: 8.038017272949219 = 1.5656017065048218 + 1.0 * 6.472415447235107
Epoch 190, val loss: 1.6035494804382324
Epoch 200, training loss: 7.983329772949219 = 1.5229390859603882 + 1.0 * 6.460390567779541
Epoch 200, val loss: 1.5683314800262451
Epoch 210, training loss: 7.925020694732666 = 1.4758638143539429 + 1.0 * 6.449156761169434
Epoch 210, val loss: 1.529802918434143
Epoch 220, training loss: 7.864352226257324 = 1.4246917963027954 + 1.0 * 6.439660549163818
Epoch 220, val loss: 1.4884252548217773
Epoch 230, training loss: 7.80462646484375 = 1.3707553148269653 + 1.0 * 6.433871269226074
Epoch 230, val loss: 1.4454820156097412
Epoch 240, training loss: 7.741077423095703 = 1.3157505989074707 + 1.0 * 6.425326824188232
Epoch 240, val loss: 1.4021836519241333
Epoch 250, training loss: 7.678458213806152 = 1.2600924968719482 + 1.0 * 6.418365478515625
Epoch 250, val loss: 1.3588112592697144
Epoch 260, training loss: 7.618060111999512 = 1.204386830329895 + 1.0 * 6.413673400878906
Epoch 260, val loss: 1.3156856298446655
Epoch 270, training loss: 7.560394287109375 = 1.1510330438613892 + 1.0 * 6.409361362457275
Epoch 270, val loss: 1.2745250463485718
Epoch 280, training loss: 7.50510311126709 = 1.100853681564331 + 1.0 * 6.40424919128418
Epoch 280, val loss: 1.2358094453811646
Epoch 290, training loss: 7.451578140258789 = 1.0534517765045166 + 1.0 * 6.398126602172852
Epoch 290, val loss: 1.1992461681365967
Epoch 300, training loss: 7.402956485748291 = 1.0087848901748657 + 1.0 * 6.394171714782715
Epoch 300, val loss: 1.1648253202438354
Epoch 310, training loss: 7.357579231262207 = 0.9666976928710938 + 1.0 * 6.390881538391113
Epoch 310, val loss: 1.132191777229309
Epoch 320, training loss: 7.319268226623535 = 0.9263601899147034 + 1.0 * 6.392908096313477
Epoch 320, val loss: 1.1009305715560913
Epoch 330, training loss: 7.271635055541992 = 0.8875737190246582 + 1.0 * 6.384061336517334
Epoch 330, val loss: 1.0706804990768433
Epoch 340, training loss: 7.230066299438477 = 0.8496699333190918 + 1.0 * 6.380396366119385
Epoch 340, val loss: 1.0409317016601562
Epoch 350, training loss: 7.1898908615112305 = 0.8123889565467834 + 1.0 * 6.377501964569092
Epoch 350, val loss: 1.0115000009536743
Epoch 360, training loss: 7.148454189300537 = 0.7756888270378113 + 1.0 * 6.37276554107666
Epoch 360, val loss: 0.9824329614639282
Epoch 370, training loss: 7.110688209533691 = 0.7395451068878174 + 1.0 * 6.371143341064453
Epoch 370, val loss: 0.9538516402244568
Epoch 380, training loss: 7.074575424194336 = 0.7045605778694153 + 1.0 * 6.370014667510986
Epoch 380, val loss: 0.9263619184494019
Epoch 390, training loss: 7.037136554718018 = 0.6711755990982056 + 1.0 * 6.365961074829102
Epoch 390, val loss: 0.9005510807037354
Epoch 400, training loss: 7.003610610961914 = 0.6393140554428101 + 1.0 * 6.3642964363098145
Epoch 400, val loss: 0.8765307664871216
Epoch 410, training loss: 6.968554973602295 = 0.6090153455734253 + 1.0 * 6.35953950881958
Epoch 410, val loss: 0.8543784618377686
Epoch 420, training loss: 6.942430019378662 = 0.580152153968811 + 1.0 * 6.362277984619141
Epoch 420, val loss: 0.8342228531837463
Epoch 430, training loss: 6.907848834991455 = 0.5529948472976685 + 1.0 * 6.354854106903076
Epoch 430, val loss: 0.8161854147911072
Epoch 440, training loss: 6.8785929679870605 = 0.5273795127868652 + 1.0 * 6.351213455200195
Epoch 440, val loss: 0.8001114726066589
Epoch 450, training loss: 6.851540565490723 = 0.5029296875 + 1.0 * 6.348610877990723
Epoch 450, val loss: 0.7855757474899292
Epoch 460, training loss: 6.825389862060547 = 0.47942477464675903 + 1.0 * 6.3459649085998535
Epoch 460, val loss: 0.7723433375358582
Epoch 470, training loss: 6.8186564445495605 = 0.45672839879989624 + 1.0 * 6.3619279861450195
Epoch 470, val loss: 0.7602307200431824
Epoch 480, training loss: 6.783705711364746 = 0.43499377369880676 + 1.0 * 6.348711967468262
Epoch 480, val loss: 0.7493240833282471
Epoch 490, training loss: 6.757008075714111 = 0.413858562707901 + 1.0 * 6.343149662017822
Epoch 490, val loss: 0.7392274141311646
Epoch 500, training loss: 6.731018543243408 = 0.39297524094581604 + 1.0 * 6.338043212890625
Epoch 500, val loss: 0.7295562028884888
Epoch 510, training loss: 6.710616111755371 = 0.3721737265586853 + 1.0 * 6.338442325592041
Epoch 510, val loss: 0.7202398180961609
Epoch 520, training loss: 6.6858696937561035 = 0.3515256643295288 + 1.0 * 6.334343910217285
Epoch 520, val loss: 0.7112864255905151
Epoch 530, training loss: 6.671721935272217 = 0.3311249315738678 + 1.0 * 6.340597152709961
Epoch 530, val loss: 0.7027488350868225
Epoch 540, training loss: 6.6438798904418945 = 0.31107592582702637 + 1.0 * 6.332804203033447
Epoch 540, val loss: 0.6946797966957092
Epoch 550, training loss: 6.620855808258057 = 0.2914223372936249 + 1.0 * 6.329433441162109
Epoch 550, val loss: 0.6871092319488525
Epoch 560, training loss: 6.602630138397217 = 0.27231690287590027 + 1.0 * 6.330313205718994
Epoch 560, val loss: 0.6801790595054626
Epoch 570, training loss: 6.5901689529418945 = 0.254031777381897 + 1.0 * 6.336137294769287
Epoch 570, val loss: 0.6741178631782532
Epoch 580, training loss: 6.565481662750244 = 0.23683024942874908 + 1.0 * 6.328651428222656
Epoch 580, val loss: 0.6689446568489075
Epoch 590, training loss: 6.546040058135986 = 0.22064882516860962 + 1.0 * 6.3253912925720215
Epoch 590, val loss: 0.6646585464477539
Epoch 600, training loss: 6.53135871887207 = 0.20550888776779175 + 1.0 * 6.325850009918213
Epoch 600, val loss: 0.661313533782959
Epoch 610, training loss: 6.515386581420898 = 0.1915181279182434 + 1.0 * 6.323868274688721
Epoch 610, val loss: 0.6589579582214355
Epoch 620, training loss: 6.501990795135498 = 0.1786852926015854 + 1.0 * 6.323305606842041
Epoch 620, val loss: 0.6574533581733704
Epoch 630, training loss: 6.487884521484375 = 0.1668698787689209 + 1.0 * 6.321014881134033
Epoch 630, val loss: 0.6567335724830627
Epoch 640, training loss: 6.47742223739624 = 0.15605981647968292 + 1.0 * 6.321362495422363
Epoch 640, val loss: 0.6567655205726624
Epoch 650, training loss: 6.463589191436768 = 0.14619913697242737 + 1.0 * 6.317389965057373
Epoch 650, val loss: 0.6574249267578125
Epoch 660, training loss: 6.451847076416016 = 0.13713352382183075 + 1.0 * 6.314713478088379
Epoch 660, val loss: 0.6586377024650574
Epoch 670, training loss: 6.446163654327393 = 0.12879371643066406 + 1.0 * 6.3173699378967285
Epoch 670, val loss: 0.6603296995162964
Epoch 680, training loss: 6.438260555267334 = 0.12114628404378891 + 1.0 * 6.317114353179932
Epoch 680, val loss: 0.6624177098274231
Epoch 690, training loss: 6.432368755340576 = 0.1141219511628151 + 1.0 * 6.318246841430664
Epoch 690, val loss: 0.6648628115653992
Epoch 700, training loss: 6.418705940246582 = 0.10763370245695114 + 1.0 * 6.31107234954834
Epoch 700, val loss: 0.6675524115562439
Epoch 710, training loss: 6.410974025726318 = 0.10161729156970978 + 1.0 * 6.309356689453125
Epoch 710, val loss: 0.670494794845581
Epoch 720, training loss: 6.410420894622803 = 0.09601134806871414 + 1.0 * 6.3144097328186035
Epoch 720, val loss: 0.6736906170845032
Epoch 730, training loss: 6.405354022979736 = 0.09081922471523285 + 1.0 * 6.314534664154053
Epoch 730, val loss: 0.6770814657211304
Epoch 740, training loss: 6.393496990203857 = 0.08600916713476181 + 1.0 * 6.307487964630127
Epoch 740, val loss: 0.6806092262268066
Epoch 750, training loss: 6.387897491455078 = 0.08151231706142426 + 1.0 * 6.306385040283203
Epoch 750, val loss: 0.6842541098594666
Epoch 760, training loss: 6.383678436279297 = 0.07730191946029663 + 1.0 * 6.3063764572143555
Epoch 760, val loss: 0.6880752444267273
Epoch 770, training loss: 6.376110553741455 = 0.07338183373212814 + 1.0 * 6.302728652954102
Epoch 770, val loss: 0.6920218467712402
Epoch 780, training loss: 6.372513294219971 = 0.06972133368253708 + 1.0 * 6.302792072296143
Epoch 780, val loss: 0.6960408687591553
Epoch 790, training loss: 6.371756553649902 = 0.06629757583141327 + 1.0 * 6.305459022521973
Epoch 790, val loss: 0.7001461982727051
Epoch 800, training loss: 6.367412090301514 = 0.06309475749731064 + 1.0 * 6.304317474365234
Epoch 800, val loss: 0.7042997479438782
Epoch 810, training loss: 6.359591484069824 = 0.06009300425648689 + 1.0 * 6.299498558044434
Epoch 810, val loss: 0.7085371613502502
Epoch 820, training loss: 6.362177848815918 = 0.05727739259600639 + 1.0 * 6.304900646209717
Epoch 820, val loss: 0.7128276824951172
Epoch 830, training loss: 6.355700492858887 = 0.05465257912874222 + 1.0 * 6.3010478019714355
Epoch 830, val loss: 0.7171271443367004
Epoch 840, training loss: 6.349815845489502 = 0.05218510702252388 + 1.0 * 6.297630786895752
Epoch 840, val loss: 0.7214984893798828
Epoch 850, training loss: 6.349949836730957 = 0.04986632615327835 + 1.0 * 6.300083637237549
Epoch 850, val loss: 0.7259140014648438
Epoch 860, training loss: 6.343865394592285 = 0.04769594594836235 + 1.0 * 6.296169281005859
Epoch 860, val loss: 0.7303025126457214
Epoch 870, training loss: 6.343109607696533 = 0.045655544847249985 + 1.0 * 6.297453880310059
Epoch 870, val loss: 0.7347949743270874
Epoch 880, training loss: 6.337595462799072 = 0.04373838007450104 + 1.0 * 6.293857097625732
Epoch 880, val loss: 0.739233136177063
Epoch 890, training loss: 6.342430114746094 = 0.04193738102912903 + 1.0 * 6.300492763519287
Epoch 890, val loss: 0.7437039613723755
Epoch 900, training loss: 6.331789970397949 = 0.04024230316281319 + 1.0 * 6.291547775268555
Epoch 900, val loss: 0.7481575012207031
Epoch 910, training loss: 6.328751087188721 = 0.03864297643303871 + 1.0 * 6.2901082038879395
Epoch 910, val loss: 0.752617597579956
Epoch 920, training loss: 6.327732086181641 = 0.03713051229715347 + 1.0 * 6.29060173034668
Epoch 920, val loss: 0.7570905089378357
Epoch 930, training loss: 6.333942413330078 = 0.03569798544049263 + 1.0 * 6.298244476318359
Epoch 930, val loss: 0.7615741491317749
Epoch 940, training loss: 6.32447624206543 = 0.034359659999608994 + 1.0 * 6.290116786956787
Epoch 940, val loss: 0.7660035490989685
Epoch 950, training loss: 6.322155475616455 = 0.033094413578510284 + 1.0 * 6.289061069488525
Epoch 950, val loss: 0.7703943848609924
Epoch 960, training loss: 6.322807312011719 = 0.03189713880419731 + 1.0 * 6.290910243988037
Epoch 960, val loss: 0.7747506499290466
Epoch 970, training loss: 6.316272258758545 = 0.030759740620851517 + 1.0 * 6.285512447357178
Epoch 970, val loss: 0.7791315913200378
Epoch 980, training loss: 6.3169121742248535 = 0.02967894822359085 + 1.0 * 6.287233352661133
Epoch 980, val loss: 0.783513605594635
Epoch 990, training loss: 6.318465709686279 = 0.0286544281989336 + 1.0 * 6.289811134338379
Epoch 990, val loss: 0.7878676056861877
Epoch 1000, training loss: 6.313589572906494 = 0.027683114632964134 + 1.0 * 6.2859063148498535
Epoch 1000, val loss: 0.7921540141105652
Epoch 1010, training loss: 6.315896987915039 = 0.02676532231271267 + 1.0 * 6.2891316413879395
Epoch 1010, val loss: 0.7964171767234802
Epoch 1020, training loss: 6.310174465179443 = 0.02589176595211029 + 1.0 * 6.284282684326172
Epoch 1020, val loss: 0.8006138205528259
Epoch 1030, training loss: 6.307356834411621 = 0.025059238076210022 + 1.0 * 6.282297611236572
Epoch 1030, val loss: 0.8048350214958191
Epoch 1040, training loss: 6.313085079193115 = 0.02426460199058056 + 1.0 * 6.288820266723633
Epoch 1040, val loss: 0.80899977684021
Epoch 1050, training loss: 6.307626247406006 = 0.02351268194615841 + 1.0 * 6.28411340713501
Epoch 1050, val loss: 0.8131162524223328
Epoch 1060, training loss: 6.303558826446533 = 0.02279246412217617 + 1.0 * 6.280766487121582
Epoch 1060, val loss: 0.8172154426574707
Epoch 1070, training loss: 6.3017258644104 = 0.022105252370238304 + 1.0 * 6.27962064743042
Epoch 1070, val loss: 0.8212505578994751
Epoch 1080, training loss: 6.3080034255981445 = 0.021448258310556412 + 1.0 * 6.286555290222168
Epoch 1080, val loss: 0.8252958059310913
Epoch 1090, training loss: 6.306356430053711 = 0.020822418853640556 + 1.0 * 6.285533905029297
Epoch 1090, val loss: 0.8293264508247375
Epoch 1100, training loss: 6.299095630645752 = 0.020224859938025475 + 1.0 * 6.278870582580566
Epoch 1100, val loss: 0.8332094550132751
Epoch 1110, training loss: 6.297098159790039 = 0.01965191774070263 + 1.0 * 6.277446269989014
Epoch 1110, val loss: 0.8371400237083435
Epoch 1120, training loss: 6.29679536819458 = 0.01910117268562317 + 1.0 * 6.277694225311279
Epoch 1120, val loss: 0.8410062193870544
Epoch 1130, training loss: 6.300405025482178 = 0.018573589622974396 + 1.0 * 6.28183126449585
Epoch 1130, val loss: 0.8448764085769653
Epoch 1140, training loss: 6.297611713409424 = 0.018073400482535362 + 1.0 * 6.279538154602051
Epoch 1140, val loss: 0.8486623167991638
Epoch 1150, training loss: 6.293868541717529 = 0.0175926610827446 + 1.0 * 6.276276111602783
Epoch 1150, val loss: 0.8523942828178406
Epoch 1160, training loss: 6.293581008911133 = 0.0171317420899868 + 1.0 * 6.276449203491211
Epoch 1160, val loss: 0.8560665845870972
Epoch 1170, training loss: 6.29589319229126 = 0.016688615083694458 + 1.0 * 6.279204368591309
Epoch 1170, val loss: 0.8597397804260254
Epoch 1180, training loss: 6.290589809417725 = 0.01626235619187355 + 1.0 * 6.274327278137207
Epoch 1180, val loss: 0.8633521795272827
Epoch 1190, training loss: 6.28920841217041 = 0.015853222459554672 + 1.0 * 6.273355007171631
Epoch 1190, val loss: 0.866934061050415
Epoch 1200, training loss: 6.292928695678711 = 0.015456839464604855 + 1.0 * 6.277472019195557
Epoch 1200, val loss: 0.870498538017273
Epoch 1210, training loss: 6.285951137542725 = 0.015078390948474407 + 1.0 * 6.270872592926025
Epoch 1210, val loss: 0.8740530610084534
Epoch 1220, training loss: 6.286356449127197 = 0.01471356488764286 + 1.0 * 6.271642684936523
Epoch 1220, val loss: 0.8775454759597778
Epoch 1230, training loss: 6.2961835861206055 = 0.014362264424562454 + 1.0 * 6.281821250915527
Epoch 1230, val loss: 0.8809933066368103
Epoch 1240, training loss: 6.287693977355957 = 0.014025500975549221 + 1.0 * 6.27366828918457
Epoch 1240, val loss: 0.8843926787376404
Epoch 1250, training loss: 6.285459995269775 = 0.013701638206839561 + 1.0 * 6.271758556365967
Epoch 1250, val loss: 0.8877484798431396
Epoch 1260, training loss: 6.282605171203613 = 0.013388578779995441 + 1.0 * 6.269216537475586
Epoch 1260, val loss: 0.8911035060882568
Epoch 1270, training loss: 6.282089710235596 = 0.013084907084703445 + 1.0 * 6.269004821777344
Epoch 1270, val loss: 0.8944384455680847
Epoch 1280, training loss: 6.2854743003845215 = 0.012790902517735958 + 1.0 * 6.272683620452881
Epoch 1280, val loss: 0.897758424282074
Epoch 1290, training loss: 6.288943767547607 = 0.012509706430137157 + 1.0 * 6.276433944702148
Epoch 1290, val loss: 0.9010471105575562
Epoch 1300, training loss: 6.283010959625244 = 0.012238625437021255 + 1.0 * 6.270772457122803
Epoch 1300, val loss: 0.9042643904685974
Epoch 1310, training loss: 6.281940460205078 = 0.011976867914199829 + 1.0 * 6.26996374130249
Epoch 1310, val loss: 0.9074727892875671
Epoch 1320, training loss: 6.279315948486328 = 0.011723061092197895 + 1.0 * 6.267592906951904
Epoch 1320, val loss: 0.9106148481369019
Epoch 1330, training loss: 6.282151222229004 = 0.011477623134851456 + 1.0 * 6.270673751831055
Epoch 1330, val loss: 0.9137461185455322
Epoch 1340, training loss: 6.278810977935791 = 0.011239935643970966 + 1.0 * 6.267570972442627
Epoch 1340, val loss: 0.916870653629303
Epoch 1350, training loss: 6.276349067687988 = 0.011011045426130295 + 1.0 * 6.265337944030762
Epoch 1350, val loss: 0.9199162721633911
Epoch 1360, training loss: 6.2752556800842285 = 0.010788633488118649 + 1.0 * 6.264467239379883
Epoch 1360, val loss: 0.9229555726051331
Epoch 1370, training loss: 6.27977991104126 = 0.010572011582553387 + 1.0 * 6.269207954406738
Epoch 1370, val loss: 0.9260045886039734
Epoch 1380, training loss: 6.27863883972168 = 0.01036405935883522 + 1.0 * 6.268274784088135
Epoch 1380, val loss: 0.9290030002593994
Epoch 1390, training loss: 6.277402400970459 = 0.010162964463233948 + 1.0 * 6.267239570617676
Epoch 1390, val loss: 0.9319246411323547
Epoch 1400, training loss: 6.274198055267334 = 0.009968564845621586 + 1.0 * 6.2642292976379395
Epoch 1400, val loss: 0.9348056316375732
Epoch 1410, training loss: 6.27241849899292 = 0.009778836742043495 + 1.0 * 6.26263952255249
Epoch 1410, val loss: 0.9376877546310425
Epoch 1420, training loss: 6.284793376922607 = 0.009594157338142395 + 1.0 * 6.2751994132995605
Epoch 1420, val loss: 0.9405819773674011
Epoch 1430, training loss: 6.275539398193359 = 0.009417342953383923 + 1.0 * 6.266121864318848
Epoch 1430, val loss: 0.9433546662330627
Epoch 1440, training loss: 6.272543907165527 = 0.009244212880730629 + 1.0 * 6.263299465179443
Epoch 1440, val loss: 0.9461706876754761
Epoch 1450, training loss: 6.277814865112305 = 0.009077193215489388 + 1.0 * 6.26873779296875
Epoch 1450, val loss: 0.9489054083824158
Epoch 1460, training loss: 6.270139694213867 = 0.008913176134228706 + 1.0 * 6.261226654052734
Epoch 1460, val loss: 0.951654851436615
Epoch 1470, training loss: 6.272800445556641 = 0.008755412884056568 + 1.0 * 6.264045238494873
Epoch 1470, val loss: 0.9543370008468628
Epoch 1480, training loss: 6.271466255187988 = 0.008602030575275421 + 1.0 * 6.262864112854004
Epoch 1480, val loss: 0.9570319056510925
Epoch 1490, training loss: 6.271596431732178 = 0.008452648296952248 + 1.0 * 6.263144016265869
Epoch 1490, val loss: 0.9597060680389404
Epoch 1500, training loss: 6.269284725189209 = 0.00830730702728033 + 1.0 * 6.260977268218994
Epoch 1500, val loss: 0.9623262882232666
Epoch 1510, training loss: 6.26731538772583 = 0.008165877312421799 + 1.0 * 6.259149551391602
Epoch 1510, val loss: 0.9649122357368469
Epoch 1520, training loss: 6.2681145668029785 = 0.008027478121221066 + 1.0 * 6.260087013244629
Epoch 1520, val loss: 0.9675055742263794
Epoch 1530, training loss: 6.270238399505615 = 0.007892313413321972 + 1.0 * 6.262346267700195
Epoch 1530, val loss: 0.970102846622467
Epoch 1540, training loss: 6.272288799285889 = 0.0077623543329536915 + 1.0 * 6.2645263671875
Epoch 1540, val loss: 0.9726214408874512
Epoch 1550, training loss: 6.266475677490234 = 0.007637010887265205 + 1.0 * 6.258838653564453
Epoch 1550, val loss: 0.9751462340354919
Epoch 1560, training loss: 6.267542839050293 = 0.0075135608203709126 + 1.0 * 6.260029315948486
Epoch 1560, val loss: 0.9776264429092407
Epoch 1570, training loss: 6.269606590270996 = 0.007393782492727041 + 1.0 * 6.262212753295898
Epoch 1570, val loss: 0.9800750613212585
Epoch 1580, training loss: 6.26671838760376 = 0.0072775776498019695 + 1.0 * 6.259440898895264
Epoch 1580, val loss: 0.9825364947319031
Epoch 1590, training loss: 6.265458583831787 = 0.007164348382502794 + 1.0 * 6.258294105529785
Epoch 1590, val loss: 0.9849613904953003
Epoch 1600, training loss: 6.263620376586914 = 0.007053525652736425 + 1.0 * 6.256567001342773
Epoch 1600, val loss: 0.9873412251472473
Epoch 1610, training loss: 6.2712836265563965 = 0.006944651249796152 + 1.0 * 6.264338970184326
Epoch 1610, val loss: 0.9897552728652954
Epoch 1620, training loss: 6.2647013664245605 = 0.00683958362787962 + 1.0 * 6.257861614227295
Epoch 1620, val loss: 0.9921452403068542
Epoch 1630, training loss: 6.26205587387085 = 0.006736936047673225 + 1.0 * 6.255319118499756
Epoch 1630, val loss: 0.9944593906402588
Epoch 1640, training loss: 6.26389741897583 = 0.006636017933487892 + 1.0 * 6.257261276245117
Epoch 1640, val loss: 0.996788740158081
Epoch 1650, training loss: 6.265285015106201 = 0.006537971086800098 + 1.0 * 6.258747100830078
Epoch 1650, val loss: 0.9991198778152466
Epoch 1660, training loss: 6.265610694885254 = 0.0064424918964505196 + 1.0 * 6.2591681480407715
Epoch 1660, val loss: 1.001415491104126
Epoch 1670, training loss: 6.26627254486084 = 0.006349836941808462 + 1.0 * 6.259922504425049
Epoch 1670, val loss: 1.003664493560791
Epoch 1680, training loss: 6.262272357940674 = 0.006260286085307598 + 1.0 * 6.256011962890625
Epoch 1680, val loss: 1.0058841705322266
Epoch 1690, training loss: 6.260872840881348 = 0.006171552464365959 + 1.0 * 6.254701137542725
Epoch 1690, val loss: 1.0080851316452026
Epoch 1700, training loss: 6.264306545257568 = 0.00608431501314044 + 1.0 * 6.2582221031188965
Epoch 1700, val loss: 1.0103298425674438
Epoch 1710, training loss: 6.259038925170898 = 0.005999332759529352 + 1.0 * 6.253039360046387
Epoch 1710, val loss: 1.012487769126892
Epoch 1720, training loss: 6.261289119720459 = 0.005916311871260405 + 1.0 * 6.255373001098633
Epoch 1720, val loss: 1.014666199684143
Epoch 1730, training loss: 6.266346454620361 = 0.005835128482431173 + 1.0 * 6.26051139831543
Epoch 1730, val loss: 1.0168710947036743
Epoch 1740, training loss: 6.261118412017822 = 0.005756831727921963 + 1.0 * 6.255361557006836
Epoch 1740, val loss: 1.0189481973648071
Epoch 1750, training loss: 6.259697914123535 = 0.0056799170561134815 + 1.0 * 6.2540178298950195
Epoch 1750, val loss: 1.0210665464401245
Epoch 1760, training loss: 6.259578227996826 = 0.005604328587651253 + 1.0 * 6.253973960876465
Epoch 1760, val loss: 1.0231424570083618
Epoch 1770, training loss: 6.257317543029785 = 0.005530265625566244 + 1.0 * 6.251787185668945
Epoch 1770, val loss: 1.025215744972229
Epoch 1780, training loss: 6.260070323944092 = 0.005457367282360792 + 1.0 * 6.254612922668457
Epoch 1780, val loss: 1.0272892713546753
Epoch 1790, training loss: 6.261237621307373 = 0.0053863232024014 + 1.0 * 6.2558512687683105
Epoch 1790, val loss: 1.0293235778808594
Epoch 1800, training loss: 6.258209705352783 = 0.005317704752087593 + 1.0 * 6.252892017364502
Epoch 1800, val loss: 1.0313578844070435
Epoch 1810, training loss: 6.258741855621338 = 0.005250726360827684 + 1.0 * 6.253490924835205
Epoch 1810, val loss: 1.033334732055664
Epoch 1820, training loss: 6.256114959716797 = 0.005184829700738192 + 1.0 * 6.250930309295654
Epoch 1820, val loss: 1.0353232622146606
Epoch 1830, training loss: 6.258335590362549 = 0.005120215471833944 + 1.0 * 6.253215312957764
Epoch 1830, val loss: 1.037279486656189
Epoch 1840, training loss: 6.255927562713623 = 0.00505641708150506 + 1.0 * 6.250871181488037
Epoch 1840, val loss: 1.039247751235962
Epoch 1850, training loss: 6.258915901184082 = 0.004993993788957596 + 1.0 * 6.253921985626221
Epoch 1850, val loss: 1.0412216186523438
Epoch 1860, training loss: 6.256073474884033 = 0.004933155607432127 + 1.0 * 6.251140117645264
Epoch 1860, val loss: 1.043107032775879
Epoch 1870, training loss: 6.254299640655518 = 0.004873308818787336 + 1.0 * 6.249426364898682
Epoch 1870, val loss: 1.0449984073638916
Epoch 1880, training loss: 6.260073184967041 = 0.004814754240214825 + 1.0 * 6.255258560180664
Epoch 1880, val loss: 1.0468777418136597
Epoch 1890, training loss: 6.257357120513916 = 0.004757672548294067 + 1.0 * 6.252599239349365
Epoch 1890, val loss: 1.0487674474716187
Epoch 1900, training loss: 6.254061222076416 = 0.004701835568994284 + 1.0 * 6.249359607696533
Epoch 1900, val loss: 1.0506092309951782
Epoch 1910, training loss: 6.259673595428467 = 0.004647118505090475 + 1.0 * 6.255026340484619
Epoch 1910, val loss: 1.0523968935012817
Epoch 1920, training loss: 6.253327369689941 = 0.0045936331152915955 + 1.0 * 6.2487335205078125
Epoch 1920, val loss: 1.054250717163086
Epoch 1930, training loss: 6.254880428314209 = 0.004541528411209583 + 1.0 * 6.250339031219482
Epoch 1930, val loss: 1.0560262203216553
Epoch 1940, training loss: 6.255157470703125 = 0.00448981998488307 + 1.0 * 6.250667572021484
Epoch 1940, val loss: 1.0578027963638306
Epoch 1950, training loss: 6.251819133758545 = 0.004439025651663542 + 1.0 * 6.247380256652832
Epoch 1950, val loss: 1.0595786571502686
Epoch 1960, training loss: 6.254021167755127 = 0.0043891556560993195 + 1.0 * 6.249631881713867
Epoch 1960, val loss: 1.061321496963501
Epoch 1970, training loss: 6.256753921508789 = 0.004340553190559149 + 1.0 * 6.252413272857666
Epoch 1970, val loss: 1.0630861520767212
Epoch 1980, training loss: 6.252711772918701 = 0.004292851313948631 + 1.0 * 6.248418807983398
Epoch 1980, val loss: 1.0648417472839355
Epoch 1990, training loss: 6.2521233558654785 = 0.004246299620717764 + 1.0 * 6.24787712097168
Epoch 1990, val loss: 1.0665141344070435
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 10.545746803283691 = 1.9488990306854248 + 1.0 * 8.596847534179688
Epoch 0, val loss: 1.9524016380310059
Epoch 10, training loss: 10.535873413085938 = 1.9392805099487305 + 1.0 * 8.596592903137207
Epoch 10, val loss: 1.9427218437194824
Epoch 20, training loss: 10.521400451660156 = 1.9270848035812378 + 1.0 * 8.594315528869629
Epoch 20, val loss: 1.9304827451705933
Epoch 30, training loss: 10.484940528869629 = 1.9098097085952759 + 1.0 * 8.575130462646484
Epoch 30, val loss: 1.913202166557312
Epoch 40, training loss: 10.349955558776855 = 1.8865376710891724 + 1.0 * 8.463418006896973
Epoch 40, val loss: 1.8907690048217773
Epoch 50, training loss: 9.930459976196289 = 1.8598368167877197 + 1.0 * 8.070623397827148
Epoch 50, val loss: 1.8661999702453613
Epoch 60, training loss: 9.68600082397461 = 1.8348827362060547 + 1.0 * 7.851118087768555
Epoch 60, val loss: 1.8437262773513794
Epoch 70, training loss: 9.283275604248047 = 1.813852071762085 + 1.0 * 7.469423294067383
Epoch 70, val loss: 1.8238615989685059
Epoch 80, training loss: 8.903422355651855 = 1.796373963356018 + 1.0 * 7.107048034667969
Epoch 80, val loss: 1.8067394495010376
Epoch 90, training loss: 8.679619789123535 = 1.7814180850982666 + 1.0 * 6.8982014656066895
Epoch 90, val loss: 1.791866421699524
Epoch 100, training loss: 8.563705444335938 = 1.7648756504058838 + 1.0 * 6.798830032348633
Epoch 100, val loss: 1.7762668132781982
Epoch 110, training loss: 8.480012893676758 = 1.7453908920288086 + 1.0 * 6.734621524810791
Epoch 110, val loss: 1.7583370208740234
Epoch 120, training loss: 8.411843299865723 = 1.7237900495529175 + 1.0 * 6.688053607940674
Epoch 120, val loss: 1.737946629524231
Epoch 130, training loss: 8.354679107666016 = 1.7000596523284912 + 1.0 * 6.6546196937561035
Epoch 130, val loss: 1.7153434753417969
Epoch 140, training loss: 8.299005508422852 = 1.6729146242141724 + 1.0 * 6.6260905265808105
Epoch 140, val loss: 1.6900365352630615
Epoch 150, training loss: 8.240459442138672 = 1.6407098770141602 + 1.0 * 6.5997490882873535
Epoch 150, val loss: 1.6608375310897827
Epoch 160, training loss: 8.178241729736328 = 1.603569507598877 + 1.0 * 6.574672222137451
Epoch 160, val loss: 1.6277697086334229
Epoch 170, training loss: 8.114479064941406 = 1.5616788864135742 + 1.0 * 6.55280065536499
Epoch 170, val loss: 1.590729832649231
Epoch 180, training loss: 8.04885482788086 = 1.5142639875411987 + 1.0 * 6.534590721130371
Epoch 180, val loss: 1.5490093231201172
Epoch 190, training loss: 7.981744766235352 = 1.4612209796905518 + 1.0 * 6.520523548126221
Epoch 190, val loss: 1.5029220581054688
Epoch 200, training loss: 7.911890983581543 = 1.4043208360671997 + 1.0 * 6.507570266723633
Epoch 200, val loss: 1.454084873199463
Epoch 210, training loss: 7.840127944946289 = 1.3449337482452393 + 1.0 * 6.495194435119629
Epoch 210, val loss: 1.4038838148117065
Epoch 220, training loss: 7.768854141235352 = 1.2844862937927246 + 1.0 * 6.484367847442627
Epoch 220, val loss: 1.353446125984192
Epoch 230, training loss: 7.700494766235352 = 1.2251410484313965 + 1.0 * 6.475353717803955
Epoch 230, val loss: 1.3045209646224976
Epoch 240, training loss: 7.633005619049072 = 1.1675413846969604 + 1.0 * 6.465464115142822
Epoch 240, val loss: 1.2577528953552246
Epoch 250, training loss: 7.572605133056641 = 1.1117221117019653 + 1.0 * 6.460883140563965
Epoch 250, val loss: 1.2128045558929443
Epoch 260, training loss: 7.5111236572265625 = 1.0586848258972168 + 1.0 * 6.452438831329346
Epoch 260, val loss: 1.1700632572174072
Epoch 270, training loss: 7.451241493225098 = 1.0077534914016724 + 1.0 * 6.443488121032715
Epoch 270, val loss: 1.1293468475341797
Epoch 280, training loss: 7.39645528793335 = 0.9589757919311523 + 1.0 * 6.437479496002197
Epoch 280, val loss: 1.0902773141860962
Epoch 290, training loss: 7.345870494842529 = 0.9127600789070129 + 1.0 * 6.433110237121582
Epoch 290, val loss: 1.0534002780914307
Epoch 300, training loss: 7.294074058532715 = 0.8690661787986755 + 1.0 * 6.4250078201293945
Epoch 300, val loss: 1.0187963247299194
Epoch 310, training loss: 7.246793746948242 = 0.8272857666015625 + 1.0 * 6.41950798034668
Epoch 310, val loss: 0.9858136177062988
Epoch 320, training loss: 7.202022075653076 = 0.7872210741043091 + 1.0 * 6.414801120758057
Epoch 320, val loss: 0.9547658562660217
Epoch 330, training loss: 7.160569190979004 = 0.7491281032562256 + 1.0 * 6.411441326141357
Epoch 330, val loss: 0.9259812235832214
Epoch 340, training loss: 7.118309497833252 = 0.7127667665481567 + 1.0 * 6.405542850494385
Epoch 340, val loss: 0.8994000554084778
Epoch 350, training loss: 7.082799434661865 = 0.6781620383262634 + 1.0 * 6.404637336730957
Epoch 350, val loss: 0.8751976490020752
Epoch 360, training loss: 7.04310941696167 = 0.6454752683639526 + 1.0 * 6.397634029388428
Epoch 360, val loss: 0.853600800037384
Epoch 370, training loss: 7.006214141845703 = 0.6142511367797852 + 1.0 * 6.391963005065918
Epoch 370, val loss: 0.8343960046768188
Epoch 380, training loss: 6.981992244720459 = 0.5844311714172363 + 1.0 * 6.397561073303223
Epoch 380, val loss: 0.8174227476119995
Epoch 390, training loss: 6.94248104095459 = 0.5565541982650757 + 1.0 * 6.385926723480225
Epoch 390, val loss: 0.8027358055114746
Epoch 400, training loss: 6.911252975463867 = 0.5301680564880371 + 1.0 * 6.38108491897583
Epoch 400, val loss: 0.7902225255966187
Epoch 410, training loss: 6.886165618896484 = 0.5051459074020386 + 1.0 * 6.381019592285156
Epoch 410, val loss: 0.7794798612594604
Epoch 420, training loss: 6.862850666046143 = 0.48154136538505554 + 1.0 * 6.381309509277344
Epoch 420, val loss: 0.7703284621238708
Epoch 430, training loss: 6.831841945648193 = 0.45929476618766785 + 1.0 * 6.372547149658203
Epoch 430, val loss: 0.7627111077308655
Epoch 440, training loss: 6.809704780578613 = 0.43817412853240967 + 1.0 * 6.371530532836914
Epoch 440, val loss: 0.7563490271568298
Epoch 450, training loss: 6.78704309463501 = 0.41813623905181885 + 1.0 * 6.3689069747924805
Epoch 450, val loss: 0.7511303424835205
Epoch 460, training loss: 6.766077041625977 = 0.39922213554382324 + 1.0 * 6.366854667663574
Epoch 460, val loss: 0.746883749961853
Epoch 470, training loss: 6.744565010070801 = 0.3811222314834595 + 1.0 * 6.363442897796631
Epoch 470, val loss: 0.7435510754585266
Epoch 480, training loss: 6.7317657470703125 = 0.36378172039985657 + 1.0 * 6.367983818054199
Epoch 480, val loss: 0.7410247921943665
Epoch 490, training loss: 6.708279609680176 = 0.3473188579082489 + 1.0 * 6.360960960388184
Epoch 490, val loss: 0.739276111125946
Epoch 500, training loss: 6.6891560554504395 = 0.33161887526512146 + 1.0 * 6.357537269592285
Epoch 500, val loss: 0.7383015155792236
Epoch 510, training loss: 6.6721882820129395 = 0.3165576159954071 + 1.0 * 6.355630874633789
Epoch 510, val loss: 0.7380573749542236
Epoch 520, training loss: 6.674206733703613 = 0.30215567350387573 + 1.0 * 6.372051239013672
Epoch 520, val loss: 0.738490641117096
Epoch 530, training loss: 6.642011642456055 = 0.2884204089641571 + 1.0 * 6.353591442108154
Epoch 530, val loss: 0.7394841313362122
Epoch 540, training loss: 6.62571907043457 = 0.27532508969306946 + 1.0 * 6.350393772125244
Epoch 540, val loss: 0.7411489486694336
Epoch 550, training loss: 6.610275745391846 = 0.26275384426116943 + 1.0 * 6.347521781921387
Epoch 550, val loss: 0.7434693574905396
Epoch 560, training loss: 6.596624851226807 = 0.25062617659568787 + 1.0 * 6.345998764038086
Epoch 560, val loss: 0.746367871761322
Epoch 570, training loss: 6.58836555480957 = 0.23899006843566895 + 1.0 * 6.3493757247924805
Epoch 570, val loss: 0.7497830986976624
Epoch 580, training loss: 6.578383445739746 = 0.22790740430355072 + 1.0 * 6.350476264953613
Epoch 580, val loss: 0.7536011934280396
Epoch 590, training loss: 6.559665679931641 = 0.2173745483160019 + 1.0 * 6.342291355133057
Epoch 590, val loss: 0.7577661275863647
Epoch 600, training loss: 6.549177169799805 = 0.20725604891777039 + 1.0 * 6.341921329498291
Epoch 600, val loss: 0.7623923420906067
Epoch 610, training loss: 6.5369133949279785 = 0.19753998517990112 + 1.0 * 6.339373588562012
Epoch 610, val loss: 0.7673882246017456
Epoch 620, training loss: 6.531994342803955 = 0.1882309466600418 + 1.0 * 6.34376335144043
Epoch 620, val loss: 0.7727031707763672
Epoch 630, training loss: 6.517379283905029 = 0.1792985200881958 + 1.0 * 6.338080883026123
Epoch 630, val loss: 0.778308093547821
Epoch 640, training loss: 6.508672714233398 = 0.17078432440757751 + 1.0 * 6.337888240814209
Epoch 640, val loss: 0.7841901779174805
Epoch 650, training loss: 6.499134540557861 = 0.16263416409492493 + 1.0 * 6.33650016784668
Epoch 650, val loss: 0.7903276085853577
Epoch 660, training loss: 6.4882121086120605 = 0.15486963093280792 + 1.0 * 6.333342552185059
Epoch 660, val loss: 0.7965894937515259
Epoch 670, training loss: 6.477880001068115 = 0.1474543958902359 + 1.0 * 6.33042573928833
Epoch 670, val loss: 0.803099513053894
Epoch 680, training loss: 6.469008445739746 = 0.14036789536476135 + 1.0 * 6.328640460968018
Epoch 680, val loss: 0.8098532557487488
Epoch 690, training loss: 6.483465194702148 = 0.1336120069026947 + 1.0 * 6.349853038787842
Epoch 690, val loss: 0.8167397379875183
Epoch 700, training loss: 6.456425666809082 = 0.12719601392745972 + 1.0 * 6.329229831695557
Epoch 700, val loss: 0.8236592411994934
Epoch 710, training loss: 6.446473121643066 = 0.12114306539297104 + 1.0 * 6.3253302574157715
Epoch 710, val loss: 0.8305131793022156
Epoch 720, training loss: 6.4382219314575195 = 0.11536648869514465 + 1.0 * 6.322855472564697
Epoch 720, val loss: 0.8377186059951782
Epoch 730, training loss: 6.430417060852051 = 0.10984378308057785 + 1.0 * 6.320573329925537
Epoch 730, val loss: 0.8450939655303955
Epoch 740, training loss: 6.423834323883057 = 0.10457833856344223 + 1.0 * 6.319255828857422
Epoch 740, val loss: 0.852537214756012
Epoch 750, training loss: 6.436535358428955 = 0.09956339746713638 + 1.0 * 6.336971759796143
Epoch 750, val loss: 0.8601078391075134
Epoch 760, training loss: 6.413050174713135 = 0.0948546752333641 + 1.0 * 6.318195343017578
Epoch 760, val loss: 0.8675749897956848
Epoch 770, training loss: 6.408872604370117 = 0.09039497375488281 + 1.0 * 6.318477630615234
Epoch 770, val loss: 0.875018298625946
Epoch 780, training loss: 6.405415058135986 = 0.08616892248392105 + 1.0 * 6.319246292114258
Epoch 780, val loss: 0.8826088309288025
Epoch 790, training loss: 6.396895408630371 = 0.08216527849435806 + 1.0 * 6.314730167388916
Epoch 790, val loss: 0.8901772499084473
Epoch 800, training loss: 6.392940521240234 = 0.07837283611297607 + 1.0 * 6.314567565917969
Epoch 800, val loss: 0.8977779746055603
Epoch 810, training loss: 6.389117240905762 = 0.07478831708431244 + 1.0 * 6.314329147338867
Epoch 810, val loss: 0.9053690433502197
Epoch 820, training loss: 6.389046669006348 = 0.07142237573862076 + 1.0 * 6.317624092102051
Epoch 820, val loss: 0.9128410816192627
Epoch 830, training loss: 6.3809814453125 = 0.06825874000787735 + 1.0 * 6.312722682952881
Epoch 830, val loss: 0.9202051162719727
Epoch 840, training loss: 6.374385833740234 = 0.06525029242038727 + 1.0 * 6.309135437011719
Epoch 840, val loss: 0.9276717305183411
Epoch 850, training loss: 6.3714704513549805 = 0.06240140274167061 + 1.0 * 6.3090691566467285
Epoch 850, val loss: 0.9351705312728882
Epoch 860, training loss: 6.370891094207764 = 0.05970647186040878 + 1.0 * 6.311184406280518
Epoch 860, val loss: 0.9426214694976807
Epoch 870, training loss: 6.364223003387451 = 0.057165637612342834 + 1.0 * 6.3070573806762695
Epoch 870, val loss: 0.9499944448471069
Epoch 880, training loss: 6.380721092224121 = 0.05476590245962143 + 1.0 * 6.325955390930176
Epoch 880, val loss: 0.9572485089302063
Epoch 890, training loss: 6.357576370239258 = 0.05252847075462341 + 1.0 * 6.305047988891602
Epoch 890, val loss: 0.9642786979675293
Epoch 900, training loss: 6.3538336753845215 = 0.05040125548839569 + 1.0 * 6.303432464599609
Epoch 900, val loss: 0.9712359309196472
Epoch 910, training loss: 6.350363731384277 = 0.04837668687105179 + 1.0 * 6.301987171173096
Epoch 910, val loss: 0.9783657789230347
Epoch 920, training loss: 6.3475422859191895 = 0.04645390808582306 + 1.0 * 6.301088333129883
Epoch 920, val loss: 0.9854549765586853
Epoch 930, training loss: 6.359411239624023 = 0.04463222995400429 + 1.0 * 6.314778804779053
Epoch 930, val loss: 0.9924947023391724
Epoch 940, training loss: 6.3454694747924805 = 0.04290524497628212 + 1.0 * 6.3025641441345215
Epoch 940, val loss: 0.999343752861023
Epoch 950, training loss: 6.340758323669434 = 0.041272711008787155 + 1.0 * 6.299485683441162
Epoch 950, val loss: 1.0060189962387085
Epoch 960, training loss: 6.346250534057617 = 0.03972253575921059 + 1.0 * 6.306528091430664
Epoch 960, val loss: 1.0127739906311035
Epoch 970, training loss: 6.336735248565674 = 0.038245975971221924 + 1.0 * 6.298489093780518
Epoch 970, val loss: 1.019510269165039
Epoch 980, training loss: 6.334555625915527 = 0.03685636818408966 + 1.0 * 6.297699451446533
Epoch 980, val loss: 1.0258080959320068
Epoch 990, training loss: 6.331507682800293 = 0.03552582487463951 + 1.0 * 6.2959818840026855
Epoch 990, val loss: 1.0323188304901123
Epoch 1000, training loss: 6.341532230377197 = 0.034259941428899765 + 1.0 * 6.307272434234619
Epoch 1000, val loss: 1.0388319492340088
Epoch 1010, training loss: 6.330268859863281 = 0.0330621674656868 + 1.0 * 6.297206878662109
Epoch 1010, val loss: 1.0451160669326782
Epoch 1020, training loss: 6.328073024749756 = 0.031922146677970886 + 1.0 * 6.2961506843566895
Epoch 1020, val loss: 1.0512443780899048
Epoch 1030, training loss: 6.326188087463379 = 0.030836811289191246 + 1.0 * 6.295351505279541
Epoch 1030, val loss: 1.057436227798462
Epoch 1040, training loss: 6.323071002960205 = 0.02980661578476429 + 1.0 * 6.293264389038086
Epoch 1040, val loss: 1.0633949041366577
Epoch 1050, training loss: 6.323585510253906 = 0.028820006176829338 + 1.0 * 6.294765472412109
Epoch 1050, val loss: 1.06937575340271
Epoch 1060, training loss: 6.324662685394287 = 0.027880024164915085 + 1.0 * 6.296782493591309
Epoch 1060, val loss: 1.0753811597824097
Epoch 1070, training loss: 6.320254325866699 = 0.026991551741957664 + 1.0 * 6.293262958526611
Epoch 1070, val loss: 1.0810425281524658
Epoch 1080, training loss: 6.319182872772217 = 0.02613622508943081 + 1.0 * 6.293046474456787
Epoch 1080, val loss: 1.0867102146148682
Epoch 1090, training loss: 6.31498384475708 = 0.025320466607809067 + 1.0 * 6.289663314819336
Epoch 1090, val loss: 1.0923984050750732
Epoch 1100, training loss: 6.313474178314209 = 0.024540478363633156 + 1.0 * 6.288933753967285
Epoch 1100, val loss: 1.0979479551315308
Epoch 1110, training loss: 6.319890975952148 = 0.023792337626218796 + 1.0 * 6.296098709106445
Epoch 1110, val loss: 1.1035255193710327
Epoch 1120, training loss: 6.313483715057373 = 0.023082967847585678 + 1.0 * 6.290400981903076
Epoch 1120, val loss: 1.1089731454849243
Epoch 1130, training loss: 6.3099164962768555 = 0.022401120513677597 + 1.0 * 6.287515163421631
Epoch 1130, val loss: 1.11422598361969
Epoch 1140, training loss: 6.31192684173584 = 0.021748540922999382 + 1.0 * 6.290178298950195
Epoch 1140, val loss: 1.119541883468628
Epoch 1150, training loss: 6.307061672210693 = 0.0211251899600029 + 1.0 * 6.28593635559082
Epoch 1150, val loss: 1.1247034072875977
Epoch 1160, training loss: 6.315149307250977 = 0.020527059212327003 + 1.0 * 6.294622421264648
Epoch 1160, val loss: 1.1298562288284302
Epoch 1170, training loss: 6.31052827835083 = 0.019954238086938858 + 1.0 * 6.290574073791504
Epoch 1170, val loss: 1.1349259614944458
Epoch 1180, training loss: 6.306541919708252 = 0.019407201558351517 + 1.0 * 6.287134647369385
Epoch 1180, val loss: 1.139814019203186
Epoch 1190, training loss: 6.3025383949279785 = 0.01887921243906021 + 1.0 * 6.283658981323242
Epoch 1190, val loss: 1.1446053981781006
Epoch 1200, training loss: 6.305373668670654 = 0.018371207639575005 + 1.0 * 6.2870025634765625
Epoch 1200, val loss: 1.1495198011398315
Epoch 1210, training loss: 6.305205345153809 = 0.017885638400912285 + 1.0 * 6.287319660186768
Epoch 1210, val loss: 1.154364824295044
Epoch 1220, training loss: 6.300362586975098 = 0.01742205210030079 + 1.0 * 6.28294038772583
Epoch 1220, val loss: 1.1590056419372559
Epoch 1230, training loss: 6.298486232757568 = 0.016972394660115242 + 1.0 * 6.281513690948486
Epoch 1230, val loss: 1.1635321378707886
Epoch 1240, training loss: 6.297238349914551 = 0.01654084026813507 + 1.0 * 6.280697345733643
Epoch 1240, val loss: 1.1681005954742432
Epoch 1250, training loss: 6.304076194763184 = 0.016124779358506203 + 1.0 * 6.287951469421387
Epoch 1250, val loss: 1.1727484464645386
Epoch 1260, training loss: 6.299868583679199 = 0.01572512648999691 + 1.0 * 6.284143447875977
Epoch 1260, val loss: 1.177259087562561
Epoch 1270, training loss: 6.298074722290039 = 0.015340505167841911 + 1.0 * 6.282734394073486
Epoch 1270, val loss: 1.1816024780273438
Epoch 1280, training loss: 6.294989109039307 = 0.01496834121644497 + 1.0 * 6.280020713806152
Epoch 1280, val loss: 1.1859103441238403
Epoch 1290, training loss: 6.31436824798584 = 0.014608305878937244 + 1.0 * 6.299759864807129
Epoch 1290, val loss: 1.1903349161148071
Epoch 1300, training loss: 6.297207355499268 = 0.01427103765308857 + 1.0 * 6.282936096191406
Epoch 1300, val loss: 1.1945077180862427
Epoch 1310, training loss: 6.291506767272949 = 0.013939522206783295 + 1.0 * 6.277567386627197
Epoch 1310, val loss: 1.1983819007873535
Epoch 1320, training loss: 6.290040969848633 = 0.013620330952107906 + 1.0 * 6.276420593261719
Epoch 1320, val loss: 1.2024619579315186
Epoch 1330, training loss: 6.289534568786621 = 0.013309688307344913 + 1.0 * 6.2762250900268555
Epoch 1330, val loss: 1.2066608667373657
Epoch 1340, training loss: 6.30360746383667 = 0.013009095564484596 + 1.0 * 6.290598392486572
Epoch 1340, val loss: 1.2108529806137085
Epoch 1350, training loss: 6.292338848114014 = 0.012723422609269619 + 1.0 * 6.27961540222168
Epoch 1350, val loss: 1.2147777080535889
Epoch 1360, training loss: 6.289831638336182 = 0.01244503352791071 + 1.0 * 6.277386665344238
Epoch 1360, val loss: 1.2185543775558472
Epoch 1370, training loss: 6.287792682647705 = 0.01217559538781643 + 1.0 * 6.2756171226501465
Epoch 1370, val loss: 1.2224278450012207
Epoch 1380, training loss: 6.285970211029053 = 0.011914962902665138 + 1.0 * 6.274055480957031
Epoch 1380, val loss: 1.2262729406356812
Epoch 1390, training loss: 6.291995525360107 = 0.011661379598081112 + 1.0 * 6.280333995819092
Epoch 1390, val loss: 1.230145812034607
Epoch 1400, training loss: 6.2876739501953125 = 0.01141788437962532 + 1.0 * 6.276256084442139
Epoch 1400, val loss: 1.2339974641799927
Epoch 1410, training loss: 6.285868167877197 = 0.011183303780853748 + 1.0 * 6.274684906005859
Epoch 1410, val loss: 1.2375048398971558
Epoch 1420, training loss: 6.288671493530273 = 0.010954578407108784 + 1.0 * 6.277717113494873
Epoch 1420, val loss: 1.2410515546798706
Epoch 1430, training loss: 6.283664703369141 = 0.01073362771421671 + 1.0 * 6.272931098937988
Epoch 1430, val loss: 1.2447998523712158
Epoch 1440, training loss: 6.286035537719727 = 0.010519541800022125 + 1.0 * 6.275516033172607
Epoch 1440, val loss: 1.248298168182373
Epoch 1450, training loss: 6.283635139465332 = 0.010312837548553944 + 1.0 * 6.273322105407715
Epoch 1450, val loss: 1.251794695854187
Epoch 1460, training loss: 6.282379150390625 = 0.010113553144037724 + 1.0 * 6.272265434265137
Epoch 1460, val loss: 1.2551329135894775
Epoch 1470, training loss: 6.281785488128662 = 0.00991771835833788 + 1.0 * 6.271867752075195
Epoch 1470, val loss: 1.2585903406143188
Epoch 1480, training loss: 6.2824811935424805 = 0.00972756277769804 + 1.0 * 6.272753715515137
Epoch 1480, val loss: 1.262097716331482
Epoch 1490, training loss: 6.287885665893555 = 0.009542902000248432 + 1.0 * 6.2783427238464355
Epoch 1490, val loss: 1.265453577041626
Epoch 1500, training loss: 6.281446933746338 = 0.009365632198750973 + 1.0 * 6.27208137512207
Epoch 1500, val loss: 1.268818736076355
Epoch 1510, training loss: 6.278520584106445 = 0.009194264188408852 + 1.0 * 6.269326210021973
Epoch 1510, val loss: 1.2718831300735474
Epoch 1520, training loss: 6.279148578643799 = 0.009026341140270233 + 1.0 * 6.270122051239014
Epoch 1520, val loss: 1.2750608921051025
Epoch 1530, training loss: 6.281087398529053 = 0.008863063529133797 + 1.0 * 6.272224426269531
Epoch 1530, val loss: 1.278368592262268
Epoch 1540, training loss: 6.280564785003662 = 0.008704246021807194 + 1.0 * 6.271860599517822
Epoch 1540, val loss: 1.2816212177276611
Epoch 1550, training loss: 6.281473636627197 = 0.008549816906452179 + 1.0 * 6.272923946380615
Epoch 1550, val loss: 1.2847238779067993
Epoch 1560, training loss: 6.27748441696167 = 0.008399033918976784 + 1.0 * 6.26908540725708
Epoch 1560, val loss: 1.2877775430679321
Epoch 1570, training loss: 6.284443378448486 = 0.008253013715147972 + 1.0 * 6.276190280914307
Epoch 1570, val loss: 1.2908532619476318
Epoch 1580, training loss: 6.275336265563965 = 0.008112221956253052 + 1.0 * 6.267223834991455
Epoch 1580, val loss: 1.2939032316207886
Epoch 1590, training loss: 6.274630069732666 = 0.007974815554916859 + 1.0 * 6.266655445098877
Epoch 1590, val loss: 1.2967814207077026
Epoch 1600, training loss: 6.277374267578125 = 0.007840082980692387 + 1.0 * 6.269534111022949
Epoch 1600, val loss: 1.2997747659683228
Epoch 1610, training loss: 6.276401042938232 = 0.0077097173780202866 + 1.0 * 6.268691539764404
Epoch 1610, val loss: 1.302829384803772
Epoch 1620, training loss: 6.279927730560303 = 0.007583624217659235 + 1.0 * 6.27234411239624
Epoch 1620, val loss: 1.3056628704071045
Epoch 1630, training loss: 6.272953033447266 = 0.0074600717052817345 + 1.0 * 6.265492916107178
Epoch 1630, val loss: 1.3084906339645386
Epoch 1640, training loss: 6.2737932205200195 = 0.0073395478539168835 + 1.0 * 6.266453742980957
Epoch 1640, val loss: 1.3111799955368042
Epoch 1650, training loss: 6.270684242248535 = 0.007222993765026331 + 1.0 * 6.263461112976074
Epoch 1650, val loss: 1.3139151334762573
Epoch 1660, training loss: 6.272770404815674 = 0.007108243182301521 + 1.0 * 6.26566219329834
Epoch 1660, val loss: 1.316771149635315
Epoch 1670, training loss: 6.275853633880615 = 0.006995976436883211 + 1.0 * 6.268857479095459
Epoch 1670, val loss: 1.319745421409607
Epoch 1680, training loss: 6.273557186126709 = 0.006886309944093227 + 1.0 * 6.2666707038879395
Epoch 1680, val loss: 1.3224574327468872
Epoch 1690, training loss: 6.271119117736816 = 0.006781384348869324 + 1.0 * 6.264337539672852
Epoch 1690, val loss: 1.324946403503418
Epoch 1700, training loss: 6.273932456970215 = 0.006678273435682058 + 1.0 * 6.26725435256958
Epoch 1700, val loss: 1.3275814056396484
Epoch 1710, training loss: 6.271259307861328 = 0.006577751599252224 + 1.0 * 6.264681339263916
Epoch 1710, val loss: 1.3301913738250732
Epoch 1720, training loss: 6.274828910827637 = 0.006479468662291765 + 1.0 * 6.268349647521973
Epoch 1720, val loss: 1.3328059911727905
Epoch 1730, training loss: 6.26727294921875 = 0.006383025553077459 + 1.0 * 6.260890007019043
Epoch 1730, val loss: 1.3354183435440063
Epoch 1740, training loss: 6.267520904541016 = 0.006289096083492041 + 1.0 * 6.261231899261475
Epoch 1740, val loss: 1.337894082069397
Epoch 1750, training loss: 6.283677101135254 = 0.006196299567818642 + 1.0 * 6.277480602264404
Epoch 1750, val loss: 1.3405210971832275
Epoch 1760, training loss: 6.270721912384033 = 0.006108738947659731 + 1.0 * 6.264613151550293
Epoch 1760, val loss: 1.3430988788604736
Epoch 1770, training loss: 6.267310619354248 = 0.006021748762577772 + 1.0 * 6.261288642883301
Epoch 1770, val loss: 1.3452999591827393
Epoch 1780, training loss: 6.26612663269043 = 0.005936344154179096 + 1.0 * 6.260190486907959
Epoch 1780, val loss: 1.3476930856704712
Epoch 1790, training loss: 6.272407054901123 = 0.005852578207850456 + 1.0 * 6.266554355621338
Epoch 1790, val loss: 1.3502352237701416
Epoch 1800, training loss: 6.272237300872803 = 0.0057703470811247826 + 1.0 * 6.266467094421387
Epoch 1800, val loss: 1.3529016971588135
Epoch 1810, training loss: 6.267172336578369 = 0.0056928894482553005 + 1.0 * 6.261479377746582
Epoch 1810, val loss: 1.355031132698059
Epoch 1820, training loss: 6.264842510223389 = 0.005615155678242445 + 1.0 * 6.259227275848389
Epoch 1820, val loss: 1.3570799827575684
Epoch 1830, training loss: 6.266010284423828 = 0.005539311561733484 + 1.0 * 6.260470867156982
Epoch 1830, val loss: 1.3594300746917725
Epoch 1840, training loss: 6.2717390060424805 = 0.005465255118906498 + 1.0 * 6.2662739753723145
Epoch 1840, val loss: 1.3619581460952759
Epoch 1850, training loss: 6.266584873199463 = 0.005392027087509632 + 1.0 * 6.261192798614502
Epoch 1850, val loss: 1.3642998933792114
Epoch 1860, training loss: 6.264920234680176 = 0.005321411415934563 + 1.0 * 6.259598731994629
Epoch 1860, val loss: 1.3663181066513062
Epoch 1870, training loss: 6.262826442718506 = 0.005251895636320114 + 1.0 * 6.257574558258057
Epoch 1870, val loss: 1.3684712648391724
Epoch 1880, training loss: 6.267958641052246 = 0.005184047389775515 + 1.0 * 6.262774467468262
Epoch 1880, val loss: 1.3707484006881714
Epoch 1890, training loss: 6.265331745147705 = 0.005116828717291355 + 1.0 * 6.260214805603027
Epoch 1890, val loss: 1.373213529586792
Epoch 1900, training loss: 6.261519908905029 = 0.005052230786532164 + 1.0 * 6.256467819213867
Epoch 1900, val loss: 1.3752213716506958
Epoch 1910, training loss: 6.261051177978516 = 0.004988530650734901 + 1.0 * 6.2560625076293945
Epoch 1910, val loss: 1.377151608467102
Epoch 1920, training loss: 6.2636871337890625 = 0.004925508983433247 + 1.0 * 6.258761405944824
Epoch 1920, val loss: 1.3793224096298218
Epoch 1930, training loss: 6.265679836273193 = 0.004864044953137636 + 1.0 * 6.260815620422363
Epoch 1930, val loss: 1.381596565246582
Epoch 1940, training loss: 6.262606143951416 = 0.0048051257617771626 + 1.0 * 6.257801055908203
Epoch 1940, val loss: 1.3836432695388794
Epoch 1950, training loss: 6.261839866638184 = 0.004746112506836653 + 1.0 * 6.257093906402588
Epoch 1950, val loss: 1.3856189250946045
Epoch 1960, training loss: 6.264029502868652 = 0.004688310902565718 + 1.0 * 6.259341239929199
Epoch 1960, val loss: 1.3876912593841553
Epoch 1970, training loss: 6.266567230224609 = 0.004631786607205868 + 1.0 * 6.261935234069824
Epoch 1970, val loss: 1.3897875547409058
Epoch 1980, training loss: 6.260794162750244 = 0.0045771426521241665 + 1.0 * 6.256217002868652
Epoch 1980, val loss: 1.3918535709381104
Epoch 1990, training loss: 6.259457588195801 = 0.0045235357247292995 + 1.0 * 6.254933834075928
Epoch 1990, val loss: 1.3936116695404053
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 10.540252685546875 = 1.9434205293655396 + 1.0 * 8.596832275390625
Epoch 0, val loss: 1.9392297267913818
Epoch 10, training loss: 10.529854774475098 = 1.933275580406189 + 1.0 * 8.596579551696777
Epoch 10, val loss: 1.9294315576553345
Epoch 20, training loss: 10.515650749206543 = 1.921056866645813 + 1.0 * 8.59459400177002
Epoch 20, val loss: 1.9170459508895874
Epoch 30, training loss: 10.482525825500488 = 1.9044338464736938 + 1.0 * 8.578091621398926
Epoch 30, val loss: 1.8996970653533936
Epoch 40, training loss: 10.348247528076172 = 1.8823373317718506 + 1.0 * 8.465909957885742
Epoch 40, val loss: 1.8771520853042603
Epoch 50, training loss: 9.789386749267578 = 1.858412504196167 + 1.0 * 7.930974006652832
Epoch 50, val loss: 1.854600429534912
Epoch 60, training loss: 9.336889266967773 = 1.8389157056808472 + 1.0 * 7.497973918914795
Epoch 60, val loss: 1.8361872434616089
Epoch 70, training loss: 8.959075927734375 = 1.8237006664276123 + 1.0 * 7.135375499725342
Epoch 70, val loss: 1.8218681812286377
Epoch 80, training loss: 8.770298957824707 = 1.8097161054611206 + 1.0 * 6.960582733154297
Epoch 80, val loss: 1.8082444667816162
Epoch 90, training loss: 8.640381813049316 = 1.7944282293319702 + 1.0 * 6.845953941345215
Epoch 90, val loss: 1.7940558195114136
Epoch 100, training loss: 8.540910720825195 = 1.778695821762085 + 1.0 * 6.762214660644531
Epoch 100, val loss: 1.780297875404358
Epoch 110, training loss: 8.462435722351074 = 1.7632496356964111 + 1.0 * 6.699186325073242
Epoch 110, val loss: 1.7665842771530151
Epoch 120, training loss: 8.392840385437012 = 1.7471469640731812 + 1.0 * 6.645693778991699
Epoch 120, val loss: 1.7519716024398804
Epoch 130, training loss: 8.334996223449707 = 1.729300618171692 + 1.0 * 6.6056952476501465
Epoch 130, val loss: 1.735620141029358
Epoch 140, training loss: 8.284544944763184 = 1.7083531618118286 + 1.0 * 6.576191425323486
Epoch 140, val loss: 1.7167891263961792
Epoch 150, training loss: 8.235699653625488 = 1.6836432218551636 + 1.0 * 6.552056312561035
Epoch 150, val loss: 1.694870948791504
Epoch 160, training loss: 8.187740325927734 = 1.6544435024261475 + 1.0 * 6.533296585083008
Epoch 160, val loss: 1.6690016984939575
Epoch 170, training loss: 8.137560844421387 = 1.61982262134552 + 1.0 * 6.517738342285156
Epoch 170, val loss: 1.6385458707809448
Epoch 180, training loss: 8.08426570892334 = 1.57962965965271 + 1.0 * 6.504635810852051
Epoch 180, val loss: 1.6033403873443604
Epoch 190, training loss: 8.024262428283691 = 1.5337916612625122 + 1.0 * 6.490470886230469
Epoch 190, val loss: 1.563567876815796
Epoch 200, training loss: 7.961908340454102 = 1.4820659160614014 + 1.0 * 6.479842662811279
Epoch 200, val loss: 1.519199013710022
Epoch 210, training loss: 7.898990631103516 = 1.42691969871521 + 1.0 * 6.472070693969727
Epoch 210, val loss: 1.4724128246307373
Epoch 220, training loss: 7.82977819442749 = 1.3691290616989136 + 1.0 * 6.460649013519287
Epoch 220, val loss: 1.4238231182098389
Epoch 230, training loss: 7.760201930999756 = 1.308111310005188 + 1.0 * 6.452090740203857
Epoch 230, val loss: 1.3730206489562988
Epoch 240, training loss: 7.6887688636779785 = 1.2439206838607788 + 1.0 * 6.44484806060791
Epoch 240, val loss: 1.320038080215454
Epoch 250, training loss: 7.61522102355957 = 1.177201747894287 + 1.0 * 6.438019275665283
Epoch 250, val loss: 1.2657043933868408
Epoch 260, training loss: 7.552809715270996 = 1.109397053718567 + 1.0 * 6.443412780761719
Epoch 260, val loss: 1.211410403251648
Epoch 270, training loss: 7.474074840545654 = 1.0443447828292847 + 1.0 * 6.42972993850708
Epoch 270, val loss: 1.1600149869918823
Epoch 280, training loss: 7.405545711517334 = 0.9827632904052734 + 1.0 * 6.4227824211120605
Epoch 280, val loss: 1.1120116710662842
Epoch 290, training loss: 7.341976165771484 = 0.9246680736541748 + 1.0 * 6.4173078536987305
Epoch 290, val loss: 1.0675686597824097
Epoch 300, training loss: 7.284965515136719 = 0.8710516691207886 + 1.0 * 6.413913726806641
Epoch 300, val loss: 1.0274935960769653
Epoch 310, training loss: 7.22991418838501 = 0.8223623037338257 + 1.0 * 6.4075517654418945
Epoch 310, val loss: 0.9918507933616638
Epoch 320, training loss: 7.181362152099609 = 0.7780058979988098 + 1.0 * 6.403356075286865
Epoch 320, val loss: 0.9603798389434814
Epoch 330, training loss: 7.142872333526611 = 0.7377691268920898 + 1.0 * 6.4051032066345215
Epoch 330, val loss: 0.9327113628387451
Epoch 340, training loss: 7.098950386047363 = 0.7014638185501099 + 1.0 * 6.397486686706543
Epoch 340, val loss: 0.908626139163971
Epoch 350, training loss: 7.061161994934082 = 0.6679956912994385 + 1.0 * 6.393166542053223
Epoch 350, val loss: 0.8872976303100586
Epoch 360, training loss: 7.0269951820373535 = 0.6369480490684509 + 1.0 * 6.390047073364258
Epoch 360, val loss: 0.868094801902771
Epoch 370, training loss: 6.994050979614258 = 0.6077882051467896 + 1.0 * 6.386262893676758
Epoch 370, val loss: 0.8510468602180481
Epoch 380, training loss: 6.962850093841553 = 0.5801740884780884 + 1.0 * 6.382676124572754
Epoch 380, val loss: 0.8356077671051025
Epoch 390, training loss: 6.934062957763672 = 0.5539814233779907 + 1.0 * 6.380081653594971
Epoch 390, val loss: 0.8216778635978699
Epoch 400, training loss: 6.905083656311035 = 0.5293291211128235 + 1.0 * 6.375754356384277
Epoch 400, val loss: 0.8094744682312012
Epoch 410, training loss: 6.880772590637207 = 0.5058732032775879 + 1.0 * 6.374899387359619
Epoch 410, val loss: 0.798681914806366
Epoch 420, training loss: 6.866559982299805 = 0.483693391084671 + 1.0 * 6.382866382598877
Epoch 420, val loss: 0.7891548275947571
Epoch 430, training loss: 6.8326416015625 = 0.46286335587501526 + 1.0 * 6.369778156280518
Epoch 430, val loss: 0.7810778021812439
Epoch 440, training loss: 6.809028625488281 = 0.44294920563697815 + 1.0 * 6.366079330444336
Epoch 440, val loss: 0.7740956544876099
Epoch 450, training loss: 6.788762092590332 = 0.4237923324108124 + 1.0 * 6.364969730377197
Epoch 450, val loss: 0.767914891242981
Epoch 460, training loss: 6.770352363586426 = 0.40534430742263794 + 1.0 * 6.3650078773498535
Epoch 460, val loss: 0.7625612616539001
Epoch 470, training loss: 6.748394966125488 = 0.3876786231994629 + 1.0 * 6.360716342926025
Epoch 470, val loss: 0.7579771280288696
Epoch 480, training loss: 6.727428913116455 = 0.3705150783061981 + 1.0 * 6.356914043426514
Epoch 480, val loss: 0.7539634704589844
Epoch 490, training loss: 6.716262340545654 = 0.35375726222991943 + 1.0 * 6.362504959106445
Epoch 490, val loss: 0.7503631711006165
Epoch 500, training loss: 6.6934309005737305 = 0.3375370502471924 + 1.0 * 6.355894088745117
Epoch 500, val loss: 0.7472301721572876
Epoch 510, training loss: 6.675119876861572 = 0.32186999917030334 + 1.0 * 6.353250026702881
Epoch 510, val loss: 0.744759738445282
Epoch 520, training loss: 6.6560773849487305 = 0.30652713775634766 + 1.0 * 6.349550247192383
Epoch 520, val loss: 0.7425835132598877
Epoch 530, training loss: 6.64109992980957 = 0.29154691100120544 + 1.0 * 6.349553108215332
Epoch 530, val loss: 0.7407942414283752
Epoch 540, training loss: 6.6259355545043945 = 0.2770790755748749 + 1.0 * 6.348856449127197
Epoch 540, val loss: 0.7394635677337646
Epoch 550, training loss: 6.608217239379883 = 0.26316091418266296 + 1.0 * 6.345056533813477
Epoch 550, val loss: 0.7386480569839478
Epoch 560, training loss: 6.594756126403809 = 0.2497553825378418 + 1.0 * 6.345000743865967
Epoch 560, val loss: 0.73822420835495
Epoch 570, training loss: 6.579165935516357 = 0.23691174387931824 + 1.0 * 6.342254161834717
Epoch 570, val loss: 0.7382672429084778
Epoch 580, training loss: 6.57509708404541 = 0.22458648681640625 + 1.0 * 6.350510597229004
Epoch 580, val loss: 0.7387704849243164
Epoch 590, training loss: 6.550201416015625 = 0.21297013759613037 + 1.0 * 6.337231159210205
Epoch 590, val loss: 0.7397695779800415
Epoch 600, training loss: 6.537896156311035 = 0.20187075436115265 + 1.0 * 6.336025238037109
Epoch 600, val loss: 0.7412472367286682
Epoch 610, training loss: 6.541350841522217 = 0.1913059502840042 + 1.0 * 6.3500447273254395
Epoch 610, val loss: 0.7431091666221619
Epoch 620, training loss: 6.516435146331787 = 0.18145152926445007 + 1.0 * 6.334983825683594
Epoch 620, val loss: 0.7453463077545166
Epoch 630, training loss: 6.504039287567139 = 0.17214591801166534 + 1.0 * 6.331893444061279
Epoch 630, val loss: 0.7481005787849426
Epoch 640, training loss: 6.494096279144287 = 0.16333599388599396 + 1.0 * 6.330760478973389
Epoch 640, val loss: 0.7511979341506958
Epoch 650, training loss: 6.49024772644043 = 0.15503738820552826 + 1.0 * 6.33521032333374
Epoch 650, val loss: 0.7546135187149048
Epoch 660, training loss: 6.478795528411865 = 0.14732161164283752 + 1.0 * 6.3314738273620605
Epoch 660, val loss: 0.7584320902824402
Epoch 670, training loss: 6.46815824508667 = 0.1400291472673416 + 1.0 * 6.328129291534424
Epoch 670, val loss: 0.7625848650932312
Epoch 680, training loss: 6.468380451202393 = 0.13316142559051514 + 1.0 * 6.335218906402588
Epoch 680, val loss: 0.766923725605011
Epoch 690, training loss: 6.452655792236328 = 0.12668465077877045 + 1.0 * 6.3259711265563965
Epoch 690, val loss: 0.7715445756912231
Epoch 700, training loss: 6.445631980895996 = 0.12060118466615677 + 1.0 * 6.32503080368042
Epoch 700, val loss: 0.7764567732810974
Epoch 710, training loss: 6.444886684417725 = 0.11486177146434784 + 1.0 * 6.330024719238281
Epoch 710, val loss: 0.7814335823059082
Epoch 720, training loss: 6.432013511657715 = 0.10951683670282364 + 1.0 * 6.3224968910217285
Epoch 720, val loss: 0.7867636680603027
Epoch 730, training loss: 6.4257683753967285 = 0.104442298412323 + 1.0 * 6.32132625579834
Epoch 730, val loss: 0.7921915650367737
Epoch 740, training loss: 6.417630672454834 = 0.09968637675046921 + 1.0 * 6.317944526672363
Epoch 740, val loss: 0.79766845703125
Epoch 750, training loss: 6.411859035491943 = 0.09520383179187775 + 1.0 * 6.316655158996582
Epoch 750, val loss: 0.8034015893936157
Epoch 760, training loss: 6.413978576660156 = 0.09096983820199966 + 1.0 * 6.3230085372924805
Epoch 760, val loss: 0.8092331290245056
Epoch 770, training loss: 6.40693473815918 = 0.08699600398540497 + 1.0 * 6.319938659667969
Epoch 770, val loss: 0.8149842023849487
Epoch 780, training loss: 6.396772861480713 = 0.08325551450252533 + 1.0 * 6.3135175704956055
Epoch 780, val loss: 0.8210064172744751
Epoch 790, training loss: 6.3955254554748535 = 0.07971810549497604 + 1.0 * 6.315807342529297
Epoch 790, val loss: 0.8270320296287537
Epoch 800, training loss: 6.387226581573486 = 0.07637782394886017 + 1.0 * 6.310848712921143
Epoch 800, val loss: 0.8329658508300781
Epoch 810, training loss: 6.384472370147705 = 0.07322471588850021 + 1.0 * 6.311247825622559
Epoch 810, val loss: 0.8391129374504089
Epoch 820, training loss: 6.381028652191162 = 0.07023977488279343 + 1.0 * 6.310789108276367
Epoch 820, val loss: 0.8451785445213318
Epoch 830, training loss: 6.376552581787109 = 0.06741146743297577 + 1.0 * 6.309141159057617
Epoch 830, val loss: 0.8513585329055786
Epoch 840, training loss: 6.380985736846924 = 0.0647413581609726 + 1.0 * 6.316244602203369
Epoch 840, val loss: 0.8574457764625549
Epoch 850, training loss: 6.367959499359131 = 0.062200386077165604 + 1.0 * 6.305758953094482
Epoch 850, val loss: 0.8635938167572021
Epoch 860, training loss: 6.364066123962402 = 0.05979882925748825 + 1.0 * 6.304267406463623
Epoch 860, val loss: 0.8698234558105469
Epoch 870, training loss: 6.364721298217773 = 0.0575096569955349 + 1.0 * 6.307211875915527
Epoch 870, val loss: 0.8759496808052063
Epoch 880, training loss: 6.368585109710693 = 0.055355917662382126 + 1.0 * 6.313229084014893
Epoch 880, val loss: 0.8818957805633545
Epoch 890, training loss: 6.355712413787842 = 0.05331428721547127 + 1.0 * 6.302398204803467
Epoch 890, val loss: 0.8880430459976196
Epoch 900, training loss: 6.351126670837402 = 0.05137547105550766 + 1.0 * 6.299751281738281
Epoch 900, val loss: 0.8941268920898438
Epoch 910, training loss: 6.349221706390381 = 0.04952382668852806 + 1.0 * 6.2996978759765625
Epoch 910, val loss: 0.9001326560974121
Epoch 920, training loss: 6.363142013549805 = 0.04776259511709213 + 1.0 * 6.315379619598389
Epoch 920, val loss: 0.9061217904090881
Epoch 930, training loss: 6.348876476287842 = 0.04607825353741646 + 1.0 * 6.302798271179199
Epoch 930, val loss: 0.9119929671287537
Epoch 940, training loss: 6.348422050476074 = 0.04448875039815903 + 1.0 * 6.303933143615723
Epoch 940, val loss: 0.9180012345314026
Epoch 950, training loss: 6.339850425720215 = 0.04296661913394928 + 1.0 * 6.296883583068848
Epoch 950, val loss: 0.9238143563270569
Epoch 960, training loss: 6.336777687072754 = 0.04151986166834831 + 1.0 * 6.295258045196533
Epoch 960, val loss: 0.9297348260879517
Epoch 970, training loss: 6.337296962738037 = 0.0401301309466362 + 1.0 * 6.29716682434082
Epoch 970, val loss: 0.9355869293212891
Epoch 980, training loss: 6.336922645568848 = 0.03881164267659187 + 1.0 * 6.2981109619140625
Epoch 980, val loss: 0.9412689208984375
Epoch 990, training loss: 6.331909656524658 = 0.03754469007253647 + 1.0 * 6.294364929199219
Epoch 990, val loss: 0.9470574259757996
Epoch 1000, training loss: 6.334362983703613 = 0.036344096064567566 + 1.0 * 6.298018932342529
Epoch 1000, val loss: 0.9527880549430847
Epoch 1010, training loss: 6.332978248596191 = 0.03519006446003914 + 1.0 * 6.297788143157959
Epoch 1010, val loss: 0.9583173990249634
Epoch 1020, training loss: 6.324824333190918 = 0.03409019857645035 + 1.0 * 6.29073429107666
Epoch 1020, val loss: 0.9639407396316528
Epoch 1030, training loss: 6.325775146484375 = 0.03303569555282593 + 1.0 * 6.292739391326904
Epoch 1030, val loss: 0.9694697856903076
Epoch 1040, training loss: 6.324448108673096 = 0.03202766180038452 + 1.0 * 6.292420387268066
Epoch 1040, val loss: 0.9748656153678894
Epoch 1050, training loss: 6.323470592498779 = 0.031069006770849228 + 1.0 * 6.2924017906188965
Epoch 1050, val loss: 0.9803652167320251
Epoch 1060, training loss: 6.319363594055176 = 0.030144069343805313 + 1.0 * 6.289219379425049
Epoch 1060, val loss: 0.9856684803962708
Epoch 1070, training loss: 6.316931247711182 = 0.029259437695145607 + 1.0 * 6.28767204284668
Epoch 1070, val loss: 0.9910016059875488
Epoch 1080, training loss: 6.315047740936279 = 0.028412090614438057 + 1.0 * 6.286635875701904
Epoch 1080, val loss: 0.9963242411613464
Epoch 1090, training loss: 6.319448947906494 = 0.02759590372443199 + 1.0 * 6.291852951049805
Epoch 1090, val loss: 1.0015090703964233
Epoch 1100, training loss: 6.317627429962158 = 0.02681392803788185 + 1.0 * 6.290813446044922
Epoch 1100, val loss: 1.0066068172454834
Epoch 1110, training loss: 6.317739486694336 = 0.026066474616527557 + 1.0 * 6.291673183441162
Epoch 1110, val loss: 1.0116435289382935
Epoch 1120, training loss: 6.3098978996276855 = 0.025352345779538155 + 1.0 * 6.284545421600342
Epoch 1120, val loss: 1.016793966293335
Epoch 1130, training loss: 6.309384822845459 = 0.024661771953105927 + 1.0 * 6.284723281860352
Epoch 1130, val loss: 1.021803855895996
Epoch 1140, training loss: 6.310252666473389 = 0.02399802766740322 + 1.0 * 6.286254405975342
Epoch 1140, val loss: 1.0265709161758423
Epoch 1150, training loss: 6.305939674377441 = 0.023361049592494965 + 1.0 * 6.282578468322754
Epoch 1150, val loss: 1.031496286392212
Epoch 1160, training loss: 6.310138702392578 = 0.022745274007320404 + 1.0 * 6.287393569946289
Epoch 1160, val loss: 1.0363471508026123
Epoch 1170, training loss: 6.302395343780518 = 0.02215842343866825 + 1.0 * 6.280236721038818
Epoch 1170, val loss: 1.0409621000289917
Epoch 1180, training loss: 6.3009934425354 = 0.021591098979115486 + 1.0 * 6.279402256011963
Epoch 1180, val loss: 1.0457894802093506
Epoch 1190, training loss: 6.300996780395508 = 0.021045012399554253 + 1.0 * 6.279951572418213
Epoch 1190, val loss: 1.0505210161209106
Epoch 1200, training loss: 6.311309814453125 = 0.020517878234386444 + 1.0 * 6.290791988372803
Epoch 1200, val loss: 1.054939866065979
Epoch 1210, training loss: 6.300812721252441 = 0.02001277357339859 + 1.0 * 6.280799865722656
Epoch 1210, val loss: 1.0594189167022705
Epoch 1220, training loss: 6.29749870300293 = 0.01952592469751835 + 1.0 * 6.27797269821167
Epoch 1220, val loss: 1.064124584197998
Epoch 1230, training loss: 6.297256946563721 = 0.019056206569075584 + 1.0 * 6.278200626373291
Epoch 1230, val loss: 1.0685981512069702
Epoch 1240, training loss: 6.3043212890625 = 0.01860230602324009 + 1.0 * 6.28571891784668
Epoch 1240, val loss: 1.0728788375854492
Epoch 1250, training loss: 6.2970757484436035 = 0.018162794411182404 + 1.0 * 6.2789130210876465
Epoch 1250, val loss: 1.0771865844726562
Epoch 1260, training loss: 6.299539089202881 = 0.017739921808242798 + 1.0 * 6.28179931640625
Epoch 1260, val loss: 1.0816348791122437
Epoch 1270, training loss: 6.293687343597412 = 0.01733148656785488 + 1.0 * 6.276355743408203
Epoch 1270, val loss: 1.0856729745864868
Epoch 1280, training loss: 6.2926249504089355 = 0.016938382759690285 + 1.0 * 6.275686740875244
Epoch 1280, val loss: 1.090005874633789
Epoch 1290, training loss: 6.296636581420898 = 0.016558421775698662 + 1.0 * 6.280077934265137
Epoch 1290, val loss: 1.0942106246948242
Epoch 1300, training loss: 6.293997287750244 = 0.016190305352211 + 1.0 * 6.277806758880615
Epoch 1300, val loss: 1.0981464385986328
Epoch 1310, training loss: 6.291451930999756 = 0.01583472453057766 + 1.0 * 6.2756171226501465
Epoch 1310, val loss: 1.1022363901138306
Epoch 1320, training loss: 6.288978099822998 = 0.015490255318582058 + 1.0 * 6.2734880447387695
Epoch 1320, val loss: 1.1063624620437622
Epoch 1330, training loss: 6.287044525146484 = 0.015155918896198273 + 1.0 * 6.271888732910156
Epoch 1330, val loss: 1.1103214025497437
Epoch 1340, training loss: 6.304197788238525 = 0.014830960892140865 + 1.0 * 6.289366722106934
Epoch 1340, val loss: 1.1140395402908325
Epoch 1350, training loss: 6.2921528816223145 = 0.014523378573358059 + 1.0 * 6.277629375457764
Epoch 1350, val loss: 1.1179674863815308
Epoch 1360, training loss: 6.28774881362915 = 0.014219772070646286 + 1.0 * 6.273529052734375
Epoch 1360, val loss: 1.121927261352539
Epoch 1370, training loss: 6.287008285522461 = 0.013928026892244816 + 1.0 * 6.273080348968506
Epoch 1370, val loss: 1.1257867813110352
Epoch 1380, training loss: 6.286993980407715 = 0.013643591664731503 + 1.0 * 6.273350238800049
Epoch 1380, val loss: 1.1294790506362915
Epoch 1390, training loss: 6.286761283874512 = 0.013369803316891193 + 1.0 * 6.273391246795654
Epoch 1390, val loss: 1.1332168579101562
Epoch 1400, training loss: 6.287802219390869 = 0.013104166835546494 + 1.0 * 6.274698257446289
Epoch 1400, val loss: 1.1369192600250244
Epoch 1410, training loss: 6.283971309661865 = 0.012845571152865887 + 1.0 * 6.271125793457031
Epoch 1410, val loss: 1.1405428647994995
Epoch 1420, training loss: 6.28389310836792 = 0.012595558539032936 + 1.0 * 6.271297454833984
Epoch 1420, val loss: 1.1442255973815918
Epoch 1430, training loss: 6.281546592712402 = 0.012352724559605122 + 1.0 * 6.269193649291992
Epoch 1430, val loss: 1.1478341817855835
Epoch 1440, training loss: 6.286769866943359 = 0.012117777951061726 + 1.0 * 6.274652004241943
Epoch 1440, val loss: 1.1513657569885254
Epoch 1450, training loss: 6.281392574310303 = 0.01188875362277031 + 1.0 * 6.269503593444824
Epoch 1450, val loss: 1.1547656059265137
Epoch 1460, training loss: 6.27937126159668 = 0.011666365899145603 + 1.0 * 6.267704963684082
Epoch 1460, val loss: 1.1583415269851685
Epoch 1470, training loss: 6.2848286628723145 = 0.011450100690126419 + 1.0 * 6.273378372192383
Epoch 1470, val loss: 1.1618127822875977
Epoch 1480, training loss: 6.279474258422852 = 0.01124001108109951 + 1.0 * 6.2682342529296875
Epoch 1480, val loss: 1.1648728847503662
Epoch 1490, training loss: 6.27772331237793 = 0.011036776937544346 + 1.0 * 6.26668643951416
Epoch 1490, val loss: 1.1683852672576904
Epoch 1500, training loss: 6.277126789093018 = 0.010838682763278484 + 1.0 * 6.2662882804870605
Epoch 1500, val loss: 1.1717966794967651
Epoch 1510, training loss: 6.285126686096191 = 0.010645069181919098 + 1.0 * 6.274481773376465
Epoch 1510, val loss: 1.1748799085617065
Epoch 1520, training loss: 6.278223991394043 = 0.010459799319505692 + 1.0 * 6.267764091491699
Epoch 1520, val loss: 1.178148865699768
Epoch 1530, training loss: 6.28029203414917 = 0.010276938788592815 + 1.0 * 6.270015239715576
Epoch 1530, val loss: 1.1814420223236084
Epoch 1540, training loss: 6.275679588317871 = 0.010100779123604298 + 1.0 * 6.265578746795654
Epoch 1540, val loss: 1.1845554113388062
Epoch 1550, training loss: 6.273922920227051 = 0.009928426705300808 + 1.0 * 6.2639946937561035
Epoch 1550, val loss: 1.1877198219299316
Epoch 1560, training loss: 6.272989273071289 = 0.009760431945323944 + 1.0 * 6.263228893280029
Epoch 1560, val loss: 1.190954566001892
Epoch 1570, training loss: 6.286028861999512 = 0.009598092176020145 + 1.0 * 6.276430606842041
Epoch 1570, val loss: 1.1939966678619385
Epoch 1580, training loss: 6.275787353515625 = 0.009437153115868568 + 1.0 * 6.266350269317627
Epoch 1580, val loss: 1.196752905845642
Epoch 1590, training loss: 6.27234411239624 = 0.009282986633479595 + 1.0 * 6.263061046600342
Epoch 1590, val loss: 1.2000569105148315
Epoch 1600, training loss: 6.270962715148926 = 0.009131659753620625 + 1.0 * 6.261831283569336
Epoch 1600, val loss: 1.203086018562317
Epoch 1610, training loss: 6.280101299285889 = 0.008984207175672054 + 1.0 * 6.271117210388184
Epoch 1610, val loss: 1.205949306488037
Epoch 1620, training loss: 6.276278018951416 = 0.008840098045766354 + 1.0 * 6.267437934875488
Epoch 1620, val loss: 1.208685040473938
Epoch 1630, training loss: 6.274415969848633 = 0.008700724691152573 + 1.0 * 6.2657151222229
Epoch 1630, val loss: 1.2116179466247559
Epoch 1640, training loss: 6.268630027770996 = 0.008565853349864483 + 1.0 * 6.260064125061035
Epoch 1640, val loss: 1.2145881652832031
Epoch 1650, training loss: 6.270383358001709 = 0.008433600887656212 + 1.0 * 6.26194953918457
Epoch 1650, val loss: 1.2175319194793701
Epoch 1660, training loss: 6.274563789367676 = 0.008304249495267868 + 1.0 * 6.266259670257568
Epoch 1660, val loss: 1.220213770866394
Epoch 1670, training loss: 6.269716262817383 = 0.008176200091838837 + 1.0 * 6.261539936065674
Epoch 1670, val loss: 1.2229079008102417
Epoch 1680, training loss: 6.273229122161865 = 0.0080529460683465 + 1.0 * 6.265176296234131
Epoch 1680, val loss: 1.2256752252578735
Epoch 1690, training loss: 6.267989635467529 = 0.007932843640446663 + 1.0 * 6.260056972503662
Epoch 1690, val loss: 1.2283538579940796
Epoch 1700, training loss: 6.267117500305176 = 0.007814997807145119 + 1.0 * 6.259302616119385
Epoch 1700, val loss: 1.2311313152313232
Epoch 1710, training loss: 6.267803192138672 = 0.007700065150856972 + 1.0 * 6.260103225708008
Epoch 1710, val loss: 1.2338663339614868
Epoch 1720, training loss: 6.270111560821533 = 0.007587284781038761 + 1.0 * 6.262524127960205
Epoch 1720, val loss: 1.2364429235458374
Epoch 1730, training loss: 6.270864486694336 = 0.007477931212633848 + 1.0 * 6.2633867263793945
Epoch 1730, val loss: 1.2389940023422241
Epoch 1740, training loss: 6.267198085784912 = 0.007371827028691769 + 1.0 * 6.259826183319092
Epoch 1740, val loss: 1.2415978908538818
Epoch 1750, training loss: 6.264690399169922 = 0.007267369423061609 + 1.0 * 6.257422924041748
Epoch 1750, val loss: 1.244228482246399
Epoch 1760, training loss: 6.26322603225708 = 0.007165591232478619 + 1.0 * 6.256060600280762
Epoch 1760, val loss: 1.2469072341918945
Epoch 1770, training loss: 6.268972873687744 = 0.007065931800752878 + 1.0 * 6.26190710067749
Epoch 1770, val loss: 1.2494547367095947
Epoch 1780, training loss: 6.264179706573486 = 0.0069681815803050995 + 1.0 * 6.257211685180664
Epoch 1780, val loss: 1.2517536878585815
Epoch 1790, training loss: 6.266422271728516 = 0.006872595753520727 + 1.0 * 6.259549617767334
Epoch 1790, val loss: 1.254300594329834
Epoch 1800, training loss: 6.263931751251221 = 0.006779402028769255 + 1.0 * 6.257152557373047
Epoch 1800, val loss: 1.2567856311798096
Epoch 1810, training loss: 6.264893054962158 = 0.006688158959150314 + 1.0 * 6.258204936981201
Epoch 1810, val loss: 1.2592391967773438
Epoch 1820, training loss: 6.263049125671387 = 0.006598716136068106 + 1.0 * 6.256450176239014
Epoch 1820, val loss: 1.2616617679595947
Epoch 1830, training loss: 6.26852560043335 = 0.006511105224490166 + 1.0 * 6.262014389038086
Epoch 1830, val loss: 1.264011025428772
Epoch 1840, training loss: 6.263128757476807 = 0.006425848230719566 + 1.0 * 6.256702899932861
Epoch 1840, val loss: 1.2663623094558716
Epoch 1850, training loss: 6.267298698425293 = 0.006341828498989344 + 1.0 * 6.260956764221191
Epoch 1850, val loss: 1.2686806917190552
Epoch 1860, training loss: 6.260149955749512 = 0.006261339411139488 + 1.0 * 6.2538886070251465
Epoch 1860, val loss: 1.2711037397384644
Epoch 1870, training loss: 6.260489463806152 = 0.006181513890624046 + 1.0 * 6.254307746887207
Epoch 1870, val loss: 1.273605227470398
Epoch 1880, training loss: 6.264888763427734 = 0.0061026266776025295 + 1.0 * 6.258786201477051
Epoch 1880, val loss: 1.275909662246704
Epoch 1890, training loss: 6.265909671783447 = 0.006025493610650301 + 1.0 * 6.259884357452393
Epoch 1890, val loss: 1.2778328657150269
Epoch 1900, training loss: 6.261175632476807 = 0.0059510995633900166 + 1.0 * 6.255224704742432
Epoch 1900, val loss: 1.2802138328552246
Epoch 1910, training loss: 6.258995056152344 = 0.005877779796719551 + 1.0 * 6.253117084503174
Epoch 1910, val loss: 1.282631516456604
Epoch 1920, training loss: 6.2608489990234375 = 0.005805820692330599 + 1.0 * 6.255043029785156
Epoch 1920, val loss: 1.2849478721618652
Epoch 1930, training loss: 6.262484073638916 = 0.0057354094460606575 + 1.0 * 6.256748676300049
Epoch 1930, val loss: 1.2868783473968506
Epoch 1940, training loss: 6.2609148025512695 = 0.005665822885930538 + 1.0 * 6.2552490234375
Epoch 1940, val loss: 1.2890734672546387
Epoch 1950, training loss: 6.266536235809326 = 0.005598270799964666 + 1.0 * 6.2609381675720215
Epoch 1950, val loss: 1.2913018465042114
Epoch 1960, training loss: 6.25901460647583 = 0.005531818140298128 + 1.0 * 6.253482818603516
Epoch 1960, val loss: 1.2933458089828491
Epoch 1970, training loss: 6.259317874908447 = 0.005466269329190254 + 1.0 * 6.253851413726807
Epoch 1970, val loss: 1.295582890510559
Epoch 1980, training loss: 6.261185646057129 = 0.005402821581810713 + 1.0 * 6.255782604217529
Epoch 1980, val loss: 1.2976411581039429
Epoch 1990, training loss: 6.257440567016602 = 0.005340409930795431 + 1.0 * 6.252099990844727
Epoch 1990, val loss: 1.2998125553131104
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8376383763837639
The final CL Acc:0.81111, 0.00800, The final GNN Acc:0.83834, 0.00179
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11686])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10598])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.558247566223145 = 1.9614131450653076 + 1.0 * 8.596834182739258
Epoch 0, val loss: 1.9575190544128418
Epoch 10, training loss: 10.546767234802246 = 1.950217604637146 + 1.0 * 8.596549987792969
Epoch 10, val loss: 1.9468706846237183
Epoch 20, training loss: 10.530644416809082 = 1.9362242221832275 + 1.0 * 8.594420433044434
Epoch 20, val loss: 1.9331544637680054
Epoch 30, training loss: 10.493766784667969 = 1.9165217876434326 + 1.0 * 8.577244758605957
Epoch 30, val loss: 1.9136269092559814
Epoch 40, training loss: 10.366655349731445 = 1.8904683589935303 + 1.0 * 8.476186752319336
Epoch 40, val loss: 1.8888176679611206
Epoch 50, training loss: 9.937983512878418 = 1.8637224435806274 + 1.0 * 8.074260711669922
Epoch 50, val loss: 1.864728569984436
Epoch 60, training loss: 9.523921012878418 = 1.841758131980896 + 1.0 * 7.682163238525391
Epoch 60, val loss: 1.8452818393707275
Epoch 70, training loss: 9.079045295715332 = 1.8276084661483765 + 1.0 * 7.251436710357666
Epoch 70, val loss: 1.8323971033096313
Epoch 80, training loss: 8.836878776550293 = 1.8145006895065308 + 1.0 * 7.022378444671631
Epoch 80, val loss: 1.8200675249099731
Epoch 90, training loss: 8.704909324645996 = 1.8007404804229736 + 1.0 * 6.904169082641602
Epoch 90, val loss: 1.807806372642517
Epoch 100, training loss: 8.625967979431152 = 1.7859761714935303 + 1.0 * 6.839992046356201
Epoch 100, val loss: 1.7959095239639282
Epoch 110, training loss: 8.561674118041992 = 1.7718274593353271 + 1.0 * 6.789846420288086
Epoch 110, val loss: 1.7845587730407715
Epoch 120, training loss: 8.495375633239746 = 1.7580711841583252 + 1.0 * 6.7373046875
Epoch 120, val loss: 1.7728079557418823
Epoch 130, training loss: 8.434717178344727 = 1.7438132762908936 + 1.0 * 6.690904140472412
Epoch 130, val loss: 1.7603919506072998
Epoch 140, training loss: 8.381179809570312 = 1.727591633796692 + 1.0 * 6.653587818145752
Epoch 140, val loss: 1.74684476852417
Epoch 150, training loss: 8.333170890808105 = 1.7086008787155151 + 1.0 * 6.624569892883301
Epoch 150, val loss: 1.7314543724060059
Epoch 160, training loss: 8.2864351272583 = 1.686708688735962 + 1.0 * 6.599726676940918
Epoch 160, val loss: 1.713945746421814
Epoch 170, training loss: 8.240455627441406 = 1.661598563194275 + 1.0 * 6.578857421875
Epoch 170, val loss: 1.6936715841293335
Epoch 180, training loss: 8.192407608032227 = 1.6324917078018188 + 1.0 * 6.559915542602539
Epoch 180, val loss: 1.6699211597442627
Epoch 190, training loss: 8.140883445739746 = 1.599045991897583 + 1.0 * 6.541837692260742
Epoch 190, val loss: 1.642468810081482
Epoch 200, training loss: 8.089218139648438 = 1.5619871616363525 + 1.0 * 6.527231216430664
Epoch 200, val loss: 1.611979603767395
Epoch 210, training loss: 8.032525062561035 = 1.5223580598831177 + 1.0 * 6.510166645050049
Epoch 210, val loss: 1.5793721675872803
Epoch 220, training loss: 7.978847026824951 = 1.4805086851119995 + 1.0 * 6.498338222503662
Epoch 220, val loss: 1.5449260473251343
Epoch 230, training loss: 7.9261932373046875 = 1.4372758865356445 + 1.0 * 6.488917350769043
Epoch 230, val loss: 1.5096360445022583
Epoch 240, training loss: 7.870826244354248 = 1.3936768770217896 + 1.0 * 6.477149486541748
Epoch 240, val loss: 1.4743508100509644
Epoch 250, training loss: 7.817631244659424 = 1.349431037902832 + 1.0 * 6.468200206756592
Epoch 250, val loss: 1.4387803077697754
Epoch 260, training loss: 7.764566421508789 = 1.3043944835662842 + 1.0 * 6.460172176361084
Epoch 260, val loss: 1.4031678438186646
Epoch 270, training loss: 7.71099853515625 = 1.2584912776947021 + 1.0 * 6.452507495880127
Epoch 270, val loss: 1.3670904636383057
Epoch 280, training loss: 7.658632755279541 = 1.2107702493667603 + 1.0 * 6.44786262512207
Epoch 280, val loss: 1.330202579498291
Epoch 290, training loss: 7.601924419403076 = 1.1615338325500488 + 1.0 * 6.440390586853027
Epoch 290, val loss: 1.2925564050674438
Epoch 300, training loss: 7.543874263763428 = 1.1102935075759888 + 1.0 * 6.4335808753967285
Epoch 300, val loss: 1.2539695501327515
Epoch 310, training loss: 7.495053768157959 = 1.057138442993164 + 1.0 * 6.437915325164795
Epoch 310, val loss: 1.2143135070800781
Epoch 320, training loss: 7.429471492767334 = 1.003809928894043 + 1.0 * 6.425661563873291
Epoch 320, val loss: 1.174911618232727
Epoch 330, training loss: 7.369659900665283 = 0.9503852725028992 + 1.0 * 6.419274806976318
Epoch 330, val loss: 1.1361637115478516
Epoch 340, training loss: 7.313192367553711 = 0.8979628086090088 + 1.0 * 6.415229320526123
Epoch 340, val loss: 1.0986164808273315
Epoch 350, training loss: 7.259069442749023 = 0.8480517864227295 + 1.0 * 6.411017417907715
Epoch 350, val loss: 1.0637527704238892
Epoch 360, training loss: 7.209236145019531 = 0.8012183308601379 + 1.0 * 6.408017635345459
Epoch 360, val loss: 1.031948208808899
Epoch 370, training loss: 7.161740303039551 = 0.7583819627761841 + 1.0 * 6.403358459472656
Epoch 370, val loss: 1.0039173364639282
Epoch 380, training loss: 7.1179070472717285 = 0.719523012638092 + 1.0 * 6.398384094238281
Epoch 380, val loss: 0.9799841046333313
Epoch 390, training loss: 7.078383445739746 = 0.683740496635437 + 1.0 * 6.3946428298950195
Epoch 390, val loss: 0.9592982530593872
Epoch 400, training loss: 7.04337215423584 = 0.6508432626724243 + 1.0 * 6.392529010772705
Epoch 400, val loss: 0.9416759610176086
Epoch 410, training loss: 7.010224342346191 = 0.6207501888275146 + 1.0 * 6.389474391937256
Epoch 410, val loss: 0.9272403717041016
Epoch 420, training loss: 6.977180480957031 = 0.5927388668060303 + 1.0 * 6.38444185256958
Epoch 420, val loss: 0.9151921272277832
Epoch 430, training loss: 6.949841022491455 = 0.5663065314292908 + 1.0 * 6.3835344314575195
Epoch 430, val loss: 0.9050632119178772
Epoch 440, training loss: 6.922667980194092 = 0.5413139462471008 + 1.0 * 6.381353855133057
Epoch 440, val loss: 0.8968681693077087
Epoch 450, training loss: 6.892858028411865 = 0.5176044702529907 + 1.0 * 6.375253677368164
Epoch 450, val loss: 0.8904129862785339
Epoch 460, training loss: 6.873915672302246 = 0.4949749708175659 + 1.0 * 6.378940582275391
Epoch 460, val loss: 0.8853060007095337
Epoch 470, training loss: 6.84581184387207 = 0.4734346568584442 + 1.0 * 6.372377395629883
Epoch 470, val loss: 0.8815943598747253
Epoch 480, training loss: 6.819952964782715 = 0.4528060257434845 + 1.0 * 6.367146968841553
Epoch 480, val loss: 0.879084050655365
Epoch 490, training loss: 6.818751811981201 = 0.43302425742149353 + 1.0 * 6.385727405548096
Epoch 490, val loss: 0.8775721788406372
Epoch 500, training loss: 6.783142566680908 = 0.414377897977829 + 1.0 * 6.368764877319336
Epoch 500, val loss: 0.8771836161613464
Epoch 510, training loss: 6.757508277893066 = 0.3965657949447632 + 1.0 * 6.360942363739014
Epoch 510, val loss: 0.8779719471931458
Epoch 520, training loss: 6.738982677459717 = 0.3793826103210449 + 1.0 * 6.359600067138672
Epoch 520, val loss: 0.8792936205863953
Epoch 530, training loss: 6.723896503448486 = 0.36282408237457275 + 1.0 * 6.361072540283203
Epoch 530, val loss: 0.8814408183097839
Epoch 540, training loss: 6.704888820648193 = 0.3469603359699249 + 1.0 * 6.357928276062012
Epoch 540, val loss: 0.8844538331031799
Epoch 550, training loss: 6.684534549713135 = 0.3316761553287506 + 1.0 * 6.352858543395996
Epoch 550, val loss: 0.8880760073661804
Epoch 560, training loss: 6.672658920288086 = 0.3169194757938385 + 1.0 * 6.355739593505859
Epoch 560, val loss: 0.8922604322433472
Epoch 570, training loss: 6.658821105957031 = 0.3027825653553009 + 1.0 * 6.356038570404053
Epoch 570, val loss: 0.896819531917572
Epoch 580, training loss: 6.638729572296143 = 0.2892454266548157 + 1.0 * 6.349483966827393
Epoch 580, val loss: 0.9022857546806335
Epoch 590, training loss: 6.627148151397705 = 0.27616405487060547 + 1.0 * 6.3509840965271
Epoch 590, val loss: 0.9079967737197876
Epoch 600, training loss: 6.610499382019043 = 0.2636208236217499 + 1.0 * 6.346878528594971
Epoch 600, val loss: 0.9141652584075928
Epoch 610, training loss: 6.595248222351074 = 0.25155109167099 + 1.0 * 6.3436970710754395
Epoch 610, val loss: 0.9209441542625427
Epoch 620, training loss: 6.583037376403809 = 0.24003398418426514 + 1.0 * 6.343003273010254
Epoch 620, val loss: 0.927802324295044
Epoch 630, training loss: 6.569953918457031 = 0.22903066873550415 + 1.0 * 6.340923309326172
Epoch 630, val loss: 0.9353021383285522
Epoch 640, training loss: 6.55573034286499 = 0.21847011148929596 + 1.0 * 6.3372602462768555
Epoch 640, val loss: 0.943071722984314
Epoch 650, training loss: 6.547482013702393 = 0.20830824971199036 + 1.0 * 6.339173793792725
Epoch 650, val loss: 0.9511675834655762
Epoch 660, training loss: 6.5390472412109375 = 0.19866272807121277 + 1.0 * 6.340384483337402
Epoch 660, val loss: 0.959322452545166
Epoch 670, training loss: 6.523634910583496 = 0.18946745991706848 + 1.0 * 6.33416748046875
Epoch 670, val loss: 0.9681164622306824
Epoch 680, training loss: 6.520639896392822 = 0.18067602813243866 + 1.0 * 6.339963912963867
Epoch 680, val loss: 0.9769757390022278
Epoch 690, training loss: 6.506478786468506 = 0.17233902215957642 + 1.0 * 6.334139823913574
Epoch 690, val loss: 0.9860554933547974
Epoch 700, training loss: 6.494746208190918 = 0.1643538475036621 + 1.0 * 6.330392360687256
Epoch 700, val loss: 0.995582640171051
Epoch 710, training loss: 6.496496677398682 = 0.15674318373203278 + 1.0 * 6.3397536277771
Epoch 710, val loss: 1.0050580501556396
Epoch 720, training loss: 6.480469226837158 = 0.14954909682273865 + 1.0 * 6.330920219421387
Epoch 720, val loss: 1.0147316455841064
Epoch 730, training loss: 6.470370292663574 = 0.14270690083503723 + 1.0 * 6.327663421630859
Epoch 730, val loss: 1.024822473526001
Epoch 740, training loss: 6.466100215911865 = 0.1361837387084961 + 1.0 * 6.329916477203369
Epoch 740, val loss: 1.0347223281860352
Epoch 750, training loss: 6.456387042999268 = 0.13001498579978943 + 1.0 * 6.326372146606445
Epoch 750, val loss: 1.0447962284088135
Epoch 760, training loss: 6.449643135070801 = 0.12415371835231781 + 1.0 * 6.325489521026611
Epoch 760, val loss: 1.0551966428756714
Epoch 770, training loss: 6.451825141906738 = 0.11857669800519943 + 1.0 * 6.333248615264893
Epoch 770, val loss: 1.0654444694519043
Epoch 780, training loss: 6.438119411468506 = 0.11332613974809647 + 1.0 * 6.324793338775635
Epoch 780, val loss: 1.075932264328003
Epoch 790, training loss: 6.428619384765625 = 0.10832469910383224 + 1.0 * 6.3202948570251465
Epoch 790, val loss: 1.0864999294281006
Epoch 800, training loss: 6.427607536315918 = 0.10357517004013062 + 1.0 * 6.324032306671143
Epoch 800, val loss: 1.0969102382659912
Epoch 810, training loss: 6.420497894287109 = 0.09907479584217072 + 1.0 * 6.321423053741455
Epoch 810, val loss: 1.1074442863464355
Epoch 820, training loss: 6.41363000869751 = 0.09482979029417038 + 1.0 * 6.318800449371338
Epoch 820, val loss: 1.118113398551941
Epoch 830, training loss: 6.408816337585449 = 0.09078055620193481 + 1.0 * 6.31803560256958
Epoch 830, val loss: 1.1286271810531616
Epoch 840, training loss: 6.404922962188721 = 0.0869554653763771 + 1.0 * 6.317967414855957
Epoch 840, val loss: 1.1390819549560547
Epoch 850, training loss: 6.402561187744141 = 0.0833418145775795 + 1.0 * 6.319219589233398
Epoch 850, val loss: 1.1496871709823608
Epoch 860, training loss: 6.394254207611084 = 0.0799189954996109 + 1.0 * 6.314335346221924
Epoch 860, val loss: 1.1601142883300781
Epoch 870, training loss: 6.389452934265137 = 0.07667288929224014 + 1.0 * 6.312779903411865
Epoch 870, val loss: 1.1706297397613525
Epoch 880, training loss: 6.386558532714844 = 0.07358813285827637 + 1.0 * 6.312970161437988
Epoch 880, val loss: 1.1809899806976318
Epoch 890, training loss: 6.386647701263428 = 0.07066460698843002 + 1.0 * 6.315983295440674
Epoch 890, val loss: 1.1913498640060425
Epoch 900, training loss: 6.382057189941406 = 0.06788764894008636 + 1.0 * 6.314169406890869
Epoch 900, val loss: 1.2016774415969849
Epoch 910, training loss: 6.377877712249756 = 0.06526375561952591 + 1.0 * 6.3126139640808105
Epoch 910, val loss: 1.2118629217147827
Epoch 920, training loss: 6.373049259185791 = 0.06276581436395645 + 1.0 * 6.310283660888672
Epoch 920, val loss: 1.2220828533172607
Epoch 930, training loss: 6.376270294189453 = 0.06039665639400482 + 1.0 * 6.315873622894287
Epoch 930, val loss: 1.2320283651351929
Epoch 940, training loss: 6.364928722381592 = 0.05815255641937256 + 1.0 * 6.30677604675293
Epoch 940, val loss: 1.2420604228973389
Epoch 950, training loss: 6.362788677215576 = 0.05601263418793678 + 1.0 * 6.30677604675293
Epoch 950, val loss: 1.2520121335983276
Epoch 960, training loss: 6.359964847564697 = 0.053979724645614624 + 1.0 * 6.305984973907471
Epoch 960, val loss: 1.2617721557617188
Epoch 970, training loss: 6.362094402313232 = 0.0520470105111599 + 1.0 * 6.310047626495361
Epoch 970, val loss: 1.2716418504714966
Epoch 980, training loss: 6.360112190246582 = 0.050208427011966705 + 1.0 * 6.309903621673584
Epoch 980, val loss: 1.2811394929885864
Epoch 990, training loss: 6.353631019592285 = 0.04847310110926628 + 1.0 * 6.3051581382751465
Epoch 990, val loss: 1.2907077074050903
Epoch 1000, training loss: 6.348552703857422 = 0.046809494495391846 + 1.0 * 6.301743030548096
Epoch 1000, val loss: 1.3002265691757202
Epoch 1010, training loss: 6.3473687171936035 = 0.045219458639621735 + 1.0 * 6.302149295806885
Epoch 1010, val loss: 1.30955171585083
Epoch 1020, training loss: 6.346370697021484 = 0.04370536282658577 + 1.0 * 6.3026652336120605
Epoch 1020, val loss: 1.318699598312378
Epoch 1030, training loss: 6.341024398803711 = 0.04226237162947655 + 1.0 * 6.29876184463501
Epoch 1030, val loss: 1.3280519247055054
Epoch 1040, training loss: 6.345655918121338 = 0.04088259115815163 + 1.0 * 6.304773330688477
Epoch 1040, val loss: 1.3371906280517578
Epoch 1050, training loss: 6.33784294128418 = 0.039571914821863174 + 1.0 * 6.298271179199219
Epoch 1050, val loss: 1.3459925651550293
Epoch 1060, training loss: 6.335867404937744 = 0.03831592574715614 + 1.0 * 6.29755163192749
Epoch 1060, val loss: 1.3551082611083984
Epoch 1070, training loss: 6.347545146942139 = 0.037117958068847656 + 1.0 * 6.310427188873291
Epoch 1070, val loss: 1.3638272285461426
Epoch 1080, training loss: 6.33444881439209 = 0.03597191348671913 + 1.0 * 6.298476696014404
Epoch 1080, val loss: 1.3723658323287964
Epoch 1090, training loss: 6.329742908477783 = 0.034877192229032516 + 1.0 * 6.294865608215332
Epoch 1090, val loss: 1.3811328411102295
Epoch 1100, training loss: 6.329546928405762 = 0.03382313624024391 + 1.0 * 6.295723915100098
Epoch 1100, val loss: 1.3895909786224365
Epoch 1110, training loss: 6.339322090148926 = 0.032814450562000275 + 1.0 * 6.306507587432861
Epoch 1110, val loss: 1.3979328870773315
Epoch 1120, training loss: 6.325390815734863 = 0.0318591333925724 + 1.0 * 6.293531894683838
Epoch 1120, val loss: 1.406241536140442
Epoch 1130, training loss: 6.325321197509766 = 0.030938146635890007 + 1.0 * 6.2943830490112305
Epoch 1130, val loss: 1.4144809246063232
Epoch 1140, training loss: 6.326521396636963 = 0.030053220689296722 + 1.0 * 6.296468257904053
Epoch 1140, val loss: 1.422623634338379
Epoch 1150, training loss: 6.321070671081543 = 0.029207034036517143 + 1.0 * 6.291863441467285
Epoch 1150, val loss: 1.4304590225219727
Epoch 1160, training loss: 6.319627285003662 = 0.028393656015396118 + 1.0 * 6.291233539581299
Epoch 1160, val loss: 1.43855881690979
Epoch 1170, training loss: 6.321106433868408 = 0.027611102908849716 + 1.0 * 6.293495178222656
Epoch 1170, val loss: 1.4463953971862793
Epoch 1180, training loss: 6.319915771484375 = 0.026863515377044678 + 1.0 * 6.2930521965026855
Epoch 1180, val loss: 1.4540642499923706
Epoch 1190, training loss: 6.322098255157471 = 0.02614634856581688 + 1.0 * 6.295951843261719
Epoch 1190, val loss: 1.4616954326629639
Epoch 1200, training loss: 6.3159050941467285 = 0.025453386828303337 + 1.0 * 6.290451526641846
Epoch 1200, val loss: 1.4692736864089966
Epoch 1210, training loss: 6.313591957092285 = 0.024788035079836845 + 1.0 * 6.288804054260254
Epoch 1210, val loss: 1.4768363237380981
Epoch 1220, training loss: 6.3178839683532715 = 0.02414550632238388 + 1.0 * 6.29373836517334
Epoch 1220, val loss: 1.4841578006744385
Epoch 1230, training loss: 6.31323766708374 = 0.02353040501475334 + 1.0 * 6.289707183837891
Epoch 1230, val loss: 1.491459846496582
Epoch 1240, training loss: 6.3143110275268555 = 0.022939393296837807 + 1.0 * 6.291371822357178
Epoch 1240, val loss: 1.498749017715454
Epoch 1250, training loss: 6.312519073486328 = 0.022369662299752235 + 1.0 * 6.290149211883545
Epoch 1250, val loss: 1.5057092905044556
Epoch 1260, training loss: 6.307535648345947 = 0.02182350493967533 + 1.0 * 6.285712242126465
Epoch 1260, val loss: 1.512778878211975
Epoch 1270, training loss: 6.305957794189453 = 0.02129368670284748 + 1.0 * 6.284664154052734
Epoch 1270, val loss: 1.519882321357727
Epoch 1280, training loss: 6.31101131439209 = 0.02078206278383732 + 1.0 * 6.290229320526123
Epoch 1280, val loss: 1.5267201662063599
Epoch 1290, training loss: 6.306866645812988 = 0.02028973400592804 + 1.0 * 6.286576747894287
Epoch 1290, val loss: 1.5333130359649658
Epoch 1300, training loss: 6.304324626922607 = 0.019815176725387573 + 1.0 * 6.284509658813477
Epoch 1300, val loss: 1.540252923965454
Epoch 1310, training loss: 6.304953098297119 = 0.019357170909643173 + 1.0 * 6.285595893859863
Epoch 1310, val loss: 1.54701828956604
Epoch 1320, training loss: 6.309769630432129 = 0.018914002925157547 + 1.0 * 6.290855407714844
Epoch 1320, val loss: 1.5534651279449463
Epoch 1330, training loss: 6.303281307220459 = 0.01849086955189705 + 1.0 * 6.284790515899658
Epoch 1330, val loss: 1.559831142425537
Epoch 1340, training loss: 6.300929546356201 = 0.018079793080687523 + 1.0 * 6.2828497886657715
Epoch 1340, val loss: 1.566469430923462
Epoch 1350, training loss: 6.298723220825195 = 0.017681168392300606 + 1.0 * 6.281042098999023
Epoch 1350, val loss: 1.572828769683838
Epoch 1360, training loss: 6.2984819412231445 = 0.017293905839323997 + 1.0 * 6.281188011169434
Epoch 1360, val loss: 1.5791947841644287
Epoch 1370, training loss: 6.309567928314209 = 0.016919681802392006 + 1.0 * 6.2926483154296875
Epoch 1370, val loss: 1.5853911638259888
Epoch 1380, training loss: 6.29922342300415 = 0.016561437398195267 + 1.0 * 6.2826619148254395
Epoch 1380, val loss: 1.5913162231445312
Epoch 1390, training loss: 6.295745849609375 = 0.016214633360505104 + 1.0 * 6.279531002044678
Epoch 1390, val loss: 1.5976389646530151
Epoch 1400, training loss: 6.295618534088135 = 0.01587756909430027 + 1.0 * 6.279740810394287
Epoch 1400, val loss: 1.6037088632583618
Epoch 1410, training loss: 6.300943851470947 = 0.015549397096037865 + 1.0 * 6.285394668579102
Epoch 1410, val loss: 1.6095863580703735
Epoch 1420, training loss: 6.2961225509643555 = 0.015232749283313751 + 1.0 * 6.280889987945557
Epoch 1420, val loss: 1.615339994430542
Epoch 1430, training loss: 6.296395301818848 = 0.014927557669579983 + 1.0 * 6.281467914581299
Epoch 1430, val loss: 1.6210867166519165
Epoch 1440, training loss: 6.291728496551514 = 0.014632072299718857 + 1.0 * 6.277096271514893
Epoch 1440, val loss: 1.6269645690917969
Epoch 1450, training loss: 6.2917304039001465 = 0.014343692921102047 + 1.0 * 6.277386665344238
Epoch 1450, val loss: 1.6327486038208008
Epoch 1460, training loss: 6.2933759689331055 = 0.014063181355595589 + 1.0 * 6.279312610626221
Epoch 1460, val loss: 1.6383775472640991
Epoch 1470, training loss: 6.294758319854736 = 0.013791007921099663 + 1.0 * 6.2809672355651855
Epoch 1470, val loss: 1.6438076496124268
Epoch 1480, training loss: 6.29125452041626 = 0.013528943993151188 + 1.0 * 6.277725696563721
Epoch 1480, val loss: 1.6494140625
Epoch 1490, training loss: 6.293305397033691 = 0.013274374417960644 + 1.0 * 6.280031204223633
Epoch 1490, val loss: 1.6550503969192505
Epoch 1500, training loss: 6.2888383865356445 = 0.013026718981564045 + 1.0 * 6.275811672210693
Epoch 1500, val loss: 1.6602290868759155
Epoch 1510, training loss: 6.289127826690674 = 0.012785637751221657 + 1.0 * 6.276342391967773
Epoch 1510, val loss: 1.6656335592269897
Epoch 1520, training loss: 6.295016288757324 = 0.012553336098790169 + 1.0 * 6.282463073730469
Epoch 1520, val loss: 1.670914888381958
Epoch 1530, training loss: 6.287668704986572 = 0.012326717376708984 + 1.0 * 6.275341987609863
Epoch 1530, val loss: 1.6760532855987549
Epoch 1540, training loss: 6.285452365875244 = 0.012105712667107582 + 1.0 * 6.273346424102783
Epoch 1540, val loss: 1.6814658641815186
Epoch 1550, training loss: 6.293885707855225 = 0.011891168542206287 + 1.0 * 6.281994342803955
Epoch 1550, val loss: 1.686547875404358
Epoch 1560, training loss: 6.2866435050964355 = 0.011681782081723213 + 1.0 * 6.274961948394775
Epoch 1560, val loss: 1.691382884979248
Epoch 1570, training loss: 6.2838897705078125 = 0.011478704400360584 + 1.0 * 6.272410869598389
Epoch 1570, val loss: 1.6967005729675293
Epoch 1580, training loss: 6.284658432006836 = 0.011280028149485588 + 1.0 * 6.273378372192383
Epoch 1580, val loss: 1.701714277267456
Epoch 1590, training loss: 6.290536880493164 = 0.011086364276707172 + 1.0 * 6.279450416564941
Epoch 1590, val loss: 1.7064789533615112
Epoch 1600, training loss: 6.285904407501221 = 0.010900510475039482 + 1.0 * 6.275003910064697
Epoch 1600, val loss: 1.7114468812942505
Epoch 1610, training loss: 6.28378438949585 = 0.010717974044382572 + 1.0 * 6.273066520690918
Epoch 1610, val loss: 1.7162625789642334
Epoch 1620, training loss: 6.284175872802734 = 0.010540999472141266 + 1.0 * 6.273634910583496
Epoch 1620, val loss: 1.7211313247680664
Epoch 1630, training loss: 6.281813621520996 = 0.010366996750235558 + 1.0 * 6.271446704864502
Epoch 1630, val loss: 1.7257887125015259
Epoch 1640, training loss: 6.290988445281982 = 0.010199539363384247 + 1.0 * 6.280788898468018
Epoch 1640, val loss: 1.730446457862854
Epoch 1650, training loss: 6.2826128005981445 = 0.01003810204565525 + 1.0 * 6.2725749015808105
Epoch 1650, val loss: 1.7348829507827759
Epoch 1660, training loss: 6.2799601554870605 = 0.009878125041723251 + 1.0 * 6.270081996917725
Epoch 1660, val loss: 1.7396329641342163
Epoch 1670, training loss: 6.2804131507873535 = 0.009722388349473476 + 1.0 * 6.27069091796875
Epoch 1670, val loss: 1.7442235946655273
Epoch 1680, training loss: 6.278839588165283 = 0.009569862857460976 + 1.0 * 6.269269943237305
Epoch 1680, val loss: 1.7486602067947388
Epoch 1690, training loss: 6.286130428314209 = 0.009422156028449535 + 1.0 * 6.276708126068115
Epoch 1690, val loss: 1.7531298398971558
Epoch 1700, training loss: 6.278759956359863 = 0.009277713485062122 + 1.0 * 6.269482135772705
Epoch 1700, val loss: 1.7575607299804688
Epoch 1710, training loss: 6.275515556335449 = 0.009136286564171314 + 1.0 * 6.266379356384277
Epoch 1710, val loss: 1.7620567083358765
Epoch 1720, training loss: 6.280568599700928 = 0.008997795172035694 + 1.0 * 6.271570682525635
Epoch 1720, val loss: 1.7664827108383179
Epoch 1730, training loss: 6.279967784881592 = 0.008863473311066628 + 1.0 * 6.271104335784912
Epoch 1730, val loss: 1.770717978477478
Epoch 1740, training loss: 6.28090763092041 = 0.00873352587223053 + 1.0 * 6.272173881530762
Epoch 1740, val loss: 1.7747442722320557
Epoch 1750, training loss: 6.275976657867432 = 0.008606767281889915 + 1.0 * 6.267369747161865
Epoch 1750, val loss: 1.7791764736175537
Epoch 1760, training loss: 6.276343822479248 = 0.008481555618345737 + 1.0 * 6.267862319946289
Epoch 1760, val loss: 1.7833251953125
Epoch 1770, training loss: 6.277606010437012 = 0.008359108120203018 + 1.0 * 6.269247055053711
Epoch 1770, val loss: 1.7874811887741089
Epoch 1780, training loss: 6.2776384353637695 = 0.008240165188908577 + 1.0 * 6.269398212432861
Epoch 1780, val loss: 1.7914613485336304
Epoch 1790, training loss: 6.272295951843262 = 0.008124090731143951 + 1.0 * 6.264172077178955
Epoch 1790, val loss: 1.7955223321914673
Epoch 1800, training loss: 6.274264812469482 = 0.008009359240531921 + 1.0 * 6.2662553787231445
Epoch 1800, val loss: 1.799722671508789
Epoch 1810, training loss: 6.27506685256958 = 0.007897621020674706 + 1.0 * 6.267168998718262
Epoch 1810, val loss: 1.8036372661590576
Epoch 1820, training loss: 6.284712314605713 = 0.007789256516844034 + 1.0 * 6.276923179626465
Epoch 1820, val loss: 1.8073866367340088
Epoch 1830, training loss: 6.274489879608154 = 0.0076843020506203175 + 1.0 * 6.266805648803711
Epoch 1830, val loss: 1.81120765209198
Epoch 1840, training loss: 6.271975994110107 = 0.0075803049840033054 + 1.0 * 6.264395713806152
Epoch 1840, val loss: 1.8153413534164429
Epoch 1850, training loss: 6.271043300628662 = 0.0074775973334908485 + 1.0 * 6.263565540313721
Epoch 1850, val loss: 1.8191975355148315
Epoch 1860, training loss: 6.286418437957764 = 0.007377226371318102 + 1.0 * 6.279041290283203
Epoch 1860, val loss: 1.8227535486221313
Epoch 1870, training loss: 6.273822784423828 = 0.007282812148332596 + 1.0 * 6.266540050506592
Epoch 1870, val loss: 1.8263704776763916
Epoch 1880, training loss: 6.270341873168945 = 0.007186210248619318 + 1.0 * 6.263155460357666
Epoch 1880, val loss: 1.8303202390670776
Epoch 1890, training loss: 6.269085884094238 = 0.00709250383079052 + 1.0 * 6.261993408203125
Epoch 1890, val loss: 1.8341253995895386
Epoch 1900, training loss: 6.280681133270264 = 0.006999817211180925 + 1.0 * 6.273681163787842
Epoch 1900, val loss: 1.837660312652588
Epoch 1910, training loss: 6.274800777435303 = 0.006912746001034975 + 1.0 * 6.267888069152832
Epoch 1910, val loss: 1.841033935546875
Epoch 1920, training loss: 6.270423889160156 = 0.006824530195444822 + 1.0 * 6.263599395751953
Epoch 1920, val loss: 1.8447515964508057
Epoch 1930, training loss: 6.268016338348389 = 0.006739181932061911 + 1.0 * 6.261277198791504
Epoch 1930, val loss: 1.8484302759170532
Epoch 1940, training loss: 6.274376392364502 = 0.006654208991676569 + 1.0 * 6.267722129821777
Epoch 1940, val loss: 1.8518412113189697
Epoch 1950, training loss: 6.2672905921936035 = 0.006573109421879053 + 1.0 * 6.260717391967773
Epoch 1950, val loss: 1.855291724205017
Epoch 1960, training loss: 6.269930839538574 = 0.00649228785187006 + 1.0 * 6.263438701629639
Epoch 1960, val loss: 1.8587729930877686
Epoch 1970, training loss: 6.269014358520508 = 0.006413093768060207 + 1.0 * 6.262601375579834
Epoch 1970, val loss: 1.8621610403060913
Epoch 1980, training loss: 6.271561622619629 = 0.006335850805044174 + 1.0 * 6.265225887298584
Epoch 1980, val loss: 1.8655561208724976
Epoch 1990, training loss: 6.268533229827881 = 0.006260807625949383 + 1.0 * 6.262272357940674
Epoch 1990, val loss: 1.8688404560089111
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 10.54893684387207 = 1.952104926109314 + 1.0 * 8.596832275390625
Epoch 0, val loss: 1.960600733757019
Epoch 10, training loss: 10.538643836975098 = 1.9420595169067383 + 1.0 * 8.59658432006836
Epoch 10, val loss: 1.9502160549163818
Epoch 20, training loss: 10.52420711517334 = 1.9296382665634155 + 1.0 * 8.594569206237793
Epoch 20, val loss: 1.9371150732040405
Epoch 30, training loss: 10.490139961242676 = 1.912405014038086 + 1.0 * 8.57773494720459
Epoch 30, val loss: 1.9189172983169556
Epoch 40, training loss: 10.365516662597656 = 1.8893258571624756 + 1.0 * 8.476190567016602
Epoch 40, val loss: 1.895338535308838
Epoch 50, training loss: 9.976237297058105 = 1.8635774850845337 + 1.0 * 8.112659454345703
Epoch 50, val loss: 1.869835376739502
Epoch 60, training loss: 9.691706657409668 = 1.8396854400634766 + 1.0 * 7.852021217346191
Epoch 60, val loss: 1.8469849824905396
Epoch 70, training loss: 9.23682975769043 = 1.8240095376968384 + 1.0 * 7.412819862365723
Epoch 70, val loss: 1.8318570852279663
Epoch 80, training loss: 8.873809814453125 = 1.81326425075531 + 1.0 * 7.060545444488525
Epoch 80, val loss: 1.8217096328735352
Epoch 90, training loss: 8.689020156860352 = 1.8006227016448975 + 1.0 * 6.888397693634033
Epoch 90, val loss: 1.8102836608886719
Epoch 100, training loss: 8.574069023132324 = 1.7859035730361938 + 1.0 * 6.788165092468262
Epoch 100, val loss: 1.7967238426208496
Epoch 110, training loss: 8.488532066345215 = 1.7721415758132935 + 1.0 * 6.716390132904053
Epoch 110, val loss: 1.7832406759262085
Epoch 120, training loss: 8.423115730285645 = 1.7583695650100708 + 1.0 * 6.664746284484863
Epoch 120, val loss: 1.769959568977356
Epoch 130, training loss: 8.371673583984375 = 1.7430641651153564 + 1.0 * 6.6286091804504395
Epoch 130, val loss: 1.755894660949707
Epoch 140, training loss: 8.324966430664062 = 1.72538423538208 + 1.0 * 6.599581718444824
Epoch 140, val loss: 1.740219235420227
Epoch 150, training loss: 8.278599739074707 = 1.7048543691635132 + 1.0 * 6.573745250701904
Epoch 150, val loss: 1.722375750541687
Epoch 160, training loss: 8.235854148864746 = 1.6805278062820435 + 1.0 * 6.555326461791992
Epoch 160, val loss: 1.7013678550720215
Epoch 170, training loss: 8.189014434814453 = 1.6519120931625366 + 1.0 * 6.537102222442627
Epoch 170, val loss: 1.6767653226852417
Epoch 180, training loss: 8.138710021972656 = 1.6182113885879517 + 1.0 * 6.520498275756836
Epoch 180, val loss: 1.6475611925125122
Epoch 190, training loss: 8.089696884155273 = 1.5786969661712646 + 1.0 * 6.511000156402588
Epoch 190, val loss: 1.613113522529602
Epoch 200, training loss: 8.027057647705078 = 1.5344737768173218 + 1.0 * 6.492584228515625
Epoch 200, val loss: 1.5749454498291016
Epoch 210, training loss: 7.963906288146973 = 1.4858338832855225 + 1.0 * 6.478072643280029
Epoch 210, val loss: 1.5330592393875122
Epoch 220, training loss: 7.900310516357422 = 1.4337947368621826 + 1.0 * 6.46651554107666
Epoch 220, val loss: 1.488554835319519
Epoch 230, training loss: 7.836310386657715 = 1.3801145553588867 + 1.0 * 6.456195831298828
Epoch 230, val loss: 1.443424105644226
Epoch 240, training loss: 7.780027389526367 = 1.325960636138916 + 1.0 * 6.454066753387451
Epoch 240, val loss: 1.3987818956375122
Epoch 250, training loss: 7.714470863342285 = 1.2735555171966553 + 1.0 * 6.440915584564209
Epoch 250, val loss: 1.3565702438354492
Epoch 260, training loss: 7.655120849609375 = 1.2224273681640625 + 1.0 * 6.4326934814453125
Epoch 260, val loss: 1.3166273832321167
Epoch 270, training loss: 7.598170280456543 = 1.1723476648330688 + 1.0 * 6.425822734832764
Epoch 270, val loss: 1.2787280082702637
Epoch 280, training loss: 7.5435638427734375 = 1.123177409172058 + 1.0 * 6.42038631439209
Epoch 280, val loss: 1.2429122924804688
Epoch 290, training loss: 7.492175102233887 = 1.0752630233764648 + 1.0 * 6.416912078857422
Epoch 290, val loss: 1.209132432937622
Epoch 300, training loss: 7.437936305999756 = 1.0274887084960938 + 1.0 * 6.410447597503662
Epoch 300, val loss: 1.176298975944519
Epoch 310, training loss: 7.393668174743652 = 0.9795265793800354 + 1.0 * 6.414141654968262
Epoch 310, val loss: 1.1439671516418457
Epoch 320, training loss: 7.337235450744629 = 0.9324454665184021 + 1.0 * 6.404789924621582
Epoch 320, val loss: 1.1127508878707886
Epoch 330, training loss: 7.284222602844238 = 0.8860480189323425 + 1.0 * 6.39817476272583
Epoch 330, val loss: 1.0825306177139282
Epoch 340, training loss: 7.2359819412231445 = 0.8404650688171387 + 1.0 * 6.395516872406006
Epoch 340, val loss: 1.0532490015029907
Epoch 350, training loss: 7.188526153564453 = 0.7968536019325256 + 1.0 * 6.391672611236572
Epoch 350, val loss: 1.0257586240768433
Epoch 360, training loss: 7.144195556640625 = 0.7557076215744019 + 1.0 * 6.388487815856934
Epoch 360, val loss: 1.0005468130111694
Epoch 370, training loss: 7.099729537963867 = 0.7166789174079895 + 1.0 * 6.383050441741943
Epoch 370, val loss: 0.9773533344268799
Epoch 380, training loss: 7.060020446777344 = 0.6797585487365723 + 1.0 * 6.3802618980407715
Epoch 380, val loss: 0.9564324021339417
Epoch 390, training loss: 7.039341926574707 = 0.6455061435699463 + 1.0 * 6.39383602142334
Epoch 390, val loss: 0.9382131695747375
Epoch 400, training loss: 6.992583274841309 = 0.6143082976341248 + 1.0 * 6.378274917602539
Epoch 400, val loss: 0.9230276346206665
Epoch 410, training loss: 6.956861972808838 = 0.5851826667785645 + 1.0 * 6.371679306030273
Epoch 410, val loss: 0.9102990627288818
Epoch 420, training loss: 6.926081657409668 = 0.5577222108840942 + 1.0 * 6.368359565734863
Epoch 420, val loss: 0.899603545665741
Epoch 430, training loss: 6.896880626678467 = 0.5317530632019043 + 1.0 * 6.3651275634765625
Epoch 430, val loss: 0.8909673094749451
Epoch 440, training loss: 6.870911121368408 = 0.507123589515686 + 1.0 * 6.363787651062012
Epoch 440, val loss: 0.8841438293457031
Epoch 450, training loss: 6.8595709800720215 = 0.4840533435344696 + 1.0 * 6.375517845153809
Epoch 450, val loss: 0.8789615035057068
Epoch 460, training loss: 6.825056076049805 = 0.4625335931777954 + 1.0 * 6.362522602081299
Epoch 460, val loss: 0.8756927251815796
Epoch 470, training loss: 6.79813289642334 = 0.44210752844810486 + 1.0 * 6.356025218963623
Epoch 470, val loss: 0.8736304640769958
Epoch 480, training loss: 6.775825500488281 = 0.4225350618362427 + 1.0 * 6.353290557861328
Epoch 480, val loss: 0.8724406361579895
Epoch 490, training loss: 6.7580060958862305 = 0.4037133455276489 + 1.0 * 6.354292869567871
Epoch 490, val loss: 0.8723469972610474
Epoch 500, training loss: 6.741414546966553 = 0.3857855796813965 + 1.0 * 6.355628967285156
Epoch 500, val loss: 0.8729346394538879
Epoch 510, training loss: 6.7201972007751465 = 0.36879074573516846 + 1.0 * 6.351406574249268
Epoch 510, val loss: 0.8747509121894836
Epoch 520, training loss: 6.698276519775391 = 0.3523738384246826 + 1.0 * 6.345902919769287
Epoch 520, val loss: 0.8771325349807739
Epoch 530, training loss: 6.679344177246094 = 0.3364439010620117 + 1.0 * 6.342900276184082
Epoch 530, val loss: 0.8800720572471619
Epoch 540, training loss: 6.6618852615356445 = 0.32097169756889343 + 1.0 * 6.340913772583008
Epoch 540, val loss: 0.8837950825691223
Epoch 550, training loss: 6.664520740509033 = 0.30598241090774536 + 1.0 * 6.3585381507873535
Epoch 550, val loss: 0.8880484104156494
Epoch 560, training loss: 6.631190776824951 = 0.29166215658187866 + 1.0 * 6.339528560638428
Epoch 560, val loss: 0.8930529356002808
Epoch 570, training loss: 6.615150451660156 = 0.2779009938240051 + 1.0 * 6.337249279022217
Epoch 570, val loss: 0.8988541960716248
Epoch 580, training loss: 6.601799488067627 = 0.26473549008369446 + 1.0 * 6.337063789367676
Epoch 580, val loss: 0.9048321843147278
Epoch 590, training loss: 6.586305141448975 = 0.2522123456001282 + 1.0 * 6.334092617034912
Epoch 590, val loss: 0.9116475582122803
Epoch 600, training loss: 6.572067737579346 = 0.24027594923973083 + 1.0 * 6.331791877746582
Epoch 600, val loss: 0.9188455939292908
Epoch 610, training loss: 6.573887825012207 = 0.22893059253692627 + 1.0 * 6.34495735168457
Epoch 610, val loss: 0.9263556003570557
Epoch 620, training loss: 6.5492939949035645 = 0.21827025711536407 + 1.0 * 6.331023693084717
Epoch 620, val loss: 0.9346122145652771
Epoch 630, training loss: 6.535097599029541 = 0.2081798017024994 + 1.0 * 6.32691764831543
Epoch 630, val loss: 0.9433373212814331
Epoch 640, training loss: 6.5286760330200195 = 0.19859512150287628 + 1.0 * 6.330080986022949
Epoch 640, val loss: 0.9523113369941711
Epoch 650, training loss: 6.515803337097168 = 0.18954998254776 + 1.0 * 6.326253414154053
Epoch 650, val loss: 0.9613416790962219
Epoch 660, training loss: 6.506170272827148 = 0.1810011863708496 + 1.0 * 6.325169086456299
Epoch 660, val loss: 0.9712013602256775
Epoch 670, training loss: 6.5022077560424805 = 0.17290183901786804 + 1.0 * 6.329306125640869
Epoch 670, val loss: 0.9809780716896057
Epoch 680, training loss: 6.493719577789307 = 0.16529177129268646 + 1.0 * 6.328427791595459
Epoch 680, val loss: 0.9908415675163269
Epoch 690, training loss: 6.480052947998047 = 0.15809203684329987 + 1.0 * 6.321960926055908
Epoch 690, val loss: 1.0012813806533813
Epoch 700, training loss: 6.469317436218262 = 0.15126128494739532 + 1.0 * 6.318056106567383
Epoch 700, val loss: 1.011679768562317
Epoch 710, training loss: 6.47326135635376 = 0.14476214349269867 + 1.0 * 6.3284993171691895
Epoch 710, val loss: 1.0222877264022827
Epoch 720, training loss: 6.46075439453125 = 0.13864728808403015 + 1.0 * 6.322107315063477
Epoch 720, val loss: 1.0327389240264893
Epoch 730, training loss: 6.449593544006348 = 0.13284696638584137 + 1.0 * 6.316746711730957
Epoch 730, val loss: 1.043771505355835
Epoch 740, training loss: 6.441654682159424 = 0.12731525301933289 + 1.0 * 6.314339637756348
Epoch 740, val loss: 1.0545943975448608
Epoch 750, training loss: 6.43862247467041 = 0.12206682562828064 + 1.0 * 6.316555500030518
Epoch 750, val loss: 1.0651168823242188
Epoch 760, training loss: 6.431365966796875 = 0.11710117757320404 + 1.0 * 6.31426477432251
Epoch 760, val loss: 1.0761184692382812
Epoch 770, training loss: 6.423285961151123 = 0.11237014830112457 + 1.0 * 6.310915946960449
Epoch 770, val loss: 1.0870815515518188
Epoch 780, training loss: 6.418922424316406 = 0.10784931480884552 + 1.0 * 6.311073303222656
Epoch 780, val loss: 1.0979859828948975
Epoch 790, training loss: 6.4143595695495605 = 0.10355328768491745 + 1.0 * 6.3108062744140625
Epoch 790, val loss: 1.1086629629135132
Epoch 800, training loss: 6.40962553024292 = 0.09947974234819412 + 1.0 * 6.310145854949951
Epoch 800, val loss: 1.1196449995040894
Epoch 810, training loss: 6.402318477630615 = 0.09558724611997604 + 1.0 * 6.306731224060059
Epoch 810, val loss: 1.1306082010269165
Epoch 820, training loss: 6.400670051574707 = 0.09186172485351562 + 1.0 * 6.308808326721191
Epoch 820, val loss: 1.141323447227478
Epoch 830, training loss: 6.393850803375244 = 0.08831706643104553 + 1.0 * 6.3055338859558105
Epoch 830, val loss: 1.151937484741211
Epoch 840, training loss: 6.393725395202637 = 0.08494267612695694 + 1.0 * 6.308782577514648
Epoch 840, val loss: 1.1626381874084473
Epoch 850, training loss: 6.385737895965576 = 0.0817183330655098 + 1.0 * 6.304019451141357
Epoch 850, val loss: 1.173353672027588
Epoch 860, training loss: 6.381981372833252 = 0.07862978428602219 + 1.0 * 6.303351402282715
Epoch 860, val loss: 1.1839717626571655
Epoch 870, training loss: 6.3832926750183105 = 0.07567531615495682 + 1.0 * 6.3076171875
Epoch 870, val loss: 1.1944308280944824
Epoch 880, training loss: 6.381939888000488 = 0.0728549063205719 + 1.0 * 6.309084892272949
Epoch 880, val loss: 1.2047312259674072
Epoch 890, training loss: 6.3717498779296875 = 0.07017365843057632 + 1.0 * 6.301576137542725
Epoch 890, val loss: 1.2152293920516968
Epoch 900, training loss: 6.375323295593262 = 0.06760711967945099 + 1.0 * 6.307716369628906
Epoch 900, val loss: 1.22533118724823
Epoch 910, training loss: 6.365494728088379 = 0.06514817476272583 + 1.0 * 6.300346374511719
Epoch 910, val loss: 1.2356294393539429
Epoch 920, training loss: 6.3611345291137695 = 0.06279630213975906 + 1.0 * 6.298338413238525
Epoch 920, val loss: 1.2458810806274414
Epoch 930, training loss: 6.35774040222168 = 0.06053675711154938 + 1.0 * 6.297203540802002
Epoch 930, val loss: 1.2559502124786377
Epoch 940, training loss: 6.366419792175293 = 0.058372024446725845 + 1.0 * 6.308047771453857
Epoch 940, val loss: 1.2655916213989258
Epoch 950, training loss: 6.354385852813721 = 0.056311920285224915 + 1.0 * 6.298073768615723
Epoch 950, val loss: 1.2755123376846313
Epoch 960, training loss: 6.3488945960998535 = 0.05433811619877815 + 1.0 * 6.294556617736816
Epoch 960, val loss: 1.2855606079101562
Epoch 970, training loss: 6.346757888793945 = 0.05244334414601326 + 1.0 * 6.294314384460449
Epoch 970, val loss: 1.2952913045883179
Epoch 980, training loss: 6.364290714263916 = 0.05062934011220932 + 1.0 * 6.313661575317383
Epoch 980, val loss: 1.304610013961792
Epoch 990, training loss: 6.343084335327148 = 0.04889164865016937 + 1.0 * 6.294192790985107
Epoch 990, val loss: 1.314056396484375
Epoch 1000, training loss: 6.338865280151367 = 0.047232549637556076 + 1.0 * 6.291632652282715
Epoch 1000, val loss: 1.3238492012023926
Epoch 1010, training loss: 6.338303565979004 = 0.0456392876803875 + 1.0 * 6.292664051055908
Epoch 1010, val loss: 1.3331068754196167
Epoch 1020, training loss: 6.340910911560059 = 0.04410850629210472 + 1.0 * 6.296802520751953
Epoch 1020, val loss: 1.3421710729599
Epoch 1030, training loss: 6.332889556884766 = 0.04264478012919426 + 1.0 * 6.2902445793151855
Epoch 1030, val loss: 1.3515042066574097
Epoch 1040, training loss: 6.330442428588867 = 0.041241779923439026 + 1.0 * 6.289200782775879
Epoch 1040, val loss: 1.3607298135757446
Epoch 1050, training loss: 6.335016250610352 = 0.03989124670624733 + 1.0 * 6.2951250076293945
Epoch 1050, val loss: 1.3696800470352173
Epoch 1060, training loss: 6.335650444030762 = 0.03860238194465637 + 1.0 * 6.297048091888428
Epoch 1060, val loss: 1.3782931566238403
Epoch 1070, training loss: 6.327530384063721 = 0.037365254014730453 + 1.0 * 6.290164947509766
Epoch 1070, val loss: 1.3873274326324463
Epoch 1080, training loss: 6.325868606567383 = 0.03618240728974342 + 1.0 * 6.28968620300293
Epoch 1080, val loss: 1.3960270881652832
Epoch 1090, training loss: 6.323517322540283 = 0.03504584729671478 + 1.0 * 6.288471698760986
Epoch 1090, val loss: 1.404676079750061
Epoch 1100, training loss: 6.319840431213379 = 0.03395587205886841 + 1.0 * 6.285884380340576
Epoch 1100, val loss: 1.4133504629135132
Epoch 1110, training loss: 6.327669143676758 = 0.032908711582422256 + 1.0 * 6.294760227203369
Epoch 1110, val loss: 1.421860933303833
Epoch 1120, training loss: 6.32185697555542 = 0.03190591558814049 + 1.0 * 6.289950847625732
Epoch 1120, val loss: 1.4298902750015259
Epoch 1130, training loss: 6.316635608673096 = 0.030943552032113075 + 1.0 * 6.28569221496582
Epoch 1130, val loss: 1.4384709596633911
Epoch 1140, training loss: 6.317988872528076 = 0.03002077527344227 + 1.0 * 6.287968158721924
Epoch 1140, val loss: 1.4464768171310425
Epoch 1150, training loss: 6.3197174072265625 = 0.029134564101696014 + 1.0 * 6.290582656860352
Epoch 1150, val loss: 1.4545586109161377
Epoch 1160, training loss: 6.31389856338501 = 0.028283653780817986 + 1.0 * 6.285614967346191
Epoch 1160, val loss: 1.462458610534668
Epoch 1170, training loss: 6.309613227844238 = 0.027469156309962273 + 1.0 * 6.282144069671631
Epoch 1170, val loss: 1.4704934358596802
Epoch 1180, training loss: 6.308701992034912 = 0.026684919372200966 + 1.0 * 6.282017230987549
Epoch 1180, val loss: 1.4782516956329346
Epoch 1190, training loss: 6.312247276306152 = 0.025929942727088928 + 1.0 * 6.286317348480225
Epoch 1190, val loss: 1.485790491104126
Epoch 1200, training loss: 6.312744617462158 = 0.025202594697475433 + 1.0 * 6.28754186630249
Epoch 1200, val loss: 1.4934877157211304
Epoch 1210, training loss: 6.306860446929932 = 0.024509746581315994 + 1.0 * 6.282350540161133
Epoch 1210, val loss: 1.5007684230804443
Epoch 1220, training loss: 6.305952548980713 = 0.02383999340236187 + 1.0 * 6.2821125984191895
Epoch 1220, val loss: 1.508497714996338
Epoch 1230, training loss: 6.310967445373535 = 0.02319781295955181 + 1.0 * 6.287769794464111
Epoch 1230, val loss: 1.515531301498413
Epoch 1240, training loss: 6.302931785583496 = 0.022582679986953735 + 1.0 * 6.280349254608154
Epoch 1240, val loss: 1.522714376449585
Epoch 1250, training loss: 6.29979133605957 = 0.021987207233905792 + 1.0 * 6.277803897857666
Epoch 1250, val loss: 1.5301421880722046
Epoch 1260, training loss: 6.299815654754639 = 0.021413566544651985 + 1.0 * 6.278401851654053
Epoch 1260, val loss: 1.5371649265289307
Epoch 1270, training loss: 6.305453300476074 = 0.02086230367422104 + 1.0 * 6.284591197967529
Epoch 1270, val loss: 1.5438671112060547
Epoch 1280, training loss: 6.302602291107178 = 0.020331647247076035 + 1.0 * 6.282270431518555
Epoch 1280, val loss: 1.5505456924438477
Epoch 1290, training loss: 6.300932884216309 = 0.01982167176902294 + 1.0 * 6.281111240386963
Epoch 1290, val loss: 1.5576215982437134
Epoch 1300, training loss: 6.296613693237305 = 0.01933356001973152 + 1.0 * 6.277280330657959
Epoch 1300, val loss: 1.5641343593597412
Epoch 1310, training loss: 6.293690204620361 = 0.018859226256608963 + 1.0 * 6.2748308181762695
Epoch 1310, val loss: 1.5709962844848633
Epoch 1320, training loss: 6.2950758934021 = 0.01840190403163433 + 1.0 * 6.276673793792725
Epoch 1320, val loss: 1.577446460723877
Epoch 1330, training loss: 6.296816349029541 = 0.017959972843527794 + 1.0 * 6.27885627746582
Epoch 1330, val loss: 1.5836185216903687
Epoch 1340, training loss: 6.293583393096924 = 0.017535196617245674 + 1.0 * 6.276048183441162
Epoch 1340, val loss: 1.5900039672851562
Epoch 1350, training loss: 6.295869827270508 = 0.017123980447649956 + 1.0 * 6.278745651245117
Epoch 1350, val loss: 1.5963054895401
Epoch 1360, training loss: 6.291738033294678 = 0.016726963222026825 + 1.0 * 6.27501106262207
Epoch 1360, val loss: 1.602473258972168
Epoch 1370, training loss: 6.29031229019165 = 0.016345234587788582 + 1.0 * 6.273967266082764
Epoch 1370, val loss: 1.6088058948516846
Epoch 1380, training loss: 6.2918829917907715 = 0.015974581241607666 + 1.0 * 6.275908470153809
Epoch 1380, val loss: 1.6147665977478027
Epoch 1390, training loss: 6.2882490158081055 = 0.015617319382727146 + 1.0 * 6.272631645202637
Epoch 1390, val loss: 1.620446801185608
Epoch 1400, training loss: 6.287576675415039 = 0.01527312770485878 + 1.0 * 6.272303581237793
Epoch 1400, val loss: 1.626389980316162
Epoch 1410, training loss: 6.290352821350098 = 0.014939158223569393 + 1.0 * 6.275413513183594
Epoch 1410, val loss: 1.63239324092865
Epoch 1420, training loss: 6.286187171936035 = 0.014616596512496471 + 1.0 * 6.271570682525635
Epoch 1420, val loss: 1.6379389762878418
Epoch 1430, training loss: 6.284572601318359 = 0.01430502999573946 + 1.0 * 6.270267486572266
Epoch 1430, val loss: 1.6437774896621704
Epoch 1440, training loss: 6.288031101226807 = 0.01400214433670044 + 1.0 * 6.274028778076172
Epoch 1440, val loss: 1.6493736505508423
Epoch 1450, training loss: 6.284289360046387 = 0.013710403814911842 + 1.0 * 6.270578861236572
Epoch 1450, val loss: 1.6544944047927856
Epoch 1460, training loss: 6.287113189697266 = 0.013427652418613434 + 1.0 * 6.273685455322266
Epoch 1460, val loss: 1.6601074934005737
Epoch 1470, training loss: 6.282947540283203 = 0.013154429383575916 + 1.0 * 6.2697930335998535
Epoch 1470, val loss: 1.6656134128570557
Epoch 1480, training loss: 6.284717559814453 = 0.012888551689684391 + 1.0 * 6.271829128265381
Epoch 1480, val loss: 1.6711070537567139
Epoch 1490, training loss: 6.283699035644531 = 0.012632288038730621 + 1.0 * 6.271066665649414
Epoch 1490, val loss: 1.6758917570114136
Epoch 1500, training loss: 6.28041934967041 = 0.012383612804114819 + 1.0 * 6.268035888671875
Epoch 1500, val loss: 1.68118417263031
Epoch 1510, training loss: 6.280426979064941 = 0.012141306884586811 + 1.0 * 6.268285751342773
Epoch 1510, val loss: 1.68656325340271
Epoch 1520, training loss: 6.291499137878418 = 0.011906780302524567 + 1.0 * 6.279592514038086
Epoch 1520, val loss: 1.6912699937820435
Epoch 1530, training loss: 6.283235549926758 = 0.011681622825562954 + 1.0 * 6.271553993225098
Epoch 1530, val loss: 1.696203589439392
Epoch 1540, training loss: 6.279755592346191 = 0.01146076712757349 + 1.0 * 6.268294811248779
Epoch 1540, val loss: 1.7013483047485352
Epoch 1550, training loss: 6.280733108520508 = 0.011247421614825726 + 1.0 * 6.2694854736328125
Epoch 1550, val loss: 1.706221103668213
Epoch 1560, training loss: 6.277521133422852 = 0.011039366014301777 + 1.0 * 6.266481876373291
Epoch 1560, val loss: 1.7107995748519897
Epoch 1570, training loss: 6.279506206512451 = 0.010837594047188759 + 1.0 * 6.2686686515808105
Epoch 1570, val loss: 1.7157539129257202
Epoch 1580, training loss: 6.279830455780029 = 0.010640914551913738 + 1.0 * 6.269189357757568
Epoch 1580, val loss: 1.7204078435897827
Epoch 1590, training loss: 6.274320125579834 = 0.010450713336467743 + 1.0 * 6.263869285583496
Epoch 1590, val loss: 1.7248793840408325
Epoch 1600, training loss: 6.278876781463623 = 0.010265648365020752 + 1.0 * 6.268610954284668
Epoch 1600, val loss: 1.7295199632644653
Epoch 1610, training loss: 6.276031494140625 = 0.01008558552712202 + 1.0 * 6.265945911407471
Epoch 1610, val loss: 1.73385751247406
Epoch 1620, training loss: 6.273134708404541 = 0.009911183267831802 + 1.0 * 6.263223648071289
Epoch 1620, val loss: 1.7383198738098145
Epoch 1630, training loss: 6.272708892822266 = 0.009741470217704773 + 1.0 * 6.262967586517334
Epoch 1630, val loss: 1.7430651187896729
Epoch 1640, training loss: 6.279441833496094 = 0.009575669653713703 + 1.0 * 6.269865989685059
Epoch 1640, val loss: 1.7471351623535156
Epoch 1650, training loss: 6.276124000549316 = 0.00941393431276083 + 1.0 * 6.26671028137207
Epoch 1650, val loss: 1.751282811164856
Epoch 1660, training loss: 6.277400016784668 = 0.009257723577320576 + 1.0 * 6.268142223358154
Epoch 1660, val loss: 1.7555910348892212
Epoch 1670, training loss: 6.271615028381348 = 0.009106197394430637 + 1.0 * 6.262508869171143
Epoch 1670, val loss: 1.759771704673767
Epoch 1680, training loss: 6.271473407745361 = 0.008958281017839909 + 1.0 * 6.262515068054199
Epoch 1680, val loss: 1.7640324831008911
Epoch 1690, training loss: 6.270905494689941 = 0.00881342776119709 + 1.0 * 6.262092113494873
Epoch 1690, val loss: 1.7680476903915405
Epoch 1700, training loss: 6.275299072265625 = 0.008671734482049942 + 1.0 * 6.266627311706543
Epoch 1700, val loss: 1.7720873355865479
Epoch 1710, training loss: 6.271035194396973 = 0.008533891290426254 + 1.0 * 6.262501239776611
Epoch 1710, val loss: 1.7760872840881348
Epoch 1720, training loss: 6.270583629608154 = 0.00840066373348236 + 1.0 * 6.26218318939209
Epoch 1720, val loss: 1.780104160308838
Epoch 1730, training loss: 6.271082878112793 = 0.00827043130993843 + 1.0 * 6.262812614440918
Epoch 1730, val loss: 1.7840347290039062
Epoch 1740, training loss: 6.269835472106934 = 0.008143112994730473 + 1.0 * 6.261692523956299
Epoch 1740, val loss: 1.788027286529541
Epoch 1750, training loss: 6.269627571105957 = 0.008019376546144485 + 1.0 * 6.261608123779297
Epoch 1750, val loss: 1.7917662858963013
Epoch 1760, training loss: 6.267096519470215 = 0.007897986099123955 + 1.0 * 6.2591986656188965
Epoch 1760, val loss: 1.7954351902008057
Epoch 1770, training loss: 6.274205207824707 = 0.007779565639793873 + 1.0 * 6.266425609588623
Epoch 1770, val loss: 1.7991911172866821
Epoch 1780, training loss: 6.270029544830322 = 0.00766483461484313 + 1.0 * 6.262364864349365
Epoch 1780, val loss: 1.8025238513946533
Epoch 1790, training loss: 6.26692008972168 = 0.00755213713273406 + 1.0 * 6.259367942810059
Epoch 1790, val loss: 1.8064525127410889
Epoch 1800, training loss: 6.266952991485596 = 0.007443439681082964 + 1.0 * 6.259509563446045
Epoch 1800, val loss: 1.8102678060531616
Epoch 1810, training loss: 6.2713303565979 = 0.007335870526731014 + 1.0 * 6.2639946937561035
Epoch 1810, val loss: 1.8135573863983154
Epoch 1820, training loss: 6.2656941413879395 = 0.007231340277940035 + 1.0 * 6.258462905883789
Epoch 1820, val loss: 1.8169901371002197
Epoch 1830, training loss: 6.263810634613037 = 0.007129111792892218 + 1.0 * 6.256681442260742
Epoch 1830, val loss: 1.820600986480713
Epoch 1840, training loss: 6.268828868865967 = 0.0070287007838487625 + 1.0 * 6.261800289154053
Epoch 1840, val loss: 1.8239188194274902
Epoch 1850, training loss: 6.264885902404785 = 0.006931038573384285 + 1.0 * 6.257955074310303
Epoch 1850, val loss: 1.8273142576217651
Epoch 1860, training loss: 6.263162136077881 = 0.006835049483925104 + 1.0 * 6.256327152252197
Epoch 1860, val loss: 1.830716848373413
Epoch 1870, training loss: 6.2730817794799805 = 0.00674150325357914 + 1.0 * 6.266340255737305
Epoch 1870, val loss: 1.8340494632720947
Epoch 1880, training loss: 6.265412330627441 = 0.006650855299085379 + 1.0 * 6.258761405944824
Epoch 1880, val loss: 1.8367727994918823
Epoch 1890, training loss: 6.26439905166626 = 0.006562370806932449 + 1.0 * 6.257836818695068
Epoch 1890, val loss: 1.8404178619384766
Epoch 1900, training loss: 6.267532825469971 = 0.006475644186139107 + 1.0 * 6.261057376861572
Epoch 1900, val loss: 1.8435184955596924
Epoch 1910, training loss: 6.263032913208008 = 0.006390699185431004 + 1.0 * 6.2566423416137695
Epoch 1910, val loss: 1.8466482162475586
Epoch 1920, training loss: 6.261927604675293 = 0.00630779517814517 + 1.0 * 6.255620002746582
Epoch 1920, val loss: 1.8499853610992432
Epoch 1930, training loss: 6.263358116149902 = 0.006226048804819584 + 1.0 * 6.257132053375244
Epoch 1930, val loss: 1.8529880046844482
Epoch 1940, training loss: 6.2597246170043945 = 0.006145742256194353 + 1.0 * 6.2535786628723145
Epoch 1940, val loss: 1.8558651208877563
Epoch 1950, training loss: 6.266356468200684 = 0.006067175418138504 + 1.0 * 6.260289192199707
Epoch 1950, val loss: 1.858957052230835
Epoch 1960, training loss: 6.262299060821533 = 0.0059903450310230255 + 1.0 * 6.256308555603027
Epoch 1960, val loss: 1.8616658449172974
Epoch 1970, training loss: 6.259252548217773 = 0.005915899761021137 + 1.0 * 6.253336429595947
Epoch 1970, val loss: 1.8647266626358032
Epoch 1980, training loss: 6.259559154510498 = 0.005842349957674742 + 1.0 * 6.253716945648193
Epoch 1980, val loss: 1.8679214715957642
Epoch 1990, training loss: 6.261847496032715 = 0.005770307965576649 + 1.0 * 6.256077289581299
Epoch 1990, val loss: 1.8705493211746216
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 10.54065990447998 = 1.9438446760177612 + 1.0 * 8.59681510925293
Epoch 0, val loss: 1.9424011707305908
Epoch 10, training loss: 10.530716896057129 = 1.9342248439788818 + 1.0 * 8.596491813659668
Epoch 10, val loss: 1.932379126548767
Epoch 20, training loss: 10.516583442687988 = 1.9225550889968872 + 1.0 * 8.59402847290039
Epoch 20, val loss: 1.9202017784118652
Epoch 30, training loss: 10.481236457824707 = 1.906812310218811 + 1.0 * 8.574423789978027
Epoch 30, val loss: 1.9036229848861694
Epoch 40, training loss: 10.3390531539917 = 1.886438250541687 + 1.0 * 8.452614784240723
Epoch 40, val loss: 1.882786512374878
Epoch 50, training loss: 9.752510070800781 = 1.8645274639129639 + 1.0 * 7.887982368469238
Epoch 50, val loss: 1.861324429512024
Epoch 60, training loss: 9.28360366821289 = 1.8459128141403198 + 1.0 * 7.4376912117004395
Epoch 60, val loss: 1.8443267345428467
Epoch 70, training loss: 8.974870681762695 = 1.832611322402954 + 1.0 * 7.142259120941162
Epoch 70, val loss: 1.8314188718795776
Epoch 80, training loss: 8.814604759216309 = 1.8179011344909668 + 1.0 * 6.996703624725342
Epoch 80, val loss: 1.8176149129867554
Epoch 90, training loss: 8.697698593139648 = 1.8023767471313477 + 1.0 * 6.895321369171143
Epoch 90, val loss: 1.80294668674469
Epoch 100, training loss: 8.599050521850586 = 1.7870471477508545 + 1.0 * 6.8120036125183105
Epoch 100, val loss: 1.7889620065689087
Epoch 110, training loss: 8.517725944519043 = 1.7732508182525635 + 1.0 * 6.744475364685059
Epoch 110, val loss: 1.776497721672058
Epoch 120, training loss: 8.454099655151367 = 1.7595789432525635 + 1.0 * 6.694520950317383
Epoch 120, val loss: 1.7642126083374023
Epoch 130, training loss: 8.395676612854004 = 1.744439959526062 + 1.0 * 6.6512370109558105
Epoch 130, val loss: 1.7509747743606567
Epoch 140, training loss: 8.345928192138672 = 1.7271912097930908 + 1.0 * 6.61873722076416
Epoch 140, val loss: 1.7358272075653076
Epoch 150, training loss: 8.298516273498535 = 1.7075282335281372 + 1.0 * 6.590987682342529
Epoch 150, val loss: 1.7186005115509033
Epoch 160, training loss: 8.26054573059082 = 1.6846524477005005 + 1.0 * 6.575893402099609
Epoch 160, val loss: 1.6988520622253418
Epoch 170, training loss: 8.21068000793457 = 1.6581611633300781 + 1.0 * 6.552518367767334
Epoch 170, val loss: 1.6758637428283691
Epoch 180, training loss: 8.162151336669922 = 1.627131462097168 + 1.0 * 6.535019874572754
Epoch 180, val loss: 1.6492266654968262
Epoch 190, training loss: 8.11097526550293 = 1.5908266305923462 + 1.0 * 6.520148277282715
Epoch 190, val loss: 1.6179715394973755
Epoch 200, training loss: 8.054704666137695 = 1.5482313632965088 + 1.0 * 6.506473064422607
Epoch 200, val loss: 1.5814213752746582
Epoch 210, training loss: 7.998428821563721 = 1.4989819526672363 + 1.0 * 6.499446868896484
Epoch 210, val loss: 1.5392394065856934
Epoch 220, training loss: 7.929123401641846 = 1.4448133707046509 + 1.0 * 6.484310150146484
Epoch 220, val loss: 1.4933029413223267
Epoch 230, training loss: 7.858744144439697 = 1.385972499847412 + 1.0 * 6.472771644592285
Epoch 230, val loss: 1.4437114000320435
Epoch 240, training loss: 7.786296844482422 = 1.3236470222473145 + 1.0 * 6.462649822235107
Epoch 240, val loss: 1.3917125463485718
Epoch 250, training loss: 7.7170915603637695 = 1.2597168684005737 + 1.0 * 6.457374572753906
Epoch 250, val loss: 1.33900785446167
Epoch 260, training loss: 7.64729642868042 = 1.1978983879089355 + 1.0 * 6.449398040771484
Epoch 260, val loss: 1.2891747951507568
Epoch 270, training loss: 7.578631401062012 = 1.1393930912017822 + 1.0 * 6.439238548278809
Epoch 270, val loss: 1.2429405450820923
Epoch 280, training loss: 7.516233444213867 = 1.0841630697250366 + 1.0 * 6.432070255279541
Epoch 280, val loss: 1.200226902961731
Epoch 290, training loss: 7.468033313751221 = 1.0329591035842896 + 1.0 * 6.435074329376221
Epoch 290, val loss: 1.1615478992462158
Epoch 300, training loss: 7.408576488494873 = 0.9860596060752869 + 1.0 * 6.422516822814941
Epoch 300, val loss: 1.127596139907837
Epoch 310, training loss: 7.358407497406006 = 0.9421549439430237 + 1.0 * 6.416252613067627
Epoch 310, val loss: 1.0967291593551636
Epoch 320, training loss: 7.310845851898193 = 0.9000900387763977 + 1.0 * 6.410755634307861
Epoch 320, val loss: 1.0680127143859863
Epoch 330, training loss: 7.2774882316589355 = 0.8596336245536804 + 1.0 * 6.4178547859191895
Epoch 330, val loss: 1.04123854637146
Epoch 340, training loss: 7.224489212036133 = 0.8215283155441284 + 1.0 * 6.402960777282715
Epoch 340, val loss: 1.0167568922042847
Epoch 350, training loss: 7.183620452880859 = 0.7848958373069763 + 1.0 * 6.398724555969238
Epoch 350, val loss: 0.994171679019928
Epoch 360, training loss: 7.147071838378906 = 0.7493715882301331 + 1.0 * 6.397700309753418
Epoch 360, val loss: 0.9729096293449402
Epoch 370, training loss: 7.1072282791137695 = 0.7150733470916748 + 1.0 * 6.392155170440674
Epoch 370, val loss: 0.9532889127731323
Epoch 380, training loss: 7.072589874267578 = 0.6819519400596619 + 1.0 * 6.3906378746032715
Epoch 380, val loss: 0.9354218244552612
Epoch 390, training loss: 7.037075042724609 = 0.650023877620697 + 1.0 * 6.387051105499268
Epoch 390, val loss: 0.9191582202911377
Epoch 400, training loss: 7.000796794891357 = 0.6192654371261597 + 1.0 * 6.381531238555908
Epoch 400, val loss: 0.9047622084617615
Epoch 410, training loss: 6.973203182220459 = 0.589554488658905 + 1.0 * 6.383648872375488
Epoch 410, val loss: 0.8920190930366516
Epoch 420, training loss: 6.937831878662109 = 0.561180830001831 + 1.0 * 6.376650810241699
Epoch 420, val loss: 0.8811147212982178
Epoch 430, training loss: 6.91015100479126 = 0.5338757634162903 + 1.0 * 6.376275062561035
Epoch 430, val loss: 0.8720811605453491
Epoch 440, training loss: 6.881173133850098 = 0.5077316761016846 + 1.0 * 6.373441219329834
Epoch 440, val loss: 0.8646524548530579
Epoch 450, training loss: 6.8512773513793945 = 0.4828174114227295 + 1.0 * 6.368460178375244
Epoch 450, val loss: 0.8589476943016052
Epoch 460, training loss: 6.824285984039307 = 0.45894739031791687 + 1.0 * 6.3653388023376465
Epoch 460, val loss: 0.854799747467041
Epoch 470, training loss: 6.801811695098877 = 0.43603387475013733 + 1.0 * 6.365777969360352
Epoch 470, val loss: 0.85196453332901
Epoch 480, training loss: 6.782414436340332 = 0.41411784291267395 + 1.0 * 6.3682966232299805
Epoch 480, val loss: 0.8501842617988586
Epoch 490, training loss: 6.754076957702637 = 0.3932732343673706 + 1.0 * 6.360803604125977
Epoch 490, val loss: 0.8499256372451782
Epoch 500, training loss: 6.732601165771484 = 0.3732908070087433 + 1.0 * 6.359310150146484
Epoch 500, val loss: 0.8504698872566223
Epoch 510, training loss: 6.708754062652588 = 0.35407859086990356 + 1.0 * 6.35467529296875
Epoch 510, val loss: 0.8518051505088806
Epoch 520, training loss: 6.6877946853637695 = 0.3355962932109833 + 1.0 * 6.352198600769043
Epoch 520, val loss: 0.8541075587272644
Epoch 530, training loss: 6.669441223144531 = 0.3177953064441681 + 1.0 * 6.3516459465026855
Epoch 530, val loss: 0.8568891882896423
Epoch 540, training loss: 6.652036190032959 = 0.3007882237434387 + 1.0 * 6.351247787475586
Epoch 540, val loss: 0.8603859543800354
Epoch 550, training loss: 6.630346775054932 = 0.28454869985580444 + 1.0 * 6.345798015594482
Epoch 550, val loss: 0.8644980788230896
Epoch 560, training loss: 6.614042282104492 = 0.2690121531486511 + 1.0 * 6.345030307769775
Epoch 560, val loss: 0.8690778017044067
Epoch 570, training loss: 6.598779678344727 = 0.2542097568511963 + 1.0 * 6.344570159912109
Epoch 570, val loss: 0.8740426898002625
Epoch 580, training loss: 6.583076477050781 = 0.2401990294456482 + 1.0 * 6.342877388000488
Epoch 580, val loss: 0.8796314001083374
Epoch 590, training loss: 6.570450305938721 = 0.22696466743946075 + 1.0 * 6.3434858322143555
Epoch 590, val loss: 0.8852482438087463
Epoch 600, training loss: 6.5525031089782715 = 0.2145722210407257 + 1.0 * 6.337930679321289
Epoch 600, val loss: 0.891380250453949
Epoch 610, training loss: 6.538435935974121 = 0.2029051035642624 + 1.0 * 6.335530757904053
Epoch 610, val loss: 0.8977997899055481
Epoch 620, training loss: 6.525653839111328 = 0.19188596308231354 + 1.0 * 6.333767890930176
Epoch 620, val loss: 0.904411256313324
Epoch 630, training loss: 6.526564121246338 = 0.1815139800310135 + 1.0 * 6.34505033493042
Epoch 630, val loss: 0.9113062024116516
Epoch 640, training loss: 6.504193305969238 = 0.17178133130073547 + 1.0 * 6.332411766052246
Epoch 640, val loss: 0.9182283282279968
Epoch 650, training loss: 6.494019031524658 = 0.1626615971326828 + 1.0 * 6.331357479095459
Epoch 650, val loss: 0.9254700541496277
Epoch 660, training loss: 6.484392166137695 = 0.15408408641815186 + 1.0 * 6.330307960510254
Epoch 660, val loss: 0.9328164458274841
Epoch 670, training loss: 6.479869365692139 = 0.1460309773683548 + 1.0 * 6.33383846282959
Epoch 670, val loss: 0.9401715397834778
Epoch 680, training loss: 6.466542720794678 = 0.13846968114376068 + 1.0 * 6.328073024749756
Epoch 680, val loss: 0.9474629759788513
Epoch 690, training loss: 6.456332206726074 = 0.1313866525888443 + 1.0 * 6.324945449829102
Epoch 690, val loss: 0.95506352186203
Epoch 700, training loss: 6.447413444519043 = 0.12468641996383667 + 1.0 * 6.322727203369141
Epoch 700, val loss: 0.9625918865203857
Epoch 710, training loss: 6.46013879776001 = 0.11838782578706741 + 1.0 * 6.3417510986328125
Epoch 710, val loss: 0.9701820611953735
Epoch 720, training loss: 6.436463356018066 = 0.11248403787612915 + 1.0 * 6.323979377746582
Epoch 720, val loss: 0.9774841070175171
Epoch 730, training loss: 6.426759243011475 = 0.10695047676563263 + 1.0 * 6.3198089599609375
Epoch 730, val loss: 0.9852421283721924
Epoch 740, training loss: 6.419101715087891 = 0.10171554982662201 + 1.0 * 6.317386150360107
Epoch 740, val loss: 0.9927253723144531
Epoch 750, training loss: 6.420781135559082 = 0.09677481651306152 + 1.0 * 6.324006080627441
Epoch 750, val loss: 1.0004355907440186
Epoch 760, training loss: 6.416067600250244 = 0.0921243354678154 + 1.0 * 6.323943138122559
Epoch 760, val loss: 1.0075995922088623
Epoch 770, training loss: 6.4017815589904785 = 0.08777577430009842 + 1.0 * 6.3140058517456055
Epoch 770, val loss: 1.0153896808624268
Epoch 780, training loss: 6.397158622741699 = 0.08366341143846512 + 1.0 * 6.31349515914917
Epoch 780, val loss: 1.023051142692566
Epoch 790, training loss: 6.3971991539001465 = 0.07977350056171417 + 1.0 * 6.317425727844238
Epoch 790, val loss: 1.030694603919983
Epoch 800, training loss: 6.394150733947754 = 0.0761074498295784 + 1.0 * 6.318043231964111
Epoch 800, val loss: 1.0380761623382568
Epoch 810, training loss: 6.383677005767822 = 0.07266416400671005 + 1.0 * 6.3110127449035645
Epoch 810, val loss: 1.0457812547683716
Epoch 820, training loss: 6.377532958984375 = 0.06940660625696182 + 1.0 * 6.308126449584961
Epoch 820, val loss: 1.053346872329712
Epoch 830, training loss: 6.373785495758057 = 0.0663185864686966 + 1.0 * 6.307466983795166
Epoch 830, val loss: 1.0609259605407715
Epoch 840, training loss: 6.386938571929932 = 0.06340628117322922 + 1.0 * 6.3235321044921875
Epoch 840, val loss: 1.0683866739273071
Epoch 850, training loss: 6.366827964782715 = 0.060663994401693344 + 1.0 * 6.306163787841797
Epoch 850, val loss: 1.0757883787155151
Epoch 860, training loss: 6.36427116394043 = 0.058080676943063736 + 1.0 * 6.306190490722656
Epoch 860, val loss: 1.083406925201416
Epoch 870, training loss: 6.359668731689453 = 0.0556320920586586 + 1.0 * 6.304036617279053
Epoch 870, val loss: 1.0907034873962402
Epoch 880, training loss: 6.367433547973633 = 0.05331096798181534 + 1.0 * 6.314122676849365
Epoch 880, val loss: 1.0979118347167969
Epoch 890, training loss: 6.356640815734863 = 0.05113539099693298 + 1.0 * 6.305505275726318
Epoch 890, val loss: 1.1054418087005615
Epoch 900, training loss: 6.356019973754883 = 0.04906387999653816 + 1.0 * 6.3069562911987305
Epoch 900, val loss: 1.1127086877822876
Epoch 910, training loss: 6.348062992095947 = 0.04711424931883812 + 1.0 * 6.300948619842529
Epoch 910, val loss: 1.1198205947875977
Epoch 920, training loss: 6.345461368560791 = 0.04526720196008682 + 1.0 * 6.300194263458252
Epoch 920, val loss: 1.12710440158844
Epoch 930, training loss: 6.342074394226074 = 0.04351607337594032 + 1.0 * 6.298558235168457
Epoch 930, val loss: 1.1343390941619873
Epoch 940, training loss: 6.350185394287109 = 0.04185456410050392 + 1.0 * 6.30833101272583
Epoch 940, val loss: 1.1413605213165283
Epoch 950, training loss: 6.34475040435791 = 0.040277816355228424 + 1.0 * 6.30447244644165
Epoch 950, val loss: 1.14794921875
Epoch 960, training loss: 6.337439060211182 = 0.03879790008068085 + 1.0 * 6.298641204833984
Epoch 960, val loss: 1.1550384759902954
Epoch 970, training loss: 6.333281517028809 = 0.03739241510629654 + 1.0 * 6.295888900756836
Epoch 970, val loss: 1.1619044542312622
Epoch 980, training loss: 6.331042766571045 = 0.03605157509446144 + 1.0 * 6.2949910163879395
Epoch 980, val loss: 1.1687066555023193
Epoch 990, training loss: 6.335427284240723 = 0.034779489040374756 + 1.0 * 6.300647735595703
Epoch 990, val loss: 1.1753469705581665
Epoch 1000, training loss: 6.333556652069092 = 0.033571578562259674 + 1.0 * 6.299984931945801
Epoch 1000, val loss: 1.1819357872009277
Epoch 1010, training loss: 6.3281779289245605 = 0.032424572855234146 + 1.0 * 6.295753479003906
Epoch 1010, val loss: 1.1885058879852295
Epoch 1020, training loss: 6.325541973114014 = 0.03133738785982132 + 1.0 * 6.2942047119140625
Epoch 1020, val loss: 1.1948354244232178
Epoch 1030, training loss: 6.3220648765563965 = 0.03030034899711609 + 1.0 * 6.291764736175537
Epoch 1030, val loss: 1.2013800144195557
Epoch 1040, training loss: 6.319814205169678 = 0.02931077964603901 + 1.0 * 6.29050350189209
Epoch 1040, val loss: 1.207695484161377
Epoch 1050, training loss: 6.326187610626221 = 0.02836708351969719 + 1.0 * 6.297820568084717
Epoch 1050, val loss: 1.213929533958435
Epoch 1060, training loss: 6.3222479820251465 = 0.02747213840484619 + 1.0 * 6.29477596282959
Epoch 1060, val loss: 1.2198600769042969
Epoch 1070, training loss: 6.318921089172363 = 0.026621883735060692 + 1.0 * 6.292299270629883
Epoch 1070, val loss: 1.226292610168457
Epoch 1080, training loss: 6.316647529602051 = 0.02581041492521763 + 1.0 * 6.290837287902832
Epoch 1080, val loss: 1.2320536375045776
Epoch 1090, training loss: 6.313508033752441 = 0.025032449513673782 + 1.0 * 6.288475513458252
Epoch 1090, val loss: 1.237951397895813
Epoch 1100, training loss: 6.311638832092285 = 0.02429102547466755 + 1.0 * 6.287347793579102
Epoch 1100, val loss: 1.2440072298049927
Epoch 1110, training loss: 6.319301605224609 = 0.023577436804771423 + 1.0 * 6.295724391937256
Epoch 1110, val loss: 1.2495344877243042
Epoch 1120, training loss: 6.3114190101623535 = 0.02290366217494011 + 1.0 * 6.288515567779541
Epoch 1120, val loss: 1.2553123235702515
Epoch 1130, training loss: 6.309507846832275 = 0.022254392504692078 + 1.0 * 6.287253379821777
Epoch 1130, val loss: 1.26120924949646
Epoch 1140, training loss: 6.307864189147949 = 0.02163243293762207 + 1.0 * 6.286231994628906
Epoch 1140, val loss: 1.266672134399414
Epoch 1150, training loss: 6.306352615356445 = 0.021036038175225258 + 1.0 * 6.285316467285156
Epoch 1150, val loss: 1.272260069847107
Epoch 1160, training loss: 6.308365345001221 = 0.0204634927213192 + 1.0 * 6.287901878356934
Epoch 1160, val loss: 1.2775846719741821
Epoch 1170, training loss: 6.30662202835083 = 0.01991370879113674 + 1.0 * 6.286708354949951
Epoch 1170, val loss: 1.2829622030258179
Epoch 1180, training loss: 6.301522731781006 = 0.019390009343624115 + 1.0 * 6.282132625579834
Epoch 1180, val loss: 1.2884612083435059
Epoch 1190, training loss: 6.300126552581787 = 0.018883923068642616 + 1.0 * 6.281242847442627
Epoch 1190, val loss: 1.2937819957733154
Epoch 1200, training loss: 6.303076267242432 = 0.018396643921732903 + 1.0 * 6.284679412841797
Epoch 1200, val loss: 1.2990213632583618
Epoch 1210, training loss: 6.308823108673096 = 0.017929641529917717 + 1.0 * 6.2908935546875
Epoch 1210, val loss: 1.303948163986206
Epoch 1220, training loss: 6.299130439758301 = 0.0174834243953228 + 1.0 * 6.281647205352783
Epoch 1220, val loss: 1.3088687658309937
Epoch 1230, training loss: 6.297543048858643 = 0.017054900527000427 + 1.0 * 6.280488014221191
Epoch 1230, val loss: 1.3140981197357178
Epoch 1240, training loss: 6.295734882354736 = 0.016641845926642418 + 1.0 * 6.279093265533447
Epoch 1240, val loss: 1.319003939628601
Epoch 1250, training loss: 6.298705101013184 = 0.01624041423201561 + 1.0 * 6.282464504241943
Epoch 1250, val loss: 1.3236682415008545
Epoch 1260, training loss: 6.29536247253418 = 0.015856625512242317 + 1.0 * 6.279505729675293
Epoch 1260, val loss: 1.3284112215042114
Epoch 1270, training loss: 6.29602575302124 = 0.015488464385271072 + 1.0 * 6.280537128448486
Epoch 1270, val loss: 1.3332138061523438
Epoch 1280, training loss: 6.294261455535889 = 0.015134943649172783 + 1.0 * 6.2791266441345215
Epoch 1280, val loss: 1.3381385803222656
Epoch 1290, training loss: 6.291104793548584 = 0.014790481887757778 + 1.0 * 6.2763142585754395
Epoch 1290, val loss: 1.3427259922027588
Epoch 1300, training loss: 6.291733741760254 = 0.014457673765718937 + 1.0 * 6.277276039123535
Epoch 1300, val loss: 1.3473658561706543
Epoch 1310, training loss: 6.295522212982178 = 0.014136657118797302 + 1.0 * 6.28138542175293
Epoch 1310, val loss: 1.3517255783081055
Epoch 1320, training loss: 6.293695449829102 = 0.013827523216605186 + 1.0 * 6.279868125915527
Epoch 1320, val loss: 1.3561495542526245
Epoch 1330, training loss: 6.291686058044434 = 0.013529821299016476 + 1.0 * 6.278156280517578
Epoch 1330, val loss: 1.3606692552566528
Epoch 1340, training loss: 6.287330150604248 = 0.013242765329778194 + 1.0 * 6.274087429046631
Epoch 1340, val loss: 1.3652260303497314
Epoch 1350, training loss: 6.290712356567383 = 0.012963416054844856 + 1.0 * 6.277749061584473
Epoch 1350, val loss: 1.3694751262664795
Epoch 1360, training loss: 6.2865118980407715 = 0.012694127857685089 + 1.0 * 6.273817539215088
Epoch 1360, val loss: 1.373382568359375
Epoch 1370, training loss: 6.28553581237793 = 0.012434314005076885 + 1.0 * 6.273101329803467
Epoch 1370, val loss: 1.377903938293457
Epoch 1380, training loss: 6.284839630126953 = 0.012181502766907215 + 1.0 * 6.272658348083496
Epoch 1380, val loss: 1.3822898864746094
Epoch 1390, training loss: 6.285800457000732 = 0.011935343965888023 + 1.0 * 6.273865222930908
Epoch 1390, val loss: 1.3863333463668823
Epoch 1400, training loss: 6.287984848022461 = 0.011697329580783844 + 1.0 * 6.27628755569458
Epoch 1400, val loss: 1.3902349472045898
Epoch 1410, training loss: 6.2937445640563965 = 0.011468466371297836 + 1.0 * 6.282276153564453
Epoch 1410, val loss: 1.3939157724380493
Epoch 1420, training loss: 6.284059047698975 = 0.011251814663410187 + 1.0 * 6.2728071212768555
Epoch 1420, val loss: 1.3982540369033813
Epoch 1430, training loss: 6.2824273109436035 = 0.011037097312510014 + 1.0 * 6.271390438079834
Epoch 1430, val loss: 1.4024977684020996
Epoch 1440, training loss: 6.280548572540283 = 0.010828579775989056 + 1.0 * 6.269720077514648
Epoch 1440, val loss: 1.4062669277191162
Epoch 1450, training loss: 6.279466152191162 = 0.010624411515891552 + 1.0 * 6.268841743469238
Epoch 1450, val loss: 1.4101053476333618
Epoch 1460, training loss: 6.279324054718018 = 0.010425481013953686 + 1.0 * 6.2688984870910645
Epoch 1460, val loss: 1.4141045808792114
Epoch 1470, training loss: 6.299581050872803 = 0.010234418325126171 + 1.0 * 6.289346694946289
Epoch 1470, val loss: 1.417632818222046
Epoch 1480, training loss: 6.278829574584961 = 0.010051114484667778 + 1.0 * 6.268778324127197
Epoch 1480, val loss: 1.4211803674697876
Epoch 1490, training loss: 6.2791314125061035 = 0.009872744791209698 + 1.0 * 6.269258499145508
Epoch 1490, val loss: 1.4254286289215088
Epoch 1500, training loss: 6.2767109870910645 = 0.009698075242340565 + 1.0 * 6.267013072967529
Epoch 1500, val loss: 1.4291212558746338
Epoch 1510, training loss: 6.2764892578125 = 0.00952734611928463 + 1.0 * 6.266962051391602
Epoch 1510, val loss: 1.4325960874557495
Epoch 1520, training loss: 6.292465686798096 = 0.009362393990159035 + 1.0 * 6.2831034660339355
Epoch 1520, val loss: 1.4361379146575928
Epoch 1530, training loss: 6.285254001617432 = 0.009201627224683762 + 1.0 * 6.276052474975586
Epoch 1530, val loss: 1.4393104314804077
Epoch 1540, training loss: 6.275758266448975 = 0.009046996012330055 + 1.0 * 6.266711235046387
Epoch 1540, val loss: 1.4433021545410156
Epoch 1550, training loss: 6.277139186859131 = 0.008895590901374817 + 1.0 * 6.268243789672852
Epoch 1550, val loss: 1.4468958377838135
Epoch 1560, training loss: 6.277280807495117 = 0.00874814298003912 + 1.0 * 6.268532752990723
Epoch 1560, val loss: 1.4500917196273804
Epoch 1570, training loss: 6.27569580078125 = 0.008604366332292557 + 1.0 * 6.267091274261475
Epoch 1570, val loss: 1.453437328338623
Epoch 1580, training loss: 6.272387504577637 = 0.008463324047625065 + 1.0 * 6.2639241218566895
Epoch 1580, val loss: 1.4569886922836304
Epoch 1590, training loss: 6.2796454429626465 = 0.008325433358550072 + 1.0 * 6.27131986618042
Epoch 1590, val loss: 1.4603933095932007
Epoch 1600, training loss: 6.27868127822876 = 0.008194229565560818 + 1.0 * 6.270486831665039
Epoch 1600, val loss: 1.4633342027664185
Epoch 1610, training loss: 6.274234771728516 = 0.00806522835046053 + 1.0 * 6.266169548034668
Epoch 1610, val loss: 1.4667589664459229
Epoch 1620, training loss: 6.270755767822266 = 0.007939430885016918 + 1.0 * 6.262816429138184
Epoch 1620, val loss: 1.4702037572860718
Epoch 1630, training loss: 6.273398399353027 = 0.007815351709723473 + 1.0 * 6.265583038330078
Epoch 1630, val loss: 1.473232626914978
Epoch 1640, training loss: 6.273168563842773 = 0.007695432286709547 + 1.0 * 6.265473365783691
Epoch 1640, val loss: 1.4760318994522095
Epoch 1650, training loss: 6.2751641273498535 = 0.007580118253827095 + 1.0 * 6.267583847045898
Epoch 1650, val loss: 1.4794902801513672
Epoch 1660, training loss: 6.272299766540527 = 0.007465529255568981 + 1.0 * 6.264834403991699
Epoch 1660, val loss: 1.4824893474578857
Epoch 1670, training loss: 6.270087242126465 = 0.007355164270848036 + 1.0 * 6.262732028961182
Epoch 1670, val loss: 1.4857523441314697
Epoch 1680, training loss: 6.269501209259033 = 0.007245826534926891 + 1.0 * 6.2622551918029785
Epoch 1680, val loss: 1.4888039827346802
Epoch 1690, training loss: 6.276017665863037 = 0.007139165420085192 + 1.0 * 6.26887845993042
Epoch 1690, val loss: 1.4916913509368896
Epoch 1700, training loss: 6.271885871887207 = 0.007035744376480579 + 1.0 * 6.26485013961792
Epoch 1700, val loss: 1.4945158958435059
Epoch 1710, training loss: 6.276424884796143 = 0.006935539655387402 + 1.0 * 6.269489288330078
Epoch 1710, val loss: 1.4976493120193481
Epoch 1720, training loss: 6.26820182800293 = 0.006837761495262384 + 1.0 * 6.261363983154297
Epoch 1720, val loss: 1.5006053447723389
Epoch 1730, training loss: 6.265946865081787 = 0.006741270422935486 + 1.0 * 6.2592058181762695
Epoch 1730, val loss: 1.5036128759384155
Epoch 1740, training loss: 6.266313552856445 = 0.006646453868597746 + 1.0 * 6.259666919708252
Epoch 1740, val loss: 1.5064656734466553
Epoch 1750, training loss: 6.276735782623291 = 0.006554268766194582 + 1.0 * 6.270181655883789
Epoch 1750, val loss: 1.5092014074325562
Epoch 1760, training loss: 6.270864963531494 = 0.006464751437306404 + 1.0 * 6.264400005340576
Epoch 1760, val loss: 1.511914849281311
Epoch 1770, training loss: 6.269024848937988 = 0.00637799734249711 + 1.0 * 6.262646675109863
Epoch 1770, val loss: 1.5149672031402588
Epoch 1780, training loss: 6.2708587646484375 = 0.006292630918323994 + 1.0 * 6.264565944671631
Epoch 1780, val loss: 1.5177580118179321
Epoch 1790, training loss: 6.263704776763916 = 0.006208604667335749 + 1.0 * 6.257496356964111
Epoch 1790, val loss: 1.5202401876449585
Epoch 1800, training loss: 6.265822410583496 = 0.006126127205789089 + 1.0 * 6.2596964836120605
Epoch 1800, val loss: 1.5230425596237183
Epoch 1810, training loss: 6.269284248352051 = 0.006045411806553602 + 1.0 * 6.263238906860352
Epoch 1810, val loss: 1.5255120992660522
Epoch 1820, training loss: 6.264524936676025 = 0.005967868957668543 + 1.0 * 6.258556842803955
Epoch 1820, val loss: 1.5282422304153442
Epoch 1830, training loss: 6.262842178344727 = 0.005890882108360529 + 1.0 * 6.256951332092285
Epoch 1830, val loss: 1.5311777591705322
Epoch 1840, training loss: 6.266586780548096 = 0.005814957432448864 + 1.0 * 6.260771751403809
Epoch 1840, val loss: 1.5337885618209839
Epoch 1850, training loss: 6.267706394195557 = 0.005741832312196493 + 1.0 * 6.261964797973633
Epoch 1850, val loss: 1.5360561609268188
Epoch 1860, training loss: 6.262694358825684 = 0.005669786594808102 + 1.0 * 6.257024765014648
Epoch 1860, val loss: 1.5385551452636719
Epoch 1870, training loss: 6.262537956237793 = 0.005599614232778549 + 1.0 * 6.256938457489014
Epoch 1870, val loss: 1.5414552688598633
Epoch 1880, training loss: 6.2650909423828125 = 0.005530176684260368 + 1.0 * 6.259560585021973
Epoch 1880, val loss: 1.5438791513442993
Epoch 1890, training loss: 6.263535976409912 = 0.005462599452584982 + 1.0 * 6.258073329925537
Epoch 1890, val loss: 1.5460904836654663
Epoch 1900, training loss: 6.262328624725342 = 0.005396686494350433 + 1.0 * 6.256931781768799
Epoch 1900, val loss: 1.54874587059021
Epoch 1910, training loss: 6.2633376121521 = 0.005331832449883223 + 1.0 * 6.258005619049072
Epoch 1910, val loss: 1.5514719486236572
Epoch 1920, training loss: 6.263906002044678 = 0.005268424283713102 + 1.0 * 6.258637428283691
Epoch 1920, val loss: 1.5534201860427856
Epoch 1930, training loss: 6.260321617126465 = 0.005206128116697073 + 1.0 * 6.255115509033203
Epoch 1930, val loss: 1.555906057357788
Epoch 1940, training loss: 6.258867263793945 = 0.005145419854670763 + 1.0 * 6.253721714019775
Epoch 1940, val loss: 1.558569312095642
Epoch 1950, training loss: 6.259716987609863 = 0.005084707401692867 + 1.0 * 6.254632472991943
Epoch 1950, val loss: 1.5608621835708618
Epoch 1960, training loss: 6.267608642578125 = 0.0050256866961717606 + 1.0 * 6.262582778930664
Epoch 1960, val loss: 1.5628654956817627
Epoch 1970, training loss: 6.26118803024292 = 0.00496825622394681 + 1.0 * 6.256219863891602
Epoch 1970, val loss: 1.5652282238006592
Epoch 1980, training loss: 6.258965015411377 = 0.004911958705633879 + 1.0 * 6.254053115844727
Epoch 1980, val loss: 1.5678763389587402
Epoch 1990, training loss: 6.266371250152588 = 0.0048563736490905285 + 1.0 * 6.261514663696289
Epoch 1990, val loss: 1.5698870420455933
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8070637849235636
The final CL Acc:0.70000, 0.01839, The final GNN Acc:0.80829, 0.00212
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13242])
remove edge: torch.Size([2, 8042])
updated graph: torch.Size([2, 10728])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.54172420501709 = 1.9448810815811157 + 1.0 * 8.596842765808105
Epoch 0, val loss: 1.9502536058425903
Epoch 10, training loss: 10.532021522521973 = 1.9354591369628906 + 1.0 * 8.596562385559082
Epoch 10, val loss: 1.940388560295105
Epoch 20, training loss: 10.5180025100708 = 1.9235740900039673 + 1.0 * 8.594428062438965
Epoch 20, val loss: 1.927805781364441
Epoch 30, training loss: 10.484426498413086 = 1.9066227674484253 + 1.0 * 8.577803611755371
Epoch 30, val loss: 1.9099265336990356
Epoch 40, training loss: 10.36180305480957 = 1.8833365440368652 + 1.0 * 8.478466033935547
Epoch 40, val loss: 1.8861266374588013
Epoch 50, training loss: 9.848402976989746 = 1.856268286705017 + 1.0 * 7.9921345710754395
Epoch 50, val loss: 1.859694004058838
Epoch 60, training loss: 9.366294860839844 = 1.8326048851013184 + 1.0 * 7.533690452575684
Epoch 60, val loss: 1.8370423316955566
Epoch 70, training loss: 9.022310256958008 = 1.8156945705413818 + 1.0 * 7.206615447998047
Epoch 70, val loss: 1.8194968700408936
Epoch 80, training loss: 8.865955352783203 = 1.7980116605758667 + 1.0 * 7.067943572998047
Epoch 80, val loss: 1.8011200428009033
Epoch 90, training loss: 8.720304489135742 = 1.7787208557128906 + 1.0 * 6.941583633422852
Epoch 90, val loss: 1.782011866569519
Epoch 100, training loss: 8.591619491577148 = 1.7612714767456055 + 1.0 * 6.830348491668701
Epoch 100, val loss: 1.7652084827423096
Epoch 110, training loss: 8.51648998260498 = 1.7445989847183228 + 1.0 * 6.771891117095947
Epoch 110, val loss: 1.7489374876022339
Epoch 120, training loss: 8.450702667236328 = 1.7259114980697632 + 1.0 * 6.724791526794434
Epoch 120, val loss: 1.7308794260025024
Epoch 130, training loss: 8.391138076782227 = 1.705068826675415 + 1.0 * 6.686069488525391
Epoch 130, val loss: 1.7116285562515259
Epoch 140, training loss: 8.33391284942627 = 1.6815911531448364 + 1.0 * 6.652321815490723
Epoch 140, val loss: 1.6908036470413208
Epoch 150, training loss: 8.276254653930664 = 1.653982162475586 + 1.0 * 6.62227201461792
Epoch 150, val loss: 1.6670037508010864
Epoch 160, training loss: 8.215742111206055 = 1.6211602687835693 + 1.0 * 6.5945820808410645
Epoch 160, val loss: 1.6388708353042603
Epoch 170, training loss: 8.154315948486328 = 1.582242727279663 + 1.0 * 6.572072982788086
Epoch 170, val loss: 1.6054532527923584
Epoch 180, training loss: 8.088812828063965 = 1.537196397781372 + 1.0 * 6.551616191864014
Epoch 180, val loss: 1.5669666528701782
Epoch 190, training loss: 8.017156600952148 = 1.4857584238052368 + 1.0 * 6.531398296356201
Epoch 190, val loss: 1.52326238155365
Epoch 200, training loss: 7.945104122161865 = 1.428232192993164 + 1.0 * 6.516871929168701
Epoch 200, val loss: 1.4747669696807861
Epoch 210, training loss: 7.866090297698975 = 1.3680797815322876 + 1.0 * 6.498010635375977
Epoch 210, val loss: 1.4247381687164307
Epoch 220, training loss: 7.789220809936523 = 1.3065903186798096 + 1.0 * 6.482630252838135
Epoch 220, val loss: 1.3740237951278687
Epoch 230, training loss: 7.718534469604492 = 1.2453067302703857 + 1.0 * 6.473227500915527
Epoch 230, val loss: 1.3243508338928223
Epoch 240, training loss: 7.648296356201172 = 1.1866929531097412 + 1.0 * 6.461603164672852
Epoch 240, val loss: 1.2774145603179932
Epoch 250, training loss: 7.583716869354248 = 1.131316065788269 + 1.0 * 6.4524006843566895
Epoch 250, val loss: 1.233588695526123
Epoch 260, training loss: 7.524038314819336 = 1.080622911453247 + 1.0 * 6.443415641784668
Epoch 260, val loss: 1.1941498517990112
Epoch 270, training loss: 7.469761848449707 = 1.0343091487884521 + 1.0 * 6.435452461242676
Epoch 270, val loss: 1.1586517095565796
Epoch 280, training loss: 7.425481796264648 = 0.9917834997177124 + 1.0 * 6.4336981773376465
Epoch 280, val loss: 1.1265709400177002
Epoch 290, training loss: 7.376796722412109 = 0.9530711770057678 + 1.0 * 6.423725605010986
Epoch 290, val loss: 1.0975464582443237
Epoch 300, training loss: 7.333810806274414 = 0.9170078635215759 + 1.0 * 6.416802883148193
Epoch 300, val loss: 1.0708630084991455
Epoch 310, training loss: 7.296087265014648 = 0.8829352855682373 + 1.0 * 6.41315221786499
Epoch 310, val loss: 1.0459378957748413
Epoch 320, training loss: 7.259799480438232 = 0.8505507111549377 + 1.0 * 6.4092488288879395
Epoch 320, val loss: 1.0224244594573975
Epoch 330, training loss: 7.2211456298828125 = 0.819061815738678 + 1.0 * 6.402083873748779
Epoch 330, val loss: 0.9996949434280396
Epoch 340, training loss: 7.191748142242432 = 0.7880815267562866 + 1.0 * 6.4036664962768555
Epoch 340, val loss: 0.9774578213691711
Epoch 350, training loss: 7.153368949890137 = 0.7580904364585876 + 1.0 * 6.395278453826904
Epoch 350, val loss: 0.9560486674308777
Epoch 360, training loss: 7.1199541091918945 = 0.7284886837005615 + 1.0 * 6.391465663909912
Epoch 360, val loss: 0.9352820515632629
Epoch 370, training loss: 7.086167812347412 = 0.6991662383079529 + 1.0 * 6.3870015144348145
Epoch 370, val loss: 0.9150418043136597
Epoch 380, training loss: 7.056495189666748 = 0.6701669096946716 + 1.0 * 6.386328220367432
Epoch 380, val loss: 0.8954851031303406
Epoch 390, training loss: 7.022403717041016 = 0.6420745253562927 + 1.0 * 6.380329132080078
Epoch 390, val loss: 0.8770567774772644
Epoch 400, training loss: 6.993931770324707 = 0.6148135662078857 + 1.0 * 6.379117965698242
Epoch 400, val loss: 0.859761655330658
Epoch 410, training loss: 6.96712064743042 = 0.5884951949119568 + 1.0 * 6.378625392913818
Epoch 410, val loss: 0.8437449932098389
Epoch 420, training loss: 6.934477806091309 = 0.5631839036941528 + 1.0 * 6.371294021606445
Epoch 420, val loss: 0.8289626240730286
Epoch 430, training loss: 6.906668663024902 = 0.5386478900909424 + 1.0 * 6.368021011352539
Epoch 430, val loss: 0.815361738204956
Epoch 440, training loss: 6.884045124053955 = 0.514859139919281 + 1.0 * 6.369185924530029
Epoch 440, val loss: 0.8029240965843201
Epoch 450, training loss: 6.856443881988525 = 0.4920651614665985 + 1.0 * 6.364378929138184
Epoch 450, val loss: 0.7917309999465942
Epoch 460, training loss: 6.831378936767578 = 0.47004055976867676 + 1.0 * 6.361338138580322
Epoch 460, val loss: 0.7815228700637817
Epoch 470, training loss: 6.805708885192871 = 0.4485175311565399 + 1.0 * 6.357191562652588
Epoch 470, val loss: 0.7721840143203735
Epoch 480, training loss: 6.783308029174805 = 0.427471786737442 + 1.0 * 6.355836391448975
Epoch 480, val loss: 0.7636156678199768
Epoch 490, training loss: 6.778059959411621 = 0.4070325791835785 + 1.0 * 6.37102746963501
Epoch 490, val loss: 0.7558069229125977
Epoch 500, training loss: 6.740847587585449 = 0.3873610198497772 + 1.0 * 6.35348653793335
Epoch 500, val loss: 0.7490049004554749
Epoch 510, training loss: 6.718210220336914 = 0.368343323469162 + 1.0 * 6.34986686706543
Epoch 510, val loss: 0.7428800463676453
Epoch 520, training loss: 6.696260929107666 = 0.34981483221054077 + 1.0 * 6.3464460372924805
Epoch 520, val loss: 0.7374820113182068
Epoch 530, training loss: 6.682188987731934 = 0.3317986726760864 + 1.0 * 6.350390434265137
Epoch 530, val loss: 0.7327967286109924
Epoch 540, training loss: 6.659383296966553 = 0.31437528133392334 + 1.0 * 6.34500789642334
Epoch 540, val loss: 0.7288193106651306
Epoch 550, training loss: 6.637556552886963 = 0.2973872125148773 + 1.0 * 6.340169429779053
Epoch 550, val loss: 0.7254437208175659
Epoch 560, training loss: 6.629778861999512 = 0.28077593445777893 + 1.0 * 6.349002838134766
Epoch 560, val loss: 0.7226576209068298
Epoch 570, training loss: 6.600715160369873 = 0.26467519998550415 + 1.0 * 6.336040019989014
Epoch 570, val loss: 0.7204923629760742
Epoch 580, training loss: 6.583352088928223 = 0.24907711148262024 + 1.0 * 6.334274768829346
Epoch 580, val loss: 0.7188159227371216
Epoch 590, training loss: 6.5738372802734375 = 0.2339794635772705 + 1.0 * 6.339858055114746
Epoch 590, val loss: 0.7177029252052307
Epoch 600, training loss: 6.556625843048096 = 0.21962566673755646 + 1.0 * 6.337000370025635
Epoch 600, val loss: 0.7172560095787048
Epoch 610, training loss: 6.540968418121338 = 0.20605245232582092 + 1.0 * 6.334916114807129
Epoch 610, val loss: 0.717306911945343
Epoch 620, training loss: 6.521669864654541 = 0.19327321648597717 + 1.0 * 6.328396797180176
Epoch 620, val loss: 0.7179996371269226
Epoch 630, training loss: 6.5118088722229 = 0.18129557371139526 + 1.0 * 6.3305134773254395
Epoch 630, val loss: 0.7192618250846863
Epoch 640, training loss: 6.49717378616333 = 0.170164093375206 + 1.0 * 6.327009677886963
Epoch 640, val loss: 0.7210221886634827
Epoch 650, training loss: 6.487176895141602 = 0.15984250605106354 + 1.0 * 6.327334403991699
Epoch 650, val loss: 0.7232843041419983
Epoch 660, training loss: 6.4784770011901855 = 0.1503196358680725 + 1.0 * 6.328157424926758
Epoch 660, val loss: 0.7260130643844604
Epoch 670, training loss: 6.463287353515625 = 0.1415010392665863 + 1.0 * 6.321786403656006
Epoch 670, val loss: 0.7291325926780701
Epoch 680, training loss: 6.45573091506958 = 0.13332125544548035 + 1.0 * 6.322409629821777
Epoch 680, val loss: 0.73265141248703
Epoch 690, training loss: 6.447447776794434 = 0.12573617696762085 + 1.0 * 6.321711540222168
Epoch 690, val loss: 0.7364988923072815
Epoch 700, training loss: 6.436526298522949 = 0.1187489703297615 + 1.0 * 6.317777156829834
Epoch 700, val loss: 0.7405838966369629
Epoch 710, training loss: 6.428889751434326 = 0.11222675442695618 + 1.0 * 6.316662788391113
Epoch 710, val loss: 0.7449614405632019
Epoch 720, training loss: 6.431876182556152 = 0.1061418354511261 + 1.0 * 6.3257341384887695
Epoch 720, val loss: 0.7495437860488892
Epoch 730, training loss: 6.417218208312988 = 0.10051503777503967 + 1.0 * 6.3167033195495605
Epoch 730, val loss: 0.754124104976654
Epoch 740, training loss: 6.408194541931152 = 0.0952908992767334 + 1.0 * 6.312903881072998
Epoch 740, val loss: 0.7588874101638794
Epoch 750, training loss: 6.401637077331543 = 0.09039613604545593 + 1.0 * 6.311241149902344
Epoch 750, val loss: 0.7637970447540283
Epoch 760, training loss: 6.402133464813232 = 0.08581097424030304 + 1.0 * 6.316322326660156
Epoch 760, val loss: 0.7687651515007019
Epoch 770, training loss: 6.397271156311035 = 0.08154033124446869 + 1.0 * 6.315731048583984
Epoch 770, val loss: 0.7737482786178589
Epoch 780, training loss: 6.388148307800293 = 0.0775505006313324 + 1.0 * 6.310597896575928
Epoch 780, val loss: 0.7787520289421082
Epoch 790, training loss: 6.381633758544922 = 0.07382343709468842 + 1.0 * 6.307810306549072
Epoch 790, val loss: 0.7837904691696167
Epoch 800, training loss: 6.37705659866333 = 0.07033731043338776 + 1.0 * 6.3067193031311035
Epoch 800, val loss: 0.7888435125350952
Epoch 810, training loss: 6.37175989151001 = 0.06705112010240555 + 1.0 * 6.304708957672119
Epoch 810, val loss: 0.7939927577972412
Epoch 820, training loss: 6.377628803253174 = 0.06396754831075668 + 1.0 * 6.313661098480225
Epoch 820, val loss: 0.7991510033607483
Epoch 830, training loss: 6.3684539794921875 = 0.06108222156763077 + 1.0 * 6.307371616363525
Epoch 830, val loss: 0.8042218089103699
Epoch 840, training loss: 6.359569072723389 = 0.05837345868349075 + 1.0 * 6.3011956214904785
Epoch 840, val loss: 0.8093603849411011
Epoch 850, training loss: 6.356754302978516 = 0.055820364505052567 + 1.0 * 6.300933837890625
Epoch 850, val loss: 0.8144950866699219
Epoch 860, training loss: 6.359571933746338 = 0.05341198667883873 + 1.0 * 6.306159973144531
Epoch 860, val loss: 0.8196339011192322
Epoch 870, training loss: 6.351834774017334 = 0.05114787071943283 + 1.0 * 6.300686836242676
Epoch 870, val loss: 0.8247048258781433
Epoch 880, training loss: 6.34845495223999 = 0.04901144281029701 + 1.0 * 6.29944372177124
Epoch 880, val loss: 0.8298162817955017
Epoch 890, training loss: 6.349061489105225 = 0.04699944332242012 + 1.0 * 6.302062034606934
Epoch 890, val loss: 0.8348912000656128
Epoch 900, training loss: 6.343364238739014 = 0.04510524496436119 + 1.0 * 6.2982587814331055
Epoch 900, val loss: 0.8398895263671875
Epoch 910, training loss: 6.339134216308594 = 0.04331318289041519 + 1.0 * 6.295821189880371
Epoch 910, val loss: 0.8449309468269348
Epoch 920, training loss: 6.343317031860352 = 0.041619546711444855 + 1.0 * 6.301697254180908
Epoch 920, val loss: 0.8499401211738586
Epoch 930, training loss: 6.336045742034912 = 0.04002188518643379 + 1.0 * 6.296023845672607
Epoch 930, val loss: 0.8548020124435425
Epoch 940, training loss: 6.331488609313965 = 0.03850812092423439 + 1.0 * 6.292980670928955
Epoch 940, val loss: 0.8597233295440674
Epoch 950, training loss: 6.3306427001953125 = 0.037074193358421326 + 1.0 * 6.2935686111450195
Epoch 950, val loss: 0.8645690679550171
Epoch 960, training loss: 6.32948637008667 = 0.035714324563741684 + 1.0 * 6.293772220611572
Epoch 960, val loss: 0.8694154620170593
Epoch 970, training loss: 6.328333854675293 = 0.03442604839801788 + 1.0 * 6.293907642364502
Epoch 970, val loss: 0.8741541504859924
Epoch 980, training loss: 6.325082302093506 = 0.03320566564798355 + 1.0 * 6.291876792907715
Epoch 980, val loss: 0.8788613677024841
Epoch 990, training loss: 6.323975086212158 = 0.03205158933997154 + 1.0 * 6.291923522949219
Epoch 990, val loss: 0.8835501074790955
Epoch 1000, training loss: 6.318993091583252 = 0.030950723215937614 + 1.0 * 6.2880425453186035
Epoch 1000, val loss: 0.8882342576980591
Epoch 1010, training loss: 6.318213939666748 = 0.029902417212724686 + 1.0 * 6.28831148147583
Epoch 1010, val loss: 0.8928731679916382
Epoch 1020, training loss: 6.319658279418945 = 0.028902914375066757 + 1.0 * 6.290755271911621
Epoch 1020, val loss: 0.8974466323852539
Epoch 1030, training loss: 6.31899356842041 = 0.02795564942061901 + 1.0 * 6.2910380363464355
Epoch 1030, val loss: 0.9019054174423218
Epoch 1040, training loss: 6.312758922576904 = 0.02705817110836506 + 1.0 * 6.285700798034668
Epoch 1040, val loss: 0.9063584208488464
Epoch 1050, training loss: 6.310675144195557 = 0.0261999499052763 + 1.0 * 6.284475326538086
Epoch 1050, val loss: 0.9108151197433472
Epoch 1060, training loss: 6.315872669219971 = 0.025380393490195274 + 1.0 * 6.290492057800293
Epoch 1060, val loss: 0.9152390956878662
Epoch 1070, training loss: 6.309834957122803 = 0.024600263684988022 + 1.0 * 6.2852349281311035
Epoch 1070, val loss: 0.9195407629013062
Epoch 1080, training loss: 6.30946683883667 = 0.02385459840297699 + 1.0 * 6.285612106323242
Epoch 1080, val loss: 0.9238194227218628
Epoch 1090, training loss: 6.305944919586182 = 0.023147165775299072 + 1.0 * 6.282797813415527
Epoch 1090, val loss: 0.9280308485031128
Epoch 1100, training loss: 6.303465366363525 = 0.02246979996562004 + 1.0 * 6.2809953689575195
Epoch 1100, val loss: 0.9322657585144043
Epoch 1110, training loss: 6.305344104766846 = 0.021819762885570526 + 1.0 * 6.283524513244629
Epoch 1110, val loss: 0.9364516735076904
Epoch 1120, training loss: 6.301604270935059 = 0.0211966373026371 + 1.0 * 6.280407428741455
Epoch 1120, val loss: 0.94056236743927
Epoch 1130, training loss: 6.301860332489014 = 0.02059999480843544 + 1.0 * 6.2812604904174805
Epoch 1130, val loss: 0.9446682333946228
Epoch 1140, training loss: 6.303687572479248 = 0.020029881969094276 + 1.0 * 6.283657550811768
Epoch 1140, val loss: 0.9486859440803528
Epoch 1150, training loss: 6.301860809326172 = 0.019483454525470734 + 1.0 * 6.282377243041992
Epoch 1150, val loss: 0.9526997804641724
Epoch 1160, training loss: 6.299467086791992 = 0.01895996183156967 + 1.0 * 6.2805070877075195
Epoch 1160, val loss: 0.9565786719322205
Epoch 1170, training loss: 6.297848224639893 = 0.018459802493453026 + 1.0 * 6.279388427734375
Epoch 1170, val loss: 0.960456907749176
Epoch 1180, training loss: 6.29410457611084 = 0.01797965168952942 + 1.0 * 6.276124954223633
Epoch 1180, val loss: 0.9643210172653198
Epoch 1190, training loss: 6.293931007385254 = 0.01751735620200634 + 1.0 * 6.276413440704346
Epoch 1190, val loss: 0.9681560397148132
Epoch 1200, training loss: 6.293300151824951 = 0.017070908099412918 + 1.0 * 6.276229381561279
Epoch 1200, val loss: 0.971951961517334
Epoch 1210, training loss: 6.300682067871094 = 0.016640951856970787 + 1.0 * 6.284040927886963
Epoch 1210, val loss: 0.9756664037704468
Epoch 1220, training loss: 6.295007705688477 = 0.016230111941695213 + 1.0 * 6.278777599334717
Epoch 1220, val loss: 0.9792687296867371
Epoch 1230, training loss: 6.291302680969238 = 0.015837764367461205 + 1.0 * 6.27546501159668
Epoch 1230, val loss: 0.982833206653595
Epoch 1240, training loss: 6.289599418640137 = 0.015461011789739132 + 1.0 * 6.274138450622559
Epoch 1240, val loss: 0.9864581227302551
Epoch 1250, training loss: 6.289076328277588 = 0.015095092356204987 + 1.0 * 6.273981094360352
Epoch 1250, val loss: 0.9900538921356201
Epoch 1260, training loss: 6.295620441436768 = 0.014740300364792347 + 1.0 * 6.280879974365234
Epoch 1260, val loss: 0.9935685992240906
Epoch 1270, training loss: 6.289536952972412 = 0.014403385110199451 + 1.0 * 6.2751336097717285
Epoch 1270, val loss: 0.9969488978385925
Epoch 1280, training loss: 6.285677909851074 = 0.014075017534196377 + 1.0 * 6.271603107452393
Epoch 1280, val loss: 1.0003876686096191
Epoch 1290, training loss: 6.284563064575195 = 0.013758139684796333 + 1.0 * 6.2708048820495605
Epoch 1290, val loss: 1.0038225650787354
Epoch 1300, training loss: 6.292383670806885 = 0.013450478203594685 + 1.0 * 6.278933048248291
Epoch 1300, val loss: 1.0071797370910645
Epoch 1310, training loss: 6.285872936248779 = 0.013155996799468994 + 1.0 * 6.272716999053955
Epoch 1310, val loss: 1.0104395151138306
Epoch 1320, training loss: 6.286805152893066 = 0.01287301629781723 + 1.0 * 6.273931980133057
Epoch 1320, val loss: 1.0136873722076416
Epoch 1330, training loss: 6.283222198486328 = 0.012600380927324295 + 1.0 * 6.2706217765808105
Epoch 1330, val loss: 1.016867995262146
Epoch 1340, training loss: 6.28056001663208 = 0.012335499748587608 + 1.0 * 6.268224716186523
Epoch 1340, val loss: 1.020113468170166
Epoch 1350, training loss: 6.279682636260986 = 0.012078631669282913 + 1.0 * 6.267603874206543
Epoch 1350, val loss: 1.0233083963394165
Epoch 1360, training loss: 6.287399768829346 = 0.011828389950096607 + 1.0 * 6.275571346282959
Epoch 1360, val loss: 1.0264606475830078
Epoch 1370, training loss: 6.281060218811035 = 0.01158917136490345 + 1.0 * 6.269471168518066
Epoch 1370, val loss: 1.0294309854507446
Epoch 1380, training loss: 6.2906293869018555 = 0.011357479728758335 + 1.0 * 6.279272079467773
Epoch 1380, val loss: 1.032472014427185
Epoch 1390, training loss: 6.278746604919434 = 0.0111326789483428 + 1.0 * 6.267613887786865
Epoch 1390, val loss: 1.0354162454605103
Epoch 1400, training loss: 6.2762837409973145 = 0.01091731060296297 + 1.0 * 6.265366554260254
Epoch 1400, val loss: 1.0384364128112793
Epoch 1410, training loss: 6.276341438293457 = 0.010706359520554543 + 1.0 * 6.265635013580322
Epoch 1410, val loss: 1.041443943977356
Epoch 1420, training loss: 6.280458450317383 = 0.010500870645046234 + 1.0 * 6.269957542419434
Epoch 1420, val loss: 1.0443682670593262
Epoch 1430, training loss: 6.276096820831299 = 0.01030102837830782 + 1.0 * 6.265795707702637
Epoch 1430, val loss: 1.0472520589828491
Epoch 1440, training loss: 6.275373458862305 = 0.010107910260558128 + 1.0 * 6.265265464782715
Epoch 1440, val loss: 1.0500901937484741
Epoch 1450, training loss: 6.277698040008545 = 0.00992102362215519 + 1.0 * 6.267776966094971
Epoch 1450, val loss: 1.0529232025146484
Epoch 1460, training loss: 6.276879787445068 = 0.009740983135998249 + 1.0 * 6.267138957977295
Epoch 1460, val loss: 1.0556950569152832
Epoch 1470, training loss: 6.2724432945251465 = 0.009564663283526897 + 1.0 * 6.26287841796875
Epoch 1470, val loss: 1.0584598779678345
Epoch 1480, training loss: 6.2725605964660645 = 0.009393487125635147 + 1.0 * 6.263166904449463
Epoch 1480, val loss: 1.0612542629241943
Epoch 1490, training loss: 6.283852577209473 = 0.009226733818650246 + 1.0 * 6.274625778198242
Epoch 1490, val loss: 1.063927173614502
Epoch 1500, training loss: 6.275558948516846 = 0.009067099541425705 + 1.0 * 6.266491889953613
Epoch 1500, val loss: 1.0665384531021118
Epoch 1510, training loss: 6.271506309509277 = 0.008910451084375381 + 1.0 * 6.2625956535339355
Epoch 1510, val loss: 1.0691272020339966
Epoch 1520, training loss: 6.269558429718018 = 0.008759765885770321 + 1.0 * 6.260798454284668
Epoch 1520, val loss: 1.0717817544937134
Epoch 1530, training loss: 6.269289016723633 = 0.008610477671027184 + 1.0 * 6.260678768157959
Epoch 1530, val loss: 1.074434518814087
Epoch 1540, training loss: 6.285345554351807 = 0.008465883322060108 + 1.0 * 6.276879787445068
Epoch 1540, val loss: 1.0769729614257812
Epoch 1550, training loss: 6.271775245666504 = 0.00832435954362154 + 1.0 * 6.263451099395752
Epoch 1550, val loss: 1.0793886184692383
Epoch 1560, training loss: 6.267422676086426 = 0.008189652115106583 + 1.0 * 6.259232997894287
Epoch 1560, val loss: 1.0818653106689453
Epoch 1570, training loss: 6.270718574523926 = 0.008057192899286747 + 1.0 * 6.262661457061768
Epoch 1570, val loss: 1.084388256072998
Epoch 1580, training loss: 6.269303321838379 = 0.00792775023728609 + 1.0 * 6.261375427246094
Epoch 1580, val loss: 1.0868251323699951
Epoch 1590, training loss: 6.268317699432373 = 0.007801579311490059 + 1.0 * 6.260516166687012
Epoch 1590, val loss: 1.0892311334609985
Epoch 1600, training loss: 6.27058219909668 = 0.007679428905248642 + 1.0 * 6.262902736663818
Epoch 1600, val loss: 1.091628074645996
Epoch 1610, training loss: 6.266569137573242 = 0.007560489233583212 + 1.0 * 6.259008884429932
Epoch 1610, val loss: 1.0939931869506836
Epoch 1620, training loss: 6.26895809173584 = 0.007444535847753286 + 1.0 * 6.261513710021973
Epoch 1620, val loss: 1.0963670015335083
Epoch 1630, training loss: 6.266018390655518 = 0.007330235093832016 + 1.0 * 6.258687973022461
Epoch 1630, val loss: 1.0986875295639038
Epoch 1640, training loss: 6.264689922332764 = 0.0072191935032606125 + 1.0 * 6.257470607757568
Epoch 1640, val loss: 1.1009719371795654
Epoch 1650, training loss: 6.267980575561523 = 0.0071116103790700436 + 1.0 * 6.260869026184082
Epoch 1650, val loss: 1.1032577753067017
Epoch 1660, training loss: 6.265397071838379 = 0.007005920633673668 + 1.0 * 6.258391380310059
Epoch 1660, val loss: 1.105507731437683
Epoch 1670, training loss: 6.263140678405762 = 0.006903418805450201 + 1.0 * 6.256237030029297
Epoch 1670, val loss: 1.1077073812484741
Epoch 1680, training loss: 6.262090682983398 = 0.006803341209888458 + 1.0 * 6.255287170410156
Epoch 1680, val loss: 1.1099456548690796
Epoch 1690, training loss: 6.265407085418701 = 0.006705024745315313 + 1.0 * 6.258702278137207
Epoch 1690, val loss: 1.1122007369995117
Epoch 1700, training loss: 6.261430263519287 = 0.006608836818486452 + 1.0 * 6.254821300506592
Epoch 1700, val loss: 1.1143368482589722
Epoch 1710, training loss: 6.261460304260254 = 0.006515441462397575 + 1.0 * 6.254944801330566
Epoch 1710, val loss: 1.1164449453353882
Epoch 1720, training loss: 6.2681145668029785 = 0.006424887105822563 + 1.0 * 6.26168966293335
Epoch 1720, val loss: 1.118558406829834
Epoch 1730, training loss: 6.265413761138916 = 0.006336119491606951 + 1.0 * 6.259077548980713
Epoch 1730, val loss: 1.1206210851669312
Epoch 1740, training loss: 6.261402606964111 = 0.0062493737787008286 + 1.0 * 6.255153179168701
Epoch 1740, val loss: 1.1226089000701904
Epoch 1750, training loss: 6.259601593017578 = 0.006165705621242523 + 1.0 * 6.253436088562012
Epoch 1750, val loss: 1.124704360961914
Epoch 1760, training loss: 6.258706569671631 = 0.006082141771912575 + 1.0 * 6.25262451171875
Epoch 1760, val loss: 1.1268125772476196
Epoch 1770, training loss: 6.26632833480835 = 0.005999910179525614 + 1.0 * 6.26032829284668
Epoch 1770, val loss: 1.1288784742355347
Epoch 1780, training loss: 6.260396480560303 = 0.00592040503397584 + 1.0 * 6.254476070404053
Epoch 1780, val loss: 1.1307650804519653
Epoch 1790, training loss: 6.26115083694458 = 0.005842584650963545 + 1.0 * 6.255308151245117
Epoch 1790, val loss: 1.132727026939392
Epoch 1800, training loss: 6.2628045082092285 = 0.0057670557871460915 + 1.0 * 6.25703763961792
Epoch 1800, val loss: 1.1346337795257568
Epoch 1810, training loss: 6.260380268096924 = 0.005693844985216856 + 1.0 * 6.25468635559082
Epoch 1810, val loss: 1.1365405321121216
Epoch 1820, training loss: 6.259641647338867 = 0.005621206946671009 + 1.0 * 6.2540202140808105
Epoch 1820, val loss: 1.1384824514389038
Epoch 1830, training loss: 6.256947994232178 = 0.005550432484596968 + 1.0 * 6.251397609710693
Epoch 1830, val loss: 1.140379548072815
Epoch 1840, training loss: 6.257697582244873 = 0.005480630323290825 + 1.0 * 6.252216815948486
Epoch 1840, val loss: 1.1423012018203735
Epoch 1850, training loss: 6.260735988616943 = 0.0054124100133776665 + 1.0 * 6.25532341003418
Epoch 1850, val loss: 1.1441587209701538
Epoch 1860, training loss: 6.255691051483154 = 0.005344822071492672 + 1.0 * 6.2503461837768555
Epoch 1860, val loss: 1.1460072994232178
Epoch 1870, training loss: 6.259612560272217 = 0.005279438570141792 + 1.0 * 6.254333019256592
Epoch 1870, val loss: 1.1478456258773804
Epoch 1880, training loss: 6.257516384124756 = 0.005215748678892851 + 1.0 * 6.25230073928833
Epoch 1880, val loss: 1.1496081352233887
Epoch 1890, training loss: 6.256131649017334 = 0.005152279045432806 + 1.0 * 6.250979423522949
Epoch 1890, val loss: 1.1514028310775757
Epoch 1900, training loss: 6.261232376098633 = 0.005091422237455845 + 1.0 * 6.256141185760498
Epoch 1900, val loss: 1.1531639099121094
Epoch 1910, training loss: 6.254116535186768 = 0.005031278356909752 + 1.0 * 6.249085426330566
Epoch 1910, val loss: 1.1549371480941772
Epoch 1920, training loss: 6.253057956695557 = 0.004972375463694334 + 1.0 * 6.2480854988098145
Epoch 1920, val loss: 1.1567047834396362
Epoch 1930, training loss: 6.259294033050537 = 0.0049141282215714455 + 1.0 * 6.254379749298096
Epoch 1930, val loss: 1.158490777015686
Epoch 1940, training loss: 6.2524871826171875 = 0.004857318475842476 + 1.0 * 6.247629642486572
Epoch 1940, val loss: 1.1601312160491943
Epoch 1950, training loss: 6.251567363739014 = 0.004801778122782707 + 1.0 * 6.246765613555908
Epoch 1950, val loss: 1.1618558168411255
Epoch 1960, training loss: 6.251195430755615 = 0.004747144877910614 + 1.0 * 6.246448516845703
Epoch 1960, val loss: 1.163601279258728
Epoch 1970, training loss: 6.254818439483643 = 0.0046929605305194855 + 1.0 * 6.250125408172607
Epoch 1970, val loss: 1.1653438806533813
Epoch 1980, training loss: 6.255281448364258 = 0.004639425780624151 + 1.0 * 6.250641822814941
Epoch 1980, val loss: 1.1669542789459229
Epoch 1990, training loss: 6.251187801361084 = 0.004588397219777107 + 1.0 * 6.246599197387695
Epoch 1990, val loss: 1.168513536453247
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 10.541169166564941 = 1.944346308708191 + 1.0 * 8.596822738647461
Epoch 0, val loss: 1.9403795003890991
Epoch 10, training loss: 10.531225204467773 = 1.9347102642059326 + 1.0 * 8.596514701843262
Epoch 10, val loss: 1.9305038452148438
Epoch 20, training loss: 10.516560554504395 = 1.9227145910263062 + 1.0 * 8.593846321105957
Epoch 20, val loss: 1.9179176092147827
Epoch 30, training loss: 10.477734565734863 = 1.9058923721313477 + 1.0 * 8.571842193603516
Epoch 30, val loss: 1.9002249240875244
Epoch 40, training loss: 10.335490226745605 = 1.8834563493728638 + 1.0 * 8.452033996582031
Epoch 40, val loss: 1.8779503107070923
Epoch 50, training loss: 9.916206359863281 = 1.8588413000106812 + 1.0 * 8.057365417480469
Epoch 50, val loss: 1.854964017868042
Epoch 60, training loss: 9.539121627807617 = 1.835980772972107 + 1.0 * 7.703141212463379
Epoch 60, val loss: 1.8343172073364258
Epoch 70, training loss: 9.098620414733887 = 1.8199185132980347 + 1.0 * 7.278702259063721
Epoch 70, val loss: 1.8193585872650146
Epoch 80, training loss: 8.836021423339844 = 1.8056352138519287 + 1.0 * 7.030385971069336
Epoch 80, val loss: 1.8057085275650024
Epoch 90, training loss: 8.645503044128418 = 1.790254831314087 + 1.0 * 6.855247974395752
Epoch 90, val loss: 1.791735053062439
Epoch 100, training loss: 8.545146942138672 = 1.7734627723693848 + 1.0 * 6.771684646606445
Epoch 100, val loss: 1.7773443460464478
Epoch 110, training loss: 8.46408462524414 = 1.7556753158569336 + 1.0 * 6.708408832550049
Epoch 110, val loss: 1.7624762058258057
Epoch 120, training loss: 8.399110794067383 = 1.737972617149353 + 1.0 * 6.661138534545898
Epoch 120, val loss: 1.7472718954086304
Epoch 130, training loss: 8.341120719909668 = 1.7191197872161865 + 1.0 * 6.6220011711120605
Epoch 130, val loss: 1.7307227849960327
Epoch 140, training loss: 8.286795616149902 = 1.6972119808197021 + 1.0 * 6.589583873748779
Epoch 140, val loss: 1.7117420434951782
Epoch 150, training loss: 8.233156204223633 = 1.6716452836990356 + 1.0 * 6.561511039733887
Epoch 150, val loss: 1.6901192665100098
Epoch 160, training loss: 8.1795015335083 = 1.6417953968048096 + 1.0 * 6.53770637512207
Epoch 160, val loss: 1.6651326417922974
Epoch 170, training loss: 8.12597370147705 = 1.6066151857376099 + 1.0 * 6.519358158111572
Epoch 170, val loss: 1.6356685161590576
Epoch 180, training loss: 8.07154655456543 = 1.5659089088439941 + 1.0 * 6.505637168884277
Epoch 180, val loss: 1.6019494533538818
Epoch 190, training loss: 8.011306762695312 = 1.520491123199463 + 1.0 * 6.49081563949585
Epoch 190, val loss: 1.5647611618041992
Epoch 200, training loss: 7.952980995178223 = 1.4708895683288574 + 1.0 * 6.482091426849365
Epoch 200, val loss: 1.524817705154419
Epoch 210, training loss: 7.887681007385254 = 1.4188882112503052 + 1.0 * 6.468792915344238
Epoch 210, val loss: 1.4839437007904053
Epoch 220, training loss: 7.8250322341918945 = 1.36503005027771 + 1.0 * 6.460002422332764
Epoch 220, val loss: 1.442312240600586
Epoch 230, training loss: 7.761810779571533 = 1.3103218078613281 + 1.0 * 6.451488971710205
Epoch 230, val loss: 1.4007583856582642
Epoch 240, training loss: 7.702400207519531 = 1.2568519115447998 + 1.0 * 6.445548057556152
Epoch 240, val loss: 1.3610371351242065
Epoch 250, training loss: 7.644139289855957 = 1.205844521522522 + 1.0 * 6.438294887542725
Epoch 250, val loss: 1.3234753608703613
Epoch 260, training loss: 7.588202953338623 = 1.1565409898757935 + 1.0 * 6.431662082672119
Epoch 260, val loss: 1.2873719930648804
Epoch 270, training loss: 7.535042762756348 = 1.1091434955596924 + 1.0 * 6.425899505615234
Epoch 270, val loss: 1.2528828382492065
Epoch 280, training loss: 7.486948013305664 = 1.0646073818206787 + 1.0 * 6.4223408699035645
Epoch 280, val loss: 1.220664620399475
Epoch 290, training loss: 7.437223434448242 = 1.023004174232483 + 1.0 * 6.414219379425049
Epoch 290, val loss: 1.1907991170883179
Epoch 300, training loss: 7.393051624298096 = 0.9838070869445801 + 1.0 * 6.409244537353516
Epoch 300, val loss: 1.1627182960510254
Epoch 310, training loss: 7.354563236236572 = 0.9468145966529846 + 1.0 * 6.407748699188232
Epoch 310, val loss: 1.1362041234970093
Epoch 320, training loss: 7.310671806335449 = 0.9114968776702881 + 1.0 * 6.399174690246582
Epoch 320, val loss: 1.1108481884002686
Epoch 330, training loss: 7.278825759887695 = 0.8768472075462341 + 1.0 * 6.401978492736816
Epoch 330, val loss: 1.0858360528945923
Epoch 340, training loss: 7.236321449279785 = 0.8427539467811584 + 1.0 * 6.3935675621032715
Epoch 340, val loss: 1.0609323978424072
Epoch 350, training loss: 7.195716857910156 = 0.8083532452583313 + 1.0 * 6.387363433837891
Epoch 350, val loss: 1.0357133150100708
Epoch 360, training loss: 7.163079261779785 = 0.7732149362564087 + 1.0 * 6.389864444732666
Epoch 360, val loss: 1.0095958709716797
Epoch 370, training loss: 7.11922550201416 = 0.7375521063804626 + 1.0 * 6.381673336029053
Epoch 370, val loss: 0.9829838275909424
Epoch 380, training loss: 7.0792131423950195 = 0.701419472694397 + 1.0 * 6.377793788909912
Epoch 380, val loss: 0.9561249613761902
Epoch 390, training loss: 7.038693428039551 = 0.6648905873298645 + 1.0 * 6.373802661895752
Epoch 390, val loss: 0.9290257692337036
Epoch 400, training loss: 7.0066399574279785 = 0.6284790635108948 + 1.0 * 6.3781609535217285
Epoch 400, val loss: 0.9023129343986511
Epoch 410, training loss: 6.962973117828369 = 0.5929513573646545 + 1.0 * 6.370021820068359
Epoch 410, val loss: 0.8770184516906738
Epoch 420, training loss: 6.9242262840271 = 0.5584004521369934 + 1.0 * 6.365825653076172
Epoch 420, val loss: 0.8533640503883362
Epoch 430, training loss: 6.888916492462158 = 0.52491694688797 + 1.0 * 6.363999366760254
Epoch 430, val loss: 0.8315066695213318
Epoch 440, training loss: 6.855757713317871 = 0.49281546473503113 + 1.0 * 6.362942218780518
Epoch 440, val loss: 0.8119206428527832
Epoch 450, training loss: 6.830214500427246 = 0.4624595046043396 + 1.0 * 6.367754936218262
Epoch 450, val loss: 0.7948293089866638
Epoch 460, training loss: 6.792327880859375 = 0.43401601910591125 + 1.0 * 6.358311653137207
Epoch 460, val loss: 0.7802773714065552
Epoch 470, training loss: 6.762358665466309 = 0.407021701335907 + 1.0 * 6.355337142944336
Epoch 470, val loss: 0.767789363861084
Epoch 480, training loss: 6.733155250549316 = 0.3813435733318329 + 1.0 * 6.35181188583374
Epoch 480, val loss: 0.7571667432785034
Epoch 490, training loss: 6.711395263671875 = 0.3569376468658447 + 1.0 * 6.354457378387451
Epoch 490, val loss: 0.7483561635017395
Epoch 500, training loss: 6.684715270996094 = 0.33422988653182983 + 1.0 * 6.350485324859619
Epoch 500, val loss: 0.7410926222801208
Epoch 510, training loss: 6.662507057189941 = 0.31301149725914 + 1.0 * 6.3494954109191895
Epoch 510, val loss: 0.7353962659835815
Epoch 520, training loss: 6.637147903442383 = 0.2930619418621063 + 1.0 * 6.344086170196533
Epoch 520, val loss: 0.7308273911476135
Epoch 530, training loss: 6.616325378417969 = 0.274260938167572 + 1.0 * 6.342064380645752
Epoch 530, val loss: 0.7273715734481812
Epoch 540, training loss: 6.60109806060791 = 0.25653886795043945 + 1.0 * 6.344559192657471
Epoch 540, val loss: 0.725001871585846
Epoch 550, training loss: 6.5867838859558105 = 0.2400164157152176 + 1.0 * 6.346767425537109
Epoch 550, val loss: 0.7234641313552856
Epoch 560, training loss: 6.56191349029541 = 0.22468960285186768 + 1.0 * 6.337224006652832
Epoch 560, val loss: 0.7228192687034607
Epoch 570, training loss: 6.546052932739258 = 0.2103433907032013 + 1.0 * 6.335709571838379
Epoch 570, val loss: 0.7228540778160095
Epoch 580, training loss: 6.5395050048828125 = 0.19694741070270538 + 1.0 * 6.342557430267334
Epoch 580, val loss: 0.7235007882118225
Epoch 590, training loss: 6.521880626678467 = 0.18446265161037445 + 1.0 * 6.337418079376221
Epoch 590, val loss: 0.72479248046875
Epoch 600, training loss: 6.505305290222168 = 0.17289681732654572 + 1.0 * 6.332408428192139
Epoch 600, val loss: 0.726686418056488
Epoch 610, training loss: 6.4931840896606445 = 0.16208848357200623 + 1.0 * 6.3310956954956055
Epoch 610, val loss: 0.7290630340576172
Epoch 620, training loss: 6.48103666305542 = 0.15205352008342743 + 1.0 * 6.328983306884766
Epoch 620, val loss: 0.7318739295005798
Epoch 630, training loss: 6.472695827484131 = 0.1427258998155594 + 1.0 * 6.329969882965088
Epoch 630, val loss: 0.7350983619689941
Epoch 640, training loss: 6.4590911865234375 = 0.1340612918138504 + 1.0 * 6.3250298500061035
Epoch 640, val loss: 0.738705039024353
Epoch 650, training loss: 6.455132484436035 = 0.12598009407520294 + 1.0 * 6.329152584075928
Epoch 650, val loss: 0.7426216006278992
Epoch 660, training loss: 6.449405193328857 = 0.11856137216091156 + 1.0 * 6.330843925476074
Epoch 660, val loss: 0.7467925548553467
Epoch 670, training loss: 6.431803226470947 = 0.11169876903295517 + 1.0 * 6.320104598999023
Epoch 670, val loss: 0.7511109709739685
Epoch 680, training loss: 6.423317909240723 = 0.10531320422887802 + 1.0 * 6.318004608154297
Epoch 680, val loss: 0.7556234002113342
Epoch 690, training loss: 6.416014194488525 = 0.0993444174528122 + 1.0 * 6.316669940948486
Epoch 690, val loss: 0.7603736519813538
Epoch 700, training loss: 6.417454242706299 = 0.09377431124448776 + 1.0 * 6.3236799240112305
Epoch 700, val loss: 0.765315592288971
Epoch 710, training loss: 6.407895565032959 = 0.0885862484574318 + 1.0 * 6.319309234619141
Epoch 710, val loss: 0.7703655958175659
Epoch 720, training loss: 6.397915840148926 = 0.08377446979284286 + 1.0 * 6.314141273498535
Epoch 720, val loss: 0.7755120992660522
Epoch 730, training loss: 6.397148132324219 = 0.07929091900587082 + 1.0 * 6.317857265472412
Epoch 730, val loss: 0.7807320356369019
Epoch 740, training loss: 6.387619495391846 = 0.07510372996330261 + 1.0 * 6.312515735626221
Epoch 740, val loss: 0.786033034324646
Epoch 750, training loss: 6.38077449798584 = 0.07119946926832199 + 1.0 * 6.309575080871582
Epoch 750, val loss: 0.7913927435874939
Epoch 760, training loss: 6.387088775634766 = 0.06755813211202621 + 1.0 * 6.319530487060547
Epoch 760, val loss: 0.7967860698699951
Epoch 770, training loss: 6.373989105224609 = 0.064173623919487 + 1.0 * 6.309815406799316
Epoch 770, val loss: 0.8021856546401978
Epoch 780, training loss: 6.366921901702881 = 0.061017077416181564 + 1.0 * 6.305904865264893
Epoch 780, val loss: 0.807616114616394
Epoch 790, training loss: 6.3655924797058105 = 0.058059461414813995 + 1.0 * 6.307532787322998
Epoch 790, val loss: 0.8130351305007935
Epoch 800, training loss: 6.3655877113342285 = 0.05530523881316185 + 1.0 * 6.3102827072143555
Epoch 800, val loss: 0.8184268474578857
Epoch 810, training loss: 6.35831880569458 = 0.052744194865226746 + 1.0 * 6.305574417114258
Epoch 810, val loss: 0.8237702250480652
Epoch 820, training loss: 6.35366678237915 = 0.05034788325428963 + 1.0 * 6.303318977355957
Epoch 820, val loss: 0.8291216492652893
Epoch 830, training loss: 6.348677158355713 = 0.048088572919368744 + 1.0 * 6.300588607788086
Epoch 830, val loss: 0.8344400525093079
Epoch 840, training loss: 6.346010208129883 = 0.04595916345715523 + 1.0 * 6.300051212310791
Epoch 840, val loss: 0.8398036360740662
Epoch 850, training loss: 6.359593391418457 = 0.04395604133605957 + 1.0 * 6.315637111663818
Epoch 850, val loss: 0.8451218605041504
Epoch 860, training loss: 6.341003894805908 = 0.04208073019981384 + 1.0 * 6.298923015594482
Epoch 860, val loss: 0.8505033254623413
Epoch 870, training loss: 6.338853359222412 = 0.04032182693481445 + 1.0 * 6.298531532287598
Epoch 870, val loss: 0.855702817440033
Epoch 880, training loss: 6.335768222808838 = 0.03866082802414894 + 1.0 * 6.297107219696045
Epoch 880, val loss: 0.8609029650688171
Epoch 890, training loss: 6.348896026611328 = 0.03709562495350838 + 1.0 * 6.311800479888916
Epoch 890, val loss: 0.866001307964325
Epoch 900, training loss: 6.333717346191406 = 0.035636309534311295 + 1.0 * 6.298080921173096
Epoch 900, val loss: 0.8711040616035461
Epoch 910, training loss: 6.328872203826904 = 0.034253936260938644 + 1.0 * 6.294618129730225
Epoch 910, val loss: 0.8761204481124878
Epoch 920, training loss: 6.327004432678223 = 0.032952189445495605 + 1.0 * 6.2940521240234375
Epoch 920, val loss: 0.8810354471206665
Epoch 930, training loss: 6.34174108505249 = 0.0317150317132473 + 1.0 * 6.310026168823242
Epoch 930, val loss: 0.8859987258911133
Epoch 940, training loss: 6.325551986694336 = 0.030551625415682793 + 1.0 * 6.2950005531311035
Epoch 940, val loss: 0.8908138275146484
Epoch 950, training loss: 6.320328235626221 = 0.02945162169635296 + 1.0 * 6.290876388549805
Epoch 950, val loss: 0.8956048488616943
Epoch 960, training loss: 6.319215297698975 = 0.02840479277074337 + 1.0 * 6.290810585021973
Epoch 960, val loss: 0.9004001021385193
Epoch 970, training loss: 6.322134971618652 = 0.02740994282066822 + 1.0 * 6.294724941253662
Epoch 970, val loss: 0.905110776424408
Epoch 980, training loss: 6.315774917602539 = 0.0264686718583107 + 1.0 * 6.289306163787842
Epoch 980, val loss: 0.9097906947135925
Epoch 990, training loss: 6.313963413238525 = 0.025573810562491417 + 1.0 * 6.288389682769775
Epoch 990, val loss: 0.9144384264945984
Epoch 1000, training loss: 6.312778949737549 = 0.024724923074245453 + 1.0 * 6.2880539894104
Epoch 1000, val loss: 0.9189900159835815
Epoch 1010, training loss: 6.313400745391846 = 0.02391340397298336 + 1.0 * 6.289487361907959
Epoch 1010, val loss: 0.9235035181045532
Epoch 1020, training loss: 6.310976982116699 = 0.023143818601965904 + 1.0 * 6.287833213806152
Epoch 1020, val loss: 0.9280123114585876
Epoch 1030, training loss: 6.310323715209961 = 0.02241308055818081 + 1.0 * 6.287910461425781
Epoch 1030, val loss: 0.9323819875717163
Epoch 1040, training loss: 6.309113502502441 = 0.02171635627746582 + 1.0 * 6.2873969078063965
Epoch 1040, val loss: 0.9367572069168091
Epoch 1050, training loss: 6.310550689697266 = 0.021053550764918327 + 1.0 * 6.289497375488281
Epoch 1050, val loss: 0.9410484433174133
Epoch 1060, training loss: 6.304507732391357 = 0.020423216745257378 + 1.0 * 6.284084320068359
Epoch 1060, val loss: 0.9452444314956665
Epoch 1070, training loss: 6.302741527557373 = 0.019820688292384148 + 1.0 * 6.282920837402344
Epoch 1070, val loss: 0.9494203925132751
Epoch 1080, training loss: 6.302485942840576 = 0.01924273371696472 + 1.0 * 6.283243179321289
Epoch 1080, val loss: 0.9535450339317322
Epoch 1090, training loss: 6.310534954071045 = 0.018688581883907318 + 1.0 * 6.29184627532959
Epoch 1090, val loss: 0.9576687216758728
Epoch 1100, training loss: 6.3078436851501465 = 0.018161650747060776 + 1.0 * 6.289681911468506
Epoch 1100, val loss: 0.9617645144462585
Epoch 1110, training loss: 6.300523281097412 = 0.017660027369856834 + 1.0 * 6.282863140106201
Epoch 1110, val loss: 0.965710461139679
Epoch 1120, training loss: 6.298495292663574 = 0.017180435359477997 + 1.0 * 6.281314849853516
Epoch 1120, val loss: 0.9695637822151184
Epoch 1130, training loss: 6.297321319580078 = 0.016718091443181038 + 1.0 * 6.280603408813477
Epoch 1130, val loss: 0.9734426736831665
Epoch 1140, training loss: 6.301671028137207 = 0.01627391204237938 + 1.0 * 6.285397052764893
Epoch 1140, val loss: 0.9772821664810181
Epoch 1150, training loss: 6.304701328277588 = 0.015848375856876373 + 1.0 * 6.288853168487549
Epoch 1150, val loss: 0.9811301827430725
Epoch 1160, training loss: 6.2955708503723145 = 0.015441657975316048 + 1.0 * 6.2801289558410645
Epoch 1160, val loss: 0.9848644137382507
Epoch 1170, training loss: 6.2945146560668945 = 0.015051535330712795 + 1.0 * 6.279463291168213
Epoch 1170, val loss: 0.9885662198066711
Epoch 1180, training loss: 6.292850017547607 = 0.014675789512693882 + 1.0 * 6.27817440032959
Epoch 1180, val loss: 0.992209792137146
Epoch 1190, training loss: 6.300296306610107 = 0.014312171377241611 + 1.0 * 6.285984039306641
Epoch 1190, val loss: 0.9958763122558594
Epoch 1200, training loss: 6.293030261993408 = 0.013965790160000324 + 1.0 * 6.279064655303955
Epoch 1200, val loss: 0.9994121193885803
Epoch 1210, training loss: 6.289337635040283 = 0.013631502166390419 + 1.0 * 6.2757062911987305
Epoch 1210, val loss: 1.0029338598251343
Epoch 1220, training loss: 6.3016204833984375 = 0.013309253379702568 + 1.0 * 6.288311004638672
Epoch 1220, val loss: 1.00642728805542
Epoch 1230, training loss: 6.2928385734558105 = 0.013000912964344025 + 1.0 * 6.279837608337402
Epoch 1230, val loss: 1.0098583698272705
Epoch 1240, training loss: 6.2888054847717285 = 0.012704129330813885 + 1.0 * 6.276101589202881
Epoch 1240, val loss: 1.0132226943969727
Epoch 1250, training loss: 6.287747383117676 = 0.01241692528128624 + 1.0 * 6.275330543518066
Epoch 1250, val loss: 1.0165668725967407
Epoch 1260, training loss: 6.294312000274658 = 0.012140104547142982 + 1.0 * 6.282171726226807
Epoch 1260, val loss: 1.0198888778686523
Epoch 1270, training loss: 6.287247657775879 = 0.01187082938849926 + 1.0 * 6.275376796722412
Epoch 1270, val loss: 1.0232408046722412
Epoch 1280, training loss: 6.285388469696045 = 0.011613383889198303 + 1.0 * 6.273775100708008
Epoch 1280, val loss: 1.026466727256775
Epoch 1290, training loss: 6.294027328491211 = 0.011363193392753601 + 1.0 * 6.2826642990112305
Epoch 1290, val loss: 1.029721736907959
Epoch 1300, training loss: 6.2860107421875 = 0.011122976429760456 + 1.0 * 6.274887561798096
Epoch 1300, val loss: 1.0328351259231567
Epoch 1310, training loss: 6.28263521194458 = 0.010891594924032688 + 1.0 * 6.2717437744140625
Epoch 1310, val loss: 1.0359617471694946
Epoch 1320, training loss: 6.287141799926758 = 0.010666300542652607 + 1.0 * 6.276475429534912
Epoch 1320, val loss: 1.0390945672988892
Epoch 1330, training loss: 6.281215667724609 = 0.010449725203216076 + 1.0 * 6.270765781402588
Epoch 1330, val loss: 1.0421684980392456
Epoch 1340, training loss: 6.282325267791748 = 0.010239691473543644 + 1.0 * 6.272085666656494
Epoch 1340, val loss: 1.0452121496200562
Epoch 1350, training loss: 6.284238815307617 = 0.010035958141088486 + 1.0 * 6.274202823638916
Epoch 1350, val loss: 1.0481910705566406
Epoch 1360, training loss: 6.279101371765137 = 0.00983844418078661 + 1.0 * 6.269262790679932
Epoch 1360, val loss: 1.051221251487732
Epoch 1370, training loss: 6.278199195861816 = 0.009647294878959656 + 1.0 * 6.268551826477051
Epoch 1370, val loss: 1.0541540384292603
Epoch 1380, training loss: 6.284297943115234 = 0.009461924433708191 + 1.0 * 6.27483606338501
Epoch 1380, val loss: 1.0570992231369019
Epoch 1390, training loss: 6.281153202056885 = 0.009282378479838371 + 1.0 * 6.2718706130981445
Epoch 1390, val loss: 1.0599896907806396
Epoch 1400, training loss: 6.28073787689209 = 0.00910989474505186 + 1.0 * 6.271627902984619
Epoch 1400, val loss: 1.0627915859222412
Epoch 1410, training loss: 6.281230449676514 = 0.008942513726651669 + 1.0 * 6.272287845611572
Epoch 1410, val loss: 1.065575122833252
Epoch 1420, training loss: 6.275421142578125 = 0.008779505267739296 + 1.0 * 6.266641616821289
Epoch 1420, val loss: 1.068347454071045
Epoch 1430, training loss: 6.276933670043945 = 0.008620895445346832 + 1.0 * 6.268312931060791
Epoch 1430, val loss: 1.071101188659668
Epoch 1440, training loss: 6.280461311340332 = 0.008466430008411407 + 1.0 * 6.2719950675964355
Epoch 1440, val loss: 1.073834776878357
Epoch 1450, training loss: 6.276798725128174 = 0.008316527120769024 + 1.0 * 6.268482208251953
Epoch 1450, val loss: 1.076535701751709
Epoch 1460, training loss: 6.274417877197266 = 0.008170947432518005 + 1.0 * 6.266246795654297
Epoch 1460, val loss: 1.0791878700256348
Epoch 1470, training loss: 6.280788898468018 = 0.008030354045331478 + 1.0 * 6.272758483886719
Epoch 1470, val loss: 1.081815242767334
Epoch 1480, training loss: 6.274368762969971 = 0.007893488742411137 + 1.0 * 6.266475200653076
Epoch 1480, val loss: 1.0844261646270752
Epoch 1490, training loss: 6.273454189300537 = 0.007761406246572733 + 1.0 * 6.265692710876465
Epoch 1490, val loss: 1.0869560241699219
Epoch 1500, training loss: 6.27212381362915 = 0.007631492335349321 + 1.0 * 6.264492511749268
Epoch 1500, val loss: 1.0895085334777832
Epoch 1510, training loss: 6.280496597290039 = 0.007504680193960667 + 1.0 * 6.272992134094238
Epoch 1510, val loss: 1.092077612876892
Epoch 1520, training loss: 6.275065898895264 = 0.007382330484688282 + 1.0 * 6.267683506011963
Epoch 1520, val loss: 1.0946297645568848
Epoch 1530, training loss: 6.272194862365723 = 0.0072638182900846004 + 1.0 * 6.2649312019348145
Epoch 1530, val loss: 1.0970185995101929
Epoch 1540, training loss: 6.270420074462891 = 0.007148011587560177 + 1.0 * 6.263272285461426
Epoch 1540, val loss: 1.099504828453064
Epoch 1550, training loss: 6.269929885864258 = 0.007034124806523323 + 1.0 * 6.262895584106445
Epoch 1550, val loss: 1.1019564867019653
Epoch 1560, training loss: 6.276289463043213 = 0.006922805681824684 + 1.0 * 6.26936674118042
Epoch 1560, val loss: 1.1044176816940308
Epoch 1570, training loss: 6.273387908935547 = 0.0068153212778270245 + 1.0 * 6.26657247543335
Epoch 1570, val loss: 1.106825828552246
Epoch 1580, training loss: 6.273469924926758 = 0.006710166111588478 + 1.0 * 6.266759872436523
Epoch 1580, val loss: 1.1091601848602295
Epoch 1590, training loss: 6.271200656890869 = 0.006609698757529259 + 1.0 * 6.264590740203857
Epoch 1590, val loss: 1.111464262008667
Epoch 1600, training loss: 6.268275737762451 = 0.006510639563202858 + 1.0 * 6.261765003204346
Epoch 1600, val loss: 1.1137974262237549
Epoch 1610, training loss: 6.269812107086182 = 0.006413808558136225 + 1.0 * 6.263398170471191
Epoch 1610, val loss: 1.116079568862915
Epoch 1620, training loss: 6.27627420425415 = 0.0063196150586009026 + 1.0 * 6.269954681396484
Epoch 1620, val loss: 1.1183345317840576
Epoch 1630, training loss: 6.2667412757873535 = 0.006228448823094368 + 1.0 * 6.260512828826904
Epoch 1630, val loss: 1.1205241680145264
Epoch 1640, training loss: 6.26556921005249 = 0.0061396146193146706 + 1.0 * 6.259429454803467
Epoch 1640, val loss: 1.1226956844329834
Epoch 1650, training loss: 6.264894485473633 = 0.0060513499192893505 + 1.0 * 6.258842945098877
Epoch 1650, val loss: 1.1249531507492065
Epoch 1660, training loss: 6.267915725708008 = 0.005964681971818209 + 1.0 * 6.261950969696045
Epoch 1660, val loss: 1.1271812915802002
Epoch 1670, training loss: 6.2683234214782715 = 0.005880783777683973 + 1.0 * 6.262442588806152
Epoch 1670, val loss: 1.1293408870697021
Epoch 1680, training loss: 6.268774509429932 = 0.005799334030598402 + 1.0 * 6.262975215911865
Epoch 1680, val loss: 1.1314513683319092
Epoch 1690, training loss: 6.268220901489258 = 0.005719526205211878 + 1.0 * 6.262501239776611
Epoch 1690, val loss: 1.1335995197296143
Epoch 1700, training loss: 6.264379024505615 = 0.005641541909426451 + 1.0 * 6.258737564086914
Epoch 1700, val loss: 1.1357086896896362
Epoch 1710, training loss: 6.262791633605957 = 0.005564884282648563 + 1.0 * 6.257226943969727
Epoch 1710, val loss: 1.1377900838851929
Epoch 1720, training loss: 6.268495082855225 = 0.0054897116497159 + 1.0 * 6.263005256652832
Epoch 1720, val loss: 1.1398217678070068
Epoch 1730, training loss: 6.264889240264893 = 0.005416263360530138 + 1.0 * 6.259472846984863
Epoch 1730, val loss: 1.1419581174850464
Epoch 1740, training loss: 6.2683563232421875 = 0.005345613695681095 + 1.0 * 6.263010501861572
Epoch 1740, val loss: 1.1439186334609985
Epoch 1750, training loss: 6.262465953826904 = 0.005276644602417946 + 1.0 * 6.2571892738342285
Epoch 1750, val loss: 1.1459100246429443
Epoch 1760, training loss: 6.261842250823975 = 0.005208676215261221 + 1.0 * 6.256633758544922
Epoch 1760, val loss: 1.1479023694992065
Epoch 1770, training loss: 6.269289493560791 = 0.005141910165548325 + 1.0 * 6.264147758483887
Epoch 1770, val loss: 1.1498531103134155
Epoch 1780, training loss: 6.2629570960998535 = 0.00507668312638998 + 1.0 * 6.257880210876465
Epoch 1780, val loss: 1.1518529653549194
Epoch 1790, training loss: 6.2613525390625 = 0.005013355053961277 + 1.0 * 6.256339073181152
Epoch 1790, val loss: 1.153742790222168
Epoch 1800, training loss: 6.262631416320801 = 0.00495090102776885 + 1.0 * 6.257680416107178
Epoch 1800, val loss: 1.155665636062622
Epoch 1810, training loss: 6.265792369842529 = 0.004889988340437412 + 1.0 * 6.260902404785156
Epoch 1810, val loss: 1.1575473546981812
Epoch 1820, training loss: 6.261368751525879 = 0.004831203259527683 + 1.0 * 6.256537437438965
Epoch 1820, val loss: 1.1593830585479736
Epoch 1830, training loss: 6.260002136230469 = 0.004772577900439501 + 1.0 * 6.255229473114014
Epoch 1830, val loss: 1.161293864250183
Epoch 1840, training loss: 6.260365962982178 = 0.0047147939912974834 + 1.0 * 6.255650997161865
Epoch 1840, val loss: 1.1631829738616943
Epoch 1850, training loss: 6.262943267822266 = 0.0046579246409237385 + 1.0 * 6.2582855224609375
Epoch 1850, val loss: 1.1650564670562744
Epoch 1860, training loss: 6.266043186187744 = 0.004602953791618347 + 1.0 * 6.261440277099609
Epoch 1860, val loss: 1.1667982339859009
Epoch 1870, training loss: 6.258686542510986 = 0.0045494879595935345 + 1.0 * 6.25413703918457
Epoch 1870, val loss: 1.168653964996338
Epoch 1880, training loss: 6.2573676109313965 = 0.004497189540416002 + 1.0 * 6.252870559692383
Epoch 1880, val loss: 1.170373558998108
Epoch 1890, training loss: 6.257083415985107 = 0.004445283208042383 + 1.0 * 6.252638339996338
Epoch 1890, val loss: 1.1721649169921875
Epoch 1900, training loss: 6.2598395347595215 = 0.00439388258382678 + 1.0 * 6.25544548034668
Epoch 1900, val loss: 1.1739606857299805
Epoch 1910, training loss: 6.259737968444824 = 0.004343952052295208 + 1.0 * 6.255393981933594
Epoch 1910, val loss: 1.175724744796753
Epoch 1920, training loss: 6.259472370147705 = 0.00429493747651577 + 1.0 * 6.2551774978637695
Epoch 1920, val loss: 1.1774537563323975
Epoch 1930, training loss: 6.260185241699219 = 0.0042468225583434105 + 1.0 * 6.255938529968262
Epoch 1930, val loss: 1.1791644096374512
Epoch 1940, training loss: 6.2566752433776855 = 0.004199839662760496 + 1.0 * 6.252475261688232
Epoch 1940, val loss: 1.1809046268463135
Epoch 1950, training loss: 6.258970260620117 = 0.004153952468186617 + 1.0 * 6.25481653213501
Epoch 1950, val loss: 1.1825644969940186
Epoch 1960, training loss: 6.255510330200195 = 0.004108514171093702 + 1.0 * 6.251401901245117
Epoch 1960, val loss: 1.184256911277771
Epoch 1970, training loss: 6.261006832122803 = 0.004063968081027269 + 1.0 * 6.2569427490234375
Epoch 1970, val loss: 1.1859642267227173
Epoch 1980, training loss: 6.256762504577637 = 0.004020769149065018 + 1.0 * 6.252741813659668
Epoch 1980, val loss: 1.187564730644226
Epoch 1990, training loss: 6.2577314376831055 = 0.003978377673774958 + 1.0 * 6.253753185272217
Epoch 1990, val loss: 1.1892033815383911
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 10.546269416809082 = 1.9494593143463135 + 1.0 * 8.596810340881348
Epoch 0, val loss: 1.9461266994476318
Epoch 10, training loss: 10.535298347473145 = 1.9388943910598755 + 1.0 * 8.596404075622559
Epoch 10, val loss: 1.9360283613204956
Epoch 20, training loss: 10.518732070922852 = 1.9257110357284546 + 1.0 * 8.593021392822266
Epoch 20, val loss: 1.9229180812835693
Epoch 30, training loss: 10.475028991699219 = 1.9075086116790771 + 1.0 * 8.567520141601562
Epoch 30, val loss: 1.9044259786605835
Epoch 40, training loss: 10.300127983093262 = 1.8847194910049438 + 1.0 * 8.41540813446045
Epoch 40, val loss: 1.8820656538009644
Epoch 50, training loss: 9.845386505126953 = 1.8605842590332031 + 1.0 * 7.984801769256592
Epoch 50, val loss: 1.8588519096374512
Epoch 60, training loss: 9.54358196258545 = 1.8395026922225952 + 1.0 * 7.7040791511535645
Epoch 60, val loss: 1.839746117591858
Epoch 70, training loss: 9.15075969696045 = 1.8237501382827759 + 1.0 * 7.327009677886963
Epoch 70, val loss: 1.8257806301116943
Epoch 80, training loss: 8.810112953186035 = 1.8123695850372314 + 1.0 * 6.997743606567383
Epoch 80, val loss: 1.8152868747711182
Epoch 90, training loss: 8.625382423400879 = 1.7989418506622314 + 1.0 * 6.826440811157227
Epoch 90, val loss: 1.802897334098816
Epoch 100, training loss: 8.509357452392578 = 1.7832635641098022 + 1.0 * 6.726093769073486
Epoch 100, val loss: 1.7892200946807861
Epoch 110, training loss: 8.4318265914917 = 1.767769455909729 + 1.0 * 6.66405725479126
Epoch 110, val loss: 1.7760040760040283
Epoch 120, training loss: 8.373371124267578 = 1.7521772384643555 + 1.0 * 6.621194362640381
Epoch 120, val loss: 1.762411117553711
Epoch 130, training loss: 8.324785232543945 = 1.7350702285766602 + 1.0 * 6.589714527130127
Epoch 130, val loss: 1.7472302913665771
Epoch 140, training loss: 8.284438133239746 = 1.7154240608215332 + 1.0 * 6.569014072418213
Epoch 140, val loss: 1.7298270463943481
Epoch 150, training loss: 8.237897872924805 = 1.6927391290664673 + 1.0 * 6.545158863067627
Epoch 150, val loss: 1.7099552154541016
Epoch 160, training loss: 8.1893949508667 = 1.6663035154342651 + 1.0 * 6.523091793060303
Epoch 160, val loss: 1.6867685317993164
Epoch 170, training loss: 8.140783309936523 = 1.635074257850647 + 1.0 * 6.505708694458008
Epoch 170, val loss: 1.6593892574310303
Epoch 180, training loss: 8.084650993347168 = 1.599187970161438 + 1.0 * 6.4854631423950195
Epoch 180, val loss: 1.6279864311218262
Epoch 190, training loss: 8.028021812438965 = 1.5580952167510986 + 1.0 * 6.469926834106445
Epoch 190, val loss: 1.5920004844665527
Epoch 200, training loss: 7.968873023986816 = 1.5112323760986328 + 1.0 * 6.457640647888184
Epoch 200, val loss: 1.5513275861740112
Epoch 210, training loss: 7.907226085662842 = 1.4598475694656372 + 1.0 * 6.447378635406494
Epoch 210, val loss: 1.5073357820510864
Epoch 220, training loss: 7.842533111572266 = 1.4052566289901733 + 1.0 * 6.437276363372803
Epoch 220, val loss: 1.4610177278518677
Epoch 230, training loss: 7.779353618621826 = 1.3484230041503906 + 1.0 * 6.4309306144714355
Epoch 230, val loss: 1.4137452840805054
Epoch 240, training loss: 7.713184356689453 = 1.2916467189788818 + 1.0 * 6.421537399291992
Epoch 240, val loss: 1.3673709630966187
Epoch 250, training loss: 7.650287628173828 = 1.235947847366333 + 1.0 * 6.414340019226074
Epoch 250, val loss: 1.3226157426834106
Epoch 260, training loss: 7.595882892608643 = 1.1821118593215942 + 1.0 * 6.413771152496338
Epoch 260, val loss: 1.2801746129989624
Epoch 270, training loss: 7.534240245819092 = 1.1316094398498535 + 1.0 * 6.402630805969238
Epoch 270, val loss: 1.2411571741104126
Epoch 280, training loss: 7.480268478393555 = 1.0842921733856201 + 1.0 * 6.395976543426514
Epoch 280, val loss: 1.2052580118179321
Epoch 290, training loss: 7.434944152832031 = 1.0399452447891235 + 1.0 * 6.394999027252197
Epoch 290, val loss: 1.1722315549850464
Epoch 300, training loss: 7.385553359985352 = 0.9989426732063293 + 1.0 * 6.386610507965088
Epoch 300, val loss: 1.1420704126358032
Epoch 310, training loss: 7.342065811157227 = 0.9606425762176514 + 1.0 * 6.381423473358154
Epoch 310, val loss: 1.1142287254333496
Epoch 320, training loss: 7.300792217254639 = 0.9241204857826233 + 1.0 * 6.37667179107666
Epoch 320, val loss: 1.0879063606262207
Epoch 330, training loss: 7.268960475921631 = 0.8889813423156738 + 1.0 * 6.379979133605957
Epoch 330, val loss: 1.062764048576355
Epoch 340, training loss: 7.226132869720459 = 0.8553429245948792 + 1.0 * 6.370790004730225
Epoch 340, val loss: 1.038631558418274
Epoch 350, training loss: 7.186986923217773 = 0.8225935101509094 + 1.0 * 6.36439323425293
Epoch 350, val loss: 1.01521897315979
Epoch 360, training loss: 7.153139114379883 = 0.7902546525001526 + 1.0 * 6.362884521484375
Epoch 360, val loss: 0.9921023845672607
Epoch 370, training loss: 7.122673988342285 = 0.7586863040924072 + 1.0 * 6.363987445831299
Epoch 370, val loss: 0.9694797992706299
Epoch 380, training loss: 7.083376407623291 = 0.7281225323677063 + 1.0 * 6.35525369644165
Epoch 380, val loss: 0.9479416608810425
Epoch 390, training loss: 7.049506187438965 = 0.6981328129768372 + 1.0 * 6.351373195648193
Epoch 390, val loss: 0.9272220730781555
Epoch 400, training loss: 7.022616386413574 = 0.6686520576477051 + 1.0 * 6.353964328765869
Epoch 400, val loss: 0.9073055982589722
Epoch 410, training loss: 6.988763332366943 = 0.6400547027587891 + 1.0 * 6.348708629608154
Epoch 410, val loss: 0.8886393308639526
Epoch 420, training loss: 6.95612096786499 = 0.6122310757637024 + 1.0 * 6.3438897132873535
Epoch 420, val loss: 0.8712334632873535
Epoch 430, training loss: 6.928699970245361 = 0.584996223449707 + 1.0 * 6.343703746795654
Epoch 430, val loss: 0.8548970222473145
Epoch 440, training loss: 6.906525611877441 = 0.5583722591400146 + 1.0 * 6.348153591156006
Epoch 440, val loss: 0.8398913145065308
Epoch 450, training loss: 6.871540069580078 = 0.5326094031333923 + 1.0 * 6.338930606842041
Epoch 450, val loss: 0.8262166380882263
Epoch 460, training loss: 6.842517375946045 = 0.5074642300605774 + 1.0 * 6.335052967071533
Epoch 460, val loss: 0.8137601613998413
Epoch 470, training loss: 6.821165561676025 = 0.48281770944595337 + 1.0 * 6.338347911834717
Epoch 470, val loss: 0.8023773431777954
Epoch 480, training loss: 6.793227195739746 = 0.4588145911693573 + 1.0 * 6.334412574768066
Epoch 480, val loss: 0.7922748923301697
Epoch 490, training loss: 6.764659404754639 = 0.4355619251728058 + 1.0 * 6.329097270965576
Epoch 490, val loss: 0.7834417223930359
Epoch 500, training loss: 6.7438554763793945 = 0.41311848163604736 + 1.0 * 6.330737113952637
Epoch 500, val loss: 0.7757033109664917
Epoch 510, training loss: 6.722186088562012 = 0.3917306959629059 + 1.0 * 6.330455303192139
Epoch 510, val loss: 0.7694475054740906
Epoch 520, training loss: 6.695150852203369 = 0.37114855647087097 + 1.0 * 6.324002265930176
Epoch 520, val loss: 0.7643823623657227
Epoch 530, training loss: 6.673110485076904 = 0.3513369560241699 + 1.0 * 6.321773529052734
Epoch 530, val loss: 0.7601957321166992
Epoch 540, training loss: 6.6551713943481445 = 0.33233046531677246 + 1.0 * 6.322841167449951
Epoch 540, val loss: 0.7571820020675659
Epoch 550, training loss: 6.643072128295898 = 0.31430524587631226 + 1.0 * 6.328766822814941
Epoch 550, val loss: 0.7552136182785034
Epoch 560, training loss: 6.618469715118408 = 0.2973617911338806 + 1.0 * 6.321107864379883
Epoch 560, val loss: 0.7544277906417847
Epoch 570, training loss: 6.602869987487793 = 0.2813011407852173 + 1.0 * 6.321568965911865
Epoch 570, val loss: 0.7543922662734985
Epoch 580, training loss: 6.582570552825928 = 0.26611799001693726 + 1.0 * 6.316452503204346
Epoch 580, val loss: 0.7551820874214172
Epoch 590, training loss: 6.570766448974609 = 0.25172263383865356 + 1.0 * 6.3190436363220215
Epoch 590, val loss: 0.7568217515945435
Epoch 600, training loss: 6.555396556854248 = 0.23814401030540466 + 1.0 * 6.3172526359558105
Epoch 600, val loss: 0.758959949016571
Epoch 610, training loss: 6.541099548339844 = 0.22536571323871613 + 1.0 * 6.315733909606934
Epoch 610, val loss: 0.7618836760520935
Epoch 620, training loss: 6.52347469329834 = 0.21330639719963074 + 1.0 * 6.310168266296387
Epoch 620, val loss: 0.7652957439422607
Epoch 630, training loss: 6.513058185577393 = 0.20189900696277618 + 1.0 * 6.311159133911133
Epoch 630, val loss: 0.7690984010696411
Epoch 640, training loss: 6.505306243896484 = 0.19113007187843323 + 1.0 * 6.314176082611084
Epoch 640, val loss: 0.7732534408569336
Epoch 650, training loss: 6.490311145782471 = 0.18097712099552155 + 1.0 * 6.309333801269531
Epoch 650, val loss: 0.7778360843658447
Epoch 660, training loss: 6.478166580200195 = 0.17143389582633972 + 1.0 * 6.306732654571533
Epoch 660, val loss: 0.7827730178833008
Epoch 670, training loss: 6.468471050262451 = 0.1623843014240265 + 1.0 * 6.306086540222168
Epoch 670, val loss: 0.7879288196563721
Epoch 680, training loss: 6.4586286544799805 = 0.15388046205043793 + 1.0 * 6.304748058319092
Epoch 680, val loss: 0.7933304309844971
Epoch 690, training loss: 6.450573444366455 = 0.14588814973831177 + 1.0 * 6.304685115814209
Epoch 690, val loss: 0.7990610599517822
Epoch 700, training loss: 6.439272403717041 = 0.13834016025066376 + 1.0 * 6.30093240737915
Epoch 700, val loss: 0.8048775792121887
Epoch 710, training loss: 6.4405131340026855 = 0.13118833303451538 + 1.0 * 6.309324741363525
Epoch 710, val loss: 0.8107993602752686
Epoch 720, training loss: 6.429182052612305 = 0.12448453158140182 + 1.0 * 6.304697513580322
Epoch 720, val loss: 0.8168959617614746
Epoch 730, training loss: 6.4232964515686035 = 0.11817054450511932 + 1.0 * 6.305125713348389
Epoch 730, val loss: 0.823226273059845
Epoch 740, training loss: 6.414125919342041 = 0.11222705990076065 + 1.0 * 6.301898956298828
Epoch 740, val loss: 0.8293578028678894
Epoch 750, training loss: 6.403862476348877 = 0.1066364198923111 + 1.0 * 6.2972259521484375
Epoch 750, val loss: 0.8358161449432373
Epoch 760, training loss: 6.396462440490723 = 0.10134443640708923 + 1.0 * 6.2951178550720215
Epoch 760, val loss: 0.8422715067863464
Epoch 770, training loss: 6.39374303817749 = 0.09633173793554306 + 1.0 * 6.2974114418029785
Epoch 770, val loss: 0.8487938642501831
Epoch 780, training loss: 6.387148857116699 = 0.09162860363721848 + 1.0 * 6.295520305633545
Epoch 780, val loss: 0.855383038520813
Epoch 790, training loss: 6.385542869567871 = 0.08719117194414139 + 1.0 * 6.298351764678955
Epoch 790, val loss: 0.8620241284370422
Epoch 800, training loss: 6.377527236938477 = 0.08302384614944458 + 1.0 * 6.294503211975098
Epoch 800, val loss: 0.8686914443969727
Epoch 810, training loss: 6.376148223876953 = 0.07910168170928955 + 1.0 * 6.297046661376953
Epoch 810, val loss: 0.8753669261932373
Epoch 820, training loss: 6.367588520050049 = 0.07539914548397064 + 1.0 * 6.292189598083496
Epoch 820, val loss: 0.8821340203285217
Epoch 830, training loss: 6.362606048583984 = 0.07191004604101181 + 1.0 * 6.290696144104004
Epoch 830, val loss: 0.8888997435569763
Epoch 840, training loss: 6.36511754989624 = 0.06861135363578796 + 1.0 * 6.296506404876709
Epoch 840, val loss: 0.8956063389778137
Epoch 850, training loss: 6.359313488006592 = 0.06550893932580948 + 1.0 * 6.29380464553833
Epoch 850, val loss: 0.9023258686065674
Epoch 860, training loss: 6.351688861846924 = 0.0625978410243988 + 1.0 * 6.289091110229492
Epoch 860, val loss: 0.9091553092002869
Epoch 870, training loss: 6.347166538238525 = 0.059841338545084 + 1.0 * 6.287325382232666
Epoch 870, val loss: 0.9158356189727783
Epoch 880, training loss: 6.350576877593994 = 0.057240571826696396 + 1.0 * 6.293336391448975
Epoch 880, val loss: 0.9225689172744751
Epoch 890, training loss: 6.348902702331543 = 0.054788414388895035 + 1.0 * 6.294114112854004
Epoch 890, val loss: 0.9292760491371155
Epoch 900, training loss: 6.338481426239014 = 0.052490148693323135 + 1.0 * 6.285991191864014
Epoch 900, val loss: 0.9360418915748596
Epoch 910, training loss: 6.336242198944092 = 0.05032020062208176 + 1.0 * 6.285922050476074
Epoch 910, val loss: 0.9426807761192322
Epoch 920, training loss: 6.337015151977539 = 0.04826924204826355 + 1.0 * 6.288745880126953
Epoch 920, val loss: 0.9492506384849548
Epoch 930, training loss: 6.330036163330078 = 0.04633142054080963 + 1.0 * 6.28370475769043
Epoch 930, val loss: 0.9558988213539124
Epoch 940, training loss: 6.327098369598389 = 0.04450109228491783 + 1.0 * 6.282597064971924
Epoch 940, val loss: 0.9624805450439453
Epoch 950, training loss: 6.328210830688477 = 0.04276299476623535 + 1.0 * 6.28544807434082
Epoch 950, val loss: 0.9689570665359497
Epoch 960, training loss: 6.328300476074219 = 0.041124746203422546 + 1.0 * 6.28717565536499
Epoch 960, val loss: 0.975410521030426
Epoch 970, training loss: 6.323749542236328 = 0.03957940638065338 + 1.0 * 6.284170150756836
Epoch 970, val loss: 0.9818747043609619
Epoch 980, training loss: 6.3174543380737305 = 0.03811744228005409 + 1.0 * 6.279336929321289
Epoch 980, val loss: 0.9882696270942688
Epoch 990, training loss: 6.315402030944824 = 0.036726780235767365 + 1.0 * 6.278675079345703
Epoch 990, val loss: 0.9945469498634338
Epoch 1000, training loss: 6.3216938972473145 = 0.03540764003992081 + 1.0 * 6.286286354064941
Epoch 1000, val loss: 1.0008032321929932
Epoch 1010, training loss: 6.316624164581299 = 0.0341639406979084 + 1.0 * 6.2824602127075195
Epoch 1010, val loss: 1.0070209503173828
Epoch 1020, training loss: 6.310781955718994 = 0.03297968953847885 + 1.0 * 6.277802467346191
Epoch 1020, val loss: 1.0132390260696411
Epoch 1030, training loss: 6.31197452545166 = 0.03185749799013138 + 1.0 * 6.280117034912109
Epoch 1030, val loss: 1.0193222761154175
Epoch 1040, training loss: 6.306821823120117 = 0.030789097771048546 + 1.0 * 6.2760329246521
Epoch 1040, val loss: 1.0253286361694336
Epoch 1050, training loss: 6.305230617523193 = 0.029773691669106483 + 1.0 * 6.27545690536499
Epoch 1050, val loss: 1.0314366817474365
Epoch 1060, training loss: 6.30524206161499 = 0.028805656358599663 + 1.0 * 6.2764363288879395
Epoch 1060, val loss: 1.037394642829895
Epoch 1070, training loss: 6.305662155151367 = 0.02788451872766018 + 1.0 * 6.277777671813965
Epoch 1070, val loss: 1.0432209968566895
Epoch 1080, training loss: 6.302065372467041 = 0.027008064091205597 + 1.0 * 6.275057315826416
Epoch 1080, val loss: 1.0491151809692383
Epoch 1090, training loss: 6.301151752471924 = 0.026173219084739685 + 1.0 * 6.2749786376953125
Epoch 1090, val loss: 1.054877519607544
Epoch 1100, training loss: 6.2979302406311035 = 0.02537715435028076 + 1.0 * 6.272552967071533
Epoch 1100, val loss: 1.0606101751327515
Epoch 1110, training loss: 6.295287132263184 = 0.024616168811917305 + 1.0 * 6.2706708908081055
Epoch 1110, val loss: 1.0663343667984009
Epoch 1120, training loss: 6.299402713775635 = 0.023885389789938927 + 1.0 * 6.275517463684082
Epoch 1120, val loss: 1.0719178915023804
Epoch 1130, training loss: 6.2949042320251465 = 0.023189034312963486 + 1.0 * 6.27171516418457
Epoch 1130, val loss: 1.0774251222610474
Epoch 1140, training loss: 6.298883438110352 = 0.02252727374434471 + 1.0 * 6.276356220245361
Epoch 1140, val loss: 1.083043098449707
Epoch 1150, training loss: 6.292840957641602 = 0.021897032856941223 + 1.0 * 6.270944118499756
Epoch 1150, val loss: 1.0884897708892822
Epoch 1160, training loss: 6.2907233238220215 = 0.021290473639965057 + 1.0 * 6.26943302154541
Epoch 1160, val loss: 1.0938055515289307
Epoch 1170, training loss: 6.290107727050781 = 0.020706573501229286 + 1.0 * 6.2694010734558105
Epoch 1170, val loss: 1.0991257429122925
Epoch 1180, training loss: 6.290655136108398 = 0.020148849114775658 + 1.0 * 6.270506381988525
Epoch 1180, val loss: 1.104413628578186
Epoch 1190, training loss: 6.292521953582764 = 0.019613811746239662 + 1.0 * 6.2729082107543945
Epoch 1190, val loss: 1.1095867156982422
Epoch 1200, training loss: 6.2862548828125 = 0.01910140924155712 + 1.0 * 6.267153263092041
Epoch 1200, val loss: 1.1147286891937256
Epoch 1210, training loss: 6.285583972930908 = 0.01861199550330639 + 1.0 * 6.266972064971924
Epoch 1210, val loss: 1.1198945045471191
Epoch 1220, training loss: 6.283565521240234 = 0.018139924854040146 + 1.0 * 6.265425682067871
Epoch 1220, val loss: 1.1249018907546997
Epoch 1230, training loss: 6.287956237792969 = 0.01768353395164013 + 1.0 * 6.270272731781006
Epoch 1230, val loss: 1.1298898458480835
Epoch 1240, training loss: 6.283719539642334 = 0.017245663329958916 + 1.0 * 6.266473770141602
Epoch 1240, val loss: 1.1346904039382935
Epoch 1250, training loss: 6.284489154815674 = 0.016828075051307678 + 1.0 * 6.267661094665527
Epoch 1250, val loss: 1.1397762298583984
Epoch 1260, training loss: 6.284825325012207 = 0.016424985602498055 + 1.0 * 6.268400192260742
Epoch 1260, val loss: 1.1444987058639526
Epoch 1270, training loss: 6.282742500305176 = 0.01603718474507332 + 1.0 * 6.266705513000488
Epoch 1270, val loss: 1.1492167711257935
Epoch 1280, training loss: 6.282650947570801 = 0.01566501334309578 + 1.0 * 6.266985893249512
Epoch 1280, val loss: 1.1539703607559204
Epoch 1290, training loss: 6.277660846710205 = 0.015304544009268284 + 1.0 * 6.262356281280518
Epoch 1290, val loss: 1.1586250066757202
Epoch 1300, training loss: 6.279788494110107 = 0.014956776052713394 + 1.0 * 6.26483154296875
Epoch 1300, val loss: 1.16325843334198
Epoch 1310, training loss: 6.279566287994385 = 0.01462082751095295 + 1.0 * 6.2649455070495605
Epoch 1310, val loss: 1.1677063703536987
Epoch 1320, training loss: 6.275808334350586 = 0.014297569170594215 + 1.0 * 6.261510848999023
Epoch 1320, val loss: 1.172303557395935
Epoch 1330, training loss: 6.276834011077881 = 0.01398640125989914 + 1.0 * 6.262847423553467
Epoch 1330, val loss: 1.1768364906311035
Epoch 1340, training loss: 6.278219223022461 = 0.01368462573736906 + 1.0 * 6.2645344734191895
Epoch 1340, val loss: 1.1811431646347046
Epoch 1350, training loss: 6.275453090667725 = 0.01339260209351778 + 1.0 * 6.262060642242432
Epoch 1350, val loss: 1.1855688095092773
Epoch 1360, training loss: 6.272881507873535 = 0.013111524283885956 + 1.0 * 6.259769916534424
Epoch 1360, val loss: 1.1899733543395996
Epoch 1370, training loss: 6.277695178985596 = 0.012837791815400124 + 1.0 * 6.264857292175293
Epoch 1370, val loss: 1.194234848022461
Epoch 1380, training loss: 6.272192478179932 = 0.012572766281664371 + 1.0 * 6.25961971282959
Epoch 1380, val loss: 1.1983802318572998
Epoch 1390, training loss: 6.271355628967285 = 0.012318643741309643 + 1.0 * 6.259037017822266
Epoch 1390, val loss: 1.202741026878357
Epoch 1400, training loss: 6.275132179260254 = 0.012070896103978157 + 1.0 * 6.263061046600342
Epoch 1400, val loss: 1.2069189548492432
Epoch 1410, training loss: 6.2702741622924805 = 0.01183110661804676 + 1.0 * 6.2584428787231445
Epoch 1410, val loss: 1.2109551429748535
Epoch 1420, training loss: 6.271444320678711 = 0.011599339544773102 + 1.0 * 6.259844779968262
Epoch 1420, val loss: 1.2151060104370117
Epoch 1430, training loss: 6.270945072174072 = 0.011374237947165966 + 1.0 * 6.259570598602295
Epoch 1430, val loss: 1.2191050052642822
Epoch 1440, training loss: 6.271401405334473 = 0.01115691289305687 + 1.0 * 6.260244369506836
Epoch 1440, val loss: 1.2231115102767944
Epoch 1450, training loss: 6.2692999839782715 = 0.010945218615233898 + 1.0 * 6.258354663848877
Epoch 1450, val loss: 1.2271113395690918
Epoch 1460, training loss: 6.276973724365234 = 0.010741780512034893 + 1.0 * 6.266232013702393
Epoch 1460, val loss: 1.231018304824829
Epoch 1470, training loss: 6.268783092498779 = 0.010543703101575375 + 1.0 * 6.258239269256592
Epoch 1470, val loss: 1.2348525524139404
Epoch 1480, training loss: 6.26614236831665 = 0.010352155193686485 + 1.0 * 6.2557902336120605
Epoch 1480, val loss: 1.238821029663086
Epoch 1490, training loss: 6.265018939971924 = 0.010163930244743824 + 1.0 * 6.254855155944824
Epoch 1490, val loss: 1.2426178455352783
Epoch 1500, training loss: 6.27298641204834 = 0.009980215691030025 + 1.0 * 6.263006210327148
Epoch 1500, val loss: 1.246344804763794
Epoch 1510, training loss: 6.269247055053711 = 0.009803633205592632 + 1.0 * 6.259443283081055
Epoch 1510, val loss: 1.2499885559082031
Epoch 1520, training loss: 6.267240524291992 = 0.009633095003664494 + 1.0 * 6.257607460021973
Epoch 1520, val loss: 1.2538288831710815
Epoch 1530, training loss: 6.27102518081665 = 0.009467416442930698 + 1.0 * 6.261557579040527
Epoch 1530, val loss: 1.257526159286499
Epoch 1540, training loss: 6.265566825866699 = 0.009304952807724476 + 1.0 * 6.256261825561523
Epoch 1540, val loss: 1.2609548568725586
Epoch 1550, training loss: 6.262861251831055 = 0.00914865080267191 + 1.0 * 6.2537126541137695
Epoch 1550, val loss: 1.2646712064743042
Epoch 1560, training loss: 6.265849590301514 = 0.008995022624731064 + 1.0 * 6.25685453414917
Epoch 1560, val loss: 1.26819908618927
Epoch 1570, training loss: 6.262244701385498 = 0.008845658972859383 + 1.0 * 6.253398895263672
Epoch 1570, val loss: 1.2716444730758667
Epoch 1580, training loss: 6.261014938354492 = 0.008701193146407604 + 1.0 * 6.252313613891602
Epoch 1580, val loss: 1.275240421295166
Epoch 1590, training loss: 6.262312412261963 = 0.008559677749872208 + 1.0 * 6.253752708435059
Epoch 1590, val loss: 1.278743028640747
Epoch 1600, training loss: 6.270517826080322 = 0.008421290665864944 + 1.0 * 6.262096405029297
Epoch 1600, val loss: 1.2820631265640259
Epoch 1610, training loss: 6.2619171142578125 = 0.008287753909826279 + 1.0 * 6.253629207611084
Epoch 1610, val loss: 1.2854211330413818
Epoch 1620, training loss: 6.258937358856201 = 0.008158822543919086 + 1.0 * 6.250778675079346
Epoch 1620, val loss: 1.2889797687530518
Epoch 1630, training loss: 6.259065628051758 = 0.008031444624066353 + 1.0 * 6.251034259796143
Epoch 1630, val loss: 1.292328953742981
Epoch 1640, training loss: 6.261873245239258 = 0.007907163351774216 + 1.0 * 6.253965854644775
Epoch 1640, val loss: 1.2955900430679321
Epoch 1650, training loss: 6.261140823364258 = 0.007785857189446688 + 1.0 * 6.253355026245117
Epoch 1650, val loss: 1.29883873462677
Epoch 1660, training loss: 6.261308670043945 = 0.00766799645498395 + 1.0 * 6.253640651702881
Epoch 1660, val loss: 1.3021652698516846
Epoch 1670, training loss: 6.257385730743408 = 0.007552574388682842 + 1.0 * 6.249833106994629
Epoch 1670, val loss: 1.3053538799285889
Epoch 1680, training loss: 6.2572526931762695 = 0.007440985646098852 + 1.0 * 6.24981164932251
Epoch 1680, val loss: 1.3086786270141602
Epoch 1690, training loss: 6.258361339569092 = 0.00733134476467967 + 1.0 * 6.251029968261719
Epoch 1690, val loss: 1.311889886856079
Epoch 1700, training loss: 6.258936405181885 = 0.007224035449326038 + 1.0 * 6.251712322235107
Epoch 1700, val loss: 1.314909815788269
Epoch 1710, training loss: 6.257040500640869 = 0.007120111491531134 + 1.0 * 6.24992036819458
Epoch 1710, val loss: 1.3180999755859375
Epoch 1720, training loss: 6.259191036224365 = 0.007018531206995249 + 1.0 * 6.252172470092773
Epoch 1720, val loss: 1.3212467432022095
Epoch 1730, training loss: 6.2551798820495605 = 0.006918765604496002 + 1.0 * 6.248260974884033
Epoch 1730, val loss: 1.3243591785430908
Epoch 1740, training loss: 6.2652482986450195 = 0.006822038441896439 + 1.0 * 6.258426189422607
Epoch 1740, val loss: 1.3274325132369995
Epoch 1750, training loss: 6.256031036376953 = 0.0067273154854774475 + 1.0 * 6.249303817749023
Epoch 1750, val loss: 1.3303290605545044
Epoch 1760, training loss: 6.253941059112549 = 0.006635301746428013 + 1.0 * 6.247305870056152
Epoch 1760, val loss: 1.3334852457046509
Epoch 1770, training loss: 6.253225326538086 = 0.0065447487868368626 + 1.0 * 6.246680736541748
Epoch 1770, val loss: 1.3365389108657837
Epoch 1780, training loss: 6.258340835571289 = 0.006455407012254 + 1.0 * 6.251885414123535
Epoch 1780, val loss: 1.3394466638565063
Epoch 1790, training loss: 6.252686977386475 = 0.006368280854076147 + 1.0 * 6.246318817138672
Epoch 1790, val loss: 1.3422701358795166
Epoch 1800, training loss: 6.25230073928833 = 0.006283680442720652 + 1.0 * 6.246016979217529
Epoch 1800, val loss: 1.345333218574524
Epoch 1810, training loss: 6.260715007781982 = 0.006200465839356184 + 1.0 * 6.254514694213867
Epoch 1810, val loss: 1.348256230354309
Epoch 1820, training loss: 6.254096508026123 = 0.006120317615568638 + 1.0 * 6.247976303100586
Epoch 1820, val loss: 1.3510396480560303
Epoch 1830, training loss: 6.255350112915039 = 0.006041026208549738 + 1.0 * 6.249309062957764
Epoch 1830, val loss: 1.3539419174194336
Epoch 1840, training loss: 6.252301216125488 = 0.005964009091258049 + 1.0 * 6.246337413787842
Epoch 1840, val loss: 1.3567436933517456
Epoch 1850, training loss: 6.254976272583008 = 0.005887748673558235 + 1.0 * 6.249088287353516
Epoch 1850, val loss: 1.3595595359802246
Epoch 1860, training loss: 6.252553939819336 = 0.005814029835164547 + 1.0 * 6.246739864349365
Epoch 1860, val loss: 1.3623230457305908
Epoch 1870, training loss: 6.251250267028809 = 0.005741854198276997 + 1.0 * 6.245508193969727
Epoch 1870, val loss: 1.3651055097579956
Epoch 1880, training loss: 6.249863147735596 = 0.005670472048223019 + 1.0 * 6.244192600250244
Epoch 1880, val loss: 1.3678865432739258
Epoch 1890, training loss: 6.2540459632873535 = 0.005600745789706707 + 1.0 * 6.2484450340271
Epoch 1890, val loss: 1.3706109523773193
Epoch 1900, training loss: 6.248297691345215 = 0.0055320183746516705 + 1.0 * 6.2427659034729
Epoch 1900, val loss: 1.3732644319534302
Epoch 1910, training loss: 6.251116752624512 = 0.005465082358568907 + 1.0 * 6.245651721954346
Epoch 1910, val loss: 1.3760484457015991
Epoch 1920, training loss: 6.256715297698975 = 0.00539901340380311 + 1.0 * 6.251316070556641
Epoch 1920, val loss: 1.3785945177078247
Epoch 1930, training loss: 6.251148700714111 = 0.005334965884685516 + 1.0 * 6.245813846588135
Epoch 1930, val loss: 1.3813177347183228
Epoch 1940, training loss: 6.250201225280762 = 0.005272022448480129 + 1.0 * 6.244929313659668
Epoch 1940, val loss: 1.3840372562408447
Epoch 1950, training loss: 6.250227451324463 = 0.005209967959672213 + 1.0 * 6.2450175285339355
Epoch 1950, val loss: 1.386643648147583
Epoch 1960, training loss: 6.2481689453125 = 0.005149264354258776 + 1.0 * 6.2430195808410645
Epoch 1960, val loss: 1.3892452716827393
Epoch 1970, training loss: 6.254583358764648 = 0.005089571699500084 + 1.0 * 6.249493598937988
Epoch 1970, val loss: 1.391850233078003
Epoch 1980, training loss: 6.247811794281006 = 0.005031535867601633 + 1.0 * 6.2427802085876465
Epoch 1980, val loss: 1.3944053649902344
Epoch 1990, training loss: 6.24684476852417 = 0.004974928218871355 + 1.0 * 6.241869926452637
Epoch 1990, val loss: 1.3970558643341064
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8355297838692674
The final CL Acc:0.80247, 0.01145, The final GNN Acc:0.83817, 0.00301
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11530])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10464])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.546235084533691 = 1.9493592977523804 + 1.0 * 8.59687614440918
Epoch 0, val loss: 1.9564158916473389
Epoch 10, training loss: 10.535894393920898 = 1.9391663074493408 + 1.0 * 8.596728324890137
Epoch 10, val loss: 1.9454149007797241
Epoch 20, training loss: 10.52212142944336 = 1.9266241788864136 + 1.0 * 8.595497131347656
Epoch 20, val loss: 1.9315061569213867
Epoch 30, training loss: 10.49345588684082 = 1.9091920852661133 + 1.0 * 8.584263801574707
Epoch 30, val loss: 1.9118989706039429
Epoch 40, training loss: 10.397293090820312 = 1.8856559991836548 + 1.0 * 8.511636734008789
Epoch 40, val loss: 1.8861792087554932
Epoch 50, training loss: 10.012592315673828 = 1.8607220649719238 + 1.0 * 8.151869773864746
Epoch 50, val loss: 1.8606632947921753
Epoch 60, training loss: 9.779217720031738 = 1.8376548290252686 + 1.0 * 7.941562652587891
Epoch 60, val loss: 1.838978886604309
Epoch 70, training loss: 9.32450008392334 = 1.8200597763061523 + 1.0 * 7.5044403076171875
Epoch 70, val loss: 1.8227221965789795
Epoch 80, training loss: 9.007737159729004 = 1.8058334589004517 + 1.0 * 7.201903820037842
Epoch 80, val loss: 1.8090534210205078
Epoch 90, training loss: 8.874534606933594 = 1.7898834943771362 + 1.0 * 7.084651470184326
Epoch 90, val loss: 1.7939233779907227
Epoch 100, training loss: 8.728788375854492 = 1.7727264165878296 + 1.0 * 6.956062316894531
Epoch 100, val loss: 1.7791773080825806
Epoch 110, training loss: 8.594541549682617 = 1.756812334060669 + 1.0 * 6.837728977203369
Epoch 110, val loss: 1.7660267353057861
Epoch 120, training loss: 8.5108003616333 = 1.7402080297470093 + 1.0 * 6.77059268951416
Epoch 120, val loss: 1.752049207687378
Epoch 130, training loss: 8.443253517150879 = 1.7210315465927124 + 1.0 * 6.722221851348877
Epoch 130, val loss: 1.7356085777282715
Epoch 140, training loss: 8.3814697265625 = 1.6984741687774658 + 1.0 * 6.682995796203613
Epoch 140, val loss: 1.7163069248199463
Epoch 150, training loss: 8.32380485534668 = 1.6721879243850708 + 1.0 * 6.65161657333374
Epoch 150, val loss: 1.694046974182129
Epoch 160, training loss: 8.259190559387207 = 1.64225172996521 + 1.0 * 6.616939067840576
Epoch 160, val loss: 1.6686716079711914
Epoch 170, training loss: 8.200204849243164 = 1.6076196432113647 + 1.0 * 6.59258508682251
Epoch 170, val loss: 1.6394176483154297
Epoch 180, training loss: 8.14349365234375 = 1.5668902397155762 + 1.0 * 6.576602935791016
Epoch 180, val loss: 1.6051346063613892
Epoch 190, training loss: 8.081149101257324 = 1.5206283330917358 + 1.0 * 6.560520648956299
Epoch 190, val loss: 1.566183090209961
Epoch 200, training loss: 8.014772415161133 = 1.4688423871994019 + 1.0 * 6.545929908752441
Epoch 200, val loss: 1.522778868675232
Epoch 210, training loss: 7.945901870727539 = 1.4110840559005737 + 1.0 * 6.534817695617676
Epoch 210, val loss: 1.4748425483703613
Epoch 220, training loss: 7.8798089027404785 = 1.3481320142745972 + 1.0 * 6.531676769256592
Epoch 220, val loss: 1.4233378171920776
Epoch 230, training loss: 7.801383018493652 = 1.2830379009246826 + 1.0 * 6.518344879150391
Epoch 230, val loss: 1.3705532550811768
Epoch 240, training loss: 7.722653865814209 = 1.2159948348999023 + 1.0 * 6.506659030914307
Epoch 240, val loss: 1.3165067434310913
Epoch 250, training loss: 7.647137641906738 = 1.14792799949646 + 1.0 * 6.499209403991699
Epoch 250, val loss: 1.2624388933181763
Epoch 260, training loss: 7.581879615783691 = 1.0815608501434326 + 1.0 * 6.50031852722168
Epoch 260, val loss: 1.2107864618301392
Epoch 270, training loss: 7.508173942565918 = 1.0198006629943848 + 1.0 * 6.488373279571533
Epoch 270, val loss: 1.163467526435852
Epoch 280, training loss: 7.442286491394043 = 0.9622218608856201 + 1.0 * 6.480064868927002
Epoch 280, val loss: 1.1204164028167725
Epoch 290, training loss: 7.381275653839111 = 0.9088603258132935 + 1.0 * 6.472415447235107
Epoch 290, val loss: 1.0818688869476318
Epoch 300, training loss: 7.3265061378479 = 0.8599185943603516 + 1.0 * 6.466587543487549
Epoch 300, val loss: 1.0479285717010498
Epoch 310, training loss: 7.28288459777832 = 0.8159216642379761 + 1.0 * 6.466962814331055
Epoch 310, val loss: 1.0190339088439941
Epoch 320, training loss: 7.233883380889893 = 0.7768492698669434 + 1.0 * 6.457034111022949
Epoch 320, val loss: 0.9948639869689941
Epoch 330, training loss: 7.191124439239502 = 0.7413117289543152 + 1.0 * 6.449812889099121
Epoch 330, val loss: 0.9744153022766113
Epoch 340, training loss: 7.157532215118408 = 0.708640992641449 + 1.0 * 6.4488911628723145
Epoch 340, val loss: 0.9568744897842407
Epoch 350, training loss: 7.119132041931152 = 0.6783736944198608 + 1.0 * 6.440758228302002
Epoch 350, val loss: 0.941911518573761
Epoch 360, training loss: 7.089886665344238 = 0.6498196125030518 + 1.0 * 6.440067291259766
Epoch 360, val loss: 0.9289373159408569
Epoch 370, training loss: 7.054704666137695 = 0.6227276921272278 + 1.0 * 6.431976795196533
Epoch 370, val loss: 0.9175887107849121
Epoch 380, training loss: 7.025236129760742 = 0.596778154373169 + 1.0 * 6.428457736968994
Epoch 380, val loss: 0.9076041579246521
Epoch 390, training loss: 6.993533134460449 = 0.5716813206672668 + 1.0 * 6.421851634979248
Epoch 390, val loss: 0.8987355828285217
Epoch 400, training loss: 6.966039180755615 = 0.5471906661987305 + 1.0 * 6.418848514556885
Epoch 400, val loss: 0.8907718658447266
Epoch 410, training loss: 6.937273979187012 = 0.5232727527618408 + 1.0 * 6.414000988006592
Epoch 410, val loss: 0.8836847543716431
Epoch 420, training loss: 6.913035869598389 = 0.5000482201576233 + 1.0 * 6.41298770904541
Epoch 420, val loss: 0.8775194883346558
Epoch 430, training loss: 6.884984016418457 = 0.47720247507095337 + 1.0 * 6.407781600952148
Epoch 430, val loss: 0.8721035718917847
Epoch 440, training loss: 6.861311912536621 = 0.4549669325351715 + 1.0 * 6.406344890594482
Epoch 440, val loss: 0.8673896789550781
Epoch 450, training loss: 6.833897590637207 = 0.4332922697067261 + 1.0 * 6.400605201721191
Epoch 450, val loss: 0.8635497093200684
Epoch 460, training loss: 6.812085151672363 = 0.4121434688568115 + 1.0 * 6.399941921234131
Epoch 460, val loss: 0.8604259490966797
Epoch 470, training loss: 6.794271945953369 = 0.39168182015419006 + 1.0 * 6.402590274810791
Epoch 470, val loss: 0.8580439686775208
Epoch 480, training loss: 6.765405178070068 = 0.3719463348388672 + 1.0 * 6.393458843231201
Epoch 480, val loss: 0.8565101623535156
Epoch 490, training loss: 6.761246204376221 = 0.35294827818870544 + 1.0 * 6.408298015594482
Epoch 490, val loss: 0.8557054400444031
Epoch 500, training loss: 6.724911689758301 = 0.3347438871860504 + 1.0 * 6.390167713165283
Epoch 500, val loss: 0.8556724190711975
Epoch 510, training loss: 6.700379371643066 = 0.31733572483062744 + 1.0 * 6.3830437660217285
Epoch 510, val loss: 0.8564709424972534
Epoch 520, training loss: 6.6816840171813965 = 0.30062004923820496 + 1.0 * 6.381063938140869
Epoch 520, val loss: 0.8579572439193726
Epoch 530, training loss: 6.680535793304443 = 0.28464755415916443 + 1.0 * 6.395888328552246
Epoch 530, val loss: 0.8601091504096985
Epoch 540, training loss: 6.64845609664917 = 0.2697252035140991 + 1.0 * 6.378730773925781
Epoch 540, val loss: 0.8629897832870483
Epoch 550, training loss: 6.630202293395996 = 0.2556239068508148 + 1.0 * 6.374578475952148
Epoch 550, val loss: 0.8665518760681152
Epoch 560, training loss: 6.619797229766846 = 0.24226294457912445 + 1.0 * 6.37753438949585
Epoch 560, val loss: 0.8706713318824768
Epoch 570, training loss: 6.601217746734619 = 0.22965210676193237 + 1.0 * 6.371565818786621
Epoch 570, val loss: 0.8753305673599243
Epoch 580, training loss: 6.587289333343506 = 0.2178286612033844 + 1.0 * 6.369460582733154
Epoch 580, val loss: 0.8804774880409241
Epoch 590, training loss: 6.576391696929932 = 0.2066233903169632 + 1.0 * 6.369768142700195
Epoch 590, val loss: 0.8860740661621094
Epoch 600, training loss: 6.56684684753418 = 0.19611427187919617 + 1.0 * 6.37073278427124
Epoch 600, val loss: 0.8920987844467163
Epoch 610, training loss: 6.550020217895508 = 0.1862471103668213 + 1.0 * 6.363772869110107
Epoch 610, val loss: 0.8985360264778137
Epoch 620, training loss: 6.537368297576904 = 0.17692390084266663 + 1.0 * 6.36044454574585
Epoch 620, val loss: 0.9052363038063049
Epoch 630, training loss: 6.537855625152588 = 0.16814087331295013 + 1.0 * 6.369714736938477
Epoch 630, val loss: 0.9122614860534668
Epoch 640, training loss: 6.519924640655518 = 0.15988372266292572 + 1.0 * 6.36004114151001
Epoch 640, val loss: 0.9195494055747986
Epoch 650, training loss: 6.508111000061035 = 0.1521020531654358 + 1.0 * 6.356009006500244
Epoch 650, val loss: 0.9270716309547424
Epoch 660, training loss: 6.500068664550781 = 0.14472782611846924 + 1.0 * 6.355340957641602
Epoch 660, val loss: 0.9348487854003906
Epoch 670, training loss: 6.492053985595703 = 0.13777318596839905 + 1.0 * 6.354280948638916
Epoch 670, val loss: 0.9428474307060242
Epoch 680, training loss: 6.481877326965332 = 0.13122676312923431 + 1.0 * 6.350650787353516
Epoch 680, val loss: 0.9509711265563965
Epoch 690, training loss: 6.478025913238525 = 0.12502260506153107 + 1.0 * 6.35300350189209
Epoch 690, val loss: 0.9592461585998535
Epoch 700, training loss: 6.467346668243408 = 0.11914671957492828 + 1.0 * 6.348199844360352
Epoch 700, val loss: 0.9676439166069031
Epoch 710, training loss: 6.467594146728516 = 0.1135953813791275 + 1.0 * 6.35399866104126
Epoch 710, val loss: 0.9761293530464172
Epoch 720, training loss: 6.455655574798584 = 0.10835391283035278 + 1.0 * 6.347301483154297
Epoch 720, val loss: 0.9847883582115173
Epoch 730, training loss: 6.447455883026123 = 0.10339303314685822 + 1.0 * 6.344062805175781
Epoch 730, val loss: 0.9934717416763306
Epoch 740, training loss: 6.450388431549072 = 0.09867741167545319 + 1.0 * 6.351710796356201
Epoch 740, val loss: 1.0022008419036865
Epoch 750, training loss: 6.4416303634643555 = 0.09425410628318787 + 1.0 * 6.347376346588135
Epoch 750, val loss: 1.0110455751419067
Epoch 760, training loss: 6.430862903594971 = 0.0900510624051094 + 1.0 * 6.340811729431152
Epoch 760, val loss: 1.0198265314102173
Epoch 770, training loss: 6.425723075866699 = 0.08607133477926254 + 1.0 * 6.339651584625244
Epoch 770, val loss: 1.0287138223648071
Epoch 780, training loss: 6.431713104248047 = 0.08229299634695053 + 1.0 * 6.349420070648193
Epoch 780, val loss: 1.037663221359253
Epoch 790, training loss: 6.4160590171813965 = 0.07871878147125244 + 1.0 * 6.337340354919434
Epoch 790, val loss: 1.0464445352554321
Epoch 800, training loss: 6.415638446807861 = 0.07533246278762817 + 1.0 * 6.340305805206299
Epoch 800, val loss: 1.0553264617919922
Epoch 810, training loss: 6.408157825469971 = 0.07212815433740616 + 1.0 * 6.336029529571533
Epoch 810, val loss: 1.0641926527023315
Epoch 820, training loss: 6.4034504890441895 = 0.06908711791038513 + 1.0 * 6.3343634605407715
Epoch 820, val loss: 1.0729798078536987
Epoch 830, training loss: 6.408565521240234 = 0.06619318574666977 + 1.0 * 6.342372417449951
Epoch 830, val loss: 1.081747055053711
Epoch 840, training loss: 6.402029037475586 = 0.06346533447504044 + 1.0 * 6.338563919067383
Epoch 840, val loss: 1.0905131101608276
Epoch 850, training loss: 6.391951560974121 = 0.0608694814145565 + 1.0 * 6.331081867218018
Epoch 850, val loss: 1.099196195602417
Epoch 860, training loss: 6.388929843902588 = 0.05840161442756653 + 1.0 * 6.330528259277344
Epoch 860, val loss: 1.10787034034729
Epoch 870, training loss: 6.391051292419434 = 0.05605515465140343 + 1.0 * 6.334996223449707
Epoch 870, val loss: 1.116507649421692
Epoch 880, training loss: 6.382880687713623 = 0.05382123589515686 + 1.0 * 6.329059600830078
Epoch 880, val loss: 1.1250125169754028
Epoch 890, training loss: 6.377447128295898 = 0.05170172080397606 + 1.0 * 6.325745582580566
Epoch 890, val loss: 1.1335556507110596
Epoch 900, training loss: 6.377431869506836 = 0.049679119139909744 + 1.0 * 6.327752590179443
Epoch 900, val loss: 1.1420729160308838
Epoch 910, training loss: 6.383992671966553 = 0.04775648191571236 + 1.0 * 6.336236000061035
Epoch 910, val loss: 1.1504782438278198
Epoch 920, training loss: 6.372570514678955 = 0.04592675715684891 + 1.0 * 6.326643943786621
Epoch 920, val loss: 1.1588127613067627
Epoch 930, training loss: 6.3671650886535645 = 0.044201143085956573 + 1.0 * 6.322963714599609
Epoch 930, val loss: 1.167035460472107
Epoch 940, training loss: 6.3646416664123535 = 0.04255159944295883 + 1.0 * 6.322090148925781
Epoch 940, val loss: 1.175320029258728
Epoch 950, training loss: 6.3654303550720215 = 0.0409766286611557 + 1.0 * 6.324453830718994
Epoch 950, val loss: 1.1835373640060425
Epoch 960, training loss: 6.3597412109375 = 0.03948025032877922 + 1.0 * 6.320261001586914
Epoch 960, val loss: 1.1915618181228638
Epoch 970, training loss: 6.3584418296813965 = 0.038058966398239136 + 1.0 * 6.320383071899414
Epoch 970, val loss: 1.1995658874511719
Epoch 980, training loss: 6.358475685119629 = 0.03670286014676094 + 1.0 * 6.321773052215576
Epoch 980, val loss: 1.2074947357177734
Epoch 990, training loss: 6.3606791496276855 = 0.03540898486971855 + 1.0 * 6.325270175933838
Epoch 990, val loss: 1.2153271436691284
Epoch 1000, training loss: 6.3535990715026855 = 0.03418809175491333 + 1.0 * 6.319410800933838
Epoch 1000, val loss: 1.2230548858642578
Epoch 1010, training loss: 6.349670886993408 = 0.03302094340324402 + 1.0 * 6.316649913787842
Epoch 1010, val loss: 1.2306901216506958
Epoch 1020, training loss: 6.347628593444824 = 0.031904179602861404 + 1.0 * 6.3157243728637695
Epoch 1020, val loss: 1.238332986831665
Epoch 1030, training loss: 6.364510536193848 = 0.03083960711956024 + 1.0 * 6.3336710929870605
Epoch 1030, val loss: 1.245806097984314
Epoch 1040, training loss: 6.346171855926514 = 0.02982884831726551 + 1.0 * 6.316342830657959
Epoch 1040, val loss: 1.2532870769500732
Epoch 1050, training loss: 6.3427324295043945 = 0.028864629566669464 + 1.0 * 6.313867568969727
Epoch 1050, val loss: 1.260622501373291
Epoch 1060, training loss: 6.34052848815918 = 0.02794090285897255 + 1.0 * 6.312587738037109
Epoch 1060, val loss: 1.2679425477981567
Epoch 1070, training loss: 6.353786468505859 = 0.02706240490078926 + 1.0 * 6.326724052429199
Epoch 1070, val loss: 1.2752915620803833
Epoch 1080, training loss: 6.347301483154297 = 0.02621590532362461 + 1.0 * 6.321085453033447
Epoch 1080, val loss: 1.2821685075759888
Epoch 1090, training loss: 6.337116718292236 = 0.02542007341980934 + 1.0 * 6.311696529388428
Epoch 1090, val loss: 1.2891597747802734
Epoch 1100, training loss: 6.334817409515381 = 0.024654317647218704 + 1.0 * 6.3101630210876465
Epoch 1100, val loss: 1.2961393594741821
Epoch 1110, training loss: 6.332625865936279 = 0.023917825892567635 + 1.0 * 6.308708190917969
Epoch 1110, val loss: 1.303065538406372
Epoch 1120, training loss: 6.335730075836182 = 0.023210348561406136 + 1.0 * 6.312519550323486
Epoch 1120, val loss: 1.3099459409713745
Epoch 1130, training loss: 6.332301616668701 = 0.022536879405379295 + 1.0 * 6.309764862060547
Epoch 1130, val loss: 1.3166193962097168
Epoch 1140, training loss: 6.333526611328125 = 0.02189473621547222 + 1.0 * 6.311631679534912
Epoch 1140, val loss: 1.323273777961731
Epoch 1150, training loss: 6.327233791351318 = 0.021277979016304016 + 1.0 * 6.30595588684082
Epoch 1150, val loss: 1.3298568725585938
Epoch 1160, training loss: 6.326667785644531 = 0.020683834329247475 + 1.0 * 6.305984020233154
Epoch 1160, val loss: 1.3363933563232422
Epoch 1170, training loss: 6.342797756195068 = 0.020112335681915283 + 1.0 * 6.322685241699219
Epoch 1170, val loss: 1.3428082466125488
Epoch 1180, training loss: 6.335048198699951 = 0.01957370527088642 + 1.0 * 6.315474510192871
Epoch 1180, val loss: 1.349251627922058
Epoch 1190, training loss: 6.3240132331848145 = 0.019050247967243195 + 1.0 * 6.304963111877441
Epoch 1190, val loss: 1.3554794788360596
Epoch 1200, training loss: 6.323274612426758 = 0.018549844622612 + 1.0 * 6.30472469329834
Epoch 1200, val loss: 1.36167311668396
Epoch 1210, training loss: 6.322113513946533 = 0.018067607656121254 + 1.0 * 6.304045677185059
Epoch 1210, val loss: 1.3678375482559204
Epoch 1220, training loss: 6.329957008361816 = 0.01760338619351387 + 1.0 * 6.312353610992432
Epoch 1220, val loss: 1.3739100694656372
Epoch 1230, training loss: 6.322286605834961 = 0.017160985618829727 + 1.0 * 6.305125713348389
Epoch 1230, val loss: 1.3800057172775269
Epoch 1240, training loss: 6.321239471435547 = 0.016732066869735718 + 1.0 * 6.304507255554199
Epoch 1240, val loss: 1.3858928680419922
Epoch 1250, training loss: 6.3274102210998535 = 0.016322536394000053 + 1.0 * 6.311087608337402
Epoch 1250, val loss: 1.3918664455413818
Epoch 1260, training loss: 6.319256782531738 = 0.01592419482767582 + 1.0 * 6.303332805633545
Epoch 1260, val loss: 1.3975903987884521
Epoch 1270, training loss: 6.31647253036499 = 0.01554302591830492 + 1.0 * 6.300929546356201
Epoch 1270, val loss: 1.4034161567687988
Epoch 1280, training loss: 6.320713043212891 = 0.015173092484474182 + 1.0 * 6.305540084838867
Epoch 1280, val loss: 1.4090778827667236
Epoch 1290, training loss: 6.314995765686035 = 0.014820446260273457 + 1.0 * 6.300175189971924
Epoch 1290, val loss: 1.4147255420684814
Epoch 1300, training loss: 6.315680503845215 = 0.014479641802608967 + 1.0 * 6.301200866699219
Epoch 1300, val loss: 1.420289158821106
Epoch 1310, training loss: 6.312739372253418 = 0.014149295166134834 + 1.0 * 6.298590183258057
Epoch 1310, val loss: 1.425795078277588
Epoch 1320, training loss: 6.312777996063232 = 0.013831662945449352 + 1.0 * 6.298946380615234
Epoch 1320, val loss: 1.4313057661056519
Epoch 1330, training loss: 6.321580410003662 = 0.013523832894861698 + 1.0 * 6.308056354522705
Epoch 1330, val loss: 1.4367094039916992
Epoch 1340, training loss: 6.314026832580566 = 0.01322892215102911 + 1.0 * 6.300797939300537
Epoch 1340, val loss: 1.4420756101608276
Epoch 1350, training loss: 6.311150550842285 = 0.012943017296493053 + 1.0 * 6.298207759857178
Epoch 1350, val loss: 1.4473258256912231
Epoch 1360, training loss: 6.311127662658691 = 0.012666159309446812 + 1.0 * 6.298461437225342
Epoch 1360, val loss: 1.4525701999664307
Epoch 1370, training loss: 6.307643413543701 = 0.01239852886646986 + 1.0 * 6.2952446937561035
Epoch 1370, val loss: 1.4577455520629883
Epoch 1380, training loss: 6.325414180755615 = 0.012141671031713486 + 1.0 * 6.313272476196289
Epoch 1380, val loss: 1.4629002809524536
Epoch 1390, training loss: 6.306341171264648 = 0.011891915462911129 + 1.0 * 6.294449329376221
Epoch 1390, val loss: 1.4678443670272827
Epoch 1400, training loss: 6.306713581085205 = 0.01165216974914074 + 1.0 * 6.2950615882873535
Epoch 1400, val loss: 1.4728533029556274
Epoch 1410, training loss: 6.304008483886719 = 0.011419105343520641 + 1.0 * 6.29258918762207
Epoch 1410, val loss: 1.4778162240982056
Epoch 1420, training loss: 6.311077117919922 = 0.011191940866410732 + 1.0 * 6.299885272979736
Epoch 1420, val loss: 1.482708215713501
Epoch 1430, training loss: 6.307657241821289 = 0.010971936397254467 + 1.0 * 6.296685218811035
Epoch 1430, val loss: 1.4875599145889282
Epoch 1440, training loss: 6.303455829620361 = 0.010761253535747528 + 1.0 * 6.292694568634033
Epoch 1440, val loss: 1.4923405647277832
Epoch 1450, training loss: 6.300891399383545 = 0.010555206798017025 + 1.0 * 6.2903361320495605
Epoch 1450, val loss: 1.4971563816070557
Epoch 1460, training loss: 6.302806377410889 = 0.01035518478602171 + 1.0 * 6.29245138168335
Epoch 1460, val loss: 1.5019433498382568
Epoch 1470, training loss: 6.302812576293945 = 0.010161555372178555 + 1.0 * 6.292651176452637
Epoch 1470, val loss: 1.5065464973449707
Epoch 1480, training loss: 6.301107406616211 = 0.009973553940653801 + 1.0 * 6.291133880615234
Epoch 1480, val loss: 1.511193037033081
Epoch 1490, training loss: 6.308754920959473 = 0.009792628698050976 + 1.0 * 6.298962116241455
Epoch 1490, val loss: 1.5156829357147217
Epoch 1500, training loss: 6.301900386810303 = 0.009615171700716019 + 1.0 * 6.292285442352295
Epoch 1500, val loss: 1.5200729370117188
Epoch 1510, training loss: 6.297711372375488 = 0.009444989264011383 + 1.0 * 6.288266181945801
Epoch 1510, val loss: 1.5246005058288574
Epoch 1520, training loss: 6.300950527191162 = 0.00927786622196436 + 1.0 * 6.291672706604004
Epoch 1520, val loss: 1.5290205478668213
Epoch 1530, training loss: 6.297822952270508 = 0.00911598652601242 + 1.0 * 6.2887067794799805
Epoch 1530, val loss: 1.5334205627441406
Epoch 1540, training loss: 6.297975540161133 = 0.008959474042057991 + 1.0 * 6.289016246795654
Epoch 1540, val loss: 1.5377388000488281
Epoch 1550, training loss: 6.295278549194336 = 0.008806576952338219 + 1.0 * 6.286471843719482
Epoch 1550, val loss: 1.5420140027999878
Epoch 1560, training loss: 6.295823574066162 = 0.00865775067359209 + 1.0 * 6.287165641784668
Epoch 1560, val loss: 1.5463324785232544
Epoch 1570, training loss: 6.296816349029541 = 0.008512402884662151 + 1.0 * 6.288303852081299
Epoch 1570, val loss: 1.5505633354187012
Epoch 1580, training loss: 6.298302173614502 = 0.0083711426705122 + 1.0 * 6.289930820465088
Epoch 1580, val loss: 1.5547645092010498
Epoch 1590, training loss: 6.296999454498291 = 0.008234936743974686 + 1.0 * 6.288764476776123
Epoch 1590, val loss: 1.558819055557251
Epoch 1600, training loss: 6.294480323791504 = 0.008102198131382465 + 1.0 * 6.286377906799316
Epoch 1600, val loss: 1.562802791595459
Epoch 1610, training loss: 6.292795658111572 = 0.007972623221576214 + 1.0 * 6.284822940826416
Epoch 1610, val loss: 1.5668623447418213
Epoch 1620, training loss: 6.295212268829346 = 0.007846365682780743 + 1.0 * 6.287365913391113
Epoch 1620, val loss: 1.5708948373794556
Epoch 1630, training loss: 6.292311191558838 = 0.007723268121480942 + 1.0 * 6.284587860107422
Epoch 1630, val loss: 1.5749109983444214
Epoch 1640, training loss: 6.2984089851379395 = 0.007604115176945925 + 1.0 * 6.290804862976074
Epoch 1640, val loss: 1.5788079500198364
Epoch 1650, training loss: 6.292109489440918 = 0.007485966198146343 + 1.0 * 6.284623622894287
Epoch 1650, val loss: 1.5826960802078247
Epoch 1660, training loss: 6.290256977081299 = 0.0073730358853936195 + 1.0 * 6.282884120941162
Epoch 1660, val loss: 1.5866632461547852
Epoch 1670, training loss: 6.300556659698486 = 0.007262425031512976 + 1.0 * 6.293294429779053
Epoch 1670, val loss: 1.590519905090332
Epoch 1680, training loss: 6.289974212646484 = 0.007153843063861132 + 1.0 * 6.282820224761963
Epoch 1680, val loss: 1.594125747680664
Epoch 1690, training loss: 6.287072658538818 = 0.007049136329442263 + 1.0 * 6.280023574829102
Epoch 1690, val loss: 1.5979340076446533
Epoch 1700, training loss: 6.287315368652344 = 0.006945503409951925 + 1.0 * 6.280369758605957
Epoch 1700, val loss: 1.601699709892273
Epoch 1710, training loss: 6.297966003417969 = 0.006844754796475172 + 1.0 * 6.291121482849121
Epoch 1710, val loss: 1.6053885221481323
Epoch 1720, training loss: 6.289745330810547 = 0.006746342405676842 + 1.0 * 6.282999038696289
Epoch 1720, val loss: 1.6089837551116943
Epoch 1730, training loss: 6.29240608215332 = 0.006650653667747974 + 1.0 * 6.285755634307861
Epoch 1730, val loss: 1.612544298171997
Epoch 1740, training loss: 6.2865986824035645 = 0.006558733526617289 + 1.0 * 6.2800397872924805
Epoch 1740, val loss: 1.616212248802185
Epoch 1750, training loss: 6.292983531951904 = 0.0064675332978367805 + 1.0 * 6.286516189575195
Epoch 1750, val loss: 1.619672417640686
Epoch 1760, training loss: 6.284860610961914 = 0.006378644146025181 + 1.0 * 6.278481960296631
Epoch 1760, val loss: 1.6232609748840332
Epoch 1770, training loss: 6.283684730529785 = 0.0062920209020376205 + 1.0 * 6.277392864227295
Epoch 1770, val loss: 1.6267828941345215
Epoch 1780, training loss: 6.291762828826904 = 0.006207077763974667 + 1.0 * 6.285555839538574
Epoch 1780, val loss: 1.630309820175171
Epoch 1790, training loss: 6.284852981567383 = 0.006123177241533995 + 1.0 * 6.2787299156188965
Epoch 1790, val loss: 1.6336658000946045
Epoch 1800, training loss: 6.283759117126465 = 0.006042507477104664 + 1.0 * 6.277716636657715
Epoch 1800, val loss: 1.6370184421539307
Epoch 1810, training loss: 6.284022331237793 = 0.00596316484734416 + 1.0 * 6.278059005737305
Epoch 1810, val loss: 1.6404262781143188
Epoch 1820, training loss: 6.284994602203369 = 0.0058857048861682415 + 1.0 * 6.279109001159668
Epoch 1820, val loss: 1.6437820196151733
Epoch 1830, training loss: 6.280228614807129 = 0.005808938294649124 + 1.0 * 6.274419784545898
Epoch 1830, val loss: 1.6470391750335693
Epoch 1840, training loss: 6.2890496253967285 = 0.005733981728553772 + 1.0 * 6.283315658569336
Epoch 1840, val loss: 1.6503502130508423
Epoch 1850, training loss: 6.284305572509766 = 0.005661329720169306 + 1.0 * 6.27864408493042
Epoch 1850, val loss: 1.6535786390304565
Epoch 1860, training loss: 6.284012317657471 = 0.0055902861058712006 + 1.0 * 6.278421878814697
Epoch 1860, val loss: 1.6566941738128662
Epoch 1870, training loss: 6.28344202041626 = 0.005520656239241362 + 1.0 * 6.277921199798584
Epoch 1870, val loss: 1.6598703861236572
Epoch 1880, training loss: 6.281803131103516 = 0.005452547688037157 + 1.0 * 6.276350498199463
Epoch 1880, val loss: 1.6629869937896729
Epoch 1890, training loss: 6.283727645874023 = 0.005385893397033215 + 1.0 * 6.278341770172119
Epoch 1890, val loss: 1.666094183921814
Epoch 1900, training loss: 6.278132438659668 = 0.005320984870195389 + 1.0 * 6.272811412811279
Epoch 1900, val loss: 1.669220209121704
Epoch 1910, training loss: 6.278025150299072 = 0.005257431883364916 + 1.0 * 6.272767543792725
Epoch 1910, val loss: 1.672296404838562
Epoch 1920, training loss: 6.28148889541626 = 0.005194359924644232 + 1.0 * 6.276294708251953
Epoch 1920, val loss: 1.675350308418274
Epoch 1930, training loss: 6.280679702758789 = 0.005132694263011217 + 1.0 * 6.275547027587891
Epoch 1930, val loss: 1.6783555746078491
Epoch 1940, training loss: 6.278026580810547 = 0.005072792060673237 + 1.0 * 6.272953987121582
Epoch 1940, val loss: 1.6812602281570435
Epoch 1950, training loss: 6.284952640533447 = 0.005013499408960342 + 1.0 * 6.2799391746521
Epoch 1950, val loss: 1.6842414140701294
Epoch 1960, training loss: 6.278115749359131 = 0.004956854972988367 + 1.0 * 6.273159027099609
Epoch 1960, val loss: 1.6871507167816162
Epoch 1970, training loss: 6.276776313781738 = 0.0049009621143341064 + 1.0 * 6.271875381469727
Epoch 1970, val loss: 1.6900049448013306
Epoch 1980, training loss: 6.276134490966797 = 0.004845418967306614 + 1.0 * 6.271288871765137
Epoch 1980, val loss: 1.692831039428711
Epoch 1990, training loss: 6.2765326499938965 = 0.004790907260030508 + 1.0 * 6.27174186706543
Epoch 1990, val loss: 1.69573974609375
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 10.542230606079102 = 1.9453637599945068 + 1.0 * 8.596866607666016
Epoch 0, val loss: 1.945561170578003
Epoch 10, training loss: 10.53218936920166 = 1.935498595237732 + 1.0 * 8.596691131591797
Epoch 10, val loss: 1.93576180934906
Epoch 20, training loss: 10.519116401672363 = 1.9238922595977783 + 1.0 * 8.595224380493164
Epoch 20, val loss: 1.9242192506790161
Epoch 30, training loss: 10.491043090820312 = 1.908257007598877 + 1.0 * 8.582786560058594
Epoch 30, val loss: 1.9087506532669067
Epoch 40, training loss: 10.392904281616211 = 1.887310266494751 + 1.0 * 8.505594253540039
Epoch 40, val loss: 1.8888931274414062
Epoch 50, training loss: 10.043777465820312 = 1.8637580871582031 + 1.0 * 8.18001937866211
Epoch 50, val loss: 1.867422342300415
Epoch 60, training loss: 9.790104866027832 = 1.8414477109909058 + 1.0 * 7.948657512664795
Epoch 60, val loss: 1.8475795984268188
Epoch 70, training loss: 9.292999267578125 = 1.8235549926757812 + 1.0 * 7.469444274902344
Epoch 70, val loss: 1.831519603729248
Epoch 80, training loss: 8.926690101623535 = 1.8098419904708862 + 1.0 * 7.116848468780518
Epoch 80, val loss: 1.8185005187988281
Epoch 90, training loss: 8.74834156036377 = 1.7939023971557617 + 1.0 * 6.954439163208008
Epoch 90, val loss: 1.8041794300079346
Epoch 100, training loss: 8.624481201171875 = 1.7762737274169922 + 1.0 * 6.848206996917725
Epoch 100, val loss: 1.7894237041473389
Epoch 110, training loss: 8.54526424407959 = 1.7586438655853271 + 1.0 * 6.786620140075684
Epoch 110, val loss: 1.77391517162323
Epoch 120, training loss: 8.478649139404297 = 1.7403056621551514 + 1.0 * 6.738343238830566
Epoch 120, val loss: 1.7572869062423706
Epoch 130, training loss: 8.419055938720703 = 1.7208186388015747 + 1.0 * 6.69823694229126
Epoch 130, val loss: 1.7399225234985352
Epoch 140, training loss: 8.361146926879883 = 1.6993637084960938 + 1.0 * 6.661782741546631
Epoch 140, val loss: 1.7216899394989014
Epoch 150, training loss: 8.304866790771484 = 1.6750752925872803 + 1.0 * 6.629791736602783
Epoch 150, val loss: 1.7016183137893677
Epoch 160, training loss: 8.251432418823242 = 1.6469895839691162 + 1.0 * 6.604443073272705
Epoch 160, val loss: 1.6786688566207886
Epoch 170, training loss: 8.194905281066895 = 1.6150447130203247 + 1.0 * 6.579860210418701
Epoch 170, val loss: 1.652708649635315
Epoch 180, training loss: 8.14042854309082 = 1.5790470838546753 + 1.0 * 6.561381816864014
Epoch 180, val loss: 1.6232882738113403
Epoch 190, training loss: 8.090192794799805 = 1.5387781858444214 + 1.0 * 6.551414489746094
Epoch 190, val loss: 1.5904901027679443
Epoch 200, training loss: 8.028416633605957 = 1.49537193775177 + 1.0 * 6.533044338226318
Epoch 200, val loss: 1.5551038980484009
Epoch 210, training loss: 7.9688615798950195 = 1.4487360715866089 + 1.0 * 6.520125389099121
Epoch 210, val loss: 1.5172131061553955
Epoch 220, training loss: 7.907594680786133 = 1.399126648902893 + 1.0 * 6.508468151092529
Epoch 220, val loss: 1.4770985841751099
Epoch 230, training loss: 7.852251052856445 = 1.347538709640503 + 1.0 * 6.5047125816345215
Epoch 230, val loss: 1.4356060028076172
Epoch 240, training loss: 7.783960819244385 = 1.2955104112625122 + 1.0 * 6.488450527191162
Epoch 240, val loss: 1.394039273262024
Epoch 250, training loss: 7.722138404846191 = 1.242955207824707 + 1.0 * 6.479183197021484
Epoch 250, val loss: 1.3524436950683594
Epoch 260, training loss: 7.665839195251465 = 1.190347671508789 + 1.0 * 6.475491523742676
Epoch 260, val loss: 1.311023235321045
Epoch 270, training loss: 7.604461669921875 = 1.1389057636260986 + 1.0 * 6.465555667877197
Epoch 270, val loss: 1.2710158824920654
Epoch 280, training loss: 7.545270919799805 = 1.088835597038269 + 1.0 * 6.456435203552246
Epoch 280, val loss: 1.2325571775436401
Epoch 290, training loss: 7.489359378814697 = 1.0399260520935059 + 1.0 * 6.449433326721191
Epoch 290, val loss: 1.1954625844955444
Epoch 300, training loss: 7.448678493499756 = 0.9932664036750793 + 1.0 * 6.455411911010742
Epoch 300, val loss: 1.1604944467544556
Epoch 310, training loss: 7.390296936035156 = 0.9492183923721313 + 1.0 * 6.4410786628723145
Epoch 310, val loss: 1.12871515750885
Epoch 320, training loss: 7.340150833129883 = 0.9074518084526062 + 1.0 * 6.432699203491211
Epoch 320, val loss: 1.0992710590362549
Epoch 330, training loss: 7.294172763824463 = 0.8673847317695618 + 1.0 * 6.426787853240967
Epoch 330, val loss: 1.071813941001892
Epoch 340, training loss: 7.250694274902344 = 0.8290388584136963 + 1.0 * 6.421655654907227
Epoch 340, val loss: 1.0465611219406128
Epoch 350, training loss: 7.219868183135986 = 0.7924836277961731 + 1.0 * 6.427384376525879
Epoch 350, val loss: 1.0234568119049072
Epoch 360, training loss: 7.173117637634277 = 0.7580815553665161 + 1.0 * 6.415036201477051
Epoch 360, val loss: 1.0027738809585571
Epoch 370, training loss: 7.136386394500732 = 0.7254990935325623 + 1.0 * 6.410887241363525
Epoch 370, val loss: 0.9841458797454834
Epoch 380, training loss: 7.099672794342041 = 0.694261372089386 + 1.0 * 6.405411243438721
Epoch 380, val loss: 0.9671002626419067
Epoch 390, training loss: 7.066493034362793 = 0.6643428206443787 + 1.0 * 6.4021501541137695
Epoch 390, val loss: 0.9514610171318054
Epoch 400, training loss: 7.037922382354736 = 0.635775625705719 + 1.0 * 6.402146816253662
Epoch 400, val loss: 0.9372802376747131
Epoch 410, training loss: 7.003960609436035 = 0.6083748936653137 + 1.0 * 6.395585536956787
Epoch 410, val loss: 0.924446702003479
Epoch 420, training loss: 6.974645137786865 = 0.5819526314735413 + 1.0 * 6.392692565917969
Epoch 420, val loss: 0.9125675559043884
Epoch 430, training loss: 6.949573516845703 = 0.5564406514167786 + 1.0 * 6.39313268661499
Epoch 430, val loss: 0.9015839695930481
Epoch 440, training loss: 6.918546676635742 = 0.531912088394165 + 1.0 * 6.386634826660156
Epoch 440, val loss: 0.8917368054389954
Epoch 450, training loss: 6.892122268676758 = 0.5081291198730469 + 1.0 * 6.383993148803711
Epoch 450, val loss: 0.8828109502792358
Epoch 460, training loss: 6.8750834465026855 = 0.485111266374588 + 1.0 * 6.38997220993042
Epoch 460, val loss: 0.8746103048324585
Epoch 470, training loss: 6.844329357147217 = 0.4628790020942688 + 1.0 * 6.381450176239014
Epoch 470, val loss: 0.8671726584434509
Epoch 480, training loss: 6.818342685699463 = 0.441449910402298 + 1.0 * 6.376892566680908
Epoch 480, val loss: 0.8607386350631714
Epoch 490, training loss: 6.796300888061523 = 0.42061281204223633 + 1.0 * 6.375688076019287
Epoch 490, val loss: 0.8548755645751953
Epoch 500, training loss: 6.774610996246338 = 0.40034976601600647 + 1.0 * 6.374261379241943
Epoch 500, val loss: 0.8496270179748535
Epoch 510, training loss: 6.752014636993408 = 0.3806452453136444 + 1.0 * 6.371369361877441
Epoch 510, val loss: 0.8451414108276367
Epoch 520, training loss: 6.733351707458496 = 0.3614749610424042 + 1.0 * 6.3718767166137695
Epoch 520, val loss: 0.8411787152290344
Epoch 530, training loss: 6.709702491760254 = 0.3428266644477844 + 1.0 * 6.366875648498535
Epoch 530, val loss: 0.8376876711845398
Epoch 540, training loss: 6.689879417419434 = 0.3246798813343048 + 1.0 * 6.365199565887451
Epoch 540, val loss: 0.83487868309021
Epoch 550, training loss: 6.669922828674316 = 0.3070354163646698 + 1.0 * 6.362887382507324
Epoch 550, val loss: 0.8326411843299866
Epoch 560, training loss: 6.6515326499938965 = 0.2899492383003235 + 1.0 * 6.361583232879639
Epoch 560, val loss: 0.8308149576187134
Epoch 570, training loss: 6.634461879730225 = 0.27353769540786743 + 1.0 * 6.360924243927002
Epoch 570, val loss: 0.8298051357269287
Epoch 580, training loss: 6.6166839599609375 = 0.25776904821395874 + 1.0 * 6.358914852142334
Epoch 580, val loss: 0.829438328742981
Epoch 590, training loss: 6.601954460144043 = 0.24267072975635529 + 1.0 * 6.359283924102783
Epoch 590, val loss: 0.8298178911209106
Epoch 600, training loss: 6.582132816314697 = 0.2282678633928299 + 1.0 * 6.353865146636963
Epoch 600, val loss: 0.8309118747711182
Epoch 610, training loss: 6.573561191558838 = 0.21462062001228333 + 1.0 * 6.358940601348877
Epoch 610, val loss: 0.8325639963150024
Epoch 620, training loss: 6.560933589935303 = 0.20180287957191467 + 1.0 * 6.359130859375
Epoch 620, val loss: 0.834913969039917
Epoch 630, training loss: 6.542036056518555 = 0.18974949419498444 + 1.0 * 6.352286338806152
Epoch 630, val loss: 0.8378661870956421
Epoch 640, training loss: 6.526459217071533 = 0.17848895490169525 + 1.0 * 6.347970485687256
Epoch 640, val loss: 0.841461718082428
Epoch 650, training loss: 6.5187153816223145 = 0.1679307520389557 + 1.0 * 6.350784778594971
Epoch 650, val loss: 0.8455312848091125
Epoch 660, training loss: 6.508645057678223 = 0.15807199478149414 + 1.0 * 6.3505730628967285
Epoch 660, val loss: 0.8498547077178955
Epoch 670, training loss: 6.492423057556152 = 0.14891521632671356 + 1.0 * 6.343507766723633
Epoch 670, val loss: 0.8547393083572388
Epoch 680, training loss: 6.4831414222717285 = 0.1403646618127823 + 1.0 * 6.342776775360107
Epoch 680, val loss: 0.8599686026573181
Epoch 690, training loss: 6.4844841957092285 = 0.1323797106742859 + 1.0 * 6.352104663848877
Epoch 690, val loss: 0.8654775619506836
Epoch 700, training loss: 6.467779159545898 = 0.12493615597486496 + 1.0 * 6.342843055725098
Epoch 700, val loss: 0.870971143245697
Epoch 710, training loss: 6.458272457122803 = 0.11802300810813904 + 1.0 * 6.340249538421631
Epoch 710, val loss: 0.8768672347068787
Epoch 720, training loss: 6.448741912841797 = 0.11156293749809265 + 1.0 * 6.337179183959961
Epoch 720, val loss: 0.8828707933425903
Epoch 730, training loss: 6.443142890930176 = 0.10551398992538452 + 1.0 * 6.3376288414001465
Epoch 730, val loss: 0.8890288472175598
Epoch 740, training loss: 6.435671329498291 = 0.09986256808042526 + 1.0 * 6.335808753967285
Epoch 740, val loss: 0.8950864672660828
Epoch 750, training loss: 6.434487819671631 = 0.09459083527326584 + 1.0 * 6.339897155761719
Epoch 750, val loss: 0.9014289975166321
Epoch 760, training loss: 6.425330638885498 = 0.08967003226280212 + 1.0 * 6.335660457611084
Epoch 760, val loss: 0.9076099991798401
Epoch 770, training loss: 6.41615629196167 = 0.08507771044969559 + 1.0 * 6.33107852935791
Epoch 770, val loss: 0.9141181111335754
Epoch 780, training loss: 6.410317897796631 = 0.08076886087656021 + 1.0 * 6.3295488357543945
Epoch 780, val loss: 0.9206064939498901
Epoch 790, training loss: 6.4070539474487305 = 0.07672268152236938 + 1.0 * 6.330331325531006
Epoch 790, val loss: 0.9272283315658569
Epoch 800, training loss: 6.409663200378418 = 0.07292740046977997 + 1.0 * 6.336735725402832
Epoch 800, val loss: 0.9336416721343994
Epoch 810, training loss: 6.398658752441406 = 0.06939566135406494 + 1.0 * 6.329263210296631
Epoch 810, val loss: 0.9401538372039795
Epoch 820, training loss: 6.392133712768555 = 0.06608158349990845 + 1.0 * 6.326052188873291
Epoch 820, val loss: 0.9467719197273254
Epoch 830, training loss: 6.388034820556641 = 0.06296509504318237 + 1.0 * 6.325069904327393
Epoch 830, val loss: 0.953292191028595
Epoch 840, training loss: 6.390326499938965 = 0.060038428753614426 + 1.0 * 6.330287933349609
Epoch 840, val loss: 0.9597850441932678
Epoch 850, training loss: 6.381882667541504 = 0.0572831816971302 + 1.0 * 6.324599266052246
Epoch 850, val loss: 0.966259777545929
Epoch 860, training loss: 6.377975940704346 = 0.05470264330506325 + 1.0 * 6.323273181915283
Epoch 860, val loss: 0.972768247127533
Epoch 870, training loss: 6.374157905578613 = 0.0522773303091526 + 1.0 * 6.321880340576172
Epoch 870, val loss: 0.9792556166648865
Epoch 880, training loss: 6.375800132751465 = 0.04999120905995369 + 1.0 * 6.325809001922607
Epoch 880, val loss: 0.9857171177864075
Epoch 890, training loss: 6.371180534362793 = 0.04784617945551872 + 1.0 * 6.323334217071533
Epoch 890, val loss: 0.9920387268066406
Epoch 900, training loss: 6.365592002868652 = 0.04582414776086807 + 1.0 * 6.319767951965332
Epoch 900, val loss: 0.9984642863273621
Epoch 910, training loss: 6.361516952514648 = 0.043923910707235336 + 1.0 * 6.317593097686768
Epoch 910, val loss: 1.004847526550293
