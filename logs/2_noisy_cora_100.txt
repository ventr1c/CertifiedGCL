Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10504])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6372680664062 = 1.9572006464004517 + 100.0 * 8.596800804138184
Epoch 0, val loss: 1.9591553211212158
Epoch 10, training loss: 861.5016479492188 = 1.9476288557052612 + 100.0 * 8.595540046691895
Epoch 10, val loss: 1.9495933055877686
Epoch 20, training loss: 860.4835205078125 = 1.9356648921966553 + 100.0 * 8.585478782653809
Epoch 20, val loss: 1.9371650218963623
Epoch 30, training loss: 853.9450073242188 = 1.9198594093322754 + 100.0 * 8.520251274108887
Epoch 30, val loss: 1.920318365097046
Epoch 40, training loss: 819.77001953125 = 1.902475118637085 + 100.0 * 8.178675651550293
Epoch 40, val loss: 1.902486801147461
Epoch 50, training loss: 763.0802001953125 = 1.8849689960479736 + 100.0 * 7.611952781677246
Epoch 50, val loss: 1.8863056898117065
Epoch 60, training loss: 733.0866088867188 = 1.8722821474075317 + 100.0 * 7.312143325805664
Epoch 60, val loss: 1.8750478029251099
Epoch 70, training loss: 710.1923217773438 = 1.8613357543945312 + 100.0 * 7.083309650421143
Epoch 70, val loss: 1.8648426532745361
Epoch 80, training loss: 692.7548828125 = 1.8505431413650513 + 100.0 * 6.909043788909912
Epoch 80, val loss: 1.8546876907348633
Epoch 90, training loss: 681.5642700195312 = 1.84027898311615 + 100.0 * 6.797239780426025
Epoch 90, val loss: 1.8451508283615112
Epoch 100, training loss: 673.50830078125 = 1.831096887588501 + 100.0 * 6.716771602630615
Epoch 100, val loss: 1.8368761539459229
Epoch 110, training loss: 667.5076904296875 = 1.8231420516967773 + 100.0 * 6.656845569610596
Epoch 110, val loss: 1.8293561935424805
Epoch 120, training loss: 663.1364135742188 = 1.8157739639282227 + 100.0 * 6.613205909729004
Epoch 120, val loss: 1.8222222328186035
Epoch 130, training loss: 659.2175903320312 = 1.808493733406067 + 100.0 * 6.574090957641602
Epoch 130, val loss: 1.8148618936538696
Epoch 140, training loss: 655.9662475585938 = 1.8011847734451294 + 100.0 * 6.541650295257568
Epoch 140, val loss: 1.8075603246688843
Epoch 150, training loss: 653.1179809570312 = 1.7939727306365967 + 100.0 * 6.513240337371826
Epoch 150, val loss: 1.8004125356674194
Epoch 160, training loss: 650.9095458984375 = 1.786464810371399 + 100.0 * 6.4912309646606445
Epoch 160, val loss: 1.7931575775146484
Epoch 170, training loss: 648.8636474609375 = 1.7785285711288452 + 100.0 * 6.470851421356201
Epoch 170, val loss: 1.7855064868927002
Epoch 180, training loss: 647.157470703125 = 1.770062804222107 + 100.0 * 6.453873634338379
Epoch 180, val loss: 1.7775405645370483
Epoch 190, training loss: 645.7019653320312 = 1.7609496116638184 + 100.0 * 6.439410209655762
Epoch 190, val loss: 1.7691650390625
Epoch 200, training loss: 645.1829833984375 = 1.7510695457458496 + 100.0 * 6.434319019317627
Epoch 200, val loss: 1.7601869106292725
Epoch 210, training loss: 643.4747314453125 = 1.740197777748108 + 100.0 * 6.4173455238342285
Epoch 210, val loss: 1.750422716140747
Epoch 220, training loss: 642.4559936523438 = 1.7285133600234985 + 100.0 * 6.4072747230529785
Epoch 220, val loss: 1.7400647401809692
Epoch 230, training loss: 641.5316772460938 = 1.7158689498901367 + 100.0 * 6.398158073425293
Epoch 230, val loss: 1.7289280891418457
Epoch 240, training loss: 641.1236572265625 = 1.7022703886032104 + 100.0 * 6.394213676452637
Epoch 240, val loss: 1.7169536352157593
Epoch 250, training loss: 639.9871826171875 = 1.687313437461853 + 100.0 * 6.382998466491699
Epoch 250, val loss: 1.7040499448776245
Epoch 260, training loss: 639.2987670898438 = 1.6713905334472656 + 100.0 * 6.3762736320495605
Epoch 260, val loss: 1.6902350187301636
Epoch 270, training loss: 638.7322387695312 = 1.6543302536010742 + 100.0 * 6.370779037475586
Epoch 270, val loss: 1.6755398511886597
Epoch 280, training loss: 638.0577392578125 = 1.6359894275665283 + 100.0 * 6.364217281341553
Epoch 280, val loss: 1.6598937511444092
Epoch 290, training loss: 637.6084594726562 = 1.616650938987732 + 100.0 * 6.359918117523193
Epoch 290, val loss: 1.6433744430541992
Epoch 300, training loss: 637.025390625 = 1.5961247682571411 + 100.0 * 6.354292392730713
Epoch 300, val loss: 1.626115322113037
Epoch 310, training loss: 637.044921875 = 1.5748038291931152 + 100.0 * 6.354701042175293
Epoch 310, val loss: 1.608114242553711
Epoch 320, training loss: 636.0830078125 = 1.5522469282150269 + 100.0 * 6.34530782699585
Epoch 320, val loss: 1.5895098447799683
Epoch 330, training loss: 635.6571655273438 = 1.5291101932525635 + 100.0 * 6.341280460357666
Epoch 330, val loss: 1.5705037117004395
Epoch 340, training loss: 635.2277221679688 = 1.5053222179412842 + 100.0 * 6.337224006652832
Epoch 340, val loss: 1.5512148141860962
Epoch 350, training loss: 635.5516357421875 = 1.4810277223587036 + 100.0 * 6.3407063484191895
Epoch 350, val loss: 1.5316241979599
Epoch 360, training loss: 634.641845703125 = 1.4560768604278564 + 100.0 * 6.331858158111572
Epoch 360, val loss: 1.5120195150375366
Epoch 370, training loss: 634.1410522460938 = 1.4308972358703613 + 100.0 * 6.327101230621338
Epoch 370, val loss: 1.492525339126587
Epoch 380, training loss: 633.8054809570312 = 1.4056785106658936 + 100.0 * 6.323997974395752
Epoch 380, val loss: 1.4732611179351807
Epoch 390, training loss: 633.6429443359375 = 1.3804845809936523 + 100.0 * 6.322624683380127
Epoch 390, val loss: 1.4542746543884277
Epoch 400, training loss: 633.6583251953125 = 1.3550853729248047 + 100.0 * 6.323032379150391
Epoch 400, val loss: 1.4357000589370728
Epoch 410, training loss: 633.0371704101562 = 1.3299304246902466 + 100.0 * 6.31707239151001
Epoch 410, val loss: 1.4173998832702637
Epoch 420, training loss: 632.6548461914062 = 1.3049325942993164 + 100.0 * 6.3134989738464355
Epoch 420, val loss: 1.3995736837387085
Epoch 430, training loss: 632.38427734375 = 1.2802172899246216 + 100.0 * 6.311040878295898
Epoch 430, val loss: 1.3823816776275635
Epoch 440, training loss: 632.2672119140625 = 1.2555830478668213 + 100.0 * 6.310116767883301
Epoch 440, val loss: 1.3654588460922241
Epoch 450, training loss: 631.9163818359375 = 1.2313477993011475 + 100.0 * 6.306850433349609
Epoch 450, val loss: 1.3489481210708618
Epoch 460, training loss: 631.6446533203125 = 1.2075039148330688 + 100.0 * 6.304371356964111
Epoch 460, val loss: 1.3331701755523682
Epoch 470, training loss: 631.364013671875 = 1.1840620040893555 + 100.0 * 6.301799297332764
Epoch 470, val loss: 1.318015217781067
Epoch 480, training loss: 631.11962890625 = 1.1610623598098755 + 100.0 * 6.299585342407227
Epoch 480, val loss: 1.3033819198608398
Epoch 490, training loss: 631.0211791992188 = 1.1385107040405273 + 100.0 * 6.298826694488525
Epoch 490, val loss: 1.2893379926681519
Epoch 500, training loss: 630.937255859375 = 1.116239070892334 + 100.0 * 6.298210620880127
Epoch 500, val loss: 1.275719165802002
Epoch 510, training loss: 630.5892333984375 = 1.0943138599395752 + 100.0 * 6.294949054718018
Epoch 510, val loss: 1.2625141143798828
Epoch 520, training loss: 630.3150634765625 = 1.072901964187622 + 100.0 * 6.292421817779541
Epoch 520, val loss: 1.2499277591705322
Epoch 530, training loss: 630.0768432617188 = 1.0521571636199951 + 100.0 * 6.290246486663818
Epoch 530, val loss: 1.2381548881530762
Epoch 540, training loss: 629.9806518554688 = 1.03183913230896 + 100.0 * 6.289487838745117
Epoch 540, val loss: 1.22688889503479
Epoch 550, training loss: 629.7537231445312 = 1.011942744255066 + 100.0 * 6.287417411804199
Epoch 550, val loss: 1.216078758239746
Epoch 560, training loss: 629.69873046875 = 0.9924107789993286 + 100.0 * 6.287063121795654
Epoch 560, val loss: 1.2055522203445435
Epoch 570, training loss: 629.3504028320312 = 0.973414421081543 + 100.0 * 6.2837700843811035
Epoch 570, val loss: 1.1956462860107422
Epoch 580, training loss: 629.254150390625 = 0.9548804759979248 + 100.0 * 6.282992362976074
Epoch 580, val loss: 1.1863104104995728
Epoch 590, training loss: 629.0884399414062 = 0.9365447163581848 + 100.0 * 6.281519412994385
Epoch 590, val loss: 1.1773085594177246
Epoch 600, training loss: 629.0073852539062 = 0.9185457229614258 + 100.0 * 6.280888557434082
Epoch 600, val loss: 1.1683845520019531
Epoch 610, training loss: 628.718505859375 = 0.9010084271430969 + 100.0 * 6.278175354003906
Epoch 610, val loss: 1.1599479913711548
Epoch 620, training loss: 628.54541015625 = 0.8839381337165833 + 100.0 * 6.276615142822266
Epoch 620, val loss: 1.1519724130630493
Epoch 630, training loss: 628.388671875 = 0.8671817183494568 + 100.0 * 6.275215148925781
Epoch 630, val loss: 1.1443880796432495
Epoch 640, training loss: 629.2511596679688 = 0.8507645726203918 + 100.0 * 6.284003734588623
Epoch 640, val loss: 1.1369620561599731
Epoch 650, training loss: 628.58154296875 = 0.8342031240463257 + 100.0 * 6.277473449707031
Epoch 650, val loss: 1.1296780109405518
Epoch 660, training loss: 628.0657348632812 = 0.8179092407226562 + 100.0 * 6.272478103637695
Epoch 660, val loss: 1.1224154233932495
Epoch 670, training loss: 627.8236694335938 = 0.8020002841949463 + 100.0 * 6.270216464996338
Epoch 670, val loss: 1.115627646446228
Epoch 680, training loss: 627.6973876953125 = 0.7863428592681885 + 100.0 * 6.269110679626465
Epoch 680, val loss: 1.1088967323303223
Epoch 690, training loss: 628.4072875976562 = 0.7708922028541565 + 100.0 * 6.276364326477051
Epoch 690, val loss: 1.1022247076034546
Epoch 700, training loss: 627.7493286132812 = 0.7552438974380493 + 100.0 * 6.2699408531188965
Epoch 700, val loss: 1.0957317352294922
Epoch 710, training loss: 627.3865356445312 = 0.7400161623954773 + 100.0 * 6.266465663909912
Epoch 710, val loss: 1.089395523071289
Epoch 720, training loss: 627.4141845703125 = 0.7248508930206299 + 100.0 * 6.26689338684082
Epoch 720, val loss: 1.0828642845153809
Epoch 730, training loss: 627.09619140625 = 0.7098435163497925 + 100.0 * 6.263863563537598
Epoch 730, val loss: 1.0769885778427124
Epoch 740, training loss: 627.0799560546875 = 0.694979727268219 + 100.0 * 6.263849258422852
Epoch 740, val loss: 1.070835828781128
Epoch 750, training loss: 627.5415649414062 = 0.6803173422813416 + 100.0 * 6.268612384796143
Epoch 750, val loss: 1.0649397373199463
Epoch 760, training loss: 626.8025512695312 = 0.665475070476532 + 100.0 * 6.26137113571167
Epoch 760, val loss: 1.0590741634368896
Epoch 770, training loss: 626.6514282226562 = 0.6509679555892944 + 100.0 * 6.260004997253418
Epoch 770, val loss: 1.0532866716384888
Epoch 780, training loss: 626.59228515625 = 0.6366740465164185 + 100.0 * 6.259556293487549
Epoch 780, val loss: 1.047650933265686
Epoch 790, training loss: 626.7701416015625 = 0.6223678588867188 + 100.0 * 6.261477947235107
Epoch 790, val loss: 1.0423232316970825
Epoch 800, training loss: 626.29736328125 = 0.6082988977432251 + 100.0 * 6.256890773773193
Epoch 800, val loss: 1.0366650819778442
Epoch 810, training loss: 626.2184448242188 = 0.5944640636444092 + 100.0 * 6.256239414215088
Epoch 810, val loss: 1.0316660404205322
Epoch 820, training loss: 626.110107421875 = 0.5807276368141174 + 100.0 * 6.255293846130371
Epoch 820, val loss: 1.0263829231262207
Epoch 830, training loss: 627.0070190429688 = 0.5673362016677856 + 100.0 * 6.264397144317627
Epoch 830, val loss: 1.0215529203414917
Epoch 840, training loss: 626.1703491210938 = 0.5536961555480957 + 100.0 * 6.256166458129883
Epoch 840, val loss: 1.0167955160140991
Epoch 850, training loss: 625.7686767578125 = 0.5405352711677551 + 100.0 * 6.252281665802002
Epoch 850, val loss: 1.0122089385986328
Epoch 860, training loss: 625.6551513671875 = 0.527603268623352 + 100.0 * 6.251275539398193
Epoch 860, val loss: 1.0076898336410522
Epoch 870, training loss: 626.017578125 = 0.5149117112159729 + 100.0 * 6.255026817321777
Epoch 870, val loss: 1.0037896633148193
Epoch 880, training loss: 625.9605712890625 = 0.5022761821746826 + 100.0 * 6.25458288192749
Epoch 880, val loss: 0.999565064907074
Epoch 890, training loss: 625.5094604492188 = 0.4898875653743744 + 100.0 * 6.2501959800720215
Epoch 890, val loss: 0.9957438111305237
Epoch 900, training loss: 625.33447265625 = 0.477861225605011 + 100.0 * 6.248566150665283
Epoch 900, val loss: 0.9923051595687866
Epoch 910, training loss: 625.2687377929688 = 0.4661356210708618 + 100.0 * 6.248025894165039
Epoch 910, val loss: 0.9892642498016357
Epoch 920, training loss: 626.0115356445312 = 0.45460277795791626 + 100.0 * 6.2555694580078125
Epoch 920, val loss: 0.9861717224121094
Epoch 930, training loss: 625.08349609375 = 0.4431358277797699 + 100.0 * 6.246403694152832
Epoch 930, val loss: 0.9832463264465332
Epoch 940, training loss: 625.037841796875 = 0.43199622631073 + 100.0 * 6.246058464050293
Epoch 940, val loss: 0.9803732633590698
Epoch 950, training loss: 624.9515991210938 = 0.42126911878585815 + 100.0 * 6.245303630828857
Epoch 950, val loss: 0.978209912776947
Epoch 960, training loss: 626.1078491210938 = 0.41075098514556885 + 100.0 * 6.2569708824157715
Epoch 960, val loss: 0.9760841727256775
Epoch 970, training loss: 624.8727416992188 = 0.40011268854141235 + 100.0 * 6.244726657867432
Epoch 970, val loss: 0.9739354848861694
Epoch 980, training loss: 624.7537841796875 = 0.3899812400341034 + 100.0 * 6.243638515472412
Epoch 980, val loss: 0.9720331430435181
Epoch 990, training loss: 624.589599609375 = 0.38012516498565674 + 100.0 * 6.24209451675415
Epoch 990, val loss: 0.9707067608833313
Epoch 1000, training loss: 624.5537719726562 = 0.3705129623413086 + 100.0 * 6.241832733154297
Epoch 1000, val loss: 0.9694150686264038
Epoch 1010, training loss: 625.3272705078125 = 0.3611223101615906 + 100.0 * 6.249660968780518
Epoch 1010, val loss: 0.968226432800293
Epoch 1020, training loss: 624.3965454101562 = 0.3517802059650421 + 100.0 * 6.240447521209717
Epoch 1020, val loss: 0.9671800136566162
Epoch 1030, training loss: 624.4712524414062 = 0.3428206145763397 + 100.0 * 6.241284370422363
Epoch 1030, val loss: 0.9666029214859009
Epoch 1040, training loss: 624.9982299804688 = 0.3341042697429657 + 100.0 * 6.246641159057617
Epoch 1040, val loss: 0.9661538600921631
Epoch 1050, training loss: 624.4508056640625 = 0.32541587948799133 + 100.0 * 6.241253852844238
Epoch 1050, val loss: 0.9651188850402832
Epoch 1060, training loss: 624.2146606445312 = 0.31710633635520935 + 100.0 * 6.238975524902344
Epoch 1060, val loss: 0.9653738737106323
Epoch 1070, training loss: 624.1618041992188 = 0.3089977502822876 + 100.0 * 6.238527774810791
Epoch 1070, val loss: 0.9652535915374756
Epoch 1080, training loss: 624.79296875 = 0.3010757267475128 + 100.0 * 6.2449188232421875
Epoch 1080, val loss: 0.9656556248664856
Epoch 1090, training loss: 624.2086181640625 = 0.29343274235725403 + 100.0 * 6.239151477813721
Epoch 1090, val loss: 0.965924084186554
Epoch 1100, training loss: 624.2042236328125 = 0.2858729660511017 + 100.0 * 6.23918342590332
Epoch 1100, val loss: 0.966738224029541
Epoch 1110, training loss: 623.8984375 = 0.2785830497741699 + 100.0 * 6.236198902130127
Epoch 1110, val loss: 0.9671757221221924
Epoch 1120, training loss: 623.8107299804688 = 0.2715224623680115 + 100.0 * 6.235391616821289
Epoch 1120, val loss: 0.9681041836738586
Epoch 1130, training loss: 623.75341796875 = 0.26466742157936096 + 100.0 * 6.234887599945068
Epoch 1130, val loss: 0.9693316221237183
Epoch 1140, training loss: 624.0077514648438 = 0.2580225169658661 + 100.0 * 6.237497806549072
Epoch 1140, val loss: 0.9706980586051941
Epoch 1150, training loss: 624.2371826171875 = 0.2513119578361511 + 100.0 * 6.239859104156494
Epoch 1150, val loss: 0.9715052843093872
Epoch 1160, training loss: 623.7898559570312 = 0.24503031373023987 + 100.0 * 6.235447883605957
Epoch 1160, val loss: 0.9733813405036926
Epoch 1170, training loss: 623.4742431640625 = 0.23879852890968323 + 100.0 * 6.232354640960693
Epoch 1170, val loss: 0.9749671220779419
Epoch 1180, training loss: 623.4585571289062 = 0.23283490538597107 + 100.0 * 6.23225736618042
Epoch 1180, val loss: 0.9769404530525208
Epoch 1190, training loss: 624.072998046875 = 0.22711823880672455 + 100.0 * 6.238458633422852
Epoch 1190, val loss: 0.9793620705604553
Epoch 1200, training loss: 623.773193359375 = 0.2213909924030304 + 100.0 * 6.235517978668213
Epoch 1200, val loss: 0.9805912971496582
Epoch 1210, training loss: 623.4058227539062 = 0.21581301093101501 + 100.0 * 6.231899738311768
Epoch 1210, val loss: 0.983339786529541
Epoch 1220, training loss: 623.2872314453125 = 0.21048958599567413 + 100.0 * 6.230767250061035
Epoch 1220, val loss: 0.9856489300727844
Epoch 1230, training loss: 623.4598999023438 = 0.20530261099338531 + 100.0 * 6.232545852661133
Epoch 1230, val loss: 0.98827064037323
Epoch 1240, training loss: 623.4893188476562 = 0.20023605227470398 + 100.0 * 6.232890605926514
Epoch 1240, val loss: 0.9910252094268799
Epoch 1250, training loss: 623.2306518554688 = 0.19530273973941803 + 100.0 * 6.230353355407715
Epoch 1250, val loss: 0.9936367273330688
Epoch 1260, training loss: 623.0299682617188 = 0.19050204753875732 + 100.0 * 6.228394985198975
Epoch 1260, val loss: 0.9963923692703247
Epoch 1270, training loss: 622.9580078125 = 0.18589378893375397 + 100.0 * 6.227720737457275
Epoch 1270, val loss: 0.999369740486145
Epoch 1280, training loss: 623.1429443359375 = 0.18143904209136963 + 100.0 * 6.229614734649658
Epoch 1280, val loss: 1.0023680925369263
Epoch 1290, training loss: 623.1456909179688 = 0.17704398930072784 + 100.0 * 6.229686260223389
Epoch 1290, val loss: 1.005561351776123
Epoch 1300, training loss: 622.9188232421875 = 0.1727260947227478 + 100.0 * 6.227460861206055
Epoch 1300, val loss: 1.0085591077804565
Epoch 1310, training loss: 622.8887329101562 = 0.16856428980827332 + 100.0 * 6.227201461791992
Epoch 1310, val loss: 1.0120882987976074
Epoch 1320, training loss: 622.94775390625 = 0.1645669937133789 + 100.0 * 6.227832317352295
Epoch 1320, val loss: 1.0154792070388794
Epoch 1330, training loss: 623.1055908203125 = 0.16067151725292206 + 100.0 * 6.229449272155762
Epoch 1330, val loss: 1.0192370414733887
Epoch 1340, training loss: 623.0288696289062 = 0.1568339765071869 + 100.0 * 6.228720188140869
Epoch 1340, val loss: 1.0224794149398804
Epoch 1350, training loss: 622.684326171875 = 0.1530264914035797 + 100.0 * 6.225313186645508
Epoch 1350, val loss: 1.0260685682296753
Epoch 1360, training loss: 622.6551513671875 = 0.14943332970142365 + 100.0 * 6.225057125091553
Epoch 1360, val loss: 1.0298101902008057
Epoch 1370, training loss: 622.6304321289062 = 0.14595918357372284 + 100.0 * 6.224844932556152
Epoch 1370, val loss: 1.0339617729187012
Epoch 1380, training loss: 623.5399169921875 = 0.14254333078861237 + 100.0 * 6.233973979949951
Epoch 1380, val loss: 1.038064956665039
Epoch 1390, training loss: 622.979248046875 = 0.1392177790403366 + 100.0 * 6.228400230407715
Epoch 1390, val loss: 1.0409204959869385
Epoch 1400, training loss: 622.6245727539062 = 0.13591736555099487 + 100.0 * 6.224886417388916
Epoch 1400, val loss: 1.0455304384231567
Epoch 1410, training loss: 622.5768432617188 = 0.132792666554451 + 100.0 * 6.224440574645996
Epoch 1410, val loss: 1.0491925477981567
Epoch 1420, training loss: 623.0384521484375 = 0.12976090610027313 + 100.0 * 6.229086875915527
Epoch 1420, val loss: 1.0536195039749146
Epoch 1430, training loss: 622.5091552734375 = 0.12673738598823547 + 100.0 * 6.223824501037598
Epoch 1430, val loss: 1.0574201345443726
Epoch 1440, training loss: 622.3507690429688 = 0.12382813543081284 + 100.0 * 6.222269535064697
Epoch 1440, val loss: 1.0615425109863281
Epoch 1450, training loss: 622.2969970703125 = 0.12103419750928879 + 100.0 * 6.221759796142578
Epoch 1450, val loss: 1.0660123825073242
Epoch 1460, training loss: 622.6878662109375 = 0.1183113306760788 + 100.0 * 6.225695610046387
Epoch 1460, val loss: 1.0702012777328491
Epoch 1470, training loss: 622.5936889648438 = 0.11560911685228348 + 100.0 * 6.224781036376953
Epoch 1470, val loss: 1.073884129524231
Epoch 1480, training loss: 622.2587280273438 = 0.11297908425331116 + 100.0 * 6.221457481384277
Epoch 1480, val loss: 1.0790002346038818
Epoch 1490, training loss: 622.194580078125 = 0.11043784022331238 + 100.0 * 6.220841407775879
Epoch 1490, val loss: 1.082880973815918
Epoch 1500, training loss: 622.1016845703125 = 0.10799659788608551 + 100.0 * 6.219936847686768
Epoch 1500, val loss: 1.0877716541290283
Epoch 1510, training loss: 622.0800170898438 = 0.10562728345394135 + 100.0 * 6.219743728637695
Epoch 1510, val loss: 1.0920109748840332
Epoch 1520, training loss: 622.9567260742188 = 0.10332614183425903 + 100.0 * 6.22853422164917
Epoch 1520, val loss: 1.096259593963623
Epoch 1530, training loss: 622.282958984375 = 0.10097768157720566 + 100.0 * 6.22182035446167
Epoch 1530, val loss: 1.1010931730270386
Epoch 1540, training loss: 622.3156127929688 = 0.09875393658876419 + 100.0 * 6.222168445587158
Epoch 1540, val loss: 1.1052204370498657
Epoch 1550, training loss: 622.0418701171875 = 0.09655657410621643 + 100.0 * 6.219452857971191
Epoch 1550, val loss: 1.1099791526794434
Epoch 1560, training loss: 622.4185791015625 = 0.09442562609910965 + 100.0 * 6.223241806030273
Epoch 1560, val loss: 1.1141849756240845
Epoch 1570, training loss: 621.9832153320312 = 0.09239206463098526 + 100.0 * 6.218908309936523
Epoch 1570, val loss: 1.118958592414856
Epoch 1580, training loss: 621.9072875976562 = 0.09040010720491409 + 100.0 * 6.21816873550415
Epoch 1580, val loss: 1.12351393699646
Epoch 1590, training loss: 621.8021850585938 = 0.0884699672460556 + 100.0 * 6.217137336730957
Epoch 1590, val loss: 1.1279809474945068
Epoch 1600, training loss: 621.8438110351562 = 0.08661053329706192 + 100.0 * 6.217572212219238
Epoch 1600, val loss: 1.1324385404586792
Epoch 1610, training loss: 622.0801391601562 = 0.08478112518787384 + 100.0 * 6.219953536987305
Epoch 1610, val loss: 1.136716365814209
Epoch 1620, training loss: 621.9739379882812 = 0.08300114423036575 + 100.0 * 6.21890926361084
Epoch 1620, val loss: 1.141788125038147
Epoch 1630, training loss: 622.5799560546875 = 0.08124496787786484 + 100.0 * 6.224987030029297
Epoch 1630, val loss: 1.1464232206344604
Epoch 1640, training loss: 622.0166625976562 = 0.07944592088460922 + 100.0 * 6.219371795654297
Epoch 1640, val loss: 1.150546908378601
Epoch 1650, training loss: 621.6946411132812 = 0.07779444009065628 + 100.0 * 6.216168403625488
Epoch 1650, val loss: 1.1553841829299927
Epoch 1660, training loss: 621.580078125 = 0.0761866644024849 + 100.0 * 6.215039253234863
Epoch 1660, val loss: 1.1601176261901855
Epoch 1670, training loss: 621.5726928710938 = 0.07463900744915009 + 100.0 * 6.214980602264404
Epoch 1670, val loss: 1.1645699739456177
Epoch 1680, training loss: 621.77294921875 = 0.07312801480293274 + 100.0 * 6.21699857711792
Epoch 1680, val loss: 1.1692602634429932
Epoch 1690, training loss: 621.81005859375 = 0.07164254039525986 + 100.0 * 6.217384338378906
Epoch 1690, val loss: 1.1742390394210815
Epoch 1700, training loss: 621.6898803710938 = 0.0701550543308258 + 100.0 * 6.216197490692139
Epoch 1700, val loss: 1.178030014038086
Epoch 1710, training loss: 621.6762084960938 = 0.06871894747018814 + 100.0 * 6.2160749435424805
Epoch 1710, val loss: 1.1828314065933228
Epoch 1720, training loss: 621.5777587890625 = 0.0673142597079277 + 100.0 * 6.215104579925537
Epoch 1720, val loss: 1.1873188018798828
Epoch 1730, training loss: 621.4793090820312 = 0.06598766893148422 + 100.0 * 6.214133262634277
Epoch 1730, val loss: 1.1919846534729004
Epoch 1740, training loss: 621.8074340820312 = 0.06468470394611359 + 100.0 * 6.2174272537231445
Epoch 1740, val loss: 1.1963664293289185
Epoch 1750, training loss: 621.77197265625 = 0.0634179413318634 + 100.0 * 6.217085361480713
Epoch 1750, val loss: 1.2012509107589722
Epoch 1760, training loss: 621.8517456054688 = 0.062137018889188766 + 100.0 * 6.217895984649658
Epoch 1760, val loss: 1.2050901651382446
Epoch 1770, training loss: 621.5091552734375 = 0.060873620212078094 + 100.0 * 6.21448278427124
Epoch 1770, val loss: 1.2101824283599854
Epoch 1780, training loss: 621.3674926757812 = 0.059703368693590164 + 100.0 * 6.213078022003174
Epoch 1780, val loss: 1.2146278619766235
Epoch 1790, training loss: 621.4005737304688 = 0.05855998396873474 + 100.0 * 6.213420391082764
Epoch 1790, val loss: 1.2194099426269531
Epoch 1800, training loss: 622.04248046875 = 0.057447612285614014 + 100.0 * 6.219850540161133
Epoch 1800, val loss: 1.2240321636199951
Epoch 1810, training loss: 621.4780883789062 = 0.05630943924188614 + 100.0 * 6.214217662811279
Epoch 1810, val loss: 1.2276626825332642
Epoch 1820, training loss: 621.2665405273438 = 0.055233098566532135 + 100.0 * 6.212112903594971
Epoch 1820, val loss: 1.232761263847351
Epoch 1830, training loss: 621.3731689453125 = 0.05419813096523285 + 100.0 * 6.213189601898193
Epoch 1830, val loss: 1.237360954284668
Epoch 1840, training loss: 621.6697998046875 = 0.05316406860947609 + 100.0 * 6.2161664962768555
Epoch 1840, val loss: 1.2415951490402222
Epoch 1850, training loss: 621.40869140625 = 0.05217082053422928 + 100.0 * 6.213565349578857
Epoch 1850, val loss: 1.2454160451889038
Epoch 1860, training loss: 621.1975708007812 = 0.05119451507925987 + 100.0 * 6.211463928222656
Epoch 1860, val loss: 1.2504570484161377
Epoch 1870, training loss: 621.2140502929688 = 0.05025744438171387 + 100.0 * 6.211638450622559
Epoch 1870, val loss: 1.255016803741455
Epoch 1880, training loss: 621.5559692382812 = 0.04934760928153992 + 100.0 * 6.215065956115723
Epoch 1880, val loss: 1.2594964504241943
Epoch 1890, training loss: 621.2015380859375 = 0.048417575657367706 + 100.0 * 6.211531639099121
Epoch 1890, val loss: 1.2632404565811157
Epoch 1900, training loss: 621.1553344726562 = 0.04754919186234474 + 100.0 * 6.21107816696167
Epoch 1900, val loss: 1.2679933309555054
Epoch 1910, training loss: 621.7500610351562 = 0.04670542851090431 + 100.0 * 6.217033386230469
Epoch 1910, val loss: 1.2722011804580688
Epoch 1920, training loss: 621.2715454101562 = 0.04582671448588371 + 100.0 * 6.212257385253906
Epoch 1920, val loss: 1.2766224145889282
Epoch 1930, training loss: 621.0675659179688 = 0.04498092830181122 + 100.0 * 6.210225582122803
Epoch 1930, val loss: 1.2807133197784424
Epoch 1940, training loss: 620.9595336914062 = 0.0441981740295887 + 100.0 * 6.209153175354004
Epoch 1940, val loss: 1.2853672504425049
Epoch 1950, training loss: 620.9332885742188 = 0.043431151658296585 + 100.0 * 6.208898544311523
Epoch 1950, val loss: 1.2896240949630737
Epoch 1960, training loss: 621.7959594726562 = 0.04270918294787407 + 100.0 * 6.217532634735107
Epoch 1960, val loss: 1.294374704360962
Epoch 1970, training loss: 621.1760864257812 = 0.041907694190740585 + 100.0 * 6.211341381072998
Epoch 1970, val loss: 1.2969950437545776
Epoch 1980, training loss: 621.020263671875 = 0.04117991402745247 + 100.0 * 6.2097907066345215
Epoch 1980, val loss: 1.3020994663238525
Epoch 1990, training loss: 620.9135131835938 = 0.04046543687582016 + 100.0 * 6.208730220794678
Epoch 1990, val loss: 1.305961012840271
Epoch 2000, training loss: 621.1536865234375 = 0.03980055823922157 + 100.0 * 6.21113920211792
Epoch 2000, val loss: 1.310066819190979
Epoch 2010, training loss: 620.955322265625 = 0.039107795804739 + 100.0 * 6.209161758422852
Epoch 2010, val loss: 1.3144261837005615
Epoch 2020, training loss: 620.8880615234375 = 0.03842616453766823 + 100.0 * 6.20849609375
Epoch 2020, val loss: 1.3183963298797607
Epoch 2030, training loss: 620.8099975585938 = 0.03779501095414162 + 100.0 * 6.207722187042236
Epoch 2030, val loss: 1.3225247859954834
Epoch 2040, training loss: 620.8866577148438 = 0.03717195987701416 + 100.0 * 6.208495140075684
Epoch 2040, val loss: 1.3264806270599365
Epoch 2050, training loss: 621.00537109375 = 0.036557070910930634 + 100.0 * 6.209688186645508
Epoch 2050, val loss: 1.330885410308838
Epoch 2060, training loss: 620.9561767578125 = 0.03593730926513672 + 100.0 * 6.209202289581299
Epoch 2060, val loss: 1.3348242044448853
Epoch 2070, training loss: 620.956298828125 = 0.0353277362883091 + 100.0 * 6.209209442138672
Epoch 2070, val loss: 1.338969111442566
Epoch 2080, training loss: 620.7549438476562 = 0.03475968912243843 + 100.0 * 6.2072014808654785
Epoch 2080, val loss: 1.342470407485962
Epoch 2090, training loss: 620.91845703125 = 0.03420799970626831 + 100.0 * 6.208842754364014
Epoch 2090, val loss: 1.3467947244644165
Epoch 2100, training loss: 620.9658203125 = 0.03365545719861984 + 100.0 * 6.209321975708008
Epoch 2100, val loss: 1.3505902290344238
Epoch 2110, training loss: 620.6627807617188 = 0.033091019839048386 + 100.0 * 6.206296920776367
Epoch 2110, val loss: 1.3546874523162842
Epoch 2120, training loss: 620.6607055664062 = 0.03257112577557564 + 100.0 * 6.2062811851501465
Epoch 2120, val loss: 1.3588858842849731
Epoch 2130, training loss: 620.879150390625 = 0.03206276148557663 + 100.0 * 6.208471298217773
Epoch 2130, val loss: 1.3622527122497559
Epoch 2140, training loss: 620.7386474609375 = 0.03155963867902756 + 100.0 * 6.207070827484131
Epoch 2140, val loss: 1.366396188735962
Epoch 2150, training loss: 620.7972412109375 = 0.031067166477441788 + 100.0 * 6.2076616287231445
Epoch 2150, val loss: 1.3701502084732056
Epoch 2160, training loss: 620.6737670898438 = 0.030581697821617126 + 100.0 * 6.206431865692139
Epoch 2160, val loss: 1.3742491006851196
Epoch 2170, training loss: 620.8862915039062 = 0.030124502256512642 + 100.0 * 6.208561420440674
Epoch 2170, val loss: 1.3783857822418213
Epoch 2180, training loss: 620.6807250976562 = 0.029649095609784126 + 100.0 * 6.206510543823242
Epoch 2180, val loss: 1.3814510107040405
Epoch 2190, training loss: 620.7770385742188 = 0.029191013425588608 + 100.0 * 6.2074785232543945
Epoch 2190, val loss: 1.3855446577072144
Epoch 2200, training loss: 620.6072387695312 = 0.028750058263540268 + 100.0 * 6.205784797668457
Epoch 2200, val loss: 1.3887019157409668
Epoch 2210, training loss: 620.7132568359375 = 0.0283199455589056 + 100.0 * 6.206849575042725
Epoch 2210, val loss: 1.392703890800476
Epoch 2220, training loss: 620.7156982421875 = 0.027892963960766792 + 100.0 * 6.206878185272217
Epoch 2220, val loss: 1.396428108215332
Epoch 2230, training loss: 620.4379272460938 = 0.027480999007821083 + 100.0 * 6.204104900360107
Epoch 2230, val loss: 1.400200366973877
Epoch 2240, training loss: 620.5231323242188 = 0.027089430019259453 + 100.0 * 6.204960346221924
Epoch 2240, val loss: 1.4040385484695435
Epoch 2250, training loss: 620.5391235351562 = 0.026694372296333313 + 100.0 * 6.205124378204346
Epoch 2250, val loss: 1.4075758457183838
Epoch 2260, training loss: 620.6121215820312 = 0.02630613185465336 + 100.0 * 6.20585823059082
Epoch 2260, val loss: 1.4113978147506714
Epoch 2270, training loss: 620.5258178710938 = 0.025929396972060204 + 100.0 * 6.20499849319458
Epoch 2270, val loss: 1.4147158861160278
Epoch 2280, training loss: 621.1570434570312 = 0.025567837059497833 + 100.0 * 6.211314678192139
Epoch 2280, val loss: 1.41801917552948
Epoch 2290, training loss: 620.5610961914062 = 0.02516733668744564 + 100.0 * 6.20535945892334
Epoch 2290, val loss: 1.4213589429855347
Epoch 2300, training loss: 620.3308715820312 = 0.02479935809969902 + 100.0 * 6.203060626983643
Epoch 2300, val loss: 1.4250465631484985
Epoch 2310, training loss: 620.277587890625 = 0.024454331025481224 + 100.0 * 6.202531337738037
Epoch 2310, val loss: 1.4286092519760132
Epoch 2320, training loss: 620.265869140625 = 0.024120649322867393 + 100.0 * 6.202417373657227
Epoch 2320, val loss: 1.432343602180481
Epoch 2330, training loss: 620.9111938476562 = 0.023792674764990807 + 100.0 * 6.208873748779297
Epoch 2330, val loss: 1.4360014200210571
Epoch 2340, training loss: 620.3814086914062 = 0.023453183472156525 + 100.0 * 6.203579902648926
Epoch 2340, val loss: 1.4388645887374878
Epoch 2350, training loss: 620.4199829101562 = 0.02311951480805874 + 100.0 * 6.203968524932861
Epoch 2350, val loss: 1.4421885013580322
Epoch 2360, training loss: 620.2894287109375 = 0.022803883999586105 + 100.0 * 6.202666282653809
Epoch 2360, val loss: 1.445281744003296
Epoch 2370, training loss: 620.1781616210938 = 0.022499511018395424 + 100.0 * 6.201556205749512
Epoch 2370, val loss: 1.4492886066436768
Epoch 2380, training loss: 620.28662109375 = 0.022206921130418777 + 100.0 * 6.202643871307373
Epoch 2380, val loss: 1.4525474309921265
Epoch 2390, training loss: 620.7125244140625 = 0.021914411336183548 + 100.0 * 6.206906318664551
Epoch 2390, val loss: 1.4558167457580566
Epoch 2400, training loss: 620.3875732421875 = 0.021607549861073494 + 100.0 * 6.203659534454346
Epoch 2400, val loss: 1.459321141242981
Epoch 2410, training loss: 620.7611694335938 = 0.021327998489141464 + 100.0 * 6.207398891448975
Epoch 2410, val loss: 1.462527871131897
Epoch 2420, training loss: 620.1978149414062 = 0.02102850191295147 + 100.0 * 6.201767444610596
Epoch 2420, val loss: 1.465993046760559
Epoch 2430, training loss: 620.0955810546875 = 0.020743640139698982 + 100.0 * 6.200748443603516
Epoch 2430, val loss: 1.4690724611282349
Epoch 2440, training loss: 620.077392578125 = 0.020480385050177574 + 100.0 * 6.200568675994873
Epoch 2440, val loss: 1.4721546173095703
Epoch 2450, training loss: 620.1248779296875 = 0.02022361569106579 + 100.0 * 6.201046466827393
Epoch 2450, val loss: 1.4752851724624634
Epoch 2460, training loss: 620.4685668945312 = 0.01996788941323757 + 100.0 * 6.20448637008667
Epoch 2460, val loss: 1.4778966903686523
Epoch 2470, training loss: 620.3397827148438 = 0.019713861867785454 + 100.0 * 6.203200817108154
Epoch 2470, val loss: 1.482305884361267
Epoch 2480, training loss: 620.4534912109375 = 0.019454311579465866 + 100.0 * 6.204339981079102
Epoch 2480, val loss: 1.4844334125518799
Epoch 2490, training loss: 620.0929565429688 = 0.019207051023840904 + 100.0 * 6.200736999511719
Epoch 2490, val loss: 1.4880852699279785
Epoch 2500, training loss: 620.5494384765625 = 0.018974460661411285 + 100.0 * 6.2053046226501465
Epoch 2500, val loss: 1.4911264181137085
Epoch 2510, training loss: 620.0264892578125 = 0.018721679225564003 + 100.0 * 6.200077533721924
Epoch 2510, val loss: 1.4940073490142822
Epoch 2520, training loss: 620.0156860351562 = 0.018483737483620644 + 100.0 * 6.199972152709961
Epoch 2520, val loss: 1.497389554977417
Epoch 2530, training loss: 619.941162109375 = 0.01826614700257778 + 100.0 * 6.1992292404174805
Epoch 2530, val loss: 1.5004768371582031
Epoch 2540, training loss: 619.968505859375 = 0.018052611500024796 + 100.0 * 6.199504375457764
Epoch 2540, val loss: 1.5032238960266113
Epoch 2550, training loss: 620.9340209960938 = 0.01784912310540676 + 100.0 * 6.209161758422852
Epoch 2550, val loss: 1.5061603784561157
Epoch 2560, training loss: 620.4326171875 = 0.01760973408818245 + 100.0 * 6.2041497230529785
Epoch 2560, val loss: 1.5096179246902466
Epoch 2570, training loss: 620.0625610351562 = 0.01739787869155407 + 100.0 * 6.200451850891113
Epoch 2570, val loss: 1.5123651027679443
Epoch 2580, training loss: 620.0015258789062 = 0.017186814919114113 + 100.0 * 6.199843406677246
Epoch 2580, val loss: 1.5150961875915527
Epoch 2590, training loss: 619.9825439453125 = 0.01698462665081024 + 100.0 * 6.199655532836914
Epoch 2590, val loss: 1.5182394981384277
Epoch 2600, training loss: 620.0506591796875 = 0.016791949048638344 + 100.0 * 6.200338840484619
Epoch 2600, val loss: 1.5212376117706299
Epoch 2610, training loss: 620.1851196289062 = 0.016596084460616112 + 100.0 * 6.201685428619385
Epoch 2610, val loss: 1.524221420288086
Epoch 2620, training loss: 619.9976806640625 = 0.016393184661865234 + 100.0 * 6.199812889099121
Epoch 2620, val loss: 1.5268042087554932
Epoch 2630, training loss: 620.1309204101562 = 0.016205687075853348 + 100.0 * 6.201147079467773
Epoch 2630, val loss: 1.5302363634109497
Epoch 2640, training loss: 619.9978637695312 = 0.016021156683564186 + 100.0 * 6.1998186111450195
Epoch 2640, val loss: 1.532397747039795
Epoch 2650, training loss: 619.9424438476562 = 0.015835192054510117 + 100.0 * 6.19926643371582
Epoch 2650, val loss: 1.5351181030273438
Epoch 2660, training loss: 620.119140625 = 0.01565631479024887 + 100.0 * 6.2010345458984375
Epoch 2660, val loss: 1.5384234189987183
Epoch 2670, training loss: 619.8587036132812 = 0.015477534383535385 + 100.0 * 6.198431968688965
Epoch 2670, val loss: 1.5406984090805054
Epoch 2680, training loss: 620.0833129882812 = 0.015303771942853928 + 100.0 * 6.200679779052734
Epoch 2680, val loss: 1.5432415008544922
Epoch 2690, training loss: 620.0711059570312 = 0.015135904774069786 + 100.0 * 6.200559616088867
Epoch 2690, val loss: 1.5465320348739624
Epoch 2700, training loss: 619.8234252929688 = 0.014970372430980206 + 100.0 * 6.198084831237793
Epoch 2700, val loss: 1.5490643978118896
Epoch 2710, training loss: 619.7274780273438 = 0.014808518812060356 + 100.0 * 6.197126388549805
Epoch 2710, val loss: 1.5515830516815186
Epoch 2720, training loss: 619.9386596679688 = 0.014654179103672504 + 100.0 * 6.199240207672119
Epoch 2720, val loss: 1.5543270111083984
Epoch 2730, training loss: 620.0797119140625 = 0.014496495015919209 + 100.0 * 6.2006516456604
Epoch 2730, val loss: 1.5567876100540161
Epoch 2740, training loss: 619.9563598632812 = 0.014326072297990322 + 100.0 * 6.199419975280762
Epoch 2740, val loss: 1.559657335281372
Epoch 2750, training loss: 619.801025390625 = 0.014169537462294102 + 100.0 * 6.197868347167969
Epoch 2750, val loss: 1.561888575553894
Epoch 2760, training loss: 619.760009765625 = 0.01402418315410614 + 100.0 * 6.197459697723389
Epoch 2760, val loss: 1.5641757249832153
Epoch 2770, training loss: 619.8786010742188 = 0.013876204378902912 + 100.0 * 6.198647499084473
Epoch 2770, val loss: 1.5670711994171143
Epoch 2780, training loss: 619.9681396484375 = 0.013735673390328884 + 100.0 * 6.1995439529418945
Epoch 2780, val loss: 1.5701786279678345
Epoch 2790, training loss: 619.8568115234375 = 0.013592023402452469 + 100.0 * 6.198431968688965
Epoch 2790, val loss: 1.5723692178726196
Epoch 2800, training loss: 620.2205810546875 = 0.013445270247757435 + 100.0 * 6.202071666717529
Epoch 2800, val loss: 1.5746217966079712
Epoch 2810, training loss: 619.7649536132812 = 0.013300497084856033 + 100.0 * 6.197516441345215
Epoch 2810, val loss: 1.5773452520370483
Epoch 2820, training loss: 619.6986694335938 = 0.013168586418032646 + 100.0 * 6.196855068206787
Epoch 2820, val loss: 1.5793524980545044
Epoch 2830, training loss: 619.8785400390625 = 0.013041500002145767 + 100.0 * 6.198654651641846
Epoch 2830, val loss: 1.5822105407714844
Epoch 2840, training loss: 619.6937255859375 = 0.01290370337665081 + 100.0 * 6.196808338165283
Epoch 2840, val loss: 1.5843461751937866
Epoch 2850, training loss: 619.6348876953125 = 0.012769315391778946 + 100.0 * 6.196221351623535
Epoch 2850, val loss: 1.5869042873382568
Epoch 2860, training loss: 619.5467529296875 = 0.012645452283322811 + 100.0 * 6.195341110229492
Epoch 2860, val loss: 1.5892823934555054
Epoch 2870, training loss: 619.6297607421875 = 0.012524044141173363 + 100.0 * 6.19617223739624
Epoch 2870, val loss: 1.5916823148727417
Epoch 2880, training loss: 619.973388671875 = 0.012407259084284306 + 100.0 * 6.199609756469727
Epoch 2880, val loss: 1.5944085121154785
Epoch 2890, training loss: 619.6734008789062 = 0.012281939387321472 + 100.0 * 6.196611404418945
Epoch 2890, val loss: 1.5961235761642456
Epoch 2900, training loss: 619.7902221679688 = 0.01216567400842905 + 100.0 * 6.197780609130859
Epoch 2900, val loss: 1.5984562635421753
Epoch 2910, training loss: 619.7440795898438 = 0.012045808136463165 + 100.0 * 6.197320461273193
Epoch 2910, val loss: 1.6005182266235352
Epoch 2920, training loss: 619.8707275390625 = 0.011919200420379639 + 100.0 * 6.1985883712768555
Epoch 2920, val loss: 1.6024678945541382
Epoch 2930, training loss: 619.5089721679688 = 0.011804131790995598 + 100.0 * 6.194972038269043
Epoch 2930, val loss: 1.6049736738204956
Epoch 2940, training loss: 619.531982421875 = 0.011692856438457966 + 100.0 * 6.195202827453613
Epoch 2940, val loss: 1.6069728136062622
Epoch 2950, training loss: 620.0275268554688 = 0.011585778556764126 + 100.0 * 6.200159072875977
Epoch 2950, val loss: 1.609004020690918
Epoch 2960, training loss: 619.7220458984375 = 0.011470941826701164 + 100.0 * 6.197105884552002
Epoch 2960, val loss: 1.6119122505187988
Epoch 2970, training loss: 619.5498046875 = 0.011362086050212383 + 100.0 * 6.195384502410889
Epoch 2970, val loss: 1.613645076751709
Epoch 2980, training loss: 619.4464111328125 = 0.011255545541644096 + 100.0 * 6.194351673126221
Epoch 2980, val loss: 1.6164222955703735
Epoch 2990, training loss: 619.4609375 = 0.011155029758810997 + 100.0 * 6.194497585296631
Epoch 2990, val loss: 1.618438959121704
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 861.6390380859375 = 1.953251838684082 + 100.0 * 8.596858024597168
Epoch 0, val loss: 1.945267915725708
Epoch 10, training loss: 861.5682983398438 = 1.9447494745254517 + 100.0 * 8.596235275268555
Epoch 10, val loss: 1.9373257160186768
Epoch 20, training loss: 861.1713256835938 = 1.9341328144073486 + 100.0 * 8.592371940612793
Epoch 20, val loss: 1.9272384643554688
Epoch 30, training loss: 858.491455078125 = 1.9202903509140015 + 100.0 * 8.565711975097656
Epoch 30, val loss: 1.91380774974823
Epoch 40, training loss: 840.1832885742188 = 1.9022079706192017 + 100.0 * 8.382810592651367
Epoch 40, val loss: 1.89647376537323
Epoch 50, training loss: 760.2388916015625 = 1.8806620836257935 + 100.0 * 7.583581924438477
Epoch 50, val loss: 1.876221776008606
Epoch 60, training loss: 736.5012817382812 = 1.8615585565567017 + 100.0 * 7.3463969230651855
Epoch 60, val loss: 1.8593847751617432
Epoch 70, training loss: 715.3564453125 = 1.8478925228118896 + 100.0 * 7.135085582733154
Epoch 70, val loss: 1.8467739820480347
Epoch 80, training loss: 705.5316772460938 = 1.8339040279388428 + 100.0 * 7.036977767944336
Epoch 80, val loss: 1.8338404893875122
Epoch 90, training loss: 695.2733154296875 = 1.820587158203125 + 100.0 * 6.9345269203186035
Epoch 90, val loss: 1.8219473361968994
Epoch 100, training loss: 686.8533325195312 = 1.809741497039795 + 100.0 * 6.850435733795166
Epoch 100, val loss: 1.8119791746139526
Epoch 110, training loss: 679.2451171875 = 1.8004895448684692 + 100.0 * 6.774446487426758
Epoch 110, val loss: 1.803679347038269
Epoch 120, training loss: 671.8372802734375 = 1.792967677116394 + 100.0 * 6.700443267822266
Epoch 120, val loss: 1.7967500686645508
Epoch 130, training loss: 666.97314453125 = 1.785759449005127 + 100.0 * 6.65187406539917
Epoch 130, val loss: 1.7898527383804321
Epoch 140, training loss: 663.350830078125 = 1.7776134014129639 + 100.0 * 6.615732669830322
Epoch 140, val loss: 1.7822668552398682
Epoch 150, training loss: 660.6058959960938 = 1.7687058448791504 + 100.0 * 6.588372230529785
Epoch 150, val loss: 1.7743322849273682
Epoch 160, training loss: 657.847900390625 = 1.7593562602996826 + 100.0 * 6.560885906219482
Epoch 160, val loss: 1.7660783529281616
Epoch 170, training loss: 655.31494140625 = 1.7498719692230225 + 100.0 * 6.535650730133057
Epoch 170, val loss: 1.7576048374176025
Epoch 180, training loss: 653.4367065429688 = 1.7396029233932495 + 100.0 * 6.516970634460449
Epoch 180, val loss: 1.7485488653182983
Epoch 190, training loss: 651.6204833984375 = 1.7282499074935913 + 100.0 * 6.498922348022461
Epoch 190, val loss: 1.7387430667877197
Epoch 200, training loss: 650.2578125 = 1.7157803773880005 + 100.0 * 6.4854207038879395
Epoch 200, val loss: 1.727980136871338
Epoch 210, training loss: 649.1198120117188 = 1.7020814418792725 + 100.0 * 6.474177360534668
Epoch 210, val loss: 1.7163362503051758
Epoch 220, training loss: 648.1865844726562 = 1.687110185623169 + 100.0 * 6.464994430541992
Epoch 220, val loss: 1.703511118888855
Epoch 230, training loss: 647.12841796875 = 1.670633316040039 + 100.0 * 6.454577445983887
Epoch 230, val loss: 1.6894264221191406
Epoch 240, training loss: 646.0524291992188 = 1.6530253887176514 + 100.0 * 6.443994045257568
Epoch 240, val loss: 1.6744415760040283
Epoch 250, training loss: 645.2379760742188 = 1.6343786716461182 + 100.0 * 6.436035633087158
Epoch 250, val loss: 1.6587547063827515
Epoch 260, training loss: 644.081298828125 = 1.6146718263626099 + 100.0 * 6.424665927886963
Epoch 260, val loss: 1.642309308052063
Epoch 270, training loss: 643.0584716796875 = 1.5939065217971802 + 100.0 * 6.414645195007324
Epoch 270, val loss: 1.6251786947250366
Epoch 280, training loss: 642.2199096679688 = 1.5719679594039917 + 100.0 * 6.406479358673096
Epoch 280, val loss: 1.607258677482605
Epoch 290, training loss: 641.4502563476562 = 1.548767328262329 + 100.0 * 6.399014949798584
Epoch 290, val loss: 1.5882940292358398
Epoch 300, training loss: 640.7333984375 = 1.5246456861495972 + 100.0 * 6.392087459564209
Epoch 300, val loss: 1.5686372518539429
Epoch 310, training loss: 639.8792724609375 = 1.4996142387390137 + 100.0 * 6.383796691894531
Epoch 310, val loss: 1.5485810041427612
Epoch 320, training loss: 639.1802368164062 = 1.4739452600479126 + 100.0 * 6.377062797546387
Epoch 320, val loss: 1.5281625986099243
Epoch 330, training loss: 638.7290649414062 = 1.4477641582489014 + 100.0 * 6.3728132247924805
Epoch 330, val loss: 1.5075318813323975
Epoch 340, training loss: 638.164794921875 = 1.4206781387329102 + 100.0 * 6.367441177368164
Epoch 340, val loss: 1.4865509271621704
Epoch 350, training loss: 637.5178833007812 = 1.3936065435409546 + 100.0 * 6.361242771148682
Epoch 350, val loss: 1.4656181335449219
Epoch 360, training loss: 636.9862670898438 = 1.3662546873092651 + 100.0 * 6.356199741363525
Epoch 360, val loss: 1.444966435432434
Epoch 370, training loss: 636.4992065429688 = 1.3387763500213623 + 100.0 * 6.351603984832764
Epoch 370, val loss: 1.4245575666427612
Epoch 380, training loss: 636.4739379882812 = 1.311274528503418 + 100.0 * 6.351626396179199
Epoch 380, val loss: 1.4048020839691162
Epoch 390, training loss: 635.6953125 = 1.2835392951965332 + 100.0 * 6.344117641448975
Epoch 390, val loss: 1.3845542669296265
Epoch 400, training loss: 635.4161987304688 = 1.2560241222381592 + 100.0 * 6.341601848602295
Epoch 400, val loss: 1.3648152351379395
Epoch 410, training loss: 635.0223388671875 = 1.2286343574523926 + 100.0 * 6.337936878204346
Epoch 410, val loss: 1.3456896543502808
Epoch 420, training loss: 634.6862182617188 = 1.2015771865844727 + 100.0 * 6.334846019744873
Epoch 420, val loss: 1.3268182277679443
Epoch 430, training loss: 634.3652954101562 = 1.1748449802398682 + 100.0 * 6.331904411315918
Epoch 430, val loss: 1.3087810277938843
Epoch 440, training loss: 634.1468505859375 = 1.1485419273376465 + 100.0 * 6.329982757568359
Epoch 440, val loss: 1.291321873664856
Epoch 450, training loss: 633.7816162109375 = 1.1226104497909546 + 100.0 * 6.326590061187744
Epoch 450, val loss: 1.2746903896331787
Epoch 460, training loss: 633.4583740234375 = 1.0972660779953003 + 100.0 * 6.323610782623291
Epoch 460, val loss: 1.2585033178329468
Epoch 470, training loss: 633.1282348632812 = 1.0725501775741577 + 100.0 * 6.320556640625
Epoch 470, val loss: 1.243313193321228
Epoch 480, training loss: 633.3284912109375 = 1.0484153032302856 + 100.0 * 6.322800636291504
Epoch 480, val loss: 1.2287375926971436
Epoch 490, training loss: 632.691162109375 = 1.0249196290969849 + 100.0 * 6.316662788391113
Epoch 490, val loss: 1.2151789665222168
Epoch 500, training loss: 632.3564453125 = 1.0021538734436035 + 100.0 * 6.31354284286499
Epoch 500, val loss: 1.2024059295654297
Epoch 510, training loss: 632.6827392578125 = 0.980097234249115 + 100.0 * 6.317026615142822
Epoch 510, val loss: 1.1904821395874023
Epoch 520, training loss: 631.98095703125 = 0.9586365818977356 + 100.0 * 6.310223579406738
Epoch 520, val loss: 1.178954005241394
Epoch 530, training loss: 631.6919555664062 = 0.9379146099090576 + 100.0 * 6.307540416717529
Epoch 530, val loss: 1.1686311960220337
Epoch 540, training loss: 631.4651489257812 = 0.9178481101989746 + 100.0 * 6.3054728507995605
Epoch 540, val loss: 1.1589159965515137
Epoch 550, training loss: 631.4410400390625 = 0.8984229564666748 + 100.0 * 6.305426120758057
Epoch 550, val loss: 1.1501234769821167
Epoch 560, training loss: 631.07568359375 = 0.8794416189193726 + 100.0 * 6.301962375640869
Epoch 560, val loss: 1.1413201093673706
Epoch 570, training loss: 631.1189575195312 = 0.8611113429069519 + 100.0 * 6.302578449249268
Epoch 570, val loss: 1.1339322328567505
Epoch 580, training loss: 630.6436767578125 = 0.8431466221809387 + 100.0 * 6.2980055809021
Epoch 580, val loss: 1.1263737678527832
Epoch 590, training loss: 630.7188720703125 = 0.8257676362991333 + 100.0 * 6.298931121826172
Epoch 590, val loss: 1.1197487115859985
Epoch 600, training loss: 630.3773803710938 = 0.8087908625602722 + 100.0 * 6.2956862449646
Epoch 600, val loss: 1.1136528253555298
Epoch 610, training loss: 630.061767578125 = 0.7922888398170471 + 100.0 * 6.292694568634033
Epoch 610, val loss: 1.1077520847320557
Epoch 620, training loss: 630.0200805664062 = 0.776216447353363 + 100.0 * 6.292438507080078
Epoch 620, val loss: 1.1026012897491455
Epoch 630, training loss: 629.9686279296875 = 0.760384202003479 + 100.0 * 6.292082786560059
Epoch 630, val loss: 1.09769868850708
Epoch 640, training loss: 629.6340942382812 = 0.7449776530265808 + 100.0 * 6.288890838623047
Epoch 640, val loss: 1.0930209159851074
Epoch 650, training loss: 629.491455078125 = 0.7298783659934998 + 100.0 * 6.28761625289917
Epoch 650, val loss: 1.0890326499938965
Epoch 660, training loss: 629.2802734375 = 0.7151105403900146 + 100.0 * 6.285651683807373
Epoch 660, val loss: 1.0853767395019531
Epoch 670, training loss: 629.30859375 = 0.700604259967804 + 100.0 * 6.2860798835754395
Epoch 670, val loss: 1.0817170143127441
Epoch 680, training loss: 629.0870971679688 = 0.6862938404083252 + 100.0 * 6.284008026123047
Epoch 680, val loss: 1.078648567199707
Epoch 690, training loss: 628.7732543945312 = 0.6723737120628357 + 100.0 * 6.281009197235107
Epoch 690, val loss: 1.0759549140930176
Epoch 700, training loss: 628.6582641601562 = 0.6587527990341187 + 100.0 * 6.279994964599609
Epoch 700, val loss: 1.0735033750534058
Epoch 710, training loss: 628.8656616210938 = 0.6453083157539368 + 100.0 * 6.282203197479248
Epoch 710, val loss: 1.071248173713684
Epoch 720, training loss: 628.61376953125 = 0.6320462226867676 + 100.0 * 6.279817581176758
Epoch 720, val loss: 1.069273591041565
Epoch 730, training loss: 628.3339233398438 = 0.6190743446350098 + 100.0 * 6.277148723602295
Epoch 730, val loss: 1.0678455829620361
Epoch 740, training loss: 628.1150512695312 = 0.6063365340232849 + 100.0 * 6.275087356567383
Epoch 740, val loss: 1.0666320323944092
Epoch 750, training loss: 628.0104370117188 = 0.5939304828643799 + 100.0 * 6.274165153503418
Epoch 750, val loss: 1.0660086870193481
Epoch 760, training loss: 628.1000366210938 = 0.5816910862922668 + 100.0 * 6.27518367767334
Epoch 760, val loss: 1.0652589797973633
Epoch 770, training loss: 627.7713012695312 = 0.569631040096283 + 100.0 * 6.272016525268555
Epoch 770, val loss: 1.0647975206375122
Epoch 780, training loss: 627.9796142578125 = 0.5578440427780151 + 100.0 * 6.27421760559082
Epoch 780, val loss: 1.0645692348480225
Epoch 790, training loss: 627.717041015625 = 0.5462740659713745 + 100.0 * 6.271708011627197
Epoch 790, val loss: 1.0651367902755737
Epoch 800, training loss: 627.4395141601562 = 0.534887969493866 + 100.0 * 6.269046306610107
Epoch 800, val loss: 1.0651991367340088
Epoch 810, training loss: 627.6630859375 = 0.5237515568733215 + 100.0 * 6.271393299102783
Epoch 810, val loss: 1.0658105611801147
Epoch 820, training loss: 627.2579345703125 = 0.5127499103546143 + 100.0 * 6.267451763153076
Epoch 820, val loss: 1.0666474103927612
Epoch 830, training loss: 627.1207275390625 = 0.502068281173706 + 100.0 * 6.266186237335205
Epoch 830, val loss: 1.0677679777145386
Epoch 840, training loss: 627.5394287109375 = 0.49158212542533875 + 100.0 * 6.27047872543335
Epoch 840, val loss: 1.06931471824646
Epoch 850, training loss: 626.974853515625 = 0.4811548590660095 + 100.0 * 6.264936923980713
Epoch 850, val loss: 1.070408821105957
Epoch 860, training loss: 626.7651977539062 = 0.4710069000720978 + 100.0 * 6.262941837310791
Epoch 860, val loss: 1.0721684694290161
Epoch 870, training loss: 626.6810913085938 = 0.4610966742038727 + 100.0 * 6.262199878692627
Epoch 870, val loss: 1.0741900205612183
Epoch 880, training loss: 627.0865478515625 = 0.451369047164917 + 100.0 * 6.266351699829102
Epoch 880, val loss: 1.0761007070541382
Epoch 890, training loss: 626.7394409179688 = 0.4417314827442169 + 100.0 * 6.262977123260498
Epoch 890, val loss: 1.0787253379821777
Epoch 900, training loss: 626.8065795898438 = 0.4323141872882843 + 100.0 * 6.263742446899414
Epoch 900, val loss: 1.0811551809310913
Epoch 910, training loss: 626.6369018554688 = 0.42304351925849915 + 100.0 * 6.262138366699219
Epoch 910, val loss: 1.0838770866394043
Epoch 920, training loss: 626.3118286132812 = 0.41391873359680176 + 100.0 * 6.258978843688965
Epoch 920, val loss: 1.0866284370422363
Epoch 930, training loss: 626.1763305664062 = 0.4050081670284271 + 100.0 * 6.2577128410339355
Epoch 930, val loss: 1.0896967649459839
Epoch 940, training loss: 626.1110229492188 = 0.39630183577537537 + 100.0 * 6.257147312164307
Epoch 940, val loss: 1.0928469896316528
Epoch 950, training loss: 626.1463012695312 = 0.38772422075271606 + 100.0 * 6.2575860023498535
Epoch 950, val loss: 1.096244215965271
Epoch 960, training loss: 626.271728515625 = 0.3793277144432068 + 100.0 * 6.2589240074157715
Epoch 960, val loss: 1.100123643875122
Epoch 970, training loss: 626.1356811523438 = 0.37090054154396057 + 100.0 * 6.25764799118042
Epoch 970, val loss: 1.1033741235733032
Epoch 980, training loss: 625.7396240234375 = 0.36272627115249634 + 100.0 * 6.2537689208984375
Epoch 980, val loss: 1.1074225902557373
Epoch 990, training loss: 625.6787109375 = 0.3547491729259491 + 100.0 * 6.253239631652832
Epoch 990, val loss: 1.1115297079086304
Epoch 1000, training loss: 625.5415649414062 = 0.34694626927375793 + 100.0 * 6.251946449279785
Epoch 1000, val loss: 1.115805983543396
Epoch 1010, training loss: 625.515380859375 = 0.3392822742462158 + 100.0 * 6.251760959625244
Epoch 1010, val loss: 1.1202313899993896
Epoch 1020, training loss: 626.0337524414062 = 0.3317467272281647 + 100.0 * 6.257020473480225
Epoch 1020, val loss: 1.1248186826705933
Epoch 1030, training loss: 625.4171142578125 = 0.3241578936576843 + 100.0 * 6.250929355621338
Epoch 1030, val loss: 1.1290061473846436
Epoch 1040, training loss: 625.3211059570312 = 0.31682154536247253 + 100.0 * 6.250042915344238
Epoch 1040, val loss: 1.133690595626831
Epoch 1050, training loss: 625.2080078125 = 0.3096708059310913 + 100.0 * 6.248982906341553
Epoch 1050, val loss: 1.1386895179748535
Epoch 1060, training loss: 625.157470703125 = 0.30269959568977356 + 100.0 * 6.2485480308532715
Epoch 1060, val loss: 1.1438132524490356
Epoch 1070, training loss: 625.432861328125 = 0.29580384492874146 + 100.0 * 6.251370906829834
Epoch 1070, val loss: 1.1490225791931152
Epoch 1080, training loss: 625.0556030273438 = 0.2889869511127472 + 100.0 * 6.247665882110596
Epoch 1080, val loss: 1.1542150974273682
Epoch 1090, training loss: 624.939453125 = 0.282360315322876 + 100.0 * 6.246571063995361
Epoch 1090, val loss: 1.1597992181777954
Epoch 1100, training loss: 624.9589233398438 = 0.2759094834327698 + 100.0 * 6.246829986572266
Epoch 1100, val loss: 1.165504813194275
Epoch 1110, training loss: 625.0357055664062 = 0.2695159316062927 + 100.0 * 6.247661590576172
Epoch 1110, val loss: 1.1712418794631958
Epoch 1120, training loss: 624.7946166992188 = 0.2632105350494385 + 100.0 * 6.245314121246338
Epoch 1120, val loss: 1.1769508123397827
Epoch 1130, training loss: 624.8323364257812 = 0.2570815086364746 + 100.0 * 6.245752334594727
Epoch 1130, val loss: 1.1829676628112793
Epoch 1140, training loss: 624.6349487304688 = 0.25110676884651184 + 100.0 * 6.243838310241699
Epoch 1140, val loss: 1.189193606376648
Epoch 1150, training loss: 624.5135498046875 = 0.24526283144950867 + 100.0 * 6.242682933807373
Epoch 1150, val loss: 1.1955795288085938
Epoch 1160, training loss: 624.8493041992188 = 0.23959362506866455 + 100.0 * 6.246096611022949
Epoch 1160, val loss: 1.202117681503296
Epoch 1170, training loss: 624.7512817382812 = 0.23394955694675446 + 100.0 * 6.245173454284668
Epoch 1170, val loss: 1.2083122730255127
Epoch 1180, training loss: 624.473876953125 = 0.22840750217437744 + 100.0 * 6.242455005645752
Epoch 1180, val loss: 1.214820384979248
Epoch 1190, training loss: 624.3687133789062 = 0.22305673360824585 + 100.0 * 6.241456031799316
Epoch 1190, val loss: 1.2215217351913452
Epoch 1200, training loss: 624.3359985351562 = 0.2178295999765396 + 100.0 * 6.24118185043335
Epoch 1200, val loss: 1.2283332347869873
Epoch 1210, training loss: 624.303466796875 = 0.2127271145582199 + 100.0 * 6.240907669067383
Epoch 1210, val loss: 1.2353771924972534
Epoch 1220, training loss: 624.3803100585938 = 0.20772618055343628 + 100.0 * 6.241725921630859
Epoch 1220, val loss: 1.2425885200500488
Epoch 1230, training loss: 624.1742553710938 = 0.20279869437217712 + 100.0 * 6.2397141456604
Epoch 1230, val loss: 1.2494478225708008
Epoch 1240, training loss: 624.1260986328125 = 0.1979994773864746 + 100.0 * 6.239280700683594
Epoch 1240, val loss: 1.256805181503296
Epoch 1250, training loss: 624.0195922851562 = 0.1933436393737793 + 100.0 * 6.238262176513672
Epoch 1250, val loss: 1.264047622680664
Epoch 1260, training loss: 624.1751708984375 = 0.1888168752193451 + 100.0 * 6.239863395690918
Epoch 1260, val loss: 1.2714570760726929
Epoch 1270, training loss: 623.966064453125 = 0.18436689674854279 + 100.0 * 6.23781681060791
Epoch 1270, val loss: 1.2790566682815552
Epoch 1280, training loss: 623.78564453125 = 0.1800200194120407 + 100.0 * 6.236056804656982
Epoch 1280, val loss: 1.2864710092544556
Epoch 1290, training loss: 623.7330322265625 = 0.17579355835914612 + 100.0 * 6.235572338104248
Epoch 1290, val loss: 1.2941008806228638
Epoch 1300, training loss: 624.0321044921875 = 0.17171284556388855 + 100.0 * 6.2386040687561035
Epoch 1300, val loss: 1.3018022775650024
Epoch 1310, training loss: 623.849609375 = 0.1676367074251175 + 100.0 * 6.236819267272949
Epoch 1310, val loss: 1.3095297813415527
Epoch 1320, training loss: 623.7459106445312 = 0.16370461881160736 + 100.0 * 6.2358222007751465
Epoch 1320, val loss: 1.3169598579406738
Epoch 1330, training loss: 623.6801147460938 = 0.15987642109394073 + 100.0 * 6.235202789306641
Epoch 1330, val loss: 1.3250013589859009
Epoch 1340, training loss: 623.591552734375 = 0.1561477929353714 + 100.0 * 6.234354496002197
Epoch 1340, val loss: 1.3327656984329224
Epoch 1350, training loss: 624.0328369140625 = 0.15253320336341858 + 100.0 * 6.238803386688232
Epoch 1350, val loss: 1.3405625820159912
Epoch 1360, training loss: 623.6266479492188 = 0.14894632995128632 + 100.0 * 6.234777450561523
Epoch 1360, val loss: 1.3483352661132812
Epoch 1370, training loss: 623.3638305664062 = 0.14545822143554688 + 100.0 * 6.232183933258057
Epoch 1370, val loss: 1.3561965227127075
Epoch 1380, training loss: 623.2870483398438 = 0.14210805296897888 + 100.0 * 6.231449604034424
Epoch 1380, val loss: 1.3642195463180542
Epoch 1390, training loss: 623.64306640625 = 0.13885921239852905 + 100.0 * 6.235042095184326
Epoch 1390, val loss: 1.3722479343414307
Epoch 1400, training loss: 623.2859497070312 = 0.13562023639678955 + 100.0 * 6.231503486633301
Epoch 1400, val loss: 1.379899501800537
Epoch 1410, training loss: 623.279541015625 = 0.1324831247329712 + 100.0 * 6.231470584869385
Epoch 1410, val loss: 1.388026475906372
Epoch 1420, training loss: 623.2426147460938 = 0.12944503128528595 + 100.0 * 6.231131553649902
Epoch 1420, val loss: 1.3959414958953857
Epoch 1430, training loss: 623.4298095703125 = 0.12649059295654297 + 100.0 * 6.233033180236816
Epoch 1430, val loss: 1.4040956497192383
Epoch 1440, training loss: 623.1808471679688 = 0.12358050793409348 + 100.0 * 6.230572700500488
Epoch 1440, val loss: 1.4118947982788086
Epoch 1450, training loss: 623.154052734375 = 0.12076589465141296 + 100.0 * 6.230332374572754
Epoch 1450, val loss: 1.4199336767196655
Epoch 1460, training loss: 623.1085815429688 = 0.11800297349691391 + 100.0 * 6.22990608215332
Epoch 1460, val loss: 1.4277130365371704
Epoch 1470, training loss: 623.015869140625 = 0.1153230294585228 + 100.0 * 6.229005813598633
Epoch 1470, val loss: 1.4356911182403564
Epoch 1480, training loss: 622.914794921875 = 0.11271671950817108 + 100.0 * 6.228020668029785
Epoch 1480, val loss: 1.4437609910964966
Epoch 1490, training loss: 622.8594970703125 = 0.11020855605602264 + 100.0 * 6.227492809295654
Epoch 1490, val loss: 1.4518249034881592
Epoch 1500, training loss: 623.5723266601562 = 0.10778999328613281 + 100.0 * 6.234645366668701
Epoch 1500, val loss: 1.459921956062317
Epoch 1510, training loss: 623.0744018554688 = 0.10530271381139755 + 100.0 * 6.229691028594971
Epoch 1510, val loss: 1.467495083808899
Epoch 1520, training loss: 622.9071044921875 = 0.10295393317937851 + 100.0 * 6.228041648864746
Epoch 1520, val loss: 1.4754853248596191
Epoch 1530, training loss: 622.7068481445312 = 0.10065782070159912 + 100.0 * 6.226061820983887
Epoch 1530, val loss: 1.4833303689956665
Epoch 1540, training loss: 622.7486572265625 = 0.09846288710832596 + 100.0 * 6.226501941680908
Epoch 1540, val loss: 1.4913465976715088
Epoch 1550, training loss: 622.8717041015625 = 0.09629550576210022 + 100.0 * 6.227753639221191
Epoch 1550, val loss: 1.498932123184204
Epoch 1560, training loss: 622.74560546875 = 0.0941445603966713 + 100.0 * 6.22651481628418
Epoch 1560, val loss: 1.5066801309585571
Epoch 1570, training loss: 622.5762329101562 = 0.0920843556523323 + 100.0 * 6.224841594696045
Epoch 1570, val loss: 1.5144717693328857
Epoch 1580, training loss: 622.7828979492188 = 0.09008949249982834 + 100.0 * 6.226928234100342
Epoch 1580, val loss: 1.5221586227416992
Epoch 1590, training loss: 622.5736083984375 = 0.08812663704156876 + 100.0 * 6.224854469299316
Epoch 1590, val loss: 1.530041217803955
Epoch 1600, training loss: 622.8877563476562 = 0.08622732758522034 + 100.0 * 6.228014945983887
Epoch 1600, val loss: 1.537591814994812
Epoch 1610, training loss: 622.5274658203125 = 0.08434323966503143 + 100.0 * 6.224431037902832
Epoch 1610, val loss: 1.5452313423156738
Epoch 1620, training loss: 622.410888671875 = 0.08253607153892517 + 100.0 * 6.223283767700195
Epoch 1620, val loss: 1.5529861450195312
Epoch 1630, training loss: 622.3909912109375 = 0.08079111576080322 + 100.0 * 6.223101615905762
Epoch 1630, val loss: 1.560755729675293
Epoch 1640, training loss: 622.9013061523438 = 0.07911530882120132 + 100.0 * 6.228221893310547
Epoch 1640, val loss: 1.5683232545852661
Epoch 1650, training loss: 622.41552734375 = 0.0774063691496849 + 100.0 * 6.223381519317627
Epoch 1650, val loss: 1.5756231546401978
Epoch 1660, training loss: 622.2728881835938 = 0.07577934861183167 + 100.0 * 6.221971035003662
Epoch 1660, val loss: 1.5832082033157349
Epoch 1670, training loss: 622.2200317382812 = 0.07420429587364197 + 100.0 * 6.2214579582214355
Epoch 1670, val loss: 1.5906741619110107
Epoch 1680, training loss: 622.4896240234375 = 0.07268448919057846 + 100.0 * 6.2241692543029785
Epoch 1680, val loss: 1.598057746887207
Epoch 1690, training loss: 622.3445434570312 = 0.07116733491420746 + 100.0 * 6.222733974456787
Epoch 1690, val loss: 1.6055011749267578
Epoch 1700, training loss: 622.3170166015625 = 0.06967934221029282 + 100.0 * 6.22247314453125
Epoch 1700, val loss: 1.6124818325042725
Epoch 1710, training loss: 622.220703125 = 0.06824235618114471 + 100.0 * 6.221524715423584
Epoch 1710, val loss: 1.619882345199585
Epoch 1720, training loss: 622.077392578125 = 0.06686212867498398 + 100.0 * 6.2201056480407715
Epoch 1720, val loss: 1.6271758079528809
Epoch 1730, training loss: 622.3455810546875 = 0.06552880257368088 + 100.0 * 6.222800254821777
Epoch 1730, val loss: 1.6343568563461304
Epoch 1740, training loss: 622.0416259765625 = 0.06419641524553299 + 100.0 * 6.21977424621582
Epoch 1740, val loss: 1.6414985656738281
Epoch 1750, training loss: 622.050537109375 = 0.0629103034734726 + 100.0 * 6.219875812530518
Epoch 1750, val loss: 1.6486179828643799
Epoch 1760, training loss: 622.2628784179688 = 0.06167628616094589 + 100.0 * 6.222011566162109
Epoch 1760, val loss: 1.6556642055511475
Epoch 1770, training loss: 622.0325317382812 = 0.06043970212340355 + 100.0 * 6.219720840454102
Epoch 1770, val loss: 1.6625442504882812
Epoch 1780, training loss: 622.2321166992188 = 0.0592493861913681 + 100.0 * 6.221728801727295
Epoch 1780, val loss: 1.6694881916046143
Epoch 1790, training loss: 621.952392578125 = 0.05808569863438606 + 100.0 * 6.218942642211914
Epoch 1790, val loss: 1.6766377687454224
Epoch 1800, training loss: 621.9133911132812 = 0.056954964995384216 + 100.0 * 6.218564510345459
Epoch 1800, val loss: 1.6835346221923828
Epoch 1810, training loss: 622.1336669921875 = 0.055873069912195206 + 100.0 * 6.220777988433838
Epoch 1810, val loss: 1.6903988122940063
Epoch 1820, training loss: 621.8463745117188 = 0.05477768927812576 + 100.0 * 6.217916011810303
Epoch 1820, val loss: 1.697142481803894
Epoch 1830, training loss: 621.9822387695312 = 0.053732287138700485 + 100.0 * 6.219285011291504
Epoch 1830, val loss: 1.7040205001831055
Epoch 1840, training loss: 622.0028686523438 = 0.05271786451339722 + 100.0 * 6.219501495361328
Epoch 1840, val loss: 1.7108196020126343
Epoch 1850, training loss: 621.9506225585938 = 0.05170348286628723 + 100.0 * 6.218989372253418
Epoch 1850, val loss: 1.7171711921691895
Epoch 1860, training loss: 621.8757934570312 = 0.050728511065244675 + 100.0 * 6.218250751495361
Epoch 1860, val loss: 1.7240900993347168
Epoch 1870, training loss: 621.6656494140625 = 0.04977300390601158 + 100.0 * 6.216159343719482
Epoch 1870, val loss: 1.7305693626403809
Epoch 1880, training loss: 621.7394409179688 = 0.04886268824338913 + 100.0 * 6.21690559387207
Epoch 1880, val loss: 1.7373018264770508
Epoch 1890, training loss: 622.0492553710938 = 0.04796399921178818 + 100.0 * 6.220012664794922
Epoch 1890, val loss: 1.7436434030532837
Epoch 1900, training loss: 621.7833251953125 = 0.04707518592476845 + 100.0 * 6.217362880706787
Epoch 1900, val loss: 1.7502856254577637
Epoch 1910, training loss: 621.71337890625 = 0.04620768874883652 + 100.0 * 6.216671943664551
Epoch 1910, val loss: 1.7566710710525513
Epoch 1920, training loss: 621.6078491210938 = 0.04536638408899307 + 100.0 * 6.215624809265137
Epoch 1920, val loss: 1.7632155418395996
Epoch 1930, training loss: 621.7344970703125 = 0.044554293155670166 + 100.0 * 6.216899394989014
Epoch 1930, val loss: 1.7696869373321533
Epoch 1940, training loss: 621.7767333984375 = 0.04374285414814949 + 100.0 * 6.217329978942871
Epoch 1940, val loss: 1.7757909297943115
Epoch 1950, training loss: 621.6082763671875 = 0.042953524738550186 + 100.0 * 6.215652942657471
Epoch 1950, val loss: 1.782252311706543
Epoch 1960, training loss: 621.4867553710938 = 0.04218503460288048 + 100.0 * 6.2144455909729
Epoch 1960, val loss: 1.7885545492172241
Epoch 1970, training loss: 621.4620971679688 = 0.04144247993826866 + 100.0 * 6.214206695556641
Epoch 1970, val loss: 1.7948306798934937
Epoch 1980, training loss: 621.5079956054688 = 0.04072384163737297 + 100.0 * 6.214672565460205
Epoch 1980, val loss: 1.801207423210144
Epoch 1990, training loss: 621.5350952148438 = 0.04001646116375923 + 100.0 * 6.2149505615234375
Epoch 1990, val loss: 1.807336449623108
Epoch 2000, training loss: 621.8699951171875 = 0.039319511502981186 + 100.0 * 6.218307018280029
Epoch 2000, val loss: 1.8132892847061157
Epoch 2010, training loss: 621.479248046875 = 0.038614146411418915 + 100.0 * 6.2144060134887695
Epoch 2010, val loss: 1.8192636966705322
Epoch 2020, training loss: 621.3763427734375 = 0.037949338555336 + 100.0 * 6.213383674621582
Epoch 2020, val loss: 1.8253523111343384
Epoch 2030, training loss: 621.30419921875 = 0.037300024181604385 + 100.0 * 6.2126688957214355
Epoch 2030, val loss: 1.8314809799194336
Epoch 2040, training loss: 621.6736450195312 = 0.03668835014104843 + 100.0 * 6.21636962890625
Epoch 2040, val loss: 1.8373706340789795
Epoch 2050, training loss: 621.3128662109375 = 0.03604709357023239 + 100.0 * 6.212768077850342
Epoch 2050, val loss: 1.843208909034729
Epoch 2060, training loss: 621.3837280273438 = 0.03544159233570099 + 100.0 * 6.213482856750488
Epoch 2060, val loss: 1.849057674407959
Epoch 2070, training loss: 621.5115356445312 = 0.03484732285141945 + 100.0 * 6.214766979217529
Epoch 2070, val loss: 1.8548977375030518
Epoch 2080, training loss: 621.2532348632812 = 0.0342564582824707 + 100.0 * 6.2121901512146
Epoch 2080, val loss: 1.8605798482894897
Epoch 2090, training loss: 621.181396484375 = 0.0336943082511425 + 100.0 * 6.211476802825928
Epoch 2090, val loss: 1.8664746284484863
Epoch 2100, training loss: 621.3836059570312 = 0.03315526992082596 + 100.0 * 6.213504791259766
Epoch 2100, val loss: 1.8723034858703613
Epoch 2110, training loss: 621.1510620117188 = 0.03260418400168419 + 100.0 * 6.211184978485107
Epoch 2110, val loss: 1.8778616189956665
Epoch 2120, training loss: 621.1154174804688 = 0.03207295760512352 + 100.0 * 6.21083402633667
Epoch 2120, val loss: 1.8835742473602295
Epoch 2130, training loss: 621.2262573242188 = 0.03156418725848198 + 100.0 * 6.211946964263916
Epoch 2130, val loss: 1.8892117738723755
Epoch 2140, training loss: 621.2040405273438 = 0.031057441607117653 + 100.0 * 6.211729526519775
Epoch 2140, val loss: 1.8948137760162354
Epoch 2150, training loss: 621.309814453125 = 0.03057226724922657 + 100.0 * 6.21279239654541
Epoch 2150, val loss: 1.900481939315796
Epoch 2160, training loss: 621.3455200195312 = 0.03007649816572666 + 100.0 * 6.213154315948486
Epoch 2160, val loss: 1.9056462049484253
Epoch 2170, training loss: 621.0364990234375 = 0.02958785369992256 + 100.0 * 6.210069179534912
Epoch 2170, val loss: 1.9111429452896118
Epoch 2180, training loss: 620.9664916992188 = 0.029123473912477493 + 100.0 * 6.209373950958252
Epoch 2180, val loss: 1.9166501760482788
Epoch 2190, training loss: 620.93505859375 = 0.028676530346274376 + 100.0 * 6.209063529968262
Epoch 2190, val loss: 1.9222139120101929
Epoch 2200, training loss: 621.0075073242188 = 0.02824402041733265 + 100.0 * 6.209792613983154
Epoch 2200, val loss: 1.9275643825531006
Epoch 2210, training loss: 621.4991455078125 = 0.027814339846372604 + 100.0 * 6.214713096618652
Epoch 2210, val loss: 1.9326459169387817
Epoch 2220, training loss: 621.0502319335938 = 0.027376489713788033 + 100.0 * 6.210228443145752
Epoch 2220, val loss: 1.9379889965057373
Epoch 2230, training loss: 620.8759155273438 = 0.02695608139038086 + 100.0 * 6.208489418029785
Epoch 2230, val loss: 1.9432628154754639
Epoch 2240, training loss: 620.8892211914062 = 0.02655625157058239 + 100.0 * 6.208626747131348
Epoch 2240, val loss: 1.9485695362091064
Epoch 2250, training loss: 621.289794921875 = 0.02617298997938633 + 100.0 * 6.2126359939575195
Epoch 2250, val loss: 1.9537514448165894
Epoch 2260, training loss: 621.0751953125 = 0.025770459324121475 + 100.0 * 6.210494518280029
Epoch 2260, val loss: 1.9588080644607544
Epoch 2270, training loss: 620.8466186523438 = 0.025381170213222504 + 100.0 * 6.208212375640869
Epoch 2270, val loss: 1.9639309644699097
Epoch 2280, training loss: 620.8142700195312 = 0.025007978081703186 + 100.0 * 6.207892417907715
Epoch 2280, val loss: 1.9690924882888794
Epoch 2290, training loss: 620.8871459960938 = 0.024650603532791138 + 100.0 * 6.208624839782715
Epoch 2290, val loss: 1.9742727279663086
Epoch 2300, training loss: 621.29931640625 = 0.024302547797560692 + 100.0 * 6.212750434875488
Epoch 2300, val loss: 1.979171633720398
Epoch 2310, training loss: 620.9966430664062 = 0.023930588737130165 + 100.0 * 6.2097272872924805
Epoch 2310, val loss: 1.9839954376220703
Epoch 2320, training loss: 620.8046264648438 = 0.023587867617607117 + 100.0 * 6.207810878753662
Epoch 2320, val loss: 1.9891167879104614
Epoch 2330, training loss: 620.711669921875 = 0.023248182609677315 + 100.0 * 6.206883907318115
Epoch 2330, val loss: 1.9940420389175415
Epoch 2340, training loss: 621.0090942382812 = 0.02292689122259617 + 100.0 * 6.2098612785339355
Epoch 2340, val loss: 1.9990711212158203
Epoch 2350, training loss: 620.7044677734375 = 0.022594857960939407 + 100.0 * 6.206818580627441
Epoch 2350, val loss: 2.004056930541992
Epoch 2360, training loss: 620.6765747070312 = 0.022270070388913155 + 100.0 * 6.20654296875
Epoch 2360, val loss: 2.008920907974243
Epoch 2370, training loss: 620.844970703125 = 0.021957799792289734 + 100.0 * 6.208230018615723
Epoch 2370, val loss: 2.013929605484009
Epoch 2380, training loss: 620.7937622070312 = 0.021645167842507362 + 100.0 * 6.20772123336792
Epoch 2380, val loss: 2.0187900066375732
Epoch 2390, training loss: 620.6898803710938 = 0.02133719064295292 + 100.0 * 6.2066850662231445
Epoch 2390, val loss: 2.0235586166381836
Epoch 2400, training loss: 620.6011962890625 = 0.021039877086877823 + 100.0 * 6.205801486968994
Epoch 2400, val loss: 2.028496026992798
Epoch 2410, training loss: 621.2799682617188 = 0.020761316642165184 + 100.0 * 6.212592124938965
Epoch 2410, val loss: 2.0331733226776123
Epoch 2420, training loss: 620.85302734375 = 0.020459553226828575 + 100.0 * 6.2083258628845215
Epoch 2420, val loss: 2.0375962257385254
Epoch 2430, training loss: 620.6029663085938 = 0.02017127349972725 + 100.0 * 6.2058281898498535
Epoch 2430, val loss: 2.0423779487609863
Epoch 2440, training loss: 620.4984130859375 = 0.019897790625691414 + 100.0 * 6.204785346984863
Epoch 2440, val loss: 2.0470824241638184
Epoch 2450, training loss: 620.4989013671875 = 0.019634265452623367 + 100.0 * 6.204792499542236
Epoch 2450, val loss: 2.0518476963043213
Epoch 2460, training loss: 621.0579833984375 = 0.019383590668439865 + 100.0 * 6.210385799407959
Epoch 2460, val loss: 2.056273937225342
Epoch 2470, training loss: 620.5812377929688 = 0.019114360213279724 + 100.0 * 6.205621242523193
Epoch 2470, val loss: 2.0609447956085205
Epoch 2480, training loss: 620.6831665039062 = 0.018859663978219032 + 100.0 * 6.206643104553223
Epoch 2480, val loss: 2.0651814937591553
Epoch 2490, training loss: 620.668212890625 = 0.01860794425010681 + 100.0 * 6.206495761871338
Epoch 2490, val loss: 2.0695977210998535
Epoch 2500, training loss: 620.7885131835938 = 0.01836031675338745 + 100.0 * 6.207701683044434
Epoch 2500, val loss: 2.073887348175049
Epoch 2510, training loss: 620.4351806640625 = 0.018116697669029236 + 100.0 * 6.2041707038879395
Epoch 2510, val loss: 2.078455924987793
Epoch 2520, training loss: 620.3878173828125 = 0.01788523606956005 + 100.0 * 6.203699111938477
Epoch 2520, val loss: 2.0828323364257812
Epoch 2530, training loss: 620.3768920898438 = 0.017659969627857208 + 100.0 * 6.203592777252197
Epoch 2530, val loss: 2.0872814655303955
Epoch 2540, training loss: 620.6986694335938 = 0.017444703727960587 + 100.0 * 6.206812381744385
Epoch 2540, val loss: 2.091644525527954
Epoch 2550, training loss: 620.5245971679688 = 0.017218386754393578 + 100.0 * 6.205073833465576
Epoch 2550, val loss: 2.095792293548584
Epoch 2560, training loss: 620.3980102539062 = 0.016992179676890373 + 100.0 * 6.203810214996338
Epoch 2560, val loss: 2.0997278690338135
Epoch 2570, training loss: 620.3715209960938 = 0.016775306314229965 + 100.0 * 6.203547477722168
Epoch 2570, val loss: 2.1039798259735107
Epoch 2580, training loss: 620.407470703125 = 0.016567913815379143 + 100.0 * 6.203909397125244
Epoch 2580, val loss: 2.108351945877075
Epoch 2590, training loss: 620.3499145507812 = 0.016364460811018944 + 100.0 * 6.203335285186768
Epoch 2590, val loss: 2.1125378608703613
Epoch 2600, training loss: 620.5839233398438 = 0.016167381778359413 + 100.0 * 6.205677509307861
Epoch 2600, val loss: 2.1166083812713623
Epoch 2610, training loss: 620.4317626953125 = 0.015968721359968185 + 100.0 * 6.204157829284668
Epoch 2610, val loss: 2.1207005977630615
Epoch 2620, training loss: 620.2647705078125 = 0.015767481178045273 + 100.0 * 6.202490329742432
Epoch 2620, val loss: 2.124737024307251
Epoch 2630, training loss: 620.3612670898438 = 0.015576982870697975 + 100.0 * 6.203456878662109
Epoch 2630, val loss: 2.128772258758545
Epoch 2640, training loss: 620.5245971679688 = 0.015395567752420902 + 100.0 * 6.205092430114746
Epoch 2640, val loss: 2.1328275203704834
Epoch 2650, training loss: 620.3114624023438 = 0.015204422175884247 + 100.0 * 6.202962398529053
Epoch 2650, val loss: 2.136629343032837
Epoch 2660, training loss: 620.2703857421875 = 0.015019788406789303 + 100.0 * 6.202553749084473
Epoch 2660, val loss: 2.1406311988830566
Epoch 2670, training loss: 620.245849609375 = 0.014845350757241249 + 100.0 * 6.202310085296631
Epoch 2670, val loss: 2.1446597576141357
Epoch 2680, training loss: 620.2598876953125 = 0.014672868885099888 + 100.0 * 6.202452659606934
Epoch 2680, val loss: 2.148541212081909
Epoch 2690, training loss: 620.2391357421875 = 0.014502898789942265 + 100.0 * 6.202246189117432
Epoch 2690, val loss: 2.1525323390960693
Epoch 2700, training loss: 620.3153076171875 = 0.014341803267598152 + 100.0 * 6.203009605407715
Epoch 2700, val loss: 2.1564345359802246
Epoch 2710, training loss: 620.3652954101562 = 0.014176802709698677 + 100.0 * 6.2035112380981445
Epoch 2710, val loss: 2.16019344329834
Epoch 2720, training loss: 620.1592407226562 = 0.014010180719196796 + 100.0 * 6.201451778411865
Epoch 2720, val loss: 2.163743734359741
Epoch 2730, training loss: 620.3853149414062 = 0.013854548335075378 + 100.0 * 6.203714847564697
Epoch 2730, val loss: 2.1676671504974365
Epoch 2740, training loss: 620.1405029296875 = 0.01369312684983015 + 100.0 * 6.201268196105957
Epoch 2740, val loss: 2.1711549758911133
Epoch 2750, training loss: 620.1393432617188 = 0.013539769686758518 + 100.0 * 6.201258182525635
Epoch 2750, val loss: 2.1748666763305664
Epoch 2760, training loss: 620.2521362304688 = 0.013393765315413475 + 100.0 * 6.202387809753418
Epoch 2760, val loss: 2.178635835647583
Epoch 2770, training loss: 620.362060546875 = 0.01324551459401846 + 100.0 * 6.203488349914551
Epoch 2770, val loss: 2.182006359100342
Epoch 2780, training loss: 620.2113037109375 = 0.01309276930987835 + 100.0 * 6.201981544494629
Epoch 2780, val loss: 2.185666561126709
Epoch 2790, training loss: 620.0271606445312 = 0.01294772233814001 + 100.0 * 6.2001423835754395
Epoch 2790, val loss: 2.1891353130340576
Epoch 2800, training loss: 620.0634765625 = 0.012810194864869118 + 100.0 * 6.200506687164307
Epoch 2800, val loss: 2.1928625106811523
Epoch 2810, training loss: 620.5516357421875 = 0.012677943333983421 + 100.0 * 6.205389499664307
Epoch 2810, val loss: 2.1961584091186523
Epoch 2820, training loss: 620.1463012695312 = 0.012536912225186825 + 100.0 * 6.201337814331055
Epoch 2820, val loss: 2.1996450424194336
Epoch 2830, training loss: 619.9710083007812 = 0.012398635037243366 + 100.0 * 6.199586391448975
Epoch 2830, val loss: 2.203233480453491
Epoch 2840, training loss: 619.9445190429688 = 0.012270762585103512 + 100.0 * 6.199322700500488
Epoch 2840, val loss: 2.206728219985962
Epoch 2850, training loss: 620.2167358398438 = 0.012148773297667503 + 100.0 * 6.202045917510986
Epoch 2850, val loss: 2.2101776599884033
Epoch 2860, training loss: 620.2205200195312 = 0.012018266133964062 + 100.0 * 6.202085018157959
Epoch 2860, val loss: 2.21328067779541
Epoch 2870, training loss: 619.9398193359375 = 0.011886704713106155 + 100.0 * 6.199279308319092
Epoch 2870, val loss: 2.216386556625366
Epoch 2880, training loss: 619.8916015625 = 0.011762337759137154 + 100.0 * 6.198798179626465
Epoch 2880, val loss: 2.2198870182037354
Epoch 2890, training loss: 619.8580932617188 = 0.011644383892416954 + 100.0 * 6.198464393615723
Epoch 2890, val loss: 2.2233846187591553
Epoch 2900, training loss: 620.234130859375 = 0.011537887156009674 + 100.0 * 6.202225685119629
Epoch 2900, val loss: 2.2267844676971436
Epoch 2910, training loss: 619.9535522460938 = 0.011410822160542011 + 100.0 * 6.199421405792236
Epoch 2910, val loss: 2.229527473449707
Epoch 2920, training loss: 619.8793334960938 = 0.011291556991636753 + 100.0 * 6.198680400848389
Epoch 2920, val loss: 2.2328031063079834
Epoch 2930, training loss: 619.8577880859375 = 0.011177361011505127 + 100.0 * 6.1984663009643555
Epoch 2930, val loss: 2.236039161682129
Epoch 2940, training loss: 619.982177734375 = 0.011074477806687355 + 100.0 * 6.199711322784424
Epoch 2940, val loss: 2.239391326904297
Epoch 2950, training loss: 620.0855712890625 = 0.010963059961795807 + 100.0 * 6.200746059417725
Epoch 2950, val loss: 2.2422115802764893
Epoch 2960, training loss: 620.3317260742188 = 0.010853003710508347 + 100.0 * 6.2032084465026855
Epoch 2960, val loss: 2.2450363636016846
Epoch 2970, training loss: 619.923095703125 = 0.010743130929768085 + 100.0 * 6.199123382568359
Epoch 2970, val loss: 2.2482552528381348
Epoch 2980, training loss: 619.8201293945312 = 0.010638551786541939 + 100.0 * 6.198094844818115
Epoch 2980, val loss: 2.251227855682373
Epoch 2990, training loss: 619.8328857421875 = 0.01054068747907877 + 100.0 * 6.198223114013672
Epoch 2990, val loss: 2.25449275970459
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6370370370370371
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 861.6162719726562 = 1.9305099248886108 + 100.0 * 8.596858024597168
Epoch 0, val loss: 1.9307011365890503
Epoch 10, training loss: 861.5377807617188 = 1.9227231740951538 + 100.0 * 8.596150398254395
Epoch 10, val loss: 1.9227272272109985
Epoch 20, training loss: 861.0440063476562 = 1.913132667541504 + 100.0 * 8.59130859375
Epoch 20, val loss: 1.9127235412597656
Epoch 30, training loss: 857.7594604492188 = 1.9009201526641846 + 100.0 * 8.558585166931152
Epoch 30, val loss: 1.8999005556106567
Epoch 40, training loss: 836.7423095703125 = 1.8863619565963745 + 100.0 * 8.348559379577637
Epoch 40, val loss: 1.8848975896835327
Epoch 50, training loss: 763.532958984375 = 1.8708364963531494 + 100.0 * 7.616621017456055
Epoch 50, val loss: 1.8691997528076172
Epoch 60, training loss: 744.1360473632812 = 1.856521487236023 + 100.0 * 7.422795295715332
Epoch 60, val loss: 1.8548871278762817
Epoch 70, training loss: 722.9191284179688 = 1.8450602293014526 + 100.0 * 7.21074104309082
Epoch 70, val loss: 1.8429255485534668
Epoch 80, training loss: 712.5 = 1.8329615592956543 + 100.0 * 7.106670379638672
Epoch 80, val loss: 1.830368161201477
Epoch 90, training loss: 704.7788696289062 = 1.8213742971420288 + 100.0 * 7.029575347900391
Epoch 90, val loss: 1.8185157775878906
Epoch 100, training loss: 695.818359375 = 1.8115326166152954 + 100.0 * 6.940068244934082
Epoch 100, val loss: 1.808332920074463
Epoch 110, training loss: 689.2598876953125 = 1.8021811246871948 + 100.0 * 6.874577045440674
Epoch 110, val loss: 1.7990624904632568
Epoch 120, training loss: 682.8385620117188 = 1.7935678958892822 + 100.0 * 6.810449600219727
Epoch 120, val loss: 1.790594220161438
Epoch 130, training loss: 675.713134765625 = 1.7866114377975464 + 100.0 * 6.739264965057373
Epoch 130, val loss: 1.783843755722046
Epoch 140, training loss: 670.0946655273438 = 1.7803092002868652 + 100.0 * 6.683143615722656
Epoch 140, val loss: 1.7777384519577026
Epoch 150, training loss: 665.1259765625 = 1.773077130317688 + 100.0 * 6.633529186248779
Epoch 150, val loss: 1.7710076570510864
Epoch 160, training loss: 660.9785766601562 = 1.7651610374450684 + 100.0 * 6.592134475708008
Epoch 160, val loss: 1.7640435695648193
Epoch 170, training loss: 658.1870727539062 = 1.7566431760787964 + 100.0 * 6.564304351806641
Epoch 170, val loss: 1.756475567817688
Epoch 180, training loss: 655.647216796875 = 1.7470134496688843 + 100.0 * 6.539001941680908
Epoch 180, val loss: 1.7481673955917358
Epoch 190, training loss: 653.5309448242188 = 1.736527919769287 + 100.0 * 6.5179443359375
Epoch 190, val loss: 1.7389183044433594
Epoch 200, training loss: 652.1996459960938 = 1.7252238988876343 + 100.0 * 6.504744052886963
Epoch 200, val loss: 1.7289533615112305
Epoch 210, training loss: 649.9495849609375 = 1.712681531906128 + 100.0 * 6.4823689460754395
Epoch 210, val loss: 1.71818208694458
Epoch 220, training loss: 648.3695068359375 = 1.6990890502929688 + 100.0 * 6.466704368591309
Epoch 220, val loss: 1.7064964771270752
Epoch 230, training loss: 647.15234375 = 1.6842325925827026 + 100.0 * 6.454681396484375
Epoch 230, val loss: 1.6938987970352173
Epoch 240, training loss: 646.2816772460938 = 1.6677924394607544 + 100.0 * 6.446138858795166
Epoch 240, val loss: 1.6799733638763428
Epoch 250, training loss: 645.1367797851562 = 1.6497962474822998 + 100.0 * 6.43487024307251
Epoch 250, val loss: 1.66495680809021
Epoch 260, training loss: 644.1884765625 = 1.6304062604904175 + 100.0 * 6.4255805015563965
Epoch 260, val loss: 1.6487804651260376
Epoch 270, training loss: 643.5437622070312 = 1.609435796737671 + 100.0 * 6.4193434715271
Epoch 270, val loss: 1.6313519477844238
Epoch 280, training loss: 642.64794921875 = 1.586990237236023 + 100.0 * 6.410609722137451
Epoch 280, val loss: 1.6128307580947876
Epoch 290, training loss: 641.7555541992188 = 1.5631673336029053 + 100.0 * 6.401923656463623
Epoch 290, val loss: 1.5932343006134033
Epoch 300, training loss: 641.0846557617188 = 1.5379257202148438 + 100.0 * 6.395467281341553
Epoch 300, val loss: 1.5726603269577026
Epoch 310, training loss: 640.6444702148438 = 1.5113667249679565 + 100.0 * 6.391331195831299
Epoch 310, val loss: 1.551052212715149
Epoch 320, training loss: 639.7434692382812 = 1.4837065935134888 + 100.0 * 6.382597923278809
Epoch 320, val loss: 1.528769850730896
Epoch 330, training loss: 639.0419921875 = 1.4551453590393066 + 100.0 * 6.375868320465088
Epoch 330, val loss: 1.5060653686523438
Epoch 340, training loss: 638.443603515625 = 1.4258697032928467 + 100.0 * 6.370177745819092
Epoch 340, val loss: 1.4829378128051758
Epoch 350, training loss: 637.9630126953125 = 1.3958972692489624 + 100.0 * 6.365671157836914
Epoch 350, val loss: 1.4594731330871582
Epoch 360, training loss: 637.4655151367188 = 1.365320086479187 + 100.0 * 6.361001968383789
Epoch 360, val loss: 1.4357686042785645
Epoch 370, training loss: 636.9395751953125 = 1.3346108198165894 + 100.0 * 6.35605001449585
Epoch 370, val loss: 1.412269115447998
Epoch 380, training loss: 636.3878784179688 = 1.3039034605026245 + 100.0 * 6.350839614868164
Epoch 380, val loss: 1.3891987800598145
Epoch 390, training loss: 635.946533203125 = 1.2733982801437378 + 100.0 * 6.346731662750244
Epoch 390, val loss: 1.3665486574172974
Epoch 400, training loss: 635.8294067382812 = 1.2429163455963135 + 100.0 * 6.345864772796631
Epoch 400, val loss: 1.3442034721374512
Epoch 410, training loss: 635.3055419921875 = 1.2128474712371826 + 100.0 * 6.3409271240234375
Epoch 410, val loss: 1.3225501775741577
Epoch 420, training loss: 634.813720703125 = 1.1833829879760742 + 100.0 * 6.336303234100342
Epoch 420, val loss: 1.301692247390747
Epoch 430, training loss: 634.4314575195312 = 1.1546030044555664 + 100.0 * 6.332768440246582
Epoch 430, val loss: 1.2818900346755981
Epoch 440, training loss: 634.3523559570312 = 1.126524567604065 + 100.0 * 6.332258224487305
Epoch 440, val loss: 1.2628902196884155
Epoch 450, training loss: 633.81982421875 = 1.0993492603302002 + 100.0 * 6.327204704284668
Epoch 450, val loss: 1.2449688911437988
Epoch 460, training loss: 633.4718627929688 = 1.0729507207870483 + 100.0 * 6.323989391326904
Epoch 460, val loss: 1.2282283306121826
Epoch 470, training loss: 633.1695556640625 = 1.0474770069122314 + 100.0 * 6.321220397949219
Epoch 470, val loss: 1.2123271226882935
Epoch 480, training loss: 633.2606201171875 = 1.0228065252304077 + 100.0 * 6.322378158569336
Epoch 480, val loss: 1.197490930557251
Epoch 490, training loss: 632.726318359375 = 0.9988113641738892 + 100.0 * 6.317274570465088
Epoch 490, val loss: 1.183514952659607
Epoch 500, training loss: 632.310546875 = 0.9758099913597107 + 100.0 * 6.313347339630127
Epoch 500, val loss: 1.1706029176712036
Epoch 510, training loss: 632.1307983398438 = 0.9537009596824646 + 100.0 * 6.311771392822266
Epoch 510, val loss: 1.1587225198745728
Epoch 520, training loss: 632.0504150390625 = 0.9321539402008057 + 100.0 * 6.311182975769043
Epoch 520, val loss: 1.1476523876190186
Epoch 530, training loss: 631.6867065429688 = 0.9114587903022766 + 100.0 * 6.30775260925293
Epoch 530, val loss: 1.1371668577194214
Epoch 540, training loss: 631.4122314453125 = 0.891417920589447 + 100.0 * 6.305208206176758
Epoch 540, val loss: 1.1277799606323242
Epoch 550, training loss: 631.224609375 = 0.8721568584442139 + 100.0 * 6.303524494171143
Epoch 550, val loss: 1.1192493438720703
Epoch 560, training loss: 631.4979858398438 = 0.853431224822998 + 100.0 * 6.306445598602295
Epoch 560, val loss: 1.1113539934158325
Epoch 570, training loss: 630.847900390625 = 0.8352937698364258 + 100.0 * 6.300126075744629
Epoch 570, val loss: 1.1038604974746704
Epoch 580, training loss: 630.5567626953125 = 0.8176655769348145 + 100.0 * 6.297390460968018
Epoch 580, val loss: 1.0972485542297363
Epoch 590, training loss: 630.5215454101562 = 0.8006139397621155 + 100.0 * 6.297209739685059
Epoch 590, val loss: 1.091268539428711
Epoch 600, training loss: 630.4524536132812 = 0.7838039994239807 + 100.0 * 6.29668664932251
Epoch 600, val loss: 1.0853673219680786
Epoch 610, training loss: 630.106201171875 = 0.7674992084503174 + 100.0 * 6.293386936187744
Epoch 610, val loss: 1.080302357673645
Epoch 620, training loss: 629.9512939453125 = 0.7516063451766968 + 100.0 * 6.291996955871582
Epoch 620, val loss: 1.0757755041122437
Epoch 630, training loss: 629.7710571289062 = 0.7361207604408264 + 100.0 * 6.290349006652832
Epoch 630, val loss: 1.071616530418396
Epoch 640, training loss: 629.5465698242188 = 0.7210456132888794 + 100.0 * 6.288254737854004
Epoch 640, val loss: 1.067952036857605
Epoch 650, training loss: 629.6720581054688 = 0.7062639594078064 + 100.0 * 6.289658069610596
Epoch 650, val loss: 1.0646553039550781
Epoch 660, training loss: 629.380126953125 = 0.6916751861572266 + 100.0 * 6.286884784698486
Epoch 660, val loss: 1.0617554187774658
Epoch 670, training loss: 628.9933471679688 = 0.6774912476539612 + 100.0 * 6.283158302307129
Epoch 670, val loss: 1.0591920614242554
Epoch 680, training loss: 628.8536376953125 = 0.6636458039283752 + 100.0 * 6.281899929046631
Epoch 680, val loss: 1.0572084188461304
Epoch 690, training loss: 629.115234375 = 0.6500994563102722 + 100.0 * 6.284651756286621
Epoch 690, val loss: 1.0554372072219849
Epoch 700, training loss: 628.7887573242188 = 0.6366259455680847 + 100.0 * 6.281521797180176
Epoch 700, val loss: 1.0539036989212036
Epoch 710, training loss: 628.8236694335938 = 0.6235160827636719 + 100.0 * 6.282001495361328
Epoch 710, val loss: 1.0527762174606323
Epoch 720, training loss: 628.5552368164062 = 0.6106094717979431 + 100.0 * 6.279446601867676
Epoch 720, val loss: 1.0518417358398438
Epoch 730, training loss: 628.1458740234375 = 0.5979711413383484 + 100.0 * 6.275479316711426
Epoch 730, val loss: 1.0513246059417725
Epoch 740, training loss: 628.0419921875 = 0.5856331586837769 + 100.0 * 6.274563789367676
Epoch 740, val loss: 1.0511231422424316
Epoch 750, training loss: 628.6580200195312 = 0.573498547077179 + 100.0 * 6.2808451652526855
Epoch 750, val loss: 1.0510121583938599
Epoch 760, training loss: 627.8206787109375 = 0.5614026784896851 + 100.0 * 6.272593021392822
Epoch 760, val loss: 1.0511877536773682
Epoch 770, training loss: 627.75732421875 = 0.5496350526809692 + 100.0 * 6.2720770835876465
Epoch 770, val loss: 1.0515906810760498
Epoch 780, training loss: 627.5219116210938 = 0.5381803512573242 + 100.0 * 6.269836902618408
Epoch 780, val loss: 1.0524049997329712
Epoch 790, training loss: 627.7108764648438 = 0.5269173979759216 + 100.0 * 6.271839618682861
Epoch 790, val loss: 1.0533593893051147
Epoch 800, training loss: 627.6769409179688 = 0.5157978534698486 + 100.0 * 6.271611213684082
Epoch 800, val loss: 1.054545283317566
Epoch 810, training loss: 627.3062744140625 = 0.5048469305038452 + 100.0 * 6.268014430999756
Epoch 810, val loss: 1.0557305812835693
Epoch 820, training loss: 627.102783203125 = 0.49414846301078796 + 100.0 * 6.266086578369141
Epoch 820, val loss: 1.0572987794876099
Epoch 830, training loss: 627.2080078125 = 0.48366039991378784 + 100.0 * 6.267243385314941
Epoch 830, val loss: 1.0590081214904785
Epoch 840, training loss: 627.1748657226562 = 0.47335582971572876 + 100.0 * 6.26701545715332
Epoch 840, val loss: 1.0609321594238281
Epoch 850, training loss: 626.8536987304688 = 0.463103711605072 + 100.0 * 6.263906002044678
Epoch 850, val loss: 1.0628728866577148
Epoch 860, training loss: 626.7249755859375 = 0.4531998038291931 + 100.0 * 6.2627177238464355
Epoch 860, val loss: 1.0651800632476807
Epoch 870, training loss: 626.6992797851562 = 0.4434517025947571 + 100.0 * 6.2625579833984375
Epoch 870, val loss: 1.067501425743103
Epoch 880, training loss: 626.6768188476562 = 0.4338569641113281 + 100.0 * 6.262429714202881
Epoch 880, val loss: 1.0701093673706055
Epoch 890, training loss: 626.5193481445312 = 0.42436906695365906 + 100.0 * 6.260949611663818
Epoch 890, val loss: 1.0726332664489746
Epoch 900, training loss: 626.3281860351562 = 0.4151403307914734 + 100.0 * 6.259130001068115
Epoch 900, val loss: 1.0755339860916138
Epoch 910, training loss: 626.255126953125 = 0.40606874227523804 + 100.0 * 6.258490562438965
Epoch 910, val loss: 1.0784389972686768
Epoch 920, training loss: 626.7152099609375 = 0.3971775472164154 + 100.0 * 6.263180732727051
Epoch 920, val loss: 1.0815767049789429
Epoch 930, training loss: 626.4588623046875 = 0.3883192539215088 + 100.0 * 6.260705471038818
Epoch 930, val loss: 1.084639072418213
Epoch 940, training loss: 626.4773559570312 = 0.3796636760234833 + 100.0 * 6.260977268218994
Epoch 940, val loss: 1.0879608392715454
Epoch 950, training loss: 626.0167236328125 = 0.3711221516132355 + 100.0 * 6.25645637512207
Epoch 950, val loss: 1.0913797616958618
Epoch 960, training loss: 625.810302734375 = 0.3628564774990082 + 100.0 * 6.254474639892578
Epoch 960, val loss: 1.0950275659561157
Epoch 970, training loss: 625.6676635742188 = 0.3547148108482361 + 100.0 * 6.253129482269287
Epoch 970, val loss: 1.0987415313720703
Epoch 980, training loss: 625.6607055664062 = 0.346746563911438 + 100.0 * 6.253139495849609
Epoch 980, val loss: 1.1026709079742432
Epoch 990, training loss: 625.9445190429688 = 0.338870108127594 + 100.0 * 6.256056785583496
Epoch 990, val loss: 1.1065599918365479
Epoch 1000, training loss: 625.8568115234375 = 0.33110877871513367 + 100.0 * 6.2552571296691895
Epoch 1000, val loss: 1.1105529069900513
Epoch 1010, training loss: 625.5206298828125 = 0.32348400354385376 + 100.0 * 6.25197172164917
Epoch 1010, val loss: 1.1147122383117676
Epoch 1020, training loss: 625.3328247070312 = 0.3160295784473419 + 100.0 * 6.2501678466796875
Epoch 1020, val loss: 1.1190905570983887
Epoch 1030, training loss: 625.2330322265625 = 0.3087771236896515 + 100.0 * 6.249242782592773
Epoch 1030, val loss: 1.1236062049865723
Epoch 1040, training loss: 625.9555053710938 = 0.30163058638572693 + 100.0 * 6.2565388679504395
Epoch 1040, val loss: 1.1280810832977295
Epoch 1050, training loss: 625.3709716796875 = 0.29457181692123413 + 100.0 * 6.2507643699646
Epoch 1050, val loss: 1.1327152252197266
Epoch 1060, training loss: 625.131103515625 = 0.28764745593070984 + 100.0 * 6.248434543609619
Epoch 1060, val loss: 1.1374374628067017
Epoch 1070, training loss: 624.984130859375 = 0.28089118003845215 + 100.0 * 6.247032642364502
Epoch 1070, val loss: 1.1422958374023438
Epoch 1080, training loss: 624.910888671875 = 0.2742873728275299 + 100.0 * 6.246366024017334
Epoch 1080, val loss: 1.147287130355835
Epoch 1090, training loss: 625.4957275390625 = 0.2678045928478241 + 100.0 * 6.252278804779053
Epoch 1090, val loss: 1.1522380113601685
Epoch 1100, training loss: 625.1268920898438 = 0.26136279106140137 + 100.0 * 6.248655319213867
Epoch 1100, val loss: 1.1572364568710327
Epoch 1110, training loss: 624.9701538085938 = 0.25503990054130554 + 100.0 * 6.247150897979736
Epoch 1110, val loss: 1.1622952222824097
Epoch 1120, training loss: 624.7227783203125 = 0.24890652298927307 + 100.0 * 6.244739055633545
Epoch 1120, val loss: 1.1676565408706665
Epoch 1130, training loss: 624.56494140625 = 0.24291180074214935 + 100.0 * 6.243220329284668
Epoch 1130, val loss: 1.173098087310791
Epoch 1140, training loss: 624.6046752929688 = 0.23706379532814026 + 100.0 * 6.24367618560791
Epoch 1140, val loss: 1.1785999536514282
Epoch 1150, training loss: 624.626953125 = 0.2313012182712555 + 100.0 * 6.243956565856934
Epoch 1150, val loss: 1.183966040611267
Epoch 1160, training loss: 624.5010986328125 = 0.22561763226985931 + 100.0 * 6.2427544593811035
Epoch 1160, val loss: 1.1894716024398804
Epoch 1170, training loss: 624.546142578125 = 0.2200922667980194 + 100.0 * 6.243260383605957
Epoch 1170, val loss: 1.1949290037155151
Epoch 1180, training loss: 624.392578125 = 0.21468272805213928 + 100.0 * 6.241779327392578
Epoch 1180, val loss: 1.2006380558013916
Epoch 1190, training loss: 624.558349609375 = 0.20942924916744232 + 100.0 * 6.2434892654418945
Epoch 1190, val loss: 1.2063491344451904
Epoch 1200, training loss: 624.2544555664062 = 0.20422419905662537 + 100.0 * 6.24050235748291
Epoch 1200, val loss: 1.2118487358093262
Epoch 1210, training loss: 624.2372436523438 = 0.19918324053287506 + 100.0 * 6.240380764007568
Epoch 1210, val loss: 1.2176527976989746
Epoch 1220, training loss: 624.3353271484375 = 0.19425837695598602 + 100.0 * 6.241410732269287
Epoch 1220, val loss: 1.2235627174377441
Epoch 1230, training loss: 624.0464477539062 = 0.18944501876831055 + 100.0 * 6.238569736480713
Epoch 1230, val loss: 1.2293908596038818
Epoch 1240, training loss: 624.0338745117188 = 0.18475446105003357 + 100.0 * 6.238491058349609
Epoch 1240, val loss: 1.235280990600586
Epoch 1250, training loss: 624.5701904296875 = 0.18022994697093964 + 100.0 * 6.243899822235107
Epoch 1250, val loss: 1.2413064241409302
Epoch 1260, training loss: 624.1337280273438 = 0.17568035423755646 + 100.0 * 6.2395806312561035
Epoch 1260, val loss: 1.2472714185714722
Epoch 1270, training loss: 623.9241333007812 = 0.17132282257080078 + 100.0 * 6.237528324127197
Epoch 1270, val loss: 1.253296971321106
Epoch 1280, training loss: 623.8357543945312 = 0.16709516942501068 + 100.0 * 6.2366862297058105
Epoch 1280, val loss: 1.2595654726028442
Epoch 1290, training loss: 624.1257934570312 = 0.16296538710594177 + 100.0 * 6.239628314971924
Epoch 1290, val loss: 1.2657479047775269
Epoch 1300, training loss: 623.7221069335938 = 0.15893489122390747 + 100.0 * 6.235631465911865
Epoch 1300, val loss: 1.2719173431396484
Epoch 1310, training loss: 623.6581420898438 = 0.1550234854221344 + 100.0 * 6.2350311279296875
Epoch 1310, val loss: 1.2782865762710571
Epoch 1320, training loss: 623.856689453125 = 0.15123239159584045 + 100.0 * 6.237054347991943
Epoch 1320, val loss: 1.2846955060958862
Epoch 1330, training loss: 623.6994018554688 = 0.14749397337436676 + 100.0 * 6.235518932342529
Epoch 1330, val loss: 1.290845274925232
Epoch 1340, training loss: 623.5385131835938 = 0.14387403428554535 + 100.0 * 6.233946323394775
Epoch 1340, val loss: 1.2973506450653076
Epoch 1350, training loss: 623.478759765625 = 0.14036446809768677 + 100.0 * 6.233383655548096
Epoch 1350, val loss: 1.3037137985229492
Epoch 1360, training loss: 623.8277587890625 = 0.1369640976190567 + 100.0 * 6.236907958984375
Epoch 1360, val loss: 1.3102384805679321
Epoch 1370, training loss: 623.5573120117188 = 0.13361488282680511 + 100.0 * 6.234237194061279
Epoch 1370, val loss: 1.3165431022644043
Epoch 1380, training loss: 623.5462036132812 = 0.13034233450889587 + 100.0 * 6.234158515930176
Epoch 1380, val loss: 1.3229305744171143
Epoch 1390, training loss: 623.4493408203125 = 0.12720176577568054 + 100.0 * 6.233221530914307
Epoch 1390, val loss: 1.3295717239379883
Epoch 1400, training loss: 623.2825317382812 = 0.12414195388555527 + 100.0 * 6.231583595275879
Epoch 1400, val loss: 1.3362549543380737
Epoch 1410, training loss: 623.20556640625 = 0.1211976557970047 + 100.0 * 6.230843544006348
Epoch 1410, val loss: 1.3429547548294067
Epoch 1420, training loss: 623.5878295898438 = 0.11832434684038162 + 100.0 * 6.234694957733154
Epoch 1420, val loss: 1.3494062423706055
Epoch 1430, training loss: 623.3027954101562 = 0.11550157517194748 + 100.0 * 6.231873035430908
Epoch 1430, val loss: 1.356048822402954
Epoch 1440, training loss: 623.3262329101562 = 0.11272484064102173 + 100.0 * 6.23213529586792
Epoch 1440, val loss: 1.3624087572097778
Epoch 1450, training loss: 623.1910400390625 = 0.11009286344051361 + 100.0 * 6.230809211730957
Epoch 1450, val loss: 1.3692567348480225
Epoch 1460, training loss: 623.4827880859375 = 0.10751935094594955 + 100.0 * 6.233752250671387
Epoch 1460, val loss: 1.3757303953170776
Epoch 1470, training loss: 623.0693969726562 = 0.10498874634504318 + 100.0 * 6.229644298553467
Epoch 1470, val loss: 1.382449984550476
Epoch 1480, training loss: 622.9949951171875 = 0.10257244855165482 + 100.0 * 6.228923797607422
Epoch 1480, val loss: 1.389070749282837
Epoch 1490, training loss: 622.915283203125 = 0.10021209716796875 + 100.0 * 6.228150367736816
Epoch 1490, val loss: 1.395677924156189
Epoch 1500, training loss: 623.184814453125 = 0.09793902188539505 + 100.0 * 6.230868339538574
Epoch 1500, val loss: 1.4023417234420776
Epoch 1510, training loss: 622.9429321289062 = 0.09569253772497177 + 100.0 * 6.2284722328186035
Epoch 1510, val loss: 1.4088568687438965
Epoch 1520, training loss: 622.8263549804688 = 0.09351073205471039 + 100.0 * 6.227328300476074
Epoch 1520, val loss: 1.4154670238494873
Epoch 1530, training loss: 622.8585815429688 = 0.09141184389591217 + 100.0 * 6.2276716232299805
Epoch 1530, val loss: 1.4221854209899902
Epoch 1540, training loss: 623.0474853515625 = 0.08938246965408325 + 100.0 * 6.229581356048584
Epoch 1540, val loss: 1.428850769996643
Epoch 1550, training loss: 622.796875 = 0.08737336099147797 + 100.0 * 6.227094650268555
Epoch 1550, val loss: 1.4354305267333984
Epoch 1560, training loss: 622.7849731445312 = 0.08544915169477463 + 100.0 * 6.22699499130249
Epoch 1560, val loss: 1.4421920776367188
Epoch 1570, training loss: 623.0408935546875 = 0.0835920199751854 + 100.0 * 6.229572772979736
Epoch 1570, val loss: 1.4487051963806152
Epoch 1580, training loss: 622.7202758789062 = 0.08174219727516174 + 100.0 * 6.226385593414307
Epoch 1580, val loss: 1.4549938440322876
Epoch 1590, training loss: 622.6939086914062 = 0.07996925711631775 + 100.0 * 6.226139545440674
Epoch 1590, val loss: 1.4616731405258179
Epoch 1600, training loss: 622.9013671875 = 0.07824937254190445 + 100.0 * 6.228231430053711
Epoch 1600, val loss: 1.468093752861023
Epoch 1610, training loss: 622.6065673828125 = 0.07656734436750412 + 100.0 * 6.225300312042236
Epoch 1610, val loss: 1.474652647972107
Epoch 1620, training loss: 622.6160888671875 = 0.07492591440677643 + 100.0 * 6.225411415100098
Epoch 1620, val loss: 1.4811155796051025
Epoch 1630, training loss: 622.9937133789062 = 0.07334158569574356 + 100.0 * 6.229203701019287
Epoch 1630, val loss: 1.48770010471344
Epoch 1640, training loss: 622.5408935546875 = 0.07177609205245972 + 100.0 * 6.224691390991211
Epoch 1640, val loss: 1.4940071105957031
Epoch 1650, training loss: 622.389892578125 = 0.07028336822986603 + 100.0 * 6.223195552825928
Epoch 1650, val loss: 1.5004624128341675
Epoch 1660, training loss: 622.4869384765625 = 0.06882850080728531 + 100.0 * 6.224180698394775
Epoch 1660, val loss: 1.5068845748901367
Epoch 1670, training loss: 622.9019165039062 = 0.0674062967300415 + 100.0 * 6.2283453941345215
Epoch 1670, val loss: 1.513089656829834
Epoch 1680, training loss: 622.58203125 = 0.06602849811315536 + 100.0 * 6.225160121917725
Epoch 1680, val loss: 1.5193901062011719
Epoch 1690, training loss: 622.4044189453125 = 0.06465677171945572 + 100.0 * 6.223397731781006
Epoch 1690, val loss: 1.5256788730621338
Epoch 1700, training loss: 622.4198608398438 = 0.06336133927106857 + 100.0 * 6.223565101623535
Epoch 1700, val loss: 1.5319849252700806
Epoch 1710, training loss: 622.3506469726562 = 0.062076639384031296 + 100.0 * 6.222885608673096
Epoch 1710, val loss: 1.538251280784607
Epoch 1720, training loss: 622.3788452148438 = 0.06084263697266579 + 100.0 * 6.223179817199707
Epoch 1720, val loss: 1.54477059841156
Epoch 1730, training loss: 622.2518920898438 = 0.0596158504486084 + 100.0 * 6.221922397613525
Epoch 1730, val loss: 1.550519585609436
Epoch 1740, training loss: 622.1754760742188 = 0.058428745716810226 + 100.0 * 6.221170902252197
Epoch 1740, val loss: 1.5567591190338135
Epoch 1750, training loss: 622.3133544921875 = 0.05727759748697281 + 100.0 * 6.222560882568359
Epoch 1750, val loss: 1.562882661819458
Epoch 1760, training loss: 622.2006225585938 = 0.05616164579987526 + 100.0 * 6.221444606781006
Epoch 1760, val loss: 1.5690582990646362
Epoch 1770, training loss: 622.2928466796875 = 0.055063396692276 + 100.0 * 6.222377777099609
Epoch 1770, val loss: 1.5749051570892334
Epoch 1780, training loss: 622.2218627929688 = 0.0539977140724659 + 100.0 * 6.221678256988525
Epoch 1780, val loss: 1.5811822414398193
Epoch 1790, training loss: 622.0625610351562 = 0.05294811725616455 + 100.0 * 6.220095634460449
Epoch 1790, val loss: 1.5872113704681396
Epoch 1800, training loss: 622.0933837890625 = 0.05194776505231857 + 100.0 * 6.220414638519287
Epoch 1800, val loss: 1.5932420492172241
Epoch 1810, training loss: 622.3651733398438 = 0.05098184943199158 + 100.0 * 6.223142147064209
Epoch 1810, val loss: 1.5992529392242432
Epoch 1820, training loss: 622.082275390625 = 0.049998145550489426 + 100.0 * 6.220323085784912
Epoch 1820, val loss: 1.6051746606826782
Epoch 1830, training loss: 621.9689331054688 = 0.049069952219724655 + 100.0 * 6.219198703765869
Epoch 1830, val loss: 1.611183524131775
Epoch 1840, training loss: 621.9915161132812 = 0.04816481098532677 + 100.0 * 6.219433784484863
Epoch 1840, val loss: 1.6169217824935913
Epoch 1850, training loss: 622.2282104492188 = 0.04729216545820236 + 100.0 * 6.221809387207031
Epoch 1850, val loss: 1.6228054761886597
Epoch 1860, training loss: 622.1119384765625 = 0.04639872908592224 + 100.0 * 6.22065544128418
Epoch 1860, val loss: 1.6285039186477661
Epoch 1870, training loss: 621.8452758789062 = 0.04552958160638809 + 100.0 * 6.2179975509643555
Epoch 1870, val loss: 1.6342594623565674
Epoch 1880, training loss: 621.7937622070312 = 0.04469912871718407 + 100.0 * 6.2174906730651855
Epoch 1880, val loss: 1.640173316001892
Epoch 1890, training loss: 621.7815551757812 = 0.04390500858426094 + 100.0 * 6.217376708984375
Epoch 1890, val loss: 1.645999789237976
Epoch 1900, training loss: 622.1537475585938 = 0.04313439503312111 + 100.0 * 6.221106052398682
Epoch 1900, val loss: 1.6516389846801758
Epoch 1910, training loss: 621.84033203125 = 0.042352791875600815 + 100.0 * 6.217979431152344
Epoch 1910, val loss: 1.6573058366775513
Epoch 1920, training loss: 621.8150634765625 = 0.04160446673631668 + 100.0 * 6.217734336853027
Epoch 1920, val loss: 1.662856101989746
Epoch 1930, training loss: 621.7603759765625 = 0.04085921868681908 + 100.0 * 6.217195510864258
Epoch 1930, val loss: 1.6685606241226196
Epoch 1940, training loss: 621.713623046875 = 0.04015188291668892 + 100.0 * 6.216734886169434
Epoch 1940, val loss: 1.6741247177124023
Epoch 1950, training loss: 621.7902221679688 = 0.03945912793278694 + 100.0 * 6.217507839202881
Epoch 1950, val loss: 1.6797010898590088
Epoch 1960, training loss: 621.9801025390625 = 0.03877995163202286 + 100.0 * 6.2194132804870605
Epoch 1960, val loss: 1.6849536895751953
Epoch 1970, training loss: 621.8120727539062 = 0.03810479864478111 + 100.0 * 6.217740058898926
Epoch 1970, val loss: 1.6907577514648438
Epoch 1980, training loss: 621.73486328125 = 0.03744177892804146 + 100.0 * 6.216974258422852
Epoch 1980, val loss: 1.6959794759750366
Epoch 1990, training loss: 621.6104125976562 = 0.03679794818162918 + 100.0 * 6.215736389160156
Epoch 1990, val loss: 1.7014179229736328
Epoch 2000, training loss: 621.734619140625 = 0.03618033975362778 + 100.0 * 6.216984272003174
Epoch 2000, val loss: 1.7069298028945923
Epoch 2010, training loss: 621.6163330078125 = 0.03557392582297325 + 100.0 * 6.2158074378967285
Epoch 2010, val loss: 1.712356448173523
Epoch 2020, training loss: 621.5557861328125 = 0.03498470410704613 + 100.0 * 6.215208053588867
Epoch 2020, val loss: 1.7175952196121216
Epoch 2030, training loss: 621.9156494140625 = 0.034406859427690506 + 100.0 * 6.218812465667725
Epoch 2030, val loss: 1.7229132652282715
Epoch 2040, training loss: 621.57080078125 = 0.03381941094994545 + 100.0 * 6.215370178222656
Epoch 2040, val loss: 1.7278865575790405
Epoch 2050, training loss: 621.4475708007812 = 0.03326607868075371 + 100.0 * 6.2141432762146
Epoch 2050, val loss: 1.7331122159957886
Epoch 2060, training loss: 621.412109375 = 0.03272753953933716 + 100.0 * 6.213794231414795
Epoch 2060, val loss: 1.738539218902588
Epoch 2070, training loss: 621.6098022460938 = 0.03220571577548981 + 100.0 * 6.215775489807129
Epoch 2070, val loss: 1.7435967922210693
Epoch 2080, training loss: 621.45751953125 = 0.03168287128210068 + 100.0 * 6.214258670806885
Epoch 2080, val loss: 1.748666524887085
Epoch 2090, training loss: 621.4647827148438 = 0.031159505248069763 + 100.0 * 6.214335918426514
Epoch 2090, val loss: 1.7535269260406494
Epoch 2100, training loss: 621.6488037109375 = 0.030663996934890747 + 100.0 * 6.216181755065918
Epoch 2100, val loss: 1.758589506149292
Epoch 2110, training loss: 621.42236328125 = 0.03017467074096203 + 100.0 * 6.213922023773193
Epoch 2110, val loss: 1.7636353969573975
Epoch 2120, training loss: 621.2774047851562 = 0.02969713881611824 + 100.0 * 6.21247673034668
Epoch 2120, val loss: 1.7689435482025146
Epoch 2130, training loss: 621.3853149414062 = 0.029237665235996246 + 100.0 * 6.213561058044434
Epoch 2130, val loss: 1.773930311203003
Epoch 2140, training loss: 621.7944946289062 = 0.028784384950995445 + 100.0 * 6.21765661239624
Epoch 2140, val loss: 1.7787957191467285
Epoch 2150, training loss: 621.464599609375 = 0.028336966410279274 + 100.0 * 6.214363098144531
Epoch 2150, val loss: 1.7836103439331055
Epoch 2160, training loss: 621.2612915039062 = 0.02789110504090786 + 100.0 * 6.212333679199219
Epoch 2160, val loss: 1.7884751558303833
Epoch 2170, training loss: 621.2655029296875 = 0.02747204340994358 + 100.0 * 6.212380409240723
Epoch 2170, val loss: 1.7935326099395752
Epoch 2180, training loss: 621.30078125 = 0.027056952938437462 + 100.0 * 6.212737560272217
Epoch 2180, val loss: 1.798183560371399
Epoch 2190, training loss: 621.1896362304688 = 0.026648985221982002 + 100.0 * 6.211629867553711
Epoch 2190, val loss: 1.8029314279556274
Epoch 2200, training loss: 621.2859497070312 = 0.02625640667974949 + 100.0 * 6.212596893310547
Epoch 2200, val loss: 1.8077670335769653
Epoch 2210, training loss: 621.5367431640625 = 0.0258689746260643 + 100.0 * 6.215108394622803
Epoch 2210, val loss: 1.8124226331710815
Epoch 2220, training loss: 621.2656860351562 = 0.025470413267612457 + 100.0 * 6.21240234375
Epoch 2220, val loss: 1.8171334266662598
Epoch 2230, training loss: 621.3030395507812 = 0.025093916803598404 + 100.0 * 6.212779521942139
Epoch 2230, val loss: 1.8214281797409058
Epoch 2240, training loss: 621.3323974609375 = 0.02472352422773838 + 100.0 * 6.213077068328857
Epoch 2240, val loss: 1.8263922929763794
Epoch 2250, training loss: 621.2945556640625 = 0.024361908435821533 + 100.0 * 6.21270227432251
Epoch 2250, val loss: 1.830973744392395
Epoch 2260, training loss: 621.102783203125 = 0.02400258556008339 + 100.0 * 6.210788249969482
Epoch 2260, val loss: 1.8351808786392212
Epoch 2270, training loss: 621.0015869140625 = 0.023656900972127914 + 100.0 * 6.209779262542725
Epoch 2270, val loss: 1.840011715888977
Epoch 2280, training loss: 620.9617309570312 = 0.023327816277742386 + 100.0 * 6.209383964538574
Epoch 2280, val loss: 1.8447359800338745
Epoch 2290, training loss: 621.0599365234375 = 0.023006638512015343 + 100.0 * 6.210369110107422
Epoch 2290, val loss: 1.8491239547729492
Epoch 2300, training loss: 621.5679321289062 = 0.022685129195451736 + 100.0 * 6.215452194213867
Epoch 2300, val loss: 1.8532410860061646
Epoch 2310, training loss: 621.3108520507812 = 0.022348478436470032 + 100.0 * 6.212884902954102
Epoch 2310, val loss: 1.857545018196106
Epoch 2320, training loss: 620.993408203125 = 0.02203528583049774 + 100.0 * 6.209713935852051
Epoch 2320, val loss: 1.8618472814559937
Epoch 2330, training loss: 620.9188842773438 = 0.021728243678808212 + 100.0 * 6.2089715003967285
Epoch 2330, val loss: 1.8665498495101929
Epoch 2340, training loss: 621.3248291015625 = 0.02143591269850731 + 100.0 * 6.213034152984619
Epoch 2340, val loss: 1.8708503246307373
Epoch 2350, training loss: 620.90673828125 = 0.02113528735935688 + 100.0 * 6.208856105804443
Epoch 2350, val loss: 1.875124454498291
Epoch 2360, training loss: 620.8729858398438 = 0.020847849547863007 + 100.0 * 6.208521366119385
Epoch 2360, val loss: 1.8794010877609253
Epoch 2370, training loss: 621.2836303710938 = 0.02057330682873726 + 100.0 * 6.212630748748779
Epoch 2370, val loss: 1.8833041191101074
Epoch 2380, training loss: 620.88720703125 = 0.02028469741344452 + 100.0 * 6.208669185638428
Epoch 2380, val loss: 1.8878545761108398
Epoch 2390, training loss: 620.8946533203125 = 0.020016148686408997 + 100.0 * 6.208746433258057
Epoch 2390, val loss: 1.8919827938079834
Epoch 2400, training loss: 620.825439453125 = 0.019748328253626823 + 100.0 * 6.208056926727295
Epoch 2400, val loss: 1.8961421251296997
Epoch 2410, training loss: 620.9130249023438 = 0.01949135772883892 + 100.0 * 6.208935260772705
Epoch 2410, val loss: 1.9005861282348633
Epoch 2420, training loss: 621.0625 = 0.019237563014030457 + 100.0 * 6.210433006286621
Epoch 2420, val loss: 1.904478669166565
Epoch 2430, training loss: 620.8444213867188 = 0.018976988270878792 + 100.0 * 6.208254337310791
Epoch 2430, val loss: 1.908105492591858
Epoch 2440, training loss: 620.7532958984375 = 0.018732169643044472 + 100.0 * 6.207345485687256
Epoch 2440, val loss: 1.912688136100769
Epoch 2450, training loss: 620.850830078125 = 0.018494676798582077 + 100.0 * 6.2083234786987305
Epoch 2450, val loss: 1.916310429573059
Epoch 2460, training loss: 621.2185668945312 = 0.018255427479743958 + 100.0 * 6.212003231048584
Epoch 2460, val loss: 1.9206351041793823
Epoch 2470, training loss: 620.8821411132812 = 0.018015453591942787 + 100.0 * 6.208641052246094
Epoch 2470, val loss: 1.9239118099212646
Epoch 2480, training loss: 620.7575073242188 = 0.017784973606467247 + 100.0 * 6.2073974609375
Epoch 2480, val loss: 1.928217887878418
Epoch 2490, training loss: 620.6024169921875 = 0.01755836047232151 + 100.0 * 6.205848217010498
Epoch 2490, val loss: 1.9320036172866821
Epoch 2500, training loss: 620.5661010742188 = 0.017344918102025986 + 100.0 * 6.2054877281188965
Epoch 2500, val loss: 1.9362984895706177
Epoch 2510, training loss: 620.5834350585938 = 0.01713494583964348 + 100.0 * 6.205662727355957
Epoch 2510, val loss: 1.940083622932434
Epoch 2520, training loss: 621.5164794921875 = 0.016939174383878708 + 100.0 * 6.21499490737915
Epoch 2520, val loss: 1.9437419176101685
Epoch 2530, training loss: 621.1514282226562 = 0.016711248084902763 + 100.0 * 6.2113471031188965
Epoch 2530, val loss: 1.9472100734710693
Epoch 2540, training loss: 620.8273315429688 = 0.016500240191817284 + 100.0 * 6.208108425140381
Epoch 2540, val loss: 1.951157808303833
Epoch 2550, training loss: 620.5955200195312 = 0.016293898224830627 + 100.0 * 6.20579195022583
Epoch 2550, val loss: 1.9549225568771362
Epoch 2560, training loss: 620.4966430664062 = 0.01609877310693264 + 100.0 * 6.204805374145508
Epoch 2560, val loss: 1.958893895149231
Epoch 2570, training loss: 620.5469360351562 = 0.01591203548014164 + 100.0 * 6.205310344696045
Epoch 2570, val loss: 1.9628078937530518
Epoch 2580, training loss: 621.0325927734375 = 0.015729760751128197 + 100.0 * 6.210168361663818
Epoch 2580, val loss: 1.9663293361663818
Epoch 2590, training loss: 620.6739501953125 = 0.015531379729509354 + 100.0 * 6.206584453582764
Epoch 2590, val loss: 1.9697152376174927
Epoch 2600, training loss: 620.466552734375 = 0.015345083549618721 + 100.0 * 6.204512596130371
Epoch 2600, val loss: 1.9734106063842773
Epoch 2610, training loss: 620.4038696289062 = 0.0151675408706069 + 100.0 * 6.20388650894165
Epoch 2610, val loss: 1.9772764444351196
Epoch 2620, training loss: 620.4557495117188 = 0.014991146512329578 + 100.0 * 6.204407215118408
Epoch 2620, val loss: 1.9808636903762817
Epoch 2630, training loss: 620.5361938476562 = 0.014823747798800468 + 100.0 * 6.20521354675293
Epoch 2630, val loss: 1.9845656156539917
Epoch 2640, training loss: 620.76171875 = 0.014655251987278461 + 100.0 * 6.207470893859863
Epoch 2640, val loss: 1.9877710342407227
Epoch 2650, training loss: 620.5093383789062 = 0.014475122094154358 + 100.0 * 6.204948902130127
Epoch 2650, val loss: 1.991363525390625
Epoch 2660, training loss: 620.680908203125 = 0.014310404658317566 + 100.0 * 6.206665992736816
Epoch 2660, val loss: 1.995092749595642
Epoch 2670, training loss: 620.4642944335938 = 0.014141466468572617 + 100.0 * 6.204501152038574
Epoch 2670, val loss: 1.9983316659927368
Epoch 2680, training loss: 620.6363525390625 = 0.013981897383928299 + 100.0 * 6.206223964691162
Epoch 2680, val loss: 2.0020434856414795
Epoch 2690, training loss: 620.4067993164062 = 0.013819395564496517 + 100.0 * 6.203929901123047
Epoch 2690, val loss: 2.005363941192627
Epoch 2700, training loss: 620.2662963867188 = 0.013666458427906036 + 100.0 * 6.202526092529297
Epoch 2700, val loss: 2.0089123249053955
Epoch 2710, training loss: 620.2581787109375 = 0.013518566265702248 + 100.0 * 6.202446937561035
Epoch 2710, val loss: 2.0126073360443115
Epoch 2720, training loss: 620.33154296875 = 0.013374263420701027 + 100.0 * 6.203181743621826
Epoch 2720, val loss: 2.015934467315674
Epoch 2730, training loss: 620.6105346679688 = 0.013230228796601295 + 100.0 * 6.205973148345947
Epoch 2730, val loss: 2.019188404083252
Epoch 2740, training loss: 620.3196411132812 = 0.013083185069262981 + 100.0 * 6.203065872192383
Epoch 2740, val loss: 2.0225231647491455
Epoch 2750, training loss: 620.7228393554688 = 0.01294143870472908 + 100.0 * 6.207098960876465
Epoch 2750, val loss: 2.0255329608917236
Epoch 2760, training loss: 620.381591796875 = 0.01279748510569334 + 100.0 * 6.20368766784668
Epoch 2760, val loss: 2.028876304626465
Epoch 2770, training loss: 620.2171020507812 = 0.012653278186917305 + 100.0 * 6.202044486999512
Epoch 2770, val loss: 2.0319461822509766
Epoch 2780, training loss: 620.1629028320312 = 0.012521893717348576 + 100.0 * 6.201503753662109
Epoch 2780, val loss: 2.0354721546173096
Epoch 2790, training loss: 620.164794921875 = 0.012393361888825893 + 100.0 * 6.201524257659912
Epoch 2790, val loss: 2.0387380123138428
Epoch 2800, training loss: 620.760986328125 = 0.012273876927793026 + 100.0 * 6.207487106323242
Epoch 2800, val loss: 2.0417163372039795
Epoch 2810, training loss: 620.2100830078125 = 0.012131542898714542 + 100.0 * 6.201979637145996
Epoch 2810, val loss: 2.045060873031616
Epoch 2820, training loss: 620.1538696289062 = 0.012006363831460476 + 100.0 * 6.201418399810791
Epoch 2820, val loss: 2.048145055770874
Epoch 2830, training loss: 620.5352172851562 = 0.011883372440934181 + 100.0 * 6.205233097076416
Epoch 2830, val loss: 2.0513153076171875
Epoch 2840, training loss: 620.3980102539062 = 0.011762109585106373 + 100.0 * 6.203862190246582
Epoch 2840, val loss: 2.054314136505127
Epoch 2850, training loss: 620.1105346679688 = 0.011635771952569485 + 100.0 * 6.20098876953125
Epoch 2850, val loss: 2.0575015544891357
Epoch 2860, training loss: 620.06884765625 = 0.01151956059038639 + 100.0 * 6.200572967529297
Epoch 2860, val loss: 2.0605664253234863
Epoch 2870, training loss: 620.0625 = 0.011406379751861095 + 100.0 * 6.2005109786987305
Epoch 2870, val loss: 2.0634989738464355
Epoch 2880, training loss: 620.3474731445312 = 0.011297518387436867 + 100.0 * 6.203361511230469
Epoch 2880, val loss: 2.0662569999694824
Epoch 2890, training loss: 620.1358642578125 = 0.011180110275745392 + 100.0 * 6.201246738433838
Epoch 2890, val loss: 2.069622039794922
Epoch 2900, training loss: 620.3431396484375 = 0.011073034256696701 + 100.0 * 6.2033209800720215
Epoch 2900, val loss: 2.072423219680786
Epoch 2910, training loss: 620.1527099609375 = 0.010958247818052769 + 100.0 * 6.201416969299316
Epoch 2910, val loss: 2.0751824378967285
Epoch 2920, training loss: 620.2822875976562 = 0.010848539881408215 + 100.0 * 6.202713966369629
Epoch 2920, val loss: 2.078263282775879
Epoch 2930, training loss: 619.9782104492188 = 0.010741150006651878 + 100.0 * 6.199674606323242
Epoch 2930, val loss: 2.0808932781219482
Epoch 2940, training loss: 619.9834594726562 = 0.010639323852956295 + 100.0 * 6.199728488922119
Epoch 2940, val loss: 2.0840036869049072
Epoch 2950, training loss: 621.1381225585938 = 0.01054307259619236 + 100.0 * 6.211275577545166
Epoch 2950, val loss: 2.086538791656494
Epoch 2960, training loss: 620.2815551757812 = 0.010435874573886395 + 100.0 * 6.20271110534668
Epoch 2960, val loss: 2.0894775390625
Epoch 2970, training loss: 619.9817504882812 = 0.010328606702387333 + 100.0 * 6.199714660644531
Epoch 2970, val loss: 2.0922837257385254
Epoch 2980, training loss: 619.8705444335938 = 0.010233717039227486 + 100.0 * 6.198602676391602
Epoch 2980, val loss: 2.0952227115631104
Epoch 2990, training loss: 619.8389892578125 = 0.010140826925635338 + 100.0 * 6.198288440704346
Epoch 2990, val loss: 2.098186492919922
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6555555555555556
0.8107538218239325
The final CL Acc:0.66420, 0.02642, The final GNN Acc:0.81075, 0.00043
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13222])
remove edge: torch.Size([2, 7866])
updated graph: torch.Size([2, 10532])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6419677734375 = 1.9575777053833008 + 100.0 * 8.596843719482422
Epoch 0, val loss: 1.9628788232803345
Epoch 10, training loss: 861.5455322265625 = 1.948760986328125 + 100.0 * 8.595967292785645
Epoch 10, val loss: 1.9545111656188965
Epoch 20, training loss: 860.8718872070312 = 1.9377423524856567 + 100.0 * 8.589341163635254
Epoch 20, val loss: 1.9438368082046509
Epoch 30, training loss: 856.6170654296875 = 1.9238371849060059 + 100.0 * 8.546932220458984
Epoch 30, val loss: 1.9302774667739868
Epoch 40, training loss: 835.6800537109375 = 1.9077744483947754 + 100.0 * 8.337722778320312
Epoch 40, val loss: 1.9153774976730347
Epoch 50, training loss: 781.1272583007812 = 1.890002965927124 + 100.0 * 7.792372703552246
Epoch 50, val loss: 1.8985199928283691
Epoch 60, training loss: 744.4445190429688 = 1.8734632730484009 + 100.0 * 7.425710201263428
Epoch 60, val loss: 1.8823117017745972
Epoch 70, training loss: 713.4993286132812 = 1.858620047569275 + 100.0 * 7.1164069175720215
Epoch 70, val loss: 1.8668642044067383
Epoch 80, training loss: 697.6959838867188 = 1.843010663986206 + 100.0 * 6.958529949188232
Epoch 80, val loss: 1.8512780666351318
Epoch 90, training loss: 687.8872680664062 = 1.8298263549804688 + 100.0 * 6.860574245452881
Epoch 90, val loss: 1.8383276462554932
Epoch 100, training loss: 680.7553100585938 = 1.8180482387542725 + 100.0 * 6.789372444152832
Epoch 100, val loss: 1.8269999027252197
Epoch 110, training loss: 674.6226196289062 = 1.8074499368667603 + 100.0 * 6.728151798248291
Epoch 110, val loss: 1.8163702487945557
Epoch 120, training loss: 669.3624267578125 = 1.7973580360412598 + 100.0 * 6.675650596618652
Epoch 120, val loss: 1.8059771060943604
Epoch 130, training loss: 664.2264404296875 = 1.7876060009002686 + 100.0 * 6.624388217926025
Epoch 130, val loss: 1.7959846258163452
Epoch 140, training loss: 659.88134765625 = 1.7777013778686523 + 100.0 * 6.581036567687988
Epoch 140, val loss: 1.786105990409851
Epoch 150, training loss: 656.7747192382812 = 1.7671877145767212 + 100.0 * 6.550075054168701
Epoch 150, val loss: 1.7757225036621094
Epoch 160, training loss: 654.3197631835938 = 1.7559155225753784 + 100.0 * 6.525638580322266
Epoch 160, val loss: 1.7647442817687988
Epoch 170, training loss: 652.0894775390625 = 1.7437199354171753 + 100.0 * 6.503457546234131
Epoch 170, val loss: 1.7530945539474487
Epoch 180, training loss: 650.0852661132812 = 1.7305619716644287 + 100.0 * 6.483546733856201
Epoch 180, val loss: 1.74063241481781
Epoch 190, training loss: 649.0988159179688 = 1.7164033651351929 + 100.0 * 6.4738240242004395
Epoch 190, val loss: 1.727149248123169
Epoch 200, training loss: 646.9720458984375 = 1.7009228467941284 + 100.0 * 6.45271110534668
Epoch 200, val loss: 1.712672233581543
Epoch 210, training loss: 645.8422241210938 = 1.6844823360443115 + 100.0 * 6.441576957702637
Epoch 210, val loss: 1.697371244430542
Epoch 220, training loss: 644.7437133789062 = 1.6667732000350952 + 100.0 * 6.430769920349121
Epoch 220, val loss: 1.6809513568878174
Epoch 230, training loss: 643.8502197265625 = 1.6477434635162354 + 100.0 * 6.422024726867676
Epoch 230, val loss: 1.6633906364440918
Epoch 240, training loss: 643.05908203125 = 1.6274254322052002 + 100.0 * 6.414316177368164
Epoch 240, val loss: 1.6448861360549927
Epoch 250, training loss: 642.3455810546875 = 1.60585618019104 + 100.0 * 6.407397270202637
Epoch 250, val loss: 1.6252803802490234
Epoch 260, training loss: 641.6257934570312 = 1.5833113193511963 + 100.0 * 6.400424957275391
Epoch 260, val loss: 1.6050846576690674
Epoch 270, training loss: 640.941650390625 = 1.5600954294204712 + 100.0 * 6.393815517425537
Epoch 270, val loss: 1.5844979286193848
Epoch 280, training loss: 640.1913452148438 = 1.5363292694091797 + 100.0 * 6.386550426483154
Epoch 280, val loss: 1.5637022256851196
Epoch 290, training loss: 639.6226806640625 = 1.512173056602478 + 100.0 * 6.381105422973633
Epoch 290, val loss: 1.5428521633148193
Epoch 300, training loss: 638.9095458984375 = 1.487749457359314 + 100.0 * 6.374217987060547
Epoch 300, val loss: 1.522060751914978
Epoch 310, training loss: 638.26953125 = 1.4633443355560303 + 100.0 * 6.3680620193481445
Epoch 310, val loss: 1.5016614198684692
Epoch 320, training loss: 637.8847045898438 = 1.4389568567276 + 100.0 * 6.364457607269287
Epoch 320, val loss: 1.481654167175293
Epoch 330, training loss: 637.2173461914062 = 1.4147223234176636 + 100.0 * 6.358026027679443
Epoch 330, val loss: 1.4622666835784912
Epoch 340, training loss: 636.7376098632812 = 1.3909275531768799 + 100.0 * 6.353466987609863
Epoch 340, val loss: 1.4434558153152466
Epoch 350, training loss: 636.23876953125 = 1.3675838708877563 + 100.0 * 6.348711967468262
Epoch 350, val loss: 1.4253745079040527
Epoch 360, training loss: 635.762939453125 = 1.3446531295776367 + 100.0 * 6.34418249130249
Epoch 360, val loss: 1.4078196287155151
Epoch 370, training loss: 635.7182006835938 = 1.3220194578170776 + 100.0 * 6.343961715698242
Epoch 370, val loss: 1.3906586170196533
Epoch 380, training loss: 635.0430297851562 = 1.2996063232421875 + 100.0 * 6.337433815002441
Epoch 380, val loss: 1.3738977909088135
Epoch 390, training loss: 634.6357421875 = 1.2775135040283203 + 100.0 * 6.333582401275635
Epoch 390, val loss: 1.3575197458267212
Epoch 400, training loss: 634.43798828125 = 1.2556535005569458 + 100.0 * 6.331823348999023
Epoch 400, val loss: 1.341598391532898
Epoch 410, training loss: 634.1705932617188 = 1.2339309453964233 + 100.0 * 6.329366683959961
Epoch 410, val loss: 1.3256492614746094
Epoch 420, training loss: 633.5576782226562 = 1.2122894525527954 + 100.0 * 6.323453903198242
Epoch 420, val loss: 1.3100892305374146
Epoch 430, training loss: 633.1727294921875 = 1.1908485889434814 + 100.0 * 6.319818496704102
Epoch 430, val loss: 1.2947524785995483
Epoch 440, training loss: 632.8206176757812 = 1.169538974761963 + 100.0 * 6.3165106773376465
Epoch 440, val loss: 1.279617190361023
Epoch 450, training loss: 633.1023559570312 = 1.148362636566162 + 100.0 * 6.319540023803711
Epoch 450, val loss: 1.26475989818573
Epoch 460, training loss: 632.4255981445312 = 1.1268913745880127 + 100.0 * 6.312987327575684
Epoch 460, val loss: 1.249795913696289
Epoch 470, training loss: 631.9155883789062 = 1.1058121919631958 + 100.0 * 6.3080973625183105
Epoch 470, val loss: 1.235128402709961
Epoch 480, training loss: 631.8843994140625 = 1.0848618745803833 + 100.0 * 6.307995796203613
Epoch 480, val loss: 1.2207762002944946
Epoch 490, training loss: 631.311279296875 = 1.0638357400894165 + 100.0 * 6.302474498748779
Epoch 490, val loss: 1.2067625522613525
Epoch 500, training loss: 631.2941284179688 = 1.0430353879928589 + 100.0 * 6.302510738372803
Epoch 500, val loss: 1.1930530071258545
Epoch 510, training loss: 630.735107421875 = 1.022492527961731 + 100.0 * 6.297125816345215
Epoch 510, val loss: 1.1795437335968018
Epoch 520, training loss: 631.3002319335938 = 1.002210021018982 + 100.0 * 6.302980422973633
Epoch 520, val loss: 1.1663371324539185
Epoch 530, training loss: 630.5302734375 = 0.9817910194396973 + 100.0 * 6.29548454284668
Epoch 530, val loss: 1.1535751819610596
Epoch 540, training loss: 630.0848999023438 = 0.9617856740951538 + 100.0 * 6.291231155395508
Epoch 540, val loss: 1.1410648822784424
Epoch 550, training loss: 629.9126586914062 = 0.9422067403793335 + 100.0 * 6.2897047996521
Epoch 550, val loss: 1.1291409730911255
Epoch 560, training loss: 629.7044677734375 = 0.922835111618042 + 100.0 * 6.287816047668457
Epoch 560, val loss: 1.1176220178604126
Epoch 570, training loss: 629.3829345703125 = 0.9037561416625977 + 100.0 * 6.284791946411133
Epoch 570, val loss: 1.1065198183059692
Epoch 580, training loss: 629.263916015625 = 0.8850785493850708 + 100.0 * 6.283788681030273
Epoch 580, val loss: 1.0959656238555908
Epoch 590, training loss: 630.05029296875 = 0.8668113946914673 + 100.0 * 6.291834831237793
Epoch 590, val loss: 1.0857194662094116
Epoch 600, training loss: 628.9359741210938 = 0.8484134078025818 + 100.0 * 6.2808756828308105
Epoch 600, val loss: 1.075989007949829
Epoch 610, training loss: 628.6004638671875 = 0.8308480978012085 + 100.0 * 6.277695655822754
Epoch 610, val loss: 1.066845417022705
Epoch 620, training loss: 628.4100341796875 = 0.8136491179466248 + 100.0 * 6.27596378326416
Epoch 620, val loss: 1.0582499504089355
Epoch 630, training loss: 628.2029418945312 = 0.7968460321426392 + 100.0 * 6.2740607261657715
Epoch 630, val loss: 1.0500634908676147
Epoch 640, training loss: 628.2307739257812 = 0.7803626656532288 + 100.0 * 6.274504661560059
Epoch 640, val loss: 1.0422298908233643
Epoch 650, training loss: 628.485107421875 = 0.7640848755836487 + 100.0 * 6.277210235595703
Epoch 650, val loss: 1.0347644090652466
Epoch 660, training loss: 627.8377685546875 = 0.7482240796089172 + 100.0 * 6.270895481109619
Epoch 660, val loss: 1.0275144577026367
Epoch 670, training loss: 627.6076049804688 = 0.7327159643173218 + 100.0 * 6.268748760223389
Epoch 670, val loss: 1.0208334922790527
Epoch 680, training loss: 627.4661254882812 = 0.7176053524017334 + 100.0 * 6.267485618591309
Epoch 680, val loss: 1.0143674612045288
Epoch 690, training loss: 628.2141723632812 = 0.702706515789032 + 100.0 * 6.275115013122559
Epoch 690, val loss: 1.008180022239685
Epoch 700, training loss: 627.52001953125 = 0.688218355178833 + 100.0 * 6.268318176269531
Epoch 700, val loss: 1.0021449327468872
Epoch 710, training loss: 627.1363525390625 = 0.6738360524177551 + 100.0 * 6.264625072479248
Epoch 710, val loss: 0.9965513944625854
Epoch 720, training loss: 626.952880859375 = 0.6599109172821045 + 100.0 * 6.262929439544678
Epoch 720, val loss: 0.9912476539611816
Epoch 730, training loss: 626.8896484375 = 0.6461348533630371 + 100.0 * 6.262435436248779
Epoch 730, val loss: 0.9862170219421387
Epoch 740, training loss: 626.89501953125 = 0.6325957775115967 + 100.0 * 6.262624740600586
Epoch 740, val loss: 0.9814518690109253
Epoch 750, training loss: 626.6585693359375 = 0.6192374229431152 + 100.0 * 6.260393142700195
Epoch 750, val loss: 0.9765592217445374
Epoch 760, training loss: 626.5039672851562 = 0.6062121391296387 + 100.0 * 6.25897741317749
Epoch 760, val loss: 0.9722564816474915
Epoch 770, training loss: 627.1934204101562 = 0.5933330655097961 + 100.0 * 6.266000747680664
Epoch 770, val loss: 0.9680883288383484
Epoch 780, training loss: 626.5287475585938 = 0.5806146860122681 + 100.0 * 6.259481430053711
Epoch 780, val loss: 0.964035153388977
Epoch 790, training loss: 626.1389770507812 = 0.5681613683700562 + 100.0 * 6.255707740783691
Epoch 790, val loss: 0.9603282809257507
Epoch 800, training loss: 626.0396118164062 = 0.555955171585083 + 100.0 * 6.254836559295654
Epoch 800, val loss: 0.9568593502044678
Epoch 810, training loss: 626.099609375 = 0.5440002679824829 + 100.0 * 6.255556106567383
Epoch 810, val loss: 0.953633725643158
Epoch 820, training loss: 625.8806762695312 = 0.5320571660995483 + 100.0 * 6.253486156463623
Epoch 820, val loss: 0.9504334330558777
Epoch 830, training loss: 625.7915649414062 = 0.5203945636749268 + 100.0 * 6.252711772918701
Epoch 830, val loss: 0.9473419785499573
Epoch 840, training loss: 625.6887817382812 = 0.5089893341064453 + 100.0 * 6.251798152923584
Epoch 840, val loss: 0.944674015045166
Epoch 850, training loss: 625.5899047851562 = 0.4978519678115845 + 100.0 * 6.250920295715332
Epoch 850, val loss: 0.9421604871749878
Epoch 860, training loss: 626.1555786132812 = 0.48686638474464417 + 100.0 * 6.256687164306641
Epoch 860, val loss: 0.9397522807121277
Epoch 870, training loss: 625.5872192382812 = 0.4760642945766449 + 100.0 * 6.2511115074157715
Epoch 870, val loss: 0.9374843239784241
Epoch 880, training loss: 625.3621215820312 = 0.46557551622390747 + 100.0 * 6.248965740203857
Epoch 880, val loss: 0.9356801509857178
Epoch 890, training loss: 625.2642822265625 = 0.45534083247184753 + 100.0 * 6.248089790344238
Epoch 890, val loss: 0.9340258836746216
Epoch 900, training loss: 625.861083984375 = 0.4453712999820709 + 100.0 * 6.254157066345215
Epoch 900, val loss: 0.9324880242347717
Epoch 910, training loss: 625.2210083007812 = 0.43537667393684387 + 100.0 * 6.247856140136719
Epoch 910, val loss: 0.9308894276618958
Epoch 920, training loss: 625.0089721679688 = 0.42581906914711 + 100.0 * 6.24583101272583
Epoch 920, val loss: 0.9298684597015381
Epoch 930, training loss: 624.883056640625 = 0.41647836565971375 + 100.0 * 6.244665622711182
Epoch 930, val loss: 0.9288551211357117
Epoch 940, training loss: 624.8806762695312 = 0.4073602557182312 + 100.0 * 6.2447333335876465
Epoch 940, val loss: 0.9280194044113159
Epoch 950, training loss: 625.3887939453125 = 0.3984726071357727 + 100.0 * 6.249903202056885
Epoch 950, val loss: 0.9273868203163147
Epoch 960, training loss: 624.7986450195312 = 0.389674574136734 + 100.0 * 6.2440900802612305
Epoch 960, val loss: 0.9268067479133606
Epoch 970, training loss: 624.7500610351562 = 0.381146639585495 + 100.0 * 6.243689060211182
Epoch 970, val loss: 0.9266308546066284
Epoch 980, training loss: 624.5567016601562 = 0.37287014722824097 + 100.0 * 6.241838455200195
Epoch 980, val loss: 0.9265209436416626
Epoch 990, training loss: 624.721923828125 = 0.36480188369750977 + 100.0 * 6.2435712814331055
Epoch 990, val loss: 0.9266151785850525
Epoch 1000, training loss: 624.5731811523438 = 0.35692334175109863 + 100.0 * 6.242162704467773
Epoch 1000, val loss: 0.9270731806755066
Epoch 1010, training loss: 624.540283203125 = 0.3491944372653961 + 100.0 * 6.241910934448242
Epoch 1010, val loss: 0.9273852705955505
Epoch 1020, training loss: 624.3465576171875 = 0.34169211983680725 + 100.0 * 6.240048885345459
Epoch 1020, val loss: 0.9281555414199829
Epoch 1030, training loss: 624.8494262695312 = 0.3343847692012787 + 100.0 * 6.245150089263916
Epoch 1030, val loss: 0.9290217161178589
Epoch 1040, training loss: 624.2332153320312 = 0.3272213041782379 + 100.0 * 6.239059925079346
Epoch 1040, val loss: 0.9298893809318542
Epoch 1050, training loss: 624.0909423828125 = 0.32022956013679504 + 100.0 * 6.237707138061523
Epoch 1050, val loss: 0.9309574961662292
Epoch 1060, training loss: 624.1305541992188 = 0.3134341835975647 + 100.0 * 6.238171577453613
Epoch 1060, val loss: 0.9322987198829651
Epoch 1070, training loss: 624.2366943359375 = 0.30682042241096497 + 100.0 * 6.2392988204956055
Epoch 1070, val loss: 0.9335923790931702
Epoch 1080, training loss: 624.7492065429688 = 0.30034083127975464 + 100.0 * 6.244488716125488
Epoch 1080, val loss: 0.9352840781211853
Epoch 1090, training loss: 624.2767944335938 = 0.293861985206604 + 100.0 * 6.239829063415527
Epoch 1090, val loss: 0.9367767572402954
Epoch 1100, training loss: 623.8907470703125 = 0.28768715262413025 + 100.0 * 6.2360310554504395
Epoch 1100, val loss: 0.9387590885162354
Epoch 1110, training loss: 623.7926025390625 = 0.28166821599006653 + 100.0 * 6.235109329223633
Epoch 1110, val loss: 0.9407926201820374
Epoch 1120, training loss: 623.7589111328125 = 0.27580273151397705 + 100.0 * 6.234830856323242
Epoch 1120, val loss: 0.9429230690002441
Epoch 1130, training loss: 624.199951171875 = 0.2700614333152771 + 100.0 * 6.2392988204956055
Epoch 1130, val loss: 0.945091962814331
Epoch 1140, training loss: 623.782470703125 = 0.2644382417201996 + 100.0 * 6.235179901123047
Epoch 1140, val loss: 0.9474260210990906
Epoch 1150, training loss: 623.762451171875 = 0.258924275636673 + 100.0 * 6.235035419464111
Epoch 1150, val loss: 0.9498561024665833
Epoch 1160, training loss: 623.9237670898438 = 0.25352826714515686 + 100.0 * 6.2367024421691895
Epoch 1160, val loss: 0.9523195624351501
Epoch 1170, training loss: 623.59423828125 = 0.24830015003681183 + 100.0 * 6.23345947265625
Epoch 1170, val loss: 0.9548999667167664
Epoch 1180, training loss: 623.4552612304688 = 0.24315385520458221 + 100.0 * 6.232120990753174
Epoch 1180, val loss: 0.9577502012252808
Epoch 1190, training loss: 623.3904418945312 = 0.23818182945251465 + 100.0 * 6.231523036956787
Epoch 1190, val loss: 0.9607481956481934
Epoch 1200, training loss: 623.4497680664062 = 0.23332224786281586 + 100.0 * 6.23216438293457
Epoch 1200, val loss: 0.963738203048706
Epoch 1210, training loss: 623.7721557617188 = 0.22853496670722961 + 100.0 * 6.23543643951416
Epoch 1210, val loss: 0.9666011333465576
Epoch 1220, training loss: 623.3831787109375 = 0.22379174828529358 + 100.0 * 6.231593608856201
Epoch 1220, val loss: 0.9697303175926208
Epoch 1230, training loss: 623.533935546875 = 0.21922846138477325 + 100.0 * 6.233147144317627
Epoch 1230, val loss: 0.9728559851646423
Epoch 1240, training loss: 623.2662963867188 = 0.21471233665943146 + 100.0 * 6.230515480041504
Epoch 1240, val loss: 0.9759893417358398
Epoch 1250, training loss: 623.142822265625 = 0.21030576527118683 + 100.0 * 6.229324817657471
Epoch 1250, val loss: 0.9795039296150208
Epoch 1260, training loss: 623.0844116210938 = 0.20607469975948334 + 100.0 * 6.22878360748291
Epoch 1260, val loss: 0.9831424355506897
Epoch 1270, training loss: 623.0000610351562 = 0.20192526280879974 + 100.0 * 6.2279815673828125
Epoch 1270, val loss: 0.9867056608200073
Epoch 1280, training loss: 623.2197875976562 = 0.19789652526378632 + 100.0 * 6.230218887329102
Epoch 1280, val loss: 0.9903010725975037
Epoch 1290, training loss: 623.3973999023438 = 0.1939072608947754 + 100.0 * 6.232035160064697
Epoch 1290, val loss: 0.9939807057380676
Epoch 1300, training loss: 622.9490966796875 = 0.18993063271045685 + 100.0 * 6.227591514587402
Epoch 1300, val loss: 0.9975247979164124
Epoch 1310, training loss: 622.9244384765625 = 0.18611100316047668 + 100.0 * 6.227383136749268
Epoch 1310, val loss: 1.001509189605713
Epoch 1320, training loss: 622.8760375976562 = 0.18241514265537262 + 100.0 * 6.226935863494873
Epoch 1320, val loss: 1.005357265472412
Epoch 1330, training loss: 623.4760131835938 = 0.17879721522331238 + 100.0 * 6.232972145080566
Epoch 1330, val loss: 1.0093355178833008
Epoch 1340, training loss: 622.9056396484375 = 0.17515236139297485 + 100.0 * 6.227304458618164
Epoch 1340, val loss: 1.013150930404663
Epoch 1350, training loss: 622.7290649414062 = 0.1716574728488922 + 100.0 * 6.225574493408203
Epoch 1350, val loss: 1.0174092054367065
Epoch 1360, training loss: 622.76123046875 = 0.16824401915073395 + 100.0 * 6.2259297370910645
Epoch 1360, val loss: 1.0213710069656372
Epoch 1370, training loss: 623.1670532226562 = 0.16489191353321075 + 100.0 * 6.2300214767456055
Epoch 1370, val loss: 1.025731086730957
Epoch 1380, training loss: 622.6767578125 = 0.16162347793579102 + 100.0 * 6.225151538848877
Epoch 1380, val loss: 1.0296770334243774
Epoch 1390, training loss: 622.6476440429688 = 0.15842436254024506 + 100.0 * 6.2248921394348145
Epoch 1390, val loss: 1.0339491367340088
Epoch 1400, training loss: 622.6140747070312 = 0.15523847937583923 + 100.0 * 6.224588871002197
Epoch 1400, val loss: 1.038241982460022
Epoch 1410, training loss: 622.4269409179688 = 0.15212582051753998 + 100.0 * 6.222748279571533
Epoch 1410, val loss: 1.0426499843597412
Epoch 1420, training loss: 622.3695678710938 = 0.1491042673587799 + 100.0 * 6.222204685211182
Epoch 1420, val loss: 1.0472354888916016
Epoch 1430, training loss: 622.3746337890625 = 0.14615565538406372 + 100.0 * 6.22228479385376
Epoch 1430, val loss: 1.0516276359558105
Epoch 1440, training loss: 623.5496826171875 = 0.14327190816402435 + 100.0 * 6.234064102172852
Epoch 1440, val loss: 1.0558226108551025
Epoch 1450, training loss: 622.3405151367188 = 0.14035503566265106 + 100.0 * 6.222001075744629
Epoch 1450, val loss: 1.059965968132019
Epoch 1460, training loss: 622.421630859375 = 0.1375666707754135 + 100.0 * 6.222840785980225
Epoch 1460, val loss: 1.0645685195922852
Epoch 1470, training loss: 622.6129760742188 = 0.1348796933889389 + 100.0 * 6.224781036376953
Epoch 1470, val loss: 1.0691381692886353
Epoch 1480, training loss: 622.2395629882812 = 0.13218730688095093 + 100.0 * 6.221073627471924
Epoch 1480, val loss: 1.0734854936599731
Epoch 1490, training loss: 622.2792358398438 = 0.12957651913166046 + 100.0 * 6.22149658203125
Epoch 1490, val loss: 1.0780785083770752
Epoch 1500, training loss: 622.1201782226562 = 0.1270582675933838 + 100.0 * 6.219931125640869
Epoch 1500, val loss: 1.0827594995498657
Epoch 1510, training loss: 622.2645874023438 = 0.12460166215896606 + 100.0 * 6.221400260925293
Epoch 1510, val loss: 1.0874298810958862
Epoch 1520, training loss: 622.212158203125 = 0.12215093523263931 + 100.0 * 6.220900535583496
Epoch 1520, val loss: 1.0919125080108643
Epoch 1530, training loss: 622.6923217773438 = 0.11973752081394196 + 100.0 * 6.2257256507873535
Epoch 1530, val loss: 1.096479892730713
Epoch 1540, training loss: 622.0859985351562 = 0.11738848686218262 + 100.0 * 6.219686031341553
Epoch 1540, val loss: 1.1010704040527344
Epoch 1550, training loss: 621.993896484375 = 0.11508775502443314 + 100.0 * 6.218788146972656
Epoch 1550, val loss: 1.1057171821594238
Epoch 1560, training loss: 622.099853515625 = 0.11286745965480804 + 100.0 * 6.219870090484619
Epoch 1560, val loss: 1.1104464530944824
Epoch 1570, training loss: 622.2035522460938 = 0.11067838966846466 + 100.0 * 6.22092866897583
Epoch 1570, val loss: 1.1152081489562988
Epoch 1580, training loss: 621.9796142578125 = 0.1085369661450386 + 100.0 * 6.218710899353027
Epoch 1580, val loss: 1.1199442148208618
Epoch 1590, training loss: 621.8811645507812 = 0.10644663870334625 + 100.0 * 6.217747211456299
Epoch 1590, val loss: 1.1245875358581543
Epoch 1600, training loss: 621.8920288085938 = 0.10441120713949203 + 100.0 * 6.217875957489014
Epoch 1600, val loss: 1.1293760538101196
Epoch 1610, training loss: 622.883056640625 = 0.10243146121501923 + 100.0 * 6.227806568145752
Epoch 1610, val loss: 1.1340315341949463
Epoch 1620, training loss: 622.059326171875 = 0.10039681941270828 + 100.0 * 6.2195892333984375
Epoch 1620, val loss: 1.1386011838912964
Epoch 1630, training loss: 621.7631225585938 = 0.09846139699220657 + 100.0 * 6.216646671295166
Epoch 1630, val loss: 1.1431653499603271
Epoch 1640, training loss: 621.7116088867188 = 0.09658748656511307 + 100.0 * 6.216150283813477
Epoch 1640, val loss: 1.148116111755371
Epoch 1650, training loss: 621.9398193359375 = 0.09476293623447418 + 100.0 * 6.21845006942749
Epoch 1650, val loss: 1.1526893377304077
Epoch 1660, training loss: 621.7332763671875 = 0.09293415397405624 + 100.0 * 6.216403007507324
Epoch 1660, val loss: 1.1576374769210815
Epoch 1670, training loss: 621.6500244140625 = 0.09116344153881073 + 100.0 * 6.215588092803955
Epoch 1670, val loss: 1.162184715270996
Epoch 1680, training loss: 621.5974731445312 = 0.08943987637758255 + 100.0 * 6.215080261230469
Epoch 1680, val loss: 1.16716468334198
Epoch 1690, training loss: 621.564208984375 = 0.08776170015335083 + 100.0 * 6.21476411819458
Epoch 1690, val loss: 1.1719146966934204
Epoch 1700, training loss: 622.3236083984375 = 0.08614330738782883 + 100.0 * 6.22237491607666
Epoch 1700, val loss: 1.177000880241394
Epoch 1710, training loss: 622.4047241210938 = 0.08445326238870621 + 100.0 * 6.223202228546143
Epoch 1710, val loss: 1.180810809135437
Epoch 1720, training loss: 621.5407104492188 = 0.08281964808702469 + 100.0 * 6.214579105377197
Epoch 1720, val loss: 1.185500979423523
Epoch 1730, training loss: 621.5243530273438 = 0.08126705884933472 + 100.0 * 6.214431285858154
Epoch 1730, val loss: 1.1908018589019775
Epoch 1740, training loss: 621.4348754882812 = 0.07975493371486664 + 100.0 * 6.213551044464111
Epoch 1740, val loss: 1.195452332496643
Epoch 1750, training loss: 621.4055786132812 = 0.07828167825937271 + 100.0 * 6.213272571563721
Epoch 1750, val loss: 1.2001413106918335
Epoch 1760, training loss: 621.6439208984375 = 0.07685062289237976 + 100.0 * 6.215671062469482
Epoch 1760, val loss: 1.2047673463821411
Epoch 1770, training loss: 621.773681640625 = 0.07540808618068695 + 100.0 * 6.216982841491699
Epoch 1770, val loss: 1.2094380855560303
Epoch 1780, training loss: 621.3692626953125 = 0.07397088408470154 + 100.0 * 6.212952613830566
Epoch 1780, val loss: 1.2139378786087036
Epoch 1790, training loss: 621.3671875 = 0.07259860634803772 + 100.0 * 6.212945938110352
Epoch 1790, val loss: 1.2188117504119873
Epoch 1800, training loss: 621.2992553710938 = 0.07127542793750763 + 100.0 * 6.212279796600342
Epoch 1800, val loss: 1.2233784198760986
Epoch 1810, training loss: 621.2847900390625 = 0.0699792429804802 + 100.0 * 6.2121477127075195
Epoch 1810, val loss: 1.2280617952346802
Epoch 1820, training loss: 621.7975463867188 = 0.06871329247951508 + 100.0 * 6.217288494110107
Epoch 1820, val loss: 1.232600450515747
Epoch 1830, training loss: 621.8069458007812 = 0.06743938475847244 + 100.0 * 6.217395305633545
Epoch 1830, val loss: 1.2371474504470825
Epoch 1840, training loss: 621.4107055664062 = 0.06619136780500412 + 100.0 * 6.21344518661499
Epoch 1840, val loss: 1.2415670156478882
Epoch 1850, training loss: 621.3947143554688 = 0.0649820864200592 + 100.0 * 6.213297367095947
Epoch 1850, val loss: 1.246217131614685
Epoch 1860, training loss: 621.3491821289062 = 0.0638008862733841 + 100.0 * 6.212853908538818
Epoch 1860, val loss: 1.2509325742721558
Epoch 1870, training loss: 621.1206665039062 = 0.06264054775238037 + 100.0 * 6.210580348968506
Epoch 1870, val loss: 1.2553290128707886
Epoch 1880, training loss: 621.2293090820312 = 0.06152768060564995 + 100.0 * 6.2116780281066895
Epoch 1880, val loss: 1.2598668336868286
Epoch 1890, training loss: 621.5686645507812 = 0.06042810529470444 + 100.0 * 6.215082168579102
Epoch 1890, val loss: 1.2643123865127563
Epoch 1900, training loss: 621.1162719726562 = 0.059340063482522964 + 100.0 * 6.210569381713867
Epoch 1900, val loss: 1.2688719034194946
Epoch 1910, training loss: 621.187255859375 = 0.05828877538442612 + 100.0 * 6.211289882659912
Epoch 1910, val loss: 1.273371934890747
Epoch 1920, training loss: 621.1777954101562 = 0.05725319683551788 + 100.0 * 6.21120548248291
Epoch 1920, val loss: 1.2777562141418457
Epoch 1930, training loss: 621.11865234375 = 0.056244995445013046 + 100.0 * 6.210623741149902
Epoch 1930, val loss: 1.2821428775787354
Epoch 1940, training loss: 621.2612915039062 = 0.05526715889573097 + 100.0 * 6.212060451507568
Epoch 1940, val loss: 1.286787986755371
Epoch 1950, training loss: 621.3945922851562 = 0.05429184436798096 + 100.0 * 6.21340274810791
Epoch 1950, val loss: 1.2911149263381958
Epoch 1960, training loss: 621.044677734375 = 0.053314656019210815 + 100.0 * 6.20991325378418
Epoch 1960, val loss: 1.295283317565918
Epoch 1970, training loss: 620.9511108398438 = 0.05238603800535202 + 100.0 * 6.208987712860107
Epoch 1970, val loss: 1.2999017238616943
Epoch 1980, training loss: 620.9276123046875 = 0.05148863419890404 + 100.0 * 6.208761215209961
Epoch 1980, val loss: 1.3043113946914673
Epoch 1990, training loss: 620.9950561523438 = 0.05061579868197441 + 100.0 * 6.209444522857666
Epoch 1990, val loss: 1.3087626695632935
Epoch 2000, training loss: 621.3116455078125 = 0.04974796995520592 + 100.0 * 6.212619304656982
Epoch 2000, val loss: 1.3127541542053223
Epoch 2010, training loss: 621.0370483398438 = 0.04888712614774704 + 100.0 * 6.20988130569458
Epoch 2010, val loss: 1.316945195198059
Epoch 2020, training loss: 621.2874755859375 = 0.04805077612400055 + 100.0 * 6.2123942375183105
Epoch 2020, val loss: 1.3214361667633057
Epoch 2030, training loss: 620.9287719726562 = 0.047230105847120285 + 100.0 * 6.208815574645996
Epoch 2030, val loss: 1.3255201578140259
Epoch 2040, training loss: 620.8939819335938 = 0.04643659293651581 + 100.0 * 6.208475112915039
Epoch 2040, val loss: 1.3301336765289307
Epoch 2050, training loss: 620.7861328125 = 0.04565782845020294 + 100.0 * 6.207404613494873
Epoch 2050, val loss: 1.334355115890503
Epoch 2060, training loss: 621.0029296875 = 0.04490978643298149 + 100.0 * 6.209579944610596
Epoch 2060, val loss: 1.3383958339691162
Epoch 2070, training loss: 620.9019775390625 = 0.044160764664411545 + 100.0 * 6.208577632904053
Epoch 2070, val loss: 1.3429906368255615
Epoch 2080, training loss: 620.8527221679688 = 0.04343048855662346 + 100.0 * 6.20809268951416
Epoch 2080, val loss: 1.346859097480774
Epoch 2090, training loss: 621.1392211914062 = 0.04271762818098068 + 100.0 * 6.210964679718018
Epoch 2090, val loss: 1.3511680364608765
Epoch 2100, training loss: 620.8123168945312 = 0.04199491813778877 + 100.0 * 6.207703113555908
Epoch 2100, val loss: 1.354905605316162
Epoch 2110, training loss: 620.711669921875 = 0.04130512475967407 + 100.0 * 6.2067036628723145
Epoch 2110, val loss: 1.359263300895691
Epoch 2120, training loss: 620.6538696289062 = 0.0406419076025486 + 100.0 * 6.206132411956787
Epoch 2120, val loss: 1.3636244535446167
Epoch 2130, training loss: 620.6302490234375 = 0.039991769939661026 + 100.0 * 6.205902576446533
Epoch 2130, val loss: 1.367688775062561
Epoch 2140, training loss: 620.7183227539062 = 0.03935622423887253 + 100.0 * 6.206789493560791
Epoch 2140, val loss: 1.371568202972412
Epoch 2150, training loss: 621.2007446289062 = 0.038719549775123596 + 100.0 * 6.211620330810547
Epoch 2150, val loss: 1.3755565881729126
Epoch 2160, training loss: 620.8167724609375 = 0.038076646625995636 + 100.0 * 6.207787036895752
Epoch 2160, val loss: 1.3796590566635132
Epoch 2170, training loss: 620.7020874023438 = 0.03745203837752342 + 100.0 * 6.206645965576172
Epoch 2170, val loss: 1.3834576606750488
Epoch 2180, training loss: 620.5384521484375 = 0.03685819357633591 + 100.0 * 6.205016136169434
Epoch 2180, val loss: 1.387508749961853
Epoch 2190, training loss: 620.4842529296875 = 0.03628009930253029 + 100.0 * 6.204480171203613
Epoch 2190, val loss: 1.3916195631027222
Epoch 2200, training loss: 620.530517578125 = 0.035722434520721436 + 100.0 * 6.2049479484558105
Epoch 2200, val loss: 1.395540475845337
Epoch 2210, training loss: 621.3743896484375 = 0.03517423942685127 + 100.0 * 6.21339225769043
Epoch 2210, val loss: 1.3993562459945679
Epoch 2220, training loss: 620.7108764648438 = 0.03462015837430954 + 100.0 * 6.206762790679932
Epoch 2220, val loss: 1.4032540321350098
Epoch 2230, training loss: 620.514892578125 = 0.03407526761293411 + 100.0 * 6.204808235168457
Epoch 2230, val loss: 1.4070608615875244
Epoch 2240, training loss: 620.7968139648438 = 0.033562108874320984 + 100.0 * 6.207632541656494
Epoch 2240, val loss: 1.4106436967849731
Epoch 2250, training loss: 620.39794921875 = 0.03303517401218414 + 100.0 * 6.203649044036865
Epoch 2250, val loss: 1.4149187803268433
Epoch 2260, training loss: 620.6065063476562 = 0.03253958746790886 + 100.0 * 6.205739498138428
Epoch 2260, val loss: 1.4186681509017944
Epoch 2270, training loss: 620.9454345703125 = 0.03205190598964691 + 100.0 * 6.209134101867676
Epoch 2270, val loss: 1.422485589981079
Epoch 2280, training loss: 620.5135498046875 = 0.031540580093860626 + 100.0 * 6.204819679260254
Epoch 2280, val loss: 1.4260295629501343
Epoch 2290, training loss: 620.369384765625 = 0.03107042983174324 + 100.0 * 6.203382968902588
Epoch 2290, val loss: 1.4298986196517944
Epoch 2300, training loss: 620.312744140625 = 0.030612867325544357 + 100.0 * 6.202821254730225
Epoch 2300, val loss: 1.4337561130523682
Epoch 2310, training loss: 620.3458862304688 = 0.030168812721967697 + 100.0 * 6.203157424926758
Epoch 2310, val loss: 1.4373714923858643
Epoch 2320, training loss: 620.9578857421875 = 0.029736189171671867 + 100.0 * 6.2092814445495605
Epoch 2320, val loss: 1.4408828020095825
Epoch 2330, training loss: 620.5298461914062 = 0.029275769367814064 + 100.0 * 6.205005645751953
Epoch 2330, val loss: 1.4447436332702637
Epoch 2340, training loss: 620.6203002929688 = 0.02884713187813759 + 100.0 * 6.205914497375488
Epoch 2340, val loss: 1.4479634761810303
Epoch 2350, training loss: 620.22509765625 = 0.02842075750231743 + 100.0 * 6.201966762542725
Epoch 2350, val loss: 1.4518932104110718
Epoch 2360, training loss: 620.2899169921875 = 0.028012758120894432 + 100.0 * 6.2026190757751465
Epoch 2360, val loss: 1.455986499786377
Epoch 2370, training loss: 620.2887573242188 = 0.02761836163699627 + 100.0 * 6.202611446380615
Epoch 2370, val loss: 1.4595236778259277
Epoch 2380, training loss: 620.4306030273438 = 0.027230435982346535 + 100.0 * 6.204033851623535
Epoch 2380, val loss: 1.4631907939910889
Epoch 2390, training loss: 620.5838012695312 = 0.02684199810028076 + 100.0 * 6.205569267272949
Epoch 2390, val loss: 1.4662842750549316
Epoch 2400, training loss: 620.263916015625 = 0.026439500972628593 + 100.0 * 6.2023749351501465
Epoch 2400, val loss: 1.4697896242141724
Epoch 2410, training loss: 620.219970703125 = 0.026060491800308228 + 100.0 * 6.201939105987549
Epoch 2410, val loss: 1.4731289148330688
Epoch 2420, training loss: 620.1704711914062 = 0.025695212185382843 + 100.0 * 6.201447486877441
Epoch 2420, val loss: 1.4771252870559692
Epoch 2430, training loss: 620.4295654296875 = 0.02534561976790428 + 100.0 * 6.204042434692383
Epoch 2430, val loss: 1.4803674221038818
Epoch 2440, training loss: 620.2407836914062 = 0.02498580329120159 + 100.0 * 6.202158451080322
Epoch 2440, val loss: 1.4838595390319824
Epoch 2450, training loss: 620.127197265625 = 0.02463988959789276 + 100.0 * 6.201025485992432
Epoch 2450, val loss: 1.487546443939209
Epoch 2460, training loss: 620.1417846679688 = 0.02430698275566101 + 100.0 * 6.201175212860107
Epoch 2460, val loss: 1.4907716512680054
Epoch 2470, training loss: 620.5426025390625 = 0.023983420804142952 + 100.0 * 6.205186367034912
Epoch 2470, val loss: 1.494559645652771
Epoch 2480, training loss: 620.0714111328125 = 0.02364467829465866 + 100.0 * 6.2004780769348145
Epoch 2480, val loss: 1.4974726438522339
Epoch 2490, training loss: 620.04052734375 = 0.023324929177761078 + 100.0 * 6.200171947479248
Epoch 2490, val loss: 1.5010943412780762
Epoch 2500, training loss: 620.3214721679688 = 0.023018350824713707 + 100.0 * 6.20298433303833
Epoch 2500, val loss: 1.504370093345642
Epoch 2510, training loss: 620.073486328125 = 0.022707033902406693 + 100.0 * 6.200508117675781
Epoch 2510, val loss: 1.5076738595962524
Epoch 2520, training loss: 620.213623046875 = 0.022407524287700653 + 100.0 * 6.2019124031066895
Epoch 2520, val loss: 1.5111415386199951
Epoch 2530, training loss: 620.3970336914062 = 0.022109057754278183 + 100.0 * 6.203749656677246
Epoch 2530, val loss: 1.514293909072876
Epoch 2540, training loss: 620.1288452148438 = 0.02179894782602787 + 100.0 * 6.201070785522461
Epoch 2540, val loss: 1.5174733400344849
Epoch 2550, training loss: 619.9581298828125 = 0.021519048139452934 + 100.0 * 6.199365615844727
Epoch 2550, val loss: 1.5207955837249756
Epoch 2560, training loss: 619.8927612304688 = 0.021243248134851456 + 100.0 * 6.1987152099609375
Epoch 2560, val loss: 1.524161458015442
Epoch 2570, training loss: 619.9590454101562 = 0.020977111533284187 + 100.0 * 6.199380397796631
Epoch 2570, val loss: 1.5273874998092651
Epoch 2580, training loss: 620.619384765625 = 0.020719077438116074 + 100.0 * 6.205986976623535
Epoch 2580, val loss: 1.5305949449539185
Epoch 2590, training loss: 620.1438598632812 = 0.020437870174646378 + 100.0 * 6.201233863830566
Epoch 2590, val loss: 1.533549189567566
Epoch 2600, training loss: 620.135009765625 = 0.020168863236904144 + 100.0 * 6.201148509979248
Epoch 2600, val loss: 1.5366220474243164
Epoch 2610, training loss: 619.9219360351562 = 0.019911542534828186 + 100.0 * 6.1990203857421875
Epoch 2610, val loss: 1.5398868322372437
Epoch 2620, training loss: 619.843505859375 = 0.019665559753775597 + 100.0 * 6.198238372802734
Epoch 2620, val loss: 1.5427783727645874
Epoch 2630, training loss: 620.0092163085938 = 0.019425803795456886 + 100.0 * 6.1998982429504395
Epoch 2630, val loss: 1.545735239982605
Epoch 2640, training loss: 620.01025390625 = 0.019180607050657272 + 100.0 * 6.199911117553711
Epoch 2640, val loss: 1.548821210861206
Epoch 2650, training loss: 620.1534423828125 = 0.018939852714538574 + 100.0 * 6.201344966888428
Epoch 2650, val loss: 1.5522600412368774
Epoch 2660, training loss: 619.9093017578125 = 0.018706103786826134 + 100.0 * 6.198906421661377
Epoch 2660, val loss: 1.5546830892562866
Epoch 2670, training loss: 620.046630859375 = 0.018480125814676285 + 100.0 * 6.200281620025635
Epoch 2670, val loss: 1.5580047369003296
Epoch 2680, training loss: 619.9060668945312 = 0.018252557143568993 + 100.0 * 6.198878288269043
Epoch 2680, val loss: 1.5610439777374268
Epoch 2690, training loss: 619.871826171875 = 0.018030837178230286 + 100.0 * 6.198538303375244
Epoch 2690, val loss: 1.56389319896698
Epoch 2700, training loss: 620.1170043945312 = 0.01781388372182846 + 100.0 * 6.200991630554199
Epoch 2700, val loss: 1.566739797592163
Epoch 2710, training loss: 619.95068359375 = 0.017593765631318092 + 100.0 * 6.199331283569336
Epoch 2710, val loss: 1.569291591644287
Epoch 2720, training loss: 619.71923828125 = 0.0173809714615345 + 100.0 * 6.197018623352051
Epoch 2720, val loss: 1.5724728107452393
Epoch 2730, training loss: 619.6787109375 = 0.017178164795041084 + 100.0 * 6.196615695953369
Epoch 2730, val loss: 1.575255274772644
Epoch 2740, training loss: 619.685546875 = 0.01698130927979946 + 100.0 * 6.196685791015625
Epoch 2740, val loss: 1.5782356262207031
Epoch 2750, training loss: 620.2503662109375 = 0.01679415814578533 + 100.0 * 6.202335834503174
Epoch 2750, val loss: 1.5810741186141968
Epoch 2760, training loss: 619.70263671875 = 0.016583502292633057 + 100.0 * 6.196860313415527
Epoch 2760, val loss: 1.5837130546569824
Epoch 2770, training loss: 619.6961669921875 = 0.01638527773320675 + 100.0 * 6.196798324584961
Epoch 2770, val loss: 1.5864319801330566
Epoch 2780, training loss: 619.7182006835938 = 0.016201362013816833 + 100.0 * 6.197020053863525
Epoch 2780, val loss: 1.589173674583435
Epoch 2790, training loss: 619.7620239257812 = 0.016015812754631042 + 100.0 * 6.197460174560547
Epoch 2790, val loss: 1.5920289754867554
Epoch 2800, training loss: 619.78515625 = 0.01583614945411682 + 100.0 * 6.197693347930908
Epoch 2800, val loss: 1.5947182178497314
Epoch 2810, training loss: 619.8583374023438 = 0.015659907832741737 + 100.0 * 6.198426723480225
Epoch 2810, val loss: 1.5971851348876953
Epoch 2820, training loss: 619.5794677734375 = 0.015480900183320045 + 100.0 * 6.195639610290527
Epoch 2820, val loss: 1.6002401113510132
Epoch 2830, training loss: 619.6624145507812 = 0.015312297269701958 + 100.0 * 6.196470737457275
Epoch 2830, val loss: 1.6029061079025269
Epoch 2840, training loss: 620.147705078125 = 0.01514859963208437 + 100.0 * 6.2013258934021
Epoch 2840, val loss: 1.6056125164031982
Epoch 2850, training loss: 619.7902221679688 = 0.014966405928134918 + 100.0 * 6.197752952575684
Epoch 2850, val loss: 1.607704997062683
Epoch 2860, training loss: 619.7061157226562 = 0.014797856099903584 + 100.0 * 6.196913719177246
Epoch 2860, val loss: 1.6105501651763916
Epoch 2870, training loss: 619.6235961914062 = 0.01463419571518898 + 100.0 * 6.196089744567871
Epoch 2870, val loss: 1.6133149862289429
Epoch 2880, training loss: 619.5416259765625 = 0.014474498108029366 + 100.0 * 6.1952714920043945
Epoch 2880, val loss: 1.6158454418182373
Epoch 2890, training loss: 619.586669921875 = 0.014319978654384613 + 100.0 * 6.195723533630371
Epoch 2890, val loss: 1.618646264076233
Epoch 2900, training loss: 619.8569946289062 = 0.01416751742362976 + 100.0 * 6.198428153991699
Epoch 2900, val loss: 1.62096107006073
Epoch 2910, training loss: 619.8662719726562 = 0.014016371220350266 + 100.0 * 6.198522090911865
Epoch 2910, val loss: 1.623166561126709
Epoch 2920, training loss: 619.5087280273438 = 0.0138611551374197 + 100.0 * 6.194948673248291
Epoch 2920, val loss: 1.6261534690856934
Epoch 2930, training loss: 619.4473266601562 = 0.013713241554796696 + 100.0 * 6.1943359375
Epoch 2930, val loss: 1.6282929182052612
Epoch 2940, training loss: 619.4223022460938 = 0.013570153154432774 + 100.0 * 6.194087505340576
Epoch 2940, val loss: 1.63112211227417
Epoch 2950, training loss: 619.8111572265625 = 0.013435429893434048 + 100.0 * 6.197977542877197
Epoch 2950, val loss: 1.6334757804870605
Epoch 2960, training loss: 619.5986328125 = 0.013287080451846123 + 100.0 * 6.195853233337402
Epoch 2960, val loss: 1.635807991027832
Epoch 2970, training loss: 619.4706420898438 = 0.013146225363016129 + 100.0 * 6.194575309753418
Epoch 2970, val loss: 1.6380623579025269
Epoch 2980, training loss: 619.3965454101562 = 0.013011335395276546 + 100.0 * 6.193835258483887
Epoch 2980, val loss: 1.640938401222229
Epoch 2990, training loss: 619.4750366210938 = 0.012882336974143982 + 100.0 * 6.194621562957764
Epoch 2990, val loss: 1.643353819847107
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 861.6273193359375 = 1.9433485269546509 + 100.0 * 8.596839904785156
Epoch 0, val loss: 1.9433293342590332
Epoch 10, training loss: 861.537109375 = 1.9346519708633423 + 100.0 * 8.596024513244629
Epoch 10, val loss: 1.9347985982894897
Epoch 20, training loss: 860.975341796875 = 1.9236843585968018 + 100.0 * 8.590516090393066
Epoch 20, val loss: 1.9234849214553833
Epoch 30, training loss: 857.2853393554688 = 1.908898949623108 + 100.0 * 8.553764343261719
Epoch 30, val loss: 1.9078041315078735
Epoch 40, training loss: 835.9632568359375 = 1.890098214149475 + 100.0 * 8.340731620788574
Epoch 40, val loss: 1.8886696100234985
Epoch 50, training loss: 757.6527709960938 = 1.8683034181594849 + 100.0 * 7.557845115661621
Epoch 50, val loss: 1.8667129278182983
Epoch 60, training loss: 732.8878784179688 = 1.850535273551941 + 100.0 * 7.310373783111572
Epoch 60, val loss: 1.849965214729309
Epoch 70, training loss: 710.9056396484375 = 1.8385289907455444 + 100.0 * 7.090671539306641
Epoch 70, val loss: 1.8381558656692505
Epoch 80, training loss: 697.1647338867188 = 1.8262829780578613 + 100.0 * 6.9533843994140625
Epoch 80, val loss: 1.8257794380187988
Epoch 90, training loss: 688.684814453125 = 1.8147327899932861 + 100.0 * 6.8687005043029785
Epoch 90, val loss: 1.8143571615219116
Epoch 100, training loss: 682.2256469726562 = 1.8037970066070557 + 100.0 * 6.804218769073486
Epoch 100, val loss: 1.8036423921585083
Epoch 110, training loss: 674.7074584960938 = 1.7948240041732788 + 100.0 * 6.729126453399658
Epoch 110, val loss: 1.7948096990585327
Epoch 120, training loss: 668.4041137695312 = 1.7876908779144287 + 100.0 * 6.666163921356201
Epoch 120, val loss: 1.7875547409057617
Epoch 130, training loss: 664.4243774414062 = 1.7806775569915771 + 100.0 * 6.626436710357666
Epoch 130, val loss: 1.7800954580307007
Epoch 140, training loss: 661.3478393554688 = 1.7724512815475464 + 100.0 * 6.5957536697387695
Epoch 140, val loss: 1.771667718887329
Epoch 150, training loss: 658.9566040039062 = 1.7631968259811401 + 100.0 * 6.571934223175049
Epoch 150, val loss: 1.7623063325881958
Epoch 160, training loss: 656.8096923828125 = 1.753641128540039 + 100.0 * 6.550560474395752
Epoch 160, val loss: 1.7529298067092896
Epoch 170, training loss: 654.1243896484375 = 1.7441171407699585 + 100.0 * 6.523802280426025
Epoch 170, val loss: 1.7437398433685303
Epoch 180, training loss: 651.6353759765625 = 1.7344821691513062 + 100.0 * 6.499008655548096
Epoch 180, val loss: 1.7345046997070312
Epoch 190, training loss: 649.5003051757812 = 1.7242987155914307 + 100.0 * 6.477759838104248
Epoch 190, val loss: 1.7247813940048218
Epoch 200, training loss: 647.4354248046875 = 1.7129931449890137 + 100.0 * 6.457224369049072
Epoch 200, val loss: 1.7142685651779175
Epoch 210, training loss: 645.7657470703125 = 1.7006686925888062 + 100.0 * 6.440650463104248
Epoch 210, val loss: 1.7028673887252808
Epoch 220, training loss: 644.47119140625 = 1.6871939897537231 + 100.0 * 6.427839756011963
Epoch 220, val loss: 1.6904864311218262
Epoch 230, training loss: 643.1506958007812 = 1.6725435256958008 + 100.0 * 6.41478157043457
Epoch 230, val loss: 1.6771719455718994
Epoch 240, training loss: 642.5221557617188 = 1.6568280458450317 + 100.0 * 6.408653736114502
Epoch 240, val loss: 1.6629595756530762
Epoch 250, training loss: 641.2031860351562 = 1.6398658752441406 + 100.0 * 6.395633220672607
Epoch 250, val loss: 1.648032784461975
Epoch 260, training loss: 640.2575073242188 = 1.6220370531082153 + 100.0 * 6.386354923248291
Epoch 260, val loss: 1.6324102878570557
Epoch 270, training loss: 639.6046752929688 = 1.603178858757019 + 100.0 * 6.3800153732299805
Epoch 270, val loss: 1.6160918474197388
Epoch 280, training loss: 638.8365478515625 = 1.5833039283752441 + 100.0 * 6.372532367706299
Epoch 280, val loss: 1.599197268486023
Epoch 290, training loss: 638.1294555664062 = 1.562697172164917 + 100.0 * 6.365667819976807
Epoch 290, val loss: 1.5816792249679565
Epoch 300, training loss: 637.3449096679688 = 1.5414143800735474 + 100.0 * 6.358034610748291
Epoch 300, val loss: 1.5638052225112915
Epoch 310, training loss: 636.9398193359375 = 1.5195200443267822 + 100.0 * 6.354202747344971
Epoch 310, val loss: 1.5457112789154053
Epoch 320, training loss: 636.3138427734375 = 1.4970608949661255 + 100.0 * 6.348167419433594
Epoch 320, val loss: 1.5272680521011353
Epoch 330, training loss: 635.6289672851562 = 1.4742369651794434 + 100.0 * 6.341547012329102
Epoch 330, val loss: 1.5089281797409058
Epoch 340, training loss: 635.3494873046875 = 1.4511197805404663 + 100.0 * 6.33898401260376
Epoch 340, val loss: 1.49042546749115
Epoch 350, training loss: 634.6945190429688 = 1.42783784866333 + 100.0 * 6.332666397094727
Epoch 350, val loss: 1.4719767570495605
Epoch 360, training loss: 634.2276000976562 = 1.404282569885254 + 100.0 * 6.328233242034912
Epoch 360, val loss: 1.4536399841308594
Epoch 370, training loss: 634.1162109375 = 1.3806179761886597 + 100.0 * 6.327355861663818
Epoch 370, val loss: 1.4353147745132446
Epoch 380, training loss: 633.4011840820312 = 1.3567496538162231 + 100.0 * 6.320444583892822
Epoch 380, val loss: 1.4169262647628784
Epoch 390, training loss: 633.6034545898438 = 1.3329724073410034 + 100.0 * 6.322705268859863
Epoch 390, val loss: 1.3986034393310547
Epoch 400, training loss: 632.6187744140625 = 1.3087737560272217 + 100.0 * 6.3130998611450195
Epoch 400, val loss: 1.3802484273910522
Epoch 410, training loss: 632.2192993164062 = 1.2847951650619507 + 100.0 * 6.309345245361328
Epoch 410, val loss: 1.3621671199798584
Epoch 420, training loss: 631.8836059570312 = 1.2608604431152344 + 100.0 * 6.306227207183838
Epoch 420, val loss: 1.3442178964614868
Epoch 430, training loss: 632.3599243164062 = 1.2369002103805542 + 100.0 * 6.311230659484863
Epoch 430, val loss: 1.326197624206543
Epoch 440, training loss: 631.6372680664062 = 1.2128338813781738 + 100.0 * 6.304244518280029
Epoch 440, val loss: 1.3083056211471558
Epoch 450, training loss: 631.0875854492188 = 1.1889790296554565 + 100.0 * 6.298985958099365
Epoch 450, val loss: 1.2905229330062866
Epoch 460, training loss: 630.81201171875 = 1.1653571128845215 + 100.0 * 6.296466827392578
Epoch 460, val loss: 1.273039698600769
Epoch 470, training loss: 630.65869140625 = 1.14181649684906 + 100.0 * 6.295168399810791
Epoch 470, val loss: 1.2557264566421509
Epoch 480, training loss: 630.4783935546875 = 1.118455410003662 + 100.0 * 6.2935991287231445
Epoch 480, val loss: 1.2388533353805542
Epoch 490, training loss: 630.0894775390625 = 1.0952719449996948 + 100.0 * 6.289941787719727
Epoch 490, val loss: 1.2222256660461426
Epoch 500, training loss: 629.7809448242188 = 1.0726556777954102 + 100.0 * 6.287083148956299
Epoch 500, val loss: 1.2061043977737427
Epoch 510, training loss: 629.6932983398438 = 1.0504156351089478 + 100.0 * 6.286428928375244
Epoch 510, val loss: 1.190515160560608
Epoch 520, training loss: 629.337890625 = 1.0283554792404175 + 100.0 * 6.283094882965088
Epoch 520, val loss: 1.175199031829834
Epoch 530, training loss: 629.2919921875 = 1.0068904161453247 + 100.0 * 6.282851219177246
Epoch 530, val loss: 1.1605210304260254
Epoch 540, training loss: 629.1822509765625 = 0.9858168363571167 + 100.0 * 6.28196382522583
Epoch 540, val loss: 1.1462717056274414
Epoch 550, training loss: 628.8988647460938 = 0.9651342630386353 + 100.0 * 6.279336929321289
Epoch 550, val loss: 1.1327823400497437
Epoch 560, training loss: 628.6144409179688 = 0.945119321346283 + 100.0 * 6.276692867279053
Epoch 560, val loss: 1.1197737455368042
Epoch 570, training loss: 628.4540405273438 = 0.9256637096405029 + 100.0 * 6.2752838134765625
Epoch 570, val loss: 1.107319951057434
Epoch 580, training loss: 628.8609008789062 = 0.9066539406776428 + 100.0 * 6.279541969299316
Epoch 580, val loss: 1.0954926013946533
Epoch 590, training loss: 628.3651733398438 = 0.8877103924751282 + 100.0 * 6.274774551391602
Epoch 590, val loss: 1.083914041519165
Epoch 600, training loss: 628.0205078125 = 0.8695330023765564 + 100.0 * 6.271510124206543
Epoch 600, val loss: 1.0732038021087646
Epoch 610, training loss: 628.1130981445312 = 0.8517693281173706 + 100.0 * 6.272613525390625
Epoch 610, val loss: 1.0630539655685425
Epoch 620, training loss: 627.73291015625 = 0.8343642950057983 + 100.0 * 6.268985748291016
Epoch 620, val loss: 1.0529662370681763
Epoch 630, training loss: 627.53955078125 = 0.817386269569397 + 100.0 * 6.267221927642822
Epoch 630, val loss: 1.0439320802688599
Epoch 640, training loss: 627.3782348632812 = 0.8007892966270447 + 100.0 * 6.265774726867676
Epoch 640, val loss: 1.0351284742355347
Epoch 650, training loss: 627.8445434570312 = 0.7844369411468506 + 100.0 * 6.270601272583008
Epoch 650, val loss: 1.0267804861068726
Epoch 660, training loss: 627.3302001953125 = 0.7683925032615662 + 100.0 * 6.265618324279785
Epoch 660, val loss: 1.0188426971435547
Epoch 670, training loss: 626.9754028320312 = 0.7527390718460083 + 100.0 * 6.262226581573486
Epoch 670, val loss: 1.0113743543624878
Epoch 680, training loss: 626.8076782226562 = 0.7374581098556519 + 100.0 * 6.260701656341553
Epoch 680, val loss: 1.00437593460083
Epoch 690, training loss: 626.9967041015625 = 0.7225589156150818 + 100.0 * 6.262741565704346
Epoch 690, val loss: 0.9979955554008484
Epoch 700, training loss: 626.99560546875 = 0.7075510025024414 + 100.0 * 6.262880325317383
Epoch 700, val loss: 0.991346001625061
Epoch 710, training loss: 626.4440307617188 = 0.6931242942810059 + 100.0 * 6.257509231567383
Epoch 710, val loss: 0.9857234358787537
Epoch 720, training loss: 626.3480224609375 = 0.6790235638618469 + 100.0 * 6.25669002532959
Epoch 720, val loss: 0.9803780913352966
Epoch 730, training loss: 626.2222900390625 = 0.6651772856712341 + 100.0 * 6.255571365356445
Epoch 730, val loss: 0.9753296971321106
Epoch 740, training loss: 626.7328491210938 = 0.6515629291534424 + 100.0 * 6.260812759399414
Epoch 740, val loss: 0.9708392024040222
Epoch 750, training loss: 626.1517333984375 = 0.6380906701087952 + 100.0 * 6.255136489868164
Epoch 750, val loss: 0.9661602973937988
Epoch 760, training loss: 626.59521484375 = 0.6249042749404907 + 100.0 * 6.259703636169434
Epoch 760, val loss: 0.9621012210845947
Epoch 770, training loss: 625.9623413085938 = 0.6119422316551208 + 100.0 * 6.253504276275635
Epoch 770, val loss: 0.9581546187400818
Epoch 780, training loss: 625.7113647460938 = 0.5991792678833008 + 100.0 * 6.251121997833252
Epoch 780, val loss: 0.9545425772666931
Epoch 790, training loss: 625.5457763671875 = 0.5867453217506409 + 100.0 * 6.2495903968811035
Epoch 790, val loss: 0.9512694478034973
Epoch 800, training loss: 625.4269409179688 = 0.5744962692260742 + 100.0 * 6.248524188995361
Epoch 800, val loss: 0.9483478665351868
Epoch 810, training loss: 626.1128540039062 = 0.5624009370803833 + 100.0 * 6.255504608154297
Epoch 810, val loss: 0.9452799558639526
Epoch 820, training loss: 625.6813354492188 = 0.5503926277160645 + 100.0 * 6.251308917999268
Epoch 820, val loss: 0.9426832795143127
Epoch 830, training loss: 625.3139038085938 = 0.5386050939559937 + 100.0 * 6.247752666473389
Epoch 830, val loss: 0.9402410387992859
Epoch 840, training loss: 625.0885009765625 = 0.527042031288147 + 100.0 * 6.245614528656006
Epoch 840, val loss: 0.9380479454994202
Epoch 850, training loss: 625.626953125 = 0.5156421065330505 + 100.0 * 6.251113414764404
Epoch 850, val loss: 0.9359379410743713
Epoch 860, training loss: 625.3143310546875 = 0.5044410228729248 + 100.0 * 6.248098850250244
Epoch 860, val loss: 0.9340282678604126
Epoch 870, training loss: 624.8516845703125 = 0.4933013617992401 + 100.0 * 6.243583679199219
Epoch 870, val loss: 0.9323664307594299
Epoch 880, training loss: 624.7131958007812 = 0.4824765920639038 + 100.0 * 6.242307186126709
Epoch 880, val loss: 0.9308957457542419
Epoch 890, training loss: 624.9627075195312 = 0.4717939794063568 + 100.0 * 6.244908809661865
Epoch 890, val loss: 0.9295732975006104
Epoch 900, training loss: 624.70751953125 = 0.46118611097335815 + 100.0 * 6.242463111877441
Epoch 900, val loss: 0.9281286001205444
Epoch 910, training loss: 624.6426391601562 = 0.45075568556785583 + 100.0 * 6.241919040679932
Epoch 910, val loss: 0.9270832538604736
Epoch 920, training loss: 624.4615478515625 = 0.4405832588672638 + 100.0 * 6.240209579467773
Epoch 920, val loss: 0.9262931942939758
Epoch 930, training loss: 624.3026123046875 = 0.43055617809295654 + 100.0 * 6.238720893859863
Epoch 930, val loss: 0.9255249500274658
Epoch 940, training loss: 624.3917846679688 = 0.42073342204093933 + 100.0 * 6.239710807800293
Epoch 940, val loss: 0.9249407649040222
Epoch 950, training loss: 624.3125 = 0.4110397696495056 + 100.0 * 6.239014625549316
Epoch 950, val loss: 0.9247115850448608
Epoch 960, training loss: 624.2206420898438 = 0.4015052914619446 + 100.0 * 6.238191604614258
Epoch 960, val loss: 0.9241062998771667
Epoch 970, training loss: 624.119384765625 = 0.3921753168106079 + 100.0 * 6.237272262573242
Epoch 970, val loss: 0.9241249561309814
Epoch 980, training loss: 624.4883422851562 = 0.38300198316574097 + 100.0 * 6.241053581237793
Epoch 980, val loss: 0.9239522814750671
Epoch 990, training loss: 623.9637451171875 = 0.3740595281124115 + 100.0 * 6.235896587371826
Epoch 990, val loss: 0.9244798421859741
Epoch 1000, training loss: 623.7965087890625 = 0.3652944564819336 + 100.0 * 6.234312057495117
Epoch 1000, val loss: 0.9248015880584717
Epoch 1010, training loss: 623.7260131835938 = 0.3567497432231903 + 100.0 * 6.233692646026611
Epoch 1010, val loss: 0.9253828525543213
Epoch 1020, training loss: 624.0404663085938 = 0.3484184443950653 + 100.0 * 6.2369208335876465
Epoch 1020, val loss: 0.9262994527816772
Epoch 1030, training loss: 624.3489990234375 = 0.3401506543159485 + 100.0 * 6.24008846282959
Epoch 1030, val loss: 0.9268214702606201
Epoch 1040, training loss: 623.5870971679688 = 0.3320910632610321 + 100.0 * 6.232550144195557
Epoch 1040, val loss: 0.9276608228683472
Epoch 1050, training loss: 623.5179443359375 = 0.3243090808391571 + 100.0 * 6.231936931610107
Epoch 1050, val loss: 0.9292232990264893
Epoch 1060, training loss: 623.4327392578125 = 0.3167538344860077 + 100.0 * 6.231159687042236
Epoch 1060, val loss: 0.9307997822761536
Epoch 1070, training loss: 623.3380737304688 = 0.30939429998397827 + 100.0 * 6.230287075042725
Epoch 1070, val loss: 0.9324696660041809
Epoch 1080, training loss: 623.9442138671875 = 0.3021954596042633 + 100.0 * 6.236420154571533
Epoch 1080, val loss: 0.934175968170166
Epoch 1090, training loss: 623.6395874023438 = 0.29509612917900085 + 100.0 * 6.233444690704346
Epoch 1090, val loss: 0.9363365173339844
Epoch 1100, training loss: 623.4656372070312 = 0.28814077377319336 + 100.0 * 6.231774806976318
Epoch 1100, val loss: 0.9382393956184387
Epoch 1110, training loss: 623.2216796875 = 0.2814434766769409 + 100.0 * 6.229402542114258
Epoch 1110, val loss: 0.9406200051307678
Epoch 1120, training loss: 623.1578979492188 = 0.27490103244781494 + 100.0 * 6.228829860687256
Epoch 1120, val loss: 0.9431806206703186
Epoch 1130, training loss: 623.1228637695312 = 0.26855647563934326 + 100.0 * 6.228542804718018
Epoch 1130, val loss: 0.9458921551704407
Epoch 1140, training loss: 623.1876831054688 = 0.2623751759529114 + 100.0 * 6.229252815246582
Epoch 1140, val loss: 0.9487908482551575
Epoch 1150, training loss: 623.0205078125 = 0.2562982738018036 + 100.0 * 6.227642059326172
Epoch 1150, val loss: 0.951647937297821
Epoch 1160, training loss: 622.8935546875 = 0.25031742453575134 + 100.0 * 6.2264323234558105
Epoch 1160, val loss: 0.9546284079551697
Epoch 1170, training loss: 622.8279418945312 = 0.2445812076330185 + 100.0 * 6.225833892822266
Epoch 1170, val loss: 0.9578331708908081
Epoch 1180, training loss: 622.79052734375 = 0.23899894952774048 + 100.0 * 6.225515365600586
Epoch 1180, val loss: 0.9611707925796509
Epoch 1190, training loss: 623.6202392578125 = 0.23352831602096558 + 100.0 * 6.233867168426514
Epoch 1190, val loss: 0.9643465280532837
Epoch 1200, training loss: 623.0703125 = 0.22816823422908783 + 100.0 * 6.228421688079834
Epoch 1200, val loss: 0.9680842757225037
Epoch 1210, training loss: 622.7164306640625 = 0.22293299436569214 + 100.0 * 6.2249345779418945
Epoch 1210, val loss: 0.9716984629631042
Epoch 1220, training loss: 622.8226318359375 = 0.2178730070590973 + 100.0 * 6.226047515869141
Epoch 1220, val loss: 0.9752217531204224
Epoch 1230, training loss: 622.6664428710938 = 0.2129458636045456 + 100.0 * 6.22453498840332
Epoch 1230, val loss: 0.9793339371681213
Epoch 1240, training loss: 622.502685546875 = 0.20816241204738617 + 100.0 * 6.222945213317871
Epoch 1240, val loss: 0.9833087921142578
Epoch 1250, training loss: 622.7338256835938 = 0.20350511372089386 + 100.0 * 6.2253031730651855
Epoch 1250, val loss: 0.9874060750007629
Epoch 1260, training loss: 622.4907836914062 = 0.1989011913537979 + 100.0 * 6.222918510437012
Epoch 1260, val loss: 0.9915076494216919
Epoch 1270, training loss: 622.5634155273438 = 0.19442978501319885 + 100.0 * 6.223689556121826
Epoch 1270, val loss: 0.9956416487693787
Epoch 1280, training loss: 622.5703735351562 = 0.1900719404220581 + 100.0 * 6.2238030433654785
Epoch 1280, val loss: 0.9998667240142822
Epoch 1290, training loss: 622.3394775390625 = 0.1858377754688263 + 100.0 * 6.221536159515381
Epoch 1290, val loss: 1.004362940788269
Epoch 1300, training loss: 622.2848510742188 = 0.18171799182891846 + 100.0 * 6.221031188964844
Epoch 1300, val loss: 1.0089211463928223
Epoch 1310, training loss: 622.2489013671875 = 0.1777145117521286 + 100.0 * 6.220711708068848
Epoch 1310, val loss: 1.0134638547897339
Epoch 1320, training loss: 622.71484375 = 0.17383123934268951 + 100.0 * 6.225409984588623
Epoch 1320, val loss: 1.018096685409546
Epoch 1330, training loss: 622.5205688476562 = 0.16993778944015503 + 100.0 * 6.223506450653076
Epoch 1330, val loss: 1.0226184129714966
Epoch 1340, training loss: 622.338134765625 = 0.16616754233837128 + 100.0 * 6.221719741821289
Epoch 1340, val loss: 1.0274091958999634
Epoch 1350, training loss: 622.2680053710938 = 0.1624992936849594 + 100.0 * 6.221055030822754
Epoch 1350, val loss: 1.03212571144104
Epoch 1360, training loss: 622.0607299804688 = 0.1589512974023819 + 100.0 * 6.21901798248291
Epoch 1360, val loss: 1.036903738975525
Epoch 1370, training loss: 622.85546875 = 0.15549971163272858 + 100.0 * 6.226999282836914
Epoch 1370, val loss: 1.0417160987854004
Epoch 1380, training loss: 622.5010986328125 = 0.1520056277513504 + 100.0 * 6.223491191864014
Epoch 1380, val loss: 1.0465716123580933
Epoch 1390, training loss: 621.9478149414062 = 0.14868035912513733 + 100.0 * 6.217991352081299
Epoch 1390, val loss: 1.0514999628067017
Epoch 1400, training loss: 621.9070434570312 = 0.14546123147010803 + 100.0 * 6.217616081237793
Epoch 1400, val loss: 1.056462287902832
Epoch 1410, training loss: 621.87158203125 = 0.142332062125206 + 100.0 * 6.217292785644531
Epoch 1410, val loss: 1.0616812705993652
Epoch 1420, training loss: 622.3746948242188 = 0.13927431404590607 + 100.0 * 6.222353935241699
Epoch 1420, val loss: 1.066636562347412
Epoch 1430, training loss: 621.8394775390625 = 0.13629192113876343 + 100.0 * 6.217031955718994
Epoch 1430, val loss: 1.0719578266143799
Epoch 1440, training loss: 621.7770385742188 = 0.13333146274089813 + 100.0 * 6.216436862945557
Epoch 1440, val loss: 1.0768071413040161
Epoch 1450, training loss: 622.4497680664062 = 0.1305209845304489 + 100.0 * 6.2231926918029785
Epoch 1450, val loss: 1.0822001695632935
Epoch 1460, training loss: 621.8902587890625 = 0.12764738500118256 + 100.0 * 6.217626094818115
Epoch 1460, val loss: 1.0872411727905273
Epoch 1470, training loss: 621.73388671875 = 0.12492449581623077 + 100.0 * 6.216089248657227
Epoch 1470, val loss: 1.092423677444458
Epoch 1480, training loss: 621.6420288085938 = 0.12227068096399307 + 100.0 * 6.215197563171387
Epoch 1480, val loss: 1.0976985692977905
Epoch 1490, training loss: 621.6896362304688 = 0.1197160854935646 + 100.0 * 6.215699195861816
Epoch 1490, val loss: 1.1029951572418213
Epoch 1500, training loss: 622.02783203125 = 0.11718568950891495 + 100.0 * 6.219106197357178
Epoch 1500, val loss: 1.1082229614257812
Epoch 1510, training loss: 621.766845703125 = 0.11470979452133179 + 100.0 * 6.216521739959717
Epoch 1510, val loss: 1.1134414672851562
Epoch 1520, training loss: 621.6190185546875 = 0.1122829020023346 + 100.0 * 6.215067386627197
Epoch 1520, val loss: 1.1186845302581787
Epoch 1530, training loss: 621.5919189453125 = 0.10993532091379166 + 100.0 * 6.21481990814209
Epoch 1530, val loss: 1.1240357160568237
Epoch 1540, training loss: 621.676513671875 = 0.10764830559492111 + 100.0 * 6.215688705444336
Epoch 1540, val loss: 1.1291693449020386
Epoch 1550, training loss: 621.4533081054688 = 0.10538459569215775 + 100.0 * 6.213479042053223
Epoch 1550, val loss: 1.134415864944458
Epoch 1560, training loss: 621.7156372070312 = 0.10320837795734406 + 100.0 * 6.216124534606934
Epoch 1560, val loss: 1.1396393775939941
Epoch 1570, training loss: 621.9915771484375 = 0.10104132443666458 + 100.0 * 6.218905448913574
Epoch 1570, val loss: 1.1449637413024902
Epoch 1580, training loss: 621.4892578125 = 0.09889833629131317 + 100.0 * 6.213903903961182
Epoch 1580, val loss: 1.1499218940734863
Epoch 1590, training loss: 621.3034057617188 = 0.09685216099023819 + 100.0 * 6.21206521987915
Epoch 1590, val loss: 1.1554311513900757
Epoch 1600, training loss: 621.2286376953125 = 0.09488345682621002 + 100.0 * 6.211337089538574
Epoch 1600, val loss: 1.1607415676116943
Epoch 1610, training loss: 621.2481079101562 = 0.09296365827322006 + 100.0 * 6.211551666259766
Epoch 1610, val loss: 1.1660434007644653
Epoch 1620, training loss: 622.3262939453125 = 0.0910828560590744 + 100.0 * 6.222352504730225
Epoch 1620, val loss: 1.1710046529769897
Epoch 1630, training loss: 621.4639282226562 = 0.08916662633419037 + 100.0 * 6.213747501373291
Epoch 1630, val loss: 1.176831603050232
Epoch 1640, training loss: 621.1358032226562 = 0.0873347595334053 + 100.0 * 6.210484504699707
Epoch 1640, val loss: 1.181978464126587
Epoch 1650, training loss: 621.1369018554688 = 0.08557388931512833 + 100.0 * 6.210513114929199
Epoch 1650, val loss: 1.1871036291122437
Epoch 1660, training loss: 621.1423950195312 = 0.08387843519449234 + 100.0 * 6.210585594177246
Epoch 1660, val loss: 1.1927156448364258
Epoch 1670, training loss: 622.1007690429688 = 0.08222436159849167 + 100.0 * 6.220185279846191
Epoch 1670, val loss: 1.1980727910995483
Epoch 1680, training loss: 621.2169189453125 = 0.08048985153436661 + 100.0 * 6.211364269256592
Epoch 1680, val loss: 1.2027573585510254
Epoch 1690, training loss: 621.0014038085938 = 0.07887373119592667 + 100.0 * 6.209225654602051
Epoch 1690, val loss: 1.2082371711730957
Epoch 1700, training loss: 621.0748291015625 = 0.07732081413269043 + 100.0 * 6.209975242614746
Epoch 1700, val loss: 1.2136073112487793
Epoch 1710, training loss: 621.4308471679688 = 0.07578481733798981 + 100.0 * 6.213550567626953
Epoch 1710, val loss: 1.218748927116394
Epoch 1720, training loss: 621.3550415039062 = 0.07427362352609634 + 100.0 * 6.212807655334473
Epoch 1720, val loss: 1.2240030765533447
Epoch 1730, training loss: 621.0715942382812 = 0.07280237972736359 + 100.0 * 6.209987640380859
Epoch 1730, val loss: 1.2293154001235962
Epoch 1740, training loss: 621.1053466796875 = 0.07136818021535873 + 100.0 * 6.2103400230407715
Epoch 1740, val loss: 1.2344611883163452
Epoch 1750, training loss: 620.8513793945312 = 0.0699591189622879 + 100.0 * 6.2078142166137695
Epoch 1750, val loss: 1.239702582359314
Epoch 1760, training loss: 620.864013671875 = 0.06860118359327316 + 100.0 * 6.207954406738281
Epoch 1760, val loss: 1.2447668313980103
Epoch 1770, training loss: 621.0883178710938 = 0.06727495789527893 + 100.0 * 6.21021032333374
Epoch 1770, val loss: 1.249982476234436
Epoch 1780, training loss: 621.4524536132812 = 0.0659717544913292 + 100.0 * 6.213864803314209
Epoch 1780, val loss: 1.2552257776260376
Epoch 1790, training loss: 620.9464721679688 = 0.064640112221241 + 100.0 * 6.208818435668945
Epoch 1790, val loss: 1.260089635848999
Epoch 1800, training loss: 620.8428955078125 = 0.06338492780923843 + 100.0 * 6.2077956199646
Epoch 1800, val loss: 1.2651982307434082
Epoch 1810, training loss: 620.72509765625 = 0.06216856464743614 + 100.0 * 6.206629276275635
Epoch 1810, val loss: 1.2704360485076904
Epoch 1820, training loss: 620.6820068359375 = 0.06099631264805794 + 100.0 * 6.206210136413574
Epoch 1820, val loss: 1.2755900621414185
Epoch 1830, training loss: 620.8347778320312 = 0.05985427647829056 + 100.0 * 6.207748889923096
Epoch 1830, val loss: 1.280811071395874
Epoch 1840, training loss: 621.0072021484375 = 0.05871611461043358 + 100.0 * 6.209484577178955
Epoch 1840, val loss: 1.2857195138931274
Epoch 1850, training loss: 620.8953857421875 = 0.05756062641739845 + 100.0 * 6.208378314971924
Epoch 1850, val loss: 1.290613055229187
Epoch 1860, training loss: 620.6219482421875 = 0.05645603686571121 + 100.0 * 6.205655097961426
Epoch 1860, val loss: 1.2957117557525635
Epoch 1870, training loss: 620.605224609375 = 0.05539650842547417 + 100.0 * 6.205497741699219
Epoch 1870, val loss: 1.3007704019546509
Epoch 1880, training loss: 620.6272583007812 = 0.054366108030080795 + 100.0 * 6.205728530883789
Epoch 1880, val loss: 1.3057719469070435
Epoch 1890, training loss: 621.1253051757812 = 0.05335613340139389 + 100.0 * 6.210719585418701
Epoch 1890, val loss: 1.3106862306594849
Epoch 1900, training loss: 620.5692138671875 = 0.05234954133629799 + 100.0 * 6.2051682472229
Epoch 1900, val loss: 1.3156967163085938
Epoch 1910, training loss: 620.5617065429688 = 0.05137351155281067 + 100.0 * 6.205103397369385
Epoch 1910, val loss: 1.3207361698150635
Epoch 1920, training loss: 620.9960327148438 = 0.05041741207242012 + 100.0 * 6.209455966949463
Epoch 1920, val loss: 1.3255808353424072
Epoch 1930, training loss: 620.7125244140625 = 0.04947953298687935 + 100.0 * 6.206630229949951
Epoch 1930, val loss: 1.3300554752349854
Epoch 1940, training loss: 620.6228637695312 = 0.04856378585100174 + 100.0 * 6.205742835998535
Epoch 1940, val loss: 1.3350785970687866
Epoch 1950, training loss: 620.6937866210938 = 0.04768193140625954 + 100.0 * 6.206461429595947
Epoch 1950, val loss: 1.3397173881530762
Epoch 1960, training loss: 620.4310913085938 = 0.04680964723229408 + 100.0 * 6.203842639923096
Epoch 1960, val loss: 1.3449374437332153
Epoch 1970, training loss: 620.3953857421875 = 0.0459711067378521 + 100.0 * 6.203494071960449
Epoch 1970, val loss: 1.3495956659317017
Epoch 1980, training loss: 620.53564453125 = 0.04515109956264496 + 100.0 * 6.204905033111572
Epoch 1980, val loss: 1.3542922735214233
Epoch 1990, training loss: 620.83203125 = 0.04433950036764145 + 100.0 * 6.207877159118652
Epoch 1990, val loss: 1.3591296672821045
Epoch 2000, training loss: 620.5533447265625 = 0.043532878160476685 + 100.0 * 6.2050981521606445
Epoch 2000, val loss: 1.3632737398147583
Epoch 2010, training loss: 620.4291381835938 = 0.04274812713265419 + 100.0 * 6.203864097595215
Epoch 2010, val loss: 1.3683850765228271
Epoch 2020, training loss: 620.3077392578125 = 0.04198509082198143 + 100.0 * 6.202657699584961
Epoch 2020, val loss: 1.3728941679000854
Epoch 2030, training loss: 620.26708984375 = 0.04125579446554184 + 100.0 * 6.202258586883545
Epoch 2030, val loss: 1.3775899410247803
Epoch 2040, training loss: 620.704833984375 = 0.040556877851486206 + 100.0 * 6.206643104553223
Epoch 2040, val loss: 1.3822296857833862
Epoch 2050, training loss: 620.428466796875 = 0.039817530661821365 + 100.0 * 6.20388650894165
Epoch 2050, val loss: 1.3865442276000977
Epoch 2060, training loss: 620.355224609375 = 0.039111070334911346 + 100.0 * 6.203160762786865
Epoch 2060, val loss: 1.3909010887145996
Epoch 2070, training loss: 620.2500610351562 = 0.038417477160692215 + 100.0 * 6.2021164894104
Epoch 2070, val loss: 1.395513653755188
Epoch 2080, training loss: 620.1573486328125 = 0.037762515246868134 + 100.0 * 6.20119571685791
Epoch 2080, val loss: 1.3999780416488647
Epoch 2090, training loss: 620.1574096679688 = 0.037129927426576614 + 100.0 * 6.201202869415283
Epoch 2090, val loss: 1.4045524597167969
Epoch 2100, training loss: 621.0180053710938 = 0.03652641922235489 + 100.0 * 6.20981502532959
Epoch 2100, val loss: 1.409023642539978
Epoch 2110, training loss: 620.46142578125 = 0.03586791083216667 + 100.0 * 6.204255104064941
Epoch 2110, val loss: 1.413235068321228
Epoch 2120, training loss: 620.1941528320312 = 0.035256195813417435 + 100.0 * 6.2015886306762695
Epoch 2120, val loss: 1.4174925088882446
Epoch 2130, training loss: 620.1376953125 = 0.034660883247852325 + 100.0 * 6.201030254364014
Epoch 2130, val loss: 1.4220192432403564
Epoch 2140, training loss: 620.4959106445312 = 0.03410366177558899 + 100.0 * 6.204617977142334
Epoch 2140, val loss: 1.426484227180481
Epoch 2150, training loss: 620.1838989257812 = 0.03351825848221779 + 100.0 * 6.201503753662109
Epoch 2150, val loss: 1.4304522275924683
Epoch 2160, training loss: 620.0747680664062 = 0.03295290842652321 + 100.0 * 6.200417995452881
Epoch 2160, val loss: 1.4347888231277466
Epoch 2170, training loss: 620.0803833007812 = 0.03241412714123726 + 100.0 * 6.200479507446289
Epoch 2170, val loss: 1.4390078783035278
Epoch 2180, training loss: 620.2655639648438 = 0.03188837692141533 + 100.0 * 6.202337265014648
Epoch 2180, val loss: 1.443071722984314
Epoch 2190, training loss: 620.256591796875 = 0.031370408833026886 + 100.0 * 6.202252388000488
Epoch 2190, val loss: 1.4474236965179443
Epoch 2200, training loss: 620.2002563476562 = 0.0308575127273798 + 100.0 * 6.201694011688232
Epoch 2200, val loss: 1.4515211582183838
Epoch 2210, training loss: 620.2421264648438 = 0.030354782938957214 + 100.0 * 6.202117919921875
Epoch 2210, val loss: 1.4553000926971436
Epoch 2220, training loss: 619.9934692382812 = 0.029863329604268074 + 100.0 * 6.199636459350586
Epoch 2220, val loss: 1.4598087072372437
Epoch 2230, training loss: 619.9588623046875 = 0.029394350945949554 + 100.0 * 6.199294567108154
Epoch 2230, val loss: 1.4637925624847412
Epoch 2240, training loss: 620.0619506835938 = 0.028931666165590286 + 100.0 * 6.2003302574157715
Epoch 2240, val loss: 1.467618703842163
Epoch 2250, training loss: 620.3382568359375 = 0.02847527153789997 + 100.0 * 6.203097343444824
Epoch 2250, val loss: 1.4715070724487305
Epoch 2260, training loss: 620.1347045898438 = 0.02802947536110878 + 100.0 * 6.201066970825195
Epoch 2260, val loss: 1.4755911827087402
Epoch 2270, training loss: 619.9672241210938 = 0.027584515511989594 + 100.0 * 6.199396133422852
Epoch 2270, val loss: 1.4795773029327393
Epoch 2280, training loss: 620.35107421875 = 0.027175066992640495 + 100.0 * 6.2032389640808105
Epoch 2280, val loss: 1.4834994077682495
Epoch 2290, training loss: 619.9205932617188 = 0.02673378586769104 + 100.0 * 6.198938846588135
Epoch 2290, val loss: 1.4870723485946655
Epoch 2300, training loss: 619.9507446289062 = 0.026333104819059372 + 100.0 * 6.199244499206543
Epoch 2300, val loss: 1.4909977912902832
Epoch 2310, training loss: 620.166259765625 = 0.025938449427485466 + 100.0 * 6.2014031410217285
Epoch 2310, val loss: 1.494681715965271
Epoch 2320, training loss: 619.8516845703125 = 0.02553892321884632 + 100.0 * 6.198261737823486
Epoch 2320, val loss: 1.498502254486084
Epoch 2330, training loss: 619.8890380859375 = 0.025155022740364075 + 100.0 * 6.198638916015625
Epoch 2330, val loss: 1.502345323562622
Epoch 2340, training loss: 620.1203002929688 = 0.024788158014416695 + 100.0 * 6.200955390930176
Epoch 2340, val loss: 1.5059704780578613
Epoch 2350, training loss: 619.8849487304688 = 0.024412861093878746 + 100.0 * 6.198605537414551
Epoch 2350, val loss: 1.5097334384918213
Epoch 2360, training loss: 619.8716430664062 = 0.02404787205159664 + 100.0 * 6.1984758377075195
Epoch 2360, val loss: 1.513022780418396
Epoch 2370, training loss: 619.9105834960938 = 0.023689910769462585 + 100.0 * 6.198868751525879
Epoch 2370, val loss: 1.5167816877365112
Epoch 2380, training loss: 619.7719116210938 = 0.023346537724137306 + 100.0 * 6.197485446929932
Epoch 2380, val loss: 1.5204139947891235
Epoch 2390, training loss: 619.7670288085938 = 0.023010388016700745 + 100.0 * 6.197440147399902
Epoch 2390, val loss: 1.5236971378326416
Epoch 2400, training loss: 620.5552978515625 = 0.022685343399643898 + 100.0 * 6.205326080322266
Epoch 2400, val loss: 1.527276873588562
Epoch 2410, training loss: 619.9121704101562 = 0.022352885454893112 + 100.0 * 6.1988983154296875
Epoch 2410, val loss: 1.5310217142105103
Epoch 2420, training loss: 619.825439453125 = 0.022028246894478798 + 100.0 * 6.198034286499023
Epoch 2420, val loss: 1.5344645977020264
Epoch 2430, training loss: 619.89892578125 = 0.021722329780459404 + 100.0 * 6.198771953582764
Epoch 2430, val loss: 1.5378947257995605
Epoch 2440, training loss: 619.6654663085938 = 0.02140788361430168 + 100.0 * 6.19644021987915
Epoch 2440, val loss: 1.5412797927856445
Epoch 2450, training loss: 619.92138671875 = 0.021113403141498566 + 100.0 * 6.199002742767334
Epoch 2450, val loss: 1.5443872213363647
Epoch 2460, training loss: 619.6640014648438 = 0.020819041877985 + 100.0 * 6.196431636810303
Epoch 2460, val loss: 1.5479907989501953
Epoch 2470, training loss: 619.6788330078125 = 0.020534290000796318 + 100.0 * 6.196583271026611
Epoch 2470, val loss: 1.5512323379516602
Epoch 2480, training loss: 619.717529296875 = 0.02025766484439373 + 100.0 * 6.196972846984863
Epoch 2480, val loss: 1.5545470714569092
Epoch 2490, training loss: 619.9589233398438 = 0.01999051682651043 + 100.0 * 6.1993889808654785
Epoch 2490, val loss: 1.5579018592834473
Epoch 2500, training loss: 619.818115234375 = 0.019703304395079613 + 100.0 * 6.197983741760254
Epoch 2500, val loss: 1.5607273578643799
Epoch 2510, training loss: 619.6373291015625 = 0.019433924928307533 + 100.0 * 6.196179389953613
Epoch 2510, val loss: 1.5642718076705933
Epoch 2520, training loss: 619.5961303710938 = 0.019175373017787933 + 100.0 * 6.195769786834717
Epoch 2520, val loss: 1.567223310470581
Epoch 2530, training loss: 619.87353515625 = 0.018924187868833542 + 100.0 * 6.198545932769775
Epoch 2530, val loss: 1.5704394578933716
Epoch 2540, training loss: 619.5675659179688 = 0.018671659752726555 + 100.0 * 6.195488929748535
Epoch 2540, val loss: 1.573657751083374
Epoch 2550, training loss: 619.4865112304688 = 0.018417995423078537 + 100.0 * 6.194680690765381
Epoch 2550, val loss: 1.5765184164047241
Epoch 2560, training loss: 619.4736938476562 = 0.018178345635533333 + 100.0 * 6.194555282592773
Epoch 2560, val loss: 1.5798640251159668
Epoch 2570, training loss: 619.7861938476562 = 0.017952343448996544 + 100.0 * 6.1976823806762695
Epoch 2570, val loss: 1.5829198360443115
Epoch 2580, training loss: 619.7147216796875 = 0.017709193751215935 + 100.0 * 6.196969985961914
Epoch 2580, val loss: 1.5857510566711426
Epoch 2590, training loss: 619.5184936523438 = 0.01747949607670307 + 100.0 * 6.195010185241699
Epoch 2590, val loss: 1.5887717008590698
Epoch 2600, training loss: 619.4240112304688 = 0.01725175604224205 + 100.0 * 6.194067478179932
Epoch 2600, val loss: 1.5918291807174683
Epoch 2610, training loss: 619.4110107421875 = 0.017039895057678223 + 100.0 * 6.193939685821533
Epoch 2610, val loss: 1.5947755575180054
Epoch 2620, training loss: 619.5606079101562 = 0.016834648326039314 + 100.0 * 6.195437908172607
Epoch 2620, val loss: 1.5977519750595093
Epoch 2630, training loss: 619.5923461914062 = 0.016620896756649017 + 100.0 * 6.1957573890686035
Epoch 2630, val loss: 1.6005412340164185
Epoch 2640, training loss: 619.63720703125 = 0.016407513990998268 + 100.0 * 6.1962080001831055
Epoch 2640, val loss: 1.603312373161316
Epoch 2650, training loss: 619.7615966796875 = 0.01619652472436428 + 100.0 * 6.197454452514648
Epoch 2650, val loss: 1.6061742305755615
Epoch 2660, training loss: 619.4605712890625 = 0.016002479940652847 + 100.0 * 6.194446086883545
Epoch 2660, val loss: 1.608947515487671
Epoch 2670, training loss: 619.3507690429688 = 0.015801308676600456 + 100.0 * 6.193349361419678
Epoch 2670, val loss: 1.6118801832199097
Epoch 2680, training loss: 619.3545532226562 = 0.015615514479577541 + 100.0 * 6.193389415740967
Epoch 2680, val loss: 1.6145204305648804
Epoch 2690, training loss: 619.7006225585938 = 0.015434975735843182 + 100.0 * 6.19685173034668
Epoch 2690, val loss: 1.6170927286148071
Epoch 2700, training loss: 619.6328125 = 0.01524793915450573 + 100.0 * 6.196175575256348
Epoch 2700, val loss: 1.620497703552246
Epoch 2710, training loss: 619.478515625 = 0.015063040889799595 + 100.0 * 6.194634437561035
Epoch 2710, val loss: 1.622739553451538
Epoch 2720, training loss: 619.3193359375 = 0.01487749069929123 + 100.0 * 6.193044185638428
Epoch 2720, val loss: 1.6255605220794678
Epoch 2730, training loss: 619.269287109375 = 0.014704030938446522 + 100.0 * 6.1925458908081055
Epoch 2730, val loss: 1.6282490491867065
Epoch 2740, training loss: 619.547607421875 = 0.014538001269102097 + 100.0 * 6.19533109664917
Epoch 2740, val loss: 1.630784273147583
Epoch 2750, training loss: 619.3059692382812 = 0.014366409741342068 + 100.0 * 6.192916393280029
Epoch 2750, val loss: 1.6333835124969482
Epoch 2760, training loss: 619.36572265625 = 0.014203691855072975 + 100.0 * 6.193514823913574
Epoch 2760, val loss: 1.6363147497177124
Epoch 2770, training loss: 619.54296875 = 0.014040707610547543 + 100.0 * 6.195289134979248
Epoch 2770, val loss: 1.6388143301010132
Epoch 2780, training loss: 619.2974853515625 = 0.013879109174013138 + 100.0 * 6.192836284637451
Epoch 2780, val loss: 1.6411892175674438
Epoch 2790, training loss: 619.5252075195312 = 0.013724971562623978 + 100.0 * 6.195114612579346
Epoch 2790, val loss: 1.6437628269195557
Epoch 2800, training loss: 619.6599731445312 = 0.013564247637987137 + 100.0 * 6.1964640617370605
Epoch 2800, val loss: 1.6463645696640015
Epoch 2810, training loss: 619.4193725585938 = 0.013403565622866154 + 100.0 * 6.194059371948242
Epoch 2810, val loss: 1.6485048532485962
Epoch 2820, training loss: 619.26904296875 = 0.013258608989417553 + 100.0 * 6.1925578117370605
Epoch 2820, val loss: 1.6514867544174194
Epoch 2830, training loss: 619.5238647460938 = 0.013112890534102917 + 100.0 * 6.195107460021973
Epoch 2830, val loss: 1.6538358926773071
Epoch 2840, training loss: 619.1995849609375 = 0.012964706867933273 + 100.0 * 6.191866397857666
Epoch 2840, val loss: 1.6562604904174805
Epoch 2850, training loss: 619.2203369140625 = 0.012823378667235374 + 100.0 * 6.192075252532959
Epoch 2850, val loss: 1.658591389656067
Epoch 2860, training loss: 619.16162109375 = 0.012686459347605705 + 100.0 * 6.191489219665527
Epoch 2860, val loss: 1.6609363555908203
Epoch 2870, training loss: 619.3322143554688 = 0.012554594315588474 + 100.0 * 6.1931962966918945
Epoch 2870, val loss: 1.6632193326950073
Epoch 2880, training loss: 619.3607788085938 = 0.012420726008713245 + 100.0 * 6.193483352661133
Epoch 2880, val loss: 1.6656687259674072
Epoch 2890, training loss: 619.3422241210938 = 0.01228890661150217 + 100.0 * 6.193299293518066
Epoch 2890, val loss: 1.668045997619629
Epoch 2900, training loss: 619.3280029296875 = 0.01215365994721651 + 100.0 * 6.1931586265563965
Epoch 2900, val loss: 1.6702886819839478
Epoch 2910, training loss: 619.2005004882812 = 0.012026224285364151 + 100.0 * 6.191884994506836
Epoch 2910, val loss: 1.672584056854248
Epoch 2920, training loss: 619.1044311523438 = 0.011899399571120739 + 100.0 * 6.190925598144531
Epoch 2920, val loss: 1.6746423244476318
Epoch 2930, training loss: 619.7371826171875 = 0.011777156963944435 + 100.0 * 6.197254180908203
Epoch 2930, val loss: 1.6767030954360962
Epoch 2940, training loss: 619.2398071289062 = 0.011658764444291592 + 100.0 * 6.192281723022461
Epoch 2940, val loss: 1.6793192625045776
Epoch 2950, training loss: 619.0606079101562 = 0.011530798859894276 + 100.0 * 6.19049072265625
Epoch 2950, val loss: 1.6813262701034546
Epoch 2960, training loss: 619.0587768554688 = 0.011419190093874931 + 100.0 * 6.190473556518555
Epoch 2960, val loss: 1.6836336851119995
Epoch 2970, training loss: 619.3989868164062 = 0.011310817673802376 + 100.0 * 6.193877220153809
Epoch 2970, val loss: 1.6857749223709106
Epoch 2980, training loss: 619.1683959960938 = 0.011190158315002918 + 100.0 * 6.191572189331055
Epoch 2980, val loss: 1.687721848487854
Epoch 2990, training loss: 619.2464599609375 = 0.011081772856414318 + 100.0 * 6.19235372543335
Epoch 2990, val loss: 1.6901752948760986
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 861.628662109375 = 1.9437059164047241 + 100.0 * 8.59684944152832
Epoch 0, val loss: 1.9392296075820923
Epoch 10, training loss: 861.5414428710938 = 1.9350252151489258 + 100.0 * 8.596064567565918
Epoch 10, val loss: 1.9299565553665161
Epoch 20, training loss: 860.934326171875 = 1.924370288848877 + 100.0 * 8.590099334716797
Epoch 20, val loss: 1.9184857606887817
Epoch 30, training loss: 856.78076171875 = 1.9108426570892334 + 100.0 * 8.548699378967285
Epoch 30, val loss: 1.904008388519287
Epoch 40, training loss: 833.3369750976562 = 1.8938567638397217 + 100.0 * 8.314431190490723
Epoch 40, val loss: 1.8863766193389893
Epoch 50, training loss: 756.6751708984375 = 1.8747937679290771 + 100.0 * 7.548003673553467
Epoch 50, val loss: 1.867531180381775
Epoch 60, training loss: 734.4191284179688 = 1.8598976135253906 + 100.0 * 7.325592041015625
Epoch 60, val loss: 1.8542656898498535
Epoch 70, training loss: 715.2627563476562 = 1.8466668128967285 + 100.0 * 7.13416051864624
Epoch 70, val loss: 1.8413151502609253
Epoch 80, training loss: 697.9060668945312 = 1.8322893381118774 + 100.0 * 6.960738182067871
Epoch 80, val loss: 1.8270798921585083
Epoch 90, training loss: 685.8126831054688 = 1.8206889629364014 + 100.0 * 6.8399200439453125
Epoch 90, val loss: 1.815451979637146
Epoch 100, training loss: 676.6376342773438 = 1.8113285303115845 + 100.0 * 6.748262882232666
Epoch 100, val loss: 1.8063291311264038
Epoch 110, training loss: 669.5626831054688 = 1.8034733533859253 + 100.0 * 6.6775922775268555
Epoch 110, val loss: 1.7987254858016968
Epoch 120, training loss: 664.896728515625 = 1.7955557107925415 + 100.0 * 6.631011962890625
Epoch 120, val loss: 1.791143774986267
Epoch 130, training loss: 661.2136840820312 = 1.7873653173446655 + 100.0 * 6.594263553619385
Epoch 130, val loss: 1.7833331823349
Epoch 140, training loss: 658.2462768554688 = 1.7788535356521606 + 100.0 * 6.564673900604248
Epoch 140, val loss: 1.7752171754837036
Epoch 150, training loss: 655.9124755859375 = 1.7699568271636963 + 100.0 * 6.5414252281188965
Epoch 150, val loss: 1.766749382019043
Epoch 160, training loss: 653.6900024414062 = 1.760498285293579 + 100.0 * 6.5192952156066895
Epoch 160, val loss: 1.757848858833313
Epoch 170, training loss: 651.6165771484375 = 1.75021231174469 + 100.0 * 6.498663902282715
Epoch 170, val loss: 1.7484240531921387
Epoch 180, training loss: 649.9716186523438 = 1.7391396760940552 + 100.0 * 6.482324600219727
Epoch 180, val loss: 1.7382934093475342
Epoch 190, training loss: 648.2833251953125 = 1.7269963026046753 + 100.0 * 6.4655632972717285
Epoch 190, val loss: 1.727157711982727
Epoch 200, training loss: 647.0772094726562 = 1.7134473323822021 + 100.0 * 6.453637599945068
Epoch 200, val loss: 1.7148555517196655
Epoch 210, training loss: 645.9315795898438 = 1.6983106136322021 + 100.0 * 6.442332744598389
Epoch 210, val loss: 1.7010263204574585
Epoch 220, training loss: 644.75048828125 = 1.6816805601119995 + 100.0 * 6.43068790435791
Epoch 220, val loss: 1.6858645677566528
Epoch 230, training loss: 643.744384765625 = 1.6635174751281738 + 100.0 * 6.420808792114258
Epoch 230, val loss: 1.6693848371505737
Epoch 240, training loss: 643.1978149414062 = 1.643842101097107 + 100.0 * 6.415539264678955
Epoch 240, val loss: 1.651542067527771
Epoch 250, training loss: 642.2793579101562 = 1.622362732887268 + 100.0 * 6.406569957733154
Epoch 250, val loss: 1.6320353746414185
Epoch 260, training loss: 641.2301025390625 = 1.5994971990585327 + 100.0 * 6.396306037902832
Epoch 260, val loss: 1.6114376783370972
Epoch 270, training loss: 640.3792724609375 = 1.575244665145874 + 100.0 * 6.388040065765381
Epoch 270, val loss: 1.5897799730300903
Epoch 280, training loss: 639.6015014648438 = 1.5497441291809082 + 100.0 * 6.380517482757568
Epoch 280, val loss: 1.5671889781951904
Epoch 290, training loss: 639.2861328125 = 1.5228773355484009 + 100.0 * 6.3776326179504395
Epoch 290, val loss: 1.5435981750488281
Epoch 300, training loss: 638.660888671875 = 1.4949454069137573 + 100.0 * 6.371659278869629
Epoch 300, val loss: 1.5192794799804688
Epoch 310, training loss: 637.7909545898438 = 1.466336965560913 + 100.0 * 6.363246440887451
Epoch 310, val loss: 1.4948327541351318
Epoch 320, training loss: 637.197021484375 = 1.4372494220733643 + 100.0 * 6.357597827911377
Epoch 320, val loss: 1.4701846837997437
Epoch 330, training loss: 637.1324462890625 = 1.4078338146209717 + 100.0 * 6.357245922088623
Epoch 330, val loss: 1.4456191062927246
Epoch 340, training loss: 636.2470092773438 = 1.3779391050338745 + 100.0 * 6.348690509796143
Epoch 340, val loss: 1.4212076663970947
Epoch 350, training loss: 635.646484375 = 1.3483952283859253 + 100.0 * 6.342980861663818
Epoch 350, val loss: 1.3972814083099365
Epoch 360, training loss: 635.4166259765625 = 1.3189153671264648 + 100.0 * 6.340977191925049
Epoch 360, val loss: 1.3738559484481812
Epoch 370, training loss: 634.8428344726562 = 1.289552092552185 + 100.0 * 6.3355326652526855
Epoch 370, val loss: 1.350953459739685
Epoch 380, training loss: 634.5808715820312 = 1.2605955600738525 + 100.0 * 6.333202362060547
Epoch 380, val loss: 1.3286641836166382
Epoch 390, training loss: 633.941162109375 = 1.2322087287902832 + 100.0 * 6.327089309692383
Epoch 390, val loss: 1.30709707736969
Epoch 400, training loss: 633.4935302734375 = 1.204270839691162 + 100.0 * 6.322892189025879
Epoch 400, val loss: 1.2862658500671387
Epoch 410, training loss: 633.235107421875 = 1.1769524812698364 + 100.0 * 6.320581912994385
Epoch 410, val loss: 1.2662192583084106
Epoch 420, training loss: 632.9991455078125 = 1.1501572132110596 + 100.0 * 6.318490028381348
Epoch 420, val loss: 1.2468031644821167
Epoch 430, training loss: 632.669189453125 = 1.1240918636322021 + 100.0 * 6.315451145172119
Epoch 430, val loss: 1.2281970977783203
Epoch 440, training loss: 632.0911254882812 = 1.0984337329864502 + 100.0 * 6.309926986694336
Epoch 440, val loss: 1.2101771831512451
Epoch 450, training loss: 631.8843383789062 = 1.0735867023468018 + 100.0 * 6.308107376098633
Epoch 450, val loss: 1.1929758787155151
Epoch 460, training loss: 631.4546508789062 = 1.049515724182129 + 100.0 * 6.304051399230957
Epoch 460, val loss: 1.1766518354415894
Epoch 470, training loss: 631.71875 = 1.0262295007705688 + 100.0 * 6.306924819946289
Epoch 470, val loss: 1.1611266136169434
Epoch 480, training loss: 631.2782592773438 = 1.0032202005386353 + 100.0 * 6.302750110626221
Epoch 480, val loss: 1.1459465026855469
Epoch 490, training loss: 630.7176513671875 = 0.9811745285987854 + 100.0 * 6.297364711761475
Epoch 490, val loss: 1.1317291259765625
Epoch 500, training loss: 630.3466796875 = 0.959947943687439 + 100.0 * 6.293867111206055
Epoch 500, val loss: 1.1183809041976929
Epoch 510, training loss: 630.2109375 = 0.9394482374191284 + 100.0 * 6.292714595794678
Epoch 510, val loss: 1.1057883501052856
Epoch 520, training loss: 629.9619750976562 = 0.919442355632782 + 100.0 * 6.2904253005981445
Epoch 520, val loss: 1.0937833786010742
Epoch 530, training loss: 629.8582763671875 = 0.9000972509384155 + 100.0 * 6.289581775665283
Epoch 530, val loss: 1.0824822187423706
Epoch 540, training loss: 629.6487426757812 = 0.8814190626144409 + 100.0 * 6.287673473358154
Epoch 540, val loss: 1.0719494819641113
Epoch 550, training loss: 629.4149169921875 = 0.8633243441581726 + 100.0 * 6.285515785217285
Epoch 550, val loss: 1.0621554851531982
Epoch 560, training loss: 629.2047729492188 = 0.8458431959152222 + 100.0 * 6.2835893630981445
Epoch 560, val loss: 1.0529338121414185
Epoch 570, training loss: 629.0955200195312 = 0.8289223909378052 + 100.0 * 6.282665729522705
Epoch 570, val loss: 1.044458031654358
Epoch 580, training loss: 628.9160766601562 = 0.8124436736106873 + 100.0 * 6.281036376953125
Epoch 580, val loss: 1.036289095878601
Epoch 590, training loss: 628.7439575195312 = 0.7963868379592896 + 100.0 * 6.279475688934326
Epoch 590, val loss: 1.0288152694702148
Epoch 600, training loss: 628.5638427734375 = 0.7807573676109314 + 100.0 * 6.277831077575684
Epoch 600, val loss: 1.0216273069381714
Epoch 610, training loss: 628.2936401367188 = 0.7656643986701965 + 100.0 * 6.275279521942139
Epoch 610, val loss: 1.0151559114456177
Epoch 620, training loss: 628.986083984375 = 0.7509137392044067 + 100.0 * 6.282351970672607
Epoch 620, val loss: 1.0091097354888916
Epoch 630, training loss: 628.152587890625 = 0.7364346981048584 + 100.0 * 6.274161338806152
Epoch 630, val loss: 1.0031414031982422
Epoch 640, training loss: 627.9292602539062 = 0.7222923636436462 + 100.0 * 6.272069931030273
Epoch 640, val loss: 0.9978283643722534
Epoch 650, training loss: 627.7811889648438 = 0.7085189819335938 + 100.0 * 6.270727157592773
Epoch 650, val loss: 0.9928867816925049
Epoch 660, training loss: 628.646240234375 = 0.695040225982666 + 100.0 * 6.279511451721191
Epoch 660, val loss: 0.9881947636604309
Epoch 670, training loss: 627.52490234375 = 0.6815263628959656 + 100.0 * 6.268434047698975
Epoch 670, val loss: 0.9835642576217651
Epoch 680, training loss: 627.4609375 = 0.6684594750404358 + 100.0 * 6.267924785614014
Epoch 680, val loss: 0.9794800877571106
Epoch 690, training loss: 627.214111328125 = 0.6556826233863831 + 100.0 * 6.2655839920043945
Epoch 690, val loss: 0.9756507873535156
Epoch 700, training loss: 627.095947265625 = 0.6430968046188354 + 100.0 * 6.264528751373291
Epoch 700, val loss: 0.9720786809921265
Epoch 710, training loss: 627.5570678710938 = 0.6306936144828796 + 100.0 * 6.269264221191406
Epoch 710, val loss: 0.96853107213974
Epoch 720, training loss: 627.6046752929688 = 0.6182190179824829 + 100.0 * 6.269864559173584
Epoch 720, val loss: 0.9653542041778564
Epoch 730, training loss: 626.9718627929688 = 0.605941653251648 + 100.0 * 6.2636590003967285
Epoch 730, val loss: 0.961970329284668
Epoch 740, training loss: 626.7009887695312 = 0.5939176678657532 + 100.0 * 6.261070251464844
Epoch 740, val loss: 0.9590393900871277
Epoch 750, training loss: 626.5526123046875 = 0.5820738673210144 + 100.0 * 6.259705066680908
Epoch 750, val loss: 0.9561974406242371
Epoch 760, training loss: 626.6908569335938 = 0.5703364610671997 + 100.0 * 6.261205196380615
Epoch 760, val loss: 0.9534584283828735
Epoch 770, training loss: 627.0640258789062 = 0.5586403012275696 + 100.0 * 6.265053749084473
Epoch 770, val loss: 0.9505457878112793
Epoch 780, training loss: 626.27685546875 = 0.5469609498977661 + 100.0 * 6.257298946380615
Epoch 780, val loss: 0.9478983283042908
Epoch 790, training loss: 626.2233276367188 = 0.5355563759803772 + 100.0 * 6.256877422332764
Epoch 790, val loss: 0.9455541968345642
Epoch 800, training loss: 626.0255126953125 = 0.5243293046951294 + 100.0 * 6.255011558532715
Epoch 800, val loss: 0.9432814121246338
Epoch 810, training loss: 625.9811401367188 = 0.5132656693458557 + 100.0 * 6.254679203033447
Epoch 810, val loss: 0.9410656690597534
Epoch 820, training loss: 626.4754638671875 = 0.5022281408309937 + 100.0 * 6.259732246398926
Epoch 820, val loss: 0.9388188123703003
Epoch 830, training loss: 625.8157348632812 = 0.49119338393211365 + 100.0 * 6.2532453536987305
Epoch 830, val loss: 0.9367186427116394
Epoch 840, training loss: 625.7361450195312 = 0.48043784499168396 + 100.0 * 6.252557277679443
Epoch 840, val loss: 0.934874951839447
Epoch 850, training loss: 625.623046875 = 0.46991732716560364 + 100.0 * 6.25153112411499
Epoch 850, val loss: 0.9329808950424194
Epoch 860, training loss: 626.5406494140625 = 0.4595303237438202 + 100.0 * 6.2608113288879395
Epoch 860, val loss: 0.9312198758125305
Epoch 870, training loss: 625.6879272460938 = 0.44907981157302856 + 100.0 * 6.2523884773254395
Epoch 870, val loss: 0.9294369220733643
Epoch 880, training loss: 625.3795166015625 = 0.4390166103839874 + 100.0 * 6.2494049072265625
Epoch 880, val loss: 0.9279116988182068
Epoch 890, training loss: 625.3408203125 = 0.42912980914115906 + 100.0 * 6.249116897583008
Epoch 890, val loss: 0.9265645742416382
Epoch 900, training loss: 625.5689697265625 = 0.41939014196395874 + 100.0 * 6.251495838165283
Epoch 900, val loss: 0.9253119230270386
Epoch 910, training loss: 625.1787719726562 = 0.40984541177749634 + 100.0 * 6.247689247131348
Epoch 910, val loss: 0.9240414500236511
Epoch 920, training loss: 625.0571899414062 = 0.40048086643218994 + 100.0 * 6.246567249298096
Epoch 920, val loss: 0.9229775667190552
Epoch 930, training loss: 625.0809326171875 = 0.3913586437702179 + 100.0 * 6.246895790100098
Epoch 930, val loss: 0.9222541451454163
Epoch 940, training loss: 625.0244140625 = 0.382378488779068 + 100.0 * 6.246420383453369
Epoch 940, val loss: 0.9213705658912659
Epoch 950, training loss: 625.0855712890625 = 0.3735721707344055 + 100.0 * 6.247119903564453
Epoch 950, val loss: 0.9206960797309875
Epoch 960, training loss: 624.9484252929688 = 0.36491188406944275 + 100.0 * 6.245834827423096
Epoch 960, val loss: 0.9202867746353149
Epoch 970, training loss: 624.777587890625 = 0.35644057393074036 + 100.0 * 6.244211196899414
Epoch 970, val loss: 0.9200588464736938
Epoch 980, training loss: 624.6390380859375 = 0.3482311964035034 + 100.0 * 6.242908477783203
Epoch 980, val loss: 0.9199475646018982
Epoch 990, training loss: 624.6483764648438 = 0.3402165472507477 + 100.0 * 6.243081569671631
Epoch 990, val loss: 0.9200301170349121
Epoch 1000, training loss: 624.8788452148438 = 0.3323087692260742 + 100.0 * 6.24546480178833
Epoch 1000, val loss: 0.9203707575798035
Epoch 1010, training loss: 624.7069091796875 = 0.32453492283821106 + 100.0 * 6.243824005126953
Epoch 1010, val loss: 0.9204520583152771
Epoch 1020, training loss: 624.5641479492188 = 0.31696459650993347 + 100.0 * 6.242471694946289
Epoch 1020, val loss: 0.920773446559906
Epoch 1030, training loss: 624.4417724609375 = 0.3095768988132477 + 100.0 * 6.241322040557861
Epoch 1030, val loss: 0.9214035868644714
Epoch 1040, training loss: 624.3121948242188 = 0.30236271023750305 + 100.0 * 6.240097999572754
Epoch 1040, val loss: 0.9220584034919739
Epoch 1050, training loss: 624.2410888671875 = 0.29535743594169617 + 100.0 * 6.239457607269287
Epoch 1050, val loss: 0.922939658164978
Epoch 1060, training loss: 624.52197265625 = 0.2885086238384247 + 100.0 * 6.242334365844727
Epoch 1060, val loss: 0.9237048029899597
Epoch 1070, training loss: 624.3636474609375 = 0.2816655933856964 + 100.0 * 6.240819931030273
Epoch 1070, val loss: 0.9247936010360718
Epoch 1080, training loss: 624.3846435546875 = 0.2750247120857239 + 100.0 * 6.241096019744873
Epoch 1080, val loss: 0.9258924126625061
Epoch 1090, training loss: 623.9547119140625 = 0.2685452997684479 + 100.0 * 6.236861705780029
Epoch 1090, val loss: 0.927264392375946
Epoch 1100, training loss: 623.9489135742188 = 0.26231563091278076 + 100.0 * 6.236865997314453
Epoch 1100, val loss: 0.9287276864051819
Epoch 1110, training loss: 624.4700927734375 = 0.25622323155403137 + 100.0 * 6.242138385772705
Epoch 1110, val loss: 0.9302157759666443
Epoch 1120, training loss: 623.8187255859375 = 0.25009065866470337 + 100.0 * 6.235686779022217
Epoch 1120, val loss: 0.9318245053291321
Epoch 1130, training loss: 623.720947265625 = 0.24420993030071259 + 100.0 * 6.234767436981201
Epoch 1130, val loss: 0.9336100816726685
Epoch 1140, training loss: 623.7323608398438 = 0.2385052591562271 + 100.0 * 6.234938144683838
Epoch 1140, val loss: 0.9354522824287415
Epoch 1150, training loss: 624.0738525390625 = 0.23289108276367188 + 100.0 * 6.238409519195557
Epoch 1150, val loss: 0.937429666519165
Epoch 1160, training loss: 623.9078979492188 = 0.22734656929969788 + 100.0 * 6.236805438995361
Epoch 1160, val loss: 0.9394436478614807
Epoch 1170, training loss: 623.741455078125 = 0.22192631661891937 + 100.0 * 6.235195159912109
Epoch 1170, val loss: 0.9414958953857422
Epoch 1180, training loss: 623.6917724609375 = 0.21668311953544617 + 100.0 * 6.234751224517822
Epoch 1180, val loss: 0.94385826587677
Epoch 1190, training loss: 623.4871826171875 = 0.21154823899269104 + 100.0 * 6.232756614685059
Epoch 1190, val loss: 0.9461822509765625
Epoch 1200, training loss: 623.6949462890625 = 0.20654238760471344 + 100.0 * 6.234884262084961
Epoch 1200, val loss: 0.9487000703811646
Epoch 1210, training loss: 623.4522094726562 = 0.20163902640342712 + 100.0 * 6.2325053215026855
Epoch 1210, val loss: 0.9509685635566711
Epoch 1220, training loss: 623.9718627929688 = 0.196855828166008 + 100.0 * 6.237750053405762
Epoch 1220, val loss: 0.9534496068954468
Epoch 1230, training loss: 623.5238037109375 = 0.19206151366233826 + 100.0 * 6.2333173751831055
Epoch 1230, val loss: 0.9557536840438843
Epoch 1240, training loss: 623.4083251953125 = 0.18750353157520294 + 100.0 * 6.232208251953125
Epoch 1240, val loss: 0.9586708545684814
Epoch 1250, training loss: 623.2943725585938 = 0.18301913142204285 + 100.0 * 6.231113433837891
Epoch 1250, val loss: 0.9613908529281616
Epoch 1260, training loss: 623.1219482421875 = 0.17864760756492615 + 100.0 * 6.229433059692383
Epoch 1260, val loss: 0.9642618894577026
Epoch 1270, training loss: 623.2161254882812 = 0.17441584169864655 + 100.0 * 6.230416774749756
Epoch 1270, val loss: 0.9670912027359009
Epoch 1280, training loss: 623.2613525390625 = 0.17026585340499878 + 100.0 * 6.230910778045654
Epoch 1280, val loss: 0.9699100255966187
Epoch 1290, training loss: 623.5131225585938 = 0.16617360711097717 + 100.0 * 6.233469009399414
Epoch 1290, val loss: 0.9729495644569397
Epoch 1300, training loss: 623.2056274414062 = 0.1621192842721939 + 100.0 * 6.230435371398926
Epoch 1300, val loss: 0.9760317206382751
Epoch 1310, training loss: 623.3772583007812 = 0.15825098752975464 + 100.0 * 6.232189655303955
Epoch 1310, val loss: 0.9789869785308838
Epoch 1320, training loss: 622.8590698242188 = 0.1544295996427536 + 100.0 * 6.227046489715576
Epoch 1320, val loss: 0.9822073578834534
Epoch 1330, training loss: 622.8894653320312 = 0.15075275301933289 + 100.0 * 6.227386951446533
Epoch 1330, val loss: 0.9855009913444519
Epoch 1340, training loss: 622.89013671875 = 0.1471615731716156 + 100.0 * 6.2274298667907715
Epoch 1340, val loss: 0.9887068867683411
Epoch 1350, training loss: 623.3037719726562 = 0.14365354180335999 + 100.0 * 6.231600761413574
Epoch 1350, val loss: 0.9917597770690918
Epoch 1360, training loss: 623.0343627929688 = 0.14010857045650482 + 100.0 * 6.228942394256592
Epoch 1360, val loss: 0.9952664971351624
Epoch 1370, training loss: 622.7039184570312 = 0.13674676418304443 + 100.0 * 6.225672245025635
Epoch 1370, val loss: 0.9984531998634338
Epoch 1380, training loss: 622.6644897460938 = 0.1334558129310608 + 100.0 * 6.225310325622559
Epoch 1380, val loss: 1.0021353960037231
Epoch 1390, training loss: 622.7974243164062 = 0.13028182089328766 + 100.0 * 6.22667121887207
Epoch 1390, val loss: 1.0056854486465454
Epoch 1400, training loss: 622.7739868164062 = 0.12713675200939178 + 100.0 * 6.226468563079834
Epoch 1400, val loss: 1.0090692043304443
Epoch 1410, training loss: 622.8051147460938 = 0.1240733340382576 + 100.0 * 6.226810455322266
Epoch 1410, val loss: 1.0123902559280396
Epoch 1420, training loss: 622.6178588867188 = 0.12106021493673325 + 100.0 * 6.224968433380127
Epoch 1420, val loss: 1.0160889625549316
Epoch 1430, training loss: 622.6348876953125 = 0.11816216260194778 + 100.0 * 6.225167274475098
Epoch 1430, val loss: 1.019707202911377
Epoch 1440, training loss: 622.4852905273438 = 0.11534638702869415 + 100.0 * 6.22369909286499
Epoch 1440, val loss: 1.0232658386230469
Epoch 1450, training loss: 622.6693725585938 = 0.11259914189577103 + 100.0 * 6.225567817687988
Epoch 1450, val loss: 1.02702796459198
Epoch 1460, training loss: 622.433349609375 = 0.1098765954375267 + 100.0 * 6.223235130310059
Epoch 1460, val loss: 1.030569076538086
Epoch 1470, training loss: 622.3688354492188 = 0.10725075751543045 + 100.0 * 6.222615718841553
Epoch 1470, val loss: 1.0344364643096924
Epoch 1480, training loss: 622.3660278320312 = 0.10471300035715103 + 100.0 * 6.2226128578186035
Epoch 1480, val loss: 1.0383539199829102
Epoch 1490, training loss: 622.5167236328125 = 0.1022203341126442 + 100.0 * 6.22414493560791
Epoch 1490, val loss: 1.0420030355453491
Epoch 1500, training loss: 622.4810791015625 = 0.09977570921182632 + 100.0 * 6.223813056945801
Epoch 1500, val loss: 1.0455548763275146
Epoch 1510, training loss: 622.4987182617188 = 0.09740512818098068 + 100.0 * 6.224013328552246
Epoch 1510, val loss: 1.0494740009307861
Epoch 1520, training loss: 622.197998046875 = 0.09507937729358673 + 100.0 * 6.221028804779053
Epoch 1520, val loss: 1.0530149936676025
Epoch 1530, training loss: 622.6515502929688 = 0.09285605698823929 + 100.0 * 6.225587368011475
Epoch 1530, val loss: 1.0569970607757568
Epoch 1540, training loss: 622.093017578125 = 0.09063392132520676 + 100.0 * 6.2200236320495605
Epoch 1540, val loss: 1.0606207847595215
Epoch 1550, training loss: 622.0469970703125 = 0.08850420266389847 + 100.0 * 6.219584941864014
Epoch 1550, val loss: 1.0645841360092163
Epoch 1560, training loss: 622.0169677734375 = 0.08646967262029648 + 100.0 * 6.21930456161499
Epoch 1560, val loss: 1.0687299966812134
Epoch 1570, training loss: 622.0982055664062 = 0.08447958528995514 + 100.0 * 6.220137596130371
Epoch 1570, val loss: 1.072892665863037
Epoch 1580, training loss: 622.2876586914062 = 0.08251073956489563 + 100.0 * 6.222051620483398
Epoch 1580, val loss: 1.0765867233276367
Epoch 1590, training loss: 621.9652099609375 = 0.08058095723390579 + 100.0 * 6.218846321105957
Epoch 1590, val loss: 1.080326795578003
Epoch 1600, training loss: 621.9033813476562 = 0.07872489094734192 + 100.0 * 6.2182464599609375
Epoch 1600, val loss: 1.0843322277069092
Epoch 1610, training loss: 622.0795288085938 = 0.07693468034267426 + 100.0 * 6.22002649307251
Epoch 1610, val loss: 1.0882428884506226
Epoch 1620, training loss: 621.865234375 = 0.07518886774778366 + 100.0 * 6.217900276184082
Epoch 1620, val loss: 1.0922456979751587
Epoch 1630, training loss: 622.3943481445312 = 0.0734877660870552 + 100.0 * 6.223208427429199
Epoch 1630, val loss: 1.0961519479751587
Epoch 1640, training loss: 621.8662109375 = 0.07180722802877426 + 100.0 * 6.217944145202637
Epoch 1640, val loss: 1.099973201751709
Epoch 1650, training loss: 621.8560180664062 = 0.07020935416221619 + 100.0 * 6.21785831451416
Epoch 1650, val loss: 1.1041181087493896
Epoch 1660, training loss: 621.9605712890625 = 0.06864530593156815 + 100.0 * 6.218919277191162
Epoch 1660, val loss: 1.1081161499023438
Epoch 1670, training loss: 621.740478515625 = 0.06712596863508224 + 100.0 * 6.216733455657959
Epoch 1670, val loss: 1.1119893789291382
Epoch 1680, training loss: 621.7378540039062 = 0.06565595418214798 + 100.0 * 6.216721534729004
Epoch 1680, val loss: 1.116024136543274
Epoch 1690, training loss: 622.0589599609375 = 0.06422506272792816 + 100.0 * 6.219947338104248
Epoch 1690, val loss: 1.119889736175537
Epoch 1700, training loss: 621.7498168945312 = 0.06280409544706345 + 100.0 * 6.216869831085205
Epoch 1700, val loss: 1.1238054037094116
Epoch 1710, training loss: 621.60595703125 = 0.06144508346915245 + 100.0 * 6.215445041656494
Epoch 1710, val loss: 1.1277474164962769
Epoch 1720, training loss: 621.7232055664062 = 0.06014338508248329 + 100.0 * 6.216630935668945
Epoch 1720, val loss: 1.1317492723464966
Epoch 1730, training loss: 622.1529541015625 = 0.05884971842169762 + 100.0 * 6.220941066741943
Epoch 1730, val loss: 1.1355984210968018
Epoch 1740, training loss: 621.636474609375 = 0.05754693225026131 + 100.0 * 6.215789318084717
Epoch 1740, val loss: 1.1392079591751099
Epoch 1750, training loss: 621.4833374023438 = 0.05633226037025452 + 100.0 * 6.214270114898682
Epoch 1750, val loss: 1.143363118171692
Epoch 1760, training loss: 621.4229125976562 = 0.055159054696559906 + 100.0 * 6.213677406311035
Epoch 1760, val loss: 1.1473547220230103
Epoch 1770, training loss: 621.420654296875 = 0.05402630940079689 + 100.0 * 6.2136664390563965
Epoch 1770, val loss: 1.1512988805770874
Epoch 1780, training loss: 622.5944213867188 = 0.05291568487882614 + 100.0 * 6.225414752960205
Epoch 1780, val loss: 1.1555413007736206
Epoch 1790, training loss: 621.9546508789062 = 0.051810383796691895 + 100.0 * 6.219028472900391
Epoch 1790, val loss: 1.158654808998108
Epoch 1800, training loss: 621.2988891601562 = 0.05070139467716217 + 100.0 * 6.212481498718262
Epoch 1800, val loss: 1.1624670028686523
Epoch 1810, training loss: 621.3485107421875 = 0.04967258870601654 + 100.0 * 6.212988376617432
Epoch 1810, val loss: 1.1663134098052979
Epoch 1820, training loss: 621.2661743164062 = 0.04869125038385391 + 100.0 * 6.212174892425537
Epoch 1820, val loss: 1.1700830459594727
Epoch 1830, training loss: 621.305908203125 = 0.04773075506091118 + 100.0 * 6.212581634521484
Epoch 1830, val loss: 1.173869252204895
Epoch 1840, training loss: 622.0250854492188 = 0.046799760311841965 + 100.0 * 6.219782829284668
Epoch 1840, val loss: 1.1774650812149048
Epoch 1850, training loss: 621.5803833007812 = 0.04583435878157616 + 100.0 * 6.21534538269043
Epoch 1850, val loss: 1.1811689138412476
Epoch 1860, training loss: 621.26171875 = 0.0449042022228241 + 100.0 * 6.212167739868164
Epoch 1860, val loss: 1.1846307516098022
Epoch 1870, training loss: 621.1857299804688 = 0.04403650388121605 + 100.0 * 6.211417198181152
Epoch 1870, val loss: 1.1885396242141724
Epoch 1880, training loss: 621.2640380859375 = 0.04319794103503227 + 100.0 * 6.212208271026611
Epoch 1880, val loss: 1.1922537088394165
Epoch 1890, training loss: 621.7229614257812 = 0.042360566556453705 + 100.0 * 6.216805934906006
Epoch 1890, val loss: 1.195798397064209
Epoch 1900, training loss: 621.236328125 = 0.04153258353471756 + 100.0 * 6.211948394775391
Epoch 1900, val loss: 1.1989589929580688
Epoch 1910, training loss: 621.082275390625 = 0.04074002429842949 + 100.0 * 6.210415363311768
Epoch 1910, val loss: 1.2027803659439087
Epoch 1920, training loss: 621.19677734375 = 0.03998443856835365 + 100.0 * 6.2115678787231445
Epoch 1920, val loss: 1.206286072731018
Epoch 1930, training loss: 621.6635131835938 = 0.039226364344358444 + 100.0 * 6.216242790222168
Epoch 1930, val loss: 1.209699034690857
Epoch 1940, training loss: 621.1882934570312 = 0.03848341479897499 + 100.0 * 6.211497783660889
Epoch 1940, val loss: 1.2128825187683105
Epoch 1950, training loss: 621.0433349609375 = 0.03776780888438225 + 100.0 * 6.210055351257324
Epoch 1950, val loss: 1.216557264328003
Epoch 1960, training loss: 620.999755859375 = 0.037088364362716675 + 100.0 * 6.209626197814941
Epoch 1960, val loss: 1.2200325727462769
Epoch 1970, training loss: 621.1819458007812 = 0.03643089160323143 + 100.0 * 6.211455345153809
Epoch 1970, val loss: 1.2234420776367188
Epoch 1980, training loss: 621.1087036132812 = 0.03576246276497841 + 100.0 * 6.210729598999023
Epoch 1980, val loss: 1.2267683744430542
Epoch 1990, training loss: 621.1535034179688 = 0.035106632858514786 + 100.0 * 6.211184024810791
Epoch 1990, val loss: 1.2300572395324707
Epoch 2000, training loss: 621.2885131835938 = 0.034481048583984375 + 100.0 * 6.212540149688721
Epoch 2000, val loss: 1.233346939086914
Epoch 2010, training loss: 621.0161743164062 = 0.03386679291725159 + 100.0 * 6.209823131561279
Epoch 2010, val loss: 1.2367032766342163
Epoch 2020, training loss: 620.8856811523438 = 0.03327297791838646 + 100.0 * 6.208524227142334
Epoch 2020, val loss: 1.239965796470642
Epoch 2030, training loss: 620.9234619140625 = 0.03269907832145691 + 100.0 * 6.208907604217529
Epoch 2030, val loss: 1.2431256771087646
Epoch 2040, training loss: 621.5824584960938 = 0.03213759884238243 + 100.0 * 6.215502738952637
Epoch 2040, val loss: 1.246290922164917
Epoch 2050, training loss: 621.1982421875 = 0.03156997635960579 + 100.0 * 6.211667060852051
Epoch 2050, val loss: 1.2495578527450562
Epoch 2060, training loss: 620.9380493164062 = 0.03101612627506256 + 100.0 * 6.209070682525635
Epoch 2060, val loss: 1.2524665594100952
Epoch 2070, training loss: 620.826416015625 = 0.03049578331410885 + 100.0 * 6.207959175109863
Epoch 2070, val loss: 1.2558501958847046
Epoch 2080, training loss: 620.7732543945312 = 0.029986785724759102 + 100.0 * 6.207432746887207
Epoch 2080, val loss: 1.2589741945266724
Epoch 2090, training loss: 621.0161743164062 = 0.02949281595647335 + 100.0 * 6.209867000579834
Epoch 2090, val loss: 1.2621679306030273
Epoch 2100, training loss: 621.0155639648438 = 0.028997566550970078 + 100.0 * 6.209865570068359
Epoch 2100, val loss: 1.2649978399276733
Epoch 2110, training loss: 620.7562255859375 = 0.028517410159111023 + 100.0 * 6.207277297973633
Epoch 2110, val loss: 1.267740249633789
Epoch 2120, training loss: 620.7759399414062 = 0.028045758605003357 + 100.0 * 6.2074785232543945
Epoch 2120, val loss: 1.2707213163375854
Epoch 2130, training loss: 620.861572265625 = 0.027595307677984238 + 100.0 * 6.208339691162109
Epoch 2130, val loss: 1.2738416194915771
Epoch 2140, training loss: 620.7272338867188 = 0.027147503569722176 + 100.0 * 6.207000732421875
Epoch 2140, val loss: 1.276636004447937
Epoch 2150, training loss: 620.7410888671875 = 0.026718836277723312 + 100.0 * 6.207143783569336
Epoch 2150, val loss: 1.2796884775161743
Epoch 2160, training loss: 620.964111328125 = 0.026300763711333275 + 100.0 * 6.209378242492676
Epoch 2160, val loss: 1.28233802318573
Epoch 2170, training loss: 620.9859008789062 = 0.02588672563433647 + 100.0 * 6.20959997177124
Epoch 2170, val loss: 1.2850620746612549
Epoch 2180, training loss: 620.8435668945312 = 0.02546842023730278 + 100.0 * 6.208180904388428
Epoch 2180, val loss: 1.2881348133087158
Epoch 2190, training loss: 620.8145141601562 = 0.025066502392292023 + 100.0 * 6.207894325256348
Epoch 2190, val loss: 1.2906936407089233
Epoch 2200, training loss: 620.6500244140625 = 0.024671630933880806 + 100.0 * 6.206253528594971
Epoch 2200, val loss: 1.2934378385543823
Epoch 2210, training loss: 620.9046020507812 = 0.02430134266614914 + 100.0 * 6.208803176879883
Epoch 2210, val loss: 1.296146273612976
Epoch 2220, training loss: 620.50830078125 = 0.023922739550471306 + 100.0 * 6.204843521118164
Epoch 2220, val loss: 1.299031138420105
Epoch 2230, training loss: 620.5236206054688 = 0.023563150316476822 + 100.0 * 6.205000877380371
Epoch 2230, val loss: 1.3017051219940186
Epoch 2240, training loss: 620.5142822265625 = 0.02321588061749935 + 100.0 * 6.204910755157471
Epoch 2240, val loss: 1.3044512271881104
Epoch 2250, training loss: 621.2252807617188 = 0.022874653339385986 + 100.0 * 6.212023735046387
Epoch 2250, val loss: 1.3073164224624634
Epoch 2260, training loss: 620.8587646484375 = 0.022531218826770782 + 100.0 * 6.208362579345703
Epoch 2260, val loss: 1.308974027633667
Epoch 2270, training loss: 620.5806274414062 = 0.022176668047904968 + 100.0 * 6.205584526062012
Epoch 2270, val loss: 1.3119577169418335
Epoch 2280, training loss: 620.50830078125 = 0.021862396970391273 + 100.0 * 6.204864501953125
Epoch 2280, val loss: 1.3143833875656128
Epoch 2290, training loss: 620.412353515625 = 0.02154667116701603 + 100.0 * 6.2039079666137695
Epoch 2290, val loss: 1.317212700843811
Epoch 2300, training loss: 620.40576171875 = 0.02124960534274578 + 100.0 * 6.203845500946045
Epoch 2300, val loss: 1.3196535110473633
Epoch 2310, training loss: 621.205078125 = 0.020962875336408615 + 100.0 * 6.211841583251953
Epoch 2310, val loss: 1.3224077224731445
Epoch 2320, training loss: 620.4515380859375 = 0.02064068801701069 + 100.0 * 6.204308986663818
Epoch 2320, val loss: 1.3241612911224365
Epoch 2330, training loss: 620.48291015625 = 0.02034693770110607 + 100.0 * 6.204625606536865
Epoch 2330, val loss: 1.3271781206130981
Epoch 2340, training loss: 620.5496826171875 = 0.020064322277903557 + 100.0 * 6.205296039581299
Epoch 2340, val loss: 1.3293559551239014
Epoch 2350, training loss: 620.3328857421875 = 0.019782448187470436 + 100.0 * 6.203131198883057
Epoch 2350, val loss: 1.3318469524383545
Epoch 2360, training loss: 620.3524780273438 = 0.019518736749887466 + 100.0 * 6.203329563140869
Epoch 2360, val loss: 1.3340610265731812
Epoch 2370, training loss: 620.4703369140625 = 0.019259842112660408 + 100.0 * 6.20451021194458
Epoch 2370, val loss: 1.3364514112472534
Epoch 2380, training loss: 620.951904296875 = 0.0190009493380785 + 100.0 * 6.209329128265381
Epoch 2380, val loss: 1.3387202024459839
Epoch 2390, training loss: 620.3903198242188 = 0.018723158165812492 + 100.0 * 6.203715801239014
Epoch 2390, val loss: 1.3409652709960938
Epoch 2400, training loss: 620.264892578125 = 0.01847044751048088 + 100.0 * 6.2024641036987305
Epoch 2400, val loss: 1.3432374000549316
Epoch 2410, training loss: 620.3910522460938 = 0.01822931505739689 + 100.0 * 6.203727722167969
Epoch 2410, val loss: 1.3456614017486572
Epoch 2420, training loss: 620.5184936523438 = 0.017992541193962097 + 100.0 * 6.205005168914795
Epoch 2420, val loss: 1.3474842309951782
Epoch 2430, training loss: 620.318359375 = 0.017753733322024345 + 100.0 * 6.203005790710449
Epoch 2430, val loss: 1.34980309009552
Epoch 2440, training loss: 620.195556640625 = 0.01752178743481636 + 100.0 * 6.201780319213867
Epoch 2440, val loss: 1.3519890308380127
Epoch 2450, training loss: 620.3224487304688 = 0.017302479594945908 + 100.0 * 6.203052043914795
Epoch 2450, val loss: 1.3540990352630615
Epoch 2460, training loss: 620.5579223632812 = 0.017082640901207924 + 100.0 * 6.205408096313477
Epoch 2460, val loss: 1.3561229705810547
Epoch 2470, training loss: 620.3626708984375 = 0.01686142571270466 + 100.0 * 6.203458309173584
Epoch 2470, val loss: 1.358074426651001
Epoch 2480, training loss: 620.3180541992188 = 0.01664023846387863 + 100.0 * 6.203013896942139
Epoch 2480, val loss: 1.360485553741455
Epoch 2490, training loss: 620.463623046875 = 0.0164309274405241 + 100.0 * 6.204472064971924
Epoch 2490, val loss: 1.3626083135604858
Epoch 2500, training loss: 620.2940063476562 = 0.01622185669839382 + 100.0 * 6.202777862548828
Epoch 2500, val loss: 1.3644649982452393
Epoch 2510, training loss: 620.1936645507812 = 0.016025718301534653 + 100.0 * 6.201776027679443
Epoch 2510, val loss: 1.366184115409851
Epoch 2520, training loss: 620.100830078125 = 0.015830785036087036 + 100.0 * 6.200850009918213
Epoch 2520, val loss: 1.3685129880905151
Epoch 2530, training loss: 620.3162841796875 = 0.01564321294426918 + 100.0 * 6.203006744384766
Epoch 2530, val loss: 1.3706488609313965
Epoch 2540, training loss: 620.4281616210938 = 0.015455435961484909 + 100.0 * 6.204127311706543
Epoch 2540, val loss: 1.3721551895141602
Epoch 2550, training loss: 620.1290893554688 = 0.015259314328432083 + 100.0 * 6.201138496398926
Epoch 2550, val loss: 1.3739734888076782
Epoch 2560, training loss: 620.1011962890625 = 0.01507311686873436 + 100.0 * 6.20086145401001
Epoch 2560, val loss: 1.3759729862213135
Epoch 2570, training loss: 620.0410766601562 = 0.014896692708134651 + 100.0 * 6.200262069702148
Epoch 2570, val loss: 1.3779655694961548
Epoch 2580, training loss: 620.4669189453125 = 0.014727188274264336 + 100.0 * 6.204522132873535
Epoch 2580, val loss: 1.380086064338684
Epoch 2590, training loss: 620.256591796875 = 0.014546822756528854 + 100.0 * 6.202420711517334
Epoch 2590, val loss: 1.381285548210144
Epoch 2600, training loss: 620.0640869140625 = 0.014372192323207855 + 100.0 * 6.200497150421143
Epoch 2600, val loss: 1.3831913471221924
Epoch 2610, training loss: 620.0003051757812 = 0.014206117950379848 + 100.0 * 6.199860572814941
Epoch 2610, val loss: 1.3849513530731201
Epoch 2620, training loss: 619.9691162109375 = 0.014046452939510345 + 100.0 * 6.199550628662109
Epoch 2620, val loss: 1.3867661952972412
Epoch 2630, training loss: 620.0535888671875 = 0.01389389205724001 + 100.0 * 6.20039701461792
Epoch 2630, val loss: 1.388690710067749
Epoch 2640, training loss: 620.6036376953125 = 0.013745470903813839 + 100.0 * 6.205898761749268
Epoch 2640, val loss: 1.3902291059494019
Epoch 2650, training loss: 620.4183959960938 = 0.013586911372840405 + 100.0 * 6.204048156738281
Epoch 2650, val loss: 1.3917555809020996
Epoch 2660, training loss: 620.0687255859375 = 0.013421659357845783 + 100.0 * 6.200552940368652
Epoch 2660, val loss: 1.393449068069458
Epoch 2670, training loss: 619.9558715820312 = 0.013275396078824997 + 100.0 * 6.19942569732666
Epoch 2670, val loss: 1.3953889608383179
Epoch 2680, training loss: 620.0297241210938 = 0.013133286498486996 + 100.0 * 6.20016622543335
Epoch 2680, val loss: 1.3971437215805054
Epoch 2690, training loss: 620.4913940429688 = 0.012993598356842995 + 100.0 * 6.204783916473389
Epoch 2690, val loss: 1.398840069770813
Epoch 2700, training loss: 620.2237548828125 = 0.012847373262047768 + 100.0 * 6.202109336853027
Epoch 2700, val loss: 1.3999866247177124
Epoch 2710, training loss: 619.975830078125 = 0.012706564739346504 + 100.0 * 6.199631214141846
Epoch 2710, val loss: 1.4017601013183594
Epoch 2720, training loss: 619.9444580078125 = 0.012574437074363232 + 100.0 * 6.199318885803223
Epoch 2720, val loss: 1.4031833410263062
Epoch 2730, training loss: 620.5069580078125 = 0.012443045154213905 + 100.0 * 6.204945087432861
Epoch 2730, val loss: 1.4046683311462402
Epoch 2740, training loss: 619.873046875 = 0.012308800593018532 + 100.0 * 6.198606967926025
Epoch 2740, val loss: 1.4062237739562988
Epoch 2750, training loss: 619.9775390625 = 0.012178455479443073 + 100.0 * 6.199653625488281
Epoch 2750, val loss: 1.4079688787460327
Epoch 2760, training loss: 620.08203125 = 0.012052551843225956 + 100.0 * 6.200700283050537
Epoch 2760, val loss: 1.40923273563385
Epoch 2770, training loss: 619.8621215820312 = 0.011933502741158009 + 100.0 * 6.1985015869140625
Epoch 2770, val loss: 1.4107944965362549
Epoch 2780, training loss: 620.1685791015625 = 0.01181966345757246 + 100.0 * 6.20156717300415
Epoch 2780, val loss: 1.4119508266448975
Epoch 2790, training loss: 619.7769165039062 = 0.01168979424983263 + 100.0 * 6.1976518630981445
Epoch 2790, val loss: 1.4135600328445435
Epoch 2800, training loss: 619.8817138671875 = 0.011571681126952171 + 100.0 * 6.19870138168335
Epoch 2800, val loss: 1.4152271747589111
Epoch 2810, training loss: 619.8861694335938 = 0.011462554335594177 + 100.0 * 6.198747158050537
Epoch 2810, val loss: 1.4164096117019653
Epoch 2820, training loss: 620.121826171875 = 0.011351589113473892 + 100.0 * 6.201104640960693
Epoch 2820, val loss: 1.4178496599197388
Epoch 2830, training loss: 620.0721435546875 = 0.011235769838094711 + 100.0 * 6.20060920715332
Epoch 2830, val loss: 1.4192227125167847
Epoch 2840, training loss: 619.869384765625 = 0.011120108887553215 + 100.0 * 6.198582649230957
Epoch 2840, val loss: 1.4204018115997314
Epoch 2850, training loss: 620.0382690429688 = 0.011011761613190174 + 100.0 * 6.200272560119629
Epoch 2850, val loss: 1.4221038818359375
Epoch 2860, training loss: 619.9635009765625 = 0.010905487462878227 + 100.0 * 6.199525833129883
Epoch 2860, val loss: 1.423058271408081
Epoch 2870, training loss: 619.736083984375 = 0.010800748132169247 + 100.0 * 6.1972527503967285
Epoch 2870, val loss: 1.4246876239776611
Epoch 2880, training loss: 619.754150390625 = 0.010699973441660404 + 100.0 * 6.197434902191162
Epoch 2880, val loss: 1.425973892211914
Epoch 2890, training loss: 620.046630859375 = 0.010601299814879894 + 100.0 * 6.20035982131958
Epoch 2890, val loss: 1.4274481534957886
Epoch 2900, training loss: 619.8018188476562 = 0.010497292503714561 + 100.0 * 6.19791316986084
Epoch 2900, val loss: 1.4287737607955933
Epoch 2910, training loss: 619.9834594726562 = 0.010401754640042782 + 100.0 * 6.19973087310791
Epoch 2910, val loss: 1.4297432899475098
Epoch 2920, training loss: 619.7451782226562 = 0.010300036519765854 + 100.0 * 6.197348594665527
Epoch 2920, val loss: 1.4310113191604614
Epoch 2930, training loss: 619.6564331054688 = 0.010204514488577843 + 100.0 * 6.196462154388428
Epoch 2930, val loss: 1.4321067333221436
Epoch 2940, training loss: 619.7896728515625 = 0.010114872828125954 + 100.0 * 6.197795391082764
Epoch 2940, val loss: 1.4335030317306519
Epoch 2950, training loss: 620.0140380859375 = 0.010023965500295162 + 100.0 * 6.200040340423584
Epoch 2950, val loss: 1.4346956014633179
Epoch 2960, training loss: 619.7362670898438 = 0.00992592889815569 + 100.0 * 6.197263240814209
Epoch 2960, val loss: 1.4358488321304321
Epoch 2970, training loss: 619.6421508789062 = 0.009837135672569275 + 100.0 * 6.196323394775391
Epoch 2970, val loss: 1.4369868040084839
Epoch 2980, training loss: 619.7982788085938 = 0.009755349718034267 + 100.0 * 6.197885036468506
Epoch 2980, val loss: 1.4383026361465454
Epoch 2990, training loss: 620.0542602539062 = 0.009670334868133068 + 100.0 * 6.200446128845215
Epoch 2990, val loss: 1.4392794370651245
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8355297838692674
The final CL Acc:0.75679, 0.01720, The final GNN Acc:0.83834, 0.00326
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10572])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6154174804688 = 1.930547833442688 + 100.0 * 8.596848487854004
Epoch 0, val loss: 1.9217047691345215
Epoch 10, training loss: 861.5328979492188 = 1.9224131107330322 + 100.0 * 8.596104621887207
Epoch 10, val loss: 1.9131512641906738
Epoch 20, training loss: 860.9730224609375 = 1.9119958877563477 + 100.0 * 8.59061050415039
Epoch 20, val loss: 1.9020731449127197
Epoch 30, training loss: 857.1349487304688 = 1.8981170654296875 + 100.0 * 8.5523681640625
Epoch 30, val loss: 1.887412667274475
Epoch 40, training loss: 834.3649291992188 = 1.881474256515503 + 100.0 * 8.324834823608398
Epoch 40, val loss: 1.8706896305084229
Epoch 50, training loss: 763.3397216796875 = 1.8641873598098755 + 100.0 * 7.614755153656006
Epoch 50, val loss: 1.854329228401184
Epoch 60, training loss: 728.3175048828125 = 1.8534042835235596 + 100.0 * 7.264641284942627
Epoch 60, val loss: 1.8446180820465088
Epoch 70, training loss: 702.5585327148438 = 1.8449589014053345 + 100.0 * 7.00713586807251
Epoch 70, val loss: 1.8369213342666626
Epoch 80, training loss: 688.3096923828125 = 1.8368842601776123 + 100.0 * 6.864727973937988
Epoch 80, val loss: 1.8294390439987183
Epoch 90, training loss: 679.1347045898438 = 1.8282573223114014 + 100.0 * 6.773064613342285
Epoch 90, val loss: 1.8214765787124634
Epoch 100, training loss: 670.8700561523438 = 1.8212612867355347 + 100.0 * 6.690487384796143
Epoch 100, val loss: 1.8149676322937012
Epoch 110, training loss: 665.392822265625 = 1.8151942491531372 + 100.0 * 6.635776519775391
Epoch 110, val loss: 1.8095096349716187
Epoch 120, training loss: 661.2431640625 = 1.809287190437317 + 100.0 * 6.594338893890381
Epoch 120, val loss: 1.804486870765686
Epoch 130, training loss: 657.9391479492188 = 1.8037548065185547 + 100.0 * 6.56135368347168
Epoch 130, val loss: 1.7997729778289795
Epoch 140, training loss: 655.0573120117188 = 1.7981369495391846 + 100.0 * 6.532591342926025
Epoch 140, val loss: 1.7948285341262817
Epoch 150, training loss: 652.6755981445312 = 1.792214035987854 + 100.0 * 6.508833885192871
Epoch 150, val loss: 1.7896602153778076
Epoch 160, training loss: 650.619140625 = 1.7861651182174683 + 100.0 * 6.4883294105529785
Epoch 160, val loss: 1.7844303846359253
Epoch 170, training loss: 649.0584716796875 = 1.7796992063522339 + 100.0 * 6.472787380218506
Epoch 170, val loss: 1.7788680791854858
Epoch 180, training loss: 647.5504760742188 = 1.7725728750228882 + 100.0 * 6.4577789306640625
Epoch 180, val loss: 1.7727410793304443
Epoch 190, training loss: 646.3277587890625 = 1.7648122310638428 + 100.0 * 6.445629119873047
Epoch 190, val loss: 1.7661293745040894
Epoch 200, training loss: 645.2669067382812 = 1.7564150094985962 + 100.0 * 6.435104846954346
Epoch 200, val loss: 1.7589292526245117
Epoch 210, training loss: 644.4827880859375 = 1.7471942901611328 + 100.0 * 6.427356243133545
Epoch 210, val loss: 1.7510740756988525
Epoch 220, training loss: 643.7525024414062 = 1.737106442451477 + 100.0 * 6.420154094696045
Epoch 220, val loss: 1.7426743507385254
Epoch 230, training loss: 642.3719482421875 = 1.7261146306991577 + 100.0 * 6.406458377838135
Epoch 230, val loss: 1.733319878578186
Epoch 240, training loss: 641.4509887695312 = 1.7142683267593384 + 100.0 * 6.397367000579834
Epoch 240, val loss: 1.7234541177749634
Epoch 250, training loss: 640.6258544921875 = 1.7014906406402588 + 100.0 * 6.3892436027526855
Epoch 250, val loss: 1.7128444910049438
Epoch 260, training loss: 639.9029541015625 = 1.6876952648162842 + 100.0 * 6.382152557373047
Epoch 260, val loss: 1.7014878988265991
Epoch 270, training loss: 639.1749267578125 = 1.6725813150405884 + 100.0 * 6.375022888183594
Epoch 270, val loss: 1.6891580820083618
Epoch 280, training loss: 638.5281372070312 = 1.656493067741394 + 100.0 * 6.368716239929199
Epoch 280, val loss: 1.676162838935852
Epoch 290, training loss: 637.7389526367188 = 1.6393131017684937 + 100.0 * 6.360996246337891
Epoch 290, val loss: 1.6624938249588013
Epoch 300, training loss: 638.0529174804688 = 1.621142864227295 + 100.0 * 6.364317417144775
Epoch 300, val loss: 1.6479848623275757
Epoch 310, training loss: 636.7499389648438 = 1.601609230041504 + 100.0 * 6.35148286819458
Epoch 310, val loss: 1.6327344179153442
Epoch 320, training loss: 636.091064453125 = 1.581397533416748 + 100.0 * 6.345096588134766
Epoch 320, val loss: 1.6173115968704224
Epoch 330, training loss: 635.5345458984375 = 1.560430884361267 + 100.0 * 6.339741230010986
Epoch 330, val loss: 1.6015552282333374
Epoch 340, training loss: 635.03076171875 = 1.538859486579895 + 100.0 * 6.334918975830078
Epoch 340, val loss: 1.5856215953826904
Epoch 350, training loss: 634.6566772460938 = 1.516734004020691 + 100.0 * 6.331399440765381
Epoch 350, val loss: 1.5695918798446655
Epoch 360, training loss: 634.6597900390625 = 1.493895173072815 + 100.0 * 6.331658840179443
Epoch 360, val loss: 1.5533483028411865
Epoch 370, training loss: 633.9074096679688 = 1.4709964990615845 + 100.0 * 6.324364185333252
Epoch 370, val loss: 1.537423849105835
Epoch 380, training loss: 633.396728515625 = 1.447921872138977 + 100.0 * 6.319488048553467
Epoch 380, val loss: 1.5217775106430054
Epoch 390, training loss: 633.1873168945312 = 1.4247779846191406 + 100.0 * 6.317625045776367
Epoch 390, val loss: 1.5063562393188477
Epoch 400, training loss: 632.8258666992188 = 1.4015368223190308 + 100.0 * 6.314243316650391
Epoch 400, val loss: 1.4913320541381836
Epoch 410, training loss: 632.3622436523438 = 1.3785971403121948 + 100.0 * 6.309836387634277
Epoch 410, val loss: 1.4768164157867432
Epoch 420, training loss: 631.9988403320312 = 1.3558844327926636 + 100.0 * 6.306429386138916
Epoch 420, val loss: 1.462746262550354
Epoch 430, training loss: 632.0130004882812 = 1.3334097862243652 + 100.0 * 6.306795597076416
Epoch 430, val loss: 1.4490604400634766
Epoch 440, training loss: 631.4794921875 = 1.3110597133636475 + 100.0 * 6.301684856414795
Epoch 440, val loss: 1.4356505870819092
Epoch 450, training loss: 631.31298828125 = 1.289132833480835 + 100.0 * 6.300238609313965
Epoch 450, val loss: 1.4228256940841675
Epoch 460, training loss: 630.8729248046875 = 1.267591953277588 + 100.0 * 6.296053409576416
Epoch 460, val loss: 1.4103914499282837
Epoch 470, training loss: 630.7540283203125 = 1.2464781999588013 + 100.0 * 6.295075416564941
Epoch 470, val loss: 1.398419737815857
Epoch 480, training loss: 630.5667724609375 = 1.225671648979187 + 100.0 * 6.2934112548828125
Epoch 480, val loss: 1.386476755142212
Epoch 490, training loss: 630.3416137695312 = 1.2052582502365112 + 100.0 * 6.291363716125488
Epoch 490, val loss: 1.3752106428146362
Epoch 500, training loss: 630.02392578125 = 1.1855316162109375 + 100.0 * 6.288383483886719
Epoch 500, val loss: 1.3643724918365479
Epoch 510, training loss: 629.7534790039062 = 1.166276216506958 + 100.0 * 6.285872459411621
Epoch 510, val loss: 1.3540345430374146
Epoch 520, training loss: 629.6416015625 = 1.1474871635437012 + 100.0 * 6.28494119644165
Epoch 520, val loss: 1.343977928161621
Epoch 530, training loss: 629.5634765625 = 1.129016399383545 + 100.0 * 6.28434419631958
Epoch 530, val loss: 1.334031581878662
Epoch 540, training loss: 629.3380737304688 = 1.1107981204986572 + 100.0 * 6.282272815704346
Epoch 540, val loss: 1.3245739936828613
Epoch 550, training loss: 629.26904296875 = 1.0931422710418701 + 100.0 * 6.281759262084961
Epoch 550, val loss: 1.3153597116470337
Epoch 560, training loss: 628.84228515625 = 1.076096534729004 + 100.0 * 6.2776618003845215
Epoch 560, val loss: 1.3065905570983887
Epoch 570, training loss: 628.6611938476562 = 1.05939781665802 + 100.0 * 6.276018142700195
Epoch 570, val loss: 1.2981207370758057
Epoch 580, training loss: 628.695068359375 = 1.043189525604248 + 100.0 * 6.27651834487915
Epoch 580, val loss: 1.2899131774902344
Epoch 590, training loss: 628.6976928710938 = 1.026938557624817 + 100.0 * 6.276707649230957
Epoch 590, val loss: 1.2819324731826782
Epoch 600, training loss: 628.177734375 = 1.0112065076828003 + 100.0 * 6.271665096282959
Epoch 600, val loss: 1.2740429639816284
Epoch 610, training loss: 628.0694580078125 = 0.9959104061126709 + 100.0 * 6.270735740661621
Epoch 610, val loss: 1.26670241355896
Epoch 620, training loss: 627.9401245117188 = 0.9810133576393127 + 100.0 * 6.269590854644775
Epoch 620, val loss: 1.2596156597137451
Epoch 630, training loss: 628.3634033203125 = 0.9663094878196716 + 100.0 * 6.273971080780029
Epoch 630, val loss: 1.2527889013290405
Epoch 640, training loss: 627.6887817382812 = 0.9517803192138672 + 100.0 * 6.267370223999023
Epoch 640, val loss: 1.245991826057434
Epoch 650, training loss: 627.6487426757812 = 0.9375282526016235 + 100.0 * 6.2671122550964355
Epoch 650, val loss: 1.2396255731582642
Epoch 660, training loss: 627.3902587890625 = 0.9236119389533997 + 100.0 * 6.26466703414917
Epoch 660, val loss: 1.2333396673202515
Epoch 670, training loss: 627.9622192382812 = 0.9097734093666077 + 100.0 * 6.270524501800537
Epoch 670, val loss: 1.2272484302520752
Epoch 680, training loss: 627.2077026367188 = 0.8961480259895325 + 100.0 * 6.263115882873535
Epoch 680, val loss: 1.22098970413208
Epoch 690, training loss: 627.1309204101562 = 0.8826435804367065 + 100.0 * 6.2624831199646
Epoch 690, val loss: 1.21527099609375
Epoch 700, training loss: 626.9185180664062 = 0.8694695234298706 + 100.0 * 6.260490894317627
Epoch 700, val loss: 1.2096092700958252
Epoch 710, training loss: 626.7852783203125 = 0.8564060926437378 + 100.0 * 6.259288787841797
Epoch 710, val loss: 1.204154133796692
Epoch 720, training loss: 627.623046875 = 0.8432950377464294 + 100.0 * 6.267797470092773
Epoch 720, val loss: 1.1985788345336914
Epoch 730, training loss: 626.6006469726562 = 0.8303561210632324 + 100.0 * 6.257702827453613
Epoch 730, val loss: 1.1930899620056152
Epoch 740, training loss: 626.4930419921875 = 0.8175381422042847 + 100.0 * 6.2567548751831055
Epoch 740, val loss: 1.1877585649490356
Epoch 750, training loss: 626.3862915039062 = 0.8049295544624329 + 100.0 * 6.2558135986328125
Epoch 750, val loss: 1.182706356048584
Epoch 760, training loss: 627.7247924804688 = 0.7924809455871582 + 100.0 * 6.269322872161865
Epoch 760, val loss: 1.177451491355896
Epoch 770, training loss: 626.2005004882812 = 0.7794614434242249 + 100.0 * 6.254209995269775
Epoch 770, val loss: 1.1722265481948853
Epoch 780, training loss: 626.1640625 = 0.7669578790664673 + 100.0 * 6.253971099853516
Epoch 780, val loss: 1.1673945188522339
Epoch 790, training loss: 625.9651489257812 = 0.754743218421936 + 100.0 * 6.252103805541992
Epoch 790, val loss: 1.1626030206680298
Epoch 800, training loss: 625.8590087890625 = 0.7426225543022156 + 100.0 * 6.251163959503174
Epoch 800, val loss: 1.1581735610961914
Epoch 810, training loss: 626.0274047851562 = 0.7305852770805359 + 100.0 * 6.2529683113098145
Epoch 810, val loss: 1.1538742780685425
Epoch 820, training loss: 625.9122314453125 = 0.7184135317802429 + 100.0 * 6.251938343048096
Epoch 820, val loss: 1.1491317749023438
Epoch 830, training loss: 625.8143920898438 = 0.7064594030380249 + 100.0 * 6.251079082489014
Epoch 830, val loss: 1.1451698541641235
Epoch 840, training loss: 625.6909790039062 = 0.6947427988052368 + 100.0 * 6.249962329864502
Epoch 840, val loss: 1.1412806510925293
Epoch 850, training loss: 625.6454467773438 = 0.6830962300300598 + 100.0 * 6.2496232986450195
Epoch 850, val loss: 1.1374880075454712
Epoch 860, training loss: 625.3690185546875 = 0.6715313196182251 + 100.0 * 6.246974945068359
Epoch 860, val loss: 1.1337790489196777
Epoch 870, training loss: 625.2743530273438 = 0.6601952314376831 + 100.0 * 6.24614143371582
Epoch 870, val loss: 1.130589485168457
Epoch 880, training loss: 625.5167236328125 = 0.6490020751953125 + 100.0 * 6.2486772537231445
Epoch 880, val loss: 1.1277185678482056
Epoch 890, training loss: 625.2667846679688 = 0.6379538774490356 + 100.0 * 6.246288299560547
Epoch 890, val loss: 1.1244592666625977
Epoch 900, training loss: 625.1573486328125 = 0.6269886493682861 + 100.0 * 6.245303630828857
Epoch 900, val loss: 1.1219109296798706
Epoch 910, training loss: 625.0359497070312 = 0.6162723898887634 + 100.0 * 6.244196891784668
Epoch 910, val loss: 1.1193523406982422
Epoch 920, training loss: 625.19677734375 = 0.6056687831878662 + 100.0 * 6.245911121368408
Epoch 920, val loss: 1.117092251777649
Epoch 930, training loss: 624.8472900390625 = 0.5953194499015808 + 100.0 * 6.242519378662109
Epoch 930, val loss: 1.1152703762054443
Epoch 940, training loss: 625.0459594726562 = 0.5851280689239502 + 100.0 * 6.244607925415039
Epoch 940, val loss: 1.113381028175354
Epoch 950, training loss: 625.0462646484375 = 0.5748628973960876 + 100.0 * 6.24471378326416
Epoch 950, val loss: 1.1114921569824219
Epoch 960, training loss: 624.6961669921875 = 0.5648338794708252 + 100.0 * 6.241313457489014
Epoch 960, val loss: 1.1103031635284424
Epoch 970, training loss: 624.5485229492188 = 0.5550363659858704 + 100.0 * 6.23993444442749
Epoch 970, val loss: 1.1091539859771729
Epoch 980, training loss: 624.5084228515625 = 0.5454823970794678 + 100.0 * 6.23962926864624
Epoch 980, val loss: 1.1081857681274414
Epoch 990, training loss: 625.0413208007812 = 0.5359713435173035 + 100.0 * 6.245053768157959
Epoch 990, val loss: 1.107420802116394
Epoch 1000, training loss: 624.5421752929688 = 0.5266533493995667 + 100.0 * 6.24015474319458
Epoch 1000, val loss: 1.1062999963760376
Epoch 1010, training loss: 624.338134765625 = 0.5174570083618164 + 100.0 * 6.23820686340332
Epoch 1010, val loss: 1.1061534881591797
Epoch 1020, training loss: 624.2799072265625 = 0.5085279941558838 + 100.0 * 6.23771333694458
Epoch 1020, val loss: 1.1057934761047363
Epoch 1030, training loss: 624.8936157226562 = 0.4997613728046417 + 100.0 * 6.243938446044922
Epoch 1030, val loss: 1.10568368434906
Epoch 1040, training loss: 624.4624633789062 = 0.4907590448856354 + 100.0 * 6.239716529846191
Epoch 1040, val loss: 1.1053500175476074
Epoch 1050, training loss: 624.1061401367188 = 0.4821685254573822 + 100.0 * 6.236239910125732
Epoch 1050, val loss: 1.1057151556015015
Epoch 1060, training loss: 624.0050048828125 = 0.47376391291618347 + 100.0 * 6.235312461853027
Epoch 1060, val loss: 1.1062400341033936
Epoch 1070, training loss: 624.5980224609375 = 0.46547168493270874 + 100.0 * 6.241325378417969
Epoch 1070, val loss: 1.1067805290222168
Epoch 1080, training loss: 624.0892944335938 = 0.4572358727455139 + 100.0 * 6.236320972442627
Epoch 1080, val loss: 1.1072781085968018
Epoch 1090, training loss: 624.0001831054688 = 0.4491898715496063 + 100.0 * 6.235509872436523
Epoch 1090, val loss: 1.1081602573394775
Epoch 1100, training loss: 623.8558959960938 = 0.44132307171821594 + 100.0 * 6.234145641326904
Epoch 1100, val loss: 1.1088920831680298
Epoch 1110, training loss: 623.9904174804688 = 0.43361005187034607 + 100.0 * 6.235568046569824
Epoch 1110, val loss: 1.1097681522369385
Epoch 1120, training loss: 623.8495483398438 = 0.42587587237358093 + 100.0 * 6.234236240386963
Epoch 1120, val loss: 1.1109765768051147
Epoch 1130, training loss: 623.6153564453125 = 0.4183533787727356 + 100.0 * 6.231970310211182
Epoch 1130, val loss: 1.1123124361038208
Epoch 1140, training loss: 624.1013793945312 = 0.4110032618045807 + 100.0 * 6.236903667449951
Epoch 1140, val loss: 1.1136671304702759
Epoch 1150, training loss: 623.7852783203125 = 0.4036683440208435 + 100.0 * 6.233816146850586
Epoch 1150, val loss: 1.1147027015686035
Epoch 1160, training loss: 623.5235595703125 = 0.3965117037296295 + 100.0 * 6.231270790100098
Epoch 1160, val loss: 1.1163339614868164
Epoch 1170, training loss: 623.3857421875 = 0.38956767320632935 + 100.0 * 6.229961395263672
Epoch 1170, val loss: 1.1180866956710815
Epoch 1180, training loss: 623.361083984375 = 0.38275226950645447 + 100.0 * 6.229783535003662
Epoch 1180, val loss: 1.1197571754455566
Epoch 1190, training loss: 623.8995971679688 = 0.3760039508342743 + 100.0 * 6.235236167907715
Epoch 1190, val loss: 1.1217063665390015
Epoch 1200, training loss: 623.7908935546875 = 0.36923840641975403 + 100.0 * 6.234216213226318
Epoch 1200, val loss: 1.1230942010879517
Epoch 1210, training loss: 623.5762329101562 = 0.3624659478664398 + 100.0 * 6.232137680053711
Epoch 1210, val loss: 1.1246241331100464
Epoch 1220, training loss: 623.3623657226562 = 0.35599592328071594 + 100.0 * 6.230063438415527
Epoch 1220, val loss: 1.1264413595199585
Epoch 1230, training loss: 623.1292724609375 = 0.3497125506401062 + 100.0 * 6.227795124053955
Epoch 1230, val loss: 1.1288398504257202
Epoch 1240, training loss: 623.0751953125 = 0.343612402677536 + 100.0 * 6.227315425872803
Epoch 1240, val loss: 1.131225347518921
Epoch 1250, training loss: 622.9959716796875 = 0.33757972717285156 + 100.0 * 6.226583957672119
Epoch 1250, val loss: 1.1335991621017456
Epoch 1260, training loss: 622.9688110351562 = 0.3316458761692047 + 100.0 * 6.2263712882995605
Epoch 1260, val loss: 1.1362295150756836
Epoch 1270, training loss: 623.4647827148438 = 0.3257841169834137 + 100.0 * 6.23138952255249
Epoch 1270, val loss: 1.1388963460922241
Epoch 1280, training loss: 623.5748901367188 = 0.31976205110549927 + 100.0 * 6.232551574707031
Epoch 1280, val loss: 1.1406232118606567
Epoch 1290, training loss: 623.0307006835938 = 0.31404924392700195 + 100.0 * 6.227166652679443
Epoch 1290, val loss: 1.143342137336731
Epoch 1300, training loss: 622.8380126953125 = 0.30841824412345886 + 100.0 * 6.2252960205078125
Epoch 1300, val loss: 1.146246075630188
Epoch 1310, training loss: 622.7904052734375 = 0.30296826362609863 + 100.0 * 6.224874496459961
Epoch 1310, val loss: 1.1491281986236572
Epoch 1320, training loss: 623.168701171875 = 0.29767945408821106 + 100.0 * 6.228710174560547
Epoch 1320, val loss: 1.1520965099334717
Epoch 1330, training loss: 622.7687377929688 = 0.29218173027038574 + 100.0 * 6.224765777587891
Epoch 1330, val loss: 1.1545113325119019
Epoch 1340, training loss: 622.7229614257812 = 0.28692054748535156 + 100.0 * 6.224360466003418
Epoch 1340, val loss: 1.1576472520828247
Epoch 1350, training loss: 622.95361328125 = 0.2817879915237427 + 100.0 * 6.226717948913574
Epoch 1350, val loss: 1.1607792377471924
Epoch 1360, training loss: 622.650146484375 = 0.2765970826148987 + 100.0 * 6.223735332489014
Epoch 1360, val loss: 1.1635710000991821
Epoch 1370, training loss: 622.5891723632812 = 0.2715533375740051 + 100.0 * 6.223176002502441
Epoch 1370, val loss: 1.1668857336044312
Epoch 1380, training loss: 622.5374145507812 = 0.26667454838752747 + 100.0 * 6.222707271575928
Epoch 1380, val loss: 1.1700528860092163
Epoch 1390, training loss: 622.5734252929688 = 0.261923223733902 + 100.0 * 6.22311544418335
Epoch 1390, val loss: 1.1737645864486694
Epoch 1400, training loss: 623.001220703125 = 0.25716403126716614 + 100.0 * 6.22744083404541
Epoch 1400, val loss: 1.1769946813583374
Epoch 1410, training loss: 622.6857299804688 = 0.2524832487106323 + 100.0 * 6.224332332611084
Epoch 1410, val loss: 1.180214285850525
Epoch 1420, training loss: 622.4886474609375 = 0.24789157509803772 + 100.0 * 6.222407817840576
Epoch 1420, val loss: 1.1835402250289917
Epoch 1430, training loss: 622.5648193359375 = 0.24342428147792816 + 100.0 * 6.223214149475098
Epoch 1430, val loss: 1.1874300241470337
Epoch 1440, training loss: 622.365966796875 = 0.23898197710514069 + 100.0 * 6.2212700843811035
Epoch 1440, val loss: 1.190905213356018
Epoch 1450, training loss: 622.25146484375 = 0.2346685379743576 + 100.0 * 6.220167636871338
Epoch 1450, val loss: 1.194772720336914
Epoch 1460, training loss: 622.5801391601562 = 0.23046331107616425 + 100.0 * 6.223496437072754
Epoch 1460, val loss: 1.198359489440918
Epoch 1470, training loss: 622.2828979492188 = 0.22620956599712372 + 100.0 * 6.220567226409912
Epoch 1470, val loss: 1.202690601348877
Epoch 1480, training loss: 622.6442260742188 = 0.22211876511573792 + 100.0 * 6.224221229553223
Epoch 1480, val loss: 1.206465244293213
Epoch 1490, training loss: 622.5825805664062 = 0.217961847782135 + 100.0 * 6.22364616394043
Epoch 1490, val loss: 1.2103793621063232
Epoch 1500, training loss: 622.1561279296875 = 0.21391935646533966 + 100.0 * 6.219421863555908
Epoch 1500, val loss: 1.2144804000854492
Epoch 1510, training loss: 622.0462646484375 = 0.21001048386096954 + 100.0 * 6.218362331390381
Epoch 1510, val loss: 1.2188142538070679
Epoch 1520, training loss: 622.0784912109375 = 0.20622335374355316 + 100.0 * 6.218722343444824
Epoch 1520, val loss: 1.223249077796936
Epoch 1530, training loss: 622.4967651367188 = 0.20247341692447662 + 100.0 * 6.222943305969238
Epoch 1530, val loss: 1.227702260017395
Epoch 1540, training loss: 622.2301025390625 = 0.1987236738204956 + 100.0 * 6.220313549041748
Epoch 1540, val loss: 1.2319062948226929
Epoch 1550, training loss: 622.0349731445312 = 0.19503027200698853 + 100.0 * 6.218399524688721
Epoch 1550, val loss: 1.236032485961914
Epoch 1560, training loss: 622.0068359375 = 0.19148655235767365 + 100.0 * 6.218153476715088
Epoch 1560, val loss: 1.2407042980194092
Epoch 1570, training loss: 622.2288818359375 = 0.18797631561756134 + 100.0 * 6.220408916473389
Epoch 1570, val loss: 1.2450389862060547
Epoch 1580, training loss: 621.9688720703125 = 0.18455319106578827 + 100.0 * 6.217843055725098
Epoch 1580, val loss: 1.2501953840255737
Epoch 1590, training loss: 622.0067749023438 = 0.18114668130874634 + 100.0 * 6.218255996704102
Epoch 1590, val loss: 1.2543373107910156
Epoch 1600, training loss: 622.1735229492188 = 0.177805557847023 + 100.0 * 6.21995735168457
Epoch 1600, val loss: 1.2593220472335815
Epoch 1610, training loss: 622.099609375 = 0.17446665465831757 + 100.0 * 6.21925163269043
Epoch 1610, val loss: 1.2638128995895386
Epoch 1620, training loss: 621.7893676757812 = 0.1712787002325058 + 100.0 * 6.216180801391602
Epoch 1620, val loss: 1.268450379371643
Epoch 1630, training loss: 621.7319946289062 = 0.16816553473472595 + 100.0 * 6.215638637542725
Epoch 1630, val loss: 1.2735111713409424
Epoch 1640, training loss: 621.8641357421875 = 0.1651652604341507 + 100.0 * 6.216989994049072
Epoch 1640, val loss: 1.278571367263794
Epoch 1650, training loss: 621.9363403320312 = 0.16211317479610443 + 100.0 * 6.217742443084717
Epoch 1650, val loss: 1.2832093238830566
Epoch 1660, training loss: 621.7540283203125 = 0.15909156203269958 + 100.0 * 6.215949058532715
Epoch 1660, val loss: 1.2881816625595093
Epoch 1670, training loss: 621.8501586914062 = 0.1562153398990631 + 100.0 * 6.216939449310303
Epoch 1670, val loss: 1.2933762073516846
Epoch 1680, training loss: 621.6942749023438 = 0.15332311391830444 + 100.0 * 6.215409755706787
Epoch 1680, val loss: 1.298213005065918
Epoch 1690, training loss: 621.8435668945312 = 0.150481179356575 + 100.0 * 6.216930866241455
Epoch 1690, val loss: 1.3029472827911377
Epoch 1700, training loss: 621.66943359375 = 0.147762268781662 + 100.0 * 6.215216636657715
Epoch 1700, val loss: 1.3084303140640259
Epoch 1710, training loss: 621.6611328125 = 0.14507628977298737 + 100.0 * 6.215160369873047
Epoch 1710, val loss: 1.3135136365890503
Epoch 1720, training loss: 621.6533203125 = 0.14243292808532715 + 100.0 * 6.215108871459961
Epoch 1720, val loss: 1.3190057277679443
Epoch 1730, training loss: 621.61865234375 = 0.1398092359304428 + 100.0 * 6.21478796005249
Epoch 1730, val loss: 1.324317216873169
Epoch 1740, training loss: 621.5197143554688 = 0.13726891577243805 + 100.0 * 6.213824272155762
Epoch 1740, val loss: 1.329679250717163
Epoch 1750, training loss: 621.6485595703125 = 0.13478992879390717 + 100.0 * 6.215137958526611
Epoch 1750, val loss: 1.3349169492721558
Epoch 1760, training loss: 621.5953369140625 = 0.1323062777519226 + 100.0 * 6.214630126953125
Epoch 1760, val loss: 1.3400264978408813
Epoch 1770, training loss: 621.545166015625 = 0.12988683581352234 + 100.0 * 6.214152812957764
Epoch 1770, val loss: 1.3453161716461182
Epoch 1780, training loss: 621.4739379882812 = 0.1275654435157776 + 100.0 * 6.21346378326416
Epoch 1780, val loss: 1.3507875204086304
Epoch 1790, training loss: 621.7656860351562 = 0.12522892653942108 + 100.0 * 6.216404438018799
Epoch 1790, val loss: 1.3560810089111328
Epoch 1800, training loss: 621.4257202148438 = 0.12297575920820236 + 100.0 * 6.213027477264404
Epoch 1800, val loss: 1.3615362644195557
Epoch 1810, training loss: 621.2998657226562 = 0.12072861194610596 + 100.0 * 6.211791515350342
Epoch 1810, val loss: 1.3671667575836182
Epoch 1820, training loss: 621.50634765625 = 0.11858440935611725 + 100.0 * 6.2138776779174805
Epoch 1820, val loss: 1.3728634119033813
Epoch 1830, training loss: 621.3929443359375 = 0.11641605198383331 + 100.0 * 6.212765693664551
Epoch 1830, val loss: 1.3777967691421509
Epoch 1840, training loss: 621.4813232421875 = 0.11432207375764847 + 100.0 * 6.213670253753662
Epoch 1840, val loss: 1.38323974609375
Epoch 1850, training loss: 621.2595825195312 = 0.11224047839641571 + 100.0 * 6.21147346496582
Epoch 1850, val loss: 1.3890513181686401
Epoch 1860, training loss: 621.2247314453125 = 0.1102617010474205 + 100.0 * 6.21114444732666
Epoch 1860, val loss: 1.3941065073013306
Epoch 1870, training loss: 621.3206787109375 = 0.10830041766166687 + 100.0 * 6.212123870849609
Epoch 1870, val loss: 1.3999483585357666
Epoch 1880, training loss: 621.1652221679688 = 0.10636329650878906 + 100.0 * 6.210588455200195
Epoch 1880, val loss: 1.405820608139038
Epoch 1890, training loss: 621.3440551757812 = 0.10448244214057922 + 100.0 * 6.212395668029785
Epoch 1890, val loss: 1.4109251499176025
Epoch 1900, training loss: 621.5994873046875 = 0.10258617252111435 + 100.0 * 6.214969158172607
Epoch 1900, val loss: 1.4160685539245605
Epoch 1910, training loss: 621.250244140625 = 0.1007332056760788 + 100.0 * 6.211495399475098
Epoch 1910, val loss: 1.421783685684204
Epoch 1920, training loss: 621.0654907226562 = 0.09894948452711105 + 100.0 * 6.209665775299072
Epoch 1920, val loss: 1.42716383934021
Epoch 1930, training loss: 620.9579467773438 = 0.09721699357032776 + 100.0 * 6.208607196807861
Epoch 1930, val loss: 1.4329755306243896
Epoch 1940, training loss: 621.0128173828125 = 0.09553520381450653 + 100.0 * 6.209173202514648
Epoch 1940, val loss: 1.4384950399398804
Epoch 1950, training loss: 621.627197265625 = 0.09386597573757172 + 100.0 * 6.215333461761475
Epoch 1950, val loss: 1.4439129829406738
Epoch 1960, training loss: 621.2122192382812 = 0.09221513569355011 + 100.0 * 6.211199760437012
Epoch 1960, val loss: 1.4490437507629395
Epoch 1970, training loss: 621.2521362304688 = 0.09055989980697632 + 100.0 * 6.211615562438965
Epoch 1970, val loss: 1.454429268836975
Epoch 1980, training loss: 620.918212890625 = 0.08897962421178818 + 100.0 * 6.208292007446289
Epoch 1980, val loss: 1.4601125717163086
Epoch 1990, training loss: 620.8713989257812 = 0.08744173496961594 + 100.0 * 6.207839488983154
Epoch 1990, val loss: 1.4654251337051392
Epoch 2000, training loss: 621.4445190429688 = 0.08599407970905304 + 100.0 * 6.213584899902344
Epoch 2000, val loss: 1.4710893630981445
Epoch 2010, training loss: 621.0030517578125 = 0.08444313704967499 + 100.0 * 6.20918607711792
Epoch 2010, val loss: 1.4758692979812622
Epoch 2020, training loss: 621.0759887695312 = 0.08298373967409134 + 100.0 * 6.209929943084717
Epoch 2020, val loss: 1.4812095165252686
Epoch 2030, training loss: 620.8301391601562 = 0.08152713626623154 + 100.0 * 6.207486152648926
Epoch 2030, val loss: 1.4861693382263184
Epoch 2040, training loss: 620.8174438476562 = 0.08015551418066025 + 100.0 * 6.207373142242432
Epoch 2040, val loss: 1.4920563697814941
Epoch 2050, training loss: 620.9395751953125 = 0.0788145363330841 + 100.0 * 6.2086076736450195
Epoch 2050, val loss: 1.497304081916809
Epoch 2060, training loss: 620.8280029296875 = 0.07747253775596619 + 100.0 * 6.207505702972412
Epoch 2060, val loss: 1.502711534500122
Epoch 2070, training loss: 621.1171264648438 = 0.07615216821432114 + 100.0 * 6.210409641265869
Epoch 2070, val loss: 1.5079081058502197
Epoch 2080, training loss: 621.0106811523438 = 0.07484432309865952 + 100.0 * 6.2093586921691895
Epoch 2080, val loss: 1.513067603111267
Epoch 2090, training loss: 621.0241088867188 = 0.07359098643064499 + 100.0 * 6.209505081176758
Epoch 2090, val loss: 1.5182783603668213
Epoch 2100, training loss: 620.867919921875 = 0.07232064753770828 + 100.0 * 6.207955837249756
Epoch 2100, val loss: 1.5241812467575073
Epoch 2110, training loss: 620.6341552734375 = 0.0711226835846901 + 100.0 * 6.205630302429199
Epoch 2110, val loss: 1.5287889242172241
Epoch 2120, training loss: 620.6101684570312 = 0.06995536386966705 + 100.0 * 6.205402374267578
Epoch 2120, val loss: 1.5342785120010376
Epoch 2130, training loss: 620.6871948242188 = 0.06882929801940918 + 100.0 * 6.206183433532715
Epoch 2130, val loss: 1.5397026538848877
Epoch 2140, training loss: 620.9069213867188 = 0.06770618259906769 + 100.0 * 6.208392143249512
Epoch 2140, val loss: 1.5449849367141724
Epoch 2150, training loss: 620.6610717773438 = 0.06656543910503387 + 100.0 * 6.205945014953613
Epoch 2150, val loss: 1.550396203994751
Epoch 2160, training loss: 620.7281494140625 = 0.06545989215373993 + 100.0 * 6.206627368927002
Epoch 2160, val loss: 1.5554215908050537
Epoch 2170, training loss: 620.6802368164062 = 0.06437868624925613 + 100.0 * 6.206158638000488
Epoch 2170, val loss: 1.560361385345459
Epoch 2180, training loss: 620.890380859375 = 0.06333942711353302 + 100.0 * 6.208270072937012
Epoch 2180, val loss: 1.565929889678955
Epoch 2190, training loss: 620.5733642578125 = 0.062272753566503525 + 100.0 * 6.205111026763916
Epoch 2190, val loss: 1.5704426765441895
Epoch 2200, training loss: 620.513671875 = 0.06127496808767319 + 100.0 * 6.204524040222168
Epoch 2200, val loss: 1.5757006406784058
Epoch 2210, training loss: 620.996337890625 = 0.06030837446451187 + 100.0 * 6.209360599517822
Epoch 2210, val loss: 1.5802862644195557
Epoch 2220, training loss: 620.6229248046875 = 0.05932750552892685 + 100.0 * 6.205636024475098
Epoch 2220, val loss: 1.5863022804260254
Epoch 2230, training loss: 620.4791870117188 = 0.058363430202007294 + 100.0 * 6.2042083740234375
Epoch 2230, val loss: 1.5907410383224487
Epoch 2240, training loss: 620.3897705078125 = 0.057458046823740005 + 100.0 * 6.2033233642578125
Epoch 2240, val loss: 1.5965157747268677
Epoch 2250, training loss: 620.71337890625 = 0.05658337473869324 + 100.0 * 6.206568241119385
Epoch 2250, val loss: 1.6014409065246582
Epoch 2260, training loss: 620.5479125976562 = 0.05567220598459244 + 100.0 * 6.204922199249268
Epoch 2260, val loss: 1.606106162071228
Epoch 2270, training loss: 620.5096435546875 = 0.05475899204611778 + 100.0 * 6.2045488357543945
Epoch 2270, val loss: 1.6108413934707642
Epoch 2280, training loss: 620.3353271484375 = 0.053897615522146225 + 100.0 * 6.20281457901001
Epoch 2280, val loss: 1.6161296367645264
Epoch 2290, training loss: 620.4085083007812 = 0.05307181179523468 + 100.0 * 6.203554153442383
Epoch 2290, val loss: 1.620965838432312
Epoch 2300, training loss: 620.4744873046875 = 0.052265092730522156 + 100.0 * 6.204222679138184
Epoch 2300, val loss: 1.6259180307388306
Epoch 2310, training loss: 620.4271240234375 = 0.05145273730158806 + 100.0 * 6.203756809234619
Epoch 2310, val loss: 1.6308188438415527
Epoch 2320, training loss: 620.4785766601562 = 0.050662390887737274 + 100.0 * 6.204278945922852
Epoch 2320, val loss: 1.6361750364303589
Epoch 2330, training loss: 620.3694458007812 = 0.04987721145153046 + 100.0 * 6.203195571899414
Epoch 2330, val loss: 1.6404926776885986
Epoch 2340, training loss: 620.6967163085938 = 0.0490962378680706 + 100.0 * 6.20647668838501
Epoch 2340, val loss: 1.6449590921401978
Epoch 2350, training loss: 620.2998046875 = 0.0483359694480896 + 100.0 * 6.2025146484375
Epoch 2350, val loss: 1.6500977277755737
Epoch 2360, training loss: 620.255615234375 = 0.047608695924282074 + 100.0 * 6.202079772949219
Epoch 2360, val loss: 1.6548843383789062
Epoch 2370, training loss: 620.12841796875 = 0.046902939677238464 + 100.0 * 6.200815677642822
Epoch 2370, val loss: 1.6597650051116943
Epoch 2380, training loss: 620.2520751953125 = 0.046228185296058655 + 100.0 * 6.202058792114258
Epoch 2380, val loss: 1.664908528327942
Epoch 2390, training loss: 620.71240234375 = 0.04554278403520584 + 100.0 * 6.206668853759766
Epoch 2390, val loss: 1.6697815656661987
Epoch 2400, training loss: 620.4947509765625 = 0.04481763392686844 + 100.0 * 6.2044997215271
Epoch 2400, val loss: 1.673269271850586
Epoch 2410, training loss: 620.3026123046875 = 0.04414143040776253 + 100.0 * 6.202584743499756
Epoch 2410, val loss: 1.6778795719146729
Epoch 2420, training loss: 620.3984985351562 = 0.04348983243107796 + 100.0 * 6.203549861907959
Epoch 2420, val loss: 1.6830286979675293
Epoch 2430, training loss: 620.1030883789062 = 0.04284358024597168 + 100.0 * 6.2006025314331055
Epoch 2430, val loss: 1.6872776746749878
Epoch 2440, training loss: 620.1110229492188 = 0.04223942384123802 + 100.0 * 6.200687885284424
Epoch 2440, val loss: 1.6921744346618652
Epoch 2450, training loss: 620.203857421875 = 0.04164843633770943 + 100.0 * 6.201622009277344
Epoch 2450, val loss: 1.697004795074463
Epoch 2460, training loss: 620.25732421875 = 0.0410512313246727 + 100.0 * 6.202162742614746
Epoch 2460, val loss: 1.7011282444000244
Epoch 2470, training loss: 620.1149291992188 = 0.04044209420681 + 100.0 * 6.20074462890625
Epoch 2470, val loss: 1.7055085897445679
Epoch 2480, training loss: 620.3782348632812 = 0.039864134043455124 + 100.0 * 6.203383922576904
Epoch 2480, val loss: 1.7099281549453735
Epoch 2490, training loss: 620.2040405273438 = 0.039282675832509995 + 100.0 * 6.2016472816467285
Epoch 2490, val loss: 1.714318871498108
Epoch 2500, training loss: 620.4263305664062 = 0.03871435672044754 + 100.0 * 6.203876495361328
Epoch 2500, val loss: 1.719041109085083
Epoch 2510, training loss: 620.1031494140625 = 0.038164086639881134 + 100.0 * 6.200649738311768
Epoch 2510, val loss: 1.7231979370117188
Epoch 2520, training loss: 619.9826049804688 = 0.037628330290317535 + 100.0 * 6.19944953918457
Epoch 2520, val loss: 1.7276426553726196
Epoch 2530, training loss: 619.9721069335938 = 0.03710995241999626 + 100.0 * 6.199349880218506
Epoch 2530, val loss: 1.7319926023483276
Epoch 2540, training loss: 620.2078857421875 = 0.03660646453499794 + 100.0 * 6.201712608337402
Epoch 2540, val loss: 1.7365188598632812
Epoch 2550, training loss: 620.1104125976562 = 0.036088068038225174 + 100.0 * 6.200743198394775
Epoch 2550, val loss: 1.7404942512512207
Epoch 2560, training loss: 620.035888671875 = 0.035593513399362564 + 100.0 * 6.200003147125244
Epoch 2560, val loss: 1.7450332641601562
Epoch 2570, training loss: 619.9682006835938 = 0.035100266337394714 + 100.0 * 6.199331283569336
Epoch 2570, val loss: 1.7490456104278564
Epoch 2580, training loss: 620.1327514648438 = 0.03463074937462807 + 100.0 * 6.200981140136719
Epoch 2580, val loss: 1.7537342309951782
Epoch 2590, training loss: 619.9949951171875 = 0.03416283801198006 + 100.0 * 6.199608325958252
Epoch 2590, val loss: 1.7580832242965698
Epoch 2600, training loss: 620.0872802734375 = 0.03369187191128731 + 100.0 * 6.200535774230957
Epoch 2600, val loss: 1.7619287967681885
Epoch 2610, training loss: 619.9673461914062 = 0.03323357179760933 + 100.0 * 6.199341297149658
Epoch 2610, val loss: 1.7654162645339966
Epoch 2620, training loss: 619.9449462890625 = 0.03280302882194519 + 100.0 * 6.199121952056885
Epoch 2620, val loss: 1.7703832387924194
Epoch 2630, training loss: 620.2881469726562 = 0.032378412783145905 + 100.0 * 6.2025580406188965
Epoch 2630, val loss: 1.7745853662490845
Epoch 2640, training loss: 619.9067993164062 = 0.031913839280605316 + 100.0 * 6.198748588562012
Epoch 2640, val loss: 1.777825951576233
Epoch 2650, training loss: 619.7843017578125 = 0.03149911016225815 + 100.0 * 6.197527885437012
Epoch 2650, val loss: 1.7825241088867188
Epoch 2660, training loss: 620.4273681640625 = 0.031096188351511955 + 100.0 * 6.203962802886963
Epoch 2660, val loss: 1.7869184017181396
Epoch 2670, training loss: 619.8408813476562 = 0.030673561617732048 + 100.0 * 6.198101997375488
Epoch 2670, val loss: 1.7894210815429688
Epoch 2680, training loss: 619.78125 = 0.03027430921792984 + 100.0 * 6.197509765625
Epoch 2680, val loss: 1.7943540811538696
Epoch 2690, training loss: 619.8672485351562 = 0.02989218942821026 + 100.0 * 6.198373317718506
Epoch 2690, val loss: 1.798257827758789
Epoch 2700, training loss: 620.004638671875 = 0.02951493300497532 + 100.0 * 6.199750900268555
Epoch 2700, val loss: 1.8021337985992432
Epoch 2710, training loss: 619.905517578125 = 0.02912241779267788 + 100.0 * 6.198764324188232
Epoch 2710, val loss: 1.8057326078414917
Epoch 2720, training loss: 619.6704711914062 = 0.028738047927618027 + 100.0 * 6.196417331695557
Epoch 2720, val loss: 1.810027003288269
Epoch 2730, training loss: 619.7122192382812 = 0.028390387073159218 + 100.0 * 6.19683837890625
Epoch 2730, val loss: 1.814132571220398
Epoch 2740, training loss: 619.9755859375 = 0.028047312051057816 + 100.0 * 6.199475288391113
Epoch 2740, val loss: 1.8176307678222656
Epoch 2750, training loss: 619.9227905273438 = 0.027676988393068314 + 100.0 * 6.198951244354248
Epoch 2750, val loss: 1.821008324623108
Epoch 2760, training loss: 619.6748657226562 = 0.027321305125951767 + 100.0 * 6.196475505828857
Epoch 2760, val loss: 1.8251069784164429
Epoch 2770, training loss: 619.5889282226562 = 0.026978155598044395 + 100.0 * 6.195619583129883
Epoch 2770, val loss: 1.828959345817566
Epoch 2780, training loss: 619.6692504882812 = 0.026659095659852028 + 100.0 * 6.196425914764404
Epoch 2780, val loss: 1.833083152770996
Epoch 2790, training loss: 620.163818359375 = 0.026349056512117386 + 100.0 * 6.201374530792236
Epoch 2790, val loss: 1.837238073348999
Epoch 2800, training loss: 619.71826171875 = 0.02599705569446087 + 100.0 * 6.196922302246094
Epoch 2800, val loss: 1.8393489122390747
Epoch 2810, training loss: 619.7687377929688 = 0.025689467787742615 + 100.0 * 6.19743013381958
Epoch 2810, val loss: 1.8438035249710083
Epoch 2820, training loss: 619.8519287109375 = 0.02537720464169979 + 100.0 * 6.198265075683594
Epoch 2820, val loss: 1.8469407558441162
Epoch 2830, training loss: 619.66015625 = 0.025049954652786255 + 100.0 * 6.196351051330566
Epoch 2830, val loss: 1.8508353233337402
Epoch 2840, training loss: 619.6036376953125 = 0.0247599259018898 + 100.0 * 6.195788383483887
Epoch 2840, val loss: 1.8548438549041748
Epoch 2850, training loss: 619.9193725585938 = 0.024479065090417862 + 100.0 * 6.198948860168457
Epoch 2850, val loss: 1.8586180210113525
Epoch 2860, training loss: 619.6366577148438 = 0.024173526093363762 + 100.0 * 6.196125030517578
Epoch 2860, val loss: 1.8616052865982056
Epoch 2870, training loss: 619.5230712890625 = 0.023880457505583763 + 100.0 * 6.1949920654296875
Epoch 2870, val loss: 1.86455500125885
Epoch 2880, training loss: 619.5223388671875 = 0.023603767156600952 + 100.0 * 6.1949872970581055
Epoch 2880, val loss: 1.868567705154419
Epoch 2890, training loss: 619.7373657226562 = 0.023338016122579575 + 100.0 * 6.197140693664551
Epoch 2890, val loss: 1.8717454671859741
Epoch 2900, training loss: 619.8076171875 = 0.023062679916620255 + 100.0 * 6.197845458984375
Epoch 2900, val loss: 1.8755226135253906
Epoch 2910, training loss: 619.8453369140625 = 0.022780584171414375 + 100.0 * 6.198225975036621
Epoch 2910, val loss: 1.8784143924713135
Epoch 2920, training loss: 619.6890258789062 = 0.02250531315803528 + 100.0 * 6.196665287017822
Epoch 2920, val loss: 1.8816125392913818
Epoch 2930, training loss: 619.4862670898438 = 0.022252678871154785 + 100.0 * 6.194640159606934
Epoch 2930, val loss: 1.8851231336593628
Epoch 2940, training loss: 619.4442749023438 = 0.022005261853337288 + 100.0 * 6.194222450256348
Epoch 2940, val loss: 1.8886504173278809
Epoch 2950, training loss: 619.7877197265625 = 0.021769393235445023 + 100.0 * 6.197659492492676
Epoch 2950, val loss: 1.8924472332000732
Epoch 2960, training loss: 619.5709228515625 = 0.02150532603263855 + 100.0 * 6.195494651794434
Epoch 2960, val loss: 1.8947612047195435
Epoch 2970, training loss: 619.624755859375 = 0.021257052198052406 + 100.0 * 6.196034908294678
Epoch 2970, val loss: 1.8979765176773071
Epoch 2980, training loss: 619.642333984375 = 0.021010152995586395 + 100.0 * 6.196213245391846
Epoch 2980, val loss: 1.9011285305023193
Epoch 2990, training loss: 619.4691162109375 = 0.0207841657102108 + 100.0 * 6.194482803344727
Epoch 2990, val loss: 1.904466152191162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 861.6290893554688 = 1.9455735683441162 + 100.0 * 8.596835136413574
Epoch 0, val loss: 1.9390238523483276
Epoch 10, training loss: 861.53466796875 = 1.9366908073425293 + 100.0 * 8.595979690551758
Epoch 10, val loss: 1.9303735494613647
Epoch 20, training loss: 860.8861694335938 = 1.9257031679153442 + 100.0 * 8.589604377746582
Epoch 20, val loss: 1.9191508293151855
Epoch 30, training loss: 856.5864868164062 = 1.9115523099899292 + 100.0 * 8.546749114990234
Epoch 30, val loss: 1.9042611122131348
Epoch 40, training loss: 834.38623046875 = 1.894504189491272 + 100.0 * 8.32491683959961
Epoch 40, val loss: 1.8864015340805054
Epoch 50, training loss: 778.8935546875 = 1.875006914138794 + 100.0 * 7.770185470581055
Epoch 50, val loss: 1.8671051263809204
Epoch 60, training loss: 738.6576538085938 = 1.8618850708007812 + 100.0 * 7.367957592010498
Epoch 60, val loss: 1.8548539876937866
Epoch 70, training loss: 702.909423828125 = 1.8518877029418945 + 100.0 * 7.010575294494629
Epoch 70, val loss: 1.8453271389007568
Epoch 80, training loss: 685.8750610351562 = 1.8422859907150269 + 100.0 * 6.840327739715576
Epoch 80, val loss: 1.8363678455352783
Epoch 90, training loss: 677.0052490234375 = 1.833126187324524 + 100.0 * 6.751720905303955
Epoch 90, val loss: 1.828056812286377
Epoch 100, training loss: 670.9036865234375 = 1.8247829675674438 + 100.0 * 6.690789222717285
Epoch 100, val loss: 1.8205333948135376
Epoch 110, training loss: 666.2345581054688 = 1.8170357942581177 + 100.0 * 6.6441755294799805
Epoch 110, val loss: 1.8138625621795654
Epoch 120, training loss: 661.8311157226562 = 1.8104583024978638 + 100.0 * 6.60020637512207
Epoch 120, val loss: 1.808148980140686
Epoch 130, training loss: 658.0054931640625 = 1.8045896291732788 + 100.0 * 6.562009334564209
Epoch 130, val loss: 1.8028532266616821
Epoch 140, training loss: 654.3872680664062 = 1.7988743782043457 + 100.0 * 6.525883674621582
Epoch 140, val loss: 1.7974669933319092
Epoch 150, training loss: 651.6005249023438 = 1.792973279953003 + 100.0 * 6.498075485229492
Epoch 150, val loss: 1.791905403137207
Epoch 160, training loss: 649.3236083984375 = 1.7868069410324097 + 100.0 * 6.475368022918701
Epoch 160, val loss: 1.78618586063385
Epoch 170, training loss: 647.5845336914062 = 1.780177116394043 + 100.0 * 6.458043575286865
Epoch 170, val loss: 1.779988169670105
Epoch 180, training loss: 646.0956420898438 = 1.7730103731155396 + 100.0 * 6.443226337432861
Epoch 180, val loss: 1.7734262943267822
Epoch 190, training loss: 644.7493286132812 = 1.7653664350509644 + 100.0 * 6.429839611053467
Epoch 190, val loss: 1.766390085220337
Epoch 200, training loss: 643.5342407226562 = 1.7571442127227783 + 100.0 * 6.417770862579346
Epoch 200, val loss: 1.7589088678359985
Epoch 210, training loss: 643.49658203125 = 1.748244047164917 + 100.0 * 6.417483806610107
Epoch 210, val loss: 1.750854730606079
Epoch 220, training loss: 641.709228515625 = 1.738533854484558 + 100.0 * 6.399706840515137
Epoch 220, val loss: 1.7421025037765503
Epoch 230, training loss: 640.7816162109375 = 1.7280919551849365 + 100.0 * 6.390535354614258
Epoch 230, val loss: 1.732704997062683
Epoch 240, training loss: 639.9613647460938 = 1.716892123222351 + 100.0 * 6.382444381713867
Epoch 240, val loss: 1.7226581573486328
Epoch 250, training loss: 639.211669921875 = 1.7048389911651611 + 100.0 * 6.375068187713623
Epoch 250, val loss: 1.7118479013442993
Epoch 260, training loss: 638.5303955078125 = 1.6919102668762207 + 100.0 * 6.368384838104248
Epoch 260, val loss: 1.7002381086349487
Epoch 270, training loss: 637.9650268554688 = 1.6779781579971313 + 100.0 * 6.362870216369629
Epoch 270, val loss: 1.6878101825714111
Epoch 280, training loss: 637.5363159179688 = 1.6630539894104004 + 100.0 * 6.358733177185059
Epoch 280, val loss: 1.674528956413269
Epoch 290, training loss: 636.7460327148438 = 1.647340178489685 + 100.0 * 6.350986957550049
Epoch 290, val loss: 1.660550594329834
Epoch 300, training loss: 636.2037353515625 = 1.6307454109191895 + 100.0 * 6.345729827880859
Epoch 300, val loss: 1.6458790302276611
Epoch 310, training loss: 636.0186767578125 = 1.6132876873016357 + 100.0 * 6.344054222106934
Epoch 310, val loss: 1.630585789680481
Epoch 320, training loss: 635.2815551757812 = 1.5948556661605835 + 100.0 * 6.336866855621338
Epoch 320, val loss: 1.61441969871521
Epoch 330, training loss: 634.9424438476562 = 1.5758296251296997 + 100.0 * 6.3336663246154785
Epoch 330, val loss: 1.5978748798370361
Epoch 340, training loss: 634.4151000976562 = 1.5561442375183105 + 100.0 * 6.32858943939209
Epoch 340, val loss: 1.580849528312683
Epoch 350, training loss: 634.3671264648438 = 1.5359185934066772 + 100.0 * 6.328312397003174
Epoch 350, val loss: 1.563477635383606
Epoch 360, training loss: 633.982421875 = 1.5149760246276855 + 100.0 * 6.324674606323242
Epoch 360, val loss: 1.5455747842788696
Epoch 370, training loss: 633.2896728515625 = 1.4937622547149658 + 100.0 * 6.317958831787109
Epoch 370, val loss: 1.5276553630828857
Epoch 380, training loss: 632.9879760742188 = 1.4721670150756836 + 100.0 * 6.315158367156982
Epoch 380, val loss: 1.5096261501312256
Epoch 390, training loss: 632.8701782226562 = 1.4502456188201904 + 100.0 * 6.314198970794678
Epoch 390, val loss: 1.49143385887146
Epoch 400, training loss: 632.7880249023438 = 1.4279985427856445 + 100.0 * 6.313600540161133
Epoch 400, val loss: 1.4731025695800781
Epoch 410, training loss: 632.1738891601562 = 1.405290126800537 + 100.0 * 6.3076863288879395
Epoch 410, val loss: 1.4547348022460938
Epoch 420, training loss: 631.8453979492188 = 1.3824776411056519 + 100.0 * 6.304628849029541
Epoch 420, val loss: 1.4362962245941162
Epoch 430, training loss: 631.497802734375 = 1.3594506978988647 + 100.0 * 6.3013834953308105
Epoch 430, val loss: 1.4180082082748413
Epoch 440, training loss: 631.8161010742188 = 1.3361339569091797 + 100.0 * 6.304800033569336
Epoch 440, val loss: 1.3995717763900757
Epoch 450, training loss: 630.9343872070312 = 1.312706470489502 + 100.0 * 6.29621696472168
Epoch 450, val loss: 1.3812556266784668
Epoch 460, training loss: 630.7420654296875 = 1.2891162633895874 + 100.0 * 6.294529438018799
Epoch 460, val loss: 1.3631532192230225
Epoch 470, training loss: 630.5853881835938 = 1.265424132347107 + 100.0 * 6.29319953918457
Epoch 470, val loss: 1.345039963722229
Epoch 480, training loss: 630.4257202148438 = 1.241647720336914 + 100.0 * 6.291840553283691
Epoch 480, val loss: 1.326716423034668
Epoch 490, training loss: 630.0543212890625 = 1.2178418636322021 + 100.0 * 6.288364887237549
Epoch 490, val loss: 1.3089371919631958
Epoch 500, training loss: 629.8150024414062 = 1.1941642761230469 + 100.0 * 6.286208629608154
Epoch 500, val loss: 1.291444182395935
Epoch 510, training loss: 629.9962768554688 = 1.1706582307815552 + 100.0 * 6.2882561683654785
Epoch 510, val loss: 1.2741739749908447
Epoch 520, training loss: 629.8046875 = 1.147254467010498 + 100.0 * 6.286573886871338
Epoch 520, val loss: 1.2574032545089722
Epoch 530, training loss: 629.3848266601562 = 1.123948097229004 + 100.0 * 6.282608509063721
Epoch 530, val loss: 1.240977168083191
Epoch 540, training loss: 629.0656127929688 = 1.1011302471160889 + 100.0 * 6.27964448928833
Epoch 540, val loss: 1.2248680591583252
Epoch 550, training loss: 628.909912109375 = 1.0787585973739624 + 100.0 * 6.278311729431152
Epoch 550, val loss: 1.2094215154647827
Epoch 560, training loss: 629.2731323242188 = 1.0567222833633423 + 100.0 * 6.282164573669434
Epoch 560, val loss: 1.1946676969528198
Epoch 570, training loss: 628.8400268554688 = 1.0351719856262207 + 100.0 * 6.278048992156982
Epoch 570, val loss: 1.1806031465530396
Epoch 580, training loss: 628.38037109375 = 1.0140749216079712 + 100.0 * 6.273662567138672
Epoch 580, val loss: 1.1670488119125366
Epoch 590, training loss: 628.2169189453125 = 0.993574857711792 + 100.0 * 6.272233486175537
Epoch 590, val loss: 1.1543049812316895
Epoch 600, training loss: 628.0518188476562 = 0.973608672618866 + 100.0 * 6.270781993865967
Epoch 600, val loss: 1.1422181129455566
Epoch 610, training loss: 628.5857543945312 = 0.9542403221130371 + 100.0 * 6.276315212249756
Epoch 610, val loss: 1.1309486627578735
Epoch 620, training loss: 628.2296752929688 = 0.9348871111869812 + 100.0 * 6.272948265075684
Epoch 620, val loss: 1.119434118270874
Epoch 630, training loss: 627.6886596679688 = 0.9163222312927246 + 100.0 * 6.267723560333252
Epoch 630, val loss: 1.109147548675537
Epoch 640, training loss: 627.51611328125 = 0.898347020149231 + 100.0 * 6.266177177429199
Epoch 640, val loss: 1.0994913578033447
Epoch 650, training loss: 627.3689575195312 = 0.881046712398529 + 100.0 * 6.26487922668457
Epoch 650, val loss: 1.090619683265686
Epoch 660, training loss: 627.2230834960938 = 0.864221453666687 + 100.0 * 6.263588905334473
Epoch 660, val loss: 1.0822428464889526
Epoch 670, training loss: 627.1080932617188 = 0.8478681445121765 + 100.0 * 6.262602806091309
Epoch 670, val loss: 1.074357509613037
Epoch 680, training loss: 628.12109375 = 0.8318795561790466 + 100.0 * 6.272891998291016
Epoch 680, val loss: 1.0667937994003296
Epoch 690, training loss: 627.2306518554688 = 0.8161866664886475 + 100.0 * 6.2641448974609375
Epoch 690, val loss: 1.0595619678497314
Epoch 700, training loss: 626.8075561523438 = 0.8010179996490479 + 100.0 * 6.26006555557251
Epoch 700, val loss: 1.0531249046325684
Epoch 710, training loss: 626.66357421875 = 0.7863385677337646 + 100.0 * 6.258772373199463
Epoch 710, val loss: 1.0471395254135132
Epoch 720, training loss: 626.5392456054688 = 0.7721277475357056 + 100.0 * 6.257670879364014
Epoch 720, val loss: 1.041698932647705
Epoch 730, training loss: 626.436767578125 = 0.7583202719688416 + 100.0 * 6.256784439086914
Epoch 730, val loss: 1.0366202592849731
Epoch 740, training loss: 626.6607666015625 = 0.7448251247406006 + 100.0 * 6.259159564971924
Epoch 740, val loss: 1.031934380531311
Epoch 750, training loss: 626.457275390625 = 0.7315873503684998 + 100.0 * 6.257256984710693
Epoch 750, val loss: 1.0269936323165894
Epoch 760, training loss: 626.2354736328125 = 0.7186480760574341 + 100.0 * 6.255168437957764
Epoch 760, val loss: 1.0228899717330933
Epoch 770, training loss: 626.0438232421875 = 0.706145703792572 + 100.0 * 6.2533769607543945
Epoch 770, val loss: 1.0190412998199463
Epoch 780, training loss: 626.4954223632812 = 0.6940115094184875 + 100.0 * 6.25801420211792
Epoch 780, val loss: 1.0156030654907227
Epoch 790, training loss: 626.0851440429688 = 0.6818272471427917 + 100.0 * 6.254033088684082
Epoch 790, val loss: 1.0120751857757568
Epoch 800, training loss: 625.87451171875 = 0.6701511144638062 + 100.0 * 6.252043724060059
Epoch 800, val loss: 1.0089792013168335
Epoch 810, training loss: 625.6650390625 = 0.6587033867835999 + 100.0 * 6.250063419342041
Epoch 810, val loss: 1.006284236907959
Epoch 820, training loss: 625.5897216796875 = 0.64756178855896 + 100.0 * 6.2494215965271
Epoch 820, val loss: 1.003652811050415
Epoch 830, training loss: 626.810791015625 = 0.6366521716117859 + 100.0 * 6.2617411613464355
Epoch 830, val loss: 1.0013182163238525
Epoch 840, training loss: 625.5452880859375 = 0.6255839467048645 + 100.0 * 6.249197006225586
Epoch 840, val loss: 0.9989116787910461
Epoch 850, training loss: 625.435546875 = 0.6149320006370544 + 100.0 * 6.24820613861084
Epoch 850, val loss: 0.9969369173049927
Epoch 860, training loss: 625.2195434570312 = 0.6045520901679993 + 100.0 * 6.246150016784668
Epoch 860, val loss: 0.995305061340332
Epoch 870, training loss: 625.2311401367188 = 0.5943751335144043 + 100.0 * 6.246367931365967
Epoch 870, val loss: 0.9936505556106567
Epoch 880, training loss: 625.572265625 = 0.584343433380127 + 100.0 * 6.249878883361816
Epoch 880, val loss: 0.9923302531242371
Epoch 890, training loss: 625.0504760742188 = 0.5742402076721191 + 100.0 * 6.244762420654297
Epoch 890, val loss: 0.9905998110771179
Epoch 900, training loss: 624.9534912109375 = 0.56443852186203 + 100.0 * 6.243890285491943
Epoch 900, val loss: 0.9896540641784668
Epoch 910, training loss: 624.8870849609375 = 0.5548660755157471 + 100.0 * 6.243322372436523
Epoch 910, val loss: 0.9888426661491394
Epoch 920, training loss: 624.903076171875 = 0.5454282760620117 + 100.0 * 6.243576526641846
Epoch 920, val loss: 0.9881686568260193
Epoch 930, training loss: 625.11767578125 = 0.5359823703765869 + 100.0 * 6.245816707611084
Epoch 930, val loss: 0.9871814846992493
Epoch 940, training loss: 625.0173950195312 = 0.5266293883323669 + 100.0 * 6.244907855987549
Epoch 940, val loss: 0.9870831370353699
Epoch 950, training loss: 624.7659301757812 = 0.5174290537834167 + 100.0 * 6.2424845695495605
Epoch 950, val loss: 0.9856948256492615
Epoch 960, training loss: 624.5282592773438 = 0.5084559321403503 + 100.0 * 6.240197658538818
Epoch 960, val loss: 0.9860805869102478
Epoch 970, training loss: 624.4046020507812 = 0.4996322691440582 + 100.0 * 6.239049911499023
Epoch 970, val loss: 0.9856861233711243
Epoch 980, training loss: 624.3501586914062 = 0.49096617102622986 + 100.0 * 6.238592147827148
Epoch 980, val loss: 0.9859527349472046
Epoch 990, training loss: 624.8341674804688 = 0.4824383556842804 + 100.0 * 6.2435173988342285
Epoch 990, val loss: 0.9860121011734009
Epoch 1000, training loss: 624.558837890625 = 0.4737951457500458 + 100.0 * 6.24084997177124
Epoch 1000, val loss: 0.9863020181655884
Epoch 1010, training loss: 624.3914184570312 = 0.46530359983444214 + 100.0 * 6.239261150360107
Epoch 1010, val loss: 0.9865704774856567
Epoch 1020, training loss: 624.2174072265625 = 0.457057386636734 + 100.0 * 6.237603664398193
Epoch 1020, val loss: 0.9871739149093628
Epoch 1030, training loss: 624.0789794921875 = 0.44891804456710815 + 100.0 * 6.236300468444824
Epoch 1030, val loss: 0.9880769848823547
Epoch 1040, training loss: 624.2567749023438 = 0.44093772768974304 + 100.0 * 6.238158702850342
Epoch 1040, val loss: 0.9894987940788269
Epoch 1050, training loss: 623.9337158203125 = 0.4329979717731476 + 100.0 * 6.235007286071777
Epoch 1050, val loss: 0.9897258877754211
Epoch 1060, training loss: 624.0422973632812 = 0.4252549111843109 + 100.0 * 6.236170768737793
Epoch 1060, val loss: 0.9911004900932312
Epoch 1070, training loss: 624.2973022460938 = 0.4175876975059509 + 100.0 * 6.238796710968018
Epoch 1070, val loss: 0.9921453595161438
Epoch 1080, training loss: 623.8807373046875 = 0.40991681814193726 + 100.0 * 6.234708309173584
Epoch 1080, val loss: 0.993929922580719
Epoch 1090, training loss: 623.7598266601562 = 0.402468204498291 + 100.0 * 6.2335734367370605
Epoch 1090, val loss: 0.9948948621749878
Epoch 1100, training loss: 623.6268920898438 = 0.3952579200267792 + 100.0 * 6.232316493988037
Epoch 1100, val loss: 0.9971408247947693
Epoch 1110, training loss: 623.591552734375 = 0.38817092776298523 + 100.0 * 6.232033729553223
Epoch 1110, val loss: 0.9988964200019836
Epoch 1120, training loss: 624.361328125 = 0.3811626434326172 + 100.0 * 6.239801406860352
Epoch 1120, val loss: 1.0011194944381714
Epoch 1130, training loss: 623.9415893554688 = 0.37425845861434937 + 100.0 * 6.235672950744629
Epoch 1130, val loss: 1.0032496452331543
Epoch 1140, training loss: 623.5086059570312 = 0.3674151599407196 + 100.0 * 6.231411933898926
Epoch 1140, val loss: 1.0049031972885132
Epoch 1150, training loss: 623.3942260742188 = 0.36081406474113464 + 100.0 * 6.2303338050842285
Epoch 1150, val loss: 1.007832407951355
Epoch 1160, training loss: 623.3833618164062 = 0.3543613851070404 + 100.0 * 6.230289936065674
Epoch 1160, val loss: 1.0101728439331055
Epoch 1170, training loss: 624.2468872070312 = 0.34806129336357117 + 100.0 * 6.238987922668457
Epoch 1170, val loss: 1.012603998184204
Epoch 1180, training loss: 623.47607421875 = 0.34166356921195984 + 100.0 * 6.231344223022461
Epoch 1180, val loss: 1.0158801078796387
Epoch 1190, training loss: 623.2832641601562 = 0.33553460240364075 + 100.0 * 6.229477405548096
Epoch 1190, val loss: 1.0184602737426758
Epoch 1200, training loss: 623.5597534179688 = 0.3295536935329437 + 100.0 * 6.232302188873291
Epoch 1200, val loss: 1.0215704441070557
Epoch 1210, training loss: 623.1504516601562 = 0.32362452149391174 + 100.0 * 6.228268623352051
Epoch 1210, val loss: 1.024971604347229
Epoch 1220, training loss: 623.1065063476562 = 0.3178320527076721 + 100.0 * 6.22788667678833
Epoch 1220, val loss: 1.0280119180679321
Epoch 1230, training loss: 623.2015380859375 = 0.3122293949127197 + 100.0 * 6.228892803192139
Epoch 1230, val loss: 1.0315871238708496
Epoch 1240, training loss: 623.2919921875 = 0.3066490888595581 + 100.0 * 6.229853630065918
Epoch 1240, val loss: 1.0347018241882324
Epoch 1250, training loss: 623.0761108398438 = 0.3010919690132141 + 100.0 * 6.227750301361084
Epoch 1250, val loss: 1.038157343864441
Epoch 1260, training loss: 622.8977661132812 = 0.29578739404678345 + 100.0 * 6.226019859313965
Epoch 1260, val loss: 1.0419985055923462
Epoch 1270, training loss: 623.1912841796875 = 0.29059499502182007 + 100.0 * 6.229007244110107
Epoch 1270, val loss: 1.0452028512954712
Epoch 1280, training loss: 622.9971313476562 = 0.28538379073143005 + 100.0 * 6.227117538452148
Epoch 1280, val loss: 1.0491408109664917
Epoch 1290, training loss: 623.0033569335938 = 0.280317485332489 + 100.0 * 6.227230072021484
Epoch 1290, val loss: 1.0532686710357666
Epoch 1300, training loss: 622.835205078125 = 0.2753223776817322 + 100.0 * 6.2255988121032715
Epoch 1300, val loss: 1.0569730997085571
Epoch 1310, training loss: 622.6964111328125 = 0.2704651355743408 + 100.0 * 6.224259376525879
Epoch 1310, val loss: 1.0612157583236694
Epoch 1320, training loss: 623.182861328125 = 0.2657596170902252 + 100.0 * 6.229171276092529
Epoch 1320, val loss: 1.0651168823242188
Epoch 1330, training loss: 622.8919677734375 = 0.2609679698944092 + 100.0 * 6.226309776306152
Epoch 1330, val loss: 1.0695381164550781
Epoch 1340, training loss: 622.6900634765625 = 0.2562803030014038 + 100.0 * 6.224337577819824
Epoch 1340, val loss: 1.0733847618103027
Epoch 1350, training loss: 622.5489501953125 = 0.2517849802970886 + 100.0 * 6.2229719161987305
Epoch 1350, val loss: 1.078046441078186
Epoch 1360, training loss: 622.6148071289062 = 0.24739646911621094 + 100.0 * 6.223674297332764
Epoch 1360, val loss: 1.0824058055877686
Epoch 1370, training loss: 622.9085693359375 = 0.24302369356155396 + 100.0 * 6.22665548324585
Epoch 1370, val loss: 1.0868606567382812
Epoch 1380, training loss: 622.5285034179688 = 0.23864834010601044 + 100.0 * 6.222898483276367
Epoch 1380, val loss: 1.0911941528320312
Epoch 1390, training loss: 622.3839721679688 = 0.2344258576631546 + 100.0 * 6.221495151519775
Epoch 1390, val loss: 1.0957510471343994
Epoch 1400, training loss: 622.4977416992188 = 0.2302922010421753 + 100.0 * 6.222674369812012
Epoch 1400, val loss: 1.1004722118377686
Epoch 1410, training loss: 622.4884643554688 = 0.22620749473571777 + 100.0 * 6.222622871398926
Epoch 1410, val loss: 1.1048513650894165
Epoch 1420, training loss: 622.8512573242188 = 0.22216925024986267 + 100.0 * 6.226290702819824
Epoch 1420, val loss: 1.1096998453140259
Epoch 1430, training loss: 622.285888671875 = 0.2180815190076828 + 100.0 * 6.220678329467773
Epoch 1430, val loss: 1.11442232131958
Epoch 1440, training loss: 622.181396484375 = 0.2142058163881302 + 100.0 * 6.219671726226807
Epoch 1440, val loss: 1.1191370487213135
Epoch 1450, training loss: 622.1234741210938 = 0.210422083735466 + 100.0 * 6.219130516052246
Epoch 1450, val loss: 1.1242427825927734
Epoch 1460, training loss: 622.1724853515625 = 0.20671652257442474 + 100.0 * 6.2196574211120605
Epoch 1460, val loss: 1.1293178796768188
Epoch 1470, training loss: 622.56201171875 = 0.20307256281375885 + 100.0 * 6.2235894203186035
Epoch 1470, val loss: 1.1340999603271484
Epoch 1480, training loss: 622.3721923828125 = 0.19937239587306976 + 100.0 * 6.2217278480529785
Epoch 1480, val loss: 1.1382403373718262
Epoch 1490, training loss: 622.110107421875 = 0.195756733417511 + 100.0 * 6.219143867492676
Epoch 1490, val loss: 1.1439614295959473
Epoch 1500, training loss: 622.0643310546875 = 0.1922430843114853 + 100.0 * 6.21872091293335
Epoch 1500, val loss: 1.1491140127182007
Epoch 1510, training loss: 622.1885375976562 = 0.18881160020828247 + 100.0 * 6.219997406005859
Epoch 1510, val loss: 1.154643177986145
Epoch 1520, training loss: 622.0186767578125 = 0.1854000985622406 + 100.0 * 6.218332767486572
Epoch 1520, val loss: 1.1586943864822388
Epoch 1530, training loss: 622.25830078125 = 0.18207596242427826 + 100.0 * 6.220762252807617
Epoch 1530, val loss: 1.1649292707443237
Epoch 1540, training loss: 621.849609375 = 0.17878708243370056 + 100.0 * 6.216708660125732
Epoch 1540, val loss: 1.1688485145568848
Epoch 1550, training loss: 621.8319702148438 = 0.17557646334171295 + 100.0 * 6.216563701629639
Epoch 1550, val loss: 1.174988031387329
Epoch 1560, training loss: 621.7687377929688 = 0.17245103418827057 + 100.0 * 6.215962886810303
Epoch 1560, val loss: 1.1797950267791748
Epoch 1570, training loss: 622.9972534179688 = 0.16946041584014893 + 100.0 * 6.228278160095215
Epoch 1570, val loss: 1.185190200805664
Epoch 1580, training loss: 622.04052734375 = 0.16624070703983307 + 100.0 * 6.218742847442627
Epoch 1580, val loss: 1.1906622648239136
Epoch 1590, training loss: 621.6782836914062 = 0.16325706243515015 + 100.0 * 6.215150356292725
Epoch 1590, val loss: 1.1956450939178467
Epoch 1600, training loss: 621.5819702148438 = 0.16035138070583344 + 100.0 * 6.214216232299805
Epoch 1600, val loss: 1.2017252445220947
Epoch 1610, training loss: 621.575927734375 = 0.15751896798610687 + 100.0 * 6.214183807373047
Epoch 1610, val loss: 1.207123041152954
Epoch 1620, training loss: 621.86181640625 = 0.15475325286388397 + 100.0 * 6.217071056365967
Epoch 1620, val loss: 1.2129653692245483
Epoch 1630, training loss: 621.7018432617188 = 0.15195882320404053 + 100.0 * 6.215498447418213
Epoch 1630, val loss: 1.218093752861023
Epoch 1640, training loss: 621.9634399414062 = 0.1492045372724533 + 100.0 * 6.218142032623291
Epoch 1640, val loss: 1.2234159708023071
Epoch 1650, training loss: 621.7195434570312 = 0.14645785093307495 + 100.0 * 6.215730667114258
Epoch 1650, val loss: 1.2292121648788452
Epoch 1660, training loss: 621.4899291992188 = 0.1438521146774292 + 100.0 * 6.213460922241211
Epoch 1660, val loss: 1.2346240282058716
Epoch 1670, training loss: 621.4080200195312 = 0.1412944346666336 + 100.0 * 6.212666988372803
Epoch 1670, val loss: 1.2405855655670166
Epoch 1680, training loss: 621.9202880859375 = 0.13881845772266388 + 100.0 * 6.217814922332764
Epoch 1680, val loss: 1.2464098930358887
Epoch 1690, training loss: 621.4892578125 = 0.1362837702035904 + 100.0 * 6.213529586791992
Epoch 1690, val loss: 1.2513563632965088
Epoch 1700, training loss: 621.3479614257812 = 0.13381659984588623 + 100.0 * 6.212141990661621
Epoch 1700, val loss: 1.2571948766708374
Epoch 1710, training loss: 621.4244995117188 = 0.1314251869916916 + 100.0 * 6.212930679321289
Epoch 1710, val loss: 1.2627804279327393
Epoch 1720, training loss: 621.5277709960938 = 0.12907403707504272 + 100.0 * 6.213986873626709
Epoch 1720, val loss: 1.2685028314590454
Epoch 1730, training loss: 621.3257446289062 = 0.1267360895872116 + 100.0 * 6.2119903564453125
Epoch 1730, val loss: 1.274487853050232
Epoch 1740, training loss: 621.8853759765625 = 0.1245085746049881 + 100.0 * 6.21760892868042
Epoch 1740, val loss: 1.2795519828796387
Epoch 1750, training loss: 621.3886108398438 = 0.12220167368650436 + 100.0 * 6.2126641273498535
Epoch 1750, val loss: 1.2856063842773438
Epoch 1760, training loss: 621.2555541992188 = 0.11996489763259888 + 100.0 * 6.211355686187744
Epoch 1760, val loss: 1.2910469770431519
Epoch 1770, training loss: 621.184326171875 = 0.11783277243375778 + 100.0 * 6.210664749145508
Epoch 1770, val loss: 1.2970995903015137
Epoch 1780, training loss: 621.12060546875 = 0.11573676019906998 + 100.0 * 6.210048675537109
Epoch 1780, val loss: 1.3029940128326416
Epoch 1790, training loss: 621.1709594726562 = 0.11368916928768158 + 100.0 * 6.210572242736816
Epoch 1790, val loss: 1.3088761568069458
Epoch 1800, training loss: 621.9954833984375 = 0.11168460547924042 + 100.0 * 6.218837738037109
Epoch 1800, val loss: 1.3140424489974976
Epoch 1810, training loss: 621.2251586914062 = 0.10960174351930618 + 100.0 * 6.211155414581299
Epoch 1810, val loss: 1.3202898502349854
Epoch 1820, training loss: 621.10888671875 = 0.10760688036680222 + 100.0 * 6.210012912750244
Epoch 1820, val loss: 1.3251490592956543
Epoch 1830, training loss: 621.0628662109375 = 0.10568801313638687 + 100.0 * 6.209571361541748
Epoch 1830, val loss: 1.3318268060684204
Epoch 1840, training loss: 621.2412719726562 = 0.10383916646242142 + 100.0 * 6.211374282836914
Epoch 1840, val loss: 1.3368839025497437
Epoch 1850, training loss: 621.2977905273438 = 0.10195619612932205 + 100.0 * 6.211958885192871
Epoch 1850, val loss: 1.3430378437042236
Epoch 1860, training loss: 621.014892578125 = 0.10007604211568832 + 100.0 * 6.209147930145264
Epoch 1860, val loss: 1.348511815071106
Epoch 1870, training loss: 620.9258422851562 = 0.09829212725162506 + 100.0 * 6.20827579498291
Epoch 1870, val loss: 1.354422688484192
Epoch 1880, training loss: 620.917236328125 = 0.09654656797647476 + 100.0 * 6.208206653594971
Epoch 1880, val loss: 1.360489845275879
Epoch 1890, training loss: 621.313720703125 = 0.09485593438148499 + 100.0 * 6.212188720703125
Epoch 1890, val loss: 1.3657935857772827
Epoch 1900, training loss: 620.9302368164062 = 0.09313248097896576 + 100.0 * 6.208371162414551
Epoch 1900, val loss: 1.3720735311508179
Epoch 1910, training loss: 621.0615844726562 = 0.09148988872766495 + 100.0 * 6.209701061248779
Epoch 1910, val loss: 1.376587986946106
Epoch 1920, training loss: 621.147216796875 = 0.08983336389064789 + 100.0 * 6.210573673248291
Epoch 1920, val loss: 1.3832887411117554
Epoch 1930, training loss: 621.1353149414062 = 0.08824631571769714 + 100.0 * 6.210470676422119
Epoch 1930, val loss: 1.3888009786605835
Epoch 1940, training loss: 620.8616333007812 = 0.0866529569029808 + 100.0 * 6.207749366760254
Epoch 1940, val loss: 1.3941173553466797
Epoch 1950, training loss: 620.7987670898438 = 0.08511244505643845 + 100.0 * 6.207136631011963
Epoch 1950, val loss: 1.4000976085662842
Epoch 1960, training loss: 620.7431640625 = 0.08361004292964935 + 100.0 * 6.206595420837402
Epoch 1960, val loss: 1.405892252922058
Epoch 1970, training loss: 621.03564453125 = 0.082151398062706 + 100.0 * 6.209535121917725
Epoch 1970, val loss: 1.4117051362991333
Epoch 1980, training loss: 621.0482177734375 = 0.08067921549081802 + 100.0 * 6.209675312042236
Epoch 1980, val loss: 1.4165624380111694
Epoch 1990, training loss: 620.7855224609375 = 0.07920080423355103 + 100.0 * 6.207062721252441
Epoch 1990, val loss: 1.4230083227157593
Epoch 2000, training loss: 620.8013916015625 = 0.07779133319854736 + 100.0 * 6.207235813140869
Epoch 2000, val loss: 1.4277551174163818
Epoch 2010, training loss: 621.2437133789062 = 0.07642781734466553 + 100.0 * 6.211673259735107
Epoch 2010, val loss: 1.43342125415802
Epoch 2020, training loss: 620.6513671875 = 0.0750301256775856 + 100.0 * 6.205763339996338
Epoch 2020, val loss: 1.4392517805099487
Epoch 2030, training loss: 620.6357421875 = 0.07370548695325851 + 100.0 * 6.205620288848877
Epoch 2030, val loss: 1.4449951648712158
Epoch 2040, training loss: 620.6190185546875 = 0.07242247462272644 + 100.0 * 6.205465793609619
Epoch 2040, val loss: 1.450332760810852
Epoch 2050, training loss: 621.159423828125 = 0.07116223871707916 + 100.0 * 6.210882663726807
Epoch 2050, val loss: 1.455570936203003
Epoch 2060, training loss: 621.0791625976562 = 0.06988630443811417 + 100.0 * 6.210093021392822
Epoch 2060, val loss: 1.4630138874053955
Epoch 2070, training loss: 620.775390625 = 0.06862114369869232 + 100.0 * 6.207067966461182
Epoch 2070, val loss: 1.4658424854278564
Epoch 2080, training loss: 620.5848388671875 = 0.06740476936101913 + 100.0 * 6.205174446105957
Epoch 2080, val loss: 1.4726396799087524
Epoch 2090, training loss: 620.50244140625 = 0.06623281538486481 + 100.0 * 6.204362392425537
Epoch 2090, val loss: 1.4779621362686157
Epoch 2100, training loss: 620.5238647460938 = 0.06509564816951752 + 100.0 * 6.204587459564209
Epoch 2100, val loss: 1.4836506843566895
Epoch 2110, training loss: 621.6239013671875 = 0.06399404257535934 + 100.0 * 6.215599536895752
Epoch 2110, val loss: 1.488433837890625
Epoch 2120, training loss: 620.8209838867188 = 0.06280652433633804 + 100.0 * 6.207581520080566
Epoch 2120, val loss: 1.4944120645523071
Epoch 2130, training loss: 620.5219116210938 = 0.06169401481747627 + 100.0 * 6.204601764678955
Epoch 2130, val loss: 1.4991792440414429
Epoch 2140, training loss: 620.4025268554688 = 0.06062118709087372 + 100.0 * 6.203419208526611
Epoch 2140, val loss: 1.5053002834320068
Epoch 2150, training loss: 620.3997802734375 = 0.05958247557282448 + 100.0 * 6.203402042388916
Epoch 2150, val loss: 1.5109031200408936
Epoch 2160, training loss: 621.2723388671875 = 0.05857423320412636 + 100.0 * 6.212137699127197
Epoch 2160, val loss: 1.5168519020080566
Epoch 2170, training loss: 620.5986938476562 = 0.05754414573311806 + 100.0 * 6.205411434173584
Epoch 2170, val loss: 1.5204843282699585
Epoch 2180, training loss: 620.4345092773438 = 0.05652377009391785 + 100.0 * 6.203779697418213
Epoch 2180, val loss: 1.5264798402786255
Epoch 2190, training loss: 620.3480834960938 = 0.05556163191795349 + 100.0 * 6.202925682067871
Epoch 2190, val loss: 1.5312979221343994
Epoch 2200, training loss: 620.5810546875 = 0.05463472381234169 + 100.0 * 6.205264091491699
Epoch 2200, val loss: 1.5368989706039429
Epoch 2210, training loss: 620.4862060546875 = 0.05368680879473686 + 100.0 * 6.204325199127197
Epoch 2210, val loss: 1.5419957637786865
Epoch 2220, training loss: 620.2958984375 = 0.05273783951997757 + 100.0 * 6.202431678771973
Epoch 2220, val loss: 1.5467156171798706
Epoch 2230, training loss: 620.2520751953125 = 0.05184347182512283 + 100.0 * 6.20200252532959
Epoch 2230, val loss: 1.552196741104126
Epoch 2240, training loss: 620.381103515625 = 0.0509754940867424 + 100.0 * 6.203301429748535
Epoch 2240, val loss: 1.557852029800415
Epoch 2250, training loss: 620.6312866210938 = 0.050113581120967865 + 100.0 * 6.205811500549316
Epoch 2250, val loss: 1.5624138116836548
Epoch 2260, training loss: 620.3863525390625 = 0.04926849156618118 + 100.0 * 6.203371047973633
Epoch 2260, val loss: 1.567004919052124
Epoch 2270, training loss: 620.3558959960938 = 0.048439767211675644 + 100.0 * 6.2030744552612305
Epoch 2270, val loss: 1.5722699165344238
Epoch 2280, training loss: 620.5232543945312 = 0.047632601112127304 + 100.0 * 6.204756259918213
Epoch 2280, val loss: 1.5771846771240234
Epoch 2290, training loss: 620.20654296875 = 0.04682624712586403 + 100.0 * 6.201597213745117
Epoch 2290, val loss: 1.582277536392212
Epoch 2300, training loss: 620.4775390625 = 0.046062394976615906 + 100.0 * 6.204314708709717
Epoch 2300, val loss: 1.5880448818206787
Epoch 2310, training loss: 620.283935546875 = 0.04528779909014702 + 100.0 * 6.202386379241943
Epoch 2310, val loss: 1.5920236110687256
Epoch 2320, training loss: 620.1891479492188 = 0.044535815715789795 + 100.0 * 6.201446056365967
Epoch 2320, val loss: 1.5976368188858032
Epoch 2330, training loss: 620.2318115234375 = 0.04380499944090843 + 100.0 * 6.201879978179932
Epoch 2330, val loss: 1.6027987003326416
Epoch 2340, training loss: 620.6007690429688 = 0.043091122061014175 + 100.0 * 6.2055768966674805
Epoch 2340, val loss: 1.6074429750442505
Epoch 2350, training loss: 620.2993774414062 = 0.04238027706742287 + 100.0 * 6.20257043838501
Epoch 2350, val loss: 1.6122208833694458
Epoch 2360, training loss: 620.08447265625 = 0.0416850820183754 + 100.0 * 6.200428009033203
Epoch 2360, val loss: 1.6160837411880493
Epoch 2370, training loss: 620.0501708984375 = 0.041011396795511246 + 100.0 * 6.200091361999512
Epoch 2370, val loss: 1.6213457584381104
Epoch 2380, training loss: 620.0245361328125 = 0.040361106395721436 + 100.0 * 6.1998419761657715
Epoch 2380, val loss: 1.6264665126800537
Epoch 2390, training loss: 620.5842895507812 = 0.039738550782203674 + 100.0 * 6.205445289611816
Epoch 2390, val loss: 1.6307588815689087
Epoch 2400, training loss: 620.2736206054688 = 0.03908673673868179 + 100.0 * 6.202345848083496
Epoch 2400, val loss: 1.6353129148483276
Epoch 2410, training loss: 620.4690551757812 = 0.038458216935396194 + 100.0 * 6.204306125640869
Epoch 2410, val loss: 1.6404728889465332
Epoch 2420, training loss: 620.0779418945312 = 0.03783110901713371 + 100.0 * 6.2004008293151855
Epoch 2420, val loss: 1.64466392993927
Epoch 2430, training loss: 620.0516357421875 = 0.03722964599728584 + 100.0 * 6.200143814086914
Epoch 2430, val loss: 1.6494020223617554
Epoch 2440, training loss: 620.63671875 = 0.036651067435741425 + 100.0 * 6.206001281738281
Epoch 2440, val loss: 1.6546772718429565
Epoch 2450, training loss: 619.9583740234375 = 0.03606964275240898 + 100.0 * 6.199223041534424
Epoch 2450, val loss: 1.657857894897461
Epoch 2460, training loss: 619.9260864257812 = 0.03551026061177254 + 100.0 * 6.1989054679870605
Epoch 2460, val loss: 1.662261724472046
Epoch 2470, training loss: 619.9583740234375 = 0.034968048334121704 + 100.0 * 6.1992340087890625
Epoch 2470, val loss: 1.667172908782959
Epoch 2480, training loss: 620.1632690429688 = 0.03444821015000343 + 100.0 * 6.20128870010376
Epoch 2480, val loss: 1.6712793111801147
Epoch 2490, training loss: 620.1058959960938 = 0.03391717001795769 + 100.0 * 6.200719356536865
Epoch 2490, val loss: 1.6754268407821655
Epoch 2500, training loss: 620.1539306640625 = 0.03338823467493057 + 100.0 * 6.201205730438232
Epoch 2500, val loss: 1.6808627843856812
Epoch 2510, training loss: 620.0055541992188 = 0.03287149593234062 + 100.0 * 6.199726581573486
Epoch 2510, val loss: 1.6842445135116577
Epoch 2520, training loss: 620.0441284179688 = 0.03238055109977722 + 100.0 * 6.200117111206055
Epoch 2520, val loss: 1.688746690750122
Epoch 2530, training loss: 620.1881103515625 = 0.03190198540687561 + 100.0 * 6.20156192779541
Epoch 2530, val loss: 1.6933412551879883
Epoch 2540, training loss: 619.874267578125 = 0.0314176008105278 + 100.0 * 6.198428153991699
Epoch 2540, val loss: 1.6973260641098022
Epoch 2550, training loss: 619.8463134765625 = 0.030955979600548744 + 100.0 * 6.198153972625732
Epoch 2550, val loss: 1.7017699480056763
Epoch 2560, training loss: 620.0442504882812 = 0.03051631525158882 + 100.0 * 6.200137615203857
Epoch 2560, val loss: 1.7060513496398926
Epoch 2570, training loss: 620.1296997070312 = 0.030068213120102882 + 100.0 * 6.200996398925781
Epoch 2570, val loss: 1.7094671726226807
Epoch 2580, training loss: 619.9066772460938 = 0.029604274779558182 + 100.0 * 6.198770523071289
Epoch 2580, val loss: 1.7128232717514038
Epoch 2590, training loss: 619.7909545898438 = 0.0291557889431715 + 100.0 * 6.197617530822754
Epoch 2590, val loss: 1.7180805206298828
Epoch 2600, training loss: 619.6989135742188 = 0.028738487511873245 + 100.0 * 6.196701526641846
Epoch 2600, val loss: 1.722029685974121
Epoch 2610, training loss: 619.699462890625 = 0.028336606919765472 + 100.0 * 6.196711540222168
Epoch 2610, val loss: 1.7261450290679932
Epoch 2620, training loss: 619.8986206054688 = 0.027948642149567604 + 100.0 * 6.19870662689209
Epoch 2620, val loss: 1.7297736406326294
Epoch 2630, training loss: 620.2258911132812 = 0.027548573911190033 + 100.0 * 6.20198392868042
Epoch 2630, val loss: 1.7334352731704712
Epoch 2640, training loss: 620.322265625 = 0.027134660631418228 + 100.0 * 6.202950954437256
Epoch 2640, val loss: 1.7387279272079468
Epoch 2650, training loss: 619.8599853515625 = 0.026747120544314384 + 100.0 * 6.198332786560059
Epoch 2650, val loss: 1.7404569387435913
Epoch 2660, training loss: 619.7329711914062 = 0.026367906481027603 + 100.0 * 6.197066307067871
Epoch 2660, val loss: 1.7462910413742065
Epoch 2670, training loss: 619.8348999023438 = 0.026012009009718895 + 100.0 * 6.198089122772217
Epoch 2670, val loss: 1.7497742176055908
Epoch 2680, training loss: 619.9972534179688 = 0.025645123794674873 + 100.0 * 6.199716091156006
Epoch 2680, val loss: 1.7535649538040161
Epoch 2690, training loss: 619.6708984375 = 0.02527838759124279 + 100.0 * 6.196456432342529
Epoch 2690, val loss: 1.7561177015304565
Epoch 2700, training loss: 619.614501953125 = 0.024934420362114906 + 100.0 * 6.195895195007324
Epoch 2700, val loss: 1.760596513748169
Epoch 2710, training loss: 619.6017456054688 = 0.024602137506008148 + 100.0 * 6.195771217346191
Epoch 2710, val loss: 1.7644896507263184
Epoch 2720, training loss: 619.8237915039062 = 0.024285845458507538 + 100.0 * 6.197995185852051
Epoch 2720, val loss: 1.7681704759597778
Epoch 2730, training loss: 619.9266967773438 = 0.023953381925821304 + 100.0 * 6.1990275382995605
Epoch 2730, val loss: 1.7711178064346313
Epoch 2740, training loss: 619.8681030273438 = 0.02361595258116722 + 100.0 * 6.198444843292236
Epoch 2740, val loss: 1.77469801902771
Epoch 2750, training loss: 619.7501831054688 = 0.02330188825726509 + 100.0 * 6.197268486022949
Epoch 2750, val loss: 1.7788808345794678
Epoch 2760, training loss: 619.6802368164062 = 0.022987842559814453 + 100.0 * 6.196572303771973
Epoch 2760, val loss: 1.7830673456192017
Epoch 2770, training loss: 619.83154296875 = 0.022693803533911705 + 100.0 * 6.198088645935059
Epoch 2770, val loss: 1.7870657444000244
Epoch 2780, training loss: 619.7893676757812 = 0.02239508554339409 + 100.0 * 6.197669506072998
Epoch 2780, val loss: 1.788894772529602
Epoch 2790, training loss: 619.624267578125 = 0.02209625206887722 + 100.0 * 6.196022033691406
Epoch 2790, val loss: 1.7925254106521606
Epoch 2800, training loss: 619.5672607421875 = 0.0218052938580513 + 100.0 * 6.1954545974731445
Epoch 2800, val loss: 1.796147346496582
Epoch 2810, training loss: 619.488525390625 = 0.02152656950056553 + 100.0 * 6.194669723510742
Epoch 2810, val loss: 1.799627661705017
Epoch 2820, training loss: 619.8743286132812 = 0.02126293256878853 + 100.0 * 6.198530673980713
Epoch 2820, val loss: 1.8036079406738281
Epoch 2830, training loss: 619.85791015625 = 0.020977778360247612 + 100.0 * 6.198369026184082
Epoch 2830, val loss: 1.8069654703140259
Epoch 2840, training loss: 619.5940551757812 = 0.02070075087249279 + 100.0 * 6.195733547210693
Epoch 2840, val loss: 1.8097422122955322
Epoch 2850, training loss: 619.5350952148438 = 0.020435113459825516 + 100.0 * 6.195146560668945
Epoch 2850, val loss: 1.8130089044570923
Epoch 2860, training loss: 619.4395141601562 = 0.020181510597467422 + 100.0 * 6.194192886352539
Epoch 2860, val loss: 1.8168342113494873
Epoch 2870, training loss: 619.488037109375 = 0.019938481971621513 + 100.0 * 6.194680690765381
Epoch 2870, val loss: 1.8205372095108032
Epoch 2880, training loss: 620.2644653320312 = 0.019703136757016182 + 100.0 * 6.202447414398193
Epoch 2880, val loss: 1.8241432905197144
Epoch 2890, training loss: 619.8512573242188 = 0.019442960619926453 + 100.0 * 6.198318004608154
Epoch 2890, val loss: 1.8259012699127197
Epoch 2900, training loss: 619.7227172851562 = 0.0191966500133276 + 100.0 * 6.19703483581543
Epoch 2900, val loss: 1.8300433158874512
Epoch 2910, training loss: 619.506591796875 = 0.018960820510983467 + 100.0 * 6.194876194000244
Epoch 2910, val loss: 1.832234501838684
Epoch 2920, training loss: 619.4315185546875 = 0.01873280480504036 + 100.0 * 6.194128036499023
Epoch 2920, val loss: 1.8359421491622925
Epoch 2930, training loss: 619.439697265625 = 0.018512165173888206 + 100.0 * 6.194211959838867
Epoch 2930, val loss: 1.8390108346939087
Epoch 2940, training loss: 619.5963134765625 = 0.018292762339115143 + 100.0 * 6.195779800415039
Epoch 2940, val loss: 1.842551827430725
Epoch 2950, training loss: 620.1611328125 = 0.01807563751935959 + 100.0 * 6.201430797576904
Epoch 2950, val loss: 1.8464529514312744
Epoch 2960, training loss: 619.50244140625 = 0.01785486750304699 + 100.0 * 6.194845676422119
Epoch 2960, val loss: 1.8469263315200806
Epoch 2970, training loss: 619.3331909179688 = 0.017633600160479546 + 100.0 * 6.193155765533447
Epoch 2970, val loss: 1.8510276079177856
Epoch 2980, training loss: 619.307373046875 = 0.017433544620871544 + 100.0 * 6.192899227142334
Epoch 2980, val loss: 1.8540083169937134
Epoch 2990, training loss: 620.0316772460938 = 0.017241884022951126 + 100.0 * 6.2001447677612305
Epoch 2990, val loss: 1.8570737838745117
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6555555555555556
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 861.6337280273438 = 1.9468963146209717 + 100.0 * 8.596868515014648
Epoch 0, val loss: 1.9566577672958374
Epoch 10, training loss: 861.5700073242188 = 1.9388458728790283 + 100.0 * 8.596311569213867
Epoch 10, val loss: 1.9483296871185303
Epoch 20, training loss: 861.1724243164062 = 1.9287556409835815 + 100.0 * 8.592436790466309
Epoch 20, val loss: 1.9378106594085693
Epoch 30, training loss: 858.6279907226562 = 1.9157947301864624 + 100.0 * 8.567122459411621
Epoch 30, val loss: 1.9242666959762573
Epoch 40, training loss: 846.0313110351562 = 1.8995237350463867 + 100.0 * 8.441317558288574
Epoch 40, val loss: 1.9073299169540405
Epoch 50, training loss: 793.2821655273438 = 1.882554292678833 + 100.0 * 7.91399621963501
Epoch 50, val loss: 1.8890035152435303
Epoch 60, training loss: 742.1814575195312 = 1.8659265041351318 + 100.0 * 7.40315580368042
Epoch 60, val loss: 1.871867299079895
Epoch 70, training loss: 716.788818359375 = 1.8528093099594116 + 100.0 * 7.149360656738281
Epoch 70, val loss: 1.858302116394043
Epoch 80, training loss: 707.7694091796875 = 1.8391307592391968 + 100.0 * 7.059303283691406
Epoch 80, val loss: 1.8442426919937134
Epoch 90, training loss: 698.1444091796875 = 1.8273327350616455 + 100.0 * 6.963170528411865
Epoch 90, val loss: 1.8323382139205933
Epoch 100, training loss: 686.2557983398438 = 1.8177099227905273 + 100.0 * 6.844380855560303
Epoch 100, val loss: 1.8222380876541138
Epoch 110, training loss: 676.3783569335938 = 1.8089808225631714 + 100.0 * 6.745694160461426
Epoch 110, val loss: 1.8130369186401367
Epoch 120, training loss: 671.1632080078125 = 1.8007694482803345 + 100.0 * 6.693624496459961
Epoch 120, val loss: 1.8043047189712524
Epoch 130, training loss: 667.2514038085938 = 1.7928719520568848 + 100.0 * 6.654585361480713
Epoch 130, val loss: 1.7960230112075806
Epoch 140, training loss: 663.6224975585938 = 1.7853419780731201 + 100.0 * 6.618371486663818
Epoch 140, val loss: 1.7882933616638184
Epoch 150, training loss: 659.8543090820312 = 1.777941107749939 + 100.0 * 6.580763816833496
Epoch 150, val loss: 1.7809927463531494
Epoch 160, training loss: 656.87060546875 = 1.770529866218567 + 100.0 * 6.551001071929932
Epoch 160, val loss: 1.773636817932129
Epoch 170, training loss: 654.056884765625 = 1.7626655101776123 + 100.0 * 6.522942066192627
Epoch 170, val loss: 1.7660126686096191
Epoch 180, training loss: 651.8229370117188 = 1.7540940046310425 + 100.0 * 6.500688552856445
Epoch 180, val loss: 1.7575746774673462
Epoch 190, training loss: 649.9480590820312 = 1.7446013689041138 + 100.0 * 6.482034206390381
Epoch 190, val loss: 1.7485296726226807
Epoch 200, training loss: 648.5174560546875 = 1.7340739965438843 + 100.0 * 6.467833995819092
Epoch 200, val loss: 1.7386120557785034
Epoch 210, training loss: 646.8648681640625 = 1.7225782871246338 + 100.0 * 6.451422691345215
Epoch 210, val loss: 1.7278181314468384
Epoch 220, training loss: 645.7054443359375 = 1.7099905014038086 + 100.0 * 6.43995475769043
Epoch 220, val loss: 1.7161121368408203
Epoch 230, training loss: 644.8020629882812 = 1.6962796449661255 + 100.0 * 6.431057929992676
Epoch 230, val loss: 1.7035000324249268
Epoch 240, training loss: 643.9241943359375 = 1.6811048984527588 + 100.0 * 6.422430992126465
Epoch 240, val loss: 1.6896682977676392
Epoch 250, training loss: 643.1217041015625 = 1.6646920442581177 + 100.0 * 6.414570331573486
Epoch 250, val loss: 1.6746420860290527
Epoch 260, training loss: 642.3883056640625 = 1.647038459777832 + 100.0 * 6.407413005828857
Epoch 260, val loss: 1.65849769115448
Epoch 270, training loss: 641.7115478515625 = 1.6283267736434937 + 100.0 * 6.400831699371338
Epoch 270, val loss: 1.6413757801055908
Epoch 280, training loss: 641.7416381835938 = 1.6085296869277954 + 100.0 * 6.401330947875977
Epoch 280, val loss: 1.6233057975769043
Epoch 290, training loss: 640.7244262695312 = 1.587154507637024 + 100.0 * 6.3913726806640625
Epoch 290, val loss: 1.6041719913482666
Epoch 300, training loss: 639.9423828125 = 1.5651378631591797 + 100.0 * 6.383772850036621
Epoch 300, val loss: 1.584302544593811
Epoch 310, training loss: 639.4113159179688 = 1.5422931909561157 + 100.0 * 6.378690242767334
Epoch 310, val loss: 1.5637502670288086
Epoch 320, training loss: 638.863037109375 = 1.5186163187026978 + 100.0 * 6.373444080352783
Epoch 320, val loss: 1.5428194999694824
Epoch 330, training loss: 638.3909912109375 = 1.494401216506958 + 100.0 * 6.368966102600098
Epoch 330, val loss: 1.5214163064956665
Epoch 340, training loss: 637.8775024414062 = 1.4696182012557983 + 100.0 * 6.364078998565674
Epoch 340, val loss: 1.4995859861373901
Epoch 350, training loss: 637.331787109375 = 1.44435715675354 + 100.0 * 6.358874320983887
Epoch 350, val loss: 1.4777157306671143
Epoch 360, training loss: 636.831787109375 = 1.419011116027832 + 100.0 * 6.354127883911133
Epoch 360, val loss: 1.4558898210525513
Epoch 370, training loss: 636.4204711914062 = 1.3936306238174438 + 100.0 * 6.350268363952637
Epoch 370, val loss: 1.4342113733291626
Epoch 380, training loss: 636.1978759765625 = 1.3679041862487793 + 100.0 * 6.348299503326416
Epoch 380, val loss: 1.4123016595840454
Epoch 390, training loss: 635.4395751953125 = 1.3423166275024414 + 100.0 * 6.340972423553467
Epoch 390, val loss: 1.3908244371414185
Epoch 400, training loss: 635.0460815429688 = 1.316901683807373 + 100.0 * 6.337291717529297
Epoch 400, val loss: 1.369721531867981
Epoch 410, training loss: 634.727294921875 = 1.291664958000183 + 100.0 * 6.33435583114624
Epoch 410, val loss: 1.3488783836364746
Epoch 420, training loss: 634.2396240234375 = 1.2665600776672363 + 100.0 * 6.329730987548828
Epoch 420, val loss: 1.3284657001495361
Epoch 430, training loss: 633.8095092773438 = 1.241806983947754 + 100.0 * 6.325676918029785
Epoch 430, val loss: 1.3084205389022827
Epoch 440, training loss: 633.5079345703125 = 1.217586636543274 + 100.0 * 6.322903633117676
Epoch 440, val loss: 1.2889137268066406
Epoch 450, training loss: 633.281982421875 = 1.1937497854232788 + 100.0 * 6.320882320404053
Epoch 450, val loss: 1.269912838935852
Epoch 460, training loss: 632.8063354492188 = 1.1701695919036865 + 100.0 * 6.316361427307129
Epoch 460, val loss: 1.2514595985412598
Epoch 470, training loss: 632.5111694335938 = 1.1472270488739014 + 100.0 * 6.3136396408081055
Epoch 470, val loss: 1.2336995601654053
Epoch 480, training loss: 632.1464233398438 = 1.1248515844345093 + 100.0 * 6.310215473175049
Epoch 480, val loss: 1.2166157960891724
Epoch 490, training loss: 632.5426025390625 = 1.103194236755371 + 100.0 * 6.314393997192383
Epoch 490, val loss: 1.200088620185852
Epoch 500, training loss: 631.7448120117188 = 1.0815284252166748 + 100.0 * 6.3066325187683105
Epoch 500, val loss: 1.18406081199646
Epoch 510, training loss: 631.3653564453125 = 1.0608211755752563 + 100.0 * 6.303045749664307
Epoch 510, val loss: 1.168808937072754
Epoch 520, training loss: 631.0691528320312 = 1.0408145189285278 + 100.0 * 6.300283432006836
Epoch 520, val loss: 1.154327392578125
Epoch 530, training loss: 630.80322265625 = 1.021445870399475 + 100.0 * 6.297818183898926
Epoch 530, val loss: 1.140527606010437
Epoch 540, training loss: 630.5738525390625 = 1.0027005672454834 + 100.0 * 6.295711517333984
Epoch 540, val loss: 1.1275134086608887
Epoch 550, training loss: 630.8838500976562 = 0.9843902587890625 + 100.0 * 6.298994541168213
Epoch 550, val loss: 1.1149523258209229
Epoch 560, training loss: 630.36669921875 = 0.9665430188179016 + 100.0 * 6.294001579284668
Epoch 560, val loss: 1.1030489206314087
Epoch 570, training loss: 629.983642578125 = 0.9493255615234375 + 100.0 * 6.290343284606934
Epoch 570, val loss: 1.0918489694595337
Epoch 580, training loss: 629.7792358398438 = 0.9327470064163208 + 100.0 * 6.2884650230407715
Epoch 580, val loss: 1.0814824104309082
Epoch 590, training loss: 630.04638671875 = 0.9165852665901184 + 100.0 * 6.2912983894348145
Epoch 590, val loss: 1.0717265605926514
Epoch 600, training loss: 629.4723510742188 = 0.9009860157966614 + 100.0 * 6.2857136726379395
Epoch 600, val loss: 1.0623587369918823
Epoch 610, training loss: 629.5512084960938 = 0.885793149471283 + 100.0 * 6.286653995513916
Epoch 610, val loss: 1.0536139011383057
Epoch 620, training loss: 629.0968017578125 = 0.8709596991539001 + 100.0 * 6.282258033752441
Epoch 620, val loss: 1.0455048084259033
Epoch 630, training loss: 628.91064453125 = 0.8567113876342773 + 100.0 * 6.280539512634277
Epoch 630, val loss: 1.0378795862197876
Epoch 640, training loss: 628.7496337890625 = 0.8428667187690735 + 100.0 * 6.279067516326904
Epoch 640, val loss: 1.0308592319488525
Epoch 650, training loss: 628.9714965820312 = 0.8294288516044617 + 100.0 * 6.281420707702637
Epoch 650, val loss: 1.0241433382034302
Epoch 660, training loss: 628.5951538085938 = 0.8160604238510132 + 100.0 * 6.2777910232543945
Epoch 660, val loss: 1.0180611610412598
Epoch 670, training loss: 628.3547973632812 = 0.8032716512680054 + 100.0 * 6.275515079498291
Epoch 670, val loss: 1.0122755765914917
Epoch 680, training loss: 628.1701049804688 = 0.7907396554946899 + 100.0 * 6.273793697357178
Epoch 680, val loss: 1.0070544481277466
Epoch 690, training loss: 628.7913208007812 = 0.7785530090332031 + 100.0 * 6.28012752532959
Epoch 690, val loss: 1.0020809173583984
Epoch 700, training loss: 628.0604248046875 = 0.7665027379989624 + 100.0 * 6.272939205169678
Epoch 700, val loss: 0.99748694896698
Epoch 710, training loss: 627.8441772460938 = 0.7547728419303894 + 100.0 * 6.2708940505981445
Epoch 710, val loss: 0.9932796955108643
Epoch 720, training loss: 627.64013671875 = 0.7434004545211792 + 100.0 * 6.268967151641846
Epoch 720, val loss: 0.989533007144928
Epoch 730, training loss: 627.7196044921875 = 0.7322520613670349 + 100.0 * 6.26987361907959
Epoch 730, val loss: 0.9861476421356201
Epoch 740, training loss: 627.7374267578125 = 0.7212302088737488 + 100.0 * 6.2701616287231445
Epoch 740, val loss: 0.9824790954589844
Epoch 750, training loss: 627.4134521484375 = 0.7103238105773926 + 100.0 * 6.267031192779541
Epoch 750, val loss: 0.9793520569801331
Epoch 760, training loss: 627.2378540039062 = 0.6997767090797424 + 100.0 * 6.265380859375
Epoch 760, val loss: 0.9767869710922241
Epoch 770, training loss: 627.1151733398438 = 0.6894646286964417 + 100.0 * 6.264257431030273
Epoch 770, val loss: 0.9744082689285278
Epoch 780, training loss: 627.430419921875 = 0.6793539524078369 + 100.0 * 6.267510414123535
Epoch 780, val loss: 0.9721204042434692
Epoch 790, training loss: 627.0132446289062 = 0.6692516207695007 + 100.0 * 6.263440132141113
Epoch 790, val loss: 0.9701052308082581
Epoch 800, training loss: 626.8514404296875 = 0.6594181656837463 + 100.0 * 6.26192045211792
Epoch 800, val loss: 0.968076229095459
Epoch 810, training loss: 627.0885009765625 = 0.6497860550880432 + 100.0 * 6.264387130737305
Epoch 810, val loss: 0.9663551449775696
Epoch 820, training loss: 626.7116088867188 = 0.640140175819397 + 100.0 * 6.260714530944824
Epoch 820, val loss: 0.9648897647857666
Epoch 830, training loss: 626.5233154296875 = 0.6307422518730164 + 100.0 * 6.258925914764404
Epoch 830, val loss: 0.9636896252632141
Epoch 840, training loss: 626.761474609375 = 0.6215113997459412 + 100.0 * 6.261399745941162
Epoch 840, val loss: 0.9623503684997559
Epoch 850, training loss: 626.3445434570312 = 0.6122446060180664 + 100.0 * 6.257323265075684
Epoch 850, val loss: 0.9614193439483643
Epoch 860, training loss: 626.2637329101562 = 0.603193461894989 + 100.0 * 6.25660514831543
Epoch 860, val loss: 0.9605227708816528
Epoch 870, training loss: 626.1177978515625 = 0.5942783355712891 + 100.0 * 6.255234718322754
Epoch 870, val loss: 0.9599378705024719
Epoch 880, training loss: 626.0084228515625 = 0.5855252146720886 + 100.0 * 6.2542290687561035
Epoch 880, val loss: 0.9593180418014526
Epoch 890, training loss: 626.6563110351562 = 0.576904296875 + 100.0 * 6.260794162750244
Epoch 890, val loss: 0.958783745765686
Epoch 900, training loss: 626.3453369140625 = 0.568024218082428 + 100.0 * 6.257772922515869
Epoch 900, val loss: 0.9583102464675903
Epoch 910, training loss: 625.9313354492188 = 0.5593911409378052 + 100.0 * 6.253719329833984
Epoch 910, val loss: 0.9580167531967163
Epoch 920, training loss: 625.7091674804688 = 0.5509921908378601 + 100.0 * 6.251582145690918
Epoch 920, val loss: 0.957862913608551
Epoch 930, training loss: 625.5897216796875 = 0.5427234768867493 + 100.0 * 6.250470161437988
Epoch 930, val loss: 0.9579644203186035
Epoch 940, training loss: 625.4892578125 = 0.5345478653907776 + 100.0 * 6.249547004699707
Epoch 940, val loss: 0.9582944512367249
Epoch 950, training loss: 625.6298217773438 = 0.5264731645584106 + 100.0 * 6.251033306121826
Epoch 950, val loss: 0.9585148692131042
Epoch 960, training loss: 626.0376586914062 = 0.5182297825813293 + 100.0 * 6.255194187164307
Epoch 960, val loss: 0.9588454365730286
Epoch 970, training loss: 625.3136596679688 = 0.5100841522216797 + 100.0 * 6.248035907745361
Epoch 970, val loss: 0.9591952562332153
Epoch 980, training loss: 625.2095336914062 = 0.502185046672821 + 100.0 * 6.247073650360107
Epoch 980, val loss: 0.9596629738807678
Epoch 990, training loss: 625.126953125 = 0.49443233013153076 + 100.0 * 6.2463250160217285
Epoch 990, val loss: 0.9603525996208191
Epoch 1000, training loss: 625.042236328125 = 0.4867996573448181 + 100.0 * 6.245553970336914
Epoch 1000, val loss: 0.9613322019577026
Epoch 1010, training loss: 626.0066528320312 = 0.47923168540000916 + 100.0 * 6.255273818969727
Epoch 1010, val loss: 0.962648868560791
Epoch 1020, training loss: 625.1748657226562 = 0.47159698605537415 + 100.0 * 6.247032642364502
Epoch 1020, val loss: 0.9631854295730591
Epoch 1030, training loss: 624.920166015625 = 0.4641125500202179 + 100.0 * 6.244560241699219
Epoch 1030, val loss: 0.9641444683074951
Epoch 1040, training loss: 624.7687377929688 = 0.4568054974079132 + 100.0 * 6.243119239807129
Epoch 1040, val loss: 0.9656713604927063
Epoch 1050, training loss: 625.3984375 = 0.4496454894542694 + 100.0 * 6.24948787689209
Epoch 1050, val loss: 0.9669409990310669
Epoch 1060, training loss: 624.8590698242188 = 0.44234228134155273 + 100.0 * 6.244167327880859
Epoch 1060, val loss: 0.9690225124359131
Epoch 1070, training loss: 624.6892700195312 = 0.43528226017951965 + 100.0 * 6.242539405822754
Epoch 1070, val loss: 0.9701903462409973
Epoch 1080, training loss: 624.5328369140625 = 0.428323894739151 + 100.0 * 6.241044998168945
Epoch 1080, val loss: 0.9722878932952881
Epoch 1090, training loss: 624.4435424804688 = 0.4215225875377655 + 100.0 * 6.240220546722412
Epoch 1090, val loss: 0.9741295576095581
Epoch 1100, training loss: 624.791748046875 = 0.41480523347854614 + 100.0 * 6.243769645690918
Epoch 1100, val loss: 0.9761979579925537
Epoch 1110, training loss: 624.4575805664062 = 0.4080711007118225 + 100.0 * 6.240495204925537
Epoch 1110, val loss: 0.9785231947898865
Epoch 1120, training loss: 624.3073120117188 = 0.4014606177806854 + 100.0 * 6.239058494567871
Epoch 1120, val loss: 0.9806696176528931
Epoch 1130, training loss: 624.1824951171875 = 0.3950088322162628 + 100.0 * 6.237874507904053
Epoch 1130, val loss: 0.9832574725151062
Epoch 1140, training loss: 624.2722778320312 = 0.3886699080467224 + 100.0 * 6.238836288452148
Epoch 1140, val loss: 0.9859413504600525
Epoch 1150, training loss: 624.1091918945312 = 0.3823321759700775 + 100.0 * 6.237268447875977
Epoch 1150, val loss: 0.9882709383964539
Epoch 1160, training loss: 625.142822265625 = 0.3761225640773773 + 100.0 * 6.24766731262207
Epoch 1160, val loss: 0.9904724955558777
Epoch 1170, training loss: 624.3507690429688 = 0.3698100745677948 + 100.0 * 6.239809513092041
Epoch 1170, val loss: 0.9941683411598206
Epoch 1180, training loss: 623.9407348632812 = 0.3637922704219818 + 100.0 * 6.235769748687744
Epoch 1180, val loss: 0.996792733669281
Epoch 1190, training loss: 623.8238525390625 = 0.35790324211120605 + 100.0 * 6.234659671783447
Epoch 1190, val loss: 0.9997873902320862
Epoch 1200, training loss: 623.7530517578125 = 0.3521116375923157 + 100.0 * 6.234009265899658
Epoch 1200, val loss: 1.003002405166626
Epoch 1210, training loss: 623.8555908203125 = 0.3464203178882599 + 100.0 * 6.235091686248779
Epoch 1210, val loss: 1.0063190460205078
Epoch 1220, training loss: 623.697265625 = 0.34068259596824646 + 100.0 * 6.233565807342529
Epoch 1220, val loss: 1.0095609426498413
Epoch 1230, training loss: 623.7101440429688 = 0.3350526690483093 + 100.0 * 6.233750343322754
Epoch 1230, val loss: 1.0131889581680298
Epoch 1240, training loss: 623.5907592773438 = 0.32953083515167236 + 100.0 * 6.232612133026123
Epoch 1240, val loss: 1.0165886878967285
Epoch 1250, training loss: 623.6255493164062 = 0.32415372133255005 + 100.0 * 6.233014106750488
Epoch 1250, val loss: 1.0206013917922974
Epoch 1260, training loss: 623.8610229492188 = 0.31877973675727844 + 100.0 * 6.235422134399414
Epoch 1260, val loss: 1.0242582559585571
Epoch 1270, training loss: 623.4591674804688 = 0.31350478529930115 + 100.0 * 6.231456756591797
Epoch 1270, val loss: 1.0275952816009521
Epoch 1280, training loss: 623.5922241210938 = 0.30835503339767456 + 100.0 * 6.2328386306762695
Epoch 1280, val loss: 1.0314232110977173
Epoch 1290, training loss: 623.3260498046875 = 0.30321893095970154 + 100.0 * 6.230228424072266
Epoch 1290, val loss: 1.0355029106140137
Epoch 1300, training loss: 623.3059692382812 = 0.29823681712150574 + 100.0 * 6.230077743530273
Epoch 1300, val loss: 1.0398036241531372
Epoch 1310, training loss: 623.2123413085938 = 0.2933485507965088 + 100.0 * 6.229190349578857
Epoch 1310, val loss: 1.043872594833374
Epoch 1320, training loss: 623.7085571289062 = 0.28852343559265137 + 100.0 * 6.234200477600098
Epoch 1320, val loss: 1.0482145547866821
Epoch 1330, training loss: 623.255859375 = 0.2837192118167877 + 100.0 * 6.229721546173096
Epoch 1330, val loss: 1.0519654750823975
Epoch 1340, training loss: 623.1403198242188 = 0.2789762616157532 + 100.0 * 6.228613376617432
Epoch 1340, val loss: 1.0564571619033813
Epoch 1350, training loss: 623.0359497070312 = 0.27440130710601807 + 100.0 * 6.2276153564453125
Epoch 1350, val loss: 1.0606921911239624
Epoch 1360, training loss: 623.2184448242188 = 0.2699025571346283 + 100.0 * 6.229485511779785
Epoch 1360, val loss: 1.0657837390899658
Epoch 1370, training loss: 622.95654296875 = 0.26538795232772827 + 100.0 * 6.226911544799805
Epoch 1370, val loss: 1.0695185661315918
Epoch 1380, training loss: 622.9610595703125 = 0.26097577810287476 + 100.0 * 6.227000713348389
Epoch 1380, val loss: 1.0742883682250977
Epoch 1390, training loss: 622.8921508789062 = 0.25668615102767944 + 100.0 * 6.226354122161865
Epoch 1390, val loss: 1.0783861875534058
Epoch 1400, training loss: 622.83056640625 = 0.25249341130256653 + 100.0 * 6.225780487060547
Epoch 1400, val loss: 1.0833779573440552
Epoch 1410, training loss: 623.4821166992188 = 0.24836240708827972 + 100.0 * 6.232337951660156
Epoch 1410, val loss: 1.0877443552017212
Epoch 1420, training loss: 622.932861328125 = 0.24420952796936035 + 100.0 * 6.226886749267578
Epoch 1420, val loss: 1.0927734375
Epoch 1430, training loss: 622.7434692382812 = 0.24016623198986053 + 100.0 * 6.225032806396484
Epoch 1430, val loss: 1.0973925590515137
Epoch 1440, training loss: 623.3825073242188 = 0.2362486869096756 + 100.0 * 6.231462478637695
Epoch 1440, val loss: 1.1013487577438354
Epoch 1450, training loss: 622.7929077148438 = 0.23227046430110931 + 100.0 * 6.2256059646606445
Epoch 1450, val loss: 1.1075328588485718
Epoch 1460, training loss: 622.6287841796875 = 0.22844639420509338 + 100.0 * 6.224003314971924
Epoch 1460, val loss: 1.111535906791687
Epoch 1470, training loss: 622.52880859375 = 0.22472473978996277 + 100.0 * 6.22304105758667
Epoch 1470, val loss: 1.1166794300079346
Epoch 1480, training loss: 622.470947265625 = 0.22108225524425507 + 100.0 * 6.222498893737793
Epoch 1480, val loss: 1.1217652559280396
Epoch 1490, training loss: 622.6222534179688 = 0.21752344071865082 + 100.0 * 6.2240471839904785
Epoch 1490, val loss: 1.1265050172805786
Epoch 1500, training loss: 622.6100463867188 = 0.21394625306129456 + 100.0 * 6.223960876464844
Epoch 1500, val loss: 1.1315444707870483
Epoch 1510, training loss: 622.4340209960938 = 0.2103879600763321 + 100.0 * 6.222236156463623
Epoch 1510, val loss: 1.136610746383667
Epoch 1520, training loss: 622.6703491210938 = 0.20698602497577667 + 100.0 * 6.224633693695068
Epoch 1520, val loss: 1.1418650150299072
Epoch 1530, training loss: 622.4323120117188 = 0.20359088480472565 + 100.0 * 6.222286701202393
Epoch 1530, val loss: 1.1466232538223267
Epoch 1540, training loss: 622.4055786132812 = 0.20026610791683197 + 100.0 * 6.222053050994873
Epoch 1540, val loss: 1.151804804801941
Epoch 1550, training loss: 622.3658447265625 = 0.19704598188400269 + 100.0 * 6.221688270568848
Epoch 1550, val loss: 1.1570978164672852
Epoch 1560, training loss: 622.4487915039062 = 0.19387032091617584 + 100.0 * 6.2225494384765625
Epoch 1560, val loss: 1.1621772050857544
Epoch 1570, training loss: 622.2184448242188 = 0.19074338674545288 + 100.0 * 6.220276832580566
Epoch 1570, val loss: 1.167240858078003
Epoch 1580, training loss: 622.3621826171875 = 0.18769942224025726 + 100.0 * 6.221745014190674
Epoch 1580, val loss: 1.172347068786621
Epoch 1590, training loss: 622.3001098632812 = 0.18465912342071533 + 100.0 * 6.221154689788818
Epoch 1590, val loss: 1.1775498390197754
Epoch 1600, training loss: 622.2352905273438 = 0.1817021667957306 + 100.0 * 6.220535755157471
Epoch 1600, val loss: 1.1828982830047607
Epoch 1610, training loss: 622.114501953125 = 0.17879611253738403 + 100.0 * 6.219357013702393
Epoch 1610, val loss: 1.1879655122756958
Epoch 1620, training loss: 622.1575317382812 = 0.17596513032913208 + 100.0 * 6.219815731048584
Epoch 1620, val loss: 1.1931718587875366
Epoch 1630, training loss: 622.5603637695312 = 0.17317743599414825 + 100.0 * 6.223872184753418
Epoch 1630, val loss: 1.197951078414917
Epoch 1640, training loss: 622.0313110351562 = 0.1703576296567917 + 100.0 * 6.218609809875488
Epoch 1640, val loss: 1.2037955522537231
Epoch 1650, training loss: 622.0018310546875 = 0.16764841973781586 + 100.0 * 6.218341827392578
Epoch 1650, val loss: 1.2086602449417114
Epoch 1660, training loss: 621.9884643554688 = 0.16502425074577332 + 100.0 * 6.218234062194824
Epoch 1660, val loss: 1.2141990661621094
Epoch 1670, training loss: 622.0233764648438 = 0.16243338584899902 + 100.0 * 6.218609809875488
Epoch 1670, val loss: 1.219382643699646
Epoch 1680, training loss: 621.9140014648438 = 0.15987379848957062 + 100.0 * 6.217541217803955
Epoch 1680, val loss: 1.2251315116882324
Epoch 1690, training loss: 621.9900512695312 = 0.1573721021413803 + 100.0 * 6.218327045440674
Epoch 1690, val loss: 1.2310930490493774
Epoch 1700, training loss: 621.934326171875 = 0.1549038589000702 + 100.0 * 6.217794418334961
Epoch 1700, val loss: 1.2365014553070068
Epoch 1710, training loss: 622.0176391601562 = 0.15248507261276245 + 100.0 * 6.21865177154541
Epoch 1710, val loss: 1.2415087223052979
Epoch 1720, training loss: 621.9447631835938 = 0.15009769797325134 + 100.0 * 6.217947006225586
Epoch 1720, val loss: 1.2467739582061768
Epoch 1730, training loss: 621.8128662109375 = 0.14774556457996368 + 100.0 * 6.21665096282959
Epoch 1730, val loss: 1.2521284818649292
Epoch 1740, training loss: 622.0380249023438 = 0.14545924961566925 + 100.0 * 6.218925952911377
Epoch 1740, val loss: 1.2574814558029175
Epoch 1750, training loss: 621.6914672851562 = 0.14316755533218384 + 100.0 * 6.215482711791992
Epoch 1750, val loss: 1.2630592584609985
Epoch 1760, training loss: 621.6703491210938 = 0.1409560590982437 + 100.0 * 6.215293884277344
Epoch 1760, val loss: 1.268752932548523
Epoch 1770, training loss: 621.9570922851562 = 0.13879577815532684 + 100.0 * 6.2181830406188965
Epoch 1770, val loss: 1.274591088294983
Epoch 1780, training loss: 621.7404174804688 = 0.13662271201610565 + 100.0 * 6.216038227081299
Epoch 1780, val loss: 1.2795301675796509
Epoch 1790, training loss: 621.5856323242188 = 0.1345006376504898 + 100.0 * 6.214510917663574
Epoch 1790, val loss: 1.2842586040496826
Epoch 1800, training loss: 621.5211791992188 = 0.13244999945163727 + 100.0 * 6.2138872146606445
Epoch 1800, val loss: 1.2903910875320435
Epoch 1810, training loss: 621.5098876953125 = 0.1304548978805542 + 100.0 * 6.213794708251953
Epoch 1810, val loss: 1.2956520318984985
Epoch 1820, training loss: 622.0274047851562 = 0.12849895656108856 + 100.0 * 6.218989372253418
Epoch 1820, val loss: 1.301236629486084
Epoch 1830, training loss: 621.5725708007812 = 0.12648968398571014 + 100.0 * 6.214461326599121
Epoch 1830, val loss: 1.3070553541183472
Epoch 1840, training loss: 621.739013671875 = 0.12458746135234833 + 100.0 * 6.216144561767578
Epoch 1840, val loss: 1.3118202686309814
Epoch 1850, training loss: 621.4714965820312 = 0.12264744937419891 + 100.0 * 6.213488578796387
Epoch 1850, val loss: 1.3173093795776367
Epoch 1860, training loss: 621.3228759765625 = 0.12076225876808167 + 100.0 * 6.2120208740234375
Epoch 1860, val loss: 1.3230143785476685
Epoch 1870, training loss: 621.2944946289062 = 0.11894945055246353 + 100.0 * 6.211755275726318
Epoch 1870, val loss: 1.328443169593811
Epoch 1880, training loss: 621.2839965820312 = 0.11717989295721054 + 100.0 * 6.211668014526367
Epoch 1880, val loss: 1.3341197967529297
Epoch 1890, training loss: 621.9011840820312 = 0.11546465754508972 + 100.0 * 6.2178568840026855
Epoch 1890, val loss: 1.3402680158615112
Epoch 1900, training loss: 621.4658813476562 = 0.11367694288492203 + 100.0 * 6.213522434234619
Epoch 1900, val loss: 1.344003677368164
Epoch 1910, training loss: 621.3472900390625 = 0.11195096373558044 + 100.0 * 6.212353706359863
Epoch 1910, val loss: 1.350007176399231
Epoch 1920, training loss: 621.2857055664062 = 0.11028287559747696 + 100.0 * 6.211753845214844
Epoch 1920, val loss: 1.3550870418548584
Epoch 1930, training loss: 621.897216796875 = 0.10866744816303253 + 100.0 * 6.217885971069336
Epoch 1930, val loss: 1.359695553779602
Epoch 1940, training loss: 621.330810546875 = 0.10699495673179626 + 100.0 * 6.212238311767578
Epoch 1940, val loss: 1.3668311834335327
Epoch 1950, training loss: 621.158203125 = 0.10540230572223663 + 100.0 * 6.2105278968811035
Epoch 1950, val loss: 1.370971918106079
Epoch 1960, training loss: 621.10009765625 = 0.10384945571422577 + 100.0 * 6.209962844848633
Epoch 1960, val loss: 1.3771040439605713
Epoch 1970, training loss: 621.6748046875 = 0.10234887152910233 + 100.0 * 6.215724468231201
Epoch 1970, val loss: 1.38198983669281
Epoch 1980, training loss: 621.2062377929688 = 0.10076959431171417 + 100.0 * 6.211054801940918
Epoch 1980, val loss: 1.3873507976531982
Epoch 1990, training loss: 621.129150390625 = 0.09926321357488632 + 100.0 * 6.210299015045166
Epoch 1990, val loss: 1.3930437564849854
Epoch 2000, training loss: 621.0421142578125 = 0.0978056788444519 + 100.0 * 6.209443092346191
Epoch 2000, val loss: 1.3979172706604004
Epoch 2010, training loss: 621.09521484375 = 0.09638671576976776 + 100.0 * 6.209988117218018
Epoch 2010, val loss: 1.4037827253341675
Epoch 2020, training loss: 621.45361328125 = 0.09496661275625229 + 100.0 * 6.213586330413818
Epoch 2020, val loss: 1.40908944606781
Epoch 2030, training loss: 621.0458374023438 = 0.09352744370698929 + 100.0 * 6.2095232009887695
Epoch 2030, val loss: 1.4141675233840942
Epoch 2040, training loss: 620.9761352539062 = 0.09213319420814514 + 100.0 * 6.2088398933410645
Epoch 2040, val loss: 1.4191471338272095
Epoch 2050, training loss: 620.933837890625 = 0.09079772233963013 + 100.0 * 6.208430290222168
Epoch 2050, val loss: 1.4250131845474243
Epoch 2060, training loss: 620.9300537109375 = 0.08948852121829987 + 100.0 * 6.2084059715271
Epoch 2060, val loss: 1.4304319620132446
Epoch 2070, training loss: 621.3377075195312 = 0.08821436017751694 + 100.0 * 6.21249532699585
Epoch 2070, val loss: 1.4349267482757568
Epoch 2080, training loss: 621.5757446289062 = 0.08689121901988983 + 100.0 * 6.214888572692871
Epoch 2080, val loss: 1.4404155015945435
Epoch 2090, training loss: 620.9420166015625 = 0.08556920289993286 + 100.0 * 6.208564281463623
Epoch 2090, val loss: 1.4461349248886108
Epoch 2100, training loss: 620.8036499023438 = 0.08431720733642578 + 100.0 * 6.207193374633789
Epoch 2100, val loss: 1.4515180587768555
Epoch 2110, training loss: 620.7786865234375 = 0.08310532569885254 + 100.0 * 6.206955432891846
Epoch 2110, val loss: 1.4568700790405273
Epoch 2120, training loss: 620.7754516601562 = 0.08191775530576706 + 100.0 * 6.206935405731201
Epoch 2120, val loss: 1.4625957012176514
Epoch 2130, training loss: 621.0288696289062 = 0.08075104653835297 + 100.0 * 6.209481239318848
Epoch 2130, val loss: 1.4675899744033813
Epoch 2140, training loss: 621.0349731445312 = 0.07955271005630493 + 100.0 * 6.209554195404053
Epoch 2140, val loss: 1.4730807542800903
Epoch 2150, training loss: 620.8519897460938 = 0.0783592090010643 + 100.0 * 6.207736492156982
Epoch 2150, val loss: 1.4785254001617432
Epoch 2160, training loss: 620.703857421875 = 0.07721631228923798 + 100.0 * 6.206266403198242
Epoch 2160, val loss: 1.4833799600601196
Epoch 2170, training loss: 620.6842651367188 = 0.0761113166809082 + 100.0 * 6.206081390380859
Epoch 2170, val loss: 1.4887820482254028
Epoch 2180, training loss: 620.6820678710938 = 0.07503422349691391 + 100.0 * 6.206070423126221
Epoch 2180, val loss: 1.4939367771148682
Epoch 2190, training loss: 620.9699096679688 = 0.0739755779504776 + 100.0 * 6.208959102630615
Epoch 2190, val loss: 1.4988325834274292
Epoch 2200, training loss: 620.7092895507812 = 0.0728897899389267 + 100.0 * 6.206364154815674
Epoch 2200, val loss: 1.504814863204956
Epoch 2210, training loss: 620.9019165039062 = 0.0718379020690918 + 100.0 * 6.208301067352295
Epoch 2210, val loss: 1.5104119777679443
Epoch 2220, training loss: 620.6486206054688 = 0.0707901120185852 + 100.0 * 6.205778121948242
Epoch 2220, val loss: 1.514890432357788
Epoch 2230, training loss: 620.5911254882812 = 0.06977452337741852 + 100.0 * 6.20521354675293
Epoch 2230, val loss: 1.521007776260376
Epoch 2240, training loss: 620.5670776367188 = 0.06878744810819626 + 100.0 * 6.204982757568359
Epoch 2240, val loss: 1.5261038541793823
Epoch 2250, training loss: 621.0875854492188 = 0.06781987100839615 + 100.0 * 6.210197448730469
Epoch 2250, val loss: 1.5313891172409058
Epoch 2260, training loss: 621.180908203125 = 0.06682930886745453 + 100.0 * 6.2111406326293945
Epoch 2260, val loss: 1.5350807905197144
Epoch 2270, training loss: 620.7156372070312 = 0.06582128256559372 + 100.0 * 6.206498622894287
Epoch 2270, val loss: 1.542063593864441
Epoch 2280, training loss: 620.5317993164062 = 0.0648704543709755 + 100.0 * 6.204668998718262
Epoch 2280, val loss: 1.5457487106323242
Epoch 2290, training loss: 620.438720703125 = 0.06395188719034195 + 100.0 * 6.203747272491455
Epoch 2290, val loss: 1.551864504814148
Epoch 2300, training loss: 620.41748046875 = 0.06305107474327087 + 100.0 * 6.2035441398620605
Epoch 2300, val loss: 1.557373046875
Epoch 2310, training loss: 620.42041015625 = 0.06216659024357796 + 100.0 * 6.203582286834717
Epoch 2310, val loss: 1.5627506971359253
Epoch 2320, training loss: 621.021728515625 = 0.06131114810705185 + 100.0 * 6.209603786468506
Epoch 2320, val loss: 1.5676980018615723
Epoch 2330, training loss: 620.6688842773438 = 0.06040768325328827 + 100.0 * 6.206084728240967
Epoch 2330, val loss: 1.5717945098876953
Epoch 2340, training loss: 620.4850463867188 = 0.05951409414410591 + 100.0 * 6.204255104064941
Epoch 2340, val loss: 1.578091025352478
Epoch 2350, training loss: 620.4942016601562 = 0.058672621846199036 + 100.0 * 6.204355239868164
Epoch 2350, val loss: 1.5827285051345825
Epoch 2360, training loss: 620.61962890625 = 0.0578489676117897 + 100.0 * 6.205617427825928
Epoch 2360, val loss: 1.5885093212127686
Epoch 2370, training loss: 620.4189453125 = 0.05701863393187523 + 100.0 * 6.203619480133057
Epoch 2370, val loss: 1.5928354263305664
Epoch 2380, training loss: 620.4778442382812 = 0.05620456114411354 + 100.0 * 6.204216003417969
Epoch 2380, val loss: 1.5980409383773804
Epoch 2390, training loss: 620.5150146484375 = 0.055410344153642654 + 100.0 * 6.204596042633057
Epoch 2390, val loss: 1.6038442850112915
Epoch 2400, training loss: 620.6333618164062 = 0.0546126663684845 + 100.0 * 6.205787658691406
Epoch 2400, val loss: 1.6085026264190674
Epoch 2410, training loss: 620.322998046875 = 0.05383214354515076 + 100.0 * 6.202691555023193
Epoch 2410, val loss: 1.6128554344177246
Epoch 2420, training loss: 620.269287109375 = 0.05307064577937126 + 100.0 * 6.20216178894043
Epoch 2420, val loss: 1.617918610572815
Epoch 2430, training loss: 620.40966796875 = 0.05233136937022209 + 100.0 * 6.203573703765869
Epoch 2430, val loss: 1.6233711242675781
Epoch 2440, training loss: 620.462158203125 = 0.051593925803899765 + 100.0 * 6.204105854034424
Epoch 2440, val loss: 1.628433346748352
Epoch 2450, training loss: 620.4533081054688 = 0.05085080489516258 + 100.0 * 6.204024791717529
Epoch 2450, val loss: 1.6328158378601074
Epoch 2460, training loss: 620.3357543945312 = 0.05013543367385864 + 100.0 * 6.202856540679932
Epoch 2460, val loss: 1.6378555297851562
Epoch 2470, training loss: 620.5560913085938 = 0.04943377897143364 + 100.0 * 6.205066680908203
Epoch 2470, val loss: 1.642562747001648
Epoch 2480, training loss: 620.19873046875 = 0.0487176738679409 + 100.0 * 6.201500415802002
Epoch 2480, val loss: 1.647810459136963
Epoch 2490, training loss: 620.158447265625 = 0.0480387881398201 + 100.0 * 6.201104164123535
Epoch 2490, val loss: 1.6531744003295898
Epoch 2500, training loss: 620.151611328125 = 0.0473678857088089 + 100.0 * 6.201042652130127
Epoch 2500, val loss: 1.6580405235290527
Epoch 2510, training loss: 620.8683471679688 = 0.04673617705702782 + 100.0 * 6.208216190338135
Epoch 2510, val loss: 1.6622799634933472
Epoch 2520, training loss: 620.353759765625 = 0.04604257270693779 + 100.0 * 6.20307731628418
Epoch 2520, val loss: 1.6675105094909668
Epoch 2530, training loss: 620.2411499023438 = 0.04538169503211975 + 100.0 * 6.2019572257995605
Epoch 2530, val loss: 1.6721583604812622
Epoch 2540, training loss: 620.1962280273438 = 0.04474787041544914 + 100.0 * 6.201515197753906
Epoch 2540, val loss: 1.6772456169128418
Epoch 2550, training loss: 620.4765014648438 = 0.04413316026329994 + 100.0 * 6.204323768615723
Epoch 2550, val loss: 1.6819032430648804
Epoch 2560, training loss: 620.0527954101562 = 0.04349890723824501 + 100.0 * 6.200092792510986
Epoch 2560, val loss: 1.6872797012329102
Epoch 2570, training loss: 620.1400146484375 = 0.04289781674742699 + 100.0 * 6.2009711265563965
Epoch 2570, val loss: 1.691987156867981
Epoch 2580, training loss: 620.311767578125 = 0.042305923998355865 + 100.0 * 6.202694892883301
Epoch 2580, val loss: 1.6965420246124268
Epoch 2590, training loss: 620.0418701171875 = 0.041701361536979675 + 100.0 * 6.2000017166137695
Epoch 2590, val loss: 1.701675534248352
Epoch 2600, training loss: 620.4171142578125 = 0.04112624749541283 + 100.0 * 6.203759670257568
Epoch 2600, val loss: 1.7076448202133179
Epoch 2610, training loss: 620.2020874023438 = 0.040532756596803665 + 100.0 * 6.201615810394287
Epoch 2610, val loss: 1.7108222246170044
Epoch 2620, training loss: 620.016357421875 = 0.039965249598026276 + 100.0 * 6.199763774871826
Epoch 2620, val loss: 1.7158838510513306
Epoch 2630, training loss: 619.9642944335938 = 0.039410680532455444 + 100.0 * 6.199248790740967
Epoch 2630, val loss: 1.7204300165176392
Epoch 2640, training loss: 619.931640625 = 0.03887959569692612 + 100.0 * 6.198927402496338
Epoch 2640, val loss: 1.7256693840026855
Epoch 2650, training loss: 619.9136352539062 = 0.038355037569999695 + 100.0 * 6.1987528800964355
Epoch 2650, val loss: 1.7309792041778564
Epoch 2660, training loss: 620.1864624023438 = 0.03784535080194473 + 100.0 * 6.201486110687256
Epoch 2660, val loss: 1.735643744468689
Epoch 2670, training loss: 620.5209350585938 = 0.03732318803668022 + 100.0 * 6.204835891723633
Epoch 2670, val loss: 1.7392964363098145
Epoch 2680, training loss: 620.1425170898438 = 0.03678736090660095 + 100.0 * 6.201056957244873
Epoch 2680, val loss: 1.7449774742126465
Epoch 2690, training loss: 619.9661865234375 = 0.03627510741353035 + 100.0 * 6.199299335479736
Epoch 2690, val loss: 1.7489988803863525
Epoch 2700, training loss: 619.8551025390625 = 0.03578796982765198 + 100.0 * 6.198193073272705
Epoch 2700, val loss: 1.7543154954910278
Epoch 2710, training loss: 619.8912963867188 = 0.03531603887677193 + 100.0 * 6.198559284210205
Epoch 2710, val loss: 1.759030818939209
Epoch 2720, training loss: 620.626708984375 = 0.034849733114242554 + 100.0 * 6.205918788909912
Epoch 2720, val loss: 1.7635140419006348
Epoch 2730, training loss: 620.657470703125 = 0.03436757251620293 + 100.0 * 6.206231117248535
Epoch 2730, val loss: 1.7700403928756714
Epoch 2740, training loss: 619.9216918945312 = 0.033874720335006714 + 100.0 * 6.198878288269043
Epoch 2740, val loss: 1.7721277475357056
Epoch 2750, training loss: 619.7899780273438 = 0.0334186777472496 + 100.0 * 6.19756555557251
Epoch 2750, val loss: 1.7771062850952148
Epoch 2760, training loss: 619.7885131835938 = 0.03298810496926308 + 100.0 * 6.1975555419921875
Epoch 2760, val loss: 1.7829519510269165
Epoch 2770, training loss: 619.8065795898438 = 0.03256351128220558 + 100.0 * 6.197740077972412
Epoch 2770, val loss: 1.787174940109253
Epoch 2780, training loss: 620.3682250976562 = 0.03216475620865822 + 100.0 * 6.203360557556152
Epoch 2780, val loss: 1.7911911010742188
Epoch 2790, training loss: 620.0789184570312 = 0.03170611709356308 + 100.0 * 6.200472354888916
Epoch 2790, val loss: 1.7954157590866089
Epoch 2800, training loss: 619.8423461914062 = 0.03128105029463768 + 100.0 * 6.198110580444336
Epoch 2800, val loss: 1.8011646270751953
Epoch 2810, training loss: 619.9735107421875 = 0.030873723328113556 + 100.0 * 6.199426174163818
Epoch 2810, val loss: 1.8054713010787964
Epoch 2820, training loss: 619.9446411132812 = 0.030470825731754303 + 100.0 * 6.199141979217529
Epoch 2820, val loss: 1.8098875284194946
Epoch 2830, training loss: 619.7911987304688 = 0.030058350414037704 + 100.0 * 6.1976118087768555
Epoch 2830, val loss: 1.8148722648620605
Epoch 2840, training loss: 619.7407836914062 = 0.029672326520085335 + 100.0 * 6.197111129760742
Epoch 2840, val loss: 1.8191701173782349
Epoch 2850, training loss: 619.9689331054688 = 0.02930276282131672 + 100.0 * 6.199396133422852
Epoch 2850, val loss: 1.8243085145950317
Epoch 2860, training loss: 619.8727416992188 = 0.02891942858695984 + 100.0 * 6.1984381675720215
Epoch 2860, val loss: 1.8277865648269653
Epoch 2870, training loss: 619.9227905273438 = 0.02854827046394348 + 100.0 * 6.198942184448242
Epoch 2870, val loss: 1.8323427438735962
Epoch 2880, training loss: 619.78955078125 = 0.028174933046102524 + 100.0 * 6.19761323928833
Epoch 2880, val loss: 1.8367310762405396
Epoch 2890, training loss: 619.6341552734375 = 0.027815045788884163 + 100.0 * 6.196063041687012
Epoch 2890, val loss: 1.8418943881988525
Epoch 2900, training loss: 619.645263671875 = 0.027471298351883888 + 100.0 * 6.196177959442139
Epoch 2900, val loss: 1.8466602563858032
Epoch 2910, training loss: 619.86279296875 = 0.027134966105222702 + 100.0 * 6.1983561515808105
Epoch 2910, val loss: 1.8515377044677734
Epoch 2920, training loss: 620.1808471679688 = 0.026789771392941475 + 100.0 * 6.201540470123291
Epoch 2920, val loss: 1.8561182022094727
Epoch 2930, training loss: 619.7312622070312 = 0.026438618078827858 + 100.0 * 6.197048187255859
Epoch 2930, val loss: 1.8589706420898438
Epoch 2940, training loss: 619.5916137695312 = 0.02609698474407196 + 100.0 * 6.19565486907959
Epoch 2940, val loss: 1.8634577989578247
Epoch 2950, training loss: 619.5645141601562 = 0.02577652782201767 + 100.0 * 6.195387363433838
Epoch 2950, val loss: 1.8675947189331055
Epoch 2960, training loss: 619.5805053710938 = 0.025470834225416183 + 100.0 * 6.195550441741943
Epoch 2960, val loss: 1.8725309371948242
Epoch 2970, training loss: 620.036376953125 = 0.025169910863041878 + 100.0 * 6.200112342834473
Epoch 2970, val loss: 1.8763059377670288
Epoch 2980, training loss: 619.5956420898438 = 0.02484668605029583 + 100.0 * 6.195708274841309
Epoch 2980, val loss: 1.8805731534957886
Epoch 2990, training loss: 619.5416259765625 = 0.024541616439819336 + 100.0 * 6.195170879364014
Epoch 2990, val loss: 1.8849256038665771
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.625925925925926
0.8118081180811808
The final CL Acc:0.64198, 0.01222, The final GNN Acc:0.81234, 0.00075
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13208])
remove edge: torch.Size([2, 7940])
updated graph: torch.Size([2, 10592])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.633056640625 = 1.9497219324111938 + 100.0 * 8.596833229064941
Epoch 0, val loss: 1.9538404941558838
Epoch 10, training loss: 861.5333251953125 = 1.9406248331069946 + 100.0 * 8.595927238464355
Epoch 10, val loss: 1.945134162902832
Epoch 20, training loss: 860.8604125976562 = 1.928974986076355 + 100.0 * 8.589314460754395
Epoch 20, val loss: 1.93370521068573
Epoch 30, training loss: 856.4447631835938 = 1.9136292934417725 + 100.0 * 8.545310974121094
Epoch 30, val loss: 1.918452262878418
Epoch 40, training loss: 834.3287963867188 = 1.8942286968231201 + 100.0 * 8.324345588684082
Epoch 40, val loss: 1.8997091054916382
Epoch 50, training loss: 773.8074951171875 = 1.8716574907302856 + 100.0 * 7.719358444213867
Epoch 50, val loss: 1.878167986869812
Epoch 60, training loss: 740.7791137695312 = 1.8542847633361816 + 100.0 * 7.389247894287109
Epoch 60, val loss: 1.862459659576416
Epoch 70, training loss: 714.1123046875 = 1.8432530164718628 + 100.0 * 7.122690200805664
Epoch 70, val loss: 1.851347804069519
Epoch 80, training loss: 694.1033325195312 = 1.8331438302993774 + 100.0 * 6.922702312469482
Epoch 80, val loss: 1.8409868478775024
Epoch 90, training loss: 681.906494140625 = 1.8250715732574463 + 100.0 * 6.800814151763916
Epoch 90, val loss: 1.8321186304092407
Epoch 100, training loss: 674.3197021484375 = 1.8173518180847168 + 100.0 * 6.7250237464904785
Epoch 100, val loss: 1.823437213897705
Epoch 110, training loss: 668.3375854492188 = 1.8097259998321533 + 100.0 * 6.665278434753418
Epoch 110, val loss: 1.8147380352020264
Epoch 120, training loss: 662.9363403320312 = 1.8028149604797363 + 100.0 * 6.611335277557373
Epoch 120, val loss: 1.8070383071899414
Epoch 130, training loss: 659.1453857421875 = 1.7970486879348755 + 100.0 * 6.573482990264893
Epoch 130, val loss: 1.8005671501159668
Epoch 140, training loss: 656.0944213867188 = 1.7913464307785034 + 100.0 * 6.543031215667725
Epoch 140, val loss: 1.7944085597991943
Epoch 150, training loss: 653.65771484375 = 1.785244107246399 + 100.0 * 6.5187249183654785
Epoch 150, val loss: 1.7881066799163818
Epoch 160, training loss: 651.81689453125 = 1.7788783311843872 + 100.0 * 6.500380516052246
Epoch 160, val loss: 1.7818008661270142
Epoch 170, training loss: 649.9876708984375 = 1.7722055912017822 + 100.0 * 6.482154846191406
Epoch 170, val loss: 1.7753608226776123
Epoch 180, training loss: 648.4212036132812 = 1.765255093574524 + 100.0 * 6.466559410095215
Epoch 180, val loss: 1.7687674760818481
Epoch 190, training loss: 646.9556274414062 = 1.757898211479187 + 100.0 * 6.451977252960205
Epoch 190, val loss: 1.761915683746338
Epoch 200, training loss: 645.7061767578125 = 1.7500214576721191 + 100.0 * 6.43956184387207
Epoch 200, val loss: 1.7547523975372314
Epoch 210, training loss: 644.6085815429688 = 1.7415175437927246 + 100.0 * 6.428670406341553
Epoch 210, val loss: 1.7471754550933838
Epoch 220, training loss: 643.4677124023438 = 1.7322765588760376 + 100.0 * 6.417354106903076
Epoch 220, val loss: 1.738954782485962
Epoch 230, training loss: 642.4406127929688 = 1.7223000526428223 + 100.0 * 6.4071831703186035
Epoch 230, val loss: 1.7301057577133179
Epoch 240, training loss: 641.9000854492188 = 1.711379885673523 + 100.0 * 6.401886940002441
Epoch 240, val loss: 1.7205227613449097
Epoch 250, training loss: 640.893798828125 = 1.6995607614517212 + 100.0 * 6.391942024230957
Epoch 250, val loss: 1.7100708484649658
Epoch 260, training loss: 640.0902099609375 = 1.6869300603866577 + 100.0 * 6.384032726287842
Epoch 260, val loss: 1.6989165544509888
Epoch 270, training loss: 639.915771484375 = 1.6734377145767212 + 100.0 * 6.382422924041748
Epoch 270, val loss: 1.6870269775390625
Epoch 280, training loss: 638.853271484375 = 1.65874183177948 + 100.0 * 6.371945381164551
Epoch 280, val loss: 1.6740599870681763
Epoch 290, training loss: 638.0866088867188 = 1.6431946754455566 + 100.0 * 6.364434242248535
Epoch 290, val loss: 1.6603599786758423
Epoch 300, training loss: 637.5831298828125 = 1.6266710758209229 + 100.0 * 6.359564781188965
Epoch 300, val loss: 1.6458717584609985
Epoch 310, training loss: 637.1414794921875 = 1.6090807914733887 + 100.0 * 6.3553242683410645
Epoch 310, val loss: 1.6302766799926758
Epoch 320, training loss: 636.6155395507812 = 1.590558648109436 + 100.0 * 6.350249767303467
Epoch 320, val loss: 1.6139370203018188
Epoch 330, training loss: 635.985595703125 = 1.5712153911590576 + 100.0 * 6.344143867492676
Epoch 330, val loss: 1.596766471862793
Epoch 340, training loss: 635.681396484375 = 1.5510966777801514 + 100.0 * 6.341302871704102
Epoch 340, val loss: 1.5790379047393799
Epoch 350, training loss: 635.2515869140625 = 1.529930830001831 + 100.0 * 6.337216854095459
Epoch 350, val loss: 1.56040358543396
Epoch 360, training loss: 634.7365112304688 = 1.5083119869232178 + 100.0 * 6.332282066345215
Epoch 360, val loss: 1.5413213968276978
Epoch 370, training loss: 634.349365234375 = 1.4861983060836792 + 100.0 * 6.32863187789917
Epoch 370, val loss: 1.521881103515625
Epoch 380, training loss: 634.5234375 = 1.4636238813400269 + 100.0 * 6.330597877502441
Epoch 380, val loss: 1.5021138191223145
Epoch 390, training loss: 633.7634887695312 = 1.4402260780334473 + 100.0 * 6.323232173919678
Epoch 390, val loss: 1.4818391799926758
Epoch 400, training loss: 633.37890625 = 1.416839599609375 + 100.0 * 6.319620132446289
Epoch 400, val loss: 1.461431860923767
Epoch 410, training loss: 632.9929809570312 = 1.3932873010635376 + 100.0 * 6.3159966468811035
Epoch 410, val loss: 1.4410029649734497
Epoch 420, training loss: 633.1746826171875 = 1.369606852531433 + 100.0 * 6.318050384521484
Epoch 420, val loss: 1.4206507205963135
Epoch 430, training loss: 632.4724731445312 = 1.3457057476043701 + 100.0 * 6.311267852783203
Epoch 430, val loss: 1.4000548124313354
Epoch 440, training loss: 632.3214111328125 = 1.3218352794647217 + 100.0 * 6.309995651245117
Epoch 440, val loss: 1.379529356956482
Epoch 450, training loss: 632.1666259765625 = 1.2978856563568115 + 100.0 * 6.308687210083008
Epoch 450, val loss: 1.3590919971466064
Epoch 460, training loss: 631.5286865234375 = 1.2740411758422852 + 100.0 * 6.302546501159668
Epoch 460, val loss: 1.3389403820037842
Epoch 470, training loss: 631.3067626953125 = 1.250426173210144 + 100.0 * 6.300563335418701
Epoch 470, val loss: 1.3191686868667603
Epoch 480, training loss: 631.2792358398438 = 1.226946234703064 + 100.0 * 6.300522804260254
Epoch 480, val loss: 1.2997113466262817
Epoch 490, training loss: 630.865234375 = 1.2038558721542358 + 100.0 * 6.296613693237305
Epoch 490, val loss: 1.2805308103561401
Epoch 500, training loss: 630.4451904296875 = 1.1807762384414673 + 100.0 * 6.292644023895264
Epoch 500, val loss: 1.26167893409729
Epoch 510, training loss: 630.2238159179688 = 1.1581876277923584 + 100.0 * 6.290656089782715
Epoch 510, val loss: 1.2433865070343018
Epoch 520, training loss: 630.3136596679688 = 1.135827660560608 + 100.0 * 6.291778564453125
Epoch 520, val loss: 1.2254951000213623
Epoch 530, training loss: 629.9349365234375 = 1.1137466430664062 + 100.0 * 6.288211822509766
Epoch 530, val loss: 1.2080401182174683
Epoch 540, training loss: 629.6241455078125 = 1.0920124053955078 + 100.0 * 6.285321235656738
Epoch 540, val loss: 1.1911702156066895
Epoch 550, training loss: 629.380859375 = 1.0707048177719116 + 100.0 * 6.283102035522461
Epoch 550, val loss: 1.1748746633529663
Epoch 560, training loss: 629.4058837890625 = 1.0497835874557495 + 100.0 * 6.283560752868652
Epoch 560, val loss: 1.15912926197052
Epoch 570, training loss: 629.4263916015625 = 1.0292397737503052 + 100.0 * 6.283971786499023
Epoch 570, val loss: 1.1439088582992554
Epoch 580, training loss: 628.8682861328125 = 1.0091181993484497 + 100.0 * 6.278592109680176
Epoch 580, val loss: 1.1291108131408691
Epoch 590, training loss: 628.6026000976562 = 0.9895164966583252 + 100.0 * 6.2761311531066895
Epoch 590, val loss: 1.1151612997055054
Epoch 600, training loss: 628.6060180664062 = 0.9703850746154785 + 100.0 * 6.276356220245361
Epoch 600, val loss: 1.1018315553665161
Epoch 610, training loss: 628.3739013671875 = 0.9514706134796143 + 100.0 * 6.274224281311035
Epoch 610, val loss: 1.0891058444976807
Epoch 620, training loss: 628.224609375 = 0.9330462217330933 + 100.0 * 6.272915363311768
Epoch 620, val loss: 1.0766786336898804
Epoch 630, training loss: 628.2528686523438 = 0.9149828553199768 + 100.0 * 6.273378849029541
Epoch 630, val loss: 1.0649394989013672
Epoch 640, training loss: 627.8544311523438 = 0.8974671959877014 + 100.0 * 6.2695698738098145
Epoch 640, val loss: 1.0538324117660522
Epoch 650, training loss: 628.1203002929688 = 0.8801562786102295 + 100.0 * 6.272401809692383
Epoch 650, val loss: 1.0430314540863037
Epoch 660, training loss: 627.5908813476562 = 0.863275945186615 + 100.0 * 6.267275810241699
Epoch 660, val loss: 1.0326710939407349
Epoch 670, training loss: 627.4285278320312 = 0.846733033657074 + 100.0 * 6.265818119049072
Epoch 670, val loss: 1.0229681730270386
Epoch 680, training loss: 627.3073120117188 = 0.8305500745773315 + 100.0 * 6.264767169952393
Epoch 680, val loss: 1.0136115550994873
Epoch 690, training loss: 627.435791015625 = 0.8147274255752563 + 100.0 * 6.266211032867432
Epoch 690, val loss: 1.0048117637634277
Epoch 700, training loss: 627.4620361328125 = 0.799006998538971 + 100.0 * 6.266630172729492
Epoch 700, val loss: 0.9962811470031738
Epoch 710, training loss: 626.9918212890625 = 0.7834358215332031 + 100.0 * 6.262083530426025
Epoch 710, val loss: 0.9878422617912292
Epoch 720, training loss: 627.004150390625 = 0.7683990001678467 + 100.0 * 6.262357711791992
Epoch 720, val loss: 0.9799960851669312
Epoch 730, training loss: 626.6591796875 = 0.7535055875778198 + 100.0 * 6.25905704498291
Epoch 730, val loss: 0.9724783301353455
Epoch 740, training loss: 626.6360473632812 = 0.7389593720436096 + 100.0 * 6.258970737457275
Epoch 740, val loss: 0.9653292894363403
Epoch 750, training loss: 626.8986206054688 = 0.7246893644332886 + 100.0 * 6.261739730834961
Epoch 750, val loss: 0.9584287405014038
Epoch 760, training loss: 626.8677368164062 = 0.7105447053909302 + 100.0 * 6.261571407318115
Epoch 760, val loss: 0.9514383673667908
Epoch 770, training loss: 626.2846069335938 = 0.6965105533599854 + 100.0 * 6.255880832672119
Epoch 770, val loss: 0.9448734521865845
Epoch 780, training loss: 626.1887817382812 = 0.6829077005386353 + 100.0 * 6.255058765411377
Epoch 780, val loss: 0.938709557056427
Epoch 790, training loss: 626.2789916992188 = 0.669674277305603 + 100.0 * 6.2560930252075195
Epoch 790, val loss: 0.9327815771102905
Epoch 800, training loss: 626.0664672851562 = 0.6564558148384094 + 100.0 * 6.254100322723389
Epoch 800, val loss: 0.9273289442062378
Epoch 810, training loss: 625.8933715820312 = 0.6435245275497437 + 100.0 * 6.252498149871826
Epoch 810, val loss: 0.9215553998947144
Epoch 820, training loss: 625.818603515625 = 0.6308767795562744 + 100.0 * 6.251877307891846
Epoch 820, val loss: 0.9165241122245789
Epoch 830, training loss: 626.2742309570312 = 0.6184428334236145 + 100.0 * 6.256557464599609
Epoch 830, val loss: 0.9114587903022766
Epoch 840, training loss: 625.7216186523438 = 0.6063547730445862 + 100.0 * 6.251152515411377
Epoch 840, val loss: 0.9068752527236938
Epoch 850, training loss: 625.5630493164062 = 0.5943704843521118 + 100.0 * 6.2496867179870605
Epoch 850, val loss: 0.9023039937019348
Epoch 860, training loss: 625.5966186523438 = 0.5827391743659973 + 100.0 * 6.250138282775879
Epoch 860, val loss: 0.8979746103286743
Epoch 870, training loss: 625.4188232421875 = 0.5713449716567993 + 100.0 * 6.248474597930908
Epoch 870, val loss: 0.8941646218299866
Epoch 880, training loss: 625.3089599609375 = 0.5602121353149414 + 100.0 * 6.2474870681762695
Epoch 880, val loss: 0.8902617692947388
Epoch 890, training loss: 625.5264282226562 = 0.5493499040603638 + 100.0 * 6.249770641326904
Epoch 890, val loss: 0.8869713544845581
Epoch 900, training loss: 625.1762084960938 = 0.5385884642601013 + 100.0 * 6.2463765144348145
Epoch 900, val loss: 0.8831391334533691
Epoch 910, training loss: 625.35107421875 = 0.5282167196273804 + 100.0 * 6.248228549957275
Epoch 910, val loss: 0.8801656365394592
Epoch 920, training loss: 624.8917846679688 = 0.5178223848342896 + 100.0 * 6.243739604949951
Epoch 920, val loss: 0.8770514130592346
Epoch 930, training loss: 624.83837890625 = 0.5078593492507935 + 100.0 * 6.243305206298828
Epoch 930, val loss: 0.8742289543151855
Epoch 940, training loss: 624.9574584960938 = 0.49812623858451843 + 100.0 * 6.244593620300293
Epoch 940, val loss: 0.8718894720077515
Epoch 950, training loss: 624.6983642578125 = 0.4885715842247009 + 100.0 * 6.242097854614258
Epoch 950, val loss: 0.8694100379943848
Epoch 960, training loss: 624.8181762695312 = 0.4792466461658478 + 100.0 * 6.243389129638672
Epoch 960, val loss: 0.8672875165939331
Epoch 970, training loss: 624.6671752929688 = 0.47008031606674194 + 100.0 * 6.241971015930176
Epoch 970, val loss: 0.8652023673057556
Epoch 980, training loss: 624.5496826171875 = 0.4611056447029114 + 100.0 * 6.2408857345581055
Epoch 980, val loss: 0.8632875680923462
Epoch 990, training loss: 624.7074584960938 = 0.4523881673812866 + 100.0 * 6.242550373077393
Epoch 990, val loss: 0.8618135452270508
Epoch 1000, training loss: 624.3338012695312 = 0.4437386691570282 + 100.0 * 6.238900661468506
Epoch 1000, val loss: 0.8601142764091492
Epoch 1010, training loss: 624.291259765625 = 0.43536481261253357 + 100.0 * 6.238558769226074
Epoch 1010, val loss: 0.8588238954544067
Epoch 1020, training loss: 624.1815795898438 = 0.42724552750587463 + 100.0 * 6.237543106079102
Epoch 1020, val loss: 0.8578606247901917
Epoch 1030, training loss: 624.3998413085938 = 0.41926512122154236 + 100.0 * 6.239805698394775
Epoch 1030, val loss: 0.8568305969238281
Epoch 1040, training loss: 624.46875 = 0.4114178419113159 + 100.0 * 6.240573406219482
Epoch 1040, val loss: 0.8559183478355408
Epoch 1050, training loss: 624.22314453125 = 0.40357154607772827 + 100.0 * 6.238195896148682
Epoch 1050, val loss: 0.8550723791122437
Epoch 1060, training loss: 623.9219970703125 = 0.396090030670166 + 100.0 * 6.235259056091309
Epoch 1060, val loss: 0.8545308709144592
Epoch 1070, training loss: 623.9149780273438 = 0.3888097107410431 + 100.0 * 6.235261917114258
Epoch 1070, val loss: 0.8542736172676086
Epoch 1080, training loss: 624.5130004882812 = 0.38164088129997253 + 100.0 * 6.241313457489014
Epoch 1080, val loss: 0.8540723919868469
Epoch 1090, training loss: 623.8526000976562 = 0.3744466006755829 + 100.0 * 6.234781742095947
Epoch 1090, val loss: 0.8538483381271362
Epoch 1100, training loss: 624.4505004882812 = 0.36757829785346985 + 100.0 * 6.2408294677734375
Epoch 1100, val loss: 0.8538427948951721
Epoch 1110, training loss: 623.9581909179688 = 0.36071646213531494 + 100.0 * 6.2359747886657715
Epoch 1110, val loss: 0.8537223935127258
Epoch 1120, training loss: 623.5737915039062 = 0.354083389043808 + 100.0 * 6.232197284698486
Epoch 1120, val loss: 0.8542328476905823
Epoch 1130, training loss: 623.49462890625 = 0.34765487909317017 + 100.0 * 6.231469631195068
Epoch 1130, val loss: 0.8546965718269348
Epoch 1140, training loss: 623.4302368164062 = 0.34139230847358704 + 100.0 * 6.230888366699219
Epoch 1140, val loss: 0.8553109169006348
Epoch 1150, training loss: 623.4334106445312 = 0.3352331817150116 + 100.0 * 6.230982303619385
Epoch 1150, val loss: 0.8559576869010925
Epoch 1160, training loss: 623.841064453125 = 0.3290724754333496 + 100.0 * 6.235119342803955
Epoch 1160, val loss: 0.8565943241119385
Epoch 1170, training loss: 623.57763671875 = 0.32295432686805725 + 100.0 * 6.232547283172607
Epoch 1170, val loss: 0.8571245670318604
Epoch 1180, training loss: 623.2319946289062 = 0.31709620356559753 + 100.0 * 6.229148864746094
Epoch 1180, val loss: 0.8584041595458984
Epoch 1190, training loss: 623.2380981445312 = 0.3114137649536133 + 100.0 * 6.229267120361328
Epoch 1190, val loss: 0.8597405552864075
Epoch 1200, training loss: 623.24267578125 = 0.30586254596710205 + 100.0 * 6.229368209838867
Epoch 1200, val loss: 0.8610868453979492
Epoch 1210, training loss: 623.4790649414062 = 0.300361692905426 + 100.0 * 6.231787204742432
Epoch 1210, val loss: 0.8622872829437256
Epoch 1220, training loss: 623.5120849609375 = 0.29485100507736206 + 100.0 * 6.232172012329102
Epoch 1220, val loss: 0.8634364604949951
Epoch 1230, training loss: 623.053466796875 = 0.2894953489303589 + 100.0 * 6.227639675140381
Epoch 1230, val loss: 0.8650385737419128
Epoch 1240, training loss: 622.9371948242188 = 0.28430992364883423 + 100.0 * 6.226529121398926
Epoch 1240, val loss: 0.8667134046554565
Epoch 1250, training loss: 623.23291015625 = 0.2792775332927704 + 100.0 * 6.229536056518555
Epoch 1250, val loss: 0.8684564828872681
Epoch 1260, training loss: 622.8412475585938 = 0.2741985619068146 + 100.0 * 6.22567081451416
Epoch 1260, val loss: 0.8700644373893738
Epoch 1270, training loss: 622.8216552734375 = 0.2692650854587555 + 100.0 * 6.225523948669434
Epoch 1270, val loss: 0.8719879984855652
Epoch 1280, training loss: 623.2200317382812 = 0.2644382417201996 + 100.0 * 6.229555606842041
Epoch 1280, val loss: 0.8739334940910339
Epoch 1290, training loss: 622.8651733398438 = 0.25968948006629944 + 100.0 * 6.226054668426514
Epoch 1290, val loss: 0.8760173916816711
Epoch 1300, training loss: 622.956298828125 = 0.25501537322998047 + 100.0 * 6.227012634277344
Epoch 1300, val loss: 0.8779346942901611
Epoch 1310, training loss: 622.689453125 = 0.25045397877693176 + 100.0 * 6.224390506744385
Epoch 1310, val loss: 0.8804323673248291
Epoch 1320, training loss: 622.6106567382812 = 0.2460147738456726 + 100.0 * 6.22364616394043
Epoch 1320, val loss: 0.8827628493309021
Epoch 1330, training loss: 622.7803344726562 = 0.24168437719345093 + 100.0 * 6.225386619567871
Epoch 1330, val loss: 0.8851487040519714
Epoch 1340, training loss: 622.56494140625 = 0.23733748495578766 + 100.0 * 6.223275661468506
Epoch 1340, val loss: 0.8876853585243225
Epoch 1350, training loss: 622.4967651367188 = 0.23308560252189636 + 100.0 * 6.222636699676514
Epoch 1350, val loss: 0.8901486396789551
Epoch 1360, training loss: 622.82666015625 = 0.2289569228887558 + 100.0 * 6.225977420806885
Epoch 1360, val loss: 0.8926206827163696
Epoch 1370, training loss: 622.5490112304688 = 0.22485770285129547 + 100.0 * 6.223241806030273
Epoch 1370, val loss: 0.8954153656959534
Epoch 1380, training loss: 622.6796875 = 0.2208440601825714 + 100.0 * 6.224588871002197
Epoch 1380, val loss: 0.898027241230011
Epoch 1390, training loss: 622.3284301757812 = 0.21687434613704681 + 100.0 * 6.221115589141846
Epoch 1390, val loss: 0.9007149934768677
Epoch 1400, training loss: 622.3718872070312 = 0.21302194893360138 + 100.0 * 6.221588611602783
Epoch 1400, val loss: 0.9036694169044495
Epoch 1410, training loss: 622.7042846679688 = 0.2092393934726715 + 100.0 * 6.224950313568115
Epoch 1410, val loss: 0.9065183997154236
Epoch 1420, training loss: 622.353759765625 = 0.2055068016052246 + 100.0 * 6.221482753753662
Epoch 1420, val loss: 0.9094886779785156
Epoch 1430, training loss: 622.2899169921875 = 0.20184184610843658 + 100.0 * 6.22088098526001
Epoch 1430, val loss: 0.9125804901123047
Epoch 1440, training loss: 622.3721923828125 = 0.1982646882534027 + 100.0 * 6.221739292144775
Epoch 1440, val loss: 0.9155852198600769
Epoch 1450, training loss: 622.1641845703125 = 0.194716215133667 + 100.0 * 6.2196946144104
Epoch 1450, val loss: 0.9187067151069641
Epoch 1460, training loss: 622.11328125 = 0.1912689059972763 + 100.0 * 6.219220161437988
Epoch 1460, val loss: 0.922098696231842
Epoch 1470, training loss: 622.3006591796875 = 0.18786904215812683 + 100.0 * 6.221127986907959
Epoch 1470, val loss: 0.9251236319541931
Epoch 1480, training loss: 622.0662841796875 = 0.1844775378704071 + 100.0 * 6.218818187713623
Epoch 1480, val loss: 0.9282824993133545
Epoch 1490, training loss: 622.0482177734375 = 0.18119245767593384 + 100.0 * 6.218669891357422
Epoch 1490, val loss: 0.9318059086799622
Epoch 1500, training loss: 622.3079833984375 = 0.17796726524829865 + 100.0 * 6.22130012512207
Epoch 1500, val loss: 0.9351122975349426
Epoch 1510, training loss: 621.8616943359375 = 0.17477546632289886 + 100.0 * 6.216868877410889
Epoch 1510, val loss: 0.9386825561523438
Epoch 1520, training loss: 621.76513671875 = 0.17167972028255463 + 100.0 * 6.2159342765808105
Epoch 1520, val loss: 0.9421588182449341
Epoch 1530, training loss: 621.8269653320312 = 0.16865848004817963 + 100.0 * 6.216583251953125
Epoch 1530, val loss: 0.9458200931549072
Epoch 1540, training loss: 622.2379760742188 = 0.16568733751773834 + 100.0 * 6.220722675323486
Epoch 1540, val loss: 0.9494845867156982
Epoch 1550, training loss: 621.9224243164062 = 0.1626877635717392 + 100.0 * 6.217597484588623
Epoch 1550, val loss: 0.9525753259658813
Epoch 1560, training loss: 621.7471923828125 = 0.15977860987186432 + 100.0 * 6.215873718261719
Epoch 1560, val loss: 0.9564999938011169
Epoch 1570, training loss: 621.8302612304688 = 0.1569458246231079 + 100.0 * 6.216733455657959
Epoch 1570, val loss: 0.9601335525512695
Epoch 1580, training loss: 621.6962280273438 = 0.15415306389331818 + 100.0 * 6.215420722961426
Epoch 1580, val loss: 0.9637439846992493
Epoch 1590, training loss: 621.8914794921875 = 0.15143954753875732 + 100.0 * 6.217400550842285
Epoch 1590, val loss: 0.9674338102340698
Epoch 1600, training loss: 621.6331787109375 = 0.14874514937400818 + 100.0 * 6.214844226837158
Epoch 1600, val loss: 0.9714828133583069
Epoch 1610, training loss: 621.5800170898438 = 0.14614030718803406 + 100.0 * 6.214338779449463
Epoch 1610, val loss: 0.975257933139801
Epoch 1620, training loss: 621.6602172851562 = 0.14357087016105652 + 100.0 * 6.2151665687561035
Epoch 1620, val loss: 0.9790215492248535
Epoch 1630, training loss: 621.6058959960938 = 0.14104896783828735 + 100.0 * 6.214648246765137
Epoch 1630, val loss: 0.9830986857414246
Epoch 1640, training loss: 621.523193359375 = 0.1385502815246582 + 100.0 * 6.213846206665039
Epoch 1640, val loss: 0.986732542514801
Epoch 1650, training loss: 621.6159057617188 = 0.13610239326953888 + 100.0 * 6.2147979736328125
Epoch 1650, val loss: 0.9906313419342041
Epoch 1660, training loss: 621.6029663085938 = 0.13373178243637085 + 100.0 * 6.214692115783691
Epoch 1660, val loss: 0.994770884513855
Epoch 1670, training loss: 621.6199951171875 = 0.13136035203933716 + 100.0 * 6.214886665344238
Epoch 1670, val loss: 0.9983261823654175
Epoch 1680, training loss: 621.3314208984375 = 0.12904906272888184 + 100.0 * 6.212024211883545
Epoch 1680, val loss: 1.002497673034668
Epoch 1690, training loss: 621.3449096679688 = 0.12681378424167633 + 100.0 * 6.2121806144714355
Epoch 1690, val loss: 1.006531834602356
Epoch 1700, training loss: 621.5955810546875 = 0.12462092936038971 + 100.0 * 6.214709281921387
Epoch 1700, val loss: 1.010406255722046
Epoch 1710, training loss: 621.351318359375 = 0.1224430501461029 + 100.0 * 6.212288856506348
Epoch 1710, val loss: 1.0143433809280396
Epoch 1720, training loss: 621.29931640625 = 0.12030710279941559 + 100.0 * 6.211790084838867
Epoch 1720, val loss: 1.0186378955841064
Epoch 1730, training loss: 621.2633056640625 = 0.11821229755878448 + 100.0 * 6.211451053619385
Epoch 1730, val loss: 1.0225107669830322
Epoch 1740, training loss: 621.2962646484375 = 0.11618553102016449 + 100.0 * 6.211800575256348
Epoch 1740, val loss: 1.0265426635742188
Epoch 1750, training loss: 621.6073608398438 = 0.11419093608856201 + 100.0 * 6.214931488037109
Epoch 1750, val loss: 1.0306802988052368
Epoch 1760, training loss: 621.2136840820312 = 0.11216305196285248 + 100.0 * 6.211015224456787
Epoch 1760, val loss: 1.0344276428222656
Epoch 1770, training loss: 621.0618896484375 = 0.11025290191173553 + 100.0 * 6.209516525268555
Epoch 1770, val loss: 1.038843035697937
Epoch 1780, training loss: 621.0160522460938 = 0.10838336497545242 + 100.0 * 6.209076404571533
Epoch 1780, val loss: 1.0428521633148193
Epoch 1790, training loss: 621.728271484375 = 0.10654500871896744 + 100.0 * 6.216217041015625
Epoch 1790, val loss: 1.0469790697097778
Epoch 1800, training loss: 621.3897705078125 = 0.10470588505268097 + 100.0 * 6.212850093841553
Epoch 1800, val loss: 1.0505450963974
Epoch 1810, training loss: 621.1022338867188 = 0.10289005935192108 + 100.0 * 6.209993362426758
Epoch 1810, val loss: 1.054945945739746
Epoch 1820, training loss: 621.149658203125 = 0.10116051882505417 + 100.0 * 6.210484981536865
Epoch 1820, val loss: 1.0589826107025146
Epoch 1830, training loss: 620.9366455078125 = 0.09942888468503952 + 100.0 * 6.208372116088867
Epoch 1830, val loss: 1.0628128051757812
Epoch 1840, training loss: 620.8895874023438 = 0.09776383638381958 + 100.0 * 6.207918167114258
Epoch 1840, val loss: 1.0669325590133667
Epoch 1850, training loss: 620.85400390625 = 0.09614307433366776 + 100.0 * 6.207578659057617
Epoch 1850, val loss: 1.0710378885269165
Epoch 1860, training loss: 620.97216796875 = 0.09455476701259613 + 100.0 * 6.208775997161865
Epoch 1860, val loss: 1.0751121044158936
Epoch 1870, training loss: 621.0780639648438 = 0.09297285974025726 + 100.0 * 6.209851264953613
Epoch 1870, val loss: 1.0791836977005005
Epoch 1880, training loss: 620.9287719726562 = 0.09138619899749756 + 100.0 * 6.2083740234375
Epoch 1880, val loss: 1.0831221342086792
Epoch 1890, training loss: 620.901611328125 = 0.08984433859586716 + 100.0 * 6.208117485046387
Epoch 1890, val loss: 1.0870246887207031
Epoch 1900, training loss: 621.1605224609375 = 0.08837136626243591 + 100.0 * 6.210721492767334
Epoch 1900, val loss: 1.0911517143249512
Epoch 1910, training loss: 620.7335815429688 = 0.08685606718063354 + 100.0 * 6.206467151641846
Epoch 1910, val loss: 1.094849705696106
Epoch 1920, training loss: 620.677734375 = 0.08542253822088242 + 100.0 * 6.205923080444336
Epoch 1920, val loss: 1.0991742610931396
Epoch 1930, training loss: 620.6693115234375 = 0.08404441922903061 + 100.0 * 6.205852508544922
Epoch 1930, val loss: 1.103316307067871
Epoch 1940, training loss: 620.71728515625 = 0.08269809931516647 + 100.0 * 6.206346035003662
Epoch 1940, val loss: 1.1073118448257446
Epoch 1950, training loss: 621.1316528320312 = 0.08136902749538422 + 100.0 * 6.210502624511719
Epoch 1950, val loss: 1.1111215353012085
Epoch 1960, training loss: 620.9107055664062 = 0.07999106496572495 + 100.0 * 6.208306789398193
Epoch 1960, val loss: 1.1147756576538086
Epoch 1970, training loss: 620.6685791015625 = 0.07868923246860504 + 100.0 * 6.205898761749268
Epoch 1970, val loss: 1.1188502311706543
Epoch 1980, training loss: 620.576416015625 = 0.07742352783679962 + 100.0 * 6.204989910125732
Epoch 1980, val loss: 1.1228159666061401
Epoch 1990, training loss: 621.0103149414062 = 0.07620959728956223 + 100.0 * 6.209340572357178
Epoch 1990, val loss: 1.126573920249939
Epoch 2000, training loss: 620.6459350585938 = 0.07494886964559555 + 100.0 * 6.205709934234619
Epoch 2000, val loss: 1.1306051015853882
Epoch 2010, training loss: 620.65771484375 = 0.07374323159456253 + 100.0 * 6.20583963394165
Epoch 2010, val loss: 1.1345120668411255
Epoch 2020, training loss: 620.611083984375 = 0.07255840301513672 + 100.0 * 6.205385208129883
Epoch 2020, val loss: 1.1383416652679443
Epoch 2030, training loss: 620.5361938476562 = 0.0713975802063942 + 100.0 * 6.204648017883301
Epoch 2030, val loss: 1.1421818733215332
Epoch 2040, training loss: 620.6143798828125 = 0.0702720507979393 + 100.0 * 6.205440998077393
Epoch 2040, val loss: 1.1460108757019043
Epoch 2050, training loss: 620.5971069335938 = 0.06916593760251999 + 100.0 * 6.20527982711792
Epoch 2050, val loss: 1.1498609781265259
Epoch 2060, training loss: 620.622314453125 = 0.06807833909988403 + 100.0 * 6.20554256439209
Epoch 2060, val loss: 1.1537086963653564
Epoch 2070, training loss: 620.4971923828125 = 0.06701038777828217 + 100.0 * 6.204301834106445
Epoch 2070, val loss: 1.1575978994369507
Epoch 2080, training loss: 620.6199340820312 = 0.06596635282039642 + 100.0 * 6.205539703369141
Epoch 2080, val loss: 1.1612080335617065
Epoch 2090, training loss: 620.412109375 = 0.06492149829864502 + 100.0 * 6.203471660614014
Epoch 2090, val loss: 1.1652079820632935
Epoch 2100, training loss: 620.428466796875 = 0.06392422318458557 + 100.0 * 6.203645706176758
Epoch 2100, val loss: 1.1691372394561768
Epoch 2110, training loss: 620.6688842773438 = 0.06293772906064987 + 100.0 * 6.206059455871582
Epoch 2110, val loss: 1.1725326776504517
Epoch 2120, training loss: 620.4331665039062 = 0.06195538491010666 + 100.0 * 6.203712463378906
Epoch 2120, val loss: 1.1761257648468018
Epoch 2130, training loss: 620.5184326171875 = 0.06099661439657211 + 100.0 * 6.2045745849609375
Epoch 2130, val loss: 1.1800360679626465
Epoch 2140, training loss: 620.5165405273438 = 0.060063451528549194 + 100.0 * 6.204564571380615
Epoch 2140, val loss: 1.1835527420043945
Epoch 2150, training loss: 620.3870239257812 = 0.059147514402866364 + 100.0 * 6.2032790184021
Epoch 2150, val loss: 1.1875883340835571
Epoch 2160, training loss: 620.2197875976562 = 0.058233194053173065 + 100.0 * 6.201615810394287
Epoch 2160, val loss: 1.1911901235580444
Epoch 2170, training loss: 620.1861572265625 = 0.05736912041902542 + 100.0 * 6.201287746429443
Epoch 2170, val loss: 1.1950500011444092
Epoch 2180, training loss: 620.2294311523438 = 0.05652792751789093 + 100.0 * 6.2017292976379395
Epoch 2180, val loss: 1.198875904083252
Epoch 2190, training loss: 620.6536865234375 = 0.055704452097415924 + 100.0 * 6.205979347229004
Epoch 2190, val loss: 1.2026170492172241
Epoch 2200, training loss: 620.472900390625 = 0.05483793094754219 + 100.0 * 6.204180717468262
Epoch 2200, val loss: 1.205452799797058
Epoch 2210, training loss: 620.7740478515625 = 0.05401493236422539 + 100.0 * 6.207200050354004
Epoch 2210, val loss: 1.2093485593795776
Epoch 2220, training loss: 620.3326416015625 = 0.0531657449901104 + 100.0 * 6.202795028686523
Epoch 2220, val loss: 1.2124251127243042
Epoch 2230, training loss: 620.1439819335938 = 0.052390843629837036 + 100.0 * 6.200916290283203
Epoch 2230, val loss: 1.2165563106536865
Epoch 2240, training loss: 620.2344360351562 = 0.05163055285811424 + 100.0 * 6.2018280029296875
Epoch 2240, val loss: 1.220046043395996
Epoch 2250, training loss: 620.2506713867188 = 0.05087161064147949 + 100.0 * 6.201998233795166
Epoch 2250, val loss: 1.2234716415405273
Epoch 2260, training loss: 620.186767578125 = 0.05012529343366623 + 100.0 * 6.201366424560547
Epoch 2260, val loss: 1.227075457572937
Epoch 2270, training loss: 620.9241333007812 = 0.049403946846723557 + 100.0 * 6.208747863769531
Epoch 2270, val loss: 1.2302583456039429
Epoch 2280, training loss: 620.2784423828125 = 0.048664841800928116 + 100.0 * 6.202298164367676
Epoch 2280, val loss: 1.2342588901519775
Epoch 2290, training loss: 620.0486450195312 = 0.047942791134119034 + 100.0 * 6.200007438659668
Epoch 2290, val loss: 1.237323522567749
Epoch 2300, training loss: 619.9892578125 = 0.04726418852806091 + 100.0 * 6.19942045211792
Epoch 2300, val loss: 1.2411854267120361
Epoch 2310, training loss: 620.0338134765625 = 0.04660492390394211 + 100.0 * 6.199872016906738
Epoch 2310, val loss: 1.2445852756500244
Epoch 2320, training loss: 620.5665893554688 = 0.045954201370477676 + 100.0 * 6.205206394195557
Epoch 2320, val loss: 1.2480077743530273
Epoch 2330, training loss: 620.079345703125 = 0.045278046280145645 + 100.0 * 6.200340270996094
Epoch 2330, val loss: 1.2514148950576782
Epoch 2340, training loss: 620.0059204101562 = 0.04463132098317146 + 100.0 * 6.199613094329834
Epoch 2340, val loss: 1.2546138763427734
Epoch 2350, training loss: 620.0670776367188 = 0.044007308781147 + 100.0 * 6.200230598449707
Epoch 2350, val loss: 1.2582727670669556
Epoch 2360, training loss: 620.0444946289062 = 0.04339184984564781 + 100.0 * 6.200011253356934
Epoch 2360, val loss: 1.2615419626235962
Epoch 2370, training loss: 619.9456176757812 = 0.042773157358169556 + 100.0 * 6.199028015136719
Epoch 2370, val loss: 1.2647144794464111
Epoch 2380, training loss: 620.3469848632812 = 0.04218536615371704 + 100.0 * 6.203048229217529
Epoch 2380, val loss: 1.2683353424072266
Epoch 2390, training loss: 619.8880004882812 = 0.04158766567707062 + 100.0 * 6.198464393615723
Epoch 2390, val loss: 1.2711737155914307
Epoch 2400, training loss: 619.908203125 = 0.041006773710250854 + 100.0 * 6.198671817779541
Epoch 2400, val loss: 1.2745628356933594
Epoch 2410, training loss: 620.277099609375 = 0.040452126413583755 + 100.0 * 6.202366352081299
Epoch 2410, val loss: 1.2781214714050293
Epoch 2420, training loss: 619.8826904296875 = 0.039882831275463104 + 100.0 * 6.198428153991699
Epoch 2420, val loss: 1.2809168100357056
Epoch 2430, training loss: 619.90087890625 = 0.03933308273553848 + 100.0 * 6.198615550994873
Epoch 2430, val loss: 1.2844728231430054
Epoch 2440, training loss: 620.024169921875 = 0.03880319744348526 + 100.0 * 6.199853420257568
Epoch 2440, val loss: 1.2876349687576294
Epoch 2450, training loss: 620.0125732421875 = 0.038267478346824646 + 100.0 * 6.199742794036865
Epoch 2450, val loss: 1.2906334400177002
Epoch 2460, training loss: 619.9074096679688 = 0.03774559497833252 + 100.0 * 6.198696613311768
Epoch 2460, val loss: 1.2937700748443604
Epoch 2470, training loss: 619.8406372070312 = 0.03723588585853577 + 100.0 * 6.198033809661865
Epoch 2470, val loss: 1.2971315383911133
Epoch 2480, training loss: 619.9706420898438 = 0.03674672171473503 + 100.0 * 6.199338912963867
Epoch 2480, val loss: 1.3004405498504639
Epoch 2490, training loss: 619.9549560546875 = 0.03625568747520447 + 100.0 * 6.1991868019104
Epoch 2490, val loss: 1.3032499551773071
Epoch 2500, training loss: 619.9295654296875 = 0.03576938435435295 + 100.0 * 6.198937892913818
Epoch 2500, val loss: 1.3068379163742065
Epoch 2510, training loss: 619.7445068359375 = 0.03528834506869316 + 100.0 * 6.197092533111572
Epoch 2510, val loss: 1.3094862699508667
Epoch 2520, training loss: 619.7041625976562 = 0.0348292700946331 + 100.0 * 6.196692943572998
Epoch 2520, val loss: 1.312606692314148
Epoch 2530, training loss: 619.9642944335938 = 0.034388914704322815 + 100.0 * 6.199299335479736
Epoch 2530, val loss: 1.3160189390182495
Epoch 2540, training loss: 619.9752807617188 = 0.03393564373254776 + 100.0 * 6.199413299560547
Epoch 2540, val loss: 1.318816900253296
Epoch 2550, training loss: 619.78857421875 = 0.03346065431833267 + 100.0 * 6.197551250457764
Epoch 2550, val loss: 1.3216760158538818
Epoch 2560, training loss: 619.920166015625 = 0.03303662687540054 + 100.0 * 6.198871612548828
Epoch 2560, val loss: 1.3245843648910522
Epoch 2570, training loss: 619.7186279296875 = 0.03260117769241333 + 100.0 * 6.196860313415527
Epoch 2570, val loss: 1.3279697895050049
Epoch 2580, training loss: 619.6962890625 = 0.03219013661146164 + 100.0 * 6.196640968322754
Epoch 2580, val loss: 1.3309749364852905
Epoch 2590, training loss: 619.9794921875 = 0.0317811444401741 + 100.0 * 6.199476718902588
Epoch 2590, val loss: 1.3338438272476196
Epoch 2600, training loss: 619.8675537109375 = 0.03136415779590607 + 100.0 * 6.198361873626709
Epoch 2600, val loss: 1.3368217945098877
Epoch 2610, training loss: 619.6085205078125 = 0.030955281108617783 + 100.0 * 6.195775508880615
Epoch 2610, val loss: 1.3393486738204956
Epoch 2620, training loss: 619.5353393554688 = 0.030565138906240463 + 100.0 * 6.195047855377197
Epoch 2620, val loss: 1.3426297903060913
Epoch 2630, training loss: 619.557373046875 = 0.030195165425539017 + 100.0 * 6.1952714920043945
Epoch 2630, val loss: 1.345827579498291
Epoch 2640, training loss: 620.03466796875 = 0.02983853779733181 + 100.0 * 6.200047969818115
Epoch 2640, val loss: 1.3489211797714233
Epoch 2650, training loss: 619.7654418945312 = 0.029436204582452774 + 100.0 * 6.197360038757324
Epoch 2650, val loss: 1.3508375883102417
Epoch 2660, training loss: 619.7010498046875 = 0.02906179428100586 + 100.0 * 6.196720123291016
Epoch 2660, val loss: 1.3542169332504272
Epoch 2670, training loss: 619.644287109375 = 0.028693346306681633 + 100.0 * 6.196156024932861
Epoch 2670, val loss: 1.3568006753921509
Epoch 2680, training loss: 619.510498046875 = 0.028347909450531006 + 100.0 * 6.194821834564209
Epoch 2680, val loss: 1.3600552082061768
Epoch 2690, training loss: 619.496826171875 = 0.028009124100208282 + 100.0 * 6.194687843322754
Epoch 2690, val loss: 1.362881064414978
Epoch 2700, training loss: 619.8963012695312 = 0.02768390066921711 + 100.0 * 6.198686122894287
Epoch 2700, val loss: 1.3661117553710938
Epoch 2710, training loss: 619.5689697265625 = 0.02733432687819004 + 100.0 * 6.195416450500488
Epoch 2710, val loss: 1.3679457902908325
Epoch 2720, training loss: 619.814697265625 = 0.026998480781912804 + 100.0 * 6.197876930236816
Epoch 2720, val loss: 1.3708826303482056
Epoch 2730, training loss: 619.544921875 = 0.02665797248482704 + 100.0 * 6.1951823234558105
Epoch 2730, val loss: 1.3733676671981812
Epoch 2740, training loss: 619.4916381835938 = 0.026339218020439148 + 100.0 * 6.194652557373047
Epoch 2740, val loss: 1.3763177394866943
Epoch 2750, training loss: 619.5868530273438 = 0.026030678302049637 + 100.0 * 6.195608615875244
Epoch 2750, val loss: 1.3790807723999023
Epoch 2760, training loss: 619.513671875 = 0.02572040446102619 + 100.0 * 6.194879531860352
Epoch 2760, val loss: 1.381826400756836
Epoch 2770, training loss: 619.470947265625 = 0.025415191426873207 + 100.0 * 6.194455623626709
Epoch 2770, val loss: 1.3843317031860352
Epoch 2780, training loss: 619.3951416015625 = 0.025121431797742844 + 100.0 * 6.193699836730957
Epoch 2780, val loss: 1.3870707750320435
Epoch 2790, training loss: 619.46240234375 = 0.024836530908942223 + 100.0 * 6.194375514984131
Epoch 2790, val loss: 1.3900271654129028
Epoch 2800, training loss: 619.6494140625 = 0.0245527196675539 + 100.0 * 6.196248531341553
Epoch 2800, val loss: 1.3925083875656128
Epoch 2810, training loss: 619.8640747070312 = 0.024257667362689972 + 100.0 * 6.198398590087891
Epoch 2810, val loss: 1.3951420783996582
Epoch 2820, training loss: 619.6085815429688 = 0.02396601438522339 + 100.0 * 6.195846080780029
Epoch 2820, val loss: 1.3976387977600098
Epoch 2830, training loss: 619.6723022460938 = 0.023687968030571938 + 100.0 * 6.196485996246338
Epoch 2830, val loss: 1.400073766708374
Epoch 2840, training loss: 619.4226684570312 = 0.02339698188006878 + 100.0 * 6.193992614746094
Epoch 2840, val loss: 1.4028708934783936
Epoch 2850, training loss: 619.3110961914062 = 0.023135097697377205 + 100.0 * 6.192879676818848
Epoch 2850, val loss: 1.4056328535079956
Epoch 2860, training loss: 619.324951171875 = 0.022881314158439636 + 100.0 * 6.193020343780518
Epoch 2860, val loss: 1.4085359573364258
Epoch 2870, training loss: 619.5508422851562 = 0.022633958607912064 + 100.0 * 6.195281982421875
Epoch 2870, val loss: 1.4113625288009644
Epoch 2880, training loss: 619.4638671875 = 0.022374985739588737 + 100.0 * 6.1944146156311035
Epoch 2880, val loss: 1.4135383367538452
Epoch 2890, training loss: 619.4993286132812 = 0.022116657346487045 + 100.0 * 6.194772243499756
Epoch 2890, val loss: 1.4156675338745117
Epoch 2900, training loss: 619.32080078125 = 0.02186909317970276 + 100.0 * 6.192989349365234
Epoch 2900, val loss: 1.4184974431991577
Epoch 2910, training loss: 619.3170166015625 = 0.02162904664874077 + 100.0 * 6.192954063415527
Epoch 2910, val loss: 1.4210723638534546
Epoch 2920, training loss: 619.77001953125 = 0.02139282040297985 + 100.0 * 6.197486400604248
Epoch 2920, val loss: 1.42343270778656
Epoch 2930, training loss: 619.6268920898438 = 0.021153151988983154 + 100.0 * 6.196057319641113
Epoch 2930, val loss: 1.4259458780288696
Epoch 2940, training loss: 619.318603515625 = 0.020911529660224915 + 100.0 * 6.192976951599121
Epoch 2940, val loss: 1.427947759628296
Epoch 2950, training loss: 619.226806640625 = 0.02068629488348961 + 100.0 * 6.192060947418213
Epoch 2950, val loss: 1.4310038089752197
Epoch 2960, training loss: 619.1822509765625 = 0.02046685665845871 + 100.0 * 6.191617965698242
Epoch 2960, val loss: 1.4335813522338867
Epoch 2970, training loss: 619.6005249023438 = 0.020258191972970963 + 100.0 * 6.195802688598633
Epoch 2970, val loss: 1.4361306428909302
Epoch 2980, training loss: 619.3870239257812 = 0.020032338798046112 + 100.0 * 6.193670272827148
Epoch 2980, val loss: 1.4381023645401
Epoch 2990, training loss: 619.4662475585938 = 0.019812673330307007 + 100.0 * 6.194464206695557
Epoch 2990, val loss: 1.4408957958221436
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 861.6381225585938 = 1.9554481506347656 + 100.0 * 8.596826553344727
Epoch 0, val loss: 1.9553437232971191
Epoch 10, training loss: 861.5263671875 = 1.9460262060165405 + 100.0 * 8.595803260803223
Epoch 10, val loss: 1.9462461471557617
Epoch 20, training loss: 860.7074584960938 = 1.934401273727417 + 100.0 * 8.587730407714844
Epoch 20, val loss: 1.934533715248108
Epoch 30, training loss: 855.3406372070312 = 1.92007577419281 + 100.0 * 8.534205436706543
Epoch 30, val loss: 1.9196350574493408
Epoch 40, training loss: 825.6171875 = 1.9045495986938477 + 100.0 * 8.237126350402832
Epoch 40, val loss: 1.9038547277450562
Epoch 50, training loss: 763.32861328125 = 1.8872301578521729 + 100.0 * 7.614414215087891
Epoch 50, val loss: 1.8859386444091797
Epoch 60, training loss: 742.546142578125 = 1.8699486255645752 + 100.0 * 7.40676212310791
Epoch 60, val loss: 1.8691965341567993
Epoch 70, training loss: 722.2581787109375 = 1.8544769287109375 + 100.0 * 7.204036712646484
Epoch 70, val loss: 1.8545373678207397
Epoch 80, training loss: 703.9822387695312 = 1.8411235809326172 + 100.0 * 7.021411418914795
Epoch 80, val loss: 1.8426978588104248
Epoch 90, training loss: 689.5855712890625 = 1.8315612077713013 + 100.0 * 6.877540111541748
Epoch 90, val loss: 1.8342056274414062
Epoch 100, training loss: 677.5120849609375 = 1.8234169483184814 + 100.0 * 6.7568864822387695
Epoch 100, val loss: 1.826298713684082
Epoch 110, training loss: 671.6063232421875 = 1.8152859210968018 + 100.0 * 6.697910308837891
Epoch 110, val loss: 1.8178021907806396
Epoch 120, training loss: 667.0324096679688 = 1.8065886497497559 + 100.0 * 6.652258396148682
Epoch 120, val loss: 1.8086782693862915
Epoch 130, training loss: 663.7562866210938 = 1.7983978986740112 + 100.0 * 6.619578838348389
Epoch 130, val loss: 1.8002663850784302
Epoch 140, training loss: 661.037841796875 = 1.7909616231918335 + 100.0 * 6.592468738555908
Epoch 140, val loss: 1.792662501335144
Epoch 150, training loss: 658.6281127929688 = 1.7832920551300049 + 100.0 * 6.568448543548584
Epoch 150, val loss: 1.784881830215454
Epoch 160, training loss: 656.3968505859375 = 1.775145173072815 + 100.0 * 6.54621696472168
Epoch 160, val loss: 1.7766361236572266
Epoch 170, training loss: 654.3509521484375 = 1.76641047000885 + 100.0 * 6.525845527648926
Epoch 170, val loss: 1.7679907083511353
Epoch 180, training loss: 652.7844848632812 = 1.7570042610168457 + 100.0 * 6.510274887084961
Epoch 180, val loss: 1.7589308023452759
Epoch 190, training loss: 650.6841430664062 = 1.746835470199585 + 100.0 * 6.489373207092285
Epoch 190, val loss: 1.7491902112960815
Epoch 200, training loss: 648.7708740234375 = 1.7359461784362793 + 100.0 * 6.470348834991455
Epoch 200, val loss: 1.7389805316925049
Epoch 210, training loss: 647.9013671875 = 1.7242239713668823 + 100.0 * 6.461771011352539
Epoch 210, val loss: 1.727906346321106
Epoch 220, training loss: 645.8106689453125 = 1.7110974788665771 + 100.0 * 6.440995216369629
Epoch 220, val loss: 1.7157727479934692
Epoch 230, training loss: 644.5879516601562 = 1.6968201398849487 + 100.0 * 6.428911209106445
Epoch 230, val loss: 1.7025794982910156
Epoch 240, training loss: 643.4862060546875 = 1.6813533306121826 + 100.0 * 6.418048858642578
Epoch 240, val loss: 1.6883795261383057
Epoch 250, training loss: 642.4581909179688 = 1.6645971536636353 + 100.0 * 6.407936096191406
Epoch 250, val loss: 1.6730732917785645
Epoch 260, training loss: 642.063232421875 = 1.646453619003296 + 100.0 * 6.404168128967285
Epoch 260, val loss: 1.6565032005310059
Epoch 270, training loss: 640.7372436523438 = 1.627028465270996 + 100.0 * 6.391102313995361
Epoch 270, val loss: 1.6389343738555908
Epoch 280, training loss: 639.7499389648438 = 1.6064647436141968 + 100.0 * 6.381434917449951
Epoch 280, val loss: 1.6204283237457275
Epoch 290, training loss: 639.3010864257812 = 1.584558129310608 + 100.0 * 6.377165794372559
Epoch 290, val loss: 1.60094153881073
Epoch 300, training loss: 638.32666015625 = 1.5614851713180542 + 100.0 * 6.36765193939209
Epoch 300, val loss: 1.5803815126419067
Epoch 310, training loss: 637.611328125 = 1.5372796058654785 + 100.0 * 6.3607401847839355
Epoch 310, val loss: 1.5593621730804443
Epoch 320, training loss: 636.9760131835938 = 1.512147068977356 + 100.0 * 6.354638576507568
Epoch 320, val loss: 1.5376557111740112
Epoch 330, training loss: 637.0679931640625 = 1.4862364530563354 + 100.0 * 6.355817794799805
Epoch 330, val loss: 1.5153846740722656
Epoch 340, training loss: 635.9739990234375 = 1.4592211246490479 + 100.0 * 6.345147609710693
Epoch 340, val loss: 1.492835283279419
Epoch 350, training loss: 635.4961547851562 = 1.4320117235183716 + 100.0 * 6.340641498565674
Epoch 350, val loss: 1.4702224731445312
Epoch 360, training loss: 634.9951782226562 = 1.404462456703186 + 100.0 * 6.335906982421875
Epoch 360, val loss: 1.4477174282073975
Epoch 370, training loss: 635.097900390625 = 1.3766858577728271 + 100.0 * 6.337211608886719
Epoch 370, val loss: 1.4254041910171509
Epoch 380, training loss: 634.3424682617188 = 1.3486913442611694 + 100.0 * 6.329937934875488
Epoch 380, val loss: 1.4030698537826538
Epoch 390, training loss: 633.8935546875 = 1.320810079574585 + 100.0 * 6.325727462768555
Epoch 390, val loss: 1.3812525272369385
Epoch 400, training loss: 633.530517578125 = 1.2930899858474731 + 100.0 * 6.32237434387207
Epoch 400, val loss: 1.359877586364746
Epoch 410, training loss: 633.0830688476562 = 1.265450358390808 + 100.0 * 6.31817626953125
Epoch 410, val loss: 1.3387306928634644
Epoch 420, training loss: 632.8213500976562 = 1.2381728887557983 + 100.0 * 6.315832138061523
Epoch 420, val loss: 1.3183430433273315
Epoch 430, training loss: 632.550537109375 = 1.2111902236938477 + 100.0 * 6.313393592834473
Epoch 430, val loss: 1.298155665397644
Epoch 440, training loss: 632.2490234375 = 1.1842622756958008 + 100.0 * 6.310647487640381
Epoch 440, val loss: 1.278670310974121
Epoch 450, training loss: 631.7767333984375 = 1.1579316854476929 + 100.0 * 6.306187629699707
Epoch 450, val loss: 1.259451985359192
Epoch 460, training loss: 631.5205688476562 = 1.1320033073425293 + 100.0 * 6.303885459899902
Epoch 460, val loss: 1.2408812046051025
Epoch 470, training loss: 632.211181640625 = 1.106349229812622 + 100.0 * 6.31104850769043
Epoch 470, val loss: 1.2228938341140747
Epoch 480, training loss: 631.0808715820312 = 1.0812078714370728 + 100.0 * 6.299996376037598
Epoch 480, val loss: 1.2049299478530884
Epoch 490, training loss: 630.7847290039062 = 1.0566612482070923 + 100.0 * 6.297280788421631
Epoch 490, val loss: 1.188037633895874
Epoch 500, training loss: 630.4080200195312 = 1.0325442552566528 + 100.0 * 6.293754577636719
Epoch 500, val loss: 1.1716123819351196
Epoch 510, training loss: 630.3198852539062 = 1.0090577602386475 + 100.0 * 6.2931084632873535
Epoch 510, val loss: 1.1555861234664917
Epoch 520, training loss: 630.155517578125 = 0.986038088798523 + 100.0 * 6.2916951179504395
Epoch 520, val loss: 1.1406439542770386
Epoch 530, training loss: 629.7213745117188 = 0.9636678099632263 + 100.0 * 6.287576675415039
Epoch 530, val loss: 1.1259517669677734
Epoch 540, training loss: 629.7545776367188 = 0.9418128132820129 + 100.0 * 6.288127422332764
Epoch 540, val loss: 1.111918568611145
Epoch 550, training loss: 629.3441162109375 = 0.9206412434577942 + 100.0 * 6.284234523773193
Epoch 550, val loss: 1.0983877182006836
Epoch 560, training loss: 629.0789184570312 = 0.9001378417015076 + 100.0 * 6.281787872314453
Epoch 560, val loss: 1.0856877565383911
Epoch 570, training loss: 629.0955200195312 = 0.8802071213722229 + 100.0 * 6.282153606414795
Epoch 570, val loss: 1.0735065937042236
Epoch 580, training loss: 629.2792358398438 = 0.860719621181488 + 100.0 * 6.284185409545898
Epoch 580, val loss: 1.0616850852966309
Epoch 590, training loss: 628.58056640625 = 0.841687798500061 + 100.0 * 6.277389049530029
Epoch 590, val loss: 1.0507569313049316
Epoch 600, training loss: 628.262939453125 = 0.8234586119651794 + 100.0 * 6.274394512176514
Epoch 600, val loss: 1.040441632270813
Epoch 610, training loss: 628.1215209960938 = 0.8057817816734314 + 100.0 * 6.273157596588135
Epoch 610, val loss: 1.0307481288909912
Epoch 620, training loss: 628.4746704101562 = 0.7886545062065125 + 100.0 * 6.276860237121582
Epoch 620, val loss: 1.02150559425354
Epoch 630, training loss: 628.0402221679688 = 0.7716056108474731 + 100.0 * 6.272686004638672
Epoch 630, val loss: 1.0126352310180664
Epoch 640, training loss: 627.6046752929688 = 0.7554264664649963 + 100.0 * 6.268492698669434
Epoch 640, val loss: 1.0045469999313354
Epoch 650, training loss: 627.4268188476562 = 0.7395919561386108 + 100.0 * 6.266872406005859
Epoch 650, val loss: 0.9970141649246216
Epoch 660, training loss: 627.4095458984375 = 0.7241751551628113 + 100.0 * 6.2668538093566895
Epoch 660, val loss: 0.9896625876426697
Epoch 670, training loss: 627.3594360351562 = 0.7090917229652405 + 100.0 * 6.26650333404541
Epoch 670, val loss: 0.9831085205078125
Epoch 680, training loss: 627.2781982421875 = 0.6943010687828064 + 100.0 * 6.265839099884033
Epoch 680, val loss: 0.9766952991485596
Epoch 690, training loss: 626.9436645507812 = 0.6799530982971191 + 100.0 * 6.262637615203857
Epoch 690, val loss: 0.9702640175819397
Epoch 700, training loss: 626.7889404296875 = 0.6660174131393433 + 100.0 * 6.261229515075684
Epoch 700, val loss: 0.9647266268730164
Epoch 710, training loss: 626.9910888671875 = 0.6524001359939575 + 100.0 * 6.2633867263793945
Epoch 710, val loss: 0.9593254923820496
Epoch 720, training loss: 626.68212890625 = 0.6388124823570251 + 100.0 * 6.260433197021484
Epoch 720, val loss: 0.9545813798904419
Epoch 730, training loss: 626.4909057617188 = 0.6257339715957642 + 100.0 * 6.2586517333984375
Epoch 730, val loss: 0.949562668800354
Epoch 740, training loss: 626.301513671875 = 0.6128564476966858 + 100.0 * 6.2568864822387695
Epoch 740, val loss: 0.9452400803565979
Epoch 750, training loss: 626.2723999023438 = 0.6003232598304749 + 100.0 * 6.256720542907715
Epoch 750, val loss: 0.941154956817627
Epoch 760, training loss: 626.1796875 = 0.5879568457603455 + 100.0 * 6.255917549133301
Epoch 760, val loss: 0.9373815655708313
Epoch 770, training loss: 626.029052734375 = 0.5757269859313965 + 100.0 * 6.254532814025879
Epoch 770, val loss: 0.9338725209236145
Epoch 780, training loss: 625.8447265625 = 0.5638427138328552 + 100.0 * 6.252808570861816
Epoch 780, val loss: 0.9303246140480042
Epoch 790, training loss: 626.1884155273438 = 0.5521652102470398 + 100.0 * 6.256362438201904
Epoch 790, val loss: 0.9271730780601501
Epoch 800, training loss: 625.9581909179688 = 0.5406319499015808 + 100.0 * 6.254175662994385
Epoch 800, val loss: 0.9245478510856628
Epoch 810, training loss: 625.780517578125 = 0.5292777419090271 + 100.0 * 6.252511978149414
Epoch 810, val loss: 0.9214386343955994
Epoch 820, training loss: 625.4432373046875 = 0.518064022064209 + 100.0 * 6.249251842498779
Epoch 820, val loss: 0.9187900424003601
Epoch 830, training loss: 625.4439697265625 = 0.5071240663528442 + 100.0 * 6.249368190765381
Epoch 830, val loss: 0.9167029857635498
Epoch 840, training loss: 625.471923828125 = 0.49636468291282654 + 100.0 * 6.249755859375
Epoch 840, val loss: 0.9144403338432312
Epoch 850, training loss: 625.8638916015625 = 0.4858718812465668 + 100.0 * 6.253779888153076
Epoch 850, val loss: 0.9125288724899292
Epoch 860, training loss: 625.2015991210938 = 0.4752626121044159 + 100.0 * 6.247263431549072
Epoch 860, val loss: 0.9103760123252869
Epoch 870, training loss: 625.0481567382812 = 0.46504607796669006 + 100.0 * 6.24583101272583
Epoch 870, val loss: 0.9088218212127686
Epoch 880, training loss: 624.8873291015625 = 0.4549640715122223 + 100.0 * 6.24432373046875
Epoch 880, val loss: 0.9072596430778503
Epoch 890, training loss: 624.917236328125 = 0.44504573941230774 + 100.0 * 6.24472188949585
Epoch 890, val loss: 0.9057663679122925
Epoch 900, training loss: 624.7584228515625 = 0.4351966977119446 + 100.0 * 6.243232250213623
Epoch 900, val loss: 0.9045913219451904
Epoch 910, training loss: 624.8687744140625 = 0.4255785346031189 + 100.0 * 6.244431972503662
Epoch 910, val loss: 0.903229832649231
Epoch 920, training loss: 624.590576171875 = 0.4161316454410553 + 100.0 * 6.241744518280029
Epoch 920, val loss: 0.9023345112800598
Epoch 930, training loss: 624.5421142578125 = 0.40689459443092346 + 100.0 * 6.241352081298828
Epoch 930, val loss: 0.9015077352523804
Epoch 940, training loss: 625.1177978515625 = 0.397809237241745 + 100.0 * 6.247200012207031
Epoch 940, val loss: 0.900809645652771
Epoch 950, training loss: 624.3440551757812 = 0.38872697949409485 + 100.0 * 6.239553451538086
Epoch 950, val loss: 0.9000605940818787
Epoch 960, training loss: 624.349609375 = 0.3798755705356598 + 100.0 * 6.239697456359863
Epoch 960, val loss: 0.8997939825057983
Epoch 970, training loss: 624.1873168945312 = 0.37130147218704224 + 100.0 * 6.238160133361816
Epoch 970, val loss: 0.8996790647506714
Epoch 980, training loss: 624.3395385742188 = 0.3628334105014801 + 100.0 * 6.239767074584961
Epoch 980, val loss: 0.8994835019111633
Epoch 990, training loss: 624.2778930664062 = 0.35443222522735596 + 100.0 * 6.239234447479248
Epoch 990, val loss: 0.8996436595916748
Epoch 1000, training loss: 624.1922607421875 = 0.34626585245132446 + 100.0 * 6.238460063934326
Epoch 1000, val loss: 0.899619996547699
Epoch 1010, training loss: 623.9536743164062 = 0.33830708265304565 + 100.0 * 6.236153602600098
Epoch 1010, val loss: 0.9001924991607666
Epoch 1020, training loss: 623.8651123046875 = 0.33048996329307556 + 100.0 * 6.235345840454102
Epoch 1020, val loss: 0.9006703495979309
Epoch 1030, training loss: 623.95361328125 = 0.32288333773612976 + 100.0 * 6.236307621002197
Epoch 1030, val loss: 0.9010447859764099
Epoch 1040, training loss: 624.0762939453125 = 0.3154083788394928 + 100.0 * 6.237608432769775
Epoch 1040, val loss: 0.9021345376968384
Epoch 1050, training loss: 623.7264404296875 = 0.30798086524009705 + 100.0 * 6.234184265136719
Epoch 1050, val loss: 0.9025524258613586
Epoch 1060, training loss: 623.6863403320312 = 0.30080655217170715 + 100.0 * 6.233855724334717
Epoch 1060, val loss: 0.9038485288619995
Epoch 1070, training loss: 623.552001953125 = 0.29383111000061035 + 100.0 * 6.232582092285156
Epoch 1070, val loss: 0.9049803018569946
Epoch 1080, training loss: 623.8240356445312 = 0.28703123331069946 + 100.0 * 6.23537015914917
Epoch 1080, val loss: 0.9062191247940063
Epoch 1090, training loss: 623.4993286132812 = 0.2803307771682739 + 100.0 * 6.232189655303955
Epoch 1090, val loss: 0.9080991148948669
Epoch 1100, training loss: 623.6141967773438 = 0.27380672097206116 + 100.0 * 6.233404159545898
Epoch 1100, val loss: 0.909650981426239
Epoch 1110, training loss: 623.7487182617188 = 0.26741474866867065 + 100.0 * 6.234813213348389
Epoch 1110, val loss: 0.9114384651184082
Epoch 1120, training loss: 623.3303833007812 = 0.261096715927124 + 100.0 * 6.2306928634643555
Epoch 1120, val loss: 0.9132716059684753
Epoch 1130, training loss: 623.2344360351562 = 0.25506412982940674 + 100.0 * 6.229793548583984
Epoch 1130, val loss: 0.9153386354446411
Epoch 1140, training loss: 623.169921875 = 0.2491571605205536 + 100.0 * 6.229207992553711
Epoch 1140, val loss: 0.9176791310310364
Epoch 1150, training loss: 623.126708984375 = 0.24342255294322968 + 100.0 * 6.228832721710205
Epoch 1150, val loss: 0.9200505018234253
Epoch 1160, training loss: 623.9395141601562 = 0.2378254383802414 + 100.0 * 6.237016677856445
Epoch 1160, val loss: 0.9227457046508789
Epoch 1170, training loss: 623.093017578125 = 0.23228776454925537 + 100.0 * 6.228607177734375
Epoch 1170, val loss: 0.9250864386558533
Epoch 1180, training loss: 622.9798583984375 = 0.22692924737930298 + 100.0 * 6.227529048919678
Epoch 1180, val loss: 0.9279881715774536
Epoch 1190, training loss: 623.3036499023438 = 0.22177965939044952 + 100.0 * 6.230818271636963
Epoch 1190, val loss: 0.931008517742157
Epoch 1200, training loss: 623.1509399414062 = 0.21668517589569092 + 100.0 * 6.229342937469482
Epoch 1200, val loss: 0.9341495633125305
Epoch 1210, training loss: 622.9833374023438 = 0.21169883012771606 + 100.0 * 6.227716445922852
Epoch 1210, val loss: 0.9370238780975342
Epoch 1220, training loss: 622.8253173828125 = 0.20688089728355408 + 100.0 * 6.226184368133545
Epoch 1220, val loss: 0.9405837655067444
Epoch 1230, training loss: 622.7348022460938 = 0.20223084092140198 + 100.0 * 6.225326061248779
Epoch 1230, val loss: 0.944100558757782
Epoch 1240, training loss: 623.2161254882812 = 0.1977052390575409 + 100.0 * 6.230184555053711
Epoch 1240, val loss: 0.9476664662361145
Epoch 1250, training loss: 622.9266357421875 = 0.1932772397994995 + 100.0 * 6.2273335456848145
Epoch 1250, val loss: 0.9513542056083679
Epoch 1260, training loss: 623.0178833007812 = 0.1889481395483017 + 100.0 * 6.2282891273498535
Epoch 1260, val loss: 0.9552780985832214
Epoch 1270, training loss: 622.559814453125 = 0.1846895068883896 + 100.0 * 6.223751068115234
Epoch 1270, val loss: 0.9587857723236084
Epoch 1280, training loss: 622.5415649414062 = 0.1805952936410904 + 100.0 * 6.223609447479248
Epoch 1280, val loss: 0.9627169966697693
Epoch 1290, training loss: 622.504150390625 = 0.17664624750614166 + 100.0 * 6.223275184631348
Epoch 1290, val loss: 0.9668796062469482
Epoch 1300, training loss: 622.9130859375 = 0.17278632521629333 + 100.0 * 6.227402687072754
Epoch 1300, val loss: 0.9711648225784302
Epoch 1310, training loss: 622.5670776367188 = 0.16903501749038696 + 100.0 * 6.22398042678833
Epoch 1310, val loss: 0.9751629829406738
Epoch 1320, training loss: 622.5828247070312 = 0.1653374284505844 + 100.0 * 6.224174499511719
Epoch 1320, val loss: 0.9797475337982178
Epoch 1330, training loss: 622.3663940429688 = 0.1617743968963623 + 100.0 * 6.2220458984375
Epoch 1330, val loss: 0.9838640093803406
Epoch 1340, training loss: 622.7208862304688 = 0.15826335549354553 + 100.0 * 6.225625991821289
Epoch 1340, val loss: 0.9880505800247192
Epoch 1350, training loss: 622.3369140625 = 0.1548720747232437 + 100.0 * 6.221820831298828
Epoch 1350, val loss: 0.993087649345398
Epoch 1360, training loss: 622.3232421875 = 0.15153706073760986 + 100.0 * 6.22171688079834
Epoch 1360, val loss: 0.9972491264343262
Epoch 1370, training loss: 622.1814575195312 = 0.14836780726909637 + 100.0 * 6.220330715179443
Epoch 1370, val loss: 1.0024948120117188
Epoch 1380, training loss: 622.0859985351562 = 0.14525854587554932 + 100.0 * 6.219407558441162
Epoch 1380, val loss: 1.0072383880615234
Epoch 1390, training loss: 622.0718994140625 = 0.14225342869758606 + 100.0 * 6.219296455383301
Epoch 1390, val loss: 1.0123201608657837
Epoch 1400, training loss: 623.018310546875 = 0.13933540880680084 + 100.0 * 6.228789806365967
Epoch 1400, val loss: 1.0175838470458984
Epoch 1410, training loss: 622.2942504882812 = 0.13640423119068146 + 100.0 * 6.221578598022461
Epoch 1410, val loss: 1.02203369140625
Epoch 1420, training loss: 621.9654541015625 = 0.13357794284820557 + 100.0 * 6.218318462371826
Epoch 1420, val loss: 1.0273430347442627
Epoch 1430, training loss: 622.0173950195312 = 0.1308540403842926 + 100.0 * 6.218865394592285
Epoch 1430, val loss: 1.0327247381210327
Epoch 1440, training loss: 622.3795166015625 = 0.12818193435668945 + 100.0 * 6.222513198852539
Epoch 1440, val loss: 1.0378602743148804
Epoch 1450, training loss: 621.9901123046875 = 0.12560594081878662 + 100.0 * 6.218645095825195
Epoch 1450, val loss: 1.0433413982391357
Epoch 1460, training loss: 621.842529296875 = 0.12307499349117279 + 100.0 * 6.2171950340271
Epoch 1460, val loss: 1.0487014055252075
Epoch 1470, training loss: 622.0763549804688 = 0.12060631811618805 + 100.0 * 6.219557762145996
Epoch 1470, val loss: 1.0540367364883423
Epoch 1480, training loss: 622.0021362304688 = 0.11820319294929504 + 100.0 * 6.218839168548584
Epoch 1480, val loss: 1.0596727132797241
Epoch 1490, training loss: 622.2921752929688 = 0.11584978550672531 + 100.0 * 6.2217631340026855
Epoch 1490, val loss: 1.0654585361480713
Epoch 1500, training loss: 621.8078002929688 = 0.11356553435325623 + 100.0 * 6.216942310333252
Epoch 1500, val loss: 1.0710920095443726
Epoch 1510, training loss: 621.7133178710938 = 0.1113252341747284 + 100.0 * 6.216020107269287
Epoch 1510, val loss: 1.0768622159957886
Epoch 1520, training loss: 621.6204833984375 = 0.10917932540178299 + 100.0 * 6.215112686157227
Epoch 1520, val loss: 1.082758903503418
Epoch 1530, training loss: 621.5919799804688 = 0.10707797110080719 + 100.0 * 6.214849472045898
Epoch 1530, val loss: 1.0885339975357056
Epoch 1540, training loss: 621.967529296875 = 0.10503003746271133 + 100.0 * 6.218624591827393
Epoch 1540, val loss: 1.0941227674484253
Epoch 1550, training loss: 621.8687744140625 = 0.10297935456037521 + 100.0 * 6.217658042907715
Epoch 1550, val loss: 1.0998677015304565
Epoch 1560, training loss: 621.5922241210938 = 0.10101962834596634 + 100.0 * 6.214911937713623
Epoch 1560, val loss: 1.1059881448745728
Epoch 1570, training loss: 621.5484619140625 = 0.09908362478017807 + 100.0 * 6.214493751525879
Epoch 1570, val loss: 1.111906886100769
Epoch 1580, training loss: 621.4932250976562 = 0.0972210094332695 + 100.0 * 6.21396017074585
Epoch 1580, val loss: 1.1176611185073853
Epoch 1590, training loss: 621.6690063476562 = 0.09540890902280807 + 100.0 * 6.215736389160156
Epoch 1590, val loss: 1.123535394668579
Epoch 1600, training loss: 621.6476440429688 = 0.0936310738325119 + 100.0 * 6.215540409088135
Epoch 1600, val loss: 1.1297690868377686
Epoch 1610, training loss: 621.4918212890625 = 0.09184988588094711 + 100.0 * 6.2139997482299805
Epoch 1610, val loss: 1.1355165243148804
Epoch 1620, training loss: 621.5206298828125 = 0.09013931453227997 + 100.0 * 6.2143049240112305
Epoch 1620, val loss: 1.1412773132324219
Epoch 1630, training loss: 621.6229248046875 = 0.08847920596599579 + 100.0 * 6.215343952178955
Epoch 1630, val loss: 1.1475142240524292
Epoch 1640, training loss: 621.5394897460938 = 0.08682795614004135 + 100.0 * 6.214526176452637
Epoch 1640, val loss: 1.1530814170837402
Epoch 1650, training loss: 621.4873046875 = 0.08520672470331192 + 100.0 * 6.2140212059021
Epoch 1650, val loss: 1.158915638923645
Epoch 1660, training loss: 621.3027954101562 = 0.08364036679267883 + 100.0 * 6.212191581726074
Epoch 1660, val loss: 1.1650891304016113
Epoch 1670, training loss: 621.2164916992188 = 0.08211915194988251 + 100.0 * 6.211344242095947
Epoch 1670, val loss: 1.1709972620010376
Epoch 1680, training loss: 621.70458984375 = 0.08062730729579926 + 100.0 * 6.2162394523620605
Epoch 1680, val loss: 1.176729679107666
Epoch 1690, training loss: 621.2940673828125 = 0.07916825264692307 + 100.0 * 6.212149143218994
Epoch 1690, val loss: 1.1828384399414062
Epoch 1700, training loss: 621.3982543945312 = 0.0777193158864975 + 100.0 * 6.213205814361572
Epoch 1700, val loss: 1.1885807514190674
Epoch 1710, training loss: 621.2750854492188 = 0.07632523775100708 + 100.0 * 6.211987018585205
Epoch 1710, val loss: 1.1947674751281738
Epoch 1720, training loss: 621.130859375 = 0.07495542615652084 + 100.0 * 6.210558891296387
Epoch 1720, val loss: 1.200860619544983
Epoch 1730, training loss: 621.126708984375 = 0.07363761961460114 + 100.0 * 6.210530757904053
Epoch 1730, val loss: 1.2070006132125854
Epoch 1740, training loss: 621.6876220703125 = 0.0723593458533287 + 100.0 * 6.216152191162109
Epoch 1740, val loss: 1.2130454778671265
Epoch 1750, training loss: 621.1856079101562 = 0.07102471590042114 + 100.0 * 6.211145877838135
Epoch 1750, val loss: 1.2182456254959106
Epoch 1760, training loss: 621.0218505859375 = 0.06976944953203201 + 100.0 * 6.2095208168029785
Epoch 1760, val loss: 1.224349021911621
Epoch 1770, training loss: 621.4379272460938 = 0.06854504346847534 + 100.0 * 6.213694095611572
Epoch 1770, val loss: 1.230105996131897
Epoch 1780, training loss: 620.9619750976562 = 0.06734393537044525 + 100.0 * 6.208946704864502
Epoch 1780, val loss: 1.2358148097991943
Epoch 1790, training loss: 620.9058837890625 = 0.06616479903459549 + 100.0 * 6.208397388458252
Epoch 1790, val loss: 1.2419624328613281
Epoch 1800, training loss: 620.8829956054688 = 0.06501230597496033 + 100.0 * 6.208179950714111
Epoch 1800, val loss: 1.2476669549942017
Epoch 1810, training loss: 620.9322509765625 = 0.06390418112277985 + 100.0 * 6.208683490753174
Epoch 1810, val loss: 1.2533831596374512
Epoch 1820, training loss: 621.480224609375 = 0.06280731409788132 + 100.0 * 6.214174270629883
Epoch 1820, val loss: 1.2588528394699097
Epoch 1830, training loss: 621.0945434570312 = 0.06171316280961037 + 100.0 * 6.210328578948975
Epoch 1830, val loss: 1.2650766372680664
Epoch 1840, training loss: 620.8436889648438 = 0.06064203009009361 + 100.0 * 6.207830429077148
Epoch 1840, val loss: 1.2709081172943115
Epoch 1850, training loss: 620.8165283203125 = 0.05961025133728981 + 100.0 * 6.207569122314453
Epoch 1850, val loss: 1.2766801118850708
Epoch 1860, training loss: 620.7449951171875 = 0.058606088161468506 + 100.0 * 6.206863880157471
Epoch 1860, val loss: 1.282482624053955
Epoch 1870, training loss: 621.0745239257812 = 0.05762083828449249 + 100.0 * 6.210169315338135
Epoch 1870, val loss: 1.2876684665679932
Epoch 1880, training loss: 620.7757568359375 = 0.056645430624485016 + 100.0 * 6.207191467285156
Epoch 1880, val loss: 1.2940902709960938
Epoch 1890, training loss: 620.8012084960938 = 0.05567871779203415 + 100.0 * 6.207455158233643
Epoch 1890, val loss: 1.299188494682312
Epoch 1900, training loss: 620.774169921875 = 0.05476411432027817 + 100.0 * 6.2071943283081055
Epoch 1900, val loss: 1.3055375814437866
Epoch 1910, training loss: 621.010986328125 = 0.05385690554976463 + 100.0 * 6.209571838378906
Epoch 1910, val loss: 1.310859203338623
Epoch 1920, training loss: 620.6884155273438 = 0.05296453461050987 + 100.0 * 6.20635461807251
Epoch 1920, val loss: 1.3167201280593872
Epoch 1930, training loss: 620.8033447265625 = 0.0520792119204998 + 100.0 * 6.207512855529785
Epoch 1930, val loss: 1.322409749031067
Epoch 1940, training loss: 620.8510131835938 = 0.05123447999358177 + 100.0 * 6.207997798919678
Epoch 1940, val loss: 1.327872633934021
Epoch 1950, training loss: 620.948974609375 = 0.0503818616271019 + 100.0 * 6.208986282348633
Epoch 1950, val loss: 1.333296775817871
Epoch 1960, training loss: 620.64794921875 = 0.04955654963850975 + 100.0 * 6.205984115600586
Epoch 1960, val loss: 1.339223027229309
Epoch 1970, training loss: 620.555419921875 = 0.04875028505921364 + 100.0 * 6.205066680908203
Epoch 1970, val loss: 1.3448529243469238
Epoch 1980, training loss: 620.6840209960938 = 0.04796968400478363 + 100.0 * 6.206360340118408
Epoch 1980, val loss: 1.3501518964767456
Epoch 1990, training loss: 620.8140869140625 = 0.047197286039590836 + 100.0 * 6.207669258117676
Epoch 1990, val loss: 1.3557441234588623
Epoch 2000, training loss: 620.7123413085938 = 0.046446602791547775 + 100.0 * 6.206658840179443
Epoch 2000, val loss: 1.3614702224731445
Epoch 2010, training loss: 620.77001953125 = 0.04570932313799858 + 100.0 * 6.207242965698242
Epoch 2010, val loss: 1.3669257164001465
Epoch 2020, training loss: 620.5062866210938 = 0.04497208818793297 + 100.0 * 6.204612731933594
Epoch 2020, val loss: 1.372773289680481
Epoch 2030, training loss: 620.4968872070312 = 0.04425973445177078 + 100.0 * 6.204526424407959
Epoch 2030, val loss: 1.3781129121780396
Epoch 2040, training loss: 620.7215576171875 = 0.04357420280575752 + 100.0 * 6.206779956817627
Epoch 2040, val loss: 1.383779764175415
Epoch 2050, training loss: 620.4193115234375 = 0.042881328612565994 + 100.0 * 6.203763961791992
Epoch 2050, val loss: 1.3894412517547607
Epoch 2060, training loss: 620.5177612304688 = 0.04219834879040718 + 100.0 * 6.204755783081055
Epoch 2060, val loss: 1.394667387008667
Epoch 2070, training loss: 621.1739501953125 = 0.04153513163328171 + 100.0 * 6.2113237380981445
Epoch 2070, val loss: 1.3999015092849731
Epoch 2080, training loss: 620.6141967773438 = 0.04090946540236473 + 100.0 * 6.205732822418213
Epoch 2080, val loss: 1.4062750339508057
Epoch 2090, training loss: 620.3951416015625 = 0.040251221507787704 + 100.0 * 6.203548908233643
Epoch 2090, val loss: 1.4111922979354858
Epoch 2100, training loss: 620.334716796875 = 0.03964591771364212 + 100.0 * 6.202950477600098
Epoch 2100, val loss: 1.4172313213348389
Epoch 2110, training loss: 620.43017578125 = 0.03904954344034195 + 100.0 * 6.203910827636719
Epoch 2110, val loss: 1.4224562644958496
Epoch 2120, training loss: 620.5770874023438 = 0.03846467658877373 + 100.0 * 6.205386638641357
Epoch 2120, val loss: 1.4278239011764526
Epoch 2130, training loss: 620.5950927734375 = 0.037866171449422836 + 100.0 * 6.205572605133057
Epoch 2130, val loss: 1.4331815242767334
Epoch 2140, training loss: 620.6865234375 = 0.037286657840013504 + 100.0 * 6.2064924240112305
Epoch 2140, val loss: 1.4387906789779663
Epoch 2150, training loss: 620.341552734375 = 0.0367157943546772 + 100.0 * 6.203048229217529
Epoch 2150, val loss: 1.4438505172729492
Epoch 2160, training loss: 620.2899780273438 = 0.036165639758110046 + 100.0 * 6.202538013458252
Epoch 2160, val loss: 1.4495397806167603
Epoch 2170, training loss: 620.2938842773438 = 0.035627078264951706 + 100.0 * 6.202582359313965
Epoch 2170, val loss: 1.4550246000289917
Epoch 2180, training loss: 620.349609375 = 0.03509946167469025 + 100.0 * 6.2031450271606445
Epoch 2180, val loss: 1.4601846933364868
Epoch 2190, training loss: 620.3123779296875 = 0.03458185866475105 + 100.0 * 6.202777862548828
Epoch 2190, val loss: 1.4655380249023438
Epoch 2200, training loss: 620.5798950195312 = 0.03407888859510422 + 100.0 * 6.205458164215088
Epoch 2200, val loss: 1.4712097644805908
Epoch 2210, training loss: 620.2243041992188 = 0.03357655927538872 + 100.0 * 6.201907634735107
Epoch 2210, val loss: 1.4766467809677124
Epoch 2220, training loss: 620.1463623046875 = 0.033091139048337936 + 100.0 * 6.201132774353027
Epoch 2220, val loss: 1.4823662042617798
Epoch 2230, training loss: 620.17724609375 = 0.032617948949337006 + 100.0 * 6.201446533203125
Epoch 2230, val loss: 1.4877396821975708
Epoch 2240, training loss: 620.5087890625 = 0.03215222433209419 + 100.0 * 6.204766273498535
Epoch 2240, val loss: 1.4929399490356445
Epoch 2250, training loss: 620.347412109375 = 0.031675152480602264 + 100.0 * 6.203157424926758
Epoch 2250, val loss: 1.497944951057434
Epoch 2260, training loss: 620.2265625 = 0.03122105449438095 + 100.0 * 6.201953411102295
Epoch 2260, val loss: 1.5036609172821045
Epoch 2270, training loss: 620.1463012695312 = 0.030769024044275284 + 100.0 * 6.201155662536621
Epoch 2270, val loss: 1.5085612535476685
Epoch 2280, training loss: 620.4234008789062 = 0.0303341057151556 + 100.0 * 6.203930377960205
Epoch 2280, val loss: 1.5137455463409424
Epoch 2290, training loss: 620.0089111328125 = 0.02991127036511898 + 100.0 * 6.199790000915527
Epoch 2290, val loss: 1.5197025537490845
Epoch 2300, training loss: 620.0565185546875 = 0.029496101662516594 + 100.0 * 6.200270175933838
Epoch 2300, val loss: 1.5252023935317993
Epoch 2310, training loss: 620.09130859375 = 0.029086505994200706 + 100.0 * 6.200622081756592
Epoch 2310, val loss: 1.5301538705825806
Epoch 2320, training loss: 620.5468139648438 = 0.02869318798184395 + 100.0 * 6.205181121826172
Epoch 2320, val loss: 1.5353857278823853
Epoch 2330, training loss: 620.2113647460938 = 0.028287936002016068 + 100.0 * 6.201830863952637
Epoch 2330, val loss: 1.5406824350357056
Epoch 2340, training loss: 619.9922485351562 = 0.0278785340487957 + 100.0 * 6.199643611907959
Epoch 2340, val loss: 1.5457290410995483
Epoch 2350, training loss: 619.9222412109375 = 0.027500327676534653 + 100.0 * 6.198947429656982
Epoch 2350, val loss: 1.5509799718856812
Epoch 2360, training loss: 619.9547729492188 = 0.027125723659992218 + 100.0 * 6.199276447296143
Epoch 2360, val loss: 1.5558573007583618
Epoch 2370, training loss: 620.5408325195312 = 0.026764366775751114 + 100.0 * 6.205140113830566
Epoch 2370, val loss: 1.5606187582015991
Epoch 2380, training loss: 619.9686889648438 = 0.026405420154333115 + 100.0 * 6.199422836303711
Epoch 2380, val loss: 1.5666793584823608
Epoch 2390, training loss: 619.8988647460938 = 0.026046134531497955 + 100.0 * 6.198728084564209
Epoch 2390, val loss: 1.571610450744629
Epoch 2400, training loss: 620.25 = 0.02570582740008831 + 100.0 * 6.202243328094482
Epoch 2400, val loss: 1.5767837762832642
Epoch 2410, training loss: 619.8823852539062 = 0.025355849415063858 + 100.0 * 6.198570728302002
Epoch 2410, val loss: 1.5819509029388428
Epoch 2420, training loss: 619.8453369140625 = 0.02501232922077179 + 100.0 * 6.198203086853027
Epoch 2420, val loss: 1.5867725610733032
Epoch 2430, training loss: 619.844482421875 = 0.02469046041369438 + 100.0 * 6.198197841644287
Epoch 2430, val loss: 1.5920382738113403
Epoch 2440, training loss: 620.0610961914062 = 0.024376219138503075 + 100.0 * 6.200367450714111
Epoch 2440, val loss: 1.5971829891204834
Epoch 2450, training loss: 619.8410034179688 = 0.024055154994130135 + 100.0 * 6.198169708251953
Epoch 2450, val loss: 1.6021474599838257
Epoch 2460, training loss: 620.2493896484375 = 0.023757342249155045 + 100.0 * 6.202256679534912
Epoch 2460, val loss: 1.6071566343307495
Epoch 2470, training loss: 619.891845703125 = 0.023433491587638855 + 100.0 * 6.198684215545654
Epoch 2470, val loss: 1.6117569208145142
Epoch 2480, training loss: 619.8446044921875 = 0.02312542125582695 + 100.0 * 6.198214530944824
Epoch 2480, val loss: 1.616495966911316
Epoch 2490, training loss: 619.9012451171875 = 0.022834762930870056 + 100.0 * 6.198783874511719
Epoch 2490, val loss: 1.621386170387268
Epoch 2500, training loss: 619.8998413085938 = 0.02254212461411953 + 100.0 * 6.19877290725708
Epoch 2500, val loss: 1.6263517141342163
Epoch 2510, training loss: 619.7203369140625 = 0.022258078679442406 + 100.0 * 6.1969804763793945
Epoch 2510, val loss: 1.6315677165985107
Epoch 2520, training loss: 619.8656005859375 = 0.021992133930325508 + 100.0 * 6.198436260223389
Epoch 2520, val loss: 1.6366431713104248
Epoch 2530, training loss: 620.0294799804688 = 0.02172071300446987 + 100.0 * 6.200077533721924
Epoch 2530, val loss: 1.6409832239151
Epoch 2540, training loss: 619.871826171875 = 0.021431809291243553 + 100.0 * 6.1985039710998535
Epoch 2540, val loss: 1.6450717449188232
Epoch 2550, training loss: 619.8695068359375 = 0.021174611523747444 + 100.0 * 6.198482990264893
Epoch 2550, val loss: 1.6506472826004028
Epoch 2560, training loss: 619.7225341796875 = 0.020906131714582443 + 100.0 * 6.197015762329102
Epoch 2560, val loss: 1.6547889709472656
Epoch 2570, training loss: 619.7068481445312 = 0.02065097726881504 + 100.0 * 6.19686222076416
Epoch 2570, val loss: 1.6595515012741089
Epoch 2580, training loss: 619.7216796875 = 0.020400453358888626 + 100.0 * 6.197012901306152
Epoch 2580, val loss: 1.664094090461731
Epoch 2590, training loss: 619.8815307617188 = 0.020165622234344482 + 100.0 * 6.198614120483398
Epoch 2590, val loss: 1.669152021408081
Epoch 2600, training loss: 619.721923828125 = 0.019911155104637146 + 100.0 * 6.197020053863525
Epoch 2600, val loss: 1.673125982284546
Epoch 2610, training loss: 619.6759033203125 = 0.019665081053972244 + 100.0 * 6.19656229019165
Epoch 2610, val loss: 1.6771724224090576
Epoch 2620, training loss: 619.8914794921875 = 0.01943884789943695 + 100.0 * 6.198720932006836
Epoch 2620, val loss: 1.681836724281311
Epoch 2630, training loss: 619.6026000976562 = 0.01919734664261341 + 100.0 * 6.195833683013916
Epoch 2630, val loss: 1.6861882209777832
Epoch 2640, training loss: 619.8460083007812 = 0.018979720771312714 + 100.0 * 6.198270320892334
Epoch 2640, val loss: 1.6907689571380615
Epoch 2650, training loss: 619.7344970703125 = 0.01874600723385811 + 100.0 * 6.197157859802246
Epoch 2650, val loss: 1.6947786808013916
Epoch 2660, training loss: 619.6582641601562 = 0.01852566935122013 + 100.0 * 6.196397304534912
Epoch 2660, val loss: 1.699326515197754
Epoch 2670, training loss: 619.8567504882812 = 0.01830967888236046 + 100.0 * 6.1983842849731445
Epoch 2670, val loss: 1.70317804813385
Epoch 2680, training loss: 619.4925537109375 = 0.018100017681717873 + 100.0 * 6.194744110107422
Epoch 2680, val loss: 1.7079927921295166
Epoch 2690, training loss: 619.5167846679688 = 0.01789332926273346 + 100.0 * 6.19498872756958
Epoch 2690, val loss: 1.7123968601226807
Epoch 2700, training loss: 619.5125732421875 = 0.017690373584628105 + 100.0 * 6.194948673248291
Epoch 2700, val loss: 1.7165279388427734
Epoch 2710, training loss: 619.8003540039062 = 0.017491277307271957 + 100.0 * 6.19782829284668
Epoch 2710, val loss: 1.7205146551132202
Epoch 2720, training loss: 619.7322998046875 = 0.017287636175751686 + 100.0 * 6.197150230407715
Epoch 2720, val loss: 1.7241390943527222
Epoch 2730, training loss: 619.5445556640625 = 0.017093580216169357 + 100.0 * 6.195274829864502
Epoch 2730, val loss: 1.7291240692138672
Epoch 2740, training loss: 619.4906005859375 = 0.016900019720196724 + 100.0 * 6.194736957550049
Epoch 2740, val loss: 1.7331660985946655
Epoch 2750, training loss: 619.4974975585938 = 0.016710037365555763 + 100.0 * 6.194808006286621
Epoch 2750, val loss: 1.7371853590011597
Epoch 2760, training loss: 619.6820678710938 = 0.0165266040712595 + 100.0 * 6.1966552734375
Epoch 2760, val loss: 1.740997314453125
Epoch 2770, training loss: 619.5582885742188 = 0.01634090393781662 + 100.0 * 6.1954193115234375
Epoch 2770, val loss: 1.7446457147598267
Epoch 2780, training loss: 619.438232421875 = 0.016164863482117653 + 100.0 * 6.194220542907715
Epoch 2780, val loss: 1.7490410804748535
Epoch 2790, training loss: 619.4867553710938 = 0.01598728448152542 + 100.0 * 6.194707870483398
Epoch 2790, val loss: 1.753097653388977
Epoch 2800, training loss: 619.8712768554688 = 0.01581602171063423 + 100.0 * 6.198554992675781
Epoch 2800, val loss: 1.756697416305542
Epoch 2810, training loss: 619.4134521484375 = 0.015646208077669144 + 100.0 * 6.193978309631348
Epoch 2810, val loss: 1.7610794305801392
Epoch 2820, training loss: 619.3597412109375 = 0.015476270578801632 + 100.0 * 6.193442344665527
Epoch 2820, val loss: 1.7650026082992554
Epoch 2830, training loss: 619.4257202148438 = 0.015313292853534222 + 100.0 * 6.194103717803955
Epoch 2830, val loss: 1.7690820693969727
Epoch 2840, training loss: 619.603515625 = 0.015158386901021004 + 100.0 * 6.195883750915527
Epoch 2840, val loss: 1.7731951475143433
Epoch 2850, training loss: 619.3072509765625 = 0.014990131370723248 + 100.0 * 6.192922115325928
Epoch 2850, val loss: 1.7764145135879517
Epoch 2860, training loss: 619.54443359375 = 0.0148338433355093 + 100.0 * 6.195296287536621
Epoch 2860, val loss: 1.7802667617797852
Epoch 2870, training loss: 619.4951782226562 = 0.014683665707707405 + 100.0 * 6.194804668426514
Epoch 2870, val loss: 1.7840077877044678
Epoch 2880, training loss: 619.4215087890625 = 0.014521381817758083 + 100.0 * 6.194069862365723
Epoch 2880, val loss: 1.787104845046997
Epoch 2890, training loss: 619.2852172851562 = 0.0143728107213974 + 100.0 * 6.192708492279053
Epoch 2890, val loss: 1.7912521362304688
Epoch 2900, training loss: 619.3720703125 = 0.014231131412088871 + 100.0 * 6.193578720092773
Epoch 2900, val loss: 1.7949780225753784
Epoch 2910, training loss: 619.6437377929688 = 0.014088530093431473 + 100.0 * 6.196296691894531
Epoch 2910, val loss: 1.7987987995147705
Epoch 2920, training loss: 619.4636840820312 = 0.013935641385614872 + 100.0 * 6.194497585296631
Epoch 2920, val loss: 1.8016101121902466
Epoch 2930, training loss: 619.3605346679688 = 0.013791274279356003 + 100.0 * 6.193467617034912
Epoch 2930, val loss: 1.805188536643982
Epoch 2940, training loss: 619.24267578125 = 0.01365303061902523 + 100.0 * 6.192290306091309
Epoch 2940, val loss: 1.8088080883026123
Epoch 2950, training loss: 619.2337646484375 = 0.013515823520720005 + 100.0 * 6.192202568054199
Epoch 2950, val loss: 1.8126496076583862
Epoch 2960, training loss: 619.5450439453125 = 0.013385320082306862 + 100.0 * 6.195316791534424
Epoch 2960, val loss: 1.8161146640777588
Epoch 2970, training loss: 619.1669311523438 = 0.013251747004687786 + 100.0 * 6.191536903381348
Epoch 2970, val loss: 1.8193937540054321
Epoch 2980, training loss: 619.332763671875 = 0.013121887110173702 + 100.0 * 6.1931962966918945
Epoch 2980, val loss: 1.8226443529129028
Epoch 2990, training loss: 619.4774169921875 = 0.012990795075893402 + 100.0 * 6.194643974304199
Epoch 2990, val loss: 1.825767993927002
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8429098576700054
=== training gcn model ===
Epoch 0, training loss: 861.6200561523438 = 1.936974048614502 + 100.0 * 8.596831321716309
Epoch 0, val loss: 1.9394029378890991
Epoch 10, training loss: 861.52197265625 = 1.9288909435272217 + 100.0 * 8.595931053161621
Epoch 10, val loss: 1.9306532144546509
Epoch 20, training loss: 860.8482666015625 = 1.9188480377197266 + 100.0 * 8.58929443359375
Epoch 20, val loss: 1.9197440147399902
Epoch 30, training loss: 856.0208129882812 = 1.9058585166931152 + 100.0 * 8.541149139404297
Epoch 30, val loss: 1.9056452512741089
Epoch 40, training loss: 822.940185546875 = 1.8898227214813232 + 100.0 * 8.210503578186035
Epoch 40, val loss: 1.8885599374771118
Epoch 50, training loss: 738.864990234375 = 1.871433138847351 + 100.0 * 7.369935512542725
Epoch 50, val loss: 1.869126796722412
Epoch 60, training loss: 717.8916625976562 = 1.856442928314209 + 100.0 * 7.1603522300720215
Epoch 60, val loss: 1.8544590473175049
Epoch 70, training loss: 698.4395751953125 = 1.8447308540344238 + 100.0 * 6.965948581695557
Epoch 70, val loss: 1.8427258729934692
Epoch 80, training loss: 688.601318359375 = 1.8336079120635986 + 100.0 * 6.867677211761475
Epoch 80, val loss: 1.8315255641937256
Epoch 90, training loss: 681.7969360351562 = 1.8233331441879272 + 100.0 * 6.799736022949219
Epoch 90, val loss: 1.8212553262710571
Epoch 100, training loss: 675.9959716796875 = 1.8139392137527466 + 100.0 * 6.741820335388184
Epoch 100, val loss: 1.8120112419128418
Epoch 110, training loss: 671.4944458007812 = 1.8056070804595947 + 100.0 * 6.696888446807861
Epoch 110, val loss: 1.8040127754211426
Epoch 120, training loss: 665.7879028320312 = 1.7982884645462036 + 100.0 * 6.639896392822266
Epoch 120, val loss: 1.797046184539795
Epoch 130, training loss: 660.8023681640625 = 1.7923426628112793 + 100.0 * 6.590100288391113
Epoch 130, val loss: 1.7915029525756836
Epoch 140, training loss: 657.0802612304688 = 1.7862017154693604 + 100.0 * 6.552940368652344
Epoch 140, val loss: 1.7856173515319824
Epoch 150, training loss: 654.0242309570312 = 1.7792969942092896 + 100.0 * 6.522449493408203
Epoch 150, val loss: 1.779205322265625
Epoch 160, training loss: 651.6343383789062 = 1.771934986114502 + 100.0 * 6.498624324798584
Epoch 160, val loss: 1.7724922895431519
Epoch 170, training loss: 649.34521484375 = 1.7639906406402588 + 100.0 * 6.4758124351501465
Epoch 170, val loss: 1.7652033567428589
Epoch 180, training loss: 647.54541015625 = 1.7554165124893188 + 100.0 * 6.457899570465088
Epoch 180, val loss: 1.757440209388733
Epoch 190, training loss: 645.9925537109375 = 1.7462270259857178 + 100.0 * 6.442463397979736
Epoch 190, val loss: 1.7490427494049072
Epoch 200, training loss: 644.68310546875 = 1.7361747026443481 + 100.0 * 6.429469585418701
Epoch 200, val loss: 1.7401213645935059
Epoch 210, training loss: 643.7794799804688 = 1.7253085374832153 + 100.0 * 6.420542240142822
Epoch 210, val loss: 1.730539083480835
Epoch 220, training loss: 642.377685546875 = 1.7133686542510986 + 100.0 * 6.406642913818359
Epoch 220, val loss: 1.7202638387680054
Epoch 230, training loss: 641.3805541992188 = 1.700575351715088 + 100.0 * 6.3968000411987305
Epoch 230, val loss: 1.7092881202697754
Epoch 240, training loss: 640.5989990234375 = 1.6866655349731445 + 100.0 * 6.389123439788818
Epoch 240, val loss: 1.6973899602890015
Epoch 250, training loss: 639.6384887695312 = 1.6717078685760498 + 100.0 * 6.37966775894165
Epoch 250, val loss: 1.684745192527771
Epoch 260, training loss: 639.0582275390625 = 1.6555676460266113 + 100.0 * 6.374026298522949
Epoch 260, val loss: 1.6712250709533691
Epoch 270, training loss: 638.213134765625 = 1.6383447647094727 + 100.0 * 6.365747928619385
Epoch 270, val loss: 1.6565842628479004
Epoch 280, training loss: 637.497314453125 = 1.619968056678772 + 100.0 * 6.358773231506348
Epoch 280, val loss: 1.6412341594696045
Epoch 290, training loss: 636.9275512695312 = 1.600542426109314 + 100.0 * 6.353270053863525
Epoch 290, val loss: 1.6250641345977783
Epoch 300, training loss: 636.265380859375 = 1.5800812244415283 + 100.0 * 6.346852779388428
Epoch 300, val loss: 1.6079614162445068
Epoch 310, training loss: 635.76953125 = 1.558724045753479 + 100.0 * 6.342108249664307
Epoch 310, val loss: 1.590319037437439
Epoch 320, training loss: 635.5775146484375 = 1.5366532802581787 + 100.0 * 6.3404083251953125
Epoch 320, val loss: 1.5720595121383667
Epoch 330, training loss: 634.7824096679688 = 1.513576626777649 + 100.0 * 6.332688808441162
Epoch 330, val loss: 1.5532763004302979
Epoch 340, training loss: 634.2734985351562 = 1.4901206493377686 + 100.0 * 6.327834129333496
Epoch 340, val loss: 1.534173607826233
Epoch 350, training loss: 634.07763671875 = 1.4660909175872803 + 100.0 * 6.326115608215332
Epoch 350, val loss: 1.5146985054016113
Epoch 360, training loss: 633.4705200195312 = 1.4414725303649902 + 100.0 * 6.320290565490723
Epoch 360, val loss: 1.4952112436294556
Epoch 370, training loss: 632.9971923828125 = 1.4168879985809326 + 100.0 * 6.315803050994873
Epoch 370, val loss: 1.4757000207901
Epoch 380, training loss: 632.6840209960938 = 1.3920871019363403 + 100.0 * 6.3129191398620605
Epoch 380, val loss: 1.4562863111495972
Epoch 390, training loss: 632.4000244140625 = 1.3671200275421143 + 100.0 * 6.310328960418701
Epoch 390, val loss: 1.436972975730896
Epoch 400, training loss: 632.1055908203125 = 1.3420683145523071 + 100.0 * 6.307635307312012
Epoch 400, val loss: 1.418004035949707
Epoch 410, training loss: 631.9093627929688 = 1.3171674013137817 + 100.0 * 6.305922031402588
Epoch 410, val loss: 1.3989100456237793
Epoch 420, training loss: 631.2982788085938 = 1.2921468019485474 + 100.0 * 6.300061225891113
Epoch 420, val loss: 1.3803845643997192
Epoch 430, training loss: 631.11865234375 = 1.2675076723098755 + 100.0 * 6.298511505126953
Epoch 430, val loss: 1.3621402978897095
Epoch 440, training loss: 630.8336791992188 = 1.2428697347640991 + 100.0 * 6.295908451080322
Epoch 440, val loss: 1.3441542387008667
Epoch 450, training loss: 630.3897094726562 = 1.2186394929885864 + 100.0 * 6.29171085357666
Epoch 450, val loss: 1.326672911643982
Epoch 460, training loss: 630.1903076171875 = 1.1947247982025146 + 100.0 * 6.289956092834473
Epoch 460, val loss: 1.3095554113388062
Epoch 470, training loss: 629.9494018554688 = 1.1707892417907715 + 100.0 * 6.28778600692749
Epoch 470, val loss: 1.2926526069641113
Epoch 480, training loss: 629.7276000976562 = 1.1473602056503296 + 100.0 * 6.285802841186523
Epoch 480, val loss: 1.2762655019760132
Epoch 490, training loss: 629.3919677734375 = 1.1242667436599731 + 100.0 * 6.282677173614502
Epoch 490, val loss: 1.26039719581604
Epoch 500, training loss: 629.0940551757812 = 1.1016329526901245 + 100.0 * 6.279924392700195
Epoch 500, val loss: 1.2450568675994873
Epoch 510, training loss: 629.303955078125 = 1.079372525215149 + 100.0 * 6.282246112823486
Epoch 510, val loss: 1.2301228046417236
Epoch 520, training loss: 628.9383544921875 = 1.0573338270187378 + 100.0 * 6.278810501098633
Epoch 520, val loss: 1.2154181003570557
Epoch 530, training loss: 628.7113647460938 = 1.0357204675674438 + 100.0 * 6.276756763458252
Epoch 530, val loss: 1.201292872428894
Epoch 540, training loss: 628.32568359375 = 1.0145454406738281 + 100.0 * 6.273111343383789
Epoch 540, val loss: 1.187724232673645
Epoch 550, training loss: 628.2315673828125 = 0.9938874840736389 + 100.0 * 6.272376537322998
Epoch 550, val loss: 1.1745582818984985
Epoch 560, training loss: 628.462646484375 = 0.9736276268959045 + 100.0 * 6.274889945983887
Epoch 560, val loss: 1.16170334815979
Epoch 570, training loss: 627.9199829101562 = 0.9534885287284851 + 100.0 * 6.269664764404297
Epoch 570, val loss: 1.1495798826217651
Epoch 580, training loss: 627.5938720703125 = 0.9340783357620239 + 100.0 * 6.266597747802734
Epoch 580, val loss: 1.1377184391021729
Epoch 590, training loss: 627.5834350585938 = 0.9151118993759155 + 100.0 * 6.266683101654053
Epoch 590, val loss: 1.1264373064041138
Epoch 600, training loss: 627.3989868164062 = 0.8963425755500793 + 100.0 * 6.265026092529297
Epoch 600, val loss: 1.1154142618179321
Epoch 610, training loss: 627.1415405273438 = 0.8780350089073181 + 100.0 * 6.262634754180908
Epoch 610, val loss: 1.1049507856369019
Epoch 620, training loss: 627.0381469726562 = 0.860256016254425 + 100.0 * 6.261779308319092
Epoch 620, val loss: 1.0948998928070068
Epoch 630, training loss: 627.2777709960938 = 0.8427356481552124 + 100.0 * 6.264350414276123
Epoch 630, val loss: 1.0854207277297974
Epoch 640, training loss: 626.7284545898438 = 0.8257721662521362 + 100.0 * 6.259027004241943
Epoch 640, val loss: 1.0761218070983887
Epoch 650, training loss: 626.5098876953125 = 0.8090806007385254 + 100.0 * 6.257008075714111
Epoch 650, val loss: 1.067495346069336
Epoch 660, training loss: 626.434814453125 = 0.7928563356399536 + 100.0 * 6.256419658660889
Epoch 660, val loss: 1.0593458414077759
Epoch 670, training loss: 626.5375366210938 = 0.776946485042572 + 100.0 * 6.257606029510498
Epoch 670, val loss: 1.0513458251953125
Epoch 680, training loss: 626.5021362304688 = 0.7613352537155151 + 100.0 * 6.2574076652526855
Epoch 680, val loss: 1.0437489748001099
Epoch 690, training loss: 626.174560546875 = 0.7458876967430115 + 100.0 * 6.254286289215088
Epoch 690, val loss: 1.0365560054779053
Epoch 700, training loss: 625.9185791015625 = 0.7309377789497375 + 100.0 * 6.251876354217529
Epoch 700, val loss: 1.0297536849975586
Epoch 710, training loss: 625.8035888671875 = 0.7164465188980103 + 100.0 * 6.250871658325195
Epoch 710, val loss: 1.023621678352356
Epoch 720, training loss: 625.7182006835938 = 0.7022216320037842 + 100.0 * 6.250160217285156
Epoch 720, val loss: 1.0177353620529175
Epoch 730, training loss: 625.9204711914062 = 0.6882063746452332 + 100.0 * 6.252322196960449
Epoch 730, val loss: 1.0121958255767822
Epoch 740, training loss: 625.6688232421875 = 0.6744270920753479 + 100.0 * 6.249943733215332
Epoch 740, val loss: 1.0069829225540161
Epoch 750, training loss: 625.4984130859375 = 0.6610679030418396 + 100.0 * 6.248373508453369
Epoch 750, val loss: 1.0020041465759277
Epoch 760, training loss: 625.2669677734375 = 0.6480257511138916 + 100.0 * 6.246189594268799
Epoch 760, val loss: 0.9975016117095947
Epoch 770, training loss: 625.18212890625 = 0.6353490352630615 + 100.0 * 6.245467662811279
Epoch 770, val loss: 0.9933401346206665
Epoch 780, training loss: 625.8665161132812 = 0.6229123473167419 + 100.0 * 6.25243616104126
Epoch 780, val loss: 0.9893444180488586
Epoch 790, training loss: 624.9834594726562 = 0.6104757785797119 + 100.0 * 6.243729591369629
Epoch 790, val loss: 0.9859021306037903
Epoch 800, training loss: 624.9042358398438 = 0.5984886288642883 + 100.0 * 6.2430572509765625
Epoch 800, val loss: 0.9826751947402954
Epoch 810, training loss: 624.8046875 = 0.5868978500366211 + 100.0 * 6.242177486419678
Epoch 810, val loss: 0.9797540903091431
Epoch 820, training loss: 625.6724853515625 = 0.5754082798957825 + 100.0 * 6.25097131729126
Epoch 820, val loss: 0.977030873298645
Epoch 830, training loss: 624.8125610351562 = 0.5642426013946533 + 100.0 * 6.242483139038086
Epoch 830, val loss: 0.9746661186218262
Epoch 840, training loss: 624.5315551757812 = 0.5532041788101196 + 100.0 * 6.23978328704834
Epoch 840, val loss: 0.9726260900497437
Epoch 850, training loss: 624.434814453125 = 0.542586088180542 + 100.0 * 6.238922119140625
Epoch 850, val loss: 0.9708933234214783
Epoch 860, training loss: 624.949951171875 = 0.5321519374847412 + 100.0 * 6.24417781829834
Epoch 860, val loss: 0.9696696996688843
Epoch 870, training loss: 624.5587768554688 = 0.5218871235847473 + 100.0 * 6.240368366241455
Epoch 870, val loss: 0.9679487347602844
Epoch 880, training loss: 624.20849609375 = 0.5118088722229004 + 100.0 * 6.236967086791992
Epoch 880, val loss: 0.966881513595581
Epoch 890, training loss: 624.3441772460938 = 0.5020703077316284 + 100.0 * 6.2384209632873535
Epoch 890, val loss: 0.9658775925636292
Epoch 900, training loss: 624.05517578125 = 0.49240541458129883 + 100.0 * 6.235627174377441
Epoch 900, val loss: 0.9659316539764404
Epoch 910, training loss: 624.0866088867188 = 0.48300468921661377 + 100.0 * 6.2360358238220215
Epoch 910, val loss: 0.9652643203735352
Epoch 920, training loss: 624.575439453125 = 0.47371020913124084 + 100.0 * 6.2410173416137695
Epoch 920, val loss: 0.9655905365943909
Epoch 930, training loss: 623.9437255859375 = 0.4647274315357208 + 100.0 * 6.234790325164795
Epoch 930, val loss: 0.9653972387313843
Epoch 940, training loss: 623.7664794921875 = 0.45588645339012146 + 100.0 * 6.2331061363220215
Epoch 940, val loss: 0.96568763256073
Epoch 950, training loss: 623.7392578125 = 0.44731903076171875 + 100.0 * 6.232919216156006
Epoch 950, val loss: 0.9666011333465576
Epoch 960, training loss: 624.5919189453125 = 0.43883490562438965 + 100.0 * 6.241530895233154
Epoch 960, val loss: 0.9670135378837585
Epoch 970, training loss: 623.5862426757812 = 0.4304687976837158 + 100.0 * 6.231557846069336
Epoch 970, val loss: 0.9682308435440063
Epoch 980, training loss: 623.6101684570312 = 0.4223518669605255 + 100.0 * 6.23187780380249
Epoch 980, val loss: 0.9694945216178894
Epoch 990, training loss: 623.4390258789062 = 0.4144621193408966 + 100.0 * 6.230245590209961
Epoch 990, val loss: 0.9710883498191833
Epoch 1000, training loss: 623.380615234375 = 0.4067726135253906 + 100.0 * 6.229738235473633
Epoch 1000, val loss: 0.9727550148963928
Epoch 1010, training loss: 624.0615234375 = 0.3992727994918823 + 100.0 * 6.236622333526611
Epoch 1010, val loss: 0.9746493697166443
Epoch 1020, training loss: 623.9525756835938 = 0.39155271649360657 + 100.0 * 6.235610485076904
Epoch 1020, val loss: 0.9761829376220703
Epoch 1030, training loss: 623.42236328125 = 0.3840927183628082 + 100.0 * 6.230382919311523
Epoch 1030, val loss: 0.97819983959198
Epoch 1040, training loss: 623.1795654296875 = 0.3769512176513672 + 100.0 * 6.228026390075684
Epoch 1040, val loss: 0.9804280400276184
Epoch 1050, training loss: 623.13037109375 = 0.3699757754802704 + 100.0 * 6.227603912353516
Epoch 1050, val loss: 0.9831786155700684
Epoch 1060, training loss: 623.7344970703125 = 0.3630863428115845 + 100.0 * 6.2337141036987305
Epoch 1060, val loss: 0.9859219193458557
Epoch 1070, training loss: 623.16650390625 = 0.35625985264778137 + 100.0 * 6.228102207183838
Epoch 1070, val loss: 0.9881243705749512
Epoch 1080, training loss: 622.947021484375 = 0.3495752215385437 + 100.0 * 6.2259745597839355
Epoch 1080, val loss: 0.9910030961036682
Epoch 1090, training loss: 622.9843139648438 = 0.34310758113861084 + 100.0 * 6.226412296295166
Epoch 1090, val loss: 0.9942214488983154
Epoch 1100, training loss: 623.4417114257812 = 0.33672744035720825 + 100.0 * 6.23105001449585
Epoch 1100, val loss: 0.9968441724777222
Epoch 1110, training loss: 623.2063598632812 = 0.3302713930606842 + 100.0 * 6.228760719299316
Epoch 1110, val loss: 0.9999143481254578
Epoch 1120, training loss: 622.9296264648438 = 0.3240744471549988 + 100.0 * 6.226055145263672
Epoch 1120, val loss: 1.0035302639007568
Epoch 1130, training loss: 622.7459106445312 = 0.318000853061676 + 100.0 * 6.224278926849365
Epoch 1130, val loss: 1.0067381858825684
Epoch 1140, training loss: 622.6688842773438 = 0.31209537386894226 + 100.0 * 6.223567962646484
Epoch 1140, val loss: 1.0105054378509521
Epoch 1150, training loss: 623.4549560546875 = 0.3062804639339447 + 100.0 * 6.231486797332764
Epoch 1150, val loss: 1.013789415359497
Epoch 1160, training loss: 622.9788208007812 = 0.3003573417663574 + 100.0 * 6.226784706115723
Epoch 1160, val loss: 1.018082857131958
Epoch 1170, training loss: 622.668701171875 = 0.29463982582092285 + 100.0 * 6.223741054534912
Epoch 1170, val loss: 1.0217058658599854
Epoch 1180, training loss: 622.5345458984375 = 0.28904959559440613 + 100.0 * 6.222455024719238
Epoch 1180, val loss: 1.0259413719177246
Epoch 1190, training loss: 622.5390014648438 = 0.28362321853637695 + 100.0 * 6.222553730010986
Epoch 1190, val loss: 1.030135989189148
Epoch 1200, training loss: 622.6425170898438 = 0.2782430052757263 + 100.0 * 6.223642826080322
Epoch 1200, val loss: 1.0344314575195312
Epoch 1210, training loss: 622.7825927734375 = 0.27295568585395813 + 100.0 * 6.225096225738525
Epoch 1210, val loss: 1.038558006286621
Epoch 1220, training loss: 622.6056518554688 = 0.2676672041416168 + 100.0 * 6.223380088806152
Epoch 1220, val loss: 1.0432024002075195
Epoch 1230, training loss: 622.3643188476562 = 0.2625460624694824 + 100.0 * 6.221017360687256
Epoch 1230, val loss: 1.0472136735916138
Epoch 1240, training loss: 622.2633666992188 = 0.2575194835662842 + 100.0 * 6.220058441162109
Epoch 1240, val loss: 1.0521008968353271
Epoch 1250, training loss: 622.178955078125 = 0.25262007117271423 + 100.0 * 6.219263553619385
Epoch 1250, val loss: 1.0569312572479248
Epoch 1260, training loss: 622.4682006835938 = 0.2478361576795578 + 100.0 * 6.222203254699707
Epoch 1260, val loss: 1.0617737770080566
Epoch 1270, training loss: 622.1445922851562 = 0.24292299151420593 + 100.0 * 6.2190165519714355
Epoch 1270, val loss: 1.0662235021591187
Epoch 1280, training loss: 622.2385864257812 = 0.23815667629241943 + 100.0 * 6.220004081726074
Epoch 1280, val loss: 1.07101571559906
Epoch 1290, training loss: 622.099365234375 = 0.23353081941604614 + 100.0 * 6.218658447265625
Epoch 1290, val loss: 1.076059341430664
Epoch 1300, training loss: 622.4422607421875 = 0.22903349995613098 + 100.0 * 6.222132682800293
Epoch 1300, val loss: 1.081364631652832
Epoch 1310, training loss: 622.017578125 = 0.22453707456588745 + 100.0 * 6.217930316925049
Epoch 1310, val loss: 1.0860549211502075
Epoch 1320, training loss: 621.99560546875 = 0.22015632688999176 + 100.0 * 6.217754364013672
Epoch 1320, val loss: 1.0913896560668945
Epoch 1330, training loss: 622.3364868164062 = 0.2158704251050949 + 100.0 * 6.221206188201904
Epoch 1330, val loss: 1.0963342189788818
Epoch 1340, training loss: 622.1232299804688 = 0.2115129977464676 + 100.0 * 6.219117164611816
Epoch 1340, val loss: 1.1015942096710205
Epoch 1350, training loss: 621.9405517578125 = 0.20726440846920013 + 100.0 * 6.21733283996582
Epoch 1350, val loss: 1.1063016653060913
Epoch 1360, training loss: 621.7908325195312 = 0.20317789912223816 + 100.0 * 6.215876579284668
Epoch 1360, val loss: 1.1121340990066528
Epoch 1370, training loss: 621.7467651367188 = 0.19922536611557007 + 100.0 * 6.215475559234619
Epoch 1370, val loss: 1.1178905963897705
Epoch 1380, training loss: 621.6976928710938 = 0.19533787667751312 + 100.0 * 6.215023994445801
Epoch 1380, val loss: 1.1235833168029785
Epoch 1390, training loss: 621.716552734375 = 0.19152690470218658 + 100.0 * 6.215250492095947
Epoch 1390, val loss: 1.129367709159851
Epoch 1400, training loss: 622.5006713867188 = 0.1877463459968567 + 100.0 * 6.2231292724609375
Epoch 1400, val loss: 1.1350239515304565
Epoch 1410, training loss: 622.0634155273438 = 0.1839011311531067 + 100.0 * 6.218795299530029
Epoch 1410, val loss: 1.1401418447494507
Epoch 1420, training loss: 621.6165161132812 = 0.18021126091480255 + 100.0 * 6.214363098144531
Epoch 1420, val loss: 1.1457278728485107
Epoch 1430, training loss: 621.614501953125 = 0.17664793133735657 + 100.0 * 6.214378356933594
Epoch 1430, val loss: 1.1516590118408203
Epoch 1440, training loss: 621.5704345703125 = 0.17318011820316315 + 100.0 * 6.213972568511963
Epoch 1440, val loss: 1.1574887037277222
Epoch 1450, training loss: 621.9674682617188 = 0.16979025304317474 + 100.0 * 6.2179765701293945
Epoch 1450, val loss: 1.1632615327835083
Epoch 1460, training loss: 621.5867309570312 = 0.16632971167564392 + 100.0 * 6.214203834533691
Epoch 1460, val loss: 1.1689879894256592
Epoch 1470, training loss: 621.4708862304688 = 0.16296641528606415 + 100.0 * 6.213079452514648
Epoch 1470, val loss: 1.1751381158828735
Epoch 1480, training loss: 621.4389038085938 = 0.15973377227783203 + 100.0 * 6.212791919708252
Epoch 1480, val loss: 1.1810721158981323
Epoch 1490, training loss: 621.5676879882812 = 0.15658096969127655 + 100.0 * 6.214111328125
Epoch 1490, val loss: 1.1873605251312256
Epoch 1500, training loss: 621.4564208984375 = 0.15346066653728485 + 100.0 * 6.213029861450195
Epoch 1500, val loss: 1.1932470798492432
Epoch 1510, training loss: 621.3756103515625 = 0.1504283994436264 + 100.0 * 6.212251663208008
Epoch 1510, val loss: 1.199281930923462
Epoch 1520, training loss: 621.8104248046875 = 0.1474735289812088 + 100.0 * 6.216629505157471
Epoch 1520, val loss: 1.2055860757827759
Epoch 1530, training loss: 621.5054931640625 = 0.14447776973247528 + 100.0 * 6.2136101722717285
Epoch 1530, val loss: 1.211824893951416
Epoch 1540, training loss: 621.6468505859375 = 0.14159199595451355 + 100.0 * 6.215052604675293
Epoch 1540, val loss: 1.2175660133361816
Epoch 1550, training loss: 621.253662109375 = 0.13873930275440216 + 100.0 * 6.211149215698242
Epoch 1550, val loss: 1.224172830581665
Epoch 1560, training loss: 621.23681640625 = 0.13601262867450714 + 100.0 * 6.211008071899414
Epoch 1560, val loss: 1.2302262783050537
Epoch 1570, training loss: 621.1430053710938 = 0.13336530327796936 + 100.0 * 6.21009635925293
Epoch 1570, val loss: 1.2366477251052856
Epoch 1580, training loss: 621.1942138671875 = 0.13076935708522797 + 100.0 * 6.210634231567383
Epoch 1580, val loss: 1.243067979812622
Epoch 1590, training loss: 621.9692993164062 = 0.12819133698940277 + 100.0 * 6.218410968780518
Epoch 1590, val loss: 1.2494012117385864
Epoch 1600, training loss: 621.1768798828125 = 0.12557831406593323 + 100.0 * 6.210513114929199
Epoch 1600, val loss: 1.2551710605621338
Epoch 1610, training loss: 621.24072265625 = 0.12303469330072403 + 100.0 * 6.211176872253418
Epoch 1610, val loss: 1.2618231773376465
Epoch 1620, training loss: 620.9991455078125 = 0.1206459030508995 + 100.0 * 6.208785057067871
Epoch 1620, val loss: 1.2679619789123535
Epoch 1630, training loss: 620.9969482421875 = 0.11831852793693542 + 100.0 * 6.2087860107421875
Epoch 1630, val loss: 1.2743042707443237
Epoch 1640, training loss: 621.121826171875 = 0.1160377636551857 + 100.0 * 6.210058212280273
Epoch 1640, val loss: 1.2806559801101685
Epoch 1650, training loss: 621.2379150390625 = 0.11374244838953018 + 100.0 * 6.211241722106934
Epoch 1650, val loss: 1.2871683835983276
Epoch 1660, training loss: 621.323486328125 = 0.11150559782981873 + 100.0 * 6.2121195793151855
Epoch 1660, val loss: 1.2934489250183105
Epoch 1670, training loss: 621.056396484375 = 0.10927337408065796 + 100.0 * 6.209471702575684
Epoch 1670, val loss: 1.299617052078247
Epoch 1680, training loss: 620.9447021484375 = 0.10716177523136139 + 100.0 * 6.208375453948975
Epoch 1680, val loss: 1.3056864738464355
Epoch 1690, training loss: 620.841552734375 = 0.10509469360113144 + 100.0 * 6.207364559173584
Epoch 1690, val loss: 1.3123550415039062
Epoch 1700, training loss: 620.93017578125 = 0.10309586673974991 + 100.0 * 6.208271026611328
Epoch 1700, val loss: 1.3187050819396973
Epoch 1710, training loss: 621.1243286132812 = 0.10111798346042633 + 100.0 * 6.210231781005859
Epoch 1710, val loss: 1.3249117136001587
Epoch 1720, training loss: 621.1533203125 = 0.09915558993816376 + 100.0 * 6.210541248321533
Epoch 1720, val loss: 1.3306196928024292
Epoch 1730, training loss: 621.0366821289062 = 0.09719835966825485 + 100.0 * 6.209395408630371
Epoch 1730, val loss: 1.337431788444519
Epoch 1740, training loss: 620.753173828125 = 0.09531968086957932 + 100.0 * 6.206578254699707
Epoch 1740, val loss: 1.343761682510376
Epoch 1750, training loss: 620.6990966796875 = 0.09351415187120438 + 100.0 * 6.206056118011475
Epoch 1750, val loss: 1.3502947092056274
Epoch 1760, training loss: 620.7635498046875 = 0.09176511317491531 + 100.0 * 6.2067179679870605
Epoch 1760, val loss: 1.3566312789916992
Epoch 1770, training loss: 621.3260498046875 = 0.0900229960680008 + 100.0 * 6.212360382080078
Epoch 1770, val loss: 1.3633124828338623
Epoch 1780, training loss: 620.817626953125 = 0.08829724043607712 + 100.0 * 6.2072930335998535
Epoch 1780, val loss: 1.3689093589782715
Epoch 1790, training loss: 620.6712646484375 = 0.08661814779043198 + 100.0 * 6.205846786499023
Epoch 1790, val loss: 1.375413179397583
Epoch 1800, training loss: 620.6227416992188 = 0.08500594645738602 + 100.0 * 6.205377101898193
Epoch 1800, val loss: 1.3821629285812378
Epoch 1810, training loss: 621.5281982421875 = 0.08345327526330948 + 100.0 * 6.214447498321533
Epoch 1810, val loss: 1.3883180618286133
Epoch 1820, training loss: 620.8596801757812 = 0.08183342218399048 + 100.0 * 6.207778453826904
Epoch 1820, val loss: 1.3941524028778076
Epoch 1830, training loss: 620.58447265625 = 0.08029957860708237 + 100.0 * 6.205041408538818
Epoch 1830, val loss: 1.4006340503692627
Epoch 1840, training loss: 620.5426025390625 = 0.07881788909435272 + 100.0 * 6.2046380043029785
Epoch 1840, val loss: 1.4070016145706177
Epoch 1850, training loss: 621.2076416015625 = 0.07737229019403458 + 100.0 * 6.211302280426025
Epoch 1850, val loss: 1.4134191274642944
Epoch 1860, training loss: 620.7390747070312 = 0.07593796402215958 + 100.0 * 6.206631660461426
Epoch 1860, val loss: 1.4191608428955078
Epoch 1870, training loss: 620.6995849609375 = 0.07450553774833679 + 100.0 * 6.2062506675720215
Epoch 1870, val loss: 1.4256654977798462
Epoch 1880, training loss: 620.5789794921875 = 0.07315030694007874 + 100.0 * 6.205058574676514
Epoch 1880, val loss: 1.4315108060836792
Epoch 1890, training loss: 620.53857421875 = 0.07181074470281601 + 100.0 * 6.204667091369629
Epoch 1890, val loss: 1.4375768899917603
Epoch 1900, training loss: 620.3997192382812 = 0.07052478194236755 + 100.0 * 6.203292369842529
Epoch 1900, val loss: 1.4439687728881836
Epoch 1910, training loss: 620.5222778320312 = 0.0692732185125351 + 100.0 * 6.204529762268066
Epoch 1910, val loss: 1.449953317642212
Epoch 1920, training loss: 620.6417236328125 = 0.06802113354206085 + 100.0 * 6.205737113952637
Epoch 1920, val loss: 1.4559290409088135
Epoch 1930, training loss: 620.3257446289062 = 0.06677791476249695 + 100.0 * 6.202589511871338
Epoch 1930, val loss: 1.4620434045791626
Epoch 1940, training loss: 620.6110229492188 = 0.06559206545352936 + 100.0 * 6.205454349517822
Epoch 1940, val loss: 1.4688293933868408
Epoch 1950, training loss: 620.3802490234375 = 0.0644136592745781 + 100.0 * 6.203158855438232
Epoch 1950, val loss: 1.4735788106918335
Epoch 1960, training loss: 620.2567749023438 = 0.06326700747013092 + 100.0 * 6.201934814453125
Epoch 1960, val loss: 1.479813814163208
Epoch 1970, training loss: 620.515380859375 = 0.06217212602496147 + 100.0 * 6.204532146453857
Epoch 1970, val loss: 1.4853912591934204
Epoch 1980, training loss: 620.1962890625 = 0.06105301156640053 + 100.0 * 6.201352596282959
Epoch 1980, val loss: 1.4920247793197632
Epoch 1990, training loss: 620.1477661132812 = 0.059987038373947144 + 100.0 * 6.200877666473389
Epoch 1990, val loss: 1.4973797798156738
Epoch 2000, training loss: 620.2324829101562 = 0.058961816132068634 + 100.0 * 6.201735019683838
Epoch 2000, val loss: 1.5034420490264893
Epoch 2010, training loss: 621.052734375 = 0.05792471766471863 + 100.0 * 6.2099480628967285
Epoch 2010, val loss: 1.5089585781097412
Epoch 2020, training loss: 620.244873046875 = 0.05689327046275139 + 100.0 * 6.201879978179932
Epoch 2020, val loss: 1.514889121055603
Epoch 2030, training loss: 620.088623046875 = 0.05590938776731491 + 100.0 * 6.200327396392822
Epoch 2030, val loss: 1.520390272140503
Epoch 2040, training loss: 620.066650390625 = 0.05496329441666603 + 100.0 * 6.2001166343688965
Epoch 2040, val loss: 1.5264859199523926
Epoch 2050, training loss: 620.08984375 = 0.054058946669101715 + 100.0 * 6.200357437133789
Epoch 2050, val loss: 1.5323114395141602
Epoch 2060, training loss: 621.2808837890625 = 0.05315523222088814 + 100.0 * 6.212277412414551
Epoch 2060, val loss: 1.5384516716003418
Epoch 2070, training loss: 620.3138427734375 = 0.052217982709407806 + 100.0 * 6.202615737915039
Epoch 2070, val loss: 1.5429409742355347
Epoch 2080, training loss: 620.0252685546875 = 0.0513157993555069 + 100.0 * 6.199739456176758
Epoch 2080, val loss: 1.548643708229065
Epoch 2090, training loss: 620.0648193359375 = 0.05047236382961273 + 100.0 * 6.200143337249756
Epoch 2090, val loss: 1.5547571182250977
Epoch 2100, training loss: 620.281494140625 = 0.049649156630039215 + 100.0 * 6.2023186683654785
Epoch 2100, val loss: 1.5601338148117065
Epoch 2110, training loss: 620.081298828125 = 0.04883454367518425 + 100.0 * 6.200325012207031
Epoch 2110, val loss: 1.5657795667648315
Epoch 2120, training loss: 620.0454711914062 = 0.04802596941590309 + 100.0 * 6.199974536895752
Epoch 2120, val loss: 1.571114182472229
Epoch 2130, training loss: 620.5515747070312 = 0.04724595695734024 + 100.0 * 6.205043315887451
Epoch 2130, val loss: 1.5766323804855347
Epoch 2140, training loss: 619.9898071289062 = 0.04643821343779564 + 100.0 * 6.199433326721191
Epoch 2140, val loss: 1.5821024179458618
Epoch 2150, training loss: 619.912841796875 = 0.045683134347200394 + 100.0 * 6.198671817779541
Epoch 2150, val loss: 1.5872912406921387
Epoch 2160, training loss: 619.9462890625 = 0.044963251799345016 + 100.0 * 6.1990132331848145
Epoch 2160, val loss: 1.5929811000823975
Epoch 2170, training loss: 620.3055419921875 = 0.044251084327697754 + 100.0 * 6.20261287689209
Epoch 2170, val loss: 1.59842050075531
Epoch 2180, training loss: 619.8865966796875 = 0.04353367164731026 + 100.0 * 6.198431015014648
Epoch 2180, val loss: 1.6035473346710205
Epoch 2190, training loss: 619.8751220703125 = 0.04284407198429108 + 100.0 * 6.198322772979736
Epoch 2190, val loss: 1.6088225841522217
Epoch 2200, training loss: 619.918212890625 = 0.04217362403869629 + 100.0 * 6.198760509490967
Epoch 2200, val loss: 1.6143516302108765
Epoch 2210, training loss: 620.0797729492188 = 0.04152146726846695 + 100.0 * 6.200382709503174
Epoch 2210, val loss: 1.619483470916748
Epoch 2220, training loss: 620.3724365234375 = 0.04087403789162636 + 100.0 * 6.203315258026123
Epoch 2220, val loss: 1.6246392726898193
Epoch 2230, training loss: 620.0572509765625 = 0.04020313173532486 + 100.0 * 6.200170516967773
Epoch 2230, val loss: 1.629672884941101
Epoch 2240, training loss: 619.8509521484375 = 0.039579447358846664 + 100.0 * 6.198113918304443
Epoch 2240, val loss: 1.634218692779541
Epoch 2250, training loss: 619.7540283203125 = 0.03897612541913986 + 100.0 * 6.197150230407715
Epoch 2250, val loss: 1.6399246454238892
Epoch 2260, training loss: 619.9083251953125 = 0.03840230405330658 + 100.0 * 6.198699474334717
Epoch 2260, val loss: 1.644766092300415
Epoch 2270, training loss: 620.0424194335938 = 0.03781464323401451 + 100.0 * 6.200046062469482
Epoch 2270, val loss: 1.6499625444412231
Epoch 2280, training loss: 619.9834594726562 = 0.03722762316465378 + 100.0 * 6.199462413787842
Epoch 2280, val loss: 1.6548882722854614
Epoch 2290, training loss: 619.6927490234375 = 0.03664623200893402 + 100.0 * 6.196561336517334
Epoch 2290, val loss: 1.6600340604782104
Epoch 2300, training loss: 619.61279296875 = 0.03610044717788696 + 100.0 * 6.195766925811768
Epoch 2300, val loss: 1.6649689674377441
Epoch 2310, training loss: 619.6881713867188 = 0.03557978942990303 + 100.0 * 6.196525573730469
Epoch 2310, val loss: 1.6699156761169434
Epoch 2320, training loss: 620.2778930664062 = 0.035061515867710114 + 100.0 * 6.202428340911865
Epoch 2320, val loss: 1.6746820211410522
Epoch 2330, training loss: 619.921875 = 0.03452036529779434 + 100.0 * 6.198873043060303
Epoch 2330, val loss: 1.6800013780593872
Epoch 2340, training loss: 619.772705078125 = 0.0340031273663044 + 100.0 * 6.197387218475342
Epoch 2340, val loss: 1.684276819229126
Epoch 2350, training loss: 619.7997436523438 = 0.03350525349378586 + 100.0 * 6.197662353515625
Epoch 2350, val loss: 1.6888283491134644
Epoch 2360, training loss: 619.777587890625 = 0.03302167356014252 + 100.0 * 6.197445869445801
Epoch 2360, val loss: 1.6934419870376587
Epoch 2370, training loss: 619.657958984375 = 0.0325380302965641 + 100.0 * 6.196254253387451
Epoch 2370, val loss: 1.6988710165023804
Epoch 2380, training loss: 619.5806884765625 = 0.03207464888691902 + 100.0 * 6.195485591888428
Epoch 2380, val loss: 1.7034673690795898
Epoch 2390, training loss: 619.5875244140625 = 0.03162166848778725 + 100.0 * 6.195559024810791
Epoch 2390, val loss: 1.708443522453308
Epoch 2400, training loss: 620.35595703125 = 0.031174838542938232 + 100.0 * 6.203247547149658
Epoch 2400, val loss: 1.7134697437286377
Epoch 2410, training loss: 619.7277221679688 = 0.030716676265001297 + 100.0 * 6.196969985961914
Epoch 2410, val loss: 1.716646432876587
Epoch 2420, training loss: 619.5584106445312 = 0.03026815690100193 + 100.0 * 6.195281505584717
Epoch 2420, val loss: 1.7220971584320068
Epoch 2430, training loss: 619.45458984375 = 0.029855459928512573 + 100.0 * 6.194247245788574
Epoch 2430, val loss: 1.7263373136520386
Epoch 2440, training loss: 619.6224975585938 = 0.029460670426487923 + 100.0 * 6.195930480957031
Epoch 2440, val loss: 1.731351375579834
Epoch 2450, training loss: 619.7596435546875 = 0.029044492170214653 + 100.0 * 6.197305679321289
Epoch 2450, val loss: 1.735290288925171
Epoch 2460, training loss: 619.43505859375 = 0.028613217175006866 + 100.0 * 6.194064617156982
Epoch 2460, val loss: 1.7396106719970703
Epoch 2470, training loss: 619.4200439453125 = 0.02822652831673622 + 100.0 * 6.193918228149414
Epoch 2470, val loss: 1.7440978288650513
Epoch 2480, training loss: 619.4383544921875 = 0.02785373106598854 + 100.0 * 6.19410514831543
Epoch 2480, val loss: 1.7488478422164917
Epoch 2490, training loss: 620.138671875 = 0.02750360034406185 + 100.0 * 6.201111316680908
Epoch 2490, val loss: 1.7525460720062256
Epoch 2500, training loss: 619.6083984375 = 0.027103248983621597 + 100.0 * 6.195812702178955
Epoch 2500, val loss: 1.7575961351394653
Epoch 2510, training loss: 619.4067993164062 = 0.026735838502645493 + 100.0 * 6.193800449371338
Epoch 2510, val loss: 1.7617107629776
Epoch 2520, training loss: 619.3692016601562 = 0.026386624202132225 + 100.0 * 6.1934285163879395
Epoch 2520, val loss: 1.7662931680679321
Epoch 2530, training loss: 619.81201171875 = 0.02605677954852581 + 100.0 * 6.197859764099121
Epoch 2530, val loss: 1.7705906629562378
Epoch 2540, training loss: 619.2991943359375 = 0.02569073624908924 + 100.0 * 6.192735195159912
Epoch 2540, val loss: 1.7749402523040771
Epoch 2550, training loss: 619.393798828125 = 0.0253493320196867 + 100.0 * 6.1936845779418945
Epoch 2550, val loss: 1.7789748907089233
Epoch 2560, training loss: 619.60009765625 = 0.02503178082406521 + 100.0 * 6.195750713348389
Epoch 2560, val loss: 1.7833356857299805
Epoch 2570, training loss: 619.374755859375 = 0.02469412051141262 + 100.0 * 6.193500518798828
Epoch 2570, val loss: 1.7875734567642212
Epoch 2580, training loss: 619.4659423828125 = 0.02438020333647728 + 100.0 * 6.194416046142578
Epoch 2580, val loss: 1.791829228401184
Epoch 2590, training loss: 619.2807006835938 = 0.024060891941189766 + 100.0 * 6.192566394805908
Epoch 2590, val loss: 1.7957993745803833
Epoch 2600, training loss: 619.2777099609375 = 0.02376057766377926 + 100.0 * 6.192539691925049
Epoch 2600, val loss: 1.7997685670852661
Epoch 2610, training loss: 619.3262329101562 = 0.023471521213650703 + 100.0 * 6.193027496337891
Epoch 2610, val loss: 1.803736686706543
Epoch 2620, training loss: 619.9935913085938 = 0.023186374455690384 + 100.0 * 6.199704170227051
Epoch 2620, val loss: 1.8076438903808594
Epoch 2630, training loss: 619.4759521484375 = 0.02286865934729576 + 100.0 * 6.194530487060547
Epoch 2630, val loss: 1.8120163679122925
Epoch 2640, training loss: 619.2485961914062 = 0.022582512348890305 + 100.0 * 6.192260265350342
Epoch 2640, val loss: 1.8156118392944336
Epoch 2650, training loss: 619.206298828125 = 0.0223062876611948 + 100.0 * 6.191840171813965
Epoch 2650, val loss: 1.8202067613601685
Epoch 2660, training loss: 619.6792602539062 = 0.022041818127036095 + 100.0 * 6.196572303771973
Epoch 2660, val loss: 1.8239773511886597
Epoch 2670, training loss: 619.2681884765625 = 0.02175745740532875 + 100.0 * 6.192464351654053
Epoch 2670, val loss: 1.82868230342865
Epoch 2680, training loss: 619.1580810546875 = 0.0214858241379261 + 100.0 * 6.191365718841553
Epoch 2680, val loss: 1.8320478200912476
Epoch 2690, training loss: 619.33203125 = 0.021234696730971336 + 100.0 * 6.193108081817627
Epoch 2690, val loss: 1.8357211351394653
Epoch 2700, training loss: 619.2887573242188 = 0.020972630009055138 + 100.0 * 6.1926774978637695
Epoch 2700, val loss: 1.8395881652832031
Epoch 2710, training loss: 619.23486328125 = 0.020721033215522766 + 100.0 * 6.192142009735107
Epoch 2710, val loss: 1.8433949947357178
Epoch 2720, training loss: 619.2100830078125 = 0.02047356590628624 + 100.0 * 6.191896438598633
Epoch 2720, val loss: 1.8472955226898193
Epoch 2730, training loss: 619.3700561523438 = 0.02023281902074814 + 100.0 * 6.193498611450195
Epoch 2730, val loss: 1.8513083457946777
Epoch 2740, training loss: 619.3480834960938 = 0.01999215967953205 + 100.0 * 6.1932806968688965
Epoch 2740, val loss: 1.8554733991622925
Epoch 2750, training loss: 619.1854858398438 = 0.019755888730287552 + 100.0 * 6.191657066345215
Epoch 2750, val loss: 1.858399748802185
Epoch 2760, training loss: 619.3170776367188 = 0.019528329372406006 + 100.0 * 6.1929755210876465
Epoch 2760, val loss: 1.8624628782272339
Epoch 2770, training loss: 619.1458740234375 = 0.019286178052425385 + 100.0 * 6.191266059875488
Epoch 2770, val loss: 1.8664703369140625
Epoch 2780, training loss: 619.1077270507812 = 0.019070103764533997 + 100.0 * 6.190886974334717
Epoch 2780, val loss: 1.8693615198135376
Epoch 2790, training loss: 618.983154296875 = 0.01885373890399933 + 100.0 * 6.189642906188965
Epoch 2790, val loss: 1.8737469911575317
Epoch 2800, training loss: 619.2367553710938 = 0.01865171082317829 + 100.0 * 6.192180633544922
Epoch 2800, val loss: 1.877204179763794
Epoch 2810, training loss: 619.2757568359375 = 0.018433455377817154 + 100.0 * 6.192573070526123
Epoch 2810, val loss: 1.8805445432662964
Epoch 2820, training loss: 619.2418212890625 = 0.018213991075754166 + 100.0 * 6.192236423492432
Epoch 2820, val loss: 1.8840997219085693
Epoch 2830, training loss: 618.974853515625 = 0.01800226978957653 + 100.0 * 6.189568519592285
Epoch 2830, val loss: 1.8874276876449585
Epoch 2840, training loss: 618.9971923828125 = 0.01780741475522518 + 100.0 * 6.189793586730957
Epoch 2840, val loss: 1.8909517526626587
Epoch 2850, training loss: 619.0238647460938 = 0.017616191878914833 + 100.0 * 6.190062522888184
Epoch 2850, val loss: 1.8946715593338013
Epoch 2860, training loss: 619.5714111328125 = 0.017429882660508156 + 100.0 * 6.195539474487305
Epoch 2860, val loss: 1.8974989652633667
Epoch 2870, training loss: 619.2335815429688 = 0.017222128808498383 + 100.0 * 6.192163944244385
Epoch 2870, val loss: 1.9023609161376953
Epoch 2880, training loss: 619.0076293945312 = 0.01703389547765255 + 100.0 * 6.189906120300293
Epoch 2880, val loss: 1.9047937393188477
Epoch 2890, training loss: 619.1087646484375 = 0.016849759966135025 + 100.0 * 6.190919399261475
Epoch 2890, val loss: 1.9090185165405273
Epoch 2900, training loss: 619.248046875 = 0.016665590927004814 + 100.0 * 6.1923136711120605
Epoch 2900, val loss: 1.9120571613311768
Epoch 2910, training loss: 619.0883178710938 = 0.016485203057527542 + 100.0 * 6.190718650817871
Epoch 2910, val loss: 1.914442777633667
Epoch 2920, training loss: 618.9249267578125 = 0.016306426376104355 + 100.0 * 6.189086437225342
Epoch 2920, val loss: 1.9184536933898926
Epoch 2930, training loss: 618.8541870117188 = 0.016134629026055336 + 100.0 * 6.188380718231201
Epoch 2930, val loss: 1.9218465089797974
Epoch 2940, training loss: 619.1375122070312 = 0.015974612906575203 + 100.0 * 6.1912150382995605
Epoch 2940, val loss: 1.9248899221420288
Epoch 2950, training loss: 618.9174194335938 = 0.01580096408724785 + 100.0 * 6.189015865325928
Epoch 2950, val loss: 1.9280191659927368
Epoch 2960, training loss: 618.923828125 = 0.015633665025234222 + 100.0 * 6.189082145690918
Epoch 2960, val loss: 1.931738257408142
Epoch 2970, training loss: 619.4032592773438 = 0.015477330423891544 + 100.0 * 6.193877696990967
Epoch 2970, val loss: 1.9349806308746338
Epoch 2980, training loss: 619.0136108398438 = 0.015306048095226288 + 100.0 * 6.189982891082764
Epoch 2980, val loss: 1.937635898590088
Epoch 2990, training loss: 618.9530639648438 = 0.015139903873205185 + 100.0 * 6.1893792152404785
Epoch 2990, val loss: 1.9409348964691162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.838165524512388
The final CL Acc:0.71235, 0.02310, The final GNN Acc:0.84027, 0.00197
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9436])
updated graph: torch.Size([2, 10474])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6409301757812 = 1.9529895782470703 + 100.0 * 8.596879005432129
Epoch 0, val loss: 1.953766942024231
Epoch 10, training loss: 861.581298828125 = 1.9442436695098877 + 100.0 * 8.596370697021484
Epoch 10, val loss: 1.9455808401107788
Epoch 20, training loss: 861.2173461914062 = 1.9335525035858154 + 100.0 * 8.592838287353516
Epoch 20, val loss: 1.9353395700454712
Epoch 30, training loss: 858.7201538085938 = 1.919864535331726 + 100.0 * 8.568002700805664
Epoch 30, val loss: 1.9220285415649414
Epoch 40, training loss: 845.2474975585938 = 1.9018354415893555 + 100.0 * 8.433456420898438
Epoch 40, val loss: 1.9046906232833862
Epoch 50, training loss: 798.8780517578125 = 1.8818222284317017 + 100.0 * 7.969962120056152
Epoch 50, val loss: 1.8857877254486084
Epoch 60, training loss: 756.9393310546875 = 1.8654261827468872 + 100.0 * 7.550739288330078
Epoch 60, val loss: 1.8707754611968994
Epoch 70, training loss: 723.375732421875 = 1.854164719581604 + 100.0 * 7.215215682983398
Epoch 70, val loss: 1.8597536087036133
Epoch 80, training loss: 702.3251953125 = 1.8427884578704834 + 100.0 * 7.004824161529541
Epoch 80, val loss: 1.8482146263122559
Epoch 90, training loss: 687.797119140625 = 1.8322492837905884 + 100.0 * 6.85964822769165
Epoch 90, val loss: 1.8378955125808716
Epoch 100, training loss: 677.552001953125 = 1.8212391138076782 + 100.0 * 6.757307529449463
Epoch 100, val loss: 1.8274376392364502
Epoch 110, training loss: 671.157958984375 = 1.8100217580795288 + 100.0 * 6.693479537963867
Epoch 110, val loss: 1.8170621395111084
Epoch 120, training loss: 666.2828369140625 = 1.7988793849945068 + 100.0 * 6.644839763641357
Epoch 120, val loss: 1.8068082332611084
Epoch 130, training loss: 663.2485961914062 = 1.7877312898635864 + 100.0 * 6.6146087646484375
Epoch 130, val loss: 1.7965987920761108
Epoch 140, training loss: 660.7041015625 = 1.7762483358383179 + 100.0 * 6.589278697967529
Epoch 140, val loss: 1.7860651016235352
Epoch 150, training loss: 658.8854370117188 = 1.7639843225479126 + 100.0 * 6.57121467590332
Epoch 150, val loss: 1.775039553642273
Epoch 160, training loss: 657.2788696289062 = 1.750876545906067 + 100.0 * 6.5552802085876465
Epoch 160, val loss: 1.763453722000122
Epoch 170, training loss: 655.7571411132812 = 1.736754298210144 + 100.0 * 6.540204048156738
Epoch 170, val loss: 1.7512168884277344
Epoch 180, training loss: 654.5211181640625 = 1.7214477062225342 + 100.0 * 6.527997016906738
Epoch 180, val loss: 1.7381612062454224
Epoch 190, training loss: 653.1857299804688 = 1.70492422580719 + 100.0 * 6.514808654785156
Epoch 190, val loss: 1.7240936756134033
Epoch 200, training loss: 652.03076171875 = 1.6868677139282227 + 100.0 * 6.503438949584961
Epoch 200, val loss: 1.7089672088623047
Epoch 210, training loss: 651.439697265625 = 1.667346715927124 + 100.0 * 6.497723579406738
Epoch 210, val loss: 1.6926263570785522
Epoch 220, training loss: 650.0828247070312 = 1.646296739578247 + 100.0 * 6.484365463256836
Epoch 220, val loss: 1.6751258373260498
Epoch 230, training loss: 649.0420532226562 = 1.6239739656448364 + 100.0 * 6.474180698394775
Epoch 230, val loss: 1.6566152572631836
Epoch 240, training loss: 648.2003784179688 = 1.6004838943481445 + 100.0 * 6.465999126434326
Epoch 240, val loss: 1.6372796297073364
Epoch 250, training loss: 647.5203857421875 = 1.575906753540039 + 100.0 * 6.459444522857666
Epoch 250, val loss: 1.617142677307129
Epoch 260, training loss: 646.4790649414062 = 1.5504395961761475 + 100.0 * 6.449286460876465
Epoch 260, val loss: 1.5965218544006348
Epoch 270, training loss: 645.519287109375 = 1.5245503187179565 + 100.0 * 6.439947605133057
Epoch 270, val loss: 1.5755575895309448
Epoch 280, training loss: 645.0006103515625 = 1.4983787536621094 + 100.0 * 6.435022354125977
Epoch 280, val loss: 1.5546834468841553
Epoch 290, training loss: 643.7655029296875 = 1.4719927310943604 + 100.0 * 6.4229350090026855
Epoch 290, val loss: 1.533888339996338
Epoch 300, training loss: 642.8399658203125 = 1.4458039999008179 + 100.0 * 6.413941383361816
Epoch 300, val loss: 1.513519287109375
Epoch 310, training loss: 642.2666625976562 = 1.4198695421218872 + 100.0 * 6.408467769622803
Epoch 310, val loss: 1.493661880493164
Epoch 320, training loss: 641.3375244140625 = 1.3939871788024902 + 100.0 * 6.399435520172119
Epoch 320, val loss: 1.473870038986206
Epoch 330, training loss: 640.5333251953125 = 1.3685017824172974 + 100.0 * 6.391647815704346
Epoch 330, val loss: 1.454661250114441
Epoch 340, training loss: 640.3499145507812 = 1.3432948589324951 + 100.0 * 6.390065670013428
Epoch 340, val loss: 1.4359186887741089
Epoch 350, training loss: 639.4429931640625 = 1.3183910846710205 + 100.0 * 6.3812456130981445
Epoch 350, val loss: 1.4173471927642822
Epoch 360, training loss: 638.7837524414062 = 1.2937225103378296 + 100.0 * 6.3749003410339355
Epoch 360, val loss: 1.3991912603378296
Epoch 370, training loss: 638.2229614257812 = 1.2693259716033936 + 100.0 * 6.36953592300415
Epoch 370, val loss: 1.3813936710357666
Epoch 380, training loss: 637.7567749023438 = 1.2451668977737427 + 100.0 * 6.365116119384766
Epoch 380, val loss: 1.3637982606887817
Epoch 390, training loss: 637.845947265625 = 1.2211198806762695 + 100.0 * 6.36624813079834
Epoch 390, val loss: 1.3462899923324585
Epoch 400, training loss: 637.1905517578125 = 1.1971698999404907 + 100.0 * 6.359934329986572
Epoch 400, val loss: 1.3291049003601074
Epoch 410, training loss: 636.5504150390625 = 1.1732269525527954 + 100.0 * 6.353771686553955
Epoch 410, val loss: 1.3117772340774536
Epoch 420, training loss: 636.2224731445312 = 1.149462342262268 + 100.0 * 6.350729942321777
Epoch 420, val loss: 1.2945775985717773
Epoch 430, training loss: 635.8200073242188 = 1.1257351636886597 + 100.0 * 6.346942901611328
Epoch 430, val loss: 1.277575969696045
Epoch 440, training loss: 635.4911499023438 = 1.1020574569702148 + 100.0 * 6.343891143798828
Epoch 440, val loss: 1.2605246305465698
Epoch 450, training loss: 635.6380004882812 = 1.0783500671386719 + 100.0 * 6.3455963134765625
Epoch 450, val loss: 1.243485927581787
Epoch 460, training loss: 634.890625 = 1.0547001361846924 + 100.0 * 6.338359355926514
Epoch 460, val loss: 1.2263820171356201
Epoch 470, training loss: 634.6151733398438 = 1.0311098098754883 + 100.0 * 6.335840702056885
Epoch 470, val loss: 1.2095162868499756
Epoch 480, training loss: 634.426513671875 = 1.0076199769973755 + 100.0 * 6.334188938140869
Epoch 480, val loss: 1.192667841911316
Epoch 490, training loss: 634.5906372070312 = 0.984353244304657 + 100.0 * 6.336062908172607
Epoch 490, val loss: 1.1760114431381226
Epoch 500, training loss: 633.803466796875 = 0.9611453413963318 + 100.0 * 6.328423500061035
Epoch 500, val loss: 1.1594969034194946
Epoch 510, training loss: 633.5306396484375 = 0.9383127689361572 + 100.0 * 6.325923442840576
Epoch 510, val loss: 1.1434576511383057
Epoch 520, training loss: 633.2930908203125 = 0.9158015251159668 + 100.0 * 6.32377290725708
Epoch 520, val loss: 1.1277427673339844
Epoch 530, training loss: 633.8909301757812 = 0.8936647176742554 + 100.0 * 6.329972743988037
Epoch 530, val loss: 1.1123509407043457
Epoch 540, training loss: 633.0081176757812 = 0.8716903924942017 + 100.0 * 6.321363925933838
Epoch 540, val loss: 1.0976791381835938
Epoch 550, training loss: 632.6260986328125 = 0.8503244519233704 + 100.0 * 6.317757606506348
Epoch 550, val loss: 1.0833404064178467
Epoch 560, training loss: 632.4467163085938 = 0.8294929265975952 + 100.0 * 6.3161725997924805
Epoch 560, val loss: 1.0696468353271484
Epoch 570, training loss: 632.5292358398438 = 0.8091605305671692 + 100.0 * 6.317201137542725
Epoch 570, val loss: 1.056606411933899
Epoch 580, training loss: 631.9957885742188 = 0.7893861532211304 + 100.0 * 6.312064170837402
Epoch 580, val loss: 1.0441999435424805
Epoch 590, training loss: 631.77294921875 = 0.7701377868652344 + 100.0 * 6.310028076171875
Epoch 590, val loss: 1.0324345827102661
Epoch 600, training loss: 632.1549682617188 = 0.7513964772224426 + 100.0 * 6.314035892486572
Epoch 600, val loss: 1.0214506387710571
Epoch 610, training loss: 631.5390014648438 = 0.7331976294517517 + 100.0 * 6.30805778503418
Epoch 610, val loss: 1.0107048749923706
Epoch 620, training loss: 631.1483154296875 = 0.7155060172080994 + 100.0 * 6.304327964782715
Epoch 620, val loss: 1.0009254217147827
Epoch 630, training loss: 630.994873046875 = 0.6983984112739563 + 100.0 * 6.302964210510254
Epoch 630, val loss: 0.9917757511138916
Epoch 640, training loss: 631.9414672851562 = 0.6818230748176575 + 100.0 * 6.312596321105957
Epoch 640, val loss: 0.9830060005187988
Epoch 650, training loss: 630.7191162109375 = 0.6654598712921143 + 100.0 * 6.300536632537842
Epoch 650, val loss: 0.9749086499214172
Epoch 660, training loss: 630.5606689453125 = 0.6497100591659546 + 100.0 * 6.29910945892334
Epoch 660, val loss: 0.9672999382019043
Epoch 670, training loss: 630.4447021484375 = 0.6344351768493652 + 100.0 * 6.298102378845215
Epoch 670, val loss: 0.9602720141410828
Epoch 680, training loss: 630.0847778320312 = 0.6195774078369141 + 100.0 * 6.294651985168457
Epoch 680, val loss: 0.9536449313163757
Epoch 690, training loss: 630.2997436523438 = 0.605058491230011 + 100.0 * 6.296947002410889
Epoch 690, val loss: 0.9475467205047607
Epoch 700, training loss: 629.9512939453125 = 0.5907860994338989 + 100.0 * 6.293605327606201
Epoch 700, val loss: 0.9416618943214417
Epoch 710, training loss: 629.7012329101562 = 0.5769153237342834 + 100.0 * 6.291243553161621
Epoch 710, val loss: 0.9362199306488037
Epoch 720, training loss: 629.5291748046875 = 0.5633466839790344 + 100.0 * 6.289658069610596
Epoch 720, val loss: 0.9312298893928528
Epoch 730, training loss: 629.38037109375 = 0.5499920845031738 + 100.0 * 6.288303852081299
Epoch 730, val loss: 0.9266334772109985
Epoch 740, training loss: 629.2130126953125 = 0.5369668006896973 + 100.0 * 6.286760330200195
Epoch 740, val loss: 0.9220694899559021
Epoch 750, training loss: 629.047607421875 = 0.5242286324501038 + 100.0 * 6.285233974456787
Epoch 750, val loss: 0.9179778099060059
Epoch 760, training loss: 628.8851928710938 = 0.5117571949958801 + 100.0 * 6.28373384475708
Epoch 760, val loss: 0.9141020774841309
Epoch 770, training loss: 628.9171752929688 = 0.4995175302028656 + 100.0 * 6.284176826477051
Epoch 770, val loss: 0.9104746580123901
Epoch 780, training loss: 628.9691162109375 = 0.48724308609962463 + 100.0 * 6.284818649291992
Epoch 780, val loss: 0.9070970416069031
Epoch 790, training loss: 628.5320434570312 = 0.4752970039844513 + 100.0 * 6.280567646026611
Epoch 790, val loss: 0.9035772085189819
Epoch 800, training loss: 628.3775024414062 = 0.4635317027568817 + 100.0 * 6.279139518737793
Epoch 800, val loss: 0.9003353714942932
Epoch 810, training loss: 628.2302856445312 = 0.45194458961486816 + 100.0 * 6.277782917022705
Epoch 810, val loss: 0.8973348140716553
Epoch 820, training loss: 628.1238403320312 = 0.4405115246772766 + 100.0 * 6.276833534240723
Epoch 820, val loss: 0.8945320248603821
Epoch 830, training loss: 628.4658203125 = 0.42914873361587524 + 100.0 * 6.280366897583008
Epoch 830, val loss: 0.8918188214302063
Epoch 840, training loss: 628.1265258789062 = 0.41785845160484314 + 100.0 * 6.2770867347717285
Epoch 840, val loss: 0.8889778256416321
Epoch 850, training loss: 627.798095703125 = 0.40672922134399414 + 100.0 * 6.273913383483887
Epoch 850, val loss: 0.8865898251533508
Epoch 860, training loss: 627.6459350585938 = 0.3957611620426178 + 100.0 * 6.2725019454956055
Epoch 860, val loss: 0.884345531463623
Epoch 870, training loss: 627.6653442382812 = 0.3849460184574127 + 100.0 * 6.272804260253906
Epoch 870, val loss: 0.8821631669998169
Epoch 880, training loss: 627.6436767578125 = 0.37420669198036194 + 100.0 * 6.2726945877075195
Epoch 880, val loss: 0.8800267577171326
Epoch 890, training loss: 627.6339721679688 = 0.3635929822921753 + 100.0 * 6.272703647613525
Epoch 890, val loss: 0.8781276941299438
Epoch 900, training loss: 627.5215454101562 = 0.3531145751476288 + 100.0 * 6.271684646606445
Epoch 900, val loss: 0.8762291073799133
Epoch 910, training loss: 627.2528686523438 = 0.3427809476852417 + 100.0 * 6.269101142883301
Epoch 910, val loss: 0.8749175667762756
Epoch 920, training loss: 627.1016235351562 = 0.3327159285545349 + 100.0 * 6.267689228057861
Epoch 920, val loss: 0.8734204173088074
Epoch 930, training loss: 627.42578125 = 0.32279449701309204 + 100.0 * 6.271029949188232
Epoch 930, val loss: 0.8725679516792297
Epoch 940, training loss: 626.8927612304688 = 0.3130587041378021 + 100.0 * 6.265797138214111
Epoch 940, val loss: 0.8712189197540283
Epoch 950, training loss: 626.7875366210938 = 0.3035467863082886 + 100.0 * 6.264840126037598
Epoch 950, val loss: 0.8706364631652832
Epoch 960, training loss: 626.6688232421875 = 0.29430893063545227 + 100.0 * 6.263744831085205
Epoch 960, val loss: 0.8702220320701599
Epoch 970, training loss: 626.8189697265625 = 0.28531327843666077 + 100.0 * 6.265336513519287
Epoch 970, val loss: 0.8701180815696716
Epoch 980, training loss: 626.525634765625 = 0.27649229764938354 + 100.0 * 6.262491226196289
Epoch 980, val loss: 0.8698509335517883
Epoch 990, training loss: 626.587890625 = 0.2679455280303955 + 100.0 * 6.263199329376221
Epoch 990, val loss: 0.8701965808868408
Epoch 1000, training loss: 626.3260498046875 = 0.2596903145313263 + 100.0 * 6.2606635093688965
Epoch 1000, val loss: 0.8707760572433472
Epoch 1010, training loss: 626.6656494140625 = 0.25170159339904785 + 100.0 * 6.264139175415039
Epoch 1010, val loss: 0.8720117211341858
Epoch 1020, training loss: 626.6153564453125 = 0.2438993901014328 + 100.0 * 6.263714790344238
Epoch 1020, val loss: 0.8721998333930969
Epoch 1030, training loss: 626.1704711914062 = 0.23634929955005646 + 100.0 * 6.259341239929199
Epoch 1030, val loss: 0.873798668384552
Epoch 1040, training loss: 626.0944213867188 = 0.22910214960575104 + 100.0 * 6.258653163909912
Epoch 1040, val loss: 0.875462532043457
Epoch 1050, training loss: 626.4672241210938 = 0.22209985554218292 + 100.0 * 6.262451171875
Epoch 1050, val loss: 0.8774658441543579
Epoch 1060, training loss: 625.9041137695312 = 0.21531151235103607 + 100.0 * 6.256887912750244
Epoch 1060, val loss: 0.8791208267211914
Epoch 1070, training loss: 625.7713012695312 = 0.20876048505306244 + 100.0 * 6.2556257247924805
Epoch 1070, val loss: 0.8816088438034058
Epoch 1080, training loss: 625.7178344726562 = 0.20246011018753052 + 100.0 * 6.255153656005859
Epoch 1080, val loss: 0.884227454662323
Epoch 1090, training loss: 626.2261962890625 = 0.19639179110527039 + 100.0 * 6.260297775268555
Epoch 1090, val loss: 0.8869792222976685
Epoch 1100, training loss: 625.8458862304688 = 0.19047269225120544 + 100.0 * 6.256553649902344
Epoch 1100, val loss: 0.8896948099136353
Epoch 1110, training loss: 625.6202392578125 = 0.18477313220500946 + 100.0 * 6.254354953765869
Epoch 1110, val loss: 0.8930982947349548
Epoch 1120, training loss: 625.53662109375 = 0.17929553985595703 + 100.0 * 6.253572940826416
Epoch 1120, val loss: 0.8963625431060791
Epoch 1130, training loss: 625.8834228515625 = 0.17398881912231445 + 100.0 * 6.257093906402588
Epoch 1130, val loss: 0.9001498818397522
Epoch 1140, training loss: 625.7030029296875 = 0.1688525527715683 + 100.0 * 6.255341529846191
Epoch 1140, val loss: 0.9034173488616943
Epoch 1150, training loss: 625.662841796875 = 0.16389408707618713 + 100.0 * 6.2549896240234375
Epoch 1150, val loss: 0.9071562886238098
Epoch 1160, training loss: 625.2235107421875 = 0.15909458696842194 + 100.0 * 6.250643730163574
Epoch 1160, val loss: 0.9112052917480469
Epoch 1170, training loss: 625.1779174804688 = 0.15447834134101868 + 100.0 * 6.250234127044678
Epoch 1170, val loss: 0.9154123663902283
Epoch 1180, training loss: 625.1234130859375 = 0.15002202987670898 + 100.0 * 6.249733924865723
Epoch 1180, val loss: 0.9198343753814697
Epoch 1190, training loss: 625.80712890625 = 0.14570112526416779 + 100.0 * 6.2566142082214355
Epoch 1190, val loss: 0.9243737459182739
Epoch 1200, training loss: 625.0315551757812 = 0.14147698879241943 + 100.0 * 6.248900890350342
Epoch 1200, val loss: 0.9287003874778748
Epoch 1210, training loss: 624.9595947265625 = 0.13744798302650452 + 100.0 * 6.248221397399902
Epoch 1210, val loss: 0.9332093596458435
Epoch 1220, training loss: 624.850830078125 = 0.1335584968328476 + 100.0 * 6.247172832489014
Epoch 1220, val loss: 0.9379782676696777
Epoch 1230, training loss: 624.8544311523438 = 0.1298016756772995 + 100.0 * 6.247246265411377
Epoch 1230, val loss: 0.9428934454917908
Epoch 1240, training loss: 625.3870849609375 = 0.12616883218288422 + 100.0 * 6.2526092529296875
Epoch 1240, val loss: 0.9476928114891052
Epoch 1250, training loss: 625.2543334960938 = 0.12260879576206207 + 100.0 * 6.251317024230957
Epoch 1250, val loss: 0.9526282548904419
Epoch 1260, training loss: 624.9318237304688 = 0.11915424466133118 + 100.0 * 6.248126983642578
Epoch 1260, val loss: 0.957985520362854
Epoch 1270, training loss: 624.6030883789062 = 0.11585621535778046 + 100.0 * 6.244872570037842
Epoch 1270, val loss: 0.962712287902832
Epoch 1280, training loss: 624.5296630859375 = 0.11268628388643265 + 100.0 * 6.244170188903809
Epoch 1280, val loss: 0.967898428440094
Epoch 1290, training loss: 624.6140747070312 = 0.1096290722489357 + 100.0 * 6.245044708251953
Epoch 1290, val loss: 0.9730799794197083
Epoch 1300, training loss: 624.630859375 = 0.10664698481559753 + 100.0 * 6.245242595672607
Epoch 1300, val loss: 0.9783607721328735
Epoch 1310, training loss: 624.5827026367188 = 0.10374999791383743 + 100.0 * 6.2447896003723145
Epoch 1310, val loss: 0.9838665127754211
Epoch 1320, training loss: 624.71044921875 = 0.10097625106573105 + 100.0 * 6.246095180511475
Epoch 1320, val loss: 0.9889960289001465
Epoch 1330, training loss: 624.253173828125 = 0.09823758900165558 + 100.0 * 6.241549015045166
Epoch 1330, val loss: 0.9947754144668579
Epoch 1340, training loss: 624.2550659179688 = 0.09563085436820984 + 100.0 * 6.241594314575195
Epoch 1340, val loss: 1.0001896619796753
Epoch 1350, training loss: 624.7274780273438 = 0.09313184022903442 + 100.0 * 6.24634313583374
Epoch 1350, val loss: 1.0052682161331177
Epoch 1360, training loss: 624.56396484375 = 0.09065208584070206 + 100.0 * 6.2447333335876465
Epoch 1360, val loss: 1.0109736919403076
Epoch 1370, training loss: 624.1527709960938 = 0.08825826644897461 + 100.0 * 6.240645408630371
Epoch 1370, val loss: 1.0164090394973755
Epoch 1380, training loss: 624.071533203125 = 0.08595885336399078 + 100.0 * 6.239856243133545
Epoch 1380, val loss: 1.022264003753662
Epoch 1390, training loss: 624.018310546875 = 0.08375456184148788 + 100.0 * 6.239345550537109
Epoch 1390, val loss: 1.0277950763702393
Epoch 1400, training loss: 624.5428466796875 = 0.08161909878253937 + 100.0 * 6.244612693786621
Epoch 1400, val loss: 1.033353328704834
Epoch 1410, training loss: 624.037841796875 = 0.07953520119190216 + 100.0 * 6.2395830154418945
Epoch 1410, val loss: 1.0384581089019775
Epoch 1420, training loss: 624.2598876953125 = 0.07752981036901474 + 100.0 * 6.241823673248291
Epoch 1420, val loss: 1.0438047647476196
Epoch 1430, training loss: 623.91552734375 = 0.07553883641958237 + 100.0 * 6.238399505615234
Epoch 1430, val loss: 1.0498948097229004
Epoch 1440, training loss: 623.7615356445312 = 0.073659747838974 + 100.0 * 6.236878871917725
Epoch 1440, val loss: 1.0552716255187988
Epoch 1450, training loss: 623.7461547851562 = 0.07184886187314987 + 100.0 * 6.236743450164795
Epoch 1450, val loss: 1.060908317565918
Epoch 1460, training loss: 623.9550170898438 = 0.07009156793355942 + 100.0 * 6.238849639892578
Epoch 1460, val loss: 1.066563606262207
Epoch 1470, training loss: 623.9820556640625 = 0.06837066262960434 + 100.0 * 6.239137172698975
Epoch 1470, val loss: 1.0718873739242554
Epoch 1480, training loss: 623.7970581054688 = 0.06669624149799347 + 100.0 * 6.237303256988525
Epoch 1480, val loss: 1.0774565935134888
Epoch 1490, training loss: 623.7547607421875 = 0.06507304310798645 + 100.0 * 6.236896991729736
Epoch 1490, val loss: 1.082899570465088
Epoch 1500, training loss: 623.8204956054688 = 0.06351306289434433 + 100.0 * 6.237569808959961
Epoch 1500, val loss: 1.0886938571929932
Epoch 1510, training loss: 623.6039428710938 = 0.061992399394512177 + 100.0 * 6.235419273376465
Epoch 1510, val loss: 1.0941563844680786
Epoch 1520, training loss: 623.6809692382812 = 0.060519225895404816 + 100.0 * 6.236204147338867
Epoch 1520, val loss: 1.0998177528381348
Epoch 1530, training loss: 623.5938720703125 = 0.059104740619659424 + 100.0 * 6.235347747802734
Epoch 1530, val loss: 1.1052420139312744
Epoch 1540, training loss: 623.442626953125 = 0.05773511901497841 + 100.0 * 6.233849048614502
Epoch 1540, val loss: 1.1104743480682373
Epoch 1550, training loss: 623.709228515625 = 0.0564117506146431 + 100.0 * 6.236528396606445
Epoch 1550, val loss: 1.1159249544143677
Epoch 1560, training loss: 623.5236206054688 = 0.055126309394836426 + 100.0 * 6.234684944152832
Epoch 1560, val loss: 1.120739221572876
Epoch 1570, training loss: 623.4088134765625 = 0.05386221408843994 + 100.0 * 6.23354959487915
Epoch 1570, val loss: 1.1261894702911377
Epoch 1580, training loss: 623.231689453125 = 0.05263715982437134 + 100.0 * 6.231790542602539
Epoch 1580, val loss: 1.1317484378814697
Epoch 1590, training loss: 623.2105102539062 = 0.05146628990769386 + 100.0 * 6.231590747833252
Epoch 1590, val loss: 1.1371307373046875
Epoch 1600, training loss: 623.8627319335938 = 0.05032937228679657 + 100.0 * 6.238123893737793
Epoch 1600, val loss: 1.142735242843628
Epoch 1610, training loss: 623.1846313476562 = 0.04920945689082146 + 100.0 * 6.231354236602783
Epoch 1610, val loss: 1.147187352180481
Epoch 1620, training loss: 623.1970825195312 = 0.048119571059942245 + 100.0 * 6.231490135192871
Epoch 1620, val loss: 1.1527419090270996
Epoch 1630, training loss: 623.2276000976562 = 0.0470748208463192 + 100.0 * 6.231805324554443
Epoch 1630, val loss: 1.157668113708496
Epoch 1640, training loss: 623.0154418945312 = 0.046064913272857666 + 100.0 * 6.22969388961792
Epoch 1640, val loss: 1.1626845598220825
Epoch 1650, training loss: 623.3563842773438 = 0.04510357603430748 + 100.0 * 6.233112812042236
Epoch 1650, val loss: 1.1674760580062866
Epoch 1660, training loss: 623.1321411132812 = 0.04412440210580826 + 100.0 * 6.230880260467529
Epoch 1660, val loss: 1.1727745532989502
Epoch 1670, training loss: 623.1163330078125 = 0.04318879172205925 + 100.0 * 6.230731010437012
Epoch 1670, val loss: 1.1775479316711426
Epoch 1680, training loss: 623.0368041992188 = 0.04228535667061806 + 100.0 * 6.229945182800293
Epoch 1680, val loss: 1.1825840473175049
Epoch 1690, training loss: 622.9254150390625 = 0.04141509160399437 + 100.0 * 6.228839874267578
Epoch 1690, val loss: 1.1874558925628662
Epoch 1700, training loss: 622.8721923828125 = 0.040566835552453995 + 100.0 * 6.228316307067871
Epoch 1700, val loss: 1.1925643682479858
Epoch 1710, training loss: 623.053955078125 = 0.03975708410143852 + 100.0 * 6.230142116546631
Epoch 1710, val loss: 1.1973048448562622
Epoch 1720, training loss: 623.0457153320312 = 0.03895030543208122 + 100.0 * 6.230067729949951
Epoch 1720, val loss: 1.2022361755371094
Epoch 1730, training loss: 622.9335327148438 = 0.03816349804401398 + 100.0 * 6.228953838348389
Epoch 1730, val loss: 1.20685875415802
Epoch 1740, training loss: 623.3485717773438 = 0.037415653467178345 + 100.0 * 6.233111381530762
Epoch 1740, val loss: 1.211368441581726
Epoch 1750, training loss: 622.7684936523438 = 0.036657847464084625 + 100.0 * 6.227318286895752
Epoch 1750, val loss: 1.2163861989974976
Epoch 1760, training loss: 622.72412109375 = 0.03593901917338371 + 100.0 * 6.226881980895996
Epoch 1760, val loss: 1.221287488937378
Epoch 1770, training loss: 622.6355590820312 = 0.03525523468852043 + 100.0 * 6.2260026931762695
Epoch 1770, val loss: 1.2258394956588745
Epoch 1780, training loss: 622.6246337890625 = 0.034587129950523376 + 100.0 * 6.225900173187256
Epoch 1780, val loss: 1.230487585067749
Epoch 1790, training loss: 623.1956176757812 = 0.033940888941287994 + 100.0 * 6.231616973876953
Epoch 1790, val loss: 1.2351100444793701
Epoch 1800, training loss: 623.06103515625 = 0.03328932076692581 + 100.0 * 6.2302775382995605
Epoch 1800, val loss: 1.2396531105041504
Epoch 1810, training loss: 622.7158203125 = 0.03265402838587761 + 100.0 * 6.226831912994385
Epoch 1810, val loss: 1.2439541816711426
Epoch 1820, training loss: 622.7962036132812 = 0.03204196318984032 + 100.0 * 6.227641582489014
Epoch 1820, val loss: 1.248849868774414
Epoch 1830, training loss: 622.6514282226562 = 0.03144975006580353 + 100.0 * 6.226200103759766
Epoch 1830, val loss: 1.253007411956787
Epoch 1840, training loss: 622.5653686523438 = 0.030887236818671227 + 100.0 * 6.225345134735107
Epoch 1840, val loss: 1.2570520639419556
Epoch 1850, training loss: 622.4140625 = 0.030325915664434433 + 100.0 * 6.223837375640869
Epoch 1850, val loss: 1.2616792917251587
Epoch 1860, training loss: 622.6992797851562 = 0.029789429157972336 + 100.0 * 6.2266950607299805
Epoch 1860, val loss: 1.2660096883773804
Epoch 1870, training loss: 622.7367553710938 = 0.029265154153108597 + 100.0 * 6.227075099945068
Epoch 1870, val loss: 1.2700589895248413
Epoch 1880, training loss: 622.4765014648438 = 0.02872690185904503 + 100.0 * 6.224477291107178
Epoch 1880, val loss: 1.2747113704681396
Epoch 1890, training loss: 622.3663330078125 = 0.028223486617207527 + 100.0 * 6.223381042480469
Epoch 1890, val loss: 1.2787679433822632
Epoch 1900, training loss: 622.3009033203125 = 0.02773374691605568 + 100.0 * 6.222732067108154
Epoch 1900, val loss: 1.2830461263656616
Epoch 1910, training loss: 622.4588012695312 = 0.02726106345653534 + 100.0 * 6.224315166473389
Epoch 1910, val loss: 1.2873872518539429
Epoch 1920, training loss: 622.3682861328125 = 0.026796525344252586 + 100.0 * 6.223414897918701
Epoch 1920, val loss: 1.2912973165512085
Epoch 1930, training loss: 622.6844482421875 = 0.026348398998379707 + 100.0 * 6.22658109664917
Epoch 1930, val loss: 1.2953526973724365
Epoch 1940, training loss: 622.3777465820312 = 0.025892028585076332 + 100.0 * 6.2235188484191895
Epoch 1940, val loss: 1.299365520477295
Epoch 1950, training loss: 622.2434692382812 = 0.025464694947004318 + 100.0 * 6.222179889678955
Epoch 1950, val loss: 1.303059458732605
Epoch 1960, training loss: 622.12939453125 = 0.025040723383426666 + 100.0 * 6.221043586730957
Epoch 1960, val loss: 1.307287335395813
Epoch 1970, training loss: 622.1524047851562 = 0.024633370339870453 + 100.0 * 6.221277713775635
Epoch 1970, val loss: 1.3114720582962036
Epoch 1980, training loss: 623.2451782226562 = 0.02424529753625393 + 100.0 * 6.2322096824646
Epoch 1980, val loss: 1.3156436681747437
Epoch 1990, training loss: 622.4393310546875 = 0.02384503185749054 + 100.0 * 6.224154949188232
Epoch 1990, val loss: 1.3184881210327148
Epoch 2000, training loss: 622.1488037109375 = 0.02345711551606655 + 100.0 * 6.221253871917725
Epoch 2000, val loss: 1.3227381706237793
Epoch 2010, training loss: 622.2103271484375 = 0.02309301123023033 + 100.0 * 6.221872806549072
Epoch 2010, val loss: 1.3263756036758423
Epoch 2020, training loss: 622.3594360351562 = 0.022733207792043686 + 100.0 * 6.223367214202881
Epoch 2020, val loss: 1.3300683498382568
Epoch 2030, training loss: 622.0728759765625 = 0.022362474352121353 + 100.0 * 6.220505237579346
Epoch 2030, val loss: 1.3342539072036743
Epoch 2040, training loss: 622.0331420898438 = 0.022019851952791214 + 100.0 * 6.22011137008667
Epoch 2040, val loss: 1.3379472494125366
Epoch 2050, training loss: 622.5914306640625 = 0.02168874815106392 + 100.0 * 6.2256975173950195
Epoch 2050, val loss: 1.3415608406066895
Epoch 2060, training loss: 622.1982421875 = 0.021351955831050873 + 100.0 * 6.221768856048584
Epoch 2060, val loss: 1.3450019359588623
Epoch 2070, training loss: 622.02490234375 = 0.021026210859417915 + 100.0 * 6.220039367675781
Epoch 2070, val loss: 1.3487595319747925
Epoch 2080, training loss: 622.1048583984375 = 0.020710507407784462 + 100.0 * 6.220841407775879
Epoch 2080, val loss: 1.3525784015655518
Epoch 2090, training loss: 622.046142578125 = 0.020401861518621445 + 100.0 * 6.22025728225708
Epoch 2090, val loss: 1.3560307025909424
Epoch 2100, training loss: 621.9534912109375 = 0.020105116069316864 + 100.0 * 6.219334125518799
Epoch 2100, val loss: 1.3594369888305664
Epoch 2110, training loss: 622.1163940429688 = 0.01981915533542633 + 100.0 * 6.220965385437012
Epoch 2110, val loss: 1.3627046346664429
Epoch 2120, training loss: 622.1239013671875 = 0.01952429860830307 + 100.0 * 6.221043586730957
Epoch 2120, val loss: 1.3662006855010986
Epoch 2130, training loss: 621.8439331054688 = 0.019238173961639404 + 100.0 * 6.218246936798096
Epoch 2130, val loss: 1.369742512702942
Epoch 2140, training loss: 621.76025390625 = 0.018964674323797226 + 100.0 * 6.21741247177124
Epoch 2140, val loss: 1.373175859451294
Epoch 2150, training loss: 621.8156127929688 = 0.018701398745179176 + 100.0 * 6.2179694175720215
Epoch 2150, val loss: 1.376408576965332
Epoch 2160, training loss: 623.097900390625 = 0.01845775730907917 + 100.0 * 6.230794429779053
Epoch 2160, val loss: 1.3793984651565552
Epoch 2170, training loss: 622.0328369140625 = 0.018168633803725243 + 100.0 * 6.220146179199219
Epoch 2170, val loss: 1.3829858303070068
Epoch 2180, training loss: 621.7246704101562 = 0.017912816256284714 + 100.0 * 6.217067718505859
Epoch 2180, val loss: 1.386397361755371
Epoch 2190, training loss: 621.651611328125 = 0.017668478190898895 + 100.0 * 6.216339588165283
Epoch 2190, val loss: 1.3896534442901611
Epoch 2200, training loss: 621.6326904296875 = 0.017431046813726425 + 100.0 * 6.216152191162109
Epoch 2200, val loss: 1.3931872844696045
Epoch 2210, training loss: 622.2039184570312 = 0.01720365323126316 + 100.0 * 6.221867084503174
Epoch 2210, val loss: 1.396315336227417
Epoch 2220, training loss: 621.8095092773438 = 0.01696695387363434 + 100.0 * 6.217925548553467
Epoch 2220, val loss: 1.3993676900863647
Epoch 2230, training loss: 621.8392333984375 = 0.016730444505810738 + 100.0 * 6.218225002288818
Epoch 2230, val loss: 1.4025733470916748
Epoch 2240, training loss: 621.6289672851562 = 0.01651482656598091 + 100.0 * 6.216124534606934
Epoch 2240, val loss: 1.4056358337402344
Epoch 2250, training loss: 621.5795288085938 = 0.01629932038486004 + 100.0 * 6.215632438659668
Epoch 2250, val loss: 1.408894419670105
Epoch 2260, training loss: 621.9215087890625 = 0.01609834097325802 + 100.0 * 6.219054222106934
Epoch 2260, val loss: 1.4119147062301636
Epoch 2270, training loss: 621.5725708007812 = 0.01588386669754982 + 100.0 * 6.215567111968994
Epoch 2270, val loss: 1.414873480796814
Epoch 2280, training loss: 621.7894287109375 = 0.015681594610214233 + 100.0 * 6.217737197875977
Epoch 2280, val loss: 1.4178392887115479
Epoch 2290, training loss: 621.8424072265625 = 0.015484649688005447 + 100.0 * 6.218269348144531
Epoch 2290, val loss: 1.4207369089126587
Epoch 2300, training loss: 621.5747680664062 = 0.0152791952714324 + 100.0 * 6.215595245361328
Epoch 2300, val loss: 1.4241371154785156
Epoch 2310, training loss: 621.6968994140625 = 0.01509040966629982 + 100.0 * 6.216818332672119
Epoch 2310, val loss: 1.4271292686462402
Epoch 2320, training loss: 621.4664916992188 = 0.014905469492077827 + 100.0 * 6.2145161628723145
Epoch 2320, val loss: 1.4298884868621826
Epoch 2330, training loss: 621.5189208984375 = 0.01472457405179739 + 100.0 * 6.2150421142578125
Epoch 2330, val loss: 1.4327399730682373
Epoch 2340, training loss: 621.7188720703125 = 0.014549680054187775 + 100.0 * 6.217043399810791
Epoch 2340, val loss: 1.43559730052948
Epoch 2350, training loss: 621.6542358398438 = 0.014366164803504944 + 100.0 * 6.2163987159729
Epoch 2350, val loss: 1.4387036561965942
Epoch 2360, training loss: 621.5633544921875 = 0.01418762281537056 + 100.0 * 6.215492248535156
Epoch 2360, val loss: 1.4416648149490356
Epoch 2370, training loss: 621.5416870117188 = 0.014020713977515697 + 100.0 * 6.21527624130249
Epoch 2370, val loss: 1.4444355964660645
Epoch 2380, training loss: 621.6587524414062 = 0.013853246346116066 + 100.0 * 6.21644926071167
Epoch 2380, val loss: 1.4471991062164307
Epoch 2390, training loss: 621.6381225585938 = 0.013697140850126743 + 100.0 * 6.216244220733643
Epoch 2390, val loss: 1.4496592283248901
Epoch 2400, training loss: 621.3460693359375 = 0.013529927469789982 + 100.0 * 6.213325500488281
Epoch 2400, val loss: 1.4524908065795898
Epoch 2410, training loss: 621.3464965820312 = 0.013374252244830132 + 100.0 * 6.21333122253418
Epoch 2410, val loss: 1.4553358554840088
Epoch 2420, training loss: 621.7645874023438 = 0.013230541720986366 + 100.0 * 6.217513561248779
Epoch 2420, val loss: 1.4577358961105347
Epoch 2430, training loss: 621.3114624023438 = 0.013068233616650105 + 100.0 * 6.212984085083008
Epoch 2430, val loss: 1.4605820178985596
Epoch 2440, training loss: 621.2278442382812 = 0.0129143837839365 + 100.0 * 6.212149143218994
Epoch 2440, val loss: 1.4634485244750977
Epoch 2450, training loss: 621.2515869140625 = 0.012772982940077782 + 100.0 * 6.212388515472412
Epoch 2450, val loss: 1.4661366939544678
Epoch 2460, training loss: 621.4947509765625 = 0.01263123843818903 + 100.0 * 6.2148213386535645
Epoch 2460, val loss: 1.4688420295715332
Epoch 2470, training loss: 621.378173828125 = 0.012489495798945427 + 100.0 * 6.213656425476074
Epoch 2470, val loss: 1.4711438417434692
Epoch 2480, training loss: 621.4470825195312 = 0.012353116646409035 + 100.0 * 6.2143473625183105
Epoch 2480, val loss: 1.4736641645431519
Epoch 2490, training loss: 621.2317504882812 = 0.01221439242362976 + 100.0 * 6.21219539642334
Epoch 2490, val loss: 1.476563811302185
Epoch 2500, training loss: 621.2285766601562 = 0.012083259411156178 + 100.0 * 6.212164878845215
Epoch 2500, val loss: 1.479285717010498
Epoch 2510, training loss: 622.0069580078125 = 0.011956692673265934 + 100.0 * 6.219950199127197
Epoch 2510, val loss: 1.4818211793899536
Epoch 2520, training loss: 621.3876342773438 = 0.011824065819382668 + 100.0 * 6.2137579917907715
Epoch 2520, val loss: 1.4838284254074097
Epoch 2530, training loss: 621.1694946289062 = 0.01169531885534525 + 100.0 * 6.211577892303467
Epoch 2530, val loss: 1.4866169691085815
Epoch 2540, training loss: 621.1146240234375 = 0.011575835756957531 + 100.0 * 6.21103048324585
Epoch 2540, val loss: 1.4890620708465576
Epoch 2550, training loss: 621.6203002929688 = 0.011464802548289299 + 100.0 * 6.21608829498291
Epoch 2550, val loss: 1.491313099861145
Epoch 2560, training loss: 621.1334228515625 = 0.011333954520523548 + 100.0 * 6.211220741271973
Epoch 2560, val loss: 1.4938068389892578
Epoch 2570, training loss: 621.25341796875 = 0.011218319647014141 + 100.0 * 6.212421894073486
Epoch 2570, val loss: 1.4960136413574219
Epoch 2580, training loss: 621.0244140625 = 0.011099576950073242 + 100.0 * 6.210133075714111
Epoch 2580, val loss: 1.498678207397461
Epoch 2590, training loss: 621.089599609375 = 0.010988802649080753 + 100.0 * 6.210785865783691
Epoch 2590, val loss: 1.5013055801391602
Epoch 2600, training loss: 621.392822265625 = 0.010883506387472153 + 100.0 * 6.21381950378418
Epoch 2600, val loss: 1.5034997463226318
Epoch 2610, training loss: 621.4161987304688 = 0.010769308544695377 + 100.0 * 6.214054584503174
Epoch 2610, val loss: 1.5056263208389282
Epoch 2620, training loss: 621.2455444335938 = 0.010659483261406422 + 100.0 * 6.212348937988281
Epoch 2620, val loss: 1.5083791017532349
Epoch 2630, training loss: 621.3206787109375 = 0.010549330152571201 + 100.0 * 6.213100910186768
Epoch 2630, val loss: 1.510588526725769
Epoch 2640, training loss: 620.9896240234375 = 0.010449081659317017 + 100.0 * 6.209792137145996
Epoch 2640, val loss: 1.5125218629837036
Epoch 2650, training loss: 620.9517822265625 = 0.010348035953938961 + 100.0 * 6.209414005279541
Epoch 2650, val loss: 1.5147935152053833
Epoch 2660, training loss: 620.91015625 = 0.010250349529087543 + 100.0 * 6.208999156951904
Epoch 2660, val loss: 1.5172398090362549
Epoch 2670, training loss: 621.6300659179688 = 0.010160088539123535 + 100.0 * 6.2161993980407715
Epoch 2670, val loss: 1.519271731376648
Epoch 2680, training loss: 620.9347534179688 = 0.010048669762909412 + 100.0 * 6.209246635437012
Epoch 2680, val loss: 1.521504521369934
Epoch 2690, training loss: 620.8602905273438 = 0.009953931905329227 + 100.0 * 6.208503723144531
Epoch 2690, val loss: 1.523605227470398
Epoch 2700, training loss: 620.9320068359375 = 0.009861092083156109 + 100.0 * 6.209220886230469
Epoch 2700, val loss: 1.5259054899215698
Epoch 2710, training loss: 620.998779296875 = 0.009768730960786343 + 100.0 * 6.209890365600586
Epoch 2710, val loss: 1.5280247926712036
Epoch 2720, training loss: 621.140869140625 = 0.00968200620263815 + 100.0 * 6.2113118171691895
Epoch 2720, val loss: 1.5299237966537476
Epoch 2730, training loss: 621.4249267578125 = 0.009591486304998398 + 100.0 * 6.214153289794922
Epoch 2730, val loss: 1.531775712966919
Epoch 2740, training loss: 621.2130737304688 = 0.009494098834693432 + 100.0 * 6.212035655975342
Epoch 2740, val loss: 1.534166693687439
Epoch 2750, training loss: 620.8604736328125 = 0.009406535886228085 + 100.0 * 6.208510875701904
Epoch 2750, val loss: 1.5358896255493164
Epoch 2760, training loss: 620.77880859375 = 0.00932192150503397 + 100.0 * 6.2076945304870605
Epoch 2760, val loss: 1.538257122039795
Epoch 2770, training loss: 620.7100219726562 = 0.009239236824214458 + 100.0 * 6.207007884979248
Epoch 2770, val loss: 1.5405100584030151
Epoch 2780, training loss: 620.691162109375 = 0.009159876964986324 + 100.0 * 6.206820011138916
Epoch 2780, val loss: 1.5425413846969604
Epoch 2790, training loss: 620.6805419921875 = 0.00908108614385128 + 100.0 * 6.206714630126953
Epoch 2790, val loss: 1.5445709228515625
Epoch 2800, training loss: 620.9409790039062 = 0.009004865773022175 + 100.0 * 6.209319591522217
Epoch 2800, val loss: 1.546593427658081
Epoch 2810, training loss: 621.32275390625 = 0.008922343142330647 + 100.0 * 6.213138580322266
Epoch 2810, val loss: 1.5485249757766724
Epoch 2820, training loss: 621.0122680664062 = 0.00884257722645998 + 100.0 * 6.210033893585205
Epoch 2820, val loss: 1.549606442451477
Epoch 2830, training loss: 620.7758178710938 = 0.008762405253946781 + 100.0 * 6.207670211791992
Epoch 2830, val loss: 1.5518323183059692
Epoch 2840, training loss: 620.64892578125 = 0.008688654750585556 + 100.0 * 6.206402778625488
Epoch 2840, val loss: 1.5539288520812988
Epoch 2850, training loss: 620.8516235351562 = 0.008619304746389389 + 100.0 * 6.208430290222168
Epoch 2850, val loss: 1.5557435750961304
Epoch 2860, training loss: 621.0426635742188 = 0.008545499294996262 + 100.0 * 6.210341453552246
Epoch 2860, val loss: 1.557548999786377
Epoch 2870, training loss: 620.8606567382812 = 0.00846873503178358 + 100.0 * 6.208521842956543
Epoch 2870, val loss: 1.5597693920135498
Epoch 2880, training loss: 620.7802124023438 = 0.008399896323680878 + 100.0 * 6.2077178955078125
Epoch 2880, val loss: 1.5613421201705933
Epoch 2890, training loss: 620.7740478515625 = 0.008330660872161388 + 100.0 * 6.207657337188721
Epoch 2890, val loss: 1.563328742980957
Epoch 2900, training loss: 620.6014404296875 = 0.008262542076408863 + 100.0 * 6.205932140350342
Epoch 2900, val loss: 1.5650445222854614
Epoch 2910, training loss: 620.6602783203125 = 0.008198702707886696 + 100.0 * 6.206521034240723
Epoch 2910, val loss: 1.566906452178955
Epoch 2920, training loss: 620.873779296875 = 0.008134526200592518 + 100.0 * 6.2086567878723145
Epoch 2920, val loss: 1.5686217546463013
Epoch 2930, training loss: 620.7059936523438 = 0.008065172471106052 + 100.0 * 6.206979274749756
Epoch 2930, val loss: 1.5701271295547485
Epoch 2940, training loss: 620.9110717773438 = 0.00800330936908722 + 100.0 * 6.209030628204346
Epoch 2940, val loss: 1.57166588306427
Epoch 2950, training loss: 620.9505004882812 = 0.007940730080008507 + 100.0 * 6.209425449371338
Epoch 2950, val loss: 1.5734821557998657
Epoch 2960, training loss: 620.5240478515625 = 0.007870244793593884 + 100.0 * 6.2051615715026855
Epoch 2960, val loss: 1.5752084255218506
Epoch 2970, training loss: 620.5328369140625 = 0.007806624751538038 + 100.0 * 6.205250263214111
Epoch 2970, val loss: 1.5771644115447998
Epoch 2980, training loss: 620.4884033203125 = 0.0077481321059167385 + 100.0 * 6.204806804656982
Epoch 2980, val loss: 1.579010009765625
Epoch 2990, training loss: 620.6686401367188 = 0.007691486272960901 + 100.0 * 6.206609725952148
Epoch 2990, val loss: 1.5806137323379517
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6703703703703704
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 861.6326904296875 = 1.946587324142456 + 100.0 * 8.596860885620117
Epoch 0, val loss: 1.9434009790420532
Epoch 10, training loss: 861.556396484375 = 1.937145709991455 + 100.0 * 8.596192359924316
Epoch 10, val loss: 1.9343268871307373
Epoch 20, training loss: 860.98876953125 = 1.9253816604614258 + 100.0 * 8.5906343460083
Epoch 20, val loss: 1.9226503372192383
Epoch 30, training loss: 857.0439453125 = 1.9097321033477783 + 100.0 * 8.551342010498047
Epoch 30, val loss: 1.9065831899642944
Epoch 40, training loss: 838.2442016601562 = 1.8900178670883179 + 100.0 * 8.363541603088379
Epoch 40, val loss: 1.886836051940918
Epoch 50, training loss: 798.20361328125 = 1.8684015274047852 + 100.0 * 7.963352203369141
Epoch 50, val loss: 1.865755558013916
Epoch 60, training loss: 767.353759765625 = 1.851667881011963 + 100.0 * 7.655020713806152
Epoch 60, val loss: 1.8515180349349976
Epoch 70, training loss: 731.2908325195312 = 1.8406614065170288 + 100.0 * 7.294501781463623
Epoch 70, val loss: 1.8418662548065186
Epoch 80, training loss: 711.5222778320312 = 1.831534743309021 + 100.0 * 7.096907615661621
Epoch 80, val loss: 1.8329540491104126
Epoch 90, training loss: 699.0763549804688 = 1.8214224576950073 + 100.0 * 6.9725494384765625
Epoch 90, val loss: 1.8232654333114624
Epoch 100, training loss: 686.460693359375 = 1.8122528791427612 + 100.0 * 6.846484661102295
Epoch 100, val loss: 1.8154109716415405
Epoch 110, training loss: 677.6555786132812 = 1.8045185804367065 + 100.0 * 6.758510589599609
Epoch 110, val loss: 1.8083913326263428
Epoch 120, training loss: 671.419189453125 = 1.7962301969528198 + 100.0 * 6.696229934692383
Epoch 120, val loss: 1.8009254932403564
Epoch 130, training loss: 666.6846313476562 = 1.7872533798217773 + 100.0 * 6.6489739418029785
Epoch 130, val loss: 1.7929058074951172
Epoch 140, training loss: 663.205810546875 = 1.7774921655654907 + 100.0 * 6.614283561706543
Epoch 140, val loss: 1.7843585014343262
Epoch 150, training loss: 659.8651123046875 = 1.7671570777893066 + 100.0 * 6.580979347229004
Epoch 150, val loss: 1.7753193378448486
Epoch 160, training loss: 657.1405029296875 = 1.7561718225479126 + 100.0 * 6.5538434982299805
Epoch 160, val loss: 1.7657067775726318
Epoch 170, training loss: 654.9022216796875 = 1.7441534996032715 + 100.0 * 6.531580924987793
Epoch 170, val loss: 1.7551751136779785
Epoch 180, training loss: 653.0916748046875 = 1.7308619022369385 + 100.0 * 6.513608455657959
Epoch 180, val loss: 1.7435994148254395
Epoch 190, training loss: 651.4285278320312 = 1.7162963151931763 + 100.0 * 6.497122287750244
Epoch 190, val loss: 1.7309459447860718
Epoch 200, training loss: 649.7489624023438 = 1.700448751449585 + 100.0 * 6.480485439300537
Epoch 200, val loss: 1.717316746711731
Epoch 210, training loss: 648.3262329101562 = 1.6834253072738647 + 100.0 * 6.466428279876709
Epoch 210, val loss: 1.7026307582855225
Epoch 220, training loss: 646.978271484375 = 1.665244698524475 + 100.0 * 6.453130722045898
Epoch 220, val loss: 1.6869670152664185
Epoch 230, training loss: 646.0023803710938 = 1.6457691192626953 + 100.0 * 6.44356632232666
Epoch 230, val loss: 1.6702256202697754
Epoch 240, training loss: 644.760009765625 = 1.6250401735305786 + 100.0 * 6.431349277496338
Epoch 240, val loss: 1.6523878574371338
Epoch 250, training loss: 643.6674194335938 = 1.603333592414856 + 100.0 * 6.42064094543457
Epoch 250, val loss: 1.6336323022842407
Epoch 260, training loss: 642.7488403320312 = 1.5806372165679932 + 100.0 * 6.41168212890625
Epoch 260, val loss: 1.614105224609375
Epoch 270, training loss: 642.42919921875 = 1.5569419860839844 + 100.0 * 6.408722400665283
Epoch 270, val loss: 1.5937634706497192
Epoch 280, training loss: 641.2745971679688 = 1.5326215028762817 + 100.0 * 6.3974199295043945
Epoch 280, val loss: 1.5729725360870361
Epoch 290, training loss: 640.5527954101562 = 1.5078612565994263 + 100.0 * 6.390449523925781
Epoch 290, val loss: 1.5518914461135864
Epoch 300, training loss: 639.927734375 = 1.4827769994735718 + 100.0 * 6.384449481964111
Epoch 300, val loss: 1.5307351350784302
Epoch 310, training loss: 639.3759765625 = 1.457493543624878 + 100.0 * 6.379184722900391
Epoch 310, val loss: 1.509602665901184
Epoch 320, training loss: 638.9262084960938 = 1.432181477546692 + 100.0 * 6.374939918518066
Epoch 320, val loss: 1.4886341094970703
Epoch 330, training loss: 638.3184814453125 = 1.4069329500198364 + 100.0 * 6.369115829467773
Epoch 330, val loss: 1.4679545164108276
Epoch 340, training loss: 637.8609008789062 = 1.3819042444229126 + 100.0 * 6.364789962768555
Epoch 340, val loss: 1.4476830959320068
Epoch 350, training loss: 637.2031860351562 = 1.3569004535675049 + 100.0 * 6.358463287353516
Epoch 350, val loss: 1.4276630878448486
Epoch 360, training loss: 637.2250366210938 = 1.3320621252059937 + 100.0 * 6.35892915725708
Epoch 360, val loss: 1.4080743789672852
Epoch 370, training loss: 636.4491577148438 = 1.3071582317352295 + 100.0 * 6.3514204025268555
Epoch 370, val loss: 1.3886562585830688
Epoch 380, training loss: 635.9749145507812 = 1.2822717428207397 + 100.0 * 6.346926212310791
Epoch 380, val loss: 1.3696483373641968
Epoch 390, training loss: 635.692138671875 = 1.2573455572128296 + 100.0 * 6.344347953796387
Epoch 390, val loss: 1.3507261276245117
Epoch 400, training loss: 635.1112670898438 = 1.232480525970459 + 100.0 * 6.33878755569458
Epoch 400, val loss: 1.33220636844635
Epoch 410, training loss: 634.6756591796875 = 1.2075529098510742 + 100.0 * 6.334681034088135
Epoch 410, val loss: 1.3140016794204712
Epoch 420, training loss: 634.318359375 = 1.1825956106185913 + 100.0 * 6.331357479095459
Epoch 420, val loss: 1.2960436344146729
Epoch 430, training loss: 634.3895263671875 = 1.1575030088424683 + 100.0 * 6.332319736480713
Epoch 430, val loss: 1.2783172130584717
Epoch 440, training loss: 633.6475219726562 = 1.1323238611221313 + 100.0 * 6.3251519203186035
Epoch 440, val loss: 1.2606366872787476
Epoch 450, training loss: 633.3956909179688 = 1.1073577404022217 + 100.0 * 6.322883605957031
Epoch 450, val loss: 1.2435227632522583
Epoch 460, training loss: 633.3168334960938 = 1.0826385021209717 + 100.0 * 6.3223419189453125
Epoch 460, val loss: 1.227000117301941
Epoch 470, training loss: 633.5305786132812 = 1.0578539371490479 + 100.0 * 6.3247270584106445
Epoch 470, val loss: 1.210425853729248
Epoch 480, training loss: 632.7381591796875 = 1.0336518287658691 + 100.0 * 6.317045211791992
Epoch 480, val loss: 1.1947282552719116
Epoch 490, training loss: 632.3046264648438 = 1.009911298751831 + 100.0 * 6.3129472732543945
Epoch 490, val loss: 1.1796647310256958
Epoch 500, training loss: 631.98974609375 = 0.9866983890533447 + 100.0 * 6.310030460357666
Epoch 500, val loss: 1.1652555465698242
Epoch 510, training loss: 631.7164916992188 = 0.9640712738037109 + 100.0 * 6.30752420425415
Epoch 510, val loss: 1.1515737771987915
Epoch 520, training loss: 632.42822265625 = 0.942064642906189 + 100.0 * 6.314861297607422
Epoch 520, val loss: 1.1387579441070557
Epoch 530, training loss: 631.3001098632812 = 0.9205420613288879 + 100.0 * 6.30379581451416
Epoch 530, val loss: 1.1261484622955322
Epoch 540, training loss: 631.0955200195312 = 0.8998176455497742 + 100.0 * 6.301956653594971
Epoch 540, val loss: 1.1146677732467651
Epoch 550, training loss: 630.88037109375 = 0.8798738718032837 + 100.0 * 6.300004959106445
Epoch 550, val loss: 1.1040509939193726
Epoch 560, training loss: 630.8466186523438 = 0.8606514930725098 + 100.0 * 6.299859523773193
Epoch 560, val loss: 1.0942611694335938
Epoch 570, training loss: 630.4996948242188 = 0.8420299887657166 + 100.0 * 6.296576499938965
Epoch 570, val loss: 1.0847923755645752
Epoch 580, training loss: 630.3214721679688 = 0.8241094350814819 + 100.0 * 6.294973850250244
Epoch 580, val loss: 1.0763189792633057
Epoch 590, training loss: 630.3920288085938 = 0.8069284558296204 + 100.0 * 6.29585075378418
Epoch 590, val loss: 1.0684834718704224
Epoch 600, training loss: 630.0227661132812 = 0.7903059720993042 + 100.0 * 6.292324542999268
Epoch 600, val loss: 1.0613120794296265
Epoch 610, training loss: 629.8110961914062 = 0.7743346095085144 + 100.0 * 6.290367603302002
Epoch 610, val loss: 1.054749608039856
Epoch 620, training loss: 630.0352783203125 = 0.7589265704154968 + 100.0 * 6.292763710021973
Epoch 620, val loss: 1.0488462448120117
Epoch 630, training loss: 629.8134765625 = 0.7439585328102112 + 100.0 * 6.2906951904296875
Epoch 630, val loss: 1.0436041355133057
Epoch 640, training loss: 629.302001953125 = 0.7295699715614319 + 100.0 * 6.285724639892578
Epoch 640, val loss: 1.0387920141220093
Epoch 650, training loss: 629.2256469726562 = 0.7156538963317871 + 100.0 * 6.285099983215332
Epoch 650, val loss: 1.0346124172210693
Epoch 660, training loss: 629.4979248046875 = 0.7022002339363098 + 100.0 * 6.287957191467285
Epoch 660, val loss: 1.0308722257614136
Epoch 670, training loss: 629.0999145507812 = 0.6890391707420349 + 100.0 * 6.284109115600586
Epoch 670, val loss: 1.027137041091919
Epoch 680, training loss: 629.052978515625 = 0.6763091087341309 + 100.0 * 6.283766269683838
Epoch 680, val loss: 1.0242342948913574
Epoch 690, training loss: 628.685546875 = 0.6639715433120728 + 100.0 * 6.280215740203857
Epoch 690, val loss: 1.0216172933578491
Epoch 700, training loss: 628.5067749023438 = 0.6519577503204346 + 100.0 * 6.278548240661621
Epoch 700, val loss: 1.0194824934005737
Epoch 710, training loss: 629.0232543945312 = 0.6402772665023804 + 100.0 * 6.283829689025879
Epoch 710, val loss: 1.017647624015808
Epoch 720, training loss: 628.545166015625 = 0.6286739110946655 + 100.0 * 6.279164791107178
Epoch 720, val loss: 1.0155820846557617
Epoch 730, training loss: 628.1025390625 = 0.6174658536911011 + 100.0 * 6.274850368499756
Epoch 730, val loss: 1.0143269300460815
Epoch 740, training loss: 628.4510498046875 = 0.6064801812171936 + 100.0 * 6.278445720672607
Epoch 740, val loss: 1.0131937265396118
Epoch 750, training loss: 627.9877319335938 = 0.5957058072090149 + 100.0 * 6.27392053604126
Epoch 750, val loss: 1.0124156475067139
Epoch 760, training loss: 627.7735595703125 = 0.5851548910140991 + 100.0 * 6.271884441375732
Epoch 760, val loss: 1.0119203329086304
Epoch 770, training loss: 627.6958618164062 = 0.5748391151428223 + 100.0 * 6.271210193634033
Epoch 770, val loss: 1.0116053819656372
Epoch 780, training loss: 627.7362670898438 = 0.5646259188652039 + 100.0 * 6.271716594696045
Epoch 780, val loss: 1.011345386505127
Epoch 790, training loss: 628.1348266601562 = 0.5545272827148438 + 100.0 * 6.275803089141846
Epoch 790, val loss: 1.0111314058303833
Epoch 800, training loss: 627.3851318359375 = 0.5446200370788574 + 100.0 * 6.268405437469482
Epoch 800, val loss: 1.0114322900772095
Epoch 810, training loss: 627.2081909179688 = 0.534869372844696 + 100.0 * 6.266733646392822
Epoch 810, val loss: 1.0117594003677368
Epoch 820, training loss: 627.0659790039062 = 0.5252833366394043 + 100.0 * 6.265407085418701
Epoch 820, val loss: 1.0123341083526611
Epoch 830, training loss: 627.1026000976562 = 0.5158373713493347 + 100.0 * 6.265867710113525
Epoch 830, val loss: 1.0130497217178345
Epoch 840, training loss: 627.4729614257812 = 0.5064326524734497 + 100.0 * 6.269665241241455
Epoch 840, val loss: 1.0138788223266602
Epoch 850, training loss: 627.0123291015625 = 0.4970833957195282 + 100.0 * 6.265152454376221
Epoch 850, val loss: 1.0146900415420532
Epoch 860, training loss: 626.7752685546875 = 0.48787689208984375 + 100.0 * 6.262874126434326
Epoch 860, val loss: 1.0158053636550903
Epoch 870, training loss: 626.5816650390625 = 0.4788261651992798 + 100.0 * 6.261028289794922
Epoch 870, val loss: 1.0170540809631348
Epoch 880, training loss: 627.2640380859375 = 0.4699220359325409 + 100.0 * 6.267940998077393
Epoch 880, val loss: 1.0185248851776123
Epoch 890, training loss: 626.9134521484375 = 0.46093955636024475 + 100.0 * 6.264524936676025
Epoch 890, val loss: 1.019713044166565
Epoch 900, training loss: 626.3126831054688 = 0.45207375288009644 + 100.0 * 6.25860595703125
Epoch 900, val loss: 1.021196722984314
Epoch 910, training loss: 626.3024291992188 = 0.44338399171829224 + 100.0 * 6.2585906982421875
Epoch 910, val loss: 1.022848129272461
Epoch 920, training loss: 626.285400390625 = 0.4347628951072693 + 100.0 * 6.2585062980651855
Epoch 920, val loss: 1.0244956016540527
Epoch 930, training loss: 626.3834228515625 = 0.42618077993392944 + 100.0 * 6.2595720291137695
Epoch 930, val loss: 1.026342511177063
Epoch 940, training loss: 626.0684204101562 = 0.41766759753227234 + 100.0 * 6.256507873535156
Epoch 940, val loss: 1.0282756090164185
Epoch 950, training loss: 626.0208129882812 = 0.40923449397087097 + 100.0 * 6.256115436553955
Epoch 950, val loss: 1.0302834510803223
Epoch 960, training loss: 626.0912475585938 = 0.40085116028785706 + 100.0 * 6.256903648376465
Epoch 960, val loss: 1.0322185754776
Epoch 970, training loss: 626.4923706054688 = 0.3925130367279053 + 100.0 * 6.260998249053955
Epoch 970, val loss: 1.0341899394989014
Epoch 980, training loss: 625.8890991210938 = 0.3842432200908661 + 100.0 * 6.255048751831055
Epoch 980, val loss: 1.036462426185608
Epoch 990, training loss: 625.6437377929688 = 0.3760191798210144 + 100.0 * 6.252676963806152
Epoch 990, val loss: 1.0385347604751587
Epoch 1000, training loss: 625.5982055664062 = 0.36788931488990784 + 100.0 * 6.252303600311279
Epoch 1000, val loss: 1.0407085418701172
Epoch 1010, training loss: 625.6661376953125 = 0.35980474948883057 + 100.0 * 6.253063201904297
Epoch 1010, val loss: 1.0428824424743652
Epoch 1020, training loss: 625.967529296875 = 0.3517232835292816 + 100.0 * 6.256157875061035
Epoch 1020, val loss: 1.045069694519043
Epoch 1030, training loss: 625.503662109375 = 0.34370869398117065 + 100.0 * 6.2515997886657715
Epoch 1030, val loss: 1.0473300218582153
Epoch 1040, training loss: 625.3994140625 = 0.3357357978820801 + 100.0 * 6.250636577606201
Epoch 1040, val loss: 1.049572467803955
Epoch 1050, training loss: 625.4633178710938 = 0.3278523087501526 + 100.0 * 6.251354217529297
Epoch 1050, val loss: 1.0519016981124878
Epoch 1060, training loss: 625.1777954101562 = 0.31998690962791443 + 100.0 * 6.248578071594238
Epoch 1060, val loss: 1.054212212562561
Epoch 1070, training loss: 625.1782836914062 = 0.31222832202911377 + 100.0 * 6.248660564422607
Epoch 1070, val loss: 1.0566242933273315
Epoch 1080, training loss: 625.5213012695312 = 0.3045082688331604 + 100.0 * 6.252167701721191
Epoch 1080, val loss: 1.0589985847473145
Epoch 1090, training loss: 625.327392578125 = 0.29684558510780334 + 100.0 * 6.25030517578125
Epoch 1090, val loss: 1.061489224433899
Epoch 1100, training loss: 625.0111083984375 = 0.2892952263355255 + 100.0 * 6.247218132019043
Epoch 1100, val loss: 1.064084529876709
Epoch 1110, training loss: 625.4207763671875 = 0.28186437487602234 + 100.0 * 6.251389026641846
Epoch 1110, val loss: 1.0667091608047485
Epoch 1120, training loss: 624.9097290039062 = 0.2744990587234497 + 100.0 * 6.246352672576904
Epoch 1120, val loss: 1.0696139335632324
Epoch 1130, training loss: 624.6921997070312 = 0.26727306842803955 + 100.0 * 6.24424934387207
Epoch 1130, val loss: 1.0724174976348877
Epoch 1140, training loss: 624.7177734375 = 0.2601942718029022 + 100.0 * 6.2445759773254395
Epoch 1140, val loss: 1.0755075216293335
Epoch 1150, training loss: 625.606201171875 = 0.2532467544078827 + 100.0 * 6.2535295486450195
Epoch 1150, val loss: 1.0786439180374146
Epoch 1160, training loss: 625.0301513671875 = 0.2462511956691742 + 100.0 * 6.247838497161865
Epoch 1160, val loss: 1.0814203023910522
Epoch 1170, training loss: 624.701171875 = 0.23952099680900574 + 100.0 * 6.244616508483887
Epoch 1170, val loss: 1.0848004817962646
Epoch 1180, training loss: 624.4337768554688 = 0.23293130099773407 + 100.0 * 6.242008686065674
Epoch 1180, val loss: 1.0881998538970947
Epoch 1190, training loss: 624.3839721679688 = 0.22654704749584198 + 100.0 * 6.241573810577393
Epoch 1190, val loss: 1.0917649269104004
Epoch 1200, training loss: 625.1454467773438 = 0.2202877253293991 + 100.0 * 6.249251842498779
Epoch 1200, val loss: 1.0953255891799927
Epoch 1210, training loss: 624.7384643554688 = 0.21414589881896973 + 100.0 * 6.245243072509766
Epoch 1210, val loss: 1.098854899406433
Epoch 1220, training loss: 624.2399291992188 = 0.20817120373249054 + 100.0 * 6.240317344665527
Epoch 1220, val loss: 1.1027461290359497
Epoch 1230, training loss: 624.7763061523438 = 0.2023783177137375 + 100.0 * 6.245738983154297
Epoch 1230, val loss: 1.1066608428955078
Epoch 1240, training loss: 624.21044921875 = 0.1967022716999054 + 100.0 * 6.240137577056885
Epoch 1240, val loss: 1.1107312440872192
Epoch 1250, training loss: 624.0700073242188 = 0.19121579825878143 + 100.0 * 6.23878812789917
Epoch 1250, val loss: 1.1148436069488525
Epoch 1260, training loss: 624.146728515625 = 0.18590585887432098 + 100.0 * 6.239608287811279
Epoch 1260, val loss: 1.1191521883010864
Epoch 1270, training loss: 624.6047973632812 = 0.18071787059307098 + 100.0 * 6.244240760803223
Epoch 1270, val loss: 1.123396635055542
Epoch 1280, training loss: 624.1939086914062 = 0.17566029727458954 + 100.0 * 6.240182399749756
Epoch 1280, val loss: 1.1277133226394653
Epoch 1290, training loss: 623.901123046875 = 0.17077724635601044 + 100.0 * 6.237303256988525
Epoch 1290, val loss: 1.1322184801101685
Epoch 1300, training loss: 623.9932250976562 = 0.16606910526752472 + 100.0 * 6.238271236419678
Epoch 1300, val loss: 1.1368823051452637
Epoch 1310, training loss: 624.1282348632812 = 0.16147060692310333 + 100.0 * 6.2396674156188965
Epoch 1310, val loss: 1.1415116786956787
Epoch 1320, training loss: 624.02880859375 = 0.15696975588798523 + 100.0 * 6.238718032836914
Epoch 1320, val loss: 1.1460626125335693
Epoch 1330, training loss: 624.1173706054688 = 0.15264713764190674 + 100.0 * 6.239647388458252
Epoch 1330, val loss: 1.1509814262390137
Epoch 1340, training loss: 623.78125 = 0.14844457805156708 + 100.0 * 6.236328125
Epoch 1340, val loss: 1.1556271314620972
Epoch 1350, training loss: 623.5933837890625 = 0.14436601102352142 + 100.0 * 6.234490394592285
Epoch 1350, val loss: 1.1606608629226685
Epoch 1360, training loss: 623.6475830078125 = 0.14043903350830078 + 100.0 * 6.235071659088135
Epoch 1360, val loss: 1.1655821800231934
Epoch 1370, training loss: 624.2506103515625 = 0.13660229742527008 + 100.0 * 6.241140365600586
Epoch 1370, val loss: 1.1705632209777832
Epoch 1380, training loss: 623.7910766601562 = 0.13286244869232178 + 100.0 * 6.236581802368164
Epoch 1380, val loss: 1.175551176071167
Epoch 1390, training loss: 623.550537109375 = 0.12923374772071838 + 100.0 * 6.234213352203369
Epoch 1390, val loss: 1.1807372570037842
Epoch 1400, training loss: 623.4688720703125 = 0.12572301924228668 + 100.0 * 6.233431339263916
Epoch 1400, val loss: 1.1856536865234375
Epoch 1410, training loss: 623.6082763671875 = 0.1223444789648056 + 100.0 * 6.234859466552734
Epoch 1410, val loss: 1.1909376382827759
Epoch 1420, training loss: 623.6268920898438 = 0.11906066536903381 + 100.0 * 6.235077857971191
Epoch 1420, val loss: 1.1961922645568848
Epoch 1430, training loss: 623.5578002929688 = 0.11584609746932983 + 100.0 * 6.234419822692871
Epoch 1430, val loss: 1.2013849020004272
Epoch 1440, training loss: 623.4238891601562 = 0.11271528154611588 + 100.0 * 6.23311185836792
Epoch 1440, val loss: 1.2064781188964844
Epoch 1450, training loss: 623.4384765625 = 0.10970873385667801 + 100.0 * 6.233287811279297
Epoch 1450, val loss: 1.2119983434677124
Epoch 1460, training loss: 623.2418212890625 = 0.10678313672542572 + 100.0 * 6.231349945068359
Epoch 1460, val loss: 1.2172120809555054
Epoch 1470, training loss: 623.2822875976562 = 0.10395622253417969 + 100.0 * 6.231783390045166
Epoch 1470, val loss: 1.2225004434585571
Epoch 1480, training loss: 624.0941772460938 = 0.10121957212686539 + 100.0 * 6.239929676055908
Epoch 1480, val loss: 1.2278403043746948
Epoch 1490, training loss: 623.3765869140625 = 0.09848503023386002 + 100.0 * 6.232780933380127
Epoch 1490, val loss: 1.2331217527389526
Epoch 1500, training loss: 623.0958862304688 = 0.09590253978967667 + 100.0 * 6.230000019073486
Epoch 1500, val loss: 1.2385369539260864
Epoch 1510, training loss: 623.158447265625 = 0.09340813755989075 + 100.0 * 6.230650424957275
Epoch 1510, val loss: 1.2439802885055542
Epoch 1520, training loss: 623.3189086914062 = 0.09097559005022049 + 100.0 * 6.232278823852539
Epoch 1520, val loss: 1.2492879629135132
Epoch 1530, training loss: 623.044189453125 = 0.08859270066022873 + 100.0 * 6.229555606842041
Epoch 1530, val loss: 1.2545896768569946
Epoch 1540, training loss: 622.9963989257812 = 0.08631043136119843 + 100.0 * 6.229101181030273
Epoch 1540, val loss: 1.2599931955337524
Epoch 1550, training loss: 623.2415161132812 = 0.08410115540027618 + 100.0 * 6.231574058532715
Epoch 1550, val loss: 1.2653710842132568
Epoch 1560, training loss: 623.5679321289062 = 0.08194834738969803 + 100.0 * 6.234859466552734
Epoch 1560, val loss: 1.2709579467773438
Epoch 1570, training loss: 622.93115234375 = 0.07983110100030899 + 100.0 * 6.228513240814209
Epoch 1570, val loss: 1.2760334014892578
Epoch 1580, training loss: 622.8026123046875 = 0.07780016213655472 + 100.0 * 6.227247714996338
Epoch 1580, val loss: 1.2815051078796387
Epoch 1590, training loss: 622.7201538085938 = 0.07585407793521881 + 100.0 * 6.226442813873291
Epoch 1590, val loss: 1.286933183670044
Epoch 1600, training loss: 623.1661376953125 = 0.07397069036960602 + 100.0 * 6.230921745300293
Epoch 1600, val loss: 1.2924132347106934
Epoch 1610, training loss: 622.8362426757812 = 0.07210542261600494 + 100.0 * 6.227641582489014
Epoch 1610, val loss: 1.2973434925079346
Epoch 1620, training loss: 622.6925048828125 = 0.07031390070915222 + 100.0 * 6.226222038269043
Epoch 1620, val loss: 1.3028656244277954
Epoch 1630, training loss: 622.6249389648438 = 0.06858497858047485 + 100.0 * 6.2255635261535645
Epoch 1630, val loss: 1.3080934286117554
Epoch 1640, training loss: 623.2151489257812 = 0.0669364184141159 + 100.0 * 6.231482028961182
Epoch 1640, val loss: 1.3135274648666382
Epoch 1650, training loss: 622.6445922851562 = 0.06527111679315567 + 100.0 * 6.225793361663818
Epoch 1650, val loss: 1.3187381029129028
Epoch 1660, training loss: 622.5161743164062 = 0.06368657201528549 + 100.0 * 6.224525451660156
Epoch 1660, val loss: 1.3240387439727783
Epoch 1670, training loss: 622.494873046875 = 0.06216245889663696 + 100.0 * 6.224327087402344
Epoch 1670, val loss: 1.3292571306228638
Epoch 1680, training loss: 622.860595703125 = 0.0606912337243557 + 100.0 * 6.227999210357666
Epoch 1680, val loss: 1.3344004154205322
Epoch 1690, training loss: 622.5496826171875 = 0.0592338927090168 + 100.0 * 6.224905014038086
Epoch 1690, val loss: 1.3396848440170288
Epoch 1700, training loss: 622.5938110351562 = 0.057814616709947586 + 100.0 * 6.225359916687012
Epoch 1700, val loss: 1.3447370529174805
Epoch 1710, training loss: 622.4445190429688 = 0.05646044760942459 + 100.0 * 6.223880767822266
Epoch 1710, val loss: 1.3500850200653076
Epoch 1720, training loss: 622.34814453125 = 0.05515972524881363 + 100.0 * 6.22292947769165
Epoch 1720, val loss: 1.3552366495132446
Epoch 1730, training loss: 622.6859130859375 = 0.05390521138906479 + 100.0 * 6.226320266723633
Epoch 1730, val loss: 1.3604001998901367
Epoch 1740, training loss: 622.3363037109375 = 0.05265790596604347 + 100.0 * 6.222836494445801
Epoch 1740, val loss: 1.3655600547790527
Epoch 1750, training loss: 622.2991943359375 = 0.051458075642585754 + 100.0 * 6.222477436065674
Epoch 1750, val loss: 1.370728611946106
Epoch 1760, training loss: 622.3505859375 = 0.050305962562561035 + 100.0 * 6.223002910614014
Epoch 1760, val loss: 1.375810980796814
Epoch 1770, training loss: 622.807373046875 = 0.0491916760802269 + 100.0 * 6.22758150100708
Epoch 1770, val loss: 1.3808362483978271
Epoch 1780, training loss: 622.5184936523438 = 0.04807811230421066 + 100.0 * 6.224704265594482
Epoch 1780, val loss: 1.3855774402618408
Epoch 1790, training loss: 622.6932983398438 = 0.047022271901369095 + 100.0 * 6.2264628410339355
Epoch 1790, val loss: 1.3907780647277832
Epoch 1800, training loss: 622.325439453125 = 0.04596599191427231 + 100.0 * 6.222794532775879
Epoch 1800, val loss: 1.3956865072250366
Epoch 1810, training loss: 622.2225341796875 = 0.044962670654058456 + 100.0 * 6.221775531768799
Epoch 1810, val loss: 1.400611162185669
Epoch 1820, training loss: 622.075927734375 = 0.04399979114532471 + 100.0 * 6.2203192710876465
Epoch 1820, val loss: 1.4055404663085938
Epoch 1830, training loss: 622.0941772460938 = 0.043070804327726364 + 100.0 * 6.220510959625244
Epoch 1830, val loss: 1.4104326963424683
Epoch 1840, training loss: 623.118896484375 = 0.0421745739877224 + 100.0 * 6.230767250061035
Epoch 1840, val loss: 1.4151490926742554
Epoch 1850, training loss: 622.4121704101562 = 0.04125327616930008 + 100.0 * 6.2237091064453125
Epoch 1850, val loss: 1.4201241731643677
Epoch 1860, training loss: 622.1727294921875 = 0.04038257151842117 + 100.0 * 6.221323490142822
Epoch 1860, val loss: 1.424742579460144
Epoch 1870, training loss: 622.4064331054688 = 0.039543185383081436 + 100.0 * 6.223668575286865
Epoch 1870, val loss: 1.4293112754821777
Epoch 1880, training loss: 622.06298828125 = 0.03872654214501381 + 100.0 * 6.220242977142334
Epoch 1880, val loss: 1.4340410232543945
Epoch 1890, training loss: 622.3551025390625 = 0.037946537137031555 + 100.0 * 6.223171234130859
Epoch 1890, val loss: 1.4387954473495483
Epoch 1900, training loss: 622.0499267578125 = 0.037165503948926926 + 100.0 * 6.220127582550049
Epoch 1900, val loss: 1.443570613861084
Epoch 1910, training loss: 621.9304809570312 = 0.036419086158275604 + 100.0 * 6.218940258026123
Epoch 1910, val loss: 1.4481922388076782
Epoch 1920, training loss: 621.9379272460938 = 0.03570348396897316 + 100.0 * 6.219022274017334
Epoch 1920, val loss: 1.4527239799499512
Epoch 1930, training loss: 622.42041015625 = 0.0350114107131958 + 100.0 * 6.223853588104248
Epoch 1930, val loss: 1.457499384880066
Epoch 1940, training loss: 622.0741577148438 = 0.03430568054318428 + 100.0 * 6.220398426055908
Epoch 1940, val loss: 1.4618499279022217
Epoch 1950, training loss: 621.9777221679688 = 0.03362966328859329 + 100.0 * 6.219440937042236
Epoch 1950, val loss: 1.4661188125610352
Epoch 1960, training loss: 622.211181640625 = 0.03298559784889221 + 100.0 * 6.221782207489014
Epoch 1960, val loss: 1.4706227779388428
Epoch 1970, training loss: 621.81494140625 = 0.032343994826078415 + 100.0 * 6.217825889587402
Epoch 1970, val loss: 1.4749023914337158
Epoch 1980, training loss: 621.8478393554688 = 0.03173264488577843 + 100.0 * 6.218161106109619
Epoch 1980, val loss: 1.4793932437896729
Epoch 1990, training loss: 621.8501586914062 = 0.031141821295022964 + 100.0 * 6.2181901931762695
Epoch 1990, val loss: 1.483662486076355
Epoch 2000, training loss: 622.091064453125 = 0.03056504763662815 + 100.0 * 6.22060489654541
Epoch 2000, val loss: 1.487923502922058
Epoch 2010, training loss: 621.9352416992188 = 0.029993215575814247 + 100.0 * 6.219052791595459
Epoch 2010, val loss: 1.4925371408462524
Epoch 2020, training loss: 622.1441040039062 = 0.029445834457874298 + 100.0 * 6.221147060394287
Epoch 2020, val loss: 1.496456265449524
Epoch 2030, training loss: 621.8057861328125 = 0.02889202907681465 + 100.0 * 6.217769145965576
Epoch 2030, val loss: 1.5008450746536255
Epoch 2040, training loss: 621.7034301757812 = 0.028368204832077026 + 100.0 * 6.216750621795654
Epoch 2040, val loss: 1.504900574684143
Epoch 2050, training loss: 621.609619140625 = 0.02786354534327984 + 100.0 * 6.215817451477051
Epoch 2050, val loss: 1.5090429782867432
Epoch 2060, training loss: 621.6551513671875 = 0.027379896491765976 + 100.0 * 6.216277599334717
Epoch 2060, val loss: 1.513145923614502
Epoch 2070, training loss: 622.4071044921875 = 0.02690301090478897 + 100.0 * 6.223801612854004
Epoch 2070, val loss: 1.5170842409133911
Epoch 2080, training loss: 622.4392700195312 = 0.02641809731721878 + 100.0 * 6.224128246307373
Epoch 2080, val loss: 1.5210821628570557
Epoch 2090, training loss: 621.6378784179688 = 0.025952141731977463 + 100.0 * 6.216119289398193
Epoch 2090, val loss: 1.5251270532608032
Epoch 2100, training loss: 621.52978515625 = 0.025505661964416504 + 100.0 * 6.215042591094971
Epoch 2100, val loss: 1.5292260646820068
Epoch 2110, training loss: 621.4807739257812 = 0.02507304958999157 + 100.0 * 6.214556694030762
Epoch 2110, val loss: 1.5332024097442627
Epoch 2120, training loss: 621.652587890625 = 0.0246599018573761 + 100.0 * 6.216279029846191
Epoch 2120, val loss: 1.5373291969299316
Epoch 2130, training loss: 622.1986083984375 = 0.024251600727438927 + 100.0 * 6.221743583679199
Epoch 2130, val loss: 1.5408109426498413
Epoch 2140, training loss: 621.5142211914062 = 0.023825567215681076 + 100.0 * 6.214904308319092
Epoch 2140, val loss: 1.544909954071045
Epoch 2150, training loss: 621.4434814453125 = 0.02342943847179413 + 100.0 * 6.214200496673584
Epoch 2150, val loss: 1.5485708713531494
Epoch 2160, training loss: 621.3961791992188 = 0.023050544783473015 + 100.0 * 6.213730812072754
Epoch 2160, val loss: 1.552537441253662
Epoch 2170, training loss: 621.7681274414062 = 0.02268996462225914 + 100.0 * 6.217454433441162
Epoch 2170, val loss: 1.5565048456192017
Epoch 2180, training loss: 621.4368896484375 = 0.02230912260711193 + 100.0 * 6.214145660400391
Epoch 2180, val loss: 1.5598422288894653
Epoch 2190, training loss: 621.3814697265625 = 0.021940764039754868 + 100.0 * 6.213595867156982
Epoch 2190, val loss: 1.5637518167495728
Epoch 2200, training loss: 621.3409423828125 = 0.02159169875085354 + 100.0 * 6.213193416595459
Epoch 2200, val loss: 1.5671498775482178
Epoch 2210, training loss: 621.2879638671875 = 0.02125832624733448 + 100.0 * 6.212666988372803
Epoch 2210, val loss: 1.5709959268569946
Epoch 2220, training loss: 621.4940795898438 = 0.02093513309955597 + 100.0 * 6.214731216430664
Epoch 2220, val loss: 1.574733853340149
Epoch 2230, training loss: 621.4080810546875 = 0.020608939230442047 + 100.0 * 6.213874340057373
Epoch 2230, val loss: 1.578071117401123
Epoch 2240, training loss: 621.4251708984375 = 0.02028491348028183 + 100.0 * 6.214048862457275
Epoch 2240, val loss: 1.5817608833312988
Epoch 2250, training loss: 621.3673095703125 = 0.019974641501903534 + 100.0 * 6.213473796844482
Epoch 2250, val loss: 1.5850458145141602
Epoch 2260, training loss: 621.27783203125 = 0.019674601033329964 + 100.0 * 6.212581634521484
Epoch 2260, val loss: 1.5886071920394897
Epoch 2270, training loss: 622.1632690429688 = 0.01939081773161888 + 100.0 * 6.221438884735107
Epoch 2270, val loss: 1.5920815467834473
Epoch 2280, training loss: 621.3904418945312 = 0.019089078530669212 + 100.0 * 6.213713645935059
Epoch 2280, val loss: 1.5953999757766724
Epoch 2290, training loss: 621.2109375 = 0.018806621432304382 + 100.0 * 6.211921215057373
Epoch 2290, val loss: 1.5989243984222412
Epoch 2300, training loss: 621.2830200195312 = 0.018536925315856934 + 100.0 * 6.212644577026367
Epoch 2300, val loss: 1.602219820022583
Epoch 2310, training loss: 621.8697509765625 = 0.018276825547218323 + 100.0 * 6.218514919281006
Epoch 2310, val loss: 1.6055700778961182
Epoch 2320, training loss: 621.3701171875 = 0.017998894676566124 + 100.0 * 6.2135210037231445
Epoch 2320, val loss: 1.6088021993637085
Epoch 2330, training loss: 621.1862182617188 = 0.01774335280060768 + 100.0 * 6.211684703826904
Epoch 2330, val loss: 1.612093448638916
Epoch 2340, training loss: 621.3121337890625 = 0.017495255917310715 + 100.0 * 6.212945938110352
Epoch 2340, val loss: 1.6152560710906982
Epoch 2350, training loss: 621.256103515625 = 0.017249474301934242 + 100.0 * 6.212388515472412
Epoch 2350, val loss: 1.6186878681182861
Epoch 2360, training loss: 621.5589599609375 = 0.017011398449540138 + 100.0 * 6.215419292449951
Epoch 2360, val loss: 1.6220978498458862
Epoch 2370, training loss: 621.1987915039062 = 0.016768259927630424 + 100.0 * 6.211820125579834
Epoch 2370, val loss: 1.6249269247055054
Epoch 2380, training loss: 621.2446899414062 = 0.01653880812227726 + 100.0 * 6.212281227111816
Epoch 2380, val loss: 1.6281437873840332
Epoch 2390, training loss: 621.293701171875 = 0.016314109787344933 + 100.0 * 6.21277379989624
Epoch 2390, val loss: 1.6312857866287231
Epoch 2400, training loss: 621.0650634765625 = 0.016094699501991272 + 100.0 * 6.210489273071289
Epoch 2400, val loss: 1.6343592405319214
Epoch 2410, training loss: 621.38916015625 = 0.015883807092905045 + 100.0 * 6.213732719421387
Epoch 2410, val loss: 1.637366771697998
Epoch 2420, training loss: 621.3643798828125 = 0.01567205786705017 + 100.0 * 6.213487148284912
Epoch 2420, val loss: 1.6404868364334106
Epoch 2430, training loss: 621.002685546875 = 0.01545347273349762 + 100.0 * 6.209872722625732
Epoch 2430, val loss: 1.643324613571167
Epoch 2440, training loss: 620.953369140625 = 0.015250377357006073 + 100.0 * 6.209381103515625
Epoch 2440, val loss: 1.646406888961792
Epoch 2450, training loss: 620.8994750976562 = 0.015055940486490726 + 100.0 * 6.20884370803833
Epoch 2450, val loss: 1.6494735479354858
Epoch 2460, training loss: 621.2654418945312 = 0.0148710822686553 + 100.0 * 6.212505340576172
Epoch 2460, val loss: 1.6524296998977661
Epoch 2470, training loss: 621.0541381835938 = 0.014675016514956951 + 100.0 * 6.210394859313965
Epoch 2470, val loss: 1.6551710367202759
Epoch 2480, training loss: 621.2949829101562 = 0.014488471671938896 + 100.0 * 6.212805271148682
Epoch 2480, val loss: 1.6579952239990234
Epoch 2490, training loss: 620.97314453125 = 0.014299621805548668 + 100.0 * 6.209588527679443
Epoch 2490, val loss: 1.660980463027954
Epoch 2500, training loss: 620.96435546875 = 0.014119302853941917 + 100.0 * 6.209502696990967
Epoch 2500, val loss: 1.6636971235275269
Epoch 2510, training loss: 620.8954467773438 = 0.013947511091828346 + 100.0 * 6.20881462097168
Epoch 2510, val loss: 1.6666483879089355
Epoch 2520, training loss: 621.28271484375 = 0.013784635812044144 + 100.0 * 6.212689399719238
Epoch 2520, val loss: 1.6693506240844727
Epoch 2530, training loss: 620.9603271484375 = 0.013609183952212334 + 100.0 * 6.20946741104126
Epoch 2530, val loss: 1.671947717666626
Epoch 2540, training loss: 621.1581420898438 = 0.013441003859043121 + 100.0 * 6.211447238922119
Epoch 2540, val loss: 1.6743237972259521
Epoch 2550, training loss: 621.0465698242188 = 0.013278498314321041 + 100.0 * 6.210332870483398
Epoch 2550, val loss: 1.677384853363037
Epoch 2560, training loss: 620.8645629882812 = 0.013118335977196693 + 100.0 * 6.208514213562012
Epoch 2560, val loss: 1.6802719831466675
Epoch 2570, training loss: 620.9210205078125 = 0.012966413982212543 + 100.0 * 6.209080696105957
Epoch 2570, val loss: 1.6828656196594238
Epoch 2580, training loss: 621.20703125 = 0.012819342315196991 + 100.0 * 6.211942195892334
Epoch 2580, val loss: 1.6855032444000244
Epoch 2590, training loss: 620.8441162109375 = 0.012664157897233963 + 100.0 * 6.208314895629883
Epoch 2590, val loss: 1.687997817993164
Epoch 2600, training loss: 620.7553100585938 = 0.012517964467406273 + 100.0 * 6.207427978515625
Epoch 2600, val loss: 1.690527081489563
Epoch 2610, training loss: 620.8399658203125 = 0.012379204854369164 + 100.0 * 6.20827579498291
Epoch 2610, val loss: 1.6930158138275146
Epoch 2620, training loss: 621.6500244140625 = 0.012239930219948292 + 100.0 * 6.2163777351379395
Epoch 2620, val loss: 1.695462703704834
Epoch 2630, training loss: 620.9949951171875 = 0.01209429930895567 + 100.0 * 6.209829330444336
Epoch 2630, val loss: 1.6985032558441162
Epoch 2640, training loss: 620.8434448242188 = 0.01195493619889021 + 100.0 * 6.208314895629883
Epoch 2640, val loss: 1.7008205652236938
Epoch 2650, training loss: 620.7791748046875 = 0.011822737753391266 + 100.0 * 6.2076735496521
Epoch 2650, val loss: 1.70344078540802
Epoch 2660, training loss: 620.7271728515625 = 0.011692458763718605 + 100.0 * 6.207154273986816
Epoch 2660, val loss: 1.7058515548706055
Epoch 2670, training loss: 620.8184814453125 = 0.011568238027393818 + 100.0 * 6.20806884765625
Epoch 2670, val loss: 1.7083839178085327
Epoch 2680, training loss: 620.902099609375 = 0.011443741619586945 + 100.0 * 6.208907127380371
Epoch 2680, val loss: 1.7106726169586182
Epoch 2690, training loss: 621.207763671875 = 0.011317499913275242 + 100.0 * 6.2119646072387695
Epoch 2690, val loss: 1.712833046913147
Epoch 2700, training loss: 620.6983032226562 = 0.011190132237970829 + 100.0 * 6.206871509552002
Epoch 2700, val loss: 1.715239405632019
Epoch 2710, training loss: 620.5948486328125 = 0.011068092659115791 + 100.0 * 6.205838203430176
Epoch 2710, val loss: 1.7175748348236084
Epoch 2720, training loss: 620.5361938476562 = 0.01095249317586422 + 100.0 * 6.205252647399902
Epoch 2720, val loss: 1.7199177742004395
Epoch 2730, training loss: 620.5399780273438 = 0.010842720046639442 + 100.0 * 6.205291271209717
Epoch 2730, val loss: 1.7223109006881714
Epoch 2740, training loss: 620.755126953125 = 0.01073526032269001 + 100.0 * 6.207443714141846
Epoch 2740, val loss: 1.7244491577148438
Epoch 2750, training loss: 620.9618530273438 = 0.010623536072671413 + 100.0 * 6.209512233734131
Epoch 2750, val loss: 1.726938009262085
Epoch 2760, training loss: 620.5951538085938 = 0.010508609004318714 + 100.0 * 6.205846786499023
Epoch 2760, val loss: 1.729000210762024
Epoch 2770, training loss: 620.5186767578125 = 0.010401589795947075 + 100.0 * 6.205082893371582
Epoch 2770, val loss: 1.7312475442886353
Epoch 2780, training loss: 620.7562866210938 = 0.010299808345735073 + 100.0 * 6.207459449768066
Epoch 2780, val loss: 1.7334867715835571
Epoch 2790, training loss: 620.821044921875 = 0.010196473449468613 + 100.0 * 6.208108425140381
Epoch 2790, val loss: 1.7354307174682617
Epoch 2800, training loss: 620.620361328125 = 0.010093781165778637 + 100.0 * 6.2061028480529785
Epoch 2800, val loss: 1.7378807067871094
Epoch 2810, training loss: 620.7386474609375 = 0.009994865395128727 + 100.0 * 6.207286357879639
Epoch 2810, val loss: 1.7399581670761108
Epoch 2820, training loss: 620.571044921875 = 0.009896278381347656 + 100.0 * 6.205611705780029
Epoch 2820, val loss: 1.7421391010284424
Epoch 2830, training loss: 620.4688720703125 = 0.009800678119063377 + 100.0 * 6.204590320587158
Epoch 2830, val loss: 1.7445074319839478
Epoch 2840, training loss: 620.7625122070312 = 0.009708377532660961 + 100.0 * 6.207528114318848
Epoch 2840, val loss: 1.7466926574707031
Epoch 2850, training loss: 620.5865478515625 = 0.009615116752684116 + 100.0 * 6.205769062042236
Epoch 2850, val loss: 1.7486838102340698
Epoch 2860, training loss: 620.6004028320312 = 0.00952435564249754 + 100.0 * 6.20590877532959
Epoch 2860, val loss: 1.7503283023834229
Epoch 2870, training loss: 620.8373413085938 = 0.009434244595468044 + 100.0 * 6.208278656005859
Epoch 2870, val loss: 1.752526044845581
Epoch 2880, training loss: 620.620849609375 = 0.009343144483864307 + 100.0 * 6.206115245819092
Epoch 2880, val loss: 1.7542539834976196
Epoch 2890, training loss: 620.3947143554688 = 0.009254900738596916 + 100.0 * 6.203854084014893
Epoch 2890, val loss: 1.7567719221115112
Epoch 2900, training loss: 620.3631591796875 = 0.00917043350636959 + 100.0 * 6.203540325164795
Epoch 2900, val loss: 1.758825421333313
Epoch 2910, training loss: 620.427978515625 = 0.009091120213270187 + 100.0 * 6.204188823699951
Epoch 2910, val loss: 1.7608245611190796
Epoch 2920, training loss: 621.0725708007812 = 0.00901290774345398 + 100.0 * 6.210635662078857
Epoch 2920, val loss: 1.762698769569397
Epoch 2930, training loss: 620.5889892578125 = 0.008923303335905075 + 100.0 * 6.205800533294678
Epoch 2930, val loss: 1.7646303176879883
Epoch 2940, training loss: 620.3805541992188 = 0.008844713680446148 + 100.0 * 6.203717231750488
Epoch 2940, val loss: 1.766416072845459
Epoch 2950, training loss: 620.4221801757812 = 0.008767654187977314 + 100.0 * 6.204133987426758
Epoch 2950, val loss: 1.7683075666427612
Epoch 2960, training loss: 620.5771484375 = 0.008690535090863705 + 100.0 * 6.205684661865234
Epoch 2960, val loss: 1.7702891826629639
Epoch 2970, training loss: 620.7637939453125 = 0.008612885139882565 + 100.0 * 6.207551956176758
Epoch 2970, val loss: 1.7717187404632568
Epoch 2980, training loss: 620.477294921875 = 0.0085399579256773 + 100.0 * 6.204687595367432
Epoch 2980, val loss: 1.7741533517837524
Epoch 2990, training loss: 620.2902221679688 = 0.008462604135274887 + 100.0 * 6.202817440032959
Epoch 2990, val loss: 1.7759885787963867
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6296296296296297
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 861.6223754882812 = 1.9357339143753052 + 100.0 * 8.596866607666016
Epoch 0, val loss: 1.930860996246338
Epoch 10, training loss: 861.5469970703125 = 1.927342176437378 + 100.0 * 8.596196174621582
Epoch 10, val loss: 1.9224663972854614
Epoch 20, training loss: 861.05126953125 = 1.9169762134552002 + 100.0 * 8.59134292602539
Epoch 20, val loss: 1.9122728109359741
Epoch 30, training loss: 857.8012084960938 = 1.90348482131958 + 100.0 * 8.558977127075195
Epoch 30, val loss: 1.8990291357040405
Epoch 40, training loss: 839.6665649414062 = 1.8859751224517822 + 100.0 * 8.377805709838867
Epoch 40, val loss: 1.8821357488632202
Epoch 50, training loss: 771.9606323242188 = 1.8649226427078247 + 100.0 * 7.70095682144165
Epoch 50, val loss: 1.8615226745605469
Epoch 60, training loss: 740.9988403320312 = 1.8475486040115356 + 100.0 * 7.391512870788574
Epoch 60, val loss: 1.8452688455581665
Epoch 70, training loss: 716.4017944335938 = 1.8352270126342773 + 100.0 * 7.145666122436523
Epoch 70, val loss: 1.8329310417175293
Epoch 80, training loss: 699.0517578125 = 1.8239606618881226 + 100.0 * 6.972277641296387
Epoch 80, val loss: 1.8224713802337646
Epoch 90, training loss: 691.4503173828125 = 1.813692331314087 + 100.0 * 6.896366119384766
Epoch 90, val loss: 1.813043236732483
Epoch 100, training loss: 684.7943115234375 = 1.8032259941101074 + 100.0 * 6.829910755157471
Epoch 100, val loss: 1.8037532567977905
Epoch 110, training loss: 679.3007202148438 = 1.7937170267105103 + 100.0 * 6.7750701904296875
Epoch 110, val loss: 1.7951017618179321
Epoch 120, training loss: 675.2318115234375 = 1.7843942642211914 + 100.0 * 6.734474182128906
Epoch 120, val loss: 1.7865655422210693
Epoch 130, training loss: 670.31494140625 = 1.7745791673660278 + 100.0 * 6.685403347015381
Epoch 130, val loss: 1.7778980731964111
Epoch 140, training loss: 665.7030639648438 = 1.7649445533752441 + 100.0 * 6.639381408691406
Epoch 140, val loss: 1.7696646451950073
Epoch 150, training loss: 661.9874877929688 = 1.7549185752868652 + 100.0 * 6.602325439453125
Epoch 150, val loss: 1.761052131652832
Epoch 160, training loss: 658.6534423828125 = 1.7441399097442627 + 100.0 * 6.569092750549316
Epoch 160, val loss: 1.7518937587738037
Epoch 170, training loss: 656.3163452148438 = 1.7320235967636108 + 100.0 * 6.545843601226807
Epoch 170, val loss: 1.7414357662200928
Epoch 180, training loss: 654.1732177734375 = 1.7179092168807983 + 100.0 * 6.524553298950195
Epoch 180, val loss: 1.7294952869415283
Epoch 190, training loss: 652.19091796875 = 1.7025203704833984 + 100.0 * 6.504884243011475
Epoch 190, val loss: 1.716417670249939
Epoch 200, training loss: 650.5967407226562 = 1.6859492063522339 + 100.0 * 6.489107608795166
Epoch 200, val loss: 1.7024507522583008
Epoch 210, training loss: 649.69873046875 = 1.667970895767212 + 100.0 * 6.480307579040527
Epoch 210, val loss: 1.687414526939392
Epoch 220, training loss: 648.014892578125 = 1.6482768058776855 + 100.0 * 6.4636664390563965
Epoch 220, val loss: 1.671054720878601
Epoch 230, training loss: 646.89501953125 = 1.6273620128631592 + 100.0 * 6.452676296234131
Epoch 230, val loss: 1.6539989709854126
Epoch 240, training loss: 645.7785034179688 = 1.6052751541137695 + 100.0 * 6.441731929779053
Epoch 240, val loss: 1.6361470222473145
Epoch 250, training loss: 644.97216796875 = 1.5821080207824707 + 100.0 * 6.433900833129883
Epoch 250, val loss: 1.6175620555877686
Epoch 260, training loss: 644.0113525390625 = 1.5575966835021973 + 100.0 * 6.424537658691406
Epoch 260, val loss: 1.5982998609542847
Epoch 270, training loss: 642.9741821289062 = 1.5323116779327393 + 100.0 * 6.414418697357178
Epoch 270, val loss: 1.5790380239486694
Epoch 280, training loss: 642.197021484375 = 1.506384015083313 + 100.0 * 6.4069061279296875
Epoch 280, val loss: 1.559415578842163
Epoch 290, training loss: 642.5332641601562 = 1.480040431022644 + 100.0 * 6.410532474517822
Epoch 290, val loss: 1.539892315864563
Epoch 300, training loss: 640.96728515625 = 1.4528608322143555 + 100.0 * 6.395143985748291
Epoch 300, val loss: 1.5201339721679688
Epoch 310, training loss: 640.1615600585938 = 1.4257620573043823 + 100.0 * 6.387357711791992
Epoch 310, val loss: 1.5008463859558105
Epoch 320, training loss: 639.4649047851562 = 1.398659586906433 + 100.0 * 6.380662441253662
Epoch 320, val loss: 1.4821280241012573
Epoch 330, training loss: 638.8339233398438 = 1.3714799880981445 + 100.0 * 6.374624729156494
Epoch 330, val loss: 1.463856816291809
Epoch 340, training loss: 638.6076049804688 = 1.3443132638931274 + 100.0 * 6.37263298034668
Epoch 340, val loss: 1.4459805488586426
Epoch 350, training loss: 637.9110717773438 = 1.3169848918914795 + 100.0 * 6.365941047668457
Epoch 350, val loss: 1.4283171892166138
Epoch 360, training loss: 637.2574462890625 = 1.289707899093628 + 100.0 * 6.359676837921143
Epoch 360, val loss: 1.411072015762329
Epoch 370, training loss: 636.8137817382812 = 1.2624315023422241 + 100.0 * 6.355513572692871
Epoch 370, val loss: 1.3941960334777832
Epoch 380, training loss: 636.8734130859375 = 1.2352010011672974 + 100.0 * 6.356381893157959
Epoch 380, val loss: 1.3775368928909302
Epoch 390, training loss: 636.0203857421875 = 1.207792043685913 + 100.0 * 6.348126411437988
Epoch 390, val loss: 1.3611708879470825
Epoch 400, training loss: 635.6403198242188 = 1.1806056499481201 + 100.0 * 6.344597339630127
Epoch 400, val loss: 1.3451629877090454
Epoch 410, training loss: 635.7749633789062 = 1.1535046100616455 + 100.0 * 6.346214294433594
Epoch 410, val loss: 1.329254388809204
Epoch 420, training loss: 635.0535278320312 = 1.1265108585357666 + 100.0 * 6.339270114898682
Epoch 420, val loss: 1.3141579627990723
Epoch 430, training loss: 634.6000366210938 = 1.099846363067627 + 100.0 * 6.3350019454956055
Epoch 430, val loss: 1.2993477582931519
Epoch 440, training loss: 634.3642578125 = 1.0735927820205688 + 100.0 * 6.332906723022461
Epoch 440, val loss: 1.2848163843154907
Epoch 450, training loss: 634.16552734375 = 1.0475369691848755 + 100.0 * 6.331180095672607
Epoch 450, val loss: 1.2710156440734863
Epoch 460, training loss: 633.8007202148438 = 1.022117257118225 + 100.0 * 6.327786445617676
Epoch 460, val loss: 1.2577850818634033
Epoch 470, training loss: 633.4012451171875 = 0.9972506165504456 + 100.0 * 6.324039936065674
Epoch 470, val loss: 1.2451417446136475
Epoch 480, training loss: 633.1502685546875 = 0.9730936288833618 + 100.0 * 6.32177209854126
Epoch 480, val loss: 1.233254313468933
Epoch 490, training loss: 633.5155029296875 = 0.9495283365249634 + 100.0 * 6.32565975189209
Epoch 490, val loss: 1.2219603061676025
Epoch 500, training loss: 632.643310546875 = 0.9264384508132935 + 100.0 * 6.317168712615967
Epoch 500, val loss: 1.2112315893173218
Epoch 510, training loss: 632.3773193359375 = 0.9041799902915955 + 100.0 * 6.314731597900391
Epoch 510, val loss: 1.2011841535568237
Epoch 520, training loss: 632.1399536132812 = 0.8826215863227844 + 100.0 * 6.312573432922363
Epoch 520, val loss: 1.192016363143921
Epoch 530, training loss: 632.3425903320312 = 0.8617987036705017 + 100.0 * 6.314807891845703
Epoch 530, val loss: 1.1833410263061523
Epoch 540, training loss: 632.0556030273438 = 0.8413243293762207 + 100.0 * 6.312142848968506
Epoch 540, val loss: 1.1754701137542725
Epoch 550, training loss: 631.4766845703125 = 0.8216221332550049 + 100.0 * 6.306550979614258
Epoch 550, val loss: 1.1679518222808838
Epoch 560, training loss: 631.4554443359375 = 0.802590012550354 + 100.0 * 6.306528568267822
Epoch 560, val loss: 1.1609331369400024
Epoch 570, training loss: 631.0801391601562 = 0.78395676612854 + 100.0 * 6.302962303161621
Epoch 570, val loss: 1.1550413370132446
Epoch 580, training loss: 630.935791015625 = 0.7659559845924377 + 100.0 * 6.301698684692383
Epoch 580, val loss: 1.1489442586898804
Epoch 590, training loss: 630.704833984375 = 0.7484725713729858 + 100.0 * 6.299563884735107
Epoch 590, val loss: 1.1438859701156616
Epoch 600, training loss: 630.6157836914062 = 0.7313254475593567 + 100.0 * 6.298844814300537
Epoch 600, val loss: 1.1390981674194336
Epoch 610, training loss: 630.4563598632812 = 0.7146632671356201 + 100.0 * 6.297417163848877
Epoch 610, val loss: 1.1343623399734497
Epoch 620, training loss: 630.18896484375 = 0.6985548734664917 + 100.0 * 6.2949042320251465
Epoch 620, val loss: 1.130307674407959
Epoch 630, training loss: 629.9280395507812 = 0.6829007863998413 + 100.0 * 6.29245138168335
Epoch 630, val loss: 1.1267741918563843
Epoch 640, training loss: 629.8198852539062 = 0.6676468253135681 + 100.0 * 6.291522026062012
Epoch 640, val loss: 1.1238720417022705
Epoch 650, training loss: 629.6734619140625 = 0.6526595950126648 + 100.0 * 6.290207862854004
Epoch 650, val loss: 1.120552897453308
Epoch 660, training loss: 629.4577026367188 = 0.6379778385162354 + 100.0 * 6.288197040557861
Epoch 660, val loss: 1.117869257926941
Epoch 670, training loss: 629.3252563476562 = 0.6237170100212097 + 100.0 * 6.287015438079834
Epoch 670, val loss: 1.1156893968582153
Epoch 680, training loss: 629.1453247070312 = 0.609830915927887 + 100.0 * 6.285355091094971
Epoch 680, val loss: 1.1137386560440063
Epoch 690, training loss: 630.424072265625 = 0.5960928797721863 + 100.0 * 6.298279762268066
Epoch 690, val loss: 1.112151861190796
Epoch 700, training loss: 629.4220581054688 = 0.5826327800750732 + 100.0 * 6.288394451141357
Epoch 700, val loss: 1.1100589036941528
Epoch 710, training loss: 628.8568115234375 = 0.5695234537124634 + 100.0 * 6.282873153686523
Epoch 710, val loss: 1.1087939739227295
Epoch 720, training loss: 628.6250610351562 = 0.5567812919616699 + 100.0 * 6.2806830406188965
Epoch 720, val loss: 1.1080138683319092
Epoch 730, training loss: 628.4614868164062 = 0.5443353652954102 + 100.0 * 6.279171466827393
Epoch 730, val loss: 1.1073224544525146
Epoch 740, training loss: 628.351806640625 = 0.5321218371391296 + 100.0 * 6.278196811676025
Epoch 740, val loss: 1.107003092765808
Epoch 750, training loss: 628.8634033203125 = 0.5199936032295227 + 100.0 * 6.28343391418457
Epoch 750, val loss: 1.1067750453948975
Epoch 760, training loss: 628.4248046875 = 0.5081412196159363 + 100.0 * 6.2791666984558105
Epoch 760, val loss: 1.1064784526824951
Epoch 770, training loss: 628.0404052734375 = 0.496512770652771 + 100.0 * 6.2754387855529785
Epoch 770, val loss: 1.1064231395721436
Epoch 780, training loss: 627.8658447265625 = 0.48517200350761414 + 100.0 * 6.273806571960449
Epoch 780, val loss: 1.1068410873413086
Epoch 790, training loss: 628.133056640625 = 0.4740273356437683 + 100.0 * 6.276590824127197
Epoch 790, val loss: 1.1075365543365479
Epoch 800, training loss: 627.6800537109375 = 0.4630507230758667 + 100.0 * 6.272169589996338
Epoch 800, val loss: 1.1079546213150024
Epoch 810, training loss: 627.55322265625 = 0.45226508378982544 + 100.0 * 6.27100944519043
Epoch 810, val loss: 1.1089541912078857
Epoch 820, training loss: 627.6157836914062 = 0.4417100250720978 + 100.0 * 6.271740913391113
Epoch 820, val loss: 1.1097099781036377
Epoch 830, training loss: 627.3313598632812 = 0.4313068091869354 + 100.0 * 6.269000053405762
Epoch 830, val loss: 1.1112728118896484
Epoch 840, training loss: 627.3049926757812 = 0.42108505964279175 + 100.0 * 6.268839359283447
Epoch 840, val loss: 1.1122969388961792
Epoch 850, training loss: 627.5097045898438 = 0.4110608994960785 + 100.0 * 6.270986080169678
Epoch 850, val loss: 1.1140815019607544
Epoch 860, training loss: 627.3729248046875 = 0.40119361877441406 + 100.0 * 6.269717216491699
Epoch 860, val loss: 1.1156460046768188
Epoch 870, training loss: 627.065185546875 = 0.3914501667022705 + 100.0 * 6.26673698425293
Epoch 870, val loss: 1.1176507472991943
Epoch 880, training loss: 627.0361938476562 = 0.3819405436515808 + 100.0 * 6.266542434692383
Epoch 880, val loss: 1.1198009252548218
Epoch 890, training loss: 626.7578125 = 0.3726060390472412 + 100.0 * 6.263852119445801
Epoch 890, val loss: 1.1216315031051636
Epoch 900, training loss: 627.011474609375 = 0.3634305000305176 + 100.0 * 6.266480445861816
Epoch 900, val loss: 1.1239211559295654
Epoch 910, training loss: 626.7286376953125 = 0.3543904721736908 + 100.0 * 6.263742446899414
Epoch 910, val loss: 1.1261520385742188
Epoch 920, training loss: 627.8219604492188 = 0.3456169664859772 + 100.0 * 6.274763107299805
Epoch 920, val loss: 1.1284027099609375
Epoch 930, training loss: 626.5973510742188 = 0.33667659759521484 + 100.0 * 6.262606620788574
Epoch 930, val loss: 1.1305432319641113
Epoch 940, training loss: 626.3772583007812 = 0.3281917870044708 + 100.0 * 6.260490894317627
Epoch 940, val loss: 1.1332589387893677
Epoch 950, training loss: 626.2205810546875 = 0.3198890686035156 + 100.0 * 6.259006977081299
Epoch 950, val loss: 1.1359795331954956
Epoch 960, training loss: 626.1452026367188 = 0.311769038438797 + 100.0 * 6.258334636688232
Epoch 960, val loss: 1.1389654874801636
Epoch 970, training loss: 627.00341796875 = 0.30381494760513306 + 100.0 * 6.266995906829834
Epoch 970, val loss: 1.1416782140731812
Epoch 980, training loss: 626.2840576171875 = 0.2958572804927826 + 100.0 * 6.25988245010376
Epoch 980, val loss: 1.1449857950210571
Epoch 990, training loss: 626.0006713867188 = 0.28819048404693604 + 100.0 * 6.257124900817871
Epoch 990, val loss: 1.1480562686920166
Epoch 1000, training loss: 626.0781860351562 = 0.2807021141052246 + 100.0 * 6.257974624633789
Epoch 1000, val loss: 1.1512713432312012
Epoch 1010, training loss: 625.8666381835938 = 0.2733856737613678 + 100.0 * 6.255932807922363
Epoch 1010, val loss: 1.1548635959625244
Epoch 1020, training loss: 625.8580932617188 = 0.266250878572464 + 100.0 * 6.255918502807617
Epoch 1020, val loss: 1.158318281173706
Epoch 1030, training loss: 625.7945556640625 = 0.2592293620109558 + 100.0 * 6.2553534507751465
Epoch 1030, val loss: 1.16180419921875
Epoch 1040, training loss: 625.9033813476562 = 0.2524009644985199 + 100.0 * 6.256509780883789
Epoch 1040, val loss: 1.1655875444412231
Epoch 1050, training loss: 625.646240234375 = 0.24571581184864044 + 100.0 * 6.254004955291748
Epoch 1050, val loss: 1.169554591178894
Epoch 1060, training loss: 625.6361694335938 = 0.239204540848732 + 100.0 * 6.253969669342041
Epoch 1060, val loss: 1.173697829246521
Epoch 1070, training loss: 625.4389038085938 = 0.23287200927734375 + 100.0 * 6.252060413360596
Epoch 1070, val loss: 1.1773781776428223
Epoch 1080, training loss: 625.4083862304688 = 0.2267124205827713 + 100.0 * 6.251816749572754
Epoch 1080, val loss: 1.1814934015274048
Epoch 1090, training loss: 625.436767578125 = 0.22071057558059692 + 100.0 * 6.252160549163818
Epoch 1090, val loss: 1.1855065822601318
Epoch 1100, training loss: 625.2288208007812 = 0.21486856043338776 + 100.0 * 6.2501397132873535
Epoch 1100, val loss: 1.1896986961364746
Epoch 1110, training loss: 625.2044677734375 = 0.20916929841041565 + 100.0 * 6.249952793121338
Epoch 1110, val loss: 1.1939927339553833
Epoch 1120, training loss: 625.4905395507812 = 0.20361332595348358 + 100.0 * 6.252869129180908
Epoch 1120, val loss: 1.1983479261398315
Epoch 1130, training loss: 625.115234375 = 0.19816546142101288 + 100.0 * 6.249170303344727
Epoch 1130, val loss: 1.203007698059082
Epoch 1140, training loss: 624.9191284179688 = 0.1929282695055008 + 100.0 * 6.247262001037598
Epoch 1140, val loss: 1.2074490785598755
Epoch 1150, training loss: 625.0589599609375 = 0.1878541260957718 + 100.0 * 6.248711109161377
Epoch 1150, val loss: 1.2122745513916016
Epoch 1160, training loss: 624.9950561523438 = 0.1828618049621582 + 100.0 * 6.248121738433838
Epoch 1160, val loss: 1.2165755033493042
Epoch 1170, training loss: 624.7730712890625 = 0.1779710352420807 + 100.0 * 6.245950698852539
Epoch 1170, val loss: 1.2213715314865112
Epoch 1180, training loss: 624.8817138671875 = 0.17328542470932007 + 100.0 * 6.247084617614746
Epoch 1180, val loss: 1.2260569334030151
Epoch 1190, training loss: 624.8875732421875 = 0.16872051358222961 + 100.0 * 6.247188568115234
Epoch 1190, val loss: 1.2309192419052124
Epoch 1200, training loss: 624.61083984375 = 0.16426238417625427 + 100.0 * 6.2444658279418945
Epoch 1200, val loss: 1.2353860139846802
Epoch 1210, training loss: 624.50048828125 = 0.15995211899280548 + 100.0 * 6.243404865264893
Epoch 1210, val loss: 1.2404025793075562
Epoch 1220, training loss: 624.5634155273438 = 0.1558057814836502 + 100.0 * 6.244075775146484
Epoch 1220, val loss: 1.2452772855758667
Epoch 1230, training loss: 624.744140625 = 0.15170827507972717 + 100.0 * 6.24592399597168
Epoch 1230, val loss: 1.2501449584960938
Epoch 1240, training loss: 624.3426513671875 = 0.14770850539207458 + 100.0 * 6.241949558258057
Epoch 1240, val loss: 1.2548742294311523
Epoch 1250, training loss: 624.2871704101562 = 0.14387285709381104 + 100.0 * 6.241433143615723
Epoch 1250, val loss: 1.2600417137145996
Epoch 1260, training loss: 624.59423828125 = 0.14017416536808014 + 100.0 * 6.244540691375732
Epoch 1260, val loss: 1.2653034925460815
Epoch 1270, training loss: 624.3855590820312 = 0.13651439547538757 + 100.0 * 6.242490291595459
Epoch 1270, val loss: 1.2694584131240845
Epoch 1280, training loss: 624.2896118164062 = 0.13296659290790558 + 100.0 * 6.241566181182861
Epoch 1280, val loss: 1.2749435901641846
Epoch 1290, training loss: 624.0613403320312 = 0.12957783043384552 + 100.0 * 6.239317417144775
Epoch 1290, val loss: 1.280266284942627
Epoch 1300, training loss: 623.9911499023438 = 0.12629272043704987 + 100.0 * 6.238648891448975
Epoch 1300, val loss: 1.2853167057037354
Epoch 1310, training loss: 624.051025390625 = 0.12311293184757233 + 100.0 * 6.239279270172119
Epoch 1310, val loss: 1.290751576423645
Epoch 1320, training loss: 624.1192016601562 = 0.11997517943382263 + 100.0 * 6.239992141723633
Epoch 1320, val loss: 1.2957013845443726
Epoch 1330, training loss: 623.8905029296875 = 0.11691693961620331 + 100.0 * 6.237735748291016
Epoch 1330, val loss: 1.3006114959716797
Epoch 1340, training loss: 624.0001220703125 = 0.11398729681968689 + 100.0 * 6.238861083984375
Epoch 1340, val loss: 1.3059717416763306
Epoch 1350, training loss: 624.1315307617188 = 0.11114366352558136 + 100.0 * 6.240203857421875
Epoch 1350, val loss: 1.3110790252685547
Epoch 1360, training loss: 623.9528198242188 = 0.10835441946983337 + 100.0 * 6.238444805145264
Epoch 1360, val loss: 1.3158552646636963
Epoch 1370, training loss: 623.8018188476562 = 0.10567144304513931 + 100.0 * 6.236961841583252
Epoch 1370, val loss: 1.3212000131607056
Epoch 1380, training loss: 624.8585815429688 = 0.10306025296449661 + 100.0 * 6.247554779052734
Epoch 1380, val loss: 1.326231598854065
Epoch 1390, training loss: 623.669677734375 = 0.10046906769275665 + 100.0 * 6.235692024230957
Epoch 1390, val loss: 1.3313183784484863
Epoch 1400, training loss: 623.6090087890625 = 0.0979892835021019 + 100.0 * 6.235110759735107
Epoch 1400, val loss: 1.3369628190994263
Epoch 1410, training loss: 623.507568359375 = 0.09563074260950089 + 100.0 * 6.234119415283203
Epoch 1410, val loss: 1.3419685363769531
Epoch 1420, training loss: 623.4321899414062 = 0.09333281964063644 + 100.0 * 6.233388423919678
Epoch 1420, val loss: 1.3474308252334595
Epoch 1430, training loss: 623.4105834960938 = 0.09110669046640396 + 100.0 * 6.233194828033447
Epoch 1430, val loss: 1.352594256401062
Epoch 1440, training loss: 624.4431762695312 = 0.08893302828073502 + 100.0 * 6.243542671203613
Epoch 1440, val loss: 1.3578720092773438
Epoch 1450, training loss: 623.5294799804688 = 0.08679104596376419 + 100.0 * 6.234426975250244
Epoch 1450, val loss: 1.3625340461730957
Epoch 1460, training loss: 623.4402465820312 = 0.08472791314125061 + 100.0 * 6.233555316925049
Epoch 1460, val loss: 1.3677126169204712
Epoch 1470, training loss: 623.711669921875 = 0.0827343612909317 + 100.0 * 6.236289024353027
Epoch 1470, val loss: 1.3729802370071411
Epoch 1480, training loss: 623.2675170898438 = 0.08076223731040955 + 100.0 * 6.231867790222168
Epoch 1480, val loss: 1.3780094385147095
Epoch 1490, training loss: 623.17333984375 = 0.07887749373912811 + 100.0 * 6.230944633483887
Epoch 1490, val loss: 1.383004903793335
Epoch 1500, training loss: 623.2169189453125 = 0.07705385237932205 + 100.0 * 6.231399059295654
Epoch 1500, val loss: 1.3880199193954468
Epoch 1510, training loss: 623.5278930664062 = 0.0752832219004631 + 100.0 * 6.23452615737915
Epoch 1510, val loss: 1.3930517435073853
Epoch 1520, training loss: 623.5042114257812 = 0.07355374097824097 + 100.0 * 6.234306335449219
Epoch 1520, val loss: 1.3983800411224365
Epoch 1530, training loss: 623.4022827148438 = 0.07186427712440491 + 100.0 * 6.233304500579834
Epoch 1530, val loss: 1.4025861024856567
Epoch 1540, training loss: 623.0028076171875 = 0.07021715492010117 + 100.0 * 6.229326248168945
Epoch 1540, val loss: 1.4080684185028076
Epoch 1550, training loss: 622.9729614257812 = 0.06862929463386536 + 100.0 * 6.229043483734131
Epoch 1550, val loss: 1.4130134582519531
Epoch 1560, training loss: 622.927001953125 = 0.0671001449227333 + 100.0 * 6.2285990715026855
Epoch 1560, val loss: 1.4181240797042847
Epoch 1570, training loss: 623.1485595703125 = 0.065618135035038 + 100.0 * 6.230829238891602
Epoch 1570, val loss: 1.4231133460998535
Epoch 1580, training loss: 623.0043334960938 = 0.06414995342493057 + 100.0 * 6.2294020652771
Epoch 1580, val loss: 1.4278514385223389
Epoch 1590, training loss: 623.3466186523438 = 0.06273151189088821 + 100.0 * 6.2328386306762695
Epoch 1590, val loss: 1.4331189393997192
Epoch 1600, training loss: 623.05810546875 = 0.06132803484797478 + 100.0 * 6.2299675941467285
Epoch 1600, val loss: 1.4373551607131958
Epoch 1610, training loss: 622.7954711914062 = 0.05997781455516815 + 100.0 * 6.227354526519775
Epoch 1610, val loss: 1.4422537088394165
Epoch 1620, training loss: 622.7070922851562 = 0.0586753711104393 + 100.0 * 6.226484298706055
Epoch 1620, val loss: 1.4472906589508057
Epoch 1630, training loss: 622.6343994140625 = 0.05742751061916351 + 100.0 * 6.225769519805908
Epoch 1630, val loss: 1.452167272567749
Epoch 1640, training loss: 623.0093994140625 = 0.056216657161712646 + 100.0 * 6.229531764984131
Epoch 1640, val loss: 1.4569473266601562
Epoch 1650, training loss: 622.67822265625 = 0.05500165373086929 + 100.0 * 6.226232051849365
Epoch 1650, val loss: 1.461431622505188
Epoch 1660, training loss: 622.7430419921875 = 0.05382977053523064 + 100.0 * 6.226891994476318
Epoch 1660, val loss: 1.4660276174545288
Epoch 1670, training loss: 622.5687255859375 = 0.05270740017294884 + 100.0 * 6.225160121917725
Epoch 1670, val loss: 1.4710063934326172
Epoch 1680, training loss: 622.7664184570312 = 0.05161886662244797 + 100.0 * 6.227148056030273
Epoch 1680, val loss: 1.4756208658218384
Epoch 1690, training loss: 622.4888305664062 = 0.050544772297143936 + 100.0 * 6.2243828773498535
Epoch 1690, val loss: 1.480467677116394
Epoch 1700, training loss: 622.6373901367188 = 0.04950624704360962 + 100.0 * 6.225878715515137
Epoch 1700, val loss: 1.485325813293457
Epoch 1710, training loss: 622.723876953125 = 0.04848473146557808 + 100.0 * 6.226754188537598
Epoch 1710, val loss: 1.4900144338607788
Epoch 1720, training loss: 622.6879272460938 = 0.04749775305390358 + 100.0 * 6.226404190063477
Epoch 1720, val loss: 1.494641900062561
Epoch 1730, training loss: 622.4677734375 = 0.0465276874601841 + 100.0 * 6.224212646484375
Epoch 1730, val loss: 1.4988305568695068
Epoch 1740, training loss: 622.4077758789062 = 0.045604027807712555 + 100.0 * 6.223621845245361
Epoch 1740, val loss: 1.5031830072402954
Epoch 1750, training loss: 622.5886840820312 = 0.04470069333910942 + 100.0 * 6.22544002532959
Epoch 1750, val loss: 1.5078550577163696
Epoch 1760, training loss: 622.362548828125 = 0.043814703822135925 + 100.0 * 6.22318696975708
Epoch 1760, val loss: 1.512039065361023
Epoch 1770, training loss: 622.2814331054688 = 0.04294021427631378 + 100.0 * 6.222384929656982
Epoch 1770, val loss: 1.516903042793274
Epoch 1780, training loss: 622.465087890625 = 0.04211309924721718 + 100.0 * 6.22422981262207
Epoch 1780, val loss: 1.521162509918213
Epoch 1790, training loss: 622.4423217773438 = 0.04129243269562721 + 100.0 * 6.224009990692139
Epoch 1790, val loss: 1.525156021118164
Epoch 1800, training loss: 622.1937255859375 = 0.04047132655978203 + 100.0 * 6.221532344818115
Epoch 1800, val loss: 1.5298378467559814
Epoch 1810, training loss: 622.4827880859375 = 0.03970707207918167 + 100.0 * 6.224430561065674
Epoch 1810, val loss: 1.5340938568115234
Epoch 1820, training loss: 622.154541015625 = 0.03893649950623512 + 100.0 * 6.221156120300293
Epoch 1820, val loss: 1.5382307767868042
Epoch 1830, training loss: 622.0938110351562 = 0.038199059665203094 + 100.0 * 6.220555782318115
Epoch 1830, val loss: 1.5425904989242554
Epoch 1840, training loss: 622.0115356445312 = 0.03748495131731033 + 100.0 * 6.219740867614746
Epoch 1840, val loss: 1.5469731092453003
Epoch 1850, training loss: 622.6031494140625 = 0.03680559620261192 + 100.0 * 6.225663661956787
Epoch 1850, val loss: 1.5512679815292358
Epoch 1860, training loss: 622.0966796875 = 0.03609812259674072 + 100.0 * 6.220606327056885
Epoch 1860, val loss: 1.5546563863754272
Epoch 1870, training loss: 622.060546875 = 0.03542370721697807 + 100.0 * 6.220251560211182
Epoch 1870, val loss: 1.5590165853500366
Epoch 1880, training loss: 621.9407958984375 = 0.03478076308965683 + 100.0 * 6.219059944152832
Epoch 1880, val loss: 1.5630213022232056
Epoch 1890, training loss: 621.98388671875 = 0.03415476530790329 + 100.0 * 6.219497203826904
Epoch 1890, val loss: 1.5676199197769165
Epoch 1900, training loss: 622.6102905273438 = 0.0335373617708683 + 100.0 * 6.225767612457275
Epoch 1900, val loss: 1.5713677406311035
Epoch 1910, training loss: 621.8731079101562 = 0.03292539343237877 + 100.0 * 6.21840238571167
Epoch 1910, val loss: 1.5748012065887451
Epoch 1920, training loss: 621.7669677734375 = 0.032329488545656204 + 100.0 * 6.21734619140625
Epoch 1920, val loss: 1.5787144899368286
Epoch 1930, training loss: 621.7667236328125 = 0.03176208585500717 + 100.0 * 6.217350006103516
Epoch 1930, val loss: 1.582984209060669
Epoch 1940, training loss: 622.1068725585938 = 0.031215552240610123 + 100.0 * 6.220756530761719
Epoch 1940, val loss: 1.586827039718628
Epoch 1950, training loss: 621.7075805664062 = 0.03065335564315319 + 100.0 * 6.216769218444824
Epoch 1950, val loss: 1.5904806852340698
Epoch 1960, training loss: 621.6653442382812 = 0.03011634759604931 + 100.0 * 6.216352462768555
Epoch 1960, val loss: 1.5942494869232178
Epoch 1970, training loss: 621.7225341796875 = 0.029600778594613075 + 100.0 * 6.2169294357299805
Epoch 1970, val loss: 1.598016381263733
Epoch 1980, training loss: 622.4252319335938 = 0.02910393849015236 + 100.0 * 6.223960876464844
Epoch 1980, val loss: 1.6015444993972778
Epoch 1990, training loss: 621.9192504882812 = 0.028582220897078514 + 100.0 * 6.218906879425049
Epoch 1990, val loss: 1.605222225189209
Epoch 2000, training loss: 621.6136474609375 = 0.028101444244384766 + 100.0 * 6.215855598449707
Epoch 2000, val loss: 1.6089272499084473
Epoch 2010, training loss: 621.5405883789062 = 0.027629908174276352 + 100.0 * 6.215129375457764
Epoch 2010, val loss: 1.6127687692642212
Epoch 2020, training loss: 621.6240234375 = 0.02717851661145687 + 100.0 * 6.215968608856201
Epoch 2020, val loss: 1.6162829399108887
Epoch 2030, training loss: 622.1827392578125 = 0.026724906638264656 + 100.0 * 6.221560001373291
Epoch 2030, val loss: 1.619955062866211
Epoch 2040, training loss: 621.7526245117188 = 0.02627575770020485 + 100.0 * 6.217263221740723
Epoch 2040, val loss: 1.622717022895813
Epoch 2050, training loss: 621.6581420898438 = 0.02583969011902809 + 100.0 * 6.216323375701904
Epoch 2050, val loss: 1.6270736455917358
Epoch 2060, training loss: 621.677978515625 = 0.02542344480752945 + 100.0 * 6.216525554656982
Epoch 2060, val loss: 1.6302272081375122
Epoch 2070, training loss: 621.6504516601562 = 0.025009067729115486 + 100.0 * 6.216254234313965
Epoch 2070, val loss: 1.6337549686431885
Epoch 2080, training loss: 621.44677734375 = 0.024610789492726326 + 100.0 * 6.214221954345703
Epoch 2080, val loss: 1.637019395828247
Epoch 2090, training loss: 621.4668579101562 = 0.024225754663348198 + 100.0 * 6.214426517486572
Epoch 2090, val loss: 1.6403918266296387
Epoch 2100, training loss: 621.4341430664062 = 0.023845553398132324 + 100.0 * 6.214102745056152
Epoch 2100, val loss: 1.6437114477157593
Epoch 2110, training loss: 622.0711669921875 = 0.023486830294132233 + 100.0 * 6.2204766273498535
Epoch 2110, val loss: 1.6466847658157349
Epoch 2120, training loss: 621.9479370117188 = 0.023099534213542938 + 100.0 * 6.2192487716674805
Epoch 2120, val loss: 1.6507760286331177
Epoch 2130, training loss: 621.4996948242188 = 0.022732524201273918 + 100.0 * 6.2147698402404785
Epoch 2130, val loss: 1.653349757194519
Epoch 2140, training loss: 621.3642578125 = 0.022378746420145035 + 100.0 * 6.213418483734131
Epoch 2140, val loss: 1.656827688217163
Epoch 2150, training loss: 621.5328979492188 = 0.02204342745244503 + 100.0 * 6.215108871459961
Epoch 2150, val loss: 1.6602847576141357
Epoch 2160, training loss: 621.397216796875 = 0.021706068888306618 + 100.0 * 6.213754653930664
Epoch 2160, val loss: 1.663124918937683
Epoch 2170, training loss: 621.3350830078125 = 0.02137572504580021 + 100.0 * 6.213137149810791
Epoch 2170, val loss: 1.666551947593689
Epoch 2180, training loss: 621.5912475585938 = 0.02106509730219841 + 100.0 * 6.215702056884766
Epoch 2180, val loss: 1.669546365737915
Epoch 2190, training loss: 621.3858032226562 = 0.020748045295476913 + 100.0 * 6.213650703430176
Epoch 2190, val loss: 1.6726000308990479
Epoch 2200, training loss: 621.6727294921875 = 0.020443109795451164 + 100.0 * 6.216522693634033
Epoch 2200, val loss: 1.6755287647247314
Epoch 2210, training loss: 621.2048950195312 = 0.020125536248087883 + 100.0 * 6.21184778213501
Epoch 2210, val loss: 1.6787999868392944
Epoch 2220, training loss: 621.1652221679688 = 0.0198354572057724 + 100.0 * 6.211453914642334
Epoch 2220, val loss: 1.6820567846298218
Epoch 2230, training loss: 621.1353759765625 = 0.019552167505025864 + 100.0 * 6.211158275604248
Epoch 2230, val loss: 1.6852726936340332
Epoch 2240, training loss: 621.45361328125 = 0.01927771233022213 + 100.0 * 6.214343070983887
Epoch 2240, val loss: 1.6884454488754272
Epoch 2250, training loss: 621.3604736328125 = 0.018997864797711372 + 100.0 * 6.213414669036865
Epoch 2250, val loss: 1.6910349130630493
Epoch 2260, training loss: 621.3657836914062 = 0.01872969977557659 + 100.0 * 6.213470458984375
Epoch 2260, val loss: 1.6936196088790894
Epoch 2270, training loss: 621.5338134765625 = 0.018462777137756348 + 100.0 * 6.215153694152832
Epoch 2270, val loss: 1.696752667427063
Epoch 2280, training loss: 621.1014404296875 = 0.018191920593380928 + 100.0 * 6.210832595825195
Epoch 2280, val loss: 1.699644684791565
Epoch 2290, training loss: 621.050537109375 = 0.017941223457455635 + 100.0 * 6.210325717926025
Epoch 2290, val loss: 1.702867865562439
Epoch 2300, training loss: 621.0769653320312 = 0.017699670046567917 + 100.0 * 6.210592746734619
Epoch 2300, val loss: 1.7056916952133179
Epoch 2310, training loss: 622.0178833007812 = 0.01746666058897972 + 100.0 * 6.220004081726074
Epoch 2310, val loss: 1.7082618474960327
Epoch 2320, training loss: 621.3892211914062 = 0.017216399312019348 + 100.0 * 6.213719844818115
Epoch 2320, val loss: 1.710784673690796
Epoch 2330, training loss: 621.0338134765625 = 0.01697722263634205 + 100.0 * 6.210168361663818
Epoch 2330, val loss: 1.7136719226837158
Epoch 2340, training loss: 620.9926147460938 = 0.01675093173980713 + 100.0 * 6.209758758544922
Epoch 2340, val loss: 1.7166081666946411
Epoch 2350, training loss: 621.3341674804688 = 0.01653154566884041 + 100.0 * 6.213176250457764
Epoch 2350, val loss: 1.7195394039154053
Epoch 2360, training loss: 621.0264892578125 = 0.016310781240463257 + 100.0 * 6.210102081298828
Epoch 2360, val loss: 1.7219650745391846
Epoch 2370, training loss: 621.33154296875 = 0.016101200133562088 + 100.0 * 6.213154315948486
Epoch 2370, val loss: 1.724350094795227
Epoch 2380, training loss: 620.9876708984375 = 0.015880856662988663 + 100.0 * 6.209717750549316
Epoch 2380, val loss: 1.72718346118927
Epoch 2390, training loss: 620.8948974609375 = 0.01567944698035717 + 100.0 * 6.208792209625244
Epoch 2390, val loss: 1.7300509214401245
Epoch 2400, training loss: 620.9777221679688 = 0.015483503229916096 + 100.0 * 6.209621906280518
Epoch 2400, val loss: 1.7326529026031494
Epoch 2410, training loss: 621.2390747070312 = 0.01528692152351141 + 100.0 * 6.212238311767578
Epoch 2410, val loss: 1.735194206237793
Epoch 2420, training loss: 621.1557006835938 = 0.015089040622115135 + 100.0 * 6.211406230926514
Epoch 2420, val loss: 1.7377949953079224
Epoch 2430, training loss: 620.977783203125 = 0.014896199107170105 + 100.0 * 6.209629058837891
Epoch 2430, val loss: 1.7404175996780396
Epoch 2440, training loss: 620.8298950195312 = 0.014707940630614758 + 100.0 * 6.208151817321777
Epoch 2440, val loss: 1.7429529428482056
Epoch 2450, training loss: 620.8641967773438 = 0.014527013525366783 + 100.0 * 6.208496570587158
Epoch 2450, val loss: 1.7456471920013428
Epoch 2460, training loss: 621.74951171875 = 0.014352732338011265 + 100.0 * 6.217351913452148
Epoch 2460, val loss: 1.7481926679611206
Epoch 2470, training loss: 621.1235961914062 = 0.014176873490214348 + 100.0 * 6.211094379425049
Epoch 2470, val loss: 1.7497124671936035
Epoch 2480, training loss: 620.8384399414062 = 0.01399307046085596 + 100.0 * 6.208244800567627
Epoch 2480, val loss: 1.7527989149093628
Epoch 2490, training loss: 620.784912109375 = 0.013827846385538578 + 100.0 * 6.2077107429504395
Epoch 2490, val loss: 1.7551673650741577
Epoch 2500, training loss: 621.1057739257812 = 0.013665717095136642 + 100.0 * 6.210920810699463
Epoch 2500, val loss: 1.7578564882278442
Epoch 2510, training loss: 620.6924438476562 = 0.01349951233714819 + 100.0 * 6.206789493560791
Epoch 2510, val loss: 1.7599005699157715
Epoch 2520, training loss: 620.780029296875 = 0.013341749086976051 + 100.0 * 6.207666397094727
Epoch 2520, val loss: 1.762271523475647
Epoch 2530, training loss: 620.9396362304688 = 0.01318720169365406 + 100.0 * 6.209264755249023
Epoch 2530, val loss: 1.7647801637649536
Epoch 2540, training loss: 620.8026733398438 = 0.013028890825808048 + 100.0 * 6.207896709442139
Epoch 2540, val loss: 1.766866683959961
Epoch 2550, training loss: 620.8032836914062 = 0.012873138301074505 + 100.0 * 6.207903861999512
Epoch 2550, val loss: 1.7692402601242065
Epoch 2560, training loss: 620.5982666015625 = 0.01272615510970354 + 100.0 * 6.205854892730713
Epoch 2560, val loss: 1.7717390060424805
Epoch 2570, training loss: 620.6404418945312 = 0.012586763128638268 + 100.0 * 6.2062788009643555
Epoch 2570, val loss: 1.7741748094558716
Epoch 2580, training loss: 621.4021606445312 = 0.012452221475541592 + 100.0 * 6.213897228240967
Epoch 2580, val loss: 1.776111125946045
Epoch 2590, training loss: 620.8265991210938 = 0.012297245673835278 + 100.0 * 6.20814323425293
Epoch 2590, val loss: 1.7786158323287964
Epoch 2600, training loss: 620.6329956054688 = 0.012158403173089027 + 100.0 * 6.2062087059021
Epoch 2600, val loss: 1.7807739973068237
Epoch 2610, training loss: 620.5484008789062 = 0.012023582123219967 + 100.0 * 6.205363750457764
Epoch 2610, val loss: 1.7832483053207397
Epoch 2620, training loss: 620.5238037109375 = 0.011894341558218002 + 100.0 * 6.2051191329956055
Epoch 2620, val loss: 1.785660982131958
Epoch 2630, training loss: 620.9912719726562 = 0.011770009994506836 + 100.0 * 6.209794998168945
Epoch 2630, val loss: 1.7882587909698486
Epoch 2640, training loss: 621.0543212890625 = 0.011640454642474651 + 100.0 * 6.2104268074035645
Epoch 2640, val loss: 1.7895643711090088
Epoch 2650, training loss: 620.8939819335938 = 0.011506740935146809 + 100.0 * 6.208824157714844
Epoch 2650, val loss: 1.7916663885116577
Epoch 2660, training loss: 620.5563354492188 = 0.011379092000424862 + 100.0 * 6.205450057983398
Epoch 2660, val loss: 1.794081449508667
Epoch 2670, training loss: 620.5712280273438 = 0.01126041915267706 + 100.0 * 6.205600261688232
Epoch 2670, val loss: 1.796311855316162
Epoch 2680, training loss: 621.0906372070312 = 0.011144865304231644 + 100.0 * 6.210794448852539
Epoch 2680, val loss: 1.7987337112426758
Epoch 2690, training loss: 620.6705322265625 = 0.011019300669431686 + 100.0 * 6.206594944000244
Epoch 2690, val loss: 1.8005882501602173
Epoch 2700, training loss: 620.5011596679688 = 0.010903795249760151 + 100.0 * 6.204902172088623
Epoch 2700, val loss: 1.8028373718261719
Epoch 2710, training loss: 620.4050903320312 = 0.010792955756187439 + 100.0 * 6.203942775726318
Epoch 2710, val loss: 1.8050791025161743
Epoch 2720, training loss: 621.093505859375 = 0.01068923156708479 + 100.0 * 6.2108283042907715
Epoch 2720, val loss: 1.8069640398025513
Epoch 2730, training loss: 620.7540893554688 = 0.010575063526630402 + 100.0 * 6.207435131072998
Epoch 2730, val loss: 1.8089758157730103
Epoch 2740, training loss: 620.5087280273438 = 0.010458488017320633 + 100.0 * 6.204982757568359
Epoch 2740, val loss: 1.8108539581298828
Epoch 2750, training loss: 620.420654296875 = 0.010351980105042458 + 100.0 * 6.204102993011475
Epoch 2750, val loss: 1.8134710788726807
Epoch 2760, training loss: 620.3773803710938 = 0.010250149294734001 + 100.0 * 6.203671455383301
Epoch 2760, val loss: 1.815602421760559
Epoch 2770, training loss: 620.46728515625 = 0.010151896625757217 + 100.0 * 6.204571723937988
Epoch 2770, val loss: 1.8179588317871094
Epoch 2780, training loss: 620.76220703125 = 0.010053093545138836 + 100.0 * 6.207521438598633
Epoch 2780, val loss: 1.819883108139038
Epoch 2790, training loss: 620.471435546875 = 0.009954779408872128 + 100.0 * 6.204615116119385
Epoch 2790, val loss: 1.8212733268737793
Epoch 2800, training loss: 620.5043334960938 = 0.009855699725449085 + 100.0 * 6.204945087432861
Epoch 2800, val loss: 1.8233861923217773
Epoch 2810, training loss: 620.7277221679688 = 0.009760294109582901 + 100.0 * 6.207179546356201
Epoch 2810, val loss: 1.8257205486297607
Epoch 2820, training loss: 620.4033203125 = 0.009665125980973244 + 100.0 * 6.20393705368042
Epoch 2820, val loss: 1.827089786529541
Epoch 2830, training loss: 620.3222045898438 = 0.009571420960128307 + 100.0 * 6.203126430511475
Epoch 2830, val loss: 1.8295432329177856
Epoch 2840, training loss: 620.2745361328125 = 0.009482424706220627 + 100.0 * 6.202651023864746
Epoch 2840, val loss: 1.8313761949539185
Epoch 2850, training loss: 620.6806030273438 = 0.009396360255777836 + 100.0 * 6.206712245941162
Epoch 2850, val loss: 1.8334245681762695
Epoch 2860, training loss: 620.4389038085938 = 0.009304606355726719 + 100.0 * 6.204296112060547
Epoch 2860, val loss: 1.835081696510315
Epoch 2870, training loss: 620.3204345703125 = 0.00921762827783823 + 100.0 * 6.2031121253967285
Epoch 2870, val loss: 1.836661696434021
Epoch 2880, training loss: 620.3716430664062 = 0.009132018312811852 + 100.0 * 6.203624725341797
Epoch 2880, val loss: 1.8390341997146606
Epoch 2890, training loss: 620.427978515625 = 0.00905038882046938 + 100.0 * 6.204189300537109
Epoch 2890, val loss: 1.8405338525772095
Epoch 2900, training loss: 620.19482421875 = 0.008964668028056622 + 100.0 * 6.2018585205078125
Epoch 2900, val loss: 1.842534065246582
Epoch 2910, training loss: 620.2520141601562 = 0.008885037153959274 + 100.0 * 6.2024312019348145
Epoch 2910, val loss: 1.844396948814392
Epoch 2920, training loss: 620.7067260742188 = 0.008811202831566334 + 100.0 * 6.206979274749756
Epoch 2920, val loss: 1.8460502624511719
Epoch 2930, training loss: 620.3723754882812 = 0.008727805688977242 + 100.0 * 6.203636169433594
Epoch 2930, val loss: 1.8481695652008057
Epoch 2940, training loss: 620.1723022460938 = 0.0086457384750247 + 100.0 * 6.20163631439209
Epoch 2940, val loss: 1.84955894947052
Epoch 2950, training loss: 620.1710205078125 = 0.008570864796638489 + 100.0 * 6.201624870300293
Epoch 2950, val loss: 1.8516478538513184
Epoch 2960, training loss: 620.7010498046875 = 0.00849911943078041 + 100.0 * 6.206925868988037
Epoch 2960, val loss: 1.853603482246399
Epoch 2970, training loss: 620.2352294921875 = 0.008420899510383606 + 100.0 * 6.202268123626709
Epoch 2970, val loss: 1.8547389507293701
Epoch 2980, training loss: 620.29345703125 = 0.008349425159394741 + 100.0 * 6.202850818634033
Epoch 2980, val loss: 1.856657862663269
Epoch 2990, training loss: 620.1830444335938 = 0.008276276290416718 + 100.0 * 6.201747417449951
Epoch 2990, val loss: 1.8583711385726929
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6518518518518519
0.8102266736953084
The final CL Acc:0.65062, 0.01666, The final GNN Acc:0.80829, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13150])
remove edge: torch.Size([2, 7864])
updated graph: torch.Size([2, 10458])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6453857421875 = 1.957212209701538 + 100.0 * 8.596881866455078
Epoch 0, val loss: 1.9610790014266968
Epoch 10, training loss: 861.5858154296875 = 1.947845458984375 + 100.0 * 8.596379280090332
Epoch 10, val loss: 1.9514580965042114
Epoch 20, training loss: 861.1652221679688 = 1.9365922212600708 + 100.0 * 8.592286109924316
Epoch 20, val loss: 1.9398202896118164
Epoch 30, training loss: 858.2418212890625 = 1.922155737876892 + 100.0 * 8.563196182250977
Epoch 30, val loss: 1.9250538349151611
Epoch 40, training loss: 845.228515625 = 1.9034130573272705 + 100.0 * 8.433250427246094
Epoch 40, val loss: 1.9067665338516235
Epoch 50, training loss: 806.6393432617188 = 1.8832086324691772 + 100.0 * 8.047561645507812
Epoch 50, val loss: 1.8876675367355347
Epoch 60, training loss: 777.7884521484375 = 1.8638039827346802 + 100.0 * 7.759246349334717
Epoch 60, val loss: 1.870849847793579
Epoch 70, training loss: 737.7205810546875 = 1.8497318029403687 + 100.0 * 7.358708381652832
Epoch 70, val loss: 1.858317494392395
Epoch 80, training loss: 710.8432006835938 = 1.835033893585205 + 100.0 * 7.090081691741943
Epoch 80, val loss: 1.8445063829421997
Epoch 90, training loss: 695.4605712890625 = 1.822287917137146 + 100.0 * 6.936383247375488
Epoch 90, val loss: 1.8321621417999268
Epoch 100, training loss: 688.7353515625 = 1.8094658851623535 + 100.0 * 6.869258880615234
Epoch 100, val loss: 1.820157527923584
Epoch 110, training loss: 680.7029418945312 = 1.7962547540664673 + 100.0 * 6.789066791534424
Epoch 110, val loss: 1.808476448059082
Epoch 120, training loss: 673.0269775390625 = 1.7844759225845337 + 100.0 * 6.7124247550964355
Epoch 120, val loss: 1.7979729175567627
Epoch 130, training loss: 667.5689086914062 = 1.773087978363037 + 100.0 * 6.657958507537842
Epoch 130, val loss: 1.7874855995178223
Epoch 140, training loss: 663.678955078125 = 1.7604091167449951 + 100.0 * 6.619184970855713
Epoch 140, val loss: 1.7761090993881226
Epoch 150, training loss: 660.6509399414062 = 1.746618628501892 + 100.0 * 6.589043140411377
Epoch 150, val loss: 1.7639812231063843
Epoch 160, training loss: 657.9404296875 = 1.732060194015503 + 100.0 * 6.562083721160889
Epoch 160, val loss: 1.7512925863265991
Epoch 170, training loss: 655.4710693359375 = 1.71664297580719 + 100.0 * 6.5375447273254395
Epoch 170, val loss: 1.73792564868927
Epoch 180, training loss: 653.7909545898438 = 1.7001779079437256 + 100.0 * 6.520907402038574
Epoch 180, val loss: 1.7236915826797485
Epoch 190, training loss: 651.9872436523438 = 1.682313323020935 + 100.0 * 6.503049373626709
Epoch 190, val loss: 1.7085150480270386
Epoch 200, training loss: 650.7862548828125 = 1.6630165576934814 + 100.0 * 6.491232395172119
Epoch 200, val loss: 1.6922153234481812
Epoch 210, training loss: 649.226318359375 = 1.6426795721054077 + 100.0 * 6.475836277008057
Epoch 210, val loss: 1.675108790397644
Epoch 220, training loss: 647.8683471679688 = 1.6213161945343018 + 100.0 * 6.462470054626465
Epoch 220, val loss: 1.6571391820907593
Epoch 230, training loss: 646.7955322265625 = 1.5989927053451538 + 100.0 * 6.45196533203125
Epoch 230, val loss: 1.6383671760559082
Epoch 240, training loss: 645.8529663085938 = 1.5753217935562134 + 100.0 * 6.442776679992676
Epoch 240, val loss: 1.6186813116073608
Epoch 250, training loss: 644.7534790039062 = 1.5509843826293945 + 100.0 * 6.432024955749512
Epoch 250, val loss: 1.5983169078826904
Epoch 260, training loss: 643.867431640625 = 1.5259047746658325 + 100.0 * 6.423415660858154
Epoch 260, val loss: 1.5774351358413696
Epoch 270, training loss: 643.2278442382812 = 1.5003377199172974 + 100.0 * 6.4172749519348145
Epoch 270, val loss: 1.5562771558761597
Epoch 280, training loss: 642.4675903320312 = 1.474152684211731 + 100.0 * 6.409934043884277
Epoch 280, val loss: 1.5348514318466187
Epoch 290, training loss: 641.6726684570312 = 1.4480048418045044 + 100.0 * 6.402246475219727
Epoch 290, val loss: 1.51345694065094
Epoch 300, training loss: 641.0317993164062 = 1.4218140840530396 + 100.0 * 6.396100044250488
Epoch 300, val loss: 1.4922956228256226
Epoch 310, training loss: 640.3961791992188 = 1.3957010507583618 + 100.0 * 6.390004634857178
Epoch 310, val loss: 1.4714510440826416
Epoch 320, training loss: 640.6101684570312 = 1.3697161674499512 + 100.0 * 6.392405033111572
Epoch 320, val loss: 1.4509050846099854
Epoch 330, training loss: 639.449951171875 = 1.3436692953109741 + 100.0 * 6.3810625076293945
Epoch 330, val loss: 1.430668592453003
Epoch 340, training loss: 638.89208984375 = 1.3181381225585938 + 100.0 * 6.375740051269531
Epoch 340, val loss: 1.410963535308838
Epoch 350, training loss: 638.4695434570312 = 1.2928173542022705 + 100.0 * 6.371767044067383
Epoch 350, val loss: 1.3916634321212769
Epoch 360, training loss: 637.7977294921875 = 1.267639398574829 + 100.0 * 6.36530065536499
Epoch 360, val loss: 1.372704267501831
Epoch 370, training loss: 637.5361328125 = 1.2427761554718018 + 100.0 * 6.36293363571167
Epoch 370, val loss: 1.3541960716247559
Epoch 380, training loss: 637.0680541992188 = 1.2183469533920288 + 100.0 * 6.358497619628906
Epoch 380, val loss: 1.336035966873169
Epoch 390, training loss: 636.4697875976562 = 1.1938925981521606 + 100.0 * 6.352758884429932
Epoch 390, val loss: 1.3183315992355347
Epoch 400, training loss: 636.0067138671875 = 1.169836163520813 + 100.0 * 6.3483686447143555
Epoch 400, val loss: 1.3007781505584717
Epoch 410, training loss: 635.6142578125 = 1.1459413766860962 + 100.0 * 6.3446831703186035
Epoch 410, val loss: 1.2836081981658936
Epoch 420, training loss: 635.281494140625 = 1.122206687927246 + 100.0 * 6.341593265533447
Epoch 420, val loss: 1.2666568756103516
Epoch 430, training loss: 635.478271484375 = 1.0983983278274536 + 100.0 * 6.343799114227295
Epoch 430, val loss: 1.2495890855789185
Epoch 440, training loss: 634.7374877929688 = 1.074967384338379 + 100.0 * 6.336625576019287
Epoch 440, val loss: 1.2328194379806519
Epoch 450, training loss: 634.3004760742188 = 1.0516934394836426 + 100.0 * 6.3324875831604
Epoch 450, val loss: 1.216349720954895
Epoch 460, training loss: 633.949951171875 = 1.028513789176941 + 100.0 * 6.329214572906494
Epoch 460, val loss: 1.1999292373657227
Epoch 470, training loss: 633.6672973632812 = 1.0055336952209473 + 100.0 * 6.326617240905762
Epoch 470, val loss: 1.1836645603179932
Epoch 480, training loss: 634.130859375 = 0.9826034307479858 + 100.0 * 6.331482410430908
Epoch 480, val loss: 1.1675044298171997
Epoch 490, training loss: 633.314453125 = 0.9600626826286316 + 100.0 * 6.323543548583984
Epoch 490, val loss: 1.1515120267868042
Epoch 500, training loss: 632.8612670898438 = 0.9376464486122131 + 100.0 * 6.3192362785339355
Epoch 500, val loss: 1.1355040073394775
Epoch 510, training loss: 632.5910034179688 = 0.915594220161438 + 100.0 * 6.31675386428833
Epoch 510, val loss: 1.1200248003005981
Epoch 520, training loss: 632.4044799804688 = 0.8938496708869934 + 100.0 * 6.315106391906738
Epoch 520, val loss: 1.1048611402511597
Epoch 530, training loss: 632.3364868164062 = 0.8722865581512451 + 100.0 * 6.314641952514648
Epoch 530, val loss: 1.0896893739700317
Epoch 540, training loss: 631.8721313476562 = 0.8511670231819153 + 100.0 * 6.310209274291992
Epoch 540, val loss: 1.075134515762329
Epoch 550, training loss: 631.5830688476562 = 0.8304282426834106 + 100.0 * 6.307526111602783
Epoch 550, val loss: 1.0609427690505981
Epoch 560, training loss: 631.5759887695312 = 0.8101714253425598 + 100.0 * 6.3076581954956055
Epoch 560, val loss: 1.0473204851150513
Epoch 570, training loss: 631.3525390625 = 0.7901411652565002 + 100.0 * 6.305623531341553
Epoch 570, val loss: 1.0337717533111572
Epoch 580, training loss: 630.9188232421875 = 0.770638644695282 + 100.0 * 6.301482200622559
Epoch 580, val loss: 1.0209556818008423
Epoch 590, training loss: 630.675537109375 = 0.7514878511428833 + 100.0 * 6.299240589141846
Epoch 590, val loss: 1.0084688663482666
Epoch 600, training loss: 630.4705810546875 = 0.7328062057495117 + 100.0 * 6.297378063201904
Epoch 600, val loss: 0.9966120719909668
Epoch 610, training loss: 630.821533203125 = 0.7144336700439453 + 100.0 * 6.3010711669921875
Epoch 610, val loss: 0.985145628452301
Epoch 620, training loss: 630.2210083007812 = 0.6964612007141113 + 100.0 * 6.29524564743042
Epoch 620, val loss: 0.9739339351654053
Epoch 630, training loss: 630.632080078125 = 0.6787737607955933 + 100.0 * 6.299533367156982
Epoch 630, val loss: 0.9632891416549683
Epoch 640, training loss: 629.9007568359375 = 0.6612877249717712 + 100.0 * 6.292394638061523
Epoch 640, val loss: 0.9528442621231079
Epoch 650, training loss: 629.6121215820312 = 0.6442369818687439 + 100.0 * 6.289679050445557
Epoch 650, val loss: 0.9429989457130432
Epoch 660, training loss: 629.3778076171875 = 0.6275530457496643 + 100.0 * 6.287502288818359
Epoch 660, val loss: 0.9335278272628784
Epoch 670, training loss: 629.2221069335938 = 0.6111311912536621 + 100.0 * 6.286109447479248
Epoch 670, val loss: 0.9244444966316223
Epoch 680, training loss: 629.492919921875 = 0.5949206948280334 + 100.0 * 6.288980007171631
Epoch 680, val loss: 0.9155362844467163
Epoch 690, training loss: 629.455810546875 = 0.578872799873352 + 100.0 * 6.288769721984863
Epoch 690, val loss: 0.9068664908409119
Epoch 700, training loss: 628.89453125 = 0.5632519125938416 + 100.0 * 6.283313274383545
Epoch 700, val loss: 0.8987563252449036
Epoch 710, training loss: 628.62646484375 = 0.5477659106254578 + 100.0 * 6.280786991119385
Epoch 710, val loss: 0.8908654451370239
Epoch 720, training loss: 628.469482421875 = 0.53261798620224 + 100.0 * 6.279368877410889
Epoch 720, val loss: 0.8834445476531982
Epoch 730, training loss: 629.10595703125 = 0.5176230669021606 + 100.0 * 6.285882949829102
Epoch 730, val loss: 0.876068115234375
Epoch 740, training loss: 628.2393188476562 = 0.5029048919677734 + 100.0 * 6.2773637771606445
Epoch 740, val loss: 0.8689820766448975
Epoch 750, training loss: 628.1090087890625 = 0.4884396493434906 + 100.0 * 6.276205539703369
Epoch 750, val loss: 0.8622543811798096
Epoch 760, training loss: 627.9456176757812 = 0.47423040866851807 + 100.0 * 6.27471399307251
Epoch 760, val loss: 0.8558675050735474
Epoch 770, training loss: 628.2236938476562 = 0.4602963626384735 + 100.0 * 6.277634143829346
Epoch 770, val loss: 0.849817156791687
Epoch 780, training loss: 627.9818115234375 = 0.4466288089752197 + 100.0 * 6.275351524353027
Epoch 780, val loss: 0.8439057469367981
Epoch 790, training loss: 627.64990234375 = 0.43313175439834595 + 100.0 * 6.272168159484863
Epoch 790, val loss: 0.8382327556610107
Epoch 800, training loss: 627.3939819335938 = 0.4199988543987274 + 100.0 * 6.269740104675293
Epoch 800, val loss: 0.8328869342803955
Epoch 810, training loss: 627.4584350585938 = 0.40718111395835876 + 100.0 * 6.270512580871582
Epoch 810, val loss: 0.8278621435165405
Epoch 820, training loss: 627.3963012695312 = 0.39457905292510986 + 100.0 * 6.270017147064209
Epoch 820, val loss: 0.8235077261924744
Epoch 830, training loss: 627.0873413085938 = 0.38236358761787415 + 100.0 * 6.267049312591553
Epoch 830, val loss: 0.818884015083313
Epoch 840, training loss: 626.9534301757812 = 0.3704666793346405 + 100.0 * 6.265829563140869
Epoch 840, val loss: 0.8151199817657471
Epoch 850, training loss: 626.8341064453125 = 0.3588908314704895 + 100.0 * 6.264752388000488
Epoch 850, val loss: 0.8114697933197021
Epoch 860, training loss: 627.5988159179688 = 0.347667932510376 + 100.0 * 6.2725114822387695
Epoch 860, val loss: 0.8082923889160156
Epoch 870, training loss: 626.6653442382812 = 0.3365771770477295 + 100.0 * 6.2632880210876465
Epoch 870, val loss: 0.805081307888031
Epoch 880, training loss: 626.5798950195312 = 0.32590171694755554 + 100.0 * 6.262539386749268
Epoch 880, val loss: 0.8023661375045776
Epoch 890, training loss: 626.4358520507812 = 0.31560418009757996 + 100.0 * 6.261202335357666
Epoch 890, val loss: 0.8000500798225403
Epoch 900, training loss: 626.6337280273438 = 0.30560940504074097 + 100.0 * 6.263281345367432
Epoch 900, val loss: 0.7978718280792236
Epoch 910, training loss: 626.8276977539062 = 0.29583191871643066 + 100.0 * 6.265318870544434
Epoch 910, val loss: 0.7958325743675232
Epoch 920, training loss: 626.4353637695312 = 0.2864404618740082 + 100.0 * 6.261489391326904
Epoch 920, val loss: 0.7945657968521118
Epoch 930, training loss: 626.0584106445312 = 0.27730295062065125 + 100.0 * 6.257811069488525
Epoch 930, val loss: 0.7930842041969299
Epoch 940, training loss: 625.92138671875 = 0.2685408890247345 + 100.0 * 6.256528377532959
Epoch 940, val loss: 0.7921226620674133
Epoch 950, training loss: 625.8419799804688 = 0.2600886821746826 + 100.0 * 6.255818843841553
Epoch 950, val loss: 0.7915981411933899
Epoch 960, training loss: 626.3222045898438 = 0.2519143521785736 + 100.0 * 6.260703086853027
Epoch 960, val loss: 0.7911455035209656
Epoch 970, training loss: 626.177734375 = 0.24396772682666779 + 100.0 * 6.259337902069092
Epoch 970, val loss: 0.790870189666748
Epoch 980, training loss: 625.6332397460938 = 0.23627805709838867 + 100.0 * 6.253969669342041
Epoch 980, val loss: 0.7908640503883362
Epoch 990, training loss: 625.6848754882812 = 0.2289317101240158 + 100.0 * 6.25455904006958
Epoch 990, val loss: 0.7911359667778015
Epoch 1000, training loss: 626.4465942382812 = 0.22192054986953735 + 100.0 * 6.262246608734131
Epoch 1000, val loss: 0.7919186353683472
Epoch 1010, training loss: 625.6616821289062 = 0.2148900330066681 + 100.0 * 6.254467487335205
Epoch 1010, val loss: 0.7921085357666016
Epoch 1020, training loss: 625.3621215820312 = 0.20830751955509186 + 100.0 * 6.251537799835205
Epoch 1020, val loss: 0.7933257222175598
Epoch 1030, training loss: 625.2577514648438 = 0.2019749879837036 + 100.0 * 6.250557899475098
Epoch 1030, val loss: 0.7942025661468506
Epoch 1040, training loss: 625.1590576171875 = 0.19587792456150055 + 100.0 * 6.249631881713867
Epoch 1040, val loss: 0.7957234978675842
Epoch 1050, training loss: 625.088134765625 = 0.19001221656799316 + 100.0 * 6.248981475830078
Epoch 1050, val loss: 0.7970739006996155
Epoch 1060, training loss: 626.148681640625 = 0.18432626128196716 + 100.0 * 6.2596435546875
Epoch 1060, val loss: 0.7986090183258057
Epoch 1070, training loss: 625.5619506835938 = 0.17884425818920135 + 100.0 * 6.253830909729004
Epoch 1070, val loss: 0.8004292845726013
Epoch 1080, training loss: 624.950927734375 = 0.1734982281923294 + 100.0 * 6.247774124145508
Epoch 1080, val loss: 0.8023370504379272
Epoch 1090, training loss: 624.8560791015625 = 0.1684214025735855 + 100.0 * 6.2468767166137695
Epoch 1090, val loss: 0.8042994737625122
Epoch 1100, training loss: 624.8212280273438 = 0.16355903446674347 + 100.0 * 6.24657678604126
Epoch 1100, val loss: 0.8065576553344727
Epoch 1110, training loss: 625.1978149414062 = 0.15885323286056519 + 100.0 * 6.250389575958252
Epoch 1110, val loss: 0.8088577389717102
Epoch 1120, training loss: 624.826416015625 = 0.1542903184890747 + 100.0 * 6.246721267700195
Epoch 1120, val loss: 0.8114131093025208
Epoch 1130, training loss: 624.7365112304688 = 0.1499202698469162 + 100.0 * 6.245865821838379
Epoch 1130, val loss: 0.8138107061386108
Epoch 1140, training loss: 624.9420166015625 = 0.1457381695508957 + 100.0 * 6.247962951660156
Epoch 1140, val loss: 0.8166404366493225
Epoch 1150, training loss: 624.5269775390625 = 0.14162197709083557 + 100.0 * 6.243853569030762
Epoch 1150, val loss: 0.8190326690673828
Epoch 1160, training loss: 624.4541625976562 = 0.13770391047000885 + 100.0 * 6.243164539337158
Epoch 1160, val loss: 0.8219677805900574
Epoch 1170, training loss: 624.4176025390625 = 0.1339627355337143 + 100.0 * 6.2428364753723145
Epoch 1170, val loss: 0.825020968914032
Epoch 1180, training loss: 625.1917724609375 = 0.13036751747131348 + 100.0 * 6.250614166259766
Epoch 1180, val loss: 0.8279104232788086
Epoch 1190, training loss: 624.7584228515625 = 0.12673571705818176 + 100.0 * 6.246317386627197
Epoch 1190, val loss: 0.8310067057609558
Epoch 1200, training loss: 624.2882690429688 = 0.12334547936916351 + 100.0 * 6.241649150848389
Epoch 1200, val loss: 0.8343747854232788
Epoch 1210, training loss: 624.1813354492188 = 0.12006165087223053 + 100.0 * 6.240612983703613
Epoch 1210, val loss: 0.837704598903656
Epoch 1220, training loss: 624.3372802734375 = 0.11691517382860184 + 100.0 * 6.242203235626221
Epoch 1220, val loss: 0.8410161137580872
Epoch 1230, training loss: 624.2716674804688 = 0.11384650319814682 + 100.0 * 6.241578578948975
Epoch 1230, val loss: 0.8444786071777344
Epoch 1240, training loss: 624.1924438476562 = 0.11086766421794891 + 100.0 * 6.240816116333008
Epoch 1240, val loss: 0.8477209210395813
Epoch 1250, training loss: 624.0289306640625 = 0.10801398754119873 + 100.0 * 6.239208698272705
Epoch 1250, val loss: 0.8513710498809814
Epoch 1260, training loss: 623.9378662109375 = 0.10528715699911118 + 100.0 * 6.238326072692871
Epoch 1260, val loss: 0.8549762964248657
Epoch 1270, training loss: 624.098388671875 = 0.10265284776687622 + 100.0 * 6.239957332611084
Epoch 1270, val loss: 0.8588873147964478
Epoch 1280, training loss: 624.2921142578125 = 0.1000702977180481 + 100.0 * 6.241920471191406
Epoch 1280, val loss: 0.8620970249176025
Epoch 1290, training loss: 623.8765258789062 = 0.09751848876476288 + 100.0 * 6.237790107727051
Epoch 1290, val loss: 0.8658269047737122
Epoch 1300, training loss: 623.7777709960938 = 0.09512259811162949 + 100.0 * 6.2368268966674805
Epoch 1300, val loss: 0.8694266676902771
Epoch 1310, training loss: 623.7366333007812 = 0.0928051695227623 + 100.0 * 6.236437797546387
Epoch 1310, val loss: 0.8733844757080078
Epoch 1320, training loss: 624.2026977539062 = 0.09056221693754196 + 100.0 * 6.241121292114258
Epoch 1320, val loss: 0.8771090507507324
Epoch 1330, training loss: 623.8193359375 = 0.08835840225219727 + 100.0 * 6.237309455871582
Epoch 1330, val loss: 0.880976140499115
Epoch 1340, training loss: 623.8544311523438 = 0.08621203899383545 + 100.0 * 6.237682342529297
Epoch 1340, val loss: 0.8846181035041809
Epoch 1350, training loss: 623.7674560546875 = 0.08414801210165024 + 100.0 * 6.236833095550537
Epoch 1350, val loss: 0.8886352777481079
Epoch 1360, training loss: 623.7229614257812 = 0.08216513693332672 + 100.0 * 6.236408233642578
Epoch 1360, val loss: 0.892682671546936
Epoch 1370, training loss: 623.5291137695312 = 0.08021780103445053 + 100.0 * 6.2344889640808105
Epoch 1370, val loss: 0.8965239524841309
Epoch 1380, training loss: 623.5006713867188 = 0.07836224883794785 + 100.0 * 6.234222888946533
Epoch 1380, val loss: 0.900508463382721
Epoch 1390, training loss: 623.8126220703125 = 0.07654476165771484 + 100.0 * 6.237360954284668
Epoch 1390, val loss: 0.9044454097747803
Epoch 1400, training loss: 623.50634765625 = 0.07477334141731262 + 100.0 * 6.234315872192383
Epoch 1400, val loss: 0.9083392024040222
Epoch 1410, training loss: 623.3433837890625 = 0.073065385222435 + 100.0 * 6.23270320892334
Epoch 1410, val loss: 0.9126895070075989
Epoch 1420, training loss: 623.4109497070312 = 0.07141126692295074 + 100.0 * 6.233395576477051
Epoch 1420, val loss: 0.9167578220367432
Epoch 1430, training loss: 623.748779296875 = 0.06979271024465561 + 100.0 * 6.236790180206299
Epoch 1430, val loss: 0.9204722046852112
Epoch 1440, training loss: 623.259521484375 = 0.06823057681322098 + 100.0 * 6.231912612915039
Epoch 1440, val loss: 0.9248889684677124
Epoch 1450, training loss: 623.1617431640625 = 0.06671438366174698 + 100.0 * 6.230950355529785
Epoch 1450, val loss: 0.9287941455841064
Epoch 1460, training loss: 623.14013671875 = 0.06524442881345749 + 100.0 * 6.230748653411865
Epoch 1460, val loss: 0.9330592155456543
Epoch 1470, training loss: 623.5585327148438 = 0.06382790207862854 + 100.0 * 6.2349467277526855
Epoch 1470, val loss: 0.937191903591156
Epoch 1480, training loss: 623.2234497070312 = 0.06244881451129913 + 100.0 * 6.23160982131958
Epoch 1480, val loss: 0.9411225318908691
Epoch 1490, training loss: 623.22119140625 = 0.061094436794519424 + 100.0 * 6.231600761413574
Epoch 1490, val loss: 0.9453791975975037
Epoch 1500, training loss: 623.2427978515625 = 0.059793516993522644 + 100.0 * 6.23183012008667
Epoch 1500, val loss: 0.9494550824165344
Epoch 1510, training loss: 623.215576171875 = 0.058503348380327225 + 100.0 * 6.231570720672607
Epoch 1510, val loss: 0.9535410404205322
Epoch 1520, training loss: 623.0311279296875 = 0.05725443735718727 + 100.0 * 6.229738712310791
Epoch 1520, val loss: 0.9574407339096069
Epoch 1530, training loss: 622.944091796875 = 0.05605628713965416 + 100.0 * 6.228880405426025
Epoch 1530, val loss: 0.9617358446121216
Epoch 1540, training loss: 622.9847412109375 = 0.05490163713693619 + 100.0 * 6.229298114776611
Epoch 1540, val loss: 0.9657129645347595
Epoch 1550, training loss: 623.0039672851562 = 0.053771112114191055 + 100.0 * 6.229502201080322
Epoch 1550, val loss: 0.969932496547699
Epoch 1560, training loss: 623.005859375 = 0.05264890938997269 + 100.0 * 6.229531764984131
Epoch 1560, val loss: 0.9739620685577393
Epoch 1570, training loss: 622.739501953125 = 0.05154408887028694 + 100.0 * 6.226879596710205
Epoch 1570, val loss: 0.9780133366584778
Epoch 1580, training loss: 622.6887817382812 = 0.05051009729504585 + 100.0 * 6.226382255554199
Epoch 1580, val loss: 0.9820716977119446
Epoch 1590, training loss: 622.907470703125 = 0.04950818791985512 + 100.0 * 6.228579521179199
Epoch 1590, val loss: 0.9862859845161438
Epoch 1600, training loss: 622.71240234375 = 0.048507992178201675 + 100.0 * 6.2266387939453125
Epoch 1600, val loss: 0.9902629852294922
Epoch 1610, training loss: 622.6847534179688 = 0.04752977937459946 + 100.0 * 6.226371765136719
Epoch 1610, val loss: 0.9943662285804749
Epoch 1620, training loss: 622.5814819335938 = 0.04659804329276085 + 100.0 * 6.225349426269531
Epoch 1620, val loss: 0.9984327554702759
Epoch 1630, training loss: 622.68115234375 = 0.04568680748343468 + 100.0 * 6.226354122161865
Epoch 1630, val loss: 1.002503514289856
Epoch 1640, training loss: 622.9053955078125 = 0.04479429870843887 + 100.0 * 6.228606224060059
Epoch 1640, val loss: 1.0062512159347534
Epoch 1650, training loss: 622.7232055664062 = 0.043912000954151154 + 100.0 * 6.22679328918457
Epoch 1650, val loss: 1.0101227760314941
Epoch 1660, training loss: 622.5068969726562 = 0.043069027364254 + 100.0 * 6.224637985229492
Epoch 1660, val loss: 1.0142807960510254
Epoch 1670, training loss: 622.4038696289062 = 0.042248234152793884 + 100.0 * 6.223616600036621
Epoch 1670, val loss: 1.018491268157959
Epoch 1680, training loss: 622.761474609375 = 0.04145445674657822 + 100.0 * 6.227200031280518
Epoch 1680, val loss: 1.0223900079727173
Epoch 1690, training loss: 622.5943603515625 = 0.04066665843129158 + 100.0 * 6.225537300109863
Epoch 1690, val loss: 1.025969386100769
Epoch 1700, training loss: 622.3668212890625 = 0.03989702835679054 + 100.0 * 6.223268985748291
Epoch 1700, val loss: 1.030067801475525
Epoch 1710, training loss: 622.4041748046875 = 0.03915746882557869 + 100.0 * 6.223649978637695
Epoch 1710, val loss: 1.0340936183929443
Epoch 1720, training loss: 622.9083251953125 = 0.038430046290159225 + 100.0 * 6.22869873046875
Epoch 1720, val loss: 1.0376204252243042
Epoch 1730, training loss: 622.2785034179688 = 0.03772846609354019 + 100.0 * 6.222407817840576
Epoch 1730, val loss: 1.041877269744873
Epoch 1740, training loss: 622.2262573242188 = 0.037043020129203796 + 100.0 * 6.221892356872559
Epoch 1740, val loss: 1.045666217803955
Epoch 1750, training loss: 622.4635620117188 = 0.036373719573020935 + 100.0 * 6.224271774291992
Epoch 1750, val loss: 1.049654483795166
Epoch 1760, training loss: 622.323974609375 = 0.03571716696023941 + 100.0 * 6.2228827476501465
Epoch 1760, val loss: 1.0533291101455688
Epoch 1770, training loss: 622.2922973632812 = 0.03506479412317276 + 100.0 * 6.222572326660156
Epoch 1770, val loss: 1.0569941997528076
Epoch 1780, training loss: 622.10791015625 = 0.03444695845246315 + 100.0 * 6.220734596252441
Epoch 1780, val loss: 1.0610493421554565
Epoch 1790, training loss: 622.0623168945312 = 0.033848363906145096 + 100.0 * 6.220284461975098
Epoch 1790, val loss: 1.0649304389953613
Epoch 1800, training loss: 622.3555908203125 = 0.03327309340238571 + 100.0 * 6.2232232093811035
Epoch 1800, val loss: 1.0688891410827637
Epoch 1810, training loss: 622.5401000976562 = 0.03268907591700554 + 100.0 * 6.22507381439209
Epoch 1810, val loss: 1.0718566179275513
Epoch 1820, training loss: 622.2393798828125 = 0.032096125185489655 + 100.0 * 6.222072601318359
Epoch 1820, val loss: 1.0763503313064575
Epoch 1830, training loss: 622.0696411132812 = 0.03153827413916588 + 100.0 * 6.220380783081055
Epoch 1830, val loss: 1.0792738199234009
Epoch 1840, training loss: 621.9373779296875 = 0.031006237491965294 + 100.0 * 6.219063758850098
Epoch 1840, val loss: 1.083345651626587
Epoch 1850, training loss: 622.0037231445312 = 0.030489090830087662 + 100.0 * 6.219732284545898
Epoch 1850, val loss: 1.0869873762130737
Epoch 1860, training loss: 622.4803466796875 = 0.02997788041830063 + 100.0 * 6.224503993988037
Epoch 1860, val loss: 1.0904083251953125
Epoch 1870, training loss: 622.0783081054688 = 0.029475372284650803 + 100.0 * 6.22048807144165
Epoch 1870, val loss: 1.0944265127182007
Epoch 1880, training loss: 622.016845703125 = 0.028978493064641953 + 100.0 * 6.219878673553467
Epoch 1880, val loss: 1.0978593826293945
Epoch 1890, training loss: 622.5104370117188 = 0.02850211411714554 + 100.0 * 6.224819183349609
Epoch 1890, val loss: 1.1014574766159058
Epoch 1900, training loss: 621.9857788085938 = 0.02803768776357174 + 100.0 * 6.219577789306641
Epoch 1900, val loss: 1.1051692962646484
Epoch 1910, training loss: 621.800048828125 = 0.027576982975006104 + 100.0 * 6.217724323272705
Epoch 1910, val loss: 1.1087058782577515
Epoch 1920, training loss: 621.8098754882812 = 0.027140995487570763 + 100.0 * 6.217827320098877
Epoch 1920, val loss: 1.1122797727584839
Epoch 1930, training loss: 622.2103881835938 = 0.026715997606515884 + 100.0 * 6.221836566925049
Epoch 1930, val loss: 1.1155253648757935
Epoch 1940, training loss: 621.919921875 = 0.026276856660842896 + 100.0 * 6.218935966491699
Epoch 1940, val loss: 1.119349718093872
Epoch 1950, training loss: 622.5050048828125 = 0.02585682086646557 + 100.0 * 6.224791049957275
Epoch 1950, val loss: 1.1225192546844482
Epoch 1960, training loss: 621.9738159179688 = 0.025449922308325768 + 100.0 * 6.219483375549316
Epoch 1960, val loss: 1.1264609098434448
Epoch 1970, training loss: 621.7172241210938 = 0.025045806542038918 + 100.0 * 6.216922283172607
Epoch 1970, val loss: 1.1296952962875366
Epoch 1980, training loss: 621.6444702148438 = 0.024662962183356285 + 100.0 * 6.216197967529297
Epoch 1980, val loss: 1.1333892345428467
Epoch 1990, training loss: 621.9957885742188 = 0.024291623383760452 + 100.0 * 6.219715118408203
Epoch 1990, val loss: 1.1368480920791626
Epoch 2000, training loss: 621.5990600585938 = 0.023913873359560966 + 100.0 * 6.2157511711120605
Epoch 2000, val loss: 1.1400455236434937
Epoch 2010, training loss: 621.5892944335938 = 0.023549724370241165 + 100.0 * 6.2156572341918945
Epoch 2010, val loss: 1.14327871799469
Epoch 2020, training loss: 621.7595825195312 = 0.023202357813715935 + 100.0 * 6.2173638343811035
Epoch 2020, val loss: 1.146819829940796
Epoch 2030, training loss: 621.8406982421875 = 0.02285659685730934 + 100.0 * 6.218178749084473
Epoch 2030, val loss: 1.1501003503799438
Epoch 2040, training loss: 621.6240234375 = 0.022499285638332367 + 100.0 * 6.216014862060547
Epoch 2040, val loss: 1.153320550918579
Epoch 2050, training loss: 621.5479125976562 = 0.022172974422574043 + 100.0 * 6.21525764465332
Epoch 2050, val loss: 1.1567491292953491
Epoch 2060, training loss: 621.7454833984375 = 0.021854041144251823 + 100.0 * 6.217236042022705
Epoch 2060, val loss: 1.1603537797927856
Epoch 2070, training loss: 621.4737548828125 = 0.02153290994465351 + 100.0 * 6.214521884918213
Epoch 2070, val loss: 1.1632243394851685
Epoch 2080, training loss: 621.4819946289062 = 0.02121877297759056 + 100.0 * 6.2146077156066895
Epoch 2080, val loss: 1.1664010286331177
Epoch 2090, training loss: 622.0621337890625 = 0.0209189523011446 + 100.0 * 6.220412254333496
Epoch 2090, val loss: 1.1693031787872314
Epoch 2100, training loss: 621.8151245117188 = 0.020607680082321167 + 100.0 * 6.217945098876953
Epoch 2100, val loss: 1.1732016801834106
Epoch 2110, training loss: 621.692626953125 = 0.020309390500187874 + 100.0 * 6.2167229652404785
Epoch 2110, val loss: 1.1757383346557617
Epoch 2120, training loss: 621.339599609375 = 0.020015129819512367 + 100.0 * 6.21319580078125
Epoch 2120, val loss: 1.1793049573898315
Epoch 2130, training loss: 621.339599609375 = 0.019737834110856056 + 100.0 * 6.213198661804199
Epoch 2130, val loss: 1.1823344230651855
Epoch 2140, training loss: 621.6554565429688 = 0.019466182217001915 + 100.0 * 6.216359615325928
Epoch 2140, val loss: 1.1854373216629028
Epoch 2150, training loss: 621.3314819335938 = 0.0191958025097847 + 100.0 * 6.213122844696045
Epoch 2150, val loss: 1.1884843111038208
Epoch 2160, training loss: 621.5181274414062 = 0.018936770036816597 + 100.0 * 6.214992046356201
Epoch 2160, val loss: 1.1914938688278198
Epoch 2170, training loss: 621.7954711914062 = 0.018674274906516075 + 100.0 * 6.21776819229126
Epoch 2170, val loss: 1.194685935974121
Epoch 2180, training loss: 621.3265991210938 = 0.018417393788695335 + 100.0 * 6.2130818367004395
Epoch 2180, val loss: 1.1979210376739502
Epoch 2190, training loss: 621.1859741210938 = 0.018168173730373383 + 100.0 * 6.2116780281066895
Epoch 2190, val loss: 1.2007983922958374
Epoch 2200, training loss: 621.1953125 = 0.01792985014617443 + 100.0 * 6.211773872375488
Epoch 2200, val loss: 1.2039803266525269
Epoch 2210, training loss: 621.5962524414062 = 0.017703760415315628 + 100.0 * 6.215785503387451
Epoch 2210, val loss: 1.206924319267273
Epoch 2220, training loss: 621.3445434570312 = 0.0174610186368227 + 100.0 * 6.213270664215088
Epoch 2220, val loss: 1.2100211381912231
Epoch 2230, training loss: 621.2367553710938 = 0.017224948853254318 + 100.0 * 6.21219539642334
Epoch 2230, val loss: 1.212719440460205
Epoch 2240, training loss: 621.3152465820312 = 0.01699962094426155 + 100.0 * 6.212982177734375
Epoch 2240, val loss: 1.2156126499176025
Epoch 2250, training loss: 621.1217651367188 = 0.016778960824012756 + 100.0 * 6.211050033569336
Epoch 2250, val loss: 1.21848464012146
Epoch 2260, training loss: 621.1157836914062 = 0.0165646281093359 + 100.0 * 6.210992336273193
Epoch 2260, val loss: 1.2214412689208984
Epoch 2270, training loss: 621.4002685546875 = 0.016357170417904854 + 100.0 * 6.213839054107666
Epoch 2270, val loss: 1.2238688468933105
Epoch 2280, training loss: 621.2138061523438 = 0.0161447711288929 + 100.0 * 6.211976051330566
Epoch 2280, val loss: 1.2270077466964722
Epoch 2290, training loss: 621.4782104492188 = 0.015935970470309258 + 100.0 * 6.214622974395752
Epoch 2290, val loss: 1.2299997806549072
Epoch 2300, training loss: 621.0294189453125 = 0.01573566533625126 + 100.0 * 6.210136890411377
Epoch 2300, val loss: 1.2328096628189087
Epoch 2310, training loss: 621.0081787109375 = 0.015540247783064842 + 100.0 * 6.209926128387451
Epoch 2310, val loss: 1.2353674173355103
Epoch 2320, training loss: 621.1309204101562 = 0.015349678695201874 + 100.0 * 6.211155891418457
Epoch 2320, val loss: 1.2383463382720947
Epoch 2330, training loss: 621.1864624023438 = 0.015160075388848782 + 100.0 * 6.2117133140563965
Epoch 2330, val loss: 1.2411500215530396
Epoch 2340, training loss: 621.1372680664062 = 0.01497521623969078 + 100.0 * 6.211223125457764
Epoch 2340, val loss: 1.243688702583313
Epoch 2350, training loss: 621.0364990234375 = 0.014791562221944332 + 100.0 * 6.210216999053955
Epoch 2350, val loss: 1.2463346719741821
Epoch 2360, training loss: 621.30810546875 = 0.014613688923418522 + 100.0 * 6.212935447692871
Epoch 2360, val loss: 1.249149203300476
Epoch 2370, training loss: 620.9252319335938 = 0.014435297809541225 + 100.0 * 6.209107398986816
Epoch 2370, val loss: 1.2518181800842285
Epoch 2380, training loss: 621.0621337890625 = 0.014264159835875034 + 100.0 * 6.210478782653809
Epoch 2380, val loss: 1.2543669939041138
Epoch 2390, training loss: 621.3390502929688 = 0.014093589037656784 + 100.0 * 6.213249683380127
Epoch 2390, val loss: 1.2567319869995117
Epoch 2400, training loss: 620.9024047851562 = 0.013929491862654686 + 100.0 * 6.2088847160339355
Epoch 2400, val loss: 1.2594690322875977
Epoch 2410, training loss: 620.9698486328125 = 0.013768336735665798 + 100.0 * 6.209560394287109
Epoch 2410, val loss: 1.262039303779602
Epoch 2420, training loss: 620.872802734375 = 0.013606562279164791 + 100.0 * 6.208591938018799
Epoch 2420, val loss: 1.2647064924240112
Epoch 2430, training loss: 620.9945068359375 = 0.013453182764351368 + 100.0 * 6.209810733795166
Epoch 2430, val loss: 1.2673219442367554
Epoch 2440, training loss: 621.1697998046875 = 0.013301284983754158 + 100.0 * 6.211565017700195
Epoch 2440, val loss: 1.2703473567962646
Epoch 2450, training loss: 620.9046630859375 = 0.013144438154995441 + 100.0 * 6.2089152336120605
Epoch 2450, val loss: 1.2724673748016357
Epoch 2460, training loss: 620.7679443359375 = 0.012990336865186691 + 100.0 * 6.207549571990967
Epoch 2460, val loss: 1.2747997045516968
Epoch 2470, training loss: 620.7124633789062 = 0.012847897596657276 + 100.0 * 6.206996440887451
Epoch 2470, val loss: 1.2777166366577148
Epoch 2480, training loss: 620.8283081054688 = 0.012709437869489193 + 100.0 * 6.208156108856201
Epoch 2480, val loss: 1.2802505493164062
Epoch 2490, training loss: 621.1712036132812 = 0.012571179307997227 + 100.0 * 6.2115864753723145
Epoch 2490, val loss: 1.282655119895935
Epoch 2500, training loss: 620.7965087890625 = 0.012425234541296959 + 100.0 * 6.207840442657471
Epoch 2500, val loss: 1.2846885919570923
Epoch 2510, training loss: 620.7061157226562 = 0.012289009988307953 + 100.0 * 6.206938743591309
Epoch 2510, val loss: 1.2873157262802124
Epoch 2520, training loss: 621.5816650390625 = 0.012164561077952385 + 100.0 * 6.215694904327393
Epoch 2520, val loss: 1.2896229028701782
Epoch 2530, training loss: 621.002197265625 = 0.012020873837172985 + 100.0 * 6.209901809692383
Epoch 2530, val loss: 1.2919914722442627
Epoch 2540, training loss: 620.7610473632812 = 0.011884618550539017 + 100.0 * 6.207491397857666
Epoch 2540, val loss: 1.2944259643554688
Epoch 2550, training loss: 620.6283569335938 = 0.011759592220187187 + 100.0 * 6.206165790557861
Epoch 2550, val loss: 1.2969099283218384
Epoch 2560, training loss: 620.5765991210938 = 0.011638565920293331 + 100.0 * 6.205649375915527
Epoch 2560, val loss: 1.2990754842758179
Epoch 2570, training loss: 620.57861328125 = 0.011520096100866795 + 100.0 * 6.2056708335876465
Epoch 2570, val loss: 1.3015351295471191
Epoch 2580, training loss: 621.5474853515625 = 0.011404805816709995 + 100.0 * 6.215360641479492
Epoch 2580, val loss: 1.3036507368087769
Epoch 2590, training loss: 621.1236572265625 = 0.011275678873062134 + 100.0 * 6.211123466491699
Epoch 2590, val loss: 1.3064888715744019
Epoch 2600, training loss: 620.715576171875 = 0.011157345958054066 + 100.0 * 6.2070441246032715
Epoch 2600, val loss: 1.308058738708496
Epoch 2610, training loss: 620.52490234375 = 0.0110422782599926 + 100.0 * 6.205138683319092
Epoch 2610, val loss: 1.3108065128326416
Epoch 2620, training loss: 620.6029663085938 = 0.010934785008430481 + 100.0 * 6.205920219421387
Epoch 2620, val loss: 1.3128907680511475
Epoch 2630, training loss: 620.7858276367188 = 0.01082789245992899 + 100.0 * 6.20775032043457
Epoch 2630, val loss: 1.3149490356445312
Epoch 2640, training loss: 620.7244262695312 = 0.01071665994822979 + 100.0 * 6.207136631011963
Epoch 2640, val loss: 1.3177800178527832
Epoch 2650, training loss: 621.1144409179688 = 0.01061219908297062 + 100.0 * 6.211038112640381
Epoch 2650, val loss: 1.3197969198226929
Epoch 2660, training loss: 620.5960083007812 = 0.010494818910956383 + 100.0 * 6.205855369567871
Epoch 2660, val loss: 1.3219484090805054
Epoch 2670, training loss: 620.4932250976562 = 0.010394436307251453 + 100.0 * 6.204828262329102
Epoch 2670, val loss: 1.3243337869644165
Epoch 2680, training loss: 620.5592041015625 = 0.010295169427990913 + 100.0 * 6.205489158630371
Epoch 2680, val loss: 1.3263863325119019
Epoch 2690, training loss: 620.5308227539062 = 0.010195733979344368 + 100.0 * 6.205206394195557
Epoch 2690, val loss: 1.3285363912582397
Epoch 2700, training loss: 620.4754638671875 = 0.010096237994730473 + 100.0 * 6.204653739929199
Epoch 2700, val loss: 1.3308836221694946
Epoch 2710, training loss: 620.7770385742188 = 0.010001963935792446 + 100.0 * 6.207670211791992
Epoch 2710, val loss: 1.333246111869812
Epoch 2720, training loss: 620.519287109375 = 0.009905468672513962 + 100.0 * 6.205093860626221
Epoch 2720, val loss: 1.3347439765930176
Epoch 2730, training loss: 620.3510131835938 = 0.00980891939252615 + 100.0 * 6.203412055969238
Epoch 2730, val loss: 1.3371500968933105
Epoch 2740, training loss: 620.6342163085938 = 0.009716821834445 + 100.0 * 6.206244945526123
Epoch 2740, val loss: 1.3394126892089844
Epoch 2750, training loss: 620.8564453125 = 0.009621149860322475 + 100.0 * 6.208467960357666
Epoch 2750, val loss: 1.3409335613250732
Epoch 2760, training loss: 620.43701171875 = 0.009525883011519909 + 100.0 * 6.204274654388428
Epoch 2760, val loss: 1.3434574604034424
Epoch 2770, training loss: 620.4075317382812 = 0.009437428787350655 + 100.0 * 6.203980445861816
Epoch 2770, val loss: 1.3450969457626343
Epoch 2780, training loss: 620.2938232421875 = 0.009350258857011795 + 100.0 * 6.202845096588135
Epoch 2780, val loss: 1.347398281097412
Epoch 2790, training loss: 620.280517578125 = 0.009267241694033146 + 100.0 * 6.202712535858154
Epoch 2790, val loss: 1.3493766784667969
Epoch 2800, training loss: 620.4664916992188 = 0.009185275062918663 + 100.0 * 6.204573154449463
Epoch 2800, val loss: 1.351301908493042
Epoch 2810, training loss: 620.5209350585938 = 0.009102906100451946 + 100.0 * 6.205118179321289
Epoch 2810, val loss: 1.3531533479690552
Epoch 2820, training loss: 620.5311279296875 = 0.009022973477840424 + 100.0 * 6.205220699310303
Epoch 2820, val loss: 1.354990839958191
Epoch 2830, training loss: 620.7241821289062 = 0.008940775878727436 + 100.0 * 6.207152843475342
Epoch 2830, val loss: 1.3572710752487183
Epoch 2840, training loss: 620.3993530273438 = 0.008858505636453629 + 100.0 * 6.20390510559082
Epoch 2840, val loss: 1.3593357801437378
Epoch 2850, training loss: 620.2932739257812 = 0.008778692223131657 + 100.0 * 6.202845096588135
Epoch 2850, val loss: 1.3608665466308594
Epoch 2860, training loss: 620.2711181640625 = 0.00870373286306858 + 100.0 * 6.2026238441467285
Epoch 2860, val loss: 1.3631279468536377
Epoch 2870, training loss: 620.4326171875 = 0.00863117165863514 + 100.0 * 6.204239845275879
Epoch 2870, val loss: 1.365060567855835
Epoch 2880, training loss: 620.3228759765625 = 0.008554340340197086 + 100.0 * 6.20314359664917
Epoch 2880, val loss: 1.3668197393417358
Epoch 2890, training loss: 620.3507690429688 = 0.008479473181068897 + 100.0 * 6.203422546386719
Epoch 2890, val loss: 1.3688161373138428
Epoch 2900, training loss: 620.2102661132812 = 0.008406244218349457 + 100.0 * 6.2020182609558105
Epoch 2900, val loss: 1.3707469701766968
Epoch 2910, training loss: 620.4025268554688 = 0.008336103521287441 + 100.0 * 6.203941822052002
Epoch 2910, val loss: 1.3726342916488647
Epoch 2920, training loss: 620.3032836914062 = 0.008265703916549683 + 100.0 * 6.202950477600098
Epoch 2920, val loss: 1.3737682104110718
Epoch 2930, training loss: 620.25146484375 = 0.008196896873414516 + 100.0 * 6.202432632446289
Epoch 2930, val loss: 1.3757672309875488
Epoch 2940, training loss: 620.2927856445312 = 0.008126681670546532 + 100.0 * 6.202846527099609
Epoch 2940, val loss: 1.3777148723602295
Epoch 2950, training loss: 620.5247802734375 = 0.008061055094003677 + 100.0 * 6.205167293548584
Epoch 2950, val loss: 1.3796203136444092
Epoch 2960, training loss: 620.0978393554688 = 0.007989701814949512 + 100.0 * 6.200898170471191
Epoch 2960, val loss: 1.38118577003479
Epoch 2970, training loss: 620.1946411132812 = 0.007923147641122341 + 100.0 * 6.20186710357666
Epoch 2970, val loss: 1.3829960823059082
Epoch 2980, training loss: 620.1975708007812 = 0.007860679179430008 + 100.0 * 6.201897144317627
Epoch 2980, val loss: 1.384617567062378
Epoch 2990, training loss: 620.2238159179688 = 0.007798470556735992 + 100.0 * 6.202159881591797
Epoch 2990, val loss: 1.3864730596542358
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 861.6285400390625 = 1.9439753293991089 + 100.0 * 8.596845626831055
Epoch 0, val loss: 1.9488399028778076
Epoch 10, training loss: 861.5404663085938 = 1.9360229969024658 + 100.0 * 8.596044540405273
Epoch 10, val loss: 1.9407457113265991
Epoch 20, training loss: 860.9674682617188 = 1.9261302947998047 + 100.0 * 8.590413093566895
Epoch 20, val loss: 1.9304592609405518
Epoch 30, training loss: 856.9842529296875 = 1.913063406944275 + 100.0 * 8.550711631774902
Epoch 30, val loss: 1.916807770729065
Epoch 40, training loss: 832.8341064453125 = 1.8959592580795288 + 100.0 * 8.309381484985352
Epoch 40, val loss: 1.8993152379989624
Epoch 50, training loss: 775.2540283203125 = 1.8747754096984863 + 100.0 * 7.733792781829834
Epoch 50, val loss: 1.877740502357483
Epoch 60, training loss: 745.4962158203125 = 1.8557876348495483 + 100.0 * 7.436404228210449
Epoch 60, val loss: 1.8597526550292969
Epoch 70, training loss: 716.1246337890625 = 1.8423817157745361 + 100.0 * 7.142822265625
Epoch 70, val loss: 1.8473554849624634
Epoch 80, training loss: 694.7719116210938 = 1.8314125537872314 + 100.0 * 6.9294047355651855
Epoch 80, val loss: 1.8364639282226562
Epoch 90, training loss: 683.2785034179688 = 1.820476770401001 + 100.0 * 6.814579963684082
Epoch 90, val loss: 1.825871229171753
Epoch 100, training loss: 676.204833984375 = 1.8091100454330444 + 100.0 * 6.74395751953125
Epoch 100, val loss: 1.8149946928024292
Epoch 110, training loss: 670.3438720703125 = 1.798624873161316 + 100.0 * 6.685452461242676
Epoch 110, val loss: 1.8046637773513794
Epoch 120, training loss: 665.3333740234375 = 1.7885990142822266 + 100.0 * 6.635447978973389
Epoch 120, val loss: 1.7946501970291138
Epoch 130, training loss: 662.5338134765625 = 1.778146505355835 + 100.0 * 6.6075568199157715
Epoch 130, val loss: 1.784398078918457
Epoch 140, training loss: 659.4736328125 = 1.766798734664917 + 100.0 * 6.577068328857422
Epoch 140, val loss: 1.7734816074371338
Epoch 150, training loss: 656.9976196289062 = 1.7548125982284546 + 100.0 * 6.552427768707275
Epoch 150, val loss: 1.7622506618499756
Epoch 160, training loss: 654.7388305664062 = 1.742316722869873 + 100.0 * 6.529965400695801
Epoch 160, val loss: 1.7506524324417114
Epoch 170, training loss: 652.7931518554688 = 1.7287896871566772 + 100.0 * 6.51064395904541
Epoch 170, val loss: 1.7383290529251099
Epoch 180, training loss: 651.005615234375 = 1.7140882015228271 + 100.0 * 6.492915153503418
Epoch 180, val loss: 1.7251182794570923
Epoch 190, training loss: 649.3701782226562 = 1.6981219053268433 + 100.0 * 6.476720809936523
Epoch 190, val loss: 1.7109670639038086
Epoch 200, training loss: 648.2305908203125 = 1.6807695627212524 + 100.0 * 6.465497970581055
Epoch 200, val loss: 1.6956775188446045
Epoch 210, training loss: 646.6412353515625 = 1.6620334386825562 + 100.0 * 6.44979190826416
Epoch 210, val loss: 1.6792480945587158
Epoch 220, training loss: 645.4021606445312 = 1.64206063747406 + 100.0 * 6.437600612640381
Epoch 220, val loss: 1.6617811918258667
Epoch 230, training loss: 644.39013671875 = 1.6207740306854248 + 100.0 * 6.4276933670043945
Epoch 230, val loss: 1.6433305740356445
Epoch 240, training loss: 643.3054809570312 = 1.5981930494308472 + 100.0 * 6.417072772979736
Epoch 240, val loss: 1.6238175630569458
Epoch 250, training loss: 642.3256225585938 = 1.5744270086288452 + 100.0 * 6.407512187957764
Epoch 250, val loss: 1.6033350229263306
Epoch 260, training loss: 641.4683227539062 = 1.549680471420288 + 100.0 * 6.399186611175537
Epoch 260, val loss: 1.582131028175354
Epoch 270, training loss: 640.850830078125 = 1.5240178108215332 + 100.0 * 6.393267631530762
Epoch 270, val loss: 1.5602971315383911
Epoch 280, training loss: 640.3621215820312 = 1.4976491928100586 + 100.0 * 6.388645172119141
Epoch 280, val loss: 1.5381730794906616
Epoch 290, training loss: 639.5093994140625 = 1.4706217050552368 + 100.0 * 6.380387783050537
Epoch 290, val loss: 1.5156151056289673
Epoch 300, training loss: 638.9337158203125 = 1.4434207677841187 + 100.0 * 6.374902725219727
Epoch 300, val loss: 1.4930000305175781
Epoch 310, training loss: 638.6873779296875 = 1.4161419868469238 + 100.0 * 6.372712135314941
Epoch 310, val loss: 1.470637559890747
Epoch 320, training loss: 638.0171508789062 = 1.3887662887573242 + 100.0 * 6.366283416748047
Epoch 320, val loss: 1.448464035987854
Epoch 330, training loss: 637.411376953125 = 1.3616811037063599 + 100.0 * 6.360496997833252
Epoch 330, val loss: 1.4265488386154175
Epoch 340, training loss: 637.071044921875 = 1.3348875045776367 + 100.0 * 6.357361316680908
Epoch 340, val loss: 1.4050638675689697
Epoch 350, training loss: 636.8287353515625 = 1.308297872543335 + 100.0 * 6.3552045822143555
Epoch 350, val loss: 1.383914589881897
Epoch 360, training loss: 636.2400512695312 = 1.2820528745651245 + 100.0 * 6.34958028793335
Epoch 360, val loss: 1.3631898164749146
Epoch 370, training loss: 635.8262939453125 = 1.256199598312378 + 100.0 * 6.345700740814209
Epoch 370, val loss: 1.342864990234375
Epoch 380, training loss: 635.5516967773438 = 1.2306634187698364 + 100.0 * 6.343210220336914
Epoch 380, val loss: 1.3228024244308472
Epoch 390, training loss: 635.2305297851562 = 1.2054752111434937 + 100.0 * 6.340250492095947
Epoch 390, val loss: 1.3032325506210327
Epoch 400, training loss: 634.92431640625 = 1.1807069778442383 + 100.0 * 6.337436199188232
Epoch 400, val loss: 1.2839878797531128
Epoch 410, training loss: 634.5694580078125 = 1.1562451124191284 + 100.0 * 6.334132194519043
Epoch 410, val loss: 1.2650413513183594
Epoch 420, training loss: 634.2698974609375 = 1.1321282386779785 + 100.0 * 6.3313775062561035
Epoch 420, val loss: 1.2465370893478394
Epoch 430, training loss: 633.8482666015625 = 1.1084290742874146 + 100.0 * 6.32739782333374
Epoch 430, val loss: 1.2284384965896606
Epoch 440, training loss: 634.0709838867188 = 1.0851019620895386 + 100.0 * 6.329858779907227
Epoch 440, val loss: 1.21058189868927
Epoch 450, training loss: 633.4850463867188 = 1.0619112253189087 + 100.0 * 6.3242316246032715
Epoch 450, val loss: 1.1929336786270142
Epoch 460, training loss: 633.0435180664062 = 1.03900945186615 + 100.0 * 6.320044994354248
Epoch 460, val loss: 1.175679087638855
Epoch 470, training loss: 632.6740112304688 = 1.016494870185852 + 100.0 * 6.316575527191162
Epoch 470, val loss: 1.158760666847229
Epoch 480, training loss: 632.6132202148438 = 0.9942619204521179 + 100.0 * 6.316189765930176
Epoch 480, val loss: 1.1421678066253662
Epoch 490, training loss: 632.4215087890625 = 0.9721301794052124 + 100.0 * 6.3144941329956055
Epoch 490, val loss: 1.1256122589111328
Epoch 500, training loss: 632.1146240234375 = 0.9502679705619812 + 100.0 * 6.311643600463867
Epoch 500, val loss: 1.1094145774841309
Epoch 510, training loss: 631.6815795898438 = 0.9287149906158447 + 100.0 * 6.307528495788574
Epoch 510, val loss: 1.0936071872711182
Epoch 520, training loss: 631.3333740234375 = 0.9073519706726074 + 100.0 * 6.30426025390625
Epoch 520, val loss: 1.0779882669448853
Epoch 530, training loss: 631.5774536132812 = 0.8863106966018677 + 100.0 * 6.306911468505859
Epoch 530, val loss: 1.0627729892730713
Epoch 540, training loss: 631.0182495117188 = 0.8653923273086548 + 100.0 * 6.301528453826904
Epoch 540, val loss: 1.047974705696106
Epoch 550, training loss: 630.7035522460938 = 0.8448523879051208 + 100.0 * 6.298587322235107
Epoch 550, val loss: 1.0333664417266846
Epoch 560, training loss: 630.9513549804688 = 0.8246209621429443 + 100.0 * 6.301267147064209
Epoch 560, val loss: 1.019216537475586
Epoch 570, training loss: 630.2822875976562 = 0.8047505617141724 + 100.0 * 6.294775485992432
Epoch 570, val loss: 1.0054876804351807
Epoch 580, training loss: 629.9750366210938 = 0.7852047085762024 + 100.0 * 6.291898250579834
Epoch 580, val loss: 0.9921804666519165
Epoch 590, training loss: 629.9293212890625 = 0.7660638093948364 + 100.0 * 6.291632652282715
Epoch 590, val loss: 0.9793965816497803
Epoch 600, training loss: 630.1275024414062 = 0.7472571134567261 + 100.0 * 6.293802261352539
Epoch 600, val loss: 0.9669431447982788
Epoch 610, training loss: 629.4971313476562 = 0.7286221981048584 + 100.0 * 6.287684917449951
Epoch 610, val loss: 0.955004870891571
Epoch 620, training loss: 629.270751953125 = 0.7105549573898315 + 100.0 * 6.285601615905762
Epoch 620, val loss: 0.9436092972755432
Epoch 630, training loss: 629.1007080078125 = 0.6928585171699524 + 100.0 * 6.284078598022461
Epoch 630, val loss: 0.9327371120452881
Epoch 640, training loss: 629.3065795898438 = 0.6755606532096863 + 100.0 * 6.286310195922852
Epoch 640, val loss: 0.9223129749298096
Epoch 650, training loss: 629.2239379882812 = 0.6584488749504089 + 100.0 * 6.2856550216674805
Epoch 650, val loss: 0.9124882221221924
Epoch 660, training loss: 628.6994018554688 = 0.6417956948280334 + 100.0 * 6.280576229095459
Epoch 660, val loss: 0.9028071165084839
Epoch 670, training loss: 628.5579833984375 = 0.6255476474761963 + 100.0 * 6.279324054718018
Epoch 670, val loss: 0.8936781287193298
Epoch 680, training loss: 628.5013427734375 = 0.6096358895301819 + 100.0 * 6.27891731262207
Epoch 680, val loss: 0.8851123452186584
Epoch 690, training loss: 628.2797241210938 = 0.5939725637435913 + 100.0 * 6.276857376098633
Epoch 690, val loss: 0.876910388469696
Epoch 700, training loss: 628.2396240234375 = 0.578647792339325 + 100.0 * 6.276609420776367
Epoch 700, val loss: 0.8690358996391296
Epoch 710, training loss: 628.007568359375 = 0.5635968446731567 + 100.0 * 6.274439811706543
Epoch 710, val loss: 0.8616021871566772
Epoch 720, training loss: 627.8409423828125 = 0.5488538146018982 + 100.0 * 6.272921085357666
Epoch 720, val loss: 0.8546692728996277
Epoch 730, training loss: 627.842041015625 = 0.5343889594078064 + 100.0 * 6.27307653427124
Epoch 730, val loss: 0.8480440974235535
Epoch 740, training loss: 628.1005859375 = 0.5201737880706787 + 100.0 * 6.275803565979004
Epoch 740, val loss: 0.8416448831558228
Epoch 750, training loss: 627.6297607421875 = 0.5061932802200317 + 100.0 * 6.271235942840576
Epoch 750, val loss: 0.8354212045669556
Epoch 760, training loss: 627.3699340820312 = 0.4924963414669037 + 100.0 * 6.268774509429932
Epoch 760, val loss: 0.8297387957572937
Epoch 770, training loss: 627.2139892578125 = 0.4790436625480652 + 100.0 * 6.2673492431640625
Epoch 770, val loss: 0.8244019746780396
Epoch 780, training loss: 627.1454467773438 = 0.4658505618572235 + 100.0 * 6.266796112060547
Epoch 780, val loss: 0.8193917870521545
Epoch 790, training loss: 627.23876953125 = 0.45282667875289917 + 100.0 * 6.26785945892334
Epoch 790, val loss: 0.8145331144332886
Epoch 800, training loss: 627.4140014648438 = 0.44002577662467957 + 100.0 * 6.269740104675293
Epoch 800, val loss: 0.8100482225418091
Epoch 810, training loss: 626.9425659179688 = 0.4274320900440216 + 100.0 * 6.265151500701904
Epoch 810, val loss: 0.8056241869926453
Epoch 820, training loss: 626.764892578125 = 0.4151057302951813 + 100.0 * 6.263497829437256
Epoch 820, val loss: 0.8015895485877991
Epoch 830, training loss: 626.7241821289062 = 0.4030008018016815 + 100.0 * 6.263211727142334
Epoch 830, val loss: 0.7978030443191528
Epoch 840, training loss: 626.5806884765625 = 0.39115282893180847 + 100.0 * 6.261895179748535
Epoch 840, val loss: 0.7942001819610596
Epoch 850, training loss: 626.8916015625 = 0.37954995036125183 + 100.0 * 6.265120029449463
Epoch 850, val loss: 0.7911121249198914
Epoch 860, training loss: 626.697509765625 = 0.36814919114112854 + 100.0 * 6.263293743133545
Epoch 860, val loss: 0.7878873348236084
Epoch 870, training loss: 626.1475219726562 = 0.35705623030662537 + 100.0 * 6.257904529571533
Epoch 870, val loss: 0.7852659821510315
Epoch 880, training loss: 626.1898193359375 = 0.34622493386268616 + 100.0 * 6.2584357261657715
Epoch 880, val loss: 0.782985508441925
Epoch 890, training loss: 626.449951171875 = 0.3356515169143677 + 100.0 * 6.261143207550049
Epoch 890, val loss: 0.7809025645256042
Epoch 900, training loss: 625.9476928710938 = 0.3254232108592987 + 100.0 * 6.256222724914551
Epoch 900, val loss: 0.7786910533905029
Epoch 910, training loss: 625.7938842773438 = 0.31541362404823303 + 100.0 * 6.25478458404541
Epoch 910, val loss: 0.7771457433700562
Epoch 920, training loss: 626.4985961914062 = 0.30571138858795166 + 100.0 * 6.261928558349609
Epoch 920, val loss: 0.7760039567947388
Epoch 930, training loss: 625.7667846679688 = 0.29626837372779846 + 100.0 * 6.254705429077148
Epoch 930, val loss: 0.7743492126464844
Epoch 940, training loss: 625.7079467773438 = 0.2871033847332001 + 100.0 * 6.254208087921143
Epoch 940, val loss: 0.773554801940918
Epoch 950, training loss: 625.5929565429688 = 0.2782217860221863 + 100.0 * 6.253147602081299
Epoch 950, val loss: 0.7727881669998169
Epoch 960, training loss: 625.4133911132812 = 0.2695860266685486 + 100.0 * 6.251438140869141
Epoch 960, val loss: 0.7723377346992493
Epoch 970, training loss: 625.4375610351562 = 0.2612655460834503 + 100.0 * 6.251762390136719
Epoch 970, val loss: 0.7720520496368408
Epoch 980, training loss: 625.5278930664062 = 0.25322917103767395 + 100.0 * 6.25274658203125
Epoch 980, val loss: 0.7719667553901672
Epoch 990, training loss: 625.3431396484375 = 0.24536383152008057 + 100.0 * 6.250977993011475
Epoch 990, val loss: 0.7718498110771179
Epoch 1000, training loss: 625.2615966796875 = 0.23781448602676392 + 100.0 * 6.250237941741943
Epoch 1000, val loss: 0.7722804546356201
Epoch 1010, training loss: 625.0211181640625 = 0.23048566281795502 + 100.0 * 6.247906684875488
Epoch 1010, val loss: 0.7726381421089172
Epoch 1020, training loss: 624.9999389648438 = 0.22347797453403473 + 100.0 * 6.247764587402344
Epoch 1020, val loss: 0.7731961011886597
Epoch 1030, training loss: 625.5006103515625 = 0.21667830646038055 + 100.0 * 6.2528395652771
Epoch 1030, val loss: 0.7739453315734863
Epoch 1040, training loss: 624.8233642578125 = 0.2100406289100647 + 100.0 * 6.246133327484131
Epoch 1040, val loss: 0.774874746799469
Epoch 1050, training loss: 624.8065795898438 = 0.20367582142353058 + 100.0 * 6.246028900146484
Epoch 1050, val loss: 0.7763928174972534
Epoch 1060, training loss: 624.702880859375 = 0.19757483899593353 + 100.0 * 6.245052814483643
Epoch 1060, val loss: 0.7775875926017761
Epoch 1070, training loss: 624.9358520507812 = 0.19166094064712524 + 100.0 * 6.247442245483398
Epoch 1070, val loss: 0.7789151668548584
Epoch 1080, training loss: 624.6981201171875 = 0.18596912920475006 + 100.0 * 6.245121479034424
Epoch 1080, val loss: 0.7806880474090576
Epoch 1090, training loss: 624.5543823242188 = 0.18043044209480286 + 100.0 * 6.243739604949951
Epoch 1090, val loss: 0.7824153304100037
Epoch 1100, training loss: 624.9923706054688 = 0.17513157427310944 + 100.0 * 6.248172760009766
Epoch 1100, val loss: 0.78415447473526
Epoch 1110, training loss: 624.5546875 = 0.16991984844207764 + 100.0 * 6.243847846984863
Epoch 1110, val loss: 0.7865232825279236
Epoch 1120, training loss: 624.3087768554688 = 0.1649211347103119 + 100.0 * 6.241438865661621
Epoch 1120, val loss: 0.7884123921394348
Epoch 1130, training loss: 624.4443359375 = 0.16010235249996185 + 100.0 * 6.242842197418213
Epoch 1130, val loss: 0.7908515930175781
Epoch 1140, training loss: 624.23779296875 = 0.1554398089647293 + 100.0 * 6.240823268890381
Epoch 1140, val loss: 0.7931835055351257
Epoch 1150, training loss: 624.3617553710938 = 0.15094852447509766 + 100.0 * 6.24210786819458
Epoch 1150, val loss: 0.7955725193023682
Epoch 1160, training loss: 624.0907592773438 = 0.1465679556131363 + 100.0 * 6.239442348480225
Epoch 1160, val loss: 0.7982600331306458
Epoch 1170, training loss: 624.0621948242188 = 0.1423506885766983 + 100.0 * 6.239198684692383
Epoch 1170, val loss: 0.8009292483329773
Epoch 1180, training loss: 624.186767578125 = 0.13830287754535675 + 100.0 * 6.240484714508057
Epoch 1180, val loss: 0.8037394881248474
Epoch 1190, training loss: 624.0003051757812 = 0.1343565136194229 + 100.0 * 6.238659858703613
Epoch 1190, val loss: 0.8063067197799683
Epoch 1200, training loss: 623.8375244140625 = 0.13052934408187866 + 100.0 * 6.237069606781006
Epoch 1200, val loss: 0.8092946410179138
Epoch 1210, training loss: 623.9857177734375 = 0.12686477601528168 + 100.0 * 6.238588333129883
Epoch 1210, val loss: 0.8121095895767212
Epoch 1220, training loss: 623.791015625 = 0.12328733503818512 + 100.0 * 6.236677169799805
Epoch 1220, val loss: 0.8153387904167175
Epoch 1230, training loss: 623.8425903320312 = 0.11983997374773026 + 100.0 * 6.237227916717529
Epoch 1230, val loss: 0.8186556696891785
Epoch 1240, training loss: 623.7691650390625 = 0.1165134385228157 + 100.0 * 6.2365264892578125
Epoch 1240, val loss: 0.821766197681427
Epoch 1250, training loss: 624.2884521484375 = 0.11328348517417908 + 100.0 * 6.241751670837402
Epoch 1250, val loss: 0.8252461552619934
Epoch 1260, training loss: 623.6932983398438 = 0.11016761511564255 + 100.0 * 6.235831260681152
Epoch 1260, val loss: 0.8278992176055908
Epoch 1270, training loss: 623.5115356445312 = 0.1071634292602539 + 100.0 * 6.234043598175049
Epoch 1270, val loss: 0.8314877152442932
Epoch 1280, training loss: 623.4315795898438 = 0.10426800698041916 + 100.0 * 6.233273029327393
Epoch 1280, val loss: 0.8347363471984863
Epoch 1290, training loss: 623.6550903320312 = 0.10147711634635925 + 100.0 * 6.235536098480225
Epoch 1290, val loss: 0.8381835222244263
Epoch 1300, training loss: 623.4371337890625 = 0.09873861819505692 + 100.0 * 6.233383655548096
Epoch 1300, val loss: 0.8419477343559265
Epoch 1310, training loss: 623.4248046875 = 0.09609100967645645 + 100.0 * 6.233287334442139
Epoch 1310, val loss: 0.8451935648918152
Epoch 1320, training loss: 623.68408203125 = 0.09354151785373688 + 100.0 * 6.235905170440674
Epoch 1320, val loss: 0.8488763570785522
Epoch 1330, training loss: 623.2910766601562 = 0.09104567021131516 + 100.0 * 6.232000350952148
Epoch 1330, val loss: 0.8526715636253357
Epoch 1340, training loss: 623.1937866210938 = 0.08865255117416382 + 100.0 * 6.231051445007324
Epoch 1340, val loss: 0.8560194373130798
Epoch 1350, training loss: 623.306396484375 = 0.08633928745985031 + 100.0 * 6.2322001457214355
Epoch 1350, val loss: 0.8600246906280518
Epoch 1360, training loss: 623.2514038085938 = 0.08410443365573883 + 100.0 * 6.231673240661621
Epoch 1360, val loss: 0.8636457920074463
Epoch 1370, training loss: 623.2713012695312 = 0.08193439245223999 + 100.0 * 6.231894016265869
Epoch 1370, val loss: 0.8673166632652283
Epoch 1380, training loss: 623.1638793945312 = 0.07983133941888809 + 100.0 * 6.230840682983398
Epoch 1380, val loss: 0.8708297610282898
Epoch 1390, training loss: 623.1871337890625 = 0.07779614627361298 + 100.0 * 6.231093406677246
Epoch 1390, val loss: 0.8745267391204834
Epoch 1400, training loss: 623.1010131835938 = 0.07581854611635208 + 100.0 * 6.230252265930176
Epoch 1400, val loss: 0.8780105113983154
Epoch 1410, training loss: 623.0316162109375 = 0.07389076054096222 + 100.0 * 6.22957706451416
Epoch 1410, val loss: 0.881758987903595
Epoch 1420, training loss: 623.2700805664062 = 0.07203635573387146 + 100.0 * 6.231980323791504
Epoch 1420, val loss: 0.8855279684066772
Epoch 1430, training loss: 622.9176025390625 = 0.07024767249822617 + 100.0 * 6.228473663330078
Epoch 1430, val loss: 0.8896620869636536
Epoch 1440, training loss: 622.7876586914062 = 0.06850558519363403 + 100.0 * 6.227191925048828
Epoch 1440, val loss: 0.8931366205215454
Epoch 1450, training loss: 622.8092041015625 = 0.0668415054678917 + 100.0 * 6.227423667907715
Epoch 1450, val loss: 0.8969942331314087
Epoch 1460, training loss: 623.4857788085938 = 0.06521192193031311 + 100.0 * 6.234206199645996
Epoch 1460, val loss: 0.9009294509887695
Epoch 1470, training loss: 623.0235595703125 = 0.06363793462514877 + 100.0 * 6.2295989990234375
Epoch 1470, val loss: 0.9040268659591675
Epoch 1480, training loss: 623.0140380859375 = 0.06210596486926079 + 100.0 * 6.229518890380859
Epoch 1480, val loss: 0.9079017639160156
Epoch 1490, training loss: 622.842529296875 = 0.06061818450689316 + 100.0 * 6.227818965911865
Epoch 1490, val loss: 0.9113086462020874
Epoch 1500, training loss: 622.6129760742188 = 0.05916896462440491 + 100.0 * 6.22553825378418
Epoch 1500, val loss: 0.9154496192932129
Epoch 1510, training loss: 622.5663452148438 = 0.05777605250477791 + 100.0 * 6.225085258483887
Epoch 1510, val loss: 0.9194958806037903
Epoch 1520, training loss: 622.760986328125 = 0.05643525719642639 + 100.0 * 6.22704553604126
Epoch 1520, val loss: 0.9235140085220337
Epoch 1530, training loss: 622.81591796875 = 0.055126339197158813 + 100.0 * 6.2276082038879395
Epoch 1530, val loss: 0.9267630577087402
Epoch 1540, training loss: 622.510986328125 = 0.05384747311472893 + 100.0 * 6.224571228027344
Epoch 1540, val loss: 0.9303558468818665
Epoch 1550, training loss: 622.5440673828125 = 0.0526147224009037 + 100.0 * 6.22491455078125
Epoch 1550, val loss: 0.9342475533485413
Epoch 1560, training loss: 623.08056640625 = 0.05143820494413376 + 100.0 * 6.230291366577148
Epoch 1560, val loss: 0.9377170205116272
Epoch 1570, training loss: 622.5607299804688 = 0.05024772509932518 + 100.0 * 6.225104808807373
Epoch 1570, val loss: 0.941545844078064
Epoch 1580, training loss: 622.3462524414062 = 0.04912792146205902 + 100.0 * 6.222971439361572
Epoch 1580, val loss: 0.9453732371330261
Epoch 1590, training loss: 622.3009033203125 = 0.04803699627518654 + 100.0 * 6.222528457641602
Epoch 1590, val loss: 0.9492246508598328
Epoch 1600, training loss: 622.7299194335938 = 0.04698600247502327 + 100.0 * 6.2268290519714355
Epoch 1600, val loss: 0.953262209892273
Epoch 1610, training loss: 622.4440307617188 = 0.045961227267980576 + 100.0 * 6.22398042678833
Epoch 1610, val loss: 0.9562845826148987
Epoch 1620, training loss: 622.2727661132812 = 0.044957324862480164 + 100.0 * 6.222278118133545
Epoch 1620, val loss: 0.960112988948822
Epoch 1630, training loss: 622.3240356445312 = 0.04399305209517479 + 100.0 * 6.222800254821777
Epoch 1630, val loss: 0.9634907841682434
Epoch 1640, training loss: 622.5314331054688 = 0.043058548122644424 + 100.0 * 6.224884033203125
Epoch 1640, val loss: 0.9675806164741516
Epoch 1650, training loss: 622.5691528320312 = 0.04214661568403244 + 100.0 * 6.225269794464111
Epoch 1650, val loss: 0.971262514591217
Epoch 1660, training loss: 622.24658203125 = 0.041249386966228485 + 100.0 * 6.222053050994873
Epoch 1660, val loss: 0.9746803641319275
Epoch 1670, training loss: 622.1282958984375 = 0.04038703441619873 + 100.0 * 6.220879077911377
Epoch 1670, val loss: 0.9782631397247314
Epoch 1680, training loss: 622.0396118164062 = 0.03955453261733055 + 100.0 * 6.220000743865967
Epoch 1680, val loss: 0.9816069602966309
Epoch 1690, training loss: 622.4829711914062 = 0.03876428306102753 + 100.0 * 6.224442005157471
Epoch 1690, val loss: 0.9848255515098572
Epoch 1700, training loss: 622.1525268554688 = 0.037957727909088135 + 100.0 * 6.2211456298828125
Epoch 1700, val loss: 0.9895707964897156
Epoch 1710, training loss: 622.0842895507812 = 0.03718416020274162 + 100.0 * 6.220470905303955
Epoch 1710, val loss: 0.9920442700386047
Epoch 1720, training loss: 622.14208984375 = 0.036434322595596313 + 100.0 * 6.2210564613342285
Epoch 1720, val loss: 0.9964405298233032
Epoch 1730, training loss: 622.1568603515625 = 0.03570674732327461 + 100.0 * 6.2212114334106445
Epoch 1730, val loss: 0.9995417594909668
Epoch 1740, training loss: 622.1572265625 = 0.03500160574913025 + 100.0 * 6.221222400665283
Epoch 1740, val loss: 1.002868413925171
Epoch 1750, training loss: 621.8772583007812 = 0.034313954412937164 + 100.0 * 6.2184295654296875
Epoch 1750, val loss: 1.006306529045105
Epoch 1760, training loss: 622.0054931640625 = 0.03365684300661087 + 100.0 * 6.2197184562683105
Epoch 1760, val loss: 1.0099300146102905
Epoch 1770, training loss: 622.0453491210938 = 0.03300853073596954 + 100.0 * 6.220123291015625
Epoch 1770, val loss: 1.0130503177642822
Epoch 1780, training loss: 621.9109497070312 = 0.03236456587910652 + 100.0 * 6.218785762786865
Epoch 1780, val loss: 1.0168529748916626
Epoch 1790, training loss: 621.9073486328125 = 0.031754862517118454 + 100.0 * 6.218756198883057
Epoch 1790, val loss: 1.0202350616455078
Epoch 1800, training loss: 622.0971069335938 = 0.031156666576862335 + 100.0 * 6.2206597328186035
Epoch 1800, val loss: 1.023392915725708
Epoch 1810, training loss: 621.8215942382812 = 0.030575627461075783 + 100.0 * 6.217910289764404
Epoch 1810, val loss: 1.0266869068145752
Epoch 1820, training loss: 621.9307861328125 = 0.030015815049409866 + 100.0 * 6.21900749206543
Epoch 1820, val loss: 1.0301916599273682
Epoch 1830, training loss: 621.8096923828125 = 0.029455101117491722 + 100.0 * 6.217802047729492
Epoch 1830, val loss: 1.0333251953125
Epoch 1840, training loss: 621.7058715820312 = 0.028915250673890114 + 100.0 * 6.216769218444824
Epoch 1840, val loss: 1.0364989042282104
Epoch 1850, training loss: 621.7939453125 = 0.028397047892212868 + 100.0 * 6.217655658721924
Epoch 1850, val loss: 1.0399644374847412
Epoch 1860, training loss: 621.946533203125 = 0.027890052646398544 + 100.0 * 6.219186305999756
Epoch 1860, val loss: 1.0436935424804688
Epoch 1870, training loss: 621.6958618164062 = 0.027383705601096153 + 100.0 * 6.216684341430664
Epoch 1870, val loss: 1.0459927320480347
Epoch 1880, training loss: 621.5516967773438 = 0.026900658383965492 + 100.0 * 6.215247631072998
Epoch 1880, val loss: 1.0496143102645874
Epoch 1890, training loss: 621.5486450195312 = 0.026432901620864868 + 100.0 * 6.215222358703613
Epoch 1890, val loss: 1.053035855293274
Epoch 1900, training loss: 622.0084228515625 = 0.02597629278898239 + 100.0 * 6.219824314117432
Epoch 1900, val loss: 1.0565682649612427
Epoch 1910, training loss: 621.7255859375 = 0.025524012744426727 + 100.0 * 6.217000961303711
Epoch 1910, val loss: 1.058897852897644
Epoch 1920, training loss: 621.6898803710938 = 0.025085553526878357 + 100.0 * 6.216648101806641
Epoch 1920, val loss: 1.0626329183578491
Epoch 1930, training loss: 621.4845581054688 = 0.024660611525177956 + 100.0 * 6.214599132537842
Epoch 1930, val loss: 1.0651001930236816
Epoch 1940, training loss: 621.44482421875 = 0.024251678958535194 + 100.0 * 6.214206218719482
Epoch 1940, val loss: 1.068027138710022
Epoch 1950, training loss: 621.3423461914062 = 0.023849191144108772 + 100.0 * 6.213184833526611
Epoch 1950, val loss: 1.0711718797683716
Epoch 1960, training loss: 621.4979858398438 = 0.023462332785129547 + 100.0 * 6.21474552154541
Epoch 1960, val loss: 1.0743740797042847
Epoch 1970, training loss: 621.590576171875 = 0.023075446486473083 + 100.0 * 6.215675354003906
Epoch 1970, val loss: 1.077246904373169
Epoch 1980, training loss: 621.5081176757812 = 0.02269125171005726 + 100.0 * 6.2148542404174805
Epoch 1980, val loss: 1.0806342363357544
Epoch 1990, training loss: 621.3082275390625 = 0.022321734577417374 + 100.0 * 6.2128586769104
Epoch 1990, val loss: 1.0829166173934937
Epoch 2000, training loss: 621.489013671875 = 0.02196432836353779 + 100.0 * 6.214670658111572
Epoch 2000, val loss: 1.0861459970474243
Epoch 2010, training loss: 621.5382690429688 = 0.021613404154777527 + 100.0 * 6.2151665687561035
Epoch 2010, val loss: 1.0891594886779785
Epoch 2020, training loss: 621.3056640625 = 0.021277550607919693 + 100.0 * 6.212843418121338
Epoch 2020, val loss: 1.0919854640960693
Epoch 2030, training loss: 621.1817016601562 = 0.02094144932925701 + 100.0 * 6.211607933044434
Epoch 2030, val loss: 1.0946511030197144
Epoch 2040, training loss: 621.5731201171875 = 0.02061743661761284 + 100.0 * 6.215525150299072
Epoch 2040, val loss: 1.098454236984253
Epoch 2050, training loss: 621.3648681640625 = 0.020303474739193916 + 100.0 * 6.213445663452148
Epoch 2050, val loss: 1.1006556749343872
Epoch 2060, training loss: 621.2064819335938 = 0.01998862437903881 + 100.0 * 6.211864948272705
Epoch 2060, val loss: 1.1030628681182861
Epoch 2070, training loss: 621.1928100585938 = 0.019685549661517143 + 100.0 * 6.21173095703125
Epoch 2070, val loss: 1.106011986732483
Epoch 2080, training loss: 621.5693969726562 = 0.019391033798456192 + 100.0 * 6.2154998779296875
Epoch 2080, val loss: 1.1087101697921753
Epoch 2090, training loss: 621.1282348632812 = 0.019103912636637688 + 100.0 * 6.2110915184021
Epoch 2090, val loss: 1.1116305589675903
Epoch 2100, training loss: 621.071044921875 = 0.01881551556289196 + 100.0 * 6.210522651672363
Epoch 2100, val loss: 1.1144732236862183
Epoch 2110, training loss: 621.5928344726562 = 0.01854836940765381 + 100.0 * 6.215743064880371
Epoch 2110, val loss: 1.1165413856506348
Epoch 2120, training loss: 621.5621948242188 = 0.018279287964105606 + 100.0 * 6.215439319610596
Epoch 2120, val loss: 1.1197504997253418
Epoch 2130, training loss: 621.1265869140625 = 0.01799919456243515 + 100.0 * 6.211085796356201
Epoch 2130, val loss: 1.1224836111068726
Epoch 2140, training loss: 621.0179443359375 = 0.017742931842803955 + 100.0 * 6.2100019454956055
Epoch 2140, val loss: 1.125207781791687
Epoch 2150, training loss: 621.085205078125 = 0.017491698265075684 + 100.0 * 6.210677146911621
Epoch 2150, val loss: 1.1276590824127197
Epoch 2160, training loss: 621.20068359375 = 0.01724870875477791 + 100.0 * 6.211833953857422
Epoch 2160, val loss: 1.130866289138794
Epoch 2170, training loss: 621.0194702148438 = 0.017000235617160797 + 100.0 * 6.210024356842041
Epoch 2170, val loss: 1.1329833269119263
Epoch 2180, training loss: 620.9177856445312 = 0.016767436638474464 + 100.0 * 6.209010124206543
Epoch 2180, val loss: 1.1352829933166504
Epoch 2190, training loss: 621.1532592773438 = 0.016544675454497337 + 100.0 * 6.211367130279541
Epoch 2190, val loss: 1.1375441551208496
Epoch 2200, training loss: 621.0264892578125 = 0.016310447826981544 + 100.0 * 6.210102081298828
Epoch 2200, val loss: 1.1409072875976562
Epoch 2210, training loss: 621.224853515625 = 0.01608874276280403 + 100.0 * 6.212087154388428
Epoch 2210, val loss: 1.1434054374694824
Epoch 2220, training loss: 621.096923828125 = 0.01586879789829254 + 100.0 * 6.210810661315918
Epoch 2220, val loss: 1.1450845003128052
Epoch 2230, training loss: 620.8178100585938 = 0.01564849726855755 + 100.0 * 6.208022117614746
Epoch 2230, val loss: 1.1481544971466064
Epoch 2240, training loss: 620.7423706054688 = 0.015443997457623482 + 100.0 * 6.207269191741943
Epoch 2240, val loss: 1.1503913402557373
Epoch 2250, training loss: 620.8026123046875 = 0.015239214524626732 + 100.0 * 6.207873344421387
Epoch 2250, val loss: 1.1529574394226074
Epoch 2260, training loss: 621.357666015625 = 0.015045546926558018 + 100.0 * 6.213425636291504
Epoch 2260, val loss: 1.155592679977417
Epoch 2270, training loss: 620.8634643554688 = 0.014847427606582642 + 100.0 * 6.208486557006836
Epoch 2270, val loss: 1.1576396226882935
Epoch 2280, training loss: 620.777587890625 = 0.014650830067694187 + 100.0 * 6.207629680633545
Epoch 2280, val loss: 1.160192608833313
Epoch 2290, training loss: 620.9789428710938 = 0.014463745057582855 + 100.0 * 6.209644794464111
Epoch 2290, val loss: 1.1627185344696045
Epoch 2300, training loss: 620.9097900390625 = 0.014277108944952488 + 100.0 * 6.20895528793335
Epoch 2300, val loss: 1.1654704809188843
Epoch 2310, training loss: 620.7794189453125 = 0.014097549021244049 + 100.0 * 6.207653045654297
Epoch 2310, val loss: 1.166623830795288
Epoch 2320, training loss: 620.7761840820312 = 0.013918697834014893 + 100.0 * 6.207622528076172
Epoch 2320, val loss: 1.1692771911621094
Epoch 2330, training loss: 620.70947265625 = 0.013743185438215733 + 100.0 * 6.2069573402404785
Epoch 2330, val loss: 1.1716792583465576
Epoch 2340, training loss: 620.6965942382812 = 0.013573653995990753 + 100.0 * 6.2068305015563965
Epoch 2340, val loss: 1.1737184524536133
Epoch 2350, training loss: 620.7633056640625 = 0.013410250656306744 + 100.0 * 6.207499027252197
Epoch 2350, val loss: 1.1758390665054321
Epoch 2360, training loss: 621.1376342773438 = 0.013246339745819569 + 100.0 * 6.211244106292725
Epoch 2360, val loss: 1.1789159774780273
Epoch 2370, training loss: 620.650390625 = 0.013083054684102535 + 100.0 * 6.20637321472168
Epoch 2370, val loss: 1.1806775331497192
Epoch 2380, training loss: 620.5318603515625 = 0.012928012758493423 + 100.0 * 6.205189228057861
Epoch 2380, val loss: 1.1827470064163208
Epoch 2390, training loss: 620.72998046875 = 0.01277492567896843 + 100.0 * 6.207172393798828
Epoch 2390, val loss: 1.184934139251709
Epoch 2400, training loss: 620.8224487304688 = 0.012627876363694668 + 100.0 * 6.208098411560059
Epoch 2400, val loss: 1.1873266696929932
Epoch 2410, training loss: 620.6698608398438 = 0.012467405758798122 + 100.0 * 6.206573963165283
Epoch 2410, val loss: 1.1899302005767822
Epoch 2420, training loss: 620.5875854492188 = 0.012327796779572964 + 100.0 * 6.205752849578857
Epoch 2420, val loss: 1.1912468671798706
Epoch 2430, training loss: 620.6885986328125 = 0.012183082289993763 + 100.0 * 6.206764221191406
Epoch 2430, val loss: 1.194107174873352
Epoch 2440, training loss: 620.4983520507812 = 0.01204350870102644 + 100.0 * 6.20486307144165
Epoch 2440, val loss: 1.1959576606750488
Epoch 2450, training loss: 620.7213745117188 = 0.011907350271940231 + 100.0 * 6.207094669342041
Epoch 2450, val loss: 1.1975408792495728
Epoch 2460, training loss: 620.799560546875 = 0.011772812344133854 + 100.0 * 6.2078776359558105
Epoch 2460, val loss: 1.199872612953186
Epoch 2470, training loss: 620.6329345703125 = 0.011636951006948948 + 100.0 * 6.206212997436523
Epoch 2470, val loss: 1.2024270296096802
Epoch 2480, training loss: 620.5631713867188 = 0.011506001465022564 + 100.0 * 6.205516338348389
Epoch 2480, val loss: 1.2046873569488525
Epoch 2490, training loss: 620.4108276367188 = 0.01137683168053627 + 100.0 * 6.2039947509765625
Epoch 2490, val loss: 1.205918312072754
Epoch 2500, training loss: 620.3417358398438 = 0.011250721290707588 + 100.0 * 6.203305244445801
Epoch 2500, val loss: 1.2085180282592773
Epoch 2510, training loss: 620.8096923828125 = 0.01112938579171896 + 100.0 * 6.207985877990723
Epoch 2510, val loss: 1.2104650735855103
Epoch 2520, training loss: 620.5336303710938 = 0.011011053808033466 + 100.0 * 6.205226421356201
Epoch 2520, val loss: 1.2116339206695557
Epoch 2530, training loss: 620.3115234375 = 0.010889041237533092 + 100.0 * 6.203006744384766
Epoch 2530, val loss: 1.2145216464996338
Epoch 2540, training loss: 620.410400390625 = 0.010770904831588268 + 100.0 * 6.203996658325195
Epoch 2540, val loss: 1.2164257764816284
Epoch 2550, training loss: 620.5789794921875 = 0.010656990110874176 + 100.0 * 6.20568323135376
Epoch 2550, val loss: 1.218656063079834
Epoch 2560, training loss: 620.2513427734375 = 0.010541042312979698 + 100.0 * 6.2024078369140625
Epoch 2560, val loss: 1.219857931137085
Epoch 2570, training loss: 620.2573852539062 = 0.010433153249323368 + 100.0 * 6.202469348907471
Epoch 2570, val loss: 1.2217991352081299
Epoch 2580, training loss: 620.3541870117188 = 0.010328680276870728 + 100.0 * 6.203438758850098
Epoch 2580, val loss: 1.2232980728149414
Epoch 2590, training loss: 620.6866455078125 = 0.010226503014564514 + 100.0 * 6.206764221191406
Epoch 2590, val loss: 1.2258163690567017
Epoch 2600, training loss: 620.5489501953125 = 0.01011404674500227 + 100.0 * 6.205388069152832
Epoch 2600, val loss: 1.2276618480682373
Epoch 2610, training loss: 620.3959350585938 = 0.010008160956203938 + 100.0 * 6.203859329223633
Epoch 2610, val loss: 1.229339361190796
Epoch 2620, training loss: 620.21728515625 = 0.009906210005283356 + 100.0 * 6.20207405090332
Epoch 2620, val loss: 1.2314380407333374
Epoch 2630, training loss: 620.1702880859375 = 0.009805481880903244 + 100.0 * 6.20160436630249
Epoch 2630, val loss: 1.2333098649978638
Epoch 2640, training loss: 620.3652954101562 = 0.009707942605018616 + 100.0 * 6.203556060791016
Epoch 2640, val loss: 1.2354762554168701
Epoch 2650, training loss: 620.35400390625 = 0.009613627567887306 + 100.0 * 6.20344352722168
Epoch 2650, val loss: 1.2369893789291382
Epoch 2660, training loss: 620.34912109375 = 0.009522272273898125 + 100.0 * 6.203395843505859
Epoch 2660, val loss: 1.2384001016616821
Epoch 2670, training loss: 620.4290161132812 = 0.00942343007773161 + 100.0 * 6.204196453094482
Epoch 2670, val loss: 1.2407457828521729
Epoch 2680, training loss: 620.3173828125 = 0.009332082234323025 + 100.0 * 6.203080177307129
Epoch 2680, val loss: 1.242678165435791
Epoch 2690, training loss: 620.163330078125 = 0.009241591207683086 + 100.0 * 6.201540946960449
Epoch 2690, val loss: 1.2436617612838745
Epoch 2700, training loss: 620.116943359375 = 0.009153817780315876 + 100.0 * 6.201077938079834
Epoch 2700, val loss: 1.2457400560379028
Epoch 2710, training loss: 620.2047119140625 = 0.009067491628229618 + 100.0 * 6.201956272125244
Epoch 2710, val loss: 1.247591257095337
Epoch 2720, training loss: 620.423828125 = 0.008983641862869263 + 100.0 * 6.204148292541504
Epoch 2720, val loss: 1.249704360961914
Epoch 2730, training loss: 620.170166015625 = 0.008896091021597385 + 100.0 * 6.20161247253418
Epoch 2730, val loss: 1.2509865760803223
Epoch 2740, training loss: 620.0992431640625 = 0.008813424967229366 + 100.0 * 6.200904369354248
Epoch 2740, val loss: 1.2529754638671875
Epoch 2750, training loss: 620.3640747070312 = 0.008732324466109276 + 100.0 * 6.203553199768066
Epoch 2750, val loss: 1.2541595697402954
Epoch 2760, training loss: 620.244140625 = 0.008651304990053177 + 100.0 * 6.202354431152344
Epoch 2760, val loss: 1.2559071779251099
Epoch 2770, training loss: 620.0631713867188 = 0.008573024533689022 + 100.0 * 6.2005462646484375
Epoch 2770, val loss: 1.257535457611084
Epoch 2780, training loss: 620.0313110351562 = 0.00849488191306591 + 100.0 * 6.200228214263916
Epoch 2780, val loss: 1.259163498878479
Epoch 2790, training loss: 620.2752075195312 = 0.008422269485890865 + 100.0 * 6.202667713165283
Epoch 2790, val loss: 1.2605493068695068
Epoch 2800, training loss: 619.9796142578125 = 0.008344126865267754 + 100.0 * 6.199712753295898
Epoch 2800, val loss: 1.2626539468765259
Epoch 2810, training loss: 619.975830078125 = 0.008269472047686577 + 100.0 * 6.199676036834717
Epoch 2810, val loss: 1.2649459838867188
Epoch 2820, training loss: 620.2416381835938 = 0.008197520859539509 + 100.0 * 6.202334403991699
Epoch 2820, val loss: 1.2668776512145996
Epoch 2830, training loss: 620.16552734375 = 0.008125001564621925 + 100.0 * 6.201574325561523
Epoch 2830, val loss: 1.2674165964126587
Epoch 2840, training loss: 620.227294921875 = 0.008057060651481152 + 100.0 * 6.202192306518555
Epoch 2840, val loss: 1.2691161632537842
Epoch 2850, training loss: 619.9535522460938 = 0.007983251474797726 + 100.0 * 6.199455261230469
Epoch 2850, val loss: 1.2705804109573364
Epoch 2860, training loss: 619.8798828125 = 0.00791612733155489 + 100.0 * 6.198719501495361
Epoch 2860, val loss: 1.2721056938171387
Epoch 2870, training loss: 619.8949584960938 = 0.007849857211112976 + 100.0 * 6.19887113571167
Epoch 2870, val loss: 1.2739251852035522
Epoch 2880, training loss: 620.0433349609375 = 0.00778786838054657 + 100.0 * 6.200355529785156
Epoch 2880, val loss: 1.275071620941162
Epoch 2890, training loss: 620.1780395507812 = 0.007721965201199055 + 100.0 * 6.20170259475708
Epoch 2890, val loss: 1.2768385410308838
Epoch 2900, training loss: 620.1835327148438 = 0.007651783060282469 + 100.0 * 6.201759338378906
Epoch 2900, val loss: 1.27862548828125
Epoch 2910, training loss: 620.0161743164062 = 0.007587937172502279 + 100.0 * 6.2000861167907715
Epoch 2910, val loss: 1.2794418334960938
Epoch 2920, training loss: 619.827392578125 = 0.007523237727582455 + 100.0 * 6.1981987953186035
Epoch 2920, val loss: 1.2810052633285522
Epoch 2930, training loss: 619.8553466796875 = 0.007462284527719021 + 100.0 * 6.198478698730469
Epoch 2930, val loss: 1.2827876806259155
Epoch 2940, training loss: 620.0991821289062 = 0.007402488030493259 + 100.0 * 6.200917720794678
Epoch 2940, val loss: 1.2842235565185547
Epoch 2950, training loss: 619.816650390625 = 0.007341380696743727 + 100.0 * 6.198093414306641
Epoch 2950, val loss: 1.2858961820602417
Epoch 2960, training loss: 619.8749389648438 = 0.0072851222939789295 + 100.0 * 6.198677062988281
Epoch 2960, val loss: 1.2875745296478271
Epoch 2970, training loss: 620.2174072265625 = 0.0072262221947312355 + 100.0 * 6.202102184295654
Epoch 2970, val loss: 1.2889790534973145
Epoch 2980, training loss: 619.923828125 = 0.007167450617998838 + 100.0 * 6.199166774749756
Epoch 2980, val loss: 1.2896391153335571
Epoch 2990, training loss: 619.7391967773438 = 0.007109581492841244 + 100.0 * 6.197320938110352
Epoch 2990, val loss: 1.2916380167007446
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 861.6273803710938 = 1.9424391984939575 + 100.0 * 8.59684944152832
Epoch 0, val loss: 1.932998776435852
Epoch 10, training loss: 861.5384521484375 = 1.9331822395324707 + 100.0 * 8.596053123474121
Epoch 10, val loss: 1.9240187406539917
Epoch 20, training loss: 860.9290771484375 = 1.9215044975280762 + 100.0 * 8.590075492858887
Epoch 20, val loss: 1.9124624729156494
Epoch 30, training loss: 856.8005981445312 = 1.9064162969589233 + 100.0 * 8.548941612243652
Epoch 30, val loss: 1.8974536657333374
Epoch 40, training loss: 833.1776123046875 = 1.8877564668655396 + 100.0 * 8.312898635864258
Epoch 40, val loss: 1.8796815872192383
Epoch 50, training loss: 760.2267456054688 = 1.8665965795516968 + 100.0 * 7.583601474761963
Epoch 50, val loss: 1.859927773475647
Epoch 60, training loss: 729.0089111328125 = 1.851669192314148 + 100.0 * 7.271572113037109
Epoch 60, val loss: 1.8466168642044067
Epoch 70, training loss: 709.4529418945312 = 1.8385261297225952 + 100.0 * 7.076144218444824
Epoch 70, val loss: 1.8343408107757568
Epoch 80, training loss: 698.0037841796875 = 1.8260849714279175 + 100.0 * 6.9617767333984375
Epoch 80, val loss: 1.8226335048675537
Epoch 90, training loss: 688.5308227539062 = 1.814270257949829 + 100.0 * 6.867165565490723
Epoch 90, val loss: 1.8116527795791626
Epoch 100, training loss: 681.4241943359375 = 1.803353190422058 + 100.0 * 6.796208381652832
Epoch 100, val loss: 1.8015081882476807
Epoch 110, training loss: 674.7830200195312 = 1.7933915853500366 + 100.0 * 6.729896068572998
Epoch 110, val loss: 1.792155385017395
Epoch 120, training loss: 669.8510131835938 = 1.7841219902038574 + 100.0 * 6.680668830871582
Epoch 120, val loss: 1.7833595275878906
Epoch 130, training loss: 666.0955200195312 = 1.7742506265640259 + 100.0 * 6.643212795257568
Epoch 130, val loss: 1.7738269567489624
Epoch 140, training loss: 662.5048217773438 = 1.7634671926498413 + 100.0 * 6.607413291931152
Epoch 140, val loss: 1.7636882066726685
Epoch 150, training loss: 658.6188354492188 = 1.7523193359375 + 100.0 * 6.568665027618408
Epoch 150, val loss: 1.753026008605957
Epoch 160, training loss: 655.0045776367188 = 1.7410402297973633 + 100.0 * 6.53263521194458
Epoch 160, val loss: 1.7424789667129517
Epoch 170, training loss: 652.1016845703125 = 1.7290716171264648 + 100.0 * 6.503726482391357
Epoch 170, val loss: 1.7312835454940796
Epoch 180, training loss: 649.7785034179688 = 1.7156122922897339 + 100.0 * 6.480628967285156
Epoch 180, val loss: 1.7189240455627441
Epoch 190, training loss: 647.8394775390625 = 1.7008284330368042 + 100.0 * 6.461386680603027
Epoch 190, val loss: 1.7052778005599976
Epoch 200, training loss: 646.6998291015625 = 1.684737205505371 + 100.0 * 6.450150966644287
Epoch 200, val loss: 1.690407633781433
Epoch 210, training loss: 645.2747192382812 = 1.6666598320007324 + 100.0 * 6.436080455780029
Epoch 210, val loss: 1.6740692853927612
Epoch 220, training loss: 644.1730346679688 = 1.6473004817962646 + 100.0 * 6.425257682800293
Epoch 220, val loss: 1.6565021276474
Epoch 230, training loss: 643.0535278320312 = 1.6265515089035034 + 100.0 * 6.414269924163818
Epoch 230, val loss: 1.6377655267715454
Epoch 240, training loss: 642.1764526367188 = 1.6043437719345093 + 100.0 * 6.4057207107543945
Epoch 240, val loss: 1.617785930633545
Epoch 250, training loss: 641.3598022460938 = 1.5807158946990967 + 100.0 * 6.397790908813477
Epoch 250, val loss: 1.5965991020202637
Epoch 260, training loss: 640.5906982421875 = 1.5557854175567627 + 100.0 * 6.390349388122559
Epoch 260, val loss: 1.5743870735168457
Epoch 270, training loss: 639.863525390625 = 1.5297603607177734 + 100.0 * 6.383337497711182
Epoch 270, val loss: 1.5512958765029907
Epoch 280, training loss: 640.3975219726562 = 1.5027161836624146 + 100.0 * 6.3889479637146
Epoch 280, val loss: 1.5275274515151978
Epoch 290, training loss: 638.6266479492188 = 1.474625587463379 + 100.0 * 6.371520519256592
Epoch 290, val loss: 1.5028584003448486
Epoch 300, training loss: 638.070556640625 = 1.4461098909378052 + 100.0 * 6.366244792938232
Epoch 300, val loss: 1.4781534671783447
Epoch 310, training loss: 637.5202026367188 = 1.417186975479126 + 100.0 * 6.361030101776123
Epoch 310, val loss: 1.4532889127731323
Epoch 320, training loss: 637.3323364257812 = 1.3879516124725342 + 100.0 * 6.3594441413879395
Epoch 320, val loss: 1.4282557964324951
Epoch 330, training loss: 636.6470336914062 = 1.3583472967147827 + 100.0 * 6.35288667678833
Epoch 330, val loss: 1.4032610654830933
Epoch 340, training loss: 636.0795288085938 = 1.3288397789001465 + 100.0 * 6.347506999969482
Epoch 340, val loss: 1.3784692287445068
Epoch 350, training loss: 635.6961059570312 = 1.299339771270752 + 100.0 * 6.343967914581299
Epoch 350, val loss: 1.3538979291915894
Epoch 360, training loss: 635.4812622070312 = 1.269736647605896 + 100.0 * 6.34211540222168
Epoch 360, val loss: 1.3295091390609741
Epoch 370, training loss: 635.051025390625 = 1.2403115034103394 + 100.0 * 6.338107585906982
Epoch 370, val loss: 1.305271029472351
Epoch 380, training loss: 634.62109375 = 1.2110404968261719 + 100.0 * 6.334100246429443
Epoch 380, val loss: 1.2813796997070312
Epoch 390, training loss: 634.244140625 = 1.1820000410079956 + 100.0 * 6.330621242523193
Epoch 390, val loss: 1.2578825950622559
Epoch 400, training loss: 634.1315307617188 = 1.153285264968872 + 100.0 * 6.329782485961914
Epoch 400, val loss: 1.2347372770309448
Epoch 410, training loss: 634.0626220703125 = 1.1245728731155396 + 100.0 * 6.329380512237549
Epoch 410, val loss: 1.2121970653533936
Epoch 420, training loss: 633.4099731445312 = 1.0964579582214355 + 100.0 * 6.3231353759765625
Epoch 420, val loss: 1.1899443864822388
Epoch 430, training loss: 633.0244140625 = 1.0688291788101196 + 100.0 * 6.319555759429932
Epoch 430, val loss: 1.1684337854385376
Epoch 440, training loss: 632.979248046875 = 1.0418070554733276 + 100.0 * 6.3193745613098145
Epoch 440, val loss: 1.1476744413375854
Epoch 450, training loss: 632.8524169921875 = 1.015194296836853 + 100.0 * 6.3183722496032715
Epoch 450, val loss: 1.1273937225341797
Epoch 460, training loss: 632.6195068359375 = 0.9892719388008118 + 100.0 * 6.31630277633667
Epoch 460, val loss: 1.108018398284912
Epoch 470, training loss: 632.0538330078125 = 0.9641866683959961 + 100.0 * 6.310896396636963
Epoch 470, val loss: 1.0895802974700928
Epoch 480, training loss: 631.6727905273438 = 0.9397361874580383 + 100.0 * 6.307330131530762
Epoch 480, val loss: 1.0719949007034302
Epoch 490, training loss: 631.6848754882812 = 0.9160757660865784 + 100.0 * 6.307687759399414
Epoch 490, val loss: 1.0553107261657715
Epoch 500, training loss: 631.15625 = 0.8931806087493896 + 100.0 * 6.302630424499512
Epoch 500, val loss: 1.0394763946533203
Epoch 510, training loss: 631.0554809570312 = 0.8709537982940674 + 100.0 * 6.301845073699951
Epoch 510, val loss: 1.0243891477584839
Epoch 520, training loss: 631.1722412109375 = 0.8493468761444092 + 100.0 * 6.303228855133057
Epoch 520, val loss: 1.010108470916748
Epoch 530, training loss: 630.6995849609375 = 0.828435480594635 + 100.0 * 6.29871129989624
Epoch 530, val loss: 0.9968518018722534
Epoch 540, training loss: 630.33447265625 = 0.8080999255180359 + 100.0 * 6.295263767242432
Epoch 540, val loss: 0.9839280843734741
Epoch 550, training loss: 630.0476684570312 = 0.7884731292724609 + 100.0 * 6.2925920486450195
Epoch 550, val loss: 0.9719480276107788
Epoch 560, training loss: 630.4697265625 = 0.7694131135940552 + 100.0 * 6.297003269195557
Epoch 560, val loss: 0.9605212211608887
Epoch 570, training loss: 629.871826171875 = 0.7506797909736633 + 100.0 * 6.2912116050720215
Epoch 570, val loss: 0.9498796463012695
Epoch 580, training loss: 629.602783203125 = 0.7325371503829956 + 100.0 * 6.288702487945557
Epoch 580, val loss: 0.9396501183509827
Epoch 590, training loss: 629.2784423828125 = 0.7148677110671997 + 100.0 * 6.285635948181152
Epoch 590, val loss: 0.929947018623352
Epoch 600, training loss: 629.4544677734375 = 0.697645366191864 + 100.0 * 6.287568092346191
Epoch 600, val loss: 0.9205999970436096
Epoch 610, training loss: 629.49658203125 = 0.6806582808494568 + 100.0 * 6.288158893585205
Epoch 610, val loss: 0.9119417667388916
Epoch 620, training loss: 628.7909545898438 = 0.6640980243682861 + 100.0 * 6.28126859664917
Epoch 620, val loss: 0.9035924673080444
Epoch 630, training loss: 628.6716918945312 = 0.6479169130325317 + 100.0 * 6.280238151550293
Epoch 630, val loss: 0.8956261873245239
Epoch 640, training loss: 629.3480224609375 = 0.6320165991783142 + 100.0 * 6.2871599197387695
Epoch 640, val loss: 0.8880373239517212
Epoch 650, training loss: 628.3902587890625 = 0.6163135170936584 + 100.0 * 6.27773904800415
Epoch 650, val loss: 0.880685567855835
Epoch 660, training loss: 628.2024536132812 = 0.6009355187416077 + 100.0 * 6.276015281677246
Epoch 660, val loss: 0.8738526701927185
Epoch 670, training loss: 628.1270141601562 = 0.5859225392341614 + 100.0 * 6.2754106521606445
Epoch 670, val loss: 0.8672661185264587
Epoch 680, training loss: 628.1199951171875 = 0.571035623550415 + 100.0 * 6.275489807128906
Epoch 680, val loss: 0.8608705401420593
Epoch 690, training loss: 627.7548828125 = 0.556347131729126 + 100.0 * 6.2719855308532715
Epoch 690, val loss: 0.8546589016914368
Epoch 700, training loss: 627.6992797851562 = 0.5419292449951172 + 100.0 * 6.271573543548584
Epoch 700, val loss: 0.848737895488739
Epoch 710, training loss: 628.007568359375 = 0.5277326107025146 + 100.0 * 6.27479887008667
Epoch 710, val loss: 0.8430995941162109
Epoch 720, training loss: 627.64208984375 = 0.5137408971786499 + 100.0 * 6.2712836265563965
Epoch 720, val loss: 0.8378857374191284
Epoch 730, training loss: 627.3545532226562 = 0.49994346499443054 + 100.0 * 6.268546104431152
Epoch 730, val loss: 0.8325960040092468
Epoch 740, training loss: 627.1846313476562 = 0.48644009232521057 + 100.0 * 6.266982078552246
Epoch 740, val loss: 0.8277490139007568
Epoch 750, training loss: 627.4078979492188 = 0.47318175435066223 + 100.0 * 6.269347190856934
Epoch 750, val loss: 0.8229914903640747
Epoch 760, training loss: 627.3978881835938 = 0.46012693643569946 + 100.0 * 6.269377708435059
Epoch 760, val loss: 0.8188700079917908
Epoch 770, training loss: 626.826904296875 = 0.44722360372543335 + 100.0 * 6.263797283172607
Epoch 770, val loss: 0.8144933581352234
Epoch 780, training loss: 626.78759765625 = 0.4346707761287689 + 100.0 * 6.263528823852539
Epoch 780, val loss: 0.8106434345245361
Epoch 790, training loss: 626.908935546875 = 0.4224269390106201 + 100.0 * 6.264864921569824
Epoch 790, val loss: 0.807055652141571
Epoch 800, training loss: 626.527099609375 = 0.41040509939193726 + 100.0 * 6.261167049407959
Epoch 800, val loss: 0.8037431836128235
Epoch 810, training loss: 626.4241333007812 = 0.3986698091030121 + 100.0 * 6.260254383087158
Epoch 810, val loss: 0.8005367517471313
Epoch 820, training loss: 626.3220825195312 = 0.3872491419315338 + 100.0 * 6.259348392486572
Epoch 820, val loss: 0.7977328300476074
Epoch 830, training loss: 626.2532348632812 = 0.3761484622955322 + 100.0 * 6.258770942687988
Epoch 830, val loss: 0.795119047164917
Epoch 840, training loss: 626.9197998046875 = 0.3653017580509186 + 100.0 * 6.265544891357422
Epoch 840, val loss: 0.7926666140556335
Epoch 850, training loss: 626.6532592773438 = 0.3546260893344879 + 100.0 * 6.262986660003662
Epoch 850, val loss: 0.7906122803688049
Epoch 860, training loss: 626.1143188476562 = 0.34431707859039307 + 100.0 * 6.257699966430664
Epoch 860, val loss: 0.7888363599777222
Epoch 870, training loss: 625.8953247070312 = 0.33432894945144653 + 100.0 * 6.255609512329102
Epoch 870, val loss: 0.7871668338775635
Epoch 880, training loss: 625.8468627929688 = 0.32464897632598877 + 100.0 * 6.255222320556641
Epoch 880, val loss: 0.7857138514518738
Epoch 890, training loss: 626.3719482421875 = 0.31528255343437195 + 100.0 * 6.260566234588623
Epoch 890, val loss: 0.7846401929855347
Epoch 900, training loss: 626.2880249023438 = 0.30609986186027527 + 100.0 * 6.259819030761719
Epoch 900, val loss: 0.7837858200073242
Epoch 910, training loss: 625.6349487304688 = 0.29714566469192505 + 100.0 * 6.253378391265869
Epoch 910, val loss: 0.7829501628875732
Epoch 920, training loss: 625.5199584960938 = 0.28857922554016113 + 100.0 * 6.252313613891602
Epoch 920, val loss: 0.7824150323867798
Epoch 930, training loss: 625.4151000976562 = 0.28026190400123596 + 100.0 * 6.25134801864624
Epoch 930, val loss: 0.7821686863899231
Epoch 940, training loss: 625.5923461914062 = 0.2722313106060028 + 100.0 * 6.253201484680176
Epoch 940, val loss: 0.782166063785553
Epoch 950, training loss: 625.270751953125 = 0.26434922218322754 + 100.0 * 6.250063896179199
Epoch 950, val loss: 0.7822007536888123
Epoch 960, training loss: 625.3350830078125 = 0.2567146122455597 + 100.0 * 6.250783920288086
Epoch 960, val loss: 0.7824826836585999
Epoch 970, training loss: 625.3915405273438 = 0.24930217862129211 + 100.0 * 6.251421928405762
Epoch 970, val loss: 0.7828201651573181
Epoch 980, training loss: 625.249755859375 = 0.24218137562274933 + 100.0 * 6.250075817108154
Epoch 980, val loss: 0.7832804918289185
Epoch 990, training loss: 624.9356689453125 = 0.2352495938539505 + 100.0 * 6.247004508972168
Epoch 990, val loss: 0.7840903997421265
Epoch 1000, training loss: 624.8893432617188 = 0.22858987748622894 + 100.0 * 6.246607780456543
Epoch 1000, val loss: 0.7850531935691833
Epoch 1010, training loss: 625.0685424804688 = 0.22215548157691956 + 100.0 * 6.2484636306762695
Epoch 1010, val loss: 0.7862034440040588
Epoch 1020, training loss: 624.9332275390625 = 0.21584047377109528 + 100.0 * 6.247174263000488
Epoch 1020, val loss: 0.7872567176818848
Epoch 1030, training loss: 625.0729370117188 = 0.2097356617450714 + 100.0 * 6.248632431030273
Epoch 1030, val loss: 0.7884975671768188
Epoch 1040, training loss: 624.826416015625 = 0.2037847340106964 + 100.0 * 6.2462263107299805
Epoch 1040, val loss: 0.7898450493812561
Epoch 1050, training loss: 624.6517944335938 = 0.1980287730693817 + 100.0 * 6.244537353515625
Epoch 1050, val loss: 0.791422426700592
Epoch 1060, training loss: 624.568603515625 = 0.19249920547008514 + 100.0 * 6.24376106262207
Epoch 1060, val loss: 0.7930548191070557
Epoch 1070, training loss: 624.6159057617188 = 0.18715085089206696 + 100.0 * 6.244287490844727
Epoch 1070, val loss: 0.7949202060699463
Epoch 1080, training loss: 624.5523681640625 = 0.18195950984954834 + 100.0 * 6.243704319000244
Epoch 1080, val loss: 0.7970377206802368
Epoch 1090, training loss: 624.9552001953125 = 0.17692041397094727 + 100.0 * 6.2477827072143555
Epoch 1090, val loss: 0.7989903092384338
Epoch 1100, training loss: 624.4859008789062 = 0.17199809849262238 + 100.0 * 6.243138790130615
Epoch 1100, val loss: 0.8007588982582092
Epoch 1110, training loss: 624.5183715820312 = 0.16727150976657867 + 100.0 * 6.243510723114014
Epoch 1110, val loss: 0.8030710816383362
Epoch 1120, training loss: 624.1084594726562 = 0.16264210641384125 + 100.0 * 6.239458084106445
Epoch 1120, val loss: 0.8052659630775452
Epoch 1130, training loss: 624.1265258789062 = 0.15819603204727173 + 100.0 * 6.239683151245117
Epoch 1130, val loss: 0.8075881004333496
Epoch 1140, training loss: 624.3675537109375 = 0.15390175580978394 + 100.0 * 6.242136001586914
Epoch 1140, val loss: 0.8101807832717896
Epoch 1150, training loss: 624.0291748046875 = 0.14972415566444397 + 100.0 * 6.238794803619385
Epoch 1150, val loss: 0.8125608563423157
Epoch 1160, training loss: 624.1093139648438 = 0.14568465948104858 + 100.0 * 6.239636421203613
Epoch 1160, val loss: 0.8149943351745605
Epoch 1170, training loss: 624.5892944335938 = 0.1417301744222641 + 100.0 * 6.244475841522217
Epoch 1170, val loss: 0.8174230456352234
Epoch 1180, training loss: 624.0886840820312 = 0.13788849115371704 + 100.0 * 6.239508152008057
Epoch 1180, val loss: 0.8200718760490417
Epoch 1190, training loss: 623.7890625 = 0.13418447971343994 + 100.0 * 6.236548900604248
Epoch 1190, val loss: 0.8227653503417969
Epoch 1200, training loss: 623.670654296875 = 0.13063086569309235 + 100.0 * 6.235400676727295
Epoch 1200, val loss: 0.8255941867828369
Epoch 1210, training loss: 623.7501831054688 = 0.1271820366382599 + 100.0 * 6.23622989654541
Epoch 1210, val loss: 0.8283791542053223
Epoch 1220, training loss: 624.0821533203125 = 0.12378784269094467 + 100.0 * 6.239583492279053
Epoch 1220, val loss: 0.8310468196868896
Epoch 1230, training loss: 623.623291015625 = 0.12048982828855515 + 100.0 * 6.235028266906738
Epoch 1230, val loss: 0.8338454961776733
Epoch 1240, training loss: 623.599853515625 = 0.1173107847571373 + 100.0 * 6.234825611114502
Epoch 1240, val loss: 0.8367459774017334
Epoch 1250, training loss: 623.455078125 = 0.11424551904201508 + 100.0 * 6.233408451080322
Epoch 1250, val loss: 0.8397475481033325
Epoch 1260, training loss: 623.649169921875 = 0.11128487437963486 + 100.0 * 6.235378742218018
Epoch 1260, val loss: 0.8429804444313049
Epoch 1270, training loss: 623.5758056640625 = 0.10837084800004959 + 100.0 * 6.234673976898193
Epoch 1270, val loss: 0.8457096219062805
Epoch 1280, training loss: 623.3547973632812 = 0.10553325712680817 + 100.0 * 6.232492446899414
Epoch 1280, val loss: 0.8486720323562622
Epoch 1290, training loss: 623.2333374023438 = 0.10282166302204132 + 100.0 * 6.231305122375488
Epoch 1290, val loss: 0.8518551588058472
Epoch 1300, training loss: 623.208251953125 = 0.10019127279520035 + 100.0 * 6.231080055236816
Epoch 1300, val loss: 0.8552160859107971
Epoch 1310, training loss: 623.7839965820312 = 0.0976671427488327 + 100.0 * 6.236863136291504
Epoch 1310, val loss: 0.8584784865379333
Epoch 1320, training loss: 623.497314453125 = 0.0950883999466896 + 100.0 * 6.23402214050293
Epoch 1320, val loss: 0.8609182238578796
Epoch 1330, training loss: 623.2733764648438 = 0.09267102926969528 + 100.0 * 6.231807231903076
Epoch 1330, val loss: 0.864760160446167
Epoch 1340, training loss: 623.0751342773438 = 0.09030021727085114 + 100.0 * 6.229848384857178
Epoch 1340, val loss: 0.8676238656044006
Epoch 1350, training loss: 622.9844360351562 = 0.08804509788751602 + 100.0 * 6.228963375091553
Epoch 1350, val loss: 0.8711408376693726
Epoch 1360, training loss: 623.7881469726562 = 0.08584215492010117 + 100.0 * 6.23702335357666
Epoch 1360, val loss: 0.8744427561759949
Epoch 1370, training loss: 623.2400512695312 = 0.0836634710431099 + 100.0 * 6.231563568115234
Epoch 1370, val loss: 0.8772152662277222
Epoch 1380, training loss: 622.981689453125 = 0.08156570047140121 + 100.0 * 6.229001522064209
Epoch 1380, val loss: 0.8806802034378052
Epoch 1390, training loss: 622.929443359375 = 0.0795484334230423 + 100.0 * 6.228499412536621
Epoch 1390, val loss: 0.8837651014328003
Epoch 1400, training loss: 623.0713500976562 = 0.07758478820323944 + 100.0 * 6.229937553405762
Epoch 1400, val loss: 0.8869929909706116
Epoch 1410, training loss: 622.9305419921875 = 0.07566867768764496 + 100.0 * 6.228548526763916
Epoch 1410, val loss: 0.8905625939369202
Epoch 1420, training loss: 623.7933959960938 = 0.07384421676397324 + 100.0 * 6.2371954917907715
Epoch 1420, val loss: 0.8936033844947815
Epoch 1430, training loss: 622.966796875 = 0.07197776436805725 + 100.0 * 6.22894811630249
Epoch 1430, val loss: 0.8969076871871948
Epoch 1440, training loss: 622.6847534179688 = 0.07022564113140106 + 100.0 * 6.226145267486572
Epoch 1440, val loss: 0.9000824689865112
Epoch 1450, training loss: 622.591796875 = 0.06853489577770233 + 100.0 * 6.2252326011657715
Epoch 1450, val loss: 0.9034910202026367
Epoch 1460, training loss: 622.5572509765625 = 0.06689634919166565 + 100.0 * 6.224903583526611
Epoch 1460, val loss: 0.9068513512611389
Epoch 1470, training loss: 623.1452026367188 = 0.06532175838947296 + 100.0 * 6.230798721313477
Epoch 1470, val loss: 0.9106143712997437
Epoch 1480, training loss: 622.7623291015625 = 0.06373617798089981 + 100.0 * 6.226985931396484
Epoch 1480, val loss: 0.9131896495819092
Epoch 1490, training loss: 622.6300659179688 = 0.06220769137144089 + 100.0 * 6.22567892074585
Epoch 1490, val loss: 0.9167302250862122
Epoch 1500, training loss: 622.4392700195312 = 0.06073805317282677 + 100.0 * 6.223785400390625
Epoch 1500, val loss: 0.9200977683067322
Epoch 1510, training loss: 622.4995727539062 = 0.059321608394384384 + 100.0 * 6.22440242767334
Epoch 1510, val loss: 0.923459529876709
Epoch 1520, training loss: 623.0087280273438 = 0.0579487606883049 + 100.0 * 6.229507923126221
Epoch 1520, val loss: 0.9266480803489685
Epoch 1530, training loss: 622.5271606445312 = 0.05657058581709862 + 100.0 * 6.224705696105957
Epoch 1530, val loss: 0.9298160076141357
Epoch 1540, training loss: 622.332275390625 = 0.055237866938114166 + 100.0 * 6.2227702140808105
Epoch 1540, val loss: 0.9329190850257874
Epoch 1550, training loss: 622.2725830078125 = 0.0539759024977684 + 100.0 * 6.22218656539917
Epoch 1550, val loss: 0.9363695979118347
Epoch 1560, training loss: 622.3533325195312 = 0.052758004516363144 + 100.0 * 6.223005771636963
Epoch 1560, val loss: 0.9395952224731445
Epoch 1570, training loss: 622.5390014648438 = 0.051560599356889725 + 100.0 * 6.224874496459961
Epoch 1570, val loss: 0.9429174065589905
Epoch 1580, training loss: 622.5994873046875 = 0.05039288476109505 + 100.0 * 6.225490570068359
Epoch 1580, val loss: 0.9461818337440491
Epoch 1590, training loss: 622.5364990234375 = 0.049244485795497894 + 100.0 * 6.224872589111328
Epoch 1590, val loss: 0.9493525624275208
Epoch 1600, training loss: 622.2709350585938 = 0.04815522953867912 + 100.0 * 6.222227573394775
Epoch 1600, val loss: 0.9523659944534302
Epoch 1610, training loss: 622.2055053710938 = 0.047088127583265305 + 100.0 * 6.221584320068359
Epoch 1610, val loss: 0.9558461308479309
Epoch 1620, training loss: 622.4324951171875 = 0.046056270599365234 + 100.0 * 6.2238640785217285
Epoch 1620, val loss: 0.9592921733856201
Epoch 1630, training loss: 622.1541137695312 = 0.045036766678094864 + 100.0 * 6.221090793609619
Epoch 1630, val loss: 0.9620048403739929
Epoch 1640, training loss: 622.1064453125 = 0.04406627267599106 + 100.0 * 6.22062349319458
Epoch 1640, val loss: 0.9655438661575317
Epoch 1650, training loss: 622.6949462890625 = 0.04313238710165024 + 100.0 * 6.226518154144287
Epoch 1650, val loss: 0.9687831401824951
Epoch 1660, training loss: 622.292724609375 = 0.04217186197638512 + 100.0 * 6.222505569458008
Epoch 1660, val loss: 0.9710303544998169
Epoch 1670, training loss: 621.9729614257812 = 0.04127459600567818 + 100.0 * 6.2193169593811035
Epoch 1670, val loss: 0.9747522473335266
Epoch 1680, training loss: 621.8878784179688 = 0.04039497300982475 + 100.0 * 6.218474864959717
Epoch 1680, val loss: 0.977594792842865
Epoch 1690, training loss: 621.9947509765625 = 0.03955370560288429 + 100.0 * 6.219552040100098
Epoch 1690, val loss: 0.9807117581367493
Epoch 1700, training loss: 622.10791015625 = 0.03873012214899063 + 100.0 * 6.220691680908203
Epoch 1700, val loss: 0.9837464690208435
Epoch 1710, training loss: 622.6482543945312 = 0.037939246743917465 + 100.0 * 6.226102828979492
Epoch 1710, val loss: 0.9868208169937134
Epoch 1720, training loss: 622.019775390625 = 0.037124790251255035 + 100.0 * 6.219826698303223
Epoch 1720, val loss: 0.9895609617233276
Epoch 1730, training loss: 621.8565673828125 = 0.03636161610484123 + 100.0 * 6.218201637268066
Epoch 1730, val loss: 0.99269038438797
Epoch 1740, training loss: 621.7691040039062 = 0.03562125936150551 + 100.0 * 6.217334747314453
Epoch 1740, val loss: 0.9956531524658203
Epoch 1750, training loss: 621.7323608398438 = 0.03491215780377388 + 100.0 * 6.21697473526001
Epoch 1750, val loss: 0.998596727848053
Epoch 1760, training loss: 622.1217651367188 = 0.03422205522656441 + 100.0 * 6.220875263214111
Epoch 1760, val loss: 1.0017924308776855
Epoch 1770, training loss: 621.68408203125 = 0.03353476896882057 + 100.0 * 6.216506004333496
Epoch 1770, val loss: 1.0042580366134644
Epoch 1780, training loss: 621.7932739257812 = 0.03286599740386009 + 100.0 * 6.217604637145996
Epoch 1780, val loss: 1.007398009300232
Epoch 1790, training loss: 621.8175048828125 = 0.03221726045012474 + 100.0 * 6.217852592468262
Epoch 1790, val loss: 1.0100176334381104
Epoch 1800, training loss: 622.4373779296875 = 0.03158736601471901 + 100.0 * 6.224057674407959
Epoch 1800, val loss: 1.0129518508911133
Epoch 1810, training loss: 621.7858276367188 = 0.030979998409748077 + 100.0 * 6.217548370361328
Epoch 1810, val loss: 1.0157451629638672
Epoch 1820, training loss: 621.587890625 = 0.030376560986042023 + 100.0 * 6.215574741363525
Epoch 1820, val loss: 1.0185511112213135
Epoch 1830, training loss: 621.66259765625 = 0.029806528240442276 + 100.0 * 6.216328144073486
Epoch 1830, val loss: 1.0215542316436768
Epoch 1840, training loss: 621.8994750976562 = 0.02924501709640026 + 100.0 * 6.21870231628418
Epoch 1840, val loss: 1.0242623090744019
Epoch 1850, training loss: 621.6255493164062 = 0.028684917837381363 + 100.0 * 6.215968608856201
Epoch 1850, val loss: 1.0262547731399536
Epoch 1860, training loss: 621.5294189453125 = 0.028154408559203148 + 100.0 * 6.215012550354004
Epoch 1860, val loss: 1.0294283628463745
Epoch 1870, training loss: 622.1484375 = 0.027632448822259903 + 100.0 * 6.221208095550537
Epoch 1870, val loss: 1.0318297147750854
Epoch 1880, training loss: 621.4546508789062 = 0.02712339162826538 + 100.0 * 6.214275360107422
Epoch 1880, val loss: 1.034585952758789
Epoch 1890, training loss: 621.414306640625 = 0.02662242390215397 + 100.0 * 6.213877201080322
Epoch 1890, val loss: 1.0370922088623047
Epoch 1900, training loss: 621.4927368164062 = 0.02614186890423298 + 100.0 * 6.214666366577148
Epoch 1900, val loss: 1.0400160551071167
Epoch 1910, training loss: 621.7951049804688 = 0.025677185505628586 + 100.0 * 6.21769380569458
Epoch 1910, val loss: 1.0425022840499878
Epoch 1920, training loss: 621.4591064453125 = 0.02520938031375408 + 100.0 * 6.214338779449463
Epoch 1920, val loss: 1.0444741249084473
Epoch 1930, training loss: 621.3490600585938 = 0.02476591244339943 + 100.0 * 6.213243007659912
Epoch 1930, val loss: 1.0473264455795288
Epoch 1940, training loss: 621.5393676757812 = 0.024337802082300186 + 100.0 * 6.215150356292725
Epoch 1940, val loss: 1.049793004989624
Epoch 1950, training loss: 621.5330810546875 = 0.023905862122774124 + 100.0 * 6.215091705322266
Epoch 1950, val loss: 1.0518934726715088
Epoch 1960, training loss: 621.5443725585938 = 0.023491879925131798 + 100.0 * 6.215208530426025
Epoch 1960, val loss: 1.0544030666351318
Epoch 1970, training loss: 621.6619262695312 = 0.023085465654730797 + 100.0 * 6.216388702392578
Epoch 1970, val loss: 1.0568068027496338
Epoch 1980, training loss: 621.3795776367188 = 0.022681664675474167 + 100.0 * 6.213568687438965
Epoch 1980, val loss: 1.059496283531189
Epoch 1990, training loss: 621.2282104492188 = 0.022295035421848297 + 100.0 * 6.212059020996094
Epoch 1990, val loss: 1.0616390705108643
Epoch 2000, training loss: 621.2294921875 = 0.021927107125520706 + 100.0 * 6.212075710296631
Epoch 2000, val loss: 1.0639101266860962
Epoch 2010, training loss: 621.5560913085938 = 0.021566588431596756 + 100.0 * 6.21534538269043
Epoch 2010, val loss: 1.0661753416061401
Epoch 2020, training loss: 621.495361328125 = 0.021211983636021614 + 100.0 * 6.214741230010986
Epoch 2020, val loss: 1.0692518949508667
Epoch 2030, training loss: 621.1796875 = 0.020849183201789856 + 100.0 * 6.211587905883789
Epoch 2030, val loss: 1.070732593536377
Epoch 2040, training loss: 621.1941528320312 = 0.020505493506789207 + 100.0 * 6.211736679077148
Epoch 2040, val loss: 1.073016881942749
Epoch 2050, training loss: 621.5765991210938 = 0.020172858610749245 + 100.0 * 6.215564250946045
Epoch 2050, val loss: 1.0753661394119263
Epoch 2060, training loss: 621.3200073242188 = 0.019846204668283463 + 100.0 * 6.213001728057861
Epoch 2060, val loss: 1.077668309211731
Epoch 2070, training loss: 621.1113891601562 = 0.019528856500983238 + 100.0 * 6.210918426513672
Epoch 2070, val loss: 1.0797051191329956
Epoch 2080, training loss: 621.0531005859375 = 0.019222375005483627 + 100.0 * 6.210338592529297
Epoch 2080, val loss: 1.0819981098175049
Epoch 2090, training loss: 621.1277465820312 = 0.01892729662358761 + 100.0 * 6.211088180541992
Epoch 2090, val loss: 1.084226131439209
Epoch 2100, training loss: 621.6339721679688 = 0.018640071153640747 + 100.0 * 6.216153621673584
Epoch 2100, val loss: 1.0864994525909424
Epoch 2110, training loss: 621.2890625 = 0.01833994872868061 + 100.0 * 6.21270751953125
Epoch 2110, val loss: 1.0885374546051025
Epoch 2120, training loss: 621.11083984375 = 0.018055515363812447 + 100.0 * 6.210927486419678
Epoch 2120, val loss: 1.0905784368515015
Epoch 2130, training loss: 621.175537109375 = 0.017781630158424377 + 100.0 * 6.211577892303467
Epoch 2130, val loss: 1.0928003787994385
Epoch 2140, training loss: 621.20166015625 = 0.017513127997517586 + 100.0 * 6.211841583251953
Epoch 2140, val loss: 1.094612956047058
Epoch 2150, training loss: 621.2996826171875 = 0.017246536910533905 + 100.0 * 6.21282434463501
Epoch 2150, val loss: 1.096444845199585
Epoch 2160, training loss: 621.0641479492188 = 0.016980798915028572 + 100.0 * 6.2104716300964355
Epoch 2160, val loss: 1.0987625122070312
Epoch 2170, training loss: 620.920654296875 = 0.016732241958379745 + 100.0 * 6.209039211273193
Epoch 2170, val loss: 1.100719690322876
Epoch 2180, training loss: 621.1066284179688 = 0.016489829868078232 + 100.0 * 6.210901260375977
Epoch 2180, val loss: 1.1029250621795654
Epoch 2190, training loss: 621.25537109375 = 0.016246799379587173 + 100.0 * 6.212391376495361
Epoch 2190, val loss: 1.1047128438949585
Epoch 2200, training loss: 621.0442504882812 = 0.016007862985134125 + 100.0 * 6.210282325744629
Epoch 2200, val loss: 1.1065443754196167
Epoch 2210, training loss: 620.921875 = 0.015777669847011566 + 100.0 * 6.2090606689453125
Epoch 2210, val loss: 1.1085771322250366
Epoch 2220, training loss: 621.0191040039062 = 0.015556751750409603 + 100.0 * 6.21003532409668
Epoch 2220, val loss: 1.1103266477584839
Epoch 2230, training loss: 621.048828125 = 0.01533565390855074 + 100.0 * 6.2103352546691895
Epoch 2230, val loss: 1.112107515335083
Epoch 2240, training loss: 621.3353271484375 = 0.015126154758036137 + 100.0 * 6.213201999664307
Epoch 2240, val loss: 1.1142144203186035
Epoch 2250, training loss: 621.34326171875 = 0.014912711456418037 + 100.0 * 6.213283538818359
Epoch 2250, val loss: 1.1165170669555664
Epoch 2260, training loss: 621.0669555664062 = 0.014697779901325703 + 100.0 * 6.210522651672363
Epoch 2260, val loss: 1.1177321672439575
Epoch 2270, training loss: 620.7507934570312 = 0.01449166052043438 + 100.0 * 6.207363128662109
Epoch 2270, val loss: 1.1198443174362183
Epoch 2280, training loss: 620.7709350585938 = 0.014294948428869247 + 100.0 * 6.207566261291504
Epoch 2280, val loss: 1.1219221353530884
Epoch 2290, training loss: 620.9559326171875 = 0.014102844521403313 + 100.0 * 6.209418296813965
Epoch 2290, val loss: 1.123666524887085
Epoch 2300, training loss: 620.9783935546875 = 0.013913525268435478 + 100.0 * 6.209644794464111
Epoch 2300, val loss: 1.1252778768539429
Epoch 2310, training loss: 620.815673828125 = 0.013731800019741058 + 100.0 * 6.208019256591797
Epoch 2310, val loss: 1.126929521560669
Epoch 2320, training loss: 620.7649536132812 = 0.013548942282795906 + 100.0 * 6.20751428604126
Epoch 2320, val loss: 1.1287997961044312
Epoch 2330, training loss: 620.9168701171875 = 0.01336914673447609 + 100.0 * 6.2090349197387695
Epoch 2330, val loss: 1.130729079246521
Epoch 2340, training loss: 620.8650512695312 = 0.013194374740123749 + 100.0 * 6.2085185050964355
Epoch 2340, val loss: 1.1321775913238525
Epoch 2350, training loss: 620.85009765625 = 0.013025127351284027 + 100.0 * 6.208371162414551
Epoch 2350, val loss: 1.1335481405258179
Epoch 2360, training loss: 620.8787231445312 = 0.01285428274422884 + 100.0 * 6.208658218383789
Epoch 2360, val loss: 1.135330080986023
Epoch 2370, training loss: 620.7246704101562 = 0.012691836804151535 + 100.0 * 6.207119941711426
Epoch 2370, val loss: 1.1373112201690674
Epoch 2380, training loss: 621.0343627929688 = 0.012535728514194489 + 100.0 * 6.21021842956543
Epoch 2380, val loss: 1.139107584953308
Epoch 2390, training loss: 621.106201171875 = 0.012372048571705818 + 100.0 * 6.210937976837158
Epoch 2390, val loss: 1.139769434928894
Epoch 2400, training loss: 620.6435546875 = 0.012211228720843792 + 100.0 * 6.206313610076904
Epoch 2400, val loss: 1.1421635150909424
Epoch 2410, training loss: 620.57958984375 = 0.012060798704624176 + 100.0 * 6.20567512512207
Epoch 2410, val loss: 1.1437689065933228
Epoch 2420, training loss: 620.546630859375 = 0.011914270929992199 + 100.0 * 6.205347537994385
Epoch 2420, val loss: 1.1451700925827026
Epoch 2430, training loss: 620.4764404296875 = 0.011772440746426582 + 100.0 * 6.204646587371826
Epoch 2430, val loss: 1.1469465494155884
Epoch 2440, training loss: 620.7949829101562 = 0.011638432741165161 + 100.0 * 6.207833290100098
Epoch 2440, val loss: 1.148624062538147
Epoch 2450, training loss: 620.789794921875 = 0.01149558275938034 + 100.0 * 6.207783222198486
Epoch 2450, val loss: 1.1497057676315308
Epoch 2460, training loss: 620.740966796875 = 0.011350315064191818 + 100.0 * 6.207296371459961
Epoch 2460, val loss: 1.1511698961257935
Epoch 2470, training loss: 620.5866088867188 = 0.011216803453862667 + 100.0 * 6.205753803253174
Epoch 2470, val loss: 1.1527937650680542
Epoch 2480, training loss: 620.5352172851562 = 0.011082307435572147 + 100.0 * 6.2052412033081055
Epoch 2480, val loss: 1.154105305671692
Epoch 2490, training loss: 620.8443603515625 = 0.010955988429486752 + 100.0 * 6.208333492279053
Epoch 2490, val loss: 1.1556671857833862
Epoch 2500, training loss: 620.7640380859375 = 0.010828308761119843 + 100.0 * 6.2075324058532715
Epoch 2500, val loss: 1.1575721502304077
Epoch 2510, training loss: 620.4302978515625 = 0.01070040836930275 + 100.0 * 6.204196453094482
Epoch 2510, val loss: 1.1587601900100708
Epoch 2520, training loss: 620.4428100585938 = 0.010578432120382786 + 100.0 * 6.204322338104248
Epoch 2520, val loss: 1.1602823734283447
Epoch 2530, training loss: 620.708984375 = 0.010461537167429924 + 100.0 * 6.2069854736328125
Epoch 2530, val loss: 1.1622419357299805
Epoch 2540, training loss: 620.70263671875 = 0.010340982116758823 + 100.0 * 6.206923007965088
Epoch 2540, val loss: 1.1632022857666016
Epoch 2550, training loss: 620.4462890625 = 0.010223807767033577 + 100.0 * 6.204360485076904
Epoch 2550, val loss: 1.1644091606140137
Epoch 2560, training loss: 620.365966796875 = 0.010110189206898212 + 100.0 * 6.203558444976807
Epoch 2560, val loss: 1.165902018547058
Epoch 2570, training loss: 620.3377685546875 = 0.010000776499509811 + 100.0 * 6.203277587890625
Epoch 2570, val loss: 1.1672241687774658
Epoch 2580, training loss: 620.9461669921875 = 0.00989685207605362 + 100.0 * 6.209362983703613
Epoch 2580, val loss: 1.1690125465393066
Epoch 2590, training loss: 620.9237670898438 = 0.009791516698896885 + 100.0 * 6.209139823913574
Epoch 2590, val loss: 1.1703490018844604
Epoch 2600, training loss: 620.52734375 = 0.009674024768173695 + 100.0 * 6.205176830291748
Epoch 2600, val loss: 1.1709667444229126
Epoch 2610, training loss: 620.3341674804688 = 0.009570424444973469 + 100.0 * 6.203246116638184
Epoch 2610, val loss: 1.17263925075531
Epoch 2620, training loss: 620.2928466796875 = 0.009471354074776173 + 100.0 * 6.202833652496338
Epoch 2620, val loss: 1.1740692853927612
Epoch 2630, training loss: 620.6439819335938 = 0.009376571513712406 + 100.0 * 6.206346035003662
Epoch 2630, val loss: 1.1753793954849243
Epoch 2640, training loss: 620.3240966796875 = 0.009273617528378963 + 100.0 * 6.203147888183594
Epoch 2640, val loss: 1.1762750148773193
Epoch 2650, training loss: 620.3350219726562 = 0.00917572621256113 + 100.0 * 6.203258514404297
Epoch 2650, val loss: 1.1776734590530396
Epoch 2660, training loss: 620.4547119140625 = 0.009083159267902374 + 100.0 * 6.204456329345703
Epoch 2660, val loss: 1.1788184642791748
Epoch 2670, training loss: 620.525634765625 = 0.008986986242234707 + 100.0 * 6.205166816711426
Epoch 2670, val loss: 1.1799205541610718
Epoch 2680, training loss: 620.3047485351562 = 0.008893770165741444 + 100.0 * 6.202958106994629
Epoch 2680, val loss: 1.1817877292633057
Epoch 2690, training loss: 620.192138671875 = 0.008804449811577797 + 100.0 * 6.201833724975586
Epoch 2690, val loss: 1.1827526092529297
Epoch 2700, training loss: 620.3660888671875 = 0.008718526922166348 + 100.0 * 6.203573703765869
Epoch 2700, val loss: 1.1836344003677368
Epoch 2710, training loss: 620.4779052734375 = 0.008632219396531582 + 100.0 * 6.204692840576172
Epoch 2710, val loss: 1.1848596334457397
Epoch 2720, training loss: 620.3944702148438 = 0.008546619676053524 + 100.0 * 6.203859329223633
Epoch 2720, val loss: 1.1865512132644653
Epoch 2730, training loss: 620.3551635742188 = 0.008464902639389038 + 100.0 * 6.203466892242432
Epoch 2730, val loss: 1.1873939037322998
Epoch 2740, training loss: 620.4127197265625 = 0.008383562788367271 + 100.0 * 6.204043865203857
Epoch 2740, val loss: 1.1889123916625977
Epoch 2750, training loss: 620.129150390625 = 0.008299056440591812 + 100.0 * 6.201208591461182
Epoch 2750, val loss: 1.1898391246795654
Epoch 2760, training loss: 620.3282470703125 = 0.008221093565225601 + 100.0 * 6.203199863433838
Epoch 2760, val loss: 1.1911097764968872
Epoch 2770, training loss: 620.437744140625 = 0.00814507994800806 + 100.0 * 6.204296112060547
Epoch 2770, val loss: 1.192352294921875
Epoch 2780, training loss: 620.2520141601562 = 0.008064772933721542 + 100.0 * 6.202439785003662
Epoch 2780, val loss: 1.1931408643722534
Epoch 2790, training loss: 620.1904907226562 = 0.007987665943801403 + 100.0 * 6.201825141906738
Epoch 2790, val loss: 1.19423246383667
Epoch 2800, training loss: 620.2581787109375 = 0.007914635352790356 + 100.0 * 6.202502250671387
Epoch 2800, val loss: 1.1951603889465332
Epoch 2810, training loss: 620.2252807617188 = 0.00784407090395689 + 100.0 * 6.202174186706543
Epoch 2810, val loss: 1.1964975595474243
Epoch 2820, training loss: 620.3479614257812 = 0.007773443590849638 + 100.0 * 6.203402042388916
Epoch 2820, val loss: 1.1977931261062622
Epoch 2830, training loss: 620.3458251953125 = 0.007699441164731979 + 100.0 * 6.203381538391113
Epoch 2830, val loss: 1.1991106271743774
Epoch 2840, training loss: 620.2587890625 = 0.007629687897861004 + 100.0 * 6.202511787414551
Epoch 2840, val loss: 1.2002158164978027
Epoch 2850, training loss: 620.2356567382812 = 0.0075605204328894615 + 100.0 * 6.2022809982299805
Epoch 2850, val loss: 1.2010825872421265
Epoch 2860, training loss: 620.08984375 = 0.007491254713386297 + 100.0 * 6.2008233070373535
Epoch 2860, val loss: 1.2021902799606323
Epoch 2870, training loss: 619.9950561523438 = 0.007424916140735149 + 100.0 * 6.199875831604004
Epoch 2870, val loss: 1.2032580375671387
Epoch 2880, training loss: 620.3812866210938 = 0.0073620411567389965 + 100.0 * 6.203739166259766
Epoch 2880, val loss: 1.2042922973632812
Epoch 2890, training loss: 620.2169799804688 = 0.007298754993826151 + 100.0 * 6.202096462249756
Epoch 2890, val loss: 1.2061046361923218
Epoch 2900, training loss: 620.0648193359375 = 0.007234448567032814 + 100.0 * 6.200575828552246
Epoch 2900, val loss: 1.206223726272583
Epoch 2910, training loss: 619.9930419921875 = 0.0071699246764183044 + 100.0 * 6.199859142303467
Epoch 2910, val loss: 1.207497000694275
Epoch 2920, training loss: 619.9614868164062 = 0.007110237143933773 + 100.0 * 6.1995439529418945
Epoch 2920, val loss: 1.208534598350525
Epoch 2930, training loss: 620.1825561523438 = 0.007052194327116013 + 100.0 * 6.201754570007324
Epoch 2930, val loss: 1.2099120616912842
Epoch 2940, training loss: 620.143310546875 = 0.00699211610481143 + 100.0 * 6.2013630867004395
Epoch 2940, val loss: 1.210946798324585
Epoch 2950, training loss: 620.73388671875 = 0.0069368877448141575 + 100.0 * 6.207269191741943
Epoch 2950, val loss: 1.2112189531326294
Epoch 2960, training loss: 620.0557250976562 = 0.006868296768516302 + 100.0 * 6.200488567352295
Epoch 2960, val loss: 1.2123609781265259
Epoch 2970, training loss: 619.8976440429688 = 0.0068092020228505135 + 100.0 * 6.19890832901001
Epoch 2970, val loss: 1.2128493785858154
Epoch 2980, training loss: 619.8956298828125 = 0.006753846537321806 + 100.0 * 6.198888778686523
Epoch 2980, val loss: 1.2141799926757812
Epoch 2990, training loss: 619.8234252929688 = 0.006700988858938217 + 100.0 * 6.198166847229004
Epoch 2990, val loss: 1.215203881263733
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8360569319978914
The final CL Acc:0.75185, 0.02095, The final GNN Acc:0.83834, 0.00174
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10562])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6333618164062 = 1.9496052265167236 + 100.0 * 8.596837997436523
Epoch 0, val loss: 1.9496384859085083
Epoch 10, training loss: 861.5458984375 = 1.9408636093139648 + 100.0 * 8.596050262451172
Epoch 10, val loss: 1.9403719902038574
Epoch 20, training loss: 860.9404907226562 = 1.9296520948410034 + 100.0 * 8.590108871459961
Epoch 20, val loss: 1.9284130334854126
Epoch 30, training loss: 856.7753295898438 = 1.9146414995193481 + 100.0 * 8.548606872558594
Epoch 30, val loss: 1.9124258756637573
Epoch 40, training loss: 833.4673461914062 = 1.8962138891220093 + 100.0 * 8.31571102142334
Epoch 40, val loss: 1.8927839994430542
Epoch 50, training loss: 789.1053466796875 = 1.8736586570739746 + 100.0 * 7.872316837310791
Epoch 50, val loss: 1.8691977262496948
Epoch 60, training loss: 762.2777709960938 = 1.8565642833709717 + 100.0 * 7.604212284088135
Epoch 60, val loss: 1.8534778356552124
Epoch 70, training loss: 733.4791259765625 = 1.848374843597412 + 100.0 * 7.316307067871094
Epoch 70, val loss: 1.8463293313980103
Epoch 80, training loss: 709.5936889648438 = 1.8418835401535034 + 100.0 * 7.077518463134766
Epoch 80, val loss: 1.8399264812469482
Epoch 90, training loss: 697.5603637695312 = 1.833337426185608 + 100.0 * 6.957270622253418
Epoch 90, val loss: 1.8316000699996948
Epoch 100, training loss: 687.4669189453125 = 1.8249151706695557 + 100.0 * 6.856420516967773
Epoch 100, val loss: 1.8237801790237427
Epoch 110, training loss: 678.5540771484375 = 1.8182697296142578 + 100.0 * 6.76735782623291
Epoch 110, val loss: 1.8173530101776123
Epoch 120, training loss: 672.5687866210938 = 1.8116588592529297 + 100.0 * 6.707571506500244
Epoch 120, val loss: 1.8109098672866821
Epoch 130, training loss: 667.3194580078125 = 1.8048183917999268 + 100.0 * 6.655146598815918
Epoch 130, val loss: 1.8041967153549194
Epoch 140, training loss: 663.2845458984375 = 1.7978179454803467 + 100.0 * 6.614867687225342
Epoch 140, val loss: 1.7974624633789062
Epoch 150, training loss: 659.904052734375 = 1.7905389070510864 + 100.0 * 6.581135272979736
Epoch 150, val loss: 1.790657639503479
Epoch 160, training loss: 657.5438232421875 = 1.782805323600769 + 100.0 * 6.557610511779785
Epoch 160, val loss: 1.783542275428772
Epoch 170, training loss: 655.0305786132812 = 1.7745025157928467 + 100.0 * 6.532561302185059
Epoch 170, val loss: 1.7760391235351562
Epoch 180, training loss: 653.2446899414062 = 1.765559434890747 + 100.0 * 6.514791011810303
Epoch 180, val loss: 1.7680888175964355
Epoch 190, training loss: 651.5536499023438 = 1.7558586597442627 + 100.0 * 6.4979777336120605
Epoch 190, val loss: 1.7595378160476685
Epoch 200, training loss: 650.0350952148438 = 1.7454081773757935 + 100.0 * 6.48289680480957
Epoch 200, val loss: 1.7505590915679932
Epoch 210, training loss: 648.713623046875 = 1.7341307401657104 + 100.0 * 6.469794750213623
Epoch 210, val loss: 1.7410050630569458
Epoch 220, training loss: 647.6287231445312 = 1.7219080924987793 + 100.0 * 6.4590678215026855
Epoch 220, val loss: 1.7306594848632812
Epoch 230, training loss: 646.5305786132812 = 1.7085882425308228 + 100.0 * 6.4482197761535645
Epoch 230, val loss: 1.7196121215820312
Epoch 240, training loss: 645.56298828125 = 1.6942462921142578 + 100.0 * 6.438686847686768
Epoch 240, val loss: 1.7077860832214355
Epoch 250, training loss: 644.6395263671875 = 1.6788666248321533 + 100.0 * 6.4296064376831055
Epoch 250, val loss: 1.695181965827942
Epoch 260, training loss: 643.98388671875 = 1.6622521877288818 + 100.0 * 6.423216342926025
Epoch 260, val loss: 1.681668758392334
Epoch 270, training loss: 643.1614990234375 = 1.64449942111969 + 100.0 * 6.415170192718506
Epoch 270, val loss: 1.667212963104248
Epoch 280, training loss: 642.3602905273438 = 1.6257864236831665 + 100.0 * 6.407344818115234
Epoch 280, val loss: 1.6522835493087769
Epoch 290, training loss: 641.5848999023438 = 1.6063405275344849 + 100.0 * 6.399785995483398
Epoch 290, val loss: 1.6368235349655151
Epoch 300, training loss: 640.910888671875 = 1.5860483646392822 + 100.0 * 6.393248558044434
Epoch 300, val loss: 1.6208394765853882
Epoch 310, training loss: 640.8977661132812 = 1.564858317375183 + 100.0 * 6.393328666687012
Epoch 310, val loss: 1.6042338609695435
Epoch 320, training loss: 639.7461547851562 = 1.5430970191955566 + 100.0 * 6.382030487060547
Epoch 320, val loss: 1.587538242340088
Epoch 330, training loss: 639.2297973632812 = 1.52092444896698 + 100.0 * 6.37708854675293
Epoch 330, val loss: 1.5707920789718628
Epoch 340, training loss: 638.6025390625 = 1.498487949371338 + 100.0 * 6.3710408210754395
Epoch 340, val loss: 1.5540297031402588
Epoch 350, training loss: 638.7591552734375 = 1.475855827331543 + 100.0 * 6.372833251953125
Epoch 350, val loss: 1.5373871326446533
Epoch 360, training loss: 637.8816528320312 = 1.452712893486023 + 100.0 * 6.364289283752441
Epoch 360, val loss: 1.5205802917480469
Epoch 370, training loss: 637.2354125976562 = 1.4296422004699707 + 100.0 * 6.358057975769043
Epoch 370, val loss: 1.504090428352356
Epoch 380, training loss: 636.770751953125 = 1.4064950942993164 + 100.0 * 6.353642463684082
Epoch 380, val loss: 1.487785816192627
Epoch 390, training loss: 636.8564453125 = 1.383267879486084 + 100.0 * 6.354732036590576
Epoch 390, val loss: 1.4716435670852661
Epoch 400, training loss: 636.5372314453125 = 1.3600155115127563 + 100.0 * 6.351772308349609
Epoch 400, val loss: 1.4557918310165405
Epoch 410, training loss: 635.8323974609375 = 1.3366936445236206 + 100.0 * 6.34495735168457
Epoch 410, val loss: 1.4400784969329834
Epoch 420, training loss: 635.3536376953125 = 1.313384771347046 + 100.0 * 6.340403079986572
Epoch 420, val loss: 1.4245946407318115
Epoch 430, training loss: 634.9691772460938 = 1.2901160717010498 + 100.0 * 6.336790561676025
Epoch 430, val loss: 1.4092744588851929
Epoch 440, training loss: 635.2503662109375 = 1.2669614553451538 + 100.0 * 6.339834213256836
Epoch 440, val loss: 1.3942344188690186
Epoch 450, training loss: 634.3716430664062 = 1.2433491945266724 + 100.0 * 6.331283092498779
Epoch 450, val loss: 1.3790770769119263
Epoch 460, training loss: 634.1570434570312 = 1.2200775146484375 + 100.0 * 6.32936954498291
Epoch 460, val loss: 1.3642889261245728
Epoch 470, training loss: 633.846923828125 = 1.1968052387237549 + 100.0 * 6.326501369476318
Epoch 470, val loss: 1.3497307300567627
Epoch 480, training loss: 633.5160522460938 = 1.1736327409744263 + 100.0 * 6.323423862457275
Epoch 480, val loss: 1.335397720336914
Epoch 490, training loss: 634.093505859375 = 1.1504180431365967 + 100.0 * 6.329431056976318
Epoch 490, val loss: 1.3208822011947632
Epoch 500, training loss: 633.1339721679688 = 1.1272690296173096 + 100.0 * 6.320066928863525
Epoch 500, val loss: 1.3071858882904053
Epoch 510, training loss: 632.8265991210938 = 1.1041996479034424 + 100.0 * 6.317224025726318
Epoch 510, val loss: 1.2933279275894165
Epoch 520, training loss: 632.5740356445312 = 1.0813945531845093 + 100.0 * 6.3149261474609375
Epoch 520, val loss: 1.2798491716384888
Epoch 530, training loss: 632.3251953125 = 1.0588228702545166 + 100.0 * 6.312663555145264
Epoch 530, val loss: 1.2666778564453125
Epoch 540, training loss: 632.5034790039062 = 1.0363397598266602 + 100.0 * 6.314671516418457
Epoch 540, val loss: 1.2536777257919312
Epoch 550, training loss: 632.068115234375 = 1.0140690803527832 + 100.0 * 6.310540199279785
Epoch 550, val loss: 1.2409816980361938
Epoch 560, training loss: 631.6128540039062 = 0.9920935034751892 + 100.0 * 6.306207656860352
Epoch 560, val loss: 1.2288233041763306
Epoch 570, training loss: 631.4417114257812 = 0.9704803228378296 + 100.0 * 6.304712772369385
Epoch 570, val loss: 1.2169928550720215
Epoch 580, training loss: 632.2077026367188 = 0.9491193294525146 + 100.0 * 6.312586307525635
Epoch 580, val loss: 1.205426573753357
Epoch 590, training loss: 631.0927734375 = 0.9281111359596252 + 100.0 * 6.301646709442139
Epoch 590, val loss: 1.1945527791976929
Epoch 600, training loss: 630.898681640625 = 0.9074282050132751 + 100.0 * 6.299912929534912
Epoch 600, val loss: 1.1839706897735596
Epoch 610, training loss: 631.1336059570312 = 0.887234628200531 + 100.0 * 6.302464008331299
Epoch 610, val loss: 1.174005389213562
Epoch 620, training loss: 630.5350341796875 = 0.8673468828201294 + 100.0 * 6.2966766357421875
Epoch 620, val loss: 1.1643121242523193
Epoch 630, training loss: 630.2864990234375 = 0.8480058908462524 + 100.0 * 6.294384956359863
Epoch 630, val loss: 1.1555198431015015
Epoch 640, training loss: 630.1610717773438 = 0.8291844129562378 + 100.0 * 6.293319225311279
Epoch 640, val loss: 1.147471308708191
Epoch 650, training loss: 630.2734375 = 0.8106914758682251 + 100.0 * 6.294627666473389
Epoch 650, val loss: 1.139406442642212
Epoch 660, training loss: 630.06640625 = 0.7926318645477295 + 100.0 * 6.29273796081543
Epoch 660, val loss: 1.1325602531433105
Epoch 670, training loss: 629.6925048828125 = 0.7750424146652222 + 100.0 * 6.289175033569336
Epoch 670, val loss: 1.1260172128677368
Epoch 680, training loss: 630.7372436523438 = 0.7580624222755432 + 100.0 * 6.2997918128967285
Epoch 680, val loss: 1.120411992073059
Epoch 690, training loss: 629.346435546875 = 0.741095781326294 + 100.0 * 6.28605318069458
Epoch 690, val loss: 1.1143289804458618
Epoch 700, training loss: 629.2308349609375 = 0.7247560024261475 + 100.0 * 6.285060882568359
Epoch 700, val loss: 1.1091948747634888
Epoch 710, training loss: 629.0031127929688 = 0.7089853882789612 + 100.0 * 6.2829413414001465
Epoch 710, val loss: 1.1048295497894287
Epoch 720, training loss: 629.3912353515625 = 0.6936289072036743 + 100.0 * 6.286976337432861
Epoch 720, val loss: 1.1013786792755127
Epoch 730, training loss: 628.9984741210938 = 0.6784006953239441 + 100.0 * 6.283200740814209
Epoch 730, val loss: 1.097367763519287
Epoch 740, training loss: 628.8609619140625 = 0.6636835932731628 + 100.0 * 6.281972408294678
Epoch 740, val loss: 1.0950088500976562
Epoch 750, training loss: 628.5028076171875 = 0.6492936611175537 + 100.0 * 6.278534889221191
Epoch 750, val loss: 1.0924601554870605
Epoch 760, training loss: 628.6427001953125 = 0.6353274583816528 + 100.0 * 6.280074119567871
Epoch 760, val loss: 1.0906438827514648
Epoch 770, training loss: 628.2124633789062 = 0.621622622013092 + 100.0 * 6.275908470153809
Epoch 770, val loss: 1.088960886001587
Epoch 780, training loss: 628.2286987304688 = 0.6082881689071655 + 100.0 * 6.2762041091918945
Epoch 780, val loss: 1.0879478454589844
Epoch 790, training loss: 628.039794921875 = 0.5951627492904663 + 100.0 * 6.274446487426758
Epoch 790, val loss: 1.0870906114578247
Epoch 800, training loss: 627.9862060546875 = 0.5823961496353149 + 100.0 * 6.274038314819336
Epoch 800, val loss: 1.0866670608520508
Epoch 810, training loss: 627.7391967773438 = 0.5699723362922668 + 100.0 * 6.271692752838135
Epoch 810, val loss: 1.086551547050476
Epoch 820, training loss: 627.6840209960938 = 0.5579230785369873 + 100.0 * 6.271260738372803
Epoch 820, val loss: 1.0871810913085938
Epoch 830, training loss: 627.7168579101562 = 0.5460310578346252 + 100.0 * 6.2717084884643555
Epoch 830, val loss: 1.0874898433685303
Epoch 840, training loss: 627.6986083984375 = 0.5343530774116516 + 100.0 * 6.271642684936523
Epoch 840, val loss: 1.0883468389511108
Epoch 850, training loss: 627.3582763671875 = 0.5230045914649963 + 100.0 * 6.268352508544922
Epoch 850, val loss: 1.0894824266433716
Epoch 860, training loss: 627.18505859375 = 0.5119908452033997 + 100.0 * 6.266731262207031
Epoch 860, val loss: 1.0908808708190918
Epoch 870, training loss: 628.1402587890625 = 0.5012199878692627 + 100.0 * 6.276390075683594
Epoch 870, val loss: 1.0922578573226929
Epoch 880, training loss: 627.3409423828125 = 0.4906684458255768 + 100.0 * 6.268502712249756
Epoch 880, val loss: 1.0946015119552612
Epoch 890, training loss: 626.92138671875 = 0.4803374707698822 + 100.0 * 6.264410495758057
Epoch 890, val loss: 1.0966260433197021
Epoch 900, training loss: 626.7767333984375 = 0.4703333079814911 + 100.0 * 6.263063907623291
Epoch 900, val loss: 1.0990428924560547
Epoch 910, training loss: 627.333984375 = 0.4605373442173004 + 100.0 * 6.268734931945801
Epoch 910, val loss: 1.101347804069519
Epoch 920, training loss: 627.3260498046875 = 0.4509798288345337 + 100.0 * 6.2687506675720215
Epoch 920, val loss: 1.1044323444366455
Epoch 930, training loss: 626.5816650390625 = 0.4415096640586853 + 100.0 * 6.261401176452637
Epoch 930, val loss: 1.1074066162109375
Epoch 940, training loss: 626.4705200195312 = 0.4323868751525879 + 100.0 * 6.26038122177124
Epoch 940, val loss: 1.1106133460998535
Epoch 950, training loss: 626.6017456054688 = 0.42352917790412903 + 100.0 * 6.261782169342041
Epoch 950, val loss: 1.1140704154968262
Epoch 960, training loss: 626.568359375 = 0.4147895872592926 + 100.0 * 6.26153564453125
Epoch 960, val loss: 1.11741042137146
Epoch 970, training loss: 626.3383178710938 = 0.40620607137680054 + 100.0 * 6.259321212768555
Epoch 970, val loss: 1.120603322982788
Epoch 980, training loss: 626.5147705078125 = 0.3979280889034271 + 100.0 * 6.261168479919434
Epoch 980, val loss: 1.1248496770858765
Epoch 990, training loss: 626.2662963867188 = 0.38973140716552734 + 100.0 * 6.258765697479248
Epoch 990, val loss: 1.1282075643539429
Epoch 1000, training loss: 626.1322021484375 = 0.38172298669815063 + 100.0 * 6.257504940032959
Epoch 1000, val loss: 1.1317245960235596
Epoch 1010, training loss: 626.0686645507812 = 0.3739544451236725 + 100.0 * 6.256947040557861
Epoch 1010, val loss: 1.1355910301208496
Epoch 1020, training loss: 626.268798828125 = 0.3663853704929352 + 100.0 * 6.259024143218994
Epoch 1020, val loss: 1.1400290727615356
Epoch 1030, training loss: 625.9611206054688 = 0.3588832914829254 + 100.0 * 6.2560224533081055
Epoch 1030, val loss: 1.1440091133117676
Epoch 1040, training loss: 625.8078002929688 = 0.3515746295452118 + 100.0 * 6.2545623779296875
Epoch 1040, val loss: 1.1478184461593628
Epoch 1050, training loss: 625.9495239257812 = 0.3444332778453827 + 100.0 * 6.256051063537598
Epoch 1050, val loss: 1.1517530679702759
Epoch 1060, training loss: 625.708251953125 = 0.33742183446884155 + 100.0 * 6.253708362579346
Epoch 1060, val loss: 1.1557081937789917
Epoch 1070, training loss: 625.92578125 = 0.33056631684303284 + 100.0 * 6.25595235824585
Epoch 1070, val loss: 1.1599135398864746
Epoch 1080, training loss: 625.5486450195312 = 0.3238106966018677 + 100.0 * 6.252248764038086
Epoch 1080, val loss: 1.164452075958252
Epoch 1090, training loss: 625.4111328125 = 0.31724053621292114 + 100.0 * 6.250938892364502
Epoch 1090, val loss: 1.1689846515655518
Epoch 1100, training loss: 625.3076171875 = 0.3107891082763672 + 100.0 * 6.2499680519104
Epoch 1100, val loss: 1.1731510162353516
Epoch 1110, training loss: 625.2402954101562 = 0.3045375943183899 + 100.0 * 6.249357223510742
Epoch 1110, val loss: 1.1775145530700684
Epoch 1120, training loss: 625.6415405273438 = 0.2983669638633728 + 100.0 * 6.253432273864746
Epoch 1120, val loss: 1.1813541650772095
Epoch 1130, training loss: 625.4789428710938 = 0.2922923266887665 + 100.0 * 6.251866340637207
Epoch 1130, val loss: 1.1862261295318604
Epoch 1140, training loss: 625.3888549804688 = 0.28630995750427246 + 100.0 * 6.251025676727295
Epoch 1140, val loss: 1.1901973485946655
Epoch 1150, training loss: 625.2039184570312 = 0.2805228531360626 + 100.0 * 6.249233722686768
Epoch 1150, val loss: 1.1951758861541748
Epoch 1160, training loss: 624.89453125 = 0.2748238444328308 + 100.0 * 6.246196746826172
Epoch 1160, val loss: 1.1997913122177124
Epoch 1170, training loss: 624.9116821289062 = 0.2692728638648987 + 100.0 * 6.246423721313477
Epoch 1170, val loss: 1.2043663263320923
Epoch 1180, training loss: 625.3367919921875 = 0.26385971903800964 + 100.0 * 6.250729560852051
Epoch 1180, val loss: 1.209465742111206
Epoch 1190, training loss: 624.9498901367188 = 0.25841429829597473 + 100.0 * 6.246914386749268
Epoch 1190, val loss: 1.2131669521331787
Epoch 1200, training loss: 624.8935546875 = 0.2531478703022003 + 100.0 * 6.246403694152832
Epoch 1200, val loss: 1.2182921171188354
Epoch 1210, training loss: 625.145263671875 = 0.2479759156703949 + 100.0 * 6.2489728927612305
Epoch 1210, val loss: 1.2228199243545532
Epoch 1220, training loss: 624.6574096679688 = 0.24279488623142242 + 100.0 * 6.244146347045898
Epoch 1220, val loss: 1.2274454832077026
Epoch 1230, training loss: 624.5233154296875 = 0.23780518770217896 + 100.0 * 6.242855072021484
Epoch 1230, val loss: 1.2316559553146362
Epoch 1240, training loss: 624.49609375 = 0.2329654097557068 + 100.0 * 6.242631435394287
Epoch 1240, val loss: 1.2369388341903687
Epoch 1250, training loss: 625.2516479492188 = 0.22817537188529968 + 100.0 * 6.250235080718994
Epoch 1250, val loss: 1.2410324811935425
Epoch 1260, training loss: 624.5773315429688 = 0.22345325350761414 + 100.0 * 6.243538856506348
Epoch 1260, val loss: 1.246660828590393
Epoch 1270, training loss: 624.4812622070312 = 0.21880681812763214 + 100.0 * 6.242624282836914
Epoch 1270, val loss: 1.251200795173645
Epoch 1280, training loss: 624.5059814453125 = 0.2142820507287979 + 100.0 * 6.242917060852051
Epoch 1280, val loss: 1.2562156915664673
Epoch 1290, training loss: 624.261962890625 = 0.20984050631523132 + 100.0 * 6.240521430969238
Epoch 1290, val loss: 1.261136770248413
Epoch 1300, training loss: 624.3992919921875 = 0.2054765820503235 + 100.0 * 6.241937637329102
Epoch 1300, val loss: 1.265868067741394
Epoch 1310, training loss: 624.5146484375 = 0.2012007236480713 + 100.0 * 6.24313497543335
Epoch 1310, val loss: 1.2709248065948486
Epoch 1320, training loss: 624.2565307617188 = 0.19696033000946045 + 100.0 * 6.240595817565918
Epoch 1320, val loss: 1.2759653329849243
Epoch 1330, training loss: 624.455078125 = 0.19285745918750763 + 100.0 * 6.242622375488281
Epoch 1330, val loss: 1.28157377243042
Epoch 1340, training loss: 624.1376953125 = 0.18878819048404694 + 100.0 * 6.2394890785217285
Epoch 1340, val loss: 1.2862316370010376
Epoch 1350, training loss: 624.2013549804688 = 0.18482327461242676 + 100.0 * 6.2401652336120605
Epoch 1350, val loss: 1.2918938398361206
Epoch 1360, training loss: 623.960205078125 = 0.18089236319065094 + 100.0 * 6.23779296875
Epoch 1360, val loss: 1.2965532541275024
Epoch 1370, training loss: 623.892578125 = 0.1770978420972824 + 100.0 * 6.237154483795166
Epoch 1370, val loss: 1.3018670082092285
Epoch 1380, training loss: 624.2874145507812 = 0.17337529361248016 + 100.0 * 6.241140365600586
Epoch 1380, val loss: 1.307062029838562
Epoch 1390, training loss: 624.3997802734375 = 0.16974379122257233 + 100.0 * 6.242300510406494
Epoch 1390, val loss: 1.313569188117981
Epoch 1400, training loss: 624.01708984375 = 0.16603690385818481 + 100.0 * 6.238510608673096
Epoch 1400, val loss: 1.3177517652511597
Epoch 1410, training loss: 623.8104248046875 = 0.16251160204410553 + 100.0 * 6.236478805541992
Epoch 1410, val loss: 1.3234456777572632
Epoch 1420, training loss: 623.6367797851562 = 0.15907423198223114 + 100.0 * 6.234777450561523
Epoch 1420, val loss: 1.3289064168930054
Epoch 1430, training loss: 623.5698852539062 = 0.1557345688343048 + 100.0 * 6.2341413497924805
Epoch 1430, val loss: 1.3347722291946411
Epoch 1440, training loss: 624.0470581054688 = 0.1524544358253479 + 100.0 * 6.238945960998535
Epoch 1440, val loss: 1.3401625156402588
Epoch 1450, training loss: 624.0985107421875 = 0.14917074143886566 + 100.0 * 6.239493370056152
Epoch 1450, val loss: 1.3454474210739136
Epoch 1460, training loss: 623.828369140625 = 0.14597898721694946 + 100.0 * 6.236824035644531
Epoch 1460, val loss: 1.351708173751831
Epoch 1470, training loss: 623.4426879882812 = 0.14284485578536987 + 100.0 * 6.232998847961426
Epoch 1470, val loss: 1.3572458028793335
Epoch 1480, training loss: 623.3719482421875 = 0.13982798159122467 + 100.0 * 6.232321262359619
Epoch 1480, val loss: 1.3630805015563965
Epoch 1490, training loss: 623.8668212890625 = 0.13688130676746368 + 100.0 * 6.237298965454102
Epoch 1490, val loss: 1.368451476097107
Epoch 1500, training loss: 623.368896484375 = 0.13394205272197723 + 100.0 * 6.232349395751953
Epoch 1500, val loss: 1.3748605251312256
Epoch 1510, training loss: 623.528076171875 = 0.13107667863368988 + 100.0 * 6.233969688415527
Epoch 1510, val loss: 1.3803553581237793
Epoch 1520, training loss: 623.939697265625 = 0.12826533615589142 + 100.0 * 6.238114356994629
Epoch 1520, val loss: 1.3863979578018188
Epoch 1530, training loss: 623.399169921875 = 0.12554427981376648 + 100.0 * 6.232736110687256
Epoch 1530, val loss: 1.3930346965789795
Epoch 1540, training loss: 623.268798828125 = 0.12286928296089172 + 100.0 * 6.231459617614746
Epoch 1540, val loss: 1.3986505270004272
Epoch 1550, training loss: 623.390625 = 0.12029805034399033 + 100.0 * 6.23270320892334
Epoch 1550, val loss: 1.40518057346344
Epoch 1560, training loss: 623.2693481445312 = 0.11773562431335449 + 100.0 * 6.231515884399414
Epoch 1560, val loss: 1.4109889268875122
Epoch 1570, training loss: 623.230224609375 = 0.11521735787391663 + 100.0 * 6.231149673461914
Epoch 1570, val loss: 1.4169390201568604
Epoch 1580, training loss: 623.3063354492188 = 0.11279866099357605 + 100.0 * 6.231935501098633
Epoch 1580, val loss: 1.423317313194275
Epoch 1590, training loss: 623.31982421875 = 0.11040593683719635 + 100.0 * 6.2320942878723145
Epoch 1590, val loss: 1.4294438362121582
Epoch 1600, training loss: 623.015869140625 = 0.10805272310972214 + 100.0 * 6.22907829284668
Epoch 1600, val loss: 1.4356366395950317
Epoch 1610, training loss: 623.0399780273438 = 0.10577885061502457 + 100.0 * 6.229341983795166
Epoch 1610, val loss: 1.4418445825576782
Epoch 1620, training loss: 623.3897094726562 = 0.10357671231031418 + 100.0 * 6.232861042022705
Epoch 1620, val loss: 1.448429822921753
Epoch 1630, training loss: 623.2999267578125 = 0.1013759970664978 + 100.0 * 6.231985569000244
Epoch 1630, val loss: 1.4541499614715576
Epoch 1640, training loss: 623.135009765625 = 0.09923204779624939 + 100.0 * 6.230357646942139
Epoch 1640, val loss: 1.4605801105499268
Epoch 1650, training loss: 622.839111328125 = 0.09712198376655579 + 100.0 * 6.227419853210449
Epoch 1650, val loss: 1.4664576053619385
Epoch 1660, training loss: 623.0596923828125 = 0.09509922564029694 + 100.0 * 6.2296462059021
Epoch 1660, val loss: 1.472416877746582
Epoch 1670, training loss: 622.893798828125 = 0.09311924874782562 + 100.0 * 6.228006362915039
Epoch 1670, val loss: 1.4788938760757446
Epoch 1680, training loss: 622.7647705078125 = 0.09119153022766113 + 100.0 * 6.226735591888428
Epoch 1680, val loss: 1.4853401184082031
Epoch 1690, training loss: 622.8795166015625 = 0.0893157348036766 + 100.0 * 6.227902412414551
Epoch 1690, val loss: 1.4914226531982422
Epoch 1700, training loss: 623.8155517578125 = 0.08745423704385757 + 100.0 * 6.23728084564209
Epoch 1700, val loss: 1.4966349601745605
Epoch 1710, training loss: 622.88427734375 = 0.08563843369483948 + 100.0 * 6.2279863357543945
Epoch 1710, val loss: 1.5041700601577759
Epoch 1720, training loss: 622.6001586914062 = 0.08385521173477173 + 100.0 * 6.225162982940674
Epoch 1720, val loss: 1.5102461576461792
Epoch 1730, training loss: 622.5787353515625 = 0.08214401453733444 + 100.0 * 6.224965572357178
Epoch 1730, val loss: 1.5164730548858643
Epoch 1740, training loss: 622.7632446289062 = 0.08050112426280975 + 100.0 * 6.226827621459961
Epoch 1740, val loss: 1.5230506658554077
Epoch 1750, training loss: 622.868408203125 = 0.07885237038135529 + 100.0 * 6.227895259857178
Epoch 1750, val loss: 1.5290788412094116
Epoch 1760, training loss: 622.8508911132812 = 0.07725255191326141 + 100.0 * 6.227736473083496
Epoch 1760, val loss: 1.5354905128479004
Epoch 1770, training loss: 622.9240112304688 = 0.07570488005876541 + 100.0 * 6.228483200073242
Epoch 1770, val loss: 1.5419563055038452
Epoch 1780, training loss: 622.5217895507812 = 0.07414443045854568 + 100.0 * 6.224476337432861
Epoch 1780, val loss: 1.5477784872055054
Epoch 1790, training loss: 622.4666748046875 = 0.0726643055677414 + 100.0 * 6.223939895629883
Epoch 1790, val loss: 1.5540827512741089
Epoch 1800, training loss: 622.5286254882812 = 0.07122720032930374 + 100.0 * 6.224574089050293
Epoch 1800, val loss: 1.5602396726608276
Epoch 1810, training loss: 622.6320190429688 = 0.06980975717306137 + 100.0 * 6.225621700286865
Epoch 1810, val loss: 1.566330909729004
Epoch 1820, training loss: 622.7031860351562 = 0.06843262165784836 + 100.0 * 6.22634744644165
Epoch 1820, val loss: 1.5725501775741577
Epoch 1830, training loss: 622.7047119140625 = 0.0670558288693428 + 100.0 * 6.226376056671143
Epoch 1830, val loss: 1.5780749320983887
Epoch 1840, training loss: 622.6331787109375 = 0.06572496891021729 + 100.0 * 6.225674629211426
Epoch 1840, val loss: 1.5843896865844727
Epoch 1850, training loss: 622.3002319335938 = 0.06443630903959274 + 100.0 * 6.222357749938965
Epoch 1850, val loss: 1.590923547744751
Epoch 1860, training loss: 622.2465209960938 = 0.06317295134067535 + 100.0 * 6.2218337059021
Epoch 1860, val loss: 1.5968703031539917
Epoch 1870, training loss: 622.2055053710938 = 0.061962854117155075 + 100.0 * 6.221435546875
Epoch 1870, val loss: 1.603108525276184
Epoch 1880, training loss: 623.0146484375 = 0.06079679727554321 + 100.0 * 6.229538440704346
Epoch 1880, val loss: 1.6092872619628906
Epoch 1890, training loss: 622.3919677734375 = 0.05959620326757431 + 100.0 * 6.223323822021484
Epoch 1890, val loss: 1.6148630380630493
Epoch 1900, training loss: 623.0326538085938 = 0.05847259983420372 + 100.0 * 6.22974157333374
Epoch 1900, val loss: 1.6212866306304932
Epoch 1910, training loss: 622.2999877929688 = 0.057302433997392654 + 100.0 * 6.222426891326904
Epoch 1910, val loss: 1.6267340183258057
Epoch 1920, training loss: 622.0936279296875 = 0.05621504783630371 + 100.0 * 6.22037410736084
Epoch 1920, val loss: 1.632905125617981
Epoch 1930, training loss: 622.0680541992188 = 0.055162083357572556 + 100.0 * 6.220129013061523
Epoch 1930, val loss: 1.638832449913025
Epoch 1940, training loss: 622.5225830078125 = 0.05414089933037758 + 100.0 * 6.224684238433838
Epoch 1940, val loss: 1.6447423696517944
Epoch 1950, training loss: 622.091064453125 = 0.053106795996427536 + 100.0 * 6.220379829406738
Epoch 1950, val loss: 1.6506046056747437
Epoch 1960, training loss: 621.9595336914062 = 0.05211286619305611 + 100.0 * 6.219074249267578
Epoch 1960, val loss: 1.6566376686096191
Epoch 1970, training loss: 622.0814819335938 = 0.051159244030714035 + 100.0 * 6.220303535461426
Epoch 1970, val loss: 1.6628035306930542
Epoch 1980, training loss: 622.6063232421875 = 0.050233352929353714 + 100.0 * 6.225561141967773
Epoch 1980, val loss: 1.6689295768737793
Epoch 1990, training loss: 622.1890869140625 = 0.04926520213484764 + 100.0 * 6.22139835357666
Epoch 1990, val loss: 1.6735470294952393
Epoch 2000, training loss: 621.9625854492188 = 0.048367805778980255 + 100.0 * 6.219142436981201
Epoch 2000, val loss: 1.6798198223114014
Epoch 2010, training loss: 621.91748046875 = 0.04749053716659546 + 100.0 * 6.218699932098389
Epoch 2010, val loss: 1.6853076219558716
Epoch 2020, training loss: 622.4468383789062 = 0.04665789380669594 + 100.0 * 6.224001884460449
Epoch 2020, val loss: 1.6913641691207886
Epoch 2030, training loss: 622.3699340820312 = 0.04581276327371597 + 100.0 * 6.223240852355957
Epoch 2030, val loss: 1.6966890096664429
Epoch 2040, training loss: 622.0247802734375 = 0.04494823142886162 + 100.0 * 6.219798564910889
Epoch 2040, val loss: 1.70156729221344
Epoch 2050, training loss: 621.78955078125 = 0.044153109192848206 + 100.0 * 6.217454433441162
Epoch 2050, val loss: 1.7077510356903076
Epoch 2060, training loss: 621.7312622070312 = 0.04336859658360481 + 100.0 * 6.216878414154053
Epoch 2060, val loss: 1.7131099700927734
Epoch 2070, training loss: 621.701171875 = 0.04261675477027893 + 100.0 * 6.216585636138916
Epoch 2070, val loss: 1.7186843156814575
Epoch 2080, training loss: 622.964111328125 = 0.04190957173705101 + 100.0 * 6.229221820831299
Epoch 2080, val loss: 1.723969578742981
Epoch 2090, training loss: 622.3277587890625 = 0.04112458974123001 + 100.0 * 6.222866058349609
Epoch 2090, val loss: 1.7290791273117065
Epoch 2100, training loss: 621.8814086914062 = 0.04038826748728752 + 100.0 * 6.218410015106201
Epoch 2100, val loss: 1.7346727848052979
Epoch 2110, training loss: 621.6370849609375 = 0.03968530520796776 + 100.0 * 6.215973854064941
Epoch 2110, val loss: 1.7399100065231323
Epoch 2120, training loss: 621.8870239257812 = 0.039008524268865585 + 100.0 * 6.218480110168457
Epoch 2120, val loss: 1.7447540760040283
Epoch 2130, training loss: 621.7993774414062 = 0.03834060579538345 + 100.0 * 6.2176103591918945
Epoch 2130, val loss: 1.7502299547195435
Epoch 2140, training loss: 621.773193359375 = 0.03770234435796738 + 100.0 * 6.217354774475098
Epoch 2140, val loss: 1.7562590837478638
Epoch 2150, training loss: 621.9475708007812 = 0.03705611452460289 + 100.0 * 6.219105243682861
Epoch 2150, val loss: 1.7608885765075684
Epoch 2160, training loss: 621.807861328125 = 0.03644189238548279 + 100.0 * 6.217714309692383
Epoch 2160, val loss: 1.7665719985961914
Epoch 2170, training loss: 621.7155151367188 = 0.035813767462968826 + 100.0 * 6.216796875
Epoch 2170, val loss: 1.7710720300674438
Epoch 2180, training loss: 621.57421875 = 0.03522328659892082 + 100.0 * 6.215390205383301
Epoch 2180, val loss: 1.7767436504364014
Epoch 2190, training loss: 621.6408081054688 = 0.0346502847969532 + 100.0 * 6.216061115264893
Epoch 2190, val loss: 1.781676173210144
Epoch 2200, training loss: 621.6754760742188 = 0.03407928720116615 + 100.0 * 6.216413974761963
Epoch 2200, val loss: 1.7865477800369263
Epoch 2210, training loss: 621.936279296875 = 0.03352980688214302 + 100.0 * 6.219027519226074
Epoch 2210, val loss: 1.7914787530899048
Epoch 2220, training loss: 621.5592651367188 = 0.03296532481908798 + 100.0 * 6.2152628898620605
Epoch 2220, val loss: 1.7962285280227661
Epoch 2230, training loss: 621.6489868164062 = 0.03243107721209526 + 100.0 * 6.216165542602539
Epoch 2230, val loss: 1.8010698556900024
Epoch 2240, training loss: 621.6884155273438 = 0.031903136521577835 + 100.0 * 6.216564655303955
Epoch 2240, val loss: 1.805335521697998
Epoch 2250, training loss: 621.613525390625 = 0.03139360994100571 + 100.0 * 6.215821743011475
Epoch 2250, val loss: 1.8106979131698608
Epoch 2260, training loss: 621.4547729492188 = 0.030886482447385788 + 100.0 * 6.214239120483398
Epoch 2260, val loss: 1.8152422904968262
Epoch 2270, training loss: 621.4993286132812 = 0.030400292947888374 + 100.0 * 6.214689254760742
Epoch 2270, val loss: 1.8199539184570312
Epoch 2280, training loss: 621.9068603515625 = 0.02992265485227108 + 100.0 * 6.218769550323486
Epoch 2280, val loss: 1.824326753616333
Epoch 2290, training loss: 621.3587646484375 = 0.02944706380367279 + 100.0 * 6.213293552398682
Epoch 2290, val loss: 1.8294286727905273
Epoch 2300, training loss: 621.4036254882812 = 0.029001979157328606 + 100.0 * 6.213746070861816
Epoch 2300, val loss: 1.8343989849090576
Epoch 2310, training loss: 621.369384765625 = 0.02854839339852333 + 100.0 * 6.213408470153809
Epoch 2310, val loss: 1.838889241218567
Epoch 2320, training loss: 621.9186401367188 = 0.028117721900343895 + 100.0 * 6.218904972076416
Epoch 2320, val loss: 1.8434333801269531
Epoch 2330, training loss: 621.5601806640625 = 0.027678724378347397 + 100.0 * 6.215325355529785
Epoch 2330, val loss: 1.848157525062561
Epoch 2340, training loss: 621.6173095703125 = 0.027247246354818344 + 100.0 * 6.215900897979736
Epoch 2340, val loss: 1.8524240255355835
Epoch 2350, training loss: 621.461181640625 = 0.026830965653061867 + 100.0 * 6.214343070983887
Epoch 2350, val loss: 1.8565971851348877
Epoch 2360, training loss: 621.3989868164062 = 0.026422934606671333 + 100.0 * 6.213725566864014
Epoch 2360, val loss: 1.8609371185302734
Epoch 2370, training loss: 621.3392333984375 = 0.026034072041511536 + 100.0 * 6.213131427764893
Epoch 2370, val loss: 1.8656989336013794
Epoch 2380, training loss: 621.344482421875 = 0.025649312883615494 + 100.0 * 6.213188171386719
Epoch 2380, val loss: 1.870327115058899
Epoch 2390, training loss: 621.641357421875 = 0.02528287284076214 + 100.0 * 6.216160774230957
Epoch 2390, val loss: 1.8750625848770142
Epoch 2400, training loss: 621.24267578125 = 0.024889083579182625 + 100.0 * 6.212177753448486
Epoch 2400, val loss: 1.878752589225769
Epoch 2410, training loss: 621.1552124023438 = 0.024519214406609535 + 100.0 * 6.211306571960449
Epoch 2410, val loss: 1.882751226425171
Epoch 2420, training loss: 621.1353759765625 = 0.02416912466287613 + 100.0 * 6.211112022399902
Epoch 2420, val loss: 1.8874372243881226
Epoch 2430, training loss: 621.1730346679688 = 0.023822298273444176 + 100.0 * 6.211492538452148
Epoch 2430, val loss: 1.8913289308547974
Epoch 2440, training loss: 621.4521484375 = 0.023481858894228935 + 100.0 * 6.2142863273620605
Epoch 2440, val loss: 1.8953310251235962
Epoch 2450, training loss: 621.7958374023438 = 0.023147139698266983 + 100.0 * 6.217727184295654
Epoch 2450, val loss: 1.8997137546539307
Epoch 2460, training loss: 621.3211059570312 = 0.022814398631453514 + 100.0 * 6.212982654571533
Epoch 2460, val loss: 1.9038686752319336
Epoch 2470, training loss: 621.4926147460938 = 0.022492628544569016 + 100.0 * 6.214700698852539
Epoch 2470, val loss: 1.908482551574707
Epoch 2480, training loss: 621.04736328125 = 0.02216125838458538 + 100.0 * 6.210252285003662
Epoch 2480, val loss: 1.9119484424591064
Epoch 2490, training loss: 621.0134887695312 = 0.021853914484381676 + 100.0 * 6.209916114807129
Epoch 2490, val loss: 1.9158161878585815
Epoch 2500, training loss: 620.9833374023438 = 0.021554552018642426 + 100.0 * 6.209617614746094
Epoch 2500, val loss: 1.919805645942688
Epoch 2510, training loss: 621.1773681640625 = 0.021262073889374733 + 100.0 * 6.21156120300293
Epoch 2510, val loss: 1.9234185218811035
Epoch 2520, training loss: 621.1981201171875 = 0.020971378311514854 + 100.0 * 6.211771011352539
Epoch 2520, val loss: 1.9277465343475342
Epoch 2530, training loss: 621.32470703125 = 0.020695529878139496 + 100.0 * 6.213040351867676
Epoch 2530, val loss: 1.9324250221252441
Epoch 2540, training loss: 621.5076293945312 = 0.020407890900969505 + 100.0 * 6.214872360229492
Epoch 2540, val loss: 1.935735821723938
Epoch 2550, training loss: 621.0144653320312 = 0.020117497071623802 + 100.0 * 6.2099432945251465
Epoch 2550, val loss: 1.9390813112258911
Epoch 2560, training loss: 621.1929321289062 = 0.01985161565244198 + 100.0 * 6.21173095703125
Epoch 2560, val loss: 1.943324327468872
Epoch 2570, training loss: 621.072509765625 = 0.01957869715988636 + 100.0 * 6.210529327392578
Epoch 2570, val loss: 1.9465230703353882
Epoch 2580, training loss: 620.9503784179688 = 0.019323259592056274 + 100.0 * 6.209310054779053
Epoch 2580, val loss: 1.951090693473816
Epoch 2590, training loss: 620.8599243164062 = 0.019067367538809776 + 100.0 * 6.208408832550049
Epoch 2590, val loss: 1.9544122219085693
Epoch 2600, training loss: 621.2611083984375 = 0.018822727724909782 + 100.0 * 6.212423324584961
Epoch 2600, val loss: 1.9578887224197388
Epoch 2610, training loss: 620.8634643554688 = 0.01857820339500904 + 100.0 * 6.208448886871338
Epoch 2610, val loss: 1.962019443511963
Epoch 2620, training loss: 620.9180908203125 = 0.018343154340982437 + 100.0 * 6.2089972496032715
Epoch 2620, val loss: 1.9659631252288818
Epoch 2630, training loss: 620.9019165039062 = 0.01810731738805771 + 100.0 * 6.208837985992432
Epoch 2630, val loss: 1.9694856405258179
Epoch 2640, training loss: 621.111083984375 = 0.017877226695418358 + 100.0 * 6.21093225479126
Epoch 2640, val loss: 1.9729430675506592
Epoch 2650, training loss: 620.980712890625 = 0.017647206783294678 + 100.0 * 6.209630489349365
Epoch 2650, val loss: 1.9759690761566162
Epoch 2660, training loss: 621.0961303710938 = 0.017421351745724678 + 100.0 * 6.210787296295166
Epoch 2660, val loss: 1.979455590248108
Epoch 2670, training loss: 620.876708984375 = 0.01720283180475235 + 100.0 * 6.208594799041748
Epoch 2670, val loss: 1.9832268953323364
Epoch 2680, training loss: 621.0997924804688 = 0.016990894451737404 + 100.0 * 6.2108283042907715
Epoch 2680, val loss: 1.986276388168335
Epoch 2690, training loss: 620.7752685546875 = 0.01678132265806198 + 100.0 * 6.207584857940674
Epoch 2690, val loss: 1.9904111623764038
Epoch 2700, training loss: 620.779296875 = 0.01657780632376671 + 100.0 * 6.207626819610596
Epoch 2700, val loss: 1.9939981698989868
Epoch 2710, training loss: 621.0939331054688 = 0.016384027898311615 + 100.0 * 6.210775852203369
Epoch 2710, val loss: 1.9974627494812012
Epoch 2720, training loss: 620.8294677734375 = 0.016171742230653763 + 100.0 * 6.208133220672607
Epoch 2720, val loss: 2.0001721382141113
Epoch 2730, training loss: 620.7861328125 = 0.015969589352607727 + 100.0 * 6.207701683044434
Epoch 2730, val loss: 2.0034162998199463
Epoch 2740, training loss: 620.634521484375 = 0.015782322734594345 + 100.0 * 6.2061872482299805
Epoch 2740, val loss: 2.007141351699829
Epoch 2750, training loss: 621.0231323242188 = 0.015600960701704025 + 100.0 * 6.2100749015808105
Epoch 2750, val loss: 2.0102715492248535
Epoch 2760, training loss: 620.8222045898438 = 0.015408642590045929 + 100.0 * 6.208068370819092
Epoch 2760, val loss: 2.013012170791626
Epoch 2770, training loss: 620.6918334960938 = 0.015224287286400795 + 100.0 * 6.206766605377197
Epoch 2770, val loss: 2.016650438308716
Epoch 2780, training loss: 620.7011108398438 = 0.01504847127944231 + 100.0 * 6.206860065460205
Epoch 2780, val loss: 2.0199763774871826
Epoch 2790, training loss: 621.1408081054688 = 0.014881807379424572 + 100.0 * 6.211258888244629
Epoch 2790, val loss: 2.0233945846557617
Epoch 2800, training loss: 621.0187377929688 = 0.014707112684845924 + 100.0 * 6.210040092468262
Epoch 2800, val loss: 2.0267903804779053
Epoch 2810, training loss: 620.665771484375 = 0.014518971554934978 + 100.0 * 6.206512451171875
Epoch 2810, val loss: 2.0290260314941406
Epoch 2820, training loss: 620.5778198242188 = 0.014353809878230095 + 100.0 * 6.205634593963623
Epoch 2820, val loss: 2.0325140953063965
Epoch 2830, training loss: 620.72314453125 = 0.014194609597325325 + 100.0 * 6.207089424133301
Epoch 2830, val loss: 2.035356044769287
Epoch 2840, training loss: 620.5936279296875 = 0.014031900092959404 + 100.0 * 6.205795764923096
Epoch 2840, val loss: 2.03849196434021
Epoch 2850, training loss: 620.831787109375 = 0.013879586942493916 + 100.0 * 6.208179473876953
Epoch 2850, val loss: 2.0417139530181885
Epoch 2860, training loss: 620.9158935546875 = 0.013729797676205635 + 100.0 * 6.20902156829834
Epoch 2860, val loss: 2.0454514026641846
Epoch 2870, training loss: 620.5247802734375 = 0.013568735681474209 + 100.0 * 6.205112457275391
Epoch 2870, val loss: 2.0477447509765625
Epoch 2880, training loss: 620.453857421875 = 0.013413844630122185 + 100.0 * 6.204404354095459
Epoch 2880, val loss: 2.050546407699585
Epoch 2890, training loss: 620.584716796875 = 0.013271304778754711 + 100.0 * 6.205714702606201
Epoch 2890, val loss: 2.053642988204956
Epoch 2900, training loss: 621.1483154296875 = 0.013136475346982479 + 100.0 * 6.2113518714904785
Epoch 2900, val loss: 2.056550979614258
Epoch 2910, training loss: 620.706298828125 = 0.012988990172743797 + 100.0 * 6.20693302154541
Epoch 2910, val loss: 2.059641122817993
Epoch 2920, training loss: 620.3552856445312 = 0.012836028821766376 + 100.0 * 6.20342493057251
Epoch 2920, val loss: 2.0620923042297363
Epoch 2930, training loss: 620.3321533203125 = 0.012699458748102188 + 100.0 * 6.203194618225098
Epoch 2930, val loss: 2.0649893283843994
Epoch 2940, training loss: 620.34716796875 = 0.012566917575895786 + 100.0 * 6.203346252441406
Epoch 2940, val loss: 2.0677945613861084
Epoch 2950, training loss: 621.134765625 = 0.012438395991921425 + 100.0 * 6.211223125457764
Epoch 2950, val loss: 2.0698113441467285
Epoch 2960, training loss: 620.4358520507812 = 0.012302874587476254 + 100.0 * 6.204235076904297
Epoch 2960, val loss: 2.0734612941741943
Epoch 2970, training loss: 620.3234252929688 = 0.012171318754553795 + 100.0 * 6.203112602233887
Epoch 2970, val loss: 2.076022148132324
Epoch 2980, training loss: 620.320068359375 = 0.012044146656990051 + 100.0 * 6.203080177307129
Epoch 2980, val loss: 2.079061269760132
Epoch 2990, training loss: 620.801025390625 = 0.011929820291697979 + 100.0 * 6.20789098739624
Epoch 2990, val loss: 2.0821990966796875
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.662962962962963
0.8017923036373221
=== training gcn model ===
Epoch 0, training loss: 861.6416015625 = 1.9582244157791138 + 100.0 * 8.596833229064941
Epoch 0, val loss: 1.9521312713623047
Epoch 10, training loss: 861.5546875 = 1.9496163129806519 + 100.0 * 8.596050262451172
Epoch 10, val loss: 1.944198489189148
Epoch 20, training loss: 861.036376953125 = 1.9387866258621216 + 100.0 * 8.590975761413574
Epoch 20, val loss: 1.9339373111724854
Epoch 30, training loss: 857.709228515625 = 1.9244483709335327 + 100.0 * 8.55784797668457
Epoch 30, val loss: 1.9198893308639526
Epoch 40, training loss: 837.622802734375 = 1.9066606760025024 + 100.0 * 8.357161521911621
Epoch 40, val loss: 1.9023460149765015
Epoch 50, training loss: 770.5611572265625 = 1.8854135274887085 + 100.0 * 7.6867570877075195
Epoch 50, val loss: 1.8810064792633057
Epoch 60, training loss: 727.0184936523438 = 1.8699250221252441 + 100.0 * 7.251485824584961
Epoch 60, val loss: 1.8661863803863525
Epoch 70, training loss: 699.48779296875 = 1.8584905862808228 + 100.0 * 6.976292610168457
Epoch 70, val loss: 1.8539930582046509
Epoch 80, training loss: 689.953369140625 = 1.847023844718933 + 100.0 * 6.881063461303711
Epoch 80, val loss: 1.8420891761779785
Epoch 90, training loss: 682.966064453125 = 1.835627555847168 + 100.0 * 6.811304569244385
Epoch 90, val loss: 1.830653429031372
Epoch 100, training loss: 677.1025390625 = 1.8253123760223389 + 100.0 * 6.752772331237793
Epoch 100, val loss: 1.8211357593536377
Epoch 110, training loss: 672.0384521484375 = 1.816702961921692 + 100.0 * 6.7022175788879395
Epoch 110, val loss: 1.8134671449661255
Epoch 120, training loss: 667.7608032226562 = 1.8092014789581299 + 100.0 * 6.659515857696533
Epoch 120, val loss: 1.8065370321273804
Epoch 130, training loss: 663.9036865234375 = 1.8018227815628052 + 100.0 * 6.621018409729004
Epoch 130, val loss: 1.7996230125427246
Epoch 140, training loss: 660.5455322265625 = 1.7944228649139404 + 100.0 * 6.58751106262207
Epoch 140, val loss: 1.792726755142212
Epoch 150, training loss: 657.7908935546875 = 1.7868988513946533 + 100.0 * 6.560039520263672
Epoch 150, val loss: 1.785700798034668
Epoch 160, training loss: 655.234619140625 = 1.7790822982788086 + 100.0 * 6.534555912017822
Epoch 160, val loss: 1.7784934043884277
Epoch 170, training loss: 652.973876953125 = 1.770959734916687 + 100.0 * 6.512029647827148
Epoch 170, val loss: 1.7710514068603516
Epoch 180, training loss: 651.0790405273438 = 1.7624585628509521 + 100.0 * 6.493165969848633
Epoch 180, val loss: 1.7633181810379028
Epoch 190, training loss: 649.2955322265625 = 1.7534250020980835 + 100.0 * 6.47542142868042
Epoch 190, val loss: 1.7550894021987915
Epoch 200, training loss: 648.085205078125 = 1.7438040971755981 + 100.0 * 6.463414192199707
Epoch 200, val loss: 1.746397852897644
Epoch 210, training loss: 646.5946044921875 = 1.7334383726119995 + 100.0 * 6.448611259460449
Epoch 210, val loss: 1.737097978591919
Epoch 220, training loss: 645.4689331054688 = 1.7223163843154907 + 100.0 * 6.437466621398926
Epoch 220, val loss: 1.7271766662597656
Epoch 230, training loss: 644.7052001953125 = 1.7103909254074097 + 100.0 * 6.429948329925537
Epoch 230, val loss: 1.7165666818618774
Epoch 240, training loss: 643.513916015625 = 1.697425127029419 + 100.0 * 6.4181647300720215
Epoch 240, val loss: 1.7051819562911987
Epoch 250, training loss: 642.6654663085938 = 1.6835548877716064 + 100.0 * 6.40981912612915
Epoch 250, val loss: 1.6930567026138306
Epoch 260, training loss: 642.0743408203125 = 1.6686781644821167 + 100.0 * 6.404056549072266
Epoch 260, val loss: 1.6801092624664307
Epoch 270, training loss: 641.5317993164062 = 1.6527315378189087 + 100.0 * 6.3987908363342285
Epoch 270, val loss: 1.6662468910217285
Epoch 280, training loss: 640.6236572265625 = 1.63571298122406 + 100.0 * 6.38987922668457
Epoch 280, val loss: 1.6514990329742432
Epoch 290, training loss: 639.9736938476562 = 1.6175652742385864 + 100.0 * 6.383561134338379
Epoch 290, val loss: 1.6359845399856567
Epoch 300, training loss: 639.5013427734375 = 1.598347783088684 + 100.0 * 6.379030227661133
Epoch 300, val loss: 1.6196568012237549
Epoch 310, training loss: 639.0960083007812 = 1.5781030654907227 + 100.0 * 6.375178813934326
Epoch 310, val loss: 1.602557897567749
Epoch 320, training loss: 638.3553466796875 = 1.5566883087158203 + 100.0 * 6.367986679077148
Epoch 320, val loss: 1.5846967697143555
Epoch 330, training loss: 637.9011840820312 = 1.5343663692474365 + 100.0 * 6.363668441772461
Epoch 330, val loss: 1.5661793947219849
Epoch 340, training loss: 638.3831787109375 = 1.5112954378128052 + 100.0 * 6.368719100952148
Epoch 340, val loss: 1.5472253561019897
Epoch 350, training loss: 637.3065795898438 = 1.4871364831924438 + 100.0 * 6.358194828033447
Epoch 350, val loss: 1.5276764631271362
Epoch 360, training loss: 636.576904296875 = 1.4623979330062866 + 100.0 * 6.351144790649414
Epoch 360, val loss: 1.5079405307769775
Epoch 370, training loss: 636.20654296875 = 1.437169075012207 + 100.0 * 6.34769344329834
Epoch 370, val loss: 1.488037109375
Epoch 380, training loss: 636.9002685546875 = 1.4116724729537964 + 100.0 * 6.354886054992676
Epoch 380, val loss: 1.4680207967758179
Epoch 390, training loss: 635.7406616210938 = 1.3853169679641724 + 100.0 * 6.34355354309082
Epoch 390, val loss: 1.44789719581604
Epoch 400, training loss: 635.1480712890625 = 1.3588236570358276 + 100.0 * 6.337892532348633
Epoch 400, val loss: 1.4280457496643066
Epoch 410, training loss: 634.7581787109375 = 1.3323334455490112 + 100.0 * 6.334258556365967
Epoch 410, val loss: 1.408597469329834
Epoch 420, training loss: 635.3273315429688 = 1.3058969974517822 + 100.0 * 6.340214252471924
Epoch 420, val loss: 1.3895277976989746
Epoch 430, training loss: 634.40625 = 1.2791553735733032 + 100.0 * 6.331270694732666
Epoch 430, val loss: 1.3706458806991577
Epoch 440, training loss: 633.9088134765625 = 1.2527551651000977 + 100.0 * 6.3265604972839355
Epoch 440, val loss: 1.352426528930664
Epoch 450, training loss: 633.5372924804688 = 1.2266708612442017 + 100.0 * 6.323106288909912
Epoch 450, val loss: 1.3349369764328003
Epoch 460, training loss: 633.3143310546875 = 1.200919270515442 + 100.0 * 6.321134090423584
Epoch 460, val loss: 1.3180664777755737
Epoch 470, training loss: 633.0721435546875 = 1.175473928451538 + 100.0 * 6.318966388702393
Epoch 470, val loss: 1.301910161972046
Epoch 480, training loss: 632.7698364257812 = 1.1502422094345093 + 100.0 * 6.316195487976074
Epoch 480, val loss: 1.286281943321228
Epoch 490, training loss: 632.4969482421875 = 1.1255898475646973 + 100.0 * 6.313713073730469
Epoch 490, val loss: 1.271528720855713
Epoch 500, training loss: 632.2571411132812 = 1.1013896465301514 + 100.0 * 6.311557769775391
Epoch 500, val loss: 1.2576637268066406
Epoch 510, training loss: 632.6278076171875 = 1.0777604579925537 + 100.0 * 6.315500259399414
Epoch 510, val loss: 1.244423270225525
Epoch 520, training loss: 631.7823486328125 = 1.0544978380203247 + 100.0 * 6.307278156280518
Epoch 520, val loss: 1.2318930625915527
Epoch 530, training loss: 631.603759765625 = 1.031829833984375 + 100.0 * 6.305719375610352
Epoch 530, val loss: 1.220226764678955
Epoch 540, training loss: 631.397216796875 = 1.0098563432693481 + 100.0 * 6.3038740158081055
Epoch 540, val loss: 1.2092880010604858
Epoch 550, training loss: 631.1702880859375 = 0.9881787896156311 + 100.0 * 6.301821231842041
Epoch 550, val loss: 1.1985803842544556
Epoch 560, training loss: 631.0028076171875 = 0.9671248197555542 + 100.0 * 6.300356864929199
Epoch 560, val loss: 1.1889545917510986
Epoch 570, training loss: 630.7883911132812 = 0.9466745853424072 + 100.0 * 6.298417091369629
Epoch 570, val loss: 1.1799843311309814
Epoch 580, training loss: 630.8035278320312 = 0.9269117116928101 + 100.0 * 6.298766136169434
Epoch 580, val loss: 1.1716909408569336
Epoch 590, training loss: 630.5797729492188 = 0.907427191734314 + 100.0 * 6.296723365783691
Epoch 590, val loss: 1.1638885736465454
Epoch 600, training loss: 630.3556518554688 = 0.8885965347290039 + 100.0 * 6.294670581817627
Epoch 600, val loss: 1.1567473411560059
Epoch 610, training loss: 630.0726318359375 = 0.8701793551445007 + 100.0 * 6.292024612426758
Epoch 610, val loss: 1.1500338315963745
Epoch 620, training loss: 629.95556640625 = 0.8523066639900208 + 100.0 * 6.291032791137695
Epoch 620, val loss: 1.143997073173523
Epoch 630, training loss: 629.8793334960938 = 0.8349162936210632 + 100.0 * 6.290444374084473
Epoch 630, val loss: 1.1384861469268799
Epoch 640, training loss: 629.5418701171875 = 0.8178757429122925 + 100.0 * 6.287240028381348
Epoch 640, val loss: 1.1333467960357666
Epoch 650, training loss: 629.618408203125 = 0.8013386726379395 + 100.0 * 6.28817081451416
Epoch 650, val loss: 1.1287195682525635
Epoch 660, training loss: 629.3582763671875 = 0.7850132584571838 + 100.0 * 6.285732269287109
Epoch 660, val loss: 1.124245047569275
Epoch 670, training loss: 629.1653442382812 = 0.7691737413406372 + 100.0 * 6.283961772918701
Epoch 670, val loss: 1.120406985282898
Epoch 680, training loss: 628.9869995117188 = 0.7536104321479797 + 100.0 * 6.282333850860596
Epoch 680, val loss: 1.1167784929275513
Epoch 690, training loss: 629.1476440429688 = 0.7385019063949585 + 100.0 * 6.284091472625732
Epoch 690, val loss: 1.1136451959609985
Epoch 700, training loss: 629.15673828125 = 0.7235292196273804 + 100.0 * 6.284332275390625
Epoch 700, val loss: 1.1105895042419434
Epoch 710, training loss: 628.5502319335938 = 0.7087536454200745 + 100.0 * 6.278415203094482
Epoch 710, val loss: 1.1077101230621338
Epoch 720, training loss: 628.4073486328125 = 0.694433331489563 + 100.0 * 6.27712869644165
Epoch 720, val loss: 1.1052709817886353
Epoch 730, training loss: 628.280029296875 = 0.6804316639900208 + 100.0 * 6.275996208190918
Epoch 730, val loss: 1.1031506061553955
Epoch 740, training loss: 628.4866943359375 = 0.6666358709335327 + 100.0 * 6.278200626373291
Epoch 740, val loss: 1.1011461019515991
Epoch 750, training loss: 628.3090209960938 = 0.6530351042747498 + 100.0 * 6.276560306549072
Epoch 750, val loss: 1.099564552307129
Epoch 760, training loss: 627.8896484375 = 0.6396805644035339 + 100.0 * 6.2724995613098145
Epoch 760, val loss: 1.0980736017227173
Epoch 770, training loss: 627.72412109375 = 0.626591145992279 + 100.0 * 6.270975112915039
Epoch 770, val loss: 1.096907138824463
Epoch 780, training loss: 627.9857177734375 = 0.6137075424194336 + 100.0 * 6.2737202644348145
Epoch 780, val loss: 1.0957510471343994
Epoch 790, training loss: 627.9290161132812 = 0.6010242104530334 + 100.0 * 6.273280143737793
Epoch 790, val loss: 1.095017671585083
Epoch 800, training loss: 627.5486450195312 = 0.5884429216384888 + 100.0 * 6.269601821899414
Epoch 800, val loss: 1.0942862033843994
Epoch 810, training loss: 627.3425903320312 = 0.5761977434158325 + 100.0 * 6.267664432525635
Epoch 810, val loss: 1.0939116477966309
Epoch 820, training loss: 627.48876953125 = 0.5641306042671204 + 100.0 * 6.2692461013793945
Epoch 820, val loss: 1.093633770942688
Epoch 830, training loss: 627.094970703125 = 0.552238941192627 + 100.0 * 6.265427112579346
Epoch 830, val loss: 1.093727469444275
Epoch 840, training loss: 626.9991455078125 = 0.5406299829483032 + 100.0 * 6.264585018157959
Epoch 840, val loss: 1.094058632850647
Epoch 850, training loss: 627.5772094726562 = 0.5291629433631897 + 100.0 * 6.270480155944824
Epoch 850, val loss: 1.0944355726242065
Epoch 860, training loss: 626.7703247070312 = 0.5177625417709351 + 100.0 * 6.26252555847168
Epoch 860, val loss: 1.0948173999786377
Epoch 870, training loss: 626.7584838867188 = 0.5066752433776855 + 100.0 * 6.262518405914307
Epoch 870, val loss: 1.0955913066864014
Epoch 880, training loss: 626.9639282226562 = 0.49581587314605713 + 100.0 * 6.264681339263916
Epoch 880, val loss: 1.0965627431869507
Epoch 890, training loss: 626.6026000976562 = 0.4851903021335602 + 100.0 * 6.261174201965332
Epoch 890, val loss: 1.0979894399642944
Epoch 900, training loss: 626.373291015625 = 0.47476550936698914 + 100.0 * 6.2589850425720215
Epoch 900, val loss: 1.0994644165039062
Epoch 910, training loss: 626.3712768554688 = 0.4645960032939911 + 100.0 * 6.259066581726074
Epoch 910, val loss: 1.1011817455291748
Epoch 920, training loss: 626.4879150390625 = 0.45452627539634705 + 100.0 * 6.260334014892578
Epoch 920, val loss: 1.1030662059783936
Epoch 930, training loss: 626.4549560546875 = 0.4446459710597992 + 100.0 * 6.260103225708008
Epoch 930, val loss: 1.1049764156341553
Epoch 940, training loss: 626.010986328125 = 0.4350997507572174 + 100.0 * 6.255758762359619
Epoch 940, val loss: 1.1074074506759644
Epoch 950, training loss: 625.9404907226562 = 0.42583730816841125 + 100.0 * 6.255146503448486
Epoch 950, val loss: 1.1102509498596191
Epoch 960, training loss: 626.1341552734375 = 0.41678351163864136 + 100.0 * 6.257173538208008
Epoch 960, val loss: 1.113156795501709
Epoch 970, training loss: 626.419677734375 = 0.40780192613601685 + 100.0 * 6.2601189613342285
Epoch 970, val loss: 1.115962266921997
Epoch 980, training loss: 625.9520263671875 = 0.3990899324417114 + 100.0 * 6.255528926849365
Epoch 980, val loss: 1.1193453073501587
Epoch 990, training loss: 625.6404418945312 = 0.3906024396419525 + 100.0 * 6.252498149871826
Epoch 990, val loss: 1.122767448425293
Epoch 1000, training loss: 625.5438842773438 = 0.382417231798172 + 100.0 * 6.251614093780518
Epoch 1000, val loss: 1.1265321969985962
Epoch 1010, training loss: 625.58837890625 = 0.37446293234825134 + 100.0 * 6.252139568328857
Epoch 1010, val loss: 1.130564570426941
Epoch 1020, training loss: 625.390380859375 = 0.36657610535621643 + 100.0 * 6.250237941741943
Epoch 1020, val loss: 1.1344447135925293
Epoch 1030, training loss: 625.462890625 = 0.3588777184486389 + 100.0 * 6.251039981842041
Epoch 1030, val loss: 1.1387720108032227
Epoch 1040, training loss: 625.2877807617188 = 0.3513961136341095 + 100.0 * 6.249363899230957
Epoch 1040, val loss: 1.1430221796035767
Epoch 1050, training loss: 625.848388671875 = 0.34414201974868774 + 100.0 * 6.255043029785156
Epoch 1050, val loss: 1.1476078033447266
Epoch 1060, training loss: 625.3394775390625 = 0.3370523154735565 + 100.0 * 6.250024318695068
Epoch 1060, val loss: 1.152652621269226
Epoch 1070, training loss: 625.2605590820312 = 0.33005523681640625 + 100.0 * 6.24930477142334
Epoch 1070, val loss: 1.1574770212173462
Epoch 1080, training loss: 625.1370239257812 = 0.3233054578304291 + 100.0 * 6.2481369972229
Epoch 1080, val loss: 1.1627963781356812
Epoch 1090, training loss: 624.9539184570312 = 0.3166888952255249 + 100.0 * 6.246372222900391
Epoch 1090, val loss: 1.1681132316589355
Epoch 1100, training loss: 624.998046875 = 0.31025877594947815 + 100.0 * 6.246878147125244
Epoch 1100, val loss: 1.1736140251159668
Epoch 1110, training loss: 625.115966796875 = 0.303952693939209 + 100.0 * 6.248120307922363
Epoch 1110, val loss: 1.1791760921478271
Epoch 1120, training loss: 624.8495483398438 = 0.29773110151290894 + 100.0 * 6.245518207550049
Epoch 1120, val loss: 1.1849695444107056
Epoch 1130, training loss: 624.7428588867188 = 0.2917200028896332 + 100.0 * 6.244511127471924
Epoch 1130, val loss: 1.1909053325653076
Epoch 1140, training loss: 624.6905517578125 = 0.2858582139015198 + 100.0 * 6.244046688079834
Epoch 1140, val loss: 1.1971348524093628
Epoch 1150, training loss: 624.8949584960938 = 0.28011780977249146 + 100.0 * 6.246148586273193
Epoch 1150, val loss: 1.2030503749847412
Epoch 1160, training loss: 625.371826171875 = 0.2744345963001251 + 100.0 * 6.250974178314209
Epoch 1160, val loss: 1.2091716527938843
Epoch 1170, training loss: 624.6179809570312 = 0.2688836455345154 + 100.0 * 6.243491172790527
Epoch 1170, val loss: 1.2157031297683716
Epoch 1180, training loss: 624.3787841796875 = 0.26346439123153687 + 100.0 * 6.241153240203857
Epoch 1180, val loss: 1.22195303440094
Epoch 1190, training loss: 624.325927734375 = 0.25822654366493225 + 100.0 * 6.2406768798828125
Epoch 1190, val loss: 1.2287191152572632
Epoch 1200, training loss: 624.950927734375 = 0.25316599011421204 + 100.0 * 6.246977806091309
Epoch 1200, val loss: 1.2355214357376099
Epoch 1210, training loss: 624.562255859375 = 0.24794703722000122 + 100.0 * 6.243143558502197
Epoch 1210, val loss: 1.2415034770965576
Epoch 1220, training loss: 624.2765502929688 = 0.2429961860179901 + 100.0 * 6.240335941314697
Epoch 1220, val loss: 1.2488197088241577
Epoch 1230, training loss: 624.1131591796875 = 0.23813658952713013 + 100.0 * 6.238749980926514
Epoch 1230, val loss: 1.2554775476455688
Epoch 1240, training loss: 624.3399658203125 = 0.23344391584396362 + 100.0 * 6.24106502532959
Epoch 1240, val loss: 1.2628055810928345
Epoch 1250, training loss: 624.0638427734375 = 0.22872959077358246 + 100.0 * 6.238350868225098
Epoch 1250, val loss: 1.2693116664886475
Epoch 1260, training loss: 624.22314453125 = 0.22414708137512207 + 100.0 * 6.239990234375
Epoch 1260, val loss: 1.2768787145614624
Epoch 1270, training loss: 624.1849365234375 = 0.21963217854499817 + 100.0 * 6.239653587341309
Epoch 1270, val loss: 1.283789038658142
Epoch 1280, training loss: 624.0049438476562 = 0.21522387862205505 + 100.0 * 6.2378973960876465
Epoch 1280, val loss: 1.2910022735595703
Epoch 1290, training loss: 623.8775024414062 = 0.21094128489494324 + 100.0 * 6.236665725708008
Epoch 1290, val loss: 1.2986512184143066
Epoch 1300, training loss: 623.9673461914062 = 0.2067311853170395 + 100.0 * 6.237606048583984
Epoch 1300, val loss: 1.3059581518173218
Epoch 1310, training loss: 623.9944458007812 = 0.20257872343063354 + 100.0 * 6.237918853759766
Epoch 1310, val loss: 1.3134492635726929
Epoch 1320, training loss: 623.9589233398438 = 0.19852636754512787 + 100.0 * 6.237603664398193
Epoch 1320, val loss: 1.321008324623108
Epoch 1330, training loss: 623.7592163085938 = 0.1945105940103531 + 100.0 * 6.235647201538086
Epoch 1330, val loss: 1.3285530805587769
Epoch 1340, training loss: 624.2137451171875 = 0.19062383472919464 + 100.0 * 6.240231513977051
Epoch 1340, val loss: 1.336182713508606
Epoch 1350, training loss: 623.6522827148438 = 0.18677663803100586 + 100.0 * 6.234654903411865
Epoch 1350, val loss: 1.3439406156539917
Epoch 1360, training loss: 623.5199584960938 = 0.18303236365318298 + 100.0 * 6.233368873596191
Epoch 1360, val loss: 1.3517793416976929
Epoch 1370, training loss: 623.5924682617188 = 0.17938484251499176 + 100.0 * 6.234130859375
Epoch 1370, val loss: 1.3595484495162964
Epoch 1380, training loss: 624.05419921875 = 0.17582738399505615 + 100.0 * 6.238783836364746
Epoch 1380, val loss: 1.367361068725586
Epoch 1390, training loss: 623.5552978515625 = 0.1721912920475006 + 100.0 * 6.23383092880249
Epoch 1390, val loss: 1.3747925758361816
Epoch 1400, training loss: 623.5155639648438 = 0.16874706745147705 + 100.0 * 6.233468055725098
Epoch 1400, val loss: 1.3827961683273315
Epoch 1410, training loss: 623.3877563476562 = 0.16534094512462616 + 100.0 * 6.232223987579346
Epoch 1410, val loss: 1.390695333480835
Epoch 1420, training loss: 623.523681640625 = 0.1620231717824936 + 100.0 * 6.233616352081299
Epoch 1420, val loss: 1.3987983465194702
Epoch 1430, training loss: 623.2546997070312 = 0.15872079133987427 + 100.0 * 6.230960369110107
Epoch 1430, val loss: 1.4065583944320679
Epoch 1440, training loss: 623.1815795898438 = 0.15549245476722717 + 100.0 * 6.230260848999023
Epoch 1440, val loss: 1.4145668745040894
Epoch 1450, training loss: 623.1886596679688 = 0.15236768126487732 + 100.0 * 6.230363368988037
Epoch 1450, val loss: 1.4226518869400024
Epoch 1460, training loss: 623.4661865234375 = 0.1493120789527893 + 100.0 * 6.233169078826904
Epoch 1460, val loss: 1.430783748626709
Epoch 1470, training loss: 623.4535522460938 = 0.14631451666355133 + 100.0 * 6.233072280883789
Epoch 1470, val loss: 1.4389030933380127
Epoch 1480, training loss: 623.14013671875 = 0.14330604672431946 + 100.0 * 6.229968070983887
Epoch 1480, val loss: 1.4468328952789307
Epoch 1490, training loss: 623.0579833984375 = 0.14039243757724762 + 100.0 * 6.229176044464111
Epoch 1490, val loss: 1.4546904563903809
Epoch 1500, training loss: 622.9391479492188 = 0.13757622241973877 + 100.0 * 6.228015899658203
Epoch 1500, val loss: 1.4631378650665283
Epoch 1510, training loss: 623.1505126953125 = 0.13482943177223206 + 100.0 * 6.230156898498535
Epoch 1510, val loss: 1.4709734916687012
Epoch 1520, training loss: 623.091552734375 = 0.13208776712417603 + 100.0 * 6.229594707489014
Epoch 1520, val loss: 1.4793211221694946
Epoch 1530, training loss: 623.1168823242188 = 0.12935848534107208 + 100.0 * 6.229875564575195
Epoch 1530, val loss: 1.4869983196258545
Epoch 1540, training loss: 622.7946166992188 = 0.126740500330925 + 100.0 * 6.226678371429443
Epoch 1540, val loss: 1.4951380491256714
Epoch 1550, training loss: 622.7832641601562 = 0.12422075867652893 + 100.0 * 6.226590633392334
Epoch 1550, val loss: 1.503717064857483
Epoch 1560, training loss: 623.0338134765625 = 0.12174692749977112 + 100.0 * 6.22912073135376
Epoch 1560, val loss: 1.5118805170059204
Epoch 1570, training loss: 622.7755126953125 = 0.11928104609251022 + 100.0 * 6.2265625
Epoch 1570, val loss: 1.5198317766189575
Epoch 1580, training loss: 622.8983764648438 = 0.11687952280044556 + 100.0 * 6.227814674377441
Epoch 1580, val loss: 1.5280539989471436
Epoch 1590, training loss: 623.0581665039062 = 0.11453722417354584 + 100.0 * 6.229435920715332
Epoch 1590, val loss: 1.5362093448638916
Epoch 1600, training loss: 622.70751953125 = 0.11219074577093124 + 100.0 * 6.225953578948975
Epoch 1600, val loss: 1.544014573097229
Epoch 1610, training loss: 622.5938720703125 = 0.1099686399102211 + 100.0 * 6.224838733673096
Epoch 1610, val loss: 1.5524041652679443
Epoch 1620, training loss: 622.509521484375 = 0.10777905583381653 + 100.0 * 6.22401762008667
Epoch 1620, val loss: 1.5607069730758667
Epoch 1630, training loss: 622.59326171875 = 0.10565142333507538 + 100.0 * 6.2248759269714355
Epoch 1630, val loss: 1.5687975883483887
Epoch 1640, training loss: 622.9844360351562 = 0.10354860872030258 + 100.0 * 6.228808403015137
Epoch 1640, val loss: 1.5767710208892822
Epoch 1650, training loss: 622.5374145507812 = 0.1014595627784729 + 100.0 * 6.224359512329102
Epoch 1650, val loss: 1.5850011110305786
Epoch 1660, training loss: 622.4107055664062 = 0.09944534301757812 + 100.0 * 6.2231125831604
Epoch 1660, val loss: 1.5934568643569946
Epoch 1670, training loss: 622.4381713867188 = 0.09750035405158997 + 100.0 * 6.223406791687012
Epoch 1670, val loss: 1.6016918420791626
Epoch 1680, training loss: 622.8078002929688 = 0.09562715142965317 + 100.0 * 6.227121829986572
Epoch 1680, val loss: 1.6101280450820923
Epoch 1690, training loss: 622.5602416992188 = 0.09369374066591263 + 100.0 * 6.224665641784668
Epoch 1690, val loss: 1.6178040504455566
Epoch 1700, training loss: 622.4640502929688 = 0.09183594584465027 + 100.0 * 6.223721981048584
Epoch 1700, val loss: 1.6258645057678223
Epoch 1710, training loss: 622.5122680664062 = 0.09001245349645615 + 100.0 * 6.224222660064697
Epoch 1710, val loss: 1.6339298486709595
Epoch 1720, training loss: 622.34375 = 0.08825880289077759 + 100.0 * 6.222555160522461
Epoch 1720, val loss: 1.6426129341125488
Epoch 1730, training loss: 622.2432861328125 = 0.08654138445854187 + 100.0 * 6.221567630767822
Epoch 1730, val loss: 1.650538444519043
Epoch 1740, training loss: 622.7867431640625 = 0.0849020853638649 + 100.0 * 6.227018356323242
Epoch 1740, val loss: 1.6586549282073975
Epoch 1750, training loss: 622.2916870117188 = 0.08320669084787369 + 100.0 * 6.222084999084473
Epoch 1750, val loss: 1.6667848825454712
Epoch 1760, training loss: 622.1442260742188 = 0.08157911151647568 + 100.0 * 6.220626354217529
Epoch 1760, val loss: 1.6745827198028564
Epoch 1770, training loss: 622.1022338867188 = 0.0800149142742157 + 100.0 * 6.220222473144531
Epoch 1770, val loss: 1.6829257011413574
Epoch 1780, training loss: 622.4224243164062 = 0.07848647981882095 + 100.0 * 6.2234392166137695
Epoch 1780, val loss: 1.690638542175293
Epoch 1790, training loss: 622.2210693359375 = 0.0769554004073143 + 100.0 * 6.22144079208374
Epoch 1790, val loss: 1.6989784240722656
Epoch 1800, training loss: 622.0654296875 = 0.07543598115444183 + 100.0 * 6.219899654388428
Epoch 1800, val loss: 1.706390380859375
Epoch 1810, training loss: 622.0562744140625 = 0.0739881694316864 + 100.0 * 6.219822883605957
Epoch 1810, val loss: 1.7148159742355347
Epoch 1820, training loss: 622.1559448242188 = 0.07258561253547668 + 100.0 * 6.220833778381348
Epoch 1820, val loss: 1.7224082946777344
Epoch 1830, training loss: 622.1138916015625 = 0.07121415436267853 + 100.0 * 6.220426559448242
Epoch 1830, val loss: 1.7306058406829834
Epoch 1840, training loss: 622.0260620117188 = 0.06983379274606705 + 100.0 * 6.219562530517578
Epoch 1840, val loss: 1.7386150360107422
Epoch 1850, training loss: 622.105224609375 = 0.06851546466350555 + 100.0 * 6.220366954803467
Epoch 1850, val loss: 1.7462373971939087
Epoch 1860, training loss: 622.0820922851562 = 0.06721127033233643 + 100.0 * 6.220149040222168
Epoch 1860, val loss: 1.7540456056594849
Epoch 1870, training loss: 622.0509643554688 = 0.06591679900884628 + 100.0 * 6.219850540161133
Epoch 1870, val loss: 1.7614974975585938
Epoch 1880, training loss: 621.8448486328125 = 0.06466864049434662 + 100.0 * 6.217801570892334
Epoch 1880, val loss: 1.7698849439620972
Epoch 1890, training loss: 621.8579711914062 = 0.06346755474805832 + 100.0 * 6.217945098876953
Epoch 1890, val loss: 1.777418851852417
Epoch 1900, training loss: 622.25537109375 = 0.062321774661540985 + 100.0 * 6.221930503845215
Epoch 1900, val loss: 1.7857245206832886
Epoch 1910, training loss: 621.9396362304688 = 0.06109779328107834 + 100.0 * 6.218785285949707
Epoch 1910, val loss: 1.7925063371658325
Epoch 1920, training loss: 621.7642822265625 = 0.059956859797239304 + 100.0 * 6.217043399810791
Epoch 1920, val loss: 1.800693154335022
Epoch 1930, training loss: 621.8016967773438 = 0.058843400329351425 + 100.0 * 6.217428684234619
Epoch 1930, val loss: 1.808121919631958
Epoch 1940, training loss: 621.855712890625 = 0.057754676789045334 + 100.0 * 6.217979431152344
Epoch 1940, val loss: 1.8159089088439941
Epoch 1950, training loss: 622.0599365234375 = 0.05668390914797783 + 100.0 * 6.220032215118408
Epoch 1950, val loss: 1.823366403579712
Epoch 1960, training loss: 621.7572021484375 = 0.05562556907534599 + 100.0 * 6.217016220092773
Epoch 1960, val loss: 1.831368327140808
Epoch 1970, training loss: 621.6201171875 = 0.05459126830101013 + 100.0 * 6.21565580368042
Epoch 1970, val loss: 1.8385331630706787
Epoch 1980, training loss: 621.75146484375 = 0.05360156297683716 + 100.0 * 6.216979026794434
Epoch 1980, val loss: 1.8464076519012451
Epoch 1990, training loss: 621.8475341796875 = 0.052632059901952744 + 100.0 * 6.217949390411377
Epoch 1990, val loss: 1.853760838508606
Epoch 2000, training loss: 621.6354370117188 = 0.05167436599731445 + 100.0 * 6.215837478637695
Epoch 2000, val loss: 1.8615801334381104
Epoch 2010, training loss: 621.6004028320312 = 0.05074424669146538 + 100.0 * 6.21549654006958
Epoch 2010, val loss: 1.8688766956329346
Epoch 2020, training loss: 622.056396484375 = 0.04984825477004051 + 100.0 * 6.220065593719482
Epoch 2020, val loss: 1.8765127658843994
Epoch 2030, training loss: 621.710205078125 = 0.04892253875732422 + 100.0 * 6.216612339019775
Epoch 2030, val loss: 1.8833227157592773
Epoch 2040, training loss: 621.4602661132812 = 0.04803118109703064 + 100.0 * 6.214122295379639
Epoch 2040, val loss: 1.8909908533096313
Epoch 2050, training loss: 621.3934936523438 = 0.04717973992228508 + 100.0 * 6.213463306427002
Epoch 2050, val loss: 1.898271083831787
Epoch 2060, training loss: 621.4343872070312 = 0.04635293781757355 + 100.0 * 6.21388053894043
Epoch 2060, val loss: 1.9057185649871826
Epoch 2070, training loss: 621.9886474609375 = 0.04556011036038399 + 100.0 * 6.219430923461914
Epoch 2070, val loss: 1.9128684997558594
Epoch 2080, training loss: 621.7897338867188 = 0.04473253712058067 + 100.0 * 6.21744966506958
Epoch 2080, val loss: 1.920857310295105
Epoch 2090, training loss: 621.70947265625 = 0.04392687603831291 + 100.0 * 6.216655254364014
Epoch 2090, val loss: 1.9274872541427612
Epoch 2100, training loss: 621.3603515625 = 0.043143048882484436 + 100.0 * 6.21317195892334
Epoch 2100, val loss: 1.9348646402359009
Epoch 2110, training loss: 621.2637939453125 = 0.042391907423734665 + 100.0 * 6.21221399307251
Epoch 2110, val loss: 1.9422591924667358
Epoch 2120, training loss: 621.3137817382812 = 0.04166823998093605 + 100.0 * 6.21272087097168
Epoch 2120, val loss: 1.949589490890503
Epoch 2130, training loss: 621.917236328125 = 0.040959421545267105 + 100.0 * 6.2187628746032715
Epoch 2130, val loss: 1.9564592838287354
Epoch 2140, training loss: 621.521240234375 = 0.04023893550038338 + 100.0 * 6.214810371398926
Epoch 2140, val loss: 1.9634301662445068
Epoch 2150, training loss: 621.59765625 = 0.03954451531171799 + 100.0 * 6.215580940246582
Epoch 2150, val loss: 1.9707499742507935
Epoch 2160, training loss: 621.2728271484375 = 0.03885652869939804 + 100.0 * 6.212339401245117
Epoch 2160, val loss: 1.9777839183807373
Epoch 2170, training loss: 621.2210693359375 = 0.038200393319129944 + 100.0 * 6.211828708648682
Epoch 2170, val loss: 1.9852509498596191
Epoch 2180, training loss: 621.1210327148438 = 0.0375606045126915 + 100.0 * 6.210834980010986
Epoch 2180, val loss: 1.9924296140670776
Epoch 2190, training loss: 621.264404296875 = 0.036949943751096725 + 100.0 * 6.212274551391602
Epoch 2190, val loss: 1.9996559619903564
Epoch 2200, training loss: 621.6189575195312 = 0.036350078880786896 + 100.0 * 6.215826034545898
Epoch 2200, val loss: 2.0064868927001953
Epoch 2210, training loss: 621.3853149414062 = 0.03569989651441574 + 100.0 * 6.213496208190918
Epoch 2210, val loss: 2.012716293334961
Epoch 2220, training loss: 621.0932006835938 = 0.0350983552634716 + 100.0 * 6.210581302642822
Epoch 2220, val loss: 2.020185708999634
Epoch 2230, training loss: 621.0613403320312 = 0.03452679142355919 + 100.0 * 6.210268020629883
Epoch 2230, val loss: 2.027186870574951
Epoch 2240, training loss: 621.062255859375 = 0.033968836069107056 + 100.0 * 6.210282325744629
Epoch 2240, val loss: 2.0340936183929443
Epoch 2250, training loss: 621.5856323242188 = 0.03343427553772926 + 100.0 * 6.215521812438965
Epoch 2250, val loss: 2.0411224365234375
Epoch 2260, training loss: 621.2235717773438 = 0.03287119045853615 + 100.0 * 6.211906909942627
Epoch 2260, val loss: 2.047008752822876
Epoch 2270, training loss: 620.993408203125 = 0.032330241054296494 + 100.0 * 6.209610462188721
Epoch 2270, val loss: 2.0543572902679443
Epoch 2280, training loss: 620.951904296875 = 0.03181355819106102 + 100.0 * 6.209201335906982
Epoch 2280, val loss: 2.0609047412872314
Epoch 2290, training loss: 620.964111328125 = 0.0313112735748291 + 100.0 * 6.2093281745910645
Epoch 2290, val loss: 2.067595958709717
Epoch 2300, training loss: 621.5606689453125 = 0.03083750791847706 + 100.0 * 6.215298175811768
Epoch 2300, val loss: 2.0743696689605713
Epoch 2310, training loss: 620.9877319335938 = 0.030332311987876892 + 100.0 * 6.209574222564697
Epoch 2310, val loss: 2.0808193683624268
Epoch 2320, training loss: 620.927001953125 = 0.029850732535123825 + 100.0 * 6.2089715003967285
Epoch 2320, val loss: 2.087240695953369
Epoch 2330, training loss: 620.92236328125 = 0.029388615861535072 + 100.0 * 6.208929538726807
Epoch 2330, val loss: 2.0938353538513184
Epoch 2340, training loss: 621.1207275390625 = 0.02894360013306141 + 100.0 * 6.210917949676514
Epoch 2340, val loss: 2.100274085998535
Epoch 2350, training loss: 621.2569580078125 = 0.02849734202027321 + 100.0 * 6.212284564971924
Epoch 2350, val loss: 2.1069977283477783
Epoch 2360, training loss: 620.9224243164062 = 0.02804124541580677 + 100.0 * 6.208943843841553
Epoch 2360, val loss: 2.112704277038574
Epoch 2370, training loss: 620.866455078125 = 0.027618523687124252 + 100.0 * 6.208387851715088
Epoch 2370, val loss: 2.119309186935425
Epoch 2380, training loss: 621.248046875 = 0.027209676802158356 + 100.0 * 6.212208271026611
Epoch 2380, val loss: 2.125811815261841
Epoch 2390, training loss: 620.8113403320312 = 0.026797566562891006 + 100.0 * 6.207845211029053
Epoch 2390, val loss: 2.1320643424987793
Epoch 2400, training loss: 620.8865966796875 = 0.026400964707136154 + 100.0 * 6.208601951599121
Epoch 2400, val loss: 2.1385998725891113
Epoch 2410, training loss: 620.8925170898438 = 0.02600906416773796 + 100.0 * 6.208665370941162
Epoch 2410, val loss: 2.1447713375091553
Epoch 2420, training loss: 620.8880615234375 = 0.02563481032848358 + 100.0 * 6.208624362945557
Epoch 2420, val loss: 2.151174306869507
Epoch 2430, training loss: 621.5185546875 = 0.0252644382417202 + 100.0 * 6.214932918548584
Epoch 2430, val loss: 2.1571168899536133
Epoch 2440, training loss: 620.9072875976562 = 0.02486727200448513 + 100.0 * 6.208824634552002
Epoch 2440, val loss: 2.162607192993164
Epoch 2450, training loss: 620.7062377929688 = 0.024507541209459305 + 100.0 * 6.206817150115967
Epoch 2450, val loss: 2.169128179550171
Epoch 2460, training loss: 620.6636962890625 = 0.02415938302874565 + 100.0 * 6.206395149230957
Epoch 2460, val loss: 2.174818754196167
Epoch 2470, training loss: 620.7374877929688 = 0.023821361362934113 + 100.0 * 6.207136631011963
Epoch 2470, val loss: 2.180645704269409
Epoch 2480, training loss: 621.1808471679688 = 0.0234872717410326 + 100.0 * 6.211573600769043
Epoch 2480, val loss: 2.18615984916687
Epoch 2490, training loss: 620.9635620117188 = 0.023155473172664642 + 100.0 * 6.209403991699219
Epoch 2490, val loss: 2.192807912826538
Epoch 2500, training loss: 620.6713256835938 = 0.02281530201435089 + 100.0 * 6.206485271453857
Epoch 2500, val loss: 2.198489189147949
Epoch 2510, training loss: 620.8836669921875 = 0.022512204945087433 + 100.0 * 6.208611488342285
Epoch 2510, val loss: 2.2046029567718506
Epoch 2520, training loss: 620.7788696289062 = 0.022196179255843163 + 100.0 * 6.207566261291504
Epoch 2520, val loss: 2.2100212574005127
Epoch 2530, training loss: 620.9171142578125 = 0.021879078820347786 + 100.0 * 6.2089524269104
Epoch 2530, val loss: 2.215268850326538
Epoch 2540, training loss: 620.8197021484375 = 0.021579405292868614 + 100.0 * 6.207981109619141
Epoch 2540, val loss: 2.2214460372924805
Epoch 2550, training loss: 620.8153076171875 = 0.02128756046295166 + 100.0 * 6.207940101623535
Epoch 2550, val loss: 2.2269859313964844
Epoch 2560, training loss: 620.52587890625 = 0.020992597565054893 + 100.0 * 6.20504903793335
Epoch 2560, val loss: 2.232412099838257
Epoch 2570, training loss: 620.5140380859375 = 0.02071138471364975 + 100.0 * 6.2049336433410645
Epoch 2570, val loss: 2.2381551265716553
Epoch 2580, training loss: 620.9553833007812 = 0.02045316807925701 + 100.0 * 6.209349155426025
Epoch 2580, val loss: 2.24381685256958
Epoch 2590, training loss: 620.6720581054688 = 0.0201715175062418 + 100.0 * 6.20651912689209
Epoch 2590, val loss: 2.2491719722747803
Epoch 2600, training loss: 620.5126953125 = 0.01989823952317238 + 100.0 * 6.204927921295166
Epoch 2600, val loss: 2.254636526107788
Epoch 2610, training loss: 620.6040649414062 = 0.019640738144516945 + 100.0 * 6.205843925476074
Epoch 2610, val loss: 2.2603819370269775
Epoch 2620, training loss: 620.424072265625 = 0.019383374601602554 + 100.0 * 6.204046726226807
Epoch 2620, val loss: 2.265570878982544
Epoch 2630, training loss: 620.53662109375 = 0.019140275195240974 + 100.0 * 6.205174446105957
Epoch 2630, val loss: 2.2710015773773193
Epoch 2640, training loss: 620.7706298828125 = 0.01889766752719879 + 100.0 * 6.207517147064209
Epoch 2640, val loss: 2.276383876800537
Epoch 2650, training loss: 620.6035766601562 = 0.0186529029160738 + 100.0 * 6.2058491706848145
Epoch 2650, val loss: 2.281073808670044
Epoch 2660, training loss: 620.7302856445312 = 0.018411455675959587 + 100.0 * 6.207118511199951
Epoch 2660, val loss: 2.286069631576538
Epoch 2670, training loss: 620.4296264648438 = 0.01817743480205536 + 100.0 * 6.2041144371032715
Epoch 2670, val loss: 2.291761636734009
Epoch 2680, training loss: 620.3670654296875 = 0.01794692687690258 + 100.0 * 6.2034912109375
Epoch 2680, val loss: 2.2969493865966797
Epoch 2690, training loss: 620.629638671875 = 0.01773061603307724 + 100.0 * 6.206119537353516
Epoch 2690, val loss: 2.3019235134124756
Epoch 2700, training loss: 620.428955078125 = 0.01750892773270607 + 100.0 * 6.2041144371032715
Epoch 2700, val loss: 2.3069138526916504
Epoch 2710, training loss: 620.40478515625 = 0.017295919358730316 + 100.0 * 6.2038750648498535
Epoch 2710, val loss: 2.312023162841797
Epoch 2720, training loss: 620.3236694335938 = 0.01708691008388996 + 100.0 * 6.203065872192383
Epoch 2720, val loss: 2.317155361175537
Epoch 2730, training loss: 621.09814453125 = 0.01689554937183857 + 100.0 * 6.210812091827393
Epoch 2730, val loss: 2.322800397872925
Epoch 2740, training loss: 620.7750244140625 = 0.016686396673321724 + 100.0 * 6.207583427429199
Epoch 2740, val loss: 2.3266196250915527
Epoch 2750, training loss: 620.3758544921875 = 0.016472669318318367 + 100.0 * 6.203593730926514
Epoch 2750, val loss: 2.3321266174316406
Epoch 2760, training loss: 620.3551635742188 = 0.01627965271472931 + 100.0 * 6.203388690948486
Epoch 2760, val loss: 2.336770534515381
Epoch 2770, training loss: 620.5625 = 0.01610017754137516 + 100.0 * 6.205463886260986
Epoch 2770, val loss: 2.342156171798706
Epoch 2780, training loss: 620.342529296875 = 0.015901250764727592 + 100.0 * 6.203266143798828
Epoch 2780, val loss: 2.3465681076049805
Epoch 2790, training loss: 620.3076171875 = 0.01571345515549183 + 100.0 * 6.2029194831848145
Epoch 2790, val loss: 2.351083517074585
Epoch 2800, training loss: 620.4387817382812 = 0.015539555810391903 + 100.0 * 6.204232215881348
Epoch 2800, val loss: 2.355783700942993
Epoch 2810, training loss: 620.33349609375 = 0.015360377728939056 + 100.0 * 6.203181266784668
Epoch 2810, val loss: 2.3605642318725586
Epoch 2820, training loss: 620.404541015625 = 0.015183873474597931 + 100.0 * 6.203893184661865
Epoch 2820, val loss: 2.364927291870117
Epoch 2830, training loss: 620.2999267578125 = 0.015006174333393574 + 100.0 * 6.202849388122559
Epoch 2830, val loss: 2.3693037033081055
Epoch 2840, training loss: 620.3369750976562 = 0.014838211238384247 + 100.0 * 6.203221321105957
Epoch 2840, val loss: 2.374086856842041
Epoch 2850, training loss: 620.55908203125 = 0.014673227444291115 + 100.0 * 6.2054443359375
Epoch 2850, val loss: 2.3780581951141357
Epoch 2860, training loss: 620.12060546875 = 0.014506524428725243 + 100.0 * 6.201060771942139
Epoch 2860, val loss: 2.3834950923919678
Epoch 2870, training loss: 620.1913452148438 = 0.014351844787597656 + 100.0 * 6.201770305633545
Epoch 2870, val loss: 2.388028383255005
Epoch 2880, training loss: 620.7089233398438 = 0.014198675751686096 + 100.0 * 6.206946849822998
Epoch 2880, val loss: 2.3918087482452393
Epoch 2890, training loss: 620.2467651367188 = 0.01403663121163845 + 100.0 * 6.202327251434326
Epoch 2890, val loss: 2.3966281414031982
Epoch 2900, training loss: 620.1309204101562 = 0.013883070088922977 + 100.0 * 6.201170921325684
Epoch 2900, val loss: 2.4012463092803955
Epoch 2910, training loss: 620.3297729492188 = 0.013740988448262215 + 100.0 * 6.203160285949707
Epoch 2910, val loss: 2.406066656112671
Epoch 2920, training loss: 620.2173461914062 = 0.013591487891972065 + 100.0 * 6.202037334442139
Epoch 2920, val loss: 2.4096384048461914
Epoch 2930, training loss: 620.3382568359375 = 0.0134437782689929 + 100.0 * 6.203248023986816
Epoch 2930, val loss: 2.4140255451202393
Epoch 2940, training loss: 620.130615234375 = 0.01330141443759203 + 100.0 * 6.201173305511475
Epoch 2940, val loss: 2.4185147285461426
Epoch 2950, training loss: 620.0323486328125 = 0.013159980066120625 + 100.0 * 6.200191497802734
Epoch 2950, val loss: 2.422731637954712
Epoch 2960, training loss: 620.0257568359375 = 0.013027009554207325 + 100.0 * 6.200127601623535
Epoch 2960, val loss: 2.4270172119140625
Epoch 2970, training loss: 620.3346557617188 = 0.012899186462163925 + 100.0 * 6.20321798324585
Epoch 2970, val loss: 2.43137788772583
Epoch 2980, training loss: 620.1621704101562 = 0.012762530706822872 + 100.0 * 6.201494216918945
Epoch 2980, val loss: 2.4349734783172607
Epoch 2990, training loss: 620.4603881835938 = 0.012639558874070644 + 100.0 * 6.204477787017822
Epoch 2990, val loss: 2.4394352436065674
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.7991565629942015
=== training gcn model ===
Epoch 0, training loss: 861.6468505859375 = 1.9598909616470337 + 100.0 * 8.596869468688965
Epoch 0, val loss: 1.9616470336914062
Epoch 10, training loss: 861.5845336914062 = 1.9509949684143066 + 100.0 * 8.596335411071777
Epoch 10, val loss: 1.952249526977539
Epoch 20, training loss: 861.1849975585938 = 1.9401782751083374 + 100.0 * 8.592448234558105
Epoch 20, val loss: 1.9406710863113403
Epoch 30, training loss: 858.322998046875 = 1.926121711730957 + 100.0 * 8.563968658447266
Epoch 30, val loss: 1.92547607421875
Epoch 40, training loss: 840.483642578125 = 1.9086830615997314 + 100.0 * 8.385749816894531
Epoch 40, val loss: 1.9073519706726074
Epoch 50, training loss: 772.752685546875 = 1.8904995918273926 + 100.0 * 7.708621978759766
Epoch 50, val loss: 1.8887674808502197
Epoch 60, training loss: 745.2999877929688 = 1.8754758834838867 + 100.0 * 7.4342451095581055
Epoch 60, val loss: 1.8743559122085571
Epoch 70, training loss: 722.1337890625 = 1.863228678703308 + 100.0 * 7.2027058601379395
Epoch 70, val loss: 1.8623231649398804
Epoch 80, training loss: 703.0324096679688 = 1.8517091274261475 + 100.0 * 7.011807441711426
Epoch 80, val loss: 1.8510984182357788
Epoch 90, training loss: 688.4598999023438 = 1.8418865203857422 + 100.0 * 6.866180419921875
Epoch 90, val loss: 1.8414586782455444
Epoch 100, training loss: 677.8114624023438 = 1.8333401679992676 + 100.0 * 6.759781360626221
Epoch 100, val loss: 1.832992434501648
Epoch 110, training loss: 671.4911499023438 = 1.825230360031128 + 100.0 * 6.696659088134766
Epoch 110, val loss: 1.8250117301940918
Epoch 120, training loss: 666.8826293945312 = 1.8175303936004639 + 100.0 * 6.650651454925537
Epoch 120, val loss: 1.817215919494629
Epoch 130, training loss: 663.5399169921875 = 1.809926152229309 + 100.0 * 6.617300033569336
Epoch 130, val loss: 1.809494137763977
Epoch 140, training loss: 660.9884643554688 = 1.8021347522735596 + 100.0 * 6.591863632202148
Epoch 140, val loss: 1.8014850616455078
Epoch 150, training loss: 658.6544799804688 = 1.793865442276001 + 100.0 * 6.568605899810791
Epoch 150, val loss: 1.7932041883468628
Epoch 160, training loss: 656.2394409179688 = 1.785103678703308 + 100.0 * 6.544543266296387
Epoch 160, val loss: 1.7844914197921753
Epoch 170, training loss: 654.154541015625 = 1.7757244110107422 + 100.0 * 6.5237884521484375
Epoch 170, val loss: 1.7753854990005493
Epoch 180, training loss: 652.2373657226562 = 1.7655702829360962 + 100.0 * 6.50471830368042
Epoch 180, val loss: 1.765620231628418
Epoch 190, training loss: 650.9708862304688 = 1.7544386386871338 + 100.0 * 6.492164134979248
Epoch 190, val loss: 1.7549638748168945
Epoch 200, training loss: 649.3336791992188 = 1.7420616149902344 + 100.0 * 6.475915908813477
Epoch 200, val loss: 1.7432544231414795
Epoch 210, training loss: 647.9799194335938 = 1.7283979654312134 + 100.0 * 6.462515354156494
Epoch 210, val loss: 1.7303518056869507
Epoch 220, training loss: 646.9454345703125 = 1.713391900062561 + 100.0 * 6.452320575714111
Epoch 220, val loss: 1.7163190841674805
Epoch 230, training loss: 646.0369262695312 = 1.6969537734985352 + 100.0 * 6.443399429321289
Epoch 230, val loss: 1.7009636163711548
Epoch 240, training loss: 645.0188598632812 = 1.679033875465393 + 100.0 * 6.433398723602295
Epoch 240, val loss: 1.6844215393066406
Epoch 250, training loss: 644.3457641601562 = 1.6597075462341309 + 100.0 * 6.426860332489014
Epoch 250, val loss: 1.6667711734771729
Epoch 260, training loss: 643.4995727539062 = 1.638899326324463 + 100.0 * 6.418606281280518
Epoch 260, val loss: 1.6480077505111694
Epoch 270, training loss: 642.7611694335938 = 1.6168574094772339 + 100.0 * 6.411442756652832
Epoch 270, val loss: 1.628371000289917
Epoch 280, training loss: 642.1397094726562 = 1.5937387943267822 + 100.0 * 6.405459403991699
Epoch 280, val loss: 1.6080466508865356
Epoch 290, training loss: 641.5624389648438 = 1.5696896314620972 + 100.0 * 6.399927616119385
Epoch 290, val loss: 1.5872406959533691
Epoch 300, training loss: 641.008544921875 = 1.5448367595672607 + 100.0 * 6.394636631011963
Epoch 300, val loss: 1.5660580396652222
Epoch 310, training loss: 640.4143676757812 = 1.5195621252059937 + 100.0 * 6.388947486877441
Epoch 310, val loss: 1.544826626777649
Epoch 320, training loss: 639.8414916992188 = 1.4941074848175049 + 100.0 * 6.383473873138428
Epoch 320, val loss: 1.5236529111862183
Epoch 330, training loss: 639.9682006835938 = 1.468717098236084 + 100.0 * 6.384994983673096
Epoch 330, val loss: 1.5026190280914307
Epoch 340, training loss: 638.9158935546875 = 1.4431934356689453 + 100.0 * 6.374727249145508
Epoch 340, val loss: 1.4820700883865356
Epoch 350, training loss: 638.4715576171875 = 1.4180880784988403 + 100.0 * 6.370534420013428
Epoch 350, val loss: 1.4620226621627808
Epoch 360, training loss: 638.0460205078125 = 1.393383264541626 + 100.0 * 6.3665266036987305
Epoch 360, val loss: 1.4425572156906128
Epoch 370, training loss: 638.0670776367188 = 1.3691179752349854 + 100.0 * 6.366979122161865
Epoch 370, val loss: 1.4236356019973755
Epoch 380, training loss: 637.2943725585938 = 1.3450138568878174 + 100.0 * 6.359493255615234
Epoch 380, val loss: 1.4050462245941162
Epoch 390, training loss: 636.955322265625 = 1.3214130401611328 + 100.0 * 6.356338977813721
Epoch 390, val loss: 1.387052059173584
Epoch 400, training loss: 636.6497192382812 = 1.2980504035949707 + 100.0 * 6.353517055511475
Epoch 400, val loss: 1.3696045875549316
Epoch 410, training loss: 636.1278076171875 = 1.2750201225280762 + 100.0 * 6.348527908325195
Epoch 410, val loss: 1.35247004032135
Epoch 420, training loss: 635.8234252929688 = 1.2522815465927124 + 100.0 * 6.345711708068848
Epoch 420, val loss: 1.3358343839645386
Epoch 430, training loss: 635.603759765625 = 1.2297402620315552 + 100.0 * 6.343740463256836
Epoch 430, val loss: 1.3195607662200928
Epoch 440, training loss: 635.1818237304688 = 1.2072941064834595 + 100.0 * 6.33974552154541
Epoch 440, val loss: 1.3036094903945923
Epoch 450, training loss: 634.8489379882812 = 1.1851365566253662 + 100.0 * 6.336638450622559
Epoch 450, val loss: 1.28803288936615
Epoch 460, training loss: 634.8963623046875 = 1.16317617893219 + 100.0 * 6.337332248687744
Epoch 460, val loss: 1.2729408740997314
Epoch 470, training loss: 634.4508666992188 = 1.14149808883667 + 100.0 * 6.333093643188477
Epoch 470, val loss: 1.2578622102737427
Epoch 480, training loss: 634.098876953125 = 1.119917631149292 + 100.0 * 6.329789638519287
Epoch 480, val loss: 1.243648886680603
Epoch 490, training loss: 633.8143310546875 = 1.0987675189971924 + 100.0 * 6.327155590057373
Epoch 490, val loss: 1.229607105255127
Epoch 500, training loss: 633.6864013671875 = 1.0777627229690552 + 100.0 * 6.326086521148682
Epoch 500, val loss: 1.2163180112838745
Epoch 510, training loss: 633.4459838867188 = 1.0571541786193848 + 100.0 * 6.323888301849365
Epoch 510, val loss: 1.2031511068344116
Epoch 520, training loss: 633.1416015625 = 1.0369293689727783 + 100.0 * 6.321046829223633
Epoch 520, val loss: 1.1909570693969727
Epoch 530, training loss: 632.9959106445312 = 1.0171014070510864 + 100.0 * 6.319787979125977
Epoch 530, val loss: 1.1791528463363647
Epoch 540, training loss: 632.7064819335938 = 0.9977220892906189 + 100.0 * 6.317087173461914
Epoch 540, val loss: 1.1679604053497314
Epoch 550, training loss: 632.5803833007812 = 0.9787063002586365 + 100.0 * 6.316016674041748
Epoch 550, val loss: 1.1573779582977295
Epoch 560, training loss: 632.3845825195312 = 0.9601826071739197 + 100.0 * 6.314243793487549
Epoch 560, val loss: 1.1473450660705566
Epoch 570, training loss: 632.3084716796875 = 0.9420791268348694 + 100.0 * 6.313663959503174
Epoch 570, val loss: 1.137926459312439
Epoch 580, training loss: 631.9899291992188 = 0.9245297908782959 + 100.0 * 6.310654163360596
Epoch 580, val loss: 1.1289973258972168
Epoch 590, training loss: 631.748291015625 = 0.9073237776756287 + 100.0 * 6.308409214019775
Epoch 590, val loss: 1.1205105781555176
Epoch 600, training loss: 631.6323852539062 = 0.8906124234199524 + 100.0 * 6.307417869567871
Epoch 600, val loss: 1.1126829385757446
Epoch 610, training loss: 631.5443725585938 = 0.8743081092834473 + 100.0 * 6.306700229644775
Epoch 610, val loss: 1.1053152084350586
Epoch 620, training loss: 631.1502075195312 = 0.858348548412323 + 100.0 * 6.302918910980225
Epoch 620, val loss: 1.0984102487564087
Epoch 630, training loss: 630.95703125 = 0.8428386449813843 + 100.0 * 6.301141738891602
Epoch 630, val loss: 1.092002511024475
Epoch 640, training loss: 631.1995239257812 = 0.8277004361152649 + 100.0 * 6.303718566894531
Epoch 640, val loss: 1.0861386060714722
Epoch 650, training loss: 631.0405883789062 = 0.812887966632843 + 100.0 * 6.302277088165283
Epoch 650, val loss: 1.0799190998077393
Epoch 660, training loss: 630.5174560546875 = 0.7982582449913025 + 100.0 * 6.297191619873047
Epoch 660, val loss: 1.074664831161499
Epoch 670, training loss: 630.3607177734375 = 0.7840674519538879 + 100.0 * 6.295766830444336
Epoch 670, val loss: 1.0696982145309448
Epoch 680, training loss: 630.5678100585938 = 0.7701779007911682 + 100.0 * 6.297976016998291
Epoch 680, val loss: 1.0649123191833496
Epoch 690, training loss: 630.2071533203125 = 0.7563645243644714 + 100.0 * 6.29450798034668
Epoch 690, val loss: 1.0599682331085205
Epoch 700, training loss: 630.4764404296875 = 0.7428019642829895 + 100.0 * 6.297336578369141
Epoch 700, val loss: 1.0557115077972412
Epoch 710, training loss: 629.8919067382812 = 0.7294934391975403 + 100.0 * 6.291624069213867
Epoch 710, val loss: 1.0516889095306396
Epoch 720, training loss: 629.7388305664062 = 0.7163293957710266 + 100.0 * 6.290225028991699
Epoch 720, val loss: 1.0474554300308228
Epoch 730, training loss: 629.769287109375 = 0.7034267783164978 + 100.0 * 6.290658473968506
Epoch 730, val loss: 1.043610692024231
Epoch 740, training loss: 629.4616088867188 = 0.6905482411384583 + 100.0 * 6.287710666656494
Epoch 740, val loss: 1.0398746728897095
Epoch 750, training loss: 629.2380981445312 = 0.6778960824012756 + 100.0 * 6.285601615905762
Epoch 750, val loss: 1.0362763404846191
Epoch 760, training loss: 629.216552734375 = 0.6653838157653809 + 100.0 * 6.285511493682861
Epoch 760, val loss: 1.0327398777008057
Epoch 770, training loss: 629.2338256835938 = 0.6530697345733643 + 100.0 * 6.2858076095581055
Epoch 770, val loss: 1.0295517444610596
Epoch 780, training loss: 629.3790893554688 = 0.6407382488250732 + 100.0 * 6.287383556365967
Epoch 780, val loss: 1.0259490013122559
Epoch 790, training loss: 629.1509399414062 = 0.6285621523857117 + 100.0 * 6.285223960876465
Epoch 790, val loss: 1.0224759578704834
Epoch 800, training loss: 628.6979370117188 = 0.6165076494216919 + 100.0 * 6.280814170837402
Epoch 800, val loss: 1.0194566249847412
Epoch 810, training loss: 628.374267578125 = 0.6047022938728333 + 100.0 * 6.277695655822754
Epoch 810, val loss: 1.0164154767990112
Epoch 820, training loss: 628.2720336914062 = 0.593083918094635 + 100.0 * 6.276789665222168
Epoch 820, val loss: 1.01345694065094
Epoch 830, training loss: 628.1771240234375 = 0.5816341042518616 + 100.0 * 6.2759552001953125
Epoch 830, val loss: 1.0107626914978027
Epoch 840, training loss: 628.7186889648438 = 0.5702943205833435 + 100.0 * 6.2814836502075195
Epoch 840, val loss: 1.0079127550125122
Epoch 850, training loss: 628.0542602539062 = 0.5588945150375366 + 100.0 * 6.274953365325928
Epoch 850, val loss: 1.0051454305648804
Epoch 860, training loss: 628.2792358398438 = 0.5478051900863647 + 100.0 * 6.27731466293335
Epoch 860, val loss: 1.0026768445968628
Epoch 870, training loss: 627.8480834960938 = 0.5368679761886597 + 100.0 * 6.2731122970581055
Epoch 870, val loss: 1.0003089904785156
Epoch 880, training loss: 627.6439819335938 = 0.5261536836624146 + 100.0 * 6.271178245544434
Epoch 880, val loss: 0.9980441331863403
Epoch 890, training loss: 627.9824829101562 = 0.515619158744812 + 100.0 * 6.2746686935424805
Epoch 890, val loss: 0.9960963726043701
Epoch 900, training loss: 627.5415649414062 = 0.5052446722984314 + 100.0 * 6.2703633308410645
Epoch 900, val loss: 0.9938295483589172
Epoch 910, training loss: 627.3912353515625 = 0.49504905939102173 + 100.0 * 6.2689619064331055
Epoch 910, val loss: 0.9920814633369446
Epoch 920, training loss: 627.4802856445312 = 0.4850453734397888 + 100.0 * 6.269952297210693
Epoch 920, val loss: 0.9904855489730835
Epoch 930, training loss: 627.1181640625 = 0.47514989972114563 + 100.0 * 6.266429901123047
Epoch 930, val loss: 0.9886422753334045
Epoch 940, training loss: 627.0615844726562 = 0.46545544266700745 + 100.0 * 6.265961170196533
Epoch 940, val loss: 0.9873046875
Epoch 950, training loss: 627.3546752929688 = 0.45595160126686096 + 100.0 * 6.26898717880249
Epoch 950, val loss: 0.9859726428985596
Epoch 960, training loss: 627.515869140625 = 0.44648417830467224 + 100.0 * 6.270693778991699
Epoch 960, val loss: 0.9844501614570618
Epoch 970, training loss: 627.0188598632812 = 0.43707072734832764 + 100.0 * 6.265818119049072
Epoch 970, val loss: 0.9833082556724548
Epoch 980, training loss: 626.67626953125 = 0.4279736876487732 + 100.0 * 6.262482643127441
Epoch 980, val loss: 0.9824711680412292
Epoch 990, training loss: 626.5695190429688 = 0.4190255403518677 + 100.0 * 6.261505126953125
Epoch 990, val loss: 0.981704592704773
Epoch 1000, training loss: 626.6004638671875 = 0.41019582748413086 + 100.0 * 6.261902332305908
Epoch 1000, val loss: 0.9809575080871582
Epoch 1010, training loss: 626.96923828125 = 0.40146419405937195 + 100.0 * 6.265677452087402
Epoch 1010, val loss: 0.980331301689148
Epoch 1020, training loss: 626.7081298828125 = 0.39272889494895935 + 100.0 * 6.263154029846191
Epoch 1020, val loss: 0.9798303842544556
Epoch 1030, training loss: 626.2388916015625 = 0.3842558264732361 + 100.0 * 6.258545875549316
Epoch 1030, val loss: 0.9795929789543152
Epoch 1040, training loss: 626.2026977539062 = 0.3759584426879883 + 100.0 * 6.258266925811768
Epoch 1040, val loss: 0.9795928001403809
Epoch 1050, training loss: 626.080810546875 = 0.36777812242507935 + 100.0 * 6.257130146026611
Epoch 1050, val loss: 0.9796426296234131
Epoch 1060, training loss: 627.333984375 = 0.3597852885723114 + 100.0 * 6.269741535186768
Epoch 1060, val loss: 0.9798014163970947
Epoch 1070, training loss: 626.6690063476562 = 0.35159316658973694 + 100.0 * 6.263174057006836
Epoch 1070, val loss: 0.9794965982437134
Epoch 1080, training loss: 626.0084228515625 = 0.34363871812820435 + 100.0 * 6.256648063659668
Epoch 1080, val loss: 0.9800267815589905
Epoch 1090, training loss: 625.8123779296875 = 0.3359232246875763 + 100.0 * 6.254764556884766
Epoch 1090, val loss: 0.9804877638816833
Epoch 1100, training loss: 625.7481689453125 = 0.3283421993255615 + 100.0 * 6.25419807434082
Epoch 1100, val loss: 0.9811183214187622
Epoch 1110, training loss: 625.7108154296875 = 0.32086682319641113 + 100.0 * 6.253899574279785
Epoch 1110, val loss: 0.9818894267082214
Epoch 1120, training loss: 626.559814453125 = 0.31346395611763 + 100.0 * 6.262463092803955
Epoch 1120, val loss: 0.9826322197914124
Epoch 1130, training loss: 625.7107543945312 = 0.3061242699623108 + 100.0 * 6.2540459632873535
Epoch 1130, val loss: 0.9833590984344482
Epoch 1140, training loss: 625.559326171875 = 0.2990051209926605 + 100.0 * 6.252603054046631
Epoch 1140, val loss: 0.9842761158943176
Epoch 1150, training loss: 625.5493774414062 = 0.29204893112182617 + 100.0 * 6.252573490142822
Epoch 1150, val loss: 0.9855250716209412
Epoch 1160, training loss: 626.0013427734375 = 0.2851760685443878 + 100.0 * 6.257161617279053
Epoch 1160, val loss: 0.9868224263191223
Epoch 1170, training loss: 625.5375366210938 = 0.2784297466278076 + 100.0 * 6.252591133117676
Epoch 1170, val loss: 0.9879807233810425
Epoch 1180, training loss: 625.3242797851562 = 0.27181148529052734 + 100.0 * 6.250524997711182
Epoch 1180, val loss: 0.9897040724754333
Epoch 1190, training loss: 625.2269897460938 = 0.2653718590736389 + 100.0 * 6.2496161460876465
Epoch 1190, val loss: 0.9913358092308044
Epoch 1200, training loss: 625.775634765625 = 0.25907161831855774 + 100.0 * 6.2551655769348145
Epoch 1200, val loss: 0.9931046366691589
Epoch 1210, training loss: 625.3656616210938 = 0.25280454754829407 + 100.0 * 6.251128673553467
Epoch 1210, val loss: 0.9944470524787903
Epoch 1220, training loss: 625.1510009765625 = 0.24672265350818634 + 100.0 * 6.249042987823486
Epoch 1220, val loss: 0.9967174530029297
Epoch 1230, training loss: 625.112548828125 = 0.24081651866436005 + 100.0 * 6.248717308044434
Epoch 1230, val loss: 0.9985564351081848
Epoch 1240, training loss: 625.6483764648438 = 0.23502105474472046 + 100.0 * 6.254133224487305
Epoch 1240, val loss: 1.0008032321929932
Epoch 1250, training loss: 625.0540771484375 = 0.22927546501159668 + 100.0 * 6.248248100280762
Epoch 1250, val loss: 1.0031455755233765
Epoch 1260, training loss: 624.949462890625 = 0.22372379899024963 + 100.0 * 6.247257709503174
Epoch 1260, val loss: 1.0055475234985352
Epoch 1270, training loss: 624.8638916015625 = 0.21831907331943512 + 100.0 * 6.246455669403076
Epoch 1270, val loss: 1.008171796798706
Epoch 1280, training loss: 625.0504760742188 = 0.21306295692920685 + 100.0 * 6.248373985290527
Epoch 1280, val loss: 1.0108442306518555
Epoch 1290, training loss: 624.9205322265625 = 0.20789487659931183 + 100.0 * 6.247126579284668
Epoch 1290, val loss: 1.0133415460586548
Epoch 1300, training loss: 624.8551635742188 = 0.2028583586215973 + 100.0 * 6.246522903442383
Epoch 1300, val loss: 1.0163450241088867
Epoch 1310, training loss: 624.6981811523438 = 0.197929248213768 + 100.0 * 6.245002269744873
Epoch 1310, val loss: 1.01936674118042
Epoch 1320, training loss: 624.5907592773438 = 0.19313561916351318 + 100.0 * 6.243976593017578
Epoch 1320, val loss: 1.0225051641464233
Epoch 1330, training loss: 624.8569946289062 = 0.18848775327205658 + 100.0 * 6.246685028076172
Epoch 1330, val loss: 1.0258703231811523
Epoch 1340, training loss: 624.526123046875 = 0.183905690908432 + 100.0 * 6.243422031402588
Epoch 1340, val loss: 1.028969645500183
Epoch 1350, training loss: 624.6319580078125 = 0.17944282293319702 + 100.0 * 6.244524955749512
Epoch 1350, val loss: 1.032549262046814
Epoch 1360, training loss: 624.8370971679688 = 0.17508019506931305 + 100.0 * 6.246620178222656
Epoch 1360, val loss: 1.035577654838562
Epoch 1370, training loss: 624.6808471679688 = 0.17080026865005493 + 100.0 * 6.245100975036621
Epoch 1370, val loss: 1.0397123098373413
Epoch 1380, training loss: 624.4210815429688 = 0.16670005023479462 + 100.0 * 6.242543697357178
Epoch 1380, val loss: 1.043212652206421
Epoch 1390, training loss: 624.289306640625 = 0.1626930981874466 + 100.0 * 6.241265773773193
Epoch 1390, val loss: 1.0473334789276123
Epoch 1400, training loss: 624.4110107421875 = 0.15882885456085205 + 100.0 * 6.242522239685059
Epoch 1400, val loss: 1.051137089729309
Epoch 1410, training loss: 624.433349609375 = 0.15500831604003906 + 100.0 * 6.242783069610596
Epoch 1410, val loss: 1.0550609827041626
Epoch 1420, training loss: 624.3877563476562 = 0.15126098692417145 + 100.0 * 6.242364883422852
Epoch 1420, val loss: 1.0589526891708374
Epoch 1430, training loss: 624.7725219726562 = 0.1476549208164215 + 100.0 * 6.246248722076416
Epoch 1430, val loss: 1.0628933906555176
Epoch 1440, training loss: 624.31201171875 = 0.1440693587064743 + 100.0 * 6.241679668426514
Epoch 1440, val loss: 1.0676077604293823
Epoch 1450, training loss: 624.1074829101562 = 0.14065586030483246 + 100.0 * 6.239667892456055
Epoch 1450, val loss: 1.071447730064392
Epoch 1460, training loss: 623.9650268554688 = 0.1373157948255539 + 100.0 * 6.238276958465576
Epoch 1460, val loss: 1.0761398077011108
Epoch 1470, training loss: 623.9989624023438 = 0.1340932995080948 + 100.0 * 6.238648891448975
Epoch 1470, val loss: 1.0804290771484375
Epoch 1480, training loss: 624.93212890625 = 0.1309283971786499 + 100.0 * 6.248012065887451
Epoch 1480, val loss: 1.084933876991272
Epoch 1490, training loss: 624.4911499023438 = 0.12783204019069672 + 100.0 * 6.243633270263672
Epoch 1490, val loss: 1.0892326831817627
Epoch 1500, training loss: 623.9495239257812 = 0.12476697564125061 + 100.0 * 6.238247871398926
Epoch 1500, val loss: 1.0933337211608887
Epoch 1510, training loss: 623.7756958007812 = 0.12185370177030563 + 100.0 * 6.236538410186768
Epoch 1510, val loss: 1.098198652267456
Epoch 1520, training loss: 623.7617797851562 = 0.1190515011548996 + 100.0 * 6.236427307128906
Epoch 1520, val loss: 1.1029775142669678
Epoch 1530, training loss: 623.992919921875 = 0.1163000538945198 + 100.0 * 6.238766670227051
Epoch 1530, val loss: 1.107731580734253
Epoch 1540, training loss: 623.8758544921875 = 0.11358745396137238 + 100.0 * 6.2376227378845215
Epoch 1540, val loss: 1.1118017435073853
Epoch 1550, training loss: 623.662109375 = 0.1109338328242302 + 100.0 * 6.235511779785156
Epoch 1550, val loss: 1.1168668270111084
Epoch 1560, training loss: 623.6813354492188 = 0.10840041190385818 + 100.0 * 6.235729217529297
Epoch 1560, val loss: 1.1216354370117188
Epoch 1570, training loss: 623.81494140625 = 0.10593904554843903 + 100.0 * 6.23708963394165
Epoch 1570, val loss: 1.1264700889587402
Epoch 1580, training loss: 623.8432006835938 = 0.10352638363838196 + 100.0 * 6.237396717071533
Epoch 1580, val loss: 1.131234049797058
Epoch 1590, training loss: 623.6524047851562 = 0.10116706043481827 + 100.0 * 6.2355122566223145
Epoch 1590, val loss: 1.1358580589294434
Epoch 1600, training loss: 623.5946655273438 = 0.0988895371556282 + 100.0 * 6.234957695007324
Epoch 1600, val loss: 1.1408681869506836
Epoch 1610, training loss: 623.8116455078125 = 0.09668872505426407 + 100.0 * 6.237149715423584
Epoch 1610, val loss: 1.1460360288619995
Epoch 1620, training loss: 623.4970092773438 = 0.09450044482946396 + 100.0 * 6.234025001525879
Epoch 1620, val loss: 1.1502982378005981
Epoch 1630, training loss: 623.672607421875 = 0.09239793568849564 + 100.0 * 6.235801696777344
Epoch 1630, val loss: 1.154958724975586
Epoch 1640, training loss: 623.4743041992188 = 0.09032797813415527 + 100.0 * 6.233839511871338
Epoch 1640, val loss: 1.1598808765411377
Epoch 1650, training loss: 623.3600463867188 = 0.08833043277263641 + 100.0 * 6.232717514038086
Epoch 1650, val loss: 1.1650422811508179
Epoch 1660, training loss: 623.2813720703125 = 0.08639632910490036 + 100.0 * 6.231949329376221
Epoch 1660, val loss: 1.1695446968078613
Epoch 1670, training loss: 623.5075073242188 = 0.08454570919275284 + 100.0 * 6.234230041503906
Epoch 1670, val loss: 1.1743611097335815
Epoch 1680, training loss: 623.3779907226562 = 0.0826774388551712 + 100.0 * 6.23295259475708
Epoch 1680, val loss: 1.1793055534362793
Epoch 1690, training loss: 623.3109741210938 = 0.08085998892784119 + 100.0 * 6.232300758361816
Epoch 1690, val loss: 1.1843376159667969
Epoch 1700, training loss: 623.406005859375 = 0.07914038747549057 + 100.0 * 6.2332682609558105
Epoch 1700, val loss: 1.1890921592712402
Epoch 1710, training loss: 623.2296142578125 = 0.07743290066719055 + 100.0 * 6.2315216064453125
Epoch 1710, val loss: 1.1940346956253052
Epoch 1720, training loss: 623.2555541992188 = 0.07578670978546143 + 100.0 * 6.231797218322754
Epoch 1720, val loss: 1.1988012790679932
Epoch 1730, training loss: 623.4829711914062 = 0.07417725026607513 + 100.0 * 6.234087944030762
Epoch 1730, val loss: 1.2032866477966309
Epoch 1740, training loss: 623.348876953125 = 0.07258457690477371 + 100.0 * 6.232762813568115
Epoch 1740, val loss: 1.2081388235092163
Epoch 1750, training loss: 623.1112670898438 = 0.07104377448558807 + 100.0 * 6.23040246963501
Epoch 1750, val loss: 1.212861180305481
Epoch 1760, training loss: 623.3892211914062 = 0.06957509368658066 + 100.0 * 6.233196258544922
Epoch 1760, val loss: 1.2178853750228882
Epoch 1770, training loss: 623.1751098632812 = 0.06809242069721222 + 100.0 * 6.231070041656494
Epoch 1770, val loss: 1.222165584564209
Epoch 1780, training loss: 623.025146484375 = 0.0666675716638565 + 100.0 * 6.22958517074585
Epoch 1780, val loss: 1.2269160747528076
Epoch 1790, training loss: 622.9857177734375 = 0.06531041860580444 + 100.0 * 6.229204177856445
Epoch 1790, val loss: 1.232221245765686
Epoch 1800, training loss: 623.0029296875 = 0.06397251039743423 + 100.0 * 6.229389667510986
Epoch 1800, val loss: 1.2368322610855103
Epoch 1810, training loss: 623.2936401367188 = 0.06266524642705917 + 100.0 * 6.232309341430664
Epoch 1810, val loss: 1.2417256832122803
Epoch 1820, training loss: 623.3262939453125 = 0.06137634813785553 + 100.0 * 6.232648849487305
Epoch 1820, val loss: 1.2453490495681763
Epoch 1830, training loss: 622.9419555664062 = 0.06011508032679558 + 100.0 * 6.228818416595459
Epoch 1830, val loss: 1.2503589391708374
Epoch 1840, training loss: 622.8533325195312 = 0.05891430377960205 + 100.0 * 6.227944374084473
Epoch 1840, val loss: 1.255042314529419
Epoch 1850, training loss: 623.1505126953125 = 0.05774269998073578 + 100.0 * 6.23092794418335
Epoch 1850, val loss: 1.2598633766174316
Epoch 1860, training loss: 622.7930908203125 = 0.056585945188999176 + 100.0 * 6.227365016937256
Epoch 1860, val loss: 1.2639790773391724
Epoch 1870, training loss: 623.42626953125 = 0.055471740663051605 + 100.0 * 6.233707904815674
Epoch 1870, val loss: 1.2685900926589966
Epoch 1880, training loss: 622.901611328125 = 0.05437444895505905 + 100.0 * 6.2284722328186035
Epoch 1880, val loss: 1.2731624841690063
Epoch 1890, training loss: 622.8441162109375 = 0.05329665169119835 + 100.0 * 6.227908134460449
Epoch 1890, val loss: 1.2775084972381592
Epoch 1900, training loss: 622.6979370117188 = 0.05225805193185806 + 100.0 * 6.226457118988037
Epoch 1900, val loss: 1.2821393013000488
Epoch 1910, training loss: 622.6568603515625 = 0.05125585198402405 + 100.0 * 6.226056098937988
Epoch 1910, val loss: 1.2862826585769653
Epoch 1920, training loss: 623.5249633789062 = 0.05029124394059181 + 100.0 * 6.234746932983398
Epoch 1920, val loss: 1.2901326417922974
Epoch 1930, training loss: 622.8388671875 = 0.04929198697209358 + 100.0 * 6.227895259857178
Epoch 1930, val loss: 1.2953507900238037
Epoch 1940, training loss: 622.6639404296875 = 0.04835090413689613 + 100.0 * 6.226156234741211
Epoch 1940, val loss: 1.2991594076156616
Epoch 1950, training loss: 622.7699584960938 = 0.0474468357861042 + 100.0 * 6.227225303649902
Epoch 1950, val loss: 1.3037889003753662
Epoch 1960, training loss: 622.6968383789062 = 0.046550948172807693 + 100.0 * 6.226502418518066
Epoch 1960, val loss: 1.3077231645584106
Epoch 1970, training loss: 622.8182983398438 = 0.045675940811634064 + 100.0 * 6.227726459503174
Epoch 1970, val loss: 1.3121137619018555
Epoch 1980, training loss: 622.4759521484375 = 0.04483390226960182 + 100.0 * 6.224310874938965
Epoch 1980, val loss: 1.3164793252944946
Epoch 1990, training loss: 622.4264526367188 = 0.04401357099413872 + 100.0 * 6.223824501037598
Epoch 1990, val loss: 1.3206980228424072
Epoch 2000, training loss: 622.9368286132812 = 0.04322144016623497 + 100.0 * 6.228936195373535
Epoch 2000, val loss: 1.324658751487732
Epoch 2010, training loss: 622.7218627929688 = 0.04241889342665672 + 100.0 * 6.226794719696045
Epoch 2010, val loss: 1.3286551237106323
Epoch 2020, training loss: 622.574951171875 = 0.0416378490626812 + 100.0 * 6.225333213806152
Epoch 2020, val loss: 1.3326963186264038
Epoch 2030, training loss: 622.8438110351562 = 0.04089987650513649 + 100.0 * 6.228029251098633
Epoch 2030, val loss: 1.3375273942947388
Epoch 2040, training loss: 622.4234619140625 = 0.04014716297388077 + 100.0 * 6.223833084106445
Epoch 2040, val loss: 1.340919017791748
Epoch 2050, training loss: 622.2744140625 = 0.03943594917654991 + 100.0 * 6.222349643707275
Epoch 2050, val loss: 1.3449698686599731
Epoch 2060, training loss: 622.2702026367188 = 0.03874897211790085 + 100.0 * 6.222314357757568
Epoch 2060, val loss: 1.349101185798645
Epoch 2070, training loss: 622.7265625 = 0.038087744265794754 + 100.0 * 6.226884841918945
Epoch 2070, val loss: 1.3529638051986694
Epoch 2080, training loss: 622.5980834960938 = 0.03740788251161575 + 100.0 * 6.225606918334961
Epoch 2080, val loss: 1.3561407327651978
Epoch 2090, training loss: 622.3651123046875 = 0.03673624247312546 + 100.0 * 6.223283767700195
Epoch 2090, val loss: 1.3608312606811523
Epoch 2100, training loss: 622.2755126953125 = 0.03610505163669586 + 100.0 * 6.22239351272583
Epoch 2100, val loss: 1.364491581916809
Epoch 2110, training loss: 622.3817749023438 = 0.03549251705408096 + 100.0 * 6.2234625816345215
Epoch 2110, val loss: 1.3685587644577026
Epoch 2120, training loss: 622.2708129882812 = 0.03488977625966072 + 100.0 * 6.2223591804504395
Epoch 2120, val loss: 1.3722496032714844
Epoch 2130, training loss: 622.0867309570312 = 0.034304842352867126 + 100.0 * 6.220524311065674
Epoch 2130, val loss: 1.3757646083831787
Epoch 2140, training loss: 622.85205078125 = 0.03375033289194107 + 100.0 * 6.228182792663574
Epoch 2140, val loss: 1.3787590265274048
Epoch 2150, training loss: 622.51123046875 = 0.033163245767354965 + 100.0 * 6.224781036376953
Epoch 2150, val loss: 1.383064866065979
Epoch 2160, training loss: 622.1949462890625 = 0.0326053611934185 + 100.0 * 6.221623420715332
Epoch 2160, val loss: 1.3870633840560913
Epoch 2170, training loss: 622.1099853515625 = 0.03207388147711754 + 100.0 * 6.2207794189453125
Epoch 2170, val loss: 1.3906269073486328
Epoch 2180, training loss: 622.55078125 = 0.03156575188040733 + 100.0 * 6.225192070007324
Epoch 2180, val loss: 1.3941650390625
Epoch 2190, training loss: 622.0914916992188 = 0.031037939712405205 + 100.0 * 6.220604419708252
Epoch 2190, val loss: 1.397873044013977
Epoch 2200, training loss: 622.086181640625 = 0.03053719736635685 + 100.0 * 6.220556735992432
Epoch 2200, val loss: 1.401604413986206
Epoch 2210, training loss: 622.4788818359375 = 0.03005080297589302 + 100.0 * 6.224488258361816
Epoch 2210, val loss: 1.4054028987884521
Epoch 2220, training loss: 621.9985961914062 = 0.02957320585846901 + 100.0 * 6.219689846038818
Epoch 2220, val loss: 1.4086304903030396
Epoch 2230, training loss: 622.0592041015625 = 0.02910745143890381 + 100.0 * 6.220301151275635
Epoch 2230, val loss: 1.4120122194290161
Epoch 2240, training loss: 622.1259765625 = 0.02865430898964405 + 100.0 * 6.220973491668701
Epoch 2240, val loss: 1.4160734415054321
Epoch 2250, training loss: 622.0835571289062 = 0.02821223996579647 + 100.0 * 6.220553874969482
Epoch 2250, val loss: 1.41977858543396
Epoch 2260, training loss: 622.077880859375 = 0.027774786576628685 + 100.0 * 6.220500946044922
Epoch 2260, val loss: 1.4227150678634644
Epoch 2270, training loss: 621.9131469726562 = 0.027334725484251976 + 100.0 * 6.218858242034912
Epoch 2270, val loss: 1.4255517721176147
Epoch 2280, training loss: 621.8311767578125 = 0.026923328638076782 + 100.0 * 6.218042850494385
Epoch 2280, val loss: 1.4294612407684326
Epoch 2290, training loss: 622.0664672851562 = 0.026518845930695534 + 100.0 * 6.220399856567383
Epoch 2290, val loss: 1.4326993227005005
Epoch 2300, training loss: 621.9254760742188 = 0.02611316181719303 + 100.0 * 6.218993663787842
Epoch 2300, val loss: 1.4358232021331787
Epoch 2310, training loss: 621.847412109375 = 0.025714416056871414 + 100.0 * 6.218217372894287
Epoch 2310, val loss: 1.4393442869186401
Epoch 2320, training loss: 622.021484375 = 0.025330521166324615 + 100.0 * 6.219961643218994
Epoch 2320, val loss: 1.4427917003631592
Epoch 2330, training loss: 621.7718505859375 = 0.024956591427326202 + 100.0 * 6.217468738555908
Epoch 2330, val loss: 1.4456502199172974
Epoch 2340, training loss: 621.9404296875 = 0.024594474583864212 + 100.0 * 6.219158172607422
Epoch 2340, val loss: 1.44882333278656
Epoch 2350, training loss: 622.0335083007812 = 0.02423514612019062 + 100.0 * 6.2200927734375
Epoch 2350, val loss: 1.4524106979370117
Epoch 2360, training loss: 621.9962768554688 = 0.023887086659669876 + 100.0 * 6.219724178314209
Epoch 2360, val loss: 1.4547425508499146
Epoch 2370, training loss: 621.7896728515625 = 0.023536110296845436 + 100.0 * 6.217660903930664
Epoch 2370, val loss: 1.4583356380462646
Epoch 2380, training loss: 621.6591796875 = 0.023199236020445824 + 100.0 * 6.216359615325928
Epoch 2380, val loss: 1.4617432355880737
Epoch 2390, training loss: 621.9190673828125 = 0.022879144176840782 + 100.0 * 6.218961715698242
Epoch 2390, val loss: 1.4651172161102295
Epoch 2400, training loss: 621.6875610351562 = 0.022555021569132805 + 100.0 * 6.216649532318115
Epoch 2400, val loss: 1.4682695865631104
Epoch 2410, training loss: 621.9456176757812 = 0.022244010120630264 + 100.0 * 6.219233989715576
Epoch 2410, val loss: 1.4717061519622803
Epoch 2420, training loss: 621.797607421875 = 0.0219277236610651 + 100.0 * 6.217757225036621
Epoch 2420, val loss: 1.473606824874878
Epoch 2430, training loss: 621.7323608398438 = 0.021617908030748367 + 100.0 * 6.217107772827148
Epoch 2430, val loss: 1.476830005645752
Epoch 2440, training loss: 621.5362548828125 = 0.021317586302757263 + 100.0 * 6.215149402618408
Epoch 2440, val loss: 1.4803282022476196
Epoch 2450, training loss: 621.544677734375 = 0.021029289811849594 + 100.0 * 6.215236186981201
Epoch 2450, val loss: 1.4833705425262451
Epoch 2460, training loss: 621.6538696289062 = 0.02075059525668621 + 100.0 * 6.2163310050964355
Epoch 2460, val loss: 1.4863431453704834
Epoch 2470, training loss: 621.929931640625 = 0.020474160090088844 + 100.0 * 6.219094753265381
Epoch 2470, val loss: 1.4888396263122559
Epoch 2480, training loss: 621.7612915039062 = 0.020198112353682518 + 100.0 * 6.217411041259766
Epoch 2480, val loss: 1.4918416738510132
Epoch 2490, training loss: 621.7801513671875 = 0.01993117295205593 + 100.0 * 6.217601776123047
Epoch 2490, val loss: 1.4945327043533325
Epoch 2500, training loss: 621.8357543945312 = 0.01966622658073902 + 100.0 * 6.218161106109619
Epoch 2500, val loss: 1.4980734586715698
Epoch 2510, training loss: 621.5344848632812 = 0.019392380490899086 + 100.0 * 6.215150833129883
Epoch 2510, val loss: 1.5004609823226929
Epoch 2520, training loss: 621.4541625976562 = 0.019144462421536446 + 100.0 * 6.214349746704102
Epoch 2520, val loss: 1.503173589706421
Epoch 2530, training loss: 621.4439697265625 = 0.018899589776992798 + 100.0 * 6.214250564575195
Epoch 2530, val loss: 1.5064513683319092
Epoch 2540, training loss: 621.8925170898438 = 0.018667886033654213 + 100.0 * 6.218738555908203
Epoch 2540, val loss: 1.5092225074768066
Epoch 2550, training loss: 621.4874877929688 = 0.018424589186906815 + 100.0 * 6.214690685272217
Epoch 2550, val loss: 1.5115686655044556
Epoch 2560, training loss: 621.4993896484375 = 0.018187550827860832 + 100.0 * 6.2148118019104
Epoch 2560, val loss: 1.5149810314178467
Epoch 2570, training loss: 621.4114379882812 = 0.017959458753466606 + 100.0 * 6.213934898376465
Epoch 2570, val loss: 1.5174897909164429
Epoch 2580, training loss: 621.747314453125 = 0.017743173986673355 + 100.0 * 6.2172956466674805
Epoch 2580, val loss: 1.5197396278381348
Epoch 2590, training loss: 621.520751953125 = 0.017506467178463936 + 100.0 * 6.215032577514648
Epoch 2590, val loss: 1.5219100713729858
Epoch 2600, training loss: 621.3433837890625 = 0.017287500202655792 + 100.0 * 6.213261127471924
Epoch 2600, val loss: 1.5245381593704224
Epoch 2610, training loss: 621.2787475585938 = 0.017076294869184494 + 100.0 * 6.212616443634033
Epoch 2610, val loss: 1.5277526378631592
Epoch 2620, training loss: 621.2396850585938 = 0.016874823719263077 + 100.0 * 6.212228298187256
Epoch 2620, val loss: 1.5301733016967773
Epoch 2630, training loss: 621.6580810546875 = 0.01668437570333481 + 100.0 * 6.216414451599121
Epoch 2630, val loss: 1.5329625606536865
Epoch 2640, training loss: 621.3668212890625 = 0.016471026465296745 + 100.0 * 6.213503360748291
Epoch 2640, val loss: 1.534677505493164
Epoch 2650, training loss: 621.3329467773438 = 0.01626787707209587 + 100.0 * 6.213166236877441
Epoch 2650, val loss: 1.5377670526504517
Epoch 2660, training loss: 621.4109497070312 = 0.016073357313871384 + 100.0 * 6.213948726654053
Epoch 2660, val loss: 1.5399746894836426
Epoch 2670, training loss: 621.6368408203125 = 0.015886440873146057 + 100.0 * 6.216209888458252
Epoch 2670, val loss: 1.5430034399032593
Epoch 2680, training loss: 621.1755981445312 = 0.01569589041173458 + 100.0 * 6.211599349975586
Epoch 2680, val loss: 1.5453290939331055
Epoch 2690, training loss: 621.21923828125 = 0.015519154258072376 + 100.0 * 6.212037086486816
Epoch 2690, val loss: 1.5477780103683472
Epoch 2700, training loss: 621.305419921875 = 0.015344256535172462 + 100.0 * 6.2129011154174805
Epoch 2700, val loss: 1.5505026578903198
Epoch 2710, training loss: 621.2719116210938 = 0.015169925056397915 + 100.0 * 6.212567329406738
Epoch 2710, val loss: 1.5528995990753174
Epoch 2720, training loss: 621.4083862304688 = 0.014995710924267769 + 100.0 * 6.21393346786499
Epoch 2720, val loss: 1.5550302267074585
Epoch 2730, training loss: 621.5690307617188 = 0.014821437187492847 + 100.0 * 6.215541839599609
Epoch 2730, val loss: 1.5573036670684814
Epoch 2740, training loss: 621.408447265625 = 0.014656346291303635 + 100.0 * 6.213937759399414
Epoch 2740, val loss: 1.559758186340332
Epoch 2750, training loss: 621.2833251953125 = 0.014485623687505722 + 100.0 * 6.212688446044922
Epoch 2750, val loss: 1.561853289604187
Epoch 2760, training loss: 621.5203857421875 = 0.014332747086882591 + 100.0 * 6.215060234069824
Epoch 2760, val loss: 1.564454436302185
Epoch 2770, training loss: 621.0841064453125 = 0.014168391935527325 + 100.0 * 6.210699558258057
Epoch 2770, val loss: 1.5659207105636597
Epoch 2780, training loss: 621.0301513671875 = 0.014014984481036663 + 100.0 * 6.210161209106445
Epoch 2780, val loss: 1.5682895183563232
Epoch 2790, training loss: 621.2049560546875 = 0.013869540765881538 + 100.0 * 6.211911201477051
Epoch 2790, val loss: 1.570422887802124
Epoch 2800, training loss: 621.36669921875 = 0.01372087374329567 + 100.0 * 6.213529586791992
Epoch 2800, val loss: 1.5721213817596436
Epoch 2810, training loss: 621.2718505859375 = 0.013568738475441933 + 100.0 * 6.212583065032959
Epoch 2810, val loss: 1.5750130414962769
Epoch 2820, training loss: 621.2322387695312 = 0.013424561358988285 + 100.0 * 6.212188243865967
Epoch 2820, val loss: 1.5770363807678223
Epoch 2830, training loss: 621.1204833984375 = 0.013280225917696953 + 100.0 * 6.211071491241455
Epoch 2830, val loss: 1.5798711776733398
Epoch 2840, training loss: 621.4030151367188 = 0.013144724071025848 + 100.0 * 6.213898658752441
Epoch 2840, val loss: 1.5815677642822266
Epoch 2850, training loss: 621.0968627929688 = 0.01300562359392643 + 100.0 * 6.210838794708252
Epoch 2850, val loss: 1.5835576057434082
Epoch 2860, training loss: 621.0464477539062 = 0.0128707280382514 + 100.0 * 6.210335731506348
Epoch 2860, val loss: 1.585902214050293
Epoch 2870, training loss: 621.0176391601562 = 0.012738027609884739 + 100.0 * 6.210048675537109
Epoch 2870, val loss: 1.588204026222229
Epoch 2880, training loss: 621.6053466796875 = 0.012612074613571167 + 100.0 * 6.2159271240234375
Epoch 2880, val loss: 1.5906819105148315
Epoch 2890, training loss: 621.2514038085938 = 0.012475385330617428 + 100.0 * 6.2123894691467285
Epoch 2890, val loss: 1.5911297798156738
Epoch 2900, training loss: 621.0059204101562 = 0.01234415639191866 + 100.0 * 6.209936141967773
Epoch 2900, val loss: 1.593868374824524
Epoch 2910, training loss: 620.867431640625 = 0.012220674194395542 + 100.0 * 6.208552360534668
Epoch 2910, val loss: 1.5957846641540527
Epoch 2920, training loss: 620.8780517578125 = 0.012105678208172321 + 100.0 * 6.208659648895264
Epoch 2920, val loss: 1.5980169773101807
Epoch 2930, training loss: 621.2880859375 = 0.01199788972735405 + 100.0 * 6.2127604484558105
Epoch 2930, val loss: 1.5998895168304443
Epoch 2940, training loss: 621.046142578125 = 0.011871644295752048 + 100.0 * 6.2103424072265625
Epoch 2940, val loss: 1.6014845371246338
Epoch 2950, training loss: 620.848388671875 = 0.011748303659260273 + 100.0 * 6.208366870880127
Epoch 2950, val loss: 1.6036101579666138
Epoch 2960, training loss: 621.1100463867188 = 0.01163634005934 + 100.0 * 6.210983753204346
Epoch 2960, val loss: 1.6053839921951294
Epoch 2970, training loss: 620.9927368164062 = 0.011522171087563038 + 100.0 * 6.209812164306641
Epoch 2970, val loss: 1.606798768043518
Epoch 2980, training loss: 621.0042724609375 = 0.01140961516648531 + 100.0 * 6.209928512573242
Epoch 2980, val loss: 1.6093487739562988
Epoch 2990, training loss: 620.888671875 = 0.011302096769213676 + 100.0 * 6.208773612976074
Epoch 2990, val loss: 1.6112852096557617
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.674074074074074
0.7949393779652083
The final CL Acc:0.67407, 0.00907, The final GNN Acc:0.79863, 0.00282
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13204])
remove edge: torch.Size([2, 7940])
updated graph: torch.Size([2, 10588])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6294555664062 = 1.9469037055969238 + 100.0 * 8.59682559967041
Epoch 0, val loss: 1.94340980052948
Epoch 10, training loss: 861.5243530273438 = 1.9386249780654907 + 100.0 * 8.595857620239258
Epoch 10, val loss: 1.9354137182235718
Epoch 20, training loss: 860.827880859375 = 1.9281084537506104 + 100.0 * 8.588997840881348
Epoch 20, val loss: 1.9248521327972412
Epoch 30, training loss: 856.1954345703125 = 1.9142861366271973 + 100.0 * 8.542811393737793
Epoch 30, val loss: 1.9106115102767944
Epoch 40, training loss: 827.8951416015625 = 1.8968935012817383 + 100.0 * 8.259982109069824
Epoch 40, val loss: 1.8928204774856567
Epoch 50, training loss: 754.146484375 = 1.877652883529663 + 100.0 * 7.522688865661621
Epoch 50, val loss: 1.873822808265686
Epoch 60, training loss: 725.6594848632812 = 1.8625376224517822 + 100.0 * 7.237969398498535
Epoch 60, val loss: 1.8597488403320312
Epoch 70, training loss: 704.9790649414062 = 1.8515284061431885 + 100.0 * 7.031275749206543
Epoch 70, val loss: 1.8494495153427124
Epoch 80, training loss: 693.7564697265625 = 1.839845895767212 + 100.0 * 6.919166088104248
Epoch 80, val loss: 1.8383363485336304
Epoch 90, training loss: 686.0026245117188 = 1.8293333053588867 + 100.0 * 6.841732501983643
Epoch 90, val loss: 1.8282729387283325
Epoch 100, training loss: 677.8285522460938 = 1.8189746141433716 + 100.0 * 6.760095596313477
Epoch 100, val loss: 1.818570613861084
Epoch 110, training loss: 671.2718505859375 = 1.8105679750442505 + 100.0 * 6.694612979888916
Epoch 110, val loss: 1.810401201248169
Epoch 120, training loss: 666.4488525390625 = 1.8020806312561035 + 100.0 * 6.646467685699463
Epoch 120, val loss: 1.8017876148223877
Epoch 130, training loss: 662.8543090820312 = 1.792657494544983 + 100.0 * 6.610616683959961
Epoch 130, val loss: 1.792425513267517
Epoch 140, training loss: 660.0068359375 = 1.7830760478973389 + 100.0 * 6.582237243652344
Epoch 140, val loss: 1.7830517292022705
Epoch 150, training loss: 657.945068359375 = 1.7732831239700317 + 100.0 * 6.561717987060547
Epoch 150, val loss: 1.7734190225601196
Epoch 160, training loss: 655.6891479492188 = 1.7629286050796509 + 100.0 * 6.539262294769287
Epoch 160, val loss: 1.7634912729263306
Epoch 170, training loss: 653.634521484375 = 1.752155065536499 + 100.0 * 6.518823623657227
Epoch 170, val loss: 1.7532545328140259
Epoch 180, training loss: 651.6905517578125 = 1.7407913208007812 + 100.0 * 6.499497890472412
Epoch 180, val loss: 1.742606520652771
Epoch 190, training loss: 649.76953125 = 1.7286869287490845 + 100.0 * 6.480408191680908
Epoch 190, val loss: 1.7313579320907593
Epoch 200, training loss: 648.3225708007812 = 1.7153304815292358 + 100.0 * 6.4660725593566895
Epoch 200, val loss: 1.7191542387008667
Epoch 210, training loss: 646.7142333984375 = 1.7006357908248901 + 100.0 * 6.450136184692383
Epoch 210, val loss: 1.7056294679641724
Epoch 220, training loss: 645.41064453125 = 1.6845707893371582 + 100.0 * 6.437260627746582
Epoch 220, val loss: 1.6908520460128784
Epoch 230, training loss: 644.989013671875 = 1.6670141220092773 + 100.0 * 6.433219909667969
Epoch 230, val loss: 1.674653172492981
Epoch 240, training loss: 643.5487060546875 = 1.6476216316223145 + 100.0 * 6.419010639190674
Epoch 240, val loss: 1.656995177268982
Epoch 250, training loss: 642.45068359375 = 1.626922845840454 + 100.0 * 6.408237457275391
Epoch 250, val loss: 1.6381384134292603
Epoch 260, training loss: 641.5952758789062 = 1.6048351526260376 + 100.0 * 6.399904251098633
Epoch 260, val loss: 1.6182039976119995
Epoch 270, training loss: 641.075927734375 = 1.5812832117080688 + 100.0 * 6.394946575164795
Epoch 270, val loss: 1.5971734523773193
Epoch 280, training loss: 640.119140625 = 1.556570053100586 + 100.0 * 6.385625839233398
Epoch 280, val loss: 1.5751558542251587
Epoch 290, training loss: 639.3037719726562 = 1.5307904481887817 + 100.0 * 6.377730369567871
Epoch 290, val loss: 1.5524555444717407
Epoch 300, training loss: 639.1791381835938 = 1.5041875839233398 + 100.0 * 6.376749515533447
Epoch 300, val loss: 1.5291513204574585
Epoch 310, training loss: 638.0896606445312 = 1.4766565561294556 + 100.0 * 6.3661298751831055
Epoch 310, val loss: 1.5054707527160645
Epoch 320, training loss: 637.4725341796875 = 1.4488166570663452 + 100.0 * 6.3602375984191895
Epoch 320, val loss: 1.481683611869812
Epoch 330, training loss: 637.0238647460938 = 1.4206764698028564 + 100.0 * 6.356031894683838
Epoch 330, val loss: 1.4579079151153564
Epoch 340, training loss: 636.6292724609375 = 1.3923875093460083 + 100.0 * 6.3523688316345215
Epoch 340, val loss: 1.434245228767395
Epoch 350, training loss: 636.0934448242188 = 1.3640788793563843 + 100.0 * 6.347293853759766
Epoch 350, val loss: 1.4106389284133911
Epoch 360, training loss: 635.4556884765625 = 1.3358206748962402 + 100.0 * 6.341198921203613
Epoch 360, val loss: 1.3874601125717163
Epoch 370, training loss: 634.9566650390625 = 1.3078685998916626 + 100.0 * 6.336487770080566
Epoch 370, val loss: 1.3647668361663818
Epoch 380, training loss: 634.55517578125 = 1.2802625894546509 + 100.0 * 6.332748889923096
Epoch 380, val loss: 1.3425586223602295
Epoch 390, training loss: 634.4630126953125 = 1.252909779548645 + 100.0 * 6.332100868225098
Epoch 390, val loss: 1.3207281827926636
Epoch 400, training loss: 634.0360107421875 = 1.2258543968200684 + 100.0 * 6.328101634979248
Epoch 400, val loss: 1.2993649244308472
Epoch 410, training loss: 633.609619140625 = 1.199223518371582 + 100.0 * 6.324103832244873
Epoch 410, val loss: 1.2784478664398193
Epoch 420, training loss: 633.1689453125 = 1.1729917526245117 + 100.0 * 6.31995964050293
Epoch 420, val loss: 1.2581791877746582
Epoch 430, training loss: 632.7686157226562 = 1.1473606824874878 + 100.0 * 6.3162126541137695
Epoch 430, val loss: 1.238372564315796
Epoch 440, training loss: 632.83837890625 = 1.1222259998321533 + 100.0 * 6.3171610832214355
Epoch 440, val loss: 1.219198226928711
Epoch 450, training loss: 632.4422607421875 = 1.0972328186035156 + 100.0 * 6.313450336456299
Epoch 450, val loss: 1.2004003524780273
Epoch 460, training loss: 632.2217407226562 = 1.0729700326919556 + 100.0 * 6.311487197875977
Epoch 460, val loss: 1.1822354793548584
Epoch 470, training loss: 631.84228515625 = 1.0491176843643188 + 100.0 * 6.307931423187256
Epoch 470, val loss: 1.1646357774734497
Epoch 480, training loss: 631.4435424804688 = 1.0258911848068237 + 100.0 * 6.3041768074035645
Epoch 480, val loss: 1.1477108001708984
Epoch 490, training loss: 631.0536499023438 = 1.0033035278320312 + 100.0 * 6.300503253936768
Epoch 490, val loss: 1.1315248012542725
Epoch 500, training loss: 630.832275390625 = 0.981367826461792 + 100.0 * 6.298509120941162
Epoch 500, val loss: 1.1159459352493286
Epoch 510, training loss: 630.89404296875 = 0.9600732922554016 + 100.0 * 6.299339294433594
Epoch 510, val loss: 1.101130485534668
Epoch 520, training loss: 631.0990600585938 = 0.9391352534294128 + 100.0 * 6.301599025726318
Epoch 520, val loss: 1.0865782499313354
Epoch 530, training loss: 630.350830078125 = 0.9190205335617065 + 100.0 * 6.294318199157715
Epoch 530, val loss: 1.0730758905410767
Epoch 540, training loss: 629.9818725585938 = 0.8995230793952942 + 100.0 * 6.290823459625244
Epoch 540, val loss: 1.060089111328125
Epoch 550, training loss: 629.9472045898438 = 0.8807021975517273 + 100.0 * 6.290665149688721
Epoch 550, val loss: 1.0478014945983887
Epoch 560, training loss: 630.02734375 = 0.8624542951583862 + 100.0 * 6.291648864746094
Epoch 560, val loss: 1.0362075567245483
Epoch 570, training loss: 629.5220947265625 = 0.8446295857429504 + 100.0 * 6.2867751121521
Epoch 570, val loss: 1.0249322652816772
Epoch 580, training loss: 629.4915771484375 = 0.8274907469749451 + 100.0 * 6.286640644073486
Epoch 580, val loss: 1.01449716091156
Epoch 590, training loss: 629.0801391601562 = 0.8109432458877563 + 100.0 * 6.2826924324035645
Epoch 590, val loss: 1.0047738552093506
Epoch 600, training loss: 628.9869384765625 = 0.7949113249778748 + 100.0 * 6.281920433044434
Epoch 600, val loss: 0.9955946207046509
Epoch 610, training loss: 629.0010986328125 = 0.7792858481407166 + 100.0 * 6.282217979431152
Epoch 610, val loss: 0.9868344664573669
Epoch 620, training loss: 628.8975830078125 = 0.7639715075492859 + 100.0 * 6.281335830688477
Epoch 620, val loss: 0.9783893823623657
Epoch 630, training loss: 628.6581420898438 = 0.7490857243537903 + 100.0 * 6.279090404510498
Epoch 630, val loss: 0.9706017374992371
Epoch 640, training loss: 628.5073852539062 = 0.7345712780952454 + 100.0 * 6.27772855758667
Epoch 640, val loss: 0.9629314541816711
Epoch 650, training loss: 628.181884765625 = 0.720440685749054 + 100.0 * 6.274614334106445
Epoch 650, val loss: 0.9560765027999878
Epoch 660, training loss: 628.7220458984375 = 0.7065563797950745 + 100.0 * 6.280155181884766
Epoch 660, val loss: 0.9493327736854553
Epoch 670, training loss: 628.53369140625 = 0.6930346488952637 + 100.0 * 6.278406620025635
Epoch 670, val loss: 0.9431167244911194
Epoch 680, training loss: 627.8543090820312 = 0.6796444654464722 + 100.0 * 6.271746635437012
Epoch 680, val loss: 0.9371795654296875
Epoch 690, training loss: 627.7022705078125 = 0.6666222214698792 + 100.0 * 6.270356178283691
Epoch 690, val loss: 0.9315870404243469
Epoch 700, training loss: 628.018798828125 = 0.6539421081542969 + 100.0 * 6.273648738861084
Epoch 700, val loss: 0.9265950918197632
Epoch 710, training loss: 627.7306518554688 = 0.6412617564201355 + 100.0 * 6.2708940505981445
Epoch 710, val loss: 0.9215751886367798
Epoch 720, training loss: 627.4371337890625 = 0.6289070248603821 + 100.0 * 6.268082141876221
Epoch 720, val loss: 0.9169731140136719
Epoch 730, training loss: 627.3761596679688 = 0.6167494058609009 + 100.0 * 6.267593860626221
Epoch 730, val loss: 0.9127767086029053
Epoch 740, training loss: 627.2147216796875 = 0.6047905087471008 + 100.0 * 6.266099452972412
Epoch 740, val loss: 0.9087517857551575
Epoch 750, training loss: 627.1195068359375 = 0.5930566787719727 + 100.0 * 6.26526403427124
Epoch 750, val loss: 0.9051308035850525
Epoch 760, training loss: 627.1142578125 = 0.5815058350563049 + 100.0 * 6.2653279304504395
Epoch 760, val loss: 0.9018875956535339
Epoch 770, training loss: 626.8675537109375 = 0.570036768913269 + 100.0 * 6.262975215911865
Epoch 770, val loss: 0.8984365463256836
Epoch 780, training loss: 626.98974609375 = 0.5587829351425171 + 100.0 * 6.264309883117676
Epoch 780, val loss: 0.8956588506698608
Epoch 790, training loss: 626.6802978515625 = 0.5476590394973755 + 100.0 * 6.261326313018799
Epoch 790, val loss: 0.8930030465126038
Epoch 800, training loss: 626.5651245117188 = 0.5367512106895447 + 100.0 * 6.260283946990967
Epoch 800, val loss: 0.8906029462814331
Epoch 810, training loss: 626.3880615234375 = 0.5260571837425232 + 100.0 * 6.258620262145996
Epoch 810, val loss: 0.8886200785636902
Epoch 820, training loss: 626.6571655273438 = 0.5155073404312134 + 100.0 * 6.261416435241699
Epoch 820, val loss: 0.8867436051368713
Epoch 830, training loss: 627.3280639648438 = 0.5049267411231995 + 100.0 * 6.268231391906738
Epoch 830, val loss: 0.8851273059844971
Epoch 840, training loss: 626.2405395507812 = 0.4946220815181732 + 100.0 * 6.2574591636657715
Epoch 840, val loss: 0.8835881352424622
Epoch 850, training loss: 626.005126953125 = 0.48447558283805847 + 100.0 * 6.25520658493042
Epoch 850, val loss: 0.8825642466545105
Epoch 860, training loss: 625.9669799804688 = 0.474556565284729 + 100.0 * 6.254924297332764
Epoch 860, val loss: 0.8818106651306152
Epoch 870, training loss: 627.2283935546875 = 0.4647657573223114 + 100.0 * 6.267635822296143
Epoch 870, val loss: 0.8811022639274597
Epoch 880, training loss: 625.8775634765625 = 0.4549964666366577 + 100.0 * 6.254225730895996
Epoch 880, val loss: 0.880339503288269
Epoch 890, training loss: 625.8038330078125 = 0.44540634751319885 + 100.0 * 6.253583908081055
Epoch 890, val loss: 0.8801275491714478
Epoch 900, training loss: 625.5858764648438 = 0.43604499101638794 + 100.0 * 6.251498699188232
Epoch 900, val loss: 0.8799975514411926
Epoch 910, training loss: 625.6163940429688 = 0.42683541774749756 + 100.0 * 6.251895904541016
Epoch 910, val loss: 0.8800626397132874
Epoch 920, training loss: 625.6765747070312 = 0.4176819920539856 + 100.0 * 6.252589225769043
Epoch 920, val loss: 0.8802080154418945
Epoch 930, training loss: 625.4264526367188 = 0.4085654020309448 + 100.0 * 6.250178813934326
Epoch 930, val loss: 0.880286455154419
Epoch 940, training loss: 625.6685180664062 = 0.3996661305427551 + 100.0 * 6.252688884735107
Epoch 940, val loss: 0.8806213736534119
Epoch 950, training loss: 625.3704223632812 = 0.3909236490726471 + 100.0 * 6.249794960021973
Epoch 950, val loss: 0.8813014030456543
Epoch 960, training loss: 625.1478271484375 = 0.3822576105594635 + 100.0 * 6.247655868530273
Epoch 960, val loss: 0.8819313049316406
Epoch 970, training loss: 625.0661010742188 = 0.3737897574901581 + 100.0 * 6.246922969818115
Epoch 970, val loss: 0.8828160166740417
Epoch 980, training loss: 625.7549438476562 = 0.36547666788101196 + 100.0 * 6.253894805908203
Epoch 980, val loss: 0.8838638067245483
Epoch 990, training loss: 625.3184814453125 = 0.3571809232234955 + 100.0 * 6.249613285064697
Epoch 990, val loss: 0.8846616744995117
Epoch 1000, training loss: 624.835693359375 = 0.34902235865592957 + 100.0 * 6.244866847991943
Epoch 1000, val loss: 0.885906994342804
Epoch 1010, training loss: 624.7750244140625 = 0.34107735753059387 + 100.0 * 6.244339466094971
Epoch 1010, val loss: 0.8870010375976562
Epoch 1020, training loss: 625.2130737304688 = 0.3332693874835968 + 100.0 * 6.248798370361328
Epoch 1020, val loss: 0.8882530927658081
Epoch 1030, training loss: 624.8043823242188 = 0.3255465030670166 + 100.0 * 6.24478816986084
Epoch 1030, val loss: 0.8898983597755432
Epoch 1040, training loss: 624.8602294921875 = 0.3179042637348175 + 100.0 * 6.245422840118408
Epoch 1040, val loss: 0.891169011592865
Epoch 1050, training loss: 624.7728271484375 = 0.3104809820652008 + 100.0 * 6.24462366104126
Epoch 1050, val loss: 0.8929451107978821
Epoch 1060, training loss: 624.513671875 = 0.3031753599643707 + 100.0 * 6.242105007171631
Epoch 1060, val loss: 0.8947039246559143
Epoch 1070, training loss: 624.4319458007812 = 0.29604190587997437 + 100.0 * 6.241359233856201
Epoch 1070, val loss: 0.8966262936592102
Epoch 1080, training loss: 624.570068359375 = 0.28902384638786316 + 100.0 * 6.2428107261657715
Epoch 1080, val loss: 0.8984750509262085
Epoch 1090, training loss: 624.4171752929688 = 0.28212758898735046 + 100.0 * 6.2413506507873535
Epoch 1090, val loss: 0.9005909562110901
Epoch 1100, training loss: 624.543212890625 = 0.2753332555294037 + 100.0 * 6.242679119110107
Epoch 1100, val loss: 0.9023849964141846
Epoch 1110, training loss: 624.3280029296875 = 0.2686673104763031 + 100.0 * 6.240593433380127
Epoch 1110, val loss: 0.9045122265815735
Epoch 1120, training loss: 624.1705932617188 = 0.26215222477912903 + 100.0 * 6.239084720611572
Epoch 1120, val loss: 0.9067760705947876
Epoch 1130, training loss: 624.0765380859375 = 0.2558270990848541 + 100.0 * 6.2382073402404785
Epoch 1130, val loss: 0.9092245697975159
Epoch 1140, training loss: 624.5458374023438 = 0.2496584951877594 + 100.0 * 6.242961883544922
Epoch 1140, val loss: 0.9117606282234192
Epoch 1150, training loss: 624.208251953125 = 0.24350304901599884 + 100.0 * 6.239647388458252
Epoch 1150, val loss: 0.9138338565826416
Epoch 1160, training loss: 624.2244873046875 = 0.2375442236661911 + 100.0 * 6.239869117736816
Epoch 1160, val loss: 0.9165278673171997
Epoch 1170, training loss: 623.8154907226562 = 0.2316514104604721 + 100.0 * 6.235838890075684
Epoch 1170, val loss: 0.9188241362571716
Epoch 1180, training loss: 623.8496704101562 = 0.22596289217472076 + 100.0 * 6.236237049102783
Epoch 1180, val loss: 0.9215039610862732
Epoch 1190, training loss: 623.9237060546875 = 0.220407634973526 + 100.0 * 6.237033367156982
Epoch 1190, val loss: 0.9242983460426331
Epoch 1200, training loss: 623.9251708984375 = 0.2149839699268341 + 100.0 * 6.237102031707764
Epoch 1200, val loss: 0.9271076321601868
Epoch 1210, training loss: 623.6834716796875 = 0.20964299142360687 + 100.0 * 6.234737873077393
Epoch 1210, val loss: 0.9300066828727722
Epoch 1220, training loss: 623.7870483398438 = 0.20444993674755096 + 100.0 * 6.235826015472412
Epoch 1220, val loss: 0.9330753684043884
Epoch 1230, training loss: 623.7841186523438 = 0.19940537214279175 + 100.0 * 6.235847473144531
Epoch 1230, val loss: 0.9359363913536072
Epoch 1240, training loss: 623.5556030273438 = 0.1944425106048584 + 100.0 * 6.233611583709717
Epoch 1240, val loss: 0.9387108087539673
Epoch 1250, training loss: 623.591796875 = 0.18964070081710815 + 100.0 * 6.2340216636657715
Epoch 1250, val loss: 0.9418214559555054
Epoch 1260, training loss: 623.6130981445312 = 0.18497060239315033 + 100.0 * 6.234281063079834
Epoch 1260, val loss: 0.9451638460159302
Epoch 1270, training loss: 623.4342651367188 = 0.1804323047399521 + 100.0 * 6.23253870010376
Epoch 1270, val loss: 0.9485690593719482
Epoch 1280, training loss: 623.3378295898438 = 0.1759698987007141 + 100.0 * 6.231618404388428
Epoch 1280, val loss: 0.9518100023269653
Epoch 1290, training loss: 623.5789184570312 = 0.17165309190750122 + 100.0 * 6.234073162078857
Epoch 1290, val loss: 0.9549681544303894
Epoch 1300, training loss: 623.6973876953125 = 0.16743315756320953 + 100.0 * 6.235299587249756
Epoch 1300, val loss: 0.9585657119750977
Epoch 1310, training loss: 623.3999633789062 = 0.16329342126846313 + 100.0 * 6.232367038726807
Epoch 1310, val loss: 0.9619243741035461
Epoch 1320, training loss: 623.1315307617188 = 0.15929871797561646 + 100.0 * 6.229722023010254
Epoch 1320, val loss: 0.9655265808105469
Epoch 1330, training loss: 623.0180053710938 = 0.15544281899929047 + 100.0 * 6.228625297546387
Epoch 1330, val loss: 0.9690303206443787
Epoch 1340, training loss: 623.4569091796875 = 0.1516890525817871 + 100.0 * 6.2330522537231445
Epoch 1340, val loss: 0.9724157452583313
Epoch 1350, training loss: 623.1583251953125 = 0.14801755547523499 + 100.0 * 6.230103015899658
Epoch 1350, val loss: 0.9762743711471558
Epoch 1360, training loss: 623.33740234375 = 0.14442114531993866 + 100.0 * 6.231929779052734
Epoch 1360, val loss: 0.9798828959465027
Epoch 1370, training loss: 622.9178466796875 = 0.1409512311220169 + 100.0 * 6.227768898010254
Epoch 1370, val loss: 0.9837762713432312
Epoch 1380, training loss: 622.8136596679688 = 0.13758932054042816 + 100.0 * 6.2267608642578125
Epoch 1380, val loss: 0.9875003099441528
Epoch 1390, training loss: 622.906005859375 = 0.13433738052845 + 100.0 * 6.227716445922852
Epoch 1390, val loss: 0.9913316369056702
Epoch 1400, training loss: 622.9845581054688 = 0.13115471601486206 + 100.0 * 6.228533744812012
Epoch 1400, val loss: 0.9950310587882996
Epoch 1410, training loss: 623.1322021484375 = 0.128075510263443 + 100.0 * 6.23004150390625
Epoch 1410, val loss: 0.9989282488822937
Epoch 1420, training loss: 622.933837890625 = 0.1250392198562622 + 100.0 * 6.228087902069092
Epoch 1420, val loss: 1.0030874013900757
Epoch 1430, training loss: 622.7152099609375 = 0.12210528552532196 + 100.0 * 6.225930690765381
Epoch 1430, val loss: 1.0069209337234497
Epoch 1440, training loss: 622.6832275390625 = 0.11926541477441788 + 100.0 * 6.225639343261719
Epoch 1440, val loss: 1.0109891891479492
Epoch 1450, training loss: 623.06103515625 = 0.11651333421468735 + 100.0 * 6.229444980621338
Epoch 1450, val loss: 1.0148741006851196
Epoch 1460, training loss: 622.6445922851562 = 0.11381233483552933 + 100.0 * 6.225307464599609
Epoch 1460, val loss: 1.0192891359329224
Epoch 1470, training loss: 622.472412109375 = 0.11120381951332092 + 100.0 * 6.223612308502197
Epoch 1470, val loss: 1.0232112407684326
Epoch 1480, training loss: 622.4285278320312 = 0.10867955535650253 + 100.0 * 6.223198413848877
Epoch 1480, val loss: 1.0275284051895142
Epoch 1490, training loss: 623.6820678710938 = 0.10624266415834427 + 100.0 * 6.235758304595947
Epoch 1490, val loss: 1.031760573387146
Epoch 1500, training loss: 622.7860717773438 = 0.10375114530324936 + 100.0 * 6.226822853088379
Epoch 1500, val loss: 1.0355440378189087
Epoch 1510, training loss: 622.4444580078125 = 0.10138841718435287 + 100.0 * 6.223430633544922
Epoch 1510, val loss: 1.039616584777832
Epoch 1520, training loss: 622.388427734375 = 0.09913158416748047 + 100.0 * 6.222892761230469
Epoch 1520, val loss: 1.0441060066223145
Epoch 1530, training loss: 622.4423217773438 = 0.09692276269197464 + 100.0 * 6.223453998565674
Epoch 1530, val loss: 1.0482028722763062
Epoch 1540, training loss: 622.3648681640625 = 0.09477508068084717 + 100.0 * 6.222701072692871
Epoch 1540, val loss: 1.0523850917816162
Epoch 1550, training loss: 622.5066528320312 = 0.09270283579826355 + 100.0 * 6.22413969039917
Epoch 1550, val loss: 1.0569106340408325
Epoch 1560, training loss: 622.3704833984375 = 0.09063799679279327 + 100.0 * 6.2227983474731445
Epoch 1560, val loss: 1.0607770681381226
Epoch 1570, training loss: 622.2279663085938 = 0.08866102993488312 + 100.0 * 6.221392631530762
Epoch 1570, val loss: 1.0652470588684082
Epoch 1580, training loss: 622.28271484375 = 0.08673662692308426 + 100.0 * 6.221959590911865
Epoch 1580, val loss: 1.069405198097229
Epoch 1590, training loss: 622.1146850585938 = 0.08486029505729675 + 100.0 * 6.2202982902526855
Epoch 1590, val loss: 1.073636531829834
Epoch 1600, training loss: 622.2168579101562 = 0.0830545425415039 + 100.0 * 6.221337795257568
Epoch 1600, val loss: 1.0780041217803955
Epoch 1610, training loss: 622.1354370117188 = 0.08127791434526443 + 100.0 * 6.220541477203369
Epoch 1610, val loss: 1.0820554494857788
Epoch 1620, training loss: 622.2026977539062 = 0.07956531643867493 + 100.0 * 6.221230983734131
Epoch 1620, val loss: 1.0863776206970215
Epoch 1630, training loss: 622.3914794921875 = 0.07788718491792679 + 100.0 * 6.223135948181152
Epoch 1630, val loss: 1.0907641649246216
Epoch 1640, training loss: 622.4639282226562 = 0.0762457326054573 + 100.0 * 6.223876953125
Epoch 1640, val loss: 1.094706654548645
Epoch 1650, training loss: 622.0405883789062 = 0.07463943958282471 + 100.0 * 6.219659328460693
Epoch 1650, val loss: 1.0988502502441406
Epoch 1660, training loss: 622.0232543945312 = 0.07308528572320938 + 100.0 * 6.219501972198486
Epoch 1660, val loss: 1.1029762029647827
Epoch 1670, training loss: 622.1658325195312 = 0.07157835364341736 + 100.0 * 6.220942497253418
Epoch 1670, val loss: 1.1073384284973145
Epoch 1680, training loss: 621.7808837890625 = 0.07011878490447998 + 100.0 * 6.217107772827148
Epoch 1680, val loss: 1.1115212440490723
Epoch 1690, training loss: 621.8915405273438 = 0.06869987398386002 + 100.0 * 6.218227863311768
Epoch 1690, val loss: 1.1155834197998047
Epoch 1700, training loss: 622.0902099609375 = 0.06731684505939484 + 100.0 * 6.220229148864746
Epoch 1700, val loss: 1.119650959968567
Epoch 1710, training loss: 622.1135864257812 = 0.06594645231962204 + 100.0 * 6.2204766273498535
Epoch 1710, val loss: 1.1238447427749634
Epoch 1720, training loss: 622.0420532226562 = 0.06462621688842773 + 100.0 * 6.21977424621582
Epoch 1720, val loss: 1.1276795864105225
Epoch 1730, training loss: 621.8372192382812 = 0.0633266270160675 + 100.0 * 6.217738628387451
Epoch 1730, val loss: 1.1320809125900269
Epoch 1740, training loss: 622.0638427734375 = 0.062090903520584106 + 100.0 * 6.220017910003662
Epoch 1740, val loss: 1.1363625526428223
Epoch 1750, training loss: 621.6621704101562 = 0.06084379553794861 + 100.0 * 6.216012954711914
Epoch 1750, val loss: 1.139945149421692
Epoch 1760, training loss: 621.6317138671875 = 0.0596560537815094 + 100.0 * 6.2157206535339355
Epoch 1760, val loss: 1.1441854238510132
Epoch 1770, training loss: 621.6200561523438 = 0.058505382388830185 + 100.0 * 6.215615272521973
Epoch 1770, val loss: 1.1480265855789185
Epoch 1780, training loss: 622.1342163085938 = 0.05738619714975357 + 100.0 * 6.220768451690674
Epoch 1780, val loss: 1.1520129442214966
Epoch 1790, training loss: 621.8638916015625 = 0.05627824366092682 + 100.0 * 6.218076229095459
Epoch 1790, val loss: 1.1560441255569458
Epoch 1800, training loss: 621.6542358398438 = 0.055179350078105927 + 100.0 * 6.2159905433654785
Epoch 1800, val loss: 1.1598681211471558
Epoch 1810, training loss: 621.8859252929688 = 0.05413118377327919 + 100.0 * 6.218317985534668
Epoch 1810, val loss: 1.1638325452804565
Epoch 1820, training loss: 621.485107421875 = 0.05310635641217232 + 100.0 * 6.214320182800293
Epoch 1820, val loss: 1.1676592826843262
Epoch 1830, training loss: 621.5426025390625 = 0.05211891233921051 + 100.0 * 6.21490478515625
Epoch 1830, val loss: 1.171467900276184
Epoch 1840, training loss: 621.9803466796875 = 0.05114664509892464 + 100.0 * 6.219292163848877
Epoch 1840, val loss: 1.174993634223938
Epoch 1850, training loss: 621.7520751953125 = 0.05019726976752281 + 100.0 * 6.217019081115723
Epoch 1850, val loss: 1.1794917583465576
Epoch 1860, training loss: 621.8143310546875 = 0.04925001040101051 + 100.0 * 6.217650890350342
Epoch 1860, val loss: 1.1828701496124268
Epoch 1870, training loss: 621.527587890625 = 0.04835348203778267 + 100.0 * 6.214792251586914
Epoch 1870, val loss: 1.1869279146194458
Epoch 1880, training loss: 621.3248901367188 = 0.04746523126959801 + 100.0 * 6.21277379989624
Epoch 1880, val loss: 1.1905931234359741
Epoch 1890, training loss: 621.2861328125 = 0.046617746353149414 + 100.0 * 6.212395191192627
Epoch 1890, val loss: 1.1944665908813477
Epoch 1900, training loss: 621.6875 = 0.045787520706653595 + 100.0 * 6.21641731262207
Epoch 1900, val loss: 1.1980427503585815
Epoch 1910, training loss: 621.5098266601562 = 0.044963445514440536 + 100.0 * 6.214648246765137
Epoch 1910, val loss: 1.2016795873641968
Epoch 1920, training loss: 621.2942504882812 = 0.04414192959666252 + 100.0 * 6.212501049041748
Epoch 1920, val loss: 1.2052407264709473
Epoch 1930, training loss: 621.3120727539062 = 0.043366558849811554 + 100.0 * 6.212687015533447
Epoch 1930, val loss: 1.2090946435928345
Epoch 1940, training loss: 621.4891357421875 = 0.04260909557342529 + 100.0 * 6.214465618133545
Epoch 1940, val loss: 1.2127227783203125
Epoch 1950, training loss: 621.3512573242188 = 0.04186668619513512 + 100.0 * 6.2130937576293945
Epoch 1950, val loss: 1.216238260269165
Epoch 1960, training loss: 621.4203491210938 = 0.041127774864435196 + 100.0 * 6.213791847229004
Epoch 1960, val loss: 1.2198960781097412
Epoch 1970, training loss: 621.2642211914062 = 0.0404195711016655 + 100.0 * 6.212238311767578
Epoch 1970, val loss: 1.2235136032104492
Epoch 1980, training loss: 621.3931884765625 = 0.039722781628370285 + 100.0 * 6.213534355163574
Epoch 1980, val loss: 1.2271391153335571
Epoch 1990, training loss: 621.3102416992188 = 0.03903266414999962 + 100.0 * 6.212711811065674
Epoch 1990, val loss: 1.2303622961044312
Epoch 2000, training loss: 621.32470703125 = 0.03837169334292412 + 100.0 * 6.212862968444824
Epoch 2000, val loss: 1.2340441942214966
Epoch 2010, training loss: 621.079833984375 = 0.03772098943591118 + 100.0 * 6.210421085357666
Epoch 2010, val loss: 1.237547516822815
Epoch 2020, training loss: 621.1782836914062 = 0.03709064796566963 + 100.0 * 6.211411952972412
Epoch 2020, val loss: 1.2411766052246094
Epoch 2030, training loss: 621.1213989257812 = 0.036475807428359985 + 100.0 * 6.210849285125732
Epoch 2030, val loss: 1.2445815801620483
Epoch 2040, training loss: 621.5088500976562 = 0.03587817773222923 + 100.0 * 6.2147297859191895
Epoch 2040, val loss: 1.2482531070709229
Epoch 2050, training loss: 621.2215576171875 = 0.03526894748210907 + 100.0 * 6.211862564086914
Epoch 2050, val loss: 1.2510952949523926
Epoch 2060, training loss: 621.1074829101562 = 0.03468278422951698 + 100.0 * 6.210728168487549
Epoch 2060, val loss: 1.2550177574157715
Epoch 2070, training loss: 621.0370483398438 = 0.034119073301553726 + 100.0 * 6.210029125213623
Epoch 2070, val loss: 1.257973074913025
Epoch 2080, training loss: 621.3529052734375 = 0.033569689840078354 + 100.0 * 6.213193416595459
Epoch 2080, val loss: 1.2614647150039673
Epoch 2090, training loss: 621.03076171875 = 0.03302077576518059 + 100.0 * 6.209977626800537
Epoch 2090, val loss: 1.2646255493164062
Epoch 2100, training loss: 621.05078125 = 0.03248868137598038 + 100.0 * 6.210183143615723
Epoch 2100, val loss: 1.2681995630264282
Epoch 2110, training loss: 621.0801391601562 = 0.03197084367275238 + 100.0 * 6.210481643676758
Epoch 2110, val loss: 1.2714593410491943
Epoch 2120, training loss: 620.921630859375 = 0.03146878629922867 + 100.0 * 6.208901405334473
Epoch 2120, val loss: 1.2749183177947998
Epoch 2130, training loss: 621.0775756835938 = 0.03098178282380104 + 100.0 * 6.210465908050537
Epoch 2130, val loss: 1.2780729532241821
Epoch 2140, training loss: 621.0075073242188 = 0.03049326129257679 + 100.0 * 6.2097697257995605
Epoch 2140, val loss: 1.2813578844070435
Epoch 2150, training loss: 621.0709838867188 = 0.030009141191840172 + 100.0 * 6.210409641265869
Epoch 2150, val loss: 1.2844064235687256
Epoch 2160, training loss: 621.02001953125 = 0.029543843120336533 + 100.0 * 6.209904670715332
Epoch 2160, val loss: 1.287516474723816
Epoch 2170, training loss: 620.9599609375 = 0.02908741496503353 + 100.0 * 6.209308624267578
Epoch 2170, val loss: 1.2905139923095703
Epoch 2180, training loss: 621.11572265625 = 0.028644753620028496 + 100.0 * 6.21087121963501
Epoch 2180, val loss: 1.2938684225082397
Epoch 2190, training loss: 620.7808837890625 = 0.028202340006828308 + 100.0 * 6.207527160644531
Epoch 2190, val loss: 1.2967982292175293
Epoch 2200, training loss: 620.7496337890625 = 0.027778897434473038 + 100.0 * 6.207218647003174
Epoch 2200, val loss: 1.3004294633865356
Epoch 2210, training loss: 620.6656494140625 = 0.027366459369659424 + 100.0 * 6.206383228302002
Epoch 2210, val loss: 1.303381323814392
Epoch 2220, training loss: 621.0253295898438 = 0.02697037346661091 + 100.0 * 6.2099833488464355
Epoch 2220, val loss: 1.3063329458236694
Epoch 2230, training loss: 620.9427490234375 = 0.026565879583358765 + 100.0 * 6.209161758422852
Epoch 2230, val loss: 1.3096095323562622
Epoch 2240, training loss: 620.7728271484375 = 0.026164349168539047 + 100.0 * 6.2074666023254395
Epoch 2240, val loss: 1.3122848272323608
Epoch 2250, training loss: 620.8587036132812 = 0.02577308937907219 + 100.0 * 6.208329200744629
Epoch 2250, val loss: 1.3156219720840454
Epoch 2260, training loss: 620.869873046875 = 0.025392305105924606 + 100.0 * 6.208444595336914
Epoch 2260, val loss: 1.3182834386825562
Epoch 2270, training loss: 620.6456909179688 = 0.025022441521286964 + 100.0 * 6.206206798553467
Epoch 2270, val loss: 1.3212605714797974
Epoch 2280, training loss: 620.6798706054688 = 0.024667056277394295 + 100.0 * 6.206552028656006
Epoch 2280, val loss: 1.3244587182998657
Epoch 2290, training loss: 620.8676147460938 = 0.024316420778632164 + 100.0 * 6.208433151245117
Epoch 2290, val loss: 1.3270899057388306
Epoch 2300, training loss: 620.724365234375 = 0.02397385984659195 + 100.0 * 6.207003593444824
Epoch 2300, val loss: 1.3302569389343262
Epoch 2310, training loss: 620.7974853515625 = 0.023630477488040924 + 100.0 * 6.207738876342773
Epoch 2310, val loss: 1.3333665132522583
Epoch 2320, training loss: 620.6298217773438 = 0.023292675614356995 + 100.0 * 6.2060651779174805
Epoch 2320, val loss: 1.335927963256836
Epoch 2330, training loss: 620.5642700195312 = 0.022966904565691948 + 100.0 * 6.205413341522217
Epoch 2330, val loss: 1.3390973806381226
Epoch 2340, training loss: 621.1905517578125 = 0.022645128890872 + 100.0 * 6.211678981781006
Epoch 2340, val loss: 1.341679573059082
Epoch 2350, training loss: 620.710693359375 = 0.02233227901160717 + 100.0 * 6.206883430480957
Epoch 2350, val loss: 1.3443307876586914
Epoch 2360, training loss: 620.4911499023438 = 0.022022610530257225 + 100.0 * 6.204691410064697
Epoch 2360, val loss: 1.3476060628890991
Epoch 2370, training loss: 620.498291015625 = 0.02172178216278553 + 100.0 * 6.204765796661377
Epoch 2370, val loss: 1.350283145904541
Epoch 2380, training loss: 620.8485717773438 = 0.021435733884572983 + 100.0 * 6.208271503448486
Epoch 2380, val loss: 1.353150725364685
Epoch 2390, training loss: 620.4537963867188 = 0.02113555185496807 + 100.0 * 6.204326629638672
Epoch 2390, val loss: 1.3555755615234375
Epoch 2400, training loss: 620.5253295898438 = 0.020848384127020836 + 100.0 * 6.205044746398926
Epoch 2400, val loss: 1.3585975170135498
Epoch 2410, training loss: 620.6958618164062 = 0.02057644911110401 + 100.0 * 6.206752777099609
Epoch 2410, val loss: 1.361451268196106
Epoch 2420, training loss: 620.4857177734375 = 0.020297789946198463 + 100.0 * 6.204653739929199
Epoch 2420, val loss: 1.3640718460083008
Epoch 2430, training loss: 620.4220581054688 = 0.02003074809908867 + 100.0 * 6.2040205001831055
Epoch 2430, val loss: 1.3666824102401733
Epoch 2440, training loss: 620.4010620117188 = 0.01976686902344227 + 100.0 * 6.203813076019287
Epoch 2440, val loss: 1.3692365884780884
Epoch 2450, training loss: 620.7921752929688 = 0.01951434463262558 + 100.0 * 6.20772647857666
Epoch 2450, val loss: 1.3717952966690063
Epoch 2460, training loss: 620.3167724609375 = 0.019260086119174957 + 100.0 * 6.202974796295166
Epoch 2460, val loss: 1.3747003078460693
Epoch 2470, training loss: 620.3324584960938 = 0.01901254616677761 + 100.0 * 6.203134059906006
Epoch 2470, val loss: 1.377301573753357
Epoch 2480, training loss: 620.9273681640625 = 0.01877666264772415 + 100.0 * 6.209085941314697
Epoch 2480, val loss: 1.3796741962432861
Epoch 2490, training loss: 620.7662963867188 = 0.018535081297159195 + 100.0 * 6.207477569580078
Epoch 2490, val loss: 1.3826358318328857
Epoch 2500, training loss: 620.4534912109375 = 0.018285498023033142 + 100.0 * 6.204351902008057
Epoch 2500, val loss: 1.3847944736480713
Epoch 2510, training loss: 620.336669921875 = 0.01806098222732544 + 100.0 * 6.20318603515625
Epoch 2510, val loss: 1.3876943588256836
Epoch 2520, training loss: 620.2342529296875 = 0.017834173515439034 + 100.0 * 6.202164173126221
Epoch 2520, val loss: 1.3899867534637451
Epoch 2530, training loss: 620.197021484375 = 0.017616435885429382 + 100.0 * 6.201793670654297
Epoch 2530, val loss: 1.392625331878662
Epoch 2540, training loss: 620.5560302734375 = 0.017406312748789787 + 100.0 * 6.205386638641357
Epoch 2540, val loss: 1.3950644731521606
Epoch 2550, training loss: 620.3500366210938 = 0.01718704216182232 + 100.0 * 6.2033281326293945
Epoch 2550, val loss: 1.3968701362609863
Epoch 2560, training loss: 620.3662109375 = 0.016972128301858902 + 100.0 * 6.203492641448975
Epoch 2560, val loss: 1.399397850036621
Epoch 2570, training loss: 620.1541137695312 = 0.016764210537075996 + 100.0 * 6.20137357711792
Epoch 2570, val loss: 1.4021872282028198
Epoch 2580, training loss: 620.171875 = 0.016566287726163864 + 100.0 * 6.2015533447265625
Epoch 2580, val loss: 1.4045509099960327
Epoch 2590, training loss: 620.5333862304688 = 0.016372697427868843 + 100.0 * 6.205170154571533
Epoch 2590, val loss: 1.4068692922592163
Epoch 2600, training loss: 620.1229858398438 = 0.016171734780073166 + 100.0 * 6.201067924499512
Epoch 2600, val loss: 1.4091181755065918
Epoch 2610, training loss: 620.265869140625 = 0.01598139852285385 + 100.0 * 6.202498912811279
Epoch 2610, val loss: 1.411512017250061
Epoch 2620, training loss: 620.6280517578125 = 0.015793396160006523 + 100.0 * 6.206122398376465
Epoch 2620, val loss: 1.413862943649292
Epoch 2630, training loss: 620.26611328125 = 0.015611115843057632 + 100.0 * 6.202504634857178
Epoch 2630, val loss: 1.4163179397583008
Epoch 2640, training loss: 620.1209716796875 = 0.015428255312144756 + 100.0 * 6.20105504989624
Epoch 2640, val loss: 1.4184815883636475
Epoch 2650, training loss: 620.0518188476562 = 0.015248823910951614 + 100.0 * 6.2003655433654785
Epoch 2650, val loss: 1.4209181070327759
Epoch 2660, training loss: 620.3990478515625 = 0.015077982097864151 + 100.0 * 6.2038397789001465
Epoch 2660, val loss: 1.4229933023452759
Epoch 2670, training loss: 620.1082153320312 = 0.014903625473380089 + 100.0 * 6.20093297958374
Epoch 2670, val loss: 1.4251374006271362
Epoch 2680, training loss: 620.2351684570312 = 0.014731613919138908 + 100.0 * 6.202204704284668
Epoch 2680, val loss: 1.4272282123565674
Epoch 2690, training loss: 620.1430053710938 = 0.014563119038939476 + 100.0 * 6.201284408569336
Epoch 2690, val loss: 1.4295552968978882
Epoch 2700, training loss: 619.9920043945312 = 0.014400013722479343 + 100.0 * 6.1997761726379395
Epoch 2700, val loss: 1.4317430257797241
Epoch 2710, training loss: 620.26123046875 = 0.014241285622119904 + 100.0 * 6.202469825744629
Epoch 2710, val loss: 1.4335635900497437
Epoch 2720, training loss: 619.95947265625 = 0.014081943780183792 + 100.0 * 6.199453830718994
Epoch 2720, val loss: 1.4360339641571045
Epoch 2730, training loss: 620.01220703125 = 0.013932585716247559 + 100.0 * 6.199982643127441
Epoch 2730, val loss: 1.4385104179382324
Epoch 2740, training loss: 620.6013793945312 = 0.013782371766865253 + 100.0 * 6.205875873565674
Epoch 2740, val loss: 1.439982533454895
Epoch 2750, training loss: 620.1758422851562 = 0.013625118881464005 + 100.0 * 6.201622486114502
Epoch 2750, val loss: 1.4426127672195435
Epoch 2760, training loss: 619.932861328125 = 0.013471953570842743 + 100.0 * 6.199193954467773
Epoch 2760, val loss: 1.4446170330047607
Epoch 2770, training loss: 619.8817749023438 = 0.013327864930033684 + 100.0 * 6.1986846923828125
Epoch 2770, val loss: 1.4467793703079224
Epoch 2780, training loss: 620.2174682617188 = 0.0131890419870615 + 100.0 * 6.202043056488037
Epoch 2780, val loss: 1.4488064050674438
Epoch 2790, training loss: 619.8059692382812 = 0.01304557640105486 + 100.0 * 6.1979289054870605
Epoch 2790, val loss: 1.4508724212646484
Epoch 2800, training loss: 619.9087524414062 = 0.012909176759421825 + 100.0 * 6.198958396911621
Epoch 2800, val loss: 1.4529073238372803
Epoch 2810, training loss: 620.276611328125 = 0.01277485303580761 + 100.0 * 6.202638626098633
Epoch 2810, val loss: 1.45466148853302
Epoch 2820, training loss: 619.8319702148438 = 0.01263863779604435 + 100.0 * 6.198193550109863
Epoch 2820, val loss: 1.4569737911224365
Epoch 2830, training loss: 619.7536010742188 = 0.0125059699639678 + 100.0 * 6.197411060333252
Epoch 2830, val loss: 1.4588524103164673
Epoch 2840, training loss: 619.7744140625 = 0.012378386221826077 + 100.0 * 6.197620391845703
Epoch 2840, val loss: 1.4609315395355225
Epoch 2850, training loss: 619.8184814453125 = 0.012258137576282024 + 100.0 * 6.198061943054199
Epoch 2850, val loss: 1.4631627798080444
Epoch 2860, training loss: 620.5407104492188 = 0.012141041457653046 + 100.0 * 6.205285549163818
Epoch 2860, val loss: 1.4648991823196411
Epoch 2870, training loss: 620.3124389648438 = 0.012003673240542412 + 100.0 * 6.203004360198975
Epoch 2870, val loss: 1.4660873413085938
Epoch 2880, training loss: 619.9937744140625 = 0.011880077421665192 + 100.0 * 6.1998186111450195
Epoch 2880, val loss: 1.468445897102356
Epoch 2890, training loss: 619.740478515625 = 0.011758391745388508 + 100.0 * 6.197287082672119
Epoch 2890, val loss: 1.4701321125030518
Epoch 2900, training loss: 619.7413330078125 = 0.01164484303444624 + 100.0 * 6.197296619415283
Epoch 2900, val loss: 1.4721969366073608
Epoch 2910, training loss: 619.7149047851562 = 0.011530949734151363 + 100.0 * 6.197033405303955
Epoch 2910, val loss: 1.4739943742752075
Epoch 2920, training loss: 619.8983154296875 = 0.011422326788306236 + 100.0 * 6.198868751525879
Epoch 2920, val loss: 1.475814700126648
Epoch 2930, training loss: 619.90869140625 = 0.011309385299682617 + 100.0 * 6.198974132537842
Epoch 2930, val loss: 1.4775471687316895
Epoch 2940, training loss: 619.8336181640625 = 0.01119766291230917 + 100.0 * 6.1982245445251465
Epoch 2940, val loss: 1.4794100522994995
Epoch 2950, training loss: 620.4346923828125 = 0.011089042760431767 + 100.0 * 6.204235553741455
Epoch 2950, val loss: 1.4810845851898193
Epoch 2960, training loss: 619.9471435546875 = 0.010978207923471928 + 100.0 * 6.199361324310303
Epoch 2960, val loss: 1.4828369617462158
Epoch 2970, training loss: 619.7559204101562 = 0.010867740958929062 + 100.0 * 6.197450637817383
Epoch 2970, val loss: 1.4846256971359253
Epoch 2980, training loss: 619.669189453125 = 0.010766763240098953 + 100.0 * 6.196584701538086
Epoch 2980, val loss: 1.4864821434020996
Epoch 2990, training loss: 619.7392578125 = 0.010669032111763954 + 100.0 * 6.1972856521606445
Epoch 2990, val loss: 1.4880441427230835
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 861.6370239257812 = 1.9558082818984985 + 100.0 * 8.59681224822998
Epoch 0, val loss: 1.9556365013122559
Epoch 10, training loss: 861.5174560546875 = 1.9470515251159668 + 100.0 * 8.595704078674316
Epoch 10, val loss: 1.9464025497436523
Epoch 20, training loss: 860.6145629882812 = 1.9361132383346558 + 100.0 * 8.586784362792969
Epoch 20, val loss: 1.9347443580627441
Epoch 30, training loss: 853.6605224609375 = 1.9213283061981201 + 100.0 * 8.5173921585083
Epoch 30, val loss: 1.9189677238464355
Epoch 40, training loss: 809.4216918945312 = 1.901856780052185 + 100.0 * 8.07519817352295
Epoch 40, val loss: 1.89876389503479
Epoch 50, training loss: 750.6178588867188 = 1.8796180486679077 + 100.0 * 7.487382411956787
Epoch 50, val loss: 1.8769562244415283
Epoch 60, training loss: 727.552978515625 = 1.8649355173110962 + 100.0 * 7.256880283355713
Epoch 60, val loss: 1.8626389503479004
Epoch 70, training loss: 708.7440185546875 = 1.8518497943878174 + 100.0 * 7.0689215660095215
Epoch 70, val loss: 1.8493096828460693
Epoch 80, training loss: 692.974609375 = 1.8405661582946777 + 100.0 * 6.911340236663818
Epoch 80, val loss: 1.8384928703308105
Epoch 90, training loss: 680.20556640625 = 1.8308964967727661 + 100.0 * 6.783746719360352
Epoch 90, val loss: 1.8286525011062622
Epoch 100, training loss: 672.9664916992188 = 1.8218345642089844 + 100.0 * 6.711446762084961
Epoch 100, val loss: 1.8191218376159668
Epoch 110, training loss: 668.0328979492188 = 1.8125113248825073 + 100.0 * 6.662204265594482
Epoch 110, val loss: 1.8092560768127441
Epoch 120, training loss: 664.653076171875 = 1.8035094738006592 + 100.0 * 6.628495216369629
Epoch 120, val loss: 1.7996976375579834
Epoch 130, training loss: 661.7277221679688 = 1.7948299646377563 + 100.0 * 6.599329471588135
Epoch 130, val loss: 1.7904548645019531
Epoch 140, training loss: 659.5162963867188 = 1.7861909866333008 + 100.0 * 6.577301025390625
Epoch 140, val loss: 1.7813127040863037
Epoch 150, training loss: 656.70849609375 = 1.7773821353912354 + 100.0 * 6.54931116104126
Epoch 150, val loss: 1.7721220254898071
Epoch 160, training loss: 654.4972534179688 = 1.7684482336044312 + 100.0 * 6.52728796005249
Epoch 160, val loss: 1.7630770206451416
Epoch 170, training loss: 652.3525390625 = 1.7592711448669434 + 100.0 * 6.505932807922363
Epoch 170, val loss: 1.7538872957229614
Epoch 180, training loss: 650.4810791015625 = 1.7496432065963745 + 100.0 * 6.487314701080322
Epoch 180, val loss: 1.7444449663162231
Epoch 190, training loss: 649.177001953125 = 1.7392672300338745 + 100.0 * 6.474377632141113
Epoch 190, val loss: 1.734305500984192
Epoch 200, training loss: 647.9490356445312 = 1.7276877164840698 + 100.0 * 6.46221399307251
Epoch 200, val loss: 1.723286747932434
Epoch 210, training loss: 646.5044555664062 = 1.7151583433151245 + 100.0 * 6.447893142700195
Epoch 210, val loss: 1.711267113685608
Epoch 220, training loss: 645.379638671875 = 1.7016799449920654 + 100.0 * 6.436779499053955
Epoch 220, val loss: 1.69846510887146
Epoch 230, training loss: 644.3602294921875 = 1.6871840953826904 + 100.0 * 6.426730155944824
Epoch 230, val loss: 1.6848182678222656
Epoch 240, training loss: 644.2838745117188 = 1.6715998649597168 + 100.0 * 6.426123142242432
Epoch 240, val loss: 1.6703602075576782
Epoch 250, training loss: 642.8560791015625 = 1.6547070741653442 + 100.0 * 6.412013530731201
Epoch 250, val loss: 1.6546701192855835
Epoch 260, training loss: 641.9020385742188 = 1.6366857290267944 + 100.0 * 6.402653694152832
Epoch 260, val loss: 1.6381633281707764
Epoch 270, training loss: 641.154296875 = 1.617558479309082 + 100.0 * 6.395367622375488
Epoch 270, val loss: 1.620848536491394
Epoch 280, training loss: 640.589111328125 = 1.597258448600769 + 100.0 * 6.389918804168701
Epoch 280, val loss: 1.6026906967163086
Epoch 290, training loss: 640.3905639648438 = 1.5758191347122192 + 100.0 * 6.388147354125977
Epoch 290, val loss: 1.5835800170898438
Epoch 300, training loss: 639.5264892578125 = 1.5533251762390137 + 100.0 * 6.379731178283691
Epoch 300, val loss: 1.5639879703521729
Epoch 310, training loss: 638.8522338867188 = 1.5300081968307495 + 100.0 * 6.3732218742370605
Epoch 310, val loss: 1.5440421104431152
Epoch 320, training loss: 638.273681640625 = 1.5060306787490845 + 100.0 * 6.367676258087158
Epoch 320, val loss: 1.5237003564834595
Epoch 330, training loss: 637.96142578125 = 1.4814716577529907 + 100.0 * 6.364799499511719
Epoch 330, val loss: 1.503204107284546
Epoch 340, training loss: 637.5859985351562 = 1.4562385082244873 + 100.0 * 6.361297607421875
Epoch 340, val loss: 1.482527732849121
Epoch 350, training loss: 636.99658203125 = 1.4308381080627441 + 100.0 * 6.35565710067749
Epoch 350, val loss: 1.4618977308273315
Epoch 360, training loss: 636.480224609375 = 1.4052913188934326 + 100.0 * 6.350749492645264
Epoch 360, val loss: 1.4416841268539429
Epoch 370, training loss: 636.2144775390625 = 1.3797434568405151 + 100.0 * 6.348347187042236
Epoch 370, val loss: 1.4217414855957031
Epoch 380, training loss: 635.9605712890625 = 1.3540682792663574 + 100.0 * 6.346065044403076
Epoch 380, val loss: 1.4015992879867554
Epoch 390, training loss: 635.3264770507812 = 1.3285657167434692 + 100.0 * 6.33997917175293
Epoch 390, val loss: 1.3820278644561768
Epoch 400, training loss: 634.99951171875 = 1.3031795024871826 + 100.0 * 6.336963653564453
Epoch 400, val loss: 1.3627887964248657
Epoch 410, training loss: 635.3815307617188 = 1.278028130531311 + 100.0 * 6.341034889221191
Epoch 410, val loss: 1.3436628580093384
Epoch 420, training loss: 634.3455810546875 = 1.2528830766677856 + 100.0 * 6.330926895141602
Epoch 420, val loss: 1.3247727155685425
Epoch 430, training loss: 633.9948120117188 = 1.2280538082122803 + 100.0 * 6.327667713165283
Epoch 430, val loss: 1.3062559366226196
Epoch 440, training loss: 634.178955078125 = 1.203582763671875 + 100.0 * 6.329753398895264
Epoch 440, val loss: 1.2878304719924927
Epoch 450, training loss: 633.7362670898438 = 1.1790062189102173 + 100.0 * 6.325572490692139
Epoch 450, val loss: 1.269830346107483
Epoch 460, training loss: 633.2730712890625 = 1.1549915075302124 + 100.0 * 6.321181297302246
Epoch 460, val loss: 1.2521673440933228
Epoch 470, training loss: 632.8955688476562 = 1.1313414573669434 + 100.0 * 6.3176422119140625
Epoch 470, val loss: 1.2348532676696777
Epoch 480, training loss: 632.650634765625 = 1.1081174612045288 + 100.0 * 6.315425395965576
Epoch 480, val loss: 1.2178369760513306
Epoch 490, training loss: 632.6024169921875 = 1.08513605594635 + 100.0 * 6.3151726722717285
Epoch 490, val loss: 1.2011981010437012
Epoch 500, training loss: 632.3085327148438 = 1.0625417232513428 + 100.0 * 6.312459468841553
Epoch 500, val loss: 1.184910535812378
Epoch 510, training loss: 631.8935546875 = 1.040544867515564 + 100.0 * 6.308530330657959
Epoch 510, val loss: 1.1692733764648438
Epoch 520, training loss: 631.76416015625 = 1.019135594367981 + 100.0 * 6.307449817657471
Epoch 520, val loss: 1.154088020324707
Epoch 530, training loss: 631.5252685546875 = 0.9981203079223633 + 100.0 * 6.305271625518799
Epoch 530, val loss: 1.1394869089126587
Epoch 540, training loss: 631.4598999023438 = 0.9777271151542664 + 100.0 * 6.304821491241455
Epoch 540, val loss: 1.1251411437988281
Epoch 550, training loss: 631.1511840820312 = 0.9577869176864624 + 100.0 * 6.301934242248535
Epoch 550, val loss: 1.1118520498275757
Epoch 560, training loss: 631.2605590820312 = 0.938534140586853 + 100.0 * 6.303220272064209
Epoch 560, val loss: 1.098633885383606
Epoch 570, training loss: 630.71826171875 = 0.919891893863678 + 100.0 * 6.297983646392822
Epoch 570, val loss: 1.0861653089523315
Epoch 580, training loss: 630.4201049804688 = 0.9018480181694031 + 100.0 * 6.295182704925537
Epoch 580, val loss: 1.0745762586593628
Epoch 590, training loss: 631.0973510742188 = 0.8844127655029297 + 100.0 * 6.302129745483398
Epoch 590, val loss: 1.0631999969482422
Epoch 600, training loss: 630.2623291015625 = 0.8673876523971558 + 100.0 * 6.293949604034424
Epoch 600, val loss: 1.0526392459869385
Epoch 610, training loss: 629.8311157226562 = 0.8509095311164856 + 100.0 * 6.289802551269531
Epoch 610, val loss: 1.0425511598587036
Epoch 620, training loss: 629.6771240234375 = 0.8350148797035217 + 100.0 * 6.288421154022217
Epoch 620, val loss: 1.0329409837722778
Epoch 630, training loss: 629.5855712890625 = 0.8195990920066833 + 100.0 * 6.287660121917725
Epoch 630, val loss: 1.0239005088806152
Epoch 640, training loss: 629.489501953125 = 0.8043766021728516 + 100.0 * 6.286850929260254
Epoch 640, val loss: 1.0151082277297974
Epoch 650, training loss: 629.3595581054688 = 0.7895277738571167 + 100.0 * 6.285699844360352
Epoch 650, val loss: 1.0066214799880981
Epoch 660, training loss: 629.0746459960938 = 0.7749863266944885 + 100.0 * 6.282996654510498
Epoch 660, val loss: 0.9985674619674683
Epoch 670, training loss: 628.99267578125 = 0.7608059048652649 + 100.0 * 6.282318592071533
Epoch 670, val loss: 0.9908360242843628
Epoch 680, training loss: 628.7123413085938 = 0.7467772364616394 + 100.0 * 6.279655933380127
Epoch 680, val loss: 0.9832025766372681
Epoch 690, training loss: 628.6004028320312 = 0.732867956161499 + 100.0 * 6.278675556182861
Epoch 690, val loss: 0.9758557081222534
Epoch 700, training loss: 628.42236328125 = 0.7191994786262512 + 100.0 * 6.277031898498535
Epoch 700, val loss: 0.9687790274620056
Epoch 710, training loss: 629.24755859375 = 0.7055585980415344 + 100.0 * 6.285419940948486
Epoch 710, val loss: 0.9618754982948303
Epoch 720, training loss: 628.2403564453125 = 0.6918623447418213 + 100.0 * 6.275485038757324
Epoch 720, val loss: 0.9547027349472046
Epoch 730, training loss: 627.987060546875 = 0.6783404350280762 + 100.0 * 6.273087024688721
Epoch 730, val loss: 0.9477531909942627
Epoch 740, training loss: 627.878662109375 = 0.6649011969566345 + 100.0 * 6.27213716506958
Epoch 740, val loss: 0.9409980177879333
Epoch 750, training loss: 628.4813232421875 = 0.651390552520752 + 100.0 * 6.278299331665039
Epoch 750, val loss: 0.9342840909957886
Epoch 760, training loss: 627.860107421875 = 0.6378075480461121 + 100.0 * 6.272222995758057
Epoch 760, val loss: 0.9274526238441467
Epoch 770, training loss: 627.6480712890625 = 0.6242337226867676 + 100.0 * 6.270238876342773
Epoch 770, val loss: 0.9207696914672852
Epoch 780, training loss: 627.33642578125 = 0.6107671856880188 + 100.0 * 6.267256259918213
Epoch 780, val loss: 0.9142036437988281
Epoch 790, training loss: 627.2559814453125 = 0.5973085165023804 + 100.0 * 6.266586780548096
Epoch 790, val loss: 0.9077009558677673
Epoch 800, training loss: 627.9749755859375 = 0.5838176608085632 + 100.0 * 6.273911952972412
Epoch 800, val loss: 0.9011688232421875
Epoch 810, training loss: 627.3329467773438 = 0.5702519416809082 + 100.0 * 6.267627239227295
Epoch 810, val loss: 0.8946685791015625
Epoch 820, training loss: 626.9774169921875 = 0.5567554235458374 + 100.0 * 6.264206409454346
Epoch 820, val loss: 0.8881580233573914
Epoch 830, training loss: 626.96826171875 = 0.543420135974884 + 100.0 * 6.264248847961426
Epoch 830, val loss: 0.8818983435630798
Epoch 840, training loss: 626.7974853515625 = 0.5300579071044922 + 100.0 * 6.262674331665039
Epoch 840, val loss: 0.875593900680542
Epoch 850, training loss: 626.7493896484375 = 0.5167993307113647 + 100.0 * 6.262325763702393
Epoch 850, val loss: 0.8695328831672668
Epoch 860, training loss: 626.6813354492188 = 0.503754734992981 + 100.0 * 6.261775493621826
Epoch 860, val loss: 0.8636230826377869
Epoch 870, training loss: 626.8845825195312 = 0.4909057021141052 + 100.0 * 6.263936996459961
Epoch 870, val loss: 0.8579487204551697
Epoch 880, training loss: 626.3777465820312 = 0.478183776140213 + 100.0 * 6.258995056152344
Epoch 880, val loss: 0.8524588942527771
Epoch 890, training loss: 626.2551879882812 = 0.4657493829727173 + 100.0 * 6.257894039154053
Epoch 890, val loss: 0.8472074866294861
Epoch 900, training loss: 626.1353149414062 = 0.45361432433128357 + 100.0 * 6.256816864013672
Epoch 900, val loss: 0.8422956466674805
Epoch 910, training loss: 626.1624755859375 = 0.4417397677898407 + 100.0 * 6.25720739364624
Epoch 910, val loss: 0.8376490473747253
Epoch 920, training loss: 626.6944580078125 = 0.4299996495246887 + 100.0 * 6.2626447677612305
Epoch 920, val loss: 0.8330279588699341
Epoch 930, training loss: 626.17919921875 = 0.418549507856369 + 100.0 * 6.257606029510498
Epoch 930, val loss: 0.8286562561988831
Epoch 940, training loss: 625.7809448242188 = 0.40744873881340027 + 100.0 * 6.253734588623047
Epoch 940, val loss: 0.8247234225273132
Epoch 950, training loss: 625.7136840820312 = 0.39672085642814636 + 100.0 * 6.253169536590576
Epoch 950, val loss: 0.8211899995803833
Epoch 960, training loss: 626.4318237304688 = 0.3862672746181488 + 100.0 * 6.260455131530762
Epoch 960, val loss: 0.8179152011871338
Epoch 970, training loss: 626.204833984375 = 0.37602683901786804 + 100.0 * 6.2582879066467285
Epoch 970, val loss: 0.8146741390228271
Epoch 980, training loss: 625.6018676757812 = 0.36600959300994873 + 100.0 * 6.252358436584473
Epoch 980, val loss: 0.8118008971214294
Epoch 990, training loss: 625.4634399414062 = 0.3564377725124359 + 100.0 * 6.251070022583008
Epoch 990, val loss: 0.8093222379684448
Epoch 1000, training loss: 625.3896484375 = 0.3472108840942383 + 100.0 * 6.250423908233643
Epoch 1000, val loss: 0.8071603775024414
Epoch 1010, training loss: 625.787841796875 = 0.33826759457588196 + 100.0 * 6.254496097564697
Epoch 1010, val loss: 0.8050516843795776
Epoch 1020, training loss: 625.5089111328125 = 0.3294679820537567 + 100.0 * 6.251794338226318
Epoch 1020, val loss: 0.8032312989234924
Epoch 1030, training loss: 625.1698608398438 = 0.32093489170074463 + 100.0 * 6.2484893798828125
Epoch 1030, val loss: 0.8016646504402161
Epoch 1040, training loss: 625.0767211914062 = 0.31279677152633667 + 100.0 * 6.247639179229736
Epoch 1040, val loss: 0.8004083633422852
Epoch 1050, training loss: 625.2300415039062 = 0.3048734962940216 + 100.0 * 6.249251842498779
Epoch 1050, val loss: 0.7993419766426086
Epoch 1060, training loss: 625.0157470703125 = 0.2971910238265991 + 100.0 * 6.247185707092285
Epoch 1060, val loss: 0.7984580993652344
Epoch 1070, training loss: 625.4655151367188 = 0.28970658779144287 + 100.0 * 6.251758098602295
Epoch 1070, val loss: 0.7977625131607056
Epoch 1080, training loss: 625.0497436523438 = 0.28243085741996765 + 100.0 * 6.247673034667969
Epoch 1080, val loss: 0.797209620475769
Epoch 1090, training loss: 624.7587280273438 = 0.27539920806884766 + 100.0 * 6.244833469390869
Epoch 1090, val loss: 0.7970103025436401
Epoch 1100, training loss: 625.0936279296875 = 0.26867544651031494 + 100.0 * 6.248249530792236
Epoch 1100, val loss: 0.7969827055931091
Epoch 1110, training loss: 624.6428833007812 = 0.26199954748153687 + 100.0 * 6.243808746337891
Epoch 1110, val loss: 0.7971421480178833
Epoch 1120, training loss: 624.5699462890625 = 0.25562191009521484 + 100.0 * 6.243143558502197
Epoch 1120, val loss: 0.7975074648857117
Epoch 1130, training loss: 624.8008422851562 = 0.2494664490222931 + 100.0 * 6.245513916015625
Epoch 1130, val loss: 0.798009991645813
Epoch 1140, training loss: 624.4776000976562 = 0.24336065351963043 + 100.0 * 6.242342472076416
Epoch 1140, val loss: 0.7985464930534363
Epoch 1150, training loss: 624.4149780273438 = 0.23747597634792328 + 100.0 * 6.241775035858154
Epoch 1150, val loss: 0.7992722392082214
Epoch 1160, training loss: 624.34423828125 = 0.23180481791496277 + 100.0 * 6.241124153137207
Epoch 1160, val loss: 0.8002743124961853
Epoch 1170, training loss: 624.2650146484375 = 0.22633004188537598 + 100.0 * 6.240386962890625
Epoch 1170, val loss: 0.8015431761741638
Epoch 1180, training loss: 624.9487915039062 = 0.22103449702262878 + 100.0 * 6.247277736663818
Epoch 1180, val loss: 0.8027331233024597
Epoch 1190, training loss: 624.8341064453125 = 0.21572066843509674 + 100.0 * 6.246184349060059
Epoch 1190, val loss: 0.8041630983352661
Epoch 1200, training loss: 624.1395263671875 = 0.21056975424289703 + 100.0 * 6.239289283752441
Epoch 1200, val loss: 0.805824875831604
Epoch 1210, training loss: 624.0829467773438 = 0.20567385852336884 + 100.0 * 6.238772869110107
Epoch 1210, val loss: 0.8075116872787476
Epoch 1220, training loss: 624.8995361328125 = 0.20092512667179108 + 100.0 * 6.246986389160156
Epoch 1220, val loss: 0.8094345331192017
Epoch 1230, training loss: 624.3394165039062 = 0.19621439278125763 + 100.0 * 6.241431713104248
Epoch 1230, val loss: 0.8115123510360718
Epoch 1240, training loss: 623.9948120117188 = 0.19164401292800903 + 100.0 * 6.238031387329102
Epoch 1240, val loss: 0.8134411573410034
Epoch 1250, training loss: 623.8287963867188 = 0.18727163970470428 + 100.0 * 6.236415386199951
Epoch 1250, val loss: 0.8157895803451538
Epoch 1260, training loss: 623.865966796875 = 0.1830243170261383 + 100.0 * 6.2368292808532715
Epoch 1260, val loss: 0.8181913495063782
Epoch 1270, training loss: 624.3671264648438 = 0.17884807288646698 + 100.0 * 6.241882801055908
Epoch 1270, val loss: 0.8205157518386841
Epoch 1280, training loss: 623.9933471679688 = 0.17477640509605408 + 100.0 * 6.238185405731201
Epoch 1280, val loss: 0.8230164051055908
Epoch 1290, training loss: 623.7634887695312 = 0.1707736998796463 + 100.0 * 6.235927104949951
Epoch 1290, val loss: 0.8256011605262756
Epoch 1300, training loss: 623.8751831054688 = 0.16690964996814728 + 100.0 * 6.237082481384277
Epoch 1300, val loss: 0.8282012939453125
Epoch 1310, training loss: 623.6085815429688 = 0.16317516565322876 + 100.0 * 6.234454154968262
Epoch 1310, val loss: 0.8309704661369324
Epoch 1320, training loss: 623.5927124023438 = 0.15954041481018066 + 100.0 * 6.2343316078186035
Epoch 1320, val loss: 0.8338453769683838
Epoch 1330, training loss: 623.789794921875 = 0.15602102875709534 + 100.0 * 6.236338138580322
Epoch 1330, val loss: 0.8367519974708557
Epoch 1340, training loss: 623.83642578125 = 0.15252527594566345 + 100.0 * 6.2368388175964355
Epoch 1340, val loss: 0.839801549911499
Epoch 1350, training loss: 623.4560546875 = 0.14908966422080994 + 100.0 * 6.23306941986084
Epoch 1350, val loss: 0.8427717685699463
Epoch 1360, training loss: 623.45458984375 = 0.14580804109573364 + 100.0 * 6.23308801651001
Epoch 1360, val loss: 0.8460555672645569
Epoch 1370, training loss: 623.6251220703125 = 0.14258740842342377 + 100.0 * 6.234825134277344
Epoch 1370, val loss: 0.8493457436561584
Epoch 1380, training loss: 623.4215087890625 = 0.1394725739955902 + 100.0 * 6.232820510864258
Epoch 1380, val loss: 0.8525512218475342
Epoch 1390, training loss: 623.4634399414062 = 0.13642190396785736 + 100.0 * 6.233270168304443
Epoch 1390, val loss: 0.8561474680900574
Epoch 1400, training loss: 623.43896484375 = 0.13344182074069977 + 100.0 * 6.233055114746094
Epoch 1400, val loss: 0.8594222664833069
Epoch 1410, training loss: 623.1485595703125 = 0.1305261105298996 + 100.0 * 6.230180263519287
Epoch 1410, val loss: 0.8629588484764099
Epoch 1420, training loss: 623.2376708984375 = 0.12772183120250702 + 100.0 * 6.2310991287231445
Epoch 1420, val loss: 0.8666466474533081
Epoch 1430, training loss: 623.3790283203125 = 0.12497629225254059 + 100.0 * 6.232540130615234
Epoch 1430, val loss: 0.8701323866844177
Epoch 1440, training loss: 623.318115234375 = 0.12228207290172577 + 100.0 * 6.231958866119385
Epoch 1440, val loss: 0.8735042214393616
Epoch 1450, training loss: 623.3912963867188 = 0.11962974071502686 + 100.0 * 6.2327165603637695
Epoch 1450, val loss: 0.8770661950111389
Epoch 1460, training loss: 622.9766845703125 = 0.11706521362066269 + 100.0 * 6.228596210479736
Epoch 1460, val loss: 0.8807543516159058
Epoch 1470, training loss: 622.9229736328125 = 0.11457500606775284 + 100.0 * 6.228084087371826
Epoch 1470, val loss: 0.884490430355072
Epoch 1480, training loss: 622.8851928710938 = 0.112189382314682 + 100.0 * 6.2277302742004395
Epoch 1480, val loss: 0.8882221579551697
Epoch 1490, training loss: 623.2683715820312 = 0.10985028743743896 + 100.0 * 6.2315850257873535
Epoch 1490, val loss: 0.8918209075927734
Epoch 1500, training loss: 623.0894775390625 = 0.10751990973949432 + 100.0 * 6.229819297790527
Epoch 1500, val loss: 0.8956277966499329
Epoch 1510, training loss: 623.0283203125 = 0.10521990060806274 + 100.0 * 6.229230880737305
Epoch 1510, val loss: 0.8989696502685547
Epoch 1520, training loss: 622.7343139648438 = 0.10303114354610443 + 100.0 * 6.226312637329102
Epoch 1520, val loss: 0.9031170606613159
Epoch 1530, training loss: 622.6849365234375 = 0.10091519355773926 + 100.0 * 6.2258405685424805
Epoch 1530, val loss: 0.9071051478385925
Epoch 1540, training loss: 623.3812866210938 = 0.09885989129543304 + 100.0 * 6.232824325561523
Epoch 1540, val loss: 0.9111270904541016
Epoch 1550, training loss: 623.462890625 = 0.0967864990234375 + 100.0 * 6.233660697937012
Epoch 1550, val loss: 0.9144346117973328
Epoch 1560, training loss: 622.8372802734375 = 0.09474527090787888 + 100.0 * 6.227425575256348
Epoch 1560, val loss: 0.9185230135917664
Epoch 1570, training loss: 622.5724487304688 = 0.09278108179569244 + 100.0 * 6.224796772003174
Epoch 1570, val loss: 0.9224095344543457
Epoch 1580, training loss: 622.5855712890625 = 0.09091243892908096 + 100.0 * 6.224946022033691
Epoch 1580, val loss: 0.9263786673545837
Epoch 1590, training loss: 622.7613525390625 = 0.08908488601446152 + 100.0 * 6.226722717285156
Epoch 1590, val loss: 0.9303653240203857
Epoch 1600, training loss: 622.7730712890625 = 0.08726881444454193 + 100.0 * 6.226858139038086
Epoch 1600, val loss: 0.93415766954422
Epoch 1610, training loss: 622.5875854492188 = 0.08548184484243393 + 100.0 * 6.225020885467529
Epoch 1610, val loss: 0.9380907416343689
Epoch 1620, training loss: 622.538330078125 = 0.0837511420249939 + 100.0 * 6.224545955657959
Epoch 1620, val loss: 0.9421975612640381
Epoch 1630, training loss: 622.6892700195312 = 0.08207809180021286 + 100.0 * 6.226071834564209
Epoch 1630, val loss: 0.9462785124778748
Epoch 1640, training loss: 622.5390625 = 0.08041836321353912 + 100.0 * 6.224586009979248
Epoch 1640, val loss: 0.9501654505729675
Epoch 1650, training loss: 622.711669921875 = 0.07881001383066177 + 100.0 * 6.2263288497924805
Epoch 1650, val loss: 0.9541874527931213
Epoch 1660, training loss: 622.4251708984375 = 0.07723978161811829 + 100.0 * 6.223479747772217
Epoch 1660, val loss: 0.9582855105400085
Epoch 1670, training loss: 622.4114379882812 = 0.07572086155414581 + 100.0 * 6.223357200622559
Epoch 1670, val loss: 0.9623887538909912
Epoch 1680, training loss: 622.5166015625 = 0.07421845197677612 + 100.0 * 6.224423885345459
Epoch 1680, val loss: 0.9663001298904419
Epoch 1690, training loss: 622.7568359375 = 0.0727413147687912 + 100.0 * 6.226840972900391
Epoch 1690, val loss: 0.9704638719558716
Epoch 1700, training loss: 622.6975708007812 = 0.07130155712366104 + 100.0 * 6.22626256942749
Epoch 1700, val loss: 0.9743239879608154
Epoch 1710, training loss: 622.239013671875 = 0.06987427175045013 + 100.0 * 6.221691131591797
Epoch 1710, val loss: 0.9783181548118591
Epoch 1720, training loss: 622.1785888671875 = 0.06851131469011307 + 100.0 * 6.2211012840271
Epoch 1720, val loss: 0.982487678527832
Epoch 1730, training loss: 622.1088256835938 = 0.06719054281711578 + 100.0 * 6.220416069030762
Epoch 1730, val loss: 0.9865990281105042
Epoch 1740, training loss: 622.3558349609375 = 0.06590873748064041 + 100.0 * 6.222898960113525
Epoch 1740, val loss: 0.9905251860618591
Epoch 1750, training loss: 622.810302734375 = 0.0646141916513443 + 100.0 * 6.227456569671631
Epoch 1750, val loss: 0.9945451617240906
Epoch 1760, training loss: 622.3630981445312 = 0.06333466619253159 + 100.0 * 6.222997188568115
Epoch 1760, val loss: 0.9984144568443298
Epoch 1770, training loss: 622.0933227539062 = 0.06207883358001709 + 100.0 * 6.220312595367432
Epoch 1770, val loss: 1.0024348497390747
Epoch 1780, training loss: 621.9393310546875 = 0.06090738996863365 + 100.0 * 6.218784332275391
Epoch 1780, val loss: 1.0066076517105103
Epoch 1790, training loss: 621.9420776367188 = 0.05976784601807594 + 100.0 * 6.218822956085205
Epoch 1790, val loss: 1.0105347633361816
Epoch 1800, training loss: 623.1290283203125 = 0.05866546928882599 + 100.0 * 6.230703830718994
Epoch 1800, val loss: 1.0143712759017944
Epoch 1810, training loss: 622.5381469726562 = 0.057499393820762634 + 100.0 * 6.224806785583496
Epoch 1810, val loss: 1.0186724662780762
Epoch 1820, training loss: 621.9951782226562 = 0.05638968199491501 + 100.0 * 6.219388008117676
Epoch 1820, val loss: 1.0222707986831665
Epoch 1830, training loss: 621.83154296875 = 0.05533505231142044 + 100.0 * 6.217761993408203
Epoch 1830, val loss: 1.0265194177627563
Epoch 1840, training loss: 621.8707885742188 = 0.054320186376571655 + 100.0 * 6.218164443969727
Epoch 1840, val loss: 1.0306636095046997
Epoch 1850, training loss: 622.709716796875 = 0.05333748459815979 + 100.0 * 6.226563930511475
Epoch 1850, val loss: 1.034645676612854
Epoch 1860, training loss: 622.0476684570312 = 0.05228882655501366 + 100.0 * 6.219953536987305
Epoch 1860, val loss: 1.038351058959961
Epoch 1870, training loss: 621.8174438476562 = 0.05133099481463432 + 100.0 * 6.217660903930664
Epoch 1870, val loss: 1.0424286127090454
Epoch 1880, training loss: 622.212158203125 = 0.05039995536208153 + 100.0 * 6.221617698669434
Epoch 1880, val loss: 1.0464200973510742
Epoch 1890, training loss: 621.8034057617188 = 0.049444008618593216 + 100.0 * 6.2175397872924805
Epoch 1890, val loss: 1.0500216484069824
Epoch 1900, training loss: 621.7652587890625 = 0.04852732643485069 + 100.0 * 6.217167377471924
Epoch 1900, val loss: 1.054256796836853
Epoch 1910, training loss: 621.7828369140625 = 0.04764947667717934 + 100.0 * 6.21735143661499
Epoch 1910, val loss: 1.0581892728805542
Epoch 1920, training loss: 621.9136962890625 = 0.046794816851615906 + 100.0 * 6.2186689376831055
Epoch 1920, val loss: 1.0622109174728394
Epoch 1930, training loss: 621.6780395507812 = 0.045952603220939636 + 100.0 * 6.216320514678955
Epoch 1930, val loss: 1.0660754442214966
Epoch 1940, training loss: 621.6961669921875 = 0.045135580003261566 + 100.0 * 6.216509819030762
Epoch 1940, val loss: 1.0700355768203735
Epoch 1950, training loss: 622.1827392578125 = 0.04433448985219002 + 100.0 * 6.221384525299072
Epoch 1950, val loss: 1.0737054347991943
Epoch 1960, training loss: 621.9147338867188 = 0.043534670025110245 + 100.0 * 6.218712329864502
Epoch 1960, val loss: 1.0779865980148315
Epoch 1970, training loss: 621.6746826171875 = 0.042760055512189865 + 100.0 * 6.2163190841674805
Epoch 1970, val loss: 1.0817149877548218
Epoch 1980, training loss: 621.6051025390625 = 0.04201364517211914 + 100.0 * 6.215631008148193
Epoch 1980, val loss: 1.085851788520813
Epoch 1990, training loss: 622.2459716796875 = 0.041285570710897446 + 100.0 * 6.222046852111816
Epoch 1990, val loss: 1.0896800756454468
Epoch 2000, training loss: 622.5560302734375 = 0.04056185111403465 + 100.0 * 6.225154399871826
Epoch 2000, val loss: 1.0932828187942505
Epoch 2010, training loss: 621.6981201171875 = 0.03982670605182648 + 100.0 * 6.216582775115967
Epoch 2010, val loss: 1.0972319841384888
Epoch 2020, training loss: 621.4457397460938 = 0.039128877222537994 + 100.0 * 6.214066028594971
Epoch 2020, val loss: 1.101152777671814
Epoch 2030, training loss: 621.4285888671875 = 0.03847455978393555 + 100.0 * 6.213901519775391
Epoch 2030, val loss: 1.1049922704696655
Epoch 2040, training loss: 621.5508422851562 = 0.03783078119158745 + 100.0 * 6.215129852294922
Epoch 2040, val loss: 1.1088895797729492
Epoch 2050, training loss: 621.9329833984375 = 0.03718779981136322 + 100.0 * 6.218958377838135
Epoch 2050, val loss: 1.1123650074005127
Epoch 2060, training loss: 621.74169921875 = 0.03654433414340019 + 100.0 * 6.2170515060424805
Epoch 2060, val loss: 1.1157922744750977
Epoch 2070, training loss: 621.5494995117188 = 0.03592398762702942 + 100.0 * 6.21513557434082
Epoch 2070, val loss: 1.1197805404663086
Epoch 2080, training loss: 621.8995361328125 = 0.03533141314983368 + 100.0 * 6.218641757965088
Epoch 2080, val loss: 1.1232383251190186
Epoch 2090, training loss: 621.38037109375 = 0.034738559275865555 + 100.0 * 6.213456153869629
Epoch 2090, val loss: 1.1273599863052368
Epoch 2100, training loss: 621.3314208984375 = 0.0341719388961792 + 100.0 * 6.212972164154053
Epoch 2100, val loss: 1.1306627988815308
Epoch 2110, training loss: 621.3477172851562 = 0.033612776547670364 + 100.0 * 6.213140964508057
Epoch 2110, val loss: 1.1344952583312988
Epoch 2120, training loss: 622.0234985351562 = 0.033069852739572525 + 100.0 * 6.219903945922852
Epoch 2120, val loss: 1.1376395225524902
Epoch 2130, training loss: 621.5015258789062 = 0.032509516924619675 + 100.0 * 6.214690208435059
Epoch 2130, val loss: 1.1415961980819702
Epoch 2140, training loss: 621.2883911132812 = 0.031982701271772385 + 100.0 * 6.212563991546631
Epoch 2140, val loss: 1.1447193622589111
Epoch 2150, training loss: 621.2340087890625 = 0.03147229552268982 + 100.0 * 6.212025165557861
Epoch 2150, val loss: 1.1491245031356812
Epoch 2160, training loss: 621.4337158203125 = 0.030988622456789017 + 100.0 * 6.214027404785156
Epoch 2160, val loss: 1.1522719860076904
Epoch 2170, training loss: 621.453857421875 = 0.03048752248287201 + 100.0 * 6.2142333984375
Epoch 2170, val loss: 1.1558030843734741
Epoch 2180, training loss: 621.4710693359375 = 0.029996883124113083 + 100.0 * 6.214410781860352
Epoch 2180, val loss: 1.1591242551803589
Epoch 2190, training loss: 621.352294921875 = 0.029521649703383446 + 100.0 * 6.213227272033691
Epoch 2190, val loss: 1.1626979112625122
Epoch 2200, training loss: 621.32958984375 = 0.02906136028468609 + 100.0 * 6.213005065917969
Epoch 2200, val loss: 1.1662681102752686
Epoch 2210, training loss: 621.2098999023438 = 0.028619075194001198 + 100.0 * 6.211812973022461
Epoch 2210, val loss: 1.1698271036148071
Epoch 2220, training loss: 621.5608520507812 = 0.02818363532423973 + 100.0 * 6.21532678604126
Epoch 2220, val loss: 1.1730914115905762
Epoch 2230, training loss: 621.09765625 = 0.027746213600039482 + 100.0 * 6.21069860458374
Epoch 2230, val loss: 1.1763088703155518
Epoch 2240, training loss: 621.1296997070312 = 0.027320465072989464 + 100.0 * 6.211023807525635
Epoch 2240, val loss: 1.1801270246505737
Epoch 2250, training loss: 621.4781494140625 = 0.02691209688782692 + 100.0 * 6.214512348175049
Epoch 2250, val loss: 1.183218240737915
Epoch 2260, training loss: 621.266357421875 = 0.026502816006541252 + 100.0 * 6.212398529052734
Epoch 2260, val loss: 1.1867494583129883
Epoch 2270, training loss: 621.2178344726562 = 0.026102542877197266 + 100.0 * 6.211916923522949
Epoch 2270, val loss: 1.1901073455810547
Epoch 2280, training loss: 621.0458984375 = 0.025713777169585228 + 100.0 * 6.210201740264893
Epoch 2280, val loss: 1.193337082862854
Epoch 2290, training loss: 621.0943603515625 = 0.025346675887703896 + 100.0 * 6.210690498352051
Epoch 2290, val loss: 1.1967803239822388
Epoch 2300, training loss: 621.2711791992188 = 0.024979284033179283 + 100.0 * 6.212461948394775
Epoch 2300, val loss: 1.2002123594284058
Epoch 2310, training loss: 621.377685546875 = 0.024607010185718536 + 100.0 * 6.213531017303467
Epoch 2310, val loss: 1.2033036947250366
Epoch 2320, training loss: 621.1425170898438 = 0.024241585284471512 + 100.0 * 6.211182594299316
Epoch 2320, val loss: 1.2066279649734497
Epoch 2330, training loss: 621.1068115234375 = 0.023892082273960114 + 100.0 * 6.210829257965088
Epoch 2330, val loss: 1.2099401950836182
Epoch 2340, training loss: 621.0510864257812 = 0.023550812155008316 + 100.0 * 6.210275173187256
Epoch 2340, val loss: 1.212975263595581
Epoch 2350, training loss: 620.92431640625 = 0.023214073851704597 + 100.0 * 6.209011077880859
Epoch 2350, val loss: 1.2160550355911255
Epoch 2360, training loss: 621.12109375 = 0.0228889100253582 + 100.0 * 6.210982322692871
Epoch 2360, val loss: 1.219067096710205
Epoch 2370, training loss: 620.9935302734375 = 0.022563286125659943 + 100.0 * 6.209709644317627
Epoch 2370, val loss: 1.2224940061569214
Epoch 2380, training loss: 620.9346923828125 = 0.022253431379795074 + 100.0 * 6.2091240882873535
Epoch 2380, val loss: 1.225842833518982
Epoch 2390, training loss: 621.4813842773438 = 0.02195783518254757 + 100.0 * 6.214593887329102
Epoch 2390, val loss: 1.228690266609192
Epoch 2400, training loss: 620.9041748046875 = 0.021630430594086647 + 100.0 * 6.208825588226318
Epoch 2400, val loss: 1.2315723896026611
Epoch 2410, training loss: 620.7842407226562 = 0.021329227834939957 + 100.0 * 6.207629680633545
Epoch 2410, val loss: 1.2347732782363892
Epoch 2420, training loss: 620.76025390625 = 0.021045180037617683 + 100.0 * 6.207391738891602
Epoch 2420, val loss: 1.237796425819397
Epoch 2430, training loss: 620.906005859375 = 0.02076701633632183 + 100.0 * 6.208852291107178
Epoch 2430, val loss: 1.24092435836792
Epoch 2440, training loss: 621.2870483398438 = 0.02048357203602791 + 100.0 * 6.212665557861328
Epoch 2440, val loss: 1.2434240579605103
Epoch 2450, training loss: 620.7852172851562 = 0.02019905298948288 + 100.0 * 6.207650184631348
Epoch 2450, val loss: 1.246953010559082
Epoch 2460, training loss: 620.692138671875 = 0.019926229491829872 + 100.0 * 6.206722259521484
Epoch 2460, val loss: 1.2496342658996582
Epoch 2470, training loss: 620.71875 = 0.01966838911175728 + 100.0 * 6.206990718841553
Epoch 2470, val loss: 1.2528196573257446
Epoch 2480, training loss: 621.0398559570312 = 0.019421428442001343 + 100.0 * 6.210204601287842
Epoch 2480, val loss: 1.255635142326355
Epoch 2490, training loss: 621.2575073242188 = 0.019163841381669044 + 100.0 * 6.212383270263672
Epoch 2490, val loss: 1.2583167552947998
Epoch 2500, training loss: 620.8003540039062 = 0.018910985440015793 + 100.0 * 6.2078142166137695
Epoch 2500, val loss: 1.26144278049469
Epoch 2510, training loss: 620.6564331054688 = 0.01865638606250286 + 100.0 * 6.2063775062561035
Epoch 2510, val loss: 1.2640537023544312
Epoch 2520, training loss: 620.7149047851562 = 0.018427690491080284 + 100.0 * 6.20696496963501
Epoch 2520, val loss: 1.2671570777893066
Epoch 2530, training loss: 621.0365600585938 = 0.01819869689643383 + 100.0 * 6.210183620452881
Epoch 2530, val loss: 1.2698863744735718
Epoch 2540, training loss: 620.6019287109375 = 0.017961964011192322 + 100.0 * 6.20583963394165
Epoch 2540, val loss: 1.2728582620620728
Epoch 2550, training loss: 620.7678833007812 = 0.017741022631525993 + 100.0 * 6.207501411437988
Epoch 2550, val loss: 1.2754374742507935
Epoch 2560, training loss: 621.03759765625 = 0.017522666603326797 + 100.0 * 6.210200786590576
Epoch 2560, val loss: 1.2780466079711914
Epoch 2570, training loss: 620.757568359375 = 0.017293008044362068 + 100.0 * 6.207403182983398
Epoch 2570, val loss: 1.281285285949707
Epoch 2580, training loss: 620.6701049804688 = 0.017084605991840363 + 100.0 * 6.2065300941467285
Epoch 2580, val loss: 1.283594012260437
Epoch 2590, training loss: 620.8057861328125 = 0.016873586922883987 + 100.0 * 6.207889556884766
Epoch 2590, val loss: 1.286712408065796
Epoch 2600, training loss: 620.517822265625 = 0.01666487753391266 + 100.0 * 6.20501184463501
Epoch 2600, val loss: 1.2890961170196533
Epoch 2610, training loss: 620.60888671875 = 0.01646745391190052 + 100.0 * 6.205924034118652
Epoch 2610, val loss: 1.2916243076324463
Epoch 2620, training loss: 620.9215087890625 = 0.016269398853182793 + 100.0 * 6.209052085876465
Epoch 2620, val loss: 1.29420804977417
Epoch 2630, training loss: 620.780029296875 = 0.01607409119606018 + 100.0 * 6.207639694213867
Epoch 2630, val loss: 1.2970330715179443
Epoch 2640, training loss: 620.649169921875 = 0.01588071882724762 + 100.0 * 6.206333160400391
Epoch 2640, val loss: 1.299757719039917
Epoch 2650, training loss: 620.8154907226562 = 0.01569463312625885 + 100.0 * 6.207997798919678
Epoch 2650, val loss: 1.3022370338439941
Epoch 2660, training loss: 620.5372924804688 = 0.015504433773458004 + 100.0 * 6.2052178382873535
Epoch 2660, val loss: 1.3047423362731934
Epoch 2670, training loss: 620.6385498046875 = 0.015328377485275269 + 100.0 * 6.206232070922852
Epoch 2670, val loss: 1.3068605661392212
Epoch 2680, training loss: 620.55419921875 = 0.015151293948292732 + 100.0 * 6.205390930175781
Epoch 2680, val loss: 1.3100244998931885
Epoch 2690, training loss: 620.6491088867188 = 0.01498111430555582 + 100.0 * 6.206341743469238
Epoch 2690, val loss: 1.3120421171188354
Epoch 2700, training loss: 620.8673095703125 = 0.01480578538030386 + 100.0 * 6.208524703979492
Epoch 2700, val loss: 1.3141084909439087
Epoch 2710, training loss: 620.5589599609375 = 0.014628582634031773 + 100.0 * 6.205442905426025
Epoch 2710, val loss: 1.31717050075531
Epoch 2720, training loss: 620.531982421875 = 0.014466223306953907 + 100.0 * 6.205175399780273
Epoch 2720, val loss: 1.3192607164382935
Epoch 2730, training loss: 620.53955078125 = 0.014302212744951248 + 100.0 * 6.205252647399902
Epoch 2730, val loss: 1.321964979171753
Epoch 2740, training loss: 620.5967407226562 = 0.014147680252790451 + 100.0 * 6.2058258056640625
Epoch 2740, val loss: 1.3240697383880615
Epoch 2750, training loss: 620.47412109375 = 0.013992278836667538 + 100.0 * 6.204601287841797
Epoch 2750, val loss: 1.3268845081329346
Epoch 2760, training loss: 620.4938354492188 = 0.013839413411915302 + 100.0 * 6.204799652099609
Epoch 2760, val loss: 1.328744649887085
Epoch 2770, training loss: 620.5596923828125 = 0.013687578029930592 + 100.0 * 6.205460071563721
Epoch 2770, val loss: 1.330714225769043
Epoch 2780, training loss: 620.4222412109375 = 0.013538912869989872 + 100.0 * 6.204086780548096
Epoch 2780, val loss: 1.3336503505706787
Epoch 2790, training loss: 620.4029541015625 = 0.013388987630605698 + 100.0 * 6.2038960456848145
Epoch 2790, val loss: 1.3357419967651367
Epoch 2800, training loss: 620.4937133789062 = 0.013247117400169373 + 100.0 * 6.204804420471191
Epoch 2800, val loss: 1.3377739191055298
Epoch 2810, training loss: 620.4044189453125 = 0.013106773607432842 + 100.0 * 6.20391321182251
Epoch 2810, val loss: 1.340396523475647
Epoch 2820, training loss: 620.5585327148438 = 0.012969051487743855 + 100.0 * 6.205455780029297
Epoch 2820, val loss: 1.3423253297805786
Epoch 2830, training loss: 620.5447387695312 = 0.01282583549618721 + 100.0 * 6.205318927764893
Epoch 2830, val loss: 1.3445028066635132
Epoch 2840, training loss: 620.2968139648438 = 0.012688332237303257 + 100.0 * 6.202841281890869
Epoch 2840, val loss: 1.3465189933776855
Epoch 2850, training loss: 620.2205200195312 = 0.012557916343212128 + 100.0 * 6.2020792961120605
Epoch 2850, val loss: 1.3486968278884888
Epoch 2860, training loss: 620.417724609375 = 0.012431787326931953 + 100.0 * 6.204052448272705
Epoch 2860, val loss: 1.3510820865631104
Epoch 2870, training loss: 620.5982666015625 = 0.012303182855248451 + 100.0 * 6.205859661102295
Epoch 2870, val loss: 1.3531153202056885
Epoch 2880, training loss: 620.404541015625 = 0.012174987234175205 + 100.0 * 6.203924179077148
Epoch 2880, val loss: 1.354769229888916
Epoch 2890, training loss: 620.43359375 = 0.01205037347972393 + 100.0 * 6.2042155265808105
Epoch 2890, val loss: 1.3569782972335815
Epoch 2900, training loss: 620.1102294921875 = 0.011925922706723213 + 100.0 * 6.20098352432251
Epoch 2900, val loss: 1.3588664531707764
Epoch 2910, training loss: 620.2940063476562 = 0.011811335571110249 + 100.0 * 6.202821731567383
Epoch 2910, val loss: 1.3615044355392456
Epoch 2920, training loss: 620.4075927734375 = 0.011696411296725273 + 100.0 * 6.203958511352539
Epoch 2920, val loss: 1.3629921674728394
Epoch 2930, training loss: 620.2754516601562 = 0.011578209698200226 + 100.0 * 6.202638626098633
Epoch 2930, val loss: 1.365432858467102
Epoch 2940, training loss: 620.2406005859375 = 0.011462044902145863 + 100.0 * 6.202291011810303
Epoch 2940, val loss: 1.3674099445343018
Epoch 2950, training loss: 620.3330688476562 = 0.011352182365953922 + 100.0 * 6.203217029571533
Epoch 2950, val loss: 1.369101643562317
Epoch 2960, training loss: 620.07958984375 = 0.011237281374633312 + 100.0 * 6.20068359375
Epoch 2960, val loss: 1.370934009552002
Epoch 2970, training loss: 620.1681518554688 = 0.011132955551147461 + 100.0 * 6.201570510864258
Epoch 2970, val loss: 1.3727617263793945
Epoch 2980, training loss: 620.2847290039062 = 0.011027692817151546 + 100.0 * 6.202736854553223
Epoch 2980, val loss: 1.3747214078903198
Epoch 2990, training loss: 620.3914184570312 = 0.010923470370471478 + 100.0 * 6.203804969787598
Epoch 2990, val loss: 1.3772263526916504
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 861.6279907226562 = 1.9418821334838867 + 100.0 * 8.596860885620117
Epoch 0, val loss: 1.9343996047973633
Epoch 10, training loss: 861.5521850585938 = 1.9332268238067627 + 100.0 * 8.596189498901367
Epoch 10, val loss: 1.9262243509292603
Epoch 20, training loss: 861.0824584960938 = 1.9231040477752686 + 100.0 * 8.591593742370605
Epoch 20, val loss: 1.9162884950637817
Epoch 30, training loss: 858.0470581054688 = 1.9106950759887695 + 100.0 * 8.561363220214844
Epoch 30, val loss: 1.9037443399429321
Epoch 40, training loss: 841.1636962890625 = 1.8950718641281128 + 100.0 * 8.392685890197754
Epoch 40, val loss: 1.8882439136505127
Epoch 50, training loss: 774.3447265625 = 1.8763068914413452 + 100.0 * 7.724684238433838
Epoch 50, val loss: 1.8701859712600708
Epoch 60, training loss: 742.4017333984375 = 1.8601168394088745 + 100.0 * 7.405416011810303
Epoch 60, val loss: 1.8567728996276855
Epoch 70, training loss: 715.1752319335938 = 1.8498293161392212 + 100.0 * 7.133253574371338
Epoch 70, val loss: 1.8478320837020874
Epoch 80, training loss: 695.6830444335938 = 1.8391807079315186 + 100.0 * 6.938438892364502
Epoch 80, val loss: 1.8384931087493896
Epoch 90, training loss: 685.2294921875 = 1.8300217390060425 + 100.0 * 6.8339948654174805
Epoch 90, val loss: 1.8302196264266968
Epoch 100, training loss: 677.56787109375 = 1.8210971355438232 + 100.0 * 6.757467746734619
Epoch 100, val loss: 1.8222209215164185
Epoch 110, training loss: 671.639892578125 = 1.8128843307495117 + 100.0 * 6.698270320892334
Epoch 110, val loss: 1.8149243593215942
Epoch 120, training loss: 666.8035888671875 = 1.8050626516342163 + 100.0 * 6.649985313415527
Epoch 120, val loss: 1.807884931564331
Epoch 130, training loss: 662.90234375 = 1.797340750694275 + 100.0 * 6.611049652099609
Epoch 130, val loss: 1.8008694648742676
Epoch 140, training loss: 660.0428466796875 = 1.7895407676696777 + 100.0 * 6.58253288269043
Epoch 140, val loss: 1.7937132120132446
Epoch 150, training loss: 657.7021484375 = 1.7814769744873047 + 100.0 * 6.559206485748291
Epoch 150, val loss: 1.7863777875900269
Epoch 160, training loss: 655.6246337890625 = 1.7730319499969482 + 100.0 * 6.538516521453857
Epoch 160, val loss: 1.7788009643554688
Epoch 170, training loss: 653.7720947265625 = 1.7640351057052612 + 100.0 * 6.52008056640625
Epoch 170, val loss: 1.77080500125885
Epoch 180, training loss: 652.0426635742188 = 1.75444495677948 + 100.0 * 6.50288200378418
Epoch 180, val loss: 1.76242196559906
Epoch 190, training loss: 650.530517578125 = 1.7441450357437134 + 100.0 * 6.487863540649414
Epoch 190, val loss: 1.7535022497177124
Epoch 200, training loss: 649.1729736328125 = 1.7328587770462036 + 100.0 * 6.474401473999023
Epoch 200, val loss: 1.7438220977783203
Epoch 210, training loss: 648.0121459960938 = 1.7205008268356323 + 100.0 * 6.462916374206543
Epoch 210, val loss: 1.7332630157470703
Epoch 220, training loss: 646.9502563476562 = 1.7069944143295288 + 100.0 * 6.452432632446289
Epoch 220, val loss: 1.7217005491256714
Epoch 230, training loss: 646.0460205078125 = 1.692418098449707 + 100.0 * 6.443535804748535
Epoch 230, val loss: 1.7093253135681152
Epoch 240, training loss: 645.1216430664062 = 1.6767066717147827 + 100.0 * 6.434449672698975
Epoch 240, val loss: 1.695973515510559
Epoch 250, training loss: 644.2633666992188 = 1.659867763519287 + 100.0 * 6.426034927368164
Epoch 250, val loss: 1.681761622428894
Epoch 260, training loss: 643.5330200195312 = 1.6418256759643555 + 100.0 * 6.418911933898926
Epoch 260, val loss: 1.6665995121002197
Epoch 270, training loss: 642.8297119140625 = 1.6226754188537598 + 100.0 * 6.412070274353027
Epoch 270, val loss: 1.6503735780715942
Epoch 280, training loss: 642.0347290039062 = 1.6022796630859375 + 100.0 * 6.404324054718018
Epoch 280, val loss: 1.6332523822784424
Epoch 290, training loss: 641.3668212890625 = 1.580898642539978 + 100.0 * 6.397859573364258
Epoch 290, val loss: 1.615286111831665
Epoch 300, training loss: 640.8731079101562 = 1.5585827827453613 + 100.0 * 6.3931450843811035
Epoch 300, val loss: 1.5964847803115845
Epoch 310, training loss: 640.1776733398438 = 1.5352318286895752 + 100.0 * 6.386424541473389
Epoch 310, val loss: 1.5770193338394165
Epoch 320, training loss: 639.5661010742188 = 1.5112491846084595 + 100.0 * 6.38054895401001
Epoch 320, val loss: 1.557080626487732
Epoch 330, training loss: 639.583984375 = 1.4865741729736328 + 100.0 * 6.380973815917969
Epoch 330, val loss: 1.5367134809494019
Epoch 340, training loss: 638.4797973632812 = 1.4613981246948242 + 100.0 * 6.37018346786499
Epoch 340, val loss: 1.515903353691101
Epoch 350, training loss: 638.0003051757812 = 1.435897946357727 + 100.0 * 6.3656439781188965
Epoch 350, val loss: 1.4949740171432495
Epoch 360, training loss: 637.4469604492188 = 1.4100958108901978 + 100.0 * 6.360368728637695
Epoch 360, val loss: 1.4740678071975708
Epoch 370, training loss: 637.0426025390625 = 1.384197473526001 + 100.0 * 6.356583595275879
Epoch 370, val loss: 1.4530706405639648
Epoch 380, training loss: 636.80224609375 = 1.3581069707870483 + 100.0 * 6.3544416427612305
Epoch 380, val loss: 1.431910514831543
Epoch 390, training loss: 636.2537231445312 = 1.3320235013961792 + 100.0 * 6.349216938018799
Epoch 390, val loss: 1.4108835458755493
Epoch 400, training loss: 635.7376098632812 = 1.3058819770812988 + 100.0 * 6.3443169593811035
Epoch 400, val loss: 1.389998197555542
Epoch 410, training loss: 635.3885498046875 = 1.28000009059906 + 100.0 * 6.341085433959961
Epoch 410, val loss: 1.369262933731079
Epoch 420, training loss: 635.0071411132812 = 1.2543399333953857 + 100.0 * 6.337528228759766
Epoch 420, val loss: 1.3489240407943726
Epoch 430, training loss: 635.0960083007812 = 1.228934407234192 + 100.0 * 6.33867073059082
Epoch 430, val loss: 1.3287327289581299
Epoch 440, training loss: 634.48779296875 = 1.203675627708435 + 100.0 * 6.332840919494629
Epoch 440, val loss: 1.3087449073791504
Epoch 450, training loss: 634.012451171875 = 1.1789206266403198 + 100.0 * 6.328335285186768
Epoch 450, val loss: 1.289240837097168
Epoch 460, training loss: 633.6768188476562 = 1.1546982526779175 + 100.0 * 6.325221061706543
Epoch 460, val loss: 1.2703192234039307
Epoch 470, training loss: 634.603515625 = 1.1311308145523071 + 100.0 * 6.334724426269531
Epoch 470, val loss: 1.2518000602722168
Epoch 480, training loss: 633.2758178710938 = 1.1075019836425781 + 100.0 * 6.321683406829834
Epoch 480, val loss: 1.2332643270492554
Epoch 490, training loss: 633.0797119140625 = 1.084765076637268 + 100.0 * 6.319949626922607
Epoch 490, val loss: 1.2155609130859375
Epoch 500, training loss: 632.6337280273438 = 1.0627487897872925 + 100.0 * 6.315709590911865
Epoch 500, val loss: 1.1985474824905396
Epoch 510, training loss: 632.357177734375 = 1.0414599180221558 + 100.0 * 6.313157558441162
Epoch 510, val loss: 1.1822372674942017
Epoch 520, training loss: 632.10888671875 = 1.0208338499069214 + 100.0 * 6.310880661010742
Epoch 520, val loss: 1.1665338277816772
Epoch 530, training loss: 633.1539916992188 = 1.0007919073104858 + 100.0 * 6.321531772613525
Epoch 530, val loss: 1.1514434814453125
Epoch 540, training loss: 632.0031127929688 = 0.9811052680015564 + 100.0 * 6.310220241546631
Epoch 540, val loss: 1.1366044282913208
Epoch 550, training loss: 631.4465942382812 = 0.962143063545227 + 100.0 * 6.304844379425049
Epoch 550, val loss: 1.122666597366333
Epoch 560, training loss: 631.2630615234375 = 0.9439244270324707 + 100.0 * 6.303191661834717
Epoch 560, val loss: 1.1094481945037842
Epoch 570, training loss: 631.2997436523438 = 0.9263167381286621 + 100.0 * 6.303733825683594
Epoch 570, val loss: 1.0967234373092651
Epoch 580, training loss: 631.0730590820312 = 0.9090273380279541 + 100.0 * 6.301640033721924
Epoch 580, val loss: 1.0846995115280151
Epoch 590, training loss: 630.682861328125 = 0.8923233151435852 + 100.0 * 6.297904968261719
Epoch 590, val loss: 1.0730828046798706
Epoch 600, training loss: 631.0030517578125 = 0.8761048913002014 + 100.0 * 6.30126953125
Epoch 600, val loss: 1.0620487928390503
Epoch 610, training loss: 630.3826904296875 = 0.8603415489196777 + 100.0 * 6.295223236083984
Epoch 610, val loss: 1.0514642000198364
Epoch 620, training loss: 630.1506958007812 = 0.8449965119361877 + 100.0 * 6.293057441711426
Epoch 620, val loss: 1.041542649269104
Epoch 630, training loss: 629.9177856445312 = 0.8301131725311279 + 100.0 * 6.290876388549805
Epoch 630, val loss: 1.0319933891296387
Epoch 640, training loss: 630.2357177734375 = 0.8155468106269836 + 100.0 * 6.294201850891113
Epoch 640, val loss: 1.0231777429580688
Epoch 650, training loss: 630.0794067382812 = 0.8011175990104675 + 100.0 * 6.292782306671143
Epoch 650, val loss: 1.0137040615081787
Epoch 660, training loss: 629.5352783203125 = 0.7869421243667603 + 100.0 * 6.2874836921691895
Epoch 660, val loss: 1.0053666830062866
Epoch 670, training loss: 629.3421020507812 = 0.7732032537460327 + 100.0 * 6.285689353942871
Epoch 670, val loss: 0.9974125623703003
Epoch 680, training loss: 629.453125 = 0.7597436308860779 + 100.0 * 6.286933898925781
Epoch 680, val loss: 0.9898247718811035
Epoch 690, training loss: 628.9556274414062 = 0.7463530898094177 + 100.0 * 6.282093048095703
Epoch 690, val loss: 0.9822392463684082
Epoch 700, training loss: 628.8912353515625 = 0.7332457304000854 + 100.0 * 6.281579971313477
Epoch 700, val loss: 0.9752815365791321
Epoch 710, training loss: 628.7299194335938 = 0.7203309535980225 + 100.0 * 6.280096054077148
Epoch 710, val loss: 0.9685713052749634
Epoch 720, training loss: 628.5603637695312 = 0.7075951099395752 + 100.0 * 6.278527736663818
Epoch 720, val loss: 0.9619683027267456
Epoch 730, training loss: 628.814453125 = 0.694887638092041 + 100.0 * 6.281195640563965
Epoch 730, val loss: 0.9551546573638916
Epoch 740, training loss: 628.9129638671875 = 0.6821134686470032 + 100.0 * 6.282308578491211
Epoch 740, val loss: 0.9490415453910828
Epoch 750, training loss: 628.1980590820312 = 0.6695569157600403 + 100.0 * 6.275285243988037
Epoch 750, val loss: 0.9426589608192444
Epoch 760, training loss: 628.0165405273438 = 0.657271146774292 + 100.0 * 6.273592472076416
Epoch 760, val loss: 0.93674236536026
Epoch 770, training loss: 627.8898315429688 = 0.6451682448387146 + 100.0 * 6.272447109222412
Epoch 770, val loss: 0.931108295917511
Epoch 780, training loss: 627.7523193359375 = 0.6331338882446289 + 100.0 * 6.2711920738220215
Epoch 780, val loss: 0.9258109927177429
Epoch 790, training loss: 628.446044921875 = 0.6211242079734802 + 100.0 * 6.278249740600586
Epoch 790, val loss: 0.9204885363578796
Epoch 800, training loss: 627.6864013671875 = 0.6090381145477295 + 100.0 * 6.270773887634277
Epoch 800, val loss: 0.9150794148445129
Epoch 810, training loss: 627.4368286132812 = 0.5971237421035767 + 100.0 * 6.268397331237793
Epoch 810, val loss: 0.9099054336547852
Epoch 820, training loss: 627.3121948242188 = 0.5854399800300598 + 100.0 * 6.267267227172852
Epoch 820, val loss: 0.9050381779670715
Epoch 830, training loss: 627.4367065429688 = 0.5739153027534485 + 100.0 * 6.268627643585205
Epoch 830, val loss: 0.9003989100456238
Epoch 840, training loss: 627.4960327148438 = 0.5623047351837158 + 100.0 * 6.269337177276611
Epoch 840, val loss: 0.8958299160003662
Epoch 850, training loss: 627.1531982421875 = 0.5507429838180542 + 100.0 * 6.266024589538574
Epoch 850, val loss: 0.8911857604980469
Epoch 860, training loss: 626.8936767578125 = 0.5394716858863831 + 100.0 * 6.2635416984558105
Epoch 860, val loss: 0.8869214057922363
Epoch 870, training loss: 626.8015747070312 = 0.5283536314964294 + 100.0 * 6.262732028961182
Epoch 870, val loss: 0.8829362392425537
Epoch 880, training loss: 626.74755859375 = 0.5174115300178528 + 100.0 * 6.262301445007324
Epoch 880, val loss: 0.8789833784103394
Epoch 890, training loss: 627.199951171875 = 0.5065510272979736 + 100.0 * 6.266933917999268
Epoch 890, val loss: 0.875309407711029
Epoch 900, training loss: 626.6741333007812 = 0.49561354517936707 + 100.0 * 6.26178503036499
Epoch 900, val loss: 0.8713763356208801
Epoch 910, training loss: 626.489013671875 = 0.4849867522716522 + 100.0 * 6.260040283203125
Epoch 910, val loss: 0.8680672645568848
Epoch 920, training loss: 626.3261108398438 = 0.4746115207672119 + 100.0 * 6.258514881134033
Epoch 920, val loss: 0.86487877368927
Epoch 930, training loss: 626.7459106445312 = 0.46443572640419006 + 100.0 * 6.262814998626709
Epoch 930, val loss: 0.8615829944610596
Epoch 940, training loss: 626.3788452148438 = 0.45428237318992615 + 100.0 * 6.2592453956604
Epoch 940, val loss: 0.8586422801017761
Epoch 950, training loss: 626.2184448242188 = 0.44432657957077026 + 100.0 * 6.2577409744262695
Epoch 950, val loss: 0.8556451797485352
Epoch 960, training loss: 625.9963989257812 = 0.4346993565559387 + 100.0 * 6.255617141723633
Epoch 960, val loss: 0.8532353639602661
Epoch 970, training loss: 625.9161376953125 = 0.4252837002277374 + 100.0 * 6.254908561706543
Epoch 970, val loss: 0.8508557081222534
Epoch 980, training loss: 626.4512939453125 = 0.4160127341747284 + 100.0 * 6.260353088378906
Epoch 980, val loss: 0.8486634492874146
Epoch 990, training loss: 625.8433837890625 = 0.40684881806373596 + 100.0 * 6.254364967346191
Epoch 990, val loss: 0.8463171124458313
Epoch 1000, training loss: 626.2957153320312 = 0.39792031049728394 + 100.0 * 6.25897741317749
Epoch 1000, val loss: 0.8441941738128662
Epoch 1010, training loss: 625.72119140625 = 0.3890960216522217 + 100.0 * 6.253320693969727
Epoch 1010, val loss: 0.842432975769043
Epoch 1020, training loss: 625.5526123046875 = 0.380621075630188 + 100.0 * 6.251719951629639
Epoch 1020, val loss: 0.8409527540206909
Epoch 1030, training loss: 625.4766235351562 = 0.3723285496234894 + 100.0 * 6.25104284286499
Epoch 1030, val loss: 0.8397181034088135
Epoch 1040, training loss: 626.0963134765625 = 0.36418092250823975 + 100.0 * 6.257321357727051
Epoch 1040, val loss: 0.8382818102836609
Epoch 1050, training loss: 625.5364379882812 = 0.3561735153198242 + 100.0 * 6.251802444458008
Epoch 1050, val loss: 0.8372126817703247
Epoch 1060, training loss: 625.3807983398438 = 0.3483833372592926 + 100.0 * 6.250324249267578
Epoch 1060, val loss: 0.8363265991210938
Epoch 1070, training loss: 625.6175537109375 = 0.34083372354507446 + 100.0 * 6.252767086029053
Epoch 1070, val loss: 0.8354604840278625
Epoch 1080, training loss: 625.2181396484375 = 0.3333549201488495 + 100.0 * 6.248847484588623
Epoch 1080, val loss: 0.8347851634025574
Epoch 1090, training loss: 625.0244750976562 = 0.3261604905128479 + 100.0 * 6.246983051300049
Epoch 1090, val loss: 0.8342601656913757
Epoch 1100, training loss: 624.9218139648438 = 0.3191709816455841 + 100.0 * 6.246026515960693
Epoch 1100, val loss: 0.8339661359786987
Epoch 1110, training loss: 625.0044555664062 = 0.31238096952438354 + 100.0 * 6.246921062469482
Epoch 1110, val loss: 0.8339309096336365
Epoch 1120, training loss: 625.5120239257812 = 0.3055652379989624 + 100.0 * 6.2520647048950195
Epoch 1120, val loss: 0.8335978388786316
Epoch 1130, training loss: 625.0504150390625 = 0.29881858825683594 + 100.0 * 6.247515678405762
Epoch 1130, val loss: 0.8330398797988892
Epoch 1140, training loss: 624.81201171875 = 0.2924128472805023 + 100.0 * 6.2451958656311035
Epoch 1140, val loss: 0.8335020542144775
Epoch 1150, training loss: 624.6080932617188 = 0.2862168550491333 + 100.0 * 6.243218898773193
Epoch 1150, val loss: 0.8338426351547241
Epoch 1160, training loss: 624.5709228515625 = 0.28019577264785767 + 100.0 * 6.2429070472717285
Epoch 1160, val loss: 0.8343325853347778
Epoch 1170, training loss: 625.441650390625 = 0.274333655834198 + 100.0 * 6.251673221588135
Epoch 1170, val loss: 0.8350036144256592
Epoch 1180, training loss: 624.9996337890625 = 0.2683318257331848 + 100.0 * 6.247313022613525
Epoch 1180, val loss: 0.8351696729660034
Epoch 1190, training loss: 624.5191040039062 = 0.26258334517478943 + 100.0 * 6.242565155029297
Epoch 1190, val loss: 0.8358334302902222
Epoch 1200, training loss: 624.3363037109375 = 0.25708770751953125 + 100.0 * 6.240792274475098
Epoch 1200, val loss: 0.8367667198181152
Epoch 1210, training loss: 624.301513671875 = 0.2517518401145935 + 100.0 * 6.240497589111328
Epoch 1210, val loss: 0.8378608822822571
Epoch 1220, training loss: 624.7935180664062 = 0.2465543895959854 + 100.0 * 6.245469093322754
Epoch 1220, val loss: 0.8390966653823853
Epoch 1230, training loss: 624.3995361328125 = 0.24129673838615417 + 100.0 * 6.241582870483398
Epoch 1230, val loss: 0.8395644426345825
Epoch 1240, training loss: 624.2864379882812 = 0.23623201251029968 + 100.0 * 6.24050235748291
Epoch 1240, val loss: 0.841045618057251
Epoch 1250, training loss: 624.2116088867188 = 0.23134063184261322 + 100.0 * 6.239802837371826
Epoch 1250, val loss: 0.8421207070350647
Epoch 1260, training loss: 624.4395141601562 = 0.22657740116119385 + 100.0 * 6.242129802703857
Epoch 1260, val loss: 0.8435184359550476
Epoch 1270, training loss: 624.1270141601562 = 0.2218388319015503 + 100.0 * 6.239051342010498
Epoch 1270, val loss: 0.8449602723121643
Epoch 1280, training loss: 623.932373046875 = 0.21728214621543884 + 100.0 * 6.237151145935059
Epoch 1280, val loss: 0.8465590476989746
Epoch 1290, training loss: 623.9475708007812 = 0.21285103261470795 + 100.0 * 6.23734712600708
Epoch 1290, val loss: 0.8481302857398987
Epoch 1300, training loss: 624.75439453125 = 0.2085278481245041 + 100.0 * 6.245458126068115
Epoch 1300, val loss: 0.8498329520225525
Epoch 1310, training loss: 624.1819458007812 = 0.20413075387477875 + 100.0 * 6.239778518676758
Epoch 1310, val loss: 0.85111403465271
Epoch 1320, training loss: 623.774658203125 = 0.19994959235191345 + 100.0 * 6.23574686050415
Epoch 1320, val loss: 0.852999746799469
Epoch 1330, training loss: 623.6858520507812 = 0.19590482115745544 + 100.0 * 6.234899044036865
Epoch 1330, val loss: 0.8548920154571533
Epoch 1340, training loss: 623.83056640625 = 0.19197294116020203 + 100.0 * 6.236385822296143
Epoch 1340, val loss: 0.8568904995918274
Epoch 1350, training loss: 624.0480346679688 = 0.18804970383644104 + 100.0 * 6.23859977722168
Epoch 1350, val loss: 0.8584784865379333
Epoch 1360, training loss: 623.6912841796875 = 0.18415509164333344 + 100.0 * 6.235071659088135
Epoch 1360, val loss: 0.8603811264038086
Epoch 1370, training loss: 623.5853881835938 = 0.18042637407779694 + 100.0 * 6.2340497970581055
Epoch 1370, val loss: 0.8623966574668884
Epoch 1380, training loss: 623.510009765625 = 0.17683923244476318 + 100.0 * 6.23333215713501
Epoch 1380, val loss: 0.8645444512367249
Epoch 1390, training loss: 623.5352783203125 = 0.17333737015724182 + 100.0 * 6.233619213104248
Epoch 1390, val loss: 0.8666937947273254
Epoch 1400, training loss: 623.9732055664062 = 0.1698984056711197 + 100.0 * 6.238032817840576
Epoch 1400, val loss: 0.8688057661056519
Epoch 1410, training loss: 623.478759765625 = 0.1664232611656189 + 100.0 * 6.233123302459717
Epoch 1410, val loss: 0.8706720471382141
Epoch 1420, training loss: 623.3643188476562 = 0.16309985518455505 + 100.0 * 6.2320122718811035
Epoch 1420, val loss: 0.8727813363075256
Epoch 1430, training loss: 623.5794067382812 = 0.15990768373012543 + 100.0 * 6.234194755554199
Epoch 1430, val loss: 0.8750797510147095
Epoch 1440, training loss: 623.264892578125 = 0.15672412514686584 + 100.0 * 6.231081485748291
Epoch 1440, val loss: 0.8772359490394592
Epoch 1450, training loss: 623.2459716796875 = 0.15365223586559296 + 100.0 * 6.230923652648926
Epoch 1450, val loss: 0.8794968128204346
Epoch 1460, training loss: 623.1619262695312 = 0.15066908299922943 + 100.0 * 6.230112552642822
Epoch 1460, val loss: 0.881877601146698
Epoch 1470, training loss: 623.2408447265625 = 0.147755429148674 + 100.0 * 6.230930805206299
Epoch 1470, val loss: 0.8841243982315063
Epoch 1480, training loss: 623.7493896484375 = 0.14485962688922882 + 100.0 * 6.2360453605651855
Epoch 1480, val loss: 0.8862151503562927
Epoch 1490, training loss: 623.1532592773438 = 0.14198115468025208 + 100.0 * 6.2301130294799805
Epoch 1490, val loss: 0.8884931802749634
Epoch 1500, training loss: 623.0789184570312 = 0.1392061710357666 + 100.0 * 6.229396820068359
Epoch 1500, val loss: 0.89091956615448
Epoch 1510, training loss: 622.9999389648438 = 0.13653425872325897 + 100.0 * 6.228633880615234
Epoch 1510, val loss: 0.8932517170906067
Epoch 1520, training loss: 622.9734497070312 = 0.1339440494775772 + 100.0 * 6.228394508361816
Epoch 1520, val loss: 0.8958582282066345
Epoch 1530, training loss: 623.4957885742188 = 0.13139238953590393 + 100.0 * 6.233643531799316
Epoch 1530, val loss: 0.8982018232345581
Epoch 1540, training loss: 622.9859619140625 = 0.1288396418094635 + 100.0 * 6.228570938110352
Epoch 1540, val loss: 0.9006530046463013
Epoch 1550, training loss: 623.380126953125 = 0.12636972963809967 + 100.0 * 6.232537746429443
Epoch 1550, val loss: 0.90310138463974
Epoch 1560, training loss: 623.0980834960938 = 0.12391302734613419 + 100.0 * 6.22974157333374
Epoch 1560, val loss: 0.9053741693496704
Epoch 1570, training loss: 622.8588256835938 = 0.12152628600597382 + 100.0 * 6.227373123168945
Epoch 1570, val loss: 0.9079466462135315
Epoch 1580, training loss: 622.8096923828125 = 0.11924746632575989 + 100.0 * 6.226904392242432
Epoch 1580, val loss: 0.9106170535087585
Epoch 1590, training loss: 623.238525390625 = 0.11698949337005615 + 100.0 * 6.231215000152588
Epoch 1590, val loss: 0.9131044745445251
Epoch 1600, training loss: 622.8373413085938 = 0.11473680287599564 + 100.0 * 6.2272257804870605
Epoch 1600, val loss: 0.9157503247261047
Epoch 1610, training loss: 622.5952758789062 = 0.11257568746805191 + 100.0 * 6.224827289581299
Epoch 1610, val loss: 0.9184862971305847
Epoch 1620, training loss: 622.5773315429688 = 0.11048850417137146 + 100.0 * 6.224668502807617
Epoch 1620, val loss: 0.9213083386421204
Epoch 1630, training loss: 623.0933227539062 = 0.10846175253391266 + 100.0 * 6.229848861694336
Epoch 1630, val loss: 0.9241623282432556
Epoch 1640, training loss: 622.5761108398438 = 0.10636439174413681 + 100.0 * 6.224697113037109
Epoch 1640, val loss: 0.9263777732849121
Epoch 1650, training loss: 622.5410766601562 = 0.10436893254518509 + 100.0 * 6.224367141723633
Epoch 1650, val loss: 0.929252564907074
Epoch 1660, training loss: 622.6034545898438 = 0.10243979841470718 + 100.0 * 6.225010395050049
Epoch 1660, val loss: 0.9320430159568787
Epoch 1670, training loss: 622.722900390625 = 0.10051792860031128 + 100.0 * 6.226223945617676
Epoch 1670, val loss: 0.9349905252456665
Epoch 1680, training loss: 622.49072265625 = 0.09859468042850494 + 100.0 * 6.223921775817871
Epoch 1680, val loss: 0.9380282759666443
Epoch 1690, training loss: 622.404541015625 = 0.09674184024333954 + 100.0 * 6.22307825088501
Epoch 1690, val loss: 0.9412808418273926
Epoch 1700, training loss: 622.893310546875 = 0.09493636339902878 + 100.0 * 6.2279839515686035
Epoch 1700, val loss: 0.9442893266677856
Epoch 1710, training loss: 622.4200439453125 = 0.09313073009252548 + 100.0 * 6.223268985748291
Epoch 1710, val loss: 0.9476028680801392
Epoch 1720, training loss: 622.4867553710938 = 0.09138382971286774 + 100.0 * 6.223953723907471
Epoch 1720, val loss: 0.9504481554031372
Epoch 1730, training loss: 622.3412475585938 = 0.08967001736164093 + 100.0 * 6.22251558303833
Epoch 1730, val loss: 0.9534134268760681
Epoch 1740, training loss: 622.3963012695312 = 0.0880160704255104 + 100.0 * 6.223083019256592
Epoch 1740, val loss: 0.9563442468643188
Epoch 1750, training loss: 622.434814453125 = 0.08639580011367798 + 100.0 * 6.223484039306641
Epoch 1750, val loss: 0.959444522857666
Epoch 1760, training loss: 622.32861328125 = 0.08479701727628708 + 100.0 * 6.222438335418701
Epoch 1760, val loss: 0.9624722003936768
Epoch 1770, training loss: 622.2584228515625 = 0.08324645459651947 + 100.0 * 6.221751689910889
Epoch 1770, val loss: 0.9655786752700806
Epoch 1780, training loss: 622.3020629882812 = 0.08172565698623657 + 100.0 * 6.222203254699707
Epoch 1780, val loss: 0.9685931205749512
Epoch 1790, training loss: 622.436279296875 = 0.0802416205406189 + 100.0 * 6.223560333251953
Epoch 1790, val loss: 0.9715096950531006
Epoch 1800, training loss: 622.33203125 = 0.07876352965831757 + 100.0 * 6.222532749176025
Epoch 1800, val loss: 0.9744869470596313
Epoch 1810, training loss: 622.1033325195312 = 0.07731983065605164 + 100.0 * 6.220260143280029
Epoch 1810, val loss: 0.9774806499481201
Epoch 1820, training loss: 622.0086059570312 = 0.07593327015638351 + 100.0 * 6.219326972961426
Epoch 1820, val loss: 0.9806103706359863
Epoch 1830, training loss: 622.278076171875 = 0.07458989322185516 + 100.0 * 6.222034931182861
Epoch 1830, val loss: 0.9837954044342041
Epoch 1840, training loss: 622.1661987304688 = 0.07321808487176895 + 100.0 * 6.2209296226501465
Epoch 1840, val loss: 0.9866977334022522
Epoch 1850, training loss: 621.9603271484375 = 0.07186705619096756 + 100.0 * 6.2188849449157715
Epoch 1850, val loss: 0.9895131587982178
Epoch 1860, training loss: 621.896484375 = 0.0705910176038742 + 100.0 * 6.218258857727051
Epoch 1860, val loss: 0.9927275776863098
Epoch 1870, training loss: 621.9550170898438 = 0.06935365498065948 + 100.0 * 6.2188568115234375
Epoch 1870, val loss: 0.9958024621009827
Epoch 1880, training loss: 622.296142578125 = 0.068137988448143 + 100.0 * 6.222280502319336
Epoch 1880, val loss: 0.9989290833473206
Epoch 1890, training loss: 622.34375 = 0.06691880524158478 + 100.0 * 6.222768783569336
Epoch 1890, val loss: 1.0021483898162842
Epoch 1900, training loss: 621.84814453125 = 0.06570609658956528 + 100.0 * 6.2178239822387695
Epoch 1900, val loss: 1.0047955513000488
Epoch 1910, training loss: 621.835693359375 = 0.06455770879983902 + 100.0 * 6.217711448669434
Epoch 1910, val loss: 1.0080534219741821
Epoch 1920, training loss: 622.0993041992188 = 0.0634390190243721 + 100.0 * 6.220358848571777
Epoch 1920, val loss: 1.011261224746704
Epoch 1930, training loss: 621.7213745117188 = 0.062315914779901505 + 100.0 * 6.216590404510498
Epoch 1930, val loss: 1.0140615701675415
Epoch 1940, training loss: 621.7488403320312 = 0.061241086572408676 + 100.0 * 6.216876029968262
Epoch 1940, val loss: 1.0173791646957397
Epoch 1950, training loss: 622.0781860351562 = 0.060197170823812485 + 100.0 * 6.220180034637451
Epoch 1950, val loss: 1.0206502676010132
Epoch 1960, training loss: 621.753662109375 = 0.059129368513822556 + 100.0 * 6.216945171356201
Epoch 1960, val loss: 1.0232019424438477
Epoch 1970, training loss: 621.96923828125 = 0.05810745060443878 + 100.0 * 6.219111442565918
Epoch 1970, val loss: 1.0266345739364624
Epoch 1980, training loss: 621.716796875 = 0.05707671865820885 + 100.0 * 6.216597557067871
Epoch 1980, val loss: 1.0291789770126343
Epoch 1990, training loss: 621.700439453125 = 0.0560859851539135 + 100.0 * 6.2164435386657715
Epoch 1990, val loss: 1.0324474573135376
Epoch 2000, training loss: 621.6030883789062 = 0.055139634758234024 + 100.0 * 6.215479850769043
Epoch 2000, val loss: 1.0355576276779175
Epoch 2010, training loss: 621.5057983398438 = 0.05421607941389084 + 100.0 * 6.2145161628723145
Epoch 2010, val loss: 1.0387290716171265
Epoch 2020, training loss: 621.6966552734375 = 0.053319744765758514 + 100.0 * 6.216433048248291
Epoch 2020, val loss: 1.0417970418930054
Epoch 2030, training loss: 621.9669799804688 = 0.052399538457393646 + 100.0 * 6.21914529800415
Epoch 2030, val loss: 1.0448120832443237
Epoch 2040, training loss: 621.7595825195312 = 0.05147648975253105 + 100.0 * 6.217081069946289
Epoch 2040, val loss: 1.047561526298523
Epoch 2050, training loss: 621.4598999023438 = 0.05060592293739319 + 100.0 * 6.21409273147583
Epoch 2050, val loss: 1.0507920980453491
Epoch 2060, training loss: 621.4266357421875 = 0.049778085201978683 + 100.0 * 6.213768482208252
Epoch 2060, val loss: 1.0540456771850586
Epoch 2070, training loss: 621.6835327148438 = 0.048973001539707184 + 100.0 * 6.21634578704834
Epoch 2070, val loss: 1.05728280544281
Epoch 2080, training loss: 621.4989013671875 = 0.04814452677965164 + 100.0 * 6.214507579803467
Epoch 2080, val loss: 1.0600625276565552
Epoch 2090, training loss: 621.5078735351562 = 0.047336552292108536 + 100.0 * 6.21460485458374
Epoch 2090, val loss: 1.063014268875122
Epoch 2100, training loss: 621.7451171875 = 0.04656004533171654 + 100.0 * 6.216985702514648
Epoch 2100, val loss: 1.0661276578903198
Epoch 2110, training loss: 621.38525390625 = 0.04578288272023201 + 100.0 * 6.213394641876221
Epoch 2110, val loss: 1.0689940452575684
Epoch 2120, training loss: 621.391845703125 = 0.045041777193546295 + 100.0 * 6.213468074798584
Epoch 2120, val loss: 1.0719995498657227
Epoch 2130, training loss: 621.3796997070312 = 0.04431921988725662 + 100.0 * 6.213353633880615
Epoch 2130, val loss: 1.0751980543136597
Epoch 2140, training loss: 621.3222045898438 = 0.04361004754900932 + 100.0 * 6.212785720825195
Epoch 2140, val loss: 1.0782623291015625
Epoch 2150, training loss: 621.2774047851562 = 0.042913272976875305 + 100.0 * 6.212345123291016
Epoch 2150, val loss: 1.0814188718795776
Epoch 2160, training loss: 622.1102905273438 = 0.042243070900440216 + 100.0 * 6.2206807136535645
Epoch 2160, val loss: 1.0844966173171997
Epoch 2170, training loss: 621.4335327148438 = 0.04153505340218544 + 100.0 * 6.213919639587402
Epoch 2170, val loss: 1.087037205696106
Epoch 2180, training loss: 621.2369384765625 = 0.04086458683013916 + 100.0 * 6.211960315704346
Epoch 2180, val loss: 1.0901201963424683
Epoch 2190, training loss: 621.4928588867188 = 0.04022538661956787 + 100.0 * 6.214526176452637
Epoch 2190, val loss: 1.0933046340942383
Epoch 2200, training loss: 621.2908325195312 = 0.03958427160978317 + 100.0 * 6.212512493133545
Epoch 2200, val loss: 1.0960419178009033
Epoch 2210, training loss: 621.4484252929688 = 0.038962721824645996 + 100.0 * 6.214095115661621
Epoch 2210, val loss: 1.0990118980407715
Epoch 2220, training loss: 621.1354370117188 = 0.03835029900074005 + 100.0 * 6.210971355438232
Epoch 2220, val loss: 1.1018946170806885
Epoch 2230, training loss: 621.173583984375 = 0.037763092666864395 + 100.0 * 6.211358070373535
Epoch 2230, val loss: 1.105004906654358
Epoch 2240, training loss: 621.4837036132812 = 0.037189703434705734 + 100.0 * 6.214465618133545
Epoch 2240, val loss: 1.1080228090286255
Epoch 2250, training loss: 621.2662353515625 = 0.036603156477212906 + 100.0 * 6.212296009063721
Epoch 2250, val loss: 1.1104336977005005
Epoch 2260, training loss: 621.3411254882812 = 0.03604196757078171 + 100.0 * 6.213050842285156
Epoch 2260, val loss: 1.11343514919281
Epoch 2270, training loss: 621.3015747070312 = 0.03548527881503105 + 100.0 * 6.212661266326904
Epoch 2270, val loss: 1.1162971258163452
Epoch 2280, training loss: 621.0421752929688 = 0.034944724291563034 + 100.0 * 6.210072040557861
Epoch 2280, val loss: 1.1195716857910156
Epoch 2290, training loss: 621.0994873046875 = 0.034424882382154465 + 100.0 * 6.210650444030762
Epoch 2290, val loss: 1.1223819255828857
Epoch 2300, training loss: 621.0745239257812 = 0.03391411155462265 + 100.0 * 6.2104058265686035
Epoch 2300, val loss: 1.1254642009735107
Epoch 2310, training loss: 621.2978515625 = 0.033413179218769073 + 100.0 * 6.212644577026367
Epoch 2310, val loss: 1.1281983852386475
Epoch 2320, training loss: 621.2570190429688 = 0.03290434181690216 + 100.0 * 6.212241172790527
Epoch 2320, val loss: 1.1311777830123901
Epoch 2330, training loss: 621.1322021484375 = 0.03240180388092995 + 100.0 * 6.210998058319092
Epoch 2330, val loss: 1.1338683366775513
Epoch 2340, training loss: 620.9669799804688 = 0.0319150947034359 + 100.0 * 6.2093505859375
Epoch 2340, val loss: 1.1366450786590576
Epoch 2350, training loss: 620.9024658203125 = 0.03145284578204155 + 100.0 * 6.208710193634033
Epoch 2350, val loss: 1.139522910118103
Epoch 2360, training loss: 620.8851318359375 = 0.031004907563328743 + 100.0 * 6.208541393280029
Epoch 2360, val loss: 1.1424615383148193
Epoch 2370, training loss: 621.364501953125 = 0.030569151043891907 + 100.0 * 6.213339328765869
Epoch 2370, val loss: 1.1452243328094482
Epoch 2380, training loss: 621.2343139648438 = 0.030110079795122147 + 100.0 * 6.212042331695557
Epoch 2380, val loss: 1.1478322744369507
Epoch 2390, training loss: 620.8958740234375 = 0.029656482860445976 + 100.0 * 6.208662033081055
Epoch 2390, val loss: 1.1504281759262085
Epoch 2400, training loss: 620.8128662109375 = 0.029230855405330658 + 100.0 * 6.207836151123047
Epoch 2400, val loss: 1.1531485319137573
Epoch 2410, training loss: 620.7996215820312 = 0.028825385496020317 + 100.0 * 6.207708358764648
Epoch 2410, val loss: 1.1559404134750366
Epoch 2420, training loss: 621.0178833007812 = 0.028428368270397186 + 100.0 * 6.20989465713501
Epoch 2420, val loss: 1.158652663230896
Epoch 2430, training loss: 620.994140625 = 0.028021687641739845 + 100.0 * 6.20966100692749
Epoch 2430, val loss: 1.1613131761550903
Epoch 2440, training loss: 620.9735107421875 = 0.02762344665825367 + 100.0 * 6.209458827972412
Epoch 2440, val loss: 1.1640503406524658
Epoch 2450, training loss: 620.9223022460938 = 0.027239317074418068 + 100.0 * 6.208950519561768
Epoch 2450, val loss: 1.1665656566619873
Epoch 2460, training loss: 620.94287109375 = 0.02686220034956932 + 100.0 * 6.209160327911377
Epoch 2460, val loss: 1.1695936918258667
Epoch 2470, training loss: 620.774658203125 = 0.026491686701774597 + 100.0 * 6.207481384277344
Epoch 2470, val loss: 1.1722822189331055
Epoch 2480, training loss: 620.8150634765625 = 0.026132535189390182 + 100.0 * 6.207889556884766
Epoch 2480, val loss: 1.1750173568725586
Epoch 2490, training loss: 620.9765625 = 0.025779396295547485 + 100.0 * 6.209507942199707
Epoch 2490, val loss: 1.1774245500564575
Epoch 2500, training loss: 621.0943603515625 = 0.02542421780526638 + 100.0 * 6.210689067840576
Epoch 2500, val loss: 1.179673671722412
Epoch 2510, training loss: 620.7338256835938 = 0.02506929263472557 + 100.0 * 6.207087516784668
Epoch 2510, val loss: 1.1824043989181519
Epoch 2520, training loss: 620.6697387695312 = 0.024735284969210625 + 100.0 * 6.206450462341309
Epoch 2520, val loss: 1.184868574142456
Epoch 2530, training loss: 620.6049194335938 = 0.024411683902144432 + 100.0 * 6.205804824829102
Epoch 2530, val loss: 1.1876944303512573
Epoch 2540, training loss: 621.0578002929688 = 0.02409871108829975 + 100.0 * 6.210337162017822
Epoch 2540, val loss: 1.1900490522384644
Epoch 2550, training loss: 620.7616577148438 = 0.023770876228809357 + 100.0 * 6.20737886428833
Epoch 2550, val loss: 1.1926548480987549
Epoch 2560, training loss: 620.845458984375 = 0.02345268987119198 + 100.0 * 6.208220481872559
Epoch 2560, val loss: 1.1952362060546875
Epoch 2570, training loss: 620.6456909179688 = 0.02313842438161373 + 100.0 * 6.206225395202637
Epoch 2570, val loss: 1.1976470947265625
Epoch 2580, training loss: 620.602294921875 = 0.022837603464722633 + 100.0 * 6.205794811248779
Epoch 2580, val loss: 1.2000635862350464
Epoch 2590, training loss: 620.642333984375 = 0.02255125902593136 + 100.0 * 6.206198215484619
Epoch 2590, val loss: 1.2024827003479004
Epoch 2600, training loss: 620.8160400390625 = 0.022265665233135223 + 100.0 * 6.207937717437744
Epoch 2600, val loss: 1.204879641532898
Epoch 2610, training loss: 620.7210693359375 = 0.021975474432110786 + 100.0 * 6.206990718841553
Epoch 2610, val loss: 1.2077444791793823
Epoch 2620, training loss: 620.7598266601562 = 0.021697858348488808 + 100.0 * 6.207381725311279
Epoch 2620, val loss: 1.2099108695983887
Epoch 2630, training loss: 620.6542358398438 = 0.02141929417848587 + 100.0 * 6.20632791519165
Epoch 2630, val loss: 1.2126333713531494
Epoch 2640, training loss: 620.62646484375 = 0.02114773541688919 + 100.0 * 6.206053256988525
Epoch 2640, val loss: 1.214656949043274
Epoch 2650, training loss: 620.6685791015625 = 0.02088574692606926 + 100.0 * 6.206477165222168
Epoch 2650, val loss: 1.2173900604248047
Epoch 2660, training loss: 620.5789184570312 = 0.020617913454771042 + 100.0 * 6.205583095550537
Epoch 2660, val loss: 1.219542384147644
Epoch 2670, training loss: 620.4600830078125 = 0.020360026508569717 + 100.0 * 6.204397201538086
Epoch 2670, val loss: 1.2217836380004883
Epoch 2680, training loss: 620.4462890625 = 0.020114850252866745 + 100.0 * 6.204261779785156
Epoch 2680, val loss: 1.2243653535842896
Epoch 2690, training loss: 620.659423828125 = 0.01987622305750847 + 100.0 * 6.206395149230957
Epoch 2690, val loss: 1.2266662120819092
Epoch 2700, training loss: 620.3994750976562 = 0.019629981368780136 + 100.0 * 6.203798294067383
Epoch 2700, val loss: 1.2289774417877197
Epoch 2710, training loss: 620.46435546875 = 0.019395390525460243 + 100.0 * 6.20444917678833
Epoch 2710, val loss: 1.231276273727417
Epoch 2720, training loss: 620.7056884765625 = 0.01916845701634884 + 100.0 * 6.206865310668945
Epoch 2720, val loss: 1.233517050743103
Epoch 2730, training loss: 621.1334228515625 = 0.01893322356045246 + 100.0 * 6.211144924163818
Epoch 2730, val loss: 1.236119270324707
Epoch 2740, training loss: 620.5921020507812 = 0.018683388829231262 + 100.0 * 6.2057342529296875
Epoch 2740, val loss: 1.2373071908950806
Epoch 2750, training loss: 620.3748779296875 = 0.01846129633486271 + 100.0 * 6.203564643859863
Epoch 2750, val loss: 1.2403181791305542
Epoch 2760, training loss: 620.2779541015625 = 0.018250437453389168 + 100.0 * 6.202597141265869
Epoch 2760, val loss: 1.242358922958374
Epoch 2770, training loss: 620.2268676757812 = 0.018045222386717796 + 100.0 * 6.202087879180908
Epoch 2770, val loss: 1.2448915243148804
Epoch 2780, training loss: 620.3018798828125 = 0.017846211791038513 + 100.0 * 6.202840805053711
Epoch 2780, val loss: 1.2470115423202515
Epoch 2790, training loss: 621.0344848632812 = 0.017646897584199905 + 100.0 * 6.210168361663818
Epoch 2790, val loss: 1.2488081455230713
Epoch 2800, training loss: 620.3685913085938 = 0.01743009313941002 + 100.0 * 6.2035112380981445
Epoch 2800, val loss: 1.251186490058899
Epoch 2810, training loss: 620.2091674804688 = 0.017226487398147583 + 100.0 * 6.2019195556640625
Epoch 2810, val loss: 1.253617525100708
Epoch 2820, training loss: 620.2252197265625 = 0.017037151381373405 + 100.0 * 6.20208215713501
Epoch 2820, val loss: 1.2556085586547852
Epoch 2830, training loss: 621.193603515625 = 0.016855329275131226 + 100.0 * 6.211767673492432
Epoch 2830, val loss: 1.257780909538269
Epoch 2840, training loss: 620.5333862304688 = 0.016649624332785606 + 100.0 * 6.205167293548584
Epoch 2840, val loss: 1.2596160173416138
Epoch 2850, training loss: 620.24169921875 = 0.016458800062537193 + 100.0 * 6.202252388000488
Epoch 2850, val loss: 1.261917233467102
Epoch 2860, training loss: 620.1695556640625 = 0.01628105901181698 + 100.0 * 6.201532363891602
Epoch 2860, val loss: 1.2642451524734497
Epoch 2870, training loss: 620.2240600585938 = 0.016108782961964607 + 100.0 * 6.2020792961120605
Epoch 2870, val loss: 1.2664892673492432
Epoch 2880, training loss: 620.7289428710938 = 0.015937838703393936 + 100.0 * 6.207129955291748
Epoch 2880, val loss: 1.2682946920394897
Epoch 2890, training loss: 620.58251953125 = 0.015755563974380493 + 100.0 * 6.205667972564697
Epoch 2890, val loss: 1.2701393365859985
Epoch 2900, training loss: 620.23681640625 = 0.015580074861645699 + 100.0 * 6.202212333679199
Epoch 2900, val loss: 1.272343635559082
Epoch 2910, training loss: 620.1705322265625 = 0.015413696877658367 + 100.0 * 6.2015509605407715
Epoch 2910, val loss: 1.2746113538742065
Epoch 2920, training loss: 620.4190673828125 = 0.01525711826980114 + 100.0 * 6.204038143157959
Epoch 2920, val loss: 1.2768172025680542
Epoch 2930, training loss: 620.11669921875 = 0.015088312327861786 + 100.0 * 6.201015949249268
Epoch 2930, val loss: 1.2787761688232422
Epoch 2940, training loss: 620.1207885742188 = 0.014928639866411686 + 100.0 * 6.201058387756348
Epoch 2940, val loss: 1.2806683778762817
Epoch 2950, training loss: 620.1698608398438 = 0.014775499701499939 + 100.0 * 6.2015509605407715
Epoch 2950, val loss: 1.2827785015106201
Epoch 2960, training loss: 620.73291015625 = 0.01462624967098236 + 100.0 * 6.20718240737915
Epoch 2960, val loss: 1.2843763828277588
Epoch 2970, training loss: 620.3637084960938 = 0.014466357417404652 + 100.0 * 6.203492164611816
Epoch 2970, val loss: 1.2862523794174194
Epoch 2980, training loss: 620.112548828125 = 0.014308848418295383 + 100.0 * 6.200982570648193
Epoch 2980, val loss: 1.288578987121582
Epoch 2990, training loss: 620.0401611328125 = 0.014165603555738926 + 100.0 * 6.200260162353516
Epoch 2990, val loss: 1.2906298637390137
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8392198207696363
The final CL Acc:0.71975, 0.01259, The final GNN Acc:0.83940, 0.00025
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9460])
updated graph: torch.Size([2, 10534])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6395263671875 = 1.9560989141464233 + 100.0 * 8.596834182739258
Epoch 0, val loss: 1.9575986862182617
Epoch 10, training loss: 861.5531616210938 = 1.9475878477096558 + 100.0 * 8.59605598449707
Epoch 10, val loss: 1.949564814567566
Epoch 20, training loss: 860.9979248046875 = 1.936941146850586 + 100.0 * 8.590609550476074
Epoch 20, val loss: 1.9391005039215088
Epoch 30, training loss: 857.4312133789062 = 1.9234477281570435 + 100.0 * 8.55507755279541
Epoch 30, val loss: 1.925441026687622
Epoch 40, training loss: 836.5052490234375 = 1.9075918197631836 + 100.0 * 8.345976829528809
Epoch 40, val loss: 1.9092737436294556
Epoch 50, training loss: 761.4186401367188 = 1.890600562095642 + 100.0 * 7.595280170440674
Epoch 50, val loss: 1.891760230064392
Epoch 60, training loss: 740.402587890625 = 1.8757812976837158 + 100.0 * 7.385268211364746
Epoch 60, val loss: 1.8779387474060059
Epoch 70, training loss: 719.4035034179688 = 1.863834261894226 + 100.0 * 7.175396919250488
Epoch 70, val loss: 1.8664685487747192
Epoch 80, training loss: 706.4932861328125 = 1.8532443046569824 + 100.0 * 7.04640007019043
Epoch 80, val loss: 1.8562945127487183
Epoch 90, training loss: 694.7667846679688 = 1.8443536758422852 + 100.0 * 6.929224491119385
Epoch 90, val loss: 1.8474440574645996
Epoch 100, training loss: 683.8911743164062 = 1.83639657497406 + 100.0 * 6.820547580718994
Epoch 100, val loss: 1.839181900024414
Epoch 110, training loss: 675.6865844726562 = 1.829190969467163 + 100.0 * 6.738574028015137
Epoch 110, val loss: 1.8309838771820068
Epoch 120, training loss: 669.1278686523438 = 1.8219361305236816 + 100.0 * 6.673058986663818
Epoch 120, val loss: 1.823207139968872
Epoch 130, training loss: 664.6730346679688 = 1.8154208660125732 + 100.0 * 6.628575801849365
Epoch 130, val loss: 1.8159512281417847
Epoch 140, training loss: 661.016357421875 = 1.8091952800750732 + 100.0 * 6.592071533203125
Epoch 140, val loss: 1.8090788125991821
Epoch 150, training loss: 657.8311157226562 = 1.8031563758850098 + 100.0 * 6.560279369354248
Epoch 150, val loss: 1.8025071620941162
Epoch 160, training loss: 655.0465087890625 = 1.7972588539123535 + 100.0 * 6.532492637634277
Epoch 160, val loss: 1.7961997985839844
Epoch 170, training loss: 652.8991088867188 = 1.7912802696228027 + 100.0 * 6.511078357696533
Epoch 170, val loss: 1.789962887763977
Epoch 180, training loss: 650.887939453125 = 1.7849454879760742 + 100.0 * 6.491029739379883
Epoch 180, val loss: 1.783724069595337
Epoch 190, training loss: 649.089599609375 = 1.7782412767410278 + 100.0 * 6.473113536834717
Epoch 190, val loss: 1.7772948741912842
Epoch 200, training loss: 647.6138305664062 = 1.7710394859313965 + 100.0 * 6.458427429199219
Epoch 200, val loss: 1.770460605621338
Epoch 210, training loss: 646.5280151367188 = 1.763235330581665 + 100.0 * 6.447647571563721
Epoch 210, val loss: 1.7631845474243164
Epoch 220, training loss: 645.7633666992188 = 1.7547004222869873 + 100.0 * 6.440086364746094
Epoch 220, val loss: 1.755301594734192
Epoch 230, training loss: 644.443359375 = 1.7453434467315674 + 100.0 * 6.426980018615723
Epoch 230, val loss: 1.7469171285629272
Epoch 240, training loss: 643.4265747070312 = 1.7352817058563232 + 100.0 * 6.41691255569458
Epoch 240, val loss: 1.7378472089767456
Epoch 250, training loss: 642.5494995117188 = 1.7243876457214355 + 100.0 * 6.4082512855529785
Epoch 250, val loss: 1.7282116413116455
Epoch 260, training loss: 641.7158203125 = 1.712694764137268 + 100.0 * 6.400031089782715
Epoch 260, val loss: 1.7178579568862915
Epoch 270, training loss: 641.33251953125 = 1.6999239921569824 + 100.0 * 6.396325588226318
Epoch 270, val loss: 1.70671546459198
Epoch 280, training loss: 640.3949584960938 = 1.6862019300460815 + 100.0 * 6.387087345123291
Epoch 280, val loss: 1.6945865154266357
Epoch 290, training loss: 639.6293334960938 = 1.6714868545532227 + 100.0 * 6.379578113555908
Epoch 290, val loss: 1.6817086935043335
Epoch 300, training loss: 639.09521484375 = 1.6558090448379517 + 100.0 * 6.374393939971924
Epoch 300, val loss: 1.6680922508239746
Epoch 310, training loss: 638.4441528320312 = 1.6391428709030151 + 100.0 * 6.3680500984191895
Epoch 310, val loss: 1.6535462141036987
Epoch 320, training loss: 638.0277709960938 = 1.621529459953308 + 100.0 * 6.364062786102295
Epoch 320, val loss: 1.6383088827133179
Epoch 330, training loss: 637.4483642578125 = 1.6031674146652222 + 100.0 * 6.358452320098877
Epoch 330, val loss: 1.6224098205566406
Epoch 340, training loss: 636.9740600585938 = 1.5841503143310547 + 100.0 * 6.353899002075195
Epoch 340, val loss: 1.605950117111206
Epoch 350, training loss: 636.8199462890625 = 1.5644272565841675 + 100.0 * 6.352554798126221
Epoch 350, val loss: 1.5890617370605469
Epoch 360, training loss: 636.263916015625 = 1.544207215309143 + 100.0 * 6.347197532653809
Epoch 360, val loss: 1.5717079639434814
Epoch 370, training loss: 635.8206787109375 = 1.523598313331604 + 100.0 * 6.342970848083496
Epoch 370, val loss: 1.554100513458252
Epoch 380, training loss: 635.2993774414062 = 1.5026923418045044 + 100.0 * 6.3379669189453125
Epoch 380, val loss: 1.5364702939987183
Epoch 390, training loss: 635.4126586914062 = 1.4818686246871948 + 100.0 * 6.33930778503418
Epoch 390, val loss: 1.5189533233642578
Epoch 400, training loss: 634.584716796875 = 1.4606146812438965 + 100.0 * 6.331240653991699
Epoch 400, val loss: 1.5013643503189087
Epoch 410, training loss: 634.1627197265625 = 1.439720630645752 + 100.0 * 6.327230453491211
Epoch 410, val loss: 1.4840337038040161
Epoch 420, training loss: 633.8075561523438 = 1.4190279245376587 + 100.0 * 6.323885440826416
Epoch 420, val loss: 1.4670476913452148
Epoch 430, training loss: 634.1220703125 = 1.3986132144927979 + 100.0 * 6.327234268188477
Epoch 430, val loss: 1.4503084421157837
Epoch 440, training loss: 633.3848266601562 = 1.377996563911438 + 100.0 * 6.320068359375
Epoch 440, val loss: 1.4340335130691528
Epoch 450, training loss: 633.0969848632812 = 1.357877254486084 + 100.0 * 6.317391395568848
Epoch 450, val loss: 1.4180134534835815
Epoch 460, training loss: 632.6487426757812 = 1.3380205631256104 + 100.0 * 6.313107013702393
Epoch 460, val loss: 1.4022037982940674
Epoch 470, training loss: 632.3428955078125 = 1.3184285163879395 + 100.0 * 6.310245037078857
Epoch 470, val loss: 1.3870474100112915
Epoch 480, training loss: 632.4154052734375 = 1.2991966009140015 + 100.0 * 6.311161994934082
Epoch 480, val loss: 1.3721414804458618
Epoch 490, training loss: 631.9274291992188 = 1.2800085544586182 + 100.0 * 6.306473731994629
Epoch 490, val loss: 1.3576633930206299
Epoch 500, training loss: 631.709716796875 = 1.2612273693084717 + 100.0 * 6.304484844207764
Epoch 500, val loss: 1.3432379961013794
Epoch 510, training loss: 631.4612426757812 = 1.242592215538025 + 100.0 * 6.302186012268066
Epoch 510, val loss: 1.3292992115020752
Epoch 520, training loss: 631.244873046875 = 1.2241833209991455 + 100.0 * 6.300207138061523
Epoch 520, val loss: 1.3156899213790894
Epoch 530, training loss: 630.939208984375 = 1.2059636116027832 + 100.0 * 6.297332286834717
Epoch 530, val loss: 1.3022156953811646
Epoch 540, training loss: 631.0159912109375 = 1.1878647804260254 + 100.0 * 6.298281192779541
Epoch 540, val loss: 1.2889013290405273
Epoch 550, training loss: 630.7025756835938 = 1.1699939966201782 + 100.0 * 6.295325756072998
Epoch 550, val loss: 1.2756212949752808
Epoch 560, training loss: 630.32421875 = 1.1523329019546509 + 100.0 * 6.291718482971191
Epoch 560, val loss: 1.2626867294311523
Epoch 570, training loss: 630.1023559570312 = 1.1348292827606201 + 100.0 * 6.289675235748291
Epoch 570, val loss: 1.2502995729446411
Epoch 580, training loss: 629.9955444335938 = 1.1175479888916016 + 100.0 * 6.2887797355651855
Epoch 580, val loss: 1.2380868196487427
Epoch 590, training loss: 629.9890747070312 = 1.1003003120422363 + 100.0 * 6.288887977600098
Epoch 590, val loss: 1.2258920669555664
Epoch 600, training loss: 629.6929321289062 = 1.0833162069320679 + 100.0 * 6.286096096038818
Epoch 600, val loss: 1.213896632194519
Epoch 610, training loss: 629.5873413085938 = 1.0663741827011108 + 100.0 * 6.285210132598877
Epoch 610, val loss: 1.2020992040634155
Epoch 620, training loss: 629.3552856445312 = 1.0496044158935547 + 100.0 * 6.283056735992432
Epoch 620, val loss: 1.190919280052185
Epoch 630, training loss: 629.149169921875 = 1.0331439971923828 + 100.0 * 6.281160354614258
Epoch 630, val loss: 1.179689884185791
Epoch 640, training loss: 629.2559814453125 = 1.0167416334152222 + 100.0 * 6.282392501831055
Epoch 640, val loss: 1.1689972877502441
Epoch 650, training loss: 629.287109375 = 1.0005450248718262 + 100.0 * 6.282865524291992
Epoch 650, val loss: 1.1581450700759888
Epoch 660, training loss: 628.7440795898438 = 0.9844902753829956 + 100.0 * 6.2775959968566895
Epoch 660, val loss: 1.147632122039795
Epoch 670, training loss: 628.5205078125 = 0.968651533126831 + 100.0 * 6.275518894195557
Epoch 670, val loss: 1.1377984285354614
Epoch 680, training loss: 628.3807373046875 = 0.95316481590271 + 100.0 * 6.274275302886963
Epoch 680, val loss: 1.1281139850616455
Epoch 690, training loss: 628.873046875 = 0.9378905892372131 + 100.0 * 6.279351711273193
Epoch 690, val loss: 1.1187450885772705
Epoch 700, training loss: 628.1415405273438 = 0.9225047826766968 + 100.0 * 6.272190570831299
Epoch 700, val loss: 1.1096923351287842
Epoch 710, training loss: 628.0150146484375 = 0.9076459407806396 + 100.0 * 6.271073341369629
Epoch 710, val loss: 1.1008445024490356
Epoch 720, training loss: 627.8497924804688 = 0.8930640816688538 + 100.0 * 6.269567012786865
Epoch 720, val loss: 1.0927932262420654
Epoch 730, training loss: 628.767333984375 = 0.8787399530410767 + 100.0 * 6.278885841369629
Epoch 730, val loss: 1.0848774909973145
Epoch 740, training loss: 627.7047729492188 = 0.8642945885658264 + 100.0 * 6.268404483795166
Epoch 740, val loss: 1.0768816471099854
Epoch 750, training loss: 627.4904174804688 = 0.850248396396637 + 100.0 * 6.266402244567871
Epoch 750, val loss: 1.0695832967758179
Epoch 760, training loss: 627.3850708007812 = 0.8366256356239319 + 100.0 * 6.265484809875488
Epoch 760, val loss: 1.0627851486206055
Epoch 770, training loss: 627.593994140625 = 0.8232734799385071 + 100.0 * 6.267706871032715
Epoch 770, val loss: 1.0563197135925293
Epoch 780, training loss: 627.4390258789062 = 0.8099404573440552 + 100.0 * 6.266290664672852
Epoch 780, val loss: 1.0499181747436523
Epoch 790, training loss: 627.162841796875 = 0.7969598174095154 + 100.0 * 6.2636590003967285
Epoch 790, val loss: 1.043860912322998
Epoch 800, training loss: 626.9764404296875 = 0.7842354774475098 + 100.0 * 6.2619218826293945
Epoch 800, val loss: 1.0382359027862549
Epoch 810, training loss: 627.2659301757812 = 0.771804928779602 + 100.0 * 6.264941215515137
Epoch 810, val loss: 1.0328718423843384
Epoch 820, training loss: 626.8267211914062 = 0.7593005299568176 + 100.0 * 6.260674476623535
Epoch 820, val loss: 1.0274989604949951
Epoch 830, training loss: 626.8323974609375 = 0.7471796870231628 + 100.0 * 6.260851860046387
Epoch 830, val loss: 1.0225417613983154
Epoch 840, training loss: 626.5780639648438 = 0.7351968884468079 + 100.0 * 6.258429050445557
Epoch 840, val loss: 1.0179893970489502
Epoch 850, training loss: 626.540283203125 = 0.7234020233154297 + 100.0 * 6.258168697357178
Epoch 850, val loss: 1.0137563943862915
Epoch 860, training loss: 626.8731079101562 = 0.7118874788284302 + 100.0 * 6.2616119384765625
Epoch 860, val loss: 1.0096827745437622
Epoch 870, training loss: 626.6532592773438 = 0.7003880739212036 + 100.0 * 6.259529113769531
Epoch 870, val loss: 1.0051807165145874
Epoch 880, training loss: 626.2394409179688 = 0.6890931725502014 + 100.0 * 6.2555036544799805
Epoch 880, val loss: 1.001624584197998
Epoch 890, training loss: 626.139892578125 = 0.6780611276626587 + 100.0 * 6.2546186447143555
Epoch 890, val loss: 0.9981415867805481
Epoch 900, training loss: 626.8086547851562 = 0.6671954989433289 + 100.0 * 6.261415004730225
Epoch 900, val loss: 0.9944834113121033
Epoch 910, training loss: 626.16162109375 = 0.656318724155426 + 100.0 * 6.2550530433654785
Epoch 910, val loss: 0.9913821816444397
Epoch 920, training loss: 626.3555908203125 = 0.6456443071365356 + 100.0 * 6.257099628448486
Epoch 920, val loss: 0.98820561170578
Epoch 930, training loss: 625.8339233398438 = 0.6351839303970337 + 100.0 * 6.251987457275391
Epoch 930, val loss: 0.9851089119911194
Epoch 940, training loss: 625.7274169921875 = 0.6248923540115356 + 100.0 * 6.251025676727295
Epoch 940, val loss: 0.9823243021965027
Epoch 950, training loss: 625.8077392578125 = 0.6147527694702148 + 100.0 * 6.251929759979248
Epoch 950, val loss: 0.9798274040222168
Epoch 960, training loss: 625.94677734375 = 0.6047040820121765 + 100.0 * 6.253420829772949
Epoch 960, val loss: 0.9775344133377075
Epoch 970, training loss: 625.5945434570312 = 0.5946621298789978 + 100.0 * 6.249998569488525
Epoch 970, val loss: 0.9746840000152588
Epoch 980, training loss: 625.5143432617188 = 0.5848170518875122 + 100.0 * 6.249295234680176
Epoch 980, val loss: 0.9727292656898499
Epoch 990, training loss: 625.393310546875 = 0.5751901268959045 + 100.0 * 6.248180866241455
Epoch 990, val loss: 0.9705467224121094
Epoch 1000, training loss: 625.569091796875 = 0.5656716823577881 + 100.0 * 6.250034332275391
Epoch 1000, val loss: 0.9687831401824951
Epoch 1010, training loss: 625.2393188476562 = 0.5562145709991455 + 100.0 * 6.246830940246582
Epoch 1010, val loss: 0.9664953351020813
Epoch 1020, training loss: 625.184814453125 = 0.5469050407409668 + 100.0 * 6.246379375457764
Epoch 1020, val loss: 0.9648615121841431
Epoch 1030, training loss: 625.3197021484375 = 0.5376839637756348 + 100.0 * 6.2478203773498535
Epoch 1030, val loss: 0.9632447361946106
Epoch 1040, training loss: 625.0863647460938 = 0.5285660028457642 + 100.0 * 6.245577812194824
Epoch 1040, val loss: 0.9617751240730286
Epoch 1050, training loss: 625.091064453125 = 0.5195008516311646 + 100.0 * 6.245715618133545
Epoch 1050, val loss: 0.9602752923965454
Epoch 1060, training loss: 625.0875244140625 = 0.5106448531150818 + 100.0 * 6.245769023895264
Epoch 1060, val loss: 0.9591507315635681
Epoch 1070, training loss: 624.9312744140625 = 0.5018423199653625 + 100.0 * 6.2442946434021
Epoch 1070, val loss: 0.9580411314964294
Epoch 1080, training loss: 624.9090576171875 = 0.4931615889072418 + 100.0 * 6.244158744812012
Epoch 1080, val loss: 0.9570544362068176
Epoch 1090, training loss: 624.8333129882812 = 0.4846053719520569 + 100.0 * 6.2434868812561035
Epoch 1090, val loss: 0.9561372995376587
Epoch 1100, training loss: 625.3782958984375 = 0.47607263922691345 + 100.0 * 6.249022006988525
Epoch 1100, val loss: 0.9552849531173706
Epoch 1110, training loss: 624.695556640625 = 0.46765902638435364 + 100.0 * 6.242279052734375
Epoch 1110, val loss: 0.9540317058563232
Epoch 1120, training loss: 624.56005859375 = 0.45936164259910583 + 100.0 * 6.241007328033447
Epoch 1120, val loss: 0.9534296989440918
Epoch 1130, training loss: 624.4909057617188 = 0.4512912631034851 + 100.0 * 6.240396022796631
Epoch 1130, val loss: 0.9530616998672485
Epoch 1140, training loss: 624.45458984375 = 0.44334226846694946 + 100.0 * 6.2401123046875
Epoch 1140, val loss: 0.9525968432426453
Epoch 1150, training loss: 625.1939697265625 = 0.43544310331344604 + 100.0 * 6.247585296630859
Epoch 1150, val loss: 0.9522036910057068
Epoch 1160, training loss: 624.5663452148438 = 0.42750775814056396 + 100.0 * 6.241388320922852
Epoch 1160, val loss: 0.9516081809997559
Epoch 1170, training loss: 624.384521484375 = 0.4197500944137573 + 100.0 * 6.23964786529541
Epoch 1170, val loss: 0.9514501690864563
Epoch 1180, training loss: 624.1966552734375 = 0.41220200061798096 + 100.0 * 6.237843990325928
Epoch 1180, val loss: 0.9514920711517334
Epoch 1190, training loss: 624.1856079101562 = 0.404817134141922 + 100.0 * 6.237807750701904
Epoch 1190, val loss: 0.9517890214920044
Epoch 1200, training loss: 624.871337890625 = 0.39744773507118225 + 100.0 * 6.244739055633545
Epoch 1200, val loss: 0.9519153833389282
Epoch 1210, training loss: 624.1556396484375 = 0.39014774560928345 + 100.0 * 6.237655162811279
Epoch 1210, val loss: 0.95164954662323
Epoch 1220, training loss: 624.0260620117188 = 0.38299211859703064 + 100.0 * 6.236430644989014
Epoch 1220, val loss: 0.9518292546272278
Epoch 1230, training loss: 623.9924926757812 = 0.375968337059021 + 100.0 * 6.2361650466918945
Epoch 1230, val loss: 0.9526035785675049
Epoch 1240, training loss: 624.1137084960938 = 0.36911723017692566 + 100.0 * 6.237445831298828
Epoch 1240, val loss: 0.9530555009841919
Epoch 1250, training loss: 623.8751220703125 = 0.3623034954071045 + 100.0 * 6.235127925872803
Epoch 1250, val loss: 0.9536347389221191
Epoch 1260, training loss: 624.0060424804688 = 0.3556341230869293 + 100.0 * 6.236504077911377
Epoch 1260, val loss: 0.9542577862739563
Epoch 1270, training loss: 623.9616088867188 = 0.34901756048202515 + 100.0 * 6.236125946044922
Epoch 1270, val loss: 0.9551516771316528
Epoch 1280, training loss: 624.1182861328125 = 0.3425326347351074 + 100.0 * 6.237757682800293
Epoch 1280, val loss: 0.9560719728469849
Epoch 1290, training loss: 623.8388061523438 = 0.33609163761138916 + 100.0 * 6.235026836395264
Epoch 1290, val loss: 0.9567147493362427
Epoch 1300, training loss: 624.0562133789062 = 0.3298446834087372 + 100.0 * 6.2372636795043945
Epoch 1300, val loss: 0.9579907655715942
Epoch 1310, training loss: 623.607666015625 = 0.32357025146484375 + 100.0 * 6.2328410148620605
Epoch 1310, val loss: 0.9589629173278809
Epoch 1320, training loss: 623.56201171875 = 0.3175099194049835 + 100.0 * 6.232444763183594
Epoch 1320, val loss: 0.9604809880256653
Epoch 1330, training loss: 623.675537109375 = 0.3115779459476471 + 100.0 * 6.233639717102051
Epoch 1330, val loss: 0.961769163608551
Epoch 1340, training loss: 623.8313598632812 = 0.3057226538658142 + 100.0 * 6.235256195068359
Epoch 1340, val loss: 0.9631800055503845
Epoch 1350, training loss: 623.3795166015625 = 0.29978859424591064 + 100.0 * 6.230797290802002
Epoch 1350, val loss: 0.964540958404541
Epoch 1360, training loss: 623.327392578125 = 0.2941209077835083 + 100.0 * 6.230332851409912
Epoch 1360, val loss: 0.9663228392601013
Epoch 1370, training loss: 623.5862426757812 = 0.28856849670410156 + 100.0 * 6.23297643661499
Epoch 1370, val loss: 0.9682753682136536
Epoch 1380, training loss: 623.3284912109375 = 0.283052921295166 + 100.0 * 6.230453968048096
Epoch 1380, val loss: 0.9700660705566406
Epoch 1390, training loss: 623.3849487304688 = 0.27764222025871277 + 100.0 * 6.231072902679443
Epoch 1390, val loss: 0.972119152545929
Epoch 1400, training loss: 623.2815551757812 = 0.2723392844200134 + 100.0 * 6.2300920486450195
Epoch 1400, val loss: 0.9741329550743103
Epoch 1410, training loss: 623.1849975585938 = 0.2671118676662445 + 100.0 * 6.2291789054870605
Epoch 1410, val loss: 0.9764673113822937
Epoch 1420, training loss: 623.6876831054688 = 0.26201799511909485 + 100.0 * 6.234256744384766
Epoch 1420, val loss: 0.97862708568573
Epoch 1430, training loss: 623.2354736328125 = 0.2568639814853668 + 100.0 * 6.229786396026611
Epoch 1430, val loss: 0.9808790683746338
Epoch 1440, training loss: 623.02734375 = 0.25192439556121826 + 100.0 * 6.227753639221191
Epoch 1440, val loss: 0.9833508729934692
Epoch 1450, training loss: 622.9481811523438 = 0.2470855712890625 + 100.0 * 6.227011203765869
Epoch 1450, val loss: 0.9859760403633118
Epoch 1460, training loss: 623.3980712890625 = 0.2424081712961197 + 100.0 * 6.231556415557861
Epoch 1460, val loss: 0.9886333346366882
Epoch 1470, training loss: 623.3965454101562 = 0.2376365214586258 + 100.0 * 6.231589317321777
Epoch 1470, val loss: 0.9911302328109741
Epoch 1480, training loss: 622.9848022460938 = 0.23295339941978455 + 100.0 * 6.227518558502197
Epoch 1480, val loss: 0.9936671257019043
Epoch 1490, training loss: 622.7427978515625 = 0.2283838987350464 + 100.0 * 6.225143909454346
Epoch 1490, val loss: 0.9964404106140137
Epoch 1500, training loss: 622.72412109375 = 0.22396673262119293 + 100.0 * 6.225001811981201
Epoch 1500, val loss: 0.9994199275970459
Epoch 1510, training loss: 622.707275390625 = 0.21965144574642181 + 100.0 * 6.2248759269714355
Epoch 1510, val loss: 1.002446174621582
Epoch 1520, training loss: 623.6972045898438 = 0.21534782648086548 + 100.0 * 6.234818935394287
Epoch 1520, val loss: 1.0052834749221802
Epoch 1530, training loss: 622.8106689453125 = 0.21108557283878326 + 100.0 * 6.225996017456055
Epoch 1530, val loss: 1.0079509019851685
Epoch 1540, training loss: 622.5528564453125 = 0.20690670609474182 + 100.0 * 6.223459720611572
Epoch 1540, val loss: 1.0112420320510864
Epoch 1550, training loss: 623.1795043945312 = 0.2028825581073761 + 100.0 * 6.229766368865967
Epoch 1550, val loss: 1.014273762702942
Epoch 1560, training loss: 622.6537475585938 = 0.19885003566741943 + 100.0 * 6.224548816680908
Epoch 1560, val loss: 1.0174998044967651
Epoch 1570, training loss: 622.4920654296875 = 0.19493065774440765 + 100.0 * 6.222971439361572
Epoch 1570, val loss: 1.0205795764923096
Epoch 1580, training loss: 622.3932495117188 = 0.19112831354141235 + 100.0 * 6.222021579742432
Epoch 1580, val loss: 1.0240627527236938
Epoch 1590, training loss: 622.8985595703125 = 0.18740330636501312 + 100.0 * 6.22711181640625
Epoch 1590, val loss: 1.027192234992981
Epoch 1600, training loss: 622.5100708007812 = 0.1836310476064682 + 100.0 * 6.223264217376709
Epoch 1600, val loss: 1.0305241346359253
Epoch 1610, training loss: 622.525634765625 = 0.17992863059043884 + 100.0 * 6.223456859588623
Epoch 1610, val loss: 1.0335761308670044
Epoch 1620, training loss: 622.3162841796875 = 0.17638687789440155 + 100.0 * 6.221398830413818
Epoch 1620, val loss: 1.0373115539550781
Epoch 1630, training loss: 622.5955810546875 = 0.17293938994407654 + 100.0 * 6.224226951599121
Epoch 1630, val loss: 1.0410945415496826
Epoch 1640, training loss: 622.1753540039062 = 0.1694677174091339 + 100.0 * 6.220058441162109
Epoch 1640, val loss: 1.044122576713562
Epoch 1650, training loss: 622.20166015625 = 0.16609960794448853 + 100.0 * 6.220355987548828
Epoch 1650, val loss: 1.048069953918457
Epoch 1660, training loss: 622.5301513671875 = 0.16283893585205078 + 100.0 * 6.223672866821289
Epoch 1660, val loss: 1.0516059398651123
Epoch 1670, training loss: 622.3516235351562 = 0.1595485508441925 + 100.0 * 6.221920967102051
Epoch 1670, val loss: 1.055331826210022
Epoch 1680, training loss: 622.2320556640625 = 0.15634705126285553 + 100.0 * 6.220756530761719
Epoch 1680, val loss: 1.0589215755462646
Epoch 1690, training loss: 622.093017578125 = 0.1532323658466339 + 100.0 * 6.21939754486084
Epoch 1690, val loss: 1.0626155138015747
Epoch 1700, training loss: 622.0213012695312 = 0.15020182728767395 + 100.0 * 6.218710899353027
Epoch 1700, val loss: 1.0663033723831177
Epoch 1710, training loss: 622.1893920898438 = 0.1472392976284027 + 100.0 * 6.22042179107666
Epoch 1710, val loss: 1.0702329874038696
Epoch 1720, training loss: 622.1641235351562 = 0.1442732959985733 + 100.0 * 6.220198154449463
Epoch 1720, val loss: 1.0738848447799683
Epoch 1730, training loss: 622.17529296875 = 0.14134468138217926 + 100.0 * 6.220339298248291
Epoch 1730, val loss: 1.077772617340088
Epoch 1740, training loss: 622.2951049804688 = 0.13849610090255737 + 100.0 * 6.221566200256348
Epoch 1740, val loss: 1.0814392566680908
Epoch 1750, training loss: 621.9280395507812 = 0.13572584092617035 + 100.0 * 6.217922687530518
Epoch 1750, val loss: 1.0855947732925415
Epoch 1760, training loss: 621.814697265625 = 0.13302171230316162 + 100.0 * 6.2168169021606445
Epoch 1760, val loss: 1.089561939239502
Epoch 1770, training loss: 621.89453125 = 0.1303928643465042 + 100.0 * 6.217641830444336
Epoch 1770, val loss: 1.0936247110366821
Epoch 1780, training loss: 622.2619018554688 = 0.12780560553073883 + 100.0 * 6.221340656280518
Epoch 1780, val loss: 1.097641110420227
Epoch 1790, training loss: 622.0146484375 = 0.12522049248218536 + 100.0 * 6.218894004821777
Epoch 1790, val loss: 1.1015831232070923
Epoch 1800, training loss: 621.8685302734375 = 0.12270542979240417 + 100.0 * 6.217458724975586
Epoch 1800, val loss: 1.10556161403656
Epoch 1810, training loss: 621.836669921875 = 0.12026391923427582 + 100.0 * 6.217164039611816
Epoch 1810, val loss: 1.1098881959915161
Epoch 1820, training loss: 621.891845703125 = 0.11787667125463486 + 100.0 * 6.217740058898926
Epoch 1820, val loss: 1.114084005355835
Epoch 1830, training loss: 621.94970703125 = 0.11553361266851425 + 100.0 * 6.218341827392578
Epoch 1830, val loss: 1.1179882287979126
Epoch 1840, training loss: 621.7636108398438 = 0.1132119819521904 + 100.0 * 6.216504096984863
Epoch 1840, val loss: 1.1220977306365967
Epoch 1850, training loss: 621.5625610351562 = 0.11094147711992264 + 100.0 * 6.2145161628723145
Epoch 1850, val loss: 1.1262160539627075
Epoch 1860, training loss: 621.5444946289062 = 0.10876956582069397 + 100.0 * 6.214357376098633
Epoch 1860, val loss: 1.1305443048477173
Epoch 1870, training loss: 621.7462158203125 = 0.10664722323417664 + 100.0 * 6.216395854949951
Epoch 1870, val loss: 1.1345700025558472
Epoch 1880, training loss: 621.7130126953125 = 0.10452034324407578 + 100.0 * 6.216085433959961
Epoch 1880, val loss: 1.1387814283370972
Epoch 1890, training loss: 621.6203002929688 = 0.10246638208627701 + 100.0 * 6.215178489685059
Epoch 1890, val loss: 1.1431394815444946
Epoch 1900, training loss: 621.8140258789062 = 0.10044515877962112 + 100.0 * 6.217135906219482
Epoch 1900, val loss: 1.1474393606185913
Epoch 1910, training loss: 621.5657958984375 = 0.09847112745046616 + 100.0 * 6.2146735191345215
Epoch 1910, val loss: 1.1517155170440674
Epoch 1920, training loss: 621.4884033203125 = 0.09652052074670792 + 100.0 * 6.213919162750244
Epoch 1920, val loss: 1.1559573411941528
Epoch 1930, training loss: 621.6831665039062 = 0.0946597084403038 + 100.0 * 6.215885162353516
Epoch 1930, val loss: 1.1605029106140137
Epoch 1940, training loss: 621.3592529296875 = 0.0927850753068924 + 100.0 * 6.212664604187012
Epoch 1940, val loss: 1.1645523309707642
Epoch 1950, training loss: 621.3108520507812 = 0.09097307175397873 + 100.0 * 6.212198257446289
Epoch 1950, val loss: 1.1689748764038086
Epoch 1960, training loss: 621.284912109375 = 0.08922085165977478 + 100.0 * 6.211956977844238
Epoch 1960, val loss: 1.1733698844909668
Epoch 1970, training loss: 621.529296875 = 0.08753452450037003 + 100.0 * 6.214417457580566
Epoch 1970, val loss: 1.1778417825698853
Epoch 1980, training loss: 621.3363647460938 = 0.085830919444561 + 100.0 * 6.212505340576172
Epoch 1980, val loss: 1.182115912437439
Epoch 1990, training loss: 621.3204345703125 = 0.08416914194822311 + 100.0 * 6.212362766265869
Epoch 1990, val loss: 1.186548113822937
Epoch 2000, training loss: 621.436279296875 = 0.08256132155656815 + 100.0 * 6.213536739349365
Epoch 2000, val loss: 1.1909300088882446
Epoch 2010, training loss: 621.2894287109375 = 0.08098135143518448 + 100.0 * 6.2120842933654785
Epoch 2010, val loss: 1.1952619552612305
Epoch 2020, training loss: 621.7283935546875 = 0.07948118448257446 + 100.0 * 6.216489315032959
Epoch 2020, val loss: 1.1998521089553833
Epoch 2030, training loss: 621.376708984375 = 0.0779154971241951 + 100.0 * 6.212987899780273
Epoch 2030, val loss: 1.2039366960525513
Epoch 2040, training loss: 621.2059326171875 = 0.07641848176717758 + 100.0 * 6.211295127868652
Epoch 2040, val loss: 1.2082632780075073
Epoch 2050, training loss: 621.0953979492188 = 0.07498951256275177 + 100.0 * 6.210203647613525
Epoch 2050, val loss: 1.2129052877426147
Epoch 2060, training loss: 621.071533203125 = 0.07360564917325974 + 100.0 * 6.20997953414917
Epoch 2060, val loss: 1.2174865007400513
Epoch 2070, training loss: 621.378662109375 = 0.0722634568810463 + 100.0 * 6.213064193725586
Epoch 2070, val loss: 1.2219926118850708
Epoch 2080, training loss: 621.3184814453125 = 0.07089844346046448 + 100.0 * 6.212475299835205
Epoch 2080, val loss: 1.2259433269500732
Epoch 2090, training loss: 621.2896118164062 = 0.06954162567853928 + 100.0 * 6.212201118469238
Epoch 2090, val loss: 1.2302865982055664
Epoch 2100, training loss: 621.0119018554688 = 0.0682535246014595 + 100.0 * 6.209436416625977
Epoch 2100, val loss: 1.2348461151123047
Epoch 2110, training loss: 620.980712890625 = 0.06701748818159103 + 100.0 * 6.209136962890625
Epoch 2110, val loss: 1.2394002676010132
Epoch 2120, training loss: 621.6209106445312 = 0.06584011763334274 + 100.0 * 6.215550422668457
Epoch 2120, val loss: 1.2439764738082886
Epoch 2130, training loss: 621.1686401367188 = 0.06457915157079697 + 100.0 * 6.211040496826172
Epoch 2130, val loss: 1.2478169202804565
Epoch 2140, training loss: 620.9879150390625 = 0.06338772177696228 + 100.0 * 6.209244728088379
Epoch 2140, val loss: 1.252179503440857
Epoch 2150, training loss: 620.9238891601562 = 0.06224263086915016 + 100.0 * 6.208616256713867
Epoch 2150, val loss: 1.2566959857940674
Epoch 2160, training loss: 621.1958618164062 = 0.06116195768117905 + 100.0 * 6.2113471031188965
Epoch 2160, val loss: 1.2611676454544067
Epoch 2170, training loss: 620.9486694335938 = 0.060040634125471115 + 100.0 * 6.20888614654541
Epoch 2170, val loss: 1.2652312517166138
Epoch 2180, training loss: 620.9559326171875 = 0.05895965173840523 + 100.0 * 6.208969593048096
Epoch 2180, val loss: 1.269675612449646
Epoch 2190, training loss: 620.9092407226562 = 0.05791759490966797 + 100.0 * 6.208513259887695
Epoch 2190, val loss: 1.273955225944519
Epoch 2200, training loss: 621.1905517578125 = 0.05691571906208992 + 100.0 * 6.211336135864258
Epoch 2200, val loss: 1.278172492980957
Epoch 2210, training loss: 620.9566650390625 = 0.055880412459373474 + 100.0 * 6.209007263183594
Epoch 2210, val loss: 1.2823219299316406
Epoch 2220, training loss: 620.798583984375 = 0.05488238483667374 + 100.0 * 6.207437038421631
Epoch 2220, val loss: 1.2864130735397339
Epoch 2230, training loss: 620.7783813476562 = 0.05393432453274727 + 100.0 * 6.207244396209717
Epoch 2230, val loss: 1.2906854152679443
Epoch 2240, training loss: 620.8948974609375 = 0.053009070456027985 + 100.0 * 6.208418846130371
Epoch 2240, val loss: 1.29499089717865
Epoch 2250, training loss: 620.9949340820312 = 0.05210021138191223 + 100.0 * 6.209427833557129
Epoch 2250, val loss: 1.2990788221359253
Epoch 2260, training loss: 620.8359985351562 = 0.051210131496191025 + 100.0 * 6.207847595214844
Epoch 2260, val loss: 1.3036073446273804
Epoch 2270, training loss: 620.8096313476562 = 0.05033702775835991 + 100.0 * 6.207592487335205
Epoch 2270, val loss: 1.3076353073120117
Epoch 2280, training loss: 620.9896850585938 = 0.04949132353067398 + 100.0 * 6.209402084350586
Epoch 2280, val loss: 1.3118975162506104
Epoch 2290, training loss: 621.260498046875 = 0.04866056144237518 + 100.0 * 6.212118625640869
Epoch 2290, val loss: 1.315827488899231
Epoch 2300, training loss: 620.8301391601562 = 0.0478062778711319 + 100.0 * 6.207823276519775
Epoch 2300, val loss: 1.3195511102676392
Epoch 2310, training loss: 620.6613159179688 = 0.047010064125061035 + 100.0 * 6.206143379211426
Epoch 2310, val loss: 1.3239120244979858
Epoch 2320, training loss: 620.6005249023438 = 0.04623846337199211 + 100.0 * 6.20554256439209
Epoch 2320, val loss: 1.3280071020126343
Epoch 2330, training loss: 620.7708129882812 = 0.04549923911690712 + 100.0 * 6.207253456115723
Epoch 2330, val loss: 1.332039713859558
Epoch 2340, training loss: 620.9190673828125 = 0.04475319758057594 + 100.0 * 6.208743572235107
Epoch 2340, val loss: 1.336153507232666
Epoch 2350, training loss: 620.7700805664062 = 0.04399760439991951 + 100.0 * 6.207260608673096
Epoch 2350, val loss: 1.3399769067764282
Epoch 2360, training loss: 620.5635986328125 = 0.04326246306300163 + 100.0 * 6.205203533172607
Epoch 2360, val loss: 1.3439915180206299
Epoch 2370, training loss: 620.4716796875 = 0.04256928339600563 + 100.0 * 6.204291343688965
Epoch 2370, val loss: 1.3482387065887451
Epoch 2380, training loss: 620.50927734375 = 0.041901592165231705 + 100.0 * 6.2046732902526855
Epoch 2380, val loss: 1.352325201034546
Epoch 2390, training loss: 621.3003540039062 = 0.04125722125172615 + 100.0 * 6.21259069442749
Epoch 2390, val loss: 1.356055498123169
Epoch 2400, training loss: 620.7813110351562 = 0.04057369753718376 + 100.0 * 6.207406997680664
Epoch 2400, val loss: 1.3597586154937744
Epoch 2410, training loss: 620.7623291015625 = 0.039932429790496826 + 100.0 * 6.207223892211914
Epoch 2410, val loss: 1.3638930320739746
Epoch 2420, training loss: 620.8076171875 = 0.03930380567908287 + 100.0 * 6.207683086395264
Epoch 2420, val loss: 1.367681860923767
Epoch 2430, training loss: 620.6163940429688 = 0.03866931423544884 + 100.0 * 6.205776691436768
Epoch 2430, val loss: 1.37122642993927
Epoch 2440, training loss: 620.3779907226562 = 0.03806595131754875 + 100.0 * 6.203399181365967
Epoch 2440, val loss: 1.3753936290740967
Epoch 2450, training loss: 620.406982421875 = 0.037491559982299805 + 100.0 * 6.203694820404053
Epoch 2450, val loss: 1.3792496919631958
Epoch 2460, training loss: 620.6200561523438 = 0.03692536801099777 + 100.0 * 6.205831050872803
Epoch 2460, val loss: 1.3828543424606323
Epoch 2470, training loss: 620.4829711914062 = 0.03635762631893158 + 100.0 * 6.204465866088867
Epoch 2470, val loss: 1.3863898515701294
Epoch 2480, training loss: 620.7181396484375 = 0.035820372402668 + 100.0 * 6.206823348999023
Epoch 2480, val loss: 1.3903546333312988
Epoch 2490, training loss: 620.8134155273438 = 0.03526322916150093 + 100.0 * 6.2077813148498535
Epoch 2490, val loss: 1.3940975666046143
Epoch 2500, training loss: 620.4955444335938 = 0.03472856432199478 + 100.0 * 6.204607963562012
Epoch 2500, val loss: 1.3977895975112915
Epoch 2510, training loss: 620.3588256835938 = 0.034205105155706406 + 100.0 * 6.203246593475342
Epoch 2510, val loss: 1.401453971862793
Epoch 2520, training loss: 620.513916015625 = 0.033719502389431 + 100.0 * 6.204802513122559
Epoch 2520, val loss: 1.4051023721694946
Epoch 2530, training loss: 620.353759765625 = 0.03321344032883644 + 100.0 * 6.203205585479736
Epoch 2530, val loss: 1.4089303016662598
Epoch 2540, training loss: 620.3923950195312 = 0.03272941708564758 + 100.0 * 6.203596591949463
Epoch 2540, val loss: 1.412553071975708
Epoch 2550, training loss: 620.392333984375 = 0.03225107118487358 + 100.0 * 6.203600883483887
Epoch 2550, val loss: 1.4160813093185425
Epoch 2560, training loss: 620.4601440429688 = 0.03177947551012039 + 100.0 * 6.204283237457275
Epoch 2560, val loss: 1.4194397926330566
Epoch 2570, training loss: 620.6353149414062 = 0.03131136670708656 + 100.0 * 6.206039905548096
Epoch 2570, val loss: 1.4225873947143555
Epoch 2580, training loss: 620.3240966796875 = 0.03085930459201336 + 100.0 * 6.202932357788086
Epoch 2580, val loss: 1.4266159534454346
Epoch 2590, training loss: 620.1886596679688 = 0.0304193664342165 + 100.0 * 6.201582908630371
Epoch 2590, val loss: 1.4300849437713623
Epoch 2600, training loss: 620.7066650390625 = 0.030011454597115517 + 100.0 * 6.206766605377197
Epoch 2600, val loss: 1.4336450099945068
Epoch 2610, training loss: 620.2473754882812 = 0.02956506237387657 + 100.0 * 6.202178478240967
Epoch 2610, val loss: 1.4369237422943115
Epoch 2620, training loss: 620.0748901367188 = 0.029139965772628784 + 100.0 * 6.200457572937012
Epoch 2620, val loss: 1.4404525756835938
Epoch 2630, training loss: 620.07373046875 = 0.028738392516970634 + 100.0 * 6.2004499435424805
Epoch 2630, val loss: 1.444023847579956
Epoch 2640, training loss: 620.2798461914062 = 0.028353802859783173 + 100.0 * 6.2025146484375
Epoch 2640, val loss: 1.447191596031189
Epoch 2650, training loss: 620.6364135742188 = 0.02796143852174282 + 100.0 * 6.206084728240967
Epoch 2650, val loss: 1.450104832649231
Epoch 2660, training loss: 620.1974487304688 = 0.02756989561021328 + 100.0 * 6.2016987800598145
Epoch 2660, val loss: 1.4537155628204346
Epoch 2670, training loss: 620.0499877929688 = 0.027185475453734398 + 100.0 * 6.200228214263916
Epoch 2670, val loss: 1.4573392868041992
Epoch 2680, training loss: 620.1275634765625 = 0.02682877890765667 + 100.0 * 6.20100736618042
Epoch 2680, val loss: 1.4608750343322754
Epoch 2690, training loss: 620.6429443359375 = 0.026482312008738518 + 100.0 * 6.206164836883545
Epoch 2690, val loss: 1.4641691446304321
Epoch 2700, training loss: 620.2545166015625 = 0.02610558085143566 + 100.0 * 6.20228385925293
Epoch 2700, val loss: 1.4673857688903809
Epoch 2710, training loss: 620.0108032226562 = 0.025751501321792603 + 100.0 * 6.199850559234619
Epoch 2710, val loss: 1.4706566333770752
Epoch 2720, training loss: 620.0294799804688 = 0.025419116020202637 + 100.0 * 6.200040817260742
Epoch 2720, val loss: 1.4740288257598877
Epoch 2730, training loss: 620.4796142578125 = 0.025097593665122986 + 100.0 * 6.204545021057129
Epoch 2730, val loss: 1.4769376516342163
Epoch 2740, training loss: 620.1548461914062 = 0.02476668357849121 + 100.0 * 6.201300621032715
Epoch 2740, val loss: 1.4801101684570312
Epoch 2750, training loss: 620.110107421875 = 0.024439875036478043 + 100.0 * 6.200857162475586
Epoch 2750, val loss: 1.483473777770996
Epoch 2760, training loss: 620.1858520507812 = 0.024128099903464317 + 100.0 * 6.201617240905762
Epoch 2760, val loss: 1.4866559505462646
Epoch 2770, training loss: 620.0733642578125 = 0.023816144093871117 + 100.0 * 6.200495719909668
Epoch 2770, val loss: 1.489965558052063
Epoch 2780, training loss: 620.0885009765625 = 0.023510051891207695 + 100.0 * 6.200649738311768
Epoch 2780, val loss: 1.4930024147033691
Epoch 2790, training loss: 620.06787109375 = 0.023210637271404266 + 100.0 * 6.200447082519531
Epoch 2790, val loss: 1.4962639808654785
Epoch 2800, training loss: 620.3193969726562 = 0.022918179631233215 + 100.0 * 6.202965259552002
Epoch 2800, val loss: 1.4989515542984009
Epoch 2810, training loss: 619.9747924804688 = 0.022630399093031883 + 100.0 * 6.199521541595459
Epoch 2810, val loss: 1.5025033950805664
Epoch 2820, training loss: 619.8741455078125 = 0.02234724722802639 + 100.0 * 6.198517799377441
Epoch 2820, val loss: 1.5055103302001953
Epoch 2830, training loss: 619.9689331054688 = 0.02207745611667633 + 100.0 * 6.19946813583374
Epoch 2830, val loss: 1.5085350275039673
Epoch 2840, training loss: 620.1765747070312 = 0.021815383806824684 + 100.0 * 6.201548099517822
Epoch 2840, val loss: 1.5115004777908325
Epoch 2850, training loss: 619.9627075195312 = 0.021542716771364212 + 100.0 * 6.199411869049072
Epoch 2850, val loss: 1.514988899230957
Epoch 2860, training loss: 620.3347778320312 = 0.021289819851517677 + 100.0 * 6.203135013580322
Epoch 2860, val loss: 1.5179754495620728
Epoch 2870, training loss: 619.8923950195312 = 0.021017801016569138 + 100.0 * 6.198714256286621
Epoch 2870, val loss: 1.5206724405288696
Epoch 2880, training loss: 620.1448974609375 = 0.02076609991490841 + 100.0 * 6.201241493225098
Epoch 2880, val loss: 1.5241619348526
Epoch 2890, training loss: 619.8666381835938 = 0.020508907735347748 + 100.0 * 6.198461532592773
Epoch 2890, val loss: 1.5266255140304565
Epoch 2900, training loss: 619.8073120117188 = 0.02026357688009739 + 100.0 * 6.19787073135376
Epoch 2900, val loss: 1.5300757884979248
Epoch 2910, training loss: 619.7385864257812 = 0.020031295716762543 + 100.0 * 6.197185516357422
Epoch 2910, val loss: 1.533186912536621
Epoch 2920, training loss: 620.0857543945312 = 0.019804183393716812 + 100.0 * 6.20065975189209
Epoch 2920, val loss: 1.535988211631775
Epoch 2930, training loss: 619.9838256835938 = 0.019564423710107803 + 100.0 * 6.199642181396484
Epoch 2930, val loss: 1.5385398864746094
Epoch 2940, training loss: 619.8165893554688 = 0.019333038479089737 + 100.0 * 6.197972297668457
Epoch 2940, val loss: 1.5416227579116821
Epoch 2950, training loss: 619.7384033203125 = 0.019105572253465652 + 100.0 * 6.197193145751953
Epoch 2950, val loss: 1.5449140071868896
Epoch 2960, training loss: 619.7190551757812 = 0.01889483630657196 + 100.0 * 6.1970014572143555
Epoch 2960, val loss: 1.5479602813720703
Epoch 2970, training loss: 619.8990478515625 = 0.018687846139073372 + 100.0 * 6.198803901672363
Epoch 2970, val loss: 1.5507774353027344
Epoch 2980, training loss: 619.86376953125 = 0.018470074981451035 + 100.0 * 6.198452472686768
Epoch 2980, val loss: 1.553381323814392
Epoch 2990, training loss: 619.9666137695312 = 0.018257012590765953 + 100.0 * 6.199483871459961
Epoch 2990, val loss: 1.5562247037887573
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 861.6224365234375 = 1.9389253854751587 + 100.0 * 8.596835136413574
Epoch 0, val loss: 1.9279059171676636
Epoch 10, training loss: 861.5330200195312 = 1.9300981760025024 + 100.0 * 8.596029281616211
Epoch 10, val loss: 1.9195441007614136
Epoch 20, training loss: 860.9738159179688 = 1.9190913438796997 + 100.0 * 8.590547561645508
Epoch 20, val loss: 1.9087753295898438
Epoch 30, training loss: 856.9215698242188 = 1.9051895141601562 + 100.0 * 8.550163269042969
Epoch 30, val loss: 1.8949661254882812
Epoch 40, training loss: 825.9149780273438 = 1.889351725578308 + 100.0 * 8.240256309509277
Epoch 40, val loss: 1.8797117471694946
Epoch 50, training loss: 754.3408813476562 = 1.872981071472168 + 100.0 * 7.524679183959961
Epoch 50, val loss: 1.864389181137085
Epoch 60, training loss: 719.3334350585938 = 1.8616338968276978 + 100.0 * 7.174717903137207
Epoch 60, val loss: 1.853384256362915
Epoch 70, training loss: 695.2901000976562 = 1.8515006303787231 + 100.0 * 6.934386253356934
Epoch 70, val loss: 1.8433876037597656
Epoch 80, training loss: 681.7200927734375 = 1.8433276414871216 + 100.0 * 6.798767566680908
Epoch 80, val loss: 1.8354803323745728
Epoch 90, training loss: 672.8284912109375 = 1.8349943161010742 + 100.0 * 6.709934711456299
Epoch 90, val loss: 1.8274272680282593
Epoch 100, training loss: 667.0286865234375 = 1.827109456062317 + 100.0 * 6.6520161628723145
Epoch 100, val loss: 1.819683313369751
Epoch 110, training loss: 663.031982421875 = 1.8195385932922363 + 100.0 * 6.612124443054199
Epoch 110, val loss: 1.8123528957366943
Epoch 120, training loss: 659.67041015625 = 1.812626838684082 + 100.0 * 6.578577995300293
Epoch 120, val loss: 1.8057096004486084
Epoch 130, training loss: 656.9790649414062 = 1.8062431812286377 + 100.0 * 6.55172872543335
Epoch 130, val loss: 1.79942786693573
Epoch 140, training loss: 655.0253295898438 = 1.7998965978622437 + 100.0 * 6.532253742218018
Epoch 140, val loss: 1.7930189371109009
Epoch 150, training loss: 652.6790161132812 = 1.7933154106140137 + 100.0 * 6.508856773376465
Epoch 150, val loss: 1.7865586280822754
Epoch 160, training loss: 650.7258911132812 = 1.7867149114608765 + 100.0 * 6.489391326904297
Epoch 160, val loss: 1.7800836563110352
Epoch 170, training loss: 649.8021240234375 = 1.779959797859192 + 100.0 * 6.480221271514893
Epoch 170, val loss: 1.7735799551010132
Epoch 180, training loss: 647.332275390625 = 1.7728513479232788 + 100.0 * 6.455594539642334
Epoch 180, val loss: 1.766758680343628
Epoch 190, training loss: 645.855224609375 = 1.7653536796569824 + 100.0 * 6.440898418426514
Epoch 190, val loss: 1.7597578763961792
Epoch 200, training loss: 644.9131469726562 = 1.7573738098144531 + 100.0 * 6.431557655334473
Epoch 200, val loss: 1.752292275428772
Epoch 210, training loss: 643.5997314453125 = 1.748652458190918 + 100.0 * 6.418510913848877
Epoch 210, val loss: 1.7442728281021118
Epoch 220, training loss: 642.6890258789062 = 1.7392897605895996 + 100.0 * 6.409496784210205
Epoch 220, val loss: 1.7357014417648315
Epoch 230, training loss: 642.114990234375 = 1.7291661500930786 + 100.0 * 6.403858184814453
Epoch 230, val loss: 1.7265026569366455
Epoch 240, training loss: 641.0877685546875 = 1.7183066606521606 + 100.0 * 6.3936944007873535
Epoch 240, val loss: 1.7166261672973633
Epoch 250, training loss: 640.3272094726562 = 1.706653356552124 + 100.0 * 6.386205196380615
Epoch 250, val loss: 1.7061176300048828
Epoch 260, training loss: 639.9039916992188 = 1.6941571235656738 + 100.0 * 6.382098197937012
Epoch 260, val loss: 1.6948304176330566
Epoch 270, training loss: 639.1819458007812 = 1.6806690692901611 + 100.0 * 6.3750128746032715
Epoch 270, val loss: 1.682877540588379
Epoch 280, training loss: 638.65283203125 = 1.6663854122161865 + 100.0 * 6.369864463806152
Epoch 280, val loss: 1.6701780557632446
Epoch 290, training loss: 638.3538208007812 = 1.6512253284454346 + 100.0 * 6.367025852203369
Epoch 290, val loss: 1.6567471027374268
Epoch 300, training loss: 637.6962890625 = 1.635055422782898 + 100.0 * 6.360611915588379
Epoch 300, val loss: 1.6425204277038574
Epoch 310, training loss: 637.0565795898438 = 1.618170976638794 + 100.0 * 6.354383945465088
Epoch 310, val loss: 1.627626657485962
Epoch 320, training loss: 636.632568359375 = 1.6005117893218994 + 100.0 * 6.350320339202881
Epoch 320, val loss: 1.6121597290039062
Epoch 330, training loss: 636.5480346679688 = 1.5819727182388306 + 100.0 * 6.349660396575928
Epoch 330, val loss: 1.5960252285003662
Epoch 340, training loss: 635.9283447265625 = 1.5627537965774536 + 100.0 * 6.343656063079834
Epoch 340, val loss: 1.5793306827545166
Epoch 350, training loss: 635.4432983398438 = 1.5429962873458862 + 100.0 * 6.339003562927246
Epoch 350, val loss: 1.5622771978378296
Epoch 360, training loss: 635.035400390625 = 1.5227131843566895 + 100.0 * 6.335126876831055
Epoch 360, val loss: 1.5449280738830566
Epoch 370, training loss: 635.4275512695312 = 1.5020980834960938 + 100.0 * 6.339254856109619
Epoch 370, val loss: 1.527206540107727
Epoch 380, training loss: 634.4700927734375 = 1.4804658889770508 + 100.0 * 6.329896450042725
Epoch 380, val loss: 1.509049415588379
Epoch 390, training loss: 634.133544921875 = 1.4588227272033691 + 100.0 * 6.326747417449951
Epoch 390, val loss: 1.4906702041625977
Epoch 400, training loss: 633.731689453125 = 1.436859369277954 + 100.0 * 6.322947978973389
Epoch 400, val loss: 1.472163438796997
Epoch 410, training loss: 633.371337890625 = 1.4146918058395386 + 100.0 * 6.31956672668457
Epoch 410, val loss: 1.4535664319992065
Epoch 420, training loss: 634.4853515625 = 1.39230215549469 + 100.0 * 6.330930709838867
Epoch 420, val loss: 1.4348156452178955
Epoch 430, training loss: 633.1034545898438 = 1.369526982307434 + 100.0 * 6.3173394203186035
Epoch 430, val loss: 1.4158035516738892
Epoch 440, training loss: 632.593505859375 = 1.3467522859573364 + 100.0 * 6.312467575073242
Epoch 440, val loss: 1.3969712257385254
Epoch 450, training loss: 632.24072265625 = 1.3240845203399658 + 100.0 * 6.309166431427002
Epoch 450, val loss: 1.378340482711792
Epoch 460, training loss: 633.44775390625 = 1.3015294075012207 + 100.0 * 6.321462154388428
Epoch 460, val loss: 1.3595808744430542
Epoch 470, training loss: 632.1484375 = 1.2785073518753052 + 100.0 * 6.308699131011963
Epoch 470, val loss: 1.3410221338272095
Epoch 480, training loss: 631.6464233398438 = 1.2557759284973145 + 100.0 * 6.303906440734863
Epoch 480, val loss: 1.322919249534607
Epoch 490, training loss: 631.213134765625 = 1.233519434928894 + 100.0 * 6.299796104431152
Epoch 490, val loss: 1.3052140474319458
Epoch 500, training loss: 631.1770629882812 = 1.21156644821167 + 100.0 * 6.299655437469482
Epoch 500, val loss: 1.287795901298523
Epoch 510, training loss: 630.74072265625 = 1.1895930767059326 + 100.0 * 6.295511722564697
Epoch 510, val loss: 1.2707602977752686
Epoch 520, training loss: 630.5326538085938 = 1.1680699586868286 + 100.0 * 6.29364538192749
Epoch 520, val loss: 1.2541234493255615
Epoch 530, training loss: 630.4629516601562 = 1.1470733880996704 + 100.0 * 6.293158531188965
Epoch 530, val loss: 1.2380034923553467
Epoch 540, training loss: 630.3470458984375 = 1.1261780261993408 + 100.0 * 6.292209148406982
Epoch 540, val loss: 1.2224068641662598
Epoch 550, training loss: 630.1350708007812 = 1.1057895421981812 + 100.0 * 6.290292739868164
Epoch 550, val loss: 1.2071589231491089
Epoch 560, training loss: 629.8543090820312 = 1.0857398509979248 + 100.0 * 6.287685394287109
Epoch 560, val loss: 1.1926674842834473
Epoch 570, training loss: 629.5943603515625 = 1.0662169456481934 + 100.0 * 6.285281658172607
Epoch 570, val loss: 1.1787229776382446
Epoch 580, training loss: 629.394287109375 = 1.0471872091293335 + 100.0 * 6.28347110748291
Epoch 580, val loss: 1.1654279232025146
Epoch 590, training loss: 629.6336669921875 = 1.0284829139709473 + 100.0 * 6.2860517501831055
Epoch 590, val loss: 1.1526626348495483
Epoch 600, training loss: 629.9497680664062 = 1.0100939273834229 + 100.0 * 6.289397239685059
Epoch 600, val loss: 1.140167236328125
Epoch 610, training loss: 629.1585083007812 = 0.992024838924408 + 100.0 * 6.281665325164795
Epoch 610, val loss: 1.128291130065918
Epoch 620, training loss: 628.7813110351562 = 0.9745417833328247 + 100.0 * 6.278067588806152
Epoch 620, val loss: 1.117310643196106
Epoch 630, training loss: 628.5896606445312 = 0.9576302170753479 + 100.0 * 6.276319980621338
Epoch 630, val loss: 1.1070125102996826
Epoch 640, training loss: 628.47265625 = 0.941100537776947 + 100.0 * 6.275315284729004
Epoch 640, val loss: 1.097239375114441
Epoch 650, training loss: 629.1386108398438 = 0.9247874021530151 + 100.0 * 6.282137870788574
Epoch 650, val loss: 1.0877716541290283
Epoch 660, training loss: 628.4672241210938 = 0.9087296724319458 + 100.0 * 6.275584697723389
Epoch 660, val loss: 1.0788414478302002
Epoch 670, training loss: 628.0487060546875 = 0.8931631445884705 + 100.0 * 6.271554946899414
Epoch 670, val loss: 1.070605993270874
Epoch 680, training loss: 627.9605102539062 = 0.8780390620231628 + 100.0 * 6.270824432373047
Epoch 680, val loss: 1.062878966331482
Epoch 690, training loss: 628.5711669921875 = 0.8632319569587708 + 100.0 * 6.2770795822143555
Epoch 690, val loss: 1.0555626153945923
Epoch 700, training loss: 627.7938842773438 = 0.8484266996383667 + 100.0 * 6.269454479217529
Epoch 700, val loss: 1.0487298965454102
Epoch 710, training loss: 627.6170043945312 = 0.8341048955917358 + 100.0 * 6.267828941345215
Epoch 710, val loss: 1.0424093008041382
Epoch 720, training loss: 627.5100708007812 = 0.8200982809066772 + 100.0 * 6.266900062561035
Epoch 720, val loss: 1.036573052406311
Epoch 730, training loss: 627.7055053710938 = 0.8062319159507751 + 100.0 * 6.268992900848389
Epoch 730, val loss: 1.0311967134475708
Epoch 740, training loss: 627.4404907226562 = 0.7926461696624756 + 100.0 * 6.266478061676025
Epoch 740, val loss: 1.0262641906738281
Epoch 750, training loss: 627.1852416992188 = 0.779371440410614 + 100.0 * 6.264059066772461
Epoch 750, val loss: 1.0218182802200317
Epoch 760, training loss: 627.1444702148438 = 0.7663043141365051 + 100.0 * 6.263782024383545
Epoch 760, val loss: 1.0176693201065063
Epoch 770, training loss: 626.9309692382812 = 0.7534069418907166 + 100.0 * 6.261775493621826
Epoch 770, val loss: 1.0143123865127563
Epoch 780, training loss: 627.0830078125 = 0.7408137917518616 + 100.0 * 6.263422012329102
Epoch 780, val loss: 1.0111398696899414
Epoch 790, training loss: 627.4031982421875 = 0.7282065153121948 + 100.0 * 6.266749858856201
Epoch 790, val loss: 1.0077612400054932
Epoch 800, training loss: 626.7611083984375 = 0.715547502040863 + 100.0 * 6.260455131530762
Epoch 800, val loss: 1.0049164295196533
Epoch 810, training loss: 626.5919189453125 = 0.7033896446228027 + 100.0 * 6.258885383605957
Epoch 810, val loss: 1.0026530027389526
Epoch 820, training loss: 626.4456176757812 = 0.691502571105957 + 100.0 * 6.257540702819824
Epoch 820, val loss: 1.0005989074707031
Epoch 830, training loss: 626.3666381835938 = 0.6797876954078674 + 100.0 * 6.256868362426758
Epoch 830, val loss: 0.9988169074058533
Epoch 840, training loss: 626.886962890625 = 0.6682287454605103 + 100.0 * 6.262187480926514
Epoch 840, val loss: 0.9973964095115662
Epoch 850, training loss: 626.6218872070312 = 0.6565535664558411 + 100.0 * 6.259653091430664
Epoch 850, val loss: 0.9958072304725647
Epoch 860, training loss: 626.175048828125 = 0.6450896859169006 + 100.0 * 6.2552995681762695
Epoch 860, val loss: 0.9946039319038391
Epoch 870, training loss: 626.0069580078125 = 0.6338807344436646 + 100.0 * 6.253730297088623
Epoch 870, val loss: 0.9937573671340942
Epoch 880, training loss: 626.359619140625 = 0.622802197933197 + 100.0 * 6.257368087768555
Epoch 880, val loss: 0.9929307699203491
Epoch 890, training loss: 625.8597412109375 = 0.6118372082710266 + 100.0 * 6.252479553222656
Epoch 890, val loss: 0.9925639033317566
Epoch 900, training loss: 625.74560546875 = 0.600972592830658 + 100.0 * 6.251446723937988
Epoch 900, val loss: 0.9922696948051453
Epoch 910, training loss: 625.83642578125 = 0.5903555154800415 + 100.0 * 6.252460956573486
Epoch 910, val loss: 0.9923093318939209
Epoch 920, training loss: 625.7137451171875 = 0.5797995924949646 + 100.0 * 6.251339912414551
Epoch 920, val loss: 0.9923084378242493
Epoch 930, training loss: 625.592529296875 = 0.5694444179534912 + 100.0 * 6.25023078918457
Epoch 930, val loss: 0.9928407073020935
Epoch 940, training loss: 625.6328125 = 0.5592320561408997 + 100.0 * 6.250736236572266
Epoch 940, val loss: 0.9932870268821716
Epoch 950, training loss: 625.5956420898438 = 0.5491347908973694 + 100.0 * 6.250464916229248
Epoch 950, val loss: 0.9939064383506775
Epoch 960, training loss: 625.3651123046875 = 0.5391784310340881 + 100.0 * 6.248259544372559
Epoch 960, val loss: 0.994438886642456
Epoch 970, training loss: 625.2886352539062 = 0.5294629335403442 + 100.0 * 6.247591495513916
Epoch 970, val loss: 0.9956821203231812
Epoch 980, training loss: 625.2269287109375 = 0.5199137926101685 + 100.0 * 6.2470703125
Epoch 980, val loss: 0.9969247579574585
Epoch 990, training loss: 626.081298828125 = 0.510581374168396 + 100.0 * 6.255707263946533
Epoch 990, val loss: 0.998690664768219
Epoch 1000, training loss: 625.3345947265625 = 0.5008972883224487 + 100.0 * 6.2483367919921875
Epoch 1000, val loss: 0.9992976188659668
Epoch 1010, training loss: 625.104248046875 = 0.4917895793914795 + 100.0 * 6.246124744415283
Epoch 1010, val loss: 1.001435399055481
Epoch 1020, training loss: 624.921142578125 = 0.48281973600387573 + 100.0 * 6.244382858276367
Epoch 1020, val loss: 1.003174901008606
Epoch 1030, training loss: 625.1006469726562 = 0.4740747809410095 + 100.0 * 6.246265888214111
Epoch 1030, val loss: 1.0053110122680664
Epoch 1040, training loss: 624.997314453125 = 0.4654596745967865 + 100.0 * 6.245318412780762
Epoch 1040, val loss: 1.0076407194137573
Epoch 1050, training loss: 624.70556640625 = 0.4569028615951538 + 100.0 * 6.242486476898193
Epoch 1050, val loss: 1.0096255540847778
Epoch 1060, training loss: 624.7039794921875 = 0.4485827684402466 + 100.0 * 6.2425537109375
Epoch 1060, val loss: 1.0119715929031372
Epoch 1070, training loss: 624.791015625 = 0.44045257568359375 + 100.0 * 6.243505954742432
Epoch 1070, val loss: 1.0145180225372314
Epoch 1080, training loss: 624.918701171875 = 0.4324066638946533 + 100.0 * 6.2448625564575195
Epoch 1080, val loss: 1.0175493955612183
Epoch 1090, training loss: 624.5604248046875 = 0.42449429631233215 + 100.0 * 6.241359233856201
Epoch 1090, val loss: 1.0203237533569336
Epoch 1100, training loss: 624.49609375 = 0.4167393147945404 + 100.0 * 6.240793704986572
Epoch 1100, val loss: 1.022993803024292
Epoch 1110, training loss: 624.6990966796875 = 0.40928035974502563 + 100.0 * 6.242897987365723
Epoch 1110, val loss: 1.0266656875610352
Epoch 1120, training loss: 624.4981079101562 = 0.40156400203704834 + 100.0 * 6.240965366363525
Epoch 1120, val loss: 1.0286378860473633
Epoch 1130, training loss: 624.4591674804688 = 0.3941996097564697 + 100.0 * 6.240649700164795
Epoch 1130, val loss: 1.0324023962020874
Epoch 1140, training loss: 624.1777954101562 = 0.38700830936431885 + 100.0 * 6.237907409667969
Epoch 1140, val loss: 1.0357952117919922
Epoch 1150, training loss: 624.1284790039062 = 0.3800220191478729 + 100.0 * 6.237484931945801
Epoch 1150, val loss: 1.0389312505722046
Epoch 1160, training loss: 624.1293334960938 = 0.37320050597190857 + 100.0 * 6.237561225891113
Epoch 1160, val loss: 1.0425212383270264
Epoch 1170, training loss: 624.6754150390625 = 0.3664086163043976 + 100.0 * 6.2430901527404785
Epoch 1170, val loss: 1.0459855794906616
Epoch 1180, training loss: 624.3306274414062 = 0.35958021879196167 + 100.0 * 6.239710807800293
Epoch 1180, val loss: 1.049536108970642
Epoch 1190, training loss: 624.1299438476562 = 0.3529176414012909 + 100.0 * 6.2377705574035645
Epoch 1190, val loss: 1.0530641078948975
Epoch 1200, training loss: 623.859375 = 0.34654346108436584 + 100.0 * 6.235127925872803
Epoch 1200, val loss: 1.0572068691253662
Epoch 1210, training loss: 623.8746337890625 = 0.340329647064209 + 100.0 * 6.235342979431152
Epoch 1210, val loss: 1.0613148212432861
Epoch 1220, training loss: 624.4927978515625 = 0.3342152535915375 + 100.0 * 6.241585731506348
Epoch 1220, val loss: 1.0653201341629028
Epoch 1230, training loss: 624.1788330078125 = 0.3279525935649872 + 100.0 * 6.238509178161621
Epoch 1230, val loss: 1.0687122344970703
Epoch 1240, training loss: 623.884521484375 = 0.32195407152175903 + 100.0 * 6.235625743865967
Epoch 1240, val loss: 1.0734531879425049
Epoch 1250, training loss: 623.6619262695312 = 0.31604447960853577 + 100.0 * 6.233458995819092
Epoch 1250, val loss: 1.0773532390594482
Epoch 1260, training loss: 623.6410522460938 = 0.31032228469848633 + 100.0 * 6.233307361602783
Epoch 1260, val loss: 1.0817110538482666
Epoch 1270, training loss: 623.8405151367188 = 0.30471986532211304 + 100.0 * 6.235357761383057
Epoch 1270, val loss: 1.0861587524414062
Epoch 1280, training loss: 623.7653198242188 = 0.29907628893852234 + 100.0 * 6.2346625328063965
Epoch 1280, val loss: 1.0903480052947998
Epoch 1290, training loss: 623.5161743164062 = 0.29351893067359924 + 100.0 * 6.232226848602295
Epoch 1290, val loss: 1.0947051048278809
Epoch 1300, training loss: 623.4266357421875 = 0.2881413996219635 + 100.0 * 6.231384754180908
Epoch 1300, val loss: 1.0992308855056763
Epoch 1310, training loss: 623.5656127929688 = 0.2829013466835022 + 100.0 * 6.232827186584473
Epoch 1310, val loss: 1.1039167642593384
Epoch 1320, training loss: 623.6192016601562 = 0.27765974402427673 + 100.0 * 6.233415603637695
Epoch 1320, val loss: 1.1082037687301636
Epoch 1330, training loss: 623.5303955078125 = 0.2724120020866394 + 100.0 * 6.232580184936523
Epoch 1330, val loss: 1.1124744415283203
Epoch 1340, training loss: 623.30810546875 = 0.26736754179000854 + 100.0 * 6.230407238006592
Epoch 1340, val loss: 1.1170381307601929
Epoch 1350, training loss: 623.7113037109375 = 0.2624002993106842 + 100.0 * 6.2344889640808105
Epoch 1350, val loss: 1.120959758758545
Epoch 1360, training loss: 623.1505126953125 = 0.2574792206287384 + 100.0 * 6.2289299964904785
Epoch 1360, val loss: 1.1265138387680054
Epoch 1370, training loss: 623.078369140625 = 0.252664715051651 + 100.0 * 6.228256702423096
Epoch 1370, val loss: 1.1309009790420532
Epoch 1380, training loss: 623.126708984375 = 0.2479918748140335 + 100.0 * 6.228787422180176
Epoch 1380, val loss: 1.135701298713684
Epoch 1390, training loss: 624.0128173828125 = 0.2433934509754181 + 100.0 * 6.237694263458252
Epoch 1390, val loss: 1.1406112909317017
Epoch 1400, training loss: 623.2047729492188 = 0.23867586255073547 + 100.0 * 6.229660987854004
Epoch 1400, val loss: 1.1448231935501099
Epoch 1410, training loss: 623.3129272460938 = 0.23416993021965027 + 100.0 * 6.23078727722168
Epoch 1410, val loss: 1.14975106716156
Epoch 1420, training loss: 622.9774169921875 = 0.22972512245178223 + 100.0 * 6.227477073669434
Epoch 1420, val loss: 1.1547335386276245
Epoch 1430, training loss: 622.964111328125 = 0.22537299990653992 + 100.0 * 6.227386951446533
Epoch 1430, val loss: 1.1593410968780518
Epoch 1440, training loss: 623.0551147460938 = 0.22114600241184235 + 100.0 * 6.228339672088623
Epoch 1440, val loss: 1.1640980243682861
Epoch 1450, training loss: 623.24169921875 = 0.216941237449646 + 100.0 * 6.230247974395752
Epoch 1450, val loss: 1.1692472696304321
Epoch 1460, training loss: 623.08642578125 = 0.21275348961353302 + 100.0 * 6.228736400604248
Epoch 1460, val loss: 1.1739875078201294
Epoch 1470, training loss: 622.8541259765625 = 0.20858703553676605 + 100.0 * 6.2264556884765625
Epoch 1470, val loss: 1.1782768964767456
Epoch 1480, training loss: 622.7664184570312 = 0.2046373188495636 + 100.0 * 6.225617408752441
Epoch 1480, val loss: 1.1835076808929443
Epoch 1490, training loss: 622.9157104492188 = 0.20072145760059357 + 100.0 * 6.227149486541748
Epoch 1490, val loss: 1.1878231763839722
Epoch 1500, training loss: 622.797607421875 = 0.196817085146904 + 100.0 * 6.22600793838501
Epoch 1500, val loss: 1.1930204629898071
Epoch 1510, training loss: 622.6646728515625 = 0.19295741617679596 + 100.0 * 6.224717617034912
Epoch 1510, val loss: 1.197936773300171
Epoch 1520, training loss: 622.8018188476562 = 0.18920853734016418 + 100.0 * 6.226126194000244
Epoch 1520, val loss: 1.2029523849487305
Epoch 1530, training loss: 622.640380859375 = 0.18550680577754974 + 100.0 * 6.224548816680908
Epoch 1530, val loss: 1.2076606750488281
Epoch 1540, training loss: 622.6675415039062 = 0.18188245594501495 + 100.0 * 6.224856853485107
Epoch 1540, val loss: 1.2127571105957031
Epoch 1550, training loss: 622.638427734375 = 0.17831754684448242 + 100.0 * 6.224600791931152
Epoch 1550, val loss: 1.2174004316329956
Epoch 1560, training loss: 623.0800170898438 = 0.17483460903167725 + 100.0 * 6.2290520668029785
Epoch 1560, val loss: 1.2226591110229492
Epoch 1570, training loss: 622.7158813476562 = 0.17129947245121002 + 100.0 * 6.225445747375488
Epoch 1570, val loss: 1.2269119024276733
Epoch 1580, training loss: 622.54736328125 = 0.16790281236171722 + 100.0 * 6.223794460296631
Epoch 1580, val loss: 1.2326499223709106
Epoch 1590, training loss: 622.3861083984375 = 0.16460545361042023 + 100.0 * 6.222214698791504
Epoch 1590, val loss: 1.2373512983322144
Epoch 1600, training loss: 622.4842529296875 = 0.16137991845607758 + 100.0 * 6.223228931427002
Epoch 1600, val loss: 1.2425585985183716
Epoch 1610, training loss: 622.6101684570312 = 0.15817669034004211 + 100.0 * 6.224519729614258
Epoch 1610, val loss: 1.2474801540374756
Epoch 1620, training loss: 622.537353515625 = 0.15504366159439087 + 100.0 * 6.223823070526123
Epoch 1620, val loss: 1.253129243850708
Epoch 1630, training loss: 622.4364624023438 = 0.15196873247623444 + 100.0 * 6.222845077514648
Epoch 1630, val loss: 1.258239507675171
Epoch 1640, training loss: 622.3320922851562 = 0.14894366264343262 + 100.0 * 6.221831798553467
Epoch 1640, val loss: 1.263255000114441
Epoch 1650, training loss: 622.27587890625 = 0.14599253237247467 + 100.0 * 6.221298694610596
Epoch 1650, val loss: 1.2690752744674683
Epoch 1660, training loss: 622.480712890625 = 0.14312632381916046 + 100.0 * 6.2233757972717285
Epoch 1660, val loss: 1.2741621732711792
Epoch 1670, training loss: 622.193115234375 = 0.1402694135904312 + 100.0 * 6.220528602600098
Epoch 1670, val loss: 1.279434323310852
Epoch 1680, training loss: 622.229736328125 = 0.13749773800373077 + 100.0 * 6.220922470092773
Epoch 1680, val loss: 1.2845731973648071
Epoch 1690, training loss: 622.46923828125 = 0.13480189442634583 + 100.0 * 6.223344326019287
Epoch 1690, val loss: 1.2905393838882446
Epoch 1700, training loss: 622.5239868164062 = 0.13206414878368378 + 100.0 * 6.223918914794922
Epoch 1700, val loss: 1.2947148084640503
Epoch 1710, training loss: 622.1863403320312 = 0.12935355305671692 + 100.0 * 6.220570087432861
Epoch 1710, val loss: 1.3000314235687256
Epoch 1720, training loss: 622.0410766601562 = 0.12680767476558685 + 100.0 * 6.219142436981201
Epoch 1720, val loss: 1.3053419589996338
Epoch 1730, training loss: 621.9411010742188 = 0.12433408200740814 + 100.0 * 6.218167781829834
Epoch 1730, val loss: 1.3108590841293335
Epoch 1740, training loss: 621.98876953125 = 0.1219208687543869 + 100.0 * 6.218668460845947
Epoch 1740, val loss: 1.315803050994873
Epoch 1750, training loss: 622.6276245117188 = 0.1195322647690773 + 100.0 * 6.225081443786621
Epoch 1750, val loss: 1.3210428953170776
Epoch 1760, training loss: 621.9141235351562 = 0.11708890646696091 + 100.0 * 6.217970848083496
Epoch 1760, val loss: 1.3262296915054321
Epoch 1770, training loss: 621.8431396484375 = 0.11476681381464005 + 100.0 * 6.217284202575684
Epoch 1770, val loss: 1.3316932916641235
Epoch 1780, training loss: 622.0408935546875 = 0.11255693435668945 + 100.0 * 6.219283580780029
Epoch 1780, val loss: 1.3370938301086426
Epoch 1790, training loss: 622.0872192382812 = 0.11031749099493027 + 100.0 * 6.219769477844238
Epoch 1790, val loss: 1.3421249389648438
Epoch 1800, training loss: 621.8323974609375 = 0.10811839997768402 + 100.0 * 6.217243194580078
Epoch 1800, val loss: 1.3475252389907837
Epoch 1810, training loss: 621.7932739257812 = 0.10601794719696045 + 100.0 * 6.216872692108154
Epoch 1810, val loss: 1.3528646230697632
Epoch 1820, training loss: 622.5994873046875 = 0.10397908091545105 + 100.0 * 6.224954605102539
Epoch 1820, val loss: 1.357508897781372
Epoch 1830, training loss: 622.2380981445312 = 0.1018754094839096 + 100.0 * 6.221362113952637
Epoch 1830, val loss: 1.3628458976745605
Epoch 1840, training loss: 621.7635498046875 = 0.09985649585723877 + 100.0 * 6.216636657714844
Epoch 1840, val loss: 1.3685004711151123
Epoch 1850, training loss: 621.66455078125 = 0.09793208539485931 + 100.0 * 6.215665817260742
Epoch 1850, val loss: 1.3739111423492432
Epoch 1860, training loss: 621.630615234375 = 0.09606374800205231 + 100.0 * 6.21534538269043
Epoch 1860, val loss: 1.3795912265777588
Epoch 1870, training loss: 621.923095703125 = 0.09426175802946091 + 100.0 * 6.218288421630859
Epoch 1870, val loss: 1.385168194770813
Epoch 1880, training loss: 621.6854248046875 = 0.09239938855171204 + 100.0 * 6.215929985046387
Epoch 1880, val loss: 1.3900600671768188
Epoch 1890, training loss: 621.8291625976562 = 0.09059403836727142 + 100.0 * 6.217385768890381
Epoch 1890, val loss: 1.394452452659607
Epoch 1900, training loss: 621.7991333007812 = 0.08887398988008499 + 100.0 * 6.217102527618408
Epoch 1900, val loss: 1.4005483388900757
Epoch 1910, training loss: 621.4959716796875 = 0.08715055882930756 + 100.0 * 6.214087963104248
Epoch 1910, val loss: 1.4054746627807617
Epoch 1920, training loss: 621.6666870117188 = 0.08550364524126053 + 100.0 * 6.215811729431152
Epoch 1920, val loss: 1.4106645584106445
Epoch 1930, training loss: 622.6337890625 = 0.08389196544885635 + 100.0 * 6.225499153137207
Epoch 1930, val loss: 1.4155172109603882
Epoch 1940, training loss: 621.689697265625 = 0.0821993350982666 + 100.0 * 6.2160749435424805
Epoch 1940, val loss: 1.4203879833221436
Epoch 1950, training loss: 621.4673461914062 = 0.0806378573179245 + 100.0 * 6.2138671875
Epoch 1950, val loss: 1.4262886047363281
Epoch 1960, training loss: 621.4022827148438 = 0.07913807034492493 + 100.0 * 6.213231086730957
Epoch 1960, val loss: 1.4312716722488403
Epoch 1970, training loss: 621.3600463867188 = 0.07768629491329193 + 100.0 * 6.212823390960693
Epoch 1970, val loss: 1.436789870262146
Epoch 1980, training loss: 621.6580200195312 = 0.07627283781766891 + 100.0 * 6.215817451477051
Epoch 1980, val loss: 1.4419564008712769
Epoch 1990, training loss: 621.4384765625 = 0.07482181489467621 + 100.0 * 6.21363639831543
Epoch 1990, val loss: 1.447117805480957
Epoch 2000, training loss: 621.6394653320312 = 0.07341250777244568 + 100.0 * 6.215660572052002
Epoch 2000, val loss: 1.4527919292449951
Epoch 2010, training loss: 621.3184814453125 = 0.07200845330953598 + 100.0 * 6.212464332580566
Epoch 2010, val loss: 1.457128882408142
Epoch 2020, training loss: 621.36376953125 = 0.0706850215792656 + 100.0 * 6.212930679321289
Epoch 2020, val loss: 1.4620271921157837
Epoch 2030, training loss: 621.2666015625 = 0.06940735131502151 + 100.0 * 6.211971759796143
Epoch 2030, val loss: 1.4672958850860596
Epoch 2040, training loss: 621.5463256835938 = 0.06816523522138596 + 100.0 * 6.214781761169434
Epoch 2040, val loss: 1.4721914529800415
Epoch 2050, training loss: 621.2772827148438 = 0.0669054165482521 + 100.0 * 6.212103843688965
Epoch 2050, val loss: 1.477768898010254
Epoch 2060, training loss: 621.544921875 = 0.06570934504270554 + 100.0 * 6.214791774749756
Epoch 2060, val loss: 1.4832009077072144
Epoch 2070, training loss: 621.3036499023438 = 0.06448761373758316 + 100.0 * 6.212391376495361
Epoch 2070, val loss: 1.4878607988357544
Epoch 2080, training loss: 621.498779296875 = 0.06334452331066132 + 100.0 * 6.214354038238525
Epoch 2080, val loss: 1.4935284852981567
Epoch 2090, training loss: 621.2708129882812 = 0.0621928796172142 + 100.0 * 6.212086200714111
Epoch 2090, val loss: 1.4981379508972168
Epoch 2100, training loss: 621.221435546875 = 0.06108361855149269 + 100.0 * 6.21160364151001
Epoch 2100, val loss: 1.503140926361084
Epoch 2110, training loss: 621.6900024414062 = 0.06002459302544594 + 100.0 * 6.216300010681152
Epoch 2110, val loss: 1.5085301399230957
Epoch 2120, training loss: 621.5101928710938 = 0.05893488973379135 + 100.0 * 6.214512348175049
Epoch 2120, val loss: 1.5133038759231567
Epoch 2130, training loss: 621.2498168945312 = 0.05786202847957611 + 100.0 * 6.211919784545898
Epoch 2130, val loss: 1.5177345275878906
Epoch 2140, training loss: 621.0585327148438 = 0.056848686188459396 + 100.0 * 6.210017204284668
Epoch 2140, val loss: 1.5229284763336182
Epoch 2150, training loss: 621.04248046875 = 0.055880106985569 + 100.0 * 6.209865570068359
Epoch 2150, val loss: 1.527769923210144
Epoch 2160, training loss: 621.6582641601562 = 0.05493408441543579 + 100.0 * 6.216033458709717
Epoch 2160, val loss: 1.5319712162017822
Epoch 2170, training loss: 621.1358642578125 = 0.05394696071743965 + 100.0 * 6.210819244384766
Epoch 2170, val loss: 1.5369784832000732
Epoch 2180, training loss: 621.030029296875 = 0.05299939960241318 + 100.0 * 6.209770202636719
Epoch 2180, val loss: 1.5419347286224365
Epoch 2190, training loss: 621.1339111328125 = 0.05210226774215698 + 100.0 * 6.210817813873291
Epoch 2190, val loss: 1.546872615814209
Epoch 2200, training loss: 621.2517700195312 = 0.05122032016515732 + 100.0 * 6.212005615234375
Epoch 2200, val loss: 1.5514038801193237
Epoch 2210, training loss: 621.0477294921875 = 0.050348471850156784 + 100.0 * 6.2099738121032715
Epoch 2210, val loss: 1.556299090385437
Epoch 2220, training loss: 621.0306396484375 = 0.04951133951544762 + 100.0 * 6.209811687469482
Epoch 2220, val loss: 1.5610827207565308
Epoch 2230, training loss: 621.1666870117188 = 0.04868089780211449 + 100.0 * 6.211179733276367
Epoch 2230, val loss: 1.5654293298721313
Epoch 2240, training loss: 621.0233764648438 = 0.04785675182938576 + 100.0 * 6.2097554206848145
Epoch 2240, val loss: 1.570448637008667
Epoch 2250, training loss: 621.2335815429688 = 0.04706661030650139 + 100.0 * 6.211865425109863
Epoch 2250, val loss: 1.5753588676452637
Epoch 2260, training loss: 621.2562866210938 = 0.046270664781332016 + 100.0 * 6.212100028991699
Epoch 2260, val loss: 1.5789504051208496
Epoch 2270, training loss: 621.185546875 = 0.045495711266994476 + 100.0 * 6.211400985717773
Epoch 2270, val loss: 1.5847774744033813
Epoch 2280, training loss: 620.882568359375 = 0.044718481600284576 + 100.0 * 6.208378314971924
Epoch 2280, val loss: 1.5888832807540894
Epoch 2290, training loss: 620.78076171875 = 0.04400213435292244 + 100.0 * 6.207367420196533
Epoch 2290, val loss: 1.5938472747802734
Epoch 2300, training loss: 620.7391967773438 = 0.043308086693286896 + 100.0 * 6.206958770751953
Epoch 2300, val loss: 1.5983374118804932
Epoch 2310, training loss: 620.7367553710938 = 0.042630016803741455 + 100.0 * 6.206941604614258
Epoch 2310, val loss: 1.603080153465271
Epoch 2320, training loss: 621.5546875 = 0.04197918251156807 + 100.0 * 6.215126991271973
Epoch 2320, val loss: 1.6072022914886475
Epoch 2330, training loss: 621.4822998046875 = 0.04127509146928787 + 100.0 * 6.214410305023193
Epoch 2330, val loss: 1.6106511354446411
Epoch 2340, training loss: 620.9329833984375 = 0.040578167885541916 + 100.0 * 6.208923816680908
Epoch 2340, val loss: 1.6164021492004395
Epoch 2350, training loss: 620.6981811523438 = 0.03993603214621544 + 100.0 * 6.206582546234131
Epoch 2350, val loss: 1.620332956314087
Epoch 2360, training loss: 620.6314697265625 = 0.03932579979300499 + 100.0 * 6.205921649932861
Epoch 2360, val loss: 1.6249662637710571
Epoch 2370, training loss: 620.6868896484375 = 0.038732558488845825 + 100.0 * 6.206481456756592
Epoch 2370, val loss: 1.629507064819336
Epoch 2380, training loss: 621.8929443359375 = 0.03815160319209099 + 100.0 * 6.218547821044922
Epoch 2380, val loss: 1.6335041522979736
Epoch 2390, training loss: 620.991455078125 = 0.037517860531806946 + 100.0 * 6.209539413452148
Epoch 2390, val loss: 1.638365626335144
Epoch 2400, training loss: 620.5995483398438 = 0.036918316036462784 + 100.0 * 6.205626010894775
Epoch 2400, val loss: 1.6423358917236328
Epoch 2410, training loss: 620.6318969726562 = 0.03636722266674042 + 100.0 * 6.2059550285339355
Epoch 2410, val loss: 1.6463686227798462
Epoch 2420, training loss: 620.942626953125 = 0.035832978785037994 + 100.0 * 6.2090678215026855
Epoch 2420, val loss: 1.6504629850387573
Epoch 2430, training loss: 620.8060913085938 = 0.03527979552745819 + 100.0 * 6.20770788192749
Epoch 2430, val loss: 1.65501070022583
Epoch 2440, training loss: 620.7593994140625 = 0.03474999964237213 + 100.0 * 6.207246780395508
Epoch 2440, val loss: 1.6596105098724365
Epoch 2450, training loss: 620.6328125 = 0.03422261029481888 + 100.0 * 6.2059855461120605
Epoch 2450, val loss: 1.6636550426483154
Epoch 2460, training loss: 620.9542846679688 = 0.033743176609277725 + 100.0 * 6.209205150604248
Epoch 2460, val loss: 1.6680494546890259
Epoch 2470, training loss: 620.7159423828125 = 0.03322717174887657 + 100.0 * 6.206827163696289
Epoch 2470, val loss: 1.6718591451644897
Epoch 2480, training loss: 620.4986572265625 = 0.03272745758295059 + 100.0 * 6.204659461975098
Epoch 2480, val loss: 1.6757595539093018
Epoch 2490, training loss: 620.538818359375 = 0.03226200491189957 + 100.0 * 6.2050652503967285
Epoch 2490, val loss: 1.6802923679351807
Epoch 2500, training loss: 620.5569458007812 = 0.03180447965860367 + 100.0 * 6.205251216888428
Epoch 2500, val loss: 1.6841506958007812
Epoch 2510, training loss: 620.8396606445312 = 0.03135905787348747 + 100.0 * 6.208082675933838
Epoch 2510, val loss: 1.6879380941390991
Epoch 2520, training loss: 620.8729248046875 = 0.030901826918125153 + 100.0 * 6.208420276641846
Epoch 2520, val loss: 1.6922762393951416
Epoch 2530, training loss: 620.6765747070312 = 0.030438100919127464 + 100.0 * 6.206461429595947
Epoch 2530, val loss: 1.696121096611023
Epoch 2540, training loss: 620.4859619140625 = 0.030005203559994698 + 100.0 * 6.204559326171875
Epoch 2540, val loss: 1.7003583908081055
Epoch 2550, training loss: 620.5125122070312 = 0.029591526836156845 + 100.0 * 6.204829216003418
Epoch 2550, val loss: 1.7048022747039795
Epoch 2560, training loss: 620.775146484375 = 0.02919553965330124 + 100.0 * 6.207459449768066
Epoch 2560, val loss: 1.7084720134735107
Epoch 2570, training loss: 620.5689697265625 = 0.02876652032136917 + 100.0 * 6.205402374267578
Epoch 2570, val loss: 1.7123098373413086
Epoch 2580, training loss: 620.6546020507812 = 0.028367266058921814 + 100.0 * 6.206262111663818
Epoch 2580, val loss: 1.7154724597930908
Epoch 2590, training loss: 620.4224853515625 = 0.027968529611825943 + 100.0 * 6.203945159912109
Epoch 2590, val loss: 1.7197768688201904
Epoch 2600, training loss: 620.3630981445312 = 0.02759106084704399 + 100.0 * 6.203354835510254
Epoch 2600, val loss: 1.723356008529663
Epoch 2610, training loss: 620.5122680664062 = 0.02722894586622715 + 100.0 * 6.204850196838379
Epoch 2610, val loss: 1.7267183065414429
Epoch 2620, training loss: 620.7517700195312 = 0.026858989149332047 + 100.0 * 6.207249164581299
Epoch 2620, val loss: 1.7301595211029053
Epoch 2630, training loss: 620.56103515625 = 0.026494555175304413 + 100.0 * 6.205345630645752
Epoch 2630, val loss: 1.7344810962677002
Epoch 2640, training loss: 620.4318237304688 = 0.026129398494958878 + 100.0 * 6.204056739807129
Epoch 2640, val loss: 1.7380257844924927
Epoch 2650, training loss: 620.5961303710938 = 0.025797471404075623 + 100.0 * 6.205703258514404
Epoch 2650, val loss: 1.742369294166565
Epoch 2660, training loss: 620.6361694335938 = 0.025447092950344086 + 100.0 * 6.206107139587402
Epoch 2660, val loss: 1.745518684387207
Epoch 2670, training loss: 620.328125 = 0.025094736367464066 + 100.0 * 6.203030586242676
Epoch 2670, val loss: 1.7487993240356445
Epoch 2680, training loss: 620.1940307617188 = 0.024768073111772537 + 100.0 * 6.201692581176758
Epoch 2680, val loss: 1.752523422241211
Epoch 2690, training loss: 620.2091064453125 = 0.024460257962346077 + 100.0 * 6.201846599578857
Epoch 2690, val loss: 1.7562344074249268
Epoch 2700, training loss: 620.5221557617188 = 0.024159280583262444 + 100.0 * 6.20497989654541
Epoch 2700, val loss: 1.7595709562301636
Epoch 2710, training loss: 620.3848266601562 = 0.02383679710328579 + 100.0 * 6.203609466552734
Epoch 2710, val loss: 1.7623366117477417
Epoch 2720, training loss: 620.3818969726562 = 0.023525282740592957 + 100.0 * 6.203583717346191
Epoch 2720, val loss: 1.7658945322036743
Epoch 2730, training loss: 620.5987548828125 = 0.02323550544679165 + 100.0 * 6.20575475692749
Epoch 2730, val loss: 1.7701834440231323
Epoch 2740, training loss: 620.5791625976562 = 0.02293524704873562 + 100.0 * 6.205562114715576
Epoch 2740, val loss: 1.7733354568481445
Epoch 2750, training loss: 620.3710327148438 = 0.02263222634792328 + 100.0 * 6.203484058380127
Epoch 2750, val loss: 1.776253342628479
Epoch 2760, training loss: 620.2037353515625 = 0.022346045821905136 + 100.0 * 6.201813697814941
Epoch 2760, val loss: 1.7794814109802246
Epoch 2770, training loss: 620.1522216796875 = 0.0220803115516901 + 100.0 * 6.201301574707031
Epoch 2770, val loss: 1.7830747365951538
Epoch 2780, training loss: 620.416259765625 = 0.02182169258594513 + 100.0 * 6.203944206237793
Epoch 2780, val loss: 1.7863454818725586
Epoch 2790, training loss: 620.3790893554688 = 0.021554362028837204 + 100.0 * 6.203575134277344
Epoch 2790, val loss: 1.789482831954956
Epoch 2800, training loss: 620.3721313476562 = 0.021280692890286446 + 100.0 * 6.203508377075195
Epoch 2800, val loss: 1.7934051752090454
Epoch 2810, training loss: 620.207763671875 = 0.021013248711824417 + 100.0 * 6.201867580413818
Epoch 2810, val loss: 1.7960302829742432
Epoch 2820, training loss: 620.603759765625 = 0.02076464146375656 + 100.0 * 6.205830097198486
Epoch 2820, val loss: 1.7991044521331787
Epoch 2830, training loss: 620.0609130859375 = 0.020499849691987038 + 100.0 * 6.200404167175293
Epoch 2830, val loss: 1.8022130727767944
Epoch 2840, training loss: 620.0464477539062 = 0.02025582268834114 + 100.0 * 6.20026159286499
Epoch 2840, val loss: 1.8053723573684692
Epoch 2850, training loss: 619.9639892578125 = 0.02002108469605446 + 100.0 * 6.199439525604248
Epoch 2850, val loss: 1.8085498809814453
Epoch 2860, training loss: 619.988525390625 = 0.019797660410404205 + 100.0 * 6.199687480926514
Epoch 2860, val loss: 1.8117108345031738
Epoch 2870, training loss: 620.6640014648438 = 0.01959848403930664 + 100.0 * 6.206444263458252
Epoch 2870, val loss: 1.8146644830703735
Epoch 2880, training loss: 620.8381958007812 = 0.019337376579642296 + 100.0 * 6.208188533782959
Epoch 2880, val loss: 1.8182013034820557
Epoch 2890, training loss: 620.2346801757812 = 0.01907856948673725 + 100.0 * 6.202155590057373
Epoch 2890, val loss: 1.819512128829956
Epoch 2900, training loss: 619.9703369140625 = 0.018855014815926552 + 100.0 * 6.199514865875244
Epoch 2900, val loss: 1.8231325149536133
Epoch 2910, training loss: 619.8932495117188 = 0.01864863559603691 + 100.0 * 6.1987457275390625
Epoch 2910, val loss: 1.8263561725616455
Epoch 2920, training loss: 619.8853149414062 = 0.018446048721671104 + 100.0 * 6.198668956756592
Epoch 2920, val loss: 1.8291537761688232
Epoch 2930, training loss: 620.4815063476562 = 0.018253136426210403 + 100.0 * 6.20463228225708
Epoch 2930, val loss: 1.831594467163086
Epoch 2940, training loss: 620.3583374023438 = 0.018032131716609 + 100.0 * 6.203403472900391
Epoch 2940, val loss: 1.8335907459259033
Epoch 2950, training loss: 620.3082275390625 = 0.017813604325056076 + 100.0 * 6.202904224395752
Epoch 2950, val loss: 1.8372523784637451
Epoch 2960, training loss: 619.9591064453125 = 0.017608778551220894 + 100.0 * 6.1994147300720215
Epoch 2960, val loss: 1.8401824235916138
Epoch 2970, training loss: 619.8323364257812 = 0.01741667650640011 + 100.0 * 6.198149681091309
Epoch 2970, val loss: 1.8430438041687012
Epoch 2980, training loss: 619.8831787109375 = 0.017234759405255318 + 100.0 * 6.198659896850586
Epoch 2980, val loss: 1.8458442687988281
Epoch 2990, training loss: 620.4481811523438 = 0.017058227211236954 + 100.0 * 6.204311370849609
Epoch 2990, val loss: 1.8481135368347168
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6555555555555556
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 861.6344604492188 = 1.953762173652649 + 100.0 * 8.596807479858398
Epoch 0, val loss: 1.9437278509140015
Epoch 10, training loss: 861.5241088867188 = 1.9445466995239258 + 100.0 * 8.595795631408691
Epoch 10, val loss: 1.9346853494644165
Epoch 20, training loss: 860.8305053710938 = 1.9326040744781494 + 100.0 * 8.58897876739502
Epoch 20, val loss: 1.9225292205810547
Epoch 30, training loss: 856.6729736328125 = 1.916685938835144 + 100.0 * 8.547562599182129
Epoch 30, val loss: 1.9059730768203735
Epoch 40, training loss: 835.0130615234375 = 1.89858078956604 + 100.0 * 8.331145286560059
Epoch 40, val loss: 1.8879386186599731
Epoch 50, training loss: 770.8276977539062 = 1.8785572052001953 + 100.0 * 7.6894917488098145
Epoch 50, val loss: 1.8677719831466675
Epoch 60, training loss: 751.99169921875 = 1.8613239526748657 + 100.0 * 7.501303672790527
Epoch 60, val loss: 1.8517324924468994
Epoch 70, training loss: 735.3724975585938 = 1.849476933479309 + 100.0 * 7.335229873657227
Epoch 70, val loss: 1.8409732580184937
Epoch 80, training loss: 712.1356201171875 = 1.8395766019821167 + 100.0 * 7.102960109710693
Epoch 80, val loss: 1.832019329071045
Epoch 90, training loss: 690.0626220703125 = 1.8320488929748535 + 100.0 * 6.882306098937988
Epoch 90, val loss: 1.8258869647979736
Epoch 100, training loss: 678.9401245117188 = 1.826279640197754 + 100.0 * 6.7711381912231445
Epoch 100, val loss: 1.8205379247665405
Epoch 110, training loss: 671.1539916992188 = 1.819181203842163 + 100.0 * 6.693348407745361
Epoch 110, val loss: 1.8143552541732788
Epoch 120, training loss: 666.1644897460938 = 1.8129702806472778 + 100.0 * 6.643515110015869
Epoch 120, val loss: 1.808969497680664
Epoch 130, training loss: 662.8575439453125 = 1.8070224523544312 + 100.0 * 6.610505104064941
Epoch 130, val loss: 1.8036038875579834
Epoch 140, training loss: 660.341552734375 = 1.8010804653167725 + 100.0 * 6.585404872894287
Epoch 140, val loss: 1.7980995178222656
Epoch 150, training loss: 657.9979858398438 = 1.7952393293380737 + 100.0 * 6.562027454376221
Epoch 150, val loss: 1.792737603187561
Epoch 160, training loss: 655.7339477539062 = 1.7895022630691528 + 100.0 * 6.539444446563721
Epoch 160, val loss: 1.7874164581298828
Epoch 170, training loss: 653.6273803710938 = 1.7835562229156494 + 100.0 * 6.51843786239624
Epoch 170, val loss: 1.7819454669952393
Epoch 180, training loss: 651.4962158203125 = 1.777230978012085 + 100.0 * 6.497189998626709
Epoch 180, val loss: 1.7762101888656616
Epoch 190, training loss: 650.1369018554688 = 1.7704426050186157 + 100.0 * 6.483664512634277
Epoch 190, val loss: 1.7700659036636353
Epoch 200, training loss: 648.458251953125 = 1.7629314661026 + 100.0 * 6.466953277587891
Epoch 200, val loss: 1.7633306980133057
Epoch 210, training loss: 647.1785278320312 = 1.7547227144241333 + 100.0 * 6.454238414764404
Epoch 210, val loss: 1.7559682130813599
Epoch 220, training loss: 646.0214233398438 = 1.745764136314392 + 100.0 * 6.442756175994873
Epoch 220, val loss: 1.7479522228240967
Epoch 230, training loss: 645.080322265625 = 1.73605477809906 + 100.0 * 6.433442115783691
Epoch 230, val loss: 1.739276647567749
Epoch 240, training loss: 644.3853759765625 = 1.7255795001983643 + 100.0 * 6.426598072052002
Epoch 240, val loss: 1.7298866510391235
Epoch 250, training loss: 643.3911743164062 = 1.714178204536438 + 100.0 * 6.416769981384277
Epoch 250, val loss: 1.7196961641311646
Epoch 260, training loss: 642.5949096679688 = 1.701877236366272 + 100.0 * 6.408929824829102
Epoch 260, val loss: 1.708654522895813
Epoch 270, training loss: 641.94580078125 = 1.6886483430862427 + 100.0 * 6.402571678161621
Epoch 270, val loss: 1.6968516111373901
Epoch 280, training loss: 641.3321533203125 = 1.674415946006775 + 100.0 * 6.39657735824585
Epoch 280, val loss: 1.6841099262237549
Epoch 290, training loss: 640.7647094726562 = 1.6592692136764526 + 100.0 * 6.391054630279541
Epoch 290, val loss: 1.6705904006958008
Epoch 300, training loss: 640.2015380859375 = 1.643086552619934 + 100.0 * 6.385584831237793
Epoch 300, val loss: 1.6561639308929443
Epoch 310, training loss: 639.8035888671875 = 1.6260576248168945 + 100.0 * 6.381775379180908
Epoch 310, val loss: 1.6410094499588013
Epoch 320, training loss: 639.2044067382812 = 1.60821533203125 + 100.0 * 6.375961780548096
Epoch 320, val loss: 1.6251264810562134
Epoch 330, training loss: 638.6084594726562 = 1.5896320343017578 + 100.0 * 6.370187759399414
Epoch 330, val loss: 1.6087199449539185
Epoch 340, training loss: 638.2117309570312 = 1.5705090761184692 + 100.0 * 6.36641263961792
Epoch 340, val loss: 1.5919015407562256
Epoch 350, training loss: 637.9705200195312 = 1.5507488250732422 + 100.0 * 6.364197731018066
Epoch 350, val loss: 1.5744953155517578
Epoch 360, training loss: 637.305908203125 = 1.5306658744812012 + 100.0 * 6.357752799987793
Epoch 360, val loss: 1.5570096969604492
Epoch 370, training loss: 636.818115234375 = 1.5103365182876587 + 100.0 * 6.3530778884887695
Epoch 370, val loss: 1.5394574403762817
Epoch 380, training loss: 637.0164794921875 = 1.4898326396942139 + 100.0 * 6.355266571044922
Epoch 380, val loss: 1.5217137336730957
Epoch 390, training loss: 636.1065673828125 = 1.4690419435501099 + 100.0 * 6.346374988555908
Epoch 390, val loss: 1.5041521787643433
Epoch 400, training loss: 635.7130737304688 = 1.4484351873397827 + 100.0 * 6.342646598815918
Epoch 400, val loss: 1.4868431091308594
Epoch 410, training loss: 635.3345947265625 = 1.4279590845108032 + 100.0 * 6.339066028594971
Epoch 410, val loss: 1.4698044061660767
Epoch 420, training loss: 635.1519165039062 = 1.407582402229309 + 100.0 * 6.3374433517456055
Epoch 420, val loss: 1.4529907703399658
Epoch 430, training loss: 634.768798828125 = 1.3872474431991577 + 100.0 * 6.333815574645996
Epoch 430, val loss: 1.4364569187164307
Epoch 440, training loss: 634.3787231445312 = 1.3670843839645386 + 100.0 * 6.3301167488098145
Epoch 440, val loss: 1.4201624393463135
Epoch 450, training loss: 634.85986328125 = 1.346988320350647 + 100.0 * 6.3351287841796875
Epoch 450, val loss: 1.404330849647522
Epoch 460, training loss: 633.8462524414062 = 1.3271195888519287 + 100.0 * 6.325191020965576
Epoch 460, val loss: 1.3886253833770752
Epoch 470, training loss: 633.5203247070312 = 1.3073997497558594 + 100.0 * 6.322129726409912
Epoch 470, val loss: 1.3732236623764038
Epoch 480, training loss: 633.1617431640625 = 1.2878079414367676 + 100.0 * 6.318739891052246
Epoch 480, val loss: 1.3581522703170776
Epoch 490, training loss: 633.4867553710938 = 1.268332600593567 + 100.0 * 6.3221845626831055
Epoch 490, val loss: 1.343377709388733
Epoch 500, training loss: 632.8622436523438 = 1.2487965822219849 + 100.0 * 6.316134929656982
Epoch 500, val loss: 1.3286793231964111
Epoch 510, training loss: 632.4342041015625 = 1.2293500900268555 + 100.0 * 6.312048435211182
Epoch 510, val loss: 1.3143295049667358
Epoch 520, training loss: 632.185546875 = 1.210055947303772 + 100.0 * 6.309754848480225
Epoch 520, val loss: 1.3003239631652832
Epoch 530, training loss: 631.9961547851562 = 1.1907316446304321 + 100.0 * 6.308053970336914
Epoch 530, val loss: 1.28639554977417
Epoch 540, training loss: 631.6242065429688 = 1.171433448791504 + 100.0 * 6.304527282714844
Epoch 540, val loss: 1.2726552486419678
Epoch 550, training loss: 631.748779296875 = 1.152165174484253 + 100.0 * 6.305965900421143
Epoch 550, val loss: 1.2590525150299072
Epoch 560, training loss: 631.4904174804688 = 1.1331053972244263 + 100.0 * 6.303573131561279
Epoch 560, val loss: 1.2457380294799805
Epoch 570, training loss: 630.9846801757812 = 1.1139360666275024 + 100.0 * 6.298707008361816
Epoch 570, val loss: 1.2323557138442993
Epoch 580, training loss: 630.775634765625 = 1.094936728477478 + 100.0 * 6.296807289123535
Epoch 580, val loss: 1.21922767162323
Epoch 590, training loss: 630.6536254882812 = 1.0761570930480957 + 100.0 * 6.295774459838867
Epoch 590, val loss: 1.206465244293213
Epoch 600, training loss: 630.8038330078125 = 1.0575121641159058 + 100.0 * 6.297463417053223
Epoch 600, val loss: 1.1940900087356567
Epoch 610, training loss: 630.2713623046875 = 1.0387881994247437 + 100.0 * 6.292325496673584
Epoch 610, val loss: 1.1816655397415161
Epoch 620, training loss: 629.9441528320312 = 1.020411491394043 + 100.0 * 6.2892374992370605
Epoch 620, val loss: 1.1697139739990234
Epoch 630, training loss: 629.7330932617188 = 1.00243079662323 + 100.0 * 6.287306308746338
Epoch 630, val loss: 1.1579837799072266
Epoch 640, training loss: 629.9317016601562 = 0.9845910668373108 + 100.0 * 6.289470672607422
Epoch 640, val loss: 1.1466939449310303
Epoch 650, training loss: 630.015625 = 0.9667685627937317 + 100.0 * 6.290488243103027
Epoch 650, val loss: 1.1352248191833496
Epoch 660, training loss: 629.424072265625 = 0.9492754936218262 + 100.0 * 6.284748077392578
Epoch 660, val loss: 1.1244779825210571
Epoch 670, training loss: 629.0859375 = 0.9322357177734375 + 100.0 * 6.281537055969238
Epoch 670, val loss: 1.1140626668930054
Epoch 680, training loss: 628.8908081054688 = 0.9155497550964355 + 100.0 * 6.279752731323242
Epoch 680, val loss: 1.1041213274002075
Epoch 690, training loss: 629.6091918945312 = 0.8991391062736511 + 100.0 * 6.287100791931152
Epoch 690, val loss: 1.0945053100585938
Epoch 700, training loss: 628.6470336914062 = 0.8828121423721313 + 100.0 * 6.277642250061035
Epoch 700, val loss: 1.085191249847412
Epoch 710, training loss: 628.5323486328125 = 0.8668701648712158 + 100.0 * 6.2766547203063965
Epoch 710, val loss: 1.076141357421875
Epoch 720, training loss: 628.5933227539062 = 0.851294219493866 + 100.0 * 6.2774200439453125
Epoch 720, val loss: 1.0676870346069336
Epoch 730, training loss: 628.5380859375 = 0.8358616828918457 + 100.0 * 6.277021884918213
Epoch 730, val loss: 1.0592297315597534
Epoch 740, training loss: 628.199462890625 = 0.8207710385322571 + 100.0 * 6.273786544799805
Epoch 740, val loss: 1.051489233970642
Epoch 750, training loss: 627.9147338867188 = 0.8059567213058472 + 100.0 * 6.271087646484375
Epoch 750, val loss: 1.0438657999038696
Epoch 760, training loss: 627.7903442382812 = 0.7915486097335815 + 100.0 * 6.2699875831604
Epoch 760, val loss: 1.0366744995117188
Epoch 770, training loss: 628.4993896484375 = 0.7774115800857544 + 100.0 * 6.277219772338867
Epoch 770, val loss: 1.0296598672866821
Epoch 780, training loss: 627.6783447265625 = 0.7630442380905151 + 100.0 * 6.269153118133545
Epoch 780, val loss: 1.0229662656784058
Epoch 790, training loss: 627.4512939453125 = 0.7491551637649536 + 100.0 * 6.267021179199219
Epoch 790, val loss: 1.016618251800537
Epoch 800, training loss: 627.3367309570312 = 0.7356332540512085 + 100.0 * 6.266010761260986
Epoch 800, val loss: 1.0107223987579346
Epoch 810, training loss: 627.151123046875 = 0.7224063277244568 + 100.0 * 6.264286994934082
Epoch 810, val loss: 1.005112886428833
Epoch 820, training loss: 628.3023681640625 = 0.7093290686607361 + 100.0 * 6.275929927825928
Epoch 820, val loss: 0.9994514584541321
Epoch 830, training loss: 627.0586547851562 = 0.6963198184967041 + 100.0 * 6.263622760772705
Epoch 830, val loss: 0.9944079518318176
Epoch 840, training loss: 626.837890625 = 0.6835928559303284 + 100.0 * 6.261542797088623
Epoch 840, val loss: 0.9894489645957947
Epoch 850, training loss: 626.7294311523438 = 0.6711973547935486 + 100.0 * 6.260582447052002
Epoch 850, val loss: 0.9849211573600769
Epoch 860, training loss: 627.7395629882812 = 0.6590725779533386 + 100.0 * 6.2708048820495605
Epoch 860, val loss: 0.9806545972824097
Epoch 870, training loss: 626.7485961914062 = 0.6466914415359497 + 100.0 * 6.261019229888916
Epoch 870, val loss: 0.9763469696044922
Epoch 880, training loss: 626.5803833007812 = 0.6347443461418152 + 100.0 * 6.259456157684326
Epoch 880, val loss: 0.9723055362701416
Epoch 890, training loss: 626.65966796875 = 0.6230915784835815 + 100.0 * 6.2603654861450195
Epoch 890, val loss: 0.9687162041664124
Epoch 900, training loss: 626.2699584960938 = 0.6115207076072693 + 100.0 * 6.256584644317627
Epoch 900, val loss: 0.9653348326683044
Epoch 910, training loss: 626.2120361328125 = 0.6001834273338318 + 100.0 * 6.2561187744140625
Epoch 910, val loss: 0.9621149301528931
Epoch 920, training loss: 626.1068115234375 = 0.5891211032867432 + 100.0 * 6.255177021026611
Epoch 920, val loss: 0.959276556968689
Epoch 930, training loss: 626.5989379882812 = 0.5782319903373718 + 100.0 * 6.260206699371338
Epoch 930, val loss: 0.9565020799636841
Epoch 940, training loss: 626.0126342773438 = 0.5672875642776489 + 100.0 * 6.254453659057617
Epoch 940, val loss: 0.9539928436279297
Epoch 950, training loss: 625.775390625 = 0.5567148327827454 + 100.0 * 6.2521867752075195
Epoch 950, val loss: 0.9516409635543823
Epoch 960, training loss: 625.6986083984375 = 0.546334981918335 + 100.0 * 6.251522541046143
Epoch 960, val loss: 0.9496863484382629
Epoch 970, training loss: 626.6485595703125 = 0.536125659942627 + 100.0 * 6.261124134063721
Epoch 970, val loss: 0.9478586316108704
Epoch 980, training loss: 625.6581420898438 = 0.5258373022079468 + 100.0 * 6.251323223114014
Epoch 980, val loss: 0.9458895325660706
Epoch 990, training loss: 625.46240234375 = 0.5157980918884277 + 100.0 * 6.2494659423828125
Epoch 990, val loss: 0.9442459940910339
Epoch 1000, training loss: 625.4152221679688 = 0.5060844421386719 + 100.0 * 6.249091148376465
Epoch 1000, val loss: 0.9429543018341064
Epoch 1010, training loss: 625.7850341796875 = 0.49654215574264526 + 100.0 * 6.252884864807129
Epoch 1010, val loss: 0.9418229460716248
Epoch 1020, training loss: 625.4968872070312 = 0.4869724214076996 + 100.0 * 6.250098705291748
Epoch 1020, val loss: 0.9410461783409119
Epoch 1030, training loss: 625.3013916015625 = 0.4775497317314148 + 100.0 * 6.248238563537598
Epoch 1030, val loss: 0.939864993095398
Epoch 1040, training loss: 625.2194213867188 = 0.4683438837528229 + 100.0 * 6.24751091003418
Epoch 1040, val loss: 0.9392545223236084
Epoch 1050, training loss: 625.1404418945312 = 0.4593020975589752 + 100.0 * 6.246811389923096
Epoch 1050, val loss: 0.9388419985771179
Epoch 1060, training loss: 625.115966796875 = 0.4504242539405823 + 100.0 * 6.246654987335205
Epoch 1060, val loss: 0.938335120677948
Epoch 1070, training loss: 624.9549560546875 = 0.4415435194969177 + 100.0 * 6.245134353637695
Epoch 1070, val loss: 0.9382504820823669
Epoch 1080, training loss: 625.2742919921875 = 0.4328695833683014 + 100.0 * 6.248414516448975
Epoch 1080, val loss: 0.9378322958946228
Epoch 1090, training loss: 624.6941528320312 = 0.42431458830833435 + 100.0 * 6.2426981925964355
Epoch 1090, val loss: 0.9379198551177979
Epoch 1100, training loss: 624.7452392578125 = 0.4160116910934448 + 100.0 * 6.243292331695557
Epoch 1100, val loss: 0.9382408857345581
Epoch 1110, training loss: 624.7579956054688 = 0.4078640043735504 + 100.0 * 6.243501663208008
Epoch 1110, val loss: 0.9387404322624207
Epoch 1120, training loss: 624.7198486328125 = 0.3997814357280731 + 100.0 * 6.243200778961182
Epoch 1120, val loss: 0.9391049742698669
Epoch 1130, training loss: 624.978515625 = 0.3917987644672394 + 100.0 * 6.2458672523498535
Epoch 1130, val loss: 0.939659595489502
Epoch 1140, training loss: 624.5577392578125 = 0.3839452266693115 + 100.0 * 6.2417378425598145
Epoch 1140, val loss: 0.9404693245887756
Epoch 1150, training loss: 624.3877563476562 = 0.3762476146221161 + 100.0 * 6.240115165710449
Epoch 1150, val loss: 0.941227912902832
Epoch 1160, training loss: 624.367431640625 = 0.36872199177742004 + 100.0 * 6.239987373352051
Epoch 1160, val loss: 0.9421303272247314
Epoch 1170, training loss: 624.5240478515625 = 0.36118370294570923 + 100.0 * 6.241628646850586
Epoch 1170, val loss: 0.9433156251907349
Epoch 1180, training loss: 624.3764038085938 = 0.35376352071762085 + 100.0 * 6.2402262687683105
Epoch 1180, val loss: 0.9440934062004089
Epoch 1190, training loss: 624.0897216796875 = 0.3465469479560852 + 100.0 * 6.237431526184082
Epoch 1190, val loss: 0.9456049203872681
Epoch 1200, training loss: 624.3142700195312 = 0.3395353853702545 + 100.0 * 6.239747524261475
Epoch 1200, val loss: 0.9473612904548645
Epoch 1210, training loss: 623.99658203125 = 0.3325391113758087 + 100.0 * 6.236640930175781
Epoch 1210, val loss: 0.9483636021614075
Epoch 1220, training loss: 623.9564208984375 = 0.3256675601005554 + 100.0 * 6.236307621002197
Epoch 1220, val loss: 0.9499944448471069
Epoch 1230, training loss: 623.9032592773438 = 0.3189999461174011 + 100.0 * 6.235843181610107
Epoch 1230, val loss: 0.9514946937561035
Epoch 1240, training loss: 624.2186279296875 = 0.31249764561653137 + 100.0 * 6.23906135559082
Epoch 1240, val loss: 0.9535329341888428
Epoch 1250, training loss: 623.846435546875 = 0.30593621730804443 + 100.0 * 6.235404968261719
Epoch 1250, val loss: 0.955068051815033
Epoch 1260, training loss: 623.8012084960938 = 0.29960960149765015 + 100.0 * 6.235015869140625
Epoch 1260, val loss: 0.9570171236991882
Epoch 1270, training loss: 623.7122192382812 = 0.29341334104537964 + 100.0 * 6.234188079833984
Epoch 1270, val loss: 0.9590117335319519
Epoch 1280, training loss: 624.046142578125 = 0.28742119669914246 + 100.0 * 6.2375874519348145
Epoch 1280, val loss: 0.9609039425849915
Epoch 1290, training loss: 623.8580322265625 = 0.2813609540462494 + 100.0 * 6.235766887664795
Epoch 1290, val loss: 0.9632909893989563
Epoch 1300, training loss: 623.7523803710938 = 0.275532990694046 + 100.0 * 6.234768867492676
Epoch 1300, val loss: 0.9651324152946472
Epoch 1310, training loss: 623.5302124023438 = 0.2698189318180084 + 100.0 * 6.232603549957275
Epoch 1310, val loss: 0.967453122138977
Epoch 1320, training loss: 623.4522094726562 = 0.2642521560192108 + 100.0 * 6.231879234313965
Epoch 1320, val loss: 0.9698903560638428
Epoch 1330, training loss: 624.0455932617188 = 0.2588278353214264 + 100.0 * 6.23786735534668
Epoch 1330, val loss: 0.9720032811164856
Epoch 1340, training loss: 623.6415405273438 = 0.253430038690567 + 100.0 * 6.23388147354126
Epoch 1340, val loss: 0.9749271869659424
Epoch 1350, training loss: 623.3944702148438 = 0.24812322854995728 + 100.0 * 6.231463432312012
Epoch 1350, val loss: 0.9771241545677185
Epoch 1360, training loss: 623.4043579101562 = 0.24303807318210602 + 100.0 * 6.2316131591796875
Epoch 1360, val loss: 0.9798443913459778
Epoch 1370, training loss: 623.506103515625 = 0.2380051463842392 + 100.0 * 6.2326812744140625
Epoch 1370, val loss: 0.9825058579444885
Epoch 1380, training loss: 623.2469482421875 = 0.23308037221431732 + 100.0 * 6.230138301849365
Epoch 1380, val loss: 0.9854192733764648
Epoch 1390, training loss: 623.2283935546875 = 0.22829237580299377 + 100.0 * 6.230001449584961
Epoch 1390, val loss: 0.9882283806800842
Epoch 1400, training loss: 623.5568237304688 = 0.22360922396183014 + 100.0 * 6.23333215713501
Epoch 1400, val loss: 0.991152286529541
Epoch 1410, training loss: 623.349609375 = 0.21897479891777039 + 100.0 * 6.231306076049805
Epoch 1410, val loss: 0.9942337870597839
Epoch 1420, training loss: 623.0929565429688 = 0.21445727348327637 + 100.0 * 6.228784561157227
Epoch 1420, val loss: 0.9971190094947815
Epoch 1430, training loss: 622.9517211914062 = 0.21010389924049377 + 100.0 * 6.227416515350342
Epoch 1430, val loss: 1.0005464553833008
Epoch 1440, training loss: 623.1685791015625 = 0.20587635040283203 + 100.0 * 6.2296271324157715
Epoch 1440, val loss: 1.003696084022522
Epoch 1450, training loss: 622.9928588867188 = 0.2016015499830246 + 100.0 * 6.227912425994873
Epoch 1450, val loss: 1.0071195363998413
Epoch 1460, training loss: 622.906494140625 = 0.19743025302886963 + 100.0 * 6.227090358734131
Epoch 1460, val loss: 1.0099588632583618
Epoch 1470, training loss: 622.8804931640625 = 0.19342069327831268 + 100.0 * 6.226870536804199
Epoch 1470, val loss: 1.0136077404022217
Epoch 1480, training loss: 623.0478515625 = 0.18952018022537231 + 100.0 * 6.228583335876465
Epoch 1480, val loss: 1.016711711883545
Epoch 1490, training loss: 622.7009887695312 = 0.18567442893981934 + 100.0 * 6.225152969360352
Epoch 1490, val loss: 1.0205390453338623
Epoch 1500, training loss: 622.740966796875 = 0.18194766342639923 + 100.0 * 6.225590229034424
Epoch 1500, val loss: 1.0241817235946655
Epoch 1510, training loss: 622.9957275390625 = 0.17830540239810944 + 100.0 * 6.228174209594727
Epoch 1510, val loss: 1.0276058912277222
Epoch 1520, training loss: 622.6214599609375 = 0.17467552423477173 + 100.0 * 6.224467754364014
Epoch 1520, val loss: 1.0318012237548828
Epoch 1530, training loss: 622.7567138671875 = 0.17117612063884735 + 100.0 * 6.225854873657227
Epoch 1530, val loss: 1.0353655815124512
Epoch 1540, training loss: 622.6329345703125 = 0.1677497774362564 + 100.0 * 6.224652290344238
Epoch 1540, val loss: 1.0391075611114502
Epoch 1550, training loss: 622.668701171875 = 0.16443005204200745 + 100.0 * 6.225042819976807
Epoch 1550, val loss: 1.0430774688720703
Epoch 1560, training loss: 622.7745971679688 = 0.16114948689937592 + 100.0 * 6.226134777069092
Epoch 1560, val loss: 1.0465964078903198
Epoch 1570, training loss: 622.5516967773438 = 0.15786466002464294 + 100.0 * 6.223938465118408
Epoch 1570, val loss: 1.0503135919570923
Epoch 1580, training loss: 622.4312133789062 = 0.1547539085149765 + 100.0 * 6.22276496887207
Epoch 1580, val loss: 1.054140567779541
Epoch 1590, training loss: 622.404541015625 = 0.15171962976455688 + 100.0 * 6.222527980804443
Epoch 1590, val loss: 1.058526635169983
Epoch 1600, training loss: 622.7938842773438 = 0.14876969158649445 + 100.0 * 6.226451396942139
Epoch 1600, val loss: 1.0625842809677124
Epoch 1610, training loss: 622.34765625 = 0.14579567313194275 + 100.0 * 6.222018718719482
Epoch 1610, val loss: 1.0662161111831665
Epoch 1620, training loss: 622.6099853515625 = 0.14293695986270905 + 100.0 * 6.22467041015625
Epoch 1620, val loss: 1.0700985193252563
Epoch 1630, training loss: 622.3079833984375 = 0.1401270180940628 + 100.0 * 6.221678256988525
Epoch 1630, val loss: 1.0745338201522827
Epoch 1640, training loss: 622.2283325195312 = 0.1374003291130066 + 100.0 * 6.220909118652344
Epoch 1640, val loss: 1.0785776376724243
Epoch 1650, training loss: 623.0988159179688 = 0.13476862013339996 + 100.0 * 6.229640483856201
Epoch 1650, val loss: 1.0831425189971924
Epoch 1660, training loss: 622.3909301757812 = 0.13207900524139404 + 100.0 * 6.222588539123535
Epoch 1660, val loss: 1.0867384672164917
Epoch 1670, training loss: 622.0782470703125 = 0.12949751317501068 + 100.0 * 6.219487190246582
Epoch 1670, val loss: 1.0911026000976562
Epoch 1680, training loss: 622.0368041992188 = 0.12703345715999603 + 100.0 * 6.219098091125488
Epoch 1680, val loss: 1.0957001447677612
Epoch 1690, training loss: 622.271240234375 = 0.1246478483080864 + 100.0 * 6.221466064453125
Epoch 1690, val loss: 1.0996683835983276
Epoch 1700, training loss: 622.1728515625 = 0.12222269177436829 + 100.0 * 6.22050666809082
Epoch 1700, val loss: 1.1042383909225464
Epoch 1710, training loss: 622.0333251953125 = 0.1198456883430481 + 100.0 * 6.219134330749512
Epoch 1710, val loss: 1.1085399389266968
Epoch 1720, training loss: 621.983154296875 = 0.11755943298339844 + 100.0 * 6.218656063079834
Epoch 1720, val loss: 1.1128202676773071
Epoch 1730, training loss: 622.0364990234375 = 0.11538306623697281 + 100.0 * 6.219211578369141
Epoch 1730, val loss: 1.117555856704712
Epoch 1740, training loss: 622.181884765625 = 0.11321302503347397 + 100.0 * 6.220686912536621
Epoch 1740, val loss: 1.1219984292984009
Epoch 1750, training loss: 622.0366821289062 = 0.1110282689332962 + 100.0 * 6.219256401062012
Epoch 1750, val loss: 1.1262662410736084
Epoch 1760, training loss: 621.9769287109375 = 0.10896275192499161 + 100.0 * 6.218679904937744
Epoch 1760, val loss: 1.1310572624206543
Epoch 1770, training loss: 621.9506225585938 = 0.10691077262163162 + 100.0 * 6.2184367179870605
Epoch 1770, val loss: 1.1354330778121948
Epoch 1780, training loss: 622.158935546875 = 0.10493216663599014 + 100.0 * 6.2205400466918945
Epoch 1780, val loss: 1.139669418334961
Epoch 1790, training loss: 622.3648681640625 = 0.10292989760637283 + 100.0 * 6.222619533538818
Epoch 1790, val loss: 1.1440083980560303
Epoch 1800, training loss: 621.9053955078125 = 0.10097384452819824 + 100.0 * 6.218044281005859
Epoch 1800, val loss: 1.1487776041030884
Epoch 1810, training loss: 621.7361450195312 = 0.09906405210494995 + 100.0 * 6.216371059417725
Epoch 1810, val loss: 1.153037428855896
Epoch 1820, training loss: 621.6860961914062 = 0.09726251661777496 + 100.0 * 6.215888023376465
Epoch 1820, val loss: 1.1577187776565552
Epoch 1830, training loss: 621.6299438476562 = 0.09549917280673981 + 100.0 * 6.215343952178955
Epoch 1830, val loss: 1.162522315979004
Epoch 1840, training loss: 622.0562744140625 = 0.0937880352139473 + 100.0 * 6.2196245193481445
Epoch 1840, val loss: 1.167093276977539
Epoch 1850, training loss: 621.6017456054688 = 0.09202484786510468 + 100.0 * 6.215096950531006
Epoch 1850, val loss: 1.1711618900299072
Epoch 1860, training loss: 621.5664672851562 = 0.09033375233411789 + 100.0 * 6.214761257171631
Epoch 1860, val loss: 1.175811767578125
Epoch 1870, training loss: 621.5525512695312 = 0.08870427310466766 + 100.0 * 6.214638710021973
Epoch 1870, val loss: 1.1803503036499023
Epoch 1880, training loss: 621.9452514648438 = 0.08711466938257217 + 100.0 * 6.218581676483154
Epoch 1880, val loss: 1.184518575668335
Epoch 1890, training loss: 621.6779174804688 = 0.08552392572164536 + 100.0 * 6.215924263000488
Epoch 1890, val loss: 1.1898897886276245
Epoch 1900, training loss: 621.7984619140625 = 0.08396134525537491 + 100.0 * 6.21714448928833
Epoch 1900, val loss: 1.1937673091888428
Epoch 1910, training loss: 621.5377197265625 = 0.08243966847658157 + 100.0 * 6.214552879333496
Epoch 1910, val loss: 1.1985461711883545
Epoch 1920, training loss: 621.4365234375 = 0.08096478879451752 + 100.0 * 6.213555335998535
Epoch 1920, val loss: 1.203337550163269
Epoch 1930, training loss: 621.4487915039062 = 0.07953816652297974 + 100.0 * 6.213692665100098
Epoch 1930, val loss: 1.2083537578582764
Epoch 1940, training loss: 621.6307373046875 = 0.07814088463783264 + 100.0 * 6.215526103973389
Epoch 1940, val loss: 1.2129182815551758
Epoch 1950, training loss: 621.6218872070312 = 0.07673899084329605 + 100.0 * 6.215451717376709
Epoch 1950, val loss: 1.2170573472976685
Epoch 1960, training loss: 621.5393676757812 = 0.07536517083644867 + 100.0 * 6.214640140533447
Epoch 1960, val loss: 1.2215455770492554
Epoch 1970, training loss: 621.6827392578125 = 0.07403212040662766 + 100.0 * 6.2160868644714355
Epoch 1970, val loss: 1.2261292934417725
Epoch 1980, training loss: 621.5931396484375 = 0.07271933555603027 + 100.0 * 6.215204238891602
Epoch 1980, val loss: 1.2302497625350952
Epoch 1990, training loss: 621.3939819335938 = 0.07143500447273254 + 100.0 * 6.213225841522217
Epoch 1990, val loss: 1.2353588342666626
Epoch 2000, training loss: 621.2305297851562 = 0.07018373161554337 + 100.0 * 6.21160364151001
Epoch 2000, val loss: 1.2398055791854858
Epoch 2010, training loss: 621.2172241210938 = 0.06899306923151016 + 100.0 * 6.211482524871826
Epoch 2010, val loss: 1.2444427013397217
Epoch 2020, training loss: 621.5162353515625 = 0.06782207638025284 + 100.0 * 6.214484214782715
Epoch 2020, val loss: 1.2487359046936035
Epoch 2030, training loss: 621.2388305664062 = 0.06663738191127777 + 100.0 * 6.211721897125244
Epoch 2030, val loss: 1.2537028789520264
Epoch 2040, training loss: 621.1846313476562 = 0.06548520177602768 + 100.0 * 6.211191177368164
Epoch 2040, val loss: 1.2579705715179443
Epoch 2050, training loss: 621.2852783203125 = 0.06437450647354126 + 100.0 * 6.2122087478637695
Epoch 2050, val loss: 1.2624998092651367
Epoch 2060, training loss: 621.499267578125 = 0.06329237669706345 + 100.0 * 6.214359760284424
Epoch 2060, val loss: 1.2671303749084473
Epoch 2070, training loss: 621.5037231445312 = 0.06218687817454338 + 100.0 * 6.214415073394775
Epoch 2070, val loss: 1.2713532447814941
Epoch 2080, training loss: 621.16943359375 = 0.0611305795609951 + 100.0 * 6.211082935333252
Epoch 2080, val loss: 1.2759822607040405
Epoch 2090, training loss: 621.0750732421875 = 0.06010895222425461 + 100.0 * 6.210149765014648
Epoch 2090, val loss: 1.280458927154541
Epoch 2100, training loss: 621.1648559570312 = 0.05912957340478897 + 100.0 * 6.211057186126709
Epoch 2100, val loss: 1.2854030132293701
Epoch 2110, training loss: 621.4680786132812 = 0.05815097317099571 + 100.0 * 6.214099407196045
Epoch 2110, val loss: 1.2897673845291138
Epoch 2120, training loss: 621.33544921875 = 0.0571710430085659 + 100.0 * 6.212782859802246
Epoch 2120, val loss: 1.293695330619812
Epoch 2130, training loss: 621.2410278320312 = 0.05621819943189621 + 100.0 * 6.211848258972168
Epoch 2130, val loss: 1.298144817352295
Epoch 2140, training loss: 621.15869140625 = 0.05529197305440903 + 100.0 * 6.211033821105957
Epoch 2140, val loss: 1.30254065990448
Epoch 2150, training loss: 621.0137939453125 = 0.05437472462654114 + 100.0 * 6.209594249725342
Epoch 2150, val loss: 1.3072222471237183
Epoch 2160, training loss: 621.0061645507812 = 0.053502943366765976 + 100.0 * 6.209526538848877
Epoch 2160, val loss: 1.311845064163208
Epoch 2170, training loss: 621.0828247070312 = 0.052653949707746506 + 100.0 * 6.210301399230957
Epoch 2170, val loss: 1.316248893737793
Epoch 2180, training loss: 621.1944580078125 = 0.05179951339960098 + 100.0 * 6.211426258087158
Epoch 2180, val loss: 1.3200815916061401
Epoch 2190, training loss: 620.9622192382812 = 0.05094887688755989 + 100.0 * 6.209112644195557
Epoch 2190, val loss: 1.3241369724273682
Epoch 2200, training loss: 620.9180908203125 = 0.05013934522867203 + 100.0 * 6.208679676055908
Epoch 2200, val loss: 1.3289766311645508
Epoch 2210, training loss: 621.4570922851562 = 0.04936124011874199 + 100.0 * 6.214076995849609
Epoch 2210, val loss: 1.332777976989746
Epoch 2220, training loss: 621.3484497070312 = 0.04854965582489967 + 100.0 * 6.21299934387207
Epoch 2220, val loss: 1.3371766805648804
Epoch 2230, training loss: 620.9024658203125 = 0.04776795208454132 + 100.0 * 6.2085466384887695
Epoch 2230, val loss: 1.341746211051941
Epoch 2240, training loss: 620.7901000976562 = 0.04701518639922142 + 100.0 * 6.207430839538574
Epoch 2240, val loss: 1.345596194267273
Epoch 2250, training loss: 620.991943359375 = 0.04630034416913986 + 100.0 * 6.209456443786621
Epoch 2250, val loss: 1.3497077226638794
Epoch 2260, training loss: 620.9451904296875 = 0.04557189345359802 + 100.0 * 6.208995819091797
Epoch 2260, val loss: 1.354505181312561
Epoch 2270, training loss: 620.7578735351562 = 0.04484381899237633 + 100.0 * 6.207130432128906
Epoch 2270, val loss: 1.3586221933364868
Epoch 2280, training loss: 620.8580322265625 = 0.04416211321949959 + 100.0 * 6.208138465881348
Epoch 2280, val loss: 1.3628926277160645
Epoch 2290, training loss: 620.9257202148438 = 0.04348636046051979 + 100.0 * 6.208822727203369
Epoch 2290, val loss: 1.3670541048049927
Epoch 2300, training loss: 620.8120727539062 = 0.04281392693519592 + 100.0 * 6.207692623138428
Epoch 2300, val loss: 1.3708285093307495
Epoch 2310, training loss: 621.1087646484375 = 0.04216107353568077 + 100.0 * 6.210666179656982
Epoch 2310, val loss: 1.374632477760315
Epoch 2320, training loss: 620.729248046875 = 0.041504841297864914 + 100.0 * 6.206877708435059
Epoch 2320, val loss: 1.379416823387146
Epoch 2330, training loss: 620.7323608398438 = 0.040875595062971115 + 100.0 * 6.20691442489624
Epoch 2330, val loss: 1.3830621242523193
Epoch 2340, training loss: 620.833251953125 = 0.04026936739683151 + 100.0 * 6.207929611206055
Epoch 2340, val loss: 1.3869621753692627
Epoch 2350, training loss: 621.0029296875 = 0.039668675512075424 + 100.0 * 6.209632873535156
Epoch 2350, val loss: 1.3912159204483032
Epoch 2360, training loss: 620.740234375 = 0.03907516226172447 + 100.0 * 6.207011699676514
Epoch 2360, val loss: 1.3950904607772827
Epoch 2370, training loss: 620.5875244140625 = 0.03847863897681236 + 100.0 * 6.205490589141846
Epoch 2370, val loss: 1.3993784189224243
Epoch 2380, training loss: 620.5274047851562 = 0.03791915997862816 + 100.0 * 6.20489501953125
Epoch 2380, val loss: 1.4034817218780518
Epoch 2390, training loss: 620.6112060546875 = 0.0373823419213295 + 100.0 * 6.205738544464111
Epoch 2390, val loss: 1.4075011014938354
Epoch 2400, training loss: 621.1887817382812 = 0.03685125336050987 + 100.0 * 6.211519241333008
Epoch 2400, val loss: 1.4111342430114746
Epoch 2410, training loss: 620.8485717773438 = 0.0362774133682251 + 100.0 * 6.208123207092285
Epoch 2410, val loss: 1.4150843620300293
Epoch 2420, training loss: 620.6666870117188 = 0.035756319761276245 + 100.0 * 6.2063093185424805
Epoch 2420, val loss: 1.4191699028015137
Epoch 2430, training loss: 620.5172729492188 = 0.03523317351937294 + 100.0 * 6.20482063293457
Epoch 2430, val loss: 1.4231300354003906
Epoch 2440, training loss: 620.5264282226562 = 0.034742508083581924 + 100.0 * 6.204916954040527
Epoch 2440, val loss: 1.4270598888397217
Epoch 2450, training loss: 621.032470703125 = 0.03425402194261551 + 100.0 * 6.209982395172119
Epoch 2450, val loss: 1.4301649332046509
Epoch 2460, training loss: 621.2756958007812 = 0.033760908991098404 + 100.0 * 6.212419509887695
Epoch 2460, val loss: 1.4338818788528442
Epoch 2470, training loss: 620.7138671875 = 0.03325199708342552 + 100.0 * 6.206806182861328
Epoch 2470, val loss: 1.4387304782867432
Epoch 2480, training loss: 620.5068359375 = 0.032776203006505966 + 100.0 * 6.204740524291992
Epoch 2480, val loss: 1.4417364597320557
Epoch 2490, training loss: 620.3975219726562 = 0.03232291340827942 + 100.0 * 6.2036519050598145
Epoch 2490, val loss: 1.4462378025054932
Epoch 2500, training loss: 620.3975830078125 = 0.03188406676054001 + 100.0 * 6.203657150268555
Epoch 2500, val loss: 1.4500129222869873
Epoch 2510, training loss: 620.928955078125 = 0.031465064734220505 + 100.0 * 6.208974361419678
Epoch 2510, val loss: 1.4537676572799683
Epoch 2520, training loss: 620.42578125 = 0.031012659892439842 + 100.0 * 6.203948020935059
Epoch 2520, val loss: 1.4572358131408691
Epoch 2530, training loss: 620.3861694335938 = 0.030587509274482727 + 100.0 * 6.203556060791016
Epoch 2530, val loss: 1.461464524269104
Epoch 2540, training loss: 620.5909423828125 = 0.030183054506778717 + 100.0 * 6.2056074142456055
Epoch 2540, val loss: 1.4652162790298462
Epoch 2550, training loss: 620.7314453125 = 0.029764322564005852 + 100.0 * 6.207016468048096
Epoch 2550, val loss: 1.4685639142990112
Epoch 2560, training loss: 620.57080078125 = 0.029341189190745354 + 100.0 * 6.205414295196533
Epoch 2560, val loss: 1.4714518785476685
Epoch 2570, training loss: 620.2964477539062 = 0.028939982876181602 + 100.0 * 6.2026753425598145
Epoch 2570, val loss: 1.4754952192306519
Epoch 2580, training loss: 620.2947998046875 = 0.028561696410179138 + 100.0 * 6.202662467956543
Epoch 2580, val loss: 1.4794350862503052
Epoch 2590, training loss: 620.3059692382812 = 0.028194447979331017 + 100.0 * 6.202777862548828
Epoch 2590, val loss: 1.4834403991699219
Epoch 2600, training loss: 620.6555786132812 = 0.027830833569169044 + 100.0 * 6.206277370452881
Epoch 2600, val loss: 1.487075924873352
Epoch 2610, training loss: 620.2673950195312 = 0.02745412476360798 + 100.0 * 6.202399253845215
Epoch 2610, val loss: 1.489772915840149
Epoch 2620, training loss: 620.3876342773438 = 0.027095630764961243 + 100.0 * 6.2036051750183105
Epoch 2620, val loss: 1.4930447340011597
Epoch 2630, training loss: 620.3794555664062 = 0.02674015611410141 + 100.0 * 6.203527450561523
Epoch 2630, val loss: 1.4969372749328613
Epoch 2640, training loss: 620.3925170898438 = 0.026395555585622787 + 100.0 * 6.2036614418029785
Epoch 2640, val loss: 1.5000957250595093
Epoch 2650, training loss: 620.461181640625 = 0.026052700355648994 + 100.0 * 6.20435094833374
Epoch 2650, val loss: 1.5037562847137451
Epoch 2660, training loss: 620.2135620117188 = 0.025714587420225143 + 100.0 * 6.201878547668457
Epoch 2660, val loss: 1.5071625709533691
Epoch 2670, training loss: 620.2796020507812 = 0.025391994044184685 + 100.0 * 6.202542304992676
Epoch 2670, val loss: 1.5104931592941284
Epoch 2680, training loss: 620.6348266601562 = 0.025072496384382248 + 100.0 * 6.20609712600708
Epoch 2680, val loss: 1.5135568380355835
Epoch 2690, training loss: 620.2464599609375 = 0.024739891290664673 + 100.0 * 6.2022175788879395
Epoch 2690, val loss: 1.5173035860061646
Epoch 2700, training loss: 620.1162719726562 = 0.024429352954030037 + 100.0 * 6.200918674468994
Epoch 2700, val loss: 1.520464301109314
Epoch 2710, training loss: 620.1974487304688 = 0.024136170744895935 + 100.0 * 6.201733112335205
Epoch 2710, val loss: 1.5240180492401123
Epoch 2720, training loss: 620.694091796875 = 0.02385280281305313 + 100.0 * 6.20670223236084
Epoch 2720, val loss: 1.5274888277053833
Epoch 2730, training loss: 620.7431030273438 = 0.02352815307676792 + 100.0 * 6.20719575881958
Epoch 2730, val loss: 1.5297317504882812
Epoch 2740, training loss: 620.173583984375 = 0.023223206400871277 + 100.0 * 6.201503753662109
Epoch 2740, val loss: 1.5331518650054932
Epoch 2750, training loss: 620.0641479492188 = 0.022926796227693558 + 100.0 * 6.200411796569824
Epoch 2750, val loss: 1.5359556674957275
Epoch 2760, training loss: 620.022216796875 = 0.022657640278339386 + 100.0 * 6.199995994567871
Epoch 2760, val loss: 1.5393421649932861
Epoch 2770, training loss: 620.0335083007812 = 0.02239157259464264 + 100.0 * 6.200110912322998
Epoch 2770, val loss: 1.5427159070968628
Epoch 2780, training loss: 620.6629638671875 = 0.022134916856884956 + 100.0 * 6.2064080238342285
Epoch 2780, val loss: 1.5454657077789307
Epoch 2790, training loss: 620.21240234375 = 0.02184976078569889 + 100.0 * 6.201905250549316
Epoch 2790, val loss: 1.5483908653259277
Epoch 2800, training loss: 620.0901489257812 = 0.021580370143055916 + 100.0 * 6.200685501098633
Epoch 2800, val loss: 1.5510026216506958
Epoch 2810, training loss: 620.0137939453125 = 0.021322164684534073 + 100.0 * 6.199924945831299
Epoch 2810, val loss: 1.554366111755371
Epoch 2820, training loss: 620.053466796875 = 0.021077575162053108 + 100.0 * 6.200324058532715
Epoch 2820, val loss: 1.55739164352417
Epoch 2830, training loss: 620.2898559570312 = 0.020834848284721375 + 100.0 * 6.202690601348877
Epoch 2830, val loss: 1.5603188276290894
Epoch 2840, training loss: 620.2434692382812 = 0.020587503910064697 + 100.0 * 6.202229022979736
Epoch 2840, val loss: 1.5635946989059448
Epoch 2850, training loss: 620.1173706054688 = 0.020343242213129997 + 100.0 * 6.200970649719238
Epoch 2850, val loss: 1.5659795999526978
Epoch 2860, training loss: 620.2110595703125 = 0.020103851333260536 + 100.0 * 6.20190954208374
Epoch 2860, val loss: 1.568794846534729
Epoch 2870, training loss: 620.0255737304688 = 0.019861700013279915 + 100.0 * 6.200057506561279
Epoch 2870, val loss: 1.5719701051712036
Epoch 2880, training loss: 620.1071166992188 = 0.019633520394563675 + 100.0 * 6.2008748054504395
Epoch 2880, val loss: 1.5750144720077515
Epoch 2890, training loss: 619.9513549804688 = 0.019409991800785065 + 100.0 * 6.199319362640381
Epoch 2890, val loss: 1.5776385068893433
Epoch 2900, training loss: 620.0106811523438 = 0.019193734973669052 + 100.0 * 6.199915409088135
Epoch 2900, val loss: 1.5804754495620728
Epoch 2910, training loss: 620.109619140625 = 0.018978724256157875 + 100.0 * 6.200906276702881
Epoch 2910, val loss: 1.5831516981124878
Epoch 2920, training loss: 620.2351684570312 = 0.018761422485113144 + 100.0 * 6.202164173126221
Epoch 2920, val loss: 1.5856094360351562
Epoch 2930, training loss: 620.045166015625 = 0.018545091152191162 + 100.0 * 6.200265884399414
Epoch 2930, val loss: 1.5889273881912231
Epoch 2940, training loss: 619.8489379882812 = 0.018334809690713882 + 100.0 * 6.198306083679199
Epoch 2940, val loss: 1.5909935235977173
Epoch 2950, training loss: 620.0970458984375 = 0.018136125057935715 + 100.0 * 6.200789451599121
Epoch 2950, val loss: 1.5936815738677979
Epoch 2960, training loss: 619.9895629882812 = 0.017935745418071747 + 100.0 * 6.199716091156006
Epoch 2960, val loss: 1.5967414379119873
Epoch 2970, training loss: 619.8743286132812 = 0.017736729234457016 + 100.0 * 6.19856595993042
Epoch 2970, val loss: 1.5994676351547241
Epoch 2980, training loss: 619.964599609375 = 0.01754719205200672 + 100.0 * 6.1994709968566895
Epoch 2980, val loss: 1.601778507232666
Epoch 2990, training loss: 620.2355346679688 = 0.017360512167215347 + 100.0 * 6.202182292938232
Epoch 2990, val loss: 1.6040765047073364
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6703703703703704
0.8128624143384291
The final CL Acc:0.66914, 0.01062, The final GNN Acc:0.81075, 0.00149
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13222])
remove edge: torch.Size([2, 7862])
updated graph: torch.Size([2, 10528])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.641357421875 = 1.963878870010376 + 100.0 * 8.59677505493164
Epoch 0, val loss: 1.9679063558578491
Epoch 10, training loss: 861.4915771484375 = 1.9549869298934937 + 100.0 * 8.595365524291992
Epoch 10, val loss: 1.959463357925415
Epoch 20, training loss: 860.4348754882812 = 1.9436856508255005 + 100.0 * 8.584912300109863
Epoch 20, val loss: 1.948587417602539
Epoch 30, training loss: 854.1705932617188 = 1.9287326335906982 + 100.0 * 8.522418975830078
Epoch 30, val loss: 1.9340505599975586
Epoch 40, training loss: 824.122314453125 = 1.9116190671920776 + 100.0 * 8.22210693359375
Epoch 40, val loss: 1.9175740480422974
Epoch 50, training loss: 772.4794921875 = 1.8924860954284668 + 100.0 * 7.705870628356934
Epoch 50, val loss: 1.8996766805648804
Epoch 60, training loss: 732.06494140625 = 1.8780317306518555 + 100.0 * 7.301868915557861
Epoch 60, val loss: 1.8867452144622803
Epoch 70, training loss: 704.0136108398438 = 1.865081548690796 + 100.0 * 7.021485805511475
Epoch 70, val loss: 1.8743215799331665
Epoch 80, training loss: 688.5498657226562 = 1.8543195724487305 + 100.0 * 6.866955280303955
Epoch 80, val loss: 1.864107608795166
Epoch 90, training loss: 678.139892578125 = 1.8432658910751343 + 100.0 * 6.762966156005859
Epoch 90, val loss: 1.8533685207366943
Epoch 100, training loss: 672.1863403320312 = 1.832527756690979 + 100.0 * 6.703537940979004
Epoch 100, val loss: 1.8428694009780884
Epoch 110, training loss: 667.3834228515625 = 1.8223220109939575 + 100.0 * 6.655611038208008
Epoch 110, val loss: 1.8329969644546509
Epoch 120, training loss: 663.2404174804688 = 1.8134993314743042 + 100.0 * 6.614269256591797
Epoch 120, val loss: 1.824272632598877
Epoch 130, training loss: 659.935302734375 = 1.8059033155441284 + 100.0 * 6.581294059753418
Epoch 130, val loss: 1.816308856010437
Epoch 140, training loss: 657.11376953125 = 1.7984925508499146 + 100.0 * 6.553152561187744
Epoch 140, val loss: 1.8082821369171143
Epoch 150, training loss: 654.7274169921875 = 1.7909250259399414 + 100.0 * 6.529364585876465
Epoch 150, val loss: 1.800156831741333
Epoch 160, training loss: 652.7803344726562 = 1.783068299293518 + 100.0 * 6.50997257232666
Epoch 160, val loss: 1.7919453382492065
Epoch 170, training loss: 651.0816040039062 = 1.7749028205871582 + 100.0 * 6.493067264556885
Epoch 170, val loss: 1.7836359739303589
Epoch 180, training loss: 649.563720703125 = 1.7663801908493042 + 100.0 * 6.477973937988281
Epoch 180, val loss: 1.7751091718673706
Epoch 190, training loss: 648.006591796875 = 1.757314920425415 + 100.0 * 6.4624924659729
Epoch 190, val loss: 1.7661691904067993
Epoch 200, training loss: 646.5830688476562 = 1.7476987838745117 + 100.0 * 6.4483537673950195
Epoch 200, val loss: 1.7567574977874756
Epoch 210, training loss: 645.29833984375 = 1.7374058961868286 + 100.0 * 6.435608863830566
Epoch 210, val loss: 1.7468266487121582
Epoch 220, training loss: 644.4605712890625 = 1.7262287139892578 + 100.0 * 6.427343368530273
Epoch 220, val loss: 1.736146330833435
Epoch 230, training loss: 643.1859741210938 = 1.714104175567627 + 100.0 * 6.4147186279296875
Epoch 230, val loss: 1.7246817350387573
Epoch 240, training loss: 642.2335815429688 = 1.7010796070098877 + 100.0 * 6.405325412750244
Epoch 240, val loss: 1.712433934211731
Epoch 250, training loss: 641.3570556640625 = 1.6870317459106445 + 100.0 * 6.396700382232666
Epoch 250, val loss: 1.699220061302185
Epoch 260, training loss: 640.635986328125 = 1.6717981100082397 + 100.0 * 6.389641761779785
Epoch 260, val loss: 1.684989333152771
Epoch 270, training loss: 639.9165649414062 = 1.655521273612976 + 100.0 * 6.382610321044922
Epoch 270, val loss: 1.6698493957519531
Epoch 280, training loss: 639.1923217773438 = 1.6381518840789795 + 100.0 * 6.375542163848877
Epoch 280, val loss: 1.653870940208435
Epoch 290, training loss: 638.4625244140625 = 1.6197577714920044 + 100.0 * 6.368427753448486
Epoch 290, val loss: 1.6370071172714233
Epoch 300, training loss: 637.8281860351562 = 1.600456714630127 + 100.0 * 6.362277030944824
Epoch 300, val loss: 1.6193511486053467
Epoch 310, training loss: 637.3845825195312 = 1.5803272724151611 + 100.0 * 6.3580427169799805
Epoch 310, val loss: 1.6009972095489502
Epoch 320, training loss: 637.0216064453125 = 1.5591468811035156 + 100.0 * 6.3546247482299805
Epoch 320, val loss: 1.5819528102874756
Epoch 330, training loss: 636.2781372070312 = 1.5374765396118164 + 100.0 * 6.347406387329102
Epoch 330, val loss: 1.5625656843185425
Epoch 340, training loss: 635.7246704101562 = 1.5152610540390015 + 100.0 * 6.3420939445495605
Epoch 340, val loss: 1.5428893566131592
Epoch 350, training loss: 635.4707641601562 = 1.4926971197128296 + 100.0 * 6.339780807495117
Epoch 350, val loss: 1.5232568979263306
Epoch 360, training loss: 635.0623779296875 = 1.4698107242584229 + 100.0 * 6.335926055908203
Epoch 360, val loss: 1.5030707120895386
Epoch 370, training loss: 634.4918212890625 = 1.4467097520828247 + 100.0 * 6.330451011657715
Epoch 370, val loss: 1.4833296537399292
Epoch 380, training loss: 634.4722900390625 = 1.4236180782318115 + 100.0 * 6.330486297607422
Epoch 380, val loss: 1.4638317823410034
Epoch 390, training loss: 633.868896484375 = 1.4004689455032349 + 100.0 * 6.3246846199035645
Epoch 390, val loss: 1.4443867206573486
Epoch 400, training loss: 633.4297485351562 = 1.3777289390563965 + 100.0 * 6.320519924163818
Epoch 400, val loss: 1.425508737564087
Epoch 410, training loss: 633.07958984375 = 1.3551034927368164 + 100.0 * 6.317245006561279
Epoch 410, val loss: 1.407145380973816
Epoch 420, training loss: 632.8491821289062 = 1.3327217102050781 + 100.0 * 6.315165042877197
Epoch 420, val loss: 1.388977289199829
Epoch 430, training loss: 632.449951171875 = 1.3105580806732178 + 100.0 * 6.311394214630127
Epoch 430, val loss: 1.3715482950210571
Epoch 440, training loss: 632.177490234375 = 1.2888085842132568 + 100.0 * 6.308887004852295
Epoch 440, val loss: 1.3547492027282715
Epoch 450, training loss: 632.360107421875 = 1.2674109935760498 + 100.0 * 6.310927391052246
Epoch 450, val loss: 1.3382550477981567
Epoch 460, training loss: 631.8360595703125 = 1.2464642524719238 + 100.0 * 6.305896282196045
Epoch 460, val loss: 1.3223998546600342
Epoch 470, training loss: 631.5029907226562 = 1.2257039546966553 + 100.0 * 6.3027729988098145
Epoch 470, val loss: 1.3069489002227783
Epoch 480, training loss: 631.2221069335938 = 1.2054197788238525 + 100.0 * 6.300166606903076
Epoch 480, val loss: 1.2919427156448364
Epoch 490, training loss: 630.9138793945312 = 1.185492753982544 + 100.0 * 6.29728364944458
Epoch 490, val loss: 1.2772634029388428
Epoch 500, training loss: 630.7242431640625 = 1.1659218072891235 + 100.0 * 6.295583248138428
Epoch 500, val loss: 1.2632017135620117
Epoch 510, training loss: 630.5822143554688 = 1.1466240882873535 + 100.0 * 6.294356346130371
Epoch 510, val loss: 1.2496393918991089
Epoch 520, training loss: 630.278564453125 = 1.1275585889816284 + 100.0 * 6.291510105133057
Epoch 520, val loss: 1.2361749410629272
Epoch 530, training loss: 630.0189208984375 = 1.1088682413101196 + 100.0 * 6.289100646972656
Epoch 530, val loss: 1.2231085300445557
Epoch 540, training loss: 629.8768310546875 = 1.0905795097351074 + 100.0 * 6.287862777709961
Epoch 540, val loss: 1.210476040840149
Epoch 550, training loss: 629.8941650390625 = 1.0724586248397827 + 100.0 * 6.288217067718506
Epoch 550, val loss: 1.1979185342788696
Epoch 560, training loss: 629.4662475585938 = 1.0546077489852905 + 100.0 * 6.284116268157959
Epoch 560, val loss: 1.1861008405685425
Epoch 570, training loss: 629.2257080078125 = 1.0370817184448242 + 100.0 * 6.281886100769043
Epoch 570, val loss: 1.1742501258850098
Epoch 580, training loss: 629.0874633789062 = 1.0198291540145874 + 100.0 * 6.280676364898682
Epoch 580, val loss: 1.1628574132919312
Epoch 590, training loss: 629.212890625 = 1.0027751922607422 + 100.0 * 6.282101154327393
Epoch 590, val loss: 1.1514579057693481
Epoch 600, training loss: 629.1409912109375 = 0.9858673810958862 + 100.0 * 6.281551361083984
Epoch 600, val loss: 1.1404659748077393
Epoch 610, training loss: 628.6272583007812 = 0.9692284464836121 + 100.0 * 6.276580333709717
Epoch 610, val loss: 1.1297357082366943
Epoch 620, training loss: 628.438232421875 = 0.9528687000274658 + 100.0 * 6.274853706359863
Epoch 620, val loss: 1.1193978786468506
Epoch 630, training loss: 628.302734375 = 0.9367786049842834 + 100.0 * 6.273659706115723
Epoch 630, val loss: 1.1092814207077026
Epoch 640, training loss: 628.513916015625 = 0.9209102392196655 + 100.0 * 6.275929927825928
Epoch 640, val loss: 1.0995304584503174
Epoch 650, training loss: 628.036865234375 = 0.9050923585891724 + 100.0 * 6.271317481994629
Epoch 650, val loss: 1.0899486541748047
Epoch 660, training loss: 628.2259521484375 = 0.8896081447601318 + 100.0 * 6.2733635902404785
Epoch 660, val loss: 1.0806254148483276
Epoch 670, training loss: 627.8153076171875 = 0.8742436170578003 + 100.0 * 6.269410133361816
Epoch 670, val loss: 1.0716931819915771
Epoch 680, training loss: 627.5936279296875 = 0.8592169284820557 + 100.0 * 6.2673444747924805
Epoch 680, val loss: 1.0630407333374023
Epoch 690, training loss: 627.6677856445312 = 0.8444408774375916 + 100.0 * 6.268233776092529
Epoch 690, val loss: 1.0545635223388672
Epoch 700, training loss: 627.5478515625 = 0.8297423720359802 + 100.0 * 6.267181396484375
Epoch 700, val loss: 1.0466258525848389
Epoch 710, training loss: 627.5438232421875 = 0.8153136968612671 + 100.0 * 6.267285346984863
Epoch 710, val loss: 1.0388402938842773
Epoch 720, training loss: 627.1483154296875 = 0.8011109232902527 + 100.0 * 6.263472557067871
Epoch 720, val loss: 1.0314222574234009
Epoch 730, training loss: 627.0297241210938 = 0.7871676683425903 + 100.0 * 6.262425422668457
Epoch 730, val loss: 1.0243016481399536
Epoch 740, training loss: 626.928955078125 = 0.7735520601272583 + 100.0 * 6.26155424118042
Epoch 740, val loss: 1.017454743385315
Epoch 750, training loss: 627.2750854492188 = 0.7601494193077087 + 100.0 * 6.2651495933532715
Epoch 750, val loss: 1.0111229419708252
Epoch 760, training loss: 626.913330078125 = 0.746643602848053 + 100.0 * 6.261666774749756
Epoch 760, val loss: 1.0045323371887207
Epoch 770, training loss: 626.67822265625 = 0.7334341406822205 + 100.0 * 6.2594475746154785
Epoch 770, val loss: 0.9986637830734253
Epoch 780, training loss: 626.4720458984375 = 0.720635175704956 + 100.0 * 6.257513999938965
Epoch 780, val loss: 0.9929529428482056
Epoch 790, training loss: 626.376220703125 = 0.7080845236778259 + 100.0 * 6.256681442260742
Epoch 790, val loss: 0.9875481724739075
Epoch 800, training loss: 626.6256713867188 = 0.6957035660743713 + 100.0 * 6.2592997550964355
Epoch 800, val loss: 0.9823810458183289
Epoch 810, training loss: 626.4656372070312 = 0.6834649443626404 + 100.0 * 6.257821559906006
Epoch 810, val loss: 0.9775218963623047
Epoch 820, training loss: 626.1937255859375 = 0.6713480949401855 + 100.0 * 6.255223751068115
Epoch 820, val loss: 0.9726677536964417
Epoch 830, training loss: 626.1061401367188 = 0.6595896482467651 + 100.0 * 6.254465103149414
Epoch 830, val loss: 0.9682098627090454
Epoch 840, training loss: 626.1447143554688 = 0.6480478644371033 + 100.0 * 6.2549662590026855
Epoch 840, val loss: 0.9639274477958679
Epoch 850, training loss: 626.1414794921875 = 0.6366121172904968 + 100.0 * 6.255048751831055
Epoch 850, val loss: 0.9594438076019287
Epoch 860, training loss: 625.835205078125 = 0.6253493428230286 + 100.0 * 6.252098560333252
Epoch 860, val loss: 0.9557575583457947
Epoch 870, training loss: 625.728759765625 = 0.6144048571586609 + 100.0 * 6.251143932342529
Epoch 870, val loss: 0.9521234631538391
Epoch 880, training loss: 625.5979614257812 = 0.6036918759346008 + 100.0 * 6.249942779541016
Epoch 880, val loss: 0.9487133026123047
Epoch 890, training loss: 625.7093505859375 = 0.5931873917579651 + 100.0 * 6.251161575317383
Epoch 890, val loss: 0.9454192519187927
Epoch 900, training loss: 625.5510864257812 = 0.582729160785675 + 100.0 * 6.249683856964111
Epoch 900, val loss: 0.9421831369400024
Epoch 910, training loss: 625.4616088867188 = 0.5724673867225647 + 100.0 * 6.248891830444336
Epoch 910, val loss: 0.9391287565231323
Epoch 920, training loss: 625.3287353515625 = 0.5624924898147583 + 100.0 * 6.247662544250488
Epoch 920, val loss: 0.9363099336624146
Epoch 930, training loss: 625.8001708984375 = 0.5527036190032959 + 100.0 * 6.252475261688232
Epoch 930, val loss: 0.933580756187439
Epoch 940, training loss: 625.3677368164062 = 0.5430147051811218 + 100.0 * 6.248247146606445
Epoch 940, val loss: 0.931114137172699
Epoch 950, training loss: 625.1232299804688 = 0.5335034132003784 + 100.0 * 6.24589729309082
Epoch 950, val loss: 0.9287824034690857
Epoch 960, training loss: 625.4691162109375 = 0.5242167711257935 + 100.0 * 6.249448776245117
Epoch 960, val loss: 0.9265865087509155
Epoch 970, training loss: 625.1278686523438 = 0.5150748491287231 + 100.0 * 6.246128082275391
Epoch 970, val loss: 0.9241777658462524
Epoch 980, training loss: 624.9127197265625 = 0.5060678124427795 + 100.0 * 6.2440667152404785
Epoch 980, val loss: 0.9224715828895569
Epoch 990, training loss: 625.0679931640625 = 0.4972900152206421 + 100.0 * 6.245707035064697
Epoch 990, val loss: 0.9206051826477051
Epoch 1000, training loss: 624.7839965820312 = 0.48863735795021057 + 100.0 * 6.242953300476074
Epoch 1000, val loss: 0.9189053177833557
Epoch 1010, training loss: 625.1100463867188 = 0.48011332750320435 + 100.0 * 6.2462992668151855
Epoch 1010, val loss: 0.9175688624382019
Epoch 1020, training loss: 624.682861328125 = 0.4716159999370575 + 100.0 * 6.242112159729004
Epoch 1020, val loss: 0.915482759475708
Epoch 1030, training loss: 624.5518798828125 = 0.4633275270462036 + 100.0 * 6.2408857345581055
Epoch 1030, val loss: 0.9144946932792664
Epoch 1040, training loss: 624.437255859375 = 0.45529642701148987 + 100.0 * 6.239819049835205
Epoch 1040, val loss: 0.9131090641021729
Epoch 1050, training loss: 624.5142822265625 = 0.44739747047424316 + 100.0 * 6.240669250488281
Epoch 1050, val loss: 0.9122713804244995
Epoch 1060, training loss: 624.6019897460938 = 0.43949708342552185 + 100.0 * 6.24162483215332
Epoch 1060, val loss: 0.9109790325164795
Epoch 1070, training loss: 624.4210815429688 = 0.4317200779914856 + 100.0 * 6.239893913269043
Epoch 1070, val loss: 0.9099345803260803
Epoch 1080, training loss: 624.2816772460938 = 0.42406561970710754 + 100.0 * 6.2385759353637695
Epoch 1080, val loss: 0.9089573621749878
Epoch 1090, training loss: 624.2679443359375 = 0.41661587357521057 + 100.0 * 6.238513469696045
Epoch 1090, val loss: 0.9082234501838684
Epoch 1100, training loss: 624.3882446289062 = 0.4092472791671753 + 100.0 * 6.239789962768555
Epoch 1100, val loss: 0.9075707793235779
Epoch 1110, training loss: 624.2578735351562 = 0.40194252133369446 + 100.0 * 6.238559722900391
Epoch 1110, val loss: 0.9070718288421631
Epoch 1120, training loss: 624.03564453125 = 0.3947715163230896 + 100.0 * 6.236408710479736
Epoch 1120, val loss: 0.906414270401001
Epoch 1130, training loss: 623.9171142578125 = 0.38773614168167114 + 100.0 * 6.235293865203857
Epoch 1130, val loss: 0.9061675667762756
Epoch 1140, training loss: 624.3057250976562 = 0.3808237910270691 + 100.0 * 6.239249229431152
Epoch 1140, val loss: 0.9059643745422363
Epoch 1150, training loss: 624.0912475585938 = 0.3739412724971771 + 100.0 * 6.237172603607178
Epoch 1150, val loss: 0.9054175615310669
Epoch 1160, training loss: 623.851318359375 = 0.3670822083950043 + 100.0 * 6.234842300415039
Epoch 1160, val loss: 0.9053913354873657
Epoch 1170, training loss: 623.6690063476562 = 0.36046338081359863 + 100.0 * 6.2330851554870605
Epoch 1170, val loss: 0.905308723449707
Epoch 1180, training loss: 624.00634765625 = 0.3539350628852844 + 100.0 * 6.2365241050720215
Epoch 1180, val loss: 0.905159056186676
Epoch 1190, training loss: 623.5887451171875 = 0.34744155406951904 + 100.0 * 6.232413291931152
Epoch 1190, val loss: 0.9054282903671265
Epoch 1200, training loss: 623.5401611328125 = 0.34103304147720337 + 100.0 * 6.231991767883301
Epoch 1200, val loss: 0.9056442975997925
Epoch 1210, training loss: 623.4874267578125 = 0.3348149359226227 + 100.0 * 6.231525897979736
Epoch 1210, val loss: 0.9060197472572327
Epoch 1220, training loss: 623.9495849609375 = 0.3286748230457306 + 100.0 * 6.236208915710449
Epoch 1220, val loss: 0.9065704345703125
Epoch 1230, training loss: 623.8636474609375 = 0.32258984446525574 + 100.0 * 6.235410690307617
Epoch 1230, val loss: 0.9060142636299133
Epoch 1240, training loss: 623.46044921875 = 0.3164612352848053 + 100.0 * 6.23144006729126
Epoch 1240, val loss: 0.9068421125411987
Epoch 1250, training loss: 623.307373046875 = 0.3105911910533905 + 100.0 * 6.2299675941467285
Epoch 1250, val loss: 0.9074193239212036
Epoch 1260, training loss: 623.2260131835938 = 0.3048267364501953 + 100.0 * 6.229212284088135
Epoch 1260, val loss: 0.9077177047729492
Epoch 1270, training loss: 623.1231079101562 = 0.29918694496154785 + 100.0 * 6.228239059448242
Epoch 1270, val loss: 0.9086222052574158
Epoch 1280, training loss: 623.7614135742188 = 0.2936502695083618 + 100.0 * 6.234677791595459
Epoch 1280, val loss: 0.9091538786888123
Epoch 1290, training loss: 623.5559692382812 = 0.28802490234375 + 100.0 * 6.23267936706543
Epoch 1290, val loss: 0.909910261631012
Epoch 1300, training loss: 623.0333862304688 = 0.28254449367523193 + 100.0 * 6.227508544921875
Epoch 1300, val loss: 0.9108130931854248
Epoch 1310, training loss: 622.9871215820312 = 0.27724388241767883 + 100.0 * 6.2270989418029785
Epoch 1310, val loss: 0.9117238521575928
Epoch 1320, training loss: 623.1597290039062 = 0.2720649838447571 + 100.0 * 6.228876113891602
Epoch 1320, val loss: 0.9129229784011841
Epoch 1330, training loss: 623.1773681640625 = 0.2668503522872925 + 100.0 * 6.229105472564697
Epoch 1330, val loss: 0.9136905670166016
Epoch 1340, training loss: 622.9009399414062 = 0.2617110311985016 + 100.0 * 6.2263922691345215
Epoch 1340, val loss: 0.9146497249603271
Epoch 1350, training loss: 622.8594360351562 = 0.25672388076782227 + 100.0 * 6.226027011871338
Epoch 1350, val loss: 0.9159335494041443
Epoch 1360, training loss: 622.941650390625 = 0.2519085705280304 + 100.0 * 6.226897716522217
Epoch 1360, val loss: 0.9172272086143494
Epoch 1370, training loss: 622.888671875 = 0.24710872769355774 + 100.0 * 6.226415157318115
Epoch 1370, val loss: 0.918552815914154
Epoch 1380, training loss: 622.78955078125 = 0.2423904836177826 + 100.0 * 6.2254719734191895
Epoch 1380, val loss: 0.9196245670318604
Epoch 1390, training loss: 622.7092895507812 = 0.23781666159629822 + 100.0 * 6.224714756011963
Epoch 1390, val loss: 0.9214608669281006
Epoch 1400, training loss: 623.0189819335938 = 0.2333400398492813 + 100.0 * 6.227856159210205
Epoch 1400, val loss: 0.9228108525276184
Epoch 1410, training loss: 622.6458129882812 = 0.22886775434017181 + 100.0 * 6.2241692543029785
Epoch 1410, val loss: 0.9242219924926758
Epoch 1420, training loss: 622.5223999023438 = 0.2245512455701828 + 100.0 * 6.222978591918945
Epoch 1420, val loss: 0.9257712364196777
Epoch 1430, training loss: 623.2561645507812 = 0.2203446626663208 + 100.0 * 6.230358123779297
Epoch 1430, val loss: 0.9276529550552368
Epoch 1440, training loss: 622.7335205078125 = 0.21610452234745026 + 100.0 * 6.2251739501953125
Epoch 1440, val loss: 0.9291867017745972
Epoch 1450, training loss: 622.4918823242188 = 0.21199534833431244 + 100.0 * 6.222798824310303
Epoch 1450, val loss: 0.931154727935791
Epoch 1460, training loss: 622.477294921875 = 0.2080451101064682 + 100.0 * 6.222692012786865
Epoch 1460, val loss: 0.9331430792808533
Epoch 1470, training loss: 622.822021484375 = 0.20415528118610382 + 100.0 * 6.2261786460876465
Epoch 1470, val loss: 0.9350258111953735
Epoch 1480, training loss: 622.4576416015625 = 0.2002672553062439 + 100.0 * 6.222573757171631
Epoch 1480, val loss: 0.9366655349731445
Epoch 1490, training loss: 622.3004760742188 = 0.1965382993221283 + 100.0 * 6.221039295196533
Epoch 1490, val loss: 0.9389804005622864
Epoch 1500, training loss: 622.37646484375 = 0.1929146647453308 + 100.0 * 6.221835136413574
Epoch 1500, val loss: 0.9409507513046265
Epoch 1510, training loss: 622.4852294921875 = 0.18931511044502258 + 100.0 * 6.222959041595459
Epoch 1510, val loss: 0.943605363368988
Epoch 1520, training loss: 622.1854858398438 = 0.1857476830482483 + 100.0 * 6.219997406005859
Epoch 1520, val loss: 0.945515513420105
Epoch 1530, training loss: 622.1541748046875 = 0.1823122501373291 + 100.0 * 6.2197184562683105
Epoch 1530, val loss: 0.9481752514839172
Epoch 1540, training loss: 622.3572387695312 = 0.17899468541145325 + 100.0 * 6.221782207489014
Epoch 1540, val loss: 0.9508012533187866
Epoch 1550, training loss: 622.209716796875 = 0.17570163309574127 + 100.0 * 6.220340251922607
Epoch 1550, val loss: 0.9529690146446228
Epoch 1560, training loss: 622.1246337890625 = 0.1724645048379898 + 100.0 * 6.219521522521973
Epoch 1560, val loss: 0.9557282328605652
Epoch 1570, training loss: 622.0108642578125 = 0.16933506727218628 + 100.0 * 6.2184157371521
Epoch 1570, val loss: 0.9581127166748047
Epoch 1580, training loss: 622.047607421875 = 0.16628563404083252 + 100.0 * 6.218813419342041
Epoch 1580, val loss: 0.9611614346504211
Epoch 1590, training loss: 622.5532836914062 = 0.16329094767570496 + 100.0 * 6.223900318145752
Epoch 1590, val loss: 0.9634579420089722
Epoch 1600, training loss: 622.0831909179688 = 0.16032922267913818 + 100.0 * 6.219228744506836
Epoch 1600, val loss: 0.9666959643363953
Epoch 1610, training loss: 621.9977416992188 = 0.15745855867862701 + 100.0 * 6.218402862548828
Epoch 1610, val loss: 0.969436764717102
Epoch 1620, training loss: 622.1206665039062 = 0.15465329587459564 + 100.0 * 6.21966028213501
Epoch 1620, val loss: 0.9724128842353821
Epoch 1630, training loss: 621.9191284179688 = 0.15190261602401733 + 100.0 * 6.217672348022461
Epoch 1630, val loss: 0.975415825843811
Epoch 1640, training loss: 621.9877319335938 = 0.14922687411308289 + 100.0 * 6.218384742736816
Epoch 1640, val loss: 0.9786872863769531
Epoch 1650, training loss: 621.938720703125 = 0.1466139405965805 + 100.0 * 6.217921257019043
Epoch 1650, val loss: 0.9816398620605469
Epoch 1660, training loss: 621.9448852539062 = 0.14406131207942963 + 100.0 * 6.218008518218994
Epoch 1660, val loss: 0.9849643111228943
Epoch 1670, training loss: 621.8855590820312 = 0.1415221095085144 + 100.0 * 6.217440128326416
Epoch 1670, val loss: 0.9879553914070129
Epoch 1680, training loss: 621.6948852539062 = 0.13904130458831787 + 100.0 * 6.2155585289001465
Epoch 1680, val loss: 0.9912956953048706
Epoch 1690, training loss: 621.6817016601562 = 0.13666898012161255 + 100.0 * 6.215450286865234
Epoch 1690, val loss: 0.9947959780693054
Epoch 1700, training loss: 621.8849487304688 = 0.1343505084514618 + 100.0 * 6.217506408691406
Epoch 1700, val loss: 0.9984650611877441
Epoch 1710, training loss: 621.6614379882812 = 0.13203118741512299 + 100.0 * 6.215293884277344
Epoch 1710, val loss: 1.0012755393981934
Epoch 1720, training loss: 621.6204833984375 = 0.12976263463497162 + 100.0 * 6.214907169342041
Epoch 1720, val loss: 1.0050297975540161
Epoch 1730, training loss: 621.9649047851562 = 0.12758946418762207 + 100.0 * 6.2183732986450195
Epoch 1730, val loss: 1.0085208415985107
Epoch 1740, training loss: 621.5955810546875 = 0.12539494037628174 + 100.0 * 6.214702129364014
Epoch 1740, val loss: 1.0117523670196533
Epoch 1750, training loss: 621.5625 = 0.12323857098817825 + 100.0 * 6.21439266204834
Epoch 1750, val loss: 1.0153511762619019
Epoch 1760, training loss: 621.4793701171875 = 0.12120084464550018 + 100.0 * 6.213581562042236
Epoch 1760, val loss: 1.019132375717163
Epoch 1770, training loss: 621.4344482421875 = 0.11921679228544235 + 100.0 * 6.2131524085998535
Epoch 1770, val loss: 1.0227810144424438
Epoch 1780, training loss: 622.1798095703125 = 0.11726849526166916 + 100.0 * 6.220625877380371
Epoch 1780, val loss: 1.0265710353851318
Epoch 1790, training loss: 621.6560668945312 = 0.11528614163398743 + 100.0 * 6.215407848358154
Epoch 1790, val loss: 1.029662013053894
Epoch 1800, training loss: 621.4657592773438 = 0.1133679747581482 + 100.0 * 6.213523864746094
Epoch 1800, val loss: 1.0337200164794922
Epoch 1810, training loss: 621.5340576171875 = 0.11153898388147354 + 100.0 * 6.2142252922058105
Epoch 1810, val loss: 1.0372687578201294
Epoch 1820, training loss: 621.596923828125 = 0.10969679802656174 + 100.0 * 6.214872360229492
Epoch 1820, val loss: 1.041130542755127
Epoch 1830, training loss: 621.2791748046875 = 0.10785933583974838 + 100.0 * 6.2117133140563965
Epoch 1830, val loss: 1.0447709560394287
Epoch 1840, training loss: 621.2904663085938 = 0.10612074285745621 + 100.0 * 6.211843490600586
Epoch 1840, val loss: 1.0486443042755127
Epoch 1850, training loss: 621.355224609375 = 0.10443054884672165 + 100.0 * 6.212507724761963
Epoch 1850, val loss: 1.0525097846984863
Epoch 1860, training loss: 621.6271362304688 = 0.1027531623840332 + 100.0 * 6.215243816375732
Epoch 1860, val loss: 1.0562570095062256
Epoch 1870, training loss: 621.4557495117188 = 0.10108467936515808 + 100.0 * 6.2135467529296875
Epoch 1870, val loss: 1.0602233409881592
Epoch 1880, training loss: 621.5365600585938 = 0.09944871813058853 + 100.0 * 6.214371204376221
Epoch 1880, val loss: 1.0642061233520508
Epoch 1890, training loss: 621.2594604492188 = 0.0978521853685379 + 100.0 * 6.211616039276123
Epoch 1890, val loss: 1.0682575702667236
Epoch 1900, training loss: 621.1888427734375 = 0.09630894660949707 + 100.0 * 6.210925102233887
Epoch 1900, val loss: 1.0720432996749878
Epoch 1910, training loss: 621.1814575195312 = 0.09479775279760361 + 100.0 * 6.210866928100586
Epoch 1910, val loss: 1.075984239578247
Epoch 1920, training loss: 621.7367553710938 = 0.09331434220075607 + 100.0 * 6.216434478759766
Epoch 1920, val loss: 1.0795207023620605
Epoch 1930, training loss: 621.2821044921875 = 0.09179607778787613 + 100.0 * 6.211903095245361
Epoch 1930, val loss: 1.0843364000320435
Epoch 1940, training loss: 621.1311645507812 = 0.09033911675214767 + 100.0 * 6.2104082107543945
Epoch 1940, val loss: 1.0879021883010864
Epoch 1950, training loss: 621.088623046875 = 0.08893958479166031 + 100.0 * 6.209996700286865
Epoch 1950, val loss: 1.0923118591308594
Epoch 1960, training loss: 621.473876953125 = 0.08757510781288147 + 100.0 * 6.213862895965576
Epoch 1960, val loss: 1.0959504842758179
Epoch 1970, training loss: 621.0391235351562 = 0.0861879289150238 + 100.0 * 6.209529399871826
Epoch 1970, val loss: 1.1004347801208496
Epoch 1980, training loss: 621.21875 = 0.08487089723348618 + 100.0 * 6.211338520050049
Epoch 1980, val loss: 1.104088544845581
Epoch 1990, training loss: 621.0736083984375 = 0.08353134989738464 + 100.0 * 6.209900379180908
Epoch 1990, val loss: 1.1083922386169434
Epoch 2000, training loss: 621.0337524414062 = 0.08223331719636917 + 100.0 * 6.20951509475708
Epoch 2000, val loss: 1.112359881401062
Epoch 2010, training loss: 621.3662719726562 = 0.08099163323640823 + 100.0 * 6.212852478027344
Epoch 2010, val loss: 1.1168034076690674
Epoch 2020, training loss: 620.921142578125 = 0.07969993352890015 + 100.0 * 6.208414554595947
Epoch 2020, val loss: 1.1206022500991821
Epoch 2030, training loss: 620.9017944335938 = 0.07848019152879715 + 100.0 * 6.208232879638672
Epoch 2030, val loss: 1.1246769428253174
Epoch 2040, training loss: 620.8443603515625 = 0.07730349898338318 + 100.0 * 6.207670211791992
Epoch 2040, val loss: 1.1288517713546753
Epoch 2050, training loss: 620.8427734375 = 0.07614871114492416 + 100.0 * 6.207665920257568
Epoch 2050, val loss: 1.1330723762512207
Epoch 2060, training loss: 621.4690551757812 = 0.07502157241106033 + 100.0 * 6.213940143585205
Epoch 2060, val loss: 1.1372272968292236
Epoch 2070, training loss: 621.2149047851562 = 0.07383391261100769 + 100.0 * 6.2114105224609375
Epoch 2070, val loss: 1.1410447359085083
Epoch 2080, training loss: 621.0662841796875 = 0.07270800322294235 + 100.0 * 6.209935665130615
Epoch 2080, val loss: 1.1450221538543701
Epoch 2090, training loss: 620.7958374023438 = 0.0716000497341156 + 100.0 * 6.207242488861084
Epoch 2090, val loss: 1.149232029914856
Epoch 2100, training loss: 620.98095703125 = 0.07054600864648819 + 100.0 * 6.209104061126709
Epoch 2100, val loss: 1.1532412767410278
Epoch 2110, training loss: 620.9868774414062 = 0.0694807693362236 + 100.0 * 6.209174156188965
Epoch 2110, val loss: 1.1571919918060303
Epoch 2120, training loss: 620.7638549804688 = 0.0684250146150589 + 100.0 * 6.206954479217529
Epoch 2120, val loss: 1.161436676979065
Epoch 2130, training loss: 620.7564086914062 = 0.06741965562105179 + 100.0 * 6.206889629364014
Epoch 2130, val loss: 1.165601134300232
Epoch 2140, training loss: 620.856201171875 = 0.06643231213092804 + 100.0 * 6.207898139953613
Epoch 2140, val loss: 1.1698890924453735
Epoch 2150, training loss: 620.8385009765625 = 0.06544849276542664 + 100.0 * 6.207730770111084
Epoch 2150, val loss: 1.173794150352478
Epoch 2160, training loss: 620.802978515625 = 0.06447505950927734 + 100.0 * 6.207385063171387
Epoch 2160, val loss: 1.1778769493103027
Epoch 2170, training loss: 620.79345703125 = 0.06351622194051743 + 100.0 * 6.20729923248291
Epoch 2170, val loss: 1.1820049285888672
Epoch 2180, training loss: 620.8792114257812 = 0.06259410828351974 + 100.0 * 6.208166122436523
Epoch 2180, val loss: 1.185933232307434
Epoch 2190, training loss: 620.8814697265625 = 0.06166558340191841 + 100.0 * 6.208198070526123
Epoch 2190, val loss: 1.1901845932006836
Epoch 2200, training loss: 620.628662109375 = 0.060742445290088654 + 100.0 * 6.205679416656494
Epoch 2200, val loss: 1.1939311027526855
Epoch 2210, training loss: 620.5852661132812 = 0.059872619807720184 + 100.0 * 6.205254077911377
Epoch 2210, val loss: 1.198156476020813
Epoch 2220, training loss: 620.66650390625 = 0.0590202771127224 + 100.0 * 6.2060747146606445
Epoch 2220, val loss: 1.2020964622497559
Epoch 2230, training loss: 620.9580078125 = 0.05816647410392761 + 100.0 * 6.208998680114746
Epoch 2230, val loss: 1.206101417541504
Epoch 2240, training loss: 620.7791748046875 = 0.057299576699733734 + 100.0 * 6.207218647003174
Epoch 2240, val loss: 1.2099330425262451
Epoch 2250, training loss: 620.5460815429688 = 0.05645950511097908 + 100.0 * 6.204896450042725
Epoch 2250, val loss: 1.214523434638977
Epoch 2260, training loss: 620.6788940429688 = 0.05565368011593819 + 100.0 * 6.20623254776001
Epoch 2260, val loss: 1.2188128232955933
Epoch 2270, training loss: 620.5887451171875 = 0.054862938821315765 + 100.0 * 6.205338954925537
Epoch 2270, val loss: 1.2224125862121582
Epoch 2280, training loss: 620.7115478515625 = 0.054085664451122284 + 100.0 * 6.206574440002441
Epoch 2280, val loss: 1.2262481451034546
Epoch 2290, training loss: 620.4619750976562 = 0.05329493060708046 + 100.0 * 6.204086780548096
Epoch 2290, val loss: 1.2304104566574097
Epoch 2300, training loss: 620.8358764648438 = 0.05254533886909485 + 100.0 * 6.207833290100098
Epoch 2300, val loss: 1.234244465827942
Epoch 2310, training loss: 620.5451049804688 = 0.05178350582718849 + 100.0 * 6.2049336433410645
Epoch 2310, val loss: 1.2381494045257568
Epoch 2320, training loss: 620.5513305664062 = 0.051015835255384445 + 100.0 * 6.205002784729004
Epoch 2320, val loss: 1.2421671152114868
Epoch 2330, training loss: 620.3692016601562 = 0.05030393600463867 + 100.0 * 6.203188896179199
Epoch 2330, val loss: 1.2461432218551636
Epoch 2340, training loss: 620.3009033203125 = 0.049602851271629333 + 100.0 * 6.202512741088867
Epoch 2340, val loss: 1.2502506971359253
Epoch 2350, training loss: 620.352783203125 = 0.0489315539598465 + 100.0 * 6.203038215637207
Epoch 2350, val loss: 1.2541340589523315
Epoch 2360, training loss: 621.1613159179688 = 0.04826785624027252 + 100.0 * 6.211130142211914
Epoch 2360, val loss: 1.2578939199447632
Epoch 2370, training loss: 620.6522827148438 = 0.04755109176039696 + 100.0 * 6.206047534942627
Epoch 2370, val loss: 1.2621536254882812
Epoch 2380, training loss: 620.2766723632812 = 0.046875838190317154 + 100.0 * 6.202298164367676
Epoch 2380, val loss: 1.2654606103897095
Epoch 2390, training loss: 620.3706665039062 = 0.04623383656144142 + 100.0 * 6.203244686126709
Epoch 2390, val loss: 1.2697569131851196
Epoch 2400, training loss: 620.6367797851562 = 0.04560494050383568 + 100.0 * 6.205911636352539
Epoch 2400, val loss: 1.2733614444732666
Epoch 2410, training loss: 620.4742431640625 = 0.04495652765035629 + 100.0 * 6.2042927742004395
Epoch 2410, val loss: 1.277214765548706
Epoch 2420, training loss: 620.2815551757812 = 0.04433870315551758 + 100.0 * 6.2023725509643555
Epoch 2420, val loss: 1.2811044454574585
Epoch 2430, training loss: 620.3184204101562 = 0.04373685270547867 + 100.0 * 6.202746391296387
Epoch 2430, val loss: 1.2845557928085327
Epoch 2440, training loss: 620.7225341796875 = 0.043144453316926956 + 100.0 * 6.206793785095215
Epoch 2440, val loss: 1.2880138158798218
Epoch 2450, training loss: 620.335205078125 = 0.04254128783941269 + 100.0 * 6.2029266357421875
Epoch 2450, val loss: 1.2921857833862305
Epoch 2460, training loss: 620.1948852539062 = 0.041959188878536224 + 100.0 * 6.201529502868652
Epoch 2460, val loss: 1.295943260192871
Epoch 2470, training loss: 620.1124267578125 = 0.04140636697411537 + 100.0 * 6.200710296630859
Epoch 2470, val loss: 1.2999156713485718
Epoch 2480, training loss: 620.3818359375 = 0.040874361991882324 + 100.0 * 6.203409194946289
Epoch 2480, val loss: 1.303626298904419
Epoch 2490, training loss: 620.4776000976562 = 0.04030955582857132 + 100.0 * 6.204373359680176
Epoch 2490, val loss: 1.3066973686218262
Epoch 2500, training loss: 620.111328125 = 0.039738595485687256 + 100.0 * 6.200716018676758
Epoch 2500, val loss: 1.3108100891113281
Epoch 2510, training loss: 620.09423828125 = 0.03920838236808777 + 100.0 * 6.200550556182861
Epoch 2510, val loss: 1.3142811059951782
Epoch 2520, training loss: 620.1919555664062 = 0.0387028343975544 + 100.0 * 6.201532363891602
Epoch 2520, val loss: 1.3178601264953613
Epoch 2530, training loss: 620.3010864257812 = 0.03819313645362854 + 100.0 * 6.2026286125183105
Epoch 2530, val loss: 1.3214304447174072
Epoch 2540, training loss: 620.6819458007812 = 0.03769231215119362 + 100.0 * 6.206442356109619
Epoch 2540, val loss: 1.3259369134902954
Epoch 2550, training loss: 620.1961669921875 = 0.037182506173849106 + 100.0 * 6.201590061187744
Epoch 2550, val loss: 1.3287264108657837
Epoch 2560, training loss: 620.0867919921875 = 0.036699000746011734 + 100.0 * 6.200500965118408
Epoch 2560, val loss: 1.3327538967132568
Epoch 2570, training loss: 620.0550537109375 = 0.03622763976454735 + 100.0 * 6.200188159942627
Epoch 2570, val loss: 1.3360685110092163
Epoch 2580, training loss: 620.7605590820312 = 0.035786423832178116 + 100.0 * 6.207248210906982
Epoch 2580, val loss: 1.3395715951919556
Epoch 2590, training loss: 620.2554931640625 = 0.03529268503189087 + 100.0 * 6.202202320098877
Epoch 2590, val loss: 1.343100905418396
Epoch 2600, training loss: 620.0900268554688 = 0.034831494092941284 + 100.0 * 6.200551509857178
Epoch 2600, val loss: 1.3467624187469482
Epoch 2610, training loss: 620.0332641601562 = 0.03438977152109146 + 100.0 * 6.199988842010498
Epoch 2610, val loss: 1.3503749370574951
Epoch 2620, training loss: 620.1738891601562 = 0.03396349772810936 + 100.0 * 6.201399803161621
Epoch 2620, val loss: 1.353563904762268
Epoch 2630, training loss: 620.4743041992188 = 0.03352153301239014 + 100.0 * 6.204408168792725
Epoch 2630, val loss: 1.3574659824371338
Epoch 2640, training loss: 619.9732666015625 = 0.03308344632387161 + 100.0 * 6.19940185546875
Epoch 2640, val loss: 1.3606998920440674
Epoch 2650, training loss: 619.875244140625 = 0.03265989571809769 + 100.0 * 6.198425769805908
Epoch 2650, val loss: 1.3640596866607666
Epoch 2660, training loss: 619.8555908203125 = 0.032256968319416046 + 100.0 * 6.198233604431152
Epoch 2660, val loss: 1.3676851987838745
Epoch 2670, training loss: 619.880859375 = 0.031869273632764816 + 100.0 * 6.198490142822266
Epoch 2670, val loss: 1.3711142539978027
Epoch 2680, training loss: 620.7802734375 = 0.03149512782692909 + 100.0 * 6.2074875831604
Epoch 2680, val loss: 1.3750442266464233
Epoch 2690, training loss: 620.1162109375 = 0.031073149293661118 + 100.0 * 6.2008514404296875
Epoch 2690, val loss: 1.377264380455017
Epoch 2700, training loss: 619.9927978515625 = 0.030681265518069267 + 100.0 * 6.199621200561523
Epoch 2700, val loss: 1.381188988685608
Epoch 2710, training loss: 620.0610961914062 = 0.03031000867486 + 100.0 * 6.200307846069336
Epoch 2710, val loss: 1.384314775466919
Epoch 2720, training loss: 619.9083862304688 = 0.029939288273453712 + 100.0 * 6.198784351348877
Epoch 2720, val loss: 1.3878108263015747
Epoch 2730, training loss: 620.3374633789062 = 0.029587991535663605 + 100.0 * 6.203078746795654
Epoch 2730, val loss: 1.3908690214157104
Epoch 2740, training loss: 619.9324951171875 = 0.029207969084382057 + 100.0 * 6.199032306671143
Epoch 2740, val loss: 1.3942911624908447
Epoch 2750, training loss: 619.8187866210938 = 0.028856797143816948 + 100.0 * 6.197898864746094
Epoch 2750, val loss: 1.3973872661590576
Epoch 2760, training loss: 619.8453979492188 = 0.028514286503195763 + 100.0 * 6.198169231414795
Epoch 2760, val loss: 1.4010803699493408
Epoch 2770, training loss: 620.14453125 = 0.02817806974053383 + 100.0 * 6.201163291931152
Epoch 2770, val loss: 1.404231071472168
Epoch 2780, training loss: 619.8671875 = 0.02783600054681301 + 100.0 * 6.19839334487915
Epoch 2780, val loss: 1.407198429107666
Epoch 2790, training loss: 619.7604370117188 = 0.02750757895410061 + 100.0 * 6.197329044342041
Epoch 2790, val loss: 1.410938024520874
Epoch 2800, training loss: 620.050537109375 = 0.02720307745039463 + 100.0 * 6.200233459472656
Epoch 2800, val loss: 1.414096713066101
Epoch 2810, training loss: 619.8889770507812 = 0.026866473257541656 + 100.0 * 6.1986212730407715
Epoch 2810, val loss: 1.4174563884735107
Epoch 2820, training loss: 619.7310791015625 = 0.026541393250226974 + 100.0 * 6.19704532623291
Epoch 2820, val loss: 1.4199024438858032
Epoch 2830, training loss: 619.6702880859375 = 0.026230808347463608 + 100.0 * 6.19644021987915
Epoch 2830, val loss: 1.4232624769210815
Epoch 2840, training loss: 619.6910400390625 = 0.025934016332030296 + 100.0 * 6.196650981903076
Epoch 2840, val loss: 1.426622748374939
Epoch 2850, training loss: 620.3132934570312 = 0.02564934827387333 + 100.0 * 6.202876567840576
Epoch 2850, val loss: 1.4295390844345093
Epoch 2860, training loss: 619.885986328125 = 0.025339597836136818 + 100.0 * 6.198606491088867
Epoch 2860, val loss: 1.4330050945281982
Epoch 2870, training loss: 619.7194213867188 = 0.025040335953235626 + 100.0 * 6.196943759918213
Epoch 2870, val loss: 1.4357702732086182
Epoch 2880, training loss: 619.8016967773438 = 0.024760182946920395 + 100.0 * 6.1977691650390625
Epoch 2880, val loss: 1.4390074014663696
Epoch 2890, training loss: 619.8117065429688 = 0.024483490735292435 + 100.0 * 6.197872161865234
Epoch 2890, val loss: 1.4418855905532837
Epoch 2900, training loss: 619.6730346679688 = 0.024208642542362213 + 100.0 * 6.196487903594971
Epoch 2900, val loss: 1.4452977180480957
Epoch 2910, training loss: 620.005615234375 = 0.02394832670688629 + 100.0 * 6.199817180633545
Epoch 2910, val loss: 1.4481910467147827
Epoch 2920, training loss: 619.8976440429688 = 0.02366410382091999 + 100.0 * 6.198739528656006
Epoch 2920, val loss: 1.4502205848693848
Epoch 2930, training loss: 619.69580078125 = 0.023393254727125168 + 100.0 * 6.1967244148254395
Epoch 2930, val loss: 1.4541569948196411
Epoch 2940, training loss: 619.5488891601562 = 0.023131702095270157 + 100.0 * 6.195257663726807
Epoch 2940, val loss: 1.4569580554962158
Epoch 2950, training loss: 619.5496826171875 = 0.022885436192154884 + 100.0 * 6.195267677307129
Epoch 2950, val loss: 1.4601807594299316
Epoch 2960, training loss: 619.9827880859375 = 0.022650495171546936 + 100.0 * 6.199601650238037
Epoch 2960, val loss: 1.4630683660507202
Epoch 2970, training loss: 619.6026000976562 = 0.02239006571471691 + 100.0 * 6.195802211761475
Epoch 2970, val loss: 1.4652782678604126
Epoch 2980, training loss: 619.62060546875 = 0.022141721099615097 + 100.0 * 6.195984363555908
Epoch 2980, val loss: 1.468612790107727
Epoch 2990, training loss: 619.6498413085938 = 0.021896550431847572 + 100.0 * 6.196279048919678
Epoch 2990, val loss: 1.4717397689819336
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 861.6387939453125 = 1.9561426639556885 + 100.0 * 8.596826553344727
Epoch 0, val loss: 1.9549399614334106
Epoch 10, training loss: 861.54345703125 = 1.947723388671875 + 100.0 * 8.595956802368164
Epoch 10, val loss: 1.9469186067581177
Epoch 20, training loss: 860.908447265625 = 1.9374151229858398 + 100.0 * 8.589710235595703
Epoch 20, val loss: 1.9367406368255615
Epoch 30, training loss: 856.6937255859375 = 1.9240070581436157 + 100.0 * 8.547697067260742
Epoch 30, val loss: 1.9230316877365112
Epoch 40, training loss: 831.8676147460938 = 1.9079337120056152 + 100.0 * 8.299596786499023
Epoch 40, val loss: 1.9067004919052124
Epoch 50, training loss: 759.5846557617188 = 1.8903082609176636 + 100.0 * 7.576943397521973
Epoch 50, val loss: 1.8889402151107788
Epoch 60, training loss: 724.2552490234375 = 1.8761752843856812 + 100.0 * 7.223791122436523
Epoch 60, val loss: 1.875352144241333
Epoch 70, training loss: 699.12109375 = 1.8632818460464478 + 100.0 * 6.972578048706055
Epoch 70, val loss: 1.862716555595398
Epoch 80, training loss: 686.2162475585938 = 1.8506500720977783 + 100.0 * 6.843656063079834
Epoch 80, val loss: 1.850967526435852
Epoch 90, training loss: 679.7682495117188 = 1.840309739112854 + 100.0 * 6.7792792320251465
Epoch 90, val loss: 1.8410629034042358
Epoch 100, training loss: 674.572998046875 = 1.8305411338806152 + 100.0 * 6.727424144744873
Epoch 100, val loss: 1.8317172527313232
Epoch 110, training loss: 669.3685913085938 = 1.821950078010559 + 100.0 * 6.675466537475586
Epoch 110, val loss: 1.8233671188354492
Epoch 120, training loss: 665.1064453125 = 1.8145021200180054 + 100.0 * 6.6329193115234375
Epoch 120, val loss: 1.816164255142212
Epoch 130, training loss: 661.8194580078125 = 1.8073108196258545 + 100.0 * 6.60012149810791
Epoch 130, val loss: 1.8092329502105713
Epoch 140, training loss: 658.912353515625 = 1.8001046180725098 + 100.0 * 6.571122169494629
Epoch 140, val loss: 1.802280068397522
Epoch 150, training loss: 656.4449462890625 = 1.7930275201797485 + 100.0 * 6.5465192794799805
Epoch 150, val loss: 1.7953753471374512
Epoch 160, training loss: 654.1851196289062 = 1.7857816219329834 + 100.0 * 6.523993492126465
Epoch 160, val loss: 1.7885113954544067
Epoch 170, training loss: 652.2278442382812 = 1.778273344039917 + 100.0 * 6.504496097564697
Epoch 170, val loss: 1.7815171480178833
Epoch 180, training loss: 650.8261108398438 = 1.770371913909912 + 100.0 * 6.4905571937561035
Epoch 180, val loss: 1.7743531465530396
Epoch 190, training loss: 649.0096435546875 = 1.7618440389633179 + 100.0 * 6.472477912902832
Epoch 190, val loss: 1.7667258977890015
Epoch 200, training loss: 647.4950561523438 = 1.7526426315307617 + 100.0 * 6.457424163818359
Epoch 200, val loss: 1.7588000297546387
Epoch 210, training loss: 646.1594848632812 = 1.7428666353225708 + 100.0 * 6.44416618347168
Epoch 210, val loss: 1.7504339218139648
Epoch 220, training loss: 644.90673828125 = 1.732136845588684 + 100.0 * 6.431746006011963
Epoch 220, val loss: 1.741077184677124
Epoch 230, training loss: 643.8197021484375 = 1.7204105854034424 + 100.0 * 6.420992851257324
Epoch 230, val loss: 1.731026291847229
Epoch 240, training loss: 642.8609008789062 = 1.7078092098236084 + 100.0 * 6.4115309715271
Epoch 240, val loss: 1.720241904258728
Epoch 250, training loss: 642.0523071289062 = 1.693953275680542 + 100.0 * 6.403583526611328
Epoch 250, val loss: 1.7083336114883423
Epoch 260, training loss: 641.0169677734375 = 1.6789413690567017 + 100.0 * 6.393380165100098
Epoch 260, val loss: 1.6955201625823975
Epoch 270, training loss: 640.1965942382812 = 1.6628175973892212 + 100.0 * 6.3853373527526855
Epoch 270, val loss: 1.6818017959594727
Epoch 280, training loss: 639.6021118164062 = 1.6454553604125977 + 100.0 * 6.379566669464111
Epoch 280, val loss: 1.6670734882354736
Epoch 290, training loss: 638.8091430664062 = 1.6269819736480713 + 100.0 * 6.371821880340576
Epoch 290, val loss: 1.6512426137924194
Epoch 300, training loss: 638.1849365234375 = 1.6075302362442017 + 100.0 * 6.365773677825928
Epoch 300, val loss: 1.634698510169983
Epoch 310, training loss: 637.9656372070312 = 1.5872383117675781 + 100.0 * 6.363784313201904
Epoch 310, val loss: 1.6172797679901123
Epoch 320, training loss: 637.1969604492188 = 1.5658763647079468 + 100.0 * 6.356310844421387
Epoch 320, val loss: 1.5993510484695435
Epoch 330, training loss: 636.56982421875 = 1.5440183877944946 + 100.0 * 6.3502583503723145
Epoch 330, val loss: 1.580968976020813
Epoch 340, training loss: 636.5366821289062 = 1.521519660949707 + 100.0 * 6.350151538848877
Epoch 340, val loss: 1.562322974205017
Epoch 350, training loss: 635.6455078125 = 1.4986960887908936 + 100.0 * 6.34146785736084
Epoch 350, val loss: 1.5429201126098633
Epoch 360, training loss: 635.2103881835938 = 1.4755953550338745 + 100.0 * 6.337347984313965
Epoch 360, val loss: 1.5236709117889404
Epoch 370, training loss: 635.0167236328125 = 1.4523916244506836 + 100.0 * 6.335643291473389
Epoch 370, val loss: 1.5045491456985474
Epoch 380, training loss: 634.6771850585938 = 1.4291664361953735 + 100.0 * 6.332480430603027
Epoch 380, val loss: 1.4850187301635742
Epoch 390, training loss: 634.0810546875 = 1.4055854082107544 + 100.0 * 6.326754570007324
Epoch 390, val loss: 1.4660608768463135
Epoch 400, training loss: 633.6632690429688 = 1.3823837041854858 + 100.0 * 6.322808742523193
Epoch 400, val loss: 1.4471408128738403
Epoch 410, training loss: 633.2866821289062 = 1.3594118356704712 + 100.0 * 6.319272518157959
Epoch 410, val loss: 1.4285520315170288
Epoch 420, training loss: 633.4093627929688 = 1.3367383480072021 + 100.0 * 6.32072639465332
Epoch 420, val loss: 1.4099171161651611
Epoch 430, training loss: 632.6942749023438 = 1.3137683868408203 + 100.0 * 6.313804626464844
Epoch 430, val loss: 1.392221450805664
Epoch 440, training loss: 632.294189453125 = 1.291428804397583 + 100.0 * 6.310027599334717
Epoch 440, val loss: 1.3744009733200073
Epoch 450, training loss: 631.9901733398438 = 1.2694121599197388 + 100.0 * 6.3072075843811035
Epoch 450, val loss: 1.3571968078613281
Epoch 460, training loss: 632.3040771484375 = 1.2478164434432983 + 100.0 * 6.310562610626221
Epoch 460, val loss: 1.3403615951538086
Epoch 470, training loss: 631.7882690429688 = 1.2260868549346924 + 100.0 * 6.305622100830078
Epoch 470, val loss: 1.3243438005447388
Epoch 480, training loss: 631.3151245117188 = 1.2049065828323364 + 100.0 * 6.301102161407471
Epoch 480, val loss: 1.3079737424850464
Epoch 490, training loss: 630.929443359375 = 1.1841955184936523 + 100.0 * 6.297452449798584
Epoch 490, val loss: 1.2927390336990356
Epoch 500, training loss: 630.686279296875 = 1.163949966430664 + 100.0 * 6.295223236083984
Epoch 500, val loss: 1.2780252695083618
Epoch 510, training loss: 631.0440063476562 = 1.1439765691757202 + 100.0 * 6.299000263214111
Epoch 510, val loss: 1.264134168624878
Epoch 520, training loss: 630.4443969726562 = 1.124457597732544 + 100.0 * 6.29319953918457
Epoch 520, val loss: 1.2498539686203003
Epoch 530, training loss: 630.0546875 = 1.1053417921066284 + 100.0 * 6.289493560791016
Epoch 530, val loss: 1.2367357015609741
Epoch 540, training loss: 629.8343505859375 = 1.0866934061050415 + 100.0 * 6.287477016448975
Epoch 540, val loss: 1.2243362665176392
Epoch 550, training loss: 630.22509765625 = 1.068576693534851 + 100.0 * 6.29156494140625
Epoch 550, val loss: 1.212080955505371
Epoch 560, training loss: 629.4026489257812 = 1.0504614114761353 + 100.0 * 6.28352165222168
Epoch 560, val loss: 1.200700044631958
Epoch 570, training loss: 629.2777709960938 = 1.0330007076263428 + 100.0 * 6.282447814941406
Epoch 570, val loss: 1.1899871826171875
Epoch 580, training loss: 629.242919921875 = 1.0161229372024536 + 100.0 * 6.282268047332764
Epoch 580, val loss: 1.1799308061599731
Epoch 590, training loss: 628.9683837890625 = 0.9994367957115173 + 100.0 * 6.279689311981201
Epoch 590, val loss: 1.1696195602416992
Epoch 600, training loss: 628.9382934570312 = 0.9832195043563843 + 100.0 * 6.279550552368164
Epoch 600, val loss: 1.1605896949768066
Epoch 610, training loss: 628.5369262695312 = 0.9674530029296875 + 100.0 * 6.275694370269775
Epoch 610, val loss: 1.1519181728363037
Epoch 620, training loss: 628.4541015625 = 0.9521200060844421 + 100.0 * 6.275019645690918
Epoch 620, val loss: 1.1438374519348145
Epoch 630, training loss: 628.4764404296875 = 0.9370980262756348 + 100.0 * 6.275393486022949
Epoch 630, val loss: 1.136387825012207
Epoch 640, training loss: 628.2307739257812 = 0.9225034713745117 + 100.0 * 6.273082733154297
Epoch 640, val loss: 1.1287083625793457
Epoch 650, training loss: 628.3268432617188 = 0.908197820186615 + 100.0 * 6.274186134338379
Epoch 650, val loss: 1.121939778327942
Epoch 660, training loss: 627.9414672851562 = 0.8940633535385132 + 100.0 * 6.270474433898926
Epoch 660, val loss: 1.11580228805542
Epoch 670, training loss: 627.6680908203125 = 0.8805152773857117 + 100.0 * 6.267875671386719
Epoch 670, val loss: 1.1097445487976074
Epoch 680, training loss: 627.5911254882812 = 0.8673288822174072 + 100.0 * 6.267238140106201
Epoch 680, val loss: 1.10426926612854
Epoch 690, training loss: 627.6358032226562 = 0.8543208837509155 + 100.0 * 6.267815113067627
Epoch 690, val loss: 1.0990639925003052
Epoch 700, training loss: 627.3294677734375 = 0.8414440751075745 + 100.0 * 6.264880657196045
Epoch 700, val loss: 1.0944410562515259
Epoch 710, training loss: 627.5277709960938 = 0.829149603843689 + 100.0 * 6.26698637008667
Epoch 710, val loss: 1.089697003364563
Epoch 720, training loss: 627.1881103515625 = 0.8166550397872925 + 100.0 * 6.263714790344238
Epoch 720, val loss: 1.0860283374786377
Epoch 730, training loss: 626.9683227539062 = 0.8047481775283813 + 100.0 * 6.261635780334473
Epoch 730, val loss: 1.0817680358886719
Epoch 740, training loss: 626.7439575195312 = 0.7930524945259094 + 100.0 * 6.259509563446045
Epoch 740, val loss: 1.0785315036773682
Epoch 750, training loss: 627.3372192382812 = 0.7817392945289612 + 100.0 * 6.265554904937744
Epoch 750, val loss: 1.0752257108688354
Epoch 760, training loss: 626.6767578125 = 0.7701700329780579 + 100.0 * 6.259066104888916
Epoch 760, val loss: 1.0718497037887573
Epoch 770, training loss: 626.5 = 0.7589971423149109 + 100.0 * 6.257410049438477
Epoch 770, val loss: 1.0689810514450073
Epoch 780, training loss: 626.3198852539062 = 0.7481038570404053 + 100.0 * 6.255717754364014
Epoch 780, val loss: 1.0664807558059692
Epoch 790, training loss: 626.4322509765625 = 0.7374297380447388 + 100.0 * 6.256948471069336
Epoch 790, val loss: 1.0641164779663086
Epoch 800, training loss: 626.2545166015625 = 0.7267032265663147 + 100.0 * 6.255278587341309
Epoch 800, val loss: 1.0617470741271973
Epoch 810, training loss: 626.171875 = 0.7160410284996033 + 100.0 * 6.254558086395264
Epoch 810, val loss: 1.059799313545227
Epoch 820, training loss: 626.0474853515625 = 0.7057101726531982 + 100.0 * 6.25341796875
Epoch 820, val loss: 1.0577336549758911
Epoch 830, training loss: 625.80908203125 = 0.6954370737075806 + 100.0 * 6.251136779785156
Epoch 830, val loss: 1.0561386346817017
Epoch 840, training loss: 625.9025268554688 = 0.6853241324424744 + 100.0 * 6.252172470092773
Epoch 840, val loss: 1.0546700954437256
Epoch 850, training loss: 625.8322143554688 = 0.6752708554267883 + 100.0 * 6.2515692710876465
Epoch 850, val loss: 1.053162932395935
Epoch 860, training loss: 626.1406860351562 = 0.6651514768600464 + 100.0 * 6.254755020141602
Epoch 860, val loss: 1.0516504049301147
Epoch 870, training loss: 625.5555419921875 = 0.6551413536071777 + 100.0 * 6.249003887176514
Epoch 870, val loss: 1.0499478578567505
Epoch 880, training loss: 625.4735717773438 = 0.6453812122344971 + 100.0 * 6.248281955718994
Epoch 880, val loss: 1.0489989519119263
Epoch 890, training loss: 625.5592041015625 = 0.6356361508369446 + 100.0 * 6.249236106872559
Epoch 890, val loss: 1.0477997064590454
Epoch 900, training loss: 625.3286743164062 = 0.6259170770645142 + 100.0 * 6.247027397155762
Epoch 900, val loss: 1.0469176769256592
Epoch 910, training loss: 625.136474609375 = 0.6163173317909241 + 100.0 * 6.245201587677002
Epoch 910, val loss: 1.0458414554595947
Epoch 920, training loss: 625.0858154296875 = 0.6068617105484009 + 100.0 * 6.2447896003723145
Epoch 920, val loss: 1.044973373413086
Epoch 930, training loss: 625.23779296875 = 0.5974262952804565 + 100.0 * 6.246403694152832
Epoch 930, val loss: 1.0442352294921875
Epoch 940, training loss: 624.9912719726562 = 0.5879873633384705 + 100.0 * 6.244032382965088
Epoch 940, val loss: 1.0428619384765625
Epoch 950, training loss: 624.8060302734375 = 0.5786903500556946 + 100.0 * 6.242273330688477
Epoch 950, val loss: 1.0421524047851562
Epoch 960, training loss: 624.7632446289062 = 0.5694799423217773 + 100.0 * 6.241937637329102
Epoch 960, val loss: 1.0411497354507446
Epoch 970, training loss: 624.6832885742188 = 0.5603654384613037 + 100.0 * 6.24122953414917
Epoch 970, val loss: 1.0404736995697021
Epoch 980, training loss: 624.8479614257812 = 0.5513062477111816 + 100.0 * 6.242966175079346
Epoch 980, val loss: 1.0395127534866333
Epoch 990, training loss: 624.673828125 = 0.5421579480171204 + 100.0 * 6.241316318511963
Epoch 990, val loss: 1.0391616821289062
Epoch 1000, training loss: 624.4354248046875 = 0.5331428050994873 + 100.0 * 6.239022731781006
Epoch 1000, val loss: 1.038272738456726
Epoch 1010, training loss: 624.420654296875 = 0.5243085622787476 + 100.0 * 6.238963603973389
Epoch 1010, val loss: 1.0376514196395874
Epoch 1020, training loss: 624.4237670898438 = 0.5154439806938171 + 100.0 * 6.239083290100098
Epoch 1020, val loss: 1.0369588136672974
Epoch 1030, training loss: 624.3493041992188 = 0.5066602230072021 + 100.0 * 6.238426685333252
Epoch 1030, val loss: 1.0363593101501465
Epoch 1040, training loss: 624.23828125 = 0.49793416261672974 + 100.0 * 6.237403392791748
Epoch 1040, val loss: 1.0358649492263794
Epoch 1050, training loss: 624.512451171875 = 0.48931145668029785 + 100.0 * 6.240231513977051
Epoch 1050, val loss: 1.035430908203125
Epoch 1060, training loss: 624.1192626953125 = 0.48082447052001953 + 100.0 * 6.236384391784668
Epoch 1060, val loss: 1.0342333316802979
Epoch 1070, training loss: 623.9754638671875 = 0.4723860025405884 + 100.0 * 6.235030651092529
Epoch 1070, val loss: 1.0339878797531128
Epoch 1080, training loss: 624.1841430664062 = 0.46404018998146057 + 100.0 * 6.23720121383667
Epoch 1080, val loss: 1.0333577394485474
Epoch 1090, training loss: 623.9161376953125 = 0.45578503608703613 + 100.0 * 6.234603404998779
Epoch 1090, val loss: 1.0325819253921509
Epoch 1100, training loss: 623.9019775390625 = 0.4475470781326294 + 100.0 * 6.234543800354004
Epoch 1100, val loss: 1.032029628753662
Epoch 1110, training loss: 623.8928833007812 = 0.4394192695617676 + 100.0 * 6.234535217285156
Epoch 1110, val loss: 1.0315272808074951
Epoch 1120, training loss: 623.6749877929688 = 0.4314025938510895 + 100.0 * 6.232436180114746
Epoch 1120, val loss: 1.0309656858444214
Epoch 1130, training loss: 624.206787109375 = 0.42348140478134155 + 100.0 * 6.237833023071289
Epoch 1130, val loss: 1.0307484865188599
Epoch 1140, training loss: 623.7033081054688 = 0.415620356798172 + 100.0 * 6.232876300811768
Epoch 1140, val loss: 1.029636025428772
Epoch 1150, training loss: 623.4857788085938 = 0.4079280197620392 + 100.0 * 6.230778694152832
Epoch 1150, val loss: 1.029333472251892
Epoch 1160, training loss: 623.4036254882812 = 0.4003697335720062 + 100.0 * 6.230032444000244
Epoch 1160, val loss: 1.028987169265747
Epoch 1170, training loss: 623.6815795898438 = 0.3929689824581146 + 100.0 * 6.23288631439209
Epoch 1170, val loss: 1.0285991430282593
Epoch 1180, training loss: 623.6529541015625 = 0.3855432868003845 + 100.0 * 6.2326741218566895
Epoch 1180, val loss: 1.0280505418777466
Epoch 1190, training loss: 623.4249877929688 = 0.3781164884567261 + 100.0 * 6.23046875
Epoch 1190, val loss: 1.028001308441162
Epoch 1200, training loss: 623.3822021484375 = 0.3709508776664734 + 100.0 * 6.230112552642822
Epoch 1200, val loss: 1.0278054475784302
Epoch 1210, training loss: 623.2123413085938 = 0.3639543354511261 + 100.0 * 6.2284836769104
Epoch 1210, val loss: 1.0276728868484497
Epoch 1220, training loss: 623.3591918945312 = 0.35708242654800415 + 100.0 * 6.2300214767456055
Epoch 1220, val loss: 1.027683973312378
Epoch 1230, training loss: 623.341796875 = 0.3501875698566437 + 100.0 * 6.229916095733643
Epoch 1230, val loss: 1.0277677774429321
Epoch 1240, training loss: 623.1484375 = 0.34345313906669617 + 100.0 * 6.2280497550964355
Epoch 1240, val loss: 1.0274996757507324
Epoch 1250, training loss: 622.9571533203125 = 0.33687087893486023 + 100.0 * 6.226202964782715
Epoch 1250, val loss: 1.0274510383605957
Epoch 1260, training loss: 622.9260864257812 = 0.33046096563339233 + 100.0 * 6.225956439971924
Epoch 1260, val loss: 1.0276265144348145
Epoch 1270, training loss: 623.0833129882812 = 0.3242075443267822 + 100.0 * 6.227591037750244
Epoch 1270, val loss: 1.0277742147445679
Epoch 1280, training loss: 623.2697143554688 = 0.3179060220718384 + 100.0 * 6.229517936706543
Epoch 1280, val loss: 1.028143048286438
Epoch 1290, training loss: 622.9977416992188 = 0.31176772713661194 + 100.0 * 6.2268595695495605
Epoch 1290, val loss: 1.0283845663070679
Epoch 1300, training loss: 622.8243408203125 = 0.30573827028274536 + 100.0 * 6.225186347961426
Epoch 1300, val loss: 1.0286425352096558
Epoch 1310, training loss: 622.7098388671875 = 0.2999374568462372 + 100.0 * 6.224099159240723
Epoch 1310, val loss: 1.0293350219726562
Epoch 1320, training loss: 622.8997802734375 = 0.29426392912864685 + 100.0 * 6.226055145263672
Epoch 1320, val loss: 1.0301618576049805
Epoch 1330, training loss: 622.7154541015625 = 0.2885575592517853 + 100.0 * 6.224268913269043
Epoch 1330, val loss: 1.030705213546753
Epoch 1340, training loss: 622.5884399414062 = 0.2829916477203369 + 100.0 * 6.2230544090271
Epoch 1340, val loss: 1.0310773849487305
Epoch 1350, training loss: 622.5278930664062 = 0.2776123285293579 + 100.0 * 6.222503185272217
Epoch 1350, val loss: 1.0320956707000732
Epoch 1360, training loss: 622.5173950195312 = 0.27240705490112305 + 100.0 * 6.222449779510498
Epoch 1360, val loss: 1.0329993963241577
Epoch 1370, training loss: 623.33740234375 = 0.2672436237335205 + 100.0 * 6.230701446533203
Epoch 1370, val loss: 1.0338449478149414
Epoch 1380, training loss: 622.55712890625 = 0.26202520728111267 + 100.0 * 6.2229509353637695
Epoch 1380, val loss: 1.034972906112671
Epoch 1390, training loss: 622.585693359375 = 0.257018506526947 + 100.0 * 6.2232866287231445
Epoch 1390, val loss: 1.0360409021377563
Epoch 1400, training loss: 622.435302734375 = 0.2521824836730957 + 100.0 * 6.22183084487915
Epoch 1400, val loss: 1.0371898412704468
Epoch 1410, training loss: 622.2921142578125 = 0.24751532077789307 + 100.0 * 6.2204461097717285
Epoch 1410, val loss: 1.0386440753936768
Epoch 1420, training loss: 622.3142700195312 = 0.24295799434185028 + 100.0 * 6.220713138580322
Epoch 1420, val loss: 1.0400713682174683
Epoch 1430, training loss: 622.497802734375 = 0.23848232626914978 + 100.0 * 6.222593307495117
Epoch 1430, val loss: 1.0413867235183716
Epoch 1440, training loss: 623.1685791015625 = 0.23395295441150665 + 100.0 * 6.22934627532959
Epoch 1440, val loss: 1.0423336029052734
Epoch 1450, training loss: 622.3505859375 = 0.22945468127727509 + 100.0 * 6.2212114334106445
Epoch 1450, val loss: 1.044340968132019
Epoch 1460, training loss: 622.1090698242188 = 0.22517752647399902 + 100.0 * 6.218839168548584
Epoch 1460, val loss: 1.0460129976272583
Epoch 1470, training loss: 622.0551147460938 = 0.22107230126857758 + 100.0 * 6.2183403968811035
Epoch 1470, val loss: 1.0477499961853027
Epoch 1480, training loss: 622.0435180664062 = 0.21705937385559082 + 100.0 * 6.218264579772949
Epoch 1480, val loss: 1.049750804901123
Epoch 1490, training loss: 622.8949584960938 = 0.213079571723938 + 100.0 * 6.226819038391113
Epoch 1490, val loss: 1.0515227317810059
Epoch 1500, training loss: 622.0731201171875 = 0.20911766588687897 + 100.0 * 6.218640327453613
Epoch 1500, val loss: 1.0533182621002197
Epoch 1510, training loss: 621.9781494140625 = 0.20528677105903625 + 100.0 * 6.217728614807129
Epoch 1510, val loss: 1.0553399324417114
Epoch 1520, training loss: 621.947021484375 = 0.20160174369812012 + 100.0 * 6.217454433441162
Epoch 1520, val loss: 1.0574685335159302
Epoch 1530, training loss: 623.1795043945312 = 0.19802363216876984 + 100.0 * 6.2298150062561035
Epoch 1530, val loss: 1.0593386888504028
Epoch 1540, training loss: 622.0057983398438 = 0.19428420066833496 + 100.0 * 6.218115329742432
Epoch 1540, val loss: 1.0613576173782349
Epoch 1550, training loss: 621.9074096679688 = 0.19073016941547394 + 100.0 * 6.217166900634766
Epoch 1550, val loss: 1.0634459257125854
Epoch 1560, training loss: 621.7754516601562 = 0.18736392259597778 + 100.0 * 6.215880870819092
Epoch 1560, val loss: 1.0659356117248535
Epoch 1570, training loss: 621.9674682617188 = 0.18408526480197906 + 100.0 * 6.217833995819092
Epoch 1570, val loss: 1.0684884786605835
Epoch 1580, training loss: 621.8502807617188 = 0.18079571425914764 + 100.0 * 6.2166948318481445
Epoch 1580, val loss: 1.0705723762512207
Epoch 1590, training loss: 621.7584228515625 = 0.17762087285518646 + 100.0 * 6.215807914733887
Epoch 1590, val loss: 1.0729434490203857
Epoch 1600, training loss: 621.7893676757812 = 0.17450161278247833 + 100.0 * 6.216148853302002
Epoch 1600, val loss: 1.075575828552246
Epoch 1610, training loss: 621.9592895507812 = 0.17150089144706726 + 100.0 * 6.2178778648376465
Epoch 1610, val loss: 1.0781253576278687
Epoch 1620, training loss: 621.6934204101562 = 0.16847583651542664 + 100.0 * 6.215249538421631
Epoch 1620, val loss: 1.0805505514144897
Epoch 1630, training loss: 621.6001586914062 = 0.16558054089546204 + 100.0 * 6.214345932006836
Epoch 1630, val loss: 1.0831536054611206
Epoch 1640, training loss: 621.7481689453125 = 0.16277241706848145 + 100.0 * 6.215854167938232
Epoch 1640, val loss: 1.0856631994247437
Epoch 1650, training loss: 621.6141357421875 = 0.1599637269973755 + 100.0 * 6.214541912078857
Epoch 1650, val loss: 1.0882620811462402
Epoch 1660, training loss: 621.5481567382812 = 0.15719398856163025 + 100.0 * 6.213910102844238
Epoch 1660, val loss: 1.0909684896469116
Epoch 1670, training loss: 621.6869506835938 = 0.15453550219535828 + 100.0 * 6.2153239250183105
Epoch 1670, val loss: 1.093750238418579
Epoch 1680, training loss: 621.652587890625 = 0.15188392996788025 + 100.0 * 6.215007305145264
Epoch 1680, val loss: 1.0963228940963745
Epoch 1690, training loss: 621.73583984375 = 0.14929191768169403 + 100.0 * 6.215865612030029
Epoch 1690, val loss: 1.099192500114441
Epoch 1700, training loss: 621.50146484375 = 0.1467900574207306 + 100.0 * 6.2135467529296875
Epoch 1700, val loss: 1.102286458015442
Epoch 1710, training loss: 621.347412109375 = 0.14433653652668 + 100.0 * 6.21203088760376
Epoch 1710, val loss: 1.1051862239837646
Epoch 1720, training loss: 621.2763061523438 = 0.1419697403907776 + 100.0 * 6.211343288421631
Epoch 1720, val loss: 1.1081933975219727
Epoch 1730, training loss: 621.605224609375 = 0.1396685540676117 + 100.0 * 6.214655876159668
Epoch 1730, val loss: 1.1112338304519653
Epoch 1740, training loss: 621.7415771484375 = 0.1373032182455063 + 100.0 * 6.216042518615723
Epoch 1740, val loss: 1.1142092943191528
Epoch 1750, training loss: 621.6612548828125 = 0.13492262363433838 + 100.0 * 6.2152628898620605
Epoch 1750, val loss: 1.1168200969696045
Epoch 1760, training loss: 621.1954956054688 = 0.13270290195941925 + 100.0 * 6.210628032684326
Epoch 1760, val loss: 1.1199345588684082
Epoch 1770, training loss: 621.2198486328125 = 0.1305808126926422 + 100.0 * 6.210892677307129
Epoch 1770, val loss: 1.1231780052185059
Epoch 1780, training loss: 621.249267578125 = 0.12850810587406158 + 100.0 * 6.211207866668701
Epoch 1780, val loss: 1.1263258457183838
Epoch 1790, training loss: 621.7025756835938 = 0.12644939124584198 + 100.0 * 6.215761184692383
Epoch 1790, val loss: 1.1292614936828613
Epoch 1800, training loss: 621.3648681640625 = 0.12431033700704575 + 100.0 * 6.212405681610107
Epoch 1800, val loss: 1.1324542760849
Epoch 1810, training loss: 621.243896484375 = 0.12231814861297607 + 100.0 * 6.211215972900391
Epoch 1810, val loss: 1.1358133554458618
Epoch 1820, training loss: 621.2152709960938 = 0.1203383132815361 + 100.0 * 6.210948944091797
Epoch 1820, val loss: 1.1389611959457397
Epoch 1830, training loss: 621.0402221679688 = 0.11842908710241318 + 100.0 * 6.2092180252075195
Epoch 1830, val loss: 1.141998052597046
Epoch 1840, training loss: 621.095703125 = 0.1165786013007164 + 100.0 * 6.20979118347168
Epoch 1840, val loss: 1.1454012393951416
Epoch 1850, training loss: 621.3048706054688 = 0.11475290358066559 + 100.0 * 6.2119011878967285
Epoch 1850, val loss: 1.1487383842468262
Epoch 1860, training loss: 621.1280517578125 = 0.11289278417825699 + 100.0 * 6.210151195526123
Epoch 1860, val loss: 1.1519147157669067
Epoch 1870, training loss: 621.1177368164062 = 0.11109290271997452 + 100.0 * 6.210066795349121
Epoch 1870, val loss: 1.154746413230896
Epoch 1880, training loss: 621.1112670898438 = 0.10934507846832275 + 100.0 * 6.210018634796143
Epoch 1880, val loss: 1.158388376235962
Epoch 1890, training loss: 620.974853515625 = 0.10763125866651535 + 100.0 * 6.208672523498535
Epoch 1890, val loss: 1.1612756252288818
Epoch 1900, training loss: 621.497802734375 = 0.10597067326307297 + 100.0 * 6.213918209075928
Epoch 1900, val loss: 1.1646397113800049
Epoch 1910, training loss: 621.0560913085938 = 0.10431186109781265 + 100.0 * 6.209517955780029
Epoch 1910, val loss: 1.1679798364639282
Epoch 1920, training loss: 620.8994750976562 = 0.10267948359251022 + 100.0 * 6.207968235015869
Epoch 1920, val loss: 1.1712701320648193
Epoch 1930, training loss: 620.8311767578125 = 0.10112974792718887 + 100.0 * 6.207300662994385
Epoch 1930, val loss: 1.1747233867645264
Epoch 1940, training loss: 621.1480712890625 = 0.09961625188589096 + 100.0 * 6.210484504699707
Epoch 1940, val loss: 1.1780734062194824
Epoch 1950, training loss: 621.0569458007812 = 0.09806232154369354 + 100.0 * 6.209588527679443
Epoch 1950, val loss: 1.1812292337417603
Epoch 1960, training loss: 620.7752075195312 = 0.09649658203125 + 100.0 * 6.206787109375
Epoch 1960, val loss: 1.184748649597168
Epoch 1970, training loss: 620.7271728515625 = 0.095009446144104 + 100.0 * 6.2063212394714355
Epoch 1970, val loss: 1.188165307044983
Epoch 1980, training loss: 620.7011108398438 = 0.09360548853874207 + 100.0 * 6.2060747146606445
Epoch 1980, val loss: 1.1917825937271118
Epoch 1990, training loss: 621.4152221679688 = 0.09222546964883804 + 100.0 * 6.213230133056641
Epoch 1990, val loss: 1.1953312158584595
Epoch 2000, training loss: 620.838134765625 = 0.09077440947294235 + 100.0 * 6.2074737548828125
Epoch 2000, val loss: 1.1982330083847046
Epoch 2010, training loss: 620.748779296875 = 0.08938531577587128 + 100.0 * 6.206593990325928
Epoch 2010, val loss: 1.2017571926116943
Epoch 2020, training loss: 620.7014770507812 = 0.08805634081363678 + 100.0 * 6.206133842468262
Epoch 2020, val loss: 1.2052485942840576
Epoch 2030, training loss: 620.8432006835938 = 0.08675693720579147 + 100.0 * 6.207564830780029
Epoch 2030, val loss: 1.2086414098739624
Epoch 2040, training loss: 620.79541015625 = 0.08545834571123123 + 100.0 * 6.207099437713623
Epoch 2040, val loss: 1.2123682498931885
Epoch 2050, training loss: 620.7946166992188 = 0.0841594785451889 + 100.0 * 6.207104206085205
Epoch 2050, val loss: 1.2154872417449951
Epoch 2060, training loss: 620.6137084960938 = 0.08288871496915817 + 100.0 * 6.205307960510254
Epoch 2060, val loss: 1.218941330909729
Epoch 2070, training loss: 620.5337524414062 = 0.08167847990989685 + 100.0 * 6.2045207023620605
Epoch 2070, val loss: 1.222800612449646
Epoch 2080, training loss: 620.5174560546875 = 0.08050645887851715 + 100.0 * 6.20436954498291
Epoch 2080, val loss: 1.2262252569198608
Epoch 2090, training loss: 621.034912109375 = 0.07936657220125198 + 100.0 * 6.209555625915527
Epoch 2090, val loss: 1.2297558784484863
Epoch 2100, training loss: 620.6918334960938 = 0.07816481590270996 + 100.0 * 6.206136703491211
Epoch 2100, val loss: 1.2330894470214844
Epoch 2110, training loss: 620.6917724609375 = 0.07699022442102432 + 100.0 * 6.206148147583008
Epoch 2110, val loss: 1.2367275953292847
Epoch 2120, training loss: 620.58056640625 = 0.07585279643535614 + 100.0 * 6.205047130584717
Epoch 2120, val loss: 1.2400263547897339
Epoch 2130, training loss: 620.5222778320312 = 0.07475149631500244 + 100.0 * 6.204474925994873
Epoch 2130, val loss: 1.2436736822128296
Epoch 2140, training loss: 620.56689453125 = 0.07367831468582153 + 100.0 * 6.20493221282959
Epoch 2140, val loss: 1.2471580505371094
Epoch 2150, training loss: 620.57470703125 = 0.07262100279331207 + 100.0 * 6.205020904541016
Epoch 2150, val loss: 1.2508021593093872
Epoch 2160, training loss: 620.6891479492188 = 0.07158808410167694 + 100.0 * 6.206175804138184
Epoch 2160, val loss: 1.254260778427124
Epoch 2170, training loss: 620.6726684570312 = 0.07052873820066452 + 100.0 * 6.206021308898926
Epoch 2170, val loss: 1.257490634918213
Epoch 2180, training loss: 620.3273315429688 = 0.06947656720876694 + 100.0 * 6.202579021453857
Epoch 2180, val loss: 1.2611826658248901
Epoch 2190, training loss: 620.3059692382812 = 0.0684940293431282 + 100.0 * 6.2023749351501465
Epoch 2190, val loss: 1.264769434928894
Epoch 2200, training loss: 620.28271484375 = 0.06753739714622498 + 100.0 * 6.202151298522949
Epoch 2200, val loss: 1.2682735919952393
Epoch 2210, training loss: 620.7521362304688 = 0.06659602373838425 + 100.0 * 6.206855297088623
Epoch 2210, val loss: 1.271676778793335
Epoch 2220, training loss: 620.329833984375 = 0.06562412530183792 + 100.0 * 6.202642440795898
Epoch 2220, val loss: 1.2752485275268555
Epoch 2230, training loss: 620.262451171875 = 0.064662866294384 + 100.0 * 6.2019782066345215
Epoch 2230, val loss: 1.2785314321517944
Epoch 2240, training loss: 620.249267578125 = 0.06377330422401428 + 100.0 * 6.201854705810547
Epoch 2240, val loss: 1.282332181930542
Epoch 2250, training loss: 620.8685302734375 = 0.06290341913700104 + 100.0 * 6.2080559730529785
Epoch 2250, val loss: 1.2857112884521484
Epoch 2260, training loss: 620.4960327148438 = 0.06199498102068901 + 100.0 * 6.204339981079102
Epoch 2260, val loss: 1.2889057397842407
Epoch 2270, training loss: 620.3676147460938 = 0.06108729913830757 + 100.0 * 6.203064918518066
Epoch 2270, val loss: 1.2925769090652466
Epoch 2280, training loss: 620.379638671875 = 0.06024071201682091 + 100.0 * 6.2031941413879395
Epoch 2280, val loss: 1.2959561347961426
Epoch 2290, training loss: 620.1817626953125 = 0.05939776822924614 + 100.0 * 6.201223850250244
Epoch 2290, val loss: 1.2995702028274536
Epoch 2300, training loss: 620.154541015625 = 0.058587297797203064 + 100.0 * 6.2009596824646
Epoch 2300, val loss: 1.30312180519104
Epoch 2310, training loss: 620.364013671875 = 0.057802774012088776 + 100.0 * 6.203062057495117
Epoch 2310, val loss: 1.306566596031189
Epoch 2320, training loss: 620.2020263671875 = 0.05700399726629257 + 100.0 * 6.201450347900391
Epoch 2320, val loss: 1.3100391626358032
Epoch 2330, training loss: 620.6397705078125 = 0.05622808262705803 + 100.0 * 6.205835342407227
Epoch 2330, val loss: 1.3131216764450073
Epoch 2340, training loss: 620.34228515625 = 0.05540445074439049 + 100.0 * 6.202868938446045
Epoch 2340, val loss: 1.3166871070861816
Epoch 2350, training loss: 620.1354370117188 = 0.05462466925382614 + 100.0 * 6.200808048248291
Epoch 2350, val loss: 1.3201007843017578
Epoch 2360, training loss: 620.10498046875 = 0.05388467386364937 + 100.0 * 6.2005109786987305
Epoch 2360, val loss: 1.323547601699829
Epoch 2370, training loss: 620.2736206054688 = 0.05317709222435951 + 100.0 * 6.202204704284668
Epoch 2370, val loss: 1.3269866704940796
Epoch 2380, training loss: 620.0761108398438 = 0.05243954807519913 + 100.0 * 6.200236797332764
Epoch 2380, val loss: 1.3304167985916138
Epoch 2390, training loss: 620.3798828125 = 0.05173585191369057 + 100.0 * 6.203281402587891
Epoch 2390, val loss: 1.3337182998657227
Epoch 2400, training loss: 620.0592041015625 = 0.05102729797363281 + 100.0 * 6.200081825256348
Epoch 2400, val loss: 1.3370174169540405
Epoch 2410, training loss: 620.2164306640625 = 0.05034221336245537 + 100.0 * 6.201660633087158
Epoch 2410, val loss: 1.3402327299118042
Epoch 2420, training loss: 620.187255859375 = 0.049669817090034485 + 100.0 * 6.201375961303711
Epoch 2420, val loss: 1.343619465827942
Epoch 2430, training loss: 620.0048217773438 = 0.0489850677549839 + 100.0 * 6.199558258056641
Epoch 2430, val loss: 1.3470052480697632
Epoch 2440, training loss: 620.1466674804688 = 0.048353880643844604 + 100.0 * 6.20098352432251
Epoch 2440, val loss: 1.3504217863082886
Epoch 2450, training loss: 620.181396484375 = 0.047704365104436874 + 100.0 * 6.20133638381958
Epoch 2450, val loss: 1.3535786867141724
Epoch 2460, training loss: 620.1078491210938 = 0.047046761959791183 + 100.0 * 6.200607776641846
Epoch 2460, val loss: 1.3566198348999023
Epoch 2470, training loss: 620.0064697265625 = 0.046430885791778564 + 100.0 * 6.1996002197265625
Epoch 2470, val loss: 1.3600752353668213
Epoch 2480, training loss: 619.9739379882812 = 0.04581698402762413 + 100.0 * 6.199280738830566
Epoch 2480, val loss: 1.3634058237075806
Epoch 2490, training loss: 620.2664184570312 = 0.04523375257849693 + 100.0 * 6.202211856842041
Epoch 2490, val loss: 1.366542935371399
Epoch 2500, training loss: 620.0632934570312 = 0.04461215063929558 + 100.0 * 6.200186729431152
Epoch 2500, val loss: 1.3697185516357422
Epoch 2510, training loss: 619.927734375 = 0.04401380568742752 + 100.0 * 6.1988372802734375
Epoch 2510, val loss: 1.3731566667556763
Epoch 2520, training loss: 619.9865112304688 = 0.04344703257083893 + 100.0 * 6.199430465698242
Epoch 2520, val loss: 1.3763337135314941
Epoch 2530, training loss: 620.1048583984375 = 0.04288880527019501 + 100.0 * 6.200619220733643
Epoch 2530, val loss: 1.379683017730713
Epoch 2540, training loss: 620.3048095703125 = 0.042344048619270325 + 100.0 * 6.202624320983887
Epoch 2540, val loss: 1.382491111755371
Epoch 2550, training loss: 619.8850708007812 = 0.04174783080816269 + 100.0 * 6.1984333992004395
Epoch 2550, val loss: 1.3856655359268188
Epoch 2560, training loss: 619.7964477539062 = 0.041217461228370667 + 100.0 * 6.197552680969238
Epoch 2560, val loss: 1.3890118598937988
Epoch 2570, training loss: 619.7927856445312 = 0.040699511766433716 + 100.0 * 6.197520732879639
Epoch 2570, val loss: 1.3922243118286133
Epoch 2580, training loss: 619.8387451171875 = 0.040199603885412216 + 100.0 * 6.1979851722717285
Epoch 2580, val loss: 1.3953520059585571
Epoch 2590, training loss: 620.2932739257812 = 0.03970057889819145 + 100.0 * 6.202536106109619
Epoch 2590, val loss: 1.3985122442245483
Epoch 2600, training loss: 620.070556640625 = 0.03917660564184189 + 100.0 * 6.200313568115234
Epoch 2600, val loss: 1.40162193775177
Epoch 2610, training loss: 619.93115234375 = 0.038669832050800323 + 100.0 * 6.198924541473389
Epoch 2610, val loss: 1.4045605659484863
Epoch 2620, training loss: 619.9035034179688 = 0.038179583847522736 + 100.0 * 6.198653221130371
Epoch 2620, val loss: 1.4075950384140015
Epoch 2630, training loss: 620.093017578125 = 0.037701573222875595 + 100.0 * 6.200552940368652
Epoch 2630, val loss: 1.4103775024414062
Epoch 2640, training loss: 619.943603515625 = 0.03721730038523674 + 100.0 * 6.199063777923584
Epoch 2640, val loss: 1.4137018918991089
Epoch 2650, training loss: 619.7026977539062 = 0.03674563020467758 + 100.0 * 6.196659564971924
Epoch 2650, val loss: 1.416803240776062
Epoch 2660, training loss: 619.6694946289062 = 0.036293160170316696 + 100.0 * 6.196331977844238
Epoch 2660, val loss: 1.4199074506759644
Epoch 2670, training loss: 619.7549438476562 = 0.03586246818304062 + 100.0 * 6.197190284729004
Epoch 2670, val loss: 1.422958254814148
Epoch 2680, training loss: 620.1646118164062 = 0.035442717373371124 + 100.0 * 6.201291561126709
Epoch 2680, val loss: 1.425817847251892
Epoch 2690, training loss: 620.0138549804688 = 0.034962449222803116 + 100.0 * 6.199788570404053
Epoch 2690, val loss: 1.4284011125564575
Epoch 2700, training loss: 619.748779296875 = 0.034525442868471146 + 100.0 * 6.197142124176025
Epoch 2700, val loss: 1.4317889213562012
Epoch 2710, training loss: 619.7531127929688 = 0.03410844877362251 + 100.0 * 6.197189807891846
Epoch 2710, val loss: 1.4346771240234375
Epoch 2720, training loss: 620.1727905273438 = 0.03369909152388573 + 100.0 * 6.201391220092773
Epoch 2720, val loss: 1.4375990629196167
Epoch 2730, training loss: 619.7564697265625 = 0.0332719087600708 + 100.0 * 6.197232246398926
Epoch 2730, val loss: 1.44024658203125
Epoch 2740, training loss: 619.5697631835938 = 0.032869603484869 + 100.0 * 6.195368766784668
Epoch 2740, val loss: 1.4434406757354736
Epoch 2750, training loss: 619.5454711914062 = 0.03249216824769974 + 100.0 * 6.195129871368408
Epoch 2750, val loss: 1.446468472480774
Epoch 2760, training loss: 619.7434692382812 = 0.03211770951747894 + 100.0 * 6.197113513946533
Epoch 2760, val loss: 1.4493848085403442
Epoch 2770, training loss: 619.8519287109375 = 0.03172251582145691 + 100.0 * 6.198201656341553
Epoch 2770, val loss: 1.451965093612671
Epoch 2780, training loss: 619.5848388671875 = 0.03132681921124458 + 100.0 * 6.195535182952881
Epoch 2780, val loss: 1.454877495765686
Epoch 2790, training loss: 619.5567626953125 = 0.03095543198287487 + 100.0 * 6.195258140563965
Epoch 2790, val loss: 1.4575120210647583
Epoch 2800, training loss: 619.7847290039062 = 0.03060052916407585 + 100.0 * 6.197541236877441
Epoch 2800, val loss: 1.460465908050537
Epoch 2810, training loss: 619.6319580078125 = 0.03024081140756607 + 100.0 * 6.196017742156982
Epoch 2810, val loss: 1.4632980823516846
Epoch 2820, training loss: 619.732666015625 = 0.02989167533814907 + 100.0 * 6.197027683258057
Epoch 2820, val loss: 1.466370701789856
Epoch 2830, training loss: 619.504638671875 = 0.029537944123148918 + 100.0 * 6.194751262664795
Epoch 2830, val loss: 1.4690581560134888
Epoch 2840, training loss: 619.5390014648438 = 0.029205508530139923 + 100.0 * 6.19509744644165
Epoch 2840, val loss: 1.4718976020812988
Epoch 2850, training loss: 619.9566650390625 = 0.028895199298858643 + 100.0 * 6.199277877807617
Epoch 2850, val loss: 1.4747730493545532
Epoch 2860, training loss: 619.6310424804688 = 0.028534073382616043 + 100.0 * 6.1960248947143555
Epoch 2860, val loss: 1.4771896600723267
Epoch 2870, training loss: 619.437255859375 = 0.02819669060409069 + 100.0 * 6.194090366363525
Epoch 2870, val loss: 1.479885220527649
Epoch 2880, training loss: 619.53271484375 = 0.02788713574409485 + 100.0 * 6.1950483322143555
Epoch 2880, val loss: 1.4827853441238403
Epoch 2890, training loss: 619.84716796875 = 0.027571750804781914 + 100.0 * 6.198195934295654
Epoch 2890, val loss: 1.4855300188064575
Epoch 2900, training loss: 619.6804809570312 = 0.027245594188570976 + 100.0 * 6.196532726287842
Epoch 2900, val loss: 1.4879827499389648
Epoch 2910, training loss: 619.4365844726562 = 0.02693476714193821 + 100.0 * 6.194096565246582
Epoch 2910, val loss: 1.4907017946243286
Epoch 2920, training loss: 619.4002075195312 = 0.02664586715400219 + 100.0 * 6.193735122680664
Epoch 2920, val loss: 1.4935529232025146
Epoch 2930, training loss: 619.8078002929688 = 0.026363831013441086 + 100.0 * 6.197814464569092
Epoch 2930, val loss: 1.4960615634918213
Epoch 2940, training loss: 619.5988159179688 = 0.026055235415697098 + 100.0 * 6.195727825164795
Epoch 2940, val loss: 1.498704195022583
Epoch 2950, training loss: 619.34765625 = 0.025746362283825874 + 100.0 * 6.193219184875488
Epoch 2950, val loss: 1.5012849569320679
Epoch 2960, training loss: 619.4157104492188 = 0.025473644956946373 + 100.0 * 6.193902492523193
Epoch 2960, val loss: 1.5040946006774902
Epoch 2970, training loss: 619.670166015625 = 0.025204116478562355 + 100.0 * 6.1964497566223145
Epoch 2970, val loss: 1.506670594215393
Epoch 2980, training loss: 619.487548828125 = 0.024926820769906044 + 100.0 * 6.194626331329346
Epoch 2980, val loss: 1.5091075897216797
Epoch 2990, training loss: 619.4707641601562 = 0.02465047687292099 + 100.0 * 6.194461345672607
Epoch 2990, val loss: 1.5119683742523193
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 861.6593017578125 = 1.9761775732040405 + 100.0 * 8.596831321716309
Epoch 0, val loss: 1.9875712394714355
Epoch 10, training loss: 861.5604248046875 = 1.9661574363708496 + 100.0 * 8.595942497253418
Epoch 10, val loss: 1.9767743349075317
Epoch 20, training loss: 860.9432373046875 = 1.9534859657287598 + 100.0 * 8.589897155761719
Epoch 20, val loss: 1.9629372358322144
Epoch 30, training loss: 856.7815551757812 = 1.937228798866272 + 100.0 * 8.548442840576172
Epoch 30, val loss: 1.94521963596344
Epoch 40, training loss: 830.2105712890625 = 1.9174033403396606 + 100.0 * 8.282931327819824
Epoch 40, val loss: 1.9242544174194336
Epoch 50, training loss: 746.94775390625 = 1.8950601816177368 + 100.0 * 7.450526714324951
Epoch 50, val loss: 1.901219367980957
Epoch 60, training loss: 718.7564086914062 = 1.879219651222229 + 100.0 * 7.168772220611572
Epoch 60, val loss: 1.8864606618881226
Epoch 70, training loss: 702.2767944335938 = 1.8654849529266357 + 100.0 * 7.00411319732666
Epoch 70, val loss: 1.8724462985992432
Epoch 80, training loss: 688.4690551757812 = 1.8527358770370483 + 100.0 * 6.86616325378418
Epoch 80, val loss: 1.8597358465194702
Epoch 90, training loss: 679.0787963867188 = 1.8409041166305542 + 100.0 * 6.772379398345947
Epoch 90, val loss: 1.847772240638733
Epoch 100, training loss: 672.4371337890625 = 1.8292065858840942 + 100.0 * 6.706079006195068
Epoch 100, val loss: 1.8363550901412964
Epoch 110, training loss: 667.6728515625 = 1.817765474319458 + 100.0 * 6.65855073928833
Epoch 110, val loss: 1.8249523639678955
Epoch 120, training loss: 663.526611328125 = 1.806925654411316 + 100.0 * 6.617196559906006
Epoch 120, val loss: 1.8139574527740479
Epoch 130, training loss: 659.883544921875 = 1.7966675758361816 + 100.0 * 6.580868244171143
Epoch 130, val loss: 1.8036959171295166
Epoch 140, training loss: 657.0021362304688 = 1.786983847618103 + 100.0 * 6.552151679992676
Epoch 140, val loss: 1.7939499616622925
Epoch 150, training loss: 654.2193603515625 = 1.777546763420105 + 100.0 * 6.524418354034424
Epoch 150, val loss: 1.784578800201416
Epoch 160, training loss: 652.0214233398438 = 1.7682194709777832 + 100.0 * 6.5025315284729
Epoch 160, val loss: 1.7754125595092773
Epoch 170, training loss: 649.9388427734375 = 1.7586467266082764 + 100.0 * 6.481801509857178
Epoch 170, val loss: 1.766146183013916
Epoch 180, training loss: 648.1278076171875 = 1.7485908269882202 + 100.0 * 6.463791847229004
Epoch 180, val loss: 1.7566603422164917
Epoch 190, training loss: 646.6834106445312 = 1.7378361225128174 + 100.0 * 6.449455261230469
Epoch 190, val loss: 1.7468187808990479
Epoch 200, training loss: 645.2234497070312 = 1.726307988166809 + 100.0 * 6.434971332550049
Epoch 200, val loss: 1.7364157438278198
Epoch 210, training loss: 644.3167114257812 = 1.7139986753463745 + 100.0 * 6.426027297973633
Epoch 210, val loss: 1.7253142595291138
Epoch 220, training loss: 643.0115966796875 = 1.7006731033325195 + 100.0 * 6.413108825683594
Epoch 220, val loss: 1.7135683298110962
Epoch 230, training loss: 642.03173828125 = 1.6864567995071411 + 100.0 * 6.4034528732299805
Epoch 230, val loss: 1.701163649559021
Epoch 240, training loss: 641.139404296875 = 1.6711723804473877 + 100.0 * 6.39468240737915
Epoch 240, val loss: 1.6878730058670044
Epoch 250, training loss: 640.2879638671875 = 1.6549015045166016 + 100.0 * 6.386330604553223
Epoch 250, val loss: 1.6738109588623047
Epoch 260, training loss: 639.8203735351562 = 1.6374913454055786 + 100.0 * 6.381828784942627
Epoch 260, val loss: 1.6588222980499268
Epoch 270, training loss: 639.0953369140625 = 1.6190364360809326 + 100.0 * 6.374763488769531
Epoch 270, val loss: 1.6429260969161987
Epoch 280, training loss: 638.2916259765625 = 1.5996302366256714 + 100.0 * 6.366920471191406
Epoch 280, val loss: 1.6263662576675415
Epoch 290, training loss: 637.6866455078125 = 1.5793862342834473 + 100.0 * 6.361072540283203
Epoch 290, val loss: 1.6091386079788208
Epoch 300, training loss: 637.2610473632812 = 1.5583922863006592 + 100.0 * 6.35702657699585
Epoch 300, val loss: 1.5913792848587036
Epoch 310, training loss: 636.8049926757812 = 1.5367282629013062 + 100.0 * 6.352682590484619
Epoch 310, val loss: 1.572983741760254
Epoch 320, training loss: 636.2070922851562 = 1.5144964456558228 + 100.0 * 6.346925735473633
Epoch 320, val loss: 1.5544100999832153
Epoch 330, training loss: 635.7189331054688 = 1.491795539855957 + 100.0 * 6.342271327972412
Epoch 330, val loss: 1.535426139831543
Epoch 340, training loss: 635.2015991210938 = 1.4688140153884888 + 100.0 * 6.33732795715332
Epoch 340, val loss: 1.5164315700531006
Epoch 350, training loss: 634.73095703125 = 1.4457240104675293 + 100.0 * 6.332851886749268
Epoch 350, val loss: 1.4975192546844482
Epoch 360, training loss: 634.9090576171875 = 1.4225096702575684 + 100.0 * 6.334865570068359
Epoch 360, val loss: 1.4786162376403809
Epoch 370, training loss: 634.0348510742188 = 1.3993330001831055 + 100.0 * 6.32635498046875
Epoch 370, val loss: 1.459816336631775
Epoch 380, training loss: 633.6009521484375 = 1.3762918710708618 + 100.0 * 6.322246551513672
Epoch 380, val loss: 1.4413511753082275
Epoch 390, training loss: 633.4589233398438 = 1.3535020351409912 + 100.0 * 6.321053981781006
Epoch 390, val loss: 1.423024296760559
Epoch 400, training loss: 633.0274658203125 = 1.3307085037231445 + 100.0 * 6.316967964172363
Epoch 400, val loss: 1.4050413370132446
Epoch 410, training loss: 632.7152099609375 = 1.308386206626892 + 100.0 * 6.314067840576172
Epoch 410, val loss: 1.3873564004898071
Epoch 420, training loss: 632.3553466796875 = 1.2863476276397705 + 100.0 * 6.310689449310303
Epoch 420, val loss: 1.3697850704193115
Epoch 430, training loss: 632.18798828125 = 1.2645527124404907 + 100.0 * 6.309234619140625
Epoch 430, val loss: 1.3528879880905151
Epoch 440, training loss: 631.7529907226562 = 1.243336796760559 + 100.0 * 6.30509614944458
Epoch 440, val loss: 1.3362329006195068
Epoch 450, training loss: 631.4716186523438 = 1.22242271900177 + 100.0 * 6.302492141723633
Epoch 450, val loss: 1.319994330406189
Epoch 460, training loss: 631.4222412109375 = 1.2019411325454712 + 100.0 * 6.3022027015686035
Epoch 460, val loss: 1.3042477369308472
Epoch 470, training loss: 631.0481567382812 = 1.1817505359649658 + 100.0 * 6.298664093017578
Epoch 470, val loss: 1.2885648012161255
Epoch 480, training loss: 630.7166748046875 = 1.1620759963989258 + 100.0 * 6.295546531677246
Epoch 480, val loss: 1.2734793424606323
Epoch 490, training loss: 630.7987060546875 = 1.1428385972976685 + 100.0 * 6.296558856964111
Epoch 490, val loss: 1.2586065530776978
Epoch 500, training loss: 630.4302368164062 = 1.1239081621170044 + 100.0 * 6.293063640594482
Epoch 500, val loss: 1.2446008920669556
Epoch 510, training loss: 630.1090698242188 = 1.1053928136825562 + 100.0 * 6.290036678314209
Epoch 510, val loss: 1.2306100130081177
Epoch 520, training loss: 629.9329833984375 = 1.0873280763626099 + 100.0 * 6.288456439971924
Epoch 520, val loss: 1.2171510457992554
Epoch 530, training loss: 629.7385864257812 = 1.0696347951889038 + 100.0 * 6.286689281463623
Epoch 530, val loss: 1.2038357257843018
Epoch 540, training loss: 629.420166015625 = 1.052256464958191 + 100.0 * 6.283679008483887
Epoch 540, val loss: 1.1912721395492554
Epoch 550, training loss: 629.2584838867188 = 1.0352956056594849 + 100.0 * 6.282232284545898
Epoch 550, val loss: 1.1789098978042603
Epoch 560, training loss: 629.5802612304688 = 1.018629550933838 + 100.0 * 6.285616397857666
Epoch 560, val loss: 1.167265772819519
Epoch 570, training loss: 629.0138549804688 = 1.0023020505905151 + 100.0 * 6.280115127563477
Epoch 570, val loss: 1.1552748680114746
Epoch 580, training loss: 628.8399658203125 = 0.9862682223320007 + 100.0 * 6.278537273406982
Epoch 580, val loss: 1.1443685293197632
Epoch 590, training loss: 628.9759521484375 = 0.9703690409660339 + 100.0 * 6.280055522918701
Epoch 590, val loss: 1.13292396068573
Epoch 600, training loss: 628.4407958984375 = 0.9548181891441345 + 100.0 * 6.274859428405762
Epoch 600, val loss: 1.1227684020996094
Epoch 610, training loss: 628.1964111328125 = 0.9396408796310425 + 100.0 * 6.2725677490234375
Epoch 610, val loss: 1.1128026247024536
Epoch 620, training loss: 628.1245727539062 = 0.9247815012931824 + 100.0 * 6.271997928619385
Epoch 620, val loss: 1.1032477617263794
Epoch 630, training loss: 628.0853881835938 = 0.9100556969642639 + 100.0 * 6.271753787994385
Epoch 630, val loss: 1.09395170211792
Epoch 640, training loss: 627.7550659179688 = 0.8955483436584473 + 100.0 * 6.268594741821289
Epoch 640, val loss: 1.0850622653961182
Epoch 650, training loss: 627.6744384765625 = 0.8812660574913025 + 100.0 * 6.2679314613342285
Epoch 650, val loss: 1.0764636993408203
Epoch 660, training loss: 628.2266235351562 = 0.8672153949737549 + 100.0 * 6.273594379425049
Epoch 660, val loss: 1.0682106018066406
Epoch 670, training loss: 627.4505615234375 = 0.8532601594924927 + 100.0 * 6.265973091125488
Epoch 670, val loss: 1.0602891445159912
Epoch 680, training loss: 627.3515625 = 0.8395116925239563 + 100.0 * 6.265120029449463
Epoch 680, val loss: 1.052656888961792
Epoch 690, training loss: 627.2122802734375 = 0.825920820236206 + 100.0 * 6.263863563537598
Epoch 690, val loss: 1.0454678535461426
Epoch 700, training loss: 627.1754760742188 = 0.8124868273735046 + 100.0 * 6.263629913330078
Epoch 700, val loss: 1.0386347770690918
Epoch 710, training loss: 626.9492797851562 = 0.7993127703666687 + 100.0 * 6.261499404907227
Epoch 710, val loss: 1.0317180156707764
Epoch 720, training loss: 626.76416015625 = 0.7862077951431274 + 100.0 * 6.259779930114746
Epoch 720, val loss: 1.0255099534988403
Epoch 730, training loss: 626.8887329101562 = 0.773372232913971 + 100.0 * 6.261153697967529
Epoch 730, val loss: 1.019417405128479
Epoch 740, training loss: 626.8839721679688 = 0.7606026530265808 + 100.0 * 6.261233806610107
Epoch 740, val loss: 1.0134307146072388
Epoch 750, training loss: 626.6060180664062 = 0.7479168176651001 + 100.0 * 6.258580684661865
Epoch 750, val loss: 1.0081913471221924
Epoch 760, training loss: 626.7001342773438 = 0.7354311347007751 + 100.0 * 6.259647369384766
Epoch 760, val loss: 1.0028150081634521
Epoch 770, training loss: 626.3018798828125 = 0.7230398058891296 + 100.0 * 6.255788803100586
Epoch 770, val loss: 0.9974688291549683
Epoch 780, training loss: 626.1173095703125 = 0.710906982421875 + 100.0 * 6.254064559936523
Epoch 780, val loss: 0.992961049079895
Epoch 790, training loss: 626.0198364257812 = 0.6989904046058655 + 100.0 * 6.253208637237549
Epoch 790, val loss: 0.9884700179100037
Epoch 800, training loss: 626.68408203125 = 0.6871806383132935 + 100.0 * 6.2599687576293945
Epoch 800, val loss: 0.984285831451416
Epoch 810, training loss: 625.805908203125 = 0.6754119396209717 + 100.0 * 6.251305103302002
Epoch 810, val loss: 0.980257511138916
Epoch 820, training loss: 625.8079223632812 = 0.6639359593391418 + 100.0 * 6.251440048217773
Epoch 820, val loss: 0.9766811728477478
Epoch 830, training loss: 625.7069091796875 = 0.6526989936828613 + 100.0 * 6.250542163848877
Epoch 830, val loss: 0.9735109806060791
Epoch 840, training loss: 625.8098754882812 = 0.6415277719497681 + 100.0 * 6.251683235168457
Epoch 840, val loss: 0.9703512787818909
Epoch 850, training loss: 625.5484619140625 = 0.6304727792739868 + 100.0 * 6.249179840087891
Epoch 850, val loss: 0.967432975769043
Epoch 860, training loss: 625.4378051757812 = 0.6196725368499756 + 100.0 * 6.248180866241455
Epoch 860, val loss: 0.9650066494941711
Epoch 870, training loss: 625.3616333007812 = 0.6091578602790833 + 100.0 * 6.247524738311768
Epoch 870, val loss: 0.9629757404327393
Epoch 880, training loss: 625.6420288085938 = 0.5986349582672119 + 100.0 * 6.250433921813965
Epoch 880, val loss: 0.9608029723167419
Epoch 890, training loss: 625.333984375 = 0.5882413387298584 + 100.0 * 6.247457504272461
Epoch 890, val loss: 0.9588209390640259
Epoch 900, training loss: 625.1512451171875 = 0.5781655311584473 + 100.0 * 6.245730876922607
Epoch 900, val loss: 0.9574208855628967
Epoch 910, training loss: 625.0216674804688 = 0.5683609247207642 + 100.0 * 6.244533061981201
Epoch 910, val loss: 0.9561955332756042
Epoch 920, training loss: 624.9016723632812 = 0.5587501525878906 + 100.0 * 6.243428707122803
Epoch 920, val loss: 0.9552167057991028
Epoch 930, training loss: 624.8524780273438 = 0.5492806434631348 + 100.0 * 6.243032455444336
Epoch 930, val loss: 0.9544155597686768
Epoch 940, training loss: 625.7411499023438 = 0.5398932695388794 + 100.0 * 6.252012252807617
Epoch 940, val loss: 0.9535434246063232
Epoch 950, training loss: 624.8259887695312 = 0.5305425524711609 + 100.0 * 6.242954730987549
Epoch 950, val loss: 0.9533457159996033
Epoch 960, training loss: 624.8443603515625 = 0.5214113593101501 + 100.0 * 6.243229389190674
Epoch 960, val loss: 0.9534642100334167
Epoch 970, training loss: 624.6932373046875 = 0.512470006942749 + 100.0 * 6.24180793762207
Epoch 970, val loss: 0.9534698724746704
Epoch 980, training loss: 624.585693359375 = 0.5037097930908203 + 100.0 * 6.240819931030273
Epoch 980, val loss: 0.9538589119911194
Epoch 990, training loss: 624.6429443359375 = 0.49512749910354614 + 100.0 * 6.241478443145752
Epoch 990, val loss: 0.9545323252677917
Epoch 1000, training loss: 624.4449462890625 = 0.48664548993110657 + 100.0 * 6.2395830154418945
Epoch 1000, val loss: 0.9550390243530273
Epoch 1010, training loss: 624.5115966796875 = 0.47825366258621216 + 100.0 * 6.240333557128906
Epoch 1010, val loss: 0.9557707905769348
Epoch 1020, training loss: 624.6986083984375 = 0.4699808955192566 + 100.0 * 6.242286682128906
Epoch 1020, val loss: 0.9567825794219971
Epoch 1030, training loss: 624.6581420898438 = 0.4618411660194397 + 100.0 * 6.241962909698486
Epoch 1030, val loss: 0.9577358961105347
Epoch 1040, training loss: 624.2616577148438 = 0.45371586084365845 + 100.0 * 6.238079071044922
Epoch 1040, val loss: 0.958750307559967
Epoch 1050, training loss: 624.1659545898438 = 0.44584596157073975 + 100.0 * 6.23720121383667
Epoch 1050, val loss: 0.9601964950561523
Epoch 1060, training loss: 624.0789794921875 = 0.438128262758255 + 100.0 * 6.236408710479736
Epoch 1060, val loss: 0.9616690278053284
Epoch 1070, training loss: 624.2437744140625 = 0.4305485785007477 + 100.0 * 6.238132476806641
Epoch 1070, val loss: 0.9632229208946228
Epoch 1080, training loss: 623.9151611328125 = 0.4230126738548279 + 100.0 * 6.234921455383301
Epoch 1080, val loss: 0.9650205373764038
Epoch 1090, training loss: 624.0650634765625 = 0.41564345359802246 + 100.0 * 6.236494064331055
Epoch 1090, val loss: 0.9668930768966675
Epoch 1100, training loss: 624.1815185546875 = 0.4082726538181305 + 100.0 * 6.237732410430908
Epoch 1100, val loss: 0.9684410095214844
Epoch 1110, training loss: 623.927978515625 = 0.4010629653930664 + 100.0 * 6.235269069671631
Epoch 1110, val loss: 0.9703710675239563
Epoch 1120, training loss: 623.7733764648438 = 0.39397841691970825 + 100.0 * 6.23379373550415
Epoch 1120, val loss: 0.9722816944122314
Epoch 1130, training loss: 623.7500610351562 = 0.3870725631713867 + 100.0 * 6.2336297035217285
Epoch 1130, val loss: 0.974446713924408
Epoch 1140, training loss: 623.9706420898438 = 0.3802398443222046 + 100.0 * 6.235903739929199
Epoch 1140, val loss: 0.976542592048645
Epoch 1150, training loss: 623.9534912109375 = 0.373457670211792 + 100.0 * 6.235800266265869
Epoch 1150, val loss: 0.9784027338027954
Epoch 1160, training loss: 623.720703125 = 0.36677423119544983 + 100.0 * 6.233539581298828
Epoch 1160, val loss: 0.9806728959083557
Epoch 1170, training loss: 623.6369018554688 = 0.36024209856987 + 100.0 * 6.232766628265381
Epoch 1170, val loss: 0.9829187393188477
Epoch 1180, training loss: 623.46240234375 = 0.35383060574531555 + 100.0 * 6.231085777282715
Epoch 1180, val loss: 0.9852902889251709
Epoch 1190, training loss: 623.655029296875 = 0.3475166857242584 + 100.0 * 6.23307466506958
Epoch 1190, val loss: 0.9875226020812988
Epoch 1200, training loss: 623.5949096679688 = 0.34123215079307556 + 100.0 * 6.232536315917969
Epoch 1200, val loss: 0.9898375272750854
Epoch 1210, training loss: 623.3148193359375 = 0.33505669236183167 + 100.0 * 6.22979736328125
Epoch 1210, val loss: 0.9924368262290955
Epoch 1220, training loss: 623.2479248046875 = 0.32905715703964233 + 100.0 * 6.229188919067383
Epoch 1220, val loss: 0.9950476884841919
Epoch 1230, training loss: 623.8091430664062 = 0.32316869497299194 + 100.0 * 6.234859466552734
Epoch 1230, val loss: 0.9976118206977844
Epoch 1240, training loss: 623.4020385742188 = 0.317231684923172 + 100.0 * 6.2308478355407715
Epoch 1240, val loss: 1.000112771987915
Epoch 1250, training loss: 623.23291015625 = 0.31147700548171997 + 100.0 * 6.229214191436768
Epoch 1250, val loss: 1.0029014348983765
Epoch 1260, training loss: 623.1232299804688 = 0.3058238923549652 + 100.0 * 6.228173732757568
Epoch 1260, val loss: 1.0057239532470703
Epoch 1270, training loss: 623.10009765625 = 0.30029720067977905 + 100.0 * 6.227997779846191
Epoch 1270, val loss: 1.008468747138977
Epoch 1280, training loss: 623.2304077148438 = 0.2948235273361206 + 100.0 * 6.229356288909912
Epoch 1280, val loss: 1.0111098289489746
Epoch 1290, training loss: 623.2537231445312 = 0.2893928289413452 + 100.0 * 6.22964334487915
Epoch 1290, val loss: 1.0139920711517334
Epoch 1300, training loss: 623.0878295898438 = 0.28406834602355957 + 100.0 * 6.2280378341674805
Epoch 1300, val loss: 1.0169423818588257
Epoch 1310, training loss: 622.98876953125 = 0.27884402871131897 + 100.0 * 6.2270989418029785
Epoch 1310, val loss: 1.0200639963150024
Epoch 1320, training loss: 622.8215942382812 = 0.27370932698249817 + 100.0 * 6.2254791259765625
Epoch 1320, val loss: 1.023010015487671
Epoch 1330, training loss: 622.9966430664062 = 0.26871922612190247 + 100.0 * 6.227279186248779
Epoch 1330, val loss: 1.0262253284454346
Epoch 1340, training loss: 622.7692260742188 = 0.2637527585029602 + 100.0 * 6.225054740905762
Epoch 1340, val loss: 1.0293673276901245
Epoch 1350, training loss: 622.9036865234375 = 0.2588919997215271 + 100.0 * 6.226447582244873
Epoch 1350, val loss: 1.0327504873275757
Epoch 1360, training loss: 623.0655517578125 = 0.25406038761138916 + 100.0 * 6.228114604949951
Epoch 1360, val loss: 1.0356879234313965
Epoch 1370, training loss: 622.8707885742188 = 0.2493094652891159 + 100.0 * 6.22621488571167
Epoch 1370, val loss: 1.0393004417419434
Epoch 1380, training loss: 622.6508178710938 = 0.2446974217891693 + 100.0 * 6.224061012268066
Epoch 1380, val loss: 1.0426435470581055
Epoch 1390, training loss: 622.6441040039062 = 0.24021846055984497 + 100.0 * 6.224038600921631
Epoch 1390, val loss: 1.0462578535079956
Epoch 1400, training loss: 622.8804931640625 = 0.23576897382736206 + 100.0 * 6.226447105407715
Epoch 1400, val loss: 1.0495878458023071
Epoch 1410, training loss: 622.784423828125 = 0.23134618997573853 + 100.0 * 6.225531101226807
Epoch 1410, val loss: 1.0531612634658813
Epoch 1420, training loss: 622.5445556640625 = 0.22701077163219452 + 100.0 * 6.223175525665283
Epoch 1420, val loss: 1.056931495666504
Epoch 1430, training loss: 622.5402221679688 = 0.22280502319335938 + 100.0 * 6.223174571990967
Epoch 1430, val loss: 1.060745120048523
Epoch 1440, training loss: 622.609619140625 = 0.21864648163318634 + 100.0 * 6.223909854888916
Epoch 1440, val loss: 1.0643396377563477
Epoch 1450, training loss: 622.4506225585938 = 0.2145618051290512 + 100.0 * 6.222360610961914
Epoch 1450, val loss: 1.068249225616455
Epoch 1460, training loss: 622.431640625 = 0.21055445075035095 + 100.0 * 6.22221040725708
Epoch 1460, val loss: 1.0720751285552979
Epoch 1470, training loss: 622.624267578125 = 0.20662492513656616 + 100.0 * 6.224176406860352
Epoch 1470, val loss: 1.0760408639907837
Epoch 1480, training loss: 622.4937133789062 = 0.20271146297454834 + 100.0 * 6.222909927368164
Epoch 1480, val loss: 1.0798004865646362
Epoch 1490, training loss: 622.3380126953125 = 0.198906809091568 + 100.0 * 6.221391201019287
Epoch 1490, val loss: 1.0840394496917725
Epoch 1500, training loss: 622.1943359375 = 0.19517108798027039 + 100.0 * 6.219991207122803
Epoch 1500, val loss: 1.0883009433746338
Epoch 1510, training loss: 622.353271484375 = 0.19155991077423096 + 100.0 * 6.221616744995117
Epoch 1510, val loss: 1.0927084684371948
Epoch 1520, training loss: 622.240234375 = 0.18795451521873474 + 100.0 * 6.220522880554199
Epoch 1520, val loss: 1.0966780185699463
Epoch 1530, training loss: 622.2236328125 = 0.18439579010009766 + 100.0 * 6.22039270401001
Epoch 1530, val loss: 1.1009083986282349
Epoch 1540, training loss: 622.421142578125 = 0.18089447915554047 + 100.0 * 6.222402095794678
Epoch 1540, val loss: 1.1050997972488403
Epoch 1550, training loss: 622.1318359375 = 0.17748019099235535 + 100.0 * 6.21954345703125
Epoch 1550, val loss: 1.1098723411560059
Epoch 1560, training loss: 622.123291015625 = 0.174138605594635 + 100.0 * 6.219491481781006
Epoch 1560, val loss: 1.114120602607727
Epoch 1570, training loss: 622.1077880859375 = 0.17086462676525116 + 100.0 * 6.219369411468506
Epoch 1570, val loss: 1.1185160875320435
Epoch 1580, training loss: 622.3410034179688 = 0.16765686869621277 + 100.0 * 6.221733570098877
Epoch 1580, val loss: 1.122939944267273
Epoch 1590, training loss: 621.9922485351562 = 0.16444426774978638 + 100.0 * 6.218278408050537
Epoch 1590, val loss: 1.1276253461837769
Epoch 1600, training loss: 621.9801025390625 = 0.161328986287117 + 100.0 * 6.2181878089904785
Epoch 1600, val loss: 1.1320959329605103
Epoch 1610, training loss: 622.1284790039062 = 0.15829987823963165 + 100.0 * 6.219701766967773
Epoch 1610, val loss: 1.137021780014038
Epoch 1620, training loss: 621.890625 = 0.15526746213436127 + 100.0 * 6.217353343963623
Epoch 1620, val loss: 1.1416245698928833
Epoch 1630, training loss: 622.041015625 = 0.15231415629386902 + 100.0 * 6.218886852264404
Epoch 1630, val loss: 1.1463847160339355
Epoch 1640, training loss: 621.8466796875 = 0.14939294755458832 + 100.0 * 6.216972827911377
Epoch 1640, val loss: 1.151063323020935
Epoch 1650, training loss: 621.9039306640625 = 0.1465730518102646 + 100.0 * 6.217574119567871
Epoch 1650, val loss: 1.1558390855789185
Epoch 1660, training loss: 621.8931274414062 = 0.14377287030220032 + 100.0 * 6.217493534088135
Epoch 1660, val loss: 1.1606467962265015
Epoch 1670, training loss: 621.872314453125 = 0.1410638839006424 + 100.0 * 6.217312335968018
Epoch 1670, val loss: 1.165880560874939
Epoch 1680, training loss: 621.8551025390625 = 0.13836275041103363 + 100.0 * 6.217167377471924
Epoch 1680, val loss: 1.17066490650177
Epoch 1690, training loss: 621.6935424804688 = 0.1357172280550003 + 100.0 * 6.215578079223633
Epoch 1690, val loss: 1.1755417585372925
Epoch 1700, training loss: 621.9616088867188 = 0.13315102458000183 + 100.0 * 6.2182841300964355
Epoch 1700, val loss: 1.1805845499038696
Epoch 1710, training loss: 621.756591796875 = 0.13060829043388367 + 100.0 * 6.216259956359863
Epoch 1710, val loss: 1.185448408126831
Epoch 1720, training loss: 621.6026000976562 = 0.12808643281459808 + 100.0 * 6.214745044708252
Epoch 1720, val loss: 1.190369725227356
Epoch 1730, training loss: 621.5126953125 = 0.12568405270576477 + 100.0 * 6.213870048522949
Epoch 1730, val loss: 1.1954622268676758
Epoch 1740, training loss: 621.5617065429688 = 0.12333061546087265 + 100.0 * 6.214383602142334
Epoch 1740, val loss: 1.2004778385162354
Epoch 1750, training loss: 622.140869140625 = 0.1210368350148201 + 100.0 * 6.220198631286621
Epoch 1750, val loss: 1.2056825160980225
Epoch 1760, training loss: 621.6675415039062 = 0.11865976452827454 + 100.0 * 6.215488910675049
Epoch 1760, val loss: 1.2103042602539062
Epoch 1770, training loss: 621.4688110351562 = 0.11638984829187393 + 100.0 * 6.213523864746094
Epoch 1770, val loss: 1.2152400016784668
Epoch 1780, training loss: 621.5286254882812 = 0.1142173707485199 + 100.0 * 6.214144229888916
Epoch 1780, val loss: 1.2204862833023071
Epoch 1790, training loss: 621.4524536132812 = 0.11207478493452072 + 100.0 * 6.213404178619385
Epoch 1790, val loss: 1.2254292964935303
Epoch 1800, training loss: 621.693359375 = 0.1099940612912178 + 100.0 * 6.21583366394043
Epoch 1800, val loss: 1.2304002046585083
Epoch 1810, training loss: 621.3781127929688 = 0.10789389908313751 + 100.0 * 6.21270227432251
Epoch 1810, val loss: 1.2355215549468994
Epoch 1820, training loss: 621.3314819335938 = 0.10587522387504578 + 100.0 * 6.212255954742432
Epoch 1820, val loss: 1.2407037019729614
Epoch 1830, training loss: 621.5939331054688 = 0.10392305999994278 + 100.0 * 6.214900016784668
Epoch 1830, val loss: 1.2458572387695312
Epoch 1840, training loss: 621.2596435546875 = 0.10194142162799835 + 100.0 * 6.21157693862915
Epoch 1840, val loss: 1.2510201930999756
Epoch 1850, training loss: 621.2564697265625 = 0.10004612058401108 + 100.0 * 6.211564064025879
Epoch 1850, val loss: 1.2560590505599976
Epoch 1860, training loss: 621.452880859375 = 0.09821178019046783 + 100.0 * 6.2135467529296875
Epoch 1860, val loss: 1.2613660097122192
Epoch 1870, training loss: 621.3300170898438 = 0.09636730700731277 + 100.0 * 6.212336540222168
Epoch 1870, val loss: 1.2661367654800415
Epoch 1880, training loss: 621.3553466796875 = 0.09455971419811249 + 100.0 * 6.2126078605651855
Epoch 1880, val loss: 1.2708600759506226
Epoch 1890, training loss: 621.361083984375 = 0.09281683713197708 + 100.0 * 6.212682247161865
Epoch 1890, val loss: 1.2761480808258057
Epoch 1900, training loss: 621.3776245117188 = 0.09108646959066391 + 100.0 * 6.212865829467773
Epoch 1900, val loss: 1.2809429168701172
Epoch 1910, training loss: 621.13671875 = 0.08941192179918289 + 100.0 * 6.21047306060791
Epoch 1910, val loss: 1.2862943410873413
Epoch 1920, training loss: 621.1210327148438 = 0.08777514845132828 + 100.0 * 6.210332870483398
Epoch 1920, val loss: 1.2914118766784668
Epoch 1930, training loss: 621.7438354492188 = 0.08619146049022675 + 100.0 * 6.21657657623291
Epoch 1930, val loss: 1.2965501546859741
Epoch 1940, training loss: 621.2313842773438 = 0.08456197381019592 + 100.0 * 6.21146821975708
Epoch 1940, val loss: 1.30086350440979
Epoch 1950, training loss: 621.029052734375 = 0.08300311118364334 + 100.0 * 6.209460258483887
Epoch 1950, val loss: 1.3063619136810303
Epoch 1960, training loss: 620.9419555664062 = 0.08151282370090485 + 100.0 * 6.208604335784912
Epoch 1960, val loss: 1.311454176902771
Epoch 1970, training loss: 621.0438232421875 = 0.08006369322538376 + 100.0 * 6.209637641906738
Epoch 1970, val loss: 1.31671142578125
Epoch 1980, training loss: 621.508544921875 = 0.07862163335084915 + 100.0 * 6.214299201965332
Epoch 1980, val loss: 1.3215441703796387
Epoch 1990, training loss: 621.152587890625 = 0.07715450972318649 + 100.0 * 6.21075439453125
Epoch 1990, val loss: 1.3265239000320435
Epoch 2000, training loss: 621.103271484375 = 0.07575175911188126 + 100.0 * 6.210275173187256
Epoch 2000, val loss: 1.331702470779419
Epoch 2010, training loss: 621.0401000976562 = 0.07437843829393387 + 100.0 * 6.209657192230225
Epoch 2010, val loss: 1.3370871543884277
Epoch 2020, training loss: 621.2415771484375 = 0.07305184751749039 + 100.0 * 6.2116851806640625
Epoch 2020, val loss: 1.3424845933914185
Epoch 2030, training loss: 620.8318481445312 = 0.07169397175312042 + 100.0 * 6.207601547241211
Epoch 2030, val loss: 1.3472598791122437
Epoch 2040, training loss: 620.8070068359375 = 0.07041958719491959 + 100.0 * 6.207365989685059
Epoch 2040, val loss: 1.3525115251541138
Epoch 2050, training loss: 621.1138916015625 = 0.06918537616729736 + 100.0 * 6.210446834564209
Epoch 2050, val loss: 1.357717752456665
Epoch 2060, training loss: 620.9790649414062 = 0.06791965663433075 + 100.0 * 6.209111213684082
Epoch 2060, val loss: 1.3625729084014893
Epoch 2070, training loss: 620.9002075195312 = 0.06668747216463089 + 100.0 * 6.208334922790527
Epoch 2070, val loss: 1.3675611019134521
Epoch 2080, training loss: 620.7694702148438 = 0.06550262868404388 + 100.0 * 6.207039833068848
Epoch 2080, val loss: 1.3726449012756348
Epoch 2090, training loss: 621.1002807617188 = 0.0643715038895607 + 100.0 * 6.2103590965271
Epoch 2090, val loss: 1.3778338432312012
Epoch 2100, training loss: 620.7603149414062 = 0.06321308016777039 + 100.0 * 6.206970691680908
Epoch 2100, val loss: 1.3825753927230835
Epoch 2110, training loss: 620.7587280273438 = 0.06210068240761757 + 100.0 * 6.206966400146484
Epoch 2110, val loss: 1.3877310752868652
Epoch 2120, training loss: 620.9635620117188 = 0.06102679297327995 + 100.0 * 6.2090253829956055
Epoch 2120, val loss: 1.392585039138794
Epoch 2130, training loss: 620.7286376953125 = 0.059942565858364105 + 100.0 * 6.206686973571777
Epoch 2130, val loss: 1.3973246812820435
Epoch 2140, training loss: 620.7025756835938 = 0.05891063064336777 + 100.0 * 6.206436634063721
Epoch 2140, val loss: 1.4022902250289917
Epoch 2150, training loss: 620.9525756835938 = 0.057911623269319534 + 100.0 * 6.208946704864502
Epoch 2150, val loss: 1.407181739807129
Epoch 2160, training loss: 620.6849365234375 = 0.05690058693289757 + 100.0 * 6.20628023147583
Epoch 2160, val loss: 1.4121519327163696
Epoch 2170, training loss: 620.878662109375 = 0.05594537407159805 + 100.0 * 6.208227157592773
Epoch 2170, val loss: 1.4171382188796997
Epoch 2180, training loss: 620.6236572265625 = 0.0549820140004158 + 100.0 * 6.205686569213867
Epoch 2180, val loss: 1.4216631650924683
Epoch 2190, training loss: 620.6971435546875 = 0.05405784770846367 + 100.0 * 6.206430912017822
Epoch 2190, val loss: 1.4264222383499146
Epoch 2200, training loss: 620.877197265625 = 0.05315730348229408 + 100.0 * 6.208240509033203
Epoch 2200, val loss: 1.4312682151794434
Epoch 2210, training loss: 620.5851440429688 = 0.05225233733654022 + 100.0 * 6.205328941345215
Epoch 2210, val loss: 1.435869812965393
Epoch 2220, training loss: 620.502197265625 = 0.051393378525972366 + 100.0 * 6.204508304595947
Epoch 2220, val loss: 1.440667986869812
Epoch 2230, training loss: 620.732666015625 = 0.05057249963283539 + 100.0 * 6.206820487976074
Epoch 2230, val loss: 1.4455100297927856
Epoch 2240, training loss: 620.6646728515625 = 0.04971446096897125 + 100.0 * 6.206149101257324
Epoch 2240, val loss: 1.449806571006775
Epoch 2250, training loss: 620.464599609375 = 0.04888081178069115 + 100.0 * 6.204156875610352
Epoch 2250, val loss: 1.454315423965454
Epoch 2260, training loss: 620.4326171875 = 0.048092007637023926 + 100.0 * 6.203845500946045
Epoch 2260, val loss: 1.4590407609939575
Epoch 2270, training loss: 621.0572509765625 = 0.04734836146235466 + 100.0 * 6.210098743438721
Epoch 2270, val loss: 1.4638161659240723
Epoch 2280, training loss: 620.49169921875 = 0.04653427377343178 + 100.0 * 6.204452037811279
Epoch 2280, val loss: 1.4677882194519043
Epoch 2290, training loss: 620.3807983398438 = 0.04579950496554375 + 100.0 * 6.203350067138672
Epoch 2290, val loss: 1.4725191593170166
Epoch 2300, training loss: 620.5172729492188 = 0.04508279636502266 + 100.0 * 6.204721927642822
Epoch 2300, val loss: 1.4767488241195679
Epoch 2310, training loss: 620.9249267578125 = 0.04436538368463516 + 100.0 * 6.208805561065674
Epoch 2310, val loss: 1.481062889099121
Epoch 2320, training loss: 620.466064453125 = 0.04362838342785835 + 100.0 * 6.204224109649658
Epoch 2320, val loss: 1.485453724861145
Epoch 2330, training loss: 620.3272094726562 = 0.0429404154419899 + 100.0 * 6.2028422355651855
Epoch 2330, val loss: 1.4899616241455078
Epoch 2340, training loss: 620.2682495117188 = 0.04228256642818451 + 100.0 * 6.202259540557861
Epoch 2340, val loss: 1.494443416595459
Epoch 2350, training loss: 620.4111328125 = 0.041652001440525055 + 100.0 * 6.203695297241211
Epoch 2350, val loss: 1.498720645904541
Epoch 2360, training loss: 620.5067749023438 = 0.0410086028277874 + 100.0 * 6.204657554626465
Epoch 2360, val loss: 1.5029205083847046
Epoch 2370, training loss: 620.4017944335938 = 0.040371496230363846 + 100.0 * 6.203614711761475
Epoch 2370, val loss: 1.5072028636932373
Epoch 2380, training loss: 620.3673706054688 = 0.03975735604763031 + 100.0 * 6.20327615737915
Epoch 2380, val loss: 1.5114959478378296
Epoch 2390, training loss: 620.496337890625 = 0.03915930911898613 + 100.0 * 6.204571723937988
Epoch 2390, val loss: 1.5157221555709839
Epoch 2400, training loss: 620.5728759765625 = 0.03855746239423752 + 100.0 * 6.205342769622803
Epoch 2400, val loss: 1.5196949243545532
Epoch 2410, training loss: 620.3604736328125 = 0.037967924028635025 + 100.0 * 6.203225135803223
Epoch 2410, val loss: 1.523893117904663
Epoch 2420, training loss: 620.353271484375 = 0.037391990423202515 + 100.0 * 6.203158855438232
Epoch 2420, val loss: 1.5280799865722656
Epoch 2430, training loss: 620.2596435546875 = 0.036840103566646576 + 100.0 * 6.202227592468262
Epoch 2430, val loss: 1.5321180820465088
Epoch 2440, training loss: 620.2088623046875 = 0.03629867732524872 + 100.0 * 6.201725482940674
Epoch 2440, val loss: 1.5361354351043701
Epoch 2450, training loss: 620.20166015625 = 0.03578116372227669 + 100.0 * 6.201659202575684
Epoch 2450, val loss: 1.5404202938079834
Epoch 2460, training loss: 620.4531860351562 = 0.035265859216451645 + 100.0 * 6.204178810119629
Epoch 2460, val loss: 1.544364094734192
Epoch 2470, training loss: 620.2710571289062 = 0.03473808243870735 + 100.0 * 6.202363014221191
Epoch 2470, val loss: 1.5480353832244873
Epoch 2480, training loss: 620.2550659179688 = 0.034234531223773956 + 100.0 * 6.202208042144775
Epoch 2480, val loss: 1.5521202087402344
Epoch 2490, training loss: 620.4892578125 = 0.03374059870839119 + 100.0 * 6.204555034637451
Epoch 2490, val loss: 1.556031346321106
Epoch 2500, training loss: 620.37158203125 = 0.03325087949633598 + 100.0 * 6.203382968902588
Epoch 2500, val loss: 1.5597045421600342
Epoch 2510, training loss: 620.1420288085938 = 0.03277469798922539 + 100.0 * 6.20109224319458
Epoch 2510, val loss: 1.5638850927352905
Epoch 2520, training loss: 620.2454223632812 = 0.032326504588127136 + 100.0 * 6.2021307945251465
Epoch 2520, val loss: 1.5678774118423462
Epoch 2530, training loss: 620.1112060546875 = 0.03186609223484993 + 100.0 * 6.200793743133545
Epoch 2530, val loss: 1.5715720653533936
Epoch 2540, training loss: 620.073974609375 = 0.03142877668142319 + 100.0 * 6.200425148010254
Epoch 2540, val loss: 1.5754313468933105
Epoch 2550, training loss: 620.4894409179688 = 0.031004995107650757 + 100.0 * 6.204584121704102
Epoch 2550, val loss: 1.5790752172470093
Epoch 2560, training loss: 620.2086181640625 = 0.030556662008166313 + 100.0 * 6.201780319213867
Epoch 2560, val loss: 1.5827980041503906
Epoch 2570, training loss: 619.9635620117188 = 0.03012145310640335 + 100.0 * 6.199334144592285
Epoch 2570, val loss: 1.58664870262146
Epoch 2580, training loss: 619.9577026367188 = 0.029713554307818413 + 100.0 * 6.19927978515625
Epoch 2580, val loss: 1.5904186964035034
Epoch 2590, training loss: 620.3613891601562 = 0.029324324801564217 + 100.0 * 6.2033209800720215
Epoch 2590, val loss: 1.594146728515625
Epoch 2600, training loss: 620.0681762695312 = 0.028911519795656204 + 100.0 * 6.200392723083496
Epoch 2600, val loss: 1.597491979598999
Epoch 2610, training loss: 619.95703125 = 0.028514010831713676 + 100.0 * 6.19928503036499
Epoch 2610, val loss: 1.601190447807312
Epoch 2620, training loss: 619.8794555664062 = 0.028132788836956024 + 100.0 * 6.198513031005859
Epoch 2620, val loss: 1.6049909591674805
Epoch 2630, training loss: 620.0419311523438 = 0.027769597247242928 + 100.0 * 6.200141906738281
Epoch 2630, val loss: 1.608473539352417
Epoch 2640, training loss: 620.3634643554688 = 0.02739991620182991 + 100.0 * 6.203360557556152
Epoch 2640, val loss: 1.6117041110992432
Epoch 2650, training loss: 620.046142578125 = 0.027026623487472534 + 100.0 * 6.200191020965576
Epoch 2650, val loss: 1.6156208515167236
Epoch 2660, training loss: 619.8710327148438 = 0.026661651208996773 + 100.0 * 6.198443412780762
Epoch 2660, val loss: 1.6192471981048584
Epoch 2670, training loss: 619.806396484375 = 0.02632501907646656 + 100.0 * 6.197800636291504
Epoch 2670, val loss: 1.6228774785995483
Epoch 2680, training loss: 619.9850463867188 = 0.025999460369348526 + 100.0 * 6.19959020614624
Epoch 2680, val loss: 1.6264156103134155
Epoch 2690, training loss: 620.1276245117188 = 0.025664513930678368 + 100.0 * 6.201019763946533
Epoch 2690, val loss: 1.6297249794006348
Epoch 2700, training loss: 620.0457153320312 = 0.025318756699562073 + 100.0 * 6.200203895568848
Epoch 2700, val loss: 1.6329418420791626
Epoch 2710, training loss: 619.86279296875 = 0.02499478869140148 + 100.0 * 6.19837760925293
Epoch 2710, val loss: 1.6366198062896729
Epoch 2720, training loss: 619.8015747070312 = 0.024682091549038887 + 100.0 * 6.1977691650390625
Epoch 2720, val loss: 1.6400251388549805
Epoch 2730, training loss: 620.24267578125 = 0.024385930970311165 + 100.0 * 6.202182769775391
Epoch 2730, val loss: 1.643640160560608
Epoch 2740, training loss: 620.021240234375 = 0.024058876559138298 + 100.0 * 6.199971675872803
Epoch 2740, val loss: 1.6466670036315918
Epoch 2750, training loss: 619.8673706054688 = 0.023745806887745857 + 100.0 * 6.198436260223389
Epoch 2750, val loss: 1.6501281261444092
Epoch 2760, training loss: 619.8655395507812 = 0.023449933156371117 + 100.0 * 6.198421001434326
Epoch 2760, val loss: 1.6532360315322876
Epoch 2770, training loss: 619.8160400390625 = 0.02316134423017502 + 100.0 * 6.1979289054870605
Epoch 2770, val loss: 1.6566358804702759
Epoch 2780, training loss: 619.894287109375 = 0.022883480414748192 + 100.0 * 6.198713779449463
Epoch 2780, val loss: 1.659899353981018
Epoch 2790, training loss: 620.02734375 = 0.022603057324886322 + 100.0 * 6.200047492980957
Epoch 2790, val loss: 1.6632522344589233
Epoch 2800, training loss: 619.8876342773438 = 0.022315936163067818 + 100.0 * 6.198653221130371
Epoch 2800, val loss: 1.6664613485336304
Epoch 2810, training loss: 619.7918701171875 = 0.02204115502536297 + 100.0 * 6.19769811630249
Epoch 2810, val loss: 1.6697862148284912
Epoch 2820, training loss: 619.7649536132812 = 0.02177935279905796 + 100.0 * 6.197431564331055
Epoch 2820, val loss: 1.6732419729232788
Epoch 2830, training loss: 619.6675415039062 = 0.021518876776099205 + 100.0 * 6.196459770202637
Epoch 2830, val loss: 1.6764403581619263
Epoch 2840, training loss: 619.6773681640625 = 0.02126888930797577 + 100.0 * 6.196561336517334
Epoch 2840, val loss: 1.679719090461731
Epoch 2850, training loss: 620.2455444335938 = 0.021029146388173103 + 100.0 * 6.202244758605957
Epoch 2850, val loss: 1.6830872297286987
Epoch 2860, training loss: 619.8223266601562 = 0.020757829770445824 + 100.0 * 6.1980156898498535
Epoch 2860, val loss: 1.6855276823043823
Epoch 2870, training loss: 619.7755126953125 = 0.020512618124485016 + 100.0 * 6.197549819946289
Epoch 2870, val loss: 1.6887611150741577
Epoch 2880, training loss: 619.8609619140625 = 0.02027587965130806 + 100.0 * 6.19840669631958
Epoch 2880, val loss: 1.6921318769454956
Epoch 2890, training loss: 619.7047119140625 = 0.020032668486237526 + 100.0 * 6.196846961975098
Epoch 2890, val loss: 1.6950807571411133
Epoch 2900, training loss: 619.87109375 = 0.019801268354058266 + 100.0 * 6.198513031005859
Epoch 2900, val loss: 1.6979563236236572
Epoch 2910, training loss: 619.6939697265625 = 0.019570855423808098 + 100.0 * 6.196743965148926
Epoch 2910, val loss: 1.7010358572006226
Epoch 2920, training loss: 619.5400390625 = 0.019344905391335487 + 100.0 * 6.195207118988037
Epoch 2920, val loss: 1.7041375637054443
Epoch 2930, training loss: 619.555419921875 = 0.019132578745484352 + 100.0 * 6.1953630447387695
Epoch 2930, val loss: 1.7073348760604858
Epoch 2940, training loss: 620.009765625 = 0.01892692968249321 + 100.0 * 6.199908256530762
Epoch 2940, val loss: 1.710274577140808
Epoch 2950, training loss: 619.6828002929688 = 0.01869788020849228 + 100.0 * 6.196640968322754
Epoch 2950, val loss: 1.7125364542007446
Epoch 2960, training loss: 619.73046875 = 0.018484650179743767 + 100.0 * 6.19711971282959
Epoch 2960, val loss: 1.7158567905426025
Epoch 2970, training loss: 619.5450439453125 = 0.018269946798682213 + 100.0 * 6.195268154144287
Epoch 2970, val loss: 1.718583345413208
Epoch 2980, training loss: 619.4555053710938 = 0.01806708611547947 + 100.0 * 6.1943745613098145
Epoch 2980, val loss: 1.7215813398361206
Epoch 2990, training loss: 619.5479125976562 = 0.017877552658319473 + 100.0 * 6.195300102233887
Epoch 2990, val loss: 1.7244092226028442
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8371112282551397
The final CL Acc:0.71728, 0.02014, The final GNN Acc:0.83834, 0.00174
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10570])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6264038085938 = 1.9397531747817993 + 100.0 * 8.596866607666016
Epoch 0, val loss: 1.9429981708526611
Epoch 10, training loss: 861.557373046875 = 1.931810975074768 + 100.0 * 8.5962553024292
Epoch 10, val loss: 1.9347574710845947
Epoch 20, training loss: 861.1158447265625 = 1.922102928161621 + 100.0 * 8.591937065124512
Epoch 20, val loss: 1.9247009754180908
Epoch 30, training loss: 857.9967041015625 = 1.909879207611084 + 100.0 * 8.560868263244629
Epoch 30, val loss: 1.9119174480438232
Epoch 40, training loss: 837.2080078125 = 1.8948886394500732 + 100.0 * 8.353131294250488
Epoch 40, val loss: 1.8964409828186035
Epoch 50, training loss: 770.2217407226562 = 1.8770400285720825 + 100.0 * 7.683447360992432
Epoch 50, val loss: 1.8779748678207397
Epoch 60, training loss: 757.4725341796875 = 1.860260009765625 + 100.0 * 7.55612325668335
Epoch 60, val loss: 1.8617032766342163
Epoch 70, training loss: 738.2796630859375 = 1.847495436668396 + 100.0 * 7.364321708679199
Epoch 70, val loss: 1.8491859436035156
Epoch 80, training loss: 717.723388671875 = 1.834944486618042 + 100.0 * 7.158884525299072
Epoch 80, val loss: 1.8367515802383423
Epoch 90, training loss: 704.810302734375 = 1.8238451480865479 + 100.0 * 7.029864311218262
Epoch 90, val loss: 1.8262720108032227
Epoch 100, training loss: 693.3094482421875 = 1.8141322135925293 + 100.0 * 6.914953231811523
Epoch 100, val loss: 1.8178738355636597
Epoch 110, training loss: 685.5192260742188 = 1.8049817085266113 + 100.0 * 6.837142467498779
Epoch 110, val loss: 1.8096380233764648
Epoch 120, training loss: 677.9705810546875 = 1.795857548713684 + 100.0 * 6.761747360229492
Epoch 120, val loss: 1.8014228343963623
Epoch 130, training loss: 672.8636474609375 = 1.7867754697799683 + 100.0 * 6.710768699645996
Epoch 130, val loss: 1.7931779623031616
Epoch 140, training loss: 668.886474609375 = 1.7767301797866821 + 100.0 * 6.671097278594971
Epoch 140, val loss: 1.7842035293579102
Epoch 150, training loss: 665.438720703125 = 1.765946865081787 + 100.0 * 6.636727809906006
Epoch 150, val loss: 1.7747000455856323
Epoch 160, training loss: 662.637451171875 = 1.7544118165969849 + 100.0 * 6.608830451965332
Epoch 160, val loss: 1.764696717262268
Epoch 170, training loss: 660.377197265625 = 1.7419703006744385 + 100.0 * 6.586352825164795
Epoch 170, val loss: 1.7541271448135376
Epoch 180, training loss: 658.709228515625 = 1.728355884552002 + 100.0 * 6.5698089599609375
Epoch 180, val loss: 1.7427654266357422
Epoch 190, training loss: 656.7188720703125 = 1.71320378780365 + 100.0 * 6.5500569343566895
Epoch 190, val loss: 1.7304229736328125
Epoch 200, training loss: 655.0421752929688 = 1.6968936920166016 + 100.0 * 6.53345251083374
Epoch 200, val loss: 1.7171331644058228
Epoch 210, training loss: 653.6651000976562 = 1.6791434288024902 + 100.0 * 6.519859790802002
Epoch 210, val loss: 1.7027941942214966
Epoch 220, training loss: 652.392578125 = 1.6598701477050781 + 100.0 * 6.507327556610107
Epoch 220, val loss: 1.6873208284378052
Epoch 230, training loss: 651.142822265625 = 1.6392436027526855 + 100.0 * 6.4950361251831055
Epoch 230, val loss: 1.670704960823059
Epoch 240, training loss: 649.6500244140625 = 1.6170872449874878 + 100.0 * 6.480329513549805
Epoch 240, val loss: 1.6530956029891968
Epoch 250, training loss: 648.4677734375 = 1.5936423540115356 + 100.0 * 6.468741416931152
Epoch 250, val loss: 1.6345160007476807
Epoch 260, training loss: 647.6275024414062 = 1.5686566829681396 + 100.0 * 6.460588455200195
Epoch 260, val loss: 1.614827036857605
Epoch 270, training loss: 646.7544555664062 = 1.542130708694458 + 100.0 * 6.452123641967773
Epoch 270, val loss: 1.5941203832626343
Epoch 280, training loss: 645.6343383789062 = 1.5143640041351318 + 100.0 * 6.441199779510498
Epoch 280, val loss: 1.5726574659347534
Epoch 290, training loss: 644.7545166015625 = 1.4857022762298584 + 100.0 * 6.432687759399414
Epoch 290, val loss: 1.5507125854492188
Epoch 300, training loss: 644.051513671875 = 1.4560494422912598 + 100.0 * 6.425954341888428
Epoch 300, val loss: 1.5282164812088013
Epoch 310, training loss: 643.3421630859375 = 1.425740361213684 + 100.0 * 6.419164657592773
Epoch 310, val loss: 1.5055054426193237
Epoch 320, training loss: 642.7832641601562 = 1.3946181535720825 + 100.0 * 6.413886547088623
Epoch 320, val loss: 1.482492446899414
Epoch 330, training loss: 641.9078369140625 = 1.3632893562316895 + 100.0 * 6.405445575714111
Epoch 330, val loss: 1.4596794843673706
Epoch 340, training loss: 641.130126953125 = 1.3317898511886597 + 100.0 * 6.397983551025391
Epoch 340, val loss: 1.4370354413986206
Epoch 350, training loss: 640.7373657226562 = 1.3002228736877441 + 100.0 * 6.394371032714844
Epoch 350, val loss: 1.4146502017974854
Epoch 360, training loss: 640.3673706054688 = 1.2683658599853516 + 100.0 * 6.390989780426025
Epoch 360, val loss: 1.3925951719284058
Epoch 370, training loss: 639.5773315429688 = 1.2370187044143677 + 100.0 * 6.383403301239014
Epoch 370, val loss: 1.37105393409729
Epoch 380, training loss: 639.0772705078125 = 1.2060465812683105 + 100.0 * 6.378712177276611
Epoch 380, val loss: 1.3503466844558716
Epoch 390, training loss: 638.7073364257812 = 1.1755733489990234 + 100.0 * 6.375317096710205
Epoch 390, val loss: 1.3304681777954102
Epoch 400, training loss: 638.5376586914062 = 1.145550012588501 + 100.0 * 6.373920917510986
Epoch 400, val loss: 1.311103105545044
Epoch 410, training loss: 637.8330688476562 = 1.1162604093551636 + 100.0 * 6.367167949676514
Epoch 410, val loss: 1.293013572692871
Epoch 420, training loss: 637.3629760742188 = 1.087708592414856 + 100.0 * 6.362752437591553
Epoch 420, val loss: 1.2757519483566284
Epoch 430, training loss: 637.219970703125 = 1.0600559711456299 + 100.0 * 6.361598968505859
Epoch 430, val loss: 1.259467363357544
Epoch 440, training loss: 637.4387817382812 = 1.033120036125183 + 100.0 * 6.364056587219238
Epoch 440, val loss: 1.2444263696670532
Epoch 450, training loss: 636.404052734375 = 1.0068447589874268 + 100.0 * 6.3539719581604
Epoch 450, val loss: 1.2299273014068604
Epoch 460, training loss: 636.0120239257812 = 0.9815583825111389 + 100.0 * 6.35030460357666
Epoch 460, val loss: 1.2164602279663086
Epoch 470, training loss: 635.704833984375 = 0.9571021199226379 + 100.0 * 6.347477436065674
Epoch 470, val loss: 1.2039103507995605
Epoch 480, training loss: 636.3916625976562 = 0.933523416519165 + 100.0 * 6.354581356048584
Epoch 480, val loss: 1.1921707391738892
Epoch 490, training loss: 635.0599365234375 = 0.9104371070861816 + 100.0 * 6.341494560241699
Epoch 490, val loss: 1.181384563446045
Epoch 500, training loss: 634.8475952148438 = 0.8882575035095215 + 100.0 * 6.339593410491943
Epoch 500, val loss: 1.1713675260543823
Epoch 510, training loss: 634.8131103515625 = 0.8668397068977356 + 100.0 * 6.339462757110596
Epoch 510, val loss: 1.162034034729004
Epoch 520, training loss: 634.8130493164062 = 0.8459643125534058 + 100.0 * 6.3396711349487305
Epoch 520, val loss: 1.1534862518310547
Epoch 530, training loss: 634.0675659179688 = 0.8257452845573425 + 100.0 * 6.332418441772461
Epoch 530, val loss: 1.1451655626296997
Epoch 540, training loss: 633.7023315429688 = 0.8062260746955872 + 100.0 * 6.328961372375488
Epoch 540, val loss: 1.1376066207885742
Epoch 550, training loss: 633.5503540039062 = 0.7872444987297058 + 100.0 * 6.327630996704102
Epoch 550, val loss: 1.1306220293045044
Epoch 560, training loss: 633.3077392578125 = 0.7687118649482727 + 100.0 * 6.325389862060547
Epoch 560, val loss: 1.1236659288406372
Epoch 570, training loss: 633.1759643554688 = 0.750670850276947 + 100.0 * 6.324253082275391
Epoch 570, val loss: 1.117564082145691
Epoch 580, training loss: 632.7789306640625 = 0.7331508994102478 + 100.0 * 6.320457458496094
Epoch 580, val loss: 1.1115087270736694
Epoch 590, training loss: 632.68505859375 = 0.7160046100616455 + 100.0 * 6.319690704345703
Epoch 590, val loss: 1.1057326793670654
Epoch 600, training loss: 632.4033813476562 = 0.6991856098175049 + 100.0 * 6.317042350769043
Epoch 600, val loss: 1.1004440784454346
Epoch 610, training loss: 633.490478515625 = 0.6825870871543884 + 100.0 * 6.328078746795654
Epoch 610, val loss: 1.0956497192382812
Epoch 620, training loss: 632.0250854492188 = 0.6663131713867188 + 100.0 * 6.313587665557861
Epoch 620, val loss: 1.0902918577194214
Epoch 630, training loss: 631.7125854492188 = 0.6503196954727173 + 100.0 * 6.310622692108154
Epoch 630, val loss: 1.0855680704116821
Epoch 640, training loss: 631.51611328125 = 0.634650468826294 + 100.0 * 6.308814525604248
Epoch 640, val loss: 1.081071376800537
Epoch 650, training loss: 632.6831665039062 = 0.6191014051437378 + 100.0 * 6.320640563964844
Epoch 650, val loss: 1.0768882036209106
Epoch 660, training loss: 631.39013671875 = 0.6037059426307678 + 100.0 * 6.307864665985107
Epoch 660, val loss: 1.072480320930481
Epoch 670, training loss: 630.99609375 = 0.5885152220726013 + 100.0 * 6.304075717926025
Epoch 670, val loss: 1.068281888961792
Epoch 680, training loss: 630.9044189453125 = 0.5735475420951843 + 100.0 * 6.303308963775635
Epoch 680, val loss: 1.0643342733383179
Epoch 690, training loss: 630.8934936523438 = 0.5586338639259338 + 100.0 * 6.303348541259766
Epoch 690, val loss: 1.0604630708694458
Epoch 700, training loss: 630.6935424804688 = 0.5438629984855652 + 100.0 * 6.301496505737305
Epoch 700, val loss: 1.0569270849227905
Epoch 710, training loss: 630.3856201171875 = 0.5292634963989258 + 100.0 * 6.2985639572143555
Epoch 710, val loss: 1.0536173582077026
Epoch 720, training loss: 630.3739013671875 = 0.5149080157279968 + 100.0 * 6.298590183258057
Epoch 720, val loss: 1.0503977537155151
Epoch 730, training loss: 630.2232666015625 = 0.5005635023117065 + 100.0 * 6.297227382659912
Epoch 730, val loss: 1.047463297843933
Epoch 740, training loss: 630.068359375 = 0.4864760935306549 + 100.0 * 6.29581880569458
Epoch 740, val loss: 1.0445271730422974
Epoch 750, training loss: 629.8197631835938 = 0.47262147068977356 + 100.0 * 6.293471813201904
Epoch 750, val loss: 1.0422134399414062
Epoch 760, training loss: 629.6311645507812 = 0.4590073227882385 + 100.0 * 6.291721820831299
Epoch 760, val loss: 1.0400320291519165
Epoch 770, training loss: 629.4817504882812 = 0.4456566870212555 + 100.0 * 6.290360450744629
Epoch 770, val loss: 1.0381381511688232
Epoch 780, training loss: 630.0147094726562 = 0.4325304329395294 + 100.0 * 6.295821666717529
Epoch 780, val loss: 1.0363726615905762
Epoch 790, training loss: 629.8294677734375 = 0.4195620119571686 + 100.0 * 6.2940993309021
Epoch 790, val loss: 1.035507082939148
Epoch 800, training loss: 629.2295532226562 = 0.40696480870246887 + 100.0 * 6.2882256507873535
Epoch 800, val loss: 1.034647822380066
Epoch 810, training loss: 628.9985961914062 = 0.3947499990463257 + 100.0 * 6.286038398742676
Epoch 810, val loss: 1.0344146490097046
Epoch 820, training loss: 628.8712158203125 = 0.3828966021537781 + 100.0 * 6.284883499145508
Epoch 820, val loss: 1.0347083806991577
Epoch 830, training loss: 629.2691650390625 = 0.37135574221611023 + 100.0 * 6.288978099822998
Epoch 830, val loss: 1.0357469320297241
Epoch 840, training loss: 629.3322143554688 = 0.3601101040840149 + 100.0 * 6.2897210121154785
Epoch 840, val loss: 1.0366384983062744
Epoch 850, training loss: 628.6181640625 = 0.3491596579551697 + 100.0 * 6.282689571380615
Epoch 850, val loss: 1.0379735231399536
Epoch 860, training loss: 628.5772705078125 = 0.33863961696624756 + 100.0 * 6.282386779785156
Epoch 860, val loss: 1.0399487018585205
Epoch 870, training loss: 628.3353271484375 = 0.32849955558776855 + 100.0 * 6.280068397521973
Epoch 870, val loss: 1.0425888299942017
Epoch 880, training loss: 628.5281982421875 = 0.3187003433704376 + 100.0 * 6.282094955444336
Epoch 880, val loss: 1.0455210208892822
Epoch 890, training loss: 628.336181640625 = 0.30918940901756287 + 100.0 * 6.280269622802734
Epoch 890, val loss: 1.0488932132720947
Epoch 900, training loss: 628.1298217773438 = 0.2999972105026245 + 100.0 * 6.278298377990723
Epoch 900, val loss: 1.0520354509353638
Epoch 910, training loss: 628.2359619140625 = 0.2911669909954071 + 100.0 * 6.27944803237915
Epoch 910, val loss: 1.0560593605041504
Epoch 920, training loss: 627.9574584960938 = 0.2825908660888672 + 100.0 * 6.2767486572265625
Epoch 920, val loss: 1.0602707862854004
Epoch 930, training loss: 627.787841796875 = 0.27434131503105164 + 100.0 * 6.275135040283203
Epoch 930, val loss: 1.0645344257354736
Epoch 940, training loss: 627.66455078125 = 0.26640626788139343 + 100.0 * 6.273981094360352
Epoch 940, val loss: 1.0694329738616943
Epoch 950, training loss: 627.8785400390625 = 0.2587568163871765 + 100.0 * 6.276198387145996
Epoch 950, val loss: 1.0740671157836914
Epoch 960, training loss: 627.7020263671875 = 0.2513284981250763 + 100.0 * 6.27450704574585
Epoch 960, val loss: 1.079666256904602
Epoch 970, training loss: 627.4950561523438 = 0.24409611523151398 + 100.0 * 6.272510051727295
Epoch 970, val loss: 1.0853278636932373
Epoch 980, training loss: 627.3349609375 = 0.2371673285961151 + 100.0 * 6.270977973937988
Epoch 980, val loss: 1.0912283658981323
Epoch 990, training loss: 627.4536743164062 = 0.2304927110671997 + 100.0 * 6.2722320556640625
Epoch 990, val loss: 1.0968360900878906
Epoch 1000, training loss: 627.357666015625 = 0.22398264706134796 + 100.0 * 6.271336555480957
Epoch 1000, val loss: 1.103479266166687
Epoch 1010, training loss: 627.2377319335938 = 0.21771816909313202 + 100.0 * 6.270200252532959
Epoch 1010, val loss: 1.109589695930481
Epoch 1020, training loss: 627.3781127929688 = 0.21165794134140015 + 100.0 * 6.271664619445801
Epoch 1020, val loss: 1.115779161453247
Epoch 1030, training loss: 626.9727783203125 = 0.2057514786720276 + 100.0 * 6.267670154571533
Epoch 1030, val loss: 1.122670292854309
Epoch 1040, training loss: 626.9268188476562 = 0.200067400932312 + 100.0 * 6.267267227172852
Epoch 1040, val loss: 1.129693627357483
Epoch 1050, training loss: 626.7933959960938 = 0.19459551572799683 + 100.0 * 6.265988349914551
Epoch 1050, val loss: 1.1362334489822388
Epoch 1060, training loss: 627.7117309570312 = 0.18928809463977814 + 100.0 * 6.275224685668945
Epoch 1060, val loss: 1.142956256866455
Epoch 1070, training loss: 626.8726806640625 = 0.18410947918891907 + 100.0 * 6.266886234283447
Epoch 1070, val loss: 1.150542140007019
Epoch 1080, training loss: 626.5568237304688 = 0.17909005284309387 + 100.0 * 6.263777732849121
Epoch 1080, val loss: 1.157599687576294
Epoch 1090, training loss: 626.4871826171875 = 0.17427118122577667 + 100.0 * 6.263129234313965
Epoch 1090, val loss: 1.1649677753448486
Epoch 1100, training loss: 626.534912109375 = 0.16961872577667236 + 100.0 * 6.263652801513672
Epoch 1100, val loss: 1.1724205017089844
Epoch 1110, training loss: 626.4544677734375 = 0.1650678664445877 + 100.0 * 6.262894153594971
Epoch 1110, val loss: 1.1798443794250488
Epoch 1120, training loss: 626.27880859375 = 0.16067057847976685 + 100.0 * 6.261181831359863
Epoch 1120, val loss: 1.1873832941055298
Epoch 1130, training loss: 626.4625244140625 = 0.15642088651657104 + 100.0 * 6.263061046600342
Epoch 1130, val loss: 1.195479393005371
Epoch 1140, training loss: 626.8807983398438 = 0.1522560715675354 + 100.0 * 6.267285346984863
Epoch 1140, val loss: 1.2020633220672607
Epoch 1150, training loss: 626.44677734375 = 0.1482192426919937 + 100.0 * 6.262985706329346
Epoch 1150, val loss: 1.2097067832946777
Epoch 1160, training loss: 626.1116333007812 = 0.14433228969573975 + 100.0 * 6.259673118591309
Epoch 1160, val loss: 1.21774160861969
Epoch 1170, training loss: 625.9406127929688 = 0.1405949741601944 + 100.0 * 6.257999897003174
Epoch 1170, val loss: 1.224873661994934
Epoch 1180, training loss: 625.866455078125 = 0.1369730830192566 + 100.0 * 6.257294654846191
Epoch 1180, val loss: 1.232667088508606
Epoch 1190, training loss: 626.3255004882812 = 0.13346867263317108 + 100.0 * 6.26192045211792
Epoch 1190, val loss: 1.2400872707366943
Epoch 1200, training loss: 626.1066284179688 = 0.12998312711715698 + 100.0 * 6.259766101837158
Epoch 1200, val loss: 1.2480652332305908
Epoch 1210, training loss: 625.9219970703125 = 0.12663859128952026 + 100.0 * 6.257953643798828
Epoch 1210, val loss: 1.2555683851242065
Epoch 1220, training loss: 625.6390380859375 = 0.12341324239969254 + 100.0 * 6.25515604019165
Epoch 1220, val loss: 1.2633860111236572
Epoch 1230, training loss: 625.6489868164062 = 0.12029601633548737 + 100.0 * 6.255286693572998
Epoch 1230, val loss: 1.2710659503936768
Epoch 1240, training loss: 626.8458251953125 = 0.11729142814874649 + 100.0 * 6.267285346984863
Epoch 1240, val loss: 1.2780044078826904
Epoch 1250, training loss: 625.7268676757812 = 0.11427118629217148 + 100.0 * 6.2561259269714355
Epoch 1250, val loss: 1.286681890487671
Epoch 1260, training loss: 625.4963989257812 = 0.11138059198856354 + 100.0 * 6.253849983215332
Epoch 1260, val loss: 1.2949455976486206
Epoch 1270, training loss: 625.427978515625 = 0.10862027108669281 + 100.0 * 6.253193378448486
Epoch 1270, val loss: 1.302046537399292
Epoch 1280, training loss: 625.3394165039062 = 0.10594774782657623 + 100.0 * 6.2523345947265625
Epoch 1280, val loss: 1.3104037046432495
Epoch 1290, training loss: 625.4754028320312 = 0.10335955023765564 + 100.0 * 6.253720760345459
Epoch 1290, val loss: 1.3178374767303467
Epoch 1300, training loss: 625.2643432617188 = 0.10077743232250214 + 100.0 * 6.251635551452637
Epoch 1300, val loss: 1.3260517120361328
Epoch 1310, training loss: 625.4285278320312 = 0.09828672558069229 + 100.0 * 6.253302574157715
Epoch 1310, val loss: 1.3335272073745728
Epoch 1320, training loss: 625.2742919921875 = 0.09588535130023956 + 100.0 * 6.251784324645996
Epoch 1320, val loss: 1.3416754007339478
Epoch 1330, training loss: 625.3970947265625 = 0.09357582777738571 + 100.0 * 6.253035068511963
Epoch 1330, val loss: 1.349594235420227
Epoch 1340, training loss: 625.0667724609375 = 0.09130923449993134 + 100.0 * 6.249754428863525
Epoch 1340, val loss: 1.356809139251709
Epoch 1350, training loss: 625.2636108398438 = 0.08911741524934769 + 100.0 * 6.251745223999023
Epoch 1350, val loss: 1.3639825582504272
Epoch 1360, training loss: 625.4266967773438 = 0.08698659390211105 + 100.0 * 6.253396987915039
Epoch 1360, val loss: 1.3721588850021362
Epoch 1370, training loss: 625.049560546875 = 0.08489788323640823 + 100.0 * 6.2496466636657715
Epoch 1370, val loss: 1.3801848888397217
Epoch 1380, training loss: 624.9999389648438 = 0.08288636058568954 + 100.0 * 6.249170303344727
Epoch 1380, val loss: 1.3876553773880005
Epoch 1390, training loss: 625.3085327148438 = 0.08094822615385056 + 100.0 * 6.2522759437561035
Epoch 1390, val loss: 1.3949588537216187
Epoch 1400, training loss: 624.9058227539062 = 0.07902657985687256 + 100.0 * 6.248267650604248
Epoch 1400, val loss: 1.4028410911560059
Epoch 1410, training loss: 625.0579223632812 = 0.07719480246305466 + 100.0 * 6.249807357788086
Epoch 1410, val loss: 1.4101568460464478
Epoch 1420, training loss: 624.8685913085938 = 0.07538057863712311 + 100.0 * 6.247931957244873
Epoch 1420, val loss: 1.4182847738265991
Epoch 1430, training loss: 624.73779296875 = 0.07363845407962799 + 100.0 * 6.246641635894775
Epoch 1430, val loss: 1.426154613494873
Epoch 1440, training loss: 624.7080078125 = 0.07195628434419632 + 100.0 * 6.2463603019714355
Epoch 1440, val loss: 1.433445930480957
Epoch 1450, training loss: 624.7682495117188 = 0.07032245397567749 + 100.0 * 6.246979236602783
Epoch 1450, val loss: 1.4416122436523438
Epoch 1460, training loss: 624.8472290039062 = 0.06872649490833282 + 100.0 * 6.2477850914001465
Epoch 1460, val loss: 1.4490314722061157
Epoch 1470, training loss: 624.7228393554688 = 0.0671619102358818 + 100.0 * 6.246556758880615
Epoch 1470, val loss: 1.4558111429214478
Epoch 1480, training loss: 624.5823364257812 = 0.06565351039171219 + 100.0 * 6.245166778564453
Epoch 1480, val loss: 1.4636653661727905
Epoch 1490, training loss: 624.681884765625 = 0.0641961619257927 + 100.0 * 6.246176719665527
Epoch 1490, val loss: 1.470832347869873
Epoch 1500, training loss: 624.462158203125 = 0.06276987493038177 + 100.0 * 6.243994235992432
Epoch 1500, val loss: 1.4784530401229858
Epoch 1510, training loss: 624.53076171875 = 0.06138833612203598 + 100.0 * 6.244693756103516
Epoch 1510, val loss: 1.485763669013977
Epoch 1520, training loss: 624.8825073242188 = 0.06003395840525627 + 100.0 * 6.248224258422852
Epoch 1520, val loss: 1.4922527074813843
Epoch 1530, training loss: 624.5099487304688 = 0.058718472719192505 + 100.0 * 6.24451208114624
Epoch 1530, val loss: 1.5007487535476685
Epoch 1540, training loss: 624.3179931640625 = 0.05744175985455513 + 100.0 * 6.242605686187744
Epoch 1540, val loss: 1.5077985525131226
Epoch 1550, training loss: 624.6953125 = 0.05620859935879707 + 100.0 * 6.2463908195495605
Epoch 1550, val loss: 1.5161758661270142
Epoch 1560, training loss: 624.5714721679688 = 0.05499394237995148 + 100.0 * 6.24516487121582
Epoch 1560, val loss: 1.522581696510315
Epoch 1570, training loss: 624.219970703125 = 0.05380554869771004 + 100.0 * 6.241661548614502
Epoch 1570, val loss: 1.529723882675171
Epoch 1580, training loss: 624.14111328125 = 0.05267034471035004 + 100.0 * 6.240884304046631
Epoch 1580, val loss: 1.5368505716323853
Epoch 1590, training loss: 624.1093139648438 = 0.05157224088907242 + 100.0 * 6.240577697753906
Epoch 1590, val loss: 1.5440362691879272
Epoch 1600, training loss: 624.7737426757812 = 0.05052148550748825 + 100.0 * 6.247231960296631
Epoch 1600, val loss: 1.550931453704834
Epoch 1610, training loss: 624.298583984375 = 0.049434367567300797 + 100.0 * 6.242491722106934
Epoch 1610, val loss: 1.558894157409668
Epoch 1620, training loss: 624.0972290039062 = 0.04841352254152298 + 100.0 * 6.240488052368164
Epoch 1620, val loss: 1.5656611919403076
Epoch 1630, training loss: 623.9933471679688 = 0.04741036519408226 + 100.0 * 6.23945951461792
Epoch 1630, val loss: 1.573226809501648
Epoch 1640, training loss: 624.547607421875 = 0.04645711928606033 + 100.0 * 6.245011806488037
Epoch 1640, val loss: 1.5807185173034668
Epoch 1650, training loss: 624.1072387695312 = 0.045494094491004944 + 100.0 * 6.240617752075195
Epoch 1650, val loss: 1.5861927270889282
Epoch 1660, training loss: 624.142578125 = 0.04458177089691162 + 100.0 * 6.24098014831543
Epoch 1660, val loss: 1.5944938659667969
Epoch 1670, training loss: 624.061279296875 = 0.043678414076566696 + 100.0 * 6.240175724029541
Epoch 1670, val loss: 1.6002146005630493
Epoch 1680, training loss: 624.268310546875 = 0.04280403256416321 + 100.0 * 6.242255210876465
Epoch 1680, val loss: 1.606887936592102
Epoch 1690, training loss: 623.872314453125 = 0.04194783791899681 + 100.0 * 6.2383036613464355
Epoch 1690, val loss: 1.614436149597168
Epoch 1700, training loss: 623.7481079101562 = 0.0411197654902935 + 100.0 * 6.237069606781006
Epoch 1700, val loss: 1.6211016178131104
Epoch 1710, training loss: 624.2049560546875 = 0.04031665623188019 + 100.0 * 6.2416462898254395
Epoch 1710, val loss: 1.628022313117981
Epoch 1720, training loss: 623.8927001953125 = 0.03951997309923172 + 100.0 * 6.238532066345215
Epoch 1720, val loss: 1.634717583656311
Epoch 1730, training loss: 623.7898559570312 = 0.03874216228723526 + 100.0 * 6.237510681152344
Epoch 1730, val loss: 1.640854001045227
Epoch 1740, training loss: 623.9190673828125 = 0.03800464794039726 + 100.0 * 6.2388105392456055
Epoch 1740, val loss: 1.6474710702896118
Epoch 1750, training loss: 623.760498046875 = 0.037264563143253326 + 100.0 * 6.237232208251953
Epoch 1750, val loss: 1.6544837951660156
Epoch 1760, training loss: 623.7601318359375 = 0.0365515872836113 + 100.0 * 6.2372355461120605
Epoch 1760, val loss: 1.6610679626464844
Epoch 1770, training loss: 623.7619018554688 = 0.03586278110742569 + 100.0 * 6.237259864807129
Epoch 1770, val loss: 1.6671090126037598
Epoch 1780, training loss: 623.76123046875 = 0.0351875014603138 + 100.0 * 6.237259864807129
Epoch 1780, val loss: 1.6734973192214966
Epoch 1790, training loss: 623.6646118164062 = 0.03452329710125923 + 100.0 * 6.236300468444824
Epoch 1790, val loss: 1.6802442073822021
Epoch 1800, training loss: 623.6763305664062 = 0.033892709761857986 + 100.0 * 6.236424446105957
Epoch 1800, val loss: 1.6863620281219482
Epoch 1810, training loss: 623.7244873046875 = 0.03325954079627991 + 100.0 * 6.236912250518799
Epoch 1810, val loss: 1.6927015781402588
Epoch 1820, training loss: 623.5325927734375 = 0.03265288472175598 + 100.0 * 6.234999656677246
Epoch 1820, val loss: 1.7000969648361206
Epoch 1830, training loss: 623.5333251953125 = 0.032057687640190125 + 100.0 * 6.235013008117676
Epoch 1830, val loss: 1.7069175243377686
Epoch 1840, training loss: 623.9022216796875 = 0.03147733584046364 + 100.0 * 6.238707542419434
Epoch 1840, val loss: 1.7127867937088013
Epoch 1850, training loss: 623.5579223632812 = 0.030911313369870186 + 100.0 * 6.2352705001831055
Epoch 1850, val loss: 1.7184661626815796
Epoch 1860, training loss: 623.9344482421875 = 0.03035719506442547 + 100.0 * 6.239041328430176
Epoch 1860, val loss: 1.7248315811157227
Epoch 1870, training loss: 623.38525390625 = 0.02981959469616413 + 100.0 * 6.233553886413574
Epoch 1870, val loss: 1.731034755706787
Epoch 1880, training loss: 623.2408447265625 = 0.029293309897184372 + 100.0 * 6.232115745544434
Epoch 1880, val loss: 1.7375130653381348
Epoch 1890, training loss: 623.2324829101562 = 0.028787897899746895 + 100.0 * 6.232036590576172
Epoch 1890, val loss: 1.743546724319458
Epoch 1900, training loss: 623.6394653320312 = 0.0283013004809618 + 100.0 * 6.236111640930176
Epoch 1900, val loss: 1.7491792440414429
Epoch 1910, training loss: 623.31298828125 = 0.027801943942904472 + 100.0 * 6.232851505279541
Epoch 1910, val loss: 1.7557464838027954
Epoch 1920, training loss: 623.2213134765625 = 0.027325810864567757 + 100.0 * 6.231939792633057
Epoch 1920, val loss: 1.7617318630218506
Epoch 1930, training loss: 623.6942749023438 = 0.02686600759625435 + 100.0 * 6.2366743087768555
Epoch 1930, val loss: 1.7661641836166382
Epoch 1940, training loss: 623.1190185546875 = 0.02640192024409771 + 100.0 * 6.230926036834717
Epoch 1940, val loss: 1.774289608001709
Epoch 1950, training loss: 623.0217895507812 = 0.025955917313694954 + 100.0 * 6.229958534240723
Epoch 1950, val loss: 1.7792468070983887
Epoch 1960, training loss: 622.9981689453125 = 0.025529995560646057 + 100.0 * 6.229726314544678
Epoch 1960, val loss: 1.785867691040039
Epoch 1970, training loss: 623.4993896484375 = 0.025128420442342758 + 100.0 * 6.234742164611816
Epoch 1970, val loss: 1.791450023651123
Epoch 1980, training loss: 623.0045166015625 = 0.02469651587307453 + 100.0 * 6.229797840118408
Epoch 1980, val loss: 1.7967387437820435
Epoch 1990, training loss: 623.10791015625 = 0.024294210597872734 + 100.0 * 6.230836391448975
Epoch 1990, val loss: 1.8026155233383179
Epoch 2000, training loss: 623.031005859375 = 0.023900842294096947 + 100.0 * 6.230071067810059
Epoch 2000, val loss: 1.8084359169006348
Epoch 2010, training loss: 622.9728393554688 = 0.023514900356531143 + 100.0 * 6.229493618011475
Epoch 2010, val loss: 1.8144135475158691
Epoch 2020, training loss: 623.5162963867188 = 0.023143328726291656 + 100.0 * 6.234931468963623
Epoch 2020, val loss: 1.8206391334533691
Epoch 2030, training loss: 622.9790649414062 = 0.02276676334440708 + 100.0 * 6.229562759399414
Epoch 2030, val loss: 1.824540376663208
Epoch 2040, training loss: 622.8681030273438 = 0.022408684715628624 + 100.0 * 6.228456974029541
Epoch 2040, val loss: 1.8310117721557617
Epoch 2050, training loss: 623.4703369140625 = 0.022063948214054108 + 100.0 * 6.234483242034912
Epoch 2050, val loss: 1.8368982076644897
Epoch 2060, training loss: 622.8605346679688 = 0.021711980924010277 + 100.0 * 6.228387832641602
Epoch 2060, val loss: 1.8412978649139404
Epoch 2070, training loss: 623.1735229492188 = 0.021378053352236748 + 100.0 * 6.2315216064453125
Epoch 2070, val loss: 1.8473315238952637
Epoch 2080, training loss: 622.8675537109375 = 0.021039651706814766 + 100.0 * 6.2284650802612305
Epoch 2080, val loss: 1.8525007963180542
Epoch 2090, training loss: 622.8152465820312 = 0.020719477906823158 + 100.0 * 6.227945804595947
Epoch 2090, val loss: 1.8575726747512817
Epoch 2100, training loss: 622.6985473632812 = 0.020412452518939972 + 100.0 * 6.226781845092773
Epoch 2100, val loss: 1.8631094694137573
Epoch 2110, training loss: 622.9411010742188 = 0.020115744322538376 + 100.0 * 6.2292094230651855
Epoch 2110, val loss: 1.867985486984253
Epoch 2120, training loss: 622.9866333007812 = 0.019811391830444336 + 100.0 * 6.229668140411377
Epoch 2120, val loss: 1.873365044593811
Epoch 2130, training loss: 622.8497314453125 = 0.019510900601744652 + 100.0 * 6.228302001953125
Epoch 2130, val loss: 1.879328966140747
Epoch 2140, training loss: 623.3792114257812 = 0.01923549547791481 + 100.0 * 6.233599662780762
Epoch 2140, val loss: 1.884969711303711
Epoch 2150, training loss: 622.81103515625 = 0.01893778331577778 + 100.0 * 6.227921009063721
Epoch 2150, val loss: 1.8887461423873901
Epoch 2160, training loss: 622.711669921875 = 0.01866576261818409 + 100.0 * 6.226929664611816
Epoch 2160, val loss: 1.8946902751922607
Epoch 2170, training loss: 622.95849609375 = 0.018400365486741066 + 100.0 * 6.229401111602783
Epoch 2170, val loss: 1.900028944015503
Epoch 2180, training loss: 622.5913696289062 = 0.018135610967874527 + 100.0 * 6.225732326507568
Epoch 2180, val loss: 1.9042229652404785
Epoch 2190, training loss: 622.587890625 = 0.01787523552775383 + 100.0 * 6.2256999015808105
Epoch 2190, val loss: 1.9087408781051636
Epoch 2200, training loss: 622.6502075195312 = 0.017627757042646408 + 100.0 * 6.226325988769531
Epoch 2200, val loss: 1.9138215780258179
Epoch 2210, training loss: 622.9144287109375 = 0.017382200807332993 + 100.0 * 6.228970050811768
Epoch 2210, val loss: 1.9188638925552368
Epoch 2220, training loss: 622.7185668945312 = 0.017133554443717003 + 100.0 * 6.227014064788818
Epoch 2220, val loss: 1.924072027206421
Epoch 2230, training loss: 622.49755859375 = 0.01689804531633854 + 100.0 * 6.224806785583496
Epoch 2230, val loss: 1.9291197061538696
Epoch 2240, training loss: 622.7435913085938 = 0.016673199832439423 + 100.0 * 6.227269172668457
Epoch 2240, val loss: 1.9346959590911865
Epoch 2250, training loss: 622.5232543945312 = 0.016441991552710533 + 100.0 * 6.225068092346191
Epoch 2250, val loss: 1.938023328781128
Epoch 2260, training loss: 622.7635498046875 = 0.01622035913169384 + 100.0 * 6.227473258972168
Epoch 2260, val loss: 1.9421504735946655
Epoch 2270, training loss: 622.4254150390625 = 0.015997856855392456 + 100.0 * 6.224094390869141
Epoch 2270, val loss: 1.947749137878418
Epoch 2280, training loss: 622.6378784179688 = 0.015787867829203606 + 100.0 * 6.226220607757568
Epoch 2280, val loss: 1.9522513151168823
Epoch 2290, training loss: 622.3681640625 = 0.015574858523905277 + 100.0 * 6.2235260009765625
Epoch 2290, val loss: 1.9570072889328003
Epoch 2300, training loss: 622.3759155273438 = 0.015371648594737053 + 100.0 * 6.223605155944824
Epoch 2300, val loss: 1.9613929986953735
Epoch 2310, training loss: 622.2933959960938 = 0.015171992592513561 + 100.0 * 6.222782135009766
Epoch 2310, val loss: 1.9657431840896606
Epoch 2320, training loss: 623.1047973632812 = 0.01498277485370636 + 100.0 * 6.230898380279541
Epoch 2320, val loss: 1.970490574836731
Epoch 2330, training loss: 622.4869384765625 = 0.014781512320041656 + 100.0 * 6.224721908569336
Epoch 2330, val loss: 1.9757875204086304
Epoch 2340, training loss: 622.6422729492188 = 0.014589999802410603 + 100.0 * 6.226276874542236
Epoch 2340, val loss: 1.9795204401016235
Epoch 2350, training loss: 622.1888427734375 = 0.01440400443971157 + 100.0 * 6.221744537353516
Epoch 2350, val loss: 1.9841758012771606
Epoch 2360, training loss: 622.2376708984375 = 0.014228036627173424 + 100.0 * 6.22223424911499
Epoch 2360, val loss: 1.9879645109176636
Epoch 2370, training loss: 622.4405517578125 = 0.014054481871426105 + 100.0 * 6.224265098571777
Epoch 2370, val loss: 1.9919153451919556
Epoch 2380, training loss: 622.4642333984375 = 0.01387239433825016 + 100.0 * 6.224503993988037
Epoch 2380, val loss: 1.996835708618164
Epoch 2390, training loss: 622.5155639648438 = 0.013699890114367008 + 100.0 * 6.2250189781188965
Epoch 2390, val loss: 2.00042986869812
Epoch 2400, training loss: 622.1218872070312 = 0.013527754694223404 + 100.0 * 6.221083641052246
Epoch 2400, val loss: 2.005709171295166
Epoch 2410, training loss: 622.126953125 = 0.01336596254259348 + 100.0 * 6.22113561630249
Epoch 2410, val loss: 2.0103676319122314
Epoch 2420, training loss: 622.1104125976562 = 0.0132066048681736 + 100.0 * 6.220972061157227
Epoch 2420, val loss: 2.013991117477417
Epoch 2430, training loss: 622.8380737304688 = 0.013055416755378246 + 100.0 * 6.228250026702881
Epoch 2430, val loss: 2.018080711364746
Epoch 2440, training loss: 622.2511596679688 = 0.012891518883407116 + 100.0 * 6.222382545471191
Epoch 2440, val loss: 2.0222883224487305
Epoch 2450, training loss: 622.0117797851562 = 0.012737777084112167 + 100.0 * 6.219990253448486
Epoch 2450, val loss: 2.0265135765075684
Epoch 2460, training loss: 621.981201171875 = 0.012590227648615837 + 100.0 * 6.219686031341553
Epoch 2460, val loss: 2.0309994220733643
Epoch 2470, training loss: 622.7054443359375 = 0.012454690411686897 + 100.0 * 6.226929664611816
Epoch 2470, val loss: 2.035952091217041
Epoch 2480, training loss: 622.121826171875 = 0.012296776287257671 + 100.0 * 6.221095561981201
Epoch 2480, val loss: 2.03829288482666
Epoch 2490, training loss: 621.9507446289062 = 0.012152514420449734 + 100.0 * 6.219386100769043
Epoch 2490, val loss: 2.0431320667266846
Epoch 2500, training loss: 621.8895263671875 = 0.012014833278954029 + 100.0 * 6.218775272369385
Epoch 2500, val loss: 2.046799421310425
Epoch 2510, training loss: 622.0148315429688 = 0.011884741485118866 + 100.0 * 6.220029354095459
Epoch 2510, val loss: 2.0502476692199707
Epoch 2520, training loss: 622.5965576171875 = 0.011752358637750149 + 100.0 * 6.2258477210998535
Epoch 2520, val loss: 2.054157018661499
Epoch 2530, training loss: 621.9996337890625 = 0.01160995289683342 + 100.0 * 6.219880104064941
Epoch 2530, val loss: 2.0586018562316895
Epoch 2540, training loss: 621.90380859375 = 0.011481903493404388 + 100.0 * 6.218923091888428
Epoch 2540, val loss: 2.0621140003204346
Epoch 2550, training loss: 622.094482421875 = 0.011358632706105709 + 100.0 * 6.220831394195557
Epoch 2550, val loss: 2.0657761096954346
Epoch 2560, training loss: 621.8652954101562 = 0.011232786811888218 + 100.0 * 6.218540668487549
Epoch 2560, val loss: 2.0700368881225586
Epoch 2570, training loss: 621.8048095703125 = 0.011112352833151817 + 100.0 * 6.217936992645264
Epoch 2570, val loss: 2.074282646179199
Epoch 2580, training loss: 622.1322021484375 = 0.010997305624186993 + 100.0 * 6.221212387084961
Epoch 2580, val loss: 2.0785653591156006
Epoch 2590, training loss: 621.8231811523438 = 0.010876181535422802 + 100.0 * 6.218122959136963
Epoch 2590, val loss: 2.081148147583008
Epoch 2600, training loss: 622.4124755859375 = 0.010763934813439846 + 100.0 * 6.224017143249512
Epoch 2600, val loss: 2.08479905128479
Epoch 2610, training loss: 622.1455688476562 = 0.010641568340361118 + 100.0 * 6.221349716186523
Epoch 2610, val loss: 2.088737726211548
Epoch 2620, training loss: 621.7899780273438 = 0.010526212863624096 + 100.0 * 6.217794895172119
Epoch 2620, val loss: 2.0926461219787598
Epoch 2630, training loss: 621.7460327148438 = 0.010420159436762333 + 100.0 * 6.217355728149414
Epoch 2630, val loss: 2.096734046936035
Epoch 2640, training loss: 621.8113403320312 = 0.010313667356967926 + 100.0 * 6.218010425567627
Epoch 2640, val loss: 2.1002464294433594
Epoch 2650, training loss: 622.0867309570312 = 0.010209806263446808 + 100.0 * 6.220765590667725
Epoch 2650, val loss: 2.104372501373291
Epoch 2660, training loss: 621.9606323242188 = 0.010102423839271069 + 100.0 * 6.2195048332214355
Epoch 2660, val loss: 2.106891632080078
Epoch 2670, training loss: 621.7821655273438 = 0.00999614130705595 + 100.0 * 6.217721462249756
Epoch 2670, val loss: 2.1108555793762207
Epoch 2680, training loss: 621.6168823242188 = 0.009895061142742634 + 100.0 * 6.21606969833374
Epoch 2680, val loss: 2.1137936115264893
Epoch 2690, training loss: 621.6655883789062 = 0.009796451777219772 + 100.0 * 6.216557502746582
Epoch 2690, val loss: 2.117044687271118
Epoch 2700, training loss: 621.6765747070312 = 0.009701002389192581 + 100.0 * 6.216668605804443
Epoch 2700, val loss: 2.120368480682373
Epoch 2710, training loss: 622.2493896484375 = 0.009605291299521923 + 100.0 * 6.222397804260254
Epoch 2710, val loss: 2.1235854625701904
Epoch 2720, training loss: 622.0723266601562 = 0.009510277770459652 + 100.0 * 6.220627784729004
Epoch 2720, val loss: 2.12882661819458
Epoch 2730, training loss: 621.7661743164062 = 0.009411295875906944 + 100.0 * 6.2175679206848145
Epoch 2730, val loss: 2.130241870880127
Epoch 2740, training loss: 621.6541137695312 = 0.009320339187979698 + 100.0 * 6.216447830200195
Epoch 2740, val loss: 2.1349570751190186
Epoch 2750, training loss: 621.7544555664062 = 0.009232625365257263 + 100.0 * 6.217452526092529
Epoch 2750, val loss: 2.1378300189971924
Epoch 2760, training loss: 621.6181030273438 = 0.009143534116446972 + 100.0 * 6.216089725494385
Epoch 2760, val loss: 2.141421318054199
Epoch 2770, training loss: 621.9627685546875 = 0.009062379598617554 + 100.0 * 6.219537258148193
Epoch 2770, val loss: 2.14399790763855
Epoch 2780, training loss: 621.5237426757812 = 0.008971921168267727 + 100.0 * 6.215147972106934
Epoch 2780, val loss: 2.147860050201416
Epoch 2790, training loss: 621.489013671875 = 0.00888547021895647 + 100.0 * 6.214800834655762
Epoch 2790, val loss: 2.150792360305786
Epoch 2800, training loss: 621.7096557617188 = 0.008806617930531502 + 100.0 * 6.217008590698242
Epoch 2800, val loss: 2.1547348499298096
Epoch 2810, training loss: 621.5577392578125 = 0.008723754435777664 + 100.0 * 6.215489864349365
Epoch 2810, val loss: 2.157475709915161
Epoch 2820, training loss: 621.5538940429688 = 0.008643003180623055 + 100.0 * 6.215452194213867
Epoch 2820, val loss: 2.15997576713562
Epoch 2830, training loss: 621.5410766601562 = 0.008565852418541908 + 100.0 * 6.215325355529785
Epoch 2830, val loss: 2.163950204849243
Epoch 2840, training loss: 621.7974853515625 = 0.008488998748362064 + 100.0 * 6.21789026260376
Epoch 2840, val loss: 2.1668286323547363
Epoch 2850, training loss: 621.5669555664062 = 0.008411498740315437 + 100.0 * 6.215585231781006
Epoch 2850, val loss: 2.168999671936035
Epoch 2860, training loss: 621.5556030273438 = 0.008335544727742672 + 100.0 * 6.21547269821167
Epoch 2860, val loss: 2.1722326278686523
Epoch 2870, training loss: 621.5082397460938 = 0.008262517862021923 + 100.0 * 6.215000152587891
Epoch 2870, val loss: 2.175504684448242
Epoch 2880, training loss: 621.6708374023438 = 0.00819140113890171 + 100.0 * 6.2166266441345215
Epoch 2880, val loss: 2.178539514541626
Epoch 2890, training loss: 621.4857788085938 = 0.00811859406530857 + 100.0 * 6.214776515960693
Epoch 2890, val loss: 2.1817739009857178
Epoch 2900, training loss: 621.5646362304688 = 0.008047846145927906 + 100.0 * 6.2155656814575195
Epoch 2900, val loss: 2.184866189956665
Epoch 2910, training loss: 621.5093994140625 = 0.007977240718901157 + 100.0 * 6.2150139808654785
Epoch 2910, val loss: 2.1869301795959473
Epoch 2920, training loss: 621.4697265625 = 0.007908833213150501 + 100.0 * 6.214617729187012
Epoch 2920, val loss: 2.189688205718994
Epoch 2930, training loss: 621.30029296875 = 0.007841364480555058 + 100.0 * 6.212924957275391
Epoch 2930, val loss: 2.1931393146514893
Epoch 2940, training loss: 621.5140380859375 = 0.007776092737913132 + 100.0 * 6.215063095092773
Epoch 2940, val loss: 2.196188449859619
Epoch 2950, training loss: 621.6989135742188 = 0.0077100275084376335 + 100.0 * 6.216912269592285
Epoch 2950, val loss: 2.198697566986084
Epoch 2960, training loss: 621.327392578125 = 0.007644180208444595 + 100.0 * 6.213197708129883
Epoch 2960, val loss: 2.201413631439209
Epoch 2970, training loss: 621.3917846679688 = 0.0075818924233317375 + 100.0 * 6.213841915130615
Epoch 2970, val loss: 2.2040863037109375
Epoch 2980, training loss: 621.6712036132812 = 0.00751845445483923 + 100.0 * 6.216637134552002
Epoch 2980, val loss: 2.2062041759490967
Epoch 2990, training loss: 621.2970581054688 = 0.007457162719219923 + 100.0 * 6.212896347045898
Epoch 2990, val loss: 2.2100114822387695
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8154981549815499
=== training gcn model ===
Epoch 0, training loss: 861.62548828125 = 1.9385427236557007 + 100.0 * 8.596869468688965
Epoch 0, val loss: 1.9438657760620117
Epoch 10, training loss: 861.5626220703125 = 1.9301047325134277 + 100.0 * 8.596324920654297
Epoch 10, val loss: 1.9350721836090088
Epoch 20, training loss: 861.1456298828125 = 1.9198263883590698 + 100.0 * 8.59225845336914
Epoch 20, val loss: 1.9242854118347168
Epoch 30, training loss: 858.3135986328125 = 1.9067306518554688 + 100.0 * 8.564068794250488
Epoch 30, val loss: 1.9106011390686035
Epoch 40, training loss: 844.7192993164062 = 1.8902472257614136 + 100.0 * 8.428290367126465
Epoch 40, val loss: 1.894395112991333
Epoch 50, training loss: 806.7754516601562 = 1.8729047775268555 + 100.0 * 8.049025535583496
Epoch 50, val loss: 1.8777629137039185
Epoch 60, training loss: 774.2269897460938 = 1.8582359552383423 + 100.0 * 7.723687648773193
Epoch 60, val loss: 1.8638522624969482
Epoch 70, training loss: 735.844482421875 = 1.847152829170227 + 100.0 * 7.339973449707031
Epoch 70, val loss: 1.852697491645813
Epoch 80, training loss: 711.3338012695312 = 1.837243914604187 + 100.0 * 7.094965934753418
Epoch 80, val loss: 1.842187523841858
Epoch 90, training loss: 695.6344604492188 = 1.8274246454238892 + 100.0 * 6.938069820404053
Epoch 90, val loss: 1.8317090272903442
Epoch 100, training loss: 684.5654907226562 = 1.8172985315322876 + 100.0 * 6.827481746673584
Epoch 100, val loss: 1.8216545581817627
Epoch 110, training loss: 677.4598388671875 = 1.8074061870574951 + 100.0 * 6.756524085998535
Epoch 110, val loss: 1.8122212886810303
Epoch 120, training loss: 672.9614868164062 = 1.796604037284851 + 100.0 * 6.711648464202881
Epoch 120, val loss: 1.8018853664398193
Epoch 130, training loss: 669.491943359375 = 1.7853360176086426 + 100.0 * 6.677066326141357
Epoch 130, val loss: 1.7911475896835327
Epoch 140, training loss: 666.6185913085938 = 1.7739804983139038 + 100.0 * 6.648446083068848
Epoch 140, val loss: 1.7806422710418701
Epoch 150, training loss: 663.94775390625 = 1.762101173400879 + 100.0 * 6.621856689453125
Epoch 150, val loss: 1.7700656652450562
Epoch 160, training loss: 661.5675048828125 = 1.749446988105774 + 100.0 * 6.598180294036865
Epoch 160, val loss: 1.7591753005981445
Epoch 170, training loss: 659.4122314453125 = 1.7359565496444702 + 100.0 * 6.576762676239014
Epoch 170, val loss: 1.7477014064788818
Epoch 180, training loss: 657.2671508789062 = 1.7212539911270142 + 100.0 * 6.555459022521973
Epoch 180, val loss: 1.7354158163070679
Epoch 190, training loss: 655.1508178710938 = 1.7053970098495483 + 100.0 * 6.534454345703125
Epoch 190, val loss: 1.7222576141357422
Epoch 200, training loss: 653.567138671875 = 1.688226580619812 + 100.0 * 6.518788814544678
Epoch 200, val loss: 1.708096981048584
Epoch 210, training loss: 651.735595703125 = 1.6695500612258911 + 100.0 * 6.500660419464111
Epoch 210, val loss: 1.693030595779419
Epoch 220, training loss: 650.2263793945312 = 1.649707555770874 + 100.0 * 6.485766887664795
Epoch 220, val loss: 1.6767991781234741
Epoch 230, training loss: 648.8302612304688 = 1.628426194190979 + 100.0 * 6.472018718719482
Epoch 230, val loss: 1.659679651260376
Epoch 240, training loss: 647.8886108398438 = 1.6057560443878174 + 100.0 * 6.462828636169434
Epoch 240, val loss: 1.641426682472229
Epoch 250, training loss: 646.7037353515625 = 1.5816116333007812 + 100.0 * 6.451221466064453
Epoch 250, val loss: 1.6223008632659912
Epoch 260, training loss: 645.571044921875 = 1.5565446615219116 + 100.0 * 6.440145492553711
Epoch 260, val loss: 1.602565050125122
Epoch 270, training loss: 645.6085205078125 = 1.5307605266571045 + 100.0 * 6.44077730178833
Epoch 270, val loss: 1.5823768377304077
Epoch 280, training loss: 644.0697631835938 = 1.503714919090271 + 100.0 * 6.425660610198975
Epoch 280, val loss: 1.5615630149841309
Epoch 290, training loss: 643.0963134765625 = 1.4763175249099731 + 100.0 * 6.416200160980225
Epoch 290, val loss: 1.5406713485717773
Epoch 300, training loss: 642.3275756835938 = 1.4486536979675293 + 100.0 * 6.408789157867432
Epoch 300, val loss: 1.5199371576309204
Epoch 310, training loss: 641.8578491210938 = 1.4208528995513916 + 100.0 * 6.404370307922363
Epoch 310, val loss: 1.4993846416473389
Epoch 320, training loss: 641.3627319335938 = 1.3926429748535156 + 100.0 * 6.399701118469238
Epoch 320, val loss: 1.478906512260437
Epoch 330, training loss: 640.45849609375 = 1.3647526502609253 + 100.0 * 6.390937805175781
Epoch 330, val loss: 1.4588758945465088
Epoch 340, training loss: 639.94189453125 = 1.3370460271835327 + 100.0 * 6.386048793792725
Epoch 340, val loss: 1.43930983543396
Epoch 350, training loss: 639.7368774414062 = 1.3094762563705444 + 100.0 * 6.384274482727051
Epoch 350, val loss: 1.4204353094100952
Epoch 360, training loss: 638.9036865234375 = 1.282096266746521 + 100.0 * 6.376215934753418
Epoch 360, val loss: 1.401711344718933
Epoch 370, training loss: 638.3565673828125 = 1.2551666498184204 + 100.0 * 6.371013641357422
Epoch 370, val loss: 1.383700966835022
Epoch 380, training loss: 637.8731079101562 = 1.228528380393982 + 100.0 * 6.366446018218994
Epoch 380, val loss: 1.3660746812820435
Epoch 390, training loss: 637.880859375 = 1.2021973133087158 + 100.0 * 6.366786479949951
Epoch 390, val loss: 1.348780632019043
Epoch 400, training loss: 637.1185302734375 = 1.176127552986145 + 100.0 * 6.359424114227295
Epoch 400, val loss: 1.331968903541565
Epoch 410, training loss: 636.6897583007812 = 1.1505557298660278 + 100.0 * 6.355391979217529
Epoch 410, val loss: 1.3157310485839844
Epoch 420, training loss: 636.2944946289062 = 1.1254268884658813 + 100.0 * 6.351690769195557
Epoch 420, val loss: 1.2997914552688599
Epoch 430, training loss: 636.6556396484375 = 1.1005749702453613 + 100.0 * 6.355550765991211
Epoch 430, val loss: 1.2844148874282837
Epoch 440, training loss: 635.6904907226562 = 1.0762441158294678 + 100.0 * 6.346142768859863
Epoch 440, val loss: 1.2692526578903198
Epoch 450, training loss: 635.2811279296875 = 1.0524283647537231 + 100.0 * 6.342287063598633
Epoch 450, val loss: 1.2547789812088013
Epoch 460, training loss: 634.9244995117188 = 1.0291531085968018 + 100.0 * 6.338953495025635
Epoch 460, val loss: 1.2406330108642578
Epoch 470, training loss: 634.6113891601562 = 1.0063860416412354 + 100.0 * 6.336050033569336
Epoch 470, val loss: 1.226996898651123
Epoch 480, training loss: 635.0206909179688 = 0.9842023849487305 + 100.0 * 6.340364933013916
Epoch 480, val loss: 1.2139666080474854
Epoch 490, training loss: 634.1971435546875 = 0.9622988700866699 + 100.0 * 6.332348346710205
Epoch 490, val loss: 1.201353669166565
Epoch 500, training loss: 633.7766723632812 = 0.9411038756370544 + 100.0 * 6.32835578918457
Epoch 500, val loss: 1.1893469095230103
Epoch 510, training loss: 633.489013671875 = 0.9205041527748108 + 100.0 * 6.325685024261475
Epoch 510, val loss: 1.17802095413208
Epoch 520, training loss: 633.802978515625 = 0.9005035161972046 + 100.0 * 6.329024791717529
Epoch 520, val loss: 1.1672823429107666
Epoch 530, training loss: 633.6608276367188 = 0.8808749318122864 + 100.0 * 6.3277997970581055
Epoch 530, val loss: 1.1570720672607422
Epoch 540, training loss: 632.9010009765625 = 0.8616846203804016 + 100.0 * 6.320393085479736
Epoch 540, val loss: 1.1471939086914062
Epoch 550, training loss: 632.493896484375 = 0.8432506918907166 + 100.0 * 6.316506385803223
Epoch 550, val loss: 1.1382479667663574
Epoch 560, training loss: 632.4010009765625 = 0.8253247141838074 + 100.0 * 6.315756797790527
Epoch 560, val loss: 1.130053997039795
Epoch 570, training loss: 632.1510009765625 = 0.8076528310775757 + 100.0 * 6.3134331703186035
Epoch 570, val loss: 1.1219561100006104
Epoch 580, training loss: 631.9473876953125 = 0.7905867099761963 + 100.0 * 6.311568260192871
Epoch 580, val loss: 1.1146774291992188
Epoch 590, training loss: 631.7073364257812 = 0.773989200592041 + 100.0 * 6.309333801269531
Epoch 590, val loss: 1.1079996824264526
Epoch 600, training loss: 631.4720458984375 = 0.7577834129333496 + 100.0 * 6.30714225769043
Epoch 600, val loss: 1.1016910076141357
Epoch 610, training loss: 631.461669921875 = 0.74198317527771 + 100.0 * 6.307196617126465
Epoch 610, val loss: 1.0960572957992554
Epoch 620, training loss: 631.1675415039062 = 0.7264536023139954 + 100.0 * 6.304410934448242
Epoch 620, val loss: 1.0903512239456177
Epoch 630, training loss: 631.05224609375 = 0.71134352684021 + 100.0 * 6.303408622741699
Epoch 630, val loss: 1.0859401226043701
Epoch 640, training loss: 630.7723999023438 = 0.6966297626495361 + 100.0 * 6.30075740814209
Epoch 640, val loss: 1.0814889669418335
Epoch 650, training loss: 630.5094604492188 = 0.6822961568832397 + 100.0 * 6.298271179199219
Epoch 650, val loss: 1.0778449773788452
Epoch 660, training loss: 630.3054809570312 = 0.6683269739151001 + 100.0 * 6.2963714599609375
Epoch 660, val loss: 1.0748170614242554
Epoch 670, training loss: 631.0573120117188 = 0.6546754837036133 + 100.0 * 6.3040266036987305
Epoch 670, val loss: 1.0720725059509277
Epoch 680, training loss: 629.992431640625 = 0.6411967277526855 + 100.0 * 6.293512344360352
Epoch 680, val loss: 1.0696698427200317
Epoch 690, training loss: 630.2327880859375 = 0.6281324028968811 + 100.0 * 6.296046733856201
Epoch 690, val loss: 1.0678867101669312
Epoch 700, training loss: 630.2396240234375 = 0.615247905254364 + 100.0 * 6.296243667602539
Epoch 700, val loss: 1.0662046670913696
Epoch 710, training loss: 629.6461791992188 = 0.6027016639709473 + 100.0 * 6.29043436050415
Epoch 710, val loss: 1.0652711391448975
Epoch 720, training loss: 629.4021606445312 = 0.5904390215873718 + 100.0 * 6.288116931915283
Epoch 720, val loss: 1.064554214477539
Epoch 730, training loss: 629.224853515625 = 0.5784753561019897 + 100.0 * 6.286463737487793
Epoch 730, val loss: 1.0644421577453613
Epoch 740, training loss: 629.2288818359375 = 0.5667886734008789 + 100.0 * 6.28662109375
Epoch 740, val loss: 1.0649141073226929
Epoch 750, training loss: 629.3046875 = 0.5551639199256897 + 100.0 * 6.287495136260986
Epoch 750, val loss: 1.065134048461914
Epoch 760, training loss: 629.0474243164062 = 0.5437678098678589 + 100.0 * 6.285036563873291
Epoch 760, val loss: 1.0662764310836792
Epoch 770, training loss: 628.7783813476562 = 0.5325552225112915 + 100.0 * 6.282458782196045
Epoch 770, val loss: 1.0671844482421875
Epoch 780, training loss: 629.2760009765625 = 0.5215533375740051 + 100.0 * 6.2875447273254395
Epoch 780, val loss: 1.0686004161834717
Epoch 790, training loss: 628.7175903320312 = 0.5107805728912354 + 100.0 * 6.282067775726318
Epoch 790, val loss: 1.0708236694335938
Epoch 800, training loss: 628.48193359375 = 0.500088095664978 + 100.0 * 6.279819011688232
Epoch 800, val loss: 1.073157548904419
Epoch 810, training loss: 628.720703125 = 0.4895862936973572 + 100.0 * 6.28231143951416
Epoch 810, val loss: 1.0754289627075195
Epoch 820, training loss: 628.38232421875 = 0.47920387983322144 + 100.0 * 6.279031276702881
Epoch 820, val loss: 1.0780596733093262
Epoch 830, training loss: 628.343994140625 = 0.46892234683036804 + 100.0 * 6.278750896453857
Epoch 830, val loss: 1.081193208694458
Epoch 840, training loss: 627.9231567382812 = 0.458811491727829 + 100.0 * 6.274643421173096
Epoch 840, val loss: 1.0847753286361694
Epoch 850, training loss: 627.939208984375 = 0.44886651635169983 + 100.0 * 6.274903774261475
Epoch 850, val loss: 1.0883277654647827
Epoch 860, training loss: 628.2391967773438 = 0.4389444589614868 + 100.0 * 6.2780022621154785
Epoch 860, val loss: 1.091969609260559
Epoch 870, training loss: 627.6948852539062 = 0.42909806966781616 + 100.0 * 6.272658348083496
Epoch 870, val loss: 1.0958460569381714
Epoch 880, training loss: 627.5148315429688 = 0.41946667432785034 + 100.0 * 6.270953178405762
Epoch 880, val loss: 1.1000837087631226
Epoch 890, training loss: 627.4144897460938 = 0.4099508821964264 + 100.0 * 6.270045280456543
Epoch 890, val loss: 1.1042943000793457
Epoch 900, training loss: 627.936767578125 = 0.4006282091140747 + 100.0 * 6.275361061096191
Epoch 900, val loss: 1.1090855598449707
Epoch 910, training loss: 627.8008422851562 = 0.3911791145801544 + 100.0 * 6.274096488952637
Epoch 910, val loss: 1.1135557889938354
Epoch 920, training loss: 627.2493896484375 = 0.3819672167301178 + 100.0 * 6.268674373626709
Epoch 920, val loss: 1.1185860633850098
Epoch 930, training loss: 627.1358642578125 = 0.372861385345459 + 100.0 * 6.267630100250244
Epoch 930, val loss: 1.1233255863189697
Epoch 940, training loss: 626.9135131835938 = 0.36391881108283997 + 100.0 * 6.265496253967285
Epoch 940, val loss: 1.1285390853881836
Epoch 950, training loss: 627.16552734375 = 0.3551017940044403 + 100.0 * 6.268104553222656
Epoch 950, val loss: 1.13359797000885
Epoch 960, training loss: 627.3963012695312 = 0.34626686573028564 + 100.0 * 6.270500659942627
Epoch 960, val loss: 1.139868140220642
Epoch 970, training loss: 626.6948852539062 = 0.33757463097572327 + 100.0 * 6.263573169708252
Epoch 970, val loss: 1.1450990438461304
Epoch 980, training loss: 626.626220703125 = 0.32911375164985657 + 100.0 * 6.262970924377441
Epoch 980, val loss: 1.1506861448287964
Epoch 990, training loss: 626.5177001953125 = 0.32076263427734375 + 100.0 * 6.261969566345215
Epoch 990, val loss: 1.1564831733703613
Epoch 1000, training loss: 626.3853149414062 = 0.3124786913394928 + 100.0 * 6.260728359222412
Epoch 1000, val loss: 1.1625834703445435
Epoch 1010, training loss: 626.8390502929688 = 0.30420634150505066 + 100.0 * 6.265348434448242
Epoch 1010, val loss: 1.1693552732467651
Epoch 1020, training loss: 627.0993041992188 = 0.2956116199493408 + 100.0 * 6.268036842346191
Epoch 1020, val loss: 1.174878478050232
Epoch 1030, training loss: 626.3858642578125 = 0.2873679995536804 + 100.0 * 6.260984897613525
Epoch 1030, val loss: 1.1815506219863892
Epoch 1040, training loss: 626.1197509765625 = 0.27933263778686523 + 100.0 * 6.258403778076172
Epoch 1040, val loss: 1.1881299018859863
Epoch 1050, training loss: 626.0358276367188 = 0.27150753140449524 + 100.0 * 6.257643699645996
Epoch 1050, val loss: 1.194809079170227
Epoch 1060, training loss: 625.958984375 = 0.26383277773857117 + 100.0 * 6.256951332092285
Epoch 1060, val loss: 1.2018598318099976
Epoch 1070, training loss: 625.9896850585938 = 0.2562751770019531 + 100.0 * 6.257334232330322
Epoch 1070, val loss: 1.2088189125061035
Epoch 1080, training loss: 626.5267333984375 = 0.24881720542907715 + 100.0 * 6.2627787590026855
Epoch 1080, val loss: 1.2156857252120972
Epoch 1090, training loss: 625.7969970703125 = 0.24150234460830688 + 100.0 * 6.255554676055908
Epoch 1090, val loss: 1.223341464996338
Epoch 1100, training loss: 625.815185546875 = 0.23437745869159698 + 100.0 * 6.255807876586914
Epoch 1100, val loss: 1.2307037115097046
Epoch 1110, training loss: 625.59765625 = 0.22743110358715057 + 100.0 * 6.253702163696289
Epoch 1110, val loss: 1.2382562160491943
Epoch 1120, training loss: 625.6240844726562 = 0.22068358957767487 + 100.0 * 6.25403356552124
Epoch 1120, val loss: 1.246159553527832
Epoch 1130, training loss: 626.0023803710938 = 0.21407341957092285 + 100.0 * 6.257883071899414
Epoch 1130, val loss: 1.2541730403900146
Epoch 1140, training loss: 625.628173828125 = 0.2075590342283249 + 100.0 * 6.25420618057251
Epoch 1140, val loss: 1.2621885538101196
Epoch 1150, training loss: 625.3803100585938 = 0.20120210945606232 + 100.0 * 6.251791477203369
Epoch 1150, val loss: 1.2701125144958496
Epoch 1160, training loss: 625.2734985351562 = 0.1950463205575943 + 100.0 * 6.250784397125244
Epoch 1160, val loss: 1.2781215906143188
Epoch 1170, training loss: 625.755615234375 = 0.18907828629016876 + 100.0 * 6.255665302276611
Epoch 1170, val loss: 1.2860398292541504
Epoch 1180, training loss: 625.3803100585938 = 0.18326205015182495 + 100.0 * 6.251970291137695
Epoch 1180, val loss: 1.2947088479995728
Epoch 1190, training loss: 625.2544555664062 = 0.17759807407855988 + 100.0 * 6.250768661499023
Epoch 1190, val loss: 1.3027925491333008
Epoch 1200, training loss: 625.4158935546875 = 0.17212611436843872 + 100.0 * 6.252437591552734
Epoch 1200, val loss: 1.3111140727996826
Epoch 1210, training loss: 625.0476684570312 = 0.16678963601589203 + 100.0 * 6.24880838394165
Epoch 1210, val loss: 1.3191994428634644
Epoch 1220, training loss: 625.0130615234375 = 0.1616358458995819 + 100.0 * 6.248514175415039
Epoch 1220, val loss: 1.3278950452804565
Epoch 1230, training loss: 625.1323852539062 = 0.1566198617219925 + 100.0 * 6.249757766723633
Epoch 1230, val loss: 1.336377739906311
Epoch 1240, training loss: 625.3319091796875 = 0.15172530710697174 + 100.0 * 6.251801490783691
Epoch 1240, val loss: 1.3447402715682983
Epoch 1250, training loss: 624.9051513671875 = 0.14697720110416412 + 100.0 * 6.247581481933594
Epoch 1250, val loss: 1.352925419807434
Epoch 1260, training loss: 624.7219848632812 = 0.14243516325950623 + 100.0 * 6.245795249938965
Epoch 1260, val loss: 1.3609848022460938
Epoch 1270, training loss: 624.6798095703125 = 0.138042151927948 + 100.0 * 6.245417594909668
Epoch 1270, val loss: 1.3693323135375977
Epoch 1280, training loss: 625.2326049804688 = 0.13379648327827454 + 100.0 * 6.250988006591797
Epoch 1280, val loss: 1.3772773742675781
Epoch 1290, training loss: 624.7711791992188 = 0.12963266670703888 + 100.0 * 6.246415138244629
Epoch 1290, val loss: 1.3861775398254395
Epoch 1300, training loss: 625.0086059570312 = 0.12563136219978333 + 100.0 * 6.2488298416137695
Epoch 1300, val loss: 1.3940839767456055
Epoch 1310, training loss: 624.4509887695312 = 0.1217537596821785 + 100.0 * 6.243292331695557
Epoch 1310, val loss: 1.4026201963424683
Epoch 1320, training loss: 624.3999633789062 = 0.11802050471305847 + 100.0 * 6.242819309234619
Epoch 1320, val loss: 1.411102056503296
Epoch 1330, training loss: 624.3311157226562 = 0.11442281305789948 + 100.0 * 6.242166519165039
Epoch 1330, val loss: 1.4195239543914795
Epoch 1340, training loss: 624.6605224609375 = 0.11097457259893417 + 100.0 * 6.245495796203613
Epoch 1340, val loss: 1.4282101392745972
Epoch 1350, training loss: 624.2868041992188 = 0.10754602402448654 + 100.0 * 6.241792678833008
Epoch 1350, val loss: 1.4363055229187012
Epoch 1360, training loss: 624.4824829101562 = 0.10428667813539505 + 100.0 * 6.243782043457031
Epoch 1360, val loss: 1.4447542428970337
Epoch 1370, training loss: 624.4265747070312 = 0.10110674053430557 + 100.0 * 6.2432541847229
Epoch 1370, val loss: 1.4529118537902832
Epoch 1380, training loss: 624.4089965820312 = 0.09805687516927719 + 100.0 * 6.243109226226807
Epoch 1380, val loss: 1.4612691402435303
Epoch 1390, training loss: 624.1648559570312 = 0.09512051939964294 + 100.0 * 6.240697860717773
Epoch 1390, val loss: 1.4693183898925781
Epoch 1400, training loss: 624.0432739257812 = 0.09230073541402817 + 100.0 * 6.2395100593566895
Epoch 1400, val loss: 1.4776709079742432
Epoch 1410, training loss: 624.0673217773438 = 0.08958233147859573 + 100.0 * 6.239777088165283
Epoch 1410, val loss: 1.4857687950134277
Epoch 1420, training loss: 624.361572265625 = 0.08694418519735336 + 100.0 * 6.242746829986572
Epoch 1420, val loss: 1.493708848953247
Epoch 1430, training loss: 624.007568359375 = 0.08440618216991425 + 100.0 * 6.239231586456299
Epoch 1430, val loss: 1.5024333000183105
Epoch 1440, training loss: 624.7125244140625 = 0.08196710795164108 + 100.0 * 6.246305465698242
Epoch 1440, val loss: 1.5107265710830688
Epoch 1450, training loss: 624.0748901367188 = 0.0795842781662941 + 100.0 * 6.23995304107666
Epoch 1450, val loss: 1.5181834697723389
Epoch 1460, training loss: 623.8135986328125 = 0.07730861753225327 + 100.0 * 6.237362861633301
Epoch 1460, val loss: 1.5265069007873535
Epoch 1470, training loss: 623.8163452148438 = 0.07512760907411575 + 100.0 * 6.237411975860596
Epoch 1470, val loss: 1.5344065427780151
Epoch 1480, training loss: 624.3820190429688 = 0.07300411909818649 + 100.0 * 6.2430901527404785
Epoch 1480, val loss: 1.5422919988632202
Epoch 1490, training loss: 623.7381591796875 = 0.07096027582883835 + 100.0 * 6.2366719245910645
Epoch 1490, val loss: 1.5504517555236816
Epoch 1500, training loss: 623.6123657226562 = 0.06900115311145782 + 100.0 * 6.235433101654053
Epoch 1500, val loss: 1.5585367679595947
Epoch 1510, training loss: 623.5884399414062 = 0.06713254749774933 + 100.0 * 6.235212802886963
Epoch 1510, val loss: 1.5661733150482178
Epoch 1520, training loss: 624.2440185546875 = 0.0653432160615921 + 100.0 * 6.241786479949951
Epoch 1520, val loss: 1.5741993188858032
Epoch 1530, training loss: 623.9807739257812 = 0.0635400265455246 + 100.0 * 6.239172458648682
Epoch 1530, val loss: 1.5813491344451904
Epoch 1540, training loss: 623.7340698242188 = 0.06184738501906395 + 100.0 * 6.236722469329834
Epoch 1540, val loss: 1.5893957614898682
Epoch 1550, training loss: 623.5204467773438 = 0.06018583104014397 + 100.0 * 6.234602928161621
Epoch 1550, val loss: 1.5969518423080444
Epoch 1560, training loss: 623.3853759765625 = 0.05862210690975189 + 100.0 * 6.233267784118652
Epoch 1560, val loss: 1.604575753211975
Epoch 1570, training loss: 623.6325073242188 = 0.057104796171188354 + 100.0 * 6.235754013061523
Epoch 1570, val loss: 1.6118301153182983
Epoch 1580, training loss: 623.4971923828125 = 0.055625103414058685 + 100.0 * 6.2344160079956055
Epoch 1580, val loss: 1.6195985078811646
Epoch 1590, training loss: 623.3119506835938 = 0.05419561266899109 + 100.0 * 6.232577323913574
Epoch 1590, val loss: 1.6266276836395264
Epoch 1600, training loss: 623.3744506835938 = 0.052837323397397995 + 100.0 * 6.233215808868408
Epoch 1600, val loss: 1.634321928024292
Epoch 1610, training loss: 623.9768676757812 = 0.05151429772377014 + 100.0 * 6.239253520965576
Epoch 1610, val loss: 1.6413174867630005
Epoch 1620, training loss: 623.377685546875 = 0.050228022038936615 + 100.0 * 6.233274459838867
Epoch 1620, val loss: 1.6484590768814087
Epoch 1630, training loss: 623.1749877929688 = 0.04899179935455322 + 100.0 * 6.231259822845459
Epoch 1630, val loss: 1.6558139324188232
Epoch 1640, training loss: 623.1632080078125 = 0.04781566932797432 + 100.0 * 6.231153964996338
Epoch 1640, val loss: 1.6628587245941162
Epoch 1650, training loss: 623.9714965820312 = 0.04668530449271202 + 100.0 * 6.239247798919678
Epoch 1650, val loss: 1.6704901456832886
Epoch 1660, training loss: 623.5244140625 = 0.045534998178482056 + 100.0 * 6.23478889465332
Epoch 1660, val loss: 1.6767823696136475
Epoch 1670, training loss: 623.3675537109375 = 0.04445315897464752 + 100.0 * 6.233231067657471
Epoch 1670, val loss: 1.6834776401519775
Epoch 1680, training loss: 623.170166015625 = 0.04341064393520355 + 100.0 * 6.231267929077148
Epoch 1680, val loss: 1.6905560493469238
Epoch 1690, training loss: 622.989990234375 = 0.04240942373871803 + 100.0 * 6.229475498199463
Epoch 1690, val loss: 1.6975377798080444
Epoch 1700, training loss: 623.0051879882812 = 0.041443414986133575 + 100.0 * 6.229637145996094
Epoch 1700, val loss: 1.7042291164398193
Epoch 1710, training loss: 623.3713989257812 = 0.04051104933023453 + 100.0 * 6.233308792114258
Epoch 1710, val loss: 1.7105854749679565
Epoch 1720, training loss: 623.4890747070312 = 0.039601948112249374 + 100.0 * 6.234494686126709
Epoch 1720, val loss: 1.717591404914856
Epoch 1730, training loss: 623.0743408203125 = 0.03869101032614708 + 100.0 * 6.230356216430664
Epoch 1730, val loss: 1.723791480064392
Epoch 1740, training loss: 622.8743896484375 = 0.037834737449884415 + 100.0 * 6.228365421295166
Epoch 1740, val loss: 1.7305023670196533
Epoch 1750, training loss: 622.7998046875 = 0.037012729793787 + 100.0 * 6.227628231048584
Epoch 1750, val loss: 1.7367658615112305
Epoch 1760, training loss: 622.906494140625 = 0.0362175852060318 + 100.0 * 6.228703022003174
Epoch 1760, val loss: 1.742821216583252
Epoch 1770, training loss: 623.1824340820312 = 0.03543652594089508 + 100.0 * 6.231469631195068
Epoch 1770, val loss: 1.749221920967102
Epoch 1780, training loss: 622.88037109375 = 0.03467874601483345 + 100.0 * 6.228456974029541
Epoch 1780, val loss: 1.7556883096694946
Epoch 1790, training loss: 622.8932495117188 = 0.033943790942430496 + 100.0 * 6.228592872619629
Epoch 1790, val loss: 1.761961817741394
Epoch 1800, training loss: 622.7593383789062 = 0.033233918249607086 + 100.0 * 6.227260589599609
Epoch 1800, val loss: 1.7677996158599854
Epoch 1810, training loss: 623.7047119140625 = 0.032568927854299545 + 100.0 * 6.236721038818359
Epoch 1810, val loss: 1.774429440498352
Epoch 1820, training loss: 622.8986206054688 = 0.03187314420938492 + 100.0 * 6.228667736053467
Epoch 1820, val loss: 1.7798279523849487
Epoch 1830, training loss: 622.68115234375 = 0.031223135069012642 + 100.0 * 6.226499080657959
Epoch 1830, val loss: 1.7862603664398193
Epoch 1840, training loss: 622.5707397460938 = 0.030597150325775146 + 100.0 * 6.225401401519775
Epoch 1840, val loss: 1.7920945882797241
Epoch 1850, training loss: 622.9420776367188 = 0.029996296390891075 + 100.0 * 6.229121208190918
Epoch 1850, val loss: 1.7980561256408691
Epoch 1860, training loss: 622.565673828125 = 0.029392976313829422 + 100.0 * 6.225362300872803
Epoch 1860, val loss: 1.803608775138855
Epoch 1870, training loss: 622.5075073242188 = 0.028816303238272667 + 100.0 * 6.224786758422852
Epoch 1870, val loss: 1.809422254562378
Epoch 1880, training loss: 622.6329956054688 = 0.028261549770832062 + 100.0 * 6.226047515869141
Epoch 1880, val loss: 1.8153764009475708
Epoch 1890, training loss: 622.6775512695312 = 0.027714991942048073 + 100.0 * 6.226498126983643
Epoch 1890, val loss: 1.8207244873046875
Epoch 1900, training loss: 623.0526123046875 = 0.027185717597603798 + 100.0 * 6.230254650115967
Epoch 1900, val loss: 1.8263225555419922
Epoch 1910, training loss: 622.5326538085938 = 0.02666444703936577 + 100.0 * 6.225059986114502
Epoch 1910, val loss: 1.832047939300537
Epoch 1920, training loss: 622.37646484375 = 0.026164406910538673 + 100.0 * 6.2235026359558105
Epoch 1920, val loss: 1.8378604650497437
Epoch 1930, training loss: 622.3049926757812 = 0.025683851912617683 + 100.0 * 6.222793102264404
Epoch 1930, val loss: 1.8433759212493896
Epoch 1940, training loss: 622.7846069335938 = 0.025229934602975845 + 100.0 * 6.227593898773193
Epoch 1940, val loss: 1.8491955995559692
Epoch 1950, training loss: 622.3572387695312 = 0.024745969101786613 + 100.0 * 6.223325252532959
Epoch 1950, val loss: 1.8534398078918457
Epoch 1960, training loss: 622.5455322265625 = 0.02429947629570961 + 100.0 * 6.225212097167969
Epoch 1960, val loss: 1.8592232465744019
Epoch 1970, training loss: 622.4720458984375 = 0.023855620995163918 + 100.0 * 6.224481582641602
Epoch 1970, val loss: 1.8641630411148071
Epoch 1980, training loss: 622.4320678710938 = 0.023437121883034706 + 100.0 * 6.224086284637451
Epoch 1980, val loss: 1.8697477579116821
Epoch 1990, training loss: 622.2229614257812 = 0.02301514334976673 + 100.0 * 6.221999645233154
Epoch 1990, val loss: 1.874853253364563
Epoch 2000, training loss: 622.32763671875 = 0.022624608129262924 + 100.0 * 6.223050117492676
Epoch 2000, val loss: 1.8803778886795044
Epoch 2010, training loss: 622.7572631835938 = 0.022230658680200577 + 100.0 * 6.22735071182251
Epoch 2010, val loss: 1.8854093551635742
Epoch 2020, training loss: 622.17138671875 = 0.021829117089509964 + 100.0 * 6.221495151519775
Epoch 2020, val loss: 1.8898184299468994
Epoch 2030, training loss: 622.1038208007812 = 0.021461667492985725 + 100.0 * 6.220823287963867
Epoch 2030, val loss: 1.8952000141143799
Epoch 2040, training loss: 622.3025512695312 = 0.021105291321873665 + 100.0 * 6.222814559936523
Epoch 2040, val loss: 1.8997992277145386
Epoch 2050, training loss: 622.3507690429688 = 0.02074901945888996 + 100.0 * 6.223299980163574
Epoch 2050, val loss: 1.9049803018569946
Epoch 2060, training loss: 622.17626953125 = 0.02039925567805767 + 100.0 * 6.221558570861816
Epoch 2060, val loss: 1.9100854396820068
Epoch 2070, training loss: 622.0709838867188 = 0.020066920667886734 + 100.0 * 6.220509052276611
Epoch 2070, val loss: 1.9147189855575562
Epoch 2080, training loss: 622.6422729492188 = 0.019745102152228355 + 100.0 * 6.226224899291992
Epoch 2080, val loss: 1.919688105583191
Epoch 2090, training loss: 622.1898803710938 = 0.01942356675863266 + 100.0 * 6.221704959869385
Epoch 2090, val loss: 1.9241782426834106
Epoch 2100, training loss: 622.2304077148438 = 0.019112635403871536 + 100.0 * 6.222113132476807
Epoch 2100, val loss: 1.9292138814926147
Epoch 2110, training loss: 622.0614013671875 = 0.018806349486112595 + 100.0 * 6.220426082611084
Epoch 2110, val loss: 1.9334911108016968
Epoch 2120, training loss: 621.8844604492188 = 0.01851070299744606 + 100.0 * 6.2186598777771
Epoch 2120, val loss: 1.938161015510559
Epoch 2130, training loss: 622.4393920898438 = 0.01823343150317669 + 100.0 * 6.2242112159729
Epoch 2130, val loss: 1.942561388015747
Epoch 2140, training loss: 622.0663452148438 = 0.01794329844415188 + 100.0 * 6.220483779907227
Epoch 2140, val loss: 1.947417140007019
Epoch 2150, training loss: 622.135986328125 = 0.017667626962065697 + 100.0 * 6.2211833000183105
Epoch 2150, val loss: 1.9515516757965088
Epoch 2160, training loss: 622.0172119140625 = 0.017392324283719063 + 100.0 * 6.219998359680176
Epoch 2160, val loss: 1.9558625221252441
Epoch 2170, training loss: 621.9901123046875 = 0.01712927408516407 + 100.0 * 6.219729423522949
Epoch 2170, val loss: 1.9601019620895386
Epoch 2180, training loss: 621.9515380859375 = 0.016873272135853767 + 100.0 * 6.21934700012207
Epoch 2180, val loss: 1.96485435962677
Epoch 2190, training loss: 622.403076171875 = 0.016629187390208244 + 100.0 * 6.223865032196045
Epoch 2190, val loss: 1.969278335571289
Epoch 2200, training loss: 621.9794311523438 = 0.016373133286833763 + 100.0 * 6.219630718231201
Epoch 2200, val loss: 1.9730839729309082
Epoch 2210, training loss: 621.817138671875 = 0.016132349148392677 + 100.0 * 6.218010425567627
Epoch 2210, val loss: 1.97756826877594
Epoch 2220, training loss: 621.8764038085938 = 0.01590055041015148 + 100.0 * 6.218605041503906
Epoch 2220, val loss: 1.9816811084747314
Epoch 2230, training loss: 622.2819213867188 = 0.015669383108615875 + 100.0 * 6.222662448883057
Epoch 2230, val loss: 1.9855204820632935
Epoch 2240, training loss: 621.7815551757812 = 0.015444084070622921 + 100.0 * 6.217660903930664
Epoch 2240, val loss: 1.98970627784729
Epoch 2250, training loss: 621.612060546875 = 0.015224684029817581 + 100.0 * 6.215968608856201
Epoch 2250, val loss: 1.993911623954773
Epoch 2260, training loss: 621.6665649414062 = 0.01501696277409792 + 100.0 * 6.21651554107666
Epoch 2260, val loss: 1.9978866577148438
Epoch 2270, training loss: 622.1694946289062 = 0.014816931448876858 + 100.0 * 6.221546649932861
Epoch 2270, val loss: 2.001835823059082
Epoch 2280, training loss: 621.7070922851562 = 0.014605168253183365 + 100.0 * 6.216925144195557
Epoch 2280, val loss: 2.0053718090057373
Epoch 2290, training loss: 622.0650634765625 = 0.014402713626623154 + 100.0 * 6.22050666809082
Epoch 2290, val loss: 2.009169816970825
Epoch 2300, training loss: 621.577392578125 = 0.01420395728200674 + 100.0 * 6.21563196182251
Epoch 2300, val loss: 2.0131635665893555
Epoch 2310, training loss: 621.5328979492188 = 0.014014973305165768 + 100.0 * 6.215188503265381
Epoch 2310, val loss: 2.0168654918670654
Epoch 2320, training loss: 621.6781616210938 = 0.01383022591471672 + 100.0 * 6.216643333435059
Epoch 2320, val loss: 2.0205118656158447
Epoch 2330, training loss: 621.8228149414062 = 0.013647052459418774 + 100.0 * 6.2180914878845215
Epoch 2330, val loss: 2.0240843296051025
Epoch 2340, training loss: 621.6842041015625 = 0.013467283919453621 + 100.0 * 6.216707229614258
Epoch 2340, val loss: 2.028228998184204
Epoch 2350, training loss: 621.4978637695312 = 0.013291701674461365 + 100.0 * 6.214845657348633
Epoch 2350, val loss: 2.0314793586730957
Epoch 2360, training loss: 621.4290161132812 = 0.013124344870448112 + 100.0 * 6.21415901184082
Epoch 2360, val loss: 2.035212516784668
Epoch 2370, training loss: 621.877685546875 = 0.01296128798276186 + 100.0 * 6.218647480010986
Epoch 2370, val loss: 2.038454294204712
Epoch 2380, training loss: 621.8436279296875 = 0.012793237343430519 + 100.0 * 6.218307971954346
Epoch 2380, val loss: 2.042567253112793
Epoch 2390, training loss: 621.6256103515625 = 0.012626896612346172 + 100.0 * 6.216129779815674
Epoch 2390, val loss: 2.0457305908203125
Epoch 2400, training loss: 621.5217895507812 = 0.012470553629100323 + 100.0 * 6.21509313583374
Epoch 2400, val loss: 2.048966884613037
Epoch 2410, training loss: 621.432373046875 = 0.012316086329519749 + 100.0 * 6.214200496673584
Epoch 2410, val loss: 2.0527448654174805
Epoch 2420, training loss: 621.6694946289062 = 0.012167616747319698 + 100.0 * 6.216573238372803
Epoch 2420, val loss: 2.055926561355591
Epoch 2430, training loss: 621.4287719726562 = 0.012018960900604725 + 100.0 * 6.214167594909668
Epoch 2430, val loss: 2.059462308883667
Epoch 2440, training loss: 621.5570678710938 = 0.011875485070049763 + 100.0 * 6.215451717376709
Epoch 2440, val loss: 2.0629332065582275
Epoch 2450, training loss: 621.5449829101562 = 0.0117304353043437 + 100.0 * 6.215332508087158
Epoch 2450, val loss: 2.066183090209961
Epoch 2460, training loss: 621.4657592773438 = 0.011594690382480621 + 100.0 * 6.214541435241699
Epoch 2460, val loss: 2.069702625274658
Epoch 2470, training loss: 621.90380859375 = 0.01145961694419384 + 100.0 * 6.218923091888428
Epoch 2470, val loss: 2.072819948196411
Epoch 2480, training loss: 621.3643798828125 = 0.01131482608616352 + 100.0 * 6.213531017303467
Epoch 2480, val loss: 2.075960159301758
Epoch 2490, training loss: 621.1860961914062 = 0.01118225883692503 + 100.0 * 6.21174955368042
Epoch 2490, val loss: 2.0792040824890137
Epoch 2500, training loss: 621.1525268554688 = 0.01105516031384468 + 100.0 * 6.211414813995361
Epoch 2500, val loss: 2.082508087158203
Epoch 2510, training loss: 621.2098388671875 = 0.010931895114481449 + 100.0 * 6.211988925933838
Epoch 2510, val loss: 2.085604429244995
Epoch 2520, training loss: 622.244384765625 = 0.010815693996846676 + 100.0 * 6.2223358154296875
Epoch 2520, val loss: 2.0889174938201904
Epoch 2530, training loss: 621.7150268554688 = 0.010681799612939358 + 100.0 * 6.217043399810791
Epoch 2530, val loss: 2.0916593074798584
Epoch 2540, training loss: 621.2718505859375 = 0.010556934401392937 + 100.0 * 6.212612628936768
Epoch 2540, val loss: 2.094698905944824
Epoch 2550, training loss: 621.1475830078125 = 0.010438691824674606 + 100.0 * 6.211371421813965
Epoch 2550, val loss: 2.097749948501587
Epoch 2560, training loss: 621.1944580078125 = 0.010325992479920387 + 100.0 * 6.211841583251953
Epoch 2560, val loss: 2.1010220050811768
Epoch 2570, training loss: 621.5316162109375 = 0.010214815847575665 + 100.0 * 6.215214252471924
Epoch 2570, val loss: 2.104048490524292
Epoch 2580, training loss: 621.3668823242188 = 0.010105161927640438 + 100.0 * 6.213567733764648
Epoch 2580, val loss: 2.107086181640625
Epoch 2590, training loss: 621.1973266601562 = 0.009992819279432297 + 100.0 * 6.2118730545043945
Epoch 2590, val loss: 2.110055446624756
Epoch 2600, training loss: 621.2305908203125 = 0.009884740225970745 + 100.0 * 6.212206840515137
Epoch 2600, val loss: 2.1128861904144287
Epoch 2610, training loss: 621.3729858398438 = 0.009780451655387878 + 100.0 * 6.213632106781006
Epoch 2610, val loss: 2.115689992904663
Epoch 2620, training loss: 621.0176391601562 = 0.009673100896179676 + 100.0 * 6.210079669952393
Epoch 2620, val loss: 2.1182572841644287
Epoch 2630, training loss: 621.161865234375 = 0.009573311544954777 + 100.0 * 6.211523056030273
Epoch 2630, val loss: 2.12099552154541
Epoch 2640, training loss: 621.4688720703125 = 0.009474743157625198 + 100.0 * 6.214593887329102
Epoch 2640, val loss: 2.1235694885253906
Epoch 2650, training loss: 621.0630493164062 = 0.009376276284456253 + 100.0 * 6.210536479949951
Epoch 2650, val loss: 2.126911163330078
Epoch 2660, training loss: 620.9708862304688 = 0.009279637597501278 + 100.0 * 6.209616184234619
Epoch 2660, val loss: 2.129512310028076
Epoch 2670, training loss: 621.031494140625 = 0.009187369607388973 + 100.0 * 6.2102227210998535
Epoch 2670, val loss: 2.13224458694458
Epoch 2680, training loss: 621.6453247070312 = 0.009095690213143826 + 100.0 * 6.216361999511719
Epoch 2680, val loss: 2.1349828243255615
Epoch 2690, training loss: 621.7180786132812 = 0.009001162834465504 + 100.0 * 6.217091083526611
Epoch 2690, val loss: 2.1370725631713867
Epoch 2700, training loss: 621.1134643554688 = 0.008906226605176926 + 100.0 * 6.211045742034912
Epoch 2700, val loss: 2.140207052230835
Epoch 2710, training loss: 620.9036254882812 = 0.008817161433398724 + 100.0 * 6.208948135375977
Epoch 2710, val loss: 2.1431262493133545
Epoch 2720, training loss: 620.8516235351562 = 0.008731451816856861 + 100.0 * 6.208428859710693
Epoch 2720, val loss: 2.1454882621765137
Epoch 2730, training loss: 620.864501953125 = 0.008649216033518314 + 100.0 * 6.208558559417725
Epoch 2730, val loss: 2.148324728012085
Epoch 2740, training loss: 621.5957641601562 = 0.008568994700908661 + 100.0 * 6.215872287750244
Epoch 2740, val loss: 2.150782346725464
Epoch 2750, training loss: 620.9334106445312 = 0.008483447134494781 + 100.0 * 6.209249496459961
Epoch 2750, val loss: 2.1533639430999756
Epoch 2760, training loss: 620.9075927734375 = 0.008400794118642807 + 100.0 * 6.208991527557373
Epoch 2760, val loss: 2.1557962894439697
Epoch 2770, training loss: 621.6920166015625 = 0.00832241028547287 + 100.0 * 6.216836929321289
Epoch 2770, val loss: 2.1580660343170166
Epoch 2780, training loss: 621.3378295898438 = 0.008238451555371284 + 100.0 * 6.213295936584473
Epoch 2780, val loss: 2.1605241298675537
Epoch 2790, training loss: 620.8873901367188 = 0.008159179240465164 + 100.0 * 6.208792209625244
Epoch 2790, val loss: 2.1632983684539795
Epoch 2800, training loss: 620.8048095703125 = 0.008085286244750023 + 100.0 * 6.207967281341553
Epoch 2800, val loss: 2.1661276817321777
Epoch 2810, training loss: 620.927490234375 = 0.008015107363462448 + 100.0 * 6.209194660186768
Epoch 2810, val loss: 2.1685304641723633
Epoch 2820, training loss: 621.2613525390625 = 0.007944400422275066 + 100.0 * 6.212534427642822
Epoch 2820, val loss: 2.170872449874878
Epoch 2830, training loss: 621.0698852539062 = 0.007866144180297852 + 100.0 * 6.210619926452637
Epoch 2830, val loss: 2.17250394821167
Epoch 2840, training loss: 620.8134765625 = 0.007794969715178013 + 100.0 * 6.208056926727295
Epoch 2840, val loss: 2.1753227710723877
Epoch 2850, training loss: 620.8616333007812 = 0.007726349402219057 + 100.0 * 6.20853853225708
Epoch 2850, val loss: 2.17740797996521
Epoch 2860, training loss: 620.8519287109375 = 0.0076577747240662575 + 100.0 * 6.2084431648254395
Epoch 2860, val loss: 2.1799509525299072
Epoch 2870, training loss: 620.8234252929688 = 0.007592896465212107 + 100.0 * 6.208158493041992
Epoch 2870, val loss: 2.182541608810425
Epoch 2880, training loss: 620.9728393554688 = 0.007527593988925219 + 100.0 * 6.209653377532959
Epoch 2880, val loss: 2.184506893157959
Epoch 2890, training loss: 620.9376220703125 = 0.0074599082581698895 + 100.0 * 6.209301471710205
Epoch 2890, val loss: 2.1865456104278564
Epoch 2900, training loss: 620.871826171875 = 0.00739436037838459 + 100.0 * 6.208644390106201
Epoch 2900, val loss: 2.188978433609009
Epoch 2910, training loss: 620.8727416992188 = 0.007331232074648142 + 100.0 * 6.208653926849365
Epoch 2910, val loss: 2.191417932510376
Epoch 2920, training loss: 620.8526000976562 = 0.007268096785992384 + 100.0 * 6.208453178405762
Epoch 2920, val loss: 2.1934008598327637
Epoch 2930, training loss: 620.9494018554688 = 0.007210928946733475 + 100.0 * 6.2094221115112305
Epoch 2930, val loss: 2.1957125663757324
Epoch 2940, training loss: 620.652587890625 = 0.007145748473703861 + 100.0 * 6.206454753875732
Epoch 2940, val loss: 2.19765567779541
Epoch 2950, training loss: 620.744873046875 = 0.007087768986821175 + 100.0 * 6.207377910614014
Epoch 2950, val loss: 2.199841260910034
Epoch 2960, training loss: 620.8696899414062 = 0.007030506618320942 + 100.0 * 6.208626747131348
Epoch 2960, val loss: 2.2017669677734375
Epoch 2970, training loss: 620.9934692382812 = 0.006971225142478943 + 100.0 * 6.209865093231201
Epoch 2970, val loss: 2.2046358585357666
Epoch 2980, training loss: 620.7014770507812 = 0.006913298740983009 + 100.0 * 6.206945896148682
Epoch 2980, val loss: 2.206162691116333
Epoch 2990, training loss: 620.7266845703125 = 0.006859396118670702 + 100.0 * 6.207198619842529
Epoch 2990, val loss: 2.2082250118255615
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6666666666666667
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 861.6285400390625 = 1.9419407844543457 + 100.0 * 8.5968656539917
Epoch 0, val loss: 1.9336401224136353
Epoch 10, training loss: 861.5599365234375 = 1.933971881866455 + 100.0 * 8.596260070800781
Epoch 10, val loss: 1.926518201828003
Epoch 20, training loss: 861.1046752929688 = 1.9240373373031616 + 100.0 * 8.591806411743164
Epoch 20, val loss: 1.917515516281128
Epoch 30, training loss: 858.0399780273438 = 1.9113000631332397 + 100.0 * 8.561286926269531
Epoch 30, val loss: 1.9057879447937012
Epoch 40, training loss: 842.0933837890625 = 1.895906686782837 + 100.0 * 8.40197467803955
Epoch 40, val loss: 1.8917559385299683
Epoch 50, training loss: 792.5889892578125 = 1.879494547843933 + 100.0 * 7.907094478607178
Epoch 50, val loss: 1.8761786222457886
Epoch 60, training loss: 753.2576904296875 = 1.8654195070266724 + 100.0 * 7.513922691345215
Epoch 60, val loss: 1.8633317947387695
Epoch 70, training loss: 724.7197875976562 = 1.8532556295394897 + 100.0 * 7.228664875030518
Epoch 70, val loss: 1.8521149158477783
Epoch 80, training loss: 707.7913208007812 = 1.8413809537887573 + 100.0 * 7.059499263763428
Epoch 80, val loss: 1.8413420915603638
Epoch 90, training loss: 693.8609008789062 = 1.8309359550476074 + 100.0 * 6.920299530029297
Epoch 90, val loss: 1.8317121267318726
Epoch 100, training loss: 684.6301879882812 = 1.8223580121994019 + 100.0 * 6.828077793121338
Epoch 100, val loss: 1.8235007524490356
Epoch 110, training loss: 677.6928100585938 = 1.814145803451538 + 100.0 * 6.758786678314209
Epoch 110, val loss: 1.8153809309005737
Epoch 120, training loss: 672.2089233398438 = 1.8058545589447021 + 100.0 * 6.704030513763428
Epoch 120, val loss: 1.8073813915252686
Epoch 130, training loss: 667.5380249023438 = 1.797580599784851 + 100.0 * 6.657403945922852
Epoch 130, val loss: 1.7995202541351318
Epoch 140, training loss: 663.789794921875 = 1.7894974946975708 + 100.0 * 6.6200032234191895
Epoch 140, val loss: 1.7916339635849
Epoch 150, training loss: 661.2994384765625 = 1.7812824249267578 + 100.0 * 6.595180988311768
Epoch 150, val loss: 1.7835018634796143
Epoch 160, training loss: 658.483642578125 = 1.7723305225372314 + 100.0 * 6.567112922668457
Epoch 160, val loss: 1.7749813795089722
Epoch 170, training loss: 656.263427734375 = 1.7629146575927734 + 100.0 * 6.545004844665527
Epoch 170, val loss: 1.766046166419983
Epoch 180, training loss: 654.441162109375 = 1.752765417098999 + 100.0 * 6.526884078979492
Epoch 180, val loss: 1.7566558122634888
Epoch 190, training loss: 652.989013671875 = 1.7416048049926758 + 100.0 * 6.512474536895752
Epoch 190, val loss: 1.7465239763259888
Epoch 200, training loss: 651.4534912109375 = 1.7295297384262085 + 100.0 * 6.497239589691162
Epoch 200, val loss: 1.7356125116348267
Epoch 210, training loss: 650.71240234375 = 1.7163726091384888 + 100.0 * 6.489960193634033
Epoch 210, val loss: 1.723778247833252
Epoch 220, training loss: 649.2006225585938 = 1.7018909454345703 + 100.0 * 6.474987030029297
Epoch 220, val loss: 1.7108829021453857
Epoch 230, training loss: 648.2147827148438 = 1.6862525939941406 + 100.0 * 6.465284824371338
Epoch 230, val loss: 1.6971083879470825
Epoch 240, training loss: 647.3016357421875 = 1.6693503856658936 + 100.0 * 6.45632266998291
Epoch 240, val loss: 1.6822309494018555
Epoch 250, training loss: 646.4010620117188 = 1.6511198282241821 + 100.0 * 6.4474992752075195
Epoch 250, val loss: 1.6664468050003052
Epoch 260, training loss: 645.6865234375 = 1.6316795349121094 + 100.0 * 6.440548419952393
Epoch 260, val loss: 1.64958918094635
Epoch 270, training loss: 644.898681640625 = 1.610801100730896 + 100.0 * 6.4328789710998535
Epoch 270, val loss: 1.6316943168640137
Epoch 280, training loss: 643.966064453125 = 1.5887141227722168 + 100.0 * 6.423773765563965
Epoch 280, val loss: 1.6130127906799316
Epoch 290, training loss: 643.1810913085938 = 1.5655688047409058 + 100.0 * 6.4161553382873535
Epoch 290, val loss: 1.5935014486312866
Epoch 300, training loss: 642.6950073242188 = 1.5410767793655396 + 100.0 * 6.411539554595947
Epoch 300, val loss: 1.573162317276001
Epoch 310, training loss: 642.0265502929688 = 1.5157952308654785 + 100.0 * 6.405107498168945
Epoch 310, val loss: 1.5522621870040894
Epoch 320, training loss: 641.1686401367188 = 1.4896187782287598 + 100.0 * 6.396790027618408
Epoch 320, val loss: 1.5309562683105469
Epoch 330, training loss: 640.738525390625 = 1.4627909660339355 + 100.0 * 6.392757415771484
Epoch 330, val loss: 1.5094068050384521
Epoch 340, training loss: 640.1826782226562 = 1.4353811740875244 + 100.0 * 6.387473106384277
Epoch 340, val loss: 1.4874330759048462
Epoch 350, training loss: 639.6612548828125 = 1.4074212312698364 + 100.0 * 6.382538318634033
Epoch 350, val loss: 1.465464472770691
Epoch 360, training loss: 639.3984375 = 1.3790875673294067 + 100.0 * 6.380193710327148
Epoch 360, val loss: 1.4434787034988403
Epoch 370, training loss: 638.6865234375 = 1.350713849067688 + 100.0 * 6.373358249664307
Epoch 370, val loss: 1.4216392040252686
Epoch 380, training loss: 638.147216796875 = 1.3222225904464722 + 100.0 * 6.368250370025635
Epoch 380, val loss: 1.4000452756881714
Epoch 390, training loss: 637.7935791015625 = 1.2937533855438232 + 100.0 * 6.3649983406066895
Epoch 390, val loss: 1.3788012266159058
Epoch 400, training loss: 637.4163818359375 = 1.2652487754821777 + 100.0 * 6.36151123046875
Epoch 400, val loss: 1.3573931455612183
Epoch 410, training loss: 637.0111083984375 = 1.2367655038833618 + 100.0 * 6.357743263244629
Epoch 410, val loss: 1.336615800857544
Epoch 420, training loss: 636.6508178710938 = 1.2085222005844116 + 100.0 * 6.3544230461120605
Epoch 420, val loss: 1.3161673545837402
Epoch 430, training loss: 636.4315185546875 = 1.1805704832077026 + 100.0 * 6.35250997543335
Epoch 430, val loss: 1.296038269996643
Epoch 440, training loss: 636.1089477539062 = 1.1526309251785278 + 100.0 * 6.349563121795654
Epoch 440, val loss: 1.2763644456863403
Epoch 450, training loss: 635.5693359375 = 1.1252224445343018 + 100.0 * 6.344440937042236
Epoch 450, val loss: 1.2571831941604614
Epoch 460, training loss: 635.1552124023438 = 1.098185420036316 + 100.0 * 6.340569972991943
Epoch 460, val loss: 1.238621473312378
Epoch 470, training loss: 635.1840209960938 = 1.0716053247451782 + 100.0 * 6.341124534606934
Epoch 470, val loss: 1.22072172164917
Epoch 480, training loss: 634.9868774414062 = 1.0453596115112305 + 100.0 * 6.339415073394775
Epoch 480, val loss: 1.2033178806304932
Epoch 490, training loss: 634.4094848632812 = 1.019624948501587 + 100.0 * 6.333898544311523
Epoch 490, val loss: 1.186650037765503
Epoch 500, training loss: 633.9820556640625 = 0.9945996999740601 + 100.0 * 6.329874038696289
Epoch 500, val loss: 1.1708014011383057
Epoch 510, training loss: 634.2265625 = 0.9701963663101196 + 100.0 * 6.332563400268555
Epoch 510, val loss: 1.1556434631347656
Epoch 520, training loss: 633.6316528320312 = 0.9461399912834167 + 100.0 * 6.326854705810547
Epoch 520, val loss: 1.1409755945205688
Epoch 530, training loss: 633.1876220703125 = 0.9228050112724304 + 100.0 * 6.322648525238037
Epoch 530, val loss: 1.1272506713867188
Epoch 540, training loss: 632.8844604492188 = 0.900209367275238 + 100.0 * 6.319842338562012
Epoch 540, val loss: 1.1143720149993896
Epoch 550, training loss: 632.718505859375 = 0.878296971321106 + 100.0 * 6.318402290344238
Epoch 550, val loss: 1.10227632522583
Epoch 560, training loss: 632.7955932617188 = 0.8568032383918762 + 100.0 * 6.319387912750244
Epoch 560, val loss: 1.0906635522842407
Epoch 570, training loss: 632.255126953125 = 0.83586186170578 + 100.0 * 6.314192771911621
Epoch 570, val loss: 1.079743504524231
Epoch 580, training loss: 631.9912109375 = 0.8156578540802002 + 100.0 * 6.311755657196045
Epoch 580, val loss: 1.0695558786392212
Epoch 590, training loss: 631.75341796875 = 0.7961117625236511 + 100.0 * 6.309573650360107
Epoch 590, val loss: 1.0601388216018677
Epoch 600, training loss: 632.4306640625 = 0.7772441506385803 + 100.0 * 6.316534519195557
Epoch 600, val loss: 1.0511071681976318
Epoch 610, training loss: 631.4432373046875 = 0.7582889795303345 + 100.0 * 6.306849479675293
Epoch 610, val loss: 1.0428645610809326
Epoch 620, training loss: 631.2324829101562 = 0.7401734590530396 + 100.0 * 6.304923057556152
Epoch 620, val loss: 1.0351375341415405
Epoch 630, training loss: 630.9573364257812 = 0.722688615322113 + 100.0 * 6.302346229553223
Epoch 630, val loss: 1.0280052423477173
Epoch 640, training loss: 631.01171875 = 0.7056873440742493 + 100.0 * 6.303060054779053
Epoch 640, val loss: 1.0211889743804932
Epoch 650, training loss: 631.2802734375 = 0.688772976398468 + 100.0 * 6.305914878845215
Epoch 650, val loss: 1.0150514841079712
Epoch 660, training loss: 630.4993286132812 = 0.6724337935447693 + 100.0 * 6.298269271850586
Epoch 660, val loss: 1.0090745687484741
Epoch 670, training loss: 630.286865234375 = 0.6565181016921997 + 100.0 * 6.296303749084473
Epoch 670, val loss: 1.0036072731018066
Epoch 680, training loss: 630.6803588867188 = 0.6410545110702515 + 100.0 * 6.300393104553223
Epoch 680, val loss: 0.9985698461532593
Epoch 690, training loss: 630.1043701171875 = 0.6258145570755005 + 100.0 * 6.294785976409912
Epoch 690, val loss: 0.993828535079956
Epoch 700, training loss: 629.8262939453125 = 0.6110774874687195 + 100.0 * 6.292152404785156
Epoch 700, val loss: 0.9895262718200684
Epoch 710, training loss: 629.7249755859375 = 0.5966378450393677 + 100.0 * 6.29128360748291
Epoch 710, val loss: 0.9855843782424927
Epoch 720, training loss: 629.8590698242188 = 0.5824276208877563 + 100.0 * 6.292766571044922
Epoch 720, val loss: 0.9817899465560913
Epoch 730, training loss: 629.4638671875 = 0.5684384703636169 + 100.0 * 6.288954734802246
Epoch 730, val loss: 0.9782696962356567
Epoch 740, training loss: 629.2453002929688 = 0.5548422932624817 + 100.0 * 6.286904335021973
Epoch 740, val loss: 0.9751346111297607
Epoch 750, training loss: 629.042724609375 = 0.5415725111961365 + 100.0 * 6.2850117683410645
Epoch 750, val loss: 0.9723520874977112
Epoch 760, training loss: 629.63037109375 = 0.5285313129425049 + 100.0 * 6.291018962860107
Epoch 760, val loss: 0.9697803854942322
Epoch 770, training loss: 629.3826293945312 = 0.5156114101409912 + 100.0 * 6.288670063018799
Epoch 770, val loss: 0.967121958732605
Epoch 780, training loss: 628.7455444335938 = 0.5028565526008606 + 100.0 * 6.282426834106445
Epoch 780, val loss: 0.9648911356925964
Epoch 790, training loss: 628.6570434570312 = 0.49047574400901794 + 100.0 * 6.281665802001953
Epoch 790, val loss: 0.9629725813865662
Epoch 800, training loss: 628.5663452148438 = 0.47834786772727966 + 100.0 * 6.280879974365234
Epoch 800, val loss: 0.9612281918525696
Epoch 810, training loss: 628.4657592773438 = 0.4664391279220581 + 100.0 * 6.279993534088135
Epoch 810, val loss: 0.9595853686332703
Epoch 820, training loss: 628.4957885742188 = 0.4547466039657593 + 100.0 * 6.280410289764404
Epoch 820, val loss: 0.9580630660057068
Epoch 830, training loss: 628.1769409179688 = 0.4430876672267914 + 100.0 * 6.27733850479126
Epoch 830, val loss: 0.9569723606109619
Epoch 840, training loss: 629.0587158203125 = 0.4318102300167084 + 100.0 * 6.286269187927246
Epoch 840, val loss: 0.9558505415916443
Epoch 850, training loss: 628.1676025390625 = 0.42041102051734924 + 100.0 * 6.277472019195557
Epoch 850, val loss: 0.9548717141151428
Epoch 860, training loss: 627.9168701171875 = 0.4093218445777893 + 100.0 * 6.275075912475586
Epoch 860, val loss: 0.9541429281234741
Epoch 870, training loss: 627.6149291992188 = 0.39858803153038025 + 100.0 * 6.2721638679504395
Epoch 870, val loss: 0.9536463022232056
Epoch 880, training loss: 627.5488891601562 = 0.388067364692688 + 100.0 * 6.271608352661133
Epoch 880, val loss: 0.9533407688140869
Epoch 890, training loss: 628.26123046875 = 0.3776957392692566 + 100.0 * 6.278835296630859
Epoch 890, val loss: 0.9531912207603455
Epoch 900, training loss: 627.5677490234375 = 0.3673945367336273 + 100.0 * 6.272003650665283
Epoch 900, val loss: 0.9527971148490906
Epoch 910, training loss: 627.6141967773438 = 0.3573145866394043 + 100.0 * 6.272569179534912
Epoch 910, val loss: 0.9528325796127319
Epoch 920, training loss: 627.3189697265625 = 0.34741976857185364 + 100.0 * 6.269715785980225
Epoch 920, val loss: 0.9531899690628052
Epoch 930, training loss: 627.190673828125 = 0.3378418982028961 + 100.0 * 6.268528461456299
Epoch 930, val loss: 0.953564465045929
Epoch 940, training loss: 627.147216796875 = 0.32846105098724365 + 100.0 * 6.268187522888184
Epoch 940, val loss: 0.9541585445404053
Epoch 950, training loss: 627.1642456054688 = 0.3193150758743286 + 100.0 * 6.268449306488037
Epoch 950, val loss: 0.9548373818397522
Epoch 960, training loss: 626.9734497070312 = 0.3103362023830414 + 100.0 * 6.266631126403809
Epoch 960, val loss: 0.9556580781936646
Epoch 970, training loss: 626.75048828125 = 0.30153888463974 + 100.0 * 6.264489650726318
Epoch 970, val loss: 0.956797182559967
Epoch 980, training loss: 626.9254150390625 = 0.2930852174758911 + 100.0 * 6.266323089599609
Epoch 980, val loss: 0.9580295085906982
Epoch 990, training loss: 626.5579223632812 = 0.284751832485199 + 100.0 * 6.262732028961182
Epoch 990, val loss: 0.9593929648399353
Epoch 1000, training loss: 626.462890625 = 0.2766914367675781 + 100.0 * 6.261862277984619
Epoch 1000, val loss: 0.9609184861183167
Epoch 1010, training loss: 626.5879516601562 = 0.26885637640953064 + 100.0 * 6.263191223144531
Epoch 1010, val loss: 0.9627047777175903
Epoch 1020, training loss: 626.3672485351562 = 0.2611828148365021 + 100.0 * 6.26106071472168
Epoch 1020, val loss: 0.9645301103591919
Epoch 1030, training loss: 626.2931518554688 = 0.2537485659122467 + 100.0 * 6.26039457321167
Epoch 1030, val loss: 0.9666136503219604
Epoch 1040, training loss: 626.54443359375 = 0.24660278856754303 + 100.0 * 6.262978553771973
Epoch 1040, val loss: 0.9688320755958557
Epoch 1050, training loss: 626.22265625 = 0.23963361978530884 + 100.0 * 6.259830474853516
Epoch 1050, val loss: 0.9708395600318909
Epoch 1060, training loss: 626.079833984375 = 0.23286189138889313 + 100.0 * 6.258470058441162
Epoch 1060, val loss: 0.9733322858810425
Epoch 1070, training loss: 626.279052734375 = 0.22641029953956604 + 100.0 * 6.260526180267334
Epoch 1070, val loss: 0.9758815169334412
Epoch 1080, training loss: 625.9276733398438 = 0.21997927129268646 + 100.0 * 6.257076740264893
Epoch 1080, val loss: 0.97843998670578
Epoch 1090, training loss: 626.0458984375 = 0.21385955810546875 + 100.0 * 6.258320331573486
Epoch 1090, val loss: 0.9811107516288757
Epoch 1100, training loss: 625.8764038085938 = 0.2079247683286667 + 100.0 * 6.256684303283691
Epoch 1100, val loss: 0.9839997291564941
Epoch 1110, training loss: 625.6278686523438 = 0.2022288590669632 + 100.0 * 6.254256725311279
Epoch 1110, val loss: 0.987143874168396
Epoch 1120, training loss: 625.695556640625 = 0.19673512876033783 + 100.0 * 6.254988193511963
Epoch 1120, val loss: 0.9903371334075928
Epoch 1130, training loss: 625.9695434570312 = 0.1913880705833435 + 100.0 * 6.257781505584717
Epoch 1130, val loss: 0.9934116005897522
Epoch 1140, training loss: 625.5247192382812 = 0.18613195419311523 + 100.0 * 6.253385543823242
Epoch 1140, val loss: 0.9966593384742737
Epoch 1150, training loss: 625.3790283203125 = 0.18114668130874634 + 100.0 * 6.251978874206543
Epoch 1150, val loss: 1.0000537633895874
Epoch 1160, training loss: 625.6912841796875 = 0.1763470470905304 + 100.0 * 6.2551493644714355
Epoch 1160, val loss: 1.0035192966461182
Epoch 1170, training loss: 625.6283569335938 = 0.17156749963760376 + 100.0 * 6.254568099975586
Epoch 1170, val loss: 1.007006049156189
Epoch 1180, training loss: 625.3541259765625 = 0.16703881323337555 + 100.0 * 6.251870632171631
Epoch 1180, val loss: 1.0106009244918823
Epoch 1190, training loss: 625.134521484375 = 0.16265542805194855 + 100.0 * 6.24971866607666
Epoch 1190, val loss: 1.0144054889678955
Epoch 1200, training loss: 625.1786499023438 = 0.1584795117378235 + 100.0 * 6.250201225280762
Epoch 1200, val loss: 1.0181819200515747
Epoch 1210, training loss: 625.2233276367188 = 0.15434527397155762 + 100.0 * 6.25068998336792
Epoch 1210, val loss: 1.0219680070877075
Epoch 1220, training loss: 624.9513549804688 = 0.15033583343029022 + 100.0 * 6.248010158538818
Epoch 1220, val loss: 1.0257354974746704
Epoch 1230, training loss: 624.8931274414062 = 0.14651811122894287 + 100.0 * 6.24746561050415
Epoch 1230, val loss: 1.0296889543533325
Epoch 1240, training loss: 624.8443603515625 = 0.14282512664794922 + 100.0 * 6.247015476226807
Epoch 1240, val loss: 1.0337352752685547
Epoch 1250, training loss: 626.10205078125 = 0.1392720490694046 + 100.0 * 6.259627819061279
Epoch 1250, val loss: 1.0377095937728882
Epoch 1260, training loss: 624.9635009765625 = 0.13563035428524017 + 100.0 * 6.248279094696045
Epoch 1260, val loss: 1.0414249897003174
Epoch 1270, training loss: 624.827392578125 = 0.13223816454410553 + 100.0 * 6.246951103210449
Epoch 1270, val loss: 1.0454330444335938
Epoch 1280, training loss: 624.603759765625 = 0.1289917528629303 + 100.0 * 6.244747638702393
Epoch 1280, val loss: 1.0496894121170044
Epoch 1290, training loss: 624.6665649414062 = 0.12586238980293274 + 100.0 * 6.2454071044921875
Epoch 1290, val loss: 1.0538538694381714
Epoch 1300, training loss: 625.130126953125 = 0.12279806286096573 + 100.0 * 6.250073432922363
Epoch 1300, val loss: 1.0579125881195068
Epoch 1310, training loss: 624.6036987304688 = 0.1197640523314476 + 100.0 * 6.244839191436768
Epoch 1310, val loss: 1.062178373336792
Epoch 1320, training loss: 624.4263916015625 = 0.11687558889389038 + 100.0 * 6.2430949211120605
Epoch 1320, val loss: 1.0664530992507935
Epoch 1330, training loss: 624.5054931640625 = 0.11409192532300949 + 100.0 * 6.2439141273498535
Epoch 1330, val loss: 1.0709056854248047
Epoch 1340, training loss: 624.648193359375 = 0.11137979477643967 + 100.0 * 6.245368003845215
Epoch 1340, val loss: 1.0751205682754517
Epoch 1350, training loss: 624.6682739257812 = 0.10867945104837418 + 100.0 * 6.245595455169678
Epoch 1350, val loss: 1.0793707370758057
Epoch 1360, training loss: 624.2913818359375 = 0.10608033090829849 + 100.0 * 6.241852760314941
Epoch 1360, val loss: 1.083734154701233
Epoch 1370, training loss: 624.2353515625 = 0.10359178483486176 + 100.0 * 6.2413177490234375
Epoch 1370, val loss: 1.0882151126861572
Epoch 1380, training loss: 624.2788696289062 = 0.10118038952350616 + 100.0 * 6.241776466369629
Epoch 1380, val loss: 1.0927878618240356
Epoch 1390, training loss: 624.4237060546875 = 0.09883400797843933 + 100.0 * 6.24324893951416
Epoch 1390, val loss: 1.0972241163253784
Epoch 1400, training loss: 624.313232421875 = 0.09652945399284363 + 100.0 * 6.242166519165039
Epoch 1400, val loss: 1.101454257965088
Epoch 1410, training loss: 624.2637939453125 = 0.09428834170103073 + 100.0 * 6.241694927215576
Epoch 1410, val loss: 1.1059796810150146
Epoch 1420, training loss: 624.1757202148438 = 0.0921398252248764 + 100.0 * 6.240835666656494
Epoch 1420, val loss: 1.1105046272277832
Epoch 1430, training loss: 624.8809814453125 = 0.09001940488815308 + 100.0 * 6.2479095458984375
Epoch 1430, val loss: 1.1149499416351318
Epoch 1440, training loss: 623.9353637695312 = 0.08792557567358017 + 100.0 * 6.238474369049072
Epoch 1440, val loss: 1.1191725730895996
Epoch 1450, training loss: 623.8997802734375 = 0.08593878895044327 + 100.0 * 6.238138198852539
Epoch 1450, val loss: 1.1238932609558105
Epoch 1460, training loss: 623.8031005859375 = 0.0840272381901741 + 100.0 * 6.2371907234191895
Epoch 1460, val loss: 1.1286131143569946
Epoch 1470, training loss: 623.7462158203125 = 0.08217199146747589 + 100.0 * 6.236640453338623
Epoch 1470, val loss: 1.1332439184188843
Epoch 1480, training loss: 624.5226440429688 = 0.08035802096128464 + 100.0 * 6.244422435760498
Epoch 1480, val loss: 1.1379274129867554
Epoch 1490, training loss: 624.2380981445312 = 0.0785582959651947 + 100.0 * 6.24159574508667
Epoch 1490, val loss: 1.142137050628662
Epoch 1500, training loss: 623.7835693359375 = 0.07677385210990906 + 100.0 * 6.237068176269531
Epoch 1500, val loss: 1.1467230319976807
Epoch 1510, training loss: 623.5921020507812 = 0.0751124694943428 + 100.0 * 6.235169887542725
Epoch 1510, val loss: 1.1514607667922974
Epoch 1520, training loss: 623.5921630859375 = 0.07350827753543854 + 100.0 * 6.23518705368042
Epoch 1520, val loss: 1.1561434268951416
Epoch 1530, training loss: 624.70751953125 = 0.07198359817266464 + 100.0 * 6.2463555335998535
Epoch 1530, val loss: 1.1606948375701904
Epoch 1540, training loss: 623.924072265625 = 0.07033637166023254 + 100.0 * 6.238537788391113
Epoch 1540, val loss: 1.1651815176010132
Epoch 1550, training loss: 623.61279296875 = 0.06883928924798965 + 100.0 * 6.235439300537109
Epoch 1550, val loss: 1.1698074340820312
Epoch 1560, training loss: 623.8036499023438 = 0.06737389415502548 + 100.0 * 6.237362384796143
Epoch 1560, val loss: 1.1744210720062256
Epoch 1570, training loss: 623.7122192382812 = 0.06592877209186554 + 100.0 * 6.2364630699157715
Epoch 1570, val loss: 1.1788723468780518
Epoch 1580, training loss: 623.8074951171875 = 0.06454362720251083 + 100.0 * 6.237430095672607
Epoch 1580, val loss: 1.183458685874939
Epoch 1590, training loss: 623.4716796875 = 0.06316950917243958 + 100.0 * 6.2340850830078125
Epoch 1590, val loss: 1.1881341934204102
Epoch 1600, training loss: 623.307373046875 = 0.06186169758439064 + 100.0 * 6.232454776763916
Epoch 1600, val loss: 1.192794919013977
Epoch 1610, training loss: 623.3737182617188 = 0.06059844791889191 + 100.0 * 6.233130931854248
Epoch 1610, val loss: 1.197398066520691
Epoch 1620, training loss: 623.69384765625 = 0.059345681220293045 + 100.0 * 6.236345291137695
Epoch 1620, val loss: 1.2019572257995605
Epoch 1630, training loss: 623.244873046875 = 0.05811423435807228 + 100.0 * 6.231867790222168
Epoch 1630, val loss: 1.2064008712768555
Epoch 1640, training loss: 623.1697387695312 = 0.05692741647362709 + 100.0 * 6.231127738952637
Epoch 1640, val loss: 1.2110090255737305
Epoch 1650, training loss: 624.470703125 = 0.05580369383096695 + 100.0 * 6.244149208068848
Epoch 1650, val loss: 1.215369701385498
Epoch 1660, training loss: 623.6057739257812 = 0.05463394150137901 + 100.0 * 6.235511779785156
Epoch 1660, val loss: 1.2198870182037354
Epoch 1670, training loss: 623.1612548828125 = 0.053523704409599304 + 100.0 * 6.231077194213867
Epoch 1670, val loss: 1.2243732213974
Epoch 1680, training loss: 623.0228881835938 = 0.05246984213590622 + 100.0 * 6.229703903198242
Epoch 1680, val loss: 1.229060411453247
Epoch 1690, training loss: 623.0786743164062 = 0.05145205184817314 + 100.0 * 6.23027229309082
Epoch 1690, val loss: 1.233667016029358
Epoch 1700, training loss: 623.4896850585938 = 0.050449784845113754 + 100.0 * 6.234392166137695
Epoch 1700, val loss: 1.2380887269973755
Epoch 1710, training loss: 623.3407592773438 = 0.04943962022662163 + 100.0 * 6.232913494110107
Epoch 1710, val loss: 1.2424477338790894
Epoch 1720, training loss: 623.0732421875 = 0.048464249819517136 + 100.0 * 6.230247974395752
Epoch 1720, val loss: 1.2468942403793335
Epoch 1730, training loss: 623.1114501953125 = 0.0475308820605278 + 100.0 * 6.2306389808654785
Epoch 1730, val loss: 1.251287817955017
Epoch 1740, training loss: 623.3196411132812 = 0.04663094878196716 + 100.0 * 6.232730388641357
Epoch 1740, val loss: 1.2555794715881348
Epoch 1750, training loss: 622.954833984375 = 0.045711271464824677 + 100.0 * 6.229091167449951
Epoch 1750, val loss: 1.2601051330566406
Epoch 1760, training loss: 622.8566284179688 = 0.04483649134635925 + 100.0 * 6.2281174659729
Epoch 1760, val loss: 1.2644661664962769
Epoch 1770, training loss: 622.85595703125 = 0.04400324821472168 + 100.0 * 6.228119373321533
Epoch 1770, val loss: 1.268998146057129
Epoch 1780, training loss: 623.3368530273438 = 0.04317755624651909 + 100.0 * 6.232936859130859
Epoch 1780, val loss: 1.2731918096542358
Epoch 1790, training loss: 622.8445434570312 = 0.0423642173409462 + 100.0 * 6.22802209854126
Epoch 1790, val loss: 1.2775354385375977
Epoch 1800, training loss: 623.1785888671875 = 0.04157938063144684 + 100.0 * 6.231369972229004
Epoch 1800, val loss: 1.2817561626434326
Epoch 1810, training loss: 622.7366943359375 = 0.040796682238578796 + 100.0 * 6.226959228515625
Epoch 1810, val loss: 1.2860420942306519
Epoch 1820, training loss: 622.7371215820312 = 0.04004758596420288 + 100.0 * 6.226970672607422
Epoch 1820, val loss: 1.290412425994873
Epoch 1830, training loss: 622.649658203125 = 0.03933282941579819 + 100.0 * 6.226103782653809
Epoch 1830, val loss: 1.2946796417236328
Epoch 1840, training loss: 623.1140747070312 = 0.03864317759871483 + 100.0 * 6.230754375457764
Epoch 1840, val loss: 1.2988191843032837
Epoch 1850, training loss: 622.8433837890625 = 0.03791609779000282 + 100.0 * 6.228054523468018
Epoch 1850, val loss: 1.3029956817626953
Epoch 1860, training loss: 623.2899780273438 = 0.03723158314824104 + 100.0 * 6.232527256011963
Epoch 1860, val loss: 1.306868553161621
Epoch 1870, training loss: 622.6795043945312 = 0.03653851896524429 + 100.0 * 6.226429462432861
Epoch 1870, val loss: 1.3110779523849487
Epoch 1880, training loss: 622.5459594726562 = 0.03589920327067375 + 100.0 * 6.225100994110107
Epoch 1880, val loss: 1.3153252601623535
Epoch 1890, training loss: 622.4508056640625 = 0.035276468843221664 + 100.0 * 6.224155426025391
Epoch 1890, val loss: 1.3195174932479858
Epoch 1900, training loss: 622.4240112304688 = 0.03467486798763275 + 100.0 * 6.223893165588379
Epoch 1900, val loss: 1.3236290216445923
Epoch 1910, training loss: 622.798095703125 = 0.03408773988485336 + 100.0 * 6.227640628814697
Epoch 1910, val loss: 1.3275421857833862
Epoch 1920, training loss: 622.5739135742188 = 0.033491428941488266 + 100.0 * 6.225404262542725
Epoch 1920, val loss: 1.3316166400909424
Epoch 1930, training loss: 622.5521240234375 = 0.03290802985429764 + 100.0 * 6.225192070007324
Epoch 1930, val loss: 1.3353937864303589
Epoch 1940, training loss: 623.107666015625 = 0.03233017772436142 + 100.0 * 6.230752944946289
Epoch 1940, val loss: 1.3395167589187622
Epoch 1950, training loss: 622.5228881835938 = 0.031774040311574936 + 100.0 * 6.224910736083984
Epoch 1950, val loss: 1.343418836593628
Epoch 1960, training loss: 622.27392578125 = 0.0312424898147583 + 100.0 * 6.222426891326904
Epoch 1960, val loss: 1.3475422859191895
Epoch 1970, training loss: 622.2285766601562 = 0.03073321469128132 + 100.0 * 6.221978187561035
Epoch 1970, val loss: 1.3516242504119873
Epoch 1980, training loss: 622.2703857421875 = 0.03023936226963997 + 100.0 * 6.2224016189575195
Epoch 1980, val loss: 1.3556149005889893
Epoch 1990, training loss: 622.9724731445312 = 0.029754092916846275 + 100.0 * 6.229427337646484
Epoch 1990, val loss: 1.3593837022781372
Epoch 2000, training loss: 622.5087890625 = 0.029238471761345863 + 100.0 * 6.224795818328857
Epoch 2000, val loss: 1.3632491827011108
Epoch 2010, training loss: 622.4149780273438 = 0.02876649983227253 + 100.0 * 6.223862171173096
Epoch 2010, val loss: 1.3671014308929443
Epoch 2020, training loss: 622.6488037109375 = 0.028295213356614113 + 100.0 * 6.226204872131348
Epoch 2020, val loss: 1.3709908723831177
Epoch 2030, training loss: 622.3234252929688 = 0.027840016409754753 + 100.0 * 6.22295618057251
Epoch 2030, val loss: 1.3746681213378906
Epoch 2040, training loss: 622.1281127929688 = 0.02739284373819828 + 100.0 * 6.221007347106934
Epoch 2040, val loss: 1.3786238431930542
Epoch 2050, training loss: 622.361572265625 = 0.026967788115143776 + 100.0 * 6.223345756530762
Epoch 2050, val loss: 1.382597804069519
Epoch 2060, training loss: 622.1925659179688 = 0.026540184393525124 + 100.0 * 6.221660137176514
Epoch 2060, val loss: 1.386033058166504
Epoch 2070, training loss: 622.2950439453125 = 0.026132013648748398 + 100.0 * 6.222689151763916
Epoch 2070, val loss: 1.3896801471710205
Epoch 2080, training loss: 622.173583984375 = 0.025715844705700874 + 100.0 * 6.2214789390563965
Epoch 2080, val loss: 1.3934932947158813
Epoch 2090, training loss: 622.131103515625 = 0.025321045890450478 + 100.0 * 6.221057891845703
Epoch 2090, val loss: 1.3971376419067383
Epoch 2100, training loss: 622.6131591796875 = 0.02493499591946602 + 100.0 * 6.225882053375244
Epoch 2100, val loss: 1.4007413387298584
Epoch 2110, training loss: 622.151123046875 = 0.02454819157719612 + 100.0 * 6.22126579284668
Epoch 2110, val loss: 1.4042636156082153
Epoch 2120, training loss: 621.9847412109375 = 0.024174321442842484 + 100.0 * 6.219605445861816
Epoch 2120, val loss: 1.4079763889312744
Epoch 2130, training loss: 621.9405517578125 = 0.02382095344364643 + 100.0 * 6.219167709350586
Epoch 2130, val loss: 1.4116474390029907
Epoch 2140, training loss: 622.38623046875 = 0.023473281413316727 + 100.0 * 6.22362756729126
Epoch 2140, val loss: 1.4151164293289185
Epoch 2150, training loss: 621.9030151367188 = 0.0231227595359087 + 100.0 * 6.218799114227295
Epoch 2150, val loss: 1.4186944961547852
Epoch 2160, training loss: 622.0296020507812 = 0.02278289571404457 + 100.0 * 6.220068454742432
Epoch 2160, val loss: 1.4222520589828491
Epoch 2170, training loss: 622.2613525390625 = 0.022449498996138573 + 100.0 * 6.222389221191406
Epoch 2170, val loss: 1.4255748987197876
Epoch 2180, training loss: 622.0888671875 = 0.02210681512951851 + 100.0 * 6.220667839050293
Epoch 2180, val loss: 1.429049015045166
Epoch 2190, training loss: 622.881103515625 = 0.021796979010105133 + 100.0 * 6.228592872619629
Epoch 2190, val loss: 1.4321832656860352
Epoch 2200, training loss: 622.0474853515625 = 0.021457474678754807 + 100.0 * 6.220260143280029
Epoch 2200, val loss: 1.4357621669769287
Epoch 2210, training loss: 621.8125 = 0.021153269335627556 + 100.0 * 6.2179131507873535
Epoch 2210, val loss: 1.4391510486602783
Epoch 2220, training loss: 621.7172241210938 = 0.020859893411397934 + 100.0 * 6.216963291168213
Epoch 2220, val loss: 1.442838430404663
Epoch 2230, training loss: 621.6746215820312 = 0.02057669125497341 + 100.0 * 6.216540813446045
Epoch 2230, val loss: 1.446360468864441
Epoch 2240, training loss: 621.664306640625 = 0.02029946818947792 + 100.0 * 6.216439723968506
Epoch 2240, val loss: 1.449773907661438
Epoch 2250, training loss: 622.0543823242188 = 0.02003241516649723 + 100.0 * 6.220343589782715
Epoch 2250, val loss: 1.4530209302902222
Epoch 2260, training loss: 621.9906616210938 = 0.019746961072087288 + 100.0 * 6.2197089195251465
Epoch 2260, val loss: 1.4559388160705566
Epoch 2270, training loss: 621.7238159179688 = 0.019464194774627686 + 100.0 * 6.217043399810791
Epoch 2270, val loss: 1.4591885805130005
Epoch 2280, training loss: 621.6688232421875 = 0.019198520109057426 + 100.0 * 6.216495990753174
Epoch 2280, val loss: 1.4624545574188232
Epoch 2290, training loss: 621.7669067382812 = 0.018945811316370964 + 100.0 * 6.217479705810547
Epoch 2290, val loss: 1.465885877609253
Epoch 2300, training loss: 622.5010986328125 = 0.018694797530770302 + 100.0 * 6.224823951721191
Epoch 2300, val loss: 1.4692302942276
Epoch 2310, training loss: 621.758544921875 = 0.018430598080158234 + 100.0 * 6.217401027679443
Epoch 2310, val loss: 1.4720104932785034
Epoch 2320, training loss: 621.5747680664062 = 0.018184645101428032 + 100.0 * 6.2155656814575195
Epoch 2320, val loss: 1.4753035306930542
Epoch 2330, training loss: 621.5420532226562 = 0.017955800518393517 + 100.0 * 6.215240955352783
Epoch 2330, val loss: 1.4787232875823975
Epoch 2340, training loss: 621.5943603515625 = 0.017731288447976112 + 100.0 * 6.215765953063965
Epoch 2340, val loss: 1.4817891120910645
Epoch 2350, training loss: 622.2095947265625 = 0.017507635056972504 + 100.0 * 6.221920490264893
Epoch 2350, val loss: 1.4847089052200317
Epoch 2360, training loss: 621.6358032226562 = 0.017262399196624756 + 100.0 * 6.216185092926025
Epoch 2360, val loss: 1.4877723455429077
Epoch 2370, training loss: 621.47314453125 = 0.017046304419636726 + 100.0 * 6.2145609855651855
Epoch 2370, val loss: 1.4908807277679443
Epoch 2380, training loss: 621.5838623046875 = 0.016838351264595985 + 100.0 * 6.215670108795166
Epoch 2380, val loss: 1.494071364402771
Epoch 2390, training loss: 621.8217163085938 = 0.016633108258247375 + 100.0 * 6.218050479888916
Epoch 2390, val loss: 1.4969456195831299
Epoch 2400, training loss: 621.7343139648438 = 0.01641053520143032 + 100.0 * 6.217178821563721
Epoch 2400, val loss: 1.4998403787612915
Epoch 2410, training loss: 621.6158447265625 = 0.0162088330835104 + 100.0 * 6.215996265411377
Epoch 2410, val loss: 1.502864956855774
Epoch 2420, training loss: 621.5860595703125 = 0.016009071841835976 + 100.0 * 6.215700626373291
Epoch 2420, val loss: 1.5058255195617676
Epoch 2430, training loss: 621.4984130859375 = 0.015811441466212273 + 100.0 * 6.2148261070251465
Epoch 2430, val loss: 1.5087714195251465
Epoch 2440, training loss: 621.580322265625 = 0.015615281648933887 + 100.0 * 6.215647220611572
Epoch 2440, val loss: 1.5116894245147705
Epoch 2450, training loss: 621.4324340820312 = 0.015433025546371937 + 100.0 * 6.214169979095459
Epoch 2450, val loss: 1.5147825479507446
Epoch 2460, training loss: 621.2946166992188 = 0.01524662971496582 + 100.0 * 6.212793350219727
Epoch 2460, val loss: 1.517709493637085
Epoch 2470, training loss: 621.93017578125 = 0.015071704983711243 + 100.0 * 6.219151020050049
Epoch 2470, val loss: 1.520405888557434
Epoch 2480, training loss: 621.6326904296875 = 0.01488393172621727 + 100.0 * 6.216177940368652
Epoch 2480, val loss: 1.523229956626892
Epoch 2490, training loss: 621.4129638671875 = 0.014699582010507584 + 100.0 * 6.213982582092285
Epoch 2490, val loss: 1.5259746313095093
Epoch 2500, training loss: 621.2926635742188 = 0.014529932290315628 + 100.0 * 6.2127814292907715
Epoch 2500, val loss: 1.5290833711624146
Epoch 2510, training loss: 621.21630859375 = 0.014365501701831818 + 100.0 * 6.212019443511963
Epoch 2510, val loss: 1.532016396522522
Epoch 2520, training loss: 621.4635620117188 = 0.014208548702299595 + 100.0 * 6.214493274688721
Epoch 2520, val loss: 1.5350373983383179
Epoch 2530, training loss: 621.3613891601562 = 0.014040662907063961 + 100.0 * 6.213473320007324
Epoch 2530, val loss: 1.5375045537948608
Epoch 2540, training loss: 621.6827392578125 = 0.013877885416150093 + 100.0 * 6.216688632965088
Epoch 2540, val loss: 1.5400390625
Epoch 2550, training loss: 621.4722290039062 = 0.013715896755456924 + 100.0 * 6.214584827423096
Epoch 2550, val loss: 1.542633295059204
Epoch 2560, training loss: 621.1669311523438 = 0.01355680264532566 + 100.0 * 6.211534023284912
Epoch 2560, val loss: 1.5453556776046753
Epoch 2570, training loss: 621.121826171875 = 0.013407489284873009 + 100.0 * 6.211083889007568
Epoch 2570, val loss: 1.5482165813446045
Epoch 2580, training loss: 621.3120727539062 = 0.013265454210340977 + 100.0 * 6.212988376617432
Epoch 2580, val loss: 1.550994634628296
Epoch 2590, training loss: 621.5257568359375 = 0.013117673806846142 + 100.0 * 6.2151265144348145
Epoch 2590, val loss: 1.5534861087799072
Epoch 2600, training loss: 621.3458251953125 = 0.012969905510544777 + 100.0 * 6.213328838348389
Epoch 2600, val loss: 1.5558016300201416
Epoch 2610, training loss: 621.2073364257812 = 0.012827749364078045 + 100.0 * 6.211945056915283
Epoch 2610, val loss: 1.5585119724273682
Epoch 2620, training loss: 621.2747802734375 = 0.01269417256116867 + 100.0 * 6.212620735168457
Epoch 2620, val loss: 1.5612167119979858
Epoch 2630, training loss: 621.1279907226562 = 0.012554076500236988 + 100.0 * 6.211153984069824
Epoch 2630, val loss: 1.5636241436004639
Epoch 2640, training loss: 621.2100830078125 = 0.012422014959156513 + 100.0 * 6.211976051330566
Epoch 2640, val loss: 1.5662227869033813
Epoch 2650, training loss: 621.2227172851562 = 0.012292147614061832 + 100.0 * 6.212104320526123
Epoch 2650, val loss: 1.5688260793685913
Epoch 2660, training loss: 621.1661987304688 = 0.012162878178060055 + 100.0 * 6.211540222167969
Epoch 2660, val loss: 1.5711958408355713
Epoch 2670, training loss: 621.52099609375 = 0.01203279010951519 + 100.0 * 6.215089797973633
Epoch 2670, val loss: 1.5735797882080078
Epoch 2680, training loss: 621.3212280273438 = 0.011903016828000546 + 100.0 * 6.213093280792236
Epoch 2680, val loss: 1.5761590003967285
Epoch 2690, training loss: 621.1542358398438 = 0.011778358370065689 + 100.0 * 6.211424827575684
Epoch 2690, val loss: 1.5785619020462036
Epoch 2700, training loss: 621.0465087890625 = 0.011657528579235077 + 100.0 * 6.210348606109619
Epoch 2700, val loss: 1.5810198783874512
Epoch 2710, training loss: 620.9122924804688 = 0.011539984494447708 + 100.0 * 6.209007263183594
Epoch 2710, val loss: 1.5834851264953613
Epoch 2720, training loss: 620.958984375 = 0.011427243240177631 + 100.0 * 6.209475994110107
Epoch 2720, val loss: 1.586004376411438
Epoch 2730, training loss: 621.6175537109375 = 0.011315659619867802 + 100.0 * 6.216062545776367
Epoch 2730, val loss: 1.5884654521942139
Epoch 2740, training loss: 621.1373291015625 = 0.011197212152183056 + 100.0 * 6.211261749267578
Epoch 2740, val loss: 1.5904276371002197
Epoch 2750, training loss: 620.8998413085938 = 0.011083348654210567 + 100.0 * 6.208887577056885
Epoch 2750, val loss: 1.5927088260650635
Epoch 2760, training loss: 620.8790893554688 = 0.010976726189255714 + 100.0 * 6.208681106567383
Epoch 2760, val loss: 1.5952554941177368
Epoch 2770, training loss: 621.6162719726562 = 0.010875680483877659 + 100.0 * 6.2160539627075195
Epoch 2770, val loss: 1.59755539894104
Epoch 2780, training loss: 621.1546020507812 = 0.010759588330984116 + 100.0 * 6.2114386558532715
Epoch 2780, val loss: 1.5995420217514038
Epoch 2790, training loss: 621.27001953125 = 0.010654326528310776 + 100.0 * 6.2125935554504395
Epoch 2790, val loss: 1.601779580116272
Epoch 2800, training loss: 620.9404296875 = 0.010551226325333118 + 100.0 * 6.209298610687256
Epoch 2800, val loss: 1.6039923429489136
Epoch 2810, training loss: 620.8123779296875 = 0.010450012050569057 + 100.0 * 6.208019256591797
Epoch 2810, val loss: 1.6064791679382324
Epoch 2820, training loss: 620.810302734375 = 0.010352312587201595 + 100.0 * 6.207999229431152
Epoch 2820, val loss: 1.608863115310669
Epoch 2830, training loss: 621.06396484375 = 0.01025744155049324 + 100.0 * 6.210536956787109
Epoch 2830, val loss: 1.611270785331726
Epoch 2840, training loss: 621.1650390625 = 0.010162843391299248 + 100.0 * 6.211548328399658
Epoch 2840, val loss: 1.613134741783142
Epoch 2850, training loss: 620.8966674804688 = 0.010063168592751026 + 100.0 * 6.208866119384766
Epoch 2850, val loss: 1.6150023937225342
Epoch 2860, training loss: 620.831787109375 = 0.009967779740691185 + 100.0 * 6.208218097686768
Epoch 2860, val loss: 1.6172986030578613
Epoch 2870, training loss: 620.8184204101562 = 0.009876617230474949 + 100.0 * 6.208085536956787
Epoch 2870, val loss: 1.6195363998413086
Epoch 2880, training loss: 620.988525390625 = 0.00978629570454359 + 100.0 * 6.209787845611572
Epoch 2880, val loss: 1.6216118335723877
Epoch 2890, training loss: 620.738037109375 = 0.00969698466360569 + 100.0 * 6.2072834968566895
Epoch 2890, val loss: 1.6236870288848877
Epoch 2900, training loss: 621.2499389648438 = 0.009612224996089935 + 100.0 * 6.212403774261475
Epoch 2900, val loss: 1.6256135702133179
Epoch 2910, training loss: 621.0222778320312 = 0.009522253647446632 + 100.0 * 6.210127353668213
Epoch 2910, val loss: 1.6274462938308716
Epoch 2920, training loss: 620.6699829101562 = 0.009434441104531288 + 100.0 * 6.206605434417725
Epoch 2920, val loss: 1.6296724081039429
Epoch 2930, training loss: 620.6348266601562 = 0.009353130124509335 + 100.0 * 6.206254959106445
Epoch 2930, val loss: 1.631899118423462
Epoch 2940, training loss: 620.6209716796875 = 0.00927452277392149 + 100.0 * 6.206116676330566
Epoch 2940, val loss: 1.6339800357818604
Epoch 2950, training loss: 621.2860107421875 = 0.00919993408024311 + 100.0 * 6.212768077850342
Epoch 2950, val loss: 1.6357733011245728
Epoch 2960, training loss: 620.9000244140625 = 0.009117730893194675 + 100.0 * 6.208909034729004
Epoch 2960, val loss: 1.637447476387024
Epoch 2970, training loss: 620.871337890625 = 0.009035518392920494 + 100.0 * 6.208622932434082
Epoch 2970, val loss: 1.6391818523406982
Epoch 2980, training loss: 620.555419921875 = 0.008955789729952812 + 100.0 * 6.2054643630981445
Epoch 2980, val loss: 1.6415678262710571
Epoch 2990, training loss: 620.6279296875 = 0.008883611299097538 + 100.0 * 6.20619010925293
Epoch 2990, val loss: 1.6437445878982544
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8091723774380601
The final CL Acc:0.70617, 0.02810, The final GNN Acc:0.81286, 0.00269
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 7928])
updated graph: torch.Size([2, 10524])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6417846679688 = 1.955375075340271 + 100.0 * 8.596863746643066
Epoch 0, val loss: 1.9604915380477905
Epoch 10, training loss: 861.56494140625 = 1.9464530944824219 + 100.0 * 8.596184730529785
Epoch 10, val loss: 1.9509923458099365
Epoch 20, training loss: 861.0497436523438 = 1.9353049993515015 + 100.0 * 8.591144561767578
Epoch 20, val loss: 1.9390263557434082
Epoch 30, training loss: 857.2476196289062 = 1.920776605606079 + 100.0 * 8.553268432617188
Epoch 30, val loss: 1.923304557800293
Epoch 40, training loss: 830.5609130859375 = 1.9023640155792236 + 100.0 * 8.286585807800293
Epoch 40, val loss: 1.9035992622375488
Epoch 50, training loss: 771.2858276367188 = 1.8802698850631714 + 100.0 * 7.694056034088135
Epoch 50, val loss: 1.8804048299789429
Epoch 60, training loss: 760.7632446289062 = 1.8607019186019897 + 100.0 * 7.589025497436523
Epoch 60, val loss: 1.8619970083236694
Epoch 70, training loss: 748.6704711914062 = 1.8457883596420288 + 100.0 * 7.468246936798096
Epoch 70, val loss: 1.8486725091934204
Epoch 80, training loss: 732.9804077148438 = 1.8319789171218872 + 100.0 * 7.311484336853027
Epoch 80, val loss: 1.8363312482833862
Epoch 90, training loss: 713.0137329101562 = 1.8206292390823364 + 100.0 * 7.111930847167969
Epoch 90, val loss: 1.8261761665344238
Epoch 100, training loss: 697.7042846679688 = 1.8117659091949463 + 100.0 * 6.958925247192383
Epoch 100, val loss: 1.8175493478775024
Epoch 110, training loss: 686.2069091796875 = 1.8031432628631592 + 100.0 * 6.8440375328063965
Epoch 110, val loss: 1.8093746900558472
Epoch 120, training loss: 679.2014770507812 = 1.7927230596542358 + 100.0 * 6.774087429046631
Epoch 120, val loss: 1.7996256351470947
Epoch 130, training loss: 673.4824829101562 = 1.7812955379486084 + 100.0 * 6.717011451721191
Epoch 130, val loss: 1.7892838716506958
Epoch 140, training loss: 669.0912475585938 = 1.770461916923523 + 100.0 * 6.673207759857178
Epoch 140, val loss: 1.779165267944336
Epoch 150, training loss: 665.4940185546875 = 1.7587921619415283 + 100.0 * 6.637352466583252
Epoch 150, val loss: 1.768225073814392
Epoch 160, training loss: 662.6429443359375 = 1.7459309101104736 + 100.0 * 6.6089701652526855
Epoch 160, val loss: 1.7563531398773193
Epoch 170, training loss: 660.0811767578125 = 1.7319974899291992 + 100.0 * 6.583491802215576
Epoch 170, val loss: 1.7437703609466553
Epoch 180, training loss: 657.6598510742188 = 1.717126488685608 + 100.0 * 6.559427261352539
Epoch 180, val loss: 1.730428695678711
Epoch 190, training loss: 656.103515625 = 1.7014703750610352 + 100.0 * 6.544020175933838
Epoch 190, val loss: 1.7163527011871338
Epoch 200, training loss: 653.9409790039062 = 1.6845988035202026 + 100.0 * 6.522563934326172
Epoch 200, val loss: 1.7014856338500977
Epoch 210, training loss: 652.2283935546875 = 1.666690468788147 + 100.0 * 6.505617141723633
Epoch 210, val loss: 1.6857850551605225
Epoch 220, training loss: 650.8544921875 = 1.6476848125457764 + 100.0 * 6.492067813873291
Epoch 220, val loss: 1.6690880060195923
Epoch 230, training loss: 649.67333984375 = 1.6274186372756958 + 100.0 * 6.480458736419678
Epoch 230, val loss: 1.6514127254486084
Epoch 240, training loss: 648.4916381835938 = 1.6060782670974731 + 100.0 * 6.468855381011963
Epoch 240, val loss: 1.6328659057617188
Epoch 250, training loss: 648.5240478515625 = 1.5837697982788086 + 100.0 * 6.46940279006958
Epoch 250, val loss: 1.6135059595108032
Epoch 260, training loss: 646.6747436523438 = 1.5600578784942627 + 100.0 * 6.451146602630615
Epoch 260, val loss: 1.5932482481002808
Epoch 270, training loss: 645.740478515625 = 1.535834789276123 + 100.0 * 6.442046642303467
Epoch 270, val loss: 1.57252037525177
Epoch 280, training loss: 644.9080200195312 = 1.5108869075775146 + 100.0 * 6.433971405029297
Epoch 280, val loss: 1.551366925239563
Epoch 290, training loss: 644.1111450195312 = 1.4854763746261597 + 100.0 * 6.4262566566467285
Epoch 290, val loss: 1.5299774408340454
Epoch 300, training loss: 643.385498046875 = 1.4596208333969116 + 100.0 * 6.419259071350098
Epoch 300, val loss: 1.508412480354309
Epoch 310, training loss: 643.39599609375 = 1.4333372116088867 + 100.0 * 6.419626712799072
Epoch 310, val loss: 1.4863924980163574
Epoch 320, training loss: 642.1502685546875 = 1.406625509262085 + 100.0 * 6.407436370849609
Epoch 320, val loss: 1.464677095413208
Epoch 330, training loss: 641.4756469726562 = 1.3799986839294434 + 100.0 * 6.400956153869629
Epoch 330, val loss: 1.4428532123565674
Epoch 340, training loss: 640.8724365234375 = 1.3534338474273682 + 100.0 * 6.3951897621154785
Epoch 340, val loss: 1.421396017074585
Epoch 350, training loss: 640.3018188476562 = 1.326882243156433 + 100.0 * 6.389749050140381
Epoch 350, val loss: 1.4001145362854004
Epoch 360, training loss: 640.9701538085938 = 1.3002485036849976 + 100.0 * 6.396698951721191
Epoch 360, val loss: 1.3788541555404663
Epoch 370, training loss: 639.4201049804688 = 1.2736213207244873 + 100.0 * 6.381464958190918
Epoch 370, val loss: 1.357753038406372
Epoch 380, training loss: 638.7816162109375 = 1.2472885847091675 + 100.0 * 6.375343322753906
Epoch 380, val loss: 1.3370780944824219
Epoch 390, training loss: 638.352783203125 = 1.2211642265319824 + 100.0 * 6.371315956115723
Epoch 390, val loss: 1.316710114479065
Epoch 400, training loss: 637.8648681640625 = 1.1953186988830566 + 100.0 * 6.366695404052734
Epoch 400, val loss: 1.296690821647644
Epoch 410, training loss: 638.0875244140625 = 1.1696118116378784 + 100.0 * 6.3691792488098145
Epoch 410, val loss: 1.2769769430160522
Epoch 420, training loss: 637.076171875 = 1.1441199779510498 + 100.0 * 6.359320640563965
Epoch 420, val loss: 1.2575528621673584
Epoch 430, training loss: 636.6547241210938 = 1.1189216375350952 + 100.0 * 6.355358123779297
Epoch 430, val loss: 1.2386090755462646
Epoch 440, training loss: 636.2841186523438 = 1.0939991474151611 + 100.0 * 6.351901531219482
Epoch 440, val loss: 1.2201539278030396
Epoch 450, training loss: 636.9930419921875 = 1.069366693496704 + 100.0 * 6.359236240386963
Epoch 450, val loss: 1.2019933462142944
Epoch 460, training loss: 635.7272338867188 = 1.0448747873306274 + 100.0 * 6.346823692321777
Epoch 460, val loss: 1.1841950416564941
Epoch 470, training loss: 635.3150634765625 = 1.0208182334899902 + 100.0 * 6.342942237854004
Epoch 470, val loss: 1.1670753955841064
Epoch 480, training loss: 634.9927978515625 = 0.9972649216651917 + 100.0 * 6.3399553298950195
Epoch 480, val loss: 1.1505539417266846
Epoch 490, training loss: 634.6998291015625 = 0.9741313457489014 + 100.0 * 6.337257385253906
Epoch 490, val loss: 1.134579062461853
Epoch 500, training loss: 634.8324584960938 = 0.9513257741928101 + 100.0 * 6.338810920715332
Epoch 500, val loss: 1.119063377380371
Epoch 510, training loss: 634.1168212890625 = 0.9288497567176819 + 100.0 * 6.331879615783691
Epoch 510, val loss: 1.1041666269302368
Epoch 520, training loss: 633.9886474609375 = 0.90683913230896 + 100.0 * 6.330818176269531
Epoch 520, val loss: 1.0900206565856934
Epoch 530, training loss: 633.7713623046875 = 0.8851532936096191 + 100.0 * 6.328862190246582
Epoch 530, val loss: 1.076142430305481
Epoch 540, training loss: 633.5498657226562 = 0.8640034198760986 + 100.0 * 6.3268585205078125
Epoch 540, val loss: 1.0629547834396362
Epoch 550, training loss: 633.1398315429688 = 0.8432363271713257 + 100.0 * 6.322965621948242
Epoch 550, val loss: 1.0504248142242432
Epoch 560, training loss: 633.1629028320312 = 0.8229796886444092 + 100.0 * 6.323399066925049
Epoch 560, val loss: 1.0384483337402344
Epoch 570, training loss: 632.854248046875 = 0.8028620481491089 + 100.0 * 6.320513725280762
Epoch 570, val loss: 1.0270674228668213
Epoch 580, training loss: 632.5296630859375 = 0.7833263278007507 + 100.0 * 6.317463397979736
Epoch 580, val loss: 1.0160778760910034
Epoch 590, training loss: 632.2567749023438 = 0.7641496658325195 + 100.0 * 6.3149261474609375
Epoch 590, val loss: 1.0057355165481567
Epoch 600, training loss: 633.2074584960938 = 0.7453498840332031 + 100.0 * 6.324621200561523
Epoch 600, val loss: 0.9958012104034424
Epoch 610, training loss: 632.1047973632812 = 0.7267247438430786 + 100.0 * 6.313780307769775
Epoch 610, val loss: 0.9862269759178162
Epoch 620, training loss: 631.6936645507812 = 0.7085105776786804 + 100.0 * 6.30985164642334
Epoch 620, val loss: 0.977317750453949
Epoch 630, training loss: 631.4931640625 = 0.6907903552055359 + 100.0 * 6.308023929595947
Epoch 630, val loss: 0.9688994288444519
Epoch 640, training loss: 631.4815063476562 = 0.6734361052513123 + 100.0 * 6.308080673217773
Epoch 640, val loss: 0.9610192179679871
Epoch 650, training loss: 631.247314453125 = 0.6563704013824463 + 100.0 * 6.305909156799316
Epoch 650, val loss: 0.9533938765525818
Epoch 660, training loss: 631.0806884765625 = 0.6395225524902344 + 100.0 * 6.304411888122559
Epoch 660, val loss: 0.9461867809295654
Epoch 670, training loss: 630.8851928710938 = 0.6231765151023865 + 100.0 * 6.3026204109191895
Epoch 670, val loss: 0.9394310712814331
Epoch 680, training loss: 630.975341796875 = 0.6071629524230957 + 100.0 * 6.30368185043335
Epoch 680, val loss: 0.9331838488578796
Epoch 690, training loss: 630.4678955078125 = 0.5914797186851501 + 100.0 * 6.298763751983643
Epoch 690, val loss: 0.9274459481239319
Epoch 700, training loss: 630.27587890625 = 0.5762331485748291 + 100.0 * 6.296996593475342
Epoch 700, val loss: 0.9219925999641418
Epoch 710, training loss: 630.13427734375 = 0.5613821744918823 + 100.0 * 6.29572868347168
Epoch 710, val loss: 0.9169561862945557
Epoch 720, training loss: 630.8765258789062 = 0.5467891693115234 + 100.0 * 6.30329704284668
Epoch 720, val loss: 0.9122238159179688
Epoch 730, training loss: 630.0327758789062 = 0.5325457453727722 + 100.0 * 6.295002460479736
Epoch 730, val loss: 0.9078242182731628
Epoch 740, training loss: 629.7174682617188 = 0.5187039375305176 + 100.0 * 6.291987895965576
Epoch 740, val loss: 0.9039435982704163
Epoch 750, training loss: 629.5472412109375 = 0.5053349137306213 + 100.0 * 6.290419578552246
Epoch 750, val loss: 0.9005833268165588
Epoch 760, training loss: 629.8081665039062 = 0.4924120604991913 + 100.0 * 6.293157577514648
Epoch 760, val loss: 0.8974781632423401
Epoch 770, training loss: 629.8064575195312 = 0.47959187626838684 + 100.0 * 6.29326868057251
Epoch 770, val loss: 0.8947004675865173
Epoch 780, training loss: 629.2676391601562 = 0.46719491481781006 + 100.0 * 6.288004398345947
Epoch 780, val loss: 0.8919912576675415
Epoch 790, training loss: 629.077880859375 = 0.45527204871177673 + 100.0 * 6.286226272583008
Epoch 790, val loss: 0.8899149894714355
Epoch 800, training loss: 628.943603515625 = 0.44371655583381653 + 100.0 * 6.284998893737793
Epoch 800, val loss: 0.8882777094841003
Epoch 810, training loss: 628.7918090820312 = 0.4325122833251953 + 100.0 * 6.28359317779541
Epoch 810, val loss: 0.8869419693946838
Epoch 820, training loss: 628.8410034179688 = 0.42165857553482056 + 100.0 * 6.284193992614746
Epoch 820, val loss: 0.8858868479728699
Epoch 830, training loss: 628.5633544921875 = 0.4109872579574585 + 100.0 * 6.28152322769165
Epoch 830, val loss: 0.8849327564239502
Epoch 840, training loss: 628.5280151367188 = 0.40065962076187134 + 100.0 * 6.28127384185791
Epoch 840, val loss: 0.884311318397522
Epoch 850, training loss: 628.8439331054688 = 0.39063048362731934 + 100.0 * 6.2845330238342285
Epoch 850, val loss: 0.8838260769844055
Epoch 860, training loss: 628.3663940429688 = 0.38080736994743347 + 100.0 * 6.279855728149414
Epoch 860, val loss: 0.8838960528373718
Epoch 870, training loss: 628.3099975585938 = 0.3713012635707855 + 100.0 * 6.279387474060059
Epoch 870, val loss: 0.8839045763015747
Epoch 880, training loss: 628.0824584960938 = 0.3619929850101471 + 100.0 * 6.277204513549805
Epoch 880, val loss: 0.8843274116516113
Epoch 890, training loss: 627.9774169921875 = 0.3529413342475891 + 100.0 * 6.276244640350342
Epoch 890, val loss: 0.8850628733634949
Epoch 900, training loss: 628.4802856445312 = 0.34408992528915405 + 100.0 * 6.2813615798950195
Epoch 900, val loss: 0.8857950568199158
Epoch 910, training loss: 627.9484252929688 = 0.3353455662727356 + 100.0 * 6.2761311531066895
Epoch 910, val loss: 0.8865002393722534
Epoch 920, training loss: 627.8038330078125 = 0.32681214809417725 + 100.0 * 6.274770259857178
Epoch 920, val loss: 0.8875946402549744
Epoch 930, training loss: 627.5914916992188 = 0.3184582591056824 + 100.0 * 6.272730350494385
Epoch 930, val loss: 0.8889353275299072
Epoch 940, training loss: 627.8701171875 = 0.3103317618370056 + 100.0 * 6.275598049163818
Epoch 940, val loss: 0.8901531100273132
Epoch 950, training loss: 627.8992919921875 = 0.30215007066726685 + 100.0 * 6.27597188949585
Epoch 950, val loss: 0.8920565247535706
Epoch 960, training loss: 627.308349609375 = 0.29423415660858154 + 100.0 * 6.270141124725342
Epoch 960, val loss: 0.8935815691947937
Epoch 970, training loss: 627.2596435546875 = 0.28652244806289673 + 100.0 * 6.269731521606445
Epoch 970, val loss: 0.8952780365943909
Epoch 980, training loss: 627.0769653320312 = 0.27894651889801025 + 100.0 * 6.267980575561523
Epoch 980, val loss: 0.897432267665863
Epoch 990, training loss: 627.8199462890625 = 0.2715286314487457 + 100.0 * 6.275484085083008
Epoch 990, val loss: 0.8997235298156738
Epoch 1000, training loss: 627.5172729492188 = 0.2640697956085205 + 100.0 * 6.272531509399414
Epoch 1000, val loss: 0.9017296433448792
Epoch 1010, training loss: 626.8803100585938 = 0.2568451762199402 + 100.0 * 6.266234874725342
Epoch 1010, val loss: 0.9038507342338562
Epoch 1020, training loss: 626.7916870117188 = 0.24983201920986176 + 100.0 * 6.265418529510498
Epoch 1020, val loss: 0.9064656496047974
Epoch 1030, training loss: 626.6944580078125 = 0.24296383559703827 + 100.0 * 6.264514923095703
Epoch 1030, val loss: 0.9089566469192505
Epoch 1040, training loss: 627.1879272460938 = 0.2362697571516037 + 100.0 * 6.269516468048096
Epoch 1040, val loss: 0.9115699529647827
Epoch 1050, training loss: 626.7969360351562 = 0.22963069379329681 + 100.0 * 6.2656731605529785
Epoch 1050, val loss: 0.9141426086425781
Epoch 1060, training loss: 626.6495971679688 = 0.22316817939281464 + 100.0 * 6.2642645835876465
Epoch 1060, val loss: 0.9168710708618164
Epoch 1070, training loss: 627.1666259765625 = 0.21688641607761383 + 100.0 * 6.269497871398926
Epoch 1070, val loss: 0.9193338751792908
Epoch 1080, training loss: 626.3724365234375 = 0.21070796251296997 + 100.0 * 6.261617660522461
Epoch 1080, val loss: 0.9224160313606262
Epoch 1090, training loss: 626.3046264648438 = 0.20476248860359192 + 100.0 * 6.260998725891113
Epoch 1090, val loss: 0.9254056811332703
Epoch 1100, training loss: 626.1741333007812 = 0.1990334689617157 + 100.0 * 6.259750843048096
Epoch 1100, val loss: 0.9284917116165161
Epoch 1110, training loss: 626.1917724609375 = 0.19344709813594818 + 100.0 * 6.259983539581299
Epoch 1110, val loss: 0.931623101234436
Epoch 1120, training loss: 626.379150390625 = 0.18799647688865662 + 100.0 * 6.261911869049072
Epoch 1120, val loss: 0.934772253036499
Epoch 1130, training loss: 626.2509765625 = 0.18267188966274261 + 100.0 * 6.260683059692383
Epoch 1130, val loss: 0.9381293058395386
Epoch 1140, training loss: 626.1283569335938 = 0.17748957872390747 + 100.0 * 6.2595086097717285
Epoch 1140, val loss: 0.9412768483161926
Epoch 1150, training loss: 626.0341186523438 = 0.17250581085681915 + 100.0 * 6.2586164474487305
Epoch 1150, val loss: 0.9444190859794617
Epoch 1160, training loss: 625.7679443359375 = 0.16770020127296448 + 100.0 * 6.256001949310303
Epoch 1160, val loss: 0.9478694200515747
Epoch 1170, training loss: 625.6986694335938 = 0.16307643055915833 + 100.0 * 6.2553558349609375
Epoch 1170, val loss: 0.9514690041542053
Epoch 1180, training loss: 626.6302490234375 = 0.15861093997955322 + 100.0 * 6.264716148376465
Epoch 1180, val loss: 0.9546104073524475
Epoch 1190, training loss: 625.885498046875 = 0.15419304370880127 + 100.0 * 6.257313251495361
Epoch 1190, val loss: 0.958560049533844
Epoch 1200, training loss: 625.5489501953125 = 0.1499616950750351 + 100.0 * 6.2539896965026855
Epoch 1200, val loss: 0.9619754552841187
Epoch 1210, training loss: 625.4611206054688 = 0.1458897739648819 + 100.0 * 6.253152370452881
Epoch 1210, val loss: 0.9656639695167542
Epoch 1220, training loss: 626.7405395507812 = 0.14195303618907928 + 100.0 * 6.265985488891602
Epoch 1220, val loss: 0.9695411920547485
Epoch 1230, training loss: 625.9634399414062 = 0.13807518780231476 + 100.0 * 6.258253574371338
Epoch 1230, val loss: 0.972664475440979
Epoch 1240, training loss: 625.3551025390625 = 0.13429638743400574 + 100.0 * 6.252208232879639
Epoch 1240, val loss: 0.9765023589134216
Epoch 1250, training loss: 625.2437744140625 = 0.13073404133319855 + 100.0 * 6.251130104064941
Epoch 1250, val loss: 0.9803591966629028
Epoch 1260, training loss: 625.1300048828125 = 0.12728796899318695 + 100.0 * 6.250027179718018
Epoch 1260, val loss: 0.9841887354850769
Epoch 1270, training loss: 625.6996459960938 = 0.12397533655166626 + 100.0 * 6.255756855010986
Epoch 1270, val loss: 0.9880010485649109
Epoch 1280, training loss: 625.2140502929688 = 0.12067937850952148 + 100.0 * 6.250933647155762
Epoch 1280, val loss: 0.9919800758361816
Epoch 1290, training loss: 625.4518432617188 = 0.11753147840499878 + 100.0 * 6.253342628479004
Epoch 1290, val loss: 0.9955178499221802
Epoch 1300, training loss: 625.0588989257812 = 0.11445891857147217 + 100.0 * 6.249444484710693
Epoch 1300, val loss: 0.9993313550949097
Epoch 1310, training loss: 624.9796752929688 = 0.11151866614818573 + 100.0 * 6.248681545257568
Epoch 1310, val loss: 1.0032862424850464
Epoch 1320, training loss: 625.0740966796875 = 0.10868533700704575 + 100.0 * 6.2496538162231445
Epoch 1320, val loss: 1.007437825202942
Epoch 1330, training loss: 624.9428100585938 = 0.1059018075466156 + 100.0 * 6.248369216918945
Epoch 1330, val loss: 1.0112799406051636
Epoch 1340, training loss: 624.753662109375 = 0.10320369899272919 + 100.0 * 6.246504783630371
Epoch 1340, val loss: 1.0154228210449219
Epoch 1350, training loss: 624.75439453125 = 0.10062604397535324 + 100.0 * 6.246537208557129
Epoch 1350, val loss: 1.0196102857589722
Epoch 1360, training loss: 625.5809936523438 = 0.09810496121644974 + 100.0 * 6.254829406738281
Epoch 1360, val loss: 1.0237516164779663
Epoch 1370, training loss: 624.7131958007812 = 0.09565431624650955 + 100.0 * 6.246175765991211
Epoch 1370, val loss: 1.0275071859359741
Epoch 1380, training loss: 624.5014038085938 = 0.0932750254869461 + 100.0 * 6.244081497192383
Epoch 1380, val loss: 1.0317612886428833
Epoch 1390, training loss: 624.5206298828125 = 0.09100794047117233 + 100.0 * 6.244296073913574
Epoch 1390, val loss: 1.0360338687896729
Epoch 1400, training loss: 625.0254516601562 = 0.08881034702062607 + 100.0 * 6.249366760253906
Epoch 1400, val loss: 1.0404202938079834
Epoch 1410, training loss: 624.990478515625 = 0.08664055913686752 + 100.0 * 6.249038219451904
Epoch 1410, val loss: 1.0442320108413696
Epoch 1420, training loss: 624.4713134765625 = 0.0845261961221695 + 100.0 * 6.243867874145508
Epoch 1420, val loss: 1.0484521389007568
Epoch 1430, training loss: 624.3327026367188 = 0.08251290023326874 + 100.0 * 6.242501735687256
Epoch 1430, val loss: 1.0525915622711182
Epoch 1440, training loss: 624.6819458007812 = 0.08056004345417023 + 100.0 * 6.246013641357422
Epoch 1440, val loss: 1.0570005178451538
Epoch 1450, training loss: 624.3302001953125 = 0.07863515615463257 + 100.0 * 6.242515563964844
Epoch 1450, val loss: 1.061287522315979
Epoch 1460, training loss: 624.2252807617188 = 0.0767822414636612 + 100.0 * 6.241485118865967
Epoch 1460, val loss: 1.0653884410858154
Epoch 1470, training loss: 624.5413208007812 = 0.07500431686639786 + 100.0 * 6.244663238525391
Epoch 1470, val loss: 1.0695585012435913
Epoch 1480, training loss: 624.14990234375 = 0.07324133068323135 + 100.0 * 6.240766525268555
Epoch 1480, val loss: 1.073989748954773
Epoch 1490, training loss: 624.07470703125 = 0.07155294716358185 + 100.0 * 6.240031719207764
Epoch 1490, val loss: 1.078283429145813
Epoch 1500, training loss: 624.6551513671875 = 0.06991127878427505 + 100.0 * 6.245852947235107
Epoch 1500, val loss: 1.082623839378357
Epoch 1510, training loss: 624.1751098632812 = 0.06830301880836487 + 100.0 * 6.241067886352539
Epoch 1510, val loss: 1.0867347717285156
Epoch 1520, training loss: 624.2398071289062 = 0.06675201654434204 + 100.0 * 6.241730213165283
Epoch 1520, val loss: 1.0910615921020508
Epoch 1530, training loss: 623.9923095703125 = 0.0652378648519516 + 100.0 * 6.2392706871032715
Epoch 1530, val loss: 1.0953043699264526
Epoch 1540, training loss: 623.96044921875 = 0.06377020478248596 + 100.0 * 6.238966464996338
Epoch 1540, val loss: 1.0996339321136475
Epoch 1550, training loss: 623.754150390625 = 0.062360864132642746 + 100.0 * 6.236917972564697
Epoch 1550, val loss: 1.1039749383926392
Epoch 1560, training loss: 623.747314453125 = 0.06099093705415726 + 100.0 * 6.236863136291504
Epoch 1560, val loss: 1.1083213090896606
Epoch 1570, training loss: 624.205322265625 = 0.059669528156518936 + 100.0 * 6.241456031799316
Epoch 1570, val loss: 1.1124348640441895
Epoch 1580, training loss: 624.0977783203125 = 0.05833923816680908 + 100.0 * 6.240394592285156
Epoch 1580, val loss: 1.1169166564941406
Epoch 1590, training loss: 623.9591064453125 = 0.0570656843483448 + 100.0 * 6.239020347595215
Epoch 1590, val loss: 1.1208709478378296
Epoch 1600, training loss: 624.036865234375 = 0.055827949196100235 + 100.0 * 6.239809989929199
Epoch 1600, val loss: 1.1248830556869507
Epoch 1610, training loss: 623.4727783203125 = 0.054628439247608185 + 100.0 * 6.2341814041137695
Epoch 1610, val loss: 1.1295899152755737
Epoch 1620, training loss: 623.4857788085938 = 0.05347776412963867 + 100.0 * 6.234323024749756
Epoch 1620, val loss: 1.1338179111480713
Epoch 1630, training loss: 623.5377197265625 = 0.05236667022109032 + 100.0 * 6.234853267669678
Epoch 1630, val loss: 1.1380681991577148
Epoch 1640, training loss: 623.9116821289062 = 0.05128362029790878 + 100.0 * 6.2386040687561035
Epoch 1640, val loss: 1.1424227952957153
Epoch 1650, training loss: 624.199462890625 = 0.05018496885895729 + 100.0 * 6.241492748260498
Epoch 1650, val loss: 1.1466119289398193
Epoch 1660, training loss: 623.4083862304688 = 0.04912717640399933 + 100.0 * 6.233592510223389
Epoch 1660, val loss: 1.1508618593215942
Epoch 1670, training loss: 623.2814331054688 = 0.04811565577983856 + 100.0 * 6.232333660125732
Epoch 1670, val loss: 1.1549012660980225
Epoch 1680, training loss: 623.2360229492188 = 0.04714333638548851 + 100.0 * 6.231889247894287
Epoch 1680, val loss: 1.1595077514648438
Epoch 1690, training loss: 623.528076171875 = 0.0462055541574955 + 100.0 * 6.234818935394287
Epoch 1690, val loss: 1.1634653806686401
Epoch 1700, training loss: 623.5628051757812 = 0.045257002115249634 + 100.0 * 6.235176086425781
Epoch 1700, val loss: 1.1676627397537231
Epoch 1710, training loss: 623.2227172851562 = 0.04433811083436012 + 100.0 * 6.231784343719482
Epoch 1710, val loss: 1.1719176769256592
Epoch 1720, training loss: 623.1624145507812 = 0.04344838112592697 + 100.0 * 6.231189727783203
Epoch 1720, val loss: 1.1761959791183472
Epoch 1730, training loss: 623.1591796875 = 0.04259660094976425 + 100.0 * 6.231165885925293
Epoch 1730, val loss: 1.1804046630859375
Epoch 1740, training loss: 623.6856689453125 = 0.04176842421293259 + 100.0 * 6.236439228057861
Epoch 1740, val loss: 1.1847279071807861
Epoch 1750, training loss: 623.250244140625 = 0.040942899882793427 + 100.0 * 6.23209285736084
Epoch 1750, val loss: 1.188599944114685
Epoch 1760, training loss: 623.1317749023438 = 0.04013712704181671 + 100.0 * 6.2309160232543945
Epoch 1760, val loss: 1.1929326057434082
Epoch 1770, training loss: 622.9476318359375 = 0.03935692086815834 + 100.0 * 6.2290825843811035
Epoch 1770, val loss: 1.1969430446624756
Epoch 1780, training loss: 622.9920043945312 = 0.03860972076654434 + 100.0 * 6.229533672332764
Epoch 1780, val loss: 1.2012670040130615
Epoch 1790, training loss: 623.2489013671875 = 0.03788053244352341 + 100.0 * 6.232110023498535
Epoch 1790, val loss: 1.205546259880066
Epoch 1800, training loss: 623.3068237304688 = 0.03716898337006569 + 100.0 * 6.232696533203125
Epoch 1800, val loss: 1.2092317342758179
Epoch 1810, training loss: 622.8875732421875 = 0.03644298389554024 + 100.0 * 6.228511333465576
Epoch 1810, val loss: 1.2134923934936523
Epoch 1820, training loss: 622.910400390625 = 0.0357632078230381 + 100.0 * 6.22874641418457
Epoch 1820, val loss: 1.2173410654067993
Epoch 1830, training loss: 623.5424194335938 = 0.035107310861349106 + 100.0 * 6.235073089599609
Epoch 1830, val loss: 1.2218453884124756
Epoch 1840, training loss: 622.885986328125 = 0.034448351711034775 + 100.0 * 6.228515625
Epoch 1840, val loss: 1.2253258228302002
Epoch 1850, training loss: 622.7109375 = 0.03381333500146866 + 100.0 * 6.226771354675293
Epoch 1850, val loss: 1.2296110391616821
Epoch 1860, training loss: 622.7462768554688 = 0.03320786729454994 + 100.0 * 6.227130889892578
Epoch 1860, val loss: 1.2335275411605835
Epoch 1870, training loss: 623.5433349609375 = 0.032617971301078796 + 100.0 * 6.235107421875
Epoch 1870, val loss: 1.237457036972046
Epoch 1880, training loss: 623.1654663085938 = 0.03202254697680473 + 100.0 * 6.231334209442139
Epoch 1880, val loss: 1.2415822744369507
Epoch 1890, training loss: 622.9381713867188 = 0.03143460303544998 + 100.0 * 6.229067325592041
Epoch 1890, val loss: 1.2451475858688354
Epoch 1900, training loss: 622.73583984375 = 0.030875232070684433 + 100.0 * 6.227049827575684
Epoch 1900, val loss: 1.2492245435714722
Epoch 1910, training loss: 622.66943359375 = 0.03033336251974106 + 100.0 * 6.226390838623047
Epoch 1910, val loss: 1.2532151937484741
Epoch 1920, training loss: 622.6270751953125 = 0.02980506606400013 + 100.0 * 6.225973129272461
Epoch 1920, val loss: 1.2573864459991455
Epoch 1930, training loss: 622.8502197265625 = 0.02929139882326126 + 100.0 * 6.228209495544434
Epoch 1930, val loss: 1.2613545656204224
Epoch 1940, training loss: 622.8654174804688 = 0.028781820088624954 + 100.0 * 6.228366374969482
Epoch 1940, val loss: 1.2647454738616943
Epoch 1950, training loss: 622.6647338867188 = 0.028279021382331848 + 100.0 * 6.226364612579346
Epoch 1950, val loss: 1.2687654495239258
Epoch 1960, training loss: 622.6043701171875 = 0.027790509164333344 + 100.0 * 6.225766181945801
Epoch 1960, val loss: 1.2721664905548096
Epoch 1970, training loss: 622.443115234375 = 0.027319515123963356 + 100.0 * 6.224157810211182
Epoch 1970, val loss: 1.2763493061065674
Epoch 1980, training loss: 622.3551025390625 = 0.026863515377044678 + 100.0 * 6.223282337188721
Epoch 1980, val loss: 1.2800935506820679
Epoch 1990, training loss: 622.3784790039062 = 0.026420986279845238 + 100.0 * 6.223520755767822
Epoch 1990, val loss: 1.2838767766952515
Epoch 2000, training loss: 623.03857421875 = 0.025991961359977722 + 100.0 * 6.230125427246094
Epoch 2000, val loss: 1.287787914276123
Epoch 2010, training loss: 622.8411865234375 = 0.02554737590253353 + 100.0 * 6.228156089782715
Epoch 2010, val loss: 1.2909576892852783
Epoch 2020, training loss: 622.6627807617188 = 0.025122204795479774 + 100.0 * 6.226376056671143
Epoch 2020, val loss: 1.2948403358459473
Epoch 2030, training loss: 622.3858032226562 = 0.024705758318305016 + 100.0 * 6.223610877990723
Epoch 2030, val loss: 1.298146367073059
Epoch 2040, training loss: 622.52490234375 = 0.024310633540153503 + 100.0 * 6.225006103515625
Epoch 2040, val loss: 1.3019238710403442
Epoch 2050, training loss: 622.3670043945312 = 0.023915760219097137 + 100.0 * 6.223430633544922
Epoch 2050, val loss: 1.3054248094558716
Epoch 2060, training loss: 622.1920776367188 = 0.023534514009952545 + 100.0 * 6.221685409545898
Epoch 2060, val loss: 1.3092397451400757
Epoch 2070, training loss: 622.3101196289062 = 0.023166850209236145 + 100.0 * 6.222869396209717
Epoch 2070, val loss: 1.3128777742385864
Epoch 2080, training loss: 622.6365966796875 = 0.022807078436017036 + 100.0 * 6.226137638092041
Epoch 2080, val loss: 1.3162461519241333
Epoch 2090, training loss: 622.2671508789062 = 0.0224454328417778 + 100.0 * 6.222446918487549
Epoch 2090, val loss: 1.319445013999939
Epoch 2100, training loss: 622.2322998046875 = 0.022092431783676147 + 100.0 * 6.222102165222168
Epoch 2100, val loss: 1.3229807615280151
Epoch 2110, training loss: 622.5765380859375 = 0.021751441061496735 + 100.0 * 6.225548267364502
Epoch 2110, val loss: 1.3263593912124634
Epoch 2120, training loss: 622.2119750976562 = 0.021412834525108337 + 100.0 * 6.22190523147583
Epoch 2120, val loss: 1.3297104835510254
Epoch 2130, training loss: 622.1304321289062 = 0.021088743582367897 + 100.0 * 6.22109317779541
Epoch 2130, val loss: 1.33324134349823
Epoch 2140, training loss: 621.9647216796875 = 0.02076973393559456 + 100.0 * 6.219439506530762
Epoch 2140, val loss: 1.3364975452423096
Epoch 2150, training loss: 622.3361206054688 = 0.020466163754463196 + 100.0 * 6.223156452178955
Epoch 2150, val loss: 1.3398805856704712
Epoch 2160, training loss: 622.303955078125 = 0.02015889622271061 + 100.0 * 6.222837924957275
Epoch 2160, val loss: 1.3432656526565552
Epoch 2170, training loss: 622.0963745117188 = 0.019855182617902756 + 100.0 * 6.220765113830566
Epoch 2170, val loss: 1.3461990356445312
Epoch 2180, training loss: 622.114013671875 = 0.0195651575922966 + 100.0 * 6.220943927764893
Epoch 2180, val loss: 1.3495771884918213
Epoch 2190, training loss: 622.1993408203125 = 0.01927363872528076 + 100.0 * 6.221800804138184
Epoch 2190, val loss: 1.3527170419692993
Epoch 2200, training loss: 622.0210571289062 = 0.018992777913808823 + 100.0 * 6.220020771026611
Epoch 2200, val loss: 1.3559819459915161
Epoch 2210, training loss: 621.90869140625 = 0.018720027059316635 + 100.0 * 6.218899250030518
Epoch 2210, val loss: 1.3592294454574585
Epoch 2220, training loss: 622.0665283203125 = 0.018456976860761642 + 100.0 * 6.220480918884277
Epoch 2220, val loss: 1.362457513809204
Epoch 2230, training loss: 622.050048828125 = 0.018193930387496948 + 100.0 * 6.220318794250488
Epoch 2230, val loss: 1.3651845455169678
Epoch 2240, training loss: 622.031005859375 = 0.017934009432792664 + 100.0 * 6.220130920410156
Epoch 2240, val loss: 1.3685832023620605
Epoch 2250, training loss: 622.10546875 = 0.017680298537015915 + 100.0 * 6.220877647399902
Epoch 2250, val loss: 1.3717577457427979
Epoch 2260, training loss: 621.819091796875 = 0.017434393987059593 + 100.0 * 6.218016147613525
Epoch 2260, val loss: 1.3745869398117065
Epoch 2270, training loss: 621.9797973632812 = 0.01719697192311287 + 100.0 * 6.219625949859619
Epoch 2270, val loss: 1.377717137336731
Epoch 2280, training loss: 621.8162231445312 = 0.01695541851222515 + 100.0 * 6.217992782592773
Epoch 2280, val loss: 1.3806555271148682
Epoch 2290, training loss: 621.8165893554688 = 0.016726473346352577 + 100.0 * 6.217998504638672
Epoch 2290, val loss: 1.383765459060669
Epoch 2300, training loss: 622.1593017578125 = 0.016503440216183662 + 100.0 * 6.221427917480469
Epoch 2300, val loss: 1.386775016784668
Epoch 2310, training loss: 621.8026733398438 = 0.01627691276371479 + 100.0 * 6.2178635597229
Epoch 2310, val loss: 1.3896338939666748
Epoch 2320, training loss: 621.6890869140625 = 0.01605229265987873 + 100.0 * 6.21673059463501
Epoch 2320, val loss: 1.3923650979995728
Epoch 2330, training loss: 621.8074340820312 = 0.015842905268073082 + 100.0 * 6.2179155349731445
Epoch 2330, val loss: 1.395400047302246
Epoch 2340, training loss: 621.9105834960938 = 0.015636468306183815 + 100.0 * 6.218949794769287
Epoch 2340, val loss: 1.3982378244400024
Epoch 2350, training loss: 621.6971435546875 = 0.015427187085151672 + 100.0 * 6.2168169021606445
Epoch 2350, val loss: 1.4012079238891602
Epoch 2360, training loss: 621.6735229492188 = 0.015227294526994228 + 100.0 * 6.216583251953125
Epoch 2360, val loss: 1.404337763786316
Epoch 2370, training loss: 622.5538330078125 = 0.01502908207476139 + 100.0 * 6.225388050079346
Epoch 2370, val loss: 1.4067851305007935
Epoch 2380, training loss: 621.8031616210938 = 0.014829277992248535 + 100.0 * 6.217883110046387
Epoch 2380, val loss: 1.4099020957946777
Epoch 2390, training loss: 621.6074829101562 = 0.014634898863732815 + 100.0 * 6.215928554534912
Epoch 2390, val loss: 1.4123049974441528
Epoch 2400, training loss: 621.478271484375 = 0.014449839480221272 + 100.0 * 6.2146382331848145
Epoch 2400, val loss: 1.4154585599899292
Epoch 2410, training loss: 621.4133911132812 = 0.014270658604800701 + 100.0 * 6.213991165161133
Epoch 2410, val loss: 1.4181594848632812
Epoch 2420, training loss: 621.7049560546875 = 0.014099129475653172 + 100.0 * 6.2169084548950195
Epoch 2420, val loss: 1.4209696054458618
Epoch 2430, training loss: 621.8529663085938 = 0.013920512981712818 + 100.0 * 6.218390464782715
Epoch 2430, val loss: 1.4233914613723755
Epoch 2440, training loss: 621.3950805664062 = 0.013738401234149933 + 100.0 * 6.213813781738281
Epoch 2440, val loss: 1.426351547241211
Epoch 2450, training loss: 621.457275390625 = 0.013566671870648861 + 100.0 * 6.214437007904053
Epoch 2450, val loss: 1.4291021823883057
Epoch 2460, training loss: 621.3535766601562 = 0.01340155303478241 + 100.0 * 6.2134013175964355
Epoch 2460, val loss: 1.4318523406982422
Epoch 2470, training loss: 621.3646850585938 = 0.013242130167782307 + 100.0 * 6.21351432800293
Epoch 2470, val loss: 1.4346150159835815
Epoch 2480, training loss: 622.3214721679688 = 0.013088149949908257 + 100.0 * 6.223083972930908
Epoch 2480, val loss: 1.437476396560669
Epoch 2490, training loss: 621.9666748046875 = 0.012922326102852821 + 100.0 * 6.219537258148193
Epoch 2490, val loss: 1.4398834705352783
Epoch 2500, training loss: 621.4404907226562 = 0.012763090431690216 + 100.0 * 6.214277267456055
Epoch 2500, val loss: 1.442375659942627
Epoch 2510, training loss: 621.3297119140625 = 0.01261142361909151 + 100.0 * 6.213170528411865
Epoch 2510, val loss: 1.445113182067871
Epoch 2520, training loss: 621.5611572265625 = 0.012467100284993649 + 100.0 * 6.215487003326416
Epoch 2520, val loss: 1.447973370552063
Epoch 2530, training loss: 621.5291137695312 = 0.01231920812278986 + 100.0 * 6.215167999267578
Epoch 2530, val loss: 1.4503692388534546
Epoch 2540, training loss: 621.2720947265625 = 0.012169777415692806 + 100.0 * 6.212599277496338
Epoch 2540, val loss: 1.4526991844177246
Epoch 2550, training loss: 621.2293090820312 = 0.01203098427504301 + 100.0 * 6.212172985076904
Epoch 2550, val loss: 1.455397129058838
Epoch 2560, training loss: 621.8035278320312 = 0.011894523166120052 + 100.0 * 6.217916011810303
Epoch 2560, val loss: 1.4580347537994385
Epoch 2570, training loss: 621.2445678710938 = 0.011757840402424335 + 100.0 * 6.21232795715332
Epoch 2570, val loss: 1.4603947401046753
Epoch 2580, training loss: 621.2998657226562 = 0.011622428894042969 + 100.0 * 6.2128825187683105
Epoch 2580, val loss: 1.4630844593048096
Epoch 2590, training loss: 621.4698486328125 = 0.011493033729493618 + 100.0 * 6.214583873748779
Epoch 2590, val loss: 1.4655323028564453
Epoch 2600, training loss: 621.1995849609375 = 0.011364149861037731 + 100.0 * 6.2118821144104
Epoch 2600, val loss: 1.4678922891616821
Epoch 2610, training loss: 621.4664916992188 = 0.011240548454225063 + 100.0 * 6.214552402496338
Epoch 2610, val loss: 1.4704797267913818
Epoch 2620, training loss: 621.117919921875 = 0.01111357007175684 + 100.0 * 6.211068153381348
Epoch 2620, val loss: 1.4727809429168701
Epoch 2630, training loss: 621.1859741210938 = 0.010991985909640789 + 100.0 * 6.211750030517578
Epoch 2630, val loss: 1.4752328395843506
Epoch 2640, training loss: 621.4515991210938 = 0.010876661166548729 + 100.0 * 6.214407444000244
Epoch 2640, val loss: 1.4777060747146606
Epoch 2650, training loss: 621.1591186523438 = 0.010757294483482838 + 100.0 * 6.211483955383301
Epoch 2650, val loss: 1.4800294637680054
Epoch 2660, training loss: 621.2031860351562 = 0.010642646811902523 + 100.0 * 6.211925506591797
Epoch 2660, val loss: 1.4825444221496582
Epoch 2670, training loss: 621.3726196289062 = 0.010528390295803547 + 100.0 * 6.213621139526367
Epoch 2670, val loss: 1.484984040260315
Epoch 2680, training loss: 621.792724609375 = 0.010418801568448544 + 100.0 * 6.217823028564453
Epoch 2680, val loss: 1.4873098134994507
Epoch 2690, training loss: 621.1040649414062 = 0.0102986516430974 + 100.0 * 6.2109375
Epoch 2690, val loss: 1.489201545715332
Epoch 2700, training loss: 621.0040893554688 = 0.010191736742854118 + 100.0 * 6.209939002990723
Epoch 2700, val loss: 1.4917593002319336
Epoch 2710, training loss: 620.97705078125 = 0.010085822083055973 + 100.0 * 6.209669589996338
Epoch 2710, val loss: 1.4942855834960938
Epoch 2720, training loss: 621.006103515625 = 0.0099846376106143 + 100.0 * 6.2099609375
Epoch 2720, val loss: 1.4964983463287354
Epoch 2730, training loss: 621.6370849609375 = 0.009886614046990871 + 100.0 * 6.216271877288818
Epoch 2730, val loss: 1.4986259937286377
Epoch 2740, training loss: 621.3032836914062 = 0.009782863780856133 + 100.0 * 6.212935447692871
Epoch 2740, val loss: 1.5006588697433472
Epoch 2750, training loss: 621.2332153320312 = 0.009680845774710178 + 100.0 * 6.212234973907471
Epoch 2750, val loss: 1.5032706260681152
Epoch 2760, training loss: 621.2448120117188 = 0.009582404047250748 + 100.0 * 6.212352275848389
Epoch 2760, val loss: 1.5053390264511108
Epoch 2770, training loss: 621.1055297851562 = 0.009481116198003292 + 100.0 * 6.210960388183594
Epoch 2770, val loss: 1.5075732469558716
Epoch 2780, training loss: 621.035888671875 = 0.009386332705616951 + 100.0 * 6.210264682769775
Epoch 2780, val loss: 1.5096213817596436
Epoch 2790, training loss: 620.9826049804688 = 0.009293085895478725 + 100.0 * 6.209733486175537
Epoch 2790, val loss: 1.5119478702545166
Epoch 2800, training loss: 621.297119140625 = 0.009203697554767132 + 100.0 * 6.212879180908203
Epoch 2800, val loss: 1.5140701532363892
Epoch 2810, training loss: 620.8250732421875 = 0.009110880084335804 + 100.0 * 6.208159923553467
Epoch 2810, val loss: 1.516282081604004
Epoch 2820, training loss: 620.9033813476562 = 0.009023033082485199 + 100.0 * 6.2089433670043945
Epoch 2820, val loss: 1.5184494256973267
Epoch 2830, training loss: 621.4207763671875 = 0.008937669917941093 + 100.0 * 6.214118957519531
Epoch 2830, val loss: 1.520815372467041
Epoch 2840, training loss: 620.8999633789062 = 0.008846933953464031 + 100.0 * 6.208911418914795
Epoch 2840, val loss: 1.522383451461792
Epoch 2850, training loss: 620.7998657226562 = 0.008760971017181873 + 100.0 * 6.207911014556885
Epoch 2850, val loss: 1.5246394872665405
Epoch 2860, training loss: 620.7537841796875 = 0.008678460493683815 + 100.0 * 6.207450866699219
Epoch 2860, val loss: 1.5270159244537354
Epoch 2870, training loss: 620.9321899414062 = 0.008600013330578804 + 100.0 * 6.209236145019531
Epoch 2870, val loss: 1.52889084815979
Epoch 2880, training loss: 621.1610107421875 = 0.008517131209373474 + 100.0 * 6.211524486541748
Epoch 2880, val loss: 1.5309064388275146
Epoch 2890, training loss: 621.0221557617188 = 0.008436186239123344 + 100.0 * 6.210137367248535
Epoch 2890, val loss: 1.5329889059066772
Epoch 2900, training loss: 620.756103515625 = 0.008357010781764984 + 100.0 * 6.207477569580078
Epoch 2900, val loss: 1.5349698066711426
Epoch 2910, training loss: 621.0504150390625 = 0.008282442577183247 + 100.0 * 6.210421085357666
Epoch 2910, val loss: 1.5371983051300049
Epoch 2920, training loss: 620.8735961914062 = 0.008204422891139984 + 100.0 * 6.208653926849365
Epoch 2920, val loss: 1.5389642715454102
Epoch 2930, training loss: 620.814697265625 = 0.008126474916934967 + 100.0 * 6.208065509796143
Epoch 2930, val loss: 1.5411213636398315
Epoch 2940, training loss: 620.8140258789062 = 0.00805419310927391 + 100.0 * 6.208059787750244
Epoch 2940, val loss: 1.5430090427398682
Epoch 2950, training loss: 620.6337280273438 = 0.007980488240718842 + 100.0 * 6.206257343292236
Epoch 2950, val loss: 1.5449415445327759
Epoch 2960, training loss: 620.8428955078125 = 0.00791217852383852 + 100.0 * 6.208349704742432
Epoch 2960, val loss: 1.5469416379928589
Epoch 2970, training loss: 620.9447021484375 = 0.00784124806523323 + 100.0 * 6.20936918258667
Epoch 2970, val loss: 1.5488547086715698
Epoch 2980, training loss: 620.76220703125 = 0.007767221424728632 + 100.0 * 6.207544803619385
Epoch 2980, val loss: 1.5508036613464355
Epoch 2990, training loss: 620.7771606445312 = 0.00770004466176033 + 100.0 * 6.2076945304870605
Epoch 2990, val loss: 1.5524834394454956
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 861.6181640625 = 1.9336045980453491 + 100.0 * 8.596845626831055
Epoch 0, val loss: 1.9331506490707397
Epoch 10, training loss: 861.5277709960938 = 1.9252058267593384 + 100.0 * 8.596025466918945
Epoch 10, val loss: 1.9255099296569824
Epoch 20, training loss: 860.8883666992188 = 1.9145621061325073 + 100.0 * 8.589737892150879
Epoch 20, val loss: 1.915562391281128
Epoch 30, training loss: 856.5679931640625 = 1.9002243280410767 + 100.0 * 8.546677589416504
Epoch 30, val loss: 1.9018478393554688
Epoch 40, training loss: 833.6134033203125 = 1.881503701210022 + 100.0 * 8.3173189163208
Epoch 40, val loss: 1.8840469121932983
Epoch 50, training loss: 786.9028930664062 = 1.858333706855774 + 100.0 * 7.850445747375488
Epoch 50, val loss: 1.862180471420288
Epoch 60, training loss: 759.92724609375 = 1.8395477533340454 + 100.0 * 7.580877304077148
Epoch 60, val loss: 1.8456438779830933
Epoch 70, training loss: 739.1763305664062 = 1.8274956941604614 + 100.0 * 7.373487949371338
Epoch 70, val loss: 1.8351844549179077
Epoch 80, training loss: 723.9342651367188 = 1.8165199756622314 + 100.0 * 7.221177577972412
Epoch 80, val loss: 1.8249503374099731
Epoch 90, training loss: 709.0855102539062 = 1.8051700592041016 + 100.0 * 7.072803497314453
Epoch 90, val loss: 1.814754605293274
Epoch 100, training loss: 699.1253051757812 = 1.7960997819900513 + 100.0 * 6.973292350769043
Epoch 100, val loss: 1.806283950805664
Epoch 110, training loss: 692.9955444335938 = 1.784596562385559 + 100.0 * 6.912109375
Epoch 110, val loss: 1.7958152294158936
Epoch 120, training loss: 686.8549194335938 = 1.7728697061538696 + 100.0 * 6.850820064544678
Epoch 120, val loss: 1.785218596458435
Epoch 130, training loss: 679.828369140625 = 1.7634512186050415 + 100.0 * 6.780649662017822
Epoch 130, val loss: 1.7760530710220337
Epoch 140, training loss: 673.3339233398438 = 1.7539926767349243 + 100.0 * 6.715799331665039
Epoch 140, val loss: 1.7669094800949097
Epoch 150, training loss: 667.9513549804688 = 1.7429245710372925 + 100.0 * 6.662084579467773
Epoch 150, val loss: 1.7572542428970337
Epoch 160, training loss: 663.83935546875 = 1.7310494184494019 + 100.0 * 6.621082782745361
Epoch 160, val loss: 1.7473443746566772
Epoch 170, training loss: 660.7960815429688 = 1.7175225019454956 + 100.0 * 6.590785503387451
Epoch 170, val loss: 1.7358812093734741
Epoch 180, training loss: 658.233154296875 = 1.7024860382080078 + 100.0 * 6.565306186676025
Epoch 180, val loss: 1.722983717918396
Epoch 190, training loss: 656.1836547851562 = 1.6863080263137817 + 100.0 * 6.544973850250244
Epoch 190, val loss: 1.7089804410934448
Epoch 200, training loss: 654.4351806640625 = 1.6688467264175415 + 100.0 * 6.527663707733154
Epoch 200, val loss: 1.6939048767089844
Epoch 210, training loss: 653.1522216796875 = 1.64976167678833 + 100.0 * 6.515024185180664
Epoch 210, val loss: 1.6776138544082642
Epoch 220, training loss: 651.5501098632812 = 1.6296031475067139 + 100.0 * 6.499205112457275
Epoch 220, val loss: 1.66036856174469
Epoch 230, training loss: 650.1427001953125 = 1.6083815097808838 + 100.0 * 6.485342979431152
Epoch 230, val loss: 1.642247200012207
Epoch 240, training loss: 649.0040283203125 = 1.5861172676086426 + 100.0 * 6.474179267883301
Epoch 240, val loss: 1.623267650604248
Epoch 250, training loss: 647.9276733398438 = 1.562838077545166 + 100.0 * 6.463647842407227
Epoch 250, val loss: 1.6035220623016357
Epoch 260, training loss: 646.9469604492188 = 1.5390199422836304 + 100.0 * 6.454079627990723
Epoch 260, val loss: 1.5833383798599243
Epoch 270, training loss: 646.0558471679688 = 1.5145866870880127 + 100.0 * 6.445412635803223
Epoch 270, val loss: 1.5627421140670776
Epoch 280, training loss: 645.3954467773438 = 1.4897494316101074 + 100.0 * 6.439056873321533
Epoch 280, val loss: 1.5419470071792603
Epoch 290, training loss: 644.4061279296875 = 1.4645519256591797 + 100.0 * 6.429416179656982
Epoch 290, val loss: 1.5207951068878174
Epoch 300, training loss: 643.6260375976562 = 1.4394853115081787 + 100.0 * 6.421864986419678
Epoch 300, val loss: 1.499907374382019
Epoch 310, training loss: 642.9213256835938 = 1.4144268035888672 + 100.0 * 6.415069103240967
Epoch 310, val loss: 1.4792709350585938
Epoch 320, training loss: 642.8206787109375 = 1.3892360925674438 + 100.0 * 6.4143147468566895
Epoch 320, val loss: 1.458588719367981
Epoch 330, training loss: 641.5902099609375 = 1.364392638206482 + 100.0 * 6.402258396148682
Epoch 330, val loss: 1.4383809566497803
Epoch 340, training loss: 641.0171508789062 = 1.3398466110229492 + 100.0 * 6.396772861480713
Epoch 340, val loss: 1.4185216426849365
Epoch 350, training loss: 640.36376953125 = 1.3155254125595093 + 100.0 * 6.390481948852539
Epoch 350, val loss: 1.399020791053772
Epoch 360, training loss: 641.2675170898438 = 1.2912286520004272 + 100.0 * 6.399763107299805
Epoch 360, val loss: 1.379701852798462
Epoch 370, training loss: 639.6411743164062 = 1.267037272453308 + 100.0 * 6.38374137878418
Epoch 370, val loss: 1.3605972528457642
Epoch 380, training loss: 638.885498046875 = 1.2431987524032593 + 100.0 * 6.376422882080078
Epoch 380, val loss: 1.3418725728988647
Epoch 390, training loss: 638.42626953125 = 1.2194762229919434 + 100.0 * 6.372067928314209
Epoch 390, val loss: 1.323479175567627
Epoch 400, training loss: 638.6447143554688 = 1.1958634853363037 + 100.0 * 6.374488353729248
Epoch 400, val loss: 1.305388331413269
Epoch 410, training loss: 637.7180786132812 = 1.172146201133728 + 100.0 * 6.365459442138672
Epoch 410, val loss: 1.287158489227295
Epoch 420, training loss: 637.2190551757812 = 1.1485674381256104 + 100.0 * 6.3607048988342285
Epoch 420, val loss: 1.2690560817718506
Epoch 430, training loss: 636.7001342773438 = 1.1251332759857178 + 100.0 * 6.35575008392334
Epoch 430, val loss: 1.2512683868408203
Epoch 440, training loss: 636.5812377929688 = 1.1018221378326416 + 100.0 * 6.354794025421143
Epoch 440, val loss: 1.2337442636489868
Epoch 450, training loss: 636.0763549804688 = 1.0785166025161743 + 100.0 * 6.349978446960449
Epoch 450, val loss: 1.2162636518478394
Epoch 460, training loss: 636.0042724609375 = 1.0552831888198853 + 100.0 * 6.349489688873291
Epoch 460, val loss: 1.1989871263504028
Epoch 470, training loss: 635.2557983398438 = 1.0322985649108887 + 100.0 * 6.342235088348389
Epoch 470, val loss: 1.1821386814117432
Epoch 480, training loss: 634.8892211914062 = 1.0094259977340698 + 100.0 * 6.3387980461120605
Epoch 480, val loss: 1.16551673412323
Epoch 490, training loss: 635.4424438476562 = 0.9867150783538818 + 100.0 * 6.344557762145996
Epoch 490, val loss: 1.1492146253585815
Epoch 500, training loss: 634.4375 = 0.9640212059020996 + 100.0 * 6.3347344398498535
Epoch 500, val loss: 1.1329081058502197
Epoch 510, training loss: 634.0645751953125 = 0.9417629241943359 + 100.0 * 6.331227779388428
Epoch 510, val loss: 1.1172170639038086
Epoch 520, training loss: 633.8521118164062 = 0.9198696613311768 + 100.0 * 6.329322338104248
Epoch 520, val loss: 1.1020410060882568
Epoch 530, training loss: 633.7356567382812 = 0.89826500415802 + 100.0 * 6.328373908996582
Epoch 530, val loss: 1.0872722864151
Epoch 540, training loss: 633.3825073242188 = 0.8769900798797607 + 100.0 * 6.325055122375488
Epoch 540, val loss: 1.072955846786499
Epoch 550, training loss: 633.03466796875 = 0.8561862707138062 + 100.0 * 6.321784973144531
Epoch 550, val loss: 1.0592249631881714
Epoch 560, training loss: 632.86083984375 = 0.835921585559845 + 100.0 * 6.320249080657959
Epoch 560, val loss: 1.0461000204086304
Epoch 570, training loss: 632.7786865234375 = 0.8160313963890076 + 100.0 * 6.319626331329346
Epoch 570, val loss: 1.0335766077041626
Epoch 580, training loss: 632.5750122070312 = 0.7965377569198608 + 100.0 * 6.317785263061523
Epoch 580, val loss: 1.0213927030563354
Epoch 590, training loss: 632.1299438476562 = 0.7776079773902893 + 100.0 * 6.313523292541504
Epoch 590, val loss: 1.0099555253982544
Epoch 600, training loss: 631.9754028320312 = 0.7592658996582031 + 100.0 * 6.312160968780518
Epoch 600, val loss: 0.9992117285728455
Epoch 610, training loss: 632.014404296875 = 0.7413422465324402 + 100.0 * 6.31273078918457
Epoch 610, val loss: 0.9889010787010193
Epoch 620, training loss: 631.774169921875 = 0.7239199280738831 + 100.0 * 6.310502052307129
Epoch 620, val loss: 0.979388415813446
Epoch 630, training loss: 632.0474853515625 = 0.7068934440612793 + 100.0 * 6.313405990600586
Epoch 630, val loss: 0.9699782729148865
Epoch 640, training loss: 631.3335571289062 = 0.6903344392776489 + 100.0 * 6.306432723999023
Epoch 640, val loss: 0.9613521099090576
Epoch 650, training loss: 631.0610961914062 = 0.6743221879005432 + 100.0 * 6.303867816925049
Epoch 650, val loss: 0.9532286524772644
Epoch 660, training loss: 630.8502807617188 = 0.6588495373725891 + 100.0 * 6.301914215087891
Epoch 660, val loss: 0.9456846117973328
Epoch 670, training loss: 630.646484375 = 0.6438003778457642 + 100.0 * 6.300026893615723
Epoch 670, val loss: 0.9385760426521301
Epoch 680, training loss: 631.5203247070312 = 0.6292563080787659 + 100.0 * 6.308910369873047
Epoch 680, val loss: 0.9320687055587769
Epoch 690, training loss: 630.5081176757812 = 0.6147310137748718 + 100.0 * 6.298933982849121
Epoch 690, val loss: 0.9253063201904297
Epoch 700, training loss: 630.2679443359375 = 0.6008215546607971 + 100.0 * 6.296671390533447
Epoch 700, val loss: 0.9192512035369873
Epoch 710, training loss: 630.0335693359375 = 0.5874276161193848 + 100.0 * 6.294461727142334
Epoch 710, val loss: 0.9138004779815674
Epoch 720, training loss: 629.9248046875 = 0.5743834376335144 + 100.0 * 6.293503761291504
Epoch 720, val loss: 0.9086836576461792
Epoch 730, training loss: 630.6986083984375 = 0.5616698265075684 + 100.0 * 6.301369667053223
Epoch 730, val loss: 0.9038298726081848
Epoch 740, training loss: 629.7620849609375 = 0.5490185022354126 + 100.0 * 6.292130470275879
Epoch 740, val loss: 0.8990156650543213
Epoch 750, training loss: 629.8198852539062 = 0.5367828607559204 + 100.0 * 6.292830944061279
Epoch 750, val loss: 0.8947004675865173
Epoch 760, training loss: 629.4253540039062 = 0.5248534679412842 + 100.0 * 6.289005279541016
Epoch 760, val loss: 0.8905494213104248
Epoch 770, training loss: 629.2957153320312 = 0.5132597088813782 + 100.0 * 6.287824630737305
Epoch 770, val loss: 0.8866679668426514
Epoch 780, training loss: 629.3574829101562 = 0.5018743276596069 + 100.0 * 6.288556098937988
Epoch 780, val loss: 0.8829262852668762
Epoch 790, training loss: 628.963134765625 = 0.49065396189689636 + 100.0 * 6.284724712371826
Epoch 790, val loss: 0.8794910907745361
Epoch 800, training loss: 629.2967529296875 = 0.4796559512615204 + 100.0 * 6.28817081451416
Epoch 800, val loss: 0.8761534690856934
Epoch 810, training loss: 629.0682983398438 = 0.46875208616256714 + 100.0 * 6.2859954833984375
Epoch 810, val loss: 0.8727940917015076
Epoch 820, training loss: 628.7461547851562 = 0.4580051898956299 + 100.0 * 6.282881259918213
Epoch 820, val loss: 0.8697977066040039
Epoch 830, training loss: 629.0488891601562 = 0.4474092125892639 + 100.0 * 6.286015033721924
Epoch 830, val loss: 0.8666448593139648
Epoch 840, training loss: 628.708984375 = 0.4370042681694031 + 100.0 * 6.282719612121582
Epoch 840, val loss: 0.8641107082366943
Epoch 850, training loss: 628.3382568359375 = 0.42659902572631836 + 100.0 * 6.279116630554199
Epoch 850, val loss: 0.8612443208694458
Epoch 860, training loss: 628.157470703125 = 0.41642582416534424 + 100.0 * 6.277410507202148
Epoch 860, val loss: 0.8586997389793396
Epoch 870, training loss: 628.505615234375 = 0.4063319265842438 + 100.0 * 6.2809929847717285
Epoch 870, val loss: 0.8561580181121826
Epoch 880, training loss: 628.8515625 = 0.39631226658821106 + 100.0 * 6.284552574157715
Epoch 880, val loss: 0.8535948991775513
Epoch 890, training loss: 627.9634399414062 = 0.3863237500190735 + 100.0 * 6.275771141052246
Epoch 890, val loss: 0.851166307926178
Epoch 900, training loss: 627.8110961914062 = 0.376520037651062 + 100.0 * 6.274345397949219
Epoch 900, val loss: 0.848837673664093
Epoch 910, training loss: 627.6205444335938 = 0.3668881356716156 + 100.0 * 6.272536754608154
Epoch 910, val loss: 0.8467248678207397
Epoch 920, training loss: 629.0953979492188 = 0.35730239748954773 + 100.0 * 6.287381172180176
Epoch 920, val loss: 0.8444293141365051
Epoch 930, training loss: 627.6365966796875 = 0.3478517234325409 + 100.0 * 6.272887706756592
Epoch 930, val loss: 0.8424447774887085
Epoch 940, training loss: 627.5176391601562 = 0.33851996064186096 + 100.0 * 6.271791458129883
Epoch 940, val loss: 0.8404922485351562
Epoch 950, training loss: 627.2039184570312 = 0.3294118344783783 + 100.0 * 6.268744945526123
Epoch 950, val loss: 0.8387067317962646
Epoch 960, training loss: 627.1336669921875 = 0.3204774856567383 + 100.0 * 6.268131732940674
Epoch 960, val loss: 0.8370175361633301
Epoch 970, training loss: 627.6181640625 = 0.3117469251155853 + 100.0 * 6.273063659667969
Epoch 970, val loss: 0.835506021976471
Epoch 980, training loss: 628.119140625 = 0.3029792606830597 + 100.0 * 6.278161525726318
Epoch 980, val loss: 0.8337647318840027
Epoch 990, training loss: 627.2318115234375 = 0.29451096057891846 + 100.0 * 6.269372940063477
Epoch 990, val loss: 0.8322702050209045
Epoch 1000, training loss: 626.8994750976562 = 0.2863185703754425 + 100.0 * 6.266131401062012
Epoch 1000, val loss: 0.8313194513320923
Epoch 1010, training loss: 626.7173461914062 = 0.2783562242984772 + 100.0 * 6.264389514923096
Epoch 1010, val loss: 0.8304488658905029
Epoch 1020, training loss: 626.5836791992188 = 0.27061671018600464 + 100.0 * 6.2631306648254395
Epoch 1020, val loss: 0.8297460079193115
Epoch 1030, training loss: 626.548583984375 = 0.2630648612976074 + 100.0 * 6.262855529785156
Epoch 1030, val loss: 0.8291266560554504
Epoch 1040, training loss: 627.3641357421875 = 0.2556628882884979 + 100.0 * 6.271084785461426
Epoch 1040, val loss: 0.8283979296684265
Epoch 1050, training loss: 626.4752197265625 = 0.24841542541980743 + 100.0 * 6.26226806640625
Epoch 1050, val loss: 0.8278636932373047
Epoch 1060, training loss: 626.373779296875 = 0.2414366900920868 + 100.0 * 6.26132345199585
Epoch 1060, val loss: 0.8278144001960754
Epoch 1070, training loss: 626.3113403320312 = 0.23468723893165588 + 100.0 * 6.260766506195068
Epoch 1070, val loss: 0.8278644680976868
Epoch 1080, training loss: 626.5997314453125 = 0.22813814878463745 + 100.0 * 6.263715744018555
Epoch 1080, val loss: 0.8280200958251953
Epoch 1090, training loss: 626.2288208007812 = 0.22170446813106537 + 100.0 * 6.260071277618408
Epoch 1090, val loss: 0.827907145023346
Epoch 1100, training loss: 626.4442138671875 = 0.2155141830444336 + 100.0 * 6.262287139892578
Epoch 1100, val loss: 0.8284487724304199
Epoch 1110, training loss: 626.1070556640625 = 0.20939131081104279 + 100.0 * 6.258976459503174
Epoch 1110, val loss: 0.8284947276115417
Epoch 1120, training loss: 626.01171875 = 0.20356935262680054 + 100.0 * 6.258081912994385
Epoch 1120, val loss: 0.8292938470840454
Epoch 1130, training loss: 625.9437866210938 = 0.19790713489055634 + 100.0 * 6.257458209991455
Epoch 1130, val loss: 0.8298119306564331
Epoch 1140, training loss: 625.8148193359375 = 0.1923844963312149 + 100.0 * 6.256224155426025
Epoch 1140, val loss: 0.830732524394989
Epoch 1150, training loss: 626.0240478515625 = 0.18703970313072205 + 100.0 * 6.258370399475098
Epoch 1150, val loss: 0.8314188718795776
Epoch 1160, training loss: 625.816650390625 = 0.181849405169487 + 100.0 * 6.256348133087158
Epoch 1160, val loss: 0.8325462341308594
Epoch 1170, training loss: 625.5579223632812 = 0.17681004106998444 + 100.0 * 6.253810882568359
Epoch 1170, val loss: 0.8335511088371277
Epoch 1180, training loss: 625.7877807617188 = 0.17190341651439667 + 100.0 * 6.25615930557251
Epoch 1180, val loss: 0.8347378969192505
Epoch 1190, training loss: 625.6002197265625 = 0.1671643704175949 + 100.0 * 6.254330158233643
Epoch 1190, val loss: 0.8359201550483704
Epoch 1200, training loss: 625.4488525390625 = 0.16256628930568695 + 100.0 * 6.25286340713501
Epoch 1200, val loss: 0.8372898697853088
Epoch 1210, training loss: 625.782958984375 = 0.15814590454101562 + 100.0 * 6.2562479972839355
Epoch 1210, val loss: 0.8388985395431519
Epoch 1220, training loss: 625.3720092773438 = 0.15375833213329315 + 100.0 * 6.252182483673096
Epoch 1220, val loss: 0.8402377963066101
Epoch 1230, training loss: 625.2802124023438 = 0.14955978095531464 + 100.0 * 6.251306533813477
Epoch 1230, val loss: 0.8418208956718445
Epoch 1240, training loss: 625.5520629882812 = 0.1454673409461975 + 100.0 * 6.254066467285156
Epoch 1240, val loss: 0.8433303236961365
Epoch 1250, training loss: 625.1099243164062 = 0.1415179967880249 + 100.0 * 6.249683856964111
Epoch 1250, val loss: 0.8452690839767456
Epoch 1260, training loss: 625.1723022460938 = 0.1376822590827942 + 100.0 * 6.2503461837768555
Epoch 1260, val loss: 0.8471285700798035
Epoch 1270, training loss: 625.25 = 0.1339477300643921 + 100.0 * 6.251160144805908
Epoch 1270, val loss: 0.8489726781845093
Epoch 1280, training loss: 625.0452880859375 = 0.130340576171875 + 100.0 * 6.249149322509766
Epoch 1280, val loss: 0.8509815335273743
Epoch 1290, training loss: 625.8515625 = 0.1268690526485443 + 100.0 * 6.257246971130371
Epoch 1290, val loss: 0.8530630469322205
Epoch 1300, training loss: 625.212646484375 = 0.12338867783546448 + 100.0 * 6.250892162322998
Epoch 1300, val loss: 0.8550114035606384
Epoch 1310, training loss: 624.835693359375 = 0.1200755313038826 + 100.0 * 6.247156620025635
Epoch 1310, val loss: 0.857066810131073
Epoch 1320, training loss: 624.6805419921875 = 0.1168808564543724 + 100.0 * 6.245636463165283
Epoch 1320, val loss: 0.8593535423278809
Epoch 1330, training loss: 624.9479370117188 = 0.11378957331180573 + 100.0 * 6.2483415603637695
Epoch 1330, val loss: 0.8614810109138489
Epoch 1340, training loss: 624.603515625 = 0.11075206845998764 + 100.0 * 6.244927406311035
Epoch 1340, val loss: 0.864041268825531
Epoch 1350, training loss: 624.580810546875 = 0.10782225430011749 + 100.0 * 6.244729518890381
Epoch 1350, val loss: 0.8663216233253479
Epoch 1360, training loss: 624.9530639648438 = 0.1049758717417717 + 100.0 * 6.248480796813965
Epoch 1360, val loss: 0.8688903450965881
Epoch 1370, training loss: 624.5703735351562 = 0.10221633315086365 + 100.0 * 6.244681358337402
Epoch 1370, val loss: 0.8711521625518799
Epoch 1380, training loss: 624.4567260742188 = 0.09951626509428024 + 100.0 * 6.243572235107422
Epoch 1380, val loss: 0.8737235069274902
Epoch 1390, training loss: 624.7046508789062 = 0.09694728255271912 + 100.0 * 6.246077060699463
Epoch 1390, val loss: 0.8762880563735962
Epoch 1400, training loss: 624.3697509765625 = 0.09440737962722778 + 100.0 * 6.242753028869629
Epoch 1400, val loss: 0.878972053527832
Epoch 1410, training loss: 624.5242309570312 = 0.09198886156082153 + 100.0 * 6.244322299957275
Epoch 1410, val loss: 0.8817294836044312
Epoch 1420, training loss: 624.4926147460938 = 0.08959624916315079 + 100.0 * 6.244029998779297
Epoch 1420, val loss: 0.8843787312507629
Epoch 1430, training loss: 624.30029296875 = 0.08732414245605469 + 100.0 * 6.242129802703857
Epoch 1430, val loss: 0.8872812986373901
Epoch 1440, training loss: 624.3670654296875 = 0.08509363979101181 + 100.0 * 6.242819786071777
Epoch 1440, val loss: 0.8901175260543823
Epoch 1450, training loss: 624.2559204101562 = 0.08289787173271179 + 100.0 * 6.241730213165283
Epoch 1450, val loss: 0.8928501009941101
Epoch 1460, training loss: 624.17333984375 = 0.08081227540969849 + 100.0 * 6.240925312042236
Epoch 1460, val loss: 0.8958107233047485
Epoch 1470, training loss: 624.5823974609375 = 0.07879482954740524 + 100.0 * 6.2450361251831055
Epoch 1470, val loss: 0.8988377451896667
Epoch 1480, training loss: 624.2039184570312 = 0.07679879665374756 + 100.0 * 6.241271495819092
Epoch 1480, val loss: 0.9017091393470764
Epoch 1490, training loss: 623.9971923828125 = 0.07490299642086029 + 100.0 * 6.239223003387451
Epoch 1490, val loss: 0.9046505689620972
Epoch 1500, training loss: 624.0555419921875 = 0.07306914776563644 + 100.0 * 6.2398247718811035
Epoch 1500, val loss: 0.9077742695808411
Epoch 1510, training loss: 624.3154296875 = 0.07128619402647018 + 100.0 * 6.242441177368164
Epoch 1510, val loss: 0.9107835292816162
Epoch 1520, training loss: 623.906982421875 = 0.06949907541275024 + 100.0 * 6.238374710083008
Epoch 1520, val loss: 0.9135777950286865
Epoch 1530, training loss: 623.918701171875 = 0.06783023476600647 + 100.0 * 6.238509178161621
Epoch 1530, val loss: 0.9167748093605042
Epoch 1540, training loss: 624.4551391601562 = 0.06621860712766647 + 100.0 * 6.243888854980469
Epoch 1540, val loss: 0.9197679758071899
Epoch 1550, training loss: 623.8878173828125 = 0.06459729373455048 + 100.0 * 6.238232135772705
Epoch 1550, val loss: 0.922885537147522
Epoch 1560, training loss: 623.7637939453125 = 0.06306259334087372 + 100.0 * 6.2370076179504395
Epoch 1560, val loss: 0.9259825944900513
Epoch 1570, training loss: 624.0923461914062 = 0.06157869100570679 + 100.0 * 6.240307331085205
Epoch 1570, val loss: 0.9291757941246033
Epoch 1580, training loss: 623.7019653320312 = 0.06011256203055382 + 100.0 * 6.2364182472229
Epoch 1580, val loss: 0.9321823120117188
Epoch 1590, training loss: 623.5892333984375 = 0.058726705610752106 + 100.0 * 6.235305309295654
Epoch 1590, val loss: 0.9354238510131836
Epoch 1600, training loss: 623.8590698242188 = 0.05739365518093109 + 100.0 * 6.2380170822143555
Epoch 1600, val loss: 0.9386484026908875
Epoch 1610, training loss: 623.564208984375 = 0.05604733154177666 + 100.0 * 6.235081672668457
Epoch 1610, val loss: 0.9417358636856079
Epoch 1620, training loss: 623.673828125 = 0.0547502376139164 + 100.0 * 6.2361907958984375
Epoch 1620, val loss: 0.9448949098587036
Epoch 1630, training loss: 623.4340209960938 = 0.05352213233709335 + 100.0 * 6.233805179595947
Epoch 1630, val loss: 0.9480499625205994
Epoch 1640, training loss: 623.3203735351562 = 0.052306804805994034 + 100.0 * 6.232680797576904
Epoch 1640, val loss: 0.9512102007865906
Epoch 1650, training loss: 623.329833984375 = 0.051149796694517136 + 100.0 * 6.232786655426025
Epoch 1650, val loss: 0.9544236660003662
Epoch 1660, training loss: 624.149658203125 = 0.05002950131893158 + 100.0 * 6.24099588394165
Epoch 1660, val loss: 0.9574435949325562
Epoch 1670, training loss: 623.5883178710938 = 0.048909127712249756 + 100.0 * 6.235394477844238
Epoch 1670, val loss: 0.9607575535774231
Epoch 1680, training loss: 623.469482421875 = 0.047826193273067474 + 100.0 * 6.234216213226318
Epoch 1680, val loss: 0.9637308716773987
Epoch 1690, training loss: 623.2076416015625 = 0.04678884148597717 + 100.0 * 6.2316083908081055
Epoch 1690, val loss: 0.9670482277870178
Epoch 1700, training loss: 623.2122192382812 = 0.045802924782037735 + 100.0 * 6.231664657592773
Epoch 1700, val loss: 0.9702207446098328
Epoch 1710, training loss: 624.02587890625 = 0.04484624043107033 + 100.0 * 6.239809989929199
Epoch 1710, val loss: 0.9733768105506897
Epoch 1720, training loss: 623.4073486328125 = 0.043851014226675034 + 100.0 * 6.233635425567627
Epoch 1720, val loss: 0.9764835834503174
Epoch 1730, training loss: 623.1843872070312 = 0.04294881969690323 + 100.0 * 6.231414318084717
Epoch 1730, val loss: 0.9796435236930847
Epoch 1740, training loss: 623.0851440429688 = 0.04204971343278885 + 100.0 * 6.230431079864502
Epoch 1740, val loss: 0.9828501343727112
Epoch 1750, training loss: 623.4821166992188 = 0.04119520261883736 + 100.0 * 6.234409332275391
Epoch 1750, val loss: 0.9859370589256287
Epoch 1760, training loss: 623.18212890625 = 0.0403476245701313 + 100.0 * 6.231417655944824
Epoch 1760, val loss: 0.989230215549469
Epoch 1770, training loss: 622.9873657226562 = 0.039527807384729385 + 100.0 * 6.229478359222412
Epoch 1770, val loss: 0.9923306107521057
Epoch 1780, training loss: 623.4946899414062 = 0.0387546569108963 + 100.0 * 6.234559535980225
Epoch 1780, val loss: 0.9955666065216064
Epoch 1790, training loss: 622.8994140625 = 0.03794323652982712 + 100.0 * 6.228614330291748
Epoch 1790, val loss: 0.9985833168029785
Epoch 1800, training loss: 622.8721313476562 = 0.03719305619597435 + 100.0 * 6.228349685668945
Epoch 1800, val loss: 1.0018023252487183
Epoch 1810, training loss: 622.99609375 = 0.03647543862462044 + 100.0 * 6.229596138000488
Epoch 1810, val loss: 1.0049316883087158
Epoch 1820, training loss: 623.1602172851562 = 0.03575512766838074 + 100.0 * 6.2312445640563965
Epoch 1820, val loss: 1.008075475692749
Epoch 1830, training loss: 622.98388671875 = 0.03504793718457222 + 100.0 * 6.229488372802734
Epoch 1830, val loss: 1.0112299919128418
Epoch 1840, training loss: 622.9557495117188 = 0.03438634052872658 + 100.0 * 6.229213714599609
Epoch 1840, val loss: 1.0143215656280518
Epoch 1850, training loss: 622.6842041015625 = 0.033716876059770584 + 100.0 * 6.226505279541016
Epoch 1850, val loss: 1.0174676179885864
Epoch 1860, training loss: 622.7531127929688 = 0.033085547387599945 + 100.0 * 6.227200031280518
Epoch 1860, val loss: 1.0206646919250488
Epoch 1870, training loss: 623.4033203125 = 0.032488901168107986 + 100.0 * 6.233708381652832
Epoch 1870, val loss: 1.0237013101577759
Epoch 1880, training loss: 623.0148315429688 = 0.0318467877805233 + 100.0 * 6.229829788208008
Epoch 1880, val loss: 1.02659010887146
Epoch 1890, training loss: 622.69482421875 = 0.03125632554292679 + 100.0 * 6.226635932922363
Epoch 1890, val loss: 1.02975594997406
Epoch 1900, training loss: 622.7457885742188 = 0.030685456469655037 + 100.0 * 6.227150917053223
Epoch 1900, val loss: 1.032781958580017
Epoch 1910, training loss: 622.83349609375 = 0.030129067599773407 + 100.0 * 6.228033542633057
Epoch 1910, val loss: 1.0359174013137817
Epoch 1920, training loss: 622.6470336914062 = 0.029580358415842056 + 100.0 * 6.226174354553223
Epoch 1920, val loss: 1.0389434099197388
Epoch 1930, training loss: 622.572265625 = 0.02904660999774933 + 100.0 * 6.225432395935059
Epoch 1930, val loss: 1.0420546531677246
Epoch 1940, training loss: 622.4808959960938 = 0.028522048145532608 + 100.0 * 6.224524021148682
Epoch 1940, val loss: 1.0450973510742188
Epoch 1950, training loss: 623.3819580078125 = 0.028024038299918175 + 100.0 * 6.233539581298828
Epoch 1950, val loss: 1.048012375831604
Epoch 1960, training loss: 622.7252197265625 = 0.02752622216939926 + 100.0 * 6.2269768714904785
Epoch 1960, val loss: 1.0510908365249634
Epoch 1970, training loss: 622.5153198242188 = 0.02704096958041191 + 100.0 * 6.22488260269165
Epoch 1970, val loss: 1.0540274381637573
Epoch 1980, training loss: 622.6299438476562 = 0.026587072759866714 + 100.0 * 6.2260332107543945
Epoch 1980, val loss: 1.0570275783538818
Epoch 1990, training loss: 622.4617919921875 = 0.026115475222468376 + 100.0 * 6.224356651306152
Epoch 1990, val loss: 1.0599576234817505
Epoch 2000, training loss: 622.6414794921875 = 0.025663111358880997 + 100.0 * 6.226158618927002
Epoch 2000, val loss: 1.0630079507827759
Epoch 2010, training loss: 622.43115234375 = 0.025229763239622116 + 100.0 * 6.224059581756592
Epoch 2010, val loss: 1.0658971071243286
Epoch 2020, training loss: 622.4080200195312 = 0.024804290384054184 + 100.0 * 6.223832607269287
Epoch 2020, val loss: 1.0689189434051514
Epoch 2030, training loss: 622.561279296875 = 0.02437877282500267 + 100.0 * 6.225369453430176
Epoch 2030, val loss: 1.0718064308166504
Epoch 2040, training loss: 622.20361328125 = 0.023982852697372437 + 100.0 * 6.22179651260376
Epoch 2040, val loss: 1.0747319459915161
Epoch 2050, training loss: 622.2919921875 = 0.023592811077833176 + 100.0 * 6.222683429718018
Epoch 2050, val loss: 1.0776602029800415
Epoch 2060, training loss: 622.6390991210938 = 0.02321045659482479 + 100.0 * 6.22615909576416
Epoch 2060, val loss: 1.0804170370101929
Epoch 2070, training loss: 622.4498901367188 = 0.022817345336079597 + 100.0 * 6.224270820617676
Epoch 2070, val loss: 1.0834851264953613
Epoch 2080, training loss: 622.2366333007812 = 0.022446688264608383 + 100.0 * 6.222141742706299
Epoch 2080, val loss: 1.0862559080123901
Epoch 2090, training loss: 622.1656494140625 = 0.02209119126200676 + 100.0 * 6.221435546875
Epoch 2090, val loss: 1.0891568660736084
Epoch 2100, training loss: 622.1264038085938 = 0.02173727937042713 + 100.0 * 6.2210469245910645
Epoch 2100, val loss: 1.0920511484146118
Epoch 2110, training loss: 622.435546875 = 0.021398676559329033 + 100.0 * 6.2241411209106445
Epoch 2110, val loss: 1.0947694778442383
Epoch 2120, training loss: 622.2827758789062 = 0.021063653752207756 + 100.0 * 6.222617149353027
Epoch 2120, val loss: 1.097578525543213
Epoch 2130, training loss: 622.2000122070312 = 0.020729636773467064 + 100.0 * 6.221792697906494
Epoch 2130, val loss: 1.1003845930099487
Epoch 2140, training loss: 622.1564331054688 = 0.02041301317512989 + 100.0 * 6.221360683441162
Epoch 2140, val loss: 1.1032464504241943
Epoch 2150, training loss: 622.5109252929688 = 0.020105818286538124 + 100.0 * 6.224908351898193
Epoch 2150, val loss: 1.1059060096740723
Epoch 2160, training loss: 622.12060546875 = 0.01977556198835373 + 100.0 * 6.22100830078125
Epoch 2160, val loss: 1.1086597442626953
Epoch 2170, training loss: 621.934326171875 = 0.01948344148695469 + 100.0 * 6.219148635864258
Epoch 2170, val loss: 1.1114400625228882
Epoch 2180, training loss: 621.9061889648438 = 0.019193576648831367 + 100.0 * 6.218870162963867
Epoch 2180, val loss: 1.1142034530639648
Epoch 2190, training loss: 622.3973388671875 = 0.01891040802001953 + 100.0 * 6.22378396987915
Epoch 2190, val loss: 1.1168222427368164
Epoch 2200, training loss: 622.2175903320312 = 0.018625807017087936 + 100.0 * 6.221989631652832
Epoch 2200, val loss: 1.119689702987671
Epoch 2210, training loss: 622.2822875976562 = 0.018339907750487328 + 100.0 * 6.222640037536621
Epoch 2210, val loss: 1.1222753524780273
Epoch 2220, training loss: 621.9938354492188 = 0.018076922744512558 + 100.0 * 6.219757556915283
Epoch 2220, val loss: 1.1249194145202637
Epoch 2230, training loss: 621.8762817382812 = 0.01781313493847847 + 100.0 * 6.2185845375061035
Epoch 2230, val loss: 1.1276129484176636
Epoch 2240, training loss: 622.1067504882812 = 0.017556503415107727 + 100.0 * 6.220891952514648
Epoch 2240, val loss: 1.1302895545959473
Epoch 2250, training loss: 621.9471435546875 = 0.017302293330430984 + 100.0 * 6.219298839569092
Epoch 2250, val loss: 1.1329946517944336
Epoch 2260, training loss: 622.0209350585938 = 0.017055761069059372 + 100.0 * 6.220039367675781
Epoch 2260, val loss: 1.1355775594711304
Epoch 2270, training loss: 621.8299560546875 = 0.01681324653327465 + 100.0 * 6.2181315422058105
Epoch 2270, val loss: 1.1380438804626465
Epoch 2280, training loss: 621.775634765625 = 0.016580801457166672 + 100.0 * 6.21759033203125
Epoch 2280, val loss: 1.140739917755127
Epoch 2290, training loss: 622.0614013671875 = 0.016354987397789955 + 100.0 * 6.220450401306152
Epoch 2290, val loss: 1.1433427333831787
Epoch 2300, training loss: 621.9567260742188 = 0.016129085794091225 + 100.0 * 6.2194061279296875
Epoch 2300, val loss: 1.1457006931304932
Epoch 2310, training loss: 621.7854614257812 = 0.015900881960988045 + 100.0 * 6.217695236206055
Epoch 2310, val loss: 1.148422360420227
Epoch 2320, training loss: 621.6673583984375 = 0.015684569254517555 + 100.0 * 6.216516971588135
Epoch 2320, val loss: 1.1509171724319458
Epoch 2330, training loss: 622.2860107421875 = 0.015483653172850609 + 100.0 * 6.222705364227295
Epoch 2330, val loss: 1.1532506942749023
Epoch 2340, training loss: 621.704833984375 = 0.01525346003472805 + 100.0 * 6.216896057128906
Epoch 2340, val loss: 1.155999779701233
Epoch 2350, training loss: 621.6029052734375 = 0.015050780959427357 + 100.0 * 6.215878486633301
Epoch 2350, val loss: 1.1583502292633057
Epoch 2360, training loss: 621.5707397460938 = 0.014854662120342255 + 100.0 * 6.215559005737305
Epoch 2360, val loss: 1.1609420776367188
Epoch 2370, training loss: 621.7933959960938 = 0.014658295549452305 + 100.0 * 6.217787742614746
Epoch 2370, val loss: 1.1635249853134155
Epoch 2380, training loss: 621.6712036132812 = 0.01446689572185278 + 100.0 * 6.216567516326904
Epoch 2380, val loss: 1.1657867431640625
Epoch 2390, training loss: 621.793212890625 = 0.014283465221524239 + 100.0 * 6.217789173126221
Epoch 2390, val loss: 1.1682692766189575
Epoch 2400, training loss: 621.9617919921875 = 0.014094692654907703 + 100.0 * 6.219476699829102
Epoch 2400, val loss: 1.1707531213760376
Epoch 2410, training loss: 621.6148071289062 = 0.013911737129092216 + 100.0 * 6.216009140014648
Epoch 2410, val loss: 1.1730811595916748
Epoch 2420, training loss: 621.4976806640625 = 0.013730567879974842 + 100.0 * 6.214839458465576
Epoch 2420, val loss: 1.1756024360656738
Epoch 2430, training loss: 621.4690551757812 = 0.013561501167714596 + 100.0 * 6.214555263519287
Epoch 2430, val loss: 1.1779948472976685
Epoch 2440, training loss: 622.1845703125 = 0.013404134660959244 + 100.0 * 6.221711158752441
Epoch 2440, val loss: 1.180201768875122
Epoch 2450, training loss: 621.7681274414062 = 0.013218659907579422 + 100.0 * 6.217548847198486
Epoch 2450, val loss: 1.1827360391616821
Epoch 2460, training loss: 621.4796752929688 = 0.013055678457021713 + 100.0 * 6.214666366577148
Epoch 2460, val loss: 1.184938669204712
Epoch 2470, training loss: 621.3775024414062 = 0.012891964055597782 + 100.0 * 6.213646411895752
Epoch 2470, val loss: 1.1873599290847778
Epoch 2480, training loss: 621.4185180664062 = 0.012735921889543533 + 100.0 * 6.214057445526123
Epoch 2480, val loss: 1.1897565126419067
Epoch 2490, training loss: 621.925048828125 = 0.012579594738781452 + 100.0 * 6.219124794006348
Epoch 2490, val loss: 1.1919893026351929
Epoch 2500, training loss: 621.5709838867188 = 0.01242507342249155 + 100.0 * 6.215585231781006
Epoch 2500, val loss: 1.1944761276245117
Epoch 2510, training loss: 621.8477783203125 = 0.012277153320610523 + 100.0 * 6.218355178833008
Epoch 2510, val loss: 1.1964179277420044
Epoch 2520, training loss: 621.6013793945312 = 0.012126753106713295 + 100.0 * 6.215892314910889
Epoch 2520, val loss: 1.198941707611084
Epoch 2530, training loss: 621.3030395507812 = 0.011977563612163067 + 100.0 * 6.2129106521606445
Epoch 2530, val loss: 1.201055645942688
Epoch 2540, training loss: 621.3269653320312 = 0.011840681545436382 + 100.0 * 6.213151454925537
Epoch 2540, val loss: 1.2034615278244019
Epoch 2550, training loss: 621.461669921875 = 0.011702200397849083 + 100.0 * 6.214499473571777
Epoch 2550, val loss: 1.205640435218811
Epoch 2560, training loss: 621.5892333984375 = 0.011561130173504353 + 100.0 * 6.2157769203186035
Epoch 2560, val loss: 1.207902431488037
Epoch 2570, training loss: 621.3576049804688 = 0.011429348960518837 + 100.0 * 6.213461875915527
Epoch 2570, val loss: 1.2100130319595337
Epoch 2580, training loss: 621.2445068359375 = 0.011301803402602673 + 100.0 * 6.212332248687744
Epoch 2580, val loss: 1.212067723274231
Epoch 2590, training loss: 621.2598266601562 = 0.011176043190062046 + 100.0 * 6.212486743927002
Epoch 2590, val loss: 1.214306354522705
Epoch 2600, training loss: 621.6775512695312 = 0.011051393114030361 + 100.0 * 6.216665267944336
Epoch 2600, val loss: 1.2164051532745361
Epoch 2610, training loss: 621.4818725585938 = 0.010914957150816917 + 100.0 * 6.214709281921387
Epoch 2610, val loss: 1.2185828685760498
Epoch 2620, training loss: 621.130615234375 = 0.010793575085699558 + 100.0 * 6.211198329925537
Epoch 2620, val loss: 1.220688819885254
Epoch 2630, training loss: 621.2423095703125 = 0.0106792151927948 + 100.0 * 6.212316513061523
Epoch 2630, val loss: 1.2228074073791504
Epoch 2640, training loss: 621.3665161132812 = 0.010559441521763802 + 100.0 * 6.213559627532959
Epoch 2640, val loss: 1.2250244617462158
Epoch 2650, training loss: 621.1925659179688 = 0.010440588928759098 + 100.0 * 6.211821556091309
Epoch 2650, val loss: 1.2271276712417603
Epoch 2660, training loss: 621.239013671875 = 0.010328764095902443 + 100.0 * 6.212286949157715
Epoch 2660, val loss: 1.2291510105133057
Epoch 2670, training loss: 621.1766357421875 = 0.010217552073299885 + 100.0 * 6.211664199829102
Epoch 2670, val loss: 1.2312829494476318
Epoch 2680, training loss: 621.2726440429688 = 0.010108268819749355 + 100.0 * 6.212625026702881
Epoch 2680, val loss: 1.2333484888076782
Epoch 2690, training loss: 621.5543212890625 = 0.01000047568231821 + 100.0 * 6.215443134307861
Epoch 2690, val loss: 1.2354730367660522
Epoch 2700, training loss: 621.2791748046875 = 0.009894128888845444 + 100.0 * 6.212692737579346
Epoch 2700, val loss: 1.2373806238174438
Epoch 2710, training loss: 621.151123046875 = 0.009785700589418411 + 100.0 * 6.211413383483887
Epoch 2710, val loss: 1.2392598390579224
Epoch 2720, training loss: 621.102294921875 = 0.009685196913778782 + 100.0 * 6.210926055908203
Epoch 2720, val loss: 1.241485357284546
Epoch 2730, training loss: 621.4107055664062 = 0.009585837833583355 + 100.0 * 6.214011192321777
Epoch 2730, val loss: 1.2434548139572144
Epoch 2740, training loss: 621.1713256835938 = 0.009479349479079247 + 100.0 * 6.211618423461914
Epoch 2740, val loss: 1.2454577684402466
Epoch 2750, training loss: 620.9949340820312 = 0.009378989227116108 + 100.0 * 6.209855556488037
Epoch 2750, val loss: 1.2475794553756714
Epoch 2760, training loss: 620.9705810546875 = 0.009286952205002308 + 100.0 * 6.20961332321167
Epoch 2760, val loss: 1.249617576599121
Epoch 2770, training loss: 620.9995727539062 = 0.009192436933517456 + 100.0 * 6.209903717041016
Epoch 2770, val loss: 1.2516329288482666
Epoch 2780, training loss: 621.680908203125 = 0.009097924456000328 + 100.0 * 6.2167181968688965
Epoch 2780, val loss: 1.253529667854309
Epoch 2790, training loss: 621.1688232421875 = 0.00900958850979805 + 100.0 * 6.211597919464111
Epoch 2790, val loss: 1.2553198337554932
Epoch 2800, training loss: 620.9580078125 = 0.008916147984564304 + 100.0 * 6.209490776062012
Epoch 2800, val loss: 1.2573684453964233
Epoch 2810, training loss: 620.985107421875 = 0.008829974569380283 + 100.0 * 6.2097625732421875
Epoch 2810, val loss: 1.2593694925308228
Epoch 2820, training loss: 621.4046630859375 = 0.008745712228119373 + 100.0 * 6.213959217071533
Epoch 2820, val loss: 1.2612569332122803
Epoch 2830, training loss: 621.0140991210938 = 0.00865862239152193 + 100.0 * 6.210054397583008
Epoch 2830, val loss: 1.2631498575210571
Epoch 2840, training loss: 621.0747680664062 = 0.008576586842536926 + 100.0 * 6.210661888122559
Epoch 2840, val loss: 1.264984369277954
Epoch 2850, training loss: 620.9859008789062 = 0.00849149189889431 + 100.0 * 6.209774017333984
Epoch 2850, val loss: 1.2669976949691772
Epoch 2860, training loss: 620.8074340820312 = 0.008404876105487347 + 100.0 * 6.2079901695251465
Epoch 2860, val loss: 1.2690531015396118
Epoch 2870, training loss: 621.068359375 = 0.008327369578182697 + 100.0 * 6.21060037612915
Epoch 2870, val loss: 1.2710514068603516
Epoch 2880, training loss: 620.8081665039062 = 0.00824771635234356 + 100.0 * 6.207999229431152
Epoch 2880, val loss: 1.2727344036102295
Epoch 2890, training loss: 621.117919921875 = 0.008171041496098042 + 100.0 * 6.211097717285156
Epoch 2890, val loss: 1.2744355201721191
Epoch 2900, training loss: 621.05859375 = 0.008090632036328316 + 100.0 * 6.210504531860352
Epoch 2900, val loss: 1.2765119075775146
Epoch 2910, training loss: 621.0658569335938 = 0.00801557395607233 + 100.0 * 6.210578918457031
Epoch 2910, val loss: 1.2782832384109497
Epoch 2920, training loss: 620.8809814453125 = 0.007939351722598076 + 100.0 * 6.208730220794678
Epoch 2920, val loss: 1.280041217803955
Epoch 2930, training loss: 620.6453247070312 = 0.007867810316383839 + 100.0 * 6.206374645233154
Epoch 2930, val loss: 1.2819782495498657
Epoch 2940, training loss: 620.70556640625 = 0.007799819111824036 + 100.0 * 6.206977844238281
Epoch 2940, val loss: 1.283860206604004
Epoch 2950, training loss: 621.1842651367188 = 0.0077346558682620525 + 100.0 * 6.211765289306641
Epoch 2950, val loss: 1.2855110168457031
Epoch 2960, training loss: 620.8192138671875 = 0.00765815656632185 + 100.0 * 6.208116054534912
Epoch 2960, val loss: 1.2873414754867554
Epoch 2970, training loss: 620.7474365234375 = 0.007588454522192478 + 100.0 * 6.207398891448975
Epoch 2970, val loss: 1.289180040359497
Epoch 2980, training loss: 620.734130859375 = 0.007519991602748632 + 100.0 * 6.207266330718994
Epoch 2980, val loss: 1.2909513711929321
Epoch 2990, training loss: 620.7803955078125 = 0.007451777346432209 + 100.0 * 6.207729339599609
Epoch 2990, val loss: 1.2927932739257812
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 861.635009765625 = 1.947737216949463 + 100.0 * 8.596872329711914
Epoch 0, val loss: 1.9440972805023193
Epoch 10, training loss: 861.5709838867188 = 1.9397798776626587 + 100.0 * 8.596312522888184
Epoch 10, val loss: 1.9365532398223877
Epoch 20, training loss: 861.15576171875 = 1.93010675907135 + 100.0 * 8.592256546020508
Epoch 20, val loss: 1.927117943763733
Epoch 30, training loss: 858.3295288085938 = 1.917607069015503 + 100.0 * 8.564119338989258
Epoch 30, val loss: 1.9146534204483032
Epoch 40, training loss: 843.3623046875 = 1.9008089303970337 + 100.0 * 8.4146146774292
Epoch 40, val loss: 1.8979663848876953
Epoch 50, training loss: 793.3247680664062 = 1.8805052042007446 + 100.0 * 7.914443016052246
Epoch 50, val loss: 1.878042459487915
Epoch 60, training loss: 754.507080078125 = 1.86288321018219 + 100.0 * 7.526442050933838
Epoch 60, val loss: 1.8613978624343872
Epoch 70, training loss: 726.891845703125 = 1.850614309310913 + 100.0 * 7.250412464141846
Epoch 70, val loss: 1.8488132953643799
Epoch 80, training loss: 710.8770751953125 = 1.8378138542175293 + 100.0 * 7.090392589569092
Epoch 80, val loss: 1.8360224962234497
Epoch 90, training loss: 697.2655029296875 = 1.8247817754745483 + 100.0 * 6.954407215118408
Epoch 90, val loss: 1.8231804370880127
Epoch 100, training loss: 684.7177734375 = 1.812996745109558 + 100.0 * 6.829048156738281
Epoch 100, val loss: 1.8115901947021484
Epoch 110, training loss: 678.0892944335938 = 1.8018499612808228 + 100.0 * 6.762874126434326
Epoch 110, val loss: 1.799781084060669
Epoch 120, training loss: 672.5067749023438 = 1.7893221378326416 + 100.0 * 6.707174777984619
Epoch 120, val loss: 1.7868136167526245
Epoch 130, training loss: 668.0748291015625 = 1.7770477533340454 + 100.0 * 6.662977695465088
Epoch 130, val loss: 1.7743717432022095
Epoch 140, training loss: 664.6039428710938 = 1.7646580934524536 + 100.0 * 6.628392696380615
Epoch 140, val loss: 1.7619264125823975
Epoch 150, training loss: 661.3460693359375 = 1.7514575719833374 + 100.0 * 6.595945835113525
Epoch 150, val loss: 1.7490363121032715
Epoch 160, training loss: 658.3029174804688 = 1.737717866897583 + 100.0 * 6.565651893615723
Epoch 160, val loss: 1.7357721328735352
Epoch 170, training loss: 655.78369140625 = 1.723052978515625 + 100.0 * 6.5406060218811035
Epoch 170, val loss: 1.7216100692749023
Epoch 180, training loss: 653.5396118164062 = 1.707000970840454 + 100.0 * 6.5183258056640625
Epoch 180, val loss: 1.7063966989517212
Epoch 190, training loss: 651.5338745117188 = 1.6897958517074585 + 100.0 * 6.498440742492676
Epoch 190, val loss: 1.6902055740356445
Epoch 200, training loss: 650.486572265625 = 1.6711509227752686 + 100.0 * 6.488154411315918
Epoch 200, val loss: 1.6728869676589966
Epoch 210, training loss: 648.2717895507812 = 1.6510207653045654 + 100.0 * 6.466207981109619
Epoch 210, val loss: 1.65419340133667
Epoch 220, training loss: 647.0319213867188 = 1.6292827129364014 + 100.0 * 6.454026222229004
Epoch 220, val loss: 1.6343154907226562
Epoch 230, training loss: 645.7993774414062 = 1.6058717966079712 + 100.0 * 6.441934585571289
Epoch 230, val loss: 1.613133430480957
Epoch 240, training loss: 645.3724975585938 = 1.5808281898498535 + 100.0 * 6.4379167556762695
Epoch 240, val loss: 1.5905568599700928
Epoch 250, training loss: 644.05908203125 = 1.554080605506897 + 100.0 * 6.425049781799316
Epoch 250, val loss: 1.566869854927063
Epoch 260, training loss: 643.0731201171875 = 1.525944471359253 + 100.0 * 6.41547155380249
Epoch 260, val loss: 1.5421395301818848
Epoch 270, training loss: 642.3319702148438 = 1.496547818183899 + 100.0 * 6.40835428237915
Epoch 270, val loss: 1.5165036916732788
Epoch 280, training loss: 642.3502197265625 = 1.465985894203186 + 100.0 * 6.408842086791992
Epoch 280, val loss: 1.4901195764541626
Epoch 290, training loss: 641.2792358398438 = 1.4344944953918457 + 100.0 * 6.398447513580322
Epoch 290, val loss: 1.4633679389953613
Epoch 300, training loss: 640.4649658203125 = 1.4024757146835327 + 100.0 * 6.390625
Epoch 300, val loss: 1.436240553855896
Epoch 310, training loss: 639.9225463867188 = 1.3702211380004883 + 100.0 * 6.385523319244385
Epoch 310, val loss: 1.4094345569610596
Epoch 320, training loss: 639.7926025390625 = 1.3378195762634277 + 100.0 * 6.384547710418701
Epoch 320, val loss: 1.3828823566436768
Epoch 330, training loss: 639.0707397460938 = 1.3051953315734863 + 100.0 * 6.377655506134033
Epoch 330, val loss: 1.3564854860305786
Epoch 340, training loss: 638.3427734375 = 1.2731958627700806 + 100.0 * 6.3706955909729
Epoch 340, val loss: 1.3308886289596558
Epoch 350, training loss: 637.856689453125 = 1.2415484189987183 + 100.0 * 6.366150856018066
Epoch 350, val loss: 1.306067705154419
Epoch 360, training loss: 637.9175415039062 = 1.2104687690734863 + 100.0 * 6.36707067489624
Epoch 360, val loss: 1.2820197343826294
Epoch 370, training loss: 637.1670532226562 = 1.1799800395965576 + 100.0 * 6.359870433807373
Epoch 370, val loss: 1.258780598640442
Epoch 380, training loss: 636.6091918945312 = 1.1503170728683472 + 100.0 * 6.354588508605957
Epoch 380, val loss: 1.2365423440933228
Epoch 390, training loss: 637.208740234375 = 1.1215044260025024 + 100.0 * 6.360872268676758
Epoch 390, val loss: 1.215223789215088
Epoch 400, training loss: 635.832275390625 = 1.0931917428970337 + 100.0 * 6.347390651702881
Epoch 400, val loss: 1.1944955587387085
Epoch 410, training loss: 635.5037231445312 = 1.0658986568450928 + 100.0 * 6.3443779945373535
Epoch 410, val loss: 1.1748170852661133
Epoch 420, training loss: 635.1019897460938 = 1.039461374282837 + 100.0 * 6.340625286102295
Epoch 420, val loss: 1.1560052633285522
Epoch 430, training loss: 635.2679443359375 = 1.0139107704162598 + 100.0 * 6.342540264129639
Epoch 430, val loss: 1.1381287574768066
Epoch 440, training loss: 634.5432739257812 = 0.9887557625770569 + 100.0 * 6.335545063018799
Epoch 440, val loss: 1.1207350492477417
Epoch 450, training loss: 634.1961059570312 = 0.9646586179733276 + 100.0 * 6.332314491271973
Epoch 450, val loss: 1.104204535484314
Epoch 460, training loss: 633.8935546875 = 0.9412789940834045 + 100.0 * 6.329522609710693
Epoch 460, val loss: 1.0884819030761719
Epoch 470, training loss: 633.585693359375 = 0.9186692833900452 + 100.0 * 6.3266706466674805
Epoch 470, val loss: 1.0735167264938354
Epoch 480, training loss: 634.2590942382812 = 0.8967304229736328 + 100.0 * 6.33362340927124
Epoch 480, val loss: 1.059206485748291
Epoch 490, training loss: 633.2428588867188 = 0.8752576112747192 + 100.0 * 6.323676109313965
Epoch 490, val loss: 1.0452724695205688
Epoch 500, training loss: 632.8124389648438 = 0.8546057939529419 + 100.0 * 6.319578170776367
Epoch 500, val loss: 1.0321152210235596
Epoch 510, training loss: 632.5927734375 = 0.8346160650253296 + 100.0 * 6.317581653594971
Epoch 510, val loss: 1.019654393196106
Epoch 520, training loss: 632.6103515625 = 0.8152675628662109 + 100.0 * 6.317951202392578
Epoch 520, val loss: 1.007826566696167
Epoch 530, training loss: 632.587158203125 = 0.7962926030158997 + 100.0 * 6.317909240722656
Epoch 530, val loss: 0.9963802695274353
Epoch 540, training loss: 631.9548950195312 = 0.777908980846405 + 100.0 * 6.311769962310791
Epoch 540, val loss: 0.9854249358177185
Epoch 550, training loss: 631.7357177734375 = 0.7600625157356262 + 100.0 * 6.309756278991699
Epoch 550, val loss: 0.9750502705574036
Epoch 560, training loss: 631.6246337890625 = 0.7426766157150269 + 100.0 * 6.30881929397583
Epoch 560, val loss: 0.9651495814323425
Epoch 570, training loss: 631.44189453125 = 0.7256203293800354 + 100.0 * 6.307162761688232
Epoch 570, val loss: 0.955561101436615
Epoch 580, training loss: 631.1497192382812 = 0.7089006304740906 + 100.0 * 6.304408073425293
Epoch 580, val loss: 0.9463226795196533
Epoch 590, training loss: 630.9221801757812 = 0.6925984025001526 + 100.0 * 6.302295684814453
Epoch 590, val loss: 0.9374313354492188
Epoch 600, training loss: 630.912109375 = 0.676652193069458 + 100.0 * 6.30235481262207
Epoch 600, val loss: 0.9289729595184326
Epoch 610, training loss: 630.677490234375 = 0.6608349084854126 + 100.0 * 6.300166606903076
Epoch 610, val loss: 0.9205480217933655
Epoch 620, training loss: 630.6165771484375 = 0.6453777551651001 + 100.0 * 6.299712181091309
Epoch 620, val loss: 0.912582278251648
Epoch 630, training loss: 630.2461547851562 = 0.6301394104957581 + 100.0 * 6.2961602210998535
Epoch 630, val loss: 0.904747724533081
Epoch 640, training loss: 630.251220703125 = 0.6152094006538391 + 100.0 * 6.296360015869141
Epoch 640, val loss: 0.8972080945968628
Epoch 650, training loss: 630.0291137695312 = 0.6004253625869751 + 100.0 * 6.294287204742432
Epoch 650, val loss: 0.8900541663169861
Epoch 660, training loss: 629.8908081054688 = 0.5858991146087646 + 100.0 * 6.293049335479736
Epoch 660, val loss: 0.8829619884490967
Epoch 670, training loss: 629.7774047851562 = 0.5715627074241638 + 100.0 * 6.292058944702148
Epoch 670, val loss: 0.876203715801239
Epoch 680, training loss: 629.5512084960938 = 0.5574654936790466 + 100.0 * 6.2899370193481445
Epoch 680, val loss: 0.8697213530540466
Epoch 690, training loss: 629.8311767578125 = 0.5435516238212585 + 100.0 * 6.292876243591309
Epoch 690, val loss: 0.8632001280784607
Epoch 700, training loss: 629.5618286132812 = 0.5298884510993958 + 100.0 * 6.290319442749023
Epoch 700, val loss: 0.8574909567832947
Epoch 710, training loss: 629.113525390625 = 0.516409158706665 + 100.0 * 6.285971164703369
Epoch 710, val loss: 0.8516461849212646
Epoch 720, training loss: 628.9414672851562 = 0.5032102465629578 + 100.0 * 6.284382343292236
Epoch 720, val loss: 0.8461560010910034
Epoch 730, training loss: 629.0552978515625 = 0.49023815989494324 + 100.0 * 6.285650730133057
Epoch 730, val loss: 0.8410629630088806
Epoch 740, training loss: 629.2996215820312 = 0.4774780869483948 + 100.0 * 6.28822135925293
Epoch 740, val loss: 0.8361369371414185
Epoch 750, training loss: 628.9352416992188 = 0.4647097587585449 + 100.0 * 6.28470516204834
Epoch 750, val loss: 0.8308542966842651
Epoch 760, training loss: 628.413330078125 = 0.4523768126964569 + 100.0 * 6.279609680175781
Epoch 760, val loss: 0.8264977931976318
Epoch 770, training loss: 628.314453125 = 0.4402560591697693 + 100.0 * 6.27874231338501
Epoch 770, val loss: 0.8220319151878357
Epoch 780, training loss: 628.2018432617188 = 0.4283410906791687 + 100.0 * 6.277734756469727
Epoch 780, val loss: 0.8178171515464783
Epoch 790, training loss: 628.8235473632812 = 0.41663190722465515 + 100.0 * 6.284069061279297
Epoch 790, val loss: 0.8135601878166199
Epoch 800, training loss: 628.2578125 = 0.40513479709625244 + 100.0 * 6.278526306152344
Epoch 800, val loss: 0.8100367784500122
Epoch 810, training loss: 627.916259765625 = 0.3938281238079071 + 100.0 * 6.275224685668945
Epoch 810, val loss: 0.8062103986740112
Epoch 820, training loss: 628.373291015625 = 0.38277262449264526 + 100.0 * 6.279905319213867
Epoch 820, val loss: 0.8026504516601562
Epoch 830, training loss: 627.82275390625 = 0.37198424339294434 + 100.0 * 6.274507522583008
Epoch 830, val loss: 0.7994142174720764
Epoch 840, training loss: 627.5552978515625 = 0.36139562726020813 + 100.0 * 6.271938800811768
Epoch 840, val loss: 0.796215832233429
Epoch 850, training loss: 627.5410766601562 = 0.3511356711387634 + 100.0 * 6.271899700164795
Epoch 850, val loss: 0.7934717535972595
Epoch 860, training loss: 627.5529174804688 = 0.340984582901001 + 100.0 * 6.272119045257568
Epoch 860, val loss: 0.790364146232605
Epoch 870, training loss: 627.3807373046875 = 0.33108362555503845 + 100.0 * 6.270496845245361
Epoch 870, val loss: 0.7877978682518005
Epoch 880, training loss: 627.1574096679688 = 0.3214609622955322 + 100.0 * 6.268359661102295
Epoch 880, val loss: 0.7850573062896729
Epoch 890, training loss: 627.0706787109375 = 0.3121304214000702 + 100.0 * 6.267585277557373
Epoch 890, val loss: 0.7826880216598511
Epoch 900, training loss: 627.4921264648438 = 0.30305030941963196 + 100.0 * 6.271891117095947
Epoch 900, val loss: 0.7805050611495972
Epoch 910, training loss: 627.0853881835938 = 0.29416805505752563 + 100.0 * 6.267911911010742
Epoch 910, val loss: 0.778576672077179
Epoch 920, training loss: 627.1260375976562 = 0.2855355441570282 + 100.0 * 6.268405437469482
Epoch 920, val loss: 0.7766318917274475
Epoch 930, training loss: 627.2377319335938 = 0.2770788371562958 + 100.0 * 6.269606113433838
Epoch 930, val loss: 0.7745945453643799
Epoch 940, training loss: 626.76806640625 = 0.2689632475376129 + 100.0 * 6.26499080657959
Epoch 940, val loss: 0.773288369178772
Epoch 950, training loss: 626.5875854492188 = 0.2610536515712738 + 100.0 * 6.263265132904053
Epoch 950, val loss: 0.7717528343200684
Epoch 960, training loss: 626.5040283203125 = 0.2534314692020416 + 100.0 * 6.262506008148193
Epoch 960, val loss: 0.7705590128898621
Epoch 970, training loss: 626.9879150390625 = 0.24606376886367798 + 100.0 * 6.267417907714844
Epoch 970, val loss: 0.769546389579773
Epoch 980, training loss: 626.3969116210938 = 0.23878324031829834 + 100.0 * 6.2615814208984375
Epoch 980, val loss: 0.7680724859237671
Epoch 990, training loss: 626.2350463867188 = 0.23182065784931183 + 100.0 * 6.2600321769714355
Epoch 990, val loss: 0.7672613263130188
Epoch 1000, training loss: 626.2981567382812 = 0.22507821023464203 + 100.0 * 6.260730743408203
Epoch 1000, val loss: 0.7664846777915955
Epoch 1010, training loss: 626.3311767578125 = 0.21849733591079712 + 100.0 * 6.261126518249512
Epoch 1010, val loss: 0.7658779621124268
Epoch 1020, training loss: 626.1117553710938 = 0.21216900646686554 + 100.0 * 6.25899600982666
Epoch 1020, val loss: 0.765537440776825
Epoch 1030, training loss: 626.0716552734375 = 0.20605991780757904 + 100.0 * 6.258656024932861
Epoch 1030, val loss: 0.7653653621673584
Epoch 1040, training loss: 626.5178833007812 = 0.2001665085554123 + 100.0 * 6.263176918029785
Epoch 1040, val loss: 0.765407383441925
Epoch 1050, training loss: 625.9570922851562 = 0.19438767433166504 + 100.0 * 6.257627010345459
Epoch 1050, val loss: 0.7648138403892517
Epoch 1060, training loss: 625.79541015625 = 0.18884189426898956 + 100.0 * 6.256065368652344
Epoch 1060, val loss: 0.7651140093803406
Epoch 1070, training loss: 625.7786254882812 = 0.18351145088672638 + 100.0 * 6.255950927734375
Epoch 1070, val loss: 0.7653563022613525
Epoch 1080, training loss: 625.9569702148438 = 0.1783147156238556 + 100.0 * 6.257786273956299
Epoch 1080, val loss: 0.7656763195991516
Epoch 1090, training loss: 625.7335815429688 = 0.17321865260601044 + 100.0 * 6.255603790283203
Epoch 1090, val loss: 0.765801191329956
Epoch 1100, training loss: 625.5048217773438 = 0.16835245490074158 + 100.0 * 6.2533650398254395
Epoch 1100, val loss: 0.7663972973823547
Epoch 1110, training loss: 625.4869995117188 = 0.16363658010959625 + 100.0 * 6.253233909606934
Epoch 1110, val loss: 0.7670146822929382
Epoch 1120, training loss: 625.8121337890625 = 0.15906588733196259 + 100.0 * 6.25653076171875
Epoch 1120, val loss: 0.767425000667572
Epoch 1130, training loss: 625.5020751953125 = 0.15462979674339294 + 100.0 * 6.253474712371826
Epoch 1130, val loss: 0.7687052488327026
Epoch 1140, training loss: 625.2994995117188 = 0.15031097829341888 + 100.0 * 6.251491546630859
Epoch 1140, val loss: 0.7694830298423767
Epoch 1150, training loss: 625.2142944335938 = 0.14617232978343964 + 100.0 * 6.250680923461914
Epoch 1150, val loss: 0.7706050872802734
Epoch 1160, training loss: 625.377685546875 = 0.14216986298561096 + 100.0 * 6.252355575561523
Epoch 1160, val loss: 0.771800696849823
Epoch 1170, training loss: 625.337158203125 = 0.13824865221977234 + 100.0 * 6.251988887786865
Epoch 1170, val loss: 0.7728751301765442
Epoch 1180, training loss: 625.1123046875 = 0.134483203291893 + 100.0 * 6.2497782707214355
Epoch 1180, val loss: 0.774456799030304
Epoch 1190, training loss: 624.9653930664062 = 0.13081008195877075 + 100.0 * 6.248345851898193
Epoch 1190, val loss: 0.7756601572036743
Epoch 1200, training loss: 624.8714599609375 = 0.12727956473827362 + 100.0 * 6.24744176864624
Epoch 1200, val loss: 0.7772236466407776
Epoch 1210, training loss: 625.3182373046875 = 0.123871810734272 + 100.0 * 6.251943111419678
Epoch 1210, val loss: 0.7788383364677429
Epoch 1220, training loss: 625.1199951171875 = 0.12052740156650543 + 100.0 * 6.249994277954102
Epoch 1220, val loss: 0.7804783582687378
Epoch 1230, training loss: 625.1392211914062 = 0.117276631295681 + 100.0 * 6.250219821929932
Epoch 1230, val loss: 0.7820258140563965
Epoch 1240, training loss: 624.686767578125 = 0.11413095891475677 + 100.0 * 6.245726585388184
Epoch 1240, val loss: 0.7835606336593628
Epoch 1250, training loss: 624.7160034179688 = 0.11111274361610413 + 100.0 * 6.246048927307129
Epoch 1250, val loss: 0.7853574156761169
Epoch 1260, training loss: 624.8894653320312 = 0.1081765815615654 + 100.0 * 6.2478132247924805
Epoch 1260, val loss: 0.7869951128959656
Epoch 1270, training loss: 625.3344116210938 = 0.10529646277427673 + 100.0 * 6.252291202545166
Epoch 1270, val loss: 0.7889624834060669
Epoch 1280, training loss: 624.755615234375 = 0.10255730152130127 + 100.0 * 6.246530532836914
Epoch 1280, val loss: 0.7911796569824219
Epoch 1290, training loss: 624.4950561523438 = 0.09985054284334183 + 100.0 * 6.24395227432251
Epoch 1290, val loss: 0.7929638624191284
Epoch 1300, training loss: 624.3753051757812 = 0.09727945923805237 + 100.0 * 6.2427802085876465
Epoch 1300, val loss: 0.7951436638832092
Epoch 1310, training loss: 624.6670532226562 = 0.0947846993803978 + 100.0 * 6.245722770690918
Epoch 1310, val loss: 0.7973366975784302
Epoch 1320, training loss: 624.4615478515625 = 0.09232197701931 + 100.0 * 6.243691921234131
Epoch 1320, val loss: 0.7991712689399719
Epoch 1330, training loss: 624.520263671875 = 0.08992999792098999 + 100.0 * 6.2443037033081055
Epoch 1330, val loss: 0.8014746904373169
Epoch 1340, training loss: 624.2550659179688 = 0.08763182163238525 + 100.0 * 6.241673946380615
Epoch 1340, val loss: 0.803621232509613
Epoch 1350, training loss: 624.333984375 = 0.08542054146528244 + 100.0 * 6.242485046386719
Epoch 1350, val loss: 0.8057752847671509
Epoch 1360, training loss: 624.6300048828125 = 0.08324310183525085 + 100.0 * 6.245467662811279
Epoch 1360, val loss: 0.8079835772514343
Epoch 1370, training loss: 624.1434936523438 = 0.08114224672317505 + 100.0 * 6.240623950958252
Epoch 1370, val loss: 0.8105655312538147
Epoch 1380, training loss: 624.01123046875 = 0.07911139726638794 + 100.0 * 6.239321231842041
Epoch 1380, val loss: 0.8130018711090088
Epoch 1390, training loss: 623.9901733398438 = 0.07715035229921341 + 100.0 * 6.239130020141602
Epoch 1390, val loss: 0.8154448866844177
Epoch 1400, training loss: 624.559326171875 = 0.07525599002838135 + 100.0 * 6.244840621948242
Epoch 1400, val loss: 0.818086564540863
Epoch 1410, training loss: 624.1326293945312 = 0.07340236008167267 + 100.0 * 6.240592002868652
Epoch 1410, val loss: 0.8204821944236755
Epoch 1420, training loss: 624.171875 = 0.07159310579299927 + 100.0 * 6.241003036499023
Epoch 1420, val loss: 0.823126494884491
Epoch 1430, training loss: 624.20849609375 = 0.06984934955835342 + 100.0 * 6.241386890411377
Epoch 1430, val loss: 0.8254660367965698
Epoch 1440, training loss: 623.8033447265625 = 0.06814267486333847 + 100.0 * 6.23735237121582
Epoch 1440, val loss: 0.8280068039894104
Epoch 1450, training loss: 623.8554077148438 = 0.06651539355516434 + 100.0 * 6.237888813018799
Epoch 1450, val loss: 0.8305200934410095
Epoch 1460, training loss: 624.1264038085938 = 0.0649251937866211 + 100.0 * 6.240614891052246
Epoch 1460, val loss: 0.8328539133071899
Epoch 1470, training loss: 623.80029296875 = 0.0633748397231102 + 100.0 * 6.237369537353516
Epoch 1470, val loss: 0.8357167840003967
Epoch 1480, training loss: 623.6713256835938 = 0.06186942011117935 + 100.0 * 6.2360944747924805
Epoch 1480, val loss: 0.8384780883789062
Epoch 1490, training loss: 623.6469116210938 = 0.06041945144534111 + 100.0 * 6.235865116119385
Epoch 1490, val loss: 0.8408885598182678
Epoch 1500, training loss: 623.6192016601562 = 0.05902984365820885 + 100.0 * 6.235601902008057
Epoch 1500, val loss: 0.8437469601631165
Epoch 1510, training loss: 624.1325073242188 = 0.05766984820365906 + 100.0 * 6.240748405456543
Epoch 1510, val loss: 0.8462783694267273
Epoch 1520, training loss: 623.8682861328125 = 0.056319162249565125 + 100.0 * 6.238119602203369
Epoch 1520, val loss: 0.8488872647285461
Epoch 1530, training loss: 623.8211669921875 = 0.05503298342227936 + 100.0 * 6.237661361694336
Epoch 1530, val loss: 0.8517414331436157
Epoch 1540, training loss: 623.8885498046875 = 0.053780246526002884 + 100.0 * 6.23834753036499
Epoch 1540, val loss: 0.8543401956558228
Epoch 1550, training loss: 623.5809326171875 = 0.052550218999385834 + 100.0 * 6.235283851623535
Epoch 1550, val loss: 0.8570202589035034
Epoch 1560, training loss: 623.4317016601562 = 0.05136072263121605 + 100.0 * 6.233803749084473
Epoch 1560, val loss: 0.8595412969589233
Epoch 1570, training loss: 623.4314575195312 = 0.05022795498371124 + 100.0 * 6.23381233215332
Epoch 1570, val loss: 0.8624210357666016
Epoch 1580, training loss: 623.9874267578125 = 0.04912920668721199 + 100.0 * 6.239383220672607
Epoch 1580, val loss: 0.8654575347900391
Epoch 1590, training loss: 623.4453125 = 0.04803220182657242 + 100.0 * 6.233972549438477
Epoch 1590, val loss: 0.8672232031822205
Epoch 1600, training loss: 623.287353515625 = 0.04697194695472717 + 100.0 * 6.232403755187988
Epoch 1600, val loss: 0.8702823519706726
Epoch 1610, training loss: 623.357421875 = 0.04595872014760971 + 100.0 * 6.233114719390869
Epoch 1610, val loss: 0.8728621602058411
Epoch 1620, training loss: 623.5826416015625 = 0.04496903717517853 + 100.0 * 6.235376834869385
Epoch 1620, val loss: 0.8754586577415466
Epoch 1630, training loss: 623.3082885742188 = 0.04400477558374405 + 100.0 * 6.232642650604248
Epoch 1630, val loss: 0.8783222436904907
Epoch 1640, training loss: 623.400146484375 = 0.043060705065727234 + 100.0 * 6.233570575714111
Epoch 1640, val loss: 0.8811063766479492
Epoch 1650, training loss: 623.9393920898438 = 0.04216061532497406 + 100.0 * 6.238972187042236
Epoch 1650, val loss: 0.8837500810623169
Epoch 1660, training loss: 623.2615356445312 = 0.041231583803892136 + 100.0 * 6.232202529907227
Epoch 1660, val loss: 0.8860430717468262
Epoch 1670, training loss: 623.1045532226562 = 0.04037280008196831 + 100.0 * 6.230641841888428
Epoch 1670, val loss: 0.8887227177619934
Epoch 1680, training loss: 623.0049438476562 = 0.03953762352466583 + 100.0 * 6.229653835296631
Epoch 1680, val loss: 0.8914992809295654
Epoch 1690, training loss: 623.0297241210938 = 0.03873567655682564 + 100.0 * 6.229909896850586
Epoch 1690, val loss: 0.8942511081695557
Epoch 1700, training loss: 623.6676025390625 = 0.037954479455947876 + 100.0 * 6.2362961769104
Epoch 1700, val loss: 0.8970815539360046
Epoch 1710, training loss: 623.2284545898438 = 0.0371631421148777 + 100.0 * 6.231913089752197
Epoch 1710, val loss: 0.8993422985076904
Epoch 1720, training loss: 623.3552856445312 = 0.03640645742416382 + 100.0 * 6.233189105987549
Epoch 1720, val loss: 0.9023911356925964
Epoch 1730, training loss: 622.9754638671875 = 0.03566642105579376 + 100.0 * 6.229398250579834
Epoch 1730, val loss: 0.9043909907341003
Epoch 1740, training loss: 623.0469360351562 = 0.03495587408542633 + 100.0 * 6.230119705200195
Epoch 1740, val loss: 0.9073693752288818
Epoch 1750, training loss: 623.0355224609375 = 0.03426454961299896 + 100.0 * 6.230012893676758
Epoch 1750, val loss: 0.909919023513794
Epoch 1760, training loss: 623.1786499023438 = 0.03359813988208771 + 100.0 * 6.231451034545898
Epoch 1760, val loss: 0.9125187993049622
Epoch 1770, training loss: 622.76123046875 = 0.032925110310316086 + 100.0 * 6.227283477783203
Epoch 1770, val loss: 0.9148609638214111
Epoch 1780, training loss: 622.8721923828125 = 0.03228887543082237 + 100.0 * 6.22839879989624
Epoch 1780, val loss: 0.9173920750617981
Epoch 1790, training loss: 623.2789306640625 = 0.03166525065898895 + 100.0 * 6.2324724197387695
Epoch 1790, val loss: 0.9197007417678833
Epoch 1800, training loss: 622.6992797851562 = 0.031046072021126747 + 100.0 * 6.226682186126709
Epoch 1800, val loss: 0.9224728345870972
Epoch 1810, training loss: 622.6168823242188 = 0.030449675396084785 + 100.0 * 6.225864410400391
Epoch 1810, val loss: 0.9249380230903625
Epoch 1820, training loss: 622.6071166992188 = 0.029881684109568596 + 100.0 * 6.225771903991699
Epoch 1820, val loss: 0.9274975061416626
Epoch 1830, training loss: 623.1212768554688 = 0.029337720945477486 + 100.0 * 6.230918884277344
Epoch 1830, val loss: 0.9299963116645813
Epoch 1840, training loss: 623.4854736328125 = 0.02878381498157978 + 100.0 * 6.234566688537598
Epoch 1840, val loss: 0.9331204891204834
Epoch 1850, training loss: 622.827392578125 = 0.028227906674146652 + 100.0 * 6.227992057800293
Epoch 1850, val loss: 0.9343732595443726
Epoch 1860, training loss: 622.5120239257812 = 0.027698198333382607 + 100.0 * 6.2248430252075195
Epoch 1860, val loss: 0.9373554587364197
Epoch 1870, training loss: 622.47705078125 = 0.02719895727932453 + 100.0 * 6.224498271942139
Epoch 1870, val loss: 0.9397249817848206
Epoch 1880, training loss: 622.4598388671875 = 0.0267132930457592 + 100.0 * 6.224330902099609
Epoch 1880, val loss: 0.9421195983886719
Epoch 1890, training loss: 622.6743774414062 = 0.02624327316880226 + 100.0 * 6.2264814376831055
Epoch 1890, val loss: 0.9445347785949707
Epoch 1900, training loss: 622.86865234375 = 0.025775814428925514 + 100.0 * 6.228428840637207
Epoch 1900, val loss: 0.9471871852874756
Epoch 1910, training loss: 622.5211791992188 = 0.025296825915575027 + 100.0 * 6.224958896636963
Epoch 1910, val loss: 0.9491896033287048
Epoch 1920, training loss: 622.464111328125 = 0.024843914434313774 + 100.0 * 6.224392890930176
Epoch 1920, val loss: 0.9513062834739685
Epoch 1930, training loss: 622.3485717773438 = 0.024411775171756744 + 100.0 * 6.223241806030273
Epoch 1930, val loss: 0.9539667367935181
Epoch 1940, training loss: 622.3275146484375 = 0.023990221321582794 + 100.0 * 6.2230353355407715
Epoch 1940, val loss: 0.9561833143234253
Epoch 1950, training loss: 622.9546508789062 = 0.023580852895975113 + 100.0 * 6.229310989379883
Epoch 1950, val loss: 0.958104133605957
Epoch 1960, training loss: 622.4434814453125 = 0.023169677704572678 + 100.0 * 6.224203109741211
Epoch 1960, val loss: 0.960599422454834
Epoch 1970, training loss: 622.3056030273438 = 0.02277289889752865 + 100.0 * 6.222828388214111
Epoch 1970, val loss: 0.9630000591278076
Epoch 1980, training loss: 622.341796875 = 0.022389790043234825 + 100.0 * 6.223194122314453
Epoch 1980, val loss: 0.9651159048080444
Epoch 1990, training loss: 622.69580078125 = 0.02201729081571102 + 100.0 * 6.2267374992370605
Epoch 1990, val loss: 0.9673750400543213
Epoch 2000, training loss: 622.3554077148438 = 0.021648069843649864 + 100.0 * 6.223337650299072
Epoch 2000, val loss: 0.9698816537857056
Epoch 2010, training loss: 622.2924194335938 = 0.021288352087140083 + 100.0 * 6.222711086273193
Epoch 2010, val loss: 0.9720168709754944
Epoch 2020, training loss: 622.9255981445312 = 0.020949294790625572 + 100.0 * 6.229046821594238
Epoch 2020, val loss: 0.9745030403137207
Epoch 2030, training loss: 622.23095703125 = 0.020586689934134483 + 100.0 * 6.222103595733643
Epoch 2030, val loss: 0.9757993221282959
Epoch 2040, training loss: 622.0647583007812 = 0.020255405455827713 + 100.0 * 6.220444679260254
Epoch 2040, val loss: 0.9782795310020447
Epoch 2050, training loss: 622.1123657226562 = 0.019938280805945396 + 100.0 * 6.220923900604248
Epoch 2050, val loss: 0.9803609251976013
Epoch 2060, training loss: 622.7089233398438 = 0.01962643675506115 + 100.0 * 6.226892471313477
Epoch 2060, val loss: 0.9822724461555481
Epoch 2070, training loss: 622.2828369140625 = 0.01930462010204792 + 100.0 * 6.222635746002197
Epoch 2070, val loss: 0.9846417307853699
Epoch 2080, training loss: 622.115966796875 = 0.019001062959432602 + 100.0 * 6.2209696769714355
Epoch 2080, val loss: 0.9864640831947327
Epoch 2090, training loss: 622.0804443359375 = 0.018704816699028015 + 100.0 * 6.220617771148682
Epoch 2090, val loss: 0.9889010787010193
Epoch 2100, training loss: 622.5546875 = 0.018420841544866562 + 100.0 * 6.225362300872803
Epoch 2100, val loss: 0.991009533405304
Epoch 2110, training loss: 622.0106201171875 = 0.018129747360944748 + 100.0 * 6.2199249267578125
Epoch 2110, val loss: 0.9924421310424805
Epoch 2120, training loss: 622.0219116210938 = 0.017853470519185066 + 100.0 * 6.220040321350098
Epoch 2120, val loss: 0.9942992925643921
Epoch 2130, training loss: 622.3975219726562 = 0.017584573477506638 + 100.0 * 6.223799228668213
Epoch 2130, val loss: 0.9964245557785034
Epoch 2140, training loss: 622.1420288085938 = 0.01731891557574272 + 100.0 * 6.221246719360352
Epoch 2140, val loss: 0.9985689520835876
Epoch 2150, training loss: 621.9428100585938 = 0.017056627199053764 + 100.0 * 6.219257831573486
Epoch 2150, val loss: 1.0004886388778687
Epoch 2160, training loss: 621.925048828125 = 0.016808366402983665 + 100.0 * 6.219082355499268
Epoch 2160, val loss: 1.0024980306625366
Epoch 2170, training loss: 622.2551879882812 = 0.016564827412366867 + 100.0 * 6.222386360168457
Epoch 2170, val loss: 1.0043103694915771
Epoch 2180, training loss: 622.1278686523438 = 0.016314292326569557 + 100.0 * 6.221115589141846
Epoch 2180, val loss: 1.0057264566421509
Epoch 2190, training loss: 622.0860595703125 = 0.016075000166893005 + 100.0 * 6.220700263977051
Epoch 2190, val loss: 1.0078628063201904
Epoch 2200, training loss: 621.8704833984375 = 0.01584244892001152 + 100.0 * 6.218546390533447
Epoch 2200, val loss: 1.0099725723266602
Epoch 2210, training loss: 621.7689208984375 = 0.015614953823387623 + 100.0 * 6.217533111572266
Epoch 2210, val loss: 1.0117217302322388
Epoch 2220, training loss: 621.8292236328125 = 0.015400253236293793 + 100.0 * 6.218138217926025
Epoch 2220, val loss: 1.0138709545135498
Epoch 2230, training loss: 622.3163452148438 = 0.015183762647211552 + 100.0 * 6.223011493682861
Epoch 2230, val loss: 1.0157055854797363
Epoch 2240, training loss: 621.8397216796875 = 0.014969760552048683 + 100.0 * 6.218247890472412
Epoch 2240, val loss: 1.0170214176177979
Epoch 2250, training loss: 621.6688232421875 = 0.014761339873075485 + 100.0 * 6.216540813446045
Epoch 2250, val loss: 1.0193192958831787
Epoch 2260, training loss: 621.8247680664062 = 0.014563590288162231 + 100.0 * 6.218101501464844
Epoch 2260, val loss: 1.0211914777755737
Epoch 2270, training loss: 621.8800659179688 = 0.014364038594067097 + 100.0 * 6.218657493591309
Epoch 2270, val loss: 1.0228025913238525
Epoch 2280, training loss: 621.8333129882812 = 0.014166688546538353 + 100.0 * 6.218191623687744
Epoch 2280, val loss: 1.0244077444076538
Epoch 2290, training loss: 621.6471557617188 = 0.013973741792142391 + 100.0 * 6.216331481933594
Epoch 2290, val loss: 1.0262013673782349
Epoch 2300, training loss: 621.57763671875 = 0.013787804171442986 + 100.0 * 6.215638637542725
Epoch 2300, val loss: 1.0278205871582031
Epoch 2310, training loss: 621.8245849609375 = 0.013613485731184483 + 100.0 * 6.218109607696533
Epoch 2310, val loss: 1.029550313949585
Epoch 2320, training loss: 621.7754516601562 = 0.013430853374302387 + 100.0 * 6.217620372772217
Epoch 2320, val loss: 1.0311553478240967
Epoch 2330, training loss: 621.7020874023438 = 0.01324948389083147 + 100.0 * 6.216888427734375
Epoch 2330, val loss: 1.0329113006591797
Epoch 2340, training loss: 622.0375366210938 = 0.013076179660856724 + 100.0 * 6.220244884490967
Epoch 2340, val loss: 1.033888339996338
Epoch 2350, training loss: 621.4932250976562 = 0.01290432270616293 + 100.0 * 6.214803218841553
Epoch 2350, val loss: 1.0359762907028198
Epoch 2360, training loss: 621.5642700195312 = 0.012739361263811588 + 100.0 * 6.21551513671875
Epoch 2360, val loss: 1.0375758409500122
Epoch 2370, training loss: 621.6620483398438 = 0.012578093446791172 + 100.0 * 6.216495037078857
Epoch 2370, val loss: 1.0388035774230957
Epoch 2380, training loss: 621.650390625 = 0.012422389350831509 + 100.0 * 6.216379165649414
Epoch 2380, val loss: 1.040750503540039
Epoch 2390, training loss: 621.3666381835938 = 0.012267405167222023 + 100.0 * 6.21354341506958
Epoch 2390, val loss: 1.042554497718811
Epoch 2400, training loss: 621.6141967773438 = 0.012119281105697155 + 100.0 * 6.216020584106445
Epoch 2400, val loss: 1.0440893173217773
Epoch 2410, training loss: 621.6716918945312 = 0.011969826184213161 + 100.0 * 6.216597557067871
Epoch 2410, val loss: 1.045555830001831
Epoch 2420, training loss: 621.5105590820312 = 0.011820834130048752 + 100.0 * 6.214987277984619
Epoch 2420, val loss: 1.0468730926513672
Epoch 2430, training loss: 621.5838623046875 = 0.011675695888698101 + 100.0 * 6.21572208404541
Epoch 2430, val loss: 1.0481798648834229
Epoch 2440, training loss: 621.67041015625 = 0.01153644174337387 + 100.0 * 6.216588973999023
Epoch 2440, val loss: 1.0496476888656616
Epoch 2450, training loss: 621.4894409179688 = 0.011400023475289345 + 100.0 * 6.214780330657959
Epoch 2450, val loss: 1.0517123937606812
Epoch 2460, training loss: 621.4966430664062 = 0.011266767047345638 + 100.0 * 6.214853763580322
Epoch 2460, val loss: 1.0534001588821411
Epoch 2470, training loss: 621.2710571289062 = 0.011130996979773045 + 100.0 * 6.212599277496338
Epoch 2470, val loss: 1.0546002388000488
Epoch 2480, training loss: 621.3574829101562 = 0.011004061438143253 + 100.0 * 6.213464736938477
Epoch 2480, val loss: 1.055808663368225
Epoch 2490, training loss: 621.5579223632812 = 0.01088173221796751 + 100.0 * 6.215470314025879
Epoch 2490, val loss: 1.057430624961853
Epoch 2500, training loss: 621.6793823242188 = 0.010756916366517544 + 100.0 * 6.216686248779297
Epoch 2500, val loss: 1.0593161582946777
Epoch 2510, training loss: 621.2503051757812 = 0.010626214556396008 + 100.0 * 6.212396621704102
Epoch 2510, val loss: 1.060373306274414
Epoch 2520, training loss: 621.2855224609375 = 0.01050556544214487 + 100.0 * 6.212750434875488
Epoch 2520, val loss: 1.0619641542434692
Epoch 2530, training loss: 621.5106811523438 = 0.01039248239248991 + 100.0 * 6.21500301361084
Epoch 2530, val loss: 1.0636345148086548
Epoch 2540, training loss: 621.4307861328125 = 0.010274864733219147 + 100.0 * 6.214205265045166
Epoch 2540, val loss: 1.0645020008087158
Epoch 2550, training loss: 621.3241577148438 = 0.010156025178730488 + 100.0 * 6.21314001083374
Epoch 2550, val loss: 1.065574049949646
Epoch 2560, training loss: 621.2056274414062 = 0.010045127011835575 + 100.0 * 6.211955547332764
Epoch 2560, val loss: 1.0671312808990479
Epoch 2570, training loss: 621.1531372070312 = 0.009936433285474777 + 100.0 * 6.211431980133057
Epoch 2570, val loss: 1.068385362625122
Epoch 2580, training loss: 621.4735107421875 = 0.009830331429839134 + 100.0 * 6.21463680267334
Epoch 2580, val loss: 1.0694940090179443
Epoch 2590, training loss: 621.117431640625 = 0.009723217226564884 + 100.0 * 6.2110772132873535
Epoch 2590, val loss: 1.0713855028152466
Epoch 2600, training loss: 621.36767578125 = 0.009620550088584423 + 100.0 * 6.213580131530762
Epoch 2600, val loss: 1.0729097127914429
Epoch 2610, training loss: 621.3091430664062 = 0.009516856633126736 + 100.0 * 6.212996482849121
Epoch 2610, val loss: 1.0735678672790527
Epoch 2620, training loss: 621.1378173828125 = 0.009417346678674221 + 100.0 * 6.211284160614014
Epoch 2620, val loss: 1.0749056339263916
Epoch 2630, training loss: 621.5909423828125 = 0.009318938478827477 + 100.0 * 6.215816020965576
Epoch 2630, val loss: 1.075798511505127
Epoch 2640, training loss: 621.0789184570312 = 0.009219550527632236 + 100.0 * 6.210697174072266
Epoch 2640, val loss: 1.0779920816421509
Epoch 2650, training loss: 621.0269165039062 = 0.009126830846071243 + 100.0 * 6.210177421569824
Epoch 2650, val loss: 1.0789695978164673
Epoch 2660, training loss: 621.068359375 = 0.009034463204443455 + 100.0 * 6.210593223571777
Epoch 2660, val loss: 1.0800198316574097
Epoch 2670, training loss: 621.444580078125 = 0.008944959379732609 + 100.0 * 6.214355945587158
Epoch 2670, val loss: 1.0813686847686768
Epoch 2680, training loss: 621.1634521484375 = 0.00885156262665987 + 100.0 * 6.211545944213867
Epoch 2680, val loss: 1.0827685594558716
Epoch 2690, training loss: 621.2975463867188 = 0.00876224972307682 + 100.0 * 6.212887763977051
Epoch 2690, val loss: 1.0837037563323975
Epoch 2700, training loss: 620.8531494140625 = 0.008674015291035175 + 100.0 * 6.208444595336914
Epoch 2700, val loss: 1.0852349996566772
Epoch 2710, training loss: 620.903564453125 = 0.00859056506305933 + 100.0 * 6.208949565887451
Epoch 2710, val loss: 1.086483359336853
Epoch 2720, training loss: 620.958251953125 = 0.008509484119713306 + 100.0 * 6.209497928619385
Epoch 2720, val loss: 1.0878742933273315
Epoch 2730, training loss: 621.9474487304688 = 0.008432080037891865 + 100.0 * 6.219390392303467
Epoch 2730, val loss: 1.0895315408706665
Epoch 2740, training loss: 621.1968383789062 = 0.008338985964655876 + 100.0 * 6.21188497543335
Epoch 2740, val loss: 1.0894856452941895
Epoch 2750, training loss: 620.9263916015625 = 0.008260169997811317 + 100.0 * 6.209181785583496
Epoch 2750, val loss: 1.0912501811981201
Epoch 2760, training loss: 620.8818969726562 = 0.008182303979992867 + 100.0 * 6.208737373352051
Epoch 2760, val loss: 1.0924321413040161
Epoch 2770, training loss: 621.5855102539062 = 0.008108816109597683 + 100.0 * 6.215774059295654
Epoch 2770, val loss: 1.0936602354049683
Epoch 2780, training loss: 621.0479736328125 = 0.00803028978407383 + 100.0 * 6.210399150848389
Epoch 2780, val loss: 1.0942041873931885
Epoch 2790, training loss: 620.8961181640625 = 0.007955063134431839 + 100.0 * 6.208881855010986
Epoch 2790, val loss: 1.095971703529358
Epoch 2800, training loss: 620.8072509765625 = 0.007880833931267262 + 100.0 * 6.207993984222412
Epoch 2800, val loss: 1.0967926979064941
Epoch 2810, training loss: 621.3919067382812 = 0.007812252268195152 + 100.0 * 6.213840961456299
Epoch 2810, val loss: 1.0979530811309814
Epoch 2820, training loss: 620.9259033203125 = 0.007737354375422001 + 100.0 * 6.209181308746338
Epoch 2820, val loss: 1.0985918045043945
Epoch 2830, training loss: 620.7301025390625 = 0.0076653193682432175 + 100.0 * 6.207223892211914
Epoch 2830, val loss: 1.0998300313949585
Epoch 2840, training loss: 620.739990234375 = 0.007596504408866167 + 100.0 * 6.207324028015137
Epoch 2840, val loss: 1.1010346412658691
Epoch 2850, training loss: 620.9679565429688 = 0.007529961410909891 + 100.0 * 6.209604740142822
Epoch 2850, val loss: 1.1016942262649536
Epoch 2860, training loss: 621.1215209960938 = 0.00746355764567852 + 100.0 * 6.2111406326293945
Epoch 2860, val loss: 1.1028213500976562
Epoch 2870, training loss: 620.9784545898438 = 0.007400560192763805 + 100.0 * 6.209710597991943
Epoch 2870, val loss: 1.104385256767273
Epoch 2880, training loss: 620.7026977539062 = 0.0073327431455254555 + 100.0 * 6.206954002380371
Epoch 2880, val loss: 1.1051976680755615
Epoch 2890, training loss: 621.169677734375 = 0.007272134535014629 + 100.0 * 6.2116241455078125
Epoch 2890, val loss: 1.106704592704773
Epoch 2900, training loss: 620.7681884765625 = 0.007208850234746933 + 100.0 * 6.207610130310059
Epoch 2900, val loss: 1.1071600914001465
Epoch 2910, training loss: 620.655029296875 = 0.007143128197640181 + 100.0 * 6.206478595733643
Epoch 2910, val loss: 1.1082394123077393
Epoch 2920, training loss: 620.5983276367188 = 0.007084656041115522 + 100.0 * 6.2059125900268555
Epoch 2920, val loss: 1.109425663948059
Epoch 2930, training loss: 620.6845703125 = 0.007027717307209969 + 100.0 * 6.206775665283203
Epoch 2930, val loss: 1.1105619668960571
Epoch 2940, training loss: 621.52392578125 = 0.0069735292345285416 + 100.0 * 6.215169429779053
Epoch 2940, val loss: 1.111890435218811
Epoch 2950, training loss: 621.0057373046875 = 0.0069105979055166245 + 100.0 * 6.209988117218018
Epoch 2950, val loss: 1.1119434833526611
Epoch 2960, training loss: 620.8013916015625 = 0.006852685008198023 + 100.0 * 6.207945823669434
Epoch 2960, val loss: 1.1135330200195312
Epoch 2970, training loss: 620.8117065429688 = 0.006796515546739101 + 100.0 * 6.208049297332764
Epoch 2970, val loss: 1.11424720287323
Epoch 2980, training loss: 620.6915893554688 = 0.0067406315356493 + 100.0 * 6.206848621368408
Epoch 2980, val loss: 1.1151798963546753
Epoch 2990, training loss: 620.561279296875 = 0.006685412954539061 + 100.0 * 6.205545425415039
Epoch 2990, val loss: 1.1158289909362793
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8323668950975225
The final CL Acc:0.76914, 0.04237, The final GNN Acc:0.83764, 0.00383
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10566])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6210327148438 = 1.9387717247009277 + 100.0 * 8.596822738647461
Epoch 0, val loss: 1.935963749885559
Epoch 10, training loss: 861.5185546875 = 1.930498480796814 + 100.0 * 8.595880508422852
Epoch 10, val loss: 1.9270977973937988
Epoch 20, training loss: 860.85546875 = 1.920113205909729 + 100.0 * 8.589353561401367
Epoch 20, val loss: 1.9158164262771606
Epoch 30, training loss: 856.3898315429688 = 1.9065899848937988 + 100.0 * 8.544832229614258
Epoch 30, val loss: 1.9011633396148682
Epoch 40, training loss: 830.23828125 = 1.8906348943710327 + 100.0 * 8.283476829528809
Epoch 40, val loss: 1.8845162391662598
Epoch 50, training loss: 763.6845092773438 = 1.8728773593902588 + 100.0 * 7.61811637878418
Epoch 50, val loss: 1.8666791915893555
Epoch 60, training loss: 732.37939453125 = 1.8607964515686035 + 100.0 * 7.3051862716674805
Epoch 60, val loss: 1.8554084300994873
Epoch 70, training loss: 709.096435546875 = 1.8508484363555908 + 100.0 * 7.072455883026123
Epoch 70, val loss: 1.8448965549468994
Epoch 80, training loss: 693.269775390625 = 1.841198444366455 + 100.0 * 6.914286136627197
Epoch 80, val loss: 1.8351144790649414
Epoch 90, training loss: 682.1520385742188 = 1.8325796127319336 + 100.0 * 6.803194522857666
Epoch 90, val loss: 1.8265546560287476
Epoch 100, training loss: 673.5140380859375 = 1.8257246017456055 + 100.0 * 6.716883182525635
Epoch 100, val loss: 1.8197822570800781
Epoch 110, training loss: 668.577880859375 = 1.819077491760254 + 100.0 * 6.667587757110596
Epoch 110, val loss: 1.8132209777832031
Epoch 120, training loss: 664.7198486328125 = 1.8123363256454468 + 100.0 * 6.629075050354004
Epoch 120, val loss: 1.806742548942566
Epoch 130, training loss: 661.526123046875 = 1.8060365915298462 + 100.0 * 6.597200870513916
Epoch 130, val loss: 1.800667405128479
Epoch 140, training loss: 659.18994140625 = 1.7999671697616577 + 100.0 * 6.573899745941162
Epoch 140, val loss: 1.7949504852294922
Epoch 150, training loss: 656.6406860351562 = 1.7937126159667969 + 100.0 * 6.5484700202941895
Epoch 150, val loss: 1.7893097400665283
Epoch 160, training loss: 654.5582275390625 = 1.7873984575271606 + 100.0 * 6.527708053588867
Epoch 160, val loss: 1.783728837966919
Epoch 170, training loss: 652.7250366210938 = 1.7808351516723633 + 100.0 * 6.509442329406738
Epoch 170, val loss: 1.7779686450958252
Epoch 180, training loss: 651.6555786132812 = 1.7737677097320557 + 100.0 * 6.498818397521973
Epoch 180, val loss: 1.7718621492385864
Epoch 190, training loss: 649.874267578125 = 1.7661683559417725 + 100.0 * 6.481081008911133
Epoch 190, val loss: 1.7653999328613281
Epoch 200, training loss: 648.4881591796875 = 1.7579960823059082 + 100.0 * 6.467301845550537
Epoch 200, val loss: 1.7585092782974243
Epoch 210, training loss: 647.3320922851562 = 1.7491590976715088 + 100.0 * 6.455829620361328
Epoch 210, val loss: 1.7511540651321411
Epoch 220, training loss: 646.3101806640625 = 1.7395356893539429 + 100.0 * 6.445706367492676
Epoch 220, val loss: 1.7431937456130981
Epoch 230, training loss: 645.394287109375 = 1.7290972471237183 + 100.0 * 6.436651706695557
Epoch 230, val loss: 1.7346153259277344
Epoch 240, training loss: 644.8872680664062 = 1.7178277969360352 + 100.0 * 6.431694030761719
Epoch 240, val loss: 1.725427269935608
Epoch 250, training loss: 643.7730712890625 = 1.7055844068527222 + 100.0 * 6.420674800872803
Epoch 250, val loss: 1.7154994010925293
Epoch 260, training loss: 643.2362670898438 = 1.6924091577529907 + 100.0 * 6.415439128875732
Epoch 260, val loss: 1.7048683166503906
Epoch 270, training loss: 642.3893432617188 = 1.6781972646713257 + 100.0 * 6.407111167907715
Epoch 270, val loss: 1.6934458017349243
Epoch 280, training loss: 641.8775634765625 = 1.6630277633666992 + 100.0 * 6.4021453857421875
Epoch 280, val loss: 1.6813558340072632
Epoch 290, training loss: 641.3765258789062 = 1.6466857194900513 + 100.0 * 6.397298336029053
Epoch 290, val loss: 1.6684640645980835
Epoch 300, training loss: 640.6264038085938 = 1.629382848739624 + 100.0 * 6.389969825744629
Epoch 300, val loss: 1.6549168825149536
Epoch 310, training loss: 640.0079345703125 = 1.611114501953125 + 100.0 * 6.383968353271484
Epoch 310, val loss: 1.6406888961791992
Epoch 320, training loss: 640.1236572265625 = 1.5918498039245605 + 100.0 * 6.385317802429199
Epoch 320, val loss: 1.6257915496826172
Epoch 330, training loss: 639.2589721679688 = 1.5715000629425049 + 100.0 * 6.376874923706055
Epoch 330, val loss: 1.6101970672607422
Epoch 340, training loss: 638.6622924804688 = 1.5503498315811157 + 100.0 * 6.371119499206543
Epoch 340, val loss: 1.5941067934036255
Epoch 350, training loss: 638.149169921875 = 1.5285003185272217 + 100.0 * 6.366206645965576
Epoch 350, val loss: 1.5776963233947754
Epoch 360, training loss: 637.7288818359375 = 1.5058950185775757 + 100.0 * 6.362229347229004
Epoch 360, val loss: 1.5609577894210815
Epoch 370, training loss: 637.290771484375 = 1.482641339302063 + 100.0 * 6.358080863952637
Epoch 370, val loss: 1.5438719987869263
Epoch 380, training loss: 636.8779907226562 = 1.4589728116989136 + 100.0 * 6.354190349578857
Epoch 380, val loss: 1.526726245880127
Epoch 390, training loss: 636.859130859375 = 1.434815764427185 + 100.0 * 6.354243278503418
Epoch 390, val loss: 1.5093878507614136
Epoch 400, training loss: 636.1982421875 = 1.4102368354797363 + 100.0 * 6.3478803634643555
Epoch 400, val loss: 1.4919731616973877
Epoch 410, training loss: 635.718994140625 = 1.3855935335159302 + 100.0 * 6.343333721160889
Epoch 410, val loss: 1.4748163223266602
Epoch 420, training loss: 635.7993774414062 = 1.360826849937439 + 100.0 * 6.344385147094727
Epoch 420, val loss: 1.4579588174819946
Epoch 430, training loss: 635.0120239257812 = 1.336129069328308 + 100.0 * 6.336759090423584
Epoch 430, val loss: 1.4413206577301025
Epoch 440, training loss: 634.7119140625 = 1.3115363121032715 + 100.0 * 6.334003925323486
Epoch 440, val loss: 1.425213098526001
Epoch 450, training loss: 635.2711181640625 = 1.2871507406234741 + 100.0 * 6.339839458465576
Epoch 450, val loss: 1.4094310998916626
Epoch 460, training loss: 634.1896362304688 = 1.262682557106018 + 100.0 * 6.3292694091796875
Epoch 460, val loss: 1.3939955234527588
Epoch 470, training loss: 633.775390625 = 1.2388192415237427 + 100.0 * 6.3253655433654785
Epoch 470, val loss: 1.379239559173584
Epoch 480, training loss: 633.8953247070312 = 1.215503454208374 + 100.0 * 6.326797962188721
Epoch 480, val loss: 1.3651522397994995
Epoch 490, training loss: 633.3654174804688 = 1.1925466060638428 + 100.0 * 6.321728706359863
Epoch 490, val loss: 1.3517457246780396
Epoch 500, training loss: 633.0119018554688 = 1.1701374053955078 + 100.0 * 6.318417549133301
Epoch 500, val loss: 1.3390198945999146
Epoch 510, training loss: 633.2093505859375 = 1.1482969522476196 + 100.0 * 6.320610046386719
Epoch 510, val loss: 1.3270375728607178
Epoch 520, training loss: 632.8724975585938 = 1.1268887519836426 + 100.0 * 6.317456245422363
Epoch 520, val loss: 1.3154994249343872
Epoch 530, training loss: 632.3658447265625 = 1.1059391498565674 + 100.0 * 6.312598705291748
Epoch 530, val loss: 1.3048025369644165
Epoch 540, training loss: 631.9560546875 = 1.0856952667236328 + 100.0 * 6.308703422546387
Epoch 540, val loss: 1.294893741607666
Epoch 550, training loss: 631.883056640625 = 1.0661053657531738 + 100.0 * 6.308169364929199
Epoch 550, val loss: 1.2857211828231812
Epoch 560, training loss: 631.6659545898438 = 1.0469642877578735 + 100.0 * 6.306190490722656
Epoch 560, val loss: 1.277100920677185
Epoch 570, training loss: 631.3518676757812 = 1.0283619165420532 + 100.0 * 6.303234577178955
Epoch 570, val loss: 1.269225835800171
Epoch 580, training loss: 631.656494140625 = 1.0103107690811157 + 100.0 * 6.306461811065674
Epoch 580, val loss: 1.2618855237960815
Epoch 590, training loss: 631.12353515625 = 0.9926170110702515 + 100.0 * 6.301309108734131
Epoch 590, val loss: 1.2550890445709229
Epoch 600, training loss: 630.7548828125 = 0.9755619168281555 + 100.0 * 6.297792911529541
Epoch 600, val loss: 1.249055027961731
Epoch 610, training loss: 630.5315551757812 = 0.9589921832084656 + 100.0 * 6.2957258224487305
Epoch 610, val loss: 1.2434566020965576
Epoch 620, training loss: 631.721435546875 = 0.9426589608192444 + 100.0 * 6.3077874183654785
Epoch 620, val loss: 1.2383689880371094
Epoch 630, training loss: 630.455810546875 = 0.9266958832740784 + 100.0 * 6.295290946960449
Epoch 630, val loss: 1.2334074974060059
Epoch 640, training loss: 629.9832763671875 = 0.9112549424171448 + 100.0 * 6.290720462799072
Epoch 640, val loss: 1.2293096780776978
Epoch 650, training loss: 629.7913208007812 = 0.8963155746459961 + 100.0 * 6.288950443267822
Epoch 650, val loss: 1.2257232666015625
Epoch 660, training loss: 629.64599609375 = 0.8817223906517029 + 100.0 * 6.287642955780029
Epoch 660, val loss: 1.2226344347000122
Epoch 670, training loss: 630.3228149414062 = 0.8672916293144226 + 100.0 * 6.294555187225342
Epoch 670, val loss: 1.21973717212677
Epoch 680, training loss: 629.6773681640625 = 0.8529722690582275 + 100.0 * 6.288244247436523
Epoch 680, val loss: 1.2167649269104004
Epoch 690, training loss: 629.2400512695312 = 0.8391035199165344 + 100.0 * 6.2840094566345215
Epoch 690, val loss: 1.214475154876709
Epoch 700, training loss: 629.0404052734375 = 0.8256699442863464 + 100.0 * 6.28214693069458
Epoch 700, val loss: 1.2127628326416016
Epoch 710, training loss: 629.089599609375 = 0.8125070929527283 + 100.0 * 6.282771110534668
Epoch 710, val loss: 1.211279034614563
Epoch 720, training loss: 629.0010375976562 = 0.7994267344474792 + 100.0 * 6.282015800476074
Epoch 720, val loss: 1.2099844217300415
Epoch 730, training loss: 628.8755493164062 = 0.7865480184555054 + 100.0 * 6.280889987945557
Epoch 730, val loss: 1.2087457180023193
Epoch 740, training loss: 628.5609130859375 = 0.774017333984375 + 100.0 * 6.27786922454834
Epoch 740, val loss: 1.2080817222595215
Epoch 750, training loss: 628.8276977539062 = 0.7616822123527527 + 100.0 * 6.280660629272461
Epoch 750, val loss: 1.2076483964920044
Epoch 760, training loss: 628.43896484375 = 0.7494494915008545 + 100.0 * 6.276895046234131
Epoch 760, val loss: 1.207266926765442
Epoch 770, training loss: 628.2180786132812 = 0.7375613451004028 + 100.0 * 6.274805068969727
Epoch 770, val loss: 1.2074158191680908
Epoch 780, training loss: 628.482421875 = 0.7258054614067078 + 100.0 * 6.277565956115723
Epoch 780, val loss: 1.2076605558395386
Epoch 790, training loss: 628.0338745117188 = 0.7141733169555664 + 100.0 * 6.273197174072266
Epoch 790, val loss: 1.2081654071807861
Epoch 800, training loss: 628.0330810546875 = 0.702731192111969 + 100.0 * 6.273303031921387
Epoch 800, val loss: 1.2087624073028564
Epoch 810, training loss: 627.8814086914062 = 0.6914834380149841 + 100.0 * 6.271899700164795
Epoch 810, val loss: 1.2097235918045044
Epoch 820, training loss: 627.693115234375 = 0.680355966091156 + 100.0 * 6.270127773284912
Epoch 820, val loss: 1.210755467414856
Epoch 830, training loss: 627.5781860351562 = 0.6694976091384888 + 100.0 * 6.269086837768555
Epoch 830, val loss: 1.212156891822815
Epoch 840, training loss: 627.8881225585938 = 0.6587647199630737 + 100.0 * 6.272293567657471
Epoch 840, val loss: 1.213610291481018
Epoch 850, training loss: 627.5521240234375 = 0.6480581164360046 + 100.0 * 6.269040584564209
Epoch 850, val loss: 1.2150288820266724
Epoch 860, training loss: 627.7293090820312 = 0.637477457523346 + 100.0 * 6.270918846130371
Epoch 860, val loss: 1.2165793180465698
Epoch 870, training loss: 627.226318359375 = 0.6270894408226013 + 100.0 * 6.265992641448975
Epoch 870, val loss: 1.2184913158416748
Epoch 880, training loss: 627.0206909179688 = 0.6169028878211975 + 100.0 * 6.2640380859375
Epoch 880, val loss: 1.2206379175186157
Epoch 890, training loss: 627.1900634765625 = 0.6068797707557678 + 100.0 * 6.26583194732666
Epoch 890, val loss: 1.2228491306304932
Epoch 900, training loss: 627.3386840820312 = 0.5968577265739441 + 100.0 * 6.267417907714844
Epoch 900, val loss: 1.2249293327331543
Epoch 910, training loss: 626.8944091796875 = 0.5869479179382324 + 100.0 * 6.2630743980407715
Epoch 910, val loss: 1.2273327112197876
Epoch 920, training loss: 626.7409057617188 = 0.5772368907928467 + 100.0 * 6.261637210845947
Epoch 920, val loss: 1.2297016382217407
Epoch 930, training loss: 627.298583984375 = 0.567654550075531 + 100.0 * 6.267309665679932
Epoch 930, val loss: 1.2321447134017944
Epoch 940, training loss: 626.78759765625 = 0.558089554309845 + 100.0 * 6.262294769287109
Epoch 940, val loss: 1.2347733974456787
Epoch 950, training loss: 626.5421142578125 = 0.5486194491386414 + 100.0 * 6.259934902191162
Epoch 950, val loss: 1.2373560667037964
Epoch 960, training loss: 626.3964233398438 = 0.5394064784049988 + 100.0 * 6.258570194244385
Epoch 960, val loss: 1.2402660846710205
Epoch 970, training loss: 627.0554809570312 = 0.5302571058273315 + 100.0 * 6.265252113342285
Epoch 970, val loss: 1.2429672479629517
Epoch 980, training loss: 626.4290161132812 = 0.5210680961608887 + 100.0 * 6.259079456329346
Epoch 980, val loss: 1.2457034587860107
Epoch 990, training loss: 626.1392822265625 = 0.5120728611946106 + 100.0 * 6.256271839141846
Epoch 990, val loss: 1.2487266063690186
Epoch 1000, training loss: 626.39599609375 = 0.5032401084899902 + 100.0 * 6.258927345275879
Epoch 1000, val loss: 1.2518521547317505
Epoch 1010, training loss: 626.2498779296875 = 0.4944068193435669 + 100.0 * 6.25755500793457
Epoch 1010, val loss: 1.25482976436615
Epoch 1020, training loss: 626.0213623046875 = 0.48566576838493347 + 100.0 * 6.255357265472412
Epoch 1020, val loss: 1.258049726486206
Epoch 1030, training loss: 626.26025390625 = 0.47706544399261475 + 100.0 * 6.257832050323486
Epoch 1030, val loss: 1.2611472606658936
Epoch 1040, training loss: 625.8071899414062 = 0.46849003434181213 + 100.0 * 6.253386974334717
Epoch 1040, val loss: 1.2643800973892212
Epoch 1050, training loss: 625.7230834960938 = 0.4601052403450012 + 100.0 * 6.25262975692749
Epoch 1050, val loss: 1.2677143812179565
Epoch 1060, training loss: 625.7191162109375 = 0.451824814081192 + 100.0 * 6.2526726722717285
Epoch 1060, val loss: 1.2712770700454712
Epoch 1070, training loss: 626.0299682617188 = 0.44359466433525085 + 100.0 * 6.255863666534424
Epoch 1070, val loss: 1.2744810581207275
Epoch 1080, training loss: 625.5740356445312 = 0.4353994131088257 + 100.0 * 6.2513861656188965
Epoch 1080, val loss: 1.2779593467712402
Epoch 1090, training loss: 625.5987548828125 = 0.42736634612083435 + 100.0 * 6.251713752746582
Epoch 1090, val loss: 1.2815488576889038
Epoch 1100, training loss: 625.5535888671875 = 0.4194093346595764 + 100.0 * 6.251341342926025
Epoch 1100, val loss: 1.2849522829055786
Epoch 1110, training loss: 625.2970581054688 = 0.41159677505493164 + 100.0 * 6.248854637145996
Epoch 1110, val loss: 1.2886930704116821
Epoch 1120, training loss: 625.3765869140625 = 0.40389686822891235 + 100.0 * 6.249727249145508
Epoch 1120, val loss: 1.2921007871627808
Epoch 1130, training loss: 625.3688354492188 = 0.39621660113334656 + 100.0 * 6.249725818634033
Epoch 1130, val loss: 1.2954829931259155
Epoch 1140, training loss: 625.0836791992188 = 0.3885827660560608 + 100.0 * 6.246950626373291
Epoch 1140, val loss: 1.2991726398468018
Epoch 1150, training loss: 625.3335571289062 = 0.38117608428001404 + 100.0 * 6.249523639678955
Epoch 1150, val loss: 1.302756905555725
Epoch 1160, training loss: 625.0848388671875 = 0.3737364411354065 + 100.0 * 6.2471113204956055
Epoch 1160, val loss: 1.3062996864318848
Epoch 1170, training loss: 624.942138671875 = 0.366497665643692 + 100.0 * 6.245756149291992
Epoch 1170, val loss: 1.3101414442062378
Epoch 1180, training loss: 624.8822631835938 = 0.3593900203704834 + 100.0 * 6.2452287673950195
Epoch 1180, val loss: 1.3138833045959473
Epoch 1190, training loss: 625.0670776367188 = 0.3524336814880371 + 100.0 * 6.2471466064453125
Epoch 1190, val loss: 1.3177759647369385
Epoch 1200, training loss: 624.8560180664062 = 0.3454294502735138 + 100.0 * 6.245105743408203
Epoch 1200, val loss: 1.321420431137085
Epoch 1210, training loss: 625.025390625 = 0.33855026960372925 + 100.0 * 6.246868133544922
Epoch 1210, val loss: 1.3250828981399536
Epoch 1220, training loss: 624.8328857421875 = 0.33183711767196655 + 100.0 * 6.2450103759765625
Epoch 1220, val loss: 1.3290165662765503
Epoch 1230, training loss: 624.6138305664062 = 0.3252333998680115 + 100.0 * 6.242885589599609
Epoch 1230, val loss: 1.3327745199203491
Epoch 1240, training loss: 624.5626220703125 = 0.31877821683883667 + 100.0 * 6.242438316345215
Epoch 1240, val loss: 1.3367048501968384
Epoch 1250, training loss: 625.3819580078125 = 0.3124176561832428 + 100.0 * 6.25069522857666
Epoch 1250, val loss: 1.3404664993286133
Epoch 1260, training loss: 624.864990234375 = 0.3060848116874695 + 100.0 * 6.245588779449463
Epoch 1260, val loss: 1.3444442749023438
Epoch 1270, training loss: 624.5048828125 = 0.2998693585395813 + 100.0 * 6.2420501708984375
Epoch 1270, val loss: 1.3481495380401611
Epoch 1280, training loss: 624.714111328125 = 0.29384952783584595 + 100.0 * 6.244203090667725
Epoch 1280, val loss: 1.3522629737854004
Epoch 1290, training loss: 624.6228637695312 = 0.28783294558525085 + 100.0 * 6.243350028991699
Epoch 1290, val loss: 1.3562556505203247
Epoch 1300, training loss: 624.3274536132812 = 0.2819225490093231 + 100.0 * 6.240455150604248
Epoch 1300, val loss: 1.3599319458007812
Epoch 1310, training loss: 624.191162109375 = 0.2762013077735901 + 100.0 * 6.239149570465088
Epoch 1310, val loss: 1.3643161058425903
Epoch 1320, training loss: 624.3286743164062 = 0.2706281840801239 + 100.0 * 6.2405805587768555
Epoch 1320, val loss: 1.3685905933380127
Epoch 1330, training loss: 624.51123046875 = 0.2651015520095825 + 100.0 * 6.242461681365967
Epoch 1330, val loss: 1.3726634979248047
Epoch 1340, training loss: 624.3297119140625 = 0.25963571667671204 + 100.0 * 6.240700721740723
Epoch 1340, val loss: 1.3767930269241333
Epoch 1350, training loss: 624.1229248046875 = 0.25426995754241943 + 100.0 * 6.238686561584473
Epoch 1350, val loss: 1.3807746171951294
Epoch 1360, training loss: 623.9708862304688 = 0.24908071756362915 + 100.0 * 6.237217903137207
Epoch 1360, val loss: 1.3852028846740723
Epoch 1370, training loss: 624.0479125976562 = 0.2440050095319748 + 100.0 * 6.238039016723633
Epoch 1370, val loss: 1.3894315958023071
Epoch 1380, training loss: 624.3612670898438 = 0.23899328708648682 + 100.0 * 6.241222381591797
Epoch 1380, val loss: 1.3936690092086792
Epoch 1390, training loss: 624.2872924804688 = 0.23405252397060394 + 100.0 * 6.240531921386719
Epoch 1390, val loss: 1.3980259895324707
Epoch 1400, training loss: 623.961181640625 = 0.22922462224960327 + 100.0 * 6.237319469451904
Epoch 1400, val loss: 1.402238368988037
Epoch 1410, training loss: 623.86767578125 = 0.22451189160346985 + 100.0 * 6.236432075500488
Epoch 1410, val loss: 1.407090425491333
Epoch 1420, training loss: 623.8681030273438 = 0.21992447972297668 + 100.0 * 6.2364821434021
Epoch 1420, val loss: 1.4115568399429321
Epoch 1430, training loss: 624.0158081054688 = 0.21538498997688293 + 100.0 * 6.238004207611084
Epoch 1430, val loss: 1.4159506559371948
Epoch 1440, training loss: 624.2549438476562 = 0.2109338343143463 + 100.0 * 6.2404398918151855
Epoch 1440, val loss: 1.420383334159851
Epoch 1450, training loss: 623.7156982421875 = 0.20651870965957642 + 100.0 * 6.235091686248779
Epoch 1450, val loss: 1.4245282411575317
Epoch 1460, training loss: 623.5550537109375 = 0.2022760957479477 + 100.0 * 6.233528137207031
Epoch 1460, val loss: 1.4293627738952637
Epoch 1470, training loss: 623.6365356445312 = 0.19817370176315308 + 100.0 * 6.234383583068848
Epoch 1470, val loss: 1.4342976808547974
Epoch 1480, training loss: 623.783447265625 = 0.19408482313156128 + 100.0 * 6.235893726348877
Epoch 1480, val loss: 1.4387980699539185
Epoch 1490, training loss: 623.6058959960938 = 0.19008758664131165 + 100.0 * 6.234158515930176
Epoch 1490, val loss: 1.4437414407730103
Epoch 1500, training loss: 624.1111450195312 = 0.18618816137313843 + 100.0 * 6.239249229431152
Epoch 1500, val loss: 1.4480570554733276
Epoch 1510, training loss: 623.4270629882812 = 0.1823401153087616 + 100.0 * 6.232447624206543
Epoch 1510, val loss: 1.4532980918884277
Epoch 1520, training loss: 623.2978515625 = 0.17859268188476562 + 100.0 * 6.231192588806152
Epoch 1520, val loss: 1.4580141305923462
Epoch 1530, training loss: 623.416748046875 = 0.1749611794948578 + 100.0 * 6.232417583465576
Epoch 1530, val loss: 1.4629309177398682
Epoch 1540, training loss: 623.624267578125 = 0.17137403786182404 + 100.0 * 6.2345290184021
Epoch 1540, val loss: 1.4677681922912598
Epoch 1550, training loss: 623.3761596679688 = 0.1678396463394165 + 100.0 * 6.232082843780518
Epoch 1550, val loss: 1.472787857055664
Epoch 1560, training loss: 623.3765869140625 = 0.16439107060432434 + 100.0 * 6.23212194442749
Epoch 1560, val loss: 1.4772802591323853
Epoch 1570, training loss: 623.1746826171875 = 0.1610344648361206 + 100.0 * 6.230136871337891
Epoch 1570, val loss: 1.4825406074523926
Epoch 1580, training loss: 623.2711181640625 = 0.1577616184949875 + 100.0 * 6.231133460998535
Epoch 1580, val loss: 1.4878987073898315
Epoch 1590, training loss: 623.3693237304688 = 0.1545412242412567 + 100.0 * 6.232147693634033
Epoch 1590, val loss: 1.4925512075424194
Epoch 1600, training loss: 623.2676391601562 = 0.15137527883052826 + 100.0 * 6.2311625480651855
Epoch 1600, val loss: 1.49787437915802
Epoch 1610, training loss: 623.9615478515625 = 0.1483028680086136 + 100.0 * 6.238132476806641
Epoch 1610, val loss: 1.5031988620758057
Epoch 1620, training loss: 623.2399291992188 = 0.14522971212863922 + 100.0 * 6.230947017669678
Epoch 1620, val loss: 1.5076004266738892
Epoch 1630, training loss: 623.0083618164062 = 0.14225992560386658 + 100.0 * 6.228661060333252
Epoch 1630, val loss: 1.5132653713226318
Epoch 1640, training loss: 622.887939453125 = 0.1394033432006836 + 100.0 * 6.227485656738281
Epoch 1640, val loss: 1.5184926986694336
Epoch 1650, training loss: 622.8615112304688 = 0.13661660254001617 + 100.0 * 6.2272491455078125
Epoch 1650, val loss: 1.523798942565918
Epoch 1660, training loss: 624.1919555664062 = 0.13387279212474823 + 100.0 * 6.240581035614014
Epoch 1660, val loss: 1.5285162925720215
Epoch 1670, training loss: 623.3043212890625 = 0.13109080493450165 + 100.0 * 6.231732368469238
Epoch 1670, val loss: 1.5339516401290894
Epoch 1680, training loss: 622.8152465820312 = 0.12839165329933167 + 100.0 * 6.226868152618408
Epoch 1680, val loss: 1.5392398834228516
Epoch 1690, training loss: 622.7359619140625 = 0.1258382946252823 + 100.0 * 6.226100921630859
Epoch 1690, val loss: 1.5446282625198364
Epoch 1700, training loss: 623.5918579101562 = 0.12335611134767532 + 100.0 * 6.234684944152832
Epoch 1700, val loss: 1.5503796339035034
Epoch 1710, training loss: 623.144287109375 = 0.12077967822551727 + 100.0 * 6.2302350997924805
Epoch 1710, val loss: 1.5543246269226074
Epoch 1720, training loss: 622.6581420898438 = 0.11834206432104111 + 100.0 * 6.225398063659668
Epoch 1720, val loss: 1.5597273111343384
Epoch 1730, training loss: 622.6195068359375 = 0.11600254476070404 + 100.0 * 6.225034713745117
Epoch 1730, val loss: 1.5655401945114136
Epoch 1740, training loss: 622.59619140625 = 0.11371419578790665 + 100.0 * 6.224824905395508
Epoch 1740, val loss: 1.5705748796463013
Epoch 1750, training loss: 623.1505737304688 = 0.11149383336305618 + 100.0 * 6.230390548706055
Epoch 1750, val loss: 1.5758601427078247
Epoch 1760, training loss: 622.8365478515625 = 0.10924994945526123 + 100.0 * 6.227272987365723
Epoch 1760, val loss: 1.58133864402771
Epoch 1770, training loss: 622.5884399414062 = 0.10702721029520035 + 100.0 * 6.224813938140869
Epoch 1770, val loss: 1.5861537456512451
Epoch 1780, training loss: 622.5098876953125 = 0.10494210571050644 + 100.0 * 6.2240495681762695
Epoch 1780, val loss: 1.591880440711975
Epoch 1790, training loss: 623.0454711914062 = 0.1029171347618103 + 100.0 * 6.22942590713501
Epoch 1790, val loss: 1.5974711179733276
Epoch 1800, training loss: 622.5361938476562 = 0.10083604604005814 + 100.0 * 6.224353790283203
Epoch 1800, val loss: 1.6019803285598755
Epoch 1810, training loss: 622.3866577148438 = 0.09882861375808716 + 100.0 * 6.222878456115723
Epoch 1810, val loss: 1.6074022054672241
Epoch 1820, training loss: 622.3453979492188 = 0.09692669659852982 + 100.0 * 6.222484588623047
Epoch 1820, val loss: 1.6128361225128174
Epoch 1830, training loss: 623.3555297851562 = 0.09506220370531082 + 100.0 * 6.23260498046875
Epoch 1830, val loss: 1.6184574365615845
Epoch 1840, training loss: 622.6558837890625 = 0.09318044036626816 + 100.0 * 6.2256269454956055
Epoch 1840, val loss: 1.6227060556411743
Epoch 1850, training loss: 622.3807373046875 = 0.09133334457874298 + 100.0 * 6.222894191741943
Epoch 1850, val loss: 1.6286035776138306
Epoch 1860, training loss: 622.2542114257812 = 0.08959932625293732 + 100.0 * 6.221646308898926
Epoch 1860, val loss: 1.6341089010238647
Epoch 1870, training loss: 622.6123046875 = 0.08791285753250122 + 100.0 * 6.225244045257568
Epoch 1870, val loss: 1.6395207643508911
Epoch 1880, training loss: 622.3077392578125 = 0.08618920296430588 + 100.0 * 6.22221565246582
Epoch 1880, val loss: 1.644311547279358
Epoch 1890, training loss: 622.2130737304688 = 0.08448731154203415 + 100.0 * 6.221285820007324
Epoch 1890, val loss: 1.6496789455413818
Epoch 1900, training loss: 622.1337890625 = 0.08288707584142685 + 100.0 * 6.220509052276611
Epoch 1900, val loss: 1.654941201210022
Epoch 1910, training loss: 622.289306640625 = 0.08134716749191284 + 100.0 * 6.222079277038574
Epoch 1910, val loss: 1.6607654094696045
Epoch 1920, training loss: 622.3458251953125 = 0.0797906443476677 + 100.0 * 6.222660541534424
Epoch 1920, val loss: 1.6657058000564575
Epoch 1930, training loss: 622.1292114257812 = 0.0782647579908371 + 100.0 * 6.2205095291137695
Epoch 1930, val loss: 1.6709730625152588
Epoch 1940, training loss: 622.1015014648438 = 0.07680873572826385 + 100.0 * 6.220247268676758
Epoch 1940, val loss: 1.6765289306640625
Epoch 1950, training loss: 622.6179809570312 = 0.07539563626050949 + 100.0 * 6.225426197052002
Epoch 1950, val loss: 1.6816045045852661
Epoch 1960, training loss: 622.4370727539062 = 0.07394398748874664 + 100.0 * 6.223631381988525
Epoch 1960, val loss: 1.687011957168579
Epoch 1970, training loss: 622.1105346679688 = 0.07255539298057556 + 100.0 * 6.220379829406738
Epoch 1970, val loss: 1.6919194459915161
Epoch 1980, training loss: 622.2279663085938 = 0.07121939957141876 + 100.0 * 6.221567630767822
Epoch 1980, val loss: 1.6971063613891602
Epoch 1990, training loss: 622.0116577148438 = 0.06990304589271545 + 100.0 * 6.219417572021484
Epoch 1990, val loss: 1.7025293111801147
Epoch 2000, training loss: 622.0829467773438 = 0.06861894577741623 + 100.0 * 6.2201433181762695
Epoch 2000, val loss: 1.7079167366027832
Epoch 2010, training loss: 622.176025390625 = 0.06737319380044937 + 100.0 * 6.221086502075195
Epoch 2010, val loss: 1.7130845785140991
Epoch 2020, training loss: 622.1906127929688 = 0.06611895561218262 + 100.0 * 6.221244812011719
Epoch 2020, val loss: 1.717621088027954
Epoch 2030, training loss: 622.3734130859375 = 0.06488043069839478 + 100.0 * 6.223085403442383
Epoch 2030, val loss: 1.7229472398757935
Epoch 2040, training loss: 621.93310546875 = 0.06369327753782272 + 100.0 * 6.218693733215332
Epoch 2040, val loss: 1.7285364866256714
Epoch 2050, training loss: 621.8020629882812 = 0.0625385269522667 + 100.0 * 6.217395305633545
Epoch 2050, val loss: 1.7335150241851807
Epoch 2060, training loss: 621.7396850585938 = 0.061446670442819595 + 100.0 * 6.216782093048096
Epoch 2060, val loss: 1.7391130924224854
Epoch 2070, training loss: 621.8583984375 = 0.06037097051739693 + 100.0 * 6.21798038482666
Epoch 2070, val loss: 1.7441074848175049
Epoch 2080, training loss: 622.3547973632812 = 0.05928679183125496 + 100.0 * 6.222955226898193
Epoch 2080, val loss: 1.7486757040023804
Epoch 2090, training loss: 622.177001953125 = 0.05821162089705467 + 100.0 * 6.221187591552734
Epoch 2090, val loss: 1.754272222518921
Epoch 2100, training loss: 621.7698974609375 = 0.057156968861818314 + 100.0 * 6.217127799987793
Epoch 2100, val loss: 1.758946418762207
Epoch 2110, training loss: 622.0955200195312 = 0.05616307258605957 + 100.0 * 6.220393657684326
Epoch 2110, val loss: 1.76438307762146
Epoch 2120, training loss: 621.802734375 = 0.055168382823467255 + 100.0 * 6.217475414276123
Epoch 2120, val loss: 1.7690423727035522
Epoch 2130, training loss: 621.6702880859375 = 0.0542140007019043 + 100.0 * 6.216160774230957
Epoch 2130, val loss: 1.7743245363235474
Epoch 2140, training loss: 621.7725219726562 = 0.053286779671907425 + 100.0 * 6.21719217300415
Epoch 2140, val loss: 1.7795050144195557
Epoch 2150, training loss: 621.8333129882812 = 0.052373651415109634 + 100.0 * 6.217809200286865
Epoch 2150, val loss: 1.7842497825622559
Epoch 2160, training loss: 621.5938720703125 = 0.051462553441524506 + 100.0 * 6.215424060821533
Epoch 2160, val loss: 1.7892813682556152
Epoch 2170, training loss: 621.8482666015625 = 0.05058697983622551 + 100.0 * 6.2179765701293945
Epoch 2170, val loss: 1.7938381433486938
Epoch 2180, training loss: 621.59619140625 = 0.049727149307727814 + 100.0 * 6.2154645919799805
Epoch 2180, val loss: 1.799211859703064
Epoch 2190, training loss: 622.13916015625 = 0.04889388009905815 + 100.0 * 6.220902919769287
Epoch 2190, val loss: 1.8033148050308228
Epoch 2200, training loss: 621.734130859375 = 0.048029497265815735 + 100.0 * 6.216860771179199
Epoch 2200, val loss: 1.8088219165802002
Epoch 2210, training loss: 621.6112060546875 = 0.047202564775943756 + 100.0 * 6.215640068054199
Epoch 2210, val loss: 1.8130756616592407
Epoch 2220, training loss: 621.4932250976562 = 0.046426575630903244 + 100.0 * 6.214468002319336
Epoch 2220, val loss: 1.818763017654419
Epoch 2230, training loss: 621.6314086914062 = 0.04567386955022812 + 100.0 * 6.21585750579834
Epoch 2230, val loss: 1.8235232830047607
Epoch 2240, training loss: 621.7152099609375 = 0.04492023214697838 + 100.0 * 6.216702938079834
Epoch 2240, val loss: 1.8282897472381592
Epoch 2250, training loss: 621.8092651367188 = 0.04417204484343529 + 100.0 * 6.217650890350342
Epoch 2250, val loss: 1.8323968648910522
Epoch 2260, training loss: 621.4043579101562 = 0.04343724623322487 + 100.0 * 6.213608741760254
Epoch 2260, val loss: 1.8372507095336914
Epoch 2270, training loss: 621.3857421875 = 0.0427461676299572 + 100.0 * 6.213429927825928
Epoch 2270, val loss: 1.8419249057769775
Epoch 2280, training loss: 621.3948974609375 = 0.04207181930541992 + 100.0 * 6.213528156280518
Epoch 2280, val loss: 1.8468925952911377
Epoch 2290, training loss: 621.573486328125 = 0.04140172153711319 + 100.0 * 6.215321063995361
Epoch 2290, val loss: 1.8512637615203857
Epoch 2300, training loss: 621.6900634765625 = 0.04073558747768402 + 100.0 * 6.216493606567383
Epoch 2300, val loss: 1.8556630611419678
Epoch 2310, training loss: 621.4501342773438 = 0.04006437212228775 + 100.0 * 6.2141008377075195
Epoch 2310, val loss: 1.8607447147369385
Epoch 2320, training loss: 621.2891235351562 = 0.03941849246621132 + 100.0 * 6.212496757507324
Epoch 2320, val loss: 1.86505925655365
Epoch 2330, training loss: 621.2019653320312 = 0.03880083188414574 + 100.0 * 6.2116312980651855
Epoch 2330, val loss: 1.8696130514144897
Epoch 2340, training loss: 621.1756591796875 = 0.0382126048207283 + 100.0 * 6.211374282836914
Epoch 2340, val loss: 1.8746312856674194
Epoch 2350, training loss: 621.7638549804688 = 0.03764102980494499 + 100.0 * 6.217261791229248
Epoch 2350, val loss: 1.879667043685913
Epoch 2360, training loss: 621.32177734375 = 0.0370340421795845 + 100.0 * 6.2128472328186035
Epoch 2360, val loss: 1.8829526901245117
Epoch 2370, training loss: 621.376708984375 = 0.036464959383010864 + 100.0 * 6.21340274810791
Epoch 2370, val loss: 1.8883610963821411
Epoch 2380, training loss: 621.302734375 = 0.035895220935344696 + 100.0 * 6.212668418884277
Epoch 2380, val loss: 1.8921161890029907
Epoch 2390, training loss: 621.17724609375 = 0.03536238521337509 + 100.0 * 6.211419105529785
Epoch 2390, val loss: 1.897093415260315
Epoch 2400, training loss: 621.2904663085938 = 0.03483986482024193 + 100.0 * 6.212555885314941
Epoch 2400, val loss: 1.9012534618377686
Epoch 2410, training loss: 621.3719482421875 = 0.034315936267375946 + 100.0 * 6.213376522064209
Epoch 2410, val loss: 1.90589439868927
Epoch 2420, training loss: 621.4332275390625 = 0.03379041329026222 + 100.0 * 6.213994026184082
Epoch 2420, val loss: 1.9101088047027588
Epoch 2430, training loss: 621.2063598632812 = 0.03328438475728035 + 100.0 * 6.21173095703125
Epoch 2430, val loss: 1.9145749807357788
Epoch 2440, training loss: 621.1187133789062 = 0.032789986580610275 + 100.0 * 6.210859298706055
Epoch 2440, val loss: 1.9190396070480347
Epoch 2450, training loss: 621.14697265625 = 0.03232376649975777 + 100.0 * 6.211146354675293
Epoch 2450, val loss: 1.923613429069519
Epoch 2460, training loss: 621.1781616210938 = 0.03185601532459259 + 100.0 * 6.21146297454834
Epoch 2460, val loss: 1.9278700351715088
Epoch 2470, training loss: 621.4901733398438 = 0.03140468895435333 + 100.0 * 6.214587688446045
Epoch 2470, val loss: 1.9322729110717773
Epoch 2480, training loss: 621.5816650390625 = 0.030923301354050636 + 100.0 * 6.2155070304870605
Epoch 2480, val loss: 1.9357267618179321
Epoch 2490, training loss: 621.0113525390625 = 0.030479593202471733 + 100.0 * 6.209808826446533
Epoch 2490, val loss: 1.9401848316192627
Epoch 2500, training loss: 620.91259765625 = 0.0300364401191473 + 100.0 * 6.208825588226318
Epoch 2500, val loss: 1.9443848133087158
Epoch 2510, training loss: 620.883056640625 = 0.029626676812767982 + 100.0 * 6.2085347175598145
Epoch 2510, val loss: 1.948771357536316
Epoch 2520, training loss: 620.9119262695312 = 0.029225613921880722 + 100.0 * 6.208827018737793
Epoch 2520, val loss: 1.9532899856567383
Epoch 2530, training loss: 621.7554321289062 = 0.028828401118516922 + 100.0 * 6.217266082763672
Epoch 2530, val loss: 1.9572618007659912
Epoch 2540, training loss: 621.4309692382812 = 0.02842218428850174 + 100.0 * 6.214025020599365
Epoch 2540, val loss: 1.9617576599121094
Epoch 2550, training loss: 621.0103149414062 = 0.028005406260490417 + 100.0 * 6.209823131561279
Epoch 2550, val loss: 1.9651676416397095
Epoch 2560, training loss: 620.9035034179688 = 0.027625521644949913 + 100.0 * 6.208758354187012
Epoch 2560, val loss: 1.9692251682281494
Epoch 2570, training loss: 621.1312866210938 = 0.027258362621068954 + 100.0 * 6.211040019989014
Epoch 2570, val loss: 1.9737604856491089
Epoch 2580, training loss: 620.9205932617188 = 0.026886826381087303 + 100.0 * 6.20893669128418
Epoch 2580, val loss: 1.977728009223938
Epoch 2590, training loss: 620.87646484375 = 0.026525819674134254 + 100.0 * 6.208499431610107
Epoch 2590, val loss: 1.9816478490829468
Epoch 2600, training loss: 620.741455078125 = 0.026177190244197845 + 100.0 * 6.207152843475342
Epoch 2600, val loss: 1.9858804941177368
Epoch 2610, training loss: 621.0007934570312 = 0.025848165154457092 + 100.0 * 6.209749698638916
Epoch 2610, val loss: 1.990046501159668
Epoch 2620, training loss: 621.1961059570312 = 0.025494249537587166 + 100.0 * 6.211706161499023
Epoch 2620, val loss: 1.9932212829589844
Epoch 2630, training loss: 620.7734375 = 0.02513217367231846 + 100.0 * 6.207482814788818
Epoch 2630, val loss: 1.9969046115875244
Epoch 2640, training loss: 620.6975708007812 = 0.024808378890156746 + 100.0 * 6.206727981567383
Epoch 2640, val loss: 2.0009989738464355
Epoch 2650, training loss: 620.8407592773438 = 0.024502957239747047 + 100.0 * 6.208162784576416
Epoch 2650, val loss: 2.0050621032714844
Epoch 2660, training loss: 621.0127563476562 = 0.02419433742761612 + 100.0 * 6.209885597229004
Epoch 2660, val loss: 2.008779287338257
Epoch 2670, training loss: 620.8292846679688 = 0.02387356199324131 + 100.0 * 6.208054065704346
Epoch 2670, val loss: 2.0127804279327393
Epoch 2680, training loss: 620.7959594726562 = 0.02357165329158306 + 100.0 * 6.207724094390869
Epoch 2680, val loss: 2.016727924346924
Epoch 2690, training loss: 621.1594848632812 = 0.02328690141439438 + 100.0 * 6.211361408233643
Epoch 2690, val loss: 2.02099871635437
Epoch 2700, training loss: 620.9453125 = 0.022978730499744415 + 100.0 * 6.209223747253418
Epoch 2700, val loss: 2.024451494216919
Epoch 2710, training loss: 620.658447265625 = 0.022683823481202126 + 100.0 * 6.206357479095459
Epoch 2710, val loss: 2.027782917022705
Epoch 2720, training loss: 620.9110107421875 = 0.022405683994293213 + 100.0 * 6.20888614654541
Epoch 2720, val loss: 2.031228542327881
Epoch 2730, training loss: 620.8262329101562 = 0.022128809243440628 + 100.0 * 6.208040714263916
Epoch 2730, val loss: 2.036038398742676
Epoch 2740, training loss: 620.60205078125 = 0.021857108920812607 + 100.0 * 6.205801963806152
Epoch 2740, val loss: 2.039504289627075
Epoch 2750, training loss: 620.6483154296875 = 0.021601974964141846 + 100.0 * 6.206267356872559
Epoch 2750, val loss: 2.0431463718414307
Epoch 2760, training loss: 620.8553466796875 = 0.02134527824819088 + 100.0 * 6.208339691162109
Epoch 2760, val loss: 2.0467376708984375
Epoch 2770, training loss: 620.6380004882812 = 0.021076956763863564 + 100.0 * 6.206169128417969
Epoch 2770, val loss: 2.05076003074646
Epoch 2780, training loss: 620.7482299804688 = 0.020826930180191994 + 100.0 * 6.207273960113525
Epoch 2780, val loss: 2.0542564392089844
Epoch 2790, training loss: 620.8053588867188 = 0.02058204635977745 + 100.0 * 6.207848072052002
Epoch 2790, val loss: 2.057668924331665
Epoch 2800, training loss: 621.1302490234375 = 0.0203289482742548 + 100.0 * 6.211099147796631
Epoch 2800, val loss: 2.0613956451416016
Epoch 2810, training loss: 620.6053466796875 = 0.02008775807917118 + 100.0 * 6.205852508544922
Epoch 2810, val loss: 2.0649709701538086
Epoch 2820, training loss: 620.4572143554688 = 0.019850311800837517 + 100.0 * 6.204373836517334
Epoch 2820, val loss: 2.0690884590148926
Epoch 2830, training loss: 620.5248413085938 = 0.019630191847682 + 100.0 * 6.205051898956299
Epoch 2830, val loss: 2.072932481765747
Epoch 2840, training loss: 620.9531860351562 = 0.019415900111198425 + 100.0 * 6.2093377113342285
Epoch 2840, val loss: 2.0764617919921875
Epoch 2850, training loss: 620.6739501953125 = 0.019175954163074493 + 100.0 * 6.206547737121582
Epoch 2850, val loss: 2.079005479812622
Epoch 2860, training loss: 620.44189453125 = 0.018948916345834732 + 100.0 * 6.204229831695557
Epoch 2860, val loss: 2.0831546783447266
Epoch 2870, training loss: 620.5870971679688 = 0.018734080716967583 + 100.0 * 6.205683708190918
Epoch 2870, val loss: 2.0865936279296875
Epoch 2880, training loss: 620.96923828125 = 0.018527017906308174 + 100.0 * 6.209506988525391
Epoch 2880, val loss: 2.090081214904785
Epoch 2890, training loss: 620.57470703125 = 0.018314506858587265 + 100.0 * 6.205564022064209
Epoch 2890, val loss: 2.0939090251922607
Epoch 2900, training loss: 620.4451904296875 = 0.018102794885635376 + 100.0 * 6.204270362854004
Epoch 2900, val loss: 2.097078323364258
Epoch 2910, training loss: 620.99169921875 = 0.017917120829224586 + 100.0 * 6.209737300872803
Epoch 2910, val loss: 2.100153923034668
Epoch 2920, training loss: 620.4072265625 = 0.017703592777252197 + 100.0 * 6.203895092010498
Epoch 2920, val loss: 2.104114055633545
Epoch 2930, training loss: 620.2847290039062 = 0.017507530748844147 + 100.0 * 6.202672004699707
Epoch 2930, val loss: 2.1075356006622314
Epoch 2940, training loss: 620.2908935546875 = 0.017322657629847527 + 100.0 * 6.202735424041748
Epoch 2940, val loss: 2.1106786727905273
Epoch 2950, training loss: 620.3352661132812 = 0.017141476273536682 + 100.0 * 6.203181266784668
Epoch 2950, val loss: 2.114656686782837
Epoch 2960, training loss: 621.4242553710938 = 0.01696091704070568 + 100.0 * 6.2140727043151855
Epoch 2960, val loss: 2.1174259185791016
Epoch 2970, training loss: 620.6971435546875 = 0.016774112358689308 + 100.0 * 6.206803798675537
Epoch 2970, val loss: 2.1204566955566406
Epoch 2980, training loss: 620.5044555664062 = 0.01658262126147747 + 100.0 * 6.204878807067871
Epoch 2980, val loss: 2.1239073276519775
Epoch 2990, training loss: 620.4395141601562 = 0.016413288190960884 + 100.0 * 6.204231262207031
Epoch 2990, val loss: 2.12715744972229
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6555555555555556
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 861.6282348632812 = 1.9429426193237305 + 100.0 * 8.596853256225586
Epoch 0, val loss: 1.9354311227798462
Epoch 10, training loss: 861.5523071289062 = 1.9338401556015015 + 100.0 * 8.596184730529785
Epoch 10, val loss: 1.9260599613189697
Epoch 20, training loss: 861.0953979492188 = 1.9227606058120728 + 100.0 * 8.591726303100586
Epoch 20, val loss: 1.9147429466247559
Epoch 30, training loss: 858.24560546875 = 1.9085294008255005 + 100.0 * 8.563370704650879
Epoch 30, val loss: 1.9001215696334839
Epoch 40, training loss: 842.2805786132812 = 1.8910354375839233 + 100.0 * 8.403895378112793
Epoch 40, val loss: 1.8824294805526733
Epoch 50, training loss: 773.1712646484375 = 1.8714070320129395 + 100.0 * 7.712998867034912
Epoch 50, val loss: 1.8625233173370361
Epoch 60, training loss: 746.769775390625 = 1.8557088375091553 + 100.0 * 7.449140548706055
Epoch 60, val loss: 1.847865104675293
Epoch 70, training loss: 721.233642578125 = 1.842795729637146 + 100.0 * 7.19390869140625
Epoch 70, val loss: 1.8353444337844849
Epoch 80, training loss: 707.5846557617188 = 1.8299133777618408 + 100.0 * 7.057547569274902
Epoch 80, val loss: 1.8231791257858276
Epoch 90, training loss: 692.504150390625 = 1.819445252418518 + 100.0 * 6.90684700012207
Epoch 90, val loss: 1.8132121562957764
Epoch 100, training loss: 682.1866455078125 = 1.8114129304885864 + 100.0 * 6.803752422332764
Epoch 100, val loss: 1.8057098388671875
Epoch 110, training loss: 675.9840698242188 = 1.8025760650634766 + 100.0 * 6.741815090179443
Epoch 110, val loss: 1.7977269887924194
Epoch 120, training loss: 671.0770874023438 = 1.793427586555481 + 100.0 * 6.692836284637451
Epoch 120, val loss: 1.789605736732483
Epoch 130, training loss: 667.1349487304688 = 1.784580945968628 + 100.0 * 6.65350341796875
Epoch 130, val loss: 1.7816081047058105
Epoch 140, training loss: 663.9970092773438 = 1.7754303216934204 + 100.0 * 6.622215270996094
Epoch 140, val loss: 1.7731430530548096
Epoch 150, training loss: 660.9410400390625 = 1.7656980752944946 + 100.0 * 6.5917534828186035
Epoch 150, val loss: 1.763914942741394
Epoch 160, training loss: 658.301025390625 = 1.7553565502166748 + 100.0 * 6.565456390380859
Epoch 160, val loss: 1.7540277242660522
Epoch 170, training loss: 656.141357421875 = 1.7440768480300903 + 100.0 * 6.543972492218018
Epoch 170, val loss: 1.743378758430481
Epoch 180, training loss: 654.2459106445312 = 1.7317103147506714 + 100.0 * 6.525142192840576
Epoch 180, val loss: 1.7319436073303223
Epoch 190, training loss: 652.4560546875 = 1.7182732820510864 + 100.0 * 6.507378101348877
Epoch 190, val loss: 1.7194952964782715
Epoch 200, training loss: 651.1786499023438 = 1.7037614583969116 + 100.0 * 6.494749069213867
Epoch 200, val loss: 1.7061644792556763
Epoch 210, training loss: 649.7357788085938 = 1.6879734992980957 + 100.0 * 6.480477809906006
Epoch 210, val loss: 1.6917179822921753
Epoch 220, training loss: 648.3787231445312 = 1.6710437536239624 + 100.0 * 6.467077255249023
Epoch 220, val loss: 1.6763004064559937
Epoch 230, training loss: 647.633056640625 = 1.6529631614685059 + 100.0 * 6.459800720214844
Epoch 230, val loss: 1.6599714756011963
Epoch 240, training loss: 646.2918090820312 = 1.633719801902771 + 100.0 * 6.44658088684082
Epoch 240, val loss: 1.6426270008087158
Epoch 250, training loss: 645.27783203125 = 1.6135234832763672 + 100.0 * 6.436643123626709
Epoch 250, val loss: 1.6245498657226562
Epoch 260, training loss: 644.63623046875 = 1.5924506187438965 + 100.0 * 6.4304375648498535
Epoch 260, val loss: 1.6058390140533447
Epoch 270, training loss: 643.8425903320312 = 1.570475697517395 + 100.0 * 6.422720909118652
Epoch 270, val loss: 1.5865012407302856
Epoch 280, training loss: 642.949951171875 = 1.5480281114578247 + 100.0 * 6.4140191078186035
Epoch 280, val loss: 1.5669658184051514
Epoch 290, training loss: 642.3272094726562 = 1.525235652923584 + 100.0 * 6.40802001953125
Epoch 290, val loss: 1.5475269556045532
Epoch 300, training loss: 641.7344360351562 = 1.502271056175232 + 100.0 * 6.402321815490723
Epoch 300, val loss: 1.5282526016235352
Epoch 310, training loss: 642.0534057617188 = 1.4793000221252441 + 100.0 * 6.405740737915039
Epoch 310, val loss: 1.5091825723648071
Epoch 320, training loss: 640.8099975585938 = 1.4561771154403687 + 100.0 * 6.393538475036621
Epoch 320, val loss: 1.4903565645217896
Epoch 330, training loss: 640.1830444335938 = 1.4333231449127197 + 100.0 * 6.3874969482421875
Epoch 330, val loss: 1.4720728397369385
Epoch 340, training loss: 639.8507690429688 = 1.4107210636138916 + 100.0 * 6.384400367736816
Epoch 340, val loss: 1.454470157623291
Epoch 350, training loss: 639.5971069335938 = 1.388281226158142 + 100.0 * 6.3820881843566895
Epoch 350, val loss: 1.4370667934417725
Epoch 360, training loss: 638.951416015625 = 1.366051197052002 + 100.0 * 6.375854015350342
Epoch 360, val loss: 1.420274019241333
Epoch 370, training loss: 638.4517822265625 = 1.344064474105835 + 100.0 * 6.371077537536621
Epoch 370, val loss: 1.4040203094482422
Epoch 380, training loss: 638.759765625 = 1.3221927881240845 + 100.0 * 6.374375343322754
Epoch 380, val loss: 1.38835871219635
Epoch 390, training loss: 637.8076171875 = 1.3003208637237549 + 100.0 * 6.365073204040527
Epoch 390, val loss: 1.3726309537887573
Epoch 400, training loss: 637.36865234375 = 1.2784483432769775 + 100.0 * 6.360902309417725
Epoch 400, val loss: 1.3572076559066772
Epoch 410, training loss: 636.9500122070312 = 1.2566198110580444 + 100.0 * 6.356934070587158
Epoch 410, val loss: 1.342118740081787
Epoch 420, training loss: 636.7721557617188 = 1.2347843647003174 + 100.0 * 6.355373382568359
Epoch 420, val loss: 1.3272978067398071
Epoch 430, training loss: 636.3464965820312 = 1.2128856182098389 + 100.0 * 6.3513360023498535
Epoch 430, val loss: 1.312677264213562
Epoch 440, training loss: 636.0885620117188 = 1.1909053325653076 + 100.0 * 6.3489766120910645
Epoch 440, val loss: 1.298385500907898
Epoch 450, training loss: 635.5706787109375 = 1.1689178943634033 + 100.0 * 6.344017505645752
Epoch 450, val loss: 1.2844022512435913
Epoch 460, training loss: 635.7692260742188 = 1.1469534635543823 + 100.0 * 6.346222400665283
Epoch 460, val loss: 1.2709195613861084
Epoch 470, training loss: 634.9834594726562 = 1.1250039339065552 + 100.0 * 6.3385844230651855
Epoch 470, val loss: 1.2573522329330444
Epoch 480, training loss: 634.7334594726562 = 1.1032122373580933 + 100.0 * 6.336302280426025
Epoch 480, val loss: 1.244611144065857
Epoch 490, training loss: 634.5308837890625 = 1.0816528797149658 + 100.0 * 6.334492206573486
Epoch 490, val loss: 1.232430100440979
Epoch 500, training loss: 634.1395263671875 = 1.0601989030838013 + 100.0 * 6.330793380737305
Epoch 500, val loss: 1.2206511497497559
Epoch 510, training loss: 633.8805541992188 = 1.039156198501587 + 100.0 * 6.328413486480713
Epoch 510, val loss: 1.209613561630249
Epoch 520, training loss: 634.1798706054688 = 1.0183335542678833 + 100.0 * 6.331615447998047
Epoch 520, val loss: 1.199005365371704
Epoch 530, training loss: 633.4532470703125 = 0.9979596734046936 + 100.0 * 6.324552536010742
Epoch 530, val loss: 1.1892127990722656
Epoch 540, training loss: 633.1195068359375 = 0.9780181050300598 + 100.0 * 6.321414947509766
Epoch 540, val loss: 1.1801677942276
Epoch 550, training loss: 632.8576049804688 = 0.9586229920387268 + 100.0 * 6.3189897537231445
Epoch 550, val loss: 1.1718685626983643
Epoch 560, training loss: 633.4822998046875 = 0.9396841526031494 + 100.0 * 6.32542610168457
Epoch 560, val loss: 1.1639620065689087
Epoch 570, training loss: 632.5763549804688 = 0.9210315346717834 + 100.0 * 6.316553115844727
Epoch 570, val loss: 1.1571367979049683
Epoch 580, training loss: 632.3320922851562 = 0.9029411673545837 + 100.0 * 6.314291477203369
Epoch 580, val loss: 1.1509313583374023
Epoch 590, training loss: 632.3533935546875 = 0.8854185938835144 + 100.0 * 6.3146796226501465
Epoch 590, val loss: 1.1452274322509766
Epoch 600, training loss: 632.0052490234375 = 0.8682255148887634 + 100.0 * 6.311370372772217
Epoch 600, val loss: 1.1399877071380615
Epoch 610, training loss: 632.055419921875 = 0.8515652418136597 + 100.0 * 6.312038421630859
Epoch 610, val loss: 1.1355952024459839
Epoch 620, training loss: 631.6724243164062 = 0.8351472020149231 + 100.0 * 6.308372974395752
Epoch 620, val loss: 1.1317371129989624
Epoch 630, training loss: 631.4790649414062 = 0.8192198872566223 + 100.0 * 6.306598663330078
Epoch 630, val loss: 1.1280828714370728
Epoch 640, training loss: 631.091796875 = 0.80357825756073 + 100.0 * 6.302882194519043
Epoch 640, val loss: 1.124901533126831
Epoch 650, training loss: 631.1639404296875 = 0.7884039878845215 + 100.0 * 6.303755760192871
Epoch 650, val loss: 1.1223078966140747
Epoch 660, training loss: 630.992919921875 = 0.7734405398368835 + 100.0 * 6.302194595336914
Epoch 660, val loss: 1.1198796033859253
Epoch 670, training loss: 630.7816162109375 = 0.7587676048278809 + 100.0 * 6.300228118896484
Epoch 670, val loss: 1.1179790496826172
Epoch 680, training loss: 630.4152221679688 = 0.7443690299987793 + 100.0 * 6.296708106994629
Epoch 680, val loss: 1.1161786317825317
Epoch 690, training loss: 630.7539672851562 = 0.7302137017250061 + 100.0 * 6.30023717880249
Epoch 690, val loss: 1.1148412227630615
Epoch 700, training loss: 630.4382934570312 = 0.7162607908248901 + 100.0 * 6.297220230102539
Epoch 700, val loss: 1.1137049198150635
Epoch 710, training loss: 630.074462890625 = 0.7024139761924744 + 100.0 * 6.293720722198486
Epoch 710, val loss: 1.112521767616272
Epoch 720, training loss: 630.6275024414062 = 0.6887819170951843 + 100.0 * 6.299387454986572
Epoch 720, val loss: 1.1118049621582031
Epoch 730, training loss: 629.8662719726562 = 0.6752759218215942 + 100.0 * 6.291909694671631
Epoch 730, val loss: 1.1111440658569336
Epoch 740, training loss: 629.6101684570312 = 0.6619127988815308 + 100.0 * 6.289482116699219
Epoch 740, val loss: 1.1107631921768188
Epoch 750, training loss: 629.4126586914062 = 0.648764967918396 + 100.0 * 6.287639141082764
Epoch 750, val loss: 1.1105644702911377
Epoch 760, training loss: 630.03759765625 = 0.6357313990592957 + 100.0 * 6.294018745422363
Epoch 760, val loss: 1.1101514101028442
Epoch 770, training loss: 629.5302734375 = 0.6226282715797424 + 100.0 * 6.289076805114746
Epoch 770, val loss: 1.110548973083496
Epoch 780, training loss: 628.996826171875 = 0.6097302436828613 + 100.0 * 6.283870697021484
Epoch 780, val loss: 1.1106986999511719
Epoch 790, training loss: 629.0654296875 = 0.5969990491867065 + 100.0 * 6.284684658050537
Epoch 790, val loss: 1.1108149290084839
Epoch 800, training loss: 629.0222778320312 = 0.5842795372009277 + 100.0 * 6.284379959106445
Epoch 800, val loss: 1.1115509271621704
Epoch 810, training loss: 628.7890014648438 = 0.5716967582702637 + 100.0 * 6.282173156738281
Epoch 810, val loss: 1.111939549446106
Epoch 820, training loss: 628.5655517578125 = 0.5592737197875977 + 100.0 * 6.280062675476074
Epoch 820, val loss: 1.1129730939865112
Epoch 830, training loss: 628.4493408203125 = 0.5469893217086792 + 100.0 * 6.279023170471191
Epoch 830, val loss: 1.1137007474899292
Epoch 840, training loss: 629.4619750976562 = 0.5348295569419861 + 100.0 * 6.289271354675293
Epoch 840, val loss: 1.1147817373275757
Epoch 850, training loss: 628.593017578125 = 0.5225951671600342 + 100.0 * 6.280704498291016
Epoch 850, val loss: 1.1156052350997925
Epoch 860, training loss: 628.1181030273438 = 0.5106716752052307 + 100.0 * 6.276074409484863
Epoch 860, val loss: 1.1168373823165894
Epoch 870, training loss: 627.959228515625 = 0.4989663362503052 + 100.0 * 6.274602890014648
Epoch 870, val loss: 1.1182997226715088
Epoch 880, training loss: 627.981689453125 = 0.48746004700660706 + 100.0 * 6.274941921234131
Epoch 880, val loss: 1.1198668479919434
Epoch 890, training loss: 627.9331665039062 = 0.47606971859931946 + 100.0 * 6.274570941925049
Epoch 890, val loss: 1.121488094329834
Epoch 900, training loss: 627.7987670898438 = 0.4648491144180298 + 100.0 * 6.27333927154541
Epoch 900, val loss: 1.1231356859207153
Epoch 910, training loss: 627.9318237304688 = 0.45387953519821167 + 100.0 * 6.274779796600342
Epoch 910, val loss: 1.1250817775726318
Epoch 920, training loss: 627.592529296875 = 0.4431101381778717 + 100.0 * 6.271494388580322
Epoch 920, val loss: 1.1274526119232178
Epoch 930, training loss: 627.6129760742188 = 0.43263429403305054 + 100.0 * 6.271803855895996
Epoch 930, val loss: 1.129696011543274
Epoch 940, training loss: 627.4332885742188 = 0.4222794473171234 + 100.0 * 6.2701096534729
Epoch 940, val loss: 1.1318374872207642
Epoch 950, training loss: 627.2656860351562 = 0.4121693968772888 + 100.0 * 6.268535137176514
Epoch 950, val loss: 1.134509801864624
Epoch 960, training loss: 627.1055297851562 = 0.4022926688194275 + 100.0 * 6.267032623291016
Epoch 960, val loss: 1.1372474431991577
Epoch 970, training loss: 627.1915283203125 = 0.3926929831504822 + 100.0 * 6.267988204956055
Epoch 970, val loss: 1.140123963356018
Epoch 980, training loss: 627.0532836914062 = 0.3832593858242035 + 100.0 * 6.266700744628906
Epoch 980, val loss: 1.143133282661438
Epoch 990, training loss: 627.3427124023438 = 0.3740379214286804 + 100.0 * 6.269686698913574
Epoch 990, val loss: 1.1463435888290405
Epoch 1000, training loss: 626.911376953125 = 0.3650316596031189 + 100.0 * 6.265463352203369
Epoch 1000, val loss: 1.1492677927017212
Epoch 1010, training loss: 627.0092163085938 = 0.35630500316619873 + 100.0 * 6.266529083251953
Epoch 1010, val loss: 1.152916431427002
Epoch 1020, training loss: 626.6246337890625 = 0.34778591990470886 + 100.0 * 6.262768268585205
Epoch 1020, val loss: 1.156294822692871
Epoch 1030, training loss: 626.5902099609375 = 0.33953651785850525 + 100.0 * 6.26250696182251
Epoch 1030, val loss: 1.160002589225769
Epoch 1040, training loss: 626.54150390625 = 0.33148694038391113 + 100.0 * 6.2621002197265625
Epoch 1040, val loss: 1.1639223098754883
Epoch 1050, training loss: 627.3546142578125 = 0.32365313172340393 + 100.0 * 6.2703094482421875
Epoch 1050, val loss: 1.167851209640503
Epoch 1060, training loss: 626.6401977539062 = 0.3158573508262634 + 100.0 * 6.263243198394775
Epoch 1060, val loss: 1.1717890501022339
Epoch 1070, training loss: 626.208984375 = 0.3083806037902832 + 100.0 * 6.259005546569824
Epoch 1070, val loss: 1.1758602857589722
Epoch 1080, training loss: 626.1879272460938 = 0.30114489793777466 + 100.0 * 6.2588677406311035
Epoch 1080, val loss: 1.180120587348938
Epoch 1090, training loss: 626.7337036132812 = 0.29410311579704285 + 100.0 * 6.264395713806152
Epoch 1090, val loss: 1.1842408180236816
Epoch 1100, training loss: 626.3817749023438 = 0.2871147096157074 + 100.0 * 6.260946750640869
Epoch 1100, val loss: 1.1887174844741821
Epoch 1110, training loss: 626.6212158203125 = 0.2803400754928589 + 100.0 * 6.263408660888672
Epoch 1110, val loss: 1.1930501461029053
Epoch 1120, training loss: 626.01025390625 = 0.27372556924819946 + 100.0 * 6.2573652267456055
Epoch 1120, val loss: 1.1975460052490234
Epoch 1130, training loss: 625.822509765625 = 0.26733240485191345 + 100.0 * 6.255551815032959
Epoch 1130, val loss: 1.2024915218353271
Epoch 1140, training loss: 625.7456665039062 = 0.2611534297466278 + 100.0 * 6.254845142364502
Epoch 1140, val loss: 1.207431435585022
Epoch 1150, training loss: 625.7030639648438 = 0.25513631105422974 + 100.0 * 6.25447940826416
Epoch 1150, val loss: 1.2123392820358276
Epoch 1160, training loss: 626.3621215820312 = 0.24923254549503326 + 100.0 * 6.261129379272461
Epoch 1160, val loss: 1.2174135446548462
Epoch 1170, training loss: 625.8617553710938 = 0.24342817068099976 + 100.0 * 6.256183624267578
Epoch 1170, val loss: 1.2224429845809937
Epoch 1180, training loss: 625.5960693359375 = 0.23776984214782715 + 100.0 * 6.253582954406738
Epoch 1180, val loss: 1.2276581525802612
Epoch 1190, training loss: 625.7448120117188 = 0.23230981826782227 + 100.0 * 6.255125045776367
Epoch 1190, val loss: 1.2328475713729858
Epoch 1200, training loss: 626.0430908203125 = 0.2269250899553299 + 100.0 * 6.258161544799805
Epoch 1200, val loss: 1.2378509044647217
Epoch 1210, training loss: 625.5178833007812 = 0.22163614630699158 + 100.0 * 6.252962589263916
Epoch 1210, val loss: 1.2436208724975586
Epoch 1220, training loss: 625.27392578125 = 0.21651685237884521 + 100.0 * 6.250574111938477
Epoch 1220, val loss: 1.2491148710250854
Epoch 1230, training loss: 625.294921875 = 0.21157245337963104 + 100.0 * 6.250833511352539
Epoch 1230, val loss: 1.2549705505371094
Epoch 1240, training loss: 625.6505737304688 = 0.20669715106487274 + 100.0 * 6.254438400268555
Epoch 1240, val loss: 1.2605525255203247
Epoch 1250, training loss: 625.2155151367188 = 0.20193950831890106 + 100.0 * 6.25013542175293
Epoch 1250, val loss: 1.2662162780761719
Epoch 1260, training loss: 625.0506591796875 = 0.19728940725326538 + 100.0 * 6.248534202575684
Epoch 1260, val loss: 1.2722985744476318
Epoch 1270, training loss: 625.0475463867188 = 0.19280153512954712 + 100.0 * 6.248547077178955
Epoch 1270, val loss: 1.2783515453338623
Epoch 1280, training loss: 625.4862060546875 = 0.1883372664451599 + 100.0 * 6.252978801727295
Epoch 1280, val loss: 1.2841840982437134
Epoch 1290, training loss: 625.0407104492188 = 0.1839408576488495 + 100.0 * 6.248567581176758
Epoch 1290, val loss: 1.2900190353393555
Epoch 1300, training loss: 624.8441162109375 = 0.1797049641609192 + 100.0 * 6.246644496917725
Epoch 1300, val loss: 1.2962143421173096
Epoch 1310, training loss: 624.9853515625 = 0.17560355365276337 + 100.0 * 6.2480974197387695
Epoch 1310, val loss: 1.3025234937667847
Epoch 1320, training loss: 624.7705688476562 = 0.17157518863677979 + 100.0 * 6.245989799499512
Epoch 1320, val loss: 1.3085393905639648
Epoch 1330, training loss: 624.648193359375 = 0.16763758659362793 + 100.0 * 6.244805335998535
Epoch 1330, val loss: 1.3148212432861328
Epoch 1340, training loss: 624.6505737304688 = 0.16380895674228668 + 100.0 * 6.244867324829102
Epoch 1340, val loss: 1.3213533163070679
Epoch 1350, training loss: 625.2219848632812 = 0.16010643541812897 + 100.0 * 6.250618934631348
Epoch 1350, val loss: 1.3277935981750488
Epoch 1360, training loss: 624.7123413085938 = 0.15638138353824615 + 100.0 * 6.2455596923828125
Epoch 1360, val loss: 1.3332830667495728
Epoch 1370, training loss: 624.7462158203125 = 0.15282224118709564 + 100.0 * 6.245933532714844
Epoch 1370, val loss: 1.3401521444320679
Epoch 1380, training loss: 624.5394287109375 = 0.14930127561092377 + 100.0 * 6.243901252746582
Epoch 1380, val loss: 1.3464878797531128
Epoch 1390, training loss: 624.5602416992188 = 0.14586587250232697 + 100.0 * 6.244143486022949
Epoch 1390, val loss: 1.3531591892242432
Epoch 1400, training loss: 624.4102783203125 = 0.14256413280963898 + 100.0 * 6.242677211761475
Epoch 1400, val loss: 1.3597781658172607
Epoch 1410, training loss: 624.3304443359375 = 0.13932347297668457 + 100.0 * 6.241910934448242
Epoch 1410, val loss: 1.3663294315338135
Epoch 1420, training loss: 624.5684204101562 = 0.1361619532108307 + 100.0 * 6.244322776794434
Epoch 1420, val loss: 1.373122215270996
Epoch 1430, training loss: 624.529296875 = 0.1330655962228775 + 100.0 * 6.243962287902832
Epoch 1430, val loss: 1.3798834085464478
Epoch 1440, training loss: 624.5862426757812 = 0.13003665208816528 + 100.0 * 6.244561672210693
Epoch 1440, val loss: 1.385932207107544
Epoch 1450, training loss: 624.2417602539062 = 0.1270226240158081 + 100.0 * 6.241147518157959
Epoch 1450, val loss: 1.3930894136428833
Epoch 1460, training loss: 624.1107788085938 = 0.1241515576839447 + 100.0 * 6.239866256713867
Epoch 1460, val loss: 1.399672269821167
Epoch 1470, training loss: 624.0419921875 = 0.1213526725769043 + 100.0 * 6.239206314086914
Epoch 1470, val loss: 1.4068742990493774
Epoch 1480, training loss: 624.961669921875 = 0.11863727867603302 + 100.0 * 6.248430252075195
Epoch 1480, val loss: 1.413910984992981
Epoch 1490, training loss: 624.4732055664062 = 0.11591789871454239 + 100.0 * 6.24357271194458
Epoch 1490, val loss: 1.419412612915039
Epoch 1500, training loss: 624.0986328125 = 0.1132645308971405 + 100.0 * 6.239853382110596
Epoch 1500, val loss: 1.426384449005127
Epoch 1510, training loss: 623.8933715820312 = 0.1107059195637703 + 100.0 * 6.237826824188232
Epoch 1510, val loss: 1.4335944652557373
Epoch 1520, training loss: 623.9306030273438 = 0.10823749750852585 + 100.0 * 6.238224029541016
Epoch 1520, val loss: 1.4403032064437866
Epoch 1530, training loss: 624.0309448242188 = 0.10582654923200607 + 100.0 * 6.239251136779785
Epoch 1530, val loss: 1.4473015069961548
Epoch 1540, training loss: 623.90673828125 = 0.10345146805047989 + 100.0 * 6.238032817840576
Epoch 1540, val loss: 1.4538654088974
Epoch 1550, training loss: 624.1572265625 = 0.10114972293376923 + 100.0 * 6.240561008453369
Epoch 1550, val loss: 1.4605973958969116
Epoch 1560, training loss: 624.318359375 = 0.09885741770267487 + 100.0 * 6.242194652557373
Epoch 1560, val loss: 1.4673043489456177
Epoch 1570, training loss: 623.837158203125 = 0.0966254249215126 + 100.0 * 6.237405300140381
Epoch 1570, val loss: 1.4740320444107056
Epoch 1580, training loss: 623.6761474609375 = 0.09448099881410599 + 100.0 * 6.235816478729248
Epoch 1580, val loss: 1.4812546968460083
Epoch 1590, training loss: 623.57666015625 = 0.09240292757749557 + 100.0 * 6.234842300415039
Epoch 1590, val loss: 1.4880386590957642
Epoch 1600, training loss: 623.6197509765625 = 0.09038590639829636 + 100.0 * 6.235293865203857
Epoch 1600, val loss: 1.4948806762695312
Epoch 1610, training loss: 624.37548828125 = 0.08839712291955948 + 100.0 * 6.242871284484863
Epoch 1610, val loss: 1.5013468265533447
Epoch 1620, training loss: 623.4947509765625 = 0.08639389276504517 + 100.0 * 6.234083652496338
Epoch 1620, val loss: 1.508150577545166
Epoch 1630, training loss: 623.505859375 = 0.08448595553636551 + 100.0 * 6.234213829040527
Epoch 1630, val loss: 1.5153497457504272
Epoch 1640, training loss: 623.4205932617188 = 0.08265639841556549 + 100.0 * 6.233379364013672
Epoch 1640, val loss: 1.522315263748169
Epoch 1650, training loss: 623.4186401367188 = 0.08087681233882904 + 100.0 * 6.233377933502197
Epoch 1650, val loss: 1.529015064239502
Epoch 1660, training loss: 624.2229614257812 = 0.07913307100534439 + 100.0 * 6.241437911987305
Epoch 1660, val loss: 1.5354639291763306
Epoch 1670, training loss: 623.5426635742188 = 0.0773811861872673 + 100.0 * 6.234652519226074
Epoch 1670, val loss: 1.5426868200302124
Epoch 1680, training loss: 623.327880859375 = 0.07570385932922363 + 100.0 * 6.232522010803223
Epoch 1680, val loss: 1.5494519472122192
Epoch 1690, training loss: 623.47119140625 = 0.07409071922302246 + 100.0 * 6.233971118927002
Epoch 1690, val loss: 1.555939793586731
Epoch 1700, training loss: 623.2210083007812 = 0.07249901443719864 + 100.0 * 6.231484889984131
Epoch 1700, val loss: 1.5630019903182983
Epoch 1710, training loss: 623.229736328125 = 0.07095680385828018 + 100.0 * 6.2315874099731445
Epoch 1710, val loss: 1.5699480772018433
Epoch 1720, training loss: 623.9434814453125 = 0.06947354227304459 + 100.0 * 6.238739967346191
Epoch 1720, val loss: 1.5763978958129883
Epoch 1730, training loss: 623.570068359375 = 0.06795177608728409 + 100.0 * 6.235021591186523
Epoch 1730, val loss: 1.582727313041687
Epoch 1740, training loss: 623.2909545898438 = 0.06651423126459122 + 100.0 * 6.232244491577148
Epoch 1740, val loss: 1.5900553464889526
Epoch 1750, training loss: 623.0707397460938 = 0.06511199474334717 + 100.0 * 6.230056285858154
Epoch 1750, val loss: 1.5964957475662231
Epoch 1760, training loss: 623.04296875 = 0.06376650929450989 + 100.0 * 6.22979211807251
Epoch 1760, val loss: 1.6031982898712158
Epoch 1770, training loss: 623.2294921875 = 0.0624547116458416 + 100.0 * 6.231670379638672
Epoch 1770, val loss: 1.609833836555481
Epoch 1780, training loss: 623.10498046875 = 0.061157260090112686 + 100.0 * 6.230438232421875
Epoch 1780, val loss: 1.6163183450698853
Epoch 1790, training loss: 623.140625 = 0.059884294867515564 + 100.0 * 6.230807781219482
Epoch 1790, val loss: 1.6228790283203125
Epoch 1800, training loss: 623.7217407226562 = 0.058668263256549835 + 100.0 * 6.236630916595459
Epoch 1800, val loss: 1.629148006439209
Epoch 1810, training loss: 623.1472778320312 = 0.057424191385507584 + 100.0 * 6.230898380279541
Epoch 1810, val loss: 1.6353938579559326
Epoch 1820, training loss: 622.956787109375 = 0.05625851824879646 + 100.0 * 6.229005336761475
Epoch 1820, val loss: 1.6420559883117676
Epoch 1830, training loss: 622.9188232421875 = 0.05512106418609619 + 100.0 * 6.228637218475342
Epoch 1830, val loss: 1.6485540866851807
Epoch 1840, training loss: 623.2220458984375 = 0.05403617024421692 + 100.0 * 6.231680393218994
Epoch 1840, val loss: 1.654598593711853
Epoch 1850, training loss: 623.0137329101562 = 0.052934303879737854 + 100.0 * 6.229608058929443
Epoch 1850, val loss: 1.661609411239624
Epoch 1860, training loss: 622.7442626953125 = 0.05186982825398445 + 100.0 * 6.226923942565918
Epoch 1860, val loss: 1.66801917552948
Epoch 1870, training loss: 622.7655029296875 = 0.05084355175495148 + 100.0 * 6.227146625518799
Epoch 1870, val loss: 1.6744040250778198
Epoch 1880, training loss: 622.8319091796875 = 0.049853112548589706 + 100.0 * 6.22782039642334
Epoch 1880, val loss: 1.6812340021133423
Epoch 1890, training loss: 623.4099731445312 = 0.04888932406902313 + 100.0 * 6.233611106872559
Epoch 1890, val loss: 1.6872321367263794
Epoch 1900, training loss: 623.06103515625 = 0.04790621995925903 + 100.0 * 6.230131149291992
Epoch 1900, val loss: 1.6929969787597656
Epoch 1910, training loss: 622.7806396484375 = 0.046960655599832535 + 100.0 * 6.227336883544922
Epoch 1910, val loss: 1.699759840965271
Epoch 1920, training loss: 623.1729125976562 = 0.04608237370848656 + 100.0 * 6.231268405914307
Epoch 1920, val loss: 1.7053512334823608
Epoch 1930, training loss: 622.634765625 = 0.045165106654167175 + 100.0 * 6.225895881652832
Epoch 1930, val loss: 1.712225079536438
Epoch 1940, training loss: 622.5906982421875 = 0.044305942952632904 + 100.0 * 6.2254638671875
Epoch 1940, val loss: 1.7183239459991455
Epoch 1950, training loss: 622.5607299804688 = 0.04346875101327896 + 100.0 * 6.225172519683838
Epoch 1950, val loss: 1.7243927717208862
Epoch 1960, training loss: 622.5769653320312 = 0.04266001656651497 + 100.0 * 6.225342750549316
Epoch 1960, val loss: 1.7308882474899292
Epoch 1970, training loss: 623.3079833984375 = 0.04186880216002464 + 100.0 * 6.232661247253418
Epoch 1970, val loss: 1.7370615005493164
Epoch 1980, training loss: 622.7117309570312 = 0.04106782004237175 + 100.0 * 6.226706504821777
Epoch 1980, val loss: 1.7421634197235107
Epoch 1990, training loss: 622.4224243164062 = 0.040291983634233475 + 100.0 * 6.223821640014648
Epoch 1990, val loss: 1.748687505722046
Epoch 2000, training loss: 622.4446411132812 = 0.039563193917274475 + 100.0 * 6.224050998687744
Epoch 2000, val loss: 1.7552953958511353
Epoch 2010, training loss: 622.9197387695312 = 0.03885713964700699 + 100.0 * 6.228808403015137
Epoch 2010, val loss: 1.7612495422363281
Epoch 2020, training loss: 622.7142944335938 = 0.03813400864601135 + 100.0 * 6.226761341094971
Epoch 2020, val loss: 1.7662204504013062
Epoch 2030, training loss: 622.530517578125 = 0.037437696009874344 + 100.0 * 6.224930763244629
Epoch 2030, val loss: 1.7726374864578247
Epoch 2040, training loss: 622.4757690429688 = 0.03675461187958717 + 100.0 * 6.224390029907227
Epoch 2040, val loss: 1.7785794734954834
Epoch 2050, training loss: 622.8585815429688 = 0.03610502928495407 + 100.0 * 6.228224277496338
Epoch 2050, val loss: 1.7850769758224487
Epoch 2060, training loss: 622.4156494140625 = 0.03544711321592331 + 100.0 * 6.223801612854004
Epoch 2060, val loss: 1.7900805473327637
Epoch 2070, training loss: 622.2384643554688 = 0.034821849316358566 + 100.0 * 6.222035884857178
Epoch 2070, val loss: 1.7963649034500122
Epoch 2080, training loss: 622.370361328125 = 0.03422374278306961 + 100.0 * 6.223361015319824
Epoch 2080, val loss: 1.8019449710845947
Epoch 2090, training loss: 622.8312377929688 = 0.03363677114248276 + 100.0 * 6.227975845336914
Epoch 2090, val loss: 1.8076363801956177
Epoch 2100, training loss: 622.5451049804688 = 0.033031295984983444 + 100.0 * 6.225120544433594
Epoch 2100, val loss: 1.8140329122543335
Epoch 2110, training loss: 622.3980102539062 = 0.03246230259537697 + 100.0 * 6.2236552238464355
Epoch 2110, val loss: 1.8190816640853882
Epoch 2120, training loss: 622.460693359375 = 0.031906966120004654 + 100.0 * 6.224287509918213
Epoch 2120, val loss: 1.8254085779190063
Epoch 2130, training loss: 622.16357421875 = 0.0313541442155838 + 100.0 * 6.221322059631348
Epoch 2130, val loss: 1.8308587074279785
Epoch 2140, training loss: 622.1588134765625 = 0.030826540663838387 + 100.0 * 6.221280097961426
Epoch 2140, val loss: 1.8365767002105713
Epoch 2150, training loss: 622.4136352539062 = 0.03031800501048565 + 100.0 * 6.223833084106445
Epoch 2150, val loss: 1.841840147972107
Epoch 2160, training loss: 622.3670654296875 = 0.02980741672217846 + 100.0 * 6.223372936248779
Epoch 2160, val loss: 1.8479485511779785
Epoch 2170, training loss: 622.0506591796875 = 0.029299527406692505 + 100.0 * 6.220213413238525
Epoch 2170, val loss: 1.853023648262024
Epoch 2180, training loss: 622.0245361328125 = 0.028813503682613373 + 100.0 * 6.21995735168457
Epoch 2180, val loss: 1.858695387840271
Epoch 2190, training loss: 622.0860595703125 = 0.028354162350296974 + 100.0 * 6.220576763153076
Epoch 2190, val loss: 1.8647363185882568
Epoch 2200, training loss: 622.8429565429688 = 0.027899935841560364 + 100.0 * 6.228150367736816
Epoch 2200, val loss: 1.8698536157608032
Epoch 2210, training loss: 622.1472778320312 = 0.027419032528996468 + 100.0 * 6.221199035644531
Epoch 2210, val loss: 1.87454354763031
Epoch 2220, training loss: 621.9077758789062 = 0.026979001238942146 + 100.0 * 6.218807697296143
Epoch 2220, val loss: 1.880654215812683
Epoch 2230, training loss: 621.8787841796875 = 0.026555342599749565 + 100.0 * 6.218522548675537
Epoch 2230, val loss: 1.886006236076355
Epoch 2240, training loss: 622.14892578125 = 0.026146460324525833 + 100.0 * 6.221228122711182
Epoch 2240, val loss: 1.8916078805923462
Epoch 2250, training loss: 622.0712280273438 = 0.025729559361934662 + 100.0 * 6.220454692840576
Epoch 2250, val loss: 1.8966853618621826
Epoch 2260, training loss: 622.0565185546875 = 0.025318020954728127 + 100.0 * 6.220312118530273
Epoch 2260, val loss: 1.9018875360488892
Epoch 2270, training loss: 621.9991455078125 = 0.024921873584389687 + 100.0 * 6.219742298126221
Epoch 2270, val loss: 1.9068598747253418
Epoch 2280, training loss: 622.3410034179688 = 0.02453664317727089 + 100.0 * 6.2231645584106445
Epoch 2280, val loss: 1.9121590852737427
Epoch 2290, training loss: 621.8240356445312 = 0.02415568195283413 + 100.0 * 6.217998504638672
Epoch 2290, val loss: 1.9172178506851196
Epoch 2300, training loss: 621.8419799804688 = 0.023793505504727364 + 100.0 * 6.218181610107422
Epoch 2300, val loss: 1.9228190183639526
Epoch 2310, training loss: 621.7993774414062 = 0.02343984879553318 + 100.0 * 6.217759609222412
Epoch 2310, val loss: 1.9277353286743164
Epoch 2320, training loss: 622.5059204101562 = 0.02310876175761223 + 100.0 * 6.224827766418457
Epoch 2320, val loss: 1.9319747686386108
Epoch 2330, training loss: 622.0209350585938 = 0.022742528468370438 + 100.0 * 6.219981670379639
Epoch 2330, val loss: 1.9380064010620117
Epoch 2340, training loss: 621.85498046875 = 0.022401532158255577 + 100.0 * 6.218325614929199
Epoch 2340, val loss: 1.9428672790527344
Epoch 2350, training loss: 621.7557373046875 = 0.022070836275815964 + 100.0 * 6.217336177825928
Epoch 2350, val loss: 1.9480952024459839
Epoch 2360, training loss: 622.27880859375 = 0.02175973169505596 + 100.0 * 6.222570419311523
Epoch 2360, val loss: 1.9529701471328735
Epoch 2370, training loss: 621.9450073242188 = 0.02143498696386814 + 100.0 * 6.219235897064209
Epoch 2370, val loss: 1.9585765600204468
Epoch 2380, training loss: 621.6476440429688 = 0.02111739106476307 + 100.0 * 6.2162652015686035
Epoch 2380, val loss: 1.962809681892395
Epoch 2390, training loss: 621.6073608398438 = 0.02081761322915554 + 100.0 * 6.215865612030029
Epoch 2390, val loss: 1.9680838584899902
Epoch 2400, training loss: 621.66796875 = 0.02052961103618145 + 100.0 * 6.216474533081055
Epoch 2400, val loss: 1.973073959350586
Epoch 2410, training loss: 622.6203002929688 = 0.020257996395230293 + 100.0 * 6.2260003089904785
Epoch 2410, val loss: 1.9776079654693604
Epoch 2420, training loss: 622.056396484375 = 0.019947003573179245 + 100.0 * 6.220364093780518
Epoch 2420, val loss: 1.9829899072647095
Epoch 2430, training loss: 621.6893920898438 = 0.019664105027914047 + 100.0 * 6.2166972160339355
Epoch 2430, val loss: 1.9871516227722168
Epoch 2440, training loss: 621.5623168945312 = 0.01939167082309723 + 100.0 * 6.215429306030273
Epoch 2440, val loss: 1.9924546480178833
Epoch 2450, training loss: 621.8900756835938 = 0.019138621166348457 + 100.0 * 6.2187089920043945
Epoch 2450, val loss: 1.9974231719970703
Epoch 2460, training loss: 621.5189208984375 = 0.018866004422307014 + 100.0 * 6.215000629425049
Epoch 2460, val loss: 2.001685857772827
Epoch 2470, training loss: 621.6742553710938 = 0.018609914928674698 + 100.0 * 6.216556549072266
Epoch 2470, val loss: 2.005922555923462
Epoch 2480, training loss: 621.9437866210938 = 0.01836245320737362 + 100.0 * 6.219254016876221
Epoch 2480, val loss: 2.0106849670410156
Epoch 2490, training loss: 621.6920166015625 = 0.01811111532151699 + 100.0 * 6.216739177703857
Epoch 2490, val loss: 2.015779495239258
Epoch 2500, training loss: 621.9883422851562 = 0.017868831753730774 + 100.0 * 6.219704627990723
Epoch 2500, val loss: 2.020188093185425
Epoch 2510, training loss: 621.5714111328125 = 0.017627468332648277 + 100.0 * 6.2155375480651855
Epoch 2510, val loss: 2.024731159210205
Epoch 2520, training loss: 621.3809204101562 = 0.017394864931702614 + 100.0 * 6.213634967803955
Epoch 2520, val loss: 2.029480457305908
Epoch 2530, training loss: 621.4100952148438 = 0.017173364758491516 + 100.0 * 6.213929176330566
Epoch 2530, val loss: 2.0340569019317627
Epoch 2540, training loss: 621.6287231445312 = 0.016962163150310516 + 100.0 * 6.2161173820495605
Epoch 2540, val loss: 2.0384087562561035
Epoch 2550, training loss: 621.674072265625 = 0.016739677637815475 + 100.0 * 6.216573715209961
Epoch 2550, val loss: 2.043072462081909
Epoch 2560, training loss: 621.683837890625 = 0.01652098447084427 + 100.0 * 6.216672897338867
Epoch 2560, val loss: 2.0480165481567383
Epoch 2570, training loss: 621.3615112304688 = 0.016303684562444687 + 100.0 * 6.213451862335205
Epoch 2570, val loss: 2.051466226577759
Epoch 2580, training loss: 621.302001953125 = 0.016100870445370674 + 100.0 * 6.2128586769104
Epoch 2580, val loss: 2.0563735961914062
Epoch 2590, training loss: 621.3941040039062 = 0.015908310189843178 + 100.0 * 6.213781833648682
Epoch 2590, val loss: 2.060997724533081
Epoch 2600, training loss: 621.7933349609375 = 0.015711888670921326 + 100.0 * 6.217776775360107
Epoch 2600, val loss: 2.0649805068969727
Epoch 2610, training loss: 621.41259765625 = 0.015512919053435326 + 100.0 * 6.213971138000488
Epoch 2610, val loss: 2.068983316421509
Epoch 2620, training loss: 621.4202270507812 = 0.0153244249522686 + 100.0 * 6.214049339294434
Epoch 2620, val loss: 2.074159622192383
Epoch 2630, training loss: 621.5744018554688 = 0.015138261020183563 + 100.0 * 6.215592384338379
Epoch 2630, val loss: 2.078134536743164
Epoch 2640, training loss: 621.63232421875 = 0.014959530904889107 + 100.0 * 6.2161736488342285
Epoch 2640, val loss: 2.0821335315704346
Epoch 2650, training loss: 621.2208862304688 = 0.014771055430173874 + 100.0 * 6.212060928344727
Epoch 2650, val loss: 2.086646795272827
Epoch 2660, training loss: 621.285888671875 = 0.01459776982665062 + 100.0 * 6.212713241577148
Epoch 2660, val loss: 2.0908501148223877
Epoch 2670, training loss: 621.48193359375 = 0.014429252594709396 + 100.0 * 6.214675426483154
Epoch 2670, val loss: 2.0950310230255127
Epoch 2680, training loss: 622.1396484375 = 0.014261075295507908 + 100.0 * 6.221253871917725
Epoch 2680, val loss: 2.0989773273468018
Epoch 2690, training loss: 621.3468017578125 = 0.014082647860050201 + 100.0 * 6.213326930999756
Epoch 2690, val loss: 2.1023290157318115
Epoch 2700, training loss: 621.202392578125 = 0.013914298266172409 + 100.0 * 6.21188497543335
Epoch 2700, val loss: 2.106937885284424
Epoch 2710, training loss: 621.1187133789062 = 0.01375757809728384 + 100.0 * 6.211050033569336
Epoch 2710, val loss: 2.111074924468994
Epoch 2720, training loss: 621.071044921875 = 0.01360313966870308 + 100.0 * 6.210574626922607
Epoch 2720, val loss: 2.1151955127716064
Epoch 2730, training loss: 621.1240844726562 = 0.013455312699079514 + 100.0 * 6.211106777191162
Epoch 2730, val loss: 2.1187989711761475
Epoch 2740, training loss: 621.7837524414062 = 0.013309132307767868 + 100.0 * 6.2177042961120605
Epoch 2740, val loss: 2.1221556663513184
Epoch 2750, training loss: 621.6671752929688 = 0.013153726235032082 + 100.0 * 6.2165398597717285
Epoch 2750, val loss: 2.1274960041046143
Epoch 2760, training loss: 621.1967163085938 = 0.012998754158616066 + 100.0 * 6.211837291717529
Epoch 2760, val loss: 2.1306216716766357
Epoch 2770, training loss: 620.9976196289062 = 0.012854703702032566 + 100.0 * 6.209847450256348
Epoch 2770, val loss: 2.134976387023926
Epoch 2780, training loss: 621.044677734375 = 0.012719028629362583 + 100.0 * 6.210319995880127
Epoch 2780, val loss: 2.138781785964966
Epoch 2790, training loss: 621.8312377929688 = 0.012591698206961155 + 100.0 * 6.218186378479004
Epoch 2790, val loss: 2.1418988704681396
Epoch 2800, training loss: 621.5067138671875 = 0.01244067121297121 + 100.0 * 6.214942932128906
Epoch 2800, val loss: 2.146686315536499
Epoch 2810, training loss: 621.1259765625 = 0.012304014526307583 + 100.0 * 6.211136341094971
Epoch 2810, val loss: 2.149672746658325
Epoch 2820, training loss: 620.9712524414062 = 0.012171153910458088 + 100.0 * 6.209590911865234
Epoch 2820, val loss: 2.154273509979248
Epoch 2830, training loss: 620.9535522460938 = 0.012046171352267265 + 100.0 * 6.209415435791016
Epoch 2830, val loss: 2.157804489135742
Epoch 2840, training loss: 622.0438232421875 = 0.011926182545721531 + 100.0 * 6.2203192710876465
Epoch 2840, val loss: 2.161560535430908
Epoch 2850, training loss: 621.2628784179688 = 0.011793320067226887 + 100.0 * 6.21251106262207
Epoch 2850, val loss: 2.164940118789673
Epoch 2860, training loss: 621.0272827148438 = 0.011665616184473038 + 100.0 * 6.210156440734863
Epoch 2860, val loss: 2.1686911582946777
Epoch 2870, training loss: 621.0735473632812 = 0.011549629271030426 + 100.0 * 6.210619926452637
Epoch 2870, val loss: 2.1724443435668945
Epoch 2880, training loss: 621.380126953125 = 0.011433319188654423 + 100.0 * 6.213686943054199
Epoch 2880, val loss: 2.175614595413208
Epoch 2890, training loss: 621.0401000976562 = 0.011311494745314121 + 100.0 * 6.210288047790527
Epoch 2890, val loss: 2.1795315742492676
Epoch 2900, training loss: 621.0150146484375 = 0.011198033578693867 + 100.0 * 6.210038661956787
Epoch 2900, val loss: 2.182948589324951
Epoch 2910, training loss: 621.2085571289062 = 0.01108663622289896 + 100.0 * 6.211974620819092
Epoch 2910, val loss: 2.1863420009613037
Epoch 2920, training loss: 620.8375854492188 = 0.010972380638122559 + 100.0 * 6.208265781402588
Epoch 2920, val loss: 2.190549612045288
Epoch 2930, training loss: 620.87353515625 = 0.010866050608456135 + 100.0 * 6.208626747131348
Epoch 2930, val loss: 2.194035768508911
Epoch 2940, training loss: 621.13720703125 = 0.010762479156255722 + 100.0 * 6.211264610290527
Epoch 2940, val loss: 2.1972577571868896
Epoch 2950, training loss: 620.9956665039062 = 0.010655486956238747 + 100.0 * 6.209849834442139
Epoch 2950, val loss: 2.2005209922790527
Epoch 2960, training loss: 620.8508911132812 = 0.010548260062932968 + 100.0 * 6.20840311050415
Epoch 2960, val loss: 2.2044074535369873
Epoch 2970, training loss: 620.7529296875 = 0.010445976629853249 + 100.0 * 6.207425117492676
Epoch 2970, val loss: 2.207719087600708
Epoch 2980, training loss: 620.7354125976562 = 0.010351177304983139 + 100.0 * 6.207250595092773
Epoch 2980, val loss: 2.2112395763397217
Epoch 2990, training loss: 621.152099609375 = 0.010263733565807343 + 100.0 * 6.211418628692627
Epoch 2990, val loss: 2.2136549949645996
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 861.6416015625 = 1.9573142528533936 + 100.0 * 8.596842765808105
Epoch 0, val loss: 1.9535943269729614
Epoch 10, training loss: 861.5604248046875 = 1.947849988937378 + 100.0 * 8.596125602722168
Epoch 10, val loss: 1.9443440437316895
Epoch 20, training loss: 861.0576171875 = 1.935813069343567 + 100.0 * 8.591217994689941
Epoch 20, val loss: 1.932058572769165
Epoch 30, training loss: 857.4271240234375 = 1.919861912727356 + 100.0 * 8.555072784423828
Epoch 30, val loss: 1.9154739379882812
Epoch 40, training loss: 833.1083984375 = 1.8998003005981445 + 100.0 * 8.31208610534668
Epoch 40, val loss: 1.8952993154525757
Epoch 50, training loss: 764.5806884765625 = 1.8782806396484375 + 100.0 * 7.627023696899414
Epoch 50, val loss: 1.8748639822006226
Epoch 60, training loss: 727.2816772460938 = 1.8652150630950928 + 100.0 * 7.254164218902588
Epoch 60, val loss: 1.8626588582992554
Epoch 70, training loss: 705.5472412109375 = 1.855545997619629 + 100.0 * 7.036917209625244
Epoch 70, val loss: 1.8524659872055054
Epoch 80, training loss: 692.2451171875 = 1.8449825048446655 + 100.0 * 6.904001712799072
Epoch 80, val loss: 1.842139482498169
Epoch 90, training loss: 684.752685546875 = 1.834071159362793 + 100.0 * 6.82918643951416
Epoch 90, val loss: 1.8315353393554688
Epoch 100, training loss: 678.6162719726562 = 1.8241952657699585 + 100.0 * 6.76792049407959
Epoch 100, val loss: 1.8217276334762573
Epoch 110, training loss: 673.0515747070312 = 1.814873218536377 + 100.0 * 6.712367057800293
Epoch 110, val loss: 1.8125392198562622
Epoch 120, training loss: 668.666259765625 = 1.8061984777450562 + 100.0 * 6.668600559234619
Epoch 120, val loss: 1.8039768934249878
Epoch 130, training loss: 664.909423828125 = 1.797668218612671 + 100.0 * 6.631117343902588
Epoch 130, val loss: 1.7956849336624146
Epoch 140, training loss: 661.7505493164062 = 1.7892980575561523 + 100.0 * 6.599612712860107
Epoch 140, val loss: 1.787443995475769
Epoch 150, training loss: 658.8345947265625 = 1.7808328866958618 + 100.0 * 6.570537567138672
Epoch 150, val loss: 1.7791268825531006
Epoch 160, training loss: 656.1134643554688 = 1.7720876932144165 + 100.0 * 6.5434136390686035
Epoch 160, val loss: 1.7707195281982422
Epoch 170, training loss: 654.2857666015625 = 1.7628668546676636 + 100.0 * 6.525228977203369
Epoch 170, val loss: 1.7620292901992798
Epoch 180, training loss: 652.3645629882812 = 1.752859115600586 + 100.0 * 6.50611686706543
Epoch 180, val loss: 1.7527289390563965
Epoch 190, training loss: 650.5917358398438 = 1.742087483406067 + 100.0 * 6.488496780395508
Epoch 190, val loss: 1.7428443431854248
Epoch 200, training loss: 649.1525268554688 = 1.730404257774353 + 100.0 * 6.474221229553223
Epoch 200, val loss: 1.7322739362716675
Epoch 210, training loss: 647.962890625 = 1.7177003622055054 + 100.0 * 6.462451934814453
Epoch 210, val loss: 1.7208830118179321
Epoch 220, training loss: 646.7737426757812 = 1.7040297985076904 + 100.0 * 6.45069694519043
Epoch 220, val loss: 1.7086998224258423
Epoch 230, training loss: 645.71337890625 = 1.6893452405929565 + 100.0 * 6.440240383148193
Epoch 230, val loss: 1.6957287788391113
Epoch 240, training loss: 644.892333984375 = 1.673641324043274 + 100.0 * 6.432187080383301
Epoch 240, val loss: 1.681916356086731
Epoch 250, training loss: 644.4528198242188 = 1.6568230390548706 + 100.0 * 6.42795991897583
Epoch 250, val loss: 1.6672236919403076
Epoch 260, training loss: 643.3948364257812 = 1.638931155204773 + 100.0 * 6.4175591468811035
Epoch 260, val loss: 1.6516528129577637
Epoch 270, training loss: 642.3860473632812 = 1.6200929880142212 + 100.0 * 6.40765905380249
Epoch 270, val loss: 1.6353604793548584
Epoch 280, training loss: 641.630859375 = 1.600425124168396 + 100.0 * 6.400304794311523
Epoch 280, val loss: 1.618590235710144
Epoch 290, training loss: 640.9512939453125 = 1.5800340175628662 + 100.0 * 6.393712997436523
Epoch 290, val loss: 1.6013449430465698
Epoch 300, training loss: 641.4039916992188 = 1.5589122772216797 + 100.0 * 6.39845085144043
Epoch 300, val loss: 1.5835280418395996
Epoch 310, training loss: 640.1591186523438 = 1.5371521711349487 + 100.0 * 6.386219501495361
Epoch 310, val loss: 1.5654892921447754
Epoch 320, training loss: 639.2633056640625 = 1.5150505304336548 + 100.0 * 6.3774824142456055
Epoch 320, val loss: 1.5473606586456299
Epoch 330, training loss: 638.734375 = 1.4927266836166382 + 100.0 * 6.3724164962768555
Epoch 330, val loss: 1.529248595237732
Epoch 340, training loss: 638.2559814453125 = 1.4702244997024536 + 100.0 * 6.367857456207275
Epoch 340, val loss: 1.5112082958221436
Epoch 350, training loss: 638.0648193359375 = 1.4475188255310059 + 100.0 * 6.366173267364502
Epoch 350, val loss: 1.493145227432251
Epoch 360, training loss: 637.5863647460938 = 1.424830436706543 + 100.0 * 6.3616156578063965
Epoch 360, val loss: 1.4753650426864624
Epoch 370, training loss: 636.9512939453125 = 1.4022538661956787 + 100.0 * 6.355490207672119
Epoch 370, val loss: 1.4580018520355225
Epoch 380, training loss: 636.6248779296875 = 1.379814624786377 + 100.0 * 6.352450847625732
Epoch 380, val loss: 1.4409321546554565
Epoch 390, training loss: 636.3976440429688 = 1.3574661016464233 + 100.0 * 6.350401401519775
Epoch 390, val loss: 1.4239425659179688
Epoch 400, training loss: 635.78369140625 = 1.335158109664917 + 100.0 * 6.344485759735107
Epoch 400, val loss: 1.4073954820632935
Epoch 410, training loss: 635.4627075195312 = 1.3129940032958984 + 100.0 * 6.34149694442749
Epoch 410, val loss: 1.3911826610565186
Epoch 420, training loss: 636.1919555664062 = 1.2910637855529785 + 100.0 * 6.349008560180664
Epoch 420, val loss: 1.375500202178955
Epoch 430, training loss: 634.99169921875 = 1.2687846422195435 + 100.0 * 6.337229251861572
Epoch 430, val loss: 1.359345555305481
Epoch 440, training loss: 634.608642578125 = 1.2466806173324585 + 100.0 * 6.333619117736816
Epoch 440, val loss: 1.3435649871826172
Epoch 450, training loss: 634.2822265625 = 1.2247077226638794 + 100.0 * 6.330574989318848
Epoch 450, val loss: 1.3280467987060547
Epoch 460, training loss: 634.3120727539062 = 1.2027560472488403 + 100.0 * 6.3310933113098145
Epoch 460, val loss: 1.312741756439209
Epoch 470, training loss: 633.79638671875 = 1.1808427572250366 + 100.0 * 6.326155662536621
Epoch 470, val loss: 1.2978333234786987
Epoch 480, training loss: 633.4734497070312 = 1.1589622497558594 + 100.0 * 6.323144912719727
Epoch 480, val loss: 1.282910704612732
Epoch 490, training loss: 633.865234375 = 1.1372120380401611 + 100.0 * 6.327280521392822
Epoch 490, val loss: 1.2681792974472046
Epoch 500, training loss: 633.041748046875 = 1.1154292821884155 + 100.0 * 6.319263458251953
Epoch 500, val loss: 1.2538814544677734
Epoch 510, training loss: 632.68603515625 = 1.0937325954437256 + 100.0 * 6.315922737121582
Epoch 510, val loss: 1.2398961782455444
Epoch 520, training loss: 632.6580810546875 = 1.0722761154174805 + 100.0 * 6.315857887268066
Epoch 520, val loss: 1.2260096073150635
Epoch 530, training loss: 632.4105834960938 = 1.051019549369812 + 100.0 * 6.313595294952393
Epoch 530, val loss: 1.212794542312622
Epoch 540, training loss: 632.1646728515625 = 1.029914140701294 + 100.0 * 6.311347961425781
Epoch 540, val loss: 1.1995102167129517
Epoch 550, training loss: 631.9326782226562 = 1.0091228485107422 + 100.0 * 6.309235572814941
Epoch 550, val loss: 1.1871951818466187
Epoch 560, training loss: 631.774169921875 = 0.9885506629943848 + 100.0 * 6.307856559753418
Epoch 560, val loss: 1.1748896837234497
Epoch 570, training loss: 631.6083984375 = 0.9682662487030029 + 100.0 * 6.306401252746582
Epoch 570, val loss: 1.163119912147522
Epoch 580, training loss: 631.8292236328125 = 0.948268473148346 + 100.0 * 6.308809757232666
Epoch 580, val loss: 1.1514219045639038
Epoch 590, training loss: 631.2830200195312 = 0.928541362285614 + 100.0 * 6.303544998168945
Epoch 590, val loss: 1.1407297849655151
Epoch 600, training loss: 630.90869140625 = 0.9091708660125732 + 100.0 * 6.299994945526123
Epoch 600, val loss: 1.130253553390503
Epoch 610, training loss: 630.9525756835938 = 0.890217661857605 + 100.0 * 6.300623893737793
Epoch 610, val loss: 1.1204545497894287
Epoch 620, training loss: 630.6123046875 = 0.871472179889679 + 100.0 * 6.297408580780029
Epoch 620, val loss: 1.1105057001113892
Epoch 630, training loss: 630.9066772460938 = 0.8531004190444946 + 100.0 * 6.300535678863525
Epoch 630, val loss: 1.1014946699142456
Epoch 640, training loss: 630.3012084960938 = 0.8349923491477966 + 100.0 * 6.294661998748779
Epoch 640, val loss: 1.0929381847381592
Epoch 650, training loss: 630.106201171875 = 0.8173519968986511 + 100.0 * 6.292888641357422
Epoch 650, val loss: 1.0847567319869995
Epoch 660, training loss: 630.1516723632812 = 0.8001453876495361 + 100.0 * 6.293515205383301
Epoch 660, val loss: 1.077014446258545
Epoch 670, training loss: 630.0206298828125 = 0.7831755876541138 + 100.0 * 6.292374134063721
Epoch 670, val loss: 1.070027232170105
Epoch 680, training loss: 629.7987060546875 = 0.7665327191352844 + 100.0 * 6.2903218269348145
Epoch 680, val loss: 1.062743902206421
Epoch 690, training loss: 629.6398315429688 = 0.7503092885017395 + 100.0 * 6.288895130157471
Epoch 690, val loss: 1.056566834449768
Epoch 700, training loss: 630.1085815429688 = 0.7344383001327515 + 100.0 * 6.293741703033447
Epoch 700, val loss: 1.0506689548492432
Epoch 710, training loss: 629.6809692382812 = 0.7188318371772766 + 100.0 * 6.289621829986572
Epoch 710, val loss: 1.0445139408111572
Epoch 720, training loss: 629.2647094726562 = 0.7036277651786804 + 100.0 * 6.285610675811768
Epoch 720, val loss: 1.0393528938293457
Epoch 730, training loss: 629.4679565429688 = 0.6887999773025513 + 100.0 * 6.287791728973389
Epoch 730, val loss: 1.0345535278320312
Epoch 740, training loss: 629.0813598632812 = 0.6742122769355774 + 100.0 * 6.284071445465088
Epoch 740, val loss: 1.0296649932861328
Epoch 750, training loss: 628.85888671875 = 0.6600227952003479 + 100.0 * 6.281988620758057
Epoch 750, val loss: 1.0257318019866943
Epoch 760, training loss: 629.2081298828125 = 0.6461706757545471 + 100.0 * 6.285619258880615
Epoch 760, val loss: 1.0218002796173096
Epoch 770, training loss: 628.6802368164062 = 0.6325266361236572 + 100.0 * 6.280477523803711
Epoch 770, val loss: 1.0179014205932617
Epoch 780, training loss: 628.483642578125 = 0.619218111038208 + 100.0 * 6.278644561767578
Epoch 780, val loss: 1.0147678852081299
Epoch 790, training loss: 629.1455078125 = 0.6061917543411255 + 100.0 * 6.285392761230469
Epoch 790, val loss: 1.0117028951644897
Epoch 800, training loss: 628.3954467773438 = 0.5933664441108704 + 100.0 * 6.27802038192749
Epoch 800, val loss: 1.0088449716567993
Epoch 810, training loss: 628.1383056640625 = 0.580847442150116 + 100.0 * 6.275574207305908
Epoch 810, val loss: 1.0063114166259766
Epoch 820, training loss: 628.215576171875 = 0.5686234831809998 + 100.0 * 6.276469707489014
Epoch 820, val loss: 1.0039052963256836
Epoch 830, training loss: 628.1176147460938 = 0.5565809011459351 + 100.0 * 6.275610446929932
Epoch 830, val loss: 1.0023009777069092
Epoch 840, training loss: 628.000732421875 = 0.5447744727134705 + 100.0 * 6.274559497833252
Epoch 840, val loss: 1.0000648498535156
Epoch 850, training loss: 627.768798828125 = 0.5332023501396179 + 100.0 * 6.272356033325195
Epoch 850, val loss: 0.9987441301345825
Epoch 860, training loss: 628.183349609375 = 0.521916925907135 + 100.0 * 6.276614665985107
Epoch 860, val loss: 0.9975103139877319
Epoch 870, training loss: 628.04150390625 = 0.5105904340744019 + 100.0 * 6.275309085845947
Epoch 870, val loss: 0.9963299632072449
Epoch 880, training loss: 627.5734252929688 = 0.49960756301879883 + 100.0 * 6.270737648010254
Epoch 880, val loss: 0.9953659772872925
Epoch 890, training loss: 627.934814453125 = 0.4887944757938385 + 100.0 * 6.274460315704346
Epoch 890, val loss: 0.9945793747901917
Epoch 900, training loss: 627.4044799804688 = 0.47807636857032776 + 100.0 * 6.269263744354248
Epoch 900, val loss: 0.9939691424369812
Epoch 910, training loss: 627.2583618164062 = 0.46761634945869446 + 100.0 * 6.267907619476318
Epoch 910, val loss: 0.993779182434082
Epoch 920, training loss: 627.2244873046875 = 0.45735862851142883 + 100.0 * 6.267671585083008
Epoch 920, val loss: 0.9938422441482544
Epoch 930, training loss: 627.3234252929688 = 0.4472368359565735 + 100.0 * 6.26876163482666
Epoch 930, val loss: 0.9938848614692688
Epoch 940, training loss: 627.4913940429688 = 0.4371962547302246 + 100.0 * 6.270542144775391
Epoch 940, val loss: 0.9938641786575317
Epoch 950, training loss: 627.1336059570312 = 0.42729905247688293 + 100.0 * 6.267063140869141
Epoch 950, val loss: 0.9940531253814697
Epoch 960, training loss: 626.917236328125 = 0.41759058833122253 + 100.0 * 6.26499605178833
Epoch 960, val loss: 0.9948296546936035
Epoch 970, training loss: 626.9913330078125 = 0.40804561972618103 + 100.0 * 6.265833377838135
Epoch 970, val loss: 0.9953994154930115
Epoch 980, training loss: 626.8514404296875 = 0.3986736834049225 + 100.0 * 6.264527797698975
Epoch 980, val loss: 0.9966720938682556
Epoch 990, training loss: 626.608154296875 = 0.3894030749797821 + 100.0 * 6.262187480926514
Epoch 990, val loss: 0.99775230884552
Epoch 1000, training loss: 627.1594848632812 = 0.38034892082214355 + 100.0 * 6.267791271209717
Epoch 1000, val loss: 0.99919593334198
Epoch 1010, training loss: 626.5885620117188 = 0.3713296055793762 + 100.0 * 6.262172222137451
Epoch 1010, val loss: 1.0000717639923096
Epoch 1020, training loss: 626.4542236328125 = 0.3625032603740692 + 100.0 * 6.2609171867370605
Epoch 1020, val loss: 1.0016841888427734
Epoch 1030, training loss: 626.90234375 = 0.35390621423721313 + 100.0 * 6.265484809875488
Epoch 1030, val loss: 1.0035617351531982
Epoch 1040, training loss: 626.3302001953125 = 0.3453500270843506 + 100.0 * 6.259848594665527
Epoch 1040, val loss: 1.0048446655273438
Epoch 1050, training loss: 626.1773681640625 = 0.33700031042099 + 100.0 * 6.258403778076172
Epoch 1050, val loss: 1.0072344541549683
Epoch 1060, training loss: 626.5260620117188 = 0.32881370186805725 + 100.0 * 6.261972427368164
Epoch 1060, val loss: 1.0092146396636963
Epoch 1070, training loss: 626.173828125 = 0.3207389712333679 + 100.0 * 6.258530616760254
Epoch 1070, val loss: 1.0112924575805664
Epoch 1080, training loss: 626.4817504882812 = 0.31282612681388855 + 100.0 * 6.26168966293335
Epoch 1080, val loss: 1.0136818885803223
Epoch 1090, training loss: 626.154052734375 = 0.3051167130470276 + 100.0 * 6.25848913192749
Epoch 1090, val loss: 1.0162500143051147
Epoch 1100, training loss: 626.3104248046875 = 0.2975160479545593 + 100.0 * 6.260128498077393
Epoch 1100, val loss: 1.0186676979064941
Epoch 1110, training loss: 625.727294921875 = 0.29011771082878113 + 100.0 * 6.2543721199035645
Epoch 1110, val loss: 1.0217516422271729
Epoch 1120, training loss: 625.736328125 = 0.28288382291793823 + 100.0 * 6.2545342445373535
Epoch 1120, val loss: 1.0247540473937988
Epoch 1130, training loss: 625.6876220703125 = 0.27582085132598877 + 100.0 * 6.254117965698242
Epoch 1130, val loss: 1.0275603532791138
Epoch 1140, training loss: 626.3602905273438 = 0.26891031861305237 + 100.0 * 6.260913848876953
Epoch 1140, val loss: 1.030288577079773
Epoch 1150, training loss: 625.5846557617188 = 0.26210877299308777 + 100.0 * 6.253225803375244
Epoch 1150, val loss: 1.0338441133499146
Epoch 1160, training loss: 625.456787109375 = 0.2555122971534729 + 100.0 * 6.252012729644775
Epoch 1160, val loss: 1.0373677015304565
Epoch 1170, training loss: 626.0570678710938 = 0.24912697076797485 + 100.0 * 6.2580790519714355
Epoch 1170, val loss: 1.0406053066253662
Epoch 1180, training loss: 625.572998046875 = 0.24282985925674438 + 100.0 * 6.253301620483398
Epoch 1180, val loss: 1.0441608428955078
Epoch 1190, training loss: 625.3162231445312 = 0.2366914004087448 + 100.0 * 6.250795364379883
Epoch 1190, val loss: 1.0477018356323242
Epoch 1200, training loss: 625.2326049804688 = 0.23074638843536377 + 100.0 * 6.25001859664917
Epoch 1200, val loss: 1.0515483617782593
Epoch 1210, training loss: 625.5936889648438 = 0.22495171427726746 + 100.0 * 6.253687381744385
Epoch 1210, val loss: 1.055219292640686
Epoch 1220, training loss: 625.2843627929688 = 0.21931733191013336 + 100.0 * 6.250650405883789
Epoch 1220, val loss: 1.0592577457427979
Epoch 1230, training loss: 625.6478271484375 = 0.21379640698432922 + 100.0 * 6.254340171813965
Epoch 1230, val loss: 1.0630793571472168
Epoch 1240, training loss: 625.2000732421875 = 0.20833715796470642 + 100.0 * 6.249917507171631
Epoch 1240, val loss: 1.0667487382888794
Epoch 1250, training loss: 624.9263916015625 = 0.2030440866947174 + 100.0 * 6.2472333908081055
Epoch 1250, val loss: 1.0703920125961304
Epoch 1260, training loss: 624.8695678710938 = 0.1979760378599167 + 100.0 * 6.246715545654297
Epoch 1260, val loss: 1.0748803615570068
Epoch 1270, training loss: 625.0145874023438 = 0.19301606714725494 + 100.0 * 6.248215675354004
Epoch 1270, val loss: 1.0787594318389893
Epoch 1280, training loss: 624.9427490234375 = 0.18817545473575592 + 100.0 * 6.2475457191467285
Epoch 1280, val loss: 1.083069920539856
Epoch 1290, training loss: 624.8890380859375 = 0.18347154557704926 + 100.0 * 6.247055530548096
Epoch 1290, val loss: 1.0872904062271118
Epoch 1300, training loss: 625.120361328125 = 0.1788795292377472 + 100.0 * 6.249414443969727
Epoch 1300, val loss: 1.0915836095809937
Epoch 1310, training loss: 624.7332763671875 = 0.17440107464790344 + 100.0 * 6.245588779449463
Epoch 1310, val loss: 1.0957419872283936
Epoch 1320, training loss: 624.5802001953125 = 0.17002840340137482 + 100.0 * 6.244101524353027
Epoch 1320, val loss: 1.1001240015029907
Epoch 1330, training loss: 624.5333251953125 = 0.16582900285720825 + 100.0 * 6.2436747550964355
Epoch 1330, val loss: 1.1046241521835327
Epoch 1340, training loss: 624.7103881835938 = 0.1617514044046402 + 100.0 * 6.245486259460449
Epoch 1340, val loss: 1.1090426445007324
Epoch 1350, training loss: 624.5214233398438 = 0.15774470567703247 + 100.0 * 6.2436370849609375
Epoch 1350, val loss: 1.113592267036438
Epoch 1360, training loss: 624.8439331054688 = 0.15385615825653076 + 100.0 * 6.24690055847168
Epoch 1360, val loss: 1.1177855730056763
Epoch 1370, training loss: 624.6838989257812 = 0.15008290112018585 + 100.0 * 6.245337963104248
Epoch 1370, val loss: 1.1226006746292114
Epoch 1380, training loss: 624.7113037109375 = 0.1463846117258072 + 100.0 * 6.245649337768555
Epoch 1380, val loss: 1.1270865201950073
Epoch 1390, training loss: 624.3630981445312 = 0.14277370274066925 + 100.0 * 6.242203235626221
Epoch 1390, val loss: 1.1317781209945679
Epoch 1400, training loss: 624.236083984375 = 0.13929340243339539 + 100.0 * 6.240967750549316
Epoch 1400, val loss: 1.1363836526870728
Epoch 1410, training loss: 624.2682495117188 = 0.13594865798950195 + 100.0 * 6.241323471069336
Epoch 1410, val loss: 1.1413366794586182
Epoch 1420, training loss: 624.5545043945312 = 0.1326809823513031 + 100.0 * 6.244217872619629
Epoch 1420, val loss: 1.1459647417068481
Epoch 1430, training loss: 624.1787719726562 = 0.12943974137306213 + 100.0 * 6.240493297576904
Epoch 1430, val loss: 1.1505399942398071
Epoch 1440, training loss: 624.0643920898438 = 0.12633948028087616 + 100.0 * 6.239380359649658
Epoch 1440, val loss: 1.1553261280059814
Epoch 1450, training loss: 624.5220947265625 = 0.12333816289901733 + 100.0 * 6.243987560272217
Epoch 1450, val loss: 1.159963846206665
Epoch 1460, training loss: 624.142333984375 = 0.12038761377334595 + 100.0 * 6.240219593048096
Epoch 1460, val loss: 1.1654691696166992
Epoch 1470, training loss: 624.2306518554688 = 0.11751445382833481 + 100.0 * 6.24113130569458
Epoch 1470, val loss: 1.1698943376541138
Epoch 1480, training loss: 623.9374389648438 = 0.11471341550350189 + 100.0 * 6.238227844238281
Epoch 1480, val loss: 1.1747686862945557
Epoch 1490, training loss: 623.8824462890625 = 0.11200844496488571 + 100.0 * 6.237704753875732
Epoch 1490, val loss: 1.1794602870941162
Epoch 1500, training loss: 624.0346069335938 = 0.10937637090682983 + 100.0 * 6.23925256729126
Epoch 1500, val loss: 1.1846468448638916
Epoch 1510, training loss: 623.8908081054688 = 0.10681279748678207 + 100.0 * 6.237839698791504
Epoch 1510, val loss: 1.189540982246399
Epoch 1520, training loss: 624.2891845703125 = 0.10431778430938721 + 100.0 * 6.241848468780518
Epoch 1520, val loss: 1.1942607164382935
Epoch 1530, training loss: 624.4222412109375 = 0.10185497999191284 + 100.0 * 6.243203639984131
Epoch 1530, val loss: 1.1990771293640137
Epoch 1540, training loss: 623.9268798828125 = 0.09948472678661346 + 100.0 * 6.238274097442627
Epoch 1540, val loss: 1.2042405605316162
Epoch 1550, training loss: 623.6793212890625 = 0.09716197848320007 + 100.0 * 6.235821723937988
Epoch 1550, val loss: 1.2097591161727905
Epoch 1560, training loss: 623.5279541015625 = 0.09493473917245865 + 100.0 * 6.234330177307129
Epoch 1560, val loss: 1.2146222591400146
Epoch 1570, training loss: 623.5196533203125 = 0.09276662766933441 + 100.0 * 6.234268665313721
Epoch 1570, val loss: 1.2198400497436523
Epoch 1580, training loss: 624.4134521484375 = 0.09067311882972717 + 100.0 * 6.243227481842041
Epoch 1580, val loss: 1.2244257926940918
Epoch 1590, training loss: 624.1435546875 = 0.08858688920736313 + 100.0 * 6.2405500411987305
Epoch 1590, val loss: 1.2302621603012085
Epoch 1600, training loss: 623.7384033203125 = 0.08653463423252106 + 100.0 * 6.236518383026123
Epoch 1600, val loss: 1.2346442937850952
Epoch 1610, training loss: 623.5067749023438 = 0.08457863330841064 + 100.0 * 6.234221935272217
Epoch 1610, val loss: 1.2403010129928589
Epoch 1620, training loss: 623.7202758789062 = 0.08268088102340698 + 100.0 * 6.23637580871582
Epoch 1620, val loss: 1.245612621307373
Epoch 1630, training loss: 623.4710693359375 = 0.08080478012561798 + 100.0 * 6.233902454376221
Epoch 1630, val loss: 1.2504149675369263
Epoch 1640, training loss: 623.3927001953125 = 0.07897744327783585 + 100.0 * 6.233137130737305
Epoch 1640, val loss: 1.2551758289337158
Epoch 1650, training loss: 623.2389526367188 = 0.0772269070148468 + 100.0 * 6.231617450714111
Epoch 1650, val loss: 1.2604748010635376
Epoch 1660, training loss: 623.6932983398438 = 0.07552467286586761 + 100.0 * 6.236177921295166
Epoch 1660, val loss: 1.2658007144927979
Epoch 1670, training loss: 623.407470703125 = 0.07383900135755539 + 100.0 * 6.233336448669434
Epoch 1670, val loss: 1.2703195810317993
Epoch 1680, training loss: 623.1675415039062 = 0.07220891863107681 + 100.0 * 6.230953216552734
Epoch 1680, val loss: 1.275788426399231
Epoch 1690, training loss: 623.133056640625 = 0.0706329420208931 + 100.0 * 6.230624198913574
Epoch 1690, val loss: 1.2808014154434204
Epoch 1700, training loss: 623.7555541992188 = 0.06911451369524002 + 100.0 * 6.2368645668029785
Epoch 1700, val loss: 1.2860862016677856
Epoch 1710, training loss: 623.2080688476562 = 0.06758752465248108 + 100.0 * 6.231404781341553
Epoch 1710, val loss: 1.2904108762741089
Epoch 1720, training loss: 623.0841064453125 = 0.06613468378782272 + 100.0 * 6.230179309844971
Epoch 1720, val loss: 1.2957985401153564
Epoch 1730, training loss: 623.8441162109375 = 0.06472081691026688 + 100.0 * 6.237794399261475
Epoch 1730, val loss: 1.2999976873397827
Epoch 1740, training loss: 623.5171508789062 = 0.06334421783685684 + 100.0 * 6.2345380783081055
Epoch 1740, val loss: 1.3056504726409912
Epoch 1750, training loss: 623.0482177734375 = 0.06197084113955498 + 100.0 * 6.229862689971924
Epoch 1750, val loss: 1.3103456497192383
Epoch 1760, training loss: 622.9008178710938 = 0.060671985149383545 + 100.0 * 6.2284016609191895
Epoch 1760, val loss: 1.3154250383377075
Epoch 1770, training loss: 622.88720703125 = 0.0594075582921505 + 100.0 * 6.228278160095215
Epoch 1770, val loss: 1.3203984498977661
Epoch 1780, training loss: 623.5760498046875 = 0.0582062304019928 + 100.0 * 6.235178470611572
Epoch 1780, val loss: 1.3254836797714233
Epoch 1790, training loss: 623.6212158203125 = 0.056970544159412384 + 100.0 * 6.235642910003662
Epoch 1790, val loss: 1.329948902130127
Epoch 1800, training loss: 623.0713500976562 = 0.0557587631046772 + 100.0 * 6.2301554679870605
Epoch 1800, val loss: 1.3344366550445557
Epoch 1810, training loss: 622.8535766601562 = 0.05461059510707855 + 100.0 * 6.227989196777344
Epoch 1810, val loss: 1.3395297527313232
Epoch 1820, training loss: 622.7286376953125 = 0.05350302904844284 + 100.0 * 6.226751327514648
Epoch 1820, val loss: 1.3443536758422852
Epoch 1830, training loss: 622.7144775390625 = 0.05242922529578209 + 100.0 * 6.226620197296143
Epoch 1830, val loss: 1.3492687940597534
Epoch 1840, training loss: 623.8108520507812 = 0.051400281488895416 + 100.0 * 6.2375946044921875
Epoch 1840, val loss: 1.353864073753357
Epoch 1850, training loss: 622.974853515625 = 0.05033639073371887 + 100.0 * 6.229245185852051
Epoch 1850, val loss: 1.358398199081421
Epoch 1860, training loss: 622.7963256835938 = 0.049328360706567764 + 100.0 * 6.2274699211120605
Epoch 1860, val loss: 1.363125205039978
Epoch 1870, training loss: 622.9686889648438 = 0.048348814249038696 + 100.0 * 6.229203701019287
Epoch 1870, val loss: 1.367527961730957
Epoch 1880, training loss: 622.7147827148438 = 0.047395188361406326 + 100.0 * 6.226673603057861
Epoch 1880, val loss: 1.372483491897583
Epoch 1890, training loss: 622.7398681640625 = 0.04647127538919449 + 100.0 * 6.226934432983398
Epoch 1890, val loss: 1.3771488666534424
Epoch 1900, training loss: 622.8684692382812 = 0.04556382820010185 + 100.0 * 6.228228569030762
Epoch 1900, val loss: 1.381682276725769
Epoch 1910, training loss: 622.8566284179688 = 0.04467075690627098 + 100.0 * 6.228119373321533
Epoch 1910, val loss: 1.3857449293136597
Epoch 1920, training loss: 622.484375 = 0.04380981624126434 + 100.0 * 6.224405765533447
Epoch 1920, val loss: 1.3911654949188232
Epoch 1930, training loss: 622.4138793945312 = 0.04296698793768883 + 100.0 * 6.2237091064453125
Epoch 1930, val loss: 1.395628809928894
Epoch 1940, training loss: 622.3975219726562 = 0.042156998068094254 + 100.0 * 6.22355318069458
Epoch 1940, val loss: 1.4001061916351318
Epoch 1950, training loss: 623.60205078125 = 0.041394464671611786 + 100.0 * 6.235606670379639
Epoch 1950, val loss: 1.4049087762832642
Epoch 1960, training loss: 622.83447265625 = 0.0405825637280941 + 100.0 * 6.227938652038574
Epoch 1960, val loss: 1.4089261293411255
Epoch 1970, training loss: 622.4896240234375 = 0.03980769217014313 + 100.0 * 6.224498271942139
Epoch 1970, val loss: 1.413214921951294
Epoch 1980, training loss: 622.3203735351562 = 0.03906448930501938 + 100.0 * 6.222813129425049
Epoch 1980, val loss: 1.417823314666748
Epoch 1990, training loss: 622.7885131835938 = 0.038368526846170425 + 100.0 * 6.227501392364502
Epoch 1990, val loss: 1.4223324060440063
Epoch 2000, training loss: 622.3216552734375 = 0.03764653950929642 + 100.0 * 6.222839832305908
Epoch 2000, val loss: 1.426556944847107
Epoch 2010, training loss: 622.2623291015625 = 0.03694821149110794 + 100.0 * 6.222254276275635
Epoch 2010, val loss: 1.4307746887207031
Epoch 2020, training loss: 622.2537841796875 = 0.036279577761888504 + 100.0 * 6.222175598144531
Epoch 2020, val loss: 1.4350825548171997
Epoch 2030, training loss: 622.7847900390625 = 0.035647060722112656 + 100.0 * 6.22749137878418
Epoch 2030, val loss: 1.4389456510543823
Epoch 2040, training loss: 622.1714477539062 = 0.03499378636479378 + 100.0 * 6.221364498138428
Epoch 2040, val loss: 1.4441615343093872
Epoch 2050, training loss: 622.1746215820312 = 0.0343770757317543 + 100.0 * 6.221402645111084
Epoch 2050, val loss: 1.4482280015945435
Epoch 2060, training loss: 622.8115844726562 = 0.033778101205825806 + 100.0 * 6.227778434753418
Epoch 2060, val loss: 1.452406644821167
Epoch 2070, training loss: 622.216796875 = 0.03317422419786453 + 100.0 * 6.221836090087891
Epoch 2070, val loss: 1.45657479763031
Epoch 2080, training loss: 622.1178588867188 = 0.03258832171559334 + 100.0 * 6.220852375030518
Epoch 2080, val loss: 1.4604218006134033
Epoch 2090, training loss: 622.0607299804688 = 0.032027944922447205 + 100.0 * 6.220286846160889
Epoch 2090, val loss: 1.4651813507080078
Epoch 2100, training loss: 622.2266235351562 = 0.031483061611652374 + 100.0 * 6.221951484680176
Epoch 2100, val loss: 1.4686833620071411
Epoch 2110, training loss: 622.3984985351562 = 0.03094787336885929 + 100.0 * 6.22367525100708
Epoch 2110, val loss: 1.4730510711669922
Epoch 2120, training loss: 622.2211303710938 = 0.030420320108532906 + 100.0 * 6.221907615661621
Epoch 2120, val loss: 1.4771287441253662
Epoch 2130, training loss: 622.0497436523438 = 0.029900062829256058 + 100.0 * 6.220198154449463
Epoch 2130, val loss: 1.4814943075180054
Epoch 2140, training loss: 621.9898071289062 = 0.029401397332549095 + 100.0 * 6.219604015350342
Epoch 2140, val loss: 1.4852396249771118
Epoch 2150, training loss: 622.2664184570312 = 0.02892625704407692 + 100.0 * 6.22237491607666
Epoch 2150, val loss: 1.4894310235977173
Epoch 2160, training loss: 622.1287841796875 = 0.02844034507870674 + 100.0 * 6.221003532409668
Epoch 2160, val loss: 1.4931586980819702
Epoch 2170, training loss: 622.3336791992188 = 0.027964400127530098 + 100.0 * 6.223057270050049
Epoch 2170, val loss: 1.4966531991958618
Epoch 2180, training loss: 622.6207275390625 = 0.027518698945641518 + 100.0 * 6.2259321212768555
Epoch 2180, val loss: 1.500735878944397
Epoch 2190, training loss: 621.98876953125 = 0.027054980397224426 + 100.0 * 6.2196173667907715
Epoch 2190, val loss: 1.5046436786651611
Epoch 2200, training loss: 621.9276733398438 = 0.02661915123462677 + 100.0 * 6.219010829925537
Epoch 2200, val loss: 1.5086888074874878
Epoch 2210, training loss: 621.7928466796875 = 0.02619570679962635 + 100.0 * 6.2176666259765625
Epoch 2210, val loss: 1.5126506090164185
Epoch 2220, training loss: 622.0017700195312 = 0.025788454338908195 + 100.0 * 6.219759464263916
Epoch 2220, val loss: 1.5165798664093018
Epoch 2230, training loss: 622.3041381835938 = 0.025389665737748146 + 100.0 * 6.222787380218506
Epoch 2230, val loss: 1.5204116106033325
Epoch 2240, training loss: 622.0135498046875 = 0.024976542219519615 + 100.0 * 6.21988582611084
Epoch 2240, val loss: 1.5235847234725952
Epoch 2250, training loss: 621.817138671875 = 0.024582812562584877 + 100.0 * 6.217925548553467
Epoch 2250, val loss: 1.5276484489440918
Epoch 2260, training loss: 621.7777709960938 = 0.024207986891269684 + 100.0 * 6.217535495758057
Epoch 2260, val loss: 1.531225562095642
Epoch 2270, training loss: 622.1156616210938 = 0.02384529821574688 + 100.0 * 6.220917701721191
Epoch 2270, val loss: 1.5344338417053223
Epoch 2280, training loss: 621.891845703125 = 0.023475410416722298 + 100.0 * 6.21868371963501
Epoch 2280, val loss: 1.5388339757919312
Epoch 2290, training loss: 621.7000122070312 = 0.023111393675208092 + 100.0 * 6.216768741607666
Epoch 2290, val loss: 1.5421873331069946
Epoch 2300, training loss: 621.9381713867188 = 0.022767269983887672 + 100.0 * 6.219153881072998
Epoch 2300, val loss: 1.5463435649871826
Epoch 2310, training loss: 622.05615234375 = 0.022420374676585197 + 100.0 * 6.220337390899658
Epoch 2310, val loss: 1.5491136312484741
Epoch 2320, training loss: 621.8449096679688 = 0.022078178822994232 + 100.0 * 6.218227863311768
Epoch 2320, val loss: 1.5525834560394287
Epoch 2330, training loss: 621.5857543945312 = 0.021749347448349 + 100.0 * 6.215640544891357
Epoch 2330, val loss: 1.5565508604049683
Epoch 2340, training loss: 621.568115234375 = 0.021432893350720406 + 100.0 * 6.2154669761657715
Epoch 2340, val loss: 1.5599170923233032
Epoch 2350, training loss: 621.99853515625 = 0.02112981490790844 + 100.0 * 6.21977424621582
Epoch 2350, val loss: 1.563170313835144
Epoch 2360, training loss: 621.5750122070312 = 0.020820360630750656 + 100.0 * 6.215541839599609
Epoch 2360, val loss: 1.5667725801467896
Epoch 2370, training loss: 621.6051025390625 = 0.02051861584186554 + 100.0 * 6.215846061706543
Epoch 2370, val loss: 1.5703258514404297
Epoch 2380, training loss: 621.7438354492188 = 0.0202261283993721 + 100.0 * 6.217236042022705
Epoch 2380, val loss: 1.5733699798583984
Epoch 2390, training loss: 621.9249267578125 = 0.01994171552360058 + 100.0 * 6.21904993057251
Epoch 2390, val loss: 1.5770946741104126
Epoch 2400, training loss: 621.7027587890625 = 0.01966136321425438 + 100.0 * 6.216831207275391
Epoch 2400, val loss: 1.580499529838562
Epoch 2410, training loss: 621.5435180664062 = 0.01938229613006115 + 100.0 * 6.215240955352783
Epoch 2410, val loss: 1.584134817123413
Epoch 2420, training loss: 621.4597778320312 = 0.019109509885311127 + 100.0 * 6.214406490325928
Epoch 2420, val loss: 1.5872336626052856
Epoch 2430, training loss: 621.6563720703125 = 0.018851976841688156 + 100.0 * 6.216375350952148
Epoch 2430, val loss: 1.5903009176254272
Epoch 2440, training loss: 621.8619384765625 = 0.01859704963862896 + 100.0 * 6.218433380126953
Epoch 2440, val loss: 1.5935041904449463
Epoch 2450, training loss: 621.6627807617188 = 0.01833670772612095 + 100.0 * 6.216444969177246
Epoch 2450, val loss: 1.5964319705963135
Epoch 2460, training loss: 621.5632934570312 = 0.018081150949001312 + 100.0 * 6.215452194213867
Epoch 2460, val loss: 1.5998159646987915
Epoch 2470, training loss: 621.436279296875 = 0.01783841662108898 + 100.0 * 6.214184284210205
Epoch 2470, val loss: 1.6031756401062012
Epoch 2480, training loss: 621.7337036132812 = 0.01760847307741642 + 100.0 * 6.217161178588867
Epoch 2480, val loss: 1.6066277027130127
Epoch 2490, training loss: 621.3799438476562 = 0.01736629381775856 + 100.0 * 6.213625431060791
Epoch 2490, val loss: 1.6096855401992798
Epoch 2500, training loss: 621.369140625 = 0.017128294333815575 + 100.0 * 6.213520050048828
Epoch 2500, val loss: 1.612549901008606
Epoch 2510, training loss: 621.3382568359375 = 0.016907846555113792 + 100.0 * 6.2132134437561035
Epoch 2510, val loss: 1.6160237789154053
Epoch 2520, training loss: 621.918212890625 = 0.016695788130164146 + 100.0 * 6.219015121459961
Epoch 2520, val loss: 1.6188939809799194
Epoch 2530, training loss: 621.6597900390625 = 0.016475124284625053 + 100.0 * 6.216433048248291
Epoch 2530, val loss: 1.6209012269973755
Epoch 2540, training loss: 621.4778442382812 = 0.01625477336347103 + 100.0 * 6.214616298675537
Epoch 2540, val loss: 1.6245535612106323
Epoch 2550, training loss: 621.2354736328125 = 0.016044137999415398 + 100.0 * 6.212193965911865
Epoch 2550, val loss: 1.6275763511657715
Epoch 2560, training loss: 621.2034912109375 = 0.015841754153370857 + 100.0 * 6.211876392364502
Epoch 2560, val loss: 1.630836844444275
Epoch 2570, training loss: 621.42236328125 = 0.015649763867259026 + 100.0 * 6.214067459106445
Epoch 2570, val loss: 1.6338685750961304
Epoch 2580, training loss: 621.3905029296875 = 0.015456624329090118 + 100.0 * 6.21375036239624
Epoch 2580, val loss: 1.6370294094085693
Epoch 2590, training loss: 621.4547729492188 = 0.01526209432631731 + 100.0 * 6.214395046234131
Epoch 2590, val loss: 1.6398619413375854
Epoch 2600, training loss: 621.4788208007812 = 0.015075517818331718 + 100.0 * 6.214637279510498
Epoch 2600, val loss: 1.6422035694122314
Epoch 2610, training loss: 621.2628784179688 = 0.014881701208651066 + 100.0 * 6.212479591369629
Epoch 2610, val loss: 1.6452751159667969
Epoch 2620, training loss: 621.44580078125 = 0.014706422574818134 + 100.0 * 6.214311122894287
Epoch 2620, val loss: 1.6483252048492432
Epoch 2630, training loss: 621.214111328125 = 0.01452193595468998 + 100.0 * 6.211996078491211
Epoch 2630, val loss: 1.6511310338974
Epoch 2640, training loss: 621.1615600585938 = 0.014344771392643452 + 100.0 * 6.211472034454346
Epoch 2640, val loss: 1.6536999940872192
Epoch 2650, training loss: 621.6654663085938 = 0.014178085140883923 + 100.0 * 6.216513156890869
Epoch 2650, val loss: 1.6566318273544312
Epoch 2660, training loss: 621.3053588867188 = 0.014002115465700626 + 100.0 * 6.212913513183594
Epoch 2660, val loss: 1.6582205295562744
Epoch 2670, training loss: 621.0172729492188 = 0.01383231207728386 + 100.0 * 6.210034370422363
Epoch 2670, val loss: 1.661888837814331
Epoch 2680, training loss: 620.9700927734375 = 0.013671210035681725 + 100.0 * 6.209564208984375
Epoch 2680, val loss: 1.6646348237991333
Epoch 2690, training loss: 621.2030639648438 = 0.013517161831259727 + 100.0 * 6.211895942687988
Epoch 2690, val loss: 1.6671026945114136
Epoch 2700, training loss: 621.3897094726562 = 0.013361111283302307 + 100.0 * 6.213763236999512
Epoch 2700, val loss: 1.669921636581421
Epoch 2710, training loss: 621.5369262695312 = 0.013205749914050102 + 100.0 * 6.215237617492676
Epoch 2710, val loss: 1.6717346906661987
Epoch 2720, training loss: 621.06884765625 = 0.013049808330833912 + 100.0 * 6.21055793762207
Epoch 2720, val loss: 1.674717903137207
Epoch 2730, training loss: 621.0034790039062 = 0.0129000348970294 + 100.0 * 6.209906101226807
Epoch 2730, val loss: 1.6775349378585815
Epoch 2740, training loss: 621.1466064453125 = 0.012755075469613075 + 100.0 * 6.211338520050049
Epoch 2740, val loss: 1.6800931692123413
Epoch 2750, training loss: 621.4849243164062 = 0.012615511193871498 + 100.0 * 6.214722633361816
Epoch 2750, val loss: 1.6827185153961182
Epoch 2760, training loss: 621.07275390625 = 0.012469311244785786 + 100.0 * 6.2106032371521
Epoch 2760, val loss: 1.6851288080215454
Epoch 2770, training loss: 620.9096069335938 = 0.012329360470175743 + 100.0 * 6.208972930908203
Epoch 2770, val loss: 1.6876541376113892
Epoch 2780, training loss: 620.8577880859375 = 0.012196351774036884 + 100.0 * 6.208455562591553
Epoch 2780, val loss: 1.6902830600738525
Epoch 2790, training loss: 621.818115234375 = 0.01207255944609642 + 100.0 * 6.218060493469238
Epoch 2790, val loss: 1.6920074224472046
Epoch 2800, training loss: 621.1558837890625 = 0.011932581663131714 + 100.0 * 6.21143913269043
Epoch 2800, val loss: 1.6951824426651
Epoch 2810, training loss: 620.951171875 = 0.011798291467130184 + 100.0 * 6.2093939781188965
Epoch 2810, val loss: 1.6972758769989014
Epoch 2820, training loss: 620.83203125 = 0.011671740561723709 + 100.0 * 6.2082037925720215
Epoch 2820, val loss: 1.7002058029174805
Epoch 2830, training loss: 620.8829345703125 = 0.01155083253979683 + 100.0 * 6.208714008331299
Epoch 2830, val loss: 1.7027382850646973
Epoch 2840, training loss: 621.2523803710938 = 0.01143280602991581 + 100.0 * 6.212409973144531
Epoch 2840, val loss: 1.7051717042922974
Epoch 2850, training loss: 621.1945190429688 = 0.011309143155813217 + 100.0 * 6.211832523345947
Epoch 2850, val loss: 1.7061909437179565
Epoch 2860, training loss: 620.8958740234375 = 0.01118689775466919 + 100.0 * 6.2088470458984375
Epoch 2860, val loss: 1.7092989683151245
Epoch 2870, training loss: 620.8121337890625 = 0.011071059852838516 + 100.0 * 6.208010673522949
Epoch 2870, val loss: 1.711743950843811
Epoch 2880, training loss: 620.8535766601562 = 0.010957677848637104 + 100.0 * 6.208425998687744
Epoch 2880, val loss: 1.7138445377349854
Epoch 2890, training loss: 621.6458129882812 = 0.01085012685507536 + 100.0 * 6.2163496017456055
Epoch 2890, val loss: 1.7157843112945557
Epoch 2900, training loss: 621.1959228515625 = 0.010730703361332417 + 100.0 * 6.211852073669434
Epoch 2900, val loss: 1.718106985092163
Epoch 2910, training loss: 620.7920532226562 = 0.010620961897075176 + 100.0 * 6.2078142166137695
Epoch 2910, val loss: 1.7205986976623535
Epoch 2920, training loss: 620.65625 = 0.010511945933103561 + 100.0 * 6.206457614898682
Epoch 2920, val loss: 1.7226094007492065
Epoch 2930, training loss: 620.7344360351562 = 0.010411910712718964 + 100.0 * 6.207240104675293
Epoch 2930, val loss: 1.7250438928604126
Epoch 2940, training loss: 621.38427734375 = 0.010316046886146069 + 100.0 * 6.213739395141602
Epoch 2940, val loss: 1.7270609140396118
Epoch 2950, training loss: 621.0103149414062 = 0.010208277963101864 + 100.0 * 6.210000991821289
Epoch 2950, val loss: 1.7293559312820435
Epoch 2960, training loss: 620.8815307617188 = 0.010102005675435066 + 100.0 * 6.208714008331299
Epoch 2960, val loss: 1.7309941053390503
Epoch 2970, training loss: 620.6326293945312 = 0.010003178380429745 + 100.0 * 6.206226348876953
Epoch 2970, val loss: 1.732945203781128
Epoch 2980, training loss: 620.5765991210938 = 0.009906746447086334 + 100.0 * 6.205667018890381
Epoch 2980, val loss: 1.735900640487671
Epoch 2990, training loss: 620.8377075195312 = 0.009818022139370441 + 100.0 * 6.208278656005859
Epoch 2990, val loss: 1.7378054857254028
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8118081180811808
The final CL Acc:0.67407, 0.03435, The final GNN Acc:0.80812, 0.00352
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13108])
remove edge: torch.Size([2, 7898])
updated graph: torch.Size([2, 10450])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6307983398438 = 1.9452576637268066 + 100.0 * 8.596855163574219
Epoch 0, val loss: 1.941743016242981
Epoch 10, training loss: 861.5504760742188 = 1.9367890357971191 + 100.0 * 8.596137046813965
Epoch 10, val loss: 1.9337223768234253
Epoch 20, training loss: 860.9922485351562 = 1.9261229038238525 + 100.0 * 8.59066104888916
Epoch 20, val loss: 1.923338770866394
Epoch 30, training loss: 856.8945922851562 = 1.9120956659317017 + 100.0 * 8.549824714660645
Epoch 30, val loss: 1.9094661474227905
Epoch 40, training loss: 830.51611328125 = 1.894335150718689 + 100.0 * 8.28621768951416
Epoch 40, val loss: 1.8923808336257935
Epoch 50, training loss: 773.0925903320312 = 1.875350832939148 + 100.0 * 7.712172031402588
Epoch 50, val loss: 1.8751869201660156
Epoch 60, training loss: 729.78857421875 = 1.8626909255981445 + 100.0 * 7.279259204864502
Epoch 60, val loss: 1.8634953498840332
Epoch 70, training loss: 703.17822265625 = 1.8525431156158447 + 100.0 * 7.013257026672363
Epoch 70, val loss: 1.8534376621246338
Epoch 80, training loss: 689.2279663085938 = 1.8427613973617554 + 100.0 * 6.873851776123047
Epoch 80, val loss: 1.8437315225601196
Epoch 90, training loss: 680.4260864257812 = 1.8321235179901123 + 100.0 * 6.7859392166137695
Epoch 90, val loss: 1.8339147567749023
Epoch 100, training loss: 673.81005859375 = 1.8218730688095093 + 100.0 * 6.719881534576416
Epoch 100, val loss: 1.824499487876892
Epoch 110, training loss: 668.01513671875 = 1.8125969171524048 + 100.0 * 6.662024974822998
Epoch 110, val loss: 1.8157776594161987
Epoch 120, training loss: 663.4938354492188 = 1.8039753437042236 + 100.0 * 6.616899013519287
Epoch 120, val loss: 1.8075000047683716
Epoch 130, training loss: 660.1690063476562 = 1.7951300144195557 + 100.0 * 6.583738803863525
Epoch 130, val loss: 1.7990469932556152
Epoch 140, training loss: 656.9764404296875 = 1.7857038974761963 + 100.0 * 6.551907539367676
Epoch 140, val loss: 1.7902114391326904
Epoch 150, training loss: 654.7223510742188 = 1.7759580612182617 + 100.0 * 6.529464244842529
Epoch 150, val loss: 1.7810156345367432
Epoch 160, training loss: 652.9249877929688 = 1.765636682510376 + 100.0 * 6.511593818664551
Epoch 160, val loss: 1.771227478981018
Epoch 170, training loss: 651.1674194335938 = 1.7545050382614136 + 100.0 * 6.494129180908203
Epoch 170, val loss: 1.7607523202896118
Epoch 180, training loss: 649.7830810546875 = 1.7426496744155884 + 100.0 * 6.480403900146484
Epoch 180, val loss: 1.7497068643569946
Epoch 190, training loss: 648.2861328125 = 1.7298189401626587 + 100.0 * 6.4655632972717285
Epoch 190, val loss: 1.737908124923706
Epoch 200, training loss: 646.9361572265625 = 1.7160484790802002 + 100.0 * 6.452200889587402
Epoch 200, val loss: 1.7253001928329468
Epoch 210, training loss: 646.6149291992188 = 1.7011562585830688 + 100.0 * 6.4491376876831055
Epoch 210, val loss: 1.711737036705017
Epoch 220, training loss: 644.9000244140625 = 1.6848887205123901 + 100.0 * 6.4321513175964355
Epoch 220, val loss: 1.6971782445907593
Epoch 230, training loss: 643.816650390625 = 1.6675382852554321 + 100.0 * 6.4214911460876465
Epoch 230, val loss: 1.6815369129180908
Epoch 240, training loss: 642.8368530273438 = 1.6489731073379517 + 100.0 * 6.41187858581543
Epoch 240, val loss: 1.6650598049163818
Epoch 250, training loss: 642.7500610351562 = 1.629291296005249 + 100.0 * 6.411207675933838
Epoch 250, val loss: 1.6476372480392456
Epoch 260, training loss: 641.4135131835938 = 1.6085312366485596 + 100.0 * 6.398049831390381
Epoch 260, val loss: 1.6293889284133911
Epoch 270, training loss: 640.611572265625 = 1.5868842601776123 + 100.0 * 6.390246868133545
Epoch 270, val loss: 1.6105231046676636
Epoch 280, training loss: 640.3486938476562 = 1.5644550323486328 + 100.0 * 6.387842178344727
Epoch 280, val loss: 1.591088056564331
Epoch 290, training loss: 639.3035278320312 = 1.541228175163269 + 100.0 * 6.377623081207275
Epoch 290, val loss: 1.5712072849273682
Epoch 300, training loss: 638.684326171875 = 1.5175352096557617 + 100.0 * 6.371668338775635
Epoch 300, val loss: 1.5511375665664673
Epoch 310, training loss: 638.1483154296875 = 1.4934934377670288 + 100.0 * 6.366548538208008
Epoch 310, val loss: 1.5310550928115845
Epoch 320, training loss: 637.6779174804688 = 1.4691354036331177 + 100.0 * 6.362088203430176
Epoch 320, val loss: 1.5109097957611084
Epoch 330, training loss: 637.297607421875 = 1.4446260929107666 + 100.0 * 6.358529567718506
Epoch 330, val loss: 1.4909756183624268
Epoch 340, training loss: 636.7547607421875 = 1.4201105833053589 + 100.0 * 6.353346824645996
Epoch 340, val loss: 1.4713155031204224
Epoch 350, training loss: 636.2786865234375 = 1.395719051361084 + 100.0 * 6.348830223083496
Epoch 350, val loss: 1.4519765377044678
Epoch 360, training loss: 636.058837890625 = 1.3714467287063599 + 100.0 * 6.346873760223389
Epoch 360, val loss: 1.4329897165298462
Epoch 370, training loss: 635.7614135742188 = 1.347307801246643 + 100.0 * 6.344141483306885
Epoch 370, val loss: 1.4143809080123901
Epoch 380, training loss: 635.1993408203125 = 1.3234412670135498 + 100.0 * 6.338758945465088
Epoch 380, val loss: 1.3964096307754517
Epoch 390, training loss: 634.8170776367188 = 1.2998253107070923 + 100.0 * 6.335172653198242
Epoch 390, val loss: 1.3787479400634766
Epoch 400, training loss: 634.7823486328125 = 1.2764936685562134 + 100.0 * 6.335058689117432
Epoch 400, val loss: 1.3614983558654785
Epoch 410, training loss: 634.1754760742188 = 1.2533153295516968 + 100.0 * 6.329221725463867
Epoch 410, val loss: 1.3447315692901611
Epoch 420, training loss: 633.8314208984375 = 1.2305225133895874 + 100.0 * 6.3260087966918945
Epoch 420, val loss: 1.3284202814102173
Epoch 430, training loss: 633.8095703125 = 1.2081557512283325 + 100.0 * 6.326014518737793
Epoch 430, val loss: 1.312430500984192
Epoch 440, training loss: 633.4443359375 = 1.18611478805542 + 100.0 * 6.322582244873047
Epoch 440, val loss: 1.2973140478134155
Epoch 450, training loss: 633.249267578125 = 1.1644437313079834 + 100.0 * 6.32084846496582
Epoch 450, val loss: 1.2826303243637085
Epoch 460, training loss: 632.6906127929688 = 1.1431647539138794 + 100.0 * 6.315474510192871
Epoch 460, val loss: 1.2683422565460205
Epoch 470, training loss: 632.4805908203125 = 1.122374415397644 + 100.0 * 6.313581943511963
Epoch 470, val loss: 1.2545896768569946
Epoch 480, training loss: 632.4837036132812 = 1.1019467115402222 + 100.0 * 6.313817501068115
Epoch 480, val loss: 1.2414458990097046
Epoch 490, training loss: 632.0343627929688 = 1.081894040107727 + 100.0 * 6.3095245361328125
Epoch 490, val loss: 1.2288036346435547
Epoch 500, training loss: 631.9515991210938 = 1.0622256994247437 + 100.0 * 6.30889368057251
Epoch 500, val loss: 1.2167434692382812
Epoch 510, training loss: 631.5970458984375 = 1.043005347251892 + 100.0 * 6.305540084838867
Epoch 510, val loss: 1.2048895359039307
Epoch 520, training loss: 631.2949829101562 = 1.0240715742111206 + 100.0 * 6.302709579467773
Epoch 520, val loss: 1.1937097311019897
Epoch 530, training loss: 631.8555908203125 = 1.0054819583892822 + 100.0 * 6.308501243591309
Epoch 530, val loss: 1.1828453540802002
Epoch 540, training loss: 630.950439453125 = 0.9871190786361694 + 100.0 * 6.299633026123047
Epoch 540, val loss: 1.1724801063537598
Epoch 550, training loss: 630.72412109375 = 0.969164252281189 + 100.0 * 6.297549247741699
Epoch 550, val loss: 1.1625632047653198
Epoch 560, training loss: 630.549072265625 = 0.9514815807342529 + 100.0 * 6.295976161956787
Epoch 560, val loss: 1.1529451608657837
Epoch 570, training loss: 630.3613891601562 = 0.9340254664421082 + 100.0 * 6.294273853302002
Epoch 570, val loss: 1.1438283920288086
Epoch 580, training loss: 630.3408203125 = 0.916715145111084 + 100.0 * 6.294241428375244
Epoch 580, val loss: 1.1346139907836914
Epoch 590, training loss: 630.0213623046875 = 0.8997847437858582 + 100.0 * 6.291215896606445
Epoch 590, val loss: 1.1260876655578613
Epoch 600, training loss: 630.180908203125 = 0.8830803036689758 + 100.0 * 6.292978763580322
Epoch 600, val loss: 1.117888331413269
Epoch 610, training loss: 629.9235229492188 = 0.8665064573287964 + 100.0 * 6.29056978225708
Epoch 610, val loss: 1.1096936464309692
Epoch 620, training loss: 629.6094970703125 = 0.8501166701316833 + 100.0 * 6.287593841552734
Epoch 620, val loss: 1.1019953489303589
Epoch 630, training loss: 629.3872680664062 = 0.8340232968330383 + 100.0 * 6.285532474517822
Epoch 630, val loss: 1.0944546461105347
Epoch 640, training loss: 629.9125366210938 = 0.8181121945381165 + 100.0 * 6.2909440994262695
Epoch 640, val loss: 1.0872868299484253
Epoch 650, training loss: 629.4375610351562 = 0.8023383617401123 + 100.0 * 6.286352157592773
Epoch 650, val loss: 1.080090045928955
Epoch 660, training loss: 628.8904418945312 = 0.7867873907089233 + 100.0 * 6.281036376953125
Epoch 660, val loss: 1.0733813047409058
Epoch 670, training loss: 629.0418701171875 = 0.7715311050415039 + 100.0 * 6.282703399658203
Epoch 670, val loss: 1.0669326782226562
Epoch 680, training loss: 628.8276977539062 = 0.7564312815666199 + 100.0 * 6.280713081359863
Epoch 680, val loss: 1.0607514381408691
Epoch 690, training loss: 628.4862670898438 = 0.7415571212768555 + 100.0 * 6.277446746826172
Epoch 690, val loss: 1.0548590421676636
Epoch 700, training loss: 628.9572143554688 = 0.7269270420074463 + 100.0 * 6.2823028564453125
Epoch 700, val loss: 1.049085021018982
Epoch 710, training loss: 628.4712524414062 = 0.71250981092453 + 100.0 * 6.277587413787842
Epoch 710, val loss: 1.04361093044281
Epoch 720, training loss: 628.1134033203125 = 0.6982699632644653 + 100.0 * 6.27415132522583
Epoch 720, val loss: 1.0384056568145752
Epoch 730, training loss: 628.204833984375 = 0.6843460202217102 + 100.0 * 6.275205135345459
Epoch 730, val loss: 1.033321499824524
Epoch 740, training loss: 628.0540771484375 = 0.6705163717269897 + 100.0 * 6.2738356590271
Epoch 740, val loss: 1.028692603111267
Epoch 750, training loss: 627.98486328125 = 0.6568369269371033 + 100.0 * 6.273280143737793
Epoch 750, val loss: 1.0239262580871582
Epoch 760, training loss: 627.78515625 = 0.6434829235076904 + 100.0 * 6.271416664123535
Epoch 760, val loss: 1.0197030305862427
Epoch 770, training loss: 627.5967407226562 = 0.6304001808166504 + 100.0 * 6.2696638107299805
Epoch 770, val loss: 1.0156794786453247
Epoch 780, training loss: 627.7384033203125 = 0.6175256967544556 + 100.0 * 6.271208763122559
Epoch 780, val loss: 1.0119580030441284
Epoch 790, training loss: 627.4332275390625 = 0.6046152114868164 + 100.0 * 6.268286228179932
Epoch 790, val loss: 1.0081108808517456
Epoch 800, training loss: 627.210205078125 = 0.5920310616493225 + 100.0 * 6.266181468963623
Epoch 800, val loss: 1.004805564880371
Epoch 810, training loss: 627.3787841796875 = 0.5796471834182739 + 100.0 * 6.267991065979004
Epoch 810, val loss: 1.0018682479858398
Epoch 820, training loss: 627.1556396484375 = 0.567373514175415 + 100.0 * 6.26588249206543
Epoch 820, val loss: 0.9983900189399719
Epoch 830, training loss: 627.0374145507812 = 0.5552124381065369 + 100.0 * 6.264821529388428
Epoch 830, val loss: 0.9957760572433472
Epoch 840, training loss: 627.3539428710938 = 0.5433467626571655 + 100.0 * 6.268105983734131
Epoch 840, val loss: 0.9932252168655396
Epoch 850, training loss: 626.873046875 = 0.5315417647361755 + 100.0 * 6.2634148597717285
Epoch 850, val loss: 0.9903711676597595
Epoch 860, training loss: 626.6168212890625 = 0.5199878215789795 + 100.0 * 6.2609686851501465
Epoch 860, val loss: 0.9885448217391968
Epoch 870, training loss: 626.5810546875 = 0.5086045861244202 + 100.0 * 6.2607245445251465
Epoch 870, val loss: 0.9864031672477722
Epoch 880, training loss: 626.6008911132812 = 0.4972820281982422 + 100.0 * 6.261036396026611
Epoch 880, val loss: 0.9843841791152954
Epoch 890, training loss: 626.4758911132812 = 0.48607319593429565 + 100.0 * 6.2598981857299805
Epoch 890, val loss: 0.9826284646987915
Epoch 900, training loss: 626.5997924804688 = 0.47507351636886597 + 100.0 * 6.261247158050537
Epoch 900, val loss: 0.980941116809845
Epoch 910, training loss: 626.2334594726562 = 0.46430712938308716 + 100.0 * 6.257691383361816
Epoch 910, val loss: 0.9793694615364075
Epoch 920, training loss: 626.0519409179688 = 0.4537053406238556 + 100.0 * 6.255981922149658
Epoch 920, val loss: 0.9784045219421387
Epoch 930, training loss: 626.3116455078125 = 0.4433569312095642 + 100.0 * 6.258682727813721
Epoch 930, val loss: 0.9774748682975769
Epoch 940, training loss: 626.2113037109375 = 0.43299996852874756 + 100.0 * 6.25778341293335
Epoch 940, val loss: 0.9763576984405518
Epoch 950, training loss: 626.5205078125 = 0.42277100682258606 + 100.0 * 6.260977268218994
Epoch 950, val loss: 0.9754922389984131
Epoch 960, training loss: 625.8325805664062 = 0.41284310817718506 + 100.0 * 6.254197597503662
Epoch 960, val loss: 0.9750697612762451
Epoch 970, training loss: 625.774169921875 = 0.4030827581882477 + 100.0 * 6.253711223602295
Epoch 970, val loss: 0.9745643734931946
Epoch 980, training loss: 625.7739868164062 = 0.39355799555778503 + 100.0 * 6.2538042068481445
Epoch 980, val loss: 0.9745610952377319
Epoch 990, training loss: 625.5735473632812 = 0.3841742277145386 + 100.0 * 6.251893997192383
Epoch 990, val loss: 0.9749997854232788
Epoch 1000, training loss: 625.4603881835938 = 0.3750538229942322 + 100.0 * 6.250853061676025
Epoch 1000, val loss: 0.9752802848815918
Epoch 1010, training loss: 625.708740234375 = 0.36611783504486084 + 100.0 * 6.253426551818848
Epoch 1010, val loss: 0.975676953792572
Epoch 1020, training loss: 625.4291381835938 = 0.3572863042354584 + 100.0 * 6.250718116760254
Epoch 1020, val loss: 0.976386308670044
Epoch 1030, training loss: 625.3370971679688 = 0.3486608862876892 + 100.0 * 6.249884605407715
Epoch 1030, val loss: 0.9774470329284668
Epoch 1040, training loss: 625.4867553710938 = 0.34027397632598877 + 100.0 * 6.25146484375
Epoch 1040, val loss: 0.9783745408058167
Epoch 1050, training loss: 625.2330932617188 = 0.3319880962371826 + 100.0 * 6.249011039733887
Epoch 1050, val loss: 0.9799471497535706
Epoch 1060, training loss: 625.4923095703125 = 0.3239403963088989 + 100.0 * 6.251684188842773
Epoch 1060, val loss: 0.9811381101608276
Epoch 1070, training loss: 625.0934448242188 = 0.31613072752952576 + 100.0 * 6.247773170471191
Epoch 1070, val loss: 0.9831497669219971
Epoch 1080, training loss: 625.1343383789062 = 0.3084314167499542 + 100.0 * 6.248259544372559
Epoch 1080, val loss: 0.9849324822425842
Epoch 1090, training loss: 624.7796630859375 = 0.30098557472229004 + 100.0 * 6.244787216186523
Epoch 1090, val loss: 0.987189531326294
Epoch 1100, training loss: 624.7871704101562 = 0.2937355935573578 + 100.0 * 6.24493408203125
Epoch 1100, val loss: 0.9898101687431335
Epoch 1110, training loss: 625.3273315429688 = 0.2866813540458679 + 100.0 * 6.250406742095947
Epoch 1110, val loss: 0.9919980764389038
Epoch 1120, training loss: 625.0611572265625 = 0.2796860337257385 + 100.0 * 6.247814655303955
Epoch 1120, val loss: 0.9945155382156372
Epoch 1130, training loss: 624.6611938476562 = 0.2728547751903534 + 100.0 * 6.2438836097717285
Epoch 1130, val loss: 0.997456431388855
Epoch 1140, training loss: 624.4940795898438 = 0.26629918813705444 + 100.0 * 6.242278099060059
Epoch 1140, val loss: 1.0004653930664062
Epoch 1150, training loss: 624.6527709960938 = 0.2599417567253113 + 100.0 * 6.2439284324646
Epoch 1150, val loss: 1.0040717124938965
Epoch 1160, training loss: 624.6519165039062 = 0.25372183322906494 + 100.0 * 6.243981838226318
Epoch 1160, val loss: 1.0069539546966553
Epoch 1170, training loss: 624.5609130859375 = 0.2475733458995819 + 100.0 * 6.243133544921875
Epoch 1170, val loss: 1.0104446411132812
Epoch 1180, training loss: 624.3438720703125 = 0.24157872796058655 + 100.0 * 6.241023063659668
Epoch 1180, val loss: 1.0135377645492554
Epoch 1190, training loss: 624.1831665039062 = 0.23583070933818817 + 100.0 * 6.239473342895508
Epoch 1190, val loss: 1.0177373886108398
Epoch 1200, training loss: 624.3436889648438 = 0.23025986552238464 + 100.0 * 6.241134166717529
Epoch 1200, val loss: 1.0215256214141846
Epoch 1210, training loss: 624.507080078125 = 0.22475579380989075 + 100.0 * 6.242823600769043
Epoch 1210, val loss: 1.0252307653427124
Epoch 1220, training loss: 624.1031494140625 = 0.21937869489192963 + 100.0 * 6.238837718963623
Epoch 1220, val loss: 1.0299521684646606
Epoch 1230, training loss: 624.2206420898438 = 0.21414072811603546 + 100.0 * 6.240065574645996
Epoch 1230, val loss: 1.0337989330291748
Epoch 1240, training loss: 623.962646484375 = 0.20907340943813324 + 100.0 * 6.2375359535217285
Epoch 1240, val loss: 1.0380916595458984
Epoch 1250, training loss: 623.9480590820312 = 0.2041720449924469 + 100.0 * 6.237438678741455
Epoch 1250, val loss: 1.0424096584320068
Epoch 1260, training loss: 624.4108276367188 = 0.19936512410640717 + 100.0 * 6.242115020751953
Epoch 1260, val loss: 1.0467379093170166
Epoch 1270, training loss: 624.0811767578125 = 0.19466163218021393 + 100.0 * 6.238865375518799
Epoch 1270, val loss: 1.0519987344741821
Epoch 1280, training loss: 623.9190673828125 = 0.1900576502084732 + 100.0 * 6.237289905548096
Epoch 1280, val loss: 1.0561599731445312
Epoch 1290, training loss: 623.8761596679688 = 0.18562373518943787 + 100.0 * 6.236905574798584
Epoch 1290, val loss: 1.0608928203582764
Epoch 1300, training loss: 623.9877319335938 = 0.18129393458366394 + 100.0 * 6.238064765930176
Epoch 1300, val loss: 1.0653598308563232
Epoch 1310, training loss: 623.7858276367188 = 0.177098348736763 + 100.0 * 6.236087322235107
Epoch 1310, val loss: 1.0709526538848877
Epoch 1320, training loss: 623.66357421875 = 0.17298077046871185 + 100.0 * 6.23490571975708
Epoch 1320, val loss: 1.0753417015075684
Epoch 1330, training loss: 623.7271118164062 = 0.1689886599779129 + 100.0 * 6.235580921173096
Epoch 1330, val loss: 1.080704689025879
Epoch 1340, training loss: 623.7470092773438 = 0.16507527232170105 + 100.0 * 6.235818862915039
Epoch 1340, val loss: 1.0862958431243896
Epoch 1350, training loss: 623.648193359375 = 0.1612308919429779 + 100.0 * 6.234869480133057
Epoch 1350, val loss: 1.0911580324172974
Epoch 1360, training loss: 623.5798950195312 = 0.15751856565475464 + 100.0 * 6.234223365783691
Epoch 1360, val loss: 1.0966320037841797
Epoch 1370, training loss: 623.4799194335938 = 0.15389366447925568 + 100.0 * 6.233260631561279
Epoch 1370, val loss: 1.1016054153442383
Epoch 1380, training loss: 623.8187255859375 = 0.15037761628627777 + 100.0 * 6.236683368682861
Epoch 1380, val loss: 1.1069890260696411
Epoch 1390, training loss: 623.5270385742188 = 0.1469445377588272 + 100.0 * 6.233800888061523
Epoch 1390, val loss: 1.1125527620315552
Epoch 1400, training loss: 623.44287109375 = 0.14355623722076416 + 100.0 * 6.232993125915527
Epoch 1400, val loss: 1.117576241493225
Epoch 1410, training loss: 623.421142578125 = 0.14031195640563965 + 100.0 * 6.2328081130981445
Epoch 1410, val loss: 1.1234099864959717
Epoch 1420, training loss: 623.2555541992188 = 0.1371338963508606 + 100.0 * 6.231184005737305
Epoch 1420, val loss: 1.1288158893585205
Epoch 1430, training loss: 623.4253540039062 = 0.1340598165988922 + 100.0 * 6.232913494110107
Epoch 1430, val loss: 1.1344873905181885
Epoch 1440, training loss: 623.2286987304688 = 0.1310655027627945 + 100.0 * 6.230976581573486
Epoch 1440, val loss: 1.1397404670715332
Epoch 1450, training loss: 623.2915649414062 = 0.12814895808696747 + 100.0 * 6.231634140014648
Epoch 1450, val loss: 1.1449599266052246
Epoch 1460, training loss: 623.2481689453125 = 0.12528277933597565 + 100.0 * 6.231228828430176
Epoch 1460, val loss: 1.1504969596862793
Epoch 1470, training loss: 623.1495361328125 = 0.1225036084651947 + 100.0 * 6.2302703857421875
Epoch 1470, val loss: 1.1564595699310303
Epoch 1480, training loss: 623.4419555664062 = 0.11981930583715439 + 100.0 * 6.233221530914307
Epoch 1480, val loss: 1.1613301038742065
Epoch 1490, training loss: 622.9764404296875 = 0.11711841076612473 + 100.0 * 6.228593349456787
Epoch 1490, val loss: 1.1669394969940186
Epoch 1500, training loss: 622.9044799804688 = 0.11453565955162048 + 100.0 * 6.227899074554443
Epoch 1500, val loss: 1.1726254224777222
Epoch 1510, training loss: 622.8829956054688 = 0.11204509437084198 + 100.0 * 6.2277092933654785
Epoch 1510, val loss: 1.1782842874526978
Epoch 1520, training loss: 623.3719482421875 = 0.10960165411233902 + 100.0 * 6.232623100280762
Epoch 1520, val loss: 1.1836457252502441
Epoch 1530, training loss: 623.1970825195312 = 0.10721848905086517 + 100.0 * 6.230898380279541
Epoch 1530, val loss: 1.189447283744812
Epoch 1540, training loss: 623.3458251953125 = 0.1048731803894043 + 100.0 * 6.232409477233887
Epoch 1540, val loss: 1.1952764987945557
Epoch 1550, training loss: 622.81396484375 = 0.10255979746580124 + 100.0 * 6.227114200592041
Epoch 1550, val loss: 1.1999390125274658
Epoch 1560, training loss: 622.6993408203125 = 0.10035727918148041 + 100.0 * 6.225989818572998
Epoch 1560, val loss: 1.205978512763977
Epoch 1570, training loss: 622.6963500976562 = 0.09822102636098862 + 100.0 * 6.22598123550415
Epoch 1570, val loss: 1.2113142013549805
Epoch 1580, training loss: 623.294189453125 = 0.09614190459251404 + 100.0 * 6.231980323791504
Epoch 1580, val loss: 1.2173738479614258
Epoch 1590, training loss: 622.9269409179688 = 0.09407730400562286 + 100.0 * 6.228328704833984
Epoch 1590, val loss: 1.2226704359054565
Epoch 1600, training loss: 622.673828125 = 0.09203734248876572 + 100.0 * 6.225818157196045
Epoch 1600, val loss: 1.2279760837554932
Epoch 1610, training loss: 622.7946166992188 = 0.09011338651180267 + 100.0 * 6.22704553604126
Epoch 1610, val loss: 1.2338746786117554
Epoch 1620, training loss: 622.8259887695312 = 0.08820776641368866 + 100.0 * 6.227377891540527
Epoch 1620, val loss: 1.238990068435669
Epoch 1630, training loss: 622.6768188476562 = 0.08636047691106796 + 100.0 * 6.22590446472168
Epoch 1630, val loss: 1.2441619634628296
Epoch 1640, training loss: 622.55322265625 = 0.08453565090894699 + 100.0 * 6.224687099456787
Epoch 1640, val loss: 1.2496337890625
Epoch 1650, training loss: 622.434814453125 = 0.08278457075357437 + 100.0 * 6.223520278930664
Epoch 1650, val loss: 1.2557826042175293
Epoch 1660, training loss: 622.3800659179688 = 0.08109457045793533 + 100.0 * 6.222989559173584
Epoch 1660, val loss: 1.2610799074172974
Epoch 1670, training loss: 622.718994140625 = 0.07945863902568817 + 100.0 * 6.226395130157471
Epoch 1670, val loss: 1.2662086486816406
Epoch 1680, training loss: 622.872802734375 = 0.07781495898962021 + 100.0 * 6.227950096130371
Epoch 1680, val loss: 1.2712582349777222
Epoch 1690, training loss: 622.6053466796875 = 0.07618175446987152 + 100.0 * 6.225291728973389
Epoch 1690, val loss: 1.2768217325210571
Epoch 1700, training loss: 622.6260986328125 = 0.07460534572601318 + 100.0 * 6.225515365600586
Epoch 1700, val loss: 1.2825134992599487
Epoch 1710, training loss: 622.2907104492188 = 0.07306099683046341 + 100.0 * 6.222176551818848
Epoch 1710, val loss: 1.287232518196106
Epoch 1720, training loss: 622.2159423828125 = 0.0716022402048111 + 100.0 * 6.2214436531066895
Epoch 1720, val loss: 1.292568564414978
Epoch 1730, training loss: 622.22216796875 = 0.07018429040908813 + 100.0 * 6.221519470214844
Epoch 1730, val loss: 1.2978103160858154
Epoch 1740, training loss: 622.6605834960938 = 0.06879009306430817 + 100.0 * 6.225917816162109
Epoch 1740, val loss: 1.3029916286468506
Epoch 1750, training loss: 622.5155639648438 = 0.06739925593137741 + 100.0 * 6.224481582641602
Epoch 1750, val loss: 1.3085699081420898
Epoch 1760, training loss: 622.2860107421875 = 0.06604452431201935 + 100.0 * 6.222199440002441
Epoch 1760, val loss: 1.3133010864257812
Epoch 1770, training loss: 622.566650390625 = 0.06475864350795746 + 100.0 * 6.2250189781188965
Epoch 1770, val loss: 1.3191710710525513
Epoch 1780, training loss: 622.2802734375 = 0.06345675140619278 + 100.0 * 6.22216796875
Epoch 1780, val loss: 1.323927402496338
Epoch 1790, training loss: 622.09228515625 = 0.06222013011574745 + 100.0 * 6.220300674438477
Epoch 1790, val loss: 1.3288129568099976
Epoch 1800, training loss: 621.9885864257812 = 0.061016570776700974 + 100.0 * 6.21927547454834
Epoch 1800, val loss: 1.3339241743087769
Epoch 1810, training loss: 621.96923828125 = 0.059857260435819626 + 100.0 * 6.2190937995910645
Epoch 1810, val loss: 1.338882565498352
Epoch 1820, training loss: 622.5254516601562 = 0.05873545631766319 + 100.0 * 6.224667072296143
Epoch 1820, val loss: 1.3432343006134033
Epoch 1830, training loss: 622.6712036132812 = 0.05758601054549217 + 100.0 * 6.226136684417725
Epoch 1830, val loss: 1.348839521408081
Epoch 1840, training loss: 622.1643676757812 = 0.05644950643181801 + 100.0 * 6.221079349517822
Epoch 1840, val loss: 1.3536640405654907
Epoch 1850, training loss: 621.9871215820312 = 0.055381350219249725 + 100.0 * 6.219317436218262
Epoch 1850, val loss: 1.3592778444290161
Epoch 1860, training loss: 622.1759033203125 = 0.05434681847691536 + 100.0 * 6.221215724945068
Epoch 1860, val loss: 1.364280104637146
Epoch 1870, training loss: 621.9691772460938 = 0.05332589149475098 + 100.0 * 6.219158172607422
Epoch 1870, val loss: 1.368554711341858
Epoch 1880, training loss: 621.8595581054688 = 0.05233835428953171 + 100.0 * 6.218071937561035
Epoch 1880, val loss: 1.3731498718261719
Epoch 1890, training loss: 621.8300170898438 = 0.051387857645750046 + 100.0 * 6.2177863121032715
Epoch 1890, val loss: 1.378565788269043
Epoch 1900, training loss: 622.735595703125 = 0.050476331263780594 + 100.0 * 6.226851463317871
Epoch 1900, val loss: 1.3827571868896484
Epoch 1910, training loss: 621.9695434570312 = 0.04947858676314354 + 100.0 * 6.219200134277344
Epoch 1910, val loss: 1.3882832527160645
Epoch 1920, training loss: 621.7823486328125 = 0.048580601811409 + 100.0 * 6.217337608337402
Epoch 1920, val loss: 1.3922570943832397
Epoch 1930, training loss: 621.69580078125 = 0.04770584776997566 + 100.0 * 6.216480731964111
Epoch 1930, val loss: 1.3978019952774048
Epoch 1940, training loss: 621.69091796875 = 0.046871062368154526 + 100.0 * 6.216440677642822
Epoch 1940, val loss: 1.4019832611083984
Epoch 1950, training loss: 623.07080078125 = 0.04604697972536087 + 100.0 * 6.230247974395752
Epoch 1950, val loss: 1.406345009803772
Epoch 1960, training loss: 622.0488891601562 = 0.045211680233478546 + 100.0 * 6.220036506652832
Epoch 1960, val loss: 1.411916732788086
Epoch 1970, training loss: 621.6600341796875 = 0.04438725858926773 + 100.0 * 6.216156482696533
Epoch 1970, val loss: 1.4160398244857788
Epoch 1980, training loss: 621.5607299804688 = 0.04362697899341583 + 100.0 * 6.215170860290527
Epoch 1980, val loss: 1.420981526374817
Epoch 1990, training loss: 621.8558959960938 = 0.042892295867204666 + 100.0 * 6.218129634857178
Epoch 1990, val loss: 1.4247750043869019
Epoch 2000, training loss: 621.5911254882812 = 0.04213989898562431 + 100.0 * 6.215489864349365
Epoch 2000, val loss: 1.4299583435058594
Epoch 2010, training loss: 621.649658203125 = 0.041388485580682755 + 100.0 * 6.216082572937012
Epoch 2010, val loss: 1.4341247081756592
Epoch 2020, training loss: 621.6691284179688 = 0.04067882150411606 + 100.0 * 6.21628475189209
Epoch 2020, val loss: 1.439041256904602
Epoch 2030, training loss: 621.48583984375 = 0.03998619690537453 + 100.0 * 6.214458465576172
Epoch 2030, val loss: 1.4434053897857666
Epoch 2040, training loss: 621.6607055664062 = 0.039317790418863297 + 100.0 * 6.216214179992676
Epoch 2040, val loss: 1.4482507705688477
Epoch 2050, training loss: 621.8716430664062 = 0.03864983096718788 + 100.0 * 6.218329906463623
Epoch 2050, val loss: 1.4526523351669312
Epoch 2060, training loss: 621.5562744140625 = 0.038001395761966705 + 100.0 * 6.215182781219482
Epoch 2060, val loss: 1.4567759037017822
Epoch 2070, training loss: 621.5103759765625 = 0.03737037628889084 + 100.0 * 6.214730262756348
Epoch 2070, val loss: 1.461142659187317
Epoch 2080, training loss: 621.6338500976562 = 0.036753736436367035 + 100.0 * 6.215970993041992
Epoch 2080, val loss: 1.4661095142364502
Epoch 2090, training loss: 621.4578857421875 = 0.036147091537714005 + 100.0 * 6.214217662811279
Epoch 2090, val loss: 1.4694253206253052
Epoch 2100, training loss: 621.4208374023438 = 0.03555625304579735 + 100.0 * 6.213852405548096
Epoch 2100, val loss: 1.4738175868988037
Epoch 2110, training loss: 621.904052734375 = 0.03498469293117523 + 100.0 * 6.218690872192383
Epoch 2110, val loss: 1.4781107902526855
Epoch 2120, training loss: 621.5176391601562 = 0.03440295532345772 + 100.0 * 6.214832305908203
Epoch 2120, val loss: 1.4818884134292603
Epoch 2130, training loss: 621.3619995117188 = 0.033837780356407166 + 100.0 * 6.213281631469727
Epoch 2130, val loss: 1.4868539571762085
Epoch 2140, training loss: 621.3994140625 = 0.03330764174461365 + 100.0 * 6.213660717010498
Epoch 2140, val loss: 1.4902763366699219
Epoch 2150, training loss: 621.5822143554688 = 0.03278796747326851 + 100.0 * 6.215494155883789
Epoch 2150, val loss: 1.4948091506958008
Epoch 2160, training loss: 621.4908447265625 = 0.0322691984474659 + 100.0 * 6.214585304260254
Epoch 2160, val loss: 1.4994570016860962
Epoch 2170, training loss: 621.3421020507812 = 0.03175367787480354 + 100.0 * 6.213103771209717
Epoch 2170, val loss: 1.5034371614456177
Epoch 2180, training loss: 621.4358520507812 = 0.03125428780913353 + 100.0 * 6.214046001434326
Epoch 2180, val loss: 1.507725715637207
Epoch 2190, training loss: 621.4732055664062 = 0.030771728605031967 + 100.0 * 6.2144246101379395
Epoch 2190, val loss: 1.511310338973999
Epoch 2200, training loss: 621.37646484375 = 0.030293941497802734 + 100.0 * 6.213461875915527
Epoch 2200, val loss: 1.5149574279785156
Epoch 2210, training loss: 621.6046142578125 = 0.029837559908628464 + 100.0 * 6.215747833251953
Epoch 2210, val loss: 1.518973708152771
Epoch 2220, training loss: 621.2069091796875 = 0.02935607172548771 + 100.0 * 6.211775302886963
Epoch 2220, val loss: 1.5237518548965454
Epoch 2230, training loss: 621.177734375 = 0.02891532890498638 + 100.0 * 6.211488246917725
Epoch 2230, val loss: 1.5271559953689575
Epoch 2240, training loss: 621.2625122070312 = 0.028490429744124413 + 100.0 * 6.212340354919434
Epoch 2240, val loss: 1.531300663948059
Epoch 2250, training loss: 621.3523559570312 = 0.02806488610804081 + 100.0 * 6.213242530822754
Epoch 2250, val loss: 1.5345765352249146
Epoch 2260, training loss: 621.4968872070312 = 0.027641214430332184 + 100.0 * 6.214692115783691
Epoch 2260, val loss: 1.5379565954208374
Epoch 2270, training loss: 621.4059448242188 = 0.027216237038373947 + 100.0 * 6.213787078857422
Epoch 2270, val loss: 1.5425362586975098
Epoch 2280, training loss: 621.0762329101562 = 0.026810986921191216 + 100.0 * 6.210494518280029
Epoch 2280, val loss: 1.5463436841964722
Epoch 2290, training loss: 621.0621948242188 = 0.02642899565398693 + 100.0 * 6.210357666015625
Epoch 2290, val loss: 1.550017237663269
Epoch 2300, training loss: 621.0511474609375 = 0.026050813496112823 + 100.0 * 6.2102508544921875
Epoch 2300, val loss: 1.554152011871338
Epoch 2310, training loss: 621.4451293945312 = 0.025687681511044502 + 100.0 * 6.214194297790527
Epoch 2310, val loss: 1.5580650568008423
Epoch 2320, training loss: 621.1825561523438 = 0.0252987053245306 + 100.0 * 6.211572647094727
Epoch 2320, val loss: 1.5610888004302979
Epoch 2330, training loss: 621.00048828125 = 0.02492706850171089 + 100.0 * 6.209755897521973
Epoch 2330, val loss: 1.5642751455307007
Epoch 2340, training loss: 620.9563598632812 = 0.024570001289248466 + 100.0 * 6.209317684173584
Epoch 2340, val loss: 1.568389654159546
Epoch 2350, training loss: 621.2313232421875 = 0.024235058575868607 + 100.0 * 6.212070941925049
Epoch 2350, val loss: 1.5716055631637573
Epoch 2360, training loss: 621.1273193359375 = 0.023889724165201187 + 100.0 * 6.211034774780273
Epoch 2360, val loss: 1.5756165981292725
Epoch 2370, training loss: 620.9772338867188 = 0.02353825606405735 + 100.0 * 6.209536552429199
Epoch 2370, val loss: 1.5790808200836182
Epoch 2380, training loss: 620.85693359375 = 0.023206200450658798 + 100.0 * 6.208337306976318
Epoch 2380, val loss: 1.5832464694976807
Epoch 2390, training loss: 620.853515625 = 0.02289552427828312 + 100.0 * 6.208306312561035
Epoch 2390, val loss: 1.58677077293396
Epoch 2400, training loss: 621.1664428710938 = 0.022597016766667366 + 100.0 * 6.2114386558532715
Epoch 2400, val loss: 1.5904008150100708
Epoch 2410, training loss: 620.9969482421875 = 0.022285528481006622 + 100.0 * 6.209746837615967
Epoch 2410, val loss: 1.5930906534194946
Epoch 2420, training loss: 621.3193969726562 = 0.02198469638824463 + 100.0 * 6.2129740715026855
Epoch 2420, val loss: 1.596387267112732
Epoch 2430, training loss: 620.8765258789062 = 0.021668437868356705 + 100.0 * 6.208548545837402
Epoch 2430, val loss: 1.600476861000061
Epoch 2440, training loss: 620.9973754882812 = 0.02138282172381878 + 100.0 * 6.2097601890563965
Epoch 2440, val loss: 1.6039577722549438
Epoch 2450, training loss: 621.2636108398438 = 0.021101612597703934 + 100.0 * 6.2124247550964355
Epoch 2450, val loss: 1.6072825193405151
Epoch 2460, training loss: 620.9495239257812 = 0.020808352157473564 + 100.0 * 6.209287166595459
Epoch 2460, val loss: 1.61038339138031
Epoch 2470, training loss: 621.1586303710938 = 0.020536983385682106 + 100.0 * 6.211381435394287
Epoch 2470, val loss: 1.614797830581665
Epoch 2480, training loss: 620.8018188476562 = 0.020263219252228737 + 100.0 * 6.207815647125244
Epoch 2480, val loss: 1.6169699430465698
Epoch 2490, training loss: 620.7243041992188 = 0.020001178607344627 + 100.0 * 6.207042694091797
Epoch 2490, val loss: 1.6200947761535645
Epoch 2500, training loss: 620.68017578125 = 0.019746366888284683 + 100.0 * 6.20660400390625
Epoch 2500, val loss: 1.623665452003479
Epoch 2510, training loss: 620.9531860351562 = 0.019502993673086166 + 100.0 * 6.209336757659912
Epoch 2510, val loss: 1.6266061067581177
Epoch 2520, training loss: 620.8421020507812 = 0.019253196194767952 + 100.0 * 6.208228588104248
Epoch 2520, val loss: 1.6302740573883057
Epoch 2530, training loss: 621.0801391601562 = 0.019007457420229912 + 100.0 * 6.210611343383789
Epoch 2530, val loss: 1.6331956386566162
Epoch 2540, training loss: 621.0165405273438 = 0.018759410828351974 + 100.0 * 6.209978103637695
Epoch 2540, val loss: 1.6365153789520264
Epoch 2550, training loss: 620.598388671875 = 0.01851264014840126 + 100.0 * 6.205799102783203
Epoch 2550, val loss: 1.6393249034881592
Epoch 2560, training loss: 620.6067504882812 = 0.018279576674103737 + 100.0 * 6.20588493347168
Epoch 2560, val loss: 1.6426173448562622
Epoch 2570, training loss: 620.72265625 = 0.018064308911561966 + 100.0 * 6.207046031951904
Epoch 2570, val loss: 1.64597749710083
Epoch 2580, training loss: 621.016357421875 = 0.017850760370492935 + 100.0 * 6.209985256195068
Epoch 2580, val loss: 1.6482722759246826
Epoch 2590, training loss: 620.9170532226562 = 0.017626935616135597 + 100.0 * 6.208994388580322
Epoch 2590, val loss: 1.6520109176635742
Epoch 2600, training loss: 620.6325073242188 = 0.017402498051524162 + 100.0 * 6.206151008605957
Epoch 2600, val loss: 1.6549979448318481
Epoch 2610, training loss: 620.9216918945312 = 0.017199907451868057 + 100.0 * 6.209044933319092
Epoch 2610, val loss: 1.6584246158599854
Epoch 2620, training loss: 620.8025512695312 = 0.016987033188343048 + 100.0 * 6.207855701446533
Epoch 2620, val loss: 1.6605867147445679
Epoch 2630, training loss: 620.6395874023438 = 0.01677325740456581 + 100.0 * 6.206227779388428
Epoch 2630, val loss: 1.6646403074264526
Epoch 2640, training loss: 620.576171875 = 0.016574759036302567 + 100.0 * 6.205595970153809
Epoch 2640, val loss: 1.667262077331543
Epoch 2650, training loss: 620.577392578125 = 0.016384897753596306 + 100.0 * 6.205610275268555
Epoch 2650, val loss: 1.670134425163269
Epoch 2660, training loss: 620.7549438476562 = 0.016195492818951607 + 100.0 * 6.207387447357178
Epoch 2660, val loss: 1.6729981899261475
Epoch 2670, training loss: 620.6583862304688 = 0.016006793826818466 + 100.0 * 6.206423759460449
Epoch 2670, val loss: 1.6755191087722778
Epoch 2680, training loss: 620.770751953125 = 0.01582752913236618 + 100.0 * 6.207549571990967
Epoch 2680, val loss: 1.6789804697036743
Epoch 2690, training loss: 620.5823974609375 = 0.015636591240763664 + 100.0 * 6.205667972564697
Epoch 2690, val loss: 1.6811888217926025
Epoch 2700, training loss: 620.9224853515625 = 0.01545962505042553 + 100.0 * 6.209070205688477
Epoch 2700, val loss: 1.6845427751541138
Epoch 2710, training loss: 620.5083618164062 = 0.015273378230631351 + 100.0 * 6.204930782318115
Epoch 2710, val loss: 1.6878390312194824
Epoch 2720, training loss: 620.5289916992188 = 0.015100879594683647 + 100.0 * 6.20513916015625
Epoch 2720, val loss: 1.6904699802398682
Epoch 2730, training loss: 621.134765625 = 0.01493742037564516 + 100.0 * 6.211197853088379
Epoch 2730, val loss: 1.6938468217849731
Epoch 2740, training loss: 620.5036010742188 = 0.014763644896447659 + 100.0 * 6.204888343811035
Epoch 2740, val loss: 1.6955527067184448
Epoch 2750, training loss: 620.3241577148438 = 0.014600696042180061 + 100.0 * 6.20309591293335
Epoch 2750, val loss: 1.6986963748931885
Epoch 2760, training loss: 620.2919921875 = 0.014446502551436424 + 100.0 * 6.202775001525879
Epoch 2760, val loss: 1.701366901397705
Epoch 2770, training loss: 620.653076171875 = 0.014300554059445858 + 100.0 * 6.206387996673584
Epoch 2770, val loss: 1.7036367654800415
Epoch 2780, training loss: 620.626708984375 = 0.014132540673017502 + 100.0 * 6.206125259399414
Epoch 2780, val loss: 1.7065027952194214
Epoch 2790, training loss: 620.383056640625 = 0.013970281928777695 + 100.0 * 6.203691005706787
Epoch 2790, val loss: 1.7089853286743164
Epoch 2800, training loss: 620.374755859375 = 0.013816862367093563 + 100.0 * 6.203609466552734
Epoch 2800, val loss: 1.7120232582092285
Epoch 2810, training loss: 620.5347290039062 = 0.013680682517588139 + 100.0 * 6.2052106857299805
Epoch 2810, val loss: 1.7138704061508179
Epoch 2820, training loss: 620.5724487304688 = 0.013534433208405972 + 100.0 * 6.2055888175964355
Epoch 2820, val loss: 1.7168419361114502
Epoch 2830, training loss: 620.7669067382812 = 0.013386472128331661 + 100.0 * 6.207535266876221
Epoch 2830, val loss: 1.7189605236053467
Epoch 2840, training loss: 620.286376953125 = 0.0132368179038167 + 100.0 * 6.202731132507324
Epoch 2840, val loss: 1.7218801975250244
Epoch 2850, training loss: 620.2503662109375 = 0.013101164251565933 + 100.0 * 6.2023725509643555
Epoch 2850, val loss: 1.7248995304107666
Epoch 2860, training loss: 620.3854370117188 = 0.012969196774065495 + 100.0 * 6.2037248611450195
Epoch 2860, val loss: 1.7276053428649902
Epoch 2870, training loss: 620.7896728515625 = 0.012840235605835915 + 100.0 * 6.207768440246582
Epoch 2870, val loss: 1.7291730642318726
Epoch 2880, training loss: 620.4344482421875 = 0.012700831517577171 + 100.0 * 6.204217433929443
Epoch 2880, val loss: 1.7327665090560913
Epoch 2890, training loss: 620.2875366210938 = 0.01256900280714035 + 100.0 * 6.202749729156494
Epoch 2890, val loss: 1.7347332239151
Epoch 2900, training loss: 620.2322387695312 = 0.012442496605217457 + 100.0 * 6.202198028564453
Epoch 2900, val loss: 1.7368918657302856
Epoch 2910, training loss: 620.4940185546875 = 0.012323186732828617 + 100.0 * 6.204816818237305
Epoch 2910, val loss: 1.739590048789978
Epoch 2920, training loss: 620.6576538085938 = 0.01219572126865387 + 100.0 * 6.206454277038574
Epoch 2920, val loss: 1.7409223318099976
Epoch 2930, training loss: 620.1485595703125 = 0.012067030183970928 + 100.0 * 6.201364994049072
Epoch 2930, val loss: 1.7442538738250732
Epoch 2940, training loss: 620.1342163085938 = 0.011945477686822414 + 100.0 * 6.2012224197387695
Epoch 2940, val loss: 1.7462952136993408
Epoch 2950, training loss: 620.2725830078125 = 0.011837027966976166 + 100.0 * 6.202607154846191
Epoch 2950, val loss: 1.7483123540878296
Epoch 2960, training loss: 620.5020751953125 = 0.011726333759725094 + 100.0 * 6.204903602600098
Epoch 2960, val loss: 1.7505748271942139
Epoch 2970, training loss: 620.2666625976562 = 0.01160169392824173 + 100.0 * 6.202550888061523
Epoch 2970, val loss: 1.753688931465149
Epoch 2980, training loss: 620.1288452148438 = 0.011492162011563778 + 100.0 * 6.201173782348633
Epoch 2980, val loss: 1.755658507347107
Epoch 2990, training loss: 620.3745727539062 = 0.011389214545488358 + 100.0 * 6.203631401062012
Epoch 2990, val loss: 1.7577296495437622
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6925925925925926
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 861.6199951171875 = 1.9366480112075806 + 100.0 * 8.596833229064941
Epoch 0, val loss: 1.9321775436401367
Epoch 10, training loss: 861.53271484375 = 1.9286667108535767 + 100.0 * 8.596040725708008
Epoch 10, val loss: 1.924652099609375
Epoch 20, training loss: 861.0145874023438 = 1.9186137914657593 + 100.0 * 8.590959548950195
Epoch 20, val loss: 1.914790153503418
Epoch 30, training loss: 857.5699462890625 = 1.9052059650421143 + 100.0 * 8.556647300720215
Epoch 30, val loss: 1.9013394117355347
Epoch 40, training loss: 834.6796264648438 = 1.8881678581237793 + 100.0 * 8.327914237976074
Epoch 40, val loss: 1.88435959815979
Epoch 50, training loss: 744.2352294921875 = 1.8690611124038696 + 100.0 * 7.423661231994629
Epoch 50, val loss: 1.865234375
Epoch 60, training loss: 721.473388671875 = 1.8525031805038452 + 100.0 * 7.196208953857422
Epoch 60, val loss: 1.8493175506591797
Epoch 70, training loss: 703.8671264648438 = 1.8388800621032715 + 100.0 * 7.020282745361328
Epoch 70, val loss: 1.8365015983581543
Epoch 80, training loss: 691.4345092773438 = 1.8275467157363892 + 100.0 * 6.896069526672363
Epoch 80, val loss: 1.825656771659851
Epoch 90, training loss: 684.3289184570312 = 1.817605972290039 + 100.0 * 6.825112819671631
Epoch 90, val loss: 1.8158848285675049
Epoch 100, training loss: 679.12548828125 = 1.8087292909622192 + 100.0 * 6.773167610168457
Epoch 100, val loss: 1.8070094585418701
Epoch 110, training loss: 674.418212890625 = 1.8006784915924072 + 100.0 * 6.726175785064697
Epoch 110, val loss: 1.7990221977233887
Epoch 120, training loss: 669.836181640625 = 1.7934192419052124 + 100.0 * 6.6804280281066895
Epoch 120, val loss: 1.7918423414230347
Epoch 130, training loss: 665.9721069335938 = 1.7870066165924072 + 100.0 * 6.64185094833374
Epoch 130, val loss: 1.785339593887329
Epoch 140, training loss: 662.0582885742188 = 1.7806012630462646 + 100.0 * 6.602777004241943
Epoch 140, val loss: 1.7789044380187988
Epoch 150, training loss: 658.9961547851562 = 1.773952603340149 + 100.0 * 6.5722222328186035
Epoch 150, val loss: 1.7722052335739136
Epoch 160, training loss: 656.6246337890625 = 1.7665281295776367 + 100.0 * 6.548581123352051
Epoch 160, val loss: 1.7648766040802002
Epoch 170, training loss: 654.6141357421875 = 1.7581675052642822 + 100.0 * 6.528559684753418
Epoch 170, val loss: 1.7567235231399536
Epoch 180, training loss: 653.0450439453125 = 1.7490055561065674 + 100.0 * 6.512960433959961
Epoch 180, val loss: 1.7479338645935059
Epoch 190, training loss: 651.8638916015625 = 1.7391096353530884 + 100.0 * 6.501247406005859
Epoch 190, val loss: 1.7384848594665527
Epoch 200, training loss: 650.399169921875 = 1.7283110618591309 + 100.0 * 6.486708641052246
Epoch 200, val loss: 1.7282863855361938
Epoch 210, training loss: 649.116943359375 = 1.7167435884475708 + 100.0 * 6.474001884460449
Epoch 210, val loss: 1.717437505722046
Epoch 220, training loss: 648.3292236328125 = 1.7043522596359253 + 100.0 * 6.466248512268066
Epoch 220, val loss: 1.7060307264328003
Epoch 230, training loss: 646.7146606445312 = 1.6909562349319458 + 100.0 * 6.450236797332764
Epoch 230, val loss: 1.6935521364212036
Epoch 240, training loss: 645.6124877929688 = 1.676674723625183 + 100.0 * 6.439357757568359
Epoch 240, val loss: 1.6804332733154297
Epoch 250, training loss: 644.6472778320312 = 1.6613506078720093 + 100.0 * 6.429859161376953
Epoch 250, val loss: 1.6664217710494995
Epoch 260, training loss: 643.7363891601562 = 1.644727349281311 + 100.0 * 6.42091703414917
Epoch 260, val loss: 1.6512763500213623
Epoch 270, training loss: 642.7698364257812 = 1.6269763708114624 + 100.0 * 6.411428928375244
Epoch 270, val loss: 1.6350854635238647
Epoch 280, training loss: 642.3099975585938 = 1.6078457832336426 + 100.0 * 6.407021522521973
Epoch 280, val loss: 1.6178860664367676
Epoch 290, training loss: 641.2030639648438 = 1.5876473188400269 + 100.0 * 6.396153926849365
Epoch 290, val loss: 1.5996021032333374
Epoch 300, training loss: 640.424560546875 = 1.566509485244751 + 100.0 * 6.388580322265625
Epoch 300, val loss: 1.5806716680526733
Epoch 310, training loss: 639.9594116210938 = 1.5443363189697266 + 100.0 * 6.384150981903076
Epoch 310, val loss: 1.5609692335128784
Epoch 320, training loss: 639.1512451171875 = 1.5211507081985474 + 100.0 * 6.376300811767578
Epoch 320, val loss: 1.5407218933105469
Epoch 330, training loss: 638.4892578125 = 1.4973750114440918 + 100.0 * 6.3699188232421875
Epoch 330, val loss: 1.5200600624084473
Epoch 340, training loss: 638.1228637695312 = 1.4729163646697998 + 100.0 * 6.366499423980713
Epoch 340, val loss: 1.4992108345031738
Epoch 350, training loss: 637.5087890625 = 1.4477204084396362 + 100.0 * 6.3606109619140625
Epoch 350, val loss: 1.4774783849716187
Epoch 360, training loss: 636.8448486328125 = 1.4221876859664917 + 100.0 * 6.354226589202881
Epoch 360, val loss: 1.4559515714645386
Epoch 370, training loss: 636.4132080078125 = 1.3963507413864136 + 100.0 * 6.350168704986572
Epoch 370, val loss: 1.4343528747558594
Epoch 380, training loss: 636.0786743164062 = 1.3701411485671997 + 100.0 * 6.347085475921631
Epoch 380, val loss: 1.4126862287521362
Epoch 390, training loss: 635.64013671875 = 1.3440362215042114 + 100.0 * 6.342960834503174
Epoch 390, val loss: 1.3911303281784058
Epoch 400, training loss: 635.12255859375 = 1.3178027868270874 + 100.0 * 6.338047504425049
Epoch 400, val loss: 1.3697768449783325
Epoch 410, training loss: 635.2547607421875 = 1.2916237115859985 + 100.0 * 6.339631080627441
Epoch 410, val loss: 1.3485409021377563
Epoch 420, training loss: 634.417724609375 = 1.2653675079345703 + 100.0 * 6.331523418426514
Epoch 420, val loss: 1.3276883363723755
Epoch 430, training loss: 634.406005859375 = 1.239387035369873 + 100.0 * 6.331665992736816
Epoch 430, val loss: 1.307242512702942
Epoch 440, training loss: 633.8312377929688 = 1.2139378786087036 + 100.0 * 6.326173305511475
Epoch 440, val loss: 1.287096619606018
Epoch 450, training loss: 633.4515991210938 = 1.188589096069336 + 100.0 * 6.322629928588867
Epoch 450, val loss: 1.267393946647644
Epoch 460, training loss: 633.1495361328125 = 1.163709044456482 + 100.0 * 6.319858551025391
Epoch 460, val loss: 1.2481824159622192
Epoch 470, training loss: 633.5764770507812 = 1.139240026473999 + 100.0 * 6.324372291564941
Epoch 470, val loss: 1.2291393280029297
Epoch 480, training loss: 632.8958129882812 = 1.1147291660308838 + 100.0 * 6.317810535430908
Epoch 480, val loss: 1.2106825113296509
Epoch 490, training loss: 632.4097290039062 = 1.0909490585327148 + 100.0 * 6.313188076019287
Epoch 490, val loss: 1.1927635669708252
Epoch 500, training loss: 632.089111328125 = 1.0677050352096558 + 100.0 * 6.310214042663574
Epoch 500, val loss: 1.1753517389297485
Epoch 510, training loss: 632.4488525390625 = 1.0449256896972656 + 100.0 * 6.31403923034668
Epoch 510, val loss: 1.1582542657852173
Epoch 520, training loss: 631.8018798828125 = 1.0221794843673706 + 100.0 * 6.307797431945801
Epoch 520, val loss: 1.1419670581817627
Epoch 530, training loss: 631.781982421875 = 1.0001400709152222 + 100.0 * 6.30781888961792
Epoch 530, val loss: 1.12587571144104
Epoch 540, training loss: 631.2374267578125 = 0.97847580909729 + 100.0 * 6.3025898933410645
Epoch 540, val loss: 1.1102614402770996
Epoch 550, training loss: 631.086669921875 = 0.9573466777801514 + 100.0 * 6.30129337310791
Epoch 550, val loss: 1.0951944589614868
Epoch 560, training loss: 630.8692016601562 = 0.9365657567977905 + 100.0 * 6.299326419830322
Epoch 560, val loss: 1.0807511806488037
Epoch 570, training loss: 630.6630859375 = 0.9163506031036377 + 100.0 * 6.2974677085876465
Epoch 570, val loss: 1.0667294263839722
Epoch 580, training loss: 630.5588989257812 = 0.8963686227798462 + 100.0 * 6.296625137329102
Epoch 580, val loss: 1.0531681776046753
Epoch 590, training loss: 630.5162353515625 = 0.8766813278198242 + 100.0 * 6.296395301818848
Epoch 590, val loss: 1.0398509502410889
Epoch 600, training loss: 630.0863647460938 = 0.8574281334877014 + 100.0 * 6.2922892570495605
Epoch 600, val loss: 1.0272138118743896
Epoch 610, training loss: 629.9024047851562 = 0.8385202884674072 + 100.0 * 6.2906389236450195
Epoch 610, val loss: 1.0151139497756958
Epoch 620, training loss: 630.10986328125 = 0.8200615048408508 + 100.0 * 6.292898178100586
Epoch 620, val loss: 1.0036622285842896
Epoch 630, training loss: 629.9989624023438 = 0.8017963171005249 + 100.0 * 6.291971683502197
Epoch 630, val loss: 0.9918467402458191
Epoch 640, training loss: 629.388916015625 = 0.7837644219398499 + 100.0 * 6.2860517501831055
Epoch 640, val loss: 0.9809573888778687
Epoch 650, training loss: 629.2973022460938 = 0.766176700592041 + 100.0 * 6.285311222076416
Epoch 650, val loss: 0.9706601500511169
Epoch 660, training loss: 629.557861328125 = 0.7488466501235962 + 100.0 * 6.288090229034424
Epoch 660, val loss: 0.9605343341827393
Epoch 670, training loss: 629.1688842773438 = 0.7318527698516846 + 100.0 * 6.284369945526123
Epoch 670, val loss: 0.9510446190834045
Epoch 680, training loss: 629.3310546875 = 0.7148392796516418 + 100.0 * 6.286162376403809
Epoch 680, val loss: 0.9416443109512329
Epoch 690, training loss: 628.7692260742188 = 0.6983939409255981 + 100.0 * 6.2807087898254395
Epoch 690, val loss: 0.9325037598609924
Epoch 700, training loss: 628.62109375 = 0.682102382183075 + 100.0 * 6.27938985824585
Epoch 700, val loss: 0.924086332321167
Epoch 710, training loss: 628.52978515625 = 0.6660556793212891 + 100.0 * 6.278636932373047
Epoch 710, val loss: 0.9157765507698059
Epoch 720, training loss: 628.3155517578125 = 0.6502754092216492 + 100.0 * 6.276652812957764
Epoch 720, val loss: 0.9081567525863647
Epoch 730, training loss: 628.1412353515625 = 0.6347744464874268 + 100.0 * 6.275064468383789
Epoch 730, val loss: 0.9006342887878418
Epoch 740, training loss: 628.3795776367188 = 0.6195464730262756 + 100.0 * 6.277600288391113
Epoch 740, val loss: 0.8937445878982544
Epoch 750, training loss: 628.2429809570312 = 0.6044515371322632 + 100.0 * 6.27638578414917
Epoch 750, val loss: 0.886496901512146
Epoch 760, training loss: 627.80810546875 = 0.5897093415260315 + 100.0 * 6.272183895111084
Epoch 760, val loss: 0.8800866007804871
Epoch 770, training loss: 627.6477661132812 = 0.575295627117157 + 100.0 * 6.270724773406982
Epoch 770, val loss: 0.8739991784095764
Epoch 780, training loss: 627.603515625 = 0.5613082647323608 + 100.0 * 6.270422458648682
Epoch 780, val loss: 0.8682863116264343
Epoch 790, training loss: 627.7374877929688 = 0.5475030541419983 + 100.0 * 6.271900177001953
Epoch 790, val loss: 0.8626312017440796
Epoch 800, training loss: 627.5357666015625 = 0.5337862968444824 + 100.0 * 6.27001953125
Epoch 800, val loss: 0.8572865128517151
Epoch 810, training loss: 627.1696166992188 = 0.5205777883529663 + 100.0 * 6.266490459442139
Epoch 810, val loss: 0.852404773235321
Epoch 820, training loss: 627.0650634765625 = 0.5077511668205261 + 100.0 * 6.265573024749756
Epoch 820, val loss: 0.8478541970252991
Epoch 830, training loss: 627.111083984375 = 0.49526068568229675 + 100.0 * 6.266158580780029
Epoch 830, val loss: 0.8436893820762634
Epoch 840, training loss: 627.0577392578125 = 0.4827762544155121 + 100.0 * 6.265749454498291
Epoch 840, val loss: 0.8394610285758972
Epoch 850, training loss: 626.8361206054688 = 0.47072485089302063 + 100.0 * 6.2636542320251465
Epoch 850, val loss: 0.8356572985649109
Epoch 860, training loss: 626.9061889648438 = 0.45904308557510376 + 100.0 * 6.264471530914307
Epoch 860, val loss: 0.8322274088859558
Epoch 870, training loss: 626.687255859375 = 0.4476873576641083 + 100.0 * 6.26239538192749
Epoch 870, val loss: 0.8290113210678101
Epoch 880, training loss: 626.5051879882812 = 0.4366817772388458 + 100.0 * 6.260684967041016
Epoch 880, val loss: 0.8259587287902832
Epoch 890, training loss: 626.77734375 = 0.42594224214553833 + 100.0 * 6.263513565063477
Epoch 890, val loss: 0.8231428861618042
Epoch 900, training loss: 626.9212646484375 = 0.4153520166873932 + 100.0 * 6.265059471130371
Epoch 900, val loss: 0.8206703662872314
Epoch 910, training loss: 626.314453125 = 0.4051752984523773 + 100.0 * 6.259093284606934
Epoch 910, val loss: 0.8184295892715454
Epoch 920, training loss: 626.1519165039062 = 0.39520516991615295 + 100.0 * 6.257566928863525
Epoch 920, val loss: 0.8162708282470703
Epoch 930, training loss: 626.0986328125 = 0.3856065571308136 + 100.0 * 6.257130146026611
Epoch 930, val loss: 0.8146138787269592
Epoch 940, training loss: 626.3434448242188 = 0.37628474831581116 + 100.0 * 6.259671688079834
Epoch 940, val loss: 0.8131711483001709
Epoch 950, training loss: 626.2684936523438 = 0.36717507243156433 + 100.0 * 6.2590131759643555
Epoch 950, val loss: 0.8112242221832275
Epoch 960, training loss: 625.8612060546875 = 0.35820630192756653 + 100.0 * 6.255029678344727
Epoch 960, val loss: 0.8100659847259521
Epoch 970, training loss: 625.8933715820312 = 0.3496389389038086 + 100.0 * 6.25543737411499
Epoch 970, val loss: 0.8089877367019653
Epoch 980, training loss: 626.0189208984375 = 0.3412623703479767 + 100.0 * 6.256776809692383
Epoch 980, val loss: 0.8079891204833984
Epoch 990, training loss: 625.7023315429688 = 0.3331863582134247 + 100.0 * 6.25369119644165
Epoch 990, val loss: 0.807364284992218
Epoch 1000, training loss: 625.5562744140625 = 0.32528722286224365 + 100.0 * 6.252309322357178
Epoch 1000, val loss: 0.8067020773887634
Epoch 1010, training loss: 625.63037109375 = 0.3176877498626709 + 100.0 * 6.253127098083496
Epoch 1010, val loss: 0.80655437707901
Epoch 1020, training loss: 625.5079956054688 = 0.3102123737335205 + 100.0 * 6.251977443695068
Epoch 1020, val loss: 0.8059715628623962
Epoch 1030, training loss: 625.4144287109375 = 0.30290305614471436 + 100.0 * 6.251114845275879
Epoch 1030, val loss: 0.8059903979301453
Epoch 1040, training loss: 625.5126953125 = 0.2958871126174927 + 100.0 * 6.252167701721191
Epoch 1040, val loss: 0.8060101270675659
Epoch 1050, training loss: 625.2740478515625 = 0.28899863362312317 + 100.0 * 6.249850749969482
Epoch 1050, val loss: 0.8062078356742859
Epoch 1060, training loss: 625.3006591796875 = 0.28233346343040466 + 100.0 * 6.25018310546875
Epoch 1060, val loss: 0.8065201044082642
Epoch 1070, training loss: 625.1170043945312 = 0.2758653461933136 + 100.0 * 6.248411178588867
Epoch 1070, val loss: 0.8070375919342041
Epoch 1080, training loss: 625.453369140625 = 0.2695821225643158 + 100.0 * 6.251837730407715
Epoch 1080, val loss: 0.8075544238090515
Epoch 1090, training loss: 625.1056518554688 = 0.26337578892707825 + 100.0 * 6.248423099517822
Epoch 1090, val loss: 0.8080730438232422
Epoch 1100, training loss: 625.348388671875 = 0.2574268579483032 + 100.0 * 6.250909328460693
Epoch 1100, val loss: 0.8087882995605469
Epoch 1110, training loss: 624.9794311523438 = 0.2515566349029541 + 100.0 * 6.247278690338135
Epoch 1110, val loss: 0.8098536729812622
Epoch 1120, training loss: 624.8388671875 = 0.2459324151277542 + 100.0 * 6.245929718017578
Epoch 1120, val loss: 0.810768723487854
Epoch 1130, training loss: 624.9674072265625 = 0.24045203626155853 + 100.0 * 6.247269153594971
Epoch 1130, val loss: 0.8120449185371399
Epoch 1140, training loss: 624.6742553710938 = 0.235098734498024 + 100.0 * 6.244391441345215
Epoch 1140, val loss: 0.8132271766662598
Epoch 1150, training loss: 624.7481689453125 = 0.2299024909734726 + 100.0 * 6.245182514190674
Epoch 1150, val loss: 0.8145855665206909
Epoch 1160, training loss: 624.7648315429688 = 0.2248191237449646 + 100.0 * 6.245400428771973
Epoch 1160, val loss: 0.8159348964691162
Epoch 1170, training loss: 624.4995727539062 = 0.2199193835258484 + 100.0 * 6.242796897888184
Epoch 1170, val loss: 0.8176809549331665
Epoch 1180, training loss: 624.633056640625 = 0.2151402235031128 + 100.0 * 6.2441792488098145
Epoch 1180, val loss: 0.819374680519104
Epoch 1190, training loss: 624.5797729492188 = 0.2104419469833374 + 100.0 * 6.2436933517456055
Epoch 1190, val loss: 0.8209145069122314
Epoch 1200, training loss: 624.3619995117188 = 0.20584161579608917 + 100.0 * 6.241561412811279
Epoch 1200, val loss: 0.822571337223053
Epoch 1210, training loss: 624.30029296875 = 0.20143872499465942 + 100.0 * 6.240988731384277
Epoch 1210, val loss: 0.8244829177856445
Epoch 1220, training loss: 624.7643432617188 = 0.19714213907718658 + 100.0 * 6.245672225952148
Epoch 1220, val loss: 0.8264204859733582
Epoch 1230, training loss: 624.4176025390625 = 0.19291003048419952 + 100.0 * 6.242246627807617
Epoch 1230, val loss: 0.8285024166107178
Epoch 1240, training loss: 624.27587890625 = 0.1887960433959961 + 100.0 * 6.240870952606201
Epoch 1240, val loss: 0.8306251764297485
Epoch 1250, training loss: 624.3071899414062 = 0.1848299503326416 + 100.0 * 6.2412238121032715
Epoch 1250, val loss: 0.8327717781066895
Epoch 1260, training loss: 624.1123657226562 = 0.18091432750225067 + 100.0 * 6.239314556121826
Epoch 1260, val loss: 0.8349481821060181
Epoch 1270, training loss: 624.0438842773438 = 0.17709994316101074 + 100.0 * 6.2386674880981445
Epoch 1270, val loss: 0.8371164202690125
Epoch 1280, training loss: 624.1762084960938 = 0.17339105904102325 + 100.0 * 6.240027904510498
Epoch 1280, val loss: 0.8394425511360168
Epoch 1290, training loss: 623.8187866210938 = 0.1697835922241211 + 100.0 * 6.236489772796631
Epoch 1290, val loss: 0.8418375253677368
Epoch 1300, training loss: 623.9224243164062 = 0.16629856824874878 + 100.0 * 6.237561225891113
Epoch 1300, val loss: 0.8442630171775818
Epoch 1310, training loss: 624.3746337890625 = 0.16288289427757263 + 100.0 * 6.242117404937744
Epoch 1310, val loss: 0.8468382954597473
Epoch 1320, training loss: 624.27783203125 = 0.15947933495044708 + 100.0 * 6.241183280944824
Epoch 1320, val loss: 0.8490834832191467
Epoch 1330, training loss: 623.79736328125 = 0.15613636374473572 + 100.0 * 6.236412525177002
Epoch 1330, val loss: 0.8515341877937317
Epoch 1340, training loss: 623.64306640625 = 0.15296775102615356 + 100.0 * 6.234901428222656
Epoch 1340, val loss: 0.854174017906189
Epoch 1350, training loss: 623.560791015625 = 0.14988897740840912 + 100.0 * 6.234108924865723
Epoch 1350, val loss: 0.856921374797821
Epoch 1360, training loss: 624.1884155273438 = 0.14689895510673523 + 100.0 * 6.240415096282959
Epoch 1360, val loss: 0.8593951463699341
Epoch 1370, training loss: 623.6338500976562 = 0.14387452602386475 + 100.0 * 6.234899997711182
Epoch 1370, val loss: 0.8623562455177307
Epoch 1380, training loss: 623.5613403320312 = 0.1409856528043747 + 100.0 * 6.234203338623047
Epoch 1380, val loss: 0.8648539185523987
Epoch 1390, training loss: 623.572998046875 = 0.13817645609378815 + 100.0 * 6.234348297119141
Epoch 1390, val loss: 0.8677884340286255
Epoch 1400, training loss: 623.4048461914062 = 0.13543109595775604 + 100.0 * 6.232694149017334
Epoch 1400, val loss: 0.8705509901046753
Epoch 1410, training loss: 623.8663940429688 = 0.13273881375789642 + 100.0 * 6.237336158752441
Epoch 1410, val loss: 0.8733765482902527
Epoch 1420, training loss: 623.863037109375 = 0.1301012635231018 + 100.0 * 6.237329006195068
Epoch 1420, val loss: 0.8759351372718811
Epoch 1430, training loss: 623.5453491210938 = 0.12748929858207703 + 100.0 * 6.23417854309082
Epoch 1430, val loss: 0.8790102601051331
Epoch 1440, training loss: 623.1943359375 = 0.12496297806501389 + 100.0 * 6.230693817138672
Epoch 1440, val loss: 0.881871223449707
Epoch 1450, training loss: 623.1331176757812 = 0.12254445999860764 + 100.0 * 6.230105876922607
Epoch 1450, val loss: 0.8848819136619568
Epoch 1460, training loss: 623.4243774414062 = 0.12019219994544983 + 100.0 * 6.233042240142822
Epoch 1460, val loss: 0.8880116939544678
Epoch 1470, training loss: 623.0621337890625 = 0.1178133562207222 + 100.0 * 6.229443073272705
Epoch 1470, val loss: 0.8907138109207153
Epoch 1480, training loss: 623.1148071289062 = 0.11551440507173538 + 100.0 * 6.229992389678955
Epoch 1480, val loss: 0.8935624957084656
Epoch 1490, training loss: 623.5081787109375 = 0.1132948026061058 + 100.0 * 6.233948707580566
Epoch 1490, val loss: 0.8967624306678772
Epoch 1500, training loss: 623.1309814453125 = 0.11105819046497345 + 100.0 * 6.230198860168457
Epoch 1500, val loss: 0.8995943665504456
Epoch 1510, training loss: 623.0403442382812 = 0.10890465974807739 + 100.0 * 6.229314804077148
Epoch 1510, val loss: 0.9028094410896301
Epoch 1520, training loss: 623.2501220703125 = 0.10682641714811325 + 100.0 * 6.231432914733887
Epoch 1520, val loss: 0.9060463309288025
Epoch 1530, training loss: 622.9324951171875 = 0.1047476977109909 + 100.0 * 6.228277683258057
Epoch 1530, val loss: 0.9089156985282898
Epoch 1540, training loss: 622.8151245117188 = 0.10270798951387405 + 100.0 * 6.227123737335205
Epoch 1540, val loss: 0.9119670987129211
Epoch 1550, training loss: 622.7440185546875 = 0.10074746608734131 + 100.0 * 6.2264323234558105
Epoch 1550, val loss: 0.9152107238769531
Epoch 1560, training loss: 622.80322265625 = 0.09884121268987656 + 100.0 * 6.227044105529785
Epoch 1560, val loss: 0.9183318018913269
Epoch 1570, training loss: 623.3536376953125 = 0.0969543382525444 + 100.0 * 6.232566833496094
Epoch 1570, val loss: 0.9214267730712891
Epoch 1580, training loss: 622.8572998046875 = 0.09508118033409119 + 100.0 * 6.227622032165527
Epoch 1580, val loss: 0.9247387051582336
Epoch 1590, training loss: 622.8724975585938 = 0.09326551854610443 + 100.0 * 6.227792263031006
Epoch 1590, val loss: 0.928034782409668
Epoch 1600, training loss: 622.6803588867188 = 0.09149420261383057 + 100.0 * 6.225888729095459
Epoch 1600, val loss: 0.9312241077423096
Epoch 1610, training loss: 622.9182739257812 = 0.08977943658828735 + 100.0 * 6.22828483581543
Epoch 1610, val loss: 0.9345951080322266
Epoch 1620, training loss: 622.6254272460938 = 0.08806551247835159 + 100.0 * 6.2253737449646
Epoch 1620, val loss: 0.9377748966217041
Epoch 1630, training loss: 622.4951782226562 = 0.08639796078205109 + 100.0 * 6.224087238311768
Epoch 1630, val loss: 0.9409525990486145
Epoch 1640, training loss: 622.7711791992188 = 0.08479320257902145 + 100.0 * 6.226863861083984
Epoch 1640, val loss: 0.9442108869552612
Epoch 1650, training loss: 622.6222534179688 = 0.08317742496728897 + 100.0 * 6.225390911102295
Epoch 1650, val loss: 0.9473819136619568
Epoch 1660, training loss: 622.5875854492188 = 0.08161274343729019 + 100.0 * 6.225059509277344
Epoch 1660, val loss: 0.9505348205566406
Epoch 1670, training loss: 622.3811645507812 = 0.08006716519594193 + 100.0 * 6.223011016845703
Epoch 1670, val loss: 0.9540199637413025
Epoch 1680, training loss: 622.38720703125 = 0.07858752459287643 + 100.0 * 6.223085880279541
Epoch 1680, val loss: 0.9572584629058838
Epoch 1690, training loss: 623.008544921875 = 0.07712939381599426 + 100.0 * 6.229313850402832
Epoch 1690, val loss: 0.9603972434997559
Epoch 1700, training loss: 622.4797973632812 = 0.07567861676216125 + 100.0 * 6.224040985107422
Epoch 1700, val loss: 0.9640422463417053
Epoch 1710, training loss: 622.2630004882812 = 0.07426172494888306 + 100.0 * 6.221887111663818
Epoch 1710, val loss: 0.9671546220779419
Epoch 1720, training loss: 622.3074951171875 = 0.072908915579319 + 100.0 * 6.222345352172852
Epoch 1720, val loss: 0.9706719517707825
Epoch 1730, training loss: 622.6383666992188 = 0.07156404852867126 + 100.0 * 6.225667953491211
Epoch 1730, val loss: 0.9737750887870789
Epoch 1740, training loss: 622.2149047851562 = 0.07023324817419052 + 100.0 * 6.221446514129639
Epoch 1740, val loss: 0.9773694276809692
Epoch 1750, training loss: 622.0787353515625 = 0.06893713027238846 + 100.0 * 6.220098495483398
Epoch 1750, val loss: 0.9807183146476746
Epoch 1760, training loss: 622.1744995117188 = 0.06768201291561127 + 100.0 * 6.221067905426025
Epoch 1760, val loss: 0.9841609001159668
Epoch 1770, training loss: 622.4932250976562 = 0.06645657122135162 + 100.0 * 6.224267482757568
Epoch 1770, val loss: 0.9875591993331909
Epoch 1780, training loss: 622.492431640625 = 0.06522819399833679 + 100.0 * 6.224271774291992
Epoch 1780, val loss: 0.9905321002006531
Epoch 1790, training loss: 622.072509765625 = 0.06401286274194717 + 100.0 * 6.2200846672058105
Epoch 1790, val loss: 0.9939330816268921
Epoch 1800, training loss: 621.94580078125 = 0.06285630911588669 + 100.0 * 6.218829154968262
Epoch 1800, val loss: 0.9974510073661804
Epoch 1810, training loss: 621.9785766601562 = 0.061741333454847336 + 100.0 * 6.219168186187744
Epoch 1810, val loss: 1.00084388256073
Epoch 1820, training loss: 622.5848388671875 = 0.060656022280454636 + 100.0 * 6.225241661071777
Epoch 1820, val loss: 1.0044046640396118
Epoch 1830, training loss: 621.9441528320312 = 0.059518881142139435 + 100.0 * 6.218846321105957
Epoch 1830, val loss: 1.0071401596069336
Epoch 1840, training loss: 621.8348388671875 = 0.058452241122722626 + 100.0 * 6.217763423919678
Epoch 1840, val loss: 1.0106269121170044
Epoch 1850, training loss: 621.8895263671875 = 0.05742685869336128 + 100.0 * 6.218320846557617
Epoch 1850, val loss: 1.0139542818069458
Epoch 1860, training loss: 622.80908203125 = 0.0564260259270668 + 100.0 * 6.227526664733887
Epoch 1860, val loss: 1.016933798789978
Epoch 1870, training loss: 622.32666015625 = 0.05539334937930107 + 100.0 * 6.222712516784668
Epoch 1870, val loss: 1.0207070112228394
Epoch 1880, training loss: 621.8675537109375 = 0.05439743027091026 + 100.0 * 6.2181315422058105
Epoch 1880, val loss: 1.0235769748687744
Epoch 1890, training loss: 621.7168579101562 = 0.05344453826546669 + 100.0 * 6.2166337966918945
Epoch 1890, val loss: 1.0271974802017212
Epoch 1900, training loss: 621.6961059570312 = 0.05252579599618912 + 100.0 * 6.216435432434082
Epoch 1900, val loss: 1.0304510593414307
Epoch 1910, training loss: 622.6143798828125 = 0.05163365975022316 + 100.0 * 6.225627422332764
Epoch 1910, val loss: 1.0337437391281128
Epoch 1920, training loss: 622.1341552734375 = 0.050713296979665756 + 100.0 * 6.220834255218506
Epoch 1920, val loss: 1.0366158485412598
Epoch 1930, training loss: 621.7293701171875 = 0.049819327890872955 + 100.0 * 6.216795444488525
Epoch 1930, val loss: 1.0401179790496826
Epoch 1940, training loss: 621.589599609375 = 0.0489661768078804 + 100.0 * 6.21540641784668
Epoch 1940, val loss: 1.0435431003570557
Epoch 1950, training loss: 621.8253784179688 = 0.04814816266298294 + 100.0 * 6.217772483825684
Epoch 1950, val loss: 1.0470457077026367
Epoch 1960, training loss: 621.6222534179688 = 0.04730714485049248 + 100.0 * 6.215749740600586
Epoch 1960, val loss: 1.049926519393921
Epoch 1970, training loss: 621.5442504882812 = 0.04649009928107262 + 100.0 * 6.214977264404297
Epoch 1970, val loss: 1.053092122077942
Epoch 1980, training loss: 621.5122680664062 = 0.045705270022153854 + 100.0 * 6.214665412902832
Epoch 1980, val loss: 1.056415319442749
Epoch 1990, training loss: 621.6617431640625 = 0.044949721544981 + 100.0 * 6.216168403625488
Epoch 1990, val loss: 1.059506893157959
Epoch 2000, training loss: 622.002197265625 = 0.04419759660959244 + 100.0 * 6.219580173492432
Epoch 2000, val loss: 1.0625622272491455
Epoch 2010, training loss: 621.5855102539062 = 0.043436672538518906 + 100.0 * 6.215420722961426
Epoch 2010, val loss: 1.065653920173645
Epoch 2020, training loss: 621.4432373046875 = 0.04270212724804878 + 100.0 * 6.214004993438721
Epoch 2020, val loss: 1.0689142942428589
Epoch 2030, training loss: 621.3640747070312 = 0.04200105741620064 + 100.0 * 6.213220596313477
Epoch 2030, val loss: 1.072218418121338
Epoch 2040, training loss: 621.3645629882812 = 0.041326623409986496 + 100.0 * 6.213232517242432
Epoch 2040, val loss: 1.0754797458648682
Epoch 2050, training loss: 621.9722290039062 = 0.040672287344932556 + 100.0 * 6.219315528869629
Epoch 2050, val loss: 1.0787006616592407
Epoch 2060, training loss: 621.4696044921875 = 0.03998846188187599 + 100.0 * 6.214296340942383
Epoch 2060, val loss: 1.0814650058746338
Epoch 2070, training loss: 621.3361206054688 = 0.0393332801759243 + 100.0 * 6.212967872619629
Epoch 2070, val loss: 1.084427833557129
Epoch 2080, training loss: 621.3199462890625 = 0.03870215639472008 + 100.0 * 6.212812423706055
Epoch 2080, val loss: 1.0877705812454224
Epoch 2090, training loss: 621.6047973632812 = 0.038097042590379715 + 100.0 * 6.215667247772217
Epoch 2090, val loss: 1.0906903743743896
Epoch 2100, training loss: 621.3617553710938 = 0.037480711936950684 + 100.0 * 6.213242530822754
Epoch 2100, val loss: 1.0937669277191162
Epoch 2110, training loss: 621.2891235351562 = 0.03687873110175133 + 100.0 * 6.212522506713867
Epoch 2110, val loss: 1.0968327522277832
Epoch 2120, training loss: 621.5035400390625 = 0.036299869418144226 + 100.0 * 6.214672088623047
Epoch 2120, val loss: 1.100005030632019
Epoch 2130, training loss: 621.2752685546875 = 0.0357121117413044 + 100.0 * 6.212395668029785
Epoch 2130, val loss: 1.1026676893234253
Epoch 2140, training loss: 621.32666015625 = 0.03515329584479332 + 100.0 * 6.212914943695068
Epoch 2140, val loss: 1.1057052612304688
Epoch 2150, training loss: 621.2593994140625 = 0.03460918366909027 + 100.0 * 6.212247848510742
Epoch 2150, val loss: 1.1089553833007812
Epoch 2160, training loss: 621.3248291015625 = 0.034080129116773605 + 100.0 * 6.212907791137695
Epoch 2160, val loss: 1.1119831800460815
Epoch 2170, training loss: 621.455078125 = 0.03355441987514496 + 100.0 * 6.21421480178833
Epoch 2170, val loss: 1.1149375438690186
Epoch 2180, training loss: 621.2141723632812 = 0.033030204474925995 + 100.0 * 6.211811542510986
Epoch 2180, val loss: 1.1177167892456055
Epoch 2190, training loss: 621.0573120117188 = 0.032522961497306824 + 100.0 * 6.210247993469238
Epoch 2190, val loss: 1.1206691265106201
Epoch 2200, training loss: 621.1898193359375 = 0.03204302489757538 + 100.0 * 6.211577892303467
Epoch 2200, val loss: 1.1234437227249146
Epoch 2210, training loss: 621.4814453125 = 0.031563401222229004 + 100.0 * 6.214498996734619
Epoch 2210, val loss: 1.1261770725250244
Epoch 2220, training loss: 621.1998291015625 = 0.031084131449460983 + 100.0 * 6.2116875648498535
Epoch 2220, val loss: 1.1293647289276123
Epoch 2230, training loss: 620.97216796875 = 0.030610403046011925 + 100.0 * 6.209415435791016
Epoch 2230, val loss: 1.1321463584899902
Epoch 2240, training loss: 621.213623046875 = 0.03016330488026142 + 100.0 * 6.21183443069458
Epoch 2240, val loss: 1.134901762008667
Epoch 2250, training loss: 621.2321166992188 = 0.029718060046434402 + 100.0 * 6.212024211883545
Epoch 2250, val loss: 1.137732982635498
Epoch 2260, training loss: 621.0045166015625 = 0.029264425858855247 + 100.0 * 6.209753036499023
Epoch 2260, val loss: 1.1405147314071655
Epoch 2270, training loss: 620.8991088867188 = 0.028838034719228745 + 100.0 * 6.20870304107666
Epoch 2270, val loss: 1.1435530185699463
Epoch 2280, training loss: 620.957275390625 = 0.02843070961534977 + 100.0 * 6.209288597106934
Epoch 2280, val loss: 1.1464859247207642
Epoch 2290, training loss: 620.9962768554688 = 0.028027456253767014 + 100.0 * 6.209682464599609
Epoch 2290, val loss: 1.1493456363677979
Epoch 2300, training loss: 621.3536376953125 = 0.02763538993895054 + 100.0 * 6.213259696960449
Epoch 2300, val loss: 1.1522716283798218
Epoch 2310, training loss: 620.938720703125 = 0.027223464101552963 + 100.0 * 6.209115028381348
Epoch 2310, val loss: 1.1545766592025757
Epoch 2320, training loss: 620.8051147460938 = 0.026826726272702217 + 100.0 * 6.207782745361328
Epoch 2320, val loss: 1.1573657989501953
Epoch 2330, training loss: 620.7693481445312 = 0.02645285800099373 + 100.0 * 6.2074294090271
Epoch 2330, val loss: 1.1603306531906128
Epoch 2340, training loss: 621.1878051757812 = 0.026100479066371918 + 100.0 * 6.2116169929504395
Epoch 2340, val loss: 1.1632508039474487
Epoch 2350, training loss: 620.84130859375 = 0.02572173997759819 + 100.0 * 6.208156108856201
Epoch 2350, val loss: 1.1654846668243408
Epoch 2360, training loss: 620.740966796875 = 0.02535293810069561 + 100.0 * 6.207156658172607
Epoch 2360, val loss: 1.1681374311447144
Epoch 2370, training loss: 620.7302856445312 = 0.025003662332892418 + 100.0 * 6.207052707672119
Epoch 2370, val loss: 1.1708025932312012
Epoch 2380, training loss: 620.7172241210938 = 0.02467275783419609 + 100.0 * 6.206925868988037
Epoch 2380, val loss: 1.1736069917678833
Epoch 2390, training loss: 621.3165283203125 = 0.02434936910867691 + 100.0 * 6.212921619415283
Epoch 2390, val loss: 1.17575204372406
Epoch 2400, training loss: 620.63623046875 = 0.024010032415390015 + 100.0 * 6.206122398376465
Epoch 2400, val loss: 1.1787348985671997
Epoch 2410, training loss: 620.7269287109375 = 0.02368783950805664 + 100.0 * 6.207032680511475
Epoch 2410, val loss: 1.1813594102859497
Epoch 2420, training loss: 621.03271484375 = 0.02337263524532318 + 100.0 * 6.2100934982299805
Epoch 2420, val loss: 1.183924913406372
Epoch 2430, training loss: 620.756591796875 = 0.02305721305310726 + 100.0 * 6.207335472106934
Epoch 2430, val loss: 1.186745524406433
Epoch 2440, training loss: 620.719970703125 = 0.022755613550543785 + 100.0 * 6.206972122192383
Epoch 2440, val loss: 1.1891995668411255
Epoch 2450, training loss: 620.8276977539062 = 0.022464245557785034 + 100.0 * 6.208052635192871
Epoch 2450, val loss: 1.1919665336608887
Epoch 2460, training loss: 620.6770629882812 = 0.02216721884906292 + 100.0 * 6.206549167633057
Epoch 2460, val loss: 1.1943730115890503
Epoch 2470, training loss: 620.7260131835938 = 0.021883312612771988 + 100.0 * 6.207041263580322
Epoch 2470, val loss: 1.1967380046844482
Epoch 2480, training loss: 621.0146484375 = 0.021600814536213875 + 100.0 * 6.209930419921875
Epoch 2480, val loss: 1.1991955041885376
Epoch 2490, training loss: 620.6676025390625 = 0.02131453901529312 + 100.0 * 6.206462860107422
Epoch 2490, val loss: 1.201611876487732
Epoch 2500, training loss: 620.5856323242188 = 0.021046863868832588 + 100.0 * 6.205645561218262
Epoch 2500, val loss: 1.2042030096054077
Epoch 2510, training loss: 620.5368041992188 = 0.020786607638001442 + 100.0 * 6.205160140991211
Epoch 2510, val loss: 1.206735610961914
Epoch 2520, training loss: 621.3892822265625 = 0.020539093762636185 + 100.0 * 6.213686943054199
Epoch 2520, val loss: 1.2093485593795776
Epoch 2530, training loss: 620.916748046875 = 0.020264387130737305 + 100.0 * 6.208964824676514
Epoch 2530, val loss: 1.2113323211669922
Epoch 2540, training loss: 620.54736328125 = 0.02000442147254944 + 100.0 * 6.205273628234863
Epoch 2540, val loss: 1.2137237787246704
Epoch 2550, training loss: 620.4732055664062 = 0.019758470356464386 + 100.0 * 6.20453405380249
Epoch 2550, val loss: 1.216173768043518
Epoch 2560, training loss: 620.707275390625 = 0.01952863298356533 + 100.0 * 6.206877708435059
Epoch 2560, val loss: 1.2184969186782837
Epoch 2570, training loss: 620.614501953125 = 0.019286107271909714 + 100.0 * 6.205952167510986
Epoch 2570, val loss: 1.2208011150360107
Epoch 2580, training loss: 620.4074096679688 = 0.019038762897253036 + 100.0 * 6.203883647918701
Epoch 2580, val loss: 1.2230262756347656
Epoch 2590, training loss: 620.5989990234375 = 0.018808627501130104 + 100.0 * 6.205801963806152
Epoch 2590, val loss: 1.2252726554870605
Epoch 2600, training loss: 620.6015625 = 0.018585318699479103 + 100.0 * 6.205829620361328
Epoch 2600, val loss: 1.2275609970092773
Epoch 2610, training loss: 620.7192993164062 = 0.018364375457167625 + 100.0 * 6.207009315490723
Epoch 2610, val loss: 1.2300301790237427
Epoch 2620, training loss: 620.4244995117188 = 0.018140140920877457 + 100.0 * 6.204063892364502
Epoch 2620, val loss: 1.2321254014968872
Epoch 2630, training loss: 620.2998046875 = 0.017929840832948685 + 100.0 * 6.202818870544434
Epoch 2630, val loss: 1.2345556020736694
Epoch 2640, training loss: 620.3865966796875 = 0.01772886887192726 + 100.0 * 6.203689098358154
Epoch 2640, val loss: 1.236938714981079
Epoch 2650, training loss: 620.8224487304688 = 0.01753060705959797 + 100.0 * 6.208049297332764
Epoch 2650, val loss: 1.238815188407898
Epoch 2660, training loss: 620.5944213867188 = 0.017315467819571495 + 100.0 * 6.205770969390869
Epoch 2660, val loss: 1.240985631942749
Epoch 2670, training loss: 620.641845703125 = 0.017118828371167183 + 100.0 * 6.206247806549072
Epoch 2670, val loss: 1.2431731224060059
Epoch 2680, training loss: 620.391357421875 = 0.01692064292728901 + 100.0 * 6.203744411468506
Epoch 2680, val loss: 1.2456386089324951
Epoch 2690, training loss: 620.4571533203125 = 0.016728360205888748 + 100.0 * 6.204404354095459
Epoch 2690, val loss: 1.2478457689285278
Epoch 2700, training loss: 620.2659301757812 = 0.016538944095373154 + 100.0 * 6.202493667602539
Epoch 2700, val loss: 1.2500001192092896
Epoch 2710, training loss: 620.2625732421875 = 0.016358215361833572 + 100.0 * 6.202462196350098
Epoch 2710, val loss: 1.2521748542785645
Epoch 2720, training loss: 620.510986328125 = 0.01618138514459133 + 100.0 * 6.2049479484558105
Epoch 2720, val loss: 1.2539855241775513
Epoch 2730, training loss: 620.3925170898438 = 0.01599903218448162 + 100.0 * 6.203765392303467
Epoch 2730, val loss: 1.2562898397445679
Epoch 2740, training loss: 620.4099731445312 = 0.01582207717001438 + 100.0 * 6.203941345214844
Epoch 2740, val loss: 1.2585970163345337
Epoch 2750, training loss: 620.4178466796875 = 0.015644386410713196 + 100.0 * 6.20402193069458
Epoch 2750, val loss: 1.2603703737258911
Epoch 2760, training loss: 620.4722900390625 = 0.015477697364985943 + 100.0 * 6.204567909240723
Epoch 2760, val loss: 1.2627863883972168
Epoch 2770, training loss: 620.2099609375 = 0.015306014567613602 + 100.0 * 6.201946258544922
Epoch 2770, val loss: 1.2649229764938354
Epoch 2780, training loss: 620.2606201171875 = 0.015147264115512371 + 100.0 * 6.202455043792725
Epoch 2780, val loss: 1.2671043872833252
Epoch 2790, training loss: 620.32763671875 = 0.014989248476922512 + 100.0 * 6.203126430511475
Epoch 2790, val loss: 1.2690036296844482
Epoch 2800, training loss: 620.2177734375 = 0.01482637133449316 + 100.0 * 6.202029228210449
Epoch 2800, val loss: 1.2709790468215942
Epoch 2810, training loss: 620.3530883789062 = 0.014674520120024681 + 100.0 * 6.2033843994140625
Epoch 2810, val loss: 1.2733261585235596
Epoch 2820, training loss: 620.4271850585938 = 0.014521101489663124 + 100.0 * 6.204126834869385
Epoch 2820, val loss: 1.2752376794815063
Epoch 2830, training loss: 620.12158203125 = 0.01436252798885107 + 100.0 * 6.2010722160339355
Epoch 2830, val loss: 1.2771859169006348
Epoch 2840, training loss: 620.1259155273438 = 0.014217336662113667 + 100.0 * 6.201117038726807
Epoch 2840, val loss: 1.2794067859649658
Epoch 2850, training loss: 620.5911254882812 = 0.014079085551202297 + 100.0 * 6.205770492553711
Epoch 2850, val loss: 1.28140127658844
Epoch 2860, training loss: 620.1700439453125 = 0.0139243109151721 + 100.0 * 6.201561450958252
Epoch 2860, val loss: 1.2832376956939697
Epoch 2870, training loss: 620.0198974609375 = 0.01378290168941021 + 100.0 * 6.200060844421387
Epoch 2870, val loss: 1.2853267192840576
Epoch 2880, training loss: 620.0416870117188 = 0.013648923486471176 + 100.0 * 6.20028018951416
Epoch 2880, val loss: 1.2874006032943726
Epoch 2890, training loss: 620.4578857421875 = 0.013518759049475193 + 100.0 * 6.20444393157959
Epoch 2890, val loss: 1.2892258167266846
Epoch 2900, training loss: 620.2234497070312 = 0.013373540714383125 + 100.0 * 6.20210075378418
Epoch 2900, val loss: 1.290697693824768
Epoch 2910, training loss: 620.1929321289062 = 0.013237264938652515 + 100.0 * 6.201797008514404
Epoch 2910, val loss: 1.2926290035247803
Epoch 2920, training loss: 620.0779418945312 = 0.013105323538184166 + 100.0 * 6.200648307800293
Epoch 2920, val loss: 1.2946536540985107
Epoch 2930, training loss: 620.2301635742188 = 0.012978446669876575 + 100.0 * 6.202171325683594
Epoch 2930, val loss: 1.2968716621398926
Epoch 2940, training loss: 619.9601440429688 = 0.012851250357925892 + 100.0 * 6.199472427368164
Epoch 2940, val loss: 1.2986098527908325
Epoch 2950, training loss: 619.9629516601562 = 0.012726429849863052 + 100.0 * 6.199501991271973
Epoch 2950, val loss: 1.3005108833312988
Epoch 2960, training loss: 619.9288940429688 = 0.012609665282070637 + 100.0 * 6.199162483215332
Epoch 2960, val loss: 1.3025747537612915
Epoch 2970, training loss: 620.900146484375 = 0.012500766664743423 + 100.0 * 6.208876132965088
Epoch 2970, val loss: 1.3047610521316528
Epoch 2980, training loss: 620.3319702148438 = 0.012367147020995617 + 100.0 * 6.203195571899414
Epoch 2980, val loss: 1.3054888248443604
Epoch 2990, training loss: 619.982421875 = 0.012244337238371372 + 100.0 * 6.19970178604126
Epoch 2990, val loss: 1.3076727390289307
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 861.6504516601562 = 1.9673470258712769 + 100.0 * 8.596831321716309
Epoch 0, val loss: 1.9611574411392212
Epoch 10, training loss: 861.548095703125 = 1.957265019416809 + 100.0 * 8.595908164978027
Epoch 10, val loss: 1.950571894645691
Epoch 20, training loss: 860.8601684570312 = 1.9447516202926636 + 100.0 * 8.589154243469238
Epoch 20, val loss: 1.9375368356704712
Epoch 30, training loss: 856.3399658203125 = 1.928481936454773 + 100.0 * 8.54411506652832
Epoch 30, val loss: 1.9208446741104126
Epoch 40, training loss: 828.2573852539062 = 1.908377766609192 + 100.0 * 8.263489723205566
Epoch 40, val loss: 1.900811791419983
Epoch 50, training loss: 768.98388671875 = 1.8841636180877686 + 100.0 * 7.670997619628906
Epoch 50, val loss: 1.8772377967834473
Epoch 60, training loss: 754.4917602539062 = 1.8644198179244995 + 100.0 * 7.526273250579834
Epoch 60, val loss: 1.8591370582580566
Epoch 70, training loss: 738.18701171875 = 1.8483377695083618 + 100.0 * 7.363386631011963
Epoch 70, val loss: 1.8439302444458008
Epoch 80, training loss: 720.9439697265625 = 1.8334475755691528 + 100.0 * 7.191105365753174
Epoch 80, val loss: 1.8303834199905396
Epoch 90, training loss: 707.4679565429688 = 1.8232437372207642 + 100.0 * 7.0564470291137695
Epoch 90, val loss: 1.8218517303466797
Epoch 100, training loss: 699.3060913085938 = 1.8137292861938477 + 100.0 * 6.974923610687256
Epoch 100, val loss: 1.812482476234436
Epoch 110, training loss: 692.9700317382812 = 1.8003835678100586 + 100.0 * 6.911696910858154
Epoch 110, val loss: 1.8002766370773315
Epoch 120, training loss: 686.401123046875 = 1.7902823686599731 + 100.0 * 6.846108436584473
Epoch 120, val loss: 1.7915009260177612
Epoch 130, training loss: 680.2614135742188 = 1.7812755107879639 + 100.0 * 6.784801483154297
Epoch 130, val loss: 1.7833760976791382
Epoch 140, training loss: 674.7828979492188 = 1.7712469100952148 + 100.0 * 6.730116844177246
Epoch 140, val loss: 1.774336338043213
Epoch 150, training loss: 669.3781127929688 = 1.7618657350540161 + 100.0 * 6.6761627197265625
Epoch 150, val loss: 1.7659696340560913
Epoch 160, training loss: 665.2727661132812 = 1.752274513244629 + 100.0 * 6.635204792022705
Epoch 160, val loss: 1.7574962377548218
Epoch 170, training loss: 662.30322265625 = 1.7412744760513306 + 100.0 * 6.605619430541992
Epoch 170, val loss: 1.7474086284637451
Epoch 180, training loss: 659.922119140625 = 1.7293400764465332 + 100.0 * 6.581927299499512
Epoch 180, val loss: 1.73671293258667
Epoch 190, training loss: 657.78955078125 = 1.7167996168136597 + 100.0 * 6.560727596282959
Epoch 190, val loss: 1.7254070043563843
Epoch 200, training loss: 655.7578735351562 = 1.7037028074264526 + 100.0 * 6.540542125701904
Epoch 200, val loss: 1.7136001586914062
Epoch 210, training loss: 653.9701538085938 = 1.6897698640823364 + 100.0 * 6.522804260253906
Epoch 210, val loss: 1.7011511325836182
Epoch 220, training loss: 652.3715209960938 = 1.6751132011413574 + 100.0 * 6.506964206695557
Epoch 220, val loss: 1.6879585981369019
Epoch 230, training loss: 650.6304321289062 = 1.6591756343841553 + 100.0 * 6.489712715148926
Epoch 230, val loss: 1.673945665359497
Epoch 240, training loss: 649.4672241210938 = 1.6422220468521118 + 100.0 * 6.478250026702881
Epoch 240, val loss: 1.658971905708313
Epoch 250, training loss: 648.2151489257812 = 1.624234676361084 + 100.0 * 6.465909481048584
Epoch 250, val loss: 1.6430270671844482
Epoch 260, training loss: 646.9217529296875 = 1.6051197052001953 + 100.0 * 6.453166484832764
Epoch 260, val loss: 1.6261825561523438
Epoch 270, training loss: 645.8749389648438 = 1.5850844383239746 + 100.0 * 6.442898273468018
Epoch 270, val loss: 1.6084212064743042
Epoch 280, training loss: 644.9563598632812 = 1.5639472007751465 + 100.0 * 6.433923721313477
Epoch 280, val loss: 1.5896586179733276
Epoch 290, training loss: 644.0375366210938 = 1.5418083667755127 + 100.0 * 6.424957275390625
Epoch 290, val loss: 1.5701475143432617
Epoch 300, training loss: 643.3412475585938 = 1.5188721418380737 + 100.0 * 6.418223857879639
Epoch 300, val loss: 1.5498775243759155
Epoch 310, training loss: 642.7550048828125 = 1.495133876800537 + 100.0 * 6.412599086761475
Epoch 310, val loss: 1.5288236141204834
Epoch 320, training loss: 641.8861694335938 = 1.4707508087158203 + 100.0 * 6.404153823852539
Epoch 320, val loss: 1.5075403451919556
Epoch 330, training loss: 641.376708984375 = 1.445844292640686 + 100.0 * 6.399308681488037
Epoch 330, val loss: 1.4856648445129395
Epoch 340, training loss: 640.4881591796875 = 1.42044198513031 + 100.0 * 6.390676975250244
Epoch 340, val loss: 1.463614821434021
Epoch 350, training loss: 640.0223388671875 = 1.3948726654052734 + 100.0 * 6.386274337768555
Epoch 350, val loss: 1.4414777755737305
Epoch 360, training loss: 639.5797729492188 = 1.3690561056137085 + 100.0 * 6.382106781005859
Epoch 360, val loss: 1.4191590547561646
Epoch 370, training loss: 638.777099609375 = 1.343194842338562 + 100.0 * 6.3743391036987305
Epoch 370, val loss: 1.3969414234161377
Epoch 380, training loss: 638.30078125 = 1.317106008529663 + 100.0 * 6.369837284088135
Epoch 380, val loss: 1.3749138116836548
Epoch 390, training loss: 638.0501708984375 = 1.2910442352294922 + 100.0 * 6.367591381072998
Epoch 390, val loss: 1.3527092933654785
Epoch 400, training loss: 637.296142578125 = 1.2651033401489258 + 100.0 * 6.3603105545043945
Epoch 400, val loss: 1.3310465812683105
Epoch 410, training loss: 636.755126953125 = 1.2393085956573486 + 100.0 * 6.35515832901001
Epoch 410, val loss: 1.309697151184082
Epoch 420, training loss: 637.7568969726562 = 1.213731288909912 + 100.0 * 6.365431308746338
Epoch 420, val loss: 1.2885991334915161
Epoch 430, training loss: 636.4642944335938 = 1.1881237030029297 + 100.0 * 6.352761745452881
Epoch 430, val loss: 1.2678918838500977
Epoch 440, training loss: 635.611083984375 = 1.1632540225982666 + 100.0 * 6.344478130340576
Epoch 440, val loss: 1.2478268146514893
Epoch 450, training loss: 635.1878662109375 = 1.1388498544692993 + 100.0 * 6.340489864349365
Epoch 450, val loss: 1.228429913520813
Epoch 460, training loss: 635.2859497070312 = 1.114985704421997 + 100.0 * 6.341709613800049
Epoch 460, val loss: 1.209671974182129
Epoch 470, training loss: 634.7006225585938 = 1.0912954807281494 + 100.0 * 6.336092948913574
Epoch 470, val loss: 1.1910886764526367
Epoch 480, training loss: 634.40966796875 = 1.068281650543213 + 100.0 * 6.333413600921631
Epoch 480, val loss: 1.1733965873718262
Epoch 490, training loss: 634.0745849609375 = 1.0457990169525146 + 100.0 * 6.330287933349609
Epoch 490, val loss: 1.1564558744430542
Epoch 500, training loss: 633.611083984375 = 1.0239875316619873 + 100.0 * 6.325870990753174
Epoch 500, val loss: 1.1402435302734375
Epoch 510, training loss: 633.2705688476562 = 1.0029118061065674 + 100.0 * 6.322676658630371
Epoch 510, val loss: 1.124686360359192
Epoch 520, training loss: 633.0560302734375 = 0.982414186000824 + 100.0 * 6.320735931396484
Epoch 520, val loss: 1.1098676919937134
Epoch 530, training loss: 633.400390625 = 0.9625149369239807 + 100.0 * 6.324378967285156
Epoch 530, val loss: 1.0955638885498047
Epoch 540, training loss: 632.642333984375 = 0.9429934024810791 + 100.0 * 6.316993236541748
Epoch 540, val loss: 1.081845760345459
Epoch 550, training loss: 632.2444458007812 = 0.9240979552268982 + 100.0 * 6.313203811645508
Epoch 550, val loss: 1.0688022375106812
Epoch 560, training loss: 632.1240844726562 = 0.9058884978294373 + 100.0 * 6.3121819496154785
Epoch 560, val loss: 1.0564228296279907
Epoch 570, training loss: 631.864501953125 = 0.8880032300949097 + 100.0 * 6.309764862060547
Epoch 570, val loss: 1.0446406602859497
Epoch 580, training loss: 631.8314208984375 = 0.8706855773925781 + 100.0 * 6.30960750579834
Epoch 580, val loss: 1.0332326889038086
Epoch 590, training loss: 631.2896118164062 = 0.8537155985832214 + 100.0 * 6.304359436035156
Epoch 590, val loss: 1.0223333835601807
Epoch 600, training loss: 631.2673950195312 = 0.8372284173965454 + 100.0 * 6.304301738739014
Epoch 600, val loss: 1.0120210647583008
Epoch 610, training loss: 631.1790771484375 = 0.8211489319801331 + 100.0 * 6.303579330444336
Epoch 610, val loss: 1.002181887626648
Epoch 620, training loss: 630.9780883789062 = 0.8052268624305725 + 100.0 * 6.301728248596191
Epoch 620, val loss: 0.9922858476638794
Epoch 630, training loss: 630.69384765625 = 0.7897277474403381 + 100.0 * 6.299041271209717
Epoch 630, val loss: 0.9832518696784973
Epoch 640, training loss: 630.3504638671875 = 0.774459183216095 + 100.0 * 6.295759677886963
Epoch 640, val loss: 0.9743947386741638
Epoch 650, training loss: 630.162841796875 = 0.7596742510795593 + 100.0 * 6.294031143188477
Epoch 650, val loss: 0.965901255607605
Epoch 660, training loss: 630.4288940429688 = 0.744995653629303 + 100.0 * 6.296838760375977
Epoch 660, val loss: 0.9577693343162537
Epoch 670, training loss: 630.208984375 = 0.7304055094718933 + 100.0 * 6.294785976409912
Epoch 670, val loss: 0.9498917460441589
Epoch 680, training loss: 629.6137084960938 = 0.7161610126495361 + 100.0 * 6.288975238800049
Epoch 680, val loss: 0.9422537684440613
Epoch 690, training loss: 629.4249877929688 = 0.7020624876022339 + 100.0 * 6.287229061126709
Epoch 690, val loss: 0.9350293278694153
Epoch 700, training loss: 630.2612915039062 = 0.6880269646644592 + 100.0 * 6.295732498168945
Epoch 700, val loss: 0.9278108477592468
Epoch 710, training loss: 629.1321411132812 = 0.6740733981132507 + 100.0 * 6.284580707550049
Epoch 710, val loss: 0.9207513928413391
Epoch 720, training loss: 629.0421752929688 = 0.660283088684082 + 100.0 * 6.28381872177124
Epoch 720, val loss: 0.9138461947441101
Epoch 730, training loss: 628.8251342773438 = 0.6466711759567261 + 100.0 * 6.281784534454346
Epoch 730, val loss: 0.9074575901031494
Epoch 740, training loss: 629.4843139648438 = 0.6331353187561035 + 100.0 * 6.288512229919434
Epoch 740, val loss: 0.901157796382904
Epoch 750, training loss: 628.5743408203125 = 0.6194191575050354 + 100.0 * 6.2795491218566895
Epoch 750, val loss: 0.8946863412857056
Epoch 760, training loss: 628.4481811523438 = 0.60589599609375 + 100.0 * 6.278422832489014
Epoch 760, val loss: 0.888629138469696
Epoch 770, training loss: 628.2955322265625 = 0.5926405787467957 + 100.0 * 6.277028560638428
Epoch 770, val loss: 0.8829319477081299
Epoch 780, training loss: 628.5164184570312 = 0.5793929696083069 + 100.0 * 6.279370307922363
Epoch 780, val loss: 0.8775006532669067
Epoch 790, training loss: 628.4635009765625 = 0.5661594271659851 + 100.0 * 6.27897310256958
Epoch 790, val loss: 0.8720953464508057
Epoch 800, training loss: 627.9429931640625 = 0.5530837178230286 + 100.0 * 6.273899078369141
Epoch 800, val loss: 0.8670462369918823
Epoch 810, training loss: 627.7686157226562 = 0.5402867794036865 + 100.0 * 6.272283554077148
Epoch 810, val loss: 0.8625052571296692
Epoch 820, training loss: 628.0556640625 = 0.5276108384132385 + 100.0 * 6.275280952453613
Epoch 820, val loss: 0.8580455183982849
Epoch 830, training loss: 627.5624389648438 = 0.5149887204170227 + 100.0 * 6.270473957061768
Epoch 830, val loss: 0.8540952205657959
Epoch 840, training loss: 627.4781494140625 = 0.5026085376739502 + 100.0 * 6.2697553634643555
Epoch 840, val loss: 0.8500602841377258
Epoch 850, training loss: 628.01025390625 = 0.49045297503471375 + 100.0 * 6.275197982788086
Epoch 850, val loss: 0.8466598391532898
Epoch 860, training loss: 627.392822265625 = 0.47837764024734497 + 100.0 * 6.269144535064697
Epoch 860, val loss: 0.8432866930961609
Epoch 870, training loss: 627.1553955078125 = 0.46662333607673645 + 100.0 * 6.266887664794922
Epoch 870, val loss: 0.8406158089637756
Epoch 880, training loss: 627.0013427734375 = 0.45513540506362915 + 100.0 * 6.2654619216918945
Epoch 880, val loss: 0.8380160331726074
Epoch 890, training loss: 626.974609375 = 0.4438687562942505 + 100.0 * 6.265307426452637
Epoch 890, val loss: 0.8358963131904602
Epoch 900, training loss: 627.1162719726562 = 0.43274515867233276 + 100.0 * 6.2668352127075195
Epoch 900, val loss: 0.8339042067527771
Epoch 910, training loss: 627.1395874023438 = 0.4218657612800598 + 100.0 * 6.267177104949951
Epoch 910, val loss: 0.8322403430938721
Epoch 920, training loss: 626.6460571289062 = 0.41134750843048096 + 100.0 * 6.2623467445373535
Epoch 920, val loss: 0.8310115337371826
Epoch 930, training loss: 626.5048828125 = 0.40115106105804443 + 100.0 * 6.261037826538086
Epoch 930, val loss: 0.8301730155944824
Epoch 940, training loss: 627.0842895507812 = 0.3911210000514984 + 100.0 * 6.266931533813477
Epoch 940, val loss: 0.829474151134491
Epoch 950, training loss: 626.8519897460938 = 0.3813185393810272 + 100.0 * 6.264706134796143
Epoch 950, val loss: 0.8292382955551147
Epoch 960, training loss: 626.3405151367188 = 0.3716511130332947 + 100.0 * 6.259688854217529
Epoch 960, val loss: 0.8290106654167175
Epoch 970, training loss: 626.9003295898438 = 0.36241501569747925 + 100.0 * 6.265378952026367
Epoch 970, val loss: 0.82933109998703
Epoch 980, training loss: 626.3009033203125 = 0.3532218039035797 + 100.0 * 6.259477138519287
Epoch 980, val loss: 0.8294790983200073
Epoch 990, training loss: 626.0848388671875 = 0.3443986177444458 + 100.0 * 6.257404327392578
Epoch 990, val loss: 0.8302269577980042
Epoch 1000, training loss: 625.989501953125 = 0.33576062321662903 + 100.0 * 6.256537437438965
Epoch 1000, val loss: 0.8308714628219604
Epoch 1010, training loss: 626.6029052734375 = 0.32735103368759155 + 100.0 * 6.262755870819092
Epoch 1010, val loss: 0.8318376541137695
Epoch 1020, training loss: 626.0565795898438 = 0.31906095147132874 + 100.0 * 6.257375240325928
Epoch 1020, val loss: 0.8331742882728577
Epoch 1030, training loss: 625.9589233398438 = 0.3110262155532837 + 100.0 * 6.256478786468506
Epoch 1030, val loss: 0.8344334959983826
Epoch 1040, training loss: 625.6756591796875 = 0.30321332812309265 + 100.0 * 6.253724575042725
Epoch 1040, val loss: 0.8359365463256836
Epoch 1050, training loss: 625.5836181640625 = 0.2956255078315735 + 100.0 * 6.252879619598389
Epoch 1050, val loss: 0.8378265500068665
Epoch 1060, training loss: 625.478759765625 = 0.28823667764663696 + 100.0 * 6.25190544128418
Epoch 1060, val loss: 0.8396382927894592
Epoch 1070, training loss: 625.5316772460938 = 0.281038761138916 + 100.0 * 6.252506256103516
Epoch 1070, val loss: 0.8417256474494934
Epoch 1080, training loss: 625.4561767578125 = 0.27391618490219116 + 100.0 * 6.251822471618652
Epoch 1080, val loss: 0.8438267707824707
Epoch 1090, training loss: 625.6307373046875 = 0.26699623465538025 + 100.0 * 6.253637790679932
Epoch 1090, val loss: 0.8460206389427185
Epoch 1100, training loss: 625.1880493164062 = 0.26031413674354553 + 100.0 * 6.249277114868164
Epoch 1100, val loss: 0.8485326170921326
Epoch 1110, training loss: 625.2129516601562 = 0.2538379430770874 + 100.0 * 6.249590873718262
Epoch 1110, val loss: 0.8511345982551575
Epoch 1120, training loss: 625.82763671875 = 0.24750971794128418 + 100.0 * 6.255801677703857
Epoch 1120, val loss: 0.853693962097168
Epoch 1130, training loss: 625.7402954101562 = 0.24123965203762054 + 100.0 * 6.254991054534912
Epoch 1130, val loss: 0.8564794659614563
Epoch 1140, training loss: 625.1261596679688 = 0.23515041172504425 + 100.0 * 6.248909950256348
Epoch 1140, val loss: 0.8593213558197021
Epoch 1150, training loss: 624.8978271484375 = 0.22928620874881744 + 100.0 * 6.246685028076172
Epoch 1150, val loss: 0.8622267842292786
Epoch 1160, training loss: 624.912841796875 = 0.22360113263130188 + 100.0 * 6.246892929077148
Epoch 1160, val loss: 0.865362286567688
Epoch 1170, training loss: 624.9664916992188 = 0.21806329488754272 + 100.0 * 6.24748420715332
Epoch 1170, val loss: 0.8685110211372375
Epoch 1180, training loss: 625.30322265625 = 0.21266113221645355 + 100.0 * 6.250905990600586
Epoch 1180, val loss: 0.8719189167022705
Epoch 1190, training loss: 624.9708251953125 = 0.2071937620639801 + 100.0 * 6.247635841369629
Epoch 1190, val loss: 0.8748899698257446
Epoch 1200, training loss: 624.8468017578125 = 0.20208021998405457 + 100.0 * 6.2464470863342285
Epoch 1200, val loss: 0.8784024715423584
Epoch 1210, training loss: 624.8258666992188 = 0.19701477885246277 + 100.0 * 6.246288299560547
Epoch 1210, val loss: 0.8817846775054932
Epoch 1220, training loss: 624.54052734375 = 0.19216078519821167 + 100.0 * 6.243484020233154
Epoch 1220, val loss: 0.8854736685752869
Epoch 1230, training loss: 624.46240234375 = 0.18739421665668488 + 100.0 * 6.24275016784668
Epoch 1230, val loss: 0.8890779614448547
Epoch 1240, training loss: 624.5136108398438 = 0.18279875814914703 + 100.0 * 6.243308067321777
Epoch 1240, val loss: 0.8929055333137512
Epoch 1250, training loss: 624.5763549804688 = 0.17827002704143524 + 100.0 * 6.243980407714844
Epoch 1250, val loss: 0.8965162634849548
Epoch 1260, training loss: 625.1448364257812 = 0.17384596168994904 + 100.0 * 6.2497100830078125
Epoch 1260, val loss: 0.9002408385276794
Epoch 1270, training loss: 624.52587890625 = 0.16951297223567963 + 100.0 * 6.243563652038574
Epoch 1270, val loss: 0.9039484858512878
Epoch 1280, training loss: 624.3387451171875 = 0.16529440879821777 + 100.0 * 6.241734504699707
Epoch 1280, val loss: 0.907824695110321
Epoch 1290, training loss: 624.1309814453125 = 0.16124534606933594 + 100.0 * 6.239697456359863
Epoch 1290, val loss: 0.9117127060890198
Epoch 1300, training loss: 624.0476684570312 = 0.15733174979686737 + 100.0 * 6.238903045654297
Epoch 1300, val loss: 0.9157789945602417
Epoch 1310, training loss: 625.0810546875 = 0.15354597568511963 + 100.0 * 6.249275207519531
Epoch 1310, val loss: 0.9197478294372559
Epoch 1320, training loss: 624.3961791992188 = 0.14968056976795197 + 100.0 * 6.242465019226074
Epoch 1320, val loss: 0.9236299395561218
Epoch 1330, training loss: 624.1093139648438 = 0.1460389345884323 + 100.0 * 6.239632606506348
Epoch 1330, val loss: 0.9276748299598694
Epoch 1340, training loss: 624.0902099609375 = 0.14247283339500427 + 100.0 * 6.239477634429932
Epoch 1340, val loss: 0.9317286610603333
Epoch 1350, training loss: 623.82275390625 = 0.13903750479221344 + 100.0 * 6.236837387084961
Epoch 1350, val loss: 0.9358660578727722
Epoch 1360, training loss: 623.8560180664062 = 0.1357109099626541 + 100.0 * 6.237203598022461
Epoch 1360, val loss: 0.939975917339325
Epoch 1370, training loss: 624.2499389648438 = 0.13243570923805237 + 100.0 * 6.241174697875977
Epoch 1370, val loss: 0.9439672827720642
Epoch 1380, training loss: 623.8302612304688 = 0.12920279800891876 + 100.0 * 6.237010478973389
Epoch 1380, val loss: 0.9480957388877869
Epoch 1390, training loss: 623.665283203125 = 0.12608900666236877 + 100.0 * 6.235391616821289
Epoch 1390, val loss: 0.9522170424461365
Epoch 1400, training loss: 623.5458984375 = 0.12310244888067245 + 100.0 * 6.234227657318115
Epoch 1400, val loss: 0.9563771486282349
Epoch 1410, training loss: 624.2249145507812 = 0.1202336847782135 + 100.0 * 6.241046905517578
Epoch 1410, val loss: 0.9605805277824402
Epoch 1420, training loss: 623.7199096679688 = 0.11731119453907013 + 100.0 * 6.236026287078857
Epoch 1420, val loss: 0.9646303653717041
Epoch 1430, training loss: 623.6387329101562 = 0.11451563984155655 + 100.0 * 6.2352423667907715
Epoch 1430, val loss: 0.9687272906303406
Epoch 1440, training loss: 623.4317016601562 = 0.11183235049247742 + 100.0 * 6.233198642730713
Epoch 1440, val loss: 0.9729745388031006
Epoch 1450, training loss: 623.3853149414062 = 0.10924029350280762 + 100.0 * 6.232760906219482
Epoch 1450, val loss: 0.9771730899810791
Epoch 1460, training loss: 624.4197387695312 = 0.10671848058700562 + 100.0 * 6.243130683898926
Epoch 1460, val loss: 0.9813084602355957
Epoch 1470, training loss: 623.6423950195312 = 0.10422447323799133 + 100.0 * 6.235381603240967
Epoch 1470, val loss: 0.9854475259780884
Epoch 1480, training loss: 623.2999877929688 = 0.10178881138563156 + 100.0 * 6.2319817543029785
Epoch 1480, val loss: 0.9896038174629211
Epoch 1490, training loss: 623.4129028320312 = 0.09946110844612122 + 100.0 * 6.2331342697143555
Epoch 1490, val loss: 0.9938977360725403
Epoch 1500, training loss: 623.5044555664062 = 0.09716685116291046 + 100.0 * 6.234073162078857
Epoch 1500, val loss: 0.9980481266975403
Epoch 1510, training loss: 623.2049560546875 = 0.0949171632528305 + 100.0 * 6.231100559234619
Epoch 1510, val loss: 1.0021859407424927
Epoch 1520, training loss: 623.342041015625 = 0.09279803186655045 + 100.0 * 6.232492446899414
Epoch 1520, val loss: 1.0065844058990479
Epoch 1530, training loss: 623.34716796875 = 0.09067218750715256 + 100.0 * 6.232564449310303
Epoch 1530, val loss: 1.0107632875442505
Epoch 1540, training loss: 623.0809326171875 = 0.08860493451356888 + 100.0 * 6.229923248291016
Epoch 1540, val loss: 1.0150707960128784
Epoch 1550, training loss: 622.986328125 = 0.0866321250796318 + 100.0 * 6.228997230529785
Epoch 1550, val loss: 1.0194271802902222
Epoch 1560, training loss: 622.9163208007812 = 0.08470574021339417 + 100.0 * 6.228315830230713
Epoch 1560, val loss: 1.0237184762954712
Epoch 1570, training loss: 623.6363525390625 = 0.08286003023386002 + 100.0 * 6.23553466796875
Epoch 1570, val loss: 1.0280226469039917
Epoch 1580, training loss: 623.121826171875 = 0.0809989795088768 + 100.0 * 6.230408191680908
Epoch 1580, val loss: 1.0320507287979126
Epoch 1590, training loss: 622.9682006835938 = 0.07918623089790344 + 100.0 * 6.228890419006348
Epoch 1590, val loss: 1.0363177061080933
Epoch 1600, training loss: 622.8031005859375 = 0.07746594399213791 + 100.0 * 6.2272562980651855
Epoch 1600, val loss: 1.0405538082122803
Epoch 1610, training loss: 622.82763671875 = 0.07580028474330902 + 100.0 * 6.227518558502197
Epoch 1610, val loss: 1.0448176860809326
Epoch 1620, training loss: 623.4274291992188 = 0.07417235523462296 + 100.0 * 6.233532428741455
Epoch 1620, val loss: 1.048997402191162
Epoch 1630, training loss: 622.8403930664062 = 0.07254964858293533 + 100.0 * 6.227678298950195
Epoch 1630, val loss: 1.0531424283981323
Epoch 1640, training loss: 622.723876953125 = 0.07097773998975754 + 100.0 * 6.226529121398926
Epoch 1640, val loss: 1.0572490692138672
Epoch 1650, training loss: 622.8644409179688 = 0.06947094202041626 + 100.0 * 6.227950096130371
Epoch 1650, val loss: 1.0614547729492188
Epoch 1660, training loss: 622.7636108398438 = 0.06800113618373871 + 100.0 * 6.226956367492676
Epoch 1660, val loss: 1.0656239986419678
Epoch 1670, training loss: 623.1554565429688 = 0.06658101826906204 + 100.0 * 6.230888366699219
Epoch 1670, val loss: 1.0696874856948853
Epoch 1680, training loss: 622.7012329101562 = 0.0651685893535614 + 100.0 * 6.226360321044922
Epoch 1680, val loss: 1.0738162994384766
Epoch 1690, training loss: 622.5778198242188 = 0.06380879133939743 + 100.0 * 6.22514009475708
Epoch 1690, val loss: 1.0780029296875
Epoch 1700, training loss: 623.11279296875 = 0.06249463930726051 + 100.0 * 6.230503082275391
Epoch 1700, val loss: 1.082021951675415
Epoch 1710, training loss: 622.5567626953125 = 0.061207592487335205 + 100.0 * 6.2249555587768555
Epoch 1710, val loss: 1.0861124992370605
Epoch 1720, training loss: 622.4555053710938 = 0.059938132762908936 + 100.0 * 6.2239556312561035
Epoch 1720, val loss: 1.0902025699615479
Epoch 1730, training loss: 622.4111938476562 = 0.05872933566570282 + 100.0 * 6.223524570465088
Epoch 1730, val loss: 1.094274878501892
Epoch 1740, training loss: 622.8189697265625 = 0.057556428015232086 + 100.0 * 6.227613925933838
Epoch 1740, val loss: 1.09822678565979
Epoch 1750, training loss: 622.3203735351562 = 0.05638733506202698 + 100.0 * 6.222639560699463
Epoch 1750, val loss: 1.1023197174072266
Epoch 1760, training loss: 622.532470703125 = 0.05525156110525131 + 100.0 * 6.2247724533081055
Epoch 1760, val loss: 1.1062135696411133
Epoch 1770, training loss: 622.7816162109375 = 0.05415033921599388 + 100.0 * 6.2272748947143555
Epoch 1770, val loss: 1.1102277040481567
Epoch 1780, training loss: 622.3534545898438 = 0.05306748300790787 + 100.0 * 6.22300386428833
Epoch 1780, val loss: 1.114216923713684
Epoch 1790, training loss: 622.249267578125 = 0.052025627344846725 + 100.0 * 6.221972465515137
Epoch 1790, val loss: 1.118243932723999
Epoch 1800, training loss: 622.185546875 = 0.05101568624377251 + 100.0 * 6.221344947814941
Epoch 1800, val loss: 1.1221942901611328
Epoch 1810, training loss: 622.186279296875 = 0.05004521831870079 + 100.0 * 6.221362113952637
Epoch 1810, val loss: 1.1261281967163086
Epoch 1820, training loss: 623.0698852539062 = 0.049110736697912216 + 100.0 * 6.230207443237305
Epoch 1820, val loss: 1.1299762725830078
Epoch 1830, training loss: 622.6950073242188 = 0.048132676631212234 + 100.0 * 6.226468563079834
Epoch 1830, val loss: 1.133906364440918
Epoch 1840, training loss: 622.3224487304688 = 0.04719710722565651 + 100.0 * 6.222752571105957
Epoch 1840, val loss: 1.1376347541809082
Epoch 1850, training loss: 622.3178100585938 = 0.0462941974401474 + 100.0 * 6.222715377807617
Epoch 1850, val loss: 1.1416817903518677
Epoch 1860, training loss: 622.1361694335938 = 0.04542119801044464 + 100.0 * 6.220907688140869
Epoch 1860, val loss: 1.1454746723175049
Epoch 1870, training loss: 622.0675048828125 = 0.044571638107299805 + 100.0 * 6.220229625701904
Epoch 1870, val loss: 1.1494715213775635
Epoch 1880, training loss: 622.2333984375 = 0.04375206679105759 + 100.0 * 6.221896648406982
Epoch 1880, val loss: 1.1533210277557373
Epoch 1890, training loss: 622.2179565429688 = 0.042931243777275085 + 100.0 * 6.221750259399414
Epoch 1890, val loss: 1.1570631265640259
Epoch 1900, training loss: 622.0847778320312 = 0.04214074835181236 + 100.0 * 6.220426559448242
Epoch 1900, val loss: 1.160826325416565
Epoch 1910, training loss: 622.1661987304688 = 0.04138117656111717 + 100.0 * 6.221248149871826
Epoch 1910, val loss: 1.164707064628601
Epoch 1920, training loss: 622.3560791015625 = 0.04060760885477066 + 100.0 * 6.2231550216674805
Epoch 1920, val loss: 1.1684050559997559
Epoch 1930, training loss: 621.9200439453125 = 0.03988221660256386 + 100.0 * 6.218801975250244
Epoch 1930, val loss: 1.1721652746200562
Epoch 1940, training loss: 622.2892456054688 = 0.039166636765003204 + 100.0 * 6.222500324249268
Epoch 1940, val loss: 1.1759098768234253
Epoch 1950, training loss: 621.8388671875 = 0.0384550578892231 + 100.0 * 6.21800422668457
Epoch 1950, val loss: 1.1796468496322632
Epoch 1960, training loss: 621.719482421875 = 0.037768617272377014 + 100.0 * 6.2168169021606445
Epoch 1960, val loss: 1.1833322048187256
Epoch 1970, training loss: 621.7639770507812 = 0.037107981741428375 + 100.0 * 6.217268943786621
Epoch 1970, val loss: 1.187038779258728
Epoch 1980, training loss: 622.353759765625 = 0.03647928684949875 + 100.0 * 6.223172664642334
Epoch 1980, val loss: 1.1906955242156982
Epoch 1990, training loss: 622.129150390625 = 0.03582490608096123 + 100.0 * 6.220932960510254
Epoch 1990, val loss: 1.194261908531189
Epoch 2000, training loss: 621.9192504882812 = 0.035189006477594376 + 100.0 * 6.218840599060059
Epoch 2000, val loss: 1.1979225873947144
Epoch 2010, training loss: 621.7655029296875 = 0.034573521465063095 + 100.0 * 6.217309474945068
Epoch 2010, val loss: 1.201429843902588
Epoch 2020, training loss: 621.986572265625 = 0.033993519842624664 + 100.0 * 6.2195258140563965
Epoch 2020, val loss: 1.2050691843032837
Epoch 2030, training loss: 621.7864990234375 = 0.03340870514512062 + 100.0 * 6.217531204223633
Epoch 2030, val loss: 1.2086933851242065
Epoch 2040, training loss: 621.6683959960938 = 0.03284396603703499 + 100.0 * 6.216355323791504
Epoch 2040, val loss: 1.2122923135757446
Epoch 2050, training loss: 621.5877685546875 = 0.032294224947690964 + 100.0 * 6.215554714202881
Epoch 2050, val loss: 1.2158139944076538
Epoch 2060, training loss: 622.333251953125 = 0.03177453950047493 + 100.0 * 6.2230143547058105
Epoch 2060, val loss: 1.2193171977996826
Epoch 2070, training loss: 621.7576293945312 = 0.031212860718369484 + 100.0 * 6.217264652252197
Epoch 2070, val loss: 1.222672462463379
Epoch 2080, training loss: 621.5665893554688 = 0.030692221596837044 + 100.0 * 6.215358734130859
Epoch 2080, val loss: 1.2262845039367676
Epoch 2090, training loss: 621.477294921875 = 0.030184049159288406 + 100.0 * 6.214470863342285
Epoch 2090, val loss: 1.229741096496582
Epoch 2100, training loss: 621.618408203125 = 0.02970382384955883 + 100.0 * 6.21588659286499
Epoch 2100, val loss: 1.2333097457885742
Epoch 2110, training loss: 621.8287963867188 = 0.02922399714589119 + 100.0 * 6.217995643615723
Epoch 2110, val loss: 1.2366544008255005
Epoch 2120, training loss: 621.4803466796875 = 0.02873922698199749 + 100.0 * 6.2145161628723145
Epoch 2120, val loss: 1.2398988008499146
Epoch 2130, training loss: 621.3487548828125 = 0.028268929570913315 + 100.0 * 6.213204860687256
Epoch 2130, val loss: 1.243224024772644
Epoch 2140, training loss: 621.4746704101562 = 0.027827441692352295 + 100.0 * 6.214468479156494
Epoch 2140, val loss: 1.2466415166854858
Epoch 2150, training loss: 622.1049194335938 = 0.02740117534995079 + 100.0 * 6.220775127410889
Epoch 2150, val loss: 1.2497308254241943
Epoch 2160, training loss: 621.5537719726562 = 0.02695128321647644 + 100.0 * 6.215267658233643
Epoch 2160, val loss: 1.2531404495239258
Epoch 2170, training loss: 621.3414916992188 = 0.026518719270825386 + 100.0 * 6.2131500244140625
Epoch 2170, val loss: 1.2564351558685303
Epoch 2180, training loss: 621.2728881835938 = 0.02611163631081581 + 100.0 * 6.212467670440674
Epoch 2180, val loss: 1.2598096132278442
Epoch 2190, training loss: 621.5254516601562 = 0.02571798488497734 + 100.0 * 6.2149977684021
Epoch 2190, val loss: 1.263010025024414
Epoch 2200, training loss: 621.4168090820312 = 0.025311555713415146 + 100.0 * 6.21391487121582
Epoch 2200, val loss: 1.266237735748291
Epoch 2210, training loss: 621.5689697265625 = 0.024916868656873703 + 100.0 * 6.21544075012207
Epoch 2210, val loss: 1.2693263292312622
Epoch 2220, training loss: 621.2180786132812 = 0.024540556594729424 + 100.0 * 6.211935520172119
Epoch 2220, val loss: 1.2727161645889282
Epoch 2230, training loss: 621.1995849609375 = 0.02417299710214138 + 100.0 * 6.211754322052002
Epoch 2230, val loss: 1.275926113128662
Epoch 2240, training loss: 621.5751953125 = 0.0238201767206192 + 100.0 * 6.215514183044434
Epoch 2240, val loss: 1.278971791267395
Epoch 2250, training loss: 621.1810913085938 = 0.023453816771507263 + 100.0 * 6.211576461791992
Epoch 2250, val loss: 1.2821974754333496
Epoch 2260, training loss: 621.182373046875 = 0.023103080689907074 + 100.0 * 6.211592197418213
Epoch 2260, val loss: 1.2853543758392334
Epoch 2270, training loss: 621.1317749023438 = 0.022763272747397423 + 100.0 * 6.211090087890625
Epoch 2270, val loss: 1.2883793115615845
Epoch 2280, training loss: 621.1421508789062 = 0.022434677928686142 + 100.0 * 6.2111968994140625
Epoch 2280, val loss: 1.2914421558380127
Epoch 2290, training loss: 621.7158813476562 = 0.022116364911198616 + 100.0 * 6.216938018798828
Epoch 2290, val loss: 1.2944340705871582
Epoch 2300, training loss: 621.290771484375 = 0.02179352380335331 + 100.0 * 6.2126898765563965
Epoch 2300, val loss: 1.2976348400115967
Epoch 2310, training loss: 621.5809936523438 = 0.02148411236703396 + 100.0 * 6.215595245361328
Epoch 2310, val loss: 1.3004915714263916
Epoch 2320, training loss: 621.1610717773438 = 0.021162647753953934 + 100.0 * 6.211399078369141
Epoch 2320, val loss: 1.3036539554595947
Epoch 2330, training loss: 621.4426879882812 = 0.02086089551448822 + 100.0 * 6.2142181396484375
Epoch 2330, val loss: 1.3065223693847656
Epoch 2340, training loss: 621.0699462890625 = 0.020560547709465027 + 100.0 * 6.210494041442871
Epoch 2340, val loss: 1.3094959259033203
Epoch 2350, training loss: 621.0286254882812 = 0.020264672115445137 + 100.0 * 6.210083484649658
Epoch 2350, val loss: 1.3124760389328003
Epoch 2360, training loss: 621.1140747070312 = 0.019992738962173462 + 100.0 * 6.210940837860107
Epoch 2360, val loss: 1.3153555393218994
Epoch 2370, training loss: 621.4166870117188 = 0.019718211144208908 + 100.0 * 6.213969707489014
Epoch 2370, val loss: 1.3182419538497925
Epoch 2380, training loss: 621.2040405273438 = 0.019436035305261612 + 100.0 * 6.211846351623535
Epoch 2380, val loss: 1.3213032484054565
Epoch 2390, training loss: 621.005126953125 = 0.019167087972164154 + 100.0 * 6.209859848022461
Epoch 2390, val loss: 1.3240715265274048
Epoch 2400, training loss: 621.198486328125 = 0.01890799030661583 + 100.0 * 6.211795806884766
Epoch 2400, val loss: 1.3270519971847534
Epoch 2410, training loss: 620.973876953125 = 0.018649261444807053 + 100.0 * 6.209551811218262
Epoch 2410, val loss: 1.329802393913269
Epoch 2420, training loss: 621.1476440429688 = 0.018403368070721626 + 100.0 * 6.211292266845703
Epoch 2420, val loss: 1.3329235315322876
Epoch 2430, training loss: 621.0951538085938 = 0.01815180294215679 + 100.0 * 6.210770130157471
Epoch 2430, val loss: 1.3355058431625366
Epoch 2440, training loss: 621.2268676757812 = 0.017909690737724304 + 100.0 * 6.212090015411377
Epoch 2440, val loss: 1.3382948637008667
Epoch 2450, training loss: 621.0059204101562 = 0.01766807585954666 + 100.0 * 6.209882736206055
Epoch 2450, val loss: 1.341044306755066
Epoch 2460, training loss: 620.8110961914062 = 0.017429647967219353 + 100.0 * 6.2079362869262695
Epoch 2460, val loss: 1.3437875509262085
Epoch 2470, training loss: 620.8260498046875 = 0.017207162454724312 + 100.0 * 6.208088397979736
Epoch 2470, val loss: 1.346527338027954
Epoch 2480, training loss: 621.0261840820312 = 0.016984136775135994 + 100.0 * 6.210092067718506
Epoch 2480, val loss: 1.3491331338882446
Epoch 2490, training loss: 621.153076171875 = 0.016760144382715225 + 100.0 * 6.211362838745117
Epoch 2490, val loss: 1.3519352674484253
Epoch 2500, training loss: 620.8911743164062 = 0.01655043475329876 + 100.0 * 6.208746433258057
Epoch 2500, val loss: 1.3546245098114014
Epoch 2510, training loss: 620.7236328125 = 0.01633078046143055 + 100.0 * 6.207072734832764
Epoch 2510, val loss: 1.357200264930725
Epoch 2520, training loss: 620.7316284179688 = 0.016128549352288246 + 100.0 * 6.207155227661133
Epoch 2520, val loss: 1.359976887702942
Epoch 2530, training loss: 621.1383666992188 = 0.015936370939016342 + 100.0 * 6.211224555969238
Epoch 2530, val loss: 1.362553596496582
Epoch 2540, training loss: 620.887939453125 = 0.015725363045930862 + 100.0 * 6.20872163772583
Epoch 2540, val loss: 1.365191102027893
Epoch 2550, training loss: 620.9244995117188 = 0.015522520989179611 + 100.0 * 6.209089756011963
Epoch 2550, val loss: 1.3678921461105347
Epoch 2560, training loss: 620.6871948242188 = 0.015328280627727509 + 100.0 * 6.206718921661377
Epoch 2560, val loss: 1.3703536987304688
Epoch 2570, training loss: 620.6433715820312 = 0.015139534138143063 + 100.0 * 6.206282615661621
Epoch 2570, val loss: 1.3731403350830078
Epoch 2580, training loss: 621.1261596679688 = 0.014965704642236233 + 100.0 * 6.211112022399902
Epoch 2580, val loss: 1.3758620023727417
Epoch 2590, training loss: 620.6692504882812 = 0.014771352522075176 + 100.0 * 6.206544876098633
Epoch 2590, val loss: 1.3784369230270386
Epoch 2600, training loss: 620.6115112304688 = 0.014589302241802216 + 100.0 * 6.205969333648682
Epoch 2600, val loss: 1.3808790445327759
Epoch 2610, training loss: 620.5993041992188 = 0.014415428042411804 + 100.0 * 6.2058491706848145
Epoch 2610, val loss: 1.3834609985351562
Epoch 2620, training loss: 620.8140258789062 = 0.014241518452763557 + 100.0 * 6.207997798919678
Epoch 2620, val loss: 1.3860565423965454
Epoch 2630, training loss: 620.6941528320312 = 0.01407073624432087 + 100.0 * 6.20680046081543
Epoch 2630, val loss: 1.388508677482605
Epoch 2640, training loss: 620.6170654296875 = 0.013906541280448437 + 100.0 * 6.206031322479248
Epoch 2640, val loss: 1.391028642654419
Epoch 2650, training loss: 620.5831909179688 = 0.01373981311917305 + 100.0 * 6.205694675445557
Epoch 2650, val loss: 1.3933680057525635
Epoch 2660, training loss: 620.8062133789062 = 0.013585556298494339 + 100.0 * 6.207926273345947
Epoch 2660, val loss: 1.3957581520080566
Epoch 2670, training loss: 620.6525268554688 = 0.013425135985016823 + 100.0 * 6.206390857696533
Epoch 2670, val loss: 1.3984514474868774
Epoch 2680, training loss: 620.8228759765625 = 0.01327370386570692 + 100.0 * 6.208096504211426
Epoch 2680, val loss: 1.4008628129959106
Epoch 2690, training loss: 620.5869140625 = 0.013113357126712799 + 100.0 * 6.205738067626953
Epoch 2690, val loss: 1.4033948183059692
Epoch 2700, training loss: 620.41015625 = 0.012963845394551754 + 100.0 * 6.203972339630127
Epoch 2700, val loss: 1.4058120250701904
Epoch 2710, training loss: 620.3958129882812 = 0.012820272706449032 + 100.0 * 6.203830242156982
Epoch 2710, val loss: 1.4081382751464844
Epoch 2720, training loss: 620.5230712890625 = 0.0126781752333045 + 100.0 * 6.205103874206543
Epoch 2720, val loss: 1.410623550415039
Epoch 2730, training loss: 620.7531127929688 = 0.012538796290755272 + 100.0 * 6.207406044006348
Epoch 2730, val loss: 1.4129503965377808
Epoch 2740, training loss: 620.7921142578125 = 0.012401637621223927 + 100.0 * 6.207797050476074
Epoch 2740, val loss: 1.4153494834899902
Epoch 2750, training loss: 620.4688720703125 = 0.012255633249878883 + 100.0 * 6.20456600189209
Epoch 2750, val loss: 1.4176034927368164
Epoch 2760, training loss: 620.6791381835938 = 0.01212388463318348 + 100.0 * 6.206669807434082
Epoch 2760, val loss: 1.419930100440979
Epoch 2770, training loss: 620.6380004882812 = 0.011988546699285507 + 100.0 * 6.2062602043151855
Epoch 2770, val loss: 1.4221512079238892
Epoch 2780, training loss: 620.3228759765625 = 0.011851471848785877 + 100.0 * 6.203110218048096
Epoch 2780, val loss: 1.4246984720230103
Epoch 2790, training loss: 620.332275390625 = 0.01172789465636015 + 100.0 * 6.203205585479736
Epoch 2790, val loss: 1.4271020889282227
Epoch 2800, training loss: 620.3082275390625 = 0.011604703031480312 + 100.0 * 6.202966213226318
Epoch 2800, val loss: 1.429408311843872
Epoch 2810, training loss: 620.7119750976562 = 0.011487803421914577 + 100.0 * 6.207005023956299
Epoch 2810, val loss: 1.4318653345108032
Epoch 2820, training loss: 620.5413818359375 = 0.011363315396010876 + 100.0 * 6.205300331115723
Epoch 2820, val loss: 1.4340397119522095
Epoch 2830, training loss: 620.3984375 = 0.011239520274102688 + 100.0 * 6.203872203826904
Epoch 2830, val loss: 1.4360239505767822
Epoch 2840, training loss: 620.5852661132812 = 0.0111235985532403 + 100.0 * 6.2057414054870605
Epoch 2840, val loss: 1.4383472204208374
Epoch 2850, training loss: 620.3781127929688 = 0.011002142913639545 + 100.0 * 6.203671455383301
Epoch 2850, val loss: 1.4405550956726074
Epoch 2860, training loss: 620.5403442382812 = 0.010885479860007763 + 100.0 * 6.205294609069824
Epoch 2860, val loss: 1.4428552389144897
Epoch 2870, training loss: 620.30517578125 = 0.010775848291814327 + 100.0 * 6.202943801879883
Epoch 2870, val loss: 1.4449814558029175
Epoch 2880, training loss: 620.2131958007812 = 0.010665034875273705 + 100.0 * 6.202024936676025
Epoch 2880, val loss: 1.4473021030426025
Epoch 2890, training loss: 620.31298828125 = 0.010559680871665478 + 100.0 * 6.203024387359619
Epoch 2890, val loss: 1.4494949579238892
Epoch 2900, training loss: 620.3984375 = 0.010453608818352222 + 100.0 * 6.2038798332214355
Epoch 2900, val loss: 1.4516031742095947
Epoch 2910, training loss: 620.2462768554688 = 0.01034351997077465 + 100.0 * 6.202359676361084
Epoch 2910, val loss: 1.4538178443908691
Epoch 2920, training loss: 620.3494262695312 = 0.010243096388876438 + 100.0 * 6.2033915519714355
Epoch 2920, val loss: 1.45600163936615
Epoch 2930, training loss: 620.67431640625 = 0.010142224840819836 + 100.0 * 6.206641674041748
Epoch 2930, val loss: 1.4581135511398315
Epoch 2940, training loss: 620.1826782226562 = 0.010037589818239212 + 100.0 * 6.20172643661499
Epoch 2940, val loss: 1.4601765871047974
Epoch 2950, training loss: 620.12841796875 = 0.009937024675309658 + 100.0 * 6.2011847496032715
Epoch 2950, val loss: 1.462325096130371
Epoch 2960, training loss: 620.2178955078125 = 0.009841503575444221 + 100.0 * 6.202080726623535
Epoch 2960, val loss: 1.464292287826538
Epoch 2970, training loss: 620.697021484375 = 0.009746897034347057 + 100.0 * 6.206872463226318
Epoch 2970, val loss: 1.4663779735565186
Epoch 2980, training loss: 620.452392578125 = 0.009650560095906258 + 100.0 * 6.204427242279053
Epoch 2980, val loss: 1.4686599969863892
Epoch 2990, training loss: 620.1284790039062 = 0.009555055759847164 + 100.0 * 6.201189041137695
Epoch 2990, val loss: 1.4706300497055054
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8376383763837639
The final CL Acc:0.74198, 0.03492, The final GNN Acc:0.83834, 0.00263
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10548])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6456909179688 = 1.9641896486282349 + 100.0 * 8.59681510925293
Epoch 0, val loss: 1.9662599563598633
Epoch 10, training loss: 861.5364990234375 = 1.9543098211288452 + 100.0 * 8.59582233428955
Epoch 10, val loss: 1.9568448066711426
Epoch 20, training loss: 860.8731689453125 = 1.9418283700942993 + 100.0 * 8.589313507080078
Epoch 20, val loss: 1.9446688890457153
Epoch 30, training loss: 856.4072265625 = 1.9255777597427368 + 100.0 * 8.544816017150879
Epoch 30, val loss: 1.928633451461792
Epoch 40, training loss: 827.25 = 1.9057185649871826 + 100.0 * 8.253442764282227
Epoch 40, val loss: 1.909230351448059
Epoch 50, training loss: 760.446044921875 = 1.881973385810852 + 100.0 * 7.585640907287598
Epoch 50, val loss: 1.886925458908081
Epoch 60, training loss: 728.2330322265625 = 1.8672075271606445 + 100.0 * 7.26365852355957
Epoch 60, val loss: 1.8738210201263428
Epoch 70, training loss: 707.45654296875 = 1.8575643301010132 + 100.0 * 7.055989742279053
Epoch 70, val loss: 1.864104151725769
Epoch 80, training loss: 695.0709838867188 = 1.8473514318466187 + 100.0 * 6.932236194610596
Epoch 80, val loss: 1.8534345626831055
Epoch 90, training loss: 684.284423828125 = 1.8382859230041504 + 100.0 * 6.824461460113525
Epoch 90, val loss: 1.8438054323196411
Epoch 100, training loss: 675.22021484375 = 1.8306726217269897 + 100.0 * 6.733895301818848
Epoch 100, val loss: 1.835631251335144
Epoch 110, training loss: 669.5294189453125 = 1.8237484693527222 + 100.0 * 6.677056789398193
Epoch 110, val loss: 1.8278955221176147
Epoch 120, training loss: 664.9588012695312 = 1.8170589208602905 + 100.0 * 6.631417274475098
Epoch 120, val loss: 1.8204056024551392
Epoch 130, training loss: 661.6653442382812 = 1.8108909130096436 + 100.0 * 6.598544597625732
Epoch 130, val loss: 1.8133599758148193
Epoch 140, training loss: 658.7291259765625 = 1.804781198501587 + 100.0 * 6.569243431091309
Epoch 140, val loss: 1.8065848350524902
Epoch 150, training loss: 656.4716796875 = 1.798668384552002 + 100.0 * 6.5467305183410645
Epoch 150, val loss: 1.7998095750808716
Epoch 160, training loss: 654.4649047851562 = 1.7925561666488647 + 100.0 * 6.526723384857178
Epoch 160, val loss: 1.7931782007217407
Epoch 170, training loss: 652.7657470703125 = 1.7862944602966309 + 100.0 * 6.509794235229492
Epoch 170, val loss: 1.7863845825195312
Epoch 180, training loss: 651.5874633789062 = 1.7796322107315063 + 100.0 * 6.498078346252441
Epoch 180, val loss: 1.7793468236923218
Epoch 190, training loss: 649.8555908203125 = 1.7726250886917114 + 100.0 * 6.480829238891602
Epoch 190, val loss: 1.7721350193023682
Epoch 200, training loss: 648.474853515625 = 1.7651939392089844 + 100.0 * 6.46709680557251
Epoch 200, val loss: 1.764636516571045
Epoch 210, training loss: 647.2465209960938 = 1.7572784423828125 + 100.0 * 6.454892635345459
Epoch 210, val loss: 1.7567219734191895
Epoch 220, training loss: 646.43310546875 = 1.7487066984176636 + 100.0 * 6.446844100952148
Epoch 220, val loss: 1.7482349872589111
Epoch 230, training loss: 645.2484741210938 = 1.739410161972046 + 100.0 * 6.435091018676758
Epoch 230, val loss: 1.7391506433486938
Epoch 240, training loss: 644.2030639648438 = 1.7294681072235107 + 100.0 * 6.4247355461120605
Epoch 240, val loss: 1.7293853759765625
Epoch 250, training loss: 643.4849853515625 = 1.718752145767212 + 100.0 * 6.417662620544434
Epoch 250, val loss: 1.7189007997512817
Epoch 260, training loss: 642.6328735351562 = 1.7070250511169434 + 100.0 * 6.4092583656311035
Epoch 260, val loss: 1.7075071334838867
Epoch 270, training loss: 641.8949584960938 = 1.6945856809616089 + 100.0 * 6.402003765106201
Epoch 270, val loss: 1.695469856262207
Epoch 280, training loss: 641.2174072265625 = 1.6813175678253174 + 100.0 * 6.395360469818115
Epoch 280, val loss: 1.68258798122406
Epoch 290, training loss: 640.710205078125 = 1.6672192811965942 + 100.0 * 6.390429973602295
Epoch 290, val loss: 1.6689248085021973
Epoch 300, training loss: 640.1890258789062 = 1.6521830558776855 + 100.0 * 6.385368347167969
Epoch 300, val loss: 1.654455304145813
Epoch 310, training loss: 639.4410400390625 = 1.6363937854766846 + 100.0 * 6.37804651260376
Epoch 310, val loss: 1.6392446756362915
Epoch 320, training loss: 638.9521484375 = 1.6198934316635132 + 100.0 * 6.373322486877441
Epoch 320, val loss: 1.623392105102539
Epoch 330, training loss: 638.5506591796875 = 1.602679967880249 + 100.0 * 6.369480133056641
Epoch 330, val loss: 1.606939673423767
Epoch 340, training loss: 637.8767700195312 = 1.5849179029464722 + 100.0 * 6.362918853759766
Epoch 340, val loss: 1.5900532007217407
Epoch 350, training loss: 637.3514404296875 = 1.5666815042495728 + 100.0 * 6.357847213745117
Epoch 350, val loss: 1.5728811025619507
Epoch 360, training loss: 636.7552490234375 = 1.5480726957321167 + 100.0 * 6.352071762084961
Epoch 360, val loss: 1.5555360317230225
Epoch 370, training loss: 636.7993774414062 = 1.5291633605957031 + 100.0 * 6.3527021408081055
Epoch 370, val loss: 1.5381159782409668
Epoch 380, training loss: 635.96630859375 = 1.509979009628296 + 100.0 * 6.3445634841918945
Epoch 380, val loss: 1.5204880237579346
Epoch 390, training loss: 635.5965576171875 = 1.4907387495040894 + 100.0 * 6.341058254241943
Epoch 390, val loss: 1.5030657052993774
Epoch 400, training loss: 635.1847534179688 = 1.4714534282684326 + 100.0 * 6.337133407592773
Epoch 400, val loss: 1.485906958580017
Epoch 410, training loss: 634.847412109375 = 1.4522277116775513 + 100.0 * 6.333951950073242
Epoch 410, val loss: 1.4690848588943481
Epoch 420, training loss: 634.867919921875 = 1.4330205917358398 + 100.0 * 6.334348678588867
Epoch 420, val loss: 1.4525822401046753
Epoch 430, training loss: 634.2564697265625 = 1.4140316247940063 + 100.0 * 6.32842493057251
Epoch 430, val loss: 1.4361757040023804
Epoch 440, training loss: 633.91552734375 = 1.3950434923171997 + 100.0 * 6.325205326080322
Epoch 440, val loss: 1.4202882051467896
Epoch 450, training loss: 633.5457763671875 = 1.376222848892212 + 100.0 * 6.321695804595947
Epoch 450, val loss: 1.4048233032226562
Epoch 460, training loss: 633.5075073242188 = 1.3577311038970947 + 100.0 * 6.321497917175293
Epoch 460, val loss: 1.389716625213623
Epoch 470, training loss: 633.1193237304688 = 1.3392194509506226 + 100.0 * 6.317800998687744
Epoch 470, val loss: 1.3750547170639038
Epoch 480, training loss: 632.7269897460938 = 1.3210551738739014 + 100.0 * 6.314059734344482
Epoch 480, val loss: 1.3607797622680664
Epoch 490, training loss: 632.6162719726562 = 1.303078055381775 + 100.0 * 6.313131809234619
Epoch 490, val loss: 1.3469401597976685
Epoch 500, training loss: 632.2864379882812 = 1.2852920293807983 + 100.0 * 6.310011386871338
Epoch 500, val loss: 1.3335634469985962
Epoch 510, training loss: 632.0535888671875 = 1.2676690816879272 + 100.0 * 6.307859420776367
Epoch 510, val loss: 1.3204505443572998
Epoch 520, training loss: 632.1066284179688 = 1.250378131866455 + 100.0 * 6.308562755584717
Epoch 520, val loss: 1.3075884580612183
Epoch 530, training loss: 631.6926879882812 = 1.233071208000183 + 100.0 * 6.304595947265625
Epoch 530, val loss: 1.2955666780471802
Epoch 540, training loss: 631.3798828125 = 1.2162156105041504 + 100.0 * 6.301636695861816
Epoch 540, val loss: 1.283544659614563
Epoch 550, training loss: 631.0872802734375 = 1.199535608291626 + 100.0 * 6.298877716064453
Epoch 550, val loss: 1.2721725702285767
Epoch 560, training loss: 631.23095703125 = 1.1831815242767334 + 100.0 * 6.300477981567383
Epoch 560, val loss: 1.261046290397644
Epoch 570, training loss: 631.2300415039062 = 1.166617751121521 + 100.0 * 6.300633907318115
Epoch 570, val loss: 1.2501521110534668
Epoch 580, training loss: 630.7569580078125 = 1.150325059890747 + 100.0 * 6.2960662841796875
Epoch 580, val loss: 1.2394886016845703
Epoch 590, training loss: 630.4220581054688 = 1.1343629360198975 + 100.0 * 6.292877197265625
Epoch 590, val loss: 1.229266881942749
Epoch 600, training loss: 630.1990356445312 = 1.1186954975128174 + 100.0 * 6.290802955627441
Epoch 600, val loss: 1.2193270921707153
Epoch 610, training loss: 630.4593505859375 = 1.1032334566116333 + 100.0 * 6.2935614585876465
Epoch 610, val loss: 1.209742546081543
Epoch 620, training loss: 630.8566284179688 = 1.087668538093567 + 100.0 * 6.297689914703369
Epoch 620, val loss: 1.2007695436477661
Epoch 630, training loss: 629.8735961914062 = 1.0723124742507935 + 100.0 * 6.288012981414795
Epoch 630, val loss: 1.1913061141967773
Epoch 640, training loss: 629.5475463867188 = 1.0572189092636108 + 100.0 * 6.284903526306152
Epoch 640, val loss: 1.1824162006378174
Epoch 650, training loss: 629.4004516601562 = 1.0423413515090942 + 100.0 * 6.283580780029297
Epoch 650, val loss: 1.1738895177841187
Epoch 660, training loss: 630.0281372070312 = 1.0275784730911255 + 100.0 * 6.290005207061768
Epoch 660, val loss: 1.1655329465866089
Epoch 670, training loss: 629.3963012695312 = 1.0128434896469116 + 100.0 * 6.283834934234619
Epoch 670, val loss: 1.1572233438491821
Epoch 680, training loss: 629.0200805664062 = 0.9982364177703857 + 100.0 * 6.280218601226807
Epoch 680, val loss: 1.149242639541626
Epoch 690, training loss: 628.817138671875 = 0.9838853478431702 + 100.0 * 6.278332233428955
Epoch 690, val loss: 1.1416314840316772
Epoch 700, training loss: 629.0697021484375 = 0.9696869850158691 + 100.0 * 6.281000137329102
Epoch 700, val loss: 1.1344975233078003
Epoch 710, training loss: 629.2545166015625 = 0.9554630517959595 + 100.0 * 6.282990455627441
Epoch 710, val loss: 1.1262447834014893
Epoch 720, training loss: 628.5384521484375 = 0.9413069486618042 + 100.0 * 6.27597188949585
Epoch 720, val loss: 1.119206428527832
Epoch 730, training loss: 628.31298828125 = 0.9274317622184753 + 100.0 * 6.273855686187744
Epoch 730, val loss: 1.1124579906463623
Epoch 740, training loss: 628.2392578125 = 0.9137994050979614 + 100.0 * 6.27325439453125
Epoch 740, val loss: 1.1059861183166504
Epoch 750, training loss: 628.3717041015625 = 0.9003131985664368 + 100.0 * 6.27471399307251
Epoch 750, val loss: 1.0997681617736816
Epoch 760, training loss: 628.142333984375 = 0.8868095278739929 + 100.0 * 6.272555828094482
Epoch 760, val loss: 1.0932282209396362
Epoch 770, training loss: 627.974365234375 = 0.8734610676765442 + 100.0 * 6.2710089683532715
Epoch 770, val loss: 1.087320327758789
Epoch 780, training loss: 627.8560791015625 = 0.8603771328926086 + 100.0 * 6.269957542419434
Epoch 780, val loss: 1.0813182592391968
Epoch 790, training loss: 627.6441650390625 = 0.8474631309509277 + 100.0 * 6.2679667472839355
Epoch 790, val loss: 1.0761222839355469
Epoch 800, training loss: 627.6925048828125 = 0.8347596526145935 + 100.0 * 6.2685770988464355
Epoch 800, val loss: 1.0710560083389282
Epoch 810, training loss: 627.6154174804688 = 0.8219748139381409 + 100.0 * 6.267934322357178
Epoch 810, val loss: 1.0655521154403687
Epoch 820, training loss: 627.5067138671875 = 0.8094057440757751 + 100.0 * 6.266973495483398
Epoch 820, val loss: 1.0608835220336914
Epoch 830, training loss: 627.301513671875 = 0.7971309423446655 + 100.0 * 6.26504373550415
Epoch 830, val loss: 1.056414008140564
Epoch 840, training loss: 627.2259521484375 = 0.7851346731185913 + 100.0 * 6.264408111572266
Epoch 840, val loss: 1.052233338356018
Epoch 850, training loss: 627.297607421875 = 0.7732057571411133 + 100.0 * 6.265244007110596
Epoch 850, val loss: 1.0480626821517944
Epoch 860, training loss: 627.205078125 = 0.7613666653633118 + 100.0 * 6.264437198638916
Epoch 860, val loss: 1.0444109439849854
Epoch 870, training loss: 626.9812622070312 = 0.7497234344482422 + 100.0 * 6.26231575012207
Epoch 870, val loss: 1.0412085056304932
Epoch 880, training loss: 626.748046875 = 0.7384582757949829 + 100.0 * 6.260095596313477
Epoch 880, val loss: 1.0381903648376465
Epoch 890, training loss: 626.8521118164062 = 0.7273728251457214 + 100.0 * 6.261247634887695
Epoch 890, val loss: 1.0354737043380737
Epoch 900, training loss: 626.7462158203125 = 0.7163326740264893 + 100.0 * 6.260299205780029
Epoch 900, val loss: 1.0324625968933105
Epoch 910, training loss: 626.688232421875 = 0.7054432034492493 + 100.0 * 6.259827613830566
Epoch 910, val loss: 1.0301233530044556
Epoch 920, training loss: 626.4807739257812 = 0.694862961769104 + 100.0 * 6.257858753204346
Epoch 920, val loss: 1.0280743837356567
Epoch 930, training loss: 626.3978271484375 = 0.6844843626022339 + 100.0 * 6.2571330070495605
Epoch 930, val loss: 1.0263580083847046
Epoch 940, training loss: 626.5870971679688 = 0.6742843389511108 + 100.0 * 6.259128570556641
Epoch 940, val loss: 1.024522304534912
Epoch 950, training loss: 626.5172729492188 = 0.6641732454299927 + 100.0 * 6.258530616760254
Epoch 950, val loss: 1.0230337381362915
Epoch 960, training loss: 626.1613159179688 = 0.654160737991333 + 100.0 * 6.255071640014648
Epoch 960, val loss: 1.0219539403915405
Epoch 970, training loss: 626.0609130859375 = 0.6444103121757507 + 100.0 * 6.254165172576904
Epoch 970, val loss: 1.0211570262908936
Epoch 980, training loss: 626.130859375 = 0.634941577911377 + 100.0 * 6.2549591064453125
Epoch 980, val loss: 1.0202414989471436
Epoch 990, training loss: 625.947998046875 = 0.6255020499229431 + 100.0 * 6.253225326538086
Epoch 990, val loss: 1.0196751356124878
Epoch 1000, training loss: 626.0119018554688 = 0.6162311434745789 + 100.0 * 6.2539567947387695
Epoch 1000, val loss: 1.0192846059799194
Epoch 1010, training loss: 625.8173217773438 = 0.6070853471755981 + 100.0 * 6.252102851867676
Epoch 1010, val loss: 1.01889169216156
Epoch 1020, training loss: 625.6998901367188 = 0.5980945229530334 + 100.0 * 6.251018047332764
Epoch 1020, val loss: 1.0186338424682617
Epoch 1030, training loss: 625.7233276367188 = 0.5893535017967224 + 100.0 * 6.251339912414551
Epoch 1030, val loss: 1.0187016725540161
Epoch 1040, training loss: 625.838623046875 = 0.5805937647819519 + 100.0 * 6.252580642700195
Epoch 1040, val loss: 1.0189083814620972
Epoch 1050, training loss: 625.562255859375 = 0.5719519853591919 + 100.0 * 6.249902725219727
Epoch 1050, val loss: 1.0188193321228027
Epoch 1060, training loss: 625.4751586914062 = 0.563461184501648 + 100.0 * 6.249116897583008
Epoch 1060, val loss: 1.019372820854187
Epoch 1070, training loss: 625.295166015625 = 0.5551778674125671 + 100.0 * 6.247399806976318
Epoch 1070, val loss: 1.020019292831421
Epoch 1080, training loss: 625.4464721679688 = 0.5470727682113647 + 100.0 * 6.24899435043335
Epoch 1080, val loss: 1.020526647567749
Epoch 1090, training loss: 625.3311767578125 = 0.538887619972229 + 100.0 * 6.247922897338867
Epoch 1090, val loss: 1.0211772918701172
Epoch 1100, training loss: 625.3139038085938 = 0.5307947993278503 + 100.0 * 6.247830867767334
Epoch 1100, val loss: 1.022411584854126
Epoch 1110, training loss: 625.1104736328125 = 0.5228712558746338 + 100.0 * 6.245875835418701
Epoch 1110, val loss: 1.023310661315918
Epoch 1120, training loss: 625.0868530273438 = 0.5150858163833618 + 100.0 * 6.245718002319336
Epoch 1120, val loss: 1.0243644714355469
Epoch 1130, training loss: 625.240478515625 = 0.5073807239532471 + 100.0 * 6.247330665588379
Epoch 1130, val loss: 1.0252934694290161
Epoch 1140, training loss: 624.9043579101562 = 0.4997156858444214 + 100.0 * 6.244046688079834
Epoch 1140, val loss: 1.0269725322723389
Epoch 1150, training loss: 624.768798828125 = 0.4922149181365967 + 100.0 * 6.2427659034729
Epoch 1150, val loss: 1.028422474861145
Epoch 1160, training loss: 625.3370971679688 = 0.4847847521305084 + 100.0 * 6.248522758483887
Epoch 1160, val loss: 1.0301076173782349
Epoch 1170, training loss: 625.220703125 = 0.4773772656917572 + 100.0 * 6.247433662414551
Epoch 1170, val loss: 1.031024694442749
Epoch 1180, training loss: 624.613525390625 = 0.46993574500083923 + 100.0 * 6.241436004638672
Epoch 1180, val loss: 1.0329930782318115
Epoch 1190, training loss: 624.5836181640625 = 0.46275487542152405 + 100.0 * 6.241208553314209
Epoch 1190, val loss: 1.0350066423416138
Epoch 1200, training loss: 624.5311889648438 = 0.4557070732116699 + 100.0 * 6.240755081176758
Epoch 1200, val loss: 1.037047266960144
Epoch 1210, training loss: 624.88525390625 = 0.44868871569633484 + 100.0 * 6.244365692138672
Epoch 1210, val loss: 1.0393590927124023
Epoch 1220, training loss: 624.4061889648438 = 0.44163569808006287 + 100.0 * 6.239645481109619
Epoch 1220, val loss: 1.0410568714141846
Epoch 1230, training loss: 624.7041015625 = 0.4346851408481598 + 100.0 * 6.24269437789917
Epoch 1230, val loss: 1.04365074634552
Epoch 1240, training loss: 624.695068359375 = 0.4277864694595337 + 100.0 * 6.242672920227051
Epoch 1240, val loss: 1.0457096099853516
Epoch 1250, training loss: 624.3360595703125 = 0.42094141244888306 + 100.0 * 6.2391510009765625
Epoch 1250, val loss: 1.0479604005813599
Epoch 1260, training loss: 624.193603515625 = 0.4142642319202423 + 100.0 * 6.237793445587158
Epoch 1260, val loss: 1.0504602193832397
Epoch 1270, training loss: 624.1197509765625 = 0.4076846241950989 + 100.0 * 6.237120151519775
Epoch 1270, val loss: 1.0533959865570068
Epoch 1280, training loss: 624.2479248046875 = 0.4011765718460083 + 100.0 * 6.238467693328857
Epoch 1280, val loss: 1.0563055276870728
Epoch 1290, training loss: 624.1659545898438 = 0.3946494162082672 + 100.0 * 6.237712860107422
Epoch 1290, val loss: 1.058656930923462
Epoch 1300, training loss: 623.962646484375 = 0.38818421959877014 + 100.0 * 6.235744476318359
Epoch 1300, val loss: 1.0614395141601562
Epoch 1310, training loss: 624.0330810546875 = 0.38185471296310425 + 100.0 * 6.236512660980225
Epoch 1310, val loss: 1.0645856857299805
Epoch 1320, training loss: 624.08544921875 = 0.3754943609237671 + 100.0 * 6.237099647521973
Epoch 1320, val loss: 1.0672121047973633
Epoch 1330, training loss: 623.928466796875 = 0.36917147040367126 + 100.0 * 6.235592365264893
Epoch 1330, val loss: 1.0701067447662354
Epoch 1340, training loss: 623.7568969726562 = 0.36297205090522766 + 100.0 * 6.233939170837402
Epoch 1340, val loss: 1.0737276077270508
Epoch 1350, training loss: 623.6507568359375 = 0.35694727301597595 + 100.0 * 6.232938289642334
Epoch 1350, val loss: 1.0771844387054443
Epoch 1360, training loss: 623.9861450195312 = 0.35098692774772644 + 100.0 * 6.236351013183594
Epoch 1360, val loss: 1.0809364318847656
Epoch 1370, training loss: 623.6626586914062 = 0.34487786889076233 + 100.0 * 6.23317813873291
Epoch 1370, val loss: 1.083349347114563
Epoch 1380, training loss: 623.7958984375 = 0.33890771865844727 + 100.0 * 6.234569549560547
Epoch 1380, val loss: 1.087075114250183
Epoch 1390, training loss: 623.5918579101562 = 0.33305466175079346 + 100.0 * 6.232587814331055
Epoch 1390, val loss: 1.0904268026351929
Epoch 1400, training loss: 623.6002197265625 = 0.3273222744464874 + 100.0 * 6.232728958129883
Epoch 1400, val loss: 1.0941414833068848
Epoch 1410, training loss: 623.5505981445312 = 0.32162246108055115 + 100.0 * 6.232289791107178
Epoch 1410, val loss: 1.0978927612304688
Epoch 1420, training loss: 623.8704223632812 = 0.3159942924976349 + 100.0 * 6.235544681549072
Epoch 1420, val loss: 1.1015511751174927
Epoch 1430, training loss: 623.4228515625 = 0.3103693425655365 + 100.0 * 6.2311248779296875
Epoch 1430, val loss: 1.1050083637237549
Epoch 1440, training loss: 623.2734985351562 = 0.30485910177230835 + 100.0 * 6.229686260223389
Epoch 1440, val loss: 1.1089048385620117
Epoch 1450, training loss: 623.1992797851562 = 0.29945269227027893 + 100.0 * 6.22899866104126
Epoch 1450, val loss: 1.1128146648406982
Epoch 1460, training loss: 623.3262939453125 = 0.2941454350948334 + 100.0 * 6.230321407318115
Epoch 1460, val loss: 1.1168813705444336
Epoch 1470, training loss: 623.5116577148438 = 0.28879842162132263 + 100.0 * 6.2322282791137695
Epoch 1470, val loss: 1.1204078197479248
Epoch 1480, training loss: 623.3271484375 = 0.283462792634964 + 100.0 * 6.2304368019104
Epoch 1480, val loss: 1.12374746799469
Epoch 1490, training loss: 623.4765014648438 = 0.2782859802246094 + 100.0 * 6.231982707977295
Epoch 1490, val loss: 1.127630591392517
Epoch 1500, training loss: 623.055908203125 = 0.27315929532051086 + 100.0 * 6.227827548980713
Epoch 1500, val loss: 1.1315258741378784
Epoch 1510, training loss: 623.2122192382812 = 0.2681829631328583 + 100.0 * 6.229440212249756
Epoch 1510, val loss: 1.1358466148376465
Epoch 1520, training loss: 623.2261352539062 = 0.2632195055484772 + 100.0 * 6.229629039764404
Epoch 1520, val loss: 1.1393322944641113
Epoch 1530, training loss: 622.9887084960938 = 0.2583073675632477 + 100.0 * 6.227303981781006
Epoch 1530, val loss: 1.1434227228164673
Epoch 1540, training loss: 622.9339599609375 = 0.2534828782081604 + 100.0 * 6.226804733276367
Epoch 1540, val loss: 1.1477454900741577
Epoch 1550, training loss: 623.6171875 = 0.2487531155347824 + 100.0 * 6.233684062957764
Epoch 1550, val loss: 1.1517690420150757
Epoch 1560, training loss: 623.0570678710938 = 0.244011789560318 + 100.0 * 6.228130340576172
Epoch 1560, val loss: 1.1551690101623535
Epoch 1570, training loss: 622.8256225585938 = 0.23932026326656342 + 100.0 * 6.225862979888916
Epoch 1570, val loss: 1.159385085105896
Epoch 1580, training loss: 622.7305908203125 = 0.23483362793922424 + 100.0 * 6.22495698928833
Epoch 1580, val loss: 1.1637336015701294
Epoch 1590, training loss: 622.7302856445312 = 0.23041854798793793 + 100.0 * 6.224998950958252
Epoch 1590, val loss: 1.1679260730743408
Epoch 1600, training loss: 623.69091796875 = 0.2261037528514862 + 100.0 * 6.23464822769165
Epoch 1600, val loss: 1.1721667051315308
Epoch 1610, training loss: 623.15087890625 = 0.22160455584526062 + 100.0 * 6.229292869567871
Epoch 1610, val loss: 1.1753613948822021
Epoch 1620, training loss: 622.7180786132812 = 0.21731767058372498 + 100.0 * 6.2250075340271
Epoch 1620, val loss: 1.1796165704727173
Epoch 1630, training loss: 622.5400390625 = 0.21310937404632568 + 100.0 * 6.223268985748291
Epoch 1630, val loss: 1.183882713317871
Epoch 1640, training loss: 622.6294555664062 = 0.20903822779655457 + 100.0 * 6.224204063415527
Epoch 1640, val loss: 1.1882528066635132
Epoch 1650, training loss: 622.9923706054688 = 0.20501716434955597 + 100.0 * 6.227873802185059
Epoch 1650, val loss: 1.1919645071029663
Epoch 1660, training loss: 622.5802001953125 = 0.20095786452293396 + 100.0 * 6.223793029785156
Epoch 1660, val loss: 1.196198582649231
Epoch 1670, training loss: 622.5101928710938 = 0.19701284170150757 + 100.0 * 6.2231316566467285
Epoch 1670, val loss: 1.2006393671035767
Epoch 1680, training loss: 622.7513427734375 = 0.19319914281368256 + 100.0 * 6.225581645965576
Epoch 1680, val loss: 1.2046403884887695
Epoch 1690, training loss: 622.5154418945312 = 0.18938736617565155 + 100.0 * 6.223260402679443
Epoch 1690, val loss: 1.208845853805542
Epoch 1700, training loss: 622.506591796875 = 0.18569494783878326 + 100.0 * 6.223209381103516
Epoch 1700, val loss: 1.2125244140625
Epoch 1710, training loss: 622.3321533203125 = 0.1820606291294098 + 100.0 * 6.221500873565674
Epoch 1710, val loss: 1.2170859575271606
Epoch 1720, training loss: 622.3787231445312 = 0.17853319644927979 + 100.0 * 6.222002029418945
Epoch 1720, val loss: 1.2211443185806274
Epoch 1730, training loss: 622.7350463867188 = 0.175008162856102 + 100.0 * 6.225600719451904
Epoch 1730, val loss: 1.2250150442123413
Epoch 1740, training loss: 622.2468872070312 = 0.17150971293449402 + 100.0 * 6.2207536697387695
Epoch 1740, val loss: 1.2292709350585938
Epoch 1750, training loss: 622.1806030273438 = 0.1681349277496338 + 100.0 * 6.2201247215271
Epoch 1750, val loss: 1.2337177991867065
Epoch 1760, training loss: 622.1364135742188 = 0.16487915813922882 + 100.0 * 6.219715595245361
Epoch 1760, val loss: 1.2379220724105835
Epoch 1770, training loss: 622.5187377929688 = 0.1617269515991211 + 100.0 * 6.223569869995117
Epoch 1770, val loss: 1.2423523664474487
Epoch 1780, training loss: 622.1441650390625 = 0.15847475826740265 + 100.0 * 6.219857215881348
Epoch 1780, val loss: 1.2460627555847168
Epoch 1790, training loss: 622.0699462890625 = 0.15530113875865936 + 100.0 * 6.219146728515625
Epoch 1790, val loss: 1.2502715587615967
Epoch 1800, training loss: 622.0383911132812 = 0.15227195620536804 + 100.0 * 6.218861103057861
Epoch 1800, val loss: 1.2547991275787354
Epoch 1810, training loss: 622.3905639648438 = 0.14933769404888153 + 100.0 * 6.222412109375
Epoch 1810, val loss: 1.259002923965454
Epoch 1820, training loss: 622.0546875 = 0.14634418487548828 + 100.0 * 6.219083309173584
Epoch 1820, val loss: 1.263200044631958
Epoch 1830, training loss: 622.0632934570312 = 0.14346739649772644 + 100.0 * 6.219197750091553
Epoch 1830, val loss: 1.2677171230316162
Epoch 1840, training loss: 622.0826416015625 = 0.1406492292881012 + 100.0 * 6.219420433044434
Epoch 1840, val loss: 1.2718449831008911
Epoch 1850, training loss: 622.2274780273438 = 0.13786737620830536 + 100.0 * 6.220896244049072
Epoch 1850, val loss: 1.275813341140747
Epoch 1860, training loss: 622.0742797851562 = 0.1351999193429947 + 100.0 * 6.219390869140625
Epoch 1860, val loss: 1.2802746295928955
Epoch 1870, training loss: 622.01904296875 = 0.13253575563430786 + 100.0 * 6.218865394592285
Epoch 1870, val loss: 1.2842439413070679
Epoch 1880, training loss: 621.8380737304688 = 0.12996958196163177 + 100.0 * 6.217081069946289
Epoch 1880, val loss: 1.2889484167099
Epoch 1890, training loss: 621.85791015625 = 0.12744589149951935 + 100.0 * 6.217304706573486
Epoch 1890, val loss: 1.2934705018997192
Epoch 1900, training loss: 622.063232421875 = 0.12498427927494049 + 100.0 * 6.219382286071777
Epoch 1900, val loss: 1.2974354028701782
Epoch 1910, training loss: 622.218017578125 = 0.12253595888614655 + 100.0 * 6.220954895019531
Epoch 1910, val loss: 1.3015565872192383
Epoch 1920, training loss: 621.9991455078125 = 0.12009060382843018 + 100.0 * 6.218790054321289
Epoch 1920, val loss: 1.3061894178390503
Epoch 1930, training loss: 621.8463745117188 = 0.1177729070186615 + 100.0 * 6.217285633087158
Epoch 1930, val loss: 1.3100342750549316
Epoch 1940, training loss: 621.8234252929688 = 0.11549896001815796 + 100.0 * 6.2170796394348145
Epoch 1940, val loss: 1.314350962638855
Epoch 1950, training loss: 621.685791015625 = 0.11328443139791489 + 100.0 * 6.215724945068359
Epoch 1950, val loss: 1.3188652992248535
Epoch 1960, training loss: 621.6842651367188 = 0.11110673099756241 + 100.0 * 6.215732097625732
Epoch 1960, val loss: 1.3233104944229126
Epoch 1970, training loss: 621.9248046875 = 0.10899221152067184 + 100.0 * 6.218157768249512
Epoch 1970, val loss: 1.3274263143539429
Epoch 1980, training loss: 621.8345947265625 = 0.10688300430774689 + 100.0 * 6.2172770500183105
Epoch 1980, val loss: 1.3317539691925049
Epoch 1990, training loss: 621.6749877929688 = 0.10482102632522583 + 100.0 * 6.215702056884766
Epoch 1990, val loss: 1.336086392402649
Epoch 2000, training loss: 621.6041870117188 = 0.10280773788690567 + 100.0 * 6.2150139808654785
Epoch 2000, val loss: 1.3401687145233154
Epoch 2010, training loss: 621.8070678710938 = 0.10089468210935593 + 100.0 * 6.217061519622803
Epoch 2010, val loss: 1.344111442565918
Epoch 2020, training loss: 621.4901733398438 = 0.09892015159130096 + 100.0 * 6.213912487030029
Epoch 2020, val loss: 1.348753809928894
Epoch 2030, training loss: 621.5651245117188 = 0.09705259650945663 + 100.0 * 6.2146806716918945
Epoch 2030, val loss: 1.3533560037612915
Epoch 2040, training loss: 621.681396484375 = 0.09523969143629074 + 100.0 * 6.215861797332764
Epoch 2040, val loss: 1.3570560216903687
Epoch 2050, training loss: 621.5029296875 = 0.09341190755367279 + 100.0 * 6.214095592498779
Epoch 2050, val loss: 1.3616482019424438
Epoch 2060, training loss: 621.5189208984375 = 0.09165854007005692 + 100.0 * 6.214272499084473
Epoch 2060, val loss: 1.3660682439804077
Epoch 2070, training loss: 621.4197387695312 = 0.0899411216378212 + 100.0 * 6.2132978439331055
Epoch 2070, val loss: 1.3701694011688232
Epoch 2080, training loss: 621.4403686523438 = 0.08827819675207138 + 100.0 * 6.2135210037231445
Epoch 2080, val loss: 1.3749853372573853
Epoch 2090, training loss: 621.4059448242188 = 0.08663356304168701 + 100.0 * 6.213193416595459
Epoch 2090, val loss: 1.3792839050292969
Epoch 2100, training loss: 621.2974853515625 = 0.08504599332809448 + 100.0 * 6.212124347686768
Epoch 2100, val loss: 1.3839545249938965
Epoch 2110, training loss: 622.2288208007812 = 0.08349461108446121 + 100.0 * 6.2214531898498535
Epoch 2110, val loss: 1.3883609771728516
Epoch 2120, training loss: 621.4912719726562 = 0.08184971660375595 + 100.0 * 6.214094161987305
Epoch 2120, val loss: 1.3920713663101196
Epoch 2130, training loss: 621.4562377929688 = 0.08034319430589676 + 100.0 * 6.213758945465088
Epoch 2130, val loss: 1.3967370986938477
Epoch 2140, training loss: 621.1630859375 = 0.07885018736124039 + 100.0 * 6.210842132568359
Epoch 2140, val loss: 1.4010980129241943
Epoch 2150, training loss: 621.1675415039062 = 0.07742300629615784 + 100.0 * 6.210901260375977
Epoch 2150, val loss: 1.4055101871490479
Epoch 2160, training loss: 621.2494506835938 = 0.07605873048305511 + 100.0 * 6.211733818054199
Epoch 2160, val loss: 1.4100360870361328
Epoch 2170, training loss: 621.89892578125 = 0.07470090687274933 + 100.0 * 6.218242168426514
Epoch 2170, val loss: 1.4139065742492676
Epoch 2180, training loss: 621.3016357421875 = 0.07327108830213547 + 100.0 * 6.212284088134766
Epoch 2180, val loss: 1.417900562286377
Epoch 2190, training loss: 621.1284790039062 = 0.07193898409605026 + 100.0 * 6.210565090179443
Epoch 2190, val loss: 1.42253577709198
Epoch 2200, training loss: 621.0965576171875 = 0.07067079097032547 + 100.0 * 6.210258483886719
Epoch 2200, val loss: 1.4267454147338867
Epoch 2210, training loss: 621.730712890625 = 0.06944992393255234 + 100.0 * 6.216612339019775
Epoch 2210, val loss: 1.4304964542388916
Epoch 2220, training loss: 621.20849609375 = 0.0681634396314621 + 100.0 * 6.2114033699035645
Epoch 2220, val loss: 1.435896635055542
Epoch 2230, training loss: 621.1948852539062 = 0.06694183498620987 + 100.0 * 6.211279392242432
Epoch 2230, val loss: 1.4394408464431763
Epoch 2240, training loss: 621.0422973632812 = 0.06575889885425568 + 100.0 * 6.209765434265137
Epoch 2240, val loss: 1.4444082975387573
Epoch 2250, training loss: 621.032470703125 = 0.0646178126335144 + 100.0 * 6.2096781730651855
Epoch 2250, val loss: 1.4486232995986938
Epoch 2260, training loss: 621.6272583007812 = 0.06351309269666672 + 100.0 * 6.21563720703125
Epoch 2260, val loss: 1.4530404806137085
Epoch 2270, training loss: 621.2191772460938 = 0.062341440469026566 + 100.0 * 6.211568355560303
Epoch 2270, val loss: 1.4568803310394287
Epoch 2280, training loss: 620.9830932617188 = 0.06126854196190834 + 100.0 * 6.2092180252075195
Epoch 2280, val loss: 1.4613858461380005
Epoch 2290, training loss: 621.2183227539062 = 0.0602119155228138 + 100.0 * 6.211580753326416
Epoch 2290, val loss: 1.4659367799758911
Epoch 2300, training loss: 620.9927978515625 = 0.059154871851205826 + 100.0 * 6.209336280822754
Epoch 2300, val loss: 1.4699175357818604
Epoch 2310, training loss: 620.9130859375 = 0.05812402442097664 + 100.0 * 6.208549976348877
Epoch 2310, val loss: 1.4738256931304932
Epoch 2320, training loss: 620.9444580078125 = 0.0571419820189476 + 100.0 * 6.208873271942139
Epoch 2320, val loss: 1.4783380031585693
Epoch 2330, training loss: 621.366455078125 = 0.05617291480302811 + 100.0 * 6.213103294372559
Epoch 2330, val loss: 1.4822335243225098
Epoch 2340, training loss: 620.9835205078125 = 0.05522136762738228 + 100.0 * 6.209282875061035
Epoch 2340, val loss: 1.486662745475769
Epoch 2350, training loss: 620.8936767578125 = 0.05426788330078125 + 100.0 * 6.2083940505981445
Epoch 2350, val loss: 1.4907290935516357
Epoch 2360, training loss: 620.984375 = 0.0533687025308609 + 100.0 * 6.209310054779053
Epoch 2360, val loss: 1.494766116142273
Epoch 2370, training loss: 621.1005859375 = 0.05247548222541809 + 100.0 * 6.210480690002441
Epoch 2370, val loss: 1.4990746974945068
Epoch 2380, training loss: 620.799072265625 = 0.05158016458153725 + 100.0 * 6.207474708557129
Epoch 2380, val loss: 1.503281831741333
Epoch 2390, training loss: 620.6984252929688 = 0.050728894770145416 + 100.0 * 6.206477165222168
Epoch 2390, val loss: 1.507301926612854
Epoch 2400, training loss: 620.729248046875 = 0.04991817846894264 + 100.0 * 6.206793308258057
Epoch 2400, val loss: 1.5115240812301636
Epoch 2410, training loss: 621.1868286132812 = 0.049126848578453064 + 100.0 * 6.211377143859863
Epoch 2410, val loss: 1.5150184631347656
Epoch 2420, training loss: 620.8798828125 = 0.04828096926212311 + 100.0 * 6.208316326141357
Epoch 2420, val loss: 1.5195785760879517
Epoch 2430, training loss: 620.8685913085938 = 0.047500066459178925 + 100.0 * 6.2082109451293945
Epoch 2430, val loss: 1.5229288339614868
Epoch 2440, training loss: 620.9276733398438 = 0.0467025451362133 + 100.0 * 6.208809852600098
Epoch 2440, val loss: 1.5270836353302002
Epoch 2450, training loss: 620.7269897460938 = 0.04594586417078972 + 100.0 * 6.206810474395752
Epoch 2450, val loss: 1.531261682510376
Epoch 2460, training loss: 620.6224975585938 = 0.04521714150905609 + 100.0 * 6.205772399902344
Epoch 2460, val loss: 1.5353498458862305
Epoch 2470, training loss: 620.9352416992188 = 0.04451996833086014 + 100.0 * 6.208907604217529
Epoch 2470, val loss: 1.538732886314392
Epoch 2480, training loss: 620.9840698242188 = 0.04381323233246803 + 100.0 * 6.209402561187744
Epoch 2480, val loss: 1.543545126914978
Epoch 2490, training loss: 620.7081298828125 = 0.0430913120508194 + 100.0 * 6.206650257110596
Epoch 2490, val loss: 1.5466530323028564
Epoch 2500, training loss: 620.5771484375 = 0.0423976369202137 + 100.0 * 6.205347061157227
Epoch 2500, val loss: 1.5511659383773804
Epoch 2510, training loss: 620.581787109375 = 0.04175305366516113 + 100.0 * 6.205400466918945
Epoch 2510, val loss: 1.5549358129501343
Epoch 2520, training loss: 620.692626953125 = 0.04111630469560623 + 100.0 * 6.206514835357666
Epoch 2520, val loss: 1.5591332912445068
Epoch 2530, training loss: 620.8524169921875 = 0.0404915027320385 + 100.0 * 6.2081193923950195
Epoch 2530, val loss: 1.5624505281448364
Epoch 2540, training loss: 620.8193969726562 = 0.03986815735697746 + 100.0 * 6.207795143127441
Epoch 2540, val loss: 1.5666972398757935
Epoch 2550, training loss: 620.5988159179688 = 0.03924906998872757 + 100.0 * 6.205595970153809
Epoch 2550, val loss: 1.5704458951950073
Epoch 2560, training loss: 620.6115112304688 = 0.03865043818950653 + 100.0 * 6.205728530883789
Epoch 2560, val loss: 1.5740904808044434
Epoch 2570, training loss: 620.6215209960938 = 0.03806662559509277 + 100.0 * 6.20583438873291
Epoch 2570, val loss: 1.5777925252914429
Epoch 2580, training loss: 620.6019897460938 = 0.0374889001250267 + 100.0 * 6.2056450843811035
Epoch 2580, val loss: 1.5817224979400635
Epoch 2590, training loss: 620.5923461914062 = 0.036935850977897644 + 100.0 * 6.205554008483887
Epoch 2590, val loss: 1.585833191871643
Epoch 2600, training loss: 620.697998046875 = 0.036382466554641724 + 100.0 * 6.206615924835205
Epoch 2600, val loss: 1.5891978740692139
Epoch 2610, training loss: 620.4853515625 = 0.03584026172757149 + 100.0 * 6.204495429992676
Epoch 2610, val loss: 1.5924746990203857
Epoch 2620, training loss: 620.4427490234375 = 0.03532018885016441 + 100.0 * 6.204073905944824
Epoch 2620, val loss: 1.5968384742736816
Epoch 2630, training loss: 621.0592041015625 = 0.03481724113225937 + 100.0 * 6.210244178771973
Epoch 2630, val loss: 1.6008681058883667
Epoch 2640, training loss: 620.5221557617188 = 0.034273166209459305 + 100.0 * 6.204878807067871
Epoch 2640, val loss: 1.6035022735595703
Epoch 2650, training loss: 620.3812866210938 = 0.03376976028084755 + 100.0 * 6.203475475311279
Epoch 2650, val loss: 1.6077370643615723
Epoch 2660, training loss: 620.371826171875 = 0.033288758248090744 + 100.0 * 6.203385829925537
Epoch 2660, val loss: 1.611204981803894
Epoch 2670, training loss: 620.6126708984375 = 0.03282032907009125 + 100.0 * 6.205798625946045
Epoch 2670, val loss: 1.614668369293213
Epoch 2680, training loss: 620.5817260742188 = 0.03234856575727463 + 100.0 * 6.205493927001953
Epoch 2680, val loss: 1.6180105209350586
Epoch 2690, training loss: 620.404296875 = 0.03188974782824516 + 100.0 * 6.203724384307861
Epoch 2690, val loss: 1.621646523475647
Epoch 2700, training loss: 620.3331298828125 = 0.03143366053700447 + 100.0 * 6.203017234802246
Epoch 2700, val loss: 1.6256581544876099
Epoch 2710, training loss: 620.269287109375 = 0.03100288286805153 + 100.0 * 6.202382564544678
Epoch 2710, val loss: 1.6289156675338745
Epoch 2720, training loss: 620.8696899414062 = 0.030588587746024132 + 100.0 * 6.208391189575195
Epoch 2720, val loss: 1.6316986083984375
Epoch 2730, training loss: 620.6976318359375 = 0.030139559879899025 + 100.0 * 6.206675052642822
Epoch 2730, val loss: 1.6354870796203613
Epoch 2740, training loss: 620.7250366210938 = 0.029707511886954308 + 100.0 * 6.206953048706055
Epoch 2740, val loss: 1.6387548446655273
Epoch 2750, training loss: 620.3141479492188 = 0.029289353638887405 + 100.0 * 6.202848434448242
Epoch 2750, val loss: 1.6422618627548218
Epoch 2760, training loss: 620.2169799804688 = 0.028891444206237793 + 100.0 * 6.201880931854248
Epoch 2760, val loss: 1.6454870700836182
Epoch 2770, training loss: 620.17724609375 = 0.02851620502769947 + 100.0 * 6.2014875411987305
Epoch 2770, val loss: 1.6490728855133057
Epoch 2780, training loss: 620.4750366210938 = 0.028153643012046814 + 100.0 * 6.204469203948975
Epoch 2780, val loss: 1.6522678136825562
Epoch 2790, training loss: 620.3661499023438 = 0.02775728330016136 + 100.0 * 6.203383922576904
Epoch 2790, val loss: 1.6555871963500977
Epoch 2800, training loss: 620.43115234375 = 0.027384020388126373 + 100.0 * 6.204037189483643
Epoch 2800, val loss: 1.6591074466705322
Epoch 2810, training loss: 620.17041015625 = 0.02700631693005562 + 100.0 * 6.201434135437012
Epoch 2810, val loss: 1.6621721982955933
Epoch 2820, training loss: 620.1480102539062 = 0.026651687920093536 + 100.0 * 6.201213359832764
Epoch 2820, val loss: 1.6654201745986938
Epoch 2830, training loss: 620.2652587890625 = 0.026317842304706573 + 100.0 * 6.202389240264893
Epoch 2830, val loss: 1.6686887741088867
Epoch 2840, training loss: 620.5310668945312 = 0.025975463911890984 + 100.0 * 6.205050468444824
Epoch 2840, val loss: 1.671936273574829
Epoch 2850, training loss: 620.7449951171875 = 0.025633271783590317 + 100.0 * 6.207193374633789
Epoch 2850, val loss: 1.6753393411636353
Epoch 2860, training loss: 620.22998046875 = 0.025279074907302856 + 100.0 * 6.202047348022461
Epoch 2860, val loss: 1.6776468753814697
Epoch 2870, training loss: 620.114990234375 = 0.024953685700893402 + 100.0 * 6.200900554656982
Epoch 2870, val loss: 1.6812958717346191
Epoch 2880, training loss: 620.0296630859375 = 0.02464795671403408 + 100.0 * 6.200050354003906
Epoch 2880, val loss: 1.6843039989471436
Epoch 2890, training loss: 620.0073852539062 = 0.024346916005015373 + 100.0 * 6.199830055236816
Epoch 2890, val loss: 1.6876999139785767
Epoch 2900, training loss: 620.4561157226562 = 0.024057893082499504 + 100.0 * 6.204320907592773
Epoch 2900, val loss: 1.6906081438064575
Epoch 2910, training loss: 620.1543579101562 = 0.023755770176649094 + 100.0 * 6.201305866241455
Epoch 2910, val loss: 1.6937514543533325
Epoch 2920, training loss: 620.307373046875 = 0.023452546447515488 + 100.0 * 6.202839374542236
Epoch 2920, val loss: 1.6962352991104126
Epoch 2930, training loss: 620.1622314453125 = 0.02314755879342556 + 100.0 * 6.201391220092773
Epoch 2930, val loss: 1.6994857788085938
Epoch 2940, training loss: 620.0726318359375 = 0.022856667637825012 + 100.0 * 6.200498104095459
Epoch 2940, val loss: 1.7029651403427124
Epoch 2950, training loss: 620.0771484375 = 0.022585323080420494 + 100.0 * 6.200545787811279
Epoch 2950, val loss: 1.7062268257141113
Epoch 2960, training loss: 620.4129638671875 = 0.022318756207823753 + 100.0 * 6.203906536102295
Epoch 2960, val loss: 1.709344744682312
Epoch 2970, training loss: 620.0438232421875 = 0.02204285003244877 + 100.0 * 6.2002177238464355
Epoch 2970, val loss: 1.7113893032073975
Epoch 2980, training loss: 619.951171875 = 0.021775316447019577 + 100.0 * 6.199293613433838
Epoch 2980, val loss: 1.7144004106521606
Epoch 2990, training loss: 619.9893188476562 = 0.021523332223296165 + 100.0 * 6.199677467346191
Epoch 2990, val loss: 1.717995285987854
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6370370370370371
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 861.6293334960938 = 1.9464786052703857 + 100.0 * 8.59682846069336
Epoch 0, val loss: 1.951991081237793
Epoch 10, training loss: 861.5310668945312 = 1.9375431537628174 + 100.0 * 8.595934867858887
Epoch 10, val loss: 1.9423480033874512
Epoch 20, training loss: 860.9297485351562 = 1.9263986349105835 + 100.0 * 8.590033531188965
Epoch 20, val loss: 1.930159330368042
Epoch 30, training loss: 856.5419921875 = 1.9123578071594238 + 100.0 * 8.546296119689941
Epoch 30, val loss: 1.9146857261657715
Epoch 40, training loss: 821.7747192382812 = 1.8950763940811157 + 100.0 * 8.198796272277832
Epoch 40, val loss: 1.8957328796386719
Epoch 50, training loss: 741.0211791992188 = 1.8761016130447388 + 100.0 * 7.391450881958008
Epoch 50, val loss: 1.8758774995803833
Epoch 60, training loss: 717.3075561523438 = 1.8643954992294312 + 100.0 * 7.1544318199157715
Epoch 60, val loss: 1.8641605377197266
Epoch 70, training loss: 703.4260864257812 = 1.8532261848449707 + 100.0 * 7.015728950500488
Epoch 70, val loss: 1.8527543544769287
Epoch 80, training loss: 689.3058471679688 = 1.8438036441802979 + 100.0 * 6.87462043762207
Epoch 80, val loss: 1.8436760902404785
Epoch 90, training loss: 679.3140869140625 = 1.836553931236267 + 100.0 * 6.774775505065918
Epoch 90, val loss: 1.8365693092346191
Epoch 100, training loss: 672.6366577148438 = 1.8302503824234009 + 100.0 * 6.708064079284668
Epoch 100, val loss: 1.8303543329238892
Epoch 110, training loss: 667.2283325195312 = 1.82431161403656 + 100.0 * 6.6540398597717285
Epoch 110, val loss: 1.8244166374206543
Epoch 120, training loss: 662.814208984375 = 1.8188512325286865 + 100.0 * 6.6099534034729
Epoch 120, val loss: 1.819008708000183
Epoch 130, training loss: 659.4016723632812 = 1.813807725906372 + 100.0 * 6.575879096984863
Epoch 130, val loss: 1.8139740228652954
Epoch 140, training loss: 656.9351196289062 = 1.808823823928833 + 100.0 * 6.551262855529785
Epoch 140, val loss: 1.8089851140975952
Epoch 150, training loss: 654.5919189453125 = 1.8036015033721924 + 100.0 * 6.527883052825928
Epoch 150, val loss: 1.803998351097107
Epoch 160, training loss: 652.531005859375 = 1.798275351524353 + 100.0 * 6.507327556610107
Epoch 160, val loss: 1.7990553379058838
Epoch 170, training loss: 651.0740966796875 = 1.7927544116973877 + 100.0 * 6.492813587188721
Epoch 170, val loss: 1.793923020362854
Epoch 180, training loss: 649.1522827148438 = 1.7867610454559326 + 100.0 * 6.4736552238464355
Epoch 180, val loss: 1.788529396057129
Epoch 190, training loss: 647.6998291015625 = 1.7803361415863037 + 100.0 * 6.459194660186768
Epoch 190, val loss: 1.7828210592269897
Epoch 200, training loss: 646.4491577148438 = 1.7733080387115479 + 100.0 * 6.446758270263672
Epoch 200, val loss: 1.7766406536102295
Epoch 210, training loss: 645.4337158203125 = 1.765594244003296 + 100.0 * 6.436681747436523
Epoch 210, val loss: 1.7698966264724731
Epoch 220, training loss: 644.294677734375 = 1.757192611694336 + 100.0 * 6.425374507904053
Epoch 220, val loss: 1.7625644207000732
Epoch 230, training loss: 643.5061645507812 = 1.747997522354126 + 100.0 * 6.417582035064697
Epoch 230, val loss: 1.7545944452285767
Epoch 240, training loss: 642.6727294921875 = 1.7378650903701782 + 100.0 * 6.409348487854004
Epoch 240, val loss: 1.74594247341156
Epoch 250, training loss: 641.8121948242188 = 1.726812481880188 + 100.0 * 6.400853633880615
Epoch 250, val loss: 1.7365778684616089
Epoch 260, training loss: 641.1869506835938 = 1.714796781539917 + 100.0 * 6.394721508026123
Epoch 260, val loss: 1.726475477218628
Epoch 270, training loss: 640.9662475585938 = 1.7017621994018555 + 100.0 * 6.39264440536499
Epoch 270, val loss: 1.7154489755630493
Epoch 280, training loss: 639.8489379882812 = 1.687558889389038 + 100.0 * 6.381613731384277
Epoch 280, val loss: 1.703568935394287
Epoch 290, training loss: 639.1094970703125 = 1.672382116317749 + 100.0 * 6.37437105178833
Epoch 290, val loss: 1.6909605264663696
Epoch 300, training loss: 638.4849853515625 = 1.656177282333374 + 100.0 * 6.368288040161133
Epoch 300, val loss: 1.6776551008224487
Epoch 310, training loss: 638.8492431640625 = 1.638946533203125 + 100.0 * 6.372103214263916
Epoch 310, val loss: 1.6634882688522339
Epoch 320, training loss: 637.6306762695312 = 1.6205099821090698 + 100.0 * 6.360101699829102
Epoch 320, val loss: 1.648620367050171
Epoch 330, training loss: 637.0517578125 = 1.6012499332427979 + 100.0 * 6.3545050621032715
Epoch 330, val loss: 1.6332364082336426
Epoch 340, training loss: 636.7570190429688 = 1.5811890363693237 + 100.0 * 6.3517584800720215
Epoch 340, val loss: 1.6173518896102905
Epoch 350, training loss: 636.33984375 = 1.56026291847229 + 100.0 * 6.3477959632873535
Epoch 350, val loss: 1.60086989402771
Epoch 360, training loss: 635.7152709960938 = 1.5386983156204224 + 100.0 * 6.341765880584717
Epoch 360, val loss: 1.5841984748840332
Epoch 370, training loss: 635.294189453125 = 1.516687035560608 + 100.0 * 6.337775230407715
Epoch 370, val loss: 1.5674759149551392
Epoch 380, training loss: 634.9467163085938 = 1.4943761825561523 + 100.0 * 6.3345232009887695
Epoch 380, val loss: 1.5506834983825684
Epoch 390, training loss: 634.8389892578125 = 1.4714995622634888 + 100.0 * 6.333674907684326
Epoch 390, val loss: 1.5339605808258057
Epoch 400, training loss: 634.353759765625 = 1.4484957456588745 + 100.0 * 6.329052925109863
Epoch 400, val loss: 1.5171691179275513
Epoch 410, training loss: 634.2295532226562 = 1.425489902496338 + 100.0 * 6.328040599822998
Epoch 410, val loss: 1.5007879734039307
Epoch 420, training loss: 633.661865234375 = 1.4024889469146729 + 100.0 * 6.322594165802002
Epoch 420, val loss: 1.4844552278518677
Epoch 430, training loss: 633.2825317382812 = 1.379549503326416 + 100.0 * 6.319029808044434
Epoch 430, val loss: 1.4688087701797485
Epoch 440, training loss: 633.28564453125 = 1.356679081916809 + 100.0 * 6.319289684295654
Epoch 440, val loss: 1.4531599283218384
Epoch 450, training loss: 632.765380859375 = 1.33379328250885 + 100.0 * 6.3143157958984375
Epoch 450, val loss: 1.4381122589111328
Epoch 460, training loss: 632.4515991210938 = 1.311247706413269 + 100.0 * 6.311403751373291
Epoch 460, val loss: 1.4230499267578125
Epoch 470, training loss: 632.2081298828125 = 1.288961410522461 + 100.0 * 6.309192180633545
Epoch 470, val loss: 1.40877366065979
Epoch 480, training loss: 632.2088012695312 = 1.2668489217758179 + 100.0 * 6.309419631958008
Epoch 480, val loss: 1.3947644233703613
Epoch 490, training loss: 632.0017700195312 = 1.2449530363082886 + 100.0 * 6.307568073272705
Epoch 490, val loss: 1.3811184167861938
Epoch 500, training loss: 631.8787231445312 = 1.2232414484024048 + 100.0 * 6.306554794311523
Epoch 500, val loss: 1.3678014278411865
Epoch 510, training loss: 631.4188842773438 = 1.2018502950668335 + 100.0 * 6.302170276641846
Epoch 510, val loss: 1.354980707168579
Epoch 520, training loss: 631.0346069335938 = 1.1809277534484863 + 100.0 * 6.298537254333496
Epoch 520, val loss: 1.3427293300628662
Epoch 530, training loss: 630.8370971679688 = 1.160418152809143 + 100.0 * 6.296766757965088
Epoch 530, val loss: 1.330944538116455
Epoch 540, training loss: 631.0866088867188 = 1.140199899673462 + 100.0 * 6.299464225769043
Epoch 540, val loss: 1.3197815418243408
Epoch 550, training loss: 630.7282104492188 = 1.12018620967865 + 100.0 * 6.296080112457275
Epoch 550, val loss: 1.3083312511444092
Epoch 560, training loss: 630.8720703125 = 1.100501298904419 + 100.0 * 6.297715663909912
Epoch 560, val loss: 1.2979236841201782
Epoch 570, training loss: 630.2625732421875 = 1.0812112092971802 + 100.0 * 6.291813373565674
Epoch 570, val loss: 1.2878084182739258
Epoch 580, training loss: 629.962646484375 = 1.0624337196350098 + 100.0 * 6.289001941680908
Epoch 580, val loss: 1.2782342433929443
Epoch 590, training loss: 630.0371704101562 = 1.0440951585769653 + 100.0 * 6.289931297302246
Epoch 590, val loss: 1.2693414688110352
Epoch 600, training loss: 629.6200561523438 = 1.0260316133499146 + 100.0 * 6.285940170288086
Epoch 600, val loss: 1.2605301141738892
Epoch 610, training loss: 629.508544921875 = 1.0084638595581055 + 100.0 * 6.285000324249268
Epoch 610, val loss: 1.2524293661117554
Epoch 620, training loss: 629.6358642578125 = 0.9911701083183289 + 100.0 * 6.286447048187256
Epoch 620, val loss: 1.2449029684066772
Epoch 630, training loss: 629.1991577148438 = 0.974230170249939 + 100.0 * 6.2822489738464355
Epoch 630, val loss: 1.237690806388855
Epoch 640, training loss: 629.1904907226562 = 0.9577669501304626 + 100.0 * 6.282327175140381
Epoch 640, val loss: 1.2309844493865967
Epoch 650, training loss: 629.03173828125 = 0.9415258169174194 + 100.0 * 6.28090238571167
Epoch 650, val loss: 1.2248692512512207
Epoch 660, training loss: 628.7813110351562 = 0.925732433795929 + 100.0 * 6.278555870056152
Epoch 660, val loss: 1.2187014818191528
Epoch 670, training loss: 628.6236572265625 = 0.9103602170944214 + 100.0 * 6.277132987976074
Epoch 670, val loss: 1.2134835720062256
Epoch 680, training loss: 628.9660034179688 = 0.8953160047531128 + 100.0 * 6.280706882476807
Epoch 680, val loss: 1.2084687948226929
Epoch 690, training loss: 628.5791625976562 = 0.8804277181625366 + 100.0 * 6.276987552642822
Epoch 690, val loss: 1.2040044069290161
Epoch 700, training loss: 628.21240234375 = 0.8659293055534363 + 100.0 * 6.273464679718018
Epoch 700, val loss: 1.199425220489502
Epoch 710, training loss: 628.1417236328125 = 0.8519139885902405 + 100.0 * 6.272897720336914
Epoch 710, val loss: 1.1956452131271362
Epoch 720, training loss: 628.3431396484375 = 0.8381814360618591 + 100.0 * 6.275049209594727
Epoch 720, val loss: 1.1921449899673462
Epoch 730, training loss: 627.99951171875 = 0.8245949149131775 + 100.0 * 6.271749019622803
Epoch 730, val loss: 1.1890629529953003
Epoch 740, training loss: 628.204345703125 = 0.8112612962722778 + 100.0 * 6.273930549621582
Epoch 740, val loss: 1.1861757040023804
Epoch 750, training loss: 627.8063354492188 = 0.7981836199760437 + 100.0 * 6.270081996917725
Epoch 750, val loss: 1.1833776235580444
Epoch 760, training loss: 627.58203125 = 0.7854611277580261 + 100.0 * 6.267965793609619
Epoch 760, val loss: 1.1812853813171387
Epoch 770, training loss: 627.47900390625 = 0.7730939388275146 + 100.0 * 6.267059326171875
Epoch 770, val loss: 1.1794739961624146
Epoch 780, training loss: 627.9812622070312 = 0.7608739137649536 + 100.0 * 6.2722039222717285
Epoch 780, val loss: 1.1779966354370117
Epoch 790, training loss: 627.4176025390625 = 0.7486563324928284 + 100.0 * 6.266689300537109
Epoch 790, val loss: 1.1762182712554932
Epoch 800, training loss: 627.3636474609375 = 0.7368131875991821 + 100.0 * 6.266268253326416
Epoch 800, val loss: 1.1751997470855713
Epoch 810, training loss: 627.0859985351562 = 0.7251476645469666 + 100.0 * 6.263608455657959
Epoch 810, val loss: 1.1739182472229004
Epoch 820, training loss: 627.6754760742188 = 0.7137376070022583 + 100.0 * 6.269617557525635
Epoch 820, val loss: 1.1731997728347778
Epoch 830, training loss: 627.0899047851562 = 0.7023993730545044 + 100.0 * 6.2638750076293945
Epoch 830, val loss: 1.1726152896881104
Epoch 840, training loss: 626.8191528320312 = 0.6912922263145447 + 100.0 * 6.2612786293029785
Epoch 840, val loss: 1.1722157001495361
Epoch 850, training loss: 627.4508056640625 = 0.6804060339927673 + 100.0 * 6.267704010009766
Epoch 850, val loss: 1.171913743019104
Epoch 860, training loss: 626.8071899414062 = 0.6694532036781311 + 100.0 * 6.261377334594727
Epoch 860, val loss: 1.1717133522033691
Epoch 870, training loss: 626.5274047851562 = 0.6588495969772339 + 100.0 * 6.258685111999512
Epoch 870, val loss: 1.1721504926681519
Epoch 880, training loss: 626.4249267578125 = 0.648532509803772 + 100.0 * 6.257763385772705
Epoch 880, val loss: 1.172739028930664
Epoch 890, training loss: 626.9614868164062 = 0.6383493542671204 + 100.0 * 6.26323127746582
Epoch 890, val loss: 1.173928141593933
Epoch 900, training loss: 626.6336669921875 = 0.628040075302124 + 100.0 * 6.260056018829346
Epoch 900, val loss: 1.1733218431472778
Epoch 910, training loss: 626.5498046875 = 0.617961585521698 + 100.0 * 6.2593183517456055
Epoch 910, val loss: 1.1748316287994385
Epoch 920, training loss: 626.1499633789062 = 0.6080549359321594 + 100.0 * 6.2554192543029785
Epoch 920, val loss: 1.1755863428115845
Epoch 930, training loss: 626.1304931640625 = 0.5984005928039551 + 100.0 * 6.255321025848389
Epoch 930, val loss: 1.1764883995056152
Epoch 940, training loss: 626.042724609375 = 0.5888850092887878 + 100.0 * 6.254538536071777
Epoch 940, val loss: 1.1780786514282227
Epoch 950, training loss: 626.2481079101562 = 0.5795044898986816 + 100.0 * 6.256685733795166
Epoch 950, val loss: 1.1792938709259033
Epoch 960, training loss: 626.14111328125 = 0.5700731873512268 + 100.0 * 6.255710601806641
Epoch 960, val loss: 1.181425929069519
Epoch 970, training loss: 625.8335571289062 = 0.5607962012290955 + 100.0 * 6.252727508544922
Epoch 970, val loss: 1.1822725534439087
Epoch 980, training loss: 625.703857421875 = 0.5517910718917847 + 100.0 * 6.251520156860352
Epoch 980, val loss: 1.1847045421600342
Epoch 990, training loss: 626.0259399414062 = 0.5429496169090271 + 100.0 * 6.2548298835754395
Epoch 990, val loss: 1.1861928701400757
Epoch 1000, training loss: 625.9144287109375 = 0.5339549779891968 + 100.0 * 6.253805160522461
Epoch 1000, val loss: 1.188122034072876
Epoch 1010, training loss: 625.591064453125 = 0.5251229405403137 + 100.0 * 6.250658988952637
Epoch 1010, val loss: 1.1896547079086304
Epoch 1020, training loss: 625.5503540039062 = 0.5165781378746033 + 100.0 * 6.250337600708008
Epoch 1020, val loss: 1.191831350326538
Epoch 1030, training loss: 625.5799560546875 = 0.5081276893615723 + 100.0 * 6.250718116760254
Epoch 1030, val loss: 1.1938152313232422
Epoch 1040, training loss: 625.4017333984375 = 0.49972403049468994 + 100.0 * 6.249020576477051
Epoch 1040, val loss: 1.1963536739349365
Epoch 1050, training loss: 625.2742919921875 = 0.4914979934692383 + 100.0 * 6.247827529907227
Epoch 1050, val loss: 1.1987533569335938
Epoch 1060, training loss: 625.9242553710938 = 0.483342707157135 + 100.0 * 6.254409313201904
Epoch 1060, val loss: 1.2009235620498657
Epoch 1070, training loss: 625.24951171875 = 0.47503775358200073 + 100.0 * 6.247745037078857
Epoch 1070, val loss: 1.2028452157974243
Epoch 1080, training loss: 625.1023559570312 = 0.4670502245426178 + 100.0 * 6.2463531494140625
Epoch 1080, val loss: 1.205649733543396
Epoch 1090, training loss: 624.9693603515625 = 0.4592880606651306 + 100.0 * 6.245100975036621
Epoch 1090, val loss: 1.2081646919250488
Epoch 1100, training loss: 625.2261352539062 = 0.45163217186927795 + 100.0 * 6.247744560241699
Epoch 1100, val loss: 1.2111258506774902
Epoch 1110, training loss: 625.040771484375 = 0.44377562403678894 + 100.0 * 6.245969772338867
Epoch 1110, val loss: 1.2136340141296387
Epoch 1120, training loss: 624.8497924804688 = 0.43611276149749756 + 100.0 * 6.244136810302734
Epoch 1120, val loss: 1.2157132625579834
Epoch 1130, training loss: 624.7880249023438 = 0.42865854501724243 + 100.0 * 6.243593692779541
Epoch 1130, val loss: 1.2187085151672363
Epoch 1140, training loss: 624.7128295898438 = 0.4213631749153137 + 100.0 * 6.242914199829102
Epoch 1140, val loss: 1.2217392921447754
Epoch 1150, training loss: 625.9705810546875 = 0.41408899426460266 + 100.0 * 6.255565166473389
Epoch 1150, val loss: 1.2252269983291626
Epoch 1160, training loss: 624.8560180664062 = 0.4065034091472626 + 100.0 * 6.244495391845703
Epoch 1160, val loss: 1.2272599935531616
Epoch 1170, training loss: 624.5753173828125 = 0.39929288625717163 + 100.0 * 6.24176025390625
Epoch 1170, val loss: 1.2299869060516357
Epoch 1180, training loss: 624.46240234375 = 0.3923307955265045 + 100.0 * 6.240700721740723
Epoch 1180, val loss: 1.233702301979065
Epoch 1190, training loss: 624.3721313476562 = 0.38547778129577637 + 100.0 * 6.239866256713867
Epoch 1190, val loss: 1.2370096445083618
Epoch 1200, training loss: 624.5339965820312 = 0.3786781430244446 + 100.0 * 6.24155330657959
Epoch 1200, val loss: 1.2406177520751953
Epoch 1210, training loss: 624.628173828125 = 0.3718106150627136 + 100.0 * 6.242563724517822
Epoch 1210, val loss: 1.2439355850219727
Epoch 1220, training loss: 624.2898559570312 = 0.36489182710647583 + 100.0 * 6.2392497062683105
Epoch 1220, val loss: 1.2464520931243896
Epoch 1230, training loss: 624.328369140625 = 0.3582358658313751 + 100.0 * 6.239701271057129
Epoch 1230, val loss: 1.2497996091842651
Epoch 1240, training loss: 624.39111328125 = 0.3517133295536041 + 100.0 * 6.240394592285156
Epoch 1240, val loss: 1.2531583309173584
Epoch 1250, training loss: 624.09228515625 = 0.3452153503894806 + 100.0 * 6.237470626831055
Epoch 1250, val loss: 1.2570281028747559
Epoch 1260, training loss: 624.084716796875 = 0.338882714509964 + 100.0 * 6.2374587059021
Epoch 1260, val loss: 1.261220932006836
Epoch 1270, training loss: 624.2491455078125 = 0.332616925239563 + 100.0 * 6.239165306091309
Epoch 1270, val loss: 1.2652424573898315
Epoch 1280, training loss: 624.4185180664062 = 0.3263257145881653 + 100.0 * 6.240921497344971
Epoch 1280, val loss: 1.2688013315200806
Epoch 1290, training loss: 623.97412109375 = 0.320081889629364 + 100.0 * 6.236540794372559
Epoch 1290, val loss: 1.2722009420394897
Epoch 1300, training loss: 623.960693359375 = 0.31402337551116943 + 100.0 * 6.236466407775879
Epoch 1300, val loss: 1.276959776878357
Epoch 1310, training loss: 624.1751098632812 = 0.30802085995674133 + 100.0 * 6.238670825958252
Epoch 1310, val loss: 1.280650019645691
Epoch 1320, training loss: 623.8604125976562 = 0.3020561635494232 + 100.0 * 6.235583782196045
Epoch 1320, val loss: 1.2841800451278687
Epoch 1330, training loss: 623.7350463867188 = 0.2962947487831116 + 100.0 * 6.2343878746032715
Epoch 1330, val loss: 1.2890100479125977
Epoch 1340, training loss: 623.91552734375 = 0.29066359996795654 + 100.0 * 6.23624849319458
Epoch 1340, val loss: 1.2932523488998413
Epoch 1350, training loss: 623.6585693359375 = 0.2849274277687073 + 100.0 * 6.233736515045166
Epoch 1350, val loss: 1.2975431680679321
Epoch 1360, training loss: 623.6844482421875 = 0.2793155014514923 + 100.0 * 6.23405122756958
Epoch 1360, val loss: 1.3021142482757568
Epoch 1370, training loss: 623.7622680664062 = 0.27384939789772034 + 100.0 * 6.234884262084961
Epoch 1370, val loss: 1.30692720413208
Epoch 1380, training loss: 623.817626953125 = 0.26845529675483704 + 100.0 * 6.23549222946167
Epoch 1380, val loss: 1.3112057447433472
Epoch 1390, training loss: 623.4606323242188 = 0.26316213607788086 + 100.0 * 6.2319746017456055
Epoch 1390, val loss: 1.3156369924545288
Epoch 1400, training loss: 623.4859619140625 = 0.25803548097610474 + 100.0 * 6.232278823852539
Epoch 1400, val loss: 1.3208051919937134
Epoch 1410, training loss: 623.6387329101562 = 0.25298163294792175 + 100.0 * 6.233857154846191
Epoch 1410, val loss: 1.3253812789916992
Epoch 1420, training loss: 623.4746704101562 = 0.24791735410690308 + 100.0 * 6.232267379760742
Epoch 1420, val loss: 1.3301794528961182
Epoch 1430, training loss: 623.5438842773438 = 0.2429622858762741 + 100.0 * 6.233009338378906
Epoch 1430, val loss: 1.3348816633224487
Epoch 1440, training loss: 623.3077392578125 = 0.23811407387256622 + 100.0 * 6.230696678161621
Epoch 1440, val loss: 1.3407669067382812
Epoch 1450, training loss: 623.7622680664062 = 0.23343513906002045 + 100.0 * 6.235288143157959
Epoch 1450, val loss: 1.3462415933609009
Epoch 1460, training loss: 623.4389038085938 = 0.22864721715450287 + 100.0 * 6.232102870941162
Epoch 1460, val loss: 1.3501180410385132
Epoch 1470, training loss: 623.1903076171875 = 0.22406119108200073 + 100.0 * 6.2296624183654785
Epoch 1470, val loss: 1.3556331396102905
Epoch 1480, training loss: 623.0828857421875 = 0.21963678300380707 + 100.0 * 6.22863245010376
Epoch 1480, val loss: 1.3609988689422607
Epoch 1490, training loss: 623.1550903320312 = 0.21533001959323883 + 100.0 * 6.229397773742676
Epoch 1490, val loss: 1.3662571907043457
Epoch 1500, training loss: 623.354248046875 = 0.21099404990673065 + 100.0 * 6.2314324378967285
Epoch 1500, val loss: 1.3716458082199097
Epoch 1510, training loss: 623.3972778320312 = 0.20669175684452057 + 100.0 * 6.231905937194824
Epoch 1510, val loss: 1.377551555633545
Epoch 1520, training loss: 623.0567016601562 = 0.20245425403118134 + 100.0 * 6.228542327880859
Epoch 1520, val loss: 1.381745457649231
Epoch 1530, training loss: 622.9924926757812 = 0.19835013151168823 + 100.0 * 6.227941513061523
Epoch 1530, val loss: 1.387248158454895
Epoch 1540, training loss: 623.2024536132812 = 0.19438521564006805 + 100.0 * 6.230080604553223
Epoch 1540, val loss: 1.3925796747207642
Epoch 1550, training loss: 622.8174438476562 = 0.1905050128698349 + 100.0 * 6.226269721984863
Epoch 1550, val loss: 1.3987865447998047
Epoch 1560, training loss: 622.858642578125 = 0.18675900995731354 + 100.0 * 6.226718902587891
Epoch 1560, val loss: 1.4050238132476807
Epoch 1570, training loss: 622.9041137695312 = 0.18307599425315857 + 100.0 * 6.22721004486084
Epoch 1570, val loss: 1.4109768867492676
Epoch 1580, training loss: 622.8484497070312 = 0.17942526936531067 + 100.0 * 6.22668981552124
Epoch 1580, val loss: 1.4164888858795166
Epoch 1590, training loss: 623.6718139648438 = 0.175826758146286 + 100.0 * 6.234959602355957
Epoch 1590, val loss: 1.4212055206298828
Epoch 1600, training loss: 622.79443359375 = 0.17216305434703827 + 100.0 * 6.226222515106201
Epoch 1600, val loss: 1.4278827905654907
Epoch 1610, training loss: 622.6210327148438 = 0.16871537268161774 + 100.0 * 6.224523544311523
Epoch 1610, val loss: 1.4331802129745483
Epoch 1620, training loss: 622.5652465820312 = 0.16544771194458008 + 100.0 * 6.223997592926025
Epoch 1620, val loss: 1.4399789571762085
Epoch 1630, training loss: 622.5812377929688 = 0.16224078834056854 + 100.0 * 6.2241902351379395
Epoch 1630, val loss: 1.446012020111084
Epoch 1640, training loss: 623.2742309570312 = 0.15905500948429108 + 100.0 * 6.231151580810547
Epoch 1640, val loss: 1.4519258737564087
Epoch 1650, training loss: 622.784912109375 = 0.15581656992435455 + 100.0 * 6.226291179656982
Epoch 1650, val loss: 1.4572404623031616
Epoch 1660, training loss: 622.8575439453125 = 0.15274332463741302 + 100.0 * 6.227047920227051
Epoch 1660, val loss: 1.4633501768112183
Epoch 1670, training loss: 622.5628662109375 = 0.14972594380378723 + 100.0 * 6.2241315841674805
Epoch 1670, val loss: 1.4698325395584106
Epoch 1680, training loss: 622.564697265625 = 0.1467769891023636 + 100.0 * 6.224178791046143
Epoch 1680, val loss: 1.4764394760131836
Epoch 1690, training loss: 622.377197265625 = 0.14391300082206726 + 100.0 * 6.222332954406738
Epoch 1690, val loss: 1.4823715686798096
Epoch 1700, training loss: 622.433837890625 = 0.14114156365394592 + 100.0 * 6.222927093505859
Epoch 1700, val loss: 1.4887561798095703
Epoch 1710, training loss: 622.4451293945312 = 0.1384078562259674 + 100.0 * 6.223067283630371
Epoch 1710, val loss: 1.4952689409255981
Epoch 1720, training loss: 623.1289672851562 = 0.13570687174797058 + 100.0 * 6.22993278503418
Epoch 1720, val loss: 1.5023446083068848
Epoch 1730, training loss: 622.6801147460938 = 0.13295766711235046 + 100.0 * 6.2254719734191895
Epoch 1730, val loss: 1.5064404010772705
Epoch 1740, training loss: 622.8372192382812 = 0.13033419847488403 + 100.0 * 6.227068901062012
Epoch 1740, val loss: 1.5128490924835205
Epoch 1750, training loss: 622.422119140625 = 0.1277589648962021 + 100.0 * 6.2229437828063965
Epoch 1750, val loss: 1.5193325281143188
Epoch 1760, training loss: 622.2320556640625 = 0.12528686225414276 + 100.0 * 6.221067428588867
Epoch 1760, val loss: 1.5259356498718262
Epoch 1770, training loss: 622.09716796875 = 0.12292654067277908 + 100.0 * 6.219742298126221
Epoch 1770, val loss: 1.532364845275879
Epoch 1780, training loss: 622.1068725585938 = 0.1206204891204834 + 100.0 * 6.219862937927246
Epoch 1780, val loss: 1.5386886596679688
Epoch 1790, training loss: 622.4869995117188 = 0.1183524876832962 + 100.0 * 6.223686218261719
Epoch 1790, val loss: 1.544450283050537
Epoch 1800, training loss: 622.1428833007812 = 0.11601797491312027 + 100.0 * 6.220268249511719
Epoch 1800, val loss: 1.5517996549606323
Epoch 1810, training loss: 622.1094970703125 = 0.11379017680883408 + 100.0 * 6.21995735168457
Epoch 1810, val loss: 1.5576190948486328
Epoch 1820, training loss: 622.3844604492188 = 0.11164216697216034 + 100.0 * 6.222728252410889
Epoch 1820, val loss: 1.5635243654251099
Epoch 1830, training loss: 622.1141357421875 = 0.10949277132749557 + 100.0 * 6.220046520233154
Epoch 1830, val loss: 1.5703072547912598
Epoch 1840, training loss: 622.0878295898438 = 0.10739839822053909 + 100.0 * 6.219803810119629
Epoch 1840, val loss: 1.5766154527664185
Epoch 1850, training loss: 622.0510864257812 = 0.10539884865283966 + 100.0 * 6.219456672668457
Epoch 1850, val loss: 1.5832555294036865
Epoch 1860, training loss: 622.2777709960938 = 0.10343179851770401 + 100.0 * 6.221743106842041
Epoch 1860, val loss: 1.588866114616394
Epoch 1870, training loss: 622.029052734375 = 0.10143187642097473 + 100.0 * 6.219276428222656
Epoch 1870, val loss: 1.5953882932662964
Epoch 1880, training loss: 621.9530639648438 = 0.0994923934340477 + 100.0 * 6.218535900115967
Epoch 1880, val loss: 1.6015719175338745
Epoch 1890, training loss: 621.9473876953125 = 0.0976598858833313 + 100.0 * 6.218497276306152
Epoch 1890, val loss: 1.6082799434661865
Epoch 1900, training loss: 621.9624633789062 = 0.09586261957883835 + 100.0 * 6.218666076660156
Epoch 1900, val loss: 1.6143007278442383
Epoch 1910, training loss: 621.990966796875 = 0.0941021665930748 + 100.0 * 6.218968391418457
Epoch 1910, val loss: 1.620954155921936
Epoch 1920, training loss: 622.1051025390625 = 0.09235944598913193 + 100.0 * 6.220127582550049
Epoch 1920, val loss: 1.6275705099105835
Epoch 1930, training loss: 622.0947265625 = 0.09063603729009628 + 100.0 * 6.220040798187256
Epoch 1930, val loss: 1.6334112882614136
Epoch 1940, training loss: 622.0440063476562 = 0.08893986791372299 + 100.0 * 6.219550609588623
Epoch 1940, val loss: 1.638777494430542
Epoch 1950, training loss: 621.7284545898438 = 0.0872996523976326 + 100.0 * 6.216411590576172
Epoch 1950, val loss: 1.645690679550171
Epoch 1960, training loss: 621.76904296875 = 0.08573658019304276 + 100.0 * 6.216832637786865
Epoch 1960, val loss: 1.6517996788024902
Epoch 1970, training loss: 622.3333740234375 = 0.08420080691576004 + 100.0 * 6.22249174118042
Epoch 1970, val loss: 1.6570684909820557
Epoch 1980, training loss: 621.8171997070312 = 0.08262182772159576 + 100.0 * 6.217345714569092
Epoch 1980, val loss: 1.664574384689331
Epoch 1990, training loss: 621.6332397460938 = 0.08115476369857788 + 100.0 * 6.21552038192749
Epoch 1990, val loss: 1.6703037023544312
Epoch 2000, training loss: 621.5926513671875 = 0.07973254472017288 + 100.0 * 6.215129375457764
Epoch 2000, val loss: 1.676729440689087
Epoch 2010, training loss: 622.7965698242188 = 0.07834166288375854 + 100.0 * 6.227181911468506
Epoch 2010, val loss: 1.6820282936096191
Epoch 2020, training loss: 622.138916015625 = 0.07686139643192291 + 100.0 * 6.220620632171631
Epoch 2020, val loss: 1.689626932144165
Epoch 2030, training loss: 621.6605834960938 = 0.07543889433145523 + 100.0 * 6.215851306915283
Epoch 2030, val loss: 1.6946223974227905
Epoch 2040, training loss: 621.46337890625 = 0.07412439584732056 + 100.0 * 6.213892936706543
Epoch 2040, val loss: 1.701145052909851
Epoch 2050, training loss: 621.5 = 0.07286673784255981 + 100.0 * 6.214271545410156
Epoch 2050, val loss: 1.7075273990631104
Epoch 2060, training loss: 622.1443481445312 = 0.0716080442070961 + 100.0 * 6.220727443695068
Epoch 2060, val loss: 1.7134253978729248
Epoch 2070, training loss: 621.6532592773438 = 0.0703381597995758 + 100.0 * 6.215829372406006
Epoch 2070, val loss: 1.7195788621902466
Epoch 2080, training loss: 622.0186767578125 = 0.06909490376710892 + 100.0 * 6.21949577331543
Epoch 2080, val loss: 1.7262039184570312
Epoch 2090, training loss: 621.5989990234375 = 0.0678747147321701 + 100.0 * 6.215311527252197
Epoch 2090, val loss: 1.7311480045318604
Epoch 2100, training loss: 621.415771484375 = 0.0667044147849083 + 100.0 * 6.2134904861450195
Epoch 2100, val loss: 1.7375963926315308
Epoch 2110, training loss: 621.385498046875 = 0.06559425592422485 + 100.0 * 6.213198661804199
Epoch 2110, val loss: 1.7437161207199097
Epoch 2120, training loss: 621.5902709960938 = 0.06450191140174866 + 100.0 * 6.21525764465332
Epoch 2120, val loss: 1.7500165700912476
Epoch 2130, training loss: 621.5302124023438 = 0.06338799744844437 + 100.0 * 6.214667797088623
Epoch 2130, val loss: 1.7555055618286133
Epoch 2140, training loss: 621.6412963867188 = 0.06230371445417404 + 100.0 * 6.215789794921875
Epoch 2140, val loss: 1.7604033946990967
Epoch 2150, training loss: 621.4960327148438 = 0.06124763563275337 + 100.0 * 6.214348316192627
Epoch 2150, val loss: 1.7674401998519897
Epoch 2160, training loss: 621.469482421875 = 0.060225438326597214 + 100.0 * 6.214092254638672
Epoch 2160, val loss: 1.7721196413040161
Epoch 2170, training loss: 621.5515747070312 = 0.059213586151599884 + 100.0 * 6.214923858642578
Epoch 2170, val loss: 1.7779335975646973
Epoch 2180, training loss: 621.3724975585938 = 0.05821135640144348 + 100.0 * 6.2131428718566895
Epoch 2180, val loss: 1.7838817834854126
Epoch 2190, training loss: 621.24853515625 = 0.05726703628897667 + 100.0 * 6.211912631988525
Epoch 2190, val loss: 1.7896093130111694
Epoch 2200, training loss: 621.2357788085938 = 0.05635463818907738 + 100.0 * 6.211794376373291
Epoch 2200, val loss: 1.7957009077072144
Epoch 2210, training loss: 621.6716918945312 = 0.05545078590512276 + 100.0 * 6.216162204742432
Epoch 2210, val loss: 1.8005554676055908
Epoch 2220, training loss: 621.247314453125 = 0.05451507121324539 + 100.0 * 6.211928367614746
Epoch 2220, val loss: 1.8069294691085815
Epoch 2230, training loss: 621.2234497070312 = 0.05362687259912491 + 100.0 * 6.211698055267334
Epoch 2230, val loss: 1.8126119375228882
Epoch 2240, training loss: 621.3722534179688 = 0.05278196930885315 + 100.0 * 6.213194370269775
Epoch 2240, val loss: 1.8182145357131958
Epoch 2250, training loss: 621.5695190429688 = 0.05191973224282265 + 100.0 * 6.215175628662109
Epoch 2250, val loss: 1.8236901760101318
Epoch 2260, training loss: 621.1716918945312 = 0.051047492772340775 + 100.0 * 6.211206912994385
Epoch 2260, val loss: 1.82872474193573
Epoch 2270, training loss: 621.0603637695312 = 0.05026248097419739 + 100.0 * 6.2101006507873535
Epoch 2270, val loss: 1.834945559501648
Epoch 2280, training loss: 621.02978515625 = 0.049499161541461945 + 100.0 * 6.209802627563477
Epoch 2280, val loss: 1.840749740600586
Epoch 2290, training loss: 621.2994384765625 = 0.048766255378723145 + 100.0 * 6.2125067710876465
Epoch 2290, val loss: 1.8471087217330933
Epoch 2300, training loss: 621.1441650390625 = 0.047978732734918594 + 100.0 * 6.210961818695068
Epoch 2300, val loss: 1.8520957231521606
Epoch 2310, training loss: 621.2431640625 = 0.04721280187368393 + 100.0 * 6.211959362030029
Epoch 2310, val loss: 1.8571048974990845
Epoch 2320, training loss: 621.0126953125 = 0.04647181183099747 + 100.0 * 6.209662437438965
Epoch 2320, val loss: 1.862263560295105
Epoch 2330, training loss: 621.3140258789062 = 0.04577646404504776 + 100.0 * 6.212682723999023
Epoch 2330, val loss: 1.8685837984085083
Epoch 2340, training loss: 621.0460815429688 = 0.04506867751479149 + 100.0 * 6.210010528564453
Epoch 2340, val loss: 1.8736640214920044
Epoch 2350, training loss: 620.9739379882812 = 0.04439462721347809 + 100.0 * 6.209295749664307
Epoch 2350, val loss: 1.8787927627563477
Epoch 2360, training loss: 621.2550659179688 = 0.04373912140727043 + 100.0 * 6.212112903594971
Epoch 2360, val loss: 1.884852409362793
Epoch 2370, training loss: 621.1837158203125 = 0.04306308552622795 + 100.0 * 6.211406230926514
Epoch 2370, val loss: 1.890325903892517
Epoch 2380, training loss: 620.910888671875 = 0.04239877685904503 + 100.0 * 6.208684921264648
Epoch 2380, val loss: 1.8941950798034668
Epoch 2390, training loss: 620.8969116210938 = 0.04178580269217491 + 100.0 * 6.208550930023193
Epoch 2390, val loss: 1.8997931480407715
Epoch 2400, training loss: 621.7593383789062 = 0.04119066521525383 + 100.0 * 6.217181205749512
Epoch 2400, val loss: 1.9049947261810303
Epoch 2410, training loss: 621.0562133789062 = 0.040552448481321335 + 100.0 * 6.2101569175720215
Epoch 2410, val loss: 1.9110392332077026
Epoch 2420, training loss: 620.8587646484375 = 0.03995317965745926 + 100.0 * 6.208188056945801
Epoch 2420, val loss: 1.915632724761963
Epoch 2430, training loss: 620.866943359375 = 0.03939766436815262 + 100.0 * 6.20827579498291
Epoch 2430, val loss: 1.9210566282272339
Epoch 2440, training loss: 621.6333618164062 = 0.03884703665971756 + 100.0 * 6.215945720672607
Epoch 2440, val loss: 1.9255187511444092
Epoch 2450, training loss: 620.9564208984375 = 0.038238659501075745 + 100.0 * 6.209181785583496
Epoch 2450, val loss: 1.9318517446517944
Epoch 2460, training loss: 620.8985595703125 = 0.03769935667514801 + 100.0 * 6.208608627319336
Epoch 2460, val loss: 1.936391830444336
Epoch 2470, training loss: 621.1587524414062 = 0.037156619131565094 + 100.0 * 6.211215972900391
Epoch 2470, val loss: 1.9415656328201294
Epoch 2480, training loss: 620.989013671875 = 0.03661835193634033 + 100.0 * 6.209523677825928
Epoch 2480, val loss: 1.9456920623779297
Epoch 2490, training loss: 620.904296875 = 0.036097317934036255 + 100.0 * 6.208682537078857
Epoch 2490, val loss: 1.950868010520935
Epoch 2500, training loss: 620.6923217773438 = 0.03559360280632973 + 100.0 * 6.206567287445068
Epoch 2500, val loss: 1.9569404125213623
Epoch 2510, training loss: 620.7540283203125 = 0.035117167979478836 + 100.0 * 6.207189559936523
Epoch 2510, val loss: 1.962083101272583
Epoch 2520, training loss: 621.3629150390625 = 0.034632354974746704 + 100.0 * 6.213283061981201
Epoch 2520, val loss: 1.9671863317489624
Epoch 2530, training loss: 620.9160766601562 = 0.03412775322794914 + 100.0 * 6.20881986618042
Epoch 2530, val loss: 1.9713977575302124
Epoch 2540, training loss: 620.7367553710938 = 0.033657338470220566 + 100.0 * 6.20703125
Epoch 2540, val loss: 1.9770605564117432
Epoch 2550, training loss: 620.843994140625 = 0.033213648945093155 + 100.0 * 6.208107948303223
Epoch 2550, val loss: 1.9821747541427612
Epoch 2560, training loss: 620.8710327148438 = 0.03275851905345917 + 100.0 * 6.208382606506348
Epoch 2560, val loss: 1.987375259399414
Epoch 2570, training loss: 620.976806640625 = 0.0323227196931839 + 100.0 * 6.209444522857666
Epoch 2570, val loss: 1.991952657699585
Epoch 2580, training loss: 620.8483276367188 = 0.03187461197376251 + 100.0 * 6.208164691925049
Epoch 2580, val loss: 1.9968467950820923
Epoch 2590, training loss: 620.7101440429688 = 0.03143465891480446 + 100.0 * 6.206787109375
Epoch 2590, val loss: 2.0010690689086914
Epoch 2600, training loss: 620.5482788085938 = 0.031014492735266685 + 100.0 * 6.205173015594482
Epoch 2600, val loss: 2.0056838989257812
Epoch 2610, training loss: 620.5259399414062 = 0.03061959519982338 + 100.0 * 6.204952716827393
Epoch 2610, val loss: 2.010497570037842
Epoch 2620, training loss: 620.7659301757812 = 0.03023768775165081 + 100.0 * 6.207356929779053
Epoch 2620, val loss: 2.0149807929992676
Epoch 2630, training loss: 620.623046875 = 0.029828045517206192 + 100.0 * 6.205932140350342
Epoch 2630, val loss: 2.02021861076355
Epoch 2640, training loss: 620.8455810546875 = 0.029440073296427727 + 100.0 * 6.2081618309021
Epoch 2640, val loss: 2.025580406188965
Epoch 2650, training loss: 621.1373291015625 = 0.0290469229221344 + 100.0 * 6.211082935333252
Epoch 2650, val loss: 2.029127359390259
Epoch 2660, training loss: 620.5873413085938 = 0.028642933815717697 + 100.0 * 6.205587387084961
Epoch 2660, val loss: 2.0345897674560547
Epoch 2670, training loss: 620.4932250976562 = 0.028273578733205795 + 100.0 * 6.204649448394775
Epoch 2670, val loss: 2.0389187335968018
Epoch 2680, training loss: 620.845458984375 = 0.027929527685046196 + 100.0 * 6.208175182342529
Epoch 2680, val loss: 2.0449352264404297
Epoch 2690, training loss: 620.4483032226562 = 0.027558166533708572 + 100.0 * 6.204206943511963
Epoch 2690, val loss: 2.047966957092285
Epoch 2700, training loss: 620.478759765625 = 0.027210256084799767 + 100.0 * 6.20451545715332
Epoch 2700, val loss: 2.0517818927764893
Epoch 2710, training loss: 620.3592529296875 = 0.026883840560913086 + 100.0 * 6.203323841094971
Epoch 2710, val loss: 2.057596206665039
Epoch 2720, training loss: 620.4306640625 = 0.026566307991743088 + 100.0 * 6.204041004180908
Epoch 2720, val loss: 2.062403440475464
Epoch 2730, training loss: 621.1426391601562 = 0.026246149092912674 + 100.0 * 6.2111639976501465
Epoch 2730, val loss: 2.0661559104919434
Epoch 2740, training loss: 620.7445068359375 = 0.02589654177427292 + 100.0 * 6.207186222076416
Epoch 2740, val loss: 2.07016921043396
Epoch 2750, training loss: 620.4630737304688 = 0.025566767901182175 + 100.0 * 6.20437479019165
Epoch 2750, val loss: 2.0756313800811768
Epoch 2760, training loss: 620.4488525390625 = 0.025266027078032494 + 100.0 * 6.204235553741455
Epoch 2760, val loss: 2.0800061225891113
Epoch 2770, training loss: 620.5932006835938 = 0.02496178261935711 + 100.0 * 6.205682277679443
Epoch 2770, val loss: 2.0843913555145264
Epoch 2780, training loss: 620.342529296875 = 0.024656502529978752 + 100.0 * 6.203178405761719
Epoch 2780, val loss: 2.0886991024017334
Epoch 2790, training loss: 620.8294067382812 = 0.02436862699687481 + 100.0 * 6.208050727844238
Epoch 2790, val loss: 2.0922155380249023
Epoch 2800, training loss: 620.6014404296875 = 0.024059873074293137 + 100.0 * 6.205773830413818
Epoch 2800, val loss: 2.0967183113098145
Epoch 2810, training loss: 620.4783935546875 = 0.023771388456225395 + 100.0 * 6.2045464515686035
Epoch 2810, val loss: 2.1010470390319824
Epoch 2820, training loss: 620.2830810546875 = 0.023488856852054596 + 100.0 * 6.2025957107543945
Epoch 2820, val loss: 2.1064260005950928
Epoch 2830, training loss: 620.5043334960938 = 0.02322574146091938 + 100.0 * 6.204811096191406
Epoch 2830, val loss: 2.111119270324707
Epoch 2840, training loss: 620.4345092773438 = 0.02294883504509926 + 100.0 * 6.204115867614746
Epoch 2840, val loss: 2.115006685256958
Epoch 2850, training loss: 620.4306030273438 = 0.02267400547862053 + 100.0 * 6.204079627990723
Epoch 2850, val loss: 2.1186254024505615
Epoch 2860, training loss: 620.4016723632812 = 0.022407852113246918 + 100.0 * 6.203792572021484
Epoch 2860, val loss: 2.1226963996887207
Epoch 2870, training loss: 620.2564086914062 = 0.022150903940200806 + 100.0 * 6.202342510223389
Epoch 2870, val loss: 2.1274454593658447
Epoch 2880, training loss: 620.3777465820312 = 0.021908707916736603 + 100.0 * 6.203558444976807
Epoch 2880, val loss: 2.1317083835601807
Epoch 2890, training loss: 620.4205932617188 = 0.021656394004821777 + 100.0 * 6.203989028930664
Epoch 2890, val loss: 2.135235071182251
Epoch 2900, training loss: 620.4253540039062 = 0.021402662619948387 + 100.0 * 6.204039573669434
Epoch 2900, val loss: 2.138122081756592
Epoch 2910, training loss: 620.4393920898438 = 0.021153640002012253 + 100.0 * 6.204182147979736
Epoch 2910, val loss: 2.142874002456665
Epoch 2920, training loss: 620.1883544921875 = 0.02091454528272152 + 100.0 * 6.201674461364746
Epoch 2920, val loss: 2.1478002071380615
Epoch 2930, training loss: 620.2025146484375 = 0.02068723924458027 + 100.0 * 6.201818466186523
Epoch 2930, val loss: 2.1516661643981934
Epoch 2940, training loss: 620.4520263671875 = 0.020465372130274773 + 100.0 * 6.204315662384033
Epoch 2940, val loss: 2.1549229621887207
Epoch 2950, training loss: 620.4766845703125 = 0.020231526345014572 + 100.0 * 6.204565048217773
Epoch 2950, val loss: 2.1586973667144775
Epoch 2960, training loss: 620.1768188476562 = 0.019987767562270164 + 100.0 * 6.201568603515625
Epoch 2960, val loss: 2.1635124683380127
Epoch 2970, training loss: 620.0980224609375 = 0.01977016218006611 + 100.0 * 6.200782299041748
Epoch 2970, val loss: 2.167391777038574
Epoch 2980, training loss: 620.1704711914062 = 0.01956799440085888 + 100.0 * 6.201508522033691
Epoch 2980, val loss: 2.1714062690734863
Epoch 2990, training loss: 620.3975830078125 = 0.01936192624270916 + 100.0 * 6.203782081604004
Epoch 2990, val loss: 2.174776077270508
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6481481481481481
0.8112809699525567
=== training gcn model ===
Epoch 0, training loss: 861.6261596679688 = 1.9435863494873047 + 100.0 * 8.59682559967041
Epoch 0, val loss: 1.9462194442749023
Epoch 10, training loss: 861.5376586914062 = 1.9353313446044922 + 100.0 * 8.596023559570312
Epoch 10, val loss: 1.9373055696487427
Epoch 20, training loss: 861.03271484375 = 1.924747347831726 + 100.0 * 8.591079711914062
Epoch 20, val loss: 1.9257744550704956
Epoch 30, training loss: 857.4778442382812 = 1.9108707904815674 + 100.0 * 8.555669784545898
Epoch 30, val loss: 1.9106085300445557
Epoch 40, training loss: 829.1673583984375 = 1.8936352729797363 + 100.0 * 8.272737503051758
Epoch 40, val loss: 1.8920061588287354
Epoch 50, training loss: 755.9514770507812 = 1.8735557794570923 + 100.0 * 7.5407795906066895
Epoch 50, val loss: 1.8711599111557007
Epoch 60, training loss: 729.4157104492188 = 1.8612773418426514 + 100.0 * 7.2755446434021
Epoch 60, val loss: 1.8595852851867676
Epoch 70, training loss: 706.373291015625 = 1.8526623249053955 + 100.0 * 7.045206069946289
Epoch 70, val loss: 1.8510034084320068
Epoch 80, training loss: 692.3900756835938 = 1.843915581703186 + 100.0 * 6.905461311340332
Epoch 80, val loss: 1.8428224325180054
Epoch 90, training loss: 684.727294921875 = 1.8356208801269531 + 100.0 * 6.828916549682617
Epoch 90, val loss: 1.8349896669387817
Epoch 100, training loss: 678.3134155273438 = 1.8274943828582764 + 100.0 * 6.764858722686768
Epoch 100, val loss: 1.8273500204086304
Epoch 110, training loss: 671.9752807617188 = 1.8211688995361328 + 100.0 * 6.701540946960449
Epoch 110, val loss: 1.8211452960968018
Epoch 120, training loss: 666.8190307617188 = 1.8159312009811401 + 100.0 * 6.650031089782715
Epoch 120, val loss: 1.8157230615615845
Epoch 130, training loss: 663.0231323242188 = 1.810665249824524 + 100.0 * 6.612124919891357
Epoch 130, val loss: 1.810184359550476
Epoch 140, training loss: 660.0435791015625 = 1.8052573204040527 + 100.0 * 6.582383155822754
Epoch 140, val loss: 1.8045884370803833
Epoch 150, training loss: 657.217529296875 = 1.7997795343399048 + 100.0 * 6.554177284240723
Epoch 150, val loss: 1.7989033460617065
Epoch 160, training loss: 654.9075927734375 = 1.793994665145874 + 100.0 * 6.5311360359191895
Epoch 160, val loss: 1.7930426597595215
Epoch 170, training loss: 652.9718627929688 = 1.7878222465515137 + 100.0 * 6.511840343475342
Epoch 170, val loss: 1.7867332696914673
Epoch 180, training loss: 651.2042236328125 = 1.7811095714569092 + 100.0 * 6.4942307472229
Epoch 180, val loss: 1.7800464630126953
Epoch 190, training loss: 649.9827880859375 = 1.773828148841858 + 100.0 * 6.482089996337891
Epoch 190, val loss: 1.7728697061538696
Epoch 200, training loss: 648.4992065429688 = 1.7658640146255493 + 100.0 * 6.4673333168029785
Epoch 200, val loss: 1.7651015520095825
Epoch 210, training loss: 647.6298828125 = 1.7572319507598877 + 100.0 * 6.45872688293457
Epoch 210, val loss: 1.756791114807129
Epoch 220, training loss: 646.304443359375 = 1.7478327751159668 + 100.0 * 6.445566177368164
Epoch 220, val loss: 1.7477307319641113
Epoch 230, training loss: 645.2057495117188 = 1.7377381324768066 + 100.0 * 6.434679985046387
Epoch 230, val loss: 1.737943172454834
Epoch 240, training loss: 644.2115478515625 = 1.7267900705337524 + 100.0 * 6.424847602844238
Epoch 240, val loss: 1.7274683713912964
Epoch 250, training loss: 643.3125610351562 = 1.7149293422698975 + 100.0 * 6.415976524353027
Epoch 250, val loss: 1.716224193572998
Epoch 260, training loss: 642.5005493164062 = 1.7021498680114746 + 100.0 * 6.407983779907227
Epoch 260, val loss: 1.7040988206863403
Epoch 270, training loss: 641.797607421875 = 1.6884292364120483 + 100.0 * 6.401092052459717
Epoch 270, val loss: 1.6910992860794067
Epoch 280, training loss: 641.0449829101562 = 1.6737287044525146 + 100.0 * 6.393712997436523
Epoch 280, val loss: 1.6773120164871216
Epoch 290, training loss: 640.6822509765625 = 1.6580289602279663 + 100.0 * 6.390242576599121
Epoch 290, val loss: 1.6626933813095093
Epoch 300, training loss: 639.8198852539062 = 1.6414531469345093 + 100.0 * 6.381783962249756
Epoch 300, val loss: 1.6472033262252808
Epoch 310, training loss: 639.166259765625 = 1.6240373849868774 + 100.0 * 6.375422477722168
Epoch 310, val loss: 1.6310863494873047
Epoch 320, training loss: 638.918212890625 = 1.605921983718872 + 100.0 * 6.3731231689453125
Epoch 320, val loss: 1.6143656969070435
Epoch 330, training loss: 638.24951171875 = 1.5869166851043701 + 100.0 * 6.366626262664795
Epoch 330, val loss: 1.5972819328308105
Epoch 340, training loss: 637.7003173828125 = 1.5674915313720703 + 100.0 * 6.361328125
Epoch 340, val loss: 1.5797308683395386
Epoch 350, training loss: 637.2733764648438 = 1.5475032329559326 + 100.0 * 6.3572587966918945
Epoch 350, val loss: 1.5620249509811401
Epoch 360, training loss: 636.8757934570312 = 1.52718186378479 + 100.0 * 6.35348653793335
Epoch 360, val loss: 1.5442018508911133
Epoch 370, training loss: 636.8777465820312 = 1.5065683126449585 + 100.0 * 6.3537116050720215
Epoch 370, val loss: 1.5264121294021606
Epoch 380, training loss: 636.1571044921875 = 1.4857574701309204 + 100.0 * 6.346713542938232
Epoch 380, val loss: 1.5084346532821655
Epoch 390, training loss: 635.6764526367188 = 1.4647701978683472 + 100.0 * 6.342116832733154
Epoch 390, val loss: 1.4907419681549072
Epoch 400, training loss: 635.3194580078125 = 1.4439489841461182 + 100.0 * 6.338754653930664
Epoch 400, val loss: 1.473317265510559
Epoch 410, training loss: 635.2184448242188 = 1.423143982887268 + 100.0 * 6.337953090667725
Epoch 410, val loss: 1.4562405347824097
Epoch 420, training loss: 634.8624877929688 = 1.402295470237732 + 100.0 * 6.334602355957031
Epoch 420, val loss: 1.4393495321273804
Epoch 430, training loss: 634.57568359375 = 1.381651520729065 + 100.0 * 6.331940174102783
Epoch 430, val loss: 1.4227010011672974
Epoch 440, training loss: 634.1861572265625 = 1.3609435558319092 + 100.0 * 6.328251838684082
Epoch 440, val loss: 1.4066377878189087
Epoch 450, training loss: 633.8197021484375 = 1.3405927419662476 + 100.0 * 6.324791431427002
Epoch 450, val loss: 1.3907432556152344
Epoch 460, training loss: 633.499267578125 = 1.320327639579773 + 100.0 * 6.321789264678955
Epoch 460, val loss: 1.3753859996795654
Epoch 470, training loss: 633.5272827148438 = 1.3002721071243286 + 100.0 * 6.322269916534424
Epoch 470, val loss: 1.360377311706543
Epoch 480, training loss: 633.2393798828125 = 1.2802833318710327 + 100.0 * 6.319591045379639
Epoch 480, val loss: 1.345795750617981
Epoch 490, training loss: 632.7450561523438 = 1.260400414466858 + 100.0 * 6.314846992492676
Epoch 490, val loss: 1.3315397500991821
Epoch 500, training loss: 632.6798095703125 = 1.2407488822937012 + 100.0 * 6.314391136169434
Epoch 500, val loss: 1.3177903890609741
Epoch 510, training loss: 632.3381958007812 = 1.2215131521224976 + 100.0 * 6.311167240142822
Epoch 510, val loss: 1.304218053817749
Epoch 520, training loss: 632.2583618164062 = 1.2022104263305664 + 100.0 * 6.310561656951904
Epoch 520, val loss: 1.291171908378601
Epoch 530, training loss: 631.8080444335938 = 1.183362364768982 + 100.0 * 6.306246757507324
Epoch 530, val loss: 1.278762698173523
Epoch 540, training loss: 631.5660400390625 = 1.1648149490356445 + 100.0 * 6.304012298583984
Epoch 540, val loss: 1.2667206525802612
Epoch 550, training loss: 631.64306640625 = 1.146585464477539 + 100.0 * 6.304964542388916
Epoch 550, val loss: 1.254975438117981
Epoch 560, training loss: 631.457763671875 = 1.1285520792007446 + 100.0 * 6.303292274475098
Epoch 560, val loss: 1.2435307502746582
Epoch 570, training loss: 630.9216918945312 = 1.1107606887817383 + 100.0 * 6.29810905456543
Epoch 570, val loss: 1.2328704595565796
Epoch 580, training loss: 630.7368774414062 = 1.0935332775115967 + 100.0 * 6.296433448791504
Epoch 580, val loss: 1.2226887941360474
Epoch 590, training loss: 630.9074096679688 = 1.0766689777374268 + 100.0 * 6.298307418823242
Epoch 590, val loss: 1.2129963636398315
Epoch 600, training loss: 630.7491455078125 = 1.060045838356018 + 100.0 * 6.296890735626221
Epoch 600, val loss: 1.2034494876861572
Epoch 610, training loss: 630.2763671875 = 1.0438551902770996 + 100.0 * 6.292324542999268
Epoch 610, val loss: 1.1944001913070679
Epoch 620, training loss: 630.2211303710938 = 1.027971625328064 + 100.0 * 6.291931629180908
Epoch 620, val loss: 1.185884952545166
Epoch 630, training loss: 629.8145751953125 = 1.012601375579834 + 100.0 * 6.288020133972168
Epoch 630, val loss: 1.1777961254119873
Epoch 640, training loss: 629.8771362304688 = 0.9974429607391357 + 100.0 * 6.288796901702881
Epoch 640, val loss: 1.1702123880386353
Epoch 650, training loss: 629.6138916015625 = 0.9826694130897522 + 100.0 * 6.286312103271484
Epoch 650, val loss: 1.162591576576233
Epoch 660, training loss: 629.4525756835938 = 0.9680472612380981 + 100.0 * 6.28484582901001
Epoch 660, val loss: 1.155754566192627
Epoch 670, training loss: 629.16845703125 = 0.9540094137191772 + 100.0 * 6.282145023345947
Epoch 670, val loss: 1.149196982383728
Epoch 680, training loss: 629.5595703125 = 0.9402640461921692 + 100.0 * 6.286193370819092
Epoch 680, val loss: 1.142912745475769
Epoch 690, training loss: 629.3547973632812 = 0.9267008304595947 + 100.0 * 6.284281253814697
Epoch 690, val loss: 1.1370614767074585
Epoch 700, training loss: 628.8020629882812 = 0.9133058190345764 + 100.0 * 6.2788872718811035
Epoch 700, val loss: 1.1314314603805542
Epoch 710, training loss: 628.83203125 = 0.9003517627716064 + 100.0 * 6.2793169021606445
Epoch 710, val loss: 1.1262011528015137
Epoch 720, training loss: 628.4701538085938 = 0.8876229524612427 + 100.0 * 6.275825500488281
Epoch 720, val loss: 1.1213759183883667
Epoch 730, training loss: 628.8139038085938 = 0.8752055764198303 + 100.0 * 6.2793869972229
Epoch 730, val loss: 1.116723656654358
Epoch 740, training loss: 628.6810913085938 = 0.8628182411193848 + 100.0 * 6.2781829833984375
Epoch 740, val loss: 1.1122572422027588
Epoch 750, training loss: 628.220703125 = 0.8506042957305908 + 100.0 * 6.273701190948486
Epoch 750, val loss: 1.108155369758606
Epoch 760, training loss: 628.2059936523438 = 0.8387182354927063 + 100.0 * 6.273672580718994
Epoch 760, val loss: 1.1043579578399658
Epoch 770, training loss: 627.9465942382812 = 0.8271358013153076 + 100.0 * 6.2711944580078125
Epoch 770, val loss: 1.1008111238479614
Epoch 780, training loss: 627.8336181640625 = 0.8156257271766663 + 100.0 * 6.2701802253723145
Epoch 780, val loss: 1.0975427627563477
Epoch 790, training loss: 627.85986328125 = 0.8043376207351685 + 100.0 * 6.27055549621582
Epoch 790, val loss: 1.0944157838821411
Epoch 800, training loss: 627.7052612304688 = 0.7930061221122742 + 100.0 * 6.269122123718262
Epoch 800, val loss: 1.091318964958191
Epoch 810, training loss: 627.6408081054688 = 0.7819207310676575 + 100.0 * 6.268589019775391
Epoch 810, val loss: 1.0887185335159302
Epoch 820, training loss: 627.4088745117188 = 0.7710660696029663 + 100.0 * 6.266377925872803
Epoch 820, val loss: 1.0864529609680176
Epoch 830, training loss: 627.4002685546875 = 0.7603217363357544 + 100.0 * 6.266399383544922
Epoch 830, val loss: 1.0840644836425781
Epoch 840, training loss: 627.4356689453125 = 0.7497289180755615 + 100.0 * 6.26685905456543
Epoch 840, val loss: 1.0820496082305908
Epoch 850, training loss: 627.1183471679688 = 0.7391067743301392 + 100.0 * 6.263792037963867
Epoch 850, val loss: 1.0801531076431274
Epoch 860, training loss: 626.9869995117188 = 0.7286843061447144 + 100.0 * 6.262582778930664
Epoch 860, val loss: 1.0785053968429565
Epoch 870, training loss: 626.96240234375 = 0.7184212803840637 + 100.0 * 6.262439727783203
Epoch 870, val loss: 1.0770273208618164
Epoch 880, training loss: 627.1076049804688 = 0.7083026170730591 + 100.0 * 6.263992786407471
Epoch 880, val loss: 1.075743317604065
Epoch 890, training loss: 626.8735961914062 = 0.6979902982711792 + 100.0 * 6.26175594329834
Epoch 890, val loss: 1.0741043090820312
Epoch 900, training loss: 627.0364990234375 = 0.688117265701294 + 100.0 * 6.263484001159668
Epoch 900, val loss: 1.0731556415557861
Epoch 910, training loss: 626.7269897460938 = 0.6779419183731079 + 100.0 * 6.260490894317627
Epoch 910, val loss: 1.0720770359039307
Epoch 920, training loss: 626.651611328125 = 0.6681364178657532 + 100.0 * 6.2598347663879395
Epoch 920, val loss: 1.0712885856628418
Epoch 930, training loss: 626.3745727539062 = 0.6582372784614563 + 100.0 * 6.257163047790527
Epoch 930, val loss: 1.070490837097168
Epoch 940, training loss: 626.260009765625 = 0.6485419869422913 + 100.0 * 6.256114482879639
Epoch 940, val loss: 1.0698884725570679
Epoch 950, training loss: 626.3676147460938 = 0.6388834714889526 + 100.0 * 6.257287502288818
Epoch 950, val loss: 1.0692596435546875
Epoch 960, training loss: 626.3760986328125 = 0.6291414499282837 + 100.0 * 6.257469654083252
Epoch 960, val loss: 1.0689033269882202
Epoch 970, training loss: 626.2091064453125 = 0.6194740533828735 + 100.0 * 6.25589656829834
Epoch 970, val loss: 1.0684926509857178
Epoch 980, training loss: 626.0518798828125 = 0.6098756790161133 + 100.0 * 6.254420280456543
Epoch 980, val loss: 1.0682047605514526
Epoch 990, training loss: 626.3170166015625 = 0.6003906726837158 + 100.0 * 6.257165908813477
Epoch 990, val loss: 1.0680999755859375
Epoch 1000, training loss: 626.0017700195312 = 0.5907453298568726 + 100.0 * 6.254110336303711
Epoch 1000, val loss: 1.067865014076233
Epoch 1010, training loss: 625.758056640625 = 0.5813305377960205 + 100.0 * 6.251766681671143
Epoch 1010, val loss: 1.0679177045822144
Epoch 1020, training loss: 625.6249389648438 = 0.571972668170929 + 100.0 * 6.250529766082764
Epoch 1020, val loss: 1.0680979490280151
Epoch 1030, training loss: 625.8635864257812 = 0.5627509951591492 + 100.0 * 6.2530083656311035
Epoch 1030, val loss: 1.0683982372283936
Epoch 1040, training loss: 625.5225830078125 = 0.5534281730651855 + 100.0 * 6.249691963195801
Epoch 1040, val loss: 1.0685456991195679
Epoch 1050, training loss: 626.0572509765625 = 0.5442383885383606 + 100.0 * 6.255130290985107
Epoch 1050, val loss: 1.0688540935516357
Epoch 1060, training loss: 625.603271484375 = 0.5349084138870239 + 100.0 * 6.250683784484863
Epoch 1060, val loss: 1.069145679473877
Epoch 1070, training loss: 625.3235473632812 = 0.5257846713066101 + 100.0 * 6.2479777336120605
Epoch 1070, val loss: 1.0694411993026733
Epoch 1080, training loss: 625.2528686523438 = 0.5167510509490967 + 100.0 * 6.247361660003662
Epoch 1080, val loss: 1.0701594352722168
Epoch 1090, training loss: 625.845458984375 = 0.5077491402626038 + 100.0 * 6.2533769607543945
Epoch 1090, val loss: 1.070677638053894
Epoch 1100, training loss: 625.2337036132812 = 0.49883267283439636 + 100.0 * 6.247348785400391
Epoch 1100, val loss: 1.0713151693344116
Epoch 1110, training loss: 624.9759521484375 = 0.48987847566604614 + 100.0 * 6.244861125946045
Epoch 1110, val loss: 1.0718961954116821
Epoch 1120, training loss: 624.9412231445312 = 0.4811238944530487 + 100.0 * 6.244600772857666
Epoch 1120, val loss: 1.072596549987793
Epoch 1130, training loss: 625.5979614257812 = 0.47249656915664673 + 100.0 * 6.251255035400391
Epoch 1130, val loss: 1.0733144283294678
Epoch 1140, training loss: 625.246337890625 = 0.46369442343711853 + 100.0 * 6.24782657623291
Epoch 1140, val loss: 1.0740424394607544
Epoch 1150, training loss: 625.0380249023438 = 0.45506682991981506 + 100.0 * 6.2458295822143555
Epoch 1150, val loss: 1.074863314628601
Epoch 1160, training loss: 624.6979370117188 = 0.4466463625431061 + 100.0 * 6.2425127029418945
Epoch 1160, val loss: 1.0761427879333496
Epoch 1170, training loss: 624.7361450195312 = 0.43833690881729126 + 100.0 * 6.242977619171143
Epoch 1170, val loss: 1.0774213075637817
Epoch 1180, training loss: 624.944580078125 = 0.4301072657108307 + 100.0 * 6.245144367218018
Epoch 1180, val loss: 1.078433871269226
Epoch 1190, training loss: 625.0009155273438 = 0.4218679368495941 + 100.0 * 6.245790481567383
Epoch 1190, val loss: 1.0792509317398071
Epoch 1200, training loss: 624.6094970703125 = 0.4138053357601166 + 100.0 * 6.24195671081543
Epoch 1200, val loss: 1.0807467699050903
Epoch 1210, training loss: 624.4775390625 = 0.4058490991592407 + 100.0 * 6.24071741104126
Epoch 1210, val loss: 1.0818407535552979
Epoch 1220, training loss: 624.73486328125 = 0.3980615437030792 + 100.0 * 6.243368148803711
Epoch 1220, val loss: 1.083323359489441
Epoch 1230, training loss: 624.3955688476562 = 0.39032626152038574 + 100.0 * 6.240052700042725
Epoch 1230, val loss: 1.0845147371292114
Epoch 1240, training loss: 624.6170043945312 = 0.38271573185920715 + 100.0 * 6.242342948913574
Epoch 1240, val loss: 1.085850477218628
Epoch 1250, training loss: 624.2680053710938 = 0.37523314356803894 + 100.0 * 6.238927364349365
Epoch 1250, val loss: 1.0874974727630615
Epoch 1260, training loss: 624.1541748046875 = 0.36789724230766296 + 100.0 * 6.237862586975098
Epoch 1260, val loss: 1.0891010761260986
Epoch 1270, training loss: 624.16748046875 = 0.36071842908859253 + 100.0 * 6.238067626953125
Epoch 1270, val loss: 1.0909086465835571
Epoch 1280, training loss: 624.7115478515625 = 0.35369017720222473 + 100.0 * 6.2435784339904785
Epoch 1280, val loss: 1.0926085710525513
Epoch 1290, training loss: 624.418701171875 = 0.3465469181537628 + 100.0 * 6.240721702575684
Epoch 1290, val loss: 1.0939712524414062
Epoch 1300, training loss: 624.0159301757812 = 0.3396974503993988 + 100.0 * 6.236762046813965
Epoch 1300, val loss: 1.096118450164795
Epoch 1310, training loss: 623.9307861328125 = 0.33298441767692566 + 100.0 * 6.235977649688721
Epoch 1310, val loss: 1.098114013671875
Epoch 1320, training loss: 623.9974365234375 = 0.32644620537757874 + 100.0 * 6.236710071563721
Epoch 1320, val loss: 1.100288987159729
Epoch 1330, training loss: 624.7006225585938 = 0.3200186491012573 + 100.0 * 6.243805885314941
Epoch 1330, val loss: 1.102180004119873
Epoch 1340, training loss: 624.1448974609375 = 0.3133784234523773 + 100.0 * 6.238315582275391
Epoch 1340, val loss: 1.1037877798080444
Epoch 1350, training loss: 623.8554077148438 = 0.30709874629974365 + 100.0 * 6.235482692718506
Epoch 1350, val loss: 1.105806827545166
Epoch 1360, training loss: 623.6917724609375 = 0.30101653933525085 + 100.0 * 6.233907699584961
Epoch 1360, val loss: 1.1082249879837036
Epoch 1370, training loss: 623.600341796875 = 0.29508471488952637 + 100.0 * 6.2330522537231445
Epoch 1370, val loss: 1.110884666442871
Epoch 1380, training loss: 623.6610107421875 = 0.2892626225948334 + 100.0 * 6.233717918395996
Epoch 1380, val loss: 1.1132625341415405
Epoch 1390, training loss: 624.0208129882812 = 0.2834973931312561 + 100.0 * 6.237372875213623
Epoch 1390, val loss: 1.1157687902450562
Epoch 1400, training loss: 623.7620239257812 = 0.27775371074676514 + 100.0 * 6.234842300415039
Epoch 1400, val loss: 1.117795705795288
Epoch 1410, training loss: 623.5784301757812 = 0.27220407128334045 + 100.0 * 6.233062267303467
Epoch 1410, val loss: 1.1208233833312988
Epoch 1420, training loss: 623.4678955078125 = 0.2667625844478607 + 100.0 * 6.232011318206787
Epoch 1420, val loss: 1.1236584186553955
Epoch 1430, training loss: 623.4212646484375 = 0.261499285697937 + 100.0 * 6.231597900390625
Epoch 1430, val loss: 1.1263887882232666
Epoch 1440, training loss: 623.7926635742188 = 0.2563490569591522 + 100.0 * 6.235363006591797
Epoch 1440, val loss: 1.1293357610702515
Epoch 1450, training loss: 623.4633178710938 = 0.2512318789958954 + 100.0 * 6.232120990753174
Epoch 1450, val loss: 1.1321475505828857
Epoch 1460, training loss: 623.425537109375 = 0.24619491398334503 + 100.0 * 6.231793403625488
Epoch 1460, val loss: 1.1348135471343994
Epoch 1470, training loss: 623.27197265625 = 0.2412509322166443 + 100.0 * 6.230307102203369
Epoch 1470, val loss: 1.1378257274627686
Epoch 1480, training loss: 623.1651000976562 = 0.23648759722709656 + 100.0 * 6.229285717010498
Epoch 1480, val loss: 1.1408836841583252
Epoch 1490, training loss: 623.1984252929688 = 0.2318768948316574 + 100.0 * 6.229665756225586
Epoch 1490, val loss: 1.1442025899887085
Epoch 1500, training loss: 623.9868774414062 = 0.22733289003372192 + 100.0 * 6.237595081329346
Epoch 1500, val loss: 1.1472973823547363
Epoch 1510, training loss: 623.3594970703125 = 0.22275486588478088 + 100.0 * 6.231367111206055
Epoch 1510, val loss: 1.1501561403274536
Epoch 1520, training loss: 623.0866088867188 = 0.21830172836780548 + 100.0 * 6.228682994842529
Epoch 1520, val loss: 1.1534544229507446
Epoch 1530, training loss: 623.000244140625 = 0.21403293311595917 + 100.0 * 6.2278618812561035
Epoch 1530, val loss: 1.1568536758422852
Epoch 1540, training loss: 623.0255737304688 = 0.20987951755523682 + 100.0 * 6.228157043457031
Epoch 1540, val loss: 1.1601676940917969
Epoch 1550, training loss: 623.5612182617188 = 0.20581601560115814 + 100.0 * 6.233553886413574
Epoch 1550, val loss: 1.1636269092559814
Epoch 1560, training loss: 623.3175048828125 = 0.20176561176776886 + 100.0 * 6.231157302856445
Epoch 1560, val loss: 1.166767954826355
Epoch 1570, training loss: 623.1204833984375 = 0.19777502119541168 + 100.0 * 6.229227542877197
Epoch 1570, val loss: 1.1701120138168335
Epoch 1580, training loss: 623.022705078125 = 0.1939212679862976 + 100.0 * 6.228287696838379
Epoch 1580, val loss: 1.1738145351409912
Epoch 1590, training loss: 622.9865112304688 = 0.19017468392848969 + 100.0 * 6.227962970733643
Epoch 1590, val loss: 1.1774429082870483
Epoch 1600, training loss: 623.0468139648438 = 0.18648529052734375 + 100.0 * 6.228603363037109
Epoch 1600, val loss: 1.180871844291687
Epoch 1610, training loss: 622.8827514648438 = 0.1828226000070572 + 100.0 * 6.226999282836914
Epoch 1610, val loss: 1.1843739748001099
Epoch 1620, training loss: 622.7174072265625 = 0.17930497229099274 + 100.0 * 6.225380897521973
Epoch 1620, val loss: 1.1881731748580933
Epoch 1630, training loss: 623.0183715820312 = 0.17590895295143127 + 100.0 * 6.228424549102783
Epoch 1630, val loss: 1.1920949220657349
Epoch 1640, training loss: 622.7811279296875 = 0.17248278856277466 + 100.0 * 6.226086139678955
Epoch 1640, val loss: 1.1958116292953491
Epoch 1650, training loss: 622.6907348632812 = 0.16912367939949036 + 100.0 * 6.225215911865234
Epoch 1650, val loss: 1.1993330717086792
Epoch 1660, training loss: 622.6734008789062 = 0.16589277982711792 + 100.0 * 6.2250752449035645
Epoch 1660, val loss: 1.2032253742218018
Epoch 1670, training loss: 622.8030395507812 = 0.16275180876255035 + 100.0 * 6.226402282714844
Epoch 1670, val loss: 1.2070966958999634
Epoch 1680, training loss: 622.6390991210938 = 0.15965576469898224 + 100.0 * 6.224794387817383
Epoch 1680, val loss: 1.2109112739562988
Epoch 1690, training loss: 622.7288208007812 = 0.15664967894554138 + 100.0 * 6.22572135925293
Epoch 1690, val loss: 1.2149854898452759
Epoch 1700, training loss: 622.609375 = 0.1536540389060974 + 100.0 * 6.224557399749756
Epoch 1700, val loss: 1.218751072883606
Epoch 1710, training loss: 622.4739379882812 = 0.15073011815547943 + 100.0 * 6.223231792449951
Epoch 1710, val loss: 1.222678542137146
Epoch 1720, training loss: 622.9278564453125 = 0.1478903591632843 + 100.0 * 6.227799415588379
Epoch 1720, val loss: 1.2262986898422241
Epoch 1730, training loss: 622.453369140625 = 0.14509032666683197 + 100.0 * 6.223083019256592
Epoch 1730, val loss: 1.2308204174041748
Epoch 1740, training loss: 622.3539428710938 = 0.14234086871147156 + 100.0 * 6.222115993499756
Epoch 1740, val loss: 1.2344071865081787
Epoch 1750, training loss: 622.3258666992188 = 0.13971392810344696 + 100.0 * 6.221861362457275
Epoch 1750, val loss: 1.2387934923171997
Epoch 1760, training loss: 622.6624145507812 = 0.13714520633220673 + 100.0 * 6.225252628326416
Epoch 1760, val loss: 1.2426241636276245
Epoch 1770, training loss: 622.2748413085938 = 0.13456088304519653 + 100.0 * 6.221402645111084
Epoch 1770, val loss: 1.2467161417007446
Epoch 1780, training loss: 622.3623657226562 = 0.13204443454742432 + 100.0 * 6.22230339050293
Epoch 1780, val loss: 1.2506738901138306
Epoch 1790, training loss: 622.4153442382812 = 0.1296045184135437 + 100.0 * 6.22285795211792
Epoch 1790, val loss: 1.25493586063385
Epoch 1800, training loss: 622.39697265625 = 0.12721993029117584 + 100.0 * 6.222697734832764
Epoch 1800, val loss: 1.2588608264923096
Epoch 1810, training loss: 622.4120483398438 = 0.12487533688545227 + 100.0 * 6.222871780395508
Epoch 1810, val loss: 1.2632652521133423
Epoch 1820, training loss: 622.1384887695312 = 0.1225447729229927 + 100.0 * 6.22015905380249
Epoch 1820, val loss: 1.2674299478530884
Epoch 1830, training loss: 622.1547241210938 = 0.12031903862953186 + 100.0 * 6.220344543457031
Epoch 1830, val loss: 1.2717292308807373
Epoch 1840, training loss: 622.37255859375 = 0.11815151572227478 + 100.0 * 6.222544193267822
Epoch 1840, val loss: 1.2758480310440063
Epoch 1850, training loss: 622.3170166015625 = 0.11595667898654938 + 100.0 * 6.222010612487793
Epoch 1850, val loss: 1.2798254489898682
Epoch 1860, training loss: 622.1436767578125 = 0.11383090913295746 + 100.0 * 6.2202982902526855
Epoch 1860, val loss: 1.284248948097229
Epoch 1870, training loss: 622.5067138671875 = 0.11174654215574265 + 100.0 * 6.223949432373047
Epoch 1870, val loss: 1.288329005241394
Epoch 1880, training loss: 622.1890258789062 = 0.10970013588666916 + 100.0 * 6.220793724060059
Epoch 1880, val loss: 1.292995810508728
Epoch 1890, training loss: 622.1954956054688 = 0.10768662393093109 + 100.0 * 6.2208781242370605
Epoch 1890, val loss: 1.297082543373108
Epoch 1900, training loss: 621.9991455078125 = 0.10570865869522095 + 100.0 * 6.218934535980225
Epoch 1900, val loss: 1.301369071006775
Epoch 1910, training loss: 622.040771484375 = 0.10381224751472473 + 100.0 * 6.219369411468506
Epoch 1910, val loss: 1.3058489561080933
Epoch 1920, training loss: 621.9384155273438 = 0.10195260494947433 + 100.0 * 6.218364715576172
Epoch 1920, val loss: 1.3103229999542236
Epoch 1930, training loss: 622.0918579101562 = 0.10014209151268005 + 100.0 * 6.219916820526123
Epoch 1930, val loss: 1.3149749040603638
Epoch 1940, training loss: 622.1296997070312 = 0.0983426570892334 + 100.0 * 6.220314025878906
Epoch 1940, val loss: 1.319192886352539
Epoch 1950, training loss: 621.948486328125 = 0.09654499590396881 + 100.0 * 6.21851921081543
Epoch 1950, val loss: 1.3232837915420532
Epoch 1960, training loss: 621.8143310546875 = 0.0948212742805481 + 100.0 * 6.2171950340271
Epoch 1960, val loss: 1.3281131982803345
Epoch 1970, training loss: 622.1033935546875 = 0.09315859526395798 + 100.0 * 6.220102787017822
Epoch 1970, val loss: 1.3321195840835571
Epoch 1980, training loss: 622.157470703125 = 0.09149914979934692 + 100.0 * 6.2206597328186035
Epoch 1980, val loss: 1.3369238376617432
Epoch 1990, training loss: 621.85791015625 = 0.0898185595870018 + 100.0 * 6.217680931091309
Epoch 1990, val loss: 1.3408536911010742
Epoch 2000, training loss: 621.770751953125 = 0.08821453899145126 + 100.0 * 6.216825485229492
Epoch 2000, val loss: 1.3456865549087524
Epoch 2010, training loss: 621.6763305664062 = 0.08668252825737 + 100.0 * 6.2158966064453125
Epoch 2010, val loss: 1.3502986431121826
Epoch 2020, training loss: 621.817138671875 = 0.08519832789897919 + 100.0 * 6.217319488525391
Epoch 2020, val loss: 1.3549690246582031
Epoch 2030, training loss: 622.0697631835938 = 0.08370800316333771 + 100.0 * 6.219860553741455
Epoch 2030, val loss: 1.3590037822723389
Epoch 2040, training loss: 621.9210815429688 = 0.08218617737293243 + 100.0 * 6.218388557434082
Epoch 2040, val loss: 1.363317608833313
Epoch 2050, training loss: 621.65771484375 = 0.08074069768190384 + 100.0 * 6.2157697677612305
Epoch 2050, val loss: 1.3678590059280396
Epoch 2060, training loss: 621.5908203125 = 0.07933513820171356 + 100.0 * 6.215114593505859
Epoch 2060, val loss: 1.3724365234375
Epoch 2070, training loss: 621.7833251953125 = 0.0779830813407898 + 100.0 * 6.217053413391113
Epoch 2070, val loss: 1.376983880996704
Epoch 2080, training loss: 621.6302490234375 = 0.07662627100944519 + 100.0 * 6.215536594390869
Epoch 2080, val loss: 1.3811718225479126
Epoch 2090, training loss: 621.6679077148438 = 0.0753021240234375 + 100.0 * 6.215925693511963
Epoch 2090, val loss: 1.3857359886169434
Epoch 2100, training loss: 621.7377319335938 = 0.07401217520236969 + 100.0 * 6.216636657714844
Epoch 2100, val loss: 1.39011812210083
Epoch 2110, training loss: 621.7651977539062 = 0.0727524533867836 + 100.0 * 6.21692419052124
Epoch 2110, val loss: 1.3947193622589111
Epoch 2120, training loss: 622.0225219726562 = 0.07150855660438538 + 100.0 * 6.219510078430176
Epoch 2120, val loss: 1.39919114112854
Epoch 2130, training loss: 621.6068115234375 = 0.07023622840642929 + 100.0 * 6.215365886688232
Epoch 2130, val loss: 1.402915596961975
Epoch 2140, training loss: 621.4473266601562 = 0.06904027611017227 + 100.0 * 6.213783264160156
Epoch 2140, val loss: 1.4078580141067505
Epoch 2150, training loss: 621.3502807617188 = 0.06788591295480728 + 100.0 * 6.21282434463501
Epoch 2150, val loss: 1.4121766090393066
Epoch 2160, training loss: 621.61083984375 = 0.06676623225212097 + 100.0 * 6.21544075012207
Epoch 2160, val loss: 1.4162768125534058
Epoch 2170, training loss: 621.4808959960938 = 0.06562986224889755 + 100.0 * 6.214152812957764
Epoch 2170, val loss: 1.4209164381027222
Epoch 2180, training loss: 621.4426879882812 = 0.06450694799423218 + 100.0 * 6.213781833648682
Epoch 2180, val loss: 1.4247421026229858
Epoch 2190, training loss: 621.4166870117188 = 0.06342602521181107 + 100.0 * 6.213532447814941
Epoch 2190, val loss: 1.4295865297317505
Epoch 2200, training loss: 621.5408935546875 = 0.06238465756177902 + 100.0 * 6.214785099029541
Epoch 2200, val loss: 1.4338258504867554
Epoch 2210, training loss: 621.5825805664062 = 0.061358582228422165 + 100.0 * 6.215212345123291
Epoch 2210, val loss: 1.4385117292404175
Epoch 2220, training loss: 621.376708984375 = 0.06032595410943031 + 100.0 * 6.21316385269165
Epoch 2220, val loss: 1.442527174949646
Epoch 2230, training loss: 621.2273559570312 = 0.059328556060791016 + 100.0 * 6.2116804122924805
Epoch 2230, val loss: 1.4469804763793945
Epoch 2240, training loss: 621.2605590820312 = 0.05837250128388405 + 100.0 * 6.212022304534912
Epoch 2240, val loss: 1.4513665437698364
Epoch 2250, training loss: 621.9420776367188 = 0.057454779744148254 + 100.0 * 6.218846321105957
Epoch 2250, val loss: 1.4553190469741821
Epoch 2260, training loss: 621.7723999023438 = 0.05649392679333687 + 100.0 * 6.217158794403076
Epoch 2260, val loss: 1.4600050449371338
Epoch 2270, training loss: 621.2323608398438 = 0.0555255264043808 + 100.0 * 6.21176815032959
Epoch 2270, val loss: 1.4635318517684937
Epoch 2280, training loss: 621.1251831054688 = 0.05463562160730362 + 100.0 * 6.210705280303955
Epoch 2280, val loss: 1.4683173894882202
Epoch 2290, training loss: 621.0985717773438 = 0.05377157777547836 + 100.0 * 6.210447788238525
Epoch 2290, val loss: 1.472719430923462
Epoch 2300, training loss: 621.4971923828125 = 0.05293755233287811 + 100.0 * 6.214442729949951
Epoch 2300, val loss: 1.4768832921981812
Epoch 2310, training loss: 621.0827026367188 = 0.052079085260629654 + 100.0 * 6.210306167602539
Epoch 2310, val loss: 1.481010913848877
Epoch 2320, training loss: 621.2764282226562 = 0.051255710422992706 + 100.0 * 6.212251663208008
Epoch 2320, val loss: 1.4856253862380981
Epoch 2330, training loss: 621.3587646484375 = 0.050437211990356445 + 100.0 * 6.213083267211914
Epoch 2330, val loss: 1.4893697500228882
Epoch 2340, training loss: 621.058837890625 = 0.04961328208446503 + 100.0 * 6.210092067718506
Epoch 2340, val loss: 1.4933744668960571
Epoch 2350, training loss: 621.0302124023438 = 0.048848602920770645 + 100.0 * 6.209813594818115
Epoch 2350, val loss: 1.4978811740875244
Epoch 2360, training loss: 621.4652099609375 = 0.04812224209308624 + 100.0 * 6.214171409606934
Epoch 2360, val loss: 1.5019848346710205
Epoch 2370, training loss: 620.986328125 = 0.047345120459795 + 100.0 * 6.209389686584473
Epoch 2370, val loss: 1.5061038732528687
Epoch 2380, training loss: 621.0701293945312 = 0.046620968729257584 + 100.0 * 6.210235118865967
Epoch 2380, val loss: 1.5104283094406128
Epoch 2390, training loss: 621.5875854492188 = 0.04592360183596611 + 100.0 * 6.21541690826416
Epoch 2390, val loss: 1.5143259763717651
Epoch 2400, training loss: 621.1407470703125 = 0.045186907052993774 + 100.0 * 6.210955619812012
Epoch 2400, val loss: 1.5180188417434692
Epoch 2410, training loss: 620.9595336914062 = 0.04449988901615143 + 100.0 * 6.209150314331055
Epoch 2410, val loss: 1.5225809812545776
Epoch 2420, training loss: 620.9193115234375 = 0.04383305460214615 + 100.0 * 6.208755016326904
Epoch 2420, val loss: 1.5266523361206055
Epoch 2430, training loss: 621.1083984375 = 0.04319518804550171 + 100.0 * 6.210651874542236
Epoch 2430, val loss: 1.5307852029800415
Epoch 2440, training loss: 621.0956420898438 = 0.04253821074962616 + 100.0 * 6.210530757904053
Epoch 2440, val loss: 1.5343660116195679
Epoch 2450, training loss: 621.083740234375 = 0.04190380871295929 + 100.0 * 6.210418224334717
Epoch 2450, val loss: 1.5385457277297974
Epoch 2460, training loss: 621.14697265625 = 0.041271235793828964 + 100.0 * 6.211057186126709
Epoch 2460, val loss: 1.542305588722229
Epoch 2470, training loss: 620.8887329101562 = 0.040654897689819336 + 100.0 * 6.2084808349609375
Epoch 2470, val loss: 1.5466097593307495
Epoch 2480, training loss: 620.9619750976562 = 0.04006476327776909 + 100.0 * 6.209219455718994
Epoch 2480, val loss: 1.5506348609924316
Epoch 2490, training loss: 621.561767578125 = 0.039477698504924774 + 100.0 * 6.2152228355407715
Epoch 2490, val loss: 1.5540859699249268
Epoch 2500, training loss: 621.0037841796875 = 0.03888363763689995 + 100.0 * 6.209649085998535
Epoch 2500, val loss: 1.5583710670471191
Epoch 2510, training loss: 620.8051147460938 = 0.03830762580037117 + 100.0 * 6.207667827606201
Epoch 2510, val loss: 1.5621083974838257
Epoch 2520, training loss: 621.3185424804688 = 0.03780006989836693 + 100.0 * 6.212807655334473
Epoch 2520, val loss: 1.566731572151184
Epoch 2530, training loss: 620.7675170898438 = 0.03720526397228241 + 100.0 * 6.207303047180176
Epoch 2530, val loss: 1.5694857835769653
Epoch 2540, training loss: 620.7174682617188 = 0.036663491278886795 + 100.0 * 6.206808090209961
Epoch 2540, val loss: 1.5739271640777588
Epoch 2550, training loss: 620.6864624023438 = 0.036137957125902176 + 100.0 * 6.206503391265869
Epoch 2550, val loss: 1.5776554346084595
Epoch 2560, training loss: 621.0755615234375 = 0.03565455973148346 + 100.0 * 6.210399150848389
Epoch 2560, val loss: 1.5813156366348267
Epoch 2570, training loss: 620.745361328125 = 0.03512844070792198 + 100.0 * 6.207101821899414
Epoch 2570, val loss: 1.5852715969085693
Epoch 2580, training loss: 620.6685180664062 = 0.034617744386196136 + 100.0 * 6.206338882446289
Epoch 2580, val loss: 1.5887302160263062
Epoch 2590, training loss: 620.7288208007812 = 0.034136392176151276 + 100.0 * 6.206947326660156
Epoch 2590, val loss: 1.5928457975387573
Epoch 2600, training loss: 621.0003662109375 = 0.0336693674325943 + 100.0 * 6.209666728973389
Epoch 2600, val loss: 1.5962778329849243
Epoch 2610, training loss: 620.6240234375 = 0.03319553658366203 + 100.0 * 6.205908298492432
Epoch 2610, val loss: 1.6006795167922974
Epoch 2620, training loss: 620.7101440429688 = 0.032751381397247314 + 100.0 * 6.20677375793457
Epoch 2620, val loss: 1.604211449623108
Epoch 2630, training loss: 621.1259155273438 = 0.032309725880622864 + 100.0 * 6.210936546325684
Epoch 2630, val loss: 1.6078909635543823
Epoch 2640, training loss: 620.7363891601562 = 0.031848613172769547 + 100.0 * 6.207045555114746
Epoch 2640, val loss: 1.611720085144043
Epoch 2650, training loss: 620.6845703125 = 0.03141803294420242 + 100.0 * 6.206531524658203
Epoch 2650, val loss: 1.615371823310852
Epoch 2660, training loss: 620.7409057617188 = 0.030998261645436287 + 100.0 * 6.207098960876465
Epoch 2660, val loss: 1.6192197799682617
Epoch 2670, training loss: 620.60498046875 = 0.03057858534157276 + 100.0 * 6.20574426651001
Epoch 2670, val loss: 1.6228108406066895
Epoch 2680, training loss: 620.9088134765625 = 0.030180858448147774 + 100.0 * 6.208786487579346
Epoch 2680, val loss: 1.6261305809020996
Epoch 2690, training loss: 621.0011596679688 = 0.029762327671051025 + 100.0 * 6.209713935852051
Epoch 2690, val loss: 1.6292037963867188
Epoch 2700, training loss: 620.5521240234375 = 0.029360821470618248 + 100.0 * 6.205227851867676
Epoch 2700, val loss: 1.6335827112197876
Epoch 2710, training loss: 620.4739379882812 = 0.028970174491405487 + 100.0 * 6.20444917678833
Epoch 2710, val loss: 1.6369959115982056
Epoch 2720, training loss: 620.4522705078125 = 0.028602195903658867 + 100.0 * 6.2042365074157715
Epoch 2720, val loss: 1.640810251235962
Epoch 2730, training loss: 620.8298950195312 = 0.028261883184313774 + 100.0 * 6.208016395568848
Epoch 2730, val loss: 1.6447043418884277
Epoch 2740, training loss: 620.6490478515625 = 0.02786763198673725 + 100.0 * 6.206211566925049
Epoch 2740, val loss: 1.6473302841186523
Epoch 2750, training loss: 620.4396362304688 = 0.0274952445179224 + 100.0 * 6.2041215896606445
Epoch 2750, val loss: 1.6507636308670044
Epoch 2760, training loss: 620.475341796875 = 0.027142196893692017 + 100.0 * 6.204482078552246
Epoch 2760, val loss: 1.6543869972229004
Epoch 2770, training loss: 621.0406494140625 = 0.026813579723238945 + 100.0 * 6.210138320922852
Epoch 2770, val loss: 1.6580376625061035
Epoch 2780, training loss: 620.5081787109375 = 0.026448138058185577 + 100.0 * 6.204817771911621
Epoch 2780, val loss: 1.6608988046646118
Epoch 2790, training loss: 620.3450927734375 = 0.026114804670214653 + 100.0 * 6.203189849853516
Epoch 2790, val loss: 1.6647096872329712
Epoch 2800, training loss: 620.5780029296875 = 0.025802047923207283 + 100.0 * 6.205521583557129
Epoch 2800, val loss: 1.6680752038955688
Epoch 2810, training loss: 620.4671020507812 = 0.02547338791191578 + 100.0 * 6.204416751861572
Epoch 2810, val loss: 1.6713440418243408
Epoch 2820, training loss: 620.3833618164062 = 0.025156980380415916 + 100.0 * 6.203582286834717
Epoch 2820, val loss: 1.674845814704895
Epoch 2830, training loss: 620.5955200195312 = 0.02485835924744606 + 100.0 * 6.20570707321167
Epoch 2830, val loss: 1.6778767108917236
Epoch 2840, training loss: 620.794677734375 = 0.024545036256313324 + 100.0 * 6.207701683044434
Epoch 2840, val loss: 1.6811662912368774
Epoch 2850, training loss: 620.4456176757812 = 0.024224594235420227 + 100.0 * 6.204214096069336
Epoch 2850, val loss: 1.6840013265609741
Epoch 2860, training loss: 620.3073120117188 = 0.02391999401152134 + 100.0 * 6.202833652496338
Epoch 2860, val loss: 1.6877080202102661
Epoch 2870, training loss: 620.23974609375 = 0.023642249405384064 + 100.0 * 6.2021613121032715
Epoch 2870, val loss: 1.691300630569458
Epoch 2880, training loss: 620.2167358398438 = 0.023369694128632545 + 100.0 * 6.20193338394165
Epoch 2880, val loss: 1.6946948766708374
Epoch 2890, training loss: 621.1658935546875 = 0.02312578074634075 + 100.0 * 6.211427688598633
Epoch 2890, val loss: 1.6973986625671387
Epoch 2900, training loss: 620.6803588867188 = 0.02281208708882332 + 100.0 * 6.206575393676758
Epoch 2900, val loss: 1.7003685235977173
Epoch 2910, training loss: 620.420166015625 = 0.02253705821931362 + 100.0 * 6.203976631164551
Epoch 2910, val loss: 1.7037415504455566
Epoch 2920, training loss: 620.2396240234375 = 0.022265369072556496 + 100.0 * 6.202173709869385
Epoch 2920, val loss: 1.7073190212249756
Epoch 2930, training loss: 620.2254638671875 = 0.022015182301402092 + 100.0 * 6.2020344734191895
Epoch 2930, val loss: 1.7104862928390503
Epoch 2940, training loss: 620.6519165039062 = 0.02176620624959469 + 100.0 * 6.206301212310791
Epoch 2940, val loss: 1.7136545181274414
Epoch 2950, training loss: 620.5186157226562 = 0.021505022421479225 + 100.0 * 6.2049713134765625
Epoch 2950, val loss: 1.7161288261413574
Epoch 2960, training loss: 620.4418334960938 = 0.021242665126919746 + 100.0 * 6.2042059898376465
Epoch 2960, val loss: 1.7193199396133423
Epoch 2970, training loss: 620.2474975585938 = 0.020990446209907532 + 100.0 * 6.20226526260376
Epoch 2970, val loss: 1.7219395637512207
Epoch 2980, training loss: 620.123046875 = 0.020753586664795876 + 100.0 * 6.201023101806641
Epoch 2980, val loss: 1.7258988618850708
Epoch 2990, training loss: 620.128173828125 = 0.0205248910933733 + 100.0 * 6.201076507568359
Epoch 2990, val loss: 1.7291662693023682
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6518518518518519
0.808645229309436
The final CL Acc:0.64568, 0.00630, The final GNN Acc:0.80952, 0.00124
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13186])
remove edge: torch.Size([2, 7896])
updated graph: torch.Size([2, 10526])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6289672851562 = 1.9454519748687744 + 100.0 * 8.596835136413574
Epoch 0, val loss: 1.94558846950531
Epoch 10, training loss: 861.5363159179688 = 1.9368363618850708 + 100.0 * 8.59599494934082
Epoch 10, val loss: 1.9367446899414062
Epoch 20, training loss: 860.8948974609375 = 1.925758957862854 + 100.0 * 8.589691162109375
Epoch 20, val loss: 1.9250720739364624
Epoch 30, training loss: 856.6746215820312 = 1.911120891571045 + 100.0 * 8.547635078430176
Epoch 30, val loss: 1.9093424081802368
Epoch 40, training loss: 833.4593505859375 = 1.893517017364502 + 100.0 * 8.315658569335938
Epoch 40, val loss: 1.8910629749298096
Epoch 50, training loss: 788.946044921875 = 1.8750450611114502 + 100.0 * 7.8707098960876465
Epoch 50, val loss: 1.872671127319336
Epoch 60, training loss: 740.1693115234375 = 1.862000823020935 + 100.0 * 7.383073329925537
Epoch 60, val loss: 1.8601089715957642
Epoch 70, training loss: 708.0399780273438 = 1.8518000841140747 + 100.0 * 7.0618815422058105
Epoch 70, val loss: 1.850170373916626
Epoch 80, training loss: 688.9752197265625 = 1.8410829305648804 + 100.0 * 6.871341705322266
Epoch 80, val loss: 1.8393014669418335
Epoch 90, training loss: 676.462646484375 = 1.830174446105957 + 100.0 * 6.74632453918457
Epoch 90, val loss: 1.82819402217865
Epoch 100, training loss: 668.4082641601562 = 1.8201804161071777 + 100.0 * 6.6658806800842285
Epoch 100, val loss: 1.8177645206451416
Epoch 110, training loss: 663.5391235351562 = 1.8103320598602295 + 100.0 * 6.617288112640381
Epoch 110, val loss: 1.8074957132339478
Epoch 120, training loss: 660.0083618164062 = 1.8005598783493042 + 100.0 * 6.582077980041504
Epoch 120, val loss: 1.797398567199707
Epoch 130, training loss: 657.1016845703125 = 1.7909351587295532 + 100.0 * 6.553107261657715
Epoch 130, val loss: 1.7875144481658936
Epoch 140, training loss: 654.7937622070312 = 1.781313180923462 + 100.0 * 6.530124664306641
Epoch 140, val loss: 1.7778209447860718
Epoch 150, training loss: 652.7052612304688 = 1.7714098691940308 + 100.0 * 6.50933837890625
Epoch 150, val loss: 1.7680325508117676
Epoch 160, training loss: 650.8190307617188 = 1.7610677480697632 + 100.0 * 6.490579605102539
Epoch 160, val loss: 1.7579377889633179
Epoch 170, training loss: 649.5071411132812 = 1.749950885772705 + 100.0 * 6.477571964263916
Epoch 170, val loss: 1.7472608089447021
Epoch 180, training loss: 647.93505859375 = 1.7379635572433472 + 100.0 * 6.461970806121826
Epoch 180, val loss: 1.735779881477356
Epoch 190, training loss: 646.6253051757812 = 1.7249444723129272 + 100.0 * 6.449004173278809
Epoch 190, val loss: 1.7235074043273926
Epoch 200, training loss: 645.57958984375 = 1.7107810974121094 + 100.0 * 6.438688278198242
Epoch 200, val loss: 1.710252046585083
Epoch 210, training loss: 644.4569091796875 = 1.6954107284545898 + 100.0 * 6.427614688873291
Epoch 210, val loss: 1.6959372758865356
Epoch 220, training loss: 643.457763671875 = 1.6787983179092407 + 100.0 * 6.417789936065674
Epoch 220, val loss: 1.6805583238601685
Epoch 230, training loss: 642.8963623046875 = 1.6608623266220093 + 100.0 * 6.412354469299316
Epoch 230, val loss: 1.6640855073928833
Epoch 240, training loss: 642.2322387695312 = 1.641512155532837 + 100.0 * 6.405907154083252
Epoch 240, val loss: 1.6464614868164062
Epoch 250, training loss: 641.11328125 = 1.6207067966461182 + 100.0 * 6.394925594329834
Epoch 250, val loss: 1.6277037858963013
Epoch 260, training loss: 640.3792724609375 = 1.5986554622650146 + 100.0 * 6.387806415557861
Epoch 260, val loss: 1.6079765558242798
Epoch 270, training loss: 639.6966552734375 = 1.5753931999206543 + 100.0 * 6.3812127113342285
Epoch 270, val loss: 1.5874016284942627
Epoch 280, training loss: 639.5317993164062 = 1.5509690046310425 + 100.0 * 6.37980842590332
Epoch 280, val loss: 1.5660172700881958
Epoch 290, training loss: 638.694091796875 = 1.5254770517349243 + 100.0 * 6.3716864585876465
Epoch 290, val loss: 1.5439943075180054
Epoch 300, training loss: 638.3884887695312 = 1.499161958694458 + 100.0 * 6.368893623352051
Epoch 300, val loss: 1.5215235948562622
Epoch 310, training loss: 637.580810546875 = 1.4720916748046875 + 100.0 * 6.361087322235107
Epoch 310, val loss: 1.498803734779358
Epoch 320, training loss: 637.0519409179688 = 1.4445433616638184 + 100.0 * 6.356074333190918
Epoch 320, val loss: 1.4760998487472534
Epoch 330, training loss: 637.1065063476562 = 1.4167890548706055 + 100.0 * 6.356896877288818
Epoch 330, val loss: 1.4534331560134888
Epoch 340, training loss: 636.2588500976562 = 1.388649821281433 + 100.0 * 6.3487019538879395
Epoch 340, val loss: 1.430890679359436
Epoch 350, training loss: 635.9197387695312 = 1.360556721687317 + 100.0 * 6.345592021942139
Epoch 350, val loss: 1.4086989164352417
Epoch 360, training loss: 635.4469604492188 = 1.332634687423706 + 100.0 * 6.3411431312561035
Epoch 360, val loss: 1.3869372606277466
Epoch 370, training loss: 635.0717163085938 = 1.3049918413162231 + 100.0 * 6.337666988372803
Epoch 370, val loss: 1.3657468557357788
Epoch 380, training loss: 636.0684814453125 = 1.2778007984161377 + 100.0 * 6.347907066345215
Epoch 380, val loss: 1.3450132608413696
Epoch 390, training loss: 634.8729858398438 = 1.2506765127182007 + 100.0 * 6.336223125457764
Epoch 390, val loss: 1.3249024152755737
Epoch 400, training loss: 634.2391967773438 = 1.2240819931030273 + 100.0 * 6.330151557922363
Epoch 400, val loss: 1.3055933713912964
Epoch 410, training loss: 633.8041381835938 = 1.1981542110443115 + 100.0 * 6.326059341430664
Epoch 410, val loss: 1.2869596481323242
Epoch 420, training loss: 633.791259765625 = 1.17286217212677 + 100.0 * 6.326183795928955
Epoch 420, val loss: 1.2690194845199585
Epoch 430, training loss: 633.3258056640625 = 1.1481823921203613 + 100.0 * 6.321776390075684
Epoch 430, val loss: 1.2519139051437378
Epoch 440, training loss: 633.321044921875 = 1.124192476272583 + 100.0 * 6.3219685554504395
Epoch 440, val loss: 1.2353638410568237
Epoch 450, training loss: 632.9080810546875 = 1.1008014678955078 + 100.0 * 6.31807279586792
Epoch 450, val loss: 1.2195916175842285
Epoch 460, training loss: 632.5659790039062 = 1.0781314373016357 + 100.0 * 6.314878463745117
Epoch 460, val loss: 1.204677700996399
Epoch 470, training loss: 632.1686401367188 = 1.0561977624893188 + 100.0 * 6.311124324798584
Epoch 470, val loss: 1.1905975341796875
Epoch 480, training loss: 631.9851684570312 = 1.0350595712661743 + 100.0 * 6.3095011711120605
Epoch 480, val loss: 1.1772167682647705
Epoch 490, training loss: 631.887451171875 = 1.014552354812622 + 100.0 * 6.30872917175293
Epoch 490, val loss: 1.1645764112472534
Epoch 500, training loss: 631.9585571289062 = 0.9946351051330566 + 100.0 * 6.3096394538879395
Epoch 500, val loss: 1.1521859169006348
Epoch 510, training loss: 631.3408813476562 = 0.9754327535629272 + 100.0 * 6.303654670715332
Epoch 510, val loss: 1.1408483982086182
Epoch 520, training loss: 630.9834594726562 = 0.9569652676582336 + 100.0 * 6.300264835357666
Epoch 520, val loss: 1.1302788257598877
Epoch 530, training loss: 630.7525634765625 = 0.9391382932662964 + 100.0 * 6.2981343269348145
Epoch 530, val loss: 1.120342493057251
Epoch 540, training loss: 631.6876220703125 = 0.9218546152114868 + 100.0 * 6.307657241821289
Epoch 540, val loss: 1.110999584197998
Epoch 550, training loss: 630.5543212890625 = 0.905065655708313 + 100.0 * 6.296492099761963
Epoch 550, val loss: 1.1020182371139526
Epoch 560, training loss: 630.2172241210938 = 0.8888407945632935 + 100.0 * 6.293283939361572
Epoch 560, val loss: 1.0935403108596802
Epoch 570, training loss: 630.4334716796875 = 0.8731639385223389 + 100.0 * 6.295602798461914
Epoch 570, val loss: 1.0855838060379028
Epoch 580, training loss: 630.118408203125 = 0.857891857624054 + 100.0 * 6.292604923248291
Epoch 580, val loss: 1.0780068635940552
Epoch 590, training loss: 629.7953491210938 = 0.8430677056312561 + 100.0 * 6.289522647857666
Epoch 590, val loss: 1.0707001686096191
Epoch 600, training loss: 629.773193359375 = 0.8286389708518982 + 100.0 * 6.289445877075195
Epoch 600, val loss: 1.0638940334320068
Epoch 610, training loss: 629.5020141601562 = 0.8146013021469116 + 100.0 * 6.286874294281006
Epoch 610, val loss: 1.057382583618164
Epoch 620, training loss: 629.215576171875 = 0.8008467555046082 + 100.0 * 6.284147262573242
Epoch 620, val loss: 1.0509270429611206
Epoch 630, training loss: 629.2346801757812 = 0.7873872518539429 + 100.0 * 6.284472942352295
Epoch 630, val loss: 1.0448272228240967
Epoch 640, training loss: 629.1163330078125 = 0.7741143703460693 + 100.0 * 6.283422470092773
Epoch 640, val loss: 1.0389394760131836
Epoch 650, training loss: 629.0101928710938 = 0.7610726356506348 + 100.0 * 6.282491683959961
Epoch 650, val loss: 1.033048391342163
Epoch 660, training loss: 628.6212158203125 = 0.7482282519340515 + 100.0 * 6.2787299156188965
Epoch 660, val loss: 1.02741539478302
Epoch 670, training loss: 628.4635009765625 = 0.7355788946151733 + 100.0 * 6.277278900146484
Epoch 670, val loss: 1.0218042135238647
Epoch 680, training loss: 628.5169677734375 = 0.7230967283248901 + 100.0 * 6.2779388427734375
Epoch 680, val loss: 1.0163700580596924
Epoch 690, training loss: 628.1646728515625 = 0.7105509042739868 + 100.0 * 6.274540901184082
Epoch 690, val loss: 1.0109202861785889
Epoch 700, training loss: 628.223876953125 = 0.6981295943260193 + 100.0 * 6.275257587432861
Epoch 700, val loss: 1.0055174827575684
Epoch 710, training loss: 628.2924194335938 = 0.6856794953346252 + 100.0 * 6.27606725692749
Epoch 710, val loss: 0.9999962449073792
Epoch 720, training loss: 627.865966796875 = 0.6733013987541199 + 100.0 * 6.2719268798828125
Epoch 720, val loss: 0.994529664516449
Epoch 730, training loss: 627.6834716796875 = 0.6609475016593933 + 100.0 * 6.2702250480651855
Epoch 730, val loss: 0.9891097545623779
Epoch 740, training loss: 627.5822143554688 = 0.6486186981201172 + 100.0 * 6.269336223602295
Epoch 740, val loss: 0.983708918094635
Epoch 750, training loss: 627.8492431640625 = 0.6361746788024902 + 100.0 * 6.272130966186523
Epoch 750, val loss: 0.9781827330589294
Epoch 760, training loss: 627.6449584960938 = 0.6235843300819397 + 100.0 * 6.270213603973389
Epoch 760, val loss: 0.9724193215370178
Epoch 770, training loss: 627.1827392578125 = 0.6110135316848755 + 100.0 * 6.265717029571533
Epoch 770, val loss: 0.9668710231781006
Epoch 780, training loss: 627.1229858398438 = 0.5984768271446228 + 100.0 * 6.26524543762207
Epoch 780, val loss: 0.9613621234893799
Epoch 790, training loss: 627.8068237304688 = 0.5859022736549377 + 100.0 * 6.272209167480469
Epoch 790, val loss: 0.955706775188446
Epoch 800, training loss: 627.0323486328125 = 0.5730693340301514 + 100.0 * 6.26459264755249
Epoch 800, val loss: 0.9500290751457214
Epoch 810, training loss: 626.94287109375 = 0.560326874256134 + 100.0 * 6.263825416564941
Epoch 810, val loss: 0.9444737434387207
Epoch 820, training loss: 626.7674560546875 = 0.5476307272911072 + 100.0 * 6.262198448181152
Epoch 820, val loss: 0.9389591813087463
Epoch 830, training loss: 626.8225708007812 = 0.5349442958831787 + 100.0 * 6.262876033782959
Epoch 830, val loss: 0.9333900809288025
Epoch 840, training loss: 626.53369140625 = 0.5222234725952148 + 100.0 * 6.260114669799805
Epoch 840, val loss: 0.9278329610824585
Epoch 850, training loss: 626.5307006835938 = 0.5096115469932556 + 100.0 * 6.260210990905762
Epoch 850, val loss: 0.9224771857261658
Epoch 860, training loss: 626.8924560546875 = 0.4970133304595947 + 100.0 * 6.2639546394348145
Epoch 860, val loss: 0.916985809803009
Epoch 870, training loss: 626.3938598632812 = 0.4845632314682007 + 100.0 * 6.259093284606934
Epoch 870, val loss: 0.9117517471313477
Epoch 880, training loss: 626.142822265625 = 0.4722073972225189 + 100.0 * 6.2567057609558105
Epoch 880, val loss: 0.906592845916748
Epoch 890, training loss: 626.4877319335938 = 0.46008962392807007 + 100.0 * 6.2602763175964355
Epoch 890, val loss: 0.9017593860626221
Epoch 900, training loss: 626.3369750976562 = 0.4481744170188904 + 100.0 * 6.258888244628906
Epoch 900, val loss: 0.8969086408615112
Epoch 910, training loss: 625.9266357421875 = 0.4363167881965637 + 100.0 * 6.2549028396606445
Epoch 910, val loss: 0.8924452066421509
Epoch 920, training loss: 625.8021850585938 = 0.42488226294517517 + 100.0 * 6.253773212432861
Epoch 920, val loss: 0.8880713582038879
Epoch 930, training loss: 625.853759765625 = 0.41366979479789734 + 100.0 * 6.254400730133057
Epoch 930, val loss: 0.8841726779937744
Epoch 940, training loss: 625.844970703125 = 0.4027217626571655 + 100.0 * 6.254422664642334
Epoch 940, val loss: 0.8801853060722351
Epoch 950, training loss: 625.7755737304688 = 0.3920133709907532 + 100.0 * 6.253835678100586
Epoch 950, val loss: 0.8765866756439209
Epoch 960, training loss: 625.4520874023438 = 0.3816070556640625 + 100.0 * 6.250705242156982
Epoch 960, val loss: 0.8731446862220764
Epoch 970, training loss: 625.4810180664062 = 0.3715417981147766 + 100.0 * 6.251094818115234
Epoch 970, val loss: 0.8702176809310913
Epoch 980, training loss: 625.4859619140625 = 0.36179232597351074 + 100.0 * 6.251241683959961
Epoch 980, val loss: 0.8674954175949097
Epoch 990, training loss: 625.1903076171875 = 0.3523252308368683 + 100.0 * 6.248380184173584
Epoch 990, val loss: 0.8651405572891235
Epoch 1000, training loss: 625.1295776367188 = 0.34317800402641296 + 100.0 * 6.24786376953125
Epoch 1000, val loss: 0.8630281686782837
Epoch 1010, training loss: 625.7809448242188 = 0.33438530564308167 + 100.0 * 6.254465103149414
Epoch 1010, val loss: 0.8611745834350586
Epoch 1020, training loss: 625.2466430664062 = 0.32568359375 + 100.0 * 6.249209403991699
Epoch 1020, val loss: 0.8595263361930847
Epoch 1030, training loss: 625.4614868164062 = 0.31739941239356995 + 100.0 * 6.25144100189209
Epoch 1030, val loss: 0.8580268025398254
Epoch 1040, training loss: 625.08544921875 = 0.3092755377292633 + 100.0 * 6.2477617263793945
Epoch 1040, val loss: 0.8572066426277161
Epoch 1050, training loss: 624.8197021484375 = 0.3015240430831909 + 100.0 * 6.245182037353516
Epoch 1050, val loss: 0.85617595911026
Epoch 1060, training loss: 624.7749633789062 = 0.2940399944782257 + 100.0 * 6.244809150695801
Epoch 1060, val loss: 0.855776846408844
Epoch 1070, training loss: 624.8065795898438 = 0.286809504032135 + 100.0 * 6.245197772979736
Epoch 1070, val loss: 0.8554747104644775
Epoch 1080, training loss: 624.9813842773438 = 0.2797955274581909 + 100.0 * 6.247015953063965
Epoch 1080, val loss: 0.8555121421813965
Epoch 1090, training loss: 624.6134033203125 = 0.2729855477809906 + 100.0 * 6.243403911590576
Epoch 1090, val loss: 0.8555071353912354
Epoch 1100, training loss: 624.4943237304688 = 0.2664271891117096 + 100.0 * 6.242279052734375
Epoch 1100, val loss: 0.8558653593063354
Epoch 1110, training loss: 624.489501953125 = 0.26008519530296326 + 100.0 * 6.2422943115234375
Epoch 1110, val loss: 0.8565536141395569
Epoch 1120, training loss: 624.9407958984375 = 0.2539336085319519 + 100.0 * 6.246869087219238
Epoch 1120, val loss: 0.8572654724121094
Epoch 1130, training loss: 624.4990844726562 = 0.24796463549137115 + 100.0 * 6.24251127243042
Epoch 1130, val loss: 0.8581002354621887
Epoch 1140, training loss: 624.3867797851562 = 0.24216333031654358 + 100.0 * 6.241446018218994
Epoch 1140, val loss: 0.8593633770942688
Epoch 1150, training loss: 624.674560546875 = 0.23656028509140015 + 100.0 * 6.244379997253418
Epoch 1150, val loss: 0.8605093955993652
Epoch 1160, training loss: 624.3067626953125 = 0.23112128674983978 + 100.0 * 6.240756511688232
Epoch 1160, val loss: 0.8621354103088379
Epoch 1170, training loss: 624.1739501953125 = 0.2258654236793518 + 100.0 * 6.239480495452881
Epoch 1170, val loss: 0.8637529611587524
Epoch 1180, training loss: 624.3619995117188 = 0.22079020738601685 + 100.0 * 6.24141263961792
Epoch 1180, val loss: 0.8655917644500732
Epoch 1190, training loss: 624.0107421875 = 0.21584996581077576 + 100.0 * 6.237949371337891
Epoch 1190, val loss: 0.8676432967185974
Epoch 1200, training loss: 624.2108154296875 = 0.21107791364192963 + 100.0 * 6.239997863769531
Epoch 1200, val loss: 0.8696260452270508
Epoch 1210, training loss: 624.1002197265625 = 0.20634984970092773 + 100.0 * 6.238938808441162
Epoch 1210, val loss: 0.8718042373657227
Epoch 1220, training loss: 623.9535522460938 = 0.2017831802368164 + 100.0 * 6.237517833709717
Epoch 1220, val loss: 0.8742031455039978
Epoch 1230, training loss: 623.8922119140625 = 0.1973818838596344 + 100.0 * 6.236948490142822
Epoch 1230, val loss: 0.8767671585083008
Epoch 1240, training loss: 624.2026977539062 = 0.19313734769821167 + 100.0 * 6.240095615386963
Epoch 1240, val loss: 0.879357099533081
Epoch 1250, training loss: 623.68701171875 = 0.18895436823368073 + 100.0 * 6.234980583190918
Epoch 1250, val loss: 0.8820830583572388
Epoch 1260, training loss: 623.715087890625 = 0.1849186271429062 + 100.0 * 6.235301494598389
Epoch 1260, val loss: 0.8849263787269592
Epoch 1270, training loss: 624.2130737304688 = 0.18100430071353912 + 100.0 * 6.240320682525635
Epoch 1270, val loss: 0.8876885771751404
Epoch 1280, training loss: 624.0390014648438 = 0.17711247503757477 + 100.0 * 6.238618850708008
Epoch 1280, val loss: 0.8905225396156311
Epoch 1290, training loss: 623.7020263671875 = 0.17333866655826569 + 100.0 * 6.235286712646484
Epoch 1290, val loss: 0.8936126828193665
Epoch 1300, training loss: 623.539794921875 = 0.16969017684459686 + 100.0 * 6.233701229095459
Epoch 1300, val loss: 0.8967316746711731
Epoch 1310, training loss: 623.5738525390625 = 0.1661846786737442 + 100.0 * 6.234076499938965
Epoch 1310, val loss: 0.9000739455223083
Epoch 1320, training loss: 623.6939697265625 = 0.16273850202560425 + 100.0 * 6.235312461853027
Epoch 1320, val loss: 0.9033620953559875
Epoch 1330, training loss: 623.573486328125 = 0.15932124853134155 + 100.0 * 6.234141826629639
Epoch 1330, val loss: 0.9067475199699402
Epoch 1340, training loss: 623.5914916992188 = 0.15606237947940826 + 100.0 * 6.234354496002197
Epoch 1340, val loss: 0.9102533459663391
Epoch 1350, training loss: 623.4248046875 = 0.15283693373203278 + 100.0 * 6.232719421386719
Epoch 1350, val loss: 0.9139060378074646
Epoch 1360, training loss: 623.33837890625 = 0.1497267335653305 + 100.0 * 6.231886386871338
Epoch 1360, val loss: 0.9176065325737
Epoch 1370, training loss: 623.7011108398438 = 0.14665761590003967 + 100.0 * 6.235544681549072
Epoch 1370, val loss: 0.9210028648376465
Epoch 1380, training loss: 623.3375244140625 = 0.14365877211093903 + 100.0 * 6.231938362121582
Epoch 1380, val loss: 0.9250101447105408
Epoch 1390, training loss: 623.2062377929688 = 0.1407386064529419 + 100.0 * 6.230654716491699
Epoch 1390, val loss: 0.928860068321228
Epoch 1400, training loss: 623.16748046875 = 0.1379426121711731 + 100.0 * 6.230295658111572
Epoch 1400, val loss: 0.9330182075500488
Epoch 1410, training loss: 623.7357788085938 = 0.13517563045024872 + 100.0 * 6.236005783081055
Epoch 1410, val loss: 0.9368561506271362
Epoch 1420, training loss: 623.4796142578125 = 0.13246770203113556 + 100.0 * 6.233471870422363
Epoch 1420, val loss: 0.9404581785202026
Epoch 1430, training loss: 623.1166381835938 = 0.1297919601202011 + 100.0 * 6.229868412017822
Epoch 1430, val loss: 0.9448300004005432
Epoch 1440, training loss: 623.1194458007812 = 0.12723825871944427 + 100.0 * 6.229921817779541
Epoch 1440, val loss: 0.9487923979759216
Epoch 1450, training loss: 623.3991088867188 = 0.12471304088830948 + 100.0 * 6.232744216918945
Epoch 1450, val loss: 0.9529582858085632
Epoch 1460, training loss: 623.0851440429688 = 0.12223746627569199 + 100.0 * 6.229629039764404
Epoch 1460, val loss: 0.957303524017334
Epoch 1470, training loss: 622.8873901367188 = 0.11983627080917358 + 100.0 * 6.227675914764404
Epoch 1470, val loss: 0.9614121317863464
Epoch 1480, training loss: 622.87646484375 = 0.11751113831996918 + 100.0 * 6.2275896072387695
Epoch 1480, val loss: 0.9659426212310791
Epoch 1490, training loss: 623.35595703125 = 0.11522414535284042 + 100.0 * 6.232407093048096
Epoch 1490, val loss: 0.9702817797660828
Epoch 1500, training loss: 623.4566040039062 = 0.11296987533569336 + 100.0 * 6.233436584472656
Epoch 1500, val loss: 0.9746593236923218
Epoch 1510, training loss: 622.911376953125 = 0.11072095483541489 + 100.0 * 6.228006839752197
Epoch 1510, val loss: 0.9785292148590088
Epoch 1520, training loss: 622.7666625976562 = 0.10856041312217712 + 100.0 * 6.22658109664917
Epoch 1520, val loss: 0.9833697080612183
Epoch 1530, training loss: 622.64111328125 = 0.106465645134449 + 100.0 * 6.225346565246582
Epoch 1530, val loss: 0.9877054691314697
Epoch 1540, training loss: 622.610107421875 = 0.10443957895040512 + 100.0 * 6.2250566482543945
Epoch 1540, val loss: 0.9923803210258484
Epoch 1550, training loss: 623.022705078125 = 0.10245247185230255 + 100.0 * 6.2292022705078125
Epoch 1550, val loss: 0.9966577887535095
Epoch 1560, training loss: 622.9510498046875 = 0.10048337280750275 + 100.0 * 6.228506088256836
Epoch 1560, val loss: 1.00137197971344
Epoch 1570, training loss: 622.7136840820312 = 0.09850547462701797 + 100.0 * 6.226151943206787
Epoch 1570, val loss: 1.0058887004852295
Epoch 1580, training loss: 622.5731201171875 = 0.09661297500133514 + 100.0 * 6.224764823913574
Epoch 1580, val loss: 1.010378360748291
Epoch 1590, training loss: 622.4518432617188 = 0.09478700160980225 + 100.0 * 6.223570823669434
Epoch 1590, val loss: 1.0150424242019653
Epoch 1600, training loss: 622.5075073242188 = 0.09300357848405838 + 100.0 * 6.22414493560791
Epoch 1600, val loss: 1.0197516679763794
Epoch 1610, training loss: 623.2817993164062 = 0.09122632443904877 + 100.0 * 6.231905460357666
Epoch 1610, val loss: 1.024221658706665
Epoch 1620, training loss: 622.3884887695312 = 0.08944115787744522 + 100.0 * 6.222990989685059
Epoch 1620, val loss: 1.0286645889282227
Epoch 1630, training loss: 622.4141845703125 = 0.08772581070661545 + 100.0 * 6.223264694213867
Epoch 1630, val loss: 1.033436894416809
Epoch 1640, training loss: 622.2916259765625 = 0.08608852326869965 + 100.0 * 6.222055912017822
Epoch 1640, val loss: 1.0381324291229248
Epoch 1650, training loss: 622.255859375 = 0.08449634909629822 + 100.0 * 6.221714019775391
Epoch 1650, val loss: 1.0427637100219727
Epoch 1660, training loss: 622.6597290039062 = 0.08294351398944855 + 100.0 * 6.225767612457275
Epoch 1660, val loss: 1.0473569631576538
Epoch 1670, training loss: 622.477294921875 = 0.08135499060153961 + 100.0 * 6.223959445953369
Epoch 1670, val loss: 1.0525190830230713
Epoch 1680, training loss: 622.2156372070312 = 0.07981409132480621 + 100.0 * 6.221357822418213
Epoch 1680, val loss: 1.0568432807922363
Epoch 1690, training loss: 622.281494140625 = 0.07831702381372452 + 100.0 * 6.222031593322754
Epoch 1690, val loss: 1.0618188381195068
Epoch 1700, training loss: 622.2630615234375 = 0.07687526941299438 + 100.0 * 6.221861839294434
Epoch 1700, val loss: 1.0664029121398926
Epoch 1710, training loss: 622.8556518554688 = 0.07545696198940277 + 100.0 * 6.227802276611328
Epoch 1710, val loss: 1.0713757276535034
Epoch 1720, training loss: 622.2882690429688 = 0.07400409132242203 + 100.0 * 6.222142696380615
Epoch 1720, val loss: 1.0757484436035156
Epoch 1730, training loss: 622.1214599609375 = 0.07263824343681335 + 100.0 * 6.22048807144165
Epoch 1730, val loss: 1.080753207206726
Epoch 1740, training loss: 622.1064453125 = 0.07131624966859818 + 100.0 * 6.220351696014404
Epoch 1740, val loss: 1.0856510400772095
Epoch 1750, training loss: 622.6146850585938 = 0.07002025842666626 + 100.0 * 6.225446701049805
Epoch 1750, val loss: 1.090676188468933
Epoch 1760, training loss: 622.1351318359375 = 0.06871994584798813 + 100.0 * 6.220664024353027
Epoch 1760, val loss: 1.0950734615325928
Epoch 1770, training loss: 622.0194702148438 = 0.06746514141559601 + 100.0 * 6.219520092010498
Epoch 1770, val loss: 1.1001505851745605
Epoch 1780, training loss: 622.82958984375 = 0.0662459060549736 + 100.0 * 6.227633953094482
Epoch 1780, val loss: 1.1049964427947998
Epoch 1790, training loss: 622.151123046875 = 0.06501460820436478 + 100.0 * 6.220860958099365
Epoch 1790, val loss: 1.1096367835998535
Epoch 1800, training loss: 621.920166015625 = 0.06383173167705536 + 100.0 * 6.218563079833984
Epoch 1800, val loss: 1.1146085262298584
Epoch 1810, training loss: 621.8158569335938 = 0.06269101053476334 + 100.0 * 6.217531681060791
Epoch 1810, val loss: 1.1193972826004028
Epoch 1820, training loss: 621.9651489257812 = 0.06158066540956497 + 100.0 * 6.219035625457764
Epoch 1820, val loss: 1.1240391731262207
Epoch 1830, training loss: 621.991943359375 = 0.0604589618742466 + 100.0 * 6.2193145751953125
Epoch 1830, val loss: 1.1290647983551025
Epoch 1840, training loss: 621.9248657226562 = 0.05935119092464447 + 100.0 * 6.218655586242676
Epoch 1840, val loss: 1.133864402770996
Epoch 1850, training loss: 622.3875122070312 = 0.05827553570270538 + 100.0 * 6.223292350769043
Epoch 1850, val loss: 1.1389367580413818
Epoch 1860, training loss: 621.9351196289062 = 0.05722746253013611 + 100.0 * 6.218778610229492
Epoch 1860, val loss: 1.1435469388961792
Epoch 1870, training loss: 621.6979370117188 = 0.056195300072431564 + 100.0 * 6.21641731262207
Epoch 1870, val loss: 1.1482713222503662
Epoch 1880, training loss: 621.6737670898438 = 0.05521086975932121 + 100.0 * 6.216185092926025
Epoch 1880, val loss: 1.1533057689666748
Epoch 1890, training loss: 622.0927734375 = 0.054252203553915024 + 100.0 * 6.2203850746154785
Epoch 1890, val loss: 1.1582521200180054
Epoch 1900, training loss: 621.8778076171875 = 0.05328395962715149 + 100.0 * 6.218245506286621
Epoch 1900, val loss: 1.1629036664962769
Epoch 1910, training loss: 621.6641845703125 = 0.052320655435323715 + 100.0 * 6.216118812561035
Epoch 1910, val loss: 1.1675077676773071
Epoch 1920, training loss: 621.673095703125 = 0.051394958049058914 + 100.0 * 6.216217041015625
Epoch 1920, val loss: 1.1722102165222168
Epoch 1930, training loss: 621.7572631835938 = 0.050501082092523575 + 100.0 * 6.217067718505859
Epoch 1930, val loss: 1.1770659685134888
Epoch 1940, training loss: 621.7860107421875 = 0.049623310565948486 + 100.0 * 6.2173638343811035
Epoch 1940, val loss: 1.1818971633911133
Epoch 1950, training loss: 621.7645874023438 = 0.04875069111585617 + 100.0 * 6.217158317565918
Epoch 1950, val loss: 1.186683177947998
Epoch 1960, training loss: 621.489013671875 = 0.04789329320192337 + 100.0 * 6.214410781860352
Epoch 1960, val loss: 1.191184401512146
Epoch 1970, training loss: 621.6354370117188 = 0.04707339406013489 + 100.0 * 6.215883731842041
Epoch 1970, val loss: 1.196055293083191
Epoch 1980, training loss: 621.7521362304688 = 0.046267878264188766 + 100.0 * 6.2170586585998535
Epoch 1980, val loss: 1.20100736618042
Epoch 1990, training loss: 621.74072265625 = 0.045458558946847916 + 100.0 * 6.216952323913574
Epoch 1990, val loss: 1.206129789352417
Epoch 2000, training loss: 621.4960327148438 = 0.04466577246785164 + 100.0 * 6.214513778686523
Epoch 2000, val loss: 1.2101296186447144
Epoch 2010, training loss: 621.4921875 = 0.043911900371313095 + 100.0 * 6.21448278427124
Epoch 2010, val loss: 1.2149566411972046
Epoch 2020, training loss: 621.765380859375 = 0.043171823024749756 + 100.0 * 6.217222213745117
Epoch 2020, val loss: 1.219765305519104
Epoch 2030, training loss: 621.4259643554688 = 0.042431071400642395 + 100.0 * 6.2138352394104
Epoch 2030, val loss: 1.224320411682129
Epoch 2040, training loss: 621.8192749023438 = 0.041730768978595734 + 100.0 * 6.217775344848633
Epoch 2040, val loss: 1.2292683124542236
Epoch 2050, training loss: 621.353759765625 = 0.04101042076945305 + 100.0 * 6.213127136230469
Epoch 2050, val loss: 1.2332606315612793
Epoch 2060, training loss: 621.2559814453125 = 0.04031601920723915 + 100.0 * 6.212156295776367
Epoch 2060, val loss: 1.2382168769836426
Epoch 2070, training loss: 621.2344360351562 = 0.03965294361114502 + 100.0 * 6.211947917938232
Epoch 2070, val loss: 1.2429842948913574
Epoch 2080, training loss: 621.3394775390625 = 0.039009932428598404 + 100.0 * 6.2130045890808105
Epoch 2080, val loss: 1.2476634979248047
Epoch 2090, training loss: 621.7914428710938 = 0.038369789719581604 + 100.0 * 6.217530250549316
Epoch 2090, val loss: 1.2522512674331665
Epoch 2100, training loss: 621.4478759765625 = 0.037737563252449036 + 100.0 * 6.214101314544678
Epoch 2100, val loss: 1.2562676668167114
Epoch 2110, training loss: 621.6537475585938 = 0.03711614012718201 + 100.0 * 6.2161664962768555
Epoch 2110, val loss: 1.2606894969940186
Epoch 2120, training loss: 621.57373046875 = 0.036502446979284286 + 100.0 * 6.215372085571289
Epoch 2120, val loss: 1.2658418416976929
Epoch 2130, training loss: 621.248779296875 = 0.035903774201869965 + 100.0 * 6.212128639221191
Epoch 2130, val loss: 1.2701011896133423
Epoch 2140, training loss: 621.1175537109375 = 0.03533075004816055 + 100.0 * 6.210822105407715
Epoch 2140, val loss: 1.2746989727020264
Epoch 2150, training loss: 621.1371459960938 = 0.034773893654346466 + 100.0 * 6.211023330688477
Epoch 2150, val loss: 1.2793447971343994
Epoch 2160, training loss: 621.6232299804688 = 0.03423574939370155 + 100.0 * 6.215889930725098
Epoch 2160, val loss: 1.2838348150253296
Epoch 2170, training loss: 621.0682373046875 = 0.0336836576461792 + 100.0 * 6.21034574508667
Epoch 2170, val loss: 1.287915587425232
Epoch 2180, training loss: 621.2753295898438 = 0.03316078335046768 + 100.0 * 6.212421894073486
Epoch 2180, val loss: 1.2920913696289062
Epoch 2190, training loss: 621.3630981445312 = 0.03263748437166214 + 100.0 * 6.21330451965332
Epoch 2190, val loss: 1.2967751026153564
Epoch 2200, training loss: 621.0941162109375 = 0.03211551532149315 + 100.0 * 6.210619926452637
Epoch 2200, val loss: 1.3009620904922485
Epoch 2210, training loss: 621.0252075195312 = 0.031623587012290955 + 100.0 * 6.209935665130615
Epoch 2210, val loss: 1.3055624961853027
Epoch 2220, training loss: 621.1841430664062 = 0.031149262562394142 + 100.0 * 6.2115302085876465
Epoch 2220, val loss: 1.3101218938827515
Epoch 2230, training loss: 621.30810546875 = 0.030678272247314453 + 100.0 * 6.21277379989624
Epoch 2230, val loss: 1.3141337633132935
Epoch 2240, training loss: 621.339599609375 = 0.030197422951459885 + 100.0 * 6.2130937576293945
Epoch 2240, val loss: 1.3180999755859375
Epoch 2250, training loss: 621.0004272460938 = 0.029735131189227104 + 100.0 * 6.209706783294678
Epoch 2250, val loss: 1.3228098154067993
Epoch 2260, training loss: 620.9398803710938 = 0.029291361570358276 + 100.0 * 6.209105968475342
Epoch 2260, val loss: 1.3270448446273804
Epoch 2270, training loss: 621.4791870117188 = 0.028864635154604912 + 100.0 * 6.214503288269043
Epoch 2270, val loss: 1.3312597274780273
Epoch 2280, training loss: 621.1261596679688 = 0.02842877432703972 + 100.0 * 6.210977077484131
Epoch 2280, val loss: 1.3352535963058472
Epoch 2290, training loss: 621.032470703125 = 0.027995849028229713 + 100.0 * 6.2100443840026855
Epoch 2290, val loss: 1.3392059803009033
Epoch 2300, training loss: 620.9483032226562 = 0.027583371847867966 + 100.0 * 6.209207057952881
Epoch 2300, val loss: 1.343719244003296
Epoch 2310, training loss: 620.9263305664062 = 0.02718430757522583 + 100.0 * 6.208991527557373
Epoch 2310, val loss: 1.3480068445205688
Epoch 2320, training loss: 621.2952880859375 = 0.026796165853738785 + 100.0 * 6.2126851081848145
Epoch 2320, val loss: 1.3521348237991333
Epoch 2330, training loss: 620.9960327148438 = 0.026395827531814575 + 100.0 * 6.2096967697143555
Epoch 2330, val loss: 1.355839729309082
Epoch 2340, training loss: 620.76806640625 = 0.026014255359768867 + 100.0 * 6.207420825958252
Epoch 2340, val loss: 1.359972596168518
Epoch 2350, training loss: 620.7944946289062 = 0.02564961276948452 + 100.0 * 6.207688808441162
Epoch 2350, val loss: 1.364201307296753
Epoch 2360, training loss: 621.1812133789062 = 0.025292912498116493 + 100.0 * 6.211559295654297
Epoch 2360, val loss: 1.3680144548416138
Epoch 2370, training loss: 620.826904296875 = 0.024928346276283264 + 100.0 * 6.208019733428955
Epoch 2370, val loss: 1.3724315166473389
Epoch 2380, training loss: 621.0261840820312 = 0.02457890287041664 + 100.0 * 6.210015773773193
Epoch 2380, val loss: 1.375969409942627
Epoch 2390, training loss: 620.9877319335938 = 0.024227652698755264 + 100.0 * 6.209634780883789
Epoch 2390, val loss: 1.3804361820220947
Epoch 2400, training loss: 620.8078002929688 = 0.023875653743743896 + 100.0 * 6.207839488983154
Epoch 2400, val loss: 1.3840628862380981
Epoch 2410, training loss: 620.706298828125 = 0.023551473394036293 + 100.0 * 6.206827640533447
Epoch 2410, val loss: 1.3882508277893066
Epoch 2420, training loss: 620.6627807617188 = 0.023236265406012535 + 100.0 * 6.206395149230957
Epoch 2420, val loss: 1.3921529054641724
Epoch 2430, training loss: 621.2977294921875 = 0.022935200482606888 + 100.0 * 6.212747573852539
Epoch 2430, val loss: 1.3963844776153564
Epoch 2440, training loss: 620.7353515625 = 0.022608695551753044 + 100.0 * 6.207127571105957
Epoch 2440, val loss: 1.3995858430862427
Epoch 2450, training loss: 620.7933959960938 = 0.022302765399217606 + 100.0 * 6.207711219787598
Epoch 2450, val loss: 1.403855800628662
Epoch 2460, training loss: 620.6660766601562 = 0.022001201286911964 + 100.0 * 6.2064409255981445
Epoch 2460, val loss: 1.4078036546707153
Epoch 2470, training loss: 621.2625732421875 = 0.021714065223932266 + 100.0 * 6.212408542633057
Epoch 2470, val loss: 1.4119751453399658
Epoch 2480, training loss: 620.7693481445312 = 0.021414095535874367 + 100.0 * 6.207479000091553
Epoch 2480, val loss: 1.4149357080459595
Epoch 2490, training loss: 620.6156616210938 = 0.0211343914270401 + 100.0 * 6.2059454917907715
Epoch 2490, val loss: 1.4192979335784912
Epoch 2500, training loss: 620.7677001953125 = 0.020862162113189697 + 100.0 * 6.207468032836914
Epoch 2500, val loss: 1.423120141029358
Epoch 2510, training loss: 620.719482421875 = 0.02059018611907959 + 100.0 * 6.206989288330078
Epoch 2510, val loss: 1.4265460968017578
Epoch 2520, training loss: 620.7582397460938 = 0.02033030427992344 + 100.0 * 6.20737886428833
Epoch 2520, val loss: 1.4300856590270996
Epoch 2530, training loss: 620.8563232421875 = 0.020066475495696068 + 100.0 * 6.208362579345703
Epoch 2530, val loss: 1.4336442947387695
Epoch 2540, training loss: 620.7096557617188 = 0.019806059077382088 + 100.0 * 6.206898212432861
Epoch 2540, val loss: 1.4371874332427979
Epoch 2550, training loss: 620.5843505859375 = 0.01955386996269226 + 100.0 * 6.205648422241211
Epoch 2550, val loss: 1.4410879611968994
Epoch 2560, training loss: 620.6273193359375 = 0.01930800825357437 + 100.0 * 6.206080436706543
Epoch 2560, val loss: 1.444920539855957
Epoch 2570, training loss: 620.7888793945312 = 0.019068164750933647 + 100.0 * 6.207698345184326
Epoch 2570, val loss: 1.4485431909561157
Epoch 2580, training loss: 620.5827026367188 = 0.018832212314009666 + 100.0 * 6.205638408660889
Epoch 2580, val loss: 1.4518344402313232
Epoch 2590, training loss: 620.5012817382812 = 0.018597692251205444 + 100.0 * 6.204826354980469
Epoch 2590, val loss: 1.4557745456695557
Epoch 2600, training loss: 620.8715209960938 = 0.01837945729494095 + 100.0 * 6.208531379699707
Epoch 2600, val loss: 1.459122657775879
Epoch 2610, training loss: 620.7387084960938 = 0.018145807087421417 + 100.0 * 6.207205772399902
Epoch 2610, val loss: 1.462738275527954
Epoch 2620, training loss: 620.4058837890625 = 0.01791834831237793 + 100.0 * 6.203879356384277
Epoch 2620, val loss: 1.4659984111785889
Epoch 2630, training loss: 620.8602294921875 = 0.017704997211694717 + 100.0 * 6.208425521850586
Epoch 2630, val loss: 1.4690768718719482
Epoch 2640, training loss: 620.4229125976562 = 0.017485328018665314 + 100.0 * 6.204054355621338
Epoch 2640, val loss: 1.473220705986023
Epoch 2650, training loss: 620.3929443359375 = 0.01727266237139702 + 100.0 * 6.203756809234619
Epoch 2650, val loss: 1.476882815361023
Epoch 2660, training loss: 620.451904296875 = 0.01707424223423004 + 100.0 * 6.204348087310791
Epoch 2660, val loss: 1.4798595905303955
Epoch 2670, training loss: 621.2008666992188 = 0.016875559464097023 + 100.0 * 6.2118401527404785
Epoch 2670, val loss: 1.4834080934524536
Epoch 2680, training loss: 620.5870971679688 = 0.016668440774083138 + 100.0 * 6.205704212188721
Epoch 2680, val loss: 1.4868236780166626
Epoch 2690, training loss: 620.3724975585938 = 0.01647059991955757 + 100.0 * 6.2035603523254395
Epoch 2690, val loss: 1.490270733833313
Epoch 2700, training loss: 620.2567749023438 = 0.016283461824059486 + 100.0 * 6.202404975891113
Epoch 2700, val loss: 1.4939706325531006
Epoch 2710, training loss: 620.3641357421875 = 0.016103094443678856 + 100.0 * 6.203480243682861
Epoch 2710, val loss: 1.497201681137085
Epoch 2720, training loss: 620.8684692382812 = 0.015919778496026993 + 100.0 * 6.208525657653809
Epoch 2720, val loss: 1.5001366138458252
Epoch 2730, training loss: 620.669677734375 = 0.015734395012259483 + 100.0 * 6.206539154052734
Epoch 2730, val loss: 1.5035066604614258
Epoch 2740, training loss: 620.689697265625 = 0.015552053228020668 + 100.0 * 6.2067413330078125
Epoch 2740, val loss: 1.5069187879562378
Epoch 2750, training loss: 620.443115234375 = 0.015371761284768581 + 100.0 * 6.204277515411377
Epoch 2750, val loss: 1.5106064081192017
Epoch 2760, training loss: 620.2242431640625 = 0.015196699649095535 + 100.0 * 6.202090740203857
Epoch 2760, val loss: 1.5134522914886475
Epoch 2770, training loss: 620.3258666992188 = 0.015034166164696217 + 100.0 * 6.203108310699463
Epoch 2770, val loss: 1.5167951583862305
Epoch 2780, training loss: 620.6268920898438 = 0.014873676933348179 + 100.0 * 6.206120014190674
Epoch 2780, val loss: 1.5202473402023315
Epoch 2790, training loss: 620.4033813476562 = 0.014703539200127125 + 100.0 * 6.20388650894165
Epoch 2790, val loss: 1.523114800453186
Epoch 2800, training loss: 620.1361083984375 = 0.014538918621838093 + 100.0 * 6.201215744018555
Epoch 2800, val loss: 1.5261194705963135
Epoch 2810, training loss: 620.42626953125 = 0.014388936571776867 + 100.0 * 6.204118728637695
Epoch 2810, val loss: 1.5291433334350586
Epoch 2820, training loss: 620.7000122070312 = 0.014234255068004131 + 100.0 * 6.206858158111572
Epoch 2820, val loss: 1.532723069190979
Epoch 2830, training loss: 620.3072509765625 = 0.014069721102714539 + 100.0 * 6.2029314041137695
Epoch 2830, val loss: 1.5360620021820068
Epoch 2840, training loss: 620.1340942382812 = 0.01392064243555069 + 100.0 * 6.201201915740967
Epoch 2840, val loss: 1.5387905836105347
Epoch 2850, training loss: 620.1207885742188 = 0.013777307234704494 + 100.0 * 6.2010698318481445
Epoch 2850, val loss: 1.5423036813735962
Epoch 2860, training loss: 620.7803344726562 = 0.013639181852340698 + 100.0 * 6.207667350769043
Epoch 2860, val loss: 1.5456990003585815
Epoch 2870, training loss: 620.1900634765625 = 0.013490916229784489 + 100.0 * 6.201766014099121
Epoch 2870, val loss: 1.5478161573410034
Epoch 2880, training loss: 620.29052734375 = 0.01334861945360899 + 100.0 * 6.2027716636657715
Epoch 2880, val loss: 1.551164984703064
Epoch 2890, training loss: 620.357421875 = 0.013208677060902119 + 100.0 * 6.203442096710205
Epoch 2890, val loss: 1.5535223484039307
Epoch 2900, training loss: 620.392333984375 = 0.013068490661680698 + 100.0 * 6.203792572021484
Epoch 2900, val loss: 1.5574489831924438
Epoch 2910, training loss: 620.07080078125 = 0.01293469313532114 + 100.0 * 6.200578689575195
Epoch 2910, val loss: 1.5600559711456299
Epoch 2920, training loss: 620.1358032226562 = 0.012808685190975666 + 100.0 * 6.201229572296143
Epoch 2920, val loss: 1.563470721244812
Epoch 2930, training loss: 620.6090698242188 = 0.012681507505476475 + 100.0 * 6.205963611602783
Epoch 2930, val loss: 1.566148281097412
Epoch 2940, training loss: 620.2144165039062 = 0.012545821256935596 + 100.0 * 6.2020182609558105
Epoch 2940, val loss: 1.5689671039581299
Epoch 2950, training loss: 619.9915161132812 = 0.012420574203133583 + 100.0 * 6.199791431427002
Epoch 2950, val loss: 1.5720776319503784
Epoch 2960, training loss: 620.0807495117188 = 0.01230363268405199 + 100.0 * 6.200684070587158
Epoch 2960, val loss: 1.5750808715820312
Epoch 2970, training loss: 620.5774536132812 = 0.012187112122774124 + 100.0 * 6.205652236938477
Epoch 2970, val loss: 1.578049898147583
Epoch 2980, training loss: 620.2625732421875 = 0.012060542590916157 + 100.0 * 6.202504634857178
Epoch 2980, val loss: 1.580527901649475
Epoch 2990, training loss: 620.1685180664062 = 0.011942371726036072 + 100.0 * 6.201565742492676
Epoch 2990, val loss: 1.5834953784942627
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 861.6325073242188 = 1.9499956369400024 + 100.0 * 8.596824645996094
Epoch 0, val loss: 1.9474706649780273
Epoch 10, training loss: 861.5279541015625 = 1.9416375160217285 + 100.0 * 8.595863342285156
Epoch 10, val loss: 1.9394503831863403
Epoch 20, training loss: 860.86083984375 = 1.931418538093567 + 100.0 * 8.58929443359375
Epoch 20, val loss: 1.9292405843734741
Epoch 30, training loss: 856.0054321289062 = 1.9185549020767212 + 100.0 * 8.540868759155273
Epoch 30, val loss: 1.9160597324371338
Epoch 40, training loss: 815.0365600585938 = 1.9029909372329712 + 100.0 * 8.131335258483887
Epoch 40, val loss: 1.9000049829483032
Epoch 50, training loss: 743.1986694335938 = 1.8857778310775757 + 100.0 * 7.413128852844238
Epoch 50, val loss: 1.8830769062042236
Epoch 60, training loss: 722.3457641601562 = 1.8728933334350586 + 100.0 * 7.204729080200195
Epoch 60, val loss: 1.8705371618270874
Epoch 70, training loss: 701.0186157226562 = 1.8623061180114746 + 100.0 * 6.991562843322754
Epoch 70, val loss: 1.8604031801223755
Epoch 80, training loss: 689.2515258789062 = 1.8527203798294067 + 100.0 * 6.873988151550293
Epoch 80, val loss: 1.8514034748077393
Epoch 90, training loss: 681.7456665039062 = 1.8437830209732056 + 100.0 * 6.799018383026123
Epoch 90, val loss: 1.8427056074142456
Epoch 100, training loss: 675.6698608398438 = 1.8348337411880493 + 100.0 * 6.7383503913879395
Epoch 100, val loss: 1.8340955972671509
Epoch 110, training loss: 670.3281860351562 = 1.8269124031066895 + 100.0 * 6.6850128173828125
Epoch 110, val loss: 1.8264408111572266
Epoch 120, training loss: 666.3043212890625 = 1.819796085357666 + 100.0 * 6.644845008850098
Epoch 120, val loss: 1.8193613290786743
Epoch 130, training loss: 663.2972412109375 = 1.8130422830581665 + 100.0 * 6.614841938018799
Epoch 130, val loss: 1.812408208847046
Epoch 140, training loss: 660.4144287109375 = 1.806588053703308 + 100.0 * 6.586078643798828
Epoch 140, val loss: 1.805774211883545
Epoch 150, training loss: 657.8016357421875 = 1.8003822565078735 + 100.0 * 6.5600128173828125
Epoch 150, val loss: 1.7993284463882446
Epoch 160, training loss: 655.3447265625 = 1.7943170070648193 + 100.0 * 6.535504341125488
Epoch 160, val loss: 1.7929327487945557
Epoch 170, training loss: 653.3888549804688 = 1.7881381511688232 + 100.0 * 6.516006946563721
Epoch 170, val loss: 1.7864083051681519
Epoch 180, training loss: 651.3619995117188 = 1.7815762758255005 + 100.0 * 6.495804309844971
Epoch 180, val loss: 1.7795912027359009
Epoch 190, training loss: 649.6463623046875 = 1.774666666984558 + 100.0 * 6.47871732711792
Epoch 190, val loss: 1.772528886795044
Epoch 200, training loss: 648.227294921875 = 1.7673180103302002 + 100.0 * 6.464599609375
Epoch 200, val loss: 1.7651383876800537
Epoch 210, training loss: 646.8729858398438 = 1.7593766450881958 + 100.0 * 6.451135635375977
Epoch 210, val loss: 1.757318139076233
Epoch 220, training loss: 645.7864379882812 = 1.7507280111312866 + 100.0 * 6.440357208251953
Epoch 220, val loss: 1.7489380836486816
Epoch 230, training loss: 644.55078125 = 1.7414745092391968 + 100.0 * 6.428093433380127
Epoch 230, val loss: 1.7398954629898071
Epoch 240, training loss: 643.819091796875 = 1.731522798538208 + 100.0 * 6.4208760261535645
Epoch 240, val loss: 1.7302677631378174
Epoch 250, training loss: 642.6803588867188 = 1.7208068370819092 + 100.0 * 6.409595489501953
Epoch 250, val loss: 1.719900369644165
Epoch 260, training loss: 641.9981689453125 = 1.70917809009552 + 100.0 * 6.402889728546143
Epoch 260, val loss: 1.7087866067886353
Epoch 270, training loss: 641.0592041015625 = 1.6966321468353271 + 100.0 * 6.393625259399414
Epoch 270, val loss: 1.6966938972473145
Epoch 280, training loss: 640.177001953125 = 1.6832853555679321 + 100.0 * 6.384937286376953
Epoch 280, val loss: 1.6838806867599487
Epoch 290, training loss: 639.4442749023438 = 1.6690387725830078 + 100.0 * 6.377752304077148
Epoch 290, val loss: 1.6702601909637451
Epoch 300, training loss: 638.955322265625 = 1.6537612676620483 + 100.0 * 6.373015880584717
Epoch 300, val loss: 1.6556446552276611
Epoch 310, training loss: 638.11376953125 = 1.6375328302383423 + 100.0 * 6.364762783050537
Epoch 310, val loss: 1.640291452407837
Epoch 320, training loss: 637.4913940429688 = 1.620484709739685 + 100.0 * 6.358709335327148
Epoch 320, val loss: 1.6241786479949951
Epoch 330, training loss: 637.4041748046875 = 1.6026052236557007 + 100.0 * 6.358016014099121
Epoch 330, val loss: 1.6073716878890991
Epoch 340, training loss: 636.4821166992188 = 1.583848476409912 + 100.0 * 6.348982334136963
Epoch 340, val loss: 1.5897265672683716
Epoch 350, training loss: 635.9065551757812 = 1.564420223236084 + 100.0 * 6.343421936035156
Epoch 350, val loss: 1.5716729164123535
Epoch 360, training loss: 635.4690551757812 = 1.5442887544631958 + 100.0 * 6.339247703552246
Epoch 360, val loss: 1.553117036819458
Epoch 370, training loss: 635.433837890625 = 1.5237128734588623 + 100.0 * 6.3391008377075195
Epoch 370, val loss: 1.5342189073562622
Epoch 380, training loss: 634.6803588867188 = 1.5025146007537842 + 100.0 * 6.331778526306152
Epoch 380, val loss: 1.5149753093719482
Epoch 390, training loss: 634.1898803710938 = 1.4810117483139038 + 100.0 * 6.327088356018066
Epoch 390, val loss: 1.4956592321395874
Epoch 400, training loss: 633.8038940429688 = 1.4593334197998047 + 100.0 * 6.3234453201293945
Epoch 400, val loss: 1.4763820171356201
Epoch 410, training loss: 634.3245239257812 = 1.437436580657959 + 100.0 * 6.32887077331543
Epoch 410, val loss: 1.4571483135223389
Epoch 420, training loss: 633.4193725585938 = 1.4148682355880737 + 100.0 * 6.320045471191406
Epoch 420, val loss: 1.4374992847442627
Epoch 430, training loss: 632.9247436523438 = 1.3924344778060913 + 100.0 * 6.3153228759765625
Epoch 430, val loss: 1.4180679321289062
Epoch 440, training loss: 632.78369140625 = 1.3699709177017212 + 100.0 * 6.314136981964111
Epoch 440, val loss: 1.3988542556762695
Epoch 450, training loss: 632.40234375 = 1.347346305847168 + 100.0 * 6.310549736022949
Epoch 450, val loss: 1.3798582553863525
Epoch 460, training loss: 632.2710571289062 = 1.32468581199646 + 100.0 * 6.3094635009765625
Epoch 460, val loss: 1.3608675003051758
Epoch 470, training loss: 631.6622924804688 = 1.3018814325332642 + 100.0 * 6.3036041259765625
Epoch 470, val loss: 1.342014193534851
Epoch 480, training loss: 631.45458984375 = 1.279162883758545 + 100.0 * 6.301753997802734
Epoch 480, val loss: 1.3234598636627197
Epoch 490, training loss: 631.68212890625 = 1.2564059495925903 + 100.0 * 6.304257392883301
Epoch 490, val loss: 1.3048477172851562
Epoch 500, training loss: 631.1987915039062 = 1.2334553003311157 + 100.0 * 6.299653053283691
Epoch 500, val loss: 1.2865161895751953
Epoch 510, training loss: 630.8162231445312 = 1.210708737373352 + 100.0 * 6.296055316925049
Epoch 510, val loss: 1.2682836055755615
Epoch 520, training loss: 630.485107421875 = 1.1881251335144043 + 100.0 * 6.292970180511475
Epoch 520, val loss: 1.2505314350128174
Epoch 530, training loss: 630.4306030273438 = 1.1656523942947388 + 100.0 * 6.292649269104004
Epoch 530, val loss: 1.2329555749893188
Epoch 540, training loss: 630.0633544921875 = 1.1432414054870605 + 100.0 * 6.289200782775879
Epoch 540, val loss: 1.215533971786499
Epoch 550, training loss: 629.8096923828125 = 1.121121883392334 + 100.0 * 6.286885738372803
Epoch 550, val loss: 1.1986596584320068
Epoch 560, training loss: 630.1075439453125 = 1.0993696451187134 + 100.0 * 6.29008150100708
Epoch 560, val loss: 1.1821928024291992
Epoch 570, training loss: 629.9923706054688 = 1.0779253244400024 + 100.0 * 6.289144515991211
Epoch 570, val loss: 1.1661972999572754
Epoch 580, training loss: 629.380126953125 = 1.056419014930725 + 100.0 * 6.283237457275391
Epoch 580, val loss: 1.1503022909164429
Epoch 590, training loss: 629.0755004882812 = 1.035665512084961 + 100.0 * 6.280398845672607
Epoch 590, val loss: 1.1353625059127808
Epoch 600, training loss: 628.8615112304688 = 1.015433669090271 + 100.0 * 6.27846097946167
Epoch 600, val loss: 1.1210143566131592
Epoch 610, training loss: 628.9971923828125 = 0.9955611228942871 + 100.0 * 6.2800164222717285
Epoch 610, val loss: 1.107195258140564
Epoch 620, training loss: 628.6202392578125 = 0.9759888052940369 + 100.0 * 6.276442050933838
Epoch 620, val loss: 1.0934494733810425
Epoch 630, training loss: 628.4137573242188 = 0.9569815993309021 + 100.0 * 6.2745680809021
Epoch 630, val loss: 1.0807774066925049
Epoch 640, training loss: 628.3807983398438 = 0.9383437037467957 + 100.0 * 6.2744245529174805
Epoch 640, val loss: 1.0680618286132812
Epoch 650, training loss: 628.2921752929688 = 0.9201366901397705 + 100.0 * 6.2737202644348145
Epoch 650, val loss: 1.056244969367981
Epoch 660, training loss: 627.8792724609375 = 0.9023228883743286 + 100.0 * 6.269769191741943
Epoch 660, val loss: 1.0447901487350464
Epoch 670, training loss: 627.7667236328125 = 0.8851863741874695 + 100.0 * 6.268815517425537
Epoch 670, val loss: 1.0340220928192139
Epoch 680, training loss: 628.2095947265625 = 0.8684898018836975 + 100.0 * 6.273411273956299
Epoch 680, val loss: 1.0238807201385498
Epoch 690, training loss: 627.6348876953125 = 0.8519853353500366 + 100.0 * 6.267828941345215
Epoch 690, val loss: 1.0138580799102783
Epoch 700, training loss: 627.3833618164062 = 0.8359030485153198 + 100.0 * 6.265474796295166
Epoch 700, val loss: 1.0043253898620605
Epoch 710, training loss: 627.2676391601562 = 0.8203474283218384 + 100.0 * 6.264472484588623
Epoch 710, val loss: 0.9954748153686523
Epoch 720, training loss: 627.4323120117188 = 0.8051671385765076 + 100.0 * 6.266271114349365
Epoch 720, val loss: 0.9870502352714539
Epoch 730, training loss: 626.9593505859375 = 0.790128231048584 + 100.0 * 6.261692523956299
Epoch 730, val loss: 0.9786051511764526
Epoch 740, training loss: 626.8309936523438 = 0.7755873203277588 + 100.0 * 6.260554313659668
Epoch 740, val loss: 0.9707021117210388
Epoch 750, training loss: 626.740234375 = 0.7614261507987976 + 100.0 * 6.2597880363464355
Epoch 750, val loss: 0.9634006023406982
Epoch 760, training loss: 627.0641479492188 = 0.7474266290664673 + 100.0 * 6.263166904449463
Epoch 760, val loss: 0.9562395811080933
Epoch 770, training loss: 626.7010498046875 = 0.7337433695793152 + 100.0 * 6.259673118591309
Epoch 770, val loss: 0.9496625065803528
Epoch 780, training loss: 626.4201049804688 = 0.7203093767166138 + 100.0 * 6.256997585296631
Epoch 780, val loss: 0.9431819319725037
Epoch 790, training loss: 626.7408447265625 = 0.7072857022285461 + 100.0 * 6.260335445404053
Epoch 790, val loss: 0.9366072416305542
Epoch 800, training loss: 626.29736328125 = 0.6945193409919739 + 100.0 * 6.256028652191162
Epoch 800, val loss: 0.9315263628959656
Epoch 810, training loss: 626.1746215820312 = 0.6820695996284485 + 100.0 * 6.254925727844238
Epoch 810, val loss: 0.9257475137710571
Epoch 820, training loss: 626.2902221679688 = 0.6699908375740051 + 100.0 * 6.256202697753906
Epoch 820, val loss: 0.92086261510849
Epoch 830, training loss: 625.9860229492188 = 0.658096969127655 + 100.0 * 6.253279209136963
Epoch 830, val loss: 0.9156904220581055
Epoch 840, training loss: 626.1165771484375 = 0.6464755535125732 + 100.0 * 6.254701137542725
Epoch 840, val loss: 0.9112959504127502
Epoch 850, training loss: 625.7405395507812 = 0.6350263357162476 + 100.0 * 6.2510552406311035
Epoch 850, val loss: 0.90663081407547
Epoch 860, training loss: 625.63232421875 = 0.6239194273948669 + 100.0 * 6.250084400177002
Epoch 860, val loss: 0.9025408029556274
Epoch 870, training loss: 626.0400390625 = 0.6128556132316589 + 100.0 * 6.254271984100342
Epoch 870, val loss: 0.898419201374054
Epoch 880, training loss: 625.470458984375 = 0.6020081639289856 + 100.0 * 6.248684406280518
Epoch 880, val loss: 0.8947160243988037
Epoch 890, training loss: 625.3744506835938 = 0.5914642214775085 + 100.0 * 6.247829437255859
Epoch 890, val loss: 0.8913576602935791
Epoch 900, training loss: 625.3064575195312 = 0.5811142325401306 + 100.0 * 6.24725341796875
Epoch 900, val loss: 0.8877924680709839
Epoch 910, training loss: 625.8916625976562 = 0.571010947227478 + 100.0 * 6.253206729888916
Epoch 910, val loss: 0.8849949836730957
Epoch 920, training loss: 625.4757690429688 = 0.5607476234436035 + 100.0 * 6.249150276184082
Epoch 920, val loss: 0.8813768625259399
Epoch 930, training loss: 625.0807495117188 = 0.5508356690406799 + 100.0 * 6.245298862457275
Epoch 930, val loss: 0.8782647252082825
Epoch 940, training loss: 625.0297241210938 = 0.5411940217018127 + 100.0 * 6.244884967803955
Epoch 940, val loss: 0.8757032155990601
Epoch 950, training loss: 625.119140625 = 0.531669557094574 + 100.0 * 6.245874881744385
Epoch 950, val loss: 0.8730428218841553
Epoch 960, training loss: 625.0559692382812 = 0.5223120450973511 + 100.0 * 6.245336532592773
Epoch 960, val loss: 0.8711566925048828
Epoch 970, training loss: 625.2168579101562 = 0.5129958987236023 + 100.0 * 6.2470383644104
Epoch 970, val loss: 0.8682634234428406
Epoch 980, training loss: 624.7884521484375 = 0.503770112991333 + 100.0 * 6.242846488952637
Epoch 980, val loss: 0.8665134906768799
Epoch 990, training loss: 624.6520385742188 = 0.49486014246940613 + 100.0 * 6.241571426391602
Epoch 990, val loss: 0.864380955696106
Epoch 1000, training loss: 624.53369140625 = 0.48614779114723206 + 100.0 * 6.240475177764893
Epoch 1000, val loss: 0.8627602458000183
Epoch 1010, training loss: 625.0820922851562 = 0.4775969982147217 + 100.0 * 6.246044635772705
Epoch 1010, val loss: 0.8610718846321106
Epoch 1020, training loss: 624.9154663085938 = 0.4688579738140106 + 100.0 * 6.2444658279418945
Epoch 1020, val loss: 0.8588985800743103
Epoch 1030, training loss: 624.4144287109375 = 0.4604247808456421 + 100.0 * 6.239539623260498
Epoch 1030, val loss: 0.8576253652572632
Epoch 1040, training loss: 624.2837524414062 = 0.45221951603889465 + 100.0 * 6.238315582275391
Epoch 1040, val loss: 0.856306791305542
Epoch 1050, training loss: 624.6520385742188 = 0.44420894980430603 + 100.0 * 6.2420783042907715
Epoch 1050, val loss: 0.8550230264663696
Epoch 1060, training loss: 624.4598388671875 = 0.4360882639884949 + 100.0 * 6.240237236022949
Epoch 1060, val loss: 0.8533181548118591
Epoch 1070, training loss: 624.257080078125 = 0.42821797728538513 + 100.0 * 6.238288402557373
Epoch 1070, val loss: 0.8528105616569519
Epoch 1080, training loss: 624.1063232421875 = 0.42051464319229126 + 100.0 * 6.236857891082764
Epoch 1080, val loss: 0.8514009714126587
Epoch 1090, training loss: 624.2028198242188 = 0.4130685329437256 + 100.0 * 6.2378973960876465
Epoch 1090, val loss: 0.851083517074585
Epoch 1100, training loss: 624.1751098632812 = 0.40554019808769226 + 100.0 * 6.237696170806885
Epoch 1100, val loss: 0.8501545786857605
Epoch 1110, training loss: 623.958984375 = 0.3981644809246063 + 100.0 * 6.235608100891113
Epoch 1110, val loss: 0.849538266658783
Epoch 1120, training loss: 624.3074340820312 = 0.3909197747707367 + 100.0 * 6.239165306091309
Epoch 1120, val loss: 0.848726212978363
Epoch 1130, training loss: 623.925048828125 = 0.3838402032852173 + 100.0 * 6.235412120819092
Epoch 1130, val loss: 0.8482514023780823
Epoch 1140, training loss: 623.76025390625 = 0.3768933415412903 + 100.0 * 6.2338337898254395
Epoch 1140, val loss: 0.8476669788360596
Epoch 1150, training loss: 624.1220703125 = 0.3701365292072296 + 100.0 * 6.237519264221191
Epoch 1150, val loss: 0.8473808765411377
Epoch 1160, training loss: 623.9912719726562 = 0.363330215215683 + 100.0 * 6.236279010772705
Epoch 1160, val loss: 0.8476563692092896
Epoch 1170, training loss: 623.8244018554688 = 0.3566521406173706 + 100.0 * 6.234677791595459
Epoch 1170, val loss: 0.8473732471466064
Epoch 1180, training loss: 623.5980224609375 = 0.3501773178577423 + 100.0 * 6.232478618621826
Epoch 1180, val loss: 0.8474886417388916
Epoch 1190, training loss: 623.5415649414062 = 0.3438376188278198 + 100.0 * 6.231977462768555
Epoch 1190, val loss: 0.8474268317222595
Epoch 1200, training loss: 623.9437255859375 = 0.33762961626052856 + 100.0 * 6.236061096191406
Epoch 1200, val loss: 0.8478955030441284
Epoch 1210, training loss: 623.7685546875 = 0.33136704564094543 + 100.0 * 6.234372138977051
Epoch 1210, val loss: 0.8482035398483276
Epoch 1220, training loss: 623.5512084960938 = 0.32526567578315735 + 100.0 * 6.232259273529053
Epoch 1220, val loss: 0.848576009273529
Epoch 1230, training loss: 623.7437133789062 = 0.3192864656448364 + 100.0 * 6.234244346618652
Epoch 1230, val loss: 0.849453330039978
Epoch 1240, training loss: 623.4693603515625 = 0.31342563033103943 + 100.0 * 6.2315592765808105
Epoch 1240, val loss: 0.8498512506484985
Epoch 1250, training loss: 623.4657592773438 = 0.30768629908561707 + 100.0 * 6.23158073425293
Epoch 1250, val loss: 0.850578248500824
Epoch 1260, training loss: 623.3463745117188 = 0.3019987940788269 + 100.0 * 6.230443954467773
Epoch 1260, val loss: 0.8510540127754211
Epoch 1270, training loss: 623.3546142578125 = 0.2964945137500763 + 100.0 * 6.230581283569336
Epoch 1270, val loss: 0.8519052267074585
Epoch 1280, training loss: 623.2550048828125 = 0.2910498380661011 + 100.0 * 6.229639053344727
Epoch 1280, val loss: 0.8530260324478149
Epoch 1290, training loss: 623.242919921875 = 0.2857382297515869 + 100.0 * 6.229571342468262
Epoch 1290, val loss: 0.8546834588050842
Epoch 1300, training loss: 623.4036254882812 = 0.28045418858528137 + 100.0 * 6.231231689453125
Epoch 1300, val loss: 0.8553965091705322
Epoch 1310, training loss: 623.18994140625 = 0.2751968801021576 + 100.0 * 6.229147434234619
Epoch 1310, val loss: 0.8561265468597412
Epoch 1320, training loss: 622.9732666015625 = 0.27013394236564636 + 100.0 * 6.227031230926514
Epoch 1320, val loss: 0.8577482104301453
Epoch 1330, training loss: 622.9243774414062 = 0.26520025730133057 + 100.0 * 6.226592063903809
Epoch 1330, val loss: 0.8591935038566589
Epoch 1340, training loss: 623.1162109375 = 0.2603839337825775 + 100.0 * 6.228558540344238
Epoch 1340, val loss: 0.8609240651130676
Epoch 1350, training loss: 623.1675415039062 = 0.25546786189079285 + 100.0 * 6.22912073135376
Epoch 1350, val loss: 0.8617141842842102
Epoch 1360, training loss: 622.990966796875 = 0.25068140029907227 + 100.0 * 6.227403163909912
Epoch 1360, val loss: 0.8638663291931152
Epoch 1370, training loss: 622.8935546875 = 0.24604497849941254 + 100.0 * 6.226475238800049
Epoch 1370, val loss: 0.8651319146156311
Epoch 1380, training loss: 622.9613037109375 = 0.24153923988342285 + 100.0 * 6.227197647094727
Epoch 1380, val loss: 0.8669763803482056
Epoch 1390, training loss: 622.7266845703125 = 0.23707284033298492 + 100.0 * 6.224896430969238
Epoch 1390, val loss: 0.8685733675956726
Epoch 1400, training loss: 623.0338134765625 = 0.23272965848445892 + 100.0 * 6.228010654449463
Epoch 1400, val loss: 0.870069682598114
Epoch 1410, training loss: 623.14697265625 = 0.2283690720796585 + 100.0 * 6.229186058044434
Epoch 1410, val loss: 0.8720809817314148
Epoch 1420, training loss: 622.8657836914062 = 0.22408896684646606 + 100.0 * 6.226417541503906
Epoch 1420, val loss: 0.8740072846412659
Epoch 1430, training loss: 622.6217651367188 = 0.21993118524551392 + 100.0 * 6.224018573760986
Epoch 1430, val loss: 0.8762056231498718
Epoch 1440, training loss: 622.5469360351562 = 0.21589826047420502 + 100.0 * 6.223310470581055
Epoch 1440, val loss: 0.8783530592918396
Epoch 1450, training loss: 622.7968139648438 = 0.2119583934545517 + 100.0 * 6.225848197937012
Epoch 1450, val loss: 0.8806729316711426
Epoch 1460, training loss: 622.6752319335938 = 0.20797397196292877 + 100.0 * 6.224672794342041
Epoch 1460, val loss: 0.8833833932876587
Epoch 1470, training loss: 622.6112670898438 = 0.2040417492389679 + 100.0 * 6.224072456359863
Epoch 1470, val loss: 0.8850148320198059
Epoch 1480, training loss: 622.5654907226562 = 0.2002001702785492 + 100.0 * 6.2236528396606445
Epoch 1480, val loss: 0.8869465589523315
Epoch 1490, training loss: 622.4705200195312 = 0.19651339948177338 + 100.0 * 6.2227396965026855
Epoch 1490, val loss: 0.8900257349014282
Epoch 1500, training loss: 622.6912841796875 = 0.19289879500865936 + 100.0 * 6.224984169006348
Epoch 1500, val loss: 0.8926287889480591
Epoch 1510, training loss: 622.4758911132812 = 0.18929824233055115 + 100.0 * 6.222866058349609
Epoch 1510, val loss: 0.8947882056236267
Epoch 1520, training loss: 622.3284912109375 = 0.18579532206058502 + 100.0 * 6.221426963806152
Epoch 1520, val loss: 0.8978082537651062
Epoch 1530, training loss: 622.64892578125 = 0.18237152695655823 + 100.0 * 6.224665641784668
Epoch 1530, val loss: 0.9000943303108215
Epoch 1540, training loss: 622.3598022460938 = 0.17888957262039185 + 100.0 * 6.221809387207031
Epoch 1540, val loss: 0.9020181894302368
Epoch 1550, training loss: 622.2950439453125 = 0.17559224367141724 + 100.0 * 6.221194744110107
Epoch 1550, val loss: 0.9051920175552368
Epoch 1560, training loss: 622.2190551757812 = 0.17235930263996124 + 100.0 * 6.2204670906066895
Epoch 1560, val loss: 0.907546877861023
Epoch 1570, training loss: 622.5380859375 = 0.16920466721057892 + 100.0 * 6.223689079284668
Epoch 1570, val loss: 0.9101024270057678
Epoch 1580, training loss: 622.3119506835938 = 0.166044220328331 + 100.0 * 6.22145938873291
Epoch 1580, val loss: 0.9138085842132568
Epoch 1590, training loss: 622.1095581054688 = 0.16295476257801056 + 100.0 * 6.219466209411621
Epoch 1590, val loss: 0.9160832166671753
Epoch 1600, training loss: 622.1536865234375 = 0.15997618436813354 + 100.0 * 6.219936847686768
Epoch 1600, val loss: 0.919337809085846
Epoch 1610, training loss: 622.8157348632812 = 0.15708205103874207 + 100.0 * 6.22658634185791
Epoch 1610, val loss: 0.9227683544158936
Epoch 1620, training loss: 622.656982421875 = 0.15406551957130432 + 100.0 * 6.225028991699219
Epoch 1620, val loss: 0.9241154193878174
Epoch 1630, training loss: 622.0337524414062 = 0.15121987462043762 + 100.0 * 6.218824863433838
Epoch 1630, val loss: 0.9277823567390442
Epoch 1640, training loss: 621.9008178710938 = 0.14845605194568634 + 100.0 * 6.217523574829102
Epoch 1640, val loss: 0.9307142496109009
Epoch 1650, training loss: 621.8899536132812 = 0.14579471945762634 + 100.0 * 6.217441558837891
Epoch 1650, val loss: 0.9336225986480713
Epoch 1660, training loss: 622.5103149414062 = 0.14318552613258362 + 100.0 * 6.2236714363098145
Epoch 1660, val loss: 0.9362032413482666
Epoch 1670, training loss: 622.0155639648438 = 0.14050228893756866 + 100.0 * 6.218750476837158
Epoch 1670, val loss: 0.9400032162666321
Epoch 1680, training loss: 621.9749145507812 = 0.13794508576393127 + 100.0 * 6.218369960784912
Epoch 1680, val loss: 0.9429208040237427
Epoch 1690, training loss: 622.2195434570312 = 0.13545358180999756 + 100.0 * 6.220840930938721
Epoch 1690, val loss: 0.9462110996246338
Epoch 1700, training loss: 621.8909301757812 = 0.13295137882232666 + 100.0 * 6.2175798416137695
Epoch 1700, val loss: 0.9488832950592041
Epoch 1710, training loss: 621.8016967773438 = 0.130563884973526 + 100.0 * 6.216711521148682
Epoch 1710, val loss: 0.9524465203285217
Epoch 1720, training loss: 621.8477783203125 = 0.12824679911136627 + 100.0 * 6.217195510864258
Epoch 1720, val loss: 0.9555156826972961
Epoch 1730, training loss: 621.795654296875 = 0.1259397268295288 + 100.0 * 6.2166972160339355
Epoch 1730, val loss: 0.958653450012207
Epoch 1740, training loss: 622.0127563476562 = 0.123692087829113 + 100.0 * 6.21889066696167
Epoch 1740, val loss: 0.9620092511177063
Epoch 1750, training loss: 621.8298950195312 = 0.12144972383975983 + 100.0 * 6.2170844078063965
Epoch 1750, val loss: 0.9649128317832947
Epoch 1760, training loss: 622.0555419921875 = 0.11927664279937744 + 100.0 * 6.219362735748291
Epoch 1760, val loss: 0.9685492515563965
Epoch 1770, training loss: 621.8530883789062 = 0.11710105091333389 + 100.0 * 6.21735954284668
Epoch 1770, val loss: 0.9712293744087219
Epoch 1780, training loss: 621.6202392578125 = 0.11500231176614761 + 100.0 * 6.215052604675293
Epoch 1780, val loss: 0.9744307994842529
Epoch 1790, training loss: 621.530517578125 = 0.11299403756856918 + 100.0 * 6.214175224304199
Epoch 1790, val loss: 0.9778573513031006
Epoch 1800, training loss: 621.7153930664062 = 0.11102849245071411 + 100.0 * 6.216043949127197
Epoch 1800, val loss: 0.9811657667160034
Epoch 1810, training loss: 621.678466796875 = 0.10905255377292633 + 100.0 * 6.215693950653076
Epoch 1810, val loss: 0.9843332767486572
Epoch 1820, training loss: 621.5957641601562 = 0.1071176826953888 + 100.0 * 6.214886665344238
Epoch 1820, val loss: 0.988304078578949
Epoch 1830, training loss: 621.5077514648438 = 0.10524206608533859 + 100.0 * 6.214025497436523
Epoch 1830, val loss: 0.9917064309120178
Epoch 1840, training loss: 622.6286010742188 = 0.10345199704170227 + 100.0 * 6.225251197814941
Epoch 1840, val loss: 0.995468020439148
Epoch 1850, training loss: 621.8111572265625 = 0.10151291638612747 + 100.0 * 6.21709680557251
Epoch 1850, val loss: 0.9977586269378662
Epoch 1860, training loss: 621.4317016601562 = 0.09973171353340149 + 100.0 * 6.213319778442383
Epoch 1860, val loss: 1.0017772912979126
Epoch 1870, training loss: 621.3116455078125 = 0.09801594167947769 + 100.0 * 6.212136268615723
Epoch 1870, val loss: 1.0052746534347534
Epoch 1880, training loss: 621.9239501953125 = 0.09636738151311874 + 100.0 * 6.218276023864746
Epoch 1880, val loss: 1.0095171928405762
Epoch 1890, training loss: 621.4163208007812 = 0.09462568163871765 + 100.0 * 6.213217258453369
Epoch 1890, val loss: 1.0117088556289673
Epoch 1900, training loss: 621.4342651367188 = 0.092974953353405 + 100.0 * 6.213413238525391
Epoch 1900, val loss: 1.0158385038375854
Epoch 1910, training loss: 621.3133544921875 = 0.09136316180229187 + 100.0 * 6.212219715118408
Epoch 1910, val loss: 1.018609642982483
Epoch 1920, training loss: 621.19189453125 = 0.08980511128902435 + 100.0 * 6.2110209465026855
Epoch 1920, val loss: 1.0223573446273804
Epoch 1930, training loss: 621.6644287109375 = 0.08830926567316055 + 100.0 * 6.215761184692383
Epoch 1930, val loss: 1.0251493453979492
Epoch 1940, training loss: 621.2550659179688 = 0.08674528449773788 + 100.0 * 6.21168327331543
Epoch 1940, val loss: 1.029902696609497
Epoch 1950, training loss: 621.169189453125 = 0.08524005115032196 + 100.0 * 6.21083927154541
Epoch 1950, val loss: 1.032483696937561
Epoch 1960, training loss: 621.0983276367188 = 0.08378691971302032 + 100.0 * 6.210145473480225
Epoch 1960, val loss: 1.036403775215149
Epoch 1970, training loss: 621.7356567382812 = 0.08240567147731781 + 100.0 * 6.2165327072143555
Epoch 1970, val loss: 1.039493441581726
Epoch 1980, training loss: 621.3126831054688 = 0.08096561580896378 + 100.0 * 6.212316989898682
Epoch 1980, val loss: 1.043439507484436
Epoch 1990, training loss: 621.1395874023438 = 0.07956754416227341 + 100.0 * 6.210599899291992
Epoch 1990, val loss: 1.0462532043457031
Epoch 2000, training loss: 621.0471801757812 = 0.07823684066534042 + 100.0 * 6.209689617156982
Epoch 2000, val loss: 1.050148844718933
Epoch 2010, training loss: 621.58642578125 = 0.07695872336626053 + 100.0 * 6.215094566345215
Epoch 2010, val loss: 1.0532630681991577
Epoch 2020, training loss: 621.0630493164062 = 0.07564537227153778 + 100.0 * 6.209874153137207
Epoch 2020, val loss: 1.0573316812515259
Epoch 2030, training loss: 621.0231323242188 = 0.07437547296285629 + 100.0 * 6.209487438201904
Epoch 2030, val loss: 1.0605146884918213
Epoch 2040, training loss: 621.0643920898438 = 0.07315827906131744 + 100.0 * 6.209911823272705
Epoch 2040, val loss: 1.0645076036453247
Epoch 2050, training loss: 621.1510009765625 = 0.07196111977100372 + 100.0 * 6.210790157318115
Epoch 2050, val loss: 1.0679067373275757
Epoch 2060, training loss: 621.7100219726562 = 0.07077082246541977 + 100.0 * 6.2163920402526855
Epoch 2060, val loss: 1.0716125965118408
Epoch 2070, training loss: 621.1879272460938 = 0.06955575942993164 + 100.0 * 6.211183547973633
Epoch 2070, val loss: 1.0737630128860474
Epoch 2080, training loss: 621.0075073242188 = 0.06840173155069351 + 100.0 * 6.209391117095947
Epoch 2080, val loss: 1.0778536796569824
Epoch 2090, training loss: 620.93994140625 = 0.06729509681463242 + 100.0 * 6.208725929260254
Epoch 2090, val loss: 1.080849051475525
Epoch 2100, training loss: 621.1148071289062 = 0.06621722131967545 + 100.0 * 6.210485935211182
Epoch 2100, val loss: 1.0841751098632812
Epoch 2110, training loss: 620.9227905273438 = 0.0651392936706543 + 100.0 * 6.208576679229736
Epoch 2110, val loss: 1.0881550312042236
Epoch 2120, training loss: 621.092041015625 = 0.06409543007612228 + 100.0 * 6.21027946472168
Epoch 2120, val loss: 1.0922406911849976
Epoch 2130, training loss: 621.0584106445312 = 0.06305121630430222 + 100.0 * 6.209953784942627
Epoch 2130, val loss: 1.095031499862671
Epoch 2140, training loss: 621.0684814453125 = 0.062033239752054214 + 100.0 * 6.210064888000488
Epoch 2140, val loss: 1.0984538793563843
Epoch 2150, training loss: 620.7987060546875 = 0.06103289872407913 + 100.0 * 6.207376480102539
Epoch 2150, val loss: 1.1017805337905884
Epoch 2160, training loss: 620.7047729492188 = 0.060077689588069916 + 100.0 * 6.206447124481201
Epoch 2160, val loss: 1.1059058904647827
Epoch 2170, training loss: 621.135009765625 = 0.059155821800231934 + 100.0 * 6.210758686065674
Epoch 2170, val loss: 1.1091408729553223
Epoch 2180, training loss: 620.6951293945312 = 0.05818454921245575 + 100.0 * 6.206369876861572
Epoch 2180, val loss: 1.112153172492981
Epoch 2190, training loss: 620.6427612304688 = 0.05726237595081329 + 100.0 * 6.205855369567871
Epoch 2190, val loss: 1.1157902479171753
Epoch 2200, training loss: 620.6135864257812 = 0.05638385936617851 + 100.0 * 6.205572128295898
Epoch 2200, val loss: 1.1194300651550293
Epoch 2210, training loss: 621.3678588867188 = 0.0555337555706501 + 100.0 * 6.213123321533203
Epoch 2210, val loss: 1.1230448484420776
Epoch 2220, training loss: 620.7388916015625 = 0.05463982746005058 + 100.0 * 6.20684289932251
Epoch 2220, val loss: 1.1261956691741943
Epoch 2230, training loss: 620.7079467773438 = 0.05378495529294014 + 100.0 * 6.206542015075684
Epoch 2230, val loss: 1.1298131942749023
Epoch 2240, training loss: 620.6585693359375 = 0.05296570062637329 + 100.0 * 6.206056118011475
Epoch 2240, val loss: 1.1329764127731323
Epoch 2250, training loss: 620.7791748046875 = 0.052167948335409164 + 100.0 * 6.207269668579102
Epoch 2250, val loss: 1.1357792615890503
Epoch 2260, training loss: 620.8067626953125 = 0.051364168524742126 + 100.0 * 6.207553863525391
Epoch 2260, val loss: 1.1390202045440674
Epoch 2270, training loss: 620.8048095703125 = 0.05058138817548752 + 100.0 * 6.2075419425964355
Epoch 2270, val loss: 1.1435819864273071
Epoch 2280, training loss: 620.6676025390625 = 0.04981059208512306 + 100.0 * 6.206177711486816
Epoch 2280, val loss: 1.1463522911071777
Epoch 2290, training loss: 620.4937744140625 = 0.04906964302062988 + 100.0 * 6.204446792602539
Epoch 2290, val loss: 1.1497657299041748
Epoch 2300, training loss: 620.5447387695312 = 0.04835828021168709 + 100.0 * 6.2049641609191895
Epoch 2300, val loss: 1.152817964553833
Epoch 2310, training loss: 620.8500366210938 = 0.04764987528324127 + 100.0 * 6.208023548126221
Epoch 2310, val loss: 1.1563698053359985
Epoch 2320, training loss: 620.747314453125 = 0.04694458097219467 + 100.0 * 6.207003593444824
Epoch 2320, val loss: 1.1612576246261597
Epoch 2330, training loss: 620.8128662109375 = 0.04623706638813019 + 100.0 * 6.207665920257568
Epoch 2330, val loss: 1.1635711193084717
Epoch 2340, training loss: 620.628662109375 = 0.04554320126771927 + 100.0 * 6.205831527709961
Epoch 2340, val loss: 1.1666544675827026
Epoch 2350, training loss: 620.61181640625 = 0.04487191513180733 + 100.0 * 6.205669403076172
Epoch 2350, val loss: 1.1692296266555786
Epoch 2360, training loss: 620.5617065429688 = 0.04422255605459213 + 100.0 * 6.205174446105957
Epoch 2360, val loss: 1.1736114025115967
Epoch 2370, training loss: 620.529052734375 = 0.043580178171396255 + 100.0 * 6.204854965209961
Epoch 2370, val loss: 1.1766430139541626
Epoch 2380, training loss: 620.8965454101562 = 0.04295431077480316 + 100.0 * 6.208535671234131
Epoch 2380, val loss: 1.17962646484375
Epoch 2390, training loss: 620.7222290039062 = 0.04230945184826851 + 100.0 * 6.206799030303955
Epoch 2390, val loss: 1.182700276374817
Epoch 2400, training loss: 620.403076171875 = 0.04168752208352089 + 100.0 * 6.203613758087158
Epoch 2400, val loss: 1.1866028308868408
Epoch 2410, training loss: 620.2885131835938 = 0.04110057279467583 + 100.0 * 6.202474117279053
Epoch 2410, val loss: 1.18938410282135
Epoch 2420, training loss: 620.3111572265625 = 0.040538109838962555 + 100.0 * 6.202706336975098
Epoch 2420, val loss: 1.1925151348114014
Epoch 2430, training loss: 620.71826171875 = 0.03999515622854233 + 100.0 * 6.206782817840576
Epoch 2430, val loss: 1.1958363056182861
Epoch 2440, training loss: 620.3114013671875 = 0.03940495848655701 + 100.0 * 6.202719688415527
Epoch 2440, val loss: 1.1995136737823486
Epoch 2450, training loss: 620.304443359375 = 0.038847628980875015 + 100.0 * 6.202656269073486
Epoch 2450, val loss: 1.202705979347229
Epoch 2460, training loss: 620.3339233398438 = 0.03831375762820244 + 100.0 * 6.202956199645996
Epoch 2460, val loss: 1.205561637878418
Epoch 2470, training loss: 620.6649169921875 = 0.03778916224837303 + 100.0 * 6.206271648406982
Epoch 2470, val loss: 1.2083796262741089
Epoch 2480, training loss: 620.5194702148438 = 0.03725927323102951 + 100.0 * 6.204822540283203
Epoch 2480, val loss: 1.2122501134872437
Epoch 2490, training loss: 620.3450317382812 = 0.03673364594578743 + 100.0 * 6.203083038330078
Epoch 2490, val loss: 1.21516752243042
Epoch 2500, training loss: 620.3562622070312 = 0.03623305261135101 + 100.0 * 6.203199863433838
Epoch 2500, val loss: 1.2192916870117188
Epoch 2510, training loss: 620.2239990234375 = 0.03573596850037575 + 100.0 * 6.201882839202881
Epoch 2510, val loss: 1.2218539714813232
Epoch 2520, training loss: 620.2423095703125 = 0.03525705263018608 + 100.0 * 6.202070236206055
Epoch 2520, val loss: 1.2250287532806396
Epoch 2530, training loss: 620.3001098632812 = 0.03478731960058212 + 100.0 * 6.202653408050537
Epoch 2530, val loss: 1.2282040119171143
Epoch 2540, training loss: 620.3002319335938 = 0.03431837260723114 + 100.0 * 6.2026591300964355
Epoch 2540, val loss: 1.2307785749435425
Epoch 2550, training loss: 620.2608032226562 = 0.033861663192510605 + 100.0 * 6.202269554138184
Epoch 2550, val loss: 1.2338491678237915
Epoch 2560, training loss: 620.2503662109375 = 0.0334128737449646 + 100.0 * 6.202169895172119
Epoch 2560, val loss: 1.2373992204666138
Epoch 2570, training loss: 620.4894409179688 = 0.03296762704849243 + 100.0 * 6.204564571380615
Epoch 2570, val loss: 1.241393804550171
Epoch 2580, training loss: 620.1624145507812 = 0.03251280635595322 + 100.0 * 6.201298713684082
Epoch 2580, val loss: 1.243653655052185
Epoch 2590, training loss: 620.0473022460938 = 0.03208361193537712 + 100.0 * 6.2001519203186035
Epoch 2590, val loss: 1.246386170387268
Epoch 2600, training loss: 620.0631713867188 = 0.03168503940105438 + 100.0 * 6.200314998626709
Epoch 2600, val loss: 1.2500505447387695
Epoch 2610, training loss: 620.6640014648438 = 0.03129197284579277 + 100.0 * 6.206326961517334
Epoch 2610, val loss: 1.2531946897506714
Epoch 2620, training loss: 620.12841796875 = 0.030861278995871544 + 100.0 * 6.20097541809082
Epoch 2620, val loss: 1.2556734085083008
Epoch 2630, training loss: 619.9952392578125 = 0.030459057539701462 + 100.0 * 6.199647903442383
Epoch 2630, val loss: 1.2589839696884155
Epoch 2640, training loss: 620.1902465820312 = 0.030081242322921753 + 100.0 * 6.201601505279541
Epoch 2640, val loss: 1.2628531455993652
Epoch 2650, training loss: 620.335205078125 = 0.02969544753432274 + 100.0 * 6.203054904937744
Epoch 2650, val loss: 1.264778733253479
Epoch 2660, training loss: 620.3585205078125 = 0.02930472232401371 + 100.0 * 6.203292369842529
Epoch 2660, val loss: 1.2669328451156616
Epoch 2670, training loss: 620.0370483398438 = 0.028927525505423546 + 100.0 * 6.2000813484191895
Epoch 2670, val loss: 1.2710782289505005
Epoch 2680, training loss: 619.9703979492188 = 0.028566865250468254 + 100.0 * 6.199418544769287
Epoch 2680, val loss: 1.2734954357147217
Epoch 2690, training loss: 620.0602416992188 = 0.028216462582349777 + 100.0 * 6.200320720672607
Epoch 2690, val loss: 1.2763603925704956
Epoch 2700, training loss: 620.0658569335938 = 0.027869002893567085 + 100.0 * 6.200379848480225
Epoch 2700, val loss: 1.2800241708755493
Epoch 2710, training loss: 620.1599731445312 = 0.02752435766160488 + 100.0 * 6.201324462890625
Epoch 2710, val loss: 1.2831891775131226
Epoch 2720, training loss: 620.12548828125 = 0.027172619476914406 + 100.0 * 6.20098352432251
Epoch 2720, val loss: 1.2851660251617432
Epoch 2730, training loss: 619.9037475585938 = 0.026836911216378212 + 100.0 * 6.1987690925598145
Epoch 2730, val loss: 1.2886515855789185
Epoch 2740, training loss: 619.9996948242188 = 0.026511352509260178 + 100.0 * 6.199732303619385
Epoch 2740, val loss: 1.2917431592941284
Epoch 2750, training loss: 620.0096435546875 = 0.026187073439359665 + 100.0 * 6.19983434677124
Epoch 2750, val loss: 1.293934941291809
Epoch 2760, training loss: 619.938232421875 = 0.025874745100736618 + 100.0 * 6.199123382568359
Epoch 2760, val loss: 1.2969940900802612
Epoch 2770, training loss: 620.1054077148438 = 0.025568006560206413 + 100.0 * 6.200798034667969
Epoch 2770, val loss: 1.2994612455368042
Epoch 2780, training loss: 619.9996337890625 = 0.025259142741560936 + 100.0 * 6.199743747711182
Epoch 2780, val loss: 1.303125023841858
Epoch 2790, training loss: 619.914794921875 = 0.024950750172138214 + 100.0 * 6.1988983154296875
Epoch 2790, val loss: 1.3058463335037231
Epoch 2800, training loss: 620.1094970703125 = 0.02467057667672634 + 100.0 * 6.20084810256958
Epoch 2800, val loss: 1.308646559715271
Epoch 2810, training loss: 619.8822021484375 = 0.024364659562706947 + 100.0 * 6.198578357696533
Epoch 2810, val loss: 1.3110218048095703
Epoch 2820, training loss: 619.8786010742188 = 0.0240803025662899 + 100.0 * 6.198544979095459
Epoch 2820, val loss: 1.313706874847412
Epoch 2830, training loss: 619.777587890625 = 0.023799054324626923 + 100.0 * 6.197537899017334
Epoch 2830, val loss: 1.3171700239181519
Epoch 2840, training loss: 620.03662109375 = 0.023533936589956284 + 100.0 * 6.200130462646484
Epoch 2840, val loss: 1.3207865953445435
Epoch 2850, training loss: 619.97412109375 = 0.023257754743099213 + 100.0 * 6.1995086669921875
Epoch 2850, val loss: 1.3233026266098022
Epoch 2860, training loss: 619.8050537109375 = 0.022980114445090294 + 100.0 * 6.197820663452148
Epoch 2860, val loss: 1.3253953456878662
Epoch 2870, training loss: 620.0042724609375 = 0.022714916616678238 + 100.0 * 6.19981575012207
Epoch 2870, val loss: 1.328312635421753
Epoch 2880, training loss: 619.9232788085938 = 0.02244262769818306 + 100.0 * 6.199007987976074
Epoch 2880, val loss: 1.330884575843811
Epoch 2890, training loss: 619.8025512695312 = 0.022186603397130966 + 100.0 * 6.197803497314453
Epoch 2890, val loss: 1.333678960800171
Epoch 2900, training loss: 619.64013671875 = 0.0219366867095232 + 100.0 * 6.1961822509765625
Epoch 2900, val loss: 1.3368513584136963
Epoch 2910, training loss: 619.5939331054688 = 0.021700307726860046 + 100.0 * 6.1957221031188965
Epoch 2910, val loss: 1.3394691944122314
Epoch 2920, training loss: 619.8192749023438 = 0.021471809595823288 + 100.0 * 6.1979780197143555
Epoch 2920, val loss: 1.3419828414916992
Epoch 2930, training loss: 619.8616943359375 = 0.021228576079010963 + 100.0 * 6.198404312133789
Epoch 2930, val loss: 1.3447177410125732
Epoch 2940, training loss: 620.0816040039062 = 0.020987410098314285 + 100.0 * 6.200606346130371
Epoch 2940, val loss: 1.3475050926208496
Epoch 2950, training loss: 619.773681640625 = 0.020739082247018814 + 100.0 * 6.197529315948486
Epoch 2950, val loss: 1.349015474319458
Epoch 2960, training loss: 619.6854858398438 = 0.02051454409956932 + 100.0 * 6.196649551391602
Epoch 2960, val loss: 1.35188627243042
Epoch 2970, training loss: 619.551513671875 = 0.02029307745397091 + 100.0 * 6.1953125
Epoch 2970, val loss: 1.3550642728805542
Epoch 2980, training loss: 619.7428588867188 = 0.020089350640773773 + 100.0 * 6.197227954864502
Epoch 2980, val loss: 1.357832908630371
Epoch 2990, training loss: 619.7040405273438 = 0.019870806485414505 + 100.0 * 6.196841239929199
Epoch 2990, val loss: 1.359999418258667
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 861.614013671875 = 1.9300568103790283 + 100.0 * 8.596839904785156
Epoch 0, val loss: 1.9327281713485718
Epoch 10, training loss: 861.5195922851562 = 1.9233477115631104 + 100.0 * 8.595962524414062
Epoch 10, val loss: 1.925718903541565
Epoch 20, training loss: 860.9205322265625 = 1.9150984287261963 + 100.0 * 8.590054512023926
Epoch 20, val loss: 1.9169331789016724
Epoch 30, training loss: 856.6383666992188 = 1.9044452905654907 + 100.0 * 8.54733943939209
Epoch 30, val loss: 1.9054994583129883
Epoch 40, training loss: 825.7631225585938 = 1.890269160270691 + 100.0 * 8.238728523254395
Epoch 40, val loss: 1.890454888343811
Epoch 50, training loss: 741.0043334960938 = 1.873318076133728 + 100.0 * 7.391310214996338
Epoch 50, val loss: 1.8730597496032715
Epoch 60, training loss: 711.6730346679688 = 1.8609540462493896 + 100.0 * 7.09812068939209
Epoch 60, val loss: 1.860297679901123
Epoch 70, training loss: 698.1585693359375 = 1.849053978919983 + 100.0 * 6.963095188140869
Epoch 70, val loss: 1.8478515148162842
Epoch 80, training loss: 688.0785522460938 = 1.837496042251587 + 100.0 * 6.862410068511963
Epoch 80, val loss: 1.836329698562622
Epoch 90, training loss: 679.4354248046875 = 1.8277205228805542 + 100.0 * 6.7760772705078125
Epoch 90, val loss: 1.8265143632888794
Epoch 100, training loss: 672.677734375 = 1.8192657232284546 + 100.0 * 6.708584785461426
Epoch 100, val loss: 1.8177766799926758
Epoch 110, training loss: 668.2079467773438 = 1.8116321563720703 + 100.0 * 6.6639628410339355
Epoch 110, val loss: 1.8095489740371704
Epoch 120, training loss: 664.4305419921875 = 1.8040341138839722 + 100.0 * 6.626265048980713
Epoch 120, val loss: 1.801303505897522
Epoch 130, training loss: 661.3309326171875 = 1.7962766885757446 + 100.0 * 6.595346927642822
Epoch 130, val loss: 1.7929844856262207
Epoch 140, training loss: 658.6365356445312 = 1.7886401414871216 + 100.0 * 6.568479061126709
Epoch 140, val loss: 1.7849071025848389
Epoch 150, training loss: 656.1904907226562 = 1.781018853187561 + 100.0 * 6.544095039367676
Epoch 150, val loss: 1.7768621444702148
Epoch 160, training loss: 654.10498046875 = 1.7729068994522095 + 100.0 * 6.52332067489624
Epoch 160, val loss: 1.7684904336929321
Epoch 170, training loss: 652.0902099609375 = 1.7641994953155518 + 100.0 * 6.503259658813477
Epoch 170, val loss: 1.7597706317901611
Epoch 180, training loss: 650.1482543945312 = 1.7548644542694092 + 100.0 * 6.483933448791504
Epoch 180, val loss: 1.7505899667739868
Epoch 190, training loss: 648.5789184570312 = 1.7447541952133179 + 100.0 * 6.468341827392578
Epoch 190, val loss: 1.74076509475708
Epoch 200, training loss: 647.1293334960938 = 1.7337356805801392 + 100.0 * 6.45395565032959
Epoch 200, val loss: 1.7302138805389404
Epoch 210, training loss: 646.3902587890625 = 1.7216920852661133 + 100.0 * 6.446685791015625
Epoch 210, val loss: 1.7186552286148071
Epoch 220, training loss: 644.7353515625 = 1.708335041999817 + 100.0 * 6.430270195007324
Epoch 220, val loss: 1.7060818672180176
Epoch 230, training loss: 643.6556396484375 = 1.6939133405685425 + 100.0 * 6.419617176055908
Epoch 230, val loss: 1.6924865245819092
Epoch 240, training loss: 642.9161376953125 = 1.6783075332641602 + 100.0 * 6.412378787994385
Epoch 240, val loss: 1.6779109239578247
Epoch 250, training loss: 641.8535766601562 = 1.6613894701004028 + 100.0 * 6.401922225952148
Epoch 250, val loss: 1.6620279550552368
Epoch 260, training loss: 641.071533203125 = 1.6432459354400635 + 100.0 * 6.394282817840576
Epoch 260, val loss: 1.6451901197433472
Epoch 270, training loss: 640.2891235351562 = 1.6239210367202759 + 100.0 * 6.38665246963501
Epoch 270, val loss: 1.6273362636566162
Epoch 280, training loss: 640.05126953125 = 1.6033645868301392 + 100.0 * 6.384478569030762
Epoch 280, val loss: 1.6085041761398315
Epoch 290, training loss: 639.1478271484375 = 1.5816707611083984 + 100.0 * 6.375661849975586
Epoch 290, val loss: 1.5886865854263306
Epoch 300, training loss: 638.4866333007812 = 1.5590283870697021 + 100.0 * 6.36927604675293
Epoch 300, val loss: 1.5681148767471313
Epoch 310, training loss: 637.9364624023438 = 1.5356389284133911 + 100.0 * 6.364007949829102
Epoch 310, val loss: 1.5468599796295166
Epoch 320, training loss: 637.4268798828125 = 1.5114498138427734 + 100.0 * 6.359154224395752
Epoch 320, val loss: 1.525177240371704
Epoch 330, training loss: 636.9667358398438 = 1.4867185354232788 + 100.0 * 6.354800701141357
Epoch 330, val loss: 1.503046989440918
Epoch 340, training loss: 637.0036010742188 = 1.461637258529663 + 100.0 * 6.355419635772705
Epoch 340, val loss: 1.4806843996047974
Epoch 350, training loss: 636.2142944335938 = 1.4362404346466064 + 100.0 * 6.347780704498291
Epoch 350, val loss: 1.458527684211731
Epoch 360, training loss: 635.7669067382812 = 1.410760760307312 + 100.0 * 6.34356164932251
Epoch 360, val loss: 1.4364134073257446
Epoch 370, training loss: 635.6923217773438 = 1.385375738143921 + 100.0 * 6.343069553375244
Epoch 370, val loss: 1.4144471883773804
Epoch 380, training loss: 635.135498046875 = 1.359808087348938 + 100.0 * 6.337757110595703
Epoch 380, val loss: 1.392836093902588
Epoch 390, training loss: 634.6963500976562 = 1.334606647491455 + 100.0 * 6.333617687225342
Epoch 390, val loss: 1.3716014623641968
Epoch 400, training loss: 634.258056640625 = 1.3096997737884521 + 100.0 * 6.329483509063721
Epoch 400, val loss: 1.3508353233337402
Epoch 410, training loss: 634.88427734375 = 1.2851523160934448 + 100.0 * 6.335990905761719
Epoch 410, val loss: 1.330786943435669
Epoch 420, training loss: 634.0784912109375 = 1.2605011463165283 + 100.0 * 6.328179836273193
Epoch 420, val loss: 1.3106884956359863
Epoch 430, training loss: 633.3568725585938 = 1.2364864349365234 + 100.0 * 6.321203708648682
Epoch 430, val loss: 1.2913299798965454
Epoch 440, training loss: 633.080322265625 = 1.2129679918289185 + 100.0 * 6.318673610687256
Epoch 440, val loss: 1.2726398706436157
Epoch 450, training loss: 633.3199462890625 = 1.1899102926254272 + 100.0 * 6.321300506591797
Epoch 450, val loss: 1.254394769668579
Epoch 460, training loss: 632.561767578125 = 1.1670900583267212 + 100.0 * 6.313946723937988
Epoch 460, val loss: 1.23701810836792
Epoch 470, training loss: 632.3057250976562 = 1.1447919607162476 + 100.0 * 6.311609268188477
Epoch 470, val loss: 1.2199963331222534
Epoch 480, training loss: 632.1734008789062 = 1.1229784488677979 + 100.0 * 6.310503959655762
Epoch 480, val loss: 1.2033323049545288
Epoch 490, training loss: 632.0236206054688 = 1.1013953685760498 + 100.0 * 6.30922269821167
Epoch 490, val loss: 1.187345027923584
Epoch 500, training loss: 631.5469970703125 = 1.08035147190094 + 100.0 * 6.304666996002197
Epoch 500, val loss: 1.171742558479309
Epoch 510, training loss: 631.4605102539062 = 1.0597881078720093 + 100.0 * 6.304007053375244
Epoch 510, val loss: 1.1568522453308105
Epoch 520, training loss: 631.144287109375 = 1.0396738052368164 + 100.0 * 6.301046371459961
Epoch 520, val loss: 1.1423025131225586
Epoch 530, training loss: 630.9347534179688 = 1.019951581954956 + 100.0 * 6.299148082733154
Epoch 530, val loss: 1.1283682584762573
Epoch 540, training loss: 630.7352294921875 = 1.000758409500122 + 100.0 * 6.297345161437988
Epoch 540, val loss: 1.1149519681930542
Epoch 550, training loss: 631.4232177734375 = 0.9818901419639587 + 100.0 * 6.304413318634033
Epoch 550, val loss: 1.1019846200942993
Epoch 560, training loss: 630.4017333984375 = 0.9632794857025146 + 100.0 * 6.294384956359863
Epoch 560, val loss: 1.0891634225845337
Epoch 570, training loss: 630.2111206054688 = 0.9451503753662109 + 100.0 * 6.292659759521484
Epoch 570, val loss: 1.076972246170044
Epoch 580, training loss: 630.2662963867188 = 0.927436113357544 + 100.0 * 6.293388366699219
Epoch 580, val loss: 1.0651901960372925
Epoch 590, training loss: 629.8442993164062 = 0.9100536108016968 + 100.0 * 6.289342403411865
Epoch 590, val loss: 1.0541985034942627
Epoch 600, training loss: 629.8436279296875 = 0.8930119276046753 + 100.0 * 6.289506435394287
Epoch 600, val loss: 1.0431272983551025
Epoch 610, training loss: 629.5103759765625 = 0.8760886788368225 + 100.0 * 6.286342620849609
Epoch 610, val loss: 1.0326274633407593
Epoch 620, training loss: 629.4317626953125 = 0.8595834970474243 + 100.0 * 6.285722255706787
Epoch 620, val loss: 1.0221911668777466
Epoch 630, training loss: 629.2266235351562 = 0.8434134721755981 + 100.0 * 6.283832550048828
Epoch 630, val loss: 1.012547492980957
Epoch 640, training loss: 629.922119140625 = 0.8274833559989929 + 100.0 * 6.2909464836120605
Epoch 640, val loss: 1.0029834508895874
Epoch 650, training loss: 629.049560546875 = 0.811661958694458 + 100.0 * 6.282379150390625
Epoch 650, val loss: 0.9934915900230408
Epoch 660, training loss: 628.7529296875 = 0.7961869835853577 + 100.0 * 6.279567241668701
Epoch 660, val loss: 0.9846259355545044
Epoch 670, training loss: 628.62841796875 = 0.7811213731765747 + 100.0 * 6.278472900390625
Epoch 670, val loss: 0.9761574268341064
Epoch 680, training loss: 629.0950927734375 = 0.7663220763206482 + 100.0 * 6.28328800201416
Epoch 680, val loss: 0.9680038094520569
Epoch 690, training loss: 629.0042724609375 = 0.7514564394950867 + 100.0 * 6.282527923583984
Epoch 690, val loss: 0.9598733186721802
Epoch 700, training loss: 628.3939208984375 = 0.7367533445358276 + 100.0 * 6.276571750640869
Epoch 700, val loss: 0.9519824981689453
Epoch 710, training loss: 628.093017578125 = 0.7225626111030579 + 100.0 * 6.273705005645752
Epoch 710, val loss: 0.9446920156478882
Epoch 720, training loss: 627.912353515625 = 0.7086722254753113 + 100.0 * 6.272036552429199
Epoch 720, val loss: 0.9378460645675659
Epoch 730, training loss: 627.8073120117188 = 0.6949883699417114 + 100.0 * 6.271122932434082
Epoch 730, val loss: 0.9312388896942139
Epoch 740, training loss: 628.30126953125 = 0.6814925074577332 + 100.0 * 6.27619743347168
Epoch 740, val loss: 0.925018846988678
Epoch 750, training loss: 628.1024780273438 = 0.6679461598396301 + 100.0 * 6.2743449211120605
Epoch 750, val loss: 0.9185250997543335
Epoch 760, training loss: 627.5244750976562 = 0.6547753810882568 + 100.0 * 6.2686967849731445
Epoch 760, val loss: 0.912528932094574
Epoch 770, training loss: 627.4490966796875 = 0.6419481039047241 + 100.0 * 6.268071174621582
Epoch 770, val loss: 0.907106876373291
Epoch 780, training loss: 627.5543823242188 = 0.6293165683746338 + 100.0 * 6.269250392913818
Epoch 780, val loss: 0.9019367098808289
Epoch 790, training loss: 627.276611328125 = 0.616977870464325 + 100.0 * 6.266595840454102
Epoch 790, val loss: 0.8972060084342957
Epoch 800, training loss: 627.0865478515625 = 0.6048206686973572 + 100.0 * 6.264817237854004
Epoch 800, val loss: 0.892333447933197
Epoch 810, training loss: 626.9405517578125 = 0.5929874777793884 + 100.0 * 6.26347541809082
Epoch 810, val loss: 0.8881261944770813
Epoch 820, training loss: 627.463623046875 = 0.5813482999801636 + 100.0 * 6.26882266998291
Epoch 820, val loss: 0.8840731978416443
Epoch 830, training loss: 626.7413940429688 = 0.5698720812797546 + 100.0 * 6.261714935302734
Epoch 830, val loss: 0.8802053332328796
Epoch 840, training loss: 626.7810668945312 = 0.5587166547775269 + 100.0 * 6.262223720550537
Epoch 840, val loss: 0.8767088651657104
Epoch 850, training loss: 626.6165161132812 = 0.5477697253227234 + 100.0 * 6.260687351226807
Epoch 850, val loss: 0.8734912872314453
Epoch 860, training loss: 626.6926879882812 = 0.5370832681655884 + 100.0 * 6.2615556716918945
Epoch 860, val loss: 0.8703867793083191
Epoch 870, training loss: 626.4498901367188 = 0.5265891551971436 + 100.0 * 6.259232997894287
Epoch 870, val loss: 0.8678147196769714
Epoch 880, training loss: 626.5390625 = 0.5162879824638367 + 100.0 * 6.260227680206299
Epoch 880, val loss: 0.8651676774024963
Epoch 890, training loss: 626.2072143554688 = 0.5063155889511108 + 100.0 * 6.257009029388428
Epoch 890, val loss: 0.8631447553634644
Epoch 900, training loss: 626.1597290039062 = 0.49654892086982727 + 100.0 * 6.256632328033447
Epoch 900, val loss: 0.8611574172973633
Epoch 910, training loss: 626.3532104492188 = 0.4869820177555084 + 100.0 * 6.258662223815918
Epoch 910, val loss: 0.8594409823417664
Epoch 920, training loss: 626.0244140625 = 0.4775603413581848 + 100.0 * 6.255468845367432
Epoch 920, val loss: 0.8579263091087341
Epoch 930, training loss: 625.9321899414062 = 0.4683883488178253 + 100.0 * 6.254638195037842
Epoch 930, val loss: 0.8566509485244751
Epoch 940, training loss: 626.14453125 = 0.45944756269454956 + 100.0 * 6.256850719451904
Epoch 940, val loss: 0.8555665612220764
Epoch 950, training loss: 625.7391357421875 = 0.45063483715057373 + 100.0 * 6.252884864807129
Epoch 950, val loss: 0.8546491265296936
Epoch 960, training loss: 625.6880493164062 = 0.4420827031135559 + 100.0 * 6.252459526062012
Epoch 960, val loss: 0.8540224432945251
Epoch 970, training loss: 626.293212890625 = 0.4338165819644928 + 100.0 * 6.258594036102295
Epoch 970, val loss: 0.8536961078643799
Epoch 980, training loss: 625.763916015625 = 0.42535534501075745 + 100.0 * 6.253385543823242
Epoch 980, val loss: 0.853284478187561
Epoch 990, training loss: 625.5780639648438 = 0.4173884689807892 + 100.0 * 6.2516069412231445
Epoch 990, val loss: 0.8533377051353455
Epoch 1000, training loss: 625.3668823242188 = 0.40950852632522583 + 100.0 * 6.249574184417725
Epoch 1000, val loss: 0.8534907102584839
Epoch 1010, training loss: 625.3494262695312 = 0.4018590450286865 + 100.0 * 6.249475479125977
Epoch 1010, val loss: 0.8539431691169739
Epoch 1020, training loss: 625.4481201171875 = 0.3943585455417633 + 100.0 * 6.250537872314453
Epoch 1020, val loss: 0.8545319437980652
Epoch 1030, training loss: 625.2058715820312 = 0.38691195845603943 + 100.0 * 6.248189449310303
Epoch 1030, val loss: 0.8550387024879456
Epoch 1040, training loss: 625.0894165039062 = 0.37967538833618164 + 100.0 * 6.247097015380859
Epoch 1040, val loss: 0.8558054566383362
Epoch 1050, training loss: 625.0880126953125 = 0.3726474344730377 + 100.0 * 6.2471537590026855
Epoch 1050, val loss: 0.8569608926773071
Epoch 1060, training loss: 625.2483520507812 = 0.36577877402305603 + 100.0 * 6.248825550079346
Epoch 1060, val loss: 0.8581832051277161
Epoch 1070, training loss: 625.05029296875 = 0.3590124547481537 + 100.0 * 6.246912956237793
Epoch 1070, val loss: 0.859472930431366
Epoch 1080, training loss: 625.0321655273438 = 0.35239213705062866 + 100.0 * 6.246797561645508
Epoch 1080, val loss: 0.8609933257102966
Epoch 1090, training loss: 624.793212890625 = 0.34593307971954346 + 100.0 * 6.244472503662109
Epoch 1090, val loss: 0.862703800201416
Epoch 1100, training loss: 624.9046630859375 = 0.3396093249320984 + 100.0 * 6.245650768280029
Epoch 1100, val loss: 0.8645116686820984
Epoch 1110, training loss: 624.7875366210938 = 0.33337050676345825 + 100.0 * 6.244541645050049
Epoch 1110, val loss: 0.8663057088851929
Epoch 1120, training loss: 624.7465209960938 = 0.3272680342197418 + 100.0 * 6.244192600250244
Epoch 1120, val loss: 0.8684425950050354
Epoch 1130, training loss: 624.5681762695312 = 0.3213020861148834 + 100.0 * 6.24246883392334
Epoch 1130, val loss: 0.8706120252609253
Epoch 1140, training loss: 624.4547729492188 = 0.3154725730419159 + 100.0 * 6.241393089294434
Epoch 1140, val loss: 0.8729248642921448
Epoch 1150, training loss: 624.9020385742188 = 0.3098241984844208 + 100.0 * 6.245922088623047
Epoch 1150, val loss: 0.8755353689193726
Epoch 1160, training loss: 624.4943237304688 = 0.3041008710861206 + 100.0 * 6.2419023513793945
Epoch 1160, val loss: 0.8778225183486938
Epoch 1170, training loss: 624.4310913085938 = 0.2985835373401642 + 100.0 * 6.2413249015808105
Epoch 1170, val loss: 0.8804608583450317
Epoch 1180, training loss: 624.2522583007812 = 0.2932310700416565 + 100.0 * 6.239590167999268
Epoch 1180, val loss: 0.8834116458892822
Epoch 1190, training loss: 624.2377319335938 = 0.28804436326026917 + 100.0 * 6.239497184753418
Epoch 1190, val loss: 0.8864151835441589
Epoch 1200, training loss: 624.9694213867188 = 0.2829238772392273 + 100.0 * 6.246865272521973
Epoch 1200, val loss: 0.8892759084701538
Epoch 1210, training loss: 624.1849975585938 = 0.27764227986335754 + 100.0 * 6.239073753356934
Epoch 1210, val loss: 0.8920902013778687
Epoch 1220, training loss: 624.1616821289062 = 0.27259913086891174 + 100.0 * 6.238891124725342
Epoch 1220, val loss: 0.8955714106559753
Epoch 1230, training loss: 623.9669189453125 = 0.26779645681381226 + 100.0 * 6.236990928649902
Epoch 1230, val loss: 0.8990410566329956
Epoch 1240, training loss: 623.9649047851562 = 0.2630991041660309 + 100.0 * 6.23701810836792
Epoch 1240, val loss: 0.9024731516838074
Epoch 1250, training loss: 624.6516723632812 = 0.2584429681301117 + 100.0 * 6.243932247161865
Epoch 1250, val loss: 0.9059814214706421
Epoch 1260, training loss: 624.1704711914062 = 0.25381171703338623 + 100.0 * 6.239166736602783
Epoch 1260, val loss: 0.909551203250885
Epoch 1270, training loss: 624.302001953125 = 0.2492600530385971 + 100.0 * 6.240527153015137
Epoch 1270, val loss: 0.9130738973617554
Epoch 1280, training loss: 623.9525146484375 = 0.2448214888572693 + 100.0 * 6.237077236175537
Epoch 1280, val loss: 0.9169695973396301
Epoch 1290, training loss: 623.7728881835938 = 0.24049633741378784 + 100.0 * 6.235323905944824
Epoch 1290, val loss: 0.9209241271018982
Epoch 1300, training loss: 623.7639770507812 = 0.23630109429359436 + 100.0 * 6.235276222229004
Epoch 1300, val loss: 0.9248819947242737
Epoch 1310, training loss: 623.892822265625 = 0.23213441669940948 + 100.0 * 6.236607074737549
Epoch 1310, val loss: 0.9289312362670898
Epoch 1320, training loss: 624.0905151367188 = 0.22806279361248016 + 100.0 * 6.238624572753906
Epoch 1320, val loss: 0.9331126809120178
Epoch 1330, training loss: 623.6874389648438 = 0.22398902475833893 + 100.0 * 6.2346343994140625
Epoch 1330, val loss: 0.9371380805969238
Epoch 1340, training loss: 623.5216674804688 = 0.22006754577159882 + 100.0 * 6.233016014099121
Epoch 1340, val loss: 0.9414519667625427
Epoch 1350, training loss: 623.5202026367188 = 0.21621721982955933 + 100.0 * 6.233039855957031
Epoch 1350, val loss: 0.9458014369010925
Epoch 1360, training loss: 623.9680786132812 = 0.21246540546417236 + 100.0 * 6.237556457519531
Epoch 1360, val loss: 0.9500117301940918
Epoch 1370, training loss: 623.5422973632812 = 0.20869646966457367 + 100.0 * 6.233336448669434
Epoch 1370, val loss: 0.9543914794921875
Epoch 1380, training loss: 623.6837768554688 = 0.2049960196018219 + 100.0 * 6.234787464141846
Epoch 1380, val loss: 0.9587253332138062
Epoch 1390, training loss: 623.7257080078125 = 0.20137351751327515 + 100.0 * 6.235243320465088
Epoch 1390, val loss: 0.9631084203720093
Epoch 1400, training loss: 623.35791015625 = 0.19782648980617523 + 100.0 * 6.231600761413574
Epoch 1400, val loss: 0.96742182970047
Epoch 1410, training loss: 623.4095458984375 = 0.19438102841377258 + 100.0 * 6.232151508331299
Epoch 1410, val loss: 0.9720750451087952
Epoch 1420, training loss: 623.4984741210938 = 0.1909657120704651 + 100.0 * 6.233075141906738
Epoch 1420, val loss: 0.9764297008514404
Epoch 1430, training loss: 623.30908203125 = 0.18758906424045563 + 100.0 * 6.231215000152588
Epoch 1430, val loss: 0.9810948371887207
Epoch 1440, training loss: 623.1356201171875 = 0.1843045949935913 + 100.0 * 6.229513168334961
Epoch 1440, val loss: 0.9857953190803528
Epoch 1450, training loss: 623.1080322265625 = 0.18112632632255554 + 100.0 * 6.229268550872803
Epoch 1450, val loss: 0.990576982498169
Epoch 1460, training loss: 624.0751953125 = 0.17798306047916412 + 100.0 * 6.238972187042236
Epoch 1460, val loss: 0.9954147934913635
Epoch 1470, training loss: 623.431396484375 = 0.1747901439666748 + 100.0 * 6.232565879821777
Epoch 1470, val loss: 0.9994484186172485
Epoch 1480, training loss: 623.1572265625 = 0.17169156670570374 + 100.0 * 6.229855060577393
Epoch 1480, val loss: 1.0045331716537476
Epoch 1490, training loss: 623.5121459960938 = 0.16868267953395844 + 100.0 * 6.233434200286865
Epoch 1490, val loss: 1.0093426704406738
Epoch 1500, training loss: 623.0529174804688 = 0.1657191812992096 + 100.0 * 6.228871822357178
Epoch 1500, val loss: 1.0136371850967407
Epoch 1510, training loss: 622.9374389648438 = 0.1628093719482422 + 100.0 * 6.227746486663818
Epoch 1510, val loss: 1.0188335180282593
Epoch 1520, training loss: 622.9667358398438 = 0.16000407934188843 + 100.0 * 6.228066921234131
Epoch 1520, val loss: 1.023569107055664
Epoch 1530, training loss: 623.2837524414062 = 0.15722130239009857 + 100.0 * 6.231265544891357
Epoch 1530, val loss: 1.0284696817398071
Epoch 1540, training loss: 623.011474609375 = 0.15441767871379852 + 100.0 * 6.228570461273193
Epoch 1540, val loss: 1.033387303352356
Epoch 1550, training loss: 623.3126220703125 = 0.15170711278915405 + 100.0 * 6.231608867645264
Epoch 1550, val loss: 1.0381953716278076
Epoch 1560, training loss: 623.1414794921875 = 0.14905841648578644 + 100.0 * 6.229924201965332
Epoch 1560, val loss: 1.0430887937545776
Epoch 1570, training loss: 622.7210083007812 = 0.1463991403579712 + 100.0 * 6.225746154785156
Epoch 1570, val loss: 1.0478951930999756
Epoch 1580, training loss: 622.6897583007812 = 0.14386355876922607 + 100.0 * 6.225459098815918
Epoch 1580, val loss: 1.0530506372451782
Epoch 1590, training loss: 622.8282470703125 = 0.1414012759923935 + 100.0 * 6.226868152618408
Epoch 1590, val loss: 1.0579307079315186
Epoch 1600, training loss: 622.8225708007812 = 0.1389131247997284 + 100.0 * 6.226836681365967
Epoch 1600, val loss: 1.06278657913208
Epoch 1610, training loss: 622.749267578125 = 0.13645777106285095 + 100.0 * 6.226128101348877
Epoch 1610, val loss: 1.067794680595398
Epoch 1620, training loss: 622.873046875 = 0.1341000497341156 + 100.0 * 6.227389812469482
Epoch 1620, val loss: 1.0725560188293457
Epoch 1630, training loss: 622.6065063476562 = 0.1317020207643509 + 100.0 * 6.224747657775879
Epoch 1630, val loss: 1.0777745246887207
Epoch 1640, training loss: 622.679443359375 = 0.12937738001346588 + 100.0 * 6.225500583648682
Epoch 1640, val loss: 1.0827492475509644
Epoch 1650, training loss: 622.581298828125 = 0.12711846828460693 + 100.0 * 6.224541664123535
Epoch 1650, val loss: 1.0877410173416138
Epoch 1660, training loss: 622.8529663085938 = 0.12488628923892975 + 100.0 * 6.227280616760254
Epoch 1660, val loss: 1.0929007530212402
Epoch 1670, training loss: 622.5089111328125 = 0.12266853451728821 + 100.0 * 6.223862171173096
Epoch 1670, val loss: 1.0981820821762085
Epoch 1680, training loss: 622.3984985351562 = 0.1205408126115799 + 100.0 * 6.222779273986816
Epoch 1680, val loss: 1.1031485795974731
Epoch 1690, training loss: 623.052978515625 = 0.11847187578678131 + 100.0 * 6.229344844818115
Epoch 1690, val loss: 1.1081782579421997
Epoch 1700, training loss: 622.933837890625 = 0.11630963534116745 + 100.0 * 6.228175163269043
Epoch 1700, val loss: 1.1133149862289429
Epoch 1710, training loss: 622.465576171875 = 0.1142510250210762 + 100.0 * 6.223513126373291
Epoch 1710, val loss: 1.1181272268295288
Epoch 1720, training loss: 622.213134765625 = 0.11224063485860825 + 100.0 * 6.221008777618408
Epoch 1720, val loss: 1.1236956119537354
Epoch 1730, training loss: 622.2030029296875 = 0.1103113666176796 + 100.0 * 6.2209272384643555
Epoch 1730, val loss: 1.1287912130355835
Epoch 1740, training loss: 622.3194580078125 = 0.1084187924861908 + 100.0 * 6.222110748291016
Epoch 1740, val loss: 1.1339167356491089
Epoch 1750, training loss: 622.666748046875 = 0.10653148591518402 + 100.0 * 6.225602626800537
Epoch 1750, val loss: 1.1389425992965698
Epoch 1760, training loss: 622.7647094726562 = 0.1046188697218895 + 100.0 * 6.2266011238098145
Epoch 1760, val loss: 1.1440482139587402
Epoch 1770, training loss: 622.3125610351562 = 0.10275519639253616 + 100.0 * 6.222097873687744
Epoch 1770, val loss: 1.148876667022705
Epoch 1780, training loss: 622.2286987304688 = 0.10095037519931793 + 100.0 * 6.221277236938477
Epoch 1780, val loss: 1.1544283628463745
Epoch 1790, training loss: 622.1481323242188 = 0.09921065717935562 + 100.0 * 6.220489501953125
Epoch 1790, val loss: 1.1594035625457764
Epoch 1800, training loss: 622.09423828125 = 0.09750230610370636 + 100.0 * 6.219967365264893
Epoch 1800, val loss: 1.164595603942871
Epoch 1810, training loss: 622.62353515625 = 0.09584598988294601 + 100.0 * 6.225276947021484
Epoch 1810, val loss: 1.1694672107696533
Epoch 1820, training loss: 622.3231811523438 = 0.09414210915565491 + 100.0 * 6.222290515899658
Epoch 1820, val loss: 1.1740087270736694
Epoch 1830, training loss: 622.072998046875 = 0.09246529638767242 + 100.0 * 6.2198052406311035
Epoch 1830, val loss: 1.1797202825546265
Epoch 1840, training loss: 621.9647216796875 = 0.0908755511045456 + 100.0 * 6.218738555908203
Epoch 1840, val loss: 1.184698462486267
Epoch 1850, training loss: 621.9673461914062 = 0.08933067321777344 + 100.0 * 6.218780040740967
Epoch 1850, val loss: 1.1898895502090454
Epoch 1860, training loss: 622.5258178710938 = 0.08780515938997269 + 100.0 * 6.224380016326904
Epoch 1860, val loss: 1.1946218013763428
Epoch 1870, training loss: 622.1181640625 = 0.0862339586019516 + 100.0 * 6.2203192710876465
Epoch 1870, val loss: 1.199668288230896
Epoch 1880, training loss: 622.0673828125 = 0.08472885936498642 + 100.0 * 6.219826698303223
Epoch 1880, val loss: 1.2048099040985107
Epoch 1890, training loss: 622.2627563476562 = 0.0832662582397461 + 100.0 * 6.221795082092285
Epoch 1890, val loss: 1.210095763206482
Epoch 1900, training loss: 621.8995361328125 = 0.0818125307559967 + 100.0 * 6.218177318572998
Epoch 1900, val loss: 1.2147576808929443
Epoch 1910, training loss: 621.7669677734375 = 0.08041755110025406 + 100.0 * 6.216865062713623
Epoch 1910, val loss: 1.2198817729949951
Epoch 1920, training loss: 621.9247436523438 = 0.07906565815210342 + 100.0 * 6.218457221984863
Epoch 1920, val loss: 1.2247451543807983
Epoch 1930, training loss: 622.2579345703125 = 0.07771855592727661 + 100.0 * 6.221802234649658
Epoch 1930, val loss: 1.22964346408844
Epoch 1940, training loss: 621.9158935546875 = 0.07633931189775467 + 100.0 * 6.218395233154297
Epoch 1940, val loss: 1.2348603010177612
Epoch 1950, training loss: 621.7906494140625 = 0.0750434398651123 + 100.0 * 6.217155933380127
Epoch 1950, val loss: 1.2395105361938477
Epoch 1960, training loss: 621.752685546875 = 0.07376433908939362 + 100.0 * 6.2167887687683105
Epoch 1960, val loss: 1.2448992729187012
Epoch 1970, training loss: 622.1160888671875 = 0.07252378761768341 + 100.0 * 6.220436096191406
Epoch 1970, val loss: 1.249475121498108
Epoch 1980, training loss: 621.8651123046875 = 0.07127051800489426 + 100.0 * 6.21793794631958
Epoch 1980, val loss: 1.253873586654663
Epoch 1990, training loss: 621.6157836914062 = 0.07002808153629303 + 100.0 * 6.215457916259766
Epoch 1990, val loss: 1.259204626083374
Epoch 2000, training loss: 621.5809326171875 = 0.06885377317667007 + 100.0 * 6.215120792388916
Epoch 2000, val loss: 1.2640491724014282
Epoch 2010, training loss: 621.6907958984375 = 0.06770205497741699 + 100.0 * 6.216231346130371
Epoch 2010, val loss: 1.2691006660461426
Epoch 2020, training loss: 621.8873291015625 = 0.06656423956155777 + 100.0 * 6.218207359313965
Epoch 2020, val loss: 1.273805022239685
Epoch 2030, training loss: 621.7469482421875 = 0.06543567776679993 + 100.0 * 6.21681547164917
Epoch 2030, val loss: 1.2783730030059814
Epoch 2040, training loss: 621.8309936523438 = 0.06432671844959259 + 100.0 * 6.2176666259765625
Epoch 2040, val loss: 1.283203125
Epoch 2050, training loss: 621.7032470703125 = 0.06324680149555206 + 100.0 * 6.216400146484375
Epoch 2050, val loss: 1.2879818677902222
Epoch 2060, training loss: 621.5669555664062 = 0.062167733907699585 + 100.0 * 6.215047836303711
Epoch 2060, val loss: 1.2930550575256348
Epoch 2070, training loss: 621.5479125976562 = 0.061139725148677826 + 100.0 * 6.21486759185791
Epoch 2070, val loss: 1.2976819276809692
Epoch 2080, training loss: 621.860107421875 = 0.060127899050712585 + 100.0 * 6.2179999351501465
Epoch 2080, val loss: 1.3022856712341309
Epoch 2090, training loss: 621.5357666015625 = 0.059129733592271805 + 100.0 * 6.214766502380371
Epoch 2090, val loss: 1.3067727088928223
Epoch 2100, training loss: 621.495361328125 = 0.05815773829817772 + 100.0 * 6.214372158050537
Epoch 2100, val loss: 1.311391830444336
Epoch 2110, training loss: 621.7645874023438 = 0.057211898267269135 + 100.0 * 6.217073917388916
Epoch 2110, val loss: 1.3158888816833496
Epoch 2120, training loss: 621.4404296875 = 0.056236885488033295 + 100.0 * 6.213842391967773
Epoch 2120, val loss: 1.3207838535308838
Epoch 2130, training loss: 621.5071411132812 = 0.055329497903585434 + 100.0 * 6.214517593383789
Epoch 2130, val loss: 1.3251800537109375
Epoch 2140, training loss: 621.3432006835938 = 0.05441895127296448 + 100.0 * 6.212887763977051
Epoch 2140, val loss: 1.3299249410629272
Epoch 2150, training loss: 621.3543090820312 = 0.05353068932890892 + 100.0 * 6.213007926940918
Epoch 2150, val loss: 1.3346829414367676
Epoch 2160, training loss: 622.1510620117188 = 0.052707489579916 + 100.0 * 6.220983505249023
Epoch 2160, val loss: 1.3388530015945435
Epoch 2170, training loss: 621.5396118164062 = 0.051788847893476486 + 100.0 * 6.214878082275391
Epoch 2170, val loss: 1.3431323766708374
Epoch 2180, training loss: 621.2744750976562 = 0.050942327827215195 + 100.0 * 6.212234973907471
Epoch 2180, val loss: 1.3478931188583374
Epoch 2190, training loss: 621.1798706054688 = 0.05013048276305199 + 100.0 * 6.211297512054443
Epoch 2190, val loss: 1.3525545597076416
Epoch 2200, training loss: 621.2197875976562 = 0.04933900758624077 + 100.0 * 6.211704730987549
Epoch 2200, val loss: 1.357263207435608
Epoch 2210, training loss: 621.6762084960938 = 0.048563335090875626 + 100.0 * 6.216276168823242
Epoch 2210, val loss: 1.3614673614501953
Epoch 2220, training loss: 621.400634765625 = 0.04779169708490372 + 100.0 * 6.213528156280518
Epoch 2220, val loss: 1.3659257888793945
Epoch 2230, training loss: 621.33984375 = 0.04700832813978195 + 100.0 * 6.212928771972656
Epoch 2230, val loss: 1.3702586889266968
Epoch 2240, training loss: 621.2962646484375 = 0.046272676438093185 + 100.0 * 6.212500095367432
Epoch 2240, val loss: 1.3748526573181152
Epoch 2250, training loss: 621.09765625 = 0.04553694278001785 + 100.0 * 6.210521221160889
Epoch 2250, val loss: 1.3790607452392578
Epoch 2260, training loss: 621.737060546875 = 0.04484447464346886 + 100.0 * 6.216922283172607
Epoch 2260, val loss: 1.3832513093948364
Epoch 2270, training loss: 621.1868896484375 = 0.04410284385085106 + 100.0 * 6.211427688598633
Epoch 2270, val loss: 1.3878638744354248
Epoch 2280, training loss: 621.0322265625 = 0.04342569038271904 + 100.0 * 6.209888458251953
Epoch 2280, val loss: 1.392095685005188
Epoch 2290, training loss: 620.9222412109375 = 0.042758215218782425 + 100.0 * 6.208794593811035
Epoch 2290, val loss: 1.396620512008667
Epoch 2300, training loss: 621.2725219726562 = 0.04212260618805885 + 100.0 * 6.21230411529541
Epoch 2300, val loss: 1.4011132717132568
Epoch 2310, training loss: 621.0610961914062 = 0.04146170616149902 + 100.0 * 6.210196495056152
Epoch 2310, val loss: 1.4045501947402954
Epoch 2320, training loss: 621.0335693359375 = 0.04081781953573227 + 100.0 * 6.209927082061768
Epoch 2320, val loss: 1.4089279174804688
Epoch 2330, training loss: 621.1168212890625 = 0.04019215330481529 + 100.0 * 6.210765838623047
Epoch 2330, val loss: 1.412653923034668
Epoch 2340, training loss: 621.029296875 = 0.03957265242934227 + 100.0 * 6.209897518157959
Epoch 2340, val loss: 1.417128086090088
Epoch 2350, training loss: 620.9910888671875 = 0.03897770121693611 + 100.0 * 6.2095208168029785
Epoch 2350, val loss: 1.4211890697479248
Epoch 2360, training loss: 621.0678100585938 = 0.038396112620830536 + 100.0 * 6.210294246673584
Epoch 2360, val loss: 1.4254839420318604
Epoch 2370, training loss: 621.1427001953125 = 0.03782539442181587 + 100.0 * 6.211048603057861
Epoch 2370, val loss: 1.4291048049926758
Epoch 2380, training loss: 620.9168701171875 = 0.03724930062890053 + 100.0 * 6.208796501159668
Epoch 2380, val loss: 1.4329872131347656
Epoch 2390, training loss: 620.8827514648438 = 0.0367061011493206 + 100.0 * 6.208460807800293
Epoch 2390, val loss: 1.4371986389160156
Epoch 2400, training loss: 620.978271484375 = 0.03617401421070099 + 100.0 * 6.209420680999756
Epoch 2400, val loss: 1.4410706758499146
Epoch 2410, training loss: 621.0005493164062 = 0.03563641756772995 + 100.0 * 6.209649085998535
Epoch 2410, val loss: 1.4449843168258667
Epoch 2420, training loss: 621.25 = 0.03509952500462532 + 100.0 * 6.212149143218994
Epoch 2420, val loss: 1.4488911628723145
Epoch 2430, training loss: 620.91943359375 = 0.0345805324614048 + 100.0 * 6.208848476409912
Epoch 2430, val loss: 1.4522966146469116
Epoch 2440, training loss: 620.8976440429688 = 0.03407919034361839 + 100.0 * 6.2086358070373535
Epoch 2440, val loss: 1.456417202949524
Epoch 2450, training loss: 621.224853515625 = 0.0335984043776989 + 100.0 * 6.211913108825684
Epoch 2450, val loss: 1.4599183797836304
Epoch 2460, training loss: 620.854736328125 = 0.033094312995672226 + 100.0 * 6.208216667175293
Epoch 2460, val loss: 1.4639678001403809
Epoch 2470, training loss: 620.6658935546875 = 0.03262266516685486 + 100.0 * 6.206333160400391
Epoch 2470, val loss: 1.467550277709961
Epoch 2480, training loss: 620.6870727539062 = 0.03217163681983948 + 100.0 * 6.206549167633057
Epoch 2480, val loss: 1.4714869260787964
Epoch 2490, training loss: 621.048583984375 = 0.03173261880874634 + 100.0 * 6.210168361663818
Epoch 2490, val loss: 1.475075602531433
Epoch 2500, training loss: 620.7205200195312 = 0.0312717966735363 + 100.0 * 6.206892490386963
Epoch 2500, val loss: 1.4785971641540527
Epoch 2510, training loss: 620.9007568359375 = 0.030840812250971794 + 100.0 * 6.2086992263793945
Epoch 2510, val loss: 1.4822648763656616
Epoch 2520, training loss: 620.8829345703125 = 0.030398812144994736 + 100.0 * 6.208525657653809
Epoch 2520, val loss: 1.486108422279358
Epoch 2530, training loss: 620.8662109375 = 0.02996101602911949 + 100.0 * 6.208362579345703
Epoch 2530, val loss: 1.4893547296524048
Epoch 2540, training loss: 620.5634155273438 = 0.029537750408053398 + 100.0 * 6.205338478088379
Epoch 2540, val loss: 1.4927713871002197
Epoch 2550, training loss: 620.5521850585938 = 0.029135117307305336 + 100.0 * 6.205230712890625
Epoch 2550, val loss: 1.4967542886734009
Epoch 2560, training loss: 620.4808959960938 = 0.02874513529241085 + 100.0 * 6.204521179199219
Epoch 2560, val loss: 1.500245451927185
Epoch 2570, training loss: 620.6444091796875 = 0.028365379199385643 + 100.0 * 6.206160068511963
Epoch 2570, val loss: 1.503752589225769
Epoch 2580, training loss: 620.915283203125 = 0.027983205392956734 + 100.0 * 6.208873271942139
Epoch 2580, val loss: 1.5070643424987793
Epoch 2590, training loss: 620.7403564453125 = 0.02758851647377014 + 100.0 * 6.207127571105957
Epoch 2590, val loss: 1.510219693183899
Epoch 2600, training loss: 620.5166015625 = 0.027213912457227707 + 100.0 * 6.204893589019775
Epoch 2600, val loss: 1.5137851238250732
Epoch 2610, training loss: 620.4544677734375 = 0.026854263618588448 + 100.0 * 6.204276084899902
Epoch 2610, val loss: 1.5174529552459717
Epoch 2620, training loss: 621.0615844726562 = 0.026520173996686935 + 100.0 * 6.210350513458252
Epoch 2620, val loss: 1.520692229270935
Epoch 2630, training loss: 620.5130004882812 = 0.026139695197343826 + 100.0 * 6.204868793487549
Epoch 2630, val loss: 1.523732304573059
Epoch 2640, training loss: 620.3906860351562 = 0.025793520733714104 + 100.0 * 6.203648567199707
Epoch 2640, val loss: 1.5273644924163818
Epoch 2650, training loss: 620.4490356445312 = 0.02546100504696369 + 100.0 * 6.204235553741455
Epoch 2650, val loss: 1.53097665309906
Epoch 2660, training loss: 620.8743896484375 = 0.025134939700365067 + 100.0 * 6.208492279052734
Epoch 2660, val loss: 1.5342613458633423
Epoch 2670, training loss: 620.5346069335938 = 0.02480330504477024 + 100.0 * 6.2050981521606445
Epoch 2670, val loss: 1.5370649099349976
Epoch 2680, training loss: 620.7708740234375 = 0.024479197338223457 + 100.0 * 6.20746374130249
Epoch 2680, val loss: 1.5405150651931763
Epoch 2690, training loss: 620.3946533203125 = 0.024161910638213158 + 100.0 * 6.203704833984375
Epoch 2690, val loss: 1.543059229850769
Epoch 2700, training loss: 620.3082275390625 = 0.023848379030823708 + 100.0 * 6.20284366607666
Epoch 2700, val loss: 1.5467314720153809
Epoch 2710, training loss: 620.3496704101562 = 0.02355251833796501 + 100.0 * 6.203261375427246
Epoch 2710, val loss: 1.5502294301986694
Epoch 2720, training loss: 621.0724487304688 = 0.023270199075341225 + 100.0 * 6.21049165725708
Epoch 2720, val loss: 1.5533140897750854
Epoch 2730, training loss: 620.5274047851562 = 0.022956207394599915 + 100.0 * 6.205044746398926
Epoch 2730, val loss: 1.555320143699646
Epoch 2740, training loss: 620.33740234375 = 0.02266419678926468 + 100.0 * 6.2031474113464355
Epoch 2740, val loss: 1.5592248439788818
Epoch 2750, training loss: 620.6083374023438 = 0.022390544414520264 + 100.0 * 6.205859661102295
Epoch 2750, val loss: 1.5621310472488403
Epoch 2760, training loss: 620.3652954101562 = 0.022109050303697586 + 100.0 * 6.203432083129883
Epoch 2760, val loss: 1.5647838115692139
Epoch 2770, training loss: 620.3262329101562 = 0.021830296143889427 + 100.0 * 6.2030439376831055
Epoch 2770, val loss: 1.5677082538604736
Epoch 2780, training loss: 620.7916870117188 = 0.02156633324921131 + 100.0 * 6.207701683044434
Epoch 2780, val loss: 1.5711627006530762
Epoch 2790, training loss: 620.3297729492188 = 0.021291369572281837 + 100.0 * 6.203084468841553
Epoch 2790, val loss: 1.5739936828613281
Epoch 2800, training loss: 620.2200317382812 = 0.02102847211062908 + 100.0 * 6.201989650726318
Epoch 2800, val loss: 1.5768104791641235
Epoch 2810, training loss: 620.1620483398438 = 0.020778564736247063 + 100.0 * 6.201412677764893
Epoch 2810, val loss: 1.5800094604492188
Epoch 2820, training loss: 620.2474365234375 = 0.02053803578019142 + 100.0 * 6.202269554138184
Epoch 2820, val loss: 1.583160161972046
Epoch 2830, training loss: 620.6572265625 = 0.020298995077610016 + 100.0 * 6.206368923187256
Epoch 2830, val loss: 1.5858771800994873
Epoch 2840, training loss: 620.2696533203125 = 0.02004525065422058 + 100.0 * 6.20249605178833
Epoch 2840, val loss: 1.5883086919784546
Epoch 2850, training loss: 620.3641967773438 = 0.019813096150755882 + 100.0 * 6.20344352722168
Epoch 2850, val loss: 1.591607689857483
Epoch 2860, training loss: 620.2240600585938 = 0.019578734412789345 + 100.0 * 6.202044486999512
Epoch 2860, val loss: 1.5941227674484253
Epoch 2870, training loss: 620.205078125 = 0.019355297088623047 + 100.0 * 6.201857089996338
Epoch 2870, val loss: 1.596709966659546
Epoch 2880, training loss: 620.1991577148438 = 0.019130360335111618 + 100.0 * 6.20180082321167
Epoch 2880, val loss: 1.5998060703277588
Epoch 2890, training loss: 620.2711181640625 = 0.01891179569065571 + 100.0 * 6.202521800994873
Epoch 2890, val loss: 1.6025516986846924
Epoch 2900, training loss: 620.2545166015625 = 0.018693413585424423 + 100.0 * 6.202358245849609
Epoch 2900, val loss: 1.6053777933120728
Epoch 2910, training loss: 620.773193359375 = 0.01848936453461647 + 100.0 * 6.207546710968018
Epoch 2910, val loss: 1.6081827878952026
Epoch 2920, training loss: 620.3450317382812 = 0.01826321892440319 + 100.0 * 6.203267574310303
Epoch 2920, val loss: 1.6099449396133423
Epoch 2930, training loss: 620.091552734375 = 0.018045777454972267 + 100.0 * 6.200734615325928
Epoch 2930, val loss: 1.613092064857483
Epoch 2940, training loss: 620.0318603515625 = 0.017848815768957138 + 100.0 * 6.200140476226807
Epoch 2940, val loss: 1.6158496141433716
Epoch 2950, training loss: 620.0032958984375 = 0.01765739731490612 + 100.0 * 6.199856281280518
Epoch 2950, val loss: 1.618808627128601
Epoch 2960, training loss: 620.3549194335938 = 0.01747705228626728 + 100.0 * 6.20337438583374
Epoch 2960, val loss: 1.6213477849960327
Epoch 2970, training loss: 620.1046752929688 = 0.01727316342294216 + 100.0 * 6.200873851776123
Epoch 2970, val loss: 1.6238195896148682
Epoch 2980, training loss: 620.3678588867188 = 0.0170778539031744 + 100.0 * 6.203507900238037
Epoch 2980, val loss: 1.6264290809631348
Epoch 2990, training loss: 620.33056640625 = 0.016882970929145813 + 100.0 * 6.203136444091797
Epoch 2990, val loss: 1.6288801431655884
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8371112282551397
The final CL Acc:0.72469, 0.01720, The final GNN Acc:0.84045, 0.00334
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10574])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6178588867188 = 1.9329100847244263 + 100.0 * 8.59684944152832
Epoch 0, val loss: 1.9257017374038696
Epoch 10, training loss: 861.5332641601562 = 1.9246540069580078 + 100.0 * 8.596085548400879
Epoch 10, val loss: 1.9170209169387817
Epoch 20, training loss: 860.9756469726562 = 1.914402723312378 + 100.0 * 8.590612411499023
Epoch 20, val loss: 1.9063178300857544
Epoch 30, training loss: 857.3123779296875 = 1.9012588262557983 + 100.0 * 8.55411148071289
Epoch 30, val loss: 1.892637014389038
Epoch 40, training loss: 836.5972290039062 = 1.8851581811904907 + 100.0 * 8.347121238708496
Epoch 40, val loss: 1.8762986660003662
Epoch 50, training loss: 773.2904052734375 = 1.8650436401367188 + 100.0 * 7.7142534255981445
Epoch 50, val loss: 1.8562486171722412
Epoch 60, training loss: 731.2805786132812 = 1.8516749143600464 + 100.0 * 7.2942891120910645
Epoch 60, val loss: 1.8444623947143555
Epoch 70, training loss: 706.9325561523438 = 1.8419201374053955 + 100.0 * 7.050906658172607
Epoch 70, val loss: 1.8353464603424072
Epoch 80, training loss: 694.33154296875 = 1.8332914113998413 + 100.0 * 6.92498254776001
Epoch 80, val loss: 1.8272606134414673
Epoch 90, training loss: 686.6596069335938 = 1.8241422176361084 + 100.0 * 6.848354339599609
Epoch 90, val loss: 1.8187886476516724
Epoch 100, training loss: 680.7360229492188 = 1.8151179552078247 + 100.0 * 6.789208889007568
Epoch 100, val loss: 1.8106904029846191
Epoch 110, training loss: 675.3753051757812 = 1.8066948652267456 + 100.0 * 6.735686302185059
Epoch 110, val loss: 1.8031890392303467
Epoch 120, training loss: 670.503662109375 = 1.7993658781051636 + 100.0 * 6.687042713165283
Epoch 120, val loss: 1.7965314388275146
Epoch 130, training loss: 666.7721557617188 = 1.792129635810852 + 100.0 * 6.6498003005981445
Epoch 130, val loss: 1.7899119853973389
Epoch 140, training loss: 663.6472778320312 = 1.784335970878601 + 100.0 * 6.618628978729248
Epoch 140, val loss: 1.7827421426773071
Epoch 150, training loss: 661.1376953125 = 1.7759732007980347 + 100.0 * 6.593616962432861
Epoch 150, val loss: 1.775222659111023
Epoch 160, training loss: 659.32373046875 = 1.7671880722045898 + 100.0 * 6.575565338134766
Epoch 160, val loss: 1.7674293518066406
Epoch 170, training loss: 656.8917236328125 = 1.7579245567321777 + 100.0 * 6.551337718963623
Epoch 170, val loss: 1.7591607570648193
Epoch 180, training loss: 654.9554443359375 = 1.748145580291748 + 100.0 * 6.532073020935059
Epoch 180, val loss: 1.750542402267456
Epoch 190, training loss: 653.5914916992188 = 1.7376919984817505 + 100.0 * 6.518537998199463
Epoch 190, val loss: 1.7413820028305054
Epoch 200, training loss: 651.78955078125 = 1.7262119054794312 + 100.0 * 6.500633239746094
Epoch 200, val loss: 1.7315125465393066
Epoch 210, training loss: 650.5076293945312 = 1.7137795686721802 + 100.0 * 6.487937927246094
Epoch 210, val loss: 1.7208366394042969
Epoch 220, training loss: 649.0017700195312 = 1.7002341747283936 + 100.0 * 6.473015308380127
Epoch 220, val loss: 1.7091565132141113
Epoch 230, training loss: 647.8449096679688 = 1.6855818033218384 + 100.0 * 6.461593151092529
Epoch 230, val loss: 1.696568250656128
Epoch 240, training loss: 646.9607543945312 = 1.6697590351104736 + 100.0 * 6.452910423278809
Epoch 240, val loss: 1.6830682754516602
Epoch 250, training loss: 645.8566284179688 = 1.6527810096740723 + 100.0 * 6.442038536071777
Epoch 250, val loss: 1.6685423851013184
Epoch 260, training loss: 644.9545288085938 = 1.6345330476760864 + 100.0 * 6.433199882507324
Epoch 260, val loss: 1.653105616569519
Epoch 270, training loss: 644.1594848632812 = 1.6151212453842163 + 100.0 * 6.425443649291992
Epoch 270, val loss: 1.6366850137710571
Epoch 280, training loss: 643.6517333984375 = 1.5944792032241821 + 100.0 * 6.420572280883789
Epoch 280, val loss: 1.6193914413452148
Epoch 290, training loss: 642.91650390625 = 1.5727123022079468 + 100.0 * 6.413438320159912
Epoch 290, val loss: 1.6012225151062012
Epoch 300, training loss: 642.2014770507812 = 1.5499743223190308 + 100.0 * 6.406515121459961
Epoch 300, val loss: 1.5825115442276
Epoch 310, training loss: 641.5052490234375 = 1.5263540744781494 + 100.0 * 6.399788856506348
Epoch 310, val loss: 1.5632588863372803
Epoch 320, training loss: 641.2908325195312 = 1.501918077468872 + 100.0 * 6.397889137268066
Epoch 320, val loss: 1.5435717105865479
Epoch 330, training loss: 640.76904296875 = 1.4767261743545532 + 100.0 * 6.392922878265381
Epoch 330, val loss: 1.5233373641967773
Epoch 340, training loss: 639.8951416015625 = 1.4509488344192505 + 100.0 * 6.384442329406738
Epoch 340, val loss: 1.5030081272125244
Epoch 350, training loss: 639.3115844726562 = 1.4248405694961548 + 100.0 * 6.378867149353027
Epoch 350, val loss: 1.4826916456222534
Epoch 360, training loss: 638.8248901367188 = 1.398451328277588 + 100.0 * 6.374264240264893
Epoch 360, val loss: 1.4624438285827637
Epoch 370, training loss: 638.7852172851562 = 1.3718657493591309 + 100.0 * 6.374133110046387
Epoch 370, val loss: 1.4423906803131104
Epoch 380, training loss: 638.335205078125 = 1.344956398010254 + 100.0 * 6.36990213394165
Epoch 380, val loss: 1.4224058389663696
Epoch 390, training loss: 637.5571899414062 = 1.3181145191192627 + 100.0 * 6.362390995025635
Epoch 390, val loss: 1.4026362895965576
Epoch 400, training loss: 637.1322021484375 = 1.291463017463684 + 100.0 * 6.358407497406006
Epoch 400, val loss: 1.3835265636444092
Epoch 410, training loss: 636.7564086914062 = 1.2651082277297974 + 100.0 * 6.354912757873535
Epoch 410, val loss: 1.3648908138275146
Epoch 420, training loss: 636.52490234375 = 1.2390228509902954 + 100.0 * 6.352859020233154
Epoch 420, val loss: 1.3469045162200928
Epoch 430, training loss: 636.299560546875 = 1.21327805519104 + 100.0 * 6.350862979888916
Epoch 430, val loss: 1.3293756246566772
Epoch 440, training loss: 635.6777954101562 = 1.1880314350128174 + 100.0 * 6.344897270202637
Epoch 440, val loss: 1.312808871269226
Epoch 450, training loss: 635.5984497070312 = 1.1634564399719238 + 100.0 * 6.3443498611450195
Epoch 450, val loss: 1.2971985340118408
Epoch 460, training loss: 635.353515625 = 1.1395565271377563 + 100.0 * 6.342140197753906
Epoch 460, val loss: 1.282122254371643
Epoch 470, training loss: 634.8203735351562 = 1.1161518096923828 + 100.0 * 6.337042331695557
Epoch 470, val loss: 1.268078327178955
Epoch 480, training loss: 634.5017700195312 = 1.0936111211776733 + 100.0 * 6.334081649780273
Epoch 480, val loss: 1.254718542098999
Epoch 490, training loss: 634.214111328125 = 1.0717592239379883 + 100.0 * 6.331423282623291
Epoch 490, val loss: 1.242465615272522
Epoch 500, training loss: 634.130859375 = 1.0505162477493286 + 100.0 * 6.330803394317627
Epoch 500, val loss: 1.2306090593338013
Epoch 510, training loss: 633.5826416015625 = 1.0298885107040405 + 100.0 * 6.325527191162109
Epoch 510, val loss: 1.2197705507278442
Epoch 520, training loss: 633.3185424804688 = 1.0100620985031128 + 100.0 * 6.323084831237793
Epoch 520, val loss: 1.209879994392395
Epoch 530, training loss: 633.6378784179688 = 0.9908673167228699 + 100.0 * 6.326470375061035
Epoch 530, val loss: 1.200693964958191
Epoch 540, training loss: 633.2848510742188 = 0.9720959067344666 + 100.0 * 6.323127269744873
Epoch 540, val loss: 1.1920784711837769
Epoch 550, training loss: 632.766845703125 = 0.9539312124252319 + 100.0 * 6.318129062652588
Epoch 550, val loss: 1.184104084968567
Epoch 560, training loss: 632.4833984375 = 0.9363820552825928 + 100.0 * 6.315469741821289
Epoch 560, val loss: 1.1768325567245483
Epoch 570, training loss: 632.6105346679688 = 0.9193634986877441 + 100.0 * 6.316911697387695
Epoch 570, val loss: 1.1701384782791138
Epoch 580, training loss: 632.377685546875 = 0.9025189876556396 + 100.0 * 6.314751625061035
Epoch 580, val loss: 1.164236307144165
Epoch 590, training loss: 632.037841796875 = 0.8862029314041138 + 100.0 * 6.311516284942627
Epoch 590, val loss: 1.1583667993545532
Epoch 600, training loss: 631.689208984375 = 0.8702320456504822 + 100.0 * 6.308189868927002
Epoch 600, val loss: 1.1530523300170898
Epoch 610, training loss: 631.3888549804688 = 0.8546506762504578 + 100.0 * 6.305341720581055
Epoch 610, val loss: 1.1483523845672607
Epoch 620, training loss: 631.1868896484375 = 0.8394742608070374 + 100.0 * 6.303474426269531
Epoch 620, val loss: 1.1440335512161255
Epoch 630, training loss: 631.3981323242188 = 0.8246168494224548 + 100.0 * 6.305735111236572
Epoch 630, val loss: 1.1402380466461182
Epoch 640, training loss: 631.7159423828125 = 0.8097893595695496 + 100.0 * 6.309061527252197
Epoch 640, val loss: 1.135806918144226
Epoch 650, training loss: 630.8579711914062 = 0.7951955795288086 + 100.0 * 6.300628185272217
Epoch 650, val loss: 1.1323999166488647
Epoch 660, training loss: 630.532958984375 = 0.7810288667678833 + 100.0 * 6.297519683837891
Epoch 660, val loss: 1.12912118434906
Epoch 670, training loss: 630.3519287109375 = 0.7671525478363037 + 100.0 * 6.2958478927612305
Epoch 670, val loss: 1.1261714696884155
Epoch 680, training loss: 630.7504272460938 = 0.7534719705581665 + 100.0 * 6.29996919631958
Epoch 680, val loss: 1.1236441135406494
Epoch 690, training loss: 630.420654296875 = 0.7398634552955627 + 100.0 * 6.296807765960693
Epoch 690, val loss: 1.12052321434021
Epoch 700, training loss: 630.0166015625 = 0.7264230251312256 + 100.0 * 6.292901515960693
Epoch 700, val loss: 1.117895483970642
Epoch 710, training loss: 629.8092041015625 = 0.7132498025894165 + 100.0 * 6.290959358215332
Epoch 710, val loss: 1.1158108711242676
Epoch 720, training loss: 629.9534912109375 = 0.7001940011978149 + 100.0 * 6.292532920837402
Epoch 720, val loss: 1.1136888265609741
Epoch 730, training loss: 629.4736938476562 = 0.6872298121452332 + 100.0 * 6.2878642082214355
Epoch 730, val loss: 1.111647367477417
Epoch 740, training loss: 629.6799926757812 = 0.6744892597198486 + 100.0 * 6.290054798126221
Epoch 740, val loss: 1.1098195314407349
Epoch 750, training loss: 629.5232543945312 = 0.6618757247924805 + 100.0 * 6.288613796234131
Epoch 750, val loss: 1.107719898223877
Epoch 760, training loss: 629.31005859375 = 0.649327278137207 + 100.0 * 6.286607265472412
Epoch 760, val loss: 1.1063919067382812
Epoch 770, training loss: 629.2990112304688 = 0.6370401382446289 + 100.0 * 6.286620140075684
Epoch 770, val loss: 1.1048529148101807
Epoch 780, training loss: 629.0043334960938 = 0.6248083114624023 + 100.0 * 6.283795356750488
Epoch 780, val loss: 1.1035394668579102
Epoch 790, training loss: 628.7422485351562 = 0.6127996444702148 + 100.0 * 6.281294822692871
Epoch 790, val loss: 1.1022233963012695
Epoch 800, training loss: 629.173095703125 = 0.601002037525177 + 100.0 * 6.2857208251953125
Epoch 800, val loss: 1.1011513471603394
Epoch 810, training loss: 628.8999633789062 = 0.5890251994132996 + 100.0 * 6.283109188079834
Epoch 810, val loss: 1.0998363494873047
Epoch 820, training loss: 628.4785766601562 = 0.5773422122001648 + 100.0 * 6.279012680053711
Epoch 820, val loss: 1.0987662076950073
Epoch 830, training loss: 628.4130249023438 = 0.5658436417579651 + 100.0 * 6.27847146987915
Epoch 830, val loss: 1.0979259014129639
Epoch 840, training loss: 628.4164428710938 = 0.5544580221176147 + 100.0 * 6.27862024307251
Epoch 840, val loss: 1.0970805883407593
Epoch 850, training loss: 628.06884765625 = 0.5432112812995911 + 100.0 * 6.275256156921387
Epoch 850, val loss: 1.0964587926864624
Epoch 860, training loss: 628.1443481445312 = 0.532127857208252 + 100.0 * 6.276122570037842
Epoch 860, val loss: 1.0958648920059204
Epoch 870, training loss: 628.6954345703125 = 0.5211288928985596 + 100.0 * 6.281743049621582
Epoch 870, val loss: 1.0950641632080078
Epoch 880, training loss: 627.8056640625 = 0.5101286172866821 + 100.0 * 6.272955417633057
Epoch 880, val loss: 1.094944953918457
Epoch 890, training loss: 627.7611694335938 = 0.49940645694732666 + 100.0 * 6.272617816925049
Epoch 890, val loss: 1.0945954322814941
Epoch 900, training loss: 628.010986328125 = 0.48886361718177795 + 100.0 * 6.27522087097168
Epoch 900, val loss: 1.094496726989746
Epoch 910, training loss: 627.8338012695312 = 0.478413850069046 + 100.0 * 6.27355432510376
Epoch 910, val loss: 1.0942175388336182
Epoch 920, training loss: 627.52587890625 = 0.46804025769233704 + 100.0 * 6.270578861236572
Epoch 920, val loss: 1.0943350791931152
Epoch 930, training loss: 627.3401489257812 = 0.4579012989997864 + 100.0 * 6.26882266998291
Epoch 930, val loss: 1.0946693420410156
Epoch 940, training loss: 627.2291870117188 = 0.4478873908519745 + 100.0 * 6.267813205718994
Epoch 940, val loss: 1.0952231884002686
Epoch 950, training loss: 627.5511474609375 = 0.4380367696285248 + 100.0 * 6.2711310386657715
Epoch 950, val loss: 1.0956146717071533
Epoch 960, training loss: 627.4990234375 = 0.42820996046066284 + 100.0 * 6.270708084106445
Epoch 960, val loss: 1.0961682796478271
Epoch 970, training loss: 627.177978515625 = 0.41847050189971924 + 100.0 * 6.267595291137695
Epoch 970, val loss: 1.0972083806991577
Epoch 980, training loss: 627.44091796875 = 0.40896087884902954 + 100.0 * 6.270319938659668
Epoch 980, val loss: 1.0982656478881836
Epoch 990, training loss: 626.9107666015625 = 0.39957788586616516 + 100.0 * 6.265111923217773
Epoch 990, val loss: 1.099082112312317
Epoch 1000, training loss: 626.6864013671875 = 0.3903842866420746 + 100.0 * 6.262960433959961
Epoch 1000, val loss: 1.1005741357803345
Epoch 1010, training loss: 626.6830444335938 = 0.38133740425109863 + 100.0 * 6.263016700744629
Epoch 1010, val loss: 1.102233648300171
Epoch 1020, training loss: 626.9295043945312 = 0.3724135160446167 + 100.0 * 6.265570640563965
Epoch 1020, val loss: 1.1038926839828491
Epoch 1030, training loss: 626.5516967773438 = 0.36354121565818787 + 100.0 * 6.2618818283081055
Epoch 1030, val loss: 1.1054153442382812
Epoch 1040, training loss: 626.5795288085938 = 0.3548634946346283 + 100.0 * 6.262246608734131
Epoch 1040, val loss: 1.1077165603637695
Epoch 1050, training loss: 626.479736328125 = 0.3463163673877716 + 100.0 * 6.261334419250488
Epoch 1050, val loss: 1.1097359657287598
Epoch 1060, training loss: 626.4172973632812 = 0.337897390127182 + 100.0 * 6.260794162750244
Epoch 1060, val loss: 1.1121059656143188
Epoch 1070, training loss: 626.4661254882812 = 0.32963892817497253 + 100.0 * 6.261364459991455
Epoch 1070, val loss: 1.1146775484085083
Epoch 1080, training loss: 626.1864624023438 = 0.32157716155052185 + 100.0 * 6.258648872375488
Epoch 1080, val loss: 1.117139220237732
Epoch 1090, training loss: 626.1405029296875 = 0.3136402666568756 + 100.0 * 6.258268356323242
Epoch 1090, val loss: 1.1201348304748535
Epoch 1100, training loss: 626.3530883789062 = 0.3058099150657654 + 100.0 * 6.260473251342773
Epoch 1100, val loss: 1.1229288578033447
Epoch 1110, training loss: 626.0498657226562 = 0.2981368601322174 + 100.0 * 6.257517337799072
Epoch 1110, val loss: 1.125902533531189
Epoch 1120, training loss: 626.1919555664062 = 0.29063621163368225 + 100.0 * 6.2590131759643555
Epoch 1120, val loss: 1.129194736480713
Epoch 1130, training loss: 625.7916870117188 = 0.28327810764312744 + 100.0 * 6.255084037780762
Epoch 1130, val loss: 1.1325141191482544
Epoch 1140, training loss: 625.8187866210938 = 0.2761036157608032 + 100.0 * 6.255426406860352
Epoch 1140, val loss: 1.1364887952804565
Epoch 1150, training loss: 625.8719482421875 = 0.269100159406662 + 100.0 * 6.256028652191162
Epoch 1150, val loss: 1.1398869752883911
Epoch 1160, training loss: 625.743408203125 = 0.2621927857398987 + 100.0 * 6.254812240600586
Epoch 1160, val loss: 1.143746018409729
Epoch 1170, training loss: 625.912841796875 = 0.25547048449516296 + 100.0 * 6.25657320022583
Epoch 1170, val loss: 1.1479229927062988
Epoch 1180, training loss: 625.765625 = 0.24887274205684662 + 100.0 * 6.255167007446289
Epoch 1180, val loss: 1.1513793468475342
Epoch 1190, training loss: 625.7852172851562 = 0.24243493378162384 + 100.0 * 6.255427837371826
Epoch 1190, val loss: 1.1558610200881958
Epoch 1200, training loss: 625.3637084960938 = 0.23612748086452484 + 100.0 * 6.251275539398193
Epoch 1200, val loss: 1.1602058410644531
Epoch 1210, training loss: 625.2733764648438 = 0.230028435587883 + 100.0 * 6.250433444976807
Epoch 1210, val loss: 1.164427638053894
Epoch 1220, training loss: 625.21923828125 = 0.22413226962089539 + 100.0 * 6.249950885772705
Epoch 1220, val loss: 1.1691356897354126
Epoch 1230, training loss: 625.8435668945312 = 0.21836262941360474 + 100.0 * 6.256251811981201
Epoch 1230, val loss: 1.173576831817627
Epoch 1240, training loss: 625.4453125 = 0.2126995474100113 + 100.0 * 6.252326011657715
Epoch 1240, val loss: 1.1783103942871094
Epoch 1250, training loss: 625.3329467773438 = 0.20718221366405487 + 100.0 * 6.25125789642334
Epoch 1250, val loss: 1.183019757270813
Epoch 1260, training loss: 625.0125732421875 = 0.20178242027759552 + 100.0 * 6.24810791015625
Epoch 1260, val loss: 1.1879607439041138
Epoch 1270, training loss: 625.360595703125 = 0.19660086929798126 + 100.0 * 6.2516398429870605
Epoch 1270, val loss: 1.1934020519256592
Epoch 1280, training loss: 625.0782470703125 = 0.19147133827209473 + 100.0 * 6.248867511749268
Epoch 1280, val loss: 1.1975305080413818
Epoch 1290, training loss: 624.9020385742188 = 0.18650689721107483 + 100.0 * 6.24715518951416
Epoch 1290, val loss: 1.2030946016311646
Epoch 1300, training loss: 624.7957763671875 = 0.1817476898431778 + 100.0 * 6.246140003204346
Epoch 1300, val loss: 1.2079945802688599
Epoch 1310, training loss: 624.700927734375 = 0.17712877690792084 + 100.0 * 6.245238304138184
Epoch 1310, val loss: 1.2134613990783691
Epoch 1320, training loss: 625.22802734375 = 0.17267994582653046 + 100.0 * 6.250553607940674
Epoch 1320, val loss: 1.2186256647109985
Epoch 1330, training loss: 624.9007568359375 = 0.16821014881134033 + 100.0 * 6.247325420379639
Epoch 1330, val loss: 1.2235431671142578
Epoch 1340, training loss: 624.8224487304688 = 0.16391688585281372 + 100.0 * 6.246585369110107
Epoch 1340, val loss: 1.2289624214172363
Epoch 1350, training loss: 624.8632202148438 = 0.15978111326694489 + 100.0 * 6.247034072875977
Epoch 1350, val loss: 1.2341417074203491
Epoch 1360, training loss: 624.5599975585938 = 0.15573401749134064 + 100.0 * 6.24404239654541
Epoch 1360, val loss: 1.2400490045547485
Epoch 1370, training loss: 624.5099487304688 = 0.15183518826961517 + 100.0 * 6.2435808181762695
Epoch 1370, val loss: 1.2452924251556396
Epoch 1380, training loss: 624.7567749023438 = 0.1480734795331955 + 100.0 * 6.246087074279785
Epoch 1380, val loss: 1.2505929470062256
Epoch 1390, training loss: 624.7994384765625 = 0.14434827864170074 + 100.0 * 6.246551036834717
Epoch 1390, val loss: 1.2564150094985962
Epoch 1400, training loss: 624.3967895507812 = 0.14071959257125854 + 100.0 * 6.242560386657715
Epoch 1400, val loss: 1.2616469860076904
Epoch 1410, training loss: 624.382080078125 = 0.13725021481513977 + 100.0 * 6.242448329925537
Epoch 1410, val loss: 1.2674939632415771
Epoch 1420, training loss: 624.592041015625 = 0.13387927412986755 + 100.0 * 6.244581699371338
Epoch 1420, val loss: 1.2729899883270264
Epoch 1430, training loss: 624.4024047851562 = 0.13057832419872284 + 100.0 * 6.24271821975708
Epoch 1430, val loss: 1.2787127494812012
Epoch 1440, training loss: 624.25439453125 = 0.12738032639026642 + 100.0 * 6.241270065307617
Epoch 1440, val loss: 1.2842541933059692
Epoch 1450, training loss: 624.1174926757812 = 0.12429743260145187 + 100.0 * 6.239932537078857
Epoch 1450, val loss: 1.2900713682174683
Epoch 1460, training loss: 624.2189331054688 = 0.1213093176484108 + 100.0 * 6.240975856781006
Epoch 1460, val loss: 1.2958189249038696
Epoch 1470, training loss: 624.6351928710938 = 0.11841847002506256 + 100.0 * 6.2451677322387695
Epoch 1470, val loss: 1.301332712173462
Epoch 1480, training loss: 624.2935791015625 = 0.11550068110227585 + 100.0 * 6.241780757904053
Epoch 1480, val loss: 1.3077846765518188
Epoch 1490, training loss: 624.0584106445312 = 0.11273684352636337 + 100.0 * 6.239456653594971
Epoch 1490, val loss: 1.3129980564117432
Epoch 1500, training loss: 624.3760375976562 = 0.1100790798664093 + 100.0 * 6.242659091949463
Epoch 1500, val loss: 1.3187494277954102
Epoch 1510, training loss: 623.8887329101562 = 0.10745153576135635 + 100.0 * 6.2378129959106445
Epoch 1510, val loss: 1.3248323202133179
Epoch 1520, training loss: 623.85205078125 = 0.1049191877245903 + 100.0 * 6.237471103668213
Epoch 1520, val loss: 1.3302925825119019
Epoch 1530, training loss: 624.0174560546875 = 0.10250342637300491 + 100.0 * 6.239149570465088
Epoch 1530, val loss: 1.3361669778823853
Epoch 1540, training loss: 623.897705078125 = 0.1001194566488266 + 100.0 * 6.23797607421875
Epoch 1540, val loss: 1.3419216871261597
Epoch 1550, training loss: 623.8320922851562 = 0.09780009835958481 + 100.0 * 6.2373433113098145
Epoch 1550, val loss: 1.347687005996704
Epoch 1560, training loss: 624.2528686523438 = 0.09557060897350311 + 100.0 * 6.241572856903076
Epoch 1560, val loss: 1.3532780408859253
Epoch 1570, training loss: 624.0979614257812 = 0.0933423861861229 + 100.0 * 6.240046501159668
Epoch 1570, val loss: 1.3588342666625977
Epoch 1580, training loss: 623.6895141601562 = 0.09119734168052673 + 100.0 * 6.235983371734619
Epoch 1580, val loss: 1.3644684553146362
Epoch 1590, training loss: 623.5890502929688 = 0.08914094418287277 + 100.0 * 6.234999656677246
Epoch 1590, val loss: 1.3699226379394531
Epoch 1600, training loss: 623.7161865234375 = 0.0871710255742073 + 100.0 * 6.236289978027344
Epoch 1600, val loss: 1.3758876323699951
Epoch 1610, training loss: 623.7240600585938 = 0.08521895855665207 + 100.0 * 6.236388683319092
Epoch 1610, val loss: 1.381218671798706
Epoch 1620, training loss: 623.9222412109375 = 0.0833122655749321 + 100.0 * 6.238389492034912
Epoch 1620, val loss: 1.386803150177002
Epoch 1630, training loss: 623.6301879882812 = 0.08146226406097412 + 100.0 * 6.23548698425293
Epoch 1630, val loss: 1.3923453092575073
Epoch 1640, training loss: 623.6326293945312 = 0.07968181371688843 + 100.0 * 6.235528945922852
Epoch 1640, val loss: 1.3975112438201904
Epoch 1650, training loss: 623.6849365234375 = 0.07794785499572754 + 100.0 * 6.236069679260254
Epoch 1650, val loss: 1.4033077955245972
Epoch 1660, training loss: 623.3831787109375 = 0.0762374997138977 + 100.0 * 6.23306941986084
Epoch 1660, val loss: 1.408347487449646
Epoch 1670, training loss: 623.4391479492188 = 0.0746045932173729 + 100.0 * 6.233645915985107
Epoch 1670, val loss: 1.4137603044509888
Epoch 1680, training loss: 623.5474243164062 = 0.07301313430070877 + 100.0 * 6.234744071960449
Epoch 1680, val loss: 1.4193191528320312
Epoch 1690, training loss: 623.3652954101562 = 0.07144717127084732 + 100.0 * 6.232938289642334
Epoch 1690, val loss: 1.4245150089263916
Epoch 1700, training loss: 623.46484375 = 0.06993363052606583 + 100.0 * 6.233948707580566
Epoch 1700, val loss: 1.429269790649414
Epoch 1710, training loss: 623.509521484375 = 0.0684351846575737 + 100.0 * 6.234410762786865
Epoch 1710, val loss: 1.4351388216018677
Epoch 1720, training loss: 623.2116088867188 = 0.06695926189422607 + 100.0 * 6.231446743011475
Epoch 1720, val loss: 1.4397470951080322
Epoch 1730, training loss: 623.1764526367188 = 0.06556709110736847 + 100.0 * 6.231109142303467
Epoch 1730, val loss: 1.4448577165603638
Epoch 1740, training loss: 623.1267700195312 = 0.06422575563192368 + 100.0 * 6.230625629425049
Epoch 1740, val loss: 1.4504657983779907
Epoch 1750, training loss: 623.8181762695312 = 0.06291390210390091 + 100.0 * 6.237552642822266
Epoch 1750, val loss: 1.4554027318954468
Epoch 1760, training loss: 623.2598266601562 = 0.061625927686691284 + 100.0 * 6.2319817543029785
Epoch 1760, val loss: 1.460048794746399
Epoch 1770, training loss: 623.2426147460938 = 0.06036623567342758 + 100.0 * 6.231822490692139
Epoch 1770, val loss: 1.4653488397598267
Epoch 1780, training loss: 623.4404907226562 = 0.059156421571969986 + 100.0 * 6.233813762664795
Epoch 1780, val loss: 1.470213770866394
Epoch 1790, training loss: 623.0808715820312 = 0.057925645262002945 + 100.0 * 6.230229377746582
Epoch 1790, val loss: 1.4749476909637451
Epoch 1800, training loss: 622.9788208007812 = 0.05677236244082451 + 100.0 * 6.229220867156982
Epoch 1800, val loss: 1.4802261590957642
Epoch 1810, training loss: 622.9004516601562 = 0.05565115064382553 + 100.0 * 6.228447914123535
Epoch 1810, val loss: 1.485235571861267
Epoch 1820, training loss: 623.099365234375 = 0.05457174777984619 + 100.0 * 6.230448246002197
Epoch 1820, val loss: 1.490431547164917
Epoch 1830, training loss: 622.9378662109375 = 0.05349244177341461 + 100.0 * 6.228844165802002
Epoch 1830, val loss: 1.494752049446106
Epoch 1840, training loss: 623.1434326171875 = 0.05244436487555504 + 100.0 * 6.230909824371338
Epoch 1840, val loss: 1.4996737241744995
Epoch 1850, training loss: 622.9714965820312 = 0.05141809210181236 + 100.0 * 6.229201316833496
Epoch 1850, val loss: 1.5042134523391724
Epoch 1860, training loss: 622.9574584960938 = 0.050426509231328964 + 100.0 * 6.229070663452148
Epoch 1860, val loss: 1.509109377861023
Epoch 1870, training loss: 622.690185546875 = 0.049452535808086395 + 100.0 * 6.226407527923584
Epoch 1870, val loss: 1.51370370388031
Epoch 1880, training loss: 622.684814453125 = 0.048520348966121674 + 100.0 * 6.226363182067871
Epoch 1880, val loss: 1.5183613300323486
Epoch 1890, training loss: 623.1201171875 = 0.047620728611946106 + 100.0 * 6.230724811553955
Epoch 1890, val loss: 1.5226726531982422
Epoch 1900, training loss: 623.0015258789062 = 0.046716898679733276 + 100.0 * 6.229548454284668
Epoch 1900, val loss: 1.526958703994751
Epoch 1910, training loss: 622.8028564453125 = 0.045828927308321 + 100.0 * 6.227570056915283
Epoch 1910, val loss: 1.5320765972137451
Epoch 1920, training loss: 622.6767578125 = 0.044966068118810654 + 100.0 * 6.226317882537842
Epoch 1920, val loss: 1.536293625831604
Epoch 1930, training loss: 622.7307739257812 = 0.04414128512144089 + 100.0 * 6.226866245269775
Epoch 1930, val loss: 1.5411497354507446
Epoch 1940, training loss: 622.7256469726562 = 0.04333532974123955 + 100.0 * 6.226823329925537
Epoch 1940, val loss: 1.545413851737976
Epoch 1950, training loss: 622.6547241210938 = 0.04254525899887085 + 100.0 * 6.22612190246582
Epoch 1950, val loss: 1.5501196384429932
Epoch 1960, training loss: 623.0261840820312 = 0.04176805913448334 + 100.0 * 6.229844093322754
Epoch 1960, val loss: 1.5541926622390747
Epoch 1970, training loss: 622.7299194335938 = 0.04100972041487694 + 100.0 * 6.226889133453369
Epoch 1970, val loss: 1.5587252378463745
Epoch 1980, training loss: 622.6002807617188 = 0.040259502828121185 + 100.0 * 6.225600242614746
Epoch 1980, val loss: 1.562735915184021
Epoch 1990, training loss: 622.5285034179688 = 0.03955146670341492 + 100.0 * 6.224889278411865
Epoch 1990, val loss: 1.567582607269287
Epoch 2000, training loss: 622.5538940429688 = 0.03885551542043686 + 100.0 * 6.225150108337402
Epoch 2000, val loss: 1.5715298652648926
Epoch 2010, training loss: 622.5388793945312 = 0.03817754238843918 + 100.0 * 6.225006580352783
Epoch 2010, val loss: 1.575892448425293
Epoch 2020, training loss: 622.7080078125 = 0.037511833012104034 + 100.0 * 6.2267045974731445
Epoch 2020, val loss: 1.5797311067581177
Epoch 2030, training loss: 622.3618774414062 = 0.03685544803738594 + 100.0 * 6.223250389099121
Epoch 2030, val loss: 1.583863615989685
Epoch 2040, training loss: 622.4544677734375 = 0.036226335912942886 + 100.0 * 6.22418212890625
Epoch 2040, val loss: 1.5879172086715698
Epoch 2050, training loss: 622.9092407226562 = 0.03559811785817146 + 100.0 * 6.228736877441406
Epoch 2050, val loss: 1.5922962427139282
Epoch 2060, training loss: 622.3346557617188 = 0.034975454211235046 + 100.0 * 6.222996711730957
Epoch 2060, val loss: 1.595834493637085
Epoch 2070, training loss: 622.2440185546875 = 0.03438473492860794 + 100.0 * 6.2220964431762695
Epoch 2070, val loss: 1.6003046035766602
Epoch 2080, training loss: 622.5111083984375 = 0.03382362425327301 + 100.0 * 6.224772930145264
Epoch 2080, val loss: 1.6040593385696411
Epoch 2090, training loss: 622.2046508789062 = 0.033247869461774826 + 100.0 * 6.221714019775391
Epoch 2090, val loss: 1.6082310676574707
Epoch 2100, training loss: 622.34619140625 = 0.032693613320589066 + 100.0 * 6.223134994506836
Epoch 2100, val loss: 1.6118545532226562
Epoch 2110, training loss: 622.5437622070312 = 0.03216906636953354 + 100.0 * 6.225115776062012
Epoch 2110, val loss: 1.6157668828964233
Epoch 2120, training loss: 622.3405151367188 = 0.0316268689930439 + 100.0 * 6.22308874130249
Epoch 2120, val loss: 1.619435429573059
Epoch 2130, training loss: 622.2177124023438 = 0.031101610511541367 + 100.0 * 6.221865653991699
Epoch 2130, val loss: 1.6235271692276
Epoch 2140, training loss: 622.4308471679688 = 0.030599696561694145 + 100.0 * 6.224002838134766
Epoch 2140, val loss: 1.6275029182434082
Epoch 2150, training loss: 622.050048828125 = 0.030100930482149124 + 100.0 * 6.2201995849609375
Epoch 2150, val loss: 1.6311438083648682
Epoch 2160, training loss: 622.0538940429688 = 0.029631400480866432 + 100.0 * 6.220242977142334
Epoch 2160, val loss: 1.6350784301757812
Epoch 2170, training loss: 622.052490234375 = 0.029169395565986633 + 100.0 * 6.220232963562012
Epoch 2170, val loss: 1.6391260623931885
Epoch 2180, training loss: 622.74267578125 = 0.028722235932946205 + 100.0 * 6.227139472961426
Epoch 2180, val loss: 1.6430326700210571
Epoch 2190, training loss: 622.4262084960938 = 0.028251588344573975 + 100.0 * 6.223979473114014
Epoch 2190, val loss: 1.6461821794509888
Epoch 2200, training loss: 622.0609741210938 = 0.027804309502243996 + 100.0 * 6.22033166885376
Epoch 2200, val loss: 1.649875283241272
Epoch 2210, training loss: 622.0437622070312 = 0.027379563078284264 + 100.0 * 6.220163345336914
Epoch 2210, val loss: 1.6534247398376465
Epoch 2220, training loss: 622.5543823242188 = 0.026966437697410583 + 100.0 * 6.225274085998535
Epoch 2220, val loss: 1.6571004390716553
Epoch 2230, training loss: 622.0455322265625 = 0.026544980704784393 + 100.0 * 6.220189571380615
Epoch 2230, val loss: 1.6609705686569214
Epoch 2240, training loss: 621.8948364257812 = 0.026141595095396042 + 100.0 * 6.218687057495117
Epoch 2240, val loss: 1.664279818534851
Epoch 2250, training loss: 621.8302612304688 = 0.02575506828725338 + 100.0 * 6.218045234680176
Epoch 2250, val loss: 1.6680865287780762
Epoch 2260, training loss: 622.2621459960938 = 0.025385098531842232 + 100.0 * 6.222367763519287
Epoch 2260, val loss: 1.6716334819793701
Epoch 2270, training loss: 621.9750366210938 = 0.025003580376505852 + 100.0 * 6.2195000648498535
Epoch 2270, val loss: 1.6742160320281982
Epoch 2280, training loss: 622.3129272460938 = 0.024635639041662216 + 100.0 * 6.2228827476501465
Epoch 2280, val loss: 1.6781288385391235
Epoch 2290, training loss: 621.8839111328125 = 0.024255968630313873 + 100.0 * 6.218596935272217
Epoch 2290, val loss: 1.6811543703079224
Epoch 2300, training loss: 621.7935791015625 = 0.02390475757420063 + 100.0 * 6.217696666717529
Epoch 2300, val loss: 1.6853259801864624
Epoch 2310, training loss: 621.7321166992188 = 0.023568591102957726 + 100.0 * 6.217085361480713
Epoch 2310, val loss: 1.6882529258728027
Epoch 2320, training loss: 622.1978759765625 = 0.023247357457876205 + 100.0 * 6.221746444702148
Epoch 2320, val loss: 1.6919572353363037
Epoch 2330, training loss: 621.7583618164062 = 0.022900689393281937 + 100.0 * 6.217354774475098
Epoch 2330, val loss: 1.694700002670288
Epoch 2340, training loss: 621.7904052734375 = 0.022573592141270638 + 100.0 * 6.217678070068359
Epoch 2340, val loss: 1.6983821392059326
Epoch 2350, training loss: 622.0188598632812 = 0.02226118929684162 + 100.0 * 6.219965934753418
Epoch 2350, val loss: 1.7010449171066284
Epoch 2360, training loss: 621.86279296875 = 0.02195228822529316 + 100.0 * 6.218408107757568
Epoch 2360, val loss: 1.7049767971038818
Epoch 2370, training loss: 621.6799926757812 = 0.02163974568247795 + 100.0 * 6.216583251953125
Epoch 2370, val loss: 1.7079904079437256
Epoch 2380, training loss: 621.6434936523438 = 0.021349459886550903 + 100.0 * 6.216221332550049
Epoch 2380, val loss: 1.7115027904510498
Epoch 2390, training loss: 621.85009765625 = 0.02106885239481926 + 100.0 * 6.218290328979492
Epoch 2390, val loss: 1.7144023180007935
Epoch 2400, training loss: 621.9088745117188 = 0.020786400884389877 + 100.0 * 6.218880653381348
Epoch 2400, val loss: 1.7174543142318726
Epoch 2410, training loss: 621.8844604492188 = 0.020498543977737427 + 100.0 * 6.218639373779297
Epoch 2410, val loss: 1.7206560373306274
Epoch 2420, training loss: 621.6295776367188 = 0.020214328542351723 + 100.0 * 6.21609354019165
Epoch 2420, val loss: 1.7238901853561401
Epoch 2430, training loss: 621.6358032226562 = 0.01994892954826355 + 100.0 * 6.216158390045166
Epoch 2430, val loss: 1.7270760536193848
Epoch 2440, training loss: 621.8200073242188 = 0.019697433337569237 + 100.0 * 6.218002796173096
Epoch 2440, val loss: 1.7302221059799194
Epoch 2450, training loss: 621.6019897460938 = 0.01943458430469036 + 100.0 * 6.215826034545898
Epoch 2450, val loss: 1.7331480979919434
Epoch 2460, training loss: 621.8546752929688 = 0.019183633849024773 + 100.0 * 6.218355178833008
Epoch 2460, val loss: 1.7363983392715454
Epoch 2470, training loss: 621.934814453125 = 0.018928643316030502 + 100.0 * 6.21915864944458
Epoch 2470, val loss: 1.7392852306365967
Epoch 2480, training loss: 621.7198486328125 = 0.018685227259993553 + 100.0 * 6.217011451721191
Epoch 2480, val loss: 1.7421625852584839
Epoch 2490, training loss: 621.5398559570312 = 0.01844552531838417 + 100.0 * 6.215214252471924
Epoch 2490, val loss: 1.7451386451721191
Epoch 2500, training loss: 621.4292602539062 = 0.018214741721749306 + 100.0 * 6.214110851287842
Epoch 2500, val loss: 1.748394250869751
Epoch 2510, training loss: 621.4022216796875 = 0.01799357309937477 + 100.0 * 6.213842391967773
Epoch 2510, val loss: 1.751065731048584
Epoch 2520, training loss: 621.9667358398438 = 0.01778683252632618 + 100.0 * 6.219490051269531
Epoch 2520, val loss: 1.7538034915924072
Epoch 2530, training loss: 621.5859985351562 = 0.017549144104123116 + 100.0 * 6.215684413909912
Epoch 2530, val loss: 1.756676435470581
Epoch 2540, training loss: 621.540283203125 = 0.017328396439552307 + 100.0 * 6.215229511260986
Epoch 2540, val loss: 1.7593379020690918
Epoch 2550, training loss: 621.4740600585938 = 0.017115017399191856 + 100.0 * 6.214569568634033
Epoch 2550, val loss: 1.762602686882019
Epoch 2560, training loss: 621.35205078125 = 0.016908159479498863 + 100.0 * 6.213351249694824
Epoch 2560, val loss: 1.7654820680618286
Epoch 2570, training loss: 621.5242309570312 = 0.01671101711690426 + 100.0 * 6.2150750160217285
Epoch 2570, val loss: 1.7680479288101196
Epoch 2580, training loss: 621.6611328125 = 0.016506798565387726 + 100.0 * 6.216446399688721
Epoch 2580, val loss: 1.7709214687347412
Epoch 2590, training loss: 621.3497314453125 = 0.016305116936564445 + 100.0 * 6.213334560394287
Epoch 2590, val loss: 1.7738476991653442
Epoch 2600, training loss: 621.3453979492188 = 0.016116619110107422 + 100.0 * 6.213293075561523
Epoch 2600, val loss: 1.7767643928527832
Epoch 2610, training loss: 621.9672241210938 = 0.015931319445371628 + 100.0 * 6.219512939453125
Epoch 2610, val loss: 1.7796127796173096
Epoch 2620, training loss: 621.417236328125 = 0.01573743298649788 + 100.0 * 6.214015007019043
Epoch 2620, val loss: 1.7815037965774536
Epoch 2630, training loss: 621.3054809570312 = 0.015558003447949886 + 100.0 * 6.212899208068848
Epoch 2630, val loss: 1.7848286628723145
Epoch 2640, training loss: 622.1764526367188 = 0.015388133004307747 + 100.0 * 6.2216105461120605
Epoch 2640, val loss: 1.7874391078948975
Epoch 2650, training loss: 621.375 = 0.015200918540358543 + 100.0 * 6.213598251342773
Epoch 2650, val loss: 1.789414644241333
Epoch 2660, training loss: 621.1663208007812 = 0.015024848282337189 + 100.0 * 6.211513042449951
Epoch 2660, val loss: 1.7923823595046997
Epoch 2670, training loss: 621.2994995117188 = 0.014865290373563766 + 100.0 * 6.212845802307129
Epoch 2670, val loss: 1.7949366569519043
Epoch 2680, training loss: 621.4924926757812 = 0.014699765481054783 + 100.0 * 6.214777946472168
Epoch 2680, val loss: 1.7973273992538452
Epoch 2690, training loss: 621.3065185546875 = 0.014530143700540066 + 100.0 * 6.212920188903809
Epoch 2690, val loss: 1.800291657447815
Epoch 2700, training loss: 621.4515380859375 = 0.014374916441738605 + 100.0 * 6.214371204376221
Epoch 2700, val loss: 1.80220627784729
Epoch 2710, training loss: 621.2096557617188 = 0.014214735478162766 + 100.0 * 6.211954116821289
Epoch 2710, val loss: 1.805177092552185
Epoch 2720, training loss: 621.317626953125 = 0.014059923589229584 + 100.0 * 6.213036060333252
Epoch 2720, val loss: 1.8071403503417969
Epoch 2730, training loss: 621.2660522460938 = 0.013909749686717987 + 100.0 * 6.212521076202393
Epoch 2730, val loss: 1.8099488019943237
Epoch 2740, training loss: 621.4319458007812 = 0.013764583505690098 + 100.0 * 6.214181423187256
Epoch 2740, val loss: 1.8123847246170044
Epoch 2750, training loss: 621.0926513671875 = 0.013607313856482506 + 100.0 * 6.210790157318115
Epoch 2750, val loss: 1.8147802352905273
Epoch 2760, training loss: 621.0118408203125 = 0.013461182825267315 + 100.0 * 6.2099833488464355
Epoch 2760, val loss: 1.8171756267547607
Epoch 2770, training loss: 621.03857421875 = 0.01332499273121357 + 100.0 * 6.21025276184082
Epoch 2770, val loss: 1.8198527097702026
Epoch 2780, training loss: 621.4962768554688 = 0.01319245994091034 + 100.0 * 6.2148308753967285
Epoch 2780, val loss: 1.8215405941009521
Epoch 2790, training loss: 621.0570068359375 = 0.013053231872618198 + 100.0 * 6.210439682006836
Epoch 2790, val loss: 1.8242080211639404
Epoch 2800, training loss: 621.0017700195312 = 0.012915863655507565 + 100.0 * 6.209888458251953
Epoch 2800, val loss: 1.8264272212982178
Epoch 2810, training loss: 621.4817504882812 = 0.012787760235369205 + 100.0 * 6.214689254760742
Epoch 2810, val loss: 1.8292137384414673
Epoch 2820, training loss: 621.0191650390625 = 0.012651539407670498 + 100.0 * 6.2100653648376465
Epoch 2820, val loss: 1.8311991691589355
Epoch 2830, training loss: 621.005126953125 = 0.012522508390247822 + 100.0 * 6.209926128387451
Epoch 2830, val loss: 1.833665132522583
Epoch 2840, training loss: 620.9927368164062 = 0.0123994629830122 + 100.0 * 6.209803581237793
Epoch 2840, val loss: 1.8359401226043701
Epoch 2850, training loss: 621.268310546875 = 0.012280927039682865 + 100.0 * 6.212560176849365
Epoch 2850, val loss: 1.8381503820419312
Epoch 2860, training loss: 621.2178955078125 = 0.01215826254338026 + 100.0 * 6.212057590484619
Epoch 2860, val loss: 1.8409192562103271
Epoch 2870, training loss: 620.9244384765625 = 0.012034711427986622 + 100.0 * 6.2091240882873535
Epoch 2870, val loss: 1.8422068357467651
Epoch 2880, training loss: 620.930908203125 = 0.011919882148504257 + 100.0 * 6.2091898918151855
Epoch 2880, val loss: 1.8448861837387085
Epoch 2890, training loss: 621.4359130859375 = 0.011812514625489712 + 100.0 * 6.214240550994873
Epoch 2890, val loss: 1.846807599067688
Epoch 2900, training loss: 620.9575805664062 = 0.011690543033182621 + 100.0 * 6.209458827972412
Epoch 2900, val loss: 1.8491700887680054
Epoch 2910, training loss: 621.0164794921875 = 0.011578777804970741 + 100.0 * 6.210048675537109
Epoch 2910, val loss: 1.8511818647384644
Epoch 2920, training loss: 620.988525390625 = 0.011470707133412361 + 100.0 * 6.209770202636719
Epoch 2920, val loss: 1.853490948677063
Epoch 2930, training loss: 620.7839965820312 = 0.011360011994838715 + 100.0 * 6.20772647857666
Epoch 2930, val loss: 1.8555965423583984
Epoch 2940, training loss: 621.0709838867188 = 0.011262001469731331 + 100.0 * 6.210597038269043
Epoch 2940, val loss: 1.8579469919204712
Epoch 2950, training loss: 621.0237426757812 = 0.011154294945299625 + 100.0 * 6.21012544631958
Epoch 2950, val loss: 1.8597803115844727
Epoch 2960, training loss: 620.8731079101562 = 0.01104668714106083 + 100.0 * 6.208620548248291
Epoch 2960, val loss: 1.8617507219314575
Epoch 2970, training loss: 621.1992797851562 = 0.01095065288245678 + 100.0 * 6.211883544921875
Epoch 2970, val loss: 1.8631749153137207
Epoch 2980, training loss: 620.7446899414062 = 0.0108425198122859 + 100.0 * 6.207338333129883
Epoch 2980, val loss: 1.866187334060669
Epoch 2990, training loss: 620.9116821289062 = 0.010746784508228302 + 100.0 * 6.209009647369385
Epoch 2990, val loss: 1.8682825565338135
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.819715340010543
=== training gcn model ===
Epoch 0, training loss: 861.6395874023438 = 1.9529314041137695 + 100.0 * 8.596866607666016
Epoch 0, val loss: 1.9420791864395142
Epoch 10, training loss: 861.5719604492188 = 1.9441436529159546 + 100.0 * 8.596278190612793
Epoch 10, val loss: 1.9339895248413086
Epoch 20, training loss: 861.1382446289062 = 1.9333150386810303 + 100.0 * 8.592049598693848
Epoch 20, val loss: 1.9235938787460327
Epoch 30, training loss: 858.1251831054688 = 1.9191075563430786 + 100.0 * 8.562060356140137
Epoch 30, val loss: 1.9095574617385864
Epoch 40, training loss: 841.7559204101562 = 1.9004565477371216 + 100.0 * 8.398554801940918
Epoch 40, val loss: 1.8910688161849976
Epoch 50, training loss: 803.8961181640625 = 1.8775075674057007 + 100.0 * 8.020186424255371
Epoch 50, val loss: 1.868262529373169
Epoch 60, training loss: 782.27099609375 = 1.8565928936004639 + 100.0 * 7.804144382476807
Epoch 60, val loss: 1.8491814136505127
Epoch 70, training loss: 750.9140625 = 1.8439654111862183 + 100.0 * 7.490700721740723
Epoch 70, val loss: 1.8385450839996338
Epoch 80, training loss: 723.5302124023438 = 1.8392257690429688 + 100.0 * 7.216909885406494
Epoch 80, val loss: 1.83425772190094
Epoch 90, training loss: 705.6864624023438 = 1.830543041229248 + 100.0 * 7.0385589599609375
Epoch 90, val loss: 1.825942039489746
Epoch 100, training loss: 689.6665649414062 = 1.8219102621078491 + 100.0 * 6.878446578979492
Epoch 100, val loss: 1.8182907104492188
Epoch 110, training loss: 679.7646484375 = 1.8148030042648315 + 100.0 * 6.779498100280762
Epoch 110, val loss: 1.811570405960083
Epoch 120, training loss: 673.1541137695312 = 1.8076380491256714 + 100.0 * 6.713464736938477
Epoch 120, val loss: 1.8046163320541382
Epoch 130, training loss: 668.767822265625 = 1.799896240234375 + 100.0 * 6.669679164886475
Epoch 130, val loss: 1.7972326278686523
Epoch 140, training loss: 665.9725952148438 = 1.791846752166748 + 100.0 * 6.6418070793151855
Epoch 140, val loss: 1.7897635698318481
Epoch 150, training loss: 663.151611328125 = 1.7837146520614624 + 100.0 * 6.6136794090271
Epoch 150, val loss: 1.7821377515792847
Epoch 160, training loss: 660.8952026367188 = 1.7752490043640137 + 100.0 * 6.5911993980407715
Epoch 160, val loss: 1.774246335029602
Epoch 170, training loss: 658.960693359375 = 1.766384482383728 + 100.0 * 6.571943283081055
Epoch 170, val loss: 1.7660750150680542
Epoch 180, training loss: 657.198486328125 = 1.756981372833252 + 100.0 * 6.554415225982666
Epoch 180, val loss: 1.7573742866516113
Epoch 190, training loss: 655.6806640625 = 1.7468279600143433 + 100.0 * 6.539338111877441
Epoch 190, val loss: 1.7481318712234497
Epoch 200, training loss: 654.2354736328125 = 1.7358752489089966 + 100.0 * 6.524995803833008
Epoch 200, val loss: 1.7381693124771118
Epoch 210, training loss: 653.4063720703125 = 1.7240079641342163 + 100.0 * 6.516823768615723
Epoch 210, val loss: 1.727339744567871
Epoch 220, training loss: 651.8502197265625 = 1.7110342979431152 + 100.0 * 6.501391887664795
Epoch 220, val loss: 1.7156270742416382
Epoch 230, training loss: 650.6235961914062 = 1.6970950365066528 + 100.0 * 6.489264965057373
Epoch 230, val loss: 1.7030022144317627
Epoch 240, training loss: 649.818359375 = 1.6820951700210571 + 100.0 * 6.481362819671631
Epoch 240, val loss: 1.6894145011901855
Epoch 250, training loss: 648.7017822265625 = 1.6657887697219849 + 100.0 * 6.470359802246094
Epoch 250, val loss: 1.6747015714645386
Epoch 260, training loss: 647.7439575195312 = 1.6484887599945068 + 100.0 * 6.460954666137695
Epoch 260, val loss: 1.6590921878814697
Epoch 270, training loss: 647.0248413085938 = 1.630034327507019 + 100.0 * 6.453948497772217
Epoch 270, val loss: 1.6424840688705444
Epoch 280, training loss: 646.23193359375 = 1.610427975654602 + 100.0 * 6.4462151527404785
Epoch 280, val loss: 1.6248139142990112
Epoch 290, training loss: 645.4408569335938 = 1.5899510383605957 + 100.0 * 6.438508987426758
Epoch 290, val loss: 1.60645592212677
Epoch 300, training loss: 644.7645874023438 = 1.5686534643173218 + 100.0 * 6.43195915222168
Epoch 300, val loss: 1.587507963180542
Epoch 310, training loss: 644.1907348632812 = 1.546667456626892 + 100.0 * 6.426440238952637
Epoch 310, val loss: 1.568025827407837
Epoch 320, training loss: 643.5560913085938 = 1.5241200923919678 + 100.0 * 6.4203200340271
Epoch 320, val loss: 1.5482834577560425
Epoch 330, training loss: 642.8810424804688 = 1.5012413263320923 + 100.0 * 6.4137983322143555
Epoch 330, val loss: 1.5284122228622437
Epoch 340, training loss: 642.9570922851562 = 1.4782143831253052 + 100.0 * 6.414788722991943
Epoch 340, val loss: 1.5085301399230957
Epoch 350, training loss: 641.8391723632812 = 1.4548686742782593 + 100.0 * 6.403842926025391
Epoch 350, val loss: 1.4887546300888062
Epoch 360, training loss: 641.32275390625 = 1.4315919876098633 + 100.0 * 6.398911952972412
Epoch 360, val loss: 1.469212293624878
Epoch 370, training loss: 641.1015625 = 1.4083870649337769 + 100.0 * 6.3969316482543945
Epoch 370, val loss: 1.4499742984771729
Epoch 380, training loss: 640.853759765625 = 1.3852386474609375 + 100.0 * 6.3946852684021
Epoch 380, val loss: 1.431127667427063
Epoch 390, training loss: 639.9701538085938 = 1.362154245376587 + 100.0 * 6.386079788208008
Epoch 390, val loss: 1.4124925136566162
Epoch 400, training loss: 639.5045776367188 = 1.3392136096954346 + 100.0 * 6.381653308868408
Epoch 400, val loss: 1.3941055536270142
Epoch 410, training loss: 639.6034545898438 = 1.3164032697677612 + 100.0 * 6.382870197296143
Epoch 410, val loss: 1.3760297298431396
Epoch 420, training loss: 638.7996826171875 = 1.2932676076889038 + 100.0 * 6.375063896179199
Epoch 420, val loss: 1.3580833673477173
Epoch 430, training loss: 638.3352661132812 = 1.270540714263916 + 100.0 * 6.370646953582764
Epoch 430, val loss: 1.3405119180679321
Epoch 440, training loss: 637.9847412109375 = 1.2479667663574219 + 100.0 * 6.367367744445801
Epoch 440, val loss: 1.3231823444366455
Epoch 450, training loss: 638.013671875 = 1.2252345085144043 + 100.0 * 6.367884635925293
Epoch 450, val loss: 1.305862307548523
Epoch 460, training loss: 637.4221801757812 = 1.202715277671814 + 100.0 * 6.362195014953613
Epoch 460, val loss: 1.2887471914291382
Epoch 470, training loss: 636.9674072265625 = 1.180267333984375 + 100.0 * 6.357871055603027
Epoch 470, val loss: 1.2719104290008545
Epoch 480, training loss: 636.67041015625 = 1.1580287218093872 + 100.0 * 6.355123996734619
Epoch 480, val loss: 1.2552934885025024
Epoch 490, training loss: 637.3394775390625 = 1.1358349323272705 + 100.0 * 6.362036228179932
Epoch 490, val loss: 1.238887071609497
Epoch 500, training loss: 636.3195190429688 = 1.1136949062347412 + 100.0 * 6.352058410644531
Epoch 500, val loss: 1.2225708961486816
Epoch 510, training loss: 635.8396606445312 = 1.0916054248809814 + 100.0 * 6.347480773925781
Epoch 510, val loss: 1.2064160108566284
Epoch 520, training loss: 635.5400390625 = 1.069861650466919 + 100.0 * 6.344701766967773
Epoch 520, val loss: 1.1906843185424805
Epoch 530, training loss: 635.4400024414062 = 1.0482263565063477 + 100.0 * 6.3439178466796875
Epoch 530, val loss: 1.175330638885498
Epoch 540, training loss: 635.2982788085938 = 1.0264924764633179 + 100.0 * 6.34271764755249
Epoch 540, val loss: 1.1598948240280151
Epoch 550, training loss: 635.1715087890625 = 1.005003809928894 + 100.0 * 6.341665267944336
Epoch 550, val loss: 1.1447418928146362
Epoch 560, training loss: 634.5499267578125 = 0.9835845828056335 + 100.0 * 6.335663318634033
Epoch 560, val loss: 1.1298432350158691
Epoch 570, training loss: 634.2308959960938 = 0.9624535441398621 + 100.0 * 6.332684516906738
Epoch 570, val loss: 1.1152877807617188
Epoch 580, training loss: 634.142822265625 = 0.941595196723938 + 100.0 * 6.332012176513672
Epoch 580, val loss: 1.1010055541992188
Epoch 590, training loss: 633.9633178710938 = 0.9207388162612915 + 100.0 * 6.33042573928833
Epoch 590, val loss: 1.0869762897491455
Epoch 600, training loss: 634.1383056640625 = 0.8999685049057007 + 100.0 * 6.332383155822754
Epoch 600, val loss: 1.0731388330459595
Epoch 610, training loss: 633.54248046875 = 0.8795912265777588 + 100.0 * 6.326629161834717
Epoch 610, val loss: 1.0594745874404907
Epoch 620, training loss: 633.16748046875 = 0.8594294786453247 + 100.0 * 6.323080539703369
Epoch 620, val loss: 1.046352744102478
Epoch 630, training loss: 633.1272583007812 = 0.8395469188690186 + 100.0 * 6.322876930236816
Epoch 630, val loss: 1.0335074663162231
Epoch 640, training loss: 633.3117065429688 = 0.8197840452194214 + 100.0 * 6.324919700622559
Epoch 640, val loss: 1.0205758810043335
Epoch 650, training loss: 632.8424682617188 = 0.8002439737319946 + 100.0 * 6.320422649383545
Epoch 650, val loss: 1.0082471370697021
Epoch 660, training loss: 632.6246337890625 = 0.7809592485427856 + 100.0 * 6.318437099456787
Epoch 660, val loss: 0.9959867000579834
Epoch 670, training loss: 632.2314453125 = 0.7620083093643188 + 100.0 * 6.314693927764893
Epoch 670, val loss: 0.9843047857284546
Epoch 680, training loss: 632.2793579101562 = 0.7434558272361755 + 100.0 * 6.315359115600586
Epoch 680, val loss: 0.9729951024055481
Epoch 690, training loss: 632.3231811523438 = 0.7251286506652832 + 100.0 * 6.315980434417725
Epoch 690, val loss: 0.9615184664726257
Epoch 700, training loss: 631.840576171875 = 0.7071301341056824 + 100.0 * 6.311334133148193
Epoch 700, val loss: 0.9510258436203003
Epoch 710, training loss: 631.567626953125 = 0.6895038485527039 + 100.0 * 6.308781147003174
Epoch 710, val loss: 0.9405058026313782
Epoch 720, training loss: 631.3731079101562 = 0.6723284721374512 + 100.0 * 6.307007789611816
Epoch 720, val loss: 0.9305275082588196
Epoch 730, training loss: 632.7950439453125 = 0.6554352045059204 + 100.0 * 6.3213958740234375
Epoch 730, val loss: 0.9207376837730408
Epoch 740, training loss: 631.1494750976562 = 0.6386155486106873 + 100.0 * 6.305108547210693
Epoch 740, val loss: 0.9110254645347595
Epoch 750, training loss: 630.854736328125 = 0.6223729252815247 + 100.0 * 6.302323341369629
Epoch 750, val loss: 0.9022626876831055
Epoch 760, training loss: 630.7177124023438 = 0.6065939664840698 + 100.0 * 6.301111221313477
Epoch 760, val loss: 0.8936904072761536
Epoch 770, training loss: 631.56298828125 = 0.5911058187484741 + 100.0 * 6.309718608856201
Epoch 770, val loss: 0.885434627532959
Epoch 780, training loss: 631.1644287109375 = 0.5759291648864746 + 100.0 * 6.305884838104248
Epoch 780, val loss: 0.8774318099021912
Epoch 790, training loss: 630.3236694335938 = 0.5610077977180481 + 100.0 * 6.297626495361328
Epoch 790, val loss: 0.869834840297699
Epoch 800, training loss: 630.2015380859375 = 0.5466192960739136 + 100.0 * 6.296548843383789
Epoch 800, val loss: 0.8627840280532837
Epoch 810, training loss: 630.003173828125 = 0.5326127409934998 + 100.0 * 6.294705867767334
Epoch 810, val loss: 0.8560016751289368
Epoch 820, training loss: 631.0527954101562 = 0.5189874172210693 + 100.0 * 6.305337905883789
Epoch 820, val loss: 0.8501063585281372
Epoch 830, training loss: 630.18359375 = 0.5053902864456177 + 100.0 * 6.29678201675415
Epoch 830, val loss: 0.8431959748268127
Epoch 840, training loss: 629.88818359375 = 0.4922225773334503 + 100.0 * 6.293959617614746
Epoch 840, val loss: 0.8374066352844238
Epoch 850, training loss: 629.5899658203125 = 0.47948378324508667 + 100.0 * 6.291104793548584
Epoch 850, val loss: 0.8319844007492065
Epoch 860, training loss: 629.6497192382812 = 0.46707361936569214 + 100.0 * 6.291826248168945
Epoch 860, val loss: 0.827069103717804
Epoch 870, training loss: 629.4708862304688 = 0.454880028963089 + 100.0 * 6.290160179138184
Epoch 870, val loss: 0.8225787878036499
Epoch 880, training loss: 629.4461669921875 = 0.44298455119132996 + 100.0 * 6.290031909942627
Epoch 880, val loss: 0.8181299567222595
Epoch 890, training loss: 629.4039916992188 = 0.431384801864624 + 100.0 * 6.2897257804870605
Epoch 890, val loss: 0.8142907619476318
Epoch 900, training loss: 628.9706420898438 = 0.4200862944126129 + 100.0 * 6.285505294799805
Epoch 900, val loss: 0.8107024431228638
Epoch 910, training loss: 629.0030517578125 = 0.4091115891933441 + 100.0 * 6.2859392166137695
Epoch 910, val loss: 0.8074873685836792
Epoch 920, training loss: 628.9904174804688 = 0.398359090089798 + 100.0 * 6.2859206199646
Epoch 920, val loss: 0.8046746253967285
Epoch 930, training loss: 629.4960327148438 = 0.38782650232315063 + 100.0 * 6.29108190536499
Epoch 930, val loss: 0.8021683096885681
Epoch 940, training loss: 628.7813110351562 = 0.37751704454421997 + 100.0 * 6.284038066864014
Epoch 940, val loss: 0.7999764084815979
Epoch 950, training loss: 628.6049194335938 = 0.3674120604991913 + 100.0 * 6.282374858856201
Epoch 950, val loss: 0.798041820526123
Epoch 960, training loss: 628.427978515625 = 0.3576854467391968 + 100.0 * 6.280703067779541
Epoch 960, val loss: 0.7965894937515259
Epoch 970, training loss: 628.6807861328125 = 0.3481821119785309 + 100.0 * 6.283325672149658
Epoch 970, val loss: 0.7956396341323853
Epoch 980, training loss: 628.5031127929688 = 0.33888745307922363 + 100.0 * 6.281642436981201
Epoch 980, val loss: 0.7943856120109558
Epoch 990, training loss: 628.30126953125 = 0.3297607898712158 + 100.0 * 6.279715061187744
Epoch 990, val loss: 0.7937301993370056
Epoch 1000, training loss: 628.2117309570312 = 0.3209572732448578 + 100.0 * 6.278907775878906
Epoch 1000, val loss: 0.7930786609649658
Epoch 1010, training loss: 628.0885620117188 = 0.31237712502479553 + 100.0 * 6.277761936187744
Epoch 1010, val loss: 0.793210506439209
Epoch 1020, training loss: 628.09375 = 0.3040258586406708 + 100.0 * 6.277897357940674
Epoch 1020, val loss: 0.7930217385292053
Epoch 1030, training loss: 628.3268432617188 = 0.295858770608902 + 100.0 * 6.280310153961182
Epoch 1030, val loss: 0.7936001420021057
Epoch 1040, training loss: 627.9343872070312 = 0.28792649507522583 + 100.0 * 6.276464939117432
Epoch 1040, val loss: 0.7940025329589844
Epoch 1050, training loss: 627.7667236328125 = 0.2801663875579834 + 100.0 * 6.274865627288818
Epoch 1050, val loss: 0.7945199608802795
Epoch 1060, training loss: 627.8435668945312 = 0.27265745401382446 + 100.0 * 6.27570915222168
Epoch 1060, val loss: 0.7955641150474548
Epoch 1070, training loss: 627.6315307617188 = 0.2653595507144928 + 100.0 * 6.2736616134643555
Epoch 1070, val loss: 0.7965244650840759
Epoch 1080, training loss: 627.7682495117188 = 0.2582743763923645 + 100.0 * 6.275099277496338
Epoch 1080, val loss: 0.7975724935531616
Epoch 1090, training loss: 627.7406616210938 = 0.25140902400016785 + 100.0 * 6.274892807006836
Epoch 1090, val loss: 0.7995678186416626
Epoch 1100, training loss: 627.272216796875 = 0.24461200833320618 + 100.0 * 6.270276069641113
Epoch 1100, val loss: 0.8006177544593811
Epoch 1110, training loss: 627.2353515625 = 0.23811766505241394 + 100.0 * 6.269972324371338
Epoch 1110, val loss: 0.8026400208473206
Epoch 1120, training loss: 627.2066040039062 = 0.23181356489658356 + 100.0 * 6.269747734069824
Epoch 1120, val loss: 0.8042858242988586
Epoch 1130, training loss: 627.7467041015625 = 0.22567735612392426 + 100.0 * 6.275210380554199
Epoch 1130, val loss: 0.8064559102058411
Epoch 1140, training loss: 627.326171875 = 0.21970579028129578 + 100.0 * 6.271064281463623
Epoch 1140, val loss: 0.8083591461181641
Epoch 1150, training loss: 627.30908203125 = 0.21390949189662933 + 100.0 * 6.270951747894287
Epoch 1150, val loss: 0.8109071254730225
Epoch 1160, training loss: 627.081298828125 = 0.20828114449977875 + 100.0 * 6.268730640411377
Epoch 1160, val loss: 0.8132686614990234
Epoch 1170, training loss: 626.9677124023438 = 0.20284360647201538 + 100.0 * 6.267648696899414
Epoch 1170, val loss: 0.8157892823219299
Epoch 1180, training loss: 626.6692504882812 = 0.19753484427928925 + 100.0 * 6.2647175788879395
Epoch 1180, val loss: 0.8183614611625671
Epoch 1190, training loss: 626.7054443359375 = 0.1924239546060562 + 100.0 * 6.265130043029785
Epoch 1190, val loss: 0.8209415674209595
Epoch 1200, training loss: 627.2457885742188 = 0.1874629408121109 + 100.0 * 6.270583629608154
Epoch 1200, val loss: 0.8236512541770935
Epoch 1210, training loss: 626.9715576171875 = 0.18261732161045074 + 100.0 * 6.267889499664307
Epoch 1210, val loss: 0.8269708156585693
Epoch 1220, training loss: 626.817138671875 = 0.17790444195270538 + 100.0 * 6.266392230987549
Epoch 1220, val loss: 0.829307496547699
Epoch 1230, training loss: 626.919921875 = 0.17334923148155212 + 100.0 * 6.267465591430664
Epoch 1230, val loss: 0.8325733542442322
Epoch 1240, training loss: 626.49560546875 = 0.16894927620887756 + 100.0 * 6.263266563415527
Epoch 1240, val loss: 0.8355368971824646
Epoch 1250, training loss: 626.255126953125 = 0.16464705765247345 + 100.0 * 6.260904312133789
Epoch 1250, val loss: 0.8387506008148193
Epoch 1260, training loss: 626.3621215820312 = 0.16050545871257782 + 100.0 * 6.2620158195495605
Epoch 1260, val loss: 0.8421536684036255
Epoch 1270, training loss: 626.5972900390625 = 0.1564769148826599 + 100.0 * 6.264408111572266
Epoch 1270, val loss: 0.8451133370399475
Epoch 1280, training loss: 626.717041015625 = 0.15255720913410187 + 100.0 * 6.2656450271606445
Epoch 1280, val loss: 0.8483959436416626
Epoch 1290, training loss: 626.2571411132812 = 0.14871308207511902 + 100.0 * 6.261084079742432
Epoch 1290, val loss: 0.8519566655158997
Epoch 1300, training loss: 626.1724853515625 = 0.14504361152648926 + 100.0 * 6.260274887084961
Epoch 1300, val loss: 0.8555711507797241
Epoch 1310, training loss: 626.8720092773438 = 0.14145825803279877 + 100.0 * 6.267305374145508
Epoch 1310, val loss: 0.8589250445365906
Epoch 1320, training loss: 626.1430053710938 = 0.13790756464004517 + 100.0 * 6.260051250457764
Epoch 1320, val loss: 0.8622407913208008
Epoch 1330, training loss: 625.8975830078125 = 0.13453146815299988 + 100.0 * 6.257630825042725
Epoch 1330, val loss: 0.8662030696868896
Epoch 1340, training loss: 625.8568115234375 = 0.13124796748161316 + 100.0 * 6.257255554199219
Epoch 1340, val loss: 0.8697717785835266
Epoch 1350, training loss: 626.69873046875 = 0.12805141508579254 + 100.0 * 6.265707015991211
Epoch 1350, val loss: 0.8734967112541199
Epoch 1360, training loss: 625.88525390625 = 0.12491191923618317 + 100.0 * 6.257603168487549
Epoch 1360, val loss: 0.8769931793212891
Epoch 1370, training loss: 625.8001098632812 = 0.1218901127576828 + 100.0 * 6.256782531738281
Epoch 1370, val loss: 0.8805900812149048
Epoch 1380, training loss: 625.8881225585938 = 0.11895877122879028 + 100.0 * 6.257691383361816
Epoch 1380, val loss: 0.8846346735954285
Epoch 1390, training loss: 625.709228515625 = 0.11610406637191772 + 100.0 * 6.255931377410889
Epoch 1390, val loss: 0.8884561061859131
Epoch 1400, training loss: 626.0422973632812 = 0.11333373188972473 + 100.0 * 6.259289264678955
Epoch 1400, val loss: 0.8921340107917786
Epoch 1410, training loss: 625.7330322265625 = 0.11059554666280746 + 100.0 * 6.256224155426025
Epoch 1410, val loss: 0.8955986499786377
Epoch 1420, training loss: 625.6837768554688 = 0.10796334594488144 + 100.0 * 6.255758285522461
Epoch 1420, val loss: 0.8997668027877808
Epoch 1430, training loss: 625.631591796875 = 0.10540182143449783 + 100.0 * 6.2552618980407715
Epoch 1430, val loss: 0.9034178853034973
Epoch 1440, training loss: 625.5693969726562 = 0.10291845351457596 + 100.0 * 6.254664897918701
Epoch 1440, val loss: 0.9072150588035583
Epoch 1450, training loss: 625.6390991210938 = 0.10050836205482483 + 100.0 * 6.255385875701904
Epoch 1450, val loss: 0.911403477191925
Epoch 1460, training loss: 625.3020629882812 = 0.09813141077756882 + 100.0 * 6.252039432525635
Epoch 1460, val loss: 0.9149352312088013
Epoch 1470, training loss: 625.5291137695312 = 0.09584306925535202 + 100.0 * 6.254333019256592
Epoch 1470, val loss: 0.918914258480072
Epoch 1480, training loss: 625.579345703125 = 0.09358953684568405 + 100.0 * 6.254857540130615
Epoch 1480, val loss: 0.9225735068321228
Epoch 1490, training loss: 625.3807983398438 = 0.09142442047595978 + 100.0 * 6.252893447875977
Epoch 1490, val loss: 0.926490306854248
Epoch 1500, training loss: 625.1124267578125 = 0.08930697292089462 + 100.0 * 6.2502312660217285
Epoch 1500, val loss: 0.9308176040649414
Epoch 1510, training loss: 625.06201171875 = 0.08727049827575684 + 100.0 * 6.249747276306152
Epoch 1510, val loss: 0.9346330165863037
Epoch 1520, training loss: 625.4718627929688 = 0.08529134094715118 + 100.0 * 6.253865718841553
Epoch 1520, val loss: 0.9387921094894409
Epoch 1530, training loss: 625.3601684570312 = 0.08334364742040634 + 100.0 * 6.252768039703369
Epoch 1530, val loss: 0.9425420761108398
Epoch 1540, training loss: 625.1533813476562 = 0.08141345530748367 + 100.0 * 6.2507195472717285
Epoch 1540, val loss: 0.9463405609130859
Epoch 1550, training loss: 624.91162109375 = 0.07955032587051392 + 100.0 * 6.248321056365967
Epoch 1550, val loss: 0.9502338767051697
Epoch 1560, training loss: 624.989990234375 = 0.07776984572410583 + 100.0 * 6.249122619628906
Epoch 1560, val loss: 0.9539487361907959
Epoch 1570, training loss: 625.189208984375 = 0.07602861523628235 + 100.0 * 6.251131534576416
Epoch 1570, val loss: 0.9580116271972656
Epoch 1580, training loss: 625.1758422851562 = 0.07432328909635544 + 100.0 * 6.2510151863098145
Epoch 1580, val loss: 0.9620854258537292
Epoch 1590, training loss: 624.9822998046875 = 0.07264195382595062 + 100.0 * 6.249096870422363
Epoch 1590, val loss: 0.9661349654197693
Epoch 1600, training loss: 624.7203979492188 = 0.07101813703775406 + 100.0 * 6.246493816375732
Epoch 1600, val loss: 0.9696469902992249
Epoch 1610, training loss: 625.3027954101562 = 0.0694594532251358 + 100.0 * 6.252333641052246
Epoch 1610, val loss: 0.9737187027931213
Epoch 1620, training loss: 624.7273559570312 = 0.06792048364877701 + 100.0 * 6.246593952178955
Epoch 1620, val loss: 0.9777950644493103
Epoch 1630, training loss: 624.690185546875 = 0.06643219292163849 + 100.0 * 6.246237754821777
Epoch 1630, val loss: 0.9817759394645691
Epoch 1640, training loss: 625.3406982421875 = 0.06500986963510513 + 100.0 * 6.2527570724487305
Epoch 1640, val loss: 0.9855285882949829
Epoch 1650, training loss: 624.552734375 = 0.06354331970214844 + 100.0 * 6.244892120361328
Epoch 1650, val loss: 0.9893468618392944
Epoch 1660, training loss: 624.371826171875 = 0.06217281147837639 + 100.0 * 6.243096351623535
Epoch 1660, val loss: 0.9932616949081421
Epoch 1670, training loss: 624.4655151367188 = 0.06085841730237007 + 100.0 * 6.244046688079834
Epoch 1670, val loss: 0.9970327615737915
Epoch 1680, training loss: 625.2811279296875 = 0.05955561250448227 + 100.0 * 6.252215385437012
Epoch 1680, val loss: 1.0005453824996948
Epoch 1690, training loss: 624.7929077148438 = 0.05826730653643608 + 100.0 * 6.2473464012146
Epoch 1690, val loss: 1.0051203966140747
Epoch 1700, training loss: 624.5317993164062 = 0.057021308690309525 + 100.0 * 6.244747638702393
Epoch 1700, val loss: 1.0086992979049683
Epoch 1710, training loss: 624.5338134765625 = 0.05581650882959366 + 100.0 * 6.24478006362915
Epoch 1710, val loss: 1.012682318687439
Epoch 1720, training loss: 624.3526000976562 = 0.05464326590299606 + 100.0 * 6.242980003356934
Epoch 1720, val loss: 1.0164170265197754
Epoch 1730, training loss: 624.7888793945312 = 0.05350257456302643 + 100.0 * 6.247353553771973
Epoch 1730, val loss: 1.0203946828842163
Epoch 1740, training loss: 624.3511962890625 = 0.052371587604284286 + 100.0 * 6.242988109588623
Epoch 1740, val loss: 1.0236854553222656
Epoch 1750, training loss: 624.1482543945312 = 0.051296431571245193 + 100.0 * 6.240970134735107
Epoch 1750, val loss: 1.0277382135391235
Epoch 1760, training loss: 624.1683959960938 = 0.05025715008378029 + 100.0 * 6.24118185043335
Epoch 1760, val loss: 1.031677007675171
Epoch 1770, training loss: 624.6354370117188 = 0.04924187436699867 + 100.0 * 6.245861530303955
Epoch 1770, val loss: 1.0353314876556396
Epoch 1780, training loss: 624.8098754882812 = 0.04821174591779709 + 100.0 * 6.247616767883301
Epoch 1780, val loss: 1.0384495258331299
Epoch 1790, training loss: 624.2081909179688 = 0.04721768572926521 + 100.0 * 6.241609573364258
Epoch 1790, val loss: 1.042917013168335
Epoch 1800, training loss: 624.0458984375 = 0.04626718536019325 + 100.0 * 6.239996433258057
Epoch 1800, val loss: 1.0459858179092407
Epoch 1810, training loss: 624.0782470703125 = 0.04535207897424698 + 100.0 * 6.240329265594482
Epoch 1810, val loss: 1.0497204065322876
Epoch 1820, training loss: 624.5098876953125 = 0.044457368552684784 + 100.0 * 6.244654655456543
Epoch 1820, val loss: 1.0534156560897827
Epoch 1830, training loss: 624.2813720703125 = 0.043553680181503296 + 100.0 * 6.242377758026123
Epoch 1830, val loss: 1.057141900062561
Epoch 1840, training loss: 624.0150146484375 = 0.04269276559352875 + 100.0 * 6.2397236824035645
Epoch 1840, val loss: 1.0606309175491333
Epoch 1850, training loss: 623.9500732421875 = 0.041860293596982956 + 100.0 * 6.239081859588623
Epoch 1850, val loss: 1.0644410848617554
Epoch 1860, training loss: 624.3432006835938 = 0.04105202108621597 + 100.0 * 6.243021011352539
Epoch 1860, val loss: 1.0679138898849487
Epoch 1870, training loss: 624.0745239257812 = 0.040239859372377396 + 100.0 * 6.24034309387207
Epoch 1870, val loss: 1.0711559057235718
Epoch 1880, training loss: 623.8047485351562 = 0.039465274661779404 + 100.0 * 6.23765230178833
Epoch 1880, val loss: 1.0748895406723022
Epoch 1890, training loss: 623.9124755859375 = 0.03871263191103935 + 100.0 * 6.2387375831604
Epoch 1890, val loss: 1.0782390832901
Epoch 1900, training loss: 624.161376953125 = 0.037974707782268524 + 100.0 * 6.241233825683594
Epoch 1900, val loss: 1.0816365480422974
Epoch 1910, training loss: 623.93798828125 = 0.037255920469760895 + 100.0 * 6.239007472991943
Epoch 1910, val loss: 1.0856280326843262
Epoch 1920, training loss: 624.086669921875 = 0.036550477147102356 + 100.0 * 6.2405009269714355
Epoch 1920, val loss: 1.0887316465377808
Epoch 1930, training loss: 623.5867309570312 = 0.03585831820964813 + 100.0 * 6.235508441925049
Epoch 1930, val loss: 1.092193365097046
Epoch 1940, training loss: 623.5783081054688 = 0.0352020226418972 + 100.0 * 6.235430717468262
Epoch 1940, val loss: 1.0955148935317993
Epoch 1950, training loss: 623.6343383789062 = 0.03456290811300278 + 100.0 * 6.235997676849365
Epoch 1950, val loss: 1.0990294218063354
Epoch 1960, training loss: 623.7344360351562 = 0.03393564000725746 + 100.0 * 6.23700475692749
Epoch 1960, val loss: 1.1023865938186646
Epoch 1970, training loss: 623.676513671875 = 0.03332272917032242 + 100.0 * 6.236432075500488
Epoch 1970, val loss: 1.1058870553970337
Epoch 1980, training loss: 624.406005859375 = 0.032723624259233475 + 100.0 * 6.243732929229736
Epoch 1980, val loss: 1.1093618869781494
Epoch 1990, training loss: 623.4493408203125 = 0.03209066018462181 + 100.0 * 6.234172344207764
Epoch 1990, val loss: 1.1123337745666504
Epoch 2000, training loss: 623.4155883789062 = 0.03151223435997963 + 100.0 * 6.2338409423828125
Epoch 2000, val loss: 1.1155582666397095
Epoch 2010, training loss: 623.36279296875 = 0.030969785526394844 + 100.0 * 6.233318328857422
Epoch 2010, val loss: 1.1188960075378418
Epoch 2020, training loss: 623.48828125 = 0.03043323941528797 + 100.0 * 6.2345781326293945
Epoch 2020, val loss: 1.1219412088394165
Epoch 2030, training loss: 623.8400268554688 = 0.029897361993789673 + 100.0 * 6.238101482391357
Epoch 2030, val loss: 1.125368595123291
Epoch 2040, training loss: 623.4356689453125 = 0.02936544641852379 + 100.0 * 6.234063148498535
Epoch 2040, val loss: 1.1287317276000977
Epoch 2050, training loss: 623.2216796875 = 0.028858352452516556 + 100.0 * 6.23192834854126
Epoch 2050, val loss: 1.1318025588989258
Epoch 2060, training loss: 623.279052734375 = 0.02837672084569931 + 100.0 * 6.23250675201416
Epoch 2060, val loss: 1.1349279880523682
Epoch 2070, training loss: 624.1680297851562 = 0.027890434488654137 + 100.0 * 6.241401195526123
Epoch 2070, val loss: 1.1377657651901245
Epoch 2080, training loss: 623.24853515625 = 0.02739708311855793 + 100.0 * 6.232211112976074
Epoch 2080, val loss: 1.1412180662155151
Epoch 2090, training loss: 623.1900634765625 = 0.02693893574178219 + 100.0 * 6.231631278991699
Epoch 2090, val loss: 1.1443818807601929
Epoch 2100, training loss: 623.1231079101562 = 0.02649899758398533 + 100.0 * 6.230966091156006
Epoch 2100, val loss: 1.1474992036819458
Epoch 2110, training loss: 623.4332275390625 = 0.026076920330524445 + 100.0 * 6.234071731567383
Epoch 2110, val loss: 1.1506555080413818
Epoch 2120, training loss: 623.29345703125 = 0.02563566155731678 + 100.0 * 6.232678413391113
Epoch 2120, val loss: 1.1533715724945068
Epoch 2130, training loss: 623.1822509765625 = 0.025207113474607468 + 100.0 * 6.231570720672607
Epoch 2130, val loss: 1.1562559604644775
Epoch 2140, training loss: 623.0676879882812 = 0.024797193706035614 + 100.0 * 6.230429172515869
Epoch 2140, val loss: 1.1596081256866455
Epoch 2150, training loss: 623.289794921875 = 0.024407492950558662 + 100.0 * 6.232654094696045
Epoch 2150, val loss: 1.1623427867889404
Epoch 2160, training loss: 623.1482543945312 = 0.024011662229895592 + 100.0 * 6.231242656707764
Epoch 2160, val loss: 1.165382981300354
Epoch 2170, training loss: 623.1021728515625 = 0.023642919957637787 + 100.0 * 6.230785369873047
Epoch 2170, val loss: 1.1683571338653564
Epoch 2180, training loss: 623.1952514648438 = 0.023268403485417366 + 100.0 * 6.231719970703125
Epoch 2180, val loss: 1.1711111068725586
Epoch 2190, training loss: 622.9638061523438 = 0.02289663627743721 + 100.0 * 6.229409217834473
Epoch 2190, val loss: 1.174073338508606
Epoch 2200, training loss: 623.2958374023438 = 0.02254658378660679 + 100.0 * 6.232733249664307
Epoch 2200, val loss: 1.1772840023040771
Epoch 2210, training loss: 622.94970703125 = 0.022186577320098877 + 100.0 * 6.229274749755859
Epoch 2210, val loss: 1.1796913146972656
Epoch 2220, training loss: 622.8904418945312 = 0.021842211484909058 + 100.0 * 6.2286858558654785
Epoch 2220, val loss: 1.182534098625183
Epoch 2230, training loss: 622.8605346679688 = 0.021512610837817192 + 100.0 * 6.228390216827393
Epoch 2230, val loss: 1.1852947473526
Epoch 2240, training loss: 623.1031494140625 = 0.02119397185742855 + 100.0 * 6.2308197021484375
Epoch 2240, val loss: 1.1883443593978882
Epoch 2250, training loss: 622.8470458984375 = 0.020867405459284782 + 100.0 * 6.228261470794678
Epoch 2250, val loss: 1.1905184984207153
Epoch 2260, training loss: 623.2792358398438 = 0.020555147901177406 + 100.0 * 6.23258638381958
Epoch 2260, val loss: 1.193176031112671
Epoch 2270, training loss: 622.8279418945312 = 0.02024976536631584 + 100.0 * 6.228076934814453
Epoch 2270, val loss: 1.1959915161132812
Epoch 2280, training loss: 622.7675170898438 = 0.019952325150370598 + 100.0 * 6.227475643157959
Epoch 2280, val loss: 1.1983263492584229
Epoch 2290, training loss: 623.2465209960938 = 0.019662652164697647 + 100.0 * 6.232268810272217
Epoch 2290, val loss: 1.2009670734405518
Epoch 2300, training loss: 622.88037109375 = 0.01937435008585453 + 100.0 * 6.228610515594482
Epoch 2300, val loss: 1.204105257987976
Epoch 2310, training loss: 622.898681640625 = 0.01909518800675869 + 100.0 * 6.228796005249023
Epoch 2310, val loss: 1.2062150239944458
Epoch 2320, training loss: 622.756103515625 = 0.018820378929376602 + 100.0 * 6.227373123168945
Epoch 2320, val loss: 1.2089817523956299
Epoch 2330, training loss: 622.549560546875 = 0.018553271889686584 + 100.0 * 6.225310325622559
Epoch 2330, val loss: 1.2112939357757568
Epoch 2340, training loss: 623.290771484375 = 0.018300605937838554 + 100.0 * 6.232724666595459
Epoch 2340, val loss: 1.2137188911437988
Epoch 2350, training loss: 622.9683227539062 = 0.01803627982735634 + 100.0 * 6.2295026779174805
Epoch 2350, val loss: 1.21663498878479
Epoch 2360, training loss: 622.5750732421875 = 0.017776969820261 + 100.0 * 6.2255730628967285
Epoch 2360, val loss: 1.2185590267181396
Epoch 2370, training loss: 622.51904296875 = 0.017536137253046036 + 100.0 * 6.225015163421631
Epoch 2370, val loss: 1.2214499711990356
Epoch 2380, training loss: 622.7735595703125 = 0.017303762957453728 + 100.0 * 6.227561950683594
Epoch 2380, val loss: 1.2237359285354614
Epoch 2390, training loss: 622.7894287109375 = 0.017066124826669693 + 100.0 * 6.227723598480225
Epoch 2390, val loss: 1.2264198064804077
Epoch 2400, training loss: 622.6441040039062 = 0.016827750951051712 + 100.0 * 6.2262725830078125
Epoch 2400, val loss: 1.2288272380828857
Epoch 2410, training loss: 622.52978515625 = 0.01659434102475643 + 100.0 * 6.225131988525391
Epoch 2410, val loss: 1.2310173511505127
Epoch 2420, training loss: 622.8449096679688 = 0.01637588068842888 + 100.0 * 6.228285312652588
Epoch 2420, val loss: 1.233130693435669
Epoch 2430, training loss: 622.4100341796875 = 0.01615471951663494 + 100.0 * 6.223938465118408
Epoch 2430, val loss: 1.2356845140457153
Epoch 2440, training loss: 622.3788452148438 = 0.0159460436552763 + 100.0 * 6.223628997802734
Epoch 2440, val loss: 1.2379404306411743
Epoch 2450, training loss: 622.27099609375 = 0.01574167236685753 + 100.0 * 6.22255277633667
Epoch 2450, val loss: 1.240265965461731
Epoch 2460, training loss: 622.6320190429688 = 0.015547574497759342 + 100.0 * 6.2261643409729
Epoch 2460, val loss: 1.2427347898483276
Epoch 2470, training loss: 622.7273559570312 = 0.015343423001468182 + 100.0 * 6.227120399475098
Epoch 2470, val loss: 1.244876503944397
Epoch 2480, training loss: 622.2760620117188 = 0.015135484747588634 + 100.0 * 6.222609043121338
Epoch 2480, val loss: 1.2469496726989746
Epoch 2490, training loss: 622.9180297851562 = 0.014944115653634071 + 100.0 * 6.229030609130859
Epoch 2490, val loss: 1.2492120265960693
Epoch 2500, training loss: 622.2425537109375 = 0.014747015200555325 + 100.0 * 6.222278118133545
Epoch 2500, val loss: 1.2512410879135132
Epoch 2510, training loss: 622.2599487304688 = 0.014565078541636467 + 100.0 * 6.222453594207764
Epoch 2510, val loss: 1.2531988620758057
Epoch 2520, training loss: 622.2925415039062 = 0.014386910945177078 + 100.0 * 6.222781658172607
Epoch 2520, val loss: 1.2554470300674438
Epoch 2530, training loss: 622.826416015625 = 0.014214957132935524 + 100.0 * 6.228121757507324
Epoch 2530, val loss: 1.257253885269165
Epoch 2540, training loss: 622.12744140625 = 0.014033656567335129 + 100.0 * 6.221134185791016
Epoch 2540, val loss: 1.2600834369659424
Epoch 2550, training loss: 622.156005859375 = 0.013866803608834743 + 100.0 * 6.221421241760254
Epoch 2550, val loss: 1.2619420289993286
Epoch 2560, training loss: 622.4815063476562 = 0.013701307587325573 + 100.0 * 6.2246785163879395
Epoch 2560, val loss: 1.2640631198883057
Epoch 2570, training loss: 622.350830078125 = 0.013535046018660069 + 100.0 * 6.223372936248779
Epoch 2570, val loss: 1.2657634019851685
Epoch 2580, training loss: 622.2299194335938 = 0.013371310196816921 + 100.0 * 6.222165584564209
Epoch 2580, val loss: 1.2675259113311768
Epoch 2590, training loss: 622.6924438476562 = 0.013217740692198277 + 100.0 * 6.226791858673096
Epoch 2590, val loss: 1.2698971033096313
Epoch 2600, training loss: 622.13427734375 = 0.01305454969406128 + 100.0 * 6.221212387084961
Epoch 2600, val loss: 1.2714641094207764
Epoch 2610, training loss: 622.0020141601562 = 0.01290131825953722 + 100.0 * 6.21989107131958
Epoch 2610, val loss: 1.2735824584960938
Epoch 2620, training loss: 621.9981689453125 = 0.012755547650158405 + 100.0 * 6.21985387802124
Epoch 2620, val loss: 1.2756925821304321
Epoch 2630, training loss: 622.2576904296875 = 0.012617510743439198 + 100.0 * 6.2224507331848145
Epoch 2630, val loss: 1.2776631116867065
Epoch 2640, training loss: 622.3052978515625 = 0.012468664906919003 + 100.0 * 6.222928524017334
Epoch 2640, val loss: 1.2795228958129883
Epoch 2650, training loss: 622.33203125 = 0.012327338568866253 + 100.0 * 6.223196983337402
Epoch 2650, val loss: 1.2809079885482788
Epoch 2660, training loss: 622.3165283203125 = 0.012181799858808517 + 100.0 * 6.223043441772461
Epoch 2660, val loss: 1.28315007686615
Epoch 2670, training loss: 621.9682006835938 = 0.012041712179780006 + 100.0 * 6.21956205368042
Epoch 2670, val loss: 1.2850062847137451
Epoch 2680, training loss: 622.04296875 = 0.011911923065781593 + 100.0 * 6.220310688018799
Epoch 2680, val loss: 1.287127137184143
Epoch 2690, training loss: 622.1151123046875 = 0.011786828748881817 + 100.0 * 6.221033096313477
Epoch 2690, val loss: 1.2889907360076904
Epoch 2700, training loss: 622.2273559570312 = 0.011656065471470356 + 100.0 * 6.222157001495361
Epoch 2700, val loss: 1.290458083152771
Epoch 2710, training loss: 621.9273681640625 = 0.011522061191499233 + 100.0 * 6.219158172607422
Epoch 2710, val loss: 1.2918827533721924
Epoch 2720, training loss: 621.979736328125 = 0.01140054315328598 + 100.0 * 6.2196831703186035
Epoch 2720, val loss: 1.2938803434371948
Epoch 2730, training loss: 621.987548828125 = 0.011281851679086685 + 100.0 * 6.219762325286865
Epoch 2730, val loss: 1.295612096786499
Epoch 2740, training loss: 622.4137573242188 = 0.011162377893924713 + 100.0 * 6.224025726318359
Epoch 2740, val loss: 1.2970373630523682
Epoch 2750, training loss: 622.0746459960938 = 0.011036721989512444 + 100.0 * 6.220635890960693
Epoch 2750, val loss: 1.2986907958984375
Epoch 2760, training loss: 621.81005859375 = 0.010919157415628433 + 100.0 * 6.217991352081299
Epoch 2760, val loss: 1.3004423379898071
Epoch 2770, training loss: 621.8772583007812 = 0.010807763785123825 + 100.0 * 6.218664646148682
Epoch 2770, val loss: 1.302470326423645
Epoch 2780, training loss: 622.3795776367188 = 0.010701067745685577 + 100.0 * 6.223689079284668
Epoch 2780, val loss: 1.3040661811828613
Epoch 2790, training loss: 622.2105102539062 = 0.010585948824882507 + 100.0 * 6.221999645233154
Epoch 2790, val loss: 1.3054485321044922
Epoch 2800, training loss: 621.8471069335938 = 0.010474123060703278 + 100.0 * 6.2183661460876465
Epoch 2800, val loss: 1.306938886642456
Epoch 2810, training loss: 621.8123779296875 = 0.010368826799094677 + 100.0 * 6.218019962310791
Epoch 2810, val loss: 1.308301568031311
Epoch 2820, training loss: 621.8977661132812 = 0.0102674076333642 + 100.0 * 6.218875408172607
Epoch 2820, val loss: 1.3098386526107788
Epoch 2830, training loss: 621.6406860351562 = 0.010162691585719585 + 100.0 * 6.216304779052734
Epoch 2830, val loss: 1.31180739402771
Epoch 2840, training loss: 621.7606811523438 = 0.010069315321743488 + 100.0 * 6.217505931854248
Epoch 2840, val loss: 1.3135969638824463
Epoch 2850, training loss: 622.6475830078125 = 0.00997384637594223 + 100.0 * 6.226376056671143
Epoch 2850, val loss: 1.3150947093963623
Epoch 2860, training loss: 621.892822265625 = 0.009859266690909863 + 100.0 * 6.218829154968262
Epoch 2860, val loss: 1.3162744045257568
Epoch 2870, training loss: 621.6541137695312 = 0.009764439426362514 + 100.0 * 6.2164435386657715
Epoch 2870, val loss: 1.3176935911178589
Epoch 2880, training loss: 621.6807250976562 = 0.009672928601503372 + 100.0 * 6.216711044311523
Epoch 2880, val loss: 1.3194694519042969
Epoch 2890, training loss: 621.8265991210938 = 0.009583125822246075 + 100.0 * 6.218170166015625
Epoch 2890, val loss: 1.3209666013717651
Epoch 2900, training loss: 621.5364379882812 = 0.009487733244895935 + 100.0 * 6.215270042419434
Epoch 2900, val loss: 1.3224622011184692
Epoch 2910, training loss: 621.7366943359375 = 0.009403013624250889 + 100.0 * 6.217272758483887
Epoch 2910, val loss: 1.3239024877548218
Epoch 2920, training loss: 622.2455444335938 = 0.009318658150732517 + 100.0 * 6.222362041473389
Epoch 2920, val loss: 1.3250733613967896
Epoch 2930, training loss: 621.7894897460938 = 0.009223638102412224 + 100.0 * 6.217803001403809
Epoch 2930, val loss: 1.326187014579773
Epoch 2940, training loss: 621.6859130859375 = 0.009135181084275246 + 100.0 * 6.21676778793335
Epoch 2940, val loss: 1.3277908563613892
Epoch 2950, training loss: 621.5303344726562 = 0.009050504304468632 + 100.0 * 6.215212821960449
Epoch 2950, val loss: 1.329620599746704
Epoch 2960, training loss: 621.411376953125 = 0.008970075286924839 + 100.0 * 6.214024066925049
Epoch 2960, val loss: 1.3308660984039307
Epoch 2970, training loss: 621.521728515625 = 0.00889354944229126 + 100.0 * 6.215127944946289
Epoch 2970, val loss: 1.332398533821106
Epoch 2980, training loss: 622.0646362304688 = 0.008815539069473743 + 100.0 * 6.2205586433410645
Epoch 2980, val loss: 1.3336215019226074
Epoch 2990, training loss: 621.7109985351562 = 0.00873321108520031 + 100.0 * 6.21702241897583
Epoch 2990, val loss: 1.3346874713897705
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8223510806536637
=== training gcn model ===
Epoch 0, training loss: 861.6294555664062 = 1.9429703950881958 + 100.0 * 8.596864700317383
Epoch 0, val loss: 1.9360934495925903
Epoch 10, training loss: 861.5594482421875 = 1.9351667165756226 + 100.0 * 8.596242904663086
Epoch 10, val loss: 1.9284650087356567
Epoch 20, training loss: 861.0962524414062 = 1.925048828125 + 100.0 * 8.59171199798584
Epoch 20, val loss: 1.918208360671997
Epoch 30, training loss: 858.094970703125 = 1.911425232887268 + 100.0 * 8.561835289001465
Epoch 30, val loss: 1.9041352272033691
Epoch 40, training loss: 841.6814575195312 = 1.8933560848236084 + 100.0 * 8.397880554199219
Epoch 40, val loss: 1.8860172033309937
Epoch 50, training loss: 801.9888305664062 = 1.8703291416168213 + 100.0 * 8.001185417175293
Epoch 50, val loss: 1.8633793592453003
Epoch 60, training loss: 775.403076171875 = 1.848449945449829 + 100.0 * 7.735546112060547
Epoch 60, val loss: 1.844291090965271
Epoch 70, training loss: 744.9754028320312 = 1.8357633352279663 + 100.0 * 7.431396484375
Epoch 70, val loss: 1.8345856666564941
Epoch 80, training loss: 718.9466552734375 = 1.828153371810913 + 100.0 * 7.171185493469238
Epoch 80, val loss: 1.8284337520599365
Epoch 90, training loss: 703.3811645507812 = 1.8180420398712158 + 100.0 * 7.015631198883057
Epoch 90, val loss: 1.819911003112793
Epoch 100, training loss: 693.7745971679688 = 1.8079800605773926 + 100.0 * 6.919666290283203
Epoch 100, val loss: 1.811668872833252
Epoch 110, training loss: 686.9251708984375 = 1.7984637022018433 + 100.0 * 6.851266860961914
Epoch 110, val loss: 1.8036212921142578
Epoch 120, training loss: 681.9229125976562 = 1.7890959978103638 + 100.0 * 6.801337718963623
Epoch 120, val loss: 1.795586347579956
Epoch 130, training loss: 677.5362548828125 = 1.7804219722747803 + 100.0 * 6.757558345794678
Epoch 130, val loss: 1.788053035736084
Epoch 140, training loss: 673.3699340820312 = 1.7717573642730713 + 100.0 * 6.715981960296631
Epoch 140, val loss: 1.7805371284484863
Epoch 150, training loss: 669.62939453125 = 1.762553334236145 + 100.0 * 6.678668022155762
Epoch 150, val loss: 1.7727574110031128
Epoch 160, training loss: 666.1522216796875 = 1.7527828216552734 + 100.0 * 6.643994331359863
Epoch 160, val loss: 1.764562964439392
Epoch 170, training loss: 663.4064331054688 = 1.7422447204589844 + 100.0 * 6.616641998291016
Epoch 170, val loss: 1.755786418914795
Epoch 180, training loss: 661.2316284179688 = 1.7305270433425903 + 100.0 * 6.595010757446289
Epoch 180, val loss: 1.7462239265441895
Epoch 190, training loss: 659.1122436523438 = 1.717676043510437 + 100.0 * 6.573945999145508
Epoch 190, val loss: 1.735737919807434
Epoch 200, training loss: 657.450439453125 = 1.7037479877471924 + 100.0 * 6.557466983795166
Epoch 200, val loss: 1.7243798971176147
Epoch 210, training loss: 655.877197265625 = 1.6887762546539307 + 100.0 * 6.541883945465088
Epoch 210, val loss: 1.7122172117233276
Epoch 220, training loss: 654.5078125 = 1.6727972030639648 + 100.0 * 6.528350353240967
Epoch 220, val loss: 1.6991647481918335
Epoch 230, training loss: 653.3450317382812 = 1.6557633876800537 + 100.0 * 6.516892910003662
Epoch 230, val loss: 1.6853753328323364
Epoch 240, training loss: 651.8436279296875 = 1.637751817703247 + 100.0 * 6.502058982849121
Epoch 240, val loss: 1.6707639694213867
Epoch 250, training loss: 650.5868530273438 = 1.6188836097717285 + 100.0 * 6.48967981338501
Epoch 250, val loss: 1.6554807424545288
Epoch 260, training loss: 649.7704467773438 = 1.59922194480896 + 100.0 * 6.4817118644714355
Epoch 260, val loss: 1.6396255493164062
Epoch 270, training loss: 648.6526489257812 = 1.5786296129226685 + 100.0 * 6.47074031829834
Epoch 270, val loss: 1.6230051517486572
Epoch 280, training loss: 647.8020629882812 = 1.5575731992721558 + 100.0 * 6.462445259094238
Epoch 280, val loss: 1.6060975790023804
Epoch 290, training loss: 646.8797607421875 = 1.5361284017562866 + 100.0 * 6.453436374664307
Epoch 290, val loss: 1.5889431238174438
Epoch 300, training loss: 647.24609375 = 1.5143952369689941 + 100.0 * 6.457316875457764
Epoch 300, val loss: 1.571494221687317
Epoch 310, training loss: 645.4371948242188 = 1.492415189743042 + 100.0 * 6.43944787979126
Epoch 310, val loss: 1.5541763305664062
Epoch 320, training loss: 644.7691650390625 = 1.470505714416504 + 100.0 * 6.432986259460449
Epoch 320, val loss: 1.536991834640503
Epoch 330, training loss: 644.104248046875 = 1.448721170425415 + 100.0 * 6.426555156707764
Epoch 330, val loss: 1.5199934244155884
Epoch 340, training loss: 643.9337158203125 = 1.4270015954971313 + 100.0 * 6.425066947937012
Epoch 340, val loss: 1.5031852722167969
Epoch 350, training loss: 643.0731201171875 = 1.405394196510315 + 100.0 * 6.416677474975586
Epoch 350, val loss: 1.4866578578948975
Epoch 360, training loss: 642.3154296875 = 1.3839752674102783 + 100.0 * 6.4093146324157715
Epoch 360, val loss: 1.4705066680908203
Epoch 370, training loss: 642.4118041992188 = 1.362756371498108 + 100.0 * 6.410490989685059
Epoch 370, val loss: 1.4545962810516357
Epoch 380, training loss: 641.368896484375 = 1.3414897918701172 + 100.0 * 6.40027379989624
Epoch 380, val loss: 1.4388504028320312
Epoch 390, training loss: 640.866943359375 = 1.320378065109253 + 100.0 * 6.395465850830078
Epoch 390, val loss: 1.4233559370040894
Epoch 400, training loss: 640.3001708984375 = 1.2994447946548462 + 100.0 * 6.390007495880127
Epoch 400, val loss: 1.4081436395645142
Epoch 410, training loss: 640.238037109375 = 1.278568148612976 + 100.0 * 6.389595031738281
Epoch 410, val loss: 1.3931280374526978
Epoch 420, training loss: 639.61767578125 = 1.2576861381530762 + 100.0 * 6.383599758148193
Epoch 420, val loss: 1.3783400058746338
Epoch 430, training loss: 639.0220336914062 = 1.2368443012237549 + 100.0 * 6.377852439880371
Epoch 430, val loss: 1.3637938499450684
Epoch 440, training loss: 638.6966552734375 = 1.2161740064620972 + 100.0 * 6.374804496765137
Epoch 440, val loss: 1.349486231803894
Epoch 450, training loss: 638.4050903320312 = 1.1954869031906128 + 100.0 * 6.372096061706543
Epoch 450, val loss: 1.3351763486862183
Epoch 460, training loss: 638.0538330078125 = 1.1749595403671265 + 100.0 * 6.368788719177246
Epoch 460, val loss: 1.3215972185134888
Epoch 470, training loss: 637.5966186523438 = 1.1545159816741943 + 100.0 * 6.3644208908081055
Epoch 470, val loss: 1.3078688383102417
Epoch 480, training loss: 637.3016357421875 = 1.1343283653259277 + 100.0 * 6.361672878265381
Epoch 480, val loss: 1.2945656776428223
Epoch 490, training loss: 637.2338256835938 = 1.114347219467163 + 100.0 * 6.361195087432861
Epoch 490, val loss: 1.2818683385849
Epoch 500, training loss: 636.7069091796875 = 1.0945631265640259 + 100.0 * 6.356123447418213
Epoch 500, val loss: 1.2694830894470215
Epoch 510, training loss: 636.3333740234375 = 1.0750329494476318 + 100.0 * 6.352583885192871
Epoch 510, val loss: 1.2574042081832886
Epoch 520, training loss: 636.531005859375 = 1.0558241605758667 + 100.0 * 6.3547515869140625
Epoch 520, val loss: 1.2460471391677856
Epoch 530, training loss: 635.884521484375 = 1.036726474761963 + 100.0 * 6.348477840423584
Epoch 530, val loss: 1.2347168922424316
Epoch 540, training loss: 635.5070190429688 = 1.0180370807647705 + 100.0 * 6.3448896408081055
Epoch 540, val loss: 1.223929762840271
Epoch 550, training loss: 635.2978515625 = 0.9997909665107727 + 100.0 * 6.34298038482666
Epoch 550, val loss: 1.2138218879699707
Epoch 560, training loss: 635.3550415039062 = 0.9818331003189087 + 100.0 * 6.3437323570251465
Epoch 560, val loss: 1.2040601968765259
Epoch 570, training loss: 635.2885131835938 = 0.9641157388687134 + 100.0 * 6.3432440757751465
Epoch 570, val loss: 1.19477379322052
Epoch 580, training loss: 634.5225830078125 = 0.9467263221740723 + 100.0 * 6.335758686065674
Epoch 580, val loss: 1.1861892938613892
Epoch 590, training loss: 634.315673828125 = 0.9297329783439636 + 100.0 * 6.333859443664551
Epoch 590, val loss: 1.1781879663467407
Epoch 600, training loss: 634.0677490234375 = 0.9131420850753784 + 100.0 * 6.331546306610107
Epoch 600, val loss: 1.170667052268982
Epoch 610, training loss: 634.4673461914062 = 0.8967676758766174 + 100.0 * 6.335705280303955
Epoch 610, val loss: 1.1632506847381592
Epoch 620, training loss: 634.0032348632812 = 0.8806299567222595 + 100.0 * 6.331226348876953
Epoch 620, val loss: 1.1566150188446045
Epoch 630, training loss: 633.4896850585938 = 0.8648557662963867 + 100.0 * 6.3262481689453125
Epoch 630, val loss: 1.150186538696289
Epoch 640, training loss: 633.2052612304688 = 0.8495057225227356 + 100.0 * 6.3235578536987305
Epoch 640, val loss: 1.1444170475006104
Epoch 650, training loss: 633.532470703125 = 0.8344953060150146 + 100.0 * 6.326980113983154
Epoch 650, val loss: 1.1392345428466797
Epoch 660, training loss: 633.7034912109375 = 0.8195738196372986 + 100.0 * 6.32883882522583
Epoch 660, val loss: 1.1339807510375977
Epoch 670, training loss: 632.9756469726562 = 0.8049395084381104 + 100.0 * 6.321707248687744
Epoch 670, val loss: 1.12933349609375
Epoch 680, training loss: 632.6105346679688 = 0.790743350982666 + 100.0 * 6.318197727203369
Epoch 680, val loss: 1.1249958276748657
Epoch 690, training loss: 632.9592895507812 = 0.7768837213516235 + 100.0 * 6.321824073791504
Epoch 690, val loss: 1.121258020401001
Epoch 700, training loss: 632.552734375 = 0.7631369829177856 + 100.0 * 6.317896366119385
Epoch 700, val loss: 1.1178988218307495
Epoch 710, training loss: 632.079345703125 = 0.7496993541717529 + 100.0 * 6.313296794891357
Epoch 710, val loss: 1.1148957014083862
Epoch 720, training loss: 631.9378662109375 = 0.7365613579750061 + 100.0 * 6.312013149261475
Epoch 720, val loss: 1.1121799945831299
Epoch 730, training loss: 631.8949584960938 = 0.7235621809959412 + 100.0 * 6.311713695526123
Epoch 730, val loss: 1.1095563173294067
Epoch 740, training loss: 631.7355346679688 = 0.7107987999916077 + 100.0 * 6.31024694442749
Epoch 740, val loss: 1.1076260805130005
Epoch 750, training loss: 631.46533203125 = 0.698256254196167 + 100.0 * 6.307671070098877
Epoch 750, val loss: 1.1056843996047974
Epoch 760, training loss: 631.5241088867188 = 0.6859478950500488 + 100.0 * 6.3083815574646
Epoch 760, val loss: 1.1040188074111938
Epoch 770, training loss: 631.2882080078125 = 0.6736751198768616 + 100.0 * 6.306145668029785
Epoch 770, val loss: 1.1025727987289429
Epoch 780, training loss: 631.17919921875 = 0.6615678071975708 + 100.0 * 6.305176258087158
Epoch 780, val loss: 1.1011769771575928
Epoch 790, training loss: 631.1150512695312 = 0.6496502161026001 + 100.0 * 6.304654121398926
Epoch 790, val loss: 1.0995757579803467
Epoch 800, training loss: 630.8240356445312 = 0.6378756165504456 + 100.0 * 6.301861763000488
Epoch 800, val loss: 1.0991448163986206
Epoch 810, training loss: 630.85205078125 = 0.626174807548523 + 100.0 * 6.3022589683532715
Epoch 810, val loss: 1.0981203317642212
Epoch 820, training loss: 630.7637939453125 = 0.6147179007530212 + 100.0 * 6.301490306854248
Epoch 820, val loss: 1.0977404117584229
Epoch 830, training loss: 630.53173828125 = 0.6032587885856628 + 100.0 * 6.2992844581604
Epoch 830, val loss: 1.0967912673950195
Epoch 840, training loss: 630.4086303710938 = 0.5919000506401062 + 100.0 * 6.2981672286987305
Epoch 840, val loss: 1.0961554050445557
Epoch 850, training loss: 630.4447021484375 = 0.580733060836792 + 100.0 * 6.29863977432251
Epoch 850, val loss: 1.0960469245910645
Epoch 860, training loss: 630.09716796875 = 0.5694805383682251 + 100.0 * 6.295277118682861
Epoch 860, val loss: 1.0953080654144287
Epoch 870, training loss: 630.0697631835938 = 0.5583827495574951 + 100.0 * 6.295113563537598
Epoch 870, val loss: 1.0948878526687622
Epoch 880, training loss: 629.91650390625 = 0.5474282503128052 + 100.0 * 6.2936906814575195
Epoch 880, val loss: 1.0946269035339355
Epoch 890, training loss: 630.0528564453125 = 0.5365506410598755 + 100.0 * 6.295162677764893
Epoch 890, val loss: 1.094327688217163
Epoch 900, training loss: 630.1085815429688 = 0.5255887508392334 + 100.0 * 6.295829772949219
Epoch 900, val loss: 1.0939602851867676
Epoch 910, training loss: 629.5760498046875 = 0.5146856904029846 + 100.0 * 6.290613174438477
Epoch 910, val loss: 1.0937250852584839
Epoch 920, training loss: 629.50390625 = 0.5039823055267334 + 100.0 * 6.289999485015869
Epoch 920, val loss: 1.0935137271881104
Epoch 930, training loss: 629.3068237304688 = 0.4933634102344513 + 100.0 * 6.288135051727295
Epoch 930, val loss: 1.0932284593582153
Epoch 940, training loss: 630.07666015625 = 0.4827823340892792 + 100.0 * 6.295938491821289
Epoch 940, val loss: 1.0927555561065674
Epoch 950, training loss: 629.6329956054688 = 0.47228994965553284 + 100.0 * 6.291606903076172
Epoch 950, val loss: 1.0930447578430176
Epoch 960, training loss: 629.1945190429688 = 0.4617421329021454 + 100.0 * 6.287327766418457
Epoch 960, val loss: 1.0925772190093994
Epoch 970, training loss: 629.357177734375 = 0.4514472484588623 + 100.0 * 6.289056777954102
Epoch 970, val loss: 1.0926809310913086
Epoch 980, training loss: 628.9874877929688 = 0.4411424696445465 + 100.0 * 6.285463333129883
Epoch 980, val loss: 1.092166543006897
Epoch 990, training loss: 628.8372802734375 = 0.43098360300064087 + 100.0 * 6.28406286239624
Epoch 990, val loss: 1.0918933153152466
Epoch 1000, training loss: 629.0255737304688 = 0.42102500796318054 + 100.0 * 6.286045551300049
Epoch 1000, val loss: 1.0919231176376343
Epoch 1010, training loss: 628.7304077148438 = 0.4110984802246094 + 100.0 * 6.283193111419678
Epoch 1010, val loss: 1.0918678045272827
Epoch 1020, training loss: 628.5779418945312 = 0.40131518244743347 + 100.0 * 6.281766414642334
Epoch 1020, val loss: 1.091650366783142
Epoch 1030, training loss: 628.86767578125 = 0.3917298913002014 + 100.0 * 6.284759521484375
Epoch 1030, val loss: 1.091755747795105
Epoch 1040, training loss: 628.5379028320312 = 0.3822484314441681 + 100.0 * 6.281556606292725
Epoch 1040, val loss: 1.091783046722412
Epoch 1050, training loss: 628.3026733398438 = 0.3729567229747772 + 100.0 * 6.279296875
Epoch 1050, val loss: 1.0921916961669922
Epoch 1060, training loss: 628.8895263671875 = 0.363841712474823 + 100.0 * 6.285256862640381
Epoch 1060, val loss: 1.0924748182296753
Epoch 1070, training loss: 628.3506469726562 = 0.35470274090766907 + 100.0 * 6.279959678649902
Epoch 1070, val loss: 1.0916670560836792
Epoch 1080, training loss: 628.1478881835938 = 0.3459538519382477 + 100.0 * 6.278019428253174
Epoch 1080, val loss: 1.0927839279174805
Epoch 1090, training loss: 628.397216796875 = 0.33730146288871765 + 100.0 * 6.280599117279053
Epoch 1090, val loss: 1.0925573110580444
Epoch 1100, training loss: 628.078369140625 = 0.3288253843784332 + 100.0 * 6.277495861053467
Epoch 1100, val loss: 1.0933886766433716
Epoch 1110, training loss: 628.1897583007812 = 0.32058006525039673 + 100.0 * 6.278692245483398
Epoch 1110, val loss: 1.0939916372299194
Epoch 1120, training loss: 627.7289428710938 = 0.3123965859413147 + 100.0 * 6.274165630340576
Epoch 1120, val loss: 1.0943468809127808
Epoch 1130, training loss: 627.572265625 = 0.3045097589492798 + 100.0 * 6.272677421569824
Epoch 1130, val loss: 1.0950214862823486
Epoch 1140, training loss: 628.5968017578125 = 0.2968497574329376 + 100.0 * 6.282999515533447
Epoch 1140, val loss: 1.095529556274414
Epoch 1150, training loss: 627.80419921875 = 0.28926026821136475 + 100.0 * 6.275149822235107
Epoch 1150, val loss: 1.097208857536316
Epoch 1160, training loss: 627.3352661132812 = 0.2819059491157532 + 100.0 * 6.270533561706543
Epoch 1160, val loss: 1.097895622253418
Epoch 1170, training loss: 627.3121948242188 = 0.27480238676071167 + 100.0 * 6.270374298095703
Epoch 1170, val loss: 1.099381685256958
Epoch 1180, training loss: 627.5621337890625 = 0.2679041028022766 + 100.0 * 6.272942543029785
Epoch 1180, val loss: 1.1007905006408691
Epoch 1190, training loss: 627.3765258789062 = 0.26110661029815674 + 100.0 * 6.271153926849365
Epoch 1190, val loss: 1.1020829677581787
Epoch 1200, training loss: 627.5282592773438 = 0.2544281780719757 + 100.0 * 6.272737979888916
Epoch 1200, val loss: 1.1034512519836426
Epoch 1210, training loss: 627.2353515625 = 0.2480533868074417 + 100.0 * 6.269873142242432
Epoch 1210, val loss: 1.1052607297897339
Epoch 1220, training loss: 627.0233154296875 = 0.2418074756860733 + 100.0 * 6.267814636230469
Epoch 1220, val loss: 1.106994390487671
Epoch 1230, training loss: 627.1461791992188 = 0.23577547073364258 + 100.0 * 6.26910400390625
Epoch 1230, val loss: 1.1088485717773438
Epoch 1240, training loss: 626.9828491210938 = 0.22988978028297424 + 100.0 * 6.267529010772705
Epoch 1240, val loss: 1.111201286315918
Epoch 1250, training loss: 627.0677490234375 = 0.224129781126976 + 100.0 * 6.268436431884766
Epoch 1250, val loss: 1.1130964756011963
Epoch 1260, training loss: 626.8773803710938 = 0.2185354381799698 + 100.0 * 6.2665886878967285
Epoch 1260, val loss: 1.1152747869491577
Epoch 1270, training loss: 627.4334106445312 = 0.21311502158641815 + 100.0 * 6.272202491760254
Epoch 1270, val loss: 1.1174674034118652
Epoch 1280, training loss: 626.7052612304688 = 0.20785392820835114 + 100.0 * 6.264974117279053
Epoch 1280, val loss: 1.1205419301986694
Epoch 1290, training loss: 626.5115966796875 = 0.20271091163158417 + 100.0 * 6.263089179992676
Epoch 1290, val loss: 1.1227236986160278
Epoch 1300, training loss: 626.8778686523438 = 0.1977921575307846 + 100.0 * 6.266800403594971
Epoch 1300, val loss: 1.1259759664535522
Epoch 1310, training loss: 626.4793090820312 = 0.1928839385509491 + 100.0 * 6.262864589691162
Epoch 1310, val loss: 1.1279202699661255
Epoch 1320, training loss: 626.4015502929688 = 0.1881435066461563 + 100.0 * 6.262134075164795
Epoch 1320, val loss: 1.131111979484558
Epoch 1330, training loss: 626.3377685546875 = 0.1835641711950302 + 100.0 * 6.261541843414307
Epoch 1330, val loss: 1.1336889266967773
Epoch 1340, training loss: 626.8359375 = 0.17912013828754425 + 100.0 * 6.266568183898926
Epoch 1340, val loss: 1.1369022130966187
Epoch 1350, training loss: 626.2571411132812 = 0.17474843561649323 + 100.0 * 6.260824203491211
Epoch 1350, val loss: 1.1397805213928223
Epoch 1360, training loss: 626.342041015625 = 0.17052578926086426 + 100.0 * 6.261714935302734
Epoch 1360, val loss: 1.1429705619812012
Epoch 1370, training loss: 626.392333984375 = 0.16639527678489685 + 100.0 * 6.262259483337402
Epoch 1370, val loss: 1.1459351778030396
Epoch 1380, training loss: 626.0803833007812 = 0.1623452752828598 + 100.0 * 6.259180068969727
Epoch 1380, val loss: 1.149346947669983
Epoch 1390, training loss: 626.193115234375 = 0.15848718583583832 + 100.0 * 6.260345935821533
Epoch 1390, val loss: 1.1530617475509644
Epoch 1400, training loss: 626.078857421875 = 0.1546698361635208 + 100.0 * 6.259242057800293
Epoch 1400, val loss: 1.1561334133148193
Epoch 1410, training loss: 625.99462890625 = 0.15094438195228577 + 100.0 * 6.258437156677246
Epoch 1410, val loss: 1.1594231128692627
Epoch 1420, training loss: 626.080078125 = 0.1474102884531021 + 100.0 * 6.259326934814453
Epoch 1420, val loss: 1.1637508869171143
Epoch 1430, training loss: 626.155517578125 = 0.14387403428554535 + 100.0 * 6.2601165771484375
Epoch 1430, val loss: 1.166894793510437
Epoch 1440, training loss: 626.0675048828125 = 0.14044475555419922 + 100.0 * 6.259270668029785
Epoch 1440, val loss: 1.170401692390442
Epoch 1450, training loss: 625.9834594726562 = 0.1371406614780426 + 100.0 * 6.258462905883789
Epoch 1450, val loss: 1.1744792461395264
Epoch 1460, training loss: 625.6353149414062 = 0.1338789016008377 + 100.0 * 6.255014896392822
Epoch 1460, val loss: 1.177804946899414
Epoch 1470, training loss: 626.2539672851562 = 0.13074901700019836 + 100.0 * 6.261232376098633
Epoch 1470, val loss: 1.1815539598464966
Epoch 1480, training loss: 625.8174438476562 = 0.12765026092529297 + 100.0 * 6.256897926330566
Epoch 1480, val loss: 1.1855864524841309
Epoch 1490, training loss: 625.5306396484375 = 0.1246381625533104 + 100.0 * 6.2540602684021
Epoch 1490, val loss: 1.1895660161972046
Epoch 1500, training loss: 625.4592895507812 = 0.12174388766288757 + 100.0 * 6.253375053405762
Epoch 1500, val loss: 1.1936120986938477
Epoch 1510, training loss: 625.9757080078125 = 0.11891903728246689 + 100.0 * 6.258568286895752
Epoch 1510, val loss: 1.1976081132888794
Epoch 1520, training loss: 625.8657836914062 = 0.11615967005491257 + 100.0 * 6.257496356964111
Epoch 1520, val loss: 1.2020212411880493
Epoch 1530, training loss: 625.4564208984375 = 0.11340898275375366 + 100.0 * 6.253429889678955
Epoch 1530, val loss: 1.2056559324264526
Epoch 1540, training loss: 625.4638671875 = 0.11081460863351822 + 100.0 * 6.253530502319336
Epoch 1540, val loss: 1.2099454402923584
Epoch 1550, training loss: 625.720703125 = 0.10824999958276749 + 100.0 * 6.256124496459961
Epoch 1550, val loss: 1.2141941785812378
Epoch 1560, training loss: 625.3682250976562 = 0.10574211925268173 + 100.0 * 6.252624988555908
Epoch 1560, val loss: 1.2184441089630127
Epoch 1570, training loss: 625.4241333007812 = 0.1033235713839531 + 100.0 * 6.253208160400391
Epoch 1570, val loss: 1.22268545627594
Epoch 1580, training loss: 625.2617797851562 = 0.10095831751823425 + 100.0 * 6.251608371734619
Epoch 1580, val loss: 1.226967215538025
Epoch 1590, training loss: 625.1384887695312 = 0.09865221381187439 + 100.0 * 6.250398635864258
Epoch 1590, val loss: 1.2314289808273315
Epoch 1600, training loss: 625.3121948242188 = 0.09642022103071213 + 100.0 * 6.252157688140869
Epoch 1600, val loss: 1.2357456684112549
Epoch 1610, training loss: 625.481689453125 = 0.09421183913946152 + 100.0 * 6.2538743019104
Epoch 1610, val loss: 1.2396578788757324
Epoch 1620, training loss: 625.1522216796875 = 0.09209143370389938 + 100.0 * 6.250601291656494
Epoch 1620, val loss: 1.2448333501815796
Epoch 1630, training loss: 625.1920776367188 = 0.08998609334230423 + 100.0 * 6.251021385192871
Epoch 1630, val loss: 1.2483817338943481
Epoch 1640, training loss: 625.0487670898438 = 0.0879889726638794 + 100.0 * 6.249607563018799
Epoch 1640, val loss: 1.2534825801849365
Epoch 1650, training loss: 624.8822631835938 = 0.08601079136133194 + 100.0 * 6.247962951660156
Epoch 1650, val loss: 1.258107304573059
Epoch 1660, training loss: 625.1635131835938 = 0.08412636071443558 + 100.0 * 6.250793933868408
Epoch 1660, val loss: 1.263012409210205
Epoch 1670, training loss: 625.2806396484375 = 0.08222934603691101 + 100.0 * 6.251984119415283
Epoch 1670, val loss: 1.2671635150909424
Epoch 1680, training loss: 625.8865966796875 = 0.08041204512119293 + 100.0 * 6.25806188583374
Epoch 1680, val loss: 1.2722269296646118
Epoch 1690, training loss: 625.0520629882812 = 0.0785709023475647 + 100.0 * 6.249735355377197
Epoch 1690, val loss: 1.2757996320724487
Epoch 1700, training loss: 624.6378784179688 = 0.07685054838657379 + 100.0 * 6.245610237121582
Epoch 1700, val loss: 1.2809537649154663
Epoch 1710, training loss: 624.5745239257812 = 0.07518605887889862 + 100.0 * 6.244993209838867
Epoch 1710, val loss: 1.285576343536377
Epoch 1720, training loss: 624.5762939453125 = 0.07356654852628708 + 100.0 * 6.245027542114258
Epoch 1720, val loss: 1.2902615070343018
Epoch 1730, training loss: 625.9133911132812 = 0.07200910896062851 + 100.0 * 6.258413791656494
Epoch 1730, val loss: 1.2952064275741577
Epoch 1740, training loss: 624.9111328125 = 0.07038950175046921 + 100.0 * 6.248407363891602
Epoch 1740, val loss: 1.2994041442871094
Epoch 1750, training loss: 624.4519653320312 = 0.0688697099685669 + 100.0 * 6.243831157684326
Epoch 1750, val loss: 1.3041943311691284
Epoch 1760, training loss: 624.4016723632812 = 0.06740546226501465 + 100.0 * 6.243342876434326
Epoch 1760, val loss: 1.3087685108184814
Epoch 1770, training loss: 624.9588012695312 = 0.06599362939596176 + 100.0 * 6.248928070068359
Epoch 1770, val loss: 1.313018798828125
Epoch 1780, training loss: 624.7305297851562 = 0.06457135081291199 + 100.0 * 6.246659755706787
Epoch 1780, val loss: 1.3184585571289062
Epoch 1790, training loss: 624.5816650390625 = 0.06318224966526031 + 100.0 * 6.245184898376465
Epoch 1790, val loss: 1.3221938610076904
Epoch 1800, training loss: 624.3763427734375 = 0.061864983290433884 + 100.0 * 6.243144512176514
Epoch 1800, val loss: 1.3277394771575928
Epoch 1810, training loss: 624.2816772460938 = 0.060587961226701736 + 100.0 * 6.242210388183594
Epoch 1810, val loss: 1.3321528434753418
Epoch 1820, training loss: 624.8318481445312 = 0.05935230478644371 + 100.0 * 6.247725009918213
Epoch 1820, val loss: 1.3369535207748413
Epoch 1830, training loss: 624.6782836914062 = 0.05811307206749916 + 100.0 * 6.246201992034912
Epoch 1830, val loss: 1.3412063121795654
Epoch 1840, training loss: 624.458984375 = 0.05689442157745361 + 100.0 * 6.244020938873291
Epoch 1840, val loss: 1.3461883068084717
Epoch 1850, training loss: 624.3155517578125 = 0.055733464658260345 + 100.0 * 6.242598533630371
Epoch 1850, val loss: 1.3511018753051758
Epoch 1860, training loss: 624.1759033203125 = 0.05460122600197792 + 100.0 * 6.241212844848633
Epoch 1860, val loss: 1.3556299209594727
Epoch 1870, training loss: 624.3475952148438 = 0.053511716425418854 + 100.0 * 6.242940902709961
Epoch 1870, val loss: 1.3605071306228638
Epoch 1880, training loss: 624.5186767578125 = 0.05242448300123215 + 100.0 * 6.244662761688232
Epoch 1880, val loss: 1.3646674156188965
Epoch 1890, training loss: 624.4130859375 = 0.05136701837182045 + 100.0 * 6.243617057800293
Epoch 1890, val loss: 1.36918044090271
Epoch 1900, training loss: 624.171142578125 = 0.05035120248794556 + 100.0 * 6.241208076477051
Epoch 1900, val loss: 1.374423861503601
Epoch 1910, training loss: 624.1732177734375 = 0.04936249181628227 + 100.0 * 6.241238117218018
Epoch 1910, val loss: 1.3789266347885132
Epoch 1920, training loss: 624.3499145507812 = 0.048396192491054535 + 100.0 * 6.243015289306641
Epoch 1920, val loss: 1.383749008178711
Epoch 1930, training loss: 623.9830932617188 = 0.047437701374292374 + 100.0 * 6.239356994628906
Epoch 1930, val loss: 1.3877148628234863
Epoch 1940, training loss: 624.24951171875 = 0.04652109369635582 + 100.0 * 6.242030143737793
Epoch 1940, val loss: 1.3923945426940918
Epoch 1950, training loss: 624.2079467773438 = 0.04561898484826088 + 100.0 * 6.241623401641846
Epoch 1950, val loss: 1.3970909118652344
Epoch 1960, training loss: 623.9826049804688 = 0.04474765062332153 + 100.0 * 6.239378929138184
Epoch 1960, val loss: 1.402280330657959
Epoch 1970, training loss: 624.3013916015625 = 0.04390595480799675 + 100.0 * 6.242575168609619
Epoch 1970, val loss: 1.4069980382919312
Epoch 1980, training loss: 624.0294189453125 = 0.043063096702098846 + 100.0 * 6.239863395690918
Epoch 1980, val loss: 1.411302089691162
Epoch 1990, training loss: 624.2185668945312 = 0.04224275425076485 + 100.0 * 6.241763114929199
Epoch 1990, val loss: 1.416062831878662
Epoch 2000, training loss: 623.9390869140625 = 0.04145513102412224 + 100.0 * 6.23897647857666
Epoch 2000, val loss: 1.4204157590866089
Epoch 2010, training loss: 623.9085693359375 = 0.04066767171025276 + 100.0 * 6.2386794090271
Epoch 2010, val loss: 1.4243109226226807
Epoch 2020, training loss: 623.9724731445312 = 0.03992817923426628 + 100.0 * 6.239325523376465
Epoch 2020, val loss: 1.429458498954773
Epoch 2030, training loss: 623.764892578125 = 0.03919242322444916 + 100.0 * 6.23725700378418
Epoch 2030, val loss: 1.4344412088394165
Epoch 2040, training loss: 623.8795166015625 = 0.038477130234241486 + 100.0 * 6.238410949707031
Epoch 2040, val loss: 1.4383304119110107
Epoch 2050, training loss: 624.0335693359375 = 0.03777865692973137 + 100.0 * 6.239957809448242
Epoch 2050, val loss: 1.4426735639572144
Epoch 2060, training loss: 623.7658081054688 = 0.03708833083510399 + 100.0 * 6.2372870445251465
Epoch 2060, val loss: 1.4472594261169434
Epoch 2070, training loss: 623.9401245117188 = 0.03642120957374573 + 100.0 * 6.239037036895752
Epoch 2070, val loss: 1.4514368772506714
Epoch 2080, training loss: 623.9666137695312 = 0.035772182047367096 + 100.0 * 6.2393083572387695
Epoch 2080, val loss: 1.4560647010803223
Epoch 2090, training loss: 623.7559204101562 = 0.035134561359882355 + 100.0 * 6.237207889556885
Epoch 2090, val loss: 1.4605754613876343
Epoch 2100, training loss: 623.6451416015625 = 0.03451000526547432 + 100.0 * 6.2361063957214355
Epoch 2100, val loss: 1.4644602537155151
Epoch 2110, training loss: 623.5433349609375 = 0.033921018242836 + 100.0 * 6.23509407043457
Epoch 2110, val loss: 1.4692771434783936
Epoch 2120, training loss: 623.7156982421875 = 0.03334222361445427 + 100.0 * 6.236824035644531
Epoch 2120, val loss: 1.473501443862915
Epoch 2130, training loss: 623.7518310546875 = 0.03277093917131424 + 100.0 * 6.2371907234191895
Epoch 2130, val loss: 1.4786396026611328
Epoch 2140, training loss: 623.6002197265625 = 0.03220855072140694 + 100.0 * 6.235679626464844
Epoch 2140, val loss: 1.4825471639633179
Epoch 2150, training loss: 623.438232421875 = 0.03164433315396309 + 100.0 * 6.234066009521484
Epoch 2150, val loss: 1.4862291812896729
Epoch 2160, training loss: 623.8424072265625 = 0.031122026965022087 + 100.0 * 6.238112926483154
Epoch 2160, val loss: 1.4908055067062378
Epoch 2170, training loss: 623.3480834960938 = 0.030601371079683304 + 100.0 * 6.233174800872803
Epoch 2170, val loss: 1.4949724674224854
Epoch 2180, training loss: 623.5006103515625 = 0.030093977227807045 + 100.0 * 6.234704971313477
Epoch 2180, val loss: 1.4986282587051392
Epoch 2190, training loss: 623.5790405273438 = 0.029598476365208626 + 100.0 * 6.235494136810303
Epoch 2190, val loss: 1.5032424926757812
Epoch 2200, training loss: 623.2727661132812 = 0.02911369688808918 + 100.0 * 6.232436656951904
Epoch 2200, val loss: 1.5074843168258667
Epoch 2210, training loss: 623.4896850585938 = 0.028651520609855652 + 100.0 * 6.234610557556152
Epoch 2210, val loss: 1.5115796327590942
Epoch 2220, training loss: 623.6087036132812 = 0.028179775923490524 + 100.0 * 6.235805034637451
Epoch 2220, val loss: 1.5149823427200317
Epoch 2230, training loss: 623.5853881835938 = 0.027731291949748993 + 100.0 * 6.235576629638672
Epoch 2230, val loss: 1.5196609497070312
Epoch 2240, training loss: 623.3590087890625 = 0.027281368151307106 + 100.0 * 6.2333173751831055
Epoch 2240, val loss: 1.5237284898757935
Epoch 2250, training loss: 623.3975219726562 = 0.02685033157467842 + 100.0 * 6.233706951141357
Epoch 2250, val loss: 1.5275988578796387
Epoch 2260, training loss: 623.2306518554688 = 0.026428721845149994 + 100.0 * 6.23204231262207
Epoch 2260, val loss: 1.5315699577331543
Epoch 2270, training loss: 623.3123168945312 = 0.0260238666087389 + 100.0 * 6.232862949371338
Epoch 2270, val loss: 1.5355308055877686
Epoch 2280, training loss: 623.9321899414062 = 0.02563636191189289 + 100.0 * 6.239065647125244
Epoch 2280, val loss: 1.5405621528625488
Epoch 2290, training loss: 623.2728881835938 = 0.025205114856362343 + 100.0 * 6.232476711273193
Epoch 2290, val loss: 1.5429153442382812
Epoch 2300, training loss: 623.0448608398438 = 0.024824373424053192 + 100.0 * 6.230200290679932
Epoch 2300, val loss: 1.5476257801055908
Epoch 2310, training loss: 623.03466796875 = 0.024450860917568207 + 100.0 * 6.230102062225342
Epoch 2310, val loss: 1.5514755249023438
Epoch 2320, training loss: 623.677490234375 = 0.024095550179481506 + 100.0 * 6.2365336418151855
Epoch 2320, val loss: 1.5553432703018188
Epoch 2330, training loss: 623.0592041015625 = 0.023724710568785667 + 100.0 * 6.2303547859191895
Epoch 2330, val loss: 1.5590401887893677
Epoch 2340, training loss: 623.0664672851562 = 0.02336757257580757 + 100.0 * 6.230431079864502
Epoch 2340, val loss: 1.562336802482605
Epoch 2350, training loss: 623.0026245117188 = 0.023023173213005066 + 100.0 * 6.229796409606934
Epoch 2350, val loss: 1.5669209957122803
Epoch 2360, training loss: 623.1182861328125 = 0.022692622616887093 + 100.0 * 6.230956077575684
Epoch 2360, val loss: 1.5703409910202026
Epoch 2370, training loss: 623.3621826171875 = 0.022364990785717964 + 100.0 * 6.2333984375
Epoch 2370, val loss: 1.5735341310501099
Epoch 2380, training loss: 623.3351440429688 = 0.02204047702252865 + 100.0 * 6.233131408691406
Epoch 2380, val loss: 1.57787024974823
Epoch 2390, training loss: 623.1277465820312 = 0.02171575278043747 + 100.0 * 6.231060028076172
Epoch 2390, val loss: 1.5813114643096924
Epoch 2400, training loss: 622.937744140625 = 0.021405695006251335 + 100.0 * 6.22916316986084
Epoch 2400, val loss: 1.5849665403366089
Epoch 2410, training loss: 622.8948974609375 = 0.021102750673890114 + 100.0 * 6.228737831115723
Epoch 2410, val loss: 1.5886523723602295
Epoch 2420, training loss: 622.9596557617188 = 0.020811446011066437 + 100.0 * 6.229388236999512
Epoch 2420, val loss: 1.592284083366394
Epoch 2430, training loss: 622.9645385742188 = 0.020524507388472557 + 100.0 * 6.229440212249756
Epoch 2430, val loss: 1.5961753129959106
Epoch 2440, training loss: 623.1223754882812 = 0.020244421437382698 + 100.0 * 6.231020927429199
Epoch 2440, val loss: 1.6003057956695557
Epoch 2450, training loss: 623.3798217773438 = 0.019962547346949577 + 100.0 * 6.233598709106445
Epoch 2450, val loss: 1.603912591934204
Epoch 2460, training loss: 622.8781127929688 = 0.019673462957143784 + 100.0 * 6.2285847663879395
Epoch 2460, val loss: 1.6065218448638916
Epoch 2470, training loss: 622.7937622070312 = 0.01941107027232647 + 100.0 * 6.227743625640869
Epoch 2470, val loss: 1.6111397743225098
Epoch 2480, training loss: 622.8073120117188 = 0.019151071086525917 + 100.0 * 6.22788143157959
Epoch 2480, val loss: 1.6139858961105347
Epoch 2490, training loss: 622.769287109375 = 0.01889665052294731 + 100.0 * 6.227503776550293
Epoch 2490, val loss: 1.6175203323364258
Epoch 2500, training loss: 623.0648193359375 = 0.018651025369763374 + 100.0 * 6.230461597442627
Epoch 2500, val loss: 1.6206421852111816
Epoch 2510, training loss: 622.7327880859375 = 0.018388908356428146 + 100.0 * 6.227144241333008
Epoch 2510, val loss: 1.6238164901733398
Epoch 2520, training loss: 622.5823364257812 = 0.018146134912967682 + 100.0 * 6.225642204284668
Epoch 2520, val loss: 1.6279106140136719
Epoch 2530, training loss: 622.5386352539062 = 0.017910918220877647 + 100.0 * 6.225207328796387
Epoch 2530, val loss: 1.6310971975326538
Epoch 2540, training loss: 622.5780029296875 = 0.017688272520899773 + 100.0 * 6.225603103637695
Epoch 2540, val loss: 1.6344832181930542
Epoch 2550, training loss: 623.1868896484375 = 0.017479030415415764 + 100.0 * 6.231694221496582
Epoch 2550, val loss: 1.6379854679107666
Epoch 2560, training loss: 622.788330078125 = 0.01723848655819893 + 100.0 * 6.227711200714111
Epoch 2560, val loss: 1.6413758993148804
Epoch 2570, training loss: 622.9188842773438 = 0.01701849140226841 + 100.0 * 6.229018688201904
Epoch 2570, val loss: 1.6443443298339844
Epoch 2580, training loss: 622.9259033203125 = 0.016794592142105103 + 100.0 * 6.229091167449951
Epoch 2580, val loss: 1.647963285446167
Epoch 2590, training loss: 622.6410522460938 = 0.01657920889556408 + 100.0 * 6.2262444496154785
Epoch 2590, val loss: 1.6507658958435059
Epoch 2600, training loss: 622.4154052734375 = 0.016373885795474052 + 100.0 * 6.223990440368652
Epoch 2600, val loss: 1.6541892290115356
Epoch 2610, training loss: 622.4569091796875 = 0.016176624223589897 + 100.0 * 6.224407196044922
Epoch 2610, val loss: 1.6574801206588745
Epoch 2620, training loss: 623.437255859375 = 0.01599052920937538 + 100.0 * 6.234212398529053
Epoch 2620, val loss: 1.6606369018554688
Epoch 2630, training loss: 622.9277954101562 = 0.015780838206410408 + 100.0 * 6.229119777679443
Epoch 2630, val loss: 1.6629868745803833
Epoch 2640, training loss: 622.3671875 = 0.015581822022795677 + 100.0 * 6.22351598739624
Epoch 2640, val loss: 1.6669723987579346
Epoch 2650, training loss: 622.4707641601562 = 0.015397831797599792 + 100.0 * 6.22455358505249
Epoch 2650, val loss: 1.6698896884918213
Epoch 2660, training loss: 623.0236206054688 = 0.015218068845570087 + 100.0 * 6.230084419250488
Epoch 2660, val loss: 1.6724302768707275
Epoch 2670, training loss: 622.3720703125 = 0.015032648108899593 + 100.0 * 6.223570823669434
Epoch 2670, val loss: 1.6765779256820679
Epoch 2680, training loss: 622.2896728515625 = 0.014858650974929333 + 100.0 * 6.222748279571533
Epoch 2680, val loss: 1.6793431043624878
Epoch 2690, training loss: 622.4925537109375 = 0.014690577052533627 + 100.0 * 6.224778652191162
Epoch 2690, val loss: 1.6823408603668213
Epoch 2700, training loss: 622.8101196289062 = 0.014527018181979656 + 100.0 * 6.2279558181762695
Epoch 2700, val loss: 1.685713768005371
Epoch 2710, training loss: 622.3873901367188 = 0.014341545291244984 + 100.0 * 6.223730564117432
Epoch 2710, val loss: 1.688213586807251
Epoch 2720, training loss: 622.5011596679688 = 0.014178252778947353 + 100.0 * 6.224870204925537
Epoch 2720, val loss: 1.6917636394500732
Epoch 2730, training loss: 622.2362060546875 = 0.01401636190712452 + 100.0 * 6.222221851348877
Epoch 2730, val loss: 1.694244146347046
Epoch 2740, training loss: 622.32763671875 = 0.013861031271517277 + 100.0 * 6.223137855529785
Epoch 2740, val loss: 1.6971979141235352
Epoch 2750, training loss: 622.5812377929688 = 0.013711806386709213 + 100.0 * 6.225675106048584
Epoch 2750, val loss: 1.7010570764541626
Epoch 2760, training loss: 622.3762817382812 = 0.013555190525949001 + 100.0 * 6.223627090454102
Epoch 2760, val loss: 1.703608512878418
Epoch 2770, training loss: 622.3192749023438 = 0.013402644544839859 + 100.0 * 6.223058700561523
Epoch 2770, val loss: 1.7056663036346436
Epoch 2780, training loss: 622.4638061523438 = 0.013257334940135479 + 100.0 * 6.224505424499512
Epoch 2780, val loss: 1.7091041803359985
Epoch 2790, training loss: 622.3312377929688 = 0.013111607171595097 + 100.0 * 6.223181247711182
Epoch 2790, val loss: 1.711985468864441
Epoch 2800, training loss: 622.7066040039062 = 0.012972017750144005 + 100.0 * 6.226935863494873
Epoch 2800, val loss: 1.713858962059021
Epoch 2810, training loss: 622.2030029296875 = 0.012821830809116364 + 100.0 * 6.221901893615723
Epoch 2810, val loss: 1.7175880670547485
Epoch 2820, training loss: 622.1641845703125 = 0.012684260495007038 + 100.0 * 6.22151517868042
Epoch 2820, val loss: 1.7194534540176392
Epoch 2830, training loss: 622.1928100585938 = 0.012553142383694649 + 100.0 * 6.221802234649658
Epoch 2830, val loss: 1.7229441404342651
Epoch 2840, training loss: 622.4163818359375 = 0.012422263622283936 + 100.0 * 6.224039554595947
Epoch 2840, val loss: 1.7260704040527344
Epoch 2850, training loss: 622.1668090820312 = 0.012290711514651775 + 100.0 * 6.221545219421387
Epoch 2850, val loss: 1.7282220125198364
Epoch 2860, training loss: 622.5068969726562 = 0.012171970680356026 + 100.0 * 6.224947452545166
Epoch 2860, val loss: 1.732004165649414
Epoch 2870, training loss: 622.1505737304688 = 0.012034107930958271 + 100.0 * 6.221385478973389
Epoch 2870, val loss: 1.7332521677017212
Epoch 2880, training loss: 622.3103637695312 = 0.011908503249287605 + 100.0 * 6.222984790802002
Epoch 2880, val loss: 1.7361620664596558
Epoch 2890, training loss: 621.9944458007812 = 0.011787629686295986 + 100.0 * 6.219826698303223
Epoch 2890, val loss: 1.738877296447754
Epoch 2900, training loss: 621.9953002929688 = 0.011667164973914623 + 100.0 * 6.219836235046387
Epoch 2900, val loss: 1.7416499853134155
Epoch 2910, training loss: 622.4986572265625 = 0.011557457968592644 + 100.0 * 6.2248711585998535
Epoch 2910, val loss: 1.7442173957824707
Epoch 2920, training loss: 622.244873046875 = 0.011435323394834995 + 100.0 * 6.222334384918213
Epoch 2920, val loss: 1.7461977005004883
Epoch 2930, training loss: 622.0514526367188 = 0.011322210542857647 + 100.0 * 6.220400810241699
Epoch 2930, val loss: 1.750251293182373
Epoch 2940, training loss: 622.1065063476562 = 0.01120691653341055 + 100.0 * 6.22095251083374
Epoch 2940, val loss: 1.7520400285720825
Epoch 2950, training loss: 622.2147216796875 = 0.011100657284259796 + 100.0 * 6.222036361694336
Epoch 2950, val loss: 1.7549221515655518
Epoch 2960, training loss: 622.0394287109375 = 0.010988240130245686 + 100.0 * 6.220284461975098
Epoch 2960, val loss: 1.7568001747131348
Epoch 2970, training loss: 622.143798828125 = 0.010886818170547485 + 100.0 * 6.221329212188721
Epoch 2970, val loss: 1.7598888874053955
Epoch 2980, training loss: 622.423828125 = 0.010782930068671703 + 100.0 * 6.224130153656006
Epoch 2980, val loss: 1.762008547782898
Epoch 2990, training loss: 621.8839111328125 = 0.010671756230294704 + 100.0 * 6.2187323570251465
Epoch 2990, val loss: 1.7648428678512573
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8186610437532947
The final CL Acc:0.71235, 0.02572, The final GNN Acc:0.82024, 0.00155
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13144])
remove edge: torch.Size([2, 7986])
updated graph: torch.Size([2, 10574])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6500244140625 = 1.9668947458267212 + 100.0 * 8.596831321716309
Epoch 0, val loss: 1.9728081226348877
Epoch 10, training loss: 861.552978515625 = 1.9581283330917358 + 100.0 * 8.595948219299316
Epoch 10, val loss: 1.9635834693908691
Epoch 20, training loss: 860.9539184570312 = 1.9473168849945068 + 100.0 * 8.590065956115723
Epoch 20, val loss: 1.9521592855453491
Epoch 30, training loss: 857.0330200195312 = 1.9336501359939575 + 100.0 * 8.550993919372559
Epoch 30, val loss: 1.937623143196106
Epoch 40, training loss: 833.3651733398438 = 1.9165070056915283 + 100.0 * 8.314486503601074
Epoch 40, val loss: 1.9196633100509644
Epoch 50, training loss: 768.7099609375 = 1.8948004245758057 + 100.0 * 7.66815185546875
Epoch 50, val loss: 1.8973840475082397
Epoch 60, training loss: 751.7935180664062 = 1.876488447189331 + 100.0 * 7.499170303344727
Epoch 60, val loss: 1.880088448524475
Epoch 70, training loss: 731.8997802734375 = 1.8623157739639282 + 100.0 * 7.300374507904053
Epoch 70, val loss: 1.866405963897705
Epoch 80, training loss: 708.828369140625 = 1.8474535942077637 + 100.0 * 7.0698089599609375
Epoch 80, val loss: 1.8523480892181396
Epoch 90, training loss: 694.7970581054688 = 1.8360978364944458 + 100.0 * 6.929609298706055
Epoch 90, val loss: 1.8416345119476318
Epoch 100, training loss: 685.9761962890625 = 1.824234962463379 + 100.0 * 6.841519832611084
Epoch 100, val loss: 1.8301105499267578
Epoch 110, training loss: 678.5814208984375 = 1.8117847442626953 + 100.0 * 6.767696380615234
Epoch 110, val loss: 1.8181943893432617
Epoch 120, training loss: 673.4107666015625 = 1.8000617027282715 + 100.0 * 6.716107368469238
Epoch 120, val loss: 1.8069381713867188
Epoch 130, training loss: 669.25390625 = 1.7884759902954102 + 100.0 * 6.674654483795166
Epoch 130, val loss: 1.795768141746521
Epoch 140, training loss: 665.8402099609375 = 1.7767207622528076 + 100.0 * 6.640635013580322
Epoch 140, val loss: 1.7844887971878052
Epoch 150, training loss: 662.9359130859375 = 1.7646305561065674 + 100.0 * 6.61171293258667
Epoch 150, val loss: 1.7730309963226318
Epoch 160, training loss: 660.2145385742188 = 1.752078890800476 + 100.0 * 6.584624767303467
Epoch 160, val loss: 1.761382818222046
Epoch 170, training loss: 657.7966918945312 = 1.7388622760772705 + 100.0 * 6.560577869415283
Epoch 170, val loss: 1.749125599861145
Epoch 180, training loss: 656.0421752929688 = 1.7244750261306763 + 100.0 * 6.543177127838135
Epoch 180, val loss: 1.7360059022903442
Epoch 190, training loss: 653.8146362304688 = 1.7088991403579712 + 100.0 * 6.52105712890625
Epoch 190, val loss: 1.7217350006103516
Epoch 200, training loss: 652.1170654296875 = 1.6922470331192017 + 100.0 * 6.504248142242432
Epoch 200, val loss: 1.7066179513931274
Epoch 210, training loss: 650.4457397460938 = 1.6743971109390259 + 100.0 * 6.48771333694458
Epoch 210, val loss: 1.6904757022857666
Epoch 220, training loss: 648.9214477539062 = 1.655269742012024 + 100.0 * 6.472661972045898
Epoch 220, val loss: 1.6733077764511108
Epoch 230, training loss: 648.0968017578125 = 1.6346648931503296 + 100.0 * 6.464621543884277
Epoch 230, val loss: 1.6548984050750732
Epoch 240, training loss: 646.4103393554688 = 1.6126800775527954 + 100.0 * 6.447976589202881
Epoch 240, val loss: 1.6353429555892944
Epoch 250, training loss: 645.235107421875 = 1.5895031690597534 + 100.0 * 6.436456203460693
Epoch 250, val loss: 1.614747166633606
Epoch 260, training loss: 644.2681274414062 = 1.5649890899658203 + 100.0 * 6.42703104019165
Epoch 260, val loss: 1.5930246114730835
Epoch 270, training loss: 643.3406372070312 = 1.5393134355545044 + 100.0 * 6.418013572692871
Epoch 270, val loss: 1.5701717138290405
Epoch 280, training loss: 642.3988037109375 = 1.5126395225524902 + 100.0 * 6.4088616371154785
Epoch 280, val loss: 1.5466313362121582
Epoch 290, training loss: 641.57958984375 = 1.485167384147644 + 100.0 * 6.400944232940674
Epoch 290, val loss: 1.522478699684143
Epoch 300, training loss: 641.0718383789062 = 1.4568681716918945 + 100.0 * 6.396149635314941
Epoch 300, val loss: 1.4977213144302368
Epoch 310, training loss: 640.3157958984375 = 1.4281656742095947 + 100.0 * 6.388876438140869
Epoch 310, val loss: 1.4727182388305664
Epoch 320, training loss: 639.49169921875 = 1.3991330862045288 + 100.0 * 6.38092565536499
Epoch 320, val loss: 1.4476020336151123
Epoch 330, training loss: 639.127685546875 = 1.3700292110443115 + 100.0 * 6.3775763511657715
Epoch 330, val loss: 1.4226528406143188
Epoch 340, training loss: 638.555419921875 = 1.3408982753753662 + 100.0 * 6.372145175933838
Epoch 340, val loss: 1.3976600170135498
Epoch 350, training loss: 637.8991088867188 = 1.311790943145752 + 100.0 * 6.365873336791992
Epoch 350, val loss: 1.3730521202087402
Epoch 360, training loss: 637.5489501953125 = 1.282973289489746 + 100.0 * 6.362659931182861
Epoch 360, val loss: 1.3487277030944824
Epoch 370, training loss: 637.0194702148438 = 1.2541757822036743 + 100.0 * 6.3576531410217285
Epoch 370, val loss: 1.324540138244629
Epoch 380, training loss: 636.5882568359375 = 1.2256814241409302 + 100.0 * 6.353625297546387
Epoch 380, val loss: 1.3008041381835938
Epoch 390, training loss: 636.0980224609375 = 1.1974542140960693 + 100.0 * 6.349005699157715
Epoch 390, val loss: 1.277400255203247
Epoch 400, training loss: 635.6663208007812 = 1.169540286064148 + 100.0 * 6.344967365264893
Epoch 400, val loss: 1.2543331384658813
Epoch 410, training loss: 635.5482177734375 = 1.1419670581817627 + 100.0 * 6.344062805175781
Epoch 410, val loss: 1.231668472290039
Epoch 420, training loss: 635.2011108398438 = 1.1145853996276855 + 100.0 * 6.340865612030029
Epoch 420, val loss: 1.2092430591583252
Epoch 430, training loss: 634.7002563476562 = 1.0876517295837402 + 100.0 * 6.336126327514648
Epoch 430, val loss: 1.187394380569458
Epoch 440, training loss: 634.5399780273438 = 1.0610620975494385 + 100.0 * 6.334789276123047
Epoch 440, val loss: 1.1658416986465454
Epoch 450, training loss: 634.078857421875 = 1.034956455230713 + 100.0 * 6.330438613891602
Epoch 450, val loss: 1.1447495222091675
Epoch 460, training loss: 633.7449951171875 = 1.0093120336532593 + 100.0 * 6.327356815338135
Epoch 460, val loss: 1.1241464614868164
Epoch 470, training loss: 633.4478759765625 = 0.9842227101325989 + 100.0 * 6.324636459350586
Epoch 470, val loss: 1.1042510271072388
Epoch 480, training loss: 633.849609375 = 0.9596611857414246 + 100.0 * 6.328899383544922
Epoch 480, val loss: 1.0847656726837158
Epoch 490, training loss: 632.936767578125 = 0.9356071352958679 + 100.0 * 6.320011615753174
Epoch 490, val loss: 1.0660040378570557
Epoch 500, training loss: 632.73388671875 = 0.9122127890586853 + 100.0 * 6.318216323852539
Epoch 500, val loss: 1.047802448272705
Epoch 510, training loss: 632.4542846679688 = 0.8895018696784973 + 100.0 * 6.315647602081299
Epoch 510, val loss: 1.0305089950561523
Epoch 520, training loss: 632.392822265625 = 0.8674112558364868 + 100.0 * 6.315253734588623
Epoch 520, val loss: 1.013889193534851
Epoch 530, training loss: 632.4029541015625 = 0.8459078073501587 + 100.0 * 6.315570831298828
Epoch 530, val loss: 0.997693657875061
Epoch 540, training loss: 631.8471069335938 = 0.8250311613082886 + 100.0 * 6.310220718383789
Epoch 540, val loss: 0.9824053645133972
Epoch 550, training loss: 631.618896484375 = 0.80478435754776 + 100.0 * 6.308140754699707
Epoch 550, val loss: 0.967806875705719
Epoch 560, training loss: 631.7139282226562 = 0.7851645350456238 + 100.0 * 6.3092875480651855
Epoch 560, val loss: 0.9539099931716919
Epoch 570, training loss: 631.5162353515625 = 0.766050398349762 + 100.0 * 6.307501792907715
Epoch 570, val loss: 0.940704882144928
Epoch 580, training loss: 631.2247924804688 = 0.7474641799926758 + 100.0 * 6.304773807525635
Epoch 580, val loss: 0.9279650449752808
Epoch 590, training loss: 631.0285034179688 = 0.7294532656669617 + 100.0 * 6.302990436553955
Epoch 590, val loss: 0.9160916209220886
Epoch 600, training loss: 630.7481079101562 = 0.7119444608688354 + 100.0 * 6.3003621101379395
Epoch 600, val loss: 0.904474675655365
Epoch 610, training loss: 630.5419311523438 = 0.6948797106742859 + 100.0 * 6.298470497131348
Epoch 610, val loss: 0.8936812281608582
Epoch 620, training loss: 630.94775390625 = 0.6782390475273132 + 100.0 * 6.302695274353027
Epoch 620, val loss: 0.8833059668540955
Epoch 630, training loss: 630.19970703125 = 0.6619495749473572 + 100.0 * 6.295377731323242
Epoch 630, val loss: 0.8734620809555054
Epoch 640, training loss: 630.1547241210938 = 0.6460839509963989 + 100.0 * 6.29508638381958
Epoch 640, val loss: 0.8639349937438965
Epoch 650, training loss: 630.547607421875 = 0.630734384059906 + 100.0 * 6.299168586730957
Epoch 650, val loss: 0.855051577091217
Epoch 660, training loss: 629.8689575195312 = 0.6155300736427307 + 100.0 * 6.292534351348877
Epoch 660, val loss: 0.8466432094573975
Epoch 670, training loss: 629.6763916015625 = 0.6008515954017639 + 100.0 * 6.290755748748779
Epoch 670, val loss: 0.8385525345802307
Epoch 680, training loss: 629.5592041015625 = 0.586559534072876 + 100.0 * 6.289726734161377
Epoch 680, val loss: 0.8309611082077026
Epoch 690, training loss: 629.5159912109375 = 0.572540819644928 + 100.0 * 6.28943395614624
Epoch 690, val loss: 0.8236318826675415
Epoch 700, training loss: 629.5108642578125 = 0.5588585138320923 + 100.0 * 6.289520263671875
Epoch 700, val loss: 0.8167413473129272
Epoch 710, training loss: 629.1713256835938 = 0.545460045337677 + 100.0 * 6.286258697509766
Epoch 710, val loss: 0.81026291847229
Epoch 720, training loss: 629.2607421875 = 0.532342255115509 + 100.0 * 6.287283897399902
Epoch 720, val loss: 0.8038749098777771
Epoch 730, training loss: 628.9292602539062 = 0.5195614695549011 + 100.0 * 6.284097194671631
Epoch 730, val loss: 0.7981241941452026
Epoch 740, training loss: 628.6596069335938 = 0.5070025324821472 + 100.0 * 6.281525611877441
Epoch 740, val loss: 0.7924421429634094
Epoch 750, training loss: 628.5780639648438 = 0.49478188157081604 + 100.0 * 6.280832767486572
Epoch 750, val loss: 0.7872069478034973
Epoch 760, training loss: 629.0761108398438 = 0.48281461000442505 + 100.0 * 6.285933494567871
Epoch 760, val loss: 0.7822186946868896
Epoch 770, training loss: 628.6254272460938 = 0.4710264801979065 + 100.0 * 6.281544208526611
Epoch 770, val loss: 0.7773281931877136
Epoch 780, training loss: 628.4628295898438 = 0.4594367444515228 + 100.0 * 6.280034065246582
Epoch 780, val loss: 0.7725813388824463
Epoch 790, training loss: 628.1270751953125 = 0.448188841342926 + 100.0 * 6.27678918838501
Epoch 790, val loss: 0.7683373689651489
Epoch 800, training loss: 627.9754638671875 = 0.43716177344322205 + 100.0 * 6.275383472442627
Epoch 800, val loss: 0.7642509937286377
Epoch 810, training loss: 628.3200073242188 = 0.42641934752464294 + 100.0 * 6.27893590927124
Epoch 810, val loss: 0.7603819966316223
Epoch 820, training loss: 628.0043334960938 = 0.4157319962978363 + 100.0 * 6.275886058807373
Epoch 820, val loss: 0.7566508650779724
Epoch 830, training loss: 627.7540283203125 = 0.4053582549095154 + 100.0 * 6.273487091064453
Epoch 830, val loss: 0.7532266974449158
Epoch 840, training loss: 628.0011596679688 = 0.3951873183250427 + 100.0 * 6.276059627532959
Epoch 840, val loss: 0.7498940825462341
Epoch 850, training loss: 627.9136962890625 = 0.3851836919784546 + 100.0 * 6.275285243988037
Epoch 850, val loss: 0.7469599843025208
Epoch 860, training loss: 627.4443359375 = 0.37536993622779846 + 100.0 * 6.270689487457275
Epoch 860, val loss: 0.7438399195671082
Epoch 870, training loss: 627.2472534179688 = 0.36582159996032715 + 100.0 * 6.2688140869140625
Epoch 870, val loss: 0.7411431074142456
Epoch 880, training loss: 627.1422729492188 = 0.3564663827419281 + 100.0 * 6.267858505249023
Epoch 880, val loss: 0.7387214303016663
Epoch 890, training loss: 627.120361328125 = 0.34731125831604004 + 100.0 * 6.267730712890625
Epoch 890, val loss: 0.7363494634628296
Epoch 900, training loss: 627.0868530273438 = 0.33831068873405457 + 100.0 * 6.267485618591309
Epoch 900, val loss: 0.7340102791786194
Epoch 910, training loss: 626.968994140625 = 0.32947012782096863 + 100.0 * 6.266395092010498
Epoch 910, val loss: 0.7318673729896545
Epoch 920, training loss: 626.97412109375 = 0.3208412826061249 + 100.0 * 6.2665324211120605
Epoch 920, val loss: 0.7298746705055237
Epoch 930, training loss: 627.0548706054688 = 0.3124158978462219 + 100.0 * 6.267424583435059
Epoch 930, val loss: 0.7281681299209595
Epoch 940, training loss: 626.6904296875 = 0.3041381239891052 + 100.0 * 6.2638630867004395
Epoch 940, val loss: 0.726498007774353
Epoch 950, training loss: 626.5904541015625 = 0.2961173355579376 + 100.0 * 6.262943267822266
Epoch 950, val loss: 0.7250659465789795
Epoch 960, training loss: 627.009033203125 = 0.2882154881954193 + 100.0 * 6.267208099365234
Epoch 960, val loss: 0.7237043976783752
Epoch 970, training loss: 626.5135498046875 = 0.28049764037132263 + 100.0 * 6.262330055236816
Epoch 970, val loss: 0.7223936319351196
Epoch 980, training loss: 626.318603515625 = 0.2729795277118683 + 100.0 * 6.260456562042236
Epoch 980, val loss: 0.7213847637176514
Epoch 990, training loss: 626.725830078125 = 0.2656525671482086 + 100.0 * 6.264602184295654
Epoch 990, val loss: 0.7203512787818909
Epoch 1000, training loss: 626.7412719726562 = 0.2584238052368164 + 100.0 * 6.264828681945801
Epoch 1000, val loss: 0.7195466160774231
Epoch 1010, training loss: 626.0991821289062 = 0.25136810541152954 + 100.0 * 6.258478164672852
Epoch 1010, val loss: 0.7188416123390198
Epoch 1020, training loss: 626.0398559570312 = 0.24451611936092377 + 100.0 * 6.257953643798828
Epoch 1020, val loss: 0.7182703018188477
Epoch 1030, training loss: 626.2886352539062 = 0.23784303665161133 + 100.0 * 6.260507583618164
Epoch 1030, val loss: 0.7178465127944946
Epoch 1040, training loss: 625.9650268554688 = 0.23129157721996307 + 100.0 * 6.25733757019043
Epoch 1040, val loss: 0.7173369526863098
Epoch 1050, training loss: 625.9816284179688 = 0.2249114066362381 + 100.0 * 6.257566928863525
Epoch 1050, val loss: 0.7171284556388855
Epoch 1060, training loss: 625.9354858398438 = 0.2186667025089264 + 100.0 * 6.257167816162109
Epoch 1060, val loss: 0.7168613076210022
Epoch 1070, training loss: 625.6785278320312 = 0.21255531907081604 + 100.0 * 6.254659175872803
Epoch 1070, val loss: 0.71680748462677
Epoch 1080, training loss: 625.6550903320312 = 0.20663270354270935 + 100.0 * 6.254485130310059
Epoch 1080, val loss: 0.7168357372283936
Epoch 1090, training loss: 625.8522338867188 = 0.2008645087480545 + 100.0 * 6.256513595581055
Epoch 1090, val loss: 0.7171068787574768
Epoch 1100, training loss: 625.634033203125 = 0.19523859024047852 + 100.0 * 6.254387855529785
Epoch 1100, val loss: 0.7173134088516235
Epoch 1110, training loss: 625.5750122070312 = 0.1897655427455902 + 100.0 * 6.253852844238281
Epoch 1110, val loss: 0.7176240682601929
Epoch 1120, training loss: 625.6119995117188 = 0.18445736169815063 + 100.0 * 6.254275321960449
Epoch 1120, val loss: 0.7181916832923889
Epoch 1130, training loss: 625.4542236328125 = 0.1792520135641098 + 100.0 * 6.252749443054199
Epoch 1130, val loss: 0.7186183333396912
Epoch 1140, training loss: 625.421142578125 = 0.1742013841867447 + 100.0 * 6.252469539642334
Epoch 1140, val loss: 0.7192001342773438
Epoch 1150, training loss: 625.294189453125 = 0.16931912302970886 + 100.0 * 6.251248836517334
Epoch 1150, val loss: 0.7199887037277222
Epoch 1160, training loss: 625.26611328125 = 0.16458719968795776 + 100.0 * 6.2510151863098145
Epoch 1160, val loss: 0.7207974195480347
Epoch 1170, training loss: 625.242919921875 = 0.15997394919395447 + 100.0 * 6.250829219818115
Epoch 1170, val loss: 0.7218726277351379
Epoch 1180, training loss: 625.0759887695312 = 0.15547113120555878 + 100.0 * 6.249205112457275
Epoch 1180, val loss: 0.7227465510368347
Epoch 1190, training loss: 625.0838623046875 = 0.15110690891742706 + 100.0 * 6.249327659606934
Epoch 1190, val loss: 0.723815381526947
Epoch 1200, training loss: 625.1032104492188 = 0.14690956473350525 + 100.0 * 6.249562740325928
Epoch 1200, val loss: 0.7249475121498108
Epoch 1210, training loss: 624.8433227539062 = 0.1427888572216034 + 100.0 * 6.247005462646484
Epoch 1210, val loss: 0.7262135148048401
Epoch 1220, training loss: 625.1277465820312 = 0.13883309066295624 + 100.0 * 6.249888896942139
Epoch 1220, val loss: 0.7274293899536133
Epoch 1230, training loss: 625.01318359375 = 0.13495682179927826 + 100.0 * 6.248782634735107
Epoch 1230, val loss: 0.7288989424705505
Epoch 1240, training loss: 624.925537109375 = 0.13120914995670319 + 100.0 * 6.24794340133667
Epoch 1240, val loss: 0.7303557991981506
Epoch 1250, training loss: 624.86572265625 = 0.1275922805070877 + 100.0 * 6.247381687164307
Epoch 1250, val loss: 0.7319400310516357
Epoch 1260, training loss: 624.7130737304688 = 0.12405426800251007 + 100.0 * 6.245889663696289
Epoch 1260, val loss: 0.733496904373169
Epoch 1270, training loss: 624.643310546875 = 0.12063120305538177 + 100.0 * 6.245226860046387
Epoch 1270, val loss: 0.7351001501083374
Epoch 1280, training loss: 624.629150390625 = 0.1173354759812355 + 100.0 * 6.245118618011475
Epoch 1280, val loss: 0.736743688583374
Epoch 1290, training loss: 624.530517578125 = 0.11413997411727905 + 100.0 * 6.244163513183594
Epoch 1290, val loss: 0.738557755947113
Epoch 1300, training loss: 624.47265625 = 0.11104350537061691 + 100.0 * 6.243616104125977
Epoch 1300, val loss: 0.7403753399848938
Epoch 1310, training loss: 624.9263916015625 = 0.10804237425327301 + 100.0 * 6.248183727264404
Epoch 1310, val loss: 0.7419341802597046
Epoch 1320, training loss: 624.4061889648438 = 0.10512129217386246 + 100.0 * 6.243010997772217
Epoch 1320, val loss: 0.7439357042312622
Epoch 1330, training loss: 624.2705688476562 = 0.10230505466461182 + 100.0 * 6.241683006286621
Epoch 1330, val loss: 0.7458444237709045
Epoch 1340, training loss: 624.64697265625 = 0.09959117323160172 + 100.0 * 6.245473384857178
Epoch 1340, val loss: 0.747862696647644
Epoch 1350, training loss: 624.3292236328125 = 0.0969233438372612 + 100.0 * 6.24232292175293
Epoch 1350, val loss: 0.7497084140777588
Epoch 1360, training loss: 624.31640625 = 0.09436294436454773 + 100.0 * 6.242220401763916
Epoch 1360, val loss: 0.7517412900924683
Epoch 1370, training loss: 624.4973754882812 = 0.09189324080944061 + 100.0 * 6.244054794311523
Epoch 1370, val loss: 0.7538514137268066
Epoch 1380, training loss: 624.1539916992188 = 0.08949651569128036 + 100.0 * 6.240645408630371
Epoch 1380, val loss: 0.75577312707901
Epoch 1390, training loss: 624.0150756835938 = 0.08717328310012817 + 100.0 * 6.239279270172119
Epoch 1390, val loss: 0.7581425309181213
Epoch 1400, training loss: 624.0546264648438 = 0.08492961525917053 + 100.0 * 6.239697456359863
Epoch 1400, val loss: 0.7601746320724487
Epoch 1410, training loss: 624.2836303710938 = 0.08274880051612854 + 100.0 * 6.242008686065674
Epoch 1410, val loss: 0.7622601985931396
Epoch 1420, training loss: 624.0888671875 = 0.08064038306474686 + 100.0 * 6.240082263946533
Epoch 1420, val loss: 0.7646653652191162
Epoch 1430, training loss: 624.0926513671875 = 0.07858550548553467 + 100.0 * 6.240140438079834
Epoch 1430, val loss: 0.7669403553009033
Epoch 1440, training loss: 623.87646484375 = 0.07661419361829758 + 100.0 * 6.2379984855651855
Epoch 1440, val loss: 0.769244372844696
Epoch 1450, training loss: 624.2265014648438 = 0.07470308244228363 + 100.0 * 6.241518020629883
Epoch 1450, val loss: 0.7715951204299927
Epoch 1460, training loss: 624.0081787109375 = 0.0728440210223198 + 100.0 * 6.239353656768799
Epoch 1460, val loss: 0.7741886377334595
Epoch 1470, training loss: 623.86767578125 = 0.07102249562740326 + 100.0 * 6.237966537475586
Epoch 1470, val loss: 0.7763898968696594
Epoch 1480, training loss: 623.7235107421875 = 0.06928355246782303 + 100.0 * 6.236542224884033
Epoch 1480, val loss: 0.7788867354393005
Epoch 1490, training loss: 623.645263671875 = 0.06759040802717209 + 100.0 * 6.235776901245117
Epoch 1490, val loss: 0.7813622355461121
Epoch 1500, training loss: 623.6060791015625 = 0.06596039235591888 + 100.0 * 6.235401153564453
Epoch 1500, val loss: 0.7839601039886475
Epoch 1510, training loss: 624.0195922851562 = 0.06438520550727844 + 100.0 * 6.239552021026611
Epoch 1510, val loss: 0.786537766456604
Epoch 1520, training loss: 623.724609375 = 0.06283361464738846 + 100.0 * 6.2366180419921875
Epoch 1520, val loss: 0.7886852622032166
Epoch 1530, training loss: 623.596923828125 = 0.06133139878511429 + 100.0 * 6.235355854034424
Epoch 1530, val loss: 0.7913822531700134
Epoch 1540, training loss: 623.781494140625 = 0.05988815799355507 + 100.0 * 6.237216472625732
Epoch 1540, val loss: 0.793786883354187
Epoch 1550, training loss: 623.4600219726562 = 0.05847715213894844 + 100.0 * 6.234015464782715
Epoch 1550, val loss: 0.7965710163116455
Epoch 1560, training loss: 623.9418334960938 = 0.057117145508527756 + 100.0 * 6.238847255706787
Epoch 1560, val loss: 0.7988624572753906
Epoch 1570, training loss: 623.4097290039062 = 0.05579431354999542 + 100.0 * 6.233539581298828
Epoch 1570, val loss: 0.8016654253005981
Epoch 1580, training loss: 623.346435546875 = 0.054507795721292496 + 100.0 * 6.232919216156006
Epoch 1580, val loss: 0.8040964007377625
Epoch 1590, training loss: 623.4878540039062 = 0.05327990651130676 + 100.0 * 6.23434591293335
Epoch 1590, val loss: 0.8068227171897888
Epoch 1600, training loss: 623.220703125 = 0.0520564466714859 + 100.0 * 6.231686115264893
Epoch 1600, val loss: 0.8093588352203369
Epoch 1610, training loss: 623.4043579101562 = 0.05089414119720459 + 100.0 * 6.233534812927246
Epoch 1610, val loss: 0.8119605779647827
Epoch 1620, training loss: 623.2049560546875 = 0.0497603639960289 + 100.0 * 6.2315521240234375
Epoch 1620, val loss: 0.8143728375434875
Epoch 1630, training loss: 623.1449584960938 = 0.04865647107362747 + 100.0 * 6.230963230133057
Epoch 1630, val loss: 0.8170491456985474
Epoch 1640, training loss: 623.9407348632812 = 0.047603365033864975 + 100.0 * 6.238931179046631
Epoch 1640, val loss: 0.8197225332260132
Epoch 1650, training loss: 623.2858276367188 = 0.0465521514415741 + 100.0 * 6.23239278793335
Epoch 1650, val loss: 0.8222008943557739
Epoch 1660, training loss: 623.0301513671875 = 0.045543719083070755 + 100.0 * 6.229846000671387
Epoch 1660, val loss: 0.8249813318252563
Epoch 1670, training loss: 623.0255126953125 = 0.04456888884305954 + 100.0 * 6.229809284210205
Epoch 1670, val loss: 0.8275838494300842
Epoch 1680, training loss: 623.4635620117188 = 0.04363035038113594 + 100.0 * 6.234199047088623
Epoch 1680, val loss: 0.8302935361862183
Epoch 1690, training loss: 623.4425048828125 = 0.0427035391330719 + 100.0 * 6.233997821807861
Epoch 1690, val loss: 0.8325820565223694
Epoch 1700, training loss: 622.9517822265625 = 0.041788820177316666 + 100.0 * 6.229099750518799
Epoch 1700, val loss: 0.8353464007377625
Epoch 1710, training loss: 622.819580078125 = 0.040915004909038544 + 100.0 * 6.227787017822266
Epoch 1710, val loss: 0.8378782272338867
Epoch 1720, training loss: 622.7953491210938 = 0.04007638618350029 + 100.0 * 6.22755241394043
Epoch 1720, val loss: 0.8404219746589661
Epoch 1730, training loss: 623.055908203125 = 0.03926694765686989 + 100.0 * 6.230166435241699
Epoch 1730, val loss: 0.8430554270744324
Epoch 1740, training loss: 622.8496704101562 = 0.038461510092020035 + 100.0 * 6.22811222076416
Epoch 1740, val loss: 0.845788836479187
Epoch 1750, training loss: 622.7861938476562 = 0.03767528384923935 + 100.0 * 6.227485656738281
Epoch 1750, val loss: 0.8481488823890686
Epoch 1760, training loss: 622.8785400390625 = 0.03692368045449257 + 100.0 * 6.2284159660339355
Epoch 1760, val loss: 0.8507608771324158
Epoch 1770, training loss: 622.6654663085938 = 0.03619071841239929 + 100.0 * 6.226292610168457
Epoch 1770, val loss: 0.8533409237861633
Epoch 1780, training loss: 622.6609497070312 = 0.035482440143823624 + 100.0 * 6.226254940032959
Epoch 1780, val loss: 0.8561327457427979
Epoch 1790, training loss: 623.2288818359375 = 0.03479504585266113 + 100.0 * 6.231941223144531
Epoch 1790, val loss: 0.858542799949646
Epoch 1800, training loss: 622.7086791992188 = 0.03411129117012024 + 100.0 * 6.22674560546875
Epoch 1800, val loss: 0.8609082102775574
Epoch 1810, training loss: 622.597412109375 = 0.03345097601413727 + 100.0 * 6.225639820098877
Epoch 1810, val loss: 0.8635702133178711
Epoch 1820, training loss: 622.7672119140625 = 0.03281542286276817 + 100.0 * 6.227344036102295
Epoch 1820, val loss: 0.8659852147102356
Epoch 1830, training loss: 622.6331787109375 = 0.03219474107027054 + 100.0 * 6.226009845733643
Epoch 1830, val loss: 0.8686383962631226
Epoch 1840, training loss: 622.8410034179688 = 0.031592704355716705 + 100.0 * 6.228094100952148
Epoch 1840, val loss: 0.8710362315177917
Epoch 1850, training loss: 622.450439453125 = 0.03099362552165985 + 100.0 * 6.224194049835205
Epoch 1850, val loss: 0.8736314177513123
Epoch 1860, training loss: 622.4086303710938 = 0.03041885793209076 + 100.0 * 6.223782062530518
Epoch 1860, val loss: 0.8760592341423035
Epoch 1870, training loss: 622.56787109375 = 0.029866255819797516 + 100.0 * 6.2253804206848145
Epoch 1870, val loss: 0.8785488605499268
Epoch 1880, training loss: 622.6248168945312 = 0.029320677742362022 + 100.0 * 6.225955009460449
Epoch 1880, val loss: 0.8808868527412415
Epoch 1890, training loss: 622.6947021484375 = 0.028796443715691566 + 100.0 * 6.226658821105957
Epoch 1890, val loss: 0.8835679292678833
Epoch 1900, training loss: 622.495849609375 = 0.028270645067095757 + 100.0 * 6.22467565536499
Epoch 1900, val loss: 0.8857049942016602
Epoch 1910, training loss: 622.4865112304688 = 0.027768876403570175 + 100.0 * 6.224587440490723
Epoch 1910, val loss: 0.8883048892021179
Epoch 1920, training loss: 622.5203247070312 = 0.027275940403342247 + 100.0 * 6.224930286407471
Epoch 1920, val loss: 0.8907944560050964
Epoch 1930, training loss: 622.3989868164062 = 0.026795709505677223 + 100.0 * 6.223721981048584
Epoch 1930, val loss: 0.8930250406265259
Epoch 1940, training loss: 622.3303833007812 = 0.026327239349484444 + 100.0 * 6.223040580749512
Epoch 1940, val loss: 0.8953582644462585
Epoch 1950, training loss: 622.4998779296875 = 0.025879301130771637 + 100.0 * 6.224740028381348
Epoch 1950, val loss: 0.8979043960571289
Epoch 1960, training loss: 622.4429321289062 = 0.025432832539081573 + 100.0 * 6.224174976348877
Epoch 1960, val loss: 0.8998465538024902
Epoch 1970, training loss: 622.2052612304688 = 0.024996519088745117 + 100.0 * 6.221802234649658
Epoch 1970, val loss: 0.9024521112442017
Epoch 1980, training loss: 622.1524658203125 = 0.024576770141720772 + 100.0 * 6.221278667449951
Epoch 1980, val loss: 0.9047532677650452
Epoch 1990, training loss: 622.2530517578125 = 0.024172643199563026 + 100.0 * 6.222289085388184
Epoch 1990, val loss: 0.9070743322372437
Epoch 2000, training loss: 622.23681640625 = 0.023774266242980957 + 100.0 * 6.222129821777344
Epoch 2000, val loss: 0.9094416499137878
Epoch 2010, training loss: 622.2515869140625 = 0.023375527933239937 + 100.0 * 6.2222819328308105
Epoch 2010, val loss: 0.9116325378417969
Epoch 2020, training loss: 622.5509643554688 = 0.022996222600340843 + 100.0 * 6.225279331207275
Epoch 2020, val loss: 0.9137483835220337
Epoch 2030, training loss: 622.1737060546875 = 0.022623641416430473 + 100.0 * 6.221510887145996
Epoch 2030, val loss: 0.9161847829818726
Epoch 2040, training loss: 622.020751953125 = 0.0222549457103014 + 100.0 * 6.219984531402588
Epoch 2040, val loss: 0.9184312224388123
Epoch 2050, training loss: 622.0513305664062 = 0.021911099553108215 + 100.0 * 6.220293998718262
Epoch 2050, val loss: 0.9207806587219238
Epoch 2060, training loss: 622.343994140625 = 0.02156711556017399 + 100.0 * 6.223224639892578
Epoch 2060, val loss: 0.9228841662406921
Epoch 2070, training loss: 622.1611938476562 = 0.021223589777946472 + 100.0 * 6.221399784088135
Epoch 2070, val loss: 0.9249749183654785
Epoch 2080, training loss: 621.9256591796875 = 0.020892353728413582 + 100.0 * 6.219047546386719
Epoch 2080, val loss: 0.9273109436035156
Epoch 2090, training loss: 622.2828369140625 = 0.020572945475578308 + 100.0 * 6.222622871398926
Epoch 2090, val loss: 0.9293338060379028
Epoch 2100, training loss: 622.2081298828125 = 0.02025437168776989 + 100.0 * 6.221878528594971
Epoch 2100, val loss: 0.9312719702720642
Epoch 2110, training loss: 622.0224609375 = 0.019945457577705383 + 100.0 * 6.220025062561035
Epoch 2110, val loss: 0.9336872100830078
Epoch 2120, training loss: 621.8793334960938 = 0.019638793542981148 + 100.0 * 6.218596935272217
Epoch 2120, val loss: 0.9357791543006897
Epoch 2130, training loss: 621.812255859375 = 0.019350524991750717 + 100.0 * 6.217928886413574
Epoch 2130, val loss: 0.937874972820282
Epoch 2140, training loss: 622.1424560546875 = 0.019066354259848595 + 100.0 * 6.22123384475708
Epoch 2140, val loss: 0.940122127532959
Epoch 2150, training loss: 621.8427734375 = 0.018783628940582275 + 100.0 * 6.218239784240723
Epoch 2150, val loss: 0.9421333074569702
Epoch 2160, training loss: 621.82275390625 = 0.018505174666643143 + 100.0 * 6.218042373657227
Epoch 2160, val loss: 0.9441354870796204
Epoch 2170, training loss: 622.0183715820312 = 0.018238259479403496 + 100.0 * 6.220001220703125
Epoch 2170, val loss: 0.9459345936775208
Epoch 2180, training loss: 621.8583984375 = 0.017975078895688057 + 100.0 * 6.2184038162231445
Epoch 2180, val loss: 0.9481756091117859
Epoch 2190, training loss: 621.810302734375 = 0.01771629974246025 + 100.0 * 6.217926025390625
Epoch 2190, val loss: 0.9503060579299927
Epoch 2200, training loss: 621.7535400390625 = 0.017466166988015175 + 100.0 * 6.217360973358154
Epoch 2200, val loss: 0.9522134065628052
Epoch 2210, training loss: 621.7074584960938 = 0.017221854999661446 + 100.0 * 6.216902732849121
Epoch 2210, val loss: 0.9541293382644653
Epoch 2220, training loss: 621.6433715820312 = 0.016980838030576706 + 100.0 * 6.216264247894287
Epoch 2220, val loss: 0.9562130570411682
Epoch 2230, training loss: 621.8489990234375 = 0.01674872264266014 + 100.0 * 6.21832275390625
Epoch 2230, val loss: 0.9581364393234253
Epoch 2240, training loss: 621.7645874023438 = 0.01651892624795437 + 100.0 * 6.217480659484863
Epoch 2240, val loss: 0.9598960876464844
Epoch 2250, training loss: 621.8160400390625 = 0.01629050448536873 + 100.0 * 6.2179975509643555
Epoch 2250, val loss: 0.9619861841201782
Epoch 2260, training loss: 621.6177978515625 = 0.01606864109635353 + 100.0 * 6.216017723083496
Epoch 2260, val loss: 0.9640228152275085
Epoch 2270, training loss: 621.58447265625 = 0.015849212184548378 + 100.0 * 6.215685844421387
Epoch 2270, val loss: 0.96586674451828
Epoch 2280, training loss: 621.4862670898438 = 0.015637673437595367 + 100.0 * 6.2147064208984375
Epoch 2280, val loss: 0.9677421450614929
Epoch 2290, training loss: 621.7647705078125 = 0.01543659158051014 + 100.0 * 6.217493534088135
Epoch 2290, val loss: 0.9693790078163147
Epoch 2300, training loss: 621.8108520507812 = 0.015226686373353004 + 100.0 * 6.21795654296875
Epoch 2300, val loss: 0.9711726307868958
Epoch 2310, training loss: 621.5437622070312 = 0.01502944529056549 + 100.0 * 6.215287208557129
Epoch 2310, val loss: 0.9732778072357178
Epoch 2320, training loss: 621.3827514648438 = 0.014832616783678532 + 100.0 * 6.213679313659668
Epoch 2320, val loss: 0.9748966693878174
Epoch 2330, training loss: 621.379150390625 = 0.014644099399447441 + 100.0 * 6.213644981384277
Epoch 2330, val loss: 0.9768007397651672
Epoch 2340, training loss: 621.9325561523438 = 0.014466991648077965 + 100.0 * 6.219181060791016
Epoch 2340, val loss: 0.9788134694099426
Epoch 2350, training loss: 621.4149780273438 = 0.014272783882915974 + 100.0 * 6.2140069007873535
Epoch 2350, val loss: 0.9803786873817444
Epoch 2360, training loss: 621.3364868164062 = 0.014096291735768318 + 100.0 * 6.213223934173584
Epoch 2360, val loss: 0.9822689294815063
Epoch 2370, training loss: 621.39111328125 = 0.013920196332037449 + 100.0 * 6.213771820068359
Epoch 2370, val loss: 0.9839890003204346
Epoch 2380, training loss: 621.802490234375 = 0.013752330094575882 + 100.0 * 6.2178874015808105
Epoch 2380, val loss: 0.9856665730476379
Epoch 2390, training loss: 621.802734375 = 0.01357924286276102 + 100.0 * 6.217891693115234
Epoch 2390, val loss: 0.9874736666679382
Epoch 2400, training loss: 621.4976806640625 = 0.013410482555627823 + 100.0 * 6.214842796325684
Epoch 2400, val loss: 0.9889926314353943
Epoch 2410, training loss: 621.4859619140625 = 0.013246222399175167 + 100.0 * 6.21472692489624
Epoch 2410, val loss: 0.9906949400901794
Epoch 2420, training loss: 621.2716674804688 = 0.013086357153952122 + 100.0 * 6.212585926055908
Epoch 2420, val loss: 0.992615282535553
Epoch 2430, training loss: 621.304443359375 = 0.012934456579387188 + 100.0 * 6.212914943695068
Epoch 2430, val loss: 0.9944774508476257
Epoch 2440, training loss: 621.5341796875 = 0.012783629819750786 + 100.0 * 6.215214252471924
Epoch 2440, val loss: 0.9961057305335999
Epoch 2450, training loss: 621.5289306640625 = 0.012631742283701897 + 100.0 * 6.215163230895996
Epoch 2450, val loss: 0.9973720908164978
Epoch 2460, training loss: 621.3011474609375 = 0.012485668994486332 + 100.0 * 6.212886333465576
Epoch 2460, val loss: 0.9992644190788269
Epoch 2470, training loss: 621.3016967773438 = 0.012338382191956043 + 100.0 * 6.212893486022949
Epoch 2470, val loss: 1.0007789134979248
Epoch 2480, training loss: 621.2742919921875 = 0.012196186929941177 + 100.0 * 6.212620735168457
Epoch 2480, val loss: 1.0023667812347412
Epoch 2490, training loss: 621.3649291992188 = 0.012056689709424973 + 100.0 * 6.213528633117676
Epoch 2490, val loss: 1.003846526145935
Epoch 2500, training loss: 621.5671997070312 = 0.01191974151879549 + 100.0 * 6.215552806854248
Epoch 2500, val loss: 1.0056021213531494
Epoch 2510, training loss: 621.1527099609375 = 0.011783430352807045 + 100.0 * 6.211409091949463
Epoch 2510, val loss: 1.0070961713790894
Epoch 2520, training loss: 621.0278930664062 = 0.0116502046585083 + 100.0 * 6.21016263961792
Epoch 2520, val loss: 1.00869619846344
Epoch 2530, training loss: 621.173583984375 = 0.011524735018610954 + 100.0 * 6.211620330810547
Epoch 2530, val loss: 1.0101814270019531
Epoch 2540, training loss: 621.3632202148438 = 0.011399024166166782 + 100.0 * 6.213518142700195
Epoch 2540, val loss: 1.011651873588562
Epoch 2550, training loss: 621.25634765625 = 0.011273572221398354 + 100.0 * 6.2124505043029785
Epoch 2550, val loss: 1.013469934463501
Epoch 2560, training loss: 621.2348022460938 = 0.011151217855513096 + 100.0 * 6.212236404418945
Epoch 2560, val loss: 1.0147933959960938
Epoch 2570, training loss: 621.2650756835938 = 0.011033468879759312 + 100.0 * 6.212540149688721
Epoch 2570, val loss: 1.0163040161132812
Epoch 2580, training loss: 620.9884643554688 = 0.01091012917459011 + 100.0 * 6.209775447845459
Epoch 2580, val loss: 1.0179001092910767
Epoch 2590, training loss: 621.0856323242188 = 0.010792139917612076 + 100.0 * 6.210748195648193
Epoch 2590, val loss: 1.0193477869033813
Epoch 2600, training loss: 621.8941040039062 = 0.010682960040867329 + 100.0 * 6.218834400177002
Epoch 2600, val loss: 1.0205496549606323
Epoch 2610, training loss: 621.1831665039062 = 0.010569008998572826 + 100.0 * 6.211726188659668
Epoch 2610, val loss: 1.0221779346466064
Epoch 2620, training loss: 620.9447631835938 = 0.010454743169248104 + 100.0 * 6.209343433380127
Epoch 2620, val loss: 1.023568868637085
Epoch 2630, training loss: 620.8427124023438 = 0.010347909294068813 + 100.0 * 6.2083234786987305
Epoch 2630, val loss: 1.0250271558761597
Epoch 2640, training loss: 620.8760375976562 = 0.010245220735669136 + 100.0 * 6.208657741546631
Epoch 2640, val loss: 1.0264662504196167
Epoch 2650, training loss: 621.7728881835938 = 0.010148574598133564 + 100.0 * 6.21762752532959
Epoch 2650, val loss: 1.0278875827789307
Epoch 2660, training loss: 621.288818359375 = 0.010035259649157524 + 100.0 * 6.212788105010986
Epoch 2660, val loss: 1.0290604829788208
Epoch 2670, training loss: 621.2514038085938 = 0.009934837929904461 + 100.0 * 6.212414264678955
Epoch 2670, val loss: 1.0306123495101929
Epoch 2680, training loss: 620.9254150390625 = 0.009833197109401226 + 100.0 * 6.209156036376953
Epoch 2680, val loss: 1.0318324565887451
Epoch 2690, training loss: 620.8141479492188 = 0.009736776351928711 + 100.0 * 6.208043575286865
Epoch 2690, val loss: 1.0334135293960571
Epoch 2700, training loss: 620.8675537109375 = 0.009643157944083214 + 100.0 * 6.208579063415527
Epoch 2700, val loss: 1.0348647832870483
Epoch 2710, training loss: 621.04296875 = 0.009549453854560852 + 100.0 * 6.210334300994873
Epoch 2710, val loss: 1.0362657308578491
Epoch 2720, training loss: 621.3685302734375 = 0.009456932544708252 + 100.0 * 6.213590621948242
Epoch 2720, val loss: 1.0371321439743042
Epoch 2730, training loss: 620.959716796875 = 0.009363682009279728 + 100.0 * 6.209503650665283
Epoch 2730, val loss: 1.0386497974395752
Epoch 2740, training loss: 620.8021240234375 = 0.009269893169403076 + 100.0 * 6.20792818069458
Epoch 2740, val loss: 1.0400972366333008
Epoch 2750, training loss: 620.7890014648438 = 0.00918457843363285 + 100.0 * 6.207798480987549
Epoch 2750, val loss: 1.0413963794708252
Epoch 2760, training loss: 621.0716552734375 = 0.00909995287656784 + 100.0 * 6.210625648498535
Epoch 2760, val loss: 1.0427119731903076
Epoch 2770, training loss: 620.70654296875 = 0.009013641625642776 + 100.0 * 6.206974983215332
Epoch 2770, val loss: 1.0438077449798584
Epoch 2780, training loss: 620.888671875 = 0.008930396288633347 + 100.0 * 6.208797454833984
Epoch 2780, val loss: 1.04503333568573
Epoch 2790, training loss: 621.1283569335938 = 0.00885134469717741 + 100.0 * 6.21119499206543
Epoch 2790, val loss: 1.0465600490570068
Epoch 2800, training loss: 621.0606079101562 = 0.008764536119997501 + 100.0 * 6.2105183601379395
Epoch 2800, val loss: 1.04780113697052
Epoch 2810, training loss: 620.698486328125 = 0.00868273712694645 + 100.0 * 6.206898212432861
Epoch 2810, val loss: 1.0489773750305176
Epoch 2820, training loss: 620.6246337890625 = 0.00860409066081047 + 100.0 * 6.206160545349121
Epoch 2820, val loss: 1.050302267074585
Epoch 2830, training loss: 621.1849365234375 = 0.008529978804290295 + 100.0 * 6.211763858795166
Epoch 2830, val loss: 1.0514345169067383
Epoch 2840, training loss: 620.673095703125 = 0.008450926281511784 + 100.0 * 6.206646919250488
Epoch 2840, val loss: 1.0525037050247192
Epoch 2850, training loss: 620.5603637695312 = 0.008375201374292374 + 100.0 * 6.205520153045654
Epoch 2850, val loss: 1.0539039373397827
Epoch 2860, training loss: 620.6322021484375 = 0.008302103728055954 + 100.0 * 6.206239223480225
Epoch 2860, val loss: 1.0548778772354126
Epoch 2870, training loss: 621.0205078125 = 0.008230146951973438 + 100.0 * 6.210122585296631
Epoch 2870, val loss: 1.0560014247894287
Epoch 2880, training loss: 620.9073486328125 = 0.008157170377671719 + 100.0 * 6.208992004394531
Epoch 2880, val loss: 1.0577006340026855
Epoch 2890, training loss: 620.8820190429688 = 0.00808777380734682 + 100.0 * 6.208738803863525
Epoch 2890, val loss: 1.0584906339645386
Epoch 2900, training loss: 620.7383422851562 = 0.008012487553060055 + 100.0 * 6.207303524017334
Epoch 2900, val loss: 1.0599145889282227
Epoch 2910, training loss: 620.5480346679688 = 0.007941733114421368 + 100.0 * 6.2054009437561035
Epoch 2910, val loss: 1.0610036849975586
Epoch 2920, training loss: 620.4884033203125 = 0.007875270210206509 + 100.0 * 6.204805374145508
Epoch 2920, val loss: 1.0622650384902954
Epoch 2930, training loss: 620.856201171875 = 0.00781025318428874 + 100.0 * 6.208483695983887
Epoch 2930, val loss: 1.063416600227356
Epoch 2940, training loss: 620.6179809570312 = 0.00774421077221632 + 100.0 * 6.20610237121582
Epoch 2940, val loss: 1.064483880996704
Epoch 2950, training loss: 620.5818481445312 = 0.007677732966840267 + 100.0 * 6.2057414054870605
Epoch 2950, val loss: 1.065553903579712
Epoch 2960, training loss: 620.4953002929688 = 0.007611184846609831 + 100.0 * 6.204876899719238
Epoch 2960, val loss: 1.0668104887008667
Epoch 2970, training loss: 620.6135864257812 = 0.007548951078206301 + 100.0 * 6.206060409545898
Epoch 2970, val loss: 1.0678579807281494
Epoch 2980, training loss: 620.6150512695312 = 0.0074875131249427795 + 100.0 * 6.206075668334961
Epoch 2980, val loss: 1.0690786838531494
Epoch 2990, training loss: 620.4850463867188 = 0.007429016288369894 + 100.0 * 6.204776287078857
Epoch 2990, val loss: 1.0702950954437256
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 861.6339111328125 = 1.9465640783309937 + 100.0 * 8.59687328338623
Epoch 0, val loss: 1.9414857625961304
Epoch 10, training loss: 861.5736694335938 = 1.9379284381866455 + 100.0 * 8.596357345581055
Epoch 10, val loss: 1.9329657554626465
Epoch 20, training loss: 861.18603515625 = 1.9272903203964233 + 100.0 * 8.5925874710083
Epoch 20, val loss: 1.9220411777496338
Epoch 30, training loss: 858.5293579101562 = 1.9134238958358765 + 100.0 * 8.56615924835205
Epoch 30, val loss: 1.9073556661605835
Epoch 40, training loss: 843.775146484375 = 1.8952103853225708 + 100.0 * 8.41879940032959
Epoch 40, val loss: 1.8885552883148193
Epoch 50, training loss: 791.64013671875 = 1.874741554260254 + 100.0 * 7.897654056549072
Epoch 50, val loss: 1.8681358098983765
Epoch 60, training loss: 757.538818359375 = 1.857840895652771 + 100.0 * 7.556809902191162
Epoch 60, val loss: 1.8523601293563843
Epoch 70, training loss: 735.9542236328125 = 1.8450673818588257 + 100.0 * 7.341091156005859
Epoch 70, val loss: 1.840131402015686
Epoch 80, training loss: 711.940673828125 = 1.8329312801361084 + 100.0 * 7.101077079772949
Epoch 80, val loss: 1.8285322189331055
Epoch 90, training loss: 696.016845703125 = 1.8223538398742676 + 100.0 * 6.9419450759887695
Epoch 90, val loss: 1.8184254169464111
Epoch 100, training loss: 685.7864990234375 = 1.8114291429519653 + 100.0 * 6.839751243591309
Epoch 100, val loss: 1.8078622817993164
Epoch 110, training loss: 678.4462890625 = 1.8008475303649902 + 100.0 * 6.766454219818115
Epoch 110, val loss: 1.7975168228149414
Epoch 120, training loss: 673.86376953125 = 1.7904536724090576 + 100.0 * 6.720733165740967
Epoch 120, val loss: 1.7873831987380981
Epoch 130, training loss: 669.6770629882812 = 1.7799168825149536 + 100.0 * 6.678971767425537
Epoch 130, val loss: 1.7774243354797363
Epoch 140, training loss: 666.3233032226562 = 1.769350528717041 + 100.0 * 6.645539283752441
Epoch 140, val loss: 1.7674057483673096
Epoch 150, training loss: 663.5591430664062 = 1.7578617334365845 + 100.0 * 6.618012428283691
Epoch 150, val loss: 1.7566365003585815
Epoch 160, training loss: 661.3839111328125 = 1.7454577684402466 + 100.0 * 6.596384525299072
Epoch 160, val loss: 1.7450520992279053
Epoch 170, training loss: 659.4636840820312 = 1.7322099208831787 + 100.0 * 6.577314376831055
Epoch 170, val loss: 1.7326644659042358
Epoch 180, training loss: 657.678955078125 = 1.7177520990371704 + 100.0 * 6.559611797332764
Epoch 180, val loss: 1.7192949056625366
Epoch 190, training loss: 655.95751953125 = 1.702257752418518 + 100.0 * 6.542552471160889
Epoch 190, val loss: 1.7051271200180054
Epoch 200, training loss: 654.4469604492188 = 1.6856722831726074 + 100.0 * 6.527613162994385
Epoch 200, val loss: 1.6900951862335205
Epoch 210, training loss: 653.0166015625 = 1.6678434610366821 + 100.0 * 6.513487339019775
Epoch 210, val loss: 1.6737916469573975
Epoch 220, training loss: 651.535888671875 = 1.648524284362793 + 100.0 * 6.498874187469482
Epoch 220, val loss: 1.6564708948135376
Epoch 230, training loss: 650.2298583984375 = 1.6279116868972778 + 100.0 * 6.486019134521484
Epoch 230, val loss: 1.6378819942474365
Epoch 240, training loss: 649.0114135742188 = 1.6059273481369019 + 100.0 * 6.47405481338501
Epoch 240, val loss: 1.6182444095611572
Epoch 250, training loss: 648.0742797851562 = 1.582555890083313 + 100.0 * 6.464917182922363
Epoch 250, val loss: 1.597353458404541
Epoch 260, training loss: 646.9559936523438 = 1.5576895475387573 + 100.0 * 6.453983306884766
Epoch 260, val loss: 1.5754432678222656
Epoch 270, training loss: 645.8191528320312 = 1.5319148302078247 + 100.0 * 6.442872524261475
Epoch 270, val loss: 1.5527170896530151
Epoch 280, training loss: 645.3695068359375 = 1.5051348209381104 + 100.0 * 6.438643932342529
Epoch 280, val loss: 1.529420018196106
Epoch 290, training loss: 644.1689453125 = 1.4773297309875488 + 100.0 * 6.426915645599365
Epoch 290, val loss: 1.5054426193237305
Epoch 300, training loss: 643.2803344726562 = 1.4490666389465332 + 100.0 * 6.4183125495910645
Epoch 300, val loss: 1.4811999797821045
Epoch 310, training loss: 642.7498168945312 = 1.420356035232544 + 100.0 * 6.413294792175293
Epoch 310, val loss: 1.4568105936050415
Epoch 320, training loss: 642.41259765625 = 1.3907601833343506 + 100.0 * 6.410218715667725
Epoch 320, val loss: 1.431870460510254
Epoch 330, training loss: 641.3654174804688 = 1.3612934350967407 + 100.0 * 6.400041580200195
Epoch 330, val loss: 1.4071966409683228
Epoch 340, training loss: 640.774658203125 = 1.3316705226898193 + 100.0 * 6.394430160522461
Epoch 340, val loss: 1.3824517726898193
Epoch 350, training loss: 640.2066040039062 = 1.3020180463790894 + 100.0 * 6.3890461921691895
Epoch 350, val loss: 1.358020305633545
Epoch 360, training loss: 639.7568359375 = 1.2723718881607056 + 100.0 * 6.3848443031311035
Epoch 360, val loss: 1.333809494972229
Epoch 370, training loss: 639.6732177734375 = 1.2426906824111938 + 100.0 * 6.384305477142334
Epoch 370, val loss: 1.3094134330749512
Epoch 380, training loss: 638.9314575195312 = 1.2131247520446777 + 100.0 * 6.377182960510254
Epoch 380, val loss: 1.2854238748550415
Epoch 390, training loss: 638.3588256835938 = 1.1837358474731445 + 100.0 * 6.371750831604004
Epoch 390, val loss: 1.2617084980010986
Epoch 400, training loss: 637.90966796875 = 1.154603123664856 + 100.0 * 6.367550849914551
Epoch 400, val loss: 1.23832106590271
Epoch 410, training loss: 637.587646484375 = 1.1255340576171875 + 100.0 * 6.364620685577393
Epoch 410, val loss: 1.2150943279266357
Epoch 420, training loss: 637.0638427734375 = 1.0967684984207153 + 100.0 * 6.359671115875244
Epoch 420, val loss: 1.1921697854995728
Epoch 430, training loss: 636.80810546875 = 1.0685282945632935 + 100.0 * 6.357395648956299
Epoch 430, val loss: 1.1697931289672852
Epoch 440, training loss: 636.385009765625 = 1.0407037734985352 + 100.0 * 6.353443145751953
Epoch 440, val loss: 1.1480884552001953
Epoch 450, training loss: 636.099609375 = 1.0134328603744507 + 100.0 * 6.3508620262146
Epoch 450, val loss: 1.1267026662826538
Epoch 460, training loss: 636.0859375 = 0.9868168830871582 + 100.0 * 6.350991249084473
Epoch 460, val loss: 1.1062216758728027
Epoch 470, training loss: 635.3865356445312 = 0.960727870464325 + 100.0 * 6.344257831573486
Epoch 470, val loss: 1.0860539674758911
Epoch 480, training loss: 635.0426025390625 = 0.9354649782180786 + 100.0 * 6.341071128845215
Epoch 480, val loss: 1.0669320821762085
Epoch 490, training loss: 635.104248046875 = 0.9110226035118103 + 100.0 * 6.34193229675293
Epoch 490, val loss: 1.048409104347229
Epoch 500, training loss: 634.5535278320312 = 0.887109637260437 + 100.0 * 6.336664199829102
Epoch 500, val loss: 1.0309332609176636
Epoch 510, training loss: 634.2792358398438 = 0.8641397953033447 + 100.0 * 6.334150791168213
Epoch 510, val loss: 1.0138540267944336
Epoch 520, training loss: 634.2727661132812 = 0.8419093489646912 + 100.0 * 6.334308624267578
Epoch 520, val loss: 0.9976649284362793
Epoch 530, training loss: 633.7530517578125 = 0.8205590844154358 + 100.0 * 6.329325199127197
Epoch 530, val loss: 0.982363224029541
Epoch 540, training loss: 633.3975219726562 = 0.7998354434967041 + 100.0 * 6.325976371765137
Epoch 540, val loss: 0.9678212404251099
Epoch 550, training loss: 633.2892456054688 = 0.7799614667892456 + 100.0 * 6.325092792510986
Epoch 550, val loss: 0.9539835453033447
Epoch 560, training loss: 633.096923828125 = 0.7607131004333496 + 100.0 * 6.323361873626709
Epoch 560, val loss: 0.9408332109451294
Epoch 570, training loss: 632.928955078125 = 0.7419627904891968 + 100.0 * 6.32187032699585
Epoch 570, val loss: 0.9283419251441956
Epoch 580, training loss: 632.5872192382812 = 0.7240405678749084 + 100.0 * 6.318631649017334
Epoch 580, val loss: 0.916591465473175
Epoch 590, training loss: 632.3694458007812 = 0.7066279649734497 + 100.0 * 6.316628456115723
Epoch 590, val loss: 0.9056549072265625
Epoch 600, training loss: 632.3251953125 = 0.6896052360534668 + 100.0 * 6.316356182098389
Epoch 600, val loss: 0.8946138024330139
Epoch 610, training loss: 631.9844970703125 = 0.6731748580932617 + 100.0 * 6.313113689422607
Epoch 610, val loss: 0.8846347332000732
Epoch 620, training loss: 631.74853515625 = 0.6572532057762146 + 100.0 * 6.3109130859375
Epoch 620, val loss: 0.8750880360603333
Epoch 630, training loss: 631.4865112304688 = 0.6417667865753174 + 100.0 * 6.308447360992432
Epoch 630, val loss: 0.8659606575965881
Epoch 640, training loss: 631.7711791992188 = 0.6266580820083618 + 100.0 * 6.311445236206055
Epoch 640, val loss: 0.8572120666503906
Epoch 650, training loss: 631.952880859375 = 0.6118762493133545 + 100.0 * 6.31341028213501
Epoch 650, val loss: 0.84871906042099
Epoch 660, training loss: 631.2100219726562 = 0.5972833633422852 + 100.0 * 6.306127071380615
Epoch 660, val loss: 0.8404964208602905
Epoch 670, training loss: 630.9583129882812 = 0.5830577611923218 + 100.0 * 6.303752422332764
Epoch 670, val loss: 0.8327370882034302
Epoch 680, training loss: 630.793212890625 = 0.5691653490066528 + 100.0 * 6.30224084854126
Epoch 680, val loss: 0.8253768086433411
Epoch 690, training loss: 630.8115234375 = 0.5555238723754883 + 100.0 * 6.302559852600098
Epoch 690, val loss: 0.8181235194206238
Epoch 700, training loss: 630.5552368164062 = 0.542169451713562 + 100.0 * 6.300130367279053
Epoch 700, val loss: 0.8112550973892212
Epoch 710, training loss: 630.2449951171875 = 0.5289193987846375 + 100.0 * 6.297160625457764
Epoch 710, val loss: 0.8045399785041809
Epoch 720, training loss: 630.5176391601562 = 0.515995442867279 + 100.0 * 6.300016403198242
Epoch 720, val loss: 0.7982866764068604
Epoch 730, training loss: 630.020751953125 = 0.5031324028968811 + 100.0 * 6.2951765060424805
Epoch 730, val loss: 0.7920475006103516
Epoch 740, training loss: 630.0089111328125 = 0.49053576588630676 + 100.0 * 6.2951836585998535
Epoch 740, val loss: 0.786094069480896
Epoch 750, training loss: 629.8468017578125 = 0.4780885577201843 + 100.0 * 6.293686866760254
Epoch 750, val loss: 0.7803820371627808
Epoch 760, training loss: 629.8060302734375 = 0.4658411741256714 + 100.0 * 6.293402194976807
Epoch 760, val loss: 0.774803876876831
Epoch 770, training loss: 629.4292602539062 = 0.45390576124191284 + 100.0 * 6.289753437042236
Epoch 770, val loss: 0.7696617245674133
Epoch 780, training loss: 629.502685546875 = 0.4421772062778473 + 100.0 * 6.290604591369629
Epoch 780, val loss: 0.7648404836654663
Epoch 790, training loss: 629.2236938476562 = 0.43055713176727295 + 100.0 * 6.287931442260742
Epoch 790, val loss: 0.7598918676376343
Epoch 800, training loss: 629.088134765625 = 0.41920921206474304 + 100.0 * 6.286689281463623
Epoch 800, val loss: 0.7554183006286621
Epoch 810, training loss: 629.2029418945312 = 0.40808629989624023 + 100.0 * 6.2879486083984375
Epoch 810, val loss: 0.7511484622955322
Epoch 820, training loss: 628.8618774414062 = 0.3971795439720154 + 100.0 * 6.284646987915039
Epoch 820, val loss: 0.7470633387565613
Epoch 830, training loss: 629.256103515625 = 0.38650834560394287 + 100.0 * 6.288695812225342
Epoch 830, val loss: 0.7433757781982422
Epoch 840, training loss: 628.7567749023438 = 0.37602394819259644 + 100.0 * 6.283807277679443
Epoch 840, val loss: 0.7396169304847717
Epoch 850, training loss: 628.6600952148438 = 0.36580026149749756 + 100.0 * 6.282943248748779
Epoch 850, val loss: 0.7364313006401062
Epoch 860, training loss: 628.450927734375 = 0.3557952642440796 + 100.0 * 6.280951499938965
Epoch 860, val loss: 0.7332409620285034
Epoch 870, training loss: 629.070556640625 = 0.34605538845062256 + 100.0 * 6.28724479675293
Epoch 870, val loss: 0.7305064797401428
Epoch 880, training loss: 628.7234497070312 = 0.33651843667030334 + 100.0 * 6.28386926651001
Epoch 880, val loss: 0.7278628945350647
Epoch 890, training loss: 628.1179809570312 = 0.32720357179641724 + 100.0 * 6.277907848358154
Epoch 890, val loss: 0.7253246307373047
Epoch 900, training loss: 627.9725341796875 = 0.31823083758354187 + 100.0 * 6.276543140411377
Epoch 900, val loss: 0.723318874835968
Epoch 910, training loss: 628.4924926757812 = 0.30951064825057983 + 100.0 * 6.281829833984375
Epoch 910, val loss: 0.7213363647460938
Epoch 920, training loss: 628.2422485351562 = 0.3008769154548645 + 100.0 * 6.27941370010376
Epoch 920, val loss: 0.7193583846092224
Epoch 930, training loss: 628.045654296875 = 0.29254284501075745 + 100.0 * 6.277531147003174
Epoch 930, val loss: 0.7180131673812866
Epoch 940, training loss: 627.7044677734375 = 0.28442487120628357 + 100.0 * 6.274200439453125
Epoch 940, val loss: 0.7166436910629272
Epoch 950, training loss: 628.167724609375 = 0.2765333652496338 + 100.0 * 6.278911590576172
Epoch 950, val loss: 0.7154526114463806
Epoch 960, training loss: 627.555419921875 = 0.2689131796360016 + 100.0 * 6.272865295410156
Epoch 960, val loss: 0.7144262194633484
Epoch 970, training loss: 627.3521118164062 = 0.2614634335041046 + 100.0 * 6.270906448364258
Epoch 970, val loss: 0.7137187719345093
Epoch 980, training loss: 627.217041015625 = 0.25427746772766113 + 100.0 * 6.269627571105957
Epoch 980, val loss: 0.7131471037864685
Epoch 990, training loss: 628.0218505859375 = 0.24728573858737946 + 100.0 * 6.277745246887207
Epoch 990, val loss: 0.7128419280052185
Epoch 1000, training loss: 627.8013916015625 = 0.24036788940429688 + 100.0 * 6.275610446929932
Epoch 1000, val loss: 0.7118558287620544
Epoch 1010, training loss: 627.0213623046875 = 0.2337041199207306 + 100.0 * 6.267876625061035
Epoch 1010, val loss: 0.7117892503738403
Epoch 1020, training loss: 626.9398193359375 = 0.227301687002182 + 100.0 * 6.267125129699707
Epoch 1020, val loss: 0.7119318842887878
Epoch 1030, training loss: 626.9561157226562 = 0.22107167541980743 + 100.0 * 6.267350196838379
Epoch 1030, val loss: 0.7120955586433411
Epoch 1040, training loss: 627.1510620117188 = 0.21497704088687897 + 100.0 * 6.2693610191345215
Epoch 1040, val loss: 0.7122328877449036
Epoch 1050, training loss: 626.8497924804688 = 0.20902030169963837 + 100.0 * 6.266407489776611
Epoch 1050, val loss: 0.7125070691108704
Epoch 1060, training loss: 626.716552734375 = 0.20327484607696533 + 100.0 * 6.265132904052734
Epoch 1060, val loss: 0.7129673361778259
Epoch 1070, training loss: 626.5425415039062 = 0.19771477580070496 + 100.0 * 6.263448238372803
Epoch 1070, val loss: 0.7136140465736389
Epoch 1080, training loss: 627.0238647460938 = 0.19231213629245758 + 100.0 * 6.268315315246582
Epoch 1080, val loss: 0.7143709659576416
Epoch 1090, training loss: 626.7009887695312 = 0.18703937530517578 + 100.0 * 6.265140056610107
Epoch 1090, val loss: 0.7150099277496338
Epoch 1100, training loss: 626.6160888671875 = 0.18188664317131042 + 100.0 * 6.264341831207275
Epoch 1100, val loss: 0.7160385847091675
Epoch 1110, training loss: 626.3316650390625 = 0.17694416642189026 + 100.0 * 6.261547088623047
Epoch 1110, val loss: 0.7169852256774902
Epoch 1120, training loss: 626.216064453125 = 0.17214973270893097 + 100.0 * 6.260438919067383
Epoch 1120, val loss: 0.7181255221366882
Epoch 1130, training loss: 626.6509399414062 = 0.16749607026576996 + 100.0 * 6.264834403991699
Epoch 1130, val loss: 0.7193522453308105
Epoch 1140, training loss: 626.2219848632812 = 0.16293667256832123 + 100.0 * 6.260590076446533
Epoch 1140, val loss: 0.7207773923873901
Epoch 1150, training loss: 626.0084838867188 = 0.15852829813957214 + 100.0 * 6.258499622344971
Epoch 1150, val loss: 0.7220606207847595
Epoch 1160, training loss: 626.4338989257812 = 0.15429094433784485 + 100.0 * 6.262795925140381
Epoch 1160, val loss: 0.7234468460083008
Epoch 1170, training loss: 626.0184936523438 = 0.15013673901557922 + 100.0 * 6.258683681488037
Epoch 1170, val loss: 0.7253602743148804
Epoch 1180, training loss: 625.9254760742188 = 0.14610841870307922 + 100.0 * 6.257793426513672
Epoch 1180, val loss: 0.7267604470252991
Epoch 1190, training loss: 626.0550537109375 = 0.14223118126392365 + 100.0 * 6.259128570556641
Epoch 1190, val loss: 0.7286813259124756
Epoch 1200, training loss: 625.8208618164062 = 0.1384565681219101 + 100.0 * 6.256824493408203
Epoch 1200, val loss: 0.7304577231407166
Epoch 1210, training loss: 626.2987060546875 = 0.13478316366672516 + 100.0 * 6.26163911819458
Epoch 1210, val loss: 0.7320499420166016
Epoch 1220, training loss: 625.8488159179688 = 0.1311899572610855 + 100.0 * 6.257176399230957
Epoch 1220, val loss: 0.7344136834144592
Epoch 1230, training loss: 625.57861328125 = 0.1277291625738144 + 100.0 * 6.2545084953308105
Epoch 1230, val loss: 0.7361721396446228
Epoch 1240, training loss: 625.42919921875 = 0.12438780069351196 + 100.0 * 6.253047943115234
Epoch 1240, val loss: 0.7383894324302673
Epoch 1250, training loss: 625.3909301757812 = 0.12116866558790207 + 100.0 * 6.252697467803955
Epoch 1250, val loss: 0.7405655980110168
Epoch 1260, training loss: 625.8709106445312 = 0.11803596466779709 + 100.0 * 6.257528781890869
Epoch 1260, val loss: 0.7426748275756836
Epoch 1270, training loss: 625.5394287109375 = 0.11496267467737198 + 100.0 * 6.254244327545166
Epoch 1270, val loss: 0.7452555894851685
Epoch 1280, training loss: 625.83740234375 = 0.11197300255298615 + 100.0 * 6.257254123687744
Epoch 1280, val loss: 0.7472765445709229
Epoch 1290, training loss: 625.2442016601562 = 0.10906319320201874 + 100.0 * 6.251351356506348
Epoch 1290, val loss: 0.7498959898948669
Epoch 1300, training loss: 625.1569213867188 = 0.10628858208656311 + 100.0 * 6.250506401062012
Epoch 1300, val loss: 0.752258837223053
Epoch 1310, training loss: 625.3064575195312 = 0.1036040186882019 + 100.0 * 6.252028942108154
Epoch 1310, val loss: 0.7548004984855652
Epoch 1320, training loss: 625.2054443359375 = 0.10097222775220871 + 100.0 * 6.251044750213623
Epoch 1320, val loss: 0.7572676539421082
Epoch 1330, training loss: 625.1148681640625 = 0.09839574992656708 + 100.0 * 6.25016450881958
Epoch 1330, val loss: 0.7597106099128723
Epoch 1340, training loss: 624.9702758789062 = 0.09593824297189713 + 100.0 * 6.248743534088135
Epoch 1340, val loss: 0.7621681094169617
Epoch 1350, training loss: 625.3077392578125 = 0.09354723244905472 + 100.0 * 6.252141952514648
Epoch 1350, val loss: 0.7647276520729065
Epoch 1360, training loss: 625.2877807617188 = 0.09121550619602203 + 100.0 * 6.2519659996032715
Epoch 1360, val loss: 0.7677834630012512
Epoch 1370, training loss: 624.9982299804688 = 0.08894526958465576 + 100.0 * 6.249093055725098
Epoch 1370, val loss: 0.7701588273048401
Epoch 1380, training loss: 624.8184204101562 = 0.08674135059118271 + 100.0 * 6.247316837310791
Epoch 1380, val loss: 0.7732027769088745
Epoch 1390, training loss: 625.1085815429688 = 0.08460813015699387 + 100.0 * 6.250239849090576
Epoch 1390, val loss: 0.775661051273346
Epoch 1400, training loss: 624.7186889648438 = 0.08254937082529068 + 100.0 * 6.24636173248291
Epoch 1400, val loss: 0.7783611416816711
Epoch 1410, training loss: 624.7376708984375 = 0.08053848892450333 + 100.0 * 6.246571063995361
Epoch 1410, val loss: 0.7811499834060669
Epoch 1420, training loss: 624.910888671875 = 0.07859957963228226 + 100.0 * 6.248322486877441
Epoch 1420, val loss: 0.7842734456062317
Epoch 1430, training loss: 624.9808959960938 = 0.07670062780380249 + 100.0 * 6.24904203414917
Epoch 1430, val loss: 0.7867503762245178
Epoch 1440, training loss: 624.5066528320312 = 0.07484618574380875 + 100.0 * 6.24431848526001
Epoch 1440, val loss: 0.7896568179130554
Epoch 1450, training loss: 624.3847045898438 = 0.0730728730559349 + 100.0 * 6.24311637878418
Epoch 1450, val loss: 0.7924780249595642
Epoch 1460, training loss: 624.3414306640625 = 0.07136449217796326 + 100.0 * 6.242701053619385
Epoch 1460, val loss: 0.7953861951828003
Epoch 1470, training loss: 624.7565307617188 = 0.06969234347343445 + 100.0 * 6.246868133544922
Epoch 1470, val loss: 0.7981218695640564
Epoch 1480, training loss: 624.9094848632812 = 0.06804527342319489 + 100.0 * 6.248414039611816
Epoch 1480, val loss: 0.8011159896850586
Epoch 1490, training loss: 624.5618896484375 = 0.06644752621650696 + 100.0 * 6.2449541091918945
Epoch 1490, val loss: 0.8039751648902893
Epoch 1500, training loss: 624.2021484375 = 0.06488174945116043 + 100.0 * 6.241372585296631
Epoch 1500, val loss: 0.8067408800125122
Epoch 1510, training loss: 624.1619873046875 = 0.06339121609926224 + 100.0 * 6.240985870361328
Epoch 1510, val loss: 0.8097953796386719
Epoch 1520, training loss: 624.8870239257812 = 0.0619509294629097 + 100.0 * 6.248250961303711
Epoch 1520, val loss: 0.8125247955322266
Epoch 1530, training loss: 624.3528442382812 = 0.06052052602171898 + 100.0 * 6.242923259735107
Epoch 1530, val loss: 0.8157913088798523
Epoch 1540, training loss: 624.0935668945312 = 0.05912346765398979 + 100.0 * 6.240344047546387
Epoch 1540, val loss: 0.8184360265731812
Epoch 1550, training loss: 624.0610961914062 = 0.05778975784778595 + 100.0 * 6.240033149719238
Epoch 1550, val loss: 0.8215645551681519
Epoch 1560, training loss: 624.4437866210938 = 0.05648795887827873 + 100.0 * 6.243873596191406
Epoch 1560, val loss: 0.8242890238761902
Epoch 1570, training loss: 624.002685546875 = 0.055212683975696564 + 100.0 * 6.239474296569824
Epoch 1570, val loss: 0.8275195360183716
Epoch 1580, training loss: 623.9280395507812 = 0.053992703557014465 + 100.0 * 6.238739967346191
Epoch 1580, val loss: 0.8304958343505859
Epoch 1590, training loss: 624.3709106445312 = 0.052800875157117844 + 100.0 * 6.243181228637695
Epoch 1590, val loss: 0.8334835767745972
Epoch 1600, training loss: 623.9555053710938 = 0.05163353681564331 + 100.0 * 6.239038944244385
Epoch 1600, val loss: 0.8361595869064331
Epoch 1610, training loss: 623.927734375 = 0.05048920959234238 + 100.0 * 6.238772869110107
Epoch 1610, val loss: 0.8389354348182678
Epoch 1620, training loss: 623.9923706054688 = 0.04939349740743637 + 100.0 * 6.239429950714111
Epoch 1620, val loss: 0.8423777222633362
Epoch 1630, training loss: 623.801025390625 = 0.048317018896341324 + 100.0 * 6.237526893615723
Epoch 1630, val loss: 0.8450913429260254
Epoch 1640, training loss: 623.863037109375 = 0.047276947647333145 + 100.0 * 6.238157272338867
Epoch 1640, val loss: 0.8477759957313538
Epoch 1650, training loss: 624.00146484375 = 0.04627082496881485 + 100.0 * 6.239552021026611
Epoch 1650, val loss: 0.8507992625236511
Epoch 1660, training loss: 623.6439208984375 = 0.045265279710292816 + 100.0 * 6.235986232757568
Epoch 1660, val loss: 0.8537864089012146
Epoch 1670, training loss: 623.7216186523438 = 0.044306639581918716 + 100.0 * 6.2367730140686035
Epoch 1670, val loss: 0.8567133545875549
Epoch 1680, training loss: 623.9485473632812 = 0.04338269308209419 + 100.0 * 6.239051342010498
Epoch 1680, val loss: 0.8595658540725708
Epoch 1690, training loss: 623.5463256835938 = 0.04247410222887993 + 100.0 * 6.2350382804870605
Epoch 1690, val loss: 0.8625783920288086
Epoch 1700, training loss: 623.961181640625 = 0.0415928028523922 + 100.0 * 6.239196300506592
Epoch 1700, val loss: 0.8652920722961426
Epoch 1710, training loss: 623.6583862304688 = 0.04072481021285057 + 100.0 * 6.236176490783691
Epoch 1710, val loss: 0.8687235713005066
Epoch 1720, training loss: 623.5366821289062 = 0.03989015519618988 + 100.0 * 6.2349677085876465
Epoch 1720, val loss: 0.8715937733650208
Epoch 1730, training loss: 623.5721435546875 = 0.039078488945961 + 100.0 * 6.235331058502197
Epoch 1730, val loss: 0.874022901058197
Epoch 1740, training loss: 623.7490234375 = 0.03828723728656769 + 100.0 * 6.237107753753662
Epoch 1740, val loss: 0.876882016658783
Epoch 1750, training loss: 623.6519165039062 = 0.03750745207071304 + 100.0 * 6.236143589019775
Epoch 1750, val loss: 0.880004346370697
Epoch 1760, training loss: 623.4201049804688 = 0.03675143048167229 + 100.0 * 6.2338337898254395
Epoch 1760, val loss: 0.8827809691429138
Epoch 1770, training loss: 623.276123046875 = 0.036034490913152695 + 100.0 * 6.232401371002197
Epoch 1770, val loss: 0.8859803676605225
Epoch 1780, training loss: 623.3854370117188 = 0.03533364087343216 + 100.0 * 6.233500957489014
Epoch 1780, val loss: 0.8889232873916626
Epoch 1790, training loss: 623.7427978515625 = 0.034643206745386124 + 100.0 * 6.237081527709961
Epoch 1790, val loss: 0.8917185068130493
Epoch 1800, training loss: 623.52392578125 = 0.03395367041230202 + 100.0 * 6.234899997711182
Epoch 1800, val loss: 0.8939001560211182
Epoch 1810, training loss: 623.4848022460938 = 0.0333106629550457 + 100.0 * 6.2345147132873535
Epoch 1810, val loss: 0.897165060043335
Epoch 1820, training loss: 623.1943359375 = 0.032656461000442505 + 100.0 * 6.231616973876953
Epoch 1820, val loss: 0.8996825218200684
Epoch 1830, training loss: 623.1396484375 = 0.032038237899541855 + 100.0 * 6.231075763702393
Epoch 1830, val loss: 0.9025276303291321
Epoch 1840, training loss: 623.2839965820312 = 0.03144438564777374 + 100.0 * 6.232525825500488
Epoch 1840, val loss: 0.905288577079773
Epoch 1850, training loss: 623.508056640625 = 0.03086419403553009 + 100.0 * 6.234771728515625
Epoch 1850, val loss: 0.9077401161193848
Epoch 1860, training loss: 623.3849487304688 = 0.030260300263762474 + 100.0 * 6.233546733856201
Epoch 1860, val loss: 0.9108395576477051
Epoch 1870, training loss: 623.004150390625 = 0.029694093391299248 + 100.0 * 6.2297444343566895
Epoch 1870, val loss: 0.9134189486503601
Epoch 1880, training loss: 623.082275390625 = 0.029149988666176796 + 100.0 * 6.230531215667725
Epoch 1880, val loss: 0.9162682294845581
Epoch 1890, training loss: 623.5003051757812 = 0.02861839160323143 + 100.0 * 6.234716892242432
Epoch 1890, val loss: 0.9189028143882751
Epoch 1900, training loss: 623.0033569335938 = 0.02809501253068447 + 100.0 * 6.229753017425537
Epoch 1900, val loss: 0.9214686155319214
Epoch 1910, training loss: 622.9149169921875 = 0.02758781611919403 + 100.0 * 6.228873252868652
Epoch 1910, val loss: 0.9243467450141907
Epoch 1920, training loss: 622.9140014648438 = 0.027101434767246246 + 100.0 * 6.2288689613342285
Epoch 1920, val loss: 0.9271872639656067
Epoch 1930, training loss: 623.4191284179688 = 0.026635561138391495 + 100.0 * 6.2339253425598145
Epoch 1930, val loss: 0.9296331405639648
Epoch 1940, training loss: 623.1655883789062 = 0.026142524555325508 + 100.0 * 6.2313947677612305
Epoch 1940, val loss: 0.932026207447052
Epoch 1950, training loss: 622.8709106445312 = 0.02568298764526844 + 100.0 * 6.228452205657959
Epoch 1950, val loss: 0.9349722862243652
Epoch 1960, training loss: 623.0046997070312 = 0.02523212507367134 + 100.0 * 6.229794979095459
Epoch 1960, val loss: 0.9376038908958435
Epoch 1970, training loss: 623.0611572265625 = 0.0248049795627594 + 100.0 * 6.230363845825195
Epoch 1970, val loss: 0.9403157830238342
Epoch 1980, training loss: 622.741943359375 = 0.024372931569814682 + 100.0 * 6.227176189422607
Epoch 1980, val loss: 0.9425345063209534
Epoch 1990, training loss: 623.0060424804688 = 0.023968584835529327 + 100.0 * 6.229820728302002
Epoch 1990, val loss: 0.9453516602516174
Epoch 2000, training loss: 622.7786254882812 = 0.023560713976621628 + 100.0 * 6.227550506591797
Epoch 2000, val loss: 0.9479051232337952
Epoch 2010, training loss: 622.9263916015625 = 0.023163480684161186 + 100.0 * 6.229032039642334
Epoch 2010, val loss: 0.9503635168075562
Epoch 2020, training loss: 623.0592651367188 = 0.022773653268814087 + 100.0 * 6.230364799499512
Epoch 2020, val loss: 0.952710747718811
Epoch 2030, training loss: 622.8278198242188 = 0.022397954016923904 + 100.0 * 6.228054046630859
Epoch 2030, val loss: 0.9549992084503174
Epoch 2040, training loss: 622.7411499023438 = 0.02203456684947014 + 100.0 * 6.22719144821167
Epoch 2040, val loss: 0.957443118095398
Epoch 2050, training loss: 622.6470336914062 = 0.021676404401659966 + 100.0 * 6.226253509521484
Epoch 2050, val loss: 0.9600158929824829
Epoch 2060, training loss: 622.7267456054688 = 0.021333934739232063 + 100.0 * 6.227054119110107
Epoch 2060, val loss: 0.9627643823623657
Epoch 2070, training loss: 623.031494140625 = 0.020998142659664154 + 100.0 * 6.230104923248291
Epoch 2070, val loss: 0.9652707576751709
Epoch 2080, training loss: 622.6963500976562 = 0.02065538614988327 + 100.0 * 6.226757049560547
Epoch 2080, val loss: 0.967484176158905
Epoch 2090, training loss: 622.65576171875 = 0.02032829448580742 + 100.0 * 6.226354598999023
Epoch 2090, val loss: 0.9698349833488464
Epoch 2100, training loss: 622.7557983398438 = 0.020013388246297836 + 100.0 * 6.227357864379883
Epoch 2100, val loss: 0.9720020294189453
Epoch 2110, training loss: 622.5043334960938 = 0.019700324162840843 + 100.0 * 6.224845886230469
Epoch 2110, val loss: 0.9746567606925964
Epoch 2120, training loss: 622.7261352539062 = 0.01940084435045719 + 100.0 * 6.227067470550537
Epoch 2120, val loss: 0.9769603610038757
Epoch 2130, training loss: 622.6143188476562 = 0.019096756353974342 + 100.0 * 6.2259521484375
Epoch 2130, val loss: 0.9792885184288025
Epoch 2140, training loss: 622.4517822265625 = 0.01880657859146595 + 100.0 * 6.224329948425293
Epoch 2140, val loss: 0.982032060623169
Epoch 2150, training loss: 622.5283813476562 = 0.018531493842601776 + 100.0 * 6.225098133087158
Epoch 2150, val loss: 0.9844453930854797
Epoch 2160, training loss: 622.5751953125 = 0.01825184002518654 + 100.0 * 6.225569725036621
Epoch 2160, val loss: 0.9863736629486084
Epoch 2170, training loss: 622.6383056640625 = 0.017988231033086777 + 100.0 * 6.226202964782715
Epoch 2170, val loss: 0.9884952306747437
Epoch 2180, training loss: 622.403076171875 = 0.017718957737088203 + 100.0 * 6.223854064941406
Epoch 2180, val loss: 0.9910998344421387
Epoch 2190, training loss: 622.256103515625 = 0.017455849796533585 + 100.0 * 6.222386360168457
Epoch 2190, val loss: 0.9933228492736816
Epoch 2200, training loss: 622.766357421875 = 0.017210176214575768 + 100.0 * 6.22749137878418
Epoch 2200, val loss: 0.9954387545585632
Epoch 2210, training loss: 622.7324829101562 = 0.01696603186428547 + 100.0 * 6.2271552085876465
Epoch 2210, val loss: 0.9978869557380676
Epoch 2220, training loss: 622.266357421875 = 0.016711361706256866 + 100.0 * 6.222496032714844
Epoch 2220, val loss: 1.000169277191162
Epoch 2230, training loss: 622.1828002929688 = 0.01648038811981678 + 100.0 * 6.221662998199463
Epoch 2230, val loss: 1.0026730298995972
Epoch 2240, training loss: 622.2260131835938 = 0.016256149858236313 + 100.0 * 6.222097873687744
Epoch 2240, val loss: 1.0049490928649902
Epoch 2250, training loss: 622.6958618164062 = 0.016033168882131577 + 100.0 * 6.226798057556152
Epoch 2250, val loss: 1.0073825120925903
Epoch 2260, training loss: 622.607421875 = 0.015803221613168716 + 100.0 * 6.225915908813477
Epoch 2260, val loss: 1.0087224245071411
Epoch 2270, training loss: 622.2930908203125 = 0.015589228831231594 + 100.0 * 6.222774982452393
Epoch 2270, val loss: 1.01124107837677
Epoch 2280, training loss: 622.419189453125 = 0.015374941751360893 + 100.0 * 6.224038124084473
Epoch 2280, val loss: 1.0128021240234375
Epoch 2290, training loss: 622.365966796875 = 0.015171666629612446 + 100.0 * 6.223507404327393
Epoch 2290, val loss: 1.015620470046997
Epoch 2300, training loss: 622.1432495117188 = 0.014963333494961262 + 100.0 * 6.221282958984375
Epoch 2300, val loss: 1.0176054239273071
Epoch 2310, training loss: 622.010986328125 = 0.01476439367979765 + 100.0 * 6.219962120056152
Epoch 2310, val loss: 1.0197811126708984
Epoch 2320, training loss: 622.1758422851562 = 0.014576095156371593 + 100.0 * 6.221612453460693
Epoch 2320, val loss: 1.0217984914779663
Epoch 2330, training loss: 622.2474365234375 = 0.014385809190571308 + 100.0 * 6.222330093383789
Epoch 2330, val loss: 1.0243091583251953
Epoch 2340, training loss: 622.8342895507812 = 0.0141944270581007 + 100.0 * 6.228200912475586
Epoch 2340, val loss: 1.0263181924819946
Epoch 2350, training loss: 622.33544921875 = 0.01400826033204794 + 100.0 * 6.223214149475098
Epoch 2350, val loss: 1.0276212692260742
Epoch 2360, training loss: 622.3191528320312 = 0.013824270106852055 + 100.0 * 6.223053455352783
Epoch 2360, val loss: 1.0303990840911865
Epoch 2370, training loss: 621.9258422851562 = 0.013647072948515415 + 100.0 * 6.219121932983398
Epoch 2370, val loss: 1.0319981575012207
Epoch 2380, training loss: 621.8737182617188 = 0.013476556167006493 + 100.0 * 6.218602180480957
Epoch 2380, val loss: 1.0341747999191284
Epoch 2390, training loss: 622.2379760742188 = 0.013314888812601566 + 100.0 * 6.2222466468811035
Epoch 2390, val loss: 1.0361777544021606
Epoch 2400, training loss: 621.892578125 = 0.01314508356153965 + 100.0 * 6.218794822692871
Epoch 2400, val loss: 1.0383199453353882
Epoch 2410, training loss: 622.2711791992188 = 0.012983744964003563 + 100.0 * 6.22258186340332
Epoch 2410, val loss: 1.040132999420166
Epoch 2420, training loss: 622.181640625 = 0.012820597738027573 + 100.0 * 6.221688270568848
Epoch 2420, val loss: 1.0419485569000244
Epoch 2430, training loss: 621.9254760742188 = 0.012662497349083424 + 100.0 * 6.219128608703613
Epoch 2430, val loss: 1.0444608926773071
Epoch 2440, training loss: 621.7415771484375 = 0.012509726919233799 + 100.0 * 6.217290878295898
Epoch 2440, val loss: 1.0462234020233154
Epoch 2450, training loss: 621.7966918945312 = 0.012362963519990444 + 100.0 * 6.217843055725098
Epoch 2450, val loss: 1.0480806827545166
Epoch 2460, training loss: 622.5181884765625 = 0.012216203846037388 + 100.0 * 6.225059509277344
Epoch 2460, val loss: 1.0497010946273804
Epoch 2470, training loss: 622.13037109375 = 0.012070219963788986 + 100.0 * 6.221182823181152
Epoch 2470, val loss: 1.052000641822815
Epoch 2480, training loss: 621.7998046875 = 0.011923342011868954 + 100.0 * 6.217879295349121
Epoch 2480, val loss: 1.0537723302841187
Epoch 2490, training loss: 622.0109252929688 = 0.011792044155299664 + 100.0 * 6.219991683959961
Epoch 2490, val loss: 1.0559313297271729
Epoch 2500, training loss: 621.8419189453125 = 0.011649464257061481 + 100.0 * 6.2183027267456055
Epoch 2500, val loss: 1.057489037513733
Epoch 2510, training loss: 621.7739868164062 = 0.01151333563029766 + 100.0 * 6.217624664306641
Epoch 2510, val loss: 1.0592806339263916
Epoch 2520, training loss: 621.7281494140625 = 0.01138630136847496 + 100.0 * 6.217167377471924
Epoch 2520, val loss: 1.0615718364715576
Epoch 2530, training loss: 622.5333862304688 = 0.011260850355029106 + 100.0 * 6.225221633911133
Epoch 2530, val loss: 1.063353419303894
Epoch 2540, training loss: 621.8506469726562 = 0.011127865873277187 + 100.0 * 6.218395233154297
Epoch 2540, val loss: 1.064823865890503
Epoch 2550, training loss: 621.6641235351562 = 0.011004160158336163 + 100.0 * 6.216531276702881
Epoch 2550, val loss: 1.0668113231658936
Epoch 2560, training loss: 622.7041015625 = 0.010894488543272018 + 100.0 * 6.226932525634766
Epoch 2560, val loss: 1.067872405052185
Epoch 2570, training loss: 621.8773193359375 = 0.010757229290902615 + 100.0 * 6.218666076660156
Epoch 2570, val loss: 1.0702967643737793
Epoch 2580, training loss: 621.62744140625 = 0.010641020722687244 + 100.0 * 6.21616792678833
Epoch 2580, val loss: 1.0718286037445068
Epoch 2590, training loss: 621.6652221679688 = 0.010525530204176903 + 100.0 * 6.216547012329102
Epoch 2590, val loss: 1.073637843132019
Epoch 2600, training loss: 621.7108154296875 = 0.010414863005280495 + 100.0 * 6.21700382232666
Epoch 2600, val loss: 1.0751177072525024
Epoch 2610, training loss: 621.996826171875 = 0.010308179073035717 + 100.0 * 6.219864845275879
Epoch 2610, val loss: 1.077231526374817
Epoch 2620, training loss: 621.4978637695312 = 0.010190832428634167 + 100.0 * 6.214876651763916
Epoch 2620, val loss: 1.0790364742279053
Epoch 2630, training loss: 621.5013427734375 = 0.010084498673677444 + 100.0 * 6.2149128913879395
Epoch 2630, val loss: 1.0809574127197266
Epoch 2640, training loss: 621.512451171875 = 0.009981980547308922 + 100.0 * 6.215024471282959
Epoch 2640, val loss: 1.0825127363204956
Epoch 2650, training loss: 621.73193359375 = 0.0098799429833889 + 100.0 * 6.217220306396484
Epoch 2650, val loss: 1.0843957662582397
Epoch 2660, training loss: 621.6786499023438 = 0.009781689383089542 + 100.0 * 6.216688632965088
Epoch 2660, val loss: 1.0858241319656372
Epoch 2670, training loss: 621.5166015625 = 0.009674349799752235 + 100.0 * 6.215068817138672
Epoch 2670, val loss: 1.0871250629425049
Epoch 2680, training loss: 621.5435180664062 = 0.009575585834681988 + 100.0 * 6.215339660644531
Epoch 2680, val loss: 1.0887893438339233
Epoch 2690, training loss: 621.5966186523438 = 0.00948182214051485 + 100.0 * 6.215871334075928
Epoch 2690, val loss: 1.0906972885131836
Epoch 2700, training loss: 621.7882080078125 = 0.009386960417032242 + 100.0 * 6.217788219451904
Epoch 2700, val loss: 1.0928350687026978
Epoch 2710, training loss: 621.385009765625 = 0.009288843721151352 + 100.0 * 6.213757514953613
Epoch 2710, val loss: 1.0936386585235596
Epoch 2720, training loss: 621.3043212890625 = 0.009200110100209713 + 100.0 * 6.212951183319092
Epoch 2720, val loss: 1.0954535007476807
Epoch 2730, training loss: 621.3121948242188 = 0.009112403728067875 + 100.0 * 6.21303129196167
Epoch 2730, val loss: 1.097208857536316
Epoch 2740, training loss: 621.6925659179688 = 0.00902597326785326 + 100.0 * 6.2168354988098145
Epoch 2740, val loss: 1.0989279747009277
Epoch 2750, training loss: 621.7377319335938 = 0.00893627479672432 + 100.0 * 6.217288494110107
Epoch 2750, val loss: 1.1002455949783325
Epoch 2760, training loss: 621.292724609375 = 0.00884783174842596 + 100.0 * 6.212838649749756
Epoch 2760, val loss: 1.1016018390655518
Epoch 2770, training loss: 621.1641235351562 = 0.00876228790730238 + 100.0 * 6.21155309677124
Epoch 2770, val loss: 1.1033304929733276
Epoch 2780, training loss: 621.650390625 = 0.008682656101882458 + 100.0 * 6.21641731262207
Epoch 2780, val loss: 1.105239987373352
Epoch 2790, training loss: 621.3740844726562 = 0.008599607273936272 + 100.0 * 6.2136549949646
Epoch 2790, val loss: 1.106392502784729
Epoch 2800, training loss: 621.8668212890625 = 0.008516699075698853 + 100.0 * 6.218582630157471
Epoch 2800, val loss: 1.1075177192687988
Epoch 2810, training loss: 621.1557006835938 = 0.008436188101768494 + 100.0 * 6.211472511291504
Epoch 2810, val loss: 1.109230875968933
Epoch 2820, training loss: 621.111328125 = 0.008360394276678562 + 100.0 * 6.211029529571533
Epoch 2820, val loss: 1.1108205318450928
Epoch 2830, training loss: 621.1211547851562 = 0.008285990916192532 + 100.0 * 6.2111287117004395
Epoch 2830, val loss: 1.1124681234359741
Epoch 2840, training loss: 621.7913818359375 = 0.008218812756240368 + 100.0 * 6.217831134796143
Epoch 2840, val loss: 1.114504098892212
Epoch 2850, training loss: 621.3543701171875 = 0.008142682723701 + 100.0 * 6.2134623527526855
Epoch 2850, val loss: 1.114937663078308
Epoch 2860, training loss: 621.5659790039062 = 0.008067960850894451 + 100.0 * 6.215579509735107
Epoch 2860, val loss: 1.1167064905166626
Epoch 2870, training loss: 621.0391235351562 = 0.007991339080035686 + 100.0 * 6.210311412811279
Epoch 2870, val loss: 1.117842197418213
Epoch 2880, training loss: 621.1203002929688 = 0.007923996075987816 + 100.0 * 6.211123466491699
Epoch 2880, val loss: 1.1195118427276611
Epoch 2890, training loss: 621.4825439453125 = 0.007858574390411377 + 100.0 * 6.214746475219727
Epoch 2890, val loss: 1.1208417415618896
Epoch 2900, training loss: 621.1432495117188 = 0.00778781296685338 + 100.0 * 6.2113542556762695
Epoch 2900, val loss: 1.1219602823257446
Epoch 2910, training loss: 621.2787475585938 = 0.007718187756836414 + 100.0 * 6.212710380554199
Epoch 2910, val loss: 1.1231982707977295
Epoch 2920, training loss: 621.2007446289062 = 0.007652398198843002 + 100.0 * 6.211931228637695
Epoch 2920, val loss: 1.1248869895935059
Epoch 2930, training loss: 620.9775390625 = 0.007587297819554806 + 100.0 * 6.209699630737305
Epoch 2930, val loss: 1.1261380910873413
Epoch 2940, training loss: 621.1218872070312 = 0.0075257522985339165 + 100.0 * 6.211143493652344
Epoch 2940, val loss: 1.1273494958877563
Epoch 2950, training loss: 621.36328125 = 0.007464243099093437 + 100.0 * 6.213558197021484
Epoch 2950, val loss: 1.1289563179016113
Epoch 2960, training loss: 621.3812866210938 = 0.007405977230519056 + 100.0 * 6.213738918304443
Epoch 2960, val loss: 1.130925178527832
Epoch 2970, training loss: 621.3198852539062 = 0.007337876595556736 + 100.0 * 6.213125705718994
Epoch 2970, val loss: 1.1316614151000977
Epoch 2980, training loss: 620.9951171875 = 0.007279369048774242 + 100.0 * 6.209878444671631
Epoch 2980, val loss: 1.1331524848937988
Epoch 2990, training loss: 620.8361206054688 = 0.00721919909119606 + 100.0 * 6.20828914642334
Epoch 2990, val loss: 1.1341403722763062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 861.6532592773438 = 1.9693156480789185 + 100.0 * 8.596839904785156
Epoch 0, val loss: 1.9616317749023438
Epoch 10, training loss: 861.561767578125 = 1.9587481021881104 + 100.0 * 8.596030235290527
Epoch 10, val loss: 1.9515186548233032
Epoch 20, training loss: 860.96533203125 = 1.9455060958862305 + 100.0 * 8.590198516845703
Epoch 20, val loss: 1.9386084079742432
Epoch 30, training loss: 857.0673217773438 = 1.9285279512405396 + 100.0 * 8.551387786865234
Epoch 30, val loss: 1.9219468832015991
Epoch 40, training loss: 834.476318359375 = 1.9086378812789917 + 100.0 * 8.325676918029785
Epoch 40, val loss: 1.9032222032546997
Epoch 50, training loss: 765.6859130859375 = 1.885960340499878 + 100.0 * 7.637999534606934
Epoch 50, val loss: 1.881736397743225
Epoch 60, training loss: 742.5477294921875 = 1.8676784038543701 + 100.0 * 7.406800270080566
Epoch 60, val loss: 1.86541748046875
Epoch 70, training loss: 722.8436889648438 = 1.8524888753890991 + 100.0 * 7.209911823272705
Epoch 70, val loss: 1.8515002727508545
Epoch 80, training loss: 703.6920776367188 = 1.8392534255981445 + 100.0 * 7.018528461456299
Epoch 80, val loss: 1.8396601676940918
Epoch 90, training loss: 691.673828125 = 1.8283820152282715 + 100.0 * 6.898454666137695
Epoch 90, val loss: 1.8294187784194946
Epoch 100, training loss: 681.2855224609375 = 1.8173943758010864 + 100.0 * 6.794681549072266
Epoch 100, val loss: 1.8187742233276367
Epoch 110, training loss: 673.3462524414062 = 1.8066295385360718 + 100.0 * 6.715395927429199
Epoch 110, val loss: 1.8085728883743286
Epoch 120, training loss: 668.520751953125 = 1.7959203720092773 + 100.0 * 6.667248725891113
Epoch 120, val loss: 1.7985053062438965
Epoch 130, training loss: 665.2111206054688 = 1.7850371599197388 + 100.0 * 6.634260654449463
Epoch 130, val loss: 1.7883676290512085
Epoch 140, training loss: 661.8815307617188 = 1.7740236520767212 + 100.0 * 6.601074695587158
Epoch 140, val loss: 1.7784836292266846
Epoch 150, training loss: 659.0924072265625 = 1.763458251953125 + 100.0 * 6.57328987121582
Epoch 150, val loss: 1.7688127756118774
Epoch 160, training loss: 656.7137451171875 = 1.7526764869689941 + 100.0 * 6.549610614776611
Epoch 160, val loss: 1.7588369846343994
Epoch 170, training loss: 654.819091796875 = 1.741193175315857 + 100.0 * 6.530778884887695
Epoch 170, val loss: 1.7483881711959839
Epoch 180, training loss: 653.3364868164062 = 1.7290561199188232 + 100.0 * 6.516074180603027
Epoch 180, val loss: 1.7374640703201294
Epoch 190, training loss: 651.630615234375 = 1.715985894203186 + 100.0 * 6.499145984649658
Epoch 190, val loss: 1.7259315252304077
Epoch 200, training loss: 650.2703857421875 = 1.7021119594573975 + 100.0 * 6.485682964324951
Epoch 200, val loss: 1.7137525081634521
Epoch 210, training loss: 649.6466064453125 = 1.6873303651809692 + 100.0 * 6.479592800140381
Epoch 210, val loss: 1.7007609605789185
Epoch 220, training loss: 647.9612426757812 = 1.671289324760437 + 100.0 * 6.462899684906006
Epoch 220, val loss: 1.6868585348129272
Epoch 230, training loss: 646.8861694335938 = 1.6542696952819824 + 100.0 * 6.4523186683654785
Epoch 230, val loss: 1.6721343994140625
Epoch 240, training loss: 645.7797241210938 = 1.6361279487609863 + 100.0 * 6.441436290740967
Epoch 240, val loss: 1.656503677368164
Epoch 250, training loss: 645.8591918945312 = 1.6167595386505127 + 100.0 * 6.442424297332764
Epoch 250, val loss: 1.6398850679397583
Epoch 260, training loss: 644.1559448242188 = 1.595983624458313 + 100.0 * 6.425599575042725
Epoch 260, val loss: 1.6222072839736938
Epoch 270, training loss: 643.23291015625 = 1.5743207931518555 + 100.0 * 6.416585445404053
Epoch 270, val loss: 1.603637933731079
Epoch 280, training loss: 642.4124145507812 = 1.551703691482544 + 100.0 * 6.408607482910156
Epoch 280, val loss: 1.584450125694275
Epoch 290, training loss: 642.1802368164062 = 1.5279942750930786 + 100.0 * 6.406522274017334
Epoch 290, val loss: 1.564509630203247
Epoch 300, training loss: 641.17041015625 = 1.5035359859466553 + 100.0 * 6.396668910980225
Epoch 300, val loss: 1.5438015460968018
Epoch 310, training loss: 640.3853759765625 = 1.4783966541290283 + 100.0 * 6.3890700340271
Epoch 310, val loss: 1.5226585865020752
Epoch 320, training loss: 639.941650390625 = 1.452724814414978 + 100.0 * 6.384889602661133
Epoch 320, val loss: 1.501136064529419
Epoch 330, training loss: 639.5350341796875 = 1.4266011714935303 + 100.0 * 6.381084442138672
Epoch 330, val loss: 1.479438066482544
Epoch 340, training loss: 638.7899169921875 = 1.4001007080078125 + 100.0 * 6.373898029327393
Epoch 340, val loss: 1.4575811624526978
Epoch 350, training loss: 638.2060546875 = 1.373488187789917 + 100.0 * 6.368325710296631
Epoch 350, val loss: 1.4357659816741943
Epoch 360, training loss: 638.503662109375 = 1.3468806743621826 + 100.0 * 6.371568202972412
Epoch 360, val loss: 1.4141043424606323
Epoch 370, training loss: 637.4725341796875 = 1.3202424049377441 + 100.0 * 6.361522674560547
Epoch 370, val loss: 1.3924026489257812
Epoch 380, training loss: 636.9710083007812 = 1.2937902212142944 + 100.0 * 6.356772422790527
Epoch 380, val loss: 1.371027946472168
Epoch 390, training loss: 636.623291015625 = 1.267549753189087 + 100.0 * 6.353557109832764
Epoch 390, val loss: 1.349995732307434
Epoch 400, training loss: 636.1467895507812 = 1.2415077686309814 + 100.0 * 6.349052429199219
Epoch 400, val loss: 1.3292263746261597
Epoch 410, training loss: 635.7088623046875 = 1.2157928943634033 + 100.0 * 6.344930648803711
Epoch 410, val loss: 1.3089585304260254
Epoch 420, training loss: 635.7421264648438 = 1.190430760383606 + 100.0 * 6.345516681671143
Epoch 420, val loss: 1.2889938354492188
Epoch 430, training loss: 635.2278442382812 = 1.1655579805374146 + 100.0 * 6.340622425079346
Epoch 430, val loss: 1.26923406124115
Epoch 440, training loss: 634.7105712890625 = 1.1409450769424438 + 100.0 * 6.335696697235107
Epoch 440, val loss: 1.2501544952392578
Epoch 450, training loss: 634.44921875 = 1.1168802976608276 + 100.0 * 6.3333234786987305
Epoch 450, val loss: 1.2314486503601074
Epoch 460, training loss: 634.1900634765625 = 1.0931239128112793 + 100.0 * 6.330969333648682
Epoch 460, val loss: 1.213130235671997
Epoch 470, training loss: 633.82470703125 = 1.0699021816253662 + 100.0 * 6.327548027038574
Epoch 470, val loss: 1.1954216957092285
Epoch 480, training loss: 633.48583984375 = 1.0472326278686523 + 100.0 * 6.324386119842529
Epoch 480, val loss: 1.1782523393630981
Epoch 490, training loss: 633.6142578125 = 1.025101900100708 + 100.0 * 6.325891971588135
Epoch 490, val loss: 1.1617836952209473
Epoch 500, training loss: 633.49169921875 = 1.0034844875335693 + 100.0 * 6.3248820304870605
Epoch 500, val loss: 1.145634412765503
Epoch 510, training loss: 632.806884765625 = 0.9823750257492065 + 100.0 * 6.3182454109191895
Epoch 510, val loss: 1.1302987337112427
Epoch 520, training loss: 632.5078735351562 = 0.9619255661964417 + 100.0 * 6.315459728240967
Epoch 520, val loss: 1.1157594919204712
Epoch 530, training loss: 632.2615356445312 = 0.9421286582946777 + 100.0 * 6.3131937980651855
Epoch 530, val loss: 1.1020053625106812
Epoch 540, training loss: 632.17919921875 = 0.9229437708854675 + 100.0 * 6.312561988830566
Epoch 540, val loss: 1.0888521671295166
Epoch 550, training loss: 631.9771728515625 = 0.9040989875793457 + 100.0 * 6.310730457305908
Epoch 550, val loss: 1.0766493082046509
Epoch 560, training loss: 632.148193359375 = 0.885898768901825 + 100.0 * 6.312622547149658
Epoch 560, val loss: 1.0647400617599487
Epoch 570, training loss: 631.5718994140625 = 0.8680596947669983 + 100.0 * 6.3070387840271
Epoch 570, val loss: 1.0535173416137695
Epoch 580, training loss: 631.3463134765625 = 0.8508234024047852 + 100.0 * 6.304955005645752
Epoch 580, val loss: 1.0431400537490845
Epoch 590, training loss: 631.05078125 = 0.8341729640960693 + 100.0 * 6.302165985107422
Epoch 590, val loss: 1.033324122428894
Epoch 600, training loss: 631.0169677734375 = 0.8179877400398254 + 100.0 * 6.301990032196045
Epoch 600, val loss: 1.024034023284912
Epoch 610, training loss: 630.8668823242188 = 0.8020404577255249 + 100.0 * 6.300648212432861
Epoch 610, val loss: 1.0155061483383179
Epoch 620, training loss: 630.720703125 = 0.7864825129508972 + 100.0 * 6.299342155456543
Epoch 620, val loss: 1.0072029829025269
Epoch 630, training loss: 630.32080078125 = 0.771350085735321 + 100.0 * 6.295494556427002
Epoch 630, val loss: 0.9996274709701538
Epoch 640, training loss: 630.41455078125 = 0.7565624117851257 + 100.0 * 6.296579360961914
Epoch 640, val loss: 0.9925205707550049
Epoch 650, training loss: 630.26953125 = 0.7418695688247681 + 100.0 * 6.295276641845703
Epoch 650, val loss: 0.9855515360832214
Epoch 660, training loss: 630.0316162109375 = 0.7275493741035461 + 100.0 * 6.293040752410889
Epoch 660, val loss: 0.9791498184204102
Epoch 670, training loss: 629.7744140625 = 0.7134963274002075 + 100.0 * 6.290609359741211
Epoch 670, val loss: 0.97325599193573
Epoch 680, training loss: 630.1804809570312 = 0.6995357871055603 + 100.0 * 6.294809341430664
Epoch 680, val loss: 0.9677134156227112
Epoch 690, training loss: 629.4065551757812 = 0.6858775019645691 + 100.0 * 6.287207126617432
Epoch 690, val loss: 0.9620243906974792
Epoch 700, training loss: 629.3585815429688 = 0.6723600625991821 + 100.0 * 6.286861896514893
Epoch 700, val loss: 0.9568288326263428
Epoch 710, training loss: 629.181396484375 = 0.65901780128479 + 100.0 * 6.285223960876465
Epoch 710, val loss: 0.952166736125946
Epoch 720, training loss: 629.658935546875 = 0.6458085775375366 + 100.0 * 6.290131092071533
Epoch 720, val loss: 0.9476028084754944
Epoch 730, training loss: 629.080078125 = 0.6326690912246704 + 100.0 * 6.284473896026611
Epoch 730, val loss: 0.9431517720222473
Epoch 740, training loss: 629.1307983398438 = 0.6196427345275879 + 100.0 * 6.285111427307129
Epoch 740, val loss: 0.9390220642089844
Epoch 750, training loss: 628.6927490234375 = 0.6066594123840332 + 100.0 * 6.280860424041748
Epoch 750, val loss: 0.9352128505706787
Epoch 760, training loss: 628.6128540039062 = 0.5939169526100159 + 100.0 * 6.280189037322998
Epoch 760, val loss: 0.9316185116767883
Epoch 770, training loss: 628.6400756835938 = 0.5812720060348511 + 100.0 * 6.280587673187256
Epoch 770, val loss: 0.9281594157218933
Epoch 780, training loss: 628.6632690429688 = 0.5687445998191833 + 100.0 * 6.280945301055908
Epoch 780, val loss: 0.9247905015945435
Epoch 790, training loss: 628.5921020507812 = 0.5561797618865967 + 100.0 * 6.280359745025635
Epoch 790, val loss: 0.9217544794082642
Epoch 800, training loss: 628.2202758789062 = 0.5437504649162292 + 100.0 * 6.2767653465271
Epoch 800, val loss: 0.9187397360801697
Epoch 810, training loss: 627.9324951171875 = 0.5314511060714722 + 100.0 * 6.27401065826416
Epoch 810, val loss: 0.9160973429679871
Epoch 820, training loss: 628.5201416015625 = 0.5192628502845764 + 100.0 * 6.280008792877197
Epoch 820, val loss: 0.9139009714126587
Epoch 830, training loss: 628.133056640625 = 0.5073039531707764 + 100.0 * 6.276257514953613
Epoch 830, val loss: 0.9109260439872742
Epoch 840, training loss: 627.7072143554688 = 0.49524974822998047 + 100.0 * 6.272119522094727
Epoch 840, val loss: 0.9090100526809692
Epoch 850, training loss: 628.3882446289062 = 0.48353224992752075 + 100.0 * 6.279047012329102
Epoch 850, val loss: 0.9067814350128174
Epoch 860, training loss: 627.7276611328125 = 0.47170189023017883 + 100.0 * 6.272560119628906
Epoch 860, val loss: 0.904769778251648
Epoch 870, training loss: 627.373779296875 = 0.4601213335990906 + 100.0 * 6.269136428833008
Epoch 870, val loss: 0.903169572353363
Epoch 880, training loss: 627.5018310546875 = 0.44875121116638184 + 100.0 * 6.270530700683594
Epoch 880, val loss: 0.9016857147216797
Epoch 890, training loss: 627.1744995117188 = 0.43737316131591797 + 100.0 * 6.26737117767334
Epoch 890, val loss: 0.8998364806175232
Epoch 900, training loss: 627.1163940429688 = 0.42625540494918823 + 100.0 * 6.26690149307251
Epoch 900, val loss: 0.8987874984741211
Epoch 910, training loss: 627.2119750976562 = 0.4152570962905884 + 100.0 * 6.2679667472839355
Epoch 910, val loss: 0.8974481225013733
Epoch 920, training loss: 627.2723388671875 = 0.4044773578643799 + 100.0 * 6.268678665161133
Epoch 920, val loss: 0.8963678479194641
Epoch 930, training loss: 626.9395751953125 = 0.3937690556049347 + 100.0 * 6.265457630157471
Epoch 930, val loss: 0.8956793546676636
Epoch 940, training loss: 626.761474609375 = 0.3832864463329315 + 100.0 * 6.263782024383545
Epoch 940, val loss: 0.8948441743850708
Epoch 950, training loss: 626.6884765625 = 0.3730526566505432 + 100.0 * 6.263154029846191
Epoch 950, val loss: 0.8942356705665588
Epoch 960, training loss: 626.9445190429688 = 0.3629732131958008 + 100.0 * 6.265815258026123
Epoch 960, val loss: 0.8938058614730835
Epoch 970, training loss: 627.1592407226562 = 0.35314467549324036 + 100.0 * 6.26806116104126
Epoch 970, val loss: 0.8932884931564331
Epoch 980, training loss: 626.7202758789062 = 0.34346747398376465 + 100.0 * 6.263768196105957
Epoch 980, val loss: 0.8930613994598389
Epoch 990, training loss: 626.2704467773438 = 0.33398568630218506 + 100.0 * 6.259364604949951
Epoch 990, val loss: 0.8929921984672546
Epoch 1000, training loss: 626.2525634765625 = 0.32480835914611816 + 100.0 * 6.25927734375
Epoch 1000, val loss: 0.8929304480552673
Epoch 1010, training loss: 626.5807495117188 = 0.3158562481403351 + 100.0 * 6.262649059295654
Epoch 1010, val loss: 0.8930256366729736
Epoch 1020, training loss: 626.4243774414062 = 0.3070422410964966 + 100.0 * 6.261173248291016
Epoch 1020, val loss: 0.893565833568573
Epoch 1030, training loss: 626.4058837890625 = 0.2984677255153656 + 100.0 * 6.261074066162109
Epoch 1030, val loss: 0.8938391804695129
Epoch 1040, training loss: 626.089599609375 = 0.29004281759262085 + 100.0 * 6.25799560546875
Epoch 1040, val loss: 0.8944581151008606
Epoch 1050, training loss: 625.8549194335938 = 0.2819626033306122 + 100.0 * 6.2557291984558105
Epoch 1050, val loss: 0.8953700661659241
Epoch 1060, training loss: 625.9263916015625 = 0.27408677339553833 + 100.0 * 6.2565226554870605
Epoch 1060, val loss: 0.8962119817733765
Epoch 1070, training loss: 625.8621215820312 = 0.2663958668708801 + 100.0 * 6.255957126617432
Epoch 1070, val loss: 0.8973533511161804
Epoch 1080, training loss: 625.8408203125 = 0.25887757539749146 + 100.0 * 6.255819797515869
Epoch 1080, val loss: 0.8985170125961304
Epoch 1090, training loss: 625.6778564453125 = 0.2515583336353302 + 100.0 * 6.254262447357178
Epoch 1090, val loss: 0.8998668789863586
Epoch 1100, training loss: 625.6180419921875 = 0.24451075494289398 + 100.0 * 6.253735065460205
Epoch 1100, val loss: 0.9012224674224854
Epoch 1110, training loss: 625.692138671875 = 0.23772649466991425 + 100.0 * 6.254543781280518
Epoch 1110, val loss: 0.9029676914215088
Epoch 1120, training loss: 625.621826171875 = 0.23105867207050323 + 100.0 * 6.253907680511475
Epoch 1120, val loss: 0.9047483801841736
Epoch 1130, training loss: 625.3868408203125 = 0.22452223300933838 + 100.0 * 6.251622676849365
Epoch 1130, val loss: 0.906510055065155
Epoch 1140, training loss: 625.3165283203125 = 0.21831688284873962 + 100.0 * 6.250982284545898
Epoch 1140, val loss: 0.9086525440216064
Epoch 1150, training loss: 626.4096069335938 = 0.21225476264953613 + 100.0 * 6.2619733810424805
Epoch 1150, val loss: 0.9106782078742981
Epoch 1160, training loss: 625.2578735351562 = 0.20626555383205414 + 100.0 * 6.250516414642334
Epoch 1160, val loss: 0.9129652380943298
Epoch 1170, training loss: 625.0771484375 = 0.20051555335521698 + 100.0 * 6.2487664222717285
Epoch 1170, val loss: 0.9153981804847717
Epoch 1180, training loss: 625.0357666015625 = 0.1949884295463562 + 100.0 * 6.248407363891602
Epoch 1180, val loss: 0.9179609417915344
Epoch 1190, training loss: 625.3592529296875 = 0.18963776528835297 + 100.0 * 6.2516961097717285
Epoch 1190, val loss: 0.9205338358879089
Epoch 1200, training loss: 624.9198608398438 = 0.18439708650112152 + 100.0 * 6.247354507446289
Epoch 1200, val loss: 0.9233434796333313
Epoch 1210, training loss: 625.3842163085938 = 0.17934906482696533 + 100.0 * 6.252048969268799
Epoch 1210, val loss: 0.9261653423309326
Epoch 1220, training loss: 625.0671997070312 = 0.17441532015800476 + 100.0 * 6.248927593231201
Epoch 1220, val loss: 0.9292051196098328
Epoch 1230, training loss: 624.8351440429688 = 0.16962075233459473 + 100.0 * 6.246654987335205
Epoch 1230, val loss: 0.9323911666870117
Epoch 1240, training loss: 624.726318359375 = 0.16502192616462708 + 100.0 * 6.245612621307373
Epoch 1240, val loss: 0.935646653175354
Epoch 1250, training loss: 624.9177856445312 = 0.16058096289634705 + 100.0 * 6.24757194519043
Epoch 1250, val loss: 0.9390451312065125
Epoch 1260, training loss: 624.6835327148438 = 0.15622267127037048 + 100.0 * 6.245272636413574
Epoch 1260, val loss: 0.9422338604927063
Epoch 1270, training loss: 624.9141235351562 = 0.15201149880886078 + 100.0 * 6.247621059417725
Epoch 1270, val loss: 0.9457203149795532
Epoch 1280, training loss: 624.7676391601562 = 0.1478765457868576 + 100.0 * 6.246197700500488
Epoch 1280, val loss: 0.9493603706359863
Epoch 1290, training loss: 624.50830078125 = 0.14388428628444672 + 100.0 * 6.2436442375183105
Epoch 1290, val loss: 0.9529039263725281
Epoch 1300, training loss: 624.5151977539062 = 0.14006765186786652 + 100.0 * 6.243751049041748
Epoch 1300, val loss: 0.9568727612495422
Epoch 1310, training loss: 624.559326171875 = 0.13635583221912384 + 100.0 * 6.244229793548584
Epoch 1310, val loss: 0.9606857895851135
Epoch 1320, training loss: 624.6217041015625 = 0.13275764882564545 + 100.0 * 6.244889259338379
Epoch 1320, val loss: 0.9647102355957031
Epoch 1330, training loss: 624.4168090820312 = 0.12920770049095154 + 100.0 * 6.242876052856445
Epoch 1330, val loss: 0.9685019254684448
Epoch 1340, training loss: 624.2957153320312 = 0.1258058398962021 + 100.0 * 6.24169921875
Epoch 1340, val loss: 0.9728361964225769
Epoch 1350, training loss: 625.0504760742188 = 0.12249263375997543 + 100.0 * 6.249279499053955
Epoch 1350, val loss: 0.9768439531326294
Epoch 1360, training loss: 624.4807739257812 = 0.11931359022855759 + 100.0 * 6.243614196777344
Epoch 1360, val loss: 0.9812615513801575
Epoch 1370, training loss: 624.161865234375 = 0.116164430975914 + 100.0 * 6.240457057952881
Epoch 1370, val loss: 0.9855014085769653
Epoch 1380, training loss: 624.0758666992188 = 0.11317461729049683 + 100.0 * 6.239626884460449
Epoch 1380, val loss: 0.9899155497550964
Epoch 1390, training loss: 624.6978149414062 = 0.11025560647249222 + 100.0 * 6.245875835418701
Epoch 1390, val loss: 0.994316816329956
Epoch 1400, training loss: 624.237060546875 = 0.10746477544307709 + 100.0 * 6.24129581451416
Epoch 1400, val loss: 0.998947024345398
Epoch 1410, training loss: 624.1445922851562 = 0.10467904806137085 + 100.0 * 6.240399360656738
Epoch 1410, val loss: 1.0034371614456177
Epoch 1420, training loss: 623.97802734375 = 0.1020364984869957 + 100.0 * 6.238759994506836
Epoch 1420, val loss: 1.0081746578216553
Epoch 1430, training loss: 624.2982788085938 = 0.09946387261152267 + 100.0 * 6.241988182067871
Epoch 1430, val loss: 1.0127352476119995
Epoch 1440, training loss: 623.8242797851562 = 0.09693195670843124 + 100.0 * 6.237273693084717
Epoch 1440, val loss: 1.0171316862106323
Epoch 1450, training loss: 623.723388671875 = 0.09450653940439224 + 100.0 * 6.236289024353027
Epoch 1450, val loss: 1.0219694375991821
Epoch 1460, training loss: 624.14599609375 = 0.09218917042016983 + 100.0 * 6.240538597106934
Epoch 1460, val loss: 1.0266923904418945
Epoch 1470, training loss: 623.7744140625 = 0.08987075835466385 + 100.0 * 6.236845970153809
Epoch 1470, val loss: 1.0311156511306763
Epoch 1480, training loss: 624.1834106445312 = 0.08765766769647598 + 100.0 * 6.240957736968994
Epoch 1480, val loss: 1.0358936786651611
Epoch 1490, training loss: 623.6835327148438 = 0.0854908749461174 + 100.0 * 6.23598051071167
Epoch 1490, val loss: 1.0407483577728271
Epoch 1500, training loss: 623.5491333007812 = 0.08340442925691605 + 100.0 * 6.2346577644348145
Epoch 1500, val loss: 1.0455617904663086
Epoch 1510, training loss: 624.2017822265625 = 0.08140531182289124 + 100.0 * 6.241203784942627
Epoch 1510, val loss: 1.0503926277160645
Epoch 1520, training loss: 623.67431640625 = 0.0794445276260376 + 100.0 * 6.23594856262207
Epoch 1520, val loss: 1.0549074411392212
Epoch 1530, training loss: 623.5374145507812 = 0.07753261923789978 + 100.0 * 6.2345991134643555
Epoch 1530, val loss: 1.0598816871643066
Epoch 1540, training loss: 623.44921875 = 0.07570187002420425 + 100.0 * 6.233735084533691
Epoch 1540, val loss: 1.0646655559539795
Epoch 1550, training loss: 624.1574096679688 = 0.07393205910921097 + 100.0 * 6.240835189819336
Epoch 1550, val loss: 1.0695091485977173
Epoch 1560, training loss: 623.589111328125 = 0.07217679172754288 + 100.0 * 6.235168933868408
Epoch 1560, val loss: 1.0743331909179688
Epoch 1570, training loss: 623.6436767578125 = 0.07050743699073792 + 100.0 * 6.235732078552246
Epoch 1570, val loss: 1.0792540311813354
Epoch 1580, training loss: 623.6288452148438 = 0.06888945400714874 + 100.0 * 6.235599517822266
Epoch 1580, val loss: 1.0840696096420288
Epoch 1590, training loss: 623.2310791015625 = 0.06727196276187897 + 100.0 * 6.231638431549072
Epoch 1590, val loss: 1.0887603759765625
Epoch 1600, training loss: 623.2337646484375 = 0.06572683155536652 + 100.0 * 6.231680393218994
Epoch 1600, val loss: 1.0936269760131836
Epoch 1610, training loss: 623.4451904296875 = 0.06425230205059052 + 100.0 * 6.233809471130371
Epoch 1610, val loss: 1.098487377166748
Epoch 1620, training loss: 623.2621459960938 = 0.06281036883592606 + 100.0 * 6.231993198394775
Epoch 1620, val loss: 1.1034431457519531
Epoch 1630, training loss: 623.4215698242188 = 0.06142379716038704 + 100.0 * 6.2336015701293945
Epoch 1630, val loss: 1.108260989189148
Epoch 1640, training loss: 623.3596801757812 = 0.060033731162548065 + 100.0 * 6.232995986938477
Epoch 1640, val loss: 1.1125495433807373
Epoch 1650, training loss: 623.1377563476562 = 0.05870472639799118 + 100.0 * 6.230790138244629
Epoch 1650, val loss: 1.1178159713745117
Epoch 1660, training loss: 623.0892944335938 = 0.057433899492025375 + 100.0 * 6.230318546295166
Epoch 1660, val loss: 1.122518539428711
Epoch 1670, training loss: 623.1937255859375 = 0.0561833418905735 + 100.0 * 6.231375217437744
Epoch 1670, val loss: 1.1273607015609741
Epoch 1680, training loss: 623.166259765625 = 0.05495630204677582 + 100.0 * 6.231113433837891
Epoch 1680, val loss: 1.1317968368530273
Epoch 1690, training loss: 623.0619506835938 = 0.053791336715221405 + 100.0 * 6.230082035064697
Epoch 1690, val loss: 1.1367824077606201
Epoch 1700, training loss: 623.3687744140625 = 0.052633337676525116 + 100.0 * 6.233161926269531
Epoch 1700, val loss: 1.1414268016815186
Epoch 1710, training loss: 623.051025390625 = 0.051519379019737244 + 100.0 * 6.229995250701904
Epoch 1710, val loss: 1.1464946269989014
Epoch 1720, training loss: 622.926513671875 = 0.050421275198459625 + 100.0 * 6.228760719299316
Epoch 1720, val loss: 1.1509968042373657
Epoch 1730, training loss: 623.2304077148438 = 0.049385711550712585 + 100.0 * 6.231810092926025
Epoch 1730, val loss: 1.1557717323303223
Epoch 1740, training loss: 622.8053588867188 = 0.04833737015724182 + 100.0 * 6.227570056915283
Epoch 1740, val loss: 1.160249948501587
Epoch 1750, training loss: 622.757568359375 = 0.047354962676763535 + 100.0 * 6.227101802825928
Epoch 1750, val loss: 1.1652647256851196
Epoch 1760, training loss: 622.7099609375 = 0.04638433828949928 + 100.0 * 6.226635932922363
Epoch 1760, val loss: 1.169764518737793
Epoch 1770, training loss: 623.4287109375 = 0.045451655983924866 + 100.0 * 6.233832359313965
Epoch 1770, val loss: 1.1743756532669067
Epoch 1780, training loss: 622.8659057617188 = 0.04454628378152847 + 100.0 * 6.228213787078857
Epoch 1780, val loss: 1.1789648532867432
Epoch 1790, training loss: 622.7725219726562 = 0.04363773763179779 + 100.0 * 6.227288722991943
Epoch 1790, val loss: 1.183584451675415
Epoch 1800, training loss: 623.1035766601562 = 0.042780857533216476 + 100.0 * 6.230607986450195
Epoch 1800, val loss: 1.1879791021347046
Epoch 1810, training loss: 622.6464233398438 = 0.04192940890789032 + 100.0 * 6.226044654846191
Epoch 1810, val loss: 1.1927913427352905
Epoch 1820, training loss: 622.6842041015625 = 0.0411158911883831 + 100.0 * 6.226430416107178
Epoch 1820, val loss: 1.1973706483840942
Epoch 1830, training loss: 623.0814819335938 = 0.0403243787586689 + 100.0 * 6.230411529541016
Epoch 1830, val loss: 1.2015002965927124
Epoch 1840, training loss: 622.7139892578125 = 0.039536673575639725 + 100.0 * 6.226744174957275
Epoch 1840, val loss: 1.2062610387802124
Epoch 1850, training loss: 622.524658203125 = 0.03876852989196777 + 100.0 * 6.22485876083374
Epoch 1850, val loss: 1.2106382846832275
Epoch 1860, training loss: 622.4581298828125 = 0.03804520145058632 + 100.0 * 6.224201202392578
Epoch 1860, val loss: 1.215333104133606
Epoch 1870, training loss: 623.037841796875 = 0.0373411551117897 + 100.0 * 6.230004787445068
Epoch 1870, val loss: 1.2193864583969116
Epoch 1880, training loss: 622.5801391601562 = 0.03663676604628563 + 100.0 * 6.225435256958008
Epoch 1880, val loss: 1.2240393161773682
Epoch 1890, training loss: 622.3864135742188 = 0.035949431359767914 + 100.0 * 6.223504543304443
Epoch 1890, val loss: 1.2284283638000488
Epoch 1900, training loss: 622.365966796875 = 0.035286568105220795 + 100.0 * 6.223306655883789
Epoch 1900, val loss: 1.2325102090835571
Epoch 1910, training loss: 622.75439453125 = 0.03464952111244202 + 100.0 * 6.227197170257568
Epoch 1910, val loss: 1.2367595434188843
Epoch 1920, training loss: 622.5889892578125 = 0.03401084616780281 + 100.0 * 6.225549697875977
Epoch 1920, val loss: 1.2413853406906128
Epoch 1930, training loss: 622.389892578125 = 0.03338884562253952 + 100.0 * 6.223565101623535
Epoch 1930, val loss: 1.2451341152191162
Epoch 1940, training loss: 622.36865234375 = 0.03279334679245949 + 100.0 * 6.223358631134033
Epoch 1940, val loss: 1.24976646900177
Epoch 1950, training loss: 622.3016967773438 = 0.03220806270837784 + 100.0 * 6.2226948738098145
Epoch 1950, val loss: 1.2538435459136963
Epoch 1960, training loss: 622.38720703125 = 0.031642086803913116 + 100.0 * 6.223556041717529
Epoch 1960, val loss: 1.25816011428833
Epoch 1970, training loss: 622.1838989257812 = 0.031091414391994476 + 100.0 * 6.221528053283691
Epoch 1970, val loss: 1.2625762224197388
Epoch 1980, training loss: 622.3896484375 = 0.03056296706199646 + 100.0 * 6.223590850830078
Epoch 1980, val loss: 1.2666449546813965
Epoch 1990, training loss: 622.2662353515625 = 0.03002586029469967 + 100.0 * 6.222362041473389
Epoch 1990, val loss: 1.2707706689834595
Epoch 2000, training loss: 623.0386962890625 = 0.0295004453510046 + 100.0 * 6.2300920486450195
Epoch 2000, val loss: 1.2745774984359741
Epoch 2010, training loss: 622.3726196289062 = 0.028996123000979424 + 100.0 * 6.22343635559082
Epoch 2010, val loss: 1.278552532196045
Epoch 2020, training loss: 622.1275024414062 = 0.028490908443927765 + 100.0 * 6.220990180969238
Epoch 2020, val loss: 1.2826987504959106
Epoch 2030, training loss: 622.1004638671875 = 0.02801813744008541 + 100.0 * 6.220724582672119
Epoch 2030, val loss: 1.2868300676345825
Epoch 2040, training loss: 622.8369140625 = 0.02755141258239746 + 100.0 * 6.228094100952148
Epoch 2040, val loss: 1.2903202772140503
Epoch 2050, training loss: 622.1925659179688 = 0.027104362845420837 + 100.0 * 6.221654415130615
Epoch 2050, val loss: 1.295047402381897
Epoch 2060, training loss: 621.9696655273438 = 0.02664290741086006 + 100.0 * 6.219429969787598
Epoch 2060, val loss: 1.2987115383148193
Epoch 2070, training loss: 621.9061889648438 = 0.026217175647616386 + 100.0 * 6.218799591064453
Epoch 2070, val loss: 1.3029183149337769
Epoch 2080, training loss: 622.9515380859375 = 0.02580633945763111 + 100.0 * 6.229257106781006
Epoch 2080, val loss: 1.3067474365234375
Epoch 2090, training loss: 622.2896118164062 = 0.025382718071341515 + 100.0 * 6.222642421722412
Epoch 2090, val loss: 1.3101898431777954
Epoch 2100, training loss: 622.0469970703125 = 0.024969549849629402 + 100.0 * 6.220220565795898
Epoch 2100, val loss: 1.3145173788070679
Epoch 2110, training loss: 621.9644165039062 = 0.024575132876634598 + 100.0 * 6.219398498535156
Epoch 2110, val loss: 1.3181687593460083
Epoch 2120, training loss: 622.1231689453125 = 0.024197855964303017 + 100.0 * 6.220990180969238
Epoch 2120, val loss: 1.3221423625946045
Epoch 2130, training loss: 621.9427490234375 = 0.0238193329423666 + 100.0 * 6.219189167022705
Epoch 2130, val loss: 1.3256503343582153
Epoch 2140, training loss: 622.247802734375 = 0.023450138047337532 + 100.0 * 6.222243785858154
Epoch 2140, val loss: 1.3296284675598145
Epoch 2150, training loss: 621.9494018554688 = 0.023087747395038605 + 100.0 * 6.219263553619385
Epoch 2150, val loss: 1.333474040031433
Epoch 2160, training loss: 622.1129150390625 = 0.022727152332663536 + 100.0 * 6.220901966094971
Epoch 2160, val loss: 1.3370953798294067
Epoch 2170, training loss: 621.85693359375 = 0.02237912453711033 + 100.0 * 6.2183451652526855
Epoch 2170, val loss: 1.3407875299453735
Epoch 2180, training loss: 621.7335815429688 = 0.022035669535398483 + 100.0 * 6.21711540222168
Epoch 2180, val loss: 1.3443329334259033
Epoch 2190, training loss: 621.7135620117188 = 0.021705012768507004 + 100.0 * 6.216918468475342
Epoch 2190, val loss: 1.3480626344680786
Epoch 2200, training loss: 622.6653442382812 = 0.021392928436398506 + 100.0 * 6.226438999176025
Epoch 2200, val loss: 1.351222276687622
Epoch 2210, training loss: 622.0342407226562 = 0.021082678809762 + 100.0 * 6.220131874084473
Epoch 2210, val loss: 1.3555591106414795
Epoch 2220, training loss: 621.817138671875 = 0.02075888216495514 + 100.0 * 6.217963695526123
Epoch 2220, val loss: 1.3588093519210815
Epoch 2230, training loss: 621.8123168945312 = 0.02046375907957554 + 100.0 * 6.217918395996094
Epoch 2230, val loss: 1.3628714084625244
Epoch 2240, training loss: 621.9432983398438 = 0.020166434347629547 + 100.0 * 6.219231605529785
Epoch 2240, val loss: 1.3662123680114746
Epoch 2250, training loss: 621.931640625 = 0.019868925213813782 + 100.0 * 6.219117641448975
Epoch 2250, val loss: 1.3692833185195923
Epoch 2260, training loss: 621.5936279296875 = 0.019582873210310936 + 100.0 * 6.215740203857422
Epoch 2260, val loss: 1.3729802370071411
Epoch 2270, training loss: 621.5924682617188 = 0.019310513511300087 + 100.0 * 6.215732097625732
Epoch 2270, val loss: 1.3765742778778076
Epoch 2280, training loss: 622.6478881835938 = 0.019048364832997322 + 100.0 * 6.226288318634033
Epoch 2280, val loss: 1.3799903392791748
Epoch 2290, training loss: 621.8084716796875 = 0.018764182925224304 + 100.0 * 6.217897415161133
Epoch 2290, val loss: 1.3828167915344238
Epoch 2300, training loss: 621.5530395507812 = 0.0185066070407629 + 100.0 * 6.21534538269043
Epoch 2300, val loss: 1.3866456747055054
Epoch 2310, training loss: 621.4584350585938 = 0.018252339214086533 + 100.0 * 6.214401721954346
Epoch 2310, val loss: 1.3900328874588013
Epoch 2320, training loss: 621.5991821289062 = 0.018012577667832375 + 100.0 * 6.215811729431152
Epoch 2320, val loss: 1.3933881521224976
Epoch 2330, training loss: 621.9220581054688 = 0.017773959785699844 + 100.0 * 6.219042778015137
Epoch 2330, val loss: 1.3968487977981567
Epoch 2340, training loss: 621.7902221679688 = 0.01751646399497986 + 100.0 * 6.217727184295654
Epoch 2340, val loss: 1.3996793031692505
Epoch 2350, training loss: 621.982666015625 = 0.017280559986829758 + 100.0 * 6.219654083251953
Epoch 2350, val loss: 1.4031888246536255
Epoch 2360, training loss: 621.616943359375 = 0.017050009220838547 + 100.0 * 6.215999126434326
Epoch 2360, val loss: 1.4063853025436401
Epoch 2370, training loss: 621.748046875 = 0.016820231452584267 + 100.0 * 6.217311859130859
Epoch 2370, val loss: 1.4097131490707397
Epoch 2380, training loss: 621.4217529296875 = 0.01659780740737915 + 100.0 * 6.214051723480225
Epoch 2380, val loss: 1.4126757383346558
Epoch 2390, training loss: 621.4427490234375 = 0.016381006687879562 + 100.0 * 6.214263916015625
Epoch 2390, val loss: 1.4160001277923584
Epoch 2400, training loss: 621.4926147460938 = 0.016171669587492943 + 100.0 * 6.21476411819458
Epoch 2400, val loss: 1.4195654392242432
Epoch 2410, training loss: 621.5009155273438 = 0.015965281054377556 + 100.0 * 6.214849472045898
Epoch 2410, val loss: 1.422588586807251
Epoch 2420, training loss: 621.31298828125 = 0.015759583562612534 + 100.0 * 6.212972640991211
Epoch 2420, val loss: 1.425262689590454
Epoch 2430, training loss: 621.309326171875 = 0.015560280531644821 + 100.0 * 6.212937355041504
Epoch 2430, val loss: 1.4285554885864258
Epoch 2440, training loss: 621.6583862304688 = 0.015365436673164368 + 100.0 * 6.216430187225342
Epoch 2440, val loss: 1.4313781261444092
Epoch 2450, training loss: 621.79638671875 = 0.015177114866673946 + 100.0 * 6.2178120613098145
Epoch 2450, val loss: 1.4349178075790405
Epoch 2460, training loss: 621.3721313476562 = 0.014978493563830853 + 100.0 * 6.213572025299072
Epoch 2460, val loss: 1.4375730752944946
Epoch 2470, training loss: 621.2415161132812 = 0.014795098453760147 + 100.0 * 6.2122673988342285
Epoch 2470, val loss: 1.4409273862838745
Epoch 2480, training loss: 621.5607299804688 = 0.014619159512221813 + 100.0 * 6.215460777282715
Epoch 2480, val loss: 1.4440057277679443
Epoch 2490, training loss: 621.2595825195312 = 0.014434002339839935 + 100.0 * 6.212451934814453
Epoch 2490, val loss: 1.446812391281128
Epoch 2500, training loss: 621.3773803710938 = 0.014262423850595951 + 100.0 * 6.2136311531066895
Epoch 2500, val loss: 1.449975609779358
Epoch 2510, training loss: 621.30908203125 = 0.014090447686612606 + 100.0 * 6.212949752807617
Epoch 2510, val loss: 1.4529638290405273
Epoch 2520, training loss: 621.4171142578125 = 0.013921491801738739 + 100.0 * 6.21403169631958
Epoch 2520, val loss: 1.455737590789795
Epoch 2530, training loss: 621.2673950195312 = 0.013753194361925125 + 100.0 * 6.212536334991455
Epoch 2530, val loss: 1.458532691001892
Epoch 2540, training loss: 621.2058715820312 = 0.013593428768217564 + 100.0 * 6.211922645568848
Epoch 2540, val loss: 1.4612336158752441
Epoch 2550, training loss: 621.4083251953125 = 0.01343693770468235 + 100.0 * 6.213949203491211
Epoch 2550, val loss: 1.4642109870910645
Epoch 2560, training loss: 621.2366943359375 = 0.013284738175570965 + 100.0 * 6.212234020233154
Epoch 2560, val loss: 1.4672430753707886
Epoch 2570, training loss: 621.265869140625 = 0.013125121593475342 + 100.0 * 6.212527751922607
Epoch 2570, val loss: 1.4699103832244873
Epoch 2580, training loss: 621.0337524414062 = 0.012970930896699429 + 100.0 * 6.210207462310791
Epoch 2580, val loss: 1.4726601839065552
Epoch 2590, training loss: 621.1209716796875 = 0.012828119099140167 + 100.0 * 6.211081504821777
Epoch 2590, val loss: 1.4753715991973877
Epoch 2600, training loss: 621.4097290039062 = 0.012684904038906097 + 100.0 * 6.213970184326172
Epoch 2600, val loss: 1.4780480861663818
Epoch 2610, training loss: 621.3906860351562 = 0.012541393749415874 + 100.0 * 6.213781833648682
Epoch 2610, val loss: 1.4810711145401
Epoch 2620, training loss: 621.2543334960938 = 0.012398671358823776 + 100.0 * 6.212419509887695
Epoch 2620, val loss: 1.4837896823883057
Epoch 2630, training loss: 621.0579223632812 = 0.012256169691681862 + 100.0 * 6.210456371307373
Epoch 2630, val loss: 1.4861054420471191
Epoch 2640, training loss: 621.0921020507812 = 0.012124390341341496 + 100.0 * 6.210799694061279
Epoch 2640, val loss: 1.4889057874679565
Epoch 2650, training loss: 621.1574096679688 = 0.011995429173111916 + 100.0 * 6.211453914642334
Epoch 2650, val loss: 1.4919592142105103
Epoch 2660, training loss: 621.010498046875 = 0.011869881302118301 + 100.0 * 6.209986686706543
Epoch 2660, val loss: 1.4945875406265259
Epoch 2670, training loss: 621.211181640625 = 0.011740465648472309 + 100.0 * 6.211994647979736
Epoch 2670, val loss: 1.497130036354065
Epoch 2680, training loss: 621.1776733398438 = 0.011613229289650917 + 100.0 * 6.211660861968994
Epoch 2680, val loss: 1.4992146492004395
Epoch 2690, training loss: 621.2963256835938 = 0.011494776234030724 + 100.0 * 6.212848663330078
Epoch 2690, val loss: 1.5025044679641724
Epoch 2700, training loss: 621.0971069335938 = 0.011362703517079353 + 100.0 * 6.210857391357422
Epoch 2700, val loss: 1.5045112371444702
Epoch 2710, training loss: 620.8869018554688 = 0.011243720538914204 + 100.0 * 6.208756446838379
Epoch 2710, val loss: 1.5072733163833618
Epoch 2720, training loss: 620.8348999023438 = 0.011125546880066395 + 100.0 * 6.208237648010254
Epoch 2720, val loss: 1.5096261501312256
Epoch 2730, training loss: 621.237060546875 = 0.011014485731720924 + 100.0 * 6.212260723114014
Epoch 2730, val loss: 1.5117683410644531
Epoch 2740, training loss: 620.9140014648438 = 0.01090622041374445 + 100.0 * 6.209030628204346
Epoch 2740, val loss: 1.5150859355926514
Epoch 2750, training loss: 620.7542114257812 = 0.010789895430207253 + 100.0 * 6.207434177398682
Epoch 2750, val loss: 1.517397165298462
Epoch 2760, training loss: 620.96435546875 = 0.010684452019631863 + 100.0 * 6.209536552429199
Epoch 2760, val loss: 1.5199278593063354
Epoch 2770, training loss: 621.0350952148438 = 0.010573807172477245 + 100.0 * 6.210245132446289
Epoch 2770, val loss: 1.5224124193191528
Epoch 2780, training loss: 621.0838012695312 = 0.010463863611221313 + 100.0 * 6.210733413696289
Epoch 2780, val loss: 1.5244711637496948
Epoch 2790, training loss: 620.7495727539062 = 0.010361874476075172 + 100.0 * 6.207391738891602
Epoch 2790, val loss: 1.5270349979400635
Epoch 2800, training loss: 621.2684936523438 = 0.010260927490890026 + 100.0 * 6.212582588195801
Epoch 2800, val loss: 1.5295823812484741
Epoch 2810, training loss: 620.64892578125 = 0.010157253593206406 + 100.0 * 6.206387996673584
Epoch 2810, val loss: 1.5315476655960083
Epoch 2820, training loss: 620.6224365234375 = 0.010060103610157967 + 100.0 * 6.2061238288879395
Epoch 2820, val loss: 1.534239649772644
Epoch 2830, training loss: 620.7083129882812 = 0.00996448565274477 + 100.0 * 6.20698356628418
Epoch 2830, val loss: 1.5366477966308594
Epoch 2840, training loss: 621.38623046875 = 0.009875346906483173 + 100.0 * 6.213763236999512
Epoch 2840, val loss: 1.5384413003921509
Epoch 2850, training loss: 620.7568359375 = 0.009773841127753258 + 100.0 * 6.207470893859863
Epoch 2850, val loss: 1.5410752296447754
Epoch 2860, training loss: 620.8348388671875 = 0.009679445065557957 + 100.0 * 6.208251476287842
Epoch 2860, val loss: 1.5432811975479126
Epoch 2870, training loss: 620.6555786132812 = 0.009589659981429577 + 100.0 * 6.206459999084473
Epoch 2870, val loss: 1.5457725524902344
Epoch 2880, training loss: 620.55029296875 = 0.009500747546553612 + 100.0 * 6.205407619476318
Epoch 2880, val loss: 1.5479049682617188
Epoch 2890, training loss: 620.9602661132812 = 0.009416613727807999 + 100.0 * 6.209508419036865
Epoch 2890, val loss: 1.550262212753296
Epoch 2900, training loss: 620.7047729492188 = 0.009322350844740868 + 100.0 * 6.206954479217529
Epoch 2900, val loss: 1.552046775817871
Epoch 2910, training loss: 620.5092163085938 = 0.009238522499799728 + 100.0 * 6.204999923706055
Epoch 2910, val loss: 1.5543314218521118
Epoch 2920, training loss: 620.5345458984375 = 0.009153900668025017 + 100.0 * 6.205254077911377
Epoch 2920, val loss: 1.5568398237228394
Epoch 2930, training loss: 620.9254150390625 = 0.009074483066797256 + 100.0 * 6.209163188934326
Epoch 2930, val loss: 1.5586504936218262
Epoch 2940, training loss: 620.51318359375 = 0.008992712013423443 + 100.0 * 6.205041885375977
Epoch 2940, val loss: 1.5610319375991821
Epoch 2950, training loss: 620.5892333984375 = 0.008912074379622936 + 100.0 * 6.205802917480469
Epoch 2950, val loss: 1.5632020235061646
Epoch 2960, training loss: 621.0228271484375 = 0.00883350521326065 + 100.0 * 6.210139751434326
Epoch 2960, val loss: 1.5648903846740723
Epoch 2970, training loss: 620.4970092773438 = 0.008758156560361385 + 100.0 * 6.204883098602295
Epoch 2970, val loss: 1.5669937133789062
Epoch 2980, training loss: 620.3863525390625 = 0.00867799948900938 + 100.0 * 6.203776836395264
Epoch 2980, val loss: 1.5692476034164429
Epoch 2990, training loss: 620.4055786132812 = 0.00860518403351307 + 100.0 * 6.203969955444336
Epoch 2990, val loss: 1.5714175701141357
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.845018450184502
The final CL Acc:0.77284, 0.02310, The final GNN Acc:0.84133, 0.00262
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10564])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6214599609375 = 1.9394111633300781 + 100.0 * 8.596820831298828
Epoch 0, val loss: 1.948100209236145
Epoch 10, training loss: 861.5303955078125 = 1.9320887327194214 + 100.0 * 8.595983505249023
Epoch 10, val loss: 1.9400856494903564
Epoch 20, training loss: 860.9911499023438 = 1.9228461980819702 + 100.0 * 8.590682983398438
Epoch 20, val loss: 1.9298847913742065
Epoch 30, training loss: 857.2362060546875 = 1.9105613231658936 + 100.0 * 8.553256034851074
Epoch 30, val loss: 1.916192889213562
Epoch 40, training loss: 828.275146484375 = 1.8945385217666626 + 100.0 * 8.263806343078613
Epoch 40, val loss: 1.8982309103012085
Epoch 50, training loss: 757.189453125 = 1.875418782234192 + 100.0 * 7.553140163421631
Epoch 50, val loss: 1.8776378631591797
Epoch 60, training loss: 731.8926391601562 = 1.8634729385375977 + 100.0 * 7.300292015075684
Epoch 60, val loss: 1.8656543493270874
Epoch 70, training loss: 716.8719482421875 = 1.8532878160476685 + 100.0 * 7.150187015533447
Epoch 70, val loss: 1.8549184799194336
Epoch 80, training loss: 703.831787109375 = 1.8432390689849854 + 100.0 * 7.019885063171387
Epoch 80, val loss: 1.8442670106887817
Epoch 90, training loss: 694.3441772460938 = 1.8345824480056763 + 100.0 * 6.925096035003662
Epoch 90, val loss: 1.8352346420288086
Epoch 100, training loss: 684.1259765625 = 1.8269193172454834 + 100.0 * 6.822990894317627
Epoch 100, val loss: 1.8274494409561157
Epoch 110, training loss: 676.0017700195312 = 1.8202245235443115 + 100.0 * 6.741815090179443
Epoch 110, val loss: 1.8206827640533447
Epoch 120, training loss: 670.3902587890625 = 1.8132524490356445 + 100.0 * 6.685770511627197
Epoch 120, val loss: 1.8136851787567139
Epoch 130, training loss: 665.5159912109375 = 1.8056399822235107 + 100.0 * 6.63710355758667
Epoch 130, val loss: 1.8060070276260376
Epoch 140, training loss: 661.6524047851562 = 1.7977818250656128 + 100.0 * 6.598546028137207
Epoch 140, val loss: 1.7981070280075073
Epoch 150, training loss: 658.9328002929688 = 1.7895047664642334 + 100.0 * 6.571433067321777
Epoch 150, val loss: 1.78976571559906
Epoch 160, training loss: 656.3702392578125 = 1.7804880142211914 + 100.0 * 6.545897006988525
Epoch 160, val loss: 1.7809185981750488
Epoch 170, training loss: 654.2881469726562 = 1.7707785367965698 + 100.0 * 6.525174140930176
Epoch 170, val loss: 1.7714751958847046
Epoch 180, training loss: 652.5228271484375 = 1.760252833366394 + 100.0 * 6.507625579833984
Epoch 180, val loss: 1.7613898515701294
Epoch 190, training loss: 650.8410034179688 = 1.748785138130188 + 100.0 * 6.490922451019287
Epoch 190, val loss: 1.7505078315734863
Epoch 200, training loss: 649.3701782226562 = 1.736270785331726 + 100.0 * 6.476339340209961
Epoch 200, val loss: 1.738763689994812
Epoch 210, training loss: 648.2322998046875 = 1.7225415706634521 + 100.0 * 6.465097427368164
Epoch 210, val loss: 1.725924015045166
Epoch 220, training loss: 646.9773559570312 = 1.7074689865112305 + 100.0 * 6.452698707580566
Epoch 220, val loss: 1.7118830680847168
Epoch 230, training loss: 645.947509765625 = 1.691057562828064 + 100.0 * 6.442564487457275
Epoch 230, val loss: 1.6966345310211182
Epoch 240, training loss: 644.9202880859375 = 1.6732571125030518 + 100.0 * 6.432469844818115
Epoch 240, val loss: 1.6801084280014038
Epoch 250, training loss: 644.0289306640625 = 1.6539242267608643 + 100.0 * 6.423749923706055
Epoch 250, val loss: 1.6623128652572632
Epoch 260, training loss: 643.138427734375 = 1.6332401037216187 + 100.0 * 6.4150519371032715
Epoch 260, val loss: 1.6433414220809937
Epoch 270, training loss: 642.5606689453125 = 1.6111711263656616 + 100.0 * 6.4094953536987305
Epoch 270, val loss: 1.6232528686523438
Epoch 280, training loss: 641.587890625 = 1.587783932685852 + 100.0 * 6.400001525878906
Epoch 280, val loss: 1.602100133895874
Epoch 290, training loss: 640.8263549804688 = 1.5633269548416138 + 100.0 * 6.392630100250244
Epoch 290, val loss: 1.580152988433838
Epoch 300, training loss: 640.4085693359375 = 1.5379551649093628 + 100.0 * 6.388706207275391
Epoch 300, val loss: 1.557669758796692
Epoch 310, training loss: 640.00341796875 = 1.5117870569229126 + 100.0 * 6.384916305541992
Epoch 310, val loss: 1.5347071886062622
Epoch 320, training loss: 639.3469848632812 = 1.4850598573684692 + 100.0 * 6.378619194030762
Epoch 320, val loss: 1.511574387550354
Epoch 330, training loss: 638.5867309570312 = 1.4579054117202759 + 100.0 * 6.371288299560547
Epoch 330, val loss: 1.488400936126709
Epoch 340, training loss: 638.0324096679688 = 1.4306539297103882 + 100.0 * 6.3660173416137695
Epoch 340, val loss: 1.4654344320297241
Epoch 350, training loss: 637.9578857421875 = 1.4033026695251465 + 100.0 * 6.365545749664307
Epoch 350, val loss: 1.442687749862671
Epoch 360, training loss: 637.2597045898438 = 1.3760583400726318 + 100.0 * 6.358836650848389
Epoch 360, val loss: 1.4202507734298706
Epoch 370, training loss: 636.731689453125 = 1.3489065170288086 + 100.0 * 6.353828430175781
Epoch 370, val loss: 1.3982815742492676
Epoch 380, training loss: 636.4208374023438 = 1.3220003843307495 + 100.0 * 6.350988388061523
Epoch 380, val loss: 1.3768823146820068
Epoch 390, training loss: 635.9288330078125 = 1.2953121662139893 + 100.0 * 6.346335411071777
Epoch 390, val loss: 1.3559073209762573
Epoch 400, training loss: 635.8043212890625 = 1.268936038017273 + 100.0 * 6.345354080200195
Epoch 400, val loss: 1.3355047702789307
Epoch 410, training loss: 635.2022094726562 = 1.2429978847503662 + 100.0 * 6.339592456817627
Epoch 410, val loss: 1.315858006477356
Epoch 420, training loss: 634.8526611328125 = 1.2173699140548706 + 100.0 * 6.336353302001953
Epoch 420, val loss: 1.2965835332870483
Epoch 430, training loss: 634.619384765625 = 1.192263126373291 + 100.0 * 6.334271430969238
Epoch 430, val loss: 1.278165578842163
Epoch 440, training loss: 634.84375 = 1.167386531829834 + 100.0 * 6.336763858795166
Epoch 440, val loss: 1.2603081464767456
Epoch 450, training loss: 633.991943359375 = 1.1430152654647827 + 100.0 * 6.328489303588867
Epoch 450, val loss: 1.2430287599563599
Epoch 460, training loss: 633.5761108398438 = 1.1191966533660889 + 100.0 * 6.324569225311279
Epoch 460, val loss: 1.226505994796753
Epoch 470, training loss: 633.6847534179688 = 1.0958137512207031 + 100.0 * 6.3258891105651855
Epoch 470, val loss: 1.210638165473938
Epoch 480, training loss: 633.1233520507812 = 1.072922706604004 + 100.0 * 6.320504188537598
Epoch 480, val loss: 1.1954662799835205
Epoch 490, training loss: 632.8171997070312 = 1.0504755973815918 + 100.0 * 6.317667007446289
Epoch 490, val loss: 1.1809577941894531
Epoch 500, training loss: 632.7041015625 = 1.0285321474075317 + 100.0 * 6.316755771636963
Epoch 500, val loss: 1.166951060295105
Epoch 510, training loss: 632.4215698242188 = 1.0069987773895264 + 100.0 * 6.314145565032959
Epoch 510, val loss: 1.1537820100784302
Epoch 520, training loss: 632.0203247070312 = 0.9859999418258667 + 100.0 * 6.310343265533447
Epoch 520, val loss: 1.1411398649215698
Epoch 530, training loss: 632.3603515625 = 0.9655156135559082 + 100.0 * 6.313948154449463
Epoch 530, val loss: 1.129089117050171
Epoch 540, training loss: 631.6605834960938 = 0.9453288912773132 + 100.0 * 6.30715274810791
Epoch 540, val loss: 1.1175282001495361
Epoch 550, training loss: 631.36767578125 = 0.9256833791732788 + 100.0 * 6.304419994354248
Epoch 550, val loss: 1.1065902709960938
Epoch 560, training loss: 631.9048461914062 = 0.9065378904342651 + 100.0 * 6.309982776641846
Epoch 560, val loss: 1.095949649810791
Epoch 570, training loss: 631.1734619140625 = 0.88778156042099 + 100.0 * 6.302856922149658
Epoch 570, val loss: 1.0862984657287598
Epoch 580, training loss: 630.8220825195312 = 0.8693463802337646 + 100.0 * 6.299527645111084
Epoch 580, val loss: 1.0768684148788452
Epoch 590, training loss: 630.9577026367188 = 0.8513303399085999 + 100.0 * 6.3010640144348145
Epoch 590, val loss: 1.067758560180664
Epoch 600, training loss: 630.54541015625 = 0.8338027596473694 + 100.0 * 6.297116279602051
Epoch 600, val loss: 1.0594966411590576
Epoch 610, training loss: 630.3709106445312 = 0.8165338039398193 + 100.0 * 6.295543670654297
Epoch 610, val loss: 1.0514761209487915
Epoch 620, training loss: 630.2245483398438 = 0.7996209263801575 + 100.0 * 6.294249534606934
Epoch 620, val loss: 1.044034481048584
Epoch 630, training loss: 630.3106689453125 = 0.7830712795257568 + 100.0 * 6.295275688171387
Epoch 630, val loss: 1.0371536016464233
Epoch 640, training loss: 629.866455078125 = 0.7668482661247253 + 100.0 * 6.290996074676514
Epoch 640, val loss: 1.0306452512741089
Epoch 650, training loss: 629.6217041015625 = 0.7509006261825562 + 100.0 * 6.288707733154297
Epoch 650, val loss: 1.0244593620300293
Epoch 660, training loss: 629.5390014648438 = 0.7352807521820068 + 100.0 * 6.288036823272705
Epoch 660, val loss: 1.0187962055206299
Epoch 670, training loss: 629.4406127929688 = 0.719936192035675 + 100.0 * 6.287207126617432
Epoch 670, val loss: 1.0136927366256714
Epoch 680, training loss: 629.5748291015625 = 0.7048441171646118 + 100.0 * 6.288700103759766
Epoch 680, val loss: 1.0086826086044312
Epoch 690, training loss: 629.2816772460938 = 0.6900234818458557 + 100.0 * 6.285916805267334
Epoch 690, val loss: 1.0041617155075073
Epoch 700, training loss: 628.9645385742188 = 0.6754602789878845 + 100.0 * 6.282890796661377
Epoch 700, val loss: 1.0001246929168701
Epoch 710, training loss: 628.6766967773438 = 0.6612354516983032 + 100.0 * 6.280154228210449
Epoch 710, val loss: 0.9963442087173462
Epoch 720, training loss: 628.601318359375 = 0.6473057270050049 + 100.0 * 6.279540538787842
Epoch 720, val loss: 0.9930296540260315
Epoch 730, training loss: 629.27587890625 = 0.633615255355835 + 100.0 * 6.2864227294921875
Epoch 730, val loss: 0.9899880886077881
Epoch 740, training loss: 628.3761596679688 = 0.6200950145721436 + 100.0 * 6.277560710906982
Epoch 740, val loss: 0.9871988892555237
Epoch 750, training loss: 628.2660522460938 = 0.6068326830863953 + 100.0 * 6.276592254638672
Epoch 750, val loss: 0.9846290946006775
Epoch 760, training loss: 628.1710205078125 = 0.5939280390739441 + 100.0 * 6.275770664215088
Epoch 760, val loss: 0.9825738072395325
Epoch 770, training loss: 628.0819091796875 = 0.5813098549842834 + 100.0 * 6.275006294250488
Epoch 770, val loss: 0.9807245135307312
Epoch 780, training loss: 627.8695068359375 = 0.5688092112541199 + 100.0 * 6.273007392883301
Epoch 780, val loss: 0.979300856590271
Epoch 790, training loss: 627.8550415039062 = 0.5565928220748901 + 100.0 * 6.272984504699707
Epoch 790, val loss: 0.977831244468689
Epoch 800, training loss: 628.1589965820312 = 0.5446438193321228 + 100.0 * 6.276143550872803
Epoch 800, val loss: 0.9769995808601379
Epoch 810, training loss: 627.6177368164062 = 0.5328976511955261 + 100.0 * 6.270848274230957
Epoch 810, val loss: 0.9764195084571838
Epoch 820, training loss: 627.4594116210938 = 0.5213986039161682 + 100.0 * 6.269379615783691
Epoch 820, val loss: 0.9761320948600769
Epoch 830, training loss: 627.4181518554688 = 0.5101889371871948 + 100.0 * 6.269079685211182
Epoch 830, val loss: 0.9760974049568176
Epoch 840, training loss: 627.4048461914062 = 0.4992092549800873 + 100.0 * 6.26905632019043
Epoch 840, val loss: 0.9763526320457458
Epoch 850, training loss: 627.83935546875 = 0.48829004168510437 + 100.0 * 6.273510932922363
Epoch 850, val loss: 0.9766604900360107
Epoch 860, training loss: 627.2553100585938 = 0.4777670204639435 + 100.0 * 6.267775535583496
Epoch 860, val loss: 0.9776911735534668
Epoch 870, training loss: 626.9508666992188 = 0.46739187836647034 + 100.0 * 6.264834403991699
Epoch 870, val loss: 0.9786887764930725
Epoch 880, training loss: 626.843017578125 = 0.45731738209724426 + 100.0 * 6.263856887817383
Epoch 880, val loss: 0.9799827337265015
Epoch 890, training loss: 627.328125 = 0.44747743010520935 + 100.0 * 6.2688069343566895
Epoch 890, val loss: 0.9814464449882507
Epoch 900, training loss: 626.8593139648438 = 0.4377574622631073 + 100.0 * 6.264215469360352
Epoch 900, val loss: 0.9830604791641235
Epoch 910, training loss: 626.8480834960938 = 0.42827609181404114 + 100.0 * 6.264198303222656
Epoch 910, val loss: 0.984964907169342
Epoch 920, training loss: 626.6680297851562 = 0.4190107583999634 + 100.0 * 6.262490272521973
Epoch 920, val loss: 0.9871987700462341
Epoch 930, training loss: 626.41748046875 = 0.4099271595478058 + 100.0 * 6.260075569152832
Epoch 930, val loss: 0.9892755746841431
Epoch 940, training loss: 626.3980712890625 = 0.40106266736984253 + 100.0 * 6.259970188140869
Epoch 940, val loss: 0.9919559955596924
Epoch 950, training loss: 626.4431762695312 = 0.3924108147621155 + 100.0 * 6.260507583618164
Epoch 950, val loss: 0.9946098327636719
Epoch 960, training loss: 626.3798828125 = 0.3838860094547272 + 100.0 * 6.259959697723389
Epoch 960, val loss: 0.9974665641784668
Epoch 970, training loss: 626.3583374023438 = 0.37553584575653076 + 100.0 * 6.259828090667725
Epoch 970, val loss: 1.0003058910369873
Epoch 980, training loss: 626.0154418945312 = 0.36732348799705505 + 100.0 * 6.256481170654297
Epoch 980, val loss: 1.0033947229385376
Epoch 990, training loss: 625.9807739257812 = 0.35934650897979736 + 100.0 * 6.256214141845703
Epoch 990, val loss: 1.0066583156585693
Epoch 1000, training loss: 625.8389282226562 = 0.35152649879455566 + 100.0 * 6.254874229431152
Epoch 1000, val loss: 1.0101547241210938
Epoch 1010, training loss: 626.1941528320312 = 0.34394770860671997 + 100.0 * 6.258502006530762
Epoch 1010, val loss: 1.0136311054229736
Epoch 1020, training loss: 626.541748046875 = 0.3363766074180603 + 100.0 * 6.262053966522217
Epoch 1020, val loss: 1.0175617933273315
Epoch 1030, training loss: 625.7236938476562 = 0.328919380903244 + 100.0 * 6.253947734832764
Epoch 1030, val loss: 1.0212291479110718
Epoch 1040, training loss: 625.7925415039062 = 0.3217242956161499 + 100.0 * 6.254708290100098
Epoch 1040, val loss: 1.0250588655471802
Epoch 1050, training loss: 625.7747802734375 = 0.3146213889122009 + 100.0 * 6.25460147857666
Epoch 1050, val loss: 1.029145359992981
Epoch 1060, training loss: 625.6990356445312 = 0.3076751232147217 + 100.0 * 6.253913402557373
Epoch 1060, val loss: 1.0332813262939453
Epoch 1070, training loss: 625.395263671875 = 0.30086714029312134 + 100.0 * 6.250944137573242
Epoch 1070, val loss: 1.0375537872314453
Epoch 1080, training loss: 625.3482055664062 = 0.29418861865997314 + 100.0 * 6.250540256500244
Epoch 1080, val loss: 1.0418946743011475
Epoch 1090, training loss: 625.5961303710938 = 0.2876589596271515 + 100.0 * 6.253084659576416
Epoch 1090, val loss: 1.0462232828140259
Epoch 1100, training loss: 625.3727416992188 = 0.2813093960285187 + 100.0 * 6.250914573669434
Epoch 1100, val loss: 1.051203966140747
Epoch 1110, training loss: 626.089599609375 = 0.2750096321105957 + 100.0 * 6.258145809173584
Epoch 1110, val loss: 1.0556726455688477
Epoch 1120, training loss: 625.2609252929688 = 0.2688480317592621 + 100.0 * 6.24992036819458
Epoch 1120, val loss: 1.0601980686187744
Epoch 1130, training loss: 624.96337890625 = 0.2628081142902374 + 100.0 * 6.247005462646484
Epoch 1130, val loss: 1.0650696754455566
Epoch 1140, training loss: 625.0553588867188 = 0.2569236159324646 + 100.0 * 6.247984886169434
Epoch 1140, val loss: 1.0698637962341309
Epoch 1150, training loss: 625.0834350585938 = 0.2511618435382843 + 100.0 * 6.248322486877441
Epoch 1150, val loss: 1.0749129056930542
Epoch 1160, training loss: 624.8959350585938 = 0.24546261131763458 + 100.0 * 6.246504783630371
Epoch 1160, val loss: 1.0796998739242554
Epoch 1170, training loss: 624.7796630859375 = 0.23992888629436493 + 100.0 * 6.245397567749023
Epoch 1170, val loss: 1.084740161895752
Epoch 1180, training loss: 625.047119140625 = 0.2345116287469864 + 100.0 * 6.248126029968262
Epoch 1180, val loss: 1.0897232294082642
Epoch 1190, training loss: 624.9794311523438 = 0.22919785976409912 + 100.0 * 6.247502326965332
Epoch 1190, val loss: 1.0950453281402588
Epoch 1200, training loss: 624.6553955078125 = 0.22399349510669708 + 100.0 * 6.244313716888428
Epoch 1200, val loss: 1.1000943183898926
Epoch 1210, training loss: 624.7264404296875 = 0.21890287101268768 + 100.0 * 6.245075225830078
Epoch 1210, val loss: 1.1052614450454712
Epoch 1220, training loss: 624.65771484375 = 0.21389563381671906 + 100.0 * 6.244438171386719
Epoch 1220, val loss: 1.1103564500808716
Epoch 1230, training loss: 624.4219360351562 = 0.20900781452655792 + 100.0 * 6.242129802703857
Epoch 1230, val loss: 1.1156787872314453
Epoch 1240, training loss: 624.4017333984375 = 0.20423975586891174 + 100.0 * 6.2419753074646
Epoch 1240, val loss: 1.1208049058914185
Epoch 1250, training loss: 624.7610473632812 = 0.19960257411003113 + 100.0 * 6.245614528656006
Epoch 1250, val loss: 1.1260886192321777
Epoch 1260, training loss: 624.6197509765625 = 0.19501946866512299 + 100.0 * 6.2442474365234375
Epoch 1260, val loss: 1.1313916444778442
Epoch 1270, training loss: 624.331298828125 = 0.19051608443260193 + 100.0 * 6.241407871246338
Epoch 1270, val loss: 1.1365867853164673
Epoch 1280, training loss: 624.1986083984375 = 0.18615077435970306 + 100.0 * 6.240124702453613
Epoch 1280, val loss: 1.1421109437942505
Epoch 1290, training loss: 624.2410888671875 = 0.18190249800682068 + 100.0 * 6.240592002868652
Epoch 1290, val loss: 1.147440791130066
Epoch 1300, training loss: 624.4682006835938 = 0.1777682602405548 + 100.0 * 6.242904186248779
Epoch 1300, val loss: 1.152772068977356
Epoch 1310, training loss: 624.1796875 = 0.17367154359817505 + 100.0 * 6.240060329437256
Epoch 1310, val loss: 1.1581887006759644
Epoch 1320, training loss: 624.1690063476562 = 0.16965992748737335 + 100.0 * 6.239993572235107
Epoch 1320, val loss: 1.1634619235992432
Epoch 1330, training loss: 624.1809692382812 = 0.16578736901283264 + 100.0 * 6.240151882171631
Epoch 1330, val loss: 1.1689952611923218
Epoch 1340, training loss: 623.9110717773438 = 0.16198424994945526 + 100.0 * 6.237490653991699
Epoch 1340, val loss: 1.1746357679367065
Epoch 1350, training loss: 623.9599609375 = 0.15828384459018707 + 100.0 * 6.2380170822143555
Epoch 1350, val loss: 1.1801085472106934
Epoch 1360, training loss: 624.1893310546875 = 0.15468716621398926 + 100.0 * 6.240346908569336
Epoch 1360, val loss: 1.1854830980300903
Epoch 1370, training loss: 624.057373046875 = 0.15109609067440033 + 100.0 * 6.239062786102295
Epoch 1370, val loss: 1.1908537149429321
Epoch 1380, training loss: 623.7882080078125 = 0.1476181298494339 + 100.0 * 6.236405372619629
Epoch 1380, val loss: 1.1965124607086182
Epoch 1390, training loss: 623.906982421875 = 0.14425025880336761 + 100.0 * 6.2376275062561035
Epoch 1390, val loss: 1.2020105123519897
Epoch 1400, training loss: 623.7185668945312 = 0.14092664420604706 + 100.0 * 6.235776424407959
Epoch 1400, val loss: 1.2075002193450928
Epoch 1410, training loss: 623.9578247070312 = 0.137703999876976 + 100.0 * 6.238201141357422
Epoch 1410, val loss: 1.2130333185195923
Epoch 1420, training loss: 623.8893432617188 = 0.13455544412136078 + 100.0 * 6.237547397613525
Epoch 1420, val loss: 1.2183738946914673
Epoch 1430, training loss: 623.68017578125 = 0.13145330548286438 + 100.0 * 6.23548698425293
Epoch 1430, val loss: 1.2239035367965698
Epoch 1440, training loss: 623.8978881835938 = 0.12846435606479645 + 100.0 * 6.237694263458252
Epoch 1440, val loss: 1.2294682264328003
Epoch 1450, training loss: 623.5144653320312 = 0.1255088448524475 + 100.0 * 6.233889579772949
Epoch 1450, val loss: 1.234902262687683
Epoch 1460, training loss: 623.4112548828125 = 0.1226322203874588 + 100.0 * 6.23288631439209
Epoch 1460, val loss: 1.2404544353485107
Epoch 1470, training loss: 623.4227294921875 = 0.11983715742826462 + 100.0 * 6.233028888702393
Epoch 1470, val loss: 1.2460218667984009
Epoch 1480, training loss: 623.958740234375 = 0.11712636053562164 + 100.0 * 6.2384161949157715
Epoch 1480, val loss: 1.251410722732544
Epoch 1490, training loss: 623.6226196289062 = 0.11443399637937546 + 100.0 * 6.235081672668457
Epoch 1490, val loss: 1.2567651271820068
Epoch 1500, training loss: 623.69287109375 = 0.11180448532104492 + 100.0 * 6.23581075668335
Epoch 1500, val loss: 1.2622170448303223
Epoch 1510, training loss: 623.4933471679688 = 0.10925218462944031 + 100.0 * 6.2338409423828125
Epoch 1510, val loss: 1.2677592039108276
Epoch 1520, training loss: 623.2452392578125 = 0.1067332923412323 + 100.0 * 6.231384754180908
Epoch 1520, val loss: 1.273220419883728
Epoch 1530, training loss: 623.1229858398438 = 0.10431476682424545 + 100.0 * 6.230186462402344
Epoch 1530, val loss: 1.27888822555542
Epoch 1540, training loss: 623.2051391601562 = 0.10197141021490097 + 100.0 * 6.23103141784668
Epoch 1540, val loss: 1.2844064235687256
Epoch 1550, training loss: 623.8082275390625 = 0.0996883362531662 + 100.0 * 6.237085342407227
Epoch 1550, val loss: 1.2897204160690308
Epoch 1560, training loss: 623.1038208007812 = 0.09736043214797974 + 100.0 * 6.230064868927002
Epoch 1560, val loss: 1.2950166463851929
Epoch 1570, training loss: 623.11572265625 = 0.09514661878347397 + 100.0 * 6.230205535888672
Epoch 1570, val loss: 1.3004695177078247
Epoch 1580, training loss: 623.261962890625 = 0.0930083766579628 + 100.0 * 6.231689453125
Epoch 1580, val loss: 1.306155800819397
Epoch 1590, training loss: 622.9961547851562 = 0.09090941399335861 + 100.0 * 6.229053020477295
Epoch 1590, val loss: 1.3113707304000854
Epoch 1600, training loss: 623.423828125 = 0.08889509737491608 + 100.0 * 6.233349323272705
Epoch 1600, val loss: 1.3167967796325684
Epoch 1610, training loss: 623.1592407226562 = 0.08685966581106186 + 100.0 * 6.230723857879639
Epoch 1610, val loss: 1.3221921920776367
Epoch 1620, training loss: 623.1124267578125 = 0.0849127322435379 + 100.0 * 6.2302751541137695
Epoch 1620, val loss: 1.3275938034057617
Epoch 1630, training loss: 622.965576171875 = 0.08300305902957916 + 100.0 * 6.228825569152832
Epoch 1630, val loss: 1.332893967628479
Epoch 1640, training loss: 622.8360595703125 = 0.08115486055612564 + 100.0 * 6.227549076080322
Epoch 1640, val loss: 1.3384085893630981
Epoch 1650, training loss: 622.9374389648438 = 0.0793517604470253 + 100.0 * 6.228580951690674
Epoch 1650, val loss: 1.3438143730163574
Epoch 1660, training loss: 623.009765625 = 0.0775802880525589 + 100.0 * 6.2293219566345215
Epoch 1660, val loss: 1.3489570617675781
Epoch 1670, training loss: 622.9924926757812 = 0.07583559304475784 + 100.0 * 6.2291669845581055
Epoch 1670, val loss: 1.3542839288711548
Epoch 1680, training loss: 622.8373413085938 = 0.07416105270385742 + 100.0 * 6.22763204574585
Epoch 1680, val loss: 1.3596057891845703
Epoch 1690, training loss: 622.7132568359375 = 0.07250311225652695 + 100.0 * 6.226407527923584
Epoch 1690, val loss: 1.3649333715438843
Epoch 1700, training loss: 622.7329711914062 = 0.0709129199385643 + 100.0 * 6.226620197296143
Epoch 1700, val loss: 1.37024986743927
Epoch 1710, training loss: 623.1347045898438 = 0.06937478482723236 + 100.0 * 6.230653285980225
Epoch 1710, val loss: 1.375504493713379
Epoch 1720, training loss: 622.7257080078125 = 0.06784190237522125 + 100.0 * 6.226578235626221
Epoch 1720, val loss: 1.380668044090271
Epoch 1730, training loss: 622.5722045898438 = 0.06635760515928268 + 100.0 * 6.225058555603027
Epoch 1730, val loss: 1.3859373331069946
Epoch 1740, training loss: 622.8910522460938 = 0.06492161005735397 + 100.0 * 6.2282609939575195
Epoch 1740, val loss: 1.391149878501892
Epoch 1750, training loss: 622.6283569335938 = 0.06351461261510849 + 100.0 * 6.225647926330566
Epoch 1750, val loss: 1.3963360786437988
Epoch 1760, training loss: 622.5596923828125 = 0.06212333217263222 + 100.0 * 6.2249755859375
Epoch 1760, val loss: 1.4012492895126343
Epoch 1770, training loss: 622.9027709960938 = 0.060811493545770645 + 100.0 * 6.228419780731201
Epoch 1770, val loss: 1.4065635204315186
Epoch 1780, training loss: 622.5109252929688 = 0.05949176102876663 + 100.0 * 6.224514007568359
Epoch 1780, val loss: 1.411758303642273
Epoch 1790, training loss: 622.4263916015625 = 0.05822845175862312 + 100.0 * 6.223681926727295
Epoch 1790, val loss: 1.4169483184814453
Epoch 1800, training loss: 622.5070190429688 = 0.05699599161744118 + 100.0 * 6.2245001792907715
Epoch 1800, val loss: 1.4220901727676392
Epoch 1810, training loss: 622.9683837890625 = 0.055819444358348846 + 100.0 * 6.229125499725342
Epoch 1810, val loss: 1.4271113872528076
Epoch 1820, training loss: 622.5071411132812 = 0.05460396781563759 + 100.0 * 6.224525451660156
Epoch 1820, val loss: 1.4319417476654053
Epoch 1830, training loss: 622.2734985351562 = 0.053456295281648636 + 100.0 * 6.222200393676758
Epoch 1830, val loss: 1.4372212886810303
Epoch 1840, training loss: 622.3206787109375 = 0.052348364144563675 + 100.0 * 6.222682952880859
Epoch 1840, val loss: 1.4424201250076294
Epoch 1850, training loss: 622.9365844726562 = 0.05127991363406181 + 100.0 * 6.228853225708008
Epoch 1850, val loss: 1.4474120140075684
Epoch 1860, training loss: 622.58544921875 = 0.05020904913544655 + 100.0 * 6.2253522872924805
Epoch 1860, val loss: 1.4521654844284058
Epoch 1870, training loss: 622.2774658203125 = 0.04916420951485634 + 100.0 * 6.222282409667969
Epoch 1870, val loss: 1.4572299718856812
Epoch 1880, training loss: 622.1798706054688 = 0.04816516116261482 + 100.0 * 6.221317291259766
Epoch 1880, val loss: 1.4622535705566406
Epoch 1890, training loss: 622.4360961914062 = 0.047199901193380356 + 100.0 * 6.223889350891113
Epoch 1890, val loss: 1.4670943021774292
Epoch 1900, training loss: 622.3027954101562 = 0.0462399423122406 + 100.0 * 6.222565174102783
Epoch 1900, val loss: 1.4720295667648315
Epoch 1910, training loss: 622.21728515625 = 0.04529563710093498 + 100.0 * 6.221719741821289
Epoch 1910, val loss: 1.4768133163452148
Epoch 1920, training loss: 622.18017578125 = 0.04439697042107582 + 100.0 * 6.221357822418213
Epoch 1920, val loss: 1.4818751811981201
Epoch 1930, training loss: 622.2977905273438 = 0.04351847991347313 + 100.0 * 6.222542762756348
Epoch 1930, val loss: 1.4866031408309937
Epoch 1940, training loss: 622.3312377929688 = 0.04266262799501419 + 100.0 * 6.222885608673096
Epoch 1940, val loss: 1.4913042783737183
Epoch 1950, training loss: 622.1725463867188 = 0.041810695081949234 + 100.0 * 6.221307277679443
Epoch 1950, val loss: 1.4959595203399658
Epoch 1960, training loss: 622.0848999023438 = 0.04099453240633011 + 100.0 * 6.2204389572143555
Epoch 1960, val loss: 1.5007387399673462
Epoch 1970, training loss: 622.3290405273438 = 0.040207862854003906 + 100.0 * 6.222888469696045
Epoch 1970, val loss: 1.5052706003189087
Epoch 1980, training loss: 622.0281372070312 = 0.039426758885383606 + 100.0 * 6.2198872566223145
Epoch 1980, val loss: 1.5099990367889404
Epoch 1990, training loss: 622.26953125 = 0.03868294507265091 + 100.0 * 6.22230863571167
Epoch 1990, val loss: 1.5145447254180908
Epoch 2000, training loss: 622.2210693359375 = 0.03793669864535332 + 100.0 * 6.22183084487915
Epoch 2000, val loss: 1.5191476345062256
Epoch 2010, training loss: 621.9583740234375 = 0.03720854967832565 + 100.0 * 6.219211578369141
Epoch 2010, val loss: 1.523730993270874
Epoch 2020, training loss: 621.8577880859375 = 0.036507852375507355 + 100.0 * 6.218212604522705
Epoch 2020, val loss: 1.5283288955688477
Epoch 2030, training loss: 621.9835815429688 = 0.035838063806295395 + 100.0 * 6.219477653503418
Epoch 2030, val loss: 1.5327868461608887
Epoch 2040, training loss: 622.2194213867188 = 0.03517743572592735 + 100.0 * 6.2218427658081055
Epoch 2040, val loss: 1.5371885299682617
Epoch 2050, training loss: 622.096435546875 = 0.03451920673251152 + 100.0 * 6.220618724822998
Epoch 2050, val loss: 1.5414634943008423
Epoch 2060, training loss: 621.9586791992188 = 0.033880848437547684 + 100.0 * 6.219248294830322
Epoch 2060, val loss: 1.5459928512573242
Epoch 2070, training loss: 621.8680419921875 = 0.033266596496105194 + 100.0 * 6.218347549438477
Epoch 2070, val loss: 1.5504969358444214
Epoch 2080, training loss: 622.0466918945312 = 0.03268028050661087 + 100.0 * 6.22014045715332
Epoch 2080, val loss: 1.5548404455184937
Epoch 2090, training loss: 621.918212890625 = 0.03209419921040535 + 100.0 * 6.218861103057861
Epoch 2090, val loss: 1.5590784549713135
Epoch 2100, training loss: 621.8967895507812 = 0.031524334102869034 + 100.0 * 6.218652725219727
Epoch 2100, val loss: 1.5634022951126099
Epoch 2110, training loss: 621.9984130859375 = 0.030959956347942352 + 100.0 * 6.219674587249756
Epoch 2110, val loss: 1.5675386190414429
Epoch 2120, training loss: 622.0523681640625 = 0.030418047681450844 + 100.0 * 6.220219612121582
Epoch 2120, val loss: 1.571630835533142
Epoch 2130, training loss: 621.8657836914062 = 0.029880082234740257 + 100.0 * 6.218358993530273
Epoch 2130, val loss: 1.5759191513061523
Epoch 2140, training loss: 621.9816284179688 = 0.02936234511435032 + 100.0 * 6.219522953033447
Epoch 2140, val loss: 1.5799669027328491
Epoch 2150, training loss: 621.6256103515625 = 0.02885262481868267 + 100.0 * 6.215967178344727
Epoch 2150, val loss: 1.584136724472046
Epoch 2160, training loss: 621.6638793945312 = 0.028367139399051666 + 100.0 * 6.216354846954346
Epoch 2160, val loss: 1.5881688594818115
Epoch 2170, training loss: 621.6932983398438 = 0.027886301279067993 + 100.0 * 6.216653823852539
Epoch 2170, val loss: 1.592109203338623
Epoch 2180, training loss: 622.1498413085938 = 0.027418676763772964 + 100.0 * 6.221224308013916
Epoch 2180, val loss: 1.5961050987243652
Epoch 2190, training loss: 621.819091796875 = 0.026959920302033424 + 100.0 * 6.217921257019043
Epoch 2190, val loss: 1.6001545190811157
Epoch 2200, training loss: 621.58984375 = 0.02650243043899536 + 100.0 * 6.215633392333984
Epoch 2200, val loss: 1.6040419340133667
Epoch 2210, training loss: 621.67041015625 = 0.026076586917042732 + 100.0 * 6.2164435386657715
Epoch 2210, val loss: 1.6081675291061401
Epoch 2220, training loss: 621.888671875 = 0.02565827965736389 + 100.0 * 6.218630313873291
Epoch 2220, val loss: 1.6116979122161865
Epoch 2230, training loss: 621.743896484375 = 0.025224631652235985 + 100.0 * 6.21718692779541
Epoch 2230, val loss: 1.6154488325119019
Epoch 2240, training loss: 621.8661499023438 = 0.024820897728204727 + 100.0 * 6.21841287612915
Epoch 2240, val loss: 1.6193848848342896
Epoch 2250, training loss: 621.564697265625 = 0.024418208748102188 + 100.0 * 6.215403079986572
Epoch 2250, val loss: 1.6229937076568604
Epoch 2260, training loss: 621.868408203125 = 0.02403503842651844 + 100.0 * 6.218443870544434
Epoch 2260, val loss: 1.626883625984192
Epoch 2270, training loss: 621.5896606445312 = 0.023647667840123177 + 100.0 * 6.215660095214844
Epoch 2270, val loss: 1.6303991079330444
Epoch 2280, training loss: 621.5538330078125 = 0.023280732333660126 + 100.0 * 6.215305805206299
Epoch 2280, val loss: 1.634300947189331
Epoch 2290, training loss: 621.4835815429688 = 0.022917119786143303 + 100.0 * 6.214607238769531
Epoch 2290, val loss: 1.6378551721572876
Epoch 2300, training loss: 621.51806640625 = 0.022570045664906502 + 100.0 * 6.214954853057861
Epoch 2300, val loss: 1.6414897441864014
Epoch 2310, training loss: 621.5193481445312 = 0.022224953398108482 + 100.0 * 6.21497106552124
Epoch 2310, val loss: 1.645046591758728
Epoch 2320, training loss: 621.6475830078125 = 0.021882666274905205 + 100.0 * 6.216256618499756
Epoch 2320, val loss: 1.6485848426818848
Epoch 2330, training loss: 621.73779296875 = 0.021552687510848045 + 100.0 * 6.217162609100342
Epoch 2330, val loss: 1.6519734859466553
Epoch 2340, training loss: 621.3511352539062 = 0.021227208897471428 + 100.0 * 6.213298797607422
Epoch 2340, val loss: 1.6556389331817627
Epoch 2350, training loss: 621.2918090820312 = 0.02090664580464363 + 100.0 * 6.212708950042725
Epoch 2350, val loss: 1.658947467803955
Epoch 2360, training loss: 621.955322265625 = 0.020606987178325653 + 100.0 * 6.21934700012207
Epoch 2360, val loss: 1.6624590158462524
Epoch 2370, training loss: 621.5394897460938 = 0.020304201170802116 + 100.0 * 6.215191841125488
Epoch 2370, val loss: 1.6655855178833008
Epoch 2380, training loss: 621.3436889648438 = 0.019999289885163307 + 100.0 * 6.2132368087768555
Epoch 2380, val loss: 1.669092059135437
Epoch 2390, training loss: 621.5502319335938 = 0.01971428655087948 + 100.0 * 6.215305328369141
Epoch 2390, val loss: 1.6724272966384888
Epoch 2400, training loss: 621.2569580078125 = 0.01942860521376133 + 100.0 * 6.212375640869141
Epoch 2400, val loss: 1.6757354736328125
Epoch 2410, training loss: 621.3033447265625 = 0.019155463203787804 + 100.0 * 6.212841987609863
Epoch 2410, val loss: 1.6788750886917114
Epoch 2420, training loss: 621.3798828125 = 0.01888355240225792 + 100.0 * 6.2136101722717285
Epoch 2420, val loss: 1.6820369958877563
Epoch 2430, training loss: 621.6218872070312 = 0.018621493130922318 + 100.0 * 6.216032981872559
Epoch 2430, val loss: 1.6853317022323608
Epoch 2440, training loss: 621.2456665039062 = 0.018350904807448387 + 100.0 * 6.212273120880127
Epoch 2440, val loss: 1.6883636713027954
Epoch 2450, training loss: 621.1647338867188 = 0.018093818798661232 + 100.0 * 6.2114667892456055
Epoch 2450, val loss: 1.6915948390960693
Epoch 2460, training loss: 621.4260864257812 = 0.01785384491086006 + 100.0 * 6.214081764221191
Epoch 2460, val loss: 1.694672703742981
Epoch 2470, training loss: 621.2149047851562 = 0.017606915906071663 + 100.0 * 6.211973190307617
Epoch 2470, val loss: 1.6976057291030884
Epoch 2480, training loss: 621.2529296875 = 0.01736685261130333 + 100.0 * 6.212355136871338
Epoch 2480, val loss: 1.700715184211731
Epoch 2490, training loss: 621.1727905273438 = 0.017129912972450256 + 100.0 * 6.211556434631348
Epoch 2490, val loss: 1.7038358449935913
Epoch 2500, training loss: 621.396728515625 = 0.01690462790429592 + 100.0 * 6.2137980461120605
Epoch 2500, val loss: 1.7067651748657227
Epoch 2510, training loss: 621.2980346679688 = 0.016679180786013603 + 100.0 * 6.212813854217529
Epoch 2510, val loss: 1.709404706954956
Epoch 2520, training loss: 621.0423583984375 = 0.01645103096961975 + 100.0 * 6.210258483886719
Epoch 2520, val loss: 1.7124221324920654
Epoch 2530, training loss: 621.4695434570312 = 0.01623906008899212 + 100.0 * 6.21453332901001
Epoch 2530, val loss: 1.7152208089828491
Epoch 2540, training loss: 621.1349487304688 = 0.016021614894270897 + 100.0 * 6.2111897468566895
Epoch 2540, val loss: 1.717869758605957
Epoch 2550, training loss: 621.0123291015625 = 0.01580970548093319 + 100.0 * 6.209965229034424
Epoch 2550, val loss: 1.7207744121551514
Epoch 2560, training loss: 620.9916381835938 = 0.01560880709439516 + 100.0 * 6.2097601890563965
Epoch 2560, val loss: 1.7235256433486938
Epoch 2570, training loss: 621.2847290039062 = 0.015412378124892712 + 100.0 * 6.212692737579346
Epoch 2570, val loss: 1.7261319160461426
Epoch 2580, training loss: 621.0718383789062 = 0.015216807834804058 + 100.0 * 6.210566520690918
Epoch 2580, val loss: 1.728989601135254
Epoch 2590, training loss: 621.0798950195312 = 0.015022842213511467 + 100.0 * 6.210649013519287
Epoch 2590, val loss: 1.7316820621490479
Epoch 2600, training loss: 621.2088012695312 = 0.014837837778031826 + 100.0 * 6.211939811706543
Epoch 2600, val loss: 1.7343014478683472
Epoch 2610, training loss: 620.979248046875 = 0.014648905023932457 + 100.0 * 6.209645748138428
Epoch 2610, val loss: 1.7370457649230957
Epoch 2620, training loss: 621.006591796875 = 0.0144672691822052 + 100.0 * 6.209921360015869
Epoch 2620, val loss: 1.7396361827850342
Epoch 2630, training loss: 621.0632934570312 = 0.014293162152171135 + 100.0 * 6.2104902267456055
Epoch 2630, val loss: 1.742200255393982
Epoch 2640, training loss: 621.161865234375 = 0.014117490500211716 + 100.0 * 6.211477756500244
Epoch 2640, val loss: 1.7444604635238647
Epoch 2650, training loss: 621.0366821289062 = 0.013943539932370186 + 100.0 * 6.2102274894714355
Epoch 2650, val loss: 1.7472233772277832
Epoch 2660, training loss: 621.0780029296875 = 0.013778859749436378 + 100.0 * 6.210641860961914
Epoch 2660, val loss: 1.7498607635498047
Epoch 2670, training loss: 620.8997192382812 = 0.013609121553599834 + 100.0 * 6.208860874176025
Epoch 2670, val loss: 1.7522294521331787
Epoch 2680, training loss: 621.4185791015625 = 0.013456991873681545 + 100.0 * 6.214051723480225
Epoch 2680, val loss: 1.7545397281646729
Epoch 2690, training loss: 621.0498046875 = 0.013284671120345592 + 100.0 * 6.210365295410156
Epoch 2690, val loss: 1.7565454244613647
Epoch 2700, training loss: 620.8740234375 = 0.01312754862010479 + 100.0 * 6.208609104156494
Epoch 2700, val loss: 1.7593433856964111
Epoch 2710, training loss: 620.8157958984375 = 0.012975865043699741 + 100.0 * 6.2080278396606445
Epoch 2710, val loss: 1.7615830898284912
Epoch 2720, training loss: 620.9972534179688 = 0.012829065322875977 + 100.0 * 6.209844589233398
Epoch 2720, val loss: 1.7638496160507202
Epoch 2730, training loss: 620.7726440429688 = 0.012679178267717361 + 100.0 * 6.207599639892578
Epoch 2730, val loss: 1.7661540508270264
Epoch 2740, training loss: 620.9817504882812 = 0.012540222145617008 + 100.0 * 6.209692478179932
Epoch 2740, val loss: 1.7689613103866577
Epoch 2750, training loss: 621.2544555664062 = 0.01239931583404541 + 100.0 * 6.21242094039917
Epoch 2750, val loss: 1.7709729671478271
Epoch 2760, training loss: 620.841064453125 = 0.012253519147634506 + 100.0 * 6.208287715911865
Epoch 2760, val loss: 1.773012399673462
Epoch 2770, training loss: 620.734375 = 0.012114839628338814 + 100.0 * 6.207222938537598
Epoch 2770, val loss: 1.7755268812179565
Epoch 2780, training loss: 620.9251098632812 = 0.011983064003288746 + 100.0 * 6.209131240844727
Epoch 2780, val loss: 1.7776896953582764
Epoch 2790, training loss: 620.7521362304688 = 0.011849695816636086 + 100.0 * 6.207403182983398
Epoch 2790, val loss: 1.7797610759735107
Epoch 2800, training loss: 621.1046142578125 = 0.01172514446079731 + 100.0 * 6.210928916931152
Epoch 2800, val loss: 1.7818721532821655
Epoch 2810, training loss: 620.673095703125 = 0.011591172777116299 + 100.0 * 6.206614971160889
Epoch 2810, val loss: 1.7840508222579956
Epoch 2820, training loss: 620.7225341796875 = 0.011470375582575798 + 100.0 * 6.207110404968262
Epoch 2820, val loss: 1.7862573862075806
Epoch 2830, training loss: 620.9165649414062 = 0.011350909247994423 + 100.0 * 6.209052085876465
Epoch 2830, val loss: 1.788413166999817
Epoch 2840, training loss: 620.6463012695312 = 0.011225691065192223 + 100.0 * 6.206350803375244
Epoch 2840, val loss: 1.790399432182312
Epoch 2850, training loss: 620.7305908203125 = 0.011105871759355068 + 100.0 * 6.207194805145264
Epoch 2850, val loss: 1.7922015190124512
Epoch 2860, training loss: 621.0794677734375 = 0.01099541038274765 + 100.0 * 6.210684776306152
Epoch 2860, val loss: 1.7942910194396973
Epoch 2870, training loss: 620.8368530273438 = 0.01087926421314478 + 100.0 * 6.2082600593566895
Epoch 2870, val loss: 1.796553134918213
Epoch 2880, training loss: 620.6137084960938 = 0.010762081481516361 + 100.0 * 6.206029891967773
Epoch 2880, val loss: 1.7983816862106323
Epoch 2890, training loss: 620.575927734375 = 0.010654937475919724 + 100.0 * 6.205652713775635
Epoch 2890, val loss: 1.8007322549819946
Epoch 2900, training loss: 621.1300659179688 = 0.010552985593676567 + 100.0 * 6.21119499206543
Epoch 2900, val loss: 1.8025846481323242
Epoch 2910, training loss: 620.888916015625 = 0.010440457612276077 + 100.0 * 6.208784580230713
Epoch 2910, val loss: 1.804076075553894
Epoch 2920, training loss: 620.6769409179688 = 0.010332615114748478 + 100.0 * 6.206665992736816
Epoch 2920, val loss: 1.806334137916565
Epoch 2930, training loss: 620.6490478515625 = 0.010228341445326805 + 100.0 * 6.206387996673584
Epoch 2930, val loss: 1.8080239295959473
Epoch 2940, training loss: 620.6522216796875 = 0.010129297152161598 + 100.0 * 6.2064208984375
Epoch 2940, val loss: 1.810368299484253
Epoch 2950, training loss: 620.5022583007812 = 0.010027242824435234 + 100.0 * 6.204922199249268
Epoch 2950, val loss: 1.8119785785675049
Epoch 2960, training loss: 620.7752685546875 = 0.009931789711117744 + 100.0 * 6.207653045654297
Epoch 2960, val loss: 1.8138192892074585
Epoch 2970, training loss: 620.830322265625 = 0.009832091629505157 + 100.0 * 6.208204746246338
Epoch 2970, val loss: 1.8154765367507935
Epoch 2980, training loss: 620.4357299804688 = 0.009735378436744213 + 100.0 * 6.204259872436523
Epoch 2980, val loss: 1.8174278736114502
Epoch 2990, training loss: 620.385498046875 = 0.009642763994634151 + 100.0 * 6.203758239746094
Epoch 2990, val loss: 1.8192912340164185
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6666666666666667
0.7970479704797049
=== training gcn model ===
Epoch 0, training loss: 861.626708984375 = 1.9418387413024902 + 100.0 * 8.596848487854004
Epoch 0, val loss: 1.9351962804794312
Epoch 10, training loss: 861.5514526367188 = 1.933476448059082 + 100.0 * 8.596179962158203
Epoch 10, val loss: 1.927520990371704
Epoch 20, training loss: 861.1249389648438 = 1.9231444597244263 + 100.0 * 8.592018127441406
Epoch 20, val loss: 1.9176068305969238
Epoch 30, training loss: 858.3812255859375 = 1.9098410606384277 + 100.0 * 8.564713478088379
Epoch 30, val loss: 1.9044824838638306
Epoch 40, training loss: 840.2459716796875 = 1.8935585021972656 + 100.0 * 8.383523941040039
Epoch 40, val loss: 1.8884228467941284
Epoch 50, training loss: 758.1038818359375 = 1.8753787279129028 + 100.0 * 7.56228494644165
Epoch 50, val loss: 1.8703786134719849
Epoch 60, training loss: 734.9150390625 = 1.8603813648223877 + 100.0 * 7.330546855926514
Epoch 60, val loss: 1.8561863899230957
Epoch 70, training loss: 716.5128784179688 = 1.8485385179519653 + 100.0 * 7.14664363861084
Epoch 70, val loss: 1.8443323373794556
Epoch 80, training loss: 700.1845092773438 = 1.837131381034851 + 100.0 * 6.983473300933838
Epoch 80, val loss: 1.8335360288619995
Epoch 90, training loss: 686.5567016601562 = 1.8290784358978271 + 100.0 * 6.847276210784912
Epoch 90, val loss: 1.8259867429733276
Epoch 100, training loss: 678.1959228515625 = 1.8216500282287598 + 100.0 * 6.763742446899414
Epoch 100, val loss: 1.8188215494155884
Epoch 110, training loss: 672.8904418945312 = 1.8139653205871582 + 100.0 * 6.7107648849487305
Epoch 110, val loss: 1.8114433288574219
Epoch 120, training loss: 669.03662109375 = 1.8063563108444214 + 100.0 * 6.672302722930908
Epoch 120, val loss: 1.8042938709259033
Epoch 130, training loss: 665.9717407226562 = 1.7991493940353394 + 100.0 * 6.641726016998291
Epoch 130, val loss: 1.7975999116897583
Epoch 140, training loss: 663.2091064453125 = 1.7923076152801514 + 100.0 * 6.614168167114258
Epoch 140, val loss: 1.7911633253097534
Epoch 150, training loss: 661.1337280273438 = 1.7854434251785278 + 100.0 * 6.593482971191406
Epoch 150, val loss: 1.7848637104034424
Epoch 160, training loss: 658.7234497070312 = 1.7782543897628784 + 100.0 * 6.569451808929443
Epoch 160, val loss: 1.7783453464508057
Epoch 170, training loss: 656.9085083007812 = 1.7705610990524292 + 100.0 * 6.551379680633545
Epoch 170, val loss: 1.7716408967971802
Epoch 180, training loss: 655.7734375 = 1.762254238128662 + 100.0 * 6.540111541748047
Epoch 180, val loss: 1.7645857334136963
Epoch 190, training loss: 653.89404296875 = 1.7531850337982178 + 100.0 * 6.521408557891846
Epoch 190, val loss: 1.7570098638534546
Epoch 200, training loss: 652.447998046875 = 1.7433594465255737 + 100.0 * 6.507046222686768
Epoch 200, val loss: 1.7489591836929321
Epoch 210, training loss: 651.199462890625 = 1.732709288597107 + 100.0 * 6.4946675300598145
Epoch 210, val loss: 1.7402700185775757
Epoch 220, training loss: 650.0562744140625 = 1.7210482358932495 + 100.0 * 6.483352184295654
Epoch 220, val loss: 1.7308406829833984
Epoch 230, training loss: 649.1454467773438 = 1.7082467079162598 + 100.0 * 6.474371910095215
Epoch 230, val loss: 1.7204012870788574
Epoch 240, training loss: 648.3485107421875 = 1.6944303512573242 + 100.0 * 6.466540813446045
Epoch 240, val loss: 1.7091665267944336
Epoch 250, training loss: 647.2685546875 = 1.6793830394744873 + 100.0 * 6.4558916091918945
Epoch 250, val loss: 1.6970680952072144
Epoch 260, training loss: 646.4298095703125 = 1.66325843334198 + 100.0 * 6.447665691375732
Epoch 260, val loss: 1.6841223239898682
Epoch 270, training loss: 645.606201171875 = 1.646030068397522 + 100.0 * 6.439601421356201
Epoch 270, val loss: 1.6702536344528198
Epoch 280, training loss: 644.8529663085938 = 1.6277674436569214 + 100.0 * 6.432251930236816
Epoch 280, val loss: 1.6557707786560059
Epoch 290, training loss: 644.0615234375 = 1.608614206314087 + 100.0 * 6.424529075622559
Epoch 290, val loss: 1.6404991149902344
Epoch 300, training loss: 643.7242431640625 = 1.5883373022079468 + 100.0 * 6.421359062194824
Epoch 300, val loss: 1.6244858503341675
Epoch 310, training loss: 642.753173828125 = 1.5673741102218628 + 100.0 * 6.411858081817627
Epoch 310, val loss: 1.6079975366592407
Epoch 320, training loss: 641.9515380859375 = 1.545778512954712 + 100.0 * 6.404057502746582
Epoch 320, val loss: 1.5912964344024658
Epoch 330, training loss: 641.2921752929688 = 1.5237234830856323 + 100.0 * 6.397684574127197
Epoch 330, val loss: 1.574321985244751
Epoch 340, training loss: 640.687255859375 = 1.501172661781311 + 100.0 * 6.3918609619140625
Epoch 340, val loss: 1.557023525238037
Epoch 350, training loss: 640.0839233398438 = 1.4782817363739014 + 100.0 * 6.386056423187256
Epoch 350, val loss: 1.5397052764892578
Epoch 360, training loss: 639.611328125 = 1.4552839994430542 + 100.0 * 6.381560802459717
Epoch 360, val loss: 1.5225423574447632
Epoch 370, training loss: 639.282470703125 = 1.4320927858352661 + 100.0 * 6.378504276275635
Epoch 370, val loss: 1.50526762008667
Epoch 380, training loss: 638.6299438476562 = 1.4088939428329468 + 100.0 * 6.37221097946167
Epoch 380, val loss: 1.4883092641830444
Epoch 390, training loss: 638.0917358398438 = 1.3859150409698486 + 100.0 * 6.367058277130127
Epoch 390, val loss: 1.4717822074890137
Epoch 400, training loss: 637.9894409179688 = 1.3631125688552856 + 100.0 * 6.366263389587402
Epoch 400, val loss: 1.455625295639038
Epoch 410, training loss: 637.3892822265625 = 1.3402538299560547 + 100.0 * 6.360489845275879
Epoch 410, val loss: 1.439584493637085
Epoch 420, training loss: 636.9363403320312 = 1.317784309387207 + 100.0 * 6.356185436248779
Epoch 420, val loss: 1.4239654541015625
Epoch 430, training loss: 636.5468139648438 = 1.2957284450531006 + 100.0 * 6.352510929107666
Epoch 430, val loss: 1.409103274345398
Epoch 440, training loss: 636.3359375 = 1.2740470170974731 + 100.0 * 6.350618839263916
Epoch 440, val loss: 1.3947975635528564
Epoch 450, training loss: 635.9774169921875 = 1.252616286277771 + 100.0 * 6.347248077392578
Epoch 450, val loss: 1.3805373907089233
Epoch 460, training loss: 635.6159057617188 = 1.2315621376037598 + 100.0 * 6.343843460083008
Epoch 460, val loss: 1.367163062095642
Epoch 470, training loss: 635.1439819335938 = 1.2110291719436646 + 100.0 * 6.339329242706299
Epoch 470, val loss: 1.3541760444641113
Epoch 480, training loss: 635.1232299804688 = 1.1910288333892822 + 100.0 * 6.339322090148926
Epoch 480, val loss: 1.3419075012207031
Epoch 490, training loss: 634.9367065429688 = 1.171134352684021 + 100.0 * 6.337655544281006
Epoch 490, val loss: 1.3299630880355835
Epoch 500, training loss: 634.4567260742188 = 1.151818871498108 + 100.0 * 6.333049297332764
Epoch 500, val loss: 1.3186341524124146
Epoch 510, training loss: 634.05810546875 = 1.133129358291626 + 100.0 * 6.329249858856201
Epoch 510, val loss: 1.3081079721450806
Epoch 520, training loss: 633.7351684570312 = 1.11500084400177 + 100.0 * 6.326201915740967
Epoch 520, val loss: 1.298266053199768
Epoch 530, training loss: 634.6715698242188 = 1.0973514318466187 + 100.0 * 6.335741996765137
Epoch 530, val loss: 1.2888370752334595
Epoch 540, training loss: 633.582763671875 = 1.0796890258789062 + 100.0 * 6.32503080368042
Epoch 540, val loss: 1.2797108888626099
Epoch 550, training loss: 633.14892578125 = 1.062715768814087 + 100.0 * 6.32086181640625
Epoch 550, val loss: 1.2715400457382202
Epoch 560, training loss: 632.7945556640625 = 1.0463597774505615 + 100.0 * 6.317481994628906
Epoch 560, val loss: 1.2640020847320557
Epoch 570, training loss: 632.56103515625 = 1.0304464101791382 + 100.0 * 6.315305709838867
Epoch 570, val loss: 1.2570477724075317
Epoch 580, training loss: 632.447021484375 = 1.0148773193359375 + 100.0 * 6.314321517944336
Epoch 580, val loss: 1.2504581212997437
Epoch 590, training loss: 632.27734375 = 0.9994935393333435 + 100.0 * 6.312778472900391
Epoch 590, val loss: 1.2440245151519775
Epoch 600, training loss: 632.11181640625 = 0.9843903183937073 + 100.0 * 6.311274528503418
Epoch 600, val loss: 1.2380180358886719
Epoch 610, training loss: 631.8419799804688 = 0.9697702527046204 + 100.0 * 6.308722019195557
Epoch 610, val loss: 1.2327079772949219
Epoch 620, training loss: 631.68798828125 = 0.9555158615112305 + 100.0 * 6.3073248863220215
Epoch 620, val loss: 1.2278350591659546
Epoch 630, training loss: 631.6911010742188 = 0.9414446353912354 + 100.0 * 6.307496070861816
Epoch 630, val loss: 1.2231369018554688
Epoch 640, training loss: 631.34423828125 = 0.9275170564651489 + 100.0 * 6.3041672706604
Epoch 640, val loss: 1.2186551094055176
Epoch 650, training loss: 631.1320190429688 = 0.9139069318771362 + 100.0 * 6.302181243896484
Epoch 650, val loss: 1.2146817445755005
Epoch 660, training loss: 630.9544677734375 = 0.9004998207092285 + 100.0 * 6.300539493560791
Epoch 660, val loss: 1.2110036611557007
Epoch 670, training loss: 631.0071411132812 = 0.8872678875923157 + 100.0 * 6.301198959350586
Epoch 670, val loss: 1.2074275016784668
Epoch 680, training loss: 630.65673828125 = 0.8740691542625427 + 100.0 * 6.297826766967773
Epoch 680, val loss: 1.2039165496826172
Epoch 690, training loss: 630.8218994140625 = 0.8609722852706909 + 100.0 * 6.299609661102295
Epoch 690, val loss: 1.2006244659423828
Epoch 700, training loss: 630.3656005859375 = 0.8479956984519958 + 100.0 * 6.295175552368164
Epoch 700, val loss: 1.1976357698440552
Epoch 710, training loss: 630.135498046875 = 0.8352043032646179 + 100.0 * 6.293003082275391
Epoch 710, val loss: 1.1948727369308472
Epoch 720, training loss: 629.9609985351562 = 0.8225308656692505 + 100.0 * 6.291384696960449
Epoch 720, val loss: 1.1922725439071655
Epoch 730, training loss: 630.635009765625 = 0.809890866279602 + 100.0 * 6.298251628875732
Epoch 730, val loss: 1.1897534132003784
Epoch 740, training loss: 630.085205078125 = 0.797152578830719 + 100.0 * 6.292880535125732
Epoch 740, val loss: 1.186921238899231
Epoch 750, training loss: 629.5621948242188 = 0.7844900488853455 + 100.0 * 6.287776947021484
Epoch 750, val loss: 1.18453049659729
Epoch 760, training loss: 629.45263671875 = 0.7720239758491516 + 100.0 * 6.286806106567383
Epoch 760, val loss: 1.1823275089263916
Epoch 770, training loss: 629.9061279296875 = 0.7596417665481567 + 100.0 * 6.291464805603027
Epoch 770, val loss: 1.1802278757095337
Epoch 780, training loss: 629.4202880859375 = 0.747031033039093 + 100.0 * 6.2867326736450195
Epoch 780, val loss: 1.1778655052185059
Epoch 790, training loss: 629.0712280273438 = 0.734592616558075 + 100.0 * 6.2833662033081055
Epoch 790, val loss: 1.1758884191513062
Epoch 800, training loss: 629.9166870117188 = 0.7221630215644836 + 100.0 * 6.291944980621338
Epoch 800, val loss: 1.173887848854065
Epoch 810, training loss: 628.9766235351562 = 0.7094753384590149 + 100.0 * 6.2826714515686035
Epoch 810, val loss: 1.1715883016586304
Epoch 820, training loss: 628.6905517578125 = 0.6970890164375305 + 100.0 * 6.279934883117676
Epoch 820, val loss: 1.1698325872421265
Epoch 830, training loss: 628.5661010742188 = 0.6847658157348633 + 100.0 * 6.278813362121582
Epoch 830, val loss: 1.1681923866271973
Epoch 840, training loss: 628.4324340820312 = 0.672451376914978 + 100.0 * 6.277600288391113
Epoch 840, val loss: 1.1666098833084106
Epoch 850, training loss: 629.3291625976562 = 0.6601315140724182 + 100.0 * 6.2866902351379395
Epoch 850, val loss: 1.1649916172027588
Epoch 860, training loss: 628.4435424804688 = 0.647498607635498 + 100.0 * 6.277960300445557
Epoch 860, val loss: 1.1628936529159546
Epoch 870, training loss: 628.1683349609375 = 0.6350414156913757 + 100.0 * 6.275332927703857
Epoch 870, val loss: 1.1612958908081055
Epoch 880, training loss: 628.1154174804688 = 0.6227430105209351 + 100.0 * 6.274926662445068
Epoch 880, val loss: 1.1597793102264404
Epoch 890, training loss: 628.1460571289062 = 0.6103521585464478 + 100.0 * 6.275357246398926
Epoch 890, val loss: 1.158079743385315
Epoch 900, training loss: 627.913330078125 = 0.5979503989219666 + 100.0 * 6.273153781890869
Epoch 900, val loss: 1.156429409980774
Epoch 910, training loss: 627.8424682617188 = 0.5857099890708923 + 100.0 * 6.2725677490234375
Epoch 910, val loss: 1.1549919843673706
Epoch 920, training loss: 628.2498168945312 = 0.5735226273536682 + 100.0 * 6.27676248550415
Epoch 920, val loss: 1.153468132019043
Epoch 930, training loss: 627.718017578125 = 0.5612837076187134 + 100.0 * 6.271567344665527
Epoch 930, val loss: 1.151747226715088
Epoch 940, training loss: 627.5015869140625 = 0.5492644906044006 + 100.0 * 6.2695231437683105
Epoch 940, val loss: 1.1506909132003784
Epoch 950, training loss: 627.3394165039062 = 0.5373714566230774 + 100.0 * 6.2680206298828125
Epoch 950, val loss: 1.1494498252868652
Epoch 960, training loss: 627.489013671875 = 0.5256195068359375 + 100.0 * 6.269633769989014
Epoch 960, val loss: 1.1483373641967773
Epoch 970, training loss: 627.58837890625 = 0.5137213468551636 + 100.0 * 6.270746231079102
Epoch 970, val loss: 1.1471973657608032
Epoch 980, training loss: 627.1126708984375 = 0.5019044876098633 + 100.0 * 6.26610803604126
Epoch 980, val loss: 1.145867109298706
Epoch 990, training loss: 627.0445556640625 = 0.4904954433441162 + 100.0 * 6.265541076660156
Epoch 990, val loss: 1.1452158689498901
Epoch 1000, training loss: 626.8795166015625 = 0.4792712926864624 + 100.0 * 6.264002799987793
Epoch 1000, val loss: 1.1448732614517212
Epoch 1010, training loss: 627.5809326171875 = 0.46823784708976746 + 100.0 * 6.271126747131348
Epoch 1010, val loss: 1.1443992853164673
Epoch 1020, training loss: 627.0900268554688 = 0.45704713463783264 + 100.0 * 6.266330242156982
Epoch 1020, val loss: 1.1436063051223755
Epoch 1030, training loss: 626.8153686523438 = 0.44619137048721313 + 100.0 * 6.2636919021606445
Epoch 1030, val loss: 1.1431503295898438
Epoch 1040, training loss: 626.7650756835938 = 0.4355385899543762 + 100.0 * 6.2632951736450195
Epoch 1040, val loss: 1.1428582668304443
Epoch 1050, training loss: 626.58154296875 = 0.42512962222099304 + 100.0 * 6.261564254760742
Epoch 1050, val loss: 1.1430671215057373
Epoch 1060, training loss: 626.4308471679688 = 0.4149332344532013 + 100.0 * 6.260159492492676
Epoch 1060, val loss: 1.143231987953186
Epoch 1070, training loss: 626.7362670898438 = 0.404953271150589 + 100.0 * 6.263313293457031
Epoch 1070, val loss: 1.14344322681427
Epoch 1080, training loss: 626.8184204101562 = 0.395077645778656 + 100.0 * 6.264233112335205
Epoch 1080, val loss: 1.1435199975967407
Epoch 1090, training loss: 626.4327392578125 = 0.3852669894695282 + 100.0 * 6.260474681854248
Epoch 1090, val loss: 1.1437817811965942
Epoch 1100, training loss: 626.1171264648438 = 0.37586259841918945 + 100.0 * 6.257412910461426
Epoch 1100, val loss: 1.1446624994277954
Epoch 1110, training loss: 626.1134033203125 = 0.36673370003700256 + 100.0 * 6.2574663162231445
Epoch 1110, val loss: 1.1456180810928345
Epoch 1120, training loss: 626.5206298828125 = 0.3577290177345276 + 100.0 * 6.261629104614258
Epoch 1120, val loss: 1.1462308168411255
Epoch 1130, training loss: 626.0986938476562 = 0.34891220927238464 + 100.0 * 6.257497787475586
Epoch 1130, val loss: 1.1472636461257935
Epoch 1140, training loss: 625.9979858398438 = 0.3404105603694916 + 100.0 * 6.256576061248779
Epoch 1140, val loss: 1.148532509803772
Epoch 1150, training loss: 626.0759887695312 = 0.33207234740257263 + 100.0 * 6.257438659667969
Epoch 1150, val loss: 1.1498231887817383
Epoch 1160, training loss: 625.9535522460938 = 0.32396817207336426 + 100.0 * 6.256295680999756
Epoch 1160, val loss: 1.1516108512878418
Epoch 1170, training loss: 625.8200073242188 = 0.31609874963760376 + 100.0 * 6.255039215087891
Epoch 1170, val loss: 1.153085708618164
Epoch 1180, training loss: 625.9112548828125 = 0.30843138694763184 + 100.0 * 6.256028175354004
Epoch 1180, val loss: 1.1550445556640625
Epoch 1190, training loss: 625.7037353515625 = 0.3009459674358368 + 100.0 * 6.254027843475342
Epoch 1190, val loss: 1.1567723751068115
Epoch 1200, training loss: 625.4873657226562 = 0.2937206029891968 + 100.0 * 6.251936912536621
Epoch 1200, val loss: 1.159197449684143
Epoch 1210, training loss: 625.3944091796875 = 0.2867354452610016 + 100.0 * 6.251076698303223
Epoch 1210, val loss: 1.1617207527160645
Epoch 1220, training loss: 625.9188232421875 = 0.2800091803073883 + 100.0 * 6.256387710571289
Epoch 1220, val loss: 1.1642502546310425
Epoch 1230, training loss: 625.5413208007812 = 0.27323654294013977 + 100.0 * 6.252680778503418
Epoch 1230, val loss: 1.1669403314590454
Epoch 1240, training loss: 625.339599609375 = 0.26682791113853455 + 100.0 * 6.250727653503418
Epoch 1240, val loss: 1.1699098348617554
Epoch 1250, training loss: 625.9375 = 0.2606188952922821 + 100.0 * 6.256768703460693
Epoch 1250, val loss: 1.1734033823013306
Epoch 1260, training loss: 625.3858642578125 = 0.2544439435005188 + 100.0 * 6.251314163208008
Epoch 1260, val loss: 1.175990343093872
Epoch 1270, training loss: 625.0811157226562 = 0.24853162467479706 + 100.0 * 6.248325824737549
Epoch 1270, val loss: 1.1794946193695068
Epoch 1280, training loss: 624.9846801757812 = 0.2428755909204483 + 100.0 * 6.24741792678833
Epoch 1280, val loss: 1.183350682258606
Epoch 1290, training loss: 625.6763305664062 = 0.2374361753463745 + 100.0 * 6.25438928604126
Epoch 1290, val loss: 1.1870325803756714
Epoch 1300, training loss: 625.3331298828125 = 0.23185241222381592 + 100.0 * 6.251012325286865
Epoch 1300, val loss: 1.1903332471847534
Epoch 1310, training loss: 624.9262084960938 = 0.22658097743988037 + 100.0 * 6.2469964027404785
Epoch 1310, val loss: 1.1943902969360352
Epoch 1320, training loss: 624.7860107421875 = 0.22151246666908264 + 100.0 * 6.245645046234131
Epoch 1320, val loss: 1.1985783576965332
Epoch 1330, training loss: 624.8999633789062 = 0.21662160754203796 + 100.0 * 6.246833801269531
Epoch 1330, val loss: 1.2027418613433838
Epoch 1340, training loss: 624.9391479492188 = 0.21180294454097748 + 100.0 * 6.2472734451293945
Epoch 1340, val loss: 1.2068253755569458
Epoch 1350, training loss: 624.9811401367188 = 0.20711900293827057 + 100.0 * 6.247740745544434
Epoch 1350, val loss: 1.2112669944763184
Epoch 1360, training loss: 624.9239501953125 = 0.2025633156299591 + 100.0 * 6.247213840484619
Epoch 1360, val loss: 1.2152230739593506
Epoch 1370, training loss: 624.7071533203125 = 0.19814974069595337 + 100.0 * 6.245090484619141
Epoch 1370, val loss: 1.2202261686325073
Epoch 1380, training loss: 624.7037353515625 = 0.19389760494232178 + 100.0 * 6.245098114013672
Epoch 1380, val loss: 1.224596381187439
Epoch 1390, training loss: 624.5491333007812 = 0.18972910940647125 + 100.0 * 6.243593692779541
Epoch 1390, val loss: 1.2295750379562378
Epoch 1400, training loss: 624.7404174804688 = 0.18570978939533234 + 100.0 * 6.245546817779541
Epoch 1400, val loss: 1.234298825263977
Epoch 1410, training loss: 624.5266723632812 = 0.18171413242816925 + 100.0 * 6.243449687957764
Epoch 1410, val loss: 1.2391619682312012
Epoch 1420, training loss: 624.3707275390625 = 0.17787888646125793 + 100.0 * 6.241928577423096
Epoch 1420, val loss: 1.2440098524093628
Epoch 1430, training loss: 624.4888305664062 = 0.17416933178901672 + 100.0 * 6.2431464195251465
Epoch 1430, val loss: 1.249066710472107
Epoch 1440, training loss: 624.4795532226562 = 0.17053209245204926 + 100.0 * 6.2430901527404785
Epoch 1440, val loss: 1.2539290189743042
Epoch 1450, training loss: 624.4476318359375 = 0.1669565737247467 + 100.0 * 6.242806911468506
Epoch 1450, val loss: 1.2590419054031372
Epoch 1460, training loss: 624.2402954101562 = 0.16348876059055328 + 100.0 * 6.240767955780029
Epoch 1460, val loss: 1.2641021013259888
Epoch 1470, training loss: 624.1246948242188 = 0.16017870604991913 + 100.0 * 6.239645481109619
Epoch 1470, val loss: 1.2696797847747803
Epoch 1480, training loss: 624.156005859375 = 0.15696106851100922 + 100.0 * 6.239990234375
Epoch 1480, val loss: 1.2749875783920288
Epoch 1490, training loss: 625.024658203125 = 0.15380673110485077 + 100.0 * 6.248708248138428
Epoch 1490, val loss: 1.279323935508728
Epoch 1500, training loss: 624.2008056640625 = 0.1505713313817978 + 100.0 * 6.24050235748291
Epoch 1500, val loss: 1.28505539894104
Epoch 1510, training loss: 623.9692993164062 = 0.1475483775138855 + 100.0 * 6.238217830657959
Epoch 1510, val loss: 1.2901865243911743
Epoch 1520, training loss: 623.928466796875 = 0.1446448415517807 + 100.0 * 6.237838268280029
Epoch 1520, val loss: 1.2959802150726318
Epoch 1530, training loss: 624.206787109375 = 0.14184240996837616 + 100.0 * 6.240649700164795
Epoch 1530, val loss: 1.3015068769454956
Epoch 1540, training loss: 623.8046264648438 = 0.1389578878879547 + 100.0 * 6.236656665802002
Epoch 1540, val loss: 1.306532621383667
Epoch 1550, training loss: 623.8079223632812 = 0.13621218502521515 + 100.0 * 6.2367167472839355
Epoch 1550, val loss: 1.3119581937789917
Epoch 1560, training loss: 623.920166015625 = 0.13357366621494293 + 100.0 * 6.237866401672363
Epoch 1560, val loss: 1.3176745176315308
Epoch 1570, training loss: 624.0459594726562 = 0.130996435880661 + 100.0 * 6.239149570465088
Epoch 1570, val loss: 1.3227828741073608
Epoch 1580, training loss: 623.8858032226562 = 0.12843604385852814 + 100.0 * 6.237574100494385
Epoch 1580, val loss: 1.3289399147033691
Epoch 1590, training loss: 623.8142700195312 = 0.1259927749633789 + 100.0 * 6.236883163452148
Epoch 1590, val loss: 1.334351658821106
Epoch 1600, training loss: 623.70849609375 = 0.12358991801738739 + 100.0 * 6.235848903656006
Epoch 1600, val loss: 1.3401892185211182
Epoch 1610, training loss: 623.6580200195312 = 0.12126529216766357 + 100.0 * 6.235367298126221
Epoch 1610, val loss: 1.3457281589508057
Epoch 1620, training loss: 624.3273315429688 = 0.11898720264434814 + 100.0 * 6.242083549499512
Epoch 1620, val loss: 1.3509800434112549
Epoch 1630, training loss: 623.8848876953125 = 0.11664660274982452 + 100.0 * 6.237682342529297
Epoch 1630, val loss: 1.3567689657211304
Epoch 1640, training loss: 623.5250244140625 = 0.1143857091665268 + 100.0 * 6.234106540679932
Epoch 1640, val loss: 1.3624111413955688
Epoch 1650, training loss: 623.4542846679688 = 0.1122756078839302 + 100.0 * 6.233419895172119
Epoch 1650, val loss: 1.3680936098098755
Epoch 1660, training loss: 623.4053344726562 = 0.1102222204208374 + 100.0 * 6.2329511642456055
Epoch 1660, val loss: 1.3743345737457275
Epoch 1670, training loss: 623.4158935546875 = 0.10822208970785141 + 100.0 * 6.233076572418213
Epoch 1670, val loss: 1.380124568939209
Epoch 1680, training loss: 624.0947875976562 = 0.10623884201049805 + 100.0 * 6.239885330200195
Epoch 1680, val loss: 1.3857147693634033
Epoch 1690, training loss: 623.4112548828125 = 0.10416941344738007 + 100.0 * 6.2330708503723145
Epoch 1690, val loss: 1.3911211490631104
Epoch 1700, training loss: 623.4066772460938 = 0.10224795341491699 + 100.0 * 6.233044147491455
Epoch 1700, val loss: 1.396424412727356
Epoch 1710, training loss: 623.2298583984375 = 0.10040200501680374 + 100.0 * 6.231294631958008
Epoch 1710, val loss: 1.4024654626846313
Epoch 1720, training loss: 623.2500610351562 = 0.09861505031585693 + 100.0 * 6.2315144538879395
Epoch 1720, val loss: 1.4083925485610962
Epoch 1730, training loss: 624.2149047851562 = 0.09685500711202621 + 100.0 * 6.241180419921875
Epoch 1730, val loss: 1.4136102199554443
Epoch 1740, training loss: 623.4188842773438 = 0.09504535049200058 + 100.0 * 6.233238220214844
Epoch 1740, val loss: 1.4196386337280273
Epoch 1750, training loss: 623.1619873046875 = 0.09331194311380386 + 100.0 * 6.230686664581299
Epoch 1750, val loss: 1.4254926443099976
Epoch 1760, training loss: 623.22216796875 = 0.09167138487100601 + 100.0 * 6.23130464553833
Epoch 1760, val loss: 1.4317233562469482
Epoch 1770, training loss: 623.5250244140625 = 0.09004039317369461 + 100.0 * 6.234349727630615
Epoch 1770, val loss: 1.4373290538787842
Epoch 1780, training loss: 623.1891479492188 = 0.08840461075305939 + 100.0 * 6.2310075759887695
Epoch 1780, val loss: 1.442240595817566
Epoch 1790, training loss: 623.0826416015625 = 0.08685316145420074 + 100.0 * 6.2299580574035645
Epoch 1790, val loss: 1.4485305547714233
Epoch 1800, training loss: 623.3722534179688 = 0.0853513851761818 + 100.0 * 6.2328691482543945
Epoch 1800, val loss: 1.4536958932876587
Epoch 1810, training loss: 623.1083984375 = 0.08383271843194962 + 100.0 * 6.230245113372803
Epoch 1810, val loss: 1.4603074789047241
Epoch 1820, training loss: 623.0352172851562 = 0.08235158026218414 + 100.0 * 6.229528903961182
Epoch 1820, val loss: 1.4653853178024292
Epoch 1830, training loss: 622.9027099609375 = 0.08092184364795685 + 100.0 * 6.228217601776123
Epoch 1830, val loss: 1.4717063903808594
Epoch 1840, training loss: 623.1071166992188 = 0.07955310493707657 + 100.0 * 6.230276107788086
Epoch 1840, val loss: 1.4776105880737305
Epoch 1850, training loss: 622.9228515625 = 0.07814610004425049 + 100.0 * 6.228446960449219
Epoch 1850, val loss: 1.4829596281051636
Epoch 1860, training loss: 622.84716796875 = 0.07676994055509567 + 100.0 * 6.22770357131958
Epoch 1860, val loss: 1.4886116981506348
Epoch 1870, training loss: 623.1237182617188 = 0.07546381652355194 + 100.0 * 6.230483055114746
Epoch 1870, val loss: 1.4948339462280273
Epoch 1880, training loss: 622.7963256835938 = 0.0741630345582962 + 100.0 * 6.227221488952637
Epoch 1880, val loss: 1.5001370906829834
Epoch 1890, training loss: 622.9557495117188 = 0.07291775941848755 + 100.0 * 6.228828430175781
Epoch 1890, val loss: 1.506435751914978
Epoch 1900, training loss: 623.2906494140625 = 0.07168012857437134 + 100.0 * 6.232190132141113
Epoch 1900, val loss: 1.5118969678878784
Epoch 1910, training loss: 622.9518432617188 = 0.07042007148265839 + 100.0 * 6.228814125061035
Epoch 1910, val loss: 1.5173074007034302
Epoch 1920, training loss: 622.8021850585938 = 0.06922794878482819 + 100.0 * 6.227329730987549
Epoch 1920, val loss: 1.523112416267395
Epoch 1930, training loss: 622.669677734375 = 0.06808074563741684 + 100.0 * 6.226016521453857
Epoch 1930, val loss: 1.5293294191360474
Epoch 1940, training loss: 622.7401123046875 = 0.0669722929596901 + 100.0 * 6.226731777191162
Epoch 1940, val loss: 1.535311222076416
Epoch 1950, training loss: 623.1992797851562 = 0.06585455685853958 + 100.0 * 6.231334209442139
Epoch 1950, val loss: 1.5403735637664795
Epoch 1960, training loss: 622.8710327148438 = 0.06473668664693832 + 100.0 * 6.228062629699707
Epoch 1960, val loss: 1.5456260442733765
Epoch 1970, training loss: 622.6437377929688 = 0.06366070359945297 + 100.0 * 6.225800514221191
Epoch 1970, val loss: 1.5520858764648438
Epoch 1980, training loss: 622.646728515625 = 0.06263177841901779 + 100.0 * 6.225841045379639
Epoch 1980, val loss: 1.5576263666152954
Epoch 1990, training loss: 622.8524169921875 = 0.06160533055663109 + 100.0 * 6.227908134460449
Epoch 1990, val loss: 1.5634576082229614
Epoch 2000, training loss: 622.6781616210938 = 0.060590606182813644 + 100.0 * 6.226175785064697
Epoch 2000, val loss: 1.569502592086792
Epoch 2010, training loss: 622.5387573242188 = 0.05960511043667793 + 100.0 * 6.224791049957275
Epoch 2010, val loss: 1.57498300075531
Epoch 2020, training loss: 622.610595703125 = 0.058659665286540985 + 100.0 * 6.22551965713501
Epoch 2020, val loss: 1.5811939239501953
Epoch 2030, training loss: 622.8233032226562 = 0.057704951614141464 + 100.0 * 6.227656364440918
Epoch 2030, val loss: 1.5861369371414185
Epoch 2040, training loss: 622.521484375 = 0.05675072968006134 + 100.0 * 6.224647521972656
Epoch 2040, val loss: 1.5921685695648193
Epoch 2050, training loss: 622.8133544921875 = 0.05584831163287163 + 100.0 * 6.227574825286865
Epoch 2050, val loss: 1.598219871520996
Epoch 2060, training loss: 622.35009765625 = 0.05494163930416107 + 100.0 * 6.222951889038086
Epoch 2060, val loss: 1.602394938468933
Epoch 2070, training loss: 622.3235473632812 = 0.054081495851278305 + 100.0 * 6.2226948738098145
Epoch 2070, val loss: 1.608436942100525
Epoch 2080, training loss: 622.588623046875 = 0.053247638046741486 + 100.0 * 6.225353717803955
Epoch 2080, val loss: 1.6142604351043701
Epoch 2090, training loss: 622.3299560546875 = 0.05240364745259285 + 100.0 * 6.222774982452393
Epoch 2090, val loss: 1.6199712753295898
Epoch 2100, training loss: 622.4014892578125 = 0.051592178642749786 + 100.0 * 6.223499298095703
Epoch 2100, val loss: 1.6256886720657349
Epoch 2110, training loss: 622.64501953125 = 0.05079161375761032 + 100.0 * 6.225942611694336
Epoch 2110, val loss: 1.6308989524841309
Epoch 2120, training loss: 622.3180541992188 = 0.04998597875237465 + 100.0 * 6.222680568695068
Epoch 2120, val loss: 1.635923147201538
Epoch 2130, training loss: 622.3547973632812 = 0.04921317100524902 + 100.0 * 6.223055839538574
Epoch 2130, val loss: 1.6419442892074585
Epoch 2140, training loss: 622.2025756835938 = 0.04846103861927986 + 100.0 * 6.221540927886963
Epoch 2140, val loss: 1.6475974321365356
Epoch 2150, training loss: 622.2009887695312 = 0.04772798344492912 + 100.0 * 6.221532344818115
Epoch 2150, val loss: 1.6528855562210083
Epoch 2160, training loss: 622.3504028320312 = 0.047007203102111816 + 100.0 * 6.223033905029297
Epoch 2160, val loss: 1.658149242401123
Epoch 2170, training loss: 622.4425659179688 = 0.04629558324813843 + 100.0 * 6.223962306976318
Epoch 2170, val loss: 1.6629070043563843
Epoch 2180, training loss: 622.4534301757812 = 0.04558543488383293 + 100.0 * 6.224078178405762
Epoch 2180, val loss: 1.6679356098175049
Epoch 2190, training loss: 622.11083984375 = 0.044863246381282806 + 100.0 * 6.2206597328186035
Epoch 2190, val loss: 1.6736010313034058
Epoch 2200, training loss: 622.1229858398438 = 0.044203270226716995 + 100.0 * 6.22078800201416
Epoch 2200, val loss: 1.6789677143096924
Epoch 2210, training loss: 622.1830444335938 = 0.04356034845113754 + 100.0 * 6.2213945388793945
Epoch 2210, val loss: 1.6842468976974487
Epoch 2220, training loss: 622.1602172851562 = 0.042912594974040985 + 100.0 * 6.221173286437988
Epoch 2220, val loss: 1.6894738674163818
Epoch 2230, training loss: 622.309326171875 = 0.04228474199771881 + 100.0 * 6.222670555114746
Epoch 2230, val loss: 1.6953911781311035
Epoch 2240, training loss: 621.9878540039062 = 0.04165459796786308 + 100.0 * 6.2194623947143555
Epoch 2240, val loss: 1.700566291809082
Epoch 2250, training loss: 622.139404296875 = 0.04105662927031517 + 100.0 * 6.220983028411865
Epoch 2250, val loss: 1.706358790397644
Epoch 2260, training loss: 622.1519165039062 = 0.04046352580189705 + 100.0 * 6.221114635467529
Epoch 2260, val loss: 1.7112634181976318
Epoch 2270, training loss: 621.9169921875 = 0.039866577833890915 + 100.0 * 6.218771457672119
Epoch 2270, val loss: 1.7158827781677246
Epoch 2280, training loss: 622.1544189453125 = 0.039310768246650696 + 100.0 * 6.221150875091553
Epoch 2280, val loss: 1.7205075025558472
Epoch 2290, training loss: 622.053955078125 = 0.038743916898965836 + 100.0 * 6.220151901245117
Epoch 2290, val loss: 1.7261905670166016
Epoch 2300, training loss: 621.93359375 = 0.03818710893392563 + 100.0 * 6.218954086303711
Epoch 2300, val loss: 1.7318603992462158
Epoch 2310, training loss: 622.0317993164062 = 0.0376516617834568 + 100.0 * 6.219941139221191
Epoch 2310, val loss: 1.7366501092910767
Epoch 2320, training loss: 622.5435180664062 = 0.03712259978055954 + 100.0 * 6.225063800811768
Epoch 2320, val loss: 1.7407690286636353
Epoch 2330, training loss: 621.8950805664062 = 0.036552462726831436 + 100.0 * 6.21858549118042
Epoch 2330, val loss: 1.7459315061569214
Epoch 2340, training loss: 621.7670288085938 = 0.03603782132267952 + 100.0 * 6.217310428619385
Epoch 2340, val loss: 1.7509515285491943
Epoch 2350, training loss: 621.722900390625 = 0.03555314242839813 + 100.0 * 6.2168731689453125
Epoch 2350, val loss: 1.7568360567092896
Epoch 2360, training loss: 621.6942749023438 = 0.03507939726114273 + 100.0 * 6.216591835021973
Epoch 2360, val loss: 1.7618919610977173
Epoch 2370, training loss: 622.423828125 = 0.03463759273290634 + 100.0 * 6.223891735076904
Epoch 2370, val loss: 1.7679554224014282
Epoch 2380, training loss: 621.8040771484375 = 0.03411701321601868 + 100.0 * 6.2176995277404785
Epoch 2380, val loss: 1.7703505754470825
Epoch 2390, training loss: 621.8110961914062 = 0.033652760088443756 + 100.0 * 6.217774868011475
Epoch 2390, val loss: 1.776288390159607
Epoch 2400, training loss: 621.9030151367188 = 0.03319830074906349 + 100.0 * 6.218698024749756
Epoch 2400, val loss: 1.7806410789489746
Epoch 2410, training loss: 621.8363647460938 = 0.03275458514690399 + 100.0 * 6.218035697937012
Epoch 2410, val loss: 1.785563588142395
Epoch 2420, training loss: 621.7695922851562 = 0.032308921217918396 + 100.0 * 6.217372894287109
Epoch 2420, val loss: 1.7898142337799072
Epoch 2430, training loss: 621.64501953125 = 0.031872835010290146 + 100.0 * 6.216131687164307
Epoch 2430, val loss: 1.7950364351272583
Epoch 2440, training loss: 621.7604370117188 = 0.0314619354903698 + 100.0 * 6.217289924621582
Epoch 2440, val loss: 1.7998617887496948
Epoch 2450, training loss: 622.20849609375 = 0.031058598309755325 + 100.0 * 6.221774101257324
Epoch 2450, val loss: 1.805025577545166
Epoch 2460, training loss: 621.7614135742188 = 0.030619189143180847 + 100.0 * 6.2173075675964355
Epoch 2460, val loss: 1.8089358806610107
Epoch 2470, training loss: 621.5845336914062 = 0.030208105221390724 + 100.0 * 6.215543270111084
Epoch 2470, val loss: 1.8132177591323853
Epoch 2480, training loss: 621.4833374023438 = 0.029827836900949478 + 100.0 * 6.214534759521484
Epoch 2480, val loss: 1.8184670209884644
Epoch 2490, training loss: 621.5578002929688 = 0.02945505455136299 + 100.0 * 6.215282917022705
Epoch 2490, val loss: 1.8232338428497314
Epoch 2500, training loss: 622.2321166992188 = 0.029077326878905296 + 100.0 * 6.2220306396484375
Epoch 2500, val loss: 1.8274896144866943
Epoch 2510, training loss: 621.7593383789062 = 0.028683656826615334 + 100.0 * 6.217306613922119
Epoch 2510, val loss: 1.8322826623916626
Epoch 2520, training loss: 621.4931640625 = 0.028313683345913887 + 100.0 * 6.214648723602295
Epoch 2520, val loss: 1.8362219333648682
Epoch 2530, training loss: 621.5123901367188 = 0.0279626976698637 + 100.0 * 6.214844226837158
Epoch 2530, val loss: 1.8406723737716675
Epoch 2540, training loss: 621.7518920898438 = 0.027618419378995895 + 100.0 * 6.217242240905762
Epoch 2540, val loss: 1.844930648803711
Epoch 2550, training loss: 621.7313232421875 = 0.027261929586529732 + 100.0 * 6.217040538787842
Epoch 2550, val loss: 1.850123405456543
Epoch 2560, training loss: 621.5625610351562 = 0.02690914459526539 + 100.0 * 6.215356349945068
Epoch 2560, val loss: 1.8529399633407593
Epoch 2570, training loss: 621.3828125 = 0.026567213237285614 + 100.0 * 6.213562488555908
Epoch 2570, val loss: 1.858742117881775
Epoch 2580, training loss: 621.30126953125 = 0.0262437891215086 + 100.0 * 6.21274995803833
Epoch 2580, val loss: 1.8632389307022095
Epoch 2590, training loss: 621.2913818359375 = 0.02593233995139599 + 100.0 * 6.2126545906066895
Epoch 2590, val loss: 1.867785096168518
Epoch 2600, training loss: 622.1151123046875 = 0.025648165494203568 + 100.0 * 6.220894813537598
Epoch 2600, val loss: 1.873530387878418
Epoch 2610, training loss: 621.8935546875 = 0.02530381642282009 + 100.0 * 6.218682289123535
Epoch 2610, val loss: 1.8754186630249023
Epoch 2620, training loss: 621.4241943359375 = 0.024971820414066315 + 100.0 * 6.213992595672607
Epoch 2620, val loss: 1.879987359046936
Epoch 2630, training loss: 621.2685546875 = 0.024676604196429253 + 100.0 * 6.212439060211182
Epoch 2630, val loss: 1.8839781284332275
Epoch 2640, training loss: 621.1793823242188 = 0.0243900828063488 + 100.0 * 6.211549758911133
Epoch 2640, val loss: 1.8890697956085205
Epoch 2650, training loss: 621.2081298828125 = 0.02411399595439434 + 100.0 * 6.2118401527404785
Epoch 2650, val loss: 1.8933807611465454
Epoch 2660, training loss: 622.0740966796875 = 0.023846864700317383 + 100.0 * 6.2205023765563965
Epoch 2660, val loss: 1.8973653316497803
Epoch 2670, training loss: 621.91845703125 = 0.02353721112012863 + 100.0 * 6.218948841094971
Epoch 2670, val loss: 1.900085210800171
Epoch 2680, training loss: 621.3535766601562 = 0.023248441517353058 + 100.0 * 6.213303089141846
Epoch 2680, val loss: 1.9047490358352661
Epoch 2690, training loss: 621.1751098632812 = 0.022973058745265007 + 100.0 * 6.211521625518799
Epoch 2690, val loss: 1.909501552581787
Epoch 2700, training loss: 621.2159423828125 = 0.022718902677297592 + 100.0 * 6.21193265914917
Epoch 2700, val loss: 1.9140269756317139
Epoch 2710, training loss: 621.6239013671875 = 0.022466231137514114 + 100.0 * 6.216014385223389
Epoch 2710, val loss: 1.9180033206939697
Epoch 2720, training loss: 621.3177490234375 = 0.022193927317857742 + 100.0 * 6.212955474853516
Epoch 2720, val loss: 1.9208430051803589
Epoch 2730, training loss: 621.28369140625 = 0.021936291828751564 + 100.0 * 6.212617874145508
Epoch 2730, val loss: 1.9255179166793823
Epoch 2740, training loss: 621.281005859375 = 0.02168566733598709 + 100.0 * 6.2125935554504395
Epoch 2740, val loss: 1.929687261581421
Epoch 2750, training loss: 621.1770629882812 = 0.02143876999616623 + 100.0 * 6.211556434631348
Epoch 2750, val loss: 1.933654546737671
Epoch 2760, training loss: 621.1470947265625 = 0.02119656465947628 + 100.0 * 6.211258888244629
Epoch 2760, val loss: 1.9375951290130615
Epoch 2770, training loss: 621.3053588867188 = 0.020967531949281693 + 100.0 * 6.212843418121338
Epoch 2770, val loss: 1.9419928789138794
Epoch 2780, training loss: 621.1580200195312 = 0.020731676369905472 + 100.0 * 6.2113728523254395
Epoch 2780, val loss: 1.945699691772461
Epoch 2790, training loss: 621.1553344726562 = 0.02049991302192211 + 100.0 * 6.211348533630371
Epoch 2790, val loss: 1.9489644765853882
Epoch 2800, training loss: 621.2576904296875 = 0.020275235176086426 + 100.0 * 6.212374210357666
Epoch 2800, val loss: 1.952850103378296
Epoch 2810, training loss: 620.9910888671875 = 0.020048754289746284 + 100.0 * 6.209710597991943
Epoch 2810, val loss: 1.9565787315368652
Epoch 2820, training loss: 621.2533569335938 = 0.019835002720355988 + 100.0 * 6.212335109710693
Epoch 2820, val loss: 1.960713267326355
Epoch 2830, training loss: 621.213134765625 = 0.019612174481153488 + 100.0 * 6.211935520172119
Epoch 2830, val loss: 1.9637740850448608
Epoch 2840, training loss: 621.02587890625 = 0.019393598660826683 + 100.0 * 6.210064888000488
Epoch 2840, val loss: 1.9666978120803833
Epoch 2850, training loss: 621.169921875 = 0.019187331199645996 + 100.0 * 6.211507797241211
Epoch 2850, val loss: 1.9706257581710815
Epoch 2860, training loss: 621.2503662109375 = 0.01897890493273735 + 100.0 * 6.212313652038574
Epoch 2860, val loss: 1.9749687910079956
Epoch 2870, training loss: 621.081787109375 = 0.01877962239086628 + 100.0 * 6.210629940032959
Epoch 2870, val loss: 1.979089617729187
Epoch 2880, training loss: 620.9164428710938 = 0.018574519082903862 + 100.0 * 6.20897912979126
Epoch 2880, val loss: 1.9816317558288574
Epoch 2890, training loss: 620.8842163085938 = 0.01838381215929985 + 100.0 * 6.208658218383789
Epoch 2890, val loss: 1.9854503870010376
Epoch 2900, training loss: 621.1328125 = 0.018201716244220734 + 100.0 * 6.211146354675293
Epoch 2900, val loss: 1.9895625114440918
Epoch 2910, training loss: 621.1848754882812 = 0.0180120300501585 + 100.0 * 6.211668491363525
Epoch 2910, val loss: 1.9932974576950073
Epoch 2920, training loss: 620.9647216796875 = 0.017814408987760544 + 100.0 * 6.209468841552734
Epoch 2920, val loss: 1.996536135673523
Epoch 2930, training loss: 620.9741821289062 = 0.01762724295258522 + 100.0 * 6.20956563949585
Epoch 2930, val loss: 1.9998236894607544
Epoch 2940, training loss: 621.005126953125 = 0.01745106279850006 + 100.0 * 6.209877014160156
Epoch 2940, val loss: 2.0030298233032227
Epoch 2950, training loss: 621.1875610351562 = 0.01727702096104622 + 100.0 * 6.211702823638916
Epoch 2950, val loss: 2.0071780681610107
Epoch 2960, training loss: 621.0477294921875 = 0.017093511298298836 + 100.0 * 6.210306167602539
Epoch 2960, val loss: 2.009996175765991
Epoch 2970, training loss: 620.933349609375 = 0.01691167801618576 + 100.0 * 6.209164619445801
Epoch 2970, val loss: 2.012864112854004
Epoch 2980, training loss: 620.9282836914062 = 0.016745055094361305 + 100.0 * 6.209115505218506
Epoch 2980, val loss: 2.017218589782715
Epoch 2990, training loss: 620.7954711914062 = 0.01657913438975811 + 100.0 * 6.207788467407227
Epoch 2990, val loss: 2.0208394527435303
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6333333333333333
0.7917764891934634
=== training gcn model ===
Epoch 0, training loss: 861.6201782226562 = 1.9376778602600098 + 100.0 * 8.596824645996094
Epoch 0, val loss: 1.9410045146942139
Epoch 10, training loss: 861.5186157226562 = 1.929845929145813 + 100.0 * 8.595887184143066
Epoch 10, val loss: 1.9326668977737427
Epoch 20, training loss: 860.87158203125 = 1.9198033809661865 + 100.0 * 8.589517593383789
Epoch 20, val loss: 1.9218605756759644
Epoch 30, training loss: 856.0535888671875 = 1.906377911567688 + 100.0 * 8.541472434997559
Epoch 30, val loss: 1.9073143005371094
Epoch 40, training loss: 812.810302734375 = 1.8895632028579712 + 100.0 * 8.109207153320312
Epoch 40, val loss: 1.889086127281189
Epoch 50, training loss: 750.444091796875 = 1.8704907894134521 + 100.0 * 7.48573637008667
Epoch 50, val loss: 1.8689494132995605
Epoch 60, training loss: 728.3994750976562 = 1.8584420680999756 + 100.0 * 7.26540994644165
Epoch 60, val loss: 1.8573083877563477
Epoch 70, training loss: 713.3955688476562 = 1.8469089269638062 + 100.0 * 7.1154866218566895
Epoch 70, val loss: 1.845800518989563
Epoch 80, training loss: 699.9127197265625 = 1.8369698524475098 + 100.0 * 6.980757236480713
Epoch 80, val loss: 1.8361908197402954
Epoch 90, training loss: 689.4338989257812 = 1.829581618309021 + 100.0 * 6.87604284286499
Epoch 90, val loss: 1.828786015510559
Epoch 100, training loss: 680.9951171875 = 1.8231666088104248 + 100.0 * 6.791719436645508
Epoch 100, val loss: 1.8223220109939575
Epoch 110, training loss: 674.184814453125 = 1.817016839981079 + 100.0 * 6.723678112030029
Epoch 110, val loss: 1.8160912990570068
Epoch 120, training loss: 668.8585205078125 = 1.8107541799545288 + 100.0 * 6.670477867126465
Epoch 120, val loss: 1.8096766471862793
Epoch 130, training loss: 664.5862426757812 = 1.8043897151947021 + 100.0 * 6.627818584442139
Epoch 130, val loss: 1.8033109903335571
Epoch 140, training loss: 660.96923828125 = 1.7978441715240479 + 100.0 * 6.591713905334473
Epoch 140, val loss: 1.796839714050293
Epoch 150, training loss: 657.9869384765625 = 1.7910611629486084 + 100.0 * 6.5619587898254395
Epoch 150, val loss: 1.7903393507003784
Epoch 160, training loss: 656.0135498046875 = 1.7839382886886597 + 100.0 * 6.542296409606934
Epoch 160, val loss: 1.783645510673523
Epoch 170, training loss: 653.6803588867188 = 1.7761682271957397 + 100.0 * 6.519041538238525
Epoch 170, val loss: 1.776545763015747
Epoch 180, training loss: 651.8938598632812 = 1.7679429054260254 + 100.0 * 6.5012593269348145
Epoch 180, val loss: 1.7691895961761475
Epoch 190, training loss: 650.4293823242188 = 1.759078860282898 + 100.0 * 6.486702919006348
Epoch 190, val loss: 1.7613978385925293
Epoch 200, training loss: 649.1422729492188 = 1.749381422996521 + 100.0 * 6.473928928375244
Epoch 200, val loss: 1.7529664039611816
Epoch 210, training loss: 647.9148559570312 = 1.738897442817688 + 100.0 * 6.461759567260742
Epoch 210, val loss: 1.744015097618103
Epoch 220, training loss: 646.682373046875 = 1.7275420427322388 + 100.0 * 6.449548244476318
Epoch 220, val loss: 1.7344034910202026
Epoch 230, training loss: 646.00048828125 = 1.7152092456817627 + 100.0 * 6.442852973937988
Epoch 230, val loss: 1.7240124940872192
Epoch 240, training loss: 644.7736206054688 = 1.701825737953186 + 100.0 * 6.430717945098877
Epoch 240, val loss: 1.712802529335022
Epoch 250, training loss: 643.8701171875 = 1.6873879432678223 + 100.0 * 6.42182731628418
Epoch 250, val loss: 1.7008254528045654
Epoch 260, training loss: 643.19921875 = 1.6718339920043945 + 100.0 * 6.415274143218994
Epoch 260, val loss: 1.6879122257232666
Epoch 270, training loss: 642.4706420898438 = 1.6550850868225098 + 100.0 * 6.40815544128418
Epoch 270, val loss: 1.6741451025009155
Epoch 280, training loss: 641.69140625 = 1.637205958366394 + 100.0 * 6.40054178237915
Epoch 280, val loss: 1.659304141998291
Epoch 290, training loss: 641.26513671875 = 1.6181886196136475 + 100.0 * 6.396469593048096
Epoch 290, val loss: 1.643616795539856
Epoch 300, training loss: 640.7069091796875 = 1.5979499816894531 + 100.0 * 6.39108943939209
Epoch 300, val loss: 1.6270467042922974
Epoch 310, training loss: 639.8369750976562 = 1.5767076015472412 + 100.0 * 6.382602691650391
Epoch 310, val loss: 1.6094788312911987
Epoch 320, training loss: 639.47705078125 = 1.55453622341156 + 100.0 * 6.37922477722168
Epoch 320, val loss: 1.5911751985549927
Epoch 330, training loss: 638.9932861328125 = 1.5314128398895264 + 100.0 * 6.3746185302734375
Epoch 330, val loss: 1.5721169710159302
Epoch 340, training loss: 638.50439453125 = 1.5074609518051147 + 100.0 * 6.369969367980957
Epoch 340, val loss: 1.5523654222488403
Epoch 350, training loss: 637.9707641601562 = 1.4829427003860474 + 100.0 * 6.364878177642822
Epoch 350, val loss: 1.5321907997131348
Epoch 360, training loss: 637.5531005859375 = 1.457968831062317 + 100.0 * 6.3609514236450195
Epoch 360, val loss: 1.511658787727356
Epoch 370, training loss: 637.3656005859375 = 1.4327311515808105 + 100.0 * 6.359328746795654
Epoch 370, val loss: 1.4909229278564453
Epoch 380, training loss: 636.8453979492188 = 1.4072593450546265 + 100.0 * 6.354381084442139
Epoch 380, val loss: 1.4698776006698608
Epoch 390, training loss: 636.292724609375 = 1.3818069696426392 + 100.0 * 6.349108695983887
Epoch 390, val loss: 1.4491783380508423
Epoch 400, training loss: 636.2367553710938 = 1.3565638065338135 + 100.0 * 6.348801612854004
Epoch 400, val loss: 1.4286491870880127
Epoch 410, training loss: 635.8796997070312 = 1.331575632095337 + 100.0 * 6.345480918884277
Epoch 410, val loss: 1.4082599878311157
Epoch 420, training loss: 635.3757934570312 = 1.3069692850112915 + 100.0 * 6.340688705444336
Epoch 420, val loss: 1.3883492946624756
Epoch 430, training loss: 635.1089477539062 = 1.2828644514083862 + 100.0 * 6.338261127471924
Epoch 430, val loss: 1.3690783977508545
Epoch 440, training loss: 634.8485107421875 = 1.2592432498931885 + 100.0 * 6.335892677307129
Epoch 440, val loss: 1.3501683473587036
Epoch 450, training loss: 634.6261596679688 = 1.2361725568771362 + 100.0 * 6.333899974822998
Epoch 450, val loss: 1.3317546844482422
Epoch 460, training loss: 634.1301879882812 = 1.2135809659957886 + 100.0 * 6.329166412353516
Epoch 460, val loss: 1.314038634300232
Epoch 470, training loss: 633.8219604492188 = 1.1916227340698242 + 100.0 * 6.326303005218506
Epoch 470, val loss: 1.2969155311584473
Epoch 480, training loss: 634.0186157226562 = 1.170073390007019 + 100.0 * 6.328485488891602
Epoch 480, val loss: 1.2802321910858154
Epoch 490, training loss: 633.3815307617188 = 1.1489542722702026 + 100.0 * 6.322326183319092
Epoch 490, val loss: 1.264272689819336
Epoch 500, training loss: 632.9955444335938 = 1.128356695175171 + 100.0 * 6.318672180175781
Epoch 500, val loss: 1.2487163543701172
Epoch 510, training loss: 632.9332275390625 = 1.1082497835159302 + 100.0 * 6.318249702453613
Epoch 510, val loss: 1.2337146997451782
Epoch 520, training loss: 632.7794799804688 = 1.0882091522216797 + 100.0 * 6.31691312789917
Epoch 520, val loss: 1.2189005613327026
Epoch 530, training loss: 632.60791015625 = 1.0686057806015015 + 100.0 * 6.315392971038818
Epoch 530, val loss: 1.2043912410736084
Epoch 540, training loss: 632.191650390625 = 1.049211859703064 + 100.0 * 6.311424732208252
Epoch 540, val loss: 1.1906746625900269
Epoch 550, training loss: 631.8836059570312 = 1.0301259756088257 + 100.0 * 6.308534622192383
Epoch 550, val loss: 1.1770342588424683
Epoch 560, training loss: 631.8194580078125 = 1.0112898349761963 + 100.0 * 6.30808162689209
Epoch 560, val loss: 1.1639270782470703
Epoch 570, training loss: 631.691162109375 = 0.9924553036689758 + 100.0 * 6.306987285614014
Epoch 570, val loss: 1.151047945022583
Epoch 580, training loss: 631.379150390625 = 0.9738547205924988 + 100.0 * 6.304052829742432
Epoch 580, val loss: 1.1381309032440186
Epoch 590, training loss: 631.195068359375 = 0.9555119276046753 + 100.0 * 6.302395343780518
Epoch 590, val loss: 1.1257191896438599
Epoch 600, training loss: 631.3724365234375 = 0.9373199343681335 + 100.0 * 6.304351329803467
Epoch 600, val loss: 1.1136956214904785
Epoch 610, training loss: 630.8073120117188 = 0.919296145439148 + 100.0 * 6.298880100250244
Epoch 610, val loss: 1.101726770401001
Epoch 620, training loss: 630.7201538085938 = 0.9015002846717834 + 100.0 * 6.298186779022217
Epoch 620, val loss: 1.0904302597045898
Epoch 630, training loss: 630.6744995117188 = 0.8839555978775024 + 100.0 * 6.297904968261719
Epoch 630, val loss: 1.0793224573135376
Epoch 640, training loss: 630.5938720703125 = 0.8665598034858704 + 100.0 * 6.2972731590271
Epoch 640, val loss: 1.068447470664978
Epoch 650, training loss: 630.3895874023438 = 0.8493519425392151 + 100.0 * 6.2954020500183105
Epoch 650, val loss: 1.0579487085342407
Epoch 660, training loss: 629.9547119140625 = 0.832360565662384 + 100.0 * 6.291224002838135
Epoch 660, val loss: 1.0478506088256836
Epoch 670, training loss: 630.06591796875 = 0.8157580494880676 + 100.0 * 6.292501926422119
Epoch 670, val loss: 1.0382648706436157
Epoch 680, training loss: 629.6775512695312 = 0.7992908358573914 + 100.0 * 6.288782596588135
Epoch 680, val loss: 1.0287681818008423
Epoch 690, training loss: 629.5499877929688 = 0.7831497192382812 + 100.0 * 6.287668704986572
Epoch 690, val loss: 1.0197468996047974
Epoch 700, training loss: 629.68701171875 = 0.7673208117485046 + 100.0 * 6.289196491241455
Epoch 700, val loss: 1.0112892389297485
Epoch 710, training loss: 629.2208862304688 = 0.751787543296814 + 100.0 * 6.284690856933594
Epoch 710, val loss: 1.002881646156311
Epoch 720, training loss: 629.1014404296875 = 0.7365545630455017 + 100.0 * 6.283648490905762
Epoch 720, val loss: 0.9952290654182434
Epoch 730, training loss: 629.2765502929688 = 0.7216141223907471 + 100.0 * 6.285549163818359
Epoch 730, val loss: 0.9877623319625854
Epoch 740, training loss: 629.0081787109375 = 0.7068471312522888 + 100.0 * 6.283013343811035
Epoch 740, val loss: 0.9808118939399719
Epoch 750, training loss: 629.4070434570312 = 0.6923695802688599 + 100.0 * 6.28714656829834
Epoch 750, val loss: 0.9739940762519836
Epoch 760, training loss: 628.7537841796875 = 0.6781456470489502 + 100.0 * 6.280755996704102
Epoch 760, val loss: 0.96770179271698
Epoch 770, training loss: 628.4254150390625 = 0.6642749309539795 + 100.0 * 6.27761173248291
Epoch 770, val loss: 0.9620508551597595
Epoch 780, training loss: 628.2911376953125 = 0.6508170962333679 + 100.0 * 6.276402950286865
Epoch 780, val loss: 0.9565394520759583
Epoch 790, training loss: 629.28125 = 0.6375940442085266 + 100.0 * 6.286436557769775
Epoch 790, val loss: 0.9514024257659912
Epoch 800, training loss: 628.45751953125 = 0.6244659423828125 + 100.0 * 6.2783308029174805
Epoch 800, val loss: 0.946774959564209
Epoch 810, training loss: 628.0137939453125 = 0.611653208732605 + 100.0 * 6.274021625518799
Epoch 810, val loss: 0.9422726035118103
Epoch 820, training loss: 628.4058837890625 = 0.5991795063018799 + 100.0 * 6.278067111968994
Epoch 820, val loss: 0.9381540417671204
Epoch 830, training loss: 627.8974609375 = 0.5869380235671997 + 100.0 * 6.273105621337891
Epoch 830, val loss: 0.9345517754554749
Epoch 840, training loss: 627.7930297851562 = 0.5750579833984375 + 100.0 * 6.27217960357666
Epoch 840, val loss: 0.931071400642395
Epoch 850, training loss: 627.8851318359375 = 0.5633077025413513 + 100.0 * 6.273218154907227
Epoch 850, val loss: 0.928024411201477
Epoch 860, training loss: 627.4523315429688 = 0.5517908334732056 + 100.0 * 6.269004821777344
Epoch 860, val loss: 0.9250508546829224
Epoch 870, training loss: 627.9238891601562 = 0.5406051874160767 + 100.0 * 6.273833274841309
Epoch 870, val loss: 0.9226067066192627
Epoch 880, training loss: 627.34326171875 = 0.5296962857246399 + 100.0 * 6.2681355476379395
Epoch 880, val loss: 0.9203431606292725
Epoch 890, training loss: 627.1148071289062 = 0.518987238407135 + 100.0 * 6.265958309173584
Epoch 890, val loss: 0.9183392524719238
Epoch 900, training loss: 627.1217651367188 = 0.5086016058921814 + 100.0 * 6.266131401062012
Epoch 900, val loss: 0.9166341423988342
Epoch 910, training loss: 627.7542114257812 = 0.49835166335105896 + 100.0 * 6.272558689117432
Epoch 910, val loss: 0.9149619340896606
Epoch 920, training loss: 627.1491088867188 = 0.48810428380966187 + 100.0 * 6.266610145568848
Epoch 920, val loss: 0.9135823845863342
Epoch 930, training loss: 626.8563842773438 = 0.47821542620658875 + 100.0 * 6.263782024383545
Epoch 930, val loss: 0.9126704335212708
Epoch 940, training loss: 626.7535400390625 = 0.4686548113822937 + 100.0 * 6.2628493309021
Epoch 940, val loss: 0.9119271636009216
Epoch 950, training loss: 627.2591552734375 = 0.45925313234329224 + 100.0 * 6.267999172210693
Epoch 950, val loss: 0.9115084409713745
Epoch 960, training loss: 626.7018432617188 = 0.4498898684978485 + 100.0 * 6.262519359588623
Epoch 960, val loss: 0.9109277129173279
Epoch 970, training loss: 626.553955078125 = 0.44077759981155396 + 100.0 * 6.261131763458252
Epoch 970, val loss: 0.9106844067573547
Epoch 980, training loss: 626.5323486328125 = 0.43184953927993774 + 100.0 * 6.261005401611328
Epoch 980, val loss: 0.9108042120933533
Epoch 990, training loss: 626.4896240234375 = 0.42312437295913696 + 100.0 * 6.260665416717529
Epoch 990, val loss: 0.9106476902961731
Epoch 1000, training loss: 626.5777587890625 = 0.4145156443119049 + 100.0 * 6.261632919311523
Epoch 1000, val loss: 0.9109237790107727
Epoch 1010, training loss: 626.2948608398438 = 0.4060414731502533 + 100.0 * 6.258887767791748
Epoch 1010, val loss: 0.911191463470459
Epoch 1020, training loss: 626.432861328125 = 0.39774569869041443 + 100.0 * 6.260351181030273
Epoch 1020, val loss: 0.9117465615272522
Epoch 1030, training loss: 626.0267333984375 = 0.3895741105079651 + 100.0 * 6.25637149810791
Epoch 1030, val loss: 0.9126287698745728
Epoch 1040, training loss: 626.0570068359375 = 0.3816326856613159 + 100.0 * 6.256753444671631
Epoch 1040, val loss: 0.9135368466377258
Epoch 1050, training loss: 626.344482421875 = 0.37382015585899353 + 100.0 * 6.259706497192383
Epoch 1050, val loss: 0.9145193099975586
Epoch 1060, training loss: 626.0757446289062 = 0.3660813868045807 + 100.0 * 6.257096767425537
Epoch 1060, val loss: 0.9154064655303955
Epoch 1070, training loss: 625.9569702148438 = 0.35852718353271484 + 100.0 * 6.255984783172607
Epoch 1070, val loss: 0.9166551232337952
Epoch 1080, training loss: 626.2594604492188 = 0.3511489927768707 + 100.0 * 6.259083271026611
Epoch 1080, val loss: 0.9180501699447632
Epoch 1090, training loss: 625.8580932617188 = 0.34377703070640564 + 100.0 * 6.255143165588379
Epoch 1090, val loss: 0.9198829531669617
Epoch 1100, training loss: 625.567626953125 = 0.3366504907608032 + 100.0 * 6.252309322357178
Epoch 1100, val loss: 0.9213540554046631
Epoch 1110, training loss: 625.8390502929688 = 0.3296980857849121 + 100.0 * 6.255093097686768
Epoch 1110, val loss: 0.923253059387207
Epoch 1120, training loss: 625.8043823242188 = 0.32269805669784546 + 100.0 * 6.254817008972168
Epoch 1120, val loss: 0.9251570701599121
Epoch 1130, training loss: 625.5587768554688 = 0.3158668279647827 + 100.0 * 6.252429008483887
Epoch 1130, val loss: 0.9267963767051697
Epoch 1140, training loss: 625.3423461914062 = 0.3092498183250427 + 100.0 * 6.250330924987793
Epoch 1140, val loss: 0.9291978478431702
Epoch 1150, training loss: 625.2361450195312 = 0.3028007745742798 + 100.0 * 6.249333381652832
Epoch 1150, val loss: 0.9316796660423279
Epoch 1160, training loss: 626.4353637695312 = 0.2965112328529358 + 100.0 * 6.261388301849365
Epoch 1160, val loss: 0.9337931871414185
Epoch 1170, training loss: 625.5106201171875 = 0.29007598757743835 + 100.0 * 6.2522053718566895
Epoch 1170, val loss: 0.9364891052246094
Epoch 1180, training loss: 625.068603515625 = 0.2838955521583557 + 100.0 * 6.247847557067871
Epoch 1180, val loss: 0.938967227935791
Epoch 1190, training loss: 625.5794677734375 = 0.27791959047317505 + 100.0 * 6.253015995025635
Epoch 1190, val loss: 0.941584050655365
Epoch 1200, training loss: 625.0123291015625 = 0.27196982502937317 + 100.0 * 6.247403621673584
Epoch 1200, val loss: 0.9443795680999756
Epoch 1210, training loss: 624.9010009765625 = 0.266154944896698 + 100.0 * 6.2463483810424805
Epoch 1210, val loss: 0.9472429752349854
Epoch 1220, training loss: 625.2894287109375 = 0.26053908467292786 + 100.0 * 6.250288486480713
Epoch 1220, val loss: 0.9500201344490051
Epoch 1230, training loss: 624.818603515625 = 0.25488218665122986 + 100.0 * 6.245636940002441
Epoch 1230, val loss: 0.953275203704834
Epoch 1240, training loss: 624.9666748046875 = 0.24940899014472961 + 100.0 * 6.247172832489014
Epoch 1240, val loss: 0.9563575983047485
Epoch 1250, training loss: 624.9443969726562 = 0.24401260912418365 + 100.0 * 6.24700403213501
Epoch 1250, val loss: 0.959538996219635
Epoch 1260, training loss: 624.8034057617188 = 0.2387673258781433 + 100.0 * 6.2456464767456055
Epoch 1260, val loss: 0.9622949957847595
Epoch 1270, training loss: 624.7138671875 = 0.23364248871803284 + 100.0 * 6.244802474975586
Epoch 1270, val loss: 0.9660529494285583
Epoch 1280, training loss: 625.1459350585938 = 0.22861211001873016 + 100.0 * 6.249172687530518
Epoch 1280, val loss: 0.9691173434257507
Epoch 1290, training loss: 624.5708618164062 = 0.22361348569393158 + 100.0 * 6.243472576141357
Epoch 1290, val loss: 0.972439706325531
Epoch 1300, training loss: 624.4368286132812 = 0.2187964767217636 + 100.0 * 6.242180347442627
Epoch 1300, val loss: 0.9760927557945251
Epoch 1310, training loss: 625.0111083984375 = 0.21411040425300598 + 100.0 * 6.247970104217529
Epoch 1310, val loss: 0.9795947074890137
Epoch 1320, training loss: 624.4143676757812 = 0.2094316929578781 + 100.0 * 6.242049694061279
Epoch 1320, val loss: 0.9830225110054016
Epoch 1330, training loss: 624.3543090820312 = 0.20489245653152466 + 100.0 * 6.241494178771973
Epoch 1330, val loss: 0.9864843487739563
Epoch 1340, training loss: 624.6795654296875 = 0.20049616694450378 + 100.0 * 6.244790554046631
Epoch 1340, val loss: 0.9902647733688354
Epoch 1350, training loss: 624.441650390625 = 0.1961042433977127 + 100.0 * 6.24245548248291
Epoch 1350, val loss: 0.99370276927948
Epoch 1360, training loss: 624.1871337890625 = 0.1918046921491623 + 100.0 * 6.23995304107666
Epoch 1360, val loss: 0.9975270628929138
Epoch 1370, training loss: 624.1060180664062 = 0.18766282498836517 + 100.0 * 6.23918342590332
Epoch 1370, val loss: 1.0014480352401733
Epoch 1380, training loss: 624.4142456054688 = 0.18364882469177246 + 100.0 * 6.242305755615234
Epoch 1380, val loss: 1.0053691864013672
Epoch 1390, training loss: 624.2105712890625 = 0.1796172559261322 + 100.0 * 6.240309238433838
Epoch 1390, val loss: 1.0090433359146118
Epoch 1400, training loss: 624.17236328125 = 0.17566074430942535 + 100.0 * 6.239967346191406
Epoch 1400, val loss: 1.0126829147338867
Epoch 1410, training loss: 623.981201171875 = 0.17186754941940308 + 100.0 * 6.238093376159668
Epoch 1410, val loss: 1.0167080163955688
Epoch 1420, training loss: 623.9808959960938 = 0.16818249225616455 + 100.0 * 6.238126754760742
Epoch 1420, val loss: 1.0209400653839111
Epoch 1430, training loss: 624.3507080078125 = 0.1645890772342682 + 100.0 * 6.241860866546631
Epoch 1430, val loss: 1.0248173475265503
Epoch 1440, training loss: 623.9114990234375 = 0.16098153591156006 + 100.0 * 6.237504959106445
Epoch 1440, val loss: 1.0287171602249146
Epoch 1450, training loss: 624.4908447265625 = 0.15752607583999634 + 100.0 * 6.243332862854004
Epoch 1450, val loss: 1.0329619646072388
Epoch 1460, training loss: 624.1993408203125 = 0.15403932332992554 + 100.0 * 6.240452766418457
Epoch 1460, val loss: 1.0368468761444092
Epoch 1470, training loss: 623.8359375 = 0.15069405734539032 + 100.0 * 6.236852169036865
Epoch 1470, val loss: 1.0406988859176636
Epoch 1480, training loss: 623.6744384765625 = 0.14748288691043854 + 100.0 * 6.235270023345947
Epoch 1480, val loss: 1.0450013875961304
Epoch 1490, training loss: 623.6593017578125 = 0.14435361325740814 + 100.0 * 6.235149383544922
Epoch 1490, val loss: 1.0494444370269775
Epoch 1500, training loss: 624.248779296875 = 0.14130617678165436 + 100.0 * 6.241075038909912
Epoch 1500, val loss: 1.0535258054733276
Epoch 1510, training loss: 624.0540771484375 = 0.13825368881225586 + 100.0 * 6.2391581535339355
Epoch 1510, val loss: 1.0573099851608276
Epoch 1520, training loss: 623.9567260742188 = 0.13526573777198792 + 100.0 * 6.23821496963501
Epoch 1520, val loss: 1.0616706609725952
Epoch 1530, training loss: 623.608154296875 = 0.1323748528957367 + 100.0 * 6.234757900238037
Epoch 1530, val loss: 1.0656449794769287
Epoch 1540, training loss: 623.7944946289062 = 0.12959063053131104 + 100.0 * 6.236649036407471
Epoch 1540, val loss: 1.0698045492172241
Epoch 1550, training loss: 623.4905395507812 = 0.12682679295539856 + 100.0 * 6.233636856079102
Epoch 1550, val loss: 1.0742043256759644
Epoch 1560, training loss: 623.5531005859375 = 0.12417125701904297 + 100.0 * 6.234289646148682
Epoch 1560, val loss: 1.0782279968261719
Epoch 1570, training loss: 624.0248413085938 = 0.12155479937791824 + 100.0 * 6.239032745361328
Epoch 1570, val loss: 1.082329511642456
Epoch 1580, training loss: 623.6133422851562 = 0.11894668638706207 + 100.0 * 6.234943866729736
Epoch 1580, val loss: 1.0867985486984253
Epoch 1590, training loss: 623.4663696289062 = 0.11646515130996704 + 100.0 * 6.233499050140381
Epoch 1590, val loss: 1.090912938117981
Epoch 1600, training loss: 623.5302734375 = 0.11403965950012207 + 100.0 * 6.2341628074646
Epoch 1600, val loss: 1.0953423976898193
Epoch 1610, training loss: 623.2022094726562 = 0.11166064441204071 + 100.0 * 6.230905532836914
Epoch 1610, val loss: 1.0994023084640503
Epoch 1620, training loss: 623.2705688476562 = 0.1093633621931076 + 100.0 * 6.231611728668213
Epoch 1620, val loss: 1.103837013244629
Epoch 1630, training loss: 624.012939453125 = 0.10711897164583206 + 100.0 * 6.239058494567871
Epoch 1630, val loss: 1.1077719926834106
Epoch 1640, training loss: 623.3880615234375 = 0.10481420159339905 + 100.0 * 6.232832908630371
Epoch 1640, val loss: 1.1121999025344849
Epoch 1650, training loss: 623.174560546875 = 0.10263171046972275 + 100.0 * 6.230719089508057
Epoch 1650, val loss: 1.1165231466293335
Epoch 1660, training loss: 623.0827026367188 = 0.10054343193769455 + 100.0 * 6.229821681976318
Epoch 1660, val loss: 1.1210039854049683
Epoch 1670, training loss: 623.0974731445312 = 0.09852512925863266 + 100.0 * 6.229989528656006
Epoch 1670, val loss: 1.1254490613937378
Epoch 1680, training loss: 623.6785888671875 = 0.09653491526842117 + 100.0 * 6.235820293426514
Epoch 1680, val loss: 1.129691243171692
Epoch 1690, training loss: 623.2380981445312 = 0.0945405587553978 + 100.0 * 6.231435298919678
Epoch 1690, val loss: 1.1338050365447998
Epoch 1700, training loss: 623.33251953125 = 0.09263361990451813 + 100.0 * 6.232398509979248
Epoch 1700, val loss: 1.1381257772445679
Epoch 1710, training loss: 622.9553833007812 = 0.0907452180981636 + 100.0 * 6.228646278381348
Epoch 1710, val loss: 1.1425554752349854
Epoch 1720, training loss: 623.6028442382812 = 0.08895577490329742 + 100.0 * 6.2351393699646
Epoch 1720, val loss: 1.1469050645828247
Epoch 1730, training loss: 623.0767822265625 = 0.08713532239198685 + 100.0 * 6.229896545410156
Epoch 1730, val loss: 1.1509360074996948
Epoch 1740, training loss: 623.0166015625 = 0.08537322282791138 + 100.0 * 6.229311943054199
Epoch 1740, val loss: 1.1554570198059082
Epoch 1750, training loss: 622.7657470703125 = 0.08368286490440369 + 100.0 * 6.226820468902588
Epoch 1750, val loss: 1.1596697568893433
Epoch 1760, training loss: 622.7366333007812 = 0.08205325156450272 + 100.0 * 6.226545810699463
Epoch 1760, val loss: 1.1642036437988281
Epoch 1770, training loss: 623.0181274414062 = 0.0804622694849968 + 100.0 * 6.229376792907715
Epoch 1770, val loss: 1.1685513257980347
Epoch 1780, training loss: 622.8782348632812 = 0.07886583358049393 + 100.0 * 6.227993488311768
Epoch 1780, val loss: 1.172624945640564
Epoch 1790, training loss: 622.6696166992188 = 0.0772981196641922 + 100.0 * 6.225923538208008
Epoch 1790, val loss: 1.1766209602355957
Epoch 1800, training loss: 623.0347290039062 = 0.07579757273197174 + 100.0 * 6.229589462280273
Epoch 1800, val loss: 1.181192398071289
Epoch 1810, training loss: 622.7017211914062 = 0.07430163770914078 + 100.0 * 6.226274490356445
Epoch 1810, val loss: 1.1851967573165894
Epoch 1820, training loss: 622.7240600585938 = 0.07287558913230896 + 100.0 * 6.2265119552612305
Epoch 1820, val loss: 1.1894252300262451
Epoch 1830, training loss: 622.8045654296875 = 0.07147868722677231 + 100.0 * 6.227330684661865
Epoch 1830, val loss: 1.1937917470932007
Epoch 1840, training loss: 622.6951293945312 = 0.0701117068529129 + 100.0 * 6.226250171661377
Epoch 1840, val loss: 1.1980299949645996
Epoch 1850, training loss: 622.5305786132812 = 0.06877709925174713 + 100.0 * 6.224617958068848
Epoch 1850, val loss: 1.2024835348129272
Epoch 1860, training loss: 623.3402709960938 = 0.06749230623245239 + 100.0 * 6.232727527618408
Epoch 1860, val loss: 1.2065984010696411
Epoch 1870, training loss: 622.7098999023438 = 0.06617901474237442 + 100.0 * 6.226437091827393
Epoch 1870, val loss: 1.2107137441635132
Epoch 1880, training loss: 622.5750122070312 = 0.06493118405342102 + 100.0 * 6.225100994110107
Epoch 1880, val loss: 1.2150013446807861
Epoch 1890, training loss: 622.5517578125 = 0.06372327357530594 + 100.0 * 6.224880218505859
Epoch 1890, val loss: 1.2190792560577393
Epoch 1900, training loss: 623.0198364257812 = 0.06255035847425461 + 100.0 * 6.229572772979736
Epoch 1900, val loss: 1.2228589057922363
Epoch 1910, training loss: 622.61181640625 = 0.06134933605790138 + 100.0 * 6.2255048751831055
Epoch 1910, val loss: 1.2274014949798584
Epoch 1920, training loss: 622.5782470703125 = 0.060205716639757156 + 100.0 * 6.225180625915527
Epoch 1920, val loss: 1.231321930885315
Epoch 1930, training loss: 622.3275756835938 = 0.059103332459926605 + 100.0 * 6.222684860229492
Epoch 1930, val loss: 1.2357561588287354
Epoch 1940, training loss: 622.3215942382812 = 0.05803533270955086 + 100.0 * 6.222635269165039
Epoch 1940, val loss: 1.2398264408111572
Epoch 1950, training loss: 622.652099609375 = 0.0569903589785099 + 100.0 * 6.225950717926025
Epoch 1950, val loss: 1.2440801858901978
Epoch 1960, training loss: 622.3358764648438 = 0.05595023185014725 + 100.0 * 6.222798824310303
Epoch 1960, val loss: 1.2480372190475464
Epoch 1970, training loss: 623.102294921875 = 0.05495379865169525 + 100.0 * 6.230473518371582
Epoch 1970, val loss: 1.2520010471343994
Epoch 1980, training loss: 622.5238037109375 = 0.05393844470381737 + 100.0 * 6.224698543548584
Epoch 1980, val loss: 1.255739688873291
Epoch 1990, training loss: 622.2503662109375 = 0.052962690591812134 + 100.0 * 6.221973896026611
Epoch 1990, val loss: 1.259771466255188
Epoch 2000, training loss: 622.1222534179688 = 0.052032675594091415 + 100.0 * 6.220702171325684
Epoch 2000, val loss: 1.2641657590866089
Epoch 2010, training loss: 622.1162719726562 = 0.05113375931978226 + 100.0 * 6.220651149749756
Epoch 2010, val loss: 1.2682600021362305
Epoch 2020, training loss: 622.6173095703125 = 0.0502520315349102 + 100.0 * 6.22567081451416
Epoch 2020, val loss: 1.2723196744918823
Epoch 2030, training loss: 622.6416015625 = 0.04936394467949867 + 100.0 * 6.225922107696533
Epoch 2030, val loss: 1.2755171060562134
Epoch 2040, training loss: 622.4022827148438 = 0.04847797751426697 + 100.0 * 6.223538398742676
Epoch 2040, val loss: 1.279878854751587
Epoch 2050, training loss: 622.2637329101562 = 0.047622133046388626 + 100.0 * 6.222161293029785
Epoch 2050, val loss: 1.2835297584533691
Epoch 2060, training loss: 622.0248413085938 = 0.04680350795388222 + 100.0 * 6.219780445098877
Epoch 2060, val loss: 1.2877144813537598
Epoch 2070, training loss: 622.58154296875 = 0.04603119194507599 + 100.0 * 6.22535514831543
Epoch 2070, val loss: 1.2913926839828491
Epoch 2080, training loss: 622.3211059570312 = 0.0452214740216732 + 100.0 * 6.222758769989014
Epoch 2080, val loss: 1.2956459522247314
Epoch 2090, training loss: 621.9531860351562 = 0.04444054886698723 + 100.0 * 6.219087600708008
Epoch 2090, val loss: 1.2992483377456665
Epoch 2100, training loss: 621.9625244140625 = 0.04369925707578659 + 100.0 * 6.219188213348389
Epoch 2100, val loss: 1.3034545183181763
Epoch 2110, training loss: 622.345458984375 = 0.042983245104551315 + 100.0 * 6.223024845123291
Epoch 2110, val loss: 1.3072211742401123
Epoch 2120, training loss: 622.3486938476562 = 0.04225767031311989 + 100.0 * 6.223064422607422
Epoch 2120, val loss: 1.3109779357910156
Epoch 2130, training loss: 621.947998046875 = 0.04154255613684654 + 100.0 * 6.219064235687256
Epoch 2130, val loss: 1.314543604850769
Epoch 2140, training loss: 621.7832641601562 = 0.040858570486307144 + 100.0 * 6.217424392700195
Epoch 2140, val loss: 1.318373680114746
Epoch 2150, training loss: 621.8255004882812 = 0.04019898548722267 + 100.0 * 6.217852592468262
Epoch 2150, val loss: 1.322251319885254
Epoch 2160, training loss: 622.4758911132812 = 0.039565373212099075 + 100.0 * 6.224363327026367
Epoch 2160, val loss: 1.325793743133545
Epoch 2170, training loss: 621.90234375 = 0.03890157490968704 + 100.0 * 6.218634605407715
Epoch 2170, val loss: 1.329412579536438
Epoch 2180, training loss: 621.9000854492188 = 0.03827032446861267 + 100.0 * 6.218618392944336
Epoch 2180, val loss: 1.333236813545227
Epoch 2190, training loss: 622.0726318359375 = 0.03765525296330452 + 100.0 * 6.2203497886657715
Epoch 2190, val loss: 1.3367574214935303
Epoch 2200, training loss: 621.8673095703125 = 0.03705402836203575 + 100.0 * 6.2183027267456055
Epoch 2200, val loss: 1.3402568101882935
Epoch 2210, training loss: 621.734130859375 = 0.036463651806116104 + 100.0 * 6.216977119445801
Epoch 2210, val loss: 1.3440501689910889
Epoch 2220, training loss: 621.7206420898438 = 0.035901859402656555 + 100.0 * 6.2168474197387695
Epoch 2220, val loss: 1.3476319313049316
Epoch 2230, training loss: 622.3037719726562 = 0.03535062447190285 + 100.0 * 6.222684383392334
Epoch 2230, val loss: 1.3513094186782837
Epoch 2240, training loss: 621.9625854492188 = 0.034780822694301605 + 100.0 * 6.219277858734131
Epoch 2240, val loss: 1.3549096584320068
Epoch 2250, training loss: 621.7071533203125 = 0.03423961251974106 + 100.0 * 6.216729164123535
Epoch 2250, val loss: 1.358235239982605
Epoch 2260, training loss: 621.7996215820312 = 0.03371681272983551 + 100.0 * 6.2176594734191895
Epoch 2260, val loss: 1.3619405031204224
Epoch 2270, training loss: 621.82421875 = 0.03320205584168434 + 100.0 * 6.217910289764404
Epoch 2270, val loss: 1.365192174911499
Epoch 2280, training loss: 621.6854248046875 = 0.032691873610019684 + 100.0 * 6.216526985168457
Epoch 2280, val loss: 1.3685815334320068
Epoch 2290, training loss: 621.8512573242188 = 0.03220463544130325 + 100.0 * 6.2181901931762695
Epoch 2290, val loss: 1.3720964193344116
Epoch 2300, training loss: 621.725830078125 = 0.031713712960481644 + 100.0 * 6.216940879821777
Epoch 2300, val loss: 1.3755769729614258
Epoch 2310, training loss: 621.702392578125 = 0.03124375268816948 + 100.0 * 6.216711521148682
Epoch 2310, val loss: 1.3790934085845947
Epoch 2320, training loss: 621.6530151367188 = 0.030778761953115463 + 100.0 * 6.216222763061523
Epoch 2320, val loss: 1.3823217153549194
Epoch 2330, training loss: 621.5567626953125 = 0.03032817877829075 + 100.0 * 6.215264320373535
Epoch 2330, val loss: 1.3857232332229614
Epoch 2340, training loss: 621.7471923828125 = 0.02989201992750168 + 100.0 * 6.217172622680664
Epoch 2340, val loss: 1.3891407251358032
Epoch 2350, training loss: 621.5037231445312 = 0.02944999746978283 + 100.0 * 6.214742660522461
Epoch 2350, val loss: 1.3925397396087646
Epoch 2360, training loss: 622.1478271484375 = 0.029024625197052956 + 100.0 * 6.221187591552734
Epoch 2360, val loss: 1.395927906036377
Epoch 2370, training loss: 621.615234375 = 0.02858734130859375 + 100.0 * 6.215866565704346
Epoch 2370, val loss: 1.3986214399337769
Epoch 2380, training loss: 621.4031372070312 = 0.028171280398964882 + 100.0 * 6.213749408721924
Epoch 2380, val loss: 1.4021670818328857
Epoch 2390, training loss: 621.2979125976562 = 0.027778077870607376 + 100.0 * 6.212701320648193
Epoch 2390, val loss: 1.4052610397338867
Epoch 2400, training loss: 621.4849243164062 = 0.027398601174354553 + 100.0 * 6.214575290679932
Epoch 2400, val loss: 1.4084465503692627
Epoch 2410, training loss: 621.8984375 = 0.027012856677174568 + 100.0 * 6.218713760375977
Epoch 2410, val loss: 1.4116771221160889
Epoch 2420, training loss: 621.568115234375 = 0.026609454303979874 + 100.0 * 6.215415000915527
Epoch 2420, val loss: 1.4148591756820679
Epoch 2430, training loss: 621.419921875 = 0.02623712085187435 + 100.0 * 6.213936805725098
Epoch 2430, val loss: 1.417911171913147
Epoch 2440, training loss: 621.4027709960938 = 0.025879379361867905 + 100.0 * 6.21376895904541
Epoch 2440, val loss: 1.421190857887268
Epoch 2450, training loss: 621.418701171875 = 0.02552914433181286 + 100.0 * 6.213932037353516
Epoch 2450, val loss: 1.4244130849838257
Epoch 2460, training loss: 621.532958984375 = 0.02518242970108986 + 100.0 * 6.2150774002075195
Epoch 2460, val loss: 1.427443265914917
Epoch 2470, training loss: 621.377685546875 = 0.024840595200657845 + 100.0 * 6.213528156280518
Epoch 2470, val loss: 1.4303871393203735
Epoch 2480, training loss: 621.2981567382812 = 0.024508077651262283 + 100.0 * 6.212736129760742
Epoch 2480, val loss: 1.433449149131775
Epoch 2490, training loss: 621.947998046875 = 0.02418806403875351 + 100.0 * 6.21923828125
Epoch 2490, val loss: 1.4362390041351318
Epoch 2500, training loss: 621.2757568359375 = 0.023847466334700584 + 100.0 * 6.21251916885376
Epoch 2500, val loss: 1.4394429922103882
Epoch 2510, training loss: 621.3831176757812 = 0.023535991087555885 + 100.0 * 6.213595867156982
Epoch 2510, val loss: 1.4425276517868042
Epoch 2520, training loss: 621.4200439453125 = 0.02322489768266678 + 100.0 * 6.213967800140381
Epoch 2520, val loss: 1.4453468322753906
Epoch 2530, training loss: 621.1302490234375 = 0.022914987057447433 + 100.0 * 6.211073875427246
Epoch 2530, val loss: 1.4483567476272583
Epoch 2540, training loss: 621.3011474609375 = 0.02262127585709095 + 100.0 * 6.212785243988037
Epoch 2540, val loss: 1.451505184173584
Epoch 2550, training loss: 621.4768676757812 = 0.022333744913339615 + 100.0 * 6.214545249938965
Epoch 2550, val loss: 1.4543390274047852
Epoch 2560, training loss: 621.2006225585938 = 0.022042563185095787 + 100.0 * 6.211785793304443
Epoch 2560, val loss: 1.4569255113601685
Epoch 2570, training loss: 621.3973999023438 = 0.021760864183306694 + 100.0 * 6.213756084442139
Epoch 2570, val loss: 1.459652066230774
Epoch 2580, training loss: 621.2319946289062 = 0.02148415520787239 + 100.0 * 6.2121052742004395
Epoch 2580, val loss: 1.4627621173858643
Epoch 2590, training loss: 621.3443603515625 = 0.0212127435952425 + 100.0 * 6.213231086730957
Epoch 2590, val loss: 1.465559482574463
Epoch 2600, training loss: 621.1744995117188 = 0.020942797884345055 + 100.0 * 6.211535930633545
Epoch 2600, val loss: 1.4681559801101685
Epoch 2610, training loss: 620.9942626953125 = 0.020676499232649803 + 100.0 * 6.209735870361328
Epoch 2610, val loss: 1.4712640047073364
Epoch 2620, training loss: 621.3045043945312 = 0.02042536623775959 + 100.0 * 6.212840557098389
Epoch 2620, val loss: 1.4742681980133057
Epoch 2630, training loss: 621.205810546875 = 0.020173925906419754 + 100.0 * 6.211856365203857
Epoch 2630, val loss: 1.4765982627868652
Epoch 2640, training loss: 621.272216796875 = 0.019924109801650047 + 100.0 * 6.212523460388184
Epoch 2640, val loss: 1.4793449640274048
Epoch 2650, training loss: 621.6036987304688 = 0.01967770792543888 + 100.0 * 6.2158403396606445
Epoch 2650, val loss: 1.4821614027023315
Epoch 2660, training loss: 621.0081787109375 = 0.019424766302108765 + 100.0 * 6.209887981414795
Epoch 2660, val loss: 1.4846125841140747
Epoch 2670, training loss: 620.9028930664062 = 0.019188635051250458 + 100.0 * 6.208837032318115
Epoch 2670, val loss: 1.487598180770874
Epoch 2680, training loss: 620.9326782226562 = 0.01896652765572071 + 100.0 * 6.209136962890625
Epoch 2680, val loss: 1.4901766777038574
Epoch 2690, training loss: 621.4627075195312 = 0.01875346153974533 + 100.0 * 6.214439868927002
Epoch 2690, val loss: 1.4930166006088257
Epoch 2700, training loss: 621.3615112304688 = 0.018521523103117943 + 100.0 * 6.213429927825928
Epoch 2700, val loss: 1.4951744079589844
Epoch 2710, training loss: 620.8966674804688 = 0.018289558589458466 + 100.0 * 6.2087836265563965
Epoch 2710, val loss: 1.4977755546569824
Epoch 2720, training loss: 620.8994750976562 = 0.018071698024868965 + 100.0 * 6.2088141441345215
Epoch 2720, val loss: 1.5003596544265747
Epoch 2730, training loss: 621.0967407226562 = 0.0178711898624897 + 100.0 * 6.210788726806641
Epoch 2730, val loss: 1.5030475854873657
Epoch 2740, training loss: 620.8514404296875 = 0.017657659947872162 + 100.0 * 6.208337783813477
Epoch 2740, val loss: 1.5055632591247559
Epoch 2750, training loss: 620.9362182617188 = 0.01745736226439476 + 100.0 * 6.2091875076293945
Epoch 2750, val loss: 1.5084247589111328
Epoch 2760, training loss: 621.2579956054688 = 0.017259500920772552 + 100.0 * 6.212407112121582
Epoch 2760, val loss: 1.5109200477600098
Epoch 2770, training loss: 621.0114135742188 = 0.017065756022930145 + 100.0 * 6.2099432945251465
Epoch 2770, val loss: 1.5130290985107422
Epoch 2780, training loss: 620.8970947265625 = 0.016866810619831085 + 100.0 * 6.208802700042725
Epoch 2780, val loss: 1.5153157711029053
Epoch 2790, training loss: 621.242431640625 = 0.016683364287018776 + 100.0 * 6.2122578620910645
Epoch 2790, val loss: 1.5181636810302734
Epoch 2800, training loss: 620.7942504882812 = 0.01648949831724167 + 100.0 * 6.207777500152588
Epoch 2800, val loss: 1.5201390981674194
Epoch 2810, training loss: 620.8602905273438 = 0.01630515232682228 + 100.0 * 6.208439826965332
Epoch 2810, val loss: 1.522520899772644
Epoch 2820, training loss: 620.8292846679688 = 0.016126329079270363 + 100.0 * 6.208131790161133
Epoch 2820, val loss: 1.524947166442871
Epoch 2830, training loss: 621.1076049804688 = 0.015954626724123955 + 100.0 * 6.210916996002197
Epoch 2830, val loss: 1.527179479598999
Epoch 2840, training loss: 620.749267578125 = 0.015769753605127335 + 100.0 * 6.207335472106934
Epoch 2840, val loss: 1.5297253131866455
Epoch 2850, training loss: 620.7694091796875 = 0.015600098296999931 + 100.0 * 6.207537651062012
Epoch 2850, val loss: 1.5319452285766602
Epoch 2860, training loss: 620.677734375 = 0.015436932444572449 + 100.0 * 6.206623077392578
Epoch 2860, val loss: 1.5341545343399048
Epoch 2870, training loss: 621.2723999023438 = 0.015278331004083157 + 100.0 * 6.212571620941162
Epoch 2870, val loss: 1.5362948179244995
Epoch 2880, training loss: 620.8934936523438 = 0.015109078027307987 + 100.0 * 6.2087836265563965
Epoch 2880, val loss: 1.5390671491622925
Epoch 2890, training loss: 620.8009643554688 = 0.014945924282073975 + 100.0 * 6.207859992980957
Epoch 2890, val loss: 1.5407594442367554
Epoch 2900, training loss: 620.7893676757812 = 0.014789397828280926 + 100.0 * 6.2077460289001465
Epoch 2900, val loss: 1.5433454513549805
Epoch 2910, training loss: 620.8093872070312 = 0.014633946120738983 + 100.0 * 6.207947254180908
Epoch 2910, val loss: 1.545535683631897
Epoch 2920, training loss: 620.5642700195312 = 0.01447964459657669 + 100.0 * 6.205497741699219
Epoch 2920, val loss: 1.5474950075149536
Epoch 2930, training loss: 620.7236938476562 = 0.014335410669445992 + 100.0 * 6.207093238830566
Epoch 2930, val loss: 1.5496840476989746
Epoch 2940, training loss: 621.1337280273438 = 0.014191652648150921 + 100.0 * 6.21119499206543
Epoch 2940, val loss: 1.5516148805618286
Epoch 2950, training loss: 620.9154663085938 = 0.014041195623576641 + 100.0 * 6.209014415740967
Epoch 2950, val loss: 1.5539219379425049
Epoch 2960, training loss: 620.7467651367188 = 0.013890010304749012 + 100.0 * 6.2073283195495605
Epoch 2960, val loss: 1.5559560060501099
Epoch 2970, training loss: 620.9236450195312 = 0.013755081221461296 + 100.0 * 6.209098815917969
Epoch 2970, val loss: 1.5583313703536987
Epoch 2980, training loss: 620.5465087890625 = 0.013614282943308353 + 100.0 * 6.205328941345215
Epoch 2980, val loss: 1.5599583387374878
Epoch 2990, training loss: 620.5457153320312 = 0.013481540605425835 + 100.0 * 6.205322265625
Epoch 2990, val loss: 1.562231421470642
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.7907221929362152
The final CL Acc:0.66049, 0.02014, The final GNN Acc:0.79318, 0.00277
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13170])
remove edge: torch.Size([2, 8022])
updated graph: torch.Size([2, 10636])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.633544921875 = 1.9501101970672607 + 100.0 * 8.596834182739258
Epoch 0, val loss: 1.9480782747268677
Epoch 10, training loss: 861.5478515625 = 1.940773606300354 + 100.0 * 8.596070289611816
Epoch 10, val loss: 1.938368558883667
Epoch 20, training loss: 860.9929809570312 = 1.9290755987167358 + 100.0 * 8.590639114379883
Epoch 20, val loss: 1.9262322187423706
Epoch 30, training loss: 857.280029296875 = 1.9137016534805298 + 100.0 * 8.55366325378418
Epoch 30, val loss: 1.9103633165359497
Epoch 40, training loss: 836.97216796875 = 1.894424319267273 + 100.0 * 8.350777626037598
Epoch 40, val loss: 1.890920877456665
Epoch 50, training loss: 795.4043579101562 = 1.8713080883026123 + 100.0 * 7.935330390930176
Epoch 50, val loss: 1.868157982826233
Epoch 60, training loss: 755.2666625976562 = 1.8539775609970093 + 100.0 * 7.5341267585754395
Epoch 60, val loss: 1.8522368669509888
Epoch 70, training loss: 722.1954345703125 = 1.8453404903411865 + 100.0 * 7.203501224517822
Epoch 70, val loss: 1.8439242839813232
Epoch 80, training loss: 703.9714965820312 = 1.8371893167495728 + 100.0 * 7.021342754364014
Epoch 80, val loss: 1.8356488943099976
Epoch 90, training loss: 690.15234375 = 1.8260234594345093 + 100.0 * 6.883262634277344
Epoch 90, val loss: 1.8250848054885864
Epoch 100, training loss: 679.8548583984375 = 1.8157932758331299 + 100.0 * 6.780390739440918
Epoch 100, val loss: 1.8153120279312134
Epoch 110, training loss: 672.6441040039062 = 1.8065845966339111 + 100.0 * 6.708375453948975
Epoch 110, val loss: 1.8063045740127563
Epoch 120, training loss: 667.9318237304688 = 1.7972601652145386 + 100.0 * 6.661345958709717
Epoch 120, val loss: 1.7972955703735352
Epoch 130, training loss: 664.2069702148438 = 1.7874611616134644 + 100.0 * 6.624195098876953
Epoch 130, val loss: 1.7880585193634033
Epoch 140, training loss: 661.2482299804688 = 1.7773932218551636 + 100.0 * 6.594708442687988
Epoch 140, val loss: 1.7787038087844849
Epoch 150, training loss: 659.177490234375 = 1.7671473026275635 + 100.0 * 6.574103355407715
Epoch 150, val loss: 1.7691597938537598
Epoch 160, training loss: 656.7975463867188 = 1.7564338445663452 + 100.0 * 6.550411224365234
Epoch 160, val loss: 1.759149193763733
Epoch 170, training loss: 654.8759765625 = 1.7449601888656616 + 100.0 * 6.531310558319092
Epoch 170, val loss: 1.7485626935958862
Epoch 180, training loss: 653.1192016601562 = 1.7328264713287354 + 100.0 * 6.513863563537598
Epoch 180, val loss: 1.7372816801071167
Epoch 190, training loss: 651.8966064453125 = 1.7197539806365967 + 100.0 * 6.501769065856934
Epoch 190, val loss: 1.7252451181411743
Epoch 200, training loss: 650.2826538085938 = 1.705659031867981 + 100.0 * 6.485769748687744
Epoch 200, val loss: 1.7122637033462524
Epoch 210, training loss: 648.8797607421875 = 1.6905112266540527 + 100.0 * 6.471892833709717
Epoch 210, val loss: 1.6983838081359863
Epoch 220, training loss: 647.7197875976562 = 1.6743117570877075 + 100.0 * 6.460454940795898
Epoch 220, val loss: 1.6835548877716064
Epoch 230, training loss: 647.0133666992188 = 1.6568979024887085 + 100.0 * 6.453564167022705
Epoch 230, val loss: 1.6676944494247437
Epoch 240, training loss: 645.9371337890625 = 1.6384410858154297 + 100.0 * 6.442986965179443
Epoch 240, val loss: 1.6507470607757568
Epoch 250, training loss: 644.8268432617188 = 1.6189838647842407 + 100.0 * 6.432078838348389
Epoch 250, val loss: 1.6329199075698853
Epoch 260, training loss: 643.981689453125 = 1.5985740423202515 + 100.0 * 6.423830986022949
Epoch 260, val loss: 1.6142395734786987
Epoch 270, training loss: 643.47607421875 = 1.5771770477294922 + 100.0 * 6.418989181518555
Epoch 270, val loss: 1.594611406326294
Epoch 280, training loss: 642.5009765625 = 1.555005669593811 + 100.0 * 6.409460067749023
Epoch 280, val loss: 1.5742511749267578
Epoch 290, training loss: 641.7152099609375 = 1.5321686267852783 + 100.0 * 6.401830196380615
Epoch 290, val loss: 1.5533380508422852
Epoch 300, training loss: 641.6819458007812 = 1.508713722229004 + 100.0 * 6.401731967926025
Epoch 300, val loss: 1.5319191217422485
Epoch 310, training loss: 640.5642700195312 = 1.484617829322815 + 100.0 * 6.390796661376953
Epoch 310, val loss: 1.5099605321884155
Epoch 320, training loss: 639.9249267578125 = 1.4602006673812866 + 100.0 * 6.384647369384766
Epoch 320, val loss: 1.487843632698059
Epoch 330, training loss: 639.2714233398438 = 1.4354593753814697 + 100.0 * 6.378359317779541
Epoch 330, val loss: 1.4655125141143799
Epoch 340, training loss: 639.1182250976562 = 1.4104090929031372 + 100.0 * 6.377078533172607
Epoch 340, val loss: 1.4429702758789062
Epoch 350, training loss: 638.3104248046875 = 1.3851162195205688 + 100.0 * 6.369253158569336
Epoch 350, val loss: 1.4204052686691284
Epoch 360, training loss: 637.8424072265625 = 1.3595938682556152 + 100.0 * 6.364827632904053
Epoch 360, val loss: 1.3977012634277344
Epoch 370, training loss: 637.5336303710938 = 1.3338253498077393 + 100.0 * 6.361998081207275
Epoch 370, val loss: 1.3750258684158325
Epoch 380, training loss: 636.9236450195312 = 1.3078246116638184 + 100.0 * 6.35615873336792
Epoch 380, val loss: 1.35219407081604
Epoch 390, training loss: 636.5170288085938 = 1.2817293405532837 + 100.0 * 6.352352619171143
Epoch 390, val loss: 1.329577088356018
Epoch 400, training loss: 636.3276977539062 = 1.2554079294204712 + 100.0 * 6.350722789764404
Epoch 400, val loss: 1.306741714477539
Epoch 410, training loss: 635.7166137695312 = 1.2289738655090332 + 100.0 * 6.344876289367676
Epoch 410, val loss: 1.2840379476547241
Epoch 420, training loss: 635.335205078125 = 1.2025187015533447 + 100.0 * 6.341326713562012
Epoch 420, val loss: 1.261518955230713
Epoch 430, training loss: 635.4595336914062 = 1.1762194633483887 + 100.0 * 6.342833042144775
Epoch 430, val loss: 1.239302635192871
Epoch 440, training loss: 635.0534057617188 = 1.1496667861938477 + 100.0 * 6.3390374183654785
Epoch 440, val loss: 1.2169580459594727
Epoch 450, training loss: 634.4673461914062 = 1.1234835386276245 + 100.0 * 6.333438873291016
Epoch 450, val loss: 1.195185661315918
Epoch 460, training loss: 634.0878295898438 = 1.0977274179458618 + 100.0 * 6.329901218414307
Epoch 460, val loss: 1.1739890575408936
Epoch 470, training loss: 633.8208618164062 = 1.0724135637283325 + 100.0 * 6.327484607696533
Epoch 470, val loss: 1.1533381938934326
Epoch 480, training loss: 634.1763916015625 = 1.0472261905670166 + 100.0 * 6.331291675567627
Epoch 480, val loss: 1.1334906816482544
Epoch 490, training loss: 633.4382934570312 = 1.0228725671768188 + 100.0 * 6.324153900146484
Epoch 490, val loss: 1.113995909690857
Epoch 500, training loss: 632.971435546875 = 0.9991543292999268 + 100.0 * 6.319723129272461
Epoch 500, val loss: 1.095490574836731
Epoch 510, training loss: 632.704345703125 = 0.9762190580368042 + 100.0 * 6.317281723022461
Epoch 510, val loss: 1.0780737400054932
Epoch 520, training loss: 632.6443481445312 = 0.9540101885795593 + 100.0 * 6.316903114318848
Epoch 520, val loss: 1.061596393585205
Epoch 530, training loss: 633.2716064453125 = 0.932120680809021 + 100.0 * 6.323394775390625
Epoch 530, val loss: 1.0456185340881348
Epoch 540, training loss: 632.36279296875 = 0.9112662672996521 + 100.0 * 6.314515590667725
Epoch 540, val loss: 1.030861258506775
Epoch 550, training loss: 631.880859375 = 0.8911375999450684 + 100.0 * 6.309897422790527
Epoch 550, val loss: 1.017065405845642
Epoch 560, training loss: 631.6173706054688 = 0.8717896342277527 + 100.0 * 6.307456016540527
Epoch 560, val loss: 1.0041435956954956
Epoch 570, training loss: 631.759033203125 = 0.8531267046928406 + 100.0 * 6.309058666229248
Epoch 570, val loss: 0.9919853210449219
Epoch 580, training loss: 631.5491333007812 = 0.8350030779838562 + 100.0 * 6.307140827178955
Epoch 580, val loss: 0.98078852891922
Epoch 590, training loss: 631.0319213867188 = 0.8175080418586731 + 100.0 * 6.3021440505981445
Epoch 590, val loss: 0.9701655507087708
Epoch 600, training loss: 632.1151123046875 = 0.800597071647644 + 100.0 * 6.313145160675049
Epoch 600, val loss: 0.960162341594696
Epoch 610, training loss: 630.7357177734375 = 0.784358561038971 + 100.0 * 6.299513816833496
Epoch 610, val loss: 0.9510324001312256
Epoch 620, training loss: 630.5191650390625 = 0.7687141299247742 + 100.0 * 6.297504425048828
Epoch 620, val loss: 0.9427441358566284
Epoch 630, training loss: 630.3377075195312 = 0.7536436915397644 + 100.0 * 6.295840740203857
Epoch 630, val loss: 0.9351263642311096
Epoch 640, training loss: 630.3609008789062 = 0.7390779852867126 + 100.0 * 6.296218395233154
Epoch 640, val loss: 0.9280155897140503
Epoch 650, training loss: 630.708740234375 = 0.7248352766036987 + 100.0 * 6.299839019775391
Epoch 650, val loss: 0.9213284850120544
Epoch 660, training loss: 630.060302734375 = 0.7110543847084045 + 100.0 * 6.293492317199707
Epoch 660, val loss: 0.9152023196220398
Epoch 670, training loss: 629.7747192382812 = 0.6978541612625122 + 100.0 * 6.290768623352051
Epoch 670, val loss: 0.9096019268035889
Epoch 680, training loss: 629.5756225585938 = 0.6851300597190857 + 100.0 * 6.288905143737793
Epoch 680, val loss: 0.9046790599822998
Epoch 690, training loss: 630.037353515625 = 0.6727803349494934 + 100.0 * 6.29364538192749
Epoch 690, val loss: 0.9002395868301392
Epoch 700, training loss: 629.5089721679688 = 0.6606194972991943 + 100.0 * 6.288483142852783
Epoch 700, val loss: 0.8960285782814026
Epoch 710, training loss: 629.6375122070312 = 0.6489096283912659 + 100.0 * 6.289885997772217
Epoch 710, val loss: 0.8924781084060669
Epoch 720, training loss: 629.2340087890625 = 0.637445330619812 + 100.0 * 6.285965442657471
Epoch 720, val loss: 0.88877934217453
Epoch 730, training loss: 629.1244506835938 = 0.6262716054916382 + 100.0 * 6.284981727600098
Epoch 730, val loss: 0.8859915137290955
Epoch 740, training loss: 628.79248046875 = 0.6154994964599609 + 100.0 * 6.281769752502441
Epoch 740, val loss: 0.8833935856819153
Epoch 750, training loss: 628.7703857421875 = 0.6049678325653076 + 100.0 * 6.281653881072998
Epoch 750, val loss: 0.8812631368637085
Epoch 760, training loss: 628.9728393554688 = 0.5945714712142944 + 100.0 * 6.283782958984375
Epoch 760, val loss: 0.8790744543075562
Epoch 770, training loss: 628.5302124023438 = 0.5844706892967224 + 100.0 * 6.2794575691223145
Epoch 770, val loss: 0.8776760697364807
Epoch 780, training loss: 628.5098266601562 = 0.5745575428009033 + 100.0 * 6.279352188110352
Epoch 780, val loss: 0.8760371208190918
Epoch 790, training loss: 628.2152709960938 = 0.5648939609527588 + 100.0 * 6.276504039764404
Epoch 790, val loss: 0.87501460313797
Epoch 800, training loss: 628.187744140625 = 0.5554661154747009 + 100.0 * 6.276322364807129
Epoch 800, val loss: 0.8741418719291687
Epoch 810, training loss: 628.0767822265625 = 0.5461412668228149 + 100.0 * 6.275306701660156
Epoch 810, val loss: 0.8734943270683289
Epoch 820, training loss: 628.1154174804688 = 0.5369710326194763 + 100.0 * 6.275784492492676
Epoch 820, val loss: 0.872980535030365
Epoch 830, training loss: 627.850341796875 = 0.5279424786567688 + 100.0 * 6.273223876953125
Epoch 830, val loss: 0.8725768327713013
Epoch 840, training loss: 627.6857299804688 = 0.5191048979759216 + 100.0 * 6.271666526794434
Epoch 840, val loss: 0.8724307417869568
Epoch 850, training loss: 628.56591796875 = 0.5103709697723389 + 100.0 * 6.280555248260498
Epoch 850, val loss: 0.8724480867385864
Epoch 860, training loss: 627.5525512695312 = 0.5017589926719666 + 100.0 * 6.2705078125
Epoch 860, val loss: 0.8729627132415771
Epoch 870, training loss: 627.3213500976562 = 0.49327534437179565 + 100.0 * 6.268280506134033
Epoch 870, val loss: 0.8734203577041626
Epoch 880, training loss: 627.2212524414062 = 0.4849497675895691 + 100.0 * 6.26736307144165
Epoch 880, val loss: 0.8740530014038086
Epoch 890, training loss: 627.4260864257812 = 0.476762980222702 + 100.0 * 6.269493579864502
Epoch 890, val loss: 0.8748420476913452
Epoch 900, training loss: 627.19140625 = 0.4685831665992737 + 100.0 * 6.267228126525879
Epoch 900, val loss: 0.8759028911590576
Epoch 910, training loss: 627.2887573242188 = 0.46047326922416687 + 100.0 * 6.268283367156982
Epoch 910, val loss: 0.8769903182983398
Epoch 920, training loss: 627.2775268554688 = 0.4525086283683777 + 100.0 * 6.268249988555908
Epoch 920, val loss: 0.878450334072113
Epoch 930, training loss: 627.0390625 = 0.44462016224861145 + 100.0 * 6.265944480895996
Epoch 930, val loss: 0.8796254992485046
Epoch 940, training loss: 626.7119750976562 = 0.43682360649108887 + 100.0 * 6.262751579284668
Epoch 940, val loss: 0.8814764618873596
Epoch 950, training loss: 626.7521362304688 = 0.42913034558296204 + 100.0 * 6.263229846954346
Epoch 950, val loss: 0.8832735419273376
Epoch 960, training loss: 626.5283203125 = 0.42146673798561096 + 100.0 * 6.261068820953369
Epoch 960, val loss: 0.8850811123847961
Epoch 970, training loss: 627.1370849609375 = 0.4138907790184021 + 100.0 * 6.2672319412231445
Epoch 970, val loss: 0.8871155977249146
Epoch 980, training loss: 626.7792358398438 = 0.40626516938209534 + 100.0 * 6.263729572296143
Epoch 980, val loss: 0.8888745903968811
Epoch 990, training loss: 626.52392578125 = 0.39880239963531494 + 100.0 * 6.261251449584961
Epoch 990, val loss: 0.8914104104042053
Epoch 1000, training loss: 626.2376708984375 = 0.39144033193588257 + 100.0 * 6.258462429046631
Epoch 1000, val loss: 0.8938490152359009
Epoch 1010, training loss: 626.1380004882812 = 0.3841741681098938 + 100.0 * 6.257538318634033
Epoch 1010, val loss: 0.896350622177124
Epoch 1020, training loss: 626.4205932617188 = 0.37699204683303833 + 100.0 * 6.260436058044434
Epoch 1020, val loss: 0.8990145921707153
Epoch 1030, training loss: 626.3886108398438 = 0.3698357045650482 + 100.0 * 6.260188102722168
Epoch 1030, val loss: 0.9018521904945374
Epoch 1040, training loss: 626.126708984375 = 0.36272910237312317 + 100.0 * 6.2576398849487305
Epoch 1040, val loss: 0.904690146446228
Epoch 1050, training loss: 625.8956909179688 = 0.3557630777359009 + 100.0 * 6.255399227142334
Epoch 1050, val loss: 0.9078313112258911
Epoch 1060, training loss: 625.8507690429688 = 0.348917156457901 + 100.0 * 6.25501823425293
Epoch 1060, val loss: 0.9109247922897339
Epoch 1070, training loss: 626.0377197265625 = 0.3421463072299957 + 100.0 * 6.256955623626709
Epoch 1070, val loss: 0.9142254590988159
Epoch 1080, training loss: 625.7886352539062 = 0.3353947103023529 + 100.0 * 6.254532337188721
Epoch 1080, val loss: 0.917222797870636
Epoch 1090, training loss: 625.8319091796875 = 0.32880541682243347 + 100.0 * 6.255031108856201
Epoch 1090, val loss: 0.9208992123603821
Epoch 1100, training loss: 625.6249389648438 = 0.322250097990036 + 100.0 * 6.253026962280273
Epoch 1100, val loss: 0.9238927364349365
Epoch 1110, training loss: 625.4803466796875 = 0.315822035074234 + 100.0 * 6.251645565032959
Epoch 1110, val loss: 0.9275816679000854
Epoch 1120, training loss: 625.9354248046875 = 0.3095149099826813 + 100.0 * 6.256258964538574
Epoch 1120, val loss: 0.9311793446540833
Epoch 1130, training loss: 625.5833129882812 = 0.3032234013080597 + 100.0 * 6.252800941467285
Epoch 1130, val loss: 0.9349159598350525
Epoch 1140, training loss: 625.3457641601562 = 0.29706859588623047 + 100.0 * 6.250486850738525
Epoch 1140, val loss: 0.9386906027793884
Epoch 1150, training loss: 625.3510131835938 = 0.2910503149032593 + 100.0 * 6.250599384307861
Epoch 1150, val loss: 0.9426684379577637
Epoch 1160, training loss: 625.465087890625 = 0.28509321808815 + 100.0 * 6.251800060272217
Epoch 1160, val loss: 0.9462195634841919
Epoch 1170, training loss: 625.1351318359375 = 0.2792261242866516 + 100.0 * 6.24855899810791
Epoch 1170, val loss: 0.9501078128814697
Epoch 1180, training loss: 624.9953002929688 = 0.2735201418399811 + 100.0 * 6.247218132019043
Epoch 1180, val loss: 0.9543521404266357
Epoch 1190, training loss: 625.029052734375 = 0.2679329812526703 + 100.0 * 6.247611045837402
Epoch 1190, val loss: 0.9583881497383118
Epoch 1200, training loss: 625.545166015625 = 0.26245948672294617 + 100.0 * 6.252827167510986
Epoch 1200, val loss: 0.9627668857574463
Epoch 1210, training loss: 625.1747436523438 = 0.2569924294948578 + 100.0 * 6.2491774559021
Epoch 1210, val loss: 0.9663243889808655
Epoch 1220, training loss: 624.7634887695312 = 0.25167402625083923 + 100.0 * 6.245118618011475
Epoch 1220, val loss: 0.9707167148590088
Epoch 1230, training loss: 624.780029296875 = 0.24650412797927856 + 100.0 * 6.245335102081299
Epoch 1230, val loss: 0.9750248789787292
Epoch 1240, training loss: 625.1193237304688 = 0.24143776297569275 + 100.0 * 6.248778820037842
Epoch 1240, val loss: 0.9789648056030273
Epoch 1250, training loss: 624.5879516601562 = 0.23647812008857727 + 100.0 * 6.2435150146484375
Epoch 1250, val loss: 0.983532726764679
Epoch 1260, training loss: 624.9740600585938 = 0.2316383719444275 + 100.0 * 6.247424125671387
Epoch 1260, val loss: 0.9878696203231812
Epoch 1270, training loss: 624.5982666015625 = 0.22681215405464172 + 100.0 * 6.243714809417725
Epoch 1270, val loss: 0.9914434552192688
Epoch 1280, training loss: 624.4854736328125 = 0.2221611887216568 + 100.0 * 6.242632865905762
Epoch 1280, val loss: 0.9959238767623901
Epoch 1290, training loss: 624.7623901367188 = 0.2176261842250824 + 100.0 * 6.245447158813477
Epoch 1290, val loss: 1.0002504587173462
Epoch 1300, training loss: 624.8079223632812 = 0.2131190001964569 + 100.0 * 6.24594783782959
Epoch 1300, val loss: 1.0035830736160278
Epoch 1310, training loss: 624.57080078125 = 0.20874659717082977 + 100.0 * 6.2436203956604
Epoch 1310, val loss: 1.0086681842803955
Epoch 1320, training loss: 624.3623657226562 = 0.20442849397659302 + 100.0 * 6.241579532623291
Epoch 1320, val loss: 1.0124021768569946
Epoch 1330, training loss: 624.3444213867188 = 0.2002580761909485 + 100.0 * 6.24144172668457
Epoch 1330, val loss: 1.016997218132019
Epoch 1340, training loss: 624.4886474609375 = 0.1961575448513031 + 100.0 * 6.242924690246582
Epoch 1340, val loss: 1.0208418369293213
Epoch 1350, training loss: 624.2435913085938 = 0.19211648404598236 + 100.0 * 6.240514278411865
Epoch 1350, val loss: 1.0247656106948853
Epoch 1360, training loss: 624.096923828125 = 0.18816258013248444 + 100.0 * 6.2390875816345215
Epoch 1360, val loss: 1.029234766960144
Epoch 1370, training loss: 624.6839599609375 = 0.18430078029632568 + 100.0 * 6.244996070861816
Epoch 1370, val loss: 1.0333325862884521
Epoch 1380, training loss: 624.2216796875 = 0.1804724931716919 + 100.0 * 6.24041223526001
Epoch 1380, val loss: 1.0371001958847046
Epoch 1390, training loss: 624.0161743164062 = 0.17673346400260925 + 100.0 * 6.238394260406494
Epoch 1390, val loss: 1.0415793657302856
Epoch 1400, training loss: 624.0003662109375 = 0.17310231924057007 + 100.0 * 6.238272666931152
Epoch 1400, val loss: 1.0451982021331787
Epoch 1410, training loss: 624.119140625 = 0.1695309579372406 + 100.0 * 6.239495754241943
Epoch 1410, val loss: 1.0493042469024658
Epoch 1420, training loss: 623.9068603515625 = 0.16604121029376984 + 100.0 * 6.237408638000488
Epoch 1420, val loss: 1.0534043312072754
Epoch 1430, training loss: 624.1142578125 = 0.16262125968933105 + 100.0 * 6.239516735076904
Epoch 1430, val loss: 1.057325839996338
Epoch 1440, training loss: 623.9382934570312 = 0.1592346876859665 + 100.0 * 6.237790584564209
Epoch 1440, val loss: 1.0612759590148926
Epoch 1450, training loss: 623.6759643554688 = 0.15593326091766357 + 100.0 * 6.235199928283691
Epoch 1450, val loss: 1.065435528755188
Epoch 1460, training loss: 623.6349487304688 = 0.15270590782165527 + 100.0 * 6.2348222732543945
Epoch 1460, val loss: 1.069170594215393
Epoch 1470, training loss: 623.8219604492188 = 0.14957104623317719 + 100.0 * 6.23672342300415
Epoch 1470, val loss: 1.073274850845337
Epoch 1480, training loss: 623.752685546875 = 0.14649143815040588 + 100.0 * 6.236062049865723
Epoch 1480, val loss: 1.07694673538208
Epoch 1490, training loss: 623.7980346679688 = 0.1434597373008728 + 100.0 * 6.236546039581299
Epoch 1490, val loss: 1.0803943872451782
Epoch 1500, training loss: 623.8125610351562 = 0.14048120379447937 + 100.0 * 6.236720561981201
Epoch 1500, val loss: 1.0844680070877075
Epoch 1510, training loss: 623.886474609375 = 0.1375756412744522 + 100.0 * 6.237489223480225
Epoch 1510, val loss: 1.0882598161697388
Epoch 1520, training loss: 623.5468139648438 = 0.1347063034772873 + 100.0 * 6.234120845794678
Epoch 1520, val loss: 1.0924627780914307
Epoch 1530, training loss: 623.382568359375 = 0.13192221522331238 + 100.0 * 6.23250675201416
Epoch 1530, val loss: 1.0964339971542358
Epoch 1540, training loss: 623.3068237304688 = 0.1292087882757187 + 100.0 * 6.231776237487793
Epoch 1540, val loss: 1.1002782583236694
Epoch 1550, training loss: 623.4948120117188 = 0.12655486166477203 + 100.0 * 6.233682632446289
Epoch 1550, val loss: 1.1040420532226562
Epoch 1560, training loss: 623.4608764648438 = 0.12394250184297562 + 100.0 * 6.23336935043335
Epoch 1560, val loss: 1.1077229976654053
Epoch 1570, training loss: 623.6771240234375 = 0.12136735022068024 + 100.0 * 6.235557556152344
Epoch 1570, val loss: 1.1119961738586426
Epoch 1580, training loss: 623.32568359375 = 0.11884942650794983 + 100.0 * 6.2320685386657715
Epoch 1580, val loss: 1.1160211563110352
Epoch 1590, training loss: 623.1908569335938 = 0.1163863092660904 + 100.0 * 6.230744361877441
Epoch 1590, val loss: 1.1199755668640137
Epoch 1600, training loss: 623.2325439453125 = 0.11400287598371506 + 100.0 * 6.231185436248779
Epoch 1600, val loss: 1.123822569847107
Epoch 1610, training loss: 623.7156372070312 = 0.11167263239622116 + 100.0 * 6.236039638519287
Epoch 1610, val loss: 1.1280933618545532
Epoch 1620, training loss: 623.2792358398438 = 0.10934046655893326 + 100.0 * 6.231698989868164
Epoch 1620, val loss: 1.1319798231124878
Epoch 1630, training loss: 622.98291015625 = 0.10709592700004578 + 100.0 * 6.228757858276367
Epoch 1630, val loss: 1.1357802152633667
Epoch 1640, training loss: 623.12841796875 = 0.10490667074918747 + 100.0 * 6.2302350997924805
Epoch 1640, val loss: 1.1397082805633545
Epoch 1650, training loss: 623.3473510742188 = 0.10274066030979156 + 100.0 * 6.232446193695068
Epoch 1650, val loss: 1.143707036972046
Epoch 1660, training loss: 623.2214965820312 = 0.10061068087816238 + 100.0 * 6.2312092781066895
Epoch 1660, val loss: 1.1482371091842651
Epoch 1670, training loss: 623.0149536132812 = 0.09853429347276688 + 100.0 * 6.2291646003723145
Epoch 1670, val loss: 1.1517536640167236
Epoch 1680, training loss: 623.1608276367188 = 0.09651001542806625 + 100.0 * 6.230643272399902
Epoch 1680, val loss: 1.1557694673538208
Epoch 1690, training loss: 623.128173828125 = 0.09451120346784592 + 100.0 * 6.230337142944336
Epoch 1690, val loss: 1.159244179725647
Epoch 1700, training loss: 622.8733520507812 = 0.0925520583987236 + 100.0 * 6.227808475494385
Epoch 1700, val loss: 1.1639131307601929
Epoch 1710, training loss: 622.9725952148438 = 0.09066025167703629 + 100.0 * 6.228819847106934
Epoch 1710, val loss: 1.1677840948104858
Epoch 1720, training loss: 622.9635620117188 = 0.08880319446325302 + 100.0 * 6.228747844696045
Epoch 1720, val loss: 1.171732783317566
Epoch 1730, training loss: 622.773193359375 = 0.08699353039264679 + 100.0 * 6.22686243057251
Epoch 1730, val loss: 1.1760704517364502
Epoch 1740, training loss: 622.906494140625 = 0.08523737639188766 + 100.0 * 6.228212356567383
Epoch 1740, val loss: 1.1803936958312988
Epoch 1750, training loss: 623.1705932617188 = 0.08350133150815964 + 100.0 * 6.230871200561523
Epoch 1750, val loss: 1.184348225593567
Epoch 1760, training loss: 622.74609375 = 0.08177165687084198 + 100.0 * 6.226643085479736
Epoch 1760, val loss: 1.1884900331497192
Epoch 1770, training loss: 622.662841796875 = 0.08012223243713379 + 100.0 * 6.225826740264893
Epoch 1770, val loss: 1.1924848556518555
Epoch 1780, training loss: 622.9440307617188 = 0.07850618660449982 + 100.0 * 6.2286553382873535
Epoch 1780, val loss: 1.1969047784805298
Epoch 1790, training loss: 622.73828125 = 0.0769038274884224 + 100.0 * 6.226613521575928
Epoch 1790, val loss: 1.2008886337280273
Epoch 1800, training loss: 622.7725219726562 = 0.07534359395503998 + 100.0 * 6.2269721031188965
Epoch 1800, val loss: 1.2049102783203125
Epoch 1810, training loss: 622.6790161132812 = 0.0738244503736496 + 100.0 * 6.2260518074035645
Epoch 1810, val loss: 1.2091130018234253
Epoch 1820, training loss: 623.0140380859375 = 0.07233985513448715 + 100.0 * 6.229416847229004
Epoch 1820, val loss: 1.2132030725479126
Epoch 1830, training loss: 622.7579345703125 = 0.07086750119924545 + 100.0 * 6.226871013641357
Epoch 1830, val loss: 1.217422366142273
Epoch 1840, training loss: 622.4976196289062 = 0.06944296509027481 + 100.0 * 6.2242817878723145
Epoch 1840, val loss: 1.2215005159378052
Epoch 1850, training loss: 622.3663330078125 = 0.06806857138872147 + 100.0 * 6.222982883453369
Epoch 1850, val loss: 1.2257555723190308
Epoch 1860, training loss: 622.5980224609375 = 0.06673724949359894 + 100.0 * 6.225313186645508
Epoch 1860, val loss: 1.2302852869033813
Epoch 1870, training loss: 622.6907348632812 = 0.06540930271148682 + 100.0 * 6.226253032684326
Epoch 1870, val loss: 1.2344571352005005
Epoch 1880, training loss: 622.4238891601562 = 0.06408777832984924 + 100.0 * 6.223598003387451
Epoch 1880, val loss: 1.237867832183838
Epoch 1890, training loss: 622.4984741210938 = 0.06283488869667053 + 100.0 * 6.224356651306152
Epoch 1890, val loss: 1.2424603700637817
Epoch 1900, training loss: 622.318359375 = 0.061600204557180405 + 100.0 * 6.222567558288574
Epoch 1900, val loss: 1.2466115951538086
Epoch 1910, training loss: 622.2742919921875 = 0.06040386110544205 + 100.0 * 6.22213888168335
Epoch 1910, val loss: 1.250691294670105
Epoch 1920, training loss: 623.1390991210938 = 0.05924530699849129 + 100.0 * 6.230798244476318
Epoch 1920, val loss: 1.2550463676452637
Epoch 1930, training loss: 622.635498046875 = 0.0580652691423893 + 100.0 * 6.225774765014648
Epoch 1930, val loss: 1.2590947151184082
Epoch 1940, training loss: 622.2991943359375 = 0.05693531036376953 + 100.0 * 6.2224225997924805
Epoch 1940, val loss: 1.2633060216903687
Epoch 1950, training loss: 622.3099975585938 = 0.05585141107439995 + 100.0 * 6.222541332244873
Epoch 1950, val loss: 1.2674129009246826
Epoch 1960, training loss: 622.4313354492188 = 0.05478335916996002 + 100.0 * 6.2237653732299805
Epoch 1960, val loss: 1.271448016166687
Epoch 1970, training loss: 622.1727294921875 = 0.053734708577394485 + 100.0 * 6.221190452575684
Epoch 1970, val loss: 1.276124119758606
Epoch 1980, training loss: 622.3072509765625 = 0.05271283537149429 + 100.0 * 6.222545146942139
Epoch 1980, val loss: 1.280056118965149
Epoch 1990, training loss: 622.8121948242188 = 0.0517326220870018 + 100.0 * 6.227604389190674
Epoch 1990, val loss: 1.2837790250778198
Epoch 2000, training loss: 622.37939453125 = 0.05074136331677437 + 100.0 * 6.2232866287231445
Epoch 2000, val loss: 1.2892240285873413
Epoch 2010, training loss: 622.0470581054688 = 0.049773622304201126 + 100.0 * 6.219973087310791
Epoch 2010, val loss: 1.292779803276062
Epoch 2020, training loss: 621.976318359375 = 0.04885421320796013 + 100.0 * 6.219274997711182
Epoch 2020, val loss: 1.297335147857666
Epoch 2030, training loss: 622.1009521484375 = 0.04795529320836067 + 100.0 * 6.220530033111572
Epoch 2030, val loss: 1.3013722896575928
Epoch 2040, training loss: 622.392333984375 = 0.0470617450773716 + 100.0 * 6.223452568054199
Epoch 2040, val loss: 1.3056882619857788
Epoch 2050, training loss: 622.0706787109375 = 0.04618662968277931 + 100.0 * 6.220244884490967
Epoch 2050, val loss: 1.3098716735839844
Epoch 2060, training loss: 622.3358764648438 = 0.04534350335597992 + 100.0 * 6.222905158996582
Epoch 2060, val loss: 1.3141839504241943
Epoch 2070, training loss: 621.9613037109375 = 0.04449131339788437 + 100.0 * 6.219168186187744
Epoch 2070, val loss: 1.3178613185882568
Epoch 2080, training loss: 621.900146484375 = 0.04367464408278465 + 100.0 * 6.218564510345459
Epoch 2080, val loss: 1.3221501111984253
Epoch 2090, training loss: 621.9266967773438 = 0.04288604483008385 + 100.0 * 6.218837738037109
Epoch 2090, val loss: 1.3261234760284424
Epoch 2100, training loss: 622.0892333984375 = 0.04211318492889404 + 100.0 * 6.220470905303955
Epoch 2100, val loss: 1.3302174806594849
Epoch 2110, training loss: 622.3446044921875 = 0.04135686159133911 + 100.0 * 6.223032474517822
Epoch 2110, val loss: 1.3345451354980469
Epoch 2120, training loss: 621.8701782226562 = 0.0406031608581543 + 100.0 * 6.218296051025391
Epoch 2120, val loss: 1.3390055894851685
Epoch 2130, training loss: 621.797119140625 = 0.039882007986307144 + 100.0 * 6.2175726890563965
Epoch 2130, val loss: 1.3427867889404297
Epoch 2140, training loss: 621.8682250976562 = 0.039180804044008255 + 100.0 * 6.218290328979492
Epoch 2140, val loss: 1.3471174240112305
Epoch 2150, training loss: 622.2376098632812 = 0.03848801180720329 + 100.0 * 6.221991062164307
Epoch 2150, val loss: 1.3510191440582275
Epoch 2160, training loss: 621.8318481445312 = 0.037801507860422134 + 100.0 * 6.217940807342529
Epoch 2160, val loss: 1.3551541566848755
Epoch 2170, training loss: 621.683349609375 = 0.03713935986161232 + 100.0 * 6.216462135314941
Epoch 2170, val loss: 1.3591288328170776
Epoch 2180, training loss: 622.5462646484375 = 0.03650129213929176 + 100.0 * 6.22509765625
Epoch 2180, val loss: 1.3626600503921509
Epoch 2190, training loss: 621.8711547851562 = 0.03584371879696846 + 100.0 * 6.218353271484375
Epoch 2190, val loss: 1.36781907081604
Epoch 2200, training loss: 621.7228393554688 = 0.03521723672747612 + 100.0 * 6.216876029968262
Epoch 2200, val loss: 1.3711875677108765
Epoch 2210, training loss: 621.9528198242188 = 0.03460900858044624 + 100.0 * 6.219182014465332
Epoch 2210, val loss: 1.3753710985183716
Epoch 2220, training loss: 621.701416015625 = 0.034012749791145325 + 100.0 * 6.216674327850342
Epoch 2220, val loss: 1.3793692588806152
Epoch 2230, training loss: 621.749267578125 = 0.033437330275774 + 100.0 * 6.217158317565918
Epoch 2230, val loss: 1.383773922920227
Epoch 2240, training loss: 621.68408203125 = 0.032862987369298935 + 100.0 * 6.216512680053711
Epoch 2240, val loss: 1.3875898122787476
Epoch 2250, training loss: 621.680908203125 = 0.03230686113238335 + 100.0 * 6.21648645401001
Epoch 2250, val loss: 1.391391396522522
Epoch 2260, training loss: 621.8160400390625 = 0.031763043254613876 + 100.0 * 6.217843055725098
Epoch 2260, val loss: 1.3957387208938599
Epoch 2270, training loss: 622.1640014648438 = 0.031226668506860733 + 100.0 * 6.221327781677246
Epoch 2270, val loss: 1.399936318397522
Epoch 2280, training loss: 621.550537109375 = 0.03069346956908703 + 100.0 * 6.215198516845703
Epoch 2280, val loss: 1.4034706354141235
Epoch 2290, training loss: 621.4628295898438 = 0.030185516923666 + 100.0 * 6.214325904846191
Epoch 2290, val loss: 1.4075430631637573
Epoch 2300, training loss: 621.499755859375 = 0.02969394251704216 + 100.0 * 6.214700222015381
Epoch 2300, val loss: 1.4114787578582764
Epoch 2310, training loss: 622.3865966796875 = 0.02921411395072937 + 100.0 * 6.223573684692383
Epoch 2310, val loss: 1.4148032665252686
Epoch 2320, training loss: 621.8159790039062 = 0.028726328164339066 + 100.0 * 6.217872619628906
Epoch 2320, val loss: 1.4194931983947754
Epoch 2330, training loss: 621.7806396484375 = 0.028254054486751556 + 100.0 * 6.217523574829102
Epoch 2330, val loss: 1.4227553606033325
Epoch 2340, training loss: 621.357666015625 = 0.027788512408733368 + 100.0 * 6.213298797607422
Epoch 2340, val loss: 1.4269894361495972
Epoch 2350, training loss: 621.364990234375 = 0.027349479496479034 + 100.0 * 6.213376522064209
Epoch 2350, val loss: 1.4308253526687622
Epoch 2360, training loss: 621.7250366210938 = 0.026920104399323463 + 100.0 * 6.216981410980225
Epoch 2360, val loss: 1.434557557106018
Epoch 2370, training loss: 621.3416137695312 = 0.026486830785870552 + 100.0 * 6.213151454925537
Epoch 2370, val loss: 1.4383820295333862
Epoch 2380, training loss: 621.5137939453125 = 0.026075564324855804 + 100.0 * 6.214877605438232
Epoch 2380, val loss: 1.4418909549713135
Epoch 2390, training loss: 621.7050170898438 = 0.025668298825621605 + 100.0 * 6.216793060302734
Epoch 2390, val loss: 1.4455389976501465
Epoch 2400, training loss: 621.4694213867188 = 0.02525903843343258 + 100.0 * 6.214441299438477
Epoch 2400, val loss: 1.449491024017334
Epoch 2410, training loss: 621.794677734375 = 0.024870263412594795 + 100.0 * 6.217698097229004
Epoch 2410, val loss: 1.4534244537353516
Epoch 2420, training loss: 621.8312377929688 = 0.02447277307510376 + 100.0 * 6.218067646026611
Epoch 2420, val loss: 1.4567680358886719
Epoch 2430, training loss: 621.3525390625 = 0.02408667281270027 + 100.0 * 6.213284492492676
Epoch 2430, val loss: 1.4603798389434814
Epoch 2440, training loss: 621.1687622070312 = 0.023719925433397293 + 100.0 * 6.211450099945068
Epoch 2440, val loss: 1.4643601179122925
Epoch 2450, training loss: 621.1754150390625 = 0.02336847223341465 + 100.0 * 6.211520195007324
Epoch 2450, val loss: 1.4680742025375366
Epoch 2460, training loss: 621.3223266601562 = 0.023024529218673706 + 100.0 * 6.212993144989014
Epoch 2460, val loss: 1.471358299255371
Epoch 2470, training loss: 621.8228149414062 = 0.022676225751638412 + 100.0 * 6.218001365661621
Epoch 2470, val loss: 1.4745208024978638
Epoch 2480, training loss: 621.2749633789062 = 0.0223244596272707 + 100.0 * 6.212526321411133
Epoch 2480, val loss: 1.4786453247070312
Epoch 2490, training loss: 621.1644287109375 = 0.021992947906255722 + 100.0 * 6.211424827575684
Epoch 2490, val loss: 1.4825581312179565
Epoch 2500, training loss: 621.1689453125 = 0.021672673523426056 + 100.0 * 6.211472511291504
Epoch 2500, val loss: 1.4860512018203735
Epoch 2510, training loss: 621.8934326171875 = 0.021361662074923515 + 100.0 * 6.21872091293335
Epoch 2510, val loss: 1.4894449710845947
Epoch 2520, training loss: 621.2099609375 = 0.021040910854935646 + 100.0 * 6.211889266967773
Epoch 2520, val loss: 1.4927608966827393
Epoch 2530, training loss: 621.139404296875 = 0.02073398418724537 + 100.0 * 6.211186408996582
Epoch 2530, val loss: 1.496285319328308
Epoch 2540, training loss: 621.6972045898438 = 0.020438488572835922 + 100.0 * 6.21676778793335
Epoch 2540, val loss: 1.4995732307434082
Epoch 2550, training loss: 621.1223754882812 = 0.02014222741127014 + 100.0 * 6.21102237701416
Epoch 2550, val loss: 1.503773808479309
Epoch 2560, training loss: 621.075439453125 = 0.019855746999382973 + 100.0 * 6.2105560302734375
Epoch 2560, val loss: 1.5071355104446411
Epoch 2570, training loss: 621.2357788085938 = 0.019577622413635254 + 100.0 * 6.212162017822266
Epoch 2570, val loss: 1.5107709169387817
Epoch 2580, training loss: 621.2445678710938 = 0.019307559356093407 + 100.0 * 6.212253093719482
Epoch 2580, val loss: 1.5142745971679688
Epoch 2590, training loss: 621.1995239257812 = 0.019037974998354912 + 100.0 * 6.2118048667907715
Epoch 2590, val loss: 1.5172779560089111
Epoch 2600, training loss: 621.36767578125 = 0.018768640235066414 + 100.0 * 6.213489055633545
Epoch 2600, val loss: 1.5205198526382446
Epoch 2610, training loss: 621.2488403320312 = 0.01850769855082035 + 100.0 * 6.212303638458252
Epoch 2610, val loss: 1.5243676900863647
Epoch 2620, training loss: 621.1035766601562 = 0.018251929432153702 + 100.0 * 6.210853576660156
Epoch 2620, val loss: 1.5271432399749756
Epoch 2630, training loss: 621.2109985351562 = 0.018001161515712738 + 100.0 * 6.211929798126221
Epoch 2630, val loss: 1.5306874513626099
Epoch 2640, training loss: 620.9030151367188 = 0.017757171764969826 + 100.0 * 6.208852767944336
Epoch 2640, val loss: 1.5343443155288696
Epoch 2650, training loss: 620.9899291992188 = 0.017523309215903282 + 100.0 * 6.209724426269531
Epoch 2650, val loss: 1.5376310348510742
Epoch 2660, training loss: 621.4498901367188 = 0.017296921461820602 + 100.0 * 6.214325904846191
Epoch 2660, val loss: 1.541152000427246
Epoch 2670, training loss: 621.17236328125 = 0.01705547794699669 + 100.0 * 6.21155309677124
Epoch 2670, val loss: 1.5435014963150024
Epoch 2680, training loss: 620.8377685546875 = 0.016824370250105858 + 100.0 * 6.20820951461792
Epoch 2680, val loss: 1.5474916696548462
Epoch 2690, training loss: 620.9807739257812 = 0.01660773903131485 + 100.0 * 6.209641933441162
Epoch 2690, val loss: 1.5507597923278809
Epoch 2700, training loss: 621.2559814453125 = 0.016391543671488762 + 100.0 * 6.212395668029785
Epoch 2700, val loss: 1.553586483001709
Epoch 2710, training loss: 621.0276489257812 = 0.01617414504289627 + 100.0 * 6.2101149559021
Epoch 2710, val loss: 1.5564308166503906
Epoch 2720, training loss: 621.054931640625 = 0.015967637300491333 + 100.0 * 6.210389137268066
Epoch 2720, val loss: 1.559982419013977
Epoch 2730, training loss: 621.0899658203125 = 0.0157619621604681 + 100.0 * 6.210742473602295
Epoch 2730, val loss: 1.563476324081421
Epoch 2740, training loss: 620.9955444335938 = 0.015560726635158062 + 100.0 * 6.209799766540527
Epoch 2740, val loss: 1.566149115562439
Epoch 2750, training loss: 620.8128051757812 = 0.015361315570771694 + 100.0 * 6.207974433898926
Epoch 2750, val loss: 1.5695136785507202
Epoch 2760, training loss: 621.0571899414062 = 0.015174565836787224 + 100.0 * 6.210419654846191
Epoch 2760, val loss: 1.5727406740188599
Epoch 2770, training loss: 621.1423950195312 = 0.014985650777816772 + 100.0 * 6.211273670196533
Epoch 2770, val loss: 1.5757012367248535
Epoch 2780, training loss: 621.0349731445312 = 0.014791036024689674 + 100.0 * 6.210201740264893
Epoch 2780, val loss: 1.5781046152114868
Epoch 2790, training loss: 620.7514038085938 = 0.014606527052819729 + 100.0 * 6.207367897033691
Epoch 2790, val loss: 1.5815880298614502
Epoch 2800, training loss: 621.1306762695312 = 0.014432352967560291 + 100.0 * 6.211162567138672
Epoch 2800, val loss: 1.5844756364822388
Epoch 2810, training loss: 620.9572143554688 = 0.01424837950617075 + 100.0 * 6.209429740905762
Epoch 2810, val loss: 1.5874886512756348
Epoch 2820, training loss: 620.8492431640625 = 0.014076143503189087 + 100.0 * 6.2083516120910645
Epoch 2820, val loss: 1.590936541557312
Epoch 2830, training loss: 620.7528686523438 = 0.013908959925174713 + 100.0 * 6.2073893547058105
Epoch 2830, val loss: 1.5936410427093506
Epoch 2840, training loss: 620.9898681640625 = 0.01374411303550005 + 100.0 * 6.209761619567871
Epoch 2840, val loss: 1.596251368522644
Epoch 2850, training loss: 620.683349609375 = 0.013574615120887756 + 100.0 * 6.206697940826416
Epoch 2850, val loss: 1.599429726600647
Epoch 2860, training loss: 620.826904296875 = 0.013412587344646454 + 100.0 * 6.208134651184082
Epoch 2860, val loss: 1.6026071310043335
Epoch 2870, training loss: 620.777099609375 = 0.01325785182416439 + 100.0 * 6.207638263702393
Epoch 2870, val loss: 1.6053680181503296
Epoch 2880, training loss: 621.0188598632812 = 0.013106483966112137 + 100.0 * 6.210057258605957
Epoch 2880, val loss: 1.6085525751113892
Epoch 2890, training loss: 620.709716796875 = 0.01294954214245081 + 100.0 * 6.206967830657959
Epoch 2890, val loss: 1.611547827720642
Epoch 2900, training loss: 621.0967407226562 = 0.012801840901374817 + 100.0 * 6.21083927154541
Epoch 2900, val loss: 1.614302158355713
Epoch 2910, training loss: 620.780517578125 = 0.012648183852434158 + 100.0 * 6.20767879486084
Epoch 2910, val loss: 1.6169519424438477
Epoch 2920, training loss: 620.6707763671875 = 0.012504653073847294 + 100.0 * 6.206582546234131
Epoch 2920, val loss: 1.6200727224349976
Epoch 2930, training loss: 620.583251953125 = 0.012364658527076244 + 100.0 * 6.2057085037231445
Epoch 2930, val loss: 1.6233054399490356
Epoch 2940, training loss: 621.2210693359375 = 0.01223455648869276 + 100.0 * 6.212088584899902
Epoch 2940, val loss: 1.6264506578445435
Epoch 2950, training loss: 620.645263671875 = 0.012090545147657394 + 100.0 * 6.206331729888916
Epoch 2950, val loss: 1.628324031829834
Epoch 2960, training loss: 620.5275268554688 = 0.011953291483223438 + 100.0 * 6.205155372619629
Epoch 2960, val loss: 1.631290078163147
Epoch 2970, training loss: 620.6499633789062 = 0.011824876070022583 + 100.0 * 6.206381320953369
Epoch 2970, val loss: 1.6340880393981934
Epoch 2980, training loss: 620.7931518554688 = 0.011699268594384193 + 100.0 * 6.2078142166137695
Epoch 2980, val loss: 1.6367168426513672
Epoch 2990, training loss: 620.4808959960938 = 0.011568409390747547 + 100.0 * 6.204692840576172
Epoch 2990, val loss: 1.6397051811218262
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 861.6300048828125 = 1.9495278596878052 + 100.0 * 8.59680461883545
Epoch 0, val loss: 1.9542491436004639
Epoch 10, training loss: 861.515625 = 1.9407198429107666 + 100.0 * 8.595748901367188
Epoch 10, val loss: 1.9457519054412842
Epoch 20, training loss: 860.7618408203125 = 1.9297229051589966 + 100.0 * 8.5883207321167
Epoch 20, val loss: 1.9349281787872314
Epoch 30, training loss: 856.0806274414062 = 1.9157369136810303 + 100.0 * 8.541648864746094
Epoch 30, val loss: 1.9209938049316406
Epoch 40, training loss: 832.3540649414062 = 1.9005874395370483 + 100.0 * 8.304534912109375
Epoch 40, val loss: 1.906646728515625
Epoch 50, training loss: 765.1947021484375 = 1.883594274520874 + 100.0 * 7.633111000061035
Epoch 50, val loss: 1.8908463716506958
Epoch 60, training loss: 734.3450927734375 = 1.8688366413116455 + 100.0 * 7.324762344360352
Epoch 60, val loss: 1.8775144815444946
Epoch 70, training loss: 710.515869140625 = 1.8570903539657593 + 100.0 * 7.086587429046631
Epoch 70, val loss: 1.865649700164795
Epoch 80, training loss: 694.1986694335938 = 1.844685435295105 + 100.0 * 6.923540115356445
Epoch 80, val loss: 1.853044867515564
Epoch 90, training loss: 682.9564819335938 = 1.8343422412872314 + 100.0 * 6.811221122741699
Epoch 90, val loss: 1.8425108194351196
Epoch 100, training loss: 675.44921875 = 1.824159860610962 + 100.0 * 6.736250877380371
Epoch 100, val loss: 1.8319430351257324
Epoch 110, training loss: 669.4898681640625 = 1.8130598068237305 + 100.0 * 6.6767683029174805
Epoch 110, val loss: 1.8204313516616821
Epoch 120, training loss: 664.8469848632812 = 1.8021069765090942 + 100.0 * 6.630448341369629
Epoch 120, val loss: 1.8088524341583252
Epoch 130, training loss: 661.4324951171875 = 1.7917717695236206 + 100.0 * 6.596407413482666
Epoch 130, val loss: 1.7976328134536743
Epoch 140, training loss: 658.4365234375 = 1.7813771963119507 + 100.0 * 6.566551208496094
Epoch 140, val loss: 1.7861758470535278
Epoch 150, training loss: 655.9168701171875 = 1.7705421447753906 + 100.0 * 6.5414628982543945
Epoch 150, val loss: 1.7744112014770508
Epoch 160, training loss: 654.2250366210938 = 1.7592403888702393 + 100.0 * 6.524658203125
Epoch 160, val loss: 1.7624011039733887
Epoch 170, training loss: 652.1610107421875 = 1.7471662759780884 + 100.0 * 6.504138469696045
Epoch 170, val loss: 1.7498289346694946
Epoch 180, training loss: 650.7539672851562 = 1.7343320846557617 + 100.0 * 6.490196704864502
Epoch 180, val loss: 1.7367048263549805
Epoch 190, training loss: 649.4691162109375 = 1.720519781112671 + 100.0 * 6.4774861335754395
Epoch 190, val loss: 1.7227563858032227
Epoch 200, training loss: 648.2266235351562 = 1.7057112455368042 + 100.0 * 6.465209484100342
Epoch 200, val loss: 1.7078773975372314
Epoch 210, training loss: 647.3973388671875 = 1.689759373664856 + 100.0 * 6.457076072692871
Epoch 210, val loss: 1.6920287609100342
Epoch 220, training loss: 646.33447265625 = 1.6725366115570068 + 100.0 * 6.446619033813477
Epoch 220, val loss: 1.6752451658248901
Epoch 230, training loss: 645.3047485351562 = 1.6541630029678345 + 100.0 * 6.4365057945251465
Epoch 230, val loss: 1.6573370695114136
Epoch 240, training loss: 644.5684204101562 = 1.6345385313034058 + 100.0 * 6.4293389320373535
Epoch 240, val loss: 1.6384224891662598
Epoch 250, training loss: 643.6837158203125 = 1.6135265827178955 + 100.0 * 6.42070198059082
Epoch 250, val loss: 1.6182726621627808
Epoch 260, training loss: 642.9244384765625 = 1.5914943218231201 + 100.0 * 6.413329601287842
Epoch 260, val loss: 1.597382664680481
Epoch 270, training loss: 642.2327270507812 = 1.5683846473693848 + 100.0 * 6.406643867492676
Epoch 270, val loss: 1.5755822658538818
Epoch 280, training loss: 641.556640625 = 1.544440507888794 + 100.0 * 6.400122165679932
Epoch 280, val loss: 1.55312180519104
Epoch 290, training loss: 641.4225463867188 = 1.5195201635360718 + 100.0 * 6.3990302085876465
Epoch 290, val loss: 1.5300973653793335
Epoch 300, training loss: 640.3748168945312 = 1.4941450357437134 + 100.0 * 6.3888068199157715
Epoch 300, val loss: 1.5068069696426392
Epoch 310, training loss: 639.8799438476562 = 1.4683551788330078 + 100.0 * 6.384115695953369
Epoch 310, val loss: 1.4832663536071777
Epoch 320, training loss: 639.4238891601562 = 1.4422770738601685 + 100.0 * 6.37981653213501
Epoch 320, val loss: 1.4599210023880005
Epoch 330, training loss: 638.7744750976562 = 1.4162442684173584 + 100.0 * 6.373581886291504
Epoch 330, val loss: 1.4368816614151
Epoch 340, training loss: 638.8057861328125 = 1.390310525894165 + 100.0 * 6.374154567718506
Epoch 340, val loss: 1.4142364263534546
Epoch 350, training loss: 637.8526611328125 = 1.3645051717758179 + 100.0 * 6.36488151550293
Epoch 350, val loss: 1.3920183181762695
Epoch 360, training loss: 637.5774536132812 = 1.3391066789627075 + 100.0 * 6.3623833656311035
Epoch 360, val loss: 1.3704408407211304
Epoch 370, training loss: 637.0100708007812 = 1.3141238689422607 + 100.0 * 6.356959342956543
Epoch 370, val loss: 1.3493837118148804
Epoch 380, training loss: 636.6372680664062 = 1.2897173166275024 + 100.0 * 6.353475093841553
Epoch 380, val loss: 1.329283595085144
Epoch 390, training loss: 636.3667602539062 = 1.265681266784668 + 100.0 * 6.351010799407959
Epoch 390, val loss: 1.3099594116210938
Epoch 400, training loss: 635.8748779296875 = 1.2423224449157715 + 100.0 * 6.346325397491455
Epoch 400, val loss: 1.291123390197754
Epoch 410, training loss: 635.3750610351562 = 1.2194771766662598 + 100.0 * 6.341556072235107
Epoch 410, val loss: 1.2732435464859009
Epoch 420, training loss: 635.6928100585938 = 1.1971945762634277 + 100.0 * 6.344955921173096
Epoch 420, val loss: 1.2560330629348755
Epoch 430, training loss: 634.7113647460938 = 1.175318956375122 + 100.0 * 6.335360527038574
Epoch 430, val loss: 1.2392305135726929
Epoch 440, training loss: 634.4039306640625 = 1.1540793180465698 + 100.0 * 6.332498550415039
Epoch 440, val loss: 1.2231261730194092
Epoch 450, training loss: 634.2352294921875 = 1.1334463357925415 + 100.0 * 6.331017971038818
Epoch 450, val loss: 1.2077264785766602
Epoch 460, training loss: 634.2211303710938 = 1.1130964756011963 + 100.0 * 6.331080436706543
Epoch 460, val loss: 1.1930493116378784
Epoch 470, training loss: 633.4632568359375 = 1.0931339263916016 + 100.0 * 6.323700904846191
Epoch 470, val loss: 1.1783781051635742
Epoch 480, training loss: 633.1634521484375 = 1.073691964149475 + 100.0 * 6.320898056030273
Epoch 480, val loss: 1.1644084453582764
Epoch 490, training loss: 633.4114379882812 = 1.0545353889465332 + 100.0 * 6.323568820953369
Epoch 490, val loss: 1.1509891748428345
Epoch 500, training loss: 632.6506958007812 = 1.0356663465499878 + 100.0 * 6.316150665283203
Epoch 500, val loss: 1.1374951601028442
Epoch 510, training loss: 632.6245727539062 = 1.0171383619308472 + 100.0 * 6.316074371337891
Epoch 510, val loss: 1.1244401931762695
Epoch 520, training loss: 632.3196411132812 = 0.9988366365432739 + 100.0 * 6.313208103179932
Epoch 520, val loss: 1.1117491722106934
Epoch 530, training loss: 631.9906616210938 = 0.980715274810791 + 100.0 * 6.3100996017456055
Epoch 530, val loss: 1.0993666648864746
Epoch 540, training loss: 632.272216796875 = 0.96280837059021 + 100.0 * 6.313094139099121
Epoch 540, val loss: 1.0872567892074585
Epoch 550, training loss: 631.4681396484375 = 0.9452104568481445 + 100.0 * 6.305229187011719
Epoch 550, val loss: 1.075457215309143
Epoch 560, training loss: 631.2371826171875 = 0.9277657866477966 + 100.0 * 6.303093910217285
Epoch 560, val loss: 1.0639368295669556
Epoch 570, training loss: 632.1435546875 = 0.910589337348938 + 100.0 * 6.3123297691345215
Epoch 570, val loss: 1.0527068376541138
Epoch 580, training loss: 630.8771362304688 = 0.8935234546661377 + 100.0 * 6.299836158752441
Epoch 580, val loss: 1.0415055751800537
Epoch 590, training loss: 630.5702514648438 = 0.8767117261886597 + 100.0 * 6.296935558319092
Epoch 590, val loss: 1.0308343172073364
Epoch 600, training loss: 630.3875122070312 = 0.8602946400642395 + 100.0 * 6.295272350311279
Epoch 600, val loss: 1.0206053256988525
Epoch 610, training loss: 630.4678955078125 = 0.8441302180290222 + 100.0 * 6.296237945556641
Epoch 610, val loss: 1.0105597972869873
Epoch 620, training loss: 630.4926147460938 = 0.8281636238098145 + 100.0 * 6.29664421081543
Epoch 620, val loss: 1.0010895729064941
Epoch 630, training loss: 629.8662109375 = 0.8124172687530518 + 100.0 * 6.2905378341674805
Epoch 630, val loss: 0.9917169809341431
Epoch 640, training loss: 630.4925537109375 = 0.7970977425575256 + 100.0 * 6.296954154968262
Epoch 640, val loss: 0.9827769994735718
Epoch 650, training loss: 629.6882934570312 = 0.7818481922149658 + 100.0 * 6.289064407348633
Epoch 650, val loss: 0.9741084575653076
Epoch 660, training loss: 629.3606567382812 = 0.7670488357543945 + 100.0 * 6.28593635559082
Epoch 660, val loss: 0.9658968448638916
Epoch 670, training loss: 629.1817626953125 = 0.7525718212127686 + 100.0 * 6.284292221069336
Epoch 670, val loss: 0.9582677483558655
Epoch 680, training loss: 629.3739013671875 = 0.7383561134338379 + 100.0 * 6.286355495452881
Epoch 680, val loss: 0.9511997699737549
Epoch 690, training loss: 629.7006225585938 = 0.7242195010185242 + 100.0 * 6.289763927459717
Epoch 690, val loss: 0.9434454441070557
Epoch 700, training loss: 628.9127807617188 = 0.7103282809257507 + 100.0 * 6.282024383544922
Epoch 700, val loss: 0.9367693662643433
Epoch 710, training loss: 628.61474609375 = 0.6968775391578674 + 100.0 * 6.279178619384766
Epoch 710, val loss: 0.930504322052002
Epoch 720, training loss: 628.5529174804688 = 0.6837353110313416 + 100.0 * 6.278692245483398
Epoch 720, val loss: 0.9246364831924438
Epoch 730, training loss: 628.5030517578125 = 0.670731246471405 + 100.0 * 6.278323173522949
Epoch 730, val loss: 0.9189916253089905
Epoch 740, training loss: 628.2212524414062 = 0.6579338312149048 + 100.0 * 6.275632858276367
Epoch 740, val loss: 0.9136820435523987
Epoch 750, training loss: 628.1793212890625 = 0.645473837852478 + 100.0 * 6.275338649749756
Epoch 750, val loss: 0.9086824655532837
Epoch 760, training loss: 628.001953125 = 0.6332283616065979 + 100.0 * 6.27368688583374
Epoch 760, val loss: 0.9040359854698181
Epoch 770, training loss: 627.8668823242188 = 0.6212134957313538 + 100.0 * 6.272456645965576
Epoch 770, val loss: 0.8996278047561646
Epoch 780, training loss: 628.3452758789062 = 0.6093152761459351 + 100.0 * 6.277359485626221
Epoch 780, val loss: 0.8954048156738281
Epoch 790, training loss: 627.5918579101562 = 0.5976843237876892 + 100.0 * 6.269942283630371
Epoch 790, val loss: 0.8916401863098145
Epoch 800, training loss: 627.4535522460938 = 0.5862607955932617 + 100.0 * 6.268672943115234
Epoch 800, val loss: 0.888079822063446
Epoch 810, training loss: 627.3627319335938 = 0.5750851035118103 + 100.0 * 6.267876625061035
Epoch 810, val loss: 0.8848128318786621
Epoch 820, training loss: 628.1986083984375 = 0.5640804171562195 + 100.0 * 6.276345252990723
Epoch 820, val loss: 0.8817839622497559
Epoch 830, training loss: 627.2194213867188 = 0.5531941652297974 + 100.0 * 6.266662120819092
Epoch 830, val loss: 0.8789101243019104
Epoch 840, training loss: 627.5634765625 = 0.5425267219543457 + 100.0 * 6.270209312438965
Epoch 840, val loss: 0.8762222528457642
Epoch 850, training loss: 627.0228271484375 = 0.5320420861244202 + 100.0 * 6.2649078369140625
Epoch 850, val loss: 0.8740543127059937
Epoch 860, training loss: 626.8547973632812 = 0.5218111872673035 + 100.0 * 6.263329982757568
Epoch 860, val loss: 0.8719133734703064
Epoch 870, training loss: 627.5504150390625 = 0.511738121509552 + 100.0 * 6.270387172698975
Epoch 870, val loss: 0.8700029850006104
Epoch 880, training loss: 626.7709350585938 = 0.501826822757721 + 100.0 * 6.262691020965576
Epoch 880, val loss: 0.8682006001472473
Epoch 890, training loss: 626.4957885742188 = 0.4920923411846161 + 100.0 * 6.260037422180176
Epoch 890, val loss: 0.8667229413986206
Epoch 900, training loss: 626.76708984375 = 0.4825907051563263 + 100.0 * 6.262844562530518
Epoch 900, val loss: 0.8654544353485107
Epoch 910, training loss: 626.5254516601562 = 0.47315552830696106 + 100.0 * 6.260522842407227
Epoch 910, val loss: 0.8643096089363098
Epoch 920, training loss: 626.3101196289062 = 0.4638766646385193 + 100.0 * 6.258462429046631
Epoch 920, val loss: 0.8633617758750916
Epoch 930, training loss: 626.1246337890625 = 0.45486462116241455 + 100.0 * 6.256697177886963
Epoch 930, val loss: 0.8627616763114929
Epoch 940, training loss: 626.30908203125 = 0.4460185468196869 + 100.0 * 6.258630275726318
Epoch 940, val loss: 0.8622301816940308
Epoch 950, training loss: 626.1495361328125 = 0.43723365664482117 + 100.0 * 6.257122993469238
Epoch 950, val loss: 0.8619304895401001
Epoch 960, training loss: 626.02978515625 = 0.4286138713359833 + 100.0 * 6.256011962890625
Epoch 960, val loss: 0.8616198301315308
Epoch 970, training loss: 626.0731811523438 = 0.42019975185394287 + 100.0 * 6.256529808044434
Epoch 970, val loss: 0.8616349101066589
Epoch 980, training loss: 625.7635498046875 = 0.4118785560131073 + 100.0 * 6.253517150878906
Epoch 980, val loss: 0.8619509339332581
Epoch 990, training loss: 625.6734619140625 = 0.4037018120288849 + 100.0 * 6.252697467803955
Epoch 990, val loss: 0.8622007369995117
Epoch 1000, training loss: 625.5612182617188 = 0.3957628607749939 + 100.0 * 6.251654624938965
Epoch 1000, val loss: 0.8628634810447693
Epoch 1010, training loss: 625.8048095703125 = 0.38795965909957886 + 100.0 * 6.254168510437012
Epoch 1010, val loss: 0.8635703921318054
Epoch 1020, training loss: 625.4846801757812 = 0.38015124201774597 + 100.0 * 6.2510457038879395
Epoch 1020, val loss: 0.8643720746040344
Epoch 1030, training loss: 625.4554443359375 = 0.3725222945213318 + 100.0 * 6.250829219818115
Epoch 1030, val loss: 0.8653215169906616
Epoch 1040, training loss: 625.4605102539062 = 0.36503785848617554 + 100.0 * 6.250954627990723
Epoch 1040, val loss: 0.8665949106216431
Epoch 1050, training loss: 625.7014770507812 = 0.3575824201107025 + 100.0 * 6.253438949584961
Epoch 1050, val loss: 0.8677224516868591
Epoch 1060, training loss: 625.2803344726562 = 0.35028520226478577 + 100.0 * 6.249300479888916
Epoch 1060, val loss: 0.8690474033355713
Epoch 1070, training loss: 625.0542602539062 = 0.34321334958076477 + 100.0 * 6.247110366821289
Epoch 1070, val loss: 0.8707065582275391
Epoch 1080, training loss: 624.901123046875 = 0.33629080653190613 + 100.0 * 6.24564790725708
Epoch 1080, val loss: 0.8725219964981079
Epoch 1090, training loss: 624.82958984375 = 0.32950249314308167 + 100.0 * 6.245000839233398
Epoch 1090, val loss: 0.8744521737098694
Epoch 1100, training loss: 624.8027954101562 = 0.3228297233581543 + 100.0 * 6.244799613952637
Epoch 1100, val loss: 0.8764985799789429
Epoch 1110, training loss: 625.3734741210938 = 0.3162170946598053 + 100.0 * 6.250572681427002
Epoch 1110, val loss: 0.8785439133644104
Epoch 1120, training loss: 625.2439575195312 = 0.30960676074028015 + 100.0 * 6.249343395233154
Epoch 1120, val loss: 0.8805245757102966
Epoch 1130, training loss: 624.8486328125 = 0.30314239859580994 + 100.0 * 6.245454788208008
Epoch 1130, val loss: 0.8827903270721436
Epoch 1140, training loss: 624.559814453125 = 0.29686179757118225 + 100.0 * 6.242629528045654
Epoch 1140, val loss: 0.8853058815002441
Epoch 1150, training loss: 624.843505859375 = 0.2907269597053528 + 100.0 * 6.245528221130371
Epoch 1150, val loss: 0.8879106044769287
Epoch 1160, training loss: 624.5698852539062 = 0.2846440374851227 + 100.0 * 6.242852210998535
Epoch 1160, val loss: 0.8904759883880615
Epoch 1170, training loss: 624.3945922851562 = 0.27867573499679565 + 100.0 * 6.241158962249756
Epoch 1170, val loss: 0.8932291269302368
Epoch 1180, training loss: 624.4749755859375 = 0.2728474736213684 + 100.0 * 6.242021560668945
Epoch 1180, val loss: 0.8961960673332214
Epoch 1190, training loss: 624.5267944335938 = 0.26708224415779114 + 100.0 * 6.2425971031188965
Epoch 1190, val loss: 0.8991053104400635
Epoch 1200, training loss: 624.2882690429688 = 0.26143011450767517 + 100.0 * 6.240268707275391
Epoch 1200, val loss: 0.9020613431930542
Epoch 1210, training loss: 624.1731567382812 = 0.2559206485748291 + 100.0 * 6.239172458648682
Epoch 1210, val loss: 0.9052972793579102
Epoch 1220, training loss: 624.7881469726562 = 0.25053462386131287 + 100.0 * 6.245376110076904
Epoch 1220, val loss: 0.9084826707839966
Epoch 1230, training loss: 624.1664428710938 = 0.24511264264583588 + 100.0 * 6.239212989807129
Epoch 1230, val loss: 0.9115789532661438
Epoch 1240, training loss: 624.0426635742188 = 0.23989994823932648 + 100.0 * 6.238027095794678
Epoch 1240, val loss: 0.9149798154830933
Epoch 1250, training loss: 624.9273681640625 = 0.2348013073205948 + 100.0 * 6.2469258308410645
Epoch 1250, val loss: 0.9182726144790649
Epoch 1260, training loss: 624.1535034179688 = 0.22962680459022522 + 100.0 * 6.239238739013672
Epoch 1260, val loss: 0.921623706817627
Epoch 1270, training loss: 623.8277587890625 = 0.2246941775083542 + 100.0 * 6.2360310554504395
Epoch 1270, val loss: 0.9252901077270508
Epoch 1280, training loss: 623.7703857421875 = 0.21985656023025513 + 100.0 * 6.235505104064941
Epoch 1280, val loss: 0.9289790987968445
Epoch 1290, training loss: 624.1752319335938 = 0.21513348817825317 + 100.0 * 6.239601135253906
Epoch 1290, val loss: 0.9328492283821106
Epoch 1300, training loss: 623.7876586914062 = 0.21043528616428375 + 100.0 * 6.235772132873535
Epoch 1300, val loss: 0.9363120198249817
Epoch 1310, training loss: 623.7017822265625 = 0.20583370327949524 + 100.0 * 6.234959602355957
Epoch 1310, val loss: 0.9401599168777466
Epoch 1320, training loss: 624.091064453125 = 0.20134510099887848 + 100.0 * 6.23889684677124
Epoch 1320, val loss: 0.943873941898346
Epoch 1330, training loss: 623.6699829101562 = 0.1968659609556198 + 100.0 * 6.234731674194336
Epoch 1330, val loss: 0.9478896260261536
Epoch 1340, training loss: 623.6497802734375 = 0.19255925714969635 + 100.0 * 6.234572410583496
Epoch 1340, val loss: 0.9518881440162659
Epoch 1350, training loss: 623.6915283203125 = 0.18830366432666779 + 100.0 * 6.235032081604004
Epoch 1350, val loss: 0.9560338854789734
Epoch 1360, training loss: 623.38623046875 = 0.184114009141922 + 100.0 * 6.232020854949951
Epoch 1360, val loss: 0.9601344466209412
Epoch 1370, training loss: 623.3182373046875 = 0.18003994226455688 + 100.0 * 6.231381893157959
Epoch 1370, val loss: 0.9644411206245422
Epoch 1380, training loss: 623.5426025390625 = 0.17608040571212769 + 100.0 * 6.2336649894714355
Epoch 1380, val loss: 0.968668520450592
Epoch 1390, training loss: 623.572509765625 = 0.1721579134464264 + 100.0 * 6.23400354385376
Epoch 1390, val loss: 0.9731203317642212
Epoch 1400, training loss: 623.2767944335938 = 0.16824929416179657 + 100.0 * 6.231085300445557
Epoch 1400, val loss: 0.9773005843162537
Epoch 1410, training loss: 623.1693725585938 = 0.16450592875480652 + 100.0 * 6.230048656463623
Epoch 1410, val loss: 0.9819914698600769
Epoch 1420, training loss: 623.1448974609375 = 0.1608678549528122 + 100.0 * 6.22983980178833
Epoch 1420, val loss: 0.9864741563796997
Epoch 1430, training loss: 623.8569946289062 = 0.15730978548526764 + 100.0 * 6.236997127532959
Epoch 1430, val loss: 0.9911129474639893
Epoch 1440, training loss: 623.5628662109375 = 0.15375152230262756 + 100.0 * 6.234091281890869
Epoch 1440, val loss: 0.99531489610672
Epoch 1450, training loss: 623.1575927734375 = 0.15026074647903442 + 100.0 * 6.23007345199585
Epoch 1450, val loss: 0.9998894333839417
Epoch 1460, training loss: 622.9675903320312 = 0.14689220488071442 + 100.0 * 6.228206634521484
Epoch 1460, val loss: 1.0043723583221436
Epoch 1470, training loss: 622.9270629882812 = 0.14363767206668854 + 100.0 * 6.227834701538086
Epoch 1470, val loss: 1.0091041326522827
Epoch 1480, training loss: 624.2804565429688 = 0.1404760479927063 + 100.0 * 6.241399765014648
Epoch 1480, val loss: 1.0137622356414795
Epoch 1490, training loss: 623.089599609375 = 0.13720619678497314 + 100.0 * 6.229523658752441
Epoch 1490, val loss: 1.018370270729065
Epoch 1500, training loss: 622.8362426757812 = 0.1341077983379364 + 100.0 * 6.22702169418335
Epoch 1500, val loss: 1.023064136505127
Epoch 1510, training loss: 623.219482421875 = 0.13111375272274017 + 100.0 * 6.230884075164795
Epoch 1510, val loss: 1.027961015701294
Epoch 1520, training loss: 622.7232055664062 = 0.12816749513149261 + 100.0 * 6.225950241088867
Epoch 1520, val loss: 1.032818078994751
Epoch 1530, training loss: 622.7539672851562 = 0.1253112107515335 + 100.0 * 6.226286888122559
Epoch 1530, val loss: 1.0376629829406738
Epoch 1540, training loss: 622.6574096679688 = 0.12251995503902435 + 100.0 * 6.225349426269531
Epoch 1540, val loss: 1.0426759719848633
Epoch 1550, training loss: 623.0634155273438 = 0.11981692165136337 + 100.0 * 6.229435920715332
Epoch 1550, val loss: 1.0478620529174805
Epoch 1560, training loss: 623.0671997070312 = 0.11710969358682632 + 100.0 * 6.229500770568848
Epoch 1560, val loss: 1.0523560047149658
Epoch 1570, training loss: 622.7677612304688 = 0.1144464761018753 + 100.0 * 6.22653341293335
Epoch 1570, val loss: 1.0573261976242065
Epoch 1580, training loss: 622.7183227539062 = 0.11190527677536011 + 100.0 * 6.226064682006836
Epoch 1580, val loss: 1.0626184940338135
Epoch 1590, training loss: 622.730224609375 = 0.10943081229925156 + 100.0 * 6.226207733154297
Epoch 1590, val loss: 1.0677217245101929
Epoch 1600, training loss: 622.5741577148438 = 0.10701368749141693 + 100.0 * 6.224671363830566
Epoch 1600, val loss: 1.0727630853652954
Epoch 1610, training loss: 623.0748901367188 = 0.10464919358491898 + 100.0 * 6.229701995849609
Epoch 1610, val loss: 1.077863097190857
Epoch 1620, training loss: 622.5792236328125 = 0.10227327793836594 + 100.0 * 6.224769592285156
Epoch 1620, val loss: 1.0827114582061768
Epoch 1630, training loss: 622.3955078125 = 0.10004960745573044 + 100.0 * 6.222954750061035
Epoch 1630, val loss: 1.088220477104187
Epoch 1640, training loss: 622.3565673828125 = 0.09786766022443771 + 100.0 * 6.2225871086120605
Epoch 1640, val loss: 1.0933395624160767
Epoch 1650, training loss: 622.5940551757812 = 0.09574726223945618 + 100.0 * 6.224982738494873
Epoch 1650, val loss: 1.0987108945846558
Epoch 1660, training loss: 622.5888061523438 = 0.09363973885774612 + 100.0 * 6.22495174407959
Epoch 1660, val loss: 1.104012131690979
Epoch 1670, training loss: 622.6119384765625 = 0.0915914997458458 + 100.0 * 6.225203037261963
Epoch 1670, val loss: 1.1089140176773071
Epoch 1680, training loss: 622.5548095703125 = 0.08957822620868683 + 100.0 * 6.224652290344238
Epoch 1680, val loss: 1.1139463186264038
Epoch 1690, training loss: 622.3599243164062 = 0.0876205787062645 + 100.0 * 6.22272253036499
Epoch 1690, val loss: 1.1190277338027954
Epoch 1700, training loss: 622.4022827148438 = 0.08574991673231125 + 100.0 * 6.223165512084961
Epoch 1700, val loss: 1.1242798566818237
Epoch 1710, training loss: 622.211669921875 = 0.08391555398702621 + 100.0 * 6.221277236938477
Epoch 1710, val loss: 1.129704236984253
Epoch 1720, training loss: 622.0466918945312 = 0.08212614059448242 + 100.0 * 6.2196455001831055
Epoch 1720, val loss: 1.134905457496643
Epoch 1730, training loss: 622.3450317382812 = 0.0804111510515213 + 100.0 * 6.222646236419678
Epoch 1730, val loss: 1.1401242017745972
Epoch 1740, training loss: 622.4453735351562 = 0.07867715507745743 + 100.0 * 6.223667144775391
Epoch 1740, val loss: 1.1448651552200317
Epoch 1750, training loss: 622.0787963867188 = 0.07698555290699005 + 100.0 * 6.22001838684082
Epoch 1750, val loss: 1.1499946117401123
Epoch 1760, training loss: 622.0939331054688 = 0.07535520941019058 + 100.0 * 6.220185279846191
Epoch 1760, val loss: 1.1552821397781372
Epoch 1770, training loss: 622.1797485351562 = 0.07379602640867233 + 100.0 * 6.221059799194336
Epoch 1770, val loss: 1.1601288318634033
Epoch 1780, training loss: 622.012451171875 = 0.07225047796964645 + 100.0 * 6.219401836395264
Epoch 1780, val loss: 1.1655625104904175
Epoch 1790, training loss: 622.5826416015625 = 0.07074740529060364 + 100.0 * 6.225119113922119
Epoch 1790, val loss: 1.1707420349121094
Epoch 1800, training loss: 622.017822265625 = 0.06928012520074844 + 100.0 * 6.219485759735107
Epoch 1800, val loss: 1.1756781339645386
Epoch 1810, training loss: 621.9229125976562 = 0.06785029917955399 + 100.0 * 6.218550682067871
Epoch 1810, val loss: 1.1809521913528442
Epoch 1820, training loss: 622.2501220703125 = 0.06647203117609024 + 100.0 * 6.221836566925049
Epoch 1820, val loss: 1.1857614517211914
Epoch 1830, training loss: 621.8753051757812 = 0.06511775404214859 + 100.0 * 6.218101501464844
Epoch 1830, val loss: 1.191542625427246
Epoch 1840, training loss: 621.952392578125 = 0.06379345059394836 + 100.0 * 6.218886375427246
Epoch 1840, val loss: 1.1963896751403809
Epoch 1850, training loss: 621.9042358398438 = 0.06250612437725067 + 100.0 * 6.218417167663574
Epoch 1850, val loss: 1.2013102769851685
Epoch 1860, training loss: 621.7732543945312 = 0.06125440448522568 + 100.0 * 6.2171196937561035
Epoch 1860, val loss: 1.2065544128417969
Epoch 1870, training loss: 622.0022583007812 = 0.06003672629594803 + 100.0 * 6.219421863555908
Epoch 1870, val loss: 1.2114530801773071
Epoch 1880, training loss: 621.8058471679688 = 0.05883905291557312 + 100.0 * 6.217470169067383
Epoch 1880, val loss: 1.2167469263076782
Epoch 1890, training loss: 621.8564453125 = 0.057679370045661926 + 100.0 * 6.217987537384033
Epoch 1890, val loss: 1.2214018106460571
Epoch 1900, training loss: 621.8232421875 = 0.056532662361860275 + 100.0 * 6.217667102813721
Epoch 1900, val loss: 1.2260856628417969
Epoch 1910, training loss: 621.6387329101562 = 0.05542382970452309 + 100.0 * 6.2158331871032715
Epoch 1910, val loss: 1.2310020923614502
Epoch 1920, training loss: 621.5928955078125 = 0.05435105040669441 + 100.0 * 6.215385913848877
Epoch 1920, val loss: 1.2359848022460938
Epoch 1930, training loss: 622.04541015625 = 0.05331658199429512 + 100.0 * 6.219920635223389
Epoch 1930, val loss: 1.2409471273422241
Epoch 1940, training loss: 621.7688598632812 = 0.052275802940130234 + 100.0 * 6.217165946960449
Epoch 1940, val loss: 1.2452257871627808
Epoch 1950, training loss: 621.8492431640625 = 0.051270630210638046 + 100.0 * 6.217979431152344
Epoch 1950, val loss: 1.24997079372406
Epoch 1960, training loss: 621.521484375 = 0.050283871591091156 + 100.0 * 6.214712142944336
Epoch 1960, val loss: 1.2548832893371582
Epoch 1970, training loss: 621.5283813476562 = 0.04933872073888779 + 100.0 * 6.2147908210754395
Epoch 1970, val loss: 1.2599685192108154
Epoch 1980, training loss: 621.8602294921875 = 0.048421625047922134 + 100.0 * 6.218118190765381
Epoch 1980, val loss: 1.2645785808563232
Epoch 1990, training loss: 621.8408813476562 = 0.04751065745949745 + 100.0 * 6.2179341316223145
Epoch 1990, val loss: 1.2688013315200806
Epoch 2000, training loss: 621.6168823242188 = 0.04661688581109047 + 100.0 * 6.215702533721924
Epoch 2000, val loss: 1.2739813327789307
Epoch 2010, training loss: 621.7841186523438 = 0.045741837471723557 + 100.0 * 6.217383861541748
Epoch 2010, val loss: 1.2783887386322021
Epoch 2020, training loss: 621.5060424804688 = 0.044887226074934006 + 100.0 * 6.214611530303955
Epoch 2020, val loss: 1.2824987173080444
Epoch 2030, training loss: 621.4041748046875 = 0.04407426714897156 + 100.0 * 6.213601112365723
Epoch 2030, val loss: 1.2873066663742065
Epoch 2040, training loss: 621.4512939453125 = 0.043279364705085754 + 100.0 * 6.214080333709717
Epoch 2040, val loss: 1.2920441627502441
Epoch 2050, training loss: 621.5595703125 = 0.04250016435980797 + 100.0 * 6.215170860290527
Epoch 2050, val loss: 1.2960896492004395
Epoch 2060, training loss: 621.3408813476562 = 0.041732046753168106 + 100.0 * 6.212991237640381
Epoch 2060, val loss: 1.300835132598877
Epoch 2070, training loss: 621.4078979492188 = 0.040988076478242874 + 100.0 * 6.2136688232421875
Epoch 2070, val loss: 1.3049112558364868
Epoch 2080, training loss: 621.348876953125 = 0.040250420570373535 + 100.0 * 6.2130866050720215
Epoch 2080, val loss: 1.3094666004180908
Epoch 2090, training loss: 621.9677734375 = 0.03954341635107994 + 100.0 * 6.219282150268555
Epoch 2090, val loss: 1.3142951726913452
Epoch 2100, training loss: 621.279541015625 = 0.0388355553150177 + 100.0 * 6.212407112121582
Epoch 2100, val loss: 1.3180071115493774
Epoch 2110, training loss: 621.1309204101562 = 0.03815753012895584 + 100.0 * 6.210927486419678
Epoch 2110, val loss: 1.3225383758544922
Epoch 2120, training loss: 621.0811157226562 = 0.037505604326725006 + 100.0 * 6.2104363441467285
Epoch 2120, val loss: 1.3268412351608276
Epoch 2130, training loss: 621.1642456054688 = 0.03687547892332077 + 100.0 * 6.211273670196533
Epoch 2130, val loss: 1.3313246965408325
Epoch 2140, training loss: 621.966064453125 = 0.036263249814510345 + 100.0 * 6.219297885894775
Epoch 2140, val loss: 1.3351951837539673
Epoch 2150, training loss: 621.385498046875 = 0.03559396415948868 + 100.0 * 6.213499069213867
Epoch 2150, val loss: 1.3399251699447632
Epoch 2160, training loss: 621.2518920898438 = 0.03497916832566261 + 100.0 * 6.212169170379639
Epoch 2160, val loss: 1.3434934616088867
Epoch 2170, training loss: 621.1220703125 = 0.03438778221607208 + 100.0 * 6.210876941680908
Epoch 2170, val loss: 1.3477823734283447
Epoch 2180, training loss: 621.1826171875 = 0.03382609039545059 + 100.0 * 6.211487770080566
Epoch 2180, val loss: 1.3518359661102295
Epoch 2190, training loss: 621.06396484375 = 0.03326444327831268 + 100.0 * 6.2103071212768555
Epoch 2190, val loss: 1.3559763431549072
Epoch 2200, training loss: 621.2245483398438 = 0.032723162323236465 + 100.0 * 6.211918354034424
Epoch 2200, val loss: 1.3598748445510864
Epoch 2210, training loss: 621.13037109375 = 0.03217652440071106 + 100.0 * 6.210982322692871
Epoch 2210, val loss: 1.36386239528656
Epoch 2220, training loss: 621.0011596679688 = 0.03164026886224747 + 100.0 * 6.209695339202881
Epoch 2220, val loss: 1.3679490089416504
Epoch 2230, training loss: 620.8607788085938 = 0.031124170869588852 + 100.0 * 6.208296298980713
Epoch 2230, val loss: 1.372294545173645
Epoch 2240, training loss: 620.8351440429688 = 0.03063243441283703 + 100.0 * 6.20804500579834
Epoch 2240, val loss: 1.3761992454528809
Epoch 2250, training loss: 621.0578002929688 = 0.030153313651680946 + 100.0 * 6.2102766036987305
Epoch 2250, val loss: 1.380170464515686
Epoch 2260, training loss: 621.1219482421875 = 0.02965773083269596 + 100.0 * 6.210922718048096
Epoch 2260, val loss: 1.3839677572250366
Epoch 2270, training loss: 620.911865234375 = 0.029176073148846626 + 100.0 * 6.208827018737793
Epoch 2270, val loss: 1.3873306512832642
Epoch 2280, training loss: 620.7837524414062 = 0.028712380677461624 + 100.0 * 6.207550525665283
Epoch 2280, val loss: 1.3912394046783447
Epoch 2290, training loss: 620.963134765625 = 0.028274428099393845 + 100.0 * 6.209348678588867
Epoch 2290, val loss: 1.3947068452835083
Epoch 2300, training loss: 620.9849853515625 = 0.027834201231598854 + 100.0 * 6.209571838378906
Epoch 2300, val loss: 1.398905873298645
Epoch 2310, training loss: 621.0177612304688 = 0.02740427665412426 + 100.0 * 6.209903717041016
Epoch 2310, val loss: 1.4028335809707642
Epoch 2320, training loss: 620.8933715820312 = 0.026975851505994797 + 100.0 * 6.2086639404296875
Epoch 2320, val loss: 1.406522274017334
Epoch 2330, training loss: 620.8468627929688 = 0.02656318061053753 + 100.0 * 6.208203315734863
Epoch 2330, val loss: 1.4106471538543701
Epoch 2340, training loss: 620.9741821289062 = 0.026167243719100952 + 100.0 * 6.209480285644531
Epoch 2340, val loss: 1.4135746955871582
Epoch 2350, training loss: 620.7259521484375 = 0.025768399238586426 + 100.0 * 6.207001686096191
Epoch 2350, val loss: 1.4175347089767456
Epoch 2360, training loss: 620.9559326171875 = 0.025390079244971275 + 100.0 * 6.209305286407471
Epoch 2360, val loss: 1.4210466146469116
Epoch 2370, training loss: 620.9596557617188 = 0.025005631148815155 + 100.0 * 6.209346294403076
Epoch 2370, val loss: 1.4249060153961182
Epoch 2380, training loss: 620.849853515625 = 0.024629447609186172 + 100.0 * 6.208251953125
Epoch 2380, val loss: 1.4282957315444946
Epoch 2390, training loss: 620.6591796875 = 0.02426699921488762 + 100.0 * 6.206348896026611
Epoch 2390, val loss: 1.431650161743164
Epoch 2400, training loss: 620.7203979492188 = 0.023916739970445633 + 100.0 * 6.20696496963501
Epoch 2400, val loss: 1.4352002143859863
Epoch 2410, training loss: 621.1021728515625 = 0.023574385792016983 + 100.0 * 6.210785865783691
Epoch 2410, val loss: 1.4386706352233887
Epoch 2420, training loss: 620.6549072265625 = 0.023219672963023186 + 100.0 * 6.206316947937012
Epoch 2420, val loss: 1.4420721530914307
Epoch 2430, training loss: 620.5731201171875 = 0.02289309911429882 + 100.0 * 6.205502033233643
Epoch 2430, val loss: 1.4456863403320312
Epoch 2440, training loss: 620.5537719726562 = 0.022576728835701942 + 100.0 * 6.2053117752075195
Epoch 2440, val loss: 1.4491040706634521
Epoch 2450, training loss: 621.2271728515625 = 0.022267194464802742 + 100.0 * 6.2120490074157715
Epoch 2450, val loss: 1.4518359899520874
Epoch 2460, training loss: 620.883056640625 = 0.02194833569228649 + 100.0 * 6.208610534667969
Epoch 2460, val loss: 1.4551723003387451
Epoch 2470, training loss: 620.9453125 = 0.02163724973797798 + 100.0 * 6.2092366218566895
Epoch 2470, val loss: 1.4587793350219727
Epoch 2480, training loss: 620.5241088867188 = 0.021329699084162712 + 100.0 * 6.205028057098389
Epoch 2480, val loss: 1.4619570970535278
Epoch 2490, training loss: 620.5098266601562 = 0.02104467712342739 + 100.0 * 6.204887390136719
Epoch 2490, val loss: 1.4656094312667847
Epoch 2500, training loss: 620.7080688476562 = 0.020769966766238213 + 100.0 * 6.206872940063477
Epoch 2500, val loss: 1.4687001705169678
Epoch 2510, training loss: 620.7677612304688 = 0.020486731082201004 + 100.0 * 6.207472324371338
Epoch 2510, val loss: 1.4716391563415527
Epoch 2520, training loss: 620.4839477539062 = 0.020198531448841095 + 100.0 * 6.20463752746582
Epoch 2520, val loss: 1.4752275943756104
Epoch 2530, training loss: 620.530517578125 = 0.019933335483074188 + 100.0 * 6.205105304718018
Epoch 2530, val loss: 1.4785064458847046
Epoch 2540, training loss: 620.7009887695312 = 0.019673343747854233 + 100.0 * 6.206813335418701
Epoch 2540, val loss: 1.4814530611038208
Epoch 2550, training loss: 620.4736938476562 = 0.019405271857976913 + 100.0 * 6.204542636871338
Epoch 2550, val loss: 1.4845404624938965
Epoch 2560, training loss: 620.6190185546875 = 0.019148128107190132 + 100.0 * 6.205998420715332
Epoch 2560, val loss: 1.4877285957336426
Epoch 2570, training loss: 620.6034545898438 = 0.018894771113991737 + 100.0 * 6.205845355987549
Epoch 2570, val loss: 1.490525245666504
Epoch 2580, training loss: 620.3867797851562 = 0.018649151548743248 + 100.0 * 6.203680992126465
Epoch 2580, val loss: 1.4937357902526855
Epoch 2590, training loss: 620.65234375 = 0.018416911363601685 + 100.0 * 6.206338882446289
Epoch 2590, val loss: 1.4967291355133057
Epoch 2600, training loss: 620.3399658203125 = 0.018170492723584175 + 100.0 * 6.20321798324585
Epoch 2600, val loss: 1.499874234199524
Epoch 2610, training loss: 620.3072509765625 = 0.01793961599469185 + 100.0 * 6.202892780303955
Epoch 2610, val loss: 1.5031356811523438
Epoch 2620, training loss: 620.4080810546875 = 0.017715537920594215 + 100.0 * 6.203903675079346
Epoch 2620, val loss: 1.506429672241211
Epoch 2630, training loss: 620.5765380859375 = 0.01749165914952755 + 100.0 * 6.20559024810791
Epoch 2630, val loss: 1.509566068649292
Epoch 2640, training loss: 620.3514404296875 = 0.01727445423603058 + 100.0 * 6.203341484069824
Epoch 2640, val loss: 1.5117855072021484
Epoch 2650, training loss: 620.4349365234375 = 0.017057690769433975 + 100.0 * 6.204179286956787
Epoch 2650, val loss: 1.5145869255065918
Epoch 2660, training loss: 620.5001220703125 = 0.01684653013944626 + 100.0 * 6.204832553863525
Epoch 2660, val loss: 1.5175423622131348
Epoch 2670, training loss: 620.8034057617188 = 0.01663759909570217 + 100.0 * 6.207867622375488
Epoch 2670, val loss: 1.5206531286239624
Epoch 2680, training loss: 620.3296508789062 = 0.016426173970103264 + 100.0 * 6.203132629394531
Epoch 2680, val loss: 1.523687720298767
Epoch 2690, training loss: 620.2318115234375 = 0.016233215108513832 + 100.0 * 6.202155590057373
Epoch 2690, val loss: 1.5265629291534424
Epoch 2700, training loss: 620.6070556640625 = 0.016042103990912437 + 100.0 * 6.2059102058410645
Epoch 2700, val loss: 1.5297170877456665
Epoch 2710, training loss: 620.1102905273438 = 0.015843739733099937 + 100.0 * 6.200944423675537
Epoch 2710, val loss: 1.532148003578186
Epoch 2720, training loss: 620.1596069335938 = 0.015660235658288002 + 100.0 * 6.201439380645752
Epoch 2720, val loss: 1.5347468852996826
Epoch 2730, training loss: 620.8447265625 = 0.015487917698919773 + 100.0 * 6.208292484283447
Epoch 2730, val loss: 1.5373797416687012
Epoch 2740, training loss: 620.2112426757812 = 0.015289844013750553 + 100.0 * 6.201959133148193
Epoch 2740, val loss: 1.5405954122543335
Epoch 2750, training loss: 620.0845336914062 = 0.015109028667211533 + 100.0 * 6.2006940841674805
Epoch 2750, val loss: 1.5434190034866333
Epoch 2760, training loss: 620.1027221679688 = 0.014942629262804985 + 100.0 * 6.200877666473389
Epoch 2760, val loss: 1.5461430549621582
Epoch 2770, training loss: 620.8681640625 = 0.014777916483581066 + 100.0 * 6.208534240722656
Epoch 2770, val loss: 1.5487319231033325
Epoch 2780, training loss: 620.5691528320312 = 0.01459027174860239 + 100.0 * 6.205545425415039
Epoch 2780, val loss: 1.5505084991455078
Epoch 2790, training loss: 620.1207885742188 = 0.014419561251997948 + 100.0 * 6.201063632965088
Epoch 2790, val loss: 1.553824543952942
Epoch 2800, training loss: 620.0193481445312 = 0.014254788868129253 + 100.0 * 6.2000508308410645
Epoch 2800, val loss: 1.556711196899414
Epoch 2810, training loss: 620.1729736328125 = 0.014104856178164482 + 100.0 * 6.2015886306762695
Epoch 2810, val loss: 1.559943437576294
Epoch 2820, training loss: 620.4280395507812 = 0.013943271711468697 + 100.0 * 6.204141139984131
Epoch 2820, val loss: 1.5619934797286987
Epoch 2830, training loss: 620.248046875 = 0.013784538954496384 + 100.0 * 6.202342510223389
Epoch 2830, val loss: 1.5638011693954468
Epoch 2840, training loss: 620.131103515625 = 0.013628548942506313 + 100.0 * 6.201175212860107
Epoch 2840, val loss: 1.5670357942581177
Epoch 2850, training loss: 620.4437866210938 = 0.013482945039868355 + 100.0 * 6.20430326461792
Epoch 2850, val loss: 1.5691497325897217
Epoch 2860, training loss: 619.969482421875 = 0.013327142223715782 + 100.0 * 6.199562072753906
Epoch 2860, val loss: 1.57208251953125
Epoch 2870, training loss: 619.9765625 = 0.0131865618750453 + 100.0 * 6.199634075164795
Epoch 2870, val loss: 1.5747038125991821
Epoch 2880, training loss: 619.9487915039062 = 0.013048086315393448 + 100.0 * 6.199357032775879
Epoch 2880, val loss: 1.5772202014923096
Epoch 2890, training loss: 620.4592895507812 = 0.012918606400489807 + 100.0 * 6.204463481903076
Epoch 2890, val loss: 1.5795178413391113
Epoch 2900, training loss: 620.2318725585938 = 0.01276942528784275 + 100.0 * 6.202191352844238
Epoch 2900, val loss: 1.5822442770004272
Epoch 2910, training loss: 619.9617309570312 = 0.012629806064069271 + 100.0 * 6.199491024017334
Epoch 2910, val loss: 1.5845229625701904
Epoch 2920, training loss: 619.93408203125 = 0.012502716854214668 + 100.0 * 6.199215888977051
Epoch 2920, val loss: 1.58702552318573
Epoch 2930, training loss: 620.146728515625 = 0.012376357801258564 + 100.0 * 6.201343536376953
Epoch 2930, val loss: 1.5896323919296265
Epoch 2940, training loss: 619.838623046875 = 0.012246099300682545 + 100.0 * 6.198263645172119
Epoch 2940, val loss: 1.5920153856277466
Epoch 2950, training loss: 620.0197143554688 = 0.012124571949243546 + 100.0 * 6.200075626373291
Epoch 2950, val loss: 1.5944703817367554
Epoch 2960, training loss: 619.99609375 = 0.01200026273727417 + 100.0 * 6.199840545654297
Epoch 2960, val loss: 1.5969090461730957
Epoch 2970, training loss: 620.4209594726562 = 0.011883825063705444 + 100.0 * 6.204090595245361
Epoch 2970, val loss: 1.5989868640899658
Epoch 2980, training loss: 620.0115966796875 = 0.011748695746064186 + 100.0 * 6.19999885559082
Epoch 2980, val loss: 1.6012520790100098
Epoch 2990, training loss: 619.9752197265625 = 0.011633191257715225 + 100.0 * 6.1996355056762695
Epoch 2990, val loss: 1.603419303894043
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 861.6295166015625 = 1.944563388824463 + 100.0 * 8.59684944152832
Epoch 0, val loss: 1.9414607286453247
Epoch 10, training loss: 861.5546875 = 1.93603515625 + 100.0 * 8.596186637878418
Epoch 10, val loss: 1.9336475133895874
Epoch 20, training loss: 861.1011962890625 = 1.9252924919128418 + 100.0 * 8.591758728027344
Epoch 20, val loss: 1.9236394166946411
Epoch 30, training loss: 858.1129760742188 = 1.911254644393921 + 100.0 * 8.562017440795898
Epoch 30, val loss: 1.9104975461959839
Epoch 40, training loss: 840.2008056640625 = 1.8944344520568848 + 100.0 * 8.383064270019531
Epoch 40, val loss: 1.8951225280761719
Epoch 50, training loss: 764.2906494140625 = 1.8770976066589355 + 100.0 * 7.624135971069336
Epoch 50, val loss: 1.879081130027771
Epoch 60, training loss: 732.9668579101562 = 1.8614176511764526 + 100.0 * 7.311054706573486
Epoch 60, val loss: 1.8648145198822021
Epoch 70, training loss: 710.178955078125 = 1.849399209022522 + 100.0 * 7.0832953453063965
Epoch 70, val loss: 1.8533296585083008
Epoch 80, training loss: 697.6937255859375 = 1.8387306928634644 + 100.0 * 6.958549499511719
Epoch 80, val loss: 1.8430774211883545
Epoch 90, training loss: 688.7698974609375 = 1.8297655582427979 + 100.0 * 6.869401454925537
Epoch 90, val loss: 1.8341220617294312
Epoch 100, training loss: 680.7630004882812 = 1.8208962678909302 + 100.0 * 6.7894206047058105
Epoch 100, val loss: 1.825432538986206
Epoch 110, training loss: 675.294921875 = 1.8123880624771118 + 100.0 * 6.734825134277344
Epoch 110, val loss: 1.8169764280319214
Epoch 120, training loss: 670.864501953125 = 1.8044819831848145 + 100.0 * 6.6905999183654785
Epoch 120, val loss: 1.8089450597763062
Epoch 130, training loss: 666.7412109375 = 1.7968106269836426 + 100.0 * 6.649444103240967
Epoch 130, val loss: 1.8012553453445435
Epoch 140, training loss: 663.3350830078125 = 1.7892787456512451 + 100.0 * 6.615458011627197
Epoch 140, val loss: 1.7936122417449951
Epoch 150, training loss: 660.7866821289062 = 1.7815403938293457 + 100.0 * 6.590051174163818
Epoch 150, val loss: 1.7857861518859863
Epoch 160, training loss: 658.20263671875 = 1.7733008861541748 + 100.0 * 6.564292907714844
Epoch 160, val loss: 1.777756690979004
Epoch 170, training loss: 655.9584350585938 = 1.764809489250183 + 100.0 * 6.541935920715332
Epoch 170, val loss: 1.769649863243103
Epoch 180, training loss: 654.0029296875 = 1.7558447122573853 + 100.0 * 6.522470951080322
Epoch 180, val loss: 1.761252999305725
Epoch 190, training loss: 652.201904296875 = 1.746164083480835 + 100.0 * 6.5045576095581055
Epoch 190, val loss: 1.7522417306900024
Epoch 200, training loss: 650.5792236328125 = 1.735694169998169 + 100.0 * 6.488434791564941
Epoch 200, val loss: 1.7426341772079468
Epoch 210, training loss: 649.1956176757812 = 1.724426507949829 + 100.0 * 6.474711894989014
Epoch 210, val loss: 1.7324246168136597
Epoch 220, training loss: 647.8765258789062 = 1.712276577949524 + 100.0 * 6.461642742156982
Epoch 220, val loss: 1.7214477062225342
Epoch 230, training loss: 646.9349365234375 = 1.6990303993225098 + 100.0 * 6.452358722686768
Epoch 230, val loss: 1.709722638130188
Epoch 240, training loss: 645.7745971679688 = 1.6848621368408203 + 100.0 * 6.440897464752197
Epoch 240, val loss: 1.6971603631973267
Epoch 250, training loss: 644.7147827148438 = 1.6697405576705933 + 100.0 * 6.430450439453125
Epoch 250, val loss: 1.683793544769287
Epoch 260, training loss: 644.216064453125 = 1.653451919555664 + 100.0 * 6.425626277923584
Epoch 260, val loss: 1.6694408655166626
Epoch 270, training loss: 643.0064697265625 = 1.635964035987854 + 100.0 * 6.413704872131348
Epoch 270, val loss: 1.6541613340377808
Epoch 280, training loss: 642.1622314453125 = 1.6173118352890015 + 100.0 * 6.405449390411377
Epoch 280, val loss: 1.6378237009048462
Epoch 290, training loss: 641.3759765625 = 1.5975587368011475 + 100.0 * 6.39778470993042
Epoch 290, val loss: 1.6205695867538452
Epoch 300, training loss: 640.5927124023438 = 1.5766903162002563 + 100.0 * 6.39016056060791
Epoch 300, val loss: 1.6024794578552246
Epoch 310, training loss: 640.5133056640625 = 1.5548763275146484 + 100.0 * 6.389584064483643
Epoch 310, val loss: 1.5835539102554321
Epoch 320, training loss: 639.5299682617188 = 1.5316002368927002 + 100.0 * 6.379983425140381
Epoch 320, val loss: 1.5636013746261597
Epoch 330, training loss: 638.7713623046875 = 1.507509708404541 + 100.0 * 6.372638702392578
Epoch 330, val loss: 1.5429866313934326
Epoch 340, training loss: 638.185302734375 = 1.4825639724731445 + 100.0 * 6.367027282714844
Epoch 340, val loss: 1.521768569946289
Epoch 350, training loss: 637.71533203125 = 1.4568365812301636 + 100.0 * 6.362585067749023
Epoch 350, val loss: 1.5000501871109009
Epoch 360, training loss: 637.4396362304688 = 1.4304627180099487 + 100.0 * 6.360091686248779
Epoch 360, val loss: 1.4780137538909912
Epoch 370, training loss: 636.7474365234375 = 1.403723120689392 + 100.0 * 6.3534369468688965
Epoch 370, val loss: 1.4555420875549316
Epoch 380, training loss: 636.2597045898438 = 1.376526951789856 + 100.0 * 6.348831653594971
Epoch 380, val loss: 1.4330651760101318
Epoch 390, training loss: 636.508544921875 = 1.3491252660751343 + 100.0 * 6.351593971252441
Epoch 390, val loss: 1.4104217290878296
Epoch 400, training loss: 635.6168212890625 = 1.3215004205703735 + 100.0 * 6.342953681945801
Epoch 400, val loss: 1.3880586624145508
Epoch 410, training loss: 635.06494140625 = 1.2938902378082275 + 100.0 * 6.337710380554199
Epoch 410, val loss: 1.3658217191696167
Epoch 420, training loss: 635.3525390625 = 1.2665077447891235 + 100.0 * 6.340860366821289
Epoch 420, val loss: 1.3437528610229492
Epoch 430, training loss: 634.40771484375 = 1.2387657165527344 + 100.0 * 6.331689357757568
Epoch 430, val loss: 1.321923851966858
Epoch 440, training loss: 634.037353515625 = 1.2115994691848755 + 100.0 * 6.3282575607299805
Epoch 440, val loss: 1.3004997968673706
Epoch 450, training loss: 633.7387084960938 = 1.1848018169403076 + 100.0 * 6.3255391120910645
Epoch 450, val loss: 1.279601812362671
Epoch 460, training loss: 633.7393188476562 = 1.158372402191162 + 100.0 * 6.325809478759766
Epoch 460, val loss: 1.2591629028320312
Epoch 470, training loss: 633.1068115234375 = 1.132269024848938 + 100.0 * 6.3197455406188965
Epoch 470, val loss: 1.239326000213623
Epoch 480, training loss: 632.8362426757812 = 1.1068048477172852 + 100.0 * 6.317294597625732
Epoch 480, val loss: 1.2201189994812012
Epoch 490, training loss: 633.1856079101562 = 1.081849217414856 + 100.0 * 6.321037769317627
Epoch 490, val loss: 1.2012994289398193
Epoch 500, training loss: 632.3742065429688 = 1.0572168827056885 + 100.0 * 6.313170433044434
Epoch 500, val loss: 1.1831138134002686
Epoch 510, training loss: 632.3935546875 = 1.0333930253982544 + 100.0 * 6.313601970672607
Epoch 510, val loss: 1.1654338836669922
Epoch 520, training loss: 631.986572265625 = 1.010023832321167 + 100.0 * 6.309765815734863
Epoch 520, val loss: 1.1485974788665771
Epoch 530, training loss: 631.654541015625 = 0.9873649477958679 + 100.0 * 6.306671619415283
Epoch 530, val loss: 1.132386326789856
Epoch 540, training loss: 631.3836669921875 = 0.9654207229614258 + 100.0 * 6.304183006286621
Epoch 540, val loss: 1.1170680522918701
Epoch 550, training loss: 631.6323852539062 = 0.9439659118652344 + 100.0 * 6.306884288787842
Epoch 550, val loss: 1.102280616760254
Epoch 560, training loss: 630.9517822265625 = 0.9232168197631836 + 100.0 * 6.300285816192627
Epoch 560, val loss: 1.0880553722381592
Epoch 570, training loss: 630.973388671875 = 0.903100311756134 + 100.0 * 6.300703048706055
Epoch 570, val loss: 1.0745855569839478
Epoch 580, training loss: 630.5882568359375 = 0.8834354281425476 + 100.0 * 6.297048091888428
Epoch 580, val loss: 1.0619269609451294
Epoch 590, training loss: 630.5068359375 = 0.8643882870674133 + 100.0 * 6.296424865722656
Epoch 590, val loss: 1.0497833490371704
Epoch 600, training loss: 630.6260986328125 = 0.8460571765899658 + 100.0 * 6.297800064086914
Epoch 600, val loss: 1.0383878946304321
Epoch 610, training loss: 630.2413330078125 = 0.8279171586036682 + 100.0 * 6.29413366317749
Epoch 610, val loss: 1.027305245399475
Epoch 620, training loss: 629.8287353515625 = 0.8105218410491943 + 100.0 * 6.290181636810303
Epoch 620, val loss: 1.0169799327850342
Epoch 630, training loss: 629.7511596679688 = 0.793624997138977 + 100.0 * 6.289575099945068
Epoch 630, val loss: 1.0072022676467896
Epoch 640, training loss: 629.7123413085938 = 0.7770844101905823 + 100.0 * 6.2893524169921875
Epoch 640, val loss: 0.9980051517486572
Epoch 650, training loss: 629.7268676757812 = 0.7609400153160095 + 100.0 * 6.28965950012207
Epoch 650, val loss: 0.9892590641975403
Epoch 660, training loss: 629.1603393554688 = 0.7451850771903992 + 100.0 * 6.284151554107666
Epoch 660, val loss: 0.9806127548217773
Epoch 670, training loss: 629.0912475585938 = 0.7299023866653442 + 100.0 * 6.283613204956055
Epoch 670, val loss: 0.972588300704956
Epoch 680, training loss: 629.3272705078125 = 0.7149905562400818 + 100.0 * 6.286122798919678
Epoch 680, val loss: 0.9651272296905518
Epoch 690, training loss: 629.2447509765625 = 0.7001100778579712 + 100.0 * 6.2854461669921875
Epoch 690, val loss: 0.9575848579406738
Epoch 700, training loss: 628.629638671875 = 0.6856638193130493 + 100.0 * 6.279439449310303
Epoch 700, val loss: 0.9505710005760193
Epoch 710, training loss: 628.5867919921875 = 0.6715755462646484 + 100.0 * 6.2791523933410645
Epoch 710, val loss: 0.9439746737480164
Epoch 720, training loss: 628.7931518554688 = 0.6576911211013794 + 100.0 * 6.2813544273376465
Epoch 720, val loss: 0.937494158744812
Epoch 730, training loss: 628.2686767578125 = 0.6441453099250793 + 100.0 * 6.2762451171875
Epoch 730, val loss: 0.9313482046127319
Epoch 740, training loss: 628.1294555664062 = 0.6307970285415649 + 100.0 * 6.274986743927002
Epoch 740, val loss: 0.9256112575531006
Epoch 750, training loss: 628.08154296875 = 0.6177590489387512 + 100.0 * 6.2746381759643555
Epoch 750, val loss: 0.919974684715271
Epoch 760, training loss: 628.04296875 = 0.6048088669776917 + 100.0 * 6.274381637573242
Epoch 760, val loss: 0.9143561720848083
Epoch 770, training loss: 627.894287109375 = 0.5920268893241882 + 100.0 * 6.273022651672363
Epoch 770, val loss: 0.9092925190925598
Epoch 780, training loss: 627.6699829101562 = 0.5795280337333679 + 100.0 * 6.270904541015625
Epoch 780, val loss: 0.9041944742202759
Epoch 790, training loss: 628.1674194335938 = 0.5672458410263062 + 100.0 * 6.276001453399658
Epoch 790, val loss: 0.8992832899093628
Epoch 800, training loss: 627.6478881835938 = 0.5551006197929382 + 100.0 * 6.270927429199219
Epoch 800, val loss: 0.8946606516838074
Epoch 810, training loss: 627.6867065429688 = 0.5431618690490723 + 100.0 * 6.271435260772705
Epoch 810, val loss: 0.8902667164802551
Epoch 820, training loss: 627.235595703125 = 0.5312705039978027 + 100.0 * 6.267043590545654
Epoch 820, val loss: 0.8859040141105652
Epoch 830, training loss: 627.2886962890625 = 0.5197063684463501 + 100.0 * 6.2676897048950195
Epoch 830, val loss: 0.8817809820175171
Epoch 840, training loss: 627.15234375 = 0.5083136558532715 + 100.0 * 6.266440391540527
Epoch 840, val loss: 0.877834677696228
Epoch 850, training loss: 626.9485473632812 = 0.4970109164714813 + 100.0 * 6.264515399932861
Epoch 850, val loss: 0.8740667104721069
Epoch 860, training loss: 627.1900634765625 = 0.48596474528312683 + 100.0 * 6.267041206359863
Epoch 860, val loss: 0.8705576062202454
Epoch 870, training loss: 626.8801879882812 = 0.47500598430633545 + 100.0 * 6.264052391052246
Epoch 870, val loss: 0.8668463826179504
Epoch 880, training loss: 626.676513671875 = 0.46423953771591187 + 100.0 * 6.262123107910156
Epoch 880, val loss: 0.863538920879364
Epoch 890, training loss: 626.7162475585938 = 0.4537605345249176 + 100.0 * 6.262625217437744
Epoch 890, val loss: 0.8603854775428772
Epoch 900, training loss: 626.6456298828125 = 0.4433434009552002 + 100.0 * 6.262022972106934
Epoch 900, val loss: 0.8572893738746643
Epoch 910, training loss: 626.4199829101562 = 0.4330250322818756 + 100.0 * 6.259869575500488
Epoch 910, val loss: 0.8543990850448608
Epoch 920, training loss: 626.3017578125 = 0.42298734188079834 + 100.0 * 6.258788108825684
Epoch 920, val loss: 0.8517162203788757
Epoch 930, training loss: 626.2859497070312 = 0.413277268409729 + 100.0 * 6.258727073669434
Epoch 930, val loss: 0.8493981957435608
Epoch 940, training loss: 626.5482177734375 = 0.40363168716430664 + 100.0 * 6.261445999145508
Epoch 940, val loss: 0.8469732999801636
Epoch 950, training loss: 626.199462890625 = 0.3942493498325348 + 100.0 * 6.258052349090576
Epoch 950, val loss: 0.8448257446289062
Epoch 960, training loss: 626.3560791015625 = 0.38500872254371643 + 100.0 * 6.259710788726807
Epoch 960, val loss: 0.8427165746688843
Epoch 970, training loss: 626.3712768554688 = 0.3759515881538391 + 100.0 * 6.259953022003174
Epoch 970, val loss: 0.8410727381706238
Epoch 980, training loss: 625.8651733398438 = 0.36698949337005615 + 100.0 * 6.254981517791748
Epoch 980, val loss: 0.8390949964523315
Epoch 990, training loss: 625.7076416015625 = 0.3583504259586334 + 100.0 * 6.253492832183838
Epoch 990, val loss: 0.8377758264541626
Epoch 1000, training loss: 625.630615234375 = 0.35000520944595337 + 100.0 * 6.252806186676025
Epoch 1000, val loss: 0.8364704847335815
Epoch 1010, training loss: 625.7835693359375 = 0.34185895323753357 + 100.0 * 6.2544169425964355
Epoch 1010, val loss: 0.8354736566543579
Epoch 1020, training loss: 625.8863525390625 = 0.33375367522239685 + 100.0 * 6.255526065826416
Epoch 1020, val loss: 0.8344554305076599
Epoch 1030, training loss: 625.6337280273438 = 0.32578346133232117 + 100.0 * 6.253079414367676
Epoch 1030, val loss: 0.8332793712615967
Epoch 1040, training loss: 625.3427124023438 = 0.31815609335899353 + 100.0 * 6.250245094299316
Epoch 1040, val loss: 0.8327159881591797
Epoch 1050, training loss: 625.39697265625 = 0.31078773736953735 + 100.0 * 6.250861644744873
Epoch 1050, val loss: 0.832267701625824
Epoch 1060, training loss: 625.6394653320312 = 0.3035449981689453 + 100.0 * 6.253359317779541
Epoch 1060, val loss: 0.8318835496902466
Epoch 1070, training loss: 625.2398071289062 = 0.296356737613678 + 100.0 * 6.249434471130371
Epoch 1070, val loss: 0.8315885663032532
Epoch 1080, training loss: 625.1520385742188 = 0.28948429226875305 + 100.0 * 6.2486252784729
Epoch 1080, val loss: 0.8314954042434692
Epoch 1090, training loss: 625.148193359375 = 0.2827964723110199 + 100.0 * 6.248653888702393
Epoch 1090, val loss: 0.8316166400909424
Epoch 1100, training loss: 624.955322265625 = 0.2762555181980133 + 100.0 * 6.246790885925293
Epoch 1100, val loss: 0.8318566083908081
Epoch 1110, training loss: 625.1633911132812 = 0.26992568373680115 + 100.0 * 6.248934745788574
Epoch 1110, val loss: 0.8322702050209045
Epoch 1120, training loss: 625.0819091796875 = 0.2636133134365082 + 100.0 * 6.248183250427246
Epoch 1120, val loss: 0.8324888944625854
Epoch 1130, training loss: 624.9765014648438 = 0.25753727555274963 + 100.0 * 6.247189998626709
Epoch 1130, val loss: 0.8330715894699097
Epoch 1140, training loss: 624.7410278320312 = 0.2516588270664215 + 100.0 * 6.244893550872803
Epoch 1140, val loss: 0.8339430093765259
Epoch 1150, training loss: 624.6896362304688 = 0.24600380659103394 + 100.0 * 6.244436264038086
Epoch 1150, val loss: 0.8349577784538269
Epoch 1160, training loss: 625.3397216796875 = 0.24049468338489532 + 100.0 * 6.250992298126221
Epoch 1160, val loss: 0.8358061909675598
Epoch 1170, training loss: 624.821044921875 = 0.2348739504814148 + 100.0 * 6.245861530303955
Epoch 1170, val loss: 0.8367062211036682
Epoch 1180, training loss: 624.4785766601562 = 0.22955825924873352 + 100.0 * 6.242490291595459
Epoch 1180, val loss: 0.8379558324813843
Epoch 1190, training loss: 624.4639282226562 = 0.22445732355117798 + 100.0 * 6.24239444732666
Epoch 1190, val loss: 0.8394037485122681
Epoch 1200, training loss: 624.8350830078125 = 0.219459667801857 + 100.0 * 6.246155738830566
Epoch 1200, val loss: 0.8407920002937317
Epoch 1210, training loss: 624.751220703125 = 0.214508056640625 + 100.0 * 6.24536657333374
Epoch 1210, val loss: 0.8421331644058228
Epoch 1220, training loss: 624.3602294921875 = 0.20969507098197937 + 100.0 * 6.241505146026611
Epoch 1220, val loss: 0.8436505198478699
Epoch 1230, training loss: 624.2924194335938 = 0.2050551027059555 + 100.0 * 6.240873336791992
Epoch 1230, val loss: 0.8453754782676697
Epoch 1240, training loss: 624.3828125 = 0.2005765289068222 + 100.0 * 6.241822242736816
Epoch 1240, val loss: 0.8471397161483765
Epoch 1250, training loss: 624.2269287109375 = 0.19614548981189728 + 100.0 * 6.240307807922363
Epoch 1250, val loss: 0.8490477800369263
Epoch 1260, training loss: 624.1403198242188 = 0.19181641936302185 + 100.0 * 6.239484786987305
Epoch 1260, val loss: 0.8510052561759949
Epoch 1270, training loss: 624.076416015625 = 0.1876513957977295 + 100.0 * 6.238887786865234
Epoch 1270, val loss: 0.8530396819114685
Epoch 1280, training loss: 624.4653930664062 = 0.18361492455005646 + 100.0 * 6.2428178787231445
Epoch 1280, val loss: 0.8552883863449097
Epoch 1290, training loss: 624.26611328125 = 0.179546520113945 + 100.0 * 6.240865230560303
Epoch 1290, val loss: 0.8572528958320618
Epoch 1300, training loss: 624.1267700195312 = 0.1755867600440979 + 100.0 * 6.239511489868164
Epoch 1300, val loss: 0.8593419194221497
Epoch 1310, training loss: 623.9090576171875 = 0.17179559171199799 + 100.0 * 6.237372398376465
Epoch 1310, val loss: 0.8617581129074097
Epoch 1320, training loss: 623.8919677734375 = 0.16814108192920685 + 100.0 * 6.23723840713501
Epoch 1320, val loss: 0.864170491695404
Epoch 1330, training loss: 624.222900390625 = 0.16453313827514648 + 100.0 * 6.240583419799805
Epoch 1330, val loss: 0.8665685057640076
Epoch 1340, training loss: 623.9573974609375 = 0.16099736094474792 + 100.0 * 6.237963676452637
Epoch 1340, val loss: 0.8692185282707214
Epoch 1350, training loss: 624.0172119140625 = 0.1575474739074707 + 100.0 * 6.2385969161987305
Epoch 1350, val loss: 0.8715838193893433
Epoch 1360, training loss: 624.1801147460938 = 0.15420839190483093 + 100.0 * 6.240258693695068
Epoch 1360, val loss: 0.8742210268974304
Epoch 1370, training loss: 623.7192993164062 = 0.1508859395980835 + 100.0 * 6.235683917999268
Epoch 1370, val loss: 0.8766979575157166
Epoch 1380, training loss: 623.59423828125 = 0.14768366515636444 + 100.0 * 6.2344651222229
Epoch 1380, val loss: 0.879421591758728
Epoch 1390, training loss: 623.5079345703125 = 0.144622802734375 + 100.0 * 6.233633518218994
Epoch 1390, val loss: 0.8823117017745972
Epoch 1400, training loss: 623.6282958984375 = 0.14164917171001434 + 100.0 * 6.234866619110107
Epoch 1400, val loss: 0.8851808905601501
Epoch 1410, training loss: 623.680908203125 = 0.1386660933494568 + 100.0 * 6.235422134399414
Epoch 1410, val loss: 0.8877384662628174
Epoch 1420, training loss: 624.062744140625 = 0.1357613503932953 + 100.0 * 6.239269733428955
Epoch 1420, val loss: 0.8904145359992981
Epoch 1430, training loss: 623.5639038085938 = 0.1328708678483963 + 100.0 * 6.234310150146484
Epoch 1430, val loss: 0.8935751914978027
Epoch 1440, training loss: 623.3455810546875 = 0.1301276832818985 + 100.0 * 6.232154846191406
Epoch 1440, val loss: 0.8963379859924316
Epoch 1450, training loss: 623.26318359375 = 0.12749336659908295 + 100.0 * 6.231357097625732
Epoch 1450, val loss: 0.8995828032493591
Epoch 1460, training loss: 623.2617797851562 = 0.12492968887090683 + 100.0 * 6.231368541717529
Epoch 1460, val loss: 0.9027509689331055
Epoch 1470, training loss: 623.809326171875 = 0.12244600057601929 + 100.0 * 6.236868858337402
Epoch 1470, val loss: 0.9058193564414978
Epoch 1480, training loss: 623.9970703125 = 0.11990899592638016 + 100.0 * 6.238771438598633
Epoch 1480, val loss: 0.9085122346878052
Epoch 1490, training loss: 623.50830078125 = 0.11739295721054077 + 100.0 * 6.2339091300964355
Epoch 1490, val loss: 0.9117034673690796
Epoch 1500, training loss: 623.1163940429688 = 0.11501820385456085 + 100.0 * 6.230014324188232
Epoch 1500, val loss: 0.9146154522895813
Epoch 1510, training loss: 623.1516723632812 = 0.11273729056119919 + 100.0 * 6.23038911819458
Epoch 1510, val loss: 0.9179235696792603
Epoch 1520, training loss: 623.4484252929688 = 0.11052453517913818 + 100.0 * 6.233378887176514
Epoch 1520, val loss: 0.9210841059684753
Epoch 1530, training loss: 623.2250366210938 = 0.10831890255212784 + 100.0 * 6.231166839599609
Epoch 1530, val loss: 0.92451012134552
Epoch 1540, training loss: 623.0712280273438 = 0.10617558658123016 + 100.0 * 6.229650497436523
Epoch 1540, val loss: 0.9275683164596558
Epoch 1550, training loss: 623.01611328125 = 0.10410542041063309 + 100.0 * 6.229119777679443
Epoch 1550, val loss: 0.9307143092155457
Epoch 1560, training loss: 623.1272583007812 = 0.10210314393043518 + 100.0 * 6.230251312255859
Epoch 1560, val loss: 0.9342462420463562
Epoch 1570, training loss: 623.2425537109375 = 0.10010600835084915 + 100.0 * 6.231424331665039
Epoch 1570, val loss: 0.937407910823822
Epoch 1580, training loss: 623.0632934570312 = 0.09812427312135696 + 100.0 * 6.22965145111084
Epoch 1580, val loss: 0.9408373236656189
Epoch 1590, training loss: 623.1900024414062 = 0.09625474363565445 + 100.0 * 6.230937480926514
Epoch 1590, val loss: 0.9442551136016846
Epoch 1600, training loss: 622.86474609375 = 0.09438619762659073 + 100.0 * 6.227704048156738
Epoch 1600, val loss: 0.9474551677703857
Epoch 1610, training loss: 623.3942260742188 = 0.09262088686227798 + 100.0 * 6.233016014099121
Epoch 1610, val loss: 0.9511038064956665
Epoch 1620, training loss: 622.8343505859375 = 0.09079146385192871 + 100.0 * 6.227435111999512
Epoch 1620, val loss: 0.9542933702468872
Epoch 1630, training loss: 622.7107543945312 = 0.089082732796669 + 100.0 * 6.2262163162231445
Epoch 1630, val loss: 0.9578885436058044
Epoch 1640, training loss: 622.630859375 = 0.08742907643318176 + 100.0 * 6.225434303283691
Epoch 1640, val loss: 0.9612827897071838
Epoch 1650, training loss: 622.6959838867188 = 0.08582361042499542 + 100.0 * 6.226101398468018
Epoch 1650, val loss: 0.9648482799530029
Epoch 1660, training loss: 623.5191040039062 = 0.08422419428825378 + 100.0 * 6.234348773956299
Epoch 1660, val loss: 0.9683831334114075
Epoch 1670, training loss: 622.866455078125 = 0.08259864896535873 + 100.0 * 6.22783899307251
Epoch 1670, val loss: 0.9714642763137817
Epoch 1680, training loss: 622.65087890625 = 0.08105066418647766 + 100.0 * 6.225698471069336
Epoch 1680, val loss: 0.9750655293464661
Epoch 1690, training loss: 622.5774536132812 = 0.07957576215267181 + 100.0 * 6.224978446960449
Epoch 1690, val loss: 0.9789009094238281
Epoch 1700, training loss: 622.5390625 = 0.0781540721654892 + 100.0 * 6.224609375
Epoch 1700, val loss: 0.9826246500015259
Epoch 1710, training loss: 622.8822021484375 = 0.0767614096403122 + 100.0 * 6.228054046630859
Epoch 1710, val loss: 0.9862369298934937
Epoch 1720, training loss: 622.5409545898438 = 0.07535173743963242 + 100.0 * 6.224655628204346
Epoch 1720, val loss: 0.9895625114440918
Epoch 1730, training loss: 622.733642578125 = 0.0739855244755745 + 100.0 * 6.226596832275391
Epoch 1730, val loss: 0.9931992292404175
Epoch 1740, training loss: 622.6747436523438 = 0.07264220714569092 + 100.0 * 6.2260212898254395
Epoch 1740, val loss: 0.9968580603599548
Epoch 1750, training loss: 622.5027465820312 = 0.07132866978645325 + 100.0 * 6.224314212799072
Epoch 1750, val loss: 1.0003976821899414
Epoch 1760, training loss: 622.3689575195312 = 0.07008301466703415 + 100.0 * 6.222988605499268
Epoch 1760, val loss: 1.0039465427398682
Epoch 1770, training loss: 622.34423828125 = 0.0688544362783432 + 100.0 * 6.222754001617432
Epoch 1770, val loss: 1.0076699256896973
Epoch 1780, training loss: 622.7958374023438 = 0.06765933334827423 + 100.0 * 6.22728157043457
Epoch 1780, val loss: 1.0113438367843628
Epoch 1790, training loss: 622.44921875 = 0.06645037978887558 + 100.0 * 6.223827362060547
Epoch 1790, val loss: 1.0146057605743408
Epoch 1800, training loss: 622.283447265625 = 0.0652872771024704 + 100.0 * 6.22218132019043
Epoch 1800, val loss: 1.0185959339141846
Epoch 1810, training loss: 622.4606323242188 = 0.06417129933834076 + 100.0 * 6.223964691162109
Epoch 1810, val loss: 1.0218298435211182
Epoch 1820, training loss: 622.3638916015625 = 0.06306476891040802 + 100.0 * 6.223008155822754
Epoch 1820, val loss: 1.0257315635681152
Epoch 1830, training loss: 622.33349609375 = 0.06196116283535957 + 100.0 * 6.222715377807617
Epoch 1830, val loss: 1.0289721488952637
Epoch 1840, training loss: 622.1528930664062 = 0.06089150905609131 + 100.0 * 6.220919609069824
Epoch 1840, val loss: 1.0331573486328125
Epoch 1850, training loss: 622.1617431640625 = 0.05986882373690605 + 100.0 * 6.2210187911987305
Epoch 1850, val loss: 1.0367364883422852
Epoch 1860, training loss: 622.5845947265625 = 0.058870479464530945 + 100.0 * 6.22525691986084
Epoch 1860, val loss: 1.0406378507614136
Epoch 1870, training loss: 622.09130859375 = 0.05786754563450813 + 100.0 * 6.220334529876709
Epoch 1870, val loss: 1.0439143180847168
Epoch 1880, training loss: 622.5098876953125 = 0.056907497346401215 + 100.0 * 6.22452974319458
Epoch 1880, val loss: 1.0477032661437988
Epoch 1890, training loss: 622.0985717773438 = 0.05592774972319603 + 100.0 * 6.220426559448242
Epoch 1890, val loss: 1.0512611865997314
Epoch 1900, training loss: 622.1322021484375 = 0.05500774085521698 + 100.0 * 6.2207722663879395
Epoch 1900, val loss: 1.055167555809021
Epoch 1910, training loss: 622.089111328125 = 0.05410216376185417 + 100.0 * 6.22035026550293
Epoch 1910, val loss: 1.0588716268539429
Epoch 1920, training loss: 622.1337890625 = 0.053216684609651566 + 100.0 * 6.220805644989014
Epoch 1920, val loss: 1.0628350973129272
Epoch 1930, training loss: 622.1446533203125 = 0.0523403137922287 + 100.0 * 6.220922946929932
Epoch 1930, val loss: 1.0661927461624146
Epoch 1940, training loss: 622.041748046875 = 0.05148983746767044 + 100.0 * 6.219902515411377
Epoch 1940, val loss: 1.0698548555374146
Epoch 1950, training loss: 622.1504516601562 = 0.050659988075494766 + 100.0 * 6.2209978103637695
Epoch 1950, val loss: 1.073663592338562
Epoch 1960, training loss: 622.013671875 = 0.049838289618492126 + 100.0 * 6.219638347625732
Epoch 1960, val loss: 1.0773764848709106
Epoch 1970, training loss: 621.9178466796875 = 0.04904453828930855 + 100.0 * 6.218688011169434
Epoch 1970, val loss: 1.0812005996704102
Epoch 1980, training loss: 621.7908935546875 = 0.048271361738443375 + 100.0 * 6.217426300048828
Epoch 1980, val loss: 1.0849542617797852
Epoch 1990, training loss: 622.362060546875 = 0.04753419756889343 + 100.0 * 6.223145008087158
Epoch 1990, val loss: 1.0886722803115845
Epoch 2000, training loss: 621.9149169921875 = 0.046746138483285904 + 100.0 * 6.218681335449219
Epoch 2000, val loss: 1.0923806428909302
Epoch 2010, training loss: 621.9223022460938 = 0.04599684476852417 + 100.0 * 6.2187628746032715
Epoch 2010, val loss: 1.0958174467086792
Epoch 2020, training loss: 622.3388061523438 = 0.04527592286467552 + 100.0 * 6.222935199737549
Epoch 2020, val loss: 1.0995197296142578
Epoch 2030, training loss: 621.8714599609375 = 0.04455318674445152 + 100.0 * 6.218269348144531
Epoch 2030, val loss: 1.1030691862106323
Epoch 2040, training loss: 621.6962890625 = 0.04386470094323158 + 100.0 * 6.216524124145508
Epoch 2040, val loss: 1.1069848537445068
Epoch 2050, training loss: 621.7080078125 = 0.04320094361901283 + 100.0 * 6.216648101806641
Epoch 2050, val loss: 1.1106235980987549
Epoch 2060, training loss: 622.1860961914062 = 0.04255995154380798 + 100.0 * 6.221435546875
Epoch 2060, val loss: 1.114017128944397
Epoch 2070, training loss: 621.9093017578125 = 0.041887588798999786 + 100.0 * 6.218674182891846
Epoch 2070, val loss: 1.1180589199066162
Epoch 2080, training loss: 621.7636108398438 = 0.04124309867620468 + 100.0 * 6.217223644256592
Epoch 2080, val loss: 1.121408462524414
Epoch 2090, training loss: 621.6827392578125 = 0.04061774164438248 + 100.0 * 6.216421604156494
Epoch 2090, val loss: 1.1255813837051392
Epoch 2100, training loss: 621.6825561523438 = 0.040014345198869705 + 100.0 * 6.21642541885376
Epoch 2100, val loss: 1.1290429830551147
Epoch 2110, training loss: 622.0471801757812 = 0.03942292183637619 + 100.0 * 6.2200775146484375
Epoch 2110, val loss: 1.1329421997070312
Epoch 2120, training loss: 621.7039184570312 = 0.03882234916090965 + 100.0 * 6.21665096282959
Epoch 2120, val loss: 1.1362017393112183
Epoch 2130, training loss: 621.5637817382812 = 0.0382404699921608 + 100.0 * 6.215255260467529
Epoch 2130, val loss: 1.139945149421692
Epoch 2140, training loss: 621.8373413085938 = 0.037688255310058594 + 100.0 * 6.217997074127197
Epoch 2140, val loss: 1.1433855295181274
Epoch 2150, training loss: 621.52880859375 = 0.03712031990289688 + 100.0 * 6.214917182922363
Epoch 2150, val loss: 1.147071361541748
Epoch 2160, training loss: 621.4813842773438 = 0.03657999634742737 + 100.0 * 6.21444845199585
Epoch 2160, val loss: 1.150757074356079
Epoch 2170, training loss: 621.8259887695312 = 0.03606068715453148 + 100.0 * 6.217899322509766
Epoch 2170, val loss: 1.1542963981628418
Epoch 2180, training loss: 622.1318969726562 = 0.035535238683223724 + 100.0 * 6.220963478088379
Epoch 2180, val loss: 1.1579039096832275
Epoch 2190, training loss: 621.4795532226562 = 0.03498450666666031 + 100.0 * 6.214446067810059
Epoch 2190, val loss: 1.1613537073135376
Epoch 2200, training loss: 621.3446044921875 = 0.034487947821617126 + 100.0 * 6.213100910186768
Epoch 2200, val loss: 1.1649024486541748
Epoch 2210, training loss: 621.2857055664062 = 0.034010857343673706 + 100.0 * 6.212516784667969
Epoch 2210, val loss: 1.1686574220657349
Epoch 2220, training loss: 621.3822021484375 = 0.03355097025632858 + 100.0 * 6.213486194610596
Epoch 2220, val loss: 1.1722843647003174
Epoch 2230, training loss: 621.8731689453125 = 0.03308657556772232 + 100.0 * 6.218400955200195
Epoch 2230, val loss: 1.1757545471191406
Epoch 2240, training loss: 621.9105834960938 = 0.03259703889489174 + 100.0 * 6.218780040740967
Epoch 2240, val loss: 1.1790517568588257
Epoch 2250, training loss: 621.340087890625 = 0.03213861584663391 + 100.0 * 6.213079452514648
Epoch 2250, val loss: 1.1828749179840088
Epoch 2260, training loss: 621.26123046875 = 0.031690530478954315 + 100.0 * 6.2122955322265625
Epoch 2260, val loss: 1.1860857009887695
Epoch 2270, training loss: 621.314697265625 = 0.03125918656587601 + 100.0 * 6.212834358215332
Epoch 2270, val loss: 1.1897644996643066
Epoch 2280, training loss: 621.6016235351562 = 0.030825965106487274 + 100.0 * 6.215708255767822
Epoch 2280, val loss: 1.193131446838379
Epoch 2290, training loss: 621.2222900390625 = 0.03039931133389473 + 100.0 * 6.211918830871582
Epoch 2290, val loss: 1.1967518329620361
Epoch 2300, training loss: 621.1289672851562 = 0.029989056289196014 + 100.0 * 6.210989952087402
Epoch 2300, val loss: 1.2000423669815063
Epoch 2310, training loss: 621.5590209960938 = 0.02960347943007946 + 100.0 * 6.215293884277344
Epoch 2310, val loss: 1.203345537185669
Epoch 2320, training loss: 621.1029663085938 = 0.02918563410639763 + 100.0 * 6.210737705230713
Epoch 2320, val loss: 1.20737624168396
Epoch 2330, training loss: 621.0462036132812 = 0.028784241527318954 + 100.0 * 6.210174083709717
Epoch 2330, val loss: 1.2102501392364502
Epoch 2340, training loss: 621.044189453125 = 0.028406288474798203 + 100.0 * 6.210157871246338
Epoch 2340, val loss: 1.214126706123352
Epoch 2350, training loss: 621.495361328125 = 0.028042515739798546 + 100.0 * 6.2146735191345215
Epoch 2350, val loss: 1.21737539768219
Epoch 2360, training loss: 621.0240478515625 = 0.027662234380841255 + 100.0 * 6.209963798522949
Epoch 2360, val loss: 1.2207305431365967
Epoch 2370, training loss: 621.3118896484375 = 0.027307134121656418 + 100.0 * 6.212845802307129
Epoch 2370, val loss: 1.2239307165145874
Epoch 2380, training loss: 621.1456909179688 = 0.026941554620862007 + 100.0 * 6.211187839508057
Epoch 2380, val loss: 1.2275474071502686
Epoch 2390, training loss: 621.0831298828125 = 0.026583746075630188 + 100.0 * 6.210565090179443
Epoch 2390, val loss: 1.2307662963867188
Epoch 2400, training loss: 620.9852294921875 = 0.02624756470322609 + 100.0 * 6.209589958190918
Epoch 2400, val loss: 1.2343106269836426
Epoch 2410, training loss: 621.4557495117188 = 0.02592005766928196 + 100.0 * 6.214298248291016
Epoch 2410, val loss: 1.237591028213501
Epoch 2420, training loss: 621.049072265625 = 0.02558136358857155 + 100.0 * 6.210235118865967
Epoch 2420, val loss: 1.2404956817626953
Epoch 2430, training loss: 620.9619140625 = 0.02524999901652336 + 100.0 * 6.209366321563721
Epoch 2430, val loss: 1.2443753480911255
Epoch 2440, training loss: 621.392333984375 = 0.02493445947766304 + 100.0 * 6.2136735916137695
Epoch 2440, val loss: 1.2471914291381836
Epoch 2450, training loss: 621.2033081054688 = 0.024612775072455406 + 100.0 * 6.211787223815918
Epoch 2450, val loss: 1.250986933708191
Epoch 2460, training loss: 620.8875732421875 = 0.02429836615920067 + 100.0 * 6.208632946014404
Epoch 2460, val loss: 1.2536782026290894
Epoch 2470, training loss: 620.8836059570312 = 0.0240007434040308 + 100.0 * 6.208596229553223
Epoch 2470, val loss: 1.2573628425598145
Epoch 2480, training loss: 620.873046875 = 0.02371121570467949 + 100.0 * 6.208493709564209
Epoch 2480, val loss: 1.2604396343231201
Epoch 2490, training loss: 621.2907104492188 = 0.02342640608549118 + 100.0 * 6.212672710418701
Epoch 2490, val loss: 1.2640068531036377
Epoch 2500, training loss: 621.1497192382812 = 0.02312428317964077 + 100.0 * 6.211266040802002
Epoch 2500, val loss: 1.2669733762741089
Epoch 2510, training loss: 620.875244140625 = 0.02283652313053608 + 100.0 * 6.208524227142334
Epoch 2510, val loss: 1.2700368165969849
Epoch 2520, training loss: 620.8450317382812 = 0.022561542689800262 + 100.0 * 6.208224296569824
Epoch 2520, val loss: 1.272932529449463
Epoch 2530, training loss: 621.0784301757812 = 0.022298071533441544 + 100.0 * 6.210561752319336
Epoch 2530, val loss: 1.2763961553573608
Epoch 2540, training loss: 620.9329833984375 = 0.022022083401679993 + 100.0 * 6.209109783172607
Epoch 2540, val loss: 1.2795233726501465
Epoch 2550, training loss: 620.7655029296875 = 0.0217472892254591 + 100.0 * 6.207437992095947
Epoch 2550, val loss: 1.28273606300354
Epoch 2560, training loss: 620.7557373046875 = 0.02149394154548645 + 100.0 * 6.207342624664307
Epoch 2560, val loss: 1.2856861352920532
Epoch 2570, training loss: 620.9816284179688 = 0.0212482251226902 + 100.0 * 6.209603786468506
Epoch 2570, val loss: 1.2890300750732422
Epoch 2580, training loss: 620.8754272460938 = 0.02098793350160122 + 100.0 * 6.2085442543029785
Epoch 2580, val loss: 1.2921735048294067
Epoch 2590, training loss: 620.8477783203125 = 0.020733017474412918 + 100.0 * 6.208270072937012
Epoch 2590, val loss: 1.2950077056884766
Epoch 2600, training loss: 620.8646850585938 = 0.020486842840909958 + 100.0 * 6.208441734313965
Epoch 2600, val loss: 1.2982245683670044
Epoch 2610, training loss: 620.6699829101562 = 0.02024916000664234 + 100.0 * 6.2064971923828125
Epoch 2610, val loss: 1.3015508651733398
Epoch 2620, training loss: 620.9889526367188 = 0.020023398101329803 + 100.0 * 6.209689617156982
Epoch 2620, val loss: 1.3047212362289429
Epoch 2630, training loss: 620.6079711914062 = 0.01978558860719204 + 100.0 * 6.2058820724487305
Epoch 2630, val loss: 1.3076173067092896
Epoch 2640, training loss: 620.6450805664062 = 0.01956416480243206 + 100.0 * 6.206254959106445
Epoch 2640, val loss: 1.3104748725891113
Epoch 2650, training loss: 620.722900390625 = 0.019342802464962006 + 100.0 * 6.207035541534424
Epoch 2650, val loss: 1.3136541843414307
Epoch 2660, training loss: 620.8787841796875 = 0.019123932346701622 + 100.0 * 6.208596706390381
Epoch 2660, val loss: 1.3168023824691772
Epoch 2670, training loss: 621.0194702148438 = 0.01890585757791996 + 100.0 * 6.210005760192871
Epoch 2670, val loss: 1.319563627243042
Epoch 2680, training loss: 620.854248046875 = 0.018685823306441307 + 100.0 * 6.20835542678833
Epoch 2680, val loss: 1.3221182823181152
Epoch 2690, training loss: 620.6566162109375 = 0.01847558096051216 + 100.0 * 6.206381320953369
Epoch 2690, val loss: 1.3250925540924072
Epoch 2700, training loss: 620.5515747070312 = 0.018272845074534416 + 100.0 * 6.205333232879639
Epoch 2700, val loss: 1.3283946514129639
Epoch 2710, training loss: 620.4769287109375 = 0.01807495951652527 + 100.0 * 6.204588890075684
Epoch 2710, val loss: 1.3312684297561646
Epoch 2720, training loss: 621.0604248046875 = 0.01788508892059326 + 100.0 * 6.21042537689209
Epoch 2720, val loss: 1.3339768648147583
Epoch 2730, training loss: 620.7318115234375 = 0.017678074538707733 + 100.0 * 6.207141399383545
Epoch 2730, val loss: 1.3373925685882568
Epoch 2740, training loss: 620.47265625 = 0.017483888193964958 + 100.0 * 6.204552173614502
Epoch 2740, val loss: 1.3394380807876587
Epoch 2750, training loss: 620.5488891601562 = 0.017297567799687386 + 100.0 * 6.205316066741943
Epoch 2750, val loss: 1.3427599668502808
Epoch 2760, training loss: 620.7850952148438 = 0.017114264890551567 + 100.0 * 6.2076802253723145
Epoch 2760, val loss: 1.3455665111541748
Epoch 2770, training loss: 620.455078125 = 0.016926271840929985 + 100.0 * 6.204381465911865
Epoch 2770, val loss: 1.3486593961715698
Epoch 2780, training loss: 620.3895263671875 = 0.01674601435661316 + 100.0 * 6.203727722167969
Epoch 2780, val loss: 1.3514113426208496
Epoch 2790, training loss: 621.061279296875 = 0.01657266914844513 + 100.0 * 6.210446834564209
Epoch 2790, val loss: 1.3540568351745605
Epoch 2800, training loss: 620.4966430664062 = 0.01639314740896225 + 100.0 * 6.204802513122559
Epoch 2800, val loss: 1.3567866086959839
Epoch 2810, training loss: 620.3952026367188 = 0.016219308599829674 + 100.0 * 6.203789710998535
Epoch 2810, val loss: 1.3594492673873901
Epoch 2820, training loss: 620.8270874023438 = 0.01605665683746338 + 100.0 * 6.208110332489014
Epoch 2820, val loss: 1.3623359203338623
Epoch 2830, training loss: 620.2701416015625 = 0.015877418220043182 + 100.0 * 6.202542781829834
Epoch 2830, val loss: 1.3649100065231323
Epoch 2840, training loss: 620.33056640625 = 0.015711141750216484 + 100.0 * 6.20314884185791
Epoch 2840, val loss: 1.3676912784576416
Epoch 2850, training loss: 620.3888549804688 = 0.015559251420199871 + 100.0 * 6.203732967376709
Epoch 2850, val loss: 1.3706214427947998
Epoch 2860, training loss: 620.6578979492188 = 0.015404517762362957 + 100.0 * 6.206425189971924
Epoch 2860, val loss: 1.3732733726501465
Epoch 2870, training loss: 620.3888549804688 = 0.015241004526615143 + 100.0 * 6.203735828399658
Epoch 2870, val loss: 1.3752541542053223
Epoch 2880, training loss: 620.3760375976562 = 0.01508825272321701 + 100.0 * 6.203609466552734
Epoch 2880, val loss: 1.3782892227172852
Epoch 2890, training loss: 620.508056640625 = 0.014934728853404522 + 100.0 * 6.204930782318115
Epoch 2890, val loss: 1.3805606365203857
Epoch 2900, training loss: 620.588623046875 = 0.014783835038542747 + 100.0 * 6.205738544464111
Epoch 2900, val loss: 1.3831015825271606
Epoch 2910, training loss: 620.3359375 = 0.014634373597800732 + 100.0 * 6.203212738037109
Epoch 2910, val loss: 1.3858798742294312
Epoch 2920, training loss: 620.3165893554688 = 0.014492846094071865 + 100.0 * 6.20302152633667
Epoch 2920, val loss: 1.3884146213531494
Epoch 2930, training loss: 620.7291259765625 = 0.01435558870434761 + 100.0 * 6.20714807510376
Epoch 2930, val loss: 1.390881896018982
Epoch 2940, training loss: 620.2534790039062 = 0.014202356338500977 + 100.0 * 6.202392578125
Epoch 2940, val loss: 1.3935844898223877
Epoch 2950, training loss: 620.1910400390625 = 0.014064894989132881 + 100.0 * 6.201770305633545
Epoch 2950, val loss: 1.396276593208313
Epoch 2960, training loss: 620.1168212890625 = 0.013932699337601662 + 100.0 * 6.201028823852539
Epoch 2960, val loss: 1.3986234664916992
Epoch 2970, training loss: 620.787841796875 = 0.01380600593984127 + 100.0 * 6.207740783691406
Epoch 2970, val loss: 1.400770664215088
Epoch 2980, training loss: 620.2139892578125 = 0.013666758313775063 + 100.0 * 6.202003479003906
Epoch 2980, val loss: 1.403838038444519
Epoch 2990, training loss: 620.4500732421875 = 0.01353259477764368 + 100.0 * 6.204365253448486
Epoch 2990, val loss: 1.4058064222335815
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8386926726410122
The final CL Acc:0.74321, 0.01968, The final GNN Acc:0.83746, 0.00108
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11664])
remove edge: torch.Size([2, 9484])
updated graph: torch.Size([2, 10592])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6441650390625 = 1.9599766731262207 + 100.0 * 8.596841812133789
Epoch 0, val loss: 1.9585187435150146
Epoch 10, training loss: 861.5656127929688 = 1.9508463144302368 + 100.0 * 8.596147537231445
Epoch 10, val loss: 1.9489665031433105
Epoch 20, training loss: 861.1471557617188 = 1.9394540786743164 + 100.0 * 8.592077255249023
Epoch 20, val loss: 1.93701171875
Epoch 30, training loss: 858.6234130859375 = 1.9247040748596191 + 100.0 * 8.566987037658691
Epoch 30, val loss: 1.9217519760131836
Epoch 40, training loss: 844.5980834960938 = 1.9057968854904175 + 100.0 * 8.426922798156738
Epoch 40, val loss: 1.903146505355835
Epoch 50, training loss: 783.7296752929688 = 1.8830487728118896 + 100.0 * 7.8184661865234375
Epoch 50, val loss: 1.881774663925171
Epoch 60, training loss: 734.036865234375 = 1.8667072057724 + 100.0 * 7.321701526641846
Epoch 60, val loss: 1.8683404922485352
Epoch 70, training loss: 703.503662109375 = 1.8552981615066528 + 100.0 * 7.016483783721924
Epoch 70, val loss: 1.8576383590698242
Epoch 80, training loss: 687.7508544921875 = 1.8451344966888428 + 100.0 * 6.8590569496154785
Epoch 80, val loss: 1.8479951620101929
Epoch 90, training loss: 679.148193359375 = 1.8350986242294312 + 100.0 * 6.773130893707275
Epoch 90, val loss: 1.8383383750915527
Epoch 100, training loss: 673.57275390625 = 1.825386643409729 + 100.0 * 6.717473983764648
Epoch 100, val loss: 1.8289695978164673
Epoch 110, training loss: 668.9481811523438 = 1.8162325620651245 + 100.0 * 6.671319484710693
Epoch 110, val loss: 1.819891333580017
Epoch 120, training loss: 665.164306640625 = 1.8077199459075928 + 100.0 * 6.633565425872803
Epoch 120, val loss: 1.8110934495925903
Epoch 130, training loss: 661.7892456054688 = 1.7998467683792114 + 100.0 * 6.599893569946289
Epoch 130, val loss: 1.8028167486190796
Epoch 140, training loss: 659.06396484375 = 1.7922468185424805 + 100.0 * 6.572717189788818
Epoch 140, val loss: 1.7949104309082031
Epoch 150, training loss: 656.59130859375 = 1.7845568656921387 + 100.0 * 6.548067569732666
Epoch 150, val loss: 1.7870808839797974
Epoch 160, training loss: 654.5914306640625 = 1.7766523361206055 + 100.0 * 6.5281476974487305
Epoch 160, val loss: 1.779116153717041
Epoch 170, training loss: 652.7246704101562 = 1.768290400505066 + 100.0 * 6.509563446044922
Epoch 170, val loss: 1.77093505859375
Epoch 180, training loss: 651.1099853515625 = 1.7595454454421997 + 100.0 * 6.493504524230957
Epoch 180, val loss: 1.7624937295913696
Epoch 190, training loss: 649.9100952148438 = 1.750211477279663 + 100.0 * 6.481598854064941
Epoch 190, val loss: 1.753665566444397
Epoch 200, training loss: 648.3469848632812 = 1.7401988506317139 + 100.0 * 6.466068267822266
Epoch 200, val loss: 1.7442553043365479
Epoch 210, training loss: 647.176513671875 = 1.72951078414917 + 100.0 * 6.454470157623291
Epoch 210, val loss: 1.734315037727356
Epoch 220, training loss: 646.2593383789062 = 1.718024492263794 + 100.0 * 6.445413112640381
Epoch 220, val loss: 1.72376549243927
Epoch 230, training loss: 645.0925903320312 = 1.7056385278701782 + 100.0 * 6.433869361877441
Epoch 230, val loss: 1.7124778032302856
Epoch 240, training loss: 644.2628173828125 = 1.6924115419387817 + 100.0 * 6.425704479217529
Epoch 240, val loss: 1.7004274129867554
Epoch 250, training loss: 643.4339599609375 = 1.6781424283981323 + 100.0 * 6.417557716369629
Epoch 250, val loss: 1.6876192092895508
Epoch 260, training loss: 642.6571044921875 = 1.6629817485809326 + 100.0 * 6.409941673278809
Epoch 260, val loss: 1.6739355325698853
Epoch 270, training loss: 642.23583984375 = 1.6467479467391968 + 100.0 * 6.405891418457031
Epoch 270, val loss: 1.6594388484954834
Epoch 280, training loss: 641.3077392578125 = 1.629431962966919 + 100.0 * 6.396782875061035
Epoch 280, val loss: 1.6440509557724
Epoch 290, training loss: 640.6668701171875 = 1.611146330833435 + 100.0 * 6.390557289123535
Epoch 290, val loss: 1.6278878450393677
Epoch 300, training loss: 640.5469970703125 = 1.5918841361999512 + 100.0 * 6.389551639556885
Epoch 300, val loss: 1.6110414266586304
Epoch 310, training loss: 639.7264404296875 = 1.5715237855911255 + 100.0 * 6.381548881530762
Epoch 310, val loss: 1.5933890342712402
Epoch 320, training loss: 639.0696411132812 = 1.5503584146499634 + 100.0 * 6.375192642211914
Epoch 320, val loss: 1.5749765634536743
Epoch 330, training loss: 638.5391845703125 = 1.528295874595642 + 100.0 * 6.370108604431152
Epoch 330, val loss: 1.5560905933380127
Epoch 340, training loss: 638.2763671875 = 1.5054843425750732 + 100.0 * 6.367708683013916
Epoch 340, val loss: 1.5367186069488525
Epoch 350, training loss: 637.865478515625 = 1.4817758798599243 + 100.0 * 6.363837242126465
Epoch 350, val loss: 1.516887903213501
Epoch 360, training loss: 637.2547607421875 = 1.4575365781784058 + 100.0 * 6.357972621917725
Epoch 360, val loss: 1.4966342449188232
Epoch 370, training loss: 636.754638671875 = 1.4327868223190308 + 100.0 * 6.3532185554504395
Epoch 370, val loss: 1.476317286491394
Epoch 380, training loss: 636.4535522460938 = 1.4076642990112305 + 100.0 * 6.350459098815918
Epoch 380, val loss: 1.4559619426727295
Epoch 390, training loss: 636.072021484375 = 1.3822448253631592 + 100.0 * 6.346897602081299
Epoch 390, val loss: 1.4351664781570435
Epoch 400, training loss: 635.5762329101562 = 1.3565579652786255 + 100.0 * 6.342196464538574
Epoch 400, val loss: 1.4147764444351196
Epoch 410, training loss: 635.4337768554688 = 1.330855369567871 + 100.0 * 6.341029167175293
Epoch 410, val loss: 1.394531488418579
Epoch 420, training loss: 634.9312133789062 = 1.3053371906280518 + 100.0 * 6.336258411407471
Epoch 420, val loss: 1.3743479251861572
Epoch 430, training loss: 634.5216064453125 = 1.2798806428909302 + 100.0 * 6.332417011260986
Epoch 430, val loss: 1.354662537574768
Epoch 440, training loss: 634.3151245117188 = 1.2547607421875 + 100.0 * 6.33060359954834
Epoch 440, val loss: 1.3353562355041504
Epoch 450, training loss: 634.1835327148438 = 1.2298414707183838 + 100.0 * 6.3295369148254395
Epoch 450, val loss: 1.3161884546279907
Epoch 460, training loss: 633.6658935546875 = 1.2052873373031616 + 100.0 * 6.324606418609619
Epoch 460, val loss: 1.2978349924087524
Epoch 470, training loss: 633.30126953125 = 1.1813260316848755 + 100.0 * 6.321199417114258
Epoch 470, val loss: 1.2799968719482422
Epoch 480, training loss: 633.4331665039062 = 1.1578683853149414 + 100.0 * 6.322752952575684
Epoch 480, val loss: 1.2628830671310425
Epoch 490, training loss: 633.1387939453125 = 1.1348767280578613 + 100.0 * 6.320038795471191
Epoch 490, val loss: 1.245924711227417
Epoch 500, training loss: 632.7088623046875 = 1.112448811531067 + 100.0 * 6.315964221954346
Epoch 500, val loss: 1.230106234550476
Epoch 510, training loss: 632.29443359375 = 1.0905556678771973 + 100.0 * 6.312038421630859
Epoch 510, val loss: 1.2146916389465332
Epoch 520, training loss: 632.248779296875 = 1.0693167448043823 + 100.0 * 6.311794281005859
Epoch 520, val loss: 1.2001078128814697
Epoch 530, training loss: 631.8746337890625 = 1.0485819578170776 + 100.0 * 6.308260440826416
Epoch 530, val loss: 1.1857271194458008
Epoch 540, training loss: 631.5831909179688 = 1.0283235311508179 + 100.0 * 6.305548667907715
Epoch 540, val loss: 1.172331690788269
Epoch 550, training loss: 631.5527954101562 = 1.0087511539459229 + 100.0 * 6.305440425872803
Epoch 550, val loss: 1.159340739250183
Epoch 560, training loss: 631.2244262695312 = 0.9893874526023865 + 100.0 * 6.3023505210876465
Epoch 560, val loss: 1.1470500230789185
Epoch 570, training loss: 631.2190551757812 = 0.9706401824951172 + 100.0 * 6.302484035491943
Epoch 570, val loss: 1.135183572769165
Epoch 580, training loss: 630.724365234375 = 0.9523890614509583 + 100.0 * 6.297719478607178
Epoch 580, val loss: 1.1241036653518677
Epoch 590, training loss: 630.5516357421875 = 0.9346529245376587 + 100.0 * 6.296170234680176
Epoch 590, val loss: 1.1134998798370361
Epoch 600, training loss: 630.7289428710938 = 0.9173937439918518 + 100.0 * 6.298115253448486
Epoch 600, val loss: 1.1032330989837646
Epoch 610, training loss: 630.2652587890625 = 0.9004241228103638 + 100.0 * 6.2936482429504395
Epoch 610, val loss: 1.0939488410949707
Epoch 620, training loss: 630.2489624023438 = 0.8839903473854065 + 100.0 * 6.293650150299072
Epoch 620, val loss: 1.084846019744873
Epoch 630, training loss: 630.2769165039062 = 0.8678622841835022 + 100.0 * 6.294090747833252
Epoch 630, val loss: 1.0760232210159302
Epoch 640, training loss: 629.8248901367188 = 0.8521704077720642 + 100.0 * 6.289727210998535
Epoch 640, val loss: 1.0678763389587402
Epoch 650, training loss: 629.5491943359375 = 0.8368651866912842 + 100.0 * 6.287123680114746
Epoch 650, val loss: 1.0602457523345947
Epoch 660, training loss: 629.3466796875 = 0.8220443725585938 + 100.0 * 6.285246849060059
Epoch 660, val loss: 1.0531597137451172
Epoch 670, training loss: 630.3048706054688 = 0.8076434135437012 + 100.0 * 6.2949724197387695
Epoch 670, val loss: 1.046384334564209
Epoch 680, training loss: 629.1002807617188 = 0.7931620478630066 + 100.0 * 6.283071517944336
Epoch 680, val loss: 1.0396140813827515
Epoch 690, training loss: 628.9767456054688 = 0.7792367935180664 + 100.0 * 6.281975269317627
Epoch 690, val loss: 1.033665418624878
Epoch 700, training loss: 628.7896118164062 = 0.7657331228256226 + 100.0 * 6.280238628387451
Epoch 700, val loss: 1.0280306339263916
Epoch 710, training loss: 629.56787109375 = 0.7526135444641113 + 100.0 * 6.28815221786499
Epoch 710, val loss: 1.0229051113128662
Epoch 720, training loss: 628.6611938476562 = 0.7394480109214783 + 100.0 * 6.27921724319458
Epoch 720, val loss: 1.017626166343689
Epoch 730, training loss: 628.4393920898438 = 0.7266897559165955 + 100.0 * 6.277127265930176
Epoch 730, val loss: 1.0129603147506714
Epoch 740, training loss: 628.213134765625 = 0.7143115401268005 + 100.0 * 6.274988174438477
Epoch 740, val loss: 1.0087946653366089
Epoch 750, training loss: 628.53271484375 = 0.7021502256393433 + 100.0 * 6.278305530548096
Epoch 750, val loss: 1.0050737857818604
Epoch 760, training loss: 628.5180053710938 = 0.6900657415390015 + 100.0 * 6.2782793045043945
Epoch 760, val loss: 1.000786542892456
Epoch 770, training loss: 627.9425048828125 = 0.6781936883926392 + 100.0 * 6.272642612457275
Epoch 770, val loss: 0.9973813891410828
Epoch 780, training loss: 627.7294921875 = 0.6666505932807922 + 100.0 * 6.270628929138184
Epoch 780, val loss: 0.9943158030509949
Epoch 790, training loss: 628.1110229492188 = 0.6553907990455627 + 100.0 * 6.2745561599731445
Epoch 790, val loss: 0.9911915063858032
Epoch 800, training loss: 627.8048706054688 = 0.6441846489906311 + 100.0 * 6.271606922149658
Epoch 800, val loss: 0.9888343811035156
Epoch 810, training loss: 627.4395141601562 = 0.6331929564476013 + 100.0 * 6.268063545227051
Epoch 810, val loss: 0.9862643480300903
Epoch 820, training loss: 627.5501708984375 = 0.6224345564842224 + 100.0 * 6.269277095794678
Epoch 820, val loss: 0.9838104248046875
Epoch 830, training loss: 627.2547607421875 = 0.6117843985557556 + 100.0 * 6.266429901123047
Epoch 830, val loss: 0.9819583296775818
Epoch 840, training loss: 627.6922607421875 = 0.6013974547386169 + 100.0 * 6.270908832550049
Epoch 840, val loss: 0.9801394939422607
Epoch 850, training loss: 627.0418701171875 = 0.5909371376037598 + 100.0 * 6.264509201049805
Epoch 850, val loss: 0.9784234166145325
Epoch 860, training loss: 626.8925170898438 = 0.5807998776435852 + 100.0 * 6.26311731338501
Epoch 860, val loss: 0.9769818186759949
Epoch 870, training loss: 626.7886352539062 = 0.5708597302436829 + 100.0 * 6.262177467346191
Epoch 870, val loss: 0.9757882356643677
Epoch 880, training loss: 627.0205078125 = 0.561049222946167 + 100.0 * 6.264595031738281
Epoch 880, val loss: 0.9748436808586121
Epoch 890, training loss: 627.6837768554688 = 0.5512198209762573 + 100.0 * 6.271325588226318
Epoch 890, val loss: 0.9737151265144348
Epoch 900, training loss: 626.724365234375 = 0.5413603186607361 + 100.0 * 6.261829853057861
Epoch 900, val loss: 0.9727393388748169
Epoch 910, training loss: 626.4625244140625 = 0.5318228006362915 + 100.0 * 6.259307384490967
Epoch 910, val loss: 0.9722379446029663
Epoch 920, training loss: 626.3259887695312 = 0.5225088000297546 + 100.0 * 6.258034706115723
Epoch 920, val loss: 0.9719919562339783
Epoch 930, training loss: 626.245849609375 = 0.5133095979690552 + 100.0 * 6.257325649261475
Epoch 930, val loss: 0.9719126224517822
Epoch 940, training loss: 627.07080078125 = 0.5042106509208679 + 100.0 * 6.2656660079956055
Epoch 940, val loss: 0.9720674753189087
Epoch 950, training loss: 626.4886474609375 = 0.49504610896110535 + 100.0 * 6.2599358558654785
Epoch 950, val loss: 0.9715631604194641
Epoch 960, training loss: 626.0068969726562 = 0.48602181673049927 + 100.0 * 6.255208492279053
Epoch 960, val loss: 0.9718998670578003
Epoch 970, training loss: 625.9675903320312 = 0.47721606492996216 + 100.0 * 6.254903793334961
Epoch 970, val loss: 0.9725351929664612
Epoch 980, training loss: 626.3106079101562 = 0.4685349464416504 + 100.0 * 6.258420944213867
Epoch 980, val loss: 0.9731441736221313
Epoch 990, training loss: 626.27197265625 = 0.4599115252494812 + 100.0 * 6.258121013641357
Epoch 990, val loss: 0.9733944535255432
Epoch 1000, training loss: 625.8101196289062 = 0.45123526453971863 + 100.0 * 6.253588676452637
Epoch 1000, val loss: 0.974256157875061
Epoch 1010, training loss: 625.6669921875 = 0.4428025484085083 + 100.0 * 6.252241611480713
Epoch 1010, val loss: 0.9752438068389893
Epoch 1020, training loss: 625.7344360351562 = 0.43457096815109253 + 100.0 * 6.2529988288879395
Epoch 1020, val loss: 0.9761911034584045
Epoch 1030, training loss: 625.6405029296875 = 0.4263526499271393 + 100.0 * 6.25214147567749
Epoch 1030, val loss: 0.9775234460830688
Epoch 1040, training loss: 625.4553833007812 = 0.41822439432144165 + 100.0 * 6.250371932983398
Epoch 1040, val loss: 0.9787942171096802
Epoch 1050, training loss: 625.6459350585938 = 0.4102277457714081 + 100.0 * 6.252357482910156
Epoch 1050, val loss: 0.9802448153495789
Epoch 1060, training loss: 625.7184448242188 = 0.4022503197193146 + 100.0 * 6.253162384033203
Epoch 1060, val loss: 0.9814463257789612
Epoch 1070, training loss: 625.350341796875 = 0.39444398880004883 + 100.0 * 6.249558448791504
Epoch 1070, val loss: 0.98332279920578
Epoch 1080, training loss: 625.1837158203125 = 0.3867368698120117 + 100.0 * 6.247970104217529
Epoch 1080, val loss: 0.9848660826683044
Epoch 1090, training loss: 625.0973510742188 = 0.3792230784893036 + 100.0 * 6.247181415557861
Epoch 1090, val loss: 0.9869405031204224
Epoch 1100, training loss: 625.4985961914062 = 0.37181681394577026 + 100.0 * 6.251267910003662
Epoch 1100, val loss: 0.9886332154273987
Epoch 1110, training loss: 625.1727294921875 = 0.36440613865852356 + 100.0 * 6.248083591461182
Epoch 1110, val loss: 0.9909914135932922
Epoch 1120, training loss: 625.1314697265625 = 0.35713306069374084 + 100.0 * 6.247743606567383
Epoch 1120, val loss: 0.9929406046867371
Epoch 1130, training loss: 625.0391845703125 = 0.3500082492828369 + 100.0 * 6.246891498565674
Epoch 1130, val loss: 0.9953078627586365
Epoch 1140, training loss: 624.8442993164062 = 0.34296542406082153 + 100.0 * 6.24501371383667
Epoch 1140, val loss: 0.9976038336753845
Epoch 1150, training loss: 624.9855346679688 = 0.33607929944992065 + 100.0 * 6.246494770050049
Epoch 1150, val loss: 1.0002118349075317
Epoch 1160, training loss: 624.83203125 = 0.32930469512939453 + 100.0 * 6.245027542114258
Epoch 1160, val loss: 1.0026400089263916
Epoch 1170, training loss: 624.7720947265625 = 0.322594553232193 + 100.0 * 6.244495391845703
Epoch 1170, val loss: 1.0055029392242432
Epoch 1180, training loss: 624.6177368164062 = 0.3160368800163269 + 100.0 * 6.243016719818115
Epoch 1180, val loss: 1.008042812347412
Epoch 1190, training loss: 624.6312255859375 = 0.30965712666511536 + 100.0 * 6.243216037750244
Epoch 1190, val loss: 1.0112450122833252
Epoch 1200, training loss: 624.7307739257812 = 0.30336177349090576 + 100.0 * 6.244274139404297
Epoch 1200, val loss: 1.0140471458435059
Epoch 1210, training loss: 624.5938720703125 = 0.2971017360687256 + 100.0 * 6.24296760559082
Epoch 1210, val loss: 1.017254114151001
Epoch 1220, training loss: 624.5100708007812 = 0.29103702306747437 + 100.0 * 6.242190361022949
Epoch 1220, val loss: 1.020422339439392
Epoch 1230, training loss: 624.291259765625 = 0.2850700616836548 + 100.0 * 6.2400617599487305
Epoch 1230, val loss: 1.0236871242523193
Epoch 1240, training loss: 624.602783203125 = 0.2792670726776123 + 100.0 * 6.243235111236572
Epoch 1240, val loss: 1.0270659923553467
Epoch 1250, training loss: 624.5223999023438 = 0.2734910249710083 + 100.0 * 6.242488861083984
Epoch 1250, val loss: 1.0304619073867798
Epoch 1260, training loss: 624.1904296875 = 0.26783084869384766 + 100.0 * 6.2392258644104
Epoch 1260, val loss: 1.0340242385864258
Epoch 1270, training loss: 624.0929565429688 = 0.26235994696617126 + 100.0 * 6.238305568695068
Epoch 1270, val loss: 1.0380393266677856
Epoch 1280, training loss: 624.0726318359375 = 0.2570267915725708 + 100.0 * 6.238156318664551
Epoch 1280, val loss: 1.0418188571929932
Epoch 1290, training loss: 624.3719482421875 = 0.25183337926864624 + 100.0 * 6.241200923919678
Epoch 1290, val loss: 1.0458612442016602
Epoch 1300, training loss: 624.5398559570312 = 0.24665245413780212 + 100.0 * 6.242931842803955
Epoch 1300, val loss: 1.0493788719177246
Epoch 1310, training loss: 624.242431640625 = 0.24148774147033691 + 100.0 * 6.240009307861328
Epoch 1310, val loss: 1.0531529188156128
Epoch 1320, training loss: 623.9306030273438 = 0.2365199476480484 + 100.0 * 6.236940860748291
Epoch 1320, val loss: 1.0572365522384644
Epoch 1330, training loss: 623.8851318359375 = 0.23170574009418488 + 100.0 * 6.236534118652344
Epoch 1330, val loss: 1.0614631175994873
Epoch 1340, training loss: 623.9632568359375 = 0.22698825597763062 + 100.0 * 6.237362861633301
Epoch 1340, val loss: 1.0654937028884888
Epoch 1350, training loss: 623.7276611328125 = 0.2223469316959381 + 100.0 * 6.235053062438965
Epoch 1350, val loss: 1.0697364807128906
Epoch 1360, training loss: 624.24169921875 = 0.21783842146396637 + 100.0 * 6.240238666534424
Epoch 1360, val loss: 1.074123740196228
Epoch 1370, training loss: 623.7960815429688 = 0.21335995197296143 + 100.0 * 6.2358269691467285
Epoch 1370, val loss: 1.07777738571167
Epoch 1380, training loss: 623.6561279296875 = 0.2089681476354599 + 100.0 * 6.234471321105957
Epoch 1380, val loss: 1.0823591947555542
Epoch 1390, training loss: 623.5897827148438 = 0.20475590229034424 + 100.0 * 6.233850002288818
Epoch 1390, val loss: 1.0865097045898438
Epoch 1400, training loss: 623.9788208007812 = 0.200636088848114 + 100.0 * 6.237782001495361
Epoch 1400, val loss: 1.0911773443222046
Epoch 1410, training loss: 623.4932861328125 = 0.19648198783397675 + 100.0 * 6.232968330383301
Epoch 1410, val loss: 1.0949015617370605
Epoch 1420, training loss: 623.43701171875 = 0.19249281287193298 + 100.0 * 6.232444763183594
Epoch 1420, val loss: 1.0996652841567993
Epoch 1430, training loss: 623.8596801757812 = 0.18862922489643097 + 100.0 * 6.236710071563721
Epoch 1430, val loss: 1.1038761138916016
Epoch 1440, training loss: 623.8595581054688 = 0.18476814031600952 + 100.0 * 6.236747741699219
Epoch 1440, val loss: 1.10857093334198
Epoch 1450, training loss: 623.393310546875 = 0.18094582855701447 + 100.0 * 6.232123374938965
Epoch 1450, val loss: 1.11276376247406
Epoch 1460, training loss: 623.2365112304688 = 0.17728717625141144 + 100.0 * 6.230591773986816
Epoch 1460, val loss: 1.1175607442855835
Epoch 1470, training loss: 623.2036743164062 = 0.1737499237060547 + 100.0 * 6.23029899597168
Epoch 1470, val loss: 1.1223467588424683
Epoch 1480, training loss: 623.5753173828125 = 0.17030978202819824 + 100.0 * 6.234050273895264
Epoch 1480, val loss: 1.1269742250442505
Epoch 1490, training loss: 623.4278564453125 = 0.16682076454162598 + 100.0 * 6.232610702514648
Epoch 1490, val loss: 1.1316078901290894
Epoch 1500, training loss: 623.1796264648438 = 0.16341443359851837 + 100.0 * 6.230162620544434
Epoch 1500, val loss: 1.135963797569275
Epoch 1510, training loss: 623.2626342773438 = 0.16011637449264526 + 100.0 * 6.231025218963623
Epoch 1510, val loss: 1.1408294439315796
Epoch 1520, training loss: 623.5957641601562 = 0.15691617131233215 + 100.0 * 6.23438835144043
Epoch 1520, val loss: 1.1455339193344116
Epoch 1530, training loss: 623.18798828125 = 0.15373803675174713 + 100.0 * 6.230342388153076
Epoch 1530, val loss: 1.1502654552459717
Epoch 1540, training loss: 623.1511840820312 = 0.15065297484397888 + 100.0 * 6.230005741119385
Epoch 1540, val loss: 1.1550304889678955
Epoch 1550, training loss: 623.152099609375 = 0.14766423404216766 + 100.0 * 6.230044364929199
Epoch 1550, val loss: 1.159935712814331
Epoch 1560, training loss: 622.9840087890625 = 0.14470086991786957 + 100.0 * 6.228393077850342
Epoch 1560, val loss: 1.165087103843689
Epoch 1570, training loss: 622.903076171875 = 0.14184439182281494 + 100.0 * 6.227612018585205
Epoch 1570, val loss: 1.1700541973114014
Epoch 1580, training loss: 623.3431396484375 = 0.1390477418899536 + 100.0 * 6.232040882110596
Epoch 1580, val loss: 1.1751004457473755
Epoch 1590, training loss: 622.9771728515625 = 0.13627426326274872 + 100.0 * 6.2284088134765625
Epoch 1590, val loss: 1.1796818971633911
Epoch 1600, training loss: 623.034912109375 = 0.13357585668563843 + 100.0 * 6.229012966156006
Epoch 1600, val loss: 1.1849027872085571
Epoch 1610, training loss: 622.8812255859375 = 0.13093163073062897 + 100.0 * 6.227502822875977
Epoch 1610, val loss: 1.1897802352905273
Epoch 1620, training loss: 622.8692016601562 = 0.1283627599477768 + 100.0 * 6.227408409118652
Epoch 1620, val loss: 1.1948922872543335
Epoch 1630, training loss: 622.7035522460938 = 0.12584102153778076 + 100.0 * 6.2257771492004395
Epoch 1630, val loss: 1.1997789144515991
Epoch 1640, training loss: 623.4915161132812 = 0.12341488897800446 + 100.0 * 6.2336812019348145
Epoch 1640, val loss: 1.2045303583145142
Epoch 1650, training loss: 622.9954833984375 = 0.12090467661619186 + 100.0 * 6.228745460510254
Epoch 1650, val loss: 1.2095391750335693
Epoch 1660, training loss: 622.7166748046875 = 0.11854502558708191 + 100.0 * 6.22598123550415
Epoch 1660, val loss: 1.214707851409912
Epoch 1670, training loss: 622.5549926757812 = 0.11624526977539062 + 100.0 * 6.224387168884277
Epoch 1670, val loss: 1.2198578119277954
Epoch 1680, training loss: 622.745849609375 = 0.11404360085725784 + 100.0 * 6.226318359375
Epoch 1680, val loss: 1.2250618934631348
Epoch 1690, training loss: 622.5973510742188 = 0.11178252100944519 + 100.0 * 6.224855899810791
Epoch 1690, val loss: 1.229615569114685
Epoch 1700, training loss: 622.505859375 = 0.10957800596952438 + 100.0 * 6.223962783813477
Epoch 1700, val loss: 1.2347922325134277
Epoch 1710, training loss: 622.5323486328125 = 0.1074768453836441 + 100.0 * 6.22424840927124
Epoch 1710, val loss: 1.2397319078445435
Epoch 1720, training loss: 622.8020629882812 = 0.10542550683021545 + 100.0 * 6.226966857910156
Epoch 1720, val loss: 1.2446925640106201
Epoch 1730, training loss: 622.4268798828125 = 0.10338708013296127 + 100.0 * 6.223235130310059
Epoch 1730, val loss: 1.2499116659164429
Epoch 1740, training loss: 622.531982421875 = 0.10140977799892426 + 100.0 * 6.224306106567383
Epoch 1740, val loss: 1.2548589706420898
Epoch 1750, training loss: 622.3994140625 = 0.09947456419467926 + 100.0 * 6.222999572753906
Epoch 1750, val loss: 1.2599643468856812
Epoch 1760, training loss: 622.4052124023438 = 0.09759566932916641 + 100.0 * 6.223075866699219
Epoch 1760, val loss: 1.265259861946106
Epoch 1770, training loss: 622.5029907226562 = 0.09575542062520981 + 100.0 * 6.224071979522705
Epoch 1770, val loss: 1.2701365947723389
Epoch 1780, training loss: 622.4981689453125 = 0.09392673522233963 + 100.0 * 6.2240424156188965
Epoch 1780, val loss: 1.275022268295288
Epoch 1790, training loss: 622.6233520507812 = 0.09215497225522995 + 100.0 * 6.225311756134033
Epoch 1790, val loss: 1.2800896167755127
Epoch 1800, training loss: 622.3868408203125 = 0.09039603918790817 + 100.0 * 6.222964763641357
Epoch 1800, val loss: 1.2849094867706299
Epoch 1810, training loss: 622.30029296875 = 0.08868113905191422 + 100.0 * 6.222115993499756
Epoch 1810, val loss: 1.2901719808578491
Epoch 1820, training loss: 622.1879272460938 = 0.08702807873487473 + 100.0 * 6.221008777618408
Epoch 1820, val loss: 1.2953122854232788
Epoch 1830, training loss: 622.2968139648438 = 0.08542495965957642 + 100.0 * 6.222113609313965
Epoch 1830, val loss: 1.3003817796707153
Epoch 1840, training loss: 622.2747802734375 = 0.08383812010288239 + 100.0 * 6.221909046173096
Epoch 1840, val loss: 1.3054697513580322
Epoch 1850, training loss: 622.2406616210938 = 0.08227863907814026 + 100.0 * 6.221583843231201
Epoch 1850, val loss: 1.310630440711975
Epoch 1860, training loss: 622.3072509765625 = 0.08075681328773499 + 100.0 * 6.222265243530273
Epoch 1860, val loss: 1.3153846263885498
Epoch 1870, training loss: 622.5419311523438 = 0.07925696671009064 + 100.0 * 6.224626541137695
Epoch 1870, val loss: 1.3200414180755615
Epoch 1880, training loss: 622.1838989257812 = 0.07777786254882812 + 100.0 * 6.2210612297058105
Epoch 1880, val loss: 1.3250350952148438
Epoch 1890, training loss: 622.0277709960938 = 0.07634296268224716 + 100.0 * 6.2195143699646
Epoch 1890, val loss: 1.3302119970321655
Epoch 1900, training loss: 622.05859375 = 0.07496730983257294 + 100.0 * 6.219836235046387
Epoch 1900, val loss: 1.3351680040359497
Epoch 1910, training loss: 622.1806640625 = 0.07361350953578949 + 100.0 * 6.221070766448975
Epoch 1910, val loss: 1.3401380777359009
Epoch 1920, training loss: 622.1170043945312 = 0.07228954136371613 + 100.0 * 6.220447540283203
Epoch 1920, val loss: 1.345036506652832
Epoch 1930, training loss: 622.2675170898438 = 0.07098829001188278 + 100.0 * 6.221965312957764
Epoch 1930, val loss: 1.349731683731079
Epoch 1940, training loss: 621.9915771484375 = 0.06967205554246902 + 100.0 * 6.219218730926514
Epoch 1940, val loss: 1.354392409324646
Epoch 1950, training loss: 621.849365234375 = 0.06843157857656479 + 100.0 * 6.217809200286865
Epoch 1950, val loss: 1.3595925569534302
Epoch 1960, training loss: 621.8721313476562 = 0.06722506135702133 + 100.0 * 6.218049049377441
Epoch 1960, val loss: 1.364400863647461
Epoch 1970, training loss: 622.1257934570312 = 0.06606040149927139 + 100.0 * 6.220597743988037
Epoch 1970, val loss: 1.3693615198135376
Epoch 1980, training loss: 621.9652709960938 = 0.06487002223730087 + 100.0 * 6.219003677368164
Epoch 1980, val loss: 1.373726725578308
Epoch 1990, training loss: 621.8728637695312 = 0.06371212750673294 + 100.0 * 6.2180914878845215
Epoch 1990, val loss: 1.3787298202514648
Epoch 2000, training loss: 621.9395751953125 = 0.06259588897228241 + 100.0 * 6.218769550323486
Epoch 2000, val loss: 1.383803367614746
Epoch 2010, training loss: 621.8265380859375 = 0.06148938834667206 + 100.0 * 6.217650890350342
Epoch 2010, val loss: 1.3883683681488037
Epoch 2020, training loss: 621.7529907226562 = 0.060417838394641876 + 100.0 * 6.216925621032715
Epoch 2020, val loss: 1.3933041095733643
Epoch 2030, training loss: 622.2235717773438 = 0.05939346179366112 + 100.0 * 6.221642017364502
Epoch 2030, val loss: 1.3980565071105957
Epoch 2040, training loss: 621.6958618164062 = 0.058334238827228546 + 100.0 * 6.21637487411499
Epoch 2040, val loss: 1.4028083086013794
Epoch 2050, training loss: 621.7322998046875 = 0.0573224276304245 + 100.0 * 6.216749668121338
Epoch 2050, val loss: 1.407302737236023
Epoch 2060, training loss: 621.6806030273438 = 0.056343674659729004 + 100.0 * 6.216242790222168
Epoch 2060, val loss: 1.4121235609054565
Epoch 2070, training loss: 621.5750122070312 = 0.055389851331710815 + 100.0 * 6.215196132659912
Epoch 2070, val loss: 1.4170408248901367
Epoch 2080, training loss: 621.7473754882812 = 0.054462142288684845 + 100.0 * 6.2169294357299805
Epoch 2080, val loss: 1.4217231273651123
Epoch 2090, training loss: 621.6765747070312 = 0.053531840443611145 + 100.0 * 6.216230392456055
Epoch 2090, val loss: 1.4262114763259888
Epoch 2100, training loss: 621.7158813476562 = 0.05262390896677971 + 100.0 * 6.216632843017578
Epoch 2100, val loss: 1.430963158607483
Epoch 2110, training loss: 621.6914672851562 = 0.051740944385528564 + 100.0 * 6.216397285461426
Epoch 2110, val loss: 1.4356324672698975
Epoch 2120, training loss: 621.541015625 = 0.05086985230445862 + 100.0 * 6.214901924133301
Epoch 2120, val loss: 1.440022587776184
Epoch 2130, training loss: 621.5498046875 = 0.05002906545996666 + 100.0 * 6.214997291564941
Epoch 2130, val loss: 1.444523572921753
Epoch 2140, training loss: 621.5765991210938 = 0.04921731352806091 + 100.0 * 6.215274333953857
Epoch 2140, val loss: 1.4493528604507446
Epoch 2150, training loss: 622.0001220703125 = 0.048431817442178726 + 100.0 * 6.219516754150391
Epoch 2150, val loss: 1.4536412954330444
Epoch 2160, training loss: 621.6898803710938 = 0.047602299600839615 + 100.0 * 6.2164225578308105
Epoch 2160, val loss: 1.4578849077224731
Epoch 2170, training loss: 621.4476318359375 = 0.04681819677352905 + 100.0 * 6.214008331298828
Epoch 2170, val loss: 1.4624758958816528
Epoch 2180, training loss: 621.4150390625 = 0.04606461524963379 + 100.0 * 6.213689804077148
Epoch 2180, val loss: 1.4669585227966309
Epoch 2190, training loss: 621.7661743164062 = 0.04534262418746948 + 100.0 * 6.217208385467529
Epoch 2190, val loss: 1.4714772701263428
Epoch 2200, training loss: 621.5797729492188 = 0.044604212045669556 + 100.0 * 6.215351581573486
Epoch 2200, val loss: 1.4753930568695068
Epoch 2210, training loss: 621.3592529296875 = 0.043875377625226974 + 100.0 * 6.213153839111328
Epoch 2210, val loss: 1.480183482170105
Epoch 2220, training loss: 621.3946533203125 = 0.04317742958664894 + 100.0 * 6.213515281677246
Epoch 2220, val loss: 1.4845608472824097
Epoch 2230, training loss: 621.6283569335938 = 0.042507950216531754 + 100.0 * 6.2158589363098145
Epoch 2230, val loss: 1.4892311096191406
Epoch 2240, training loss: 621.3510131835938 = 0.04182884097099304 + 100.0 * 6.21309232711792
Epoch 2240, val loss: 1.4931734800338745
Epoch 2250, training loss: 621.3467407226562 = 0.04117901250720024 + 100.0 * 6.21305513381958
Epoch 2250, val loss: 1.4977599382400513
Epoch 2260, training loss: 621.621826171875 = 0.040552761405706406 + 100.0 * 6.215813159942627
Epoch 2260, val loss: 1.5020737648010254
Epoch 2270, training loss: 621.3875122070312 = 0.0399058572947979 + 100.0 * 6.213476181030273
Epoch 2270, val loss: 1.5063776969909668
Epoch 2280, training loss: 621.4464721679688 = 0.039284106343984604 + 100.0 * 6.214071750640869
Epoch 2280, val loss: 1.51032292842865
Epoch 2290, training loss: 621.2210693359375 = 0.038676753640174866 + 100.0 * 6.2118239402771
Epoch 2290, val loss: 1.5150097608566284
Epoch 2300, training loss: 621.1535034179688 = 0.03809062018990517 + 100.0 * 6.211153984069824
Epoch 2300, val loss: 1.5190149545669556
Epoch 2310, training loss: 621.521484375 = 0.03753722831606865 + 100.0 * 6.214839458465576
Epoch 2310, val loss: 1.5232741832733154
Epoch 2320, training loss: 621.1471557617188 = 0.036948949098587036 + 100.0 * 6.21110200881958
Epoch 2320, val loss: 1.527513861656189
Epoch 2330, training loss: 621.2333984375 = 0.03638617694377899 + 100.0 * 6.211970329284668
Epoch 2330, val loss: 1.5315231084823608
Epoch 2340, training loss: 621.3404541015625 = 0.03583501651883125 + 100.0 * 6.213046073913574
Epoch 2340, val loss: 1.5356521606445312
Epoch 2350, training loss: 621.1775512695312 = 0.03530507907271385 + 100.0 * 6.211422920227051
Epoch 2350, val loss: 1.539945125579834
Epoch 2360, training loss: 621.0863647460938 = 0.034783534705638885 + 100.0 * 6.21051549911499
Epoch 2360, val loss: 1.5441040992736816
Epoch 2370, training loss: 621.401611328125 = 0.03428754583001137 + 100.0 * 6.213673114776611
Epoch 2370, val loss: 1.5485305786132812
Epoch 2380, training loss: 621.1776733398438 = 0.033774469047784805 + 100.0 * 6.21143913269043
Epoch 2380, val loss: 1.5520910024642944
Epoch 2390, training loss: 621.1707763671875 = 0.03327541798353195 + 100.0 * 6.2113752365112305
Epoch 2390, val loss: 1.5560901165008545
Epoch 2400, training loss: 621.2301025390625 = 0.03278028219938278 + 100.0 * 6.211973190307617
Epoch 2400, val loss: 1.559821367263794
Epoch 2410, training loss: 621.0462646484375 = 0.03230925649404526 + 100.0 * 6.210139751434326
Epoch 2410, val loss: 1.564158320426941
Epoch 2420, training loss: 621.2445068359375 = 0.03186001628637314 + 100.0 * 6.212126731872559
Epoch 2420, val loss: 1.5681941509246826
Epoch 2430, training loss: 621.15625 = 0.03139824420213699 + 100.0 * 6.211248874664307
Epoch 2430, val loss: 1.5718966722488403
Epoch 2440, training loss: 621.0819702148438 = 0.030940448865294456 + 100.0 * 6.21051025390625
Epoch 2440, val loss: 1.5758050680160522
Epoch 2450, training loss: 621.0606689453125 = 0.030511444434523582 + 100.0 * 6.210301399230957
Epoch 2450, val loss: 1.5799403190612793
Epoch 2460, training loss: 621.0328369140625 = 0.030084917321801186 + 100.0 * 6.21002721786499
Epoch 2460, val loss: 1.5837284326553345
Epoch 2470, training loss: 620.9329223632812 = 0.02966931276023388 + 100.0 * 6.2090325355529785
Epoch 2470, val loss: 1.5874234437942505
Epoch 2480, training loss: 621.2799072265625 = 0.02926601469516754 + 100.0 * 6.2125067710876465
Epoch 2480, val loss: 1.5910346508026123
Epoch 2490, training loss: 621.147216796875 = 0.028856204822659492 + 100.0 * 6.211183547973633
Epoch 2490, val loss: 1.5951247215270996
Epoch 2500, training loss: 620.9053955078125 = 0.02844657562673092 + 100.0 * 6.20876932144165
Epoch 2500, val loss: 1.5985157489776611
Epoch 2510, training loss: 620.8355102539062 = 0.028058459982275963 + 100.0 * 6.20807409286499
Epoch 2510, val loss: 1.6026206016540527
Epoch 2520, training loss: 621.1556396484375 = 0.027690667659044266 + 100.0 * 6.211279392242432
Epoch 2520, val loss: 1.6062289476394653
Epoch 2530, training loss: 621.0121459960938 = 0.027310585603117943 + 100.0 * 6.209848880767822
Epoch 2530, val loss: 1.6097885370254517
Epoch 2540, training loss: 620.8095092773438 = 0.02692646160721779 + 100.0 * 6.207826137542725
Epoch 2540, val loss: 1.6133220195770264
Epoch 2550, training loss: 620.7927856445312 = 0.02657346986234188 + 100.0 * 6.207662105560303
Epoch 2550, val loss: 1.6173464059829712
Epoch 2560, training loss: 620.9201049804688 = 0.02623024582862854 + 100.0 * 6.2089385986328125
Epoch 2560, val loss: 1.6208224296569824
Epoch 2570, training loss: 620.9730224609375 = 0.025883609429001808 + 100.0 * 6.209471702575684
Epoch 2570, val loss: 1.6244195699691772
Epoch 2580, training loss: 620.8812255859375 = 0.02554311603307724 + 100.0 * 6.20855712890625
Epoch 2580, val loss: 1.6280784606933594
Epoch 2590, training loss: 621.0283813476562 = 0.025210512802004814 + 100.0 * 6.210031509399414
Epoch 2590, val loss: 1.6311070919036865
Epoch 2600, training loss: 620.9700927734375 = 0.024887628853321075 + 100.0 * 6.209451675415039
Epoch 2600, val loss: 1.635209321975708
Epoch 2610, training loss: 621.0958251953125 = 0.024565784260630608 + 100.0 * 6.210712909698486
Epoch 2610, val loss: 1.6385304927825928
Epoch 2620, training loss: 620.73095703125 = 0.024235419929027557 + 100.0 * 6.207067012786865
Epoch 2620, val loss: 1.6421890258789062
Epoch 2630, training loss: 620.7505493164062 = 0.02392859011888504 + 100.0 * 6.207266330718994
Epoch 2630, val loss: 1.6458996534347534
Epoch 2640, training loss: 621.4415893554688 = 0.023635366931557655 + 100.0 * 6.214179515838623
Epoch 2640, val loss: 1.649421215057373
Epoch 2650, training loss: 620.8131103515625 = 0.02331787906587124 + 100.0 * 6.207898139953613
Epoch 2650, val loss: 1.6524711847305298
Epoch 2660, training loss: 620.5944213867188 = 0.023020705208182335 + 100.0 * 6.205714225769043
Epoch 2660, val loss: 1.6562267541885376
Epoch 2670, training loss: 620.5584106445312 = 0.022738970816135406 + 100.0 * 6.205356597900391
Epoch 2670, val loss: 1.659732460975647
Epoch 2680, training loss: 620.638916015625 = 0.022470219060778618 + 100.0 * 6.206164360046387
Epoch 2680, val loss: 1.6630090475082397
Epoch 2690, training loss: 621.4088134765625 = 0.022214919328689575 + 100.0 * 6.213866233825684
Epoch 2690, val loss: 1.6661540269851685
Epoch 2700, training loss: 620.9244995117188 = 0.02191551774740219 + 100.0 * 6.209025859832764
Epoch 2700, val loss: 1.6697148084640503
Epoch 2710, training loss: 620.72607421875 = 0.021643321961164474 + 100.0 * 6.2070441246032715
Epoch 2710, val loss: 1.673144817352295
Epoch 2720, training loss: 620.825439453125 = 0.02138638123869896 + 100.0 * 6.208040714263916
Epoch 2720, val loss: 1.6765830516815186
Epoch 2730, training loss: 620.6650390625 = 0.021125834435224533 + 100.0 * 6.20643949508667
Epoch 2730, val loss: 1.679785966873169
Epoch 2740, training loss: 620.8200073242188 = 0.020870545879006386 + 100.0 * 6.207991600036621
Epoch 2740, val loss: 1.6831133365631104
Epoch 2750, training loss: 620.658935546875 = 0.020616918802261353 + 100.0 * 6.206383228302002
Epoch 2750, val loss: 1.6862449645996094
Epoch 2760, training loss: 621.0010375976562 = 0.020377077162265778 + 100.0 * 6.209806442260742
Epoch 2760, val loss: 1.6895207166671753
Epoch 2770, training loss: 620.6187744140625 = 0.02012774720788002 + 100.0 * 6.205986022949219
Epoch 2770, val loss: 1.6928695440292358
Epoch 2780, training loss: 620.5079956054688 = 0.01988913118839264 + 100.0 * 6.204881191253662
Epoch 2780, val loss: 1.696089506149292
Epoch 2790, training loss: 620.4305419921875 = 0.019661132246255875 + 100.0 * 6.204108715057373
Epoch 2790, val loss: 1.6994901895523071
Epoch 2800, training loss: 620.4927978515625 = 0.019442830234766006 + 100.0 * 6.204733371734619
Epoch 2800, val loss: 1.7028318643569946
Epoch 2810, training loss: 621.1497192382812 = 0.01922871544957161 + 100.0 * 6.211305141448975
Epoch 2810, val loss: 1.705855131149292
Epoch 2820, training loss: 620.625732421875 = 0.018992867320775986 + 100.0 * 6.2060675621032715
Epoch 2820, val loss: 1.7085964679718018
Epoch 2830, training loss: 620.51318359375 = 0.018775133416056633 + 100.0 * 6.204943656921387
Epoch 2830, val loss: 1.7121061086654663
Epoch 2840, training loss: 620.8372192382812 = 0.01856907829642296 + 100.0 * 6.208186626434326
Epoch 2840, val loss: 1.7149717807769775
Epoch 2850, training loss: 620.6190795898438 = 0.01835441030561924 + 100.0 * 6.20600700378418
Epoch 2850, val loss: 1.7178727388381958
Epoch 2860, training loss: 620.5100708007812 = 0.01814078353345394 + 100.0 * 6.204919338226318
Epoch 2860, val loss: 1.721156120300293
Epoch 2870, training loss: 620.4063110351562 = 0.01793460175395012 + 100.0 * 6.203883647918701
Epoch 2870, val loss: 1.7241921424865723
Epoch 2880, training loss: 620.5151977539062 = 0.017739620059728622 + 100.0 * 6.20497465133667
Epoch 2880, val loss: 1.7270869016647339
Epoch 2890, training loss: 620.842041015625 = 0.017552271485328674 + 100.0 * 6.208244800567627
Epoch 2890, val loss: 1.7299573421478271
Epoch 2900, training loss: 620.4667358398438 = 0.01734994351863861 + 100.0 * 6.204493999481201
Epoch 2900, val loss: 1.733091115951538
Epoch 2910, training loss: 620.3589477539062 = 0.01715756393969059 + 100.0 * 6.203417778015137
Epoch 2910, val loss: 1.736311674118042
Epoch 2920, training loss: 620.3065185546875 = 0.01697462610900402 + 100.0 * 6.202895641326904
Epoch 2920, val loss: 1.7392823696136475
Epoch 2930, training loss: 620.2656860351562 = 0.016797931864857674 + 100.0 * 6.202488899230957
Epoch 2930, val loss: 1.7425453662872314
Epoch 2940, training loss: 621.0277099609375 = 0.016638459637761116 + 100.0 * 6.210110664367676
Epoch 2940, val loss: 1.7455190420150757
Epoch 2950, training loss: 620.6638793945312 = 0.016434986144304276 + 100.0 * 6.206474304199219
Epoch 2950, val loss: 1.7475230693817139
Epoch 2960, training loss: 620.47021484375 = 0.01625696010887623 + 100.0 * 6.204539775848389
Epoch 2960, val loss: 1.751172661781311
Epoch 2970, training loss: 620.3194580078125 = 0.01607983373105526 + 100.0 * 6.203033924102783
Epoch 2970, val loss: 1.7538477182388306
Epoch 2980, training loss: 620.7822875976562 = 0.015921831130981445 + 100.0 * 6.207663536071777
Epoch 2980, val loss: 1.7569985389709473
Epoch 2990, training loss: 620.2548828125 = 0.015750886872410774 + 100.0 * 6.202391147613525
Epoch 2990, val loss: 1.7596927881240845
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.801265155508698
=== training gcn model ===
Epoch 0, training loss: 861.633544921875 = 1.949090600013733 + 100.0 * 8.596844673156738
Epoch 0, val loss: 1.9520689249038696
Epoch 10, training loss: 861.5601806640625 = 1.9404648542404175 + 100.0 * 8.596197128295898
Epoch 10, val loss: 1.9428330659866333
Epoch 20, training loss: 861.165771484375 = 1.9295345544815063 + 100.0 * 8.592362403869629
Epoch 20, val loss: 1.9308134317398071
Epoch 30, training loss: 858.6058349609375 = 1.915622591972351 + 100.0 * 8.566902160644531
Epoch 30, val loss: 1.9153964519500732
Epoch 40, training loss: 841.2424926757812 = 1.8990228176116943 + 100.0 * 8.393434524536133
Epoch 40, val loss: 1.8972995281219482
Epoch 50, training loss: 758.2183837890625 = 1.8805078268051147 + 100.0 * 7.563378810882568
Epoch 50, val loss: 1.8770134449005127
Epoch 60, training loss: 730.4937133789062 = 1.865487813949585 + 100.0 * 7.286282539367676
Epoch 60, val loss: 1.862420916557312
Epoch 70, training loss: 713.2496948242188 = 1.8556495904922485 + 100.0 * 7.113940238952637
Epoch 70, val loss: 1.8533260822296143
Epoch 80, training loss: 700.2719116210938 = 1.8449664115905762 + 100.0 * 6.984269618988037
Epoch 80, val loss: 1.8429361581802368
Epoch 90, training loss: 691.6563110351562 = 1.8351032733917236 + 100.0 * 6.898212432861328
Epoch 90, val loss: 1.8338161706924438
Epoch 100, training loss: 683.9652099609375 = 1.8266671895980835 + 100.0 * 6.821385383605957
Epoch 100, val loss: 1.825804591178894
Epoch 110, training loss: 677.6343383789062 = 1.8194018602371216 + 100.0 * 6.758149147033691
Epoch 110, val loss: 1.8189682960510254
Epoch 120, training loss: 672.4427490234375 = 1.8125776052474976 + 100.0 * 6.706302165985107
Epoch 120, val loss: 1.8122888803482056
Epoch 130, training loss: 668.2957153320312 = 1.8058452606201172 + 100.0 * 6.664898872375488
Epoch 130, val loss: 1.8057900667190552
Epoch 140, training loss: 665.0286865234375 = 1.7990143299102783 + 100.0 * 6.632296562194824
Epoch 140, val loss: 1.7994964122772217
Epoch 150, training loss: 662.4063110351562 = 1.791970133781433 + 100.0 * 6.606142997741699
Epoch 150, val loss: 1.7931857109069824
Epoch 160, training loss: 659.6820678710938 = 1.7846237421035767 + 100.0 * 6.578974723815918
Epoch 160, val loss: 1.7865853309631348
Epoch 170, training loss: 657.3463134765625 = 1.7769120931625366 + 100.0 * 6.555694103240967
Epoch 170, val loss: 1.7798696756362915
Epoch 180, training loss: 655.2679443359375 = 1.7687413692474365 + 100.0 * 6.534992218017578
Epoch 180, val loss: 1.7728221416473389
Epoch 190, training loss: 653.56494140625 = 1.759963870048523 + 100.0 * 6.518049716949463
Epoch 190, val loss: 1.765340805053711
Epoch 200, training loss: 651.920654296875 = 1.7504584789276123 + 100.0 * 6.501701354980469
Epoch 200, val loss: 1.7573950290679932
Epoch 210, training loss: 650.4186401367188 = 1.7401431798934937 + 100.0 * 6.4867844581604
Epoch 210, val loss: 1.7488632202148438
Epoch 220, training loss: 649.2981567382812 = 1.728821873664856 + 100.0 * 6.475693225860596
Epoch 220, val loss: 1.7395294904708862
Epoch 230, training loss: 647.9161376953125 = 1.7165324687957764 + 100.0 * 6.461996078491211
Epoch 230, val loss: 1.7294679880142212
Epoch 240, training loss: 646.7515258789062 = 1.7033007144927979 + 100.0 * 6.450482368469238
Epoch 240, val loss: 1.7186548709869385
Epoch 250, training loss: 645.8182373046875 = 1.6890045404434204 + 100.0 * 6.4412922859191895
Epoch 250, val loss: 1.7069997787475586
Epoch 260, training loss: 645.0679931640625 = 1.6733800172805786 + 100.0 * 6.433945655822754
Epoch 260, val loss: 1.694288969039917
Epoch 270, training loss: 644.06298828125 = 1.656774878501892 + 100.0 * 6.4240617752075195
Epoch 270, val loss: 1.6807302236557007
Epoch 280, training loss: 643.27587890625 = 1.6389946937561035 + 100.0 * 6.4163689613342285
Epoch 280, val loss: 1.666333556175232
Epoch 290, training loss: 642.5686645507812 = 1.6201215982437134 + 100.0 * 6.409485340118408
Epoch 290, val loss: 1.6510217189788818
Epoch 300, training loss: 641.9631958007812 = 1.6003410816192627 + 100.0 * 6.403628826141357
Epoch 300, val loss: 1.6350113153457642
Epoch 310, training loss: 641.1895141601562 = 1.5795849561691284 + 100.0 * 6.396099090576172
Epoch 310, val loss: 1.6183242797851562
Epoch 320, training loss: 640.6071166992188 = 1.5582237243652344 + 100.0 * 6.390489101409912
Epoch 320, val loss: 1.601121187210083
Epoch 330, training loss: 640.0682983398438 = 1.5361624956130981 + 100.0 * 6.385321617126465
Epoch 330, val loss: 1.5835762023925781
Epoch 340, training loss: 639.4324340820312 = 1.5136593580245972 + 100.0 * 6.37918758392334
Epoch 340, val loss: 1.5658297538757324
Epoch 350, training loss: 638.8623046875 = 1.4908140897750854 + 100.0 * 6.373715400695801
Epoch 350, val loss: 1.5479512214660645
Epoch 360, training loss: 638.609619140625 = 1.4678152799606323 + 100.0 * 6.371417999267578
Epoch 360, val loss: 1.5301626920700073
Epoch 370, training loss: 638.2005615234375 = 1.4445127248764038 + 100.0 * 6.367560386657715
Epoch 370, val loss: 1.5120946168899536
Epoch 380, training loss: 637.5482177734375 = 1.4212753772735596 + 100.0 * 6.361269474029541
Epoch 380, val loss: 1.494469165802002
Epoch 390, training loss: 636.9898071289062 = 1.3981834650039673 + 100.0 * 6.3559160232543945
Epoch 390, val loss: 1.4772775173187256
Epoch 400, training loss: 636.508544921875 = 1.3752638101577759 + 100.0 * 6.351333141326904
Epoch 400, val loss: 1.460326075553894
Epoch 410, training loss: 637.1032104492188 = 1.3524320125579834 + 100.0 * 6.357508182525635
Epoch 410, val loss: 1.4437830448150635
Epoch 420, training loss: 635.9938354492188 = 1.329639196395874 + 100.0 * 6.346642017364502
Epoch 420, val loss: 1.4273524284362793
Epoch 430, training loss: 635.3536376953125 = 1.307055950164795 + 100.0 * 6.340465545654297
Epoch 430, val loss: 1.411629319190979
Epoch 440, training loss: 635.1156616210938 = 1.2847859859466553 + 100.0 * 6.338308811187744
Epoch 440, val loss: 1.3961381912231445
Epoch 450, training loss: 634.8577880859375 = 1.2625555992126465 + 100.0 * 6.335952281951904
Epoch 450, val loss: 1.3809713125228882
Epoch 460, training loss: 634.2747192382812 = 1.2406376600265503 + 100.0 * 6.330340385437012
Epoch 460, val loss: 1.3663361072540283
Epoch 470, training loss: 634.1229248046875 = 1.218998670578003 + 100.0 * 6.329039573669434
Epoch 470, val loss: 1.351866602897644
Epoch 480, training loss: 633.8518676757812 = 1.1974979639053345 + 100.0 * 6.32654333114624
Epoch 480, val loss: 1.3382943868637085
Epoch 490, training loss: 633.3367919921875 = 1.1762263774871826 + 100.0 * 6.321605682373047
Epoch 490, val loss: 1.3246244192123413
Epoch 500, training loss: 633.0936279296875 = 1.1552693843841553 + 100.0 * 6.31938362121582
Epoch 500, val loss: 1.311397910118103
Epoch 510, training loss: 632.9033203125 = 1.1346465349197388 + 100.0 * 6.317687034606934
Epoch 510, val loss: 1.2987055778503418
Epoch 520, training loss: 633.0784301757812 = 1.1138415336608887 + 100.0 * 6.319645881652832
Epoch 520, val loss: 1.2862672805786133
Epoch 530, training loss: 632.4392700195312 = 1.0935405492782593 + 100.0 * 6.313457012176514
Epoch 530, val loss: 1.2740778923034668
Epoch 540, training loss: 632.0833740234375 = 1.073573350906372 + 100.0 * 6.310098171234131
Epoch 540, val loss: 1.2623686790466309
Epoch 550, training loss: 632.6304321289062 = 1.0538498163223267 + 100.0 * 6.315765857696533
Epoch 550, val loss: 1.2509174346923828
Epoch 560, training loss: 631.62109375 = 1.0343432426452637 + 100.0 * 6.3058671951293945
Epoch 560, val loss: 1.239915132522583
Epoch 570, training loss: 631.3784790039062 = 1.0151945352554321 + 100.0 * 6.303632736206055
Epoch 570, val loss: 1.2294046878814697
Epoch 580, training loss: 631.2275390625 = 0.9964215755462646 + 100.0 * 6.302311420440674
Epoch 580, val loss: 1.2192481756210327
Epoch 590, training loss: 630.9425048828125 = 0.977772057056427 + 100.0 * 6.299647331237793
Epoch 590, val loss: 1.2093080282211304
Epoch 600, training loss: 630.8397216796875 = 0.9594313502311707 + 100.0 * 6.298802852630615
Epoch 600, val loss: 1.199944019317627
Epoch 610, training loss: 630.5814819335938 = 0.9414618015289307 + 100.0 * 6.29640007019043
Epoch 610, val loss: 1.1909871101379395
Epoch 620, training loss: 630.3430786132812 = 0.9238360524177551 + 100.0 * 6.294192790985107
Epoch 620, val loss: 1.182356595993042
Epoch 630, training loss: 631.339111328125 = 0.9064841270446777 + 100.0 * 6.304326057434082
Epoch 630, val loss: 1.1741502285003662
Epoch 640, training loss: 630.0350341796875 = 0.8890959024429321 + 100.0 * 6.291459560394287
Epoch 640, val loss: 1.1663243770599365
Epoch 650, training loss: 629.8773193359375 = 0.8721339106559753 + 100.0 * 6.2900519371032715
Epoch 650, val loss: 1.1586450338363647
Epoch 660, training loss: 629.9913330078125 = 0.8554555773735046 + 100.0 * 6.291358947753906
Epoch 660, val loss: 1.151322603225708
Epoch 670, training loss: 629.6859741210938 = 0.8390642404556274 + 100.0 * 6.288469314575195
Epoch 670, val loss: 1.1444767713546753
Epoch 680, training loss: 629.3521118164062 = 0.8230013847351074 + 100.0 * 6.2852911949157715
Epoch 680, val loss: 1.1379214525222778
Epoch 690, training loss: 629.20703125 = 0.8072854280471802 + 100.0 * 6.283997058868408
Epoch 690, val loss: 1.131596326828003
Epoch 700, training loss: 629.6167602539062 = 0.791847288608551 + 100.0 * 6.2882490158081055
Epoch 700, val loss: 1.1256344318389893
Epoch 710, training loss: 629.367919921875 = 0.7764135599136353 + 100.0 * 6.285914897918701
Epoch 710, val loss: 1.1205389499664307
Epoch 720, training loss: 628.8135986328125 = 0.7613641619682312 + 100.0 * 6.280522346496582
Epoch 720, val loss: 1.1148897409439087
Epoch 730, training loss: 628.6676635742188 = 0.7466838359832764 + 100.0 * 6.279209613800049
Epoch 730, val loss: 1.1102182865142822
Epoch 740, training loss: 628.5831909179688 = 0.7323021292686462 + 100.0 * 6.278509140014648
Epoch 740, val loss: 1.1056172847747803
Epoch 750, training loss: 628.5314331054688 = 0.71800297498703 + 100.0 * 6.278133869171143
Epoch 750, val loss: 1.1014502048492432
Epoch 760, training loss: 628.6162109375 = 0.703916609287262 + 100.0 * 6.279122829437256
Epoch 760, val loss: 1.096893310546875
Epoch 770, training loss: 628.2213134765625 = 0.6900730729103088 + 100.0 * 6.275312423706055
Epoch 770, val loss: 1.093711256980896
Epoch 780, training loss: 628.0071411132812 = 0.6766026020050049 + 100.0 * 6.273305892944336
Epoch 780, val loss: 1.0903089046478271
Epoch 790, training loss: 627.9085083007812 = 0.6634120345115662 + 100.0 * 6.272451400756836
Epoch 790, val loss: 1.0874760150909424
Epoch 800, training loss: 628.3696899414062 = 0.6503568291664124 + 100.0 * 6.277193546295166
Epoch 800, val loss: 1.084998369216919
Epoch 810, training loss: 627.7157592773438 = 0.6374008059501648 + 100.0 * 6.2707839012146
Epoch 810, val loss: 1.0822985172271729
Epoch 820, training loss: 627.625732421875 = 0.6247420907020569 + 100.0 * 6.270009994506836
Epoch 820, val loss: 1.0793930292129517
Epoch 830, training loss: 627.8296508789062 = 0.6124312281608582 + 100.0 * 6.272172451019287
Epoch 830, val loss: 1.0777307748794556
Epoch 840, training loss: 627.5833129882812 = 0.6001725196838379 + 100.0 * 6.269831657409668
Epoch 840, val loss: 1.0760188102722168
Epoch 850, training loss: 627.3896484375 = 0.5880983471870422 + 100.0 * 6.2680158615112305
Epoch 850, val loss: 1.0741182565689087
Epoch 860, training loss: 627.1416625976562 = 0.5763157606124878 + 100.0 * 6.265653610229492
Epoch 860, val loss: 1.0728362798690796
Epoch 870, training loss: 627.0819091796875 = 0.5647957921028137 + 100.0 * 6.265171051025391
Epoch 870, val loss: 1.072060465812683
Epoch 880, training loss: 627.3699340820312 = 0.5534216165542603 + 100.0 * 6.268165588378906
Epoch 880, val loss: 1.0710114240646362
Epoch 890, training loss: 627.1307373046875 = 0.5420714616775513 + 100.0 * 6.2658867835998535
Epoch 890, val loss: 1.070679783821106
Epoch 900, training loss: 626.894287109375 = 0.5308959484100342 + 100.0 * 6.263634204864502
Epoch 900, val loss: 1.0695335865020752
Epoch 910, training loss: 626.6532592773438 = 0.5200839042663574 + 100.0 * 6.261332035064697
Epoch 910, val loss: 1.069746971130371
Epoch 920, training loss: 626.572021484375 = 0.5094591975212097 + 100.0 * 6.26062536239624
Epoch 920, val loss: 1.0699279308319092
Epoch 930, training loss: 627.5421752929688 = 0.4989140033721924 + 100.0 * 6.270432472229004
Epoch 930, val loss: 1.0704082250595093
Epoch 940, training loss: 626.4111328125 = 0.4884006679058075 + 100.0 * 6.259227275848389
Epoch 940, val loss: 1.0704902410507202
Epoch 950, training loss: 626.4263305664062 = 0.4781914949417114 + 100.0 * 6.259481430053711
Epoch 950, val loss: 1.0703877210617065
Epoch 960, training loss: 626.20751953125 = 0.46824538707733154 + 100.0 * 6.257392406463623
Epoch 960, val loss: 1.0714459419250488
Epoch 970, training loss: 626.62109375 = 0.4585040807723999 + 100.0 * 6.26162576675415
Epoch 970, val loss: 1.0726125240325928
Epoch 980, training loss: 626.4749755859375 = 0.44875630736351013 + 100.0 * 6.260262489318848
Epoch 980, val loss: 1.073691725730896
Epoch 990, training loss: 626.1554565429688 = 0.4391687214374542 + 100.0 * 6.257163047790527
Epoch 990, val loss: 1.0744673013687134
Epoch 1000, training loss: 626.4090576171875 = 0.429819792509079 + 100.0 * 6.259792327880859
Epoch 1000, val loss: 1.0761504173278809
Epoch 1010, training loss: 625.9501342773438 = 0.4207040071487427 + 100.0 * 6.255294322967529
Epoch 1010, val loss: 1.0774047374725342
Epoch 1020, training loss: 625.7747802734375 = 0.41175577044487 + 100.0 * 6.253630638122559
Epoch 1020, val loss: 1.0791972875595093
Epoch 1030, training loss: 625.8159790039062 = 0.40299472212791443 + 100.0 * 6.254129886627197
Epoch 1030, val loss: 1.0810784101486206
Epoch 1040, training loss: 626.0458374023438 = 0.3943884074687958 + 100.0 * 6.256514072418213
Epoch 1040, val loss: 1.0828955173492432
Epoch 1050, training loss: 625.5991821289062 = 0.3858816921710968 + 100.0 * 6.252133369445801
Epoch 1050, val loss: 1.0854650735855103
Epoch 1060, training loss: 625.5762939453125 = 0.3776134252548218 + 100.0 * 6.251986980438232
Epoch 1060, val loss: 1.0878554582595825
Epoch 1070, training loss: 626.0899047851562 = 0.3695877194404602 + 100.0 * 6.257203578948975
Epoch 1070, val loss: 1.089980959892273
Epoch 1080, training loss: 625.5513916015625 = 0.36158961057662964 + 100.0 * 6.251898288726807
Epoch 1080, val loss: 1.0928291082382202
Epoch 1090, training loss: 625.3363037109375 = 0.3538950979709625 + 100.0 * 6.249824047088623
Epoch 1090, val loss: 1.0956263542175293
Epoch 1100, training loss: 625.7103271484375 = 0.34640923142433167 + 100.0 * 6.253638744354248
Epoch 1100, val loss: 1.098724126815796
Epoch 1110, training loss: 625.3345336914062 = 0.33896058797836304 + 100.0 * 6.249955177307129
Epoch 1110, val loss: 1.1009544134140015
Epoch 1120, training loss: 625.3161010742188 = 0.33172836899757385 + 100.0 * 6.249843597412109
Epoch 1120, val loss: 1.1043769121170044
Epoch 1130, training loss: 625.286865234375 = 0.324680358171463 + 100.0 * 6.249621868133545
Epoch 1130, val loss: 1.1071325540542603
Epoch 1140, training loss: 624.9999389648438 = 0.31780678033828735 + 100.0 * 6.246821403503418
Epoch 1140, val loss: 1.1110199689865112
Epoch 1150, training loss: 625.2772216796875 = 0.3111227750778198 + 100.0 * 6.249660968780518
Epoch 1150, val loss: 1.1143553256988525
Epoch 1160, training loss: 625.4144897460938 = 0.30451759696006775 + 100.0 * 6.251099586486816
Epoch 1160, val loss: 1.1177195310592651
Epoch 1170, training loss: 624.99951171875 = 0.2980203628540039 + 100.0 * 6.24701452255249
Epoch 1170, val loss: 1.1209512948989868
Epoch 1180, training loss: 624.7666015625 = 0.29174888134002686 + 100.0 * 6.244748592376709
Epoch 1180, val loss: 1.1244632005691528
Epoch 1190, training loss: 624.8889770507812 = 0.28566813468933105 + 100.0 * 6.246033191680908
Epoch 1190, val loss: 1.1281546354293823
Epoch 1200, training loss: 624.87939453125 = 0.279697448015213 + 100.0 * 6.245996952056885
Epoch 1200, val loss: 1.1322245597839355
Epoch 1210, training loss: 624.7115478515625 = 0.27377527952194214 + 100.0 * 6.244377136230469
Epoch 1210, val loss: 1.135454535484314
Epoch 1220, training loss: 624.6760864257812 = 0.2680857181549072 + 100.0 * 6.244080066680908
Epoch 1220, val loss: 1.1399914026260376
Epoch 1230, training loss: 624.5785522460938 = 0.2625105381011963 + 100.0 * 6.243160247802734
Epoch 1230, val loss: 1.1438531875610352
Epoch 1240, training loss: 624.7747802734375 = 0.25711897015571594 + 100.0 * 6.245176315307617
Epoch 1240, val loss: 1.1481088399887085
Epoch 1250, training loss: 624.5208129882812 = 0.25175294280052185 + 100.0 * 6.242690563201904
Epoch 1250, val loss: 1.1516722440719604
Epoch 1260, training loss: 624.6905517578125 = 0.24654212594032288 + 100.0 * 6.24444055557251
Epoch 1260, val loss: 1.15580153465271
Epoch 1270, training loss: 624.3977661132812 = 0.24144813418388367 + 100.0 * 6.241563320159912
Epoch 1270, val loss: 1.1599059104919434
Epoch 1280, training loss: 624.6595458984375 = 0.23647671937942505 + 100.0 * 6.244231224060059
Epoch 1280, val loss: 1.1642671823501587
Epoch 1290, training loss: 624.251708984375 = 0.23160851001739502 + 100.0 * 6.240200996398926
Epoch 1290, val loss: 1.1682286262512207
Epoch 1300, training loss: 624.1500244140625 = 0.22689206898212433 + 100.0 * 6.239231586456299
Epoch 1300, val loss: 1.1727404594421387
Epoch 1310, training loss: 624.1646728515625 = 0.22230906784534454 + 100.0 * 6.239423751831055
Epoch 1310, val loss: 1.176900863647461
Epoch 1320, training loss: 624.7470703125 = 0.21782606840133667 + 100.0 * 6.2452921867370605
Epoch 1320, val loss: 1.180937647819519
Epoch 1330, training loss: 624.259033203125 = 0.21332353353500366 + 100.0 * 6.240457057952881
Epoch 1330, val loss: 1.18593168258667
Epoch 1340, training loss: 624.1732177734375 = 0.2089618295431137 + 100.0 * 6.23964262008667
Epoch 1340, val loss: 1.1901452541351318
Epoch 1350, training loss: 624.0510864257812 = 0.2047709971666336 + 100.0 * 6.238462924957275
Epoch 1350, val loss: 1.1949132680892944
Epoch 1360, training loss: 624.109130859375 = 0.2006683200597763 + 100.0 * 6.239084720611572
Epoch 1360, val loss: 1.199285626411438
Epoch 1370, training loss: 624.0100708007812 = 0.1966414749622345 + 100.0 * 6.238133907318115
Epoch 1370, val loss: 1.2044188976287842
Epoch 1380, training loss: 624.166259765625 = 0.1926548331975937 + 100.0 * 6.239736557006836
Epoch 1380, val loss: 1.2087904214859009
Epoch 1390, training loss: 624.0529174804688 = 0.18878617882728577 + 100.0 * 6.238641262054443
Epoch 1390, val loss: 1.2123068571090698
Epoch 1400, training loss: 623.818115234375 = 0.18501238524913788 + 100.0 * 6.236330986022949
Epoch 1400, val loss: 1.2179616689682007
Epoch 1410, training loss: 624.207763671875 = 0.1813524216413498 + 100.0 * 6.240264415740967
Epoch 1410, val loss: 1.2212110757827759
Epoch 1420, training loss: 623.8551025390625 = 0.1777344048023224 + 100.0 * 6.236773490905762
Epoch 1420, val loss: 1.2270753383636475
Epoch 1430, training loss: 623.6446533203125 = 0.1741824448108673 + 100.0 * 6.234704494476318
Epoch 1430, val loss: 1.230960726737976
Epoch 1440, training loss: 623.5802612304688 = 0.17079614102840424 + 100.0 * 6.234095096588135
Epoch 1440, val loss: 1.236356258392334
Epoch 1450, training loss: 623.765869140625 = 0.1674851030111313 + 100.0 * 6.235983848571777
Epoch 1450, val loss: 1.241106629371643
Epoch 1460, training loss: 623.7548217773438 = 0.1641717553138733 + 100.0 * 6.235906600952148
Epoch 1460, val loss: 1.2451610565185547
Epoch 1470, training loss: 623.750244140625 = 0.16090638935565948 + 100.0 * 6.235893726348877
Epoch 1470, val loss: 1.2501698732376099
Epoch 1480, training loss: 623.6885375976562 = 0.1576923131942749 + 100.0 * 6.2353081703186035
Epoch 1480, val loss: 1.2539585828781128
Epoch 1490, training loss: 623.6362915039062 = 0.154621884226799 + 100.0 * 6.234817028045654
Epoch 1490, val loss: 1.2592228651046753
Epoch 1500, training loss: 623.3394165039062 = 0.15158690512180328 + 100.0 * 6.23187780380249
Epoch 1500, val loss: 1.2632898092269897
Epoch 1510, training loss: 623.3395385742188 = 0.1486787647008896 + 100.0 * 6.231908798217773
Epoch 1510, val loss: 1.2686084508895874
Epoch 1520, training loss: 623.6793212890625 = 0.14584021270275116 + 100.0 * 6.235335350036621
Epoch 1520, val loss: 1.27342689037323
Epoch 1530, training loss: 623.436767578125 = 0.14301079511642456 + 100.0 * 6.232937335968018
Epoch 1530, val loss: 1.277323842048645
Epoch 1540, training loss: 623.2637329101562 = 0.1402312070131302 + 100.0 * 6.231234550476074
Epoch 1540, val loss: 1.282586693763733
Epoch 1550, training loss: 623.3580322265625 = 0.13753338158130646 + 100.0 * 6.232205390930176
Epoch 1550, val loss: 1.286744236946106
Epoch 1560, training loss: 623.2933349609375 = 0.13491742312908173 + 100.0 * 6.231584548950195
Epoch 1560, val loss: 1.2918336391448975
Epoch 1570, training loss: 623.3927001953125 = 0.13232363760471344 + 100.0 * 6.232604026794434
Epoch 1570, val loss: 1.2965925931930542
Epoch 1580, training loss: 623.1674194335938 = 0.12983156740665436 + 100.0 * 6.230376243591309
Epoch 1580, val loss: 1.301134467124939
Epoch 1590, training loss: 623.4728393554688 = 0.12738484144210815 + 100.0 * 6.233454704284668
Epoch 1590, val loss: 1.3057180643081665
Epoch 1600, training loss: 623.1246337890625 = 0.12494346499443054 + 100.0 * 6.229997158050537
Epoch 1600, val loss: 1.3105525970458984
Epoch 1610, training loss: 623.060546875 = 0.12256083637475967 + 100.0 * 6.229379653930664
Epoch 1610, val loss: 1.3157408237457275
Epoch 1620, training loss: 623.06005859375 = 0.12028579413890839 + 100.0 * 6.229397773742676
Epoch 1620, val loss: 1.320306658744812
Epoch 1630, training loss: 623.1843872070312 = 0.11804195493459702 + 100.0 * 6.230663299560547
Epoch 1630, val loss: 1.3250901699066162
Epoch 1640, training loss: 623.3637084960938 = 0.11582186073064804 + 100.0 * 6.232478618621826
Epoch 1640, val loss: 1.3293107748031616
Epoch 1650, training loss: 622.9535522460938 = 0.1136271208524704 + 100.0 * 6.22839879989624
Epoch 1650, val loss: 1.3344783782958984
Epoch 1660, training loss: 622.84765625 = 0.11151813715696335 + 100.0 * 6.227361679077148
Epoch 1660, val loss: 1.3386380672454834
Epoch 1670, training loss: 622.7814331054688 = 0.10947771370410919 + 100.0 * 6.226719379425049
Epoch 1670, val loss: 1.3436543941497803
Epoch 1680, training loss: 623.1774291992188 = 0.10748539119958878 + 100.0 * 6.23069953918457
Epoch 1680, val loss: 1.3479303121566772
Epoch 1690, training loss: 622.8880615234375 = 0.105488121509552 + 100.0 * 6.227826118469238
Epoch 1690, val loss: 1.3534576892852783
Epoch 1700, training loss: 622.9144897460938 = 0.10355685651302338 + 100.0 * 6.228109359741211
Epoch 1700, val loss: 1.3578746318817139
Epoch 1710, training loss: 622.992431640625 = 0.10163714736700058 + 100.0 * 6.228908061981201
Epoch 1710, val loss: 1.3623335361480713
Epoch 1720, training loss: 622.7279052734375 = 0.09976457804441452 + 100.0 * 6.22628116607666
Epoch 1720, val loss: 1.3675740957260132
Epoch 1730, training loss: 622.60791015625 = 0.09796992689371109 + 100.0 * 6.225099563598633
Epoch 1730, val loss: 1.372461199760437
Epoch 1740, training loss: 623.511962890625 = 0.09625031799077988 + 100.0 * 6.234157085418701
Epoch 1740, val loss: 1.3783035278320312
Epoch 1750, training loss: 622.8961181640625 = 0.09445217251777649 + 100.0 * 6.228016376495361
Epoch 1750, val loss: 1.3807429075241089
Epoch 1760, training loss: 622.5869750976562 = 0.09272695332765579 + 100.0 * 6.224942684173584
Epoch 1760, val loss: 1.386582612991333
Epoch 1770, training loss: 622.4708251953125 = 0.09109137207269669 + 100.0 * 6.22379732131958
Epoch 1770, val loss: 1.3913058042526245
Epoch 1780, training loss: 622.4738159179688 = 0.08950795233249664 + 100.0 * 6.223843097686768
Epoch 1780, val loss: 1.3961079120635986
Epoch 1790, training loss: 623.2066040039062 = 0.08795176446437836 + 100.0 * 6.231186389923096
Epoch 1790, val loss: 1.400876760482788
Epoch 1800, training loss: 623.294677734375 = 0.08635975420475006 + 100.0 * 6.232082843780518
Epoch 1800, val loss: 1.405932903289795
Epoch 1810, training loss: 622.56201171875 = 0.08476872742176056 + 100.0 * 6.2247724533081055
Epoch 1810, val loss: 1.4100518226623535
Epoch 1820, training loss: 622.3121337890625 = 0.08328434824943542 + 100.0 * 6.222288131713867
Epoch 1820, val loss: 1.4147945642471313
Epoch 1830, training loss: 622.3130493164062 = 0.08185978978872299 + 100.0 * 6.222311973571777
Epoch 1830, val loss: 1.4198598861694336
Epoch 1840, training loss: 622.7979125976562 = 0.08045929670333862 + 100.0 * 6.227174758911133
Epoch 1840, val loss: 1.424156904220581
Epoch 1850, training loss: 622.3819580078125 = 0.0790385976433754 + 100.0 * 6.223029136657715
Epoch 1850, val loss: 1.4295135736465454
Epoch 1860, training loss: 622.6884765625 = 0.07763204723596573 + 100.0 * 6.226108551025391
Epoch 1860, val loss: 1.4334558248519897
Epoch 1870, training loss: 622.2996215820312 = 0.07629036903381348 + 100.0 * 6.222233295440674
Epoch 1870, val loss: 1.4383512735366821
Epoch 1880, training loss: 622.213134765625 = 0.07498598843812943 + 100.0 * 6.221381187438965
Epoch 1880, val loss: 1.4430700540542603
Epoch 1890, training loss: 622.1290283203125 = 0.07373135536909103 + 100.0 * 6.220552921295166
Epoch 1890, val loss: 1.4481744766235352
Epoch 1900, training loss: 622.3358154296875 = 0.07249987125396729 + 100.0 * 6.222632884979248
Epoch 1900, val loss: 1.4526852369308472
Epoch 1910, training loss: 622.4526977539062 = 0.07124588638544083 + 100.0 * 6.223814964294434
Epoch 1910, val loss: 1.4571983814239502
Epoch 1920, training loss: 622.3941650390625 = 0.070005863904953 + 100.0 * 6.223241806030273
Epoch 1920, val loss: 1.46170973777771
Epoch 1930, training loss: 622.1427001953125 = 0.06881444156169891 + 100.0 * 6.220738887786865
Epoch 1930, val loss: 1.4665179252624512
Epoch 1940, training loss: 622.0626220703125 = 0.06768319010734558 + 100.0 * 6.219949245452881
Epoch 1940, val loss: 1.4712796211242676
Epoch 1950, training loss: 622.1653442382812 = 0.0665806233882904 + 100.0 * 6.220987796783447
Epoch 1950, val loss: 1.4754031896591187
Epoch 1960, training loss: 622.4813842773438 = 0.06548569351434708 + 100.0 * 6.224159240722656
Epoch 1960, val loss: 1.4800643920898438
Epoch 1970, training loss: 622.515380859375 = 0.06435547769069672 + 100.0 * 6.224510669708252
Epoch 1970, val loss: 1.485516905784607
Epoch 1980, training loss: 621.9847412109375 = 0.06326745450496674 + 100.0 * 6.21921443939209
Epoch 1980, val loss: 1.4897828102111816
Epoch 1990, training loss: 621.9255981445312 = 0.062230877578258514 + 100.0 * 6.21863317489624
Epoch 1990, val loss: 1.494201421737671
Epoch 2000, training loss: 621.9073486328125 = 0.06123683601617813 + 100.0 * 6.218461513519287
Epoch 2000, val loss: 1.4988975524902344
Epoch 2010, training loss: 622.2211303710938 = 0.060266125947237015 + 100.0 * 6.221609115600586
Epoch 2010, val loss: 1.502541184425354
Epoch 2020, training loss: 621.9718627929688 = 0.05928406864404678 + 100.0 * 6.219126224517822
Epoch 2020, val loss: 1.508522868156433
Epoch 2030, training loss: 622.0285034179688 = 0.05832471698522568 + 100.0 * 6.219701766967773
Epoch 2030, val loss: 1.5126605033874512
Epoch 2040, training loss: 622.057861328125 = 0.057400092482566833 + 100.0 * 6.220005035400391
Epoch 2040, val loss: 1.5174624919891357
Epoch 2050, training loss: 621.93994140625 = 0.05647363141179085 + 100.0 * 6.21883487701416
Epoch 2050, val loss: 1.5215195417404175
Epoch 2060, training loss: 621.9510498046875 = 0.05558283254504204 + 100.0 * 6.218954563140869
Epoch 2060, val loss: 1.5263503789901733
Epoch 2070, training loss: 621.8511962890625 = 0.054705001413822174 + 100.0 * 6.217965126037598
Epoch 2070, val loss: 1.5306429862976074
Epoch 2080, training loss: 622.140380859375 = 0.053849976509809494 + 100.0 * 6.220865249633789
Epoch 2080, val loss: 1.5346678495407104
Epoch 2090, training loss: 622.0384521484375 = 0.052998680621385574 + 100.0 * 6.219854831695557
Epoch 2090, val loss: 1.5398736000061035
Epoch 2100, training loss: 621.760009765625 = 0.05215861275792122 + 100.0 * 6.21707820892334
Epoch 2100, val loss: 1.5439423322677612
Epoch 2110, training loss: 621.879150390625 = 0.05136392265558243 + 100.0 * 6.218277454376221
Epoch 2110, val loss: 1.5487746000289917
Epoch 2120, training loss: 621.9119873046875 = 0.05056793615221977 + 100.0 * 6.218614101409912
Epoch 2120, val loss: 1.5527664422988892
Epoch 2130, training loss: 621.7067260742188 = 0.04978041350841522 + 100.0 * 6.216568946838379
Epoch 2130, val loss: 1.5575460195541382
Epoch 2140, training loss: 621.7935791015625 = 0.0490272119641304 + 100.0 * 6.2174458503723145
Epoch 2140, val loss: 1.5613765716552734
Epoch 2150, training loss: 622.1417846679688 = 0.048300255089998245 + 100.0 * 6.220935344696045
Epoch 2150, val loss: 1.5661842823028564
Epoch 2160, training loss: 621.9111328125 = 0.04753962531685829 + 100.0 * 6.2186360359191895
Epoch 2160, val loss: 1.570165753364563
Epoch 2170, training loss: 622.060546875 = 0.0468059703707695 + 100.0 * 6.220137596130371
Epoch 2170, val loss: 1.5745514631271362
Epoch 2180, training loss: 621.6886596679688 = 0.046081483364105225 + 100.0 * 6.216425895690918
Epoch 2180, val loss: 1.579533576965332
Epoch 2190, training loss: 621.5215454101562 = 0.045408863574266434 + 100.0 * 6.214761257171631
Epoch 2190, val loss: 1.5837947130203247
Epoch 2200, training loss: 621.5477294921875 = 0.04474658891558647 + 100.0 * 6.215030193328857
Epoch 2200, val loss: 1.587794303894043
Epoch 2210, training loss: 621.9444580078125 = 0.0441063717007637 + 100.0 * 6.219003200531006
Epoch 2210, val loss: 1.5924309492111206
Epoch 2220, training loss: 621.909912109375 = 0.043437663465738297 + 100.0 * 6.218664646148682
Epoch 2220, val loss: 1.5964301824569702
Epoch 2230, training loss: 621.4962768554688 = 0.04277561232447624 + 100.0 * 6.214534759521484
Epoch 2230, val loss: 1.6007808446884155
Epoch 2240, training loss: 621.7623901367188 = 0.04214799776673317 + 100.0 * 6.217202186584473
Epoch 2240, val loss: 1.604862928390503
Epoch 2250, training loss: 621.7407836914062 = 0.04153399541974068 + 100.0 * 6.2169928550720215
Epoch 2250, val loss: 1.6098319292068481
Epoch 2260, training loss: 621.7083129882812 = 0.04092374071478844 + 100.0 * 6.216674327850342
Epoch 2260, val loss: 1.613959789276123
Epoch 2270, training loss: 621.4960327148438 = 0.040315546095371246 + 100.0 * 6.21455717086792
Epoch 2270, val loss: 1.6171482801437378
Epoch 2280, training loss: 621.3995971679688 = 0.03974531590938568 + 100.0 * 6.213598728179932
Epoch 2280, val loss: 1.6222410202026367
Epoch 2290, training loss: 621.37109375 = 0.039184536784887314 + 100.0 * 6.213318824768066
Epoch 2290, val loss: 1.6264055967330933
Epoch 2300, training loss: 621.7377319335938 = 0.03864355385303497 + 100.0 * 6.216990947723389
Epoch 2300, val loss: 1.63015615940094
Epoch 2310, training loss: 621.5631103515625 = 0.03807636722922325 + 100.0 * 6.215250492095947
Epoch 2310, val loss: 1.6354174613952637
Epoch 2320, training loss: 621.4590454101562 = 0.037524543702602386 + 100.0 * 6.214215278625488
Epoch 2320, val loss: 1.6390295028686523
Epoch 2330, training loss: 621.3685913085938 = 0.03699648007750511 + 100.0 * 6.213315963745117
Epoch 2330, val loss: 1.6432667970657349
Epoch 2340, training loss: 621.5562133789062 = 0.036489102989435196 + 100.0 * 6.2151970863342285
Epoch 2340, val loss: 1.6465340852737427
Epoch 2350, training loss: 621.3655395507812 = 0.03596959635615349 + 100.0 * 6.213295936584473
Epoch 2350, val loss: 1.6516166925430298
Epoch 2360, training loss: 621.26416015625 = 0.035472143441438675 + 100.0 * 6.212286949157715
Epoch 2360, val loss: 1.6561763286590576
Epoch 2370, training loss: 621.3526000976562 = 0.034994304180145264 + 100.0 * 6.213176250457764
Epoch 2370, val loss: 1.6593732833862305
Epoch 2380, training loss: 621.461181640625 = 0.03451482951641083 + 100.0 * 6.214266777038574
Epoch 2380, val loss: 1.6636699438095093
Epoch 2390, training loss: 621.49169921875 = 0.034031543880701065 + 100.0 * 6.214576721191406
Epoch 2390, val loss: 1.667428731918335
Epoch 2400, training loss: 621.325927734375 = 0.03355947509407997 + 100.0 * 6.212923526763916
Epoch 2400, val loss: 1.6715894937515259
Epoch 2410, training loss: 621.3504028320312 = 0.03310598433017731 + 100.0 * 6.2131733894348145
Epoch 2410, val loss: 1.6760531663894653
Epoch 2420, training loss: 621.1279907226562 = 0.03266279399394989 + 100.0 * 6.210953235626221
Epoch 2420, val loss: 1.6797430515289307
Epoch 2430, training loss: 621.2879638671875 = 0.03223838657140732 + 100.0 * 6.212557315826416
Epoch 2430, val loss: 1.6841871738433838
Epoch 2440, training loss: 621.4129028320312 = 0.0318019837141037 + 100.0 * 6.213810920715332
Epoch 2440, val loss: 1.6879041194915771
Epoch 2450, training loss: 621.122802734375 = 0.03136928379535675 + 100.0 * 6.210914134979248
Epoch 2450, val loss: 1.6917952299118042
Epoch 2460, training loss: 621.447509765625 = 0.030961189419031143 + 100.0 * 6.214165687561035
Epoch 2460, val loss: 1.6960482597351074
Epoch 2470, training loss: 621.1343994140625 = 0.030542029067873955 + 100.0 * 6.211039066314697
Epoch 2470, val loss: 1.7002238035202026
Epoch 2480, training loss: 621.0623168945312 = 0.030142422765493393 + 100.0 * 6.210321426391602
Epoch 2480, val loss: 1.7036008834838867
Epoch 2490, training loss: 621.4522705078125 = 0.029767785221338272 + 100.0 * 6.214224815368652
Epoch 2490, val loss: 1.7086890935897827
Epoch 2500, training loss: 621.01318359375 = 0.02935595065355301 + 100.0 * 6.209838390350342
Epoch 2500, val loss: 1.710340976715088
Epoch 2510, training loss: 620.9861450195312 = 0.02898036316037178 + 100.0 * 6.209571361541748
Epoch 2510, val loss: 1.7149648666381836
Epoch 2520, training loss: 621.292724609375 = 0.028632495552301407 + 100.0 * 6.212640762329102
Epoch 2520, val loss: 1.7182961702346802
Epoch 2530, training loss: 621.1400146484375 = 0.02824999950826168 + 100.0 * 6.211117744445801
Epoch 2530, val loss: 1.722376823425293
Epoch 2540, training loss: 620.96337890625 = 0.02787141688168049 + 100.0 * 6.209354877471924
Epoch 2540, val loss: 1.7254096269607544
Epoch 2550, training loss: 620.857421875 = 0.027518050745129585 + 100.0 * 6.208299160003662
Epoch 2550, val loss: 1.7297619581222534
Epoch 2560, training loss: 620.9218139648438 = 0.02718265913426876 + 100.0 * 6.208946704864502
Epoch 2560, val loss: 1.7335861921310425
Epoch 2570, training loss: 621.593017578125 = 0.026853716000914574 + 100.0 * 6.215661525726318
Epoch 2570, val loss: 1.737499475479126
Epoch 2580, training loss: 621.1412963867188 = 0.02650274895131588 + 100.0 * 6.211147785186768
Epoch 2580, val loss: 1.7400952577590942
Epoch 2590, training loss: 620.9131469726562 = 0.026163317263126373 + 100.0 * 6.208869457244873
Epoch 2590, val loss: 1.7442244291305542
Epoch 2600, training loss: 621.1883544921875 = 0.025847777724266052 + 100.0 * 6.211625576019287
Epoch 2600, val loss: 1.7478710412979126
Epoch 2610, training loss: 620.7748413085938 = 0.025517815724015236 + 100.0 * 6.207493305206299
Epoch 2610, val loss: 1.7511805295944214
Epoch 2620, training loss: 620.858154296875 = 0.025209972634911537 + 100.0 * 6.208329200744629
Epoch 2620, val loss: 1.7549285888671875
Epoch 2630, training loss: 620.8743896484375 = 0.02491130493581295 + 100.0 * 6.208495140075684
Epoch 2630, val loss: 1.7582277059555054
Epoch 2640, training loss: 621.4223022460938 = 0.024612778797745705 + 100.0 * 6.213976860046387
Epoch 2640, val loss: 1.7616028785705566
Epoch 2650, training loss: 620.988037109375 = 0.02431083470582962 + 100.0 * 6.209637641906738
Epoch 2650, val loss: 1.7657787799835205
Epoch 2660, training loss: 621.007080078125 = 0.02401425689458847 + 100.0 * 6.2098307609558105
Epoch 2660, val loss: 1.7690244913101196
Epoch 2670, training loss: 620.885498046875 = 0.023726074025034904 + 100.0 * 6.208617687225342
Epoch 2670, val loss: 1.7729631662368774
Epoch 2680, training loss: 620.794921875 = 0.02344195917248726 + 100.0 * 6.207715034484863
Epoch 2680, val loss: 1.775818943977356
Epoch 2690, training loss: 620.7426147460938 = 0.023169245570898056 + 100.0 * 6.2071943283081055
Epoch 2690, val loss: 1.779414415359497
Epoch 2700, training loss: 621.054443359375 = 0.02290765941143036 + 100.0 * 6.210315704345703
Epoch 2700, val loss: 1.7830442190170288
Epoch 2710, training loss: 620.7415771484375 = 0.022625574842095375 + 100.0 * 6.207189083099365
Epoch 2710, val loss: 1.7855874300003052
Epoch 2720, training loss: 620.6756591796875 = 0.0223634485155344 + 100.0 * 6.206532955169678
Epoch 2720, val loss: 1.7892000675201416
Epoch 2730, training loss: 621.1534423828125 = 0.02212272584438324 + 100.0 * 6.211313247680664
Epoch 2730, val loss: 1.7926548719406128
Epoch 2740, training loss: 621.1755981445312 = 0.02184748649597168 + 100.0 * 6.2115373611450195
Epoch 2740, val loss: 1.7958447933197021
Epoch 2750, training loss: 620.7993774414062 = 0.021577177569270134 + 100.0 * 6.207777500152588
Epoch 2750, val loss: 1.7985080480575562
Epoch 2760, training loss: 620.650634765625 = 0.021329913288354874 + 100.0 * 6.206293106079102
Epoch 2760, val loss: 1.8023556470870972
Epoch 2770, training loss: 620.7457885742188 = 0.021099042147397995 + 100.0 * 6.207246780395508
Epoch 2770, val loss: 1.804700493812561
Epoch 2780, training loss: 620.8370361328125 = 0.020861035212874413 + 100.0 * 6.2081618309021
Epoch 2780, val loss: 1.8081501722335815
Epoch 2790, training loss: 620.670654296875 = 0.020624347031116486 + 100.0 * 6.206500053405762
Epoch 2790, val loss: 1.8130691051483154
Epoch 2800, training loss: 620.8150024414062 = 0.020394358783960342 + 100.0 * 6.207946300506592
Epoch 2800, val loss: 1.8149181604385376
Epoch 2810, training loss: 620.5703125 = 0.020155753940343857 + 100.0 * 6.205501556396484
Epoch 2810, val loss: 1.8186264038085938
Epoch 2820, training loss: 620.7374877929688 = 0.019935032352805138 + 100.0 * 6.207175254821777
Epoch 2820, val loss: 1.821088433265686
Epoch 2830, training loss: 620.6558227539062 = 0.019710801541805267 + 100.0 * 6.206361293792725
Epoch 2830, val loss: 1.8241854906082153
Epoch 2840, training loss: 620.5703735351562 = 0.019498514011502266 + 100.0 * 6.205509185791016
Epoch 2840, val loss: 1.8285579681396484
Epoch 2850, training loss: 620.6967163085938 = 0.019291752949357033 + 100.0 * 6.2067742347717285
Epoch 2850, val loss: 1.8307609558105469
Epoch 2860, training loss: 620.755615234375 = 0.019083958119153976 + 100.0 * 6.207365036010742
Epoch 2860, val loss: 1.8345316648483276
Epoch 2870, training loss: 620.6609497070312 = 0.01886449195444584 + 100.0 * 6.2064208984375
Epoch 2870, val loss: 1.8369392156600952
Epoch 2880, training loss: 620.4205932617188 = 0.018654804676771164 + 100.0 * 6.204019069671631
Epoch 2880, val loss: 1.8399649858474731
Epoch 2890, training loss: 620.4411010742188 = 0.01845976710319519 + 100.0 * 6.204226970672607
Epoch 2890, val loss: 1.8431733846664429
Epoch 2900, training loss: 620.9796142578125 = 0.01826922409236431 + 100.0 * 6.209613800048828
Epoch 2900, val loss: 1.846107840538025
Epoch 2910, training loss: 620.7256469726562 = 0.018077930435538292 + 100.0 * 6.207076072692871
Epoch 2910, val loss: 1.8496488332748413
Epoch 2920, training loss: 620.4614868164062 = 0.01787039078772068 + 100.0 * 6.204436302185059
Epoch 2920, val loss: 1.85191011428833
Epoch 2930, training loss: 620.5084838867188 = 0.0176846981048584 + 100.0 * 6.2049078941345215
Epoch 2930, val loss: 1.855330228805542
Epoch 2940, training loss: 620.454833984375 = 0.017500029876828194 + 100.0 * 6.204373359680176
Epoch 2940, val loss: 1.8581230640411377
Epoch 2950, training loss: 620.83349609375 = 0.0173256266862154 + 100.0 * 6.208161354064941
Epoch 2950, val loss: 1.8615459203720093
Epoch 2960, training loss: 620.561279296875 = 0.01713692955672741 + 100.0 * 6.205441474914551
Epoch 2960, val loss: 1.8627471923828125
Epoch 2970, training loss: 620.3362426757812 = 0.016954200342297554 + 100.0 * 6.203192710876465
Epoch 2970, val loss: 1.8666938543319702
Epoch 2980, training loss: 620.349853515625 = 0.016787445172667503 + 100.0 * 6.2033305168151855
Epoch 2980, val loss: 1.8697494268417358
Epoch 2990, training loss: 620.821044921875 = 0.0166289284825325 + 100.0 * 6.208044528961182
Epoch 2990, val loss: 1.8730727434158325
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6777777777777778
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 861.6360473632812 = 1.954250454902649 + 100.0 * 8.596817970275879
Epoch 0, val loss: 1.9544214010238647
Epoch 10, training loss: 861.5382080078125 = 1.946321725845337 + 100.0 * 8.595918655395508
Epoch 10, val loss: 1.946730136871338
Epoch 20, training loss: 860.9307250976562 = 1.9365198612213135 + 100.0 * 8.58994197845459
Epoch 20, val loss: 1.9367761611938477
Epoch 30, training loss: 856.2783813476562 = 1.9236881732940674 + 100.0 * 8.543546676635742
Epoch 30, val loss: 1.9233554601669312
Epoch 40, training loss: 810.107177734375 = 1.908111333847046 + 100.0 * 8.081991195678711
Epoch 40, val loss: 1.9067102670669556
Epoch 50, training loss: 749.941650390625 = 1.8926165103912354 + 100.0 * 7.480490207672119
Epoch 50, val loss: 1.8909324407577515
Epoch 60, training loss: 714.4739379882812 = 1.8816519975662231 + 100.0 * 7.125923156738281
Epoch 60, val loss: 1.8801974058151245
Epoch 70, training loss: 688.8441772460938 = 1.8710672855377197 + 100.0 * 6.8697309494018555
Epoch 70, val loss: 1.8693430423736572
Epoch 80, training loss: 676.057373046875 = 1.8615890741348267 + 100.0 * 6.741958141326904
Epoch 80, val loss: 1.859828233718872
Epoch 90, training loss: 669.501953125 = 1.8518587350845337 + 100.0 * 6.6765007972717285
Epoch 90, val loss: 1.850096583366394
Epoch 100, training loss: 664.7312622070312 = 1.8421295881271362 + 100.0 * 6.628891468048096
Epoch 100, val loss: 1.8401354551315308
Epoch 110, training loss: 661.1478881835938 = 1.8331022262573242 + 100.0 * 6.5931477546691895
Epoch 110, val loss: 1.8306353092193604
Epoch 120, training loss: 658.1846923828125 = 1.8249921798706055 + 100.0 * 6.563596725463867
Epoch 120, val loss: 1.8217453956604004
Epoch 130, training loss: 655.726806640625 = 1.8175405263900757 + 100.0 * 6.539092540740967
Epoch 130, val loss: 1.8134636878967285
Epoch 140, training loss: 653.6871337890625 = 1.8104705810546875 + 100.0 * 6.518766403198242
Epoch 140, val loss: 1.805625319480896
Epoch 150, training loss: 651.7683715820312 = 1.8037219047546387 + 100.0 * 6.4996466636657715
Epoch 150, val loss: 1.798176884651184
Epoch 160, training loss: 650.1089477539062 = 1.7971473932266235 + 100.0 * 6.483118534088135
Epoch 160, val loss: 1.7910326719284058
Epoch 170, training loss: 648.6380004882812 = 1.7905558347702026 + 100.0 * 6.468474864959717
Epoch 170, val loss: 1.7840791940689087
Epoch 180, training loss: 647.345947265625 = 1.7837644815444946 + 100.0 * 6.455621719360352
Epoch 180, val loss: 1.777152419090271
Epoch 190, training loss: 645.9966430664062 = 1.776685357093811 + 100.0 * 6.44219970703125
Epoch 190, val loss: 1.7701165676116943
Epoch 200, training loss: 644.9575805664062 = 1.7691861391067505 + 100.0 * 6.431884288787842
Epoch 200, val loss: 1.7629133462905884
Epoch 210, training loss: 644.3052978515625 = 1.761181116104126 + 100.0 * 6.425441265106201
Epoch 210, val loss: 1.7552613019943237
Epoch 220, training loss: 643.190673828125 = 1.7524826526641846 + 100.0 * 6.414381980895996
Epoch 220, val loss: 1.747226595878601
Epoch 230, training loss: 642.2660522460938 = 1.7431517839431763 + 100.0 * 6.405228614807129
Epoch 230, val loss: 1.7386616468429565
Epoch 240, training loss: 641.4971313476562 = 1.7331255674362183 + 100.0 * 6.397639751434326
Epoch 240, val loss: 1.7295361757278442
Epoch 250, training loss: 640.7736206054688 = 1.7222319841384888 + 100.0 * 6.390513896942139
Epoch 250, val loss: 1.719756007194519
Epoch 260, training loss: 640.162841796875 = 1.7104939222335815 + 100.0 * 6.384523391723633
Epoch 260, val loss: 1.7092243432998657
Epoch 270, training loss: 639.7401733398438 = 1.697803258895874 + 100.0 * 6.380423545837402
Epoch 270, val loss: 1.6979804039001465
Epoch 280, training loss: 638.9985961914062 = 1.6841355562210083 + 100.0 * 6.373144626617432
Epoch 280, val loss: 1.6858919858932495
Epoch 290, training loss: 638.3780517578125 = 1.6694397926330566 + 100.0 * 6.367086410522461
Epoch 290, val loss: 1.672981858253479
Epoch 300, training loss: 637.8565673828125 = 1.6538039445877075 + 100.0 * 6.362027645111084
Epoch 300, val loss: 1.6593425273895264
Epoch 310, training loss: 637.8643188476562 = 1.6370488405227661 + 100.0 * 6.3622727394104
Epoch 310, val loss: 1.6448882818222046
Epoch 320, training loss: 636.9722900390625 = 1.6193071603775024 + 100.0 * 6.353529930114746
Epoch 320, val loss: 1.6295608282089233
Epoch 330, training loss: 636.4640502929688 = 1.6006237268447876 + 100.0 * 6.348634243011475
Epoch 330, val loss: 1.6135585308074951
Epoch 340, training loss: 636.5382080078125 = 1.5810706615447998 + 100.0 * 6.349571704864502
Epoch 340, val loss: 1.5968122482299805
Epoch 350, training loss: 635.7858276367188 = 1.5604124069213867 + 100.0 * 6.342254161834717
Epoch 350, val loss: 1.5795704126358032
Epoch 360, training loss: 635.3561401367188 = 1.5390876531600952 + 100.0 * 6.338170528411865
Epoch 360, val loss: 1.5618008375167847
Epoch 370, training loss: 635.20947265625 = 1.5170656442642212 + 100.0 * 6.336924076080322
Epoch 370, val loss: 1.5436586141586304
Epoch 380, training loss: 634.694580078125 = 1.4945470094680786 + 100.0 * 6.332000255584717
Epoch 380, val loss: 1.5250879526138306
Epoch 390, training loss: 634.276123046875 = 1.4714452028274536 + 100.0 * 6.328046798706055
Epoch 390, val loss: 1.506387710571289
Epoch 400, training loss: 634.1882934570312 = 1.4480736255645752 + 100.0 * 6.327402114868164
Epoch 400, val loss: 1.4875986576080322
Epoch 410, training loss: 633.9568481445312 = 1.424268364906311 + 100.0 * 6.325325965881348
Epoch 410, val loss: 1.4686685800552368
Epoch 420, training loss: 633.3707275390625 = 1.400382161140442 + 100.0 * 6.319703578948975
Epoch 420, val loss: 1.449858546257019
Epoch 430, training loss: 633.25048828125 = 1.3765336275100708 + 100.0 * 6.318739414215088
Epoch 430, val loss: 1.4313535690307617
Epoch 440, training loss: 632.8715209960938 = 1.3527867794036865 + 100.0 * 6.315187454223633
Epoch 440, val loss: 1.412867546081543
Epoch 450, training loss: 632.4843139648438 = 1.3292192220687866 + 100.0 * 6.311550617218018
Epoch 450, val loss: 1.3949953317642212
Epoch 460, training loss: 632.8867797851562 = 1.3059253692626953 + 100.0 * 6.3158087730407715
Epoch 460, val loss: 1.377442717552185
Epoch 470, training loss: 632.0367431640625 = 1.2829138040542603 + 100.0 * 6.3075385093688965
Epoch 470, val loss: 1.360328197479248
Epoch 480, training loss: 631.7119750976562 = 1.2603847980499268 + 100.0 * 6.304515838623047
Epoch 480, val loss: 1.3437962532043457
Epoch 490, training loss: 631.4707641601562 = 1.2384313344955444 + 100.0 * 6.302323341369629
Epoch 490, val loss: 1.3279211521148682
Epoch 500, training loss: 631.8585815429688 = 1.2170697450637817 + 100.0 * 6.306415557861328
Epoch 500, val loss: 1.3125216960906982
Epoch 510, training loss: 631.36572265625 = 1.196108102798462 + 100.0 * 6.301696300506592
Epoch 510, val loss: 1.2980926036834717
Epoch 520, training loss: 630.9408569335938 = 1.1756778955459595 + 100.0 * 6.297651767730713
Epoch 520, val loss: 1.2841054201126099
Epoch 530, training loss: 630.7422485351562 = 1.156002402305603 + 100.0 * 6.295862197875977
Epoch 530, val loss: 1.2708340883255005
Epoch 540, training loss: 630.4428100585938 = 1.1369012594223022 + 100.0 * 6.2930588722229
Epoch 540, val loss: 1.258027195930481
Epoch 550, training loss: 630.4395141601562 = 1.1184113025665283 + 100.0 * 6.293210983276367
Epoch 550, val loss: 1.2460744380950928
Epoch 560, training loss: 630.1417846679688 = 1.1004323959350586 + 100.0 * 6.290413856506348
Epoch 560, val loss: 1.234675407409668
Epoch 570, training loss: 629.9595336914062 = 1.083016276359558 + 100.0 * 6.2887654304504395
Epoch 570, val loss: 1.2240761518478394
Epoch 580, training loss: 629.9963989257812 = 1.0662052631378174 + 100.0 * 6.289301872253418
Epoch 580, val loss: 1.2139614820480347
Epoch 590, training loss: 629.7938842773438 = 1.0499306917190552 + 100.0 * 6.287439346313477
Epoch 590, val loss: 1.2042030096054077
Epoch 600, training loss: 629.5032348632812 = 1.0340282917022705 + 100.0 * 6.28469181060791
Epoch 600, val loss: 1.1953245401382446
Epoch 610, training loss: 629.4237670898438 = 1.0186665058135986 + 100.0 * 6.284050941467285
Epoch 610, val loss: 1.1865921020507812
Epoch 620, training loss: 629.1853637695312 = 1.003783941268921 + 100.0 * 6.281816005706787
Epoch 620, val loss: 1.1784955263137817
Epoch 630, training loss: 629.2095947265625 = 0.9893085956573486 + 100.0 * 6.28220272064209
Epoch 630, val loss: 1.1709359884262085
Epoch 640, training loss: 629.0114135742188 = 0.9751653671264648 + 100.0 * 6.280362606048584
Epoch 640, val loss: 1.1631217002868652
Epoch 650, training loss: 628.889404296875 = 0.9613550305366516 + 100.0 * 6.279280662536621
Epoch 650, val loss: 1.1563464403152466
Epoch 660, training loss: 628.5111083984375 = 0.9479228854179382 + 100.0 * 6.275631427764893
Epoch 660, val loss: 1.1497900485992432
Epoch 670, training loss: 628.47021484375 = 0.9349191188812256 + 100.0 * 6.275352478027344
Epoch 670, val loss: 1.1436065435409546
Epoch 680, training loss: 628.45751953125 = 0.9222264885902405 + 100.0 * 6.275352954864502
Epoch 680, val loss: 1.1379941701889038
Epoch 690, training loss: 628.3851928710938 = 0.9097422957420349 + 100.0 * 6.274754524230957
Epoch 690, val loss: 1.1325819492340088
Epoch 700, training loss: 628.482666015625 = 0.8974586129188538 + 100.0 * 6.275852203369141
Epoch 700, val loss: 1.1271388530731201
Epoch 710, training loss: 628.1530151367188 = 0.8854491114616394 + 100.0 * 6.272675514221191
Epoch 710, val loss: 1.1220154762268066
Epoch 720, training loss: 627.8839721679688 = 0.8737110495567322 + 100.0 * 6.270102500915527
Epoch 720, val loss: 1.117472767829895
Epoch 730, training loss: 627.8909912109375 = 0.8622710108757019 + 100.0 * 6.27028751373291
Epoch 730, val loss: 1.1130975484848022
Epoch 740, training loss: 627.5775756835938 = 0.8510284423828125 + 100.0 * 6.267265796661377
Epoch 740, val loss: 1.1087439060211182
Epoch 750, training loss: 627.9551391601562 = 0.8399884104728699 + 100.0 * 6.271151542663574
Epoch 750, val loss: 1.1049689054489136
Epoch 760, training loss: 627.6869506835938 = 0.8289831876754761 + 100.0 * 6.268579959869385
Epoch 760, val loss: 1.1010208129882812
Epoch 770, training loss: 627.383056640625 = 0.8182165622711182 + 100.0 * 6.265647888183594
Epoch 770, val loss: 1.0973632335662842
Epoch 780, training loss: 627.1232299804688 = 0.8076992034912109 + 100.0 * 6.263155460357666
Epoch 780, val loss: 1.0940539836883545
Epoch 790, training loss: 627.1646118164062 = 0.7973946332931519 + 100.0 * 6.263671875
Epoch 790, val loss: 1.0908242464065552
Epoch 800, training loss: 627.2374877929688 = 0.7871954441070557 + 100.0 * 6.264503479003906
Epoch 800, val loss: 1.0878067016601562
Epoch 810, training loss: 627.0186157226562 = 0.7771215438842773 + 100.0 * 6.262415409088135
Epoch 810, val loss: 1.0845203399658203
Epoch 820, training loss: 627.1820068359375 = 0.7671948075294495 + 100.0 * 6.264147758483887
Epoch 820, val loss: 1.0819584131240845
Epoch 830, training loss: 626.8746948242188 = 0.7573896646499634 + 100.0 * 6.261173248291016
Epoch 830, val loss: 1.0793461799621582
Epoch 840, training loss: 627.016357421875 = 0.7477830052375793 + 100.0 * 6.262685298919678
Epoch 840, val loss: 1.076818585395813
Epoch 850, training loss: 626.5971069335938 = 0.7381750345230103 + 100.0 * 6.258589744567871
Epoch 850, val loss: 1.0742655992507935
Epoch 860, training loss: 626.3901977539062 = 0.7289139628410339 + 100.0 * 6.256612300872803
Epoch 860, val loss: 1.0724223852157593
Epoch 870, training loss: 626.3446655273438 = 0.7198159098625183 + 100.0 * 6.256248950958252
Epoch 870, val loss: 1.0705170631408691
Epoch 880, training loss: 626.714599609375 = 0.7107911705970764 + 100.0 * 6.260037899017334
Epoch 880, val loss: 1.0688230991363525
Epoch 890, training loss: 626.65185546875 = 0.7017467617988586 + 100.0 * 6.2595014572143555
Epoch 890, val loss: 1.0666019916534424
Epoch 900, training loss: 626.1052856445312 = 0.6928942203521729 + 100.0 * 6.254124164581299
Epoch 900, val loss: 1.065146565437317
Epoch 910, training loss: 626.095458984375 = 0.6842195987701416 + 100.0 * 6.254112243652344
Epoch 910, val loss: 1.0637803077697754
Epoch 920, training loss: 626.306640625 = 0.675663411617279 + 100.0 * 6.256309986114502
Epoch 920, val loss: 1.0624792575836182
Epoch 930, training loss: 625.9826049804688 = 0.6670251488685608 + 100.0 * 6.25315523147583
Epoch 930, val loss: 1.0612682104110718
Epoch 940, training loss: 625.8222045898438 = 0.6586450338363647 + 100.0 * 6.251635551452637
Epoch 940, val loss: 1.0604937076568604
Epoch 950, training loss: 625.7540893554688 = 0.6504901051521301 + 100.0 * 6.251035690307617
Epoch 950, val loss: 1.0596272945404053
Epoch 960, training loss: 626.3409423828125 = 0.6423969268798828 + 100.0 * 6.256985664367676
Epoch 960, val loss: 1.0592187643051147
Epoch 970, training loss: 626.039306640625 = 0.6343719363212585 + 100.0 * 6.254048824310303
Epoch 970, val loss: 1.0586533546447754
Epoch 980, training loss: 625.6624145507812 = 0.6263400316238403 + 100.0 * 6.250360488891602
Epoch 980, val loss: 1.0579092502593994
Epoch 990, training loss: 625.7013549804688 = 0.6185764074325562 + 100.0 * 6.250827789306641
Epoch 990, val loss: 1.0577902793884277
Epoch 1000, training loss: 625.3839721679688 = 0.6108719706535339 + 100.0 * 6.247730731964111
Epoch 1000, val loss: 1.057632565498352
Epoch 1010, training loss: 625.5020141601562 = 0.6033109426498413 + 100.0 * 6.248986721038818
Epoch 1010, val loss: 1.0576680898666382
Epoch 1020, training loss: 625.5681762695312 = 0.5957907438278198 + 100.0 * 6.249724388122559
Epoch 1020, val loss: 1.0575047731399536
Epoch 1030, training loss: 625.3648681640625 = 0.5883536338806152 + 100.0 * 6.247764587402344
Epoch 1030, val loss: 1.0575332641601562
Epoch 1040, training loss: 625.1959228515625 = 0.5810107588768005 + 100.0 * 6.246149063110352
Epoch 1040, val loss: 1.0582072734832764
Epoch 1050, training loss: 625.18310546875 = 0.5738451480865479 + 100.0 * 6.246092796325684
Epoch 1050, val loss: 1.058467149734497
Epoch 1060, training loss: 625.44482421875 = 0.5666303038597107 + 100.0 * 6.248781681060791
Epoch 1060, val loss: 1.0591310262680054
Epoch 1070, training loss: 624.9086303710938 = 0.5595069527626038 + 100.0 * 6.243491172790527
Epoch 1070, val loss: 1.059700846672058
Epoch 1080, training loss: 624.919921875 = 0.5525477528572083 + 100.0 * 6.243673801422119
Epoch 1080, val loss: 1.0606685876846313
Epoch 1090, training loss: 624.8646850585938 = 0.5456964373588562 + 100.0 * 6.243189811706543
Epoch 1090, val loss: 1.061635136604309
Epoch 1100, training loss: 625.6649169921875 = 0.5390016436576843 + 100.0 * 6.2512593269348145
Epoch 1100, val loss: 1.0628217458724976
Epoch 1110, training loss: 624.8750610351562 = 0.5320664048194885 + 100.0 * 6.243430137634277
Epoch 1110, val loss: 1.0636711120605469
Epoch 1120, training loss: 624.7022705078125 = 0.5253665447235107 + 100.0 * 6.241768836975098
Epoch 1120, val loss: 1.064855694770813
Epoch 1130, training loss: 625.1646728515625 = 0.5188717842102051 + 100.0 * 6.246458053588867
Epoch 1130, val loss: 1.0663988590240479
Epoch 1140, training loss: 624.535400390625 = 0.5122933983802795 + 100.0 * 6.240231513977051
Epoch 1140, val loss: 1.0672918558120728
Epoch 1150, training loss: 624.4624633789062 = 0.5058542490005493 + 100.0 * 6.239566326141357
Epoch 1150, val loss: 1.0690125226974487
Epoch 1160, training loss: 624.9053344726562 = 0.49952852725982666 + 100.0 * 6.244058132171631
Epoch 1160, val loss: 1.0705801248550415
Epoch 1170, training loss: 624.4895629882812 = 0.4931372106075287 + 100.0 * 6.239964008331299
Epoch 1170, val loss: 1.0719026327133179
Epoch 1180, training loss: 624.6314086914062 = 0.4868309497833252 + 100.0 * 6.241446018218994
Epoch 1180, val loss: 1.07370126247406
Epoch 1190, training loss: 624.3191528320312 = 0.4806460440158844 + 100.0 * 6.238385200500488
Epoch 1190, val loss: 1.075619101524353
Epoch 1200, training loss: 624.2901611328125 = 0.47456684708595276 + 100.0 * 6.238155841827393
Epoch 1200, val loss: 1.0776209831237793
Epoch 1210, training loss: 624.5197143554688 = 0.46858909726142883 + 100.0 * 6.240511417388916
Epoch 1210, val loss: 1.0795246362686157
Epoch 1220, training loss: 624.4871826171875 = 0.4625348746776581 + 100.0 * 6.240246295928955
Epoch 1220, val loss: 1.0809043645858765
Epoch 1230, training loss: 624.2938842773438 = 0.4564834535121918 + 100.0 * 6.23837423324585
Epoch 1230, val loss: 1.0834490060806274
Epoch 1240, training loss: 624.20654296875 = 0.4506170451641083 + 100.0 * 6.2375593185424805
Epoch 1240, val loss: 1.0852717161178589
Epoch 1250, training loss: 624.545654296875 = 0.4447440505027771 + 100.0 * 6.241008758544922
Epoch 1250, val loss: 1.0876307487487793
Epoch 1260, training loss: 624.210205078125 = 0.43888336420059204 + 100.0 * 6.237712860107422
Epoch 1260, val loss: 1.0893874168395996
Epoch 1270, training loss: 623.9341430664062 = 0.43310728669166565 + 100.0 * 6.235010147094727
Epoch 1270, val loss: 1.0918126106262207
Epoch 1280, training loss: 623.9337158203125 = 0.42746105790138245 + 100.0 * 6.235062122344971
Epoch 1280, val loss: 1.0943974256515503
Epoch 1290, training loss: 624.3732299804688 = 0.42181894183158875 + 100.0 * 6.239514350891113
Epoch 1290, val loss: 1.096638798713684
Epoch 1300, training loss: 623.8704223632812 = 0.4161567687988281 + 100.0 * 6.2345428466796875
Epoch 1300, val loss: 1.0989445447921753
Epoch 1310, training loss: 623.83642578125 = 0.41056838631629944 + 100.0 * 6.23425817489624
Epoch 1310, val loss: 1.1014044284820557
Epoch 1320, training loss: 624.0513916015625 = 0.40504390001296997 + 100.0 * 6.23646354675293
Epoch 1320, val loss: 1.1037843227386475
Epoch 1330, training loss: 623.859375 = 0.39952552318573 + 100.0 * 6.234598636627197
Epoch 1330, val loss: 1.106501579284668
Epoch 1340, training loss: 623.8057861328125 = 0.3940373361110687 + 100.0 * 6.23411750793457
Epoch 1340, val loss: 1.1089953184127808
Epoch 1350, training loss: 623.7135620117188 = 0.38861891627311707 + 100.0 * 6.233249664306641
Epoch 1350, val loss: 1.1117459535598755
Epoch 1360, training loss: 624.2805786132812 = 0.3832303583621979 + 100.0 * 6.238973617553711
Epoch 1360, val loss: 1.1144225597381592
Epoch 1370, training loss: 623.7072143554688 = 0.37785208225250244 + 100.0 * 6.233293533325195
Epoch 1370, val loss: 1.1168618202209473
Epoch 1380, training loss: 623.5357666015625 = 0.3725210130214691 + 100.0 * 6.231632709503174
Epoch 1380, val loss: 1.1200200319290161
Epoch 1390, training loss: 623.85791015625 = 0.3673134446144104 + 100.0 * 6.23490571975708
Epoch 1390, val loss: 1.122928261756897
Epoch 1400, training loss: 623.4256591796875 = 0.3619917035102844 + 100.0 * 6.2306365966796875
Epoch 1400, val loss: 1.125151515007019
Epoch 1410, training loss: 623.7282104492188 = 0.3567855954170227 + 100.0 * 6.2337141036987305
Epoch 1410, val loss: 1.1283608675003052
Epoch 1420, training loss: 623.3500366210938 = 0.3515579104423523 + 100.0 * 6.229984760284424
Epoch 1420, val loss: 1.1307801008224487
Epoch 1430, training loss: 623.5360107421875 = 0.3464319407939911 + 100.0 * 6.231895923614502
Epoch 1430, val loss: 1.1337957382202148
Epoch 1440, training loss: 623.4374389648438 = 0.34130826592445374 + 100.0 * 6.230961322784424
Epoch 1440, val loss: 1.1364212036132812
Epoch 1450, training loss: 623.3848266601562 = 0.3362189829349518 + 100.0 * 6.230485916137695
Epoch 1450, val loss: 1.1396197080612183
Epoch 1460, training loss: 623.1880493164062 = 0.3312112092971802 + 100.0 * 6.228568077087402
Epoch 1460, val loss: 1.1425180435180664
Epoch 1470, training loss: 623.3505249023438 = 0.3262561559677124 + 100.0 * 6.230242729187012
Epoch 1470, val loss: 1.1457093954086304
Epoch 1480, training loss: 623.4509887695312 = 0.321320116519928 + 100.0 * 6.231296539306641
Epoch 1480, val loss: 1.1488786935806274
Epoch 1490, training loss: 623.35986328125 = 0.3163181245326996 + 100.0 * 6.230435371398926
Epoch 1490, val loss: 1.151624083518982
Epoch 1500, training loss: 623.0665893554688 = 0.3114113509654999 + 100.0 * 6.2275519371032715
Epoch 1500, val loss: 1.1546413898468018
Epoch 1510, training loss: 623.0307006835938 = 0.3066026568412781 + 100.0 * 6.227241039276123
Epoch 1510, val loss: 1.1581029891967773
Epoch 1520, training loss: 623.7701416015625 = 0.3018335700035095 + 100.0 * 6.234683513641357
Epoch 1520, val loss: 1.161078691482544
Epoch 1530, training loss: 623.1906127929688 = 0.29703542590141296 + 100.0 * 6.228935241699219
Epoch 1530, val loss: 1.1644971370697021
Epoch 1540, training loss: 623.0117797851562 = 0.29227644205093384 + 100.0 * 6.227194786071777
Epoch 1540, val loss: 1.1676180362701416
Epoch 1550, training loss: 623.1007080078125 = 0.28764426708221436 + 100.0 * 6.228130340576172
Epoch 1550, val loss: 1.17115318775177
Epoch 1560, training loss: 623.177978515625 = 0.28298407793045044 + 100.0 * 6.228950500488281
Epoch 1560, val loss: 1.1740520000457764
Epoch 1570, training loss: 622.9698486328125 = 0.27838417887687683 + 100.0 * 6.226914405822754
Epoch 1570, val loss: 1.1773924827575684
Epoch 1580, training loss: 623.0087890625 = 0.2738257944583893 + 100.0 * 6.227349758148193
Epoch 1580, val loss: 1.1809706687927246
Epoch 1590, training loss: 622.7200927734375 = 0.26932793855667114 + 100.0 * 6.2245073318481445
Epoch 1590, val loss: 1.1842280626296997
Epoch 1600, training loss: 622.6686401367188 = 0.2649509906768799 + 100.0 * 6.224036693572998
Epoch 1600, val loss: 1.187987208366394
Epoch 1610, training loss: 622.8737182617188 = 0.2606426775455475 + 100.0 * 6.226130962371826
Epoch 1610, val loss: 1.1912922859191895
Epoch 1620, training loss: 622.88671875 = 0.2563053369522095 + 100.0 * 6.226304054260254
Epoch 1620, val loss: 1.194845199584961
Epoch 1630, training loss: 622.8516235351562 = 0.25193464756011963 + 100.0 * 6.225996971130371
Epoch 1630, val loss: 1.1979589462280273
Epoch 1640, training loss: 622.5606689453125 = 0.2476622760295868 + 100.0 * 6.223129749298096
Epoch 1640, val loss: 1.2016040086746216
Epoch 1650, training loss: 622.5360717773438 = 0.24351583421230316 + 100.0 * 6.222925186157227
Epoch 1650, val loss: 1.2051817178726196
Epoch 1660, training loss: 622.4286499023438 = 0.23947405815124512 + 100.0 * 6.221891403198242
Epoch 1660, val loss: 1.2089450359344482
Epoch 1670, training loss: 622.5556030273438 = 0.2354981154203415 + 100.0 * 6.223201274871826
Epoch 1670, val loss: 1.2128713130950928
Epoch 1680, training loss: 622.822265625 = 0.23149819672107697 + 100.0 * 6.225907325744629
Epoch 1680, val loss: 1.216335415840149
Epoch 1690, training loss: 622.6958618164062 = 0.22749444842338562 + 100.0 * 6.22468376159668
Epoch 1690, val loss: 1.2191922664642334
Epoch 1700, training loss: 622.777099609375 = 0.22357910871505737 + 100.0 * 6.2255353927612305
Epoch 1700, val loss: 1.223115086555481
Epoch 1710, training loss: 622.4959106445312 = 0.2197079360485077 + 100.0 * 6.222761631011963
Epoch 1710, val loss: 1.2262905836105347
Epoch 1720, training loss: 622.52392578125 = 0.2159266620874405 + 100.0 * 6.223079681396484
Epoch 1720, val loss: 1.2299046516418457
Epoch 1730, training loss: 622.6937255859375 = 0.2121853083372116 + 100.0 * 6.224815368652344
Epoch 1730, val loss: 1.2338989973068237
Epoch 1740, training loss: 622.37255859375 = 0.20857501029968262 + 100.0 * 6.221640110015869
Epoch 1740, val loss: 1.2372201681137085
Epoch 1750, training loss: 622.4765625 = 0.20499081909656525 + 100.0 * 6.222715377807617
Epoch 1750, val loss: 1.2410837411880493
Epoch 1760, training loss: 622.2376098632812 = 0.2014659345149994 + 100.0 * 6.220361232757568
Epoch 1760, val loss: 1.2446097135543823
Epoch 1770, training loss: 622.258056640625 = 0.1980143040418625 + 100.0 * 6.220600605010986
Epoch 1770, val loss: 1.248505711555481
Epoch 1780, training loss: 622.7058715820312 = 0.19461943209171295 + 100.0 * 6.225112438201904
Epoch 1780, val loss: 1.2522622346878052
Epoch 1790, training loss: 622.3372802734375 = 0.1912141889333725 + 100.0 * 6.221460342407227
Epoch 1790, val loss: 1.2560333013534546
Epoch 1800, training loss: 622.2108154296875 = 0.18786415457725525 + 100.0 * 6.220229625701904
Epoch 1800, val loss: 1.259432315826416
Epoch 1810, training loss: 622.4179077148438 = 0.18463410437107086 + 100.0 * 6.222332954406738
Epoch 1810, val loss: 1.2636168003082275
Epoch 1820, training loss: 622.1074829101562 = 0.18140260875225067 + 100.0 * 6.2192606925964355
Epoch 1820, val loss: 1.2672220468521118
Epoch 1830, training loss: 622.0676879882812 = 0.1782660037279129 + 100.0 * 6.218894004821777
Epoch 1830, val loss: 1.2709698677062988
Epoch 1840, training loss: 622.11376953125 = 0.17521753907203674 + 100.0 * 6.219385147094727
Epoch 1840, val loss: 1.2754013538360596
Epoch 1850, training loss: 622.5084838867188 = 0.17218804359436035 + 100.0 * 6.223362922668457
Epoch 1850, val loss: 1.2790565490722656
Epoch 1860, training loss: 622.150146484375 = 0.1691294014453888 + 100.0 * 6.2198100090026855
Epoch 1860, val loss: 1.2825145721435547
Epoch 1870, training loss: 622.0005493164062 = 0.16620837152004242 + 100.0 * 6.218343734741211
Epoch 1870, val loss: 1.2865567207336426
Epoch 1880, training loss: 622.3098754882812 = 0.16337278485298157 + 100.0 * 6.22146463394165
Epoch 1880, val loss: 1.2906138896942139
Epoch 1890, training loss: 622.2208862304688 = 0.16050204634666443 + 100.0 * 6.2206034660339355
Epoch 1890, val loss: 1.2936418056488037
Epoch 1900, training loss: 621.9102172851562 = 0.157703697681427 + 100.0 * 6.217525005340576
Epoch 1900, val loss: 1.2976590394973755
Epoch 1910, training loss: 621.8192138671875 = 0.1549772173166275 + 100.0 * 6.216642379760742
Epoch 1910, val loss: 1.3014650344848633
Epoch 1920, training loss: 622.545166015625 = 0.152342289686203 + 100.0 * 6.223928451538086
Epoch 1920, val loss: 1.3051092624664307
Epoch 1930, training loss: 622.066162109375 = 0.14966902136802673 + 100.0 * 6.219165325164795
Epoch 1930, val loss: 1.3091648817062378
Epoch 1940, training loss: 621.9259643554688 = 0.14703114330768585 + 100.0 * 6.217789173126221
Epoch 1940, val loss: 1.3124934434890747
Epoch 1950, training loss: 621.8224487304688 = 0.1445314884185791 + 100.0 * 6.2167792320251465
Epoch 1950, val loss: 1.3170143365859985
Epoch 1960, training loss: 621.8107299804688 = 0.14207208156585693 + 100.0 * 6.216686248779297
Epoch 1960, val loss: 1.3206031322479248
Epoch 1970, training loss: 621.9153442382812 = 0.13966791331768036 + 100.0 * 6.217757225036621
Epoch 1970, val loss: 1.3247759342193604
Epoch 1980, training loss: 622.1002197265625 = 0.13727903366088867 + 100.0 * 6.219629287719727
Epoch 1980, val loss: 1.3280305862426758
Epoch 1990, training loss: 621.739990234375 = 0.13487286865711212 + 100.0 * 6.21605110168457
Epoch 1990, val loss: 1.331957459449768
Epoch 2000, training loss: 621.7677001953125 = 0.1325695812702179 + 100.0 * 6.216351509094238
Epoch 2000, val loss: 1.335593819618225
Epoch 2010, training loss: 622.0142211914062 = 0.1303112953901291 + 100.0 * 6.218839168548584
Epoch 2010, val loss: 1.3392095565795898
Epoch 2020, training loss: 621.8743286132812 = 0.1280708760023117 + 100.0 * 6.21746301651001
Epoch 2020, val loss: 1.3432987928390503
Epoch 2030, training loss: 621.6026611328125 = 0.1258784383535385 + 100.0 * 6.214767932891846
Epoch 2030, val loss: 1.3469924926757812
Epoch 2040, training loss: 621.5790405273438 = 0.12380068749189377 + 100.0 * 6.214552879333496
Epoch 2040, val loss: 1.351370930671692
Epoch 2050, training loss: 622.1372680664062 = 0.12173643708229065 + 100.0 * 6.220154762268066
Epoch 2050, val loss: 1.3555562496185303
Epoch 2060, training loss: 621.8045043945312 = 0.1196528971195221 + 100.0 * 6.216848850250244
Epoch 2060, val loss: 1.3584164381027222
Epoch 2070, training loss: 621.5288696289062 = 0.11760035902261734 + 100.0 * 6.214112281799316
Epoch 2070, val loss: 1.36251699924469
Epoch 2080, training loss: 621.793701171875 = 0.11564590036869049 + 100.0 * 6.216780662536621
Epoch 2080, val loss: 1.3661012649536133
Epoch 2090, training loss: 621.4534912109375 = 0.11367737501859665 + 100.0 * 6.213398456573486
Epoch 2090, val loss: 1.3701671361923218
Epoch 2100, training loss: 621.5100708007812 = 0.11179603636264801 + 100.0 * 6.213982582092285
Epoch 2100, val loss: 1.3741159439086914
Epoch 2110, training loss: 621.7861938476562 = 0.10995697975158691 + 100.0 * 6.216762065887451
Epoch 2110, val loss: 1.377774953842163
Epoch 2120, training loss: 621.4230346679688 = 0.10810574889183044 + 100.0 * 6.213149547576904
Epoch 2120, val loss: 1.3816806077957153
Epoch 2130, training loss: 621.7890014648438 = 0.10631916671991348 + 100.0 * 6.216826915740967
Epoch 2130, val loss: 1.3852624893188477
Epoch 2140, training loss: 621.4530029296875 = 0.10452579706907272 + 100.0 * 6.213484764099121
Epoch 2140, val loss: 1.3890914916992188
Epoch 2150, training loss: 621.4765014648438 = 0.10281763225793839 + 100.0 * 6.213736534118652
Epoch 2150, val loss: 1.393115758895874
Epoch 2160, training loss: 621.6996459960938 = 0.10114608705043793 + 100.0 * 6.21598482131958
Epoch 2160, val loss: 1.3968510627746582
Epoch 2170, training loss: 621.4552612304688 = 0.09947723895311356 + 100.0 * 6.213557720184326
Epoch 2170, val loss: 1.4008338451385498
Epoch 2180, training loss: 621.3519897460938 = 0.09782501310110092 + 100.0 * 6.212541580200195
Epoch 2180, val loss: 1.4046423435211182
Epoch 2190, training loss: 621.3848876953125 = 0.0962309017777443 + 100.0 * 6.212886333465576
Epoch 2190, val loss: 1.408396601676941
Epoch 2200, training loss: 621.4465942382812 = 0.09469283372163773 + 100.0 * 6.21351957321167
Epoch 2200, val loss: 1.412396788597107
Epoch 2210, training loss: 621.275390625 = 0.0931461825966835 + 100.0 * 6.211822509765625
Epoch 2210, val loss: 1.4160871505737305
Epoch 2220, training loss: 621.1541137695312 = 0.09165889024734497 + 100.0 * 6.2106242179870605
Epoch 2220, val loss: 1.4203505516052246
Epoch 2230, training loss: 621.6359252929688 = 0.09021975845098495 + 100.0 * 6.215457439422607
Epoch 2230, val loss: 1.4242761135101318
Epoch 2240, training loss: 621.6105346679688 = 0.08874434232711792 + 100.0 * 6.2152180671691895
Epoch 2240, val loss: 1.4269976615905762
Epoch 2250, training loss: 621.2487182617188 = 0.08727066218852997 + 100.0 * 6.21161413192749
Epoch 2250, val loss: 1.4309515953063965
Epoch 2260, training loss: 621.29443359375 = 0.0858747586607933 + 100.0 * 6.212085723876953
Epoch 2260, val loss: 1.434675931930542
Epoch 2270, training loss: 621.4395141601562 = 0.0845269188284874 + 100.0 * 6.213549613952637
Epoch 2270, val loss: 1.4389410018920898
Epoch 2280, training loss: 621.303466796875 = 0.08317152410745621 + 100.0 * 6.212202548980713
Epoch 2280, val loss: 1.4421888589859009
Epoch 2290, training loss: 621.0928955078125 = 0.08184625953435898 + 100.0 * 6.210110187530518
Epoch 2290, val loss: 1.4461060762405396
Epoch 2300, training loss: 621.1416625976562 = 0.08058924973011017 + 100.0 * 6.210610866546631
Epoch 2300, val loss: 1.4497991800308228
Epoch 2310, training loss: 621.1455078125 = 0.07934344559907913 + 100.0 * 6.210661888122559
Epoch 2310, val loss: 1.4537354707717896
Epoch 2320, training loss: 621.605712890625 = 0.07808422297239304 + 100.0 * 6.21527624130249
Epoch 2320, val loss: 1.45745849609375
Epoch 2330, training loss: 621.09814453125 = 0.07682842761278152 + 100.0 * 6.2102131843566895
Epoch 2330, val loss: 1.4605690240859985
Epoch 2340, training loss: 620.9463500976562 = 0.07562959939241409 + 100.0 * 6.208707332611084
Epoch 2340, val loss: 1.4645427465438843
Epoch 2350, training loss: 621.0291748046875 = 0.074489526450634 + 100.0 * 6.20954704284668
Epoch 2350, val loss: 1.468477487564087
Epoch 2360, training loss: 621.3104858398438 = 0.07336300611495972 + 100.0 * 6.212371349334717
Epoch 2360, val loss: 1.4722038507461548
Epoch 2370, training loss: 621.5508422851562 = 0.07222425192594528 + 100.0 * 6.214786529541016
Epoch 2370, val loss: 1.475180983543396
Epoch 2380, training loss: 621.1921997070312 = 0.07108248770236969 + 100.0 * 6.21121072769165
Epoch 2380, val loss: 1.4782147407531738
Epoch 2390, training loss: 621.1539306640625 = 0.06998573988676071 + 100.0 * 6.21083927154541
Epoch 2390, val loss: 1.4824286699295044
Epoch 2400, training loss: 621.1383666992188 = 0.06892512738704681 + 100.0 * 6.210694313049316
Epoch 2400, val loss: 1.4857772588729858
Epoch 2410, training loss: 620.9822998046875 = 0.06788907200098038 + 100.0 * 6.209144592285156
Epoch 2410, val loss: 1.4897089004516602
Epoch 2420, training loss: 621.0220336914062 = 0.0668870136141777 + 100.0 * 6.2095513343811035
Epoch 2420, val loss: 1.493828296661377
Epoch 2430, training loss: 621.1311645507812 = 0.0658845454454422 + 100.0 * 6.210653305053711
Epoch 2430, val loss: 1.4968341588974
Epoch 2440, training loss: 620.7936401367188 = 0.06488445401191711 + 100.0 * 6.207287788391113
Epoch 2440, val loss: 1.5002329349517822
Epoch 2450, training loss: 621.0579833984375 = 0.06394410878419876 + 100.0 * 6.209940433502197
Epoch 2450, val loss: 1.5040395259857178
Epoch 2460, training loss: 621.1659545898438 = 0.06298326700925827 + 100.0 * 6.211029529571533
Epoch 2460, val loss: 1.5075170993804932
Epoch 2470, training loss: 620.9034423828125 = 0.06202774494886398 + 100.0 * 6.208414554595947
Epoch 2470, val loss: 1.5108308792114258
Epoch 2480, training loss: 620.8477172851562 = 0.06110900267958641 + 100.0 * 6.207866191864014
Epoch 2480, val loss: 1.5145334005355835
Epoch 2490, training loss: 621.0032348632812 = 0.06022787094116211 + 100.0 * 6.209429740905762
Epoch 2490, val loss: 1.5181975364685059
Epoch 2500, training loss: 621.0836791992188 = 0.05933469533920288 + 100.0 * 6.2102437019348145
Epoch 2500, val loss: 1.5213955640792847
Epoch 2510, training loss: 620.833740234375 = 0.05843360349535942 + 100.0 * 6.2077531814575195
Epoch 2510, val loss: 1.5242196321487427
Epoch 2520, training loss: 620.7114868164062 = 0.05758338421583176 + 100.0 * 6.206539154052734
Epoch 2520, val loss: 1.5288090705871582
Epoch 2530, training loss: 620.89013671875 = 0.056780025362968445 + 100.0 * 6.208333492279053
Epoch 2530, val loss: 1.5325604677200317
Epoch 2540, training loss: 620.9962158203125 = 0.0559515543282032 + 100.0 * 6.209402561187744
Epoch 2540, val loss: 1.5356614589691162
Epoch 2550, training loss: 620.9201049804688 = 0.05513779819011688 + 100.0 * 6.208649635314941
Epoch 2550, val loss: 1.5386676788330078
Epoch 2560, training loss: 620.9948120117188 = 0.05436033383011818 + 100.0 * 6.209403991699219
Epoch 2560, val loss: 1.5433099269866943
Epoch 2570, training loss: 620.7532348632812 = 0.05356108024716377 + 100.0 * 6.206996440887451
Epoch 2570, val loss: 1.5462640523910522
Epoch 2580, training loss: 620.6905517578125 = 0.052810996770858765 + 100.0 * 6.2063775062561035
Epoch 2580, val loss: 1.549357533454895
Epoch 2590, training loss: 620.6546020507812 = 0.05207910016179085 + 100.0 * 6.20602560043335
Epoch 2590, val loss: 1.5533455610275269
Epoch 2600, training loss: 621.0922241210938 = 0.05136163905262947 + 100.0 * 6.2104082107543945
Epoch 2600, val loss: 1.5564714670181274
Epoch 2610, training loss: 620.723388671875 = 0.050627775490283966 + 100.0 * 6.206727981567383
Epoch 2610, val loss: 1.5596736669540405
Epoch 2620, training loss: 620.8758544921875 = 0.04991934448480606 + 100.0 * 6.208259582519531
Epoch 2620, val loss: 1.5635653734207153
Epoch 2630, training loss: 620.6458740234375 = 0.049229711294174194 + 100.0 * 6.205966472625732
Epoch 2630, val loss: 1.566409945487976
Epoch 2640, training loss: 620.6355590820312 = 0.04854961112141609 + 100.0 * 6.205870628356934
Epoch 2640, val loss: 1.57011878490448
Epoch 2650, training loss: 621.0709228515625 = 0.04788827896118164 + 100.0 * 6.210229873657227
Epoch 2650, val loss: 1.5734665393829346
Epoch 2660, training loss: 620.4796142578125 = 0.04721248894929886 + 100.0 * 6.204323768615723
Epoch 2660, val loss: 1.5769119262695312
Epoch 2670, training loss: 620.5963745117188 = 0.046578265726566315 + 100.0 * 6.205497741699219
Epoch 2670, val loss: 1.5800840854644775
Epoch 2680, training loss: 620.9385375976562 = 0.04595446214079857 + 100.0 * 6.208925724029541
Epoch 2680, val loss: 1.5836889743804932
Epoch 2690, training loss: 620.5858764648438 = 0.04532425478100777 + 100.0 * 6.205405235290527
Epoch 2690, val loss: 1.586935043334961
Epoch 2700, training loss: 620.622802734375 = 0.04471667855978012 + 100.0 * 6.205780506134033
Epoch 2700, val loss: 1.5905734300613403
Epoch 2710, training loss: 620.4871215820312 = 0.044124506413936615 + 100.0 * 6.204429626464844
Epoch 2710, val loss: 1.5939240455627441
Epoch 2720, training loss: 620.5523071289062 = 0.0435585156083107 + 100.0 * 6.205087184906006
Epoch 2720, val loss: 1.5974080562591553
Epoch 2730, training loss: 620.9724731445312 = 0.04298956319689751 + 100.0 * 6.209295272827148
Epoch 2730, val loss: 1.6008349657058716
Epoch 2740, training loss: 620.8321533203125 = 0.04240139201283455 + 100.0 * 6.207897186279297
Epoch 2740, val loss: 1.6029504537582397
Epoch 2750, training loss: 620.7606201171875 = 0.04183143004775047 + 100.0 * 6.207188129425049
Epoch 2750, val loss: 1.6066970825195312
Epoch 2760, training loss: 620.4498901367188 = 0.04127931967377663 + 100.0 * 6.2040863037109375
Epoch 2760, val loss: 1.6095994710922241
Epoch 2770, training loss: 620.3418579101562 = 0.040748968720436096 + 100.0 * 6.2030110359191895
Epoch 2770, val loss: 1.6136507987976074
Epoch 2780, training loss: 620.7654418945312 = 0.04024314507842064 + 100.0 * 6.207252502441406
Epoch 2780, val loss: 1.6163709163665771
Epoch 2790, training loss: 620.4744262695312 = 0.039713021367788315 + 100.0 * 6.204346656799316
Epoch 2790, val loss: 1.6193342208862305
Epoch 2800, training loss: 620.417724609375 = 0.03919338062405586 + 100.0 * 6.203785419464111
Epoch 2800, val loss: 1.62251877784729
Epoch 2810, training loss: 620.5484619140625 = 0.03869645297527313 + 100.0 * 6.205097675323486
Epoch 2810, val loss: 1.6263916492462158
Epoch 2820, training loss: 620.395751953125 = 0.03820303827524185 + 100.0 * 6.203575134277344
Epoch 2820, val loss: 1.629166841506958
Epoch 2830, training loss: 620.4786987304688 = 0.03772466629743576 + 100.0 * 6.204410076141357
Epoch 2830, val loss: 1.6324684619903564
Epoch 2840, training loss: 620.6937866210938 = 0.0372457355260849 + 100.0 * 6.2065653800964355
Epoch 2840, val loss: 1.635890245437622
Epoch 2850, training loss: 620.3894653320312 = 0.03674175590276718 + 100.0 * 6.203527450561523
Epoch 2850, val loss: 1.6383631229400635
Epoch 2860, training loss: 620.26171875 = 0.03629235923290253 + 100.0 * 6.202253818511963
Epoch 2860, val loss: 1.6415435075759888
Epoch 2870, training loss: 620.3269653320312 = 0.03585683926939964 + 100.0 * 6.202911376953125
Epoch 2870, val loss: 1.64559006690979
Epoch 2880, training loss: 620.757568359375 = 0.0354299433529377 + 100.0 * 6.207221508026123
Epoch 2880, val loss: 1.6482878923416138
Epoch 2890, training loss: 620.39501953125 = 0.03497422859072685 + 100.0 * 6.2036004066467285
Epoch 2890, val loss: 1.6515792608261108
Epoch 2900, training loss: 620.3153686523438 = 0.0345425084233284 + 100.0 * 6.202808380126953
Epoch 2900, val loss: 1.6549060344696045
Epoch 2910, training loss: 620.4996337890625 = 0.034135475754737854 + 100.0 * 6.204655170440674
Epoch 2910, val loss: 1.6577564477920532
Epoch 2920, training loss: 620.3527221679688 = 0.033701810985803604 + 100.0 * 6.203190326690674
Epoch 2920, val loss: 1.660627841949463
Epoch 2930, training loss: 620.2417602539062 = 0.03328840434551239 + 100.0 * 6.202085018157959
Epoch 2930, val loss: 1.6640677452087402
Epoch 2940, training loss: 620.3029174804688 = 0.03290724754333496 + 100.0 * 6.202700138092041
Epoch 2940, val loss: 1.667474627494812
Epoch 2950, training loss: 620.5690307617188 = 0.03251452371478081 + 100.0 * 6.205365180969238
Epoch 2950, val loss: 1.6706349849700928
Epoch 2960, training loss: 620.2830200195312 = 0.032114699482917786 + 100.0 * 6.202508926391602
Epoch 2960, val loss: 1.6728676557540894
Epoch 2970, training loss: 620.097900390625 = 0.031735751777887344 + 100.0 * 6.200661659240723
Epoch 2970, val loss: 1.6764662265777588
Epoch 2980, training loss: 620.2925415039062 = 0.03137783706188202 + 100.0 * 6.202611446380615
Epoch 2980, val loss: 1.6799664497375488
Epoch 2990, training loss: 620.5718383789062 = 0.03100595809519291 + 100.0 * 6.205408096313477
Epoch 2990, val loss: 1.6824336051940918
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.8181338956246705
The final CL Acc:0.67037, 0.01889, The final GNN Acc:0.80812, 0.00724
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13146])
remove edge: torch.Size([2, 7914])
updated graph: torch.Size([2, 10504])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6290893554688 = 1.9453754425048828 + 100.0 * 8.596837043762207
Epoch 0, val loss: 1.9515478610992432
Epoch 10, training loss: 861.547119140625 = 1.9366134405136108 + 100.0 * 8.596105575561523
Epoch 10, val loss: 1.9419951438903809
Epoch 20, training loss: 861.0466918945312 = 1.925827980041504 + 100.0 * 8.591208457946777
Epoch 20, val loss: 1.930038332939148
Epoch 30, training loss: 857.4765014648438 = 1.911795973777771 + 100.0 * 8.555646896362305
Epoch 30, val loss: 1.9143991470336914
Epoch 40, training loss: 834.1105346679688 = 1.8945107460021973 + 100.0 * 8.322159767150879
Epoch 40, val loss: 1.8960026502609253
Epoch 50, training loss: 779.105712890625 = 1.8739430904388428 + 100.0 * 7.772317409515381
Epoch 50, val loss: 1.8747678995132446
Epoch 60, training loss: 755.7598876953125 = 1.8589200973510742 + 100.0 * 7.5390095710754395
Epoch 60, val loss: 1.8603771924972534
Epoch 70, training loss: 726.4320068359375 = 1.8514798879623413 + 100.0 * 7.245805263519287
Epoch 70, val loss: 1.8529667854309082
Epoch 80, training loss: 702.89404296875 = 1.8446651697158813 + 100.0 * 7.010493755340576
Epoch 80, val loss: 1.8458034992218018
Epoch 90, training loss: 690.6873779296875 = 1.8357112407684326 + 100.0 * 6.888516902923584
Epoch 90, val loss: 1.837404727935791
Epoch 100, training loss: 680.531005859375 = 1.8273875713348389 + 100.0 * 6.787036418914795
Epoch 100, val loss: 1.8297499418258667
Epoch 110, training loss: 673.2295532226562 = 1.8206344842910767 + 100.0 * 6.714089393615723
Epoch 110, val loss: 1.8229643106460571
Epoch 120, training loss: 668.015380859375 = 1.8139536380767822 + 100.0 * 6.662014007568359
Epoch 120, val loss: 1.8160237073898315
Epoch 130, training loss: 663.7855834960938 = 1.8070725202560425 + 100.0 * 6.619785308837891
Epoch 130, val loss: 1.8091208934783936
Epoch 140, training loss: 660.3161010742188 = 1.8002694845199585 + 100.0 * 6.585158348083496
Epoch 140, val loss: 1.8023875951766968
Epoch 150, training loss: 657.701416015625 = 1.793568730354309 + 100.0 * 6.559078216552734
Epoch 150, val loss: 1.7957215309143066
Epoch 160, training loss: 655.1272583007812 = 1.786530613899231 + 100.0 * 6.533407211303711
Epoch 160, val loss: 1.7889097929000854
Epoch 170, training loss: 653.0177612304688 = 1.7793114185333252 + 100.0 * 6.51238489151001
Epoch 170, val loss: 1.78203284740448
Epoch 180, training loss: 651.2307739257812 = 1.7717676162719727 + 100.0 * 6.494589805603027
Epoch 180, val loss: 1.7749511003494263
Epoch 190, training loss: 649.618896484375 = 1.7636072635650635 + 100.0 * 6.47855281829834
Epoch 190, val loss: 1.767470359802246
Epoch 200, training loss: 648.1640014648438 = 1.7549172639846802 + 100.0 * 6.464090824127197
Epoch 200, val loss: 1.7595287561416626
Epoch 210, training loss: 647.0198364257812 = 1.7454115152359009 + 100.0 * 6.452744007110596
Epoch 210, val loss: 1.751038670539856
Epoch 220, training loss: 645.8145141601562 = 1.7351152896881104 + 100.0 * 6.440793991088867
Epoch 220, val loss: 1.7417937517166138
Epoch 230, training loss: 644.8046264648438 = 1.7239961624145508 + 100.0 * 6.4308061599731445
Epoch 230, val loss: 1.7319461107254028
Epoch 240, training loss: 644.3030395507812 = 1.7119985818862915 + 100.0 * 6.425910949707031
Epoch 240, val loss: 1.7213704586029053
Epoch 250, training loss: 643.1447143554688 = 1.6989097595214844 + 100.0 * 6.414458274841309
Epoch 250, val loss: 1.7099274396896362
Epoch 260, training loss: 642.2730102539062 = 1.6848335266113281 + 100.0 * 6.405881881713867
Epoch 260, val loss: 1.6977559328079224
Epoch 270, training loss: 641.4990234375 = 1.669785976409912 + 100.0 * 6.398292541503906
Epoch 270, val loss: 1.6846519708633423
Epoch 280, training loss: 641.1504516601562 = 1.6536271572113037 + 100.0 * 6.394968032836914
Epoch 280, val loss: 1.670710563659668
Epoch 290, training loss: 640.2153930664062 = 1.6363763809204102 + 100.0 * 6.3857903480529785
Epoch 290, val loss: 1.6558302640914917
Epoch 300, training loss: 639.5631713867188 = 1.6180559396743774 + 100.0 * 6.379451274871826
Epoch 300, val loss: 1.6401852369308472
Epoch 310, training loss: 638.9711303710938 = 1.5987575054168701 + 100.0 * 6.37372350692749
Epoch 310, val loss: 1.6237117052078247
Epoch 320, training loss: 638.8300170898438 = 1.5784200429916382 + 100.0 * 6.372515678405762
Epoch 320, val loss: 1.6062347888946533
Epoch 330, training loss: 638.052490234375 = 1.5568783283233643 + 100.0 * 6.364955902099609
Epoch 330, val loss: 1.5879356861114502
Epoch 340, training loss: 637.5491943359375 = 1.5345888137817383 + 100.0 * 6.3601460456848145
Epoch 340, val loss: 1.568975806236267
Epoch 350, training loss: 637.6954956054688 = 1.5113661289215088 + 100.0 * 6.361841678619385
Epoch 350, val loss: 1.5492973327636719
Epoch 360, training loss: 636.7345581054688 = 1.487397313117981 + 100.0 * 6.352471351623535
Epoch 360, val loss: 1.5289390087127686
Epoch 370, training loss: 636.1829833984375 = 1.4628238677978516 + 100.0 * 6.347201824188232
Epoch 370, val loss: 1.5082069635391235
Epoch 380, training loss: 635.8289184570312 = 1.4377264976501465 + 100.0 * 6.343911647796631
Epoch 380, val loss: 1.4871480464935303
Epoch 390, training loss: 635.5352783203125 = 1.4120056629180908 + 100.0 * 6.341232776641846
Epoch 390, val loss: 1.4654481410980225
Epoch 400, training loss: 635.0623779296875 = 1.385758876800537 + 100.0 * 6.336766242980957
Epoch 400, val loss: 1.443485140800476
Epoch 410, training loss: 634.728759765625 = 1.3593106269836426 + 100.0 * 6.3336944580078125
Epoch 410, val loss: 1.4213591814041138
Epoch 420, training loss: 634.5746459960938 = 1.3327337503433228 + 100.0 * 6.332418918609619
Epoch 420, val loss: 1.399294376373291
Epoch 430, training loss: 634.1961669921875 = 1.3059964179992676 + 100.0 * 6.328901767730713
Epoch 430, val loss: 1.3768996000289917
Epoch 440, training loss: 634.3228149414062 = 1.279039740562439 + 100.0 * 6.330437660217285
Epoch 440, val loss: 1.3546886444091797
Epoch 450, training loss: 633.6077880859375 = 1.252066731452942 + 100.0 * 6.323557376861572
Epoch 450, val loss: 1.332404375076294
Epoch 460, training loss: 633.2517700195312 = 1.2253797054290771 + 100.0 * 6.320263862609863
Epoch 460, val loss: 1.310423493385315
Epoch 470, training loss: 633.5921020507812 = 1.1989332437515259 + 100.0 * 6.323931694030762
Epoch 470, val loss: 1.2887450456619263
Epoch 480, training loss: 632.7625122070312 = 1.172581672668457 + 100.0 * 6.315898895263672
Epoch 480, val loss: 1.267362117767334
Epoch 490, training loss: 632.4349975585938 = 1.1467763185501099 + 100.0 * 6.312881946563721
Epoch 490, val loss: 1.246535301208496
Epoch 500, training loss: 632.1798095703125 = 1.1215288639068604 + 100.0 * 6.310582637786865
Epoch 500, val loss: 1.2264482975006104
Epoch 510, training loss: 632.6137084960938 = 1.096849799156189 + 100.0 * 6.315168380737305
Epoch 510, val loss: 1.206961989402771
Epoch 520, training loss: 631.7533569335938 = 1.0723708868026733 + 100.0 * 6.306809902191162
Epoch 520, val loss: 1.1879907846450806
Epoch 530, training loss: 631.5468139648438 = 1.0487768650054932 + 100.0 * 6.304980278015137
Epoch 530, val loss: 1.1698883771896362
Epoch 540, training loss: 631.7745361328125 = 1.0258148908615112 + 100.0 * 6.3074870109558105
Epoch 540, val loss: 1.1527243852615356
Epoch 550, training loss: 631.2906494140625 = 1.003524661064148 + 100.0 * 6.302871227264404
Epoch 550, val loss: 1.1358367204666138
Epoch 560, training loss: 631.0416870117188 = 0.9818646907806396 + 100.0 * 6.30059814453125
Epoch 560, val loss: 1.1197936534881592
Epoch 570, training loss: 630.7520141601562 = 0.9609123468399048 + 100.0 * 6.297910690307617
Epoch 570, val loss: 1.1048409938812256
Epoch 580, training loss: 631.3097534179688 = 0.9407171607017517 + 100.0 * 6.303689956665039
Epoch 580, val loss: 1.0905547142028809
Epoch 590, training loss: 630.3182373046875 = 0.920884370803833 + 100.0 * 6.293973445892334
Epoch 590, val loss: 1.0771123170852661
Epoch 600, training loss: 630.14404296875 = 0.9019017219543457 + 100.0 * 6.292421340942383
Epoch 600, val loss: 1.0644277334213257
Epoch 610, training loss: 629.9511108398438 = 0.8836228847503662 + 100.0 * 6.290675163269043
Epoch 610, val loss: 1.052566647529602
Epoch 620, training loss: 630.1849975585938 = 0.8657532334327698 + 100.0 * 6.293192386627197
Epoch 620, val loss: 1.0412508249282837
Epoch 630, training loss: 630.0444946289062 = 0.8484514951705933 + 100.0 * 6.2919602394104
Epoch 630, val loss: 1.0306018590927124
Epoch 640, training loss: 629.4329833984375 = 0.8316869735717773 + 100.0 * 6.286013126373291
Epoch 640, val loss: 1.0207123756408691
Epoch 650, training loss: 629.2673950195312 = 0.8155686259269714 + 100.0 * 6.284518718719482
Epoch 650, val loss: 1.0115681886672974
Epoch 660, training loss: 629.1671752929688 = 0.8000104427337646 + 100.0 * 6.283671855926514
Epoch 660, val loss: 1.0031311511993408
Epoch 670, training loss: 629.1838989257812 = 0.7848101854324341 + 100.0 * 6.28399133682251
Epoch 670, val loss: 0.9950584173202515
Epoch 680, training loss: 628.9276733398438 = 0.7699288725852966 + 100.0 * 6.281577110290527
Epoch 680, val loss: 0.9872503280639648
Epoch 690, training loss: 628.7750244140625 = 0.755571186542511 + 100.0 * 6.2801947593688965
Epoch 690, val loss: 0.9802407622337341
Epoch 700, training loss: 628.5050659179688 = 0.7417092323303223 + 100.0 * 6.2776336669921875
Epoch 700, val loss: 0.9738761782646179
Epoch 710, training loss: 628.800048828125 = 0.7283307313919067 + 100.0 * 6.280717372894287
Epoch 710, val loss: 0.9680482149124146
Epoch 720, training loss: 628.5617065429688 = 0.7149071097373962 + 100.0 * 6.278468132019043
Epoch 720, val loss: 0.9618589282035828
Epoch 730, training loss: 628.2943725585938 = 0.7019691467285156 + 100.0 * 6.275924205780029
Epoch 730, val loss: 0.9565967917442322
Epoch 740, training loss: 628.0924072265625 = 0.6894189119338989 + 100.0 * 6.2740302085876465
Epoch 740, val loss: 0.9517326354980469
Epoch 750, training loss: 627.9153442382812 = 0.6771800518035889 + 100.0 * 6.27238130569458
Epoch 750, val loss: 0.9473418593406677
Epoch 760, training loss: 628.1472778320312 = 0.6651313900947571 + 100.0 * 6.2748212814331055
Epoch 760, val loss: 0.9430974125862122
Epoch 770, training loss: 627.6383056640625 = 0.6532441973686218 + 100.0 * 6.269850730895996
Epoch 770, val loss: 0.9391419887542725
Epoch 780, training loss: 627.5223999023438 = 0.6417089700698853 + 100.0 * 6.2688069343566895
Epoch 780, val loss: 0.9355714321136475
Epoch 790, training loss: 627.904541015625 = 0.630457878112793 + 100.0 * 6.272740840911865
Epoch 790, val loss: 0.9325774908065796
Epoch 800, training loss: 627.482421875 = 0.6191015839576721 + 100.0 * 6.2686333656311035
Epoch 800, val loss: 0.9291825294494629
Epoch 810, training loss: 627.4793090820312 = 0.6081752777099609 + 100.0 * 6.268711566925049
Epoch 810, val loss: 0.9264755249023438
Epoch 820, training loss: 627.1095581054688 = 0.5973084568977356 + 100.0 * 6.265122890472412
Epoch 820, val loss: 0.9239075779914856
Epoch 830, training loss: 627.0507202148438 = 0.5867313742637634 + 100.0 * 6.264639854431152
Epoch 830, val loss: 0.9215375185012817
Epoch 840, training loss: 627.710205078125 = 0.5763320922851562 + 100.0 * 6.27133846282959
Epoch 840, val loss: 0.9192267060279846
Epoch 850, training loss: 626.8989868164062 = 0.5658058524131775 + 100.0 * 6.263331890106201
Epoch 850, val loss: 0.9172133207321167
Epoch 860, training loss: 626.7243041992188 = 0.5556899309158325 + 100.0 * 6.261686325073242
Epoch 860, val loss: 0.9154868721961975
Epoch 870, training loss: 626.5421752929688 = 0.545768678188324 + 100.0 * 6.2599639892578125
Epoch 870, val loss: 0.9138974547386169
Epoch 880, training loss: 627.067138671875 = 0.5360019207000732 + 100.0 * 6.265311241149902
Epoch 880, val loss: 0.9126865863800049
Epoch 890, training loss: 626.5647583007812 = 0.5261919498443604 + 100.0 * 6.260385990142822
Epoch 890, val loss: 0.9108678102493286
Epoch 900, training loss: 626.3538818359375 = 0.5165975093841553 + 100.0 * 6.258372783660889
Epoch 900, val loss: 0.9099403619766235
Epoch 910, training loss: 626.2421875 = 0.5072212815284729 + 100.0 * 6.257349967956543
Epoch 910, val loss: 0.9087112545967102
Epoch 920, training loss: 626.7745361328125 = 0.49791091680526733 + 100.0 * 6.262766361236572
Epoch 920, val loss: 0.9079453349113464
Epoch 930, training loss: 626.1940307617188 = 0.48858553171157837 + 100.0 * 6.257054328918457
Epoch 930, val loss: 0.9066734910011292
Epoch 940, training loss: 625.8881225585938 = 0.4795597195625305 + 100.0 * 6.254085540771484
Epoch 940, val loss: 0.9060823321342468
Epoch 950, training loss: 625.8956909179688 = 0.47077110409736633 + 100.0 * 6.254249572753906
Epoch 950, val loss: 0.9056165814399719
Epoch 960, training loss: 626.5137939453125 = 0.4620673954486847 + 100.0 * 6.260517120361328
Epoch 960, val loss: 0.9049212336540222
Epoch 970, training loss: 625.8071899414062 = 0.4533795118331909 + 100.0 * 6.253538131713867
Epoch 970, val loss: 0.9044848680496216
Epoch 980, training loss: 625.560791015625 = 0.44491976499557495 + 100.0 * 6.251158237457275
Epoch 980, val loss: 0.9041275978088379
Epoch 990, training loss: 625.6632080078125 = 0.43665358424186707 + 100.0 * 6.252265930175781
Epoch 990, val loss: 0.9039785265922546
Epoch 1000, training loss: 625.6163940429688 = 0.4284908175468445 + 100.0 * 6.2518792152404785
Epoch 1000, val loss: 0.903849720954895
Epoch 1010, training loss: 625.439208984375 = 0.4204561710357666 + 100.0 * 6.250187397003174
Epoch 1010, val loss: 0.9037767648696899
Epoch 1020, training loss: 625.8901977539062 = 0.41256794333457947 + 100.0 * 6.2547760009765625
Epoch 1020, val loss: 0.903972864151001
Epoch 1030, training loss: 625.4762573242188 = 0.4047456979751587 + 100.0 * 6.250715255737305
Epoch 1030, val loss: 0.9037249684333801
Epoch 1040, training loss: 625.3342895507812 = 0.39705711603164673 + 100.0 * 6.249372482299805
Epoch 1040, val loss: 0.9040599465370178
Epoch 1050, training loss: 625.1110229492188 = 0.389591246843338 + 100.0 * 6.247214317321777
Epoch 1050, val loss: 0.9045072793960571
Epoch 1060, training loss: 625.144775390625 = 0.382235586643219 + 100.0 * 6.247625350952148
Epoch 1060, val loss: 0.9048741459846497
Epoch 1070, training loss: 625.154296875 = 0.3749864101409912 + 100.0 * 6.247792720794678
Epoch 1070, val loss: 0.9055757522583008
Epoch 1080, training loss: 625.4544067382812 = 0.36783239245414734 + 100.0 * 6.250865459442139
Epoch 1080, val loss: 0.9062225818634033
Epoch 1090, training loss: 624.9395141601562 = 0.3606429100036621 + 100.0 * 6.24578857421875
Epoch 1090, val loss: 0.9065178036689758
Epoch 1100, training loss: 624.77587890625 = 0.3537522554397583 + 100.0 * 6.244221210479736
Epoch 1100, val loss: 0.907352864742279
Epoch 1110, training loss: 624.6808471679688 = 0.3469763696193695 + 100.0 * 6.243338584899902
Epoch 1110, val loss: 0.908304750919342
Epoch 1120, training loss: 624.7736206054688 = 0.3403719663619995 + 100.0 * 6.244332313537598
Epoch 1120, val loss: 0.9091614484786987
Epoch 1130, training loss: 624.8128662109375 = 0.33374834060668945 + 100.0 * 6.244791030883789
Epoch 1130, val loss: 0.9098512530326843
Epoch 1140, training loss: 624.5921020507812 = 0.32723021507263184 + 100.0 * 6.242649078369141
Epoch 1140, val loss: 0.9110776782035828
Epoch 1150, training loss: 624.4926147460938 = 0.3208860754966736 + 100.0 * 6.241717338562012
Epoch 1150, val loss: 0.9121407866477966
Epoch 1160, training loss: 624.92724609375 = 0.3147117495536804 + 100.0 * 6.246125221252441
Epoch 1160, val loss: 0.9135400652885437
Epoch 1170, training loss: 624.4149780273438 = 0.3084680438041687 + 100.0 * 6.24106502532959
Epoch 1170, val loss: 0.914734423160553
Epoch 1180, training loss: 624.4788818359375 = 0.30242881178855896 + 100.0 * 6.241764545440674
Epoch 1180, val loss: 0.9159753322601318
Epoch 1190, training loss: 624.5858154296875 = 0.29645809531211853 + 100.0 * 6.242893695831299
Epoch 1190, val loss: 0.9173980951309204
Epoch 1200, training loss: 624.2495727539062 = 0.29056233167648315 + 100.0 * 6.239589691162109
Epoch 1200, val loss: 0.9184767007827759
Epoch 1210, training loss: 624.1416625976562 = 0.2848615050315857 + 100.0 * 6.238568305969238
Epoch 1210, val loss: 0.9199033379554749
Epoch 1220, training loss: 624.5592651367188 = 0.2792631685733795 + 100.0 * 6.242800235748291
Epoch 1220, val loss: 0.9214089512825012
Epoch 1230, training loss: 624.2109985351562 = 0.2736814618110657 + 100.0 * 6.239373207092285
Epoch 1230, val loss: 0.9229175448417664
Epoch 1240, training loss: 624.3818969726562 = 0.26824751496315 + 100.0 * 6.24113655090332
Epoch 1240, val loss: 0.9242308735847473
Epoch 1250, training loss: 624.17529296875 = 0.26285961270332336 + 100.0 * 6.239124298095703
Epoch 1250, val loss: 0.9255653619766235
Epoch 1260, training loss: 623.9580688476562 = 0.25759464502334595 + 100.0 * 6.237005233764648
Epoch 1260, val loss: 0.9274165034294128
Epoch 1270, training loss: 623.9113159179688 = 0.25245556235313416 + 100.0 * 6.236588478088379
Epoch 1270, val loss: 0.9290034770965576
Epoch 1280, training loss: 623.9480590820312 = 0.24742549657821655 + 100.0 * 6.237006187438965
Epoch 1280, val loss: 0.9307423233985901
Epoch 1290, training loss: 623.9137573242188 = 0.2424934059381485 + 100.0 * 6.236712455749512
Epoch 1290, val loss: 0.9325218796730042
Epoch 1300, training loss: 623.6519775390625 = 0.23757559061050415 + 100.0 * 6.23414421081543
Epoch 1300, val loss: 0.9341451525688171
Epoch 1310, training loss: 623.6150512695312 = 0.2328367829322815 + 100.0 * 6.233821868896484
Epoch 1310, val loss: 0.9362434148788452
Epoch 1320, training loss: 624.1066284179688 = 0.22822396457195282 + 100.0 * 6.238784313201904
Epoch 1320, val loss: 0.9378856420516968
Epoch 1330, training loss: 623.7050170898438 = 0.22356697916984558 + 100.0 * 6.234814167022705
Epoch 1330, val loss: 0.9400448203086853
Epoch 1340, training loss: 623.5652465820312 = 0.21904462575912476 + 100.0 * 6.233461856842041
Epoch 1340, val loss: 0.9418566823005676
Epoch 1350, training loss: 623.4378662109375 = 0.21469372510910034 + 100.0 * 6.232231140136719
Epoch 1350, val loss: 0.944317638874054
Epoch 1360, training loss: 623.7075805664062 = 0.21042773127555847 + 100.0 * 6.234971523284912
Epoch 1360, val loss: 0.9463632702827454
Epoch 1370, training loss: 623.3815307617188 = 0.20619681477546692 + 100.0 * 6.231753826141357
Epoch 1370, val loss: 0.9484233856201172
Epoch 1380, training loss: 623.3319702148438 = 0.20203781127929688 + 100.0 * 6.23129940032959
Epoch 1380, val loss: 0.9505992531776428
Epoch 1390, training loss: 623.2265625 = 0.19800856709480286 + 100.0 * 6.23028564453125
Epoch 1390, val loss: 0.9527214765548706
Epoch 1400, training loss: 623.4743041992188 = 0.1940985471010208 + 100.0 * 6.232802391052246
Epoch 1400, val loss: 0.9551132917404175
Epoch 1410, training loss: 623.2548217773438 = 0.19018897414207458 + 100.0 * 6.230646133422852
Epoch 1410, val loss: 0.9574467539787292
Epoch 1420, training loss: 623.2689208984375 = 0.18640144169330597 + 100.0 * 6.230824947357178
Epoch 1420, val loss: 0.959960401058197
Epoch 1430, training loss: 623.3137817382812 = 0.18267132341861725 + 100.0 * 6.231310844421387
Epoch 1430, val loss: 0.9622344970703125
Epoch 1440, training loss: 623.3048095703125 = 0.17905350029468536 + 100.0 * 6.231257438659668
Epoch 1440, val loss: 0.9647123217582703
Epoch 1450, training loss: 623.0943603515625 = 0.17549443244934082 + 100.0 * 6.229188919067383
Epoch 1450, val loss: 0.9671194553375244
Epoch 1460, training loss: 623.158203125 = 0.1720433533191681 + 100.0 * 6.229861259460449
Epoch 1460, val loss: 0.9699684381484985
Epoch 1470, training loss: 623.1944580078125 = 0.16862332820892334 + 100.0 * 6.230257987976074
Epoch 1470, val loss: 0.9724171757698059
Epoch 1480, training loss: 623.0452880859375 = 0.16529260575771332 + 100.0 * 6.228799819946289
Epoch 1480, val loss: 0.9753741025924683
Epoch 1490, training loss: 622.8735961914062 = 0.1620381772518158 + 100.0 * 6.227115631103516
Epoch 1490, val loss: 0.9778863787651062
Epoch 1500, training loss: 623.4557495117188 = 0.15890538692474365 + 100.0 * 6.232968330383301
Epoch 1500, val loss: 0.9809355735778809
Epoch 1510, training loss: 623.430419921875 = 0.15569986402988434 + 100.0 * 6.2327470779418945
Epoch 1510, val loss: 0.9832530617713928
Epoch 1520, training loss: 622.87158203125 = 0.1525985449552536 + 100.0 * 6.227190017700195
Epoch 1520, val loss: 0.9863524436950684
Epoch 1530, training loss: 622.6757202148438 = 0.1496100127696991 + 100.0 * 6.225261211395264
Epoch 1530, val loss: 0.9894726872444153
Epoch 1540, training loss: 622.6565551757812 = 0.1467365026473999 + 100.0 * 6.225098133087158
Epoch 1540, val loss: 0.9925768971443176
Epoch 1550, training loss: 623.7998657226562 = 0.14392295479774475 + 100.0 * 6.2365593910217285
Epoch 1550, val loss: 0.9958188533782959
Epoch 1560, training loss: 623.0018920898438 = 0.14100836217403412 + 100.0 * 6.228609085083008
Epoch 1560, val loss: 0.9980611205101013
Epoch 1570, training loss: 622.6767578125 = 0.13823945820331573 + 100.0 * 6.2253851890563965
Epoch 1570, val loss: 1.0015442371368408
Epoch 1580, training loss: 622.598876953125 = 0.13557672500610352 + 100.0 * 6.22463321685791
Epoch 1580, val loss: 1.0045137405395508
Epoch 1590, training loss: 622.9261474609375 = 0.13298377394676208 + 100.0 * 6.227931499481201
Epoch 1590, val loss: 1.0078824758529663
Epoch 1600, training loss: 622.479736328125 = 0.13035988807678223 + 100.0 * 6.223493576049805
Epoch 1600, val loss: 1.011122703552246
Epoch 1610, training loss: 622.5038452148438 = 0.12785190343856812 + 100.0 * 6.223759651184082
Epoch 1610, val loss: 1.0144970417022705
Epoch 1620, training loss: 622.8088989257812 = 0.12540094554424286 + 100.0 * 6.226834774017334
Epoch 1620, val loss: 1.0174176692962646
Epoch 1630, training loss: 622.638671875 = 0.1229480430483818 + 100.0 * 6.225157260894775
Epoch 1630, val loss: 1.0210671424865723
Epoch 1640, training loss: 622.3568725585938 = 0.12056878954172134 + 100.0 * 6.222363471984863
Epoch 1640, val loss: 1.024118185043335
Epoch 1650, training loss: 622.73388671875 = 0.11829137802124023 + 100.0 * 6.226156234741211
Epoch 1650, val loss: 1.0278854370117188
Epoch 1660, training loss: 622.538330078125 = 0.11597838997840881 + 100.0 * 6.224223613739014
Epoch 1660, val loss: 1.030938982963562
Epoch 1670, training loss: 622.291259765625 = 0.1137232556939125 + 100.0 * 6.221775531768799
Epoch 1670, val loss: 1.0345700979232788
Epoch 1680, training loss: 622.2564697265625 = 0.11156892776489258 + 100.0 * 6.22144889831543
Epoch 1680, val loss: 1.0379170179367065
Epoch 1690, training loss: 622.5470581054688 = 0.1094757691025734 + 100.0 * 6.2243757247924805
Epoch 1690, val loss: 1.0416688919067383
Epoch 1700, training loss: 622.3065795898438 = 0.10736333578824997 + 100.0 * 6.221992015838623
Epoch 1700, val loss: 1.044970154762268
Epoch 1710, training loss: 622.1803588867188 = 0.10531727224588394 + 100.0 * 6.220749855041504
Epoch 1710, val loss: 1.0483680963516235
Epoch 1720, training loss: 622.0938720703125 = 0.10332219302654266 + 100.0 * 6.219905376434326
Epoch 1720, val loss: 1.0519795417785645
Epoch 1730, training loss: 622.3516845703125 = 0.10140173882246017 + 100.0 * 6.222503185272217
Epoch 1730, val loss: 1.0556992292404175
Epoch 1740, training loss: 622.18505859375 = 0.09945856034755707 + 100.0 * 6.220855712890625
Epoch 1740, val loss: 1.0590814352035522
Epoch 1750, training loss: 622.00537109375 = 0.09755869954824448 + 100.0 * 6.219078540802002
Epoch 1750, val loss: 1.0625921487808228
Epoch 1760, training loss: 622.03662109375 = 0.09573300927877426 + 100.0 * 6.219408988952637
Epoch 1760, val loss: 1.066294550895691
Epoch 1770, training loss: 622.486083984375 = 0.09393689781427383 + 100.0 * 6.223921775817871
Epoch 1770, val loss: 1.0696920156478882
Epoch 1780, training loss: 622.3482666015625 = 0.09214609861373901 + 100.0 * 6.222560882568359
Epoch 1780, val loss: 1.073203444480896
Epoch 1790, training loss: 622.0411376953125 = 0.09039128571748734 + 100.0 * 6.219507217407227
Epoch 1790, val loss: 1.0768405199050903
Epoch 1800, training loss: 621.8684692382812 = 0.08871323615312576 + 100.0 * 6.217797756195068
Epoch 1800, val loss: 1.0805765390396118
Epoch 1810, training loss: 621.8174438476562 = 0.08709210157394409 + 100.0 * 6.217303276062012
Epoch 1810, val loss: 1.0844767093658447
Epoch 1820, training loss: 622.4888916015625 = 0.08551932871341705 + 100.0 * 6.224033832550049
Epoch 1820, val loss: 1.0882201194763184
Epoch 1830, training loss: 622.0440063476562 = 0.083882637321949 + 100.0 * 6.219601154327393
Epoch 1830, val loss: 1.0913774967193604
Epoch 1840, training loss: 621.8759765625 = 0.0823151245713234 + 100.0 * 6.2179365158081055
Epoch 1840, val loss: 1.0950896739959717
Epoch 1850, training loss: 622.0873413085938 = 0.08081018924713135 + 100.0 * 6.220065593719482
Epoch 1850, val loss: 1.0987974405288696
Epoch 1860, training loss: 621.8139038085938 = 0.07930893450975418 + 100.0 * 6.21734619140625
Epoch 1860, val loss: 1.1023310422897339
Epoch 1870, training loss: 621.8198852539062 = 0.07785365730524063 + 100.0 * 6.2174201011657715
Epoch 1870, val loss: 1.106083631515503
Epoch 1880, training loss: 621.783447265625 = 0.07642923295497894 + 100.0 * 6.21707010269165
Epoch 1880, val loss: 1.1097441911697388
Epoch 1890, training loss: 621.8341064453125 = 0.07504479587078094 + 100.0 * 6.21759033203125
Epoch 1890, val loss: 1.1133731603622437
Epoch 1900, training loss: 622.114990234375 = 0.07368531078100204 + 100.0 * 6.2204132080078125
Epoch 1900, val loss: 1.1169369220733643
Epoch 1910, training loss: 621.6376342773438 = 0.07231897115707397 + 100.0 * 6.215652942657471
Epoch 1910, val loss: 1.1208590269088745
Epoch 1920, training loss: 621.5386962890625 = 0.07102174311876297 + 100.0 * 6.214676380157471
Epoch 1920, val loss: 1.1245763301849365
Epoch 1930, training loss: 621.5443115234375 = 0.06976202875375748 + 100.0 * 6.21474552154541
Epoch 1930, val loss: 1.1282182931900024
Epoch 1940, training loss: 622.4611206054688 = 0.06853242218494415 + 100.0 * 6.223926067352295
Epoch 1940, val loss: 1.1319169998168945
Epoch 1950, training loss: 621.8533935546875 = 0.06726757436990738 + 100.0 * 6.217861175537109
Epoch 1950, val loss: 1.135271668434143
Epoch 1960, training loss: 621.632080078125 = 0.06605808436870575 + 100.0 * 6.215660572052002
Epoch 1960, val loss: 1.138936161994934
Epoch 1970, training loss: 621.4349975585938 = 0.06489162147045135 + 100.0 * 6.213701248168945
Epoch 1970, val loss: 1.1428800821304321
Epoch 1980, training loss: 621.4541015625 = 0.06376613676548004 + 100.0 * 6.213902950286865
Epoch 1980, val loss: 1.1465336084365845
Epoch 1990, training loss: 622.1013793945312 = 0.0626644492149353 + 100.0 * 6.220386981964111
Epoch 1990, val loss: 1.1503132581710815
Epoch 2000, training loss: 622.2816162109375 = 0.06153794750571251 + 100.0 * 6.222200870513916
Epoch 2000, val loss: 1.1538355350494385
Epoch 2010, training loss: 621.5224609375 = 0.060396332293748856 + 100.0 * 6.214620113372803
Epoch 2010, val loss: 1.1570690870285034
Epoch 2020, training loss: 621.270751953125 = 0.05934315174818039 + 100.0 * 6.212114334106445
Epoch 2020, val loss: 1.1609065532684326
Epoch 2030, training loss: 621.2315063476562 = 0.058334704488515854 + 100.0 * 6.211731433868408
Epoch 2030, val loss: 1.1647472381591797
Epoch 2040, training loss: 621.2361450195312 = 0.05735419690608978 + 100.0 * 6.211787700653076
Epoch 2040, val loss: 1.1685295104980469
Epoch 2050, training loss: 621.9850463867188 = 0.05639835447072983 + 100.0 * 6.2192864418029785
Epoch 2050, val loss: 1.1720902919769287
Epoch 2060, training loss: 621.4871215820312 = 0.055386193096637726 + 100.0 * 6.214317798614502
Epoch 2060, val loss: 1.1752192974090576
Epoch 2070, training loss: 621.4005126953125 = 0.054433953016996384 + 100.0 * 6.213460922241211
Epoch 2070, val loss: 1.1789156198501587
Epoch 2080, training loss: 621.629638671875 = 0.053504716604948044 + 100.0 * 6.215761184692383
Epoch 2080, val loss: 1.1823474168777466
Epoch 2090, training loss: 621.2965087890625 = 0.052593111991882324 + 100.0 * 6.212439060211182
Epoch 2090, val loss: 1.1862812042236328
Epoch 2100, training loss: 621.2337036132812 = 0.051706522703170776 + 100.0 * 6.211820125579834
Epoch 2100, val loss: 1.1899992227554321
Epoch 2110, training loss: 621.2698364257812 = 0.050846073776483536 + 100.0 * 6.212189674377441
Epoch 2110, val loss: 1.1935230493545532
Epoch 2120, training loss: 621.6012573242188 = 0.04999810457229614 + 100.0 * 6.215512752532959
Epoch 2120, val loss: 1.19696044921875
Epoch 2130, training loss: 621.3142700195312 = 0.04913187399506569 + 100.0 * 6.212651252746582
Epoch 2130, val loss: 1.2005503177642822
Epoch 2140, training loss: 621.1768188476562 = 0.048308223485946655 + 100.0 * 6.211285591125488
Epoch 2140, val loss: 1.2039599418640137
Epoch 2150, training loss: 621.365478515625 = 0.047517482191324234 + 100.0 * 6.213179111480713
Epoch 2150, val loss: 1.2076644897460938
Epoch 2160, training loss: 621.1029052734375 = 0.04672963172197342 + 100.0 * 6.210561752319336
Epoch 2160, val loss: 1.2110040187835693
Epoch 2170, training loss: 621.0362548828125 = 0.04596390202641487 + 100.0 * 6.209903240203857
Epoch 2170, val loss: 1.2146615982055664
Epoch 2180, training loss: 620.9425048828125 = 0.045224275439977646 + 100.0 * 6.208972930908203
Epoch 2180, val loss: 1.2181949615478516
Epoch 2190, training loss: 621.1557006835938 = 0.04450803995132446 + 100.0 * 6.211112022399902
Epoch 2190, val loss: 1.2217743396759033
Epoch 2200, training loss: 621.2996826171875 = 0.04377966746687889 + 100.0 * 6.212559223175049
Epoch 2200, val loss: 1.2249774932861328
Epoch 2210, training loss: 621.0473022460938 = 0.043041691184043884 + 100.0 * 6.210042476654053
Epoch 2210, val loss: 1.2285292148590088
Epoch 2220, training loss: 620.8999633789062 = 0.04233425483107567 + 100.0 * 6.208576202392578
Epoch 2220, val loss: 1.2319824695587158
Epoch 2230, training loss: 621.158203125 = 0.04167192056775093 + 100.0 * 6.211165428161621
Epoch 2230, val loss: 1.235761284828186
Epoch 2240, training loss: 620.9598999023438 = 0.04099823161959648 + 100.0 * 6.209188938140869
Epoch 2240, val loss: 1.2388774156570435
Epoch 2250, training loss: 620.9085693359375 = 0.04035080224275589 + 100.0 * 6.208682537078857
Epoch 2250, val loss: 1.2423757314682007
Epoch 2260, training loss: 620.8885498046875 = 0.039723943918943405 + 100.0 * 6.2084879875183105
Epoch 2260, val loss: 1.2457525730133057
Epoch 2270, training loss: 621.1494750976562 = 0.03910389170050621 + 100.0 * 6.211103439331055
Epoch 2270, val loss: 1.2490116357803345
Epoch 2280, training loss: 621.142333984375 = 0.038479410111904144 + 100.0 * 6.211039066314697
Epoch 2280, val loss: 1.2524687051773071
Epoch 2290, training loss: 621.057861328125 = 0.03786691650748253 + 100.0 * 6.210200309753418
Epoch 2290, val loss: 1.2558256387710571
Epoch 2300, training loss: 620.875244140625 = 0.03726828098297119 + 100.0 * 6.208379745483398
Epoch 2300, val loss: 1.2588471174240112
Epoch 2310, training loss: 620.7405395507812 = 0.03669515252113342 + 100.0 * 6.207038402557373
Epoch 2310, val loss: 1.2626147270202637
Epoch 2320, training loss: 620.77197265625 = 0.03614532947540283 + 100.0 * 6.207358360290527
Epoch 2320, val loss: 1.2659751176834106
Epoch 2330, training loss: 620.9140625 = 0.03560270741581917 + 100.0 * 6.208784580230713
Epoch 2330, val loss: 1.2693960666656494
Epoch 2340, training loss: 620.7425537109375 = 0.03505135700106621 + 100.0 * 6.207075119018555
Epoch 2340, val loss: 1.2725037336349487
Epoch 2350, training loss: 621.2252197265625 = 0.03453083336353302 + 100.0 * 6.211906909942627
Epoch 2350, val loss: 1.2755712270736694
Epoch 2360, training loss: 620.9802856445312 = 0.03397785872220993 + 100.0 * 6.209463119506836
Epoch 2360, val loss: 1.2786591053009033
Epoch 2370, training loss: 620.6923828125 = 0.03345298022031784 + 100.0 * 6.206589221954346
Epoch 2370, val loss: 1.2818440198898315
Epoch 2380, training loss: 620.597900390625 = 0.03295818716287613 + 100.0 * 6.205649375915527
Epoch 2380, val loss: 1.285400152206421
Epoch 2390, training loss: 620.522705078125 = 0.03248745575547218 + 100.0 * 6.204902172088623
Epoch 2390, val loss: 1.2887924909591675
Epoch 2400, training loss: 620.7202758789062 = 0.03202731907367706 + 100.0 * 6.206882476806641
Epoch 2400, val loss: 1.2921916246414185
Epoch 2410, training loss: 620.77734375 = 0.031546395272016525 + 100.0 * 6.207458019256592
Epoch 2410, val loss: 1.2948596477508545
Epoch 2420, training loss: 620.7119750976562 = 0.031065089628100395 + 100.0 * 6.206809043884277
Epoch 2420, val loss: 1.2978137731552124
Epoch 2430, training loss: 620.62451171875 = 0.03060736507177353 + 100.0 * 6.205939292907715
Epoch 2430, val loss: 1.3012261390686035
Epoch 2440, training loss: 620.6182250976562 = 0.030167045071721077 + 100.0 * 6.205880641937256
Epoch 2440, val loss: 1.3045988082885742
Epoch 2450, training loss: 620.4979858398438 = 0.02973967231810093 + 100.0 * 6.20468282699585
Epoch 2450, val loss: 1.307584524154663
Epoch 2460, training loss: 621.2879028320312 = 0.02933318354189396 + 100.0 * 6.21258544921875
Epoch 2460, val loss: 1.3105087280273438
Epoch 2470, training loss: 620.5882568359375 = 0.028883961960673332 + 100.0 * 6.205593585968018
Epoch 2470, val loss: 1.3137089014053345
Epoch 2480, training loss: 620.4691772460938 = 0.028474286198616028 + 100.0 * 6.20440673828125
Epoch 2480, val loss: 1.3164262771606445
Epoch 2490, training loss: 620.4712524414062 = 0.028085187077522278 + 100.0 * 6.204431533813477
Epoch 2490, val loss: 1.3198540210723877
Epoch 2500, training loss: 621.014404296875 = 0.02770340070128441 + 100.0 * 6.209867000579834
Epoch 2500, val loss: 1.3227221965789795
Epoch 2510, training loss: 620.584228515625 = 0.02730318158864975 + 100.0 * 6.205569267272949
Epoch 2510, val loss: 1.3256604671478271
Epoch 2520, training loss: 620.5291137695312 = 0.026919148862361908 + 100.0 * 6.205021858215332
Epoch 2520, val loss: 1.3288195133209229
Epoch 2530, training loss: 620.5083618164062 = 0.026550326496362686 + 100.0 * 6.204818248748779
Epoch 2530, val loss: 1.3318537473678589
Epoch 2540, training loss: 620.6392211914062 = 0.02618340775370598 + 100.0 * 6.206130504608154
Epoch 2540, val loss: 1.3346912860870361
Epoch 2550, training loss: 620.4356689453125 = 0.025824816897511482 + 100.0 * 6.204098701477051
Epoch 2550, val loss: 1.33767831325531
Epoch 2560, training loss: 620.2828979492188 = 0.02547623962163925 + 100.0 * 6.202574729919434
Epoch 2560, val loss: 1.3404420614242554
Epoch 2570, training loss: 620.2535400390625 = 0.025145024061203003 + 100.0 * 6.20228385925293
Epoch 2570, val loss: 1.3436317443847656
Epoch 2580, training loss: 620.984130859375 = 0.024832170456647873 + 100.0 * 6.209592819213867
Epoch 2580, val loss: 1.346342921257019
Epoch 2590, training loss: 620.746337890625 = 0.02447456493973732 + 100.0 * 6.207218647003174
Epoch 2590, val loss: 1.3494123220443726
Epoch 2600, training loss: 620.5093994140625 = 0.024137509986758232 + 100.0 * 6.204853057861328
Epoch 2600, val loss: 1.3519352674484253
Epoch 2610, training loss: 620.2113647460938 = 0.02381397970020771 + 100.0 * 6.201875686645508
Epoch 2610, val loss: 1.355107307434082
Epoch 2620, training loss: 620.3428955078125 = 0.023513134568929672 + 100.0 * 6.2031941413879395
Epoch 2620, val loss: 1.3579294681549072
Epoch 2630, training loss: 620.547607421875 = 0.02321118302643299 + 100.0 * 6.205244064331055
Epoch 2630, val loss: 1.3609414100646973
Epoch 2640, training loss: 620.37451171875 = 0.022900251671671867 + 100.0 * 6.203516006469727
Epoch 2640, val loss: 1.3636589050292969
Epoch 2650, training loss: 620.286865234375 = 0.02260737493634224 + 100.0 * 6.202642917633057
Epoch 2650, val loss: 1.3664096593856812
Epoch 2660, training loss: 620.6826171875 = 0.022328710183501244 + 100.0 * 6.206602573394775
Epoch 2660, val loss: 1.3695827722549438
Epoch 2670, training loss: 620.1484375 = 0.022022884339094162 + 100.0 * 6.201263904571533
Epoch 2670, val loss: 1.3718205690383911
Epoch 2680, training loss: 620.1220092773438 = 0.021749865263700485 + 100.0 * 6.201003074645996
Epoch 2680, val loss: 1.3748273849487305
Epoch 2690, training loss: 620.2626953125 = 0.021489165723323822 + 100.0 * 6.202412128448486
Epoch 2690, val loss: 1.3776978254318237
Epoch 2700, training loss: 620.6497802734375 = 0.021217361092567444 + 100.0 * 6.20628547668457
Epoch 2700, val loss: 1.3801170587539673
Epoch 2710, training loss: 620.1572875976562 = 0.02092767134308815 + 100.0 * 6.201363563537598
Epoch 2710, val loss: 1.3827192783355713
Epoch 2720, training loss: 620.0836181640625 = 0.020668933168053627 + 100.0 * 6.200629234313965
Epoch 2720, val loss: 1.3856186866760254
Epoch 2730, training loss: 620.0321655273438 = 0.020426303148269653 + 100.0 * 6.200117111206055
Epoch 2730, val loss: 1.388321876525879
Epoch 2740, training loss: 620.4879760742188 = 0.02019399031996727 + 100.0 * 6.204677581787109
Epoch 2740, val loss: 1.3908072710037231
Epoch 2750, training loss: 620.16943359375 = 0.019935183227062225 + 100.0 * 6.2014946937561035
Epoch 2750, val loss: 1.3934835195541382
Epoch 2760, training loss: 620.0071411132812 = 0.019681695848703384 + 100.0 * 6.1998748779296875
Epoch 2760, val loss: 1.3957756757736206
Epoch 2770, training loss: 620.0717163085938 = 0.01944855786859989 + 100.0 * 6.200522422790527
Epoch 2770, val loss: 1.398855447769165
Epoch 2780, training loss: 620.5263061523438 = 0.019224584102630615 + 100.0 * 6.205070972442627
Epoch 2780, val loss: 1.4011070728302002
Epoch 2790, training loss: 620.2086791992188 = 0.018988462164998055 + 100.0 * 6.201897144317627
Epoch 2790, val loss: 1.403977394104004
Epoch 2800, training loss: 620.0797729492188 = 0.018763519823551178 + 100.0 * 6.200610637664795
Epoch 2800, val loss: 1.4063220024108887
Epoch 2810, training loss: 620.0159912109375 = 0.018547195941209793 + 100.0 * 6.199974536895752
Epoch 2810, val loss: 1.4092137813568115
Epoch 2820, training loss: 620.20556640625 = 0.018337460234761238 + 100.0 * 6.201872825622559
Epoch 2820, val loss: 1.4116246700286865
Epoch 2830, training loss: 619.9909057617188 = 0.018117211759090424 + 100.0 * 6.199728012084961
Epoch 2830, val loss: 1.4141514301300049
Epoch 2840, training loss: 620.1490478515625 = 0.017913272604346275 + 100.0 * 6.2013115882873535
Epoch 2840, val loss: 1.4165371656417847
Epoch 2850, training loss: 620.2675170898438 = 0.017705608159303665 + 100.0 * 6.202497959136963
Epoch 2850, val loss: 1.418874979019165
Epoch 2860, training loss: 620.1173095703125 = 0.017494523897767067 + 100.0 * 6.200997829437256
Epoch 2860, val loss: 1.4215866327285767
Epoch 2870, training loss: 619.8905029296875 = 0.0172891765832901 + 100.0 * 6.198732376098633
Epoch 2870, val loss: 1.4238966703414917
Epoch 2880, training loss: 619.9087524414062 = 0.017098212614655495 + 100.0 * 6.198916912078857
Epoch 2880, val loss: 1.4262542724609375
Epoch 2890, training loss: 620.3831787109375 = 0.016915634274482727 + 100.0 * 6.203662872314453
Epoch 2890, val loss: 1.428858757019043
Epoch 2900, training loss: 619.940185546875 = 0.016711585223674774 + 100.0 * 6.199234485626221
Epoch 2900, val loss: 1.4311596155166626
Epoch 2910, training loss: 619.7977294921875 = 0.016520248726010323 + 100.0 * 6.197812080383301
Epoch 2910, val loss: 1.4335508346557617
Epoch 2920, training loss: 619.8740234375 = 0.016342906281352043 + 100.0 * 6.198576927185059
Epoch 2920, val loss: 1.4359990358352661
Epoch 2930, training loss: 620.3257446289062 = 0.016170183196663857 + 100.0 * 6.20309591293335
Epoch 2930, val loss: 1.43831467628479
Epoch 2940, training loss: 620.4325561523438 = 0.01598574034869671 + 100.0 * 6.204165458679199
Epoch 2940, val loss: 1.4405007362365723
Epoch 2950, training loss: 619.907470703125 = 0.015794070437550545 + 100.0 * 6.198916912078857
Epoch 2950, val loss: 1.4425441026687622
Epoch 2960, training loss: 619.7905883789062 = 0.01562020555138588 + 100.0 * 6.197749614715576
Epoch 2960, val loss: 1.445170283317566
Epoch 2970, training loss: 619.7396240234375 = 0.015455745160579681 + 100.0 * 6.19724178314209
Epoch 2970, val loss: 1.447609543800354
Epoch 2980, training loss: 619.7675170898438 = 0.015298194251954556 + 100.0 * 6.197521686553955
Epoch 2980, val loss: 1.45000422000885
Epoch 2990, training loss: 620.530029296875 = 0.015141954645514488 + 100.0 * 6.205148696899414
Epoch 2990, val loss: 1.4520772695541382
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 861.6338500976562 = 1.9506217241287231 + 100.0 * 8.596832275390625
Epoch 0, val loss: 1.9527711868286133
Epoch 10, training loss: 861.5488891601562 = 1.941766381263733 + 100.0 * 8.596071243286133
Epoch 10, val loss: 1.9434897899627686
Epoch 20, training loss: 861.05517578125 = 1.930806040763855 + 100.0 * 8.591243743896484
Epoch 20, val loss: 1.931855320930481
Epoch 30, training loss: 858.0240478515625 = 1.9163236618041992 + 100.0 * 8.561077117919922
Epoch 30, val loss: 1.9163881540298462
Epoch 40, training loss: 840.97216796875 = 1.897456169128418 + 100.0 * 8.3907470703125
Epoch 40, val loss: 1.896737813949585
Epoch 50, training loss: 772.988525390625 = 1.8750516176223755 + 100.0 * 7.711134910583496
Epoch 50, val loss: 1.873909831047058
Epoch 60, training loss: 740.4122924804688 = 1.8580256700515747 + 100.0 * 7.385542392730713
Epoch 60, val loss: 1.8579950332641602
Epoch 70, training loss: 714.7496337890625 = 1.8457673788070679 + 100.0 * 7.1290388107299805
Epoch 70, val loss: 1.8460050821304321
Epoch 80, training loss: 698.7731323242188 = 1.834232211112976 + 100.0 * 6.969388961791992
Epoch 80, val loss: 1.8350714445114136
Epoch 90, training loss: 687.608154296875 = 1.8244482278823853 + 100.0 * 6.857837200164795
Epoch 90, val loss: 1.8253860473632812
Epoch 100, training loss: 678.3048706054688 = 1.8158915042877197 + 100.0 * 6.764889240264893
Epoch 100, val loss: 1.8169305324554443
Epoch 110, training loss: 671.7487182617188 = 1.8080735206604004 + 100.0 * 6.699406623840332
Epoch 110, val loss: 1.809146761894226
Epoch 120, training loss: 666.9923095703125 = 1.8005329370498657 + 100.0 * 6.651917934417725
Epoch 120, val loss: 1.8014975786209106
Epoch 130, training loss: 663.661865234375 = 1.7930861711502075 + 100.0 * 6.618687629699707
Epoch 130, val loss: 1.7939122915267944
Epoch 140, training loss: 661.007568359375 = 1.7855252027511597 + 100.0 * 6.592220306396484
Epoch 140, val loss: 1.786272644996643
Epoch 150, training loss: 658.8097534179688 = 1.777666687965393 + 100.0 * 6.570321083068848
Epoch 150, val loss: 1.7785080671310425
Epoch 160, training loss: 656.5256958007812 = 1.7696248292922974 + 100.0 * 6.547560214996338
Epoch 160, val loss: 1.7705483436584473
Epoch 170, training loss: 654.59375 = 1.761385202407837 + 100.0 * 6.528323650360107
Epoch 170, val loss: 1.7623987197875977
Epoch 180, training loss: 653.2047119140625 = 1.752855658531189 + 100.0 * 6.5145182609558105
Epoch 180, val loss: 1.7539373636245728
Epoch 190, training loss: 651.2408447265625 = 1.7437137365341187 + 100.0 * 6.49497127532959
Epoch 190, val loss: 1.745125651359558
Epoch 200, training loss: 649.614990234375 = 1.7341094017028809 + 100.0 * 6.478808403015137
Epoch 200, val loss: 1.7359639406204224
Epoch 210, training loss: 648.4873657226562 = 1.7238948345184326 + 100.0 * 6.467634677886963
Epoch 210, val loss: 1.7262284755706787
Epoch 220, training loss: 647.058349609375 = 1.7127856016159058 + 100.0 * 6.453455924987793
Epoch 220, val loss: 1.7157597541809082
Epoch 230, training loss: 645.9365844726562 = 1.7008326053619385 + 100.0 * 6.442358016967773
Epoch 230, val loss: 1.7045592069625854
Epoch 240, training loss: 645.076416015625 = 1.6879148483276367 + 100.0 * 6.433885097503662
Epoch 240, val loss: 1.6924713850021362
Epoch 250, training loss: 644.2183837890625 = 1.6739897727966309 + 100.0 * 6.425443649291992
Epoch 250, val loss: 1.6795132160186768
Epoch 260, training loss: 643.3265991210938 = 1.6591639518737793 + 100.0 * 6.4166741371154785
Epoch 260, val loss: 1.6657912731170654
Epoch 270, training loss: 642.548095703125 = 1.6434770822525024 + 100.0 * 6.409046173095703
Epoch 270, val loss: 1.6512978076934814
Epoch 280, training loss: 642.6488037109375 = 1.6269534826278687 + 100.0 * 6.410218238830566
Epoch 280, val loss: 1.6359765529632568
Epoch 290, training loss: 641.4405517578125 = 1.6091722249984741 + 100.0 * 6.398313522338867
Epoch 290, val loss: 1.6196221113204956
Epoch 300, training loss: 640.7329711914062 = 1.5907139778137207 + 100.0 * 6.391422748565674
Epoch 300, val loss: 1.6027655601501465
Epoch 310, training loss: 640.0765991210938 = 1.5715826749801636 + 100.0 * 6.385050296783447
Epoch 310, val loss: 1.585231900215149
Epoch 320, training loss: 639.5255737304688 = 1.5517501831054688 + 100.0 * 6.3797383308410645
Epoch 320, val loss: 1.5671589374542236
Epoch 330, training loss: 639.0526123046875 = 1.5313270092010498 + 100.0 * 6.375213146209717
Epoch 330, val loss: 1.5486090183258057
Epoch 340, training loss: 639.1934204101562 = 1.5103718042373657 + 100.0 * 6.376830577850342
Epoch 340, val loss: 1.5295454263687134
Epoch 350, training loss: 638.10791015625 = 1.4887166023254395 + 100.0 * 6.366191864013672
Epoch 350, val loss: 1.5100371837615967
Epoch 360, training loss: 637.7014770507812 = 1.4669382572174072 + 100.0 * 6.3623456954956055
Epoch 360, val loss: 1.490563154220581
Epoch 370, training loss: 637.2222900390625 = 1.4450722932815552 + 100.0 * 6.357772350311279
Epoch 370, val loss: 1.4710776805877686
Epoch 380, training loss: 637.1637573242188 = 1.4230321645736694 + 100.0 * 6.357407093048096
Epoch 380, val loss: 1.4515031576156616
Epoch 390, training loss: 636.4915771484375 = 1.4007703065872192 + 100.0 * 6.350908279418945
Epoch 390, val loss: 1.431937336921692
Epoch 400, training loss: 636.0298461914062 = 1.3785752058029175 + 100.0 * 6.346512317657471
Epoch 400, val loss: 1.412558913230896
Epoch 410, training loss: 635.7029418945312 = 1.356385350227356 + 100.0 * 6.343465805053711
Epoch 410, val loss: 1.3931918144226074
Epoch 420, training loss: 635.29345703125 = 1.334245204925537 + 100.0 * 6.339591979980469
Epoch 420, val loss: 1.3741116523742676
Epoch 430, training loss: 634.9013061523438 = 1.3121670484542847 + 100.0 * 6.335891246795654
Epoch 430, val loss: 1.3552360534667969
Epoch 440, training loss: 635.5362548828125 = 1.2902326583862305 + 100.0 * 6.3424601554870605
Epoch 440, val loss: 1.3366020917892456
Epoch 450, training loss: 634.3612670898438 = 1.2681889533996582 + 100.0 * 6.330930709838867
Epoch 450, val loss: 1.3179959058761597
Epoch 460, training loss: 633.9160766601562 = 1.2464213371276855 + 100.0 * 6.326696872711182
Epoch 460, val loss: 1.299869418144226
Epoch 470, training loss: 633.5987548828125 = 1.224960446357727 + 100.0 * 6.323738098144531
Epoch 470, val loss: 1.2821592092514038
Epoch 480, training loss: 633.52978515625 = 1.2037394046783447 + 100.0 * 6.32326078414917
Epoch 480, val loss: 1.2648279666900635
Epoch 490, training loss: 633.4437255859375 = 1.1821823120117188 + 100.0 * 6.322615146636963
Epoch 490, val loss: 1.2474209070205688
Epoch 500, training loss: 632.8292236328125 = 1.1611064672470093 + 100.0 * 6.316680908203125
Epoch 500, val loss: 1.2306256294250488
Epoch 510, training loss: 632.5347290039062 = 1.1403385400772095 + 100.0 * 6.313944339752197
Epoch 510, val loss: 1.2142456769943237
Epoch 520, training loss: 632.4641723632812 = 1.1198557615280151 + 100.0 * 6.313443183898926
Epoch 520, val loss: 1.1982700824737549
Epoch 530, training loss: 632.0112915039062 = 1.0995004177093506 + 100.0 * 6.309117794036865
Epoch 530, val loss: 1.182680368423462
Epoch 540, training loss: 631.808837890625 = 1.079482913017273 + 100.0 * 6.30729341506958
Epoch 540, val loss: 1.1674848794937134
Epoch 550, training loss: 631.653076171875 = 1.059774398803711 + 100.0 * 6.305933475494385
Epoch 550, val loss: 1.1527131795883179
Epoch 560, training loss: 631.7970581054688 = 1.0403372049331665 + 100.0 * 6.307567119598389
Epoch 560, val loss: 1.1385725736618042
Epoch 570, training loss: 631.2552490234375 = 1.0210243463516235 + 100.0 * 6.302342414855957
Epoch 570, val loss: 1.1245231628417969
Epoch 580, training loss: 630.8909912109375 = 1.00239098072052 + 100.0 * 6.298885822296143
Epoch 580, val loss: 1.1113888025283813
Epoch 590, training loss: 631.4131469726562 = 0.9841695427894592 + 100.0 * 6.3042893409729
Epoch 590, val loss: 1.0988531112670898
Epoch 600, training loss: 630.5336303710938 = 0.965925931930542 + 100.0 * 6.2956767082214355
Epoch 600, val loss: 1.086401104927063
Epoch 610, training loss: 630.3818359375 = 0.948309063911438 + 100.0 * 6.29433536529541
Epoch 610, val loss: 1.0746612548828125
Epoch 620, training loss: 630.1452026367188 = 0.9311761260032654 + 100.0 * 6.292140483856201
Epoch 620, val loss: 1.0636087656021118
Epoch 630, training loss: 630.7305297851562 = 0.9143458604812622 + 100.0 * 6.298161506652832
Epoch 630, val loss: 1.0530208349227905
Epoch 640, training loss: 629.8734130859375 = 0.8974708914756775 + 100.0 * 6.289759635925293
Epoch 640, val loss: 1.0423732995986938
Epoch 650, training loss: 629.8124389648438 = 0.8811671137809753 + 100.0 * 6.289312839508057
Epoch 650, val loss: 1.0324550867080688
Epoch 660, training loss: 629.4910888671875 = 0.8651105761528015 + 100.0 * 6.286259651184082
Epoch 660, val loss: 1.0227771997451782
Epoch 670, training loss: 629.3098754882812 = 0.8494665026664734 + 100.0 * 6.284603595733643
Epoch 670, val loss: 1.0138174295425415
Epoch 680, training loss: 629.1812744140625 = 0.8342597484588623 + 100.0 * 6.2834696769714355
Epoch 680, val loss: 1.0054179430007935
Epoch 690, training loss: 630.1762084960938 = 0.8192757368087769 + 100.0 * 6.293569564819336
Epoch 690, val loss: 0.9973483681678772
Epoch 700, training loss: 628.92041015625 = 0.8042434453964233 + 100.0 * 6.281161785125732
Epoch 700, val loss: 0.9893124103546143
Epoch 710, training loss: 628.7078247070312 = 0.7897667288780212 + 100.0 * 6.27918004989624
Epoch 710, val loss: 0.9817897081375122
Epoch 720, training loss: 628.5731201171875 = 0.7756804823875427 + 100.0 * 6.2779741287231445
Epoch 720, val loss: 0.9749088883399963
Epoch 730, training loss: 628.7312622070312 = 0.7619317173957825 + 100.0 * 6.279693603515625
Epoch 730, val loss: 0.9686022996902466
Epoch 740, training loss: 628.6183471679688 = 0.7481482625007629 + 100.0 * 6.2787017822265625
Epoch 740, val loss: 0.961552619934082
Epoch 750, training loss: 628.3130493164062 = 0.7346434593200684 + 100.0 * 6.275784492492676
Epoch 750, val loss: 0.9560684561729431
Epoch 760, training loss: 628.1188354492188 = 0.7215611338615417 + 100.0 * 6.273972511291504
Epoch 760, val loss: 0.9502789378166199
Epoch 770, training loss: 628.4268798828125 = 0.7088232040405273 + 100.0 * 6.2771806716918945
Epoch 770, val loss: 0.9449530839920044
Epoch 780, training loss: 627.9978637695312 = 0.6961045861244202 + 100.0 * 6.273017406463623
Epoch 780, val loss: 0.9402480125427246
Epoch 790, training loss: 627.7852783203125 = 0.6838182806968689 + 100.0 * 6.271014213562012
Epoch 790, val loss: 0.9356977939605713
Epoch 800, training loss: 627.9763793945312 = 0.6717337965965271 + 100.0 * 6.273046493530273
Epoch 800, val loss: 0.9311944842338562
Epoch 810, training loss: 627.8373413085938 = 0.6598789691925049 + 100.0 * 6.271774768829346
Epoch 810, val loss: 0.9275550246238708
Epoch 820, training loss: 627.437744140625 = 0.6482065320014954 + 100.0 * 6.267895221710205
Epoch 820, val loss: 0.9235442280769348
Epoch 830, training loss: 627.288330078125 = 0.6368802189826965 + 100.0 * 6.266514301300049
Epoch 830, val loss: 0.9199820160865784
Epoch 840, training loss: 627.2380981445312 = 0.6258791089057922 + 100.0 * 6.266122341156006
Epoch 840, val loss: 0.9168865084648132
Epoch 850, training loss: 627.5955200195312 = 0.6150370240211487 + 100.0 * 6.26980447769165
Epoch 850, val loss: 0.9142897725105286
Epoch 860, training loss: 627.1185302734375 = 0.6041522026062012 + 100.0 * 6.265144348144531
Epoch 860, val loss: 0.9108951687812805
Epoch 870, training loss: 626.9486083984375 = 0.5936323404312134 + 100.0 * 6.2635498046875
Epoch 870, val loss: 0.9083977341651917
Epoch 880, training loss: 626.97216796875 = 0.5834692716598511 + 100.0 * 6.263886451721191
Epoch 880, val loss: 0.9059601426124573
Epoch 890, training loss: 626.8143920898438 = 0.5734007358551025 + 100.0 * 6.262409687042236
Epoch 890, val loss: 0.904281735420227
Epoch 900, training loss: 626.69189453125 = 0.5634733438491821 + 100.0 * 6.261284351348877
Epoch 900, val loss: 0.9020673632621765
Epoch 910, training loss: 626.5042724609375 = 0.55382239818573 + 100.0 * 6.259504318237305
Epoch 910, val loss: 0.9008650183677673
Epoch 920, training loss: 626.4173583984375 = 0.5444290041923523 + 100.0 * 6.258728981018066
Epoch 920, val loss: 0.899199366569519
Epoch 930, training loss: 627.0487060546875 = 0.5351530909538269 + 100.0 * 6.265135765075684
Epoch 930, val loss: 0.8977435231208801
Epoch 940, training loss: 627.1045532226562 = 0.5258780121803284 + 100.0 * 6.265786647796631
Epoch 940, val loss: 0.8961315751075745
Epoch 950, training loss: 626.3866577148438 = 0.5168379545211792 + 100.0 * 6.258697986602783
Epoch 950, val loss: 0.8954008221626282
Epoch 960, training loss: 626.0999145507812 = 0.5080488920211792 + 100.0 * 6.255918502807617
Epoch 960, val loss: 0.8939211964607239
Epoch 970, training loss: 626.006591796875 = 0.4995603859424591 + 100.0 * 6.255070209503174
Epoch 970, val loss: 0.8935072422027588
Epoch 980, training loss: 625.9752807617188 = 0.49122685194015503 + 100.0 * 6.254840850830078
Epoch 980, val loss: 0.893204391002655
Epoch 990, training loss: 626.1664428710938 = 0.4829443395137787 + 100.0 * 6.256834506988525
Epoch 990, val loss: 0.8923770189285278
Epoch 1000, training loss: 626.0218505859375 = 0.4747013747692108 + 100.0 * 6.255471229553223
Epoch 1000, val loss: 0.8912432789802551
Epoch 1010, training loss: 625.9183959960938 = 0.4666438400745392 + 100.0 * 6.254517555236816
Epoch 1010, val loss: 0.8908229470252991
Epoch 1020, training loss: 625.7496948242188 = 0.4588317573070526 + 100.0 * 6.252909183502197
Epoch 1020, val loss: 0.8910571932792664
Epoch 1030, training loss: 625.6753540039062 = 0.45116451382637024 + 100.0 * 6.252241611480713
Epoch 1030, val loss: 0.890735387802124
Epoch 1040, training loss: 625.7875366210938 = 0.44366270303726196 + 100.0 * 6.253438949584961
Epoch 1040, val loss: 0.8905389308929443
Epoch 1050, training loss: 625.6090698242188 = 0.436285138130188 + 100.0 * 6.251728057861328
Epoch 1050, val loss: 0.8908150792121887
Epoch 1060, training loss: 625.5070190429688 = 0.4289993345737457 + 100.0 * 6.25078010559082
Epoch 1060, val loss: 0.8909968137741089
Epoch 1070, training loss: 625.457763671875 = 0.42185747623443604 + 100.0 * 6.250359058380127
Epoch 1070, val loss: 0.8908329606056213
Epoch 1080, training loss: 625.3804321289062 = 0.4148651659488678 + 100.0 * 6.249655723571777
Epoch 1080, val loss: 0.8908689022064209
Epoch 1090, training loss: 625.156005859375 = 0.407981812953949 + 100.0 * 6.247480392456055
Epoch 1090, val loss: 0.891046941280365
Epoch 1100, training loss: 625.13720703125 = 0.4013063609600067 + 100.0 * 6.247358798980713
Epoch 1100, val loss: 0.8917891383171082
Epoch 1110, training loss: 625.4541015625 = 0.3947083652019501 + 100.0 * 6.250594139099121
Epoch 1110, val loss: 0.8922129273414612
Epoch 1120, training loss: 625.1611328125 = 0.3880843222141266 + 100.0 * 6.247730731964111
Epoch 1120, val loss: 0.8920584321022034
Epoch 1130, training loss: 624.9425048828125 = 0.3816644847393036 + 100.0 * 6.245608806610107
Epoch 1130, val loss: 0.8924657106399536
Epoch 1140, training loss: 624.9998168945312 = 0.3754616975784302 + 100.0 * 6.246243000030518
Epoch 1140, val loss: 0.8931007981300354
Epoch 1150, training loss: 624.989013671875 = 0.36928069591522217 + 100.0 * 6.246197700500488
Epoch 1150, val loss: 0.8936601877212524
Epoch 1160, training loss: 624.9102172851562 = 0.36316630244255066 + 100.0 * 6.2454705238342285
Epoch 1160, val loss: 0.8946645855903625
Epoch 1170, training loss: 624.6422729492188 = 0.357186496257782 + 100.0 * 6.2428507804870605
Epoch 1170, val loss: 0.8950473070144653
Epoch 1180, training loss: 624.586181640625 = 0.35137680172920227 + 100.0 * 6.2423481941223145
Epoch 1180, val loss: 0.8958615064620972
Epoch 1190, training loss: 624.735595703125 = 0.3457067012786865 + 100.0 * 6.243898868560791
Epoch 1190, val loss: 0.8965751528739929
Epoch 1200, training loss: 624.689208984375 = 0.33997654914855957 + 100.0 * 6.243492603302002
Epoch 1200, val loss: 0.8978687524795532
Epoch 1210, training loss: 624.5415649414062 = 0.33435070514678955 + 100.0 * 6.242072105407715
Epoch 1210, val loss: 0.898320198059082
Epoch 1220, training loss: 624.4302368164062 = 0.32892704010009766 + 100.0 * 6.241013050079346
Epoch 1220, val loss: 0.8996164202690125
Epoch 1230, training loss: 624.4915771484375 = 0.3236364722251892 + 100.0 * 6.241679668426514
Epoch 1230, val loss: 0.9010067582130432
Epoch 1240, training loss: 624.3649291992188 = 0.3183247148990631 + 100.0 * 6.240466594696045
Epoch 1240, val loss: 0.9018542170524597
Epoch 1250, training loss: 624.3580322265625 = 0.3131006360054016 + 100.0 * 6.240448951721191
Epoch 1250, val loss: 0.9027588963508606
Epoch 1260, training loss: 624.3267822265625 = 0.30801114439964294 + 100.0 * 6.240188121795654
Epoch 1260, val loss: 0.9039644002914429
Epoch 1270, training loss: 624.1690063476562 = 0.30299490690231323 + 100.0 * 6.2386603355407715
Epoch 1270, val loss: 0.9053155779838562
Epoch 1280, training loss: 624.409912109375 = 0.29809290170669556 + 100.0 * 6.241118431091309
Epoch 1280, val loss: 0.9065694212913513
Epoch 1290, training loss: 624.2061767578125 = 0.2932001054286957 + 100.0 * 6.239129543304443
Epoch 1290, val loss: 0.906876802444458
Epoch 1300, training loss: 623.9539184570312 = 0.2883980870246887 + 100.0 * 6.236655235290527
Epoch 1300, val loss: 0.9091399312019348
Epoch 1310, training loss: 624.2252197265625 = 0.28374961018562317 + 100.0 * 6.239414691925049
Epoch 1310, val loss: 0.9100801348686218
Epoch 1320, training loss: 623.9059448242188 = 0.27904170751571655 + 100.0 * 6.236268997192383
Epoch 1320, val loss: 0.9118754267692566
Epoch 1330, training loss: 624.344970703125 = 0.27445095777511597 + 100.0 * 6.2407050132751465
Epoch 1330, val loss: 0.9123241305351257
Epoch 1340, training loss: 623.826416015625 = 0.2699440121650696 + 100.0 * 6.235564708709717
Epoch 1340, val loss: 0.9147244095802307
Epoch 1350, training loss: 623.685791015625 = 0.26556599140167236 + 100.0 * 6.2342023849487305
Epoch 1350, val loss: 0.9160336256027222
Epoch 1360, training loss: 623.681884765625 = 0.2612841725349426 + 100.0 * 6.234206199645996
Epoch 1360, val loss: 0.9176066517829895
Epoch 1370, training loss: 624.3666381835938 = 0.257095068693161 + 100.0 * 6.241095542907715
Epoch 1370, val loss: 0.91998291015625
Epoch 1380, training loss: 623.8958740234375 = 0.25271323323249817 + 100.0 * 6.236432075500488
Epoch 1380, val loss: 0.9201440215110779
Epoch 1390, training loss: 623.6113891601562 = 0.24857057631015778 + 100.0 * 6.233627796173096
Epoch 1390, val loss: 0.921596884727478
Epoch 1400, training loss: 623.5173950195312 = 0.24452508985996246 + 100.0 * 6.232728958129883
Epoch 1400, val loss: 0.9235009551048279
Epoch 1410, training loss: 624.301025390625 = 0.24054725468158722 + 100.0 * 6.240604877471924
Epoch 1410, val loss: 0.9245634078979492
Epoch 1420, training loss: 623.6722412109375 = 0.23650409281253815 + 100.0 * 6.2343573570251465
Epoch 1420, val loss: 0.9263889789581299
Epoch 1430, training loss: 623.4898681640625 = 0.2325723022222519 + 100.0 * 6.23257303237915
Epoch 1430, val loss: 0.9280388355255127
Epoch 1440, training loss: 623.6448364257812 = 0.22877490520477295 + 100.0 * 6.234160900115967
Epoch 1440, val loss: 0.9298129677772522
Epoch 1450, training loss: 623.2744140625 = 0.22496233880519867 + 100.0 * 6.230494499206543
Epoch 1450, val loss: 0.9309258460998535
Epoch 1460, training loss: 623.29638671875 = 0.22128178179264069 + 100.0 * 6.2307515144348145
Epoch 1460, val loss: 0.9324434399604797
Epoch 1470, training loss: 623.2598266601562 = 0.21766945719718933 + 100.0 * 6.230421543121338
Epoch 1470, val loss: 0.9340261220932007
Epoch 1480, training loss: 623.7517700195312 = 0.21408160030841827 + 100.0 * 6.235376834869385
Epoch 1480, val loss: 0.9353784322738647
Epoch 1490, training loss: 623.3905639648438 = 0.21041448414325714 + 100.0 * 6.231801509857178
Epoch 1490, val loss: 0.9376214742660522
Epoch 1500, training loss: 623.1375122070312 = 0.20692214369773865 + 100.0 * 6.229305744171143
Epoch 1500, val loss: 0.9390521049499512
Epoch 1510, training loss: 623.0222778320312 = 0.20351260900497437 + 100.0 * 6.2281880378723145
Epoch 1510, val loss: 0.9407702088356018
Epoch 1520, training loss: 623.14404296875 = 0.20019859075546265 + 100.0 * 6.229438781738281
Epoch 1520, val loss: 0.9422362446784973
Epoch 1530, training loss: 623.1448364257812 = 0.19681397080421448 + 100.0 * 6.229479789733887
Epoch 1530, val loss: 0.9447310566902161
Epoch 1540, training loss: 623.0215454101562 = 0.19345387816429138 + 100.0 * 6.228280544281006
Epoch 1540, val loss: 0.945570170879364
Epoch 1550, training loss: 622.933349609375 = 0.19025486707687378 + 100.0 * 6.227430820465088
Epoch 1550, val loss: 0.9479600787162781
Epoch 1560, training loss: 623.0806884765625 = 0.18714934587478638 + 100.0 * 6.228935241699219
Epoch 1560, val loss: 0.9500569701194763
Epoch 1570, training loss: 622.9134521484375 = 0.18395906686782837 + 100.0 * 6.227294921875
Epoch 1570, val loss: 0.9512267112731934
Epoch 1580, training loss: 622.9783935546875 = 0.18085503578186035 + 100.0 * 6.227975368499756
Epoch 1580, val loss: 0.9534853100776672
Epoch 1590, training loss: 622.8877563476562 = 0.17779456079006195 + 100.0 * 6.227099895477295
Epoch 1590, val loss: 0.9547367691993713
Epoch 1600, training loss: 622.9511108398438 = 0.17484059929847717 + 100.0 * 6.227762699127197
Epoch 1600, val loss: 0.9564521908760071
Epoch 1610, training loss: 622.8304443359375 = 0.17189458012580872 + 100.0 * 6.226585388183594
Epoch 1610, val loss: 0.9583940505981445
Epoch 1620, training loss: 622.7507934570312 = 0.16903124749660492 + 100.0 * 6.225818157196045
Epoch 1620, val loss: 0.9608175754547119
Epoch 1630, training loss: 622.896728515625 = 0.16620823740959167 + 100.0 * 6.2273054122924805
Epoch 1630, val loss: 0.9629500508308411
Epoch 1640, training loss: 622.84130859375 = 0.16339342296123505 + 100.0 * 6.226778984069824
Epoch 1640, val loss: 0.9643154740333557
Epoch 1650, training loss: 622.7469482421875 = 0.16064968705177307 + 100.0 * 6.225862979888916
Epoch 1650, val loss: 0.9660024046897888
Epoch 1660, training loss: 622.6702270507812 = 0.1579340100288391 + 100.0 * 6.225122928619385
Epoch 1660, val loss: 0.9679908156394958
Epoch 1670, training loss: 622.6388549804688 = 0.15528316795825958 + 100.0 * 6.2248358726501465
Epoch 1670, val loss: 0.9698458313941956
Epoch 1680, training loss: 622.7068481445312 = 0.15270303189754486 + 100.0 * 6.225541591644287
Epoch 1680, val loss: 0.9724324345588684
Epoch 1690, training loss: 622.7100219726562 = 0.1501028835773468 + 100.0 * 6.22559928894043
Epoch 1690, val loss: 0.9751890897750854
Epoch 1700, training loss: 622.4720458984375 = 0.14755283296108246 + 100.0 * 6.223244667053223
Epoch 1700, val loss: 0.9763712286949158
Epoch 1710, training loss: 622.438720703125 = 0.14509336650371552 + 100.0 * 6.222936630249023
Epoch 1710, val loss: 0.978413999080658
Epoch 1720, training loss: 622.4309692382812 = 0.1426849663257599 + 100.0 * 6.2228827476501465
Epoch 1720, val loss: 0.9808741807937622
Epoch 1730, training loss: 622.9351196289062 = 0.14027614891529083 + 100.0 * 6.2279486656188965
Epoch 1730, val loss: 0.982668399810791
Epoch 1740, training loss: 622.7730102539062 = 0.1378992199897766 + 100.0 * 6.226351261138916
Epoch 1740, val loss: 0.9855611324310303
Epoch 1750, training loss: 622.4267578125 = 0.1355028748512268 + 100.0 * 6.222912311553955
Epoch 1750, val loss: 0.986885130405426
Epoch 1760, training loss: 622.3209228515625 = 0.13326990604400635 + 100.0 * 6.221877098083496
Epoch 1760, val loss: 0.9894979000091553
Epoch 1770, training loss: 622.3854370117188 = 0.13105575740337372 + 100.0 * 6.222543716430664
Epoch 1770, val loss: 0.9924649596214294
Epoch 1780, training loss: 622.5615844726562 = 0.12886933982372284 + 100.0 * 6.224327087402344
Epoch 1780, val loss: 0.9944875240325928
Epoch 1790, training loss: 622.3457641601562 = 0.12665213644504547 + 100.0 * 6.222190856933594
Epoch 1790, val loss: 0.996005117893219
Epoch 1800, training loss: 622.2030029296875 = 0.12453895062208176 + 100.0 * 6.220785140991211
Epoch 1800, val loss: 0.9990983605384827
Epoch 1810, training loss: 622.1129150390625 = 0.12249201536178589 + 100.0 * 6.219903945922852
Epoch 1810, val loss: 1.001308560371399
Epoch 1820, training loss: 622.353271484375 = 0.12049871683120728 + 100.0 * 6.222328186035156
Epoch 1820, val loss: 1.0041048526763916
Epoch 1830, training loss: 622.1988525390625 = 0.11844723671674728 + 100.0 * 6.220803737640381
Epoch 1830, val loss: 1.0057557821273804
Epoch 1840, training loss: 622.0370483398438 = 0.11641515791416168 + 100.0 * 6.2192063331604
Epoch 1840, val loss: 1.0085240602493286
Epoch 1850, training loss: 621.979248046875 = 0.1145116314291954 + 100.0 * 6.218647480010986
Epoch 1850, val loss: 1.010891318321228
Epoch 1860, training loss: 621.9866943359375 = 0.11265241354703903 + 100.0 * 6.218739986419678
Epoch 1860, val loss: 1.0134656429290771
Epoch 1870, training loss: 622.9088745117188 = 0.1108124777674675 + 100.0 * 6.227980136871338
Epoch 1870, val loss: 1.014704942703247
Epoch 1880, training loss: 622.43505859375 = 0.10889589786529541 + 100.0 * 6.223261833190918
Epoch 1880, val loss: 1.0179418325424194
Epoch 1890, training loss: 621.96923828125 = 0.10705007612705231 + 100.0 * 6.218621730804443
Epoch 1890, val loss: 1.0207924842834473
Epoch 1900, training loss: 621.9379272460938 = 0.1052980124950409 + 100.0 * 6.218326568603516
Epoch 1900, val loss: 1.0243324041366577
Epoch 1910, training loss: 622.040283203125 = 0.10360366106033325 + 100.0 * 6.219367027282715
Epoch 1910, val loss: 1.0267081260681152
Epoch 1920, training loss: 622.0068359375 = 0.10188590735197067 + 100.0 * 6.21904993057251
Epoch 1920, val loss: 1.0290755033493042
Epoch 1930, training loss: 621.9375610351562 = 0.10020111501216888 + 100.0 * 6.2183732986450195
Epoch 1930, val loss: 1.032143473625183
Epoch 1940, training loss: 621.9295654296875 = 0.0985790491104126 + 100.0 * 6.2183098793029785
Epoch 1940, val loss: 1.0351290702819824
Epoch 1950, training loss: 621.8955078125 = 0.0969710499048233 + 100.0 * 6.217985153198242
Epoch 1950, val loss: 1.0375851392745972
Epoch 1960, training loss: 621.7908935546875 = 0.09539196640253067 + 100.0 * 6.216954708099365
Epoch 1960, val loss: 1.0391387939453125
Epoch 1970, training loss: 621.8472900390625 = 0.09385264664888382 + 100.0 * 6.217534065246582
Epoch 1970, val loss: 1.0422838926315308
Epoch 1980, training loss: 622.2676391601562 = 0.09231705963611603 + 100.0 * 6.221753120422363
Epoch 1980, val loss: 1.045243740081787
Epoch 1990, training loss: 621.8583984375 = 0.09078044444322586 + 100.0 * 6.217676162719727
Epoch 1990, val loss: 1.048339605331421
Epoch 2000, training loss: 621.6658325195312 = 0.08929368853569031 + 100.0 * 6.215765476226807
Epoch 2000, val loss: 1.0504264831542969
Epoch 2010, training loss: 621.5963745117188 = 0.08788976818323135 + 100.0 * 6.215084552764893
Epoch 2010, val loss: 1.0541398525238037
Epoch 2020, training loss: 621.934814453125 = 0.08651792258024216 + 100.0 * 6.218482971191406
Epoch 2020, val loss: 1.0571056604385376
Epoch 2030, training loss: 621.6018676757812 = 0.08508003503084183 + 100.0 * 6.215167999267578
Epoch 2030, val loss: 1.0592740774154663
Epoch 2040, training loss: 621.5762329101562 = 0.08369641751050949 + 100.0 * 6.214925765991211
Epoch 2040, val loss: 1.0623277425765991
Epoch 2050, training loss: 621.8998413085938 = 0.08238998800516129 + 100.0 * 6.218174457550049
Epoch 2050, val loss: 1.0658007860183716
Epoch 2060, training loss: 621.5006103515625 = 0.08102544397115707 + 100.0 * 6.214195251464844
Epoch 2060, val loss: 1.068264126777649
Epoch 2070, training loss: 621.5725708007812 = 0.07973293960094452 + 100.0 * 6.21492862701416
Epoch 2070, val loss: 1.0713173151016235
Epoch 2080, training loss: 621.7625732421875 = 0.07848519086837769 + 100.0 * 6.216840744018555
Epoch 2080, val loss: 1.0740348100662231
Epoch 2090, training loss: 621.5552368164062 = 0.0772395059466362 + 100.0 * 6.214780330657959
Epoch 2090, val loss: 1.0770400762557983
Epoch 2100, training loss: 621.7264404296875 = 0.07603412866592407 + 100.0 * 6.216504096984863
Epoch 2100, val loss: 1.079263687133789
Epoch 2110, training loss: 621.46484375 = 0.0748114287853241 + 100.0 * 6.213900089263916
Epoch 2110, val loss: 1.0825691223144531
Epoch 2120, training loss: 621.7356567382812 = 0.07364708185195923 + 100.0 * 6.216619968414307
Epoch 2120, val loss: 1.0843688249588013
Epoch 2130, training loss: 621.5320434570312 = 0.07248431444168091 + 100.0 * 6.214595317840576
Epoch 2130, val loss: 1.087734341621399
Epoch 2140, training loss: 621.3770141601562 = 0.07134126871824265 + 100.0 * 6.213056564331055
Epoch 2140, val loss: 1.0914312601089478
Epoch 2150, training loss: 621.3989868164062 = 0.07023759186267853 + 100.0 * 6.213287353515625
Epoch 2150, val loss: 1.0945947170257568
Epoch 2160, training loss: 621.6875 = 0.06917992234230042 + 100.0 * 6.216183185577393
Epoch 2160, val loss: 1.0965960025787354
Epoch 2170, training loss: 621.5073852539062 = 0.06805428862571716 + 100.0 * 6.214393615722656
Epoch 2170, val loss: 1.1005855798721313
Epoch 2180, training loss: 621.3466186523438 = 0.06698139011859894 + 100.0 * 6.212796688079834
Epoch 2180, val loss: 1.1030638217926025
Epoch 2190, training loss: 621.4532470703125 = 0.06595133244991302 + 100.0 * 6.213872909545898
Epoch 2190, val loss: 1.1060646772384644
Epoch 2200, training loss: 621.5641479492188 = 0.06493426114320755 + 100.0 * 6.214992046356201
Epoch 2200, val loss: 1.1078855991363525
Epoch 2210, training loss: 621.4546508789062 = 0.06392544507980347 + 100.0 * 6.213907241821289
Epoch 2210, val loss: 1.1112183332443237
Epoch 2220, training loss: 621.3056640625 = 0.06293955445289612 + 100.0 * 6.212427616119385
Epoch 2220, val loss: 1.1140649318695068
Epoch 2230, training loss: 621.4118041992188 = 0.061997875571250916 + 100.0 * 6.213497638702393
Epoch 2230, val loss: 1.1172388792037964
Epoch 2240, training loss: 621.2758178710938 = 0.06104828417301178 + 100.0 * 6.2121477127075195
Epoch 2240, val loss: 1.1207823753356934
Epoch 2250, training loss: 621.2926025390625 = 0.06013479456305504 + 100.0 * 6.212325096130371
Epoch 2250, val loss: 1.1243197917938232
Epoch 2260, training loss: 621.3054809570312 = 0.059235259890556335 + 100.0 * 6.212461948394775
Epoch 2260, val loss: 1.1270524263381958
Epoch 2270, training loss: 621.1671752929688 = 0.058331675827503204 + 100.0 * 6.211088180541992
Epoch 2270, val loss: 1.1298010349273682
Epoch 2280, training loss: 621.8653564453125 = 0.05746456980705261 + 100.0 * 6.218079090118408
Epoch 2280, val loss: 1.1328190565109253
Epoch 2290, training loss: 621.2596435546875 = 0.05656204745173454 + 100.0 * 6.21203088760376
Epoch 2290, val loss: 1.1348062753677368
Epoch 2300, training loss: 621.0460205078125 = 0.055696748197078705 + 100.0 * 6.209903240203857
Epoch 2300, val loss: 1.137870192527771
Epoch 2310, training loss: 621.0187377929688 = 0.054894108325242996 + 100.0 * 6.209638595581055
Epoch 2310, val loss: 1.1410319805145264
Epoch 2320, training loss: 621.0994873046875 = 0.0541064590215683 + 100.0 * 6.210453987121582
Epoch 2320, val loss: 1.144585132598877
Epoch 2330, training loss: 621.3651733398438 = 0.05331723764538765 + 100.0 * 6.213118553161621
Epoch 2330, val loss: 1.1470867395401
Epoch 2340, training loss: 621.2904663085938 = 0.052516788244247437 + 100.0 * 6.2123799324035645
Epoch 2340, val loss: 1.1492799520492554
Epoch 2350, training loss: 621.5231323242188 = 0.05173739790916443 + 100.0 * 6.2147135734558105
Epoch 2350, val loss: 1.1532034873962402
Epoch 2360, training loss: 621.0748901367188 = 0.05093513801693916 + 100.0 * 6.210239410400391
Epoch 2360, val loss: 1.1542268991470337
Epoch 2370, training loss: 621.0285034179688 = 0.0501796118915081 + 100.0 * 6.209783554077148
Epoch 2370, val loss: 1.1581954956054688
Epoch 2380, training loss: 621.0184326171875 = 0.04947609081864357 + 100.0 * 6.209689617156982
Epoch 2380, val loss: 1.1612032651901245
Epoch 2390, training loss: 621.2009887695312 = 0.0487692728638649 + 100.0 * 6.211522102355957
Epoch 2390, val loss: 1.1645128726959229
Epoch 2400, training loss: 620.904296875 = 0.048057060688734055 + 100.0 * 6.208562850952148
Epoch 2400, val loss: 1.1675617694854736
Epoch 2410, training loss: 621.3038330078125 = 0.04739011451601982 + 100.0 * 6.212564468383789
Epoch 2410, val loss: 1.1712695360183716
Epoch 2420, training loss: 621.0022583007812 = 0.04668377339839935 + 100.0 * 6.209555625915527
Epoch 2420, val loss: 1.1724376678466797
Epoch 2430, training loss: 620.9264526367188 = 0.04600392282009125 + 100.0 * 6.208804607391357
Epoch 2430, val loss: 1.1758393049240112
Epoch 2440, training loss: 620.8785400390625 = 0.0453588142991066 + 100.0 * 6.208332061767578
Epoch 2440, val loss: 1.1787070035934448
Epoch 2450, training loss: 621.0631103515625 = 0.044739145785570145 + 100.0 * 6.210183620452881
Epoch 2450, val loss: 1.1817927360534668
Epoch 2460, training loss: 621.004150390625 = 0.04409366101026535 + 100.0 * 6.209600925445557
Epoch 2460, val loss: 1.1849446296691895
Epoch 2470, training loss: 620.841064453125 = 0.043465521186590195 + 100.0 * 6.2079758644104
Epoch 2470, val loss: 1.1866247653961182
Epoch 2480, training loss: 620.951171875 = 0.04286929965019226 + 100.0 * 6.209083557128906
Epoch 2480, val loss: 1.190199851989746
Epoch 2490, training loss: 621.1748046875 = 0.04226992279291153 + 100.0 * 6.211325168609619
Epoch 2490, val loss: 1.1929914951324463
Epoch 2500, training loss: 620.7678833007812 = 0.041645243763923645 + 100.0 * 6.2072625160217285
Epoch 2500, val loss: 1.1952489614486694
Epoch 2510, training loss: 620.7232055664062 = 0.041066981852054596 + 100.0 * 6.206821441650391
Epoch 2510, val loss: 1.1977906227111816
Epoch 2520, training loss: 620.7418823242188 = 0.040521763265132904 + 100.0 * 6.2070136070251465
Epoch 2520, val loss: 1.2008394002914429
Epoch 2530, training loss: 621.2061157226562 = 0.03998975083231926 + 100.0 * 6.211661338806152
Epoch 2530, val loss: 1.2026575803756714
Epoch 2540, training loss: 620.798828125 = 0.039415180683135986 + 100.0 * 6.20759391784668
Epoch 2540, val loss: 1.2071915864944458
Epoch 2550, training loss: 620.7379150390625 = 0.03887904807925224 + 100.0 * 6.2069902420043945
Epoch 2550, val loss: 1.2094794511795044
Epoch 2560, training loss: 620.8668212890625 = 0.0383586548268795 + 100.0 * 6.208284854888916
Epoch 2560, val loss: 1.212531328201294
Epoch 2570, training loss: 621.0072021484375 = 0.037833746522665024 + 100.0 * 6.209693431854248
Epoch 2570, val loss: 1.2134654521942139
Epoch 2580, training loss: 620.8760986328125 = 0.03730544075369835 + 100.0 * 6.208388328552246
Epoch 2580, val loss: 1.2186315059661865
Epoch 2590, training loss: 620.640380859375 = 0.03680724278092384 + 100.0 * 6.206035614013672
Epoch 2590, val loss: 1.2206186056137085
Epoch 2600, training loss: 620.6280517578125 = 0.036325983703136444 + 100.0 * 6.2059173583984375
Epoch 2600, val loss: 1.2236964702606201
Epoch 2610, training loss: 620.7521362304688 = 0.035866059362888336 + 100.0 * 6.207162380218506
Epoch 2610, val loss: 1.2272779941558838
Epoch 2620, training loss: 620.8924560546875 = 0.035391196608543396 + 100.0 * 6.20857048034668
Epoch 2620, val loss: 1.2297495603561401
Epoch 2630, training loss: 620.978515625 = 0.034905027598142624 + 100.0 * 6.209435939788818
Epoch 2630, val loss: 1.2310662269592285
Epoch 2640, training loss: 620.8348999023438 = 0.03444080799818039 + 100.0 * 6.208004951477051
Epoch 2640, val loss: 1.2349817752838135
Epoch 2650, training loss: 620.8819580078125 = 0.033980004489421844 + 100.0 * 6.208479404449463
Epoch 2650, val loss: 1.2363636493682861
Epoch 2660, training loss: 620.5496215820312 = 0.033531881868839264 + 100.0 * 6.205161094665527
Epoch 2660, val loss: 1.2400847673416138
Epoch 2670, training loss: 620.5396118164062 = 0.03310972452163696 + 100.0 * 6.2050652503967285
Epoch 2670, val loss: 1.2426512241363525
Epoch 2680, training loss: 620.6298828125 = 0.03270138427615166 + 100.0 * 6.205971717834473
Epoch 2680, val loss: 1.2453120946884155
Epoch 2690, training loss: 620.9038696289062 = 0.03228537365794182 + 100.0 * 6.208715915679932
Epoch 2690, val loss: 1.2481021881103516
Epoch 2700, training loss: 620.754150390625 = 0.03184417262673378 + 100.0 * 6.207222938537598
Epoch 2700, val loss: 1.250126600265503
Epoch 2710, training loss: 620.6016845703125 = 0.031440917402505875 + 100.0 * 6.205702781677246
Epoch 2710, val loss: 1.2527230978012085
Epoch 2720, training loss: 620.6909790039062 = 0.031043430790305138 + 100.0 * 6.206599235534668
Epoch 2720, val loss: 1.2565710544586182
Epoch 2730, training loss: 620.6658325195312 = 0.030646104365587234 + 100.0 * 6.2063517570495605
Epoch 2730, val loss: 1.2588059902191162
Epoch 2740, training loss: 620.697998046875 = 0.030247503891587257 + 100.0 * 6.206676959991455
Epoch 2740, val loss: 1.260611891746521
Epoch 2750, training loss: 620.4515991210938 = 0.029860053211450577 + 100.0 * 6.204217433929443
Epoch 2750, val loss: 1.2632684707641602
Epoch 2760, training loss: 620.3960571289062 = 0.02949206531047821 + 100.0 * 6.203665733337402
Epoch 2760, val loss: 1.2667075395584106
Epoch 2770, training loss: 620.3748779296875 = 0.029141459614038467 + 100.0 * 6.203457355499268
Epoch 2770, val loss: 1.269322395324707
Epoch 2780, training loss: 620.7108154296875 = 0.028799118474125862 + 100.0 * 6.206820011138916
Epoch 2780, val loss: 1.2712197303771973
Epoch 2790, training loss: 620.673583984375 = 0.028420982882380486 + 100.0 * 6.206451416015625
Epoch 2790, val loss: 1.273111343383789
Epoch 2800, training loss: 620.553466796875 = 0.028054561465978622 + 100.0 * 6.205254077911377
Epoch 2800, val loss: 1.276663899421692
Epoch 2810, training loss: 620.3482055664062 = 0.027707044035196304 + 100.0 * 6.203205108642578
Epoch 2810, val loss: 1.2786070108413696
Epoch 2820, training loss: 620.330078125 = 0.02738391049206257 + 100.0 * 6.20302677154541
Epoch 2820, val loss: 1.2817882299423218
Epoch 2830, training loss: 620.9165649414062 = 0.02707224152982235 + 100.0 * 6.208894729614258
Epoch 2830, val loss: 1.2833690643310547
Epoch 2840, training loss: 620.4246215820312 = 0.02671797201037407 + 100.0 * 6.203979015350342
Epoch 2840, val loss: 1.2859420776367188
Epoch 2850, training loss: 620.34716796875 = 0.026388579979538918 + 100.0 * 6.203207969665527
Epoch 2850, val loss: 1.2889254093170166
Epoch 2860, training loss: 620.3312377929688 = 0.02608114667236805 + 100.0 * 6.203052043914795
Epoch 2860, val loss: 1.2915014028549194
Epoch 2870, training loss: 620.5818481445312 = 0.02578120119869709 + 100.0 * 6.20556116104126
Epoch 2870, val loss: 1.2939907312393188
Epoch 2880, training loss: 620.277587890625 = 0.025468967854976654 + 100.0 * 6.202521324157715
Epoch 2880, val loss: 1.2967671155929565
Epoch 2890, training loss: 620.4108276367188 = 0.025179509073495865 + 100.0 * 6.203855991363525
Epoch 2890, val loss: 1.2995374202728271
Epoch 2900, training loss: 620.5748291015625 = 0.02488711103796959 + 100.0 * 6.205499172210693
Epoch 2900, val loss: 1.3031067848205566
Epoch 2910, training loss: 620.4141235351562 = 0.024583790451288223 + 100.0 * 6.203895092010498
Epoch 2910, val loss: 1.3037316799163818
Epoch 2920, training loss: 620.3125610351562 = 0.024294879287481308 + 100.0 * 6.202882766723633
Epoch 2920, val loss: 1.3061991930007935
Epoch 2930, training loss: 620.272705078125 = 0.02401610091328621 + 100.0 * 6.202487468719482
Epoch 2930, val loss: 1.3092246055603027
Epoch 2940, training loss: 621.1016235351562 = 0.023751085624098778 + 100.0 * 6.210778713226318
Epoch 2940, val loss: 1.3095200061798096
Epoch 2950, training loss: 620.6790161132812 = 0.0234623271971941 + 100.0 * 6.2065558433532715
Epoch 2950, val loss: 1.312941551208496
Epoch 2960, training loss: 620.2772827148438 = 0.023170577362179756 + 100.0 * 6.202540874481201
Epoch 2960, val loss: 1.3156694173812866
Epoch 2970, training loss: 620.1638793945312 = 0.022918367758393288 + 100.0 * 6.201409816741943
Epoch 2970, val loss: 1.3180820941925049
Epoch 2980, training loss: 620.13037109375 = 0.022672070190310478 + 100.0 * 6.201077461242676
Epoch 2980, val loss: 1.3209683895111084
Epoch 2990, training loss: 620.3292236328125 = 0.02243507094681263 + 100.0 * 6.203067779541016
Epoch 2990, val loss: 1.3241000175476074
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 861.6331787109375 = 1.9518591165542603 + 100.0 * 8.596813201904297
Epoch 0, val loss: 1.9487175941467285
Epoch 10, training loss: 861.52490234375 = 1.9435622692108154 + 100.0 * 8.595813751220703
Epoch 10, val loss: 1.9408385753631592
Epoch 20, training loss: 860.8245239257812 = 1.9330748319625854 + 100.0 * 8.58891487121582
Epoch 20, val loss: 1.9306156635284424
Epoch 30, training loss: 856.0554809570312 = 1.9191960096359253 + 100.0 * 8.541362762451172
Epoch 30, val loss: 1.9167946577072144
Epoch 40, training loss: 825.5260620117188 = 1.9010889530181885 + 100.0 * 8.236249923706055
Epoch 40, val loss: 1.898844838142395
Epoch 50, training loss: 769.817138671875 = 1.8804436922073364 + 100.0 * 7.6793670654296875
Epoch 50, val loss: 1.8796800374984741
Epoch 60, training loss: 723.0745849609375 = 1.8667999505996704 + 100.0 * 7.212077617645264
Epoch 60, val loss: 1.8670305013656616
Epoch 70, training loss: 700.1962890625 = 1.8559972047805786 + 100.0 * 6.983402729034424
Epoch 70, val loss: 1.8564567565917969
Epoch 80, training loss: 689.7153930664062 = 1.845016360282898 + 100.0 * 6.878703594207764
Epoch 80, val loss: 1.8455675840377808
Epoch 90, training loss: 679.8585815429688 = 1.833845615386963 + 100.0 * 6.780247211456299
Epoch 90, val loss: 1.8348536491394043
Epoch 100, training loss: 673.6654052734375 = 1.8244661092758179 + 100.0 * 6.718409538269043
Epoch 100, val loss: 1.8255306482315063
Epoch 110, training loss: 668.50390625 = 1.815611720085144 + 100.0 * 6.6668829917907715
Epoch 110, val loss: 1.816451072692871
Epoch 120, training loss: 664.1332397460938 = 1.8069626092910767 + 100.0 * 6.623262882232666
Epoch 120, val loss: 1.8073923587799072
Epoch 130, training loss: 660.4259643554688 = 1.7985953092575073 + 100.0 * 6.586273670196533
Epoch 130, val loss: 1.798460841178894
Epoch 140, training loss: 657.2587280273438 = 1.7903459072113037 + 100.0 * 6.554683685302734
Epoch 140, val loss: 1.7895781993865967
Epoch 150, training loss: 654.744384765625 = 1.7817327976226807 + 100.0 * 6.529626369476318
Epoch 150, val loss: 1.7803109884262085
Epoch 160, training loss: 652.4188842773438 = 1.7724579572677612 + 100.0 * 6.50646448135376
Epoch 160, val loss: 1.770448923110962
Epoch 170, training loss: 650.5592041015625 = 1.762428879737854 + 100.0 * 6.487967491149902
Epoch 170, val loss: 1.759856939315796
Epoch 180, training loss: 649.2633666992188 = 1.751585602760315 + 100.0 * 6.4751176834106445
Epoch 180, val loss: 1.7485370635986328
Epoch 190, training loss: 647.8168334960938 = 1.7396308183670044 + 100.0 * 6.4607720375061035
Epoch 190, val loss: 1.7361868619918823
Epoch 200, training loss: 646.4610595703125 = 1.7266322374343872 + 100.0 * 6.4473443031311035
Epoch 200, val loss: 1.7228435277938843
Epoch 210, training loss: 645.445068359375 = 1.7124440670013428 + 100.0 * 6.437325954437256
Epoch 210, val loss: 1.708376407623291
Epoch 220, training loss: 644.6595458984375 = 1.6969832181930542 + 100.0 * 6.429625988006592
Epoch 220, val loss: 1.692647099494934
Epoch 230, training loss: 643.7339477539062 = 1.6801382303237915 + 100.0 * 6.420538425445557
Epoch 230, val loss: 1.6757421493530273
Epoch 240, training loss: 642.7891845703125 = 1.662036418914795 + 100.0 * 6.411271095275879
Epoch 240, val loss: 1.6577215194702148
Epoch 250, training loss: 642.18994140625 = 1.6427220106124878 + 100.0 * 6.405472278594971
Epoch 250, val loss: 1.6386555433273315
Epoch 260, training loss: 641.4666137695312 = 1.6221438646316528 + 100.0 * 6.398444652557373
Epoch 260, val loss: 1.6186429262161255
Epoch 270, training loss: 640.6537475585938 = 1.600645661354065 + 100.0 * 6.390531063079834
Epoch 270, val loss: 1.5978589057922363
Epoch 280, training loss: 639.9769287109375 = 1.5783346891403198 + 100.0 * 6.383986473083496
Epoch 280, val loss: 1.5765703916549683
Epoch 290, training loss: 639.423095703125 = 1.555437684059143 + 100.0 * 6.378676891326904
Epoch 290, val loss: 1.5549448728561401
Epoch 300, training loss: 638.87646484375 = 1.5322130918502808 + 100.0 * 6.37344217300415
Epoch 300, val loss: 1.5332077741622925
Epoch 310, training loss: 638.4248046875 = 1.5087002515792847 + 100.0 * 6.3691606521606445
Epoch 310, val loss: 1.511513113975525
Epoch 320, training loss: 637.751953125 = 1.4853568077087402 + 100.0 * 6.362666130065918
Epoch 320, val loss: 1.4900857210159302
Epoch 330, training loss: 637.4506225585938 = 1.4622254371643066 + 100.0 * 6.359884262084961
Epoch 330, val loss: 1.4690834283828735
Epoch 340, training loss: 636.8416137695312 = 1.439275860786438 + 100.0 * 6.354023456573486
Epoch 340, val loss: 1.4485605955123901
Epoch 350, training loss: 636.30322265625 = 1.41675865650177 + 100.0 * 6.348865032196045
Epoch 350, val loss: 1.428571343421936
Epoch 360, training loss: 636.3306274414062 = 1.3944330215454102 + 100.0 * 6.349362373352051
Epoch 360, val loss: 1.408906102180481
Epoch 370, training loss: 635.5223388671875 = 1.3723875284194946 + 100.0 * 6.3414998054504395
Epoch 370, val loss: 1.389892339706421
Epoch 380, training loss: 635.0606689453125 = 1.3506425619125366 + 100.0 * 6.337100505828857
Epoch 380, val loss: 1.3713276386260986
Epoch 390, training loss: 634.6458129882812 = 1.3291981220245361 + 100.0 * 6.333166122436523
Epoch 390, val loss: 1.353112816810608
Epoch 400, training loss: 634.783935546875 = 1.3077888488769531 + 100.0 * 6.334761142730713
Epoch 400, val loss: 1.3351538181304932
Epoch 410, training loss: 634.1063842773438 = 1.2863203287124634 + 100.0 * 6.328200817108154
Epoch 410, val loss: 1.317198634147644
Epoch 420, training loss: 633.69873046875 = 1.2648547887802124 + 100.0 * 6.324338912963867
Epoch 420, val loss: 1.2994799613952637
Epoch 430, training loss: 633.3893432617188 = 1.2434089183807373 + 100.0 * 6.3214592933654785
Epoch 430, val loss: 1.2819647789001465
Epoch 440, training loss: 633.10302734375 = 1.221898078918457 + 100.0 * 6.318810939788818
Epoch 440, val loss: 1.2645177841186523
Epoch 450, training loss: 632.8925170898438 = 1.2003817558288574 + 100.0 * 6.316921234130859
Epoch 450, val loss: 1.2472010850906372
Epoch 460, training loss: 632.68310546875 = 1.1788374185562134 + 100.0 * 6.315042972564697
Epoch 460, val loss: 1.2300173044204712
Epoch 470, training loss: 632.357177734375 = 1.157236933708191 + 100.0 * 6.311999320983887
Epoch 470, val loss: 1.2130391597747803
Epoch 480, training loss: 632.0604248046875 = 1.1357654333114624 + 100.0 * 6.309247016906738
Epoch 480, val loss: 1.1961768865585327
Epoch 490, training loss: 631.9203491210938 = 1.1143746376037598 + 100.0 * 6.3080596923828125
Epoch 490, val loss: 1.179679274559021
Epoch 500, training loss: 631.5567626953125 = 1.0931652784347534 + 100.0 * 6.304636001586914
Epoch 500, val loss: 1.163340449333191
Epoch 510, training loss: 631.3087158203125 = 1.0720957517623901 + 100.0 * 6.302366256713867
Epoch 510, val loss: 1.1473801136016846
Epoch 520, training loss: 631.0999755859375 = 1.0514051914215088 + 100.0 * 6.300485610961914
Epoch 520, val loss: 1.1319329738616943
Epoch 530, training loss: 631.1016235351562 = 1.03086519241333 + 100.0 * 6.3007073402404785
Epoch 530, val loss: 1.1168981790542603
Epoch 540, training loss: 630.8665771484375 = 1.0106345415115356 + 100.0 * 6.298559665679932
Epoch 540, val loss: 1.1022517681121826
Epoch 550, training loss: 630.4522705078125 = 0.9908257722854614 + 100.0 * 6.294614315032959
Epoch 550, val loss: 1.0882295370101929
Epoch 560, training loss: 630.2469482421875 = 0.9714239239692688 + 100.0 * 6.292755126953125
Epoch 560, val loss: 1.0747592449188232
Epoch 570, training loss: 630.21484375 = 0.9522817730903625 + 100.0 * 6.292625904083252
Epoch 570, val loss: 1.061776876449585
Epoch 580, training loss: 629.9476318359375 = 0.933451771736145 + 100.0 * 6.290141582489014
Epoch 580, val loss: 1.049325942993164
Epoch 590, training loss: 630.0892333984375 = 0.9149893522262573 + 100.0 * 6.291742324829102
Epoch 590, val loss: 1.0375274419784546
Epoch 600, training loss: 629.6878051757812 = 0.8969649076461792 + 100.0 * 6.287908554077148
Epoch 600, val loss: 1.0262765884399414
Epoch 610, training loss: 629.6317138671875 = 0.879198431968689 + 100.0 * 6.287525177001953
Epoch 610, val loss: 1.0154958963394165
Epoch 620, training loss: 629.2689819335938 = 0.8618060350418091 + 100.0 * 6.284071445465088
Epoch 620, val loss: 1.0053631067276
Epoch 630, training loss: 629.1843872070312 = 0.844781756401062 + 100.0 * 6.283396244049072
Epoch 630, val loss: 0.9957460761070251
Epoch 640, training loss: 629.0079345703125 = 0.8278738856315613 + 100.0 * 6.281800746917725
Epoch 640, val loss: 0.986409604549408
Epoch 650, training loss: 628.8468627929688 = 0.8113422393798828 + 100.0 * 6.280355453491211
Epoch 650, val loss: 0.9776801466941833
Epoch 660, training loss: 628.71923828125 = 0.7951352000236511 + 100.0 * 6.27924108505249
Epoch 660, val loss: 0.9694477319717407
Epoch 670, training loss: 628.5221557617188 = 0.7791737914085388 + 100.0 * 6.277429580688477
Epoch 670, val loss: 0.9615461826324463
Epoch 680, training loss: 628.583251953125 = 0.763480544090271 + 100.0 * 6.278197765350342
Epoch 680, val loss: 0.9541622400283813
Epoch 690, training loss: 628.2163696289062 = 0.7480316758155823 + 100.0 * 6.274682998657227
Epoch 690, val loss: 0.9471255540847778
Epoch 700, training loss: 628.1783447265625 = 0.7329385876655579 + 100.0 * 6.274454116821289
Epoch 700, val loss: 0.9405906796455383
Epoch 710, training loss: 628.45751953125 = 0.7180996537208557 + 100.0 * 6.2773942947387695
Epoch 710, val loss: 0.9343723654747009
Epoch 720, training loss: 627.984619140625 = 0.703468382358551 + 100.0 * 6.272811412811279
Epoch 720, val loss: 0.9285825490951538
Epoch 730, training loss: 627.74169921875 = 0.6891069412231445 + 100.0 * 6.27052640914917
Epoch 730, val loss: 0.9231576919555664
Epoch 740, training loss: 627.6337890625 = 0.6751943826675415 + 100.0 * 6.269586086273193
Epoch 740, val loss: 0.9183058738708496
Epoch 750, training loss: 628.3096313476562 = 0.6615095138549805 + 100.0 * 6.2764811515808105
Epoch 750, val loss: 0.9138146638870239
Epoch 760, training loss: 627.3780517578125 = 0.6479777097702026 + 100.0 * 6.267301082611084
Epoch 760, val loss: 0.9093793034553528
Epoch 770, training loss: 627.3450317382812 = 0.6348639726638794 + 100.0 * 6.267101287841797
Epoch 770, val loss: 0.9055891036987305
Epoch 780, training loss: 627.6334228515625 = 0.6220176815986633 + 100.0 * 6.270113945007324
Epoch 780, val loss: 0.9021217226982117
Epoch 790, training loss: 627.2145385742188 = 0.6094353199005127 + 100.0 * 6.266051292419434
Epoch 790, val loss: 0.8989819288253784
Epoch 800, training loss: 627.04248046875 = 0.5971693992614746 + 100.0 * 6.264452934265137
Epoch 800, val loss: 0.8962799906730652
Epoch 810, training loss: 627.0125122070312 = 0.5852003693580627 + 100.0 * 6.264273166656494
Epoch 810, val loss: 0.8938037157058716
Epoch 820, training loss: 626.9078979492188 = 0.5734630227088928 + 100.0 * 6.2633442878723145
Epoch 820, val loss: 0.8915546536445618
Epoch 830, training loss: 626.7435302734375 = 0.5619944930076599 + 100.0 * 6.261815071105957
Epoch 830, val loss: 0.8896687030792236
Epoch 840, training loss: 626.5597534179688 = 0.5507872700691223 + 100.0 * 6.260089874267578
Epoch 840, val loss: 0.8880633115768433
Epoch 850, training loss: 626.690673828125 = 0.5399280190467834 + 100.0 * 6.261507511138916
Epoch 850, val loss: 0.8869558572769165
Epoch 860, training loss: 626.5477905273438 = 0.5292941927909851 + 100.0 * 6.2601847648620605
Epoch 860, val loss: 0.8858304619789124
Epoch 870, training loss: 626.3508911132812 = 0.5188596844673157 + 100.0 * 6.258320331573486
Epoch 870, val loss: 0.8850593566894531
Epoch 880, training loss: 626.2767944335938 = 0.508746325969696 + 100.0 * 6.257680892944336
Epoch 880, val loss: 0.8845430016517639
Epoch 890, training loss: 626.25390625 = 0.49889442324638367 + 100.0 * 6.25754976272583
Epoch 890, val loss: 0.8843398094177246
Epoch 900, training loss: 626.60498046875 = 0.4892483949661255 + 100.0 * 6.261157512664795
Epoch 900, val loss: 0.8842856287956238
Epoch 910, training loss: 626.2936401367188 = 0.4796634614467621 + 100.0 * 6.258139610290527
Epoch 910, val loss: 0.884100615978241
Epoch 920, training loss: 626.0910034179688 = 0.47043004631996155 + 100.0 * 6.2562055587768555
Epoch 920, val loss: 0.8844873309135437
Epoch 930, training loss: 625.7946166992188 = 0.4613655209541321 + 100.0 * 6.253332614898682
Epoch 930, val loss: 0.8848230242729187
Epoch 940, training loss: 625.8211059570312 = 0.4525912404060364 + 100.0 * 6.253685474395752
Epoch 940, val loss: 0.8855149149894714
Epoch 950, training loss: 625.9705810546875 = 0.44395875930786133 + 100.0 * 6.255266189575195
Epoch 950, val loss: 0.8863860964775085
Epoch 960, training loss: 625.867431640625 = 0.435474693775177 + 100.0 * 6.254319190979004
Epoch 960, val loss: 0.8874305486679077
Epoch 970, training loss: 625.56103515625 = 0.4271714687347412 + 100.0 * 6.251338481903076
Epoch 970, val loss: 0.8883639574050903
Epoch 980, training loss: 625.5969848632812 = 0.41913142800331116 + 100.0 * 6.251778602600098
Epoch 980, val loss: 0.889714241027832
Epoch 990, training loss: 625.5667114257812 = 0.41116711497306824 + 100.0 * 6.2515549659729
Epoch 990, val loss: 0.89110267162323
Epoch 1000, training loss: 625.4300537109375 = 0.40338265895843506 + 100.0 * 6.2502665519714355
Epoch 1000, val loss: 0.8927671313285828
Epoch 1010, training loss: 625.5534057617188 = 0.39573246240615845 + 100.0 * 6.2515764236450195
Epoch 1010, val loss: 0.8943744897842407
Epoch 1020, training loss: 625.3853149414062 = 0.3882528245449066 + 100.0 * 6.249970436096191
Epoch 1020, val loss: 0.8960485458374023
Epoch 1030, training loss: 625.1660766601562 = 0.3808969259262085 + 100.0 * 6.247851848602295
Epoch 1030, val loss: 0.8980215787887573
Epoch 1040, training loss: 625.1570434570312 = 0.3737063407897949 + 100.0 * 6.247833251953125
Epoch 1040, val loss: 0.8999689221382141
Epoch 1050, training loss: 625.18603515625 = 0.3666380047798157 + 100.0 * 6.248193740844727
Epoch 1050, val loss: 0.902084469795227
Epoch 1060, training loss: 624.9994506835938 = 0.35968950390815735 + 100.0 * 6.246397495269775
Epoch 1060, val loss: 0.9043874144554138
Epoch 1070, training loss: 624.9722900390625 = 0.35288622975349426 + 100.0 * 6.246193885803223
Epoch 1070, val loss: 0.9065794348716736
Epoch 1080, training loss: 625.22021484375 = 0.3461590111255646 + 100.0 * 6.2487406730651855
Epoch 1080, val loss: 0.9089226126670837
Epoch 1090, training loss: 624.98046875 = 0.33952751755714417 + 100.0 * 6.2464094161987305
Epoch 1090, val loss: 0.9113051295280457
Epoch 1100, training loss: 624.9387817382812 = 0.33302024006843567 + 100.0 * 6.246057510375977
Epoch 1100, val loss: 0.9137796759605408
Epoch 1110, training loss: 624.6636352539062 = 0.32662233710289 + 100.0 * 6.243370056152344
Epoch 1110, val loss: 0.9160945415496826
Epoch 1120, training loss: 624.5348510742188 = 0.3204106390476227 + 100.0 * 6.2421441078186035
Epoch 1120, val loss: 0.9188478589057922
Epoch 1130, training loss: 625.1397094726562 = 0.3142916262149811 + 100.0 * 6.248254299163818
Epoch 1130, val loss: 0.921478271484375
Epoch 1140, training loss: 624.8992919921875 = 0.30815649032592773 + 100.0 * 6.245911121368408
Epoch 1140, val loss: 0.9242837429046631
Epoch 1150, training loss: 624.634033203125 = 0.3021309971809387 + 100.0 * 6.243319034576416
Epoch 1150, val loss: 0.9268237352371216
Epoch 1160, training loss: 624.3195190429688 = 0.2962244749069214 + 100.0 * 6.240233421325684
Epoch 1160, val loss: 0.929474949836731
Epoch 1170, training loss: 624.2608032226562 = 0.29050326347351074 + 100.0 * 6.2397027015686035
Epoch 1170, val loss: 0.932383120059967
Epoch 1180, training loss: 624.6828002929688 = 0.28485238552093506 + 100.0 * 6.243979454040527
Epoch 1180, val loss: 0.9351165890693665
Epoch 1190, training loss: 624.468994140625 = 0.2792084515094757 + 100.0 * 6.2418975830078125
Epoch 1190, val loss: 0.938212513923645
Epoch 1200, training loss: 624.2550659179688 = 0.2736937403678894 + 100.0 * 6.239813804626465
Epoch 1200, val loss: 0.9409787058830261
Epoch 1210, training loss: 624.4149780273438 = 0.2682661712169647 + 100.0 * 6.241466999053955
Epoch 1210, val loss: 0.9441386461257935
Epoch 1220, training loss: 624.1962890625 = 0.2629019320011139 + 100.0 * 6.2393341064453125
Epoch 1220, val loss: 0.9466890692710876
Epoch 1230, training loss: 624.1940307617188 = 0.25768163800239563 + 100.0 * 6.239363670349121
Epoch 1230, val loss: 0.9499604105949402
Epoch 1240, training loss: 623.9514770507812 = 0.25252047181129456 + 100.0 * 6.236989498138428
Epoch 1240, val loss: 0.9529845118522644
Epoch 1250, training loss: 624.0145874023438 = 0.2474832534790039 + 100.0 * 6.2376708984375
Epoch 1250, val loss: 0.9560757279396057
Epoch 1260, training loss: 624.044189453125 = 0.24252045154571533 + 100.0 * 6.2380170822143555
Epoch 1260, val loss: 0.959012508392334
Epoch 1270, training loss: 624.2098388671875 = 0.2376125454902649 + 100.0 * 6.23972225189209
Epoch 1270, val loss: 0.9621868133544922
Epoch 1280, training loss: 623.98486328125 = 0.2327086329460144 + 100.0 * 6.237521171569824
Epoch 1280, val loss: 0.9650668501853943
Epoch 1290, training loss: 623.7743530273438 = 0.22797216475009918 + 100.0 * 6.235464096069336
Epoch 1290, val loss: 0.9684314727783203
Epoch 1300, training loss: 623.6324462890625 = 0.22335995733737946 + 100.0 * 6.234090805053711
Epoch 1300, val loss: 0.9716147184371948
Epoch 1310, training loss: 623.5664672851562 = 0.21885621547698975 + 100.0 * 6.233476161956787
Epoch 1310, val loss: 0.975023627281189
Epoch 1320, training loss: 624.3526611328125 = 0.21445630490779877 + 100.0 * 6.241381645202637
Epoch 1320, val loss: 0.9784529209136963
Epoch 1330, training loss: 624.3663940429688 = 0.2099645435810089 + 100.0 * 6.2415642738342285
Epoch 1330, val loss: 0.9813595414161682
Epoch 1340, training loss: 623.49951171875 = 0.20556220412254333 + 100.0 * 6.232939720153809
Epoch 1340, val loss: 0.9843622446060181
Epoch 1350, training loss: 623.42724609375 = 0.2013603001832962 + 100.0 * 6.2322587966918945
Epoch 1350, val loss: 0.9879314303398132
Epoch 1360, training loss: 623.353759765625 = 0.19726578891277313 + 100.0 * 6.231564998626709
Epoch 1360, val loss: 0.9912674427032471
Epoch 1370, training loss: 624.0008544921875 = 0.1932651549577713 + 100.0 * 6.238076210021973
Epoch 1370, val loss: 0.9948441386222839
Epoch 1380, training loss: 623.7379760742188 = 0.18921871483325958 + 100.0 * 6.235487937927246
Epoch 1380, val loss: 0.9978488683700562
Epoch 1390, training loss: 623.3929443359375 = 0.185293510556221 + 100.0 * 6.232076168060303
Epoch 1390, val loss: 1.001090407371521
Epoch 1400, training loss: 623.1591796875 = 0.18149001896381378 + 100.0 * 6.229776382446289
Epoch 1400, val loss: 1.0046041011810303
Epoch 1410, training loss: 623.4779663085938 = 0.17778535187244415 + 100.0 * 6.233001708984375
Epoch 1410, val loss: 1.0079041719436646
Epoch 1420, training loss: 623.2442016601562 = 0.17409034073352814 + 100.0 * 6.230701446533203
Epoch 1420, val loss: 1.0115069150924683
Epoch 1430, training loss: 623.137451171875 = 0.1704624742269516 + 100.0 * 6.22967004776001
Epoch 1430, val loss: 1.014836072921753
Epoch 1440, training loss: 623.1927490234375 = 0.16698162257671356 + 100.0 * 6.230257511138916
Epoch 1440, val loss: 1.0184462070465088
Epoch 1450, training loss: 623.1463623046875 = 0.16352924704551697 + 100.0 * 6.229828357696533
Epoch 1450, val loss: 1.0217500925064087
Epoch 1460, training loss: 623.2796630859375 = 0.1601647287607193 + 100.0 * 6.231194972991943
Epoch 1460, val loss: 1.0249831676483154
Epoch 1470, training loss: 623.06689453125 = 0.1568525731563568 + 100.0 * 6.229100227355957
Epoch 1470, val loss: 1.028795599937439
Epoch 1480, training loss: 622.9542846679688 = 0.1535990983247757 + 100.0 * 6.228006362915039
Epoch 1480, val loss: 1.0319701433181763
Epoch 1490, training loss: 622.8710327148438 = 0.15045566856861115 + 100.0 * 6.227205753326416
Epoch 1490, val loss: 1.0354557037353516
Epoch 1500, training loss: 622.8634643554688 = 0.14739269018173218 + 100.0 * 6.227160930633545
Epoch 1500, val loss: 1.0388883352279663
Epoch 1510, training loss: 623.1468505859375 = 0.1443951278924942 + 100.0 * 6.230024337768555
Epoch 1510, val loss: 1.0423030853271484
Epoch 1520, training loss: 623.126708984375 = 0.14142081141471863 + 100.0 * 6.229852676391602
Epoch 1520, val loss: 1.0461174249649048
Epoch 1530, training loss: 622.8807373046875 = 0.13849300146102905 + 100.0 * 6.22742223739624
Epoch 1530, val loss: 1.0493214130401611
Epoch 1540, training loss: 622.8297119140625 = 0.1356576383113861 + 100.0 * 6.226940155029297
Epoch 1540, val loss: 1.0529539585113525
Epoch 1550, training loss: 622.927001953125 = 0.13288864493370056 + 100.0 * 6.227941513061523
Epoch 1550, val loss: 1.0563865900039673
Epoch 1560, training loss: 622.8231811523438 = 0.1301925629377365 + 100.0 * 6.226929664611816
Epoch 1560, val loss: 1.0599702596664429
Epoch 1570, training loss: 622.6284790039062 = 0.1275421679019928 + 100.0 * 6.225008964538574
Epoch 1570, val loss: 1.0632930994033813
Epoch 1580, training loss: 622.6050415039062 = 0.12497437745332718 + 100.0 * 6.2248005867004395
Epoch 1580, val loss: 1.066827416419983
Epoch 1590, training loss: 622.7513427734375 = 0.12246179580688477 + 100.0 * 6.226288795471191
Epoch 1590, val loss: 1.0700384378433228
Epoch 1600, training loss: 622.7598876953125 = 0.11996150761842728 + 100.0 * 6.2263994216918945
Epoch 1600, val loss: 1.0738518238067627
Epoch 1610, training loss: 622.5025634765625 = 0.11751709133386612 + 100.0 * 6.223850727081299
Epoch 1610, val loss: 1.0773552656173706
Epoch 1620, training loss: 622.424072265625 = 0.11517035961151123 + 100.0 * 6.22308874130249
Epoch 1620, val loss: 1.080723524093628
Epoch 1630, training loss: 622.35546875 = 0.11289511620998383 + 100.0 * 6.22242546081543
Epoch 1630, val loss: 1.0846123695373535
Epoch 1640, training loss: 623.2151489257812 = 0.1106896698474884 + 100.0 * 6.231044292449951
Epoch 1640, val loss: 1.0879968404769897
Epoch 1650, training loss: 623.0919189453125 = 0.10840709507465363 + 100.0 * 6.229835510253906
Epoch 1650, val loss: 1.0913331508636475
Epoch 1660, training loss: 622.3861083984375 = 0.10618998855352402 + 100.0 * 6.222798824310303
Epoch 1660, val loss: 1.0948131084442139
Epoch 1670, training loss: 622.189208984375 = 0.10408730804920197 + 100.0 * 6.220851421356201
Epoch 1670, val loss: 1.098516583442688
Epoch 1680, training loss: 622.5848999023438 = 0.102068692445755 + 100.0 * 6.224828720092773
Epoch 1680, val loss: 1.1021636724472046
Epoch 1690, training loss: 622.236083984375 = 0.1000271663069725 + 100.0 * 6.221360683441162
Epoch 1690, val loss: 1.1055618524551392
Epoch 1700, training loss: 622.2608642578125 = 0.09804925322532654 + 100.0 * 6.221628189086914
Epoch 1700, val loss: 1.1090670824050903
Epoch 1710, training loss: 622.3130493164062 = 0.09613453596830368 + 100.0 * 6.222169399261475
Epoch 1710, val loss: 1.112579345703125
Epoch 1720, training loss: 622.1049194335938 = 0.0942537933588028 + 100.0 * 6.220107078552246
Epoch 1720, val loss: 1.1162164211273193
Epoch 1730, training loss: 622.0628051757812 = 0.09244267642498016 + 100.0 * 6.219703197479248
Epoch 1730, val loss: 1.1198880672454834
Epoch 1740, training loss: 622.6428833007812 = 0.0906706377863884 + 100.0 * 6.225521564483643
Epoch 1740, val loss: 1.123442530632019
Epoch 1750, training loss: 622.4427490234375 = 0.08886802941560745 + 100.0 * 6.223538875579834
Epoch 1750, val loss: 1.1271177530288696
Epoch 1760, training loss: 622.6060791015625 = 0.08713161200284958 + 100.0 * 6.225189208984375
Epoch 1760, val loss: 1.1303261518478394
Epoch 1770, training loss: 622.0205078125 = 0.08542076498270035 + 100.0 * 6.219350814819336
Epoch 1770, val loss: 1.1340121030807495
Epoch 1780, training loss: 621.9609375 = 0.08379027992486954 + 100.0 * 6.218771457672119
Epoch 1780, val loss: 1.1378934383392334
Epoch 1790, training loss: 621.9451293945312 = 0.08221690356731415 + 100.0 * 6.218628883361816
Epoch 1790, val loss: 1.1413582563400269
Epoch 1800, training loss: 622.73828125 = 0.08066004514694214 + 100.0 * 6.22657585144043
Epoch 1800, val loss: 1.144712209701538
Epoch 1810, training loss: 622.0964965820312 = 0.07907896488904953 + 100.0 * 6.2201738357543945
Epoch 1810, val loss: 1.1485399007797241
Epoch 1820, training loss: 621.8591918945312 = 0.0775865688920021 + 100.0 * 6.217816352844238
Epoch 1820, val loss: 1.152310848236084
Epoch 1830, training loss: 621.8152465820312 = 0.07614485919475555 + 100.0 * 6.217391014099121
Epoch 1830, val loss: 1.1561211347579956
Epoch 1840, training loss: 622.0255126953125 = 0.07475124299526215 + 100.0 * 6.219507694244385
Epoch 1840, val loss: 1.1599041223526
Epoch 1850, training loss: 621.826171875 = 0.07332251220941544 + 100.0 * 6.217528820037842
Epoch 1850, val loss: 1.1633135080337524
Epoch 1860, training loss: 622.307373046875 = 0.07194578647613525 + 100.0 * 6.222353935241699
Epoch 1860, val loss: 1.1669327020645142
Epoch 1870, training loss: 621.7520751953125 = 0.07056061178445816 + 100.0 * 6.21681547164917
Epoch 1870, val loss: 1.1703022718429565
Epoch 1880, training loss: 621.5975952148438 = 0.06925198435783386 + 100.0 * 6.215283393859863
Epoch 1880, val loss: 1.1741207838058472
Epoch 1890, training loss: 621.5989379882812 = 0.06799698621034622 + 100.0 * 6.2153096199035645
Epoch 1890, val loss: 1.177725911140442
Epoch 1900, training loss: 621.88720703125 = 0.06677736341953278 + 100.0 * 6.218204498291016
Epoch 1900, val loss: 1.1812000274658203
Epoch 1910, training loss: 621.6345825195312 = 0.06552340090274811 + 100.0 * 6.2156901359558105
Epoch 1910, val loss: 1.1848101615905762
Epoch 1920, training loss: 621.6224365234375 = 0.06430698186159134 + 100.0 * 6.215580940246582
Epoch 1920, val loss: 1.1882503032684326
Epoch 1930, training loss: 621.83447265625 = 0.06314007937908173 + 100.0 * 6.217713356018066
Epoch 1930, val loss: 1.1922125816345215
Epoch 1940, training loss: 621.72412109375 = 0.06199360266327858 + 100.0 * 6.216620922088623
Epoch 1940, val loss: 1.1958534717559814
Epoch 1950, training loss: 621.66552734375 = 0.060860179364681244 + 100.0 * 6.2160468101501465
Epoch 1950, val loss: 1.1996263265609741
Epoch 1960, training loss: 621.4938354492188 = 0.05977262556552887 + 100.0 * 6.214340686798096
Epoch 1960, val loss: 1.203041672706604
Epoch 1970, training loss: 621.4085083007812 = 0.0587163120508194 + 100.0 * 6.213497638702393
Epoch 1970, val loss: 1.2067269086837769
Epoch 1980, training loss: 622.0201416015625 = 0.05769917368888855 + 100.0 * 6.2196245193481445
Epoch 1980, val loss: 1.2101843357086182
Epoch 1990, training loss: 621.602294921875 = 0.056640833616256714 + 100.0 * 6.215456485748291
Epoch 1990, val loss: 1.2137930393218994
Epoch 2000, training loss: 621.7897338867188 = 0.05563448742032051 + 100.0 * 6.217340469360352
Epoch 2000, val loss: 1.2175215482711792
Epoch 2010, training loss: 621.7012939453125 = 0.054635919630527496 + 100.0 * 6.216466426849365
Epoch 2010, val loss: 1.2213331460952759
Epoch 2020, training loss: 621.4594116210938 = 0.05367829650640488 + 100.0 * 6.214057445526123
Epoch 2020, val loss: 1.224519968032837
Epoch 2030, training loss: 621.6390380859375 = 0.05274416133761406 + 100.0 * 6.215863227844238
Epoch 2030, val loss: 1.228371024131775
Epoch 2040, training loss: 621.2919311523438 = 0.05181201919913292 + 100.0 * 6.212401390075684
Epoch 2040, val loss: 1.2318998575210571
Epoch 2050, training loss: 621.3538208007812 = 0.05092403292655945 + 100.0 * 6.213028907775879
Epoch 2050, val loss: 1.235228419303894
Epoch 2060, training loss: 621.4447021484375 = 0.050061214715242386 + 100.0 * 6.213946342468262
Epoch 2060, val loss: 1.2389562129974365
Epoch 2070, training loss: 621.5126342773438 = 0.049200281500816345 + 100.0 * 6.214634418487549
Epoch 2070, val loss: 1.242617130279541
Epoch 2080, training loss: 621.3212280273438 = 0.0483408197760582 + 100.0 * 6.212728977203369
Epoch 2080, val loss: 1.245758295059204
Epoch 2090, training loss: 621.2529907226562 = 0.04752025753259659 + 100.0 * 6.21205472946167
Epoch 2090, val loss: 1.2493821382522583
Epoch 2100, training loss: 621.6251831054688 = 0.04673279821872711 + 100.0 * 6.215784072875977
Epoch 2100, val loss: 1.252907156944275
Epoch 2110, training loss: 621.8049926757812 = 0.04593256115913391 + 100.0 * 6.21759033203125
Epoch 2110, val loss: 1.2562400102615356
Epoch 2120, training loss: 621.3445434570312 = 0.04511850327253342 + 100.0 * 6.212994575500488
Epoch 2120, val loss: 1.259368658065796
Epoch 2130, training loss: 621.1580200195312 = 0.044362638145685196 + 100.0 * 6.211136341094971
Epoch 2130, val loss: 1.2632967233657837
Epoch 2140, training loss: 621.0669555664062 = 0.043637100607156754 + 100.0 * 6.210233211517334
Epoch 2140, val loss: 1.2665867805480957
Epoch 2150, training loss: 621.466552734375 = 0.04293210059404373 + 100.0 * 6.214236259460449
Epoch 2150, val loss: 1.2698683738708496
Epoch 2160, training loss: 621.2564697265625 = 0.042209841310977936 + 100.0 * 6.212142467498779
Epoch 2160, val loss: 1.2733186483383179
Epoch 2170, training loss: 621.2140502929688 = 0.041492562741041183 + 100.0 * 6.21172571182251
Epoch 2170, val loss: 1.2761436700820923
Epoch 2180, training loss: 621.0338745117188 = 0.04081400856375694 + 100.0 * 6.209930419921875
Epoch 2180, val loss: 1.2800397872924805
Epoch 2190, training loss: 621.046142578125 = 0.04016382247209549 + 100.0 * 6.210060119628906
Epoch 2190, val loss: 1.2832478284835815
Epoch 2200, training loss: 621.4046020507812 = 0.039536263793706894 + 100.0 * 6.213650703430176
Epoch 2200, val loss: 1.286341667175293
Epoch 2210, training loss: 621.2302856445312 = 0.038873929530382156 + 100.0 * 6.2119140625
Epoch 2210, val loss: 1.2897032499313354
Epoch 2220, training loss: 620.924560546875 = 0.03823850676417351 + 100.0 * 6.208863258361816
Epoch 2220, val loss: 1.2933460474014282
Epoch 2230, training loss: 620.8788452148438 = 0.037635914981365204 + 100.0 * 6.208411693572998
Epoch 2230, val loss: 1.2964625358581543
Epoch 2240, training loss: 621.199462890625 = 0.0370631143450737 + 100.0 * 6.2116241455078125
Epoch 2240, val loss: 1.2999179363250732
Epoch 2250, training loss: 621.01708984375 = 0.03645426034927368 + 100.0 * 6.209806442260742
Epoch 2250, val loss: 1.303015112876892
Epoch 2260, training loss: 620.8262939453125 = 0.03585951030254364 + 100.0 * 6.207903861999512
Epoch 2260, val loss: 1.306060791015625
Epoch 2270, training loss: 620.8335571289062 = 0.03530706837773323 + 100.0 * 6.207983016967773
Epoch 2270, val loss: 1.3092986345291138
Epoch 2280, training loss: 621.3734130859375 = 0.03477315977215767 + 100.0 * 6.213386535644531
Epoch 2280, val loss: 1.3127843141555786
Epoch 2290, training loss: 620.818359375 = 0.03420969471335411 + 100.0 * 6.207841873168945
Epoch 2290, val loss: 1.315678596496582
Epoch 2300, training loss: 620.7548828125 = 0.03368242084980011 + 100.0 * 6.207211971282959
Epoch 2300, val loss: 1.3189945220947266
Epoch 2310, training loss: 620.8299560546875 = 0.03318077698349953 + 100.0 * 6.207967281341553
Epoch 2310, val loss: 1.3221352100372314
Epoch 2320, training loss: 621.2522583007812 = 0.03268446773290634 + 100.0 * 6.21219539642334
Epoch 2320, val loss: 1.3251092433929443
Epoch 2330, training loss: 621.0466918945312 = 0.03216946870088577 + 100.0 * 6.210145473480225
Epoch 2330, val loss: 1.3280402421951294
Epoch 2340, training loss: 620.8436889648438 = 0.03167817369103432 + 100.0 * 6.208120346069336
Epoch 2340, val loss: 1.331146240234375
Epoch 2350, training loss: 621.0391235351562 = 0.031203757971525192 + 100.0 * 6.210079193115234
Epoch 2350, val loss: 1.3342622518539429
Epoch 2360, training loss: 620.744384765625 = 0.03072538785636425 + 100.0 * 6.207136631011963
Epoch 2360, val loss: 1.3374292850494385
Epoch 2370, training loss: 621.0009765625 = 0.030273158103227615 + 100.0 * 6.209706783294678
Epoch 2370, val loss: 1.3408589363098145
Epoch 2380, training loss: 620.8463134765625 = 0.02982083521783352 + 100.0 * 6.208164691925049
Epoch 2380, val loss: 1.3435860872268677
Epoch 2390, training loss: 620.6401977539062 = 0.029380733147263527 + 100.0 * 6.206108093261719
Epoch 2390, val loss: 1.346727728843689
Epoch 2400, training loss: 620.720947265625 = 0.028960179537534714 + 100.0 * 6.206920146942139
Epoch 2400, val loss: 1.349729061126709
Epoch 2410, training loss: 621.21435546875 = 0.028548307716846466 + 100.0 * 6.211857795715332
Epoch 2410, val loss: 1.3528225421905518
Epoch 2420, training loss: 620.7339477539062 = 0.028112072497606277 + 100.0 * 6.207057952880859
Epoch 2420, val loss: 1.3556547164916992
Epoch 2430, training loss: 620.5407104492188 = 0.02770405262708664 + 100.0 * 6.205130100250244
Epoch 2430, val loss: 1.3586021661758423
Epoch 2440, training loss: 620.588623046875 = 0.02731066383421421 + 100.0 * 6.205613136291504
Epoch 2440, val loss: 1.361609935760498
Epoch 2450, training loss: 620.9382934570312 = 0.02693166770040989 + 100.0 * 6.209114074707031
Epoch 2450, val loss: 1.3643993139266968
Epoch 2460, training loss: 620.66650390625 = 0.02654525451362133 + 100.0 * 6.206399440765381
Epoch 2460, val loss: 1.3673925399780273
Epoch 2470, training loss: 620.5088500976562 = 0.026163935661315918 + 100.0 * 6.204826831817627
Epoch 2470, val loss: 1.3704187870025635
Epoch 2480, training loss: 620.69921875 = 0.025805572047829628 + 100.0 * 6.2067341804504395
Epoch 2480, val loss: 1.3733484745025635
Epoch 2490, training loss: 620.5689086914062 = 0.025439925491809845 + 100.0 * 6.205434322357178
Epoch 2490, val loss: 1.3762696981430054
Epoch 2500, training loss: 621.0653076171875 = 0.025085324421525 + 100.0 * 6.210402011871338
Epoch 2500, val loss: 1.3791165351867676
Epoch 2510, training loss: 620.66748046875 = 0.024723954498767853 + 100.0 * 6.206427574157715
Epoch 2510, val loss: 1.3818435668945312
Epoch 2520, training loss: 620.502197265625 = 0.02437763288617134 + 100.0 * 6.20477819442749
Epoch 2520, val loss: 1.3848624229431152
Epoch 2530, training loss: 620.381103515625 = 0.024051692336797714 + 100.0 * 6.203570365905762
Epoch 2530, val loss: 1.387842059135437
Epoch 2540, training loss: 620.4119873046875 = 0.023736678063869476 + 100.0 * 6.203882217407227
Epoch 2540, val loss: 1.390820860862732
Epoch 2550, training loss: 620.9720458984375 = 0.0234286617487669 + 100.0 * 6.20948600769043
Epoch 2550, val loss: 1.3938759565353394
Epoch 2560, training loss: 620.8099975585938 = 0.02309824898838997 + 100.0 * 6.207869052886963
Epoch 2560, val loss: 1.3964248895645142
Epoch 2570, training loss: 620.4686889648438 = 0.02277955412864685 + 100.0 * 6.204459190368652
Epoch 2570, val loss: 1.3990074396133423
Epoch 2580, training loss: 620.3753662109375 = 0.022482404485344887 + 100.0 * 6.203528881072998
Epoch 2580, val loss: 1.4020718336105347
Epoch 2590, training loss: 620.4895629882812 = 0.022191915661096573 + 100.0 * 6.2046732902526855
Epoch 2590, val loss: 1.404910922050476
Epoch 2600, training loss: 620.3927001953125 = 0.021901341155171394 + 100.0 * 6.203707695007324
Epoch 2600, val loss: 1.4075732231140137
Epoch 2610, training loss: 620.7550048828125 = 0.021629109978675842 + 100.0 * 6.207334041595459
Epoch 2610, val loss: 1.4104112386703491
Epoch 2620, training loss: 620.48193359375 = 0.02132108435034752 + 100.0 * 6.204606533050537
Epoch 2620, val loss: 1.413265347480774
Epoch 2630, training loss: 620.2998657226562 = 0.02103976160287857 + 100.0 * 6.20278787612915
Epoch 2630, val loss: 1.415657877922058
Epoch 2640, training loss: 620.216064453125 = 0.020768150687217712 + 100.0 * 6.201953411102295
Epoch 2640, val loss: 1.4187101125717163
Epoch 2650, training loss: 620.1748657226562 = 0.02051662467420101 + 100.0 * 6.201543807983398
Epoch 2650, val loss: 1.4214825630187988
Epoch 2660, training loss: 620.4752807617188 = 0.020268768072128296 + 100.0 * 6.204550266265869
Epoch 2660, val loss: 1.4241729974746704
Epoch 2670, training loss: 620.3495483398438 = 0.02000202052295208 + 100.0 * 6.2032952308654785
Epoch 2670, val loss: 1.4266067743301392
Epoch 2680, training loss: 620.2622680664062 = 0.019740527495741844 + 100.0 * 6.202425479888916
Epoch 2680, val loss: 1.4295223951339722
Epoch 2690, training loss: 620.3671875 = 0.019497530534863472 + 100.0 * 6.203476905822754
Epoch 2690, val loss: 1.4324228763580322
Epoch 2700, training loss: 620.158203125 = 0.019254500046372414 + 100.0 * 6.201389789581299
Epoch 2700, val loss: 1.4348515272140503
Epoch 2710, training loss: 620.339111328125 = 0.01902506873011589 + 100.0 * 6.203200817108154
Epoch 2710, val loss: 1.4375441074371338
Epoch 2720, training loss: 620.3394775390625 = 0.01879565790295601 + 100.0 * 6.203207015991211
Epoch 2720, val loss: 1.4401580095291138
Epoch 2730, training loss: 620.192138671875 = 0.01856403425335884 + 100.0 * 6.201735973358154
Epoch 2730, val loss: 1.4426555633544922
Epoch 2740, training loss: 620.14013671875 = 0.01834097132086754 + 100.0 * 6.201218128204346
Epoch 2740, val loss: 1.4451435804367065
Epoch 2750, training loss: 620.4376220703125 = 0.01813191920518875 + 100.0 * 6.204195022583008
Epoch 2750, val loss: 1.4474902153015137
Epoch 2760, training loss: 620.4242553710938 = 0.017911238595843315 + 100.0 * 6.204063415527344
Epoch 2760, val loss: 1.450082778930664
Epoch 2770, training loss: 620.3410034179688 = 0.017684178426861763 + 100.0 * 6.203233242034912
Epoch 2770, val loss: 1.4526149034500122
Epoch 2780, training loss: 620.3080444335938 = 0.01747162453830242 + 100.0 * 6.202906131744385
Epoch 2780, val loss: 1.4551118612289429
Epoch 2790, training loss: 620.269287109375 = 0.01726083643734455 + 100.0 * 6.20251989364624
Epoch 2790, val loss: 1.4579198360443115
Epoch 2800, training loss: 620.0733642578125 = 0.017062008380889893 + 100.0 * 6.200562953948975
Epoch 2800, val loss: 1.460444688796997
Epoch 2810, training loss: 620.0045776367188 = 0.016866762191057205 + 100.0 * 6.1998772621154785
Epoch 2810, val loss: 1.463011622428894
Epoch 2820, training loss: 620.2620239257812 = 0.01668332889676094 + 100.0 * 6.20245361328125
Epoch 2820, val loss: 1.4654185771942139
Epoch 2830, training loss: 620.1835327148438 = 0.01648245006799698 + 100.0 * 6.2016706466674805
Epoch 2830, val loss: 1.4677613973617554
Epoch 2840, training loss: 620.1489868164062 = 0.016289392486214638 + 100.0 * 6.201326847076416
Epoch 2840, val loss: 1.470018744468689
Epoch 2850, training loss: 620.094482421875 = 0.01610609143972397 + 100.0 * 6.200783729553223
Epoch 2850, val loss: 1.4724342823028564
Epoch 2860, training loss: 620.48974609375 = 0.015927312895655632 + 100.0 * 6.204738140106201
Epoch 2860, val loss: 1.474793553352356
Epoch 2870, training loss: 620.320556640625 = 0.01573929749429226 + 100.0 * 6.203048229217529
Epoch 2870, val loss: 1.477704644203186
Epoch 2880, training loss: 619.9876708984375 = 0.015559026971459389 + 100.0 * 6.199721336364746
Epoch 2880, val loss: 1.4797440767288208
Epoch 2890, training loss: 619.8912353515625 = 0.015391099266707897 + 100.0 * 6.198758602142334
Epoch 2890, val loss: 1.482097864151001
Epoch 2900, training loss: 619.9175415039062 = 0.015228251926600933 + 100.0 * 6.199023723602295
Epoch 2900, val loss: 1.4845969676971436
Epoch 2910, training loss: 620.290771484375 = 0.01507174875587225 + 100.0 * 6.202756881713867
Epoch 2910, val loss: 1.486759066581726
Epoch 2920, training loss: 620.4495849609375 = 0.014898584224283695 + 100.0 * 6.204346656799316
Epoch 2920, val loss: 1.48931884765625
Epoch 2930, training loss: 620.04736328125 = 0.014718041755259037 + 100.0 * 6.200326442718506
Epoch 2930, val loss: 1.4911692142486572
Epoch 2940, training loss: 619.9087524414062 = 0.014559922739863396 + 100.0 * 6.198941707611084
Epoch 2940, val loss: 1.4939099550247192
Epoch 2950, training loss: 619.910400390625 = 0.014407888986170292 + 100.0 * 6.198959827423096
Epoch 2950, val loss: 1.496309757232666
Epoch 2960, training loss: 620.1636352539062 = 0.014260086230933666 + 100.0 * 6.201493263244629
Epoch 2960, val loss: 1.498663067817688
Epoch 2970, training loss: 619.8850708007812 = 0.014105364680290222 + 100.0 * 6.198709964752197
Epoch 2970, val loss: 1.50062096118927
Epoch 2980, training loss: 620.2371826171875 = 0.013959908857941628 + 100.0 * 6.2022318840026855
Epoch 2980, val loss: 1.5028151273727417
Epoch 2990, training loss: 619.8328857421875 = 0.013809624128043652 + 100.0 * 6.198190689086914
Epoch 2990, val loss: 1.5051544904708862
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8386926726410122
The final CL Acc:0.70741, 0.00605, The final GNN Acc:0.84098, 0.00163
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9448])
updated graph: torch.Size([2, 10486])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6161499023438 = 1.9312489032745361 + 100.0 * 8.596848487854004
Epoch 0, val loss: 1.9307942390441895
Epoch 10, training loss: 861.5386962890625 = 1.9229496717453003 + 100.0 * 8.59615707397461
Epoch 10, val loss: 1.9219441413879395
Epoch 20, training loss: 861.054443359375 = 1.9127103090286255 + 100.0 * 8.59141731262207
Epoch 20, val loss: 1.9107469320297241
Epoch 30, training loss: 857.5076293945312 = 1.8995219469070435 + 100.0 * 8.55608081817627
Epoch 30, val loss: 1.8961924314498901
Epoch 40, training loss: 831.1578369140625 = 1.884659767150879 + 100.0 * 8.292732238769531
Epoch 40, val loss: 1.880062460899353
Epoch 50, training loss: 773.1735229492188 = 1.8689649105072021 + 100.0 * 7.713045597076416
Epoch 50, val loss: 1.863560438156128
Epoch 60, training loss: 748.9296264648438 = 1.8577362298965454 + 100.0 * 7.470718860626221
Epoch 60, val loss: 1.8525340557098389
Epoch 70, training loss: 721.84375 = 1.8489171266555786 + 100.0 * 7.199947834014893
Epoch 70, val loss: 1.8436425924301147
Epoch 80, training loss: 701.1810302734375 = 1.8400921821594238 + 100.0 * 6.993409633636475
Epoch 80, val loss: 1.8348090648651123
Epoch 90, training loss: 688.651611328125 = 1.8326325416564941 + 100.0 * 6.868189811706543
Epoch 90, val loss: 1.8267710208892822
Epoch 100, training loss: 681.427490234375 = 1.825141429901123 + 100.0 * 6.796023845672607
Epoch 100, val loss: 1.8182570934295654
Epoch 110, training loss: 675.5958862304688 = 1.8173600435256958 + 100.0 * 6.7377848625183105
Epoch 110, val loss: 1.809933066368103
Epoch 120, training loss: 670.765380859375 = 1.8100428581237793 + 100.0 * 6.689553260803223
Epoch 120, val loss: 1.8022936582565308
Epoch 130, training loss: 666.8087158203125 = 1.802880883216858 + 100.0 * 6.650058746337891
Epoch 130, val loss: 1.794952630996704
Epoch 140, training loss: 663.7886962890625 = 1.7955689430236816 + 100.0 * 6.619930744171143
Epoch 140, val loss: 1.7876487970352173
Epoch 150, training loss: 661.117431640625 = 1.7878960371017456 + 100.0 * 6.593295574188232
Epoch 150, val loss: 1.7801017761230469
Epoch 160, training loss: 658.8057250976562 = 1.7796670198440552 + 100.0 * 6.570260524749756
Epoch 160, val loss: 1.7722055912017822
Epoch 170, training loss: 657.0 = 1.7707712650299072 + 100.0 * 6.552292346954346
Epoch 170, val loss: 1.763945460319519
Epoch 180, training loss: 655.1580810546875 = 1.7612398862838745 + 100.0 * 6.533968448638916
Epoch 180, val loss: 1.755143642425537
Epoch 190, training loss: 654.055419921875 = 1.7509233951568604 + 100.0 * 6.523045063018799
Epoch 190, val loss: 1.7459309101104736
Epoch 200, training loss: 652.2764282226562 = 1.7396926879882812 + 100.0 * 6.505367279052734
Epoch 200, val loss: 1.735883116722107
Epoch 210, training loss: 650.7951049804688 = 1.7275062799453735 + 100.0 * 6.490676403045654
Epoch 210, val loss: 1.7251200675964355
Epoch 220, training loss: 649.653076171875 = 1.7142280340194702 + 100.0 * 6.479388236999512
Epoch 220, val loss: 1.7134476900100708
Epoch 230, training loss: 648.4497680664062 = 1.6996817588806152 + 100.0 * 6.467500686645508
Epoch 230, val loss: 1.7009328603744507
Epoch 240, training loss: 648.1455688476562 = 1.683835744857788 + 100.0 * 6.4646172523498535
Epoch 240, val loss: 1.6873118877410889
Epoch 250, training loss: 646.5767211914062 = 1.6664793491363525 + 100.0 * 6.449102401733398
Epoch 250, val loss: 1.672356128692627
Epoch 260, training loss: 645.5783081054688 = 1.64768385887146 + 100.0 * 6.439305782318115
Epoch 260, val loss: 1.6562968492507935
Epoch 270, training loss: 644.8471069335938 = 1.6273776292800903 + 100.0 * 6.432197093963623
Epoch 270, val loss: 1.6390827894210815
Epoch 280, training loss: 644.3868408203125 = 1.6055176258087158 + 100.0 * 6.4278130531311035
Epoch 280, val loss: 1.6204463243484497
Epoch 290, training loss: 643.3538208007812 = 1.5821199417114258 + 100.0 * 6.417716979980469
Epoch 290, val loss: 1.6007182598114014
Epoch 300, training loss: 642.7501220703125 = 1.5573498010635376 + 100.0 * 6.411927700042725
Epoch 300, val loss: 1.5799109935760498
Epoch 310, training loss: 642.2003173828125 = 1.5310958623886108 + 100.0 * 6.4066925048828125
Epoch 310, val loss: 1.558159351348877
Epoch 320, training loss: 641.4710083007812 = 1.5037338733673096 + 100.0 * 6.399672985076904
Epoch 320, val loss: 1.5353301763534546
Epoch 330, training loss: 641.4635009765625 = 1.4752215147018433 + 100.0 * 6.399882793426514
Epoch 330, val loss: 1.5118904113769531
Epoch 340, training loss: 640.3449096679688 = 1.4457788467407227 + 100.0 * 6.388991355895996
Epoch 340, val loss: 1.487797498703003
Epoch 350, training loss: 639.915283203125 = 1.4156566858291626 + 100.0 * 6.38499641418457
Epoch 350, val loss: 1.4632104635238647
Epoch 360, training loss: 639.3914184570312 = 1.3849267959594727 + 100.0 * 6.380064964294434
Epoch 360, val loss: 1.4383771419525146
Epoch 370, training loss: 639.4004516601562 = 1.35378098487854 + 100.0 * 6.380466938018799
Epoch 370, val loss: 1.4134011268615723
Epoch 380, training loss: 638.5094604492188 = 1.3225096464157104 + 100.0 * 6.3718695640563965
Epoch 380, val loss: 1.3883033990859985
Epoch 390, training loss: 638.0596313476562 = 1.2913061380386353 + 100.0 * 6.367683410644531
Epoch 390, val loss: 1.3635035753250122
Epoch 400, training loss: 637.8971557617188 = 1.2602671384811401 + 100.0 * 6.366369247436523
Epoch 400, val loss: 1.3389489650726318
Epoch 410, training loss: 637.2642822265625 = 1.2293895483016968 + 100.0 * 6.360349178314209
Epoch 410, val loss: 1.3148608207702637
Epoch 420, training loss: 637.7310791015625 = 1.1990127563476562 + 100.0 * 6.365320682525635
Epoch 420, val loss: 1.2913180589675903
Epoch 430, training loss: 636.5597534179688 = 1.1690866947174072 + 100.0 * 6.353906631469727
Epoch 430, val loss: 1.2683837413787842
Epoch 440, training loss: 636.1336669921875 = 1.1399017572402954 + 100.0 * 6.349937915802002
Epoch 440, val loss: 1.24619722366333
Epoch 450, training loss: 636.05224609375 = 1.1114206314086914 + 100.0 * 6.349408149719238
Epoch 450, val loss: 1.2250032424926758
Epoch 460, training loss: 636.0245971679688 = 1.0836297273635864 + 100.0 * 6.349410057067871
Epoch 460, val loss: 1.2042357921600342
Epoch 470, training loss: 635.2517700195312 = 1.0564558506011963 + 100.0 * 6.341953277587891
Epoch 470, val loss: 1.1845970153808594
Epoch 480, training loss: 634.8737182617188 = 1.0301437377929688 + 100.0 * 6.338435649871826
Epoch 480, val loss: 1.1659002304077148
Epoch 490, training loss: 635.5154418945312 = 1.0045584440231323 + 100.0 * 6.345108509063721
Epoch 490, val loss: 1.1479285955429077
Epoch 500, training loss: 634.64990234375 = 0.9795364141464233 + 100.0 * 6.336703300476074
Epoch 500, val loss: 1.130774974822998
Epoch 510, training loss: 634.158935546875 = 0.9552627801895142 + 100.0 * 6.332036972045898
Epoch 510, val loss: 1.1145880222320557
Epoch 520, training loss: 633.8878784179688 = 0.931806743144989 + 100.0 * 6.329560279846191
Epoch 520, val loss: 1.0992904901504517
Epoch 530, training loss: 634.2646484375 = 0.908935010433197 + 100.0 * 6.33355712890625
Epoch 530, val loss: 1.0851432085037231
Epoch 540, training loss: 633.6815795898438 = 0.8865264058113098 + 100.0 * 6.327950477600098
Epoch 540, val loss: 1.0709564685821533
Epoch 550, training loss: 633.2608642578125 = 0.8647114038467407 + 100.0 * 6.3239617347717285
Epoch 550, val loss: 1.0578125715255737
Epoch 560, training loss: 632.9411010742188 = 0.8434819579124451 + 100.0 * 6.3209757804870605
Epoch 560, val loss: 1.0455445051193237
Epoch 570, training loss: 633.3035888671875 = 0.8227570652961731 + 100.0 * 6.324808597564697
Epoch 570, val loss: 1.0337848663330078
Epoch 580, training loss: 632.8325805664062 = 0.8021576404571533 + 100.0 * 6.320303916931152
Epoch 580, val loss: 1.0229676961898804
Epoch 590, training loss: 632.2525634765625 = 0.7821942567825317 + 100.0 * 6.314703941345215
Epoch 590, val loss: 1.01239812374115
Epoch 600, training loss: 632.1429443359375 = 0.7625969648361206 + 100.0 * 6.313803672790527
Epoch 600, val loss: 1.002607822418213
Epoch 610, training loss: 632.3108520507812 = 0.7432272434234619 + 100.0 * 6.315676212310791
Epoch 610, val loss: 0.9933307766914368
Epoch 620, training loss: 631.8411254882812 = 0.724225640296936 + 100.0 * 6.311168670654297
Epoch 620, val loss: 0.9840937852859497
Epoch 630, training loss: 631.6600952148438 = 0.7054023146629333 + 100.0 * 6.309546947479248
Epoch 630, val loss: 0.9758579730987549
Epoch 640, training loss: 631.6038208007812 = 0.6870417594909668 + 100.0 * 6.309167861938477
Epoch 640, val loss: 0.9677937626838684
Epoch 650, training loss: 631.4317626953125 = 0.668695867061615 + 100.0 * 6.30763053894043
Epoch 650, val loss: 0.9606437683105469
Epoch 660, training loss: 631.1061401367188 = 0.650722086429596 + 100.0 * 6.304554462432861
Epoch 660, val loss: 0.9535341858863831
Epoch 670, training loss: 631.62060546875 = 0.6329762935638428 + 100.0 * 6.309875965118408
Epoch 670, val loss: 0.9468814134597778
Epoch 680, training loss: 631.25927734375 = 0.6153026223182678 + 100.0 * 6.3064398765563965
Epoch 680, val loss: 0.9407876133918762
Epoch 690, training loss: 630.5831909179688 = 0.5979556441307068 + 100.0 * 6.29985237121582
Epoch 690, val loss: 0.9351485967636108
Epoch 700, training loss: 630.3485107421875 = 0.5809509754180908 + 100.0 * 6.297675609588623
Epoch 700, val loss: 0.9298954606056213
Epoch 710, training loss: 630.2730102539062 = 0.5642155408859253 + 100.0 * 6.297088146209717
Epoch 710, val loss: 0.9250234961509705
Epoch 720, training loss: 630.537353515625 = 0.5476395487785339 + 100.0 * 6.299896717071533
Epoch 720, val loss: 0.920622706413269
Epoch 730, training loss: 630.400146484375 = 0.5311780571937561 + 100.0 * 6.298689365386963
Epoch 730, val loss: 0.91629958152771
Epoch 740, training loss: 629.9087524414062 = 0.5150933265686035 + 100.0 * 6.293936729431152
Epoch 740, val loss: 0.912590742111206
Epoch 750, training loss: 629.6493530273438 = 0.49930718541145325 + 100.0 * 6.291500091552734
Epoch 750, val loss: 0.9094989895820618
Epoch 760, training loss: 630.0999755859375 = 0.48388388752937317 + 100.0 * 6.296160697937012
Epoch 760, val loss: 0.9065544605255127
Epoch 770, training loss: 629.725341796875 = 0.4686618745326996 + 100.0 * 6.292566299438477
Epoch 770, val loss: 0.9041730165481567
Epoch 780, training loss: 629.6107788085938 = 0.4537504017353058 + 100.0 * 6.291570663452148
Epoch 780, val loss: 0.9019359946250916
Epoch 790, training loss: 629.3223266601562 = 0.43913328647613525 + 100.0 * 6.28883171081543
Epoch 790, val loss: 0.9002459645271301
Epoch 800, training loss: 629.1300659179688 = 0.42493024468421936 + 100.0 * 6.287051677703857
Epoch 800, val loss: 0.8988979458808899
Epoch 810, training loss: 629.1597900390625 = 0.41110143065452576 + 100.0 * 6.287487030029297
Epoch 810, val loss: 0.8978703022003174
Epoch 820, training loss: 628.92529296875 = 0.39758458733558655 + 100.0 * 6.285277366638184
Epoch 820, val loss: 0.8971453905105591
Epoch 830, training loss: 629.2210693359375 = 0.3844356834888458 + 100.0 * 6.288365840911865
Epoch 830, val loss: 0.8969453573226929
Epoch 840, training loss: 628.7568359375 = 0.37166351079940796 + 100.0 * 6.2838521003723145
Epoch 840, val loss: 0.8966113328933716
Epoch 850, training loss: 628.5681762695312 = 0.3592495024204254 + 100.0 * 6.2820892333984375
Epoch 850, val loss: 0.8972180485725403
Epoch 860, training loss: 628.3377075195312 = 0.34731021523475647 + 100.0 * 6.279903888702393
Epoch 860, val loss: 0.8979973196983337
Epoch 870, training loss: 628.8252563476562 = 0.3357110619544983 + 100.0 * 6.284895896911621
Epoch 870, val loss: 0.8994030356407166
Epoch 880, training loss: 628.7749633789062 = 0.32457321882247925 + 100.0 * 6.284503936767578
Epoch 880, val loss: 0.9003187417984009
Epoch 890, training loss: 628.1504516601562 = 0.3136475384235382 + 100.0 * 6.27836799621582
Epoch 890, val loss: 0.9020559191703796
Epoch 900, training loss: 627.9742431640625 = 0.30324554443359375 + 100.0 * 6.276710510253906
Epoch 900, val loss: 0.9040840268135071
Epoch 910, training loss: 628.017822265625 = 0.2932564616203308 + 100.0 * 6.27724552154541
Epoch 910, val loss: 0.906305730342865
Epoch 920, training loss: 628.1757202148438 = 0.2835733890533447 + 100.0 * 6.278921604156494
Epoch 920, val loss: 0.9087157249450684
Epoch 930, training loss: 628.1244506835938 = 0.27421024441719055 + 100.0 * 6.278501987457275
Epoch 930, val loss: 0.9113993048667908
Epoch 940, training loss: 627.6768798828125 = 0.26524364948272705 + 100.0 * 6.274116039276123
Epoch 940, val loss: 0.91416335105896
Epoch 950, training loss: 627.5217895507812 = 0.25661084055900574 + 100.0 * 6.2726521492004395
Epoch 950, val loss: 0.9175752401351929
Epoch 960, training loss: 627.448974609375 = 0.24836872518062592 + 100.0 * 6.272006511688232
Epoch 960, val loss: 0.9210712313652039
Epoch 970, training loss: 628.524658203125 = 0.2404932677745819 + 100.0 * 6.282841682434082
Epoch 970, val loss: 0.9244811534881592
Epoch 980, training loss: 627.5805053710938 = 0.23273149132728577 + 100.0 * 6.273477554321289
Epoch 980, val loss: 0.9284093379974365
Epoch 990, training loss: 627.2012939453125 = 0.2253623753786087 + 100.0 * 6.269759654998779
Epoch 990, val loss: 0.932313859462738
Epoch 1000, training loss: 627.2191162109375 = 0.21832165122032166 + 100.0 * 6.270008087158203
Epoch 1000, val loss: 0.9365981221199036
Epoch 1010, training loss: 627.5538330078125 = 0.21155108511447906 + 100.0 * 6.273422718048096
Epoch 1010, val loss: 0.9408628344535828
Epoch 1020, training loss: 627.1741333007812 = 0.2049754112958908 + 100.0 * 6.2696919441223145
Epoch 1020, val loss: 0.9453229308128357
Epoch 1030, training loss: 627.1480712890625 = 0.19874738156795502 + 100.0 * 6.269493579864502
Epoch 1030, val loss: 0.9498860836029053
Epoch 1040, training loss: 627.0505981445312 = 0.19270753860473633 + 100.0 * 6.268579006195068
Epoch 1040, val loss: 0.95463627576828
Epoch 1050, training loss: 626.9038696289062 = 0.186876580119133 + 100.0 * 6.267169952392578
Epoch 1050, val loss: 0.9594906568527222
Epoch 1060, training loss: 626.7626953125 = 0.1813262701034546 + 100.0 * 6.265813827514648
Epoch 1060, val loss: 0.9643551111221313
Epoch 1070, training loss: 626.7462158203125 = 0.17597223818302155 + 100.0 * 6.265702724456787
Epoch 1070, val loss: 0.9694599509239197
Epoch 1080, training loss: 626.9883422851562 = 0.1708078235387802 + 100.0 * 6.26817512512207
Epoch 1080, val loss: 0.9747887253761292
Epoch 1090, training loss: 626.6688232421875 = 0.16575856506824493 + 100.0 * 6.265030384063721
Epoch 1090, val loss: 0.979697585105896
Epoch 1100, training loss: 626.5657348632812 = 0.16097547113895416 + 100.0 * 6.264047622680664
Epoch 1100, val loss: 0.985081672668457
Epoch 1110, training loss: 626.2942504882812 = 0.1563633680343628 + 100.0 * 6.261378765106201
Epoch 1110, val loss: 0.9906773567199707
Epoch 1120, training loss: 626.2714233398438 = 0.1519606113433838 + 100.0 * 6.261194229125977
Epoch 1120, val loss: 0.9962111711502075
Epoch 1130, training loss: 626.9033203125 = 0.14770911633968353 + 100.0 * 6.267556190490723
Epoch 1130, val loss: 1.001567006111145
Epoch 1140, training loss: 626.2607421875 = 0.14353743195533752 + 100.0 * 6.261171817779541
Epoch 1140, val loss: 1.0070494413375854
Epoch 1150, training loss: 626.1383056640625 = 0.13954654335975647 + 100.0 * 6.259987831115723
Epoch 1150, val loss: 1.0128896236419678
Epoch 1160, training loss: 625.9647216796875 = 0.13573549687862396 + 100.0 * 6.258289813995361
Epoch 1160, val loss: 1.0187464952468872
Epoch 1170, training loss: 627.0274047851562 = 0.13214924931526184 + 100.0 * 6.2689528465271
Epoch 1170, val loss: 1.0242507457733154
Epoch 1180, training loss: 626.834228515625 = 0.12845049798488617 + 100.0 * 6.267057418823242
Epoch 1180, val loss: 1.0303425788879395
Epoch 1190, training loss: 625.8651733398438 = 0.1250094324350357 + 100.0 * 6.257401466369629
Epoch 1190, val loss: 1.0359523296356201
Epoch 1200, training loss: 625.7996826171875 = 0.1217120960354805 + 100.0 * 6.256779670715332
Epoch 1200, val loss: 1.0419416427612305
Epoch 1210, training loss: 625.7813110351562 = 0.11851855367422104 + 100.0 * 6.256627559661865
Epoch 1210, val loss: 1.0481109619140625
Epoch 1220, training loss: 626.14892578125 = 0.11547420173883438 + 100.0 * 6.260334491729736
Epoch 1220, val loss: 1.0538685321807861
Epoch 1230, training loss: 625.6392822265625 = 0.11244813352823257 + 100.0 * 6.255268573760986
Epoch 1230, val loss: 1.059698224067688
Epoch 1240, training loss: 625.7998046875 = 0.10958658158779144 + 100.0 * 6.25690221786499
Epoch 1240, val loss: 1.0655368566513062
Epoch 1250, training loss: 626.063720703125 = 0.10679945349693298 + 100.0 * 6.25956916809082
Epoch 1250, val loss: 1.0716023445129395
Epoch 1260, training loss: 625.8104858398438 = 0.1041124165058136 + 100.0 * 6.257063865661621
Epoch 1260, val loss: 1.0772877931594849
Epoch 1270, training loss: 625.6795043945312 = 0.10147342830896378 + 100.0 * 6.25577974319458
Epoch 1270, val loss: 1.0834629535675049
Epoch 1280, training loss: 625.4854125976562 = 0.09897920489311218 + 100.0 * 6.253864288330078
Epoch 1280, val loss: 1.08931565284729
Epoch 1290, training loss: 625.3419189453125 = 0.09654935449361801 + 100.0 * 6.252453327178955
Epoch 1290, val loss: 1.0953538417816162
Epoch 1300, training loss: 625.3729858398438 = 0.09421627223491669 + 100.0 * 6.2527875900268555
Epoch 1300, val loss: 1.1013771295547485
Epoch 1310, training loss: 625.44482421875 = 0.09192807227373123 + 100.0 * 6.253529071807861
Epoch 1310, val loss: 1.1073740720748901
Epoch 1320, training loss: 625.7638549804688 = 0.08969057351350784 + 100.0 * 6.256742000579834
Epoch 1320, val loss: 1.1131820678710938
Epoch 1330, training loss: 625.3047485351562 = 0.08756303787231445 + 100.0 * 6.252171516418457
Epoch 1330, val loss: 1.1188215017318726
Epoch 1340, training loss: 625.439208984375 = 0.08548831194639206 + 100.0 * 6.253536701202393
Epoch 1340, val loss: 1.12508225440979
Epoch 1350, training loss: 625.0147094726562 = 0.08346915245056152 + 100.0 * 6.249311923980713
Epoch 1350, val loss: 1.130699634552002
Epoch 1360, training loss: 625.2180786132812 = 0.08151803910732269 + 100.0 * 6.2513651847839355
Epoch 1360, val loss: 1.1366692781448364
Epoch 1370, training loss: 625.3273315429688 = 0.07963766902685165 + 100.0 * 6.252476692199707
Epoch 1370, val loss: 1.1425652503967285
Epoch 1380, training loss: 625.2738647460938 = 0.077825628221035 + 100.0 * 6.251960277557373
Epoch 1380, val loss: 1.1479594707489014
Epoch 1390, training loss: 624.8558959960938 = 0.07603565603494644 + 100.0 * 6.247798442840576
Epoch 1390, val loss: 1.1541205644607544
Epoch 1400, training loss: 624.8052368164062 = 0.07434210926294327 + 100.0 * 6.247308731079102
Epoch 1400, val loss: 1.1599555015563965
Epoch 1410, training loss: 625.5315551757812 = 0.07270517945289612 + 100.0 * 6.254588603973389
Epoch 1410, val loss: 1.1654592752456665
Epoch 1420, training loss: 625.0303955078125 = 0.07105658203363419 + 100.0 * 6.249593734741211
Epoch 1420, val loss: 1.1715105772018433
Epoch 1430, training loss: 624.890869140625 = 0.06947775185108185 + 100.0 * 6.248214244842529
Epoch 1430, val loss: 1.1770660877227783
Epoch 1440, training loss: 624.8914794921875 = 0.06795996427536011 + 100.0 * 6.24823522567749
Epoch 1440, val loss: 1.1829134225845337
Epoch 1450, training loss: 624.552490234375 = 0.06649285554885864 + 100.0 * 6.2448601722717285
Epoch 1450, val loss: 1.188454508781433
Epoch 1460, training loss: 625.0108032226562 = 0.0650830790400505 + 100.0 * 6.249457359313965
Epoch 1460, val loss: 1.1940840482711792
Epoch 1470, training loss: 625.1475219726562 = 0.06366001814603806 + 100.0 * 6.250838756561279
Epoch 1470, val loss: 1.2000397443771362
Epoch 1480, training loss: 624.68359375 = 0.06230893358588219 + 100.0 * 6.246212482452393
Epoch 1480, val loss: 1.2049201726913452
Epoch 1490, training loss: 624.3897094726562 = 0.06099117919802666 + 100.0 * 6.243287086486816
Epoch 1490, val loss: 1.2107853889465332
Epoch 1500, training loss: 624.4407958984375 = 0.05973237752914429 + 100.0 * 6.243810653686523
Epoch 1500, val loss: 1.2164264917373657
Epoch 1510, training loss: 625.0546264648438 = 0.058508045971393585 + 100.0 * 6.249960899353027
Epoch 1510, val loss: 1.2218189239501953
Epoch 1520, training loss: 624.53271484375 = 0.05728857219219208 + 100.0 * 6.244754314422607
Epoch 1520, val loss: 1.2269933223724365
Epoch 1530, training loss: 624.366943359375 = 0.0561106838285923 + 100.0 * 6.24310827255249
Epoch 1530, val loss: 1.2324308156967163
Epoch 1540, training loss: 624.43212890625 = 0.054991915822029114 + 100.0 * 6.243771076202393
Epoch 1540, val loss: 1.2379555702209473
Epoch 1550, training loss: 624.2467651367188 = 0.05387992411851883 + 100.0 * 6.241928577423096
Epoch 1550, val loss: 1.2432196140289307
Epoch 1560, training loss: 624.4783325195312 = 0.05280853062868118 + 100.0 * 6.244255065917969
Epoch 1560, val loss: 1.2487766742706299
Epoch 1570, training loss: 624.3193969726562 = 0.051752157509326935 + 100.0 * 6.242676258087158
Epoch 1570, val loss: 1.2537068128585815
Epoch 1580, training loss: 624.2571411132812 = 0.05074983090162277 + 100.0 * 6.242063999176025
Epoch 1580, val loss: 1.2589499950408936
Epoch 1590, training loss: 624.4730224609375 = 0.0497526153922081 + 100.0 * 6.244232654571533
Epoch 1590, val loss: 1.2642256021499634
Epoch 1600, training loss: 624.1201782226562 = 0.04879603534936905 + 100.0 * 6.240714073181152
Epoch 1600, val loss: 1.2693769931793213
Epoch 1610, training loss: 624.0396728515625 = 0.0478614941239357 + 100.0 * 6.239918231964111
Epoch 1610, val loss: 1.2745786905288696
Epoch 1620, training loss: 624.9342041015625 = 0.04697578027844429 + 100.0 * 6.248871803283691
Epoch 1620, val loss: 1.2798970937728882
Epoch 1630, training loss: 624.2665405273438 = 0.04605632647871971 + 100.0 * 6.242204666137695
Epoch 1630, val loss: 1.2843074798583984
Epoch 1640, training loss: 623.9795532226562 = 0.04518983140587807 + 100.0 * 6.239344120025635
Epoch 1640, val loss: 1.289670705795288
Epoch 1650, training loss: 623.8884887695312 = 0.04434700310230255 + 100.0 * 6.238441467285156
Epoch 1650, val loss: 1.2946668863296509
Epoch 1660, training loss: 624.4093627929688 = 0.04353386536240578 + 100.0 * 6.243658542633057
Epoch 1660, val loss: 1.300095796585083
Epoch 1670, training loss: 624.27294921875 = 0.042725756764411926 + 100.0 * 6.242301940917969
Epoch 1670, val loss: 1.3046797513961792
Epoch 1680, training loss: 623.9036865234375 = 0.041929278522729874 + 100.0 * 6.238617420196533
Epoch 1680, val loss: 1.309380054473877
Epoch 1690, training loss: 623.7953491210938 = 0.04117363691329956 + 100.0 * 6.237541675567627
Epoch 1690, val loss: 1.314294695854187
Epoch 1700, training loss: 623.684814453125 = 0.040439408272504807 + 100.0 * 6.236443519592285
Epoch 1700, val loss: 1.3193365335464478
Epoch 1710, training loss: 624.3108520507812 = 0.03974863886833191 + 100.0 * 6.242711067199707
Epoch 1710, val loss: 1.3237441778182983
Epoch 1720, training loss: 624.1823120117188 = 0.03899972885847092 + 100.0 * 6.241433143615723
Epoch 1720, val loss: 1.3290958404541016
Epoch 1730, training loss: 623.86767578125 = 0.03830820322036743 + 100.0 * 6.238293170928955
Epoch 1730, val loss: 1.3331354856491089
Epoch 1740, training loss: 623.5971069335938 = 0.037624042481184006 + 100.0 * 6.235595226287842
Epoch 1740, val loss: 1.3382552862167358
Epoch 1750, training loss: 623.5105590820312 = 0.03698145970702171 + 100.0 * 6.234735488891602
Epoch 1750, val loss: 1.3431445360183716
Epoch 1760, training loss: 624.1376953125 = 0.03636109456419945 + 100.0 * 6.241013050079346
Epoch 1760, val loss: 1.3478790521621704
Epoch 1770, training loss: 623.96826171875 = 0.03572676703333855 + 100.0 * 6.239325523376465
Epoch 1770, val loss: 1.3520597219467163
Epoch 1780, training loss: 623.8195190429688 = 0.03510774299502373 + 100.0 * 6.237843990325928
Epoch 1780, val loss: 1.35707688331604
Epoch 1790, training loss: 623.467529296875 = 0.034499309957027435 + 100.0 * 6.234330654144287
Epoch 1790, val loss: 1.3613232374191284
Epoch 1800, training loss: 623.4866333007812 = 0.033923279494047165 + 100.0 * 6.234527111053467
Epoch 1800, val loss: 1.366018295288086
Epoch 1810, training loss: 623.9293212890625 = 0.033358827233314514 + 100.0 * 6.238959312438965
Epoch 1810, val loss: 1.3705750703811646
Epoch 1820, training loss: 623.733642578125 = 0.03281126171350479 + 100.0 * 6.237008094787598
Epoch 1820, val loss: 1.3753628730773926
Epoch 1830, training loss: 623.6431884765625 = 0.03225870430469513 + 100.0 * 6.236108779907227
Epoch 1830, val loss: 1.3795114755630493
Epoch 1840, training loss: 623.3618774414062 = 0.03173206374049187 + 100.0 * 6.233301162719727
Epoch 1840, val loss: 1.384299874305725
Epoch 1850, training loss: 623.9072265625 = 0.031222902238368988 + 100.0 * 6.238759994506836
Epoch 1850, val loss: 1.388693928718567
Epoch 1860, training loss: 623.454345703125 = 0.0307141300290823 + 100.0 * 6.234236240386963
Epoch 1860, val loss: 1.3928157091140747
Epoch 1870, training loss: 623.270751953125 = 0.030208930373191833 + 100.0 * 6.232405662536621
Epoch 1870, val loss: 1.3971725702285767
Epoch 1880, training loss: 623.4088134765625 = 0.02974131517112255 + 100.0 * 6.233790874481201
Epoch 1880, val loss: 1.4016237258911133
Epoch 1890, training loss: 623.4088134765625 = 0.02926800586283207 + 100.0 * 6.233795166015625
Epoch 1890, val loss: 1.4057097434997559
Epoch 1900, training loss: 623.23779296875 = 0.02879466861486435 + 100.0 * 6.232089996337891
Epoch 1900, val loss: 1.4106240272521973
Epoch 1910, training loss: 623.5088500976562 = 0.028347698971629143 + 100.0 * 6.234805583953857
Epoch 1910, val loss: 1.4147601127624512
Epoch 1920, training loss: 623.5091552734375 = 0.02790161594748497 + 100.0 * 6.2348127365112305
Epoch 1920, val loss: 1.4185311794281006
Epoch 1930, training loss: 623.194091796875 = 0.027461158111691475 + 100.0 * 6.231666088104248
Epoch 1930, val loss: 1.4231804609298706
Epoch 1940, training loss: 623.087158203125 = 0.02703700214624405 + 100.0 * 6.2306013107299805
Epoch 1940, val loss: 1.4275527000427246
Epoch 1950, training loss: 623.1160278320312 = 0.02662678249180317 + 100.0 * 6.230894088745117
Epoch 1950, val loss: 1.4318851232528687
Epoch 1960, training loss: 623.67822265625 = 0.02622321806848049 + 100.0 * 6.236519813537598
Epoch 1960, val loss: 1.4356791973114014
Epoch 1970, training loss: 623.3819580078125 = 0.0258298572152853 + 100.0 * 6.2335615158081055
Epoch 1970, val loss: 1.4393826723098755
Epoch 1980, training loss: 623.163330078125 = 0.025425700470805168 + 100.0 * 6.23137903213501
Epoch 1980, val loss: 1.4439114332199097
Epoch 1990, training loss: 623.0180053710938 = 0.025053204968571663 + 100.0 * 6.2299299240112305
Epoch 1990, val loss: 1.447814702987671
Epoch 2000, training loss: 623.2406005859375 = 0.024686481803655624 + 100.0 * 6.23215913772583
Epoch 2000, val loss: 1.4522494077682495
Epoch 2010, training loss: 622.9192504882812 = 0.024319956079125404 + 100.0 * 6.228949546813965
Epoch 2010, val loss: 1.4560691118240356
Epoch 2020, training loss: 623.3063354492188 = 0.023965420201420784 + 100.0 * 6.232823371887207
Epoch 2020, val loss: 1.4597718715667725
Epoch 2030, training loss: 623.3818359375 = 0.02360537089407444 + 100.0 * 6.233582019805908
Epoch 2030, val loss: 1.4636050462722778
Epoch 2040, training loss: 622.8514404296875 = 0.02325177937746048 + 100.0 * 6.2282819747924805
Epoch 2040, val loss: 1.468150019645691
Epoch 2050, training loss: 622.716796875 = 0.022919783368706703 + 100.0 * 6.226938724517822
Epoch 2050, val loss: 1.4718273878097534
Epoch 2060, training loss: 623.4112548828125 = 0.022607287392020226 + 100.0 * 6.23388671875
Epoch 2060, val loss: 1.4752709865570068
Epoch 2070, training loss: 622.76318359375 = 0.022271636873483658 + 100.0 * 6.2274088859558105
Epoch 2070, val loss: 1.4797056913375854
Epoch 2080, training loss: 622.6419677734375 = 0.02195386216044426 + 100.0 * 6.226200103759766
Epoch 2080, val loss: 1.4832714796066284
Epoch 2090, training loss: 622.6724853515625 = 0.021649308502674103 + 100.0 * 6.226508140563965
Epoch 2090, val loss: 1.4870818853378296
Epoch 2100, training loss: 623.316650390625 = 0.021355632692575455 + 100.0 * 6.23295259475708
Epoch 2100, val loss: 1.4907395839691162
Epoch 2110, training loss: 622.8069458007812 = 0.021061046048998833 + 100.0 * 6.227859020233154
Epoch 2110, val loss: 1.4946569204330444
Epoch 2120, training loss: 622.8489379882812 = 0.02076619677245617 + 100.0 * 6.2282819747924805
Epoch 2120, val loss: 1.4982727766036987
Epoch 2130, training loss: 623.0823364257812 = 0.020478637889027596 + 100.0 * 6.230618476867676
Epoch 2130, val loss: 1.502216100692749
Epoch 2140, training loss: 622.6416625976562 = 0.02019834704697132 + 100.0 * 6.22621488571167
Epoch 2140, val loss: 1.5051982402801514
Epoch 2150, training loss: 622.545654296875 = 0.01992603950202465 + 100.0 * 6.225257873535156
Epoch 2150, val loss: 1.5093460083007812
Epoch 2160, training loss: 622.9798583984375 = 0.019666019827127457 + 100.0 * 6.229601860046387
Epoch 2160, val loss: 1.5131577253341675
Epoch 2170, training loss: 622.5746459960938 = 0.019398244097828865 + 100.0 * 6.225552558898926
Epoch 2170, val loss: 1.516352653503418
Epoch 2180, training loss: 622.78125 = 0.01914471574127674 + 100.0 * 6.227621078491211
Epoch 2180, val loss: 1.520042061805725
Epoch 2190, training loss: 622.51171875 = 0.01888325996696949 + 100.0 * 6.224928855895996
Epoch 2190, val loss: 1.5234706401824951
Epoch 2200, training loss: 622.5098266601562 = 0.01864304021000862 + 100.0 * 6.224912166595459
Epoch 2200, val loss: 1.5267157554626465
Epoch 2210, training loss: 622.9950561523438 = 0.01841035857796669 + 100.0 * 6.229766368865967
Epoch 2210, val loss: 1.5300590991973877
Epoch 2220, training loss: 622.5648193359375 = 0.018161118030548096 + 100.0 * 6.225466251373291
Epoch 2220, val loss: 1.533652663230896
Epoch 2230, training loss: 622.5767211914062 = 0.017927559092640877 + 100.0 * 6.225587844848633
Epoch 2230, val loss: 1.5372763872146606
Epoch 2240, training loss: 622.6925659179688 = 0.01769876666367054 + 100.0 * 6.226748466491699
Epoch 2240, val loss: 1.5402988195419312
Epoch 2250, training loss: 622.433349609375 = 0.017475472763180733 + 100.0 * 6.224159240722656
Epoch 2250, val loss: 1.5438849925994873
Epoch 2260, training loss: 622.4013061523438 = 0.017254801467061043 + 100.0 * 6.223840236663818
Epoch 2260, val loss: 1.5475389957427979
Epoch 2270, training loss: 622.7869262695312 = 0.017045343294739723 + 100.0 * 6.227699279785156
Epoch 2270, val loss: 1.5506601333618164
Epoch 2280, training loss: 622.523681640625 = 0.016830509528517723 + 100.0 * 6.225068092346191
Epoch 2280, val loss: 1.553773045539856
Epoch 2290, training loss: 622.2190551757812 = 0.016618775203824043 + 100.0 * 6.222024440765381
Epoch 2290, val loss: 1.5573298931121826
Epoch 2300, training loss: 622.3193969726562 = 0.01641937904059887 + 100.0 * 6.223029613494873
Epoch 2300, val loss: 1.5610969066619873
Epoch 2310, training loss: 623.0365600585938 = 0.016224900260567665 + 100.0 * 6.230203151702881
Epoch 2310, val loss: 1.564026951789856
Epoch 2320, training loss: 622.7029418945312 = 0.016020067036151886 + 100.0 * 6.226869583129883
Epoch 2320, val loss: 1.566957950592041
Epoch 2330, training loss: 622.2505493164062 = 0.015823936089873314 + 100.0 * 6.222347259521484
Epoch 2330, val loss: 1.5703743696212769
Epoch 2340, training loss: 622.3407592773438 = 0.01564071513712406 + 100.0 * 6.2232513427734375
Epoch 2340, val loss: 1.5736191272735596
Epoch 2350, training loss: 622.2249755859375 = 0.015453701838850975 + 100.0 * 6.222095489501953
Epoch 2350, val loss: 1.5766445398330688
Epoch 2360, training loss: 622.676025390625 = 0.01527838408946991 + 100.0 * 6.226607799530029
Epoch 2360, val loss: 1.5794018507003784
Epoch 2370, training loss: 622.3773803710938 = 0.015092216432094574 + 100.0 * 6.223622798919678
Epoch 2370, val loss: 1.5828920602798462
Epoch 2380, training loss: 622.0809936523438 = 0.014909266494214535 + 100.0 * 6.220661163330078
Epoch 2380, val loss: 1.5860443115234375
Epoch 2390, training loss: 622.0224609375 = 0.014737056568264961 + 100.0 * 6.2200775146484375
Epoch 2390, val loss: 1.5893046855926514
Epoch 2400, training loss: 622.0673217773438 = 0.014570934697985649 + 100.0 * 6.220527172088623
Epoch 2400, val loss: 1.5926759243011475
Epoch 2410, training loss: 622.4797973632812 = 0.01440905686467886 + 100.0 * 6.224654197692871
Epoch 2410, val loss: 1.595710039138794
Epoch 2420, training loss: 622.1718139648438 = 0.014245530590415001 + 100.0 * 6.22157621383667
Epoch 2420, val loss: 1.5986467599868774
Epoch 2430, training loss: 622.1561889648438 = 0.014082000590860844 + 100.0 * 6.221420764923096
Epoch 2430, val loss: 1.6014097929000854
Epoch 2440, training loss: 622.4284057617188 = 0.013926910236477852 + 100.0 * 6.22414493560791
Epoch 2440, val loss: 1.6037237644195557
Epoch 2450, training loss: 621.9796752929688 = 0.013768110424280167 + 100.0 * 6.219658851623535
Epoch 2450, val loss: 1.607411503791809
Epoch 2460, training loss: 622.014404296875 = 0.013617427088320255 + 100.0 * 6.22000789642334
Epoch 2460, val loss: 1.6101676225662231
Epoch 2470, training loss: 622.4517822265625 = 0.013473881408572197 + 100.0 * 6.2243828773498535
Epoch 2470, val loss: 1.612870454788208
Epoch 2480, training loss: 621.98388671875 = 0.01332149375230074 + 100.0 * 6.219706058502197
Epoch 2480, val loss: 1.6161288022994995
Epoch 2490, training loss: 622.2098999023438 = 0.013182636350393295 + 100.0 * 6.2219672203063965
Epoch 2490, val loss: 1.6186566352844238
Epoch 2500, training loss: 622.0298461914062 = 0.013032740913331509 + 100.0 * 6.220167636871338
Epoch 2500, val loss: 1.6217429637908936
Epoch 2510, training loss: 622.1458129882812 = 0.0128953717648983 + 100.0 * 6.221329212188721
Epoch 2510, val loss: 1.6248811483383179
Epoch 2520, training loss: 622.0363159179688 = 0.012757117860019207 + 100.0 * 6.220235824584961
Epoch 2520, val loss: 1.6272224187850952
Epoch 2530, training loss: 622.0690307617188 = 0.012625758536159992 + 100.0 * 6.220563888549805
Epoch 2530, val loss: 1.6298774480819702
Epoch 2540, training loss: 622.4671020507812 = 0.012493923306465149 + 100.0 * 6.224545955657959
Epoch 2540, val loss: 1.6326911449432373
Epoch 2550, training loss: 621.9822387695312 = 0.012354240752756596 + 100.0 * 6.219698905944824
Epoch 2550, val loss: 1.63565194606781
Epoch 2560, training loss: 621.8116455078125 = 0.012227911502122879 + 100.0 * 6.217994689941406
Epoch 2560, val loss: 1.6384479999542236
Epoch 2570, training loss: 621.7332153320312 = 0.012100831605494022 + 100.0 * 6.2172112464904785
Epoch 2570, val loss: 1.641405701637268
Epoch 2580, training loss: 622.0205078125 = 0.01198060717433691 + 100.0 * 6.220085620880127
Epoch 2580, val loss: 1.6443458795547485
Epoch 2590, training loss: 622.2213134765625 = 0.011860428377985954 + 100.0 * 6.222095012664795
Epoch 2590, val loss: 1.646724820137024
Epoch 2600, training loss: 621.9022216796875 = 0.011737005785107613 + 100.0 * 6.218904972076416
Epoch 2600, val loss: 1.649247646331787
Epoch 2610, training loss: 621.742919921875 = 0.011613871902227402 + 100.0 * 6.217313289642334
Epoch 2610, val loss: 1.651840329170227
Epoch 2620, training loss: 621.655517578125 = 0.011500309221446514 + 100.0 * 6.216440677642822
Epoch 2620, val loss: 1.654612421989441
Epoch 2630, training loss: 621.96240234375 = 0.011390479281544685 + 100.0 * 6.219510078430176
Epoch 2630, val loss: 1.6568397283554077
Epoch 2640, training loss: 621.9542846679688 = 0.011276879347860813 + 100.0 * 6.219429969787598
Epoch 2640, val loss: 1.6595808267593384
Epoch 2650, training loss: 621.783447265625 = 0.011160710826516151 + 100.0 * 6.2177228927612305
Epoch 2650, val loss: 1.66221284866333
Epoch 2660, training loss: 621.7688598632812 = 0.011050472036004066 + 100.0 * 6.217578411102295
Epoch 2660, val loss: 1.6650279760360718
Epoch 2670, training loss: 621.9766845703125 = 0.010946634225547314 + 100.0 * 6.2196574211120605
Epoch 2670, val loss: 1.6677266359329224
Epoch 2680, training loss: 621.6991577148438 = 0.010840249247848988 + 100.0 * 6.216883182525635
Epoch 2680, val loss: 1.669894814491272
Epoch 2690, training loss: 621.7166748046875 = 0.010738974437117577 + 100.0 * 6.217059135437012
Epoch 2690, val loss: 1.6722685098648071
Epoch 2700, training loss: 621.5796508789062 = 0.010633805766701698 + 100.0 * 6.2156901359558105
Epoch 2700, val loss: 1.6749522686004639
Epoch 2710, training loss: 622.0512084960938 = 0.010538674890995026 + 100.0 * 6.220406532287598
Epoch 2710, val loss: 1.677910327911377
Epoch 2720, training loss: 621.8162231445312 = 0.01043916679918766 + 100.0 * 6.218057632446289
Epoch 2720, val loss: 1.679617166519165
Epoch 2730, training loss: 621.5597534179688 = 0.010337968356907368 + 100.0 * 6.215494155883789
Epoch 2730, val loss: 1.6823327541351318
Epoch 2740, training loss: 621.6755981445312 = 0.010241570882499218 + 100.0 * 6.216653347015381
Epoch 2740, val loss: 1.6843820810317993
Epoch 2750, training loss: 621.8511352539062 = 0.01015176996588707 + 100.0 * 6.218410015106201
Epoch 2750, val loss: 1.6867403984069824
Epoch 2760, training loss: 621.9778442382812 = 0.010061901994049549 + 100.0 * 6.219677925109863
Epoch 2760, val loss: 1.6891167163848877
Epoch 2770, training loss: 621.5291137695312 = 0.00996228028088808 + 100.0 * 6.215191841125488
Epoch 2770, val loss: 1.6911920309066772
Epoch 2780, training loss: 621.5538940429688 = 0.009873596951365471 + 100.0 * 6.215440273284912
Epoch 2780, val loss: 1.6938471794128418
Epoch 2790, training loss: 621.919677734375 = 0.009791520424187183 + 100.0 * 6.219099044799805
Epoch 2790, val loss: 1.6954121589660645
Epoch 2800, training loss: 621.5589599609375 = 0.009698783047497272 + 100.0 * 6.2154927253723145
Epoch 2800, val loss: 1.6984729766845703
Epoch 2810, training loss: 621.523681640625 = 0.009609803557395935 + 100.0 * 6.2151408195495605
Epoch 2810, val loss: 1.7007914781570435
Epoch 2820, training loss: 621.9052734375 = 0.009527434594929218 + 100.0 * 6.218957424163818
Epoch 2820, val loss: 1.7029870748519897
Epoch 2830, training loss: 621.4198608398438 = 0.009441077709197998 + 100.0 * 6.214103698730469
Epoch 2830, val loss: 1.705017328262329
Epoch 2840, training loss: 621.4153442382812 = 0.009360420517623425 + 100.0 * 6.214060306549072
Epoch 2840, val loss: 1.707309365272522
Epoch 2850, training loss: 621.5846557617188 = 0.009281138889491558 + 100.0 * 6.21575403213501
Epoch 2850, val loss: 1.7097159624099731
Epoch 2860, training loss: 621.532958984375 = 0.00920024886727333 + 100.0 * 6.215237617492676
Epoch 2860, val loss: 1.7119429111480713
Epoch 2870, training loss: 621.5477905273438 = 0.009121177718043327 + 100.0 * 6.215386867523193
Epoch 2870, val loss: 1.714235782623291
Epoch 2880, training loss: 621.638916015625 = 0.009043876081705093 + 100.0 * 6.216299057006836
Epoch 2880, val loss: 1.715855598449707
Epoch 2890, training loss: 621.8240966796875 = 0.00897227507084608 + 100.0 * 6.218151092529297
Epoch 2890, val loss: 1.717751383781433
Epoch 2900, training loss: 621.4451904296875 = 0.008889089338481426 + 100.0 * 6.214362621307373
Epoch 2900, val loss: 1.7202943563461304
Epoch 2910, training loss: 621.3187255859375 = 0.008813698776066303 + 100.0 * 6.213099479675293
Epoch 2910, val loss: 1.7222425937652588
Epoch 2920, training loss: 621.2341918945312 = 0.008742799051105976 + 100.0 * 6.212254524230957
Epoch 2920, val loss: 1.7245452404022217
Epoch 2930, training loss: 621.4738159179688 = 0.008673164062201977 + 100.0 * 6.214651584625244
Epoch 2930, val loss: 1.7264903783798218
Epoch 2940, training loss: 622.0410766601562 = 0.00860088411718607 + 100.0 * 6.220324993133545
Epoch 2940, val loss: 1.7290958166122437
Epoch 2950, training loss: 621.390625 = 0.008524490520358086 + 100.0 * 6.213820934295654
Epoch 2950, val loss: 1.730635404586792
Epoch 2960, training loss: 621.1589965820312 = 0.008449303917586803 + 100.0 * 6.211505889892578
Epoch 2960, val loss: 1.7322595119476318
Epoch 2970, training loss: 621.1286010742188 = 0.008381856605410576 + 100.0 * 6.211202621459961
Epoch 2970, val loss: 1.7348589897155762
Epoch 2980, training loss: 621.181884765625 = 0.008318709209561348 + 100.0 * 6.211735725402832
Epoch 2980, val loss: 1.736648678779602
Epoch 2990, training loss: 622.0535278320312 = 0.008260254748165607 + 100.0 * 6.220452785491943
Epoch 2990, val loss: 1.7382088899612427
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 861.6198120117188 = 1.9338148832321167 + 100.0 * 8.5968599319458
Epoch 0, val loss: 1.9277712106704712
Epoch 10, training loss: 861.548095703125 = 1.9257277250289917 + 100.0 * 8.596223831176758
Epoch 10, val loss: 1.9200190305709839
Epoch 20, training loss: 861.067138671875 = 1.9159163236618042 + 100.0 * 8.591512680053711
Epoch 20, val loss: 1.910338044166565
Epoch 30, training loss: 857.7971801757812 = 1.9032750129699707 + 100.0 * 8.558938980102539
Epoch 30, val loss: 1.89749014377594
Epoch 40, training loss: 839.3875732421875 = 1.8879446983337402 + 100.0 * 8.374996185302734
Epoch 40, val loss: 1.8824622631072998
Epoch 50, training loss: 803.9242553710938 = 1.8716163635253906 + 100.0 * 8.020525932312012
Epoch 50, val loss: 1.8673031330108643
Epoch 60, training loss: 759.8812866210938 = 1.8600444793701172 + 100.0 * 7.580212116241455
Epoch 60, val loss: 1.8568862676620483
Epoch 70, training loss: 718.439208984375 = 1.850395679473877 + 100.0 * 7.16588830947876
Epoch 70, val loss: 1.8475549221038818
Epoch 80, training loss: 701.3851318359375 = 1.8408570289611816 + 100.0 * 6.9954423904418945
Epoch 80, val loss: 1.838305950164795
Epoch 90, training loss: 690.5114135742188 = 1.8304895162582397 + 100.0 * 6.886809349060059
Epoch 90, val loss: 1.8287378549575806
Epoch 100, training loss: 682.8279418945312 = 1.8210376501083374 + 100.0 * 6.8100690841674805
Epoch 100, val loss: 1.8202394247055054
Epoch 110, training loss: 676.4312744140625 = 1.8126709461212158 + 100.0 * 6.746185779571533
Epoch 110, val loss: 1.8127411603927612
Epoch 120, training loss: 671.0795288085938 = 1.804788589477539 + 100.0 * 6.692747116088867
Epoch 120, val loss: 1.8056488037109375
Epoch 130, training loss: 667.094482421875 = 1.796746015548706 + 100.0 * 6.652976989746094
Epoch 130, val loss: 1.7983235120773315
Epoch 140, training loss: 663.7977294921875 = 1.7883247137069702 + 100.0 * 6.620093822479248
Epoch 140, val loss: 1.790740966796875
Epoch 150, training loss: 660.99658203125 = 1.779401421546936 + 100.0 * 6.592171669006348
Epoch 150, val loss: 1.7828058004379272
Epoch 160, training loss: 658.6356811523438 = 1.769806981086731 + 100.0 * 6.568658351898193
Epoch 160, val loss: 1.7743558883666992
Epoch 170, training loss: 656.54443359375 = 1.7595034837722778 + 100.0 * 6.547849178314209
Epoch 170, val loss: 1.765376329421997
Epoch 180, training loss: 655.103759765625 = 1.7482422590255737 + 100.0 * 6.533555030822754
Epoch 180, val loss: 1.7556155920028687
Epoch 190, training loss: 653.0969848632812 = 1.7358258962631226 + 100.0 * 6.513611316680908
Epoch 190, val loss: 1.7450013160705566
Epoch 200, training loss: 651.6075439453125 = 1.722257137298584 + 100.0 * 6.4988532066345215
Epoch 200, val loss: 1.7334238290786743
Epoch 210, training loss: 650.3284912109375 = 1.707352876663208 + 100.0 * 6.48621129989624
Epoch 210, val loss: 1.7206979990005493
Epoch 220, training loss: 648.9846801757812 = 1.6910278797149658 + 100.0 * 6.472936153411865
Epoch 220, val loss: 1.7068157196044922
Epoch 230, training loss: 648.0991821289062 = 1.6732224225997925 + 100.0 * 6.464259624481201
Epoch 230, val loss: 1.6916897296905518
Epoch 240, training loss: 647.0279541015625 = 1.6538571119308472 + 100.0 * 6.45374059677124
Epoch 240, val loss: 1.675297737121582
Epoch 250, training loss: 646.1355590820312 = 1.632877230644226 + 100.0 * 6.445026874542236
Epoch 250, val loss: 1.657572865486145
Epoch 260, training loss: 645.3416748046875 = 1.610276222229004 + 100.0 * 6.437313556671143
Epoch 260, val loss: 1.6384974718093872
Epoch 270, training loss: 644.26806640625 = 1.5861637592315674 + 100.0 * 6.42681884765625
Epoch 270, val loss: 1.6183351278305054
Epoch 280, training loss: 643.9700317382812 = 1.560601830482483 + 100.0 * 6.424094200134277
Epoch 280, val loss: 1.5970852375030518
Epoch 290, training loss: 642.9474487304688 = 1.5337083339691162 + 100.0 * 6.414137363433838
Epoch 290, val loss: 1.5747284889221191
Epoch 300, training loss: 642.207275390625 = 1.5055731534957886 + 100.0 * 6.407017230987549
Epoch 300, val loss: 1.5515491962432861
Epoch 310, training loss: 642.426513671875 = 1.4763720035552979 + 100.0 * 6.409501075744629
Epoch 310, val loss: 1.5277247428894043
Epoch 320, training loss: 641.2508544921875 = 1.4462168216705322 + 100.0 * 6.398046493530273
Epoch 320, val loss: 1.5032312870025635
Epoch 330, training loss: 640.487060546875 = 1.4155464172363281 + 100.0 * 6.390715599060059
Epoch 330, val loss: 1.4786231517791748
Epoch 340, training loss: 640.1732177734375 = 1.3845587968826294 + 100.0 * 6.3878865242004395
Epoch 340, val loss: 1.453944444656372
Epoch 350, training loss: 639.5538940429688 = 1.3532662391662598 + 100.0 * 6.3820061683654785
Epoch 350, val loss: 1.4292504787445068
Epoch 360, training loss: 639.0 = 1.3221341371536255 + 100.0 * 6.376778602600098
Epoch 360, val loss: 1.4050586223602295
Epoch 370, training loss: 638.60302734375 = 1.2912347316741943 + 100.0 * 6.373117446899414
Epoch 370, val loss: 1.3812686204910278
Epoch 380, training loss: 638.2799682617188 = 1.2607487440109253 + 100.0 * 6.370192050933838
Epoch 380, val loss: 1.358303427696228
Epoch 390, training loss: 637.7403564453125 = 1.2308297157287598 + 100.0 * 6.365095138549805
Epoch 390, val loss: 1.3359321355819702
Epoch 400, training loss: 637.221923828125 = 1.2016675472259521 + 100.0 * 6.360202789306641
Epoch 400, val loss: 1.3144445419311523
Epoch 410, training loss: 637.5725708007812 = 1.1732969284057617 + 100.0 * 6.363993167877197
Epoch 410, val loss: 1.2939344644546509
Epoch 420, training loss: 636.7364501953125 = 1.1456509828567505 + 100.0 * 6.355908393859863
Epoch 420, val loss: 1.2745611667633057
Epoch 430, training loss: 636.1406860351562 = 1.1189014911651611 + 100.0 * 6.350217819213867
Epoch 430, val loss: 1.2561259269714355
Epoch 440, training loss: 636.075439453125 = 1.0930572748184204 + 100.0 * 6.349823474884033
Epoch 440, val loss: 1.2388246059417725
Epoch 450, training loss: 635.747802734375 = 1.0679880380630493 + 100.0 * 6.346797943115234
Epoch 450, val loss: 1.2223334312438965
Epoch 460, training loss: 635.1895751953125 = 1.043752670288086 + 100.0 * 6.341457843780518
Epoch 460, val loss: 1.2069523334503174
Epoch 470, training loss: 634.8773193359375 = 1.0204119682312012 + 100.0 * 6.338569164276123
Epoch 470, val loss: 1.1924984455108643
Epoch 480, training loss: 635.2106323242188 = 0.9977405667304993 + 100.0 * 6.342128753662109
Epoch 480, val loss: 1.1788723468780518
Epoch 490, training loss: 634.369140625 = 0.97576904296875 + 100.0 * 6.3339338302612305
Epoch 490, val loss: 1.1662148237228394
Epoch 500, training loss: 634.0870971679688 = 0.9544888734817505 + 100.0 * 6.331326484680176
Epoch 500, val loss: 1.1541316509246826
Epoch 510, training loss: 634.3284912109375 = 0.933971107006073 + 100.0 * 6.333945274353027
Epoch 510, val loss: 1.1429247856140137
Epoch 520, training loss: 633.6203002929688 = 0.9138982892036438 + 100.0 * 6.327064514160156
Epoch 520, val loss: 1.1324939727783203
Epoch 530, training loss: 633.2600708007812 = 0.8944686651229858 + 100.0 * 6.32365608215332
Epoch 530, val loss: 1.1227056980133057
Epoch 540, training loss: 634.1378173828125 = 0.8755052089691162 + 100.0 * 6.332623481750488
Epoch 540, val loss: 1.1134974956512451
Epoch 550, training loss: 632.83447265625 = 0.8569548726081848 + 100.0 * 6.319775581359863
Epoch 550, val loss: 1.1046305894851685
Epoch 560, training loss: 632.580322265625 = 0.8388616442680359 + 100.0 * 6.317414283752441
Epoch 560, val loss: 1.0964418649673462
Epoch 570, training loss: 633.2731323242188 = 0.8212507367134094 + 100.0 * 6.324519157409668
Epoch 570, val loss: 1.0889339447021484
Epoch 580, training loss: 632.430419921875 = 0.8038533329963684 + 100.0 * 6.316266059875488
Epoch 580, val loss: 1.0813714265823364
Epoch 590, training loss: 631.9745483398438 = 0.786839485168457 + 100.0 * 6.3118767738342285
Epoch 590, val loss: 1.074436068534851
Epoch 600, training loss: 632.8519287109375 = 0.7701724171638489 + 100.0 * 6.320817470550537
Epoch 600, val loss: 1.0677214860916138
Epoch 610, training loss: 631.7274169921875 = 0.7536340951919556 + 100.0 * 6.309737682342529
Epoch 610, val loss: 1.0616331100463867
Epoch 620, training loss: 631.4457397460938 = 0.7374218702316284 + 100.0 * 6.3070831298828125
Epoch 620, val loss: 1.0557591915130615
Epoch 630, training loss: 631.1713256835938 = 0.7214804887771606 + 100.0 * 6.304498195648193
Epoch 630, val loss: 1.0500590801239014
Epoch 640, training loss: 631.284423828125 = 0.7057552933692932 + 100.0 * 6.305786609649658
Epoch 640, val loss: 1.0447516441345215
Epoch 650, training loss: 631.945068359375 = 0.6901379227638245 + 100.0 * 6.312549591064453
Epoch 650, val loss: 1.0397039651870728
Epoch 660, training loss: 630.87255859375 = 0.6746772527694702 + 100.0 * 6.301978588104248
Epoch 660, val loss: 1.0346949100494385
Epoch 670, training loss: 630.5502319335938 = 0.6594565510749817 + 100.0 * 6.298907279968262
Epoch 670, val loss: 1.0302499532699585
Epoch 680, training loss: 630.3931274414062 = 0.6444679498672485 + 100.0 * 6.297486305236816
Epoch 680, val loss: 1.0259381532669067
Epoch 690, training loss: 630.9533081054688 = 0.6296760439872742 + 100.0 * 6.30323600769043
Epoch 690, val loss: 1.0217740535736084
Epoch 700, training loss: 630.5696411132812 = 0.614943265914917 + 100.0 * 6.29954719543457
Epoch 700, val loss: 1.0181794166564941
Epoch 710, training loss: 630.0533447265625 = 0.6004223227500916 + 100.0 * 6.294529438018799
Epoch 710, val loss: 1.0144931077957153
Epoch 720, training loss: 629.98828125 = 0.5861337780952454 + 100.0 * 6.2940216064453125
Epoch 720, val loss: 1.0111924409866333
Epoch 730, training loss: 629.73388671875 = 0.5720207095146179 + 100.0 * 6.291618347167969
Epoch 730, val loss: 1.0085787773132324
Epoch 740, training loss: 630.125732421875 = 0.5581051707267761 + 100.0 * 6.295676231384277
Epoch 740, val loss: 1.0058597326278687
Epoch 750, training loss: 629.7059936523438 = 0.5443167090415955 + 100.0 * 6.291616916656494
Epoch 750, val loss: 1.0034235715866089
Epoch 760, training loss: 629.6981811523438 = 0.5307068228721619 + 100.0 * 6.291674613952637
Epoch 760, val loss: 1.001402735710144
Epoch 770, training loss: 629.301513671875 = 0.5173108577728271 + 100.0 * 6.287841796875
Epoch 770, val loss: 0.999511182308197
Epoch 780, training loss: 629.0988159179688 = 0.5041338205337524 + 100.0 * 6.285946369171143
Epoch 780, val loss: 0.9983118176460266
Epoch 790, training loss: 629.0812377929688 = 0.49115583300590515 + 100.0 * 6.285900592803955
Epoch 790, val loss: 0.9971912503242493
Epoch 800, training loss: 629.01416015625 = 0.4783666431903839 + 100.0 * 6.28535795211792
Epoch 800, val loss: 0.9961575865745544
Epoch 810, training loss: 628.84765625 = 0.46572959423065186 + 100.0 * 6.28381872177124
Epoch 810, val loss: 0.9955470561981201
Epoch 820, training loss: 629.1500854492188 = 0.45338043570518494 + 100.0 * 6.2869672775268555
Epoch 820, val loss: 0.9950845241546631
Epoch 830, training loss: 628.5170288085938 = 0.44116348028182983 + 100.0 * 6.280758857727051
Epoch 830, val loss: 0.9950670003890991
Epoch 840, training loss: 628.379638671875 = 0.42921245098114014 + 100.0 * 6.279504299163818
Epoch 840, val loss: 0.995331346988678
Epoch 850, training loss: 628.3807373046875 = 0.4175367057323456 + 100.0 * 6.279632091522217
Epoch 850, val loss: 0.9957444667816162
Epoch 860, training loss: 628.697265625 = 0.4060510993003845 + 100.0 * 6.282911777496338
Epoch 860, val loss: 0.9963591694831848
Epoch 870, training loss: 628.38671875 = 0.39469173550605774 + 100.0 * 6.2799201011657715
Epoch 870, val loss: 0.9974420666694641
Epoch 880, training loss: 628.0699462890625 = 0.3836481273174286 + 100.0 * 6.276863098144531
Epoch 880, val loss: 0.9985785484313965
Epoch 890, training loss: 629.1327514648438 = 0.37283140420913696 + 100.0 * 6.287599563598633
Epoch 890, val loss: 1.0000262260437012
Epoch 900, training loss: 628.0259399414062 = 0.36220523715019226 + 100.0 * 6.276637554168701
Epoch 900, val loss: 1.001461386680603
Epoch 910, training loss: 627.7570190429688 = 0.35188212990760803 + 100.0 * 6.274051666259766
Epoch 910, val loss: 1.003494381904602
Epoch 920, training loss: 627.7938842773438 = 0.34178319573402405 + 100.0 * 6.2745208740234375
Epoch 920, val loss: 1.0057703256607056
Epoch 930, training loss: 627.8533935546875 = 0.33190733194351196 + 100.0 * 6.275214672088623
Epoch 930, val loss: 1.0077863931655884
Epoch 940, training loss: 627.451171875 = 0.32227960228919983 + 100.0 * 6.271289348602295
Epoch 940, val loss: 1.0103598833084106
Epoch 950, training loss: 627.6390991210938 = 0.3129165470600128 + 100.0 * 6.273261547088623
Epoch 950, val loss: 1.0131224393844604
Epoch 960, training loss: 627.3270874023438 = 0.3038044273853302 + 100.0 * 6.270232677459717
Epoch 960, val loss: 1.0159672498703003
Epoch 970, training loss: 627.3649291992188 = 0.2949678897857666 + 100.0 * 6.270699501037598
Epoch 970, val loss: 1.018831491470337
Epoch 980, training loss: 627.8212280273438 = 0.28635093569755554 + 100.0 * 6.275348663330078
Epoch 980, val loss: 1.0223162174224854
Epoch 990, training loss: 627.1631469726562 = 0.2779776155948639 + 100.0 * 6.2688517570495605
Epoch 990, val loss: 1.0256010293960571
Epoch 1000, training loss: 627.102294921875 = 0.2698819637298584 + 100.0 * 6.26832389831543
Epoch 1000, val loss: 1.0294244289398193
Epoch 1010, training loss: 627.15283203125 = 0.2620185315608978 + 100.0 * 6.2689080238342285
Epoch 1010, val loss: 1.0328474044799805
Epoch 1020, training loss: 627.03125 = 0.2544015645980835 + 100.0 * 6.267768383026123
Epoch 1020, val loss: 1.036766767501831
Epoch 1030, training loss: 626.7037353515625 = 0.24701310694217682 + 100.0 * 6.2645673751831055
Epoch 1030, val loss: 1.0408060550689697
Epoch 1040, training loss: 626.9541015625 = 0.23989766836166382 + 100.0 * 6.267142295837402
Epoch 1040, val loss: 1.0452321767807007
Epoch 1050, training loss: 626.9423828125 = 0.23293238878250122 + 100.0 * 6.267094612121582
Epoch 1050, val loss: 1.0493215322494507
Epoch 1060, training loss: 626.6799926757812 = 0.22624258697032928 + 100.0 * 6.264537334442139
Epoch 1060, val loss: 1.0536291599273682
Epoch 1070, training loss: 626.6944580078125 = 0.2197505384683609 + 100.0 * 6.264747142791748
Epoch 1070, val loss: 1.0583475828170776
Epoch 1080, training loss: 626.54296875 = 0.21347658336162567 + 100.0 * 6.263294696807861
Epoch 1080, val loss: 1.0629260540008545
Epoch 1090, training loss: 626.424560546875 = 0.2074120193719864 + 100.0 * 6.262171745300293
Epoch 1090, val loss: 1.0676698684692383
Epoch 1100, training loss: 626.3058471679688 = 0.20157115161418915 + 100.0 * 6.261042594909668
Epoch 1100, val loss: 1.0725305080413818
Epoch 1110, training loss: 627.0046997070312 = 0.1959693282842636 + 100.0 * 6.268087387084961
Epoch 1110, val loss: 1.0771814584732056
Epoch 1120, training loss: 626.3746948242188 = 0.19040697813034058 + 100.0 * 6.261842727661133
Epoch 1120, val loss: 1.0828838348388672
Epoch 1130, training loss: 626.1763916015625 = 0.1851341426372528 + 100.0 * 6.259912490844727
Epoch 1130, val loss: 1.0874501466751099
Epoch 1140, training loss: 626.638916015625 = 0.1800178438425064 + 100.0 * 6.264589309692383
Epoch 1140, val loss: 1.0930622816085815
Epoch 1150, training loss: 626.1664428710938 = 0.17506498098373413 + 100.0 * 6.259913921356201
Epoch 1150, val loss: 1.0978657007217407
Epoch 1160, training loss: 625.80078125 = 0.17026302218437195 + 100.0 * 6.256304740905762
Epoch 1160, val loss: 1.1032588481903076
Epoch 1170, training loss: 625.7665405273438 = 0.16565313935279846 + 100.0 * 6.256009101867676
Epoch 1170, val loss: 1.1086965799331665
Epoch 1180, training loss: 626.4299926757812 = 0.1612195074558258 + 100.0 * 6.262688159942627
Epoch 1180, val loss: 1.1140015125274658
Epoch 1190, training loss: 626.0252075195312 = 0.15684738755226135 + 100.0 * 6.258683681488037
Epoch 1190, val loss: 1.1198158264160156
Epoch 1200, training loss: 625.6618041992188 = 0.15264807641506195 + 100.0 * 6.255091667175293
Epoch 1200, val loss: 1.1248328685760498
Epoch 1210, training loss: 625.5181884765625 = 0.14859387278556824 + 100.0 * 6.253695487976074
Epoch 1210, val loss: 1.1309185028076172
Epoch 1220, training loss: 625.5587768554688 = 0.14470452070236206 + 100.0 * 6.254140377044678
Epoch 1220, val loss: 1.1364210844039917
Epoch 1230, training loss: 625.9852905273438 = 0.14091597497463226 + 100.0 * 6.258443355560303
Epoch 1230, val loss: 1.1418147087097168
Epoch 1240, training loss: 625.5776977539062 = 0.137213334441185 + 100.0 * 6.2544050216674805
Epoch 1240, val loss: 1.1475013494491577
Epoch 1250, training loss: 625.3443603515625 = 0.13364271819591522 + 100.0 * 6.2521071434021
Epoch 1250, val loss: 1.1533565521240234
Epoch 1260, training loss: 625.2312622070312 = 0.13022255897521973 + 100.0 * 6.251010417938232
Epoch 1260, val loss: 1.1591960191726685
Epoch 1270, training loss: 626.2105712890625 = 0.12691333889961243 + 100.0 * 6.260837078094482
Epoch 1270, val loss: 1.1647735834121704
Epoch 1280, training loss: 625.4715576171875 = 0.12364676594734192 + 100.0 * 6.25347900390625
Epoch 1280, val loss: 1.1709849834442139
Epoch 1290, training loss: 625.2843627929688 = 0.12051518261432648 + 100.0 * 6.251638412475586
Epoch 1290, val loss: 1.176594614982605
Epoch 1300, training loss: 625.792236328125 = 0.11750819534063339 + 100.0 * 6.256747245788574
Epoch 1300, val loss: 1.1823575496673584
Epoch 1310, training loss: 625.1321411132812 = 0.11453955620527267 + 100.0 * 6.250175952911377
Epoch 1310, val loss: 1.1884368658065796
Epoch 1320, training loss: 624.9483642578125 = 0.11169100552797318 + 100.0 * 6.248366832733154
Epoch 1320, val loss: 1.1943498849868774
Epoch 1330, training loss: 626.1969604492188 = 0.1089494526386261 + 100.0 * 6.260879993438721
Epoch 1330, val loss: 1.20014488697052
Epoch 1340, training loss: 625.1910400390625 = 0.10624270886182785 + 100.0 * 6.250847816467285
Epoch 1340, val loss: 1.2058223485946655
Epoch 1350, training loss: 624.8382568359375 = 0.10364553332328796 + 100.0 * 6.247345924377441
Epoch 1350, val loss: 1.2118685245513916
Epoch 1360, training loss: 624.7451171875 = 0.10113997757434845 + 100.0 * 6.2464399337768555
Epoch 1360, val loss: 1.217833399772644
Epoch 1370, training loss: 625.2645263671875 = 0.09874624013900757 + 100.0 * 6.251657485961914
Epoch 1370, val loss: 1.2232630252838135
Epoch 1380, training loss: 624.8993530273438 = 0.09631561487913132 + 100.0 * 6.248030185699463
Epoch 1380, val loss: 1.2296226024627686
Epoch 1390, training loss: 624.7649536132812 = 0.09402471035718918 + 100.0 * 6.246708869934082
Epoch 1390, val loss: 1.23511803150177
Epoch 1400, training loss: 624.6893310546875 = 0.09178788959980011 + 100.0 * 6.245975494384766
Epoch 1400, val loss: 1.2411702871322632
Epoch 1410, training loss: 624.9093017578125 = 0.08963598310947418 + 100.0 * 6.248196601867676
Epoch 1410, val loss: 1.2469903230667114
Epoch 1420, training loss: 624.5057373046875 = 0.08751410245895386 + 100.0 * 6.244182109832764
Epoch 1420, val loss: 1.2529420852661133
Epoch 1430, training loss: 625.2965698242188 = 0.08547790348529816 + 100.0 * 6.252110958099365
Epoch 1430, val loss: 1.2586138248443604
Epoch 1440, training loss: 625.064453125 = 0.08348643034696579 + 100.0 * 6.249809265136719
Epoch 1440, val loss: 1.264553427696228
Epoch 1450, training loss: 624.6143798828125 = 0.08153266459703445 + 100.0 * 6.245328426361084
Epoch 1450, val loss: 1.2701820135116577
Epoch 1460, training loss: 624.366943359375 = 0.07967228442430496 + 100.0 * 6.242872714996338
Epoch 1460, val loss: 1.2761868238449097
Epoch 1470, training loss: 624.2379150390625 = 0.077859066426754 + 100.0 * 6.241600513458252
Epoch 1470, val loss: 1.2819494009017944
Epoch 1480, training loss: 624.9505004882812 = 0.07611875981092453 + 100.0 * 6.248744010925293
Epoch 1480, val loss: 1.2877129316329956
Epoch 1490, training loss: 624.3583984375 = 0.074367456138134 + 100.0 * 6.242840766906738
Epoch 1490, val loss: 1.2933698892593384
Epoch 1500, training loss: 624.30615234375 = 0.07269711047410965 + 100.0 * 6.242334365844727
Epoch 1500, val loss: 1.2990779876708984
Epoch 1510, training loss: 624.6537475585938 = 0.07108768820762634 + 100.0 * 6.245826721191406
Epoch 1510, val loss: 1.3045405149459839
Epoch 1520, training loss: 624.1041259765625 = 0.06950308382511139 + 100.0 * 6.2403459548950195
Epoch 1520, val loss: 1.3104150295257568
Epoch 1530, training loss: 624.2228393554688 = 0.06797277927398682 + 100.0 * 6.241548538208008
Epoch 1530, val loss: 1.3159162998199463
Epoch 1540, training loss: 624.1926879882812 = 0.06649371236562729 + 100.0 * 6.241261959075928
Epoch 1540, val loss: 1.3215447664260864
Epoch 1550, training loss: 624.2186889648438 = 0.06504945456981659 + 100.0 * 6.2415361404418945
Epoch 1550, val loss: 1.3273943662643433
Epoch 1560, training loss: 624.2360229492188 = 0.0636284276843071 + 100.0 * 6.241724491119385
Epoch 1560, val loss: 1.3331310749053955
Epoch 1570, training loss: 623.8632202148438 = 0.06225180625915527 + 100.0 * 6.238009929656982
Epoch 1570, val loss: 1.3388383388519287
Epoch 1580, training loss: 623.7825317382812 = 0.060929086059331894 + 100.0 * 6.237215995788574
Epoch 1580, val loss: 1.3442609310150146
Epoch 1590, training loss: 623.9930419921875 = 0.05965724587440491 + 100.0 * 6.2393341064453125
Epoch 1590, val loss: 1.3498808145523071
Epoch 1600, training loss: 623.9910888671875 = 0.058389145880937576 + 100.0 * 6.2393269538879395
Epoch 1600, val loss: 1.355811357498169
Epoch 1610, training loss: 623.8974609375 = 0.05716114118695259 + 100.0 * 6.238402843475342
Epoch 1610, val loss: 1.3611191511154175
Epoch 1620, training loss: 624.0559692382812 = 0.05597992613911629 + 100.0 * 6.239999771118164
Epoch 1620, val loss: 1.3663164377212524
Epoch 1630, training loss: 623.6355590820312 = 0.05481445789337158 + 100.0 * 6.235807418823242
Epoch 1630, val loss: 1.3717097043991089
Epoch 1640, training loss: 624.0000610351562 = 0.053695645183324814 + 100.0 * 6.2394633293151855
Epoch 1640, val loss: 1.377458095550537
Epoch 1650, training loss: 624.11865234375 = 0.0526014119386673 + 100.0 * 6.240660667419434
Epoch 1650, val loss: 1.3825933933258057
Epoch 1660, training loss: 623.7349853515625 = 0.051526397466659546 + 100.0 * 6.23683500289917
Epoch 1660, val loss: 1.3881410360336304
Epoch 1670, training loss: 623.5492553710938 = 0.05049097165465355 + 100.0 * 6.234987735748291
Epoch 1670, val loss: 1.3934707641601562
Epoch 1680, training loss: 623.4917602539062 = 0.04948857054114342 + 100.0 * 6.23442268371582
Epoch 1680, val loss: 1.3989335298538208
Epoch 1690, training loss: 624.5130615234375 = 0.04853680729866028 + 100.0 * 6.244645595550537
Epoch 1690, val loss: 1.404127836227417
Epoch 1700, training loss: 623.8824462890625 = 0.04753755033016205 + 100.0 * 6.238348960876465
Epoch 1700, val loss: 1.4094940423965454
Epoch 1710, training loss: 623.4489135742188 = 0.04660365730524063 + 100.0 * 6.234023094177246
Epoch 1710, val loss: 1.4146769046783447
Epoch 1720, training loss: 623.962890625 = 0.04569490998983383 + 100.0 * 6.239171981811523
Epoch 1720, val loss: 1.4205257892608643
Epoch 1730, training loss: 623.4130859375 = 0.04480590671300888 + 100.0 * 6.233682632446289
Epoch 1730, val loss: 1.4251567125320435
Epoch 1740, training loss: 623.6295776367188 = 0.043946072459220886 + 100.0 * 6.235856533050537
Epoch 1740, val loss: 1.4304771423339844
Epoch 1750, training loss: 623.27685546875 = 0.04309704899787903 + 100.0 * 6.232337474822998
Epoch 1750, val loss: 1.4352697134017944
Epoch 1760, training loss: 623.5345458984375 = 0.04229072481393814 + 100.0 * 6.234922409057617
Epoch 1760, val loss: 1.440422773361206
Epoch 1770, training loss: 623.656005859375 = 0.04148860275745392 + 100.0 * 6.23614501953125
Epoch 1770, val loss: 1.4456390142440796
Epoch 1780, training loss: 623.352294921875 = 0.040706995874643326 + 100.0 * 6.2331156730651855
Epoch 1780, val loss: 1.4507533311843872
Epoch 1790, training loss: 623.2205810546875 = 0.03995594382286072 + 100.0 * 6.231805801391602
Epoch 1790, val loss: 1.4557958841323853
Epoch 1800, training loss: 623.733642578125 = 0.039223041385412216 + 100.0 * 6.23694372177124
Epoch 1800, val loss: 1.4606722593307495
Epoch 1810, training loss: 623.2905883789062 = 0.03848773241043091 + 100.0 * 6.232521057128906
Epoch 1810, val loss: 1.4661316871643066
Epoch 1820, training loss: 623.138671875 = 0.037790752947330475 + 100.0 * 6.231009006500244
Epoch 1820, val loss: 1.4708436727523804
Epoch 1830, training loss: 623.3264770507812 = 0.037111226469278336 + 100.0 * 6.232893466949463
Epoch 1830, val loss: 1.475911021232605
Epoch 1840, training loss: 623.4254150390625 = 0.036442551761865616 + 100.0 * 6.233890056610107
Epoch 1840, val loss: 1.480662226676941
Epoch 1850, training loss: 623.2286987304688 = 0.03579265996813774 + 100.0 * 6.231929302215576
Epoch 1850, val loss: 1.4852291345596313
Epoch 1860, training loss: 623.21533203125 = 0.03514878824353218 + 100.0 * 6.231801509857178
Epoch 1860, val loss: 1.4903801679611206
Epoch 1870, training loss: 623.1200561523438 = 0.03452072665095329 + 100.0 * 6.2308549880981445
Epoch 1870, val loss: 1.495455265045166
Epoch 1880, training loss: 623.0913696289062 = 0.033918414264917374 + 100.0 * 6.230574607849121
Epoch 1880, val loss: 1.499922275543213
Epoch 1890, training loss: 622.9563598632812 = 0.03333042562007904 + 100.0 * 6.2292304039001465
Epoch 1890, val loss: 1.5046463012695312
Epoch 1900, training loss: 622.9553833007812 = 0.03275848180055618 + 100.0 * 6.229226112365723
Epoch 1900, val loss: 1.5095083713531494
Epoch 1910, training loss: 623.0538330078125 = 0.032197922468185425 + 100.0 * 6.230216026306152
Epoch 1910, val loss: 1.5143252611160278
Epoch 1920, training loss: 623.4319458007812 = 0.03164626657962799 + 100.0 * 6.23400354385376
Epoch 1920, val loss: 1.5194035768508911
Epoch 1930, training loss: 623.1531372070312 = 0.031106237322092056 + 100.0 * 6.231220245361328
Epoch 1930, val loss: 1.5232552289962769
Epoch 1940, training loss: 622.9215698242188 = 0.030585788190364838 + 100.0 * 6.228909969329834
Epoch 1940, val loss: 1.527918815612793
Epoch 1950, training loss: 622.9829711914062 = 0.03007131814956665 + 100.0 * 6.229528903961182
Epoch 1950, val loss: 1.5322797298431396
Epoch 1960, training loss: 623.1018676757812 = 0.029583360999822617 + 100.0 * 6.230722427368164
Epoch 1960, val loss: 1.5369110107421875
Epoch 1970, training loss: 623.3380737304688 = 0.0290910042822361 + 100.0 * 6.233089447021484
Epoch 1970, val loss: 1.5419626235961914
Epoch 1980, training loss: 622.9833984375 = 0.028609370812773705 + 100.0 * 6.229547500610352
Epoch 1980, val loss: 1.5458601713180542
Epoch 1990, training loss: 622.6980590820312 = 0.028136665001511574 + 100.0 * 6.226699352264404
Epoch 1990, val loss: 1.5509252548217773
Epoch 2000, training loss: 622.545654296875 = 0.02768406644463539 + 100.0 * 6.225179195404053
Epoch 2000, val loss: 1.5551085472106934
Epoch 2010, training loss: 622.6339111328125 = 0.027246499434113503 + 100.0 * 6.226067066192627
Epoch 2010, val loss: 1.5594322681427002
Epoch 2020, training loss: 623.7097778320312 = 0.02682599611580372 + 100.0 * 6.2368292808532715
Epoch 2020, val loss: 1.5638647079467773
Epoch 2030, training loss: 623.2450561523438 = 0.02638649195432663 + 100.0 * 6.232186794281006
Epoch 2030, val loss: 1.568420171737671
Epoch 2040, training loss: 622.77001953125 = 0.02596396394073963 + 100.0 * 6.22744083404541
Epoch 2040, val loss: 1.5725998878479004
Epoch 2050, training loss: 622.6366577148438 = 0.025557976216077805 + 100.0 * 6.226110935211182
Epoch 2050, val loss: 1.5770992040634155
Epoch 2060, training loss: 622.6174926757812 = 0.025166567414999008 + 100.0 * 6.225923538208008
Epoch 2060, val loss: 1.581421971321106
Epoch 2070, training loss: 623.0610961914062 = 0.02478661574423313 + 100.0 * 6.230363368988037
Epoch 2070, val loss: 1.5854954719543457
Epoch 2080, training loss: 622.58837890625 = 0.02440277300775051 + 100.0 * 6.225639343261719
Epoch 2080, val loss: 1.589755892753601
Epoch 2090, training loss: 622.4691162109375 = 0.024029644206166267 + 100.0 * 6.224450588226318
Epoch 2090, val loss: 1.5937597751617432
Epoch 2100, training loss: 623.2904052734375 = 0.023683562874794006 + 100.0 * 6.232666969299316
Epoch 2100, val loss: 1.5977882146835327
Epoch 2110, training loss: 622.5332641601562 = 0.023310160264372826 + 100.0 * 6.225099563598633
Epoch 2110, val loss: 1.602361798286438
Epoch 2120, training loss: 622.4231567382812 = 0.022962013259530067 + 100.0 * 6.224001884460449
Epoch 2120, val loss: 1.606185793876648
Epoch 2130, training loss: 622.3269653320312 = 0.022625617682933807 + 100.0 * 6.223043441772461
Epoch 2130, val loss: 1.6105499267578125
Epoch 2140, training loss: 622.3004760742188 = 0.022300228476524353 + 100.0 * 6.222782135009766
Epoch 2140, val loss: 1.6147689819335938
Epoch 2150, training loss: 623.3255615234375 = 0.021983085200190544 + 100.0 * 6.233036041259766
Epoch 2150, val loss: 1.6193952560424805
Epoch 2160, training loss: 622.6974487304688 = 0.021660346537828445 + 100.0 * 6.226758003234863
Epoch 2160, val loss: 1.622277021408081
Epoch 2170, training loss: 622.519287109375 = 0.02134351059794426 + 100.0 * 6.224979400634766
Epoch 2170, val loss: 1.6267552375793457
Epoch 2180, training loss: 622.4654541015625 = 0.021051377058029175 + 100.0 * 6.2244439125061035
Epoch 2180, val loss: 1.6298737525939941
Epoch 2190, training loss: 622.5802001953125 = 0.02075011655688286 + 100.0 * 6.225594520568848
Epoch 2190, val loss: 1.6343085765838623
Epoch 2200, training loss: 622.2551879882812 = 0.02045596018433571 + 100.0 * 6.222347259521484
Epoch 2200, val loss: 1.638322353363037
Epoch 2210, training loss: 622.2113647460938 = 0.02017279341816902 + 100.0 * 6.221911907196045
Epoch 2210, val loss: 1.6427773237228394
Epoch 2220, training loss: 622.4906005859375 = 0.019900081679224968 + 100.0 * 6.224707126617432
Epoch 2220, val loss: 1.6466742753982544
Epoch 2230, training loss: 622.8661499023438 = 0.01962953992187977 + 100.0 * 6.2284650802612305
Epoch 2230, val loss: 1.6497690677642822
Epoch 2240, training loss: 622.2593994140625 = 0.019353389739990234 + 100.0 * 6.222400665283203
Epoch 2240, val loss: 1.653855800628662
Epoch 2250, training loss: 622.1055908203125 = 0.019085317850112915 + 100.0 * 6.220864772796631
Epoch 2250, val loss: 1.6580123901367188
Epoch 2260, training loss: 622.0360107421875 = 0.018835410475730896 + 100.0 * 6.2201714515686035
Epoch 2260, val loss: 1.6616692543029785
Epoch 2270, training loss: 622.2688598632812 = 0.01858695223927498 + 100.0 * 6.222502708435059
Epoch 2270, val loss: 1.6659730672836304
Epoch 2280, training loss: 622.3804931640625 = 0.018339527770876884 + 100.0 * 6.223621845245361
Epoch 2280, val loss: 1.6691128015518188
Epoch 2290, training loss: 622.6511840820312 = 0.018097173422574997 + 100.0 * 6.226330757141113
Epoch 2290, val loss: 1.6732077598571777
Epoch 2300, training loss: 622.2142333984375 = 0.017851529642939568 + 100.0 * 6.221964359283447
Epoch 2300, val loss: 1.676318645477295
Epoch 2310, training loss: 621.995361328125 = 0.017619920894503593 + 100.0 * 6.2197771072387695
Epoch 2310, val loss: 1.6801766157150269
Epoch 2320, training loss: 621.9943237304688 = 0.017397107556462288 + 100.0 * 6.219769477844238
Epoch 2320, val loss: 1.683995246887207
Epoch 2330, training loss: 622.2221069335938 = 0.01718076504766941 + 100.0 * 6.222049713134766
Epoch 2330, val loss: 1.6874369382858276
Epoch 2340, training loss: 622.2814331054688 = 0.01696045882999897 + 100.0 * 6.222644805908203
Epoch 2340, val loss: 1.6911931037902832
Epoch 2350, training loss: 622.058349609375 = 0.016740333288908005 + 100.0 * 6.220416069030762
Epoch 2350, val loss: 1.6953637599945068
Epoch 2360, training loss: 622.2230834960938 = 0.016533421352505684 + 100.0 * 6.222065448760986
Epoch 2360, val loss: 1.6987415552139282
Epoch 2370, training loss: 622.488525390625 = 0.01632925495505333 + 100.0 * 6.224721908569336
Epoch 2370, val loss: 1.7022373676300049
Epoch 2380, training loss: 622.0675048828125 = 0.01612178422510624 + 100.0 * 6.220513820648193
Epoch 2380, val loss: 1.705470085144043
Epoch 2390, training loss: 621.8544921875 = 0.015924569219350815 + 100.0 * 6.218385696411133
Epoch 2390, val loss: 1.7090566158294678
Epoch 2400, training loss: 621.8814697265625 = 0.015733899548649788 + 100.0 * 6.218657493591309
Epoch 2400, val loss: 1.71262788772583
Epoch 2410, training loss: 622.3281860351562 = 0.015550311654806137 + 100.0 * 6.223126411437988
Epoch 2410, val loss: 1.7157440185546875
Epoch 2420, training loss: 622.3480224609375 = 0.015356910414993763 + 100.0 * 6.223326206207275
Epoch 2420, val loss: 1.7192895412445068
Epoch 2430, training loss: 622.0823974609375 = 0.015168527141213417 + 100.0 * 6.220672130584717
Epoch 2430, val loss: 1.723228096961975
Epoch 2440, training loss: 621.7657470703125 = 0.014985254034399986 + 100.0 * 6.217507362365723
Epoch 2440, val loss: 1.7262946367263794
Epoch 2450, training loss: 621.8365478515625 = 0.014808245934545994 + 100.0 * 6.218217372894287
Epoch 2450, val loss: 1.7302054166793823
Epoch 2460, training loss: 622.2870483398438 = 0.014640572480857372 + 100.0 * 6.222723960876465
Epoch 2460, val loss: 1.7336065769195557
Epoch 2470, training loss: 621.94482421875 = 0.014467681758105755 + 100.0 * 6.219303607940674
Epoch 2470, val loss: 1.7363349199295044
Epoch 2480, training loss: 621.9509887695312 = 0.014299181289970875 + 100.0 * 6.219367027282715
Epoch 2480, val loss: 1.739725112915039
Epoch 2490, training loss: 621.80224609375 = 0.014131327159702778 + 100.0 * 6.217880725860596
Epoch 2490, val loss: 1.743285059928894
Epoch 2500, training loss: 622.5222778320312 = 0.013977988623082638 + 100.0 * 6.225082874298096
Epoch 2500, val loss: 1.746118187904358
Epoch 2510, training loss: 621.6588745117188 = 0.013808759860694408 + 100.0 * 6.2164506912231445
Epoch 2510, val loss: 1.7501152753829956
Epoch 2520, training loss: 621.6279907226562 = 0.013655364513397217 + 100.0 * 6.2161431312561035
Epoch 2520, val loss: 1.7530486583709717
Epoch 2530, training loss: 621.5945434570312 = 0.013503573834896088 + 100.0 * 6.215810298919678
Epoch 2530, val loss: 1.756554365158081
Epoch 2540, training loss: 621.7090454101562 = 0.013357894495129585 + 100.0 * 6.216957092285156
Epoch 2540, val loss: 1.7599389553070068
Epoch 2550, training loss: 622.5026245117188 = 0.013217378407716751 + 100.0 * 6.224894046783447
Epoch 2550, val loss: 1.7629321813583374
Epoch 2560, training loss: 621.9965209960938 = 0.013065509498119354 + 100.0 * 6.219834804534912
Epoch 2560, val loss: 1.7660750150680542
Epoch 2570, training loss: 621.7612915039062 = 0.012922092340886593 + 100.0 * 6.2174835205078125
Epoch 2570, val loss: 1.7695170640945435
Epoch 2580, training loss: 621.9593505859375 = 0.012782146222889423 + 100.0 * 6.219466209411621
Epoch 2580, val loss: 1.7728615999221802
Epoch 2590, training loss: 621.8056640625 = 0.01264270395040512 + 100.0 * 6.217930316925049
Epoch 2590, val loss: 1.7757267951965332
Epoch 2600, training loss: 621.7151489257812 = 0.012507051229476929 + 100.0 * 6.217026233673096
Epoch 2600, val loss: 1.7784711122512817
Epoch 2610, training loss: 621.5819091796875 = 0.012374594807624817 + 100.0 * 6.215694904327393
Epoch 2610, val loss: 1.782133936882019
Epoch 2620, training loss: 621.7636108398438 = 0.01224828977137804 + 100.0 * 6.217513561248779
Epoch 2620, val loss: 1.7848397493362427
Epoch 2630, training loss: 621.6707763671875 = 0.012120656669139862 + 100.0 * 6.216586112976074
Epoch 2630, val loss: 1.7874590158462524
Epoch 2640, training loss: 621.8548583984375 = 0.011999025009572506 + 100.0 * 6.218428134918213
Epoch 2640, val loss: 1.7902363538742065
Epoch 2650, training loss: 621.8555297851562 = 0.011871390044689178 + 100.0 * 6.218436241149902
Epoch 2650, val loss: 1.793514609336853
Epoch 2660, training loss: 621.7968139648438 = 0.011744189076125622 + 100.0 * 6.217851161956787
Epoch 2660, val loss: 1.797226905822754
Epoch 2670, training loss: 621.7308959960938 = 0.011628039181232452 + 100.0 * 6.21719217300415
Epoch 2670, val loss: 1.7996751070022583
Epoch 2680, training loss: 621.4425048828125 = 0.011508124880492687 + 100.0 * 6.2143096923828125
Epoch 2680, val loss: 1.8026915788650513
Epoch 2690, training loss: 621.3507080078125 = 0.01139412447810173 + 100.0 * 6.213393211364746
Epoch 2690, val loss: 1.8058120012283325
Epoch 2700, training loss: 621.4796752929688 = 0.011284885928034782 + 100.0 * 6.214683532714844
Epoch 2700, val loss: 1.8084688186645508
Epoch 2710, training loss: 621.9575805664062 = 0.011178886517882347 + 100.0 * 6.21946382522583
Epoch 2710, val loss: 1.8107088804244995
Epoch 2720, training loss: 621.736083984375 = 0.011060505174100399 + 100.0 * 6.217250347137451
Epoch 2720, val loss: 1.8146144151687622
Epoch 2730, training loss: 621.862548828125 = 0.010950129479169846 + 100.0 * 6.2185163497924805
Epoch 2730, val loss: 1.8174906969070435
Epoch 2740, training loss: 621.5653076171875 = 0.010844268836081028 + 100.0 * 6.215544700622559
Epoch 2740, val loss: 1.819889783859253
Epoch 2750, training loss: 621.3674926757812 = 0.010736116208136082 + 100.0 * 6.21356725692749
Epoch 2750, val loss: 1.822816014289856
Epoch 2760, training loss: 621.3218994140625 = 0.010634569451212883 + 100.0 * 6.213112831115723
Epoch 2760, val loss: 1.8255999088287354
Epoch 2770, training loss: 621.4161376953125 = 0.010534408502280712 + 100.0 * 6.21405553817749
Epoch 2770, val loss: 1.828640103340149
Epoch 2780, training loss: 621.9940795898438 = 0.010437313467264175 + 100.0 * 6.219836235046387
Epoch 2780, val loss: 1.8311396837234497
Epoch 2790, training loss: 621.5367431640625 = 0.010337385348975658 + 100.0 * 6.215264320373535
Epoch 2790, val loss: 1.834398627281189
Epoch 2800, training loss: 621.3301391601562 = 0.010236057452857494 + 100.0 * 6.213198661804199
Epoch 2800, val loss: 1.8369630575180054
Epoch 2810, training loss: 621.4512939453125 = 0.010146035812795162 + 100.0 * 6.214411735534668
Epoch 2810, val loss: 1.839766025543213
Epoch 2820, training loss: 621.9259033203125 = 0.010052901692688465 + 100.0 * 6.219158172607422
Epoch 2820, val loss: 1.841858983039856
Epoch 2830, training loss: 621.4521484375 = 0.00995782669633627 + 100.0 * 6.214422225952148
Epoch 2830, val loss: 1.8446831703186035
Epoch 2840, training loss: 621.2437133789062 = 0.009863479994237423 + 100.0 * 6.212337970733643
Epoch 2840, val loss: 1.8475984334945679
Epoch 2850, training loss: 621.136474609375 = 0.009776480495929718 + 100.0 * 6.211266994476318
Epoch 2850, val loss: 1.8503810167312622
Epoch 2860, training loss: 621.1716918945312 = 0.009690661914646626 + 100.0 * 6.211619853973389
Epoch 2860, val loss: 1.8535113334655762
Epoch 2870, training loss: 622.3450927734375 = 0.009608662687242031 + 100.0 * 6.223354816436768
Epoch 2870, val loss: 1.8566560745239258
Epoch 2880, training loss: 621.6306762695312 = 0.00952238030731678 + 100.0 * 6.216211318969727
Epoch 2880, val loss: 1.8573601245880127
Epoch 2890, training loss: 621.3099365234375 = 0.009432587772607803 + 100.0 * 6.2130045890808105
Epoch 2890, val loss: 1.860894799232483
Epoch 2900, training loss: 621.8455200195312 = 0.009357260540127754 + 100.0 * 6.218361854553223
Epoch 2900, val loss: 1.862856388092041
Epoch 2910, training loss: 621.0516967773438 = 0.009267324581742287 + 100.0 * 6.210424423217773
Epoch 2910, val loss: 1.8658802509307861
Epoch 2920, training loss: 621.060791015625 = 0.009187152609229088 + 100.0 * 6.210515975952148
Epoch 2920, val loss: 1.8682563304901123
Epoch 2930, training loss: 621.06591796875 = 0.009110156446695328 + 100.0 * 6.210567951202393
Epoch 2930, val loss: 1.8708497285842896
Epoch 2940, training loss: 621.0107421875 = 0.009033085778355598 + 100.0 * 6.210017204284668
Epoch 2940, val loss: 1.8738442659378052
Epoch 2950, training loss: 622.3853759765625 = 0.008959942497313023 + 100.0 * 6.223763942718506
Epoch 2950, val loss: 1.8767013549804688
Epoch 2960, training loss: 621.6852416992188 = 0.008883507922291756 + 100.0 * 6.216763496398926
Epoch 2960, val loss: 1.8780475854873657
Epoch 2970, training loss: 621.2196655273438 = 0.008804106153547764 + 100.0 * 6.212108612060547
Epoch 2970, val loss: 1.8810949325561523
Epoch 2980, training loss: 621.0504150390625 = 0.008731985464692116 + 100.0 * 6.210416793823242
Epoch 2980, val loss: 1.8833659887313843
Epoch 2990, training loss: 621.4386596679688 = 0.008661269210278988 + 100.0 * 6.21429967880249
Epoch 2990, val loss: 1.8863693475723267
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 861.628662109375 = 1.9440046548843384 + 100.0 * 8.596846580505371
Epoch 0, val loss: 1.9460928440093994
Epoch 10, training loss: 861.5405883789062 = 1.9349751472473145 + 100.0 * 8.59605598449707
Epoch 10, val loss: 1.9376754760742188
Epoch 20, training loss: 860.9849243164062 = 1.9237070083618164 + 100.0 * 8.590612411499023
Epoch 20, val loss: 1.9267297983169556
Epoch 30, training loss: 857.1740112304688 = 1.9089709520339966 + 100.0 * 8.552650451660156
Epoch 30, val loss: 1.9119423627853394
Epoch 40, training loss: 830.9491577148438 = 1.8914695978164673 + 100.0 * 8.290576934814453
Epoch 40, val loss: 1.894360899925232
Epoch 50, training loss: 757.3920288085938 = 1.872200846672058 + 100.0 * 7.5551981925964355
Epoch 50, val loss: 1.8753595352172852
Epoch 60, training loss: 733.04638671875 = 1.8574934005737305 + 100.0 * 7.311889171600342
Epoch 60, val loss: 1.8611505031585693
Epoch 70, training loss: 715.4685668945312 = 1.8453797101974487 + 100.0 * 7.136231899261475
Epoch 70, val loss: 1.8493820428848267
Epoch 80, training loss: 705.8363647460938 = 1.8336067199707031 + 100.0 * 7.040027618408203
Epoch 80, val loss: 1.83773672580719
Epoch 90, training loss: 699.015869140625 = 1.8243659734725952 + 100.0 * 6.971915245056152
Epoch 90, val loss: 1.8287608623504639
Epoch 100, training loss: 691.8334350585938 = 1.8164424896240234 + 100.0 * 6.900169849395752
Epoch 100, val loss: 1.8208386898040771
Epoch 110, training loss: 683.8563232421875 = 1.8093806505203247 + 100.0 * 6.820469379425049
Epoch 110, val loss: 1.81392502784729
Epoch 120, training loss: 677.9191284179688 = 1.8029963970184326 + 100.0 * 6.7611613273620605
Epoch 120, val loss: 1.8071709871292114
Epoch 130, training loss: 673.2125854492188 = 1.795445442199707 + 100.0 * 6.714171409606934
Epoch 130, val loss: 1.799414038658142
Epoch 140, training loss: 669.3610229492188 = 1.787528157234192 + 100.0 * 6.675734996795654
Epoch 140, val loss: 1.7913868427276611
Epoch 150, training loss: 665.8349609375 = 1.779314398765564 + 100.0 * 6.640556335449219
Epoch 150, val loss: 1.78329598903656
Epoch 160, training loss: 662.9396362304688 = 1.7706713676452637 + 100.0 * 6.611689567565918
Epoch 160, val loss: 1.7749930620193481
Epoch 170, training loss: 660.602783203125 = 1.7614442110061646 + 100.0 * 6.588413238525391
Epoch 170, val loss: 1.7663084268569946
Epoch 180, training loss: 658.4324340820312 = 1.7515287399291992 + 100.0 * 6.566809177398682
Epoch 180, val loss: 1.7572458982467651
Epoch 190, training loss: 656.5978393554688 = 1.740865707397461 + 100.0 * 6.548569679260254
Epoch 190, val loss: 1.747584342956543
Epoch 200, training loss: 654.9501342773438 = 1.7290239334106445 + 100.0 * 6.5322113037109375
Epoch 200, val loss: 1.7371431589126587
Epoch 210, training loss: 653.3657836914062 = 1.7163259983062744 + 100.0 * 6.5164947509765625
Epoch 210, val loss: 1.7258762121200562
Epoch 220, training loss: 651.941162109375 = 1.7025479078292847 + 100.0 * 6.50238561630249
Epoch 220, val loss: 1.7137072086334229
Epoch 230, training loss: 650.8197021484375 = 1.687406063079834 + 100.0 * 6.491323471069336
Epoch 230, val loss: 1.700518012046814
Epoch 240, training loss: 649.6226196289062 = 1.6711714267730713 + 100.0 * 6.4795145988464355
Epoch 240, val loss: 1.6862950325012207
Epoch 250, training loss: 648.4425659179688 = 1.6535615921020508 + 100.0 * 6.46789026260376
Epoch 250, val loss: 1.6711045503616333
Epoch 260, training loss: 647.5003051757812 = 1.6346497535705566 + 100.0 * 6.4586567878723145
Epoch 260, val loss: 1.6548376083374023
Epoch 270, training loss: 646.9739990234375 = 1.6143144369125366 + 100.0 * 6.453597068786621
Epoch 270, val loss: 1.6374130249023438
Epoch 280, training loss: 645.7408447265625 = 1.5927125215530396 + 100.0 * 6.441481113433838
Epoch 280, val loss: 1.6189658641815186
Epoch 290, training loss: 644.9462280273438 = 1.5698399543762207 + 100.0 * 6.4337639808654785
Epoch 290, val loss: 1.5996301174163818
Epoch 300, training loss: 644.6152954101562 = 1.5455986261367798 + 100.0 * 6.430696964263916
Epoch 300, val loss: 1.5790953636169434
Epoch 310, training loss: 643.7147216796875 = 1.5203306674957275 + 100.0 * 6.4219441413879395
Epoch 310, val loss: 1.5581315755844116
Epoch 320, training loss: 642.9833374023438 = 1.4941420555114746 + 100.0 * 6.414891719818115
Epoch 320, val loss: 1.5364630222320557
Epoch 330, training loss: 642.328125 = 1.4671763181686401 + 100.0 * 6.408609867095947
Epoch 330, val loss: 1.5144953727722168
Epoch 340, training loss: 641.708984375 = 1.4395819902420044 + 100.0 * 6.402694225311279
Epoch 340, val loss: 1.4921842813491821
Epoch 350, training loss: 641.526123046875 = 1.411542296409607 + 100.0 * 6.4011454582214355
Epoch 350, val loss: 1.4695290327072144
Epoch 360, training loss: 640.7261352539062 = 1.3826411962509155 + 100.0 * 6.393435001373291
Epoch 360, val loss: 1.446670651435852
Epoch 370, training loss: 640.2949829101562 = 1.3538020849227905 + 100.0 * 6.389411926269531
Epoch 370, val loss: 1.4240777492523193
Epoch 380, training loss: 639.6441650390625 = 1.3248777389526367 + 100.0 * 6.383193016052246
Epoch 380, val loss: 1.401729702949524
Epoch 390, training loss: 639.41015625 = 1.296068787574768 + 100.0 * 6.38114070892334
Epoch 390, val loss: 1.379657506942749
Epoch 400, training loss: 639.4769287109375 = 1.2673934698104858 + 100.0 * 6.3820953369140625
Epoch 400, val loss: 1.3574631214141846
Epoch 410, training loss: 638.5360107421875 = 1.238733172416687 + 100.0 * 6.3729729652404785
Epoch 410, val loss: 1.3358805179595947
Epoch 420, training loss: 637.9550170898438 = 1.2107257843017578 + 100.0 * 6.367442607879639
Epoch 420, val loss: 1.3149861097335815
Epoch 430, training loss: 637.6968383789062 = 1.1832292079925537 + 100.0 * 6.36513614654541
Epoch 430, val loss: 1.2946301698684692
Epoch 440, training loss: 637.11572265625 = 1.1560181379318237 + 100.0 * 6.359597206115723
Epoch 440, val loss: 1.274699091911316
Epoch 450, training loss: 637.202392578125 = 1.129364013671875 + 100.0 * 6.3607306480407715
Epoch 450, val loss: 1.2553291320800781
Epoch 460, training loss: 636.56005859375 = 1.1030935049057007 + 100.0 * 6.354569911956787
Epoch 460, val loss: 1.2362109422683716
Epoch 470, training loss: 636.1788330078125 = 1.0774177312850952 + 100.0 * 6.351014137268066
Epoch 470, val loss: 1.2177073955535889
Epoch 480, training loss: 635.7996826171875 = 1.0521785020828247 + 100.0 * 6.347475051879883
Epoch 480, val loss: 1.1999156475067139
Epoch 490, training loss: 635.4298095703125 = 1.027588963508606 + 100.0 * 6.344022274017334
Epoch 490, val loss: 1.182444453239441
Epoch 500, training loss: 636.1600952148438 = 1.0033354759216309 + 100.0 * 6.351567268371582
Epoch 500, val loss: 1.165587306022644
Epoch 510, training loss: 634.85498046875 = 0.9792466759681702 + 100.0 * 6.338757514953613
Epoch 510, val loss: 1.1484147310256958
Epoch 520, training loss: 634.8054809570312 = 0.955656111240387 + 100.0 * 6.338498592376709
Epoch 520, val loss: 1.1319352388381958
Epoch 530, training loss: 634.494384765625 = 0.9325845241546631 + 100.0 * 6.335618019104004
Epoch 530, val loss: 1.1159236431121826
Epoch 540, training loss: 634.124267578125 = 0.9099287986755371 + 100.0 * 6.332143783569336
Epoch 540, val loss: 1.1001900434494019
Epoch 550, training loss: 633.8421630859375 = 0.8877341151237488 + 100.0 * 6.3295440673828125
Epoch 550, val loss: 1.0853084325790405
Epoch 560, training loss: 633.95166015625 = 0.8659835457801819 + 100.0 * 6.330856800079346
Epoch 560, val loss: 1.0705753564834595
Epoch 570, training loss: 633.7167358398438 = 0.8444156646728516 + 100.0 * 6.328723430633545
Epoch 570, val loss: 1.0561538934707642
Epoch 580, training loss: 633.3518676757812 = 0.8232928514480591 + 100.0 * 6.325285911560059
Epoch 580, val loss: 1.0422332286834717
Epoch 590, training loss: 632.99951171875 = 0.8024914264678955 + 100.0 * 6.321970462799072
Epoch 590, val loss: 1.029011845588684
Epoch 600, training loss: 632.6624145507812 = 0.7823177576065063 + 100.0 * 6.318801403045654
Epoch 600, val loss: 1.016207218170166
Epoch 610, training loss: 633.3871459960938 = 0.7626089453697205 + 100.0 * 6.326245307922363
Epoch 610, val loss: 1.0039695501327515
Epoch 620, training loss: 632.389404296875 = 0.742864191532135 + 100.0 * 6.316465377807617
Epoch 620, val loss: 0.991808295249939
Epoch 630, training loss: 632.1234130859375 = 0.7237035632133484 + 100.0 * 6.313997268676758
Epoch 630, val loss: 0.9804016947746277
Epoch 640, training loss: 631.8599243164062 = 0.7051379680633545 + 100.0 * 6.311547756195068
Epoch 640, val loss: 0.9697031378746033
Epoch 650, training loss: 631.9508666992188 = 0.6870018243789673 + 100.0 * 6.312638282775879
Epoch 650, val loss: 0.9595560431480408
Epoch 660, training loss: 631.662841796875 = 0.6689102053642273 + 100.0 * 6.309939384460449
Epoch 660, val loss: 0.949735164642334
Epoch 670, training loss: 631.4725341796875 = 0.6514043211936951 + 100.0 * 6.308211326599121
Epoch 670, val loss: 0.9406130909919739
Epoch 680, training loss: 631.113525390625 = 0.6342135667800903 + 100.0 * 6.304792881011963
Epoch 680, val loss: 0.9322634339332581
Epoch 690, training loss: 631.619140625 = 0.6174671053886414 + 100.0 * 6.310016632080078
Epoch 690, val loss: 0.9244882464408875
Epoch 700, training loss: 631.1008911132812 = 0.6010167002677917 + 100.0 * 6.304998874664307
Epoch 700, val loss: 0.9169332981109619
Epoch 710, training loss: 630.7036743164062 = 0.5848706364631653 + 100.0 * 6.301187992095947
Epoch 710, val loss: 0.9102628827095032
Epoch 720, training loss: 630.7600708007812 = 0.5692128539085388 + 100.0 * 6.301908493041992
Epoch 720, val loss: 0.9041693210601807
Epoch 730, training loss: 630.62451171875 = 0.5538403391838074 + 100.0 * 6.30070686340332
Epoch 730, val loss: 0.8983845114707947
Epoch 740, training loss: 630.2146606445312 = 0.5388457179069519 + 100.0 * 6.29675817489624
Epoch 740, val loss: 0.8934332132339478
Epoch 750, training loss: 630.2720336914062 = 0.5242009162902832 + 100.0 * 6.297477722167969
Epoch 750, val loss: 0.8886746168136597
Epoch 760, training loss: 630.0759887695312 = 0.5098428726196289 + 100.0 * 6.295661926269531
Epoch 760, val loss: 0.8844798803329468
Epoch 770, training loss: 629.8446044921875 = 0.49590539932250977 + 100.0 * 6.293487071990967
Epoch 770, val loss: 0.8808130621910095
Epoch 780, training loss: 629.547119140625 = 0.4823521673679352 + 100.0 * 6.290647506713867
Epoch 780, val loss: 0.8777622580528259
Epoch 790, training loss: 629.70068359375 = 0.46914738416671753 + 100.0 * 6.2923150062561035
Epoch 790, val loss: 0.8751691579818726
Epoch 800, training loss: 629.6199951171875 = 0.4562065601348877 + 100.0 * 6.291637897491455
Epoch 800, val loss: 0.8727541565895081
Epoch 810, training loss: 629.4197387695312 = 0.44345083832740784 + 100.0 * 6.289763450622559
Epoch 810, val loss: 0.8707640767097473
Epoch 820, training loss: 629.0687255859375 = 0.43122637271881104 + 100.0 * 6.286375045776367
Epoch 820, val loss: 0.8694403171539307
Epoch 830, training loss: 629.1596069335938 = 0.41935786604881287 + 100.0 * 6.287402153015137
Epoch 830, val loss: 0.868444561958313
Epoch 840, training loss: 629.0086669921875 = 0.407713383436203 + 100.0 * 6.286009311676025
Epoch 840, val loss: 0.8678089380264282
Epoch 850, training loss: 628.9254760742188 = 0.3964458107948303 + 100.0 * 6.285290241241455
Epoch 850, val loss: 0.8675171732902527
Epoch 860, training loss: 628.6533203125 = 0.38541996479034424 + 100.0 * 6.282678604125977
Epoch 860, val loss: 0.8675112128257751
Epoch 870, training loss: 628.5242309570312 = 0.37479835748672485 + 100.0 * 6.281494140625
Epoch 870, val loss: 0.8680514693260193
Epoch 880, training loss: 628.3723754882812 = 0.36446335911750793 + 100.0 * 6.280079364776611
Epoch 880, val loss: 0.8687770962715149
Epoch 890, training loss: 629.0245361328125 = 0.3544023633003235 + 100.0 * 6.286701202392578
Epoch 890, val loss: 0.8697355389595032
Epoch 900, training loss: 628.2069702148438 = 0.3445608615875244 + 100.0 * 6.278624534606934
Epoch 900, val loss: 0.8708969354629517
Epoch 910, training loss: 628.4219970703125 = 0.33505791425704956 + 100.0 * 6.280869007110596
Epoch 910, val loss: 0.8723956346511841
Epoch 920, training loss: 627.9517822265625 = 0.32576751708984375 + 100.0 * 6.2762603759765625
Epoch 920, val loss: 0.8740226030349731
Epoch 930, training loss: 628.1077270507812 = 0.31687578558921814 + 100.0 * 6.2779083251953125
Epoch 930, val loss: 0.8759832382202148
Epoch 940, training loss: 627.9395751953125 = 0.3082246482372284 + 100.0 * 6.276313781738281
Epoch 940, val loss: 0.8783659934997559
Epoch 950, training loss: 627.87646484375 = 0.29980915784835815 + 100.0 * 6.275766849517822
Epoch 950, val loss: 0.8806083798408508
Epoch 960, training loss: 627.669189453125 = 0.2915925085544586 + 100.0 * 6.273776531219482
Epoch 960, val loss: 0.8831152319908142
Epoch 970, training loss: 627.5896606445312 = 0.28375136852264404 + 100.0 * 6.273059368133545
Epoch 970, val loss: 0.8862266540527344
Epoch 980, training loss: 627.6701049804688 = 0.2760850489139557 + 100.0 * 6.273940563201904
Epoch 980, val loss: 0.8891110420227051
Epoch 990, training loss: 627.51904296875 = 0.2686527669429779 + 100.0 * 6.272503852844238
Epoch 990, val loss: 0.8922891616821289
Epoch 1000, training loss: 627.2879638671875 = 0.2614012658596039 + 100.0 * 6.270265579223633
Epoch 1000, val loss: 0.895576536655426
Epoch 1010, training loss: 627.0870971679688 = 0.25440528988838196 + 100.0 * 6.268327236175537
Epoch 1010, val loss: 0.8989881873130798
Epoch 1020, training loss: 627.0255126953125 = 0.247677281498909 + 100.0 * 6.267778396606445
Epoch 1020, val loss: 0.9027010202407837
Epoch 1030, training loss: 628.1129150390625 = 0.24119023978710175 + 100.0 * 6.278717041015625
Epoch 1030, val loss: 0.9066063761711121
Epoch 1040, training loss: 626.9877319335938 = 0.23457881808280945 + 100.0 * 6.267531871795654
Epoch 1040, val loss: 0.9098013043403625
Epoch 1050, training loss: 626.9348754882812 = 0.22838358581066132 + 100.0 * 6.267064571380615
Epoch 1050, val loss: 0.9137001037597656
Epoch 1060, training loss: 626.7198486328125 = 0.22245511412620544 + 100.0 * 6.2649736404418945
Epoch 1060, val loss: 0.9180124402046204
Epoch 1070, training loss: 626.6676025390625 = 0.2166876196861267 + 100.0 * 6.264509201049805
Epoch 1070, val loss: 0.9222381711006165
Epoch 1080, training loss: 627.2879028320312 = 0.21104606986045837 + 100.0 * 6.270768642425537
Epoch 1080, val loss: 0.9264110326766968
Epoch 1090, training loss: 626.5150146484375 = 0.20550979673862457 + 100.0 * 6.263094902038574
Epoch 1090, val loss: 0.9306660890579224
Epoch 1100, training loss: 626.8438720703125 = 0.20022372901439667 + 100.0 * 6.26643705368042
Epoch 1100, val loss: 0.9353913068771362
Epoch 1110, training loss: 626.3697509765625 = 0.19503605365753174 + 100.0 * 6.261747360229492
Epoch 1110, val loss: 0.9396018981933594
Epoch 1120, training loss: 626.3779296875 = 0.19006332755088806 + 100.0 * 6.261878967285156
Epoch 1120, val loss: 0.9442187547683716
Epoch 1130, training loss: 626.2223510742188 = 0.18525296449661255 + 100.0 * 6.260371208190918
Epoch 1130, val loss: 0.9490880966186523
Epoch 1140, training loss: 626.81884765625 = 0.18058909475803375 + 100.0 * 6.266382217407227
Epoch 1140, val loss: 0.9536679983139038
Epoch 1150, training loss: 626.8573608398438 = 0.17598876357078552 + 100.0 * 6.266814231872559
Epoch 1150, val loss: 0.9585757851600647
Epoch 1160, training loss: 626.322021484375 = 0.17151165008544922 + 100.0 * 6.261505126953125
Epoch 1160, val loss: 0.9633162021636963
Epoch 1170, training loss: 625.9673461914062 = 0.16722933948040009 + 100.0 * 6.258001327514648
Epoch 1170, val loss: 0.9683927297592163
Epoch 1180, training loss: 625.8872680664062 = 0.16309070587158203 + 100.0 * 6.257241725921631
Epoch 1180, val loss: 0.9734393954277039
Epoch 1190, training loss: 626.0984497070312 = 0.15907450020313263 + 100.0 * 6.25939416885376
Epoch 1190, val loss: 0.9784621596336365
Epoch 1200, training loss: 625.8912963867188 = 0.15511749684810638 + 100.0 * 6.257362365722656
Epoch 1200, val loss: 0.9835453629493713
Epoch 1210, training loss: 625.951416015625 = 0.15127933025360107 + 100.0 * 6.258001327514648
Epoch 1210, val loss: 0.9887869358062744
Epoch 1220, training loss: 626.2233276367188 = 0.1475433111190796 + 100.0 * 6.260757923126221
Epoch 1220, val loss: 0.9937952160835266
Epoch 1230, training loss: 625.67041015625 = 0.14385852217674255 + 100.0 * 6.255265712738037
Epoch 1230, val loss: 0.9988191723823547
Epoch 1240, training loss: 625.5675048828125 = 0.14038099348545074 + 100.0 * 6.254271030426025
Epoch 1240, val loss: 1.0042665004730225
Epoch 1250, training loss: 625.5186157226562 = 0.13700023293495178 + 100.0 * 6.2538161277771
Epoch 1250, val loss: 1.0096603631973267
Epoch 1260, training loss: 626.0966796875 = 0.1337079256772995 + 100.0 * 6.259629249572754
Epoch 1260, val loss: 1.0146769285202026
Epoch 1270, training loss: 625.8019409179688 = 0.13043321669101715 + 100.0 * 6.256714820861816
Epoch 1270, val loss: 1.0199297666549683
Epoch 1280, training loss: 625.4703979492188 = 0.12728069722652435 + 100.0 * 6.25343132019043
Epoch 1280, val loss: 1.0253195762634277
Epoch 1290, training loss: 625.3994140625 = 0.1242498829960823 + 100.0 * 6.252751350402832
Epoch 1290, val loss: 1.0309793949127197
Epoch 1300, training loss: 625.8688354492188 = 0.12129849940538406 + 100.0 * 6.25747537612915
Epoch 1300, val loss: 1.0364265441894531
Epoch 1310, training loss: 625.4325561523438 = 0.11839180439710617 + 100.0 * 6.253141403198242
Epoch 1310, val loss: 1.0417038202285767
Epoch 1320, training loss: 625.4193725585938 = 0.11561577767133713 + 100.0 * 6.253037929534912
Epoch 1320, val loss: 1.0474797487258911
Epoch 1330, training loss: 625.3607788085938 = 0.1128654032945633 + 100.0 * 6.252479553222656
Epoch 1330, val loss: 1.0526269674301147
Epoch 1340, training loss: 625.137939453125 = 0.11017763614654541 + 100.0 * 6.250277519226074
Epoch 1340, val loss: 1.0578864812850952
Epoch 1350, training loss: 624.982421875 = 0.10763028264045715 + 100.0 * 6.248748302459717
Epoch 1350, val loss: 1.0636460781097412
Epoch 1360, training loss: 625.4400634765625 = 0.10516909509897232 + 100.0 * 6.2533488273620605
Epoch 1360, val loss: 1.069108247756958
Epoch 1370, training loss: 625.1539916992188 = 0.10269231349229813 + 100.0 * 6.250512599945068
Epoch 1370, val loss: 1.07424795627594
Epoch 1380, training loss: 624.927734375 = 0.1003047451376915 + 100.0 * 6.248274803161621
Epoch 1380, val loss: 1.079834222793579
Epoch 1390, training loss: 624.8814697265625 = 0.09803212434053421 + 100.0 * 6.2478346824646
Epoch 1390, val loss: 1.0854179859161377
Epoch 1400, training loss: 625.527099609375 = 0.09580907970666885 + 100.0 * 6.254312992095947
Epoch 1400, val loss: 1.0904790163040161
Epoch 1410, training loss: 625.243896484375 = 0.09357354044914246 + 100.0 * 6.2515034675598145
Epoch 1410, val loss: 1.095967411994934
Epoch 1420, training loss: 624.8277587890625 = 0.09145274758338928 + 100.0 * 6.247363090515137
Epoch 1420, val loss: 1.1016600131988525
Epoch 1430, training loss: 624.666748046875 = 0.08940333127975464 + 100.0 * 6.2457733154296875
Epoch 1430, val loss: 1.1070399284362793
Epoch 1440, training loss: 625.2146606445312 = 0.08742015808820724 + 100.0 * 6.251272678375244
Epoch 1440, val loss: 1.1125844717025757
Epoch 1450, training loss: 624.8939819335938 = 0.08545182645320892 + 100.0 * 6.2480854988098145
Epoch 1450, val loss: 1.1180249452590942
Epoch 1460, training loss: 624.6085205078125 = 0.08353780955076218 + 100.0 * 6.2452497482299805
Epoch 1460, val loss: 1.1232264041900635
Epoch 1470, training loss: 624.8564453125 = 0.08170317113399506 + 100.0 * 6.247747421264648
Epoch 1470, val loss: 1.128557562828064
Epoch 1480, training loss: 624.4995727539062 = 0.07988743484020233 + 100.0 * 6.244196891784668
Epoch 1480, val loss: 1.1339683532714844
Epoch 1490, training loss: 624.5956420898438 = 0.07815413922071457 + 100.0 * 6.245175361633301
Epoch 1490, val loss: 1.1394927501678467
Epoch 1500, training loss: 624.8099365234375 = 0.07644020766019821 + 100.0 * 6.247335433959961
Epoch 1500, val loss: 1.1445006132125854
Epoch 1510, training loss: 624.3395385742188 = 0.07476097345352173 + 100.0 * 6.242647647857666
Epoch 1510, val loss: 1.150061011314392
Epoch 1520, training loss: 624.3992309570312 = 0.07315809279680252 + 100.0 * 6.243260383605957
Epoch 1520, val loss: 1.1555885076522827
Epoch 1530, training loss: 624.680419921875 = 0.07159114629030228 + 100.0 * 6.24608850479126
Epoch 1530, val loss: 1.1607145071029663
Epoch 1540, training loss: 624.37158203125 = 0.07004820555448532 + 100.0 * 6.243015289306641
Epoch 1540, val loss: 1.1660559177398682
Epoch 1550, training loss: 624.2605590820312 = 0.06854259967803955 + 100.0 * 6.241919994354248
Epoch 1550, val loss: 1.1713223457336426
Epoch 1560, training loss: 624.2802734375 = 0.06709758192300797 + 100.0 * 6.24213171005249
Epoch 1560, val loss: 1.1765743494033813
Epoch 1570, training loss: 624.32666015625 = 0.06568283587694168 + 100.0 * 6.242609977722168
Epoch 1570, val loss: 1.181890845298767
Epoch 1580, training loss: 624.6964721679688 = 0.06432531774044037 + 100.0 * 6.246321678161621
Epoch 1580, val loss: 1.1875014305114746
Epoch 1590, training loss: 624.1281127929688 = 0.06291765719652176 + 100.0 * 6.240652084350586
Epoch 1590, val loss: 1.191959261894226
Epoch 1600, training loss: 624.1802368164062 = 0.061618782579898834 + 100.0 * 6.241186141967773
Epoch 1600, val loss: 1.1975860595703125
Epoch 1610, training loss: 624.4849853515625 = 0.06034599244594574 + 100.0 * 6.244246006011963
Epoch 1610, val loss: 1.2026289701461792
Epoch 1620, training loss: 624.0645141601562 = 0.05909382179379463 + 100.0 * 6.240054607391357
Epoch 1620, val loss: 1.2077687978744507
Epoch 1630, training loss: 623.9306030273438 = 0.05788779631257057 + 100.0 * 6.238727569580078
Epoch 1630, val loss: 1.212910771369934
Epoch 1640, training loss: 624.8096923828125 = 0.056735455989837646 + 100.0 * 6.247529029846191
Epoch 1640, val loss: 1.2180973291397095
Epoch 1650, training loss: 624.0924072265625 = 0.05555686727166176 + 100.0 * 6.240368366241455
Epoch 1650, val loss: 1.2230327129364014
Epoch 1660, training loss: 623.879638671875 = 0.054424699395895004 + 100.0 * 6.238251686096191
Epoch 1660, val loss: 1.2281348705291748
Epoch 1670, training loss: 624.009765625 = 0.05335814505815506 + 100.0 * 6.239564418792725
Epoch 1670, val loss: 1.2334463596343994
Epoch 1680, training loss: 623.9267578125 = 0.0522836372256279 + 100.0 * 6.238744258880615
Epoch 1680, val loss: 1.2382926940917969
Epoch 1690, training loss: 623.9837036132812 = 0.05124574527144432 + 100.0 * 6.23932409286499
Epoch 1690, val loss: 1.2432979345321655
Epoch 1700, training loss: 624.2591552734375 = 0.050244010984897614 + 100.0 * 6.24208927154541
Epoch 1700, val loss: 1.2480792999267578
Epoch 1710, training loss: 623.8558959960938 = 0.04922612011432648 + 100.0 * 6.23806619644165
Epoch 1710, val loss: 1.2527180910110474
Epoch 1720, training loss: 623.6705932617188 = 0.04827338084578514 + 100.0 * 6.236223220825195
Epoch 1720, val loss: 1.2578033208847046
Epoch 1730, training loss: 623.5918579101562 = 0.047349121421575546 + 100.0 * 6.235445022583008
Epoch 1730, val loss: 1.2627654075622559
Epoch 1740, training loss: 624.0520629882812 = 0.04645600914955139 + 100.0 * 6.240056037902832
Epoch 1740, val loss: 1.26741623878479
Epoch 1750, training loss: 623.5692749023438 = 0.04555004835128784 + 100.0 * 6.2352375984191895
Epoch 1750, val loss: 1.272503137588501
Epoch 1760, training loss: 623.8363037109375 = 0.04469366371631622 + 100.0 * 6.237915992736816
Epoch 1760, val loss: 1.2771203517913818
Epoch 1770, training loss: 623.7056274414062 = 0.04382568225264549 + 100.0 * 6.2366180419921875
Epoch 1770, val loss: 1.2816840410232544
Epoch 1780, training loss: 623.5759887695312 = 0.043005771934986115 + 100.0 * 6.235329627990723
Epoch 1780, val loss: 1.2866482734680176
Epoch 1790, training loss: 623.6064453125 = 0.04220215976238251 + 100.0 * 6.235642910003662
Epoch 1790, val loss: 1.2913025617599487
Epoch 1800, training loss: 623.5424194335938 = 0.04142481088638306 + 100.0 * 6.235009670257568
Epoch 1800, val loss: 1.296061396598816
Epoch 1810, training loss: 623.4266357421875 = 0.04066364839673042 + 100.0 * 6.233860015869141
Epoch 1810, val loss: 1.3008266687393188
Epoch 1820, training loss: 623.6229248046875 = 0.039926186203956604 + 100.0 * 6.235830307006836
Epoch 1820, val loss: 1.3053711652755737
Epoch 1830, training loss: 623.4332275390625 = 0.03919319063425064 + 100.0 * 6.233940601348877
Epoch 1830, val loss: 1.3100135326385498
Epoch 1840, training loss: 623.3361206054688 = 0.03847894072532654 + 100.0 * 6.23297643661499
Epoch 1840, val loss: 1.314695119857788
Epoch 1850, training loss: 623.8062744140625 = 0.037803031504154205 + 100.0 * 6.237684726715088
Epoch 1850, val loss: 1.3194172382354736
Epoch 1860, training loss: 623.2642822265625 = 0.0370941236615181 + 100.0 * 6.232271671295166
Epoch 1860, val loss: 1.3234200477600098
Epoch 1870, training loss: 623.2798461914062 = 0.036436788737773895 + 100.0 * 6.232433795928955
Epoch 1870, val loss: 1.327918529510498
Epoch 1880, training loss: 623.7145385742188 = 0.03580547124147415 + 100.0 * 6.236786842346191
Epoch 1880, val loss: 1.3324787616729736
Epoch 1890, training loss: 623.2728881835938 = 0.03516260161995888 + 100.0 * 6.232377052307129
Epoch 1890, val loss: 1.3367667198181152
Epoch 1900, training loss: 623.1874389648438 = 0.03455450385808945 + 100.0 * 6.2315287590026855
Epoch 1900, val loss: 1.3412582874298096
Epoch 1910, training loss: 623.305908203125 = 0.033956363797187805 + 100.0 * 6.232719421386719
Epoch 1910, val loss: 1.3454177379608154
Epoch 1920, training loss: 623.3444213867188 = 0.033366091549396515 + 100.0 * 6.233110427856445
Epoch 1920, val loss: 1.3496410846710205
Epoch 1930, training loss: 623.3841552734375 = 0.032789357006549835 + 100.0 * 6.233513832092285
Epoch 1930, val loss: 1.3536720275878906
Epoch 1940, training loss: 623.1686401367188 = 0.03223563730716705 + 100.0 * 6.2313642501831055
Epoch 1940, val loss: 1.3583240509033203
Epoch 1950, training loss: 623.1484375 = 0.031693413853645325 + 100.0 * 6.231167316436768
Epoch 1950, val loss: 1.3624045848846436
Epoch 1960, training loss: 623.046142578125 = 0.031160499900579453 + 100.0 * 6.230149745941162
Epoch 1960, val loss: 1.366480827331543
Epoch 1970, training loss: 623.1868286132812 = 0.030636902898550034 + 100.0 * 6.23156213760376
Epoch 1970, val loss: 1.3706499338150024
Epoch 1980, training loss: 623.1477661132812 = 0.030116213485598564 + 100.0 * 6.231176853179932
Epoch 1980, val loss: 1.3744550943374634
Epoch 1990, training loss: 622.9592895507812 = 0.029617471620440483 + 100.0 * 6.229296684265137
Epoch 1990, val loss: 1.3785244226455688
Epoch 2000, training loss: 622.9144287109375 = 0.029137227684259415 + 100.0 * 6.228853225708008
Epoch 2000, val loss: 1.3827441930770874
Epoch 2010, training loss: 623.0302734375 = 0.028673818334937096 + 100.0 * 6.230015754699707
Epoch 2010, val loss: 1.3868298530578613
Epoch 2020, training loss: 622.8331909179688 = 0.028204360976815224 + 100.0 * 6.2280497550964355
Epoch 2020, val loss: 1.3907263278961182
Epoch 2030, training loss: 623.0252685546875 = 0.027767635881900787 + 100.0 * 6.229974746704102
Epoch 2030, val loss: 1.3948208093643188
Epoch 2040, training loss: 623.0562133789062 = 0.02731906995177269 + 100.0 * 6.230288505554199
Epoch 2040, val loss: 1.3985087871551514
Epoch 2050, training loss: 622.7545166015625 = 0.02687334269285202 + 100.0 * 6.22727632522583
Epoch 2050, val loss: 1.4022284746170044
Epoch 2060, training loss: 622.6489868164062 = 0.026450861245393753 + 100.0 * 6.22622537612915
Epoch 2060, val loss: 1.4061613082885742
Epoch 2070, training loss: 622.6832885742188 = 0.02604798600077629 + 100.0 * 6.226572513580322
Epoch 2070, val loss: 1.4102150201797485
Epoch 2080, training loss: 623.2532958984375 = 0.025662535801529884 + 100.0 * 6.232276916503906
Epoch 2080, val loss: 1.4140985012054443
Epoch 2090, training loss: 623.4714965820312 = 0.025265220552682877 + 100.0 * 6.234462261199951
Epoch 2090, val loss: 1.4175779819488525
Epoch 2100, training loss: 622.8287963867188 = 0.02484605275094509 + 100.0 * 6.228039264678955
Epoch 2100, val loss: 1.4209016561508179
Epoch 2110, training loss: 622.6150512695312 = 0.024471571668982506 + 100.0 * 6.225905895233154
Epoch 2110, val loss: 1.4248713254928589
Epoch 2120, training loss: 622.5651245117188 = 0.024112386628985405 + 100.0 * 6.225409984588623
Epoch 2120, val loss: 1.4286521673202515
Epoch 2130, training loss: 623.1767578125 = 0.023767337203025818 + 100.0 * 6.23153018951416
Epoch 2130, val loss: 1.432199478149414
Epoch 2140, training loss: 622.8811645507812 = 0.02339239977300167 + 100.0 * 6.228578090667725
Epoch 2140, val loss: 1.4356213808059692
Epoch 2150, training loss: 622.6954956054688 = 0.023045049980282784 + 100.0 * 6.226724147796631
Epoch 2150, val loss: 1.4392430782318115
Epoch 2160, training loss: 622.6090087890625 = 0.022707629948854446 + 100.0 * 6.225862979888916
Epoch 2160, val loss: 1.442996621131897
Epoch 2170, training loss: 622.5718994140625 = 0.02237640507519245 + 100.0 * 6.225494861602783
Epoch 2170, val loss: 1.446701169013977
Epoch 2180, training loss: 623.0317993164062 = 0.022061597555875778 + 100.0 * 6.230097770690918
Epoch 2180, val loss: 1.4502345323562622
Epoch 2190, training loss: 622.6654663085938 = 0.021725688129663467 + 100.0 * 6.226437568664551
Epoch 2190, val loss: 1.4531465768814087
Epoch 2200, training loss: 622.4833374023438 = 0.02141404151916504 + 100.0 * 6.224619388580322
Epoch 2200, val loss: 1.4567818641662598
Epoch 2210, training loss: 622.3624877929688 = 0.02111532725393772 + 100.0 * 6.223413467407227
Epoch 2210, val loss: 1.4603841304779053
Epoch 2220, training loss: 622.5233154296875 = 0.020830579102039337 + 100.0 * 6.225025177001953
Epoch 2220, val loss: 1.4639393091201782
Epoch 2230, training loss: 623.028564453125 = 0.020531781017780304 + 100.0 * 6.230080604553223
Epoch 2230, val loss: 1.466681957244873
Epoch 2240, training loss: 622.448974609375 = 0.020234178751707077 + 100.0 * 6.224287033081055
Epoch 2240, val loss: 1.4700686931610107
Epoch 2250, training loss: 622.3453979492188 = 0.019947540014982224 + 100.0 * 6.223254680633545
Epoch 2250, val loss: 1.4733372926712036
Epoch 2260, training loss: 622.2086181640625 = 0.019680028781294823 + 100.0 * 6.221889495849609
Epoch 2260, val loss: 1.4770658016204834
Epoch 2270, training loss: 622.2391357421875 = 0.01942056603729725 + 100.0 * 6.222197532653809
Epoch 2270, val loss: 1.4804445505142212
Epoch 2280, training loss: 623.0148315429688 = 0.01916689984500408 + 100.0 * 6.22995662689209
Epoch 2280, val loss: 1.4834176301956177
Epoch 2290, training loss: 622.8070068359375 = 0.018904680386185646 + 100.0 * 6.227880954742432
Epoch 2290, val loss: 1.486846685409546
Epoch 2300, training loss: 622.6143798828125 = 0.01863163337111473 + 100.0 * 6.225957870483398
Epoch 2300, val loss: 1.4895740747451782
Epoch 2310, training loss: 622.1978759765625 = 0.018376244232058525 + 100.0 * 6.221795082092285
Epoch 2310, val loss: 1.4924659729003906
Epoch 2320, training loss: 622.1551513671875 = 0.018138211220502853 + 100.0 * 6.221370220184326
Epoch 2320, val loss: 1.4960613250732422
Epoch 2330, training loss: 622.0924682617188 = 0.017907338216900826 + 100.0 * 6.22074556350708
Epoch 2330, val loss: 1.4992609024047852
Epoch 2340, training loss: 622.10791015625 = 0.017683086916804314 + 100.0 * 6.220901966094971
Epoch 2340, val loss: 1.5025405883789062
Epoch 2350, training loss: 622.8843994140625 = 0.0174692515283823 + 100.0 * 6.228669166564941
Epoch 2350, val loss: 1.5055763721466064
Epoch 2360, training loss: 622.3043823242188 = 0.017224812880158424 + 100.0 * 6.222871780395508
Epoch 2360, val loss: 1.5080171823501587
Epoch 2370, training loss: 622.503173828125 = 0.016999823972582817 + 100.0 * 6.2248616218566895
Epoch 2370, val loss: 1.5110766887664795
Epoch 2380, training loss: 622.10595703125 = 0.01678173430263996 + 100.0 * 6.220891952514648
Epoch 2380, val loss: 1.5140830278396606
Epoch 2390, training loss: 622.01220703125 = 0.016573231667280197 + 100.0 * 6.219955921173096
Epoch 2390, val loss: 1.5173856019973755
Epoch 2400, training loss: 622.0847778320312 = 0.01637490838766098 + 100.0 * 6.220684051513672
Epoch 2400, val loss: 1.5205274820327759
Epoch 2410, training loss: 622.5009155273438 = 0.016177162528038025 + 100.0 * 6.224847316741943
Epoch 2410, val loss: 1.523323893547058
Epoch 2420, training loss: 622.1853637695312 = 0.015965593978762627 + 100.0 * 6.221693992614746
Epoch 2420, val loss: 1.5258517265319824
Epoch 2430, training loss: 622.1424560546875 = 0.015771053731441498 + 100.0 * 6.221267223358154
Epoch 2430, val loss: 1.5288095474243164
Epoch 2440, training loss: 622.2201538085938 = 0.015576503239572048 + 100.0 * 6.2220458984375
Epoch 2440, val loss: 1.5314496755599976
Epoch 2450, training loss: 622.0489501953125 = 0.01538616232573986 + 100.0 * 6.220335960388184
Epoch 2450, val loss: 1.5344057083129883
Epoch 2460, training loss: 621.9783325195312 = 0.015204244293272495 + 100.0 * 6.219631195068359
Epoch 2460, val loss: 1.5374459028244019
Epoch 2470, training loss: 622.5851440429688 = 0.015026656910777092 + 100.0 * 6.225701332092285
Epoch 2470, val loss: 1.540284276008606
Epoch 2480, training loss: 621.9779052734375 = 0.014842666685581207 + 100.0 * 6.219630718231201
Epoch 2480, val loss: 1.5425236225128174
Epoch 2490, training loss: 621.8809814453125 = 0.01466747559607029 + 100.0 * 6.218663215637207
Epoch 2490, val loss: 1.5454514026641846
Epoch 2500, training loss: 622.2294921875 = 0.014501100406050682 + 100.0 * 6.222149848937988
Epoch 2500, val loss: 1.5482784509658813
Epoch 2510, training loss: 621.8673095703125 = 0.014327994547784328 + 100.0 * 6.21852970123291
Epoch 2510, val loss: 1.5507539510726929
Epoch 2520, training loss: 621.8321533203125 = 0.014161191880702972 + 100.0 * 6.218180179595947
Epoch 2520, val loss: 1.5534639358520508
Epoch 2530, training loss: 622.2361450195312 = 0.014003660529851913 + 100.0 * 6.222221851348877
Epoch 2530, val loss: 1.5560147762298584
Epoch 2540, training loss: 621.97265625 = 0.013834046199917793 + 100.0 * 6.219587802886963
Epoch 2540, val loss: 1.5582960844039917
Epoch 2550, training loss: 621.7908935546875 = 0.013673769310116768 + 100.0 * 6.217772483825684
Epoch 2550, val loss: 1.5609471797943115
Epoch 2560, training loss: 621.6896362304688 = 0.013521714136004448 + 100.0 * 6.216760635375977
Epoch 2560, val loss: 1.5637258291244507
Epoch 2570, training loss: 621.6541137695312 = 0.013377465307712555 + 100.0 * 6.216407299041748
Epoch 2570, val loss: 1.5664827823638916
Epoch 2580, training loss: 622.5040283203125 = 0.013244764879345894 + 100.0 * 6.224907875061035
Epoch 2580, val loss: 1.569286823272705
Epoch 2590, training loss: 621.9690551757812 = 0.01308334618806839 + 100.0 * 6.219559669494629
Epoch 2590, val loss: 1.570724368095398
Epoch 2600, training loss: 621.7840576171875 = 0.012938045896589756 + 100.0 * 6.217711448669434
Epoch 2600, val loss: 1.5736865997314453
Epoch 2610, training loss: 621.6533813476562 = 0.012795385904610157 + 100.0 * 6.216405868530273
Epoch 2610, val loss: 1.5761481523513794
Epoch 2620, training loss: 621.99560546875 = 0.012666110880672932 + 100.0 * 6.219829082489014
Epoch 2620, val loss: 1.5786008834838867
Epoch 2630, training loss: 621.687744140625 = 0.012519881129264832 + 100.0 * 6.216752052307129
Epoch 2630, val loss: 1.5808783769607544
Epoch 2640, training loss: 622.388427734375 = 0.012390230782330036 + 100.0 * 6.22376012802124
Epoch 2640, val loss: 1.5830272436141968
Epoch 2650, training loss: 621.6990356445312 = 0.012249099090695381 + 100.0 * 6.216867446899414
Epoch 2650, val loss: 1.5854049921035767
Epoch 2660, training loss: 621.5670166015625 = 0.012118665501475334 + 100.0 * 6.215548515319824
Epoch 2660, val loss: 1.5875872373580933
Epoch 2670, training loss: 621.531005859375 = 0.011996260844171047 + 100.0 * 6.2151899337768555
Epoch 2670, val loss: 1.5903208255767822
Epoch 2680, training loss: 621.6275634765625 = 0.011876920238137245 + 100.0 * 6.216156482696533
Epoch 2680, val loss: 1.5926589965820312
Epoch 2690, training loss: 622.0289306640625 = 0.01175489742308855 + 100.0 * 6.2201714515686035
Epoch 2690, val loss: 1.5948641300201416
Epoch 2700, training loss: 621.8512573242188 = 0.011633588001132011 + 100.0 * 6.218395709991455
Epoch 2700, val loss: 1.5970417261123657
Epoch 2710, training loss: 622.0947265625 = 0.011512807570397854 + 100.0 * 6.220831871032715
Epoch 2710, val loss: 1.5990344285964966
Epoch 2720, training loss: 621.7437133789062 = 0.011390570551156998 + 100.0 * 6.217323303222656
Epoch 2720, val loss: 1.6015372276306152
Epoch 2730, training loss: 621.5055541992188 = 0.01127685233950615 + 100.0 * 6.214942932128906
Epoch 2730, val loss: 1.6037445068359375
Epoch 2740, training loss: 621.431640625 = 0.011167384684085846 + 100.0 * 6.214204788208008
Epoch 2740, val loss: 1.6061480045318604
Epoch 2750, training loss: 622.2302856445312 = 0.011062073521316051 + 100.0 * 6.222192287445068
Epoch 2750, val loss: 1.6081269979476929
Epoch 2760, training loss: 621.489501953125 = 0.010946077294647694 + 100.0 * 6.214786052703857
Epoch 2760, val loss: 1.6100572347640991
Epoch 2770, training loss: 621.4417724609375 = 0.010836723260581493 + 100.0 * 6.214309215545654
Epoch 2770, val loss: 1.612293004989624
Epoch 2780, training loss: 621.8828125 = 0.010736695490777493 + 100.0 * 6.21872091293335
Epoch 2780, val loss: 1.614466667175293
Epoch 2790, training loss: 621.4339599609375 = 0.010627510026097298 + 100.0 * 6.2142333984375
Epoch 2790, val loss: 1.6165401935577393
Epoch 2800, training loss: 621.440673828125 = 0.010526257567107677 + 100.0 * 6.214302062988281
Epoch 2800, val loss: 1.6186494827270508
Epoch 2810, training loss: 621.689697265625 = 0.010430951602756977 + 100.0 * 6.216792583465576
Epoch 2810, val loss: 1.6208906173706055
Epoch 2820, training loss: 621.3798828125 = 0.01032820250838995 + 100.0 * 6.213695526123047
Epoch 2820, val loss: 1.622815489768982
Epoch 2830, training loss: 621.45654296875 = 0.010230263695120811 + 100.0 * 6.214462757110596
Epoch 2830, val loss: 1.6248860359191895
Epoch 2840, training loss: 621.558837890625 = 0.010137715376913548 + 100.0 * 6.215487003326416
Epoch 2840, val loss: 1.6269962787628174
Epoch 2850, training loss: 621.4535522460938 = 0.010038580745458603 + 100.0 * 6.214435577392578
Epoch 2850, val loss: 1.628523588180542
Epoch 2860, training loss: 621.4078979492188 = 0.00994433369487524 + 100.0 * 6.213979721069336
Epoch 2860, val loss: 1.630584955215454
Epoch 2870, training loss: 621.7577514648438 = 0.009856578893959522 + 100.0 * 6.217479228973389
Epoch 2870, val loss: 1.6327351331710815
Epoch 2880, training loss: 621.3399047851562 = 0.009762536734342575 + 100.0 * 6.213301658630371
Epoch 2880, val loss: 1.6344021558761597
Epoch 2890, training loss: 621.4959106445312 = 0.009676674380898476 + 100.0 * 6.21486234664917
Epoch 2890, val loss: 1.6366395950317383
Epoch 2900, training loss: 621.5232543945312 = 0.009589512832462788 + 100.0 * 6.215136528015137
Epoch 2900, val loss: 1.6384012699127197
Epoch 2910, training loss: 621.248779296875 = 0.009499271400272846 + 100.0 * 6.212392807006836
Epoch 2910, val loss: 1.640181303024292
Epoch 2920, training loss: 621.3846435546875 = 0.00941743329167366 + 100.0 * 6.213752269744873
Epoch 2920, val loss: 1.6419624090194702
Epoch 2930, training loss: 621.4874877929688 = 0.009336004965007305 + 100.0 * 6.214781761169434
Epoch 2930, val loss: 1.643984079360962
Epoch 2940, training loss: 621.4346923828125 = 0.009251601994037628 + 100.0 * 6.214254379272461
Epoch 2940, val loss: 1.6457353830337524
Epoch 2950, training loss: 621.2311401367188 = 0.009169076569378376 + 100.0 * 6.212219715118408
Epoch 2950, val loss: 1.6474891901016235
Epoch 2960, training loss: 621.176025390625 = 0.009090464562177658 + 100.0 * 6.211669445037842
Epoch 2960, val loss: 1.6494404077529907
Epoch 2970, training loss: 621.313232421875 = 0.009017644450068474 + 100.0 * 6.21304178237915
Epoch 2970, val loss: 1.6512173414230347
Epoch 2980, training loss: 621.606201171875 = 0.00894186832010746 + 100.0 * 6.215972423553467
Epoch 2980, val loss: 1.6529479026794434
Epoch 2990, training loss: 621.2530517578125 = 0.008859789930284023 + 100.0 * 6.212441921234131
Epoch 2990, val loss: 1.6546276807785034
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8102266736953084
The final CL Acc:0.69753, 0.00924, The final GNN Acc:0.80759, 0.00215
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13224])
remove edge: torch.Size([2, 8044])
updated graph: torch.Size([2, 10712])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6250610351562 = 1.940482258796692 + 100.0 * 8.596845626831055
Epoch 0, val loss: 1.9385615587234497
Epoch 10, training loss: 861.5460815429688 = 1.9318983554840088 + 100.0 * 8.596141815185547
Epoch 10, val loss: 1.9301875829696655
Epoch 20, training loss: 861.0242919921875 = 1.9211764335632324 + 100.0 * 8.591031074523926
Epoch 20, val loss: 1.9193793535232544
Epoch 30, training loss: 857.5651245117188 = 1.9071413278579712 + 100.0 * 8.55657958984375
Epoch 30, val loss: 1.9047788381576538
Epoch 40, training loss: 839.2691650390625 = 1.8900748491287231 + 100.0 * 8.373790740966797
Epoch 40, val loss: 1.8877137899398804
Epoch 50, training loss: 783.7327880859375 = 1.87131667137146 + 100.0 * 7.818614482879639
Epoch 50, val loss: 1.868487000465393
Epoch 60, training loss: 755.8721313476562 = 1.8534188270568848 + 100.0 * 7.540187358856201
Epoch 60, val loss: 1.8513492345809937
Epoch 70, training loss: 740.46240234375 = 1.8403198719024658 + 100.0 * 7.386220932006836
Epoch 70, val loss: 1.838783860206604
Epoch 80, training loss: 723.2748413085938 = 1.8277032375335693 + 100.0 * 7.214471340179443
Epoch 80, val loss: 1.826542854309082
Epoch 90, training loss: 704.3760986328125 = 1.8185958862304688 + 100.0 * 7.025575160980225
Epoch 90, val loss: 1.818449854850769
Epoch 100, training loss: 693.4947509765625 = 1.8105543851852417 + 100.0 * 6.916841983795166
Epoch 100, val loss: 1.8104007244110107
Epoch 110, training loss: 684.313232421875 = 1.799943208694458 + 100.0 * 6.825133323669434
Epoch 110, val loss: 1.7998844385147095
Epoch 120, training loss: 677.9546508789062 = 1.78984797000885 + 100.0 * 6.761647701263428
Epoch 120, val loss: 1.7899889945983887
Epoch 130, training loss: 672.4591674804688 = 1.7801923751831055 + 100.0 * 6.706789493560791
Epoch 130, val loss: 1.7803858518600464
Epoch 140, training loss: 668.0411987304688 = 1.7699724435806274 + 100.0 * 6.662712097167969
Epoch 140, val loss: 1.7701302766799927
Epoch 150, training loss: 664.5051879882812 = 1.7589508295059204 + 100.0 * 6.627462387084961
Epoch 150, val loss: 1.7590584754943848
Epoch 160, training loss: 661.673828125 = 1.7472081184387207 + 100.0 * 6.599266052246094
Epoch 160, val loss: 1.747362732887268
Epoch 170, training loss: 659.4524536132812 = 1.7344660758972168 + 100.0 * 6.577179908752441
Epoch 170, val loss: 1.7347946166992188
Epoch 180, training loss: 657.677490234375 = 1.7207555770874023 + 100.0 * 6.559567451477051
Epoch 180, val loss: 1.721177339553833
Epoch 190, training loss: 656.0261840820312 = 1.7057996988296509 + 100.0 * 6.543203830718994
Epoch 190, val loss: 1.706515908241272
Epoch 200, training loss: 654.3889770507812 = 1.6896644830703735 + 100.0 * 6.526993274688721
Epoch 200, val loss: 1.6907378435134888
Epoch 210, training loss: 652.9593505859375 = 1.6722486019134521 + 100.0 * 6.512871265411377
Epoch 210, val loss: 1.6736429929733276
Epoch 220, training loss: 651.786865234375 = 1.653305172920227 + 100.0 * 6.501335620880127
Epoch 220, val loss: 1.6551653146743774
Epoch 230, training loss: 650.4822387695312 = 1.6329368352890015 + 100.0 * 6.488492965698242
Epoch 230, val loss: 1.6354010105133057
Epoch 240, training loss: 649.6967163085938 = 1.6111009120941162 + 100.0 * 6.480856418609619
Epoch 240, val loss: 1.6142807006835938
Epoch 250, training loss: 648.4642944335938 = 1.58781898021698 + 100.0 * 6.468764781951904
Epoch 250, val loss: 1.591929316520691
Epoch 260, training loss: 647.5150146484375 = 1.5632127523422241 + 100.0 * 6.459517955780029
Epoch 260, val loss: 1.5685322284698486
Epoch 270, training loss: 646.580078125 = 1.5373626947402954 + 100.0 * 6.450427532196045
Epoch 270, val loss: 1.544235348701477
Epoch 280, training loss: 645.7823486328125 = 1.510454535484314 + 100.0 * 6.442718982696533
Epoch 280, val loss: 1.5191186666488647
Epoch 290, training loss: 644.9489135742188 = 1.4826611280441284 + 100.0 * 6.434662342071533
Epoch 290, val loss: 1.4935277700424194
Epoch 300, training loss: 644.6871948242188 = 1.4541528224945068 + 100.0 * 6.432330131530762
Epoch 300, val loss: 1.4677059650421143
Epoch 310, training loss: 643.5553588867188 = 1.4251015186309814 + 100.0 * 6.421302318572998
Epoch 310, val loss: 1.4416004419326782
Epoch 320, training loss: 643.1624755859375 = 1.3958379030227661 + 100.0 * 6.417666912078857
Epoch 320, val loss: 1.4155641794204712
Epoch 330, training loss: 642.405517578125 = 1.3661917448043823 + 100.0 * 6.410393238067627
Epoch 330, val loss: 1.3898348808288574
Epoch 340, training loss: 641.7810668945312 = 1.3367574214935303 + 100.0 * 6.404443264007568
Epoch 340, val loss: 1.3644201755523682
Epoch 350, training loss: 641.1788940429688 = 1.3074339628219604 + 100.0 * 6.398714542388916
Epoch 350, val loss: 1.3396340608596802
Epoch 360, training loss: 640.6040649414062 = 1.2783054113388062 + 100.0 * 6.3932576179504395
Epoch 360, val loss: 1.3152306079864502
Epoch 370, training loss: 640.117431640625 = 1.2495065927505493 + 100.0 * 6.388679027557373
Epoch 370, val loss: 1.2916737794876099
Epoch 380, training loss: 639.7352905273438 = 1.2210168838500977 + 100.0 * 6.385142803192139
Epoch 380, val loss: 1.2682487964630127
Epoch 390, training loss: 639.1801147460938 = 1.192935585975647 + 100.0 * 6.379871845245361
Epoch 390, val loss: 1.2458291053771973
Epoch 400, training loss: 638.6321411132812 = 1.1654412746429443 + 100.0 * 6.374666690826416
Epoch 400, val loss: 1.2238768339157104
Epoch 410, training loss: 638.546630859375 = 1.138384461402893 + 100.0 * 6.374082565307617
Epoch 410, val loss: 1.202834963798523
Epoch 420, training loss: 637.8408203125 = 1.1117169857025146 + 100.0 * 6.367291450500488
Epoch 420, val loss: 1.1820844411849976
Epoch 430, training loss: 637.6390991210938 = 1.085688829421997 + 100.0 * 6.36553430557251
Epoch 430, val loss: 1.1620951890945435
Epoch 440, training loss: 637.07080078125 = 1.0598644018173218 + 100.0 * 6.360109329223633
Epoch 440, val loss: 1.1424227952957153
Epoch 450, training loss: 636.6174926757812 = 1.034729242324829 + 100.0 * 6.355827808380127
Epoch 450, val loss: 1.1235263347625732
Epoch 460, training loss: 636.2317504882812 = 1.0103284120559692 + 100.0 * 6.352214336395264
Epoch 460, val loss: 1.105381965637207
Epoch 470, training loss: 636.6393432617188 = 0.9864916205406189 + 100.0 * 6.356528282165527
Epoch 470, val loss: 1.0877115726470947
Epoch 480, training loss: 635.6650390625 = 0.9630169868469238 + 100.0 * 6.347020149230957
Epoch 480, val loss: 1.0707557201385498
Epoch 490, training loss: 635.1820068359375 = 0.9402772188186646 + 100.0 * 6.342417240142822
Epoch 490, val loss: 1.0541844367980957
Epoch 500, training loss: 634.9434814453125 = 0.9182193875312805 + 100.0 * 6.34025239944458
Epoch 500, val loss: 1.0383375883102417
Epoch 510, training loss: 634.8743286132812 = 0.8967360854148865 + 100.0 * 6.339776039123535
Epoch 510, val loss: 1.0229852199554443
Epoch 520, training loss: 634.6143798828125 = 0.8755326867103577 + 100.0 * 6.337388515472412
Epoch 520, val loss: 1.0088071823120117
Epoch 530, training loss: 634.0748901367188 = 0.8550387024879456 + 100.0 * 6.332198619842529
Epoch 530, val loss: 0.9943988919258118
Epoch 540, training loss: 633.6818237304688 = 0.8349936008453369 + 100.0 * 6.328468322753906
Epoch 540, val loss: 0.9809079766273499
Epoch 550, training loss: 633.6026000976562 = 0.815550684928894 + 100.0 * 6.3278703689575195
Epoch 550, val loss: 0.9676437973976135
Epoch 560, training loss: 633.9268798828125 = 0.7962629795074463 + 100.0 * 6.331306457519531
Epoch 560, val loss: 0.9551646113395691
Epoch 570, training loss: 632.9183349609375 = 0.7773162126541138 + 100.0 * 6.321410179138184
Epoch 570, val loss: 0.9426791071891785
Epoch 580, training loss: 632.7634887695312 = 0.7588316798210144 + 100.0 * 6.320046424865723
Epoch 580, val loss: 0.9305907487869263
Epoch 590, training loss: 632.4640502929688 = 0.7407339811325073 + 100.0 * 6.317233562469482
Epoch 590, val loss: 0.9192234873771667
Epoch 600, training loss: 632.476806640625 = 0.7228509187698364 + 100.0 * 6.317539691925049
Epoch 600, val loss: 0.9081077575683594
Epoch 610, training loss: 632.1653442382812 = 0.7052064538002014 + 100.0 * 6.314601421356201
Epoch 610, val loss: 0.8969078063964844
Epoch 620, training loss: 631.8358154296875 = 0.6879024505615234 + 100.0 * 6.311478614807129
Epoch 620, val loss: 0.8864426612854004
Epoch 630, training loss: 631.9037475585938 = 0.6707526445388794 + 100.0 * 6.3123297691345215
Epoch 630, val loss: 0.876234769821167
Epoch 640, training loss: 631.5845947265625 = 0.6537853479385376 + 100.0 * 6.30930757522583
Epoch 640, val loss: 0.8659777641296387
Epoch 650, training loss: 631.3056030273438 = 0.6371036171913147 + 100.0 * 6.306685447692871
Epoch 650, val loss: 0.8563759922981262
Epoch 660, training loss: 631.121337890625 = 0.6206299066543579 + 100.0 * 6.305007457733154
Epoch 660, val loss: 0.8466984629631042
Epoch 670, training loss: 631.2010498046875 = 0.604220986366272 + 100.0 * 6.305968284606934
Epoch 670, val loss: 0.8380781412124634
Epoch 680, training loss: 630.7190551757812 = 0.5879618525505066 + 100.0 * 6.301311016082764
Epoch 680, val loss: 0.8287736773490906
Epoch 690, training loss: 630.5216674804688 = 0.5720618367195129 + 100.0 * 6.299495697021484
Epoch 690, val loss: 0.8201727867126465
Epoch 700, training loss: 630.3308715820312 = 0.5563807487487793 + 100.0 * 6.2977447509765625
Epoch 700, val loss: 0.8120856881141663
Epoch 710, training loss: 631.0720825195312 = 0.5408909320831299 + 100.0 * 6.305312156677246
Epoch 710, val loss: 0.8041569590568542
Epoch 720, training loss: 630.4595947265625 = 0.5254759192466736 + 100.0 * 6.299341678619385
Epoch 720, val loss: 0.79651939868927
Epoch 730, training loss: 630.1475219726562 = 0.5104938745498657 + 100.0 * 6.296370029449463
Epoch 730, val loss: 0.7893995046615601
Epoch 740, training loss: 629.7919921875 = 0.49580076336860657 + 100.0 * 6.292962074279785
Epoch 740, val loss: 0.7827119827270508
Epoch 750, training loss: 629.7583618164062 = 0.4814613461494446 + 100.0 * 6.292768955230713
Epoch 750, val loss: 0.776354968547821
Epoch 760, training loss: 629.9366455078125 = 0.46742650866508484 + 100.0 * 6.294692516326904
Epoch 760, val loss: 0.7702974081039429
Epoch 770, training loss: 629.5224609375 = 0.4534584581851959 + 100.0 * 6.2906904220581055
Epoch 770, val loss: 0.7646865844726562
Epoch 780, training loss: 629.3026733398438 = 0.44009631872177124 + 100.0 * 6.288625240325928
Epoch 780, val loss: 0.7592727541923523
Epoch 790, training loss: 629.2311401367188 = 0.42698293924331665 + 100.0 * 6.288041591644287
Epoch 790, val loss: 0.7545265555381775
Epoch 800, training loss: 629.1915893554688 = 0.41426438093185425 + 100.0 * 6.287773609161377
Epoch 800, val loss: 0.7499241232872009
Epoch 810, training loss: 628.8670043945312 = 0.4018286466598511 + 100.0 * 6.284651756286621
Epoch 810, val loss: 0.7457570433616638
Epoch 820, training loss: 628.9945068359375 = 0.38979363441467285 + 100.0 * 6.286047458648682
Epoch 820, val loss: 0.7420554757118225
Epoch 830, training loss: 628.5447387695312 = 0.3780805170536041 + 100.0 * 6.2816667556762695
Epoch 830, val loss: 0.738497793674469
Epoch 840, training loss: 628.48095703125 = 0.36673596501350403 + 100.0 * 6.281142234802246
Epoch 840, val loss: 0.7354576587677002
Epoch 850, training loss: 628.926513671875 = 0.3558309078216553 + 100.0 * 6.285706996917725
Epoch 850, val loss: 0.7326859831809998
Epoch 860, training loss: 628.4420776367188 = 0.3450813889503479 + 100.0 * 6.280970096588135
Epoch 860, val loss: 0.7301502227783203
Epoch 870, training loss: 628.2877807617188 = 0.3348430097103119 + 100.0 * 6.279529571533203
Epoch 870, val loss: 0.7281219363212585
Epoch 880, training loss: 628.4635009765625 = 0.3249058425426483 + 100.0 * 6.281386375427246
Epoch 880, val loss: 0.7262195348739624
Epoch 890, training loss: 628.0797729492188 = 0.3151862919330597 + 100.0 * 6.277645587921143
Epoch 890, val loss: 0.724838137626648
Epoch 900, training loss: 628.1046142578125 = 0.30591920018196106 + 100.0 * 6.277987003326416
Epoch 900, val loss: 0.7235957384109497
Epoch 910, training loss: 627.8995361328125 = 0.2969043552875519 + 100.0 * 6.276026725769043
Epoch 910, val loss: 0.7225380539894104
Epoch 920, training loss: 628.2246704101562 = 0.2882024943828583 + 100.0 * 6.279364585876465
Epoch 920, val loss: 0.7218941450119019
Epoch 930, training loss: 627.6856079101562 = 0.27976587414741516 + 100.0 * 6.2740583419799805
Epoch 930, val loss: 0.7212576866149902
Epoch 940, training loss: 627.5428466796875 = 0.2716984450817108 + 100.0 * 6.272711277008057
Epoch 940, val loss: 0.721063494682312
Epoch 950, training loss: 628.092041015625 = 0.2638959288597107 + 100.0 * 6.278281211853027
Epoch 950, val loss: 0.7211818099021912
Epoch 960, training loss: 627.5385131835938 = 0.25624409317970276 + 100.0 * 6.272822856903076
Epoch 960, val loss: 0.7211825847625732
Epoch 970, training loss: 627.5753173828125 = 0.24891377985477448 + 100.0 * 6.273264408111572
Epoch 970, val loss: 0.7217190265655518
Epoch 980, training loss: 627.2111206054688 = 0.24183610081672668 + 100.0 * 6.269692897796631
Epoch 980, val loss: 0.7223004698753357
Epoch 990, training loss: 627.3402099609375 = 0.23503811657428741 + 100.0 * 6.271051406860352
Epoch 990, val loss: 0.7232906222343445
Epoch 1000, training loss: 627.0957641601562 = 0.2283829003572464 + 100.0 * 6.268673419952393
Epoch 1000, val loss: 0.7240213751792908
Epoch 1010, training loss: 627.0955200195312 = 0.22199557721614838 + 100.0 * 6.268735408782959
Epoch 1010, val loss: 0.7253869771957397
Epoch 1020, training loss: 627.0651245117188 = 0.21584397554397583 + 100.0 * 6.268493175506592
Epoch 1020, val loss: 0.726506233215332
Epoch 1030, training loss: 626.7733764648438 = 0.2098914384841919 + 100.0 * 6.265635013580322
Epoch 1030, val loss: 0.7280596494674683
Epoch 1040, training loss: 627.2131958007812 = 0.20418545603752136 + 100.0 * 6.270090103149414
Epoch 1040, val loss: 0.7296501994132996
Epoch 1050, training loss: 626.820556640625 = 0.19854551553726196 + 100.0 * 6.2662200927734375
Epoch 1050, val loss: 0.7315375804901123
Epoch 1060, training loss: 626.7295532226562 = 0.19314886629581451 + 100.0 * 6.265363693237305
Epoch 1060, val loss: 0.7332477569580078
Epoch 1070, training loss: 626.9617309570312 = 0.18789809942245483 + 100.0 * 6.267738342285156
Epoch 1070, val loss: 0.7352806925773621
Epoch 1080, training loss: 626.7875366210938 = 0.18286234140396118 + 100.0 * 6.26604700088501
Epoch 1080, val loss: 0.7374060153961182
Epoch 1090, training loss: 626.3572387695312 = 0.17791596055030823 + 100.0 * 6.26179313659668
Epoch 1090, val loss: 0.7397171258926392
Epoch 1100, training loss: 626.293212890625 = 0.17322014272212982 + 100.0 * 6.261199951171875
Epoch 1100, val loss: 0.7420721650123596
Epoch 1110, training loss: 626.4885864257812 = 0.16865861415863037 + 100.0 * 6.263199329376221
Epoch 1110, val loss: 0.7445186376571655
Epoch 1120, training loss: 626.4127197265625 = 0.16425062716007233 + 100.0 * 6.262484550476074
Epoch 1120, val loss: 0.7473318576812744
Epoch 1130, training loss: 626.078369140625 = 0.1599103808403015 + 100.0 * 6.25918436050415
Epoch 1130, val loss: 0.7497379183769226
Epoch 1140, training loss: 625.9741821289062 = 0.15573662519454956 + 100.0 * 6.258184432983398
Epoch 1140, val loss: 0.7525928020477295
Epoch 1150, training loss: 626.1948852539062 = 0.15175193548202515 + 100.0 * 6.26043176651001
Epoch 1150, val loss: 0.7555248737335205
Epoch 1160, training loss: 626.3662719726562 = 0.14780212938785553 + 100.0 * 6.2621846199035645
Epoch 1160, val loss: 0.7582210898399353
Epoch 1170, training loss: 625.9658203125 = 0.14394572377204895 + 100.0 * 6.258219242095947
Epoch 1170, val loss: 0.7613351941108704
Epoch 1180, training loss: 625.8106689453125 = 0.1402764618396759 + 100.0 * 6.256704330444336
Epoch 1180, val loss: 0.7643035650253296
Epoch 1190, training loss: 625.7548828125 = 0.13675187528133392 + 100.0 * 6.256180763244629
Epoch 1190, val loss: 0.7675946354866028
Epoch 1200, training loss: 626.265380859375 = 0.1333213448524475 + 100.0 * 6.2613205909729
Epoch 1200, val loss: 0.7707061767578125
Epoch 1210, training loss: 626.2933959960938 = 0.12992171943187714 + 100.0 * 6.261634826660156
Epoch 1210, val loss: 0.7738772630691528
Epoch 1220, training loss: 625.7559814453125 = 0.1266559213399887 + 100.0 * 6.256293296813965
Epoch 1220, val loss: 0.777325451374054
Epoch 1230, training loss: 625.4391479492188 = 0.1234685480594635 + 100.0 * 6.253156661987305
Epoch 1230, val loss: 0.7806079387664795
Epoch 1240, training loss: 625.3646240234375 = 0.12042972445487976 + 100.0 * 6.252441883087158
Epoch 1240, val loss: 0.7840539216995239
Epoch 1250, training loss: 625.90478515625 = 0.11748164147138596 + 100.0 * 6.257873058319092
Epoch 1250, val loss: 0.7874115109443665
Epoch 1260, training loss: 625.4263305664062 = 0.11459685117006302 + 100.0 * 6.253117084503174
Epoch 1260, val loss: 0.7910698056221008
Epoch 1270, training loss: 625.4121704101562 = 0.11178264766931534 + 100.0 * 6.25300407409668
Epoch 1270, val loss: 0.7945130467414856
Epoch 1280, training loss: 625.6072387695312 = 0.10908444225788116 + 100.0 * 6.254981517791748
Epoch 1280, val loss: 0.7981493473052979
Epoch 1290, training loss: 625.214111328125 = 0.10643864423036575 + 100.0 * 6.251076698303223
Epoch 1290, val loss: 0.801581859588623
Epoch 1300, training loss: 625.543212890625 = 0.1038932129740715 + 100.0 * 6.254393100738525
Epoch 1300, val loss: 0.8053117990493774
Epoch 1310, training loss: 625.1444702148438 = 0.10138805955648422 + 100.0 * 6.250431060791016
Epoch 1310, val loss: 0.8088188171386719
Epoch 1320, training loss: 624.9887084960938 = 0.0990002229809761 + 100.0 * 6.248897075653076
Epoch 1320, val loss: 0.8125263452529907
Epoch 1330, training loss: 625.1122436523438 = 0.09669749438762665 + 100.0 * 6.250155448913574
Epoch 1330, val loss: 0.8162661790847778
Epoch 1340, training loss: 625.0468139648438 = 0.09440380334854126 + 100.0 * 6.249523639678955
Epoch 1340, val loss: 0.8198007941246033
Epoch 1350, training loss: 625.1079711914062 = 0.0921575054526329 + 100.0 * 6.250158309936523
Epoch 1350, val loss: 0.8235142230987549
Epoch 1360, training loss: 624.8720092773438 = 0.08997917920351028 + 100.0 * 6.2478203773498535
Epoch 1360, val loss: 0.8272536396980286
Epoch 1370, training loss: 624.7308349609375 = 0.08792446553707123 + 100.0 * 6.246428966522217
Epoch 1370, val loss: 0.8310542106628418
Epoch 1380, training loss: 625.1361694335938 = 0.08593001961708069 + 100.0 * 6.250502586364746
Epoch 1380, val loss: 0.8349406719207764
Epoch 1390, training loss: 624.6351928710938 = 0.08394289761781693 + 100.0 * 6.245512962341309
Epoch 1390, val loss: 0.8384115695953369
Epoch 1400, training loss: 624.57958984375 = 0.08203772455453873 + 100.0 * 6.244975566864014
Epoch 1400, val loss: 0.8423032164573669
Epoch 1410, training loss: 625.1040649414062 = 0.08019500225782394 + 100.0 * 6.250238418579102
Epoch 1410, val loss: 0.8460497260093689
Epoch 1420, training loss: 624.498779296875 = 0.0783611536026001 + 100.0 * 6.244204044342041
Epoch 1420, val loss: 0.8497721552848816
Epoch 1430, training loss: 624.4075317382812 = 0.07660810649394989 + 100.0 * 6.243309497833252
Epoch 1430, val loss: 0.8536180853843689
Epoch 1440, training loss: 624.378662109375 = 0.07491274923086166 + 100.0 * 6.243037700653076
Epoch 1440, val loss: 0.8574779033660889
Epoch 1450, training loss: 625.268310546875 = 0.07327823340892792 + 100.0 * 6.251950263977051
Epoch 1450, val loss: 0.8611668944358826
Epoch 1460, training loss: 624.892333984375 = 0.07162745296955109 + 100.0 * 6.248207092285156
Epoch 1460, val loss: 0.8649636507034302
Epoch 1470, training loss: 624.3824462890625 = 0.07001689821481705 + 100.0 * 6.243124485015869
Epoch 1470, val loss: 0.8687771558761597
Epoch 1480, training loss: 624.1447143554688 = 0.06850515305995941 + 100.0 * 6.240762233734131
Epoch 1480, val loss: 0.872623860836029
Epoch 1490, training loss: 624.1945190429688 = 0.06705386191606522 + 100.0 * 6.241274356842041
Epoch 1490, val loss: 0.876539945602417
Epoch 1500, training loss: 625.0758666992188 = 0.06563675403594971 + 100.0 * 6.250102519989014
Epoch 1500, val loss: 0.8801960945129395
Epoch 1510, training loss: 624.2385864257812 = 0.06415582448244095 + 100.0 * 6.241744518280029
Epoch 1510, val loss: 0.8840059041976929
Epoch 1520, training loss: 624.0440673828125 = 0.06278331577777863 + 100.0 * 6.239812850952148
Epoch 1520, val loss: 0.8877900838851929
Epoch 1530, training loss: 624.1046142578125 = 0.06147144362330437 + 100.0 * 6.240431308746338
Epoch 1530, val loss: 0.8916399478912354
Epoch 1540, training loss: 624.4540405273438 = 0.060179367661476135 + 100.0 * 6.243938446044922
Epoch 1540, val loss: 0.8953518271446228
Epoch 1550, training loss: 624.3674926757812 = 0.05890357121825218 + 100.0 * 6.243085861206055
Epoch 1550, val loss: 0.8989443778991699
Epoch 1560, training loss: 624.0477905273438 = 0.057662010192871094 + 100.0 * 6.239901065826416
Epoch 1560, val loss: 0.9028178453445435
Epoch 1570, training loss: 624.0283813476562 = 0.056460656225681305 + 100.0 * 6.239719390869141
Epoch 1570, val loss: 0.9064608812332153
Epoch 1580, training loss: 623.9905395507812 = 0.05530310422182083 + 100.0 * 6.239352226257324
Epoch 1580, val loss: 0.9102886915206909
Epoch 1590, training loss: 623.9406127929688 = 0.05417119711637497 + 100.0 * 6.238864421844482
Epoch 1590, val loss: 0.9139489531517029
Epoch 1600, training loss: 624.0362548828125 = 0.053061921149492264 + 100.0 * 6.239832401275635
Epoch 1600, val loss: 0.9177077412605286
Epoch 1610, training loss: 623.9051513671875 = 0.05198390409350395 + 100.0 * 6.238531589508057
Epoch 1610, val loss: 0.9215354323387146
Epoch 1620, training loss: 624.1578979492188 = 0.05093483254313469 + 100.0 * 6.241069316864014
Epoch 1620, val loss: 0.9251622557640076
Epoch 1630, training loss: 623.8860473632812 = 0.04990607127547264 + 100.0 * 6.238361358642578
Epoch 1630, val loss: 0.9287525415420532
Epoch 1640, training loss: 623.6995849609375 = 0.04888654127717018 + 100.0 * 6.236506938934326
Epoch 1640, val loss: 0.9324117302894592
Epoch 1650, training loss: 623.6328125 = 0.0479368194937706 + 100.0 * 6.235848903656006
Epoch 1650, val loss: 0.9362324476242065
Epoch 1660, training loss: 623.7659301757812 = 0.04700510948896408 + 100.0 * 6.237189292907715
Epoch 1660, val loss: 0.9397421479225159
Epoch 1670, training loss: 623.747314453125 = 0.04607684537768364 + 100.0 * 6.2370123863220215
Epoch 1670, val loss: 0.9433643221855164
Epoch 1680, training loss: 623.608154296875 = 0.04515409469604492 + 100.0 * 6.235630035400391
Epoch 1680, val loss: 0.9469581246376038
Epoch 1690, training loss: 623.8550415039062 = 0.044275619089603424 + 100.0 * 6.238108158111572
Epoch 1690, val loss: 0.9506267309188843
Epoch 1700, training loss: 623.491455078125 = 0.0434206947684288 + 100.0 * 6.234480381011963
Epoch 1700, val loss: 0.9540925025939941
Epoch 1710, training loss: 623.3587646484375 = 0.042586006224155426 + 100.0 * 6.233161926269531
Epoch 1710, val loss: 0.9577956795692444
Epoch 1720, training loss: 623.7423095703125 = 0.0417858250439167 + 100.0 * 6.23700475692749
Epoch 1720, val loss: 0.9613040685653687
Epoch 1730, training loss: 623.4763793945312 = 0.04097910597920418 + 100.0 * 6.234354019165039
Epoch 1730, val loss: 0.9647432565689087
Epoch 1740, training loss: 623.49658203125 = 0.04019530862569809 + 100.0 * 6.23456335067749
Epoch 1740, val loss: 0.9683368802070618
Epoch 1750, training loss: 623.31103515625 = 0.039439305663108826 + 100.0 * 6.232716083526611
Epoch 1750, val loss: 0.9718614220619202
Epoch 1760, training loss: 623.5095825195312 = 0.038716431707143784 + 100.0 * 6.234708786010742
Epoch 1760, val loss: 0.9753512740135193
Epoch 1770, training loss: 623.2676391601562 = 0.03798452019691467 + 100.0 * 6.232296466827393
Epoch 1770, val loss: 0.9787354469299316
Epoch 1780, training loss: 623.5620727539062 = 0.03728767856955528 + 100.0 * 6.23524808883667
Epoch 1780, val loss: 0.9820501208305359
Epoch 1790, training loss: 623.4302368164062 = 0.03661329299211502 + 100.0 * 6.233936309814453
Epoch 1790, val loss: 0.9855158925056458
Epoch 1800, training loss: 623.2425537109375 = 0.0359250083565712 + 100.0 * 6.2320661544799805
Epoch 1800, val loss: 0.9888995289802551
Epoch 1810, training loss: 623.0374145507812 = 0.03527195379137993 + 100.0 * 6.2300214767456055
Epoch 1810, val loss: 0.9924241900444031
Epoch 1820, training loss: 623.4373168945312 = 0.03465241566300392 + 100.0 * 6.2340264320373535
Epoch 1820, val loss: 0.9958969950675964
Epoch 1830, training loss: 623.2318115234375 = 0.03401840478181839 + 100.0 * 6.231978416442871
Epoch 1830, val loss: 0.9990023970603943
Epoch 1840, training loss: 623.2513427734375 = 0.0334017314016819 + 100.0 * 6.232179641723633
Epoch 1840, val loss: 1.0025402307510376
Epoch 1850, training loss: 623.0902099609375 = 0.03279814124107361 + 100.0 * 6.230574607849121
Epoch 1850, val loss: 1.0057586431503296
Epoch 1860, training loss: 623.0308837890625 = 0.032225944101810455 + 100.0 * 6.229986667633057
Epoch 1860, val loss: 1.0091413259506226
Epoch 1870, training loss: 622.8631591796875 = 0.03166653960943222 + 100.0 * 6.2283148765563965
Epoch 1870, val loss: 1.0124958753585815
Epoch 1880, training loss: 622.9862670898438 = 0.031128546223044395 + 100.0 * 6.229551315307617
Epoch 1880, val loss: 1.0158098936080933
Epoch 1890, training loss: 624.1770629882812 = 0.030595751479268074 + 100.0 * 6.241464614868164
Epoch 1890, val loss: 1.0189672708511353
Epoch 1900, training loss: 622.9896240234375 = 0.030036570504307747 + 100.0 * 6.229596138000488
Epoch 1900, val loss: 1.0221188068389893
Epoch 1910, training loss: 622.8262939453125 = 0.029517920687794685 + 100.0 * 6.227967262268066
Epoch 1910, val loss: 1.0253721475601196
Epoch 1920, training loss: 622.7679443359375 = 0.02902781032025814 + 100.0 * 6.227388858795166
Epoch 1920, val loss: 1.0287418365478516
Epoch 1930, training loss: 622.7149658203125 = 0.028549831360578537 + 100.0 * 6.226863861083984
Epoch 1930, val loss: 1.0319464206695557
Epoch 1940, training loss: 623.5900268554688 = 0.028099961578845978 + 100.0 * 6.23561954498291
Epoch 1940, val loss: 1.0350548028945923
Epoch 1950, training loss: 622.9773559570312 = 0.02759399637579918 + 100.0 * 6.229497909545898
Epoch 1950, val loss: 1.0383700132369995
Epoch 1960, training loss: 623.0160522460938 = 0.027134034782648087 + 100.0 * 6.229888916015625
Epoch 1960, val loss: 1.0413527488708496
Epoch 1970, training loss: 622.97607421875 = 0.026682816445827484 + 100.0 * 6.229494094848633
Epoch 1970, val loss: 1.0445637702941895
Epoch 1980, training loss: 622.843017578125 = 0.026250682771205902 + 100.0 * 6.22816801071167
Epoch 1980, val loss: 1.0475637912750244
Epoch 1990, training loss: 622.5792846679688 = 0.02582598105072975 + 100.0 * 6.225534439086914
Epoch 1990, val loss: 1.050804615020752
Epoch 2000, training loss: 622.6477661132812 = 0.025412332266569138 + 100.0 * 6.226223468780518
Epoch 2000, val loss: 1.0538897514343262
Epoch 2010, training loss: 623.341064453125 = 0.02501065842807293 + 100.0 * 6.233160495758057
Epoch 2010, val loss: 1.0568536520004272
Epoch 2020, training loss: 622.8999633789062 = 0.024612396955490112 + 100.0 * 6.228753566741943
Epoch 2020, val loss: 1.059962511062622
Epoch 2030, training loss: 622.62548828125 = 0.024204080924391747 + 100.0 * 6.226012706756592
Epoch 2030, val loss: 1.0629537105560303
Epoch 2040, training loss: 622.5121459960938 = 0.023833705112338066 + 100.0 * 6.224883556365967
Epoch 2040, val loss: 1.0661671161651611
Epoch 2050, training loss: 622.8924560546875 = 0.023467358201742172 + 100.0 * 6.228690147399902
Epoch 2050, val loss: 1.0691421031951904
Epoch 2060, training loss: 622.4755859375 = 0.023097606375813484 + 100.0 * 6.224525451660156
Epoch 2060, val loss: 1.0719804763793945
Epoch 2070, training loss: 622.697998046875 = 0.02273605205118656 + 100.0 * 6.226752281188965
Epoch 2070, val loss: 1.0749281644821167
Epoch 2080, training loss: 622.385986328125 = 0.022385353222489357 + 100.0 * 6.223636150360107
Epoch 2080, val loss: 1.0779447555541992
Epoch 2090, training loss: 622.5877075195312 = 0.022049911320209503 + 100.0 * 6.225656986236572
Epoch 2090, val loss: 1.0808813571929932
Epoch 2100, training loss: 622.6712036132812 = 0.021714136004447937 + 100.0 * 6.226494789123535
Epoch 2100, val loss: 1.0839145183563232
Epoch 2110, training loss: 622.5653686523438 = 0.021381594240665436 + 100.0 * 6.22544002532959
Epoch 2110, val loss: 1.086534023284912
Epoch 2120, training loss: 622.4790649414062 = 0.021060673519968987 + 100.0 * 6.22458028793335
Epoch 2120, val loss: 1.089573860168457
Epoch 2130, training loss: 622.4296264648438 = 0.020747805014252663 + 100.0 * 6.224088668823242
Epoch 2130, val loss: 1.0924911499023438
Epoch 2140, training loss: 622.6102905273438 = 0.020438645035028458 + 100.0 * 6.225898265838623
Epoch 2140, val loss: 1.095237374305725
Epoch 2150, training loss: 622.3572998046875 = 0.020141026005148888 + 100.0 * 6.223371505737305
Epoch 2150, val loss: 1.0980074405670166
Epoch 2160, training loss: 622.3945922851562 = 0.01985050179064274 + 100.0 * 6.223747253417969
Epoch 2160, val loss: 1.1007025241851807
Epoch 2170, training loss: 622.2952880859375 = 0.019559171050786972 + 100.0 * 6.222757816314697
Epoch 2170, val loss: 1.1035910844802856
Epoch 2180, training loss: 622.2445678710938 = 0.01928316056728363 + 100.0 * 6.22225284576416
Epoch 2180, val loss: 1.106445550918579
Epoch 2190, training loss: 622.8748168945312 = 0.0190112441778183 + 100.0 * 6.228558540344238
Epoch 2190, val loss: 1.1089848279953003
Epoch 2200, training loss: 622.766845703125 = 0.01872539147734642 + 100.0 * 6.227481365203857
Epoch 2200, val loss: 1.111855149269104
Epoch 2210, training loss: 622.1298217773438 = 0.01845153421163559 + 100.0 * 6.221114158630371
Epoch 2210, val loss: 1.114497423171997
Epoch 2220, training loss: 622.0868530273438 = 0.01819676160812378 + 100.0 * 6.220686912536621
Epoch 2220, val loss: 1.1173884868621826
Epoch 2230, training loss: 622.043212890625 = 0.017948107793927193 + 100.0 * 6.220252990722656
Epoch 2230, val loss: 1.1201080083847046
Epoch 2240, training loss: 622.3623657226562 = 0.01770797371864319 + 100.0 * 6.223446369171143
Epoch 2240, val loss: 1.1229652166366577
Epoch 2250, training loss: 622.2925415039062 = 0.01746041513979435 + 100.0 * 6.222751140594482
Epoch 2250, val loss: 1.1254874467849731
Epoch 2260, training loss: 622.2434692382812 = 0.017213715240359306 + 100.0 * 6.222262382507324
Epoch 2260, val loss: 1.1279455423355103
Epoch 2270, training loss: 622.2918090820312 = 0.01697761006653309 + 100.0 * 6.222748279571533
Epoch 2270, val loss: 1.1304272413253784
Epoch 2280, training loss: 622.0538330078125 = 0.01674623042345047 + 100.0 * 6.220371246337891
Epoch 2280, val loss: 1.1332374811172485
Epoch 2290, training loss: 621.9577026367188 = 0.016527578234672546 + 100.0 * 6.219411849975586
Epoch 2290, val loss: 1.1357842683792114
Epoch 2300, training loss: 622.0834350585938 = 0.01631535217165947 + 100.0 * 6.2206711769104
Epoch 2300, val loss: 1.1383658647537231
Epoch 2310, training loss: 622.1864624023438 = 0.016100887209177017 + 100.0 * 6.22170352935791
Epoch 2310, val loss: 1.1408077478408813
Epoch 2320, training loss: 622.3851318359375 = 0.01588391698896885 + 100.0 * 6.223692417144775
Epoch 2320, val loss: 1.143560767173767
Epoch 2330, training loss: 622.1094970703125 = 0.015671804547309875 + 100.0 * 6.220938205718994
Epoch 2330, val loss: 1.1457222700119019
Epoch 2340, training loss: 622.0458374023438 = 0.01547196414321661 + 100.0 * 6.220304012298584
Epoch 2340, val loss: 1.1482031345367432
Epoch 2350, training loss: 622.043701171875 = 0.015269672498106956 + 100.0 * 6.220284461975098
Epoch 2350, val loss: 1.1507930755615234
Epoch 2360, training loss: 621.9318237304688 = 0.015080943703651428 + 100.0 * 6.219167709350586
Epoch 2360, val loss: 1.1532886028289795
Epoch 2370, training loss: 622.0984497070312 = 0.01489662379026413 + 100.0 * 6.2208356857299805
Epoch 2370, val loss: 1.1555595397949219
Epoch 2380, training loss: 622.1301879882812 = 0.014704377390444279 + 100.0 * 6.221154689788818
Epoch 2380, val loss: 1.1581447124481201
Epoch 2390, training loss: 621.9530029296875 = 0.014512317255139351 + 100.0 * 6.219384670257568
Epoch 2390, val loss: 1.1604316234588623
Epoch 2400, training loss: 622.1527709960938 = 0.014336883090436459 + 100.0 * 6.221384048461914
Epoch 2400, val loss: 1.1628221273422241
Epoch 2410, training loss: 621.8378295898438 = 0.01415209285914898 + 100.0 * 6.218236446380615
Epoch 2410, val loss: 1.165204405784607
Epoch 2420, training loss: 621.7809448242188 = 0.013982411473989487 + 100.0 * 6.21766996383667
Epoch 2420, val loss: 1.1676973104476929
Epoch 2430, training loss: 621.8834228515625 = 0.013818706385791302 + 100.0 * 6.218696594238281
Epoch 2430, val loss: 1.1698341369628906
Epoch 2440, training loss: 622.17578125 = 0.01364697515964508 + 100.0 * 6.221621036529541
Epoch 2440, val loss: 1.1721564531326294
Epoch 2450, training loss: 621.8067626953125 = 0.01347561739385128 + 100.0 * 6.21793270111084
Epoch 2450, val loss: 1.1747970581054688
Epoch 2460, training loss: 621.7411499023438 = 0.01331972237676382 + 100.0 * 6.217278480529785
Epoch 2460, val loss: 1.176863670349121
Epoch 2470, training loss: 622.1922607421875 = 0.013165323063731194 + 100.0 * 6.221790790557861
Epoch 2470, val loss: 1.1793148517608643
Epoch 2480, training loss: 621.7984008789062 = 0.013000442646443844 + 100.0 * 6.217854022979736
Epoch 2480, val loss: 1.181425929069519
Epoch 2490, training loss: 621.6281127929688 = 0.01284870132803917 + 100.0 * 6.216152191162109
Epoch 2490, val loss: 1.1836771965026855
Epoch 2500, training loss: 621.5697631835938 = 0.012698772363364697 + 100.0 * 6.215570449829102
Epoch 2500, val loss: 1.1860918998718262
Epoch 2510, training loss: 621.6510009765625 = 0.012556653469800949 + 100.0 * 6.216384410858154
Epoch 2510, val loss: 1.1882643699645996
Epoch 2520, training loss: 622.2635498046875 = 0.012420459650456905 + 100.0 * 6.2225117683410645
Epoch 2520, val loss: 1.1903655529022217
Epoch 2530, training loss: 621.9901733398438 = 0.01226512249559164 + 100.0 * 6.219779014587402
Epoch 2530, val loss: 1.192548155784607
Epoch 2540, training loss: 621.8347778320312 = 0.012120770290493965 + 100.0 * 6.218226432800293
Epoch 2540, val loss: 1.194788932800293
Epoch 2550, training loss: 621.6607666015625 = 0.011982887051999569 + 100.0 * 6.216487884521484
Epoch 2550, val loss: 1.1969521045684814
Epoch 2560, training loss: 621.6935424804688 = 0.011850609444081783 + 100.0 * 6.2168169021606445
Epoch 2560, val loss: 1.1990737915039062
Epoch 2570, training loss: 621.6453857421875 = 0.011720134876668453 + 100.0 * 6.216336727142334
Epoch 2570, val loss: 1.2013260126113892
Epoch 2580, training loss: 621.7355346679688 = 0.01159227266907692 + 100.0 * 6.2172393798828125
Epoch 2580, val loss: 1.203575611114502
Epoch 2590, training loss: 621.4970092773438 = 0.011461338959634304 + 100.0 * 6.214855194091797
Epoch 2590, val loss: 1.2056207656860352
Epoch 2600, training loss: 621.6438598632812 = 0.011340875178575516 + 100.0 * 6.216325283050537
Epoch 2600, val loss: 1.2075616121292114
Epoch 2610, training loss: 621.7958984375 = 0.011216884478926659 + 100.0 * 6.217846870422363
Epoch 2610, val loss: 1.2095179557800293
Epoch 2620, training loss: 621.5655517578125 = 0.011091014370322227 + 100.0 * 6.215544700622559
Epoch 2620, val loss: 1.211962103843689
Epoch 2630, training loss: 621.673095703125 = 0.010971101000905037 + 100.0 * 6.216620922088623
Epoch 2630, val loss: 1.2137607336044312
Epoch 2640, training loss: 621.5264892578125 = 0.010857373476028442 + 100.0 * 6.215156555175781
Epoch 2640, val loss: 1.2157411575317383
Epoch 2650, training loss: 621.6235961914062 = 0.010741546750068665 + 100.0 * 6.216128826141357
Epoch 2650, val loss: 1.2178622484207153
Epoch 2660, training loss: 621.4688720703125 = 0.01062562596052885 + 100.0 * 6.214582443237305
Epoch 2660, val loss: 1.2198436260223389
Epoch 2670, training loss: 621.748046875 = 0.010515835136175156 + 100.0 * 6.2173752784729
Epoch 2670, val loss: 1.221749186515808
Epoch 2680, training loss: 621.4058837890625 = 0.010402717627584934 + 100.0 * 6.213954925537109
Epoch 2680, val loss: 1.2238309383392334
Epoch 2690, training loss: 621.3363037109375 = 0.010295646265149117 + 100.0 * 6.213259696960449
Epoch 2690, val loss: 1.2257956266403198
Epoch 2700, training loss: 621.4320068359375 = 0.010192102752625942 + 100.0 * 6.2142181396484375
Epoch 2700, val loss: 1.2277003526687622
Epoch 2710, training loss: 621.60009765625 = 0.010088725946843624 + 100.0 * 6.215900421142578
Epoch 2710, val loss: 1.229739785194397
Epoch 2720, training loss: 621.790771484375 = 0.00998708326369524 + 100.0 * 6.217807769775391
Epoch 2720, val loss: 1.2317802906036377
Epoch 2730, training loss: 621.275146484375 = 0.009880597703158855 + 100.0 * 6.212652683258057
Epoch 2730, val loss: 1.2334460020065308
Epoch 2740, training loss: 621.2249755859375 = 0.009783362969756126 + 100.0 * 6.212152004241943
Epoch 2740, val loss: 1.2355350255966187
Epoch 2750, training loss: 621.3781127929688 = 0.00969098974019289 + 100.0 * 6.21368408203125
Epoch 2750, val loss: 1.2372839450836182
Epoch 2760, training loss: 621.7492065429688 = 0.009598888456821442 + 100.0 * 6.217396259307861
Epoch 2760, val loss: 1.2390385866165161
Epoch 2770, training loss: 621.47021484375 = 0.00949716754257679 + 100.0 * 6.214607238769531
Epoch 2770, val loss: 1.2411317825317383
Epoch 2780, training loss: 621.2041015625 = 0.00940481573343277 + 100.0 * 6.211946964263916
Epoch 2780, val loss: 1.2427984476089478
Epoch 2790, training loss: 621.3140869140625 = 0.009316874668002129 + 100.0 * 6.213047504425049
Epoch 2790, val loss: 1.2447519302368164
Epoch 2800, training loss: 621.4261474609375 = 0.009227599948644638 + 100.0 * 6.214169025421143
Epoch 2800, val loss: 1.246683955192566
Epoch 2810, training loss: 621.3272705078125 = 0.009136473760008812 + 100.0 * 6.213181018829346
Epoch 2810, val loss: 1.2484859228134155
Epoch 2820, training loss: 621.2028198242188 = 0.00905301608145237 + 100.0 * 6.21193790435791
Epoch 2820, val loss: 1.2501349449157715
Epoch 2830, training loss: 621.7286376953125 = 0.008973542600870132 + 100.0 * 6.217196464538574
Epoch 2830, val loss: 1.2518296241760254
Epoch 2840, training loss: 621.1456909179688 = 0.008876768872141838 + 100.0 * 6.211368560791016
Epoch 2840, val loss: 1.253892183303833
Epoch 2850, training loss: 621.1133422851562 = 0.008797022514045238 + 100.0 * 6.211045742034912
Epoch 2850, val loss: 1.255557656288147
Epoch 2860, training loss: 621.0220336914062 = 0.008718695491552353 + 100.0 * 6.210133075714111
Epoch 2860, val loss: 1.257279872894287
Epoch 2870, training loss: 622.07275390625 = 0.008646892383694649 + 100.0 * 6.220641136169434
Epoch 2870, val loss: 1.2589226961135864
Epoch 2880, training loss: 621.4697265625 = 0.008560052141547203 + 100.0 * 6.214611530303955
Epoch 2880, val loss: 1.2608680725097656
Epoch 2890, training loss: 621.1287231445312 = 0.008476372808218002 + 100.0 * 6.211202621459961
Epoch 2890, val loss: 1.2623480558395386
Epoch 2900, training loss: 621.044189453125 = 0.008402829058468342 + 100.0 * 6.210357666015625
Epoch 2900, val loss: 1.264298915863037
Epoch 2910, training loss: 621.2667236328125 = 0.008330625481903553 + 100.0 * 6.212584495544434
Epoch 2910, val loss: 1.2661164999008179
Epoch 2920, training loss: 621.2237548828125 = 0.00825593713670969 + 100.0 * 6.212154865264893
Epoch 2920, val loss: 1.2677435874938965
Epoch 2930, training loss: 621.1947021484375 = 0.008182898163795471 + 100.0 * 6.211865425109863
Epoch 2930, val loss: 1.2691376209259033
Epoch 2940, training loss: 621.5354614257812 = 0.008109265007078648 + 100.0 * 6.215273380279541
Epoch 2940, val loss: 1.271050214767456
Epoch 2950, training loss: 621.2332153320312 = 0.008034955710172653 + 100.0 * 6.212251663208008
Epoch 2950, val loss: 1.2723932266235352
Epoch 2960, training loss: 621.0000610351562 = 0.0079655097797513 + 100.0 * 6.209920406341553
Epoch 2960, val loss: 1.2742706537246704
Epoch 2970, training loss: 620.9434814453125 = 0.007898690178990364 + 100.0 * 6.20935583114624
Epoch 2970, val loss: 1.2758560180664062
Epoch 2980, training loss: 621.0491943359375 = 0.00783330388367176 + 100.0 * 6.210413932800293
Epoch 2980, val loss: 1.2776492834091187
Epoch 2990, training loss: 621.22021484375 = 0.0077680968679487705 + 100.0 * 6.212124347686768
Epoch 2990, val loss: 1.2791297435760498
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 861.6341552734375 = 1.9471824169158936 + 100.0 * 8.596869468688965
Epoch 0, val loss: 1.9548968076705933
Epoch 10, training loss: 861.572998046875 = 1.9382741451263428 + 100.0 * 8.596346855163574
Epoch 10, val loss: 1.9450188875198364
Epoch 20, training loss: 861.1959838867188 = 1.926837682723999 + 100.0 * 8.592691421508789
Epoch 20, val loss: 1.9322434663772583
Epoch 30, training loss: 858.8082885742188 = 1.9112855195999146 + 100.0 * 8.5689697265625
Epoch 30, val loss: 1.9149452447891235
Epoch 40, training loss: 847.767822265625 = 1.8903625011444092 + 100.0 * 8.45877456665039
Epoch 40, val loss: 1.8930788040161133
Epoch 50, training loss: 816.7342529296875 = 1.8669648170471191 + 100.0 * 8.148673057556152
Epoch 50, val loss: 1.8701833486557007
Epoch 60, training loss: 782.1497192382812 = 1.8468579053878784 + 100.0 * 7.803028583526611
Epoch 60, val loss: 1.8512974977493286
Epoch 70, training loss: 730.394775390625 = 1.8325600624084473 + 100.0 * 7.2856221199035645
Epoch 70, val loss: 1.8375954627990723
Epoch 80, training loss: 702.535400390625 = 1.8214677572250366 + 100.0 * 7.007139205932617
Epoch 80, val loss: 1.826411485671997
Epoch 90, training loss: 691.7857055664062 = 1.8105189800262451 + 100.0 * 6.899751663208008
Epoch 90, val loss: 1.814665675163269
Epoch 100, training loss: 683.9472045898438 = 1.7972908020019531 + 100.0 * 6.821498870849609
Epoch 100, val loss: 1.8009742498397827
Epoch 110, training loss: 679.0081176757812 = 1.7843791246414185 + 100.0 * 6.772237300872803
Epoch 110, val loss: 1.7875181436538696
Epoch 120, training loss: 675.2399291992188 = 1.7721174955368042 + 100.0 * 6.734678268432617
Epoch 120, val loss: 1.7750316858291626
Epoch 130, training loss: 671.8687133789062 = 1.7608133554458618 + 100.0 * 6.701079368591309
Epoch 130, val loss: 1.763675332069397
Epoch 140, training loss: 668.6438598632812 = 1.749558448791504 + 100.0 * 6.668942928314209
Epoch 140, val loss: 1.752560019493103
Epoch 150, training loss: 665.7100219726562 = 1.7375844717025757 + 100.0 * 6.639724254608154
Epoch 150, val loss: 1.7411222457885742
Epoch 160, training loss: 662.936279296875 = 1.7248220443725586 + 100.0 * 6.612114906311035
Epoch 160, val loss: 1.729131817817688
Epoch 170, training loss: 660.6805419921875 = 1.7111557722091675 + 100.0 * 6.589693546295166
Epoch 170, val loss: 1.7163934707641602
Epoch 180, training loss: 658.3650512695312 = 1.6962462663650513 + 100.0 * 6.566688060760498
Epoch 180, val loss: 1.7026405334472656
Epoch 190, training loss: 656.5230712890625 = 1.6801838874816895 + 100.0 * 6.548429012298584
Epoch 190, val loss: 1.6878660917282104
Epoch 200, training loss: 654.85693359375 = 1.6628539562225342 + 100.0 * 6.531940937042236
Epoch 200, val loss: 1.6721447706222534
Epoch 210, training loss: 653.4190673828125 = 1.644347906112671 + 100.0 * 6.517747402191162
Epoch 210, val loss: 1.6554641723632812
Epoch 220, training loss: 652.2147827148438 = 1.6245489120483398 + 100.0 * 6.50590181350708
Epoch 220, val loss: 1.637911319732666
Epoch 230, training loss: 650.913818359375 = 1.6036359071731567 + 100.0 * 6.493102073669434
Epoch 230, val loss: 1.619400978088379
Epoch 240, training loss: 650.1056518554688 = 1.5815845727920532 + 100.0 * 6.485240459442139
Epoch 240, val loss: 1.6000802516937256
Epoch 250, training loss: 648.9627685546875 = 1.5584102869033813 + 100.0 * 6.474043846130371
Epoch 250, val loss: 1.5801279544830322
Epoch 260, training loss: 648.0382690429688 = 1.5342950820922852 + 100.0 * 6.4650397300720215
Epoch 260, val loss: 1.559639573097229
Epoch 270, training loss: 647.5745849609375 = 1.5092829465866089 + 100.0 * 6.460653305053711
Epoch 270, val loss: 1.538734793663025
Epoch 280, training loss: 646.4864501953125 = 1.4835752248764038 + 100.0 * 6.450028419494629
Epoch 280, val loss: 1.5174965858459473
Epoch 290, training loss: 645.8379516601562 = 1.4574699401855469 + 100.0 * 6.443804740905762
Epoch 290, val loss: 1.4963221549987793
Epoch 300, training loss: 645.2890014648438 = 1.430981993675232 + 100.0 * 6.438580513000488
Epoch 300, val loss: 1.4752613306045532
Epoch 310, training loss: 644.3819580078125 = 1.4043147563934326 + 100.0 * 6.429776668548584
Epoch 310, val loss: 1.4543559551239014
Epoch 320, training loss: 643.8526000976562 = 1.3775277137756348 + 100.0 * 6.424751281738281
Epoch 320, val loss: 1.4337258338928223
Epoch 330, training loss: 643.1946411132812 = 1.350697636604309 + 100.0 * 6.4184393882751465
Epoch 330, val loss: 1.4135181903839111
Epoch 340, training loss: 642.5181274414062 = 1.3238211870193481 + 100.0 * 6.411943435668945
Epoch 340, val loss: 1.3935009241104126
Epoch 350, training loss: 642.0224609375 = 1.29703688621521 + 100.0 * 6.407253742218018
Epoch 350, val loss: 1.3738000392913818
Epoch 360, training loss: 641.5267944335938 = 1.2703059911727905 + 100.0 * 6.402565002441406
Epoch 360, val loss: 1.354306697845459
Epoch 370, training loss: 640.7605590820312 = 1.243642807006836 + 100.0 * 6.395168781280518
Epoch 370, val loss: 1.3351272344589233
Epoch 380, training loss: 640.1521606445312 = 1.2171919345855713 + 100.0 * 6.389349937438965
Epoch 380, val loss: 1.3163092136383057
Epoch 390, training loss: 640.197509765625 = 1.190989375114441 + 100.0 * 6.3900651931762695
Epoch 390, val loss: 1.297713279724121
Epoch 400, training loss: 639.232421875 = 1.164896845817566 + 100.0 * 6.380675315856934
Epoch 400, val loss: 1.2794047594070435
Epoch 410, training loss: 638.7362060546875 = 1.1393048763275146 + 100.0 * 6.375969409942627
Epoch 410, val loss: 1.2617437839508057
Epoch 420, training loss: 638.7488403320312 = 1.1141875982284546 + 100.0 * 6.376346588134766
Epoch 420, val loss: 1.2444591522216797
Epoch 430, training loss: 637.96728515625 = 1.0891848802566528 + 100.0 * 6.368781089782715
Epoch 430, val loss: 1.2275919914245605
Epoch 440, training loss: 637.5220947265625 = 1.06484055519104 + 100.0 * 6.364573001861572
Epoch 440, val loss: 1.2112175226211548
Epoch 450, training loss: 637.1616821289062 = 1.0411005020141602 + 100.0 * 6.3612060546875
Epoch 450, val loss: 1.1954896450042725
Epoch 460, training loss: 637.4937744140625 = 1.0179164409637451 + 100.0 * 6.364758014678955
Epoch 460, val loss: 1.1803539991378784
Epoch 470, training loss: 636.5078735351562 = 0.9951854348182678 + 100.0 * 6.355126857757568
Epoch 470, val loss: 1.1658532619476318
Epoch 480, training loss: 636.1768188476562 = 0.9731500744819641 + 100.0 * 6.352036952972412
Epoch 480, val loss: 1.1521241664886475
Epoch 490, training loss: 635.858642578125 = 0.9518206715583801 + 100.0 * 6.3490681648254395
Epoch 490, val loss: 1.1390371322631836
Epoch 500, training loss: 635.781494140625 = 0.9309157729148865 + 100.0 * 6.348505973815918
Epoch 500, val loss: 1.126542568206787
Epoch 510, training loss: 635.2986450195312 = 0.9105653166770935 + 100.0 * 6.343880653381348
Epoch 510, val loss: 1.114471435546875
Epoch 520, training loss: 635.004638671875 = 0.8908550143241882 + 100.0 * 6.341137409210205
Epoch 520, val loss: 1.1033282279968262
Epoch 530, training loss: 634.980224609375 = 0.8717235326766968 + 100.0 * 6.341085433959961
Epoch 530, val loss: 1.0928325653076172
Epoch 540, training loss: 634.5142211914062 = 0.8530396819114685 + 100.0 * 6.336612224578857
Epoch 540, val loss: 1.082916498184204
Epoch 550, training loss: 634.3721313476562 = 0.8349186182022095 + 100.0 * 6.335372447967529
Epoch 550, val loss: 1.0736414194107056
Epoch 560, training loss: 633.9510498046875 = 0.8171443343162537 + 100.0 * 6.331338882446289
Epoch 560, val loss: 1.0648974180221558
Epoch 570, training loss: 633.8801879882812 = 0.7998452186584473 + 100.0 * 6.330803394317627
Epoch 570, val loss: 1.0567219257354736
Epoch 580, training loss: 633.492919921875 = 0.7829564213752747 + 100.0 * 6.327099800109863
Epoch 580, val loss: 1.0488041639328003
Epoch 590, training loss: 633.2460327148438 = 0.7664762735366821 + 100.0 * 6.324795722961426
Epoch 590, val loss: 1.0416573286056519
Epoch 600, training loss: 633.2955322265625 = 0.7503846287727356 + 100.0 * 6.325451850891113
Epoch 600, val loss: 1.034868836402893
Epoch 610, training loss: 633.0693969726562 = 0.734603762626648 + 100.0 * 6.323347568511963
Epoch 610, val loss: 1.0284173488616943
Epoch 620, training loss: 632.5922241210938 = 0.7191146612167358 + 100.0 * 6.318731307983398
Epoch 620, val loss: 1.0225266218185425
Epoch 630, training loss: 632.5498657226562 = 0.7040340900421143 + 100.0 * 6.318458080291748
Epoch 630, val loss: 1.016977071762085
Epoch 640, training loss: 632.3505859375 = 0.689238429069519 + 100.0 * 6.316613674163818
Epoch 640, val loss: 1.0117706060409546
Epoch 650, training loss: 632.126708984375 = 0.6746490597724915 + 100.0 * 6.314520835876465
Epoch 650, val loss: 1.0069001913070679
Epoch 660, training loss: 631.8555297851562 = 0.6603988409042358 + 100.0 * 6.311951160430908
Epoch 660, val loss: 1.0022910833358765
Epoch 670, training loss: 632.0734252929688 = 0.6464274525642395 + 100.0 * 6.31427001953125
Epoch 670, val loss: 0.9980463981628418
Epoch 680, training loss: 631.6079711914062 = 0.632657527923584 + 100.0 * 6.30975341796875
Epoch 680, val loss: 0.9940287470817566
Epoch 690, training loss: 631.3455200195312 = 0.6191385388374329 + 100.0 * 6.3072638511657715
Epoch 690, val loss: 0.9901719689369202
Epoch 700, training loss: 631.6912841796875 = 0.6059348583221436 + 100.0 * 6.310853481292725
Epoch 700, val loss: 0.9865965843200684
Epoch 710, training loss: 631.2100219726562 = 0.5928639769554138 + 100.0 * 6.306171894073486
Epoch 710, val loss: 0.9833346605300903
Epoch 720, training loss: 630.8779907226562 = 0.5800418257713318 + 100.0 * 6.302979469299316
Epoch 720, val loss: 0.9802806973457336
Epoch 730, training loss: 630.9951171875 = 0.5674463510513306 + 100.0 * 6.304276943206787
Epoch 730, val loss: 0.977279782295227
Epoch 740, training loss: 630.5271606445312 = 0.5549803972244263 + 100.0 * 6.299721717834473
Epoch 740, val loss: 0.9746311902999878
Epoch 750, training loss: 630.4487915039062 = 0.5427735447883606 + 100.0 * 6.299060344696045
Epoch 750, val loss: 0.9720548391342163
Epoch 760, training loss: 631.225341796875 = 0.530705451965332 + 100.0 * 6.306946277618408
Epoch 760, val loss: 0.9696216583251953
Epoch 770, training loss: 630.185791015625 = 0.5186612606048584 + 100.0 * 6.296671390533447
Epoch 770, val loss: 0.9670131206512451
Epoch 780, training loss: 630.0657958984375 = 0.5068631768226624 + 100.0 * 6.295589447021484
Epoch 780, val loss: 0.9646314978599548
Epoch 790, training loss: 629.8584594726562 = 0.49525904655456543 + 100.0 * 6.2936320304870605
Epoch 790, val loss: 0.962489128112793
Epoch 800, training loss: 630.558349609375 = 0.48381027579307556 + 100.0 * 6.300745010375977
Epoch 800, val loss: 0.9604280591011047
Epoch 810, training loss: 630.0829467773438 = 0.47226741909980774 + 100.0 * 6.296106815338135
Epoch 810, val loss: 0.958015501499176
Epoch 820, training loss: 629.6351928710938 = 0.4609070420265198 + 100.0 * 6.291742324829102
Epoch 820, val loss: 0.9558917284011841
Epoch 830, training loss: 629.3687744140625 = 0.4497623145580292 + 100.0 * 6.28918981552124
Epoch 830, val loss: 0.9539393782615662
Epoch 840, training loss: 629.37255859375 = 0.43874120712280273 + 100.0 * 6.2893385887146
Epoch 840, val loss: 0.9519838690757751
Epoch 850, training loss: 629.3137817382812 = 0.4277441203594208 + 100.0 * 6.288860321044922
Epoch 850, val loss: 0.9498269557952881
Epoch 860, training loss: 629.3395385742188 = 0.41678324341773987 + 100.0 * 6.289227485656738
Epoch 860, val loss: 0.947681188583374
Epoch 870, training loss: 629.0180053710938 = 0.4059617519378662 + 100.0 * 6.286120414733887
Epoch 870, val loss: 0.9456837773323059
Epoch 880, training loss: 628.9212036132812 = 0.3953021466732025 + 100.0 * 6.285258769989014
Epoch 880, val loss: 0.9437764883041382
Epoch 890, training loss: 629.1566772460938 = 0.38474103808403015 + 100.0 * 6.287719249725342
Epoch 890, val loss: 0.9418749809265137
Epoch 900, training loss: 628.9217529296875 = 0.3742462396621704 + 100.0 * 6.28547477722168
Epoch 900, val loss: 0.9396462440490723
Epoch 910, training loss: 628.6345825195312 = 0.36387959122657776 + 100.0 * 6.2827067375183105
Epoch 910, val loss: 0.9377338290214539
Epoch 920, training loss: 628.695068359375 = 0.35369032621383667 + 100.0 * 6.283413410186768
Epoch 920, val loss: 0.9359495639801025
Epoch 930, training loss: 628.4786987304688 = 0.34361502528190613 + 100.0 * 6.281350612640381
Epoch 930, val loss: 0.9342159628868103
Epoch 940, training loss: 628.3477783203125 = 0.3337228298187256 + 100.0 * 6.280140399932861
Epoch 940, val loss: 0.9326542615890503
Epoch 950, training loss: 628.3436889648438 = 0.32403600215911865 + 100.0 * 6.280196666717529
Epoch 950, val loss: 0.9311965107917786
Epoch 960, training loss: 629.1075439453125 = 0.31453725695610046 + 100.0 * 6.287930011749268
Epoch 960, val loss: 0.9296053647994995
Epoch 970, training loss: 628.4803466796875 = 0.3050582706928253 + 100.0 * 6.281753063201904
Epoch 970, val loss: 0.9285658597946167
Epoch 980, training loss: 627.9486083984375 = 0.2959103286266327 + 100.0 * 6.276527404785156
Epoch 980, val loss: 0.927464485168457
Epoch 990, training loss: 627.9329223632812 = 0.2870488464832306 + 100.0 * 6.276458740234375
Epoch 990, val loss: 0.926567792892456
Epoch 1000, training loss: 628.1646728515625 = 0.27841395139694214 + 100.0 * 6.278862476348877
Epoch 1000, val loss: 0.9259939789772034
Epoch 1010, training loss: 628.0828857421875 = 0.26992419362068176 + 100.0 * 6.278129577636719
Epoch 1010, val loss: 0.9252756237983704
Epoch 1020, training loss: 627.94140625 = 0.2616744041442871 + 100.0 * 6.276797771453857
Epoch 1020, val loss: 0.924842357635498
Epoch 1030, training loss: 627.602783203125 = 0.25366079807281494 + 100.0 * 6.273490905761719
Epoch 1030, val loss: 0.9247377514839172
Epoch 1040, training loss: 627.5593872070312 = 0.24596671760082245 + 100.0 * 6.273134231567383
Epoch 1040, val loss: 0.9248435497283936
Epoch 1050, training loss: 627.8726196289062 = 0.23850497603416443 + 100.0 * 6.276340961456299
Epoch 1050, val loss: 0.9251639246940613
Epoch 1060, training loss: 627.4782104492188 = 0.23116253316402435 + 100.0 * 6.272470951080322
Epoch 1060, val loss: 0.9258013963699341
Epoch 1070, training loss: 627.5482788085938 = 0.2241542488336563 + 100.0 * 6.27324104309082
Epoch 1070, val loss: 0.9266332983970642
Epoch 1080, training loss: 627.2638549804688 = 0.21730175614356995 + 100.0 * 6.270465850830078
Epoch 1080, val loss: 0.9275654554367065
Epoch 1090, training loss: 627.1885986328125 = 0.2107211947441101 + 100.0 * 6.269779205322266
Epoch 1090, val loss: 0.9288312196731567
Epoch 1100, training loss: 627.16650390625 = 0.20436991751194 + 100.0 * 6.269621849060059
Epoch 1100, val loss: 0.9303266406059265
Epoch 1110, training loss: 627.1350708007812 = 0.1982204169034958 + 100.0 * 6.2693681716918945
Epoch 1110, val loss: 0.9320164918899536
Epoch 1120, training loss: 627.680419921875 = 0.19227585196495056 + 100.0 * 6.274881362915039
Epoch 1120, val loss: 0.9336721897125244
Epoch 1130, training loss: 627.0663452148438 = 0.18645581603050232 + 100.0 * 6.268798828125
Epoch 1130, val loss: 0.9358033537864685
Epoch 1140, training loss: 626.84814453125 = 0.18088583648204803 + 100.0 * 6.266672134399414
Epoch 1140, val loss: 0.9380624890327454
Epoch 1150, training loss: 626.7196044921875 = 0.17555223405361176 + 100.0 * 6.265440940856934
Epoch 1150, val loss: 0.9406371116638184
Epoch 1160, training loss: 627.0370483398438 = 0.1704004555940628 + 100.0 * 6.2686662673950195
Epoch 1160, val loss: 0.9434133172035217
Epoch 1170, training loss: 626.8095092773438 = 0.16535256803035736 + 100.0 * 6.266441345214844
Epoch 1170, val loss: 0.9457392692565918
Epoch 1180, training loss: 626.8369750976562 = 0.1604841947555542 + 100.0 * 6.266765117645264
Epoch 1180, val loss: 0.9487667679786682
Epoch 1190, training loss: 626.5740966796875 = 0.1557927429676056 + 100.0 * 6.2641825675964355
Epoch 1190, val loss: 0.9517346620559692
Epoch 1200, training loss: 626.4464721679688 = 0.15128996968269348 + 100.0 * 6.262951850891113
Epoch 1200, val loss: 0.9550067186355591
Epoch 1210, training loss: 626.4033203125 = 0.1469581574201584 + 100.0 * 6.262563705444336
Epoch 1210, val loss: 0.9584320187568665
Epoch 1220, training loss: 626.7122192382812 = 0.1427563726902008 + 100.0 * 6.265694618225098
Epoch 1220, val loss: 0.9618129134178162
Epoch 1230, training loss: 626.27490234375 = 0.1386709362268448 + 100.0 * 6.261362552642822
Epoch 1230, val loss: 0.9652498364448547
Epoch 1240, training loss: 626.7135009765625 = 0.13472943007946014 + 100.0 * 6.2657880783081055
Epoch 1240, val loss: 0.9690493941307068
Epoch 1250, training loss: 626.374755859375 = 0.1309015154838562 + 100.0 * 6.2624382972717285
Epoch 1250, val loss: 0.9724151492118835
Epoch 1260, training loss: 626.3556518554688 = 0.1272277683019638 + 100.0 * 6.262284278869629
Epoch 1260, val loss: 0.9764056205749512
Epoch 1270, training loss: 626.080078125 = 0.12369398027658463 + 100.0 * 6.259563446044922
Epoch 1270, val loss: 0.9802701473236084
Epoch 1280, training loss: 625.9530639648438 = 0.12030497193336487 + 100.0 * 6.258327484130859
Epoch 1280, val loss: 0.9845390319824219
Epoch 1290, training loss: 626.2872924804688 = 0.11703895777463913 + 100.0 * 6.261702537536621
Epoch 1290, val loss: 0.988777220249176
Epoch 1300, training loss: 626.1920776367188 = 0.11385559290647507 + 100.0 * 6.260782241821289
Epoch 1300, val loss: 0.9924274682998657
Epoch 1310, training loss: 626.158447265625 = 0.11074349284172058 + 100.0 * 6.260477542877197
Epoch 1310, val loss: 0.996704638004303
Epoch 1320, training loss: 625.8217163085938 = 0.10775256156921387 + 100.0 * 6.257139682769775
Epoch 1320, val loss: 1.0008292198181152
Epoch 1330, training loss: 625.6636962890625 = 0.10489913076162338 + 100.0 * 6.255587577819824
Epoch 1330, val loss: 1.0052872896194458
Epoch 1340, training loss: 625.7542724609375 = 0.10215501487255096 + 100.0 * 6.256521224975586
Epoch 1340, val loss: 1.0097538232803345
Epoch 1350, training loss: 625.976318359375 = 0.09947289526462555 + 100.0 * 6.258768081665039
Epoch 1350, val loss: 1.01399827003479
Epoch 1360, training loss: 625.7718505859375 = 0.0968691036105156 + 100.0 * 6.256750106811523
Epoch 1360, val loss: 1.0181347131729126
Epoch 1370, training loss: 625.9796142578125 = 0.0943637490272522 + 100.0 * 6.258852481842041
Epoch 1370, val loss: 1.022618055343628
Epoch 1380, training loss: 625.6497192382812 = 0.09191839396953583 + 100.0 * 6.25557804107666
Epoch 1380, val loss: 1.0272011756896973
Epoch 1390, training loss: 625.3809204101562 = 0.08958574384450912 + 100.0 * 6.252913475036621
Epoch 1390, val loss: 1.0315824747085571
Epoch 1400, training loss: 625.3394165039062 = 0.08733803033828735 + 100.0 * 6.252520561218262
Epoch 1400, val loss: 1.0362029075622559
Epoch 1410, training loss: 625.9261474609375 = 0.08518295735120773 + 100.0 * 6.25840950012207
Epoch 1410, val loss: 1.0408837795257568
Epoch 1420, training loss: 625.45068359375 = 0.08303683996200562 + 100.0 * 6.253676891326904
Epoch 1420, val loss: 1.0452663898468018
Epoch 1430, training loss: 625.4505004882812 = 0.08099197596311569 + 100.0 * 6.253695011138916
Epoch 1430, val loss: 1.049992561340332
Epoch 1440, training loss: 625.5105590820312 = 0.07900603115558624 + 100.0 * 6.2543158531188965
Epoch 1440, val loss: 1.0544756650924683
Epoch 1450, training loss: 625.3705444335938 = 0.07706490904092789 + 100.0 * 6.252934455871582
Epoch 1450, val loss: 1.059149146080017
Epoch 1460, training loss: 625.1083984375 = 0.07520213723182678 + 100.0 * 6.250331878662109
Epoch 1460, val loss: 1.0639139413833618
Epoch 1470, training loss: 625.0419311523438 = 0.07342101633548737 + 100.0 * 6.249685287475586
Epoch 1470, val loss: 1.0687798261642456
Epoch 1480, training loss: 625.8140258789062 = 0.07169927656650543 + 100.0 * 6.257422924041748
Epoch 1480, val loss: 1.0736174583435059
Epoch 1490, training loss: 625.2335815429688 = 0.06998305767774582 + 100.0 * 6.251635551452637
Epoch 1490, val loss: 1.077850103378296
Epoch 1500, training loss: 625.012451171875 = 0.06832952797412872 + 100.0 * 6.249441146850586
Epoch 1500, val loss: 1.0827372074127197
Epoch 1510, training loss: 624.9456787109375 = 0.06676308810710907 + 100.0 * 6.248788833618164
Epoch 1510, val loss: 1.0874210596084595
Epoch 1520, training loss: 625.4657592773438 = 0.06524240970611572 + 100.0 * 6.254005432128906
Epoch 1520, val loss: 1.092034935951233
Epoch 1530, training loss: 624.9271240234375 = 0.06371666491031647 + 100.0 * 6.248633861541748
Epoch 1530, val loss: 1.0970677137374878
Epoch 1540, training loss: 624.8565673828125 = 0.062277860939502716 + 100.0 * 6.247942924499512
Epoch 1540, val loss: 1.1016485691070557
Epoch 1550, training loss: 624.8034057617188 = 0.06087740510702133 + 100.0 * 6.247425556182861
Epoch 1550, val loss: 1.106495976448059
Epoch 1560, training loss: 624.7058715820312 = 0.059518761932849884 + 100.0 * 6.246463775634766
Epoch 1560, val loss: 1.1112163066864014
Epoch 1570, training loss: 625.5279541015625 = 0.05820669233798981 + 100.0 * 6.254697322845459
Epoch 1570, val loss: 1.1158589124679565
Epoch 1580, training loss: 624.9057006835938 = 0.056894268840551376 + 100.0 * 6.248487949371338
Epoch 1580, val loss: 1.1206843852996826
Epoch 1590, training loss: 624.6583251953125 = 0.05565384775400162 + 100.0 * 6.246026515960693
Epoch 1590, val loss: 1.1252708435058594
Epoch 1600, training loss: 624.700439453125 = 0.05446009337902069 + 100.0 * 6.2464599609375
Epoch 1600, val loss: 1.1302456855773926
Epoch 1610, training loss: 624.7669677734375 = 0.053293172270059586 + 100.0 * 6.24713659286499
Epoch 1610, val loss: 1.1347119808197021
Epoch 1620, training loss: 624.715087890625 = 0.05214870721101761 + 100.0 * 6.24662971496582
Epoch 1620, val loss: 1.1392815113067627
Epoch 1630, training loss: 624.8802490234375 = 0.05103738233447075 + 100.0 * 6.248291969299316
Epoch 1630, val loss: 1.144089937210083
Epoch 1640, training loss: 624.5227661132812 = 0.04996184632182121 + 100.0 * 6.244727611541748
Epoch 1640, val loss: 1.1485648155212402
Epoch 1650, training loss: 624.3715209960938 = 0.04891551285982132 + 100.0 * 6.243226528167725
Epoch 1650, val loss: 1.1533061265945435
Epoch 1660, training loss: 624.540283203125 = 0.047913458198308945 + 100.0 * 6.2449235916137695
Epoch 1660, val loss: 1.1579208374023438
Epoch 1670, training loss: 624.59033203125 = 0.04692074656486511 + 100.0 * 6.245433807373047
Epoch 1670, val loss: 1.1623976230621338
Epoch 1680, training loss: 624.2325439453125 = 0.04594848304986954 + 100.0 * 6.241865634918213
Epoch 1680, val loss: 1.1668897867202759
Epoch 1690, training loss: 624.22314453125 = 0.04502396285533905 + 100.0 * 6.241781234741211
Epoch 1690, val loss: 1.1714802980422974
Epoch 1700, training loss: 624.375244140625 = 0.04412368685007095 + 100.0 * 6.243310928344727
Epoch 1700, val loss: 1.1761080026626587
Epoch 1710, training loss: 624.4794311523438 = 0.04324626177549362 + 100.0 * 6.244361400604248
Epoch 1710, val loss: 1.1806437969207764
Epoch 1720, training loss: 624.3856811523438 = 0.04237166792154312 + 100.0 * 6.243433475494385
Epoch 1720, val loss: 1.1848987340927124
Epoch 1730, training loss: 624.1073608398438 = 0.041535984724760056 + 100.0 * 6.240657806396484
Epoch 1730, val loss: 1.1893374919891357
Epoch 1740, training loss: 624.1946411132812 = 0.040731724351644516 + 100.0 * 6.241539478302002
Epoch 1740, val loss: 1.1939345598220825
Epoch 1750, training loss: 624.2665405273438 = 0.039941299706697464 + 100.0 * 6.2422661781311035
Epoch 1750, val loss: 1.1984705924987793
Epoch 1760, training loss: 624.1303100585938 = 0.03916487097740173 + 100.0 * 6.24091100692749
Epoch 1760, val loss: 1.2026370763778687
Epoch 1770, training loss: 624.086669921875 = 0.0384221188724041 + 100.0 * 6.240482330322266
Epoch 1770, val loss: 1.2069157361984253
Epoch 1780, training loss: 624.3875732421875 = 0.03769778460264206 + 100.0 * 6.243498802185059
Epoch 1780, val loss: 1.2112150192260742
Epoch 1790, training loss: 623.9219970703125 = 0.03697523474693298 + 100.0 * 6.238850116729736
Epoch 1790, val loss: 1.2158972024917603
Epoch 1800, training loss: 624.0202026367188 = 0.03629061579704285 + 100.0 * 6.23983907699585
Epoch 1800, val loss: 1.220214605331421
Epoch 1810, training loss: 623.9545288085938 = 0.03562118858098984 + 100.0 * 6.2391886711120605
Epoch 1810, val loss: 1.224417805671692
Epoch 1820, training loss: 624.2783813476562 = 0.034980498254299164 + 100.0 * 6.242434024810791
Epoch 1820, val loss: 1.2283961772918701
Epoch 1830, training loss: 624.3853149414062 = 0.03432013466954231 + 100.0 * 6.2435102462768555
Epoch 1830, val loss: 1.2328734397888184
Epoch 1840, training loss: 623.9487915039062 = 0.03368523344397545 + 100.0 * 6.2391510009765625
Epoch 1840, val loss: 1.2369797229766846
Epoch 1850, training loss: 623.7503662109375 = 0.03307938203215599 + 100.0 * 6.237172603607178
Epoch 1850, val loss: 1.241282343864441
Epoch 1860, training loss: 623.8285522460938 = 0.03250058740377426 + 100.0 * 6.2379608154296875
Epoch 1860, val loss: 1.2455953359603882
Epoch 1870, training loss: 624.0736694335938 = 0.03191986307501793 + 100.0 * 6.24041748046875
Epoch 1870, val loss: 1.2495514154434204
Epoch 1880, training loss: 623.7062377929688 = 0.03133875131607056 + 100.0 * 6.236749172210693
Epoch 1880, val loss: 1.2535967826843262
Epoch 1890, training loss: 623.6536865234375 = 0.030797583982348442 + 100.0 * 6.2362284660339355
Epoch 1890, val loss: 1.25771164894104
Epoch 1900, training loss: 624.1160278320312 = 0.03027063049376011 + 100.0 * 6.2408576011657715
Epoch 1900, val loss: 1.2616217136383057
Epoch 1910, training loss: 623.9421997070312 = 0.029734203591942787 + 100.0 * 6.239124774932861
Epoch 1910, val loss: 1.2660555839538574
Epoch 1920, training loss: 623.6589965820312 = 0.029215071350336075 + 100.0 * 6.236297607421875
Epoch 1920, val loss: 1.2698614597320557
Epoch 1930, training loss: 623.52392578125 = 0.028720956295728683 + 100.0 * 6.234951972961426
Epoch 1930, val loss: 1.2741647958755493
Epoch 1940, training loss: 623.6737670898438 = 0.02824537642300129 + 100.0 * 6.236454963684082
Epoch 1940, val loss: 1.2780729532241821
Epoch 1950, training loss: 623.56982421875 = 0.02776726521551609 + 100.0 * 6.2354207038879395
Epoch 1950, val loss: 1.2820936441421509
Epoch 1960, training loss: 623.735107421875 = 0.02730107679963112 + 100.0 * 6.2370781898498535
Epoch 1960, val loss: 1.2860113382339478
Epoch 1970, training loss: 623.4379272460938 = 0.02684730477631092 + 100.0 * 6.2341108322143555
Epoch 1970, val loss: 1.289788007736206
Epoch 1980, training loss: 623.5635375976562 = 0.026408890262246132 + 100.0 * 6.235371112823486
Epoch 1980, val loss: 1.2939982414245605
Epoch 1990, training loss: 623.8328857421875 = 0.025975454598665237 + 100.0 * 6.2380690574646
Epoch 1990, val loss: 1.2976194620132446
Epoch 2000, training loss: 623.344970703125 = 0.02553948201239109 + 100.0 * 6.233194351196289
Epoch 2000, val loss: 1.301154613494873
Epoch 2010, training loss: 623.2223510742188 = 0.025130001828074455 + 100.0 * 6.2319722175598145
Epoch 2010, val loss: 1.305080533027649
Epoch 2020, training loss: 623.18212890625 = 0.024736851453781128 + 100.0 * 6.231574058532715
Epoch 2020, val loss: 1.3091366291046143
Epoch 2030, training loss: 623.258544921875 = 0.024353081360459328 + 100.0 * 6.232341766357422
Epoch 2030, val loss: 1.3131637573242188
Epoch 2040, training loss: 624.0309448242188 = 0.02397789992392063 + 100.0 * 6.24006986618042
Epoch 2040, val loss: 1.316820740699768
Epoch 2050, training loss: 623.6194458007812 = 0.023578519001603127 + 100.0 * 6.235958576202393
Epoch 2050, val loss: 1.3199979066848755
Epoch 2060, training loss: 623.2596435546875 = 0.023204253986477852 + 100.0 * 6.232364654541016
Epoch 2060, val loss: 1.3238306045532227
Epoch 2070, training loss: 623.1384887695312 = 0.022847425192594528 + 100.0 * 6.231156826019287
Epoch 2070, val loss: 1.3277716636657715
Epoch 2080, training loss: 623.3336791992188 = 0.022510068491101265 + 100.0 * 6.233111381530762
Epoch 2080, val loss: 1.3315176963806152
Epoch 2090, training loss: 623.2557983398438 = 0.022164953872561455 + 100.0 * 6.232336521148682
Epoch 2090, val loss: 1.335134744644165
Epoch 2100, training loss: 623.0924072265625 = 0.021823668852448463 + 100.0 * 6.230705738067627
Epoch 2100, val loss: 1.3387880325317383
Epoch 2110, training loss: 623.1394653320312 = 0.02150125801563263 + 100.0 * 6.231179714202881
Epoch 2110, val loss: 1.3425052165985107
Epoch 2120, training loss: 623.5224609375 = 0.021184612065553665 + 100.0 * 6.235012531280518
Epoch 2120, val loss: 1.3459409475326538
Epoch 2130, training loss: 623.695068359375 = 0.020863423123955727 + 100.0 * 6.23674201965332
Epoch 2130, val loss: 1.3496696949005127
Epoch 2140, training loss: 623.015380859375 = 0.02054470218718052 + 100.0 * 6.229948043823242
Epoch 2140, val loss: 1.352940559387207
Epoch 2150, training loss: 622.9686279296875 = 0.020248182117938995 + 100.0 * 6.229483604431152
Epoch 2150, val loss: 1.3566898107528687
Epoch 2160, training loss: 622.8861694335938 = 0.019962597638368607 + 100.0 * 6.228662014007568
Epoch 2160, val loss: 1.3601384162902832
Epoch 2170, training loss: 623.0719604492188 = 0.019685467705130577 + 100.0 * 6.230522632598877
Epoch 2170, val loss: 1.3635979890823364
Epoch 2180, training loss: 623.1992797851562 = 0.019400812685489655 + 100.0 * 6.2317986488342285
Epoch 2180, val loss: 1.367063283920288
Epoch 2190, training loss: 622.9149169921875 = 0.019111040979623795 + 100.0 * 6.2289581298828125
Epoch 2190, val loss: 1.3705230951309204
Epoch 2200, training loss: 622.8636474609375 = 0.01884477771818638 + 100.0 * 6.228447914123535
Epoch 2200, val loss: 1.3739291429519653
Epoch 2210, training loss: 623.5146484375 = 0.018586572259664536 + 100.0 * 6.234960556030273
Epoch 2210, val loss: 1.377596139907837
Epoch 2220, training loss: 622.9503784179688 = 0.018317148089408875 + 100.0 * 6.229320526123047
Epoch 2220, val loss: 1.3806039094924927
Epoch 2230, training loss: 622.7857666015625 = 0.018063794821500778 + 100.0 * 6.227676868438721
Epoch 2230, val loss: 1.384170413017273
Epoch 2240, training loss: 622.699951171875 = 0.017819954082369804 + 100.0 * 6.226821422576904
Epoch 2240, val loss: 1.387591004371643
Epoch 2250, training loss: 623.2554321289062 = 0.017588943243026733 + 100.0 * 6.2323784828186035
Epoch 2250, val loss: 1.3910225629806519
Epoch 2260, training loss: 623.0122680664062 = 0.01733529381453991 + 100.0 * 6.229949474334717
Epoch 2260, val loss: 1.394019603729248
Epoch 2270, training loss: 622.8746948242188 = 0.017095422372221947 + 100.0 * 6.228576183319092
Epoch 2270, val loss: 1.397473692893982
Epoch 2280, training loss: 622.9275512695312 = 0.016865137964487076 + 100.0 * 6.229106903076172
Epoch 2280, val loss: 1.400976300239563
Epoch 2290, training loss: 622.7070922851562 = 0.01664087548851967 + 100.0 * 6.226904392242432
Epoch 2290, val loss: 1.4040580987930298
Epoch 2300, training loss: 622.8251342773438 = 0.016426049172878265 + 100.0 * 6.228086948394775
Epoch 2300, val loss: 1.4072647094726562
Epoch 2310, training loss: 623.1798706054688 = 0.016211483627557755 + 100.0 * 6.2316365242004395
Epoch 2310, val loss: 1.4103984832763672
Epoch 2320, training loss: 622.720703125 = 0.015994755551218987 + 100.0 * 6.227046966552734
Epoch 2320, val loss: 1.4133020639419556
Epoch 2330, training loss: 622.5317993164062 = 0.015789711847901344 + 100.0 * 6.225160121917725
Epoch 2330, val loss: 1.4167125225067139
Epoch 2340, training loss: 622.6702270507812 = 0.015595619566738605 + 100.0 * 6.226546287536621
Epoch 2340, val loss: 1.419704556465149
Epoch 2350, training loss: 622.9694213867188 = 0.015398746356368065 + 100.0 * 6.2295403480529785
Epoch 2350, val loss: 1.4226642847061157
Epoch 2360, training loss: 622.6190185546875 = 0.015195190906524658 + 100.0 * 6.226037979125977
Epoch 2360, val loss: 1.4262508153915405
Epoch 2370, training loss: 622.4723510742188 = 0.015006679110229015 + 100.0 * 6.224573135375977
Epoch 2370, val loss: 1.4290879964828491
Epoch 2380, training loss: 622.8060302734375 = 0.014825690537691116 + 100.0 * 6.227911949157715
Epoch 2380, val loss: 1.432300329208374
Epoch 2390, training loss: 622.4676513671875 = 0.014636819250881672 + 100.0 * 6.22452974319458
Epoch 2390, val loss: 1.4352647066116333
Epoch 2400, training loss: 622.6044311523438 = 0.014455660246312618 + 100.0 * 6.225899696350098
Epoch 2400, val loss: 1.4382684230804443
Epoch 2410, training loss: 622.592041015625 = 0.014277208596467972 + 100.0 * 6.225777626037598
Epoch 2410, val loss: 1.4413368701934814
Epoch 2420, training loss: 622.4976806640625 = 0.014101577922701836 + 100.0 * 6.2248358726501465
Epoch 2420, val loss: 1.4440668821334839
Epoch 2430, training loss: 622.4465942382812 = 0.01393325999379158 + 100.0 * 6.2243266105651855
Epoch 2430, val loss: 1.447140097618103
Epoch 2440, training loss: 622.859130859375 = 0.013769214041531086 + 100.0 * 6.228453636169434
Epoch 2440, val loss: 1.4502254724502563
Epoch 2450, training loss: 622.3547973632812 = 0.013600246049463749 + 100.0 * 6.223412036895752
Epoch 2450, val loss: 1.453244686126709
Epoch 2460, training loss: 622.2908935546875 = 0.013439309783279896 + 100.0 * 6.222774505615234
Epoch 2460, val loss: 1.4562175273895264
Epoch 2470, training loss: 622.2785034179688 = 0.013285627588629723 + 100.0 * 6.222651958465576
Epoch 2470, val loss: 1.4591957330703735
Epoch 2480, training loss: 623.0245361328125 = 0.013139637187123299 + 100.0 * 6.230113983154297
Epoch 2480, val loss: 1.4624844789505005
Epoch 2490, training loss: 622.5254516601562 = 0.012975971214473248 + 100.0 * 6.225124359130859
Epoch 2490, val loss: 1.4645178318023682
Epoch 2500, training loss: 622.2967529296875 = 0.012821757234632969 + 100.0 * 6.22283935546875
Epoch 2500, val loss: 1.467586636543274
Epoch 2510, training loss: 622.1737670898438 = 0.01267771515995264 + 100.0 * 6.2216105461120605
Epoch 2510, val loss: 1.4705020189285278
Epoch 2520, training loss: 622.1416625976562 = 0.012538949958980083 + 100.0 * 6.221291542053223
Epoch 2520, val loss: 1.4736565351486206
Epoch 2530, training loss: 622.825439453125 = 0.012404619716107845 + 100.0 * 6.228130340576172
Epoch 2530, val loss: 1.4767605066299438
Epoch 2540, training loss: 622.4464721679688 = 0.012260951101779938 + 100.0 * 6.224341869354248
Epoch 2540, val loss: 1.478679895401001
Epoch 2550, training loss: 622.2533569335938 = 0.012119376100599766 + 100.0 * 6.222412109375
Epoch 2550, val loss: 1.481858730316162
Epoch 2560, training loss: 622.36669921875 = 0.011988844722509384 + 100.0 * 6.223547458648682
Epoch 2560, val loss: 1.4845235347747803
Epoch 2570, training loss: 622.3279418945312 = 0.01185537688434124 + 100.0 * 6.223161220550537
Epoch 2570, val loss: 1.4874380826950073
Epoch 2580, training loss: 622.097412109375 = 0.011720046401023865 + 100.0 * 6.2208571434021
Epoch 2580, val loss: 1.4901220798492432
Epoch 2590, training loss: 622.05810546875 = 0.01159615907818079 + 100.0 * 6.220465183258057
Epoch 2590, val loss: 1.4927687644958496
Epoch 2600, training loss: 622.3135986328125 = 0.011476541869342327 + 100.0 * 6.223021030426025
Epoch 2600, val loss: 1.4957600831985474
Epoch 2610, training loss: 622.3281860351562 = 0.011351889930665493 + 100.0 * 6.22316837310791
Epoch 2610, val loss: 1.4980947971343994
Epoch 2620, training loss: 622.1361694335938 = 0.01122606173157692 + 100.0 * 6.221249103546143
Epoch 2620, val loss: 1.5007752180099487
Epoch 2630, training loss: 622.0166015625 = 0.011106334626674652 + 100.0 * 6.220055103302002
Epoch 2630, val loss: 1.5033477544784546
Epoch 2640, training loss: 622.157470703125 = 0.01099261362105608 + 100.0 * 6.22146463394165
Epoch 2640, val loss: 1.5062884092330933
Epoch 2650, training loss: 622.1303100585938 = 0.010877774097025394 + 100.0 * 6.221194267272949
Epoch 2650, val loss: 1.5088120698928833
Epoch 2660, training loss: 622.1161499023438 = 0.010766858235001564 + 100.0 * 6.2210540771484375
Epoch 2660, val loss: 1.5114949941635132
Epoch 2670, training loss: 622.2073974609375 = 0.010657002218067646 + 100.0 * 6.2219672203063965
Epoch 2670, val loss: 1.5140241384506226
Epoch 2680, training loss: 621.9058837890625 = 0.010546454228460789 + 100.0 * 6.2189531326293945
Epoch 2680, val loss: 1.5165305137634277
Epoch 2690, training loss: 621.9601440429688 = 0.010439574718475342 + 100.0 * 6.219497203826904
Epoch 2690, val loss: 1.5189794301986694
Epoch 2700, training loss: 622.242919921875 = 0.010339086875319481 + 100.0 * 6.222326278686523
Epoch 2700, val loss: 1.5218745470046997
Epoch 2710, training loss: 622.08154296875 = 0.010232600383460522 + 100.0 * 6.220713138580322
Epoch 2710, val loss: 1.524067997932434
Epoch 2720, training loss: 621.9710083007812 = 0.010126455686986446 + 100.0 * 6.219608783721924
Epoch 2720, val loss: 1.5266538858413696
Epoch 2730, training loss: 622.0031127929688 = 0.01002683024853468 + 100.0 * 6.219931125640869
Epoch 2730, val loss: 1.5289491415023804
Epoch 2740, training loss: 621.8840942382812 = 0.00992993637919426 + 100.0 * 6.218741416931152
Epoch 2740, val loss: 1.531675100326538
Epoch 2750, training loss: 621.8580932617188 = 0.00983575452119112 + 100.0 * 6.218482971191406
Epoch 2750, val loss: 1.5339796543121338
Epoch 2760, training loss: 621.9645385742188 = 0.009742197580635548 + 100.0 * 6.219547748565674
Epoch 2760, val loss: 1.5366290807724
Epoch 2770, training loss: 622.083984375 = 0.009648147970438004 + 100.0 * 6.220743179321289
Epoch 2770, val loss: 1.5388920307159424
Epoch 2780, training loss: 622.390380859375 = 0.009554716758430004 + 100.0 * 6.2238078117370605
Epoch 2780, val loss: 1.5415263175964355
Epoch 2790, training loss: 621.8640747070312 = 0.009457097388803959 + 100.0 * 6.218545913696289
Epoch 2790, val loss: 1.5433053970336914
Epoch 2800, training loss: 621.7684326171875 = 0.00936805922538042 + 100.0 * 6.217590808868408
Epoch 2800, val loss: 1.5457696914672852
Epoch 2810, training loss: 621.7042236328125 = 0.00928324181586504 + 100.0 * 6.216949462890625
Epoch 2810, val loss: 1.548359751701355
Epoch 2820, training loss: 621.8157348632812 = 0.009202767163515091 + 100.0 * 6.21806526184082
Epoch 2820, val loss: 1.550671100616455
Epoch 2830, training loss: 622.29638671875 = 0.009119571186602116 + 100.0 * 6.222872734069824
Epoch 2830, val loss: 1.5529675483703613
Epoch 2840, training loss: 621.8929443359375 = 0.009029582142829895 + 100.0 * 6.218839168548584
Epoch 2840, val loss: 1.5553313493728638
Epoch 2850, training loss: 621.7218627929688 = 0.008945696987211704 + 100.0 * 6.217128753662109
Epoch 2850, val loss: 1.5577384233474731
Epoch 2860, training loss: 621.7648315429688 = 0.008866412565112114 + 100.0 * 6.217559814453125
Epoch 2860, val loss: 1.560269832611084
Epoch 2870, training loss: 621.924072265625 = 0.00878886692225933 + 100.0 * 6.219152927398682
Epoch 2870, val loss: 1.5624419450759888
Epoch 2880, training loss: 621.6193237304688 = 0.008709206245839596 + 100.0 * 6.216105937957764
Epoch 2880, val loss: 1.5644265413284302
Epoch 2890, training loss: 622.072021484375 = 0.008635573089122772 + 100.0 * 6.2206339836120605
Epoch 2890, val loss: 1.566763162612915
Epoch 2900, training loss: 621.7669677734375 = 0.008556613698601723 + 100.0 * 6.217584133148193
Epoch 2900, val loss: 1.5691134929656982
Epoch 2910, training loss: 621.6712036132812 = 0.008481333032250404 + 100.0 * 6.21662712097168
Epoch 2910, val loss: 1.5713715553283691
Epoch 2920, training loss: 621.689697265625 = 0.008408090099692345 + 100.0 * 6.216812610626221
Epoch 2920, val loss: 1.5735644102096558
Epoch 2930, training loss: 621.734619140625 = 0.008336486294865608 + 100.0 * 6.2172627449035645
Epoch 2930, val loss: 1.5759994983673096
Epoch 2940, training loss: 622.1725463867188 = 0.008264796808362007 + 100.0 * 6.221642971038818
Epoch 2940, val loss: 1.5780290365219116
Epoch 2950, training loss: 621.7130737304688 = 0.008189177140593529 + 100.0 * 6.2170491218566895
Epoch 2950, val loss: 1.5800397396087646
Epoch 2960, training loss: 621.5545043945312 = 0.008121373131871223 + 100.0 * 6.215464115142822
Epoch 2960, val loss: 1.5822246074676514
Epoch 2970, training loss: 621.4769897460938 = 0.008055473677814007 + 100.0 * 6.214689254760742
Epoch 2970, val loss: 1.584633708000183
Epoch 2980, training loss: 622.233154296875 = 0.00799297634512186 + 100.0 * 6.2222514152526855
Epoch 2980, val loss: 1.5869675874710083
Epoch 2990, training loss: 621.8325805664062 = 0.007923167198896408 + 100.0 * 6.2182464599609375
Epoch 2990, val loss: 1.5887011289596558
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 861.6367797851562 = 1.9507871866226196 + 100.0 * 8.5968599319458
Epoch 0, val loss: 1.9518588781356812
Epoch 10, training loss: 861.5631713867188 = 1.94184148311615 + 100.0 * 8.596213340759277
Epoch 10, val loss: 1.9432564973831177
Epoch 20, training loss: 861.084716796875 = 1.930694580078125 + 100.0 * 8.591540336608887
Epoch 20, val loss: 1.9323251247406006
Epoch 30, training loss: 857.9338989257812 = 1.9163188934326172 + 100.0 * 8.560175895690918
Epoch 30, val loss: 1.9179480075836182
Epoch 40, training loss: 840.6581420898438 = 1.8986469507217407 + 100.0 * 8.387595176696777
Epoch 40, val loss: 1.9003961086273193
Epoch 50, training loss: 787.1239013671875 = 1.8784136772155762 + 100.0 * 7.852455139160156
Epoch 50, val loss: 1.8804694414138794
Epoch 60, training loss: 744.264404296875 = 1.8625133037567139 + 100.0 * 7.4240193367004395
Epoch 60, val loss: 1.8655320405960083
Epoch 70, training loss: 719.1355590820312 = 1.8524481058120728 + 100.0 * 7.172831058502197
Epoch 70, val loss: 1.8550324440002441
Epoch 80, training loss: 704.6520385742188 = 1.8410166501998901 + 100.0 * 7.028110504150391
Epoch 80, val loss: 1.8436731100082397
Epoch 90, training loss: 690.0720825195312 = 1.8300018310546875 + 100.0 * 6.882420539855957
Epoch 90, val loss: 1.8333969116210938
Epoch 100, training loss: 680.9935302734375 = 1.8201134204864502 + 100.0 * 6.791733741760254
Epoch 100, val loss: 1.8236093521118164
Epoch 110, training loss: 674.1265258789062 = 1.80916428565979 + 100.0 * 6.723174095153809
Epoch 110, val loss: 1.8122905492782593
Epoch 120, training loss: 669.4445190429688 = 1.7977052927017212 + 100.0 * 6.6764678955078125
Epoch 120, val loss: 1.8007153272628784
Epoch 130, training loss: 665.918701171875 = 1.7863490581512451 + 100.0 * 6.641323089599609
Epoch 130, val loss: 1.789145588874817
Epoch 140, training loss: 663.0565185546875 = 1.7748348712921143 + 100.0 * 6.61281681060791
Epoch 140, val loss: 1.7775629758834839
Epoch 150, training loss: 660.3800659179688 = 1.7629140615463257 + 100.0 * 6.5861711502075195
Epoch 150, val loss: 1.765523910522461
Epoch 160, training loss: 658.137939453125 = 1.75018310546875 + 100.0 * 6.563877582550049
Epoch 160, val loss: 1.7529630661010742
Epoch 170, training loss: 656.256103515625 = 1.7365611791610718 + 100.0 * 6.545195579528809
Epoch 170, val loss: 1.7397761344909668
Epoch 180, training loss: 654.3404541015625 = 1.7217400074005127 + 100.0 * 6.526186943054199
Epoch 180, val loss: 1.7256666421890259
Epoch 190, training loss: 652.5993041992188 = 1.705671787261963 + 100.0 * 6.508935928344727
Epoch 190, val loss: 1.7105649709701538
Epoch 200, training loss: 651.2364501953125 = 1.68813955783844 + 100.0 * 6.4954833984375
Epoch 200, val loss: 1.6942986249923706
Epoch 210, training loss: 650.0886840820312 = 1.669166922569275 + 100.0 * 6.484194755554199
Epoch 210, val loss: 1.676611304283142
Epoch 220, training loss: 648.7376708984375 = 1.648506999015808 + 100.0 * 6.47089147567749
Epoch 220, val loss: 1.6577726602554321
Epoch 230, training loss: 648.0403442382812 = 1.626255989074707 + 100.0 * 6.464140892028809
Epoch 230, val loss: 1.6375659704208374
Epoch 240, training loss: 646.7546997070312 = 1.6025056838989258 + 100.0 * 6.451522350311279
Epoch 240, val loss: 1.6161553859710693
Epoch 250, training loss: 645.9719848632812 = 1.5773569345474243 + 100.0 * 6.443946838378906
Epoch 250, val loss: 1.593720555305481
Epoch 260, training loss: 645.2247924804688 = 1.5508396625518799 + 100.0 * 6.436739444732666
Epoch 260, val loss: 1.5703999996185303
Epoch 270, training loss: 644.2852172851562 = 1.5232912302017212 + 100.0 * 6.427618980407715
Epoch 270, val loss: 1.5463937520980835
Epoch 280, training loss: 643.5769653320312 = 1.4949252605438232 + 100.0 * 6.420820236206055
Epoch 280, val loss: 1.5219290256500244
Epoch 290, training loss: 642.9924926757812 = 1.4658862352371216 + 100.0 * 6.415266036987305
Epoch 290, val loss: 1.4970974922180176
Epoch 300, training loss: 642.6827392578125 = 1.4359251260757446 + 100.0 * 6.412468433380127
Epoch 300, val loss: 1.4720128774642944
Epoch 310, training loss: 641.9647216796875 = 1.4061039686203003 + 100.0 * 6.405586242675781
Epoch 310, val loss: 1.4469504356384277
Epoch 320, training loss: 641.1971435546875 = 1.376020073890686 + 100.0 * 6.3982110023498535
Epoch 320, val loss: 1.4220927953720093
Epoch 330, training loss: 640.5751953125 = 1.346198320388794 + 100.0 * 6.392290115356445
Epoch 330, val loss: 1.3975111246109009
Epoch 340, training loss: 640.0185546875 = 1.3165197372436523 + 100.0 * 6.387020111083984
Epoch 340, val loss: 1.3733028173446655
Epoch 350, training loss: 640.1433715820312 = 1.2871304750442505 + 100.0 * 6.3885626792907715
Epoch 350, val loss: 1.3495469093322754
Epoch 360, training loss: 639.2566528320312 = 1.2577987909317017 + 100.0 * 6.379988193511963
Epoch 360, val loss: 1.3259080648422241
Epoch 370, training loss: 638.6786499023438 = 1.229021668434143 + 100.0 * 6.3744964599609375
Epoch 370, val loss: 1.302857756614685
Epoch 380, training loss: 638.202392578125 = 1.2008321285247803 + 100.0 * 6.370016098022461
Epoch 380, val loss: 1.2803984880447388
Epoch 390, training loss: 638.2620239257812 = 1.173044204711914 + 100.0 * 6.370890140533447
Epoch 390, val loss: 1.2582751512527466
Epoch 400, training loss: 637.4921875 = 1.1455509662628174 + 100.0 * 6.363466262817383
Epoch 400, val loss: 1.2367411851882935
Epoch 410, training loss: 636.9815673828125 = 1.118923306465149 + 100.0 * 6.358626842498779
Epoch 410, val loss: 1.215822458267212
Epoch 420, training loss: 636.5825805664062 = 1.0928354263305664 + 100.0 * 6.354897499084473
Epoch 420, val loss: 1.1955381631851196
Epoch 430, training loss: 636.7396850585938 = 1.0673290491104126 + 100.0 * 6.356723785400391
Epoch 430, val loss: 1.175879716873169
Epoch 440, training loss: 636.0827026367188 = 1.0427441596984863 + 100.0 * 6.350399494171143
Epoch 440, val loss: 1.1565654277801514
Epoch 450, training loss: 635.61474609375 = 1.0184659957885742 + 100.0 * 6.3459625244140625
Epoch 450, val loss: 1.138238549232483
Epoch 460, training loss: 635.59814453125 = 0.9951079487800598 + 100.0 * 6.346030235290527
Epoch 460, val loss: 1.1205388307571411
Epoch 470, training loss: 635.1307983398438 = 0.9722400307655334 + 100.0 * 6.341585636138916
Epoch 470, val loss: 1.1034586429595947
Epoch 480, training loss: 634.6742553710938 = 0.9501245617866516 + 100.0 * 6.337241172790527
Epoch 480, val loss: 1.0871769189834595
Epoch 490, training loss: 634.443603515625 = 0.9287473559379578 + 100.0 * 6.335148334503174
Epoch 490, val loss: 1.0716125965118408
Epoch 500, training loss: 634.2192993164062 = 0.9077438116073608 + 100.0 * 6.333116054534912
Epoch 500, val loss: 1.056855320930481
Epoch 510, training loss: 633.9422607421875 = 0.8875930905342102 + 100.0 * 6.330546855926514
Epoch 510, val loss: 1.0425410270690918
Epoch 520, training loss: 633.6056518554688 = 0.8680117130279541 + 100.0 * 6.327376365661621
Epoch 520, val loss: 1.0292176008224487
Epoch 530, training loss: 633.48779296875 = 0.848972499370575 + 100.0 * 6.326387882232666
Epoch 530, val loss: 1.0165977478027344
Epoch 540, training loss: 633.6317749023438 = 0.8304125070571899 + 100.0 * 6.328013896942139
Epoch 540, val loss: 1.0043909549713135
Epoch 550, training loss: 633.0431518554688 = 0.8122982382774353 + 100.0 * 6.32230806350708
Epoch 550, val loss: 0.9929885268211365
Epoch 560, training loss: 632.7025756835938 = 0.7947952747344971 + 100.0 * 6.319077491760254
Epoch 560, val loss: 0.9822680354118347
Epoch 570, training loss: 633.3256225585938 = 0.777626633644104 + 100.0 * 6.325479984283447
Epoch 570, val loss: 0.9721029996871948
Epoch 580, training loss: 632.2943725585938 = 0.7608100175857544 + 100.0 * 6.315335750579834
Epoch 580, val loss: 0.9622952938079834
Epoch 590, training loss: 632.1685791015625 = 0.7442886233329773 + 100.0 * 6.314243316650391
Epoch 590, val loss: 0.9532126188278198
Epoch 600, training loss: 631.968994140625 = 0.7281032204627991 + 100.0 * 6.312408924102783
Epoch 600, val loss: 0.9447713494300842
Epoch 610, training loss: 631.78857421875 = 0.7122585773468018 + 100.0 * 6.310762882232666
Epoch 610, val loss: 0.9366577863693237
Epoch 620, training loss: 632.0950317382812 = 0.6967290639877319 + 100.0 * 6.313982963562012
Epoch 620, val loss: 0.9290399551391602
Epoch 630, training loss: 631.6099243164062 = 0.681306779384613 + 100.0 * 6.309286117553711
Epoch 630, val loss: 0.9219588041305542
Epoch 640, training loss: 631.2345581054688 = 0.6663407683372498 + 100.0 * 6.30568265914917
Epoch 640, val loss: 0.9155125617980957
Epoch 650, training loss: 631.0505981445312 = 0.6517215371131897 + 100.0 * 6.303988456726074
Epoch 650, val loss: 0.9096260070800781
Epoch 660, training loss: 631.7561645507812 = 0.6372849941253662 + 100.0 * 6.3111891746521
Epoch 660, val loss: 0.90415358543396
Epoch 670, training loss: 630.9871215820312 = 0.6232987642288208 + 100.0 * 6.303638458251953
Epoch 670, val loss: 0.8987928628921509
Epoch 680, training loss: 630.546142578125 = 0.6093661189079285 + 100.0 * 6.299367427825928
Epoch 680, val loss: 0.8941784501075745
Epoch 690, training loss: 630.4166259765625 = 0.5958531498908997 + 100.0 * 6.298208236694336
Epoch 690, val loss: 0.8899551630020142
Epoch 700, training loss: 630.8794555664062 = 0.5825511813163757 + 100.0 * 6.302968502044678
Epoch 700, val loss: 0.8860282897949219
Epoch 710, training loss: 630.3121337890625 = 0.5694650411605835 + 100.0 * 6.297426700592041
Epoch 710, val loss: 0.8824177980422974
Epoch 720, training loss: 629.975341796875 = 0.5566748380661011 + 100.0 * 6.294186115264893
Epoch 720, val loss: 0.8792627453804016
Epoch 730, training loss: 629.9613037109375 = 0.5442656874656677 + 100.0 * 6.294170379638672
Epoch 730, val loss: 0.8764967322349548
Epoch 740, training loss: 629.7023315429688 = 0.5319341421127319 + 100.0 * 6.291704177856445
Epoch 740, val loss: 0.874035120010376
Epoch 750, training loss: 629.7286376953125 = 0.5199357867240906 + 100.0 * 6.292086601257324
Epoch 750, val loss: 0.8718082904815674
Epoch 760, training loss: 629.3923950195312 = 0.5081769824028015 + 100.0 * 6.28884220123291
Epoch 760, val loss: 0.870171308517456
Epoch 770, training loss: 629.4879760742188 = 0.4967673420906067 + 100.0 * 6.289912223815918
Epoch 770, val loss: 0.8688151836395264
Epoch 780, training loss: 629.293701171875 = 0.48548176884651184 + 100.0 * 6.288082122802734
Epoch 780, val loss: 0.8675829768180847
Epoch 790, training loss: 629.439697265625 = 0.47438544034957886 + 100.0 * 6.289653301239014
Epoch 790, val loss: 0.8666889667510986
Epoch 800, training loss: 628.91552734375 = 0.46367061138153076 + 100.0 * 6.284518718719482
Epoch 800, val loss: 0.8661196827888489
Epoch 810, training loss: 628.9157104492188 = 0.45323511958122253 + 100.0 * 6.2846245765686035
Epoch 810, val loss: 0.865838348865509
Epoch 820, training loss: 628.8024291992188 = 0.44298070669174194 + 100.0 * 6.283594131469727
Epoch 820, val loss: 0.8657976984977722
Epoch 830, training loss: 628.7801513671875 = 0.43293827772140503 + 100.0 * 6.283472537994385
Epoch 830, val loss: 0.8659805655479431
Epoch 840, training loss: 628.8385620117188 = 0.42306697368621826 + 100.0 * 6.284154891967773
Epoch 840, val loss: 0.8664151430130005
Epoch 850, training loss: 628.389892578125 = 0.4134775996208191 + 100.0 * 6.279764175415039
Epoch 850, val loss: 0.8669405579566956
Epoch 860, training loss: 628.4185180664062 = 0.404128760099411 + 100.0 * 6.280144214630127
Epoch 860, val loss: 0.8678415417671204
Epoch 870, training loss: 628.4402465820312 = 0.3950030505657196 + 100.0 * 6.280452251434326
Epoch 870, val loss: 0.8688675165176392
Epoch 880, training loss: 628.180908203125 = 0.38615795969963074 + 100.0 * 6.277947425842285
Epoch 880, val loss: 0.8700347542762756
Epoch 890, training loss: 627.9810791015625 = 0.37747254967689514 + 100.0 * 6.276035785675049
Epoch 890, val loss: 0.8715062737464905
Epoch 900, training loss: 628.2884521484375 = 0.3690483868122101 + 100.0 * 6.279194355010986
Epoch 900, val loss: 0.8731008172035217
Epoch 910, training loss: 627.9345092773438 = 0.36066287755966187 + 100.0 * 6.27573823928833
Epoch 910, val loss: 0.8746720552444458
Epoch 920, training loss: 627.9835205078125 = 0.3525450825691223 + 100.0 * 6.276309967041016
Epoch 920, val loss: 0.8765414357185364
Epoch 930, training loss: 627.5888671875 = 0.344520628452301 + 100.0 * 6.2724432945251465
Epoch 930, val loss: 0.8785389065742493
Epoch 940, training loss: 627.6376342773438 = 0.3367410898208618 + 100.0 * 6.273008823394775
Epoch 940, val loss: 0.8807888627052307
Epoch 950, training loss: 627.4618530273438 = 0.3291380703449249 + 100.0 * 6.271327018737793
Epoch 950, val loss: 0.8829997777938843
Epoch 960, training loss: 627.7015380859375 = 0.321713387966156 + 100.0 * 6.273797988891602
Epoch 960, val loss: 0.8853814005851746
Epoch 970, training loss: 627.4086303710938 = 0.3143404424190521 + 100.0 * 6.2709431648254395
Epoch 970, val loss: 0.8878201246261597
Epoch 980, training loss: 627.5145263671875 = 0.3071528971195221 + 100.0 * 6.272074222564697
Epoch 980, val loss: 0.8904236555099487
Epoch 990, training loss: 627.0591430664062 = 0.30008214712142944 + 100.0 * 6.267590045928955
Epoch 990, val loss: 0.8932750821113586
Epoch 1000, training loss: 627.179443359375 = 0.2931787371635437 + 100.0 * 6.268863201141357
Epoch 1000, val loss: 0.8962010741233826
Epoch 1010, training loss: 626.9991455078125 = 0.28637880086898804 + 100.0 * 6.267127990722656
Epoch 1010, val loss: 0.8990939855575562
Epoch 1020, training loss: 626.9935913085938 = 0.2797176241874695 + 100.0 * 6.267138957977295
Epoch 1020, val loss: 0.9021952748298645
Epoch 1030, training loss: 626.89697265625 = 0.27317291498184204 + 100.0 * 6.266237735748291
Epoch 1030, val loss: 0.9053399562835693
Epoch 1040, training loss: 626.7387084960938 = 0.2668059170246124 + 100.0 * 6.264719009399414
Epoch 1040, val loss: 0.9085701107978821
Epoch 1050, training loss: 626.7926635742188 = 0.26051798462867737 + 100.0 * 6.265321731567383
Epoch 1050, val loss: 0.9120888113975525
Epoch 1060, training loss: 627.0634765625 = 0.254341721534729 + 100.0 * 6.268091678619385
Epoch 1060, val loss: 0.9157667756080627
Epoch 1070, training loss: 626.6087646484375 = 0.24817802011966705 + 100.0 * 6.263606071472168
Epoch 1070, val loss: 0.9191655516624451
Epoch 1080, training loss: 626.4220581054688 = 0.24223150312900543 + 100.0 * 6.261797904968262
Epoch 1080, val loss: 0.9230520129203796
Epoch 1090, training loss: 626.6885375976562 = 0.23640817403793335 + 100.0 * 6.264521598815918
Epoch 1090, val loss: 0.9267319440841675
Epoch 1100, training loss: 626.41943359375 = 0.2306392788887024 + 100.0 * 6.261888027191162
Epoch 1100, val loss: 0.9309023022651672
Epoch 1110, training loss: 626.2620239257812 = 0.22496291995048523 + 100.0 * 6.26037073135376
Epoch 1110, val loss: 0.9348360896110535
Epoch 1120, training loss: 626.1630249023438 = 0.21945714950561523 + 100.0 * 6.259435176849365
Epoch 1120, val loss: 0.939091682434082
Epoch 1130, training loss: 626.3983154296875 = 0.2140459567308426 + 100.0 * 6.261842727661133
Epoch 1130, val loss: 0.9432896971702576
Epoch 1140, training loss: 626.5789184570312 = 0.20874127745628357 + 100.0 * 6.263701915740967
Epoch 1140, val loss: 0.9476746916770935
Epoch 1150, training loss: 626.3746337890625 = 0.20352640748023987 + 100.0 * 6.2617106437683105
Epoch 1150, val loss: 0.9518954753875732
Epoch 1160, training loss: 625.9834594726562 = 0.1983930766582489 + 100.0 * 6.2578511238098145
Epoch 1160, val loss: 0.9566006064414978
Epoch 1170, training loss: 625.7863159179688 = 0.19344386458396912 + 100.0 * 6.255928993225098
Epoch 1170, val loss: 0.9612413048744202
Epoch 1180, training loss: 625.7866821289062 = 0.1886010617017746 + 100.0 * 6.255980968475342
Epoch 1180, val loss: 0.9661309123039246
Epoch 1190, training loss: 626.09765625 = 0.18383784592151642 + 100.0 * 6.259138107299805
Epoch 1190, val loss: 0.9707726240158081
Epoch 1200, training loss: 625.7213745117188 = 0.17911513149738312 + 100.0 * 6.255422115325928
Epoch 1200, val loss: 0.9757742285728455
Epoch 1210, training loss: 625.5960083007812 = 0.17458027601242065 + 100.0 * 6.254214763641357
Epoch 1210, val loss: 0.9805837273597717
Epoch 1220, training loss: 625.50146484375 = 0.17016510665416718 + 100.0 * 6.253313064575195
Epoch 1220, val loss: 0.9857606291770935
Epoch 1230, training loss: 626.3448486328125 = 0.16587097942829132 + 100.0 * 6.261789321899414
Epoch 1230, val loss: 0.9909214973449707
Epoch 1240, training loss: 625.8263549804688 = 0.1615787297487259 + 100.0 * 6.256648063659668
Epoch 1240, val loss: 0.9959590435028076
Epoch 1250, training loss: 625.58154296875 = 0.15744897723197937 + 100.0 * 6.254240989685059
Epoch 1250, val loss: 1.0012942552566528
Epoch 1260, training loss: 625.5103759765625 = 0.15344613790512085 + 100.0 * 6.25356912612915
Epoch 1260, val loss: 1.0066299438476562
Epoch 1270, training loss: 625.3206787109375 = 0.14953967928886414 + 100.0 * 6.251711368560791
Epoch 1270, val loss: 1.0118407011032104
Epoch 1280, training loss: 625.4237060546875 = 0.1457502245903015 + 100.0 * 6.252779483795166
Epoch 1280, val loss: 1.0172332525253296
Epoch 1290, training loss: 625.443359375 = 0.1420331746339798 + 100.0 * 6.2530131340026855
Epoch 1290, val loss: 1.0228261947631836
Epoch 1300, training loss: 625.8138427734375 = 0.13839325308799744 + 100.0 * 6.2567548751831055
Epoch 1300, val loss: 1.0282074213027954
Epoch 1310, training loss: 625.17333984375 = 0.13489709794521332 + 100.0 * 6.250384330749512
Epoch 1310, val loss: 1.0336326360702515
Epoch 1320, training loss: 625.0397338867188 = 0.13146743178367615 + 100.0 * 6.249082565307617
Epoch 1320, val loss: 1.0393500328063965
Epoch 1330, training loss: 624.9716186523438 = 0.12815921008586884 + 100.0 * 6.248434543609619
Epoch 1330, val loss: 1.044966459274292
Epoch 1340, training loss: 625.3380126953125 = 0.12495801597833633 + 100.0 * 6.252130508422852
Epoch 1340, val loss: 1.0506854057312012
Epoch 1350, training loss: 625.3461303710938 = 0.12182430922985077 + 100.0 * 6.2522430419921875
Epoch 1350, val loss: 1.0560784339904785
Epoch 1360, training loss: 625.0964965820312 = 0.11870993673801422 + 100.0 * 6.249777793884277
Epoch 1360, val loss: 1.0617862939834595
Epoch 1370, training loss: 624.9896240234375 = 0.11576447635889053 + 100.0 * 6.2487382888793945
Epoch 1370, val loss: 1.0675963163375854
Epoch 1380, training loss: 624.7678833007812 = 0.11288542300462723 + 100.0 * 6.246549606323242
Epoch 1380, val loss: 1.073331594467163
Epoch 1390, training loss: 624.9253540039062 = 0.11010138690471649 + 100.0 * 6.248152732849121
Epoch 1390, val loss: 1.0789214372634888
Epoch 1400, training loss: 624.7780151367188 = 0.10738871991634369 + 100.0 * 6.246706485748291
Epoch 1400, val loss: 1.0845427513122559
Epoch 1410, training loss: 624.9967041015625 = 0.10473982244729996 + 100.0 * 6.248919486999512
Epoch 1410, val loss: 1.090192437171936
Epoch 1420, training loss: 624.7652587890625 = 0.1021517962217331 + 100.0 * 6.246631145477295
Epoch 1420, val loss: 1.0958360433578491
Epoch 1430, training loss: 624.6429443359375 = 0.09966467320919037 + 100.0 * 6.2454328536987305
Epoch 1430, val loss: 1.1017948389053345
Epoch 1440, training loss: 624.9384155273438 = 0.0972522720694542 + 100.0 * 6.248411655426025
Epoch 1440, val loss: 1.107427954673767
Epoch 1450, training loss: 624.4916381835938 = 0.09488523751497269 + 100.0 * 6.243967533111572
Epoch 1450, val loss: 1.1130131483078003
Epoch 1460, training loss: 624.41259765625 = 0.09260154515504837 + 100.0 * 6.243199825286865
Epoch 1460, val loss: 1.1184492111206055
Epoch 1470, training loss: 624.5531616210938 = 0.09040601551532745 + 100.0 * 6.244627952575684
Epoch 1470, val loss: 1.1241685152053833
Epoch 1480, training loss: 625.2734985351562 = 0.08826743811368942 + 100.0 * 6.251852512359619
Epoch 1480, val loss: 1.129903793334961
Epoch 1490, training loss: 624.44677734375 = 0.08609237521886826 + 100.0 * 6.2436065673828125
Epoch 1490, val loss: 1.1353552341461182
Epoch 1500, training loss: 624.2420654296875 = 0.08407212048768997 + 100.0 * 6.241580009460449
Epoch 1500, val loss: 1.14105224609375
Epoch 1510, training loss: 624.1492309570312 = 0.08211291581392288 + 100.0 * 6.240671157836914
Epoch 1510, val loss: 1.146745204925537
Epoch 1520, training loss: 624.260009765625 = 0.08021152764558792 + 100.0 * 6.241797924041748
Epoch 1520, val loss: 1.152165174484253
Epoch 1530, training loss: 624.40869140625 = 0.07833613455295563 + 100.0 * 6.2433037757873535
Epoch 1530, val loss: 1.1575747728347778
Epoch 1540, training loss: 624.0545043945312 = 0.07649867981672287 + 100.0 * 6.239780426025391
Epoch 1540, val loss: 1.163145899772644
Epoch 1550, training loss: 624.061767578125 = 0.07474373281002045 + 100.0 * 6.239870071411133
Epoch 1550, val loss: 1.1688133478164673
Epoch 1560, training loss: 624.5079956054688 = 0.07305067777633667 + 100.0 * 6.244349479675293
Epoch 1560, val loss: 1.1742122173309326
Epoch 1570, training loss: 624.0392456054688 = 0.07137884944677353 + 100.0 * 6.239678859710693
Epoch 1570, val loss: 1.179247498512268
Epoch 1580, training loss: 624.055908203125 = 0.06975097209215164 + 100.0 * 6.239861488342285
Epoch 1580, val loss: 1.1850275993347168
Epoch 1590, training loss: 623.9158935546875 = 0.06818972527980804 + 100.0 * 6.2384772300720215
Epoch 1590, val loss: 1.190320372581482
Epoch 1600, training loss: 624.162841796875 = 0.066689632833004 + 100.0 * 6.240961074829102
Epoch 1600, val loss: 1.1957849264144897
Epoch 1610, training loss: 623.9380493164062 = 0.06519618630409241 + 100.0 * 6.2387285232543945
Epoch 1610, val loss: 1.2009010314941406
Epoch 1620, training loss: 623.8820190429688 = 0.06374318897724152 + 100.0 * 6.23818302154541
Epoch 1620, val loss: 1.2059509754180908
Epoch 1630, training loss: 624.1760864257812 = 0.06235494464635849 + 100.0 * 6.2411370277404785
Epoch 1630, val loss: 1.2112772464752197
Epoch 1640, training loss: 623.7264404296875 = 0.060974713414907455 + 100.0 * 6.236654758453369
Epoch 1640, val loss: 1.2165296077728271
Epoch 1650, training loss: 623.6405029296875 = 0.059657055884599686 + 100.0 * 6.235808849334717
Epoch 1650, val loss: 1.2216402292251587
Epoch 1660, training loss: 623.9697265625 = 0.058395154774188995 + 100.0 * 6.2391133308410645
Epoch 1660, val loss: 1.2264678478240967
Epoch 1670, training loss: 623.7628784179688 = 0.057137586176395416 + 100.0 * 6.237057685852051
Epoch 1670, val loss: 1.2317930459976196
Epoch 1680, training loss: 623.5369262695312 = 0.055912964046001434 + 100.0 * 6.2348103523254395
Epoch 1680, val loss: 1.2368837594985962
Epoch 1690, training loss: 623.54345703125 = 0.05473433807492256 + 100.0 * 6.23488712310791
Epoch 1690, val loss: 1.241953730583191
Epoch 1700, training loss: 623.8540649414062 = 0.05360662564635277 + 100.0 * 6.238004684448242
Epoch 1700, val loss: 1.2468897104263306
Epoch 1710, training loss: 623.5335083007812 = 0.05247139185667038 + 100.0 * 6.2348103523254395
Epoch 1710, val loss: 1.2518479824066162
Epoch 1720, training loss: 623.575439453125 = 0.05134263634681702 + 100.0 * 6.235240936279297
Epoch 1720, val loss: 1.2570990324020386
Epoch 1730, training loss: 623.9534912109375 = 0.050271738320589066 + 100.0 * 6.239031791687012
Epoch 1730, val loss: 1.2625257968902588
Epoch 1740, training loss: 623.48583984375 = 0.049198005348443985 + 100.0 * 6.234366416931152
Epoch 1740, val loss: 1.267607569694519
Epoch 1750, training loss: 623.3445434570312 = 0.048162639141082764 + 100.0 * 6.232964038848877
Epoch 1750, val loss: 1.2729164361953735
Epoch 1760, training loss: 623.3904418945312 = 0.04716959223151207 + 100.0 * 6.233432769775391
Epoch 1760, val loss: 1.2775886058807373
Epoch 1770, training loss: 623.4901123046875 = 0.04620670899748802 + 100.0 * 6.234438896179199
Epoch 1770, val loss: 1.2825915813446045
Epoch 1780, training loss: 623.7186889648438 = 0.04527168720960617 + 100.0 * 6.236733913421631
Epoch 1780, val loss: 1.287575602531433
Epoch 1790, training loss: 623.5674438476562 = 0.0443461574614048 + 100.0 * 6.235230445861816
Epoch 1790, val loss: 1.291772723197937
Epoch 1800, training loss: 623.2040405273438 = 0.04343605414032936 + 100.0 * 6.2316060066223145
Epoch 1800, val loss: 1.2967866659164429
Epoch 1810, training loss: 623.1172485351562 = 0.04257877543568611 + 100.0 * 6.230746269226074
Epoch 1810, val loss: 1.301292061805725
Epoch 1820, training loss: 623.1405639648438 = 0.04174765944480896 + 100.0 * 6.230988025665283
Epoch 1820, val loss: 1.306022047996521
Epoch 1830, training loss: 623.8550415039062 = 0.04093621298670769 + 100.0 * 6.238141059875488
Epoch 1830, val loss: 1.3105336427688599
Epoch 1840, training loss: 623.466552734375 = 0.0401451513171196 + 100.0 * 6.234263896942139
Epoch 1840, val loss: 1.3149207830429077
Epoch 1850, training loss: 623.2841186523438 = 0.03935966640710831 + 100.0 * 6.232447624206543
Epoch 1850, val loss: 1.3193563222885132
Epoch 1860, training loss: 623.1408081054688 = 0.03860186040401459 + 100.0 * 6.231022357940674
Epoch 1860, val loss: 1.3238791227340698
Epoch 1870, training loss: 623.3171997070312 = 0.03786841779947281 + 100.0 * 6.232793807983398
Epoch 1870, val loss: 1.3283214569091797
Epoch 1880, training loss: 623.03076171875 = 0.03714519366621971 + 100.0 * 6.229936122894287
Epoch 1880, val loss: 1.3324593305587769
Epoch 1890, training loss: 622.9863891601562 = 0.03645014390349388 + 100.0 * 6.229499340057373
Epoch 1890, val loss: 1.3366080522537231
Epoch 1900, training loss: 623.2283325195312 = 0.0357678197324276 + 100.0 * 6.2319254875183105
Epoch 1900, val loss: 1.341036081314087
Epoch 1910, training loss: 622.8778076171875 = 0.03510851785540581 + 100.0 * 6.228426933288574
Epoch 1910, val loss: 1.345143437385559
Epoch 1920, training loss: 622.9541625976562 = 0.03446551784873009 + 100.0 * 6.229197025299072
Epoch 1920, val loss: 1.3494385480880737
Epoch 1930, training loss: 623.2291259765625 = 0.033837780356407166 + 100.0 * 6.231953144073486
Epoch 1930, val loss: 1.3535592555999756
Epoch 1940, training loss: 623.0455932617188 = 0.03321940079331398 + 100.0 * 6.230123996734619
Epoch 1940, val loss: 1.3572864532470703
Epoch 1950, training loss: 622.7403564453125 = 0.032611750066280365 + 100.0 * 6.227077484130859
Epoch 1950, val loss: 1.3614623546600342
Epoch 1960, training loss: 622.7769775390625 = 0.032035280019044876 + 100.0 * 6.227449417114258
Epoch 1960, val loss: 1.365444302558899
Epoch 1970, training loss: 623.2706298828125 = 0.031467992812395096 + 100.0 * 6.232391357421875
Epoch 1970, val loss: 1.369218349456787
Epoch 1980, training loss: 622.817626953125 = 0.030909109860658646 + 100.0 * 6.227867603302002
Epoch 1980, val loss: 1.3732823133468628
Epoch 1990, training loss: 622.5611572265625 = 0.03036816418170929 + 100.0 * 6.225307464599609
Epoch 1990, val loss: 1.3772722482681274
Epoch 2000, training loss: 622.7252197265625 = 0.029850337654352188 + 100.0 * 6.226953983306885
Epoch 2000, val loss: 1.380964756011963
Epoch 2010, training loss: 623.0057983398438 = 0.02933766320347786 + 100.0 * 6.229764461517334
Epoch 2010, val loss: 1.384514570236206
Epoch 2020, training loss: 622.6593627929688 = 0.028816841542720795 + 100.0 * 6.2263054847717285
Epoch 2020, val loss: 1.3885313272476196
Epoch 2030, training loss: 622.5746459960938 = 0.028325101360678673 + 100.0 * 6.225463390350342
Epoch 2030, val loss: 1.392503261566162
Epoch 2040, training loss: 622.687744140625 = 0.02784857712686062 + 100.0 * 6.226599216461182
Epoch 2040, val loss: 1.3962137699127197
Epoch 2050, training loss: 622.4109497070312 = 0.02738090232014656 + 100.0 * 6.223835468292236
Epoch 2050, val loss: 1.3996400833129883
Epoch 2060, training loss: 623.06640625 = 0.026931673288345337 + 100.0 * 6.2303948402404785
Epoch 2060, val loss: 1.403111457824707
Epoch 2070, training loss: 622.7537841796875 = 0.026474812999367714 + 100.0 * 6.227272987365723
Epoch 2070, val loss: 1.40744948387146
Epoch 2080, training loss: 622.45703125 = 0.026036640629172325 + 100.0 * 6.22430944442749
Epoch 2080, val loss: 1.4104427099227905
Epoch 2090, training loss: 622.3951416015625 = 0.025615356862545013 + 100.0 * 6.223694801330566
Epoch 2090, val loss: 1.4145431518554688
Epoch 2100, training loss: 622.8470458984375 = 0.0252083670347929 + 100.0 * 6.2282185554504395
Epoch 2100, val loss: 1.4179279804229736
Epoch 2110, training loss: 622.321533203125 = 0.024792324751615524 + 100.0 * 6.222967624664307
Epoch 2110, val loss: 1.421269416809082
Epoch 2120, training loss: 622.3853759765625 = 0.024389786645770073 + 100.0 * 6.223609447479248
Epoch 2120, val loss: 1.4246972799301147
Epoch 2130, training loss: 622.709228515625 = 0.024004274979233742 + 100.0 * 6.2268524169921875
Epoch 2130, val loss: 1.4284390211105347
Epoch 2140, training loss: 622.442138671875 = 0.023625265806913376 + 100.0 * 6.224184989929199
Epoch 2140, val loss: 1.431298851966858
Epoch 2150, training loss: 622.4808349609375 = 0.023253750056028366 + 100.0 * 6.224575996398926
Epoch 2150, val loss: 1.4349350929260254
Epoch 2160, training loss: 622.3602905273438 = 0.022885752841830254 + 100.0 * 6.223373889923096
Epoch 2160, val loss: 1.4381088018417358
Epoch 2170, training loss: 622.2164306640625 = 0.022532902657985687 + 100.0 * 6.2219390869140625
Epoch 2170, val loss: 1.4416005611419678
Epoch 2180, training loss: 622.3544311523438 = 0.02218988537788391 + 100.0 * 6.22332239151001
Epoch 2180, val loss: 1.4446598291397095
Epoch 2190, training loss: 622.1696166992188 = 0.021847933530807495 + 100.0 * 6.221477508544922
Epoch 2190, val loss: 1.447827696800232
Epoch 2200, training loss: 622.3901977539062 = 0.02151547372341156 + 100.0 * 6.223686218261719
Epoch 2200, val loss: 1.4510654211044312
Epoch 2210, training loss: 622.3161010742188 = 0.02118692733347416 + 100.0 * 6.222949504852295
Epoch 2210, val loss: 1.4538990259170532
Epoch 2220, training loss: 622.0625 = 0.020868448540568352 + 100.0 * 6.22041654586792
Epoch 2220, val loss: 1.4572423696517944
Epoch 2230, training loss: 622.1074829101562 = 0.020561538636684418 + 100.0 * 6.220869064331055
Epoch 2230, val loss: 1.4604688882827759
Epoch 2240, training loss: 622.5866088867188 = 0.020267240703105927 + 100.0 * 6.225663661956787
Epoch 2240, val loss: 1.4632863998413086
Epoch 2250, training loss: 622.0634155273438 = 0.019955523312091827 + 100.0 * 6.220434665679932
Epoch 2250, val loss: 1.466246247291565
Epoch 2260, training loss: 622.1513671875 = 0.019666602835059166 + 100.0 * 6.221317291259766
Epoch 2260, val loss: 1.469330906867981
Epoch 2270, training loss: 622.1064453125 = 0.019380314275622368 + 100.0 * 6.220870494842529
Epoch 2270, val loss: 1.472264051437378
Epoch 2280, training loss: 621.9938354492188 = 0.019099673256278038 + 100.0 * 6.219747543334961
Epoch 2280, val loss: 1.4753649234771729
Epoch 2290, training loss: 623.180419921875 = 0.018835896626114845 + 100.0 * 6.2316155433654785
Epoch 2290, val loss: 1.478472113609314
Epoch 2300, training loss: 622.205810546875 = 0.0185482669621706 + 100.0 * 6.221872329711914
Epoch 2300, val loss: 1.4809402227401733
Epoch 2310, training loss: 621.903564453125 = 0.018285755068063736 + 100.0 * 6.218852519989014
Epoch 2310, val loss: 1.4840763807296753
Epoch 2320, training loss: 621.8074340820312 = 0.01803353987634182 + 100.0 * 6.217894554138184
Epoch 2320, val loss: 1.4868305921554565
Epoch 2330, training loss: 621.7792358398438 = 0.017789462581276894 + 100.0 * 6.217614650726318
Epoch 2330, val loss: 1.4896727800369263
Epoch 2340, training loss: 622.6210327148438 = 0.017556207254529 + 100.0 * 6.226034641265869
Epoch 2340, val loss: 1.4923704862594604
Epoch 2350, training loss: 621.941162109375 = 0.017300009727478027 + 100.0 * 6.219238758087158
Epoch 2350, val loss: 1.4951746463775635
Epoch 2360, training loss: 621.7107543945312 = 0.017062390223145485 + 100.0 * 6.2169365882873535
Epoch 2360, val loss: 1.4978973865509033
Epoch 2370, training loss: 621.6419067382812 = 0.01683240942656994 + 100.0 * 6.216250896453857
Epoch 2370, val loss: 1.500701665878296
Epoch 2380, training loss: 622.1077270507812 = 0.016614018008112907 + 100.0 * 6.220911502838135
Epoch 2380, val loss: 1.5035804510116577
Epoch 2390, training loss: 621.9517211914062 = 0.016389882192015648 + 100.0 * 6.219352722167969
Epoch 2390, val loss: 1.5059131383895874
Epoch 2400, training loss: 621.8123779296875 = 0.016162171959877014 + 100.0 * 6.21796178817749
Epoch 2400, val loss: 1.5087040662765503
Epoch 2410, training loss: 621.7012939453125 = 0.015948085114359856 + 100.0 * 6.216853618621826
Epoch 2410, val loss: 1.5110456943511963
Epoch 2420, training loss: 621.6063842773438 = 0.015739211812615395 + 100.0 * 6.215906143188477
Epoch 2420, val loss: 1.5139107704162598
Epoch 2430, training loss: 621.8407592773438 = 0.015541165135800838 + 100.0 * 6.218252182006836
Epoch 2430, val loss: 1.5163809061050415
Epoch 2440, training loss: 621.9129638671875 = 0.015342424623668194 + 100.0 * 6.2189764976501465
Epoch 2440, val loss: 1.51894211769104
Epoch 2450, training loss: 621.7097778320312 = 0.015140723437070847 + 100.0 * 6.216946601867676
Epoch 2450, val loss: 1.5213567018508911
Epoch 2460, training loss: 621.5376586914062 = 0.014943096786737442 + 100.0 * 6.215227127075195
Epoch 2460, val loss: 1.5240026712417603
Epoch 2470, training loss: 621.61962890625 = 0.014755218289792538 + 100.0 * 6.216048717498779
Epoch 2470, val loss: 1.5263477563858032
Epoch 2480, training loss: 621.9749755859375 = 0.014571466483175755 + 100.0 * 6.219604015350342
Epoch 2480, val loss: 1.5286390781402588
Epoch 2490, training loss: 621.599853515625 = 0.01438971422612667 + 100.0 * 6.215854644775391
Epoch 2490, val loss: 1.5310558080673218
Epoch 2500, training loss: 621.4572143554688 = 0.01420848723500967 + 100.0 * 6.21442985534668
Epoch 2500, val loss: 1.533545970916748
Epoch 2510, training loss: 621.4556884765625 = 0.014039847068488598 + 100.0 * 6.21441650390625
Epoch 2510, val loss: 1.5360149145126343
Epoch 2520, training loss: 622.83935546875 = 0.013878216035664082 + 100.0 * 6.228255271911621
Epoch 2520, val loss: 1.538483738899231
Epoch 2530, training loss: 621.8038330078125 = 0.013693299144506454 + 100.0 * 6.217901706695557
Epoch 2530, val loss: 1.5403681993484497
Epoch 2540, training loss: 621.4122924804688 = 0.013521554879844189 + 100.0 * 6.213987350463867
Epoch 2540, val loss: 1.542515754699707
Epoch 2550, training loss: 621.3172607421875 = 0.013364512473344803 + 100.0 * 6.213038921356201
Epoch 2550, val loss: 1.5450550317764282
Epoch 2560, training loss: 621.309326171875 = 0.013208511285483837 + 100.0 * 6.212961196899414
Epoch 2560, val loss: 1.5471769571304321
Epoch 2570, training loss: 621.75341796875 = 0.01306152530014515 + 100.0 * 6.217403411865234
Epoch 2570, val loss: 1.5490130186080933
Epoch 2580, training loss: 621.9512329101562 = 0.012903223745524883 + 100.0 * 6.219383239746094
Epoch 2580, val loss: 1.551524043083191
Epoch 2590, training loss: 621.4769287109375 = 0.012746165506541729 + 100.0 * 6.214641571044922
Epoch 2590, val loss: 1.553467869758606
Epoch 2600, training loss: 621.3461303710938 = 0.012597129680216312 + 100.0 * 6.2133355140686035
Epoch 2600, val loss: 1.5559149980545044
Epoch 2610, training loss: 621.4049682617188 = 0.012456773780286312 + 100.0 * 6.213924884796143
Epoch 2610, val loss: 1.558017373085022
Epoch 2620, training loss: 621.8070068359375 = 0.012316679581999779 + 100.0 * 6.217946529388428
Epoch 2620, val loss: 1.5602833032608032
Epoch 2630, training loss: 621.2818603515625 = 0.012169800698757172 + 100.0 * 6.2126970291137695
Epoch 2630, val loss: 1.5619052648544312
Epoch 2640, training loss: 621.2127685546875 = 0.012032304890453815 + 100.0 * 6.212007522583008
Epoch 2640, val loss: 1.5640842914581299
Epoch 2650, training loss: 621.9368896484375 = 0.01190338097512722 + 100.0 * 6.219249725341797
Epoch 2650, val loss: 1.566406011581421
Epoch 2660, training loss: 621.3383178710938 = 0.01176727470010519 + 100.0 * 6.213265419006348
Epoch 2660, val loss: 1.5675616264343262
Epoch 2670, training loss: 621.2257690429688 = 0.011634591966867447 + 100.0 * 6.212141036987305
Epoch 2670, val loss: 1.57011878490448
Epoch 2680, training loss: 621.1287841796875 = 0.01150790136307478 + 100.0 * 6.211172580718994
Epoch 2680, val loss: 1.5718028545379639
Epoch 2690, training loss: 621.8922119140625 = 0.011389877647161484 + 100.0 * 6.218807697296143
Epoch 2690, val loss: 1.5739353895187378
Epoch 2700, training loss: 621.2801513671875 = 0.01126254815608263 + 100.0 * 6.212688446044922
Epoch 2700, val loss: 1.5752508640289307
Epoch 2710, training loss: 621.132080078125 = 0.011134404689073563 + 100.0 * 6.211209774017334
Epoch 2710, val loss: 1.5775022506713867
Epoch 2720, training loss: 621.1780395507812 = 0.011018462479114532 + 100.0 * 6.211669921875
Epoch 2720, val loss: 1.5793192386627197
Epoch 2730, training loss: 621.9481811523438 = 0.010905247181653976 + 100.0 * 6.219372272491455
Epoch 2730, val loss: 1.581536889076233
Epoch 2740, training loss: 621.3484497070312 = 0.010785945691168308 + 100.0 * 6.213376522064209
Epoch 2740, val loss: 1.5826740264892578
Epoch 2750, training loss: 621.153564453125 = 0.010666551068425179 + 100.0 * 6.211429119110107
Epoch 2750, val loss: 1.5848220586776733
Epoch 2760, training loss: 621.1963500976562 = 0.01055857539176941 + 100.0 * 6.211857795715332
Epoch 2760, val loss: 1.5864613056182861
Epoch 2770, training loss: 621.7442016601562 = 0.010450189933180809 + 100.0 * 6.217337608337402
Epoch 2770, val loss: 1.5883852243423462
Epoch 2780, training loss: 621.2391357421875 = 0.010343583300709724 + 100.0 * 6.2122883796691895
Epoch 2780, val loss: 1.589725136756897
Epoch 2790, training loss: 621.00341796875 = 0.010234396904706955 + 100.0 * 6.20993185043335
Epoch 2790, val loss: 1.5917646884918213
Epoch 2800, training loss: 621.145263671875 = 0.010135282762348652 + 100.0 * 6.21135139465332
Epoch 2800, val loss: 1.5933268070220947
Epoch 2810, training loss: 621.5032958984375 = 0.01003450620919466 + 100.0 * 6.214932918548584
Epoch 2810, val loss: 1.5951311588287354
Epoch 2820, training loss: 621.2166137695312 = 0.009928278625011444 + 100.0 * 6.212066650390625
Epoch 2820, val loss: 1.5968196392059326
Epoch 2830, training loss: 621.271484375 = 0.009830629453063011 + 100.0 * 6.212616443634033
Epoch 2830, val loss: 1.598586082458496
Epoch 2840, training loss: 620.91943359375 = 0.009729797020554543 + 100.0 * 6.209097385406494
Epoch 2840, val loss: 1.5999423265457153
Epoch 2850, training loss: 621.0744018554688 = 0.009637037292122841 + 100.0 * 6.2106475830078125
Epoch 2850, val loss: 1.6016583442687988
Epoch 2860, training loss: 621.4739379882812 = 0.009544383734464645 + 100.0 * 6.214644432067871
Epoch 2860, val loss: 1.60336172580719
Epoch 2870, training loss: 621.1074829101562 = 0.009447766467928886 + 100.0 * 6.210980415344238
Epoch 2870, val loss: 1.6045706272125244
Epoch 2880, training loss: 620.9539184570312 = 0.009357623755931854 + 100.0 * 6.209445953369141
Epoch 2880, val loss: 1.6062780618667603
Epoch 2890, training loss: 621.21826171875 = 0.009271928109228611 + 100.0 * 6.212090015411377
Epoch 2890, val loss: 1.6076756715774536
Epoch 2900, training loss: 621.1856689453125 = 0.009180998429656029 + 100.0 * 6.211765289306641
Epoch 2900, val loss: 1.60947585105896
Epoch 2910, training loss: 620.9679565429688 = 0.009090546518564224 + 100.0 * 6.209588527679443
Epoch 2910, val loss: 1.610565423965454
Epoch 2920, training loss: 620.8907470703125 = 0.00900593213737011 + 100.0 * 6.208817005157471
Epoch 2920, val loss: 1.6121842861175537
Epoch 2930, training loss: 621.01953125 = 0.008922877721488476 + 100.0 * 6.210106372833252
Epoch 2930, val loss: 1.6138414144515991
Epoch 2940, training loss: 621.1611328125 = 0.00883899349719286 + 100.0 * 6.211523056030273
Epoch 2940, val loss: 1.6152266263961792
Epoch 2950, training loss: 621.0476684570312 = 0.008755406364798546 + 100.0 * 6.210389137268066
Epoch 2950, val loss: 1.6165465116500854
Epoch 2960, training loss: 620.8533935546875 = 0.00867684930562973 + 100.0 * 6.208447456359863
Epoch 2960, val loss: 1.6177946329116821
Epoch 2970, training loss: 620.8405151367188 = 0.008600018918514252 + 100.0 * 6.208319187164307
Epoch 2970, val loss: 1.6194307804107666
Epoch 2980, training loss: 621.2455444335938 = 0.008524230681359768 + 100.0 * 6.212369918823242
Epoch 2980, val loss: 1.6206209659576416
Epoch 2990, training loss: 620.8099365234375 = 0.008443612605333328 + 100.0 * 6.208014965057373
Epoch 2990, val loss: 1.622162938117981
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8418555614127571
The final CL Acc:0.76667, 0.01512, The final GNN Acc:0.84045, 0.00131
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10528])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.63232421875 = 1.9465117454528809 + 100.0 * 8.596858024597168
Epoch 0, val loss: 1.9414808750152588
Epoch 10, training loss: 861.5498046875 = 1.9371390342712402 + 100.0 * 8.596126556396484
Epoch 10, val loss: 1.9316569566726685
Epoch 20, training loss: 860.989990234375 = 1.9254155158996582 + 100.0 * 8.590645790100098
Epoch 20, val loss: 1.9194411039352417
Epoch 30, training loss: 856.84326171875 = 1.910394310951233 + 100.0 * 8.549328804016113
Epoch 30, val loss: 1.90397047996521
Epoch 40, training loss: 827.0748291015625 = 1.891845941543579 + 100.0 * 8.251830101013184
Epoch 40, val loss: 1.885373830795288
Epoch 50, training loss: 751.8093872070312 = 1.8740235567092896 + 100.0 * 7.499353408813477
Epoch 50, val loss: 1.868334174156189
Epoch 60, training loss: 724.544921875 = 1.8617396354675293 + 100.0 * 7.226831912994385
Epoch 60, val loss: 1.857279896736145
Epoch 70, training loss: 707.17041015625 = 1.8515070676803589 + 100.0 * 7.053188800811768
Epoch 70, val loss: 1.8479301929473877
Epoch 80, training loss: 692.3519287109375 = 1.8419324159622192 + 100.0 * 6.905100345611572
Epoch 80, val loss: 1.8394384384155273
Epoch 90, training loss: 683.3453979492188 = 1.8339048624038696 + 100.0 * 6.815114974975586
Epoch 90, val loss: 1.8322067260742188
Epoch 100, training loss: 677.6029663085938 = 1.8259079456329346 + 100.0 * 6.757770538330078
Epoch 100, val loss: 1.8250893354415894
Epoch 110, training loss: 672.236083984375 = 1.818586826324463 + 100.0 * 6.704174995422363
Epoch 110, val loss: 1.818740963935852
Epoch 120, training loss: 667.9158935546875 = 1.8123116493225098 + 100.0 * 6.661036014556885
Epoch 120, val loss: 1.8132615089416504
Epoch 130, training loss: 664.196533203125 = 1.8059804439544678 + 100.0 * 6.623905658721924
Epoch 130, val loss: 1.8076523542404175
Epoch 140, training loss: 661.3637084960938 = 1.7992165088653564 + 100.0 * 6.595645427703857
Epoch 140, val loss: 1.8017070293426514
Epoch 150, training loss: 658.4764404296875 = 1.7920877933502197 + 100.0 * 6.566843032836914
Epoch 150, val loss: 1.7953650951385498
Epoch 160, training loss: 655.8944702148438 = 1.7847365140914917 + 100.0 * 6.541097640991211
Epoch 160, val loss: 1.7888946533203125
Epoch 170, training loss: 653.9025268554688 = 1.7770110368728638 + 100.0 * 6.521255016326904
Epoch 170, val loss: 1.7821227312088013
Epoch 180, training loss: 652.0485229492188 = 1.7685760259628296 + 100.0 * 6.5027995109558105
Epoch 180, val loss: 1.7748467922210693
Epoch 190, training loss: 650.3284301757812 = 1.7593735456466675 + 100.0 * 6.485690593719482
Epoch 190, val loss: 1.7669384479522705
Epoch 200, training loss: 649.4205322265625 = 1.7493321895599365 + 100.0 * 6.476712226867676
Epoch 200, val loss: 1.758471131324768
Epoch 210, training loss: 648.0645141601562 = 1.7379992008209229 + 100.0 * 6.463265419006348
Epoch 210, val loss: 1.748820424079895
Epoch 220, training loss: 646.6129760742188 = 1.725679636001587 + 100.0 * 6.4488725662231445
Epoch 220, val loss: 1.7382882833480835
Epoch 230, training loss: 645.57421875 = 1.7123291492462158 + 100.0 * 6.4386186599731445
Epoch 230, val loss: 1.726928472518921
Epoch 240, training loss: 644.6674194335938 = 1.6978181600570679 + 100.0 * 6.429696083068848
Epoch 240, val loss: 1.7145990133285522
Epoch 250, training loss: 643.8897705078125 = 1.681915283203125 + 100.0 * 6.422079086303711
Epoch 250, val loss: 1.701149582862854
Epoch 260, training loss: 643.1356811523438 = 1.6647980213165283 + 100.0 * 6.414708614349365
Epoch 260, val loss: 1.6866532564163208
Epoch 270, training loss: 642.16259765625 = 1.646583914756775 + 100.0 * 6.405159950256348
Epoch 270, val loss: 1.6713756322860718
Epoch 280, training loss: 641.4390869140625 = 1.6272283792495728 + 100.0 * 6.398118495941162
Epoch 280, val loss: 1.6550003290176392
Epoch 290, training loss: 640.9046630859375 = 1.60686457157135 + 100.0 * 6.392977714538574
Epoch 290, val loss: 1.6379450559616089
Epoch 300, training loss: 640.3534545898438 = 1.585559606552124 + 100.0 * 6.387679100036621
Epoch 300, val loss: 1.6200428009033203
Epoch 310, training loss: 639.5280151367188 = 1.563536524772644 + 100.0 * 6.379644870758057
Epoch 310, val loss: 1.6016227006912231
Epoch 320, training loss: 639.24609375 = 1.5410155057907104 + 100.0 * 6.377050876617432
Epoch 320, val loss: 1.5828890800476074
Epoch 330, training loss: 638.5877685546875 = 1.5182139873504639 + 100.0 * 6.370696067810059
Epoch 330, val loss: 1.5642210245132446
Epoch 340, training loss: 638.216796875 = 1.495052456855774 + 100.0 * 6.367217540740967
Epoch 340, val loss: 1.5450252294540405
Epoch 350, training loss: 637.443115234375 = 1.4720076322555542 + 100.0 * 6.359711170196533
Epoch 350, val loss: 1.5262582302093506
Epoch 360, training loss: 637.076171875 = 1.4490630626678467 + 100.0 * 6.356271266937256
Epoch 360, val loss: 1.5077358484268188
Epoch 370, training loss: 636.6083984375 = 1.4261126518249512 + 100.0 * 6.351823329925537
Epoch 370, val loss: 1.4891008138656616
Epoch 380, training loss: 636.3013916015625 = 1.4035228490829468 + 100.0 * 6.3489789962768555
Epoch 380, val loss: 1.4710239171981812
Epoch 390, training loss: 635.7086181640625 = 1.3811185359954834 + 100.0 * 6.34327507019043
Epoch 390, val loss: 1.4533098936080933
Epoch 400, training loss: 635.32275390625 = 1.3592034578323364 + 100.0 * 6.339635848999023
Epoch 400, val loss: 1.4360448122024536
Epoch 410, training loss: 635.02978515625 = 1.337496280670166 + 100.0 * 6.336922645568848
Epoch 410, val loss: 1.4191417694091797
Epoch 420, training loss: 634.7097778320312 = 1.316121220588684 + 100.0 * 6.33393669128418
Epoch 420, val loss: 1.4023535251617432
Epoch 430, training loss: 634.3678588867188 = 1.2949692010879517 + 100.0 * 6.330728530883789
Epoch 430, val loss: 1.386396050453186
Epoch 440, training loss: 633.9393310546875 = 1.2743123769760132 + 100.0 * 6.326650142669678
Epoch 440, val loss: 1.3705661296844482
Epoch 450, training loss: 633.6126098632812 = 1.2540174722671509 + 100.0 * 6.3235859870910645
Epoch 450, val loss: 1.3554282188415527
Epoch 460, training loss: 634.0216064453125 = 1.2340689897537231 + 100.0 * 6.327875137329102
Epoch 460, val loss: 1.3406163454055786
Epoch 470, training loss: 633.0231323242188 = 1.2141671180725098 + 100.0 * 6.318089485168457
Epoch 470, val loss: 1.325892448425293
Epoch 480, training loss: 632.6690063476562 = 1.194800615310669 + 100.0 * 6.314741611480713
Epoch 480, val loss: 1.3118400573730469
Epoch 490, training loss: 632.5844116210938 = 1.1757967472076416 + 100.0 * 6.314086437225342
Epoch 490, val loss: 1.2982159852981567
Epoch 500, training loss: 632.8545532226562 = 1.1569136381149292 + 100.0 * 6.316976070404053
Epoch 500, val loss: 1.2847521305084229
Epoch 510, training loss: 632.2059936523438 = 1.1383893489837646 + 100.0 * 6.310676574707031
Epoch 510, val loss: 1.2722338438034058
Epoch 520, training loss: 631.7423706054688 = 1.120063304901123 + 100.0 * 6.306222915649414
Epoch 520, val loss: 1.2595422267913818
Epoch 530, training loss: 631.4436645507812 = 1.1021298170089722 + 100.0 * 6.303415298461914
Epoch 530, val loss: 1.2472807168960571
Epoch 540, training loss: 631.756591796875 = 1.0844614505767822 + 100.0 * 6.306721210479736
Epoch 540, val loss: 1.2353399991989136
Epoch 550, training loss: 631.3839721679688 = 1.0670472383499146 + 100.0 * 6.303169250488281
Epoch 550, val loss: 1.2244772911071777
Epoch 560, training loss: 630.804443359375 = 1.0497586727142334 + 100.0 * 6.297546863555908
Epoch 560, val loss: 1.2131799459457397
Epoch 570, training loss: 630.5931396484375 = 1.0328474044799805 + 100.0 * 6.295602798461914
Epoch 570, val loss: 1.2025325298309326
Epoch 580, training loss: 631.1273803710938 = 1.016241431236267 + 100.0 * 6.301111221313477
Epoch 580, val loss: 1.1927478313446045
Epoch 590, training loss: 630.6756591796875 = 0.9994122982025146 + 100.0 * 6.296762943267822
Epoch 590, val loss: 1.182020902633667
Epoch 600, training loss: 630.0521240234375 = 0.9830392003059387 + 100.0 * 6.290690898895264
Epoch 600, val loss: 1.172511339187622
Epoch 610, training loss: 630.4901123046875 = 0.9668433666229248 + 100.0 * 6.295232772827148
Epoch 610, val loss: 1.1634061336517334
Epoch 620, training loss: 629.932373046875 = 0.9506920576095581 + 100.0 * 6.289816856384277
Epoch 620, val loss: 1.154163122177124
Epoch 630, training loss: 629.6114501953125 = 0.9347227811813354 + 100.0 * 6.286767482757568
Epoch 630, val loss: 1.145333170890808
Epoch 640, training loss: 629.6422729492188 = 0.9189889430999756 + 100.0 * 6.287232398986816
Epoch 640, val loss: 1.137081503868103
Epoch 650, training loss: 629.3208618164062 = 0.9033347368240356 + 100.0 * 6.284175395965576
Epoch 650, val loss: 1.1286088228225708
Epoch 660, training loss: 629.745849609375 = 0.8878650069236755 + 100.0 * 6.288579940795898
Epoch 660, val loss: 1.1212003231048584
Epoch 670, training loss: 629.1730346679688 = 0.8722570538520813 + 100.0 * 6.283008098602295
Epoch 670, val loss: 1.1125530004501343
Epoch 680, training loss: 629.0756225585938 = 0.8569734692573547 + 100.0 * 6.282186031341553
Epoch 680, val loss: 1.105509877204895
Epoch 690, training loss: 628.6665649414062 = 0.8418157696723938 + 100.0 * 6.278247833251953
Epoch 690, val loss: 1.0979875326156616
Epoch 700, training loss: 628.6148681640625 = 0.8268489837646484 + 100.0 * 6.2778801918029785
Epoch 700, val loss: 1.0911884307861328
Epoch 710, training loss: 629.2388305664062 = 0.8120627403259277 + 100.0 * 6.284267425537109
Epoch 710, val loss: 1.085078477859497
Epoch 720, training loss: 628.6569213867188 = 0.7969736456871033 + 100.0 * 6.278599262237549
Epoch 720, val loss: 1.0774266719818115
Epoch 730, training loss: 628.25244140625 = 0.7823681235313416 + 100.0 * 6.274701118469238
Epoch 730, val loss: 1.0717426538467407
Epoch 740, training loss: 628.0537109375 = 0.7678260803222656 + 100.0 * 6.2728590965271
Epoch 740, val loss: 1.0657061338424683
Epoch 750, training loss: 628.1854248046875 = 0.7535026669502258 + 100.0 * 6.274319648742676
Epoch 750, val loss: 1.0605751276016235
Epoch 760, training loss: 628.1453247070312 = 0.739139199256897 + 100.0 * 6.274062156677246
Epoch 760, val loss: 1.0539952516555786
Epoch 770, training loss: 628.2315673828125 = 0.7249742150306702 + 100.0 * 6.275065898895264
Epoch 770, val loss: 1.0496655702590942
Epoch 780, training loss: 627.7157592773438 = 0.7108461260795593 + 100.0 * 6.270049095153809
Epoch 780, val loss: 1.0443801879882812
Epoch 790, training loss: 627.508544921875 = 0.697075366973877 + 100.0 * 6.2681145668029785
Epoch 790, val loss: 1.0398242473602295
Epoch 800, training loss: 627.4522705078125 = 0.6834493279457092 + 100.0 * 6.267688274383545
Epoch 800, val loss: 1.035334825515747
Epoch 810, training loss: 627.8434448242188 = 0.6699028015136719 + 100.0 * 6.271735191345215
Epoch 810, val loss: 1.0310310125350952
Epoch 820, training loss: 627.5609130859375 = 0.6564903259277344 + 100.0 * 6.269044399261475
Epoch 820, val loss: 1.0276762247085571
Epoch 830, training loss: 627.2233276367188 = 0.643255352973938 + 100.0 * 6.265800952911377
Epoch 830, val loss: 1.0239412784576416
Epoch 840, training loss: 627.0697631835938 = 0.6301754117012024 + 100.0 * 6.264395713806152
Epoch 840, val loss: 1.0206141471862793
Epoch 850, training loss: 627.293701171875 = 0.6173276901245117 + 100.0 * 6.266763687133789
Epoch 850, val loss: 1.0179364681243896
Epoch 860, training loss: 626.891357421875 = 0.6043930053710938 + 100.0 * 6.262869834899902
Epoch 860, val loss: 1.0142626762390137
Epoch 870, training loss: 626.8753662109375 = 0.5917364954948425 + 100.0 * 6.262836456298828
Epoch 870, val loss: 1.011539340019226
Epoch 880, training loss: 626.7984008789062 = 0.5792357325553894 + 100.0 * 6.2621917724609375
Epoch 880, val loss: 1.0091075897216797
Epoch 890, training loss: 626.8128051757812 = 0.5667739510536194 + 100.0 * 6.262460231781006
Epoch 890, val loss: 1.0063145160675049
Epoch 900, training loss: 626.444580078125 = 0.5545715689659119 + 100.0 * 6.258900165557861
Epoch 900, val loss: 1.0043320655822754
Epoch 910, training loss: 626.4035034179688 = 0.5425913333892822 + 100.0 * 6.258609294891357
Epoch 910, val loss: 1.0027543306350708
Epoch 920, training loss: 626.51806640625 = 0.5308191180229187 + 100.0 * 6.2598724365234375
Epoch 920, val loss: 1.0017122030258179
Epoch 930, training loss: 626.3043823242188 = 0.5190073251724243 + 100.0 * 6.257853984832764
Epoch 930, val loss: 0.9997686147689819
Epoch 940, training loss: 626.0902099609375 = 0.5073843598365784 + 100.0 * 6.255828380584717
Epoch 940, val loss: 0.998537540435791
Epoch 950, training loss: 625.9983520507812 = 0.4960491359233856 + 100.0 * 6.25502347946167
Epoch 950, val loss: 0.9973422288894653
Epoch 960, training loss: 626.0637817382812 = 0.4849444031715393 + 100.0 * 6.255788803100586
Epoch 960, val loss: 0.9964925050735474
Epoch 970, training loss: 625.9334106445312 = 0.47395631670951843 + 100.0 * 6.254594802856445
Epoch 970, val loss: 0.996376633644104
Epoch 980, training loss: 626.0745849609375 = 0.46314993500709534 + 100.0 * 6.256114482879639
Epoch 980, val loss: 0.9959811568260193
Epoch 990, training loss: 625.84765625 = 0.4525173008441925 + 100.0 * 6.253951549530029
Epoch 990, val loss: 0.99624103307724
Epoch 1000, training loss: 625.907470703125 = 0.44213926792144775 + 100.0 * 6.254653453826904
Epoch 1000, val loss: 0.9966506361961365
Epoch 1010, training loss: 625.6368408203125 = 0.43190914392471313 + 100.0 * 6.252049446105957
Epoch 1010, val loss: 0.9966585636138916
Epoch 1020, training loss: 625.5601196289062 = 0.4218863844871521 + 100.0 * 6.251382350921631
Epoch 1020, val loss: 0.996912956237793
Epoch 1030, training loss: 625.730712890625 = 0.4120960831642151 + 100.0 * 6.253186225891113
Epoch 1030, val loss: 0.9975810647010803
Epoch 1040, training loss: 625.6117553710938 = 0.40243107080459595 + 100.0 * 6.25209379196167
Epoch 1040, val loss: 0.9981496334075928
Epoch 1050, training loss: 625.6716918945312 = 0.39290088415145874 + 100.0 * 6.252788066864014
Epoch 1050, val loss: 0.99958735704422
Epoch 1060, training loss: 625.2766723632812 = 0.38373512029647827 + 100.0 * 6.248929500579834
Epoch 1060, val loss: 1.0015649795532227
Epoch 1070, training loss: 625.1329956054688 = 0.3747294843196869 + 100.0 * 6.24758243560791
Epoch 1070, val loss: 1.0027036666870117
Epoch 1080, training loss: 625.2233276367188 = 0.3660181164741516 + 100.0 * 6.248573303222656
Epoch 1080, val loss: 1.0046329498291016
Epoch 1090, training loss: 625.1154174804688 = 0.35741838812828064 + 100.0 * 6.247580051422119
Epoch 1090, val loss: 1.0065476894378662
Epoch 1100, training loss: 624.9478759765625 = 0.34896886348724365 + 100.0 * 6.245988845825195
Epoch 1100, val loss: 1.0084121227264404
Epoch 1110, training loss: 625.1753540039062 = 0.3408697843551636 + 100.0 * 6.248344421386719
Epoch 1110, val loss: 1.011414647102356
Epoch 1120, training loss: 624.8951416015625 = 0.33278989791870117 + 100.0 * 6.24562406539917
Epoch 1120, val loss: 1.0124616622924805
Epoch 1130, training loss: 624.7939453125 = 0.3249609172344208 + 100.0 * 6.24468994140625
Epoch 1130, val loss: 1.0144450664520264
Epoch 1140, training loss: 624.88134765625 = 0.31742653250694275 + 100.0 * 6.245639324188232
Epoch 1140, val loss: 1.016885757446289
Epoch 1150, training loss: 624.8804321289062 = 0.3100139796733856 + 100.0 * 6.245704174041748
Epoch 1150, val loss: 1.0192421674728394
Epoch 1160, training loss: 624.7163696289062 = 0.3028823137283325 + 100.0 * 6.244134902954102
Epoch 1160, val loss: 1.0228266716003418
Epoch 1170, training loss: 624.6490478515625 = 0.2958926856517792 + 100.0 * 6.243531703948975
Epoch 1170, val loss: 1.0256420373916626
Epoch 1180, training loss: 624.7203369140625 = 0.28909534215927124 + 100.0 * 6.244312286376953
Epoch 1180, val loss: 1.0279121398925781
Epoch 1190, training loss: 624.4725952148438 = 0.28242892026901245 + 100.0 * 6.241901874542236
Epoch 1190, val loss: 1.0308151245117188
Epoch 1200, training loss: 624.4937744140625 = 0.27599525451660156 + 100.0 * 6.242177963256836
Epoch 1200, val loss: 1.0342278480529785
Epoch 1210, training loss: 624.6716918945312 = 0.2697371244430542 + 100.0 * 6.244019985198975
Epoch 1210, val loss: 1.0381134748458862
Epoch 1220, training loss: 624.496826171875 = 0.2635568082332611 + 100.0 * 6.242332458496094
Epoch 1220, val loss: 1.0405066013336182
Epoch 1230, training loss: 624.357666015625 = 0.25760993361473083 + 100.0 * 6.241000175476074
Epoch 1230, val loss: 1.043994665145874
Epoch 1240, training loss: 624.245849609375 = 0.2518492341041565 + 100.0 * 6.239940166473389
Epoch 1240, val loss: 1.047532558441162
Epoch 1250, training loss: 624.4632568359375 = 0.24624675512313843 + 100.0 * 6.2421698570251465
Epoch 1250, val loss: 1.051625370979309
Epoch 1260, training loss: 624.1666259765625 = 0.24068796634674072 + 100.0 * 6.239259719848633
Epoch 1260, val loss: 1.0535916090011597
Epoch 1270, training loss: 623.9921264648438 = 0.235313281416893 + 100.0 * 6.237568378448486
Epoch 1270, val loss: 1.0570894479751587
Epoch 1280, training loss: 624.1825561523438 = 0.23013252019882202 + 100.0 * 6.2395243644714355
Epoch 1280, val loss: 1.060537576675415
Epoch 1290, training loss: 624.3112182617188 = 0.2250538319349289 + 100.0 * 6.240861892700195
Epoch 1290, val loss: 1.0641788244247437
Epoch 1300, training loss: 623.9716186523438 = 0.22009189426898956 + 100.0 * 6.237514972686768
Epoch 1300, val loss: 1.0687216520309448
Epoch 1310, training loss: 623.8422241210938 = 0.2152266502380371 + 100.0 * 6.236270427703857
Epoch 1310, val loss: 1.0715184211730957
Epoch 1320, training loss: 623.83154296875 = 0.21058088541030884 + 100.0 * 6.236209869384766
Epoch 1320, val loss: 1.0757168531417847
Epoch 1330, training loss: 624.7250366210938 = 0.20598290860652924 + 100.0 * 6.245190143585205
Epoch 1330, val loss: 1.0784542560577393
Epoch 1340, training loss: 623.8485717773438 = 0.2015414983034134 + 100.0 * 6.2364702224731445
Epoch 1340, val loss: 1.0839622020721436
Epoch 1350, training loss: 623.6278076171875 = 0.19716869294643402 + 100.0 * 6.234306335449219
Epoch 1350, val loss: 1.08766770362854
Epoch 1360, training loss: 623.5711669921875 = 0.19299893081188202 + 100.0 * 6.233781814575195
Epoch 1360, val loss: 1.0917613506317139
Epoch 1370, training loss: 624.1597900390625 = 0.18894685804843903 + 100.0 * 6.239707946777344
Epoch 1370, val loss: 1.0966533422470093
Epoch 1380, training loss: 623.9622192382812 = 0.1848950982093811 + 100.0 * 6.237773418426514
Epoch 1380, val loss: 1.1006691455841064
Epoch 1390, training loss: 623.5513305664062 = 0.18089471757411957 + 100.0 * 6.233704090118408
Epoch 1390, val loss: 1.104402780532837
Epoch 1400, training loss: 623.4325561523438 = 0.1770937144756317 + 100.0 * 6.2325544357299805
Epoch 1400, val loss: 1.1086374521255493
Epoch 1410, training loss: 623.4815673828125 = 0.17339186370372772 + 100.0 * 6.233081817626953
Epoch 1410, val loss: 1.1124407052993774
Epoch 1420, training loss: 623.589599609375 = 0.16976971924304962 + 100.0 * 6.234198093414307
Epoch 1420, val loss: 1.1168098449707031
Epoch 1430, training loss: 623.5888061523438 = 0.16620752215385437 + 100.0 * 6.234226226806641
Epoch 1430, val loss: 1.1202446222305298
Epoch 1440, training loss: 623.58642578125 = 0.16268017888069153 + 100.0 * 6.2342376708984375
Epoch 1440, val loss: 1.1253280639648438
Epoch 1450, training loss: 623.412353515625 = 0.15928015112876892 + 100.0 * 6.23253059387207
Epoch 1450, val loss: 1.1303012371063232
Epoch 1460, training loss: 623.19287109375 = 0.15596480667591095 + 100.0 * 6.2303690910339355
Epoch 1460, val loss: 1.1342682838439941
Epoch 1470, training loss: 623.0973510742188 = 0.15279825031757355 + 100.0 * 6.229445934295654
Epoch 1470, val loss: 1.1392711400985718
Epoch 1480, training loss: 623.5118408203125 = 0.14971284568309784 + 100.0 * 6.233621120452881
Epoch 1480, val loss: 1.1435247659683228
Epoch 1490, training loss: 623.2142333984375 = 0.14656579494476318 + 100.0 * 6.230677127838135
Epoch 1490, val loss: 1.146981954574585
Epoch 1500, training loss: 623.5868530273438 = 0.1435401737689972 + 100.0 * 6.234432697296143
Epoch 1500, val loss: 1.1520905494689941
Epoch 1510, training loss: 623.07568359375 = 0.1405968964099884 + 100.0 * 6.229350566864014
Epoch 1510, val loss: 1.1567403078079224
Epoch 1520, training loss: 622.9265747070312 = 0.13773363828659058 + 100.0 * 6.227888107299805
Epoch 1520, val loss: 1.1611655950546265
Epoch 1530, training loss: 623.3814086914062 = 0.13497410714626312 + 100.0 * 6.232464790344238
Epoch 1530, val loss: 1.1647645235061646
Epoch 1540, training loss: 622.9149780273438 = 0.13220562040805817 + 100.0 * 6.227827548980713
Epoch 1540, val loss: 1.170037031173706
Epoch 1550, training loss: 622.928466796875 = 0.1295211762189865 + 100.0 * 6.227989196777344
Epoch 1550, val loss: 1.1736352443695068
Epoch 1560, training loss: 623.1297607421875 = 0.12691427767276764 + 100.0 * 6.2300286293029785
Epoch 1560, val loss: 1.1782084703445435
Epoch 1570, training loss: 622.8555908203125 = 0.12435480207204819 + 100.0 * 6.2273125648498535
Epoch 1570, val loss: 1.183485984802246
Epoch 1580, training loss: 622.907958984375 = 0.12188319116830826 + 100.0 * 6.227860450744629
Epoch 1580, val loss: 1.1889489889144897
Epoch 1590, training loss: 622.9092407226562 = 0.11946167796850204 + 100.0 * 6.227898120880127
Epoch 1590, val loss: 1.193463921546936
Epoch 1600, training loss: 622.8113403320312 = 0.11709143966436386 + 100.0 * 6.226942539215088
Epoch 1600, val loss: 1.198093295097351
Epoch 1610, training loss: 622.8944091796875 = 0.11474978923797607 + 100.0 * 6.22779655456543
Epoch 1610, val loss: 1.2010225057601929
Epoch 1620, training loss: 622.8756713867188 = 0.11244270205497742 + 100.0 * 6.227632522583008
Epoch 1620, val loss: 1.206587791442871
Epoch 1630, training loss: 622.6888427734375 = 0.11022496968507767 + 100.0 * 6.225786209106445
Epoch 1630, val loss: 1.2124570608139038
Epoch 1640, training loss: 622.530029296875 = 0.10805277526378632 + 100.0 * 6.224220275878906
Epoch 1640, val loss: 1.2162061929702759
Epoch 1650, training loss: 622.5352783203125 = 0.10597299039363861 + 100.0 * 6.224293231964111
Epoch 1650, val loss: 1.2213841676712036
Epoch 1660, training loss: 623.3208618164062 = 0.1039617508649826 + 100.0 * 6.232169151306152
Epoch 1660, val loss: 1.2273155450820923
Epoch 1670, training loss: 622.9054565429688 = 0.10185790061950684 + 100.0 * 6.228035926818848
Epoch 1670, val loss: 1.2308731079101562
Epoch 1680, training loss: 622.5384521484375 = 0.09985259920358658 + 100.0 * 6.224385738372803
Epoch 1680, val loss: 1.23568594455719
Epoch 1690, training loss: 622.6256103515625 = 0.09791973978281021 + 100.0 * 6.225276947021484
Epoch 1690, val loss: 1.2401680946350098
Epoch 1700, training loss: 622.5574340820312 = 0.09604685753583908 + 100.0 * 6.224613666534424
Epoch 1700, val loss: 1.246039628982544
Epoch 1710, training loss: 622.5447387695312 = 0.09418480843305588 + 100.0 * 6.224505424499512
Epoch 1710, val loss: 1.2505360841751099
Epoch 1720, training loss: 622.3538818359375 = 0.09235522150993347 + 100.0 * 6.2226152420043945
Epoch 1720, val loss: 1.2544082403182983
Epoch 1730, training loss: 622.5542602539062 = 0.09060013294219971 + 100.0 * 6.224636554718018
Epoch 1730, val loss: 1.2588785886764526
Epoch 1740, training loss: 622.2846069335938 = 0.08885892480611801 + 100.0 * 6.221957206726074
Epoch 1740, val loss: 1.2643858194351196
Epoch 1750, training loss: 622.4302368164062 = 0.08718216419219971 + 100.0 * 6.223430633544922
Epoch 1750, val loss: 1.2698585987091064
Epoch 1760, training loss: 622.6013793945312 = 0.08553952723741531 + 100.0 * 6.22515869140625
Epoch 1760, val loss: 1.2743895053863525
Epoch 1770, training loss: 622.4954223632812 = 0.08388174325227737 + 100.0 * 6.224115371704102
Epoch 1770, val loss: 1.2791231870651245
Epoch 1780, training loss: 622.2184448242188 = 0.08223354071378708 + 100.0 * 6.221362113952637
Epoch 1780, val loss: 1.281850814819336
Epoch 1790, training loss: 622.1058349609375 = 0.08071387559175491 + 100.0 * 6.220251560211182
Epoch 1790, val loss: 1.2880817651748657
Epoch 1800, training loss: 622.0468139648438 = 0.07921537011861801 + 100.0 * 6.2196760177612305
Epoch 1800, val loss: 1.292027473449707
Epoch 1810, training loss: 622.7665405273438 = 0.07776479423046112 + 100.0 * 6.2268877029418945
Epoch 1810, val loss: 1.2956373691558838
Epoch 1820, training loss: 622.1211547851562 = 0.07629894465208054 + 100.0 * 6.2204484939575195
Epoch 1820, val loss: 1.302219033241272
Epoch 1830, training loss: 622.0103149414062 = 0.07486894726753235 + 100.0 * 6.219354152679443
Epoch 1830, val loss: 1.3058958053588867
Epoch 1840, training loss: 622.2296142578125 = 0.07351914048194885 + 100.0 * 6.221560478210449
Epoch 1840, val loss: 1.3111486434936523
Epoch 1850, training loss: 622.039794921875 = 0.07215631008148193 + 100.0 * 6.219676494598389
Epoch 1850, val loss: 1.3154181241989136
Epoch 1860, training loss: 622.0538940429688 = 0.07083507627248764 + 100.0 * 6.219830513000488
Epoch 1860, val loss: 1.3215994834899902
Epoch 1870, training loss: 622.6488037109375 = 0.0695633664727211 + 100.0 * 6.225791931152344
Epoch 1870, val loss: 1.3261700868606567
Epoch 1880, training loss: 622.1365356445312 = 0.06828415393829346 + 100.0 * 6.220682621002197
Epoch 1880, val loss: 1.3304238319396973
Epoch 1890, training loss: 622.3760375976562 = 0.0670427605509758 + 100.0 * 6.223090171813965
Epoch 1890, val loss: 1.3360955715179443
Epoch 1900, training loss: 621.8038940429688 = 0.06580127775669098 + 100.0 * 6.217381000518799
Epoch 1900, val loss: 1.3390228748321533
Epoch 1910, training loss: 621.8546752929688 = 0.06462185829877853 + 100.0 * 6.217900276184082
Epoch 1910, val loss: 1.3432649374008179
Epoch 1920, training loss: 621.842041015625 = 0.06348583847284317 + 100.0 * 6.217785358428955
Epoch 1920, val loss: 1.3483175039291382
Epoch 1930, training loss: 622.2742919921875 = 0.062379613518714905 + 100.0 * 6.222119331359863
Epoch 1930, val loss: 1.352637529373169
Epoch 1940, training loss: 621.9620361328125 = 0.06127170845866203 + 100.0 * 6.21900749206543
Epoch 1940, val loss: 1.3579535484313965
Epoch 1950, training loss: 621.7825317382812 = 0.060184407979249954 + 100.0 * 6.217223644256592
Epoch 1950, val loss: 1.3623961210250854
Epoch 1960, training loss: 621.7308959960938 = 0.059136588126420975 + 100.0 * 6.21671724319458
Epoch 1960, val loss: 1.3677185773849487
Epoch 1970, training loss: 622.147216796875 = 0.05814581364393234 + 100.0 * 6.220890522003174
Epoch 1970, val loss: 1.3732372522354126
Epoch 1980, training loss: 621.9976196289062 = 0.05708642676472664 + 100.0 * 6.219405651092529
Epoch 1980, val loss: 1.3755457401275635
Epoch 1990, training loss: 621.6893920898438 = 0.056052930653095245 + 100.0 * 6.216333866119385
Epoch 1990, val loss: 1.380374550819397
Epoch 2000, training loss: 621.5955200195312 = 0.05509598180651665 + 100.0 * 6.215404033660889
Epoch 2000, val loss: 1.38398015499115
Epoch 2010, training loss: 621.5454711914062 = 0.05417492613196373 + 100.0 * 6.2149128913879395
Epoch 2010, val loss: 1.389363408088684
Epoch 2020, training loss: 621.5960693359375 = 0.05327353626489639 + 100.0 * 6.215427875518799
Epoch 2020, val loss: 1.3934348821640015
Epoch 2030, training loss: 622.2891235351562 = 0.05238471180200577 + 100.0 * 6.222367763519287
Epoch 2030, val loss: 1.3973010778427124
Epoch 2040, training loss: 621.7696533203125 = 0.05149221420288086 + 100.0 * 6.21718168258667
Epoch 2040, val loss: 1.4033725261688232
Epoch 2050, training loss: 621.76025390625 = 0.050620704889297485 + 100.0 * 6.21709680557251
Epoch 2050, val loss: 1.40730881690979
Epoch 2060, training loss: 621.8822021484375 = 0.04979761689901352 + 100.0 * 6.218323707580566
Epoch 2060, val loss: 1.4127095937728882
Epoch 2070, training loss: 621.796875 = 0.04895143210887909 + 100.0 * 6.217479228973389
Epoch 2070, val loss: 1.4164875745773315
Epoch 2080, training loss: 621.539794921875 = 0.04811417683959007 + 100.0 * 6.214917182922363
Epoch 2080, val loss: 1.4192732572555542
Epoch 2090, training loss: 621.4317016601562 = 0.04733068868517876 + 100.0 * 6.213843822479248
Epoch 2090, val loss: 1.424457311630249
Epoch 2100, training loss: 621.4261474609375 = 0.046568211168050766 + 100.0 * 6.2137956619262695
Epoch 2100, val loss: 1.4287749528884888
Epoch 2110, training loss: 622.1316528320312 = 0.04583519697189331 + 100.0 * 6.220858097076416
Epoch 2110, val loss: 1.433113932609558
Epoch 2120, training loss: 621.6976318359375 = 0.045078344643116 + 100.0 * 6.216525554656982
Epoch 2120, val loss: 1.4389666318893433
Epoch 2130, training loss: 621.76611328125 = 0.04434811696410179 + 100.0 * 6.217217445373535
Epoch 2130, val loss: 1.4433116912841797
Epoch 2140, training loss: 621.6715698242188 = 0.04361999034881592 + 100.0 * 6.216279029846191
Epoch 2140, val loss: 1.446255087852478
Epoch 2150, training loss: 621.4228515625 = 0.04289475083351135 + 100.0 * 6.213799476623535
Epoch 2150, val loss: 1.4498515129089355
Epoch 2160, training loss: 621.298828125 = 0.04223405197262764 + 100.0 * 6.212565898895264
Epoch 2160, val loss: 1.4556561708450317
Epoch 2170, training loss: 621.2615356445312 = 0.04157555475831032 + 100.0 * 6.212199687957764
Epoch 2170, val loss: 1.4592560529708862
Epoch 2180, training loss: 621.759765625 = 0.04094100371003151 + 100.0 * 6.217188358306885
Epoch 2180, val loss: 1.4631439447402954
Epoch 2190, training loss: 621.6044921875 = 0.04028892517089844 + 100.0 * 6.215641975402832
Epoch 2190, val loss: 1.4657559394836426
Epoch 2200, training loss: 621.2578735351562 = 0.039649393409490585 + 100.0 * 6.21218204498291
Epoch 2200, val loss: 1.4725985527038574
Epoch 2210, training loss: 621.182861328125 = 0.03904473036527634 + 100.0 * 6.211437702178955
Epoch 2210, val loss: 1.4762393236160278
Epoch 2220, training loss: 621.30712890625 = 0.03846285119652748 + 100.0 * 6.212687015533447
Epoch 2220, val loss: 1.4810634851455688
Epoch 2230, training loss: 621.3455810546875 = 0.03787410631775856 + 100.0 * 6.213076591491699
Epoch 2230, val loss: 1.483864426612854
Epoch 2240, training loss: 621.48486328125 = 0.037283096462488174 + 100.0 * 6.214475631713867
Epoch 2240, val loss: 1.487273931503296
Epoch 2250, training loss: 621.1818237304688 = 0.036715250462293625 + 100.0 * 6.211451053619385
Epoch 2250, val loss: 1.4921711683273315
Epoch 2260, training loss: 621.2449340820312 = 0.03617267310619354 + 100.0 * 6.212087154388428
Epoch 2260, val loss: 1.4968112707138062
Epoch 2270, training loss: 621.2249755859375 = 0.03562983125448227 + 100.0 * 6.211893558502197
Epoch 2270, val loss: 1.4999220371246338
Epoch 2280, training loss: 621.2390747070312 = 0.035107631236314774 + 100.0 * 6.212039947509766
Epoch 2280, val loss: 1.5043045282363892
Epoch 2290, training loss: 621.176513671875 = 0.034591905772686005 + 100.0 * 6.211419105529785
Epoch 2290, val loss: 1.5085625648498535
Epoch 2300, training loss: 621.124267578125 = 0.03408845514059067 + 100.0 * 6.210901260375977
Epoch 2300, val loss: 1.5128962993621826
Epoch 2310, training loss: 621.8333129882812 = 0.033604707568883896 + 100.0 * 6.217997074127197
Epoch 2310, val loss: 1.5158289670944214
Epoch 2320, training loss: 621.6237182617188 = 0.03307989984750748 + 100.0 * 6.215906620025635
Epoch 2320, val loss: 1.5204838514328003
Epoch 2330, training loss: 621.1737670898438 = 0.032601483166217804 + 100.0 * 6.211411952972412
Epoch 2330, val loss: 1.5250686407089233
Epoch 2340, training loss: 620.9638061523438 = 0.03212884068489075 + 100.0 * 6.209316730499268
Epoch 2340, val loss: 1.5295823812484741
Epoch 2350, training loss: 620.9378662109375 = 0.031681980937719345 + 100.0 * 6.209062099456787
Epoch 2350, val loss: 1.533566951751709
Epoch 2360, training loss: 621.4550170898438 = 0.031266797333955765 + 100.0 * 6.214237689971924
Epoch 2360, val loss: 1.5384730100631714
Epoch 2370, training loss: 621.0528564453125 = 0.030802570283412933 + 100.0 * 6.2102203369140625
Epoch 2370, val loss: 1.542090892791748
Epoch 2380, training loss: 620.890380859375 = 0.030356641858816147 + 100.0 * 6.2086005210876465
Epoch 2380, val loss: 1.5458645820617676
Epoch 2390, training loss: 621.0414428710938 = 0.029939930886030197 + 100.0 * 6.2101149559021
Epoch 2390, val loss: 1.5500777959823608
Epoch 2400, training loss: 620.9391479492188 = 0.02952343039214611 + 100.0 * 6.2090959548950195
Epoch 2400, val loss: 1.5542101860046387
Epoch 2410, training loss: 621.0958251953125 = 0.029127193614840508 + 100.0 * 6.210667133331299
Epoch 2410, val loss: 1.5582764148712158
Epoch 2420, training loss: 621.0020751953125 = 0.028720887377858162 + 100.0 * 6.209733009338379
Epoch 2420, val loss: 1.561959981918335
Epoch 2430, training loss: 620.9991455078125 = 0.02832288108766079 + 100.0 * 6.209708213806152
Epoch 2430, val loss: 1.564857840538025
Epoch 2440, training loss: 621.2154541015625 = 0.02794233150780201 + 100.0 * 6.211874961853027
Epoch 2440, val loss: 1.5684071779251099
Epoch 2450, training loss: 620.8678588867188 = 0.02755061537027359 + 100.0 * 6.20840311050415
Epoch 2450, val loss: 1.571980357170105
Epoch 2460, training loss: 620.765625 = 0.027183011174201965 + 100.0 * 6.2073845863342285
Epoch 2460, val loss: 1.5763475894927979
Epoch 2470, training loss: 620.9029541015625 = 0.02683357335627079 + 100.0 * 6.208760738372803
Epoch 2470, val loss: 1.5796735286712646
Epoch 2480, training loss: 621.11474609375 = 0.02647659182548523 + 100.0 * 6.210882663726807
Epoch 2480, val loss: 1.5828075408935547
Epoch 2490, training loss: 621.2710571289062 = 0.026121633127331734 + 100.0 * 6.212449073791504
Epoch 2490, val loss: 1.5859336853027344
Epoch 2500, training loss: 621.0267333984375 = 0.025772012770175934 + 100.0 * 6.210010051727295
Epoch 2500, val loss: 1.5905929803848267
Epoch 2510, training loss: 620.7116088867188 = 0.025428691878914833 + 100.0 * 6.20686149597168
Epoch 2510, val loss: 1.5958691835403442
Epoch 2520, training loss: 620.6784057617188 = 0.0251077301800251 + 100.0 * 6.206533432006836
Epoch 2520, val loss: 1.5991662740707397
Epoch 2530, training loss: 620.7271728515625 = 0.024799076840281487 + 100.0 * 6.207024097442627
Epoch 2530, val loss: 1.6038157939910889
Epoch 2540, training loss: 621.1412963867188 = 0.024498475715517998 + 100.0 * 6.21116828918457
Epoch 2540, val loss: 1.6084153652191162
Epoch 2550, training loss: 620.8372192382812 = 0.024167628958821297 + 100.0 * 6.208130359649658
Epoch 2550, val loss: 1.6103829145431519
Epoch 2560, training loss: 621.0650024414062 = 0.02385694906115532 + 100.0 * 6.210411071777344
Epoch 2560, val loss: 1.6126264333724976
Epoch 2570, training loss: 620.675048828125 = 0.02355017699301243 + 100.0 * 6.206514835357666
Epoch 2570, val loss: 1.6179430484771729
Epoch 2580, training loss: 620.5872192382812 = 0.023248540237545967 + 100.0 * 6.205639362335205
Epoch 2580, val loss: 1.6203136444091797
Epoch 2590, training loss: 620.6026000976562 = 0.02296997606754303 + 100.0 * 6.205796241760254
Epoch 2590, val loss: 1.6239969730377197
Epoch 2600, training loss: 621.0985717773438 = 0.022693419829010963 + 100.0 * 6.210758686065674
Epoch 2600, val loss: 1.6263861656188965
Epoch 2610, training loss: 620.689208984375 = 0.022408224642276764 + 100.0 * 6.206668376922607
Epoch 2610, val loss: 1.6321591138839722
Epoch 2620, training loss: 620.887939453125 = 0.02214430831372738 + 100.0 * 6.208657741546631
Epoch 2620, val loss: 1.6348905563354492
Epoch 2630, training loss: 620.6644897460938 = 0.021863991394639015 + 100.0 * 6.20642614364624
Epoch 2630, val loss: 1.6393934488296509
Epoch 2640, training loss: 620.7633056640625 = 0.021603327244520187 + 100.0 * 6.207417011260986
Epoch 2640, val loss: 1.6430073976516724
Epoch 2650, training loss: 620.7298583984375 = 0.021343998610973358 + 100.0 * 6.207084655761719
Epoch 2650, val loss: 1.6460976600646973
Epoch 2660, training loss: 620.6486206054688 = 0.021079646423459053 + 100.0 * 6.206275463104248
Epoch 2660, val loss: 1.6484944820404053
Epoch 2670, training loss: 620.5394897460938 = 0.02083171345293522 + 100.0 * 6.20518684387207
Epoch 2670, val loss: 1.6531884670257568
Epoch 2680, training loss: 620.49755859375 = 0.020587118342518806 + 100.0 * 6.204770088195801
Epoch 2680, val loss: 1.65631902217865
Epoch 2690, training loss: 620.5818481445312 = 0.020349327474832535 + 100.0 * 6.205615520477295
Epoch 2690, val loss: 1.6593557596206665
Epoch 2700, training loss: 620.7877197265625 = 0.020114751532673836 + 100.0 * 6.207675933837891
Epoch 2700, val loss: 1.6613268852233887
Epoch 2710, training loss: 621.2578125 = 0.019877715036273003 + 100.0 * 6.212378978729248
Epoch 2710, val loss: 1.6645973920822144
Epoch 2720, training loss: 620.7045288085938 = 0.019638346508145332 + 100.0 * 6.206848621368408
Epoch 2720, val loss: 1.6702579259872437
Epoch 2730, training loss: 620.4121704101562 = 0.019403774291276932 + 100.0 * 6.203927516937256
Epoch 2730, val loss: 1.6724311113357544
Epoch 2740, training loss: 620.3232421875 = 0.019191835075616837 + 100.0 * 6.203041076660156
Epoch 2740, val loss: 1.6766705513000488
Epoch 2750, training loss: 620.326416015625 = 0.018984636291861534 + 100.0 * 6.2030744552612305
Epoch 2750, val loss: 1.679898977279663
Epoch 2760, training loss: 621.0326538085938 = 0.01879681833088398 + 100.0 * 6.210138320922852
Epoch 2760, val loss: 1.684126853942871
Epoch 2770, training loss: 620.55224609375 = 0.01856558583676815 + 100.0 * 6.205337047576904
Epoch 2770, val loss: 1.6867481470108032
Epoch 2780, training loss: 620.9283447265625 = 0.018362537026405334 + 100.0 * 6.209099769592285
Epoch 2780, val loss: 1.690977931022644
Epoch 2790, training loss: 620.4998779296875 = 0.018134072422981262 + 100.0 * 6.204817771911621
Epoch 2790, val loss: 1.6925910711288452
Epoch 2800, training loss: 620.322021484375 = 0.017939087003469467 + 100.0 * 6.203041076660156
Epoch 2800, val loss: 1.696498155593872
Epoch 2810, training loss: 620.5514526367188 = 0.017750462517142296 + 100.0 * 6.205337047576904
Epoch 2810, val loss: 1.6997214555740356
Epoch 2820, training loss: 620.3585205078125 = 0.017550954595208168 + 100.0 * 6.203409194946289
Epoch 2820, val loss: 1.7015727758407593
Epoch 2830, training loss: 620.7840576171875 = 0.017365016043186188 + 100.0 * 6.207666397094727
Epoch 2830, val loss: 1.703368067741394
Epoch 2840, training loss: 620.5374755859375 = 0.017172304913401604 + 100.0 * 6.205203533172607
Epoch 2840, val loss: 1.708800196647644
Epoch 2850, training loss: 620.3199462890625 = 0.016981951892375946 + 100.0 * 6.203029632568359
Epoch 2850, val loss: 1.71096932888031
Epoch 2860, training loss: 620.1951904296875 = 0.0168069489300251 + 100.0 * 6.201784133911133
Epoch 2860, val loss: 1.7158259153366089
Epoch 2870, training loss: 620.2532958984375 = 0.016637781634926796 + 100.0 * 6.202366352081299
Epoch 2870, val loss: 1.7194159030914307
Epoch 2880, training loss: 620.650146484375 = 0.016473960131406784 + 100.0 * 6.206336498260498
Epoch 2880, val loss: 1.722669005393982
Epoch 2890, training loss: 620.2474975585938 = 0.016290126368403435 + 100.0 * 6.202311992645264
Epoch 2890, val loss: 1.725106954574585
Epoch 2900, training loss: 620.4703979492188 = 0.016124337911605835 + 100.0 * 6.204542636871338
Epoch 2900, val loss: 1.7274576425552368
Epoch 2910, training loss: 620.537353515625 = 0.01595541276037693 + 100.0 * 6.205214500427246
Epoch 2910, val loss: 1.731902837753296
Epoch 2920, training loss: 620.27392578125 = 0.01578276790678501 + 100.0 * 6.20258092880249
Epoch 2920, val loss: 1.7333509922027588
Epoch 2930, training loss: 620.138427734375 = 0.015616416931152344 + 100.0 * 6.201228141784668
Epoch 2930, val loss: 1.7360341548919678
Epoch 2940, training loss: 620.120361328125 = 0.015462448820471764 + 100.0 * 6.201049327850342
Epoch 2940, val loss: 1.7397410869598389
Epoch 2950, training loss: 620.4608764648438 = 0.015312480740249157 + 100.0 * 6.204455375671387
Epoch 2950, val loss: 1.741731882095337
Epoch 2960, training loss: 620.4661865234375 = 0.01515638455748558 + 100.0 * 6.20451021194458
Epoch 2960, val loss: 1.7445456981658936
Epoch 2970, training loss: 620.1258544921875 = 0.015000011771917343 + 100.0 * 6.201108455657959
Epoch 2970, val loss: 1.7494298219680786
Epoch 2980, training loss: 620.07080078125 = 0.014848492108285427 + 100.0 * 6.200559616088867
Epoch 2980, val loss: 1.751984715461731
Epoch 2990, training loss: 620.0993041992188 = 0.014708763919770718 + 100.0 * 6.200845718383789
Epoch 2990, val loss: 1.7556086778640747
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 861.6176147460938 = 1.9320826530456543 + 100.0 * 8.596855163574219
Epoch 0, val loss: 1.9234929084777832
Epoch 10, training loss: 861.5359497070312 = 1.9239799976348877 + 100.0 * 8.59611988067627
Epoch 10, val loss: 1.9158930778503418
Epoch 20, training loss: 861.0260009765625 = 1.913739800453186 + 100.0 * 8.5911226272583
Epoch 20, val loss: 1.9060497283935547
Epoch 30, training loss: 857.4695434570312 = 1.900814175605774 + 100.0 * 8.555686950683594
Epoch 30, val loss: 1.8934576511383057
Epoch 40, training loss: 831.5803833007812 = 1.8856256008148193 + 100.0 * 8.296947479248047
Epoch 40, val loss: 1.878967046737671
Epoch 50, training loss: 751.6727905273438 = 1.8697346448898315 + 100.0 * 7.498030662536621
Epoch 50, val loss: 1.8641449213027954
Epoch 60, training loss: 729.2765502929688 = 1.8564085960388184 + 100.0 * 7.2742018699646
Epoch 60, val loss: 1.8523154258728027
Epoch 70, training loss: 711.6404418945312 = 1.846279501914978 + 100.0 * 7.097941875457764
Epoch 70, val loss: 1.8426411151885986
Epoch 80, training loss: 699.3893432617188 = 1.8357717990875244 + 100.0 * 6.975535869598389
Epoch 80, val loss: 1.8332722187042236
Epoch 90, training loss: 691.5667114257812 = 1.8268260955810547 + 100.0 * 6.897398948669434
Epoch 90, val loss: 1.8248867988586426
Epoch 100, training loss: 685.50244140625 = 1.817944884300232 + 100.0 * 6.8368449211120605
Epoch 100, val loss: 1.8165960311889648
Epoch 110, training loss: 680.531494140625 = 1.8097076416015625 + 100.0 * 6.78721809387207
Epoch 110, val loss: 1.8088648319244385
Epoch 120, training loss: 675.75634765625 = 1.8017247915267944 + 100.0 * 6.739546298980713
Epoch 120, val loss: 1.8011022806167603
Epoch 130, training loss: 670.8212890625 = 1.7940621376037598 + 100.0 * 6.690272331237793
Epoch 130, val loss: 1.7935917377471924
Epoch 140, training loss: 666.3945922851562 = 1.7865922451019287 + 100.0 * 6.6460795402526855
Epoch 140, val loss: 1.7863103151321411
Epoch 150, training loss: 662.6527099609375 = 1.7786602973937988 + 100.0 * 6.608740329742432
Epoch 150, val loss: 1.77876877784729
Epoch 160, training loss: 659.1948852539062 = 1.770163893699646 + 100.0 * 6.574247360229492
Epoch 160, val loss: 1.7708295583724976
Epoch 170, training loss: 656.5971069335938 = 1.7608444690704346 + 100.0 * 6.5483622550964355
Epoch 170, val loss: 1.7621921300888062
Epoch 180, training loss: 654.595458984375 = 1.7503310441970825 + 100.0 * 6.528451442718506
Epoch 180, val loss: 1.7526224851608276
Epoch 190, training loss: 652.8943481445312 = 1.7386947870254517 + 100.0 * 6.511556148529053
Epoch 190, val loss: 1.7420525550842285
Epoch 200, training loss: 651.8326416015625 = 1.725859522819519 + 100.0 * 6.501068115234375
Epoch 200, val loss: 1.730195164680481
Epoch 210, training loss: 650.2058715820312 = 1.7116960287094116 + 100.0 * 6.4849419593811035
Epoch 210, val loss: 1.7173429727554321
Epoch 220, training loss: 648.9718627929688 = 1.6963456869125366 + 100.0 * 6.472755432128906
Epoch 220, val loss: 1.7034518718719482
Epoch 230, training loss: 648.3490600585938 = 1.6797503232955933 + 100.0 * 6.46669340133667
Epoch 230, val loss: 1.6883729696273804
Epoch 240, training loss: 646.9531860351562 = 1.661720871925354 + 100.0 * 6.452914237976074
Epoch 240, val loss: 1.672087550163269
Epoch 250, training loss: 646.006103515625 = 1.6421689987182617 + 100.0 * 6.443639278411865
Epoch 250, val loss: 1.6547670364379883
Epoch 260, training loss: 644.845458984375 = 1.6214174032211304 + 100.0 * 6.4322404861450195
Epoch 260, val loss: 1.6362724304199219
Epoch 270, training loss: 643.919921875 = 1.5993201732635498 + 100.0 * 6.423206329345703
Epoch 270, val loss: 1.616664171218872
Epoch 280, training loss: 643.1273803710938 = 1.5758633613586426 + 100.0 * 6.415514945983887
Epoch 280, val loss: 1.5959080457687378
Epoch 290, training loss: 642.3787841796875 = 1.5511209964752197 + 100.0 * 6.408276557922363
Epoch 290, val loss: 1.5743640661239624
Epoch 300, training loss: 641.559326171875 = 1.525353193283081 + 100.0 * 6.4003400802612305
Epoch 300, val loss: 1.5520257949829102
Epoch 310, training loss: 641.1265258789062 = 1.4985811710357666 + 100.0 * 6.396279335021973
Epoch 310, val loss: 1.5291187763214111
Epoch 320, training loss: 640.263671875 = 1.4710949659347534 + 100.0 * 6.38792610168457
Epoch 320, val loss: 1.5054168701171875
Epoch 330, training loss: 639.6441650390625 = 1.4428255558013916 + 100.0 * 6.382013320922852
Epoch 330, val loss: 1.481497049331665
Epoch 340, training loss: 639.1467895507812 = 1.4141098260879517 + 100.0 * 6.377326488494873
Epoch 340, val loss: 1.4572646617889404
Epoch 350, training loss: 638.7918701171875 = 1.3848505020141602 + 100.0 * 6.374070167541504
Epoch 350, val loss: 1.4325963258743286
Epoch 360, training loss: 638.1796875 = 1.3553237915039062 + 100.0 * 6.368243217468262
Epoch 360, val loss: 1.4081170558929443
Epoch 370, training loss: 637.693359375 = 1.3256949186325073 + 100.0 * 6.36367654800415
Epoch 370, val loss: 1.3836135864257812
Epoch 380, training loss: 637.7225341796875 = 1.2961841821670532 + 100.0 * 6.364263534545898
Epoch 380, val loss: 1.3593063354492188
Epoch 390, training loss: 637.0042724609375 = 1.2664486169815063 + 100.0 * 6.3573784828186035
Epoch 390, val loss: 1.3352704048156738
Epoch 400, training loss: 636.5277709960938 = 1.2372462749481201 + 100.0 * 6.3529052734375
Epoch 400, val loss: 1.3115770816802979
Epoch 410, training loss: 636.1088256835938 = 1.2083492279052734 + 100.0 * 6.349004745483398
Epoch 410, val loss: 1.2883570194244385
Epoch 420, training loss: 635.87158203125 = 1.1798226833343506 + 100.0 * 6.346917629241943
Epoch 420, val loss: 1.2656158208847046
Epoch 430, training loss: 635.6243286132812 = 1.1519265174865723 + 100.0 * 6.344724178314209
Epoch 430, val loss: 1.2438430786132812
Epoch 440, training loss: 635.2950439453125 = 1.1243339776992798 + 100.0 * 6.341707229614258
Epoch 440, val loss: 1.2224597930908203
Epoch 450, training loss: 634.8992919921875 = 1.0976383686065674 + 100.0 * 6.338016510009766
Epoch 450, val loss: 1.201872706413269
Epoch 460, training loss: 634.5009155273438 = 1.0714671611785889 + 100.0 * 6.334294319152832
Epoch 460, val loss: 1.1821067333221436
Epoch 470, training loss: 634.2227172851562 = 1.0461379289627075 + 100.0 * 6.331765651702881
Epoch 470, val loss: 1.1632088422775269
Epoch 480, training loss: 633.8888549804688 = 1.0215471982955933 + 100.0 * 6.328672885894775
Epoch 480, val loss: 1.1454206705093384
Epoch 490, training loss: 634.0718994140625 = 0.997664749622345 + 100.0 * 6.330742359161377
Epoch 490, val loss: 1.1285725831985474
Epoch 500, training loss: 633.6403198242188 = 0.9744423031806946 + 100.0 * 6.326659202575684
Epoch 500, val loss: 1.1119614839553833
Epoch 510, training loss: 633.2730712890625 = 0.9518920183181763 + 100.0 * 6.323211669921875
Epoch 510, val loss: 1.0965638160705566
Epoch 520, training loss: 633.0628051757812 = 0.9300400614738464 + 100.0 * 6.3213276863098145
Epoch 520, val loss: 1.0822007656097412
Epoch 530, training loss: 632.7078857421875 = 0.9087313413619995 + 100.0 * 6.317991256713867
Epoch 530, val loss: 1.0684903860092163
Epoch 540, training loss: 632.409423828125 = 0.8880651593208313 + 100.0 * 6.315213680267334
Epoch 540, val loss: 1.0554994344711304
Epoch 550, training loss: 632.1534423828125 = 0.867911696434021 + 100.0 * 6.312855243682861
Epoch 550, val loss: 1.0431852340698242
Epoch 560, training loss: 634.0065307617188 = 0.8481610417366028 + 100.0 * 6.3315839767456055
Epoch 560, val loss: 1.031478762626648
Epoch 570, training loss: 632.36328125 = 0.8287401795387268 + 100.0 * 6.315345287322998
Epoch 570, val loss: 1.0202555656433105
Epoch 580, training loss: 631.5582885742188 = 0.8098564743995667 + 100.0 * 6.307484149932861
Epoch 580, val loss: 1.0095672607421875
Epoch 590, training loss: 631.3112182617188 = 0.7916109561920166 + 100.0 * 6.3051958084106445
Epoch 590, val loss: 0.999758780002594
Epoch 600, training loss: 631.126220703125 = 0.7737062573432922 + 100.0 * 6.303525447845459
Epoch 600, val loss: 0.9904835820198059
Epoch 610, training loss: 631.2049560546875 = 0.7561790347099304 + 100.0 * 6.304488182067871
Epoch 610, val loss: 0.9817035794258118
Epoch 620, training loss: 631.28271484375 = 0.7386388778686523 + 100.0 * 6.305440425872803
Epoch 620, val loss: 0.9728004932403564
Epoch 630, training loss: 630.6073608398438 = 0.7216389775276184 + 100.0 * 6.298857688903809
Epoch 630, val loss: 0.9647114872932434
Epoch 640, training loss: 630.442138671875 = 0.705064594745636 + 100.0 * 6.297370910644531
Epoch 640, val loss: 0.9572023749351501
Epoch 650, training loss: 631.1016235351562 = 0.6887728571891785 + 100.0 * 6.304128646850586
Epoch 650, val loss: 0.9500095844268799
Epoch 660, training loss: 630.1257934570312 = 0.672562301158905 + 100.0 * 6.294532299041748
Epoch 660, val loss: 0.9429791569709778
Epoch 670, training loss: 629.9634399414062 = 0.6567942500114441 + 100.0 * 6.293066501617432
Epoch 670, val loss: 0.9362096786499023
Epoch 680, training loss: 629.9528198242188 = 0.6414073705673218 + 100.0 * 6.293114185333252
Epoch 680, val loss: 0.9301689863204956
Epoch 690, training loss: 629.7692260742188 = 0.6261752247810364 + 100.0 * 6.291430950164795
Epoch 690, val loss: 0.9244057536125183
Epoch 700, training loss: 629.5947875976562 = 0.6112700700759888 + 100.0 * 6.289835453033447
Epoch 700, val loss: 0.918728232383728
Epoch 710, training loss: 629.4164428710938 = 0.5967265367507935 + 100.0 * 6.288197040557861
Epoch 710, val loss: 0.9136435985565186
Epoch 720, training loss: 629.5817260742188 = 0.582487940788269 + 100.0 * 6.289992809295654
Epoch 720, val loss: 0.9088709950447083
Epoch 730, training loss: 629.2991943359375 = 0.5685681700706482 + 100.0 * 6.287306308746338
Epoch 730, val loss: 0.904211163520813
Epoch 740, training loss: 629.4047241210938 = 0.554848849773407 + 100.0 * 6.288498401641846
Epoch 740, val loss: 0.9000744819641113
Epoch 750, training loss: 628.9638671875 = 0.5413702726364136 + 100.0 * 6.284224987030029
Epoch 750, val loss: 0.8960867524147034
Epoch 760, training loss: 628.7378540039062 = 0.5283822417259216 + 100.0 * 6.282094955444336
Epoch 760, val loss: 0.892726480960846
Epoch 770, training loss: 629.1242065429688 = 0.5156823992729187 + 100.0 * 6.28608512878418
Epoch 770, val loss: 0.8895067572593689
Epoch 780, training loss: 628.6793212890625 = 0.5030732750892639 + 100.0 * 6.281762599945068
Epoch 780, val loss: 0.8867012858390808
Epoch 790, training loss: 628.494384765625 = 0.49094918370246887 + 100.0 * 6.280034065246582
Epoch 790, val loss: 0.8839884996414185
Epoch 800, training loss: 628.361083984375 = 0.4790806770324707 + 100.0 * 6.278820037841797
Epoch 800, val loss: 0.8817550539970398
Epoch 810, training loss: 628.2433471679688 = 0.46743977069854736 + 100.0 * 6.277759075164795
Epoch 810, val loss: 0.8795803189277649
Epoch 820, training loss: 628.544677734375 = 0.4561792016029358 + 100.0 * 6.280884742736816
Epoch 820, val loss: 0.8776876330375671
Epoch 830, training loss: 628.1614379882812 = 0.4449760317802429 + 100.0 * 6.277164936065674
Epoch 830, val loss: 0.8762955665588379
Epoch 840, training loss: 627.9119262695312 = 0.4342288374900818 + 100.0 * 6.274777412414551
Epoch 840, val loss: 0.8749288320541382
Epoch 850, training loss: 627.7181396484375 = 0.4237856864929199 + 100.0 * 6.272943496704102
Epoch 850, val loss: 0.8738723993301392
Epoch 860, training loss: 627.6128540039062 = 0.4136415421962738 + 100.0 * 6.271992206573486
Epoch 860, val loss: 0.8731886744499207
Epoch 870, training loss: 628.3936767578125 = 0.4036106765270233 + 100.0 * 6.279900550842285
Epoch 870, val loss: 0.8725500106811523
Epoch 880, training loss: 627.8276977539062 = 0.39380761981010437 + 100.0 * 6.274338722229004
Epoch 880, val loss: 0.8720970153808594
Epoch 890, training loss: 627.4307250976562 = 0.38437336683273315 + 100.0 * 6.270462989807129
Epoch 890, val loss: 0.8722337484359741
Epoch 900, training loss: 627.2583618164062 = 0.3751588761806488 + 100.0 * 6.268831729888916
Epoch 900, val loss: 0.8726489543914795
Epoch 910, training loss: 627.0918579101562 = 0.36627691984176636 + 100.0 * 6.267255783081055
Epoch 910, val loss: 0.8732172250747681
Epoch 920, training loss: 627.240234375 = 0.35761770606040955 + 100.0 * 6.268826484680176
Epoch 920, val loss: 0.8738861083984375
Epoch 930, training loss: 627.1585083007812 = 0.3490414619445801 + 100.0 * 6.268094539642334
Epoch 930, val loss: 0.8746740221977234
Epoch 940, training loss: 627.0872192382812 = 0.340769499540329 + 100.0 * 6.267464637756348
Epoch 940, val loss: 0.8757206201553345
Epoch 950, training loss: 626.77392578125 = 0.3327529728412628 + 100.0 * 6.264411926269531
Epoch 950, val loss: 0.8770696520805359
Epoch 960, training loss: 626.8346557617188 = 0.32501617074012756 + 100.0 * 6.265096187591553
Epoch 960, val loss: 0.878756046295166
Epoch 970, training loss: 626.9678955078125 = 0.31744644045829773 + 100.0 * 6.266504764556885
Epoch 970, val loss: 0.8804178833961487
Epoch 980, training loss: 626.8477172851562 = 0.30994802713394165 + 100.0 * 6.265377521514893
Epoch 980, val loss: 0.8821505308151245
Epoch 990, training loss: 626.7286987304688 = 0.3027033507823944 + 100.0 * 6.264260292053223
Epoch 990, val loss: 0.8842900395393372
Epoch 1000, training loss: 626.4113159179688 = 0.29572317004203796 + 100.0 * 6.26115608215332
Epoch 1000, val loss: 0.8864135146141052
Epoch 1010, training loss: 626.3739624023438 = 0.2889472246170044 + 100.0 * 6.260850429534912
Epoch 1010, val loss: 0.8889555335044861
Epoch 1020, training loss: 626.4266357421875 = 0.2823270857334137 + 100.0 * 6.261443138122559
Epoch 1020, val loss: 0.8915390372276306
Epoch 1030, training loss: 626.78271484375 = 0.27586230635643005 + 100.0 * 6.265068054199219
Epoch 1030, val loss: 0.8944641351699829
Epoch 1040, training loss: 626.3148193359375 = 0.26951879262924194 + 100.0 * 6.260452747344971
Epoch 1040, val loss: 0.8974862694740295
Epoch 1050, training loss: 626.2944946289062 = 0.2634134590625763 + 100.0 * 6.260310649871826
Epoch 1050, val loss: 0.900615394115448
Epoch 1060, training loss: 626.5328979492188 = 0.2574284076690674 + 100.0 * 6.262754440307617
Epoch 1060, val loss: 0.9039226174354553
Epoch 1070, training loss: 625.9852905273438 = 0.2514664828777313 + 100.0 * 6.257338523864746
Epoch 1070, val loss: 0.9070520401000977
Epoch 1080, training loss: 625.8219604492188 = 0.2458326369524002 + 100.0 * 6.25576114654541
Epoch 1080, val loss: 0.91082763671875
Epoch 1090, training loss: 625.7655029296875 = 0.24034646153450012 + 100.0 * 6.255251407623291
Epoch 1090, val loss: 0.9146714806556702
Epoch 1100, training loss: 625.7281494140625 = 0.23497557640075684 + 100.0 * 6.254931926727295
Epoch 1100, val loss: 0.9186385273933411
Epoch 1110, training loss: 626.247802734375 = 0.22975464165210724 + 100.0 * 6.260180950164795
Epoch 1110, val loss: 0.9227539896965027
Epoch 1120, training loss: 626.1681518554688 = 0.2245098352432251 + 100.0 * 6.25943660736084
Epoch 1120, val loss: 0.9261285066604614
Epoch 1130, training loss: 625.4591064453125 = 0.2194274216890335 + 100.0 * 6.252397060394287
Epoch 1130, val loss: 0.9303880929946899
Epoch 1140, training loss: 625.432373046875 = 0.2145194113254547 + 100.0 * 6.252178192138672
Epoch 1140, val loss: 0.9348549246788025
Epoch 1150, training loss: 626.7626953125 = 0.2096979022026062 + 100.0 * 6.265529632568359
Epoch 1150, val loss: 0.9389566779136658
Epoch 1160, training loss: 625.5120239257812 = 0.20505553483963013 + 100.0 * 6.2530694007873535
Epoch 1160, val loss: 0.943581759929657
Epoch 1170, training loss: 625.3224487304688 = 0.20043326914310455 + 100.0 * 6.251220226287842
Epoch 1170, val loss: 0.9482197165489197
Epoch 1180, training loss: 625.254150390625 = 0.19597473740577698 + 100.0 * 6.250581741333008
Epoch 1180, val loss: 0.9529441595077515
Epoch 1190, training loss: 625.5677490234375 = 0.1916736364364624 + 100.0 * 6.253761291503906
Epoch 1190, val loss: 0.9576824903488159
Epoch 1200, training loss: 625.1676635742188 = 0.18734495341777802 + 100.0 * 6.24980354309082
Epoch 1200, val loss: 0.9624889492988586
Epoch 1210, training loss: 625.46875 = 0.18319843709468842 + 100.0 * 6.25285530090332
Epoch 1210, val loss: 0.9674612283706665
Epoch 1220, training loss: 625.0231323242188 = 0.17908895015716553 + 100.0 * 6.248440742492676
Epoch 1220, val loss: 0.9724265933036804
Epoch 1230, training loss: 625.072998046875 = 0.1751018613576889 + 100.0 * 6.248978614807129
Epoch 1230, val loss: 0.9776816964149475
Epoch 1240, training loss: 625.1195068359375 = 0.17123466730117798 + 100.0 * 6.24948263168335
Epoch 1240, val loss: 0.9826524257659912
Epoch 1250, training loss: 624.8670654296875 = 0.16740289330482483 + 100.0 * 6.2469964027404785
Epoch 1250, val loss: 0.9878695011138916
Epoch 1260, training loss: 625.16552734375 = 0.16372470557689667 + 100.0 * 6.25001859664917
Epoch 1260, val loss: 0.9931207895278931
Epoch 1270, training loss: 624.8414916992188 = 0.16002359986305237 + 100.0 * 6.246814727783203
Epoch 1270, val loss: 0.998273491859436
Epoch 1280, training loss: 624.7334594726562 = 0.1564486175775528 + 100.0 * 6.245770454406738
Epoch 1280, val loss: 1.0035347938537598
Epoch 1290, training loss: 625.35986328125 = 0.15295246243476868 + 100.0 * 6.252068996429443
Epoch 1290, val loss: 1.0087021589279175
Epoch 1300, training loss: 625.0191040039062 = 0.1495172083377838 + 100.0 * 6.2486958503723145
Epoch 1300, val loss: 1.0143293142318726
Epoch 1310, training loss: 624.5762329101562 = 0.14616277813911438 + 100.0 * 6.244300842285156
Epoch 1310, val loss: 1.0200556516647339
Epoch 1320, training loss: 624.389404296875 = 0.1429353803396225 + 100.0 * 6.242464542388916
Epoch 1320, val loss: 1.0256575345993042
Epoch 1330, training loss: 624.3656005859375 = 0.1397886425256729 + 100.0 * 6.242258071899414
Epoch 1330, val loss: 1.0312410593032837
Epoch 1340, training loss: 625.1400146484375 = 0.1367362141609192 + 100.0 * 6.250032901763916
Epoch 1340, val loss: 1.0369548797607422
Epoch 1350, training loss: 625.0559692382812 = 0.13362587988376617 + 100.0 * 6.249223709106445
Epoch 1350, val loss: 1.042847990989685
Epoch 1360, training loss: 624.274169921875 = 0.1305788904428482 + 100.0 * 6.241436004638672
Epoch 1360, val loss: 1.0480670928955078
Epoch 1370, training loss: 624.2603759765625 = 0.12769058346748352 + 100.0 * 6.241326808929443
Epoch 1370, val loss: 1.0538630485534668
Epoch 1380, training loss: 624.1381225585938 = 0.12486568838357925 + 100.0 * 6.2401323318481445
Epoch 1380, val loss: 1.0597047805786133
Epoch 1390, training loss: 624.69482421875 = 0.12212714552879333 + 100.0 * 6.245727062225342
Epoch 1390, val loss: 1.065460205078125
Epoch 1400, training loss: 624.2858276367188 = 0.1193566620349884 + 100.0 * 6.241664409637451
Epoch 1400, val loss: 1.0710806846618652
Epoch 1410, training loss: 625.2457275390625 = 0.11666133254766464 + 100.0 * 6.251290798187256
Epoch 1410, val loss: 1.0763483047485352
Epoch 1420, training loss: 624.3369750976562 = 0.11406798660755157 + 100.0 * 6.242228984832764
Epoch 1420, val loss: 1.0829365253448486
Epoch 1430, training loss: 623.9918212890625 = 0.1114964559674263 + 100.0 * 6.238803386688232
Epoch 1430, val loss: 1.0883824825286865
Epoch 1440, training loss: 623.8810424804688 = 0.10905739665031433 + 100.0 * 6.237720012664795
Epoch 1440, val loss: 1.0944838523864746
Epoch 1450, training loss: 623.881103515625 = 0.10665927082300186 + 100.0 * 6.237744331359863
Epoch 1450, val loss: 1.100225806236267
Epoch 1460, training loss: 624.677734375 = 0.10428588837385178 + 100.0 * 6.245734214782715
Epoch 1460, val loss: 1.1060457229614258
Epoch 1470, training loss: 623.917724609375 = 0.10198330134153366 + 100.0 * 6.238157272338867
Epoch 1470, val loss: 1.1121728420257568
Epoch 1480, training loss: 623.8248291015625 = 0.0997176393866539 + 100.0 * 6.237251281738281
Epoch 1480, val loss: 1.1181411743164062
Epoch 1490, training loss: 623.965087890625 = 0.09753691405057907 + 100.0 * 6.238675594329834
Epoch 1490, val loss: 1.1240136623382568
Epoch 1500, training loss: 623.6422119140625 = 0.09537818282842636 + 100.0 * 6.23546838760376
Epoch 1500, val loss: 1.1296601295471191
Epoch 1510, training loss: 623.8563842773438 = 0.09329776465892792 + 100.0 * 6.237630367279053
Epoch 1510, val loss: 1.1352307796478271
Epoch 1520, training loss: 624.1435546875 = 0.09124965220689774 + 100.0 * 6.240523338317871
Epoch 1520, val loss: 1.141464352607727
Epoch 1530, training loss: 623.5933227539062 = 0.08923283964395523 + 100.0 * 6.235040664672852
Epoch 1530, val loss: 1.147149920463562
Epoch 1540, training loss: 623.4396362304688 = 0.08729246258735657 + 100.0 * 6.233523845672607
Epoch 1540, val loss: 1.1530189514160156
Epoch 1550, training loss: 623.4291381835938 = 0.08542169630527496 + 100.0 * 6.2334370613098145
Epoch 1550, val loss: 1.1589500904083252
Epoch 1560, training loss: 624.5189819335938 = 0.08365016430616379 + 100.0 * 6.244353294372559
Epoch 1560, val loss: 1.1651856899261475
Epoch 1570, training loss: 623.9219970703125 = 0.08173204213380814 + 100.0 * 6.238402843475342
Epoch 1570, val loss: 1.1697983741760254
Epoch 1580, training loss: 623.4750366210938 = 0.07997997105121613 + 100.0 * 6.233950614929199
Epoch 1580, val loss: 1.17616868019104
Epoch 1590, training loss: 623.2835083007812 = 0.0782618448138237 + 100.0 * 6.232052326202393
Epoch 1590, val loss: 1.1818270683288574
Epoch 1600, training loss: 623.5560913085938 = 0.07661736011505127 + 100.0 * 6.234794616699219
Epoch 1600, val loss: 1.1874760389328003
Epoch 1610, training loss: 623.261962890625 = 0.07498470693826675 + 100.0 * 6.231869220733643
Epoch 1610, val loss: 1.1934232711791992
Epoch 1620, training loss: 623.4263916015625 = 0.07339441776275635 + 100.0 * 6.233530521392822
Epoch 1620, val loss: 1.1990768909454346
Epoch 1630, training loss: 623.3890991210938 = 0.0718543604016304 + 100.0 * 6.233172416687012
Epoch 1630, val loss: 1.2048662900924683
Epoch 1640, training loss: 623.2479248046875 = 0.07032087445259094 + 100.0 * 6.231776237487793
Epoch 1640, val loss: 1.2104873657226562
Epoch 1650, training loss: 623.7625122070312 = 0.06886065006256104 + 100.0 * 6.236936569213867
Epoch 1650, val loss: 1.2158252000808716
Epoch 1660, training loss: 623.2320556640625 = 0.06739496439695358 + 100.0 * 6.23164701461792
Epoch 1660, val loss: 1.221529245376587
Epoch 1670, training loss: 623.1143188476562 = 0.06597292423248291 + 100.0 * 6.230483531951904
Epoch 1670, val loss: 1.2270177602767944
Epoch 1680, training loss: 622.9995727539062 = 0.0646209642291069 + 100.0 * 6.229349136352539
Epoch 1680, val loss: 1.232835292816162
Epoch 1690, training loss: 623.0018310546875 = 0.06330665200948715 + 100.0 * 6.2293853759765625
Epoch 1690, val loss: 1.2384800910949707
Epoch 1700, training loss: 623.835205078125 = 0.062062524259090424 + 100.0 * 6.237731456756592
Epoch 1700, val loss: 1.2445721626281738
Epoch 1710, training loss: 623.3431396484375 = 0.06073601543903351 + 100.0 * 6.232824325561523
Epoch 1710, val loss: 1.2489502429962158
Epoch 1720, training loss: 623.1021728515625 = 0.05948378145694733 + 100.0 * 6.230426788330078
Epoch 1720, val loss: 1.2546570301055908
Epoch 1730, training loss: 622.9649047851562 = 0.05828326568007469 + 100.0 * 6.229065895080566
Epoch 1730, val loss: 1.259980320930481
Epoch 1740, training loss: 623.0831909179688 = 0.05712662264704704 + 100.0 * 6.230260848999023
Epoch 1740, val loss: 1.2655235528945923
Epoch 1750, training loss: 623.1107177734375 = 0.055960554629564285 + 100.0 * 6.2305474281311035
Epoch 1750, val loss: 1.270508885383606
Epoch 1760, training loss: 622.9412841796875 = 0.054824091494083405 + 100.0 * 6.228864669799805
Epoch 1760, val loss: 1.2757041454315186
Epoch 1770, training loss: 622.8232421875 = 0.05374513193964958 + 100.0 * 6.227694511413574
Epoch 1770, val loss: 1.2810550928115845
Epoch 1780, training loss: 622.7695922851562 = 0.05268904194235802 + 100.0 * 6.227169036865234
Epoch 1780, val loss: 1.2861058712005615
Epoch 1790, training loss: 623.0802612304688 = 0.051662202924489975 + 100.0 * 6.230286121368408
Epoch 1790, val loss: 1.2910494804382324
Epoch 1800, training loss: 622.8768920898438 = 0.05063427984714508 + 100.0 * 6.228262424468994
Epoch 1800, val loss: 1.2960270643234253
Epoch 1810, training loss: 622.7745971679688 = 0.04965370148420334 + 100.0 * 6.2272491455078125
Epoch 1810, val loss: 1.3012285232543945
Epoch 1820, training loss: 622.8116455078125 = 0.0486842505633831 + 100.0 * 6.227629661560059
Epoch 1820, val loss: 1.3059024810791016
Epoch 1830, training loss: 622.7280883789062 = 0.047739140689373016 + 100.0 * 6.226803779602051
Epoch 1830, val loss: 1.3111708164215088
Epoch 1840, training loss: 622.7506713867188 = 0.04682708531618118 + 100.0 * 6.227038383483887
Epoch 1840, val loss: 1.3158882856369019
Epoch 1850, training loss: 622.5139770507812 = 0.04593725875020027 + 100.0 * 6.224679946899414
Epoch 1850, val loss: 1.3209909200668335
Epoch 1860, training loss: 622.6044311523438 = 0.04507475718855858 + 100.0 * 6.225593566894531
Epoch 1860, val loss: 1.3261961936950684
Epoch 1870, training loss: 622.8753051757812 = 0.04424208402633667 + 100.0 * 6.228310585021973
Epoch 1870, val loss: 1.3311625719070435
Epoch 1880, training loss: 622.708251953125 = 0.04340476542711258 + 100.0 * 6.226648807525635
Epoch 1880, val loss: 1.335669755935669
Epoch 1890, training loss: 622.5252075195312 = 0.042586881667375565 + 100.0 * 6.224825859069824
Epoch 1890, val loss: 1.340734601020813
Epoch 1900, training loss: 622.9929809570312 = 0.04183502495288849 + 100.0 * 6.229511737823486
Epoch 1900, val loss: 1.345596432685852
Epoch 1910, training loss: 622.4149780273438 = 0.04101746901869774 + 100.0 * 6.2237396240234375
Epoch 1910, val loss: 1.349624514579773
Epoch 1920, training loss: 622.3455810546875 = 0.040268030017614365 + 100.0 * 6.223052978515625
Epoch 1920, val loss: 1.3541213274002075
Epoch 1930, training loss: 622.4029541015625 = 0.039547935128211975 + 100.0 * 6.223634243011475
Epoch 1930, val loss: 1.3587676286697388
Epoch 1940, training loss: 622.9205932617188 = 0.03884543478488922 + 100.0 * 6.228817462921143
Epoch 1940, val loss: 1.3636354207992554
Epoch 1950, training loss: 622.7089233398438 = 0.03812979534268379 + 100.0 * 6.226707458496094
Epoch 1950, val loss: 1.3680930137634277
Epoch 1960, training loss: 622.3126831054688 = 0.03743458166718483 + 100.0 * 6.222752571105957
Epoch 1960, val loss: 1.372240424156189
Epoch 1970, training loss: 622.208740234375 = 0.0367700420320034 + 100.0 * 6.221719741821289
Epoch 1970, val loss: 1.3769367933273315
Epoch 1980, training loss: 622.2427368164062 = 0.036134589463472366 + 100.0 * 6.2220659255981445
Epoch 1980, val loss: 1.3814724683761597
Epoch 1990, training loss: 622.7693481445312 = 0.035519227385520935 + 100.0 * 6.2273383140563965
Epoch 1990, val loss: 1.3861602544784546
Epoch 2000, training loss: 622.7234497070312 = 0.034891653805971146 + 100.0 * 6.2268853187561035
Epoch 2000, val loss: 1.390716791152954
Epoch 2010, training loss: 622.2561645507812 = 0.034262992441654205 + 100.0 * 6.222218990325928
Epoch 2010, val loss: 1.3943716287612915
Epoch 2020, training loss: 622.1221313476562 = 0.03366059064865112 + 100.0 * 6.220884799957275
Epoch 2020, val loss: 1.398834228515625
Epoch 2030, training loss: 622.1824951171875 = 0.033099714666604996 + 100.0 * 6.221494197845459
Epoch 2030, val loss: 1.4029783010482788
Epoch 2040, training loss: 622.8828735351562 = 0.03254874423146248 + 100.0 * 6.228503227233887
Epoch 2040, val loss: 1.4073411226272583
Epoch 2050, training loss: 622.2691650390625 = 0.03197881206870079 + 100.0 * 6.222372055053711
Epoch 2050, val loss: 1.4115006923675537
Epoch 2060, training loss: 622.0567016601562 = 0.03142791986465454 + 100.0 * 6.220252990722656
Epoch 2060, val loss: 1.4152525663375854
Epoch 2070, training loss: 622.1017456054688 = 0.030909741297364235 + 100.0 * 6.220708847045898
Epoch 2070, val loss: 1.419358730316162
Epoch 2080, training loss: 622.4556274414062 = 0.030398394912481308 + 100.0 * 6.224252223968506
Epoch 2080, val loss: 1.4232560396194458
Epoch 2090, training loss: 622.1192626953125 = 0.02990773133933544 + 100.0 * 6.220893383026123
Epoch 2090, val loss: 1.4280953407287598
Epoch 2100, training loss: 621.979736328125 = 0.029410040006041527 + 100.0 * 6.219502925872803
Epoch 2100, val loss: 1.4316853284835815
Epoch 2110, training loss: 622.2955322265625 = 0.02894498221576214 + 100.0 * 6.222666263580322
Epoch 2110, val loss: 1.4360644817352295
Epoch 2120, training loss: 621.8812255859375 = 0.028469478711485863 + 100.0 * 6.218527793884277
Epoch 2120, val loss: 1.439700722694397
Epoch 2130, training loss: 621.888427734375 = 0.028011664748191833 + 100.0 * 6.21860408782959
Epoch 2130, val loss: 1.4435014724731445
Epoch 2140, training loss: 622.7061767578125 = 0.027595577761530876 + 100.0 * 6.226786136627197
Epoch 2140, val loss: 1.4479812383651733
Epoch 2150, training loss: 622.035888671875 = 0.02712622657418251 + 100.0 * 6.220088005065918
Epoch 2150, val loss: 1.450982928276062
Epoch 2160, training loss: 621.8568725585938 = 0.026690522208809853 + 100.0 * 6.218301773071289
Epoch 2160, val loss: 1.4548155069351196
Epoch 2170, training loss: 621.7544555664062 = 0.02627747692167759 + 100.0 * 6.217281341552734
Epoch 2170, val loss: 1.4586306810379028
Epoch 2180, training loss: 621.9452514648438 = 0.025882145389914513 + 100.0 * 6.219193935394287
Epoch 2180, val loss: 1.4625414609909058
Epoch 2190, training loss: 622.0084228515625 = 0.02548312023282051 + 100.0 * 6.219829082489014
Epoch 2190, val loss: 1.465775728225708
Epoch 2200, training loss: 621.8777465820312 = 0.02508116513490677 + 100.0 * 6.218526363372803
Epoch 2200, val loss: 1.469279170036316
Epoch 2210, training loss: 621.879638671875 = 0.024701302871108055 + 100.0 * 6.2185492515563965
Epoch 2210, val loss: 1.4730535745620728
Epoch 2220, training loss: 622.094970703125 = 0.024329137057065964 + 100.0 * 6.220706462860107
Epoch 2220, val loss: 1.4760313034057617
Epoch 2230, training loss: 622.2410888671875 = 0.023962201550602913 + 100.0 * 6.222171306610107
Epoch 2230, val loss: 1.479980230331421
Epoch 2240, training loss: 621.7911376953125 = 0.023595014587044716 + 100.0 * 6.21767520904541
Epoch 2240, val loss: 1.4837028980255127
Epoch 2250, training loss: 621.6437377929688 = 0.02324257232248783 + 100.0 * 6.21620512008667
Epoch 2250, val loss: 1.4871248006820679
Epoch 2260, training loss: 621.6426391601562 = 0.022904587909579277 + 100.0 * 6.216197490692139
Epoch 2260, val loss: 1.4906443357467651
Epoch 2270, training loss: 622.0635986328125 = 0.02257358841598034 + 100.0 * 6.220410346984863
Epoch 2270, val loss: 1.4938948154449463
Epoch 2280, training loss: 621.6676025390625 = 0.0222381129860878 + 100.0 * 6.216453552246094
Epoch 2280, val loss: 1.497549057006836
Epoch 2290, training loss: 621.7982788085938 = 0.021925020962953568 + 100.0 * 6.217763900756836
Epoch 2290, val loss: 1.5011528730392456
Epoch 2300, training loss: 621.8426513671875 = 0.021613260731101036 + 100.0 * 6.218210220336914
Epoch 2300, val loss: 1.5047714710235596
Epoch 2310, training loss: 621.7998046875 = 0.02129937894642353 + 100.0 * 6.217784881591797
Epoch 2310, val loss: 1.5074375867843628
Epoch 2320, training loss: 621.4928588867188 = 0.020980818197131157 + 100.0 * 6.214718341827393
Epoch 2320, val loss: 1.5105711221694946
Epoch 2330, training loss: 621.42919921875 = 0.020687714219093323 + 100.0 * 6.214085102081299
Epoch 2330, val loss: 1.5137238502502441
Epoch 2340, training loss: 621.6625366210938 = 0.020406480878591537 + 100.0 * 6.216421604156494
Epoch 2340, val loss: 1.5167415142059326
Epoch 2350, training loss: 621.7413940429688 = 0.02012508362531662 + 100.0 * 6.217212677001953
Epoch 2350, val loss: 1.5199017524719238
Epoch 2360, training loss: 621.6294555664062 = 0.01985698565840721 + 100.0 * 6.216095924377441
Epoch 2360, val loss: 1.5241752862930298
Epoch 2370, training loss: 621.9069213867188 = 0.019589241594076157 + 100.0 * 6.218873023986816
Epoch 2370, val loss: 1.5269038677215576
Epoch 2380, training loss: 621.4749755859375 = 0.019305957481265068 + 100.0 * 6.21455717086792
Epoch 2380, val loss: 1.529558777809143
Epoch 2390, training loss: 621.7825927734375 = 0.019045881927013397 + 100.0 * 6.217635631561279
Epoch 2390, val loss: 1.533218264579773
Epoch 2400, training loss: 621.416748046875 = 0.018789466470479965 + 100.0 * 6.213979721069336
Epoch 2400, val loss: 1.5357003211975098
Epoch 2410, training loss: 621.6022338867188 = 0.01853344216942787 + 100.0 * 6.215837001800537
Epoch 2410, val loss: 1.539009690284729
Epoch 2420, training loss: 621.37158203125 = 0.018292147666215897 + 100.0 * 6.2135329246521
Epoch 2420, val loss: 1.5420597791671753
Epoch 2430, training loss: 621.29296875 = 0.018053729087114334 + 100.0 * 6.212749004364014
Epoch 2430, val loss: 1.544817566871643
Epoch 2440, training loss: 621.3661499023438 = 0.017822913825511932 + 100.0 * 6.2134833335876465
Epoch 2440, val loss: 1.5476099252700806
Epoch 2450, training loss: 621.9124755859375 = 0.017597198486328125 + 100.0 * 6.218948841094971
Epoch 2450, val loss: 1.5502760410308838
Epoch 2460, training loss: 621.6383056640625 = 0.017359493300318718 + 100.0 * 6.216209888458252
Epoch 2460, val loss: 1.5537974834442139
Epoch 2470, training loss: 621.323974609375 = 0.017139997333288193 + 100.0 * 6.213068008422852
Epoch 2470, val loss: 1.5562478303909302
Epoch 2480, training loss: 621.37158203125 = 0.01692596636712551 + 100.0 * 6.2135467529296875
Epoch 2480, val loss: 1.5599467754364014
Epoch 2490, training loss: 621.7689208984375 = 0.016720512881875038 + 100.0 * 6.217522144317627
Epoch 2490, val loss: 1.562485933303833
Epoch 2500, training loss: 621.3187866210938 = 0.01649126224219799 + 100.0 * 6.2130231857299805
Epoch 2500, val loss: 1.5642427206039429
Epoch 2510, training loss: 621.1514282226562 = 0.016288060694932938 + 100.0 * 6.21135139465332
Epoch 2510, val loss: 1.567611575126648
Epoch 2520, training loss: 621.5881958007812 = 0.01609848625957966 + 100.0 * 6.2157206535339355
Epoch 2520, val loss: 1.5705093145370483
Epoch 2530, training loss: 621.456787109375 = 0.015892120078206062 + 100.0 * 6.214409351348877
Epoch 2530, val loss: 1.572605013847351
Epoch 2540, training loss: 621.2767944335938 = 0.01569303125143051 + 100.0 * 6.212611198425293
Epoch 2540, val loss: 1.5757752656936646
Epoch 2550, training loss: 621.1137084960938 = 0.015496063977479935 + 100.0 * 6.210982322692871
Epoch 2550, val loss: 1.5775526762008667
Epoch 2560, training loss: 621.0630493164062 = 0.015312235802412033 + 100.0 * 6.210477352142334
Epoch 2560, val loss: 1.5804812908172607
Epoch 2570, training loss: 621.1571655273438 = 0.015134090557694435 + 100.0 * 6.21142053604126
Epoch 2570, val loss: 1.5829253196716309
Epoch 2580, training loss: 621.7352294921875 = 0.014955769293010235 + 100.0 * 6.217202663421631
Epoch 2580, val loss: 1.585326075553894
Epoch 2590, training loss: 621.189208984375 = 0.014780322089791298 + 100.0 * 6.21174430847168
Epoch 2590, val loss: 1.5880775451660156
Epoch 2600, training loss: 621.0881958007812 = 0.014603745192289352 + 100.0 * 6.210736274719238
Epoch 2600, val loss: 1.5907583236694336
Epoch 2610, training loss: 621.2798461914062 = 0.014439287595450878 + 100.0 * 6.212654113769531
Epoch 2610, val loss: 1.5932422876358032
Epoch 2620, training loss: 621.2053833007812 = 0.014267958700656891 + 100.0 * 6.211911201477051
Epoch 2620, val loss: 1.5949347019195557
Epoch 2630, training loss: 621.2213134765625 = 0.014100059866905212 + 100.0 * 6.212072372436523
Epoch 2630, val loss: 1.5976617336273193
Epoch 2640, training loss: 621.1195678710938 = 0.013946842402219772 + 100.0 * 6.211055755615234
Epoch 2640, val loss: 1.6007275581359863
Epoch 2650, training loss: 621.178466796875 = 0.013783297501504421 + 100.0 * 6.211646556854248
Epoch 2650, val loss: 1.6030160188674927
Epoch 2660, training loss: 621.4995727539062 = 0.013639740645885468 + 100.0 * 6.214859485626221
Epoch 2660, val loss: 1.6056928634643555
Epoch 2670, training loss: 621.0189208984375 = 0.013468872755765915 + 100.0 * 6.210054397583008
Epoch 2670, val loss: 1.6070257425308228
Epoch 2680, training loss: 620.9104614257812 = 0.013317367993295193 + 100.0 * 6.2089715003967285
Epoch 2680, val loss: 1.6094940900802612
Epoch 2690, training loss: 620.892578125 = 0.013174539431929588 + 100.0 * 6.208793640136719
Epoch 2690, val loss: 1.6121923923492432
Epoch 2700, training loss: 620.8853759765625 = 0.013033382594585419 + 100.0 * 6.208723068237305
Epoch 2700, val loss: 1.614533543586731
Epoch 2710, training loss: 621.5486450195312 = 0.012902368791401386 + 100.0 * 6.215357780456543
Epoch 2710, val loss: 1.6169549226760864
Epoch 2720, training loss: 621.145263671875 = 0.012752573005855083 + 100.0 * 6.211325168609619
Epoch 2720, val loss: 1.6185786724090576
Epoch 2730, training loss: 621.0702514648438 = 0.012612491846084595 + 100.0 * 6.210576057434082
Epoch 2730, val loss: 1.6213326454162598
Epoch 2740, training loss: 621.22802734375 = 0.012485344894230366 + 100.0 * 6.212154865264893
Epoch 2740, val loss: 1.6234917640686035
Epoch 2750, training loss: 620.9884643554688 = 0.012342428788542747 + 100.0 * 6.209761142730713
Epoch 2750, val loss: 1.6252940893173218
Epoch 2760, training loss: 620.8058471679688 = 0.012210249900817871 + 100.0 * 6.2079362869262695
Epoch 2760, val loss: 1.627344012260437
Epoch 2770, training loss: 620.9396362304688 = 0.012088263407349586 + 100.0 * 6.209275722503662
Epoch 2770, val loss: 1.6298412084579468
Epoch 2780, training loss: 621.20654296875 = 0.01196490228176117 + 100.0 * 6.211945533752441
Epoch 2780, val loss: 1.6317411661148071
Epoch 2790, training loss: 621.038330078125 = 0.01183814276009798 + 100.0 * 6.210265159606934
Epoch 2790, val loss: 1.6332757472991943
Epoch 2800, training loss: 620.8851928710938 = 0.01171195413917303 + 100.0 * 6.208734512329102
Epoch 2800, val loss: 1.6359989643096924
Epoch 2810, training loss: 620.9389038085938 = 0.011590138077735901 + 100.0 * 6.209272861480713
Epoch 2810, val loss: 1.6375514268875122
Epoch 2820, training loss: 621.1901245117188 = 0.011469113640487194 + 100.0 * 6.211786270141602
Epoch 2820, val loss: 1.6388622522354126
Epoch 2830, training loss: 620.9010620117188 = 0.011354292742908001 + 100.0 * 6.208897113800049
Epoch 2830, val loss: 1.6410045623779297
Epoch 2840, training loss: 620.77001953125 = 0.011243599466979504 + 100.0 * 6.207587718963623
Epoch 2840, val loss: 1.6440389156341553
Epoch 2850, training loss: 620.7041015625 = 0.011127377860248089 + 100.0 * 6.206930160522461
Epoch 2850, val loss: 1.645608901977539
Epoch 2860, training loss: 621.4262084960938 = 0.011027004569768906 + 100.0 * 6.214151859283447
Epoch 2860, val loss: 1.6478391885757446
Epoch 2870, training loss: 620.7864379882812 = 0.010912233963608742 + 100.0 * 6.207755088806152
Epoch 2870, val loss: 1.6493421792984009
Epoch 2880, training loss: 620.6582641601562 = 0.01079939492046833 + 100.0 * 6.206474304199219
Epoch 2880, val loss: 1.6508926153182983
Epoch 2890, training loss: 621.0519409179688 = 0.010696971789002419 + 100.0 * 6.210412502288818
Epoch 2890, val loss: 1.6526631116867065
Epoch 2900, training loss: 620.670166015625 = 0.010594865307211876 + 100.0 * 6.206595420837402
Epoch 2900, val loss: 1.6548807621002197
Epoch 2910, training loss: 620.6832885742188 = 0.010493112727999687 + 100.0 * 6.206727981567383
Epoch 2910, val loss: 1.657322883605957
Epoch 2920, training loss: 620.67919921875 = 0.01039330754429102 + 100.0 * 6.206687927246094
Epoch 2920, val loss: 1.6588796377182007
Epoch 2930, training loss: 621.1231689453125 = 0.010303829796612263 + 100.0 * 6.2111287117004395
Epoch 2930, val loss: 1.6609641313552856
Epoch 2940, training loss: 620.7108154296875 = 0.01019875518977642 + 100.0 * 6.207006454467773
Epoch 2940, val loss: 1.6618868112564087
Epoch 2950, training loss: 620.8018188476562 = 0.010104241780936718 + 100.0 * 6.207916736602783
Epoch 2950, val loss: 1.6641700267791748
Epoch 2960, training loss: 620.5393676757812 = 0.010005876421928406 + 100.0 * 6.205293655395508
Epoch 2960, val loss: 1.6655455827713013
Epoch 2970, training loss: 620.6282348632812 = 0.009913366287946701 + 100.0 * 6.206183433532715
Epoch 2970, val loss: 1.6670767068862915
Epoch 2980, training loss: 621.0911865234375 = 0.009825059212744236 + 100.0 * 6.210813522338867
Epoch 2980, val loss: 1.6682534217834473
Epoch 2990, training loss: 620.7783203125 = 0.009734188206493855 + 100.0 * 6.207685947418213
Epoch 2990, val loss: 1.6709305047988892
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 861.6328735351562 = 1.948492169380188 + 100.0 * 8.596843719482422
Epoch 0, val loss: 1.9427331686019897
Epoch 10, training loss: 861.5441284179688 = 1.9397861957550049 + 100.0 * 8.596043586730957
Epoch 10, val loss: 1.934086799621582
Epoch 20, training loss: 860.9446411132812 = 1.9290149211883545 + 100.0 * 8.590156555175781
Epoch 20, val loss: 1.9229449033737183
Epoch 30, training loss: 856.703369140625 = 1.9155172109603882 + 100.0 * 8.54787826538086
Epoch 30, val loss: 1.908697247505188
Epoch 40, training loss: 830.4842529296875 = 1.8993984460830688 + 100.0 * 8.285848617553711
Epoch 40, val loss: 1.89189612865448
Epoch 50, training loss: 763.0284423828125 = 1.881956934928894 + 100.0 * 7.611464977264404
Epoch 50, val loss: 1.8743499517440796
Epoch 60, training loss: 735.3611450195312 = 1.868857979774475 + 100.0 * 7.334922790527344
Epoch 60, val loss: 1.8622757196426392
Epoch 70, training loss: 712.1710205078125 = 1.85732102394104 + 100.0 * 7.103137493133545
Epoch 70, val loss: 1.8511630296707153
Epoch 80, training loss: 694.7990112304688 = 1.8465321063995361 + 100.0 * 6.9295244216918945
Epoch 80, val loss: 1.8414382934570312
Epoch 90, training loss: 683.5286865234375 = 1.83682119846344 + 100.0 * 6.816918849945068
Epoch 90, val loss: 1.83241605758667
Epoch 100, training loss: 676.019287109375 = 1.8277748823165894 + 100.0 * 6.741915225982666
Epoch 100, val loss: 1.824028730392456
Epoch 110, training loss: 670.66796875 = 1.819006085395813 + 100.0 * 6.6884894371032715
Epoch 110, val loss: 1.8158788681030273
Epoch 120, training loss: 666.1215209960938 = 1.8106215000152588 + 100.0 * 6.64310884475708
Epoch 120, val loss: 1.808297872543335
Epoch 130, training loss: 662.2276611328125 = 1.8028560876846313 + 100.0 * 6.604248046875
Epoch 130, val loss: 1.801398754119873
Epoch 140, training loss: 659.2025146484375 = 1.7953437566757202 + 100.0 * 6.574071407318115
Epoch 140, val loss: 1.7947311401367188
Epoch 150, training loss: 656.7703857421875 = 1.7875635623931885 + 100.0 * 6.54982852935791
Epoch 150, val loss: 1.787811040878296
Epoch 160, training loss: 654.5472412109375 = 1.7793782949447632 + 100.0 * 6.527678966522217
Epoch 160, val loss: 1.780704379081726
Epoch 170, training loss: 652.8241577148438 = 1.770900011062622 + 100.0 * 6.510532855987549
Epoch 170, val loss: 1.7733560800552368
Epoch 180, training loss: 651.2155151367188 = 1.7618792057037354 + 100.0 * 6.49453592300415
Epoch 180, val loss: 1.7655510902404785
Epoch 190, training loss: 650.0877075195312 = 1.7522199153900146 + 100.0 * 6.4833550453186035
Epoch 190, val loss: 1.7571991682052612
Epoch 200, training loss: 648.6595458984375 = 1.7416772842407227 + 100.0 * 6.469178199768066
Epoch 200, val loss: 1.7481863498687744
Epoch 210, training loss: 647.5020141601562 = 1.73036527633667 + 100.0 * 6.457716464996338
Epoch 210, val loss: 1.7384898662567139
Epoch 220, training loss: 646.539794921875 = 1.7180519104003906 + 100.0 * 6.448216915130615
Epoch 220, val loss: 1.7279784679412842
Epoch 230, training loss: 645.8414916992188 = 1.7046639919281006 + 100.0 * 6.441368579864502
Epoch 230, val loss: 1.716590166091919
Epoch 240, training loss: 644.7968139648438 = 1.6901652812957764 + 100.0 * 6.431066513061523
Epoch 240, val loss: 1.7042484283447266
Epoch 250, training loss: 643.9425048828125 = 1.6745237112045288 + 100.0 * 6.422679901123047
Epoch 250, val loss: 1.691023826599121
Epoch 260, training loss: 643.3582763671875 = 1.6577521562576294 + 100.0 * 6.4170050621032715
Epoch 260, val loss: 1.676786184310913
Epoch 270, training loss: 642.5950317382812 = 1.6395766735076904 + 100.0 * 6.409554481506348
Epoch 270, val loss: 1.6615227460861206
Epoch 280, training loss: 641.835205078125 = 1.6203495264053345 + 100.0 * 6.402148723602295
Epoch 280, val loss: 1.6453912258148193
Epoch 290, training loss: 641.1497192382812 = 1.6000124216079712 + 100.0 * 6.395496845245361
Epoch 290, val loss: 1.6284669637680054
Epoch 300, training loss: 640.6040649414062 = 1.578657627105713 + 100.0 * 6.390254020690918
Epoch 300, val loss: 1.6107640266418457
Epoch 310, training loss: 640.209228515625 = 1.5562832355499268 + 100.0 * 6.386529445648193
Epoch 310, val loss: 1.592201828956604
Epoch 320, training loss: 639.6036987304688 = 1.5330662727355957 + 100.0 * 6.380706310272217
Epoch 320, val loss: 1.5730788707733154
Epoch 330, training loss: 638.9496459960938 = 1.5090755224227905 + 100.0 * 6.374405384063721
Epoch 330, val loss: 1.5537391901016235
Epoch 340, training loss: 638.306640625 = 1.4846277236938477 + 100.0 * 6.368220329284668
Epoch 340, val loss: 1.534188985824585
Epoch 350, training loss: 637.868408203125 = 1.4597272872924805 + 100.0 * 6.364086627960205
Epoch 350, val loss: 1.5144641399383545
Epoch 360, training loss: 637.5272216796875 = 1.434332251548767 + 100.0 * 6.360929012298584
Epoch 360, val loss: 1.494504451751709
Epoch 370, training loss: 637.0482177734375 = 1.4086956977844238 + 100.0 * 6.356395244598389
Epoch 370, val loss: 1.4746509790420532
Epoch 380, training loss: 636.5787963867188 = 1.3830088376998901 + 100.0 * 6.35195779800415
Epoch 380, val loss: 1.4549620151519775
Epoch 390, training loss: 636.3233642578125 = 1.3572696447372437 + 100.0 * 6.349660396575928
Epoch 390, val loss: 1.4354658126831055
Epoch 400, training loss: 635.8705444335938 = 1.3313928842544556 + 100.0 * 6.345391273498535
Epoch 400, val loss: 1.4161635637283325
Epoch 410, training loss: 635.4953002929688 = 1.305703043937683 + 100.0 * 6.341896057128906
Epoch 410, val loss: 1.3970025777816772
Epoch 420, training loss: 635.2869873046875 = 1.2799973487854004 + 100.0 * 6.3400702476501465
Epoch 420, val loss: 1.378147006034851
Epoch 430, training loss: 634.8619384765625 = 1.2546281814575195 + 100.0 * 6.33607292175293
Epoch 430, val loss: 1.3595751523971558
Epoch 440, training loss: 634.547119140625 = 1.2293322086334229 + 100.0 * 6.3331780433654785
Epoch 440, val loss: 1.3414684534072876
Epoch 450, training loss: 634.2017822265625 = 1.2043273448944092 + 100.0 * 6.329974174499512
Epoch 450, val loss: 1.3236029148101807
Epoch 460, training loss: 634.0593872070312 = 1.1795636415481567 + 100.0 * 6.328798294067383
Epoch 460, val loss: 1.3061301708221436
Epoch 470, training loss: 633.687255859375 = 1.1549137830734253 + 100.0 * 6.325323581695557
Epoch 470, val loss: 1.2889937162399292
Epoch 480, training loss: 633.3922729492188 = 1.130743145942688 + 100.0 * 6.322615146636963
Epoch 480, val loss: 1.2723109722137451
Epoch 490, training loss: 633.4609985351562 = 1.1068966388702393 + 100.0 * 6.323541164398193
Epoch 490, val loss: 1.256247639656067
Epoch 500, training loss: 633.2724609375 = 1.0834757089614868 + 100.0 * 6.321889877319336
Epoch 500, val loss: 1.2402724027633667
Epoch 510, training loss: 632.6624145507812 = 1.0601751804351807 + 100.0 * 6.3160223960876465
Epoch 510, val loss: 1.2248834371566772
Epoch 520, training loss: 632.4348754882812 = 1.0374736785888672 + 100.0 * 6.313973903656006
Epoch 520, val loss: 1.2103151082992554
Epoch 530, training loss: 632.140380859375 = 1.0153119564056396 + 100.0 * 6.311250686645508
Epoch 530, val loss: 1.1962889432907104
Epoch 540, training loss: 632.1549072265625 = 0.9935398697853088 + 100.0 * 6.311614036560059
Epoch 540, val loss: 1.1826508045196533
Epoch 550, training loss: 631.9169921875 = 0.9720639586448669 + 100.0 * 6.309449195861816
Epoch 550, val loss: 1.1695103645324707
Epoch 560, training loss: 631.733642578125 = 0.9510356187820435 + 100.0 * 6.307826042175293
Epoch 560, val loss: 1.1569949388504028
Epoch 570, training loss: 631.7044677734375 = 0.9305035471916199 + 100.0 * 6.307739734649658
Epoch 570, val loss: 1.1449652910232544
Epoch 580, training loss: 631.258544921875 = 0.9104860424995422 + 100.0 * 6.303480625152588
Epoch 580, val loss: 1.1335926055908203
Epoch 590, training loss: 630.9810180664062 = 0.890910804271698 + 100.0 * 6.300900936126709
Epoch 590, val loss: 1.122759222984314
Epoch 600, training loss: 631.00537109375 = 0.8717905879020691 + 100.0 * 6.301336288452148
Epoch 600, val loss: 1.1125414371490479
Epoch 610, training loss: 630.8304443359375 = 0.8529221415519714 + 100.0 * 6.29977560043335
Epoch 610, val loss: 1.1025177240371704
Epoch 620, training loss: 630.4639282226562 = 0.8345236778259277 + 100.0 * 6.29629373550415
Epoch 620, val loss: 1.0931264162063599
Epoch 630, training loss: 630.5263671875 = 0.8165804147720337 + 100.0 * 6.297097682952881
Epoch 630, val loss: 1.0843349695205688
Epoch 640, training loss: 630.3206787109375 = 0.7990033626556396 + 100.0 * 6.2952165603637695
Epoch 640, val loss: 1.0758880376815796
Epoch 650, training loss: 630.0956420898438 = 0.7817660570144653 + 100.0 * 6.2931389808654785
Epoch 650, val loss: 1.067916989326477
Epoch 660, training loss: 629.77392578125 = 0.7650524973869324 + 100.0 * 6.290088653564453
Epoch 660, val loss: 1.060583472251892
Epoch 670, training loss: 629.7050170898438 = 0.7486896514892578 + 100.0 * 6.289562702178955
Epoch 670, val loss: 1.0535944700241089
Epoch 680, training loss: 629.6160278320312 = 0.732538640499115 + 100.0 * 6.288834571838379
Epoch 680, val loss: 1.0471149682998657
Epoch 690, training loss: 629.5051879882812 = 0.7166884541511536 + 100.0 * 6.2878851890563965
Epoch 690, val loss: 1.040575385093689
Epoch 700, training loss: 629.5254516601562 = 0.7011715173721313 + 100.0 * 6.288242816925049
Epoch 700, val loss: 1.0349594354629517
Epoch 710, training loss: 629.2639770507812 = 0.6859632730484009 + 100.0 * 6.28577995300293
Epoch 710, val loss: 1.0294514894485474
Epoch 720, training loss: 628.9664306640625 = 0.6711046695709229 + 100.0 * 6.282953262329102
Epoch 720, val loss: 1.0246347188949585
Epoch 730, training loss: 628.82958984375 = 0.6565725207328796 + 100.0 * 6.2817301750183105
Epoch 730, val loss: 1.0199875831604004
Epoch 740, training loss: 629.0791625976562 = 0.6423496603965759 + 100.0 * 6.284368515014648
Epoch 740, val loss: 1.015986442565918
Epoch 750, training loss: 628.7391357421875 = 0.6283060312271118 + 100.0 * 6.281108379364014
Epoch 750, val loss: 1.011980414390564
Epoch 760, training loss: 628.6609497070312 = 0.614523708820343 + 100.0 * 6.2804646492004395
Epoch 760, val loss: 1.0084789991378784
Epoch 770, training loss: 628.37548828125 = 0.6011068224906921 + 100.0 * 6.277743816375732
Epoch 770, val loss: 1.0053507089614868
Epoch 780, training loss: 628.30419921875 = 0.5879847407341003 + 100.0 * 6.277162075042725
Epoch 780, val loss: 1.002573013305664
Epoch 790, training loss: 628.413330078125 = 0.5751004815101624 + 100.0 * 6.278382301330566
Epoch 790, val loss: 1.000137448310852
Epoch 800, training loss: 628.1817016601562 = 0.5625476837158203 + 100.0 * 6.276191711425781
Epoch 800, val loss: 0.9981335401535034
Epoch 810, training loss: 627.9058837890625 = 0.550149142742157 + 100.0 * 6.273557186126709
Epoch 810, val loss: 0.9962218403816223
Epoch 820, training loss: 628.30126953125 = 0.5381929278373718 + 100.0 * 6.277630805969238
Epoch 820, val loss: 0.9951313138008118
Epoch 830, training loss: 627.8721923828125 = 0.5263044834136963 + 100.0 * 6.273458957672119
Epoch 830, val loss: 0.993446409702301
Epoch 840, training loss: 627.564697265625 = 0.5147941708564758 + 100.0 * 6.270499229431152
Epoch 840, val loss: 0.9929054379463196
Epoch 850, training loss: 627.7640380859375 = 0.5035821795463562 + 100.0 * 6.272604465484619
Epoch 850, val loss: 0.9925409555435181
Epoch 860, training loss: 627.587158203125 = 0.49253639578819275 + 100.0 * 6.270946025848389
Epoch 860, val loss: 0.991935133934021
Epoch 870, training loss: 627.2843627929688 = 0.48170140385627747 + 100.0 * 6.268026828765869
Epoch 870, val loss: 0.9919750094413757
Epoch 880, training loss: 627.6970825195312 = 0.4712042808532715 + 100.0 * 6.272258758544922
Epoch 880, val loss: 0.9924629330635071
Epoch 890, training loss: 627.1585693359375 = 0.4609168767929077 + 100.0 * 6.266976356506348
Epoch 890, val loss: 0.9927477836608887
Epoch 900, training loss: 626.9857788085938 = 0.4508703947067261 + 100.0 * 6.265349388122559
Epoch 900, val loss: 0.9936710000038147
Epoch 910, training loss: 627.445556640625 = 0.44107258319854736 + 100.0 * 6.270044803619385
Epoch 910, val loss: 0.9946796298027039
Epoch 920, training loss: 626.9896240234375 = 0.4314756691455841 + 100.0 * 6.2655816078186035
Epoch 920, val loss: 0.9956088662147522
Epoch 930, training loss: 626.6618041992188 = 0.422075480222702 + 100.0 * 6.262397289276123
Epoch 930, val loss: 0.9972679018974304
Epoch 940, training loss: 626.6956787109375 = 0.41298630833625793 + 100.0 * 6.262827396392822
Epoch 940, val loss: 0.9993176460266113
Epoch 950, training loss: 626.741943359375 = 0.4040311872959137 + 100.0 * 6.263379096984863
Epoch 950, val loss: 1.0007790327072144
Epoch 960, training loss: 626.5155639648438 = 0.3951965868473053 + 100.0 * 6.261203765869141
Epoch 960, val loss: 1.0029327869415283
Epoch 970, training loss: 626.6426391601562 = 0.3867097795009613 + 100.0 * 6.262559413909912
Epoch 970, val loss: 1.005155086517334
Epoch 980, training loss: 626.4556274414062 = 0.3782185912132263 + 100.0 * 6.260773658752441
Epoch 980, val loss: 1.0074743032455444
Epoch 990, training loss: 626.33154296875 = 0.37000012397766113 + 100.0 * 6.259615421295166
Epoch 990, val loss: 1.0101220607757568
Epoch 1000, training loss: 626.7235717773438 = 0.3619101345539093 + 100.0 * 6.26361608505249
Epoch 1000, val loss: 1.0128648281097412
Epoch 1010, training loss: 626.1672973632812 = 0.35404735803604126 + 100.0 * 6.258132457733154
Epoch 1010, val loss: 1.0161705017089844
Epoch 1020, training loss: 625.9524536132812 = 0.34637266397476196 + 100.0 * 6.25606107711792
Epoch 1020, val loss: 1.0193005800247192
Epoch 1030, training loss: 626.0396118164062 = 0.33888357877731323 + 100.0 * 6.257007598876953
Epoch 1030, val loss: 1.0227837562561035
Epoch 1040, training loss: 625.9906005859375 = 0.33147066831588745 + 100.0 * 6.256591320037842
Epoch 1040, val loss: 1.02597975730896
Epoch 1050, training loss: 625.752685546875 = 0.32418569922447205 + 100.0 * 6.2542853355407715
Epoch 1050, val loss: 1.029518723487854
Epoch 1060, training loss: 625.970458984375 = 0.31710296869277954 + 100.0 * 6.256533622741699
Epoch 1060, val loss: 1.0330256223678589
Epoch 1070, training loss: 625.62255859375 = 0.3101612627506256 + 100.0 * 6.253123760223389
Epoch 1070, val loss: 1.036791443824768
Epoch 1080, training loss: 625.611328125 = 0.3034091889858246 + 100.0 * 6.253079414367676
Epoch 1080, val loss: 1.0410711765289307
Epoch 1090, training loss: 625.5213623046875 = 0.2967774569988251 + 100.0 * 6.252246379852295
Epoch 1090, val loss: 1.044995665550232
Epoch 1100, training loss: 626.0800170898438 = 0.2902747690677643 + 100.0 * 6.25789737701416
Epoch 1100, val loss: 1.04899263381958
Epoch 1110, training loss: 625.4923095703125 = 0.28383830189704895 + 100.0 * 6.252085208892822
Epoch 1110, val loss: 1.0527828931808472
Epoch 1120, training loss: 625.2490234375 = 0.2775554060935974 + 100.0 * 6.2497148513793945
Epoch 1120, val loss: 1.0572623014450073
Epoch 1130, training loss: 625.171630859375 = 0.27146637439727783 + 100.0 * 6.249001502990723
Epoch 1130, val loss: 1.0617258548736572
Epoch 1140, training loss: 625.9124145507812 = 0.2655012607574463 + 100.0 * 6.256469249725342
Epoch 1140, val loss: 1.0659977197647095
Epoch 1150, training loss: 625.670654296875 = 0.2595600485801697 + 100.0 * 6.254110813140869
Epoch 1150, val loss: 1.0710372924804688
Epoch 1160, training loss: 625.0928955078125 = 0.25374382734298706 + 100.0 * 6.248391628265381
Epoch 1160, val loss: 1.07521390914917
Epoch 1170, training loss: 625.4228515625 = 0.2480934113264084 + 100.0 * 6.2517476081848145
Epoch 1170, val loss: 1.079591155052185
Epoch 1180, training loss: 624.962158203125 = 0.24259234964847565 + 100.0 * 6.247195720672607
Epoch 1180, val loss: 1.084791660308838
Epoch 1190, training loss: 624.8636474609375 = 0.23721951246261597 + 100.0 * 6.2462639808654785
Epoch 1190, val loss: 1.089680552482605
Epoch 1200, training loss: 624.7759399414062 = 0.23197738826274872 + 100.0 * 6.245439529418945
Epoch 1200, val loss: 1.094347596168518
Epoch 1210, training loss: 625.4344482421875 = 0.22687473893165588 + 100.0 * 6.252075672149658
Epoch 1210, val loss: 1.0991474390029907
Epoch 1220, training loss: 625.1884155273438 = 0.22170408070087433 + 100.0 * 6.249667167663574
Epoch 1220, val loss: 1.1039987802505493
Epoch 1230, training loss: 624.6803588867188 = 0.21672114729881287 + 100.0 * 6.244636535644531
Epoch 1230, val loss: 1.108741283416748
Epoch 1240, training loss: 624.5645141601562 = 0.2119111567735672 + 100.0 * 6.243525981903076
Epoch 1240, val loss: 1.1142078638076782
Epoch 1250, training loss: 624.5304565429688 = 0.20722992718219757 + 100.0 * 6.243232250213623
Epoch 1250, val loss: 1.119284987449646
Epoch 1260, training loss: 624.925537109375 = 0.20266848802566528 + 100.0 * 6.247228622436523
Epoch 1260, val loss: 1.1241652965545654
Epoch 1270, training loss: 624.5015258789062 = 0.1981292963027954 + 100.0 * 6.2430338859558105
Epoch 1270, val loss: 1.1294986009597778
Epoch 1280, training loss: 624.66748046875 = 0.19370900094509125 + 100.0 * 6.24473762512207
Epoch 1280, val loss: 1.1340680122375488
Epoch 1290, training loss: 624.7975463867188 = 0.18936026096343994 + 100.0 * 6.246082305908203
Epoch 1290, val loss: 1.1398464441299438
Epoch 1300, training loss: 624.3756103515625 = 0.18513807654380798 + 100.0 * 6.2419047355651855
Epoch 1300, val loss: 1.1446106433868408
Epoch 1310, training loss: 624.2150268554688 = 0.18104000389575958 + 100.0 * 6.240340232849121
Epoch 1310, val loss: 1.1504712104797363
Epoch 1320, training loss: 624.1220092773438 = 0.17708374559879303 + 100.0 * 6.239449501037598
Epoch 1320, val loss: 1.1557316780090332
Epoch 1330, training loss: 624.0892944335938 = 0.17321623861789703 + 100.0 * 6.239161014556885
Epoch 1330, val loss: 1.1613091230392456
Epoch 1340, training loss: 625.0315551757812 = 0.16943047940731049 + 100.0 * 6.248620986938477
Epoch 1340, val loss: 1.1664423942565918
Epoch 1350, training loss: 624.6668701171875 = 0.1656481772661209 + 100.0 * 6.245012283325195
Epoch 1350, val loss: 1.1722161769866943
Epoch 1360, training loss: 623.9826049804688 = 0.1619577556848526 + 100.0 * 6.238205909729004
Epoch 1360, val loss: 1.1773401498794556
Epoch 1370, training loss: 623.9583740234375 = 0.15841346979141235 + 100.0 * 6.23799991607666
Epoch 1370, val loss: 1.1827322244644165
Epoch 1380, training loss: 623.9379272460938 = 0.15498526394367218 + 100.0 * 6.237829685211182
Epoch 1380, val loss: 1.1884928941726685
Epoch 1390, training loss: 624.262451171875 = 0.1516283005475998 + 100.0 * 6.241108417510986
Epoch 1390, val loss: 1.1934995651245117
Epoch 1400, training loss: 624.0119018554688 = 0.1483316421508789 + 100.0 * 6.238636016845703
Epoch 1400, val loss: 1.1993874311447144
Epoch 1410, training loss: 623.8870849609375 = 0.1450725793838501 + 100.0 * 6.237420082092285
Epoch 1410, val loss: 1.2049047946929932
Epoch 1420, training loss: 623.8028564453125 = 0.1419028788805008 + 100.0 * 6.23660945892334
Epoch 1420, val loss: 1.2105655670166016
Epoch 1430, training loss: 623.7715454101562 = 0.13884088397026062 + 100.0 * 6.236326694488525
Epoch 1430, val loss: 1.215986967086792
Epoch 1440, training loss: 623.64404296875 = 0.13587448000907898 + 100.0 * 6.235081672668457
Epoch 1440, val loss: 1.2219631671905518
Epoch 1450, training loss: 623.73193359375 = 0.13298200070858002 + 100.0 * 6.235989093780518
Epoch 1450, val loss: 1.2275718450546265
Epoch 1460, training loss: 623.9186401367188 = 0.1301310956478119 + 100.0 * 6.237884998321533
Epoch 1460, val loss: 1.232893943786621
Epoch 1470, training loss: 623.6704711914062 = 0.12736573815345764 + 100.0 * 6.235430717468262
Epoch 1470, val loss: 1.2386914491653442
Epoch 1480, training loss: 623.4766235351562 = 0.1246480718255043 + 100.0 * 6.233520030975342
Epoch 1480, val loss: 1.2445385456085205
Epoch 1490, training loss: 623.8826904296875 = 0.12204892188310623 + 100.0 * 6.237606048583984
Epoch 1490, val loss: 1.2505133152008057
Epoch 1500, training loss: 623.5343017578125 = 0.119419164955616 + 100.0 * 6.2341485023498535
Epoch 1500, val loss: 1.2551697492599487
Epoch 1510, training loss: 623.3614501953125 = 0.11686961352825165 + 100.0 * 6.23244571685791
Epoch 1510, val loss: 1.2611058950424194
Epoch 1520, training loss: 623.2982177734375 = 0.11443228274583817 + 100.0 * 6.231837749481201
Epoch 1520, val loss: 1.2666031122207642
Epoch 1530, training loss: 623.2383422851562 = 0.11207333952188492 + 100.0 * 6.231262683868408
Epoch 1530, val loss: 1.2723006010055542
Epoch 1540, training loss: 623.7304077148438 = 0.10979163646697998 + 100.0 * 6.2362060546875
Epoch 1540, val loss: 1.2778385877609253
Epoch 1550, training loss: 623.7881469726562 = 0.10747350752353668 + 100.0 * 6.236806869506836
Epoch 1550, val loss: 1.2836146354675293
Epoch 1560, training loss: 623.3549194335938 = 0.10520849376916885 + 100.0 * 6.232496738433838
Epoch 1560, val loss: 1.2887629270553589
Epoch 1570, training loss: 623.1068725585938 = 0.10304372757673264 + 100.0 * 6.230038642883301
Epoch 1570, val loss: 1.2945770025253296
Epoch 1580, training loss: 623.0824584960938 = 0.10095863044261932 + 100.0 * 6.2298150062561035
Epoch 1580, val loss: 1.3004008531570435
Epoch 1590, training loss: 623.2042236328125 = 0.09894608706235886 + 100.0 * 6.231052875518799
Epoch 1590, val loss: 1.3061041831970215
Epoch 1600, training loss: 623.3117065429688 = 0.09692294895648956 + 100.0 * 6.232147693634033
Epoch 1600, val loss: 1.3115533590316772
Epoch 1610, training loss: 623.0051879882812 = 0.09489127993583679 + 100.0 * 6.229102611541748
Epoch 1610, val loss: 1.316983938217163
Epoch 1620, training loss: 623.057861328125 = 0.09296377003192902 + 100.0 * 6.229649066925049
Epoch 1620, val loss: 1.3226817846298218
Epoch 1630, training loss: 623.623779296875 = 0.09114464372396469 + 100.0 * 6.235326766967773
Epoch 1630, val loss: 1.3282108306884766
Epoch 1640, training loss: 623.0498046875 = 0.08927025645971298 + 100.0 * 6.229605197906494
Epoch 1640, val loss: 1.333756923675537
Epoch 1650, training loss: 622.999755859375 = 0.08747037500143051 + 100.0 * 6.229122638702393
Epoch 1650, val loss: 1.3392983675003052
Epoch 1660, training loss: 622.857421875 = 0.08575040102005005 + 100.0 * 6.22771692276001
Epoch 1660, val loss: 1.3449761867523193
Epoch 1670, training loss: 622.8646240234375 = 0.08407973498106003 + 100.0 * 6.227805137634277
Epoch 1670, val loss: 1.3503401279449463
Epoch 1680, training loss: 623.6608276367188 = 0.08243356645107269 + 100.0 * 6.235783576965332
Epoch 1680, val loss: 1.3552939891815186
Epoch 1690, training loss: 622.9212646484375 = 0.0807829275727272 + 100.0 * 6.228404521942139
Epoch 1690, val loss: 1.3615533113479614
Epoch 1700, training loss: 622.7005615234375 = 0.07919594645500183 + 100.0 * 6.226213455200195
Epoch 1700, val loss: 1.3669366836547852
Epoch 1710, training loss: 622.7360229492188 = 0.0776754841208458 + 100.0 * 6.226583480834961
Epoch 1710, val loss: 1.3722589015960693
Epoch 1720, training loss: 623.433349609375 = 0.07617857307195663 + 100.0 * 6.233572006225586
Epoch 1720, val loss: 1.3777743577957153
Epoch 1730, training loss: 623.5769653320312 = 0.07470531016588211 + 100.0 * 6.23502254486084
Epoch 1730, val loss: 1.3828787803649902
Epoch 1740, training loss: 622.6983032226562 = 0.07321441918611526 + 100.0 * 6.226250648498535
Epoch 1740, val loss: 1.3882694244384766
Epoch 1750, training loss: 622.6130981445312 = 0.07181151211261749 + 100.0 * 6.225412845611572
Epoch 1750, val loss: 1.3938653469085693
Epoch 1760, training loss: 622.5272216796875 = 0.07046722620725632 + 100.0 * 6.224567413330078
Epoch 1760, val loss: 1.3993436098098755
Epoch 1770, training loss: 622.5111083984375 = 0.06915441155433655 + 100.0 * 6.224419593811035
Epoch 1770, val loss: 1.4047489166259766
Epoch 1780, training loss: 623.2740478515625 = 0.06788546591997147 + 100.0 * 6.232061862945557
Epoch 1780, val loss: 1.410079002380371
Epoch 1790, training loss: 622.813232421875 = 0.06656347960233688 + 100.0 * 6.227466583251953
Epoch 1790, val loss: 1.4145840406417847
Epoch 1800, training loss: 622.5330200195312 = 0.06531280279159546 + 100.0 * 6.224677085876465
Epoch 1800, val loss: 1.4203733205795288
Epoch 1810, training loss: 622.4948120117188 = 0.06410165876150131 + 100.0 * 6.224307537078857
Epoch 1810, val loss: 1.4254182577133179
Epoch 1820, training loss: 623.4623413085938 = 0.06293007731437683 + 100.0 * 6.233994007110596
Epoch 1820, val loss: 1.4300962686538696
Epoch 1830, training loss: 622.6321411132812 = 0.06171650066971779 + 100.0 * 6.225704193115234
Epoch 1830, val loss: 1.4357157945632935
Epoch 1840, training loss: 622.3436889648438 = 0.06057840213179588 + 100.0 * 6.222830772399902
Epoch 1840, val loss: 1.4406026601791382
Epoch 1850, training loss: 622.30419921875 = 0.05948491021990776 + 100.0 * 6.222446918487549
Epoch 1850, val loss: 1.4459681510925293
Epoch 1860, training loss: 622.80517578125 = 0.058425333350896835 + 100.0 * 6.2274675369262695
Epoch 1860, val loss: 1.4502935409545898
Epoch 1870, training loss: 622.3767700195312 = 0.0573519729077816 + 100.0 * 6.223194122314453
Epoch 1870, val loss: 1.4561233520507812
Epoch 1880, training loss: 622.5272827148438 = 0.05630746856331825 + 100.0 * 6.224709510803223
Epoch 1880, val loss: 1.4605821371078491
Epoch 1890, training loss: 622.2926635742188 = 0.055288203060626984 + 100.0 * 6.2223734855651855
Epoch 1890, val loss: 1.466024398803711
Epoch 1900, training loss: 622.2077026367188 = 0.05430305004119873 + 100.0 * 6.22153377532959
Epoch 1900, val loss: 1.4706971645355225
Epoch 1910, training loss: 622.1563720703125 = 0.053355272859334946 + 100.0 * 6.221030235290527
Epoch 1910, val loss: 1.4760620594024658
Epoch 1920, training loss: 622.6815185546875 = 0.05244828015565872 + 100.0 * 6.226291179656982
Epoch 1920, val loss: 1.4809718132019043
Epoch 1930, training loss: 622.218994140625 = 0.0514809675514698 + 100.0 * 6.221675395965576
Epoch 1930, val loss: 1.4853930473327637
Epoch 1940, training loss: 622.4389038085938 = 0.05057413876056671 + 100.0 * 6.223883152008057
Epoch 1940, val loss: 1.489970326423645
Epoch 1950, training loss: 622.1822509765625 = 0.049673568457365036 + 100.0 * 6.221325397491455
Epoch 1950, val loss: 1.495088815689087
Epoch 1960, training loss: 622.1796875 = 0.048828814178705215 + 100.0 * 6.221308708190918
Epoch 1960, val loss: 1.4999579191207886
Epoch 1970, training loss: 622.3213500976562 = 0.04799670726060867 + 100.0 * 6.222733974456787
Epoch 1970, val loss: 1.5047650337219238
Epoch 1980, training loss: 622.2234497070312 = 0.04716657102108002 + 100.0 * 6.221762657165527
Epoch 1980, val loss: 1.5090296268463135
Epoch 1990, training loss: 622.2919921875 = 0.046349380165338516 + 100.0 * 6.222456932067871
Epoch 1990, val loss: 1.5138188600540161
Epoch 2000, training loss: 622.0069580078125 = 0.04555414989590645 + 100.0 * 6.219614028930664
Epoch 2000, val loss: 1.5187627077102661
Epoch 2010, training loss: 622.0084838867188 = 0.044788677245378494 + 100.0 * 6.219636917114258
Epoch 2010, val loss: 1.523533582687378
Epoch 2020, training loss: 622.3426513671875 = 0.04404130578041077 + 100.0 * 6.222985744476318
Epoch 2020, val loss: 1.5276533365249634
Epoch 2030, training loss: 621.9864501953125 = 0.04330897331237793 + 100.0 * 6.219431400299072
Epoch 2030, val loss: 1.5330779552459717
Epoch 2040, training loss: 622.2467651367188 = 0.04259990528225899 + 100.0 * 6.222041606903076
Epoch 2040, val loss: 1.5377061367034912
Epoch 2050, training loss: 622.1290893554688 = 0.04187682643532753 + 100.0 * 6.220871925354004
Epoch 2050, val loss: 1.541786789894104
Epoch 2060, training loss: 621.9509887695312 = 0.041185978800058365 + 100.0 * 6.21909761428833
Epoch 2060, val loss: 1.5462524890899658
Epoch 2070, training loss: 621.9031982421875 = 0.04051988571882248 + 100.0 * 6.218626499176025
Epoch 2070, val loss: 1.5509960651397705
Epoch 2080, training loss: 622.1785278320312 = 0.039864346385002136 + 100.0 * 6.221386909484863
Epoch 2080, val loss: 1.5547986030578613
Epoch 2090, training loss: 621.8834228515625 = 0.039213694632053375 + 100.0 * 6.218442440032959
Epoch 2090, val loss: 1.5596998929977417
Epoch 2100, training loss: 622.1114501953125 = 0.038594186305999756 + 100.0 * 6.220728874206543
Epoch 2100, val loss: 1.5642359256744385
Epoch 2110, training loss: 621.83251953125 = 0.037965327501297 + 100.0 * 6.217945575714111
Epoch 2110, val loss: 1.5682467222213745
Epoch 2120, training loss: 621.9661254882812 = 0.037362098693847656 + 100.0 * 6.219287872314453
Epoch 2120, val loss: 1.5728415250778198
Epoch 2130, training loss: 621.6552124023438 = 0.03676385059952736 + 100.0 * 6.216184616088867
Epoch 2130, val loss: 1.5771260261535645
Epoch 2140, training loss: 621.7418212890625 = 0.0361928716301918 + 100.0 * 6.2170562744140625
Epoch 2140, val loss: 1.5815529823303223
Epoch 2150, training loss: 622.748291015625 = 0.03564625605940819 + 100.0 * 6.227126598358154
Epoch 2150, val loss: 1.5861988067626953
Epoch 2160, training loss: 621.9888916015625 = 0.03506135568022728 + 100.0 * 6.219538688659668
Epoch 2160, val loss: 1.5895180702209473
Epoch 2170, training loss: 621.73583984375 = 0.03450973704457283 + 100.0 * 6.217013359069824
Epoch 2170, val loss: 1.5940511226654053
Epoch 2180, training loss: 621.6429443359375 = 0.03398602083325386 + 100.0 * 6.216089725494385
Epoch 2180, val loss: 1.598368525505066
Epoch 2190, training loss: 621.7445678710938 = 0.03347613289952278 + 100.0 * 6.217111110687256
Epoch 2190, val loss: 1.6024810075759888
Epoch 2200, training loss: 621.708984375 = 0.032968487590551376 + 100.0 * 6.216760158538818
Epoch 2200, val loss: 1.606459617614746
Epoch 2210, training loss: 622.0072631835938 = 0.03247092291712761 + 100.0 * 6.219748020172119
Epoch 2210, val loss: 1.6098346710205078
Epoch 2220, training loss: 621.7050170898438 = 0.031968701630830765 + 100.0 * 6.21673059463501
Epoch 2220, val loss: 1.6143467426300049
Epoch 2230, training loss: 621.4874267578125 = 0.03148540109395981 + 100.0 * 6.214559555053711
Epoch 2230, val loss: 1.618348479270935
Epoch 2240, training loss: 621.4627685546875 = 0.031022705137729645 + 100.0 * 6.214317321777344
Epoch 2240, val loss: 1.622897744178772
Epoch 2250, training loss: 621.548583984375 = 0.030575355514883995 + 100.0 * 6.215179920196533
Epoch 2250, val loss: 1.6266719102859497
Epoch 2260, training loss: 621.8809204101562 = 0.030131259933114052 + 100.0 * 6.218507766723633
Epoch 2260, val loss: 1.6304185390472412
Epoch 2270, training loss: 621.79345703125 = 0.029694385826587677 + 100.0 * 6.217637538909912
Epoch 2270, val loss: 1.6343282461166382
Epoch 2280, training loss: 621.7037353515625 = 0.029256537556648254 + 100.0 * 6.216744899749756
Epoch 2280, val loss: 1.6387420892715454
Epoch 2290, training loss: 621.497802734375 = 0.02883622609078884 + 100.0 * 6.214690208435059
Epoch 2290, val loss: 1.6425584554672241
Epoch 2300, training loss: 621.4717407226562 = 0.02842520736157894 + 100.0 * 6.214432716369629
Epoch 2300, val loss: 1.6463682651519775
Epoch 2310, training loss: 621.728759765625 = 0.028030160814523697 + 100.0 * 6.217007160186768
Epoch 2310, val loss: 1.6505062580108643
Epoch 2320, training loss: 621.91064453125 = 0.027614647522568703 + 100.0 * 6.218830585479736
Epoch 2320, val loss: 1.6537660360336304
Epoch 2330, training loss: 621.4361572265625 = 0.02721627987921238 + 100.0 * 6.214089393615723
Epoch 2330, val loss: 1.6573702096939087
Epoch 2340, training loss: 621.3212890625 = 0.02683904580771923 + 100.0 * 6.212944030761719
Epoch 2340, val loss: 1.6616512537002563
Epoch 2350, training loss: 621.2512817382812 = 0.02646915428340435 + 100.0 * 6.212247848510742
Epoch 2350, val loss: 1.6651556491851807
Epoch 2360, training loss: 621.3235473632812 = 0.026115864515304565 + 100.0 * 6.2129740715026855
Epoch 2360, val loss: 1.6690000295639038
Epoch 2370, training loss: 622.2559204101562 = 0.025765081867575645 + 100.0 * 6.222301483154297
Epoch 2370, val loss: 1.6725127696990967
Epoch 2380, training loss: 621.592041015625 = 0.02540237456560135 + 100.0 * 6.215666770935059
Epoch 2380, val loss: 1.6762152910232544
Epoch 2390, training loss: 621.2691040039062 = 0.025042209774255753 + 100.0 * 6.2124409675598145
Epoch 2390, val loss: 1.680123209953308
Epoch 2400, training loss: 621.4456176757812 = 0.024714477360248566 + 100.0 * 6.214209079742432
Epoch 2400, val loss: 1.6841715574264526
Epoch 2410, training loss: 621.3157958984375 = 0.024379190057516098 + 100.0 * 6.21291446685791
Epoch 2410, val loss: 1.6873600482940674
Epoch 2420, training loss: 621.4461059570312 = 0.024049730971455574 + 100.0 * 6.2142205238342285
Epoch 2420, val loss: 1.6907280683517456
Epoch 2430, training loss: 621.6585693359375 = 0.023732103407382965 + 100.0 * 6.216348171234131
Epoch 2430, val loss: 1.6944786310195923
Epoch 2440, training loss: 621.3375854492188 = 0.02341499738395214 + 100.0 * 6.213141441345215
Epoch 2440, val loss: 1.6977676153182983
Epoch 2450, training loss: 621.2672119140625 = 0.02310917153954506 + 100.0 * 6.2124409675598145
Epoch 2450, val loss: 1.7015817165374756
Epoch 2460, training loss: 621.1731567382812 = 0.022808371111750603 + 100.0 * 6.211503505706787
Epoch 2460, val loss: 1.7050955295562744
Epoch 2470, training loss: 621.14990234375 = 0.022522127255797386 + 100.0 * 6.211273670196533
Epoch 2470, val loss: 1.7088654041290283
Epoch 2480, training loss: 621.5985717773438 = 0.022250592708587646 + 100.0 * 6.215763092041016
Epoch 2480, val loss: 1.7126035690307617
Epoch 2490, training loss: 621.2994995117188 = 0.021946541965007782 + 100.0 * 6.212775230407715
Epoch 2490, val loss: 1.7148782014846802
Epoch 2500, training loss: 621.212646484375 = 0.02166176587343216 + 100.0 * 6.211909770965576
Epoch 2500, val loss: 1.7190208435058594
Epoch 2510, training loss: 621.0598754882812 = 0.02138521336019039 + 100.0 * 6.210384845733643
Epoch 2510, val loss: 1.7220840454101562
Epoch 2520, training loss: 621.3121337890625 = 0.02112589403986931 + 100.0 * 6.212910175323486
Epoch 2520, val loss: 1.725515604019165
Epoch 2530, training loss: 621.0791625976562 = 0.020857514813542366 + 100.0 * 6.210582733154297
Epoch 2530, val loss: 1.728569746017456
Epoch 2540, training loss: 621.5646362304688 = 0.020605450496077538 + 100.0 * 6.215440273284912
Epoch 2540, val loss: 1.7315012216567993
Epoch 2550, training loss: 621.1527099609375 = 0.020345648750662804 + 100.0 * 6.2113237380981445
Epoch 2550, val loss: 1.735686182975769
Epoch 2560, training loss: 621.0213623046875 = 0.020091691985726357 + 100.0 * 6.210012912750244
Epoch 2560, val loss: 1.7387421131134033
Epoch 2570, training loss: 621.08642578125 = 0.01985376514494419 + 100.0 * 6.210666179656982
Epoch 2570, val loss: 1.7421910762786865
Epoch 2580, training loss: 621.266357421875 = 0.019622016698122025 + 100.0 * 6.212467670440674
Epoch 2580, val loss: 1.7455557584762573
Epoch 2590, training loss: 621.0440673828125 = 0.019383644685149193 + 100.0 * 6.210246562957764
Epoch 2590, val loss: 1.748225212097168
Epoch 2600, training loss: 621.00927734375 = 0.01915285736322403 + 100.0 * 6.209900856018066
Epoch 2600, val loss: 1.7514418363571167
Epoch 2610, training loss: 621.0725708007812 = 0.01892874762415886 + 100.0 * 6.210536479949951
Epoch 2610, val loss: 1.7548574209213257
Epoch 2620, training loss: 621.5455932617188 = 0.018706485629081726 + 100.0 * 6.215269088745117
Epoch 2620, val loss: 1.7574419975280762
Epoch 2630, training loss: 621.113525390625 = 0.01847485639154911 + 100.0 * 6.2109503746032715
Epoch 2630, val loss: 1.7608919143676758
Epoch 2640, training loss: 620.8797607421875 = 0.018254876136779785 + 100.0 * 6.208614826202393
Epoch 2640, val loss: 1.7636229991912842
Epoch 2650, training loss: 620.7994384765625 = 0.01805039681494236 + 100.0 * 6.207813739776611
Epoch 2650, val loss: 1.7672408819198608
Epoch 2660, training loss: 620.8278198242188 = 0.01785269007086754 + 100.0 * 6.208099842071533
Epoch 2660, val loss: 1.7702945470809937
Epoch 2670, training loss: 621.8192749023438 = 0.017669353634119034 + 100.0 * 6.218016147613525
Epoch 2670, val loss: 1.7732149362564087
Epoch 2680, training loss: 621.2889404296875 = 0.01744469441473484 + 100.0 * 6.212714672088623
Epoch 2680, val loss: 1.7756563425064087
Epoch 2690, training loss: 620.9380493164062 = 0.017241371795535088 + 100.0 * 6.2092084884643555
Epoch 2690, val loss: 1.7785297632217407
Epoch 2700, training loss: 621.0083618164062 = 0.0170453991740942 + 100.0 * 6.20991325378418
Epoch 2700, val loss: 1.7816585302352905
Epoch 2710, training loss: 621.362060546875 = 0.016859764233231544 + 100.0 * 6.213452339172363
Epoch 2710, val loss: 1.7842603921890259
Epoch 2720, training loss: 620.9999389648438 = 0.016668956726789474 + 100.0 * 6.209832668304443
Epoch 2720, val loss: 1.787307620048523
Epoch 2730, training loss: 620.8302001953125 = 0.016480086371302605 + 100.0 * 6.208137512207031
Epoch 2730, val loss: 1.7902252674102783
Epoch 2740, training loss: 620.7863159179688 = 0.01630924828350544 + 100.0 * 6.207700252532959
Epoch 2740, val loss: 1.7934643030166626
Epoch 2750, training loss: 621.0140380859375 = 0.01613641157746315 + 100.0 * 6.209979057312012
Epoch 2750, val loss: 1.795886754989624
Epoch 2760, training loss: 620.7532348632812 = 0.01595711149275303 + 100.0 * 6.207373142242432
Epoch 2760, val loss: 1.7988561391830444
Epoch 2770, training loss: 621.0503540039062 = 0.015787487849593163 + 100.0 * 6.21034574508667
Epoch 2770, val loss: 1.8018592596054077
Epoch 2780, training loss: 620.8156127929688 = 0.01561727561056614 + 100.0 * 6.2079997062683105
Epoch 2780, val loss: 1.8040733337402344
Epoch 2790, training loss: 621.025146484375 = 0.015455813147127628 + 100.0 * 6.210097312927246
Epoch 2790, val loss: 1.8071950674057007
Epoch 2800, training loss: 620.6782836914062 = 0.015285775996744633 + 100.0 * 6.206630229949951
Epoch 2800, val loss: 1.810142159461975
Epoch 2810, training loss: 620.6215209960938 = 0.015129607170820236 + 100.0 * 6.206063747406006
Epoch 2810, val loss: 1.8128938674926758
Epoch 2820, training loss: 620.721923828125 = 0.014980118721723557 + 100.0 * 6.2070698738098145
Epoch 2820, val loss: 1.815746784210205
Epoch 2830, training loss: 621.428955078125 = 0.014832873828709126 + 100.0 * 6.214141368865967
Epoch 2830, val loss: 1.8184610605239868
Epoch 2840, training loss: 620.9469604492188 = 0.014662657864391804 + 100.0 * 6.209323406219482
Epoch 2840, val loss: 1.8198416233062744
Epoch 2850, training loss: 620.7213745117188 = 0.014508001506328583 + 100.0 * 6.20706844329834
Epoch 2850, val loss: 1.823155164718628
Epoch 2860, training loss: 620.6310424804688 = 0.014359665103256702 + 100.0 * 6.206167221069336
Epoch 2860, val loss: 1.8258469104766846
Epoch 2870, training loss: 620.7801513671875 = 0.014220868237316608 + 100.0 * 6.2076592445373535
Epoch 2870, val loss: 1.8282612562179565
Epoch 2880, training loss: 620.9166870117188 = 0.014079095795750618 + 100.0 * 6.209025859832764
Epoch 2880, val loss: 1.8309181928634644
Epoch 2890, training loss: 621.05615234375 = 0.013943897560238838 + 100.0 * 6.210422515869141
Epoch 2890, val loss: 1.8338148593902588
Epoch 2900, training loss: 620.7463989257812 = 0.01379268616437912 + 100.0 * 6.2073259353637695
Epoch 2900, val loss: 1.835849404335022
Epoch 2910, training loss: 620.6041259765625 = 0.013659590855240822 + 100.0 * 6.205904483795166
Epoch 2910, val loss: 1.8386118412017822
Epoch 2920, training loss: 620.48876953125 = 0.013525220565497875 + 100.0 * 6.204751968383789
Epoch 2920, val loss: 1.8408160209655762
Epoch 2930, training loss: 620.7119750976562 = 0.013399293646216393 + 100.0 * 6.2069854736328125
Epoch 2930, val loss: 1.8431271314620972
Epoch 2940, training loss: 620.8861083984375 = 0.013270554132759571 + 100.0 * 6.208728790283203
Epoch 2940, val loss: 1.8452403545379639
Epoch 2950, training loss: 620.9059448242188 = 0.01313837245106697 + 100.0 * 6.208928108215332
Epoch 2950, val loss: 1.8477915525436401
Epoch 2960, training loss: 620.5452270507812 = 0.013005007058382034 + 100.0 * 6.205322265625
Epoch 2960, val loss: 1.8499778509140015
Epoch 2970, training loss: 620.5069580078125 = 0.012882224284112453 + 100.0 * 6.2049407958984375
Epoch 2970, val loss: 1.8526644706726074
Epoch 2980, training loss: 620.7323608398438 = 0.012762460857629776 + 100.0 * 6.20719575881958
Epoch 2980, val loss: 1.8547134399414062
Epoch 2990, training loss: 620.4915771484375 = 0.012644710950553417 + 100.0 * 6.204789638519287
Epoch 2990, val loss: 1.8572721481323242
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8033737480231946
The final CL Acc:0.69136, 0.00349, The final GNN Acc:0.80759, 0.00424
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13148])
remove edge: torch.Size([2, 7852])
updated graph: torch.Size([2, 10444])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.63623046875 = 1.9537297487258911 + 100.0 * 8.596824645996094
Epoch 0, val loss: 1.9483107328414917
Epoch 10, training loss: 861.5322875976562 = 1.944854497909546 + 100.0 * 8.595874786376953
Epoch 10, val loss: 1.938826084136963
Epoch 20, training loss: 860.8168334960938 = 1.9336711168289185 + 100.0 * 8.588831901550293
Epoch 20, val loss: 1.9269696474075317
Epoch 30, training loss: 856.2604370117188 = 1.9188467264175415 + 100.0 * 8.543416023254395
Epoch 30, val loss: 1.9111361503601074
Epoch 40, training loss: 832.6455078125 = 1.900925874710083 + 100.0 * 8.307445526123047
Epoch 40, val loss: 1.892638921737671
Epoch 50, training loss: 769.3109130859375 = 1.8803331851959229 + 100.0 * 7.6743059158325195
Epoch 50, val loss: 1.8721718788146973
Epoch 60, training loss: 743.0928955078125 = 1.8637570142745972 + 100.0 * 7.412291526794434
Epoch 60, val loss: 1.857424020767212
Epoch 70, training loss: 718.8225708007812 = 1.8520549535751343 + 100.0 * 7.169705390930176
Epoch 70, val loss: 1.847120761871338
Epoch 80, training loss: 700.4240112304688 = 1.839719533920288 + 100.0 * 6.985843181610107
Epoch 80, val loss: 1.8358031511306763
Epoch 90, training loss: 686.9468994140625 = 1.8284976482391357 + 100.0 * 6.851184368133545
Epoch 90, val loss: 1.8258317708969116
Epoch 100, training loss: 677.580810546875 = 1.8181684017181396 + 100.0 * 6.757626056671143
Epoch 100, val loss: 1.8161982297897339
Epoch 110, training loss: 672.212646484375 = 1.808274745941162 + 100.0 * 6.704043865203857
Epoch 110, val loss: 1.8067227602005005
Epoch 120, training loss: 668.020751953125 = 1.7985730171203613 + 100.0 * 6.662221908569336
Epoch 120, val loss: 1.797625184059143
Epoch 130, training loss: 664.60791015625 = 1.7890819311141968 + 100.0 * 6.628188610076904
Epoch 130, val loss: 1.7888152599334717
Epoch 140, training loss: 661.1754150390625 = 1.7799525260925293 + 100.0 * 6.593954563140869
Epoch 140, val loss: 1.7805018424987793
Epoch 150, training loss: 658.3165283203125 = 1.770826816558838 + 100.0 * 6.565456867218018
Epoch 150, val loss: 1.7722373008728027
Epoch 160, training loss: 655.653564453125 = 1.7611896991729736 + 100.0 * 6.538923740386963
Epoch 160, val loss: 1.7633652687072754
Epoch 170, training loss: 653.319091796875 = 1.750889539718628 + 100.0 * 6.515681743621826
Epoch 170, val loss: 1.753980278968811
Epoch 180, training loss: 651.468505859375 = 1.7397117614746094 + 100.0 * 6.497288227081299
Epoch 180, val loss: 1.7439234256744385
Epoch 190, training loss: 649.9994506835938 = 1.7274609804153442 + 100.0 * 6.482719421386719
Epoch 190, val loss: 1.7328094244003296
Epoch 200, training loss: 648.598388671875 = 1.714070200920105 + 100.0 * 6.468843460083008
Epoch 200, val loss: 1.720860242843628
Epoch 210, training loss: 647.502685546875 = 1.6995985507965088 + 100.0 * 6.458030700683594
Epoch 210, val loss: 1.7079311609268188
Epoch 220, training loss: 646.7022094726562 = 1.683889627456665 + 100.0 * 6.450182914733887
Epoch 220, val loss: 1.6938279867172241
Epoch 230, training loss: 645.6341552734375 = 1.6669280529022217 + 100.0 * 6.439672470092773
Epoch 230, val loss: 1.6787729263305664
Epoch 240, training loss: 644.8602294921875 = 1.648751139640808 + 100.0 * 6.432115077972412
Epoch 240, val loss: 1.6626379489898682
Epoch 250, training loss: 644.1007080078125 = 1.6293485164642334 + 100.0 * 6.424713611602783
Epoch 250, val loss: 1.6455070972442627
Epoch 260, training loss: 643.6694946289062 = 1.6086704730987549 + 100.0 * 6.4206085205078125
Epoch 260, val loss: 1.6272627115249634
Epoch 270, training loss: 642.8440551757812 = 1.5867987871170044 + 100.0 * 6.412572860717773
Epoch 270, val loss: 1.6081238985061646
Epoch 280, training loss: 641.9725952148438 = 1.563887357711792 + 100.0 * 6.404087066650391
Epoch 280, val loss: 1.588231086730957
Epoch 290, training loss: 641.2730712890625 = 1.5400147438049316 + 100.0 * 6.397330284118652
Epoch 290, val loss: 1.5677083730697632
Epoch 300, training loss: 640.8402709960938 = 1.5152734518051147 + 100.0 * 6.393249988555908
Epoch 300, val loss: 1.5465112924575806
Epoch 310, training loss: 640.1959838867188 = 1.4896305799484253 + 100.0 * 6.387063503265381
Epoch 310, val loss: 1.524963617324829
Epoch 320, training loss: 639.9143676757812 = 1.463448405265808 + 100.0 * 6.384509563446045
Epoch 320, val loss: 1.503144383430481
Epoch 330, training loss: 638.9972534179688 = 1.4368033409118652 + 100.0 * 6.375604152679443
Epoch 330, val loss: 1.4811331033706665
Epoch 340, training loss: 638.4337158203125 = 1.4099329710006714 + 100.0 * 6.370238304138184
Epoch 340, val loss: 1.459411382675171
Epoch 350, training loss: 637.9067993164062 = 1.383028507232666 + 100.0 * 6.365237712860107
Epoch 350, val loss: 1.4378389120101929
Epoch 360, training loss: 638.0771484375 = 1.3560456037521362 + 100.0 * 6.36721134185791
Epoch 360, val loss: 1.4166005849838257
Epoch 370, training loss: 637.2437133789062 = 1.3290857076644897 + 100.0 * 6.3591461181640625
Epoch 370, val loss: 1.3956611156463623
Epoch 380, training loss: 636.7329711914062 = 1.3024561405181885 + 100.0 * 6.354305267333984
Epoch 380, val loss: 1.3754924535751343
Epoch 390, training loss: 636.2760620117188 = 1.2762186527252197 + 100.0 * 6.3499979972839355
Epoch 390, val loss: 1.3558677434921265
Epoch 400, training loss: 635.8244018554688 = 1.2504448890686035 + 100.0 * 6.345739841461182
Epoch 400, val loss: 1.3368794918060303
Epoch 410, training loss: 635.9143676757812 = 1.2249205112457275 + 100.0 * 6.346894264221191
Epoch 410, val loss: 1.3186613321304321
Epoch 420, training loss: 635.1478881835938 = 1.1998836994171143 + 100.0 * 6.339479923248291
Epoch 420, val loss: 1.3008482456207275
Epoch 430, training loss: 634.7003784179688 = 1.17528235912323 + 100.0 * 6.3352508544921875
Epoch 430, val loss: 1.283836007118225
Epoch 440, training loss: 634.3287963867188 = 1.1512517929077148 + 100.0 * 6.331775665283203
Epoch 440, val loss: 1.267500638961792
Epoch 450, training loss: 634.8666381835938 = 1.1275509595870972 + 100.0 * 6.337390899658203
Epoch 450, val loss: 1.2516330480575562
Epoch 460, training loss: 633.9156494140625 = 1.1040964126586914 + 100.0 * 6.328115463256836
Epoch 460, val loss: 1.236388921737671
Epoch 470, training loss: 633.4270629882812 = 1.0810624361038208 + 100.0 * 6.323460102081299
Epoch 470, val loss: 1.221684455871582
Epoch 480, training loss: 633.633056640625 = 1.0584728717803955 + 100.0 * 6.325746059417725
Epoch 480, val loss: 1.2073928117752075
Epoch 490, training loss: 632.9125366210938 = 1.0359587669372559 + 100.0 * 6.318766117095947
Epoch 490, val loss: 1.1934620141983032
Epoch 500, training loss: 632.9212036132812 = 1.0138931274414062 + 100.0 * 6.319072723388672
Epoch 500, val loss: 1.179910659790039
Epoch 510, training loss: 632.3775024414062 = 0.9919509291648865 + 100.0 * 6.3138556480407715
Epoch 510, val loss: 1.1667258739471436
Epoch 520, training loss: 631.9815063476562 = 0.9704050421714783 + 100.0 * 6.310111045837402
Epoch 520, val loss: 1.153988003730774
Epoch 530, training loss: 631.7943725585938 = 0.9491351246833801 + 100.0 * 6.308452129364014
Epoch 530, val loss: 1.1415786743164062
Epoch 540, training loss: 631.8629150390625 = 0.927993655204773 + 100.0 * 6.309349536895752
Epoch 540, val loss: 1.1295459270477295
Epoch 550, training loss: 631.3505859375 = 0.9070934057235718 + 100.0 * 6.304434776306152
Epoch 550, val loss: 1.1177531480789185
Epoch 560, training loss: 631.0396728515625 = 0.8865321278572083 + 100.0 * 6.3015313148498535
Epoch 560, val loss: 1.1065489053726196
Epoch 570, training loss: 631.5584716796875 = 0.8662799596786499 + 100.0 * 6.30692195892334
Epoch 570, val loss: 1.0954971313476562
Epoch 580, training loss: 630.7496948242188 = 0.8461139798164368 + 100.0 * 6.299035549163818
Epoch 580, val loss: 1.084959864616394
Epoch 590, training loss: 630.448486328125 = 0.8263964056968689 + 100.0 * 6.296220779418945
Epoch 590, val loss: 1.0747662782669067
Epoch 600, training loss: 630.2782592773438 = 0.8070501685142517 + 100.0 * 6.294712066650391
Epoch 600, val loss: 1.0650395154953003
Epoch 610, training loss: 630.3045654296875 = 0.7880316376686096 + 100.0 * 6.295165538787842
Epoch 610, val loss: 1.0557602643966675
Epoch 620, training loss: 629.9962768554688 = 0.7693251967430115 + 100.0 * 6.292269229888916
Epoch 620, val loss: 1.0469211339950562
Epoch 630, training loss: 630.0477905273438 = 0.7508997917175293 + 100.0 * 6.29296875
Epoch 630, val loss: 1.0383919477462769
Epoch 640, training loss: 629.6190795898438 = 0.7328904271125793 + 100.0 * 6.2888617515563965
Epoch 640, val loss: 1.0304243564605713
Epoch 650, training loss: 629.3328857421875 = 0.7153135538101196 + 100.0 * 6.286175727844238
Epoch 650, val loss: 1.022749900817871
Epoch 660, training loss: 629.1895141601562 = 0.6981475949287415 + 100.0 * 6.284914016723633
Epoch 660, val loss: 1.0156676769256592
Epoch 670, training loss: 630.047607421875 = 0.6813132762908936 + 100.0 * 6.2936625480651855
Epoch 670, val loss: 1.0089448690414429
Epoch 680, training loss: 629.1704711914062 = 0.6646544933319092 + 100.0 * 6.28505802154541
Epoch 680, val loss: 1.0024899244308472
Epoch 690, training loss: 628.8001708984375 = 0.648446261882782 + 100.0 * 6.281517505645752
Epoch 690, val loss: 0.9966487288475037
Epoch 700, training loss: 628.5684204101562 = 0.6326946020126343 + 100.0 * 6.279357433319092
Epoch 700, val loss: 0.9912047386169434
Epoch 710, training loss: 628.4310302734375 = 0.6173164248466492 + 100.0 * 6.27813720703125
Epoch 710, val loss: 0.9862253665924072
Epoch 720, training loss: 628.7819213867188 = 0.6021836400032043 + 100.0 * 6.281797409057617
Epoch 720, val loss: 0.9814779758453369
Epoch 730, training loss: 628.4796142578125 = 0.5872823596000671 + 100.0 * 6.278923034667969
Epoch 730, val loss: 0.9771327376365662
Epoch 740, training loss: 628.1183471679688 = 0.5727788805961609 + 100.0 * 6.275455951690674
Epoch 740, val loss: 0.9729647040367126
Epoch 750, training loss: 627.9317016601562 = 0.5586532354354858 + 100.0 * 6.273730278015137
Epoch 750, val loss: 0.9692419171333313
Epoch 760, training loss: 628.2333374023438 = 0.5448610782623291 + 100.0 * 6.276885032653809
Epoch 760, val loss: 0.9659190773963928
Epoch 770, training loss: 627.9031982421875 = 0.5313352346420288 + 100.0 * 6.27371883392334
Epoch 770, val loss: 0.9628337621688843
Epoch 780, training loss: 627.8280029296875 = 0.51812344789505 + 100.0 * 6.273098945617676
Epoch 780, val loss: 0.96016925573349
Epoch 790, training loss: 627.590576171875 = 0.505185604095459 + 100.0 * 6.2708539962768555
Epoch 790, val loss: 0.9576528072357178
Epoch 800, training loss: 627.3775634765625 = 0.49255314469337463 + 100.0 * 6.268850326538086
Epoch 800, val loss: 0.9553683996200562
Epoch 810, training loss: 627.2360229492188 = 0.48026788234710693 + 100.0 * 6.267557621002197
Epoch 810, val loss: 0.9536128044128418
Epoch 820, training loss: 628.0083618164062 = 0.4682420790195465 + 100.0 * 6.2754011154174805
Epoch 820, val loss: 0.952023446559906
Epoch 830, training loss: 627.2969970703125 = 0.4564620554447174 + 100.0 * 6.268405437469482
Epoch 830, val loss: 0.9502316117286682
Epoch 840, training loss: 626.9354248046875 = 0.4449322521686554 + 100.0 * 6.264904975891113
Epoch 840, val loss: 0.9490345120429993
Epoch 850, training loss: 627.0125122070312 = 0.4337309002876282 + 100.0 * 6.2657880783081055
Epoch 850, val loss: 0.948168933391571
Epoch 860, training loss: 626.751953125 = 0.4228146970272064 + 100.0 * 6.263291835784912
Epoch 860, val loss: 0.9473013281822205
Epoch 870, training loss: 626.7936401367188 = 0.4121457636356354 + 100.0 * 6.263814449310303
Epoch 870, val loss: 0.9468379616737366
Epoch 880, training loss: 626.7193603515625 = 0.4016963243484497 + 100.0 * 6.263176918029785
Epoch 880, val loss: 0.9463807344436646
Epoch 890, training loss: 626.7198486328125 = 0.39149242639541626 + 100.0 * 6.263283729553223
Epoch 890, val loss: 0.946187436580658
Epoch 900, training loss: 626.4862670898438 = 0.38148581981658936 + 100.0 * 6.261047840118408
Epoch 900, val loss: 0.9461379647254944
Epoch 910, training loss: 626.343505859375 = 0.3717086911201477 + 100.0 * 6.25971794128418
Epoch 910, val loss: 0.9461604356765747
Epoch 920, training loss: 626.1392211914062 = 0.3622415363788605 + 100.0 * 6.257769584655762
Epoch 920, val loss: 0.9463518857955933
Epoch 930, training loss: 626.038330078125 = 0.35302498936653137 + 100.0 * 6.256853103637695
Epoch 930, val loss: 0.9469770193099976
Epoch 940, training loss: 627.2028198242188 = 0.3439680337905884 + 100.0 * 6.268588542938232
Epoch 940, val loss: 0.947645902633667
Epoch 950, training loss: 625.8961181640625 = 0.3350635766983032 + 100.0 * 6.255610466003418
Epoch 950, val loss: 0.9481236934661865
Epoch 960, training loss: 625.8784790039062 = 0.32641416788101196 + 100.0 * 6.255520343780518
Epoch 960, val loss: 0.9490386843681335
Epoch 970, training loss: 625.7340698242188 = 0.31799352169036865 + 100.0 * 6.2541608810424805
Epoch 970, val loss: 0.9501263499259949
Epoch 980, training loss: 626.50390625 = 0.30976811051368713 + 100.0 * 6.261941432952881
Epoch 980, val loss: 0.9511194229125977
Epoch 990, training loss: 626.0853271484375 = 0.3016674220561981 + 100.0 * 6.25783634185791
Epoch 990, val loss: 0.9526370763778687
Epoch 1000, training loss: 625.5726928710938 = 0.2937381863594055 + 100.0 * 6.252789497375488
Epoch 1000, val loss: 0.9539244771003723
Epoch 1010, training loss: 625.7091064453125 = 0.2860528230667114 + 100.0 * 6.254230499267578
Epoch 1010, val loss: 0.9553663730621338
Epoch 1020, training loss: 625.3933715820312 = 0.2785433530807495 + 100.0 * 6.251148223876953
Epoch 1020, val loss: 0.9570380449295044
Epoch 1030, training loss: 625.4030151367188 = 0.2712000012397766 + 100.0 * 6.251318454742432
Epoch 1030, val loss: 0.958926260471344
Epoch 1040, training loss: 625.41015625 = 0.2640340030193329 + 100.0 * 6.251461029052734
Epoch 1040, val loss: 0.960907518863678
Epoch 1050, training loss: 625.316162109375 = 0.2570174038410187 + 100.0 * 6.250591278076172
Epoch 1050, val loss: 0.9629870653152466
Epoch 1060, training loss: 625.2568969726562 = 0.2501160204410553 + 100.0 * 6.250067710876465
Epoch 1060, val loss: 0.965016782283783
Epoch 1070, training loss: 625.181640625 = 0.24343840777873993 + 100.0 * 6.249382495880127
Epoch 1070, val loss: 0.9670633673667908
Epoch 1080, training loss: 625.11083984375 = 0.2369288057088852 + 100.0 * 6.248739242553711
Epoch 1080, val loss: 0.9696398973464966
Epoch 1090, training loss: 625.2672119140625 = 0.23058032989501953 + 100.0 * 6.2503662109375
Epoch 1090, val loss: 0.9719967246055603
Epoch 1100, training loss: 624.9367065429688 = 0.22440411150455475 + 100.0 * 6.247122764587402
Epoch 1100, val loss: 0.9744045734405518
Epoch 1110, training loss: 624.7968139648438 = 0.21836502850055695 + 100.0 * 6.245784282684326
Epoch 1110, val loss: 0.9772725701332092
Epoch 1120, training loss: 625.4765014648438 = 0.2124980390071869 + 100.0 * 6.2526397705078125
Epoch 1120, val loss: 0.9798545837402344
Epoch 1130, training loss: 624.8651733398438 = 0.20675837993621826 + 100.0 * 6.246583938598633
Epoch 1130, val loss: 0.9827015995979309
Epoch 1140, training loss: 624.6244506835938 = 0.20115599036216736 + 100.0 * 6.244232654571533
Epoch 1140, val loss: 0.9856272339820862
Epoch 1150, training loss: 625.1572265625 = 0.19574286043643951 + 100.0 * 6.249614715576172
Epoch 1150, val loss: 0.9886944890022278
Epoch 1160, training loss: 624.67724609375 = 0.19047503173351288 + 100.0 * 6.244867324829102
Epoch 1160, val loss: 0.9914766550064087
Epoch 1170, training loss: 624.5349731445312 = 0.18534022569656372 + 100.0 * 6.243495941162109
Epoch 1170, val loss: 0.9947569966316223
Epoch 1180, training loss: 624.5969848632812 = 0.18037474155426025 + 100.0 * 6.244166374206543
Epoch 1180, val loss: 0.9980231523513794
Epoch 1190, training loss: 624.3372802734375 = 0.175563246011734 + 100.0 * 6.241617679595947
Epoch 1190, val loss: 1.0012482404708862
Epoch 1200, training loss: 624.7461547851562 = 0.1708844155073166 + 100.0 * 6.245752334594727
Epoch 1200, val loss: 1.0045526027679443
Epoch 1210, training loss: 624.3509521484375 = 0.16632603108882904 + 100.0 * 6.241846084594727
Epoch 1210, val loss: 1.0080407857894897
Epoch 1220, training loss: 624.2407836914062 = 0.16188645362854004 + 100.0 * 6.240789413452148
Epoch 1220, val loss: 1.011541485786438
Epoch 1230, training loss: 624.6890869140625 = 0.1576475203037262 + 100.0 * 6.245314598083496
Epoch 1230, val loss: 1.0151557922363281
Epoch 1240, training loss: 624.3667602539062 = 0.1534479558467865 + 100.0 * 6.242133140563965
Epoch 1240, val loss: 1.0192766189575195
Epoch 1250, training loss: 624.1222534179688 = 0.14941687881946564 + 100.0 * 6.2397284507751465
Epoch 1250, val loss: 1.0224909782409668
Epoch 1260, training loss: 624.404052734375 = 0.1455104798078537 + 100.0 * 6.2425856590271
Epoch 1260, val loss: 1.0264371633529663
Epoch 1270, training loss: 623.94091796875 = 0.14173011481761932 + 100.0 * 6.237991809844971
Epoch 1270, val loss: 1.030349612236023
Epoch 1280, training loss: 623.9231567382812 = 0.1380596160888672 + 100.0 * 6.237851142883301
Epoch 1280, val loss: 1.034074068069458
Epoch 1290, training loss: 624.1463012695312 = 0.13449408113956451 + 100.0 * 6.24011754989624
Epoch 1290, val loss: 1.0381726026535034
Epoch 1300, training loss: 623.8389892578125 = 0.13100245594978333 + 100.0 * 6.237080097198486
Epoch 1300, val loss: 1.0425306558609009
Epoch 1310, training loss: 624.3407592773438 = 0.1276405304670334 + 100.0 * 6.242131233215332
Epoch 1310, val loss: 1.0462281703948975
Epoch 1320, training loss: 623.8389892578125 = 0.12439658492803574 + 100.0 * 6.237145900726318
Epoch 1320, val loss: 1.0508607625961304
Epoch 1330, training loss: 623.6537475585938 = 0.12122000753879547 + 100.0 * 6.235325336456299
Epoch 1330, val loss: 1.0549803972244263
Epoch 1340, training loss: 623.8873901367188 = 0.11819043755531311 + 100.0 * 6.237692356109619
Epoch 1340, val loss: 1.0593143701553345
Epoch 1350, training loss: 623.5616455078125 = 0.1152137890458107 + 100.0 * 6.234464168548584
Epoch 1350, val loss: 1.0635510683059692
Epoch 1360, training loss: 623.6409912109375 = 0.11232531070709229 + 100.0 * 6.235286712646484
Epoch 1360, val loss: 1.0679038763046265
Epoch 1370, training loss: 623.8599243164062 = 0.10953939706087112 + 100.0 * 6.237503528594971
Epoch 1370, val loss: 1.0724073648452759
Epoch 1380, training loss: 623.5911865234375 = 0.10684080421924591 + 100.0 * 6.234843730926514
Epoch 1380, val loss: 1.0764168500900269
Epoch 1390, training loss: 623.479248046875 = 0.1042097806930542 + 100.0 * 6.233750820159912
Epoch 1390, val loss: 1.0807592868804932
Epoch 1400, training loss: 623.30615234375 = 0.10168437659740448 + 100.0 * 6.232044696807861
Epoch 1400, val loss: 1.0854359865188599
Epoch 1410, training loss: 623.3756103515625 = 0.0992441475391388 + 100.0 * 6.232763767242432
Epoch 1410, val loss: 1.0897071361541748
Epoch 1420, training loss: 623.5055541992188 = 0.09687264263629913 + 100.0 * 6.234086990356445
Epoch 1420, val loss: 1.0941951274871826
Epoch 1430, training loss: 623.295654296875 = 0.09455409646034241 + 100.0 * 6.232010841369629
Epoch 1430, val loss: 1.0987287759780884
Epoch 1440, training loss: 623.3346557617188 = 0.09233162552118301 + 100.0 * 6.232423305511475
Epoch 1440, val loss: 1.1029729843139648
Epoch 1450, training loss: 623.4432373046875 = 0.09013675898313522 + 100.0 * 6.2335309982299805
Epoch 1450, val loss: 1.1080021858215332
Epoch 1460, training loss: 623.0883178710938 = 0.08798965066671371 + 100.0 * 6.2300028800964355
Epoch 1460, val loss: 1.1118850708007812
Epoch 1470, training loss: 623.1785888671875 = 0.08595822006464005 + 100.0 * 6.230926513671875
Epoch 1470, val loss: 1.1166757345199585
Epoch 1480, training loss: 623.2980346679688 = 0.08397407084703445 + 100.0 * 6.23214054107666
Epoch 1480, val loss: 1.1208562850952148
Epoch 1490, training loss: 623.091064453125 = 0.08203889429569244 + 100.0 * 6.230090618133545
Epoch 1490, val loss: 1.1256768703460693
Epoch 1500, training loss: 623.0814208984375 = 0.08018898963928223 + 100.0 * 6.230011940002441
Epoch 1500, val loss: 1.1300179958343506
Epoch 1510, training loss: 622.9376831054688 = 0.0783749371767044 + 100.0 * 6.228593349456787
Epoch 1510, val loss: 1.1346683502197266
Epoch 1520, training loss: 622.8238525390625 = 0.07662348449230194 + 100.0 * 6.22747278213501
Epoch 1520, val loss: 1.1391193866729736
Epoch 1530, training loss: 623.127197265625 = 0.07493757456541061 + 100.0 * 6.230522155761719
Epoch 1530, val loss: 1.1435980796813965
Epoch 1540, training loss: 622.8363037109375 = 0.0732673704624176 + 100.0 * 6.227630615234375
Epoch 1540, val loss: 1.1480923891067505
Epoch 1550, training loss: 623.2115478515625 = 0.07165421545505524 + 100.0 * 6.231399059295654
Epoch 1550, val loss: 1.1526906490325928
Epoch 1560, training loss: 622.8763427734375 = 0.07007792592048645 + 100.0 * 6.228062629699707
Epoch 1560, val loss: 1.1567515134811401
Epoch 1570, training loss: 622.7626953125 = 0.06855734437704086 + 100.0 * 6.2269415855407715
Epoch 1570, val loss: 1.1613905429840088
Epoch 1580, training loss: 623.0145263671875 = 0.0670941174030304 + 100.0 * 6.2294745445251465
Epoch 1580, val loss: 1.1656988859176636
Epoch 1590, training loss: 622.66796875 = 0.06565964221954346 + 100.0 * 6.226022720336914
Epoch 1590, val loss: 1.1701529026031494
Epoch 1600, training loss: 622.843994140625 = 0.06427692621946335 + 100.0 * 6.227797031402588
Epoch 1600, val loss: 1.1744428873062134
Epoch 1610, training loss: 622.6356201171875 = 0.0629124641418457 + 100.0 * 6.225727081298828
Epoch 1610, val loss: 1.178868293762207
Epoch 1620, training loss: 622.6397705078125 = 0.06160126253962517 + 100.0 * 6.2257819175720215
Epoch 1620, val loss: 1.183123230934143
Epoch 1630, training loss: 622.745849609375 = 0.060322005301713943 + 100.0 * 6.226855754852295
Epoch 1630, val loss: 1.187749981880188
Epoch 1640, training loss: 622.7972412109375 = 0.059083424508571625 + 100.0 * 6.227381706237793
Epoch 1640, val loss: 1.1913976669311523
Epoch 1650, training loss: 622.5418090820312 = 0.05784925818443298 + 100.0 * 6.224839210510254
Epoch 1650, val loss: 1.196251630783081
Epoch 1660, training loss: 622.3922729492188 = 0.05667617544531822 + 100.0 * 6.223355770111084
Epoch 1660, val loss: 1.2003092765808105
Epoch 1670, training loss: 622.3902587890625 = 0.05554052069783211 + 100.0 * 6.223347187042236
Epoch 1670, val loss: 1.2048816680908203
Epoch 1680, training loss: 622.8223876953125 = 0.05443420261144638 + 100.0 * 6.22767972946167
Epoch 1680, val loss: 1.2091971635818481
Epoch 1690, training loss: 622.4049682617188 = 0.053341709077358246 + 100.0 * 6.22351598739624
Epoch 1690, val loss: 1.2132843732833862
Epoch 1700, training loss: 622.3604736328125 = 0.052287034690380096 + 100.0 * 6.223081588745117
Epoch 1700, val loss: 1.2177555561065674
Epoch 1710, training loss: 622.5979614257812 = 0.05126398801803589 + 100.0 * 6.225466728210449
Epoch 1710, val loss: 1.2221148014068604
Epoch 1720, training loss: 622.5576171875 = 0.05025871843099594 + 100.0 * 6.22507381439209
Epoch 1720, val loss: 1.2262080907821655
Epoch 1730, training loss: 622.2670288085938 = 0.04926389455795288 + 100.0 * 6.222177982330322
Epoch 1730, val loss: 1.230392575263977
Epoch 1740, training loss: 622.1419067382812 = 0.048322536051273346 + 100.0 * 6.220935821533203
Epoch 1740, val loss: 1.2347772121429443
Epoch 1750, training loss: 622.1466064453125 = 0.04741409048438072 + 100.0 * 6.220991611480713
Epoch 1750, val loss: 1.2388663291931152
Epoch 1760, training loss: 622.5081787109375 = 0.04652953892946243 + 100.0 * 6.224616527557373
Epoch 1760, val loss: 1.2431734800338745
Epoch 1770, training loss: 622.0843505859375 = 0.0456366091966629 + 100.0 * 6.220386981964111
Epoch 1770, val loss: 1.2473700046539307
Epoch 1780, training loss: 622.0813598632812 = 0.04477536305785179 + 100.0 * 6.220365524291992
Epoch 1780, val loss: 1.251531958580017
Epoch 1790, training loss: 622.3239135742188 = 0.04394548758864403 + 100.0 * 6.222799777984619
Epoch 1790, val loss: 1.2559314966201782
Epoch 1800, training loss: 622.3838500976562 = 0.043122418224811554 + 100.0 * 6.223406791687012
Epoch 1800, val loss: 1.2603360414505005
Epoch 1810, training loss: 622.1580810546875 = 0.042323991656303406 + 100.0 * 6.221158027648926
Epoch 1810, val loss: 1.263663649559021
Epoch 1820, training loss: 622.064208984375 = 0.041545458137989044 + 100.0 * 6.220226287841797
Epoch 1820, val loss: 1.268288254737854
Epoch 1830, training loss: 621.9622192382812 = 0.04079708829522133 + 100.0 * 6.21921443939209
Epoch 1830, val loss: 1.2722247838974
Epoch 1840, training loss: 621.9599609375 = 0.04007220268249512 + 100.0 * 6.219198703765869
Epoch 1840, val loss: 1.2764955759048462
Epoch 1850, training loss: 622.153076171875 = 0.039364252239465714 + 100.0 * 6.221137046813965
Epoch 1850, val loss: 1.2803521156311035
Epoch 1860, training loss: 622.0040893554688 = 0.038667164742946625 + 100.0 * 6.219654083251953
Epoch 1860, val loss: 1.2844455242156982
Epoch 1870, training loss: 621.9357299804688 = 0.03798012435436249 + 100.0 * 6.218977928161621
Epoch 1870, val loss: 1.2886232137680054
Epoch 1880, training loss: 622.2282104492188 = 0.03731586039066315 + 100.0 * 6.221909046173096
Epoch 1880, val loss: 1.2927801609039307
Epoch 1890, training loss: 621.8656616210938 = 0.03665195032954216 + 100.0 * 6.218289852142334
Epoch 1890, val loss: 1.2968262434005737
Epoch 1900, training loss: 621.9446411132812 = 0.03601314127445221 + 100.0 * 6.219086170196533
Epoch 1900, val loss: 1.3005996942520142
Epoch 1910, training loss: 621.82568359375 = 0.03539646416902542 + 100.0 * 6.217903137207031
Epoch 1910, val loss: 1.3047780990600586
Epoch 1920, training loss: 621.739501953125 = 0.03478645533323288 + 100.0 * 6.217047214508057
Epoch 1920, val loss: 1.3087828159332275
Epoch 1930, training loss: 622.0559692382812 = 0.034204140305519104 + 100.0 * 6.220218181610107
Epoch 1930, val loss: 1.3126678466796875
Epoch 1940, training loss: 621.7483520507812 = 0.03363076597452164 + 100.0 * 6.217147350311279
Epoch 1940, val loss: 1.3166002035140991
Epoch 1950, training loss: 621.7811889648438 = 0.03306431323289871 + 100.0 * 6.2174811363220215
Epoch 1950, val loss: 1.3207098245620728
Epoch 1960, training loss: 621.7910766601562 = 0.032512471079826355 + 100.0 * 6.217585563659668
Epoch 1960, val loss: 1.3247802257537842
Epoch 1970, training loss: 621.8729858398438 = 0.03197561204433441 + 100.0 * 6.218410015106201
Epoch 1970, val loss: 1.3282761573791504
Epoch 1980, training loss: 622.015625 = 0.031445130705833435 + 100.0 * 6.219841957092285
Epoch 1980, val loss: 1.3321726322174072
Epoch 1990, training loss: 621.693359375 = 0.03092562034726143 + 100.0 * 6.2166242599487305
Epoch 1990, val loss: 1.3359837532043457
Epoch 2000, training loss: 621.6611328125 = 0.030422115698456764 + 100.0 * 6.216307163238525
Epoch 2000, val loss: 1.3401117324829102
Epoch 2010, training loss: 621.5680541992188 = 0.029934240505099297 + 100.0 * 6.215381622314453
Epoch 2010, val loss: 1.3437235355377197
Epoch 2020, training loss: 621.5283813476562 = 0.02945900149643421 + 100.0 * 6.214989185333252
Epoch 2020, val loss: 1.3476239442825317
Epoch 2030, training loss: 622.0780639648438 = 0.029001710936427116 + 100.0 * 6.2204909324646
Epoch 2030, val loss: 1.3514609336853027
Epoch 2040, training loss: 621.7675170898438 = 0.028535393998026848 + 100.0 * 6.2173895835876465
Epoch 2040, val loss: 1.3547518253326416
Epoch 2050, training loss: 621.4867553710938 = 0.028074165806174278 + 100.0 * 6.2145867347717285
Epoch 2050, val loss: 1.3589465618133545
Epoch 2060, training loss: 621.4580078125 = 0.02763318456709385 + 100.0 * 6.214303493499756
Epoch 2060, val loss: 1.3626391887664795
Epoch 2070, training loss: 621.60693359375 = 0.027211586013436317 + 100.0 * 6.215796947479248
Epoch 2070, val loss: 1.3665337562561035
Epoch 2080, training loss: 621.5568237304688 = 0.026791634038090706 + 100.0 * 6.2153000831604
Epoch 2080, val loss: 1.3698183298110962
Epoch 2090, training loss: 621.433349609375 = 0.02637750282883644 + 100.0 * 6.214069843292236
Epoch 2090, val loss: 1.373268961906433
Epoch 2100, training loss: 621.674560546875 = 0.025969259440898895 + 100.0 * 6.21648645401001
Epoch 2100, val loss: 1.3773717880249023
Epoch 2110, training loss: 621.3975830078125 = 0.025575170293450356 + 100.0 * 6.213719844818115
Epoch 2110, val loss: 1.3808220624923706
Epoch 2120, training loss: 621.6356811523438 = 0.02518668957054615 + 100.0 * 6.216104507446289
Epoch 2120, val loss: 1.3841739892959595
Epoch 2130, training loss: 621.4793701171875 = 0.024804402142763138 + 100.0 * 6.214545726776123
Epoch 2130, val loss: 1.387906789779663
Epoch 2140, training loss: 621.2684326171875 = 0.02442556619644165 + 100.0 * 6.212440490722656
Epoch 2140, val loss: 1.3915811777114868
Epoch 2150, training loss: 621.1815795898438 = 0.02407042495906353 + 100.0 * 6.211575031280518
Epoch 2150, val loss: 1.3951994180679321
Epoch 2160, training loss: 621.168701171875 = 0.023722156882286072 + 100.0 * 6.21144962310791
Epoch 2160, val loss: 1.3988449573516846
Epoch 2170, training loss: 621.7965087890625 = 0.02338617481291294 + 100.0 * 6.217731475830078
Epoch 2170, val loss: 1.4023648500442505
Epoch 2180, training loss: 621.1983032226562 = 0.023035559803247452 + 100.0 * 6.211752891540527
Epoch 2180, val loss: 1.4056764841079712
Epoch 2190, training loss: 621.2525634765625 = 0.022699620574712753 + 100.0 * 6.21229887008667
Epoch 2190, val loss: 1.4093533754348755
Epoch 2200, training loss: 621.5413818359375 = 0.022371551021933556 + 100.0 * 6.2151899337768555
Epoch 2200, val loss: 1.4129525423049927
Epoch 2210, training loss: 621.4639282226562 = 0.02205386757850647 + 100.0 * 6.214418888092041
Epoch 2210, val loss: 1.415920376777649
Epoch 2220, training loss: 621.27294921875 = 0.021732939407229424 + 100.0 * 6.212512016296387
Epoch 2220, val loss: 1.4198640584945679
Epoch 2230, training loss: 621.3909912109375 = 0.02142496407032013 + 100.0 * 6.213695526123047
Epoch 2230, val loss: 1.4228378534317017
Epoch 2240, training loss: 621.1185302734375 = 0.02112535759806633 + 100.0 * 6.210974216461182
Epoch 2240, val loss: 1.4264987707138062
Epoch 2250, training loss: 620.9783935546875 = 0.020832423120737076 + 100.0 * 6.209575653076172
Epoch 2250, val loss: 1.4297711849212646
Epoch 2260, training loss: 620.996826171875 = 0.02055182307958603 + 100.0 * 6.2097625732421875
Epoch 2260, val loss: 1.4332938194274902
Epoch 2270, training loss: 621.3290405273438 = 0.02027924358844757 + 100.0 * 6.213088035583496
Epoch 2270, val loss: 1.436592936515808
Epoch 2280, training loss: 621.0602416992188 = 0.019997593015432358 + 100.0 * 6.210402011871338
Epoch 2280, val loss: 1.4399362802505493
Epoch 2290, training loss: 621.199462890625 = 0.01971927098929882 + 100.0 * 6.21179723739624
Epoch 2290, val loss: 1.44352126121521
Epoch 2300, training loss: 621.0870361328125 = 0.01944752037525177 + 100.0 * 6.2106757164001465
Epoch 2300, val loss: 1.446435809135437
Epoch 2310, training loss: 620.9901123046875 = 0.019188929349184036 + 100.0 * 6.209709644317627
Epoch 2310, val loss: 1.4500142335891724
Epoch 2320, training loss: 621.2117309570312 = 0.01893676444888115 + 100.0 * 6.211928367614746
Epoch 2320, val loss: 1.4536656141281128
Epoch 2330, training loss: 620.8780517578125 = 0.01868021860718727 + 100.0 * 6.208593845367432
Epoch 2330, val loss: 1.4565472602844238
Epoch 2340, training loss: 620.8350219726562 = 0.018430761992931366 + 100.0 * 6.208166122436523
Epoch 2340, val loss: 1.459963083267212
Epoch 2350, training loss: 620.8363037109375 = 0.018192175775766373 + 100.0 * 6.208180904388428
Epoch 2350, val loss: 1.463322401046753
Epoch 2360, training loss: 621.3075561523438 = 0.017963623628020287 + 100.0 * 6.212896347045898
Epoch 2360, val loss: 1.4664630889892578
Epoch 2370, training loss: 620.900146484375 = 0.017727944999933243 + 100.0 * 6.208824634552002
Epoch 2370, val loss: 1.469874620437622
Epoch 2380, training loss: 620.8588256835938 = 0.017495829612016678 + 100.0 * 6.208413124084473
Epoch 2380, val loss: 1.4728044271469116
Epoch 2390, training loss: 620.9232177734375 = 0.01727144420146942 + 100.0 * 6.209059238433838
Epoch 2390, val loss: 1.4764078855514526
Epoch 2400, training loss: 621.2686767578125 = 0.017049184069037437 + 100.0 * 6.2125163078308105
Epoch 2400, val loss: 1.4790523052215576
Epoch 2410, training loss: 620.90380859375 = 0.016831032931804657 + 100.0 * 6.208869457244873
Epoch 2410, val loss: 1.4821221828460693
Epoch 2420, training loss: 620.76318359375 = 0.016617298126220703 + 100.0 * 6.207466125488281
Epoch 2420, val loss: 1.4854785203933716
Epoch 2430, training loss: 620.6869506835938 = 0.01641397923231125 + 100.0 * 6.206705093383789
Epoch 2430, val loss: 1.4885780811309814
Epoch 2440, training loss: 620.9420166015625 = 0.01621832326054573 + 100.0 * 6.20925760269165
Epoch 2440, val loss: 1.491995930671692
Epoch 2450, training loss: 620.8013916015625 = 0.016012953594326973 + 100.0 * 6.207854270935059
Epoch 2450, val loss: 1.4946300983428955
Epoch 2460, training loss: 620.6240234375 = 0.015811799094080925 + 100.0 * 6.206081867218018
Epoch 2460, val loss: 1.4976259469985962
Epoch 2470, training loss: 620.621826171875 = 0.01561950333416462 + 100.0 * 6.206061840057373
Epoch 2470, val loss: 1.5007051229476929
Epoch 2480, training loss: 621.09619140625 = 0.0154331736266613 + 100.0 * 6.2108073234558105
Epoch 2480, val loss: 1.5039061307907104
Epoch 2490, training loss: 620.713623046875 = 0.015244468115270138 + 100.0 * 6.20698356628418
Epoch 2490, val loss: 1.5069628953933716
Epoch 2500, training loss: 620.5571899414062 = 0.015060712583363056 + 100.0 * 6.205421447753906
Epoch 2500, val loss: 1.5098974704742432
Epoch 2510, training loss: 620.7767333984375 = 0.014887280762195587 + 100.0 * 6.207618236541748
Epoch 2510, val loss: 1.5127599239349365
Epoch 2520, training loss: 620.86279296875 = 0.01471162959933281 + 100.0 * 6.2084808349609375
Epoch 2520, val loss: 1.516110897064209
Epoch 2530, training loss: 620.7728881835938 = 0.014532370492815971 + 100.0 * 6.207583427429199
Epoch 2530, val loss: 1.5190718173980713
Epoch 2540, training loss: 620.5542602539062 = 0.01436393242329359 + 100.0 * 6.205399036407471
Epoch 2540, val loss: 1.521814227104187
Epoch 2550, training loss: 620.7499389648438 = 0.014201045036315918 + 100.0 * 6.207356929779053
Epoch 2550, val loss: 1.5248165130615234
Epoch 2560, training loss: 620.6617431640625 = 0.014035954140126705 + 100.0 * 6.206477165222168
Epoch 2560, val loss: 1.5276795625686646
Epoch 2570, training loss: 620.778564453125 = 0.01387988030910492 + 100.0 * 6.20764684677124
Epoch 2570, val loss: 1.5305743217468262
Epoch 2580, training loss: 620.7611083984375 = 0.013718409463763237 + 100.0 * 6.2074737548828125
Epoch 2580, val loss: 1.5331308841705322
Epoch 2590, training loss: 620.7140502929688 = 0.013557874597609043 + 100.0 * 6.207005023956299
Epoch 2590, val loss: 1.5366958379745483
Epoch 2600, training loss: 620.7527465820312 = 0.013406564481556416 + 100.0 * 6.207393169403076
Epoch 2600, val loss: 1.5392723083496094
Epoch 2610, training loss: 620.4083862304688 = 0.013252069242298603 + 100.0 * 6.203951358795166
Epoch 2610, val loss: 1.5421512126922607
Epoch 2620, training loss: 620.4979858398438 = 0.013107679784297943 + 100.0 * 6.204848766326904
Epoch 2620, val loss: 1.5447767972946167
Epoch 2630, training loss: 620.701904296875 = 0.01296597346663475 + 100.0 * 6.206889629364014
Epoch 2630, val loss: 1.5478116273880005
Epoch 2640, training loss: 620.4271240234375 = 0.01282388623803854 + 100.0 * 6.204143047332764
Epoch 2640, val loss: 1.550735592842102
Epoch 2650, training loss: 620.4652709960938 = 0.01268458366394043 + 100.0 * 6.204525470733643
Epoch 2650, val loss: 1.5532853603363037
Epoch 2660, training loss: 620.5231323242188 = 0.01254979521036148 + 100.0 * 6.205105304718018
Epoch 2660, val loss: 1.5562101602554321
Epoch 2670, training loss: 620.7871704101562 = 0.012415106408298016 + 100.0 * 6.207747936248779
Epoch 2670, val loss: 1.558838963508606
Epoch 2680, training loss: 620.7346801757812 = 0.012280511669814587 + 100.0 * 6.207224369049072
Epoch 2680, val loss: 1.561538815498352
Epoch 2690, training loss: 620.4523315429688 = 0.012146539986133575 + 100.0 * 6.204401969909668
Epoch 2690, val loss: 1.5641292333602905
Epoch 2700, training loss: 620.39404296875 = 0.012020102702081203 + 100.0 * 6.20382022857666
Epoch 2700, val loss: 1.5669914484024048
Epoch 2710, training loss: 620.55615234375 = 0.01189727894961834 + 100.0 * 6.205442428588867
Epoch 2710, val loss: 1.5699384212493896
Epoch 2720, training loss: 620.5538940429688 = 0.011771057732403278 + 100.0 * 6.205421447753906
Epoch 2720, val loss: 1.5727458000183105
Epoch 2730, training loss: 620.52197265625 = 0.01164675410836935 + 100.0 * 6.205103397369385
Epoch 2730, val loss: 1.5749462842941284
Epoch 2740, training loss: 620.3974609375 = 0.011523704044520855 + 100.0 * 6.203859329223633
Epoch 2740, val loss: 1.5777111053466797
Epoch 2750, training loss: 620.3350830078125 = 0.011401309631764889 + 100.0 * 6.2032365798950195
Epoch 2750, val loss: 1.5806565284729004
Epoch 2760, training loss: 620.3143310546875 = 0.011285236105322838 + 100.0 * 6.203030586242676
Epoch 2760, val loss: 1.583344578742981
Epoch 2770, training loss: 620.5592041015625 = 0.011170133017003536 + 100.0 * 6.205480575561523
Epoch 2770, val loss: 1.5862183570861816
Epoch 2780, training loss: 620.2859497070312 = 0.01105872169137001 + 100.0 * 6.202749252319336
Epoch 2780, val loss: 1.5886075496673584
Epoch 2790, training loss: 620.2481079101562 = 0.010945608839392662 + 100.0 * 6.202372074127197
Epoch 2790, val loss: 1.5909425020217896
Epoch 2800, training loss: 620.2182006835938 = 0.010836350731551647 + 100.0 * 6.202073097229004
Epoch 2800, val loss: 1.5938115119934082
Epoch 2810, training loss: 620.743896484375 = 0.010734026320278645 + 100.0 * 6.207331657409668
Epoch 2810, val loss: 1.596577525138855
Epoch 2820, training loss: 620.2628173828125 = 0.0106267798691988 + 100.0 * 6.202521800994873
Epoch 2820, val loss: 1.5988048315048218
Epoch 2830, training loss: 620.3321533203125 = 0.010518117807805538 + 100.0 * 6.203216552734375
Epoch 2830, val loss: 1.6016920804977417
Epoch 2840, training loss: 620.2269897460938 = 0.01041525136679411 + 100.0 * 6.202165603637695
Epoch 2840, val loss: 1.603916049003601
Epoch 2850, training loss: 620.1731567382812 = 0.010314542800188065 + 100.0 * 6.2016282081604
Epoch 2850, val loss: 1.6065962314605713
Epoch 2860, training loss: 620.1607666015625 = 0.01021608803421259 + 100.0 * 6.201505661010742
Epoch 2860, val loss: 1.6091138124465942
Epoch 2870, training loss: 620.8310546875 = 0.010123100131750107 + 100.0 * 6.20820951461792
Epoch 2870, val loss: 1.6118927001953125
Epoch 2880, training loss: 620.329833984375 = 0.010018880479037762 + 100.0 * 6.203197956085205
Epoch 2880, val loss: 1.6138403415679932
Epoch 2890, training loss: 620.168701171875 = 0.00992344319820404 + 100.0 * 6.201587677001953
Epoch 2890, val loss: 1.6166795492172241
Epoch 2900, training loss: 620.3154907226562 = 0.009830519556999207 + 100.0 * 6.203056335449219
Epoch 2900, val loss: 1.6192206144332886
Epoch 2910, training loss: 620.09912109375 = 0.009738168679177761 + 100.0 * 6.200893402099609
Epoch 2910, val loss: 1.6215656995773315
Epoch 2920, training loss: 620.0956420898438 = 0.009646572172641754 + 100.0 * 6.200860023498535
Epoch 2920, val loss: 1.6237051486968994
Epoch 2930, training loss: 620.1737670898438 = 0.009561131708323956 + 100.0 * 6.201642036437988
Epoch 2930, val loss: 1.6261485815048218
Epoch 2940, training loss: 620.2216796875 = 0.00947357714176178 + 100.0 * 6.202122211456299
Epoch 2940, val loss: 1.628702998161316
Epoch 2950, training loss: 620.5582885742188 = 0.009384856559336185 + 100.0 * 6.205489158630371
Epoch 2950, val loss: 1.630929708480835
Epoch 2960, training loss: 620.0447387695312 = 0.009296603500843048 + 100.0 * 6.20035457611084
Epoch 2960, val loss: 1.633209466934204
Epoch 2970, training loss: 620.0287475585938 = 0.009209568612277508 + 100.0 * 6.2001953125
Epoch 2970, val loss: 1.6358526945114136
Epoch 2980, training loss: 620.4981689453125 = 0.00913036335259676 + 100.0 * 6.204890251159668
Epoch 2980, val loss: 1.6384449005126953
Epoch 2990, training loss: 620.0538940429688 = 0.009046640247106552 + 100.0 * 6.200448513031006
Epoch 2990, val loss: 1.6403142213821411
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 861.6259155273438 = 1.941274881362915 + 100.0 * 8.596846580505371
Epoch 0, val loss: 1.9391156435012817
Epoch 10, training loss: 861.5386352539062 = 1.9321643114089966 + 100.0 * 8.596064567565918
Epoch 10, val loss: 1.9293694496154785
Epoch 20, training loss: 860.9200439453125 = 1.920777678489685 + 100.0 * 8.58999252319336
Epoch 20, val loss: 1.9169458150863647
Epoch 30, training loss: 856.4127807617188 = 1.9058926105499268 + 100.0 * 8.545068740844727
Epoch 30, val loss: 1.9007495641708374
Epoch 40, training loss: 830.647216796875 = 1.8881274461746216 + 100.0 * 8.287590980529785
Epoch 40, val loss: 1.8821289539337158
Epoch 50, training loss: 776.2130737304688 = 1.868762731552124 + 100.0 * 7.743443012237549
Epoch 50, val loss: 1.8631560802459717
Epoch 60, training loss: 732.3622436523438 = 1.855258822441101 + 100.0 * 7.305069446563721
Epoch 60, val loss: 1.8509103059768677
Epoch 70, training loss: 703.4651489257812 = 1.844340205192566 + 100.0 * 7.016208171844482
Epoch 70, val loss: 1.84033203125
Epoch 80, training loss: 690.2918701171875 = 1.8347418308258057 + 100.0 * 6.884571552276611
Epoch 80, val loss: 1.8308608531951904
Epoch 90, training loss: 679.7093505859375 = 1.8247933387756348 + 100.0 * 6.77884578704834
Epoch 90, val loss: 1.8214203119277954
Epoch 100, training loss: 670.8278198242188 = 1.8158884048461914 + 100.0 * 6.69011926651001
Epoch 100, val loss: 1.8132063150405884
Epoch 110, training loss: 665.2728271484375 = 1.80793297290802 + 100.0 * 6.63464879989624
Epoch 110, val loss: 1.8056652545928955
Epoch 120, training loss: 661.2344360351562 = 1.8000526428222656 + 100.0 * 6.594344139099121
Epoch 120, val loss: 1.7980791330337524
Epoch 130, training loss: 657.9074096679688 = 1.7920255661010742 + 100.0 * 6.561153411865234
Epoch 130, val loss: 1.7902626991271973
Epoch 140, training loss: 655.395263671875 = 1.783876657485962 + 100.0 * 6.536113739013672
Epoch 140, val loss: 1.782381534576416
Epoch 150, training loss: 653.229248046875 = 1.7754182815551758 + 100.0 * 6.514538764953613
Epoch 150, val loss: 1.7742249965667725
Epoch 160, training loss: 651.1718139648438 = 1.7663953304290771 + 100.0 * 6.494053840637207
Epoch 160, val loss: 1.7656605243682861
Epoch 170, training loss: 649.6821899414062 = 1.7567540407180786 + 100.0 * 6.479254245758057
Epoch 170, val loss: 1.7565840482711792
Epoch 180, training loss: 647.8433837890625 = 1.746209740638733 + 100.0 * 6.460971832275391
Epoch 180, val loss: 1.7468441724777222
Epoch 190, training loss: 646.31884765625 = 1.734829306602478 + 100.0 * 6.445840358734131
Epoch 190, val loss: 1.7363855838775635
Epoch 200, training loss: 645.1329956054688 = 1.722446322441101 + 100.0 * 6.434104919433594
Epoch 200, val loss: 1.725077509880066
Epoch 210, training loss: 643.9856567382812 = 1.7088037729263306 + 100.0 * 6.422768592834473
Epoch 210, val loss: 1.712668776512146
Epoch 220, training loss: 642.8978881835938 = 1.694003701210022 + 100.0 * 6.412038326263428
Epoch 220, val loss: 1.6991688013076782
Epoch 230, training loss: 642.1025390625 = 1.6779582500457764 + 100.0 * 6.404245376586914
Epoch 230, val loss: 1.6846396923065186
Epoch 240, training loss: 641.4424438476562 = 1.660600185394287 + 100.0 * 6.397818565368652
Epoch 240, val loss: 1.66884446144104
Epoch 250, training loss: 640.5438842773438 = 1.6419074535369873 + 100.0 * 6.389019966125488
Epoch 250, val loss: 1.6519168615341187
Epoch 260, training loss: 639.7865600585938 = 1.621950387954712 + 100.0 * 6.381646156311035
Epoch 260, val loss: 1.6338818073272705
Epoch 270, training loss: 639.9517822265625 = 1.6007256507873535 + 100.0 * 6.383510589599609
Epoch 270, val loss: 1.6146708726882935
Epoch 280, training loss: 638.8357543945312 = 1.578248143196106 + 100.0 * 6.372575283050537
Epoch 280, val loss: 1.5943286418914795
Epoch 290, training loss: 638.1431274414062 = 1.5547800064086914 + 100.0 * 6.3658833503723145
Epoch 290, val loss: 1.5731127262115479
Epoch 300, training loss: 637.4578857421875 = 1.5304381847381592 + 100.0 * 6.359274387359619
Epoch 300, val loss: 1.5511165857315063
Epoch 310, training loss: 637.4207153320312 = 1.5053523778915405 + 100.0 * 6.3591532707214355
Epoch 310, val loss: 1.5285084247589111
Epoch 320, training loss: 636.6416625976562 = 1.4794435501098633 + 100.0 * 6.351622104644775
Epoch 320, val loss: 1.5052400827407837
Epoch 330, training loss: 636.115478515625 = 1.4532582759857178 + 100.0 * 6.346622467041016
Epoch 330, val loss: 1.4817312955856323
Epoch 340, training loss: 635.6363525390625 = 1.426815152168274 + 100.0 * 6.342095375061035
Epoch 340, val loss: 1.4581140279769897
Epoch 350, training loss: 635.1697998046875 = 1.4002654552459717 + 100.0 * 6.337695598602295
Epoch 350, val loss: 1.4344950914382935
Epoch 360, training loss: 635.6110229492188 = 1.3737777471542358 + 100.0 * 6.342372417449951
Epoch 360, val loss: 1.4109952449798584
Epoch 370, training loss: 634.5701904296875 = 1.3472436666488647 + 100.0 * 6.3322296142578125
Epoch 370, val loss: 1.3876103162765503
Epoch 380, training loss: 634.0526733398438 = 1.3210978507995605 + 100.0 * 6.327315807342529
Epoch 380, val loss: 1.3646166324615479
Epoch 390, training loss: 634.273681640625 = 1.2953022718429565 + 100.0 * 6.329783916473389
Epoch 390, val loss: 1.342051386833191
Epoch 400, training loss: 633.5323486328125 = 1.2700284719467163 + 100.0 * 6.322623252868652
Epoch 400, val loss: 1.3200037479400635
Epoch 410, training loss: 633.08154296875 = 1.2452046871185303 + 100.0 * 6.318363666534424
Epoch 410, val loss: 1.2986042499542236
Epoch 420, training loss: 632.7888793945312 = 1.220940351486206 + 100.0 * 6.31567907333374
Epoch 420, val loss: 1.277838945388794
Epoch 430, training loss: 632.5056762695312 = 1.1971511840820312 + 100.0 * 6.313085556030273
Epoch 430, val loss: 1.2576299905776978
Epoch 440, training loss: 632.1949462890625 = 1.1739391088485718 + 100.0 * 6.31020975112915
Epoch 440, val loss: 1.2380532026290894
Epoch 450, training loss: 631.9187622070312 = 1.1512666940689087 + 100.0 * 6.307675361633301
Epoch 450, val loss: 1.2190487384796143
Epoch 460, training loss: 632.0255737304688 = 1.1291468143463135 + 100.0 * 6.308964252471924
Epoch 460, val loss: 1.2006926536560059
Epoch 470, training loss: 631.68359375 = 1.1074076890945435 + 100.0 * 6.305761814117432
Epoch 470, val loss: 1.1828875541687012
Epoch 480, training loss: 631.3816528320312 = 1.0861068964004517 + 100.0 * 6.302955150604248
Epoch 480, val loss: 1.165600061416626
Epoch 490, training loss: 631.0415649414062 = 1.0653126239776611 + 100.0 * 6.299762725830078
Epoch 490, val loss: 1.1490199565887451
Epoch 500, training loss: 630.8176879882812 = 1.0449285507202148 + 100.0 * 6.297727584838867
Epoch 500, val loss: 1.1329182386398315
Epoch 510, training loss: 630.7802734375 = 1.0248730182647705 + 100.0 * 6.297553539276123
Epoch 510, val loss: 1.117263674736023
Epoch 520, training loss: 630.5737915039062 = 1.0052061080932617 + 100.0 * 6.2956862449646
Epoch 520, val loss: 1.1023178100585938
Epoch 530, training loss: 630.2890014648438 = 0.9857689738273621 + 100.0 * 6.293032169342041
Epoch 530, val loss: 1.0876069068908691
Epoch 540, training loss: 630.0342407226562 = 0.9667736291885376 + 100.0 * 6.290674209594727
Epoch 540, val loss: 1.0735639333724976
Epoch 550, training loss: 629.8873901367188 = 0.9479135274887085 + 100.0 * 6.289394378662109
Epoch 550, val loss: 1.0599066019058228
Epoch 560, training loss: 629.7857055664062 = 0.9294071197509766 + 100.0 * 6.288563251495361
Epoch 560, val loss: 1.0464739799499512
Epoch 570, training loss: 629.42919921875 = 0.9112535119056702 + 100.0 * 6.285179138183594
Epoch 570, val loss: 1.033681869506836
Epoch 580, training loss: 629.211669921875 = 0.8934434056282043 + 100.0 * 6.283182621002197
Epoch 580, val loss: 1.0214735269546509
Epoch 590, training loss: 630.1392211914062 = 0.8757670521736145 + 100.0 * 6.292634010314941
Epoch 590, val loss: 1.00950026512146
Epoch 600, training loss: 629.0447998046875 = 0.8582019209861755 + 100.0 * 6.28186559677124
Epoch 600, val loss: 0.9978917837142944
Epoch 610, training loss: 628.69580078125 = 0.8409853577613831 + 100.0 * 6.278548240661621
Epoch 610, val loss: 0.9867348074913025
Epoch 620, training loss: 628.5781860351562 = 0.8240947723388672 + 100.0 * 6.277541160583496
Epoch 620, val loss: 0.9760702848434448
Epoch 630, training loss: 628.5177001953125 = 0.8074147701263428 + 100.0 * 6.277102947235107
Epoch 630, val loss: 0.9657642245292664
Epoch 640, training loss: 628.9411010742188 = 0.7908862233161926 + 100.0 * 6.2815022468566895
Epoch 640, val loss: 0.9559723138809204
Epoch 650, training loss: 628.2833251953125 = 0.7744849324226379 + 100.0 * 6.275088310241699
Epoch 650, val loss: 0.946068286895752
Epoch 660, training loss: 628.0233764648438 = 0.7584011554718018 + 100.0 * 6.272649765014648
Epoch 660, val loss: 0.9368916749954224
Epoch 670, training loss: 627.8404541015625 = 0.742689847946167 + 100.0 * 6.270977973937988
Epoch 670, val loss: 0.9281781911849976
Epoch 680, training loss: 627.7717895507812 = 0.7272364497184753 + 100.0 * 6.270445823669434
Epoch 680, val loss: 0.9199147820472717
Epoch 690, training loss: 628.0066528320312 = 0.7119661569595337 + 100.0 * 6.272946834564209
Epoch 690, val loss: 0.9119507670402527
Epoch 700, training loss: 627.5009765625 = 0.696813702583313 + 100.0 * 6.268041133880615
Epoch 700, val loss: 0.9042221903800964
Epoch 710, training loss: 627.4151611328125 = 0.6820662021636963 + 100.0 * 6.267330646514893
Epoch 710, val loss: 0.8971810936927795
Epoch 720, training loss: 627.35986328125 = 0.6675974130630493 + 100.0 * 6.266922473907471
Epoch 720, val loss: 0.8904609680175781
Epoch 730, training loss: 627.1068115234375 = 0.6533580422401428 + 100.0 * 6.2645344734191895
Epoch 730, val loss: 0.8841602802276611
Epoch 740, training loss: 627.0881958007812 = 0.6393696665763855 + 100.0 * 6.264488220214844
Epoch 740, val loss: 0.8782197833061218
Epoch 750, training loss: 627.2456665039062 = 0.6257448792457581 + 100.0 * 6.266199588775635
Epoch 750, val loss: 0.8729265928268433
Epoch 760, training loss: 626.7518310546875 = 0.6123201251029968 + 100.0 * 6.261395454406738
Epoch 760, val loss: 0.8677753806114197
Epoch 770, training loss: 626.6370849609375 = 0.5992040634155273 + 100.0 * 6.260379314422607
Epoch 770, val loss: 0.8632174730300903
Epoch 780, training loss: 626.7371826171875 = 0.5863915681838989 + 100.0 * 6.261507987976074
Epoch 780, val loss: 0.85905522108078
Epoch 790, training loss: 626.5635375976562 = 0.5737885236740112 + 100.0 * 6.259897708892822
Epoch 790, val loss: 0.8550287485122681
Epoch 800, training loss: 626.615966796875 = 0.5614196062088013 + 100.0 * 6.26054573059082
Epoch 800, val loss: 0.8514617681503296
Epoch 810, training loss: 626.291015625 = 0.5493322610855103 + 100.0 * 6.25741720199585
Epoch 810, val loss: 0.8483170866966248
Epoch 820, training loss: 626.1419677734375 = 0.5375779867172241 + 100.0 * 6.256043910980225
Epoch 820, val loss: 0.845471978187561
Epoch 830, training loss: 626.0951538085938 = 0.5260633230209351 + 100.0 * 6.255691051483154
Epoch 830, val loss: 0.842917263507843
Epoch 840, training loss: 626.3334350585938 = 0.5147490501403809 + 100.0 * 6.2581868171691895
Epoch 840, val loss: 0.8406716585159302
Epoch 850, training loss: 626.3695068359375 = 0.5035015940666199 + 100.0 * 6.258660316467285
Epoch 850, val loss: 0.8385295271873474
Epoch 860, training loss: 626.10498046875 = 0.49256572127342224 + 100.0 * 6.256124019622803
Epoch 860, val loss: 0.8367323279380798
Epoch 870, training loss: 625.8519287109375 = 0.48181241750717163 + 100.0 * 6.253701210021973
Epoch 870, val loss: 0.8351297378540039
Epoch 880, training loss: 625.7132568359375 = 0.4713755249977112 + 100.0 * 6.2524189949035645
Epoch 880, val loss: 0.8338102698326111
Epoch 890, training loss: 625.509521484375 = 0.46112048625946045 + 100.0 * 6.250483989715576
Epoch 890, val loss: 0.8328080177307129
Epoch 900, training loss: 626.1063232421875 = 0.45108142495155334 + 100.0 * 6.256552219390869
Epoch 900, val loss: 0.8320807218551636
Epoch 910, training loss: 625.6327514648438 = 0.4410652220249176 + 100.0 * 6.251916885375977
Epoch 910, val loss: 0.8311851024627686
Epoch 920, training loss: 625.5753173828125 = 0.43130630254745483 + 100.0 * 6.251439571380615
Epoch 920, val loss: 0.8308769464492798
Epoch 930, training loss: 625.1700439453125 = 0.42175936698913574 + 100.0 * 6.247482776641846
Epoch 930, val loss: 0.8307054042816162
Epoch 940, training loss: 625.314208984375 = 0.4124292731285095 + 100.0 * 6.24901819229126
Epoch 940, val loss: 0.8306950330734253
Epoch 950, training loss: 625.4637451171875 = 0.4032075107097626 + 100.0 * 6.250605583190918
Epoch 950, val loss: 0.8309736847877502
Epoch 960, training loss: 625.0953369140625 = 0.3942469656467438 + 100.0 * 6.247011184692383
Epoch 960, val loss: 0.8313300609588623
Epoch 970, training loss: 625.0040283203125 = 0.3854358196258545 + 100.0 * 6.246185779571533
Epoch 970, val loss: 0.8320258855819702
Epoch 980, training loss: 625.015625 = 0.37681612372398376 + 100.0 * 6.246387958526611
Epoch 980, val loss: 0.8328536152839661
Epoch 990, training loss: 624.7717895507812 = 0.3683900237083435 + 100.0 * 6.2440338134765625
Epoch 990, val loss: 0.8340123891830444
Epoch 1000, training loss: 624.9762573242188 = 0.3601318895816803 + 100.0 * 6.246161460876465
Epoch 1000, val loss: 0.8354900479316711
Epoch 1010, training loss: 624.6482543945312 = 0.3520546853542328 + 100.0 * 6.242961883544922
Epoch 1010, val loss: 0.836870014667511
Epoch 1020, training loss: 624.5177001953125 = 0.34417814016342163 + 100.0 * 6.241734981536865
Epoch 1020, val loss: 0.838908851146698
Epoch 1030, training loss: 624.763671875 = 0.3364770710468292 + 100.0 * 6.244271755218506
Epoch 1030, val loss: 0.8411794900894165
Epoch 1040, training loss: 624.6176147460938 = 0.3288765549659729 + 100.0 * 6.242887496948242
Epoch 1040, val loss: 0.8434575200080872
Epoch 1050, training loss: 624.4498901367188 = 0.32148364186286926 + 100.0 * 6.241284370422363
Epoch 1050, val loss: 0.8461679220199585
Epoch 1060, training loss: 624.2535400390625 = 0.31434085965156555 + 100.0 * 6.239391803741455
Epoch 1060, val loss: 0.8494277596473694
Epoch 1070, training loss: 624.230712890625 = 0.307426780462265 + 100.0 * 6.239233016967773
Epoch 1070, val loss: 0.8529847264289856
Epoch 1080, training loss: 624.9926147460938 = 0.30068886280059814 + 100.0 * 6.246919631958008
Epoch 1080, val loss: 0.8568243980407715
Epoch 1090, training loss: 624.427001953125 = 0.2939586639404297 + 100.0 * 6.241330623626709
Epoch 1090, val loss: 0.860735297203064
Epoch 1100, training loss: 624.1810913085938 = 0.2874639630317688 + 100.0 * 6.238935947418213
Epoch 1100, val loss: 0.8650703430175781
Epoch 1110, training loss: 624.1102905273438 = 0.2811877131462097 + 100.0 * 6.238291263580322
Epoch 1110, val loss: 0.8697055578231812
Epoch 1120, training loss: 623.9134521484375 = 0.27511170506477356 + 100.0 * 6.236383438110352
Epoch 1120, val loss: 0.8744333982467651
Epoch 1130, training loss: 624.1859741210938 = 0.269216388463974 + 100.0 * 6.2391676902771
Epoch 1130, val loss: 0.8793959021568298
Epoch 1140, training loss: 623.7662353515625 = 0.263418048620224 + 100.0 * 6.23502779006958
Epoch 1140, val loss: 0.8845323324203491
Epoch 1150, training loss: 623.9483032226562 = 0.2578090727329254 + 100.0 * 6.236905097961426
Epoch 1150, val loss: 0.8897318243980408
Epoch 1160, training loss: 624.3303833007812 = 0.25232911109924316 + 100.0 * 6.240780830383301
Epoch 1160, val loss: 0.8951713442802429
Epoch 1170, training loss: 623.7543334960938 = 0.246862530708313 + 100.0 * 6.235074520111084
Epoch 1170, val loss: 0.9004961848258972
Epoch 1180, training loss: 623.6229248046875 = 0.24166570603847504 + 100.0 * 6.2338128089904785
Epoch 1180, val loss: 0.9061914682388306
Epoch 1190, training loss: 623.5247192382812 = 0.23663583397865295 + 100.0 * 6.232880592346191
Epoch 1190, val loss: 0.9117979407310486
Epoch 1200, training loss: 623.8717651367188 = 0.23176157474517822 + 100.0 * 6.236400127410889
Epoch 1200, val loss: 0.9176000356674194
Epoch 1210, training loss: 623.5291137695312 = 0.2268703430891037 + 100.0 * 6.233022689819336
Epoch 1210, val loss: 0.9231801629066467
Epoch 1220, training loss: 623.5320434570312 = 0.22215676307678223 + 100.0 * 6.233098983764648
Epoch 1220, val loss: 0.9290214776992798
Epoch 1230, training loss: 623.5272216796875 = 0.21758031845092773 + 100.0 * 6.233096599578857
Epoch 1230, val loss: 0.9349175095558167
Epoch 1240, training loss: 623.375244140625 = 0.2131233960390091 + 100.0 * 6.231621265411377
Epoch 1240, val loss: 0.9407995939254761
Epoch 1250, training loss: 623.691162109375 = 0.2087889015674591 + 100.0 * 6.234823703765869
Epoch 1250, val loss: 0.9467540383338928
Epoch 1260, training loss: 623.3975830078125 = 0.2045057713985443 + 100.0 * 6.231930732727051
Epoch 1260, val loss: 0.9527168273925781
Epoch 1270, training loss: 623.279296875 = 0.20032572746276855 + 100.0 * 6.230789661407471
Epoch 1270, val loss: 0.9587030410766602
Epoch 1280, training loss: 623.213623046875 = 0.1963106095790863 + 100.0 * 6.230173587799072
Epoch 1280, val loss: 0.9648745059967041
Epoch 1290, training loss: 623.3226318359375 = 0.19239181280136108 + 100.0 * 6.231302261352539
Epoch 1290, val loss: 0.970995306968689
Epoch 1300, training loss: 623.0894165039062 = 0.18853241205215454 + 100.0 * 6.229008674621582
Epoch 1300, val loss: 0.9772422313690186
Epoch 1310, training loss: 623.831787109375 = 0.1847507357597351 + 100.0 * 6.2364702224731445
Epoch 1310, val loss: 0.9835198521614075
Epoch 1320, training loss: 623.2059936523438 = 0.1810406595468521 + 100.0 * 6.230249404907227
Epoch 1320, val loss: 0.9894202351570129
Epoch 1330, training loss: 622.986572265625 = 0.17744098603725433 + 100.0 * 6.228091239929199
Epoch 1330, val loss: 0.9958080053329468
Epoch 1340, training loss: 622.9725952148438 = 0.17396724224090576 + 100.0 * 6.2279863357543945
Epoch 1340, val loss: 1.0021467208862305
Epoch 1350, training loss: 623.247314453125 = 0.17054970562458038 + 100.0 * 6.230767726898193
Epoch 1350, val loss: 1.008485198020935
Epoch 1360, training loss: 622.9124755859375 = 0.16718514263629913 + 100.0 * 6.227453231811523
Epoch 1360, val loss: 1.0144832134246826
Epoch 1370, training loss: 622.8316040039062 = 0.16392658650875092 + 100.0 * 6.2266764640808105
Epoch 1370, val loss: 1.0208598375320435
Epoch 1380, training loss: 623.2784423828125 = 0.16073808073997498 + 100.0 * 6.231176853179932
Epoch 1380, val loss: 1.0270895957946777
Epoch 1390, training loss: 623.0125732421875 = 0.1575975865125656 + 100.0 * 6.228549957275391
Epoch 1390, val loss: 1.0330742597579956
Epoch 1400, training loss: 622.7882080078125 = 0.1545068472623825 + 100.0 * 6.226337432861328
Epoch 1400, val loss: 1.0394350290298462
Epoch 1410, training loss: 622.6213989257812 = 0.15153105556964874 + 100.0 * 6.224698543548584
Epoch 1410, val loss: 1.045857548713684
Epoch 1420, training loss: 622.6513671875 = 0.14863470196723938 + 100.0 * 6.225027561187744
Epoch 1420, val loss: 1.0523005723953247
Epoch 1430, training loss: 623.2776489257812 = 0.14578655362129211 + 100.0 * 6.231318473815918
Epoch 1430, val loss: 1.0585771799087524
Epoch 1440, training loss: 622.92333984375 = 0.14294812083244324 + 100.0 * 6.227804183959961
Epoch 1440, val loss: 1.0646123886108398
Epoch 1450, training loss: 622.552490234375 = 0.14019857347011566 + 100.0 * 6.224123001098633
Epoch 1450, val loss: 1.0709526538848877
Epoch 1460, training loss: 622.4309692382812 = 0.13753154873847961 + 100.0 * 6.222934722900391
Epoch 1460, val loss: 1.0775800943374634
Epoch 1470, training loss: 622.3956909179688 = 0.13495232164859772 + 100.0 * 6.222607135772705
Epoch 1470, val loss: 1.0839658975601196
Epoch 1480, training loss: 622.955810546875 = 0.13243161141872406 + 100.0 * 6.228233337402344
Epoch 1480, val loss: 1.090255856513977
Epoch 1490, training loss: 622.6953125 = 0.12990988790988922 + 100.0 * 6.225654125213623
Epoch 1490, val loss: 1.0964497327804565
Epoch 1500, training loss: 622.48779296875 = 0.12741617858409882 + 100.0 * 6.22360372543335
Epoch 1500, val loss: 1.1025158166885376
Epoch 1510, training loss: 622.3157958984375 = 0.12503491342067719 + 100.0 * 6.221907615661621
Epoch 1510, val loss: 1.109147548675537
Epoch 1520, training loss: 622.6224365234375 = 0.12271831929683685 + 100.0 * 6.224997043609619
Epoch 1520, val loss: 1.1154099702835083
Epoch 1530, training loss: 622.2621459960938 = 0.12040920555591583 + 100.0 * 6.22141695022583
Epoch 1530, val loss: 1.1216672658920288
Epoch 1540, training loss: 622.6490478515625 = 0.11816110461950302 + 100.0 * 6.225308895111084
Epoch 1540, val loss: 1.1279466152191162
Epoch 1550, training loss: 622.4017333984375 = 0.11594610661268234 + 100.0 * 6.22285795211792
Epoch 1550, val loss: 1.1340951919555664
Epoch 1560, training loss: 622.1812744140625 = 0.11378912627696991 + 100.0 * 6.220674991607666
Epoch 1560, val loss: 1.1403732299804688
Epoch 1570, training loss: 622.2244873046875 = 0.11169875413179398 + 100.0 * 6.221127986907959
Epoch 1570, val loss: 1.1466399431228638
Epoch 1580, training loss: 622.2445678710938 = 0.10964056849479675 + 100.0 * 6.221349239349365
Epoch 1580, val loss: 1.1528483629226685
Epoch 1590, training loss: 622.2006225585938 = 0.10761784017086029 + 100.0 * 6.220930099487305
Epoch 1590, val loss: 1.159005045890808
Epoch 1600, training loss: 622.1331176757812 = 0.10564073175191879 + 100.0 * 6.220274448394775
Epoch 1600, val loss: 1.1649402379989624
Epoch 1610, training loss: 623.0847778320312 = 0.10371218621730804 + 100.0 * 6.22981071472168
Epoch 1610, val loss: 1.1709479093551636
Epoch 1620, training loss: 622.317138671875 = 0.10176777094602585 + 100.0 * 6.222154140472412
Epoch 1620, val loss: 1.1767431497573853
Epoch 1630, training loss: 622.0111083984375 = 0.09989245235919952 + 100.0 * 6.219111919403076
Epoch 1630, val loss: 1.1829447746276855
Epoch 1640, training loss: 621.9285278320312 = 0.09809977561235428 + 100.0 * 6.21830415725708
Epoch 1640, val loss: 1.1890645027160645
Epoch 1650, training loss: 622.480712890625 = 0.09634967893362045 + 100.0 * 6.223843097686768
Epoch 1650, val loss: 1.195090651512146
Epoch 1660, training loss: 621.9610595703125 = 0.09457516670227051 + 100.0 * 6.218664646148682
Epoch 1660, val loss: 1.200849175453186
Epoch 1670, training loss: 622.1255493164062 = 0.09286950528621674 + 100.0 * 6.2203264236450195
Epoch 1670, val loss: 1.2067348957061768
Epoch 1680, training loss: 621.851806640625 = 0.0911722183227539 + 100.0 * 6.217606067657471
Epoch 1680, val loss: 1.212800145149231
Epoch 1690, training loss: 621.8599853515625 = 0.08953828364610672 + 100.0 * 6.2177042961120605
Epoch 1690, val loss: 1.2188369035720825
Epoch 1700, training loss: 621.9398803710938 = 0.0879494696855545 + 100.0 * 6.21851921081543
Epoch 1700, val loss: 1.224662184715271
Epoch 1710, training loss: 622.0028076171875 = 0.08639468997716904 + 100.0 * 6.2191643714904785
Epoch 1710, val loss: 1.2303653955459595
Epoch 1720, training loss: 621.6791381835938 = 0.08483098447322845 + 100.0 * 6.215942859649658
Epoch 1720, val loss: 1.2362585067749023
Epoch 1730, training loss: 622.021728515625 = 0.08334401994943619 + 100.0 * 6.219383716583252
Epoch 1730, val loss: 1.2421616315841675
Epoch 1740, training loss: 622.0731201171875 = 0.0818607360124588 + 100.0 * 6.219912528991699
Epoch 1740, val loss: 1.2477432489395142
Epoch 1750, training loss: 621.7149047851562 = 0.08035989105701447 + 100.0 * 6.216345310211182
Epoch 1750, val loss: 1.2536062002182007
Epoch 1760, training loss: 621.6162109375 = 0.07894819974899292 + 100.0 * 6.2153730392456055
Epoch 1760, val loss: 1.2594350576400757
Epoch 1770, training loss: 621.5975952148438 = 0.07757627218961716 + 100.0 * 6.215199947357178
Epoch 1770, val loss: 1.2653342485427856
Epoch 1780, training loss: 622.3016357421875 = 0.07625720649957657 + 100.0 * 6.222253799438477
Epoch 1780, val loss: 1.270856261253357
Epoch 1790, training loss: 621.65966796875 = 0.07486724853515625 + 100.0 * 6.215847492218018
Epoch 1790, val loss: 1.2765182256698608
Epoch 1800, training loss: 621.5146484375 = 0.07355839759111404 + 100.0 * 6.214410781860352
Epoch 1800, val loss: 1.2821754217147827
Epoch 1810, training loss: 621.6312255859375 = 0.07229378819465637 + 100.0 * 6.21558952331543
Epoch 1810, val loss: 1.2879024744033813
Epoch 1820, training loss: 621.6737060546875 = 0.07104459404945374 + 100.0 * 6.216026306152344
Epoch 1820, val loss: 1.293492317199707
Epoch 1830, training loss: 621.70947265625 = 0.06981212645769119 + 100.0 * 6.216396331787109
Epoch 1830, val loss: 1.2990950345993042
Epoch 1840, training loss: 621.67919921875 = 0.06859530508518219 + 100.0 * 6.216105937957764
Epoch 1840, val loss: 1.3045880794525146
Epoch 1850, training loss: 621.4338989257812 = 0.06738689541816711 + 100.0 * 6.213665008544922
Epoch 1850, val loss: 1.3104058504104614
Epoch 1860, training loss: 621.3790893554688 = 0.06623871624469757 + 100.0 * 6.213128566741943
Epoch 1860, val loss: 1.3159403800964355
Epoch 1870, training loss: 621.9417114257812 = 0.06513891369104385 + 100.0 * 6.218765735626221
Epoch 1870, val loss: 1.3215315341949463
Epoch 1880, training loss: 621.3873901367188 = 0.06399126350879669 + 100.0 * 6.2132344245910645
Epoch 1880, val loss: 1.3270328044891357
Epoch 1890, training loss: 621.3528442382812 = 0.06290612369775772 + 100.0 * 6.212899208068848
Epoch 1890, val loss: 1.3326045274734497
Epoch 1900, training loss: 621.523681640625 = 0.06185435876250267 + 100.0 * 6.214618682861328
Epoch 1900, val loss: 1.3379826545715332
Epoch 1910, training loss: 621.3148193359375 = 0.060796212404966354 + 100.0 * 6.212540149688721
Epoch 1910, val loss: 1.3435355424880981
Epoch 1920, training loss: 621.7182006835938 = 0.05977647006511688 + 100.0 * 6.2165846824646
Epoch 1920, val loss: 1.3489755392074585
Epoch 1930, training loss: 621.2882080078125 = 0.0587577186524868 + 100.0 * 6.212294578552246
Epoch 1930, val loss: 1.3542433977127075
Epoch 1940, training loss: 621.2521362304688 = 0.057768452912569046 + 100.0 * 6.211944103240967
Epoch 1940, val loss: 1.359474539756775
Epoch 1950, training loss: 621.3106689453125 = 0.0568164587020874 + 100.0 * 6.212538719177246
Epoch 1950, val loss: 1.3650131225585938
Epoch 1960, training loss: 621.4430541992188 = 0.05587751418352127 + 100.0 * 6.213871955871582
Epoch 1960, val loss: 1.3704657554626465
Epoch 1970, training loss: 621.56884765625 = 0.054955195635557175 + 100.0 * 6.215139389038086
Epoch 1970, val loss: 1.3758748769760132
Epoch 1980, training loss: 621.1529541015625 = 0.054030485451221466 + 100.0 * 6.210989475250244
Epoch 1980, val loss: 1.3809810876846313
Epoch 1990, training loss: 621.0809936523438 = 0.05314704030752182 + 100.0 * 6.210278034210205
Epoch 1990, val loss: 1.386460304260254
Epoch 2000, training loss: 621.36328125 = 0.05230022221803665 + 100.0 * 6.213109970092773
Epoch 2000, val loss: 1.3916983604431152
Epoch 2010, training loss: 621.2529296875 = 0.05143722891807556 + 100.0 * 6.212014675140381
Epoch 2010, val loss: 1.3967316150665283
Epoch 2020, training loss: 621.1368408203125 = 0.05058551952242851 + 100.0 * 6.210862159729004
Epoch 2020, val loss: 1.4019768238067627
Epoch 2030, training loss: 621.2068481445312 = 0.0497710257768631 + 100.0 * 6.211571216583252
Epoch 2030, val loss: 1.4072071313858032
Epoch 2040, training loss: 621.3028564453125 = 0.04897483065724373 + 100.0 * 6.212539196014404
Epoch 2040, val loss: 1.4123163223266602
Epoch 2050, training loss: 621.5385131835938 = 0.048192400485277176 + 100.0 * 6.214902877807617
Epoch 2050, val loss: 1.417340636253357
Epoch 2060, training loss: 621.0654296875 = 0.04738844558596611 + 100.0 * 6.210180759429932
Epoch 2060, val loss: 1.4225822687149048
Epoch 2070, training loss: 620.924072265625 = 0.04663945361971855 + 100.0 * 6.208774566650391
Epoch 2070, val loss: 1.4277218580245972
Epoch 2080, training loss: 620.8836059570312 = 0.045910052955150604 + 100.0 * 6.208376884460449
Epoch 2080, val loss: 1.4329866170883179
Epoch 2090, training loss: 620.9642944335938 = 0.045200325548648834 + 100.0 * 6.209190368652344
Epoch 2090, val loss: 1.4380478858947754
Epoch 2100, training loss: 621.6749877929688 = 0.04450274631381035 + 100.0 * 6.216304779052734
Epoch 2100, val loss: 1.4427986145019531
Epoch 2110, training loss: 621.0399169921875 = 0.04376957193017006 + 100.0 * 6.209961414337158
Epoch 2110, val loss: 1.4478703737258911
Epoch 2120, training loss: 620.833740234375 = 0.043078165501356125 + 100.0 * 6.207906723022461
Epoch 2120, val loss: 1.4528472423553467
Epoch 2130, training loss: 620.9546508789062 = 0.04242464900016785 + 100.0 * 6.209122180938721
Epoch 2130, val loss: 1.4579360485076904
Epoch 2140, training loss: 621.1503295898438 = 0.04177166149020195 + 100.0 * 6.211085796356201
Epoch 2140, val loss: 1.4626412391662598
Epoch 2150, training loss: 620.9542846679688 = 0.04111860319972038 + 100.0 * 6.209131240844727
Epoch 2150, val loss: 1.4678581953048706
Epoch 2160, training loss: 620.8775634765625 = 0.040490128099918365 + 100.0 * 6.208371162414551
Epoch 2160, val loss: 1.4725474119186401
Epoch 2170, training loss: 621.0064086914062 = 0.03987685590982437 + 100.0 * 6.209665298461914
Epoch 2170, val loss: 1.4774051904678345
Epoch 2180, training loss: 621.1008911132812 = 0.03926545009016991 + 100.0 * 6.210616588592529
Epoch 2180, val loss: 1.48226797580719
Epoch 2190, training loss: 621.130126953125 = 0.03866264596581459 + 100.0 * 6.2109150886535645
Epoch 2190, val loss: 1.4870095252990723
Epoch 2200, training loss: 621.0473022460938 = 0.038060661405324936 + 100.0 * 6.210092067718506
Epoch 2200, val loss: 1.4915646314620972
Epoch 2210, training loss: 620.77001953125 = 0.037477921694517136 + 100.0 * 6.207325458526611
Epoch 2210, val loss: 1.4965540170669556
Epoch 2220, training loss: 620.6764526367188 = 0.036917544901371 + 100.0 * 6.206395149230957
Epoch 2220, val loss: 1.5014766454696655
Epoch 2230, training loss: 620.6276245117188 = 0.03638216108083725 + 100.0 * 6.2059125900268555
Epoch 2230, val loss: 1.506333351135254
Epoch 2240, training loss: 620.59814453125 = 0.03585755079984665 + 100.0 * 6.205623149871826
Epoch 2240, val loss: 1.5110282897949219
Epoch 2250, training loss: 620.9435424804688 = 0.03535152226686478 + 100.0 * 6.209082126617432
Epoch 2250, val loss: 1.5155819654464722
Epoch 2260, training loss: 620.810302734375 = 0.03481527790427208 + 100.0 * 6.207755088806152
Epoch 2260, val loss: 1.5199247598648071
Epoch 2270, training loss: 620.6045532226562 = 0.03429688513278961 + 100.0 * 6.205702781677246
Epoch 2270, val loss: 1.5245156288146973
Epoch 2280, training loss: 620.5858764648438 = 0.033792730420827866 + 100.0 * 6.2055206298828125
Epoch 2280, val loss: 1.5294997692108154
Epoch 2290, training loss: 621.2188110351562 = 0.03332601487636566 + 100.0 * 6.211854934692383
Epoch 2290, val loss: 1.534096121788025
Epoch 2300, training loss: 620.6900634765625 = 0.032815102487802505 + 100.0 * 6.206572532653809
Epoch 2300, val loss: 1.5384130477905273
Epoch 2310, training loss: 620.5462646484375 = 0.03233928605914116 + 100.0 * 6.20513916015625
Epoch 2310, val loss: 1.5431538820266724
Epoch 2320, training loss: 620.4959106445312 = 0.031883034855127335 + 100.0 * 6.2046403884887695
Epoch 2320, val loss: 1.5478308200836182
Epoch 2330, training loss: 620.5288696289062 = 0.03144122287631035 + 100.0 * 6.204974174499512
Epoch 2330, val loss: 1.5525262355804443
Epoch 2340, training loss: 621.0769653320312 = 0.031010065227746964 + 100.0 * 6.2104597091674805
Epoch 2340, val loss: 1.5570521354675293
Epoch 2350, training loss: 620.7503051757812 = 0.03055967018008232 + 100.0 * 6.207197189331055
Epoch 2350, val loss: 1.5607969760894775
Epoch 2360, training loss: 620.5601196289062 = 0.030120162293314934 + 100.0 * 6.205300331115723
Epoch 2360, val loss: 1.5654841661453247
Epoch 2370, training loss: 620.8822631835938 = 0.029713643714785576 + 100.0 * 6.208525657653809
Epoch 2370, val loss: 1.5699070692062378
Epoch 2380, training loss: 620.4049682617188 = 0.029282068833708763 + 100.0 * 6.203756809234619
Epoch 2380, val loss: 1.5744662284851074
Epoch 2390, training loss: 620.454833984375 = 0.028884705156087875 + 100.0 * 6.204259872436523
Epoch 2390, val loss: 1.5789077281951904
Epoch 2400, training loss: 620.4068603515625 = 0.02849329262971878 + 100.0 * 6.2037835121154785
Epoch 2400, val loss: 1.5832408666610718
Epoch 2410, training loss: 621.5599365234375 = 0.02812112122774124 + 100.0 * 6.215318202972412
Epoch 2410, val loss: 1.5873750448226929
Epoch 2420, training loss: 620.7385864257812 = 0.02771318145096302 + 100.0 * 6.207108974456787
Epoch 2420, val loss: 1.59129798412323
Epoch 2430, training loss: 620.4075317382812 = 0.02732636034488678 + 100.0 * 6.20380163192749
Epoch 2430, val loss: 1.595889687538147
Epoch 2440, training loss: 620.2915649414062 = 0.026963599026203156 + 100.0 * 6.202645778656006
Epoch 2440, val loss: 1.6002840995788574
Epoch 2450, training loss: 620.3450317382812 = 0.02661377750337124 + 100.0 * 6.203184127807617
Epoch 2450, val loss: 1.6045870780944824
Epoch 2460, training loss: 621.04296875 = 0.026271838694810867 + 100.0 * 6.210166931152344
Epoch 2460, val loss: 1.6086959838867188
Epoch 2470, training loss: 621.020751953125 = 0.025917960330843925 + 100.0 * 6.2099480628967285
Epoch 2470, val loss: 1.6122617721557617
Epoch 2480, training loss: 620.4140014648438 = 0.02554871141910553 + 100.0 * 6.203884124755859
Epoch 2480, val loss: 1.6167068481445312
Epoch 2490, training loss: 620.3114624023438 = 0.02520996890962124 + 100.0 * 6.20286226272583
Epoch 2490, val loss: 1.6209759712219238
Epoch 2500, training loss: 620.2532958984375 = 0.024886343628168106 + 100.0 * 6.20228385925293
Epoch 2500, val loss: 1.6253430843353271
Epoch 2510, training loss: 620.571533203125 = 0.024576200172305107 + 100.0 * 6.205469131469727
Epoch 2510, val loss: 1.629274606704712
Epoch 2520, training loss: 620.4342651367188 = 0.02425127476453781 + 100.0 * 6.204100131988525
Epoch 2520, val loss: 1.6329677104949951
Epoch 2530, training loss: 620.2781372070312 = 0.023931603878736496 + 100.0 * 6.202542304992676
Epoch 2530, val loss: 1.6370244026184082
Epoch 2540, training loss: 620.3760986328125 = 0.023623846471309662 + 100.0 * 6.203524589538574
Epoch 2540, val loss: 1.6413177251815796
Epoch 2550, training loss: 620.3140869140625 = 0.02331535331904888 + 100.0 * 6.202907562255859
Epoch 2550, val loss: 1.6451416015625
Epoch 2560, training loss: 620.1456909179688 = 0.023018984124064445 + 100.0 * 6.201226711273193
Epoch 2560, val loss: 1.6491917371749878
Epoch 2570, training loss: 620.2803955078125 = 0.02273804321885109 + 100.0 * 6.202576160430908
Epoch 2570, val loss: 1.65315842628479
Epoch 2580, training loss: 620.6716918945312 = 0.022453228011727333 + 100.0 * 6.2064924240112305
Epoch 2580, val loss: 1.657135009765625
Epoch 2590, training loss: 620.2360229492188 = 0.022161902859807014 + 100.0 * 6.202138423919678
Epoch 2590, val loss: 1.6607340574264526
Epoch 2600, training loss: 620.2639770507812 = 0.021889101713895798 + 100.0 * 6.202420711517334
Epoch 2600, val loss: 1.6646544933319092
Epoch 2610, training loss: 620.5590209960938 = 0.02162208966910839 + 100.0 * 6.205374240875244
Epoch 2610, val loss: 1.66865873336792
Epoch 2620, training loss: 620.2321166992188 = 0.02134491503238678 + 100.0 * 6.2021074295043945
Epoch 2620, val loss: 1.6721247434616089
Epoch 2630, training loss: 620.1409912109375 = 0.021083174273371696 + 100.0 * 6.201199054718018
Epoch 2630, val loss: 1.6760616302490234
Epoch 2640, training loss: 620.02783203125 = 0.02082763984799385 + 100.0 * 6.200069904327393
Epoch 2640, val loss: 1.6799437999725342
Epoch 2650, training loss: 620.3187255859375 = 0.020585518330335617 + 100.0 * 6.202981472015381
Epoch 2650, val loss: 1.6836535930633545
Epoch 2660, training loss: 620.22412109375 = 0.02032981999218464 + 100.0 * 6.202037811279297
Epoch 2660, val loss: 1.6870877742767334
Epoch 2670, training loss: 620.5343627929688 = 0.02007920667529106 + 100.0 * 6.205142974853516
Epoch 2670, val loss: 1.6905537843704224
Epoch 2680, training loss: 620.190673828125 = 0.019832808524370193 + 100.0 * 6.2017083168029785
Epoch 2680, val loss: 1.6943217515945435
Epoch 2690, training loss: 620.0447387695312 = 0.019596818834543228 + 100.0 * 6.200251579284668
Epoch 2690, val loss: 1.6979666948318481
Epoch 2700, training loss: 620.0140380859375 = 0.019368775188922882 + 100.0 * 6.199946880340576
Epoch 2700, val loss: 1.7017322778701782
Epoch 2710, training loss: 620.3013305664062 = 0.0191506240516901 + 100.0 * 6.202821731567383
Epoch 2710, val loss: 1.7051846981048584
Epoch 2720, training loss: 620.1393432617188 = 0.018921444192528725 + 100.0 * 6.201204299926758
Epoch 2720, val loss: 1.7084351778030396
Epoch 2730, training loss: 620.168701171875 = 0.01869232952594757 + 100.0 * 6.201500415802002
Epoch 2730, val loss: 1.7122838497161865
Epoch 2740, training loss: 620.2686767578125 = 0.01847129315137863 + 100.0 * 6.2025017738342285
Epoch 2740, val loss: 1.7157726287841797
Epoch 2750, training loss: 620.00244140625 = 0.018255403265357018 + 100.0 * 6.1998419761657715
Epoch 2750, val loss: 1.7193154096603394
Epoch 2760, training loss: 620.002685546875 = 0.018047833815217018 + 100.0 * 6.199846267700195
Epoch 2760, val loss: 1.722983956336975
Epoch 2770, training loss: 620.2105712890625 = 0.017848754301667213 + 100.0 * 6.201927661895752
Epoch 2770, val loss: 1.7263457775115967
Epoch 2780, training loss: 619.9063110351562 = 0.01763755828142166 + 100.0 * 6.198886871337891
Epoch 2780, val loss: 1.7294353246688843
Epoch 2790, training loss: 620.0678100585938 = 0.0174406785517931 + 100.0 * 6.200503826141357
Epoch 2790, val loss: 1.7328544855117798
Epoch 2800, training loss: 620.1565551757812 = 0.01724892295897007 + 100.0 * 6.201392650604248
Epoch 2800, val loss: 1.736340880393982
Epoch 2810, training loss: 619.9528198242188 = 0.017049601301550865 + 100.0 * 6.199357986450195
Epoch 2810, val loss: 1.7397111654281616
Epoch 2820, training loss: 619.9953002929688 = 0.016857508569955826 + 100.0 * 6.199784278869629
Epoch 2820, val loss: 1.7429059743881226
Epoch 2830, training loss: 619.9793701171875 = 0.01667427085340023 + 100.0 * 6.199626922607422
Epoch 2830, val loss: 1.7462832927703857
Epoch 2840, training loss: 620.1221313476562 = 0.016494613140821457 + 100.0 * 6.201056480407715
Epoch 2840, val loss: 1.7497187852859497
Epoch 2850, training loss: 620.1265869140625 = 0.016307175159454346 + 100.0 * 6.2011027336120605
Epoch 2850, val loss: 1.752718448638916
Epoch 2860, training loss: 619.895263671875 = 0.01611829176545143 + 100.0 * 6.19879150390625
Epoch 2860, val loss: 1.756011724472046
Epoch 2870, training loss: 619.8058471679688 = 0.01594792865216732 + 100.0 * 6.197899341583252
Epoch 2870, val loss: 1.7593456506729126
Epoch 2880, training loss: 620.0195922851562 = 0.01578335650265217 + 100.0 * 6.200037956237793
Epoch 2880, val loss: 1.7626079320907593
Epoch 2890, training loss: 619.9395141601562 = 0.01561057846993208 + 100.0 * 6.1992387771606445
Epoch 2890, val loss: 1.7654865980148315
Epoch 2900, training loss: 619.9227294921875 = 0.015440970659255981 + 100.0 * 6.19907283782959
Epoch 2900, val loss: 1.76876962184906
Epoch 2910, training loss: 619.8505859375 = 0.015276283025741577 + 100.0 * 6.198353290557861
Epoch 2910, val loss: 1.7718738317489624
Epoch 2920, training loss: 619.8074340820312 = 0.015115244314074516 + 100.0 * 6.197923183441162
Epoch 2920, val loss: 1.7752078771591187
Epoch 2930, training loss: 619.8124389648438 = 0.01495674904435873 + 100.0 * 6.197975158691406
Epoch 2930, val loss: 1.7783173322677612
Epoch 2940, training loss: 620.3883056640625 = 0.014805839397013187 + 100.0 * 6.203734874725342
Epoch 2940, val loss: 1.7815405130386353
Epoch 2950, training loss: 619.9658203125 = 0.014645936898887157 + 100.0 * 6.199511528015137
Epoch 2950, val loss: 1.7838374376296997
Epoch 2960, training loss: 619.8015747070312 = 0.014480240643024445 + 100.0 * 6.197871208190918
Epoch 2960, val loss: 1.7870596647262573
Epoch 2970, training loss: 619.7052612304688 = 0.014335287734866142 + 100.0 * 6.196909427642822
Epoch 2970, val loss: 1.7904480695724487
Epoch 2980, training loss: 619.8177490234375 = 0.014193476177752018 + 100.0 * 6.19803524017334
Epoch 2980, val loss: 1.7936712503433228
Epoch 2990, training loss: 620.0103759765625 = 0.014051814563572407 + 100.0 * 6.199963569641113
Epoch 2990, val loss: 1.796566128730774
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 861.6157836914062 = 1.9318288564682007 + 100.0 * 8.596839904785156
Epoch 0, val loss: 1.9233014583587646
Epoch 10, training loss: 861.524658203125 = 1.9233224391937256 + 100.0 * 8.596013069152832
Epoch 10, val loss: 1.9151568412780762
Epoch 20, training loss: 860.9092407226562 = 1.912941813468933 + 100.0 * 8.58996295928955
Epoch 20, val loss: 1.904969334602356
Epoch 30, training loss: 857.02978515625 = 1.8998883962631226 + 100.0 * 8.551299095153809
Epoch 30, val loss: 1.8919119834899902
Epoch 40, training loss: 838.45751953125 = 1.8850724697113037 + 100.0 * 8.365724563598633
Epoch 40, val loss: 1.8774348497390747
Epoch 50, training loss: 785.0757446289062 = 1.8702133893966675 + 100.0 * 7.83205509185791
Epoch 50, val loss: 1.8627393245697021
Epoch 60, training loss: 741.0239868164062 = 1.855921745300293 + 100.0 * 7.391680717468262
Epoch 60, val loss: 1.8488998413085938
Epoch 70, training loss: 713.3070678710938 = 1.8429571390151978 + 100.0 * 7.114641189575195
Epoch 70, val loss: 1.8356029987335205
Epoch 80, training loss: 698.0939331054688 = 1.8306597471237183 + 100.0 * 6.962632179260254
Epoch 80, val loss: 1.8235872983932495
Epoch 90, training loss: 688.5635375976562 = 1.8197263479232788 + 100.0 * 6.867438316345215
Epoch 90, val loss: 1.812899112701416
Epoch 100, training loss: 681.1972045898438 = 1.8097580671310425 + 100.0 * 6.793874263763428
Epoch 100, val loss: 1.8031264543533325
Epoch 110, training loss: 675.99951171875 = 1.8007005453109741 + 100.0 * 6.741987705230713
Epoch 110, val loss: 1.7943733930587769
Epoch 120, training loss: 671.68798828125 = 1.7919195890426636 + 100.0 * 6.698960304260254
Epoch 120, val loss: 1.7860051393508911
Epoch 130, training loss: 667.8704833984375 = 1.7830091714859009 + 100.0 * 6.660874366760254
Epoch 130, val loss: 1.7776786088943481
Epoch 140, training loss: 664.507080078125 = 1.7738847732543945 + 100.0 * 6.6273322105407715
Epoch 140, val loss: 1.769335389137268
Epoch 150, training loss: 661.7435913085938 = 1.7643499374389648 + 100.0 * 6.59979248046875
Epoch 150, val loss: 1.7605267763137817
Epoch 160, training loss: 658.95703125 = 1.754083514213562 + 100.0 * 6.5720295906066895
Epoch 160, val loss: 1.7510641813278198
Epoch 170, training loss: 656.5306396484375 = 1.7429404258728027 + 100.0 * 6.547877311706543
Epoch 170, val loss: 1.7408632040023804
Epoch 180, training loss: 654.5994873046875 = 1.7307403087615967 + 100.0 * 6.528687953948975
Epoch 180, val loss: 1.7297991514205933
Epoch 190, training loss: 652.7440185546875 = 1.7173430919647217 + 100.0 * 6.5102667808532715
Epoch 190, val loss: 1.7177294492721558
Epoch 200, training loss: 651.7551879882812 = 1.702728509902954 + 100.0 * 6.500524044036865
Epoch 200, val loss: 1.7045849561691284
Epoch 210, training loss: 650.0006713867188 = 1.6864726543426514 + 100.0 * 6.483142375946045
Epoch 210, val loss: 1.689928412437439
Epoch 220, training loss: 648.6673583984375 = 1.668757677078247 + 100.0 * 6.4699859619140625
Epoch 220, val loss: 1.6740280389785767
Epoch 230, training loss: 647.49365234375 = 1.6495423316955566 + 100.0 * 6.458441257476807
Epoch 230, val loss: 1.656724214553833
Epoch 240, training loss: 647.2335205078125 = 1.6288796663284302 + 100.0 * 6.456046104431152
Epoch 240, val loss: 1.638192892074585
Epoch 250, training loss: 645.406982421875 = 1.606513261795044 + 100.0 * 6.438004970550537
Epoch 250, val loss: 1.6183207035064697
Epoch 260, training loss: 644.4650268554688 = 1.5829697847366333 + 100.0 * 6.428820610046387
Epoch 260, val loss: 1.5974277257919312
Epoch 270, training loss: 643.6021728515625 = 1.558258056640625 + 100.0 * 6.420439720153809
Epoch 270, val loss: 1.5757393836975098
Epoch 280, training loss: 643.4485473632812 = 1.532108187675476 + 100.0 * 6.419164657592773
Epoch 280, val loss: 1.5531562566757202
Epoch 290, training loss: 642.1038818359375 = 1.5053107738494873 + 100.0 * 6.4059858322143555
Epoch 290, val loss: 1.5298885107040405
Epoch 300, training loss: 641.1062622070312 = 1.4777603149414062 + 100.0 * 6.396285057067871
Epoch 300, val loss: 1.5064095258712769
Epoch 310, training loss: 640.31884765625 = 1.4497233629226685 + 100.0 * 6.388691425323486
Epoch 310, val loss: 1.4826840162277222
Epoch 320, training loss: 639.6383666992188 = 1.421251654624939 + 100.0 * 6.382171154022217
Epoch 320, val loss: 1.4588996171951294
Epoch 330, training loss: 639.7810668945312 = 1.3923832178115845 + 100.0 * 6.383886814117432
Epoch 330, val loss: 1.4348520040512085
Epoch 340, training loss: 638.78515625 = 1.363125205039978 + 100.0 * 6.374220371246338
Epoch 340, val loss: 1.4108353853225708
Epoch 350, training loss: 638.0069580078125 = 1.3337875604629517 + 100.0 * 6.366731643676758
Epoch 350, val loss: 1.3870364427566528
Epoch 360, training loss: 637.471923828125 = 1.3045811653137207 + 100.0 * 6.361673355102539
Epoch 360, val loss: 1.3635979890823364
Epoch 370, training loss: 637.2064819335938 = 1.2754526138305664 + 100.0 * 6.359310150146484
Epoch 370, val loss: 1.3405309915542603
Epoch 380, training loss: 636.7525634765625 = 1.2461899518966675 + 100.0 * 6.355063438415527
Epoch 380, val loss: 1.3176392316818237
Epoch 390, training loss: 636.125244140625 = 1.2176034450531006 + 100.0 * 6.349076747894287
Epoch 390, val loss: 1.2955002784729004
Epoch 400, training loss: 635.7402954101562 = 1.1893621683120728 + 100.0 * 6.345509052276611
Epoch 400, val loss: 1.2738673686981201
Epoch 410, training loss: 635.990234375 = 1.1614524126052856 + 100.0 * 6.348288059234619
Epoch 410, val loss: 1.252761960029602
Epoch 420, training loss: 635.2340698242188 = 1.1338499784469604 + 100.0 * 6.341001987457275
Epoch 420, val loss: 1.2322192192077637
Epoch 430, training loss: 634.734619140625 = 1.1069648265838623 + 100.0 * 6.336276531219482
Epoch 430, val loss: 1.2124491930007935
Epoch 440, training loss: 634.3888549804688 = 1.0805903673171997 + 100.0 * 6.333082675933838
Epoch 440, val loss: 1.1933515071868896
Epoch 450, training loss: 634.3128662109375 = 1.0548326969146729 + 100.0 * 6.33258056640625
Epoch 450, val loss: 1.1749616861343384
Epoch 460, training loss: 633.833984375 = 1.029433012008667 + 100.0 * 6.32804536819458
Epoch 460, val loss: 1.1569608449935913
Epoch 470, training loss: 633.4012451171875 = 1.00491464138031 + 100.0 * 6.323963165283203
Epoch 470, val loss: 1.1400625705718994
Epoch 480, training loss: 633.2030029296875 = 0.9810588955879211 + 100.0 * 6.322219371795654
Epoch 480, val loss: 1.1236392259597778
Epoch 490, training loss: 632.9242553710938 = 0.9576972126960754 + 100.0 * 6.319665431976318
Epoch 490, val loss: 1.108081579208374
Epoch 500, training loss: 632.6317749023438 = 0.935078501701355 + 100.0 * 6.316967010498047
Epoch 500, val loss: 1.0930702686309814
Epoch 510, training loss: 632.3203735351562 = 0.9130328297615051 + 100.0 * 6.31407356262207
Epoch 510, val loss: 1.0786786079406738
Epoch 520, training loss: 632.2866821289062 = 0.8915899991989136 + 100.0 * 6.313951015472412
Epoch 520, val loss: 1.0649954080581665
Epoch 530, training loss: 632.4703979492188 = 0.870622456073761 + 100.0 * 6.315998077392578
Epoch 530, val loss: 1.0518757104873657
Epoch 540, training loss: 631.5661010742188 = 0.8502665758132935 + 100.0 * 6.307158470153809
Epoch 540, val loss: 1.0393065214157104
Epoch 550, training loss: 631.3204956054688 = 0.8304580450057983 + 100.0 * 6.304900646209717
Epoch 550, val loss: 1.027442216873169
Epoch 560, training loss: 631.0634155273438 = 0.8112315535545349 + 100.0 * 6.3025221824646
Epoch 560, val loss: 1.016112208366394
Epoch 570, training loss: 631.1981811523438 = 0.7925309538841248 + 100.0 * 6.304056167602539
Epoch 570, val loss: 1.005312204360962
Epoch 580, training loss: 631.06640625 = 0.7738155126571655 + 100.0 * 6.302926063537598
Epoch 580, val loss: 0.9948062300682068
Epoch 590, training loss: 630.4922485351562 = 0.7558526992797852 + 100.0 * 6.297363758087158
Epoch 590, val loss: 0.9847414493560791
Epoch 600, training loss: 630.3341674804688 = 0.738269031047821 + 100.0 * 6.295958995819092
Epoch 600, val loss: 0.9751966595649719
Epoch 610, training loss: 630.05810546875 = 0.7210253477096558 + 100.0 * 6.293371200561523
Epoch 610, val loss: 0.9662310481071472
Epoch 620, training loss: 629.9357299804688 = 0.7041962146759033 + 100.0 * 6.2923150062561035
Epoch 620, val loss: 0.9575240612030029
Epoch 630, training loss: 629.7493286132812 = 0.6877433657646179 + 100.0 * 6.290616035461426
Epoch 630, val loss: 0.9495293498039246
Epoch 640, training loss: 629.59228515625 = 0.6715303659439087 + 100.0 * 6.289207935333252
Epoch 640, val loss: 0.9415255188941956
Epoch 650, training loss: 629.4220581054688 = 0.6556318402290344 + 100.0 * 6.28766393661499
Epoch 650, val loss: 0.9341081380844116
Epoch 660, training loss: 629.1992797851562 = 0.640125036239624 + 100.0 * 6.2855916023254395
Epoch 660, val loss: 0.9271371960639954
Epoch 670, training loss: 629.0711059570312 = 0.6249786019325256 + 100.0 * 6.28446102142334
Epoch 670, val loss: 0.9205513000488281
Epoch 680, training loss: 628.8980102539062 = 0.6100432872772217 + 100.0 * 6.282879829406738
Epoch 680, val loss: 0.914139449596405
Epoch 690, training loss: 628.8531494140625 = 0.595395565032959 + 100.0 * 6.2825775146484375
Epoch 690, val loss: 0.9082642197608948
Epoch 700, training loss: 628.525146484375 = 0.5811913013458252 + 100.0 * 6.279439449310303
Epoch 700, val loss: 0.9027741551399231
Epoch 710, training loss: 628.3787841796875 = 0.5672652125358582 + 100.0 * 6.278115272521973
Epoch 710, val loss: 0.8976914286613464
Epoch 720, training loss: 628.5692749023438 = 0.5535604357719421 + 100.0 * 6.28015661239624
Epoch 720, val loss: 0.8928543329238892
Epoch 730, training loss: 628.0602416992188 = 0.5401375889778137 + 100.0 * 6.275200843811035
Epoch 730, val loss: 0.8883842825889587
Epoch 740, training loss: 628.3369750976562 = 0.5270939469337463 + 100.0 * 6.2780985832214355
Epoch 740, val loss: 0.8844691514968872
Epoch 750, training loss: 628.0526733398438 = 0.5141849517822266 + 100.0 * 6.275384902954102
Epoch 750, val loss: 0.8803961277008057
Epoch 760, training loss: 627.716552734375 = 0.5016559362411499 + 100.0 * 6.272149085998535
Epoch 760, val loss: 0.877212643623352
Epoch 770, training loss: 627.5957641601562 = 0.48948919773101807 + 100.0 * 6.27106237411499
Epoch 770, val loss: 0.8741908073425293
Epoch 780, training loss: 627.960693359375 = 0.4775940179824829 + 100.0 * 6.2748308181762695
Epoch 780, val loss: 0.8716225028038025
Epoch 790, training loss: 627.66943359375 = 0.46593645215034485 + 100.0 * 6.272035121917725
Epoch 790, val loss: 0.8689260482788086
Epoch 800, training loss: 627.2576904296875 = 0.4545665979385376 + 100.0 * 6.268031120300293
Epoch 800, val loss: 0.8668590784072876
Epoch 810, training loss: 627.4846801757812 = 0.4435742497444153 + 100.0 * 6.270411014556885
Epoch 810, val loss: 0.8651078343391418
Epoch 820, training loss: 626.97412109375 = 0.43264859914779663 + 100.0 * 6.265414237976074
Epoch 820, val loss: 0.8635812997817993
Epoch 830, training loss: 626.893310546875 = 0.4221247434616089 + 100.0 * 6.264711856842041
Epoch 830, val loss: 0.8624289035797119
Epoch 840, training loss: 627.4131469726562 = 0.4118354916572571 + 100.0 * 6.270012855529785
Epoch 840, val loss: 0.8614851236343384
Epoch 850, training loss: 626.9015502929688 = 0.40181466937065125 + 100.0 * 6.264997482299805
Epoch 850, val loss: 0.8607617616653442
Epoch 860, training loss: 626.6414794921875 = 0.3920094072818756 + 100.0 * 6.2624945640563965
Epoch 860, val loss: 0.8602736592292786
Epoch 870, training loss: 626.581298828125 = 0.3825131952762604 + 100.0 * 6.261987686157227
Epoch 870, val loss: 0.8601670265197754
Epoch 880, training loss: 626.7108154296875 = 0.3731779158115387 + 100.0 * 6.263376712799072
Epoch 880, val loss: 0.8602198958396912
Epoch 890, training loss: 626.463623046875 = 0.3640069365501404 + 100.0 * 6.260995864868164
Epoch 890, val loss: 0.8602960705757141
Epoch 900, training loss: 626.3999633789062 = 0.35514330863952637 + 100.0 * 6.260447978973389
Epoch 900, val loss: 0.8606511950492859
Epoch 910, training loss: 626.2113037109375 = 0.34654924273490906 + 100.0 * 6.258647441864014
Epoch 910, val loss: 0.8613073825836182
Epoch 920, training loss: 626.1861572265625 = 0.338138222694397 + 100.0 * 6.258480072021484
Epoch 920, val loss: 0.8621665835380554
Epoch 930, training loss: 626.3423461914062 = 0.32986748218536377 + 100.0 * 6.260124683380127
Epoch 930, val loss: 0.8631647229194641
Epoch 940, training loss: 626.0962524414062 = 0.32188180088996887 + 100.0 * 6.2577433586120605
Epoch 940, val loss: 0.8640684485435486
Epoch 950, training loss: 625.9421997070312 = 0.31402456760406494 + 100.0 * 6.256281852722168
Epoch 950, val loss: 0.86546790599823
Epoch 960, training loss: 625.7498779296875 = 0.3064674437046051 + 100.0 * 6.254434108734131
Epoch 960, val loss: 0.866940438747406
Epoch 970, training loss: 625.7001342773438 = 0.2990815341472626 + 100.0 * 6.2540106773376465
Epoch 970, val loss: 0.8686756491661072
Epoch 980, training loss: 626.164794921875 = 0.2918379604816437 + 100.0 * 6.258729934692383
Epoch 980, val loss: 0.8703696727752686
Epoch 990, training loss: 625.5811767578125 = 0.28481820225715637 + 100.0 * 6.252964019775391
Epoch 990, val loss: 0.8722947239875793
Epoch 1000, training loss: 625.3909912109375 = 0.27796629071235657 + 100.0 * 6.2511305809021
Epoch 1000, val loss: 0.8742513060569763
Epoch 1010, training loss: 625.28857421875 = 0.2713429629802704 + 100.0 * 6.250172138214111
Epoch 1010, val loss: 0.876511812210083
Epoch 1020, training loss: 625.6273803710938 = 0.264883428812027 + 100.0 * 6.25362491607666
Epoch 1020, val loss: 0.8787177205085754
Epoch 1030, training loss: 625.9283447265625 = 0.258526474237442 + 100.0 * 6.256698131561279
Epoch 1030, val loss: 0.8812199831008911
Epoch 1040, training loss: 625.1658935546875 = 0.25230008363723755 + 100.0 * 6.249135971069336
Epoch 1040, val loss: 0.8834665417671204
Epoch 1050, training loss: 625.1270141601562 = 0.24633270502090454 + 100.0 * 6.248806953430176
Epoch 1050, val loss: 0.8862529993057251
Epoch 1060, training loss: 625.0216064453125 = 0.24052293598651886 + 100.0 * 6.2478108406066895
Epoch 1060, val loss: 0.8890587687492371
Epoch 1070, training loss: 625.7034301757812 = 0.23483921587467194 + 100.0 * 6.254685878753662
Epoch 1070, val loss: 0.8920785784721375
Epoch 1080, training loss: 625.0304565429688 = 0.22938327491283417 + 100.0 * 6.248010635375977
Epoch 1080, val loss: 0.8948283195495605
Epoch 1090, training loss: 624.7850341796875 = 0.2239854484796524 + 100.0 * 6.245610237121582
Epoch 1090, val loss: 0.8980274200439453
Epoch 1100, training loss: 624.8280639648438 = 0.21884018182754517 + 100.0 * 6.246092796325684
Epoch 1100, val loss: 0.9013578295707703
Epoch 1110, training loss: 624.857421875 = 0.21372836828231812 + 100.0 * 6.246437072753906
Epoch 1110, val loss: 0.904569685459137
Epoch 1120, training loss: 624.616943359375 = 0.20873525738716125 + 100.0 * 6.244081974029541
Epoch 1120, val loss: 0.9079970121383667
Epoch 1130, training loss: 624.622314453125 = 0.20395542681217194 + 100.0 * 6.244183540344238
Epoch 1130, val loss: 0.91138756275177
Epoch 1140, training loss: 624.858642578125 = 0.1992652267217636 + 100.0 * 6.246593475341797
Epoch 1140, val loss: 0.9150506258010864
Epoch 1150, training loss: 624.647705078125 = 0.19468379020690918 + 100.0 * 6.244530200958252
Epoch 1150, val loss: 0.9185616970062256
Epoch 1160, training loss: 624.4197387695312 = 0.190264031291008 + 100.0 * 6.242294788360596
Epoch 1160, val loss: 0.9224642515182495
Epoch 1170, training loss: 624.5214233398438 = 0.18598641455173492 + 100.0 * 6.243354320526123
Epoch 1170, val loss: 0.9263311624526978
Epoch 1180, training loss: 624.3635864257812 = 0.18174861371517181 + 100.0 * 6.241817951202393
Epoch 1180, val loss: 0.9303479790687561
Epoch 1190, training loss: 625.0170288085938 = 0.1776599884033203 + 100.0 * 6.248393535614014
Epoch 1190, val loss: 0.9343979358673096
Epoch 1200, training loss: 624.4784545898438 = 0.1735813170671463 + 100.0 * 6.243048667907715
Epoch 1200, val loss: 0.9382386207580566
Epoch 1210, training loss: 624.1355590820312 = 0.169693723320961 + 100.0 * 6.239658832550049
Epoch 1210, val loss: 0.9425675272941589
Epoch 1220, training loss: 624.0531005859375 = 0.16591525077819824 + 100.0 * 6.238872051239014
Epoch 1220, val loss: 0.9469954371452332
Epoch 1230, training loss: 623.9451293945312 = 0.16227012872695923 + 100.0 * 6.237828254699707
Epoch 1230, val loss: 0.9514676928520203
Epoch 1240, training loss: 623.9039306640625 = 0.15868937969207764 + 100.0 * 6.237452507019043
Epoch 1240, val loss: 0.9559438228607178
Epoch 1250, training loss: 625.0309448242188 = 0.15517550706863403 + 100.0 * 6.248757839202881
Epoch 1250, val loss: 0.9604282975196838
Epoch 1260, training loss: 624.2341918945312 = 0.15176764130592346 + 100.0 * 6.240824222564697
Epoch 1260, val loss: 0.964661180973053
Epoch 1270, training loss: 623.7786865234375 = 0.14837326109409332 + 100.0 * 6.236303329467773
Epoch 1270, val loss: 0.9692113995552063
Epoch 1280, training loss: 623.7437744140625 = 0.14513535797595978 + 100.0 * 6.235986232757568
Epoch 1280, val loss: 0.9740225076675415
Epoch 1290, training loss: 623.7753295898438 = 0.1420147567987442 + 100.0 * 6.236332893371582
Epoch 1290, val loss: 0.9787134528160095
Epoch 1300, training loss: 623.99560546875 = 0.13890567421913147 + 100.0 * 6.238566875457764
Epoch 1300, val loss: 0.9833720922470093
Epoch 1310, training loss: 623.6199951171875 = 0.13584142923355103 + 100.0 * 6.234841346740723
Epoch 1310, val loss: 0.9878937005996704
Epoch 1320, training loss: 623.9282836914062 = 0.1329183131456375 + 100.0 * 6.2379536628723145
Epoch 1320, val loss: 0.9929379820823669
Epoch 1330, training loss: 623.5369262695312 = 0.13004237413406372 + 100.0 * 6.234068393707275
Epoch 1330, val loss: 0.9974929094314575
Epoch 1340, training loss: 623.4769897460938 = 0.12724322080612183 + 100.0 * 6.233497142791748
Epoch 1340, val loss: 1.0025129318237305
Epoch 1350, training loss: 623.524169921875 = 0.12454055994749069 + 100.0 * 6.233996868133545
Epoch 1350, val loss: 1.007487416267395
Epoch 1360, training loss: 623.8988647460938 = 0.12185542285442352 + 100.0 * 6.2377705574035645
Epoch 1360, val loss: 1.0122872591018677
Epoch 1370, training loss: 623.4779663085938 = 0.11927146464586258 + 100.0 * 6.23358678817749
Epoch 1370, val loss: 1.0172510147094727
Epoch 1380, training loss: 623.3748779296875 = 0.11671919375658035 + 100.0 * 6.232582092285156
Epoch 1380, val loss: 1.0222498178482056
Epoch 1390, training loss: 623.308349609375 = 0.1142490804195404 + 100.0 * 6.231940746307373
Epoch 1390, val loss: 1.0273710489273071
Epoch 1400, training loss: 623.27001953125 = 0.11184392124414444 + 100.0 * 6.231582164764404
Epoch 1400, val loss: 1.0326554775238037
Epoch 1410, training loss: 623.2931518554688 = 0.10949231684207916 + 100.0 * 6.231836318969727
Epoch 1410, val loss: 1.0378731489181519
Epoch 1420, training loss: 623.7315673828125 = 0.10717690736055374 + 100.0 * 6.236244201660156
Epoch 1420, val loss: 1.0430623292922974
Epoch 1430, training loss: 623.3543090820312 = 0.10489104688167572 + 100.0 * 6.232493877410889
Epoch 1430, val loss: 1.0478782653808594
Epoch 1440, training loss: 623.1629638671875 = 0.1026860922574997 + 100.0 * 6.230602741241455
Epoch 1440, val loss: 1.0531624555587769
Epoch 1450, training loss: 623.1847534179688 = 0.10053371638059616 + 100.0 * 6.230842113494873
Epoch 1450, val loss: 1.058458924293518
Epoch 1460, training loss: 623.21728515625 = 0.09843422472476959 + 100.0 * 6.2311882972717285
Epoch 1460, val loss: 1.0637089014053345
Epoch 1470, training loss: 623.1298217773438 = 0.09639526158571243 + 100.0 * 6.230334281921387
Epoch 1470, val loss: 1.0689787864685059
Epoch 1480, training loss: 623.13818359375 = 0.09438774734735489 + 100.0 * 6.230438232421875
Epoch 1480, val loss: 1.0740511417388916
Epoch 1490, training loss: 623.0156860351562 = 0.09244079887866974 + 100.0 * 6.229232311248779
Epoch 1490, val loss: 1.0793769359588623
Epoch 1500, training loss: 622.9664306640625 = 0.09055033326148987 + 100.0 * 6.228758335113525
Epoch 1500, val loss: 1.084583044052124
Epoch 1510, training loss: 623.2337646484375 = 0.08869978785514832 + 100.0 * 6.231451034545898
Epoch 1510, val loss: 1.0897732973098755
Epoch 1520, training loss: 622.9126586914062 = 0.08683065325021744 + 100.0 * 6.22825813293457
Epoch 1520, val loss: 1.0949915647506714
Epoch 1530, training loss: 622.8175659179688 = 0.08506659418344498 + 100.0 * 6.227324962615967
Epoch 1530, val loss: 1.100271463394165
Epoch 1540, training loss: 622.9459838867188 = 0.08333657681941986 + 100.0 * 6.228626728057861
Epoch 1540, val loss: 1.1055099964141846
Epoch 1550, training loss: 622.8544921875 = 0.08162681013345718 + 100.0 * 6.227728843688965
Epoch 1550, val loss: 1.1107984781265259
Epoch 1560, training loss: 622.8690795898438 = 0.07994934171438217 + 100.0 * 6.227891445159912
Epoch 1560, val loss: 1.115740418434143
Epoch 1570, training loss: 622.6210327148438 = 0.07832110673189163 + 100.0 * 6.225427150726318
Epoch 1570, val loss: 1.1212210655212402
Epoch 1580, training loss: 622.5485229492188 = 0.07674887776374817 + 100.0 * 6.22471809387207
Epoch 1580, val loss: 1.1265592575073242
Epoch 1590, training loss: 622.607666015625 = 0.07522030919790268 + 100.0 * 6.225324630737305
Epoch 1590, val loss: 1.1319999694824219
Epoch 1600, training loss: 623.0570068359375 = 0.07371777296066284 + 100.0 * 6.229832649230957
Epoch 1600, val loss: 1.137330174446106
Epoch 1610, training loss: 622.5686645507812 = 0.07221751660108566 + 100.0 * 6.224964618682861
Epoch 1610, val loss: 1.1422985792160034
Epoch 1620, training loss: 622.6678466796875 = 0.07079604268074036 + 100.0 * 6.22597074508667
Epoch 1620, val loss: 1.1474950313568115
Epoch 1630, training loss: 622.6657104492188 = 0.06937333941459656 + 100.0 * 6.225963115692139
Epoch 1630, val loss: 1.1529268026351929
Epoch 1640, training loss: 622.5079956054688 = 0.06797715276479721 + 100.0 * 6.224400043487549
Epoch 1640, val loss: 1.1579749584197998
Epoch 1650, training loss: 622.3888549804688 = 0.06666017323732376 + 100.0 * 6.223221778869629
Epoch 1650, val loss: 1.1633541584014893
Epoch 1660, training loss: 622.5510864257812 = 0.06535802781581879 + 100.0 * 6.224857330322266
Epoch 1660, val loss: 1.1683998107910156
Epoch 1670, training loss: 622.3619995117188 = 0.06407499313354492 + 100.0 * 6.2229790687561035
Epoch 1670, val loss: 1.1737746000289917
Epoch 1680, training loss: 622.3298950195312 = 0.06283155083656311 + 100.0 * 6.222671031951904
Epoch 1680, val loss: 1.179051399230957
Epoch 1690, training loss: 622.9692993164062 = 0.061604470014572144 + 100.0 * 6.229076862335205
Epoch 1690, val loss: 1.1840873956680298
Epoch 1700, training loss: 622.3690185546875 = 0.06042147055268288 + 100.0 * 6.223085880279541
Epoch 1700, val loss: 1.1891905069351196
Epoch 1710, training loss: 622.2034912109375 = 0.059236980974674225 + 100.0 * 6.221442222595215
Epoch 1710, val loss: 1.1944137811660767
Epoch 1720, training loss: 622.1481323242188 = 0.05811728537082672 + 100.0 * 6.220900535583496
Epoch 1720, val loss: 1.1998810768127441
Epoch 1730, training loss: 622.8829956054688 = 0.057016026228666306 + 100.0 * 6.228260040283203
Epoch 1730, val loss: 1.205271601676941
Epoch 1740, training loss: 622.2981567382812 = 0.05592961981892586 + 100.0 * 6.2224225997924805
Epoch 1740, val loss: 1.2094154357910156
Epoch 1750, training loss: 622.233642578125 = 0.05485612899065018 + 100.0 * 6.221787929534912
Epoch 1750, val loss: 1.215038776397705
Epoch 1760, training loss: 622.27978515625 = 0.05383198708295822 + 100.0 * 6.222259521484375
Epoch 1760, val loss: 1.219827651977539
Epoch 1770, training loss: 622.1239624023438 = 0.052823446691036224 + 100.0 * 6.220711708068848
Epoch 1770, val loss: 1.2253304719924927
Epoch 1780, training loss: 622.6561279296875 = 0.051856547594070435 + 100.0 * 6.2260422706604
Epoch 1780, val loss: 1.2301119565963745
Epoch 1790, training loss: 622.1616821289062 = 0.050867293030023575 + 100.0 * 6.221108436584473
Epoch 1790, val loss: 1.235234260559082
Epoch 1800, training loss: 622.0236206054688 = 0.04993399605154991 + 100.0 * 6.2197370529174805
Epoch 1800, val loss: 1.2402914762496948
Epoch 1810, training loss: 622.3543090820312 = 0.0490255281329155 + 100.0 * 6.223052978515625
Epoch 1810, val loss: 1.2454478740692139
Epoch 1820, training loss: 621.9155883789062 = 0.04811914265155792 + 100.0 * 6.218674659729004
Epoch 1820, val loss: 1.2502206563949585
Epoch 1830, training loss: 621.8995971679688 = 0.04723486676812172 + 100.0 * 6.2185235023498535
Epoch 1830, val loss: 1.2553331851959229
Epoch 1840, training loss: 621.984130859375 = 0.04639005661010742 + 100.0 * 6.219377517700195
Epoch 1840, val loss: 1.2601728439331055
Epoch 1850, training loss: 622.237548828125 = 0.04556726664304733 + 100.0 * 6.221919536590576
Epoch 1850, val loss: 1.2653883695602417
Epoch 1860, training loss: 622.1710815429688 = 0.0447329506278038 + 100.0 * 6.221263408660889
Epoch 1860, val loss: 1.2699793577194214
Epoch 1870, training loss: 622.3347778320312 = 0.043922219425439835 + 100.0 * 6.2229084968566895
Epoch 1870, val loss: 1.2749297618865967
Epoch 1880, training loss: 621.917236328125 = 0.04311459884047508 + 100.0 * 6.218741416931152
Epoch 1880, val loss: 1.2796066999435425
Epoch 1890, training loss: 621.7749633789062 = 0.04236146807670593 + 100.0 * 6.2173261642456055
Epoch 1890, val loss: 1.284491777420044
Epoch 1900, training loss: 621.9487915039062 = 0.04161985591053963 + 100.0 * 6.219071388244629
Epoch 1900, val loss: 1.2893767356872559
Epoch 1910, training loss: 621.6793823242188 = 0.040881864726543427 + 100.0 * 6.2163848876953125
Epoch 1910, val loss: 1.294121265411377
Epoch 1920, training loss: 621.7509765625 = 0.040169231593608856 + 100.0 * 6.217108249664307
Epoch 1920, val loss: 1.2992454767227173
Epoch 1930, training loss: 621.9741821289062 = 0.039479002356529236 + 100.0 * 6.21934700012207
Epoch 1930, val loss: 1.3037179708480835
Epoch 1940, training loss: 621.8317260742188 = 0.03880377858877182 + 100.0 * 6.217928886413574
Epoch 1940, val loss: 1.3087372779846191
Epoch 1950, training loss: 621.8445434570312 = 0.03814113140106201 + 100.0 * 6.218063831329346
Epoch 1950, val loss: 1.3134883642196655
Epoch 1960, training loss: 621.5610961914062 = 0.0374683253467083 + 100.0 * 6.215236186981201
Epoch 1960, val loss: 1.3181354999542236
Epoch 1970, training loss: 621.6475219726562 = 0.03683152794837952 + 100.0 * 6.216107368469238
Epoch 1970, val loss: 1.3228297233581543
Epoch 1980, training loss: 621.760009765625 = 0.03621510788798332 + 100.0 * 6.217237949371338
Epoch 1980, val loss: 1.3276206254959106
Epoch 1990, training loss: 621.5606079101562 = 0.03560506924986839 + 100.0 * 6.215250492095947
Epoch 1990, val loss: 1.3321961164474487
Epoch 2000, training loss: 621.6143188476562 = 0.035012137144804 + 100.0 * 6.215792655944824
Epoch 2000, val loss: 1.3367036581039429
Epoch 2010, training loss: 621.7288208007812 = 0.03442561626434326 + 100.0 * 6.216943740844727
Epoch 2010, val loss: 1.3413792848587036
Epoch 2020, training loss: 621.901123046875 = 0.033847518265247345 + 100.0 * 6.218672752380371
Epoch 2020, val loss: 1.345969557762146
Epoch 2030, training loss: 621.5328979492188 = 0.0332818441092968 + 100.0 * 6.214996337890625
Epoch 2030, val loss: 1.3506742715835571
Epoch 2040, training loss: 621.3841552734375 = 0.03273036330938339 + 100.0 * 6.21351432800293
Epoch 2040, val loss: 1.3550939559936523
Epoch 2050, training loss: 621.5147094726562 = 0.03219759091734886 + 100.0 * 6.214824676513672
Epoch 2050, val loss: 1.359520435333252
Epoch 2060, training loss: 621.5467529296875 = 0.03167274594306946 + 100.0 * 6.215150833129883
Epoch 2060, val loss: 1.3639670610427856
Epoch 2070, training loss: 621.94140625 = 0.031161373481154442 + 100.0 * 6.219101905822754
Epoch 2070, val loss: 1.368473768234253
Epoch 2080, training loss: 621.3889770507812 = 0.030654607340693474 + 100.0 * 6.213583469390869
Epoch 2080, val loss: 1.3728234767913818
Epoch 2090, training loss: 621.27392578125 = 0.030168434605002403 + 100.0 * 6.212437629699707
Epoch 2090, val loss: 1.3774068355560303
Epoch 2100, training loss: 621.3456420898438 = 0.029691483825445175 + 100.0 * 6.213159561157227
Epoch 2100, val loss: 1.3817601203918457
Epoch 2110, training loss: 621.87060546875 = 0.02922949008643627 + 100.0 * 6.218413829803467
Epoch 2110, val loss: 1.3861021995544434
Epoch 2120, training loss: 621.4227905273438 = 0.02876388654112816 + 100.0 * 6.213940143585205
Epoch 2120, val loss: 1.3902865648269653
Epoch 2130, training loss: 621.3140258789062 = 0.02831212617456913 + 100.0 * 6.212856769561768
Epoch 2130, val loss: 1.3944896459579468
Epoch 2140, training loss: 621.7503662109375 = 0.027878135442733765 + 100.0 * 6.217224597930908
Epoch 2140, val loss: 1.3988018035888672
Epoch 2150, training loss: 621.2947998046875 = 0.027432911098003387 + 100.0 * 6.212674140930176
Epoch 2150, val loss: 1.403286099433899
Epoch 2160, training loss: 621.1502075195312 = 0.0270090214908123 + 100.0 * 6.211231708526611
Epoch 2160, val loss: 1.407349705696106
Epoch 2170, training loss: 621.2163696289062 = 0.02659921534359455 + 100.0 * 6.211897373199463
Epoch 2170, val loss: 1.4115749597549438
Epoch 2180, training loss: 621.6936645507812 = 0.02619335427880287 + 100.0 * 6.2166748046875
Epoch 2180, val loss: 1.4156635999679565
Epoch 2190, training loss: 621.246337890625 = 0.02580615133047104 + 100.0 * 6.212205410003662
Epoch 2190, val loss: 1.4200522899627686
Epoch 2200, training loss: 621.0579833984375 = 0.025409188121557236 + 100.0 * 6.210325717926025
Epoch 2200, val loss: 1.4241831302642822
Epoch 2210, training loss: 621.2348022460938 = 0.025037521496415138 + 100.0 * 6.212097644805908
Epoch 2210, val loss: 1.4282933473587036
Epoch 2220, training loss: 621.2301025390625 = 0.02466701529920101 + 100.0 * 6.21205472946167
Epoch 2220, val loss: 1.4323585033416748
Epoch 2230, training loss: 621.0757446289062 = 0.024300094693899155 + 100.0 * 6.210514545440674
Epoch 2230, val loss: 1.4362404346466064
Epoch 2240, training loss: 621.0691528320312 = 0.023944290354847908 + 100.0 * 6.210452079772949
Epoch 2240, val loss: 1.4402763843536377
Epoch 2250, training loss: 621.7718505859375 = 0.023601513355970383 + 100.0 * 6.217482089996338
Epoch 2250, val loss: 1.4442764520645142
Epoch 2260, training loss: 621.217041015625 = 0.023250127211213112 + 100.0 * 6.21193790435791
Epoch 2260, val loss: 1.4481083154678345
Epoch 2270, training loss: 621.3944702148438 = 0.02291242778301239 + 100.0 * 6.213715553283691
Epoch 2270, val loss: 1.4521751403808594
Epoch 2280, training loss: 620.927490234375 = 0.022578716278076172 + 100.0 * 6.209049224853516
Epoch 2280, val loss: 1.4559229612350464
Epoch 2290, training loss: 620.9300537109375 = 0.02226124331355095 + 100.0 * 6.209077835083008
Epoch 2290, val loss: 1.4600428342819214
Epoch 2300, training loss: 621.0057983398438 = 0.02194923721253872 + 100.0 * 6.209838390350342
Epoch 2300, val loss: 1.4640181064605713
Epoch 2310, training loss: 621.3795166015625 = 0.021639734506607056 + 100.0 * 6.213578701019287
Epoch 2310, val loss: 1.467726469039917
Epoch 2320, training loss: 621.0142211914062 = 0.021327432245016098 + 100.0 * 6.2099289894104
Epoch 2320, val loss: 1.471369743347168
Epoch 2330, training loss: 621.0338745117188 = 0.021029233932495117 + 100.0 * 6.210128307342529
Epoch 2330, val loss: 1.47542142868042
Epoch 2340, training loss: 620.9681396484375 = 0.020737338811159134 + 100.0 * 6.209474086761475
Epoch 2340, val loss: 1.47903311252594
Epoch 2350, training loss: 620.7944946289062 = 0.0204542875289917 + 100.0 * 6.207740306854248
Epoch 2350, val loss: 1.4828776121139526
Epoch 2360, training loss: 621.0834350585938 = 0.02018156088888645 + 100.0 * 6.21063232421875
Epoch 2360, val loss: 1.4863353967666626
Epoch 2370, training loss: 620.8864135742188 = 0.0199038777500391 + 100.0 * 6.208664894104004
Epoch 2370, val loss: 1.490087628364563
Epoch 2380, training loss: 620.9421997070312 = 0.019634446129202843 + 100.0 * 6.209225654602051
Epoch 2380, val loss: 1.494043231010437
Epoch 2390, training loss: 620.8505249023438 = 0.019369758665561676 + 100.0 * 6.208311557769775
Epoch 2390, val loss: 1.4976948499679565
Epoch 2400, training loss: 620.916259765625 = 0.01912146620452404 + 100.0 * 6.2089715003967285
Epoch 2400, val loss: 1.5015337467193604
Epoch 2410, training loss: 621.2314453125 = 0.01886885240674019 + 100.0 * 6.212125778198242
Epoch 2410, val loss: 1.504859447479248
Epoch 2420, training loss: 620.8458862304688 = 0.018597645685076714 + 100.0 * 6.208272933959961
Epoch 2420, val loss: 1.5084630250930786
Epoch 2430, training loss: 620.7122192382812 = 0.01836029626429081 + 100.0 * 6.20693826675415
Epoch 2430, val loss: 1.512052059173584
Epoch 2440, training loss: 620.6282348632812 = 0.01811930164694786 + 100.0 * 6.206100940704346
Epoch 2440, val loss: 1.5156100988388062
Epoch 2450, training loss: 621.2391357421875 = 0.017893215641379356 + 100.0 * 6.212212562561035
Epoch 2450, val loss: 1.5191335678100586
Epoch 2460, training loss: 620.8549194335938 = 0.01765565015375614 + 100.0 * 6.208372592926025
Epoch 2460, val loss: 1.522153615951538
Epoch 2470, training loss: 620.8031616210938 = 0.01742604188621044 + 100.0 * 6.207857131958008
Epoch 2470, val loss: 1.526092290878296
Epoch 2480, training loss: 621.035400390625 = 0.017202790826559067 + 100.0 * 6.210181713104248
Epoch 2480, val loss: 1.5292932987213135
Epoch 2490, training loss: 620.6818237304688 = 0.016981493681669235 + 100.0 * 6.206648826599121
Epoch 2490, val loss: 1.532868504524231
Epoch 2500, training loss: 620.6155395507812 = 0.0167681947350502 + 100.0 * 6.205987453460693
Epoch 2500, val loss: 1.5363450050354004
Epoch 2510, training loss: 620.9425659179688 = 0.016564499586820602 + 100.0 * 6.2092604637146
Epoch 2510, val loss: 1.5397546291351318
Epoch 2520, training loss: 620.5831298828125 = 0.016353752464056015 + 100.0 * 6.205667972564697
Epoch 2520, val loss: 1.5430415868759155
Epoch 2530, training loss: 620.685546875 = 0.016155188903212547 + 100.0 * 6.206693649291992
Epoch 2530, val loss: 1.54632568359375
Epoch 2540, training loss: 620.6035766601562 = 0.015954799950122833 + 100.0 * 6.205876350402832
Epoch 2540, val loss: 1.5498578548431396
Epoch 2550, training loss: 620.831787109375 = 0.015765851363539696 + 100.0 * 6.208160400390625
Epoch 2550, val loss: 1.5533103942871094
Epoch 2560, training loss: 620.6449584960938 = 0.01556633785367012 + 100.0 * 6.206294059753418
Epoch 2560, val loss: 1.556156039237976
Epoch 2570, training loss: 620.5509643554688 = 0.015374206937849522 + 100.0 * 6.205355644226074
Epoch 2570, val loss: 1.5591330528259277
Epoch 2580, training loss: 620.6192016601562 = 0.015194552950561047 + 100.0 * 6.206039905548096
Epoch 2580, val loss: 1.5624507665634155
Epoch 2590, training loss: 620.6845092773438 = 0.015012298710644245 + 100.0 * 6.206695079803467
Epoch 2590, val loss: 1.5655810832977295
Epoch 2600, training loss: 620.6055908203125 = 0.014827435836195946 + 100.0 * 6.205907344818115
Epoch 2600, val loss: 1.5687940120697021
Epoch 2610, training loss: 620.4404296875 = 0.014651852659881115 + 100.0 * 6.204257965087891
Epoch 2610, val loss: 1.572096586227417
Epoch 2620, training loss: 620.4456787109375 = 0.014482559636235237 + 100.0 * 6.204311847686768
Epoch 2620, val loss: 1.575279712677002
Epoch 2630, training loss: 620.8628540039062 = 0.01431442890316248 + 100.0 * 6.208485126495361
Epoch 2630, val loss: 1.5780905485153198
Epoch 2640, training loss: 620.4996337890625 = 0.014143512584269047 + 100.0 * 6.204854965209961
Epoch 2640, val loss: 1.5816762447357178
Epoch 2650, training loss: 620.35791015625 = 0.01398458331823349 + 100.0 * 6.203439235687256
Epoch 2650, val loss: 1.5844453573226929
Epoch 2660, training loss: 620.322509765625 = 0.01382320374250412 + 100.0 * 6.203086853027344
Epoch 2660, val loss: 1.5875979661941528
Epoch 2670, training loss: 620.574951171875 = 0.013672410510480404 + 100.0 * 6.205612659454346
Epoch 2670, val loss: 1.590745210647583
Epoch 2680, training loss: 620.5741577148438 = 0.013514421880245209 + 100.0 * 6.205606460571289
Epoch 2680, val loss: 1.5932902097702026
Epoch 2690, training loss: 620.54248046875 = 0.013359232805669308 + 100.0 * 6.205291271209717
Epoch 2690, val loss: 1.5966061353683472
Epoch 2700, training loss: 620.2642211914062 = 0.01320496667176485 + 100.0 * 6.202510356903076
Epoch 2700, val loss: 1.5995597839355469
Epoch 2710, training loss: 620.30810546875 = 0.013062051497399807 + 100.0 * 6.202950477600098
Epoch 2710, val loss: 1.6028518676757812
Epoch 2720, training loss: 620.6798095703125 = 0.012924415990710258 + 100.0 * 6.206668853759766
Epoch 2720, val loss: 1.6059858798980713
Epoch 2730, training loss: 620.4214477539062 = 0.012778879143297672 + 100.0 * 6.204086780548096
Epoch 2730, val loss: 1.6082900762557983
Epoch 2740, training loss: 620.2263793945312 = 0.01263550017029047 + 100.0 * 6.202137470245361
Epoch 2740, val loss: 1.6113228797912598
Epoch 2750, training loss: 620.472900390625 = 0.01250374037772417 + 100.0 * 6.204604148864746
Epoch 2750, val loss: 1.6143717765808105
Epoch 2760, training loss: 620.3267211914062 = 0.01236726064234972 + 100.0 * 6.20314359664917
Epoch 2760, val loss: 1.6170234680175781
Epoch 2770, training loss: 620.3019409179688 = 0.012233441695570946 + 100.0 * 6.202897071838379
Epoch 2770, val loss: 1.620112657546997
Epoch 2780, training loss: 620.2767333984375 = 0.01210674550384283 + 100.0 * 6.202646732330322
Epoch 2780, val loss: 1.6230275630950928
Epoch 2790, training loss: 620.6657104492188 = 0.011986247263848782 + 100.0 * 6.20653772354126
Epoch 2790, val loss: 1.6257396936416626
Epoch 2800, training loss: 620.6137084960938 = 0.011847177520394325 + 100.0 * 6.206018447875977
Epoch 2800, val loss: 1.6281077861785889
Epoch 2810, training loss: 620.3046875 = 0.011724219657480717 + 100.0 * 6.202929973602295
Epoch 2810, val loss: 1.631319284439087
Epoch 2820, training loss: 620.26220703125 = 0.011599472723901272 + 100.0 * 6.202506065368652
Epoch 2820, val loss: 1.6340192556381226
Epoch 2830, training loss: 620.2678833007812 = 0.011485953815281391 + 100.0 * 6.202564239501953
Epoch 2830, val loss: 1.6366965770721436
Epoch 2840, training loss: 620.3115844726562 = 0.01136714592576027 + 100.0 * 6.203002452850342
Epoch 2840, val loss: 1.6394966840744019
Epoch 2850, training loss: 620.1292724609375 = 0.011247715912759304 + 100.0 * 6.201180458068848
Epoch 2850, val loss: 1.6419811248779297
Epoch 2860, training loss: 620.339111328125 = 0.01113509014248848 + 100.0 * 6.203279972076416
Epoch 2860, val loss: 1.644628643989563
Epoch 2870, training loss: 620.1951293945312 = 0.011022761464118958 + 100.0 * 6.201840877532959
Epoch 2870, val loss: 1.647489309310913
Epoch 2880, training loss: 620.378173828125 = 0.010917330160737038 + 100.0 * 6.203672409057617
Epoch 2880, val loss: 1.6500695943832397
Epoch 2890, training loss: 620.6690673828125 = 0.01080318819731474 + 100.0 * 6.206582546234131
Epoch 2890, val loss: 1.6525983810424805
Epoch 2900, training loss: 620.2638549804688 = 0.01069176010787487 + 100.0 * 6.202531814575195
Epoch 2900, val loss: 1.6552187204360962
Epoch 2910, training loss: 620.0796508789062 = 0.010585631243884563 + 100.0 * 6.200691223144531
Epoch 2910, val loss: 1.6576441526412964
Epoch 2920, training loss: 619.9815063476562 = 0.01048505213111639 + 100.0 * 6.199710369110107
Epoch 2920, val loss: 1.6605722904205322
Epoch 2930, training loss: 620.1503295898438 = 0.010386076755821705 + 100.0 * 6.201399803161621
Epoch 2930, val loss: 1.6627697944641113
Epoch 2940, training loss: 620.3512573242188 = 0.010284900665283203 + 100.0 * 6.203409671783447
Epoch 2940, val loss: 1.6653525829315186
Epoch 2950, training loss: 620.1445922851562 = 0.010185671038925648 + 100.0 * 6.201344013214111
Epoch 2950, val loss: 1.6681082248687744
Epoch 2960, training loss: 620.1399536132812 = 0.010084553621709347 + 100.0 * 6.201298713684082
Epoch 2960, val loss: 1.6705644130706787
Epoch 2970, training loss: 620.2554321289062 = 0.009989324025809765 + 100.0 * 6.202454090118408
Epoch 2970, val loss: 1.6729627847671509
Epoch 2980, training loss: 620.06982421875 = 0.009896582923829556 + 100.0 * 6.200599670410156
Epoch 2980, val loss: 1.6752686500549316
Epoch 2990, training loss: 619.9692993164062 = 0.009802809916436672 + 100.0 * 6.199594974517822
Epoch 2990, val loss: 1.6779704093933105
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8402741170268846
The final CL Acc:0.73086, 0.00175, The final GNN Acc:0.83869, 0.00224
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10576])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6276245117188 = 1.943241834640503 + 100.0 * 8.596843719482422
Epoch 0, val loss: 1.9481379985809326
Epoch 10, training loss: 861.5458984375 = 1.9347646236419678 + 100.0 * 8.596111297607422
Epoch 10, val loss: 1.9395416975021362
Epoch 20, training loss: 861.0189819335938 = 1.9235903024673462 + 100.0 * 8.590953826904297
Epoch 20, val loss: 1.9275243282318115
Epoch 30, training loss: 857.5648193359375 = 1.9082791805267334 + 100.0 * 8.556565284729004
Epoch 30, val loss: 1.9106016159057617
Epoch 40, training loss: 836.6436767578125 = 1.889914631843567 + 100.0 * 8.347537994384766
Epoch 40, val loss: 1.8909916877746582
Epoch 50, training loss: 776.43359375 = 1.8705679178237915 + 100.0 * 7.745630741119385
Epoch 50, val loss: 1.8704432249069214
Epoch 60, training loss: 742.1979370117188 = 1.8565962314605713 + 100.0 * 7.403413772583008
Epoch 60, val loss: 1.856637716293335
Epoch 70, training loss: 714.2616577148438 = 1.845875859260559 + 100.0 * 7.124157428741455
Epoch 70, val loss: 1.845793604850769
Epoch 80, training loss: 694.7191162109375 = 1.835897445678711 + 100.0 * 6.928832530975342
Epoch 80, val loss: 1.8363043069839478
Epoch 90, training loss: 683.9124755859375 = 1.8274561166763306 + 100.0 * 6.820850372314453
Epoch 90, val loss: 1.8280922174453735
Epoch 100, training loss: 678.2738647460938 = 1.8202282190322876 + 100.0 * 6.764535903930664
Epoch 100, val loss: 1.8209402561187744
Epoch 110, training loss: 673.8435668945312 = 1.8128998279571533 + 100.0 * 6.720306396484375
Epoch 110, val loss: 1.8137117624282837
Epoch 120, training loss: 669.89697265625 = 1.806014060974121 + 100.0 * 6.680909156799316
Epoch 120, val loss: 1.8069980144500732
Epoch 130, training loss: 666.0116577148438 = 1.8001441955566406 + 100.0 * 6.642115116119385
Epoch 130, val loss: 1.8012586832046509
Epoch 140, training loss: 662.690673828125 = 1.7947793006896973 + 100.0 * 6.608958721160889
Epoch 140, val loss: 1.7959575653076172
Epoch 150, training loss: 659.6622314453125 = 1.7890211343765259 + 100.0 * 6.578732013702393
Epoch 150, val loss: 1.7904508113861084
Epoch 160, training loss: 656.9518432617188 = 1.7828693389892578 + 100.0 * 6.551689147949219
Epoch 160, val loss: 1.784658670425415
Epoch 170, training loss: 654.6036376953125 = 1.7763545513153076 + 100.0 * 6.52827262878418
Epoch 170, val loss: 1.7786667346954346
Epoch 180, training loss: 652.5264282226562 = 1.769578456878662 + 100.0 * 6.507568359375
Epoch 180, val loss: 1.7724448442459106
Epoch 190, training loss: 651.0578002929688 = 1.7621996402740479 + 100.0 * 6.492956161499023
Epoch 190, val loss: 1.7657997608184814
Epoch 200, training loss: 649.5432739257812 = 1.754132866859436 + 100.0 * 6.477890968322754
Epoch 200, val loss: 1.7585951089859009
Epoch 210, training loss: 648.0352172851562 = 1.7453272342681885 + 100.0 * 6.462899208068848
Epoch 210, val loss: 1.7508546113967896
Epoch 220, training loss: 646.8273315429688 = 1.7358334064483643 + 100.0 * 6.4509148597717285
Epoch 220, val loss: 1.7426594495773315
Epoch 230, training loss: 645.8397216796875 = 1.7255350351333618 + 100.0 * 6.4411420822143555
Epoch 230, val loss: 1.733703851699829
Epoch 240, training loss: 644.7492065429688 = 1.7144209146499634 + 100.0 * 6.430347919464111
Epoch 240, val loss: 1.7241448163986206
Epoch 250, training loss: 643.8199462890625 = 1.7025189399719238 + 100.0 * 6.421174049377441
Epoch 250, val loss: 1.7139559984207153
Epoch 260, training loss: 642.9794311523438 = 1.6897153854370117 + 100.0 * 6.41289758682251
Epoch 260, val loss: 1.7031153440475464
Epoch 270, training loss: 642.1597290039062 = 1.675934910774231 + 100.0 * 6.404837608337402
Epoch 270, val loss: 1.6913775205612183
Epoch 280, training loss: 641.5761108398438 = 1.6611348390579224 + 100.0 * 6.3991498947143555
Epoch 280, val loss: 1.6789075136184692
Epoch 290, training loss: 640.704345703125 = 1.6453877687454224 + 100.0 * 6.390589714050293
Epoch 290, val loss: 1.6657297611236572
Epoch 300, training loss: 640.2264404296875 = 1.6286957263946533 + 100.0 * 6.385977268218994
Epoch 300, val loss: 1.6517560482025146
Epoch 310, training loss: 639.5343017578125 = 1.610670566558838 + 100.0 * 6.379236221313477
Epoch 310, val loss: 1.6368200778961182
Epoch 320, training loss: 638.8861694335938 = 1.5917611122131348 + 100.0 * 6.372944355010986
Epoch 320, val loss: 1.621139407157898
Epoch 330, training loss: 638.3433837890625 = 1.5718443393707275 + 100.0 * 6.367715358734131
Epoch 330, val loss: 1.6047415733337402
Epoch 340, training loss: 637.8314208984375 = 1.5510642528533936 + 100.0 * 6.3628034591674805
Epoch 340, val loss: 1.5877660512924194
Epoch 350, training loss: 637.9696044921875 = 1.5294601917266846 + 100.0 * 6.364401340484619
Epoch 350, val loss: 1.5701950788497925
Epoch 360, training loss: 637.1776123046875 = 1.5065635442733765 + 100.0 * 6.356710433959961
Epoch 360, val loss: 1.551795244216919
Epoch 370, training loss: 636.5101928710938 = 1.4832181930541992 + 100.0 * 6.350269794464111
Epoch 370, val loss: 1.5328912734985352
Epoch 380, training loss: 636.09765625 = 1.4592180252075195 + 100.0 * 6.346384048461914
Epoch 380, val loss: 1.5137525796890259
Epoch 390, training loss: 635.66650390625 = 1.4347609281539917 + 100.0 * 6.342317581176758
Epoch 390, val loss: 1.494442105293274
Epoch 400, training loss: 635.5887451171875 = 1.409883737564087 + 100.0 * 6.341788291931152
Epoch 400, val loss: 1.4749075174331665
Epoch 410, training loss: 635.3584594726562 = 1.3844681978225708 + 100.0 * 6.339739799499512
Epoch 410, val loss: 1.4552894830703735
Epoch 420, training loss: 634.7429809570312 = 1.3588049411773682 + 100.0 * 6.333841323852539
Epoch 420, val loss: 1.435714602470398
Epoch 430, training loss: 634.375244140625 = 1.3331153392791748 + 100.0 * 6.330421447753906
Epoch 430, val loss: 1.4165140390396118
Epoch 440, training loss: 634.4007568359375 = 1.3073896169662476 + 100.0 * 6.330934047698975
Epoch 440, val loss: 1.3974735736846924
Epoch 450, training loss: 633.6941528320312 = 1.2820016145706177 + 100.0 * 6.324121952056885
Epoch 450, val loss: 1.3788026571273804
Epoch 460, training loss: 633.3793334960938 = 1.2567728757858276 + 100.0 * 6.321225643157959
Epoch 460, val loss: 1.3607779741287231
Epoch 470, training loss: 633.75244140625 = 1.2319101095199585 + 100.0 * 6.325205326080322
Epoch 470, val loss: 1.3434548377990723
Epoch 480, training loss: 632.8795166015625 = 1.2073262929916382 + 100.0 * 6.3167219161987305
Epoch 480, val loss: 1.3264204263687134
Epoch 490, training loss: 632.7122802734375 = 1.1833254098892212 + 100.0 * 6.31528902053833
Epoch 490, val loss: 1.3101348876953125
Epoch 500, training loss: 632.3834838867188 = 1.1598058938980103 + 100.0 * 6.312236785888672
Epoch 500, val loss: 1.295038104057312
Epoch 510, training loss: 632.1094360351562 = 1.136995553970337 + 100.0 * 6.309723854064941
Epoch 510, val loss: 1.280442476272583
Epoch 520, training loss: 632.0018920898438 = 1.114797592163086 + 100.0 * 6.308870792388916
Epoch 520, val loss: 1.266741394996643
Epoch 530, training loss: 632.1238403320312 = 1.0928033590316772 + 100.0 * 6.3103108406066895
Epoch 530, val loss: 1.2536895275115967
Epoch 540, training loss: 631.6004638671875 = 1.0716458559036255 + 100.0 * 6.305288314819336
Epoch 540, val loss: 1.2412086725234985
Epoch 550, training loss: 631.2539672851562 = 1.0510905981063843 + 100.0 * 6.302028656005859
Epoch 550, val loss: 1.2297362089157104
Epoch 560, training loss: 630.9869384765625 = 1.0312936305999756 + 100.0 * 6.299556255340576
Epoch 560, val loss: 1.2190741300582886
Epoch 570, training loss: 630.787841796875 = 1.012113332748413 + 100.0 * 6.297757625579834
Epoch 570, val loss: 1.2091338634490967
Epoch 580, training loss: 630.9285278320312 = 0.9931919574737549 + 100.0 * 6.29935359954834
Epoch 580, val loss: 1.199628233909607
Epoch 590, training loss: 630.5953369140625 = 0.9747702479362488 + 100.0 * 6.296205520629883
Epoch 590, val loss: 1.1905406713485718
Epoch 600, training loss: 630.2840576171875 = 0.9569506049156189 + 100.0 * 6.293270587921143
Epoch 600, val loss: 1.182267427444458
Epoch 610, training loss: 630.0712280273438 = 0.9397277235984802 + 100.0 * 6.29131555557251
Epoch 610, val loss: 1.1746957302093506
Epoch 620, training loss: 630.4595947265625 = 0.9230255484580994 + 100.0 * 6.295365810394287
Epoch 620, val loss: 1.167446494102478
Epoch 630, training loss: 630.0195922851562 = 0.906354546546936 + 100.0 * 6.29113245010376
Epoch 630, val loss: 1.1610959768295288
Epoch 640, training loss: 629.645263671875 = 0.8901890516281128 + 100.0 * 6.287550449371338
Epoch 640, val loss: 1.1548675298690796
Epoch 650, training loss: 629.4256591796875 = 0.8745506405830383 + 100.0 * 6.285511016845703
Epoch 650, val loss: 1.1490930318832397
Epoch 660, training loss: 629.741943359375 = 0.8591941595077515 + 100.0 * 6.288827419281006
Epoch 660, val loss: 1.1439595222473145
Epoch 670, training loss: 629.401123046875 = 0.8441118001937866 + 100.0 * 6.28557014465332
Epoch 670, val loss: 1.138485312461853
Epoch 680, training loss: 629.20654296875 = 0.8292022347450256 + 100.0 * 6.283773422241211
Epoch 680, val loss: 1.1342370510101318
Epoch 690, training loss: 628.9150390625 = 0.8147629499435425 + 100.0 * 6.281002521514893
Epoch 690, val loss: 1.129866600036621
Epoch 700, training loss: 628.9319458007812 = 0.8005817532539368 + 100.0 * 6.281313419342041
Epoch 700, val loss: 1.1260031461715698
Epoch 710, training loss: 628.7462768554688 = 0.7866447567939758 + 100.0 * 6.27959680557251
Epoch 710, val loss: 1.122627854347229
Epoch 720, training loss: 628.634033203125 = 0.7729510068893433 + 100.0 * 6.278610706329346
Epoch 720, val loss: 1.119125247001648
Epoch 730, training loss: 628.51025390625 = 0.7593926191329956 + 100.0 * 6.27750825881958
Epoch 730, val loss: 1.1163054704666138
Epoch 740, training loss: 628.5252075195312 = 0.7461963891983032 + 100.0 * 6.277790069580078
Epoch 740, val loss: 1.1133842468261719
Epoch 750, training loss: 628.6847534179688 = 0.7329514622688293 + 100.0 * 6.279518127441406
Epoch 750, val loss: 1.1108026504516602
Epoch 760, training loss: 628.1392211914062 = 0.7200498580932617 + 100.0 * 6.274191856384277
Epoch 760, val loss: 1.1085212230682373
Epoch 770, training loss: 627.9959106445312 = 0.7074168920516968 + 100.0 * 6.272884845733643
Epoch 770, val loss: 1.1061241626739502
Epoch 780, training loss: 627.8558959960938 = 0.6949837803840637 + 100.0 * 6.271608829498291
Epoch 780, val loss: 1.10440194606781
Epoch 790, training loss: 628.3493041992188 = 0.6827125549316406 + 100.0 * 6.276665687561035
Epoch 790, val loss: 1.1025114059448242
Epoch 800, training loss: 627.8118286132812 = 0.6704426407814026 + 100.0 * 6.271413803100586
Epoch 800, val loss: 1.1008992195129395
Epoch 810, training loss: 627.5020751953125 = 0.65842604637146 + 100.0 * 6.268436431884766
Epoch 810, val loss: 1.0996026992797852
Epoch 820, training loss: 627.6198120117188 = 0.6466771960258484 + 100.0 * 6.269731521606445
Epoch 820, val loss: 1.098454236984253
Epoch 830, training loss: 627.3870239257812 = 0.6349464058876038 + 100.0 * 6.267520904541016
Epoch 830, val loss: 1.0970746278762817
Epoch 840, training loss: 627.2595825195312 = 0.6233280897140503 + 100.0 * 6.266362190246582
Epoch 840, val loss: 1.0960485935211182
Epoch 850, training loss: 627.4934692382812 = 0.6119673252105713 + 100.0 * 6.268815517425537
Epoch 850, val loss: 1.0952861309051514
Epoch 860, training loss: 627.3480834960938 = 0.6006420850753784 + 100.0 * 6.267474174499512
Epoch 860, val loss: 1.0949008464813232
Epoch 870, training loss: 626.9735107421875 = 0.5895438194274902 + 100.0 * 6.2638397216796875
Epoch 870, val loss: 1.0939363241195679
Epoch 880, training loss: 626.87646484375 = 0.5786540508270264 + 100.0 * 6.2629780769348145
Epoch 880, val loss: 1.0936483144760132
Epoch 890, training loss: 626.8890991210938 = 0.567931592464447 + 100.0 * 6.263211727142334
Epoch 890, val loss: 1.0934534072875977
Epoch 900, training loss: 627.0126953125 = 0.557274341583252 + 100.0 * 6.264554500579834
Epoch 900, val loss: 1.093320608139038
Epoch 910, training loss: 626.8489379882812 = 0.5465797185897827 + 100.0 * 6.263023853302002
Epoch 910, val loss: 1.093376636505127
Epoch 920, training loss: 627.04296875 = 0.5361856818199158 + 100.0 * 6.2650675773620605
Epoch 920, val loss: 1.0928727388381958
Epoch 930, training loss: 626.5283203125 = 0.5258415341377258 + 100.0 * 6.2600250244140625
Epoch 930, val loss: 1.0932719707489014
Epoch 940, training loss: 626.3412475585938 = 0.5158292651176453 + 100.0 * 6.258254528045654
Epoch 940, val loss: 1.0935094356536865
Epoch 950, training loss: 626.2732543945312 = 0.5060170292854309 + 100.0 * 6.257672309875488
Epoch 950, val loss: 1.0939313173294067
Epoch 960, training loss: 626.5884399414062 = 0.49634799361228943 + 100.0 * 6.260921001434326
Epoch 960, val loss: 1.0943597555160522
Epoch 970, training loss: 626.4503173828125 = 0.4866465628147125 + 100.0 * 6.259636878967285
Epoch 970, val loss: 1.094619870185852
Epoch 980, training loss: 626.4524536132812 = 0.4770987331867218 + 100.0 * 6.259753227233887
Epoch 980, val loss: 1.0952928066253662
Epoch 990, training loss: 625.9739990234375 = 0.4677221477031708 + 100.0 * 6.255063056945801
Epoch 990, val loss: 1.0960137844085693
Epoch 1000, training loss: 625.9200439453125 = 0.45863986015319824 + 100.0 * 6.254614353179932
Epoch 1000, val loss: 1.0969908237457275
Epoch 1010, training loss: 625.968994140625 = 0.4497370719909668 + 100.0 * 6.255192756652832
Epoch 1010, val loss: 1.0981415510177612
Epoch 1020, training loss: 626.0611572265625 = 0.44090303778648376 + 100.0 * 6.256202697753906
Epoch 1020, val loss: 1.0990848541259766
Epoch 1030, training loss: 625.8023071289062 = 0.4322671890258789 + 100.0 * 6.2537007331848145
Epoch 1030, val loss: 1.1001006364822388
Epoch 1040, training loss: 625.7085571289062 = 0.4238208830356598 + 100.0 * 6.252847194671631
Epoch 1040, val loss: 1.1012492179870605
Epoch 1050, training loss: 625.977783203125 = 0.4154989719390869 + 100.0 * 6.255622863769531
Epoch 1050, val loss: 1.1029140949249268
Epoch 1060, training loss: 625.5234985351562 = 0.40722349286079407 + 100.0 * 6.251162528991699
Epoch 1060, val loss: 1.1038038730621338
Epoch 1070, training loss: 625.5503540039062 = 0.3991948664188385 + 100.0 * 6.251511573791504
Epoch 1070, val loss: 1.1059619188308716
Epoch 1080, training loss: 625.5180053710938 = 0.3914734721183777 + 100.0 * 6.251265525817871
Epoch 1080, val loss: 1.107634425163269
Epoch 1090, training loss: 625.4539184570312 = 0.3838472068309784 + 100.0 * 6.250700950622559
Epoch 1090, val loss: 1.109573245048523
Epoch 1100, training loss: 625.3427734375 = 0.3763710856437683 + 100.0 * 6.249664306640625
Epoch 1100, val loss: 1.1117470264434814
Epoch 1110, training loss: 625.7154541015625 = 0.3691268563270569 + 100.0 * 6.253463268280029
Epoch 1110, val loss: 1.1135780811309814
Epoch 1120, training loss: 625.292724609375 = 0.361823707818985 + 100.0 * 6.249309062957764
Epoch 1120, val loss: 1.1162045001983643
Epoch 1130, training loss: 625.0787963867188 = 0.3548380434513092 + 100.0 * 6.247239112854004
Epoch 1130, val loss: 1.1183381080627441
Epoch 1140, training loss: 624.99658203125 = 0.3480833172798157 + 100.0 * 6.246485233306885
Epoch 1140, val loss: 1.1209940910339355
Epoch 1150, training loss: 625.1513061523438 = 0.3415415287017822 + 100.0 * 6.2480974197387695
Epoch 1150, val loss: 1.1238185167312622
Epoch 1160, training loss: 624.9459838867188 = 0.33494803309440613 + 100.0 * 6.246110439300537
Epoch 1160, val loss: 1.126029372215271
Epoch 1170, training loss: 624.7836303710938 = 0.32850027084350586 + 100.0 * 6.244551658630371
Epoch 1170, val loss: 1.12876558303833
Epoch 1180, training loss: 624.7942504882812 = 0.3223174512386322 + 100.0 * 6.2447190284729
Epoch 1180, val loss: 1.1317344903945923
Epoch 1190, training loss: 625.181396484375 = 0.31629496812820435 + 100.0 * 6.248651027679443
Epoch 1190, val loss: 1.1347543001174927
Epoch 1200, training loss: 624.6715698242188 = 0.3103230893611908 + 100.0 * 6.243612766265869
Epoch 1200, val loss: 1.1374479532241821
Epoch 1210, training loss: 624.5849609375 = 0.3045588433742523 + 100.0 * 6.242804050445557
Epoch 1210, val loss: 1.1405521631240845
Epoch 1220, training loss: 624.9051513671875 = 0.2989245653152466 + 100.0 * 6.2460618019104
Epoch 1220, val loss: 1.1440855264663696
Epoch 1230, training loss: 624.94921875 = 0.2933679223060608 + 100.0 * 6.24655818939209
Epoch 1230, val loss: 1.1464953422546387
Epoch 1240, training loss: 624.69873046875 = 0.2878175675868988 + 100.0 * 6.2441086769104
Epoch 1240, val loss: 1.1505759954452515
Epoch 1250, training loss: 624.3705444335938 = 0.282498836517334 + 100.0 * 6.240880489349365
Epoch 1250, val loss: 1.1538927555084229
Epoch 1260, training loss: 624.4115600585938 = 0.27736157178878784 + 100.0 * 6.241342067718506
Epoch 1260, val loss: 1.1575324535369873
Epoch 1270, training loss: 624.8519287109375 = 0.27226099371910095 + 100.0 * 6.2457966804504395
Epoch 1270, val loss: 1.1612827777862549
Epoch 1280, training loss: 624.56201171875 = 0.26724982261657715 + 100.0 * 6.242947578430176
Epoch 1280, val loss: 1.1645671129226685
Epoch 1290, training loss: 624.39306640625 = 0.26231926679611206 + 100.0 * 6.241307258605957
Epoch 1290, val loss: 1.1682424545288086
Epoch 1300, training loss: 624.4588012695312 = 0.2575254738330841 + 100.0 * 6.242012977600098
Epoch 1300, val loss: 1.1722710132598877
Epoch 1310, training loss: 624.060302734375 = 0.2528302073478699 + 100.0 * 6.238074779510498
Epoch 1310, val loss: 1.1761008501052856
Epoch 1320, training loss: 624.0679931640625 = 0.24827787280082703 + 100.0 * 6.238196849822998
Epoch 1320, val loss: 1.1802023649215698
Epoch 1330, training loss: 624.1826782226562 = 0.24380679428577423 + 100.0 * 6.239388465881348
Epoch 1330, val loss: 1.1845828294754028
Epoch 1340, training loss: 624.177978515625 = 0.23937444388866425 + 100.0 * 6.239386081695557
Epoch 1340, val loss: 1.1883494853973389
Epoch 1350, training loss: 624.190185546875 = 0.23500584065914154 + 100.0 * 6.239552021026611
Epoch 1350, val loss: 1.1924108266830444
Epoch 1360, training loss: 624.0547485351562 = 0.23063261806964874 + 100.0 * 6.238240718841553
Epoch 1360, val loss: 1.1962616443634033
Epoch 1370, training loss: 623.8812866210938 = 0.22637391090393066 + 100.0 * 6.236548900604248
Epoch 1370, val loss: 1.201048731803894
Epoch 1380, training loss: 623.7805786132812 = 0.22235456109046936 + 100.0 * 6.23558235168457
Epoch 1380, val loss: 1.2054649591445923
Epoch 1390, training loss: 623.7061767578125 = 0.21842174232006073 + 100.0 * 6.234877586364746
Epoch 1390, val loss: 1.2102069854736328
Epoch 1400, training loss: 623.702392578125 = 0.21458588540554047 + 100.0 * 6.234878063201904
Epoch 1400, val loss: 1.2148405313491821
Epoch 1410, training loss: 624.4015502929688 = 0.2107921987771988 + 100.0 * 6.241907596588135
Epoch 1410, val loss: 1.219085693359375
Epoch 1420, training loss: 624.1411743164062 = 0.20692327618598938 + 100.0 * 6.23934268951416
Epoch 1420, val loss: 1.2243684530258179
Epoch 1430, training loss: 623.6187133789062 = 0.20313599705696106 + 100.0 * 6.234156131744385
Epoch 1430, val loss: 1.2288284301757812
Epoch 1440, training loss: 623.5546264648438 = 0.19950446486473083 + 100.0 * 6.233551025390625
Epoch 1440, val loss: 1.2334413528442383
Epoch 1450, training loss: 623.4917602539062 = 0.19599519670009613 + 100.0 * 6.23295783996582
Epoch 1450, val loss: 1.2388375997543335
Epoch 1460, training loss: 624.1121215820312 = 0.19257687032222748 + 100.0 * 6.239195823669434
Epoch 1460, val loss: 1.2437784671783447
Epoch 1470, training loss: 623.5924072265625 = 0.18903526663780212 + 100.0 * 6.234033584594727
Epoch 1470, val loss: 1.2480617761611938
Epoch 1480, training loss: 623.4700317382812 = 0.18563248217105865 + 100.0 * 6.232844352722168
Epoch 1480, val loss: 1.2532098293304443
Epoch 1490, training loss: 623.92626953125 = 0.18236835300922394 + 100.0 * 6.237438678741455
Epoch 1490, val loss: 1.258065104484558
Epoch 1500, training loss: 623.36181640625 = 0.1789827048778534 + 100.0 * 6.231828689575195
Epoch 1500, val loss: 1.2631570100784302
Epoch 1510, training loss: 623.3936767578125 = 0.175750270485878 + 100.0 * 6.232179641723633
Epoch 1510, val loss: 1.2681020498275757
Epoch 1520, training loss: 623.2850952148438 = 0.1726360023021698 + 100.0 * 6.2311248779296875
Epoch 1520, val loss: 1.2736467123031616
Epoch 1530, training loss: 623.1857299804688 = 0.16964396834373474 + 100.0 * 6.230161190032959
Epoch 1530, val loss: 1.2788262367248535
Epoch 1540, training loss: 623.1217041015625 = 0.1666789948940277 + 100.0 * 6.229549884796143
Epoch 1540, val loss: 1.28437077999115
Epoch 1550, training loss: 624.22021484375 = 0.16378669440746307 + 100.0 * 6.240564346313477
Epoch 1550, val loss: 1.2897928953170776
Epoch 1560, training loss: 623.6728515625 = 0.16074222326278687 + 100.0 * 6.23512077331543
Epoch 1560, val loss: 1.2938958406448364
Epoch 1570, training loss: 623.0441284179688 = 0.1578032672405243 + 100.0 * 6.22886323928833
Epoch 1570, val loss: 1.299362301826477
Epoch 1580, training loss: 623.0346069335938 = 0.15503282845020294 + 100.0 * 6.228796005249023
Epoch 1580, val loss: 1.3050932884216309
Epoch 1590, training loss: 623.0369262695312 = 0.15234433114528656 + 100.0 * 6.228845596313477
Epoch 1590, val loss: 1.3105511665344238
Epoch 1600, training loss: 623.4417114257812 = 0.149660125374794 + 100.0 * 6.2329206466674805
Epoch 1600, val loss: 1.3158317804336548
Epoch 1610, training loss: 623.0735473632812 = 0.1470012068748474 + 100.0 * 6.2292656898498535
Epoch 1610, val loss: 1.32114577293396
Epoch 1620, training loss: 623.2935791015625 = 0.1443874090909958 + 100.0 * 6.231491565704346
Epoch 1620, val loss: 1.326582670211792
Epoch 1630, training loss: 623.0272216796875 = 0.14178961515426636 + 100.0 * 6.228854656219482
Epoch 1630, val loss: 1.332419991493225
Epoch 1640, training loss: 622.8331298828125 = 0.1392834335565567 + 100.0 * 6.226938724517822
Epoch 1640, val loss: 1.337390661239624
Epoch 1650, training loss: 622.8734130859375 = 0.1368546485900879 + 100.0 * 6.227365970611572
Epoch 1650, val loss: 1.3428659439086914
Epoch 1660, training loss: 622.9541015625 = 0.134482741355896 + 100.0 * 6.228196144104004
Epoch 1660, val loss: 1.3486367464065552
Epoch 1670, training loss: 623.1162109375 = 0.13210172951221466 + 100.0 * 6.229841232299805
Epoch 1670, val loss: 1.353952407836914
Epoch 1680, training loss: 623.3168334960938 = 0.12972129881381989 + 100.0 * 6.231871604919434
Epoch 1680, val loss: 1.359662652015686
Epoch 1690, training loss: 622.7554931640625 = 0.12739011645317078 + 100.0 * 6.22628116607666
Epoch 1690, val loss: 1.3644827604293823
Epoch 1700, training loss: 622.6746826171875 = 0.12514571845531464 + 100.0 * 6.2254958152771
Epoch 1700, val loss: 1.3706015348434448
Epoch 1710, training loss: 622.615478515625 = 0.12297990173101425 + 100.0 * 6.2249250411987305
Epoch 1710, val loss: 1.3761643171310425
Epoch 1720, training loss: 622.8086547851562 = 0.12086967378854752 + 100.0 * 6.2268781661987305
Epoch 1720, val loss: 1.3815351724624634
Epoch 1730, training loss: 622.7395629882812 = 0.11872876435518265 + 100.0 * 6.226208209991455
Epoch 1730, val loss: 1.387332558631897
Epoch 1740, training loss: 622.8777465820312 = 0.1166590079665184 + 100.0 * 6.227611064910889
Epoch 1740, val loss: 1.3926947116851807
Epoch 1750, training loss: 622.8121948242188 = 0.11458013206720352 + 100.0 * 6.22697639465332
Epoch 1750, val loss: 1.398293137550354
Epoch 1760, training loss: 622.7291259765625 = 0.11255398392677307 + 100.0 * 6.226165771484375
Epoch 1760, val loss: 1.4041705131530762
Epoch 1770, training loss: 622.5194091796875 = 0.11056205630302429 + 100.0 * 6.224088668823242
Epoch 1770, val loss: 1.4097518920898438
Epoch 1780, training loss: 622.4869995117188 = 0.1086602658033371 + 100.0 * 6.223783493041992
Epoch 1780, val loss: 1.4156272411346436
Epoch 1790, training loss: 622.7654418945312 = 0.10677655041217804 + 100.0 * 6.226586818695068
Epoch 1790, val loss: 1.4212766885757446
Epoch 1800, training loss: 622.4833374023438 = 0.10487977415323257 + 100.0 * 6.223784923553467
Epoch 1800, val loss: 1.426672339439392
Epoch 1810, training loss: 622.446044921875 = 0.10303808748722076 + 100.0 * 6.223430156707764
Epoch 1810, val loss: 1.4322032928466797
Epoch 1820, training loss: 622.4332885742188 = 0.1012553870677948 + 100.0 * 6.223320484161377
Epoch 1820, val loss: 1.438027262687683
Epoch 1830, training loss: 622.6111450195312 = 0.09953171759843826 + 100.0 * 6.225115776062012
Epoch 1830, val loss: 1.4436308145523071
Epoch 1840, training loss: 622.4261474609375 = 0.09776774048805237 + 100.0 * 6.223283767700195
Epoch 1840, val loss: 1.4492628574371338
Epoch 1850, training loss: 622.3983154296875 = 0.09605755656957626 + 100.0 * 6.2230224609375
Epoch 1850, val loss: 1.4551841020584106
Epoch 1860, training loss: 622.181396484375 = 0.09438467770814896 + 100.0 * 6.220870494842529
Epoch 1860, val loss: 1.461073398590088
Epoch 1870, training loss: 622.2474365234375 = 0.09277588129043579 + 100.0 * 6.221546649932861
Epoch 1870, val loss: 1.4670052528381348
Epoch 1880, training loss: 622.6689453125 = 0.0912105143070221 + 100.0 * 6.225777626037598
Epoch 1880, val loss: 1.4726719856262207
Epoch 1890, training loss: 622.201416015625 = 0.0896013155579567 + 100.0 * 6.221118450164795
Epoch 1890, val loss: 1.4778298139572144
Epoch 1900, training loss: 622.1561889648438 = 0.08806810528039932 + 100.0 * 6.220681190490723
Epoch 1900, val loss: 1.4840503931045532
Epoch 1910, training loss: 623.0211791992188 = 0.08657950907945633 + 100.0 * 6.229345798492432
Epoch 1910, val loss: 1.4897007942199707
Epoch 1920, training loss: 622.2588500976562 = 0.08505718410015106 + 100.0 * 6.221737861633301
Epoch 1920, val loss: 1.494836449623108
Epoch 1930, training loss: 622.0288696289062 = 0.08359555900096893 + 100.0 * 6.219452381134033
Epoch 1930, val loss: 1.500899076461792
Epoch 1940, training loss: 621.942138671875 = 0.08220642805099487 + 100.0 * 6.218599319458008
Epoch 1940, val loss: 1.5067142248153687
Epoch 1950, training loss: 621.9619140625 = 0.08085650205612183 + 100.0 * 6.218810558319092
Epoch 1950, val loss: 1.5125110149383545
Epoch 1960, training loss: 622.6492309570312 = 0.07955081760883331 + 100.0 * 6.225697040557861
Epoch 1960, val loss: 1.517653226852417
Epoch 1970, training loss: 622.633056640625 = 0.07812437415122986 + 100.0 * 6.225549221038818
Epoch 1970, val loss: 1.5239711999893188
Epoch 1980, training loss: 621.990234375 = 0.07675327360630035 + 100.0 * 6.21913480758667
Epoch 1980, val loss: 1.5289021730422974
Epoch 1990, training loss: 621.8844604492188 = 0.07546444237232208 + 100.0 * 6.218090057373047
Epoch 1990, val loss: 1.5349464416503906
Epoch 2000, training loss: 621.836669921875 = 0.07423040270805359 + 100.0 * 6.217624664306641
Epoch 2000, val loss: 1.5410560369491577
Epoch 2010, training loss: 621.8733520507812 = 0.07302824407815933 + 100.0 * 6.218003273010254
Epoch 2010, val loss: 1.5468586683273315
Epoch 2020, training loss: 622.6172485351562 = 0.07181375473737717 + 100.0 * 6.225454330444336
Epoch 2020, val loss: 1.5522964000701904
Epoch 2030, training loss: 621.8811645507812 = 0.07056151330471039 + 100.0 * 6.218105792999268
Epoch 2030, val loss: 1.5574312210083008
Epoch 2040, training loss: 622.0646362304688 = 0.06938023865222931 + 100.0 * 6.21995210647583
Epoch 2040, val loss: 1.5629788637161255
Epoch 2050, training loss: 621.8546142578125 = 0.06822003424167633 + 100.0 * 6.217864036560059
Epoch 2050, val loss: 1.568908929824829
Epoch 2060, training loss: 621.705322265625 = 0.0671209916472435 + 100.0 * 6.216381549835205
Epoch 2060, val loss: 1.5743310451507568
Epoch 2070, training loss: 621.8929443359375 = 0.06605210155248642 + 100.0 * 6.218269348144531
Epoch 2070, val loss: 1.5798381567001343
Epoch 2080, training loss: 621.9381713867188 = 0.06497139483690262 + 100.0 * 6.2187323570251465
Epoch 2080, val loss: 1.5852810144424438
Epoch 2090, training loss: 622.1577758789062 = 0.06390698254108429 + 100.0 * 6.220938682556152
Epoch 2090, val loss: 1.591282844543457
Epoch 2100, training loss: 621.6024780273438 = 0.06284157931804657 + 100.0 * 6.215395927429199
Epoch 2100, val loss: 1.596187710762024
Epoch 2110, training loss: 621.582275390625 = 0.06182869151234627 + 100.0 * 6.215204238891602
Epoch 2110, val loss: 1.6021329164505005
Epoch 2120, training loss: 622.2952270507812 = 0.06086035817861557 + 100.0 * 6.222343921661377
Epoch 2120, val loss: 1.6079891920089722
Epoch 2130, training loss: 621.6541748046875 = 0.059856344014406204 + 100.0 * 6.215942859649658
Epoch 2130, val loss: 1.6127541065216064
Epoch 2140, training loss: 621.5393676757812 = 0.058874987065792084 + 100.0 * 6.214804649353027
Epoch 2140, val loss: 1.6187807321548462
Epoch 2150, training loss: 621.560791015625 = 0.057958751916885376 + 100.0 * 6.215028285980225
Epoch 2150, val loss: 1.6239509582519531
Epoch 2160, training loss: 622.280517578125 = 0.05705251917243004 + 100.0 * 6.22223424911499
Epoch 2160, val loss: 1.6296732425689697
Epoch 2170, training loss: 621.7852172851562 = 0.05612682178616524 + 100.0 * 6.217290878295898
Epoch 2170, val loss: 1.6347951889038086
Epoch 2180, training loss: 621.6769409179688 = 0.055205535143613815 + 100.0 * 6.216217517852783
Epoch 2180, val loss: 1.6400904655456543
Epoch 2190, training loss: 621.470458984375 = 0.054346583783626556 + 100.0 * 6.214161396026611
Epoch 2190, val loss: 1.6457489728927612
Epoch 2200, training loss: 621.3912963867188 = 0.05350439250469208 + 100.0 * 6.213377475738525
Epoch 2200, val loss: 1.651200532913208
Epoch 2210, training loss: 621.5964965820312 = 0.05270581692457199 + 100.0 * 6.215437889099121
Epoch 2210, val loss: 1.6563224792480469
Epoch 2220, training loss: 621.6790771484375 = 0.0518643818795681 + 100.0 * 6.216271877288818
Epoch 2220, val loss: 1.6614526510238647
Epoch 2230, training loss: 621.5966186523438 = 0.0510094091296196 + 100.0 * 6.215456008911133
Epoch 2230, val loss: 1.6664243936538696
Epoch 2240, training loss: 621.4351196289062 = 0.05019215866923332 + 100.0 * 6.2138495445251465
Epoch 2240, val loss: 1.6722261905670166
Epoch 2250, training loss: 621.3310546875 = 0.0494377538561821 + 100.0 * 6.21281623840332
Epoch 2250, val loss: 1.6775166988372803
Epoch 2260, training loss: 621.3324584960938 = 0.04870317503809929 + 100.0 * 6.2128376960754395
Epoch 2260, val loss: 1.6831289529800415
Epoch 2270, training loss: 621.7686767578125 = 0.04798637330532074 + 100.0 * 6.217206954956055
Epoch 2270, val loss: 1.6883492469787598
Epoch 2280, training loss: 621.5701904296875 = 0.04722561687231064 + 100.0 * 6.215229511260986
Epoch 2280, val loss: 1.693243145942688
Epoch 2290, training loss: 621.3955688476562 = 0.04649118334054947 + 100.0 * 6.2134904861450195
Epoch 2290, val loss: 1.6982098817825317
Epoch 2300, training loss: 621.2801513671875 = 0.04578013718128204 + 100.0 * 6.212343692779541
Epoch 2300, val loss: 1.7030564546585083
Epoch 2310, training loss: 621.4122924804688 = 0.04510585963726044 + 100.0 * 6.213672161102295
Epoch 2310, val loss: 1.7082751989364624
Epoch 2320, training loss: 621.4027099609375 = 0.04441958665847778 + 100.0 * 6.213582992553711
Epoch 2320, val loss: 1.7137693166732788
Epoch 2330, training loss: 621.2649536132812 = 0.04375206306576729 + 100.0 * 6.212211608886719
Epoch 2330, val loss: 1.7183140516281128
Epoch 2340, training loss: 621.4613037109375 = 0.04312242940068245 + 100.0 * 6.214181423187256
Epoch 2340, val loss: 1.7230660915374756
Epoch 2350, training loss: 621.2760620117188 = 0.04245883226394653 + 100.0 * 6.21233606338501
Epoch 2350, val loss: 1.7286231517791748
Epoch 2360, training loss: 621.7276000976562 = 0.041842855513095856 + 100.0 * 6.216857433319092
Epoch 2360, val loss: 1.7337197065353394
Epoch 2370, training loss: 621.2115478515625 = 0.041201986372470856 + 100.0 * 6.211703300476074
Epoch 2370, val loss: 1.7377774715423584
Epoch 2380, training loss: 621.142333984375 = 0.040597718209028244 + 100.0 * 6.211017608642578
Epoch 2380, val loss: 1.743169903755188
Epoch 2390, training loss: 621.2598876953125 = 0.040023207664489746 + 100.0 * 6.212198734283447
Epoch 2390, val loss: 1.7478740215301514
Epoch 2400, training loss: 621.4644775390625 = 0.039434727281332016 + 100.0 * 6.214250564575195
Epoch 2400, val loss: 1.7528634071350098
Epoch 2410, training loss: 621.1099243164062 = 0.03884396702051163 + 100.0 * 6.2107110023498535
Epoch 2410, val loss: 1.7577762603759766
Epoch 2420, training loss: 621.0789184570312 = 0.03828926011919975 + 100.0 * 6.210406303405762
Epoch 2420, val loss: 1.7626731395721436
Epoch 2430, training loss: 621.4292602539062 = 0.03775126859545708 + 100.0 * 6.21391487121582
Epoch 2430, val loss: 1.7677158117294312
Epoch 2440, training loss: 621.0493774414062 = 0.03720157966017723 + 100.0 * 6.2101216316223145
Epoch 2440, val loss: 1.7717597484588623
Epoch 2450, training loss: 621.0545654296875 = 0.036674533039331436 + 100.0 * 6.210178852081299
Epoch 2450, val loss: 1.7765405178070068
Epoch 2460, training loss: 621.2664794921875 = 0.03616175800561905 + 100.0 * 6.212303638458252
Epoch 2460, val loss: 1.7813466787338257
Epoch 2470, training loss: 621.098876953125 = 0.03565041720867157 + 100.0 * 6.21063232421875
Epoch 2470, val loss: 1.7858887910842896
Epoch 2480, training loss: 621.2410888671875 = 0.03514792397618294 + 100.0 * 6.212059497833252
Epoch 2480, val loss: 1.7911056280136108
Epoch 2490, training loss: 621.1090698242188 = 0.034634243696928024 + 100.0 * 6.210744857788086
Epoch 2490, val loss: 1.7951375246047974
Epoch 2500, training loss: 621.0094604492188 = 0.03415117412805557 + 100.0 * 6.209753036499023
Epoch 2500, val loss: 1.7995290756225586
Epoch 2510, training loss: 620.8720703125 = 0.03368070349097252 + 100.0 * 6.208383560180664
Epoch 2510, val loss: 1.8046168088912964
Epoch 2520, training loss: 621.1663818359375 = 0.03323802351951599 + 100.0 * 6.211331367492676
Epoch 2520, val loss: 1.8091245889663696
Epoch 2530, training loss: 620.8877563476562 = 0.03276979550719261 + 100.0 * 6.208549499511719
Epoch 2530, val loss: 1.8132728338241577
Epoch 2540, training loss: 620.8229370117188 = 0.032312531024217606 + 100.0 * 6.207906723022461
Epoch 2540, val loss: 1.8175169229507446
Epoch 2550, training loss: 620.9114990234375 = 0.031882334500551224 + 100.0 * 6.208796501159668
Epoch 2550, val loss: 1.8226079940795898
Epoch 2560, training loss: 621.3340454101562 = 0.03145863860845566 + 100.0 * 6.21302604675293
Epoch 2560, val loss: 1.8265959024429321
Epoch 2570, training loss: 621.0496215820312 = 0.031026137992739677 + 100.0 * 6.210186004638672
Epoch 2570, val loss: 1.830130934715271
Epoch 2580, training loss: 620.8724365234375 = 0.030595926567912102 + 100.0 * 6.208418846130371
Epoch 2580, val loss: 1.8354246616363525
Epoch 2590, training loss: 621.0232543945312 = 0.03020169585943222 + 100.0 * 6.209930419921875
Epoch 2590, val loss: 1.8394557237625122
Epoch 2600, training loss: 620.904296875 = 0.029799921438097954 + 100.0 * 6.208745002746582
Epoch 2600, val loss: 1.8438153266906738
Epoch 2610, training loss: 620.9657592773438 = 0.029411500319838524 + 100.0 * 6.2093634605407715
Epoch 2610, val loss: 1.8479893207550049
Epoch 2620, training loss: 621.0311279296875 = 0.02901473641395569 + 100.0 * 6.210021495819092
Epoch 2620, val loss: 1.8519846200942993
Epoch 2630, training loss: 620.6678466796875 = 0.02862531691789627 + 100.0 * 6.206392288208008
Epoch 2630, val loss: 1.8558733463287354
Epoch 2640, training loss: 620.720703125 = 0.02826545760035515 + 100.0 * 6.2069244384765625
Epoch 2640, val loss: 1.8604345321655273
Epoch 2650, training loss: 620.6626586914062 = 0.02790546603500843 + 100.0 * 6.206347465515137
Epoch 2650, val loss: 1.864763855934143
Epoch 2660, training loss: 620.8859252929688 = 0.027565035969018936 + 100.0 * 6.208583354949951
Epoch 2660, val loss: 1.8687618970870972
Epoch 2670, training loss: 620.9308471679688 = 0.027205122634768486 + 100.0 * 6.209036350250244
Epoch 2670, val loss: 1.8728822469711304
Epoch 2680, training loss: 620.731689453125 = 0.026835935190320015 + 100.0 * 6.207048416137695
Epoch 2680, val loss: 1.8762657642364502
Epoch 2690, training loss: 620.6477661132812 = 0.026488328352570534 + 100.0 * 6.206212520599365
Epoch 2690, val loss: 1.8808237314224243
Epoch 2700, training loss: 620.5518798828125 = 0.026159755885601044 + 100.0 * 6.205256938934326
Epoch 2700, val loss: 1.8849389553070068
Epoch 2710, training loss: 620.8074340820312 = 0.02585293911397457 + 100.0 * 6.207815647125244
Epoch 2710, val loss: 1.8896088600158691
Epoch 2720, training loss: 620.8433837890625 = 0.02552146278321743 + 100.0 * 6.208178520202637
Epoch 2720, val loss: 1.8926392793655396
Epoch 2730, training loss: 620.6073608398438 = 0.025191428139805794 + 100.0 * 6.205821514129639
Epoch 2730, val loss: 1.896226167678833
Epoch 2740, training loss: 620.565673828125 = 0.024880005046725273 + 100.0 * 6.205407619476318
Epoch 2740, val loss: 1.9001277685165405
Epoch 2750, training loss: 621.3493041992188 = 0.02459724247455597 + 100.0 * 6.213246822357178
Epoch 2750, val loss: 1.9042785167694092
Epoch 2760, training loss: 620.6658325195312 = 0.024261431768536568 + 100.0 * 6.206416130065918
Epoch 2760, val loss: 1.9073892831802368
Epoch 2770, training loss: 620.4893798828125 = 0.02396242693066597 + 100.0 * 6.204653739929199
Epoch 2770, val loss: 1.911539077758789
Epoch 2780, training loss: 620.4534301757812 = 0.023682544007897377 + 100.0 * 6.2042975425720215
Epoch 2780, val loss: 1.9151923656463623
Epoch 2790, training loss: 620.7440185546875 = 0.023425091058015823 + 100.0 * 6.207205772399902
Epoch 2790, val loss: 1.918838620185852
Epoch 2800, training loss: 620.7280883789062 = 0.02312575653195381 + 100.0 * 6.207049369812012
Epoch 2800, val loss: 1.9221458435058594
Epoch 2810, training loss: 620.4310302734375 = 0.022824306041002274 + 100.0 * 6.204082012176514
Epoch 2810, val loss: 1.9255642890930176
Epoch 2820, training loss: 620.3945922851562 = 0.022547081112861633 + 100.0 * 6.203720569610596
Epoch 2820, val loss: 1.9297820329666138
Epoch 2830, training loss: 620.404052734375 = 0.02229524776339531 + 100.0 * 6.203817844390869
Epoch 2830, val loss: 1.9335579872131348
Epoch 2840, training loss: 620.6281127929688 = 0.022043311968445778 + 100.0 * 6.206060886383057
Epoch 2840, val loss: 1.9375262260437012
Epoch 2850, training loss: 620.8302001953125 = 0.021785926073789597 + 100.0 * 6.2080841064453125
Epoch 2850, val loss: 1.940565824508667
Epoch 2860, training loss: 620.6190795898438 = 0.021522006019949913 + 100.0 * 6.20597505569458
Epoch 2860, val loss: 1.9436970949172974
Epoch 2870, training loss: 620.4525146484375 = 0.021258577704429626 + 100.0 * 6.204312801361084
Epoch 2870, val loss: 1.9472219944000244
Epoch 2880, training loss: 620.342529296875 = 0.021023137494921684 + 100.0 * 6.203215599060059
Epoch 2880, val loss: 1.9511171579360962
Epoch 2890, training loss: 620.353759765625 = 0.02079363353550434 + 100.0 * 6.203329563140869
Epoch 2890, val loss: 1.9547719955444336
Epoch 2900, training loss: 620.9382934570312 = 0.020568447187542915 + 100.0 * 6.209177494049072
Epoch 2900, val loss: 1.958052396774292
Epoch 2910, training loss: 620.7445678710938 = 0.020330630242824554 + 100.0 * 6.207242488861084
Epoch 2910, val loss: 1.9602389335632324
Epoch 2920, training loss: 620.5045776367188 = 0.02008104883134365 + 100.0 * 6.204844951629639
Epoch 2920, val loss: 1.9641201496124268
Epoch 2930, training loss: 620.3936157226562 = 0.019854335114359856 + 100.0 * 6.203737735748291
Epoch 2930, val loss: 1.9675636291503906
Epoch 2940, training loss: 620.5146484375 = 0.01964106410741806 + 100.0 * 6.204949855804443
Epoch 2940, val loss: 1.9709068536758423
Epoch 2950, training loss: 620.79443359375 = 0.019414642825722694 + 100.0 * 6.20775032043457
Epoch 2950, val loss: 1.9741394519805908
Epoch 2960, training loss: 620.37646484375 = 0.019182676449418068 + 100.0 * 6.203572750091553
Epoch 2960, val loss: 1.9769799709320068
Epoch 2970, training loss: 620.2462768554688 = 0.018970932811498642 + 100.0 * 6.202272891998291
Epoch 2970, val loss: 1.9804909229278564
Epoch 2980, training loss: 620.2003173828125 = 0.018774712458252907 + 100.0 * 6.201815128326416
Epoch 2980, val loss: 1.9838266372680664
Epoch 2990, training loss: 620.2293701171875 = 0.018581129610538483 + 100.0 * 6.202107906341553
Epoch 2990, val loss: 1.9868453741073608
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6333333333333333
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 861.6143188476562 = 1.9305771589279175 + 100.0 * 8.596837043762207
Epoch 0, val loss: 1.9340499639511108
Epoch 10, training loss: 861.5291137695312 = 1.9223723411560059 + 100.0 * 8.596067428588867
Epoch 10, val loss: 1.9257451295852661
Epoch 20, training loss: 860.9988403320312 = 1.9117714166641235 + 100.0 * 8.59087085723877
Epoch 20, val loss: 1.9144675731658936
Epoch 30, training loss: 857.604248046875 = 1.8975749015808105 + 100.0 * 8.557066917419434
Epoch 30, val loss: 1.898935317993164
Epoch 40, training loss: 838.873046875 = 1.8803491592407227 + 100.0 * 8.369926452636719
Epoch 40, val loss: 1.8809882402420044
Epoch 50, training loss: 777.0532836914062 = 1.8609309196472168 + 100.0 * 7.751923561096191
Epoch 50, val loss: 1.8614484071731567
Epoch 60, training loss: 731.9277954101562 = 1.8484019041061401 + 100.0 * 7.3007941246032715
Epoch 60, val loss: 1.849454402923584
Epoch 70, training loss: 702.263427734375 = 1.8397716283798218 + 100.0 * 7.004236221313477
Epoch 70, val loss: 1.8406603336334229
Epoch 80, training loss: 688.1209716796875 = 1.8325116634368896 + 100.0 * 6.862884521484375
Epoch 80, val loss: 1.8330092430114746
Epoch 90, training loss: 678.4571533203125 = 1.8251277208328247 + 100.0 * 6.76632022857666
Epoch 90, val loss: 1.8253631591796875
Epoch 100, training loss: 672.1370239257812 = 1.8182848691940308 + 100.0 * 6.703186988830566
Epoch 100, val loss: 1.8184492588043213
Epoch 110, training loss: 667.3285522460938 = 1.812121868133545 + 100.0 * 6.6551642417907715
Epoch 110, val loss: 1.8123438358306885
Epoch 120, training loss: 663.1622314453125 = 1.806512713432312 + 100.0 * 6.613556861877441
Epoch 120, val loss: 1.8067926168441772
Epoch 130, training loss: 660.096435546875 = 1.8012467622756958 + 100.0 * 6.582951545715332
Epoch 130, val loss: 1.801436185836792
Epoch 140, training loss: 657.60595703125 = 1.795661211013794 + 100.0 * 6.558103084564209
Epoch 140, val loss: 1.7958393096923828
Epoch 150, training loss: 655.2147216796875 = 1.7898473739624023 + 100.0 * 6.5342488288879395
Epoch 150, val loss: 1.7903023958206177
Epoch 160, training loss: 653.0761108398438 = 1.7839373350143433 + 100.0 * 6.5129218101501465
Epoch 160, val loss: 1.7848141193389893
Epoch 170, training loss: 651.0064086914062 = 1.7776210308074951 + 100.0 * 6.492287635803223
Epoch 170, val loss: 1.7791149616241455
Epoch 180, training loss: 649.1410522460938 = 1.7709463834762573 + 100.0 * 6.473701000213623
Epoch 180, val loss: 1.7731901407241821
Epoch 190, training loss: 647.7171020507812 = 1.7635997533798218 + 100.0 * 6.459535121917725
Epoch 190, val loss: 1.7668616771697998
Epoch 200, training loss: 646.1046752929688 = 1.7556110620498657 + 100.0 * 6.443490505218506
Epoch 200, val loss: 1.7599397897720337
Epoch 210, training loss: 645.11767578125 = 1.7468984127044678 + 100.0 * 6.4337077140808105
Epoch 210, val loss: 1.752407193183899
Epoch 220, training loss: 643.8068237304688 = 1.7373576164245605 + 100.0 * 6.420694828033447
Epoch 220, val loss: 1.7442964315414429
Epoch 230, training loss: 642.6649780273438 = 1.7270876169204712 + 100.0 * 6.409378528594971
Epoch 230, val loss: 1.735593557357788
Epoch 240, training loss: 641.756103515625 = 1.7160515785217285 + 100.0 * 6.400400638580322
Epoch 240, val loss: 1.7262853384017944
Epoch 250, training loss: 641.1309814453125 = 1.7039653062820435 + 100.0 * 6.394269943237305
Epoch 250, val loss: 1.7161215543746948
Epoch 260, training loss: 640.1597290039062 = 1.6911131143569946 + 100.0 * 6.38468599319458
Epoch 260, val loss: 1.7052900791168213
Epoch 270, training loss: 639.4141235351562 = 1.6774353981018066 + 100.0 * 6.37736701965332
Epoch 270, val loss: 1.6938542127609253
Epoch 280, training loss: 638.8715209960938 = 1.6628789901733398 + 100.0 * 6.372086048126221
Epoch 280, val loss: 1.6817569732666016
Epoch 290, training loss: 638.1849365234375 = 1.647502064704895 + 100.0 * 6.3653740882873535
Epoch 290, val loss: 1.6691447496414185
Epoch 300, training loss: 637.6092529296875 = 1.6315053701400757 + 100.0 * 6.359777450561523
Epoch 300, val loss: 1.6560559272766113
Epoch 310, training loss: 637.4360961914062 = 1.614906907081604 + 100.0 * 6.358211517333984
Epoch 310, val loss: 1.6425855159759521
Epoch 320, training loss: 636.6368408203125 = 1.597601056098938 + 100.0 * 6.3503923416137695
Epoch 320, val loss: 1.6287288665771484
Epoch 330, training loss: 636.1911010742188 = 1.579984188079834 + 100.0 * 6.346111297607422
Epoch 330, val loss: 1.6149182319641113
Epoch 340, training loss: 635.8553466796875 = 1.562045931816101 + 100.0 * 6.34293270111084
Epoch 340, val loss: 1.6011909246444702
Epoch 350, training loss: 635.6182861328125 = 1.543763518333435 + 100.0 * 6.340744972229004
Epoch 350, val loss: 1.5873831510543823
Epoch 360, training loss: 635.0797729492188 = 1.5253822803497314 + 100.0 * 6.335543632507324
Epoch 360, val loss: 1.5739144086837769
Epoch 370, training loss: 634.5458984375 = 1.5068408250808716 + 100.0 * 6.330390453338623
Epoch 370, val loss: 1.560669183731079
Epoch 380, training loss: 634.2676391601562 = 1.4883551597595215 + 100.0 * 6.327793121337891
Epoch 380, val loss: 1.547898769378662
Epoch 390, training loss: 633.8924560546875 = 1.4698617458343506 + 100.0 * 6.324225902557373
Epoch 390, val loss: 1.5355699062347412
Epoch 400, training loss: 633.6133422851562 = 1.4514343738555908 + 100.0 * 6.321619033813477
Epoch 400, val loss: 1.5234692096710205
Epoch 410, training loss: 633.2831420898438 = 1.433225154876709 + 100.0 * 6.3184990882873535
Epoch 410, val loss: 1.5120707750320435
Epoch 420, training loss: 633.1638793945312 = 1.4148905277252197 + 100.0 * 6.3174896240234375
Epoch 420, val loss: 1.5007803440093994
Epoch 430, training loss: 632.7523803710938 = 1.3966584205627441 + 100.0 * 6.313557147979736
Epoch 430, val loss: 1.4898552894592285
Epoch 440, training loss: 632.4224853515625 = 1.3785735368728638 + 100.0 * 6.310438632965088
Epoch 440, val loss: 1.4793968200683594
Epoch 450, training loss: 632.2014770507812 = 1.360703468322754 + 100.0 * 6.308407306671143
Epoch 450, val loss: 1.4694026708602905
Epoch 460, training loss: 631.9761352539062 = 1.3428120613098145 + 100.0 * 6.306333065032959
Epoch 460, val loss: 1.4596937894821167
Epoch 470, training loss: 631.765380859375 = 1.3250313997268677 + 100.0 * 6.304403781890869
Epoch 470, val loss: 1.4501556158065796
Epoch 480, training loss: 631.4895629882812 = 1.3073699474334717 + 100.0 * 6.301821708679199
Epoch 480, val loss: 1.4411089420318604
Epoch 490, training loss: 631.5589599609375 = 1.2897796630859375 + 100.0 * 6.302691459655762
Epoch 490, val loss: 1.4323033094406128
Epoch 500, training loss: 631.1331787109375 = 1.2724313735961914 + 100.0 * 6.298607349395752
Epoch 500, val loss: 1.4239342212677002
Epoch 510, training loss: 631.0337524414062 = 1.2550134658813477 + 100.0 * 6.297787189483643
Epoch 510, val loss: 1.4156641960144043
Epoch 520, training loss: 630.6167602539062 = 1.2378076314926147 + 100.0 * 6.293789386749268
Epoch 520, val loss: 1.4078279733657837
Epoch 530, training loss: 630.3886108398438 = 1.220778226852417 + 100.0 * 6.291678428649902
Epoch 530, val loss: 1.400383710861206
Epoch 540, training loss: 630.49072265625 = 1.2040069103240967 + 100.0 * 6.292867660522461
Epoch 540, val loss: 1.3933433294296265
Epoch 550, training loss: 630.4457397460938 = 1.18701171875 + 100.0 * 6.2925872802734375
Epoch 550, val loss: 1.3858602046966553
Epoch 560, training loss: 629.9398803710938 = 1.1703991889953613 + 100.0 * 6.287694931030273
Epoch 560, val loss: 1.3791558742523193
Epoch 570, training loss: 629.7066040039062 = 1.1540348529815674 + 100.0 * 6.285525321960449
Epoch 570, val loss: 1.3727396726608276
Epoch 580, training loss: 629.7601928710938 = 1.1378952264785767 + 100.0 * 6.286223411560059
Epoch 580, val loss: 1.3667373657226562
Epoch 590, training loss: 629.4258422851562 = 1.1216981410980225 + 100.0 * 6.283041477203369
Epoch 590, val loss: 1.360686182975769
Epoch 600, training loss: 629.9414672851562 = 1.1059671640396118 + 100.0 * 6.288355350494385
Epoch 600, val loss: 1.355320692062378
Epoch 610, training loss: 629.043701171875 = 1.0902011394500732 + 100.0 * 6.279534816741943
Epoch 610, val loss: 1.349767804145813
Epoch 620, training loss: 629.2702026367188 = 1.0747735500335693 + 100.0 * 6.281954288482666
Epoch 620, val loss: 1.3448511362075806
Epoch 630, training loss: 628.8289794921875 = 1.0593820810317993 + 100.0 * 6.277695655822754
Epoch 630, val loss: 1.3398537635803223
Epoch 640, training loss: 628.5966796875 = 1.0444424152374268 + 100.0 * 6.275522708892822
Epoch 640, val loss: 1.3353303670883179
Epoch 650, training loss: 628.5596923828125 = 1.0297340154647827 + 100.0 * 6.275299549102783
Epoch 650, val loss: 1.3311684131622314
Epoch 660, training loss: 628.3661499023438 = 1.015057921409607 + 100.0 * 6.273510932922363
Epoch 660, val loss: 1.3269622325897217
Epoch 670, training loss: 628.3590087890625 = 1.0005531311035156 + 100.0 * 6.273584365844727
Epoch 670, val loss: 1.3230464458465576
Epoch 680, training loss: 628.0879516601562 = 0.9863991737365723 + 100.0 * 6.271015644073486
Epoch 680, val loss: 1.3194619417190552
Epoch 690, training loss: 628.342041015625 = 0.9724899530410767 + 100.0 * 6.273695468902588
Epoch 690, val loss: 1.316197395324707
Epoch 700, training loss: 627.9661865234375 = 0.9585527181625366 + 100.0 * 6.270076274871826
Epoch 700, val loss: 1.3129981756210327
Epoch 710, training loss: 627.6619262695312 = 0.9449061155319214 + 100.0 * 6.267170429229736
Epoch 710, val loss: 1.3097261190414429
Epoch 720, training loss: 627.5963134765625 = 0.9315390586853027 + 100.0 * 6.266647815704346
Epoch 720, val loss: 1.3071609735488892
Epoch 730, training loss: 627.6213989257812 = 0.9182422161102295 + 100.0 * 6.267032146453857
Epoch 730, val loss: 1.3043590784072876
Epoch 740, training loss: 627.2566528320312 = 0.9049239754676819 + 100.0 * 6.263517379760742
Epoch 740, val loss: 1.3015854358673096
Epoch 750, training loss: 627.1560668945312 = 0.892002284526825 + 100.0 * 6.262640476226807
Epoch 750, val loss: 1.2993426322937012
Epoch 760, training loss: 627.5628662109375 = 0.8793076276779175 + 100.0 * 6.2668352127075195
Epoch 760, val loss: 1.297338604927063
Epoch 770, training loss: 627.1589965820312 = 0.8665039539337158 + 100.0 * 6.262924671173096
Epoch 770, val loss: 1.2953872680664062
Epoch 780, training loss: 626.9943237304688 = 0.8539608716964722 + 100.0 * 6.261404037475586
Epoch 780, val loss: 1.2935837507247925
Epoch 790, training loss: 627.19189453125 = 0.841583788394928 + 100.0 * 6.263503074645996
Epoch 790, val loss: 1.2919222116470337
Epoch 800, training loss: 626.6670532226562 = 0.8292190432548523 + 100.0 * 6.258378028869629
Epoch 800, val loss: 1.290170431137085
Epoch 810, training loss: 626.5379638671875 = 0.8171867728233337 + 100.0 * 6.25720739364624
Epoch 810, val loss: 1.2889227867126465
Epoch 820, training loss: 626.4371948242188 = 0.8053299188613892 + 100.0 * 6.25631856918335
Epoch 820, val loss: 1.2878048419952393
Epoch 830, training loss: 627.173828125 = 0.793590247631073 + 100.0 * 6.263802528381348
Epoch 830, val loss: 1.2867965698242188
Epoch 840, training loss: 626.3064575195312 = 0.7815568447113037 + 100.0 * 6.2552490234375
Epoch 840, val loss: 1.2850395441055298
Epoch 850, training loss: 626.1716918945312 = 0.7699581980705261 + 100.0 * 6.254017353057861
Epoch 850, val loss: 1.2839027643203735
Epoch 860, training loss: 626.8515014648438 = 0.7584472298622131 + 100.0 * 6.26093053817749
Epoch 860, val loss: 1.2827833890914917
Epoch 870, training loss: 626.1580810546875 = 0.7470626831054688 + 100.0 * 6.254110336303711
Epoch 870, val loss: 1.2823162078857422
Epoch 880, training loss: 625.8809814453125 = 0.735722541809082 + 100.0 * 6.251452445983887
Epoch 880, val loss: 1.2815135717391968
Epoch 890, training loss: 625.9396362304688 = 0.7247251272201538 + 100.0 * 6.2521491050720215
Epoch 890, val loss: 1.2807496786117554
Epoch 900, training loss: 625.6595458984375 = 0.7136949896812439 + 100.0 * 6.2494587898254395
Epoch 900, val loss: 1.2801249027252197
Epoch 910, training loss: 625.6651611328125 = 0.702842116355896 + 100.0 * 6.2496232986450195
Epoch 910, val loss: 1.2796785831451416
Epoch 920, training loss: 626.3712768554688 = 0.6921098232269287 + 100.0 * 6.256791114807129
Epoch 920, val loss: 1.2796316146850586
Epoch 930, training loss: 625.6315307617188 = 0.6811298131942749 + 100.0 * 6.2495036125183105
Epoch 930, val loss: 1.2783282995224
Epoch 940, training loss: 625.3792724609375 = 0.6705545783042908 + 100.0 * 6.247087478637695
Epoch 940, val loss: 1.27847421169281
Epoch 950, training loss: 625.2882690429688 = 0.6602597236633301 + 100.0 * 6.246280193328857
Epoch 950, val loss: 1.2784820795059204
Epoch 960, training loss: 626.0945434570312 = 0.6501376032829285 + 100.0 * 6.254444122314453
Epoch 960, val loss: 1.278645634651184
Epoch 970, training loss: 625.4323120117188 = 0.6396144032478333 + 100.0 * 6.247927188873291
Epoch 970, val loss: 1.2780859470367432
Epoch 980, training loss: 625.1592407226562 = 0.6295251846313477 + 100.0 * 6.245297431945801
Epoch 980, val loss: 1.2783387899398804
Epoch 990, training loss: 625.0084228515625 = 0.6196515560150146 + 100.0 * 6.243887901306152
Epoch 990, val loss: 1.2785676717758179
Epoch 1000, training loss: 624.9097900390625 = 0.6099562048912048 + 100.0 * 6.242998123168945
Epoch 1000, val loss: 1.2792065143585205
Epoch 1010, training loss: 625.486083984375 = 0.6003291606903076 + 100.0 * 6.248857498168945
Epoch 1010, val loss: 1.2795112133026123
Epoch 1020, training loss: 624.9716186523438 = 0.5905460119247437 + 100.0 * 6.243810176849365
Epoch 1020, val loss: 1.2792903184890747
Epoch 1030, training loss: 624.7506103515625 = 0.5810766220092773 + 100.0 * 6.241695404052734
Epoch 1030, val loss: 1.2797210216522217
Epoch 1040, training loss: 624.734375 = 0.5717888474464417 + 100.0 * 6.241626262664795
Epoch 1040, val loss: 1.2804750204086304
Epoch 1050, training loss: 624.826416015625 = 0.5626124143600464 + 100.0 * 6.242637634277344
Epoch 1050, val loss: 1.2812626361846924
Epoch 1060, training loss: 624.5899047851562 = 0.5533748269081116 + 100.0 * 6.240365505218506
Epoch 1060, val loss: 1.2816007137298584
Epoch 1070, training loss: 624.4935302734375 = 0.5443404316902161 + 100.0 * 6.239491939544678
Epoch 1070, val loss: 1.2821383476257324
Epoch 1080, training loss: 624.4466552734375 = 0.5355583429336548 + 100.0 * 6.239110469818115
Epoch 1080, val loss: 1.2832056283950806
Epoch 1090, training loss: 624.6463623046875 = 0.5268160700798035 + 100.0 * 6.2411956787109375
Epoch 1090, val loss: 1.283887267112732
Epoch 1100, training loss: 624.3118286132812 = 0.5181005597114563 + 100.0 * 6.237936973571777
Epoch 1100, val loss: 1.2849622964859009
Epoch 1110, training loss: 624.7491455078125 = 0.5096165537834167 + 100.0 * 6.242394924163818
Epoch 1110, val loss: 1.2861576080322266
Epoch 1120, training loss: 624.273681640625 = 0.5009728670120239 + 100.0 * 6.237727165222168
Epoch 1120, val loss: 1.2865067720413208
Epoch 1130, training loss: 624.1130981445312 = 0.4926922023296356 + 100.0 * 6.236204147338867
Epoch 1130, val loss: 1.2878938913345337
Epoch 1140, training loss: 624.1707763671875 = 0.4845925271511078 + 100.0 * 6.236861705780029
Epoch 1140, val loss: 1.2891627550125122
Epoch 1150, training loss: 624.0858764648438 = 0.476484090089798 + 100.0 * 6.236093521118164
Epoch 1150, val loss: 1.2903462648391724
Epoch 1160, training loss: 623.9967651367188 = 0.4683969020843506 + 100.0 * 6.235283851623535
Epoch 1160, val loss: 1.2908140420913696
Epoch 1170, training loss: 624.0020751953125 = 0.4605088233947754 + 100.0 * 6.235415458679199
Epoch 1170, val loss: 1.2924604415893555
Epoch 1180, training loss: 623.9680786132812 = 0.4526682198047638 + 100.0 * 6.235153675079346
Epoch 1180, val loss: 1.2932957410812378
Epoch 1190, training loss: 623.8532104492188 = 0.4449273943901062 + 100.0 * 6.2340826988220215
Epoch 1190, val loss: 1.2949237823486328
Epoch 1200, training loss: 623.73486328125 = 0.437368780374527 + 100.0 * 6.232975006103516
Epoch 1200, val loss: 1.2961047887802124
Epoch 1210, training loss: 623.8072509765625 = 0.4299587905406952 + 100.0 * 6.233773231506348
Epoch 1210, val loss: 1.2978802919387817
Epoch 1220, training loss: 624.0482788085938 = 0.4225614070892334 + 100.0 * 6.236257553100586
Epoch 1220, val loss: 1.3000141382217407
Epoch 1230, training loss: 623.5813598632812 = 0.41501709818840027 + 100.0 * 6.231663227081299
Epoch 1230, val loss: 1.3007913827896118
Epoch 1240, training loss: 623.5042724609375 = 0.4078594148159027 + 100.0 * 6.230964660644531
Epoch 1240, val loss: 1.3031750917434692
Epoch 1250, training loss: 623.4005126953125 = 0.40085431933403015 + 100.0 * 6.229996204376221
Epoch 1250, val loss: 1.3049887418746948
Epoch 1260, training loss: 623.7227783203125 = 0.39400434494018555 + 100.0 * 6.233287811279297
Epoch 1260, val loss: 1.3075460195541382
Epoch 1270, training loss: 623.4324340820312 = 0.38686248660087585 + 100.0 * 6.2304558753967285
Epoch 1270, val loss: 1.3084319829940796
Epoch 1280, training loss: 623.42724609375 = 0.37992796301841736 + 100.0 * 6.230473041534424
Epoch 1280, val loss: 1.3108618259429932
Epoch 1290, training loss: 623.26123046875 = 0.37323305010795593 + 100.0 * 6.228879928588867
Epoch 1290, val loss: 1.3131275177001953
Epoch 1300, training loss: 623.2216796875 = 0.36674582958221436 + 100.0 * 6.228549480438232
Epoch 1300, val loss: 1.3156455755233765
Epoch 1310, training loss: 623.847900390625 = 0.36024489998817444 + 100.0 * 6.23487663269043
Epoch 1310, val loss: 1.3180991411209106
Epoch 1320, training loss: 623.306640625 = 0.35360655188560486 + 100.0 * 6.2295308113098145
Epoch 1320, val loss: 1.3199131488800049
Epoch 1330, training loss: 623.1382446289062 = 0.3473087251186371 + 100.0 * 6.227909564971924
Epoch 1330, val loss: 1.3227109909057617
Epoch 1340, training loss: 623.3162231445312 = 0.3410889208316803 + 100.0 * 6.2297515869140625
Epoch 1340, val loss: 1.325516700744629
Epoch 1350, training loss: 623.5145263671875 = 0.33490079641342163 + 100.0 * 6.2317962646484375
Epoch 1350, val loss: 1.3282814025878906
Epoch 1360, training loss: 623.0715942382812 = 0.3286674916744232 + 100.0 * 6.227429389953613
Epoch 1360, val loss: 1.3308225870132446
Epoch 1370, training loss: 622.89404296875 = 0.322761207818985 + 100.0 * 6.225712776184082
Epoch 1370, val loss: 1.3340574502944946
Epoch 1380, training loss: 622.7958984375 = 0.31697121262550354 + 100.0 * 6.224789619445801
Epoch 1380, val loss: 1.3371429443359375
Epoch 1390, training loss: 622.806884765625 = 0.3112936019897461 + 100.0 * 6.224956035614014
Epoch 1390, val loss: 1.3401453495025635
Epoch 1400, training loss: 623.4129638671875 = 0.3055548369884491 + 100.0 * 6.231074333190918
Epoch 1400, val loss: 1.3433839082717896
Epoch 1410, training loss: 622.8912353515625 = 0.29979389905929565 + 100.0 * 6.225914478302002
Epoch 1410, val loss: 1.3467025756835938
Epoch 1420, training loss: 622.7639770507812 = 0.2942087650299072 + 100.0 * 6.224698066711426
Epoch 1420, val loss: 1.3507229089736938
Epoch 1430, training loss: 622.5912475585938 = 0.2888578772544861 + 100.0 * 6.223023891448975
Epoch 1430, val loss: 1.3542364835739136
Epoch 1440, training loss: 622.7296752929688 = 0.28362855315208435 + 100.0 * 6.224460601806641
Epoch 1440, val loss: 1.3574304580688477
Epoch 1450, training loss: 622.8416137695312 = 0.27838870882987976 + 100.0 * 6.225632190704346
Epoch 1450, val loss: 1.3618032932281494
Epoch 1460, training loss: 622.922119140625 = 0.27321019768714905 + 100.0 * 6.226489543914795
Epoch 1460, val loss: 1.3660422563552856
Epoch 1470, training loss: 622.5646362304688 = 0.2680349051952362 + 100.0 * 6.222966194152832
Epoch 1470, val loss: 1.3692939281463623
Epoch 1480, training loss: 622.40673828125 = 0.2630849778652191 + 100.0 * 6.221436500549316
Epoch 1480, val loss: 1.3735755681991577
Epoch 1490, training loss: 622.3822021484375 = 0.25829899311065674 + 100.0 * 6.22123908996582
Epoch 1490, val loss: 1.378104567527771
Epoch 1500, training loss: 622.489501953125 = 0.2535915672779083 + 100.0 * 6.2223591804504395
Epoch 1500, val loss: 1.3823121786117554
Epoch 1510, training loss: 622.8743286132812 = 0.24882633984088898 + 100.0 * 6.226254940032959
Epoch 1510, val loss: 1.3864895105361938
Epoch 1520, training loss: 622.3643188476562 = 0.24400989711284637 + 100.0 * 6.221202850341797
Epoch 1520, val loss: 1.3906543254852295
Epoch 1530, training loss: 622.35693359375 = 0.23943346738815308 + 100.0 * 6.221175193786621
Epoch 1530, val loss: 1.3955508470535278
Epoch 1540, training loss: 622.29931640625 = 0.23505622148513794 + 100.0 * 6.220642566680908
Epoch 1540, val loss: 1.4005075693130493
Epoch 1550, training loss: 622.4926147460938 = 0.2307622879743576 + 100.0 * 6.222618579864502
Epoch 1550, val loss: 1.4052095413208008
Epoch 1560, training loss: 622.1068115234375 = 0.22645893692970276 + 100.0 * 6.218803882598877
Epoch 1560, val loss: 1.4101943969726562
Epoch 1570, training loss: 622.1749877929688 = 0.22231383621692657 + 100.0 * 6.219527244567871
Epoch 1570, val loss: 1.4148668050765991
Epoch 1580, training loss: 622.6905517578125 = 0.2182048261165619 + 100.0 * 6.2247233390808105
Epoch 1580, val loss: 1.4195351600646973
Epoch 1590, training loss: 622.3486938476562 = 0.21404507756233215 + 100.0 * 6.221346378326416
Epoch 1590, val loss: 1.4243415594100952
Epoch 1600, training loss: 622.1998901367188 = 0.2099931538105011 + 100.0 * 6.219898700714111
Epoch 1600, val loss: 1.429757833480835
Epoch 1610, training loss: 621.9932861328125 = 0.20614930987358093 + 100.0 * 6.217871189117432
Epoch 1610, val loss: 1.4351143836975098
Epoch 1620, training loss: 621.9306030273438 = 0.20240150392055511 + 100.0 * 6.217282295227051
Epoch 1620, val loss: 1.4407861232757568
Epoch 1630, training loss: 622.3477172851562 = 0.19876347482204437 + 100.0 * 6.221488952636719
Epoch 1630, val loss: 1.446759581565857
Epoch 1640, training loss: 621.98828125 = 0.19494283199310303 + 100.0 * 6.217933654785156
Epoch 1640, val loss: 1.4506503343582153
Epoch 1650, training loss: 622.1038818359375 = 0.1913076490163803 + 100.0 * 6.219125747680664
Epoch 1650, val loss: 1.4569305181503296
Epoch 1660, training loss: 621.9569091796875 = 0.1876935213804245 + 100.0 * 6.2176923751831055
Epoch 1660, val loss: 1.461628794670105
Epoch 1670, training loss: 621.8713989257812 = 0.18424376845359802 + 100.0 * 6.21687126159668
Epoch 1670, val loss: 1.4671653509140015
Epoch 1680, training loss: 621.9988403320312 = 0.18087433278560638 + 100.0 * 6.218180179595947
Epoch 1680, val loss: 1.4722548723220825
Epoch 1690, training loss: 621.8795166015625 = 0.1774854063987732 + 100.0 * 6.217020511627197
Epoch 1690, val loss: 1.4779778718948364
Epoch 1700, training loss: 621.9147338867188 = 0.17419056594371796 + 100.0 * 6.217405319213867
Epoch 1700, val loss: 1.4838910102844238
Epoch 1710, training loss: 622.1321411132812 = 0.170919731259346 + 100.0 * 6.2196125984191895
Epoch 1710, val loss: 1.4882811307907104
Epoch 1720, training loss: 621.7823486328125 = 0.16770093142986298 + 100.0 * 6.216145992279053
Epoch 1720, val loss: 1.4941902160644531
Epoch 1730, training loss: 621.6550903320312 = 0.1646268665790558 + 100.0 * 6.21490478515625
Epoch 1730, val loss: 1.4999314546585083
Epoch 1740, training loss: 621.6709594726562 = 0.16165392100811005 + 100.0 * 6.215092658996582
Epoch 1740, val loss: 1.5058223009109497
Epoch 1750, training loss: 622.0857543945312 = 0.15868432819843292 + 100.0 * 6.219270706176758
Epoch 1750, val loss: 1.5105074644088745
Epoch 1760, training loss: 621.7276000976562 = 0.1556948721408844 + 100.0 * 6.215719223022461
Epoch 1760, val loss: 1.5167189836502075
Epoch 1770, training loss: 621.5679931640625 = 0.15285596251487732 + 100.0 * 6.214151382446289
Epoch 1770, val loss: 1.5224589109420776
Epoch 1780, training loss: 621.7543334960938 = 0.1500900536775589 + 100.0 * 6.216042518615723
Epoch 1780, val loss: 1.5279853343963623
Epoch 1790, training loss: 621.6307983398438 = 0.1472712755203247 + 100.0 * 6.214835166931152
Epoch 1790, val loss: 1.5332880020141602
Epoch 1800, training loss: 621.5107421875 = 0.14455091953277588 + 100.0 * 6.213662147521973
Epoch 1800, val loss: 1.539155125617981
Epoch 1810, training loss: 621.5806884765625 = 0.1419309675693512 + 100.0 * 6.214387893676758
Epoch 1810, val loss: 1.5454127788543701
Epoch 1820, training loss: 621.5770263671875 = 0.1393323689699173 + 100.0 * 6.214376926422119
Epoch 1820, val loss: 1.5510679483413696
Epoch 1830, training loss: 621.7543334960938 = 0.13675673305988312 + 100.0 * 6.216175556182861
Epoch 1830, val loss: 1.5563746690750122
Epoch 1840, training loss: 621.4359741210938 = 0.13421986997127533 + 100.0 * 6.213017463684082
Epoch 1840, val loss: 1.5622618198394775
Epoch 1850, training loss: 621.3631591796875 = 0.1317959725856781 + 100.0 * 6.212313652038574
Epoch 1850, val loss: 1.5682687759399414
Epoch 1860, training loss: 621.5269165039062 = 0.1294451355934143 + 100.0 * 6.213974475860596
Epoch 1860, val loss: 1.5744497776031494
Epoch 1870, training loss: 621.45263671875 = 0.12705323100090027 + 100.0 * 6.213255405426025
Epoch 1870, val loss: 1.5796493291854858
Epoch 1880, training loss: 621.3019409179688 = 0.12472924590110779 + 100.0 * 6.2117719650268555
Epoch 1880, val loss: 1.5854241847991943
Epoch 1890, training loss: 621.4179077148438 = 0.12252350896596909 + 100.0 * 6.212954044342041
Epoch 1890, val loss: 1.592126727104187
Epoch 1900, training loss: 621.3756713867188 = 0.1202770546078682 + 100.0 * 6.21255350112915
Epoch 1900, val loss: 1.5976017713546753
Epoch 1910, training loss: 621.4674682617188 = 0.11808732897043228 + 100.0 * 6.213493347167969
Epoch 1910, val loss: 1.604045033454895
Epoch 1920, training loss: 621.1395874023438 = 0.11592226475477219 + 100.0 * 6.210236549377441
Epoch 1920, val loss: 1.6093895435333252
Epoch 1930, training loss: 621.154541015625 = 0.11386588215827942 + 100.0 * 6.210406303405762
Epoch 1930, val loss: 1.6149284839630127
Epoch 1940, training loss: 621.2166137695312 = 0.1118796169757843 + 100.0 * 6.211047649383545
Epoch 1940, val loss: 1.6214066743850708
Epoch 1950, training loss: 621.5648193359375 = 0.10986552387475967 + 100.0 * 6.214549541473389
Epoch 1950, val loss: 1.6274726390838623
Epoch 1960, training loss: 621.2559204101562 = 0.10782445222139359 + 100.0 * 6.211480617523193
Epoch 1960, val loss: 1.6330721378326416
Epoch 1970, training loss: 621.0723266601562 = 0.1059107705950737 + 100.0 * 6.209664344787598
Epoch 1970, val loss: 1.6389514207839966
Epoch 1980, training loss: 621.0 = 0.10407042503356934 + 100.0 * 6.208959102630615
Epoch 1980, val loss: 1.6451812982559204
Epoch 1990, training loss: 621.4769287109375 = 0.10227740556001663 + 100.0 * 6.213746070861816
Epoch 1990, val loss: 1.650587558746338
Epoch 2000, training loss: 620.95166015625 = 0.10041658580303192 + 100.0 * 6.208512783050537
Epoch 2000, val loss: 1.6577529907226562
Epoch 2010, training loss: 620.9998779296875 = 0.0986446887254715 + 100.0 * 6.209012508392334
Epoch 2010, val loss: 1.6635257005691528
Epoch 2020, training loss: 621.3719482421875 = 0.09693848341703415 + 100.0 * 6.212750434875488
Epoch 2020, val loss: 1.6698334217071533
Epoch 2030, training loss: 621.0506591796875 = 0.09520300477743149 + 100.0 * 6.209554672241211
Epoch 2030, val loss: 1.6747589111328125
Epoch 2040, training loss: 621.009521484375 = 0.09352997690439224 + 100.0 * 6.209160327911377
Epoch 2040, val loss: 1.6810510158538818
Epoch 2050, training loss: 620.955322265625 = 0.09192857891321182 + 100.0 * 6.208633899688721
Epoch 2050, val loss: 1.6878752708435059
Epoch 2060, training loss: 620.8800048828125 = 0.09034507721662521 + 100.0 * 6.207896709442139
Epoch 2060, val loss: 1.693680763244629
Epoch 2070, training loss: 621.11474609375 = 0.08882355690002441 + 100.0 * 6.210259437561035
Epoch 2070, val loss: 1.7003145217895508
Epoch 2080, training loss: 620.8630981445312 = 0.08726319670677185 + 100.0 * 6.207757949829102
Epoch 2080, val loss: 1.7058417797088623
Epoch 2090, training loss: 621.1746215820312 = 0.08576442301273346 + 100.0 * 6.210888385772705
Epoch 2090, val loss: 1.7111819982528687
Epoch 2100, training loss: 621.0810546875 = 0.08423478156328201 + 100.0 * 6.209968090057373
Epoch 2100, val loss: 1.7165076732635498
Epoch 2110, training loss: 620.8147583007812 = 0.08276142179965973 + 100.0 * 6.207320213317871
Epoch 2110, val loss: 1.723199486732483
Epoch 2120, training loss: 620.6947631835938 = 0.08135954290628433 + 100.0 * 6.206133842468262
Epoch 2120, val loss: 1.7292280197143555
Epoch 2130, training loss: 620.795166015625 = 0.08001294732093811 + 100.0 * 6.207151412963867
Epoch 2130, val loss: 1.7350431680679321
Epoch 2140, training loss: 621.289794921875 = 0.07866596430540085 + 100.0 * 6.212111473083496
Epoch 2140, val loss: 1.7409374713897705
Epoch 2150, training loss: 620.950439453125 = 0.07728311419487 + 100.0 * 6.208731651306152
Epoch 2150, val loss: 1.747270941734314
Epoch 2160, training loss: 620.6807250976562 = 0.07595688104629517 + 100.0 * 6.206048011779785
Epoch 2160, val loss: 1.7527514696121216
Epoch 2170, training loss: 620.7478637695312 = 0.07473617047071457 + 100.0 * 6.20673131942749
Epoch 2170, val loss: 1.7592331171035767
Epoch 2180, training loss: 621.1163330078125 = 0.07350269705057144 + 100.0 * 6.210428237915039
Epoch 2180, val loss: 1.7654043436050415
Epoch 2190, training loss: 620.9315185546875 = 0.07221775501966476 + 100.0 * 6.208593368530273
Epoch 2190, val loss: 1.7703452110290527
Epoch 2200, training loss: 620.6802978515625 = 0.07101430743932724 + 100.0 * 6.2060933113098145
Epoch 2200, val loss: 1.7764852046966553
Epoch 2210, training loss: 620.6396484375 = 0.06985808908939362 + 100.0 * 6.205697536468506
Epoch 2210, val loss: 1.7823314666748047
Epoch 2220, training loss: 620.7554321289062 = 0.06872349232435226 + 100.0 * 6.206867218017578
Epoch 2220, val loss: 1.7883495092391968
Epoch 2230, training loss: 620.6012573242188 = 0.06760328263044357 + 100.0 * 6.205336093902588
Epoch 2230, val loss: 1.7947555780410767
Epoch 2240, training loss: 620.9838256835938 = 0.06651941686868668 + 100.0 * 6.20917272567749
Epoch 2240, val loss: 1.801177978515625
Epoch 2250, training loss: 620.715576171875 = 0.0653790757060051 + 100.0 * 6.2065019607543945
Epoch 2250, val loss: 1.8052185773849487
Epoch 2260, training loss: 620.4608764648438 = 0.0643063634634018 + 100.0 * 6.203965663909912
Epoch 2260, val loss: 1.8116768598556519
Epoch 2270, training loss: 620.47705078125 = 0.06329721957445145 + 100.0 * 6.204137325286865
Epoch 2270, val loss: 1.8171528577804565
Epoch 2280, training loss: 621.2402954101562 = 0.06232471391558647 + 100.0 * 6.211779594421387
Epoch 2280, val loss: 1.8239843845367432
Epoch 2290, training loss: 620.7156982421875 = 0.06124042347073555 + 100.0 * 6.206544876098633
Epoch 2290, val loss: 1.8284748792648315
Epoch 2300, training loss: 620.5037841796875 = 0.06023929640650749 + 100.0 * 6.204435348510742
Epoch 2300, val loss: 1.8341200351715088
Epoch 2310, training loss: 620.46728515625 = 0.05930524319410324 + 100.0 * 6.204079627990723
Epoch 2310, val loss: 1.8402341604232788
Epoch 2320, training loss: 620.7008666992188 = 0.05838947743177414 + 100.0 * 6.206424713134766
Epoch 2320, val loss: 1.8460153341293335
Epoch 2330, training loss: 620.4837646484375 = 0.05744696781039238 + 100.0 * 6.204263210296631
Epoch 2330, val loss: 1.851762294769287
Epoch 2340, training loss: 620.5556640625 = 0.05653039366006851 + 100.0 * 6.204991340637207
Epoch 2340, val loss: 1.8567107915878296
Epoch 2350, training loss: 620.6452026367188 = 0.05562880262732506 + 100.0 * 6.205895900726318
Epoch 2350, val loss: 1.8624529838562012
Epoch 2360, training loss: 620.5325927734375 = 0.05475080385804176 + 100.0 * 6.204778671264648
Epoch 2360, val loss: 1.8672922849655151
Epoch 2370, training loss: 620.3285522460938 = 0.05389832332730293 + 100.0 * 6.202746868133545
Epoch 2370, val loss: 1.8734288215637207
Epoch 2380, training loss: 620.416259765625 = 0.05308828130364418 + 100.0 * 6.20363187789917
Epoch 2380, val loss: 1.8787720203399658
Epoch 2390, training loss: 620.6180419921875 = 0.052267204970121384 + 100.0 * 6.205657958984375
Epoch 2390, val loss: 1.884520411491394
Epoch 2400, training loss: 620.5101928710938 = 0.051441412419080734 + 100.0 * 6.204587459564209
Epoch 2400, val loss: 1.8904989957809448
Epoch 2410, training loss: 620.3462524414062 = 0.05063899978995323 + 100.0 * 6.202956199645996
Epoch 2410, val loss: 1.8954095840454102
Epoch 2420, training loss: 620.2232055664062 = 0.04987505078315735 + 100.0 * 6.201733589172363
Epoch 2420, val loss: 1.901051640510559
Epoch 2430, training loss: 620.3060913085938 = 0.04914971813559532 + 100.0 * 6.202569484710693
Epoch 2430, val loss: 1.907017707824707
Epoch 2440, training loss: 620.6587524414062 = 0.04841836541891098 + 100.0 * 6.206103801727295
Epoch 2440, val loss: 1.9123706817626953
Epoch 2450, training loss: 620.4270629882812 = 0.04765206575393677 + 100.0 * 6.203794002532959
Epoch 2450, val loss: 1.917587399482727
Epoch 2460, training loss: 620.5018310546875 = 0.04694240167737007 + 100.0 * 6.2045488357543945
Epoch 2460, val loss: 1.9235835075378418
Epoch 2470, training loss: 620.3923950195312 = 0.046230994164943695 + 100.0 * 6.203461647033691
Epoch 2470, val loss: 1.9280496835708618
Epoch 2480, training loss: 620.225341796875 = 0.045533474534749985 + 100.0 * 6.201797962188721
Epoch 2480, val loss: 1.933531403541565
Epoch 2490, training loss: 620.09521484375 = 0.04486460983753204 + 100.0 * 6.200503826141357
Epoch 2490, val loss: 1.938908338546753
Epoch 2500, training loss: 620.315673828125 = 0.04423263296484947 + 100.0 * 6.202713966369629
Epoch 2500, val loss: 1.9440158605575562
Epoch 2510, training loss: 620.357421875 = 0.0435447059571743 + 100.0 * 6.203139305114746
Epoch 2510, val loss: 1.9485760927200317
Epoch 2520, training loss: 620.0504150390625 = 0.04286130145192146 + 100.0 * 6.200075626373291
Epoch 2520, val loss: 1.9542462825775146
Epoch 2530, training loss: 620.0415649414062 = 0.04225244000554085 + 100.0 * 6.199993133544922
Epoch 2530, val loss: 1.9592289924621582
Epoch 2540, training loss: 620.0037231445312 = 0.04167012870311737 + 100.0 * 6.199620246887207
Epoch 2540, val loss: 1.964721441268921
Epoch 2550, training loss: 620.0684204101562 = 0.04111358895897865 + 100.0 * 6.200273036956787
Epoch 2550, val loss: 1.9698824882507324
Epoch 2560, training loss: 620.659423828125 = 0.0405423678457737 + 100.0 * 6.206188678741455
Epoch 2560, val loss: 1.9754905700683594
Epoch 2570, training loss: 620.27001953125 = 0.039916422218084335 + 100.0 * 6.202301025390625
Epoch 2570, val loss: 1.9799489974975586
Epoch 2580, training loss: 620.2545166015625 = 0.03936087712645531 + 100.0 * 6.202151298522949
Epoch 2580, val loss: 1.9853684902191162
Epoch 2590, training loss: 620.2302856445312 = 0.03879575803875923 + 100.0 * 6.2019147872924805
Epoch 2590, val loss: 1.9899455308914185
Epoch 2600, training loss: 619.9774780273438 = 0.03823920339345932 + 100.0 * 6.199391841888428
Epoch 2600, val loss: 1.9940720796585083
Epoch 2610, training loss: 620.1049194335938 = 0.03772719204425812 + 100.0 * 6.200672149658203
Epoch 2610, val loss: 1.9991298913955688
Epoch 2620, training loss: 620.2564697265625 = 0.037204381078481674 + 100.0 * 6.202192306518555
Epoch 2620, val loss: 2.004932403564453
Epoch 2630, training loss: 619.9967651367188 = 0.036659277975559235 + 100.0 * 6.199600696563721
Epoch 2630, val loss: 2.00864839553833
Epoch 2640, training loss: 619.90869140625 = 0.03617660701274872 + 100.0 * 6.19872522354126
Epoch 2640, val loss: 2.0142953395843506
Epoch 2650, training loss: 619.8770751953125 = 0.0357058122754097 + 100.0 * 6.198413848876953
Epoch 2650, val loss: 2.0190351009368896
Epoch 2660, training loss: 620.616943359375 = 0.03525399789214134 + 100.0 * 6.205816745758057
Epoch 2660, val loss: 2.0242700576782227
Epoch 2670, training loss: 620.0751342773438 = 0.03473346307873726 + 100.0 * 6.200404167175293
Epoch 2670, val loss: 2.0279533863067627
Epoch 2680, training loss: 620.0107421875 = 0.03425460308790207 + 100.0 * 6.199765205383301
Epoch 2680, val loss: 2.0334103107452393
Epoch 2690, training loss: 619.883544921875 = 0.03379671648144722 + 100.0 * 6.198497295379639
Epoch 2690, val loss: 2.0377326011657715
Epoch 2700, training loss: 620.0992431640625 = 0.03336561843752861 + 100.0 * 6.200658321380615
Epoch 2700, val loss: 2.0415291786193848
Epoch 2710, training loss: 620.0213623046875 = 0.032909732311964035 + 100.0 * 6.199884414672852
Epoch 2710, val loss: 2.0468759536743164
Epoch 2720, training loss: 619.8775634765625 = 0.03246389701962471 + 100.0 * 6.198451042175293
Epoch 2720, val loss: 2.0515296459198
Epoch 2730, training loss: 619.8322143554688 = 0.03203901648521423 + 100.0 * 6.198001861572266
Epoch 2730, val loss: 2.0568768978118896
Epoch 2740, training loss: 619.8518676757812 = 0.03163953125476837 + 100.0 * 6.198202610015869
Epoch 2740, val loss: 2.0614702701568604
Epoch 2750, training loss: 619.9859619140625 = 0.031234970316290855 + 100.0 * 6.199546813964844
Epoch 2750, val loss: 2.0656933784484863
Epoch 2760, training loss: 619.781982421875 = 0.03082595206797123 + 100.0 * 6.197511672973633
Epoch 2760, val loss: 2.070309638977051
Epoch 2770, training loss: 619.9172973632812 = 0.030438361689448357 + 100.0 * 6.198868274688721
Epoch 2770, val loss: 2.075336456298828
Epoch 2780, training loss: 619.8873291015625 = 0.030054444447159767 + 100.0 * 6.198573112487793
Epoch 2780, val loss: 2.0792016983032227
Epoch 2790, training loss: 620.064697265625 = 0.02966667152941227 + 100.0 * 6.200350284576416
Epoch 2790, val loss: 2.084351062774658
Epoch 2800, training loss: 619.7587280273438 = 0.029280204325914383 + 100.0 * 6.197294235229492
Epoch 2800, val loss: 2.0879013538360596
Epoch 2810, training loss: 619.6867065429688 = 0.028920788317918777 + 100.0 * 6.196577548980713
Epoch 2810, val loss: 2.0925674438476562
Epoch 2820, training loss: 620.3344116210938 = 0.028577325865626335 + 100.0 * 6.20305871963501
Epoch 2820, val loss: 2.0964457988739014
Epoch 2830, training loss: 619.763916015625 = 0.028196098282933235 + 100.0 * 6.197357177734375
Epoch 2830, val loss: 2.10084867477417
Epoch 2840, training loss: 619.8365478515625 = 0.0278465673327446 + 100.0 * 6.198087215423584
Epoch 2840, val loss: 2.1051554679870605
Epoch 2850, training loss: 619.9852294921875 = 0.027499310672283173 + 100.0 * 6.1995768547058105
Epoch 2850, val loss: 2.108743906021118
Epoch 2860, training loss: 619.8089599609375 = 0.027159681543707848 + 100.0 * 6.197817802429199
Epoch 2860, val loss: 2.1145870685577393
Epoch 2870, training loss: 619.5811767578125 = 0.026830632239580154 + 100.0 * 6.19554328918457
Epoch 2870, val loss: 2.1182637214660645
Epoch 2880, training loss: 619.5778198242188 = 0.026524659246206284 + 100.0 * 6.195512771606445
Epoch 2880, val loss: 2.122889280319214
Epoch 2890, training loss: 619.748779296875 = 0.026226894930005074 + 100.0 * 6.197225093841553
Epoch 2890, val loss: 2.1272239685058594
Epoch 2900, training loss: 619.8362426757812 = 0.025903958827257156 + 100.0 * 6.198103427886963
Epoch 2900, val loss: 2.1319262981414795
Epoch 2910, training loss: 619.6315307617188 = 0.025571510195732117 + 100.0 * 6.196059703826904
Epoch 2910, val loss: 2.135646343231201
Epoch 2920, training loss: 619.7293090820312 = 0.025275519117712975 + 100.0 * 6.197040557861328
Epoch 2920, val loss: 2.1389997005462646
Epoch 2930, training loss: 619.9231567382812 = 0.024973195046186447 + 100.0 * 6.198982238769531
Epoch 2930, val loss: 2.1433627605438232
Epoch 2940, training loss: 619.748291015625 = 0.024668710306286812 + 100.0 * 6.19723653793335
Epoch 2940, val loss: 2.147754430770874
Epoch 2950, training loss: 619.7666015625 = 0.024370605126023293 + 100.0 * 6.197422504425049
Epoch 2950, val loss: 2.1506898403167725
Epoch 2960, training loss: 619.6756591796875 = 0.02408740296959877 + 100.0 * 6.1965155601501465
Epoch 2960, val loss: 2.155900001525879
Epoch 2970, training loss: 619.57958984375 = 0.023816464468836784 + 100.0 * 6.195557594299316
Epoch 2970, val loss: 2.1594865322113037
Epoch 2980, training loss: 619.5904541015625 = 0.023555316030979156 + 100.0 * 6.195668697357178
Epoch 2980, val loss: 2.1641845703125
Epoch 2990, training loss: 619.7615356445312 = 0.023286957293748856 + 100.0 * 6.19738245010376
Epoch 2990, val loss: 2.167741537094116
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.625925925925926
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 861.6301879882812 = 1.9455746412277222 + 100.0 * 8.596846580505371
Epoch 0, val loss: 1.9476048946380615
Epoch 10, training loss: 861.55029296875 = 1.9369308948516846 + 100.0 * 8.5961332321167
Epoch 10, val loss: 1.9387938976287842
Epoch 20, training loss: 861.0467529296875 = 1.9258098602294922 + 100.0 * 8.591209411621094
Epoch 20, val loss: 1.9273709058761597
Epoch 30, training loss: 857.7709350585938 = 1.911393642425537 + 100.0 * 8.558595657348633
Epoch 30, val loss: 1.9125971794128418
Epoch 40, training loss: 840.4181518554688 = 1.8950693607330322 + 100.0 * 8.385231018066406
Epoch 40, val loss: 1.8968225717544556
Epoch 50, training loss: 793.2716064453125 = 1.8789563179016113 + 100.0 * 7.913926124572754
Epoch 50, val loss: 1.8811277151107788
Epoch 60, training loss: 750.769775390625 = 1.8666678667068481 + 100.0 * 7.4890313148498535
Epoch 60, val loss: 1.8698489665985107
Epoch 70, training loss: 708.182861328125 = 1.8569276332855225 + 100.0 * 7.063259124755859
Epoch 70, val loss: 1.8600151538848877
Epoch 80, training loss: 687.7836303710938 = 1.8475840091705322 + 100.0 * 6.859360218048096
Epoch 80, val loss: 1.8501960039138794
Epoch 90, training loss: 677.8740844726562 = 1.839030146598816 + 100.0 * 6.760350227355957
Epoch 90, val loss: 1.8410000801086426
Epoch 100, training loss: 670.6987915039062 = 1.8314355611801147 + 100.0 * 6.688673973083496
Epoch 100, val loss: 1.8331143856048584
Epoch 110, training loss: 666.01611328125 = 1.8247991800308228 + 100.0 * 6.641912937164307
Epoch 110, val loss: 1.8263015747070312
Epoch 120, training loss: 662.1680908203125 = 1.8186299800872803 + 100.0 * 6.603495121002197
Epoch 120, val loss: 1.8199330568313599
Epoch 130, training loss: 658.8789672851562 = 1.8127833604812622 + 100.0 * 6.570661544799805
Epoch 130, val loss: 1.8138830661773682
Epoch 140, training loss: 656.2554321289062 = 1.807250738143921 + 100.0 * 6.5444817543029785
Epoch 140, val loss: 1.8080904483795166
Epoch 150, training loss: 654.0529174804688 = 1.8016527891159058 + 100.0 * 6.522512912750244
Epoch 150, val loss: 1.802512764930725
Epoch 160, training loss: 652.1448364257812 = 1.795792818069458 + 100.0 * 6.503490447998047
Epoch 160, val loss: 1.7969814538955688
Epoch 170, training loss: 650.625244140625 = 1.7895973920822144 + 100.0 * 6.488356113433838
Epoch 170, val loss: 1.7913408279418945
Epoch 180, training loss: 649.11376953125 = 1.7829782962799072 + 100.0 * 6.473308086395264
Epoch 180, val loss: 1.785405158996582
Epoch 190, training loss: 647.8486938476562 = 1.7758599519729614 + 100.0 * 6.460728168487549
Epoch 190, val loss: 1.7792315483093262
Epoch 200, training loss: 646.6900024414062 = 1.7682210206985474 + 100.0 * 6.449217319488525
Epoch 200, val loss: 1.7726633548736572
Epoch 210, training loss: 646.0184326171875 = 1.7598907947540283 + 100.0 * 6.442585468292236
Epoch 210, val loss: 1.765641450881958
Epoch 220, training loss: 644.7212524414062 = 1.750887393951416 + 100.0 * 6.429703235626221
Epoch 220, val loss: 1.7581027746200562
Epoch 230, training loss: 643.7014770507812 = 1.7411938905715942 + 100.0 * 6.419602394104004
Epoch 230, val loss: 1.7501490116119385
Epoch 240, training loss: 642.7595825195312 = 1.7307233810424805 + 100.0 * 6.4102888107299805
Epoch 240, val loss: 1.7415673732757568
Epoch 250, training loss: 642.6583251953125 = 1.719286561012268 + 100.0 * 6.409389972686768
Epoch 250, val loss: 1.7323211431503296
Epoch 260, training loss: 641.2730712890625 = 1.706880807876587 + 100.0 * 6.3956618309021
Epoch 260, val loss: 1.7222808599472046
Epoch 270, training loss: 640.6257934570312 = 1.6935220956802368 + 100.0 * 6.389322280883789
Epoch 270, val loss: 1.7114896774291992
Epoch 280, training loss: 639.9613647460938 = 1.6791003942489624 + 100.0 * 6.3828229904174805
Epoch 280, val loss: 1.6999770402908325
Epoch 290, training loss: 639.6996459960938 = 1.66362726688385 + 100.0 * 6.380360126495361
Epoch 290, val loss: 1.687556505203247
Epoch 300, training loss: 638.924072265625 = 1.6469415426254272 + 100.0 * 6.372771739959717
Epoch 300, val loss: 1.6744059324264526
Epoch 310, training loss: 638.3639526367188 = 1.6292864084243774 + 100.0 * 6.36734676361084
Epoch 310, val loss: 1.660454511642456
Epoch 320, training loss: 638.5686645507812 = 1.610648274421692 + 100.0 * 6.369580268859863
Epoch 320, val loss: 1.645754098892212
Epoch 330, training loss: 637.6024780273438 = 1.5908429622650146 + 100.0 * 6.360116481781006
Epoch 330, val loss: 1.630150318145752
Epoch 340, training loss: 637.08251953125 = 1.5702507495880127 + 100.0 * 6.3551225662231445
Epoch 340, val loss: 1.6140763759613037
Epoch 350, training loss: 636.6445922851562 = 1.548934817314148 + 100.0 * 6.350956439971924
Epoch 350, val loss: 1.5975745916366577
Epoch 360, training loss: 636.8619384765625 = 1.5269057750701904 + 100.0 * 6.3533501625061035
Epoch 360, val loss: 1.5806491374969482
Epoch 370, training loss: 635.9304809570312 = 1.5042518377304077 + 100.0 * 6.34426212310791
Epoch 370, val loss: 1.563148021697998
Epoch 380, training loss: 635.5758056640625 = 1.4812018871307373 + 100.0 * 6.340946197509766
Epoch 380, val loss: 1.5454615354537964
Epoch 390, training loss: 635.228759765625 = 1.4578585624694824 + 100.0 * 6.337708950042725
Epoch 390, val loss: 1.5277364253997803
Epoch 400, training loss: 634.9671630859375 = 1.4341717958450317 + 100.0 * 6.335330009460449
Epoch 400, val loss: 1.509742021560669
Epoch 410, training loss: 634.644287109375 = 1.4103292226791382 + 100.0 * 6.332339286804199
Epoch 410, val loss: 1.491973876953125
Epoch 420, training loss: 634.3175659179688 = 1.3865454196929932 + 100.0 * 6.329310417175293
Epoch 420, val loss: 1.4744547605514526
Epoch 430, training loss: 634.2369384765625 = 1.3628772497177124 + 100.0 * 6.32874059677124
Epoch 430, val loss: 1.4570810794830322
Epoch 440, training loss: 633.7175903320312 = 1.3394103050231934 + 100.0 * 6.323781490325928
Epoch 440, val loss: 1.4399224519729614
Epoch 450, training loss: 633.5271606445312 = 1.3161084651947021 + 100.0 * 6.322110652923584
Epoch 450, val loss: 1.4231970310211182
Epoch 460, training loss: 633.24267578125 = 1.2929363250732422 + 100.0 * 6.319497585296631
Epoch 460, val loss: 1.406800389289856
Epoch 470, training loss: 632.9041748046875 = 1.270164132118225 + 100.0 * 6.316340446472168
Epoch 470, val loss: 1.3906842470169067
Epoch 480, training loss: 632.5761108398438 = 1.2477757930755615 + 100.0 * 6.3132829666137695
Epoch 480, val loss: 1.375319004058838
Epoch 490, training loss: 632.689208984375 = 1.225730299949646 + 100.0 * 6.314635276794434
Epoch 490, val loss: 1.3601465225219727
Epoch 500, training loss: 632.33203125 = 1.2038127183914185 + 100.0 * 6.311282634735107
Epoch 500, val loss: 1.3456594944000244
Epoch 510, training loss: 631.9844360351562 = 1.1823177337646484 + 100.0 * 6.308021545410156
Epoch 510, val loss: 1.331352710723877
Epoch 520, training loss: 631.602783203125 = 1.1614290475845337 + 100.0 * 6.304413318634033
Epoch 520, val loss: 1.3178482055664062
Epoch 530, training loss: 631.4854736328125 = 1.1409955024719238 + 100.0 * 6.303444862365723
Epoch 530, val loss: 1.3048465251922607
Epoch 540, training loss: 631.3901977539062 = 1.1207389831542969 + 100.0 * 6.302694797515869
Epoch 540, val loss: 1.292459487915039
Epoch 550, training loss: 631.0424194335938 = 1.1008743047714233 + 100.0 * 6.299415111541748
Epoch 550, val loss: 1.2801198959350586
Epoch 560, training loss: 630.8825073242188 = 1.081445574760437 + 100.0 * 6.29801082611084
Epoch 560, val loss: 1.2687528133392334
Epoch 570, training loss: 630.6846923828125 = 1.0624479055404663 + 100.0 * 6.296222686767578
Epoch 570, val loss: 1.257262945175171
Epoch 580, training loss: 630.3990478515625 = 1.0437880754470825 + 100.0 * 6.293552875518799
Epoch 580, val loss: 1.24690580368042
Epoch 590, training loss: 630.1729736328125 = 1.0256669521331787 + 100.0 * 6.291472911834717
Epoch 590, val loss: 1.2368484735488892
Epoch 600, training loss: 630.0023803710938 = 1.007993459701538 + 100.0 * 6.289943695068359
Epoch 600, val loss: 1.2273057699203491
Epoch 610, training loss: 629.9452514648438 = 0.9907269477844238 + 100.0 * 6.289545059204102
Epoch 610, val loss: 1.2184391021728516
Epoch 620, training loss: 629.9738159179688 = 0.9735046625137329 + 100.0 * 6.290002822875977
Epoch 620, val loss: 1.2096086740493774
Epoch 630, training loss: 629.6761474609375 = 0.9566099643707275 + 100.0 * 6.287195205688477
Epoch 630, val loss: 1.2010483741760254
Epoch 640, training loss: 629.39208984375 = 0.9402060508728027 + 100.0 * 6.284519195556641
Epoch 640, val loss: 1.193399429321289
Epoch 650, training loss: 629.2011108398438 = 0.9242576360702515 + 100.0 * 6.282768726348877
Epoch 650, val loss: 1.1861951351165771
Epoch 660, training loss: 629.081787109375 = 0.9086586236953735 + 100.0 * 6.281731605529785
Epoch 660, val loss: 1.1794580221176147
Epoch 670, training loss: 629.6368408203125 = 0.8932906985282898 + 100.0 * 6.287435054779053
Epoch 670, val loss: 1.1727954149246216
Epoch 680, training loss: 628.964111328125 = 0.8780300617218018 + 100.0 * 6.280860424041748
Epoch 680, val loss: 1.1666007041931152
Epoch 690, training loss: 628.7822265625 = 0.8631098866462708 + 100.0 * 6.279191493988037
Epoch 690, val loss: 1.1607517004013062
Epoch 700, training loss: 628.6046752929688 = 0.848518967628479 + 100.0 * 6.277561664581299
Epoch 700, val loss: 1.1552976369857788
Epoch 710, training loss: 628.4120483398438 = 0.8342298269271851 + 100.0 * 6.275778293609619
Epoch 710, val loss: 1.150246500968933
Epoch 720, training loss: 628.8405151367188 = 0.8202214241027832 + 100.0 * 6.280202865600586
Epoch 720, val loss: 1.145548701286316
Epoch 730, training loss: 628.4354858398438 = 0.8061075806617737 + 100.0 * 6.276294231414795
Epoch 730, val loss: 1.1410993337631226
Epoch 740, training loss: 628.0513305664062 = 0.7924887537956238 + 100.0 * 6.27258825302124
Epoch 740, val loss: 1.1366539001464844
Epoch 750, training loss: 627.9923095703125 = 0.7791236042976379 + 100.0 * 6.27213191986084
Epoch 750, val loss: 1.1328397989273071
Epoch 760, training loss: 627.9637451171875 = 0.7657467722892761 + 100.0 * 6.271980285644531
Epoch 760, val loss: 1.129116177558899
Epoch 770, training loss: 627.9175415039062 = 0.7526166439056396 + 100.0 * 6.271649360656738
Epoch 770, val loss: 1.1254291534423828
Epoch 780, training loss: 627.576904296875 = 0.7397729158401489 + 100.0 * 6.26837158203125
Epoch 780, val loss: 1.1223504543304443
Epoch 790, training loss: 627.447509765625 = 0.7272094488143921 + 100.0 * 6.267202854156494
Epoch 790, val loss: 1.1196259260177612
Epoch 800, training loss: 628.1627807617188 = 0.7148765325546265 + 100.0 * 6.274478912353516
Epoch 800, val loss: 1.116441011428833
Epoch 810, training loss: 627.5182495117188 = 0.7023338079452515 + 100.0 * 6.26815938949585
Epoch 810, val loss: 1.1145440340042114
Epoch 820, training loss: 627.1435546875 = 0.6902197599411011 + 100.0 * 6.264533042907715
Epoch 820, val loss: 1.1122491359710693
Epoch 830, training loss: 627.3699340820312 = 0.6783637404441833 + 100.0 * 6.266915798187256
Epoch 830, val loss: 1.1098498106002808
Epoch 840, training loss: 626.9968872070312 = 0.6665133237838745 + 100.0 * 6.263303756713867
Epoch 840, val loss: 1.108270525932312
Epoch 850, training loss: 626.90283203125 = 0.6549304723739624 + 100.0 * 6.262479305267334
Epoch 850, val loss: 1.1066153049468994
Epoch 860, training loss: 626.870849609375 = 0.6436046957969666 + 100.0 * 6.262272357940674
Epoch 860, val loss: 1.105260968208313
Epoch 870, training loss: 626.9602661132812 = 0.6323253512382507 + 100.0 * 6.263279438018799
Epoch 870, val loss: 1.1036728620529175
Epoch 880, training loss: 626.6427001953125 = 0.621100127696991 + 100.0 * 6.260215759277344
Epoch 880, val loss: 1.1029198169708252
Epoch 890, training loss: 626.5198364257812 = 0.6102178692817688 + 100.0 * 6.259096145629883
Epoch 890, val loss: 1.101895809173584
Epoch 900, training loss: 626.4730834960938 = 0.5995001196861267 + 100.0 * 6.2587361335754395
Epoch 900, val loss: 1.1013935804367065
Epoch 910, training loss: 626.5588989257812 = 0.5888049602508545 + 100.0 * 6.259700775146484
Epoch 910, val loss: 1.1007859706878662
Epoch 920, training loss: 626.4752807617188 = 0.578187108039856 + 100.0 * 6.258970737457275
Epoch 920, val loss: 1.1000981330871582
Epoch 930, training loss: 626.2006225585938 = 0.5679172873497009 + 100.0 * 6.256326675415039
Epoch 930, val loss: 1.0999369621276855
Epoch 940, training loss: 626.1069946289062 = 0.5578625798225403 + 100.0 * 6.255491256713867
Epoch 940, val loss: 1.100172519683838
Epoch 950, training loss: 626.7083129882812 = 0.5479001998901367 + 100.0 * 6.261603832244873
Epoch 950, val loss: 1.10047447681427
Epoch 960, training loss: 626.1764526367188 = 0.5378814339637756 + 100.0 * 6.256385803222656
Epoch 960, val loss: 1.100155234336853
Epoch 970, training loss: 626.0353393554688 = 0.5281733870506287 + 100.0 * 6.255071640014648
Epoch 970, val loss: 1.1006245613098145
Epoch 980, training loss: 626.0477294921875 = 0.5185777544975281 + 100.0 * 6.25529146194458
Epoch 980, val loss: 1.1007914543151855
Epoch 990, training loss: 625.8650512695312 = 0.5090495347976685 + 100.0 * 6.2535600662231445
Epoch 990, val loss: 1.1015437841415405
Epoch 1000, training loss: 625.6993408203125 = 0.49973538517951965 + 100.0 * 6.25199556350708
Epoch 1000, val loss: 1.1027218103408813
Epoch 1010, training loss: 625.6376953125 = 0.4906540513038635 + 100.0 * 6.251470565795898
Epoch 1010, val loss: 1.1038525104522705
Epoch 1020, training loss: 625.8209838867188 = 0.4816878139972687 + 100.0 * 6.253392696380615
Epoch 1020, val loss: 1.105044960975647
Epoch 1030, training loss: 626.18505859375 = 0.4727846086025238 + 100.0 * 6.257122993469238
Epoch 1030, val loss: 1.1053332090377808
Epoch 1040, training loss: 625.6625366210938 = 0.46390873193740845 + 100.0 * 6.251986026763916
Epoch 1040, val loss: 1.1070955991744995
Epoch 1050, training loss: 625.3499145507812 = 0.45528650283813477 + 100.0 * 6.248946666717529
Epoch 1050, val loss: 1.1083189249038696
Epoch 1060, training loss: 625.2459716796875 = 0.4469112753868103 + 100.0 * 6.247990608215332
Epoch 1060, val loss: 1.1100276708602905
Epoch 1070, training loss: 625.4139404296875 = 0.4386667013168335 + 100.0 * 6.249752521514893
Epoch 1070, val loss: 1.111794352531433
Epoch 1080, training loss: 625.2271728515625 = 0.43039703369140625 + 100.0 * 6.24796724319458
Epoch 1080, val loss: 1.113599181175232
Epoch 1090, training loss: 625.2565307617188 = 0.4222520887851715 + 100.0 * 6.248342990875244
Epoch 1090, val loss: 1.11505925655365
Epoch 1100, training loss: 625.2820434570312 = 0.41430726647377014 + 100.0 * 6.2486772537231445
Epoch 1100, val loss: 1.1170663833618164
Epoch 1110, training loss: 624.9698486328125 = 0.4064846336841583 + 100.0 * 6.245633602142334
Epoch 1110, val loss: 1.1194912195205688
Epoch 1120, training loss: 624.9832763671875 = 0.3988506495952606 + 100.0 * 6.245843887329102
Epoch 1120, val loss: 1.1218600273132324
Epoch 1130, training loss: 625.3309936523438 = 0.3913092017173767 + 100.0 * 6.249396800994873
Epoch 1130, val loss: 1.124376654624939
Epoch 1140, training loss: 625.1105346679688 = 0.3838002383708954 + 100.0 * 6.247267723083496
Epoch 1140, val loss: 1.1258693933486938
Epoch 1150, training loss: 624.8693237304688 = 0.3764094412326813 + 100.0 * 6.244929313659668
Epoch 1150, val loss: 1.1291863918304443
Epoch 1160, training loss: 624.7291870117188 = 0.36925768852233887 + 100.0 * 6.2435994148254395
Epoch 1160, val loss: 1.1314687728881836
Epoch 1170, training loss: 625.1392822265625 = 0.36222711205482483 + 100.0 * 6.247770309448242
Epoch 1170, val loss: 1.1342370510101318
Epoch 1180, training loss: 624.7750244140625 = 0.35518956184387207 + 100.0 * 6.244198799133301
Epoch 1180, val loss: 1.1373718976974487
Epoch 1190, training loss: 624.7144775390625 = 0.3482978641986847 + 100.0 * 6.243661403656006
Epoch 1190, val loss: 1.140051007270813
Epoch 1200, training loss: 624.577880859375 = 0.34157589077949524 + 100.0 * 6.242363452911377
Epoch 1200, val loss: 1.143231749534607
Epoch 1210, training loss: 625.1685791015625 = 0.3350215554237366 + 100.0 * 6.248335838317871
Epoch 1210, val loss: 1.1459206342697144
Epoch 1220, training loss: 624.688720703125 = 0.3283623158931732 + 100.0 * 6.243603706359863
Epoch 1220, val loss: 1.1495819091796875
Epoch 1230, training loss: 624.41015625 = 0.3219577968120575 + 100.0 * 6.24088191986084
Epoch 1230, val loss: 1.1525394916534424
Epoch 1240, training loss: 624.3094482421875 = 0.31571176648139954 + 100.0 * 6.2399373054504395
Epoch 1240, val loss: 1.1560420989990234
Epoch 1250, training loss: 624.8148193359375 = 0.3095833957195282 + 100.0 * 6.245052337646484
Epoch 1250, val loss: 1.1592146158218384
Epoch 1260, training loss: 624.435791015625 = 0.30340924859046936 + 100.0 * 6.241323947906494
Epoch 1260, val loss: 1.1629778146743774
Epoch 1270, training loss: 624.3370971679688 = 0.29739251732826233 + 100.0 * 6.2403974533081055
Epoch 1270, val loss: 1.1664185523986816
Epoch 1280, training loss: 624.475341796875 = 0.29149487614631653 + 100.0 * 6.241838455200195
Epoch 1280, val loss: 1.1701394319534302
Epoch 1290, training loss: 624.1837768554688 = 0.2857147753238678 + 100.0 * 6.238980770111084
Epoch 1290, val loss: 1.1735519170761108
Epoch 1300, training loss: 624.0460205078125 = 0.28006336092948914 + 100.0 * 6.237659454345703
Epoch 1300, val loss: 1.177717685699463
Epoch 1310, training loss: 624.6998291015625 = 0.2745521664619446 + 100.0 * 6.244253158569336
Epoch 1310, val loss: 1.1819039583206177
Epoch 1320, training loss: 624.2626953125 = 0.2689574062824249 + 100.0 * 6.2399373054504395
Epoch 1320, val loss: 1.1846402883529663
Epoch 1330, training loss: 624.072021484375 = 0.2635245621204376 + 100.0 * 6.23808479309082
Epoch 1330, val loss: 1.1891463994979858
Epoch 1340, training loss: 623.8602294921875 = 0.2582714855670929 + 100.0 * 6.236019134521484
Epoch 1340, val loss: 1.1930662393569946
Epoch 1350, training loss: 623.7922973632812 = 0.2531346082687378 + 100.0 * 6.235391616821289
Epoch 1350, val loss: 1.1973341703414917
Epoch 1360, training loss: 624.15380859375 = 0.24810515344142914 + 100.0 * 6.2390570640563965
Epoch 1360, val loss: 1.2017391920089722
Epoch 1370, training loss: 623.860107421875 = 0.24304553866386414 + 100.0 * 6.236170768737793
Epoch 1370, val loss: 1.205341100692749
Epoch 1380, training loss: 623.7490234375 = 0.23805969953536987 + 100.0 * 6.235109806060791
Epoch 1380, val loss: 1.2098824977874756
Epoch 1390, training loss: 623.7274780273438 = 0.23326270282268524 + 100.0 * 6.2349419593811035
Epoch 1390, val loss: 1.2141211032867432
Epoch 1400, training loss: 623.7491455078125 = 0.22857187688350677 + 100.0 * 6.23520565032959
Epoch 1400, val loss: 1.2187389135360718
Epoch 1410, training loss: 623.6089477539062 = 0.22396698594093323 + 100.0 * 6.233850002288818
Epoch 1410, val loss: 1.2230556011199951
Epoch 1420, training loss: 624.1452026367188 = 0.21944458782672882 + 100.0 * 6.2392578125
Epoch 1420, val loss: 1.2271655797958374
Epoch 1430, training loss: 623.6897583007812 = 0.214883491396904 + 100.0 * 6.234748363494873
Epoch 1430, val loss: 1.232192039489746
Epoch 1440, training loss: 623.6702270507812 = 0.21050186455249786 + 100.0 * 6.234597206115723
Epoch 1440, val loss: 1.236838698387146
Epoch 1450, training loss: 623.5853881835938 = 0.2061864137649536 + 100.0 * 6.233792304992676
Epoch 1450, val loss: 1.2415364980697632
Epoch 1460, training loss: 623.5201416015625 = 0.20196162164211273 + 100.0 * 6.233181953430176
Epoch 1460, val loss: 1.2462159395217896
Epoch 1470, training loss: 623.7623291015625 = 0.19788292050361633 + 100.0 * 6.235644340515137
Epoch 1470, val loss: 1.2516173124313354
Epoch 1480, training loss: 623.4600830078125 = 0.19380927085876465 + 100.0 * 6.232662677764893
Epoch 1480, val loss: 1.255430817604065
Epoch 1490, training loss: 623.3335571289062 = 0.18987925350666046 + 100.0 * 6.231436729431152
Epoch 1490, val loss: 1.2608569860458374
Epoch 1500, training loss: 623.3522338867188 = 0.18604101240634918 + 100.0 * 6.231661796569824
Epoch 1500, val loss: 1.2655380964279175
Epoch 1510, training loss: 623.4166259765625 = 0.18226715922355652 + 100.0 * 6.232343673706055
Epoch 1510, val loss: 1.2706609964370728
Epoch 1520, training loss: 623.4712524414062 = 0.17854587733745575 + 100.0 * 6.232927322387695
Epoch 1520, val loss: 1.2759912014007568
Epoch 1530, training loss: 623.2350463867188 = 0.1748979687690735 + 100.0 * 6.2306013107299805
Epoch 1530, val loss: 1.2804477214813232
Epoch 1540, training loss: 623.4799194335938 = 0.17139454185962677 + 100.0 * 6.2330851554870605
Epoch 1540, val loss: 1.2860088348388672
Epoch 1550, training loss: 623.2389526367188 = 0.16787199676036835 + 100.0 * 6.230710983276367
Epoch 1550, val loss: 1.2908096313476562
Epoch 1560, training loss: 623.014404296875 = 0.16447150707244873 + 100.0 * 6.228499412536621
Epoch 1560, val loss: 1.296478509902954
Epoch 1570, training loss: 623.0361938476562 = 0.16118371486663818 + 100.0 * 6.228749752044678
Epoch 1570, val loss: 1.3015735149383545
Epoch 1580, training loss: 623.3710327148438 = 0.15797455608844757 + 100.0 * 6.232130527496338
Epoch 1580, val loss: 1.3068045377731323
Epoch 1590, training loss: 623.38525390625 = 0.15473982691764832 + 100.0 * 6.23230504989624
Epoch 1590, val loss: 1.311871886253357
Epoch 1600, training loss: 622.9651489257812 = 0.15156814455986023 + 100.0 * 6.22813606262207
Epoch 1600, val loss: 1.317290186882019
Epoch 1610, training loss: 623.0712280273438 = 0.14855897426605225 + 100.0 * 6.229226589202881
Epoch 1610, val loss: 1.3227213621139526
Epoch 1620, training loss: 622.9830932617188 = 0.14557547867298126 + 100.0 * 6.22837495803833
Epoch 1620, val loss: 1.3281012773513794
Epoch 1630, training loss: 622.9398193359375 = 0.14266201853752136 + 100.0 * 6.22797155380249
Epoch 1630, val loss: 1.3334088325500488
Epoch 1640, training loss: 622.737060546875 = 0.13981099426746368 + 100.0 * 6.2259721755981445
Epoch 1640, val loss: 1.3388605117797852
Epoch 1650, training loss: 622.78955078125 = 0.1370687037706375 + 100.0 * 6.226524829864502
Epoch 1650, val loss: 1.3448023796081543
Epoch 1660, training loss: 623.2308349609375 = 0.13437528908252716 + 100.0 * 6.230964660644531
Epoch 1660, val loss: 1.350085973739624
Epoch 1670, training loss: 623.044921875 = 0.13167229294776917 + 100.0 * 6.229132652282715
Epoch 1670, val loss: 1.3548507690429688
Epoch 1680, training loss: 622.7156372070312 = 0.1290367841720581 + 100.0 * 6.225865840911865
Epoch 1680, val loss: 1.3605098724365234
Epoch 1690, training loss: 622.6239624023438 = 0.1265365481376648 + 100.0 * 6.224974155426025
Epoch 1690, val loss: 1.3659371137619019
Epoch 1700, training loss: 622.7047729492188 = 0.12409719079732895 + 100.0 * 6.225806713104248
Epoch 1700, val loss: 1.3717923164367676
Epoch 1710, training loss: 622.8544921875 = 0.121680848300457 + 100.0 * 6.227327823638916
Epoch 1710, val loss: 1.377274513244629
Epoch 1720, training loss: 622.711181640625 = 0.11928297579288483 + 100.0 * 6.225919246673584
Epoch 1720, val loss: 1.3825139999389648
Epoch 1730, training loss: 622.8680419921875 = 0.11696720868349075 + 100.0 * 6.227510929107666
Epoch 1730, val loss: 1.3885977268218994
Epoch 1740, training loss: 622.6510620117188 = 0.11465571075677872 + 100.0 * 6.225363731384277
Epoch 1740, val loss: 1.393812656402588
Epoch 1750, training loss: 622.4548950195312 = 0.11241233348846436 + 100.0 * 6.223424911499023
Epoch 1750, val loss: 1.3989813327789307
Epoch 1760, training loss: 622.4305419921875 = 0.11026972532272339 + 100.0 * 6.223202705383301
Epoch 1760, val loss: 1.4049007892608643
Epoch 1770, training loss: 622.5349731445312 = 0.10818536579608917 + 100.0 * 6.224267482757568
Epoch 1770, val loss: 1.4105303287506104
Epoch 1780, training loss: 622.4334106445312 = 0.10610894113779068 + 100.0 * 6.223273277282715
Epoch 1780, val loss: 1.4163271188735962
Epoch 1790, training loss: 622.8639526367188 = 0.1041036918759346 + 100.0 * 6.227598190307617
Epoch 1790, val loss: 1.4216067790985107
Epoch 1800, training loss: 622.4335327148438 = 0.10206860303878784 + 100.0 * 6.2233147621154785
Epoch 1800, val loss: 1.4269299507141113
Epoch 1810, training loss: 622.3193969726562 = 0.10012447088956833 + 100.0 * 6.222193241119385
Epoch 1810, val loss: 1.432834506034851
Epoch 1820, training loss: 622.5623168945312 = 0.09825202077627182 + 100.0 * 6.224640369415283
Epoch 1820, val loss: 1.439033031463623
Epoch 1830, training loss: 622.3052978515625 = 0.09638150036334991 + 100.0 * 6.2220892906188965
Epoch 1830, val loss: 1.4437288045883179
Epoch 1840, training loss: 622.3047485351562 = 0.09456939995288849 + 100.0 * 6.222102165222168
Epoch 1840, val loss: 1.4492340087890625
Epoch 1850, training loss: 622.3182373046875 = 0.0928099974989891 + 100.0 * 6.222254276275635
Epoch 1850, val loss: 1.455110788345337
Epoch 1860, training loss: 622.2020263671875 = 0.09108132123947144 + 100.0 * 6.221109867095947
Epoch 1860, val loss: 1.4601017236709595
Epoch 1870, training loss: 622.311767578125 = 0.08940741419792175 + 100.0 * 6.222223281860352
Epoch 1870, val loss: 1.4649463891983032
Epoch 1880, training loss: 622.3184814453125 = 0.08774842321872711 + 100.0 * 6.222307205200195
Epoch 1880, val loss: 1.4710915088653564
Epoch 1890, training loss: 622.728759765625 = 0.0861065536737442 + 100.0 * 6.226426124572754
Epoch 1890, val loss: 1.476164698600769
Epoch 1900, training loss: 622.2442016601562 = 0.08446992933750153 + 100.0 * 6.221597194671631
Epoch 1900, val loss: 1.4817073345184326
Epoch 1910, training loss: 622.041015625 = 0.08293000608682632 + 100.0 * 6.21958065032959
Epoch 1910, val loss: 1.4870288372039795
Epoch 1920, training loss: 621.9425659179688 = 0.08144998550415039 + 100.0 * 6.218611240386963
Epoch 1920, val loss: 1.4927090406417847
Epoch 1930, training loss: 622.1096801757812 = 0.08001033961772919 + 100.0 * 6.220296382904053
Epoch 1930, val loss: 1.4981504678726196
Epoch 1940, training loss: 622.0449829101562 = 0.07853417098522186 + 100.0 * 6.219664573669434
Epoch 1940, val loss: 1.5036935806274414
Epoch 1950, training loss: 622.137451171875 = 0.07708805054426193 + 100.0 * 6.2206034660339355
Epoch 1950, val loss: 1.5084420442581177
Epoch 1960, training loss: 621.8931884765625 = 0.075688436627388 + 100.0 * 6.218174934387207
Epoch 1960, val loss: 1.5139238834381104
Epoch 1970, training loss: 621.9194946289062 = 0.0743497833609581 + 100.0 * 6.218451499938965
Epoch 1970, val loss: 1.5195560455322266
Epoch 1980, training loss: 622.0667724609375 = 0.07305658608675003 + 100.0 * 6.219936847686768
Epoch 1980, val loss: 1.5246537923812866
Epoch 1990, training loss: 622.1376953125 = 0.07174466550350189 + 100.0 * 6.2206597328186035
Epoch 1990, val loss: 1.5297218561172485
Epoch 2000, training loss: 621.7813720703125 = 0.07043267786502838 + 100.0 * 6.217109680175781
Epoch 2000, val loss: 1.5355861186981201
Epoch 2010, training loss: 621.8330688476562 = 0.06920962035655975 + 100.0 * 6.2176384925842285
Epoch 2010, val loss: 1.5410645008087158
Epoch 2020, training loss: 622.1275024414062 = 0.06802000850439072 + 100.0 * 6.220594882965088
Epoch 2020, val loss: 1.5463142395019531
Epoch 2030, training loss: 621.8511352539062 = 0.06680700182914734 + 100.0 * 6.217843055725098
Epoch 2030, val loss: 1.5509687662124634
Epoch 2040, training loss: 621.7526245117188 = 0.06561745703220367 + 100.0 * 6.216869831085205
Epoch 2040, val loss: 1.5560580492019653
Epoch 2050, training loss: 621.6359252929688 = 0.06449951231479645 + 100.0 * 6.215713977813721
Epoch 2050, val loss: 1.5616649389266968
Epoch 2060, training loss: 621.62255859375 = 0.0634199008345604 + 100.0 * 6.2155914306640625
Epoch 2060, val loss: 1.5671724081039429
Epoch 2070, training loss: 622.3884887695312 = 0.06237658113241196 + 100.0 * 6.22326135635376
Epoch 2070, val loss: 1.57282555103302
Epoch 2080, training loss: 621.81494140625 = 0.06126738712191582 + 100.0 * 6.217536926269531
Epoch 2080, val loss: 1.5766850709915161
Epoch 2090, training loss: 621.8557739257812 = 0.06021713465452194 + 100.0 * 6.217955112457275
Epoch 2090, val loss: 1.582956314086914
Epoch 2100, training loss: 621.7261352539062 = 0.05919013172388077 + 100.0 * 6.216669082641602
Epoch 2100, val loss: 1.587240219116211
Epoch 2110, training loss: 621.6021118164062 = 0.0581984668970108 + 100.0 * 6.2154388427734375
Epoch 2110, val loss: 1.591845989227295
Epoch 2120, training loss: 621.5676879882812 = 0.057248495519161224 + 100.0 * 6.215104103088379
Epoch 2120, val loss: 1.5975186824798584
Epoch 2130, training loss: 622.1853637695312 = 0.056315455585718155 + 100.0 * 6.221290588378906
Epoch 2130, val loss: 1.6025429964065552
Epoch 2140, training loss: 621.6549682617188 = 0.055337730795145035 + 100.0 * 6.215996265411377
Epoch 2140, val loss: 1.606799840927124
Epoch 2150, training loss: 621.5582885742188 = 0.05443252623081207 + 100.0 * 6.215038299560547
Epoch 2150, val loss: 1.612319827079773
Epoch 2160, training loss: 622.0425415039062 = 0.05355612561106682 + 100.0 * 6.219890117645264
Epoch 2160, val loss: 1.617560625076294
Epoch 2170, training loss: 621.5064086914062 = 0.0526566281914711 + 100.0 * 6.214537620544434
Epoch 2170, val loss: 1.6213171482086182
Epoch 2180, training loss: 621.3903198242188 = 0.051800552755594254 + 100.0 * 6.213385105133057
Epoch 2180, val loss: 1.626766324043274
Epoch 2190, training loss: 621.3727416992188 = 0.05098659545183182 + 100.0 * 6.213217735290527
Epoch 2190, val loss: 1.6319432258605957
Epoch 2200, training loss: 621.5515747070312 = 0.05019471049308777 + 100.0 * 6.2150139808654785
Epoch 2200, val loss: 1.6370915174484253
Epoch 2210, training loss: 621.6154174804688 = 0.04937358573079109 + 100.0 * 6.215660572052002
Epoch 2210, val loss: 1.6408222913742065
Epoch 2220, training loss: 621.5026245117188 = 0.048575252294540405 + 100.0 * 6.214540481567383
Epoch 2220, val loss: 1.6456501483917236
Epoch 2230, training loss: 621.8240356445312 = 0.04778394103050232 + 100.0 * 6.217762470245361
Epoch 2230, val loss: 1.649754285812378
Epoch 2240, training loss: 621.4234619140625 = 0.047013286501169205 + 100.0 * 6.213764667510986
Epoch 2240, val loss: 1.6557400226593018
Epoch 2250, training loss: 621.2901611328125 = 0.046291183680295944 + 100.0 * 6.212439060211182
Epoch 2250, val loss: 1.6597884893417358
Epoch 2260, training loss: 621.7536010742188 = 0.04559466242790222 + 100.0 * 6.217080116271973
Epoch 2260, val loss: 1.6651890277862549
Epoch 2270, training loss: 621.342529296875 = 0.044866904616355896 + 100.0 * 6.212976455688477
Epoch 2270, val loss: 1.6692932844161987
Epoch 2280, training loss: 621.2504272460938 = 0.044173598289489746 + 100.0 * 6.212062358856201
Epoch 2280, val loss: 1.674235224723816
Epoch 2290, training loss: 621.2869873046875 = 0.04351208359003067 + 100.0 * 6.212434768676758
Epoch 2290, val loss: 1.6788933277130127
Epoch 2300, training loss: 621.561279296875 = 0.04285965859889984 + 100.0 * 6.215184211730957
Epoch 2300, val loss: 1.6836994886398315
Epoch 2310, training loss: 621.2489624023438 = 0.04219701513648033 + 100.0 * 6.2120680809021
Epoch 2310, val loss: 1.6879518032073975
Epoch 2320, training loss: 621.1889038085938 = 0.041564084589481354 + 100.0 * 6.21147346496582
Epoch 2320, val loss: 1.6925474405288696
Epoch 2330, training loss: 621.4412231445312 = 0.04095853492617607 + 100.0 * 6.21400260925293
Epoch 2330, val loss: 1.696862816810608
Epoch 2340, training loss: 621.3641967773438 = 0.04034198448061943 + 100.0 * 6.21323823928833
Epoch 2340, val loss: 1.7005259990692139
Epoch 2350, training loss: 621.51220703125 = 0.039731234312057495 + 100.0 * 6.214724540710449
Epoch 2350, val loss: 1.7049180269241333
Epoch 2360, training loss: 621.2439575195312 = 0.03913194313645363 + 100.0 * 6.212048053741455
Epoch 2360, val loss: 1.7103019952774048
Epoch 2370, training loss: 621.1158447265625 = 0.038559213280677795 + 100.0 * 6.21077299118042
Epoch 2370, val loss: 1.7143808603286743
Epoch 2380, training loss: 621.1721801757812 = 0.03801927715539932 + 100.0 * 6.211341381072998
Epoch 2380, val loss: 1.719021201133728
Epoch 2390, training loss: 621.531494140625 = 0.03746993839740753 + 100.0 * 6.214940071105957
Epoch 2390, val loss: 1.723418116569519
Epoch 2400, training loss: 621.5150756835938 = 0.03689940273761749 + 100.0 * 6.214781761169434
Epoch 2400, val loss: 1.7272543907165527
Epoch 2410, training loss: 621.1038818359375 = 0.03634846955537796 + 100.0 * 6.21067476272583
Epoch 2410, val loss: 1.7311004400253296
Epoch 2420, training loss: 620.9984741210938 = 0.03584019094705582 + 100.0 * 6.209626197814941
Epoch 2420, val loss: 1.7361005544662476
Epoch 2430, training loss: 620.9901733398438 = 0.03535424917936325 + 100.0 * 6.209548473358154
Epoch 2430, val loss: 1.7406522035598755
Epoch 2440, training loss: 621.2905883789062 = 0.03487414866685867 + 100.0 * 6.212557315826416
Epoch 2440, val loss: 1.745021104812622
Epoch 2450, training loss: 620.9821166992188 = 0.03437095507979393 + 100.0 * 6.209477424621582
Epoch 2450, val loss: 1.7486201524734497
Epoch 2460, training loss: 621.185302734375 = 0.03389931842684746 + 100.0 * 6.211514472961426
Epoch 2460, val loss: 1.7520520687103271
Epoch 2470, training loss: 621.1895141601562 = 0.03342439979314804 + 100.0 * 6.2115607261657715
Epoch 2470, val loss: 1.7567901611328125
Epoch 2480, training loss: 621.156005859375 = 0.03294846788048744 + 100.0 * 6.211230278015137
Epoch 2480, val loss: 1.7611610889434814
Epoch 2490, training loss: 620.9591064453125 = 0.03248726949095726 + 100.0 * 6.209266185760498
Epoch 2490, val loss: 1.765319585800171
Epoch 2500, training loss: 620.9085693359375 = 0.03205318748950958 + 100.0 * 6.208765506744385
Epoch 2500, val loss: 1.7691435813903809
Epoch 2510, training loss: 620.9817504882812 = 0.031629521399736404 + 100.0 * 6.209501266479492
Epoch 2510, val loss: 1.7733670473098755
Epoch 2520, training loss: 621.4646606445312 = 0.031207067891955376 + 100.0 * 6.214334487915039
Epoch 2520, val loss: 1.777326226234436
Epoch 2530, training loss: 621.1307373046875 = 0.03076561726629734 + 100.0 * 6.210999488830566
Epoch 2530, val loss: 1.780968427658081
Epoch 2540, training loss: 620.86474609375 = 0.030343037098646164 + 100.0 * 6.208343982696533
Epoch 2540, val loss: 1.784953236579895
Epoch 2550, training loss: 620.789794921875 = 0.02995213121175766 + 100.0 * 6.2075982093811035
Epoch 2550, val loss: 1.7895361185073853
Epoch 2560, training loss: 620.7769775390625 = 0.029573872685432434 + 100.0 * 6.2074737548828125
Epoch 2560, val loss: 1.7937774658203125
Epoch 2570, training loss: 621.1780395507812 = 0.02921057678759098 + 100.0 * 6.211488246917725
Epoch 2570, val loss: 1.7979663610458374
Epoch 2580, training loss: 620.8677978515625 = 0.028812041506171227 + 100.0 * 6.208389759063721
Epoch 2580, val loss: 1.8006867170333862
Epoch 2590, training loss: 621.1369018554688 = 0.02843421697616577 + 100.0 * 6.211084365844727
Epoch 2590, val loss: 1.8039536476135254
Epoch 2600, training loss: 620.8389282226562 = 0.02804573066532612 + 100.0 * 6.208108425140381
Epoch 2600, val loss: 1.8084229230880737
Epoch 2610, training loss: 620.7091674804688 = 0.02768774703145027 + 100.0 * 6.206814765930176
Epoch 2610, val loss: 1.8122724294662476
Epoch 2620, training loss: 620.9524536132812 = 0.02735353633761406 + 100.0 * 6.2092509269714355
Epoch 2620, val loss: 1.816156268119812
Epoch 2630, training loss: 620.88037109375 = 0.026997601613402367 + 100.0 * 6.208534240722656
Epoch 2630, val loss: 1.8200359344482422
Epoch 2640, training loss: 620.836669921875 = 0.026658112183213234 + 100.0 * 6.208099842071533
Epoch 2640, val loss: 1.8239786624908447
Epoch 2650, training loss: 620.8513793945312 = 0.026327939704060555 + 100.0 * 6.208250522613525
Epoch 2650, val loss: 1.8276538848876953
Epoch 2660, training loss: 620.7568359375 = 0.02599833346903324 + 100.0 * 6.207308292388916
Epoch 2660, val loss: 1.8316385746002197
Epoch 2670, training loss: 620.7637329101562 = 0.025676589459180832 + 100.0 * 6.207380294799805
Epoch 2670, val loss: 1.8354005813598633
Epoch 2680, training loss: 620.75244140625 = 0.025368167087435722 + 100.0 * 6.207270622253418
Epoch 2680, val loss: 1.8383136987686157
Epoch 2690, training loss: 621.2464599609375 = 0.025058425962924957 + 100.0 * 6.21221399307251
Epoch 2690, val loss: 1.8420205116271973
Epoch 2700, training loss: 620.687744140625 = 0.02472488209605217 + 100.0 * 6.206630229949951
Epoch 2700, val loss: 1.8452450037002563
Epoch 2710, training loss: 620.5502319335938 = 0.02442980743944645 + 100.0 * 6.205258369445801
Epoch 2710, val loss: 1.849640130996704
Epoch 2720, training loss: 620.5362548828125 = 0.02414664626121521 + 100.0 * 6.205121040344238
Epoch 2720, val loss: 1.8531473875045776
Epoch 2730, training loss: 621.2642822265625 = 0.02387828752398491 + 100.0 * 6.212404251098633
Epoch 2730, val loss: 1.8567118644714355
Epoch 2740, training loss: 620.7158813476562 = 0.023566635325551033 + 100.0 * 6.206923007965088
Epoch 2740, val loss: 1.8602114915847778
Epoch 2750, training loss: 620.6543579101562 = 0.023278240114450455 + 100.0 * 6.206311225891113
Epoch 2750, val loss: 1.8635286092758179
Epoch 2760, training loss: 620.5697631835938 = 0.023006049916148186 + 100.0 * 6.205467700958252
Epoch 2760, val loss: 1.8675565719604492
Epoch 2770, training loss: 620.64697265625 = 0.02274979092180729 + 100.0 * 6.206242084503174
Epoch 2770, val loss: 1.8713704347610474
Epoch 2780, training loss: 620.6456909179688 = 0.02248450741171837 + 100.0 * 6.206232070922852
Epoch 2780, val loss: 1.8746522665023804
Epoch 2790, training loss: 620.5009155273438 = 0.022224852815270424 + 100.0 * 6.204786777496338
Epoch 2790, val loss: 1.877557396888733
Epoch 2800, training loss: 621.1549682617188 = 0.02198043279349804 + 100.0 * 6.211329936981201
Epoch 2800, val loss: 1.8809189796447754
Epoch 2810, training loss: 620.8505249023438 = 0.021708229556679726 + 100.0 * 6.208287715911865
Epoch 2810, val loss: 1.8842053413391113
Epoch 2820, training loss: 620.5555419921875 = 0.02145286649465561 + 100.0 * 6.205341339111328
Epoch 2820, val loss: 1.8871510028839111
Epoch 2830, training loss: 620.4382934570312 = 0.02121380716562271 + 100.0 * 6.2041707038879395
Epoch 2830, val loss: 1.8914800882339478
Epoch 2840, training loss: 620.4360961914062 = 0.020989039912819862 + 100.0 * 6.204151153564453
Epoch 2840, val loss: 1.8948253393173218
Epoch 2850, training loss: 620.9086303710938 = 0.020764879882335663 + 100.0 * 6.208878993988037
Epoch 2850, val loss: 1.8985315561294556
Epoch 2860, training loss: 620.41015625 = 0.0205178614705801 + 100.0 * 6.203896522521973
Epoch 2860, val loss: 1.9012222290039062
Epoch 2870, training loss: 620.7450561523438 = 0.020293008536100388 + 100.0 * 6.207247734069824
Epoch 2870, val loss: 1.9051611423492432
Epoch 2880, training loss: 620.3821411132812 = 0.02005458064377308 + 100.0 * 6.203620910644531
Epoch 2880, val loss: 1.9073522090911865
Epoch 2890, training loss: 620.4911499023438 = 0.019837133586406708 + 100.0 * 6.204712867736816
Epoch 2890, val loss: 1.9108120203018188
Epoch 2900, training loss: 620.9323120117188 = 0.01962488889694214 + 100.0 * 6.2091264724731445
Epoch 2900, val loss: 1.9142935276031494
Epoch 2910, training loss: 620.5794677734375 = 0.019405558705329895 + 100.0 * 6.205600738525391
Epoch 2910, val loss: 1.9166666269302368
Epoch 2920, training loss: 620.6119384765625 = 0.0191915575414896 + 100.0 * 6.205927848815918
Epoch 2920, val loss: 1.9199786186218262
Epoch 2930, training loss: 620.3741455078125 = 0.018977845087647438 + 100.0 * 6.203551769256592
Epoch 2930, val loss: 1.923863410949707
Epoch 2940, training loss: 620.3494873046875 = 0.01878339983522892 + 100.0 * 6.203306674957275
Epoch 2940, val loss: 1.9270833730697632
Epoch 2950, training loss: 620.404541015625 = 0.018592454493045807 + 100.0 * 6.203859329223633
Epoch 2950, val loss: 1.9305051565170288
Epoch 2960, training loss: 620.490234375 = 0.01840076968073845 + 100.0 * 6.204718589782715
Epoch 2960, val loss: 1.9331400394439697
Epoch 2970, training loss: 620.5386962890625 = 0.018204374238848686 + 100.0 * 6.205204963684082
Epoch 2970, val loss: 1.9362921714782715
Epoch 2980, training loss: 620.4531860351562 = 0.01800595410168171 + 100.0 * 6.204351902008057
Epoch 2980, val loss: 1.939561128616333
Epoch 2990, training loss: 620.4530029296875 = 0.01781878061592579 + 100.0 * 6.204351902008057
Epoch 2990, val loss: 1.942448377609253
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.625925925925926
0.8075909330521878
The final CL Acc:0.62840, 0.00349, The final GNN Acc:0.80636, 0.00108
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13238])
remove edge: torch.Size([2, 7944])
updated graph: torch.Size([2, 10626])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6301879882812 = 1.9492501020431519 + 100.0 * 8.596809387207031
Epoch 0, val loss: 1.9434391260147095
Epoch 10, training loss: 861.5169677734375 = 1.9395219087600708 + 100.0 * 8.59577465057373
Epoch 10, val loss: 1.9337899684906006
Epoch 20, training loss: 860.7936401367188 = 1.9270583391189575 + 100.0 * 8.588665962219238
Epoch 20, val loss: 1.920976161956787
Epoch 30, training loss: 856.1583862304688 = 1.9102075099945068 + 100.0 * 8.542481422424316
Epoch 30, val loss: 1.9034955501556396
Epoch 40, training loss: 828.5245361328125 = 1.8897658586502075 + 100.0 * 8.266347885131836
Epoch 40, val loss: 1.883409857749939
Epoch 50, training loss: 770.8330688476562 = 1.8651375770568848 + 100.0 * 7.6896796226501465
Epoch 50, val loss: 1.8598582744598389
Epoch 60, training loss: 739.7860107421875 = 1.8485075235366821 + 100.0 * 7.379374980926514
Epoch 60, val loss: 1.8451988697052002
Epoch 70, training loss: 713.9564819335938 = 1.8391705751419067 + 100.0 * 7.121172904968262
Epoch 70, val loss: 1.8362146615982056
Epoch 80, training loss: 699.5266723632812 = 1.8306351900100708 + 100.0 * 6.9769606590271
Epoch 80, val loss: 1.8273746967315674
Epoch 90, training loss: 688.3314819335938 = 1.821234107017517 + 100.0 * 6.865102767944336
Epoch 90, val loss: 1.817849040031433
Epoch 100, training loss: 681.7991333007812 = 1.8117711544036865 + 100.0 * 6.7998738288879395
Epoch 100, val loss: 1.8087660074234009
Epoch 110, training loss: 676.4682006835938 = 1.8035311698913574 + 100.0 * 6.746646881103516
Epoch 110, val loss: 1.8008623123168945
Epoch 120, training loss: 670.8131103515625 = 1.7967034578323364 + 100.0 * 6.690164089202881
Epoch 120, val loss: 1.7940984964370728
Epoch 130, training loss: 665.2160034179688 = 1.7906235456466675 + 100.0 * 6.63425350189209
Epoch 130, val loss: 1.7879897356033325
Epoch 140, training loss: 661.022705078125 = 1.7844618558883667 + 100.0 * 6.592382431030273
Epoch 140, val loss: 1.7817504405975342
Epoch 150, training loss: 658.2911987304688 = 1.7776777744293213 + 100.0 * 6.565135478973389
Epoch 150, val loss: 1.774906873703003
Epoch 160, training loss: 655.8410034179688 = 1.7703851461410522 + 100.0 * 6.540706157684326
Epoch 160, val loss: 1.76786470413208
Epoch 170, training loss: 653.8560180664062 = 1.7632390260696411 + 100.0 * 6.520927429199219
Epoch 170, val loss: 1.7609198093414307
Epoch 180, training loss: 652.2124633789062 = 1.7560739517211914 + 100.0 * 6.504563808441162
Epoch 180, val loss: 1.7539970874786377
Epoch 190, training loss: 650.8844604492188 = 1.7484198808670044 + 100.0 * 6.491360664367676
Epoch 190, val loss: 1.7467150688171387
Epoch 200, training loss: 649.3060302734375 = 1.7402136325836182 + 100.0 * 6.475657939910889
Epoch 200, val loss: 1.7390893697738647
Epoch 210, training loss: 647.9058227539062 = 1.7315627336502075 + 100.0 * 6.461742401123047
Epoch 210, val loss: 1.7311660051345825
Epoch 220, training loss: 646.8955078125 = 1.7225427627563477 + 100.0 * 6.451729774475098
Epoch 220, val loss: 1.7229907512664795
Epoch 230, training loss: 645.5624389648438 = 1.712662935256958 + 100.0 * 6.438498020172119
Epoch 230, val loss: 1.7141095399856567
Epoch 240, training loss: 644.1923828125 = 1.702141523361206 + 100.0 * 6.424902439117432
Epoch 240, val loss: 1.704784631729126
Epoch 250, training loss: 643.3446655273438 = 1.6909184455871582 + 100.0 * 6.416537761688232
Epoch 250, val loss: 1.6947896480560303
Epoch 260, training loss: 642.1697998046875 = 1.6786887645721436 + 100.0 * 6.404911041259766
Epoch 260, val loss: 1.683972716331482
Epoch 270, training loss: 641.326904296875 = 1.6656242609024048 + 100.0 * 6.396612644195557
Epoch 270, val loss: 1.6724917888641357
Epoch 280, training loss: 640.8030395507812 = 1.6515583992004395 + 100.0 * 6.391514778137207
Epoch 280, val loss: 1.6601181030273438
Epoch 290, training loss: 639.9710693359375 = 1.6365970373153687 + 100.0 * 6.383344650268555
Epoch 290, val loss: 1.6469569206237793
Epoch 300, training loss: 639.2840576171875 = 1.6208633184432983 + 100.0 * 6.376632213592529
Epoch 300, val loss: 1.6332545280456543
Epoch 310, training loss: 639.0650634765625 = 1.6042801141738892 + 100.0 * 6.374607563018799
Epoch 310, val loss: 1.6188567876815796
Epoch 320, training loss: 638.24365234375 = 1.586936354637146 + 100.0 * 6.366567134857178
Epoch 320, val loss: 1.6037876605987549
Epoch 330, training loss: 637.748779296875 = 1.568965196609497 + 100.0 * 6.361798286437988
Epoch 330, val loss: 1.588295340538025
Epoch 340, training loss: 637.1844482421875 = 1.55032217502594 + 100.0 * 6.356341361999512
Epoch 340, val loss: 1.5723645687103271
Epoch 350, training loss: 637.2205810546875 = 1.5311591625213623 + 100.0 * 6.356894016265869
Epoch 350, val loss: 1.556030035018921
Epoch 360, training loss: 636.3351440429688 = 1.511436104774475 + 100.0 * 6.34823751449585
Epoch 360, val loss: 1.5392801761627197
Epoch 370, training loss: 635.8834838867188 = 1.4914382696151733 + 100.0 * 6.3439202308654785
Epoch 370, val loss: 1.5224312543869019
Epoch 380, training loss: 635.4972534179688 = 1.4711908102035522 + 100.0 * 6.3402605056762695
Epoch 380, val loss: 1.5054882764816284
Epoch 390, training loss: 635.346923828125 = 1.4505270719528198 + 100.0 * 6.338964462280273
Epoch 390, val loss: 1.4884403944015503
Epoch 400, training loss: 634.8545532226562 = 1.4296207427978516 + 100.0 * 6.334249496459961
Epoch 400, val loss: 1.4712361097335815
Epoch 410, training loss: 634.50146484375 = 1.4086954593658447 + 100.0 * 6.330927848815918
Epoch 410, val loss: 1.4540787935256958
Epoch 420, training loss: 634.1123046875 = 1.3877110481262207 + 100.0 * 6.327246189117432
Epoch 420, val loss: 1.4371166229248047
Epoch 430, training loss: 634.294189453125 = 1.366579532623291 + 100.0 * 6.329276084899902
Epoch 430, val loss: 1.4202218055725098
Epoch 440, training loss: 633.7189331054688 = 1.3451753854751587 + 100.0 * 6.323737621307373
Epoch 440, val loss: 1.40316641330719
Epoch 450, training loss: 633.286376953125 = 1.3238998651504517 + 100.0 * 6.319624423980713
Epoch 450, val loss: 1.386373519897461
Epoch 460, training loss: 633.0106811523438 = 1.3026365041732788 + 100.0 * 6.317080974578857
Epoch 460, val loss: 1.369755744934082
Epoch 470, training loss: 632.8252563476562 = 1.2811095714569092 + 100.0 * 6.315441131591797
Epoch 470, val loss: 1.3530824184417725
Epoch 480, training loss: 632.598388671875 = 1.2596070766448975 + 100.0 * 6.313387870788574
Epoch 480, val loss: 1.3365182876586914
Epoch 490, training loss: 632.1943359375 = 1.2380635738372803 + 100.0 * 6.309563159942627
Epoch 490, val loss: 1.3199783563613892
Epoch 500, training loss: 632.009033203125 = 1.216628909111023 + 100.0 * 6.307924270629883
Epoch 500, val loss: 1.303654432296753
Epoch 510, training loss: 631.6796264648438 = 1.1951614618301392 + 100.0 * 6.304844379425049
Epoch 510, val loss: 1.2875473499298096
Epoch 520, training loss: 631.5564575195312 = 1.1737338304519653 + 100.0 * 6.30382776260376
Epoch 520, val loss: 1.271773338317871
Epoch 530, training loss: 631.480224609375 = 1.1522873640060425 + 100.0 * 6.303279399871826
Epoch 530, val loss: 1.2558280229568481
Epoch 540, training loss: 630.9850463867188 = 1.1307752132415771 + 100.0 * 6.298542499542236
Epoch 540, val loss: 1.2398813962936401
Epoch 550, training loss: 630.74169921875 = 1.109649896621704 + 100.0 * 6.29632043838501
Epoch 550, val loss: 1.2247473001480103
Epoch 560, training loss: 630.5532836914062 = 1.088765025138855 + 100.0 * 6.294645309448242
Epoch 560, val loss: 1.2097489833831787
Epoch 570, training loss: 630.6010131835938 = 1.067965030670166 + 100.0 * 6.295330047607422
Epoch 570, val loss: 1.1949079036712646
Epoch 580, training loss: 630.4235229492188 = 1.0472012758255005 + 100.0 * 6.293763637542725
Epoch 580, val loss: 1.1803604364395142
Epoch 590, training loss: 629.9598999023438 = 1.0267337560653687 + 100.0 * 6.289331436157227
Epoch 590, val loss: 1.1660597324371338
Epoch 600, training loss: 629.7716674804688 = 1.006670594215393 + 100.0 * 6.287650108337402
Epoch 600, val loss: 1.152190089225769
Epoch 610, training loss: 629.6589965820312 = 0.9867595434188843 + 100.0 * 6.286722660064697
Epoch 610, val loss: 1.1387308835983276
Epoch 620, training loss: 629.3692626953125 = 0.967179000377655 + 100.0 * 6.284020900726318
Epoch 620, val loss: 1.1255319118499756
Epoch 630, training loss: 629.3919067382812 = 0.9480087757110596 + 100.0 * 6.2844390869140625
Epoch 630, val loss: 1.1130268573760986
Epoch 640, training loss: 629.168701171875 = 0.929132878780365 + 100.0 * 6.282395362854004
Epoch 640, val loss: 1.1007438898086548
Epoch 650, training loss: 628.9879760742188 = 0.9106119275093079 + 100.0 * 6.280773639678955
Epoch 650, val loss: 1.0888813734054565
Epoch 660, training loss: 628.8511962890625 = 0.8925184011459351 + 100.0 * 6.2795867919921875
Epoch 660, val loss: 1.07757568359375
Epoch 670, training loss: 628.5595703125 = 0.8746830821037292 + 100.0 * 6.276848793029785
Epoch 670, val loss: 1.0663758516311646
Epoch 680, training loss: 628.4202270507812 = 0.8574544787406921 + 100.0 * 6.275627136230469
Epoch 680, val loss: 1.0562078952789307
Epoch 690, training loss: 628.2611083984375 = 0.8405362367630005 + 100.0 * 6.274206161499023
Epoch 690, val loss: 1.0461459159851074
Epoch 700, training loss: 628.4111328125 = 0.8238841891288757 + 100.0 * 6.275872230529785
Epoch 700, val loss: 1.03648841381073
Epoch 710, training loss: 627.917724609375 = 0.8076584339141846 + 100.0 * 6.2711005210876465
Epoch 710, val loss: 1.0274618864059448
Epoch 720, training loss: 627.889404296875 = 0.7918726205825806 + 100.0 * 6.270975589752197
Epoch 720, val loss: 1.0190181732177734
Epoch 730, training loss: 627.987548828125 = 0.7762746810913086 + 100.0 * 6.27211332321167
Epoch 730, val loss: 1.010438084602356
Epoch 740, training loss: 627.6568603515625 = 0.7609987854957581 + 100.0 * 6.268958568572998
Epoch 740, val loss: 1.0026332139968872
Epoch 750, training loss: 627.4104614257812 = 0.7462548613548279 + 100.0 * 6.266641616821289
Epoch 750, val loss: 0.9952427744865417
Epoch 760, training loss: 627.2568969726562 = 0.732042670249939 + 100.0 * 6.2652482986450195
Epoch 760, val loss: 0.9885324239730835
Epoch 770, training loss: 627.1484375 = 0.7181468605995178 + 100.0 * 6.264302730560303
Epoch 770, val loss: 0.9821584224700928
Epoch 780, training loss: 627.50244140625 = 0.7045403718948364 + 100.0 * 6.267979145050049
Epoch 780, val loss: 0.9761123061180115
Epoch 790, training loss: 627.2041015625 = 0.6911036968231201 + 100.0 * 6.265130043029785
Epoch 790, val loss: 0.9702250361442566
Epoch 800, training loss: 626.8782958984375 = 0.677979052066803 + 100.0 * 6.262002944946289
Epoch 800, val loss: 0.9649480581283569
Epoch 810, training loss: 627.8121948242188 = 0.6651667952537537 + 100.0 * 6.271470546722412
Epoch 810, val loss: 0.9598328471183777
Epoch 820, training loss: 626.609619140625 = 0.6526100039482117 + 100.0 * 6.259570598602295
Epoch 820, val loss: 0.9551559090614319
Epoch 830, training loss: 626.5150756835938 = 0.6403782367706299 + 100.0 * 6.258747100830078
Epoch 830, val loss: 0.9507156014442444
Epoch 840, training loss: 626.4053344726562 = 0.6285096406936646 + 100.0 * 6.257767677307129
Epoch 840, val loss: 0.946780800819397
Epoch 850, training loss: 626.3206176757812 = 0.616888701915741 + 100.0 * 6.257037162780762
Epoch 850, val loss: 0.9431204795837402
Epoch 860, training loss: 626.5529174804688 = 0.6053003668785095 + 100.0 * 6.259476184844971
Epoch 860, val loss: 0.9396507143974304
Epoch 870, training loss: 626.3799438476562 = 0.5939605236053467 + 100.0 * 6.25786018371582
Epoch 870, val loss: 0.9362263083457947
Epoch 880, training loss: 626.0404052734375 = 0.5829622745513916 + 100.0 * 6.254574775695801
Epoch 880, val loss: 0.9333445429801941
Epoch 890, training loss: 625.8929443359375 = 0.5722723603248596 + 100.0 * 6.253206729888916
Epoch 890, val loss: 0.930856466293335
Epoch 900, training loss: 625.83642578125 = 0.5617663264274597 + 100.0 * 6.25274658203125
Epoch 900, val loss: 0.928523600101471
Epoch 910, training loss: 626.6493530273438 = 0.5512798428535461 + 100.0 * 6.260980606079102
Epoch 910, val loss: 0.9260172247886658
Epoch 920, training loss: 625.683349609375 = 0.5409679412841797 + 100.0 * 6.2514238357543945
Epoch 920, val loss: 0.9240078330039978
Epoch 930, training loss: 625.641357421875 = 0.5309051871299744 + 100.0 * 6.251104831695557
Epoch 930, val loss: 0.9224693179130554
Epoch 940, training loss: 625.6209106445312 = 0.5210925936698914 + 100.0 * 6.250998020172119
Epoch 940, val loss: 0.9210101366043091
Epoch 950, training loss: 625.5077514648438 = 0.5113478302955627 + 100.0 * 6.249963760375977
Epoch 950, val loss: 0.9193171858787537
Epoch 960, training loss: 625.7491455078125 = 0.5017519593238831 + 100.0 * 6.252473831176758
Epoch 960, val loss: 0.9178731441497803
Epoch 970, training loss: 625.388427734375 = 0.4923098087310791 + 100.0 * 6.248961448669434
Epoch 970, val loss: 0.9168497323989868
Epoch 980, training loss: 625.290283203125 = 0.4830023944377899 + 100.0 * 6.248072624206543
Epoch 980, val loss: 0.9158625602722168
Epoch 990, training loss: 625.1541137695312 = 0.47391706705093384 + 100.0 * 6.246801853179932
Epoch 990, val loss: 0.9150183200836182
Epoch 1000, training loss: 625.072509765625 = 0.4650029242038727 + 100.0 * 6.246074676513672
Epoch 1000, val loss: 0.9144724607467651
Epoch 1010, training loss: 625.3197631835938 = 0.4562428891658783 + 100.0 * 6.248635292053223
Epoch 1010, val loss: 0.9138426780700684
Epoch 1020, training loss: 625.3314819335938 = 0.44736865162849426 + 100.0 * 6.248840808868408
Epoch 1020, val loss: 0.9129630327224731
Epoch 1030, training loss: 624.8512573242188 = 0.4386994242668152 + 100.0 * 6.2441253662109375
Epoch 1030, val loss: 0.9126011729240417
Epoch 1040, training loss: 624.8042602539062 = 0.43014881014823914 + 100.0 * 6.243741035461426
Epoch 1040, val loss: 0.9122572541236877
Epoch 1050, training loss: 624.7063598632812 = 0.42189541459083557 + 100.0 * 6.242845058441162
Epoch 1050, val loss: 0.9122771620750427
Epoch 1060, training loss: 624.6589965820312 = 0.41374269127845764 + 100.0 * 6.242452621459961
Epoch 1060, val loss: 0.9122781753540039
Epoch 1070, training loss: 625.3294677734375 = 0.40561676025390625 + 100.0 * 6.249238014221191
Epoch 1070, val loss: 0.9121518731117249
Epoch 1080, training loss: 624.5814819335938 = 0.3975040912628174 + 100.0 * 6.24183988571167
Epoch 1080, val loss: 0.9121531844139099
Epoch 1090, training loss: 624.5518188476562 = 0.389614999294281 + 100.0 * 6.241622447967529
Epoch 1090, val loss: 0.9125464558601379
Epoch 1100, training loss: 624.3551635742188 = 0.38193392753601074 + 100.0 * 6.239732265472412
Epoch 1100, val loss: 0.913007915019989
Epoch 1110, training loss: 624.3351440429688 = 0.3744174838066101 + 100.0 * 6.239607334136963
Epoch 1110, val loss: 0.9135708212852478
Epoch 1120, training loss: 625.1344604492188 = 0.36700674891471863 + 100.0 * 6.247674465179443
Epoch 1120, val loss: 0.9139787554740906
Epoch 1130, training loss: 624.5512084960938 = 0.3594685196876526 + 100.0 * 6.241917133331299
Epoch 1130, val loss: 0.9145920872688293
Epoch 1140, training loss: 624.1443481445312 = 0.3521824777126312 + 100.0 * 6.237921714782715
Epoch 1140, val loss: 0.9154189229011536
Epoch 1150, training loss: 624.10791015625 = 0.345099538564682 + 100.0 * 6.23762845993042
Epoch 1150, val loss: 0.9162496328353882
Epoch 1160, training loss: 624.1949462890625 = 0.33815065026283264 + 100.0 * 6.238568305969238
Epoch 1160, val loss: 0.9172718524932861
Epoch 1170, training loss: 624.1281127929688 = 0.3312614858150482 + 100.0 * 6.237968921661377
Epoch 1170, val loss: 0.918302595615387
Epoch 1180, training loss: 624.3094482421875 = 0.3244185447692871 + 100.0 * 6.2398505210876465
Epoch 1180, val loss: 0.9189987778663635
Epoch 1190, training loss: 623.904541015625 = 0.31774115562438965 + 100.0 * 6.235867977142334
Epoch 1190, val loss: 0.9204238057136536
Epoch 1200, training loss: 623.8409423828125 = 0.3111821711063385 + 100.0 * 6.235298156738281
Epoch 1200, val loss: 0.9215309023857117
Epoch 1210, training loss: 623.7964477539062 = 0.304863840341568 + 100.0 * 6.234915733337402
Epoch 1210, val loss: 0.9230054616928101
Epoch 1220, training loss: 624.5587768554688 = 0.298648864030838 + 100.0 * 6.24260139465332
Epoch 1220, val loss: 0.9245240092277527
Epoch 1230, training loss: 623.8251342773438 = 0.29237139225006104 + 100.0 * 6.23532772064209
Epoch 1230, val loss: 0.9255194067955017
Epoch 1240, training loss: 623.5702514648438 = 0.2863520681858063 + 100.0 * 6.2328386306762695
Epoch 1240, val loss: 0.927219033241272
Epoch 1250, training loss: 623.5294799804688 = 0.28051555156707764 + 100.0 * 6.232489585876465
Epoch 1250, val loss: 0.9289522767066956
Epoch 1260, training loss: 623.9605712890625 = 0.27484023571014404 + 100.0 * 6.2368574142456055
Epoch 1260, val loss: 0.9307005405426025
Epoch 1270, training loss: 624.1132202148438 = 0.2690942883491516 + 100.0 * 6.238441467285156
Epoch 1270, val loss: 0.9322448968887329
Epoch 1280, training loss: 623.5028686523438 = 0.2634139955043793 + 100.0 * 6.232394218444824
Epoch 1280, val loss: 0.9338842034339905
Epoch 1290, training loss: 623.4119873046875 = 0.25798213481903076 + 100.0 * 6.231540203094482
Epoch 1290, val loss: 0.9357740879058838
Epoch 1300, training loss: 623.2903442382812 = 0.25275754928588867 + 100.0 * 6.230376243591309
Epoch 1300, val loss: 0.9379598498344421
Epoch 1310, training loss: 623.2597045898438 = 0.24764569103717804 + 100.0 * 6.23012113571167
Epoch 1310, val loss: 0.94011390209198
Epoch 1320, training loss: 624.268310546875 = 0.24259436130523682 + 100.0 * 6.2402567863464355
Epoch 1320, val loss: 0.9420843124389648
Epoch 1330, training loss: 623.3599853515625 = 0.2375614047050476 + 100.0 * 6.231224536895752
Epoch 1330, val loss: 0.9442563652992249
Epoch 1340, training loss: 623.188232421875 = 0.2326805144548416 + 100.0 * 6.229555606842041
Epoch 1340, val loss: 0.9466047286987305
Epoch 1350, training loss: 623.2496948242188 = 0.22796641290187836 + 100.0 * 6.230216979980469
Epoch 1350, val loss: 0.9488689303398132
Epoch 1360, training loss: 623.25537109375 = 0.2233629822731018 + 100.0 * 6.230319976806641
Epoch 1360, val loss: 0.9512192606925964
Epoch 1370, training loss: 623.0772094726562 = 0.21887193620204926 + 100.0 * 6.228583335876465
Epoch 1370, val loss: 0.9537646174430847
Epoch 1380, training loss: 623.0188598632812 = 0.21446798741817474 + 100.0 * 6.228044033050537
Epoch 1380, val loss: 0.9561872482299805
Epoch 1390, training loss: 623.434326171875 = 0.21014320850372314 + 100.0 * 6.232241630554199
Epoch 1390, val loss: 0.9587340354919434
Epoch 1400, training loss: 623.049560546875 = 0.2059386968612671 + 100.0 * 6.22843599319458
Epoch 1400, val loss: 0.9615811109542847
Epoch 1410, training loss: 623.0438232421875 = 0.2017824351787567 + 100.0 * 6.228420257568359
Epoch 1410, val loss: 0.9640344977378845
Epoch 1420, training loss: 622.912109375 = 0.19778120517730713 + 100.0 * 6.22714376449585
Epoch 1420, val loss: 0.966977596282959
Epoch 1430, training loss: 622.8388671875 = 0.19383171200752258 + 100.0 * 6.226449966430664
Epoch 1430, val loss: 0.969566285610199
Epoch 1440, training loss: 622.8424682617188 = 0.1900155395269394 + 100.0 * 6.226524353027344
Epoch 1440, val loss: 0.9724825024604797
Epoch 1450, training loss: 622.829345703125 = 0.1862623542547226 + 100.0 * 6.226430416107178
Epoch 1450, val loss: 0.9753140211105347
Epoch 1460, training loss: 622.9895629882812 = 0.18257200717926025 + 100.0 * 6.22806978225708
Epoch 1460, val loss: 0.9780863523483276
Epoch 1470, training loss: 622.7201538085938 = 0.17895923554897308 + 100.0 * 6.225411891937256
Epoch 1470, val loss: 0.9811473488807678
Epoch 1480, training loss: 622.6917724609375 = 0.17546868324279785 + 100.0 * 6.225162982940674
Epoch 1480, val loss: 0.984117865562439
Epoch 1490, training loss: 622.8463745117188 = 0.17204388976097107 + 100.0 * 6.226743221282959
Epoch 1490, val loss: 0.9869476556777954
Epoch 1500, training loss: 622.6235961914062 = 0.16867710649967194 + 100.0 * 6.224548816680908
Epoch 1500, val loss: 0.989740788936615
Epoch 1510, training loss: 622.6212158203125 = 0.16540612280368805 + 100.0 * 6.224557876586914
Epoch 1510, val loss: 0.992871880531311
Epoch 1520, training loss: 622.6192626953125 = 0.16220395267009735 + 100.0 * 6.224570274353027
Epoch 1520, val loss: 0.9957746863365173
Epoch 1530, training loss: 622.6417846679688 = 0.159048929810524 + 100.0 * 6.224827289581299
Epoch 1530, val loss: 0.9987393021583557
Epoch 1540, training loss: 622.408447265625 = 0.156019389629364 + 100.0 * 6.222524166107178
Epoch 1540, val loss: 1.002029299736023
Epoch 1550, training loss: 622.9461669921875 = 0.1530429869890213 + 100.0 * 6.227931499481201
Epoch 1550, val loss: 1.0050327777862549
Epoch 1560, training loss: 622.4597778320312 = 0.15002956986427307 + 100.0 * 6.223097324371338
Epoch 1560, val loss: 1.0079143047332764
Epoch 1570, training loss: 622.3032836914062 = 0.1471676528453827 + 100.0 * 6.221561431884766
Epoch 1570, val loss: 1.0111905336380005
Epoch 1580, training loss: 622.3109741210938 = 0.14439377188682556 + 100.0 * 6.221665859222412
Epoch 1580, val loss: 1.0143907070159912
Epoch 1590, training loss: 622.4954833984375 = 0.14166082441806793 + 100.0 * 6.223538398742676
Epoch 1590, val loss: 1.017441987991333
Epoch 1600, training loss: 622.2998046875 = 0.13893507421016693 + 100.0 * 6.221609115600586
Epoch 1600, val loss: 1.0206106901168823
Epoch 1610, training loss: 622.1751098632812 = 0.1363103836774826 + 100.0 * 6.220388412475586
Epoch 1610, val loss: 1.0237261056900024
Epoch 1620, training loss: 622.715087890625 = 0.13373900949954987 + 100.0 * 6.225813865661621
Epoch 1620, val loss: 1.0266786813735962
Epoch 1630, training loss: 622.2506103515625 = 0.13116170465946198 + 100.0 * 6.221194267272949
Epoch 1630, val loss: 1.0303386449813843
Epoch 1640, training loss: 622.0836181640625 = 0.12869678437709808 + 100.0 * 6.219549179077148
Epoch 1640, val loss: 1.0332661867141724
Epoch 1650, training loss: 622.0859375 = 0.126310333609581 + 100.0 * 6.2195963859558105
Epoch 1650, val loss: 1.0368022918701172
Epoch 1660, training loss: 622.3469848632812 = 0.1239684522151947 + 100.0 * 6.222230434417725
Epoch 1660, val loss: 1.0397741794586182
Epoch 1670, training loss: 622.0430908203125 = 0.1216193437576294 + 100.0 * 6.21921443939209
Epoch 1670, val loss: 1.0430480241775513
Epoch 1680, training loss: 622.0263671875 = 0.11938036233186722 + 100.0 * 6.219069957733154
Epoch 1680, val loss: 1.0464191436767578
Epoch 1690, training loss: 622.135986328125 = 0.1171661838889122 + 100.0 * 6.220188140869141
Epoch 1690, val loss: 1.0494921207427979
Epoch 1700, training loss: 621.8975219726562 = 0.11496296525001526 + 100.0 * 6.217825412750244
Epoch 1700, val loss: 1.0526131391525269
Epoch 1710, training loss: 621.8191528320312 = 0.11285687983036041 + 100.0 * 6.217062950134277
Epoch 1710, val loss: 1.0561498403549194
Epoch 1720, training loss: 622.2249755859375 = 0.11081048846244812 + 100.0 * 6.221141338348389
Epoch 1720, val loss: 1.059237003326416
Epoch 1730, training loss: 622.0918579101562 = 0.10873918980360031 + 100.0 * 6.2198309898376465
Epoch 1730, val loss: 1.0621685981750488
Epoch 1740, training loss: 622.0014038085938 = 0.10669981688261032 + 100.0 * 6.218947410583496
Epoch 1740, val loss: 1.0656756162643433
Epoch 1750, training loss: 621.7477416992188 = 0.1047385185956955 + 100.0 * 6.216430187225342
Epoch 1750, val loss: 1.0687681436538696
Epoch 1760, training loss: 621.64794921875 = 0.10284791141748428 + 100.0 * 6.215450763702393
Epoch 1760, val loss: 1.072177767753601
Epoch 1770, training loss: 621.7644653320312 = 0.1010151207447052 + 100.0 * 6.216634273529053
Epoch 1770, val loss: 1.0755400657653809
Epoch 1780, training loss: 622.0369262695312 = 0.09917612373828888 + 100.0 * 6.219377517700195
Epoch 1780, val loss: 1.078252911567688
Epoch 1790, training loss: 621.8041381835938 = 0.0973232239484787 + 100.0 * 6.217067718505859
Epoch 1790, val loss: 1.0813668966293335
Epoch 1800, training loss: 621.6670532226562 = 0.09555314481258392 + 100.0 * 6.215714931488037
Epoch 1800, val loss: 1.0847289562225342
Epoch 1810, training loss: 621.510498046875 = 0.09383101016283035 + 100.0 * 6.21416711807251
Epoch 1810, val loss: 1.0880192518234253
Epoch 1820, training loss: 621.4988403320312 = 0.09218490123748779 + 100.0 * 6.214066982269287
Epoch 1820, val loss: 1.091472864151001
Epoch 1830, training loss: 621.9169311523438 = 0.09057284891605377 + 100.0 * 6.218263626098633
Epoch 1830, val loss: 1.0948280096054077
Epoch 1840, training loss: 621.5433349609375 = 0.08891396224498749 + 100.0 * 6.21454381942749
Epoch 1840, val loss: 1.0977609157562256
Epoch 1850, training loss: 621.4876708984375 = 0.08731891959905624 + 100.0 * 6.214003562927246
Epoch 1850, val loss: 1.1012064218521118
Epoch 1860, training loss: 621.6785888671875 = 0.08578284829854965 + 100.0 * 6.215928554534912
Epoch 1860, val loss: 1.1044225692749023
Epoch 1870, training loss: 621.520263671875 = 0.08423809707164764 + 100.0 * 6.214360237121582
Epoch 1870, val loss: 1.1077163219451904
Epoch 1880, training loss: 622.0144653320312 = 0.08274431526660919 + 100.0 * 6.2193169593811035
Epoch 1880, val loss: 1.1113197803497314
Epoch 1890, training loss: 621.5147094726562 = 0.0812654197216034 + 100.0 * 6.214334964752197
Epoch 1890, val loss: 1.1139559745788574
Epoch 1900, training loss: 621.3225708007812 = 0.0798305869102478 + 100.0 * 6.212427616119385
Epoch 1900, val loss: 1.1176480054855347
Epoch 1910, training loss: 621.2579345703125 = 0.07846785336732864 + 100.0 * 6.211794376373291
Epoch 1910, val loss: 1.1210302114486694
Epoch 1920, training loss: 621.3017578125 = 0.07714147120714188 + 100.0 * 6.212245941162109
Epoch 1920, val loss: 1.1245635747909546
Epoch 1930, training loss: 621.7939453125 = 0.07581835985183716 + 100.0 * 6.217181205749512
Epoch 1930, val loss: 1.1277495622634888
Epoch 1940, training loss: 621.409423828125 = 0.0744665265083313 + 100.0 * 6.21334981918335
Epoch 1940, val loss: 1.1309198141098022
Epoch 1950, training loss: 621.3185424804688 = 0.07319732755422592 + 100.0 * 6.212453365325928
Epoch 1950, val loss: 1.1343705654144287
Epoch 1960, training loss: 621.3960571289062 = 0.07195249944925308 + 100.0 * 6.213241100311279
Epoch 1960, val loss: 1.1375350952148438
Epoch 1970, training loss: 621.1915893554688 = 0.07072179019451141 + 100.0 * 6.211208343505859
Epoch 1970, val loss: 1.1411826610565186
Epoch 1980, training loss: 621.6544189453125 = 0.06952397525310516 + 100.0 * 6.215848922729492
Epoch 1980, val loss: 1.1444604396820068
Epoch 1990, training loss: 621.6026611328125 = 0.06832730770111084 + 100.0 * 6.215343475341797
Epoch 1990, val loss: 1.1474521160125732
Epoch 2000, training loss: 621.1821899414062 = 0.06715255230665207 + 100.0 * 6.211150646209717
Epoch 2000, val loss: 1.1509829759597778
Epoch 2010, training loss: 621.08837890625 = 0.06603393703699112 + 100.0 * 6.210223197937012
Epoch 2010, val loss: 1.1541998386383057
Epoch 2020, training loss: 621.0593872070312 = 0.06494715809822083 + 100.0 * 6.209944725036621
Epoch 2020, val loss: 1.1574957370758057
Epoch 2030, training loss: 621.3651123046875 = 0.06388743221759796 + 100.0 * 6.213012218475342
Epoch 2030, val loss: 1.1605749130249023
Epoch 2040, training loss: 621.0262451171875 = 0.06280606240034103 + 100.0 * 6.209634304046631
Epoch 2040, val loss: 1.1641755104064941
Epoch 2050, training loss: 621.315185546875 = 0.06177016720175743 + 100.0 * 6.212534427642822
Epoch 2050, val loss: 1.1674567461013794
Epoch 2060, training loss: 621.1456298828125 = 0.06072470545768738 + 100.0 * 6.210849285125732
Epoch 2060, val loss: 1.1704578399658203
Epoch 2070, training loss: 620.9850463867188 = 0.059717364609241486 + 100.0 * 6.209253311157227
Epoch 2070, val loss: 1.1740487813949585
Epoch 2080, training loss: 621.1073608398438 = 0.05874849483370781 + 100.0 * 6.210485935211182
Epoch 2080, val loss: 1.1771228313446045
Epoch 2090, training loss: 621.2302856445312 = 0.057795871049165726 + 100.0 * 6.211724758148193
Epoch 2090, val loss: 1.1805763244628906
Epoch 2100, training loss: 620.949462890625 = 0.05685574561357498 + 100.0 * 6.208925724029541
Epoch 2100, val loss: 1.183821678161621
Epoch 2110, training loss: 620.8972778320312 = 0.05594158172607422 + 100.0 * 6.208413124084473
Epoch 2110, val loss: 1.1873033046722412
Epoch 2120, training loss: 621.1068115234375 = 0.05505754053592682 + 100.0 * 6.210517406463623
Epoch 2120, val loss: 1.190442442893982
Epoch 2130, training loss: 620.8674926757812 = 0.054160960018634796 + 100.0 * 6.208133697509766
Epoch 2130, val loss: 1.1936677694320679
Epoch 2140, training loss: 620.915771484375 = 0.053282346576452255 + 100.0 * 6.208624839782715
Epoch 2140, val loss: 1.196861743927002
Epoch 2150, training loss: 620.9165649414062 = 0.05243522673845291 + 100.0 * 6.208641529083252
Epoch 2150, val loss: 1.2003002166748047
Epoch 2160, training loss: 620.9996337890625 = 0.051613058894872665 + 100.0 * 6.209480285644531
Epoch 2160, val loss: 1.203550100326538
Epoch 2170, training loss: 620.7418212890625 = 0.050798870623111725 + 100.0 * 6.206910133361816
Epoch 2170, val loss: 1.20686936378479
Epoch 2180, training loss: 621.1378173828125 = 0.05002458021044731 + 100.0 * 6.210877418518066
Epoch 2180, val loss: 1.2099794149398804
Epoch 2190, training loss: 620.7730712890625 = 0.0492105558514595 + 100.0 * 6.207238674163818
Epoch 2190, val loss: 1.213165044784546
Epoch 2200, training loss: 620.7001342773438 = 0.04843660444021225 + 100.0 * 6.206516742706299
Epoch 2200, val loss: 1.2165666818618774
Epoch 2210, training loss: 620.665771484375 = 0.04770669341087341 + 100.0 * 6.206180572509766
Epoch 2210, val loss: 1.219950556755066
Epoch 2220, training loss: 620.6881103515625 = 0.04699193313717842 + 100.0 * 6.206410884857178
Epoch 2220, val loss: 1.2234318256378174
Epoch 2230, training loss: 621.3809204101562 = 0.04628213495016098 + 100.0 * 6.213346481323242
Epoch 2230, val loss: 1.2264049053192139
Epoch 2240, training loss: 620.91650390625 = 0.045568984001874924 + 100.0 * 6.208709239959717
Epoch 2240, val loss: 1.2297860383987427
Epoch 2250, training loss: 620.7449340820312 = 0.04485803097486496 + 100.0 * 6.207000732421875
Epoch 2250, val loss: 1.232951045036316
Epoch 2260, training loss: 620.6194458007812 = 0.04418465122580528 + 100.0 * 6.205752849578857
Epoch 2260, val loss: 1.2362432479858398
Epoch 2270, training loss: 620.7667846679688 = 0.04353661462664604 + 100.0 * 6.207232475280762
Epoch 2270, val loss: 1.239575743675232
Epoch 2280, training loss: 620.71875 = 0.042888958007097244 + 100.0 * 6.206758499145508
Epoch 2280, val loss: 1.2429083585739136
Epoch 2290, training loss: 620.5665283203125 = 0.042251817882061005 + 100.0 * 6.20524263381958
Epoch 2290, val loss: 1.2461259365081787
Epoch 2300, training loss: 620.54931640625 = 0.04163522273302078 + 100.0 * 6.205076694488525
Epoch 2300, val loss: 1.2492680549621582
Epoch 2310, training loss: 620.705322265625 = 0.041036490350961685 + 100.0 * 6.206643104553223
Epoch 2310, val loss: 1.2525272369384766
Epoch 2320, training loss: 620.5242919921875 = 0.04042591527104378 + 100.0 * 6.204838752746582
Epoch 2320, val loss: 1.2558537721633911
Epoch 2330, training loss: 620.5731811523438 = 0.03984842821955681 + 100.0 * 6.205333232879639
Epoch 2330, val loss: 1.2592629194259644
Epoch 2340, training loss: 620.900146484375 = 0.03927180916070938 + 100.0 * 6.208609104156494
Epoch 2340, val loss: 1.2624976634979248
Epoch 2350, training loss: 620.7973022460938 = 0.038691453635692596 + 100.0 * 6.207586288452148
Epoch 2350, val loss: 1.2651753425598145
Epoch 2360, training loss: 620.5099487304688 = 0.03811989724636078 + 100.0 * 6.204718112945557
Epoch 2360, val loss: 1.26845383644104
Epoch 2370, training loss: 620.4007568359375 = 0.037585943937301636 + 100.0 * 6.20363187789917
Epoch 2370, val loss: 1.271781086921692
Epoch 2380, training loss: 620.3799438476562 = 0.037066131830215454 + 100.0 * 6.203429222106934
Epoch 2380, val loss: 1.2750811576843262
Epoch 2390, training loss: 620.7489013671875 = 0.036564603447914124 + 100.0 * 6.207123279571533
Epoch 2390, val loss: 1.2784109115600586
Epoch 2400, training loss: 620.3992919921875 = 0.03603258356451988 + 100.0 * 6.203632831573486
Epoch 2400, val loss: 1.2814809083938599
Epoch 2410, training loss: 620.4551391601562 = 0.035525787621736526 + 100.0 * 6.204196453094482
Epoch 2410, val loss: 1.2844758033752441
Epoch 2420, training loss: 620.372314453125 = 0.03503415361046791 + 100.0 * 6.203372955322266
Epoch 2420, val loss: 1.287609338760376
Epoch 2430, training loss: 620.6397094726562 = 0.03455640748143196 + 100.0 * 6.206051826477051
Epoch 2430, val loss: 1.291002631187439
Epoch 2440, training loss: 620.4059448242188 = 0.03407388925552368 + 100.0 * 6.203719139099121
Epoch 2440, val loss: 1.2940088510513306
Epoch 2450, training loss: 620.2791748046875 = 0.03360036015510559 + 100.0 * 6.202455520629883
Epoch 2450, val loss: 1.296952486038208
Epoch 2460, training loss: 620.2974243164062 = 0.033156756311655045 + 100.0 * 6.202642917633057
Epoch 2460, val loss: 1.299950361251831
Epoch 2470, training loss: 620.3407592773438 = 0.032718442380428314 + 100.0 * 6.203080177307129
Epoch 2470, val loss: 1.3030941486358643
Epoch 2480, training loss: 620.419921875 = 0.0322832427918911 + 100.0 * 6.203876495361328
Epoch 2480, val loss: 1.3061405420303345
Epoch 2490, training loss: 620.6797485351562 = 0.03186153247952461 + 100.0 * 6.206478595733643
Epoch 2490, val loss: 1.3094916343688965
Epoch 2500, training loss: 620.5709838867188 = 0.031423427164554596 + 100.0 * 6.205395221710205
Epoch 2500, val loss: 1.3120437860488892
Epoch 2510, training loss: 620.3275146484375 = 0.030987832695245743 + 100.0 * 6.202965259552002
Epoch 2510, val loss: 1.315272331237793
Epoch 2520, training loss: 620.2026977539062 = 0.03057134337723255 + 100.0 * 6.20172119140625
Epoch 2520, val loss: 1.3183146715164185
Epoch 2530, training loss: 620.1300659179688 = 0.03018920123577118 + 100.0 * 6.200998783111572
Epoch 2530, val loss: 1.3215168714523315
Epoch 2540, training loss: 620.2144165039062 = 0.029813967645168304 + 100.0 * 6.201846599578857
Epoch 2540, val loss: 1.3247448205947876
Epoch 2550, training loss: 620.4988403320312 = 0.029437681660056114 + 100.0 * 6.2046942710876465
Epoch 2550, val loss: 1.3275717496871948
Epoch 2560, training loss: 620.4754028320312 = 0.029042573645710945 + 100.0 * 6.204463481903076
Epoch 2560, val loss: 1.3299479484558105
Epoch 2570, training loss: 620.2039794921875 = 0.028657907620072365 + 100.0 * 6.20175313949585
Epoch 2570, val loss: 1.3329086303710938
Epoch 2580, training loss: 620.1929321289062 = 0.028295965865254402 + 100.0 * 6.201646327972412
Epoch 2580, val loss: 1.335810661315918
Epoch 2590, training loss: 620.2282104492188 = 0.027944806963205338 + 100.0 * 6.20200252532959
Epoch 2590, val loss: 1.3387686014175415
Epoch 2600, training loss: 620.44091796875 = 0.027589676901698112 + 100.0 * 6.204133033752441
Epoch 2600, val loss: 1.341636300086975
Epoch 2610, training loss: 620.1152954101562 = 0.027220526710152626 + 100.0 * 6.200880527496338
Epoch 2610, val loss: 1.3444552421569824
Epoch 2620, training loss: 619.9922485351562 = 0.02688106708228588 + 100.0 * 6.1996541023254395
Epoch 2620, val loss: 1.3474247455596924
Epoch 2630, training loss: 620.0029296875 = 0.026560790836811066 + 100.0 * 6.199763774871826
Epoch 2630, val loss: 1.3503860235214233
Epoch 2640, training loss: 620.1817626953125 = 0.026245448738336563 + 100.0 * 6.201555252075195
Epoch 2640, val loss: 1.3533048629760742
Epoch 2650, training loss: 620.1905517578125 = 0.02591863088309765 + 100.0 * 6.201646327972412
Epoch 2650, val loss: 1.3561139106750488
Epoch 2660, training loss: 620.4979248046875 = 0.025598332285881042 + 100.0 * 6.204723358154297
Epoch 2660, val loss: 1.3588908910751343
Epoch 2670, training loss: 620.0492553710938 = 0.025267306715250015 + 100.0 * 6.200240135192871
Epoch 2670, val loss: 1.3615049123764038
Epoch 2680, training loss: 619.9772338867188 = 0.024951597675681114 + 100.0 * 6.199522972106934
Epoch 2680, val loss: 1.364601731300354
Epoch 2690, training loss: 619.9745483398438 = 0.0246623232960701 + 100.0 * 6.199498653411865
Epoch 2690, val loss: 1.367461919784546
Epoch 2700, training loss: 620.2850952148438 = 0.024376530200242996 + 100.0 * 6.202607154846191
Epoch 2700, val loss: 1.3703441619873047
Epoch 2710, training loss: 619.9556884765625 = 0.024074736982584 + 100.0 * 6.199316501617432
Epoch 2710, val loss: 1.3728383779525757
Epoch 2720, training loss: 620.0731811523438 = 0.023792868480086327 + 100.0 * 6.200493812561035
Epoch 2720, val loss: 1.3753695487976074
Epoch 2730, training loss: 620.1417236328125 = 0.023510660976171494 + 100.0 * 6.2011823654174805
Epoch 2730, val loss: 1.3780187368392944
Epoch 2740, training loss: 619.9672241210938 = 0.023238128051161766 + 100.0 * 6.199439525604248
Epoch 2740, val loss: 1.3812683820724487
Epoch 2750, training loss: 619.8173217773438 = 0.02297006919980049 + 100.0 * 6.197943687438965
Epoch 2750, val loss: 1.383887767791748
Epoch 2760, training loss: 620.0000610351562 = 0.02272024191915989 + 100.0 * 6.19977331161499
Epoch 2760, val loss: 1.3864837884902954
Epoch 2770, training loss: 620.1901245117188 = 0.02246011048555374 + 100.0 * 6.201676845550537
Epoch 2770, val loss: 1.3891526460647583
Epoch 2780, training loss: 619.9827880859375 = 0.022179769352078438 + 100.0 * 6.199606418609619
Epoch 2780, val loss: 1.391861915588379
Epoch 2790, training loss: 620.0890502929688 = 0.021929778158664703 + 100.0 * 6.200671195983887
Epoch 2790, val loss: 1.3944039344787598
Epoch 2800, training loss: 620.0693359375 = 0.02167854644358158 + 100.0 * 6.20047664642334
Epoch 2800, val loss: 1.3972569704055786
Epoch 2810, training loss: 619.8727416992188 = 0.021426746621727943 + 100.0 * 6.198513031005859
Epoch 2810, val loss: 1.3993362188339233
Epoch 2820, training loss: 619.9013061523438 = 0.02118528261780739 + 100.0 * 6.198801517486572
Epoch 2820, val loss: 1.4020453691482544
Epoch 2830, training loss: 619.8373413085938 = 0.02094929665327072 + 100.0 * 6.198163986206055
Epoch 2830, val loss: 1.4047502279281616
Epoch 2840, training loss: 619.7241821289062 = 0.020720504224300385 + 100.0 * 6.19703483581543
Epoch 2840, val loss: 1.407376766204834
Epoch 2850, training loss: 619.7681274414062 = 0.02049947716295719 + 100.0 * 6.197475910186768
Epoch 2850, val loss: 1.4100677967071533
Epoch 2860, training loss: 620.490234375 = 0.020281244069337845 + 100.0 * 6.204699516296387
Epoch 2860, val loss: 1.412455439567566
Epoch 2870, training loss: 620.1500244140625 = 0.020046336576342583 + 100.0 * 6.201300144195557
Epoch 2870, val loss: 1.4145015478134155
Epoch 2880, training loss: 619.7681884765625 = 0.01980721764266491 + 100.0 * 6.197483539581299
Epoch 2880, val loss: 1.417231559753418
Epoch 2890, training loss: 619.6420288085938 = 0.01959841512143612 + 100.0 * 6.196224212646484
Epoch 2890, val loss: 1.4198633432388306
Epoch 2900, training loss: 619.6156616210938 = 0.019399594515562057 + 100.0 * 6.195962429046631
Epoch 2900, val loss: 1.422574758529663
Epoch 2910, training loss: 619.7073974609375 = 0.01920490525662899 + 100.0 * 6.1968817710876465
Epoch 2910, val loss: 1.425215244293213
Epoch 2920, training loss: 620.3610229492188 = 0.018998919054865837 + 100.0 * 6.203420639038086
Epoch 2920, val loss: 1.4275436401367188
Epoch 2930, training loss: 619.8369750976562 = 0.018779492005705833 + 100.0 * 6.198181629180908
Epoch 2930, val loss: 1.429426908493042
Epoch 2940, training loss: 619.5960083007812 = 0.018568769097328186 + 100.0 * 6.195774555206299
Epoch 2940, val loss: 1.4317193031311035
Epoch 2950, training loss: 619.6182861328125 = 0.018377436324954033 + 100.0 * 6.1959991455078125
Epoch 2950, val loss: 1.4343425035476685
Epoch 2960, training loss: 619.6315307617188 = 0.018197735771536827 + 100.0 * 6.196133136749268
Epoch 2960, val loss: 1.4368213415145874
Epoch 2970, training loss: 620.0921630859375 = 0.018012484535574913 + 100.0 * 6.200741291046143
Epoch 2970, val loss: 1.4393233060836792
Epoch 2980, training loss: 620.32958984375 = 0.017826266586780548 + 100.0 * 6.203117847442627
Epoch 2980, val loss: 1.4415098428726196
Epoch 2990, training loss: 619.8151245117188 = 0.01761722005903721 + 100.0 * 6.197975158691406
Epoch 2990, val loss: 1.4431869983673096
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 861.630859375 = 1.9477381706237793 + 100.0 * 8.596831321716309
Epoch 0, val loss: 1.9442541599273682
Epoch 10, training loss: 861.5352172851562 = 1.9393337965011597 + 100.0 * 8.595958709716797
Epoch 10, val loss: 1.9364172220230103
Epoch 20, training loss: 860.9439697265625 = 1.9287312030792236 + 100.0 * 8.590152740478516
Epoch 20, val loss: 1.9262663125991821
Epoch 30, training loss: 856.8993530273438 = 1.9148447513580322 + 100.0 * 8.549844741821289
Epoch 30, val loss: 1.9127817153930664
Epoch 40, training loss: 828.475341796875 = 1.8975845575332642 + 100.0 * 8.265777587890625
Epoch 40, val loss: 1.8964036703109741
Epoch 50, training loss: 753.6150512695312 = 1.8771685361862183 + 100.0 * 7.517378807067871
Epoch 50, val loss: 1.8769021034240723
Epoch 60, training loss: 736.7750854492188 = 1.8601269721984863 + 100.0 * 7.349149703979492
Epoch 60, val loss: 1.8611716032028198
Epoch 70, training loss: 711.49169921875 = 1.8463187217712402 + 100.0 * 7.096453666687012
Epoch 70, val loss: 1.8480432033538818
Epoch 80, training loss: 697.6901245117188 = 1.834272027015686 + 100.0 * 6.958558082580566
Epoch 80, val loss: 1.8369786739349365
Epoch 90, training loss: 689.7579956054688 = 1.8239655494689941 + 100.0 * 6.879340171813965
Epoch 90, val loss: 1.8274424076080322
Epoch 100, training loss: 682.342529296875 = 1.8149166107177734 + 100.0 * 6.805275917053223
Epoch 100, val loss: 1.8189443349838257
Epoch 110, training loss: 674.8624877929688 = 1.8081798553466797 + 100.0 * 6.73054313659668
Epoch 110, val loss: 1.8123736381530762
Epoch 120, training loss: 669.5866088867188 = 1.8028017282485962 + 100.0 * 6.677838325500488
Epoch 120, val loss: 1.806626319885254
Epoch 130, training loss: 665.729248046875 = 1.796894907951355 + 100.0 * 6.639323711395264
Epoch 130, val loss: 1.8002991676330566
Epoch 140, training loss: 662.569580078125 = 1.7906206846237183 + 100.0 * 6.607789039611816
Epoch 140, val loss: 1.7938448190689087
Epoch 150, training loss: 659.5620727539062 = 1.7845380306243896 + 100.0 * 6.577775001525879
Epoch 150, val loss: 1.7876918315887451
Epoch 160, training loss: 656.8851928710938 = 1.778761386871338 + 100.0 * 6.551064491271973
Epoch 160, val loss: 1.781904935836792
Epoch 170, training loss: 654.333984375 = 1.7730201482772827 + 100.0 * 6.525609970092773
Epoch 170, val loss: 1.776318073272705
Epoch 180, training loss: 651.9678344726562 = 1.7667195796966553 + 100.0 * 6.502011299133301
Epoch 180, val loss: 1.770466685295105
Epoch 190, training loss: 650.1207275390625 = 1.7597628831863403 + 100.0 * 6.483609676361084
Epoch 190, val loss: 1.7641918659210205
Epoch 200, training loss: 648.72998046875 = 1.7521402835845947 + 100.0 * 6.469778537750244
Epoch 200, val loss: 1.757375717163086
Epoch 210, training loss: 647.3777465820312 = 1.7437262535095215 + 100.0 * 6.456340312957764
Epoch 210, val loss: 1.7499945163726807
Epoch 220, training loss: 646.083984375 = 1.7346935272216797 + 100.0 * 6.443492889404297
Epoch 220, val loss: 1.7420728206634521
Epoch 230, training loss: 644.87890625 = 1.7250511646270752 + 100.0 * 6.4315385818481445
Epoch 230, val loss: 1.7337645292282104
Epoch 240, training loss: 644.0816040039062 = 1.714672327041626 + 100.0 * 6.423669338226318
Epoch 240, val loss: 1.7249763011932373
Epoch 250, training loss: 643.0389404296875 = 1.7035325765609741 + 100.0 * 6.41335391998291
Epoch 250, val loss: 1.7155216932296753
Epoch 260, training loss: 641.9483642578125 = 1.6914576292037964 + 100.0 * 6.402568817138672
Epoch 260, val loss: 1.7054020166397095
Epoch 270, training loss: 641.0 = 1.6785391569137573 + 100.0 * 6.393214702606201
Epoch 270, val loss: 1.694707989692688
Epoch 280, training loss: 640.34228515625 = 1.6647151708602905 + 100.0 * 6.386775493621826
Epoch 280, val loss: 1.6834137439727783
Epoch 290, training loss: 639.6620483398438 = 1.64985191822052 + 100.0 * 6.380122184753418
Epoch 290, val loss: 1.6710691452026367
Epoch 300, training loss: 638.8465576171875 = 1.6339738368988037 + 100.0 * 6.372125625610352
Epoch 300, val loss: 1.6581149101257324
Epoch 310, training loss: 638.2374267578125 = 1.617169737815857 + 100.0 * 6.366202354431152
Epoch 310, val loss: 1.6445510387420654
Epoch 320, training loss: 637.9893798828125 = 1.5993632078170776 + 100.0 * 6.363900184631348
Epoch 320, val loss: 1.6303390264511108
Epoch 330, training loss: 637.234375 = 1.580366849899292 + 100.0 * 6.356539726257324
Epoch 330, val loss: 1.6150866746902466
Epoch 340, training loss: 636.7122192382812 = 1.5605202913284302 + 100.0 * 6.3515167236328125
Epoch 340, val loss: 1.599333643913269
Epoch 350, training loss: 636.2069702148438 = 1.5398225784301758 + 100.0 * 6.3466715812683105
Epoch 350, val loss: 1.5829565525054932
Epoch 360, training loss: 636.4891967773438 = 1.518349528312683 + 100.0 * 6.349708557128906
Epoch 360, val loss: 1.5659962892532349
Epoch 370, training loss: 635.4085693359375 = 1.4956549406051636 + 100.0 * 6.3391289710998535
Epoch 370, val loss: 1.5484954118728638
Epoch 380, training loss: 634.8960571289062 = 1.472551703453064 + 100.0 * 6.334235191345215
Epoch 380, val loss: 1.5307104587554932
Epoch 390, training loss: 634.5584716796875 = 1.4489115476608276 + 100.0 * 6.3310956954956055
Epoch 390, val loss: 1.512607455253601
Epoch 400, training loss: 634.2401123046875 = 1.4246277809143066 + 100.0 * 6.328155040740967
Epoch 400, val loss: 1.494187355041504
Epoch 410, training loss: 633.8346557617188 = 1.3998684883117676 + 100.0 * 6.324348449707031
Epoch 410, val loss: 1.4756778478622437
Epoch 420, training loss: 633.6143798828125 = 1.374920129776001 + 100.0 * 6.322394371032715
Epoch 420, val loss: 1.4570952653884888
Epoch 430, training loss: 633.1673583984375 = 1.349853754043579 + 100.0 * 6.318175315856934
Epoch 430, val loss: 1.4386014938354492
Epoch 440, training loss: 632.9490356445312 = 1.3245879411697388 + 100.0 * 6.316244602203369
Epoch 440, val loss: 1.420118808746338
Epoch 450, training loss: 632.5481567382812 = 1.2991621494293213 + 100.0 * 6.312489986419678
Epoch 450, val loss: 1.4020308256149292
Epoch 460, training loss: 632.228759765625 = 1.2740757465362549 + 100.0 * 6.309546947479248
Epoch 460, val loss: 1.3842296600341797
Epoch 470, training loss: 632.3059692382812 = 1.2489458322525024 + 100.0 * 6.310569763183594
Epoch 470, val loss: 1.3668361902236938
Epoch 480, training loss: 631.8836059570312 = 1.2239876985549927 + 100.0 * 6.306595802307129
Epoch 480, val loss: 1.3491495847702026
Epoch 490, training loss: 631.4552612304688 = 1.1992814540863037 + 100.0 * 6.302559852600098
Epoch 490, val loss: 1.3323211669921875
Epoch 500, training loss: 631.1634521484375 = 1.17512047290802 + 100.0 * 6.2998833656311035
Epoch 500, val loss: 1.3162493705749512
Epoch 510, training loss: 630.9296264648438 = 1.1513761281967163 + 100.0 * 6.2977824211120605
Epoch 510, val loss: 1.3006244897842407
Epoch 520, training loss: 630.8228149414062 = 1.1277726888656616 + 100.0 * 6.296950817108154
Epoch 520, val loss: 1.2852859497070312
Epoch 530, training loss: 630.7076416015625 = 1.1046838760375977 + 100.0 * 6.296029567718506
Epoch 530, val loss: 1.2703782320022583
Epoch 540, training loss: 630.3460693359375 = 1.0821053981781006 + 100.0 * 6.29263973236084
Epoch 540, val loss: 1.2562810182571411
Epoch 550, training loss: 630.0598754882812 = 1.0601747035980225 + 100.0 * 6.289997100830078
Epoch 550, val loss: 1.2429639101028442
Epoch 560, training loss: 630.0585327148438 = 1.0386924743652344 + 100.0 * 6.29019832611084
Epoch 560, val loss: 1.2299870252609253
Epoch 570, training loss: 630.0350341796875 = 1.017621397972107 + 100.0 * 6.2901740074157715
Epoch 570, val loss: 1.2172448635101318
Epoch 580, training loss: 629.583740234375 = 0.9970362186431885 + 100.0 * 6.285867214202881
Epoch 580, val loss: 1.2055091857910156
Epoch 590, training loss: 629.3035278320312 = 0.9771692156791687 + 100.0 * 6.283263683319092
Epoch 590, val loss: 1.1942172050476074
Epoch 600, training loss: 629.2301025390625 = 0.9578561782836914 + 100.0 * 6.282722473144531
Epoch 600, val loss: 1.1835795640945435
Epoch 610, training loss: 629.0409545898438 = 0.9388790726661682 + 100.0 * 6.281020641326904
Epoch 610, val loss: 1.1732815504074097
Epoch 620, training loss: 628.822021484375 = 0.9203281998634338 + 100.0 * 6.279016971588135
Epoch 620, val loss: 1.1636688709259033
Epoch 630, training loss: 628.7930297851562 = 0.9024237990379333 + 100.0 * 6.278906345367432
Epoch 630, val loss: 1.1543587446212769
Epoch 640, training loss: 628.615234375 = 0.8848211765289307 + 100.0 * 6.277304172515869
Epoch 640, val loss: 1.1456977128982544
Epoch 650, training loss: 628.3384399414062 = 0.8677543997764587 + 100.0 * 6.274707317352295
Epoch 650, val loss: 1.1374362707138062
Epoch 660, training loss: 628.1367797851562 = 0.8512327075004578 + 100.0 * 6.272855281829834
Epoch 660, val loss: 1.1297197341918945
Epoch 670, training loss: 628.77587890625 = 0.8352043032646179 + 100.0 * 6.279406547546387
Epoch 670, val loss: 1.1222368478775024
Epoch 680, training loss: 628.1978149414062 = 0.8191615343093872 + 100.0 * 6.273786544799805
Epoch 680, val loss: 1.1151883602142334
Epoch 690, training loss: 627.7798461914062 = 0.8037176132202148 + 100.0 * 6.269761085510254
Epoch 690, val loss: 1.1085939407348633
Epoch 700, training loss: 627.62255859375 = 0.7888652086257935 + 100.0 * 6.268336772918701
Epoch 700, val loss: 1.1024270057678223
Epoch 710, training loss: 627.7407836914062 = 0.7743852734565735 + 100.0 * 6.2696638107299805
Epoch 710, val loss: 1.096737027168274
Epoch 720, training loss: 627.3978271484375 = 0.7601223587989807 + 100.0 * 6.266376972198486
Epoch 720, val loss: 1.0911439657211304
Epoch 730, training loss: 627.533203125 = 0.7462652921676636 + 100.0 * 6.267869472503662
Epoch 730, val loss: 1.0861023664474487
Epoch 740, training loss: 627.2189331054688 = 0.7326400876045227 + 100.0 * 6.264862537384033
Epoch 740, val loss: 1.0811125040054321
Epoch 750, training loss: 627.040771484375 = 0.7194608449935913 + 100.0 * 6.263213157653809
Epoch 750, val loss: 1.0767561197280884
Epoch 760, training loss: 627.0377807617188 = 0.706649661064148 + 100.0 * 6.26331090927124
Epoch 760, val loss: 1.0727465152740479
Epoch 770, training loss: 626.8033447265625 = 0.6939263939857483 + 100.0 * 6.261094093322754
Epoch 770, val loss: 1.0687671899795532
Epoch 780, training loss: 626.8748779296875 = 0.6815399527549744 + 100.0 * 6.261933326721191
Epoch 780, val loss: 1.0650852918624878
Epoch 790, training loss: 626.618896484375 = 0.669375479221344 + 100.0 * 6.259495258331299
Epoch 790, val loss: 1.061622142791748
Epoch 800, training loss: 626.500244140625 = 0.6575653553009033 + 100.0 * 6.258426666259766
Epoch 800, val loss: 1.0585851669311523
Epoch 810, training loss: 626.6124877929688 = 0.6459944844245911 + 100.0 * 6.259665012359619
Epoch 810, val loss: 1.0559413433074951
Epoch 820, training loss: 626.3468627929688 = 0.6346052885055542 + 100.0 * 6.257122993469238
Epoch 820, val loss: 1.0530742406845093
Epoch 830, training loss: 626.4395141601562 = 0.6234462261199951 + 100.0 * 6.25816011428833
Epoch 830, val loss: 1.0508917570114136
Epoch 840, training loss: 626.0814819335938 = 0.6124513745307922 + 100.0 * 6.254690647125244
Epoch 840, val loss: 1.0486022233963013
Epoch 850, training loss: 626.0145263671875 = 0.6016854643821716 + 100.0 * 6.254128456115723
Epoch 850, val loss: 1.0466232299804688
Epoch 860, training loss: 625.9512329101562 = 0.5912175178527832 + 100.0 * 6.253600120544434
Epoch 860, val loss: 1.0449976921081543
Epoch 870, training loss: 625.9086303710938 = 0.5808865427970886 + 100.0 * 6.25327730178833
Epoch 870, val loss: 1.043447732925415
Epoch 880, training loss: 626.06884765625 = 0.5706932544708252 + 100.0 * 6.254981994628906
Epoch 880, val loss: 1.0418587923049927
Epoch 890, training loss: 625.6416625976562 = 0.560623288154602 + 100.0 * 6.250810623168945
Epoch 890, val loss: 1.0410964488983154
Epoch 900, training loss: 625.5353393554688 = 0.5508524179458618 + 100.0 * 6.249845027923584
Epoch 900, val loss: 1.0400551557540894
Epoch 910, training loss: 625.8348388671875 = 0.5413345694541931 + 100.0 * 6.252935409545898
Epoch 910, val loss: 1.0395784378051758
Epoch 920, training loss: 625.5328369140625 = 0.5316395163536072 + 100.0 * 6.250011920928955
Epoch 920, val loss: 1.0385767221450806
Epoch 930, training loss: 625.3377685546875 = 0.5222833752632141 + 100.0 * 6.248155117034912
Epoch 930, val loss: 1.038305640220642
Epoch 940, training loss: 625.1898803710938 = 0.5131600499153137 + 100.0 * 6.246767044067383
Epoch 940, val loss: 1.0382306575775146
Epoch 950, training loss: 625.5337524414062 = 0.5042210817337036 + 100.0 * 6.250295639038086
Epoch 950, val loss: 1.0381901264190674
Epoch 960, training loss: 625.1809692382812 = 0.4951848089694977 + 100.0 * 6.246857643127441
Epoch 960, val loss: 1.0381606817245483
Epoch 970, training loss: 625.0562133789062 = 0.48639193177223206 + 100.0 * 6.24569845199585
Epoch 970, val loss: 1.0381823778152466
Epoch 980, training loss: 624.9342041015625 = 0.4778767228126526 + 100.0 * 6.244563102722168
Epoch 980, val loss: 1.0387921333312988
Epoch 990, training loss: 625.231689453125 = 0.469429075717926 + 100.0 * 6.247622489929199
Epoch 990, val loss: 1.0391199588775635
Epoch 1000, training loss: 624.8751220703125 = 0.46104252338409424 + 100.0 * 6.244140625
Epoch 1000, val loss: 1.0400985479354858
Epoch 1010, training loss: 625.113037109375 = 0.45277997851371765 + 100.0 * 6.246603012084961
Epoch 1010, val loss: 1.0407311916351318
Epoch 1020, training loss: 624.8291625976562 = 0.44463610649108887 + 100.0 * 6.243844985961914
Epoch 1020, val loss: 1.0412951707839966
Epoch 1030, training loss: 624.6051635742188 = 0.4366617798805237 + 100.0 * 6.241685390472412
Epoch 1030, val loss: 1.0425572395324707
Epoch 1040, training loss: 624.760009765625 = 0.4288645088672638 + 100.0 * 6.243310928344727
Epoch 1040, val loss: 1.0436527729034424
Epoch 1050, training loss: 624.47705078125 = 0.42109352350234985 + 100.0 * 6.2405595779418945
Epoch 1050, val loss: 1.0445796251296997
Epoch 1060, training loss: 624.4686889648438 = 0.413520872592926 + 100.0 * 6.240551471710205
Epoch 1060, val loss: 1.0461413860321045
Epoch 1070, training loss: 624.6156005859375 = 0.4059833288192749 + 100.0 * 6.242095947265625
Epoch 1070, val loss: 1.0470607280731201
Epoch 1080, training loss: 624.4845581054688 = 0.39850056171417236 + 100.0 * 6.240860462188721
Epoch 1080, val loss: 1.048694372177124
Epoch 1090, training loss: 624.28857421875 = 0.3912571668624878 + 100.0 * 6.238973617553711
Epoch 1090, val loss: 1.0502175092697144
Epoch 1100, training loss: 624.1151123046875 = 0.38410961627960205 + 100.0 * 6.237310409545898
Epoch 1100, val loss: 1.0521552562713623
Epoch 1110, training loss: 624.3844604492188 = 0.3771320879459381 + 100.0 * 6.240073204040527
Epoch 1110, val loss: 1.0542715787887573
Epoch 1120, training loss: 624.007080078125 = 0.37010297179222107 + 100.0 * 6.236369609832764
Epoch 1120, val loss: 1.0556573867797852
Epoch 1130, training loss: 624.057373046875 = 0.36328986287117004 + 100.0 * 6.236940860748291
Epoch 1130, val loss: 1.0580329895019531
Epoch 1140, training loss: 624.2794799804688 = 0.3564945459365845 + 100.0 * 6.239229679107666
Epoch 1140, val loss: 1.059793472290039
Epoch 1150, training loss: 624.017822265625 = 0.34976768493652344 + 100.0 * 6.236680030822754
Epoch 1150, val loss: 1.0622127056121826
Epoch 1160, training loss: 623.8097534179688 = 0.34327077865600586 + 100.0 * 6.2346649169921875
Epoch 1160, val loss: 1.0646482706069946
Epoch 1170, training loss: 623.7139892578125 = 0.3368784487247467 + 100.0 * 6.233771324157715
Epoch 1170, val loss: 1.067095160484314
Epoch 1180, training loss: 623.8994750976562 = 0.3306640684604645 + 100.0 * 6.235687732696533
Epoch 1180, val loss: 1.0697880983352661
Epoch 1190, training loss: 623.6996459960938 = 0.32432812452316284 + 100.0 * 6.233753204345703
Epoch 1190, val loss: 1.0720229148864746
Epoch 1200, training loss: 623.7454223632812 = 0.31817007064819336 + 100.0 * 6.234272480010986
Epoch 1200, val loss: 1.0747886896133423
Epoch 1210, training loss: 623.658203125 = 0.3121137320995331 + 100.0 * 6.233460426330566
Epoch 1210, val loss: 1.077362060546875
Epoch 1220, training loss: 623.6798095703125 = 0.3062082529067993 + 100.0 * 6.233736038208008
Epoch 1220, val loss: 1.08028244972229
Epoch 1230, training loss: 623.4332885742188 = 0.3004087209701538 + 100.0 * 6.231328964233398
Epoch 1230, val loss: 1.083430528640747
Epoch 1240, training loss: 623.359375 = 0.29475224018096924 + 100.0 * 6.23064661026001
Epoch 1240, val loss: 1.0863622426986694
Epoch 1250, training loss: 623.7477416992188 = 0.2892032265663147 + 100.0 * 6.234585285186768
Epoch 1250, val loss: 1.0894500017166138
Epoch 1260, training loss: 623.4349365234375 = 0.283634215593338 + 100.0 * 6.231513023376465
Epoch 1260, val loss: 1.0926642417907715
Epoch 1270, training loss: 623.3256225585938 = 0.27818870544433594 + 100.0 * 6.230474472045898
Epoch 1270, val loss: 1.0956084728240967
Epoch 1280, training loss: 623.3510131835938 = 0.27291685342788696 + 100.0 * 6.230781555175781
Epoch 1280, val loss: 1.0991803407669067
Epoch 1290, training loss: 623.3099365234375 = 0.26770156621932983 + 100.0 * 6.230422496795654
Epoch 1290, val loss: 1.1025980710983276
Epoch 1300, training loss: 623.4185180664062 = 0.26257508993148804 + 100.0 * 6.2315592765808105
Epoch 1300, val loss: 1.1061147451400757
Epoch 1310, training loss: 623.17236328125 = 0.2574680745601654 + 100.0 * 6.229149341583252
Epoch 1310, val loss: 1.1091477870941162
Epoch 1320, training loss: 623.1157836914062 = 0.25253528356552124 + 100.0 * 6.22863245010376
Epoch 1320, val loss: 1.113096833229065
Epoch 1330, training loss: 622.9596557617188 = 0.24773187935352325 + 100.0 * 6.227118968963623
Epoch 1330, val loss: 1.1166363954544067
Epoch 1340, training loss: 622.9048461914062 = 0.24307405948638916 + 100.0 * 6.226617336273193
Epoch 1340, val loss: 1.1207059621810913
Epoch 1350, training loss: 623.3825073242188 = 0.23853370547294617 + 100.0 * 6.231439590454102
Epoch 1350, val loss: 1.124512791633606
Epoch 1360, training loss: 622.9431762695312 = 0.23385393619537354 + 100.0 * 6.22709321975708
Epoch 1360, val loss: 1.1280372142791748
Epoch 1370, training loss: 622.8826293945312 = 0.22940294444561005 + 100.0 * 6.226531982421875
Epoch 1370, val loss: 1.1319741010665894
Epoch 1380, training loss: 622.9496459960938 = 0.2250588834285736 + 100.0 * 6.227246284484863
Epoch 1380, val loss: 1.1358439922332764
Epoch 1390, training loss: 622.8510131835938 = 0.22076237201690674 + 100.0 * 6.226302623748779
Epoch 1390, val loss: 1.139831781387329
Epoch 1400, training loss: 623.006103515625 = 0.21655115485191345 + 100.0 * 6.227895736694336
Epoch 1400, val loss: 1.1441158056259155
Epoch 1410, training loss: 622.81689453125 = 0.2124030441045761 + 100.0 * 6.226044654846191
Epoch 1410, val loss: 1.1478254795074463
Epoch 1420, training loss: 622.670166015625 = 0.20835286378860474 + 100.0 * 6.224617958068848
Epoch 1420, val loss: 1.15188467502594
Epoch 1430, training loss: 622.6873168945312 = 0.2044573575258255 + 100.0 * 6.224828720092773
Epoch 1430, val loss: 1.1563776731491089
Epoch 1440, training loss: 623.0588989257812 = 0.2005704790353775 + 100.0 * 6.228583335876465
Epoch 1440, val loss: 1.1602834463119507
Epoch 1450, training loss: 622.7680053710938 = 0.19671255350112915 + 100.0 * 6.225712776184082
Epoch 1450, val loss: 1.1645796298980713
Epoch 1460, training loss: 622.5452270507812 = 0.19295883178710938 + 100.0 * 6.223523139953613
Epoch 1460, val loss: 1.1687341928482056
Epoch 1470, training loss: 622.4002685546875 = 0.18933258950710297 + 100.0 * 6.222109317779541
Epoch 1470, val loss: 1.1731395721435547
Epoch 1480, training loss: 622.7442626953125 = 0.18581929802894592 + 100.0 * 6.225584983825684
Epoch 1480, val loss: 1.1775081157684326
Epoch 1490, training loss: 622.4511108398438 = 0.18221871554851532 + 100.0 * 6.222689151763916
Epoch 1490, val loss: 1.1816374063491821
Epoch 1500, training loss: 622.4208984375 = 0.1787172108888626 + 100.0 * 6.222421646118164
Epoch 1500, val loss: 1.1858140230178833
Epoch 1510, training loss: 622.3002319335938 = 0.17541374266147614 + 100.0 * 6.221248149871826
Epoch 1510, val loss: 1.1904373168945312
Epoch 1520, training loss: 622.2469482421875 = 0.17218230664730072 + 100.0 * 6.220747947692871
Epoch 1520, val loss: 1.1951642036437988
Epoch 1530, training loss: 622.8585205078125 = 0.16902315616607666 + 100.0 * 6.226894855499268
Epoch 1530, val loss: 1.1994129419326782
Epoch 1540, training loss: 622.6541748046875 = 0.16580231487751007 + 100.0 * 6.224883556365967
Epoch 1540, val loss: 1.2040081024169922
Epoch 1550, training loss: 622.3006591796875 = 0.16263827681541443 + 100.0 * 6.22137975692749
Epoch 1550, val loss: 1.2082948684692383
Epoch 1560, training loss: 622.0902099609375 = 0.159624844789505 + 100.0 * 6.219305992126465
Epoch 1560, val loss: 1.2129942178726196
Epoch 1570, training loss: 622.13818359375 = 0.15673662722110748 + 100.0 * 6.219814300537109
Epoch 1570, val loss: 1.2179323434829712
Epoch 1580, training loss: 622.455810546875 = 0.15386414527893066 + 100.0 * 6.223019599914551
Epoch 1580, val loss: 1.222566843032837
Epoch 1590, training loss: 622.5174560546875 = 0.15098637342453003 + 100.0 * 6.223664283752441
Epoch 1590, val loss: 1.2270667552947998
Epoch 1600, training loss: 622.3961791992188 = 0.14815573394298553 + 100.0 * 6.222480773925781
Epoch 1600, val loss: 1.2313575744628906
Epoch 1610, training loss: 621.9830322265625 = 0.14539514482021332 + 100.0 * 6.218376159667969
Epoch 1610, val loss: 1.2360785007476807
Epoch 1620, training loss: 621.9190063476562 = 0.14277368783950806 + 100.0 * 6.217762470245361
Epoch 1620, val loss: 1.240843653678894
Epoch 1630, training loss: 621.8817749023438 = 0.1402195543050766 + 100.0 * 6.217415809631348
Epoch 1630, val loss: 1.2457540035247803
Epoch 1640, training loss: 622.2490844726562 = 0.1377214789390564 + 100.0 * 6.221114158630371
Epoch 1640, val loss: 1.250275731086731
Epoch 1650, training loss: 622.3276977539062 = 0.1351643055677414 + 100.0 * 6.221924781799316
Epoch 1650, val loss: 1.255487322807312
Epoch 1660, training loss: 621.9967041015625 = 0.13263489305973053 + 100.0 * 6.2186408042907715
Epoch 1660, val loss: 1.259186863899231
Epoch 1670, training loss: 621.8336181640625 = 0.13024115562438965 + 100.0 * 6.217033386230469
Epoch 1670, val loss: 1.2646591663360596
Epoch 1680, training loss: 621.79443359375 = 0.1279325634241104 + 100.0 * 6.216664791107178
Epoch 1680, val loss: 1.2694212198257446
Epoch 1690, training loss: 622.3673706054688 = 0.12568746507167816 + 100.0 * 6.222416877746582
Epoch 1690, val loss: 1.2741966247558594
Epoch 1700, training loss: 621.9603271484375 = 0.12340747565031052 + 100.0 * 6.218369007110596
Epoch 1700, val loss: 1.2791930437088013
Epoch 1710, training loss: 621.775146484375 = 0.12116727977991104 + 100.0 * 6.2165398597717285
Epoch 1710, val loss: 1.2838332653045654
Epoch 1720, training loss: 621.9822387695312 = 0.11905905604362488 + 100.0 * 6.218631744384766
Epoch 1720, val loss: 1.2890304327011108
Epoch 1730, training loss: 621.6436767578125 = 0.11690369993448257 + 100.0 * 6.215267658233643
Epoch 1730, val loss: 1.2934025526046753
Epoch 1740, training loss: 621.6575317382812 = 0.11486007273197174 + 100.0 * 6.215426921844482
Epoch 1740, val loss: 1.2981550693511963
Epoch 1750, training loss: 621.7443237304688 = 0.11285994201898575 + 100.0 * 6.216314792633057
Epoch 1750, val loss: 1.3032282590866089
Epoch 1760, training loss: 621.9912719726562 = 0.1108797937631607 + 100.0 * 6.218803405761719
Epoch 1760, val loss: 1.3077542781829834
Epoch 1770, training loss: 621.5641479492188 = 0.10886561870574951 + 100.0 * 6.214552402496338
Epoch 1770, val loss: 1.3126760721206665
Epoch 1780, training loss: 621.5076293945312 = 0.10696951299905777 + 100.0 * 6.214006423950195
Epoch 1780, val loss: 1.3174999952316284
Epoch 1790, training loss: 621.6406860351562 = 0.10514278709888458 + 100.0 * 6.215354919433594
Epoch 1790, val loss: 1.3225665092468262
Epoch 1800, training loss: 621.666259765625 = 0.10329875349998474 + 100.0 * 6.215629577636719
Epoch 1800, val loss: 1.3272303342819214
Epoch 1810, training loss: 621.5054321289062 = 0.10149387270212173 + 100.0 * 6.214039325714111
Epoch 1810, val loss: 1.3320392370224
Epoch 1820, training loss: 621.496826171875 = 0.09975289553403854 + 100.0 * 6.213971138000488
Epoch 1820, val loss: 1.337180256843567
Epoch 1830, training loss: 621.64013671875 = 0.09805513173341751 + 100.0 * 6.215420722961426
Epoch 1830, val loss: 1.3419710397720337
Epoch 1840, training loss: 621.7562255859375 = 0.09634668380022049 + 100.0 * 6.2165985107421875
Epoch 1840, val loss: 1.3466112613677979
Epoch 1850, training loss: 621.4149780273438 = 0.09466876089572906 + 100.0 * 6.213203430175781
Epoch 1850, val loss: 1.3508578538894653
Epoch 1860, training loss: 621.4320068359375 = 0.09306525439023972 + 100.0 * 6.2133893966674805
Epoch 1860, val loss: 1.356357216835022
Epoch 1870, training loss: 621.7479858398438 = 0.09150108695030212 + 100.0 * 6.216565132141113
Epoch 1870, val loss: 1.3606404066085815
Epoch 1880, training loss: 621.482421875 = 0.08989614993333817 + 100.0 * 6.213924884796143
Epoch 1880, val loss: 1.3654450178146362
Epoch 1890, training loss: 621.2591552734375 = 0.08838719874620438 + 100.0 * 6.211707592010498
Epoch 1890, val loss: 1.3703763484954834
Epoch 1900, training loss: 621.228515625 = 0.08692573755979538 + 100.0 * 6.211416244506836
Epoch 1900, val loss: 1.3753364086151123
Epoch 1910, training loss: 621.521240234375 = 0.0855199471116066 + 100.0 * 6.214357376098633
Epoch 1910, val loss: 1.3797224760055542
Epoch 1920, training loss: 621.2093505859375 = 0.08402679860591888 + 100.0 * 6.2112531661987305
Epoch 1920, val loss: 1.384511113166809
Epoch 1930, training loss: 621.1852416992188 = 0.08261550962924957 + 100.0 * 6.211026191711426
Epoch 1930, val loss: 1.389038324356079
Epoch 1940, training loss: 621.53369140625 = 0.08125434815883636 + 100.0 * 6.214524745941162
Epoch 1940, val loss: 1.394005537033081
Epoch 1950, training loss: 621.2766723632812 = 0.07986940443515778 + 100.0 * 6.211967468261719
Epoch 1950, val loss: 1.398771047592163
Epoch 1960, training loss: 621.1336669921875 = 0.07851406931877136 + 100.0 * 6.210551738739014
Epoch 1960, val loss: 1.402902603149414
Epoch 1970, training loss: 621.0460205078125 = 0.07723807543516159 + 100.0 * 6.209688186645508
Epoch 1970, val loss: 1.4080973863601685
Epoch 1980, training loss: 621.03662109375 = 0.0760008916258812 + 100.0 * 6.209606170654297
Epoch 1980, val loss: 1.4129045009613037
Epoch 1990, training loss: 621.6636352539062 = 0.07479685544967651 + 100.0 * 6.215888500213623
Epoch 1990, val loss: 1.417365550994873
Epoch 2000, training loss: 621.4068603515625 = 0.07352391630411148 + 100.0 * 6.2133331298828125
Epoch 2000, val loss: 1.4214621782302856
Epoch 2010, training loss: 621.1085205078125 = 0.07230960577726364 + 100.0 * 6.210361957550049
Epoch 2010, val loss: 1.4265698194503784
Epoch 2020, training loss: 621.1341552734375 = 0.0711483359336853 + 100.0 * 6.210629940032959
Epoch 2020, val loss: 1.4311752319335938
Epoch 2030, training loss: 621.1658935546875 = 0.0699978694319725 + 100.0 * 6.210958957672119
Epoch 2030, val loss: 1.4354740381240845
Epoch 2040, training loss: 621.020751953125 = 0.06884779036045074 + 100.0 * 6.209518909454346
Epoch 2040, val loss: 1.440588355064392
Epoch 2050, training loss: 620.8646240234375 = 0.06775190681219101 + 100.0 * 6.207968711853027
Epoch 2050, val loss: 1.4451930522918701
Epoch 2060, training loss: 621.0717163085938 = 0.06670616567134857 + 100.0 * 6.210050106048584
Epoch 2060, val loss: 1.449955940246582
Epoch 2070, training loss: 621.2374877929688 = 0.06563286483287811 + 100.0 * 6.211719036102295
Epoch 2070, val loss: 1.454056978225708
Epoch 2080, training loss: 620.8551635742188 = 0.06453443318605423 + 100.0 * 6.207906723022461
Epoch 2080, val loss: 1.4585531949996948
Epoch 2090, training loss: 620.7945556640625 = 0.0635177418589592 + 100.0 * 6.207310199737549
Epoch 2090, val loss: 1.4629490375518799
Epoch 2100, training loss: 620.752197265625 = 0.06253857910633087 + 100.0 * 6.2068963050842285
Epoch 2100, val loss: 1.4679269790649414
Epoch 2110, training loss: 620.7556762695312 = 0.06158949434757233 + 100.0 * 6.2069411277771
Epoch 2110, val loss: 1.4725432395935059
Epoch 2120, training loss: 621.3418579101562 = 0.060656458139419556 + 100.0 * 6.2128119468688965
Epoch 2120, val loss: 1.4769198894500732
Epoch 2130, training loss: 621.4351806640625 = 0.05969014763832092 + 100.0 * 6.213754653930664
Epoch 2130, val loss: 1.4805186986923218
Epoch 2140, training loss: 621.0162963867188 = 0.058697789907455444 + 100.0 * 6.209575653076172
Epoch 2140, val loss: 1.4850420951843262
Epoch 2150, training loss: 620.8070678710938 = 0.05776457116007805 + 100.0 * 6.207493305206299
Epoch 2150, val loss: 1.4899694919586182
Epoch 2160, training loss: 620.6835327148438 = 0.056890811771154404 + 100.0 * 6.206266403198242
Epoch 2160, val loss: 1.4942907094955444
Epoch 2170, training loss: 620.6567993164062 = 0.05605040118098259 + 100.0 * 6.206007480621338
Epoch 2170, val loss: 1.4989699125289917
Epoch 2180, training loss: 620.973388671875 = 0.055222444236278534 + 100.0 * 6.209181785583496
Epoch 2180, val loss: 1.5031311511993408
Epoch 2190, training loss: 620.7242431640625 = 0.054345548152923584 + 100.0 * 6.206699371337891
Epoch 2190, val loss: 1.507835865020752
Epoch 2200, training loss: 620.6609497070312 = 0.053504578769207 + 100.0 * 6.206074237823486
Epoch 2200, val loss: 1.511971354484558
Epoch 2210, training loss: 620.6370849609375 = 0.05270307883620262 + 100.0 * 6.205843925476074
Epoch 2210, val loss: 1.5164662599563599
Epoch 2220, training loss: 620.7640991210938 = 0.0519302599132061 + 100.0 * 6.207121849060059
Epoch 2220, val loss: 1.5211107730865479
Epoch 2230, training loss: 620.7191162109375 = 0.05114490166306496 + 100.0 * 6.206679821014404
Epoch 2230, val loss: 1.524950623512268
Epoch 2240, training loss: 620.6587524414062 = 0.05038655549287796 + 100.0 * 6.206083297729492
Epoch 2240, val loss: 1.5294848680496216
Epoch 2250, training loss: 620.5108642578125 = 0.04963487759232521 + 100.0 * 6.2046122550964355
Epoch 2250, val loss: 1.533969521522522
Epoch 2260, training loss: 620.6798706054688 = 0.04891619458794594 + 100.0 * 6.206309795379639
Epoch 2260, val loss: 1.5382335186004639
Epoch 2270, training loss: 620.599365234375 = 0.048198454082012177 + 100.0 * 6.205511569976807
Epoch 2270, val loss: 1.542595386505127
Epoch 2280, training loss: 621.0849609375 = 0.0474751852452755 + 100.0 * 6.21037483215332
Epoch 2280, val loss: 1.5467760562896729
Epoch 2290, training loss: 620.6065673828125 = 0.04673862084746361 + 100.0 * 6.205597877502441
Epoch 2290, val loss: 1.5500452518463135
Epoch 2300, training loss: 620.498779296875 = 0.04604145139455795 + 100.0 * 6.204527854919434
Epoch 2300, val loss: 1.5548254251480103
Epoch 2310, training loss: 620.3794555664062 = 0.045389991253614426 + 100.0 * 6.203340530395508
Epoch 2310, val loss: 1.559095025062561
Epoch 2320, training loss: 620.402099609375 = 0.044757138937711716 + 100.0 * 6.203573703765869
Epoch 2320, val loss: 1.5635335445404053
Epoch 2330, training loss: 620.9006958007812 = 0.04413421452045441 + 100.0 * 6.208565711975098
Epoch 2330, val loss: 1.5676395893096924
Epoch 2340, training loss: 621.120361328125 = 0.043471816927194595 + 100.0 * 6.210769176483154
Epoch 2340, val loss: 1.5709829330444336
Epoch 2350, training loss: 620.5797729492188 = 0.04281415417790413 + 100.0 * 6.20536994934082
Epoch 2350, val loss: 1.5753356218338013
Epoch 2360, training loss: 620.3008422851562 = 0.04219286888837814 + 100.0 * 6.202586650848389
Epoch 2360, val loss: 1.579285740852356
Epoch 2370, training loss: 620.287841796875 = 0.041613105684518814 + 100.0 * 6.202462196350098
Epoch 2370, val loss: 1.5835386514663696
Epoch 2380, training loss: 620.365966796875 = 0.041050318628549576 + 100.0 * 6.203248977661133
Epoch 2380, val loss: 1.5877561569213867
Epoch 2390, training loss: 620.792724609375 = 0.04049088433384895 + 100.0 * 6.207522869110107
Epoch 2390, val loss: 1.5913951396942139
Epoch 2400, training loss: 620.3754272460938 = 0.03988558053970337 + 100.0 * 6.20335578918457
Epoch 2400, val loss: 1.5957586765289307
Epoch 2410, training loss: 620.3798217773438 = 0.039332807064056396 + 100.0 * 6.203405380249023
Epoch 2410, val loss: 1.5995864868164062
Epoch 2420, training loss: 620.5042724609375 = 0.03878190740942955 + 100.0 * 6.204655170440674
Epoch 2420, val loss: 1.603276014328003
Epoch 2430, training loss: 620.431884765625 = 0.03823459520936012 + 100.0 * 6.20393705368042
Epoch 2430, val loss: 1.6071293354034424
Epoch 2440, training loss: 620.208251953125 = 0.0377018004655838 + 100.0 * 6.201705455780029
Epoch 2440, val loss: 1.6114933490753174
Epoch 2450, training loss: 620.230224609375 = 0.037197716534137726 + 100.0 * 6.201930522918701
Epoch 2450, val loss: 1.6154471635818481
Epoch 2460, training loss: 620.533935546875 = 0.03671681135892868 + 100.0 * 6.204971790313721
Epoch 2460, val loss: 1.6193591356277466
Epoch 2470, training loss: 620.3289184570312 = 0.03619461879134178 + 100.0 * 6.202927112579346
Epoch 2470, val loss: 1.6227003335952759
Epoch 2480, training loss: 620.2127075195312 = 0.03569336608052254 + 100.0 * 6.201769828796387
Epoch 2480, val loss: 1.6269505023956299
Epoch 2490, training loss: 620.2533569335938 = 0.035224154591560364 + 100.0 * 6.202181339263916
Epoch 2490, val loss: 1.630914330482483
Epoch 2500, training loss: 620.1439208984375 = 0.03474635258316994 + 100.0 * 6.201091766357422
Epoch 2500, val loss: 1.6348743438720703
Epoch 2510, training loss: 620.1459350585938 = 0.03429543599486351 + 100.0 * 6.20111608505249
Epoch 2510, val loss: 1.6391150951385498
Epoch 2520, training loss: 620.63037109375 = 0.033855851739645004 + 100.0 * 6.205965042114258
Epoch 2520, val loss: 1.643212914466858
Epoch 2530, training loss: 620.4412231445312 = 0.03337780013680458 + 100.0 * 6.204078197479248
Epoch 2530, val loss: 1.6461976766586304
Epoch 2540, training loss: 620.16455078125 = 0.032929014414548874 + 100.0 * 6.201315879821777
Epoch 2540, val loss: 1.6495211124420166
Epoch 2550, training loss: 620.0543823242188 = 0.03250284492969513 + 100.0 * 6.200218200683594
Epoch 2550, val loss: 1.6539967060089111
Epoch 2560, training loss: 620.2237548828125 = 0.03210557624697685 + 100.0 * 6.201916694641113
Epoch 2560, val loss: 1.6576766967773438
Epoch 2570, training loss: 620.1201782226562 = 0.03167479485273361 + 100.0 * 6.20088529586792
Epoch 2570, val loss: 1.6614034175872803
Epoch 2580, training loss: 620.181640625 = 0.031268324702978134 + 100.0 * 6.201503753662109
Epoch 2580, val loss: 1.6649534702301025
Epoch 2590, training loss: 620.16796875 = 0.0308702252805233 + 100.0 * 6.201370716094971
Epoch 2590, val loss: 1.6688870191574097
Epoch 2600, training loss: 620.0819702148438 = 0.030477866530418396 + 100.0 * 6.200515270233154
Epoch 2600, val loss: 1.6725138425827026
Epoch 2610, training loss: 620.2346801757812 = 0.030097199603915215 + 100.0 * 6.202045917510986
Epoch 2610, val loss: 1.6762197017669678
Epoch 2620, training loss: 620.5668334960938 = 0.029719872400164604 + 100.0 * 6.205371379852295
Epoch 2620, val loss: 1.6791234016418457
Epoch 2630, training loss: 619.9800415039062 = 0.02930832840502262 + 100.0 * 6.199507236480713
Epoch 2630, val loss: 1.683189034461975
Epoch 2640, training loss: 619.8941650390625 = 0.028943561017513275 + 100.0 * 6.198652267456055
Epoch 2640, val loss: 1.6867636442184448
Epoch 2650, training loss: 619.8690185546875 = 0.028596971184015274 + 100.0 * 6.198403835296631
Epoch 2650, val loss: 1.6905361413955688
Epoch 2660, training loss: 620.0714721679688 = 0.028260745108127594 + 100.0 * 6.200431823730469
Epoch 2660, val loss: 1.6946794986724854
Epoch 2670, training loss: 620.0677490234375 = 0.02790565975010395 + 100.0 * 6.2003984451293945
Epoch 2670, val loss: 1.6973425149917603
Epoch 2680, training loss: 620.0238647460938 = 0.027544885873794556 + 100.0 * 6.199963092803955
Epoch 2680, val loss: 1.7004481554031372
Epoch 2690, training loss: 620.1482543945312 = 0.027217015624046326 + 100.0 * 6.2012104988098145
Epoch 2690, val loss: 1.7044131755828857
Epoch 2700, training loss: 619.880859375 = 0.026872755959630013 + 100.0 * 6.198539733886719
Epoch 2700, val loss: 1.7072780132293701
Epoch 2710, training loss: 619.8256225585938 = 0.026544179767370224 + 100.0 * 6.197990894317627
Epoch 2710, val loss: 1.7110594511032104
Epoch 2720, training loss: 619.8358154296875 = 0.026234742254018784 + 100.0 * 6.198095798492432
Epoch 2720, val loss: 1.7147549390792847
Epoch 2730, training loss: 620.2022094726562 = 0.02593209035694599 + 100.0 * 6.201762676239014
Epoch 2730, val loss: 1.7180875539779663
Epoch 2740, training loss: 619.7435913085938 = 0.025606097653508186 + 100.0 * 6.197179794311523
Epoch 2740, val loss: 1.7214076519012451
Epoch 2750, training loss: 619.7891845703125 = 0.025307703763246536 + 100.0 * 6.197638511657715
Epoch 2750, val loss: 1.724908471107483
Epoch 2760, training loss: 620.0234985351562 = 0.025015227496623993 + 100.0 * 6.199984550476074
Epoch 2760, val loss: 1.7284271717071533
Epoch 2770, training loss: 619.9744873046875 = 0.024721449241042137 + 100.0 * 6.199497699737549
Epoch 2770, val loss: 1.731583595275879
Epoch 2780, training loss: 620.1229858398438 = 0.02441108413040638 + 100.0 * 6.200985431671143
Epoch 2780, val loss: 1.734879970550537
Epoch 2790, training loss: 619.78955078125 = 0.024115484207868576 + 100.0 * 6.1976542472839355
Epoch 2790, val loss: 1.7372105121612549
Epoch 2800, training loss: 619.66552734375 = 0.023837096989154816 + 100.0 * 6.19641637802124
Epoch 2800, val loss: 1.7413368225097656
Epoch 2810, training loss: 619.623046875 = 0.023574836552143097 + 100.0 * 6.195994853973389
Epoch 2810, val loss: 1.744710087776184
Epoch 2820, training loss: 619.61328125 = 0.023317091166973114 + 100.0 * 6.195899963378906
Epoch 2820, val loss: 1.748019814491272
Epoch 2830, training loss: 620.0243530273438 = 0.023074667900800705 + 100.0 * 6.200012683868408
Epoch 2830, val loss: 1.7513296604156494
Epoch 2840, training loss: 620.0216674804688 = 0.022785453125834465 + 100.0 * 6.199988842010498
Epoch 2840, val loss: 1.7535185813903809
Epoch 2850, training loss: 619.8897705078125 = 0.022506987676024437 + 100.0 * 6.198672771453857
Epoch 2850, val loss: 1.756744623184204
Epoch 2860, training loss: 619.6045532226562 = 0.02224268950521946 + 100.0 * 6.1958231925964355
Epoch 2860, val loss: 1.7600524425506592
Epoch 2870, training loss: 619.5969848632812 = 0.02200658805668354 + 100.0 * 6.195749282836914
Epoch 2870, val loss: 1.7634044885635376
Epoch 2880, training loss: 619.9528198242188 = 0.021779261529445648 + 100.0 * 6.199310302734375
Epoch 2880, val loss: 1.7665976285934448
Epoch 2890, training loss: 619.5611572265625 = 0.021524010226130486 + 100.0 * 6.1953959465026855
Epoch 2890, val loss: 1.7698416709899902
Epoch 2900, training loss: 619.5470581054688 = 0.021286899223923683 + 100.0 * 6.195257663726807
Epoch 2900, val loss: 1.772963523864746
Epoch 2910, training loss: 619.6373291015625 = 0.021065745502710342 + 100.0 * 6.196162700653076
Epoch 2910, val loss: 1.7760016918182373
Epoch 2920, training loss: 620.06640625 = 0.020842019468545914 + 100.0 * 6.200456142425537
Epoch 2920, val loss: 1.7792705297470093
Epoch 2930, training loss: 619.8563232421875 = 0.02059912495315075 + 100.0 * 6.198357582092285
Epoch 2930, val loss: 1.7824238538742065
Epoch 2940, training loss: 620.2752075195312 = 0.020375022664666176 + 100.0 * 6.202548027038574
Epoch 2940, val loss: 1.785184383392334
Epoch 2950, training loss: 619.6015625 = 0.02012828178703785 + 100.0 * 6.19581413269043
Epoch 2950, val loss: 1.7872909307479858
Epoch 2960, training loss: 619.504150390625 = 0.019910501316189766 + 100.0 * 6.194842338562012
Epoch 2960, val loss: 1.7907664775848389
Epoch 2970, training loss: 619.4450073242188 = 0.019704828038811684 + 100.0 * 6.194252967834473
Epoch 2970, val loss: 1.7938388586044312
Epoch 2980, training loss: 619.4297485351562 = 0.019507693126797676 + 100.0 * 6.1941022872924805
Epoch 2980, val loss: 1.7968928813934326
Epoch 2990, training loss: 619.4852905273438 = 0.0193126630038023 + 100.0 * 6.194660186767578
Epoch 2990, val loss: 1.7999536991119385
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6666666666666667
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 861.6303100585938 = 1.9474116563796997 + 100.0 * 8.596829414367676
Epoch 0, val loss: 1.9419119358062744
Epoch 10, training loss: 861.53857421875 = 1.9391374588012695 + 100.0 * 8.595993995666504
Epoch 10, val loss: 1.9329040050506592
Epoch 20, training loss: 860.9844360351562 = 1.928906798362732 + 100.0 * 8.590555191040039
Epoch 20, val loss: 1.9216982126235962
Epoch 30, training loss: 857.5818481445312 = 1.9157664775848389 + 100.0 * 8.556660652160645
Epoch 30, val loss: 1.9074159860610962
Epoch 40, training loss: 838.450927734375 = 1.8993515968322754 + 100.0 * 8.36551570892334
Epoch 40, val loss: 1.8904130458831787
Epoch 50, training loss: 771.1368408203125 = 1.88077974319458 + 100.0 * 7.69256067276001
Epoch 50, val loss: 1.871531367301941
Epoch 60, training loss: 749.4832763671875 = 1.863938808441162 + 100.0 * 7.476192951202393
Epoch 60, val loss: 1.8554072380065918
Epoch 70, training loss: 732.1302490234375 = 1.8519532680511475 + 100.0 * 7.302783489227295
Epoch 70, val loss: 1.844542384147644
Epoch 80, training loss: 714.4124755859375 = 1.8399858474731445 + 100.0 * 7.125724792480469
Epoch 80, val loss: 1.8331917524337769
Epoch 90, training loss: 699.4895629882812 = 1.8274842500686646 + 100.0 * 6.976620197296143
Epoch 90, val loss: 1.821694016456604
Epoch 100, training loss: 687.6405639648438 = 1.817620873451233 + 100.0 * 6.858229637145996
Epoch 100, val loss: 1.813156247138977
Epoch 110, training loss: 678.8985595703125 = 1.8088597059249878 + 100.0 * 6.770896911621094
Epoch 110, val loss: 1.8054620027542114
Epoch 120, training loss: 672.395263671875 = 1.8002896308898926 + 100.0 * 6.705949783325195
Epoch 120, val loss: 1.79753839969635
Epoch 130, training loss: 667.49755859375 = 1.7918380498886108 + 100.0 * 6.657057762145996
Epoch 130, val loss: 1.7895889282226562
Epoch 140, training loss: 663.723388671875 = 1.783711314201355 + 100.0 * 6.619396686553955
Epoch 140, val loss: 1.7817546129226685
Epoch 150, training loss: 660.5261840820312 = 1.7753790616989136 + 100.0 * 6.587507724761963
Epoch 150, val loss: 1.7738217115402222
Epoch 160, training loss: 657.6323852539062 = 1.7667858600616455 + 100.0 * 6.558655738830566
Epoch 160, val loss: 1.7655065059661865
Epoch 170, training loss: 655.4811401367188 = 1.7577835321426392 + 100.0 * 6.537233352661133
Epoch 170, val loss: 1.7567870616912842
Epoch 180, training loss: 653.3523559570312 = 1.7480319738388062 + 100.0 * 6.516043186187744
Epoch 180, val loss: 1.7474207878112793
Epoch 190, training loss: 651.7030029296875 = 1.7375818490982056 + 100.0 * 6.4996538162231445
Epoch 190, val loss: 1.7374943494796753
Epoch 200, training loss: 650.3139038085938 = 1.72622811794281 + 100.0 * 6.485876560211182
Epoch 200, val loss: 1.7267754077911377
Epoch 210, training loss: 648.9865112304688 = 1.7137829065322876 + 100.0 * 6.472726821899414
Epoch 210, val loss: 1.7150670289993286
Epoch 220, training loss: 647.736328125 = 1.7004038095474243 + 100.0 * 6.460359573364258
Epoch 220, val loss: 1.702587366104126
Epoch 230, training loss: 646.6419677734375 = 1.6860458850860596 + 100.0 * 6.449559211730957
Epoch 230, val loss: 1.6891757249832153
Epoch 240, training loss: 645.8117065429688 = 1.6705024242401123 + 100.0 * 6.441411972045898
Epoch 240, val loss: 1.6746020317077637
Epoch 250, training loss: 644.723876953125 = 1.6537513732910156 + 100.0 * 6.43070125579834
Epoch 250, val loss: 1.6589479446411133
Epoch 260, training loss: 643.8368530273438 = 1.6358063220977783 + 100.0 * 6.42201042175293
Epoch 260, val loss: 1.6422994136810303
Epoch 270, training loss: 643.2171630859375 = 1.6166929006576538 + 100.0 * 6.416004657745361
Epoch 270, val loss: 1.6244080066680908
Epoch 280, training loss: 642.4878540039062 = 1.5963084697723389 + 100.0 * 6.4089155197143555
Epoch 280, val loss: 1.6052849292755127
Epoch 290, training loss: 641.7329711914062 = 1.5747077465057373 + 100.0 * 6.401582717895508
Epoch 290, val loss: 1.5852982997894287
Epoch 300, training loss: 641.0823364257812 = 1.5522133111953735 + 100.0 * 6.395301342010498
Epoch 300, val loss: 1.564469575881958
Epoch 310, training loss: 640.5546875 = 1.52885901927948 + 100.0 * 6.390258312225342
Epoch 310, val loss: 1.5429271459579468
Epoch 320, training loss: 640.0845947265625 = 1.5045785903930664 + 100.0 * 6.385799884796143
Epoch 320, val loss: 1.5206191539764404
Epoch 330, training loss: 639.5798950195312 = 1.479575753211975 + 100.0 * 6.381003379821777
Epoch 330, val loss: 1.4977657794952393
Epoch 340, training loss: 638.9851684570312 = 1.454150915145874 + 100.0 * 6.375309944152832
Epoch 340, val loss: 1.4747039079666138
Epoch 350, training loss: 638.3630981445312 = 1.4283705949783325 + 100.0 * 6.36934757232666
Epoch 350, val loss: 1.451550006866455
Epoch 360, training loss: 637.88427734375 = 1.4023358821868896 + 100.0 * 6.364819526672363
Epoch 360, val loss: 1.4283472299575806
Epoch 370, training loss: 638.1396484375 = 1.3761314153671265 + 100.0 * 6.3676347732543945
Epoch 370, val loss: 1.405118703842163
Epoch 380, training loss: 637.2227172851562 = 1.3494387865066528 + 100.0 * 6.358733177185059
Epoch 380, val loss: 1.3818418979644775
Epoch 390, training loss: 636.595458984375 = 1.32293701171875 + 100.0 * 6.352725028991699
Epoch 390, val loss: 1.3589987754821777
Epoch 400, training loss: 636.1737060546875 = 1.2966035604476929 + 100.0 * 6.348770618438721
Epoch 400, val loss: 1.3364254236221313
Epoch 410, training loss: 635.8968505859375 = 1.2702786922454834 + 100.0 * 6.34626579284668
Epoch 410, val loss: 1.314483404159546
Epoch 420, training loss: 635.5066528320312 = 1.2441823482513428 + 100.0 * 6.342624664306641
Epoch 420, val loss: 1.2926063537597656
Epoch 430, training loss: 635.0557861328125 = 1.2184500694274902 + 100.0 * 6.33837366104126
Epoch 430, val loss: 1.271665096282959
Epoch 440, training loss: 635.1445922851562 = 1.1930984258651733 + 100.0 * 6.33951473236084
Epoch 440, val loss: 1.2512962818145752
Epoch 450, training loss: 634.4260864257812 = 1.1683177947998047 + 100.0 * 6.332577228546143
Epoch 450, val loss: 1.2317450046539307
Epoch 460, training loss: 634.0732421875 = 1.1440672874450684 + 100.0 * 6.329291820526123
Epoch 460, val loss: 1.2128944396972656
Epoch 470, training loss: 633.9466552734375 = 1.120461106300354 + 100.0 * 6.328261852264404
Epoch 470, val loss: 1.1947991847991943
Epoch 480, training loss: 633.7059326171875 = 1.097433090209961 + 100.0 * 6.326085090637207
Epoch 480, val loss: 1.177701711654663
Epoch 490, training loss: 633.1975708007812 = 1.075122594833374 + 100.0 * 6.321224212646484
Epoch 490, val loss: 1.1613116264343262
Epoch 500, training loss: 632.8790283203125 = 1.0536308288574219 + 100.0 * 6.318253993988037
Epoch 500, val loss: 1.1460530757904053
Epoch 510, training loss: 633.822509765625 = 1.0328055620193481 + 100.0 * 6.327897071838379
Epoch 510, val loss: 1.1313414573669434
Epoch 520, training loss: 632.4092407226562 = 1.0122077465057373 + 100.0 * 6.313970565795898
Epoch 520, val loss: 1.1176135540008545
Epoch 530, training loss: 632.2862548828125 = 0.9925727844238281 + 100.0 * 6.312936782836914
Epoch 530, val loss: 1.1047559976577759
Epoch 540, training loss: 631.88916015625 = 0.9738629460334778 + 100.0 * 6.309153079986572
Epoch 540, val loss: 1.0928529500961304
Epoch 550, training loss: 631.66162109375 = 0.9557576179504395 + 100.0 * 6.307058811187744
Epoch 550, val loss: 1.0817809104919434
Epoch 560, training loss: 631.4517211914062 = 0.9382029175758362 + 100.0 * 6.3051347732543945
Epoch 560, val loss: 1.0714987516403198
Epoch 570, training loss: 631.5719604492188 = 0.9210538268089294 + 100.0 * 6.306508541107178
Epoch 570, val loss: 1.0615860223770142
Epoch 580, training loss: 631.1954956054688 = 0.9042768478393555 + 100.0 * 6.302911758422852
Epoch 580, val loss: 1.0523077249526978
Epoch 590, training loss: 630.844970703125 = 0.8881375789642334 + 100.0 * 6.2995686531066895
Epoch 590, val loss: 1.043788194656372
Epoch 600, training loss: 630.6967163085938 = 0.872556746006012 + 100.0 * 6.29824161529541
Epoch 600, val loss: 1.0359675884246826
Epoch 610, training loss: 630.6768188476562 = 0.8573119640350342 + 100.0 * 6.2981953620910645
Epoch 610, val loss: 1.0284792184829712
Epoch 620, training loss: 630.3059692382812 = 0.8424417972564697 + 100.0 * 6.294634819030762
Epoch 620, val loss: 1.02157723903656
Epoch 630, training loss: 630.0596313476562 = 0.8278737664222717 + 100.0 * 6.2923173904418945
Epoch 630, val loss: 1.0151156187057495
Epoch 640, training loss: 629.8809814453125 = 0.813697338104248 + 100.0 * 6.290672779083252
Epoch 640, val loss: 1.0091248750686646
Epoch 650, training loss: 630.4010009765625 = 0.7997695803642273 + 100.0 * 6.2960124015808105
Epoch 650, val loss: 1.0034172534942627
Epoch 660, training loss: 629.69140625 = 0.7858930826187134 + 100.0 * 6.289055347442627
Epoch 660, val loss: 0.9979733824729919
Epoch 670, training loss: 629.3983154296875 = 0.7723613977432251 + 100.0 * 6.286259651184082
Epoch 670, val loss: 0.9927658438682556
Epoch 680, training loss: 629.2864990234375 = 0.759126603603363 + 100.0 * 6.285273551940918
Epoch 680, val loss: 0.988059937953949
Epoch 690, training loss: 629.5308227539062 = 0.7460265159606934 + 100.0 * 6.287847995758057
Epoch 690, val loss: 0.9834661483764648
Epoch 700, training loss: 628.9503784179688 = 0.7330824136734009 + 100.0 * 6.282173156738281
Epoch 700, val loss: 0.9792488217353821
Epoch 710, training loss: 628.8406372070312 = 0.7204000949859619 + 100.0 * 6.28120231628418
Epoch 710, val loss: 0.9753531217575073
Epoch 720, training loss: 628.6627197265625 = 0.7079387307167053 + 100.0 * 6.279547691345215
Epoch 720, val loss: 0.9717249870300293
Epoch 730, training loss: 629.323486328125 = 0.6957071423530579 + 100.0 * 6.286277770996094
Epoch 730, val loss: 0.9682148098945618
Epoch 740, training loss: 628.515625 = 0.6831536293029785 + 100.0 * 6.278324604034424
Epoch 740, val loss: 0.9646610021591187
Epoch 750, training loss: 628.3101196289062 = 0.6709765195846558 + 100.0 * 6.276391506195068
Epoch 750, val loss: 0.9614141583442688
Epoch 760, training loss: 628.1436767578125 = 0.6590768694877625 + 100.0 * 6.274846076965332
Epoch 760, val loss: 0.9586150050163269
Epoch 770, training loss: 628.0147705078125 = 0.6473625898361206 + 100.0 * 6.273674488067627
Epoch 770, val loss: 0.9559666514396667
Epoch 780, training loss: 628.5107421875 = 0.6357645392417908 + 100.0 * 6.278749942779541
Epoch 780, val loss: 0.953468382358551
Epoch 790, training loss: 627.9788208007812 = 0.6240590214729309 + 100.0 * 6.273547172546387
Epoch 790, val loss: 0.9506742358207703
Epoch 800, training loss: 627.8308715820312 = 0.6126211285591125 + 100.0 * 6.272182464599609
Epoch 800, val loss: 0.948430597782135
Epoch 810, training loss: 627.5723876953125 = 0.6013432741165161 + 100.0 * 6.269710540771484
Epoch 810, val loss: 0.9462641477584839
Epoch 820, training loss: 627.7545166015625 = 0.5902525186538696 + 100.0 * 6.271642684936523
Epoch 820, val loss: 0.9441852569580078
Epoch 830, training loss: 627.55908203125 = 0.5791361331939697 + 100.0 * 6.26979923248291
Epoch 830, val loss: 0.9423497915267944
Epoch 840, training loss: 627.2866821289062 = 0.568239152431488 + 100.0 * 6.267184257507324
Epoch 840, val loss: 0.9405472278594971
Epoch 850, training loss: 627.1317138671875 = 0.5575231909751892 + 100.0 * 6.265742301940918
Epoch 850, val loss: 0.9390340447425842
Epoch 860, training loss: 627.28173828125 = 0.5470007658004761 + 100.0 * 6.26734733581543
Epoch 860, val loss: 0.9376446604728699
Epoch 870, training loss: 627.5790405273438 = 0.5363500118255615 + 100.0 * 6.2704267501831055
Epoch 870, val loss: 0.9359796047210693
Epoch 880, training loss: 626.8516235351562 = 0.5258415341377258 + 100.0 * 6.26325798034668
Epoch 880, val loss: 0.9345252513885498
Epoch 890, training loss: 626.6785888671875 = 0.5157281160354614 + 100.0 * 6.2616286277771
Epoch 890, val loss: 0.9333716034889221
Epoch 900, training loss: 626.5824584960938 = 0.5057770609855652 + 100.0 * 6.260766506195068
Epoch 900, val loss: 0.9324397444725037
Epoch 910, training loss: 626.4872436523438 = 0.4959755837917328 + 100.0 * 6.259912490844727
Epoch 910, val loss: 0.9315827488899231
Epoch 920, training loss: 627.2446899414062 = 0.4862514138221741 + 100.0 * 6.267584323883057
Epoch 920, val loss: 0.9307148456573486
Epoch 930, training loss: 626.7537231445312 = 0.4765869677066803 + 100.0 * 6.2627716064453125
Epoch 930, val loss: 0.9296447038650513
Epoch 940, training loss: 626.2718505859375 = 0.46707814931869507 + 100.0 * 6.258047580718994
Epoch 940, val loss: 0.9289463758468628
Epoch 950, training loss: 626.1539916992188 = 0.4578312635421753 + 100.0 * 6.256961822509766
Epoch 950, val loss: 0.928528368473053
Epoch 960, training loss: 626.1841430664062 = 0.4488045573234558 + 100.0 * 6.257353782653809
Epoch 960, val loss: 0.9281768798828125
Epoch 970, training loss: 626.1185913085938 = 0.43986964225769043 + 100.0 * 6.256787300109863
Epoch 970, val loss: 0.927952766418457
Epoch 980, training loss: 626.0070190429688 = 0.43099817633628845 + 100.0 * 6.255760669708252
Epoch 980, val loss: 0.927622377872467
Epoch 990, training loss: 625.8952026367188 = 0.4223141372203827 + 100.0 * 6.254729270935059
Epoch 990, val loss: 0.927614152431488
Epoch 1000, training loss: 625.7623291015625 = 0.4139000475406647 + 100.0 * 6.253484725952148
Epoch 1000, val loss: 0.9277283549308777
Epoch 1010, training loss: 625.974609375 = 0.405623197555542 + 100.0 * 6.25568962097168
Epoch 1010, val loss: 0.9279809594154358
Epoch 1020, training loss: 625.5780029296875 = 0.3974277973175049 + 100.0 * 6.251805782318115
Epoch 1020, val loss: 0.9283121228218079
Epoch 1030, training loss: 625.510986328125 = 0.3894375264644623 + 100.0 * 6.251214981079102
Epoch 1030, val loss: 0.9288349747657776
Epoch 1040, training loss: 626.349609375 = 0.3816118836402893 + 100.0 * 6.259680271148682
Epoch 1040, val loss: 0.9295192956924438
Epoch 1050, training loss: 625.4053344726562 = 0.37368258833885193 + 100.0 * 6.250316619873047
Epoch 1050, val loss: 0.9299691915512085
Epoch 1060, training loss: 625.3849487304688 = 0.36609947681427 + 100.0 * 6.250188827514648
Epoch 1060, val loss: 0.9308587908744812
Epoch 1070, training loss: 625.2010498046875 = 0.35875383019447327 + 100.0 * 6.248423099517822
Epoch 1070, val loss: 0.9319735169410706
Epoch 1080, training loss: 625.41748046875 = 0.3515593111515045 + 100.0 * 6.250658988952637
Epoch 1080, val loss: 0.9331399202346802
Epoch 1090, training loss: 625.1038208007812 = 0.34437769651412964 + 100.0 * 6.247594356536865
Epoch 1090, val loss: 0.934394359588623
Epoch 1100, training loss: 625.2437744140625 = 0.33738499879837036 + 100.0 * 6.249063491821289
Epoch 1100, val loss: 0.9356482625007629
Epoch 1110, training loss: 625.1024169921875 = 0.3304876685142517 + 100.0 * 6.2477192878723145
Epoch 1110, val loss: 0.9370729923248291
Epoch 1120, training loss: 624.9244384765625 = 0.32378673553466797 + 100.0 * 6.246006488800049
Epoch 1120, val loss: 0.9385971426963806
Epoch 1130, training loss: 624.8944702148438 = 0.3172820806503296 + 100.0 * 6.245771884918213
Epoch 1130, val loss: 0.9403919577598572
Epoch 1140, training loss: 625.308349609375 = 0.31086376309394836 + 100.0 * 6.249974727630615
Epoch 1140, val loss: 0.942142903804779
Epoch 1150, training loss: 624.9730834960938 = 0.30448606610298157 + 100.0 * 6.246685981750488
Epoch 1150, val loss: 0.9439328908920288
Epoch 1160, training loss: 624.7058715820312 = 0.29829180240631104 + 100.0 * 6.244075775146484
Epoch 1160, val loss: 0.945923388004303
Epoch 1170, training loss: 624.6702270507812 = 0.29227355122566223 + 100.0 * 6.243779182434082
Epoch 1170, val loss: 0.9480636119842529
Epoch 1180, training loss: 624.6240844726562 = 0.2863776385784149 + 100.0 * 6.243377208709717
Epoch 1180, val loss: 0.9503738880157471
Epoch 1190, training loss: 624.7763061523438 = 0.280596524477005 + 100.0 * 6.244957447052002
Epoch 1190, val loss: 0.9527686238288879
Epoch 1200, training loss: 624.7245483398438 = 0.2748253047466278 + 100.0 * 6.244496822357178
Epoch 1200, val loss: 0.9550133347511292
Epoch 1210, training loss: 624.4601440429688 = 0.2692086100578308 + 100.0 * 6.241909027099609
Epoch 1210, val loss: 0.957392692565918
Epoch 1220, training loss: 624.3551635742188 = 0.2638048827648163 + 100.0 * 6.2409138679504395
Epoch 1220, val loss: 0.9602001309394836
Epoch 1230, training loss: 624.286376953125 = 0.2585456669330597 + 100.0 * 6.240278244018555
Epoch 1230, val loss: 0.9631520509719849
Epoch 1240, training loss: 624.938232421875 = 0.25341907143592834 + 100.0 * 6.246848106384277
Epoch 1240, val loss: 0.966163694858551
Epoch 1250, training loss: 624.4287719726562 = 0.24816682934761047 + 100.0 * 6.2418060302734375
Epoch 1250, val loss: 0.968826174736023
Epoch 1260, training loss: 624.1752319335938 = 0.24320168793201447 + 100.0 * 6.239319801330566
Epoch 1260, val loss: 0.9719372987747192
Epoch 1270, training loss: 624.2014770507812 = 0.23832948505878448 + 100.0 * 6.239631175994873
Epoch 1270, val loss: 0.9751393795013428
Epoch 1280, training loss: 624.1219482421875 = 0.233548104763031 + 100.0 * 6.238883972167969
Epoch 1280, val loss: 0.9784508347511292
Epoch 1290, training loss: 624.2149658203125 = 0.2288651168346405 + 100.0 * 6.239860534667969
Epoch 1290, val loss: 0.981814444065094
Epoch 1300, training loss: 624.0903930664062 = 0.2242560237646103 + 100.0 * 6.238661766052246
Epoch 1300, val loss: 0.9850339293479919
Epoch 1310, training loss: 623.9310913085938 = 0.21975556015968323 + 100.0 * 6.2371134757995605
Epoch 1310, val loss: 0.9884451627731323
Epoch 1320, training loss: 623.8486938476562 = 0.21538060903549194 + 100.0 * 6.236332893371582
Epoch 1320, val loss: 0.9921186566352844
Epoch 1330, training loss: 624.479736328125 = 0.21114231646060944 + 100.0 * 6.2426862716674805
Epoch 1330, val loss: 0.9956834316253662
Epoch 1340, training loss: 624.0452880859375 = 0.20676739513874054 + 100.0 * 6.238385200500488
Epoch 1340, val loss: 0.999153196811676
Epoch 1350, training loss: 623.7855224609375 = 0.20261992514133453 + 100.0 * 6.235828876495361
Epoch 1350, val loss: 1.0027387142181396
Epoch 1360, training loss: 623.6381225585938 = 0.19860467314720154 + 100.0 * 6.2343950271606445
Epoch 1360, val loss: 1.006778597831726
Epoch 1370, training loss: 623.5786743164062 = 0.19472911953926086 + 100.0 * 6.233839511871338
Epoch 1370, val loss: 1.0107783079147339
Epoch 1380, training loss: 623.6920166015625 = 0.19091568887233734 + 100.0 * 6.235011100769043
Epoch 1380, val loss: 1.0147398710250854
Epoch 1390, training loss: 623.7401123046875 = 0.18707157671451569 + 100.0 * 6.235530376434326
Epoch 1390, val loss: 1.0187870264053345
Epoch 1400, training loss: 623.7120971679688 = 0.1832820177078247 + 100.0 * 6.235288143157959
Epoch 1400, val loss: 1.0222479104995728
Epoch 1410, training loss: 623.4378662109375 = 0.1796722114086151 + 100.0 * 6.232582092285156
Epoch 1410, val loss: 1.0264053344726562
Epoch 1420, training loss: 623.3880615234375 = 0.17620353400707245 + 100.0 * 6.232118606567383
Epoch 1420, val loss: 1.0308481454849243
Epoch 1430, training loss: 623.4022216796875 = 0.17279554903507233 + 100.0 * 6.232294082641602
Epoch 1430, val loss: 1.0352003574371338
Epoch 1440, training loss: 623.6939086914062 = 0.1694299578666687 + 100.0 * 6.2352447509765625
Epoch 1440, val loss: 1.0392428636550903
Epoch 1450, training loss: 623.65087890625 = 0.16607140004634857 + 100.0 * 6.2348480224609375
Epoch 1450, val loss: 1.0433491468429565
Epoch 1460, training loss: 623.333251953125 = 0.16278496384620667 + 100.0 * 6.2317047119140625
Epoch 1460, val loss: 1.0477731227874756
Epoch 1470, training loss: 623.295166015625 = 0.1596205085515976 + 100.0 * 6.231355667114258
Epoch 1470, val loss: 1.0519654750823975
Epoch 1480, training loss: 623.1817016601562 = 0.15653349459171295 + 100.0 * 6.230251312255859
Epoch 1480, val loss: 1.0563384294509888
Epoch 1490, training loss: 623.1563720703125 = 0.15354090929031372 + 100.0 * 6.23002815246582
Epoch 1490, val loss: 1.060765266418457
Epoch 1500, training loss: 623.7666015625 = 0.15061651170253754 + 100.0 * 6.236159801483154
Epoch 1500, val loss: 1.0651065111160278
Epoch 1510, training loss: 623.4208984375 = 0.14760936796665192 + 100.0 * 6.232733249664307
Epoch 1510, val loss: 1.0696334838867188
Epoch 1520, training loss: 623.1058959960938 = 0.14474135637283325 + 100.0 * 6.229611873626709
Epoch 1520, val loss: 1.0739800930023193
Epoch 1530, training loss: 622.9727172851562 = 0.14196987450122833 + 100.0 * 6.228307723999023
Epoch 1530, val loss: 1.0784832239151
Epoch 1540, training loss: 623.0737915039062 = 0.13928337395191193 + 100.0 * 6.229344844818115
Epoch 1540, val loss: 1.0831421613693237
Epoch 1550, training loss: 623.1549072265625 = 0.13659624755382538 + 100.0 * 6.230183124542236
Epoch 1550, val loss: 1.0877633094787598
Epoch 1560, training loss: 623.074951171875 = 0.13394813239574432 + 100.0 * 6.229409694671631
Epoch 1560, val loss: 1.0924513339996338
Epoch 1570, training loss: 622.8630981445312 = 0.13139250874519348 + 100.0 * 6.227316856384277
Epoch 1570, val loss: 1.096766710281372
Epoch 1580, training loss: 622.869873046875 = 0.12890365719795227 + 100.0 * 6.227409839630127
Epoch 1580, val loss: 1.1015119552612305
Epoch 1590, training loss: 623.7294311523438 = 0.12645813822746277 + 100.0 * 6.236029624938965
Epoch 1590, val loss: 1.106176495552063
Epoch 1600, training loss: 622.965576171875 = 0.12398864328861237 + 100.0 * 6.2284159660339355
Epoch 1600, val loss: 1.11034095287323
Epoch 1610, training loss: 622.695556640625 = 0.12160338461399078 + 100.0 * 6.2257399559021
Epoch 1610, val loss: 1.1153225898742676
Epoch 1620, training loss: 622.67333984375 = 0.11934495717287064 + 100.0 * 6.2255401611328125
Epoch 1620, val loss: 1.1200200319290161
Epoch 1630, training loss: 622.61328125 = 0.11713016778230667 + 100.0 * 6.224961280822754
Epoch 1630, val loss: 1.125004768371582
Epoch 1640, training loss: 622.611083984375 = 0.11495137214660645 + 100.0 * 6.224961280822754
Epoch 1640, val loss: 1.129884958267212
Epoch 1650, training loss: 623.59814453125 = 0.11279486119747162 + 100.0 * 6.234853267669678
Epoch 1650, val loss: 1.1348960399627686
Epoch 1660, training loss: 622.5552368164062 = 0.11059839278459549 + 100.0 * 6.2244462966918945
Epoch 1660, val loss: 1.1390817165374756
Epoch 1670, training loss: 622.6064453125 = 0.10852833092212677 + 100.0 * 6.224979400634766
Epoch 1670, val loss: 1.1438453197479248
Epoch 1680, training loss: 622.4876708984375 = 0.10651803016662598 + 100.0 * 6.223811626434326
Epoch 1680, val loss: 1.148680567741394
Epoch 1690, training loss: 622.9465942382812 = 0.10456220805644989 + 100.0 * 6.228420257568359
Epoch 1690, val loss: 1.1533517837524414
Epoch 1700, training loss: 622.6122436523438 = 0.10257479548454285 + 100.0 * 6.225096702575684
Epoch 1700, val loss: 1.158521294593811
Epoch 1710, training loss: 622.4873046875 = 0.10063755512237549 + 100.0 * 6.2238664627075195
Epoch 1710, val loss: 1.1627883911132812
Epoch 1720, training loss: 622.3963623046875 = 0.0987904816865921 + 100.0 * 6.222975730895996
Epoch 1720, val loss: 1.1679717302322388
Epoch 1730, training loss: 622.369140625 = 0.096991166472435 + 100.0 * 6.222721576690674
Epoch 1730, val loss: 1.1726945638656616
Epoch 1740, training loss: 622.89306640625 = 0.09523986279964447 + 100.0 * 6.227978706359863
Epoch 1740, val loss: 1.1774486303329468
Epoch 1750, training loss: 622.6302490234375 = 0.09342925250530243 + 100.0 * 6.225368022918701
Epoch 1750, val loss: 1.181898832321167
Epoch 1760, training loss: 622.3946533203125 = 0.09169410914182663 + 100.0 * 6.223029613494873
Epoch 1760, val loss: 1.1867398023605347
Epoch 1770, training loss: 622.2940673828125 = 0.08999662846326828 + 100.0 * 6.222040176391602
Epoch 1770, val loss: 1.1917494535446167
Epoch 1780, training loss: 622.221923828125 = 0.08837225288152695 + 100.0 * 6.221335411071777
Epoch 1780, val loss: 1.1966978311538696
Epoch 1790, training loss: 622.4458618164062 = 0.0867835134267807 + 100.0 * 6.223590850830078
Epoch 1790, val loss: 1.2018988132476807
Epoch 1800, training loss: 622.1480712890625 = 0.08518217504024506 + 100.0 * 6.22062873840332
Epoch 1800, val loss: 1.205926537513733
Epoch 1810, training loss: 622.2052612304688 = 0.08363284915685654 + 100.0 * 6.221216678619385
Epoch 1810, val loss: 1.2108113765716553
Epoch 1820, training loss: 622.3850708007812 = 0.08212149143218994 + 100.0 * 6.223029613494873
Epoch 1820, val loss: 1.2153092622756958
Epoch 1830, training loss: 622.31982421875 = 0.08060535788536072 + 100.0 * 6.2223920822143555
Epoch 1830, val loss: 1.2202355861663818
Epoch 1840, training loss: 622.35595703125 = 0.07914659380912781 + 100.0 * 6.2227678298950195
Epoch 1840, val loss: 1.2249705791473389
Epoch 1850, training loss: 622.0724487304688 = 0.07770989835262299 + 100.0 * 6.219947814941406
Epoch 1850, val loss: 1.2295860052108765
Epoch 1860, training loss: 622.03857421875 = 0.07632283866405487 + 100.0 * 6.219622611999512
Epoch 1860, val loss: 1.234626054763794
Epoch 1870, training loss: 622.0881958007812 = 0.07498156279325485 + 100.0 * 6.220132350921631
Epoch 1870, val loss: 1.2391690015792847
Epoch 1880, training loss: 622.1835327148438 = 0.0736452266573906 + 100.0 * 6.22109842300415
Epoch 1880, val loss: 1.2438549995422363
Epoch 1890, training loss: 622.0867919921875 = 0.0723314955830574 + 100.0 * 6.220144748687744
Epoch 1890, val loss: 1.2487579584121704
Epoch 1900, training loss: 622.1339111328125 = 0.07105261087417603 + 100.0 * 6.22062873840332
Epoch 1900, val loss: 1.2531808614730835
Epoch 1910, training loss: 621.9175415039062 = 0.06979414820671082 + 100.0 * 6.218477249145508
Epoch 1910, val loss: 1.2576509714126587
Epoch 1920, training loss: 622.3681030273438 = 0.06858131289482117 + 100.0 * 6.222995281219482
Epoch 1920, val loss: 1.2625670433044434
Epoch 1930, training loss: 622.048583984375 = 0.06732043623924255 + 100.0 * 6.219812870025635
Epoch 1930, val loss: 1.2669428586959839
Epoch 1940, training loss: 621.8124389648438 = 0.06614077091217041 + 100.0 * 6.21746301651001
Epoch 1940, val loss: 1.271285891532898
Epoch 1950, training loss: 621.819580078125 = 0.0649971142411232 + 100.0 * 6.217545509338379
Epoch 1950, val loss: 1.2758406400680542
Epoch 1960, training loss: 622.2234497070312 = 0.06389336287975311 + 100.0 * 6.221595764160156
Epoch 1960, val loss: 1.2803139686584473
Epoch 1970, training loss: 621.9530029296875 = 0.06274691224098206 + 100.0 * 6.218902587890625
Epoch 1970, val loss: 1.2854034900665283
Epoch 1980, training loss: 621.7904663085938 = 0.06166573613882065 + 100.0 * 6.217288494110107
Epoch 1980, val loss: 1.289402961730957
Epoch 1990, training loss: 621.7872924804688 = 0.06061125174164772 + 100.0 * 6.217267036437988
Epoch 1990, val loss: 1.294147253036499
Epoch 2000, training loss: 622.583984375 = 0.059593744575977325 + 100.0 * 6.225244045257568
Epoch 2000, val loss: 1.2980842590332031
Epoch 2010, training loss: 621.9065551757812 = 0.058491770178079605 + 100.0 * 6.218481063842773
Epoch 2010, val loss: 1.3022234439849854
Epoch 2020, training loss: 621.65625 = 0.057488907128572464 + 100.0 * 6.215987682342529
Epoch 2020, val loss: 1.3070778846740723
Epoch 2030, training loss: 621.59033203125 = 0.05652554705739021 + 100.0 * 6.215338230133057
Epoch 2030, val loss: 1.3115005493164062
Epoch 2040, training loss: 621.5753173828125 = 0.055590011179447174 + 100.0 * 6.2151970863342285
Epoch 2040, val loss: 1.3159661293029785
Epoch 2050, training loss: 622.0883178710938 = 0.05466900020837784 + 100.0 * 6.220336437225342
Epoch 2050, val loss: 1.3205821514129639
Epoch 2060, training loss: 621.75439453125 = 0.05372914671897888 + 100.0 * 6.217006683349609
Epoch 2060, val loss: 1.3244154453277588
Epoch 2070, training loss: 621.6469116210938 = 0.05280591920018196 + 100.0 * 6.215941429138184
Epoch 2070, val loss: 1.3286336660385132
Epoch 2080, training loss: 621.5739135742188 = 0.05193528160452843 + 100.0 * 6.215219974517822
Epoch 2080, val loss: 1.3328841924667358
Epoch 2090, training loss: 621.8289794921875 = 0.051083482801914215 + 100.0 * 6.217779159545898
Epoch 2090, val loss: 1.3375085592269897
Epoch 2100, training loss: 621.7222290039062 = 0.050214577466249466 + 100.0 * 6.216720104217529
Epoch 2100, val loss: 1.340882420539856
Epoch 2110, training loss: 621.4468994140625 = 0.04935365915298462 + 100.0 * 6.213975429534912
Epoch 2110, val loss: 1.3454159498214722
Epoch 2120, training loss: 621.4177856445312 = 0.04854368790984154 + 100.0 * 6.213692665100098
Epoch 2120, val loss: 1.3492207527160645
Epoch 2130, training loss: 621.4251098632812 = 0.04776306822896004 + 100.0 * 6.213773250579834
Epoch 2130, val loss: 1.3536250591278076
Epoch 2140, training loss: 621.8573608398438 = 0.0469973050057888 + 100.0 * 6.218103408813477
Epoch 2140, val loss: 1.3577827215194702
Epoch 2150, training loss: 621.7935791015625 = 0.04620469734072685 + 100.0 * 6.217473983764648
Epoch 2150, val loss: 1.3618953227996826
Epoch 2160, training loss: 621.4347534179688 = 0.04544108733534813 + 100.0 * 6.213892936706543
Epoch 2160, val loss: 1.3654389381408691
Epoch 2170, training loss: 621.3926391601562 = 0.044708721339702606 + 100.0 * 6.213479042053223
Epoch 2170, val loss: 1.3696941137313843
Epoch 2180, training loss: 621.542236328125 = 0.04400640353560448 + 100.0 * 6.214982032775879
Epoch 2180, val loss: 1.3740448951721191
Epoch 2190, training loss: 621.419921875 = 0.04328909516334534 + 100.0 * 6.213766574859619
Epoch 2190, val loss: 1.3777364492416382
Epoch 2200, training loss: 621.42724609375 = 0.0425923690199852 + 100.0 * 6.213846206665039
Epoch 2200, val loss: 1.381827712059021
Epoch 2210, training loss: 621.3385009765625 = 0.04191196337342262 + 100.0 * 6.212965488433838
Epoch 2210, val loss: 1.3858340978622437
Epoch 2220, training loss: 621.2843017578125 = 0.04125801473855972 + 100.0 * 6.212430477142334
Epoch 2220, val loss: 1.3895052671432495
Epoch 2230, training loss: 622.0116577148438 = 0.04062843322753906 + 100.0 * 6.219710350036621
Epoch 2230, val loss: 1.3937984704971313
Epoch 2240, training loss: 621.4513549804688 = 0.03995637968182564 + 100.0 * 6.214113712310791
Epoch 2240, val loss: 1.3965082168579102
Epoch 2250, training loss: 621.38818359375 = 0.039335064589977264 + 100.0 * 6.213488578796387
Epoch 2250, val loss: 1.4009191989898682
Epoch 2260, training loss: 621.5447998046875 = 0.03872686251997948 + 100.0 * 6.215060234069824
Epoch 2260, val loss: 1.4042726755142212
Epoch 2270, training loss: 621.192626953125 = 0.038102053105831146 + 100.0 * 6.211545467376709
Epoch 2270, val loss: 1.408957600593567
Epoch 2280, training loss: 621.149658203125 = 0.03752627968788147 + 100.0 * 6.211121082305908
Epoch 2280, val loss: 1.412597894668579
Epoch 2290, training loss: 621.4754638671875 = 0.03697046637535095 + 100.0 * 6.214385032653809
Epoch 2290, val loss: 1.4168062210083008
Epoch 2300, training loss: 621.1468505859375 = 0.036375731229782104 + 100.0 * 6.211104869842529
Epoch 2300, val loss: 1.4197708368301392
Epoch 2310, training loss: 621.1090087890625 = 0.03581845015287399 + 100.0 * 6.2107319831848145
Epoch 2310, val loss: 1.4235074520111084
Epoch 2320, training loss: 621.39794921875 = 0.03528160974383354 + 100.0 * 6.213626861572266
Epoch 2320, val loss: 1.4270859956741333
Epoch 2330, training loss: 621.1279296875 = 0.034738339483737946 + 100.0 * 6.210931777954102
Epoch 2330, val loss: 1.431272268295288
Epoch 2340, training loss: 621.1126708984375 = 0.03421585261821747 + 100.0 * 6.210784435272217
Epoch 2340, val loss: 1.4346741437911987
Epoch 2350, training loss: 621.059814453125 = 0.033711694180965424 + 100.0 * 6.210261344909668
Epoch 2350, val loss: 1.4383147954940796
Epoch 2360, training loss: 621.2313842773438 = 0.03321506828069687 + 100.0 * 6.211981773376465
Epoch 2360, val loss: 1.4420878887176514
Epoch 2370, training loss: 621.4382934570312 = 0.0327138714492321 + 100.0 * 6.21405553817749
Epoch 2370, val loss: 1.44571053981781
Epoch 2380, training loss: 621.2219848632812 = 0.03222179040312767 + 100.0 * 6.211897850036621
Epoch 2380, val loss: 1.4481793642044067
Epoch 2390, training loss: 621.08837890625 = 0.03173771873116493 + 100.0 * 6.210566520690918
Epoch 2390, val loss: 1.4526206254959106
Epoch 2400, training loss: 621.0049438476562 = 0.03128122538328171 + 100.0 * 6.209736347198486
Epoch 2400, val loss: 1.455605149269104
Epoch 2410, training loss: 620.9598388671875 = 0.03082948736846447 + 100.0 * 6.209290027618408
Epoch 2410, val loss: 1.4596164226531982
Epoch 2420, training loss: 620.9607543945312 = 0.03039042465388775 + 100.0 * 6.209303855895996
Epoch 2420, val loss: 1.4629329442977905
Epoch 2430, training loss: 621.2323608398438 = 0.029966184869408607 + 100.0 * 6.212024211883545
Epoch 2430, val loss: 1.466247797012329
Epoch 2440, training loss: 621.1033325195312 = 0.029521940276026726 + 100.0 * 6.210738182067871
Epoch 2440, val loss: 1.4696831703186035
Epoch 2450, training loss: 620.9239501953125 = 0.029084691777825356 + 100.0 * 6.208948135375977
Epoch 2450, val loss: 1.4729087352752686
Epoch 2460, training loss: 620.8795166015625 = 0.028675900772213936 + 100.0 * 6.208508014678955
Epoch 2460, val loss: 1.476474404335022
Epoch 2470, training loss: 621.0966796875 = 0.028291266411542892 + 100.0 * 6.210683345794678
Epoch 2470, val loss: 1.4794212579727173
Epoch 2480, training loss: 621.0762329101562 = 0.027884144335985184 + 100.0 * 6.210483551025391
Epoch 2480, val loss: 1.4828805923461914
Epoch 2490, training loss: 620.96142578125 = 0.027478832751512527 + 100.0 * 6.209339618682861
Epoch 2490, val loss: 1.485975980758667
Epoch 2500, training loss: 620.89013671875 = 0.02710622176527977 + 100.0 * 6.208630084991455
Epoch 2500, val loss: 1.4890128374099731
Epoch 2510, training loss: 620.86572265625 = 0.026735570281744003 + 100.0 * 6.208389759063721
Epoch 2510, val loss: 1.4923555850982666
Epoch 2520, training loss: 620.8467407226562 = 0.026374656707048416 + 100.0 * 6.2082037925720215
Epoch 2520, val loss: 1.4954642057418823
Epoch 2530, training loss: 620.919677734375 = 0.026019949465990067 + 100.0 * 6.20893669128418
Epoch 2530, val loss: 1.4986977577209473
Epoch 2540, training loss: 620.9556274414062 = 0.025660334154963493 + 100.0 * 6.2093000411987305
Epoch 2540, val loss: 1.5025272369384766
Epoch 2550, training loss: 620.8644409179688 = 0.025314485654234886 + 100.0 * 6.208391189575195
Epoch 2550, val loss: 1.505124807357788
Epoch 2560, training loss: 620.8512573242188 = 0.024969732388854027 + 100.0 * 6.208262920379639
Epoch 2560, val loss: 1.5079619884490967
Epoch 2570, training loss: 620.7952270507812 = 0.024627897888422012 + 100.0 * 6.207705974578857
Epoch 2570, val loss: 1.5114736557006836
Epoch 2580, training loss: 620.8759155273438 = 0.024304406717419624 + 100.0 * 6.2085161209106445
Epoch 2580, val loss: 1.5152583122253418
Epoch 2590, training loss: 620.6854858398438 = 0.023978853598237038 + 100.0 * 6.206614971160889
Epoch 2590, val loss: 1.5178718566894531
Epoch 2600, training loss: 620.7308349609375 = 0.02366764284670353 + 100.0 * 6.207071304321289
Epoch 2600, val loss: 1.5203609466552734
Epoch 2610, training loss: 620.8382568359375 = 0.02335997484624386 + 100.0 * 6.208148956298828
Epoch 2610, val loss: 1.523801326751709
Epoch 2620, training loss: 620.88525390625 = 0.023052498698234558 + 100.0 * 6.208621978759766
Epoch 2620, val loss: 1.527099609375
Epoch 2630, training loss: 620.9874877929688 = 0.022744297981262207 + 100.0 * 6.209647178649902
Epoch 2630, val loss: 1.5292015075683594
Epoch 2640, training loss: 620.6798706054688 = 0.02243679203093052 + 100.0 * 6.206573963165283
Epoch 2640, val loss: 1.532814860343933
Epoch 2650, training loss: 620.6004638671875 = 0.022150807082653046 + 100.0 * 6.205782890319824
Epoch 2650, val loss: 1.5357881784439087
Epoch 2660, training loss: 620.8944702148438 = 0.021874498575925827 + 100.0 * 6.208725929260254
Epoch 2660, val loss: 1.5389760732650757
Epoch 2670, training loss: 620.525390625 = 0.0215917956084013 + 100.0 * 6.205037593841553
Epoch 2670, val loss: 1.5414701700210571
Epoch 2680, training loss: 620.5303344726562 = 0.021324802190065384 + 100.0 * 6.205090045928955
Epoch 2680, val loss: 1.544224500656128
Epoch 2690, training loss: 620.528076171875 = 0.02105664648115635 + 100.0 * 6.2050700187683105
Epoch 2690, val loss: 1.5476828813552856
Epoch 2700, training loss: 621.5656127929688 = 0.020803174003958702 + 100.0 * 6.215447902679443
Epoch 2700, val loss: 1.5508122444152832
Epoch 2710, training loss: 620.8621215820312 = 0.02053295262157917 + 100.0 * 6.208415985107422
Epoch 2710, val loss: 1.552935004234314
Epoch 2720, training loss: 620.5177001953125 = 0.02026664838194847 + 100.0 * 6.20497465133667
Epoch 2720, val loss: 1.5563329458236694
Epoch 2730, training loss: 620.4920654296875 = 0.020027192309498787 + 100.0 * 6.204720497131348
Epoch 2730, val loss: 1.5591614246368408
Epoch 2740, training loss: 620.76416015625 = 0.01979213021695614 + 100.0 * 6.207443714141846
Epoch 2740, val loss: 1.5616676807403564
Epoch 2750, training loss: 620.7266235351562 = 0.019550079479813576 + 100.0 * 6.207070827484131
Epoch 2750, val loss: 1.5640071630477905
Epoch 2760, training loss: 620.431884765625 = 0.01930001936852932 + 100.0 * 6.204125881195068
Epoch 2760, val loss: 1.5674364566802979
Epoch 2770, training loss: 620.4450073242188 = 0.019069110974669456 + 100.0 * 6.204259872436523
Epoch 2770, val loss: 1.5703554153442383
Epoch 2780, training loss: 620.5145874023438 = 0.01884644664824009 + 100.0 * 6.204957008361816
Epoch 2780, val loss: 1.5730949640274048
Epoch 2790, training loss: 620.6312866210938 = 0.01862502098083496 + 100.0 * 6.206126689910889
Epoch 2790, val loss: 1.5755046606063843
Epoch 2800, training loss: 620.4118041992188 = 0.0184011273086071 + 100.0 * 6.203934192657471
Epoch 2800, val loss: 1.578283667564392
Epoch 2810, training loss: 620.4494018554688 = 0.018187200650572777 + 100.0 * 6.204311847686768
Epoch 2810, val loss: 1.5815972089767456
Epoch 2820, training loss: 620.9751586914062 = 0.017979677766561508 + 100.0 * 6.209571838378906
Epoch 2820, val loss: 1.5843461751937866
Epoch 2830, training loss: 620.460205078125 = 0.017740432173013687 + 100.0 * 6.2044243812561035
Epoch 2830, val loss: 1.5867220163345337
Epoch 2840, training loss: 620.3016357421875 = 0.01753237470984459 + 100.0 * 6.202841281890869
Epoch 2840, val loss: 1.5891153812408447
Epoch 2850, training loss: 620.2582397460938 = 0.017335684970021248 + 100.0 * 6.202409267425537
Epoch 2850, val loss: 1.5921943187713623
Epoch 2860, training loss: 620.2551879882812 = 0.017144925892353058 + 100.0 * 6.202380657196045
Epoch 2860, val loss: 1.5947703123092651
Epoch 2870, training loss: 620.5341186523438 = 0.016964368522167206 + 100.0 * 6.205171585083008
Epoch 2870, val loss: 1.5970491170883179
Epoch 2880, training loss: 620.5375366210938 = 0.01676713302731514 + 100.0 * 6.205207824707031
Epoch 2880, val loss: 1.5997228622436523
Epoch 2890, training loss: 620.2647094726562 = 0.016559939831495285 + 100.0 * 6.202481746673584
Epoch 2890, val loss: 1.602747917175293
Epoch 2900, training loss: 620.2052001953125 = 0.016375895589590073 + 100.0 * 6.201888561248779
Epoch 2900, val loss: 1.6054993867874146
Epoch 2910, training loss: 620.302978515625 = 0.016201674938201904 + 100.0 * 6.2028679847717285
Epoch 2910, val loss: 1.6083341836929321
Epoch 2920, training loss: 620.5939331054688 = 0.016023723408579826 + 100.0 * 6.205779075622559
Epoch 2920, val loss: 1.6110985279083252
Epoch 2930, training loss: 620.478515625 = 0.01584598980844021 + 100.0 * 6.204626560211182
Epoch 2930, val loss: 1.6127369403839111
Epoch 2940, training loss: 620.4366455078125 = 0.015667051076889038 + 100.0 * 6.204209804534912
Epoch 2940, val loss: 1.6159946918487549
Epoch 2950, training loss: 620.2009887695312 = 0.015488814562559128 + 100.0 * 6.201854705810547
Epoch 2950, val loss: 1.6181800365447998
Epoch 2960, training loss: 620.2001953125 = 0.01532600075006485 + 100.0 * 6.20184850692749
Epoch 2960, val loss: 1.6208282709121704
Epoch 2970, training loss: 620.2943115234375 = 0.015165986493229866 + 100.0 * 6.202791690826416
Epoch 2970, val loss: 1.6233683824539185
Epoch 2980, training loss: 620.5931396484375 = 0.015005737543106079 + 100.0 * 6.205780982971191
Epoch 2980, val loss: 1.625738501548767
Epoch 2990, training loss: 620.202392578125 = 0.014831497333943844 + 100.0 * 6.201875686645508
Epoch 2990, val loss: 1.6283780336380005
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8360569319978914
The final CL Acc:0.69630, 0.02181, The final GNN Acc:0.83676, 0.00066
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11580])
remove edge: torch.Size([2, 9534])
updated graph: torch.Size([2, 10558])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6460571289062 = 1.958909034729004 + 100.0 * 8.596871376037598
Epoch 0, val loss: 1.9667290449142456
Epoch 10, training loss: 861.577392578125 = 1.9495819807052612 + 100.0 * 8.596278190612793
Epoch 10, val loss: 1.9573752880096436
Epoch 20, training loss: 861.1072998046875 = 1.9380162954330444 + 100.0 * 8.591692924499512
Epoch 20, val loss: 1.9451090097427368
Epoch 30, training loss: 857.6621704101562 = 1.922676920890808 + 100.0 * 8.557394981384277
Epoch 30, val loss: 1.9282958507537842
Epoch 40, training loss: 837.5821533203125 = 1.903495192527771 + 100.0 * 8.356786727905273
Epoch 40, val loss: 1.9077939987182617
Epoch 50, training loss: 786.7244873046875 = 1.881829857826233 + 100.0 * 7.848426342010498
Epoch 50, val loss: 1.8856005668640137
Epoch 60, training loss: 745.703369140625 = 1.8675990104675293 + 100.0 * 7.438357353210449
Epoch 60, val loss: 1.8733528852462769
Epoch 70, training loss: 714.6585693359375 = 1.855401873588562 + 100.0 * 7.1280317306518555
Epoch 70, val loss: 1.8621671199798584
Epoch 80, training loss: 693.31396484375 = 1.8455231189727783 + 100.0 * 6.914684295654297
Epoch 80, val loss: 1.8529889583587646
Epoch 90, training loss: 682.8712158203125 = 1.835768461227417 + 100.0 * 6.810354709625244
Epoch 90, val loss: 1.8436466455459595
Epoch 100, training loss: 676.7326049804688 = 1.825508952140808 + 100.0 * 6.74907112121582
Epoch 100, val loss: 1.8338041305541992
Epoch 110, training loss: 671.92041015625 = 1.8155072927474976 + 100.0 * 6.701049327850342
Epoch 110, val loss: 1.8241658210754395
Epoch 120, training loss: 667.7242431640625 = 1.806205153465271 + 100.0 * 6.659180164337158
Epoch 120, val loss: 1.8152716159820557
Epoch 130, training loss: 663.9647216796875 = 1.7972522974014282 + 100.0 * 6.62167501449585
Epoch 130, val loss: 1.8066890239715576
Epoch 140, training loss: 660.7872924804688 = 1.7881836891174316 + 100.0 * 6.589990615844727
Epoch 140, val loss: 1.7981605529785156
Epoch 150, training loss: 658.4318237304688 = 1.7784782648086548 + 100.0 * 6.566533088684082
Epoch 150, val loss: 1.7891911268234253
Epoch 160, training loss: 656.009033203125 = 1.7681406736373901 + 100.0 * 6.5424089431762695
Epoch 160, val loss: 1.779802918434143
Epoch 170, training loss: 654.0748291015625 = 1.7571845054626465 + 100.0 * 6.523176193237305
Epoch 170, val loss: 1.770071268081665
Epoch 180, training loss: 652.3253784179688 = 1.7453755140304565 + 100.0 * 6.505800247192383
Epoch 180, val loss: 1.7597187757492065
Epoch 190, training loss: 650.8768310546875 = 1.7327029705047607 + 100.0 * 6.491440773010254
Epoch 190, val loss: 1.7487521171569824
Epoch 200, training loss: 649.7508544921875 = 1.719100832939148 + 100.0 * 6.480317115783691
Epoch 200, val loss: 1.7370332479476929
Epoch 210, training loss: 648.3687744140625 = 1.7043973207473755 + 100.0 * 6.466643810272217
Epoch 210, val loss: 1.7244465351104736
Epoch 220, training loss: 647.3057250976562 = 1.6886277198791504 + 100.0 * 6.45617151260376
Epoch 220, val loss: 1.7110103368759155
Epoch 230, training loss: 646.4197387695312 = 1.6717119216918945 + 100.0 * 6.447480201721191
Epoch 230, val loss: 1.6966359615325928
Epoch 240, training loss: 645.6947021484375 = 1.653591513633728 + 100.0 * 6.440411567687988
Epoch 240, val loss: 1.6812487840652466
Epoch 250, training loss: 644.7433471679688 = 1.6342508792877197 + 100.0 * 6.431090831756592
Epoch 250, val loss: 1.6649013757705688
Epoch 260, training loss: 643.888671875 = 1.6138678789138794 + 100.0 * 6.422747611999512
Epoch 260, val loss: 1.6476186513900757
Epoch 270, training loss: 643.5137939453125 = 1.5923473834991455 + 100.0 * 6.419214725494385
Epoch 270, val loss: 1.629400372505188
Epoch 280, training loss: 642.6483764648438 = 1.5699695348739624 + 100.0 * 6.4107842445373535
Epoch 280, val loss: 1.610466480255127
Epoch 290, training loss: 641.8770141601562 = 1.5467276573181152 + 100.0 * 6.4033026695251465
Epoch 290, val loss: 1.5908671617507935
Epoch 300, training loss: 641.60009765625 = 1.522825837135315 + 100.0 * 6.400772571563721
Epoch 300, val loss: 1.570787787437439
Epoch 310, training loss: 640.844970703125 = 1.4982500076293945 + 100.0 * 6.393467426300049
Epoch 310, val loss: 1.5501407384872437
Epoch 320, training loss: 640.2852783203125 = 1.4733357429504395 + 100.0 * 6.388119220733643
Epoch 320, val loss: 1.529359221458435
Epoch 330, training loss: 639.7694091796875 = 1.448180913925171 + 100.0 * 6.383212566375732
Epoch 330, val loss: 1.5084714889526367
Epoch 340, training loss: 639.2195434570312 = 1.4229785203933716 + 100.0 * 6.377965450286865
Epoch 340, val loss: 1.487751841545105
Epoch 350, training loss: 638.7323608398438 = 1.397753357887268 + 100.0 * 6.373345851898193
Epoch 350, val loss: 1.4671598672866821
Epoch 360, training loss: 638.367919921875 = 1.3727359771728516 + 100.0 * 6.3699517250061035
Epoch 360, val loss: 1.4469386339187622
Epoch 370, training loss: 638.0721435546875 = 1.3480079174041748 + 100.0 * 6.367240905761719
Epoch 370, val loss: 1.4271221160888672
Epoch 380, training loss: 637.9781494140625 = 1.323714017868042 + 100.0 * 6.366544246673584
Epoch 380, val loss: 1.408284068107605
Epoch 390, training loss: 637.1128540039062 = 1.2997562885284424 + 100.0 * 6.358131408691406
Epoch 390, val loss: 1.38963782787323
Epoch 400, training loss: 636.6165771484375 = 1.2763631343841553 + 100.0 * 6.353402137756348
Epoch 400, val loss: 1.371760606765747
Epoch 410, training loss: 636.2463989257812 = 1.2535496950149536 + 100.0 * 6.349928855895996
Epoch 410, val loss: 1.3546174764633179
Epoch 420, training loss: 636.1785278320312 = 1.231223464012146 + 100.0 * 6.349473476409912
Epoch 420, val loss: 1.3381390571594238
Epoch 430, training loss: 636.0740356445312 = 1.2093638181686401 + 100.0 * 6.348647117614746
Epoch 430, val loss: 1.3223536014556885
Epoch 440, training loss: 635.1947631835938 = 1.188023567199707 + 100.0 * 6.340067386627197
Epoch 440, val loss: 1.3071390390396118
Epoch 450, training loss: 634.9402465820312 = 1.1672731637954712 + 100.0 * 6.337729454040527
Epoch 450, val loss: 1.2927234172821045
Epoch 460, training loss: 634.5841674804688 = 1.147059440612793 + 100.0 * 6.334371566772461
Epoch 460, val loss: 1.279045581817627
Epoch 470, training loss: 634.73876953125 = 1.127234697341919 + 100.0 * 6.336114883422852
Epoch 470, val loss: 1.2658568620681763
Epoch 480, training loss: 634.4075317382812 = 1.1075674295425415 + 100.0 * 6.3329997062683105
Epoch 480, val loss: 1.2528406381607056
Epoch 490, training loss: 633.8517456054688 = 1.0883673429489136 + 100.0 * 6.327633857727051
Epoch 490, val loss: 1.2405915260314941
Epoch 500, training loss: 633.7822265625 = 1.0694435834884644 + 100.0 * 6.327127933502197
Epoch 500, val loss: 1.2287672758102417
Epoch 510, training loss: 633.4993286132812 = 1.050691843032837 + 100.0 * 6.324486255645752
Epoch 510, val loss: 1.2170289754867554
Epoch 520, training loss: 633.1332397460938 = 1.0321661233901978 + 100.0 * 6.321010589599609
Epoch 520, val loss: 1.205938696861267
Epoch 530, training loss: 633.07177734375 = 1.0137032270431519 + 100.0 * 6.32058048248291
Epoch 530, val loss: 1.19497549533844
Epoch 540, training loss: 632.7451782226562 = 0.9952623844146729 + 100.0 * 6.31749963760376
Epoch 540, val loss: 1.184356927871704
Epoch 550, training loss: 632.5451049804688 = 0.9769014120101929 + 100.0 * 6.3156819343566895
Epoch 550, val loss: 1.1735223531723022
Epoch 560, training loss: 632.1212768554688 = 0.9585968255996704 + 100.0 * 6.311626434326172
Epoch 560, val loss: 1.163280963897705
Epoch 570, training loss: 631.9314575195312 = 0.9403808116912842 + 100.0 * 6.309910774230957
Epoch 570, val loss: 1.15320885181427
Epoch 580, training loss: 632.0933837890625 = 0.9221943020820618 + 100.0 * 6.311712265014648
Epoch 580, val loss: 1.1432219743728638
Epoch 590, training loss: 631.7449340820312 = 0.9038322567939758 + 100.0 * 6.308411121368408
Epoch 590, val loss: 1.1330347061157227
Epoch 600, training loss: 631.340576171875 = 0.8856594562530518 + 100.0 * 6.304548740386963
Epoch 600, val loss: 1.1233614683151245
Epoch 610, training loss: 631.150634765625 = 0.8675268888473511 + 100.0 * 6.302830696105957
Epoch 610, val loss: 1.1136611700057983
Epoch 620, training loss: 631.4320068359375 = 0.8494477272033691 + 100.0 * 6.305825710296631
Epoch 620, val loss: 1.1040271520614624
Epoch 630, training loss: 630.9805908203125 = 0.8313255310058594 + 100.0 * 6.301493167877197
Epoch 630, val loss: 1.0947084426879883
Epoch 640, training loss: 630.7129516601562 = 0.8133402466773987 + 100.0 * 6.2989959716796875
Epoch 640, val loss: 1.0855025053024292
Epoch 650, training loss: 630.5192260742188 = 0.7955499291419983 + 100.0 * 6.297236919403076
Epoch 650, val loss: 1.0766874551773071
Epoch 660, training loss: 630.2881469726562 = 0.777951180934906 + 100.0 * 6.295102119445801
Epoch 660, val loss: 1.0680586099624634
Epoch 670, training loss: 630.3004150390625 = 0.7605233788490295 + 100.0 * 6.295399188995361
Epoch 670, val loss: 1.0596554279327393
Epoch 680, training loss: 630.3223876953125 = 0.7431809306144714 + 100.0 * 6.295792102813721
Epoch 680, val loss: 1.0514566898345947
Epoch 690, training loss: 629.8724365234375 = 0.7260422706604004 + 100.0 * 6.291464328765869
Epoch 690, val loss: 1.0437051057815552
Epoch 700, training loss: 630.1210327148438 = 0.7091960310935974 + 100.0 * 6.294118404388428
Epoch 700, val loss: 1.0362558364868164
Epoch 710, training loss: 629.4916381835938 = 0.6924651861190796 + 100.0 * 6.287992000579834
Epoch 710, val loss: 1.0288163423538208
Epoch 720, training loss: 629.3494262695312 = 0.6761704087257385 + 100.0 * 6.2867326736450195
Epoch 720, val loss: 1.0219711065292358
Epoch 730, training loss: 629.194091796875 = 0.6601441502571106 + 100.0 * 6.28533935546875
Epoch 730, val loss: 1.0156384706497192
Epoch 740, training loss: 629.647216796875 = 0.6443859934806824 + 100.0 * 6.290028095245361
Epoch 740, val loss: 1.0095229148864746
Epoch 750, training loss: 629.176513671875 = 0.6287030577659607 + 100.0 * 6.285477638244629
Epoch 750, val loss: 1.0036765336990356
Epoch 760, training loss: 629.484375 = 0.6134656667709351 + 100.0 * 6.2887091636657715
Epoch 760, val loss: 0.998392641544342
Epoch 770, training loss: 628.8187866210938 = 0.5983197689056396 + 100.0 * 6.282204627990723
Epoch 770, val loss: 0.9929516911506653
Epoch 780, training loss: 628.6304931640625 = 0.5836239457130432 + 100.0 * 6.280468940734863
Epoch 780, val loss: 0.9883379936218262
Epoch 790, training loss: 628.9912109375 = 0.5692026019096375 + 100.0 * 6.284219741821289
Epoch 790, val loss: 0.9839245676994324
Epoch 800, training loss: 628.4855346679688 = 0.5550018548965454 + 100.0 * 6.279305458068848
Epoch 800, val loss: 0.9798099994659424
Epoch 810, training loss: 628.3250122070312 = 0.5411027073860168 + 100.0 * 6.277839183807373
Epoch 810, val loss: 0.9760476350784302
Epoch 820, training loss: 628.346435546875 = 0.5274505615234375 + 100.0 * 6.278189659118652
Epoch 820, val loss: 0.972673773765564
Epoch 830, training loss: 628.2579956054688 = 0.5139879584312439 + 100.0 * 6.277440071105957
Epoch 830, val loss: 0.9692932963371277
Epoch 840, training loss: 628.0864868164062 = 0.5008507966995239 + 100.0 * 6.2758564949035645
Epoch 840, val loss: 0.9666422605514526
Epoch 850, training loss: 627.7935791015625 = 0.4879854917526245 + 100.0 * 6.2730560302734375
Epoch 850, val loss: 0.9639530777931213
Epoch 860, training loss: 627.6843872070312 = 0.47539734840393066 + 100.0 * 6.272089958190918
Epoch 860, val loss: 0.9619204998016357
Epoch 870, training loss: 627.7349853515625 = 0.46308714151382446 + 100.0 * 6.272718906402588
Epoch 870, val loss: 0.9599687457084656
Epoch 880, training loss: 628.131591796875 = 0.4509044289588928 + 100.0 * 6.276806354522705
Epoch 880, val loss: 0.9581844806671143
Epoch 890, training loss: 627.8135986328125 = 0.4388958215713501 + 100.0 * 6.273746967315674
Epoch 890, val loss: 0.9562880992889404
Epoch 900, training loss: 627.2825317382812 = 0.42715537548065186 + 100.0 * 6.268553256988525
Epoch 900, val loss: 0.9548323154449463
Epoch 910, training loss: 627.2694702148438 = 0.4157481789588928 + 100.0 * 6.2685370445251465
Epoch 910, val loss: 0.9538724422454834
Epoch 920, training loss: 627.4795532226562 = 0.40455666184425354 + 100.0 * 6.270750045776367
Epoch 920, val loss: 0.9529411196708679
Epoch 930, training loss: 627.2471923828125 = 0.39347437024116516 + 100.0 * 6.2685370445251465
Epoch 930, val loss: 0.9521587491035461
Epoch 940, training loss: 626.9785766601562 = 0.38264620304107666 + 100.0 * 6.265959739685059
Epoch 940, val loss: 0.9515846371650696
Epoch 950, training loss: 627.1890258789062 = 0.37205788493156433 + 100.0 * 6.268169403076172
Epoch 950, val loss: 0.9512615203857422
Epoch 960, training loss: 626.7642211914062 = 0.3617117404937744 + 100.0 * 6.2640252113342285
Epoch 960, val loss: 0.9510510563850403
Epoch 970, training loss: 626.6271362304688 = 0.35158097743988037 + 100.0 * 6.262755870819092
Epoch 970, val loss: 0.9510711431503296
Epoch 980, training loss: 626.7059326171875 = 0.341693639755249 + 100.0 * 6.26364278793335
Epoch 980, val loss: 0.951248049736023
Epoch 990, training loss: 626.6275634765625 = 0.3319542407989502 + 100.0 * 6.262955665588379
Epoch 990, val loss: 0.9515177607536316
Epoch 1000, training loss: 626.5609130859375 = 0.32242798805236816 + 100.0 * 6.262384414672852
Epoch 1000, val loss: 0.952133297920227
Epoch 1010, training loss: 626.3800048828125 = 0.313183456659317 + 100.0 * 6.2606682777404785
Epoch 1010, val loss: 0.9528374671936035
Epoch 1020, training loss: 626.4984741210938 = 0.30418890714645386 + 100.0 * 6.2619428634643555
Epoch 1020, val loss: 0.9536901116371155
Epoch 1030, training loss: 626.2518310546875 = 0.29534798860549927 + 100.0 * 6.2595648765563965
Epoch 1030, val loss: 0.9549098014831543
Epoch 1040, training loss: 626.3286743164062 = 0.2867848575115204 + 100.0 * 6.26041841506958
Epoch 1040, val loss: 0.9560153484344482
Epoch 1050, training loss: 626.2469482421875 = 0.27847370505332947 + 100.0 * 6.2596845626831055
Epoch 1050, val loss: 0.9574189186096191
Epoch 1060, training loss: 626.13525390625 = 0.2703634202480316 + 100.0 * 6.258648872375488
Epoch 1060, val loss: 0.958686888217926
Epoch 1070, training loss: 626.3346557617188 = 0.26251405477523804 + 100.0 * 6.260721683502197
Epoch 1070, val loss: 0.9602700471878052
Epoch 1080, training loss: 625.9563598632812 = 0.2548626661300659 + 100.0 * 6.257014751434326
Epoch 1080, val loss: 0.9621424674987793
Epoch 1090, training loss: 625.7737426757812 = 0.24745965003967285 + 100.0 * 6.255263328552246
Epoch 1090, val loss: 0.9639121890068054
Epoch 1100, training loss: 625.6929321289062 = 0.24030709266662598 + 100.0 * 6.254526615142822
Epoch 1100, val loss: 0.9660083651542664
Epoch 1110, training loss: 626.4718017578125 = 0.23336650431156158 + 100.0 * 6.262384414672852
Epoch 1110, val loss: 0.9680879712104797
Epoch 1120, training loss: 625.9222412109375 = 0.22667106986045837 + 100.0 * 6.256955623626709
Epoch 1120, val loss: 0.9706026315689087
Epoch 1130, training loss: 625.7378540039062 = 0.220100536942482 + 100.0 * 6.2551774978637695
Epoch 1130, val loss: 0.9728831052780151
Epoch 1140, training loss: 625.7026977539062 = 0.2138097733259201 + 100.0 * 6.254889011383057
Epoch 1140, val loss: 0.9750646352767944
Epoch 1150, training loss: 625.501708984375 = 0.20774084329605103 + 100.0 * 6.252939701080322
Epoch 1150, val loss: 0.9780439734458923
Epoch 1160, training loss: 625.3986206054688 = 0.2018575519323349 + 100.0 * 6.251967906951904
Epoch 1160, val loss: 0.9805459380149841
Epoch 1170, training loss: 625.4906005859375 = 0.19620254635810852 + 100.0 * 6.252943992614746
Epoch 1170, val loss: 0.9833055138587952
Epoch 1180, training loss: 625.4707641601562 = 0.19065922498703003 + 100.0 * 6.252800941467285
Epoch 1180, val loss: 0.9862619042396545
Epoch 1190, training loss: 625.4142456054688 = 0.18531376123428345 + 100.0 * 6.252289295196533
Epoch 1190, val loss: 0.9888836741447449
Epoch 1200, training loss: 625.78759765625 = 0.18016664683818817 + 100.0 * 6.256073951721191
Epoch 1200, val loss: 0.9919955730438232
Epoch 1210, training loss: 625.2762451171875 = 0.17518015205860138 + 100.0 * 6.251010894775391
Epoch 1210, val loss: 0.9952509999275208
Epoch 1220, training loss: 625.0701904296875 = 0.17036187648773193 + 100.0 * 6.248998641967773
Epoch 1220, val loss: 0.9983388185501099
Epoch 1230, training loss: 625.5723876953125 = 0.16579018533229828 + 100.0 * 6.254066467285156
Epoch 1230, val loss: 1.0020147562026978
Epoch 1240, training loss: 625.072021484375 = 0.16121512651443481 + 100.0 * 6.24910831451416
Epoch 1240, val loss: 1.0047422647476196
Epoch 1250, training loss: 624.9654541015625 = 0.1568908989429474 + 100.0 * 6.2480854988098145
Epoch 1250, val loss: 1.0084524154663086
Epoch 1260, training loss: 624.9229125976562 = 0.1527090221643448 + 100.0 * 6.247702121734619
Epoch 1260, val loss: 1.0118322372436523
Epoch 1270, training loss: 625.2571411132812 = 0.1486838161945343 + 100.0 * 6.251084804534912
Epoch 1270, val loss: 1.0155991315841675
Epoch 1280, training loss: 624.956298828125 = 0.14471851289272308 + 100.0 * 6.2481160163879395
Epoch 1280, val loss: 1.0188140869140625
Epoch 1290, training loss: 624.8626708984375 = 0.14092586934566498 + 100.0 * 6.247217178344727
Epoch 1290, val loss: 1.022501826286316
Epoch 1300, training loss: 624.6486206054688 = 0.13725872337818146 + 100.0 * 6.245113372802734
Epoch 1300, val loss: 1.0261698961257935
Epoch 1310, training loss: 625.1953735351562 = 0.13369707763195038 + 100.0 * 6.250617027282715
Epoch 1310, val loss: 1.029725193977356
Epoch 1320, training loss: 624.8027954101562 = 0.1302749514579773 + 100.0 * 6.246725559234619
Epoch 1320, val loss: 1.0334957838058472
Epoch 1330, training loss: 624.504150390625 = 0.12692108750343323 + 100.0 * 6.243772506713867
Epoch 1330, val loss: 1.0370595455169678
Epoch 1340, training loss: 624.4833984375 = 0.12372744083404541 + 100.0 * 6.24359655380249
Epoch 1340, val loss: 1.0408598184585571
Epoch 1350, training loss: 624.6116943359375 = 0.1206275075674057 + 100.0 * 6.244910717010498
Epoch 1350, val loss: 1.044442057609558
Epoch 1360, training loss: 624.31005859375 = 0.11760997772216797 + 100.0 * 6.241924285888672
Epoch 1360, val loss: 1.0483758449554443
Epoch 1370, training loss: 624.8204956054688 = 0.11471325159072876 + 100.0 * 6.247057914733887
Epoch 1370, val loss: 1.0518982410430908
Epoch 1380, training loss: 624.8328247070312 = 0.11186166107654572 + 100.0 * 6.247209548950195
Epoch 1380, val loss: 1.055896282196045
Epoch 1390, training loss: 624.45361328125 = 0.10907689481973648 + 100.0 * 6.24344539642334
Epoch 1390, val loss: 1.059734582901001
Epoch 1400, training loss: 624.256591796875 = 0.10641278326511383 + 100.0 * 6.241502285003662
Epoch 1400, val loss: 1.063131332397461
Epoch 1410, training loss: 624.5866088867188 = 0.10382901877164841 + 100.0 * 6.244827747344971
Epoch 1410, val loss: 1.06694757938385
Epoch 1420, training loss: 624.1903076171875 = 0.10134442150592804 + 100.0 * 6.240890026092529
Epoch 1420, val loss: 1.0710344314575195
Epoch 1430, training loss: 624.034912109375 = 0.09888822585344315 + 100.0 * 6.239360332489014
Epoch 1430, val loss: 1.0746994018554688
Epoch 1440, training loss: 624.1494140625 = 0.0965435728430748 + 100.0 * 6.240528583526611
Epoch 1440, val loss: 1.0786950588226318
Epoch 1450, training loss: 624.2638549804688 = 0.0942724421620369 + 100.0 * 6.241695404052734
Epoch 1450, val loss: 1.0825053453445435
Epoch 1460, training loss: 624.0962524414062 = 0.09202402085065842 + 100.0 * 6.240042209625244
Epoch 1460, val loss: 1.0862571001052856
Epoch 1470, training loss: 624.0944213867188 = 0.08987940847873688 + 100.0 * 6.240045070648193
Epoch 1470, val loss: 1.0899816751480103
Epoch 1480, training loss: 624.428955078125 = 0.08776728808879852 + 100.0 * 6.243412017822266
Epoch 1480, val loss: 1.0940624475479126
Epoch 1490, training loss: 623.9034423828125 = 0.08572053909301758 + 100.0 * 6.238177299499512
Epoch 1490, val loss: 1.0980103015899658
Epoch 1500, training loss: 623.7879638671875 = 0.0837426632642746 + 100.0 * 6.23704195022583
Epoch 1500, val loss: 1.1019309759140015
Epoch 1510, training loss: 624.0386962890625 = 0.08186738938093185 + 100.0 * 6.239568710327148
Epoch 1510, val loss: 1.106008768081665
Epoch 1520, training loss: 623.7659912109375 = 0.07996676862239838 + 100.0 * 6.236860275268555
Epoch 1520, val loss: 1.1094212532043457
Epoch 1530, training loss: 623.6507568359375 = 0.07815635949373245 + 100.0 * 6.2357258796691895
Epoch 1530, val loss: 1.1136267185211182
Epoch 1540, training loss: 623.616455078125 = 0.07640647143125534 + 100.0 * 6.235400676727295
Epoch 1540, val loss: 1.1173683404922485
Epoch 1550, training loss: 624.2037353515625 = 0.07474435865879059 + 100.0 * 6.2412896156311035
Epoch 1550, val loss: 1.1209919452667236
Epoch 1560, training loss: 623.650146484375 = 0.07303301244974136 + 100.0 * 6.2357707023620605
Epoch 1560, val loss: 1.1251018047332764
Epoch 1570, training loss: 623.5509033203125 = 0.07143103331327438 + 100.0 * 6.234794616699219
Epoch 1570, val loss: 1.1287736892700195
Epoch 1580, training loss: 623.7634887695312 = 0.06988382339477539 + 100.0 * 6.236936092376709
Epoch 1580, val loss: 1.1328511238098145
Epoch 1590, training loss: 623.6209716796875 = 0.06834546476602554 + 100.0 * 6.235526084899902
Epoch 1590, val loss: 1.1364951133728027
Epoch 1600, training loss: 624.0345458984375 = 0.06685243546962738 + 100.0 * 6.2396769523620605
Epoch 1600, val loss: 1.1403015851974487
Epoch 1610, training loss: 623.4472045898438 = 0.06538624316453934 + 100.0 * 6.233818054199219
Epoch 1610, val loss: 1.1439460515975952
Epoch 1620, training loss: 623.3348999023438 = 0.06399542838335037 + 100.0 * 6.232708930969238
Epoch 1620, val loss: 1.1478163003921509
Epoch 1630, training loss: 623.6630859375 = 0.06264600902795792 + 100.0 * 6.236004829406738
Epoch 1630, val loss: 1.151228427886963
Epoch 1640, training loss: 623.3228149414062 = 0.06131504103541374 + 100.0 * 6.232614517211914
Epoch 1640, val loss: 1.1552729606628418
Epoch 1650, training loss: 623.4269409179688 = 0.0600377582013607 + 100.0 * 6.233668804168701
Epoch 1650, val loss: 1.1593384742736816
Epoch 1660, training loss: 623.670654296875 = 0.058775316923856735 + 100.0 * 6.236118793487549
Epoch 1660, val loss: 1.162674069404602
Epoch 1670, training loss: 623.2835693359375 = 0.05755064636468887 + 100.0 * 6.232260227203369
Epoch 1670, val loss: 1.1665167808532715
Epoch 1680, training loss: 623.1942138671875 = 0.05636690929532051 + 100.0 * 6.231378078460693
Epoch 1680, val loss: 1.1703274250030518
Epoch 1690, training loss: 623.2107543945312 = 0.05522853136062622 + 100.0 * 6.231554985046387
Epoch 1690, val loss: 1.1742794513702393
Epoch 1700, training loss: 623.585693359375 = 0.05411054939031601 + 100.0 * 6.235315322875977
Epoch 1700, val loss: 1.1780309677124023
Epoch 1710, training loss: 623.5800170898438 = 0.05297964811325073 + 100.0 * 6.2352705001831055
Epoch 1710, val loss: 1.1809337139129639
Epoch 1720, training loss: 623.2359619140625 = 0.051909931004047394 + 100.0 * 6.23184061050415
Epoch 1720, val loss: 1.1849632263183594
Epoch 1730, training loss: 623.187744140625 = 0.05087283253669739 + 100.0 * 6.231368541717529
Epoch 1730, val loss: 1.188226342201233
Epoch 1740, training loss: 623.5142822265625 = 0.049857765436172485 + 100.0 * 6.234644412994385
Epoch 1740, val loss: 1.1918991804122925
Epoch 1750, training loss: 623.1981201171875 = 0.04885957017540932 + 100.0 * 6.231492042541504
Epoch 1750, val loss: 1.19570791721344
Epoch 1760, training loss: 623.847900390625 = 0.04789714142680168 + 100.0 * 6.23799991607666
Epoch 1760, val loss: 1.1991629600524902
Epoch 1770, training loss: 623.08642578125 = 0.04696192592382431 + 100.0 * 6.2303948402404785
Epoch 1770, val loss: 1.2025933265686035
Epoch 1780, training loss: 622.8980102539062 = 0.04603073373436928 + 100.0 * 6.228519916534424
Epoch 1780, val loss: 1.206180214881897
Epoch 1790, training loss: 622.8405151367188 = 0.0451563224196434 + 100.0 * 6.2279534339904785
Epoch 1790, val loss: 1.2096415758132935
Epoch 1800, training loss: 623.11181640625 = 0.04430823773145676 + 100.0 * 6.230674743652344
Epoch 1800, val loss: 1.2130157947540283
Epoch 1810, training loss: 622.8178100585938 = 0.04343969002366066 + 100.0 * 6.227743625640869
Epoch 1810, val loss: 1.2165364027023315
Epoch 1820, training loss: 622.7931518554688 = 0.04260769113898277 + 100.0 * 6.227505207061768
Epoch 1820, val loss: 1.2201920747756958
Epoch 1830, training loss: 622.9942626953125 = 0.041812311857938766 + 100.0 * 6.229524612426758
Epoch 1830, val loss: 1.223692536354065
Epoch 1840, training loss: 623.0525512695312 = 0.04102342948317528 + 100.0 * 6.2301154136657715
Epoch 1840, val loss: 1.2268292903900146
Epoch 1850, training loss: 622.846435546875 = 0.04025449976325035 + 100.0 * 6.228061676025391
Epoch 1850, val loss: 1.2304177284240723
Epoch 1860, training loss: 622.7022705078125 = 0.039523765444755554 + 100.0 * 6.226627349853516
Epoch 1860, val loss: 1.2339564561843872
Epoch 1870, training loss: 622.6435546875 = 0.03881491348147392 + 100.0 * 6.226047515869141
Epoch 1870, val loss: 1.2373614311218262
Epoch 1880, training loss: 623.3298950195312 = 0.03813885897397995 + 100.0 * 6.232917785644531
Epoch 1880, val loss: 1.240666151046753
Epoch 1890, training loss: 622.9149169921875 = 0.03740422800183296 + 100.0 * 6.2287750244140625
Epoch 1890, val loss: 1.2438796758651733
Epoch 1900, training loss: 622.766357421875 = 0.03673514351248741 + 100.0 * 6.227295875549316
Epoch 1900, val loss: 1.2470530271530151
Epoch 1910, training loss: 622.7662963867188 = 0.036080311983823776 + 100.0 * 6.227302551269531
Epoch 1910, val loss: 1.2505378723144531
Epoch 1920, training loss: 622.8206176757812 = 0.035446103662252426 + 100.0 * 6.227851867675781
Epoch 1920, val loss: 1.2534868717193604
Epoch 1930, training loss: 622.9464111328125 = 0.03482651710510254 + 100.0 * 6.2291154861450195
Epoch 1930, val loss: 1.2573914527893066
Epoch 1940, training loss: 622.9390258789062 = 0.03420494124293327 + 100.0 * 6.229048728942871
Epoch 1940, val loss: 1.2605812549591064
Epoch 1950, training loss: 622.547119140625 = 0.033604688942432404 + 100.0 * 6.22513484954834
Epoch 1950, val loss: 1.263326644897461
Epoch 1960, training loss: 622.4166870117188 = 0.0330367237329483 + 100.0 * 6.223836898803711
Epoch 1960, val loss: 1.2671270370483398
Epoch 1970, training loss: 622.4152221679688 = 0.032485269010066986 + 100.0 * 6.223827362060547
Epoch 1970, val loss: 1.2703046798706055
Epoch 1980, training loss: 623.0623779296875 = 0.031961940228939056 + 100.0 * 6.23030424118042
Epoch 1980, val loss: 1.2735092639923096
Epoch 1990, training loss: 622.528076171875 = 0.0313902422785759 + 100.0 * 6.224967002868652
Epoch 1990, val loss: 1.2763398885726929
Epoch 2000, training loss: 622.635498046875 = 0.030863767489790916 + 100.0 * 6.226046085357666
Epoch 2000, val loss: 1.2794029712677002
Epoch 2010, training loss: 622.374267578125 = 0.030341584235429764 + 100.0 * 6.2234392166137695
Epoch 2010, val loss: 1.2827039957046509
Epoch 2020, training loss: 622.7028198242188 = 0.029846400022506714 + 100.0 * 6.226729869842529
Epoch 2020, val loss: 1.286041021347046
Epoch 2030, training loss: 622.5154418945312 = 0.029350269585847855 + 100.0 * 6.224861145019531
Epoch 2030, val loss: 1.2889879941940308
Epoch 2040, training loss: 622.472412109375 = 0.028865547850728035 + 100.0 * 6.224435329437256
Epoch 2040, val loss: 1.2919845581054688
Epoch 2050, training loss: 622.5447998046875 = 0.028395412489771843 + 100.0 * 6.225164413452148
Epoch 2050, val loss: 1.2949875593185425
Epoch 2060, training loss: 622.4653930664062 = 0.02793685905635357 + 100.0 * 6.224374294281006
Epoch 2060, val loss: 1.2986960411071777
Epoch 2070, training loss: 622.232666015625 = 0.02749698981642723 + 100.0 * 6.222051620483398
Epoch 2070, val loss: 1.301391363143921
Epoch 2080, training loss: 622.2631225585938 = 0.02706889621913433 + 100.0 * 6.222360610961914
Epoch 2080, val loss: 1.3044307231903076
Epoch 2090, training loss: 622.4990234375 = 0.026652025058865547 + 100.0 * 6.2247233390808105
Epoch 2090, val loss: 1.307436466217041
Epoch 2100, training loss: 622.2372436523438 = 0.026227803900837898 + 100.0 * 6.222110271453857
Epoch 2100, val loss: 1.3105753660202026
Epoch 2110, training loss: 622.320556640625 = 0.025815261527895927 + 100.0 * 6.222947597503662
Epoch 2110, val loss: 1.313300371170044
Epoch 2120, training loss: 622.3682250976562 = 0.02542235143482685 + 100.0 * 6.223427772521973
Epoch 2120, val loss: 1.3159222602844238
Epoch 2130, training loss: 622.47412109375 = 0.02503456547856331 + 100.0 * 6.224491119384766
Epoch 2130, val loss: 1.3189746141433716
Epoch 2140, training loss: 622.2689208984375 = 0.024654697626829147 + 100.0 * 6.222442626953125
Epoch 2140, val loss: 1.322170376777649
Epoch 2150, training loss: 622.22509765625 = 0.024284565821290016 + 100.0 * 6.222008228302002
Epoch 2150, val loss: 1.3245265483856201
Epoch 2160, training loss: 622.2777099609375 = 0.02392314374446869 + 100.0 * 6.222537994384766
Epoch 2160, val loss: 1.3276598453521729
Epoch 2170, training loss: 621.969482421875 = 0.023565111681818962 + 100.0 * 6.219459056854248
Epoch 2170, val loss: 1.3308284282684326
Epoch 2180, training loss: 622.0364379882812 = 0.02322300896048546 + 100.0 * 6.220132350921631
Epoch 2180, val loss: 1.3334450721740723
Epoch 2190, training loss: 622.306884765625 = 0.022893015295267105 + 100.0 * 6.222839832305908
Epoch 2190, val loss: 1.3363287448883057
Epoch 2200, training loss: 622.1405639648438 = 0.022554757073521614 + 100.0 * 6.221179962158203
Epoch 2200, val loss: 1.3388521671295166
Epoch 2210, training loss: 621.96337890625 = 0.02222227305173874 + 100.0 * 6.219411849975586
Epoch 2210, val loss: 1.3419432640075684
Epoch 2220, training loss: 622.0260009765625 = 0.021905342116951942 + 100.0 * 6.220040798187256
Epoch 2220, val loss: 1.3442778587341309
Epoch 2230, training loss: 622.3510131835938 = 0.021606527268886566 + 100.0 * 6.223293781280518
Epoch 2230, val loss: 1.3471755981445312
Epoch 2240, training loss: 622.0863037109375 = 0.02129180170595646 + 100.0 * 6.2206501960754395
Epoch 2240, val loss: 1.3497865200042725
Epoch 2250, training loss: 621.8678588867188 = 0.020980317145586014 + 100.0 * 6.21846866607666
Epoch 2250, val loss: 1.3525909185409546
Epoch 2260, training loss: 621.7731323242188 = 0.02069699764251709 + 100.0 * 6.217524528503418
Epoch 2260, val loss: 1.3553072214126587
Epoch 2270, training loss: 622.2783813476562 = 0.020422259345650673 + 100.0 * 6.222579479217529
Epoch 2270, val loss: 1.3583714962005615
Epoch 2280, training loss: 621.7523193359375 = 0.020120276138186455 + 100.0 * 6.217321872711182
Epoch 2280, val loss: 1.3602575063705444
Epoch 2290, training loss: 622.0750122070312 = 0.01984993740916252 + 100.0 * 6.220551490783691
Epoch 2290, val loss: 1.3629674911499023
Epoch 2300, training loss: 621.7207641601562 = 0.019576529040932655 + 100.0 * 6.217011451721191
Epoch 2300, val loss: 1.3655606508255005
Epoch 2310, training loss: 621.7539672851562 = 0.019320577383041382 + 100.0 * 6.21734619140625
Epoch 2310, val loss: 1.3685996532440186
Epoch 2320, training loss: 621.7958374023438 = 0.01906699687242508 + 100.0 * 6.217767715454102
Epoch 2320, val loss: 1.3711297512054443
Epoch 2330, training loss: 622.1531372070312 = 0.0188152939081192 + 100.0 * 6.221343517303467
Epoch 2330, val loss: 1.3736125230789185
Epoch 2340, training loss: 621.9307250976562 = 0.018565604463219643 + 100.0 * 6.219121932983398
Epoch 2340, val loss: 1.3754513263702393
Epoch 2350, training loss: 622.295166015625 = 0.018323738127946854 + 100.0 * 6.222768783569336
Epoch 2350, val loss: 1.3784676790237427
Epoch 2360, training loss: 621.7987670898438 = 0.018075430765748024 + 100.0 * 6.217807292938232
Epoch 2360, val loss: 1.380636215209961
Epoch 2370, training loss: 621.6077880859375 = 0.017844846472144127 + 100.0 * 6.215899467468262
Epoch 2370, val loss: 1.3834247589111328
Epoch 2380, training loss: 621.6006469726562 = 0.017621666193008423 + 100.0 * 6.215830326080322
Epoch 2380, val loss: 1.385955810546875
Epoch 2390, training loss: 621.6806030273438 = 0.017398975789546967 + 100.0 * 6.21663236618042
Epoch 2390, val loss: 1.3882777690887451
Epoch 2400, training loss: 621.881591796875 = 0.01717945747077465 + 100.0 * 6.218644618988037
Epoch 2400, val loss: 1.3903818130493164
Epoch 2410, training loss: 621.7974853515625 = 0.016962071880698204 + 100.0 * 6.217804908752441
Epoch 2410, val loss: 1.3925927877426147
Epoch 2420, training loss: 621.570068359375 = 0.016750041395425797 + 100.0 * 6.215533256530762
Epoch 2420, val loss: 1.395696997642517
Epoch 2430, training loss: 621.8344116210938 = 0.016549916937947273 + 100.0 * 6.218178749084473
Epoch 2430, val loss: 1.3976318836212158
Epoch 2440, training loss: 621.5498657226562 = 0.016340836882591248 + 100.0 * 6.215335369110107
Epoch 2440, val loss: 1.3998055458068848
Epoch 2450, training loss: 622.1030883789062 = 0.016150863841176033 + 100.0 * 6.220869064331055
Epoch 2450, val loss: 1.4031122922897339
Epoch 2460, training loss: 621.4988403320312 = 0.015942329540848732 + 100.0 * 6.214828968048096
Epoch 2460, val loss: 1.404177188873291
Epoch 2470, training loss: 621.4114990234375 = 0.01574593223631382 + 100.0 * 6.213957786560059
Epoch 2470, val loss: 1.4070746898651123
Epoch 2480, training loss: 621.3433227539062 = 0.015561502426862717 + 100.0 * 6.213277816772461
Epoch 2480, val loss: 1.4091413021087646
Epoch 2490, training loss: 621.343994140625 = 0.015384887345135212 + 100.0 * 6.21328592300415
Epoch 2490, val loss: 1.4115785360336304
Epoch 2500, training loss: 622.2389526367188 = 0.015210534445941448 + 100.0 * 6.222237586975098
Epoch 2500, val loss: 1.414013385772705
Epoch 2510, training loss: 621.533447265625 = 0.015029488131403923 + 100.0 * 6.215184211730957
Epoch 2510, val loss: 1.4155538082122803
Epoch 2520, training loss: 621.6111450195312 = 0.014854422770440578 + 100.0 * 6.215962886810303
Epoch 2520, val loss: 1.4182521104812622
Epoch 2530, training loss: 621.733154296875 = 0.014679851941764355 + 100.0 * 6.217184543609619
Epoch 2530, val loss: 1.4202240705490112
Epoch 2540, training loss: 621.6367797851562 = 0.014506230130791664 + 100.0 * 6.216222763061523
Epoch 2540, val loss: 1.42244553565979
Epoch 2550, training loss: 621.380126953125 = 0.014342800714075565 + 100.0 * 6.213657855987549
Epoch 2550, val loss: 1.4243922233581543
Epoch 2560, training loss: 621.5071411132812 = 0.014185607433319092 + 100.0 * 6.214930057525635
Epoch 2560, val loss: 1.4269354343414307
Epoch 2570, training loss: 621.3873901367188 = 0.014026700519025326 + 100.0 * 6.213733673095703
Epoch 2570, val loss: 1.4287549257278442
Epoch 2580, training loss: 621.305908203125 = 0.01387169398367405 + 100.0 * 6.212920665740967
Epoch 2580, val loss: 1.4309757947921753
Epoch 2590, training loss: 621.4959106445312 = 0.01372381392866373 + 100.0 * 6.214821815490723
Epoch 2590, val loss: 1.4333388805389404
Epoch 2600, training loss: 621.2881469726562 = 0.01356925442814827 + 100.0 * 6.2127461433410645
Epoch 2600, val loss: 1.4349159002304077
Epoch 2610, training loss: 621.3275756835938 = 0.013415954075753689 + 100.0 * 6.213141441345215
Epoch 2610, val loss: 1.4367823600769043
Epoch 2620, training loss: 621.791748046875 = 0.013273871503770351 + 100.0 * 6.217784881591797
Epoch 2620, val loss: 1.4388465881347656
Epoch 2630, training loss: 621.3102416992188 = 0.013122417032718658 + 100.0 * 6.212971210479736
Epoch 2630, val loss: 1.4413765668869019
Epoch 2640, training loss: 621.2551879882812 = 0.012986325658857822 + 100.0 * 6.212421894073486
Epoch 2640, val loss: 1.4430164098739624
Epoch 2650, training loss: 621.462646484375 = 0.012851579114794731 + 100.0 * 6.2144975662231445
Epoch 2650, val loss: 1.4451346397399902
Epoch 2660, training loss: 621.357666015625 = 0.012716079130768776 + 100.0 * 6.213449478149414
Epoch 2660, val loss: 1.4474655389785767
Epoch 2670, training loss: 621.1566162109375 = 0.012578798457980156 + 100.0 * 6.211440563201904
Epoch 2670, val loss: 1.449091911315918
Epoch 2680, training loss: 621.11962890625 = 0.012452935799956322 + 100.0 * 6.211071491241455
Epoch 2680, val loss: 1.451373815536499
Epoch 2690, training loss: 621.4685668945312 = 0.012334015220403671 + 100.0 * 6.21456241607666
Epoch 2690, val loss: 1.4532314538955688
Epoch 2700, training loss: 621.164794921875 = 0.012199130840599537 + 100.0 * 6.211525917053223
Epoch 2700, val loss: 1.4550968408584595
Epoch 2710, training loss: 621.1610107421875 = 0.01207505539059639 + 100.0 * 6.211489200592041
Epoch 2710, val loss: 1.4570121765136719
Epoch 2720, training loss: 621.3192138671875 = 0.011956138536334038 + 100.0 * 6.213072299957275
Epoch 2720, val loss: 1.4590915441513062
Epoch 2730, training loss: 621.224365234375 = 0.011832325719296932 + 100.0 * 6.212125301361084
Epoch 2730, val loss: 1.4611482620239258
Epoch 2740, training loss: 621.504150390625 = 0.011714010499417782 + 100.0 * 6.214924335479736
Epoch 2740, val loss: 1.463195562362671
Epoch 2750, training loss: 621.0714721679688 = 0.011591248214244843 + 100.0 * 6.210598468780518
Epoch 2750, val loss: 1.4649460315704346
Epoch 2760, training loss: 621.1223754882812 = 0.011481687426567078 + 100.0 * 6.211109161376953
Epoch 2760, val loss: 1.466944694519043
Epoch 2770, training loss: 621.3892211914062 = 0.011374672874808311 + 100.0 * 6.213778972625732
Epoch 2770, val loss: 1.4689407348632812
Epoch 2780, training loss: 621.0861206054688 = 0.011258807964622974 + 100.0 * 6.21074914932251
Epoch 2780, val loss: 1.4705162048339844
Epoch 2790, training loss: 620.9249877929688 = 0.011152045801281929 + 100.0 * 6.209137916564941
Epoch 2790, val loss: 1.4724961519241333
Epoch 2800, training loss: 621.1681518554688 = 0.011050720699131489 + 100.0 * 6.211570739746094
Epoch 2800, val loss: 1.4742910861968994
Epoch 2810, training loss: 621.184326171875 = 0.010941049084067345 + 100.0 * 6.211733818054199
Epoch 2810, val loss: 1.476017713546753
Epoch 2820, training loss: 621.1161499023438 = 0.010832599364221096 + 100.0 * 6.211053371429443
Epoch 2820, val loss: 1.477927327156067
Epoch 2830, training loss: 620.9954223632812 = 0.010737016797065735 + 100.0 * 6.2098469734191895
Epoch 2830, val loss: 1.4801105260849
Epoch 2840, training loss: 621.236083984375 = 0.010642725974321365 + 100.0 * 6.212254524230957
Epoch 2840, val loss: 1.4816478490829468
Epoch 2850, training loss: 621.0764770507812 = 0.010538481175899506 + 100.0 * 6.210659027099609
Epoch 2850, val loss: 1.4827719926834106
Epoch 2860, training loss: 620.8088989257812 = 0.010438465513288975 + 100.0 * 6.207984447479248
Epoch 2860, val loss: 1.485230803489685
Epoch 2870, training loss: 620.8707885742188 = 0.01034697238355875 + 100.0 * 6.208604335784912
Epoch 2870, val loss: 1.4873236417770386
Epoch 2880, training loss: 621.3493041992188 = 0.01025770790874958 + 100.0 * 6.213390350341797
Epoch 2880, val loss: 1.488846778869629
Epoch 2890, training loss: 620.8442993164062 = 0.010158172808587551 + 100.0 * 6.208341598510742
Epoch 2890, val loss: 1.4901940822601318
Epoch 2900, training loss: 621.0999145507812 = 0.010068787261843681 + 100.0 * 6.210898399353027
Epoch 2900, val loss: 1.4921255111694336
Epoch 2910, training loss: 620.8456420898438 = 0.009976436384022236 + 100.0 * 6.208356857299805
Epoch 2910, val loss: 1.4939762353897095
Epoch 2920, training loss: 621.1265869140625 = 0.009894181974232197 + 100.0 * 6.211166858673096
Epoch 2920, val loss: 1.4953190088272095
Epoch 2930, training loss: 620.7660522460938 = 0.00979936495423317 + 100.0 * 6.20756196975708
Epoch 2930, val loss: 1.497545599937439
Epoch 2940, training loss: 620.7717895507812 = 0.009716157801449299 + 100.0 * 6.207621097564697
Epoch 2940, val loss: 1.4991222620010376
Epoch 2950, training loss: 620.778076171875 = 0.009637436829507351 + 100.0 * 6.207684516906738
Epoch 2950, val loss: 1.500494360923767
Epoch 2960, training loss: 621.0933837890625 = 0.009560015052556992 + 100.0 * 6.2108378410339355
Epoch 2960, val loss: 1.5024927854537964
Epoch 2970, training loss: 621.1107177734375 = 0.009476722218096256 + 100.0 * 6.211012363433838
Epoch 2970, val loss: 1.5047669410705566
Epoch 2980, training loss: 620.6847534179688 = 0.009389853104948997 + 100.0 * 6.206753253936768
Epoch 2980, val loss: 1.505534291267395
Epoch 2990, training loss: 620.6641845703125 = 0.00930857378989458 + 100.0 * 6.206548690795898
Epoch 2990, val loss: 1.5077382326126099
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8017923036373221
=== training gcn model ===
Epoch 0, training loss: 861.6361083984375 = 1.949462890625 + 100.0 * 8.596866607666016
Epoch 0, val loss: 1.9519754648208618
Epoch 10, training loss: 861.5650024414062 = 1.9411656856536865 + 100.0 * 8.596238136291504
Epoch 10, val loss: 1.943243145942688
Epoch 20, training loss: 861.0550537109375 = 1.9305970668792725 + 100.0 * 8.5912446975708
Epoch 20, val loss: 1.9319738149642944
Epoch 30, training loss: 857.3352661132812 = 1.9165291786193848 + 100.0 * 8.554187774658203
Epoch 30, val loss: 1.9169321060180664
Epoch 40, training loss: 835.598876953125 = 1.8988299369812012 + 100.0 * 8.337000846862793
Epoch 40, val loss: 1.89876389503479
Epoch 50, training loss: 786.8012084960938 = 1.8792346715927124 + 100.0 * 7.849219799041748
Epoch 50, val loss: 1.879420280456543
Epoch 60, training loss: 748.0462646484375 = 1.8651127815246582 + 100.0 * 7.461811542510986
Epoch 60, val loss: 1.8657281398773193
Epoch 70, training loss: 717.2405395507812 = 1.8529267311096191 + 100.0 * 7.153876304626465
Epoch 70, val loss: 1.8535668849945068
Epoch 80, training loss: 696.2366333007812 = 1.8424208164215088 + 100.0 * 6.943942070007324
Epoch 80, val loss: 1.8434327840805054
Epoch 90, training loss: 685.7045288085938 = 1.8323825597763062 + 100.0 * 6.83872127532959
Epoch 90, val loss: 1.8340675830841064
Epoch 100, training loss: 678.806396484375 = 1.82244873046875 + 100.0 * 6.769839286804199
Epoch 100, val loss: 1.824656367301941
Epoch 110, training loss: 672.453369140625 = 1.8130929470062256 + 100.0 * 6.706402778625488
Epoch 110, val loss: 1.8154572248458862
Epoch 120, training loss: 667.3783569335938 = 1.8044168949127197 + 100.0 * 6.6557393074035645
Epoch 120, val loss: 1.807062029838562
Epoch 130, training loss: 663.7512817382812 = 1.79594886302948 + 100.0 * 6.619553089141846
Epoch 130, val loss: 1.7992031574249268
Epoch 140, training loss: 660.4103393554688 = 1.7871689796447754 + 100.0 * 6.586231708526611
Epoch 140, val loss: 1.7911239862442017
Epoch 150, training loss: 657.615478515625 = 1.7777833938598633 + 100.0 * 6.558377265930176
Epoch 150, val loss: 1.7826037406921387
Epoch 160, training loss: 655.3916015625 = 1.7676129341125488 + 100.0 * 6.5362396240234375
Epoch 160, val loss: 1.7733429670333862
Epoch 170, training loss: 653.346435546875 = 1.756582498550415 + 100.0 * 6.515898704528809
Epoch 170, val loss: 1.763371467590332
Epoch 180, training loss: 651.521484375 = 1.744653582572937 + 100.0 * 6.497768402099609
Epoch 180, val loss: 1.7526875734329224
Epoch 190, training loss: 650.3572998046875 = 1.7316577434539795 + 100.0 * 6.4862565994262695
Epoch 190, val loss: 1.741154432296753
Epoch 200, training loss: 648.6885375976562 = 1.7175366878509521 + 100.0 * 6.469710350036621
Epoch 200, val loss: 1.7285370826721191
Epoch 210, training loss: 647.40869140625 = 1.7021536827087402 + 100.0 * 6.457065582275391
Epoch 210, val loss: 1.7148659229278564
Epoch 220, training loss: 646.4773559570312 = 1.685470700263977 + 100.0 * 6.447918891906738
Epoch 220, val loss: 1.6999554634094238
Epoch 230, training loss: 645.39306640625 = 1.667160987854004 + 100.0 * 6.437259197235107
Epoch 230, val loss: 1.6837304830551147
Epoch 240, training loss: 644.5413818359375 = 1.6475623846054077 + 100.0 * 6.428937911987305
Epoch 240, val loss: 1.6662681102752686
Epoch 250, training loss: 643.5664672851562 = 1.626573085784912 + 100.0 * 6.419398784637451
Epoch 250, val loss: 1.6476200819015503
Epoch 260, training loss: 643.0053100585938 = 1.6042710542678833 + 100.0 * 6.414010524749756
Epoch 260, val loss: 1.627784013748169
Epoch 270, training loss: 642.3917236328125 = 1.5806996822357178 + 100.0 * 6.408110618591309
Epoch 270, val loss: 1.6068958044052124
Epoch 280, training loss: 641.528564453125 = 1.556044101715088 + 100.0 * 6.399725437164307
Epoch 280, val loss: 1.5850462913513184
Epoch 290, training loss: 640.9066772460938 = 1.530500888824463 + 100.0 * 6.39376163482666
Epoch 290, val loss: 1.562480092048645
Epoch 300, training loss: 640.8223876953125 = 1.504158854484558 + 100.0 * 6.393182277679443
Epoch 300, val loss: 1.5394243001937866
Epoch 310, training loss: 639.8656616210938 = 1.4770604372024536 + 100.0 * 6.383886337280273
Epoch 310, val loss: 1.5158436298370361
Epoch 320, training loss: 639.1666259765625 = 1.4497199058532715 + 100.0 * 6.377169132232666
Epoch 320, val loss: 1.4922387599945068
Epoch 330, training loss: 638.6981811523438 = 1.4221984148025513 + 100.0 * 6.372759819030762
Epoch 330, val loss: 1.468638300895691
Epoch 340, training loss: 638.3207397460938 = 1.3944323062896729 + 100.0 * 6.369263172149658
Epoch 340, val loss: 1.4450773000717163
Epoch 350, training loss: 637.81005859375 = 1.3666893243789673 + 100.0 * 6.364433765411377
Epoch 350, val loss: 1.4218194484710693
Epoch 360, training loss: 637.3281860351562 = 1.3391711711883545 + 100.0 * 6.359889984130859
Epoch 360, val loss: 1.3989747762680054
Epoch 370, training loss: 636.9996948242188 = 1.3118724822998047 + 100.0 * 6.35687780380249
Epoch 370, val loss: 1.376700758934021
Epoch 380, training loss: 637.03271484375 = 1.2847431898117065 + 100.0 * 6.357479572296143
Epoch 380, val loss: 1.3545234203338623
Epoch 390, training loss: 636.4110107421875 = 1.2581082582473755 + 100.0 * 6.351528644561768
Epoch 390, val loss: 1.332949161529541
Epoch 400, training loss: 635.8253784179688 = 1.2317818403244019 + 100.0 * 6.345935821533203
Epoch 400, val loss: 1.3121120929718018
Epoch 410, training loss: 635.4140014648438 = 1.2060679197311401 + 100.0 * 6.3420796394348145
Epoch 410, val loss: 1.2919126749038696
Epoch 420, training loss: 635.4681396484375 = 1.1807222366333008 + 100.0 * 6.342874050140381
Epoch 420, val loss: 1.2721799612045288
Epoch 430, training loss: 634.9072265625 = 1.1558290719985962 + 100.0 * 6.3375139236450195
Epoch 430, val loss: 1.2532292604446411
Epoch 440, training loss: 634.6318969726562 = 1.131407380104065 + 100.0 * 6.335004806518555
Epoch 440, val loss: 1.2347657680511475
Epoch 450, training loss: 634.2536010742188 = 1.1074997186660767 + 100.0 * 6.331461429595947
Epoch 450, val loss: 1.2169846296310425
Epoch 460, training loss: 634.23388671875 = 1.08415949344635 + 100.0 * 6.3314971923828125
Epoch 460, val loss: 1.199762225151062
Epoch 470, training loss: 633.6598510742188 = 1.061096429824829 + 100.0 * 6.325987339019775
Epoch 470, val loss: 1.1831283569335938
Epoch 480, training loss: 633.500732421875 = 1.0386590957641602 + 100.0 * 6.324621200561523
Epoch 480, val loss: 1.1670318841934204
Epoch 490, training loss: 633.1113891601562 = 1.0166774988174438 + 100.0 * 6.320947170257568
Epoch 490, val loss: 1.151676893234253
Epoch 500, training loss: 633.1814575195312 = 0.995122492313385 + 100.0 * 6.321863174438477
Epoch 500, val loss: 1.1367796659469604
Epoch 510, training loss: 632.7088012695312 = 0.9738797545433044 + 100.0 * 6.317348957061768
Epoch 510, val loss: 1.1224548816680908
Epoch 520, training loss: 633.2118530273438 = 0.9530103802680969 + 100.0 * 6.3225884437561035
Epoch 520, val loss: 1.1084895133972168
Epoch 530, training loss: 632.2623901367188 = 0.9325969815254211 + 100.0 * 6.313297748565674
Epoch 530, val loss: 1.0951592922210693
Epoch 540, training loss: 631.9767456054688 = 0.9126134514808655 + 100.0 * 6.310641765594482
Epoch 540, val loss: 1.0824460983276367
Epoch 550, training loss: 631.7422485351562 = 0.8929881453514099 + 100.0 * 6.308492660522461
Epoch 550, val loss: 1.0703126192092896
Epoch 560, training loss: 631.559326171875 = 0.8737112283706665 + 100.0 * 6.306856155395508
Epoch 560, val loss: 1.0586864948272705
Epoch 570, training loss: 631.6661987304688 = 0.8546171188354492 + 100.0 * 6.3081159591674805
Epoch 570, val loss: 1.047120213508606
Epoch 580, training loss: 631.1946411132812 = 0.8359174132347107 + 100.0 * 6.303586959838867
Epoch 580, val loss: 1.036324143409729
Epoch 590, training loss: 630.9241943359375 = 0.8175278902053833 + 100.0 * 6.301066875457764
Epoch 590, val loss: 1.0260100364685059
Epoch 600, training loss: 630.7487182617188 = 0.7995368838310242 + 100.0 * 6.2994914054870605
Epoch 600, val loss: 1.0162303447723389
Epoch 610, training loss: 631.5493774414062 = 0.7817404866218567 + 100.0 * 6.307676315307617
Epoch 610, val loss: 1.0067170858383179
Epoch 620, training loss: 630.6256713867188 = 0.7643553614616394 + 100.0 * 6.29861307144165
Epoch 620, val loss: 0.9980465173721313
Epoch 630, training loss: 630.3187866210938 = 0.7471001744270325 + 100.0 * 6.295717239379883
Epoch 630, val loss: 0.989449143409729
Epoch 640, training loss: 630.1532592773438 = 0.7302759289741516 + 100.0 * 6.294229984283447
Epoch 640, val loss: 0.9815225005149841
Epoch 650, training loss: 630.441650390625 = 0.7137168049812317 + 100.0 * 6.297279357910156
Epoch 650, val loss: 0.9740387797355652
Epoch 660, training loss: 629.9810180664062 = 0.6974191069602966 + 100.0 * 6.292835712432861
Epoch 660, val loss: 0.9668142795562744
Epoch 670, training loss: 629.7339477539062 = 0.6813307404518127 + 100.0 * 6.290526390075684
Epoch 670, val loss: 0.9600353240966797
Epoch 680, training loss: 630.7640380859375 = 0.6655248999595642 + 100.0 * 6.300985336303711
Epoch 680, val loss: 0.9536803364753723
Epoch 690, training loss: 629.46484375 = 0.6500480771064758 + 100.0 * 6.288147926330566
Epoch 690, val loss: 0.9476897716522217
Epoch 700, training loss: 629.3248901367188 = 0.6347805857658386 + 100.0 * 6.286901473999023
Epoch 700, val loss: 0.942217230796814
Epoch 710, training loss: 629.0609130859375 = 0.6198589205741882 + 100.0 * 6.28441047668457
Epoch 710, val loss: 0.937121570110321
Epoch 720, training loss: 629.1843872070312 = 0.6052145957946777 + 100.0 * 6.285791397094727
Epoch 720, val loss: 0.9322001338005066
Epoch 730, training loss: 629.0048217773438 = 0.5908071398735046 + 100.0 * 6.284140110015869
Epoch 730, val loss: 0.9280569553375244
Epoch 740, training loss: 628.8125 = 0.5766074657440186 + 100.0 * 6.2823591232299805
Epoch 740, val loss: 0.9238269329071045
Epoch 750, training loss: 629.576171875 = 0.5628079771995544 + 100.0 * 6.290133953094482
Epoch 750, val loss: 0.9202851057052612
Epoch 760, training loss: 628.5191040039062 = 0.5490262508392334 + 100.0 * 6.279701232910156
Epoch 760, val loss: 0.9167227149009705
Epoch 770, training loss: 628.4517211914062 = 0.5356336236000061 + 100.0 * 6.279160976409912
Epoch 770, val loss: 0.9135360717773438
Epoch 780, training loss: 628.194091796875 = 0.5225624442100525 + 100.0 * 6.276715278625488
Epoch 780, val loss: 0.9108877778053284
Epoch 790, training loss: 628.3494262695312 = 0.5097460746765137 + 100.0 * 6.2783966064453125
Epoch 790, val loss: 0.9084860682487488
Epoch 800, training loss: 628.1864624023438 = 0.49712952971458435 + 100.0 * 6.276893615722656
Epoch 800, val loss: 0.9066112041473389
Epoch 810, training loss: 628.181884765625 = 0.4846920073032379 + 100.0 * 6.27697229385376
Epoch 810, val loss: 0.9043993353843689
Epoch 820, training loss: 627.9590454101562 = 0.4726174473762512 + 100.0 * 6.274864196777344
Epoch 820, val loss: 0.903050422668457
Epoch 830, training loss: 627.754638671875 = 0.4608469307422638 + 100.0 * 6.272937774658203
Epoch 830, val loss: 0.9021328091621399
Epoch 840, training loss: 627.908447265625 = 0.44927266240119934 + 100.0 * 6.274591445922852
Epoch 840, val loss: 0.9012649059295654
Epoch 850, training loss: 627.801513671875 = 0.4379013776779175 + 100.0 * 6.2736358642578125
Epoch 850, val loss: 0.9002211689949036
Epoch 860, training loss: 627.6248779296875 = 0.4269002377986908 + 100.0 * 6.271980285644531
Epoch 860, val loss: 0.9001409411430359
Epoch 870, training loss: 627.6292114257812 = 0.4159749448299408 + 100.0 * 6.272132873535156
Epoch 870, val loss: 0.8997193574905396
Epoch 880, training loss: 627.2808837890625 = 0.4054894745349884 + 100.0 * 6.268753528594971
Epoch 880, val loss: 0.9003517627716064
Epoch 890, training loss: 627.1349487304688 = 0.3951449990272522 + 100.0 * 6.267397880554199
Epoch 890, val loss: 0.9006454944610596
Epoch 900, training loss: 627.2059326171875 = 0.38508400321006775 + 100.0 * 6.2682085037231445
Epoch 900, val loss: 0.9014871716499329
Epoch 910, training loss: 627.3359985351562 = 0.3752611577510834 + 100.0 * 6.2696075439453125
Epoch 910, val loss: 0.9025534391403198
Epoch 920, training loss: 627.2705688476562 = 0.3655336797237396 + 100.0 * 6.269050121307373
Epoch 920, val loss: 0.902798593044281
Epoch 930, training loss: 626.8917846679688 = 0.3561674654483795 + 100.0 * 6.265356540679932
Epoch 930, val loss: 0.9045554399490356
Epoch 940, training loss: 626.8157348632812 = 0.3470386266708374 + 100.0 * 6.2646870613098145
Epoch 940, val loss: 0.9059327244758606
Epoch 950, training loss: 626.8417358398438 = 0.33812057971954346 + 100.0 * 6.265036106109619
Epoch 950, val loss: 0.9075607061386108
Epoch 960, training loss: 626.8078002929688 = 0.3294227719306946 + 100.0 * 6.26478385925293
Epoch 960, val loss: 0.9095382690429688
Epoch 970, training loss: 626.4945068359375 = 0.32100823521614075 + 100.0 * 6.261735439300537
Epoch 970, val loss: 0.911804735660553
Epoch 980, training loss: 626.482177734375 = 0.31279829144477844 + 100.0 * 6.261693477630615
Epoch 980, val loss: 0.9140689373016357
Epoch 990, training loss: 626.9871215820312 = 0.3047579824924469 + 100.0 * 6.266823768615723
Epoch 990, val loss: 0.9165008068084717
Epoch 1000, training loss: 626.3193359375 = 0.29690149426460266 + 100.0 * 6.260224342346191
Epoch 1000, val loss: 0.9189918637275696
Epoch 1010, training loss: 626.22314453125 = 0.28931161761283875 + 100.0 * 6.25933837890625
Epoch 1010, val loss: 0.9217824935913086
Epoch 1020, training loss: 626.3129272460938 = 0.28190845251083374 + 100.0 * 6.260310173034668
Epoch 1020, val loss: 0.9246613383293152
Epoch 1030, training loss: 626.2874755859375 = 0.27473220229148865 + 100.0 * 6.260127544403076
Epoch 1030, val loss: 0.9281789660453796
Epoch 1040, training loss: 626.0299682617188 = 0.26775944232940674 + 100.0 * 6.257621765136719
Epoch 1040, val loss: 0.9314324259757996
Epoch 1050, training loss: 626.3613891601562 = 0.26090800762176514 + 100.0 * 6.261004447937012
Epoch 1050, val loss: 0.9345199465751648
Epoch 1060, training loss: 626.1669311523438 = 0.25425949692726135 + 100.0 * 6.259126663208008
Epoch 1060, val loss: 0.9384822845458984
Epoch 1070, training loss: 625.8226928710938 = 0.24780479073524475 + 100.0 * 6.255748748779297
Epoch 1070, val loss: 0.9421923160552979
Epoch 1080, training loss: 625.6738891601562 = 0.24156533181667328 + 100.0 * 6.2543230056762695
Epoch 1080, val loss: 0.9461633563041687
Epoch 1090, training loss: 626.6009521484375 = 0.2355177253484726 + 100.0 * 6.2636542320251465
Epoch 1090, val loss: 0.9500118494033813
Epoch 1100, training loss: 625.8615112304688 = 0.22945256531238556 + 100.0 * 6.256320953369141
Epoch 1100, val loss: 0.95391845703125
Epoch 1110, training loss: 625.4783325195312 = 0.22365882992744446 + 100.0 * 6.252547264099121
Epoch 1110, val loss: 0.9580307602882385
Epoch 1120, training loss: 625.5692749023438 = 0.21801242232322693 + 100.0 * 6.253512382507324
Epoch 1120, val loss: 0.9620333313941956
Epoch 1130, training loss: 625.6544799804688 = 0.21250002086162567 + 100.0 * 6.254419803619385
Epoch 1130, val loss: 0.9660503268241882
Epoch 1140, training loss: 625.5551147460938 = 0.20711596310138702 + 100.0 * 6.253480434417725
Epoch 1140, val loss: 0.9703916311264038
Epoch 1150, training loss: 625.2301635742188 = 0.20191088318824768 + 100.0 * 6.2502827644348145
Epoch 1150, val loss: 0.974998414516449
Epoch 1160, training loss: 625.20068359375 = 0.19685259461402893 + 100.0 * 6.2500386238098145
Epoch 1160, val loss: 0.9794600009918213
Epoch 1170, training loss: 625.1844482421875 = 0.19196908175945282 + 100.0 * 6.249924659729004
Epoch 1170, val loss: 0.9840412139892578
Epoch 1180, training loss: 625.8524780273438 = 0.1871996521949768 + 100.0 * 6.25665283203125
Epoch 1180, val loss: 0.9886396527290344
Epoch 1190, training loss: 625.22900390625 = 0.18240152299404144 + 100.0 * 6.250466346740723
Epoch 1190, val loss: 0.9927350282669067
Epoch 1200, training loss: 624.9935302734375 = 0.17784622311592102 + 100.0 * 6.248156547546387
Epoch 1200, val loss: 0.9975581169128418
Epoch 1210, training loss: 625.1075439453125 = 0.17341141402721405 + 100.0 * 6.2493414878845215
Epoch 1210, val loss: 1.0020884275436401
Epoch 1220, training loss: 624.90087890625 = 0.16907021403312683 + 100.0 * 6.247318267822266
Epoch 1220, val loss: 1.0067120790481567
Epoch 1230, training loss: 624.8567504882812 = 0.16488312184810638 + 100.0 * 6.246918678283691
Epoch 1230, val loss: 1.0117032527923584
Epoch 1240, training loss: 624.9652099609375 = 0.16082875430583954 + 100.0 * 6.248044013977051
Epoch 1240, val loss: 1.0164415836334229
Epoch 1250, training loss: 624.9443969726562 = 0.15683376789093018 + 100.0 * 6.247875213623047
Epoch 1250, val loss: 1.021196722984314
Epoch 1260, training loss: 624.6834716796875 = 0.15289609134197235 + 100.0 * 6.245306015014648
Epoch 1260, val loss: 1.025834083557129
Epoch 1270, training loss: 624.7625122070312 = 0.14912600815296173 + 100.0 * 6.246133804321289
Epoch 1270, val loss: 1.0308107137680054
Epoch 1280, training loss: 624.5770263671875 = 0.14547109603881836 + 100.0 * 6.2443156242370605
Epoch 1280, val loss: 1.035657286643982
Epoch 1290, training loss: 624.85107421875 = 0.14195045828819275 + 100.0 * 6.247091293334961
Epoch 1290, val loss: 1.0408607721328735
Epoch 1300, training loss: 624.4782104492188 = 0.1384131908416748 + 100.0 * 6.2433977127075195
Epoch 1300, val loss: 1.0452507734298706
Epoch 1310, training loss: 624.4871826171875 = 0.13500982522964478 + 100.0 * 6.243521690368652
Epoch 1310, val loss: 1.0500763654708862
Epoch 1320, training loss: 624.5892944335938 = 0.13175146281719208 + 100.0 * 6.244575500488281
Epoch 1320, val loss: 1.0552594661712646
Epoch 1330, training loss: 624.7178344726562 = 0.12852303683757782 + 100.0 * 6.2458930015563965
Epoch 1330, val loss: 1.0596710443496704
Epoch 1340, training loss: 624.7807006835938 = 0.12542091310024261 + 100.0 * 6.246552467346191
Epoch 1340, val loss: 1.064719557762146
Epoch 1350, training loss: 624.3350219726562 = 0.12234247475862503 + 100.0 * 6.242126941680908
Epoch 1350, val loss: 1.0697705745697021
Epoch 1360, training loss: 624.1596069335938 = 0.11942310631275177 + 100.0 * 6.240401744842529
Epoch 1360, val loss: 1.0748536586761475
Epoch 1370, training loss: 624.1394653320312 = 0.11657660454511642 + 100.0 * 6.240228652954102
Epoch 1370, val loss: 1.079926609992981
Epoch 1380, training loss: 624.5281982421875 = 0.11383962631225586 + 100.0 * 6.244143486022949
Epoch 1380, val loss: 1.0852086544036865
Epoch 1390, training loss: 625.0004272460938 = 0.11106368154287338 + 100.0 * 6.2488932609558105
Epoch 1390, val loss: 1.089611291885376
Epoch 1400, training loss: 624.3446044921875 = 0.10842065513134003 + 100.0 * 6.242362022399902
Epoch 1400, val loss: 1.0948482751846313
Epoch 1410, training loss: 623.9663696289062 = 0.10582290589809418 + 100.0 * 6.238605499267578
Epoch 1410, val loss: 1.0998057126998901
Epoch 1420, training loss: 623.90869140625 = 0.10334677994251251 + 100.0 * 6.238053798675537
Epoch 1420, val loss: 1.1046816110610962
Epoch 1430, training loss: 623.8713989257812 = 0.10096978396177292 + 100.0 * 6.237704277038574
Epoch 1430, val loss: 1.1098800897598267
Epoch 1440, training loss: 624.792724609375 = 0.09867216646671295 + 100.0 * 6.2469401359558105
Epoch 1440, val loss: 1.1150627136230469
Epoch 1450, training loss: 624.5798950195312 = 0.09631918370723724 + 100.0 * 6.24483585357666
Epoch 1450, val loss: 1.1199873685836792
Epoch 1460, training loss: 623.8610229492188 = 0.09406020492315292 + 100.0 * 6.237669467926025
Epoch 1460, val loss: 1.1246947050094604
Epoch 1470, training loss: 623.7227783203125 = 0.09189663827419281 + 100.0 * 6.236308574676514
Epoch 1470, val loss: 1.1298075914382935
Epoch 1480, training loss: 623.7421875 = 0.0898127481341362 + 100.0 * 6.2365241050720215
Epoch 1480, val loss: 1.13491952419281
Epoch 1490, training loss: 624.3963623046875 = 0.08777784556150436 + 100.0 * 6.243085861206055
Epoch 1490, val loss: 1.1396244764328003
Epoch 1500, training loss: 623.9304809570312 = 0.08577827364206314 + 100.0 * 6.238447189331055
Epoch 1500, val loss: 1.1446712017059326
Epoch 1510, training loss: 623.6567993164062 = 0.08383673429489136 + 100.0 * 6.235729217529297
Epoch 1510, val loss: 1.1497119665145874
Epoch 1520, training loss: 623.6544799804688 = 0.08196684718132019 + 100.0 * 6.235724925994873
Epoch 1520, val loss: 1.1546787023544312
Epoch 1530, training loss: 624.1671752929688 = 0.08014869689941406 + 100.0 * 6.240870475769043
Epoch 1530, val loss: 1.1597570180892944
Epoch 1540, training loss: 623.8800659179688 = 0.078359454870224 + 100.0 * 6.2380170822143555
Epoch 1540, val loss: 1.1649259328842163
Epoch 1550, training loss: 623.6182250976562 = 0.07660853117704391 + 100.0 * 6.235416412353516
Epoch 1550, val loss: 1.1695448160171509
Epoch 1560, training loss: 623.6343383789062 = 0.07493443787097931 + 100.0 * 6.235593795776367
Epoch 1560, val loss: 1.1743407249450684
Epoch 1570, training loss: 623.6187133789062 = 0.07330041378736496 + 100.0 * 6.235454082489014
Epoch 1570, val loss: 1.1793502569198608
Epoch 1580, training loss: 623.49755859375 = 0.07172878831624985 + 100.0 * 6.23425817489624
Epoch 1580, val loss: 1.184674620628357
Epoch 1590, training loss: 623.4432373046875 = 0.07018126547336578 + 100.0 * 6.233730316162109
Epoch 1590, val loss: 1.18923819065094
Epoch 1600, training loss: 623.8612060546875 = 0.06871341913938522 + 100.0 * 6.237925052642822
Epoch 1600, val loss: 1.1943275928497314
Epoch 1610, training loss: 623.2974243164062 = 0.06721199303865433 + 100.0 * 6.232302188873291
Epoch 1610, val loss: 1.199230670928955
Epoch 1620, training loss: 623.4679565429688 = 0.06578639149665833 + 100.0 * 6.2340216636657715
Epoch 1620, val loss: 1.204001784324646
Epoch 1630, training loss: 623.4015502929688 = 0.06441888213157654 + 100.0 * 6.233371734619141
Epoch 1630, val loss: 1.2089253664016724
Epoch 1640, training loss: 623.3104248046875 = 0.06307139247655869 + 100.0 * 6.232473850250244
Epoch 1640, val loss: 1.2142175436019897
Epoch 1650, training loss: 623.2068481445312 = 0.06175883486866951 + 100.0 * 6.231451034545898
Epoch 1650, val loss: 1.2189682722091675
Epoch 1660, training loss: 623.538818359375 = 0.06049710139632225 + 100.0 * 6.234783172607422
Epoch 1660, val loss: 1.2238680124282837
Epoch 1670, training loss: 623.3253784179688 = 0.0592639222741127 + 100.0 * 6.232661247253418
Epoch 1670, val loss: 1.2288447618484497
Epoch 1680, training loss: 623.2050170898438 = 0.05803195387125015 + 100.0 * 6.231469631195068
Epoch 1680, val loss: 1.2335542440414429
Epoch 1690, training loss: 623.364501953125 = 0.05686439573764801 + 100.0 * 6.233076095581055
Epoch 1690, val loss: 1.2387770414352417
Epoch 1700, training loss: 623.035888671875 = 0.05569871887564659 + 100.0 * 6.229801654815674
Epoch 1700, val loss: 1.2434216737747192
Epoch 1710, training loss: 623.1434326171875 = 0.05458987131714821 + 100.0 * 6.230888366699219
Epoch 1710, val loss: 1.2481893301010132
Epoch 1720, training loss: 623.30322265625 = 0.0535048246383667 + 100.0 * 6.232496738433838
Epoch 1720, val loss: 1.2531030178070068
Epoch 1730, training loss: 623.2293701171875 = 0.05244353041052818 + 100.0 * 6.231769561767578
Epoch 1730, val loss: 1.2576693296432495
Epoch 1740, training loss: 623.1980590820312 = 0.051403991878032684 + 100.0 * 6.231466770172119
Epoch 1740, val loss: 1.2620638608932495
Epoch 1750, training loss: 623.0597534179688 = 0.050391871482133865 + 100.0 * 6.230093479156494
Epoch 1750, val loss: 1.2671926021575928
Epoch 1760, training loss: 622.888916015625 = 0.04942706599831581 + 100.0 * 6.228394508361816
Epoch 1760, val loss: 1.2723686695098877
Epoch 1770, training loss: 622.84521484375 = 0.04847913607954979 + 100.0 * 6.227967262268066
Epoch 1770, val loss: 1.276881456375122
Epoch 1780, training loss: 623.4443969726562 = 0.047557905316352844 + 100.0 * 6.233968257904053
Epoch 1780, val loss: 1.281548023223877
Epoch 1790, training loss: 623.149658203125 = 0.04663925617933273 + 100.0 * 6.231029987335205
Epoch 1790, val loss: 1.2861963510513306
Epoch 1800, training loss: 623.1490478515625 = 0.04574361443519592 + 100.0 * 6.2310333251953125
Epoch 1800, val loss: 1.2908895015716553
Epoch 1810, training loss: 623.0379028320312 = 0.04488702490925789 + 100.0 * 6.229930400848389
Epoch 1810, val loss: 1.2956069707870483
Epoch 1820, training loss: 622.7719116210938 = 0.04404705762863159 + 100.0 * 6.227278709411621
Epoch 1820, val loss: 1.2999351024627686
Epoch 1830, training loss: 622.6422119140625 = 0.04322899878025055 + 100.0 * 6.225990295410156
Epoch 1830, val loss: 1.304758071899414
Epoch 1840, training loss: 623.0661010742188 = 0.042441271245479584 + 100.0 * 6.230236530303955
Epoch 1840, val loss: 1.309097409248352
Epoch 1850, training loss: 622.9133911132812 = 0.04165826737880707 + 100.0 * 6.228716850280762
Epoch 1850, val loss: 1.313581943511963
Epoch 1860, training loss: 622.8709716796875 = 0.040890298783779144 + 100.0 * 6.22830057144165
Epoch 1860, val loss: 1.3185135126113892
Epoch 1870, training loss: 622.7068481445312 = 0.04014783352613449 + 100.0 * 6.2266669273376465
Epoch 1870, val loss: 1.3229076862335205
Epoch 1880, training loss: 622.6043701171875 = 0.03943014144897461 + 100.0 * 6.225649356842041
Epoch 1880, val loss: 1.327573299407959
Epoch 1890, training loss: 623.1397094726562 = 0.03873607516288757 + 100.0 * 6.231009483337402
Epoch 1890, val loss: 1.3322359323501587
Epoch 1900, training loss: 622.7672119140625 = 0.0380399115383625 + 100.0 * 6.227292060852051
Epoch 1900, val loss: 1.336395502090454
Epoch 1910, training loss: 622.4885864257812 = 0.03736808896064758 + 100.0 * 6.224512577056885
Epoch 1910, val loss: 1.3408100605010986
Epoch 1920, training loss: 622.4793701171875 = 0.03671583533287048 + 100.0 * 6.22442626953125
Epoch 1920, val loss: 1.3451192378997803
Epoch 1930, training loss: 623.5474243164062 = 0.036083340644836426 + 100.0 * 6.235113620758057
Epoch 1930, val loss: 1.3492974042892456
Epoch 1940, training loss: 622.7894897460938 = 0.03545421361923218 + 100.0 * 6.227540493011475
Epoch 1940, val loss: 1.3543281555175781
Epoch 1950, training loss: 622.4663696289062 = 0.03482867404818535 + 100.0 * 6.224315166473389
Epoch 1950, val loss: 1.3580071926116943
Epoch 1960, training loss: 622.366455078125 = 0.034249644726514816 + 100.0 * 6.22332239151001
Epoch 1960, val loss: 1.362652063369751
Epoch 1970, training loss: 622.3789672851562 = 0.033675115555524826 + 100.0 * 6.223452568054199
Epoch 1970, val loss: 1.3666279315948486
Epoch 1980, training loss: 623.017578125 = 0.03312927111983299 + 100.0 * 6.229844093322754
Epoch 1980, val loss: 1.3708481788635254
Epoch 1990, training loss: 622.97314453125 = 0.032569486647844315 + 100.0 * 6.229405403137207
Epoch 1990, val loss: 1.375617504119873
Epoch 2000, training loss: 622.5834350585938 = 0.03200112283229828 + 100.0 * 6.2255144119262695
Epoch 2000, val loss: 1.3791587352752686
Epoch 2010, training loss: 622.2572631835938 = 0.0314580537378788 + 100.0 * 6.222258567810059
Epoch 2010, val loss: 1.383385181427002
Epoch 2020, training loss: 622.2448120117188 = 0.030949555337429047 + 100.0 * 6.222138404846191
Epoch 2020, val loss: 1.3877804279327393
Epoch 2030, training loss: 622.3455200195312 = 0.030448423698544502 + 100.0 * 6.223150730133057
Epoch 2030, val loss: 1.3919821977615356
Epoch 2040, training loss: 622.5885620117188 = 0.029953965917229652 + 100.0 * 6.2255859375
Epoch 2040, val loss: 1.3959459066390991
Epoch 2050, training loss: 622.7031860351562 = 0.029483752325177193 + 100.0 * 6.226737022399902
Epoch 2050, val loss: 1.399773359298706
Epoch 2060, training loss: 622.3951416015625 = 0.0289949718862772 + 100.0 * 6.223661422729492
Epoch 2060, val loss: 1.4044352769851685
Epoch 2070, training loss: 622.2371215820312 = 0.028528977185487747 + 100.0 * 6.222086429595947
Epoch 2070, val loss: 1.4079557657241821
Epoch 2080, training loss: 622.3128662109375 = 0.02807384915649891 + 100.0 * 6.222847938537598
Epoch 2080, val loss: 1.412135124206543
Epoch 2090, training loss: 622.21533203125 = 0.027639616280794144 + 100.0 * 6.221877098083496
Epoch 2090, val loss: 1.416115641593933
Epoch 2100, training loss: 622.1068725585938 = 0.027210570871829987 + 100.0 * 6.220796585083008
Epoch 2100, val loss: 1.4201102256774902
Epoch 2110, training loss: 622.142333984375 = 0.02678767591714859 + 100.0 * 6.221155166625977
Epoch 2110, val loss: 1.4239470958709717
Epoch 2120, training loss: 622.8275756835938 = 0.026368767023086548 + 100.0 * 6.2280120849609375
Epoch 2120, val loss: 1.427495002746582
Epoch 2130, training loss: 622.3043823242188 = 0.025970865041017532 + 100.0 * 6.22278356552124
Epoch 2130, val loss: 1.431707739830017
Epoch 2140, training loss: 622.0846557617188 = 0.025562701746821404 + 100.0 * 6.220590591430664
Epoch 2140, val loss: 1.4355818033218384
Epoch 2150, training loss: 622.115478515625 = 0.025182005017995834 + 100.0 * 6.220902919769287
Epoch 2150, val loss: 1.4394214153289795
Epoch 2160, training loss: 622.2647094726562 = 0.02480543591082096 + 100.0 * 6.2223992347717285
Epoch 2160, val loss: 1.442962408065796
Epoch 2170, training loss: 622.0488891601562 = 0.024433918297290802 + 100.0 * 6.220244884490967
Epoch 2170, val loss: 1.4467226266860962
Epoch 2180, training loss: 622.2155151367188 = 0.02407650463283062 + 100.0 * 6.221914768218994
Epoch 2180, val loss: 1.450410008430481
Epoch 2190, training loss: 622.3147583007812 = 0.023721925914287567 + 100.0 * 6.222909927368164
Epoch 2190, val loss: 1.4546154737472534
Epoch 2200, training loss: 621.9735107421875 = 0.023364583030343056 + 100.0 * 6.219501495361328
Epoch 2200, val loss: 1.4581811428070068
Epoch 2210, training loss: 621.862060546875 = 0.023028695955872536 + 100.0 * 6.218390464782715
Epoch 2210, val loss: 1.4619358777999878
Epoch 2220, training loss: 621.8473510742188 = 0.022700583562254906 + 100.0 * 6.2182464599609375
Epoch 2220, val loss: 1.4656052589416504
Epoch 2230, training loss: 622.2528686523438 = 0.02237822487950325 + 100.0 * 6.222304821014404
Epoch 2230, val loss: 1.4691495895385742
Epoch 2240, training loss: 622.3134155273438 = 0.02204722911119461 + 100.0 * 6.22291374206543
Epoch 2240, val loss: 1.4721447229385376
Epoch 2250, training loss: 621.86865234375 = 0.021736007183790207 + 100.0 * 6.218469142913818
Epoch 2250, val loss: 1.4761879444122314
Epoch 2260, training loss: 621.7692260742188 = 0.021425887942314148 + 100.0 * 6.217477798461914
Epoch 2260, val loss: 1.479504108428955
Epoch 2270, training loss: 621.811279296875 = 0.021135063841938972 + 100.0 * 6.217901706695557
Epoch 2270, val loss: 1.4832947254180908
Epoch 2280, training loss: 622.5767211914062 = 0.02084672823548317 + 100.0 * 6.225558280944824
Epoch 2280, val loss: 1.487104058265686
Epoch 2290, training loss: 622.0759887695312 = 0.02055477723479271 + 100.0 * 6.220554351806641
Epoch 2290, val loss: 1.489803433418274
Epoch 2300, training loss: 621.893310546875 = 0.020269198343157768 + 100.0 * 6.218730449676514
Epoch 2300, val loss: 1.4934718608856201
Epoch 2310, training loss: 622.0814819335938 = 0.01999872922897339 + 100.0 * 6.220614910125732
Epoch 2310, val loss: 1.496641755104065
Epoch 2320, training loss: 621.7700805664062 = 0.019722001627087593 + 100.0 * 6.217503547668457
Epoch 2320, val loss: 1.500022053718567
Epoch 2330, training loss: 621.8021850585938 = 0.01945766992866993 + 100.0 * 6.217827320098877
Epoch 2330, val loss: 1.5032597780227661
Epoch 2340, training loss: 622.0609741210938 = 0.01920236647129059 + 100.0 * 6.220417499542236
Epoch 2340, val loss: 1.5067294836044312
Epoch 2350, training loss: 621.8324584960938 = 0.018949048593640327 + 100.0 * 6.218135356903076
Epoch 2350, val loss: 1.510040044784546
Epoch 2360, training loss: 621.742919921875 = 0.018698185682296753 + 100.0 * 6.217242240905762
Epoch 2360, val loss: 1.5132474899291992
Epoch 2370, training loss: 621.6200561523438 = 0.01845400221645832 + 100.0 * 6.2160162925720215
Epoch 2370, val loss: 1.516339659690857
Epoch 2380, training loss: 621.5728759765625 = 0.018213240429759026 + 100.0 * 6.215546607971191
Epoch 2380, val loss: 1.5195063352584839
Epoch 2390, training loss: 621.8990478515625 = 0.017988936975598335 + 100.0 * 6.218810558319092
Epoch 2390, val loss: 1.5224086046218872
Epoch 2400, training loss: 621.9188232421875 = 0.01776512898504734 + 100.0 * 6.219010829925537
Epoch 2400, val loss: 1.52583909034729
Epoch 2410, training loss: 621.7974243164062 = 0.017527682706713676 + 100.0 * 6.217799186706543
Epoch 2410, val loss: 1.5290321111679077
Epoch 2420, training loss: 621.5464477539062 = 0.017299233004450798 + 100.0 * 6.215291500091553
Epoch 2420, val loss: 1.5319093465805054
Epoch 2430, training loss: 621.7278442382812 = 0.017091110348701477 + 100.0 * 6.217107772827148
Epoch 2430, val loss: 1.5353695154190063
Epoch 2440, training loss: 622.036865234375 = 0.01688285358250141 + 100.0 * 6.2201995849609375
Epoch 2440, val loss: 1.5384211540222168
Epoch 2450, training loss: 621.6262817382812 = 0.016656745225191116 + 100.0 * 6.216095924377441
Epoch 2450, val loss: 1.5408891439437866
Epoch 2460, training loss: 621.4390869140625 = 0.016455810517072678 + 100.0 * 6.214226245880127
Epoch 2460, val loss: 1.544163703918457
Epoch 2470, training loss: 621.591552734375 = 0.0162623580545187 + 100.0 * 6.215753078460693
Epoch 2470, val loss: 1.5472570657730103
Epoch 2480, training loss: 622.071044921875 = 0.01606876403093338 + 100.0 * 6.220550060272217
Epoch 2480, val loss: 1.5498815774917603
Epoch 2490, training loss: 621.6453247070312 = 0.01586811989545822 + 100.0 * 6.216294765472412
Epoch 2490, val loss: 1.5528864860534668
Epoch 2500, training loss: 621.4798583984375 = 0.0156813096255064 + 100.0 * 6.214641571044922
Epoch 2500, val loss: 1.5558172464370728
Epoch 2510, training loss: 621.667236328125 = 0.015507419593632221 + 100.0 * 6.216517448425293
Epoch 2510, val loss: 1.5585635900497437
Epoch 2520, training loss: 621.6969604492188 = 0.015321629121899605 + 100.0 * 6.216816425323486
Epoch 2520, val loss: 1.5613429546356201
Epoch 2530, training loss: 621.6975708007812 = 0.015131782740354538 + 100.0 * 6.216824054718018
Epoch 2530, val loss: 1.5643184185028076
Epoch 2540, training loss: 621.732421875 = 0.014963270165026188 + 100.0 * 6.217174530029297
Epoch 2540, val loss: 1.5669724941253662
Epoch 2550, training loss: 621.449951171875 = 0.014779388904571533 + 100.0 * 6.214351654052734
Epoch 2550, val loss: 1.5696793794631958
Epoch 2560, training loss: 621.5198364257812 = 0.014617090113461018 + 100.0 * 6.215052604675293
Epoch 2560, val loss: 1.5721598863601685
Epoch 2570, training loss: 621.3588256835938 = 0.014448677189648151 + 100.0 * 6.213443756103516
Epoch 2570, val loss: 1.5750313997268677
Epoch 2580, training loss: 621.6016845703125 = 0.0142909474670887 + 100.0 * 6.215874195098877
Epoch 2580, val loss: 1.5777878761291504
Epoch 2590, training loss: 621.3804931640625 = 0.01413011271506548 + 100.0 * 6.213663578033447
Epoch 2590, val loss: 1.5802119970321655
Epoch 2600, training loss: 622.0470581054688 = 0.013972587883472443 + 100.0 * 6.220330715179443
Epoch 2600, val loss: 1.5829205513000488
Epoch 2610, training loss: 621.5304565429688 = 0.01381555013358593 + 100.0 * 6.2151665687561035
Epoch 2610, val loss: 1.5858649015426636
Epoch 2620, training loss: 621.32275390625 = 0.013661458157002926 + 100.0 * 6.213090896606445
Epoch 2620, val loss: 1.5885668992996216
Epoch 2630, training loss: 621.2484130859375 = 0.013512627221643925 + 100.0 * 6.2123494148254395
Epoch 2630, val loss: 1.5911887884140015
Epoch 2640, training loss: 621.386474609375 = 0.013375148177146912 + 100.0 * 6.213731288909912
Epoch 2640, val loss: 1.593757152557373
Epoch 2650, training loss: 621.6390380859375 = 0.013231877237558365 + 100.0 * 6.2162580490112305
Epoch 2650, val loss: 1.5960687398910522
Epoch 2660, training loss: 621.2723388671875 = 0.013075378723442554 + 100.0 * 6.212592601776123
Epoch 2660, val loss: 1.5982890129089355
Epoch 2670, training loss: 621.2171630859375 = 0.012935979291796684 + 100.0 * 6.212042331695557
Epoch 2670, val loss: 1.6010948419570923
Epoch 2680, training loss: 621.3309326171875 = 0.012804086320102215 + 100.0 * 6.213181018829346
Epoch 2680, val loss: 1.6035763025283813
Epoch 2690, training loss: 621.4603881835938 = 0.012673096731305122 + 100.0 * 6.214477062225342
Epoch 2690, val loss: 1.606384515762329
Epoch 2700, training loss: 621.4248657226562 = 0.01254140492528677 + 100.0 * 6.214123725891113
Epoch 2700, val loss: 1.609092116355896
Epoch 2710, training loss: 621.32177734375 = 0.012407174333930016 + 100.0 * 6.2130937576293945
Epoch 2710, val loss: 1.6107498407363892
Epoch 2720, training loss: 621.423095703125 = 0.012281130999326706 + 100.0 * 6.214107990264893
Epoch 2720, val loss: 1.6132616996765137
Epoch 2730, training loss: 621.4607543945312 = 0.012153739109635353 + 100.0 * 6.214486122131348
Epoch 2730, val loss: 1.6155641078948975
Epoch 2740, training loss: 621.1057739257812 = 0.012030619196593761 + 100.0 * 6.2109375
Epoch 2740, val loss: 1.6180386543273926
Epoch 2750, training loss: 621.1776123046875 = 0.011916370131075382 + 100.0 * 6.2116570472717285
Epoch 2750, val loss: 1.620590090751648
Epoch 2760, training loss: 621.3917236328125 = 0.011802603490650654 + 100.0 * 6.213799476623535
Epoch 2760, val loss: 1.622788667678833
Epoch 2770, training loss: 621.1632690429688 = 0.011678542010486126 + 100.0 * 6.2115159034729
Epoch 2770, val loss: 1.6247013807296753
Epoch 2780, training loss: 621.3339233398438 = 0.011562475003302097 + 100.0 * 6.213223934173584
Epoch 2780, val loss: 1.6267821788787842
Epoch 2790, training loss: 621.612548828125 = 0.011459320783615112 + 100.0 * 6.216010570526123
Epoch 2790, val loss: 1.6294633150100708
Epoch 2800, training loss: 621.2369384765625 = 0.011336066760122776 + 100.0 * 6.212255954742432
Epoch 2800, val loss: 1.6316901445388794
Epoch 2810, training loss: 621.0729370117188 = 0.011227333918213844 + 100.0 * 6.2106170654296875
Epoch 2810, val loss: 1.633795976638794
Epoch 2820, training loss: 621.0660400390625 = 0.011122629977762699 + 100.0 * 6.210549354553223
Epoch 2820, val loss: 1.6360827684402466
Epoch 2830, training loss: 621.3092651367188 = 0.011021899990737438 + 100.0 * 6.212982177734375
Epoch 2830, val loss: 1.6384329795837402
Epoch 2840, training loss: 621.0817260742188 = 0.010916514322161674 + 100.0 * 6.210708141326904
Epoch 2840, val loss: 1.640474557876587
Epoch 2850, training loss: 621.3069458007812 = 0.010810695588588715 + 100.0 * 6.212961196899414
Epoch 2850, val loss: 1.642768144607544
Epoch 2860, training loss: 621.5204467773438 = 0.010708780027925968 + 100.0 * 6.215097427368164
Epoch 2860, val loss: 1.6449649333953857
Epoch 2870, training loss: 621.0989990234375 = 0.01060816366225481 + 100.0 * 6.210884094238281
Epoch 2870, val loss: 1.646504282951355
Epoch 2880, training loss: 621.1182250976562 = 0.010511575266718864 + 100.0 * 6.2110772132873535
Epoch 2880, val loss: 1.6485868692398071
Epoch 2890, training loss: 621.1692504882812 = 0.01041644811630249 + 100.0 * 6.211587905883789
Epoch 2890, val loss: 1.6506282091140747
Epoch 2900, training loss: 620.9520263671875 = 0.010322865098714828 + 100.0 * 6.20941686630249
Epoch 2900, val loss: 1.6527475118637085
Epoch 2910, training loss: 621.1671752929688 = 0.010231388732790947 + 100.0 * 6.211569309234619
Epoch 2910, val loss: 1.6546590328216553
Epoch 2920, training loss: 621.099853515625 = 0.01014110166579485 + 100.0 * 6.210896968841553
Epoch 2920, val loss: 1.6568641662597656
Epoch 2930, training loss: 621.4664306640625 = 0.010050084441900253 + 100.0 * 6.214563846588135
Epoch 2930, val loss: 1.6593252420425415
Epoch 2940, training loss: 620.9486083984375 = 0.009957129135727882 + 100.0 * 6.209386825561523
Epoch 2940, val loss: 1.660750150680542
Epoch 2950, training loss: 620.8810424804688 = 0.009869779460132122 + 100.0 * 6.208711624145508
Epoch 2950, val loss: 1.6628594398498535
Epoch 2960, training loss: 620.9691772460938 = 0.00978630967438221 + 100.0 * 6.209594249725342
Epoch 2960, val loss: 1.6648832559585571
Epoch 2970, training loss: 621.6131591796875 = 0.009704123251140118 + 100.0 * 6.216034412384033
Epoch 2970, val loss: 1.6668390035629272
Epoch 2980, training loss: 621.0252075195312 = 0.009620334953069687 + 100.0 * 6.210155487060547
Epoch 2980, val loss: 1.668297529220581
Epoch 2990, training loss: 620.8798217773438 = 0.009534335695207119 + 100.0 * 6.20870304107666
Epoch 2990, val loss: 1.6702731847763062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.7959936742224566
=== training gcn model ===
Epoch 0, training loss: 861.666259765625 = 1.9779508113861084 + 100.0 * 8.596882820129395
Epoch 0, val loss: 1.971998691558838
Epoch 10, training loss: 861.6094970703125 = 1.967767596244812 + 100.0 * 8.596417427062988
Epoch 10, val loss: 1.9610315561294556
Epoch 20, training loss: 861.2717895507812 = 1.9552448987960815 + 100.0 * 8.593165397644043
Epoch 20, val loss: 1.9474424123764038
Epoch 30, training loss: 859.1234130859375 = 1.9388630390167236 + 100.0 * 8.571846008300781
Epoch 30, val loss: 1.929626226425171
Epoch 40, training loss: 849.394287109375 = 1.9184744358062744 + 100.0 * 8.47475814819336
Epoch 40, val loss: 1.9088979959487915
Epoch 50, training loss: 813.5082397460938 = 1.8983817100524902 + 100.0 * 8.116098403930664
Epoch 50, val loss: 1.8889394998550415
Epoch 60, training loss: 783.4615478515625 = 1.8801467418670654 + 100.0 * 7.81581449508667
Epoch 60, val loss: 1.8724026679992676
Epoch 70, training loss: 737.759521484375 = 1.8652749061584473 + 100.0 * 7.358942031860352
Epoch 70, val loss: 1.8588335514068604
Epoch 80, training loss: 709.5830078125 = 1.8518472909927368 + 100.0 * 7.0773115158081055
Epoch 80, val loss: 1.846526861190796
Epoch 90, training loss: 694.5776977539062 = 1.841068983078003 + 100.0 * 6.927366256713867
Epoch 90, val loss: 1.8367252349853516
Epoch 100, training loss: 685.0883178710938 = 1.8304497003555298 + 100.0 * 6.832578659057617
Epoch 100, val loss: 1.8269729614257812
Epoch 110, training loss: 678.4767456054688 = 1.818808913230896 + 100.0 * 6.766579627990723
Epoch 110, val loss: 1.8169203996658325
Epoch 120, training loss: 673.4974365234375 = 1.8075695037841797 + 100.0 * 6.7168989181518555
Epoch 120, val loss: 1.8075225353240967
Epoch 130, training loss: 669.3431396484375 = 1.79716956615448 + 100.0 * 6.675459384918213
Epoch 130, val loss: 1.7987421751022339
Epoch 140, training loss: 666.0526123046875 = 1.7871824502944946 + 100.0 * 6.6426544189453125
Epoch 140, val loss: 1.7903281450271606
Epoch 150, training loss: 663.6812133789062 = 1.777193307876587 + 100.0 * 6.619040012359619
Epoch 150, val loss: 1.781782627105713
Epoch 160, training loss: 661.1593017578125 = 1.7666655778884888 + 100.0 * 6.593926429748535
Epoch 160, val loss: 1.772990345954895
Epoch 170, training loss: 658.9520874023438 = 1.7557615041732788 + 100.0 * 6.571963310241699
Epoch 170, val loss: 1.7639471292495728
Epoch 180, training loss: 656.9750366210938 = 1.74434494972229 + 100.0 * 6.55230712890625
Epoch 180, val loss: 1.754474401473999
Epoch 190, training loss: 655.1695556640625 = 1.7322790622711182 + 100.0 * 6.534372806549072
Epoch 190, val loss: 1.7445454597473145
Epoch 200, training loss: 653.3519287109375 = 1.7196308374404907 + 100.0 * 6.516323089599609
Epoch 200, val loss: 1.7340891361236572
Epoch 210, training loss: 652.201171875 = 1.706211805343628 + 100.0 * 6.504949569702148
Epoch 210, val loss: 1.7229926586151123
Epoch 220, training loss: 650.4088134765625 = 1.6917917728424072 + 100.0 * 6.487170219421387
Epoch 220, val loss: 1.7112287282943726
Epoch 230, training loss: 649.1136474609375 = 1.6766191720962524 + 100.0 * 6.474370002746582
Epoch 230, val loss: 1.6988509893417358
Epoch 240, training loss: 647.940673828125 = 1.6606837511062622 + 100.0 * 6.462799549102783
Epoch 240, val loss: 1.6859906911849976
Epoch 250, training loss: 646.9989013671875 = 1.6439534425735474 + 100.0 * 6.453548908233643
Epoch 250, val loss: 1.6724492311477661
Epoch 260, training loss: 646.1006469726562 = 1.6261911392211914 + 100.0 * 6.444744110107422
Epoch 260, val loss: 1.6584175825119019
Epoch 270, training loss: 645.0225830078125 = 1.6077563762664795 + 100.0 * 6.43414831161499
Epoch 270, val loss: 1.6438243389129639
Epoch 280, training loss: 644.07421875 = 1.5885426998138428 + 100.0 * 6.424856662750244
Epoch 280, val loss: 1.6287721395492554
Epoch 290, training loss: 643.2740478515625 = 1.568591594696045 + 100.0 * 6.417054176330566
Epoch 290, val loss: 1.6131113767623901
Epoch 300, training loss: 642.4855346679688 = 1.54787278175354 + 100.0 * 6.409377098083496
Epoch 300, val loss: 1.5970112085342407
Epoch 310, training loss: 641.800537109375 = 1.5266212224960327 + 100.0 * 6.40273904800415
Epoch 310, val loss: 1.5804953575134277
Epoch 320, training loss: 641.0419921875 = 1.5048309564590454 + 100.0 * 6.395371913909912
Epoch 320, val loss: 1.563747763633728
Epoch 330, training loss: 640.6024780273438 = 1.4825376272201538 + 100.0 * 6.391199111938477
Epoch 330, val loss: 1.546788215637207
Epoch 340, training loss: 639.898681640625 = 1.4599323272705078 + 100.0 * 6.384387016296387
Epoch 340, val loss: 1.5295137166976929
Epoch 350, training loss: 640.0948486328125 = 1.436704158782959 + 100.0 * 6.3865814208984375
Epoch 350, val loss: 1.5122212171554565
Epoch 360, training loss: 639.0040283203125 = 1.4132506847381592 + 100.0 * 6.3759074211120605
Epoch 360, val loss: 1.4944970607757568
Epoch 370, training loss: 638.33935546875 = 1.3895094394683838 + 100.0 * 6.369498252868652
Epoch 370, val loss: 1.477019190788269
Epoch 380, training loss: 637.9059448242188 = 1.3656266927719116 + 100.0 * 6.365403175354004
Epoch 380, val loss: 1.45962655544281
Epoch 390, training loss: 637.5031127929688 = 1.3416131734848022 + 100.0 * 6.361615180969238
Epoch 390, val loss: 1.4423402547836304
Epoch 400, training loss: 637.4523315429688 = 1.31752347946167 + 100.0 * 6.3613481521606445
Epoch 400, val loss: 1.425058126449585
Epoch 410, training loss: 636.8056640625 = 1.2932995557785034 + 100.0 * 6.355123996734619
Epoch 410, val loss: 1.4079800844192505
Epoch 420, training loss: 636.504150390625 = 1.2691842317581177 + 100.0 * 6.352349758148193
Epoch 420, val loss: 1.3912640810012817
Epoch 430, training loss: 636.2652587890625 = 1.2452114820480347 + 100.0 * 6.350200176239014
Epoch 430, val loss: 1.3748666048049927
Epoch 440, training loss: 635.8167114257812 = 1.2214293479919434 + 100.0 * 6.34595251083374
Epoch 440, val loss: 1.3585978746414185
Epoch 450, training loss: 635.5779418945312 = 1.1977616548538208 + 100.0 * 6.343801975250244
Epoch 450, val loss: 1.3428784608840942
Epoch 460, training loss: 635.6104125976562 = 1.1744451522827148 + 100.0 * 6.344359874725342
Epoch 460, val loss: 1.3273677825927734
Epoch 470, training loss: 635.081298828125 = 1.151305079460144 + 100.0 * 6.33929967880249
Epoch 470, val loss: 1.3123416900634766
Epoch 480, training loss: 634.734130859375 = 1.1285605430603027 + 100.0 * 6.336055755615234
Epoch 480, val loss: 1.297853946685791
Epoch 490, training loss: 634.4990844726562 = 1.106233835220337 + 100.0 * 6.333928108215332
Epoch 490, val loss: 1.283873438835144
Epoch 500, training loss: 635.0344848632812 = 1.0842084884643555 + 100.0 * 6.339502334594727
Epoch 500, val loss: 1.270453929901123
Epoch 510, training loss: 634.0684814453125 = 1.0627429485321045 + 100.0 * 6.330057621002197
Epoch 510, val loss: 1.2572154998779297
Epoch 520, training loss: 633.86767578125 = 1.0416220426559448 + 100.0 * 6.32826042175293
Epoch 520, val loss: 1.2447593212127686
Epoch 530, training loss: 633.5322265625 = 1.0209829807281494 + 100.0 * 6.325112342834473
Epoch 530, val loss: 1.2330000400543213
Epoch 540, training loss: 633.4456787109375 = 1.000760793685913 + 100.0 * 6.32444953918457
Epoch 540, val loss: 1.2218745946884155
Epoch 550, training loss: 633.3310546875 = 0.9810259342193604 + 100.0 * 6.323500156402588
Epoch 550, val loss: 1.2108807563781738
Epoch 560, training loss: 632.8518676757812 = 0.9616391062736511 + 100.0 * 6.318902492523193
Epoch 560, val loss: 1.200645923614502
Epoch 570, training loss: 632.6976318359375 = 0.9427430033683777 + 100.0 * 6.317548751831055
Epoch 570, val loss: 1.1910202503204346
Epoch 580, training loss: 632.5784912109375 = 0.9242043495178223 + 100.0 * 6.316543102264404
Epoch 580, val loss: 1.1817249059677124
Epoch 590, training loss: 633.0579833984375 = 0.9060153961181641 + 100.0 * 6.32151985168457
Epoch 590, val loss: 1.1726930141448975
Epoch 600, training loss: 632.1265869140625 = 0.8880363702774048 + 100.0 * 6.312385082244873
Epoch 600, val loss: 1.1642521619796753
Epoch 610, training loss: 631.8314208984375 = 0.8704497218132019 + 100.0 * 6.309609889984131
Epoch 610, val loss: 1.1561917066574097
Epoch 620, training loss: 631.7324829101562 = 0.853216826915741 + 100.0 * 6.308792591094971
Epoch 620, val loss: 1.14857816696167
Epoch 630, training loss: 631.50634765625 = 0.8362575173377991 + 100.0 * 6.306701183319092
Epoch 630, val loss: 1.1411340236663818
Epoch 640, training loss: 631.2642211914062 = 0.8196078538894653 + 100.0 * 6.304446697235107
Epoch 640, val loss: 1.1340765953063965
Epoch 650, training loss: 631.1328125 = 0.8032047748565674 + 100.0 * 6.303295612335205
Epoch 650, val loss: 1.1273022890090942
Epoch 660, training loss: 631.4498291015625 = 0.7869676351547241 + 100.0 * 6.306628227233887
Epoch 660, val loss: 1.1206943988800049
Epoch 670, training loss: 630.8099365234375 = 0.770872175693512 + 100.0 * 6.300390720367432
Epoch 670, val loss: 1.1141185760498047
Epoch 680, training loss: 630.9566040039062 = 0.7549877762794495 + 100.0 * 6.302015781402588
Epoch 680, val loss: 1.1077152490615845
Epoch 690, training loss: 630.4769897460938 = 0.7393990755081177 + 100.0 * 6.2973761558532715
Epoch 690, val loss: 1.1016159057617188
Epoch 700, training loss: 630.287109375 = 0.7239696979522705 + 100.0 * 6.295631408691406
Epoch 700, val loss: 1.095854640007019
Epoch 710, training loss: 630.4702758789062 = 0.7087964415550232 + 100.0 * 6.297615051269531
Epoch 710, val loss: 1.0899474620819092
Epoch 720, training loss: 630.2164916992188 = 0.6934614777565002 + 100.0 * 6.295230388641357
Epoch 720, val loss: 1.0845173597335815
Epoch 730, training loss: 629.954833984375 = 0.6785925626754761 + 100.0 * 6.292762279510498
Epoch 730, val loss: 1.0788564682006836
Epoch 740, training loss: 629.6995239257812 = 0.6637746691703796 + 100.0 * 6.29035758972168
Epoch 740, val loss: 1.0738252401351929
Epoch 750, training loss: 629.6669311523438 = 0.6491926312446594 + 100.0 * 6.290177345275879
Epoch 750, val loss: 1.0687541961669922
Epoch 760, training loss: 629.7542114257812 = 0.6347710490226746 + 100.0 * 6.291194438934326
Epoch 760, val loss: 1.0637660026550293
Epoch 770, training loss: 629.4923706054688 = 0.6204158067703247 + 100.0 * 6.288719654083252
Epoch 770, val loss: 1.059174656867981
Epoch 780, training loss: 629.2827758789062 = 0.6063271760940552 + 100.0 * 6.286764621734619
Epoch 780, val loss: 1.0548840761184692
Epoch 790, training loss: 629.0984497070312 = 0.5925023555755615 + 100.0 * 6.285059452056885
Epoch 790, val loss: 1.0507780313491821
Epoch 800, training loss: 629.5615234375 = 0.5788497924804688 + 100.0 * 6.2898268699646
Epoch 800, val loss: 1.046862006187439
Epoch 810, training loss: 629.1848754882812 = 0.5654705762863159 + 100.0 * 6.28619384765625
Epoch 810, val loss: 1.043137550354004
Epoch 820, training loss: 628.7891845703125 = 0.5521112084388733 + 100.0 * 6.282370567321777
Epoch 820, val loss: 1.0398920774459839
Epoch 830, training loss: 628.9037475585938 = 0.5391704440116882 + 100.0 * 6.2836456298828125
Epoch 830, val loss: 1.0368245840072632
Epoch 840, training loss: 628.6327514648438 = 0.5263498425483704 + 100.0 * 6.281063556671143
Epoch 840, val loss: 1.034117579460144
Epoch 850, training loss: 628.6194458007812 = 0.5137962102890015 + 100.0 * 6.2810564041137695
Epoch 850, val loss: 1.0314079523086548
Epoch 860, training loss: 628.3367309570312 = 0.5015019774436951 + 100.0 * 6.278351783752441
Epoch 860, val loss: 1.0293841361999512
Epoch 870, training loss: 628.2274169921875 = 0.489479660987854 + 100.0 * 6.277379035949707
Epoch 870, val loss: 1.027564287185669
Epoch 880, training loss: 628.6563720703125 = 0.477703720331192 + 100.0 * 6.2817864418029785
Epoch 880, val loss: 1.025941014289856
Epoch 890, training loss: 628.1495971679688 = 0.46612975001335144 + 100.0 * 6.276834487915039
Epoch 890, val loss: 1.0247578620910645
Epoch 900, training loss: 627.9700927734375 = 0.4548357427120209 + 100.0 * 6.275152683258057
Epoch 900, val loss: 1.0237611532211304
Epoch 910, training loss: 628.0895385742188 = 0.4438828229904175 + 100.0 * 6.276456356048584
Epoch 910, val loss: 1.0231029987335205
Epoch 920, training loss: 627.7793579101562 = 0.43302762508392334 + 100.0 * 6.273463249206543
Epoch 920, val loss: 1.0228211879730225
Epoch 930, training loss: 627.7198486328125 = 0.42248067259788513 + 100.0 * 6.272973537445068
Epoch 930, val loss: 1.0228391885757446
Epoch 940, training loss: 627.627685546875 = 0.41220664978027344 + 100.0 * 6.272154808044434
Epoch 940, val loss: 1.0229182243347168
Epoch 950, training loss: 627.7229614257812 = 0.40219646692276 + 100.0 * 6.273207187652588
Epoch 950, val loss: 1.023634910583496
Epoch 960, training loss: 627.50244140625 = 0.3924190104007721 + 100.0 * 6.2711005210876465
Epoch 960, val loss: 1.0242458581924438
Epoch 970, training loss: 627.4297485351562 = 0.3828537166118622 + 100.0 * 6.270468711853027
Epoch 970, val loss: 1.0255478620529175
Epoch 980, training loss: 627.3270263671875 = 0.37359529733657837 + 100.0 * 6.269534587860107
Epoch 980, val loss: 1.0268213748931885
Epoch 990, training loss: 627.3140869140625 = 0.36455416679382324 + 100.0 * 6.269495010375977
Epoch 990, val loss: 1.0283716917037964
Epoch 1000, training loss: 627.13525390625 = 0.35563594102859497 + 100.0 * 6.267796039581299
Epoch 1000, val loss: 1.0303759574890137
Epoch 1010, training loss: 627.092529296875 = 0.347000390291214 + 100.0 * 6.267455577850342
Epoch 1010, val loss: 1.032152533531189
Epoch 1020, training loss: 626.8837890625 = 0.3386003077030182 + 100.0 * 6.265451908111572
Epoch 1020, val loss: 1.0346976518630981
Epoch 1030, training loss: 626.9275512695312 = 0.33045658469200134 + 100.0 * 6.2659711837768555
Epoch 1030, val loss: 1.0373693704605103
Epoch 1040, training loss: 626.9158935546875 = 0.32246044278144836 + 100.0 * 6.265934467315674
Epoch 1040, val loss: 1.0398834943771362
Epoch 1050, training loss: 626.7460327148438 = 0.31466034054756165 + 100.0 * 6.264313697814941
Epoch 1050, val loss: 1.042928695678711
Epoch 1060, training loss: 626.7185668945312 = 0.3071002960205078 + 100.0 * 6.2641143798828125
Epoch 1060, val loss: 1.0462779998779297
Epoch 1070, training loss: 626.4915771484375 = 0.2997032403945923 + 100.0 * 6.261919021606445
Epoch 1070, val loss: 1.0495811700820923
Epoch 1080, training loss: 626.6026611328125 = 0.2925865352153778 + 100.0 * 6.263100624084473
Epoch 1080, val loss: 1.053180456161499
Epoch 1090, training loss: 626.549072265625 = 0.2855278253555298 + 100.0 * 6.262635231018066
Epoch 1090, val loss: 1.0568411350250244
Epoch 1100, training loss: 626.3275756835938 = 0.2786305844783783 + 100.0 * 6.260489463806152
Epoch 1100, val loss: 1.060732126235962
Epoch 1110, training loss: 626.3363647460938 = 0.2719448506832123 + 100.0 * 6.26064395904541
Epoch 1110, val loss: 1.064774513244629
Epoch 1120, training loss: 626.4073486328125 = 0.265458881855011 + 100.0 * 6.261419296264648
Epoch 1120, val loss: 1.068939208984375
Epoch 1130, training loss: 626.1043701171875 = 0.25909847021102905 + 100.0 * 6.258452892303467
Epoch 1130, val loss: 1.0734713077545166
Epoch 1140, training loss: 626.0546264648438 = 0.25294387340545654 + 100.0 * 6.258017063140869
Epoch 1140, val loss: 1.0780320167541504
Epoch 1150, training loss: 626.0021362304688 = 0.2469300478696823 + 100.0 * 6.257552146911621
Epoch 1150, val loss: 1.082561731338501
Epoch 1160, training loss: 626.4078369140625 = 0.24104580283164978 + 100.0 * 6.2616682052612305
Epoch 1160, val loss: 1.0871455669403076
Epoch 1170, training loss: 626.0908203125 = 0.23528552055358887 + 100.0 * 6.2585554122924805
Epoch 1170, val loss: 1.0919249057769775
Epoch 1180, training loss: 625.8345947265625 = 0.22969044744968414 + 100.0 * 6.256049156188965
Epoch 1180, val loss: 1.0970605611801147
Epoch 1190, training loss: 625.7678833007812 = 0.22423744201660156 + 100.0 * 6.255436420440674
Epoch 1190, val loss: 1.1020158529281616
Epoch 1200, training loss: 625.9918212890625 = 0.21897628903388977 + 100.0 * 6.257728576660156
Epoch 1200, val loss: 1.1072640419006348
Epoch 1210, training loss: 625.703857421875 = 0.21376168727874756 + 100.0 * 6.25490140914917
Epoch 1210, val loss: 1.112303614616394
Epoch 1220, training loss: 625.6883544921875 = 0.20871052145957947 + 100.0 * 6.254796028137207
Epoch 1220, val loss: 1.1176952123641968
Epoch 1230, training loss: 625.7091064453125 = 0.20377205312252045 + 100.0 * 6.2550530433654785
Epoch 1230, val loss: 1.122982382774353
Epoch 1240, training loss: 625.444580078125 = 0.19899217784404755 + 100.0 * 6.252456188201904
Epoch 1240, val loss: 1.1288570165634155
Epoch 1250, training loss: 625.7557373046875 = 0.1943349987268448 + 100.0 * 6.255613803863525
Epoch 1250, val loss: 1.1344776153564453
Epoch 1260, training loss: 625.4019775390625 = 0.18974058330059052 + 100.0 * 6.252121925354004
Epoch 1260, val loss: 1.1399415731430054
Epoch 1270, training loss: 625.4090576171875 = 0.18530072271823883 + 100.0 * 6.252237319946289
Epoch 1270, val loss: 1.1460756063461304
Epoch 1280, training loss: 625.5184936523438 = 0.18097230792045593 + 100.0 * 6.253375053405762
Epoch 1280, val loss: 1.151809573173523
Epoch 1290, training loss: 625.5652465820312 = 0.17672309279441833 + 100.0 * 6.253885746002197
Epoch 1290, val loss: 1.1577743291854858
Epoch 1300, training loss: 625.1947631835938 = 0.17254771292209625 + 100.0 * 6.250222206115723
Epoch 1300, val loss: 1.1637670993804932
Epoch 1310, training loss: 625.1336669921875 = 0.1685166209936142 + 100.0 * 6.2496514320373535
Epoch 1310, val loss: 1.1699790954589844
Epoch 1320, training loss: 625.0206298828125 = 0.16460967063903809 + 100.0 * 6.248560428619385
Epoch 1320, val loss: 1.1761287450790405
Epoch 1330, training loss: 625.2567138671875 = 0.16080555319786072 + 100.0 * 6.2509589195251465
Epoch 1330, val loss: 1.1822479963302612
Epoch 1340, training loss: 625.08935546875 = 0.157068133354187 + 100.0 * 6.24932336807251
Epoch 1340, val loss: 1.1886409521102905
Epoch 1350, training loss: 625.3026123046875 = 0.1534261405467987 + 100.0 * 6.251491546630859
Epoch 1350, val loss: 1.1947382688522339
Epoch 1360, training loss: 624.9951782226562 = 0.14985670149326324 + 100.0 * 6.248453617095947
Epoch 1360, val loss: 1.2013306617736816
Epoch 1370, training loss: 624.75390625 = 0.14639900624752045 + 100.0 * 6.246074676513672
Epoch 1370, val loss: 1.2076373100280762
Epoch 1380, training loss: 624.7725830078125 = 0.14305180311203003 + 100.0 * 6.24629545211792
Epoch 1380, val loss: 1.2140194177627563
Epoch 1390, training loss: 625.32861328125 = 0.13978484272956848 + 100.0 * 6.251888275146484
Epoch 1390, val loss: 1.2201594114303589
Epoch 1400, training loss: 624.9743041992188 = 0.13657058775424957 + 100.0 * 6.248376846313477
Epoch 1400, val loss: 1.2266020774841309
Epoch 1410, training loss: 624.7559814453125 = 0.13342630863189697 + 100.0 * 6.246225833892822
Epoch 1410, val loss: 1.232966661453247
Epoch 1420, training loss: 624.7688598632812 = 0.13038477301597595 + 100.0 * 6.246385097503662
Epoch 1420, val loss: 1.2392350435256958
Epoch 1430, training loss: 625.3506469726562 = 0.127426415681839 + 100.0 * 6.252232074737549
Epoch 1430, val loss: 1.245654582977295
Epoch 1440, training loss: 624.71337890625 = 0.12449315935373306 + 100.0 * 6.245888710021973
Epoch 1440, val loss: 1.2522212266921997
Epoch 1450, training loss: 624.4888916015625 = 0.12164415419101715 + 100.0 * 6.2436723709106445
Epoch 1450, val loss: 1.258536458015442
Epoch 1460, training loss: 624.3831176757812 = 0.11891245096921921 + 100.0 * 6.242641925811768
Epoch 1460, val loss: 1.265084147453308
Epoch 1470, training loss: 624.387939453125 = 0.11625798046588898 + 100.0 * 6.2427167892456055
Epoch 1470, val loss: 1.27158522605896
Epoch 1480, training loss: 625.274169921875 = 0.11368249356746674 + 100.0 * 6.2516045570373535
Epoch 1480, val loss: 1.277876615524292
Epoch 1490, training loss: 624.3457641601562 = 0.11103887110948563 + 100.0 * 6.242347717285156
Epoch 1490, val loss: 1.2838557958602905
Epoch 1500, training loss: 624.2234497070312 = 0.1085318773984909 + 100.0 * 6.241149425506592
Epoch 1500, val loss: 1.2902047634124756
Epoch 1510, training loss: 624.3400268554688 = 0.10612758249044418 + 100.0 * 6.24233865737915
Epoch 1510, val loss: 1.2966989278793335
Epoch 1520, training loss: 624.4238891601562 = 0.1037360355257988 + 100.0 * 6.24320125579834
Epoch 1520, val loss: 1.30278742313385
Epoch 1530, training loss: 624.1469116210938 = 0.10139910131692886 + 100.0 * 6.240455627441406
Epoch 1530, val loss: 1.3091012239456177
Epoch 1540, training loss: 624.2874755859375 = 0.09914328902959824 + 100.0 * 6.241883754730225
Epoch 1540, val loss: 1.3152271509170532
Epoch 1550, training loss: 624.1574096679688 = 0.09694517403841019 + 100.0 * 6.240604877471924
Epoch 1550, val loss: 1.321783185005188
Epoch 1560, training loss: 624.2318725585938 = 0.0948009043931961 + 100.0 * 6.241371154785156
Epoch 1560, val loss: 1.327978491783142
Epoch 1570, training loss: 624.1746826171875 = 0.09270362555980682 + 100.0 * 6.240819931030273
Epoch 1570, val loss: 1.3343157768249512
Epoch 1580, training loss: 624.0487060546875 = 0.09066002815961838 + 100.0 * 6.2395806312561035
Epoch 1580, val loss: 1.340705156326294
Epoch 1590, training loss: 624.1845092773438 = 0.08867506682872772 + 100.0 * 6.240958213806152
Epoch 1590, val loss: 1.346995234489441
Epoch 1600, training loss: 623.9746704101562 = 0.08672128617763519 + 100.0 * 6.238879203796387
Epoch 1600, val loss: 1.3531513214111328
Epoch 1610, training loss: 623.9234619140625 = 0.08480771631002426 + 100.0 * 6.238387107849121
Epoch 1610, val loss: 1.3591784238815308
Epoch 1620, training loss: 623.8085327148438 = 0.08297022432088852 + 100.0 * 6.237255573272705
Epoch 1620, val loss: 1.365477442741394
Epoch 1630, training loss: 623.9854736328125 = 0.08119301497936249 + 100.0 * 6.23904275894165
Epoch 1630, val loss: 1.3717033863067627
Epoch 1640, training loss: 623.9744262695312 = 0.07942548394203186 + 100.0 * 6.238950252532959
Epoch 1640, val loss: 1.3774031400680542
Epoch 1650, training loss: 623.8605346679688 = 0.07770096510648727 + 100.0 * 6.237828254699707
Epoch 1650, val loss: 1.3837907314300537
Epoch 1660, training loss: 624.1016235351562 = 0.0760369598865509 + 100.0 * 6.240255832672119
Epoch 1660, val loss: 1.3897584676742554
Epoch 1670, training loss: 623.7323608398438 = 0.074395552277565 + 100.0 * 6.236579895019531
Epoch 1670, val loss: 1.3959635496139526
Epoch 1680, training loss: 623.6082763671875 = 0.07280489802360535 + 100.0 * 6.235354423522949
Epoch 1680, val loss: 1.4020100831985474
Epoch 1690, training loss: 623.609375 = 0.07126809656620026 + 100.0 * 6.235381126403809
Epoch 1690, val loss: 1.407909631729126
Epoch 1700, training loss: 624.9229125976562 = 0.06977777183055878 + 100.0 * 6.248531341552734
Epoch 1700, val loss: 1.4135116338729858
Epoch 1710, training loss: 623.803955078125 = 0.06828140467405319 + 100.0 * 6.237356662750244
Epoch 1710, val loss: 1.4197230339050293
Epoch 1720, training loss: 623.5357055664062 = 0.06681445986032486 + 100.0 * 6.234688758850098
Epoch 1720, val loss: 1.4259308576583862
Epoch 1730, training loss: 623.440185546875 = 0.06541096419095993 + 100.0 * 6.233747482299805
Epoch 1730, val loss: 1.4314696788787842
Epoch 1740, training loss: 623.42626953125 = 0.0640694722533226 + 100.0 * 6.233622074127197
Epoch 1740, val loss: 1.4374115467071533
Epoch 1750, training loss: 623.9798583984375 = 0.06276079267263412 + 100.0 * 6.239171028137207
Epoch 1750, val loss: 1.443077564239502
Epoch 1760, training loss: 623.5244140625 = 0.061463601887226105 + 100.0 * 6.2346296310424805
Epoch 1760, val loss: 1.449110507965088
Epoch 1770, training loss: 623.3630981445312 = 0.06019609048962593 + 100.0 * 6.233028888702393
Epoch 1770, val loss: 1.4548554420471191
Epoch 1780, training loss: 623.4745483398438 = 0.05898157134652138 + 100.0 * 6.234156131744385
Epoch 1780, val loss: 1.4605127573013306
Epoch 1790, training loss: 623.7595825195312 = 0.05779949948191643 + 100.0 * 6.23701810836792
Epoch 1790, val loss: 1.4663236141204834
Epoch 1800, training loss: 623.4815673828125 = 0.05659596621990204 + 100.0 * 6.234249591827393
Epoch 1800, val loss: 1.4715461730957031
Epoch 1810, training loss: 623.3356323242188 = 0.05544889345765114 + 100.0 * 6.232801914215088
Epoch 1810, val loss: 1.477535605430603
Epoch 1820, training loss: 623.1845703125 = 0.054343122988939285 + 100.0 * 6.231302261352539
Epoch 1820, val loss: 1.4830811023712158
Epoch 1830, training loss: 623.1369018554688 = 0.05328742042183876 + 100.0 * 6.230836391448975
Epoch 1830, val loss: 1.4887466430664062
Epoch 1840, training loss: 623.1665649414062 = 0.052251704037189484 + 100.0 * 6.231143474578857
Epoch 1840, val loss: 1.4941133260726929
Epoch 1850, training loss: 624.0267944335938 = 0.051248617470264435 + 100.0 * 6.239755153656006
Epoch 1850, val loss: 1.499248743057251
Epoch 1860, training loss: 623.3153686523438 = 0.050237640738487244 + 100.0 * 6.232651233673096
Epoch 1860, val loss: 1.5050222873687744
Epoch 1870, training loss: 623.0834350585938 = 0.04925113171339035 + 100.0 * 6.230341911315918
Epoch 1870, val loss: 1.5104702711105347
Epoch 1880, training loss: 623.2162475585938 = 0.048317477107048035 + 100.0 * 6.2316789627075195
Epoch 1880, val loss: 1.5159032344818115
Epoch 1890, training loss: 623.1228637695312 = 0.04738887771964073 + 100.0 * 6.230754852294922
Epoch 1890, val loss: 1.5209578275680542
Epoch 1900, training loss: 623.1377563476562 = 0.0464961938560009 + 100.0 * 6.230912685394287
Epoch 1900, val loss: 1.5264520645141602
Epoch 1910, training loss: 623.0465698242188 = 0.04561348259449005 + 100.0 * 6.230010032653809
Epoch 1910, val loss: 1.5317116975784302
Epoch 1920, training loss: 623.184814453125 = 0.044768042862415314 + 100.0 * 6.231400489807129
Epoch 1920, val loss: 1.536899447441101
Epoch 1930, training loss: 623.044921875 = 0.04393358901143074 + 100.0 * 6.230010032653809
Epoch 1930, val loss: 1.5424675941467285
Epoch 1940, training loss: 623.12548828125 = 0.0431261882185936 + 100.0 * 6.230823516845703
Epoch 1940, val loss: 1.5478976964950562
Epoch 1950, training loss: 622.8280639648438 = 0.04232079163193703 + 100.0 * 6.22785758972168
Epoch 1950, val loss: 1.5527064800262451
Epoch 1960, training loss: 622.864990234375 = 0.04155156761407852 + 100.0 * 6.22823429107666
Epoch 1960, val loss: 1.5577958822250366
Epoch 1970, training loss: 623.274169921875 = 0.0408218577504158 + 100.0 * 6.232333660125732
Epoch 1970, val loss: 1.5631656646728516
Epoch 1980, training loss: 623.0698852539062 = 0.040064990520477295 + 100.0 * 6.2302985191345215
Epoch 1980, val loss: 1.5676565170288086
Epoch 1990, training loss: 622.9556274414062 = 0.03932511433959007 + 100.0 * 6.22916316986084
Epoch 1990, val loss: 1.5730777978897095
Epoch 2000, training loss: 622.7105102539062 = 0.03862278163433075 + 100.0 * 6.226718902587891
Epoch 2000, val loss: 1.5780773162841797
Epoch 2010, training loss: 622.7852783203125 = 0.03794606402516365 + 100.0 * 6.227473258972168
Epoch 2010, val loss: 1.5829368829727173
Epoch 2020, training loss: 623.1290283203125 = 0.037286560982465744 + 100.0 * 6.230917453765869
Epoch 2020, val loss: 1.5878897905349731
Epoch 2030, training loss: 622.9139404296875 = 0.03664074465632439 + 100.0 * 6.22877311706543
Epoch 2030, val loss: 1.593307614326477
Epoch 2040, training loss: 623.0638427734375 = 0.035994552075862885 + 100.0 * 6.230278015136719
Epoch 2040, val loss: 1.5978379249572754
Epoch 2050, training loss: 622.6450805664062 = 0.0353742279112339 + 100.0 * 6.2260966300964355
Epoch 2050, val loss: 1.603270411491394
Epoch 2060, training loss: 622.6961059570312 = 0.03476296365261078 + 100.0 * 6.2266130447387695
Epoch 2060, val loss: 1.607697606086731
Epoch 2070, training loss: 622.813720703125 = 0.034188833087682724 + 100.0 * 6.227795124053955
Epoch 2070, val loss: 1.6131426095962524
Epoch 2080, training loss: 622.6256713867188 = 0.03360578790307045 + 100.0 * 6.225920677185059
Epoch 2080, val loss: 1.6175328493118286
Epoch 2090, training loss: 622.578125 = 0.03303543105721474 + 100.0 * 6.2254509925842285
Epoch 2090, val loss: 1.622580647468567
Epoch 2100, training loss: 623.3558959960938 = 0.03250045329332352 + 100.0 * 6.233234405517578
Epoch 2100, val loss: 1.6276429891586304
Epoch 2110, training loss: 622.6482543945312 = 0.03193415328860283 + 100.0 * 6.226163387298584
Epoch 2110, val loss: 1.631708025932312
Epoch 2120, training loss: 622.4541015625 = 0.0314040370285511 + 100.0 * 6.224226474761963
Epoch 2120, val loss: 1.6367909908294678
Epoch 2130, training loss: 622.4415893554688 = 0.030892275273799896 + 100.0 * 6.224107265472412
Epoch 2130, val loss: 1.6411957740783691
Epoch 2140, training loss: 622.74072265625 = 0.030403736978769302 + 100.0 * 6.227103233337402
Epoch 2140, val loss: 1.6459442377090454
Epoch 2150, training loss: 622.4369506835938 = 0.02990601398050785 + 100.0 * 6.2240705490112305
Epoch 2150, val loss: 1.650436282157898
Epoch 2160, training loss: 622.646728515625 = 0.029430484399199486 + 100.0 * 6.226173400878906
Epoch 2160, val loss: 1.6555049419403076
Epoch 2170, training loss: 622.4579467773438 = 0.02895490452647209 + 100.0 * 6.224290370941162
Epoch 2170, val loss: 1.6594078540802002
Epoch 2180, training loss: 622.36962890625 = 0.028489457443356514 + 100.0 * 6.2234110832214355
Epoch 2180, val loss: 1.6643949747085571
Epoch 2190, training loss: 622.3195190429688 = 0.02804824709892273 + 100.0 * 6.222914218902588
Epoch 2190, val loss: 1.6688575744628906
Epoch 2200, training loss: 622.5460205078125 = 0.027622701600193977 + 100.0 * 6.225183486938477
Epoch 2200, val loss: 1.673430323600769
Epoch 2210, training loss: 622.2183227539062 = 0.027188533917069435 + 100.0 * 6.221911907196045
Epoch 2210, val loss: 1.677768349647522
Epoch 2220, training loss: 622.52197265625 = 0.026781601831316948 + 100.0 * 6.22495174407959
Epoch 2220, val loss: 1.682373046875
Epoch 2230, training loss: 622.7825927734375 = 0.026377243921160698 + 100.0 * 6.227561950683594
Epoch 2230, val loss: 1.686915636062622
Epoch 2240, training loss: 622.3529052734375 = 0.025945426896214485 + 100.0 * 6.223269939422607
Epoch 2240, val loss: 1.6906859874725342
Epoch 2250, training loss: 622.1698608398438 = 0.025553854182362556 + 100.0 * 6.221443176269531
Epoch 2250, val loss: 1.6950961351394653
Epoch 2260, training loss: 622.157470703125 = 0.025176074355840683 + 100.0 * 6.221323490142822
Epoch 2260, val loss: 1.699757695198059
Epoch 2270, training loss: 622.4088745117188 = 0.024811530485749245 + 100.0 * 6.223840236663818
Epoch 2270, val loss: 1.7038418054580688
Epoch 2280, training loss: 622.1430053710938 = 0.024442024528980255 + 100.0 * 6.22118616104126
Epoch 2280, val loss: 1.7081867456436157
Epoch 2290, training loss: 622.493896484375 = 0.02409234829246998 + 100.0 * 6.224698066711426
Epoch 2290, val loss: 1.7127512693405151
Epoch 2300, training loss: 622.0640869140625 = 0.02373264729976654 + 100.0 * 6.220403671264648
Epoch 2300, val loss: 1.7166869640350342
Epoch 2310, training loss: 622.0613403320312 = 0.02338673360645771 + 100.0 * 6.220379829406738
Epoch 2310, val loss: 1.7209863662719727
Epoch 2320, training loss: 622.0145874023438 = 0.02305443212389946 + 100.0 * 6.219915390014648
Epoch 2320, val loss: 1.725062370300293
Epoch 2330, training loss: 622.6705932617188 = 0.02273968979716301 + 100.0 * 6.226478099822998
Epoch 2330, val loss: 1.728938102722168
Epoch 2340, training loss: 622.4498901367188 = 0.022417409345507622 + 100.0 * 6.224274635314941
Epoch 2340, val loss: 1.7336021661758423
Epoch 2350, training loss: 622.3275756835938 = 0.0220935195684433 + 100.0 * 6.223054885864258
Epoch 2350, val loss: 1.7376481294631958
Epoch 2360, training loss: 621.9581298828125 = 0.021775171160697937 + 100.0 * 6.219363689422607
Epoch 2360, val loss: 1.7413520812988281
Epoch 2370, training loss: 622.0950317382812 = 0.021474340930581093 + 100.0 * 6.220735549926758
Epoch 2370, val loss: 1.7455506324768066
Epoch 2380, training loss: 622.127685546875 = 0.02118116244673729 + 100.0 * 6.221065044403076
Epoch 2380, val loss: 1.7496346235275269
Epoch 2390, training loss: 622.1449584960938 = 0.020889362320303917 + 100.0 * 6.221240997314453
Epoch 2390, val loss: 1.753587245941162
Epoch 2400, training loss: 622.1248168945312 = 0.0205982718616724 + 100.0 * 6.221042633056641
Epoch 2400, val loss: 1.756904125213623
Epoch 2410, training loss: 622.0323486328125 = 0.020318713039159775 + 100.0 * 6.220120429992676
Epoch 2410, val loss: 1.7614843845367432
Epoch 2420, training loss: 621.8383178710938 = 0.02004389278590679 + 100.0 * 6.2181830406188965
Epoch 2420, val loss: 1.76530921459198
Epoch 2430, training loss: 621.8846435546875 = 0.019782181829214096 + 100.0 * 6.218648910522461
Epoch 2430, val loss: 1.7694262266159058
Epoch 2440, training loss: 621.964111328125 = 0.019523566588759422 + 100.0 * 6.219445705413818
Epoch 2440, val loss: 1.773425579071045
Epoch 2450, training loss: 622.1829223632812 = 0.019265281036496162 + 100.0 * 6.2216362953186035
Epoch 2450, val loss: 1.7767893075942993
Epoch 2460, training loss: 621.9959106445312 = 0.019004708155989647 + 100.0 * 6.21976900100708
Epoch 2460, val loss: 1.7803593873977661
Epoch 2470, training loss: 621.7933349609375 = 0.01875595562160015 + 100.0 * 6.217745780944824
Epoch 2470, val loss: 1.7848515510559082
Epoch 2480, training loss: 621.8751831054688 = 0.018514536321163177 + 100.0 * 6.21856689453125
Epoch 2480, val loss: 1.788624882698059
Epoch 2490, training loss: 622.1380004882812 = 0.01828361675143242 + 100.0 * 6.221197128295898
Epoch 2490, val loss: 1.7921515703201294
Epoch 2500, training loss: 621.86279296875 = 0.018033962696790695 + 100.0 * 6.218447685241699
Epoch 2500, val loss: 1.7955354452133179
Epoch 2510, training loss: 621.9280395507812 = 0.017811881378293037 + 100.0 * 6.219102382659912
Epoch 2510, val loss: 1.7995859384536743
Epoch 2520, training loss: 621.6597290039062 = 0.017581099644303322 + 100.0 * 6.216421604156494
Epoch 2520, val loss: 1.8030188083648682
Epoch 2530, training loss: 621.6654052734375 = 0.017362473532557487 + 100.0 * 6.216480731964111
Epoch 2530, val loss: 1.8066436052322388
Epoch 2540, training loss: 621.986328125 = 0.017156582325696945 + 100.0 * 6.219691753387451
Epoch 2540, val loss: 1.8102935552597046
Epoch 2550, training loss: 621.7498168945312 = 0.016936544328927994 + 100.0 * 6.217329025268555
Epoch 2550, val loss: 1.8135148286819458
Epoch 2560, training loss: 621.6339721679688 = 0.016724353656172752 + 100.0 * 6.216172218322754
Epoch 2560, val loss: 1.817173957824707
Epoch 2570, training loss: 621.807861328125 = 0.016523493453860283 + 100.0 * 6.2179131507873535
Epoch 2570, val loss: 1.8205190896987915
Epoch 2580, training loss: 621.7857666015625 = 0.016320044174790382 + 100.0 * 6.2176947593688965
Epoch 2580, val loss: 1.823742389678955
Epoch 2590, training loss: 621.9862670898438 = 0.016128558665513992 + 100.0 * 6.219701766967773
Epoch 2590, val loss: 1.8280085325241089
Epoch 2600, training loss: 621.56689453125 = 0.015924980863928795 + 100.0 * 6.21550989151001
Epoch 2600, val loss: 1.8306111097335815
Epoch 2610, training loss: 621.5414428710938 = 0.015739204362034798 + 100.0 * 6.215256690979004
Epoch 2610, val loss: 1.8347307443618774
Epoch 2620, training loss: 621.8089599609375 = 0.015561292879283428 + 100.0 * 6.2179341316223145
Epoch 2620, val loss: 1.8380892276763916
Epoch 2630, training loss: 621.5431518554688 = 0.015370805747807026 + 100.0 * 6.215277671813965
Epoch 2630, val loss: 1.8411325216293335
Epoch 2640, training loss: 621.5589599609375 = 0.015189879573881626 + 100.0 * 6.215437889099121
Epoch 2640, val loss: 1.844269871711731
Epoch 2650, training loss: 621.4716796875 = 0.01501452550292015 + 100.0 * 6.214566707611084
Epoch 2650, val loss: 1.847915530204773
Epoch 2660, training loss: 621.7883911132812 = 0.014852040447294712 + 100.0 * 6.217735767364502
Epoch 2660, val loss: 1.8514654636383057
Epoch 2670, training loss: 621.5826416015625 = 0.01467383373528719 + 100.0 * 6.215680122375488
Epoch 2670, val loss: 1.8542600870132446
Epoch 2680, training loss: 621.4292602539062 = 0.014500604011118412 + 100.0 * 6.214147090911865
Epoch 2680, val loss: 1.8572620153427124
Epoch 2690, training loss: 621.613525390625 = 0.014340529218316078 + 100.0 * 6.215991973876953
Epoch 2690, val loss: 1.8605667352676392
Epoch 2700, training loss: 621.3492431640625 = 0.014175008051097393 + 100.0 * 6.213350772857666
Epoch 2700, val loss: 1.8641619682312012
Epoch 2710, training loss: 621.3796997070312 = 0.01402192935347557 + 100.0 * 6.213656425476074
Epoch 2710, val loss: 1.8674414157867432
Epoch 2720, training loss: 621.8681640625 = 0.013875178061425686 + 100.0 * 6.21854305267334
Epoch 2720, val loss: 1.8706475496292114
Epoch 2730, training loss: 621.58935546875 = 0.013716078363358974 + 100.0 * 6.215755939483643
Epoch 2730, val loss: 1.873426079750061
Epoch 2740, training loss: 621.3709716796875 = 0.013561639934778214 + 100.0 * 6.213574409484863
Epoch 2740, val loss: 1.8769917488098145
Epoch 2750, training loss: 621.263671875 = 0.013411503285169601 + 100.0 * 6.212502479553223
Epoch 2750, val loss: 1.8799927234649658
Epoch 2760, training loss: 621.3135375976562 = 0.013269701041281223 + 100.0 * 6.213003158569336
Epoch 2760, val loss: 1.8829269409179688
Epoch 2770, training loss: 621.762451171875 = 0.013135937042534351 + 100.0 * 6.217493534088135
Epoch 2770, val loss: 1.8862234354019165
Epoch 2780, training loss: 621.4793701171875 = 0.012994288466870785 + 100.0 * 6.214663505554199
Epoch 2780, val loss: 1.889559268951416
Epoch 2790, training loss: 621.4580078125 = 0.012852812185883522 + 100.0 * 6.214451313018799
Epoch 2790, val loss: 1.8921489715576172
Epoch 2800, training loss: 621.185546875 = 0.01271355152130127 + 100.0 * 6.211728572845459
Epoch 2800, val loss: 1.8952560424804688
Epoch 2810, training loss: 621.2411499023438 = 0.012580814771354198 + 100.0 * 6.21228551864624
Epoch 2810, val loss: 1.8982417583465576
Epoch 2820, training loss: 621.3541870117188 = 0.012454339303076267 + 100.0 * 6.2134175300598145
Epoch 2820, val loss: 1.900880217552185
Epoch 2830, training loss: 621.48828125 = 0.012330577708780766 + 100.0 * 6.214759826660156
Epoch 2830, val loss: 1.9043596982955933
Epoch 2840, training loss: 621.5264282226562 = 0.012207542546093464 + 100.0 * 6.215142250061035
Epoch 2840, val loss: 1.9072961807250977
Epoch 2850, training loss: 621.2178955078125 = 0.01207481324672699 + 100.0 * 6.212058067321777
Epoch 2850, val loss: 1.9100710153579712
Epoch 2860, training loss: 621.2571411132812 = 0.011954296380281448 + 100.0 * 6.212451934814453
Epoch 2860, val loss: 1.9131205081939697
Epoch 2870, training loss: 621.1932983398438 = 0.01183290220797062 + 100.0 * 6.2118144035339355
Epoch 2870, val loss: 1.9161876440048218
Epoch 2880, training loss: 621.46435546875 = 0.011717173270881176 + 100.0 * 6.214526176452637
Epoch 2880, val loss: 1.9187599420547485
Epoch 2890, training loss: 621.1406860351562 = 0.011597918346524239 + 100.0 * 6.2112908363342285
Epoch 2890, val loss: 1.9214071035385132
Epoch 2900, training loss: 621.1351318359375 = 0.0114842364564538 + 100.0 * 6.211236476898193
Epoch 2900, val loss: 1.9240763187408447
Epoch 2910, training loss: 621.26708984375 = 0.011372791603207588 + 100.0 * 6.212557315826416
Epoch 2910, val loss: 1.9271509647369385
Epoch 2920, training loss: 621.3919677734375 = 0.011263907887041569 + 100.0 * 6.213806629180908
Epoch 2920, val loss: 1.9294928312301636
Epoch 2930, training loss: 621.2850341796875 = 0.01115473173558712 + 100.0 * 6.212738513946533
Epoch 2930, val loss: 1.93235182762146
Epoch 2940, training loss: 621.1921997070312 = 0.011046812869608402 + 100.0 * 6.211811542510986
Epoch 2940, val loss: 1.9355930089950562
Epoch 2950, training loss: 621.1889038085938 = 0.010941935703158379 + 100.0 * 6.211779594421387
Epoch 2950, val loss: 1.9382097721099854
Epoch 2960, training loss: 621.1947021484375 = 0.010842796415090561 + 100.0 * 6.211838245391846
Epoch 2960, val loss: 1.9413524866104126
Epoch 2970, training loss: 621.1488647460938 = 0.010740573517978191 + 100.0 * 6.211380958557129
Epoch 2970, val loss: 1.9437828063964844
Epoch 2980, training loss: 621.376953125 = 0.010638522915542126 + 100.0 * 6.213663101196289
Epoch 2980, val loss: 1.9461114406585693
Epoch 2990, training loss: 621.207763671875 = 0.010539110749959946 + 100.0 * 6.211971759796143
Epoch 2990, val loss: 1.9489861726760864
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8002108592514497
The final CL Acc:0.70741, 0.01600, The final GNN Acc:0.79933, 0.00245
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13120])
remove edge: torch.Size([2, 7982])
updated graph: torch.Size([2, 10546])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6419067382812 = 1.9581271409988403 + 100.0 * 8.596837997436523
Epoch 0, val loss: 1.9536980390548706
Epoch 10, training loss: 861.5477905273438 = 1.949435830116272 + 100.0 * 8.595983505249023
Epoch 10, val loss: 1.9451515674591064
Epoch 20, training loss: 860.9188842773438 = 1.9384442567825317 + 100.0 * 8.589804649353027
Epoch 20, val loss: 1.9340262413024902
Epoch 30, training loss: 856.3482666015625 = 1.9239357709884644 + 100.0 * 8.544242858886719
Epoch 30, val loss: 1.9190605878829956
Epoch 40, training loss: 825.3696899414062 = 1.9056833982467651 + 100.0 * 8.234640121459961
Epoch 40, val loss: 1.9007760286331177
Epoch 50, training loss: 752.6747436523438 = 1.8854786157608032 + 100.0 * 7.507892608642578
Epoch 50, val loss: 1.8815739154815674
Epoch 60, training loss: 729.568115234375 = 1.8712985515594482 + 100.0 * 7.276968479156494
Epoch 60, val loss: 1.868587613105774
Epoch 70, training loss: 708.2730102539062 = 1.8579139709472656 + 100.0 * 7.064151287078857
Epoch 70, val loss: 1.8561795949935913
Epoch 80, training loss: 693.4771118164062 = 1.8461329936981201 + 100.0 * 6.916309833526611
Epoch 80, val loss: 1.8456709384918213
Epoch 90, training loss: 682.3043823242188 = 1.8335835933685303 + 100.0 * 6.804708480834961
Epoch 90, val loss: 1.8342950344085693
Epoch 100, training loss: 673.164306640625 = 1.821498990058899 + 100.0 * 6.713428497314453
Epoch 100, val loss: 1.8233014345169067
Epoch 110, training loss: 667.0650024414062 = 1.80997896194458 + 100.0 * 6.652549743652344
Epoch 110, val loss: 1.8128726482391357
Epoch 120, training loss: 663.2655639648438 = 1.7982747554779053 + 100.0 * 6.614673137664795
Epoch 120, val loss: 1.8021674156188965
Epoch 130, training loss: 659.947509765625 = 1.7859487533569336 + 100.0 * 6.581615447998047
Epoch 130, val loss: 1.791025161743164
Epoch 140, training loss: 657.1246337890625 = 1.7737274169921875 + 100.0 * 6.553508758544922
Epoch 140, val loss: 1.7799614667892456
Epoch 150, training loss: 654.727783203125 = 1.7613465785980225 + 100.0 * 6.529664516448975
Epoch 150, val loss: 1.7687361240386963
Epoch 160, training loss: 652.6339111328125 = 1.748348593711853 + 100.0 * 6.508855819702148
Epoch 160, val loss: 1.7568026781082153
Epoch 170, training loss: 650.8139038085938 = 1.7345356941223145 + 100.0 * 6.490793228149414
Epoch 170, val loss: 1.7441293001174927
Epoch 180, training loss: 649.2848510742188 = 1.7198128700256348 + 100.0 * 6.475650787353516
Epoch 180, val loss: 1.7306550741195679
Epoch 190, training loss: 647.7359619140625 = 1.7039530277252197 + 100.0 * 6.460319995880127
Epoch 190, val loss: 1.7163231372833252
Epoch 200, training loss: 646.5537719726562 = 1.686904788017273 + 100.0 * 6.448668956756592
Epoch 200, val loss: 1.7009408473968506
Epoch 210, training loss: 645.1693115234375 = 1.6684764623641968 + 100.0 * 6.435008525848389
Epoch 210, val loss: 1.6844323873519897
Epoch 220, training loss: 644.0989990234375 = 1.6487534046173096 + 100.0 * 6.424502849578857
Epoch 220, val loss: 1.6668204069137573
Epoch 230, training loss: 643.5086669921875 = 1.6276369094848633 + 100.0 * 6.4188103675842285
Epoch 230, val loss: 1.647944688796997
Epoch 240, training loss: 642.3909912109375 = 1.6051583290100098 + 100.0 * 6.407858371734619
Epoch 240, val loss: 1.627984642982483
Epoch 250, training loss: 641.6159057617188 = 1.5814753770828247 + 100.0 * 6.400344371795654
Epoch 250, val loss: 1.6069926023483276
Epoch 260, training loss: 641.4732666015625 = 1.5565861463546753 + 100.0 * 6.399166584014893
Epoch 260, val loss: 1.5849372148513794
Epoch 270, training loss: 640.5308227539062 = 1.530778169631958 + 100.0 * 6.390000343322754
Epoch 270, val loss: 1.5621209144592285
Epoch 280, training loss: 639.791015625 = 1.504097819328308 + 100.0 * 6.382869243621826
Epoch 280, val loss: 1.5386229753494263
Epoch 290, training loss: 639.154541015625 = 1.4768004417419434 + 100.0 * 6.376777172088623
Epoch 290, val loss: 1.5146448612213135
Epoch 300, training loss: 638.9852905273438 = 1.4489936828613281 + 100.0 * 6.375363349914551
Epoch 300, val loss: 1.4901924133300781
Epoch 310, training loss: 638.145751953125 = 1.4207725524902344 + 100.0 * 6.367249488830566
Epoch 310, val loss: 1.4657583236694336
Epoch 320, training loss: 637.6056518554688 = 1.3925280570983887 + 100.0 * 6.362131595611572
Epoch 320, val loss: 1.4412811994552612
Epoch 330, training loss: 637.3576049804688 = 1.3642953634262085 + 100.0 * 6.359932899475098
Epoch 330, val loss: 1.416958212852478
Epoch 340, training loss: 636.8218383789062 = 1.3359010219573975 + 100.0 * 6.354859828948975
Epoch 340, val loss: 1.392545461654663
Epoch 350, training loss: 636.1834716796875 = 1.307897925376892 + 100.0 * 6.348755359649658
Epoch 350, val loss: 1.3684672117233276
Epoch 360, training loss: 635.7464599609375 = 1.280045509338379 + 100.0 * 6.344664573669434
Epoch 360, val loss: 1.3446481227874756
Epoch 370, training loss: 636.3618774414062 = 1.2524816989898682 + 100.0 * 6.351093769073486
Epoch 370, val loss: 1.320938229560852
Epoch 380, training loss: 635.1650390625 = 1.225084900856018 + 100.0 * 6.339399337768555
Epoch 380, val loss: 1.2977490425109863
Epoch 390, training loss: 634.6409301757812 = 1.1980952024459839 + 100.0 * 6.334428310394287
Epoch 390, val loss: 1.2747492790222168
Epoch 400, training loss: 634.2069091796875 = 1.1715260744094849 + 100.0 * 6.330353736877441
Epoch 400, val loss: 1.2520427703857422
Epoch 410, training loss: 634.2988891601562 = 1.1454156637191772 + 100.0 * 6.3315348625183105
Epoch 410, val loss: 1.2295653820037842
Epoch 420, training loss: 634.0225830078125 = 1.1195080280303955 + 100.0 * 6.329030990600586
Epoch 420, val loss: 1.207672119140625
Epoch 430, training loss: 633.3780517578125 = 1.094227910041809 + 100.0 * 6.322838306427002
Epoch 430, val loss: 1.1861640214920044
Epoch 440, training loss: 632.984619140625 = 1.0695500373840332 + 100.0 * 6.319150447845459
Epoch 440, val loss: 1.1649630069732666
Epoch 450, training loss: 632.97119140625 = 1.0454912185668945 + 100.0 * 6.3192572593688965
Epoch 450, val loss: 1.144494891166687
Epoch 460, training loss: 632.6641235351562 = 1.022032380104065 + 100.0 * 6.316421031951904
Epoch 460, val loss: 1.124191164970398
Epoch 470, training loss: 632.1533203125 = 0.9993852376937866 + 100.0 * 6.311539173126221
Epoch 470, val loss: 1.1049816608428955
Epoch 480, training loss: 631.9762573242188 = 0.9775448441505432 + 100.0 * 6.3099870681762695
Epoch 480, val loss: 1.0864211320877075
Epoch 490, training loss: 631.9207763671875 = 0.9565204977989197 + 100.0 * 6.309642314910889
Epoch 490, val loss: 1.06868314743042
Epoch 500, training loss: 631.4483642578125 = 0.936339795589447 + 100.0 * 6.30511999130249
Epoch 500, val loss: 1.0518250465393066
Epoch 510, training loss: 631.20068359375 = 0.9170942306518555 + 100.0 * 6.302835941314697
Epoch 510, val loss: 1.0358778238296509
Epoch 520, training loss: 631.12646484375 = 0.8987461924552917 + 100.0 * 6.302277088165283
Epoch 520, val loss: 1.020822286605835
Epoch 530, training loss: 630.9412231445312 = 0.8812184929847717 + 100.0 * 6.300600051879883
Epoch 530, val loss: 1.0068553686141968
Epoch 540, training loss: 630.5721435546875 = 0.8645082116127014 + 100.0 * 6.29707670211792
Epoch 540, val loss: 0.9936447739601135
Epoch 550, training loss: 630.5991821289062 = 0.8486402034759521 + 100.0 * 6.2975053787231445
Epoch 550, val loss: 0.9813788533210754
Epoch 560, training loss: 630.1336669921875 = 0.8334919810295105 + 100.0 * 6.293001651763916
Epoch 560, val loss: 0.969950795173645
Epoch 570, training loss: 630.26220703125 = 0.8189985156059265 + 100.0 * 6.294432640075684
Epoch 570, val loss: 0.9594131112098694
Epoch 580, training loss: 629.94677734375 = 0.8051162958145142 + 100.0 * 6.291416645050049
Epoch 580, val loss: 0.9491081237792969
Epoch 590, training loss: 629.6472778320312 = 0.7917741537094116 + 100.0 * 6.288555145263672
Epoch 590, val loss: 0.9397376179695129
Epoch 600, training loss: 629.447509765625 = 0.7789375185966492 + 100.0 * 6.286685943603516
Epoch 600, val loss: 0.9307438135147095
Epoch 610, training loss: 630.1434326171875 = 0.7664667367935181 + 100.0 * 6.293769359588623
Epoch 610, val loss: 0.9221804738044739
Epoch 620, training loss: 629.1900634765625 = 0.7543601989746094 + 100.0 * 6.28435754776001
Epoch 620, val loss: 0.9142983555793762
Epoch 630, training loss: 629.1932983398438 = 0.742464005947113 + 100.0 * 6.284508228302002
Epoch 630, val loss: 0.9066130518913269
Epoch 640, training loss: 628.893798828125 = 0.7307511568069458 + 100.0 * 6.281630516052246
Epoch 640, val loss: 0.8992965221405029
Epoch 650, training loss: 628.8634643554688 = 0.7191815972328186 + 100.0 * 6.281443119049072
Epoch 650, val loss: 0.8920866847038269
Epoch 660, training loss: 628.7306518554688 = 0.7076526284217834 + 100.0 * 6.2802300453186035
Epoch 660, val loss: 0.8847481608390808
Epoch 670, training loss: 628.4556274414062 = 0.6960564851760864 + 100.0 * 6.2775959968566895
Epoch 670, val loss: 0.8775116801261902
Epoch 680, training loss: 628.4755859375 = 0.6843712329864502 + 100.0 * 6.277912139892578
Epoch 680, val loss: 0.8704158067703247
Epoch 690, training loss: 628.3150024414062 = 0.6725113987922668 + 100.0 * 6.276424884796143
Epoch 690, val loss: 0.8631823062896729
Epoch 700, training loss: 628.067626953125 = 0.6605116128921509 + 100.0 * 6.274070739746094
Epoch 700, val loss: 0.85569167137146
Epoch 710, training loss: 628.0677490234375 = 0.6482571959495544 + 100.0 * 6.274194717407227
Epoch 710, val loss: 0.8482759594917297
Epoch 720, training loss: 627.9953002929688 = 0.635631263256073 + 100.0 * 6.27359676361084
Epoch 720, val loss: 0.840559720993042
Epoch 730, training loss: 628.0105590820312 = 0.6226739287376404 + 100.0 * 6.273878574371338
Epoch 730, val loss: 0.8327983617782593
Epoch 740, training loss: 627.5676879882812 = 0.6093692183494568 + 100.0 * 6.269583225250244
Epoch 740, val loss: 0.8247572779655457
Epoch 750, training loss: 627.7433471679688 = 0.5957610607147217 + 100.0 * 6.271475791931152
Epoch 750, val loss: 0.8167322278022766
Epoch 760, training loss: 627.4072875976562 = 0.5818061828613281 + 100.0 * 6.268255233764648
Epoch 760, val loss: 0.8085207343101501
Epoch 770, training loss: 627.774169921875 = 0.5675695538520813 + 100.0 * 6.272066116333008
Epoch 770, val loss: 0.8004559874534607
Epoch 780, training loss: 627.2359619140625 = 0.553056001663208 + 100.0 * 6.266829490661621
Epoch 780, val loss: 0.791702151298523
Epoch 790, training loss: 627.0413818359375 = 0.5383360981941223 + 100.0 * 6.265030384063721
Epoch 790, val loss: 0.7835466861724854
Epoch 800, training loss: 627.0055541992188 = 0.5234917998313904 + 100.0 * 6.264820575714111
Epoch 800, val loss: 0.7755544185638428
Epoch 810, training loss: 627.6129150390625 = 0.508497416973114 + 100.0 * 6.2710442543029785
Epoch 810, val loss: 0.7673207521438599
Epoch 820, training loss: 626.81884765625 = 0.49352654814720154 + 100.0 * 6.263253211975098
Epoch 820, val loss: 0.7595179080963135
Epoch 830, training loss: 626.7662963867188 = 0.4785822927951813 + 100.0 * 6.262876987457275
Epoch 830, val loss: 0.7519118785858154
Epoch 840, training loss: 626.8477172851562 = 0.4637759327888489 + 100.0 * 6.263839244842529
Epoch 840, val loss: 0.7448021173477173
Epoch 850, training loss: 626.4957275390625 = 0.4491296708583832 + 100.0 * 6.260465621948242
Epoch 850, val loss: 0.7377283573150635
Epoch 860, training loss: 626.4944458007812 = 0.43473362922668457 + 100.0 * 6.260597229003906
Epoch 860, val loss: 0.7311913967132568
Epoch 870, training loss: 626.6893920898438 = 0.42065975069999695 + 100.0 * 6.2626872062683105
Epoch 870, val loss: 0.7251272201538086
Epoch 880, training loss: 626.4403686523438 = 0.40696609020233154 + 100.0 * 6.260334014892578
Epoch 880, val loss: 0.7195882201194763
Epoch 890, training loss: 626.2870483398438 = 0.3936121463775635 + 100.0 * 6.258934020996094
Epoch 890, val loss: 0.714198112487793
Epoch 900, training loss: 626.106689453125 = 0.3807026743888855 + 100.0 * 6.257259845733643
Epoch 900, val loss: 0.7093581557273865
Epoch 910, training loss: 626.07861328125 = 0.36825263500213623 + 100.0 * 6.25710391998291
Epoch 910, val loss: 0.7051472663879395
Epoch 920, training loss: 626.2023315429688 = 0.35624510049819946 + 100.0 * 6.258460998535156
Epoch 920, val loss: 0.701370358467102
Epoch 930, training loss: 626.055908203125 = 0.344634473323822 + 100.0 * 6.257112979888916
Epoch 930, val loss: 0.6980889439582825
Epoch 940, training loss: 625.942626953125 = 0.33343270421028137 + 100.0 * 6.256092071533203
Epoch 940, val loss: 0.6950923800468445
Epoch 950, training loss: 625.7964477539062 = 0.3227139711380005 + 100.0 * 6.254737854003906
Epoch 950, val loss: 0.6925504803657532
Epoch 960, training loss: 625.575439453125 = 0.31246647238731384 + 100.0 * 6.25262975692749
Epoch 960, val loss: 0.6906307935714722
Epoch 970, training loss: 625.6575927734375 = 0.30262458324432373 + 100.0 * 6.253550052642822
Epoch 970, val loss: 0.6890904903411865
Epoch 980, training loss: 625.6567993164062 = 0.29313790798187256 + 100.0 * 6.253636360168457
Epoch 980, val loss: 0.687659502029419
Epoch 990, training loss: 625.4859008789062 = 0.28403210639953613 + 100.0 * 6.252018451690674
Epoch 990, val loss: 0.6865500807762146
Epoch 1000, training loss: 625.51953125 = 0.2753388285636902 + 100.0 * 6.252441883087158
Epoch 1000, val loss: 0.6857038736343384
Epoch 1010, training loss: 625.2581176757812 = 0.26694250106811523 + 100.0 * 6.249911785125732
Epoch 1010, val loss: 0.6855008602142334
Epoch 1020, training loss: 625.1736450195312 = 0.25891438126564026 + 100.0 * 6.249147415161133
Epoch 1020, val loss: 0.685206413269043
Epoch 1030, training loss: 625.1981201171875 = 0.2512117922306061 + 100.0 * 6.249468803405762
Epoch 1030, val loss: 0.6853490471839905
Epoch 1040, training loss: 625.3433227539062 = 0.24381598830223083 + 100.0 * 6.25099515914917
Epoch 1040, val loss: 0.6855915784835815
Epoch 1050, training loss: 625.1556396484375 = 0.23661965131759644 + 100.0 * 6.249189853668213
Epoch 1050, val loss: 0.6863274574279785
Epoch 1060, training loss: 625.115966796875 = 0.2297317087650299 + 100.0 * 6.248862266540527
Epoch 1060, val loss: 0.6869388222694397
Epoch 1070, training loss: 624.9892578125 = 0.22308249771595 + 100.0 * 6.247661590576172
Epoch 1070, val loss: 0.6878589987754822
Epoch 1080, training loss: 624.835205078125 = 0.2166997641324997 + 100.0 * 6.246185302734375
Epoch 1080, val loss: 0.6889528632164001
Epoch 1090, training loss: 624.8554077148438 = 0.21055035293102264 + 100.0 * 6.246448516845703
Epoch 1090, val loss: 0.6902590990066528
Epoch 1100, training loss: 624.8870849609375 = 0.20463839173316956 + 100.0 * 6.246824264526367
Epoch 1100, val loss: 0.6915302276611328
Epoch 1110, training loss: 624.8821411132812 = 0.19890427589416504 + 100.0 * 6.246832370758057
Epoch 1110, val loss: 0.6932770609855652
Epoch 1120, training loss: 624.8564453125 = 0.1933429092168808 + 100.0 * 6.246631145477295
Epoch 1120, val loss: 0.6951266527175903
Epoch 1130, training loss: 624.6785888671875 = 0.1879585236310959 + 100.0 * 6.244905948638916
Epoch 1130, val loss: 0.6965405941009521
Epoch 1140, training loss: 624.6884765625 = 0.18278875946998596 + 100.0 * 6.245056629180908
Epoch 1140, val loss: 0.6988153457641602
Epoch 1150, training loss: 624.5615234375 = 0.17779159545898438 + 100.0 * 6.243837356567383
Epoch 1150, val loss: 0.7006729245185852
Epoch 1160, training loss: 624.5765991210938 = 0.17291614413261414 + 100.0 * 6.24403715133667
Epoch 1160, val loss: 0.7029355764389038
Epoch 1170, training loss: 624.5968017578125 = 0.1682083159685135 + 100.0 * 6.244286060333252
Epoch 1170, val loss: 0.7051082253456116
Epoch 1180, training loss: 624.4346313476562 = 0.16367822885513306 + 100.0 * 6.242709636688232
Epoch 1180, val loss: 0.7074230313301086
Epoch 1190, training loss: 624.2350463867188 = 0.1592610478401184 + 100.0 * 6.240757942199707
Epoch 1190, val loss: 0.7098276019096375
Epoch 1200, training loss: 624.2304077148438 = 0.15499994158744812 + 100.0 * 6.240753650665283
Epoch 1200, val loss: 0.7124459743499756
Epoch 1210, training loss: 624.7908325195312 = 0.1509006917476654 + 100.0 * 6.246399402618408
Epoch 1210, val loss: 0.7147253155708313
Epoch 1220, training loss: 624.2200927734375 = 0.14686386287212372 + 100.0 * 6.240732669830322
Epoch 1220, val loss: 0.7178738117218018
Epoch 1230, training loss: 624.1698608398438 = 0.1429852992296219 + 100.0 * 6.240268707275391
Epoch 1230, val loss: 0.7204224467277527
Epoch 1240, training loss: 624.017333984375 = 0.1392289400100708 + 100.0 * 6.238780975341797
Epoch 1240, val loss: 0.72331303358078
Epoch 1250, training loss: 623.9068603515625 = 0.13558270037174225 + 100.0 * 6.237712860107422
Epoch 1250, val loss: 0.7260355949401855
Epoch 1260, training loss: 624.1644287109375 = 0.1320701390504837 + 100.0 * 6.240323543548584
Epoch 1260, val loss: 0.7288289666175842
Epoch 1270, training loss: 624.0822143554688 = 0.12864884734153748 + 100.0 * 6.239535331726074
Epoch 1270, val loss: 0.7321211099624634
Epoch 1280, training loss: 623.869873046875 = 0.1253015398979187 + 100.0 * 6.237445831298828
Epoch 1280, val loss: 0.7347654104232788
Epoch 1290, training loss: 623.8528442382812 = 0.12207430601119995 + 100.0 * 6.237307548522949
Epoch 1290, val loss: 0.7381237149238586
Epoch 1300, training loss: 624.0018310546875 = 0.11894679069519043 + 100.0 * 6.238828659057617
Epoch 1300, val loss: 0.7411409616470337
Epoch 1310, training loss: 623.846923828125 = 0.11594420671463013 + 100.0 * 6.237309455871582
Epoch 1310, val loss: 0.7444819808006287
Epoch 1320, training loss: 623.6492919921875 = 0.11298785358667374 + 100.0 * 6.235363006591797
Epoch 1320, val loss: 0.7476484775543213
Epoch 1330, training loss: 623.6875610351562 = 0.1101417988538742 + 100.0 * 6.235774040222168
Epoch 1330, val loss: 0.7509902119636536
Epoch 1340, training loss: 623.7794189453125 = 0.10739069432020187 + 100.0 * 6.236720561981201
Epoch 1340, val loss: 0.7541759014129639
Epoch 1350, training loss: 623.5855712890625 = 0.10470136255025864 + 100.0 * 6.234808921813965
Epoch 1350, val loss: 0.7572287321090698
Epoch 1360, training loss: 624.16796875 = 0.1021062508225441 + 100.0 * 6.240658283233643
Epoch 1360, val loss: 0.7606462240219116
Epoch 1370, training loss: 623.621337890625 = 0.09952937066555023 + 100.0 * 6.235218048095703
Epoch 1370, val loss: 0.7636141180992126
Epoch 1380, training loss: 623.4404296875 = 0.09706734120845795 + 100.0 * 6.233433723449707
Epoch 1380, val loss: 0.7671942114830017
Epoch 1390, training loss: 623.4566040039062 = 0.09468597173690796 + 100.0 * 6.233619213104248
Epoch 1390, val loss: 0.7704502940177917
Epoch 1400, training loss: 623.7487182617188 = 0.09237316250801086 + 100.0 * 6.236563682556152
Epoch 1400, val loss: 0.7740611433982849
Epoch 1410, training loss: 623.3330078125 = 0.09011074155569077 + 100.0 * 6.232429504394531
Epoch 1410, val loss: 0.7772024869918823
Epoch 1420, training loss: 623.2326049804688 = 0.08792585134506226 + 100.0 * 6.231446743011475
Epoch 1420, val loss: 0.7805856466293335
Epoch 1430, training loss: 623.4566650390625 = 0.08581764996051788 + 100.0 * 6.233708381652832
Epoch 1430, val loss: 0.7841321229934692
Epoch 1440, training loss: 623.2076416015625 = 0.0837552472949028 + 100.0 * 6.231238842010498
Epoch 1440, val loss: 0.7873356938362122
Epoch 1450, training loss: 623.3858642578125 = 0.08176237344741821 + 100.0 * 6.233040809631348
Epoch 1450, val loss: 0.7905910015106201
Epoch 1460, training loss: 623.3855590820312 = 0.07982879132032394 + 100.0 * 6.233057022094727
Epoch 1460, val loss: 0.793914258480072
Epoch 1470, training loss: 623.3143310546875 = 0.07793974131345749 + 100.0 * 6.232364177703857
Epoch 1470, val loss: 0.7969268560409546
Epoch 1480, training loss: 623.1605224609375 = 0.07606548815965652 + 100.0 * 6.230844974517822
Epoch 1480, val loss: 0.8002991080284119
Epoch 1490, training loss: 623.1148681640625 = 0.07430019229650497 + 100.0 * 6.230405807495117
Epoch 1490, val loss: 0.8038635849952698
Epoch 1500, training loss: 623.0258178710938 = 0.07257261872291565 + 100.0 * 6.229532241821289
Epoch 1500, val loss: 0.8068474531173706
Epoch 1510, training loss: 623.0451049804688 = 0.07089826464653015 + 100.0 * 6.22974157333374
Epoch 1510, val loss: 0.8100872039794922
Epoch 1520, training loss: 623.3234252929688 = 0.06926369667053223 + 100.0 * 6.232541561126709
Epoch 1520, val loss: 0.8134937882423401
Epoch 1530, training loss: 623.25830078125 = 0.06766735017299652 + 100.0 * 6.231905937194824
Epoch 1530, val loss: 0.8170624375343323
Epoch 1540, training loss: 623.1666870117188 = 0.06612163782119751 + 100.0 * 6.231006145477295
Epoch 1540, val loss: 0.8198489546775818
Epoch 1550, training loss: 623.0714111328125 = 0.06461554020643234 + 100.0 * 6.230067729949951
Epoch 1550, val loss: 0.8231003284454346
Epoch 1560, training loss: 622.8844604492188 = 0.06313955038785934 + 100.0 * 6.228213787078857
Epoch 1560, val loss: 0.8261141777038574
Epoch 1570, training loss: 622.83447265625 = 0.06172434613108635 + 100.0 * 6.227727890014648
Epoch 1570, val loss: 0.8294053077697754
Epoch 1580, training loss: 622.9085693359375 = 0.06036429852247238 + 100.0 * 6.228482246398926
Epoch 1580, val loss: 0.8324288725852966
Epoch 1590, training loss: 622.7682495117188 = 0.05901886150240898 + 100.0 * 6.227092266082764
Epoch 1590, val loss: 0.8357597589492798
Epoch 1600, training loss: 622.91357421875 = 0.05771757662296295 + 100.0 * 6.22855806350708
Epoch 1600, val loss: 0.8392314314842224
Epoch 1610, training loss: 622.9428100585938 = 0.056449368596076965 + 100.0 * 6.22886323928833
Epoch 1610, val loss: 0.8423471450805664
Epoch 1620, training loss: 623.0465087890625 = 0.05520087853074074 + 100.0 * 6.229913234710693
Epoch 1620, val loss: 0.8450480103492737
Epoch 1630, training loss: 622.8291625976562 = 0.05398815870285034 + 100.0 * 6.227751731872559
Epoch 1630, val loss: 0.848328709602356
Epoch 1640, training loss: 622.6481323242188 = 0.052809931337833405 + 100.0 * 6.225953102111816
Epoch 1640, val loss: 0.8515891432762146
Epoch 1650, training loss: 622.5960083007812 = 0.051674939692020416 + 100.0 * 6.225442886352539
Epoch 1650, val loss: 0.8549157381057739
Epoch 1660, training loss: 622.66552734375 = 0.05057312175631523 + 100.0 * 6.226149082183838
Epoch 1660, val loss: 0.8578130006790161
Epoch 1670, training loss: 622.9413452148438 = 0.0495029091835022 + 100.0 * 6.228918552398682
Epoch 1670, val loss: 0.8604801297187805
Epoch 1680, training loss: 622.737548828125 = 0.04846687987446785 + 100.0 * 6.226891040802002
Epoch 1680, val loss: 0.8636086583137512
Epoch 1690, training loss: 622.5431518554688 = 0.04742271825671196 + 100.0 * 6.22495698928833
Epoch 1690, val loss: 0.8669770359992981
Epoch 1700, training loss: 622.3564453125 = 0.04643179103732109 + 100.0 * 6.223100185394287
Epoch 1700, val loss: 0.8696778416633606
Epoch 1710, training loss: 622.4533081054688 = 0.04547809064388275 + 100.0 * 6.224078178405762
Epoch 1710, val loss: 0.87261962890625
Epoch 1720, training loss: 623.2454833984375 = 0.044539157301187515 + 100.0 * 6.232009410858154
Epoch 1720, val loss: 0.8758418560028076
Epoch 1730, training loss: 622.8341674804688 = 0.043621789664030075 + 100.0 * 6.2279052734375
Epoch 1730, val loss: 0.8788769841194153
Epoch 1740, training loss: 622.3180541992188 = 0.04270515590906143 + 100.0 * 6.222753524780273
Epoch 1740, val loss: 0.8814119100570679
Epoch 1750, training loss: 622.2368774414062 = 0.04183756560087204 + 100.0 * 6.221950531005859
Epoch 1750, val loss: 0.8845433592796326
Epoch 1760, training loss: 622.1973266601562 = 0.0409991592168808 + 100.0 * 6.22156286239624
Epoch 1760, val loss: 0.8874367475509644
Epoch 1770, training loss: 622.5445556640625 = 0.04020066559314728 + 100.0 * 6.225043296813965
Epoch 1770, val loss: 0.8902775645256042
Epoch 1780, training loss: 622.385498046875 = 0.039383962750434875 + 100.0 * 6.223461151123047
Epoch 1780, val loss: 0.893244206905365
Epoch 1790, training loss: 622.40185546875 = 0.03859889507293701 + 100.0 * 6.2236328125
Epoch 1790, val loss: 0.8956834077835083
Epoch 1800, training loss: 622.4345703125 = 0.037817299365997314 + 100.0 * 6.223967552185059
Epoch 1800, val loss: 0.898762047290802
Epoch 1810, training loss: 622.226806640625 = 0.037075869739055634 + 100.0 * 6.221897602081299
Epoch 1810, val loss: 0.9014759063720703
Epoch 1820, training loss: 622.0880126953125 = 0.036352600902318954 + 100.0 * 6.220516204833984
Epoch 1820, val loss: 0.9040052890777588
Epoch 1830, training loss: 622.048828125 = 0.03565386310219765 + 100.0 * 6.220131874084473
Epoch 1830, val loss: 0.9068272709846497
Epoch 1840, training loss: 622.4275512695312 = 0.03498132899403572 + 100.0 * 6.223925590515137
Epoch 1840, val loss: 0.9095109105110168
Epoch 1850, training loss: 622.556884765625 = 0.034302547574043274 + 100.0 * 6.225225925445557
Epoch 1850, val loss: 0.9127991199493408
Epoch 1860, training loss: 622.2012939453125 = 0.033639296889305115 + 100.0 * 6.221676826477051
Epoch 1860, val loss: 0.9143308401107788
Epoch 1870, training loss: 622.0374145507812 = 0.03299196809530258 + 100.0 * 6.220043659210205
Epoch 1870, val loss: 0.9175916314125061
Epoch 1880, training loss: 622.0199584960938 = 0.032379016280174255 + 100.0 * 6.219876289367676
Epoch 1880, val loss: 0.9201720356941223
Epoch 1890, training loss: 622.8616333007812 = 0.03179066255688667 + 100.0 * 6.228298187255859
Epoch 1890, val loss: 0.9226388335227966
Epoch 1900, training loss: 621.9971923828125 = 0.031174590811133385 + 100.0 * 6.21966028213501
Epoch 1900, val loss: 0.9251214861869812
Epoch 1910, training loss: 621.8865356445312 = 0.030594144016504288 + 100.0 * 6.218559265136719
Epoch 1910, val loss: 0.9277332425117493
Epoch 1920, training loss: 621.8756713867188 = 0.030036725103855133 + 100.0 * 6.218456268310547
Epoch 1920, val loss: 0.9304932951927185
Epoch 1930, training loss: 622.6768188476562 = 0.02951059117913246 + 100.0 * 6.226472854614258
Epoch 1930, val loss: 0.9335058331489563
Epoch 1940, training loss: 622.0791015625 = 0.02896379493176937 + 100.0 * 6.220500946044922
Epoch 1940, val loss: 0.9348238110542297
Epoch 1950, training loss: 621.9203491210938 = 0.028436942026019096 + 100.0 * 6.218919277191162
Epoch 1950, val loss: 0.9377800822257996
Epoch 1960, training loss: 621.8849487304688 = 0.027928132563829422 + 100.0 * 6.218569755554199
Epoch 1960, val loss: 0.9400525093078613
Epoch 1970, training loss: 621.9248046875 = 0.027436550706624985 + 100.0 * 6.218973636627197
Epoch 1970, val loss: 0.942671537399292
Epoch 1980, training loss: 621.8190307617188 = 0.02695261500775814 + 100.0 * 6.217920303344727
Epoch 1980, val loss: 0.9451473951339722
Epoch 1990, training loss: 622.3421020507812 = 0.0264828372746706 + 100.0 * 6.223155975341797
Epoch 1990, val loss: 0.947780191898346
Epoch 2000, training loss: 621.8685302734375 = 0.02601769007742405 + 100.0 * 6.218425273895264
Epoch 2000, val loss: 0.9493319988250732
Epoch 2010, training loss: 621.91943359375 = 0.02556830458343029 + 100.0 * 6.218938827514648
Epoch 2010, val loss: 0.9521779417991638
Epoch 2020, training loss: 621.8333129882812 = 0.025129739195108414 + 100.0 * 6.218081951141357
Epoch 2020, val loss: 0.9543110132217407
Epoch 2030, training loss: 621.7435302734375 = 0.024694325402379036 + 100.0 * 6.217187881469727
Epoch 2030, val loss: 0.9564111828804016
Epoch 2040, training loss: 621.8244018554688 = 0.024281252175569534 + 100.0 * 6.218001365661621
Epoch 2040, val loss: 0.9589642286300659
Epoch 2050, training loss: 621.7861938476562 = 0.023876352235674858 + 100.0 * 6.217623233795166
Epoch 2050, val loss: 0.9610540270805359
Epoch 2060, training loss: 621.7456665039062 = 0.02346995286643505 + 100.0 * 6.217221736907959
Epoch 2060, val loss: 0.9630139470100403
Epoch 2070, training loss: 622.0368041992188 = 0.02308531478047371 + 100.0 * 6.220137596130371
Epoch 2070, val loss: 0.9652665853500366
Epoch 2080, training loss: 621.609130859375 = 0.02270396798849106 + 100.0 * 6.215864181518555
Epoch 2080, val loss: 0.967590868473053
Epoch 2090, training loss: 621.6849975585938 = 0.02233334816992283 + 100.0 * 6.2166266441345215
Epoch 2090, val loss: 0.9699643850326538
Epoch 2100, training loss: 622.0347290039062 = 0.02197413146495819 + 100.0 * 6.220127582550049
Epoch 2100, val loss: 0.9722949862480164
Epoch 2110, training loss: 622.0989990234375 = 0.02161499857902527 + 100.0 * 6.220773696899414
Epoch 2110, val loss: 0.9735320806503296
Epoch 2120, training loss: 621.5125122070312 = 0.021262511610984802 + 100.0 * 6.2149128913879395
Epoch 2120, val loss: 0.9762026071548462
Epoch 2130, training loss: 621.4437255859375 = 0.020923936739563942 + 100.0 * 6.214227676391602
Epoch 2130, val loss: 0.977973997592926
Epoch 2140, training loss: 621.4254760742188 = 0.02059435099363327 + 100.0 * 6.214049339294434
Epoch 2140, val loss: 0.980240523815155
Epoch 2150, training loss: 621.9931030273438 = 0.02028224989771843 + 100.0 * 6.219728469848633
Epoch 2150, val loss: 0.9829257726669312
Epoch 2160, training loss: 621.504638671875 = 0.0199611634016037 + 100.0 * 6.214846611022949
Epoch 2160, val loss: 0.9837847948074341
Epoch 2170, training loss: 621.5113525390625 = 0.019646791741251945 + 100.0 * 6.214917182922363
Epoch 2170, val loss: 0.9863166809082031
Epoch 2180, training loss: 621.6111450195312 = 0.019342757761478424 + 100.0 * 6.215918064117432
Epoch 2180, val loss: 0.9878438115119934
Epoch 2190, training loss: 621.62109375 = 0.019051307812333107 + 100.0 * 6.216020584106445
Epoch 2190, val loss: 0.9902814030647278
Epoch 2200, training loss: 621.491455078125 = 0.018763462081551552 + 100.0 * 6.214727401733398
Epoch 2200, val loss: 0.99158775806427
Epoch 2210, training loss: 621.970947265625 = 0.018484540283679962 + 100.0 * 6.219524383544922
Epoch 2210, val loss: 0.9934450387954712
Epoch 2220, training loss: 621.4716796875 = 0.018199235200881958 + 100.0 * 6.214534759521484
Epoch 2220, val loss: 0.9959658980369568
Epoch 2230, training loss: 621.2925415039062 = 0.017928680405020714 + 100.0 * 6.2127461433410645
Epoch 2230, val loss: 0.9973680377006531
Epoch 2240, training loss: 621.2985229492188 = 0.01766865886747837 + 100.0 * 6.212809085845947
Epoch 2240, val loss: 0.9996557235717773
Epoch 2250, training loss: 621.9393310546875 = 0.017426885664463043 + 100.0 * 6.219218730926514
Epoch 2250, val loss: 1.0013554096221924
Epoch 2260, training loss: 621.678955078125 = 0.017161397263407707 + 100.0 * 6.216618061065674
Epoch 2260, val loss: 1.0032488107681274
Epoch 2270, training loss: 621.4525146484375 = 0.016912244260311127 + 100.0 * 6.214355945587158
Epoch 2270, val loss: 1.004433035850525
Epoch 2280, training loss: 621.3801879882812 = 0.016665298491716385 + 100.0 * 6.213634967803955
Epoch 2280, val loss: 1.0064468383789062
Epoch 2290, training loss: 621.5015258789062 = 0.01643555238842964 + 100.0 * 6.214850902557373
Epoch 2290, val loss: 1.0083345174789429
Epoch 2300, training loss: 621.2200927734375 = 0.016194742172956467 + 100.0 * 6.212039470672607
Epoch 2300, val loss: 1.0097647905349731
Epoch 2310, training loss: 621.5836791992188 = 0.015975842252373695 + 100.0 * 6.215676784515381
Epoch 2310, val loss: 1.0116510391235352
Epoch 2320, training loss: 621.2969970703125 = 0.015752682462334633 + 100.0 * 6.212812423706055
Epoch 2320, val loss: 1.0131276845932007
Epoch 2330, training loss: 621.2257690429688 = 0.015530596487224102 + 100.0 * 6.212102890014648
Epoch 2330, val loss: 1.015103816986084
Epoch 2340, training loss: 621.4952392578125 = 0.015325214713811874 + 100.0 * 6.214798927307129
Epoch 2340, val loss: 1.0166456699371338
Epoch 2350, training loss: 621.150146484375 = 0.01511208713054657 + 100.0 * 6.211349964141846
Epoch 2350, val loss: 1.0182487964630127
Epoch 2360, training loss: 621.0917358398438 = 0.014903359115123749 + 100.0 * 6.210768699645996
Epoch 2360, val loss: 1.019803762435913
Epoch 2370, training loss: 621.3323364257812 = 0.01470930501818657 + 100.0 * 6.213176250457764
Epoch 2370, val loss: 1.021601676940918
Epoch 2380, training loss: 621.2616577148438 = 0.014513610862195492 + 100.0 * 6.2124714851379395
Epoch 2380, val loss: 1.0229190587997437
Epoch 2390, training loss: 621.171142578125 = 0.014319677837193012 + 100.0 * 6.2115678787231445
Epoch 2390, val loss: 1.0241838693618774
Epoch 2400, training loss: 621.1173706054688 = 0.01413161400705576 + 100.0 * 6.211032390594482
Epoch 2400, val loss: 1.0259169340133667
Epoch 2410, training loss: 621.361328125 = 0.01395169086754322 + 100.0 * 6.213473796844482
Epoch 2410, val loss: 1.0275301933288574
Epoch 2420, training loss: 621.5437622070312 = 0.013774571940302849 + 100.0 * 6.215299606323242
Epoch 2420, val loss: 1.028950572013855
Epoch 2430, training loss: 621.0409545898438 = 0.013590862974524498 + 100.0 * 6.210273742675781
Epoch 2430, val loss: 1.030142903327942
Epoch 2440, training loss: 621.0001220703125 = 0.013413848355412483 + 100.0 * 6.209867000579834
Epoch 2440, val loss: 1.0316541194915771
Epoch 2450, training loss: 620.9425048828125 = 0.013247677125036716 + 100.0 * 6.209292888641357
Epoch 2450, val loss: 1.0332391262054443
Epoch 2460, training loss: 621.6516723632812 = 0.013087589293718338 + 100.0 * 6.216385841369629
Epoch 2460, val loss: 1.0343555212020874
Epoch 2470, training loss: 621.0462036132812 = 0.012916416861116886 + 100.0 * 6.210332870483398
Epoch 2470, val loss: 1.0360373258590698
Epoch 2480, training loss: 621.1967163085938 = 0.012755929492413998 + 100.0 * 6.21183967590332
Epoch 2480, val loss: 1.0373895168304443
Epoch 2490, training loss: 620.9472045898438 = 0.01259705238044262 + 100.0 * 6.209346294403076
Epoch 2490, val loss: 1.0385912656784058
Epoch 2500, training loss: 621.3668212890625 = 0.012454413808882236 + 100.0 * 6.213543891906738
Epoch 2500, val loss: 1.0393227338790894
Epoch 2510, training loss: 620.9169311523438 = 0.012293331325054169 + 100.0 * 6.209046363830566
Epoch 2510, val loss: 1.041269302368164
Epoch 2520, training loss: 620.8714599609375 = 0.01214637141674757 + 100.0 * 6.208593368530273
Epoch 2520, val loss: 1.0426234006881714
Epoch 2530, training loss: 620.8944702148438 = 0.012004975229501724 + 100.0 * 6.208824157714844
Epoch 2530, val loss: 1.0437382459640503
Epoch 2540, training loss: 621.501708984375 = 0.011868461966514587 + 100.0 * 6.214898586273193
Epoch 2540, val loss: 1.0454020500183105
Epoch 2550, training loss: 621.1389770507812 = 0.01171964593231678 + 100.0 * 6.211272716522217
Epoch 2550, val loss: 1.046879529953003
Epoch 2560, training loss: 620.8521728515625 = 0.01158325094729662 + 100.0 * 6.2084059715271
Epoch 2560, val loss: 1.0474971532821655
Epoch 2570, training loss: 620.7365112304688 = 0.011446024291217327 + 100.0 * 6.207250595092773
Epoch 2570, val loss: 1.049163818359375
Epoch 2580, training loss: 620.7801513671875 = 0.011319292709231377 + 100.0 * 6.207688808441162
Epoch 2580, val loss: 1.0503413677215576
Epoch 2590, training loss: 621.844482421875 = 0.011204876005649567 + 100.0 * 6.218332290649414
Epoch 2590, val loss: 1.0519455671310425
Epoch 2600, training loss: 621.1956787109375 = 0.011063646525144577 + 100.0 * 6.211846351623535
Epoch 2600, val loss: 1.052608847618103
Epoch 2610, training loss: 620.8360595703125 = 0.010934898629784584 + 100.0 * 6.208251476287842
Epoch 2610, val loss: 1.0537039041519165
Epoch 2620, training loss: 620.8284912109375 = 0.01081083994358778 + 100.0 * 6.208177089691162
Epoch 2620, val loss: 1.055055022239685
Epoch 2630, training loss: 620.8814086914062 = 0.010691606439650059 + 100.0 * 6.208707332611084
Epoch 2630, val loss: 1.0563064813613892
Epoch 2640, training loss: 620.7132568359375 = 0.010573088191449642 + 100.0 * 6.207026958465576
Epoch 2640, val loss: 1.0573172569274902
Epoch 2650, training loss: 620.7732543945312 = 0.010458195582032204 + 100.0 * 6.20762825012207
Epoch 2650, val loss: 1.058331847190857
Epoch 2660, training loss: 620.9024047851562 = 0.010345274582505226 + 100.0 * 6.208920955657959
Epoch 2660, val loss: 1.0599788427352905
Epoch 2670, training loss: 620.9179077148438 = 0.010234576649963856 + 100.0 * 6.209076404571533
Epoch 2670, val loss: 1.060735821723938
Epoch 2680, training loss: 620.69287109375 = 0.010120936669409275 + 100.0 * 6.206827163696289
Epoch 2680, val loss: 1.0616846084594727
Epoch 2690, training loss: 620.6093139648438 = 0.010008934885263443 + 100.0 * 6.205993175506592
Epoch 2690, val loss: 1.062418818473816
Epoch 2700, training loss: 620.5869140625 = 0.009903449565172195 + 100.0 * 6.205770492553711
Epoch 2700, val loss: 1.0637304782867432
Epoch 2710, training loss: 620.8590698242188 = 0.009803926572203636 + 100.0 * 6.208492279052734
Epoch 2710, val loss: 1.0645400285720825
Epoch 2720, training loss: 620.6405639648438 = 0.009702247567474842 + 100.0 * 6.206308364868164
Epoch 2720, val loss: 1.065833568572998
Epoch 2730, training loss: 620.6959838867188 = 0.009601742029190063 + 100.0 * 6.206863880157471
Epoch 2730, val loss: 1.0669857263565063
Epoch 2740, training loss: 620.75390625 = 0.009502163156867027 + 100.0 * 6.207443714141846
Epoch 2740, val loss: 1.068062663078308
Epoch 2750, training loss: 620.7495727539062 = 0.009405887685716152 + 100.0 * 6.207401752471924
Epoch 2750, val loss: 1.0691481828689575
Epoch 2760, training loss: 620.5966796875 = 0.009309246204793453 + 100.0 * 6.205873489379883
Epoch 2760, val loss: 1.0694624185562134
Epoch 2770, training loss: 620.6905517578125 = 0.009215951897203922 + 100.0 * 6.206813335418701
Epoch 2770, val loss: 1.0702898502349854
Epoch 2780, training loss: 621.1117553710938 = 0.009123856201767921 + 100.0 * 6.211026668548584
Epoch 2780, val loss: 1.072209358215332
Epoch 2790, training loss: 620.6847534179688 = 0.009035221301019192 + 100.0 * 6.206757068634033
Epoch 2790, val loss: 1.0724055767059326
Epoch 2800, training loss: 620.460205078125 = 0.008940625935792923 + 100.0 * 6.204513072967529
Epoch 2800, val loss: 1.0734646320343018
Epoch 2810, training loss: 620.4544677734375 = 0.00885294284671545 + 100.0 * 6.204456329345703
Epoch 2810, val loss: 1.0745266675949097
Epoch 2820, training loss: 620.4908447265625 = 0.008768674917519093 + 100.0 * 6.20482063293457
Epoch 2820, val loss: 1.075608730316162
Epoch 2830, training loss: 621.0084838867188 = 0.008688217028975487 + 100.0 * 6.20999813079834
Epoch 2830, val loss: 1.076597809791565
Epoch 2840, training loss: 620.752685546875 = 0.008604524657130241 + 100.0 * 6.2074408531188965
Epoch 2840, val loss: 1.0770026445388794
Epoch 2850, training loss: 620.6860961914062 = 0.008518361486494541 + 100.0 * 6.206775665283203
Epoch 2850, val loss: 1.0778250694274902
Epoch 2860, training loss: 620.5847778320312 = 0.008440838195383549 + 100.0 * 6.205763339996338
Epoch 2860, val loss: 1.0783191919326782
Epoch 2870, training loss: 620.8005981445312 = 0.00836179032921791 + 100.0 * 6.207922458648682
Epoch 2870, val loss: 1.0795103311538696
Epoch 2880, training loss: 620.6939697265625 = 0.008279955014586449 + 100.0 * 6.206856727600098
Epoch 2880, val loss: 1.080513834953308
Epoch 2890, training loss: 620.3689575195312 = 0.008202692493796349 + 100.0 * 6.20360803604126
Epoch 2890, val loss: 1.0813355445861816
Epoch 2900, training loss: 620.2987060546875 = 0.008127071894705296 + 100.0 * 6.202906131744385
Epoch 2900, val loss: 1.0819175243377686
Epoch 2910, training loss: 620.3201293945312 = 0.008053824305534363 + 100.0 * 6.203120708465576
Epoch 2910, val loss: 1.0829325914382935
Epoch 2920, training loss: 620.8169555664062 = 0.007989325560629368 + 100.0 * 6.208089351654053
Epoch 2920, val loss: 1.0838197469711304
Epoch 2930, training loss: 620.47119140625 = 0.00790964812040329 + 100.0 * 6.204632759094238
Epoch 2930, val loss: 1.0840197801589966
Epoch 2940, training loss: 620.3540649414062 = 0.007838284596800804 + 100.0 * 6.203462600708008
Epoch 2940, val loss: 1.085340976715088
Epoch 2950, training loss: 620.2984619140625 = 0.0077665261924266815 + 100.0 * 6.202907085418701
Epoch 2950, val loss: 1.0860745906829834
Epoch 2960, training loss: 620.3322143554688 = 0.007698517292737961 + 100.0 * 6.203245162963867
Epoch 2960, val loss: 1.0869410037994385
Epoch 2970, training loss: 620.5575561523438 = 0.007634216453880072 + 100.0 * 6.205499172210693
Epoch 2970, val loss: 1.0876362323760986
Epoch 2980, training loss: 620.8504638671875 = 0.007570462301373482 + 100.0 * 6.208428859710693
Epoch 2980, val loss: 1.0879253149032593
Epoch 2990, training loss: 620.5521850585938 = 0.007497273851186037 + 100.0 * 6.205446720123291
Epoch 2990, val loss: 1.0897655487060547
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 861.6285400390625 = 1.9425816535949707 + 100.0 * 8.5968599319458
Epoch 0, val loss: 1.9390579462051392
Epoch 10, training loss: 861.547607421875 = 1.9340258836746216 + 100.0 * 8.596136093139648
Epoch 10, val loss: 1.930810809135437
Epoch 20, training loss: 861.0088500976562 = 1.9234594106674194 + 100.0 * 8.590853691101074
Epoch 20, val loss: 1.9203945398330688
Epoch 30, training loss: 857.2637939453125 = 1.9094958305358887 + 100.0 * 8.553543090820312
Epoch 30, val loss: 1.9063342809677124
Epoch 40, training loss: 834.6146240234375 = 1.8911395072937012 + 100.0 * 8.327235221862793
Epoch 40, val loss: 1.8879146575927734
Epoch 50, training loss: 764.201171875 = 1.8691540956497192 + 100.0 * 7.623320579528809
Epoch 50, val loss: 1.86628258228302
Epoch 60, training loss: 733.192626953125 = 1.8527840375900269 + 100.0 * 7.313398361206055
Epoch 60, val loss: 1.8514093160629272
Epoch 70, training loss: 717.1192626953125 = 1.8393045663833618 + 100.0 * 7.152799606323242
Epoch 70, val loss: 1.8386881351470947
Epoch 80, training loss: 707.8216552734375 = 1.8256665468215942 + 100.0 * 7.059959411621094
Epoch 80, val loss: 1.8256256580352783
Epoch 90, training loss: 698.2867431640625 = 1.8119622468948364 + 100.0 * 6.964747905731201
Epoch 90, val loss: 1.812974214553833
Epoch 100, training loss: 690.457763671875 = 1.8002116680145264 + 100.0 * 6.886575222015381
Epoch 100, val loss: 1.8022099733352661
Epoch 110, training loss: 683.9910278320312 = 1.7897155284881592 + 100.0 * 6.822012901306152
Epoch 110, val loss: 1.7924045324325562
Epoch 120, training loss: 678.8807983398438 = 1.7791237831115723 + 100.0 * 6.771017074584961
Epoch 120, val loss: 1.7820794582366943
Epoch 130, training loss: 674.6321411132812 = 1.7674273252487183 + 100.0 * 6.728646755218506
Epoch 130, val loss: 1.7708879709243774
Epoch 140, training loss: 670.085205078125 = 1.7554957866668701 + 100.0 * 6.683297157287598
Epoch 140, val loss: 1.7597180604934692
Epoch 150, training loss: 665.7339477539062 = 1.7433664798736572 + 100.0 * 6.63990592956543
Epoch 150, val loss: 1.7485431432724
Epoch 160, training loss: 661.7298583984375 = 1.7306076288223267 + 100.0 * 6.599992752075195
Epoch 160, val loss: 1.7367579936981201
Epoch 170, training loss: 658.9290161132812 = 1.7165173292160034 + 100.0 * 6.572125434875488
Epoch 170, val loss: 1.7239540815353394
Epoch 180, training loss: 656.2529296875 = 1.7004345655441284 + 100.0 * 6.545524597167969
Epoch 180, val loss: 1.7097288370132446
Epoch 190, training loss: 654.1428833007812 = 1.6827152967453003 + 100.0 * 6.524601459503174
Epoch 190, val loss: 1.6940447092056274
Epoch 200, training loss: 652.4468383789062 = 1.6634771823883057 + 100.0 * 6.507833957672119
Epoch 200, val loss: 1.677024483680725
Epoch 210, training loss: 650.9957885742188 = 1.6426663398742676 + 100.0 * 6.493531227111816
Epoch 210, val loss: 1.6587169170379639
Epoch 220, training loss: 649.6582641601562 = 1.6203309297561646 + 100.0 * 6.480379104614258
Epoch 220, val loss: 1.6391202211380005
Epoch 230, training loss: 648.3926391601562 = 1.596450686454773 + 100.0 * 6.467962265014648
Epoch 230, val loss: 1.6184852123260498
Epoch 240, training loss: 647.0316162109375 = 1.5714884996414185 + 100.0 * 6.454601287841797
Epoch 240, val loss: 1.596976399421692
Epoch 250, training loss: 645.9091186523438 = 1.5454816818237305 + 100.0 * 6.443636417388916
Epoch 250, val loss: 1.5747123956680298
Epoch 260, training loss: 645.336181640625 = 1.5183976888656616 + 100.0 * 6.438178062438965
Epoch 260, val loss: 1.5516993999481201
Epoch 270, training loss: 644.0855102539062 = 1.490714430809021 + 100.0 * 6.425947666168213
Epoch 270, val loss: 1.5281227827072144
Epoch 280, training loss: 643.0555419921875 = 1.4626299142837524 + 100.0 * 6.415928840637207
Epoch 280, val loss: 1.5045945644378662
Epoch 290, training loss: 642.1962890625 = 1.4345390796661377 + 100.0 * 6.407618045806885
Epoch 290, val loss: 1.4811068773269653
Epoch 300, training loss: 641.4170532226562 = 1.4065414667129517 + 100.0 * 6.400104999542236
Epoch 300, val loss: 1.45791757106781
Epoch 310, training loss: 641.0244140625 = 1.3787139654159546 + 100.0 * 6.396456718444824
Epoch 310, val loss: 1.4350651502609253
Epoch 320, training loss: 640.032958984375 = 1.3513009548187256 + 100.0 * 6.386816501617432
Epoch 320, val loss: 1.4126620292663574
Epoch 330, training loss: 639.4925537109375 = 1.3244166374206543 + 100.0 * 6.381681442260742
Epoch 330, val loss: 1.3908815383911133
Epoch 340, training loss: 638.894287109375 = 1.2981147766113281 + 100.0 * 6.375961780548096
Epoch 340, val loss: 1.3697798252105713
Epoch 350, training loss: 638.5552978515625 = 1.2724260091781616 + 100.0 * 6.372828960418701
Epoch 350, val loss: 1.3493177890777588
Epoch 360, training loss: 638.2222290039062 = 1.2472554445266724 + 100.0 * 6.369750022888184
Epoch 360, val loss: 1.3293604850769043
Epoch 370, training loss: 637.5090942382812 = 1.2228416204452515 + 100.0 * 6.362862586975098
Epoch 370, val loss: 1.3101801872253418
Epoch 380, training loss: 637.0108032226562 = 1.1991692781448364 + 100.0 * 6.358116149902344
Epoch 380, val loss: 1.2918692827224731
Epoch 390, training loss: 636.5472412109375 = 1.1762371063232422 + 100.0 * 6.353710174560547
Epoch 390, val loss: 1.2742714881896973
Epoch 400, training loss: 636.1239013671875 = 1.1540231704711914 + 100.0 * 6.349698543548584
Epoch 400, val loss: 1.2572753429412842
Epoch 410, training loss: 637.1260986328125 = 1.132517695426941 + 100.0 * 6.359935760498047
Epoch 410, val loss: 1.2410893440246582
Epoch 420, training loss: 635.8704223632812 = 1.111219048500061 + 100.0 * 6.347591876983643
Epoch 420, val loss: 1.224921464920044
Epoch 430, training loss: 635.0784912109375 = 1.09058678150177 + 100.0 * 6.339879035949707
Epoch 430, val loss: 1.209423542022705
Epoch 440, training loss: 634.7380981445312 = 1.0705503225326538 + 100.0 * 6.33667516708374
Epoch 440, val loss: 1.1944732666015625
Epoch 450, training loss: 634.42333984375 = 1.0509803295135498 + 100.0 * 6.333724021911621
Epoch 450, val loss: 1.1800167560577393
Epoch 460, training loss: 634.4304809570312 = 1.0317418575286865 + 100.0 * 6.333987236022949
Epoch 460, val loss: 1.1658450365066528
Epoch 470, training loss: 633.9994506835938 = 1.012748122215271 + 100.0 * 6.329866886138916
Epoch 470, val loss: 1.1520355939865112
Epoch 480, training loss: 633.5528564453125 = 0.9941024780273438 + 100.0 * 6.325587749481201
Epoch 480, val loss: 1.1384916305541992
Epoch 490, training loss: 633.3639526367188 = 0.9757243394851685 + 100.0 * 6.323882579803467
Epoch 490, val loss: 1.1253427267074585
Epoch 500, training loss: 633.0899047851562 = 0.9575759172439575 + 100.0 * 6.321323394775391
Epoch 500, val loss: 1.1123508214950562
Epoch 510, training loss: 632.7637939453125 = 0.9395317435264587 + 100.0 * 6.31824254989624
Epoch 510, val loss: 1.0995697975158691
Epoch 520, training loss: 632.5755004882812 = 0.9217718839645386 + 100.0 * 6.316537380218506
Epoch 520, val loss: 1.0870295763015747
Epoch 530, training loss: 632.3989868164062 = 0.9040149450302124 + 100.0 * 6.314949989318848
Epoch 530, val loss: 1.0748766660690308
Epoch 540, training loss: 632.0370483398438 = 0.8863635063171387 + 100.0 * 6.311507225036621
Epoch 540, val loss: 1.0624816417694092
Epoch 550, training loss: 631.8006591796875 = 0.8689016103744507 + 100.0 * 6.309317588806152
Epoch 550, val loss: 1.050653100013733
Epoch 560, training loss: 632.078857421875 = 0.8515129685401917 + 100.0 * 6.3122735023498535
Epoch 560, val loss: 1.0389050245285034
Epoch 570, training loss: 631.733154296875 = 0.8343477249145508 + 100.0 * 6.308988094329834
Epoch 570, val loss: 1.0277284383773804
Epoch 580, training loss: 631.1582641601562 = 0.8171349763870239 + 100.0 * 6.303411483764648
Epoch 580, val loss: 1.0165525674819946
Epoch 590, training loss: 630.9490356445312 = 0.8001022934913635 + 100.0 * 6.301489353179932
Epoch 590, val loss: 1.0056391954421997
Epoch 600, training loss: 631.295166015625 = 0.7831960916519165 + 100.0 * 6.305119514465332
Epoch 600, val loss: 0.9951710104942322
Epoch 610, training loss: 630.7559814453125 = 0.7663113474845886 + 100.0 * 6.299896717071533
Epoch 610, val loss: 0.9848084449768066
Epoch 620, training loss: 630.3631591796875 = 0.7495042085647583 + 100.0 * 6.296136379241943
Epoch 620, val loss: 0.9746083617210388
Epoch 630, training loss: 630.2399291992188 = 0.7327835559844971 + 100.0 * 6.295071601867676
Epoch 630, val loss: 0.9646903872489929
Epoch 640, training loss: 630.1993408203125 = 0.7161537408828735 + 100.0 * 6.294832229614258
Epoch 640, val loss: 0.95513916015625
Epoch 650, training loss: 630.0545043945312 = 0.69953852891922 + 100.0 * 6.29355001449585
Epoch 650, val loss: 0.9460625052452087
Epoch 660, training loss: 629.8666381835938 = 0.6831449866294861 + 100.0 * 6.291834831237793
Epoch 660, val loss: 0.9371798038482666
Epoch 670, training loss: 629.9954223632812 = 0.6668158173561096 + 100.0 * 6.293285846710205
Epoch 670, val loss: 0.9286234378814697
Epoch 680, training loss: 629.47119140625 = 0.6506497859954834 + 100.0 * 6.288205623626709
Epoch 680, val loss: 0.9205618500709534
Epoch 690, training loss: 629.2748413085938 = 0.6346494555473328 + 100.0 * 6.286402225494385
Epoch 690, val loss: 0.9126564860343933
Epoch 700, training loss: 629.1787719726562 = 0.6187679171562195 + 100.0 * 6.285600185394287
Epoch 700, val loss: 0.9050712585449219
Epoch 710, training loss: 629.420166015625 = 0.6029410362243652 + 100.0 * 6.288172245025635
Epoch 710, val loss: 0.8977404236793518
Epoch 720, training loss: 629.1148071289062 = 0.5871713161468506 + 100.0 * 6.285276412963867
Epoch 720, val loss: 0.8906970024108887
Epoch 730, training loss: 628.7984008789062 = 0.5717095732688904 + 100.0 * 6.282266616821289
Epoch 730, val loss: 0.8839423656463623
Epoch 740, training loss: 628.8408813476562 = 0.5562946796417236 + 100.0 * 6.282845973968506
Epoch 740, val loss: 0.8773059844970703
Epoch 750, training loss: 628.6217041015625 = 0.5411253571510315 + 100.0 * 6.280805587768555
Epoch 750, val loss: 0.8709790110588074
Epoch 760, training loss: 628.3389282226562 = 0.5261019468307495 + 100.0 * 6.278128147125244
Epoch 760, val loss: 0.8650733828544617
Epoch 770, training loss: 628.1874389648438 = 0.5114009976387024 + 100.0 * 6.276760101318359
Epoch 770, val loss: 0.8594318628311157
Epoch 780, training loss: 628.4751586914062 = 0.4969441294670105 + 100.0 * 6.279782295227051
Epoch 780, val loss: 0.8541989922523499
Epoch 790, training loss: 628.2742919921875 = 0.4826183021068573 + 100.0 * 6.27791690826416
Epoch 790, val loss: 0.8489981889724731
Epoch 800, training loss: 627.9207763671875 = 0.4685686230659485 + 100.0 * 6.274522304534912
Epoch 800, val loss: 0.8442209959030151
Epoch 810, training loss: 628.0112915039062 = 0.45479872822761536 + 100.0 * 6.275565147399902
Epoch 810, val loss: 0.8396138548851013
Epoch 820, training loss: 627.7101440429688 = 0.441341370344162 + 100.0 * 6.272687911987305
Epoch 820, val loss: 0.8354477882385254
Epoch 830, training loss: 627.71484375 = 0.42813554406166077 + 100.0 * 6.272866725921631
Epoch 830, val loss: 0.8314900994300842
Epoch 840, training loss: 627.4337768554688 = 0.41517579555511475 + 100.0 * 6.270185947418213
Epoch 840, val loss: 0.8280261754989624
Epoch 850, training loss: 627.293212890625 = 0.40262243151664734 + 100.0 * 6.2689056396484375
Epoch 850, val loss: 0.8246957063674927
Epoch 860, training loss: 627.5371704101562 = 0.39038825035095215 + 100.0 * 6.271468162536621
Epoch 860, val loss: 0.8217341899871826
Epoch 870, training loss: 627.4869995117188 = 0.3783615231513977 + 100.0 * 6.2710862159729
Epoch 870, val loss: 0.8190597891807556
Epoch 880, training loss: 627.1843872070312 = 0.36675482988357544 + 100.0 * 6.268176555633545
Epoch 880, val loss: 0.8167284727096558
Epoch 890, training loss: 626.98876953125 = 0.35548505187034607 + 100.0 * 6.266333103179932
Epoch 890, val loss: 0.814788281917572
Epoch 900, training loss: 627.3463134765625 = 0.34457895159721375 + 100.0 * 6.270017147064209
Epoch 900, val loss: 0.8129606246948242
Epoch 910, training loss: 627.0848999023438 = 0.3338250517845154 + 100.0 * 6.267510890960693
Epoch 910, val loss: 0.8117215633392334
Epoch 920, training loss: 626.75341796875 = 0.3235844671726227 + 100.0 * 6.264297962188721
Epoch 920, val loss: 0.8106651902198792
Epoch 930, training loss: 626.6193237304688 = 0.31361261010169983 + 100.0 * 6.263057231903076
Epoch 930, val loss: 0.8099690079689026
Epoch 940, training loss: 626.8099365234375 = 0.3040017783641815 + 100.0 * 6.265059471130371
Epoch 940, val loss: 0.8095258474349976
Epoch 950, training loss: 626.8417358398438 = 0.2947099208831787 + 100.0 * 6.265470027923584
Epoch 950, val loss: 0.8091303110122681
Epoch 960, training loss: 626.4478149414062 = 0.2856738865375519 + 100.0 * 6.261621475219727
Epoch 960, val loss: 0.8094091415405273
Epoch 970, training loss: 626.28662109375 = 0.2770623564720154 + 100.0 * 6.260095596313477
Epoch 970, val loss: 0.8098137974739075
Epoch 980, training loss: 626.2432250976562 = 0.26874175667762756 + 100.0 * 6.259744644165039
Epoch 980, val loss: 0.8104639053344727
Epoch 990, training loss: 626.4861450195312 = 0.2607131600379944 + 100.0 * 6.262253761291504
Epoch 990, val loss: 0.8113659620285034
Epoch 1000, training loss: 626.2800903320312 = 0.25289255380630493 + 100.0 * 6.260272026062012
Epoch 1000, val loss: 0.8124966025352478
Epoch 1010, training loss: 626.0918579101562 = 0.2454119622707367 + 100.0 * 6.258464336395264
Epoch 1010, val loss: 0.8138869404792786
Epoch 1020, training loss: 626.0820922851562 = 0.2381463497877121 + 100.0 * 6.258439064025879
Epoch 1020, val loss: 0.8154294490814209
Epoch 1030, training loss: 625.7930297851562 = 0.23122303187847137 + 100.0 * 6.255618572235107
Epoch 1030, val loss: 0.817236065864563
Epoch 1040, training loss: 625.8743896484375 = 0.22454914450645447 + 100.0 * 6.256498336791992
Epoch 1040, val loss: 0.8190743327140808
Epoch 1050, training loss: 625.8323974609375 = 0.21804066002368927 + 100.0 * 6.256143569946289
Epoch 1050, val loss: 0.821358323097229
Epoch 1060, training loss: 625.6890869140625 = 0.2117978036403656 + 100.0 * 6.254773139953613
Epoch 1060, val loss: 0.8236736059188843
Epoch 1070, training loss: 625.4872436523438 = 0.2057827115058899 + 100.0 * 6.252814292907715
Epoch 1070, val loss: 0.826337993144989
Epoch 1080, training loss: 625.95361328125 = 0.19999593496322632 + 100.0 * 6.257535934448242
Epoch 1080, val loss: 0.8290854096412659
Epoch 1090, training loss: 625.5081787109375 = 0.19438111782073975 + 100.0 * 6.253138065338135
Epoch 1090, val loss: 0.831744372844696
Epoch 1100, training loss: 625.3916015625 = 0.18896964192390442 + 100.0 * 6.252026557922363
Epoch 1100, val loss: 0.8348453640937805
Epoch 1110, training loss: 625.428955078125 = 0.1837610900402069 + 100.0 * 6.2524518966674805
Epoch 1110, val loss: 0.8378226161003113
Epoch 1120, training loss: 625.36572265625 = 0.17872828245162964 + 100.0 * 6.251870155334473
Epoch 1120, val loss: 0.8410649299621582
Epoch 1130, training loss: 625.2492065429688 = 0.17386466264724731 + 100.0 * 6.250752925872803
Epoch 1130, val loss: 0.8444571495056152
Epoch 1140, training loss: 625.4160766601562 = 0.1691504418849945 + 100.0 * 6.252469539642334
Epoch 1140, val loss: 0.8477787375450134
Epoch 1150, training loss: 625.2632446289062 = 0.1646394580602646 + 100.0 * 6.250986576080322
Epoch 1150, val loss: 0.8515730500221252
Epoch 1160, training loss: 625.0144653320312 = 0.1602376252412796 + 100.0 * 6.248542785644531
Epoch 1160, val loss: 0.8548824787139893
Epoch 1170, training loss: 624.9884033203125 = 0.15602028369903564 + 100.0 * 6.248323917388916
Epoch 1170, val loss: 0.8587033152580261
Epoch 1180, training loss: 625.206298828125 = 0.15191450715065002 + 100.0 * 6.250543594360352
Epoch 1180, val loss: 0.8623495101928711
Epoch 1190, training loss: 624.9146118164062 = 0.14790178835391998 + 100.0 * 6.24766731262207
Epoch 1190, val loss: 0.8663670420646667
Epoch 1200, training loss: 624.9716796875 = 0.14406120777130127 + 100.0 * 6.248276233673096
Epoch 1200, val loss: 0.8702446818351746
Epoch 1210, training loss: 624.6124267578125 = 0.1403350979089737 + 100.0 * 6.244720935821533
Epoch 1210, val loss: 0.874134361743927
Epoch 1220, training loss: 625.0070190429688 = 0.1367613822221756 + 100.0 * 6.248702526092529
Epoch 1220, val loss: 0.878095269203186
Epoch 1230, training loss: 624.8927612304688 = 0.13321255147457123 + 100.0 * 6.247595310211182
Epoch 1230, val loss: 0.8823977708816528
Epoch 1240, training loss: 624.6224975585938 = 0.12983408570289612 + 100.0 * 6.244926452636719
Epoch 1240, val loss: 0.8862491250038147
Epoch 1250, training loss: 624.411865234375 = 0.1265380084514618 + 100.0 * 6.242853164672852
Epoch 1250, val loss: 0.8905893564224243
Epoch 1260, training loss: 624.3373413085938 = 0.1233673095703125 + 100.0 * 6.24213981628418
Epoch 1260, val loss: 0.8948510885238647
Epoch 1270, training loss: 625.0029907226562 = 0.12029624730348587 + 100.0 * 6.24882698059082
Epoch 1270, val loss: 0.899062991142273
Epoch 1280, training loss: 624.6529541015625 = 0.11728682368993759 + 100.0 * 6.245356559753418
Epoch 1280, val loss: 0.903520941734314
Epoch 1290, training loss: 624.3948364257812 = 0.11436261236667633 + 100.0 * 6.242804527282715
Epoch 1290, val loss: 0.9077938795089722
Epoch 1300, training loss: 624.548828125 = 0.1115703210234642 + 100.0 * 6.244372844696045
Epoch 1300, val loss: 0.9123268127441406
Epoch 1310, training loss: 624.1190795898438 = 0.10881007462739944 + 100.0 * 6.240102767944336
Epoch 1310, val loss: 0.9165955781936646
Epoch 1320, training loss: 624.1172485351562 = 0.10615705698728561 + 100.0 * 6.240110874176025
Epoch 1320, val loss: 0.9211405515670776
Epoch 1330, training loss: 624.1099243164062 = 0.10361402481794357 + 100.0 * 6.240062713623047
Epoch 1330, val loss: 0.9256284236907959
Epoch 1340, training loss: 624.5636596679688 = 0.10112524032592773 + 100.0 * 6.244625091552734
Epoch 1340, val loss: 0.9299598932266235
Epoch 1350, training loss: 624.123779296875 = 0.0986749604344368 + 100.0 * 6.240251064300537
Epoch 1350, val loss: 0.9345269799232483
Epoch 1360, training loss: 624.339599609375 = 0.09634305536746979 + 100.0 * 6.242432594299316
Epoch 1360, val loss: 0.9389762878417969
Epoch 1370, training loss: 623.8392333984375 = 0.09403172135353088 + 100.0 * 6.237451553344727
Epoch 1370, val loss: 0.9435277581214905
Epoch 1380, training loss: 623.8052368164062 = 0.09182153642177582 + 100.0 * 6.2371344566345215
Epoch 1380, val loss: 0.9480986595153809
Epoch 1390, training loss: 623.75146484375 = 0.08967601507902145 + 100.0 * 6.2366180419921875
Epoch 1390, val loss: 0.952681839466095
Epoch 1400, training loss: 624.0318603515625 = 0.08760010451078415 + 100.0 * 6.239442825317383
Epoch 1400, val loss: 0.9573549628257751
Epoch 1410, training loss: 623.9341430664062 = 0.08555569499731064 + 100.0 * 6.238485813140869
Epoch 1410, val loss: 0.9615038633346558
Epoch 1420, training loss: 623.7691650390625 = 0.0835651084780693 + 100.0 * 6.236855983734131
Epoch 1420, val loss: 0.9662044048309326
Epoch 1430, training loss: 624.1328125 = 0.08164031058549881 + 100.0 * 6.240511417388916
Epoch 1430, val loss: 0.9706294536590576
Epoch 1440, training loss: 623.6320190429688 = 0.07978083193302155 + 100.0 * 6.235522270202637
Epoch 1440, val loss: 0.9750145673751831
Epoch 1450, training loss: 623.5509033203125 = 0.07796527445316315 + 100.0 * 6.234729766845703
Epoch 1450, val loss: 0.979576051235199
Epoch 1460, training loss: 623.532470703125 = 0.07621193677186966 + 100.0 * 6.234562397003174
Epoch 1460, val loss: 0.9840516448020935
Epoch 1470, training loss: 624.0468139648438 = 0.07451309263706207 + 100.0 * 6.239722728729248
Epoch 1470, val loss: 0.9885396361351013
Epoch 1480, training loss: 623.67919921875 = 0.07281839847564697 + 100.0 * 6.2360639572143555
Epoch 1480, val loss: 0.992803156375885
Epoch 1490, training loss: 623.54296875 = 0.07119341939687729 + 100.0 * 6.234718322753906
Epoch 1490, val loss: 0.9973611235618591
Epoch 1500, training loss: 623.5784912109375 = 0.06961230933666229 + 100.0 * 6.235088348388672
Epoch 1500, val loss: 1.0017378330230713
Epoch 1510, training loss: 623.4266967773438 = 0.06808914989233017 + 100.0 * 6.233585834503174
Epoch 1510, val loss: 1.0061522722244263
Epoch 1520, training loss: 623.8069458007812 = 0.06660082191228867 + 100.0 * 6.237403392791748
Epoch 1520, val loss: 1.0105639696121216
Epoch 1530, training loss: 623.396728515625 = 0.06512565910816193 + 100.0 * 6.233315944671631
Epoch 1530, val loss: 1.014695405960083
Epoch 1540, training loss: 623.2552490234375 = 0.06371086835861206 + 100.0 * 6.231915473937988
Epoch 1540, val loss: 1.0190879106521606
Epoch 1550, training loss: 623.2354125976562 = 0.062334537506103516 + 100.0 * 6.231730937957764
Epoch 1550, val loss: 1.0235270261764526
Epoch 1560, training loss: 623.4400024414062 = 0.061008572578430176 + 100.0 * 6.233789920806885
Epoch 1560, val loss: 1.0278371572494507
Epoch 1570, training loss: 623.2742919921875 = 0.05969740450382233 + 100.0 * 6.232146263122559
Epoch 1570, val loss: 1.031697392463684
Epoch 1580, training loss: 623.1134643554688 = 0.05842498317360878 + 100.0 * 6.230550765991211
Epoch 1580, val loss: 1.0362379550933838
Epoch 1590, training loss: 623.3461303710938 = 0.05718206614255905 + 100.0 * 6.232889652252197
Epoch 1590, val loss: 1.0405163764953613
Epoch 1600, training loss: 623.163330078125 = 0.05596758425235748 + 100.0 * 6.23107385635376
Epoch 1600, val loss: 1.0444546937942505
Epoch 1610, training loss: 622.9562377929688 = 0.05478726327419281 + 100.0 * 6.2290143966674805
Epoch 1610, val loss: 1.048594355583191
Epoch 1620, training loss: 622.9171142578125 = 0.053654130548238754 + 100.0 * 6.228634357452393
Epoch 1620, val loss: 1.052983283996582
Epoch 1630, training loss: 622.9827270507812 = 0.05255652964115143 + 100.0 * 6.229301452636719
Epoch 1630, val loss: 1.057059407234192
Epoch 1640, training loss: 623.4578857421875 = 0.05148050934076309 + 100.0 * 6.23406457901001
Epoch 1640, val loss: 1.0611648559570312
Epoch 1650, training loss: 623.3572387695312 = 0.05040853098034859 + 100.0 * 6.233067989349365
Epoch 1650, val loss: 1.0650660991668701
Epoch 1660, training loss: 622.9093017578125 = 0.049382440745830536 + 100.0 * 6.2285990715026855
Epoch 1660, val loss: 1.069118618965149
Epoch 1670, training loss: 622.8001098632812 = 0.04837504401803017 + 100.0 * 6.227517127990723
Epoch 1670, val loss: 1.073168158531189
Epoch 1680, training loss: 622.750732421875 = 0.047410234808921814 + 100.0 * 6.2270331382751465
Epoch 1680, val loss: 1.0771887302398682
Epoch 1690, training loss: 623.0648803710938 = 0.04646991938352585 + 100.0 * 6.230184555053711
Epoch 1690, val loss: 1.0811971426010132
Epoch 1700, training loss: 622.8284301757812 = 0.0455368272960186 + 100.0 * 6.2278289794921875
Epoch 1700, val loss: 1.0852315425872803
Epoch 1710, training loss: 623.0274658203125 = 0.04462533816695213 + 100.0 * 6.229828357696533
Epoch 1710, val loss: 1.0890320539474487
Epoch 1720, training loss: 622.8150024414062 = 0.043741706758737564 + 100.0 * 6.227712154388428
Epoch 1720, val loss: 1.0927941799163818
Epoch 1730, training loss: 622.6557006835938 = 0.04288078472018242 + 100.0 * 6.226127624511719
Epoch 1730, val loss: 1.0966572761535645
Epoch 1740, training loss: 622.6384887695312 = 0.042046960443258286 + 100.0 * 6.225964546203613
Epoch 1740, val loss: 1.1006275415420532
Epoch 1750, training loss: 622.8839721679688 = 0.04123672470450401 + 100.0 * 6.228426933288574
Epoch 1750, val loss: 1.1044069528579712
Epoch 1760, training loss: 622.5965576171875 = 0.04043816030025482 + 100.0 * 6.225561141967773
Epoch 1760, val loss: 1.1080379486083984
Epoch 1770, training loss: 622.6917114257812 = 0.039665594696998596 + 100.0 * 6.226520538330078
Epoch 1770, val loss: 1.1114633083343506
Epoch 1780, training loss: 622.8145141601562 = 0.038914311677217484 + 100.0 * 6.227755546569824
Epoch 1780, val loss: 1.115332841873169
Epoch 1790, training loss: 622.6072998046875 = 0.038176849484443665 + 100.0 * 6.225691318511963
Epoch 1790, val loss: 1.1191352605819702
Epoch 1800, training loss: 622.7351684570312 = 0.03746305778622627 + 100.0 * 6.2269768714904785
Epoch 1800, val loss: 1.122814416885376
Epoch 1810, training loss: 622.4664306640625 = 0.03674805536866188 + 100.0 * 6.224297046661377
Epoch 1810, val loss: 1.126295804977417
Epoch 1820, training loss: 622.6242065429688 = 0.036068834364414215 + 100.0 * 6.225881576538086
Epoch 1820, val loss: 1.130205750465393
Epoch 1830, training loss: 622.6217651367188 = 0.03540610894560814 + 100.0 * 6.225863456726074
Epoch 1830, val loss: 1.1334872245788574
Epoch 1840, training loss: 622.4816284179688 = 0.03475291654467583 + 100.0 * 6.22446870803833
Epoch 1840, val loss: 1.13702392578125
Epoch 1850, training loss: 622.5057373046875 = 0.034118860960006714 + 100.0 * 6.2247161865234375
Epoch 1850, val loss: 1.1407450437545776
Epoch 1860, training loss: 622.396240234375 = 0.033497877418994904 + 100.0 * 6.22362756729126
Epoch 1860, val loss: 1.1440362930297852
Epoch 1870, training loss: 622.3912963867188 = 0.032899364829063416 + 100.0 * 6.223584175109863
Epoch 1870, val loss: 1.1473684310913086
Epoch 1880, training loss: 622.50390625 = 0.03230985626578331 + 100.0 * 6.2247161865234375
Epoch 1880, val loss: 1.150932788848877
Epoch 1890, training loss: 622.2738647460938 = 0.03174246847629547 + 100.0 * 6.222421169281006
Epoch 1890, val loss: 1.1544432640075684
Epoch 1900, training loss: 622.1727294921875 = 0.031182406470179558 + 100.0 * 6.2214155197143555
Epoch 1900, val loss: 1.1578178405761719
Epoch 1910, training loss: 622.703125 = 0.03064800798892975 + 100.0 * 6.226724624633789
Epoch 1910, val loss: 1.1610521078109741
Epoch 1920, training loss: 622.482177734375 = 0.030104655772447586 + 100.0 * 6.224521160125732
Epoch 1920, val loss: 1.1645348072052002
Epoch 1930, training loss: 622.3646850585938 = 0.029569245874881744 + 100.0 * 6.22335147857666
Epoch 1930, val loss: 1.1675573587417603
Epoch 1940, training loss: 622.11669921875 = 0.029059287160634995 + 100.0 * 6.220876693725586
Epoch 1940, val loss: 1.1711275577545166
Epoch 1950, training loss: 622.0322265625 = 0.028563834726810455 + 100.0 * 6.220036506652832
Epoch 1950, val loss: 1.1743959188461304
Epoch 1960, training loss: 622.2953491210938 = 0.028085101395845413 + 100.0 * 6.222672939300537
Epoch 1960, val loss: 1.1776418685913086
Epoch 1970, training loss: 622.05078125 = 0.027614645659923553 + 100.0 * 6.220232009887695
Epoch 1970, val loss: 1.1807352304458618
Epoch 1980, training loss: 622.1506958007812 = 0.027153728529810905 + 100.0 * 6.221235275268555
Epoch 1980, val loss: 1.1838265657424927
Epoch 1990, training loss: 622.33837890625 = 0.026702623814344406 + 100.0 * 6.223116874694824
Epoch 1990, val loss: 1.1872180700302124
Epoch 2000, training loss: 622.2250366210938 = 0.026255663484334946 + 100.0 * 6.221988201141357
Epoch 2000, val loss: 1.1904138326644897
Epoch 2010, training loss: 622.00732421875 = 0.02582363784313202 + 100.0 * 6.219815254211426
Epoch 2010, val loss: 1.1934552192687988
Epoch 2020, training loss: 621.9195556640625 = 0.025402363389730453 + 100.0 * 6.218941688537598
Epoch 2020, val loss: 1.1965441703796387
Epoch 2030, training loss: 622.2869873046875 = 0.025000818073749542 + 100.0 * 6.222619533538818
Epoch 2030, val loss: 1.199641466140747
Epoch 2040, training loss: 621.9564208984375 = 0.024594590067863464 + 100.0 * 6.219318389892578
Epoch 2040, val loss: 1.2026560306549072
Epoch 2050, training loss: 622.1090698242188 = 0.024204090237617493 + 100.0 * 6.220848083496094
Epoch 2050, val loss: 1.2058073282241821
Epoch 2060, training loss: 621.835205078125 = 0.023809470236301422 + 100.0 * 6.218113899230957
Epoch 2060, val loss: 1.2087618112564087
Epoch 2070, training loss: 621.83447265625 = 0.023434972390532494 + 100.0 * 6.218110084533691
Epoch 2070, val loss: 1.211743950843811
Epoch 2080, training loss: 621.985595703125 = 0.023071231320500374 + 100.0 * 6.219625473022461
Epoch 2080, val loss: 1.2147692441940308
Epoch 2090, training loss: 622.1737670898438 = 0.02271466888487339 + 100.0 * 6.221510887145996
Epoch 2090, val loss: 1.217988133430481
Epoch 2100, training loss: 622.1797485351562 = 0.022360149770975113 + 100.0 * 6.221574306488037
Epoch 2100, val loss: 1.2207272052764893
Epoch 2110, training loss: 621.8012084960938 = 0.02201005443930626 + 100.0 * 6.21779203414917
Epoch 2110, val loss: 1.2235063314437866
Epoch 2120, training loss: 621.763427734375 = 0.021677494049072266 + 100.0 * 6.2174177169799805
Epoch 2120, val loss: 1.2263489961624146
Epoch 2130, training loss: 622.06787109375 = 0.021360117942094803 + 100.0 * 6.220465183258057
Epoch 2130, val loss: 1.2291953563690186
Epoch 2140, training loss: 621.8435668945312 = 0.021034035831689835 + 100.0 * 6.218225002288818
Epoch 2140, val loss: 1.2322510480880737
Epoch 2150, training loss: 621.7357177734375 = 0.020710662007331848 + 100.0 * 6.2171502113342285
Epoch 2150, val loss: 1.2348819971084595
Epoch 2160, training loss: 621.646240234375 = 0.02040773443877697 + 100.0 * 6.216258525848389
Epoch 2160, val loss: 1.237718939781189
Epoch 2170, training loss: 621.5817260742188 = 0.020106762647628784 + 100.0 * 6.215616703033447
Epoch 2170, val loss: 1.240525722503662
Epoch 2180, training loss: 622.5551147460938 = 0.019821535795927048 + 100.0 * 6.225352764129639
Epoch 2180, val loss: 1.2429685592651367
Epoch 2190, training loss: 622.1179809570312 = 0.019527822732925415 + 100.0 * 6.22098445892334
Epoch 2190, val loss: 1.2462289333343506
Epoch 2200, training loss: 621.7864990234375 = 0.019234899431467056 + 100.0 * 6.217672824859619
Epoch 2200, val loss: 1.2488542795181274
Epoch 2210, training loss: 621.5764770507812 = 0.018959151580929756 + 100.0 * 6.215574741363525
Epoch 2210, val loss: 1.251338005065918
Epoch 2220, training loss: 621.4900512695312 = 0.018687518313527107 + 100.0 * 6.2147135734558105
Epoch 2220, val loss: 1.2542991638183594
Epoch 2230, training loss: 621.6983032226562 = 0.018427589908242226 + 100.0 * 6.216798782348633
Epoch 2230, val loss: 1.2570170164108276
Epoch 2240, training loss: 621.7081298828125 = 0.018170278519392014 + 100.0 * 6.216899394989014
Epoch 2240, val loss: 1.2596855163574219
Epoch 2250, training loss: 621.68115234375 = 0.01791875809431076 + 100.0 * 6.21663236618042
Epoch 2250, val loss: 1.262058138847351
Epoch 2260, training loss: 621.6491088867188 = 0.017665276303887367 + 100.0 * 6.216314792633057
Epoch 2260, val loss: 1.264784574508667
Epoch 2270, training loss: 621.5109252929688 = 0.017424248158931732 + 100.0 * 6.214935302734375
Epoch 2270, val loss: 1.2675050497055054
Epoch 2280, training loss: 621.5018920898438 = 0.017185933887958527 + 100.0 * 6.214846611022949
Epoch 2280, val loss: 1.2701694965362549
Epoch 2290, training loss: 621.768310546875 = 0.01695553958415985 + 100.0 * 6.217513561248779
Epoch 2290, val loss: 1.272616982460022
Epoch 2300, training loss: 621.585693359375 = 0.016722312197089195 + 100.0 * 6.215689659118652
Epoch 2300, val loss: 1.275080919265747
Epoch 2310, training loss: 621.5426635742188 = 0.01650182344019413 + 100.0 * 6.215261936187744
Epoch 2310, val loss: 1.277537226676941
Epoch 2320, training loss: 621.851318359375 = 0.01628655381500721 + 100.0 * 6.218350410461426
Epoch 2320, val loss: 1.2800301313400269
Epoch 2330, training loss: 621.596435546875 = 0.016064675524830818 + 100.0 * 6.215804100036621
Epoch 2330, val loss: 1.2830778360366821
Epoch 2340, training loss: 621.7979736328125 = 0.015859248116612434 + 100.0 * 6.21782112121582
Epoch 2340, val loss: 1.285178542137146
Epoch 2350, training loss: 621.3049926757812 = 0.01563941314816475 + 100.0 * 6.212893486022949
Epoch 2350, val loss: 1.2876920700073242
Epoch 2360, training loss: 621.2785034179688 = 0.015438326634466648 + 100.0 * 6.212630748748779
Epoch 2360, val loss: 1.2900347709655762
Epoch 2370, training loss: 621.4083251953125 = 0.015244519338011742 + 100.0 * 6.213930606842041
Epoch 2370, val loss: 1.2926117181777954
Epoch 2380, training loss: 621.6779174804688 = 0.015051638707518578 + 100.0 * 6.216628551483154
Epoch 2380, val loss: 1.295114517211914
Epoch 2390, training loss: 621.39453125 = 0.0148551594465971 + 100.0 * 6.213797092437744
Epoch 2390, val loss: 1.297216534614563
Epoch 2400, training loss: 621.3423461914062 = 0.014665746130049229 + 100.0 * 6.2132768630981445
Epoch 2400, val loss: 1.299714207649231
Epoch 2410, training loss: 621.4307250976562 = 0.014483101665973663 + 100.0 * 6.214162826538086
Epoch 2410, val loss: 1.3019425868988037
Epoch 2420, training loss: 621.2326049804688 = 0.014303571544587612 + 100.0 * 6.212183475494385
Epoch 2420, val loss: 1.3044031858444214
Epoch 2430, training loss: 621.5230712890625 = 0.014135142788290977 + 100.0 * 6.215088844299316
Epoch 2430, val loss: 1.3068569898605347
Epoch 2440, training loss: 621.279541015625 = 0.0139540433883667 + 100.0 * 6.212655544281006
Epoch 2440, val loss: 1.3090569972991943
Epoch 2450, training loss: 621.179931640625 = 0.013780811801552773 + 100.0 * 6.211661338806152
Epoch 2450, val loss: 1.3113701343536377
Epoch 2460, training loss: 621.1458129882812 = 0.013616838492453098 + 100.0 * 6.21132230758667
Epoch 2460, val loss: 1.3136056661605835
Epoch 2470, training loss: 621.3585205078125 = 0.013455440290272236 + 100.0 * 6.213450908660889
Epoch 2470, val loss: 1.315719485282898
Epoch 2480, training loss: 621.3148803710938 = 0.013291813433170319 + 100.0 * 6.213016033172607
Epoch 2480, val loss: 1.3179184198379517
Epoch 2490, training loss: 621.2813720703125 = 0.01313065830618143 + 100.0 * 6.212682247161865
Epoch 2490, val loss: 1.320464015007019
Epoch 2500, training loss: 621.6653442382812 = 0.012980179861187935 + 100.0 * 6.216523170471191
Epoch 2500, val loss: 1.3226017951965332
Epoch 2510, training loss: 621.1513061523438 = 0.01281775813549757 + 100.0 * 6.2113847732543945
Epoch 2510, val loss: 1.324854850769043
Epoch 2520, training loss: 621.0071411132812 = 0.012669868767261505 + 100.0 * 6.209944725036621
Epoch 2520, val loss: 1.3268471956253052
Epoch 2530, training loss: 620.9537963867188 = 0.012523485347628593 + 100.0 * 6.209412574768066
Epoch 2530, val loss: 1.3291676044464111
Epoch 2540, training loss: 621.1443481445312 = 0.012385730631649494 + 100.0 * 6.211319446563721
Epoch 2540, val loss: 1.3312519788742065
Epoch 2550, training loss: 621.3914794921875 = 0.012246431782841682 + 100.0 * 6.213791847229004
Epoch 2550, val loss: 1.3335851430892944
Epoch 2560, training loss: 621.1078491210938 = 0.012100030668079853 + 100.0 * 6.2109575271606445
Epoch 2560, val loss: 1.3355684280395508
Epoch 2570, training loss: 620.9716796875 = 0.011962372809648514 + 100.0 * 6.209597110748291
Epoch 2570, val loss: 1.3376189470291138
Epoch 2580, training loss: 621.0095825195312 = 0.01183121558278799 + 100.0 * 6.209977626800537
Epoch 2580, val loss: 1.3399327993392944
Epoch 2590, training loss: 621.4332275390625 = 0.01170763187110424 + 100.0 * 6.21421480178833
Epoch 2590, val loss: 1.3420690298080444
Epoch 2600, training loss: 621.0076904296875 = 0.01157248392701149 + 100.0 * 6.2099609375
Epoch 2600, val loss: 1.3437315225601196
Epoch 2610, training loss: 620.90966796875 = 0.011446044780313969 + 100.0 * 6.208981990814209
Epoch 2610, val loss: 1.3458850383758545
Epoch 2620, training loss: 620.9912719726562 = 0.011323225684463978 + 100.0 * 6.209799289703369
Epoch 2620, val loss: 1.3479164838790894
Epoch 2630, training loss: 621.4434204101562 = 0.01120314933359623 + 100.0 * 6.214321613311768
Epoch 2630, val loss: 1.3495042324066162
Epoch 2640, training loss: 621.050537109375 = 0.011076672002673149 + 100.0 * 6.210394859313965
Epoch 2640, val loss: 1.3519892692565918
Epoch 2650, training loss: 620.8956909179688 = 0.010958662256598473 + 100.0 * 6.2088470458984375
Epoch 2650, val loss: 1.3538780212402344
Epoch 2660, training loss: 621.0130004882812 = 0.010843335650861263 + 100.0 * 6.210021495819092
Epoch 2660, val loss: 1.356134057044983
Epoch 2670, training loss: 621.0526123046875 = 0.01073040533810854 + 100.0 * 6.210418701171875
Epoch 2670, val loss: 1.3577769994735718
Epoch 2680, training loss: 620.9000854492188 = 0.01061968132853508 + 100.0 * 6.208894729614258
Epoch 2680, val loss: 1.3596572875976562
Epoch 2690, training loss: 621.208984375 = 0.01051041018217802 + 100.0 * 6.211985111236572
Epoch 2690, val loss: 1.3614100217819214
Epoch 2700, training loss: 620.7720336914062 = 0.01039872132241726 + 100.0 * 6.207616806030273
Epoch 2700, val loss: 1.3637839555740356
Epoch 2710, training loss: 620.7669677734375 = 0.010294944047927856 + 100.0 * 6.207566261291504
Epoch 2710, val loss: 1.3655544519424438
Epoch 2720, training loss: 620.8535766601562 = 0.010191035456955433 + 100.0 * 6.208434104919434
Epoch 2720, val loss: 1.367417812347412
Epoch 2730, training loss: 621.4357299804688 = 0.010090438649058342 + 100.0 * 6.214256763458252
Epoch 2730, val loss: 1.3694783449172974
Epoch 2740, training loss: 620.910888671875 = 0.009984982199966908 + 100.0 * 6.209008693695068
Epoch 2740, val loss: 1.370693325996399
Epoch 2750, training loss: 620.72021484375 = 0.009884028695523739 + 100.0 * 6.207103252410889
Epoch 2750, val loss: 1.3729521036148071
Epoch 2760, training loss: 620.7164306640625 = 0.009789560921490192 + 100.0 * 6.207066535949707
Epoch 2760, val loss: 1.3746519088745117
Epoch 2770, training loss: 621.3480834960938 = 0.009700914844870567 + 100.0 * 6.213383674621582
Epoch 2770, val loss: 1.3765019178390503
Epoch 2780, training loss: 620.8103637695312 = 0.009598899632692337 + 100.0 * 6.2080078125
Epoch 2780, val loss: 1.378448724746704
Epoch 2790, training loss: 620.7434692382812 = 0.009506156668066978 + 100.0 * 6.207339286804199
Epoch 2790, val loss: 1.3800686597824097
Epoch 2800, training loss: 621.0670776367188 = 0.009415882639586926 + 100.0 * 6.21057653427124
Epoch 2800, val loss: 1.3817896842956543
Epoch 2810, training loss: 620.9048461914062 = 0.009322503581643105 + 100.0 * 6.20895528793335
Epoch 2810, val loss: 1.383460283279419
Epoch 2820, training loss: 620.5701293945312 = 0.009234131313860416 + 100.0 * 6.20560884475708
Epoch 2820, val loss: 1.385582685470581
Epoch 2830, training loss: 620.6038208007812 = 0.00914880819618702 + 100.0 * 6.205946445465088
Epoch 2830, val loss: 1.3874632120132446
Epoch 2840, training loss: 620.8025512695312 = 0.009065735153853893 + 100.0 * 6.207934856414795
Epoch 2840, val loss: 1.3890469074249268
Epoch 2850, training loss: 620.7054443359375 = 0.008980417624115944 + 100.0 * 6.20696496963501
Epoch 2850, val loss: 1.390708088874817
Epoch 2860, training loss: 620.764404296875 = 0.008896857500076294 + 100.0 * 6.207554817199707
Epoch 2860, val loss: 1.3923110961914062
Epoch 2870, training loss: 620.7811279296875 = 0.00881421659141779 + 100.0 * 6.207723617553711
Epoch 2870, val loss: 1.3941458463668823
Epoch 2880, training loss: 620.9921875 = 0.008734282106161118 + 100.0 * 6.209834575653076
Epoch 2880, val loss: 1.3959407806396484
Epoch 2890, training loss: 620.6934814453125 = 0.008657035417854786 + 100.0 * 6.20684814453125
Epoch 2890, val loss: 1.3973865509033203
Epoch 2900, training loss: 620.5445556640625 = 0.00857519917190075 + 100.0 * 6.205359935760498
Epoch 2900, val loss: 1.3992774486541748
Epoch 2910, training loss: 620.6338500976562 = 0.008502828888595104 + 100.0 * 6.206253528594971
Epoch 2910, val loss: 1.401071548461914
Epoch 2920, training loss: 620.7597045898438 = 0.00842759758234024 + 100.0 * 6.207512855529785
Epoch 2920, val loss: 1.402614951133728
Epoch 2930, training loss: 620.5511474609375 = 0.008350500836968422 + 100.0 * 6.205427646636963
Epoch 2930, val loss: 1.404067873954773
Epoch 2940, training loss: 620.653564453125 = 0.00827957782894373 + 100.0 * 6.2064528465271
Epoch 2940, val loss: 1.405979871749878
Epoch 2950, training loss: 620.5748291015625 = 0.008208812214434147 + 100.0 * 6.205666542053223
Epoch 2950, val loss: 1.407497763633728
Epoch 2960, training loss: 620.5867309570312 = 0.008136005140841007 + 100.0 * 6.205786228179932
Epoch 2960, val loss: 1.4088386297225952
Epoch 2970, training loss: 620.8702392578125 = 0.008066514506936073 + 100.0 * 6.208621978759766
Epoch 2970, val loss: 1.4104605913162231
Epoch 2980, training loss: 620.5350952148438 = 0.007995338179171085 + 100.0 * 6.205271244049072
Epoch 2980, val loss: 1.4117918014526367
Epoch 2990, training loss: 620.3981323242188 = 0.007925975136458874 + 100.0 * 6.203902244567871
Epoch 2990, val loss: 1.4134639501571655
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 861.6160278320312 = 1.9288891553878784 + 100.0 * 8.596871376037598
Epoch 0, val loss: 1.9227609634399414
Epoch 10, training loss: 861.5494384765625 = 1.9208321571350098 + 100.0 * 8.596285820007324
Epoch 10, val loss: 1.9150203466415405
Epoch 20, training loss: 861.131103515625 = 1.9110726118087769 + 100.0 * 8.59220027923584
Epoch 20, val loss: 1.9052013158798218
Epoch 30, training loss: 858.300537109375 = 1.8985077142715454 + 100.0 * 8.564020156860352
Epoch 30, val loss: 1.8921502828598022
Epoch 40, training loss: 841.1746826171875 = 1.8821465969085693 + 100.0 * 8.392925262451172
Epoch 40, val loss: 1.8754149675369263
Epoch 50, training loss: 768.8984375 = 1.862950325012207 + 100.0 * 7.67035436630249
Epoch 50, val loss: 1.8554421663284302
Epoch 60, training loss: 748.146484375 = 1.844696044921875 + 100.0 * 7.46301794052124
Epoch 60, val loss: 1.8377126455307007
Epoch 70, training loss: 725.745361328125 = 1.8316625356674194 + 100.0 * 7.239137172698975
Epoch 70, val loss: 1.8249809741973877
Epoch 80, training loss: 711.496337890625 = 1.8195581436157227 + 100.0 * 7.096767425537109
Epoch 80, val loss: 1.8129394054412842
Epoch 90, training loss: 702.2610473632812 = 1.8083535432815552 + 100.0 * 7.0045270919799805
Epoch 90, val loss: 1.8020579814910889
Epoch 100, training loss: 691.6348876953125 = 1.7999681234359741 + 100.0 * 6.898349285125732
Epoch 100, val loss: 1.7942482233047485
Epoch 110, training loss: 682.757080078125 = 1.793274998664856 + 100.0 * 6.809638023376465
Epoch 110, val loss: 1.7879393100738525
Epoch 120, training loss: 676.85107421875 = 1.7842074632644653 + 100.0 * 6.750669002532959
Epoch 120, val loss: 1.7797267436981201
Epoch 130, training loss: 671.7567749023438 = 1.7734007835388184 + 100.0 * 6.699833869934082
Epoch 130, val loss: 1.769959568977356
Epoch 140, training loss: 667.6854248046875 = 1.7629114389419556 + 100.0 * 6.659224987030029
Epoch 140, val loss: 1.7604050636291504
Epoch 150, training loss: 664.255859375 = 1.7522661685943604 + 100.0 * 6.625035762786865
Epoch 150, val loss: 1.750619649887085
Epoch 160, training loss: 661.3792724609375 = 1.740270972251892 + 100.0 * 6.5963897705078125
Epoch 160, val loss: 1.7398728132247925
Epoch 170, training loss: 659.3809204101562 = 1.7268283367156982 + 100.0 * 6.576540946960449
Epoch 170, val loss: 1.7280222177505493
Epoch 180, training loss: 657.8242797851562 = 1.7118440866470337 + 100.0 * 6.561124324798584
Epoch 180, val loss: 1.7149749994277954
Epoch 190, training loss: 656.2767333984375 = 1.6954066753387451 + 100.0 * 6.545813083648682
Epoch 190, val loss: 1.7007428407669067
Epoch 200, training loss: 654.6287841796875 = 1.6775133609771729 + 100.0 * 6.529512882232666
Epoch 200, val loss: 1.6856375932693481
Epoch 210, training loss: 653.2393798828125 = 1.6581538915634155 + 100.0 * 6.515812397003174
Epoch 210, val loss: 1.6693196296691895
Epoch 220, training loss: 651.9768676757812 = 1.6371492147445679 + 100.0 * 6.503396987915039
Epoch 220, val loss: 1.6517237424850464
Epoch 230, training loss: 650.6884765625 = 1.6143851280212402 + 100.0 * 6.49074125289917
Epoch 230, val loss: 1.63276207447052
Epoch 240, training loss: 649.6398315429688 = 1.5900074243545532 + 100.0 * 6.480498313903809
Epoch 240, val loss: 1.6124773025512695
Epoch 250, training loss: 648.5809326171875 = 1.5639280080795288 + 100.0 * 6.470170021057129
Epoch 250, val loss: 1.5910264253616333
Epoch 260, training loss: 647.6574096679688 = 1.5365986824035645 + 100.0 * 6.461207866668701
Epoch 260, val loss: 1.5684220790863037
Epoch 270, training loss: 646.5491943359375 = 1.5079998970031738 + 100.0 * 6.450412273406982
Epoch 270, val loss: 1.5449062585830688
Epoch 280, training loss: 646.0911865234375 = 1.4786008596420288 + 100.0 * 6.4461259841918945
Epoch 280, val loss: 1.5208460092544556
Epoch 290, training loss: 644.877197265625 = 1.4481762647628784 + 100.0 * 6.434290409088135
Epoch 290, val loss: 1.496171236038208
Epoch 300, training loss: 644.01708984375 = 1.4174004793167114 + 100.0 * 6.425996780395508
Epoch 300, val loss: 1.4713413715362549
Epoch 310, training loss: 643.2161865234375 = 1.3863133192062378 + 100.0 * 6.418298721313477
Epoch 310, val loss: 1.446422815322876
Epoch 320, training loss: 643.3088989257812 = 1.3550763130187988 + 100.0 * 6.4195380210876465
Epoch 320, val loss: 1.421476125717163
Epoch 330, training loss: 642.0319213867188 = 1.3237063884735107 + 100.0 * 6.4070820808410645
Epoch 330, val loss: 1.3966871500015259
Epoch 340, training loss: 641.6431274414062 = 1.2924962043762207 + 100.0 * 6.403506755828857
Epoch 340, val loss: 1.372278094291687
Epoch 350, training loss: 640.6605224609375 = 1.2616623640060425 + 100.0 * 6.393988609313965
Epoch 350, val loss: 1.3483402729034424
Epoch 360, training loss: 640.1055908203125 = 1.231187343597412 + 100.0 * 6.388743877410889
Epoch 360, val loss: 1.3250453472137451
Epoch 370, training loss: 639.8947143554688 = 1.2011663913726807 + 100.0 * 6.386935710906982
Epoch 370, val loss: 1.3025238513946533
Epoch 380, training loss: 639.2406616210938 = 1.1716701984405518 + 100.0 * 6.38068962097168
Epoch 380, val loss: 1.2806305885314941
Epoch 390, training loss: 638.59814453125 = 1.1427096128463745 + 100.0 * 6.37455415725708
Epoch 390, val loss: 1.2595512866973877
Epoch 400, training loss: 638.0806274414062 = 1.1146454811096191 + 100.0 * 6.369659900665283
Epoch 400, val loss: 1.2397253513336182
Epoch 410, training loss: 637.76416015625 = 1.0873417854309082 + 100.0 * 6.3667683601379395
Epoch 410, val loss: 1.220875859260559
Epoch 420, training loss: 637.4971313476562 = 1.0607835054397583 + 100.0 * 6.364363193511963
Epoch 420, val loss: 1.2028913497924805
Epoch 430, training loss: 636.7536010742188 = 1.0350240468978882 + 100.0 * 6.3571858406066895
Epoch 430, val loss: 1.1860158443450928
Epoch 440, training loss: 636.3406372070312 = 1.0101361274719238 + 100.0 * 6.353304862976074
Epoch 440, val loss: 1.1702690124511719
Epoch 450, training loss: 636.7286987304688 = 0.9861987233161926 + 100.0 * 6.357424736022949
Epoch 450, val loss: 1.1556658744812012
Epoch 460, training loss: 635.637939453125 = 0.9627876281738281 + 100.0 * 6.346751689910889
Epoch 460, val loss: 1.1417075395584106
Epoch 470, training loss: 635.2504272460938 = 0.9403626322746277 + 100.0 * 6.343100547790527
Epoch 470, val loss: 1.128969430923462
Epoch 480, training loss: 634.8826904296875 = 0.9188331365585327 + 100.0 * 6.339638710021973
Epoch 480, val loss: 1.1172717809677124
Epoch 490, training loss: 634.868896484375 = 0.898135244846344 + 100.0 * 6.339707374572754
Epoch 490, val loss: 1.1065229177474976
Epoch 500, training loss: 634.8439331054688 = 0.8777649998664856 + 100.0 * 6.339662075042725
Epoch 500, val loss: 1.0965360403060913
Epoch 510, training loss: 634.1408081054688 = 0.8583555221557617 + 100.0 * 6.33282470703125
Epoch 510, val loss: 1.0874388217926025
Epoch 520, training loss: 633.7317504882812 = 0.8396121263504028 + 100.0 * 6.328921318054199
Epoch 520, val loss: 1.0791804790496826
Epoch 530, training loss: 634.0545654296875 = 0.8214597105979919 + 100.0 * 6.33233118057251
Epoch 530, val loss: 1.0717413425445557
Epoch 540, training loss: 633.5307006835938 = 0.803750216960907 + 100.0 * 6.327269554138184
Epoch 540, val loss: 1.064703345298767
Epoch 550, training loss: 633.0679321289062 = 0.7865070700645447 + 100.0 * 6.322814464569092
Epoch 550, val loss: 1.058388113975525
Epoch 560, training loss: 632.7352905273438 = 0.7698468565940857 + 100.0 * 6.31965446472168
Epoch 560, val loss: 1.052749514579773
Epoch 570, training loss: 632.60400390625 = 0.7536394000053406 + 100.0 * 6.318503379821777
Epoch 570, val loss: 1.0476285219192505
Epoch 580, training loss: 632.2544555664062 = 0.7377126812934875 + 100.0 * 6.31516695022583
Epoch 580, val loss: 1.0429068803787231
Epoch 590, training loss: 632.0308227539062 = 0.7221249341964722 + 100.0 * 6.313086986541748
Epoch 590, val loss: 1.0386465787887573
Epoch 600, training loss: 632.004638671875 = 0.7069067358970642 + 100.0 * 6.312977313995361
Epoch 600, val loss: 1.034835696220398
Epoch 610, training loss: 631.8411254882812 = 0.6918213963508606 + 100.0 * 6.311492919921875
Epoch 610, val loss: 1.0312820672988892
Epoch 620, training loss: 631.6827392578125 = 0.6771031618118286 + 100.0 * 6.310056209564209
Epoch 620, val loss: 1.02813720703125
Epoch 630, training loss: 631.2079467773438 = 0.6624654531478882 + 100.0 * 6.305454730987549
Epoch 630, val loss: 1.0250298976898193
Epoch 640, training loss: 631.109375 = 0.6481655240058899 + 100.0 * 6.304611682891846
Epoch 640, val loss: 1.0225014686584473
Epoch 650, training loss: 631.1066284179688 = 0.6340492963790894 + 100.0 * 6.3047261238098145
Epoch 650, val loss: 1.0200707912445068
Epoch 660, training loss: 630.6727294921875 = 0.6200321316719055 + 100.0 * 6.3005266189575195
Epoch 660, val loss: 1.0179415941238403
Epoch 670, training loss: 631.1101684570312 = 0.6061822175979614 + 100.0 * 6.305039882659912
Epoch 670, val loss: 1.015912413597107
Epoch 680, training loss: 630.2508544921875 = 0.5924961566925049 + 100.0 * 6.296583652496338
Epoch 680, val loss: 1.014169692993164
Epoch 690, training loss: 630.1558227539062 = 0.5789843201637268 + 100.0 * 6.2957682609558105
Epoch 690, val loss: 1.0127393007278442
Epoch 700, training loss: 629.938232421875 = 0.5656377673149109 + 100.0 * 6.293725967407227
Epoch 700, val loss: 1.011443018913269
Epoch 710, training loss: 630.645751953125 = 0.5523868799209595 + 100.0 * 6.300933837890625
Epoch 710, val loss: 1.0103180408477783
Epoch 720, training loss: 629.7152099609375 = 0.5392060875892639 + 100.0 * 6.291759967803955
Epoch 720, val loss: 1.0091952085494995
Epoch 730, training loss: 629.4749755859375 = 0.5262057781219482 + 100.0 * 6.289487838745117
Epoch 730, val loss: 1.0085467100143433
Epoch 740, training loss: 629.345458984375 = 0.513438880443573 + 100.0 * 6.288320064544678
Epoch 740, val loss: 1.0080775022506714
Epoch 750, training loss: 629.7457275390625 = 0.5008071660995483 + 100.0 * 6.292449474334717
Epoch 750, val loss: 1.0077563524246216
Epoch 760, training loss: 629.4385986328125 = 0.48831063508987427 + 100.0 * 6.28950309753418
Epoch 760, val loss: 1.007453203201294
Epoch 770, training loss: 628.9901123046875 = 0.47595828771591187 + 100.0 * 6.285141468048096
Epoch 770, val loss: 1.0074001550674438
Epoch 780, training loss: 628.7807006835938 = 0.4638829231262207 + 100.0 * 6.283168315887451
Epoch 780, val loss: 1.0076510906219482
Epoch 790, training loss: 629.5054931640625 = 0.45204317569732666 + 100.0 * 6.290534496307373
Epoch 790, val loss: 1.008020043373108
Epoch 800, training loss: 628.712890625 = 0.4402073621749878 + 100.0 * 6.282726764678955
Epoch 800, val loss: 1.0084806680679321
Epoch 810, training loss: 628.6050415039062 = 0.42869213223457336 + 100.0 * 6.281763553619385
Epoch 810, val loss: 1.0090733766555786
Epoch 820, training loss: 628.2763061523438 = 0.4174565374851227 + 100.0 * 6.27858829498291
Epoch 820, val loss: 1.0100805759429932
Epoch 830, training loss: 628.62060546875 = 0.40649884939193726 + 100.0 * 6.282141208648682
Epoch 830, val loss: 1.0112437009811401
Epoch 840, training loss: 628.6251831054688 = 0.3955724239349365 + 100.0 * 6.282296180725098
Epoch 840, val loss: 1.0122627019882202
Epoch 850, training loss: 628.08447265625 = 0.3850135803222656 + 100.0 * 6.276994705200195
Epoch 850, val loss: 1.0135990381240845
Epoch 860, training loss: 627.8372192382812 = 0.3747592568397522 + 100.0 * 6.274624347686768
Epoch 860, val loss: 1.0153050422668457
Epoch 870, training loss: 627.719482421875 = 0.364772230386734 + 100.0 * 6.273547172546387
Epoch 870, val loss: 1.017151951789856
Epoch 880, training loss: 628.0310668945312 = 0.3550647497177124 + 100.0 * 6.276760101318359
Epoch 880, val loss: 1.019049882888794
Epoch 890, training loss: 627.725830078125 = 0.3455154299736023 + 100.0 * 6.273803234100342
Epoch 890, val loss: 1.021302580833435
Epoch 900, training loss: 627.6196899414062 = 0.336256742477417 + 100.0 * 6.272834300994873
Epoch 900, val loss: 1.0234748125076294
Epoch 910, training loss: 627.3886108398438 = 0.3272683918476105 + 100.0 * 6.270613193511963
Epoch 910, val loss: 1.0259147882461548
Epoch 920, training loss: 628.321044921875 = 0.3185681402683258 + 100.0 * 6.280025005340576
Epoch 920, val loss: 1.028646469116211
Epoch 930, training loss: 627.12646484375 = 0.30995994806289673 + 100.0 * 6.268165111541748
Epoch 930, val loss: 1.031273603439331
Epoch 940, training loss: 627.077392578125 = 0.3017216920852661 + 100.0 * 6.267756938934326
Epoch 940, val loss: 1.0342386960983276
Epoch 950, training loss: 626.9254760742188 = 0.29377108812332153 + 100.0 * 6.266317367553711
Epoch 950, val loss: 1.0374189615249634
Epoch 960, training loss: 627.0060424804688 = 0.28604376316070557 + 100.0 * 6.267199993133545
Epoch 960, val loss: 1.0407955646514893
Epoch 970, training loss: 627.0540771484375 = 0.278498113155365 + 100.0 * 6.267755508422852
Epoch 970, val loss: 1.0442097187042236
Epoch 980, training loss: 626.8875732421875 = 0.27111515402793884 + 100.0 * 6.266164302825928
Epoch 980, val loss: 1.0475815534591675
Epoch 990, training loss: 626.5531616210938 = 0.2640285789966583 + 100.0 * 6.2628912925720215
Epoch 990, val loss: 1.0513337850570679
Epoch 1000, training loss: 626.4844970703125 = 0.2571711242198944 + 100.0 * 6.262273788452148
Epoch 1000, val loss: 1.0553172826766968
Epoch 1010, training loss: 626.9293212890625 = 0.250556081533432 + 100.0 * 6.266787528991699
Epoch 1010, val loss: 1.0592522621154785
Epoch 1020, training loss: 626.8633422851562 = 0.24395878612995148 + 100.0 * 6.266193866729736
Epoch 1020, val loss: 1.063292145729065
Epoch 1030, training loss: 626.2318115234375 = 0.23764969408512115 + 100.0 * 6.259941577911377
Epoch 1030, val loss: 1.0674800872802734
Epoch 1040, training loss: 626.1957397460938 = 0.23156246542930603 + 100.0 * 6.259641647338867
Epoch 1040, val loss: 1.0719249248504639
Epoch 1050, training loss: 626.0445556640625 = 0.22567971050739288 + 100.0 * 6.258188724517822
Epoch 1050, val loss: 1.0764895677566528
Epoch 1060, training loss: 626.2073974609375 = 0.21998146176338196 + 100.0 * 6.25987434387207
Epoch 1060, val loss: 1.081055760383606
Epoch 1070, training loss: 626.105224609375 = 0.2143464982509613 + 100.0 * 6.258908748626709
Epoch 1070, val loss: 1.085773229598999
Epoch 1080, training loss: 626.0464477539062 = 0.2089039534330368 + 100.0 * 6.25837516784668
Epoch 1080, val loss: 1.0903133153915405
Epoch 1090, training loss: 625.8084716796875 = 0.20365674793720245 + 100.0 * 6.256048202514648
Epoch 1090, val loss: 1.0951259136199951
Epoch 1100, training loss: 625.7801513671875 = 0.19856590032577515 + 100.0 * 6.2558159828186035
Epoch 1100, val loss: 1.1002708673477173
Epoch 1110, training loss: 625.9827880859375 = 0.19362707436084747 + 100.0 * 6.257891654968262
Epoch 1110, val loss: 1.1052292585372925
Epoch 1120, training loss: 625.9998779296875 = 0.18878187239170074 + 100.0 * 6.258111000061035
Epoch 1120, val loss: 1.1104710102081299
Epoch 1130, training loss: 625.7278442382812 = 0.18406426906585693 + 100.0 * 6.25543737411499
Epoch 1130, val loss: 1.1154334545135498
Epoch 1140, training loss: 625.7289428710938 = 0.17949825525283813 + 100.0 * 6.255494117736816
Epoch 1140, val loss: 1.1208616495132446
Epoch 1150, training loss: 625.4103393554688 = 0.17507395148277283 + 100.0 * 6.252353191375732
Epoch 1150, val loss: 1.126280426979065
Epoch 1160, training loss: 625.3095703125 = 0.17077229917049408 + 100.0 * 6.251388072967529
Epoch 1160, val loss: 1.1317408084869385
Epoch 1170, training loss: 625.4930419921875 = 0.16660721600055695 + 100.0 * 6.253264427185059
Epoch 1170, val loss: 1.1372287273406982
Epoch 1180, training loss: 625.2764282226562 = 0.1624806821346283 + 100.0 * 6.2511396408081055
Epoch 1180, val loss: 1.1428176164627075
Epoch 1190, training loss: 625.2233276367188 = 0.1585005223751068 + 100.0 * 6.250648498535156
Epoch 1190, val loss: 1.1482242345809937
Epoch 1200, training loss: 625.4595947265625 = 0.1546221822500229 + 100.0 * 6.253049850463867
Epoch 1200, val loss: 1.1541023254394531
Epoch 1210, training loss: 625.0918579101562 = 0.15085597336292267 + 100.0 * 6.2494096755981445
Epoch 1210, val loss: 1.1596542596817017
Epoch 1220, training loss: 625.1298217773438 = 0.147178515791893 + 100.0 * 6.249826908111572
Epoch 1220, val loss: 1.1655274629592896
Epoch 1230, training loss: 624.8952026367188 = 0.14361751079559326 + 100.0 * 6.247515678405762
Epoch 1230, val loss: 1.1714204549789429
Epoch 1240, training loss: 625.207763671875 = 0.1401757001876831 + 100.0 * 6.250675678253174
Epoch 1240, val loss: 1.177517056465149
Epoch 1250, training loss: 625.1514892578125 = 0.13676287233829498 + 100.0 * 6.250146865844727
Epoch 1250, val loss: 1.1831755638122559
Epoch 1260, training loss: 624.813232421875 = 0.13346464931964874 + 100.0 * 6.246797561645508
Epoch 1260, val loss: 1.1892825365066528
Epoch 1270, training loss: 624.6032104492188 = 0.13026300072669983 + 100.0 * 6.244729518890381
Epoch 1270, val loss: 1.195250153541565
Epoch 1280, training loss: 624.6227416992188 = 0.12716762721538544 + 100.0 * 6.244955539703369
Epoch 1280, val loss: 1.201406717300415
Epoch 1290, training loss: 624.7958374023438 = 0.12413779646158218 + 100.0 * 6.2467169761657715
Epoch 1290, val loss: 1.207509994506836
Epoch 1300, training loss: 624.9972534179688 = 0.12117596715688705 + 100.0 * 6.24876070022583
Epoch 1300, val loss: 1.2134853601455688
Epoch 1310, training loss: 624.4400024414062 = 0.11825433373451233 + 100.0 * 6.243217468261719
Epoch 1310, val loss: 1.2196348905563354
Epoch 1320, training loss: 624.368408203125 = 0.11545644700527191 + 100.0 * 6.242529392242432
Epoch 1320, val loss: 1.2256596088409424
Epoch 1330, training loss: 624.3795776367188 = 0.11274399608373642 + 100.0 * 6.242668628692627
Epoch 1330, val loss: 1.2318822145462036
Epoch 1340, training loss: 624.521240234375 = 0.11009571701288223 + 100.0 * 6.24411153793335
Epoch 1340, val loss: 1.2380818128585815
Epoch 1350, training loss: 624.226806640625 = 0.1074976995587349 + 100.0 * 6.2411932945251465
Epoch 1350, val loss: 1.244384527206421
Epoch 1360, training loss: 624.1526489257812 = 0.10498540103435516 + 100.0 * 6.240476608276367
Epoch 1360, val loss: 1.2505661249160767
Epoch 1370, training loss: 624.6522827148438 = 0.10256891697645187 + 100.0 * 6.245497703552246
Epoch 1370, val loss: 1.256921410560608
Epoch 1380, training loss: 624.1090698242188 = 0.1001313254237175 + 100.0 * 6.240089416503906
Epoch 1380, val loss: 1.2630350589752197
Epoch 1390, training loss: 624.322509765625 = 0.09783985465765 + 100.0 * 6.242246627807617
Epoch 1390, val loss: 1.269712209701538
Epoch 1400, training loss: 623.990966796875 = 0.09554833173751831 + 100.0 * 6.238954544067383
Epoch 1400, val loss: 1.2754088640213013
Epoch 1410, training loss: 623.8374633789062 = 0.09335383027791977 + 100.0 * 6.237441062927246
Epoch 1410, val loss: 1.28194260597229
Epoch 1420, training loss: 623.8944091796875 = 0.09122610837221146 + 100.0 * 6.238031387329102
Epoch 1420, val loss: 1.2880651950836182
Epoch 1430, training loss: 624.11376953125 = 0.0891510397195816 + 100.0 * 6.240245819091797
Epoch 1430, val loss: 1.2944507598876953
Epoch 1440, training loss: 623.9085693359375 = 0.0871204286813736 + 100.0 * 6.23821496963501
Epoch 1440, val loss: 1.3007971048355103
Epoch 1450, training loss: 623.7424926757812 = 0.08514638245105743 + 100.0 * 6.236573219299316
Epoch 1450, val loss: 1.3070367574691772
Epoch 1460, training loss: 623.8082275390625 = 0.08324260264635086 + 100.0 * 6.237249851226807
Epoch 1460, val loss: 1.3135477304458618
Epoch 1470, training loss: 623.7913208007812 = 0.08137689530849457 + 100.0 * 6.237099647521973
Epoch 1470, val loss: 1.319474220275879
Epoch 1480, training loss: 623.6192016601562 = 0.07956329733133316 + 100.0 * 6.235396385192871
Epoch 1480, val loss: 1.3259104490280151
Epoch 1490, training loss: 623.5057983398438 = 0.07780500501394272 + 100.0 * 6.234279632568359
Epoch 1490, val loss: 1.3319745063781738
Epoch 1500, training loss: 624.5731201171875 = 0.0761212557554245 + 100.0 * 6.244969844818115
Epoch 1500, val loss: 1.3382152318954468
Epoch 1510, training loss: 623.6712646484375 = 0.07437751442193985 + 100.0 * 6.235968589782715
Epoch 1510, val loss: 1.3441559076309204
Epoch 1520, training loss: 623.3290405273438 = 0.07275397330522537 + 100.0 * 6.232563018798828
Epoch 1520, val loss: 1.3505733013153076
Epoch 1530, training loss: 623.254150390625 = 0.07117968052625656 + 100.0 * 6.23183012008667
Epoch 1530, val loss: 1.3566498756408691
Epoch 1540, training loss: 623.231689453125 = 0.06965324282646179 + 100.0 * 6.2316203117370605
Epoch 1540, val loss: 1.3629924058914185
Epoch 1550, training loss: 624.24560546875 = 0.06817412376403809 + 100.0 * 6.241774082183838
Epoch 1550, val loss: 1.369175672531128
Epoch 1560, training loss: 623.9638061523438 = 0.06668221950531006 + 100.0 * 6.23897123336792
Epoch 1560, val loss: 1.3749501705169678
Epoch 1570, training loss: 623.1146850585938 = 0.06523393839597702 + 100.0 * 6.230494499206543
Epoch 1570, val loss: 1.3807199001312256
Epoch 1580, training loss: 623.1264038085938 = 0.06385356932878494 + 100.0 * 6.230625629425049
Epoch 1580, val loss: 1.3870441913604736
Epoch 1590, training loss: 623.2685546875 = 0.06251167505979538 + 100.0 * 6.232060432434082
Epoch 1590, val loss: 1.3929967880249023
Epoch 1600, training loss: 623.1273193359375 = 0.06118619814515114 + 100.0 * 6.230661869049072
Epoch 1600, val loss: 1.3988381624221802
Epoch 1610, training loss: 622.95068359375 = 0.05989617109298706 + 100.0 * 6.228908061981201
Epoch 1610, val loss: 1.4047499895095825
Epoch 1620, training loss: 622.9092407226562 = 0.05865449458360672 + 100.0 * 6.228505611419678
Epoch 1620, val loss: 1.410614252090454
Epoch 1630, training loss: 622.9108276367188 = 0.057450488209724426 + 100.0 * 6.228533744812012
Epoch 1630, val loss: 1.416660189628601
Epoch 1640, training loss: 623.5303955078125 = 0.056278541684150696 + 100.0 * 6.2347412109375
Epoch 1640, val loss: 1.4225095510482788
Epoch 1650, training loss: 622.9822998046875 = 0.05511295422911644 + 100.0 * 6.22927188873291
Epoch 1650, val loss: 1.4280418157577515
Epoch 1660, training loss: 623.1430053710938 = 0.05399652197957039 + 100.0 * 6.230889797210693
Epoch 1660, val loss: 1.4340126514434814
Epoch 1670, training loss: 623.436279296875 = 0.05289434641599655 + 100.0 * 6.2338337898254395
Epoch 1670, val loss: 1.4389811754226685
Epoch 1680, training loss: 622.8646850585938 = 0.05180499330163002 + 100.0 * 6.228128910064697
Epoch 1680, val loss: 1.4453486204147339
Epoch 1690, training loss: 622.688232421875 = 0.05076718330383301 + 100.0 * 6.226374626159668
Epoch 1690, val loss: 1.450738787651062
Epoch 1700, training loss: 622.6065063476562 = 0.04976440221071243 + 100.0 * 6.225567817687988
Epoch 1700, val loss: 1.4564735889434814
Epoch 1710, training loss: 622.580322265625 = 0.04879176989197731 + 100.0 * 6.225315570831299
Epoch 1710, val loss: 1.4620660543441772
Epoch 1720, training loss: 623.0264892578125 = 0.04785160347819328 + 100.0 * 6.229786396026611
Epoch 1720, val loss: 1.4673027992248535
Epoch 1730, training loss: 623.0390625 = 0.04689617455005646 + 100.0 * 6.229921817779541
Epoch 1730, val loss: 1.4732224941253662
Epoch 1740, training loss: 622.8500366210938 = 0.045964326709508896 + 100.0 * 6.22804069519043
Epoch 1740, val loss: 1.4780486822128296
Epoch 1750, training loss: 622.4696655273438 = 0.045076556503772736 + 100.0 * 6.224245548248291
Epoch 1750, val loss: 1.4839178323745728
Epoch 1760, training loss: 622.4830322265625 = 0.04422076791524887 + 100.0 * 6.2243876457214355
Epoch 1760, val loss: 1.4894407987594604
Epoch 1770, training loss: 622.875732421875 = 0.043394606560468674 + 100.0 * 6.228323459625244
Epoch 1770, val loss: 1.494568943977356
Epoch 1780, training loss: 622.4058837890625 = 0.04254917800426483 + 100.0 * 6.223633289337158
Epoch 1780, val loss: 1.5001987218856812
Epoch 1790, training loss: 622.8428344726562 = 0.04174929857254028 + 100.0 * 6.228010654449463
Epoch 1790, val loss: 1.5053621530532837
Epoch 1800, training loss: 622.3939819335938 = 0.04095437377691269 + 100.0 * 6.223530292510986
Epoch 1800, val loss: 1.5105767250061035
Epoch 1810, training loss: 622.3289184570312 = 0.0401826836168766 + 100.0 * 6.2228875160217285
Epoch 1810, val loss: 1.5159204006195068
Epoch 1820, training loss: 622.306884765625 = 0.03944585844874382 + 100.0 * 6.22267484664917
Epoch 1820, val loss: 1.5213158130645752
Epoch 1830, training loss: 622.2384033203125 = 0.03872380033135414 + 100.0 * 6.221996784210205
Epoch 1830, val loss: 1.5266196727752686
Epoch 1840, training loss: 623.3984985351562 = 0.03802955150604248 + 100.0 * 6.233604431152344
Epoch 1840, val loss: 1.5316121578216553
Epoch 1850, training loss: 623.175048828125 = 0.03731834143400192 + 100.0 * 6.231377601623535
Epoch 1850, val loss: 1.5369510650634766
Epoch 1860, training loss: 622.20361328125 = 0.036610763520002365 + 100.0 * 6.221669673919678
Epoch 1860, val loss: 1.5414878129959106
Epoch 1870, training loss: 622.22900390625 = 0.035951510071754456 + 100.0 * 6.221930503845215
Epoch 1870, val loss: 1.5470143556594849
Epoch 1880, training loss: 622.0826416015625 = 0.035316143184900284 + 100.0 * 6.220472812652588
Epoch 1880, val loss: 1.5520232915878296
Epoch 1890, training loss: 622.11279296875 = 0.03470152989029884 + 100.0 * 6.220780849456787
Epoch 1890, val loss: 1.5571391582489014
Epoch 1900, training loss: 622.8809204101562 = 0.03410042077302933 + 100.0 * 6.22846794128418
Epoch 1900, val loss: 1.562143325805664
Epoch 1910, training loss: 622.3955688476562 = 0.033485546708106995 + 100.0 * 6.223620414733887
Epoch 1910, val loss: 1.5669152736663818
Epoch 1920, training loss: 622.1787719726562 = 0.03289703279733658 + 100.0 * 6.221458911895752
Epoch 1920, val loss: 1.5720829963684082
Epoch 1930, training loss: 622.1398315429688 = 0.03232720494270325 + 100.0 * 6.22107458114624
Epoch 1930, val loss: 1.5768160820007324
Epoch 1940, training loss: 622.4468383789062 = 0.03177454322576523 + 100.0 * 6.224150657653809
Epoch 1940, val loss: 1.581961989402771
Epoch 1950, training loss: 622.2674560546875 = 0.03122752346098423 + 100.0 * 6.222362041473389
Epoch 1950, val loss: 1.5861303806304932
Epoch 1960, training loss: 622.2675170898438 = 0.030690180137753487 + 100.0 * 6.222368240356445
Epoch 1960, val loss: 1.5914859771728516
Epoch 1970, training loss: 621.953857421875 = 0.030162831768393517 + 100.0 * 6.219236850738525
Epoch 1970, val loss: 1.595800518989563
Epoch 1980, training loss: 621.880859375 = 0.02965691313147545 + 100.0 * 6.218512058258057
Epoch 1980, val loss: 1.6008003950119019
Epoch 1990, training loss: 622.1312255859375 = 0.029170336201786995 + 100.0 * 6.221020221710205
Epoch 1990, val loss: 1.6056736707687378
Epoch 2000, training loss: 621.92431640625 = 0.028674272820353508 + 100.0 * 6.218955993652344
Epoch 2000, val loss: 1.6099045276641846
Epoch 2010, training loss: 621.787353515625 = 0.0281955786049366 + 100.0 * 6.217591762542725
Epoch 2010, val loss: 1.614725112915039
Epoch 2020, training loss: 621.7547607421875 = 0.02773338370025158 + 100.0 * 6.217270374298096
Epoch 2020, val loss: 1.6192938089370728
Epoch 2030, training loss: 621.7655029296875 = 0.027288878336548805 + 100.0 * 6.217382431030273
Epoch 2030, val loss: 1.6240136623382568
Epoch 2040, training loss: 622.6822509765625 = 0.026857685297727585 + 100.0 * 6.226553916931152
Epoch 2040, val loss: 1.628754734992981
Epoch 2050, training loss: 622.072509765625 = 0.026409056037664413 + 100.0 * 6.220460891723633
Epoch 2050, val loss: 1.6328967809677124
Epoch 2060, training loss: 621.8599243164062 = 0.02598343975841999 + 100.0 * 6.218338966369629
Epoch 2060, val loss: 1.6374238729476929
Epoch 2070, training loss: 622.1237182617188 = 0.025570238009095192 + 100.0 * 6.220981597900391
Epoch 2070, val loss: 1.6421438455581665
Epoch 2080, training loss: 621.6439208984375 = 0.025160690769553185 + 100.0 * 6.216187953948975
Epoch 2080, val loss: 1.646429181098938
Epoch 2090, training loss: 621.6893310546875 = 0.024767322465777397 + 100.0 * 6.216645240783691
Epoch 2090, val loss: 1.6508100032806396
Epoch 2100, training loss: 621.8661499023438 = 0.024389443919062614 + 100.0 * 6.218417167663574
Epoch 2100, val loss: 1.655256748199463
Epoch 2110, training loss: 621.7869873046875 = 0.02400612086057663 + 100.0 * 6.217629909515381
Epoch 2110, val loss: 1.6595513820648193
Epoch 2120, training loss: 622.0404052734375 = 0.02363728918135166 + 100.0 * 6.220167636871338
Epoch 2120, val loss: 1.6634471416473389
Epoch 2130, training loss: 621.6585083007812 = 0.023268084973096848 + 100.0 * 6.216352462768555
Epoch 2130, val loss: 1.6679219007492065
Epoch 2140, training loss: 621.607666015625 = 0.022914040833711624 + 100.0 * 6.215847492218018
Epoch 2140, val loss: 1.671912670135498
Epoch 2150, training loss: 621.5165405273438 = 0.02257218398153782 + 100.0 * 6.214939594268799
Epoch 2150, val loss: 1.6763967275619507
Epoch 2160, training loss: 621.5444946289062 = 0.022239910438656807 + 100.0 * 6.2152228355407715
Epoch 2160, val loss: 1.680428147315979
Epoch 2170, training loss: 622.3944091796875 = 0.02191145345568657 + 100.0 * 6.223724842071533
Epoch 2170, val loss: 1.684309720993042
Epoch 2180, training loss: 621.8140869140625 = 0.02157628908753395 + 100.0 * 6.21792459487915
Epoch 2180, val loss: 1.6886214017868042
Epoch 2190, training loss: 621.7308959960938 = 0.021255357190966606 + 100.0 * 6.21709680557251
Epoch 2190, val loss: 1.6924433708190918
Epoch 2200, training loss: 621.6820678710938 = 0.020943475887179375 + 100.0 * 6.216611385345459
Epoch 2200, val loss: 1.6964731216430664
Epoch 2210, training loss: 621.6151733398438 = 0.02064106985926628 + 100.0 * 6.215945720672607
Epoch 2210, val loss: 1.7005988359451294
Epoch 2220, training loss: 621.426025390625 = 0.020340410992503166 + 100.0 * 6.214056968688965
Epoch 2220, val loss: 1.7044109106063843
Epoch 2230, training loss: 621.4269409179688 = 0.020054630935192108 + 100.0 * 6.21406888961792
Epoch 2230, val loss: 1.7086234092712402
Epoch 2240, training loss: 622.0711669921875 = 0.01977485604584217 + 100.0 * 6.220513820648193
Epoch 2240, val loss: 1.711918592453003
Epoch 2250, training loss: 621.7402954101562 = 0.01949058100581169 + 100.0 * 6.217208385467529
Epoch 2250, val loss: 1.7159380912780762
Epoch 2260, training loss: 621.3851318359375 = 0.01921435073018074 + 100.0 * 6.213658809661865
Epoch 2260, val loss: 1.7198076248168945
Epoch 2270, training loss: 621.3590698242188 = 0.018946636468172073 + 100.0 * 6.2134013175964355
Epoch 2270, val loss: 1.7242622375488281
Epoch 2280, training loss: 621.742431640625 = 0.018694940954446793 + 100.0 * 6.21723747253418
Epoch 2280, val loss: 1.7277683019638062
Epoch 2290, training loss: 621.3712768554688 = 0.0184282548725605 + 100.0 * 6.213528156280518
Epoch 2290, val loss: 1.7313636541366577
Epoch 2300, training loss: 621.23046875 = 0.018172048032283783 + 100.0 * 6.212122917175293
Epoch 2300, val loss: 1.7350908517837524
Epoch 2310, training loss: 621.2796020507812 = 0.017930323258042336 + 100.0 * 6.212616443634033
Epoch 2310, val loss: 1.7392197847366333
Epoch 2320, training loss: 621.5887451171875 = 0.017695434391498566 + 100.0 * 6.215710639953613
Epoch 2320, val loss: 1.742425799369812
Epoch 2330, training loss: 621.3682250976562 = 0.017455415800213814 + 100.0 * 6.213507652282715
Epoch 2330, val loss: 1.7462302446365356
Epoch 2340, training loss: 621.169677734375 = 0.01721985451877117 + 100.0 * 6.211524486541748
Epoch 2340, val loss: 1.749878168106079
Epoch 2350, training loss: 621.182373046875 = 0.01699598878622055 + 100.0 * 6.211654186248779
Epoch 2350, val loss: 1.753607153892517
Epoch 2360, training loss: 621.3876953125 = 0.016782039776444435 + 100.0 * 6.213708877563477
Epoch 2360, val loss: 1.757584810256958
Epoch 2370, training loss: 621.3961181640625 = 0.016559993848204613 + 100.0 * 6.2137956619262695
Epoch 2370, val loss: 1.7604668140411377
Epoch 2380, training loss: 621.3446655273438 = 0.0163432527333498 + 100.0 * 6.213283061981201
Epoch 2380, val loss: 1.7637078762054443
Epoch 2390, training loss: 621.2976684570312 = 0.016131587326526642 + 100.0 * 6.212815284729004
Epoch 2390, val loss: 1.7671312093734741
Epoch 2400, training loss: 621.5032958984375 = 0.01591949164867401 + 100.0 * 6.214873790740967
Epoch 2400, val loss: 1.7714159488677979
Epoch 2410, training loss: 621.2074584960938 = 0.015719907358288765 + 100.0 * 6.211917400360107
Epoch 2410, val loss: 1.774153470993042
Epoch 2420, training loss: 621.0701293945312 = 0.01552056148648262 + 100.0 * 6.210546493530273
Epoch 2420, val loss: 1.7778416872024536
Epoch 2430, training loss: 621.0971069335938 = 0.015331310220062733 + 100.0 * 6.210817813873291
Epoch 2430, val loss: 1.7813581228256226
Epoch 2440, training loss: 621.7944946289062 = 0.01514405757188797 + 100.0 * 6.2177934646606445
Epoch 2440, val loss: 1.7849667072296143
Epoch 2450, training loss: 621.315185546875 = 0.01495526172220707 + 100.0 * 6.2130022048950195
Epoch 2450, val loss: 1.7876859903335571
Epoch 2460, training loss: 621.1240844726562 = 0.01477058045566082 + 100.0 * 6.211092948913574
Epoch 2460, val loss: 1.7914615869522095
Epoch 2470, training loss: 621.327880859375 = 0.014593195170164108 + 100.0 * 6.213132858276367
Epoch 2470, val loss: 1.7944027185440063
Epoch 2480, training loss: 621.2340698242188 = 0.014412150718271732 + 100.0 * 6.2121968269348145
Epoch 2480, val loss: 1.7975387573242188
Epoch 2490, training loss: 620.9942016601562 = 0.01423588301986456 + 100.0 * 6.209799766540527
Epoch 2490, val loss: 1.8008613586425781
Epoch 2500, training loss: 620.9461059570312 = 0.014067869633436203 + 100.0 * 6.209320545196533
Epoch 2500, val loss: 1.8042514324188232
Epoch 2510, training loss: 620.9111938476562 = 0.013903320766985416 + 100.0 * 6.208972930908203
Epoch 2510, val loss: 1.8074922561645508
Epoch 2520, training loss: 621.5855102539062 = 0.013747235760092735 + 100.0 * 6.215717792510986
Epoch 2520, val loss: 1.8109767436981201
Epoch 2530, training loss: 621.2376708984375 = 0.013578774407505989 + 100.0 * 6.212241172790527
Epoch 2530, val loss: 1.813205361366272
Epoch 2540, training loss: 621.0783081054688 = 0.013416068628430367 + 100.0 * 6.210649013519287
Epoch 2540, val loss: 1.816544532775879
Epoch 2550, training loss: 620.9038696289062 = 0.01325982715934515 + 100.0 * 6.208906173706055
Epoch 2550, val loss: 1.819699764251709
Epoch 2560, training loss: 620.9594116210938 = 0.0131130525842309 + 100.0 * 6.209463119506836
Epoch 2560, val loss: 1.8228570222854614
Epoch 2570, training loss: 621.156982421875 = 0.012966160662472248 + 100.0 * 6.211440563201904
Epoch 2570, val loss: 1.8258979320526123
Epoch 2580, training loss: 621.0789794921875 = 0.01281364168971777 + 100.0 * 6.210661888122559
Epoch 2580, val loss: 1.8284941911697388
Epoch 2590, training loss: 621.0624389648438 = 0.01267064269632101 + 100.0 * 6.2104973793029785
Epoch 2590, val loss: 1.8316922187805176
Epoch 2600, training loss: 620.8017578125 = 0.012528403662145138 + 100.0 * 6.207892417907715
Epoch 2600, val loss: 1.8346667289733887
Epoch 2610, training loss: 620.880859375 = 0.01239041704684496 + 100.0 * 6.208684921264648
Epoch 2610, val loss: 1.8373456001281738
Epoch 2620, training loss: 621.5789794921875 = 0.012256668880581856 + 100.0 * 6.215667247772217
Epoch 2620, val loss: 1.8402265310287476
Epoch 2630, training loss: 621.0509033203125 = 0.01211615651845932 + 100.0 * 6.210387706756592
Epoch 2630, val loss: 1.8439555168151855
Epoch 2640, training loss: 620.7971801757812 = 0.01198146864771843 + 100.0 * 6.207851886749268
Epoch 2640, val loss: 1.846088171005249
Epoch 2650, training loss: 620.7693481445312 = 0.011852052062749863 + 100.0 * 6.207574844360352
Epoch 2650, val loss: 1.8493432998657227
Epoch 2660, training loss: 621.0034790039062 = 0.011729540303349495 + 100.0 * 6.2099175453186035
Epoch 2660, val loss: 1.8521733283996582
Epoch 2670, training loss: 620.7467041015625 = 0.011601691134274006 + 100.0 * 6.207351207733154
Epoch 2670, val loss: 1.855223536491394
Epoch 2680, training loss: 621.0127563476562 = 0.01148304808884859 + 100.0 * 6.210012912750244
Epoch 2680, val loss: 1.8580095767974854
Epoch 2690, training loss: 620.8605346679688 = 0.01136038452386856 + 100.0 * 6.208491802215576
Epoch 2690, val loss: 1.860325813293457
Epoch 2700, training loss: 620.8712768554688 = 0.011240337044000626 + 100.0 * 6.2086005210876465
Epoch 2700, val loss: 1.862939476966858
Epoch 2710, training loss: 620.6995849609375 = 0.011121117509901524 + 100.0 * 6.206884860992432
Epoch 2710, val loss: 1.8658413887023926
Epoch 2720, training loss: 620.6611938476562 = 0.0110066132619977 + 100.0 * 6.2065019607543945
Epoch 2720, val loss: 1.8685153722763062
Epoch 2730, training loss: 620.834228515625 = 0.010898075997829437 + 100.0 * 6.208232879638672
Epoch 2730, val loss: 1.8715815544128418
Epoch 2740, training loss: 620.8790283203125 = 0.01078587956726551 + 100.0 * 6.208682537078857
Epoch 2740, val loss: 1.8740935325622559
Epoch 2750, training loss: 621.0768432617188 = 0.010674665682017803 + 100.0 * 6.210661888122559
Epoch 2750, val loss: 1.8766142129898071
Epoch 2760, training loss: 620.7057495117188 = 0.010563427582383156 + 100.0 * 6.20695161819458
Epoch 2760, val loss: 1.8787891864776611
Epoch 2770, training loss: 620.577880859375 = 0.010458539240062237 + 100.0 * 6.205674648284912
Epoch 2770, val loss: 1.8820557594299316
Epoch 2780, training loss: 620.552978515625 = 0.010357176885008812 + 100.0 * 6.205426216125488
Epoch 2780, val loss: 1.8842737674713135
Epoch 2790, training loss: 620.8009033203125 = 0.010261032730340958 + 100.0 * 6.207906723022461
Epoch 2790, val loss: 1.8868157863616943
Epoch 2800, training loss: 620.7581176757812 = 0.01015364658087492 + 100.0 * 6.207479953765869
Epoch 2800, val loss: 1.8894418478012085
Epoch 2810, training loss: 620.5045776367188 = 0.010052121244370937 + 100.0 * 6.204945087432861
Epoch 2810, val loss: 1.891638159751892
Epoch 2820, training loss: 620.5272216796875 = 0.009955096058547497 + 100.0 * 6.205173015594482
Epoch 2820, val loss: 1.8944226503372192
Epoch 2830, training loss: 620.4774780273438 = 0.009861871600151062 + 100.0 * 6.204675674438477
Epoch 2830, val loss: 1.8969846963882446
Epoch 2840, training loss: 620.604248046875 = 0.009771331213414669 + 100.0 * 6.205945014953613
Epoch 2840, val loss: 1.8994698524475098
Epoch 2850, training loss: 620.880859375 = 0.00967835821211338 + 100.0 * 6.208711624145508
Epoch 2850, val loss: 1.9016892910003662
Epoch 2860, training loss: 620.659912109375 = 0.009584810584783554 + 100.0 * 6.206503391265869
Epoch 2860, val loss: 1.9043757915496826
Epoch 2870, training loss: 620.8541259765625 = 0.009496239945292473 + 100.0 * 6.208446025848389
Epoch 2870, val loss: 1.9066928625106812
Epoch 2880, training loss: 620.6337280273438 = 0.009403456933796406 + 100.0 * 6.206243515014648
Epoch 2880, val loss: 1.9091269969940186
Epoch 2890, training loss: 620.6058349609375 = 0.009317277930676937 + 100.0 * 6.205965042114258
Epoch 2890, val loss: 1.9110503196716309
Epoch 2900, training loss: 620.4266967773438 = 0.009231035597622395 + 100.0 * 6.204174995422363
Epoch 2900, val loss: 1.913766622543335
Epoch 2910, training loss: 620.9852905273438 = 0.00914997048676014 + 100.0 * 6.209761142730713
Epoch 2910, val loss: 1.915421962738037
Epoch 2920, training loss: 620.7703247070312 = 0.009065045975148678 + 100.0 * 6.20761251449585
Epoch 2920, val loss: 1.9184739589691162
Epoch 2930, training loss: 620.5188598632812 = 0.008977904915809631 + 100.0 * 6.205099105834961
Epoch 2930, val loss: 1.9202754497528076
Epoch 2940, training loss: 620.4147338867188 = 0.00889868475496769 + 100.0 * 6.2040581703186035
Epoch 2940, val loss: 1.9229176044464111
Epoch 2950, training loss: 620.3258056640625 = 0.008821327239274979 + 100.0 * 6.203169822692871
Epoch 2950, val loss: 1.9252526760101318
Epoch 2960, training loss: 620.4717407226562 = 0.008748948574066162 + 100.0 * 6.204629898071289
Epoch 2960, val loss: 1.9276418685913086
Epoch 2970, training loss: 620.697509765625 = 0.008672443218529224 + 100.0 * 6.206888198852539
Epoch 2970, val loss: 1.929418921470642
Epoch 2980, training loss: 620.5768432617188 = 0.008596963249146938 + 100.0 * 6.205682277679443
Epoch 2980, val loss: 1.9316602945327759
Epoch 2990, training loss: 620.53857421875 = 0.0085211880505085 + 100.0 * 6.205300331115723
Epoch 2990, val loss: 1.9338161945343018
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8371112282551397
The final CL Acc:0.73333, 0.03408, The final GNN Acc:0.83781, 0.00050
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10548])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.6509399414062 = 1.9665790796279907 + 100.0 * 8.596843719482422
Epoch 0, val loss: 1.968469262123108
Epoch 10, training loss: 861.5703735351562 = 1.9563394784927368 + 100.0 * 8.596139907836914
Epoch 10, val loss: 1.9576138257980347
Epoch 20, training loss: 861.0983276367188 = 1.943725824356079 + 100.0 * 8.591546058654785
Epoch 20, val loss: 1.9440842866897583
Epoch 30, training loss: 857.97412109375 = 1.9276388883590698 + 100.0 * 8.560464859008789
Epoch 30, val loss: 1.92678701877594
Epoch 40, training loss: 835.9317016601562 = 1.907443642616272 + 100.0 * 8.340242385864258
Epoch 40, val loss: 1.9054958820343018
Epoch 50, training loss: 773.3155517578125 = 1.8844401836395264 + 100.0 * 7.714311122894287
Epoch 50, val loss: 1.8820791244506836
Epoch 60, training loss: 753.4664916992188 = 1.869680643081665 + 100.0 * 7.515968322753906
Epoch 60, val loss: 1.8678735494613647
Epoch 70, training loss: 726.8089599609375 = 1.858734130859375 + 100.0 * 7.249502182006836
Epoch 70, val loss: 1.85670804977417
Epoch 80, training loss: 708.3463745117188 = 1.8477178812026978 + 100.0 * 7.064986705780029
Epoch 80, val loss: 1.8457703590393066
Epoch 90, training loss: 697.2935791015625 = 1.8383688926696777 + 100.0 * 6.954552173614502
Epoch 90, val loss: 1.8365788459777832
Epoch 100, training loss: 687.9761352539062 = 1.8299115896224976 + 100.0 * 6.861462116241455
Epoch 100, val loss: 1.8284449577331543
Epoch 110, training loss: 679.6720581054688 = 1.8218187093734741 + 100.0 * 6.778501987457275
Epoch 110, val loss: 1.8205798864364624
Epoch 120, training loss: 674.21875 = 1.8139318227767944 + 100.0 * 6.724048614501953
Epoch 120, val loss: 1.813099980354309
Epoch 130, training loss: 670.0400390625 = 1.8061350584030151 + 100.0 * 6.682338714599609
Epoch 130, val loss: 1.8056384325027466
Epoch 140, training loss: 666.92431640625 = 1.7986481189727783 + 100.0 * 6.651256561279297
Epoch 140, val loss: 1.7983421087265015
Epoch 150, training loss: 664.4595947265625 = 1.7910277843475342 + 100.0 * 6.626686096191406
Epoch 150, val loss: 1.7910045385360718
Epoch 160, training loss: 661.921630859375 = 1.7833715677261353 + 100.0 * 6.601382732391357
Epoch 160, val loss: 1.7835137844085693
Epoch 170, training loss: 659.65576171875 = 1.7755869626998901 + 100.0 * 6.578802108764648
Epoch 170, val loss: 1.7759202718734741
Epoch 180, training loss: 657.4071044921875 = 1.7674763202667236 + 100.0 * 6.556396484375
Epoch 180, val loss: 1.7680383920669556
Epoch 190, training loss: 655.5244140625 = 1.7588757276535034 + 100.0 * 6.537655830383301
Epoch 190, val loss: 1.7596794366836548
Epoch 200, training loss: 653.994873046875 = 1.7495499849319458 + 100.0 * 6.5224528312683105
Epoch 200, val loss: 1.7507259845733643
Epoch 210, training loss: 652.1286010742188 = 1.7394903898239136 + 100.0 * 6.5038909912109375
Epoch 210, val loss: 1.741073489189148
Epoch 220, training loss: 650.7418823242188 = 1.7286391258239746 + 100.0 * 6.4901323318481445
Epoch 220, val loss: 1.7306697368621826
Epoch 230, training loss: 649.481689453125 = 1.7169368267059326 + 100.0 * 6.47764778137207
Epoch 230, val loss: 1.7194081544876099
Epoch 240, training loss: 648.3121948242188 = 1.7043182849884033 + 100.0 * 6.466078281402588
Epoch 240, val loss: 1.7073339223861694
Epoch 250, training loss: 647.3612670898438 = 1.6907438039779663 + 100.0 * 6.456705093383789
Epoch 250, val loss: 1.6943639516830444
Epoch 260, training loss: 646.299560546875 = 1.6761345863342285 + 100.0 * 6.446234226226807
Epoch 260, val loss: 1.6804594993591309
Epoch 270, training loss: 645.3599853515625 = 1.6605806350708008 + 100.0 * 6.4369940757751465
Epoch 270, val loss: 1.665715217590332
Epoch 280, training loss: 644.844970703125 = 1.6440637111663818 + 100.0 * 6.432009220123291
Epoch 280, val loss: 1.650073766708374
Epoch 290, training loss: 644.1276245117188 = 1.626346468925476 + 100.0 * 6.425013065338135
Epoch 290, val loss: 1.6334980726242065
Epoch 300, training loss: 643.0654907226562 = 1.6077427864074707 + 100.0 * 6.414577484130859
Epoch 300, val loss: 1.6161983013153076
Epoch 310, training loss: 642.3713989257812 = 1.5883148908615112 + 100.0 * 6.407830715179443
Epoch 310, val loss: 1.5981886386871338
Epoch 320, training loss: 641.8153076171875 = 1.568122148513794 + 100.0 * 6.402472019195557
Epoch 320, val loss: 1.5796008110046387
Epoch 330, training loss: 641.276611328125 = 1.5471234321594238 + 100.0 * 6.397294998168945
Epoch 330, val loss: 1.5605673789978027
Epoch 340, training loss: 640.529296875 = 1.5254837274551392 + 100.0 * 6.390038013458252
Epoch 340, val loss: 1.5410165786743164
Epoch 350, training loss: 640.033447265625 = 1.5033963918685913 + 100.0 * 6.385300159454346
Epoch 350, val loss: 1.5212236642837524
Epoch 360, training loss: 640.9707641601562 = 1.4808710813522339 + 100.0 * 6.394898891448975
Epoch 360, val loss: 1.5011346340179443
Epoch 370, training loss: 639.43798828125 = 1.457834005355835 + 100.0 * 6.3798017501831055
Epoch 370, val loss: 1.480980634689331
Epoch 380, training loss: 638.5672607421875 = 1.4346758127212524 + 100.0 * 6.371325969696045
Epoch 380, val loss: 1.4609133005142212
Epoch 390, training loss: 638.147216796875 = 1.4114632606506348 + 100.0 * 6.3673577308654785
Epoch 390, val loss: 1.440895915031433
Epoch 400, training loss: 637.7654418945312 = 1.388182520866394 + 100.0 * 6.363772869110107
Epoch 400, val loss: 1.4210323095321655
Epoch 410, training loss: 637.3108520507812 = 1.364743709564209 + 100.0 * 6.359460830688477
Epoch 410, val loss: 1.4011924266815186
Epoch 420, training loss: 637.1607055664062 = 1.3413554430007935 + 100.0 * 6.358193397521973
Epoch 420, val loss: 1.3816901445388794
Epoch 430, training loss: 636.626220703125 = 1.317934513092041 + 100.0 * 6.353082656860352
Epoch 430, val loss: 1.362231969833374
Epoch 440, training loss: 636.22119140625 = 1.294655203819275 + 100.0 * 6.349265098571777
Epoch 440, val loss: 1.343133807182312
Epoch 450, training loss: 636.0555419921875 = 1.2715286016464233 + 100.0 * 6.347839832305908
Epoch 450, val loss: 1.3242971897125244
Epoch 460, training loss: 636.01171875 = 1.248401403427124 + 100.0 * 6.347632884979248
Epoch 460, val loss: 1.3059120178222656
Epoch 470, training loss: 635.4015502929688 = 1.2254334688186646 + 100.0 * 6.341760635375977
Epoch 470, val loss: 1.2876957654953003
Epoch 480, training loss: 635.01904296875 = 1.2027587890625 + 100.0 * 6.338162899017334
Epoch 480, val loss: 1.2698452472686768
Epoch 490, training loss: 634.6613159179688 = 1.1803600788116455 + 100.0 * 6.334809303283691
Epoch 490, val loss: 1.2524585723876953
Epoch 500, training loss: 635.0961303710938 = 1.158097267150879 + 100.0 * 6.339380741119385
Epoch 500, val loss: 1.2352920770645142
Epoch 510, training loss: 634.2893676757812 = 1.1358338594436646 + 100.0 * 6.3315348625183105
Epoch 510, val loss: 1.2185485363006592
Epoch 520, training loss: 633.87255859375 = 1.1140016317367554 + 100.0 * 6.327585220336914
Epoch 520, val loss: 1.202189326286316
Epoch 530, training loss: 633.5907592773438 = 1.092570424079895 + 100.0 * 6.324981689453125
Epoch 530, val loss: 1.186457633972168
Epoch 540, training loss: 633.4895629882812 = 1.0715025663375854 + 100.0 * 6.324181079864502
Epoch 540, val loss: 1.1713522672653198
Epoch 550, training loss: 633.4126586914062 = 1.050633192062378 + 100.0 * 6.323619842529297
Epoch 550, val loss: 1.1562509536743164
Epoch 560, training loss: 633.03173828125 = 1.0300233364105225 + 100.0 * 6.320016860961914
Epoch 560, val loss: 1.1419987678527832
Epoch 570, training loss: 632.75732421875 = 1.0098457336425781 + 100.0 * 6.317474842071533
Epoch 570, val loss: 1.1281416416168213
Epoch 580, training loss: 632.4929809570312 = 0.9901048541069031 + 100.0 * 6.315028667449951
Epoch 580, val loss: 1.11484694480896
Epoch 590, training loss: 632.4736938476562 = 0.9707379937171936 + 100.0 * 6.315029144287109
Epoch 590, val loss: 1.1020426750183105
Epoch 600, training loss: 632.6244506835938 = 0.9516749382019043 + 100.0 * 6.316727638244629
Epoch 600, val loss: 1.0899560451507568
Epoch 610, training loss: 632.00390625 = 0.9329590201377869 + 100.0 * 6.310708999633789
Epoch 610, val loss: 1.0778920650482178
Epoch 620, training loss: 631.6349487304688 = 0.9147607088088989 + 100.0 * 6.307201862335205
Epoch 620, val loss: 1.066780924797058
Epoch 630, training loss: 631.5322265625 = 0.8969979286193848 + 100.0 * 6.306352615356445
Epoch 630, val loss: 1.0561851263046265
Epoch 640, training loss: 631.6328735351562 = 0.8795632123947144 + 100.0 * 6.307533264160156
Epoch 640, val loss: 1.0461503267288208
Epoch 650, training loss: 631.18603515625 = 0.862356960773468 + 100.0 * 6.303236961364746
Epoch 650, val loss: 1.0363061428070068
Epoch 660, training loss: 631.0658569335938 = 0.8456029891967773 + 100.0 * 6.3022027015686035
Epoch 660, val loss: 1.0270030498504639
Epoch 670, training loss: 630.9943237304688 = 0.8292489647865295 + 100.0 * 6.3016510009765625
Epoch 670, val loss: 1.018355131149292
Epoch 680, training loss: 630.7139282226562 = 0.813252866268158 + 100.0 * 6.299006938934326
Epoch 680, val loss: 1.0104097127914429
Epoch 690, training loss: 630.55859375 = 0.7975314855575562 + 100.0 * 6.297610759735107
Epoch 690, val loss: 1.0024235248565674
Epoch 700, training loss: 630.2992553710938 = 0.7821110486984253 + 100.0 * 6.29517126083374
Epoch 700, val loss: 0.9951413869857788
Epoch 710, training loss: 630.1527709960938 = 0.76706463098526 + 100.0 * 6.293857097625732
Epoch 710, val loss: 0.9879968762397766
Epoch 720, training loss: 630.3289794921875 = 0.7523209452629089 + 100.0 * 6.295766353607178
Epoch 720, val loss: 0.9812426567077637
Epoch 730, training loss: 630.4267578125 = 0.7376951575279236 + 100.0 * 6.296890735626221
Epoch 730, val loss: 0.9756801128387451
Epoch 740, training loss: 629.7946166992188 = 0.7232808470726013 + 100.0 * 6.290713310241699
Epoch 740, val loss: 0.9691970348358154
Epoch 750, training loss: 629.57421875 = 0.7092325687408447 + 100.0 * 6.288650035858154
Epoch 750, val loss: 0.9638765454292297
Epoch 760, training loss: 629.439208984375 = 0.6954407095909119 + 100.0 * 6.287437438964844
Epoch 760, val loss: 0.9585704803466797
Epoch 770, training loss: 630.0137329101562 = 0.6818581223487854 + 100.0 * 6.293318271636963
Epoch 770, val loss: 0.953700065612793
Epoch 780, training loss: 629.80712890625 = 0.6683791875839233 + 100.0 * 6.291387557983398
Epoch 780, val loss: 0.9490747451782227
Epoch 790, training loss: 629.3139038085938 = 0.6549994945526123 + 100.0 * 6.286588668823242
Epoch 790, val loss: 0.9443782567977905
Epoch 800, training loss: 629.0159912109375 = 0.6419875621795654 + 100.0 * 6.283740520477295
Epoch 800, val loss: 0.9403970837593079
Epoch 810, training loss: 628.8363647460938 = 0.6291908025741577 + 100.0 * 6.282071590423584
Epoch 810, val loss: 0.9365392327308655
Epoch 820, training loss: 629.0850219726562 = 0.6166008114814758 + 100.0 * 6.284684658050537
Epoch 820, val loss: 0.9331808090209961
Epoch 830, training loss: 629.2638549804688 = 0.6041362881660461 + 100.0 * 6.28659725189209
Epoch 830, val loss: 0.9300631880760193
Epoch 840, training loss: 628.9432373046875 = 0.5916926860809326 + 100.0 * 6.283515930175781
Epoch 840, val loss: 0.9264180660247803
Epoch 850, training loss: 628.5110473632812 = 0.5795436501502991 + 100.0 * 6.27931547164917
Epoch 850, val loss: 0.9237028360366821
Epoch 860, training loss: 628.236572265625 = 0.5677127838134766 + 100.0 * 6.276688575744629
Epoch 860, val loss: 0.9212658405303955
Epoch 870, training loss: 628.1470947265625 = 0.5560966730117798 + 100.0 * 6.275909900665283
Epoch 870, val loss: 0.9193878173828125
Epoch 880, training loss: 628.8829956054688 = 0.5447206497192383 + 100.0 * 6.283382415771484
Epoch 880, val loss: 0.9176569581031799
Epoch 890, training loss: 628.5458984375 = 0.5332768559455872 + 100.0 * 6.280126094818115
Epoch 890, val loss: 0.9153209328651428
Epoch 900, training loss: 627.8338012695312 = 0.5221573114395142 + 100.0 * 6.273116588592529
Epoch 900, val loss: 0.9139302968978882
Epoch 910, training loss: 627.8129272460938 = 0.5113709568977356 + 100.0 * 6.273015975952148
Epoch 910, val loss: 0.9130887985229492
Epoch 920, training loss: 627.961181640625 = 0.5008200407028198 + 100.0 * 6.274603843688965
Epoch 920, val loss: 0.9125201106071472
Epoch 930, training loss: 627.8314819335938 = 0.49036309123039246 + 100.0 * 6.273411273956299
Epoch 930, val loss: 0.9110375046730042
Epoch 940, training loss: 627.7124633789062 = 0.4801211655139923 + 100.0 * 6.2723236083984375
Epoch 940, val loss: 0.9113526344299316
Epoch 950, training loss: 627.4406127929688 = 0.4701240658760071 + 100.0 * 6.269704341888428
Epoch 950, val loss: 0.9112776517868042
Epoch 960, training loss: 627.3837280273438 = 0.4603905975818634 + 100.0 * 6.269233226776123
Epoch 960, val loss: 0.9114097952842712
Epoch 970, training loss: 627.6378784179688 = 0.4508490562438965 + 100.0 * 6.271870136260986
Epoch 970, val loss: 0.911799430847168
Epoch 980, training loss: 627.29150390625 = 0.44140273332595825 + 100.0 * 6.268501281738281
Epoch 980, val loss: 0.9125916957855225
Epoch 990, training loss: 627.3409423828125 = 0.4322362244129181 + 100.0 * 6.269086837768555
Epoch 990, val loss: 0.9132175445556641
Epoch 1000, training loss: 627.3671875 = 0.42315325140953064 + 100.0 * 6.269440174102783
Epoch 1000, val loss: 0.9144670963287354
Epoch 1010, training loss: 626.9815063476562 = 0.41425368189811707 + 100.0 * 6.26567268371582
Epoch 1010, val loss: 0.91546231508255
Epoch 1020, training loss: 626.8964233398438 = 0.40562787652015686 + 100.0 * 6.2649078369140625
Epoch 1020, val loss: 0.9169992208480835
Epoch 1030, training loss: 626.8170776367188 = 0.3972085118293762 + 100.0 * 6.2641987800598145
Epoch 1030, val loss: 0.918785035610199
Epoch 1040, training loss: 627.0864868164062 = 0.38895919919013977 + 100.0 * 6.266974925994873
Epoch 1040, val loss: 0.9208575487136841
Epoch 1050, training loss: 626.6235961914062 = 0.380779504776001 + 100.0 * 6.262428283691406
Epoch 1050, val loss: 0.9225044250488281
Epoch 1060, training loss: 626.8300170898438 = 0.3728540241718292 + 100.0 * 6.264571666717529
Epoch 1060, val loss: 0.9246482849121094
Epoch 1070, training loss: 626.7230834960938 = 0.3649914860725403 + 100.0 * 6.263580799102783
Epoch 1070, val loss: 0.9268661141395569
Epoch 1080, training loss: 626.472412109375 = 0.35726577043533325 + 100.0 * 6.2611517906188965
Epoch 1080, val loss: 0.9291184544563293
Epoch 1090, training loss: 626.3207397460938 = 0.3497788906097412 + 100.0 * 6.259709358215332
Epoch 1090, val loss: 0.9316926002502441
Epoch 1100, training loss: 626.564697265625 = 0.342520534992218 + 100.0 * 6.262221813201904
Epoch 1100, val loss: 0.9348982572555542
Epoch 1110, training loss: 626.3206787109375 = 0.33522850275039673 + 100.0 * 6.259854793548584
Epoch 1110, val loss: 0.936588704586029
Epoch 1120, training loss: 626.2847290039062 = 0.32814764976501465 + 100.0 * 6.259566307067871
Epoch 1120, val loss: 0.9400659203529358
Epoch 1130, training loss: 626.0151977539062 = 0.3212205171585083 + 100.0 * 6.256939888000488
Epoch 1130, val loss: 0.9426765441894531
Epoch 1140, training loss: 626.0057373046875 = 0.3144930303096771 + 100.0 * 6.2569122314453125
Epoch 1140, val loss: 0.9457098245620728
Epoch 1150, training loss: 626.7295532226562 = 0.30785632133483887 + 100.0 * 6.264216899871826
Epoch 1150, val loss: 0.9490711092948914
Epoch 1160, training loss: 625.9066162109375 = 0.30121687054634094 + 100.0 * 6.256053924560547
Epoch 1160, val loss: 0.9518530964851379
Epoch 1170, training loss: 625.9452514648438 = 0.2948189973831177 + 100.0 * 6.256504535675049
Epoch 1170, val loss: 0.9548364877700806
Epoch 1180, training loss: 625.708740234375 = 0.2885971963405609 + 100.0 * 6.254201889038086
Epoch 1180, val loss: 0.9584148526191711
Epoch 1190, training loss: 626.013427734375 = 0.28254204988479614 + 100.0 * 6.2573089599609375
Epoch 1190, val loss: 0.9621797800064087
Epoch 1200, training loss: 626.0234375 = 0.27647915482521057 + 100.0 * 6.257469177246094
Epoch 1200, val loss: 0.9653151631355286
Epoch 1210, training loss: 625.8267822265625 = 0.2705378830432892 + 100.0 * 6.255562782287598
Epoch 1210, val loss: 0.9689654111862183
Epoch 1220, training loss: 625.6591186523438 = 0.2647520899772644 + 100.0 * 6.25394344329834
Epoch 1220, val loss: 0.9724240899085999
Epoch 1230, training loss: 625.5440673828125 = 0.25912097096443176 + 100.0 * 6.252849578857422
Epoch 1230, val loss: 0.9763035774230957
Epoch 1240, training loss: 625.48193359375 = 0.2535921633243561 + 100.0 * 6.252283096313477
Epoch 1240, val loss: 0.9797860383987427
Epoch 1250, training loss: 625.5924682617188 = 0.24819886684417725 + 100.0 * 6.253443241119385
Epoch 1250, val loss: 0.9835057854652405
Epoch 1260, training loss: 625.5433959960938 = 0.24288994073867798 + 100.0 * 6.253004550933838
Epoch 1260, val loss: 0.9871084690093994
Epoch 1270, training loss: 625.4303588867188 = 0.237672820687294 + 100.0 * 6.251926898956299
Epoch 1270, val loss: 0.9906656742095947
Epoch 1280, training loss: 625.3563232421875 = 0.23258619010448456 + 100.0 * 6.251236915588379
Epoch 1280, val loss: 0.9943962097167969
Epoch 1290, training loss: 625.5420532226562 = 0.22761522233486176 + 100.0 * 6.253144264221191
Epoch 1290, val loss: 0.9979978203773499
Epoch 1300, training loss: 625.3864135742188 = 0.2227102816104889 + 100.0 * 6.251636981964111
Epoch 1300, val loss: 1.0027220249176025
Epoch 1310, training loss: 625.0643920898438 = 0.2179083228111267 + 100.0 * 6.248465061187744
Epoch 1310, val loss: 1.0060027837753296
Epoch 1320, training loss: 624.96240234375 = 0.2132648378610611 + 100.0 * 6.247491359710693
Epoch 1320, val loss: 1.010368824005127
Epoch 1330, training loss: 625.0366821289062 = 0.20874719321727753 + 100.0 * 6.248279571533203
Epoch 1330, val loss: 1.0144847631454468
Epoch 1340, training loss: 625.2859497070312 = 0.2043052762746811 + 100.0 * 6.250816822052002
Epoch 1340, val loss: 1.0184016227722168
Epoch 1350, training loss: 625.726806640625 = 0.19990083575248718 + 100.0 * 6.2552690505981445
Epoch 1350, val loss: 1.0222587585449219
Epoch 1360, training loss: 625.0059204101562 = 0.19558878242969513 + 100.0 * 6.248103141784668
Epoch 1360, val loss: 1.0261226892471313
Epoch 1370, training loss: 624.7918090820312 = 0.19139961898326874 + 100.0 * 6.246004104614258
Epoch 1370, val loss: 1.0301402807235718
Epoch 1380, training loss: 624.908447265625 = 0.18734711408615112 + 100.0 * 6.247211456298828
Epoch 1380, val loss: 1.0344343185424805
Epoch 1390, training loss: 624.750732421875 = 0.1833597868680954 + 100.0 * 6.245673656463623
Epoch 1390, val loss: 1.038456916809082
Epoch 1400, training loss: 624.7081909179688 = 0.17946991324424744 + 100.0 * 6.2452874183654785
Epoch 1400, val loss: 1.0431450605392456
Epoch 1410, training loss: 624.8102416992188 = 0.17567136883735657 + 100.0 * 6.2463459968566895
Epoch 1410, val loss: 1.047181248664856
Epoch 1420, training loss: 624.89404296875 = 0.17192906141281128 + 100.0 * 6.247220993041992
Epoch 1420, val loss: 1.0513510704040527
Epoch 1430, training loss: 624.9745483398438 = 0.16824544966220856 + 100.0 * 6.248062610626221
Epoch 1430, val loss: 1.0560795068740845
Epoch 1440, training loss: 624.5816650390625 = 0.16461831331253052 + 100.0 * 6.244170665740967
Epoch 1440, val loss: 1.0598070621490479
Epoch 1450, training loss: 624.441162109375 = 0.16114389896392822 + 100.0 * 6.242800235748291
Epoch 1450, val loss: 1.0643693208694458
Epoch 1460, training loss: 624.3052368164062 = 0.1577598750591278 + 100.0 * 6.241474628448486
Epoch 1460, val loss: 1.0688129663467407
Epoch 1470, training loss: 624.3742065429688 = 0.154465451836586 + 100.0 * 6.242197513580322
Epoch 1470, val loss: 1.073037028312683
Epoch 1480, training loss: 625.2008666992188 = 0.15119510889053345 + 100.0 * 6.250496864318848
Epoch 1480, val loss: 1.077163577079773
Epoch 1490, training loss: 624.2816772460938 = 0.14791053533554077 + 100.0 * 6.241337776184082
Epoch 1490, val loss: 1.0814522504806519
Epoch 1500, training loss: 624.3414306640625 = 0.1447879821062088 + 100.0 * 6.241966724395752
Epoch 1500, val loss: 1.0867929458618164
Epoch 1510, training loss: 624.150634765625 = 0.1417481154203415 + 100.0 * 6.240089416503906
Epoch 1510, val loss: 1.0909266471862793
Epoch 1520, training loss: 624.6941528320312 = 0.1388045847415924 + 100.0 * 6.245553493499756
Epoch 1520, val loss: 1.0950334072113037
Epoch 1530, training loss: 624.26953125 = 0.13587157428264618 + 100.0 * 6.241336822509766
Epoch 1530, val loss: 1.100518822669983
Epoch 1540, training loss: 624.6049194335938 = 0.1330183446407318 + 100.0 * 6.2447190284729
Epoch 1540, val loss: 1.1042097806930542
Epoch 1550, training loss: 624.2483520507812 = 0.13020899891853333 + 100.0 * 6.24118185043335
Epoch 1550, val loss: 1.1096687316894531
Epoch 1560, training loss: 624.1278686523438 = 0.12748073041439056 + 100.0 * 6.24000358581543
Epoch 1560, val loss: 1.1141616106033325
Epoch 1570, training loss: 623.97705078125 = 0.12483029067516327 + 100.0 * 6.238522052764893
Epoch 1570, val loss: 1.1189794540405273
Epoch 1580, training loss: 624.0244140625 = 0.1222643181681633 + 100.0 * 6.2390217781066895
Epoch 1580, val loss: 1.1237627267837524
Epoch 1590, training loss: 624.0885009765625 = 0.11974100768566132 + 100.0 * 6.239687442779541
Epoch 1590, val loss: 1.1285086870193481
Epoch 1600, training loss: 624.1251220703125 = 0.11725235730409622 + 100.0 * 6.240078449249268
Epoch 1600, val loss: 1.1326133012771606
Epoch 1610, training loss: 624.3516235351562 = 0.11483795195817947 + 100.0 * 6.242367744445801
Epoch 1610, val loss: 1.1375690698623657
Epoch 1620, training loss: 623.9217529296875 = 0.11243676394224167 + 100.0 * 6.238093376159668
Epoch 1620, val loss: 1.1426405906677246
Epoch 1630, training loss: 623.81787109375 = 0.11014270782470703 + 100.0 * 6.237076759338379
Epoch 1630, val loss: 1.1472028493881226
Epoch 1640, training loss: 624.4509887695312 = 0.10789994150400162 + 100.0 * 6.2434306144714355
Epoch 1640, val loss: 1.1524147987365723
Epoch 1650, training loss: 623.8321533203125 = 0.10564567148685455 + 100.0 * 6.237265110015869
Epoch 1650, val loss: 1.1567819118499756
Epoch 1660, training loss: 623.6023559570312 = 0.10349877178668976 + 100.0 * 6.234988689422607
Epoch 1660, val loss: 1.1617454290390015
Epoch 1670, training loss: 623.5654296875 = 0.10142069309949875 + 100.0 * 6.234640121459961
Epoch 1670, val loss: 1.1666117906570435
Epoch 1680, training loss: 624.4473876953125 = 0.0994286835193634 + 100.0 * 6.2434797286987305
Epoch 1680, val loss: 1.171746850013733
Epoch 1690, training loss: 623.9490356445312 = 0.0973551943898201 + 100.0 * 6.238516807556152
Epoch 1690, val loss: 1.175394058227539
Epoch 1700, training loss: 623.6150512695312 = 0.09537740051746368 + 100.0 * 6.235196590423584
Epoch 1700, val loss: 1.1807903051376343
Epoch 1710, training loss: 623.429931640625 = 0.0934670940041542 + 100.0 * 6.233365058898926
Epoch 1710, val loss: 1.185380220413208
Epoch 1720, training loss: 623.576416015625 = 0.09162689745426178 + 100.0 * 6.2348480224609375
Epoch 1720, val loss: 1.1902955770492554
Epoch 1730, training loss: 623.6283569335938 = 0.0897974744439125 + 100.0 * 6.235385894775391
Epoch 1730, val loss: 1.194883942604065
Epoch 1740, training loss: 623.7741088867188 = 0.08800986409187317 + 100.0 * 6.236861228942871
Epoch 1740, val loss: 1.200096607208252
Epoch 1750, training loss: 623.6928100585938 = 0.0862555056810379 + 100.0 * 6.23606538772583
Epoch 1750, val loss: 1.2050408124923706
Epoch 1760, training loss: 623.3848266601562 = 0.08454547822475433 + 100.0 * 6.23300313949585
Epoch 1760, val loss: 1.2100427150726318
Epoch 1770, training loss: 623.52685546875 = 0.08288173377513885 + 100.0 * 6.234439849853516
Epoch 1770, val loss: 1.214815616607666
Epoch 1780, training loss: 623.4479370117188 = 0.08126934617757797 + 100.0 * 6.23366641998291
Epoch 1780, val loss: 1.2199028730392456
Epoch 1790, training loss: 623.1716918945312 = 0.07967859506607056 + 100.0 * 6.230920314788818
Epoch 1790, val loss: 1.224746584892273
Epoch 1800, training loss: 623.2716064453125 = 0.07815131545066833 + 100.0 * 6.231935024261475
Epoch 1800, val loss: 1.229472041130066
Epoch 1810, training loss: 623.8089599609375 = 0.07665788382291794 + 100.0 * 6.237322807312012
Epoch 1810, val loss: 1.2339640855789185
Epoch 1820, training loss: 623.3372192382812 = 0.07513923197984695 + 100.0 * 6.232620716094971
Epoch 1820, val loss: 1.2389819622039795
Epoch 1830, training loss: 623.1575927734375 = 0.07367446273565292 + 100.0 * 6.230839252471924
Epoch 1830, val loss: 1.2436848878860474
Epoch 1840, training loss: 623.0695190429688 = 0.07228433340787888 + 100.0 * 6.2299723625183105
Epoch 1840, val loss: 1.248731255531311
Epoch 1850, training loss: 623.3250732421875 = 0.07094603776931763 + 100.0 * 6.232541561126709
Epoch 1850, val loss: 1.253967046737671
Epoch 1860, training loss: 623.0692749023438 = 0.06957049667835236 + 100.0 * 6.229997158050537
Epoch 1860, val loss: 1.2581374645233154
Epoch 1870, training loss: 623.0294799804688 = 0.06823274493217468 + 100.0 * 6.229612350463867
Epoch 1870, val loss: 1.2629337310791016
Epoch 1880, training loss: 623.0598754882812 = 0.0669529065489769 + 100.0 * 6.229928970336914
Epoch 1880, val loss: 1.2673074007034302
Epoch 1890, training loss: 623.1858520507812 = 0.06571823358535767 + 100.0 * 6.231201171875
Epoch 1890, val loss: 1.2723151445388794
Epoch 1900, training loss: 623.0338745117188 = 0.06448865681886673 + 100.0 * 6.22969388961792
Epoch 1900, val loss: 1.2766183614730835
Epoch 1910, training loss: 623.5914306640625 = 0.06330376118421555 + 100.0 * 6.235281467437744
Epoch 1910, val loss: 1.2809268236160278
Epoch 1920, training loss: 623.1749267578125 = 0.0621214359998703 + 100.0 * 6.231127738952637
Epoch 1920, val loss: 1.285991907119751
Epoch 1930, training loss: 623.0933837890625 = 0.06097077950835228 + 100.0 * 6.2303242683410645
Epoch 1930, val loss: 1.2901819944381714
Epoch 1940, training loss: 622.858642578125 = 0.059854764491319656 + 100.0 * 6.227987766265869
Epoch 1940, val loss: 1.2955634593963623
Epoch 1950, training loss: 622.8258056640625 = 0.058776140213012695 + 100.0 * 6.227670192718506
Epoch 1950, val loss: 1.3000166416168213
Epoch 1960, training loss: 622.965087890625 = 0.05772864446043968 + 100.0 * 6.229073524475098
Epoch 1960, val loss: 1.3049836158752441
Epoch 1970, training loss: 623.1908569335938 = 0.05669468641281128 + 100.0 * 6.231341361999512
Epoch 1970, val loss: 1.3104139566421509
Epoch 1980, training loss: 622.7482299804688 = 0.0556466318666935 + 100.0 * 6.226925373077393
Epoch 1980, val loss: 1.3132466077804565
Epoch 1990, training loss: 622.7305908203125 = 0.05464404076337814 + 100.0 * 6.226759910583496
Epoch 1990, val loss: 1.3184146881103516
Epoch 2000, training loss: 623.4420776367188 = 0.053677745163440704 + 100.0 * 6.233884334564209
Epoch 2000, val loss: 1.3223754167556763
Epoch 2010, training loss: 622.8056640625 = 0.05270962044596672 + 100.0 * 6.227529525756836
Epoch 2010, val loss: 1.3273144960403442
Epoch 2020, training loss: 622.5960083007812 = 0.05177741497755051 + 100.0 * 6.225442409515381
Epoch 2020, val loss: 1.3315025568008423
Epoch 2030, training loss: 622.5758666992188 = 0.05088057368993759 + 100.0 * 6.225249767303467
Epoch 2030, val loss: 1.3361481428146362
Epoch 2040, training loss: 622.69189453125 = 0.050012290477752686 + 100.0 * 6.226418972015381
Epoch 2040, val loss: 1.340489387512207
Epoch 2050, training loss: 622.7180786132812 = 0.049139901995658875 + 100.0 * 6.226689338684082
Epoch 2050, val loss: 1.3449928760528564
Epoch 2060, training loss: 622.654296875 = 0.04826640337705612 + 100.0 * 6.226060390472412
Epoch 2060, val loss: 1.3492885828018188
Epoch 2070, training loss: 622.8428344726562 = 0.04743824154138565 + 100.0 * 6.227954387664795
Epoch 2070, val loss: 1.3539562225341797
Epoch 2080, training loss: 622.5460205078125 = 0.04661475867033005 + 100.0 * 6.224993705749512
Epoch 2080, val loss: 1.3578956127166748
Epoch 2090, training loss: 622.7139282226562 = 0.04582494869828224 + 100.0 * 6.226680755615234
Epoch 2090, val loss: 1.3618066310882568
Epoch 2100, training loss: 622.4776000976562 = 0.04504583030939102 + 100.0 * 6.224325656890869
Epoch 2100, val loss: 1.3665556907653809
Epoch 2110, training loss: 622.5850830078125 = 0.04430125653743744 + 100.0 * 6.225407600402832
Epoch 2110, val loss: 1.371144413948059
Epoch 2120, training loss: 622.6103515625 = 0.043559711426496506 + 100.0 * 6.225667953491211
Epoch 2120, val loss: 1.3747225999832153
Epoch 2130, training loss: 622.7322998046875 = 0.042826853692531586 + 100.0 * 6.226894378662109
Epoch 2130, val loss: 1.3793189525604248
Epoch 2140, training loss: 622.5034790039062 = 0.042104557156562805 + 100.0 * 6.224613666534424
Epoch 2140, val loss: 1.3836125135421753
Epoch 2150, training loss: 622.5156860351562 = 0.04140973836183548 + 100.0 * 6.224742889404297
Epoch 2150, val loss: 1.3873276710510254
Epoch 2160, training loss: 622.461181640625 = 0.04073644429445267 + 100.0 * 6.2242045402526855
Epoch 2160, val loss: 1.3919302225112915
Epoch 2170, training loss: 622.405029296875 = 0.040064625442028046 + 100.0 * 6.223649978637695
Epoch 2170, val loss: 1.3959088325500488
Epoch 2180, training loss: 622.4844970703125 = 0.03940520063042641 + 100.0 * 6.224450588226318
Epoch 2180, val loss: 1.3991996049880981
Epoch 2190, training loss: 622.326171875 = 0.038769401609897614 + 100.0 * 6.222874164581299
Epoch 2190, val loss: 1.403743863105774
Epoch 2200, training loss: 622.4021606445312 = 0.03814570605754852 + 100.0 * 6.223639965057373
Epoch 2200, val loss: 1.4077556133270264
Epoch 2210, training loss: 622.2540283203125 = 0.037520214915275574 + 100.0 * 6.222165107727051
Epoch 2210, val loss: 1.411486268043518
Epoch 2220, training loss: 622.5751342773438 = 0.03692910447716713 + 100.0 * 6.225382328033447
Epoch 2220, val loss: 1.4162348508834839
Epoch 2230, training loss: 622.3738403320312 = 0.03631257638335228 + 100.0 * 6.22337532043457
Epoch 2230, val loss: 1.4202228784561157
Epoch 2240, training loss: 622.4088745117188 = 0.035710178315639496 + 100.0 * 6.223731994628906
Epoch 2240, val loss: 1.422800064086914
Epoch 2250, training loss: 622.53564453125 = 0.03513491898775101 + 100.0 * 6.225005149841309
Epoch 2250, val loss: 1.4281412363052368
Epoch 2260, training loss: 622.139892578125 = 0.034550949931144714 + 100.0 * 6.221053600311279
Epoch 2260, val loss: 1.43204927444458
Epoch 2270, training loss: 622.1279907226562 = 0.033996500074863434 + 100.0 * 6.220939636230469
Epoch 2270, val loss: 1.4364399909973145
Epoch 2280, training loss: 622.595703125 = 0.033451441675424576 + 100.0 * 6.225622653961182
Epoch 2280, val loss: 1.4398887157440186
Epoch 2290, training loss: 622.1685180664062 = 0.03290335834026337 + 100.0 * 6.221356391906738
Epoch 2290, val loss: 1.4441027641296387
Epoch 2300, training loss: 622.2586669921875 = 0.032389894127845764 + 100.0 * 6.222262382507324
Epoch 2300, val loss: 1.4481377601623535
Epoch 2310, training loss: 622.083984375 = 0.03186899423599243 + 100.0 * 6.220521450042725
Epoch 2310, val loss: 1.451588749885559
Epoch 2320, training loss: 622.0037841796875 = 0.03136609494686127 + 100.0 * 6.219724178314209
Epoch 2320, val loss: 1.4557018280029297
Epoch 2330, training loss: 622.0573120117188 = 0.030882230028510094 + 100.0 * 6.220264434814453
Epoch 2330, val loss: 1.4599339962005615
Epoch 2340, training loss: 622.241455078125 = 0.030408337712287903 + 100.0 * 6.222110748291016
Epoch 2340, val loss: 1.463931918144226
Epoch 2350, training loss: 622.1280517578125 = 0.029924310743808746 + 100.0 * 6.220981597900391
Epoch 2350, val loss: 1.4672763347625732
Epoch 2360, training loss: 622.0840454101562 = 0.029454851523041725 + 100.0 * 6.220545768737793
Epoch 2360, val loss: 1.4704113006591797
Epoch 2370, training loss: 621.9869384765625 = 0.029001053422689438 + 100.0 * 6.219579219818115
Epoch 2370, val loss: 1.4747350215911865
Epoch 2380, training loss: 621.8501586914062 = 0.028557084500789642 + 100.0 * 6.2182159423828125
Epoch 2380, val loss: 1.4783751964569092
Epoch 2390, training loss: 622.11962890625 = 0.02814117819070816 + 100.0 * 6.220914840698242
Epoch 2390, val loss: 1.4821439981460571
Epoch 2400, training loss: 622.11083984375 = 0.02770879678428173 + 100.0 * 6.220831394195557
Epoch 2400, val loss: 1.4850369691848755
Epoch 2410, training loss: 622.2084350585938 = 0.027279909700155258 + 100.0 * 6.221811771392822
Epoch 2410, val loss: 1.4882011413574219
Epoch 2420, training loss: 622.0697631835938 = 0.026875359937548637 + 100.0 * 6.220428943634033
Epoch 2420, val loss: 1.4917763471603394
Epoch 2430, training loss: 621.7855224609375 = 0.026466481387615204 + 100.0 * 6.21759033203125
Epoch 2430, val loss: 1.4950239658355713
Epoch 2440, training loss: 621.7152099609375 = 0.02608509361743927 + 100.0 * 6.216891765594482
Epoch 2440, val loss: 1.4986485242843628
Epoch 2450, training loss: 621.7799682617188 = 0.025713128969073296 + 100.0 * 6.21754264831543
Epoch 2450, val loss: 1.5022714138031006
Epoch 2460, training loss: 622.240966796875 = 0.02535041980445385 + 100.0 * 6.222156524658203
Epoch 2460, val loss: 1.5053200721740723
Epoch 2470, training loss: 622.1880493164062 = 0.02497178316116333 + 100.0 * 6.221630573272705
Epoch 2470, val loss: 1.5079108476638794
Epoch 2480, training loss: 621.8455810546875 = 0.024608930572867393 + 100.0 * 6.218209743499756
Epoch 2480, val loss: 1.5122060775756836
Epoch 2490, training loss: 621.9035034179688 = 0.02426016516983509 + 100.0 * 6.218792915344238
Epoch 2490, val loss: 1.5152772665023804
Epoch 2500, training loss: 621.873291015625 = 0.02391047589480877 + 100.0 * 6.218493938446045
Epoch 2500, val loss: 1.5180848836898804
Epoch 2510, training loss: 621.6541748046875 = 0.023572783917188644 + 100.0 * 6.216306209564209
Epoch 2510, val loss: 1.5210180282592773
Epoch 2520, training loss: 621.6555786132812 = 0.023252874612808228 + 100.0 * 6.216323375701904
Epoch 2520, val loss: 1.524406909942627
Epoch 2530, training loss: 622.04248046875 = 0.022935200482606888 + 100.0 * 6.220195293426514
Epoch 2530, val loss: 1.527158260345459
Epoch 2540, training loss: 621.7845458984375 = 0.02261832170188427 + 100.0 * 6.217618942260742
Epoch 2540, val loss: 1.530869722366333
Epoch 2550, training loss: 621.7977294921875 = 0.022311201319098473 + 100.0 * 6.217753887176514
Epoch 2550, val loss: 1.5343708992004395
Epoch 2560, training loss: 621.5250244140625 = 0.02200603485107422 + 100.0 * 6.215029716491699
Epoch 2560, val loss: 1.5370146036148071
Epoch 2570, training loss: 621.4415283203125 = 0.02171364799141884 + 100.0 * 6.214198112487793
Epoch 2570, val loss: 1.539995789527893
Epoch 2580, training loss: 621.7979125976562 = 0.021433454006910324 + 100.0 * 6.217764854431152
Epoch 2580, val loss: 1.5430749654769897
Epoch 2590, training loss: 621.9784545898438 = 0.021144159138202667 + 100.0 * 6.219573497772217
Epoch 2590, val loss: 1.5458261966705322
Epoch 2600, training loss: 621.5702514648438 = 0.020856034010648727 + 100.0 * 6.215493679046631
Epoch 2600, val loss: 1.5484713315963745
Epoch 2610, training loss: 621.5111694335938 = 0.020576218143105507 + 100.0 * 6.214905738830566
Epoch 2610, val loss: 1.5511878728866577
Epoch 2620, training loss: 621.7218017578125 = 0.020318446680903435 + 100.0 * 6.217014789581299
Epoch 2620, val loss: 1.5542374849319458
Epoch 2630, training loss: 621.5872192382812 = 0.020057301968336105 + 100.0 * 6.215671539306641
Epoch 2630, val loss: 1.557854175567627
Epoch 2640, training loss: 621.6911010742188 = 0.019805962219834328 + 100.0 * 6.216712474822998
Epoch 2640, val loss: 1.5610913038253784
Epoch 2650, training loss: 621.4912109375 = 0.01954100839793682 + 100.0 * 6.214716911315918
Epoch 2650, val loss: 1.5625176429748535
Epoch 2660, training loss: 621.502685546875 = 0.019297268241643906 + 100.0 * 6.214833736419678
Epoch 2660, val loss: 1.5659431219100952
Epoch 2670, training loss: 621.4467163085938 = 0.019059112295508385 + 100.0 * 6.2142767906188965
Epoch 2670, val loss: 1.5691392421722412
Epoch 2680, training loss: 621.415283203125 = 0.018822336569428444 + 100.0 * 6.213964939117432
Epoch 2680, val loss: 1.5717582702636719
Epoch 2690, training loss: 621.8263549804688 = 0.018592065200209618 + 100.0 * 6.218077659606934
Epoch 2690, val loss: 1.5740498304367065
Epoch 2700, training loss: 621.5106811523438 = 0.018352311104536057 + 100.0 * 6.214922904968262
Epoch 2700, val loss: 1.5762288570404053
Epoch 2710, training loss: 621.5079956054688 = 0.018130267038941383 + 100.0 * 6.214898586273193
Epoch 2710, val loss: 1.579319953918457
Epoch 2720, training loss: 621.4703369140625 = 0.01790427230298519 + 100.0 * 6.214524745941162
Epoch 2720, val loss: 1.5816881656646729
Epoch 2730, training loss: 621.439453125 = 0.017685826867818832 + 100.0 * 6.214217662811279
Epoch 2730, val loss: 1.584982991218567
Epoch 2740, training loss: 621.8267211914062 = 0.01747577078640461 + 100.0 * 6.218092918395996
Epoch 2740, val loss: 1.5872600078582764
Epoch 2750, training loss: 621.2548828125 = 0.01725347340106964 + 100.0 * 6.212376117706299
Epoch 2750, val loss: 1.5891270637512207
Epoch 2760, training loss: 621.1494140625 = 0.017050445079803467 + 100.0 * 6.2113237380981445
Epoch 2760, val loss: 1.5921512842178345
Epoch 2770, training loss: 621.2294311523438 = 0.01685822196304798 + 100.0 * 6.212125778198242
Epoch 2770, val loss: 1.5948872566223145
Epoch 2780, training loss: 621.9050903320312 = 0.016665589064359665 + 100.0 * 6.218883991241455
Epoch 2780, val loss: 1.5964409112930298
Epoch 2790, training loss: 621.5247802734375 = 0.016463689506053925 + 100.0 * 6.215083122253418
Epoch 2790, val loss: 1.599084496498108
Epoch 2800, training loss: 621.2793579101562 = 0.01627148874104023 + 100.0 * 6.212630748748779
Epoch 2800, val loss: 1.6020337343215942
Epoch 2810, training loss: 621.2112426757812 = 0.016087647527456284 + 100.0 * 6.21195125579834
Epoch 2810, val loss: 1.6045259237289429
Epoch 2820, training loss: 621.79443359375 = 0.015909967944025993 + 100.0 * 6.217784881591797
Epoch 2820, val loss: 1.6069525480270386
Epoch 2830, training loss: 621.3649291992188 = 0.015717990696430206 + 100.0 * 6.213491916656494
Epoch 2830, val loss: 1.6093034744262695
Epoch 2840, training loss: 621.2811279296875 = 0.015540776774287224 + 100.0 * 6.212655544281006
Epoch 2840, val loss: 1.6110491752624512
Epoch 2850, training loss: 621.6361694335938 = 0.015372799709439278 + 100.0 * 6.216207981109619
Epoch 2850, val loss: 1.613650918006897
Epoch 2860, training loss: 621.2174072265625 = 0.01519875880330801 + 100.0 * 6.212022304534912
Epoch 2860, val loss: 1.6168992519378662
Epoch 2870, training loss: 621.1904296875 = 0.015028879977762699 + 100.0 * 6.211754322052002
Epoch 2870, val loss: 1.6190447807312012
Epoch 2880, training loss: 621.2766723632812 = 0.014862088486552238 + 100.0 * 6.212617874145508
Epoch 2880, val loss: 1.6208786964416504
Epoch 2890, training loss: 621.022216796875 = 0.01469840295612812 + 100.0 * 6.2100749015808105
Epoch 2890, val loss: 1.623503565788269
Epoch 2900, training loss: 621.4432373046875 = 0.014546186663210392 + 100.0 * 6.214286804199219
Epoch 2900, val loss: 1.6260452270507812
Epoch 2910, training loss: 621.1491088867188 = 0.01438158843666315 + 100.0 * 6.2113471031188965
Epoch 2910, val loss: 1.6274131536483765
Epoch 2920, training loss: 621.0214233398438 = 0.014225076884031296 + 100.0 * 6.210072040557861
Epoch 2920, val loss: 1.6294454336166382
Epoch 2930, training loss: 621.5208129882812 = 0.014075339771807194 + 100.0 * 6.215067386627197
Epoch 2930, val loss: 1.6313098669052124
Epoch 2940, training loss: 621.0554809570312 = 0.013920744881033897 + 100.0 * 6.210415840148926
Epoch 2940, val loss: 1.633655309677124
Epoch 2950, training loss: 621.14013671875 = 0.013776703737676144 + 100.0 * 6.211263179779053
Epoch 2950, val loss: 1.6356301307678223
Epoch 2960, training loss: 621.4020385742188 = 0.013631441630423069 + 100.0 * 6.213884353637695
Epoch 2960, val loss: 1.6366389989852905
Epoch 2970, training loss: 620.9490966796875 = 0.013487804681062698 + 100.0 * 6.209356307983398
Epoch 2970, val loss: 1.6406302452087402
Epoch 2980, training loss: 620.9833374023438 = 0.01335123460739851 + 100.0 * 6.209699630737305
Epoch 2980, val loss: 1.6424299478530884
Epoch 2990, training loss: 621.0120849609375 = 0.013213488273322582 + 100.0 * 6.209989070892334
Epoch 2990, val loss: 1.6437357664108276
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 861.636962890625 = 1.9540643692016602 + 100.0 * 8.596829414367676
Epoch 0, val loss: 1.953004002571106
Epoch 10, training loss: 861.5399169921875 = 1.9454032182693481 + 100.0 * 8.595945358276367
Epoch 10, val loss: 1.9445921182632446
Epoch 20, training loss: 860.9146118164062 = 1.9344935417175293 + 100.0 * 8.589800834655762
Epoch 20, val loss: 1.9336386919021606
Epoch 30, training loss: 856.5369873046875 = 1.920155644416809 + 100.0 * 8.546168327331543
Epoch 30, val loss: 1.9189722537994385
Epoch 40, training loss: 827.9937133789062 = 1.9025287628173828 + 100.0 * 8.26091194152832
Epoch 40, val loss: 1.9012161493301392
Epoch 50, training loss: 760.3442993164062 = 1.8850303888320923 + 100.0 * 7.584592819213867
Epoch 50, val loss: 1.884791374206543
Epoch 60, training loss: 719.7078247070312 = 1.874088168144226 + 100.0 * 7.178337097167969
Epoch 60, val loss: 1.8744243383407593
Epoch 70, training loss: 699.5219116210938 = 1.864278793334961 + 100.0 * 6.976576805114746
Epoch 70, val loss: 1.8653336763381958
Epoch 80, training loss: 688.47216796875 = 1.8546658754348755 + 100.0 * 6.866174697875977
Epoch 80, val loss: 1.8562496900558472
Epoch 90, training loss: 679.8165283203125 = 1.8451404571533203 + 100.0 * 6.7797136306762695
Epoch 90, val loss: 1.8472518920898438
Epoch 100, training loss: 673.6873779296875 = 1.8355414867401123 + 100.0 * 6.718518257141113
Epoch 100, val loss: 1.8380682468414307
Epoch 110, training loss: 669.1040649414062 = 1.8262017965316772 + 100.0 * 6.672779083251953
Epoch 110, val loss: 1.8291075229644775
Epoch 120, training loss: 665.4803466796875 = 1.8174679279327393 + 100.0 * 6.636629104614258
Epoch 120, val loss: 1.8207149505615234
Epoch 130, training loss: 662.2191162109375 = 1.808973789215088 + 100.0 * 6.604101657867432
Epoch 130, val loss: 1.8126775026321411
Epoch 140, training loss: 659.505615234375 = 1.8007320165634155 + 100.0 * 6.5770487785339355
Epoch 140, val loss: 1.8050496578216553
Epoch 150, training loss: 657.1826782226562 = 1.792563557624817 + 100.0 * 6.553901195526123
Epoch 150, val loss: 1.7974472045898438
Epoch 160, training loss: 655.0286254882812 = 1.7841984033584595 + 100.0 * 6.532444477081299
Epoch 160, val loss: 1.7898616790771484
Epoch 170, training loss: 653.2405395507812 = 1.7756075859069824 + 100.0 * 6.514648914337158
Epoch 170, val loss: 1.7822071313858032
Epoch 180, training loss: 651.5775756835938 = 1.7665398120880127 + 100.0 * 6.498110294342041
Epoch 180, val loss: 1.7741808891296387
Epoch 190, training loss: 650.1895141601562 = 1.7568953037261963 + 100.0 * 6.484325885772705
Epoch 190, val loss: 1.7657759189605713
Epoch 200, training loss: 648.918701171875 = 1.746644377708435 + 100.0 * 6.4717206954956055
Epoch 200, val loss: 1.756880521774292
Epoch 210, training loss: 647.9387817382812 = 1.7355643510818481 + 100.0 * 6.462032318115234
Epoch 210, val loss: 1.7473475933074951
Epoch 220, training loss: 646.6826782226562 = 1.7236266136169434 + 100.0 * 6.449590682983398
Epoch 220, val loss: 1.737156867980957
Epoch 230, training loss: 646.2957763671875 = 1.710822582244873 + 100.0 * 6.445849895477295
Epoch 230, val loss: 1.7262176275253296
Epoch 240, training loss: 645.033203125 = 1.696804404258728 + 100.0 * 6.433364391326904
Epoch 240, val loss: 1.714364767074585
Epoch 250, training loss: 644.0987548828125 = 1.6818690299987793 + 100.0 * 6.424168586730957
Epoch 250, val loss: 1.7016851902008057
Epoch 260, training loss: 643.371826171875 = 1.665831208229065 + 100.0 * 6.417059898376465
Epoch 260, val loss: 1.6881135702133179
Epoch 270, training loss: 642.885009765625 = 1.64854097366333 + 100.0 * 6.412364482879639
Epoch 270, val loss: 1.6734992265701294
Epoch 280, training loss: 641.9100341796875 = 1.6301764249801636 + 100.0 * 6.402798652648926
Epoch 280, val loss: 1.6580069065093994
Epoch 290, training loss: 641.2499389648438 = 1.6107544898986816 + 100.0 * 6.396391868591309
Epoch 290, val loss: 1.641677737236023
Epoch 300, training loss: 641.691650390625 = 1.5904120206832886 + 100.0 * 6.401012420654297
Epoch 300, val loss: 1.6245332956314087
Epoch 310, training loss: 640.4292602539062 = 1.5687155723571777 + 100.0 * 6.38860559463501
Epoch 310, val loss: 1.60642671585083
Epoch 320, training loss: 639.6572265625 = 1.5462123155593872 + 100.0 * 6.381110191345215
Epoch 320, val loss: 1.587670087814331
Epoch 330, training loss: 639.0709228515625 = 1.5229030847549438 + 100.0 * 6.3754801750183105
Epoch 330, val loss: 1.5684280395507812
Epoch 340, training loss: 639.0272827148438 = 1.4990121126174927 + 100.0 * 6.3752827644348145
Epoch 340, val loss: 1.5488237142562866
Epoch 350, training loss: 638.3872680664062 = 1.4743516445159912 + 100.0 * 6.369129180908203
Epoch 350, val loss: 1.5285136699676514
Epoch 360, training loss: 637.7888793945312 = 1.449194073677063 + 100.0 * 6.363396644592285
Epoch 360, val loss: 1.5079315900802612
Epoch 370, training loss: 637.364013671875 = 1.423757553100586 + 100.0 * 6.359402179718018
Epoch 370, val loss: 1.4873580932617188
Epoch 380, training loss: 637.2131958007812 = 1.3980796337127686 + 100.0 * 6.358151435852051
Epoch 380, val loss: 1.4667739868164062
Epoch 390, training loss: 636.6040649414062 = 1.372239112854004 + 100.0 * 6.352318286895752
Epoch 390, val loss: 1.4460963010787964
Epoch 400, training loss: 636.1436157226562 = 1.3463397026062012 + 100.0 * 6.347972869873047
Epoch 400, val loss: 1.4255216121673584
Epoch 410, training loss: 636.1421508789062 = 1.3204295635223389 + 100.0 * 6.348217010498047
Epoch 410, val loss: 1.4049742221832275
Epoch 420, training loss: 635.6819458007812 = 1.2945784330368042 + 100.0 * 6.343873977661133
Epoch 420, val loss: 1.3848541975021362
Epoch 430, training loss: 635.6157836914062 = 1.2689218521118164 + 100.0 * 6.34346866607666
Epoch 430, val loss: 1.3651221990585327
Epoch 440, training loss: 635.1365356445312 = 1.2432715892791748 + 100.0 * 6.338932514190674
Epoch 440, val loss: 1.3454700708389282
Epoch 450, training loss: 634.5646362304688 = 1.21815025806427 + 100.0 * 6.333465099334717
Epoch 450, val loss: 1.3264774084091187
Epoch 460, training loss: 634.3461303710938 = 1.1933528184890747 + 100.0 * 6.3315277099609375
Epoch 460, val loss: 1.3079990148544312
Epoch 470, training loss: 634.3506469726562 = 1.1689084768295288 + 100.0 * 6.331817626953125
Epoch 470, val loss: 1.2899096012115479
Epoch 480, training loss: 633.7787475585938 = 1.1449707746505737 + 100.0 * 6.326337814331055
Epoch 480, val loss: 1.2727999687194824
Epoch 490, training loss: 633.611572265625 = 1.1215583086013794 + 100.0 * 6.324900150299072
Epoch 490, val loss: 1.25596022605896
Epoch 500, training loss: 633.2837524414062 = 1.0987567901611328 + 100.0 * 6.321849822998047
Epoch 500, val loss: 1.2401264905929565
Epoch 510, training loss: 633.2788696289062 = 1.076519250869751 + 100.0 * 6.322023391723633
Epoch 510, val loss: 1.2248319387435913
Epoch 520, training loss: 632.77099609375 = 1.0549505949020386 + 100.0 * 6.317160606384277
Epoch 520, val loss: 1.2102283239364624
Epoch 530, training loss: 632.5454711914062 = 1.033986210823059 + 100.0 * 6.315114974975586
Epoch 530, val loss: 1.1964834928512573
Epoch 540, training loss: 632.5406494140625 = 1.0136972665786743 + 100.0 * 6.315269947052002
Epoch 540, val loss: 1.183418869972229
Epoch 550, training loss: 632.537353515625 = 0.9940733909606934 + 100.0 * 6.315432548522949
Epoch 550, val loss: 1.171230435371399
Epoch 560, training loss: 631.8721313476562 = 0.9748795628547668 + 100.0 * 6.3089728355407715
Epoch 560, val loss: 1.1595383882522583
Epoch 570, training loss: 631.7688598632812 = 0.9563405513763428 + 100.0 * 6.308125019073486
Epoch 570, val loss: 1.1485366821289062
Epoch 580, training loss: 631.846923828125 = 0.9384058713912964 + 100.0 * 6.309085369110107
Epoch 580, val loss: 1.1382720470428467
Epoch 590, training loss: 631.3572387695312 = 0.9211151599884033 + 100.0 * 6.304360866546631
Epoch 590, val loss: 1.128769874572754
Epoch 600, training loss: 631.5989990234375 = 0.9042487740516663 + 100.0 * 6.306947708129883
Epoch 600, val loss: 1.1198647022247314
Epoch 610, training loss: 631.0567016601562 = 0.8877073526382446 + 100.0 * 6.301690101623535
Epoch 610, val loss: 1.1110037565231323
Epoch 620, training loss: 630.7652587890625 = 0.8716896176338196 + 100.0 * 6.298935413360596
Epoch 620, val loss: 1.1030763387680054
Epoch 630, training loss: 630.7874755859375 = 0.8562153577804565 + 100.0 * 6.299312591552734
Epoch 630, val loss: 1.0958921909332275
Epoch 640, training loss: 630.5859985351562 = 0.8407741785049438 + 100.0 * 6.297452449798584
Epoch 640, val loss: 1.0885759592056274
Epoch 650, training loss: 630.4207763671875 = 0.8258777260780334 + 100.0 * 6.2959489822387695
Epoch 650, val loss: 1.0821311473846436
Epoch 660, training loss: 630.1980590820312 = 0.8112291097640991 + 100.0 * 6.293868541717529
Epoch 660, val loss: 1.0761271715164185
Epoch 670, training loss: 630.0618286132812 = 0.7969833612442017 + 100.0 * 6.2926483154296875
Epoch 670, val loss: 1.0705137252807617
Epoch 680, training loss: 629.985107421875 = 0.7827997207641602 + 100.0 * 6.292023181915283
Epoch 680, val loss: 1.0656362771987915
Epoch 690, training loss: 629.7755737304688 = 0.7689332962036133 + 100.0 * 6.290066242218018
Epoch 690, val loss: 1.0602611303329468
Epoch 700, training loss: 629.5947875976562 = 0.7553687691688538 + 100.0 * 6.288394451141357
Epoch 700, val loss: 1.0558351278305054
Epoch 710, training loss: 629.6988525390625 = 0.742133378982544 + 100.0 * 6.289567470550537
Epoch 710, val loss: 1.0517863035202026
Epoch 720, training loss: 629.537353515625 = 0.7289862632751465 + 100.0 * 6.288083553314209
Epoch 720, val loss: 1.0480034351348877
Epoch 730, training loss: 629.3735961914062 = 0.7160359621047974 + 100.0 * 6.2865753173828125
Epoch 730, val loss: 1.0444403886795044
Epoch 740, training loss: 629.094482421875 = 0.7033771872520447 + 100.0 * 6.283911228179932
Epoch 740, val loss: 1.041544795036316
Epoch 750, training loss: 629.447998046875 = 0.6909250020980835 + 100.0 * 6.287570953369141
Epoch 750, val loss: 1.0388678312301636
Epoch 760, training loss: 628.9464721679688 = 0.6785232424736023 + 100.0 * 6.282679557800293
Epoch 760, val loss: 1.0357643365859985
Epoch 770, training loss: 629.03955078125 = 0.6663916110992432 + 100.0 * 6.283731460571289
Epoch 770, val loss: 1.033492922782898
Epoch 780, training loss: 628.6580200195312 = 0.6544252038002014 + 100.0 * 6.280035972595215
Epoch 780, val loss: 1.0314805507659912
Epoch 790, training loss: 628.4814453125 = 0.642615795135498 + 100.0 * 6.278388023376465
Epoch 790, val loss: 1.029761791229248
Epoch 800, training loss: 629.0921020507812 = 0.6310338973999023 + 100.0 * 6.284610748291016
Epoch 800, val loss: 1.0282247066497803
Epoch 810, training loss: 628.5083618164062 = 0.6193254590034485 + 100.0 * 6.278890132904053
Epoch 810, val loss: 1.0266999006271362
Epoch 820, training loss: 628.176025390625 = 0.6079611778259277 + 100.0 * 6.2756805419921875
Epoch 820, val loss: 1.025718331336975
Epoch 830, training loss: 628.0287475585938 = 0.5967470407485962 + 100.0 * 6.274320125579834
Epoch 830, val loss: 1.0248609781265259
Epoch 840, training loss: 628.08642578125 = 0.5856581330299377 + 100.0 * 6.275008201599121
Epoch 840, val loss: 1.0243139266967773
Epoch 850, training loss: 628.3323974609375 = 0.5745764970779419 + 100.0 * 6.277577877044678
Epoch 850, val loss: 1.0232266187667847
Epoch 860, training loss: 628.3040161132812 = 0.5635135173797607 + 100.0 * 6.27740478515625
Epoch 860, val loss: 1.0234675407409668
Epoch 870, training loss: 627.6183471679688 = 0.5526077151298523 + 100.0 * 6.270657539367676
Epoch 870, val loss: 1.0226527452468872
Epoch 880, training loss: 627.5559692382812 = 0.5419005155563354 + 100.0 * 6.270141124725342
Epoch 880, val loss: 1.022540807723999
Epoch 890, training loss: 627.8551635742188 = 0.5313487648963928 + 100.0 * 6.273237705230713
Epoch 890, val loss: 1.0225030183792114
Epoch 900, training loss: 627.4954223632812 = 0.5208784341812134 + 100.0 * 6.269745349884033
Epoch 900, val loss: 1.0233402252197266
Epoch 910, training loss: 627.6387329101562 = 0.5103817582130432 + 100.0 * 6.2712836265563965
Epoch 910, val loss: 1.0232244729995728
Epoch 920, training loss: 627.395263671875 = 0.5001108050346375 + 100.0 * 6.268951416015625
Epoch 920, val loss: 1.0239428281784058
Epoch 930, training loss: 627.212890625 = 0.48995915055274963 + 100.0 * 6.2672295570373535
Epoch 930, val loss: 1.0246505737304688
Epoch 940, training loss: 626.9692993164062 = 0.4798612892627716 + 100.0 * 6.264894485473633
Epoch 940, val loss: 1.025347352027893
Epoch 950, training loss: 627.03955078125 = 0.46994975209236145 + 100.0 * 6.265695571899414
Epoch 950, val loss: 1.026363492012024
Epoch 960, training loss: 626.9939575195312 = 0.46015265583992004 + 100.0 * 6.265337944030762
Epoch 960, val loss: 1.0273871421813965
Epoch 970, training loss: 627.1273803710938 = 0.45045042037963867 + 100.0 * 6.2667694091796875
Epoch 970, val loss: 1.0285444259643555
Epoch 980, training loss: 626.679443359375 = 0.4407913088798523 + 100.0 * 6.262386322021484
Epoch 980, val loss: 1.0298640727996826
Epoch 990, training loss: 626.5436401367188 = 0.43141093850135803 + 100.0 * 6.261122226715088
Epoch 990, val loss: 1.0311992168426514
Epoch 1000, training loss: 626.9525146484375 = 0.4222111999988556 + 100.0 * 6.265302658081055
Epoch 1000, val loss: 1.0327104330062866
Epoch 1010, training loss: 626.4807739257812 = 0.4130001962184906 + 100.0 * 6.260677337646484
Epoch 1010, val loss: 1.0341497659683228
Epoch 1020, training loss: 626.4688110351562 = 0.40399375557899475 + 100.0 * 6.260648250579834
Epoch 1020, val loss: 1.0357121229171753
Epoch 1030, training loss: 626.783203125 = 0.3951965272426605 + 100.0 * 6.263880252838135
Epoch 1030, val loss: 1.0372647047042847
Epoch 1040, training loss: 626.2537841796875 = 0.3865460753440857 + 100.0 * 6.258672714233398
Epoch 1040, val loss: 1.0387730598449707
Epoch 1050, training loss: 626.518310546875 = 0.3780611455440521 + 100.0 * 6.261402606964111
Epoch 1050, val loss: 1.0405333042144775
Epoch 1060, training loss: 626.0266723632812 = 0.3696434497833252 + 100.0 * 6.256570339202881
Epoch 1060, val loss: 1.0425597429275513
Epoch 1070, training loss: 625.9605712890625 = 0.36147215962409973 + 100.0 * 6.255991458892822
Epoch 1070, val loss: 1.0441919565200806
Epoch 1080, training loss: 626.3346557617188 = 0.35345780849456787 + 100.0 * 6.259811878204346
Epoch 1080, val loss: 1.0460504293441772
Epoch 1090, training loss: 626.0037231445312 = 0.3455887734889984 + 100.0 * 6.2565813064575195
Epoch 1090, val loss: 1.0484551191329956
Epoch 1100, training loss: 625.9000244140625 = 0.3378840386867523 + 100.0 * 6.255621433258057
Epoch 1100, val loss: 1.050163984298706
Epoch 1110, training loss: 625.6904907226562 = 0.33032259345054626 + 100.0 * 6.253601551055908
Epoch 1110, val loss: 1.0525602102279663
Epoch 1120, training loss: 625.843505859375 = 0.3230084180831909 + 100.0 * 6.255205154418945
Epoch 1120, val loss: 1.05503249168396
Epoch 1130, training loss: 625.7259521484375 = 0.3157413899898529 + 100.0 * 6.2541022300720215
Epoch 1130, val loss: 1.0566463470458984
Epoch 1140, training loss: 626.0833129882812 = 0.308673232793808 + 100.0 * 6.257746696472168
Epoch 1140, val loss: 1.059065341949463
Epoch 1150, training loss: 625.404052734375 = 0.3017308712005615 + 100.0 * 6.251022815704346
Epoch 1150, val loss: 1.0613549947738647
Epoch 1160, training loss: 625.3018798828125 = 0.29500812292099 + 100.0 * 6.2500691413879395
Epoch 1160, val loss: 1.0638861656188965
Epoch 1170, training loss: 625.6720581054688 = 0.2884404957294464 + 100.0 * 6.253836154937744
Epoch 1170, val loss: 1.0662791728973389
Epoch 1180, training loss: 625.3480834960938 = 0.2819174528121948 + 100.0 * 6.250661373138428
Epoch 1180, val loss: 1.068184494972229
Epoch 1190, training loss: 625.2123413085938 = 0.2756135165691376 + 100.0 * 6.2493672370910645
Epoch 1190, val loss: 1.0708509683609009
Epoch 1200, training loss: 625.1893920898438 = 0.2694832980632782 + 100.0 * 6.249199390411377
Epoch 1200, val loss: 1.0735236406326294
Epoch 1210, training loss: 625.6771850585938 = 0.26351219415664673 + 100.0 * 6.25413703918457
Epoch 1210, val loss: 1.0759916305541992
Epoch 1220, training loss: 625.236083984375 = 0.25755515694618225 + 100.0 * 6.24978494644165
Epoch 1220, val loss: 1.0782808065414429
Epoch 1230, training loss: 625.207275390625 = 0.2518039643764496 + 100.0 * 6.24955415725708
Epoch 1230, val loss: 1.0810413360595703
Epoch 1240, training loss: 625.1626586914062 = 0.2462116777896881 + 100.0 * 6.249164581298828
Epoch 1240, val loss: 1.083573579788208
Epoch 1250, training loss: 624.822021484375 = 0.24073943495750427 + 100.0 * 6.245812892913818
Epoch 1250, val loss: 1.0863838195800781
Epoch 1260, training loss: 625.0046997070312 = 0.23544082045555115 + 100.0 * 6.247692584991455
Epoch 1260, val loss: 1.0893409252166748
Epoch 1270, training loss: 624.7987060546875 = 0.2301739901304245 + 100.0 * 6.245685577392578
Epoch 1270, val loss: 1.0919269323349
Epoch 1280, training loss: 624.7666015625 = 0.225047767162323 + 100.0 * 6.245415687561035
Epoch 1280, val loss: 1.0947284698486328
Epoch 1290, training loss: 624.7307739257812 = 0.22004415094852448 + 100.0 * 6.245107173919678
Epoch 1290, val loss: 1.0975068807601929
Epoch 1300, training loss: 624.7539672851562 = 0.215178444981575 + 100.0 * 6.245388031005859
Epoch 1300, val loss: 1.1000691652297974
Epoch 1310, training loss: 625.1438598632812 = 0.21040265262126923 + 100.0 * 6.249334812164307
Epoch 1310, val loss: 1.1031118631362915
Epoch 1320, training loss: 624.7882690429688 = 0.20565570890903473 + 100.0 * 6.245826721191406
Epoch 1320, val loss: 1.106033205986023
Epoch 1330, training loss: 624.403076171875 = 0.201028510928154 + 100.0 * 6.242020130157471
Epoch 1330, val loss: 1.1093302965164185
Epoch 1340, training loss: 624.367431640625 = 0.19660772383213043 + 100.0 * 6.241708278656006
Epoch 1340, val loss: 1.1124364137649536
Epoch 1350, training loss: 624.2784423828125 = 0.1922755092382431 + 100.0 * 6.240861892700195
Epoch 1350, val loss: 1.1154850721359253
Epoch 1360, training loss: 625.8124389648438 = 0.18802200257778168 + 100.0 * 6.256243705749512
Epoch 1360, val loss: 1.1179907321929932
Epoch 1370, training loss: 624.299072265625 = 0.18374751508235931 + 100.0 * 6.241153240203857
Epoch 1370, val loss: 1.1212972402572632
Epoch 1380, training loss: 624.2626953125 = 0.17966315150260925 + 100.0 * 6.240829944610596
Epoch 1380, val loss: 1.1250721216201782
Epoch 1390, training loss: 624.0418701171875 = 0.17571960389614105 + 100.0 * 6.238661766052246
Epoch 1390, val loss: 1.128191351890564
Epoch 1400, training loss: 624.5360717773438 = 0.17191758751869202 + 100.0 * 6.243641376495361
Epoch 1400, val loss: 1.1317331790924072
Epoch 1410, training loss: 623.9559326171875 = 0.16803157329559326 + 100.0 * 6.237879276275635
Epoch 1410, val loss: 1.1347943544387817
Epoch 1420, training loss: 623.8988037109375 = 0.1642920970916748 + 100.0 * 6.237344741821289
Epoch 1420, val loss: 1.1383675336837769
Epoch 1430, training loss: 623.975830078125 = 0.1606927216053009 + 100.0 * 6.2381510734558105
Epoch 1430, val loss: 1.1420568227767944
Epoch 1440, training loss: 624.6026000976562 = 0.15713460743427277 + 100.0 * 6.244454860687256
Epoch 1440, val loss: 1.1457970142364502
Epoch 1450, training loss: 624.09814453125 = 0.15362374484539032 + 100.0 * 6.239445209503174
Epoch 1450, val loss: 1.148688793182373
Epoch 1460, training loss: 624.10693359375 = 0.15021318197250366 + 100.0 * 6.239567279815674
Epoch 1460, val loss: 1.15236496925354
Epoch 1470, training loss: 623.8578491210938 = 0.14691641926765442 + 100.0 * 6.237109661102295
Epoch 1470, val loss: 1.1562559604644775
Epoch 1480, training loss: 623.8340454101562 = 0.1436765491962433 + 100.0 * 6.236903667449951
Epoch 1480, val loss: 1.1599359512329102
Epoch 1490, training loss: 623.90478515625 = 0.14052335917949677 + 100.0 * 6.237642765045166
Epoch 1490, val loss: 1.1632611751556396
Epoch 1500, training loss: 623.71142578125 = 0.137411430478096 + 100.0 * 6.2357401847839355
Epoch 1500, val loss: 1.1673681735992432
Epoch 1510, training loss: 623.7842407226562 = 0.13440263271331787 + 100.0 * 6.2364983558654785
Epoch 1510, val loss: 1.1716755628585815
Epoch 1520, training loss: 624.3677978515625 = 0.1314781755208969 + 100.0 * 6.242363452911377
Epoch 1520, val loss: 1.175196647644043
Epoch 1530, training loss: 623.72900390625 = 0.12852123379707336 + 100.0 * 6.23600435256958
Epoch 1530, val loss: 1.1786619424819946
Epoch 1540, training loss: 623.5502319335938 = 0.12572111189365387 + 100.0 * 6.2342448234558105
Epoch 1540, val loss: 1.1832916736602783
Epoch 1550, training loss: 623.6008911132812 = 0.1229945719242096 + 100.0 * 6.234778881072998
Epoch 1550, val loss: 1.186733365058899
Epoch 1560, training loss: 623.9100341796875 = 0.12030312418937683 + 100.0 * 6.2378973960876465
Epoch 1560, val loss: 1.1906569004058838
Epoch 1570, training loss: 623.5574340820312 = 0.11768708378076553 + 100.0 * 6.2343974113464355
Epoch 1570, val loss: 1.1948448419570923
Epoch 1580, training loss: 623.3390502929688 = 0.11510667204856873 + 100.0 * 6.232239246368408
Epoch 1580, val loss: 1.1989467144012451
Epoch 1590, training loss: 623.4594116210938 = 0.11264122277498245 + 100.0 * 6.2334675788879395
Epoch 1590, val loss: 1.2030774354934692
Epoch 1600, training loss: 623.9908447265625 = 0.11019955575466156 + 100.0 * 6.238806247711182
Epoch 1600, val loss: 1.207037329673767
Epoch 1610, training loss: 623.3760375976562 = 0.10777080059051514 + 100.0 * 6.232682704925537
Epoch 1610, val loss: 1.2113646268844604
Epoch 1620, training loss: 623.1983642578125 = 0.1054539680480957 + 100.0 * 6.230928897857666
Epoch 1620, val loss: 1.2155743837356567
Epoch 1630, training loss: 623.1303100585938 = 0.10321591794490814 + 100.0 * 6.230270862579346
Epoch 1630, val loss: 1.219860553741455
Epoch 1640, training loss: 623.8411254882812 = 0.10103851556777954 + 100.0 * 6.237401008605957
Epoch 1640, val loss: 1.2242748737335205
Epoch 1650, training loss: 623.1553344726562 = 0.09886873513460159 + 100.0 * 6.230564594268799
Epoch 1650, val loss: 1.2283778190612793
Epoch 1660, training loss: 623.043212890625 = 0.09676429629325867 + 100.0 * 6.229464530944824
Epoch 1660, val loss: 1.2329504489898682
Epoch 1670, training loss: 623.1879272460938 = 0.09474298357963562 + 100.0 * 6.230931758880615
Epoch 1670, val loss: 1.2375022172927856
Epoch 1680, training loss: 623.075439453125 = 0.09275287389755249 + 100.0 * 6.229826927185059
Epoch 1680, val loss: 1.2416239976882935
Epoch 1690, training loss: 622.967041015625 = 0.09080788493156433 + 100.0 * 6.228762149810791
Epoch 1690, val loss: 1.245664119720459
Epoch 1700, training loss: 623.68798828125 = 0.08893635869026184 + 100.0 * 6.235990524291992
Epoch 1700, val loss: 1.2493160963058472
Epoch 1710, training loss: 623.4298706054688 = 0.08705250918865204 + 100.0 * 6.233428478240967
Epoch 1710, val loss: 1.2547937631607056
Epoch 1720, training loss: 623.0076904296875 = 0.08519624173641205 + 100.0 * 6.229225158691406
Epoch 1720, val loss: 1.2585749626159668
Epoch 1730, training loss: 622.7753295898438 = 0.08345035463571548 + 100.0 * 6.226918697357178
Epoch 1730, val loss: 1.2633088827133179
Epoch 1740, training loss: 622.7156982421875 = 0.08176250755786896 + 100.0 * 6.226338863372803
Epoch 1740, val loss: 1.2678831815719604
Epoch 1750, training loss: 622.7334594726562 = 0.08011419326066971 + 100.0 * 6.22653341293335
Epoch 1750, val loss: 1.2724018096923828
Epoch 1760, training loss: 624.29345703125 = 0.07850859314203262 + 100.0 * 6.242149829864502
Epoch 1760, val loss: 1.2769389152526855
Epoch 1770, training loss: 623.0804443359375 = 0.07686680555343628 + 100.0 * 6.230035781860352
Epoch 1770, val loss: 1.2807198762893677
Epoch 1780, training loss: 622.6158447265625 = 0.07528140395879745 + 100.0 * 6.225405693054199
Epoch 1780, val loss: 1.285732388496399
Epoch 1790, training loss: 622.5958251953125 = 0.07377272099256516 + 100.0 * 6.225220203399658
Epoch 1790, val loss: 1.2903251647949219
Epoch 1800, training loss: 623.1511840820312 = 0.07232601940631866 + 100.0 * 6.230788707733154
Epoch 1800, val loss: 1.295125126838684
Epoch 1810, training loss: 622.6707763671875 = 0.07085040211677551 + 100.0 * 6.225998878479004
Epoch 1810, val loss: 1.2984931468963623
Epoch 1820, training loss: 622.6432495117188 = 0.06941712647676468 + 100.0 * 6.225738525390625
Epoch 1820, val loss: 1.303708553314209
Epoch 1830, training loss: 622.9945068359375 = 0.06805212795734406 + 100.0 * 6.229264259338379
Epoch 1830, val loss: 1.3075276613235474
Epoch 1840, training loss: 622.5422973632812 = 0.06670236587524414 + 100.0 * 6.224755764007568
Epoch 1840, val loss: 1.3126685619354248
Epoch 1850, training loss: 622.5740356445312 = 0.06541072577238083 + 100.0 * 6.225086212158203
Epoch 1850, val loss: 1.317007303237915
Epoch 1860, training loss: 622.5243530273438 = 0.06414191424846649 + 100.0 * 6.224602222442627
Epoch 1860, val loss: 1.3215404748916626
Epoch 1870, training loss: 622.9370727539062 = 0.0629064291715622 + 100.0 * 6.22874116897583
Epoch 1870, val loss: 1.3255642652511597
Epoch 1880, training loss: 622.776611328125 = 0.0616617389023304 + 100.0 * 6.227149486541748
Epoch 1880, val loss: 1.33090341091156
Epoch 1890, training loss: 622.3787231445312 = 0.060460612177848816 + 100.0 * 6.223182678222656
Epoch 1890, val loss: 1.3349519968032837
Epoch 1900, training loss: 622.365234375 = 0.05930488556623459 + 100.0 * 6.223059177398682
Epoch 1900, val loss: 1.3398230075836182
Epoch 1910, training loss: 622.9661254882812 = 0.05819104239344597 + 100.0 * 6.229079723358154
Epoch 1910, val loss: 1.3439186811447144
Epoch 1920, training loss: 622.922607421875 = 0.057048067450523376 + 100.0 * 6.2286553382873535
Epoch 1920, val loss: 1.3485766649246216
Epoch 1930, training loss: 622.4993286132812 = 0.055953145027160645 + 100.0 * 6.224433422088623
Epoch 1930, val loss: 1.353000283241272
Epoch 1940, training loss: 622.228515625 = 0.05489175021648407 + 100.0 * 6.221736431121826
Epoch 1940, val loss: 1.357682704925537
Epoch 1950, training loss: 622.1672973632812 = 0.053881678730249405 + 100.0 * 6.221134185791016
Epoch 1950, val loss: 1.3622634410858154
Epoch 1960, training loss: 622.2555541992188 = 0.05289854854345322 + 100.0 * 6.222026348114014
Epoch 1960, val loss: 1.3667131662368774
Epoch 1970, training loss: 622.86962890625 = 0.051927294582128525 + 100.0 * 6.228176593780518
Epoch 1970, val loss: 1.3710732460021973
Epoch 1980, training loss: 622.9942626953125 = 0.05094873905181885 + 100.0 * 6.229433059692383
Epoch 1980, val loss: 1.3752275705337524
Epoch 1990, training loss: 622.3394165039062 = 0.04999716207385063 + 100.0 * 6.222894191741943
Epoch 1990, val loss: 1.380172610282898
Epoch 2000, training loss: 622.121337890625 = 0.049074508249759674 + 100.0 * 6.220722675323486
Epoch 2000, val loss: 1.3848719596862793
Epoch 2010, training loss: 622.085693359375 = 0.048194728791713715 + 100.0 * 6.220375061035156
Epoch 2010, val loss: 1.3893831968307495
Epoch 2020, training loss: 622.7247314453125 = 0.047350648790597916 + 100.0 * 6.226773738861084
Epoch 2020, val loss: 1.3941949605941772
Epoch 2030, training loss: 622.1402587890625 = 0.04647243767976761 + 100.0 * 6.220938205718994
Epoch 2030, val loss: 1.3978896141052246
Epoch 2040, training loss: 621.9797973632812 = 0.045631587505340576 + 100.0 * 6.219341278076172
Epoch 2040, val loss: 1.4025460481643677
Epoch 2050, training loss: 622.0324096679688 = 0.04482875391840935 + 100.0 * 6.219876289367676
Epoch 2050, val loss: 1.4073296785354614
Epoch 2060, training loss: 622.8294067382812 = 0.04405982047319412 + 100.0 * 6.227853298187256
Epoch 2060, val loss: 1.411437749862671
Epoch 2070, training loss: 622.3671875 = 0.043245431035757065 + 100.0 * 6.223238945007324
Epoch 2070, val loss: 1.415270447731018
Epoch 2080, training loss: 622.06298828125 = 0.042476993054151535 + 100.0 * 6.220204830169678
Epoch 2080, val loss: 1.4203252792358398
Epoch 2090, training loss: 621.8997192382812 = 0.04173755645751953 + 100.0 * 6.2185797691345215
Epoch 2090, val loss: 1.4249297380447388
Epoch 2100, training loss: 622.2646484375 = 0.04103473201394081 + 100.0 * 6.222236156463623
Epoch 2100, val loss: 1.4296777248382568
Epoch 2110, training loss: 622.1542358398438 = 0.040313366800546646 + 100.0 * 6.221139430999756
Epoch 2110, val loss: 1.433031439781189
Epoch 2120, training loss: 621.8655395507812 = 0.03961464390158653 + 100.0 * 6.218259334564209
Epoch 2120, val loss: 1.4381150007247925
Epoch 2130, training loss: 621.8213500976562 = 0.038945261389017105 + 100.0 * 6.2178239822387695
Epoch 2130, val loss: 1.4424049854278564
Epoch 2140, training loss: 621.7996826171875 = 0.038299668580293655 + 100.0 * 6.217613220214844
Epoch 2140, val loss: 1.4469834566116333
Epoch 2150, training loss: 622.1710205078125 = 0.037682995200157166 + 100.0 * 6.2213335037231445
Epoch 2150, val loss: 1.4513493776321411
Epoch 2160, training loss: 622.1450805664062 = 0.037040624767541885 + 100.0 * 6.221080303192139
Epoch 2160, val loss: 1.4556609392166138
Epoch 2170, training loss: 621.9595336914062 = 0.03639843687415123 + 100.0 * 6.219231605529785
Epoch 2170, val loss: 1.4594889879226685
Epoch 2180, training loss: 621.7775268554688 = 0.035795148462057114 + 100.0 * 6.2174177169799805
Epoch 2180, val loss: 1.4640201330184937
Epoch 2190, training loss: 621.8790283203125 = 0.03521491587162018 + 100.0 * 6.218438148498535
Epoch 2190, val loss: 1.4679198265075684
Epoch 2200, training loss: 621.88720703125 = 0.034639861434698105 + 100.0 * 6.218525409698486
Epoch 2200, val loss: 1.47231125831604
Epoch 2210, training loss: 621.7470703125 = 0.03408185765147209 + 100.0 * 6.217130184173584
Epoch 2210, val loss: 1.477089285850525
Epoch 2220, training loss: 621.806396484375 = 0.033537961542606354 + 100.0 * 6.217729091644287
Epoch 2220, val loss: 1.4811782836914062
Epoch 2230, training loss: 621.9654541015625 = 0.03300018236041069 + 100.0 * 6.219324588775635
Epoch 2230, val loss: 1.4850698709487915
Epoch 2240, training loss: 622.2664794921875 = 0.03246891498565674 + 100.0 * 6.222340106964111
Epoch 2240, val loss: 1.4892776012420654
Epoch 2250, training loss: 621.6722412109375 = 0.03194815292954445 + 100.0 * 6.216403484344482
Epoch 2250, val loss: 1.4933592081069946
Epoch 2260, training loss: 621.5618286132812 = 0.03145034238696098 + 100.0 * 6.215303897857666
Epoch 2260, val loss: 1.4978095293045044
Epoch 2270, training loss: 621.67529296875 = 0.030973102897405624 + 100.0 * 6.2164435386657715
Epoch 2270, val loss: 1.5017844438552856
Epoch 2280, training loss: 622.0955200195312 = 0.030490582808852196 + 100.0 * 6.2206501960754395
Epoch 2280, val loss: 1.505926251411438
Epoch 2290, training loss: 621.6655883789062 = 0.029997974634170532 + 100.0 * 6.21635627746582
Epoch 2290, val loss: 1.5101391077041626
Epoch 2300, training loss: 621.5789794921875 = 0.029542604461312294 + 100.0 * 6.215494155883789
Epoch 2300, val loss: 1.5143729448318481
Epoch 2310, training loss: 621.7440185546875 = 0.029099974781274796 + 100.0 * 6.217148780822754
Epoch 2310, val loss: 1.5183802843093872
Epoch 2320, training loss: 621.5930786132812 = 0.028646573424339294 + 100.0 * 6.215644359588623
Epoch 2320, val loss: 1.5219297409057617
Epoch 2330, training loss: 621.5709838867188 = 0.028219278901815414 + 100.0 * 6.215427875518799
Epoch 2330, val loss: 1.5257939100265503
Epoch 2340, training loss: 621.6044921875 = 0.027805885300040245 + 100.0 * 6.215766906738281
Epoch 2340, val loss: 1.530299425125122
Epoch 2350, training loss: 621.8972778320312 = 0.027393754571676254 + 100.0 * 6.218698501586914
Epoch 2350, val loss: 1.534224271774292
Epoch 2360, training loss: 621.5680541992188 = 0.026970239356160164 + 100.0 * 6.2154107093811035
Epoch 2360, val loss: 1.5380516052246094
Epoch 2370, training loss: 621.3881225585938 = 0.02657024748623371 + 100.0 * 6.213615894317627
Epoch 2370, val loss: 1.541746973991394
Epoch 2380, training loss: 621.3404541015625 = 0.026186926290392876 + 100.0 * 6.2131428718566895
Epoch 2380, val loss: 1.5458002090454102
Epoch 2390, training loss: 621.4166870117188 = 0.02581658773124218 + 100.0 * 6.213908672332764
Epoch 2390, val loss: 1.5496032238006592
Epoch 2400, training loss: 622.454345703125 = 0.0254472978413105 + 100.0 * 6.2242889404296875
Epoch 2400, val loss: 1.5529265403747559
Epoch 2410, training loss: 621.555908203125 = 0.025067318230867386 + 100.0 * 6.21530818939209
Epoch 2410, val loss: 1.5571386814117432
Epoch 2420, training loss: 621.3011474609375 = 0.024698322638869286 + 100.0 * 6.212764263153076
Epoch 2420, val loss: 1.5608618259429932
Epoch 2430, training loss: 621.21533203125 = 0.024356499314308167 + 100.0 * 6.211909770965576
Epoch 2430, val loss: 1.565012812614441
Epoch 2440, training loss: 621.2731323242188 = 0.024020811542868614 + 100.0 * 6.212491035461426
Epoch 2440, val loss: 1.568618893623352
Epoch 2450, training loss: 622.0623779296875 = 0.02369137853384018 + 100.0 * 6.220386981964111
Epoch 2450, val loss: 1.5717995166778564
Epoch 2460, training loss: 621.419921875 = 0.023344263434410095 + 100.0 * 6.213966369628906
Epoch 2460, val loss: 1.5755161046981812
Epoch 2470, training loss: 621.2144775390625 = 0.0230130385607481 + 100.0 * 6.211914539337158
Epoch 2470, val loss: 1.5798014402389526
Epoch 2480, training loss: 621.3242797851562 = 0.022701477631926537 + 100.0 * 6.213016033172607
Epoch 2480, val loss: 1.5831996202468872
Epoch 2490, training loss: 621.7011108398438 = 0.022395268082618713 + 100.0 * 6.216786861419678
Epoch 2490, val loss: 1.5866154432296753
Epoch 2500, training loss: 621.3890380859375 = 0.022096950560808182 + 100.0 * 6.213669300079346
Epoch 2500, val loss: 1.5911130905151367
Epoch 2510, training loss: 621.1407470703125 = 0.021789895370602608 + 100.0 * 6.2111897468566895
Epoch 2510, val loss: 1.594585657119751
Epoch 2520, training loss: 621.1893920898438 = 0.02150147780776024 + 100.0 * 6.211678981781006
Epoch 2520, val loss: 1.5981570482254028
Epoch 2530, training loss: 621.5846557617188 = 0.021225547417998314 + 100.0 * 6.215633869171143
Epoch 2530, val loss: 1.602028489112854
Epoch 2540, training loss: 621.0751953125 = 0.020937081426382065 + 100.0 * 6.210542678833008
Epoch 2540, val loss: 1.6050220727920532
Epoch 2550, training loss: 621.3843383789062 = 0.020671598613262177 + 100.0 * 6.21363639831543
Epoch 2550, val loss: 1.6089779138565063
Epoch 2560, training loss: 621.2391967773438 = 0.020393408834934235 + 100.0 * 6.212188243865967
Epoch 2560, val loss: 1.6123046875
Epoch 2570, training loss: 621.2063598632812 = 0.02012873999774456 + 100.0 * 6.211862087249756
Epoch 2570, val loss: 1.6151505708694458
Epoch 2580, training loss: 621.4625244140625 = 0.019872836768627167 + 100.0 * 6.214426517486572
Epoch 2580, val loss: 1.6189826726913452
Epoch 2590, training loss: 621.3588256835938 = 0.019613005220890045 + 100.0 * 6.21339225769043
Epoch 2590, val loss: 1.622981071472168
Epoch 2600, training loss: 620.988525390625 = 0.019357670098543167 + 100.0 * 6.209692001342773
Epoch 2600, val loss: 1.6256704330444336
Epoch 2610, training loss: 620.9625854492188 = 0.01911257579922676 + 100.0 * 6.209434509277344
Epoch 2610, val loss: 1.629217505455017
Epoch 2620, training loss: 621.0375366210938 = 0.018878357484936714 + 100.0 * 6.21018648147583
Epoch 2620, val loss: 1.632682204246521
Epoch 2630, training loss: 621.3986206054688 = 0.018648507073521614 + 100.0 * 6.213799476623535
Epoch 2630, val loss: 1.6356253623962402
Epoch 2640, training loss: 621.1023559570312 = 0.018411219120025635 + 100.0 * 6.21083927154541
Epoch 2640, val loss: 1.6390914916992188
Epoch 2650, training loss: 621.0156860351562 = 0.018180081620812416 + 100.0 * 6.209975242614746
Epoch 2650, val loss: 1.6426432132720947
Epoch 2660, training loss: 621.6189575195312 = 0.01796012930572033 + 100.0 * 6.216010093688965
Epoch 2660, val loss: 1.645870566368103
Epoch 2670, training loss: 621.0880737304688 = 0.017737405374646187 + 100.0 * 6.210702896118164
Epoch 2670, val loss: 1.6490870714187622
Epoch 2680, training loss: 621.2183837890625 = 0.017521508038043976 + 100.0 * 6.212008953094482
Epoch 2680, val loss: 1.6526519060134888
Epoch 2690, training loss: 620.9378662109375 = 0.017307685688138008 + 100.0 * 6.209205627441406
Epoch 2690, val loss: 1.6558805704116821
Epoch 2700, training loss: 620.9699096679688 = 0.017106302082538605 + 100.0 * 6.209527969360352
Epoch 2700, val loss: 1.6591932773590088
Epoch 2710, training loss: 621.3906860351562 = 0.016913169994950294 + 100.0 * 6.213737964630127
Epoch 2710, val loss: 1.6623972654342651
Epoch 2720, training loss: 620.9356079101562 = 0.016702350229024887 + 100.0 * 6.209188938140869
Epoch 2720, val loss: 1.6648870706558228
Epoch 2730, training loss: 620.7875366210938 = 0.01650363951921463 + 100.0 * 6.2077107429504395
Epoch 2730, val loss: 1.6682692766189575
Epoch 2740, training loss: 620.847900390625 = 0.016315076500177383 + 100.0 * 6.208315849304199
Epoch 2740, val loss: 1.671384334564209
Epoch 2750, training loss: 621.5675048828125 = 0.01613551937043667 + 100.0 * 6.215513706207275
Epoch 2750, val loss: 1.6734428405761719
Epoch 2760, training loss: 621.1239624023438 = 0.015935339033603668 + 100.0 * 6.211080074310303
Epoch 2760, val loss: 1.67752206325531
Epoch 2770, training loss: 620.8355712890625 = 0.015751123428344727 + 100.0 * 6.208198070526123
Epoch 2770, val loss: 1.6801172494888306
Epoch 2780, training loss: 620.7246704101562 = 0.015572630800306797 + 100.0 * 6.207090854644775
Epoch 2780, val loss: 1.683854103088379
Epoch 2790, training loss: 621.0400390625 = 0.015406280755996704 + 100.0 * 6.210246562957764
Epoch 2790, val loss: 1.6869125366210938
Epoch 2800, training loss: 620.8977661132812 = 0.015227112919092178 + 100.0 * 6.208825588226318
Epoch 2800, val loss: 1.6892403364181519
Epoch 2810, training loss: 620.7694091796875 = 0.015053498558700085 + 100.0 * 6.20754337310791
Epoch 2810, val loss: 1.692221760749817
Epoch 2820, training loss: 620.8654174804688 = 0.014888070523738861 + 100.0 * 6.208505153656006
Epoch 2820, val loss: 1.695502519607544
Epoch 2830, training loss: 620.960205078125 = 0.01472556497901678 + 100.0 * 6.2094550132751465
Epoch 2830, val loss: 1.6979795694351196
Epoch 2840, training loss: 620.6444091796875 = 0.014564157463610172 + 100.0 * 6.206298351287842
Epoch 2840, val loss: 1.7014219760894775
Epoch 2850, training loss: 621.1199951171875 = 0.014417311176657677 + 100.0 * 6.211055755615234
Epoch 2850, val loss: 1.7044562101364136
Epoch 2860, training loss: 620.8349609375 = 0.014251532033085823 + 100.0 * 6.208207607269287
Epoch 2860, val loss: 1.7069162130355835
Epoch 2870, training loss: 620.626220703125 = 0.014095859602093697 + 100.0 * 6.20612096786499
Epoch 2870, val loss: 1.7098139524459839
Epoch 2880, training loss: 620.65673828125 = 0.013951265253126621 + 100.0 * 6.206427574157715
Epoch 2880, val loss: 1.71256422996521
Epoch 2890, training loss: 620.9066772460938 = 0.013810129836201668 + 100.0 * 6.20892858505249
Epoch 2890, val loss: 1.7151055335998535
Epoch 2900, training loss: 620.9605102539062 = 0.013657890260219574 + 100.0 * 6.209468364715576
Epoch 2900, val loss: 1.717834711074829
Epoch 2910, training loss: 620.714111328125 = 0.013507436029613018 + 100.0 * 6.207006454467773
Epoch 2910, val loss: 1.7203785181045532
Epoch 2920, training loss: 620.5694580078125 = 0.013364397920668125 + 100.0 * 6.20556116104126
Epoch 2920, val loss: 1.7236497402191162
Epoch 2930, training loss: 620.5061645507812 = 0.013228227384388447 + 100.0 * 6.204929351806641
Epoch 2930, val loss: 1.7263749837875366
Epoch 2940, training loss: 620.5051879882812 = 0.013095442205667496 + 100.0 * 6.204920768737793
Epoch 2940, val loss: 1.7292423248291016
Epoch 2950, training loss: 620.95556640625 = 0.012969955801963806 + 100.0 * 6.209426403045654
Epoch 2950, val loss: 1.7324494123458862
Epoch 2960, training loss: 620.8552856445312 = 0.012833725661039352 + 100.0 * 6.2084245681762695
Epoch 2960, val loss: 1.7338618040084839
Epoch 2970, training loss: 620.53955078125 = 0.012696413323283195 + 100.0 * 6.205268383026123
Epoch 2970, val loss: 1.7364082336425781
Epoch 2980, training loss: 620.5479736328125 = 0.012567681260406971 + 100.0 * 6.2053542137146
Epoch 2980, val loss: 1.739216923713684
Epoch 2990, training loss: 620.79150390625 = 0.01244613528251648 + 100.0 * 6.207790374755859
Epoch 2990, val loss: 1.7421016693115234
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 861.6463012695312 = 1.9619390964508057 + 100.0 * 8.596843719482422
Epoch 0, val loss: 1.963708519935608
Epoch 10, training loss: 861.562744140625 = 1.9532793760299683 + 100.0 * 8.596094131469727
Epoch 10, val loss: 1.9544718265533447
Epoch 20, training loss: 861.0457153320312 = 1.9424470663070679 + 100.0 * 8.591032981872559
Epoch 20, val loss: 1.9428715705871582
Epoch 30, training loss: 857.8035278320312 = 1.9280277490615845 + 100.0 * 8.558754920959473
Epoch 30, val loss: 1.9275306463241577
Epoch 40, training loss: 841.7662353515625 = 1.9108548164367676 + 100.0 * 8.398553848266602
Epoch 40, val loss: 1.9102314710617065
Epoch 50, training loss: 798.7798461914062 = 1.8906151056289673 + 100.0 * 7.9688920974731445
Epoch 50, val loss: 1.889631748199463
Epoch 60, training loss: 751.1283569335938 = 1.8745745420455933 + 100.0 * 7.492537975311279
Epoch 60, val loss: 1.8750747442245483
Epoch 70, training loss: 714.5980224609375 = 1.86335027217865 + 100.0 * 7.127346992492676
Epoch 70, val loss: 1.8643462657928467
Epoch 80, training loss: 699.489990234375 = 1.853525161743164 + 100.0 * 6.976364612579346
Epoch 80, val loss: 1.854824423789978
Epoch 90, training loss: 688.3818359375 = 1.842740774154663 + 100.0 * 6.865391254425049
Epoch 90, val loss: 1.8443714380264282
Epoch 100, training loss: 679.8569946289062 = 1.8322722911834717 + 100.0 * 6.780247211456299
Epoch 100, val loss: 1.8344430923461914
Epoch 110, training loss: 673.945068359375 = 1.8225675821304321 + 100.0 * 6.721225261688232
Epoch 110, val loss: 1.8252390623092651
Epoch 120, training loss: 669.0445556640625 = 1.8135876655578613 + 100.0 * 6.672309875488281
Epoch 120, val loss: 1.8165514469146729
Epoch 130, training loss: 665.085693359375 = 1.8048394918441772 + 100.0 * 6.632808685302734
Epoch 130, val loss: 1.8079050779342651
Epoch 140, training loss: 661.85107421875 = 1.7959779500961304 + 100.0 * 6.600551128387451
Epoch 140, val loss: 1.7991077899932861
Epoch 150, training loss: 659.1022338867188 = 1.7871243953704834 + 100.0 * 6.573151111602783
Epoch 150, val loss: 1.7904478311538696
Epoch 160, training loss: 656.9151000976562 = 1.7781338691711426 + 100.0 * 6.551369667053223
Epoch 160, val loss: 1.7816935777664185
Epoch 170, training loss: 654.6838989257812 = 1.7688257694244385 + 100.0 * 6.52915096282959
Epoch 170, val loss: 1.7727210521697998
Epoch 180, training loss: 652.7977905273438 = 1.7591490745544434 + 100.0 * 6.5103864669799805
Epoch 180, val loss: 1.7635048627853394
Epoch 190, training loss: 651.28662109375 = 1.748856782913208 + 100.0 * 6.495378017425537
Epoch 190, val loss: 1.7538890838623047
Epoch 200, training loss: 649.8746948242188 = 1.7379405498504639 + 100.0 * 6.481368064880371
Epoch 200, val loss: 1.7438217401504517
Epoch 210, training loss: 648.6166381835938 = 1.7263550758361816 + 100.0 * 6.468902587890625
Epoch 210, val loss: 1.7331836223602295
Epoch 220, training loss: 647.302490234375 = 1.714021921157837 + 100.0 * 6.4558844566345215
Epoch 220, val loss: 1.7220234870910645
Epoch 230, training loss: 646.3392333984375 = 1.7008875608444214 + 100.0 * 6.446383953094482
Epoch 230, val loss: 1.7102543115615845
Epoch 240, training loss: 645.2360229492188 = 1.6868433952331543 + 100.0 * 6.435492038726807
Epoch 240, val loss: 1.6977874040603638
Epoch 250, training loss: 644.4813842773438 = 1.6718956232070923 + 100.0 * 6.428094863891602
Epoch 250, val loss: 1.6846152544021606
Epoch 260, training loss: 643.4409790039062 = 1.6559170484542847 + 100.0 * 6.417850494384766
Epoch 260, val loss: 1.6706700325012207
Epoch 270, training loss: 642.6484375 = 1.6389939785003662 + 100.0 * 6.410094738006592
Epoch 270, val loss: 1.6559789180755615
Epoch 280, training loss: 641.9907836914062 = 1.6209789514541626 + 100.0 * 6.403697967529297
Epoch 280, val loss: 1.6405123472213745
Epoch 290, training loss: 641.2357788085938 = 1.6019606590270996 + 100.0 * 6.396337985992432
Epoch 290, val loss: 1.624182939529419
Epoch 300, training loss: 640.5504760742188 = 1.581912875175476 + 100.0 * 6.38968563079834
Epoch 300, val loss: 1.6071420907974243
Epoch 310, training loss: 640.2916870117188 = 1.5609521865844727 + 100.0 * 6.387307167053223
Epoch 310, val loss: 1.5893293619155884
Epoch 320, training loss: 639.5535888671875 = 1.5390621423721313 + 100.0 * 6.380145072937012
Epoch 320, val loss: 1.5707429647445679
Epoch 330, training loss: 638.9237060546875 = 1.516289472579956 + 100.0 * 6.3740739822387695
Epoch 330, val loss: 1.5516338348388672
Epoch 340, training loss: 638.9107666015625 = 1.4927326440811157 + 100.0 * 6.374180316925049
Epoch 340, val loss: 1.5318832397460938
Epoch 350, training loss: 637.9970703125 = 1.4684555530548096 + 100.0 * 6.365286350250244
Epoch 350, val loss: 1.5116362571716309
Epoch 360, training loss: 637.4417114257812 = 1.443503975868225 + 100.0 * 6.359982013702393
Epoch 360, val loss: 1.490895390510559
Epoch 370, training loss: 637.0003662109375 = 1.4180079698562622 + 100.0 * 6.355823516845703
Epoch 370, val loss: 1.469731330871582
Epoch 380, training loss: 637.373779296875 = 1.3919364213943481 + 100.0 * 6.359818458557129
Epoch 380, val loss: 1.4481483697891235
Epoch 390, training loss: 636.5032348632812 = 1.3653178215026855 + 100.0 * 6.35137939453125
Epoch 390, val loss: 1.4264500141143799
Epoch 400, training loss: 635.8973388671875 = 1.3383674621582031 + 100.0 * 6.345589637756348
Epoch 400, val loss: 1.4044188261032104
Epoch 410, training loss: 635.51318359375 = 1.311179757118225 + 100.0 * 6.342020511627197
Epoch 410, val loss: 1.382315993309021
Epoch 420, training loss: 635.4432983398438 = 1.2837214469909668 + 100.0 * 6.3415961265563965
Epoch 420, val loss: 1.3601415157318115
Epoch 430, training loss: 634.9999389648438 = 1.256181240081787 + 100.0 * 6.337437629699707
Epoch 430, val loss: 1.337930679321289
Epoch 440, training loss: 634.5265502929688 = 1.228695034980774 + 100.0 * 6.33297872543335
Epoch 440, val loss: 1.3159966468811035
Epoch 450, training loss: 634.6409912109375 = 1.2013112306594849 + 100.0 * 6.334396839141846
Epoch 450, val loss: 1.2942930459976196
Epoch 460, training loss: 634.153076171875 = 1.1740620136260986 + 100.0 * 6.329790115356445
Epoch 460, val loss: 1.2726527452468872
Epoch 470, training loss: 633.8981323242188 = 1.1471118927001953 + 100.0 * 6.327510356903076
Epoch 470, val loss: 1.251672387123108
Epoch 480, training loss: 633.4080810546875 = 1.1207436323165894 + 100.0 * 6.322873592376709
Epoch 480, val loss: 1.2311816215515137
Epoch 490, training loss: 633.0809936523438 = 1.0948151350021362 + 100.0 * 6.319862365722656
Epoch 490, val loss: 1.211205005645752
Epoch 500, training loss: 633.6279907226562 = 1.0695140361785889 + 100.0 * 6.325584888458252
Epoch 500, val loss: 1.1921271085739136
Epoch 510, training loss: 632.7642211914062 = 1.0447810888290405 + 100.0 * 6.31719446182251
Epoch 510, val loss: 1.17318856716156
Epoch 520, training loss: 632.383544921875 = 1.020729422569275 + 100.0 * 6.31362771987915
Epoch 520, val loss: 1.1554864645004272
Epoch 530, training loss: 632.1093139648438 = 0.9973939061164856 + 100.0 * 6.311119556427002
Epoch 530, val loss: 1.1385600566864014
Epoch 540, training loss: 632.6687622070312 = 0.9748143553733826 + 100.0 * 6.316939830780029
Epoch 540, val loss: 1.1223605871200562
Epoch 550, training loss: 631.8573608398438 = 0.9528473615646362 + 100.0 * 6.309045314788818
Epoch 550, val loss: 1.1068971157073975
Epoch 560, training loss: 631.5390625 = 0.9315805435180664 + 100.0 * 6.306075096130371
Epoch 560, val loss: 1.0920268297195435
Epoch 570, training loss: 631.3663330078125 = 0.9110296964645386 + 100.0 * 6.304553031921387
Epoch 570, val loss: 1.0780144929885864
Epoch 580, training loss: 631.3431396484375 = 0.8909380435943604 + 100.0 * 6.3045220375061035
Epoch 580, val loss: 1.0648761987686157
Epoch 590, training loss: 631.0294799804688 = 0.8715851902961731 + 100.0 * 6.301578998565674
Epoch 590, val loss: 1.052004337310791
Epoch 600, training loss: 630.7368774414062 = 0.8527682423591614 + 100.0 * 6.2988409996032715
Epoch 600, val loss: 1.0401833057403564
Epoch 610, training loss: 630.5072631835938 = 0.8345521688461304 + 100.0 * 6.296727180480957
Epoch 610, val loss: 1.0288560390472412
Epoch 620, training loss: 630.311767578125 = 0.8168568015098572 + 100.0 * 6.294949054718018
Epoch 620, val loss: 1.018316626548767
Epoch 630, training loss: 630.89404296875 = 0.7997066378593445 + 100.0 * 6.300943374633789
Epoch 630, val loss: 1.0082204341888428
Epoch 640, training loss: 630.8842163085938 = 0.7827154397964478 + 100.0 * 6.3010149002075195
Epoch 640, val loss: 0.9990392327308655
Epoch 650, training loss: 630.0445556640625 = 0.7662546038627625 + 100.0 * 6.292783260345459
Epoch 650, val loss: 0.9899501204490662
Epoch 660, training loss: 629.7377319335938 = 0.7503440976142883 + 100.0 * 6.2898736000061035
Epoch 660, val loss: 0.9816542863845825
Epoch 670, training loss: 629.5294799804688 = 0.7349047660827637 + 100.0 * 6.287945747375488
Epoch 670, val loss: 0.9739171862602234
Epoch 680, training loss: 629.4502563476562 = 0.7198624610900879 + 100.0 * 6.287303924560547
Epoch 680, val loss: 0.9666761755943298
Epoch 690, training loss: 629.3827514648438 = 0.7051256895065308 + 100.0 * 6.286776065826416
Epoch 690, val loss: 0.9600955247879028
Epoch 700, training loss: 629.1910400390625 = 0.69073885679245 + 100.0 * 6.285003185272217
Epoch 700, val loss: 0.9537696242332458
Epoch 710, training loss: 628.9928588867188 = 0.67673659324646 + 100.0 * 6.283161163330078
Epoch 710, val loss: 0.947913408279419
Epoch 720, training loss: 628.9113159179688 = 0.663155198097229 + 100.0 * 6.282481670379639
Epoch 720, val loss: 0.9426276683807373
Epoch 730, training loss: 628.7589721679688 = 0.6498224139213562 + 100.0 * 6.281091213226318
Epoch 730, val loss: 0.9376956224441528
Epoch 740, training loss: 629.0490112304688 = 0.6367743015289307 + 100.0 * 6.284122467041016
Epoch 740, val loss: 0.9332746863365173
Epoch 750, training loss: 628.5654907226562 = 0.6239615082740784 + 100.0 * 6.279415130615234
Epoch 750, val loss: 0.9288777112960815
Epoch 760, training loss: 628.34716796875 = 0.6114933490753174 + 100.0 * 6.2773566246032715
Epoch 760, val loss: 0.9252052307128906
Epoch 770, training loss: 628.224365234375 = 0.5993216633796692 + 100.0 * 6.27625036239624
Epoch 770, val loss: 0.9218883514404297
Epoch 780, training loss: 629.0790405273438 = 0.5873593091964722 + 100.0 * 6.284916877746582
Epoch 780, val loss: 0.918716311454773
Epoch 790, training loss: 628.0355834960938 = 0.5755709409713745 + 100.0 * 6.274600028991699
Epoch 790, val loss: 0.9160593152046204
Epoch 800, training loss: 627.9017333984375 = 0.5640154480934143 + 100.0 * 6.273376941680908
Epoch 800, val loss: 0.913549542427063
Epoch 810, training loss: 627.7840576171875 = 0.5526795387268066 + 100.0 * 6.272313594818115
Epoch 810, val loss: 0.9114102721214294
Epoch 820, training loss: 628.6060791015625 = 0.5415950417518616 + 100.0 * 6.280645370483398
Epoch 820, val loss: 0.9096803069114685
Epoch 830, training loss: 627.7172241210938 = 0.5305368304252625 + 100.0 * 6.271867275238037
Epoch 830, val loss: 0.908146858215332
Epoch 840, training loss: 627.5026245117188 = 0.5196834802627563 + 100.0 * 6.269829750061035
Epoch 840, val loss: 0.9066977500915527
Epoch 850, training loss: 628.0537109375 = 0.5090368986129761 + 100.0 * 6.275446891784668
Epoch 850, val loss: 0.9056462049484253
Epoch 860, training loss: 627.4248046875 = 0.4985066056251526 + 100.0 * 6.269262790679932
Epoch 860, val loss: 0.9044950008392334
Epoch 870, training loss: 627.1795654296875 = 0.4881373941898346 + 100.0 * 6.266914367675781
Epoch 870, val loss: 0.90400630235672
Epoch 880, training loss: 627.0919799804688 = 0.47795790433883667 + 100.0 * 6.266139984130859
Epoch 880, val loss: 0.9034321904182434
Epoch 890, training loss: 627.7855834960938 = 0.46795448660850525 + 100.0 * 6.273176193237305
Epoch 890, val loss: 0.9034180045127869
Epoch 900, training loss: 627.049072265625 = 0.45791301131248474 + 100.0 * 6.265912055969238
Epoch 900, val loss: 0.9028365015983582
Epoch 910, training loss: 627.0906982421875 = 0.4481395483016968 + 100.0 * 6.266425609588623
Epoch 910, val loss: 0.9030501842498779
Epoch 920, training loss: 626.733154296875 = 0.438556969165802 + 100.0 * 6.262946128845215
Epoch 920, val loss: 0.9032068252563477
Epoch 930, training loss: 626.6228637695312 = 0.429069459438324 + 100.0 * 6.261938095092773
Epoch 930, val loss: 0.9034866690635681
Epoch 940, training loss: 626.6710815429688 = 0.41980499029159546 + 100.0 * 6.262512683868408
Epoch 940, val loss: 0.9041633605957031
Epoch 950, training loss: 626.5944213867188 = 0.4106447994709015 + 100.0 * 6.261837482452393
Epoch 950, val loss: 0.905123233795166
Epoch 960, training loss: 626.4229736328125 = 0.40164411067962646 + 100.0 * 6.2602128982543945
Epoch 960, val loss: 0.9058852791786194
Epoch 970, training loss: 626.5701904296875 = 0.39279255270957947 + 100.0 * 6.261773586273193
Epoch 970, val loss: 0.9075521230697632
Epoch 980, training loss: 626.2503662109375 = 0.3841283321380615 + 100.0 * 6.258662223815918
Epoch 980, val loss: 0.9087917804718018
Epoch 990, training loss: 626.2982788085938 = 0.3756202161312103 + 100.0 * 6.2592267990112305
Epoch 990, val loss: 0.9104026556015015
Epoch 1000, training loss: 626.421142578125 = 0.3673006594181061 + 100.0 * 6.260538101196289
Epoch 1000, val loss: 0.912421464920044
Epoch 1010, training loss: 626.0626831054688 = 0.3590753376483917 + 100.0 * 6.257036209106445
Epoch 1010, val loss: 0.9145309329032898
Epoch 1020, training loss: 625.9644165039062 = 0.35105618834495544 + 100.0 * 6.256133556365967
Epoch 1020, val loss: 0.916858434677124
Epoch 1030, training loss: 626.6072387695312 = 0.3432025611400604 + 100.0 * 6.262640476226807
Epoch 1030, val loss: 0.9192769527435303
Epoch 1040, training loss: 625.8538208007812 = 0.3353928327560425 + 100.0 * 6.255184173583984
Epoch 1040, val loss: 0.9217686057090759
Epoch 1050, training loss: 625.7877197265625 = 0.32786324620246887 + 100.0 * 6.254598617553711
Epoch 1050, val loss: 0.9244041442871094
Epoch 1060, training loss: 625.6692504882812 = 0.320483535528183 + 100.0 * 6.253487586975098
Epoch 1060, val loss: 0.9275237917900085
Epoch 1070, training loss: 626.0408325195312 = 0.31326431035995483 + 100.0 * 6.257275104522705
Epoch 1070, val loss: 0.9305249452590942
Epoch 1080, training loss: 625.8585205078125 = 0.3061739504337311 + 100.0 * 6.255523681640625
Epoch 1080, val loss: 0.9336807727813721
Epoch 1090, training loss: 625.5370483398438 = 0.29919469356536865 + 100.0 * 6.252378463745117
Epoch 1090, val loss: 0.937226414680481
Epoch 1100, training loss: 625.4092407226562 = 0.29241418838500977 + 100.0 * 6.251168251037598
Epoch 1100, val loss: 0.9408687949180603
Epoch 1110, training loss: 625.4794311523438 = 0.2857929468154907 + 100.0 * 6.251936912536621
Epoch 1110, val loss: 0.9446070194244385
Epoch 1120, training loss: 625.1964111328125 = 0.2792743444442749 + 100.0 * 6.249171257019043
Epoch 1120, val loss: 0.9485732316970825
Epoch 1130, training loss: 625.4729614257812 = 0.2729337215423584 + 100.0 * 6.251999855041504
Epoch 1130, val loss: 0.952735960483551
Epoch 1140, training loss: 625.4203491210938 = 0.26668551564216614 + 100.0 * 6.251536846160889
Epoch 1140, val loss: 0.9565026164054871
Epoch 1150, training loss: 625.150634765625 = 0.26053762435913086 + 100.0 * 6.248900890350342
Epoch 1150, val loss: 0.9608671069145203
Epoch 1160, training loss: 625.107666015625 = 0.25457489490509033 + 100.0 * 6.248530864715576
Epoch 1160, val loss: 0.9651013612747192
Epoch 1170, training loss: 625.3613891601562 = 0.2487575262784958 + 100.0 * 6.251126289367676
Epoch 1170, val loss: 0.9695941209793091
Epoch 1180, training loss: 624.878173828125 = 0.24305219948291779 + 100.0 * 6.24635124206543
Epoch 1180, val loss: 0.9743342399597168
Epoch 1190, training loss: 624.8998413085938 = 0.2374950349330902 + 100.0 * 6.246623516082764
Epoch 1190, val loss: 0.9790510535240173
Epoch 1200, training loss: 625.1484985351562 = 0.2320420742034912 + 100.0 * 6.249164581298828
Epoch 1200, val loss: 0.983931839466095
Epoch 1210, training loss: 624.7905883789062 = 0.22669821977615356 + 100.0 * 6.245639324188232
Epoch 1210, val loss: 0.9885639548301697
Epoch 1220, training loss: 624.7017211914062 = 0.22150644659996033 + 100.0 * 6.244802474975586
Epoch 1220, val loss: 0.9936999082565308
Epoch 1230, training loss: 624.929931640625 = 0.21644802391529083 + 100.0 * 6.247135162353516
Epoch 1230, val loss: 0.9988843202590942
Epoch 1240, training loss: 624.8446655273438 = 0.21147237718105316 + 100.0 * 6.246331691741943
Epoch 1240, val loss: 1.0036083459854126
Epoch 1250, training loss: 624.7401123046875 = 0.20658282935619354 + 100.0 * 6.245335102081299
Epoch 1250, val loss: 1.0094624757766724
Epoch 1260, training loss: 624.7643432617188 = 0.20180648565292358 + 100.0 * 6.2456254959106445
Epoch 1260, val loss: 1.0145390033721924
Epoch 1270, training loss: 624.3989868164062 = 0.19718603789806366 + 100.0 * 6.24201774597168
Epoch 1270, val loss: 1.020040512084961
Epoch 1280, training loss: 624.3661499023438 = 0.19268396496772766 + 100.0 * 6.241734504699707
Epoch 1280, val loss: 1.0258700847625732
Epoch 1290, training loss: 624.3932495117188 = 0.18829676508903503 + 100.0 * 6.242049694061279
Epoch 1290, val loss: 1.0312018394470215
Epoch 1300, training loss: 624.7998657226562 = 0.184010311961174 + 100.0 * 6.246158599853516
Epoch 1300, val loss: 1.0370440483093262
Epoch 1310, training loss: 624.44580078125 = 0.17974165081977844 + 100.0 * 6.2426605224609375
Epoch 1310, val loss: 1.04251229763031
Epoch 1320, training loss: 624.2220458984375 = 0.17562523484230042 + 100.0 * 6.240464210510254
Epoch 1320, val loss: 1.0483161211013794
Epoch 1330, training loss: 624.181884765625 = 0.17163775861263275 + 100.0 * 6.240102767944336
Epoch 1330, val loss: 1.0542914867401123
Epoch 1340, training loss: 624.620361328125 = 0.16774408519268036 + 100.0 * 6.244526386260986
Epoch 1340, val loss: 1.0600574016571045
Epoch 1350, training loss: 624.321044921875 = 0.16390985250473022 + 100.0 * 6.241571426391602
Epoch 1350, val loss: 1.065989375114441
Epoch 1360, training loss: 624.0867919921875 = 0.16016046702861786 + 100.0 * 6.239266395568848
Epoch 1360, val loss: 1.0721849203109741
Epoch 1370, training loss: 624.2208862304688 = 0.15653112530708313 + 100.0 * 6.24064302444458
Epoch 1370, val loss: 1.0779870748519897
Epoch 1380, training loss: 624.0419921875 = 0.1529867947101593 + 100.0 * 6.238889694213867
Epoch 1380, val loss: 1.0840345621109009
Epoch 1390, training loss: 623.8718872070312 = 0.1495237648487091 + 100.0 * 6.2372236251831055
Epoch 1390, val loss: 1.090483546257019
Epoch 1400, training loss: 623.81005859375 = 0.14615343511104584 + 100.0 * 6.236639022827148
Epoch 1400, val loss: 1.0965555906295776
Epoch 1410, training loss: 623.9937744140625 = 0.14290425181388855 + 100.0 * 6.238509178161621
Epoch 1410, val loss: 1.1028646230697632
Epoch 1420, training loss: 623.9395751953125 = 0.13966190814971924 + 100.0 * 6.237998962402344
Epoch 1420, val loss: 1.1089503765106201
Epoch 1430, training loss: 623.9827880859375 = 0.13649295270442963 + 100.0 * 6.238462924957275
Epoch 1430, val loss: 1.115153431892395
Epoch 1440, training loss: 623.7261352539062 = 0.13341772556304932 + 100.0 * 6.235927104949951
Epoch 1440, val loss: 1.1217037439346313
Epoch 1450, training loss: 623.6243896484375 = 0.1304677277803421 + 100.0 * 6.234939098358154
Epoch 1450, val loss: 1.1282275915145874
Epoch 1460, training loss: 623.7025756835938 = 0.1275937408208847 + 100.0 * 6.235750198364258
Epoch 1460, val loss: 1.1346640586853027
Epoch 1470, training loss: 623.7640380859375 = 0.1247701570391655 + 100.0 * 6.236392974853516
Epoch 1470, val loss: 1.1410104036331177
Epoch 1480, training loss: 624.152099609375 = 0.12199204415082932 + 100.0 * 6.240301132202148
Epoch 1480, val loss: 1.1472638845443726
Epoch 1490, training loss: 623.6610717773438 = 0.11926362663507462 + 100.0 * 6.23541784286499
Epoch 1490, val loss: 1.154281735420227
Epoch 1500, training loss: 623.5261840820312 = 0.11665023118257523 + 100.0 * 6.234095573425293
Epoch 1500, val loss: 1.160676121711731
Epoch 1510, training loss: 624.1897583007812 = 0.11410994827747345 + 100.0 * 6.240756511688232
Epoch 1510, val loss: 1.1673939228057861
Epoch 1520, training loss: 623.4681396484375 = 0.11155763268470764 + 100.0 * 6.233565807342529
Epoch 1520, val loss: 1.1736098527908325
Epoch 1530, training loss: 623.27294921875 = 0.10912387818098068 + 100.0 * 6.231638431549072
Epoch 1530, val loss: 1.1805206537246704
Epoch 1540, training loss: 623.2260131835938 = 0.10676737129688263 + 100.0 * 6.231192588806152
Epoch 1540, val loss: 1.1873034238815308
Epoch 1550, training loss: 623.33203125 = 0.10448179394006729 + 100.0 * 6.232275485992432
Epoch 1550, val loss: 1.1938560009002686
Epoch 1560, training loss: 623.6449584960938 = 0.10222285240888596 + 100.0 * 6.235427379608154
Epoch 1560, val loss: 1.2002909183502197
Epoch 1570, training loss: 623.3305053710938 = 0.09997810423374176 + 100.0 * 6.23230504989624
Epoch 1570, val loss: 1.206600308418274
Epoch 1580, training loss: 623.1908569335938 = 0.09782036393880844 + 100.0 * 6.230930328369141
Epoch 1580, val loss: 1.213761568069458
Epoch 1590, training loss: 623.1681518554688 = 0.09574432671070099 + 100.0 * 6.230723857879639
Epoch 1590, val loss: 1.2203859090805054
Epoch 1600, training loss: 623.8001098632812 = 0.09372269362211227 + 100.0 * 6.237063884735107
Epoch 1600, val loss: 1.2267701625823975
Epoch 1610, training loss: 623.6928100585938 = 0.09173925220966339 + 100.0 * 6.236010551452637
Epoch 1610, val loss: 1.2332817316055298
Epoch 1620, training loss: 623.0813598632812 = 0.08974331617355347 + 100.0 * 6.229916095733643
Epoch 1620, val loss: 1.2400094270706177
Epoch 1630, training loss: 622.94580078125 = 0.08785728365182877 + 100.0 * 6.228579521179199
Epoch 1630, val loss: 1.24657142162323
Epoch 1640, training loss: 622.9327392578125 = 0.08603546768426895 + 100.0 * 6.228466510772705
Epoch 1640, val loss: 1.2531793117523193
Epoch 1650, training loss: 623.8599853515625 = 0.08427097648382187 + 100.0 * 6.237756729125977
Epoch 1650, val loss: 1.2594877481460571
Epoch 1660, training loss: 623.2230834960938 = 0.08248879015445709 + 100.0 * 6.231406211853027
Epoch 1660, val loss: 1.2660576105117798
Epoch 1670, training loss: 622.9544067382812 = 0.08076637983322144 + 100.0 * 6.228736400604248
Epoch 1670, val loss: 1.2726625204086304
Epoch 1680, training loss: 623.0831909179688 = 0.07912318408489227 + 100.0 * 6.230041027069092
Epoch 1680, val loss: 1.279115915298462
Epoch 1690, training loss: 622.8169555664062 = 0.07748706638813019 + 100.0 * 6.2273945808410645
Epoch 1690, val loss: 1.2856218814849854
Epoch 1700, training loss: 622.794189453125 = 0.07590494304895401 + 100.0 * 6.227182865142822
Epoch 1700, val loss: 1.2917753458023071
Epoch 1710, training loss: 622.873291015625 = 0.07436511665582657 + 100.0 * 6.227989673614502
Epoch 1710, val loss: 1.2983185052871704
Epoch 1720, training loss: 623.0078125 = 0.07287012040615082 + 100.0 * 6.229349136352539
Epoch 1720, val loss: 1.3045847415924072
Epoch 1730, training loss: 622.75244140625 = 0.0714002400636673 + 100.0 * 6.226810455322266
Epoch 1730, val loss: 1.3113622665405273
Epoch 1740, training loss: 623.3839721679688 = 0.06997399032115936 + 100.0 * 6.233139991760254
Epoch 1740, val loss: 1.3176437616348267
Epoch 1750, training loss: 622.8604736328125 = 0.06854947656393051 + 100.0 * 6.227919578552246
Epoch 1750, val loss: 1.3237295150756836
Epoch 1760, training loss: 622.60888671875 = 0.06717979162931442 + 100.0 * 6.225417137145996
Epoch 1760, val loss: 1.3302130699157715
Epoch 1770, training loss: 622.54638671875 = 0.06586620956659317 + 100.0 * 6.2248053550720215
Epoch 1770, val loss: 1.336471676826477
Epoch 1780, training loss: 622.9046020507812 = 0.06459293514490128 + 100.0 * 6.228400230407715
Epoch 1780, val loss: 1.3425893783569336
Epoch 1790, training loss: 622.6453247070312 = 0.06330747157335281 + 100.0 * 6.225820064544678
Epoch 1790, val loss: 1.3490033149719238
Epoch 1800, training loss: 622.611572265625 = 0.06207127496600151 + 100.0 * 6.225494861602783
Epoch 1800, val loss: 1.3551312685012817
Epoch 1810, training loss: 622.5962524414062 = 0.06086653470993042 + 100.0 * 6.225354194641113
Epoch 1810, val loss: 1.3613548278808594
Epoch 1820, training loss: 622.52783203125 = 0.05971099063754082 + 100.0 * 6.224681377410889
Epoch 1820, val loss: 1.367722988128662
Epoch 1830, training loss: 622.6805419921875 = 0.05857697129249573 + 100.0 * 6.226219177246094
Epoch 1830, val loss: 1.3739653825759888
Epoch 1840, training loss: 622.5062866210938 = 0.05745228752493858 + 100.0 * 6.224488735198975
Epoch 1840, val loss: 1.3799669742584229
Epoch 1850, training loss: 622.4418334960938 = 0.05635744705796242 + 100.0 * 6.223855018615723
Epoch 1850, val loss: 1.386155128479004
Epoch 1860, training loss: 622.5775756835938 = 0.05530542507767677 + 100.0 * 6.225223064422607
Epoch 1860, val loss: 1.3924436569213867
Epoch 1870, training loss: 622.3751831054688 = 0.05425187572836876 + 100.0 * 6.223209381103516
Epoch 1870, val loss: 1.398220419883728
Epoch 1880, training loss: 622.2723999023438 = 0.053233884274959564 + 100.0 * 6.22219181060791
Epoch 1880, val loss: 1.4043322801589966
Epoch 1890, training loss: 622.6273193359375 = 0.05226265266537666 + 100.0 * 6.22575044631958
Epoch 1890, val loss: 1.4106472730636597
Epoch 1900, training loss: 622.2677612304688 = 0.05127522349357605 + 100.0 * 6.222165107727051
Epoch 1900, val loss: 1.415879487991333
Epoch 1910, training loss: 622.2636108398438 = 0.05033309385180473 + 100.0 * 6.222132682800293
Epoch 1910, val loss: 1.4222582578659058
Epoch 1920, training loss: 622.4711303710938 = 0.04942912235856056 + 100.0 * 6.224216938018799
Epoch 1920, val loss: 1.4280602931976318
Epoch 1930, training loss: 622.2651977539062 = 0.0485200360417366 + 100.0 * 6.222167015075684
Epoch 1930, val loss: 1.4342899322509766
Epoch 1940, training loss: 622.2473754882812 = 0.04765225574374199 + 100.0 * 6.221996784210205
Epoch 1940, val loss: 1.4399783611297607
Epoch 1950, training loss: 622.0803833007812 = 0.04679490625858307 + 100.0 * 6.220335960388184
Epoch 1950, val loss: 1.4459004402160645
Epoch 1960, training loss: 622.5048217773438 = 0.04597744345664978 + 100.0 * 6.224588394165039
Epoch 1960, val loss: 1.4514933824539185
Epoch 1970, training loss: 622.5186157226562 = 0.045160047709941864 + 100.0 * 6.224734783172607
Epoch 1970, val loss: 1.4571634531021118
Epoch 1980, training loss: 622.2734985351562 = 0.04433708265423775 + 100.0 * 6.222291946411133
Epoch 1980, val loss: 1.4632360935211182
Epoch 1990, training loss: 622.0538330078125 = 0.04355984181165695 + 100.0 * 6.220102787017822
Epoch 1990, val loss: 1.468671202659607
Epoch 2000, training loss: 621.9469604492188 = 0.042805977165699005 + 100.0 * 6.21904182434082
Epoch 2000, val loss: 1.4745906591415405
Epoch 2010, training loss: 621.8933715820312 = 0.04207543283700943 + 100.0 * 6.218513488769531
Epoch 2010, val loss: 1.4803367853164673
Epoch 2020, training loss: 622.126220703125 = 0.041371408849954605 + 100.0 * 6.220848083496094
Epoch 2020, val loss: 1.486012578010559
Epoch 2030, training loss: 622.051513671875 = 0.04066211357712746 + 100.0 * 6.220108509063721
Epoch 2030, val loss: 1.4914348125457764
Epoch 2040, training loss: 621.882080078125 = 0.039952393621206284 + 100.0 * 6.21842098236084
Epoch 2040, val loss: 1.4971429109573364
Epoch 2050, training loss: 622.0040283203125 = 0.039282213896512985 + 100.0 * 6.21964693069458
Epoch 2050, val loss: 1.5028841495513916
Epoch 2060, training loss: 622.3999633789062 = 0.038625482469797134 + 100.0 * 6.223613262176514
Epoch 2060, val loss: 1.5082828998565674
Epoch 2070, training loss: 622.0855102539062 = 0.03797239065170288 + 100.0 * 6.220475196838379
Epoch 2070, val loss: 1.5134685039520264
Epoch 2080, training loss: 621.9453125 = 0.037337686866521835 + 100.0 * 6.219079494476318
Epoch 2080, val loss: 1.5195345878601074
Epoch 2090, training loss: 621.8724975585938 = 0.03672754764556885 + 100.0 * 6.218357563018799
Epoch 2090, val loss: 1.5245366096496582
Epoch 2100, training loss: 622.1130981445312 = 0.03613308072090149 + 100.0 * 6.22076940536499
Epoch 2100, val loss: 1.5302850008010864
Epoch 2110, training loss: 621.887451171875 = 0.03554781526327133 + 100.0 * 6.21851921081543
Epoch 2110, val loss: 1.535523533821106
Epoch 2120, training loss: 621.8435668945312 = 0.034969862550497055 + 100.0 * 6.218086242675781
Epoch 2120, val loss: 1.5409774780273438
Epoch 2130, training loss: 621.7276000976562 = 0.034398820251226425 + 100.0 * 6.2169318199157715
Epoch 2130, val loss: 1.5461424589157104
Epoch 2140, training loss: 621.625244140625 = 0.03385674208402634 + 100.0 * 6.215913772583008
Epoch 2140, val loss: 1.5515222549438477
Epoch 2150, training loss: 621.9567260742188 = 0.03333613649010658 + 100.0 * 6.219233989715576
Epoch 2150, val loss: 1.5568510293960571
Epoch 2160, training loss: 621.8082275390625 = 0.03279358893632889 + 100.0 * 6.217754364013672
Epoch 2160, val loss: 1.5616815090179443
Epoch 2170, training loss: 621.6973876953125 = 0.03227182477712631 + 100.0 * 6.21665096282959
Epoch 2170, val loss: 1.5671643018722534
Epoch 2180, training loss: 621.6661376953125 = 0.03176291286945343 + 100.0 * 6.216343879699707
Epoch 2180, val loss: 1.572258472442627
Epoch 2190, training loss: 621.9451904296875 = 0.03127913922071457 + 100.0 * 6.219139575958252
Epoch 2190, val loss: 1.5775805711746216
Epoch 2200, training loss: 621.589599609375 = 0.03078489564359188 + 100.0 * 6.215588092803955
Epoch 2200, val loss: 1.5827165842056274
Epoch 2210, training loss: 621.80908203125 = 0.03031562827527523 + 100.0 * 6.217787742614746
Epoch 2210, val loss: 1.5875641107559204
Epoch 2220, training loss: 621.7882690429688 = 0.029849804937839508 + 100.0 * 6.217584133148193
Epoch 2220, val loss: 1.5927722454071045
Epoch 2230, training loss: 621.6271362304688 = 0.029400117695331573 + 100.0 * 6.215977191925049
Epoch 2230, val loss: 1.5980627536773682
Epoch 2240, training loss: 621.9136352539062 = 0.028957165777683258 + 100.0 * 6.218847274780273
Epoch 2240, val loss: 1.60295832157135
Epoch 2250, training loss: 621.4609375 = 0.02851206623017788 + 100.0 * 6.214324474334717
Epoch 2250, val loss: 1.6076031923294067
Epoch 2260, training loss: 621.4053955078125 = 0.02808736078441143 + 100.0 * 6.213773250579834
Epoch 2260, val loss: 1.6126998662948608
Epoch 2270, training loss: 621.39892578125 = 0.027678048238158226 + 100.0 * 6.213712692260742
Epoch 2270, val loss: 1.6177879571914673
Epoch 2280, training loss: 621.9252319335938 = 0.027289260178804398 + 100.0 * 6.218979358673096
Epoch 2280, val loss: 1.622849941253662
Epoch 2290, training loss: 621.4647827148438 = 0.026870500296354294 + 100.0 * 6.21437931060791
Epoch 2290, val loss: 1.6269992589950562
Epoch 2300, training loss: 621.4421997070312 = 0.02647692896425724 + 100.0 * 6.2141571044921875
Epoch 2300, val loss: 1.6320992708206177
Epoch 2310, training loss: 621.5201416015625 = 0.02609739452600479 + 100.0 * 6.214940071105957
Epoch 2310, val loss: 1.6369718313217163
Epoch 2320, training loss: 621.5360107421875 = 0.02572353556752205 + 100.0 * 6.2151031494140625
Epoch 2320, val loss: 1.6417683362960815
Epoch 2330, training loss: 621.3108520507812 = 0.025351684540510178 + 100.0 * 6.212855339050293
Epoch 2330, val loss: 1.6462689638137817
Epoch 2340, training loss: 621.4521484375 = 0.025000955909490585 + 100.0 * 6.214271068572998
Epoch 2340, val loss: 1.650958776473999
Epoch 2350, training loss: 621.581298828125 = 0.024653524160385132 + 100.0 * 6.215566158294678
Epoch 2350, val loss: 1.655783772468567
Epoch 2360, training loss: 621.5081176757812 = 0.024291664361953735 + 100.0 * 6.21483850479126
Epoch 2360, val loss: 1.659913420677185
Epoch 2370, training loss: 621.401123046875 = 0.023954303935170174 + 100.0 * 6.213771820068359
Epoch 2370, val loss: 1.6644964218139648
Epoch 2380, training loss: 621.2722778320312 = 0.02361583337187767 + 100.0 * 6.212486743927002
Epoch 2380, val loss: 1.6690940856933594
Epoch 2390, training loss: 621.2347412109375 = 0.023295937106013298 + 100.0 * 6.212114334106445
Epoch 2390, val loss: 1.6737626791000366
Epoch 2400, training loss: 621.5823974609375 = 0.022983964532613754 + 100.0 * 6.2155938148498535
Epoch 2400, val loss: 1.6779733896255493
Epoch 2410, training loss: 621.2780151367188 = 0.02265901491045952 + 100.0 * 6.21255350112915
Epoch 2410, val loss: 1.6827142238616943
Epoch 2420, training loss: 621.3651123046875 = 0.022357206791639328 + 100.0 * 6.213428020477295
Epoch 2420, val loss: 1.6872628927230835
Epoch 2430, training loss: 621.33154296875 = 0.022050652652978897 + 100.0 * 6.213095188140869
Epoch 2430, val loss: 1.6914600133895874
Epoch 2440, training loss: 621.5274047851562 = 0.02175479754805565 + 100.0 * 6.215056896209717
Epoch 2440, val loss: 1.6957244873046875
Epoch 2450, training loss: 621.112548828125 = 0.0214560404419899 + 100.0 * 6.210910797119141
Epoch 2450, val loss: 1.6999318599700928
Epoch 2460, training loss: 621.46630859375 = 0.021181417629122734 + 100.0 * 6.214451313018799
Epoch 2460, val loss: 1.704321026802063
Epoch 2470, training loss: 621.187744140625 = 0.020890874788165092 + 100.0 * 6.211668491363525
Epoch 2470, val loss: 1.7085695266723633
Epoch 2480, training loss: 621.2857666015625 = 0.02061992883682251 + 100.0 * 6.212651252746582
Epoch 2480, val loss: 1.7130405902862549
Epoch 2490, training loss: 621.0299682617188 = 0.020344700664281845 + 100.0 * 6.21009635925293
Epoch 2490, val loss: 1.7170510292053223
Epoch 2500, training loss: 620.9822998046875 = 0.020086873322725296 + 100.0 * 6.209621906280518
Epoch 2500, val loss: 1.7215911149978638
Epoch 2510, training loss: 621.2255859375 = 0.01983550935983658 + 100.0 * 6.212057590484619
Epoch 2510, val loss: 1.7257922887802124
Epoch 2520, training loss: 621.1513671875 = 0.019578471779823303 + 100.0 * 6.211318016052246
Epoch 2520, val loss: 1.7298345565795898
Epoch 2530, training loss: 621.3983764648438 = 0.01933194324374199 + 100.0 * 6.213790416717529
Epoch 2530, val loss: 1.7335211038589478
Epoch 2540, training loss: 621.0765380859375 = 0.019081611186265945 + 100.0 * 6.210574626922607
Epoch 2540, val loss: 1.7382310628890991
Epoch 2550, training loss: 620.9898071289062 = 0.018843673169612885 + 100.0 * 6.209709644317627
Epoch 2550, val loss: 1.742194414138794
Epoch 2560, training loss: 620.9444580078125 = 0.0186094231903553 + 100.0 * 6.209258556365967
Epoch 2560, val loss: 1.7464592456817627
Epoch 2570, training loss: 621.9382934570312 = 0.0183855090290308 + 100.0 * 6.219199180603027
Epoch 2570, val loss: 1.750378966331482
Epoch 2580, training loss: 621.3228759765625 = 0.018152719363570213 + 100.0 * 6.213047504425049
Epoch 2580, val loss: 1.753805160522461
Epoch 2590, training loss: 620.926025390625 = 0.017918473109602928 + 100.0 * 6.209080696105957
Epoch 2590, val loss: 1.7581661939620972
Epoch 2600, training loss: 620.790771484375 = 0.017702193930745125 + 100.0 * 6.207730770111084
Epoch 2600, val loss: 1.7622102499008179
Epoch 2610, training loss: 620.8277587890625 = 0.017493635416030884 + 100.0 * 6.208102226257324
Epoch 2610, val loss: 1.7662631273269653
Epoch 2620, training loss: 621.4555053710938 = 0.01729435846209526 + 100.0 * 6.214382171630859
Epoch 2620, val loss: 1.7700917720794678
Epoch 2630, training loss: 621.015869140625 = 0.017072174698114395 + 100.0 * 6.209987640380859
Epoch 2630, val loss: 1.7735143899917603
Epoch 2640, training loss: 620.8872680664062 = 0.01687004417181015 + 100.0 * 6.208704471588135
Epoch 2640, val loss: 1.7776539325714111
Epoch 2650, training loss: 621.016845703125 = 0.01666758954524994 + 100.0 * 6.2100019454956055
Epoch 2650, val loss: 1.7812000513076782
Epoch 2660, training loss: 620.7791137695312 = 0.01647278293967247 + 100.0 * 6.2076263427734375
Epoch 2660, val loss: 1.7852182388305664
Epoch 2670, training loss: 621.1129760742188 = 0.016285089775919914 + 100.0 * 6.210967063903809
Epoch 2670, val loss: 1.7886713743209839
Epoch 2680, training loss: 621.1353759765625 = 0.016097700223326683 + 100.0 * 6.211192607879639
Epoch 2680, val loss: 1.7928369045257568
Epoch 2690, training loss: 620.8548583984375 = 0.01590464450418949 + 100.0 * 6.2083892822265625
Epoch 2690, val loss: 1.7961795330047607
Epoch 2700, training loss: 620.722900390625 = 0.0157198254019022 + 100.0 * 6.207071304321289
Epoch 2700, val loss: 1.8001947402954102
Epoch 2710, training loss: 620.6520385742188 = 0.015541878528892994 + 100.0 * 6.206364631652832
Epoch 2710, val loss: 1.8036597967147827
Epoch 2720, training loss: 620.7529907226562 = 0.015372637659311295 + 100.0 * 6.207376003265381
Epoch 2720, val loss: 1.8072636127471924
Epoch 2730, training loss: 621.4274291992188 = 0.015203387476503849 + 100.0 * 6.214122295379639
Epoch 2730, val loss: 1.8104753494262695
Epoch 2740, training loss: 621.0103149414062 = 0.015031319111585617 + 100.0 * 6.2099528312683105
Epoch 2740, val loss: 1.814570665359497
Epoch 2750, training loss: 620.9971923828125 = 0.01486105378717184 + 100.0 * 6.2098236083984375
Epoch 2750, val loss: 1.8178354501724243
Epoch 2760, training loss: 620.9443969726562 = 0.014696984551846981 + 100.0 * 6.209297180175781
Epoch 2760, val loss: 1.8212580680847168
Epoch 2770, training loss: 620.7345581054688 = 0.014528962783515453 + 100.0 * 6.207200050354004
Epoch 2770, val loss: 1.8249088525772095
Epoch 2780, training loss: 620.601806640625 = 0.014371768571436405 + 100.0 * 6.205874443054199
Epoch 2780, val loss: 1.8285129070281982
Epoch 2790, training loss: 620.5345458984375 = 0.014218626543879509 + 100.0 * 6.205203533172607
Epoch 2790, val loss: 1.832014799118042
Epoch 2800, training loss: 620.9908447265625 = 0.014072299003601074 + 100.0 * 6.2097673416137695
Epoch 2800, val loss: 1.8352251052856445
Epoch 2810, training loss: 620.6138916015625 = 0.013915509916841984 + 100.0 * 6.205999851226807
Epoch 2810, val loss: 1.8384535312652588
Epoch 2820, training loss: 620.5691528320312 = 0.013763201422989368 + 100.0 * 6.205554008483887
Epoch 2820, val loss: 1.841954231262207
Epoch 2830, training loss: 620.9352416992188 = 0.013621375896036625 + 100.0 * 6.209216594696045
Epoch 2830, val loss: 1.8456039428710938
Epoch 2840, training loss: 620.5772094726562 = 0.01347466092556715 + 100.0 * 6.205636978149414
Epoch 2840, val loss: 1.848983645439148
Epoch 2850, training loss: 620.5554809570312 = 0.013334380462765694 + 100.0 * 6.2054219245910645
Epoch 2850, val loss: 1.8522518873214722
Epoch 2860, training loss: 620.5822143554688 = 0.01320094894617796 + 100.0 * 6.205690383911133
Epoch 2860, val loss: 1.8554397821426392
Epoch 2870, training loss: 621.052978515625 = 0.01307042594999075 + 100.0 * 6.210399150848389
Epoch 2870, val loss: 1.858901023864746
Epoch 2880, training loss: 620.666015625 = 0.012927352450788021 + 100.0 * 6.206530570983887
Epoch 2880, val loss: 1.8614108562469482
Epoch 2890, training loss: 620.5724487304688 = 0.012799041345715523 + 100.0 * 6.205596446990967
Epoch 2890, val loss: 1.8650786876678467
Epoch 2900, training loss: 620.574462890625 = 0.01266779750585556 + 100.0 * 6.205617427825928
Epoch 2900, val loss: 1.8679903745651245
Epoch 2910, training loss: 620.5756225585938 = 0.012544292025268078 + 100.0 * 6.205630302429199
Epoch 2910, val loss: 1.871342658996582
Epoch 2920, training loss: 620.739501953125 = 0.012420224957168102 + 100.0 * 6.207271099090576
Epoch 2920, val loss: 1.8749421834945679
Epoch 2930, training loss: 620.6734008789062 = 0.012293285690248013 + 100.0 * 6.206611156463623
Epoch 2930, val loss: 1.8776419162750244
Epoch 2940, training loss: 620.5130615234375 = 0.012171386741101742 + 100.0 * 6.2050089836120605
Epoch 2940, val loss: 1.8804558515548706
Epoch 2950, training loss: 620.4127807617188 = 0.01204894483089447 + 100.0 * 6.204007625579834
Epoch 2950, val loss: 1.883798599243164
Epoch 2960, training loss: 620.3535766601562 = 0.011934477835893631 + 100.0 * 6.203416347503662
Epoch 2960, val loss: 1.887011170387268
Epoch 2970, training loss: 620.8916015625 = 0.011823895387351513 + 100.0 * 6.208797454833984
Epoch 2970, val loss: 1.889776349067688
Epoch 2980, training loss: 620.8897705078125 = 0.011708354577422142 + 100.0 * 6.208780765533447
Epoch 2980, val loss: 1.8924083709716797
Epoch 2990, training loss: 620.4898681640625 = 0.011587433516979218 + 100.0 * 6.204782485961914
Epoch 2990, val loss: 1.895683765411377
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8244596731681603
The final CL Acc:0.71975, 0.01522, The final GNN Acc:0.81989, 0.00348
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13198])
remove edge: torch.Size([2, 7964])
updated graph: torch.Size([2, 10606])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.632568359375 = 1.9507293701171875 + 100.0 * 8.596817970275879
Epoch 0, val loss: 1.957006812095642
Epoch 10, training loss: 861.5372314453125 = 1.942155122756958 + 100.0 * 8.595951080322266
Epoch 10, val loss: 1.9479314088821411
Epoch 20, training loss: 860.9451293945312 = 1.9314357042312622 + 100.0 * 8.590136528015137
Epoch 20, val loss: 1.9365625381469727
Epoch 30, training loss: 857.239990234375 = 1.9169689416885376 + 100.0 * 8.553230285644531
Epoch 30, val loss: 1.9211511611938477
Epoch 40, training loss: 837.3682861328125 = 1.8982574939727783 + 100.0 * 8.354700088500977
Epoch 40, val loss: 1.9019036293029785
Epoch 50, training loss: 766.9734497070312 = 1.8756468296051025 + 100.0 * 7.650977611541748
Epoch 50, val loss: 1.8790682554244995
Epoch 60, training loss: 742.123779296875 = 1.8567403554916382 + 100.0 * 7.402670383453369
Epoch 60, val loss: 1.861915111541748
Epoch 70, training loss: 720.3013916015625 = 1.8435081243515015 + 100.0 * 7.184578895568848
Epoch 70, val loss: 1.8495428562164307
Epoch 80, training loss: 704.1549072265625 = 1.8317350149154663 + 100.0 * 7.0232319831848145
Epoch 80, val loss: 1.8384673595428467
Epoch 90, training loss: 690.986083984375 = 1.8218762874603271 + 100.0 * 6.891641616821289
Epoch 90, val loss: 1.8292769193649292
Epoch 100, training loss: 682.5662231445312 = 1.811956763267517 + 100.0 * 6.80754280090332
Epoch 100, val loss: 1.819945216178894
Epoch 110, training loss: 676.8929443359375 = 1.8021906614303589 + 100.0 * 6.7509074211120605
Epoch 110, val loss: 1.8106266260147095
Epoch 120, training loss: 672.4662475585938 = 1.7922258377075195 + 100.0 * 6.706739902496338
Epoch 120, val loss: 1.8010776042938232
Epoch 130, training loss: 668.5501098632812 = 1.7821173667907715 + 100.0 * 6.667680263519287
Epoch 130, val loss: 1.7915549278259277
Epoch 140, training loss: 664.6873168945312 = 1.7721421718597412 + 100.0 * 6.629151344299316
Epoch 140, val loss: 1.7822514772415161
Epoch 150, training loss: 661.5204467773438 = 1.7618584632873535 + 100.0 * 6.597586154937744
Epoch 150, val loss: 1.7724074125289917
Epoch 160, training loss: 659.1343383789062 = 1.7504026889801025 + 100.0 * 6.57383918762207
Epoch 160, val loss: 1.7615406513214111
Epoch 170, training loss: 657.17919921875 = 1.7377605438232422 + 100.0 * 6.554414749145508
Epoch 170, val loss: 1.7497568130493164
Epoch 180, training loss: 655.4144287109375 = 1.7240678071975708 + 100.0 * 6.5369038581848145
Epoch 180, val loss: 1.7371232509613037
Epoch 190, training loss: 653.8284912109375 = 1.7093844413757324 + 100.0 * 6.521190643310547
Epoch 190, val loss: 1.7237409353256226
Epoch 200, training loss: 652.655029296875 = 1.6937806606292725 + 100.0 * 6.509612560272217
Epoch 200, val loss: 1.7095905542373657
Epoch 210, training loss: 651.0684204101562 = 1.6767112016677856 + 100.0 * 6.493916988372803
Epoch 210, val loss: 1.6943070888519287
Epoch 220, training loss: 649.639892578125 = 1.6585017442703247 + 100.0 * 6.479813575744629
Epoch 220, val loss: 1.6781139373779297
Epoch 230, training loss: 648.7111206054688 = 1.639241337776184 + 100.0 * 6.470718860626221
Epoch 230, val loss: 1.660950779914856
Epoch 240, training loss: 647.2869873046875 = 1.618465781211853 + 100.0 * 6.4566850662231445
Epoch 240, val loss: 1.6426217555999756
Epoch 250, training loss: 646.2665405273438 = 1.5965839624404907 + 100.0 * 6.446700096130371
Epoch 250, val loss: 1.6232670545578003
Epoch 260, training loss: 645.9223022460938 = 1.5736637115478516 + 100.0 * 6.443486213684082
Epoch 260, val loss: 1.6030396223068237
Epoch 270, training loss: 644.5864868164062 = 1.549517035484314 + 100.0 * 6.430369853973389
Epoch 270, val loss: 1.5819984674453735
Epoch 280, training loss: 643.5521850585938 = 1.5246444940567017 + 100.0 * 6.4202752113342285
Epoch 280, val loss: 1.5604044198989868
Epoch 290, training loss: 642.9544067382812 = 1.4992220401763916 + 100.0 * 6.414552211761475
Epoch 290, val loss: 1.5383635759353638
Epoch 300, training loss: 642.0834350585938 = 1.4728983640670776 + 100.0 * 6.4061055183410645
Epoch 300, val loss: 1.5157452821731567
Epoch 310, training loss: 641.1969604492188 = 1.4464269876480103 + 100.0 * 6.397505760192871
Epoch 310, val loss: 1.4932317733764648
Epoch 320, training loss: 640.537841796875 = 1.4196412563323975 + 100.0 * 6.3911824226379395
Epoch 320, val loss: 1.4703887701034546
Epoch 330, training loss: 640.277587890625 = 1.3927912712097168 + 100.0 * 6.388848304748535
Epoch 330, val loss: 1.4475336074829102
Epoch 340, training loss: 639.3597412109375 = 1.3654897212982178 + 100.0 * 6.379942417144775
Epoch 340, val loss: 1.4245723485946655
Epoch 350, training loss: 638.766845703125 = 1.338313341140747 + 100.0 * 6.3742852210998535
Epoch 350, val loss: 1.4016238451004028
Epoch 360, training loss: 638.2373046875 = 1.31118643283844 + 100.0 * 6.369261741638184
Epoch 360, val loss: 1.3787882328033447
Epoch 370, training loss: 638.2597045898438 = 1.28412926197052 + 100.0 * 6.369755744934082
Epoch 370, val loss: 1.3559761047363281
Epoch 380, training loss: 637.3834838867188 = 1.256917953491211 + 100.0 * 6.361266136169434
Epoch 380, val loss: 1.3331243991851807
Epoch 390, training loss: 636.848876953125 = 1.229909896850586 + 100.0 * 6.356189727783203
Epoch 390, val loss: 1.310448169708252
Epoch 400, training loss: 636.55859375 = 1.2031556367874146 + 100.0 * 6.3535542488098145
Epoch 400, val loss: 1.2880405187606812
Epoch 410, training loss: 636.515625 = 1.1762651205062866 + 100.0 * 6.3533935546875
Epoch 410, val loss: 1.2655718326568604
Epoch 420, training loss: 635.7323608398438 = 1.149752140045166 + 100.0 * 6.345825672149658
Epoch 420, val loss: 1.2433162927627563
Epoch 430, training loss: 635.2991943359375 = 1.123514175415039 + 100.0 * 6.341756343841553
Epoch 430, val loss: 1.2213902473449707
Epoch 440, training loss: 634.9894409179688 = 1.0976203680038452 + 100.0 * 6.338918209075928
Epoch 440, val loss: 1.1997953653335571
Epoch 450, training loss: 634.9253540039062 = 1.0718674659729004 + 100.0 * 6.338535308837891
Epoch 450, val loss: 1.1784199476242065
Epoch 460, training loss: 634.4259643554688 = 1.0465426445007324 + 100.0 * 6.333793640136719
Epoch 460, val loss: 1.1574150323867798
Epoch 470, training loss: 634.0034790039062 = 1.021690011024475 + 100.0 * 6.329818248748779
Epoch 470, val loss: 1.136887788772583
Epoch 480, training loss: 633.6796264648438 = 0.9973770380020142 + 100.0 * 6.326822280883789
Epoch 480, val loss: 1.11689031124115
Epoch 490, training loss: 633.6284790039062 = 0.9735094904899597 + 100.0 * 6.326549530029297
Epoch 490, val loss: 1.09743332862854
Epoch 500, training loss: 633.5421142578125 = 0.9502201080322266 + 100.0 * 6.325919151306152
Epoch 500, val loss: 1.078431248664856
Epoch 510, training loss: 632.9900512695312 = 0.9273637533187866 + 100.0 * 6.320626735687256
Epoch 510, val loss: 1.0600217580795288
Epoch 520, training loss: 632.6329345703125 = 0.9051821827888489 + 100.0 * 6.317277431488037
Epoch 520, val loss: 1.0422884225845337
Epoch 530, training loss: 632.4186401367188 = 0.8836358785629272 + 100.0 * 6.31535005569458
Epoch 530, val loss: 1.0252060890197754
Epoch 540, training loss: 632.2383422851562 = 0.8624258041381836 + 100.0 * 6.3137593269348145
Epoch 540, val loss: 1.0085114240646362
Epoch 550, training loss: 631.9467163085938 = 0.84187251329422 + 100.0 * 6.31104850769043
Epoch 550, val loss: 0.9925171732902527
Epoch 560, training loss: 631.6425170898438 = 0.8218507170677185 + 100.0 * 6.308207035064697
Epoch 560, val loss: 0.9770708084106445
Epoch 570, training loss: 631.5635986328125 = 0.8024246692657471 + 100.0 * 6.30761194229126
Epoch 570, val loss: 0.9622809290885925
Epoch 580, training loss: 631.4716186523438 = 0.7834555506706238 + 100.0 * 6.306881427764893
Epoch 580, val loss: 0.9482835531234741
Epoch 590, training loss: 631.1362915039062 = 0.7647983431816101 + 100.0 * 6.303715229034424
Epoch 590, val loss: 0.934390664100647
Epoch 600, training loss: 630.88623046875 = 0.7467442750930786 + 100.0 * 6.301394939422607
Epoch 600, val loss: 0.9215248227119446
Epoch 610, training loss: 630.736572265625 = 0.7290800213813782 + 100.0 * 6.300075054168701
Epoch 610, val loss: 0.9089184403419495
Epoch 620, training loss: 630.6648559570312 = 0.7117990255355835 + 100.0 * 6.299530506134033
Epoch 620, val loss: 0.8969771862030029
Epoch 630, training loss: 630.574951171875 = 0.6948591470718384 + 100.0 * 6.298800468444824
Epoch 630, val loss: 0.8853554129600525
Epoch 640, training loss: 630.1801147460938 = 0.6782768964767456 + 100.0 * 6.295018196105957
Epoch 640, val loss: 0.8743115067481995
Epoch 650, training loss: 630.0175170898438 = 0.6621373891830444 + 100.0 * 6.293554306030273
Epoch 650, val loss: 0.8638392090797424
Epoch 660, training loss: 629.9734497070312 = 0.6463181972503662 + 100.0 * 6.293271541595459
Epoch 660, val loss: 0.8537546396255493
Epoch 670, training loss: 629.9916381835938 = 0.6307908296585083 + 100.0 * 6.29360818862915
Epoch 670, val loss: 0.8440895080566406
Epoch 680, training loss: 629.5372924804688 = 0.6154680848121643 + 100.0 * 6.289217948913574
Epoch 680, val loss: 0.8349974751472473
Epoch 690, training loss: 629.3344116210938 = 0.6005131602287292 + 100.0 * 6.287338733673096
Epoch 690, val loss: 0.8262096643447876
Epoch 700, training loss: 629.1922607421875 = 0.5858629941940308 + 100.0 * 6.2860636711120605
Epoch 700, val loss: 0.8179577589035034
Epoch 710, training loss: 629.8093872070312 = 0.571444034576416 + 100.0 * 6.292379379272461
Epoch 710, val loss: 0.8099942803382874
Epoch 720, training loss: 629.0038452148438 = 0.5571683049201965 + 100.0 * 6.284466743469238
Epoch 720, val loss: 0.8023087382316589
Epoch 730, training loss: 628.8350219726562 = 0.5431520342826843 + 100.0 * 6.282918930053711
Epoch 730, val loss: 0.7949504852294922
Epoch 740, training loss: 628.7305908203125 = 0.5294381976127625 + 100.0 * 6.28201150894165
Epoch 740, val loss: 0.7879768013954163
Epoch 750, training loss: 628.699951171875 = 0.5159621238708496 + 100.0 * 6.281839847564697
Epoch 750, val loss: 0.781394362449646
Epoch 760, training loss: 628.7593994140625 = 0.5026248693466187 + 100.0 * 6.282567501068115
Epoch 760, val loss: 0.775177001953125
Epoch 770, training loss: 628.6155395507812 = 0.4895130693912506 + 100.0 * 6.2812604904174805
Epoch 770, val loss: 0.7692015767097473
Epoch 780, training loss: 628.2615966796875 = 0.47653913497924805 + 100.0 * 6.277850151062012
Epoch 780, val loss: 0.7634474635124207
Epoch 790, training loss: 628.0397338867188 = 0.46391475200653076 + 100.0 * 6.275757789611816
Epoch 790, val loss: 0.7580079436302185
Epoch 800, training loss: 627.95947265625 = 0.4514957070350647 + 100.0 * 6.275079727172852
Epoch 800, val loss: 0.7528680562973022
Epoch 810, training loss: 628.255126953125 = 0.439260333776474 + 100.0 * 6.278158664703369
Epoch 810, val loss: 0.7479664087295532
Epoch 820, training loss: 627.9603271484375 = 0.4271860718727112 + 100.0 * 6.275331497192383
Epoch 820, val loss: 0.7431354522705078
Epoch 830, training loss: 627.7158203125 = 0.4152510166168213 + 100.0 * 6.273005962371826
Epoch 830, val loss: 0.7386168837547302
Epoch 840, training loss: 627.5435180664062 = 0.40353208780288696 + 100.0 * 6.271400451660156
Epoch 840, val loss: 0.734379768371582
Epoch 850, training loss: 627.4226684570312 = 0.39215195178985596 + 100.0 * 6.270305156707764
Epoch 850, val loss: 0.7304900884628296
Epoch 860, training loss: 627.5829467773438 = 0.38097745180130005 + 100.0 * 6.272019386291504
Epoch 860, val loss: 0.7267472147941589
Epoch 870, training loss: 627.284423828125 = 0.36999017000198364 + 100.0 * 6.269144535064697
Epoch 870, val loss: 0.7231045961380005
Epoch 880, training loss: 627.1893310546875 = 0.3592603802680969 + 100.0 * 6.268300533294678
Epoch 880, val loss: 0.7198089957237244
Epoch 890, training loss: 627.29638671875 = 0.3488209545612335 + 100.0 * 6.26947546005249
Epoch 890, val loss: 0.7167489528656006
Epoch 900, training loss: 627.151611328125 = 0.3385361135005951 + 100.0 * 6.268130302429199
Epoch 900, val loss: 0.7140052318572998
Epoch 910, training loss: 627.067138671875 = 0.3286369740962982 + 100.0 * 6.267385482788086
Epoch 910, val loss: 0.7113317847251892
Epoch 920, training loss: 626.7221069335938 = 0.3189452886581421 + 100.0 * 6.264031410217285
Epoch 920, val loss: 0.7088158130645752
Epoch 930, training loss: 626.6658325195312 = 0.3096033036708832 + 100.0 * 6.263562202453613
Epoch 930, val loss: 0.7066890001296997
Epoch 940, training loss: 627.1663208007812 = 0.30049949884414673 + 100.0 * 6.268658638000488
Epoch 940, val loss: 0.704734206199646
Epoch 950, training loss: 626.50732421875 = 0.291720449924469 + 100.0 * 6.262155532836914
Epoch 950, val loss: 0.702843427658081
Epoch 960, training loss: 626.4368896484375 = 0.2832026481628418 + 100.0 * 6.261537075042725
Epoch 960, val loss: 0.7013141512870789
Epoch 970, training loss: 626.5023193359375 = 0.2749860882759094 + 100.0 * 6.262273788452148
Epoch 970, val loss: 0.7000301480293274
Epoch 980, training loss: 626.2828369140625 = 0.2670048177242279 + 100.0 * 6.260158061981201
Epoch 980, val loss: 0.6987656354904175
Epoch 990, training loss: 626.163818359375 = 0.2592636048793793 + 100.0 * 6.259045600891113
Epoch 990, val loss: 0.6978939771652222
Epoch 1000, training loss: 626.0902709960938 = 0.2518593966960907 + 100.0 * 6.2583842277526855
Epoch 1000, val loss: 0.6971731781959534
Epoch 1010, training loss: 626.3368530273438 = 0.2447367012500763 + 100.0 * 6.260921001434326
Epoch 1010, val loss: 0.6966821551322937
Epoch 1020, training loss: 626.7430419921875 = 0.2377951741218567 + 100.0 * 6.265052318572998
Epoch 1020, val loss: 0.6961427927017212
Epoch 1030, training loss: 625.9904174804688 = 0.23095713555812836 + 100.0 * 6.257594585418701
Epoch 1030, val loss: 0.6960723400115967
Epoch 1040, training loss: 625.8555297851562 = 0.2245229184627533 + 100.0 * 6.256309509277344
Epoch 1040, val loss: 0.6962239146232605
Epoch 1050, training loss: 625.7037963867188 = 0.21832241117954254 + 100.0 * 6.254854679107666
Epoch 1050, val loss: 0.6965138912200928
Epoch 1060, training loss: 625.7200927734375 = 0.21235664188861847 + 100.0 * 6.255077362060547
Epoch 1060, val loss: 0.6969121694564819
Epoch 1070, training loss: 625.8305053710938 = 0.20655599236488342 + 100.0 * 6.256239891052246
Epoch 1070, val loss: 0.6974899768829346
Epoch 1080, training loss: 625.4696044921875 = 0.20088273286819458 + 100.0 * 6.252687454223633
Epoch 1080, val loss: 0.6980910897254944
Epoch 1090, training loss: 625.4381713867188 = 0.19546395540237427 + 100.0 * 6.252427577972412
Epoch 1090, val loss: 0.6989281177520752
Epoch 1100, training loss: 625.7451782226562 = 0.19029240310192108 + 100.0 * 6.25554895401001
Epoch 1100, val loss: 0.6998372077941895
Epoch 1110, training loss: 625.68701171875 = 0.1851734220981598 + 100.0 * 6.25501823425293
Epoch 1110, val loss: 0.7010310888290405
Epoch 1120, training loss: 625.419677734375 = 0.18021884560585022 + 100.0 * 6.252394199371338
Epoch 1120, val loss: 0.7019296288490295
Epoch 1130, training loss: 625.1989135742188 = 0.17550376057624817 + 100.0 * 6.250234127044678
Epoch 1130, val loss: 0.7033267021179199
Epoch 1140, training loss: 625.1260986328125 = 0.170933336019516 + 100.0 * 6.249551296234131
Epoch 1140, val loss: 0.7047624588012695
Epoch 1150, training loss: 625.4574584960938 = 0.16657564043998718 + 100.0 * 6.252909183502197
Epoch 1150, val loss: 0.7061439156532288
Epoch 1160, training loss: 625.503173828125 = 0.16215680539608002 + 100.0 * 6.2534098625183105
Epoch 1160, val loss: 0.7078860998153687
Epoch 1170, training loss: 624.9584350585938 = 0.15795758366584778 + 100.0 * 6.248004913330078
Epoch 1170, val loss: 0.7094387412071228
Epoch 1180, training loss: 624.9646606445312 = 0.1539442241191864 + 100.0 * 6.248107433319092
Epoch 1180, val loss: 0.7112513780593872
Epoch 1190, training loss: 624.8060302734375 = 0.15005099773406982 + 100.0 * 6.246560096740723
Epoch 1190, val loss: 0.7132933735847473
Epoch 1200, training loss: 624.8768310546875 = 0.14629670977592468 + 100.0 * 6.247305393218994
Epoch 1200, val loss: 0.7153782844543457
Epoch 1210, training loss: 625.069580078125 = 0.14258070290088654 + 100.0 * 6.249269962310791
Epoch 1210, val loss: 0.7172688841819763
Epoch 1220, training loss: 624.928955078125 = 0.1389850676059723 + 100.0 * 6.247899532318115
Epoch 1220, val loss: 0.7195019721984863
Epoch 1230, training loss: 624.6538696289062 = 0.13547953963279724 + 100.0 * 6.24518346786499
Epoch 1230, val loss: 0.7215183973312378
Epoch 1240, training loss: 624.6005859375 = 0.13213522732257843 + 100.0 * 6.244684219360352
Epoch 1240, val loss: 0.7238542437553406
Epoch 1250, training loss: 624.72509765625 = 0.12891457974910736 + 100.0 * 6.245961666107178
Epoch 1250, val loss: 0.7263787984848022
Epoch 1260, training loss: 624.7821044921875 = 0.12575763463974 + 100.0 * 6.246563911437988
Epoch 1260, val loss: 0.7288693189620972
Epoch 1270, training loss: 624.4580688476562 = 0.12262348085641861 + 100.0 * 6.243354320526123
Epoch 1270, val loss: 0.7312509417533875
Epoch 1280, training loss: 624.7053833007812 = 0.11964382976293564 + 100.0 * 6.2458577156066895
Epoch 1280, val loss: 0.7339122891426086
Epoch 1290, training loss: 624.3563232421875 = 0.11672884970903397 + 100.0 * 6.242396354675293
Epoch 1290, val loss: 0.7364221215248108
Epoch 1300, training loss: 624.3346557617188 = 0.11391633749008179 + 100.0 * 6.2422075271606445
Epoch 1300, val loss: 0.7390363812446594
Epoch 1310, training loss: 624.2435302734375 = 0.11118091642856598 + 100.0 * 6.241323471069336
Epoch 1310, val loss: 0.7420281171798706
Epoch 1320, training loss: 624.8755493164062 = 0.10853990912437439 + 100.0 * 6.2476701736450195
Epoch 1320, val loss: 0.7447736263275146
Epoch 1330, training loss: 624.5465698242188 = 0.10591571778059006 + 100.0 * 6.244406700134277
Epoch 1330, val loss: 0.7473178505897522
Epoch 1340, training loss: 624.3448486328125 = 0.10335159301757812 + 100.0 * 6.242415428161621
Epoch 1340, val loss: 0.7503513097763062
Epoch 1350, training loss: 624.0300903320312 = 0.10087736696004868 + 100.0 * 6.239292144775391
Epoch 1350, val loss: 0.7533189654350281
Epoch 1360, training loss: 624.0870361328125 = 0.09849022328853607 + 100.0 * 6.239885330200195
Epoch 1360, val loss: 0.756109893321991
Epoch 1370, training loss: 624.1839599609375 = 0.09617629647254944 + 100.0 * 6.240877628326416
Epoch 1370, val loss: 0.7593187093734741
Epoch 1380, training loss: 624.1698608398438 = 0.09391122311353683 + 100.0 * 6.240759372711182
Epoch 1380, val loss: 0.7623569965362549
Epoch 1390, training loss: 624.0978393554688 = 0.09168229252099991 + 100.0 * 6.2400617599487305
Epoch 1390, val loss: 0.7651874423027039
Epoch 1400, training loss: 624.0702514648438 = 0.0895395576953888 + 100.0 * 6.23980712890625
Epoch 1400, val loss: 0.7685396671295166
Epoch 1410, training loss: 623.8544311523438 = 0.08742090314626694 + 100.0 * 6.237670421600342
Epoch 1410, val loss: 0.7714201807975769
Epoch 1420, training loss: 623.7988891601562 = 0.08541493862867355 + 100.0 * 6.23713493347168
Epoch 1420, val loss: 0.7749486565589905
Epoch 1430, training loss: 624.0680541992188 = 0.08345966786146164 + 100.0 * 6.239846229553223
Epoch 1430, val loss: 0.7781039476394653
Epoch 1440, training loss: 623.9323120117188 = 0.0815337598323822 + 100.0 * 6.2385077476501465
Epoch 1440, val loss: 0.7811678647994995
Epoch 1450, training loss: 623.7490844726562 = 0.0796528086066246 + 100.0 * 6.2366943359375
Epoch 1450, val loss: 0.7843999266624451
Epoch 1460, training loss: 623.7246704101562 = 0.07782290875911713 + 100.0 * 6.23646879196167
Epoch 1460, val loss: 0.7877395153045654
Epoch 1470, training loss: 623.6383666992188 = 0.07606183737516403 + 100.0 * 6.235623359680176
Epoch 1470, val loss: 0.7909592986106873
Epoch 1480, training loss: 624.5086669921875 = 0.07436732947826385 + 100.0 * 6.244343280792236
Epoch 1480, val loss: 0.7940284609794617
Epoch 1490, training loss: 623.8369140625 = 0.07262372225522995 + 100.0 * 6.237642765045166
Epoch 1490, val loss: 0.797595739364624
Epoch 1500, training loss: 623.4654541015625 = 0.0709887221455574 + 100.0 * 6.233944892883301
Epoch 1500, val loss: 0.8006813526153564
Epoch 1510, training loss: 623.4298706054688 = 0.06941765546798706 + 100.0 * 6.233604431152344
Epoch 1510, val loss: 0.8041988015174866
Epoch 1520, training loss: 623.3829956054688 = 0.06789078563451767 + 100.0 * 6.233150959014893
Epoch 1520, val loss: 0.8074716925621033
Epoch 1530, training loss: 624.2861328125 = 0.06643639504909515 + 100.0 * 6.242197513580322
Epoch 1530, val loss: 0.810894250869751
Epoch 1540, training loss: 623.5940551757812 = 0.06492267549037933 + 100.0 * 6.235291004180908
Epoch 1540, val loss: 0.8141305446624756
Epoch 1550, training loss: 623.3670654296875 = 0.0635019838809967 + 100.0 * 6.233036041259766
Epoch 1550, val loss: 0.8174452185630798
Epoch 1560, training loss: 623.3457641601562 = 0.062124352902173996 + 100.0 * 6.2328362464904785
Epoch 1560, val loss: 0.8208505511283875
Epoch 1570, training loss: 623.7431640625 = 0.06079268082976341 + 100.0 * 6.236823558807373
Epoch 1570, val loss: 0.8241753578186035
Epoch 1580, training loss: 623.3036499023438 = 0.059443164616823196 + 100.0 * 6.2324419021606445
Epoch 1580, val loss: 0.8276737928390503
Epoch 1590, training loss: 623.1588134765625 = 0.05817755311727524 + 100.0 * 6.231006622314453
Epoch 1590, val loss: 0.8309484124183655
Epoch 1600, training loss: 623.4664306640625 = 0.05693951994180679 + 100.0 * 6.234095096588135
Epoch 1600, val loss: 0.8343142867088318
Epoch 1610, training loss: 623.2416381835938 = 0.0557437390089035 + 100.0 * 6.23185920715332
Epoch 1610, val loss: 0.8376706838607788
Epoch 1620, training loss: 623.079345703125 = 0.05454606935381889 + 100.0 * 6.230247974395752
Epoch 1620, val loss: 0.841158926486969
Epoch 1630, training loss: 623.0263061523438 = 0.05340902507305145 + 100.0 * 6.229728698730469
Epoch 1630, val loss: 0.8447409868240356
Epoch 1640, training loss: 623.2597045898438 = 0.052310507744550705 + 100.0 * 6.23207426071167
Epoch 1640, val loss: 0.8481241464614868
Epoch 1650, training loss: 623.045166015625 = 0.05121490731835365 + 100.0 * 6.2299394607543945
Epoch 1650, val loss: 0.8511238098144531
Epoch 1660, training loss: 622.93212890625 = 0.05014742910861969 + 100.0 * 6.228819847106934
Epoch 1660, val loss: 0.8546911478042603
Epoch 1670, training loss: 622.9059448242188 = 0.04913320392370224 + 100.0 * 6.228568077087402
Epoch 1670, val loss: 0.8581163883209229
Epoch 1680, training loss: 623.2079467773438 = 0.048148538917303085 + 100.0 * 6.231597900390625
Epoch 1680, val loss: 0.861282467842102
Epoch 1690, training loss: 622.8932495117188 = 0.04715273901820183 + 100.0 * 6.228460788726807
Epoch 1690, val loss: 0.8648346662521362
Epoch 1700, training loss: 622.7971801757812 = 0.04619079828262329 + 100.0 * 6.22750997543335
Epoch 1700, val loss: 0.8679157495498657
Epoch 1710, training loss: 622.8310546875 = 0.04528004303574562 + 100.0 * 6.22785758972168
Epoch 1710, val loss: 0.8715662956237793
Epoch 1720, training loss: 623.0128784179688 = 0.04438190907239914 + 100.0 * 6.229684829711914
Epoch 1720, val loss: 0.8749292492866516
Epoch 1730, training loss: 622.7808837890625 = 0.04348702356219292 + 100.0 * 6.22737455368042
Epoch 1730, val loss: 0.8779482245445251
Epoch 1740, training loss: 623.22021484375 = 0.04264301806688309 + 100.0 * 6.231775283813477
Epoch 1740, val loss: 0.8818681240081787
Epoch 1750, training loss: 622.84130859375 = 0.04180092737078667 + 100.0 * 6.227994918823242
Epoch 1750, val loss: 0.8843556046485901
Epoch 1760, training loss: 622.7798461914062 = 0.04098474606871605 + 100.0 * 6.227388858795166
Epoch 1760, val loss: 0.8879790306091309
Epoch 1770, training loss: 622.9039306640625 = 0.04019622132182121 + 100.0 * 6.228637218475342
Epoch 1770, val loss: 0.8913005590438843
Epoch 1780, training loss: 622.8152465820312 = 0.03941747546195984 + 100.0 * 6.227758407592773
Epoch 1780, val loss: 0.894499659538269
Epoch 1790, training loss: 622.5587158203125 = 0.03865271061658859 + 100.0 * 6.225200653076172
Epoch 1790, val loss: 0.8975908160209656
Epoch 1800, training loss: 622.54296875 = 0.037921611219644547 + 100.0 * 6.225050449371338
Epoch 1800, val loss: 0.9009767770767212
Epoch 1810, training loss: 622.5433349609375 = 0.037210311740636826 + 100.0 * 6.225060939788818
Epoch 1810, val loss: 0.9039080142974854
Epoch 1820, training loss: 622.9091796875 = 0.036514829844236374 + 100.0 * 6.228726863861084
Epoch 1820, val loss: 0.9072466492652893
Epoch 1830, training loss: 622.7257690429688 = 0.03583378344774246 + 100.0 * 6.226899147033691
Epoch 1830, val loss: 0.9108932018280029
Epoch 1840, training loss: 622.5836181640625 = 0.03515946865081787 + 100.0 * 6.225484848022461
Epoch 1840, val loss: 0.9133431911468506
Epoch 1850, training loss: 622.8192749023438 = 0.03452074155211449 + 100.0 * 6.227847576141357
Epoch 1850, val loss: 0.9168622493743896
Epoch 1860, training loss: 622.5903930664062 = 0.033873166888952255 + 100.0 * 6.225564956665039
Epoch 1860, val loss: 0.9200883507728577
Epoch 1870, training loss: 622.3945922851562 = 0.03325662761926651 + 100.0 * 6.223613262176514
Epoch 1870, val loss: 0.9231106638908386
Epoch 1880, training loss: 622.2999877929688 = 0.03265416622161865 + 100.0 * 6.222673416137695
Epoch 1880, val loss: 0.9263755083084106
Epoch 1890, training loss: 622.3009643554688 = 0.03207787871360779 + 100.0 * 6.222688674926758
Epoch 1890, val loss: 0.9297090768814087
Epoch 1900, training loss: 622.8253784179688 = 0.0315142460167408 + 100.0 * 6.227938652038574
Epoch 1900, val loss: 0.9325912594795227
Epoch 1910, training loss: 622.5230102539062 = 0.030952900648117065 + 100.0 * 6.224920749664307
Epoch 1910, val loss: 0.9360580444335938
Epoch 1920, training loss: 622.2807006835938 = 0.030390717089176178 + 100.0 * 6.222503185272217
Epoch 1920, val loss: 0.9385914206504822
Epoch 1930, training loss: 622.2178344726562 = 0.029863735660910606 + 100.0 * 6.221879959106445
Epoch 1930, val loss: 0.9422139525413513
Epoch 1940, training loss: 622.1520385742188 = 0.02934633567929268 + 100.0 * 6.221226692199707
Epoch 1940, val loss: 0.9450787305831909
Epoch 1950, training loss: 622.8098754882812 = 0.02884550765156746 + 100.0 * 6.227809906005859
Epoch 1950, val loss: 0.9485052227973938
Epoch 1960, training loss: 622.5563354492188 = 0.028354603797197342 + 100.0 * 6.225279808044434
Epoch 1960, val loss: 0.950762927532196
Epoch 1970, training loss: 622.2452392578125 = 0.02785026654601097 + 100.0 * 6.222174167633057
Epoch 1970, val loss: 0.9542686343193054
Epoch 1980, training loss: 622.0911254882812 = 0.02738960087299347 + 100.0 * 6.220637321472168
Epoch 1980, val loss: 0.9571117162704468
Epoch 1990, training loss: 622.0889892578125 = 0.026935800909996033 + 100.0 * 6.220620632171631
Epoch 1990, val loss: 0.9602113962173462
Epoch 2000, training loss: 622.3861694335938 = 0.026499489322304726 + 100.0 * 6.223596572875977
Epoch 2000, val loss: 0.9631124138832092
Epoch 2010, training loss: 622.0889892578125 = 0.02604733034968376 + 100.0 * 6.2206292152404785
Epoch 2010, val loss: 0.9657999873161316
Epoch 2020, training loss: 622.2484130859375 = 0.025614378973841667 + 100.0 * 6.222227573394775
Epoch 2020, val loss: 0.9689294695854187
Epoch 2030, training loss: 622.3369140625 = 0.025193633511662483 + 100.0 * 6.223116874694824
Epoch 2030, val loss: 0.9719712734222412
Epoch 2040, training loss: 621.9456787109375 = 0.024772344157099724 + 100.0 * 6.219208717346191
Epoch 2040, val loss: 0.9745081067085266
Epoch 2050, training loss: 621.9315795898438 = 0.024372410029172897 + 100.0 * 6.219072341918945
Epoch 2050, val loss: 0.9774980545043945
Epoch 2060, training loss: 621.9119262695312 = 0.023988494649529457 + 100.0 * 6.218879699707031
Epoch 2060, val loss: 0.9804387092590332
Epoch 2070, training loss: 622.1592407226562 = 0.023614605888724327 + 100.0 * 6.221356391906738
Epoch 2070, val loss: 0.9832395315170288
Epoch 2080, training loss: 621.8987426757812 = 0.02323828637599945 + 100.0 * 6.218754768371582
Epoch 2080, val loss: 0.9861878752708435
Epoch 2090, training loss: 622.0095825195312 = 0.022872675210237503 + 100.0 * 6.21986722946167
Epoch 2090, val loss: 0.9885753393173218
Epoch 2100, training loss: 621.9185791015625 = 0.02250918559730053 + 100.0 * 6.218960285186768
Epoch 2100, val loss: 0.9915631413459778
Epoch 2110, training loss: 622.0140991210938 = 0.022164512425661087 + 100.0 * 6.219919204711914
Epoch 2110, val loss: 0.9942718744277954
Epoch 2120, training loss: 621.8433227539062 = 0.021824633702635765 + 100.0 * 6.218214511871338
Epoch 2120, val loss: 0.9972322583198547
Epoch 2130, training loss: 621.9910888671875 = 0.021497046574950218 + 100.0 * 6.219696044921875
Epoch 2130, val loss: 0.9997462630271912
Epoch 2140, training loss: 621.782958984375 = 0.02116156555712223 + 100.0 * 6.217617511749268
Epoch 2140, val loss: 1.002106785774231
Epoch 2150, training loss: 622.0436401367188 = 0.02084517292678356 + 100.0 * 6.2202277183532715
Epoch 2150, val loss: 1.0047979354858398
Epoch 2160, training loss: 621.91455078125 = 0.020535748451948166 + 100.0 * 6.218940258026123
Epoch 2160, val loss: 1.0080541372299194
Epoch 2170, training loss: 622.103759765625 = 0.02022966742515564 + 100.0 * 6.2208356857299805
Epoch 2170, val loss: 1.0105667114257812
Epoch 2180, training loss: 621.7452392578125 = 0.01991944946348667 + 100.0 * 6.2172532081604
Epoch 2180, val loss: 1.0130161046981812
Epoch 2190, training loss: 621.7431030273438 = 0.019635964184999466 + 100.0 * 6.2172346115112305
Epoch 2190, val loss: 1.0157876014709473
Epoch 2200, training loss: 622.0342407226562 = 0.019355569034814835 + 100.0 * 6.220149040222168
Epoch 2200, val loss: 1.0184954404830933
Epoch 2210, training loss: 621.852294921875 = 0.01907126046717167 + 100.0 * 6.218332290649414
Epoch 2210, val loss: 1.0207501649856567
Epoch 2220, training loss: 621.676025390625 = 0.018794404342770576 + 100.0 * 6.216572284698486
Epoch 2220, val loss: 1.0232828855514526
Epoch 2230, training loss: 621.859375 = 0.01853048987686634 + 100.0 * 6.218408107757568
Epoch 2230, val loss: 1.0253762006759644
Epoch 2240, training loss: 621.6073608398438 = 0.018264012411236763 + 100.0 * 6.215890884399414
Epoch 2240, val loss: 1.0284205675125122
Epoch 2250, training loss: 621.627197265625 = 0.018008844926953316 + 100.0 * 6.216092109680176
Epoch 2250, val loss: 1.0313373804092407
Epoch 2260, training loss: 621.7250366210938 = 0.0177609845995903 + 100.0 * 6.217072486877441
Epoch 2260, val loss: 1.0337166786193848
Epoch 2270, training loss: 621.9696655273438 = 0.017515532672405243 + 100.0 * 6.219521522521973
Epoch 2270, val loss: 1.0362348556518555
Epoch 2280, training loss: 621.5289916992188 = 0.017268864437937737 + 100.0 * 6.21511697769165
Epoch 2280, val loss: 1.0380332469940186
Epoch 2290, training loss: 621.4746704101562 = 0.017028216272592545 + 100.0 * 6.214576721191406
Epoch 2290, val loss: 1.0407849550247192
Epoch 2300, training loss: 621.4970703125 = 0.016803603619337082 + 100.0 * 6.2148027420043945
Epoch 2300, val loss: 1.042922854423523
Epoch 2310, training loss: 622.0345458984375 = 0.016592962667346 + 100.0 * 6.220179557800293
Epoch 2310, val loss: 1.0455046892166138
Epoch 2320, training loss: 621.5662231445312 = 0.01635502278804779 + 100.0 * 6.215498447418213
Epoch 2320, val loss: 1.0484763383865356
Epoch 2330, training loss: 621.5108642578125 = 0.01613936759531498 + 100.0 * 6.214947700500488
Epoch 2330, val loss: 1.0501232147216797
Epoch 2340, training loss: 621.6796264648438 = 0.015929892659187317 + 100.0 * 6.216636657714844
Epoch 2340, val loss: 1.0528936386108398
Epoch 2350, training loss: 621.5477905273438 = 0.015715928748250008 + 100.0 * 6.215321063995361
Epoch 2350, val loss: 1.0548149347305298
Epoch 2360, training loss: 621.5217895507812 = 0.015511775389313698 + 100.0 * 6.215063095092773
Epoch 2360, val loss: 1.0574278831481934
Epoch 2370, training loss: 621.4766845703125 = 0.015313948504626751 + 100.0 * 6.214613437652588
Epoch 2370, val loss: 1.0600404739379883
Epoch 2380, training loss: 621.4443969726562 = 0.015120316296815872 + 100.0 * 6.214292526245117
Epoch 2380, val loss: 1.0622808933258057
Epoch 2390, training loss: 621.392578125 = 0.014929198659956455 + 100.0 * 6.213776111602783
Epoch 2390, val loss: 1.0646520853042603
Epoch 2400, training loss: 621.3753662109375 = 0.01474143099039793 + 100.0 * 6.213605880737305
Epoch 2400, val loss: 1.0664371252059937
Epoch 2410, training loss: 621.7572021484375 = 0.014561502262949944 + 100.0 * 6.217426300048828
Epoch 2410, val loss: 1.0684922933578491
Epoch 2420, training loss: 621.5327758789062 = 0.014368667267262936 + 100.0 * 6.215184211730957
Epoch 2420, val loss: 1.0710172653198242
Epoch 2430, training loss: 621.3408203125 = 0.014190594665706158 + 100.0 * 6.213266849517822
Epoch 2430, val loss: 1.0731405019760132
Epoch 2440, training loss: 621.337890625 = 0.01402019988745451 + 100.0 * 6.21323823928833
Epoch 2440, val loss: 1.0757770538330078
Epoch 2450, training loss: 621.6900634765625 = 0.013856175355613232 + 100.0 * 6.216762065887451
Epoch 2450, val loss: 1.0776021480560303
Epoch 2460, training loss: 621.437744140625 = 0.013680058531463146 + 100.0 * 6.214240550994873
Epoch 2460, val loss: 1.0804699659347534
Epoch 2470, training loss: 621.2831420898438 = 0.013514382764697075 + 100.0 * 6.212696552276611
Epoch 2470, val loss: 1.081952452659607
Epoch 2480, training loss: 621.4237670898438 = 0.013356453739106655 + 100.0 * 6.214103698730469
Epoch 2480, val loss: 1.0842736959457397
Epoch 2490, training loss: 621.22021484375 = 0.013191995210945606 + 100.0 * 6.212070465087891
Epoch 2490, val loss: 1.0860018730163574
Epoch 2500, training loss: 621.2015380859375 = 0.0130351223051548 + 100.0 * 6.211884498596191
Epoch 2500, val loss: 1.0885369777679443
Epoch 2510, training loss: 621.4528198242188 = 0.012886183336377144 + 100.0 * 6.214399337768555
Epoch 2510, val loss: 1.0899360179901123
Epoch 2520, training loss: 621.1109008789062 = 0.012734605930745602 + 100.0 * 6.210981369018555
Epoch 2520, val loss: 1.0927332639694214
Epoch 2530, training loss: 621.1185913085938 = 0.012588731944561005 + 100.0 * 6.211060047149658
Epoch 2530, val loss: 1.0949586629867554
Epoch 2540, training loss: 621.25390625 = 0.012447742745280266 + 100.0 * 6.212414264678955
Epoch 2540, val loss: 1.0969879627227783
Epoch 2550, training loss: 621.326904296875 = 0.012305784039199352 + 100.0 * 6.213145732879639
Epoch 2550, val loss: 1.0992162227630615
Epoch 2560, training loss: 621.2089233398438 = 0.012158336117863655 + 100.0 * 6.211967945098877
Epoch 2560, val loss: 1.100477695465088
Epoch 2570, training loss: 621.3375854492188 = 0.012029295787215233 + 100.0 * 6.213255405426025
Epoch 2570, val loss: 1.102839708328247
Epoch 2580, training loss: 621.0484008789062 = 0.011889306828379631 + 100.0 * 6.210364818572998
Epoch 2580, val loss: 1.1046319007873535
Epoch 2590, training loss: 621.1324462890625 = 0.01176320668309927 + 100.0 * 6.211206912994385
Epoch 2590, val loss: 1.1071248054504395
Epoch 2600, training loss: 621.58642578125 = 0.011635798960924149 + 100.0 * 6.215747833251953
Epoch 2600, val loss: 1.1084997653961182
Epoch 2610, training loss: 621.0890502929688 = 0.011498566716909409 + 100.0 * 6.210775852203369
Epoch 2610, val loss: 1.1105401515960693
Epoch 2620, training loss: 620.9574584960938 = 0.011373413726687431 + 100.0 * 6.209461212158203
Epoch 2620, val loss: 1.112457036972046
Epoch 2630, training loss: 620.8991088867188 = 0.01125327404588461 + 100.0 * 6.208878993988037
Epoch 2630, val loss: 1.1143803596496582
Epoch 2640, training loss: 621.1856689453125 = 0.011142787523567677 + 100.0 * 6.211745262145996
Epoch 2640, val loss: 1.1166386604309082
Epoch 2650, training loss: 620.9910888671875 = 0.011014568619430065 + 100.0 * 6.209801197052002
Epoch 2650, val loss: 1.1179261207580566
Epoch 2660, training loss: 620.9081420898438 = 0.01089379470795393 + 100.0 * 6.208972930908203
Epoch 2660, val loss: 1.1202895641326904
Epoch 2670, training loss: 620.9254150390625 = 0.010779366828501225 + 100.0 * 6.209146022796631
Epoch 2670, val loss: 1.1217819452285767
Epoch 2680, training loss: 621.5125122070312 = 0.010674838908016682 + 100.0 * 6.215018272399902
Epoch 2680, val loss: 1.124008059501648
Epoch 2690, training loss: 620.8428955078125 = 0.010555233806371689 + 100.0 * 6.2083234786987305
Epoch 2690, val loss: 1.1253490447998047
Epoch 2700, training loss: 620.7965087890625 = 0.010446706786751747 + 100.0 * 6.207860469818115
Epoch 2700, val loss: 1.127122163772583
Epoch 2710, training loss: 620.9931640625 = 0.010346321389079094 + 100.0 * 6.209827899932861
Epoch 2710, val loss: 1.1290936470031738
Epoch 2720, training loss: 621.202392578125 = 0.01024253573268652 + 100.0 * 6.211921691894531
Epoch 2720, val loss: 1.1307146549224854
Epoch 2730, training loss: 620.9720458984375 = 0.010129068046808243 + 100.0 * 6.209619045257568
Epoch 2730, val loss: 1.1326603889465332
Epoch 2740, training loss: 621.3729248046875 = 0.01003340259194374 + 100.0 * 6.213629245758057
Epoch 2740, val loss: 1.1337350606918335
Epoch 2750, training loss: 620.8289184570312 = 0.0099262660369277 + 100.0 * 6.208189964294434
Epoch 2750, val loss: 1.136151909828186
Epoch 2760, training loss: 620.7883911132812 = 0.009830023162066936 + 100.0 * 6.207785606384277
Epoch 2760, val loss: 1.137630581855774
Epoch 2770, training loss: 620.884765625 = 0.009734615683555603 + 100.0 * 6.2087507247924805
Epoch 2770, val loss: 1.1394591331481934
Epoch 2780, training loss: 621.0236206054688 = 0.009641334414482117 + 100.0 * 6.210139751434326
Epoch 2780, val loss: 1.1409990787506104
Epoch 2790, training loss: 620.8817749023438 = 0.009545777924358845 + 100.0 * 6.2087225914001465
Epoch 2790, val loss: 1.142838954925537
Epoch 2800, training loss: 620.9552612304688 = 0.009453492239117622 + 100.0 * 6.209457874298096
Epoch 2800, val loss: 1.144730806350708
Epoch 2810, training loss: 620.827880859375 = 0.009363332763314247 + 100.0 * 6.208185195922852
Epoch 2810, val loss: 1.1461962461471558
Epoch 2820, training loss: 620.7264404296875 = 0.009274177253246307 + 100.0 * 6.20717191696167
Epoch 2820, val loss: 1.1478880643844604
Epoch 2830, training loss: 620.755126953125 = 0.009189802221953869 + 100.0 * 6.207458972930908
Epoch 2830, val loss: 1.1497398614883423
Epoch 2840, training loss: 621.0318603515625 = 0.009105498902499676 + 100.0 * 6.2102274894714355
Epoch 2840, val loss: 1.151238203048706
Epoch 2850, training loss: 621.022705078125 = 0.009016728959977627 + 100.0 * 6.210136890411377
Epoch 2850, val loss: 1.1515361070632935
Epoch 2860, training loss: 620.6016845703125 = 0.00893027987331152 + 100.0 * 6.205927848815918
Epoch 2860, val loss: 1.1547690629959106
Epoch 2870, training loss: 620.56103515625 = 0.008847803808748722 + 100.0 * 6.205522060394287
Epoch 2870, val loss: 1.155611515045166
Epoch 2880, training loss: 620.63720703125 = 0.008770234882831573 + 100.0 * 6.206284046173096
Epoch 2880, val loss: 1.157360315322876
Epoch 2890, training loss: 621.2291870117188 = 0.008696183562278748 + 100.0 * 6.212205410003662
Epoch 2890, val loss: 1.1587165594100952
Epoch 2900, training loss: 620.8655395507812 = 0.008611827157437801 + 100.0 * 6.208569049835205
Epoch 2900, val loss: 1.1605528593063354
Epoch 2910, training loss: 620.7432250976562 = 0.008533632382750511 + 100.0 * 6.2073469161987305
Epoch 2910, val loss: 1.161913514137268
Epoch 2920, training loss: 620.7366943359375 = 0.00845501758158207 + 100.0 * 6.207282066345215
Epoch 2920, val loss: 1.1634790897369385
Epoch 2930, training loss: 620.7003784179688 = 0.008381771855056286 + 100.0 * 6.206920146942139
Epoch 2930, val loss: 1.1650937795639038
Epoch 2940, training loss: 620.6244506835938 = 0.008310724049806595 + 100.0 * 6.2061614990234375
Epoch 2940, val loss: 1.1662869453430176
Epoch 2950, training loss: 620.6373901367188 = 0.008239606395363808 + 100.0 * 6.206291198730469
Epoch 2950, val loss: 1.1681220531463623
Epoch 2960, training loss: 621.1349487304688 = 0.008168190717697144 + 100.0 * 6.211267471313477
Epoch 2960, val loss: 1.1698096990585327
Epoch 2970, training loss: 620.7155151367188 = 0.008097165264189243 + 100.0 * 6.207074165344238
Epoch 2970, val loss: 1.1707866191864014
Epoch 2980, training loss: 620.5154418945312 = 0.00802604854106903 + 100.0 * 6.205074310302734
Epoch 2980, val loss: 1.1727137565612793
Epoch 2990, training loss: 620.5194702148438 = 0.007960178889334202 + 100.0 * 6.20511531829834
Epoch 2990, val loss: 1.1738531589508057
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 861.640380859375 = 1.9583330154418945 + 100.0 * 8.596820831298828
Epoch 0, val loss: 1.9640557765960693
Epoch 10, training loss: 861.5419311523438 = 1.9496086835861206 + 100.0 * 8.59592342376709
Epoch 10, val loss: 1.9550145864486694
Epoch 20, training loss: 860.9423828125 = 1.9386496543884277 + 100.0 * 8.59003734588623
Epoch 20, val loss: 1.9437154531478882
Epoch 30, training loss: 856.9938354492188 = 1.9242160320281982 + 100.0 * 8.55069637298584
Epoch 30, val loss: 1.9286246299743652
Epoch 40, training loss: 831.452392578125 = 1.906489372253418 + 100.0 * 8.295458793640137
Epoch 40, val loss: 1.910211443901062
Epoch 50, training loss: 755.9370727539062 = 1.886319875717163 + 100.0 * 7.540507793426514
Epoch 50, val loss: 1.8897613286972046
Epoch 60, training loss: 723.17578125 = 1.8711433410644531 + 100.0 * 7.213046073913574
Epoch 60, val loss: 1.875683307647705
Epoch 70, training loss: 703.0382690429688 = 1.8610421419143677 + 100.0 * 7.011772632598877
Epoch 70, val loss: 1.8660246133804321
Epoch 80, training loss: 690.2437744140625 = 1.8505690097808838 + 100.0 * 6.883931636810303
Epoch 80, val loss: 1.855398416519165
Epoch 90, training loss: 680.5217895507812 = 1.8401185274124146 + 100.0 * 6.786816596984863
Epoch 90, val loss: 1.8449326753616333
Epoch 100, training loss: 674.0811157226562 = 1.8293377161026 + 100.0 * 6.722517490386963
Epoch 100, val loss: 1.8345794677734375
Epoch 110, training loss: 669.2801513671875 = 1.8193846940994263 + 100.0 * 6.674607753753662
Epoch 110, val loss: 1.824867606163025
Epoch 120, training loss: 664.87744140625 = 1.8101918697357178 + 100.0 * 6.630672454833984
Epoch 120, val loss: 1.8157150745391846
Epoch 130, training loss: 661.1897583007812 = 1.8014144897460938 + 100.0 * 6.593883514404297
Epoch 130, val loss: 1.8067594766616821
Epoch 140, training loss: 658.2158813476562 = 1.7927048206329346 + 100.0 * 6.5642313957214355
Epoch 140, val loss: 1.7979891300201416
Epoch 150, training loss: 655.6642456054688 = 1.7838457822799683 + 100.0 * 6.538803577423096
Epoch 150, val loss: 1.7892042398452759
Epoch 160, training loss: 653.7366333007812 = 1.7747108936309814 + 100.0 * 6.519618988037109
Epoch 160, val loss: 1.780247688293457
Epoch 170, training loss: 651.8603515625 = 1.7649927139282227 + 100.0 * 6.500953197479248
Epoch 170, val loss: 1.7709052562713623
Epoch 180, training loss: 650.3945922851562 = 1.7545820474624634 + 100.0 * 6.486400127410889
Epoch 180, val loss: 1.7610617876052856
Epoch 190, training loss: 648.8994750976562 = 1.7433652877807617 + 100.0 * 6.471561431884766
Epoch 190, val loss: 1.7505449056625366
Epoch 200, training loss: 647.6214599609375 = 1.731236457824707 + 100.0 * 6.458901882171631
Epoch 200, val loss: 1.7393449544906616
Epoch 210, training loss: 646.6845703125 = 1.7180874347686768 + 100.0 * 6.449665069580078
Epoch 210, val loss: 1.727248191833496
Epoch 220, training loss: 645.6129150390625 = 1.7036858797073364 + 100.0 * 6.43909215927124
Epoch 220, val loss: 1.7141989469528198
Epoch 230, training loss: 644.45263671875 = 1.68805992603302 + 100.0 * 6.427645683288574
Epoch 230, val loss: 1.7001113891601562
Epoch 240, training loss: 643.5064086914062 = 1.6711921691894531 + 100.0 * 6.418352127075195
Epoch 240, val loss: 1.6850029230117798
Epoch 250, training loss: 643.0521850585938 = 1.6530051231384277 + 100.0 * 6.413991451263428
Epoch 250, val loss: 1.668691873550415
Epoch 260, training loss: 642.0684204101562 = 1.6333463191986084 + 100.0 * 6.404350280761719
Epoch 260, val loss: 1.6513197422027588
Epoch 270, training loss: 641.25537109375 = 1.6122759580612183 + 100.0 * 6.396430969238281
Epoch 270, val loss: 1.6329376697540283
Epoch 280, training loss: 640.5405883789062 = 1.5900150537490845 + 100.0 * 6.389505386352539
Epoch 280, val loss: 1.6134577989578247
Epoch 290, training loss: 640.0057983398438 = 1.5664703845977783 + 100.0 * 6.384393215179443
Epoch 290, val loss: 1.5931774377822876
Epoch 300, training loss: 639.354248046875 = 1.541799783706665 + 100.0 * 6.378124237060547
Epoch 300, val loss: 1.571867823600769
Epoch 310, training loss: 638.8040161132812 = 1.5162259340286255 + 100.0 * 6.372878074645996
Epoch 310, val loss: 1.549957275390625
Epoch 320, training loss: 638.3609008789062 = 1.4898810386657715 + 100.0 * 6.368710517883301
Epoch 320, val loss: 1.5275684595108032
Epoch 330, training loss: 638.01904296875 = 1.4628815650939941 + 100.0 * 6.365561485290527
Epoch 330, val loss: 1.5048394203186035
Epoch 340, training loss: 637.3131103515625 = 1.4352502822875977 + 100.0 * 6.358778953552246
Epoch 340, val loss: 1.4818092584609985
Epoch 350, training loss: 636.8348388671875 = 1.4074028730392456 + 100.0 * 6.354274272918701
Epoch 350, val loss: 1.4586381912231445
Epoch 360, training loss: 636.614013671875 = 1.379327654838562 + 100.0 * 6.352346897125244
Epoch 360, val loss: 1.435448408126831
Epoch 370, training loss: 636.1139526367188 = 1.3510626554489136 + 100.0 * 6.347628593444824
Epoch 370, val loss: 1.4120471477508545
Epoch 380, training loss: 635.6099853515625 = 1.3228507041931152 + 100.0 * 6.342871189117432
Epoch 380, val loss: 1.3891098499298096
Epoch 390, training loss: 635.086181640625 = 1.2948307991027832 + 100.0 * 6.3379130363464355
Epoch 390, val loss: 1.3664147853851318
Epoch 400, training loss: 634.6920166015625 = 1.2670879364013672 + 100.0 * 6.334249496459961
Epoch 400, val loss: 1.3443142175674438
Epoch 410, training loss: 634.7883911132812 = 1.2396891117095947 + 100.0 * 6.335487365722656
Epoch 410, val loss: 1.322494387626648
Epoch 420, training loss: 634.2666015625 = 1.2123605012893677 + 100.0 * 6.33054256439209
Epoch 420, val loss: 1.3011977672576904
Epoch 430, training loss: 633.8301391601562 = 1.1855885982513428 + 100.0 * 6.32644510269165
Epoch 430, val loss: 1.2803003787994385
Epoch 440, training loss: 633.4166870117188 = 1.1593283414840698 + 100.0 * 6.322573661804199
Epoch 440, val loss: 1.2600880861282349
Epoch 450, training loss: 633.3302001953125 = 1.133629560470581 + 100.0 * 6.321966171264648
Epoch 450, val loss: 1.2405067682266235
Epoch 460, training loss: 633.0010375976562 = 1.1083004474639893 + 100.0 * 6.318927764892578
Epoch 460, val loss: 1.2216647863388062
Epoch 470, training loss: 632.6652221679688 = 1.0836066007614136 + 100.0 * 6.3158159255981445
Epoch 470, val loss: 1.2036640644073486
Epoch 480, training loss: 632.41357421875 = 1.059417963027954 + 100.0 * 6.313541412353516
Epoch 480, val loss: 1.1860846281051636
Epoch 490, training loss: 631.9935913085938 = 1.0359294414520264 + 100.0 * 6.309576511383057
Epoch 490, val loss: 1.1695226430892944
Epoch 500, training loss: 631.6973266601562 = 1.0131382942199707 + 100.0 * 6.30684232711792
Epoch 500, val loss: 1.1538197994232178
Epoch 510, training loss: 631.5501708984375 = 0.9910325407981873 + 100.0 * 6.305591583251953
Epoch 510, val loss: 1.1389577388763428
Epoch 520, training loss: 631.3731689453125 = 0.9694122672080994 + 100.0 * 6.304037570953369
Epoch 520, val loss: 1.124222755432129
Epoch 530, training loss: 631.0745239257812 = 0.9482963681221008 + 100.0 * 6.301262378692627
Epoch 530, val loss: 1.1109455823898315
Epoch 540, training loss: 630.8167114257812 = 0.9279394745826721 + 100.0 * 6.298887729644775
Epoch 540, val loss: 1.0979827642440796
Epoch 550, training loss: 631.79833984375 = 0.9081661701202393 + 100.0 * 6.308902263641357
Epoch 550, val loss: 1.085843563079834
Epoch 560, training loss: 630.6279296875 = 0.8888109922409058 + 100.0 * 6.297391414642334
Epoch 560, val loss: 1.0742285251617432
Epoch 570, training loss: 630.239501953125 = 0.8700953722000122 + 100.0 * 6.293694019317627
Epoch 570, val loss: 1.0635178089141846
Epoch 580, training loss: 629.9799194335938 = 0.8520181179046631 + 100.0 * 6.291279315948486
Epoch 580, val loss: 1.0535026788711548
Epoch 590, training loss: 629.7692260742188 = 0.8344321846961975 + 100.0 * 6.289348125457764
Epoch 590, val loss: 1.0441654920578003
Epoch 600, training loss: 630.3721923828125 = 0.8172900676727295 + 100.0 * 6.295549392700195
Epoch 600, val loss: 1.0351675748825073
Epoch 610, training loss: 629.5369873046875 = 0.8004398941993713 + 100.0 * 6.287365913391113
Epoch 610, val loss: 1.0267243385314941
Epoch 620, training loss: 629.5291748046875 = 0.7840713262557983 + 100.0 * 6.287451267242432
Epoch 620, val loss: 1.0187933444976807
Epoch 630, training loss: 629.111572265625 = 0.7680944204330444 + 100.0 * 6.283435344696045
Epoch 630, val loss: 1.011584758758545
Epoch 640, training loss: 628.9718627929688 = 0.7525922060012817 + 100.0 * 6.282192707061768
Epoch 640, val loss: 1.0046827793121338
Epoch 650, training loss: 629.7181396484375 = 0.7374430298805237 + 100.0 * 6.289806842803955
Epoch 650, val loss: 0.9981771111488342
Epoch 660, training loss: 628.7149047851562 = 0.7225522994995117 + 100.0 * 6.279923439025879
Epoch 660, val loss: 0.9919980764389038
Epoch 670, training loss: 628.74560546875 = 0.707997739315033 + 100.0 * 6.280375957489014
Epoch 670, val loss: 0.9864017963409424
Epoch 680, training loss: 628.3816528320312 = 0.6937926411628723 + 100.0 * 6.276878356933594
Epoch 680, val loss: 0.9808688759803772
Epoch 690, training loss: 628.3675537109375 = 0.6799318194389343 + 100.0 * 6.276876449584961
Epoch 690, val loss: 0.9758801460266113
Epoch 700, training loss: 628.357421875 = 0.6662551760673523 + 100.0 * 6.276911735534668
Epoch 700, val loss: 0.9713168740272522
Epoch 710, training loss: 628.057861328125 = 0.6528496742248535 + 100.0 * 6.274050235748291
Epoch 710, val loss: 0.9668033123016357
Epoch 720, training loss: 627.873291015625 = 0.6396886110305786 + 100.0 * 6.272335529327393
Epoch 720, val loss: 0.9626737236976624
Epoch 730, training loss: 628.1705322265625 = 0.6268021464347839 + 100.0 * 6.275436878204346
Epoch 730, val loss: 0.9587547183036804
Epoch 740, training loss: 627.8220825195312 = 0.6140826344490051 + 100.0 * 6.272079944610596
Epoch 740, val loss: 0.9553932547569275
Epoch 750, training loss: 627.5487060546875 = 0.6015577912330627 + 100.0 * 6.269471168518066
Epoch 750, val loss: 0.9520017504692078
Epoch 760, training loss: 627.4769287109375 = 0.5893152952194214 + 100.0 * 6.268876075744629
Epoch 760, val loss: 0.9489231705665588
Epoch 770, training loss: 627.5037231445312 = 0.5772320032119751 + 100.0 * 6.269265174865723
Epoch 770, val loss: 0.9460991621017456
Epoch 780, training loss: 627.4136962890625 = 0.5652294754981995 + 100.0 * 6.268484592437744
Epoch 780, val loss: 0.9429826736450195
Epoch 790, training loss: 627.1217651367188 = 0.5534369349479675 + 100.0 * 6.265682697296143
Epoch 790, val loss: 0.9404820799827576
Epoch 800, training loss: 626.9553833007812 = 0.5418779850006104 + 100.0 * 6.264134883880615
Epoch 800, val loss: 0.9382396936416626
Epoch 810, training loss: 626.9618530273438 = 0.5304762721061707 + 100.0 * 6.264313697814941
Epoch 810, val loss: 0.9360564351081848
Epoch 820, training loss: 626.8676147460938 = 0.519206702709198 + 100.0 * 6.263484001159668
Epoch 820, val loss: 0.9340255856513977
Epoch 830, training loss: 627.218505859375 = 0.5080450773239136 + 100.0 * 6.267104625701904
Epoch 830, val loss: 0.9318910837173462
Epoch 840, training loss: 626.7243041992188 = 0.4969395101070404 + 100.0 * 6.262273788452148
Epoch 840, val loss: 0.9303035736083984
Epoch 850, training loss: 626.5115966796875 = 0.48601436614990234 + 100.0 * 6.260255813598633
Epoch 850, val loss: 0.9286019206047058
Epoch 860, training loss: 626.3118286132812 = 0.4753720462322235 + 100.0 * 6.258364677429199
Epoch 860, val loss: 0.9273581504821777
Epoch 870, training loss: 626.5265502929688 = 0.464865505695343 + 100.0 * 6.260617256164551
Epoch 870, val loss: 0.9262083768844604
Epoch 880, training loss: 626.4508666992188 = 0.45428943634033203 + 100.0 * 6.259965896606445
Epoch 880, val loss: 0.9249979853630066
Epoch 890, training loss: 626.2501831054688 = 0.44391486048698425 + 100.0 * 6.258062839508057
Epoch 890, val loss: 0.9240097403526306
Epoch 900, training loss: 625.9833374023438 = 0.4337916374206543 + 100.0 * 6.255495548248291
Epoch 900, val loss: 0.92323237657547
Epoch 910, training loss: 625.9114379882812 = 0.4238699674606323 + 100.0 * 6.254875659942627
Epoch 910, val loss: 0.9228273630142212
Epoch 920, training loss: 626.9253540039062 = 0.41400814056396484 + 100.0 * 6.265113353729248
Epoch 920, val loss: 0.92238849401474
Epoch 930, training loss: 625.7744750976562 = 0.40424078702926636 + 100.0 * 6.253702640533447
Epoch 930, val loss: 0.9216893911361694
Epoch 940, training loss: 625.7195434570312 = 0.39473575353622437 + 100.0 * 6.25324821472168
Epoch 940, val loss: 0.9214653968811035
Epoch 950, training loss: 625.5628051757812 = 0.38547301292419434 + 100.0 * 6.251773357391357
Epoch 950, val loss: 0.9215267896652222
Epoch 960, training loss: 625.7000122070312 = 0.3764190971851349 + 100.0 * 6.253236293792725
Epoch 960, val loss: 0.9219824075698853
Epoch 970, training loss: 625.6646728515625 = 0.36740952730178833 + 100.0 * 6.25297212600708
Epoch 970, val loss: 0.921862781047821
Epoch 980, training loss: 625.5191040039062 = 0.3585636019706726 + 100.0 * 6.25160551071167
Epoch 980, val loss: 0.9221921563148499
Epoch 990, training loss: 625.2805786132812 = 0.3499810993671417 + 100.0 * 6.2493062019348145
Epoch 990, val loss: 0.9224848747253418
Epoch 1000, training loss: 625.1864013671875 = 0.3416314721107483 + 100.0 * 6.248447895050049
Epoch 1000, val loss: 0.9232064485549927
Epoch 1010, training loss: 625.6505737304688 = 0.3335014283657074 + 100.0 * 6.253170967102051
Epoch 1010, val loss: 0.924199104309082
Epoch 1020, training loss: 625.5584716796875 = 0.325349897146225 + 100.0 * 6.252330780029297
Epoch 1020, val loss: 0.9243022203445435
Epoch 1030, training loss: 625.052490234375 = 0.31742438673973083 + 100.0 * 6.247350215911865
Epoch 1030, val loss: 0.9253472685813904
Epoch 1040, training loss: 624.9197998046875 = 0.309810072183609 + 100.0 * 6.246099948883057
Epoch 1040, val loss: 0.9266313910484314
Epoch 1050, training loss: 624.8601684570312 = 0.30240944027900696 + 100.0 * 6.245577335357666
Epoch 1050, val loss: 0.9277746081352234
Epoch 1060, training loss: 625.7487182617188 = 0.2952094078063965 + 100.0 * 6.25453519821167
Epoch 1060, val loss: 0.9291206002235413
Epoch 1070, training loss: 625.064208984375 = 0.28797033429145813 + 100.0 * 6.247762680053711
Epoch 1070, val loss: 0.9305440783500671
Epoch 1080, training loss: 624.7113647460938 = 0.28097522258758545 + 100.0 * 6.244304180145264
Epoch 1080, val loss: 0.9319460391998291
Epoch 1090, training loss: 624.8115844726562 = 0.2742456793785095 + 100.0 * 6.245373249053955
Epoch 1090, val loss: 0.9335875511169434
Epoch 1100, training loss: 624.5732421875 = 0.2676449418067932 + 100.0 * 6.243056297302246
Epoch 1100, val loss: 0.9353652000427246
Epoch 1110, training loss: 624.676025390625 = 0.2612375020980835 + 100.0 * 6.244147777557373
Epoch 1110, val loss: 0.9370858073234558
Epoch 1120, training loss: 624.62890625 = 0.25493577122688293 + 100.0 * 6.243739604949951
Epoch 1120, val loss: 0.9388478398323059
Epoch 1130, training loss: 624.4278564453125 = 0.24875250458717346 + 100.0 * 6.241790771484375
Epoch 1130, val loss: 0.940687358379364
Epoch 1140, training loss: 624.3823852539062 = 0.24279797077178955 + 100.0 * 6.241395950317383
Epoch 1140, val loss: 0.9429063200950623
Epoch 1150, training loss: 624.5558471679688 = 0.2369835525751114 + 100.0 * 6.243188381195068
Epoch 1150, val loss: 0.9448314309120178
Epoch 1160, training loss: 624.4558715820312 = 0.23129862546920776 + 100.0 * 6.242245197296143
Epoch 1160, val loss: 0.9474489092826843
Epoch 1170, training loss: 624.427734375 = 0.2257053405046463 + 100.0 * 6.242020130157471
Epoch 1170, val loss: 0.9495669007301331
Epoch 1180, training loss: 624.3145141601562 = 0.22031442821025848 + 100.0 * 6.240942001342773
Epoch 1180, val loss: 0.9519550204277039
Epoch 1190, training loss: 624.083984375 = 0.21502067148685455 + 100.0 * 6.238689422607422
Epoch 1190, val loss: 0.9543707966804504
Epoch 1200, training loss: 624.0226440429688 = 0.2099309116601944 + 100.0 * 6.238126754760742
Epoch 1200, val loss: 0.9570374488830566
Epoch 1210, training loss: 624.3074340820312 = 0.20494888722896576 + 100.0 * 6.241024494171143
Epoch 1210, val loss: 0.9595513343811035
Epoch 1220, training loss: 624.2693481445312 = 0.20002628862857819 + 100.0 * 6.240693092346191
Epoch 1220, val loss: 0.9620766043663025
Epoch 1230, training loss: 624.0245361328125 = 0.19522245228290558 + 100.0 * 6.238292694091797
Epoch 1230, val loss: 0.9649935364723206
Epoch 1240, training loss: 624.1074829101562 = 0.19053535163402557 + 100.0 * 6.239169120788574
Epoch 1240, val loss: 0.9674109816551208
Epoch 1250, training loss: 623.8421630859375 = 0.18598328530788422 + 100.0 * 6.2365617752075195
Epoch 1250, val loss: 0.970118522644043
Epoch 1260, training loss: 623.7368774414062 = 0.18158099055290222 + 100.0 * 6.23555326461792
Epoch 1260, val loss: 0.9734029173851013
Epoch 1270, training loss: 623.7214965820312 = 0.17728538811206818 + 100.0 * 6.235442161560059
Epoch 1270, val loss: 0.976298451423645
Epoch 1280, training loss: 624.1547241210938 = 0.17306193709373474 + 100.0 * 6.239816665649414
Epoch 1280, val loss: 0.9790780544281006
Epoch 1290, training loss: 623.9935302734375 = 0.1689254492521286 + 100.0 * 6.238245964050293
Epoch 1290, val loss: 0.9819023013114929
Epoch 1300, training loss: 623.5698852539062 = 0.16487008333206177 + 100.0 * 6.234050273895264
Epoch 1300, val loss: 0.9850673079490662
Epoch 1310, training loss: 623.4301147460938 = 0.16099965572357178 + 100.0 * 6.232691287994385
Epoch 1310, val loss: 0.9880728125572205
Epoch 1320, training loss: 623.6704711914062 = 0.15724118053913116 + 100.0 * 6.235132217407227
Epoch 1320, val loss: 0.9912222623825073
Epoch 1330, training loss: 623.4570922851562 = 0.15350934863090515 + 100.0 * 6.233036041259766
Epoch 1330, val loss: 0.9945685267448425
Epoch 1340, training loss: 623.5850830078125 = 0.14984992146492004 + 100.0 * 6.2343525886535645
Epoch 1340, val loss: 0.9974284768104553
Epoch 1350, training loss: 623.44921875 = 0.1462913453578949 + 100.0 * 6.233028888702393
Epoch 1350, val loss: 1.00059974193573
Epoch 1360, training loss: 623.3389282226562 = 0.14286842942237854 + 100.0 * 6.231960296630859
Epoch 1360, val loss: 1.0039182901382446
Epoch 1370, training loss: 623.5380859375 = 0.1395420879125595 + 100.0 * 6.233985900878906
Epoch 1370, val loss: 1.0067673921585083
Epoch 1380, training loss: 623.2708129882812 = 0.13625332713127136 + 100.0 * 6.2313456535339355
Epoch 1380, val loss: 1.0106920003890991
Epoch 1390, training loss: 623.1778564453125 = 0.13304917514324188 + 100.0 * 6.230448246002197
Epoch 1390, val loss: 1.013714075088501
Epoch 1400, training loss: 623.39013671875 = 0.12998712062835693 + 100.0 * 6.232601165771484
Epoch 1400, val loss: 1.0173195600509644
Epoch 1410, training loss: 623.051025390625 = 0.1269301474094391 + 100.0 * 6.229240894317627
Epoch 1410, val loss: 1.0205384492874146
Epoch 1420, training loss: 623.0015869140625 = 0.12396284192800522 + 100.0 * 6.228776454925537
Epoch 1420, val loss: 1.024383783340454
Epoch 1430, training loss: 623.09375 = 0.12111314386129379 + 100.0 * 6.229726314544678
Epoch 1430, val loss: 1.0275565385818481
Epoch 1440, training loss: 623.486572265625 = 0.11829379200935364 + 100.0 * 6.233682632446289
Epoch 1440, val loss: 1.0310100317001343
Epoch 1450, training loss: 622.9716186523438 = 0.11551036685705185 + 100.0 * 6.228560924530029
Epoch 1450, val loss: 1.0346097946166992
Epoch 1460, training loss: 622.8585815429688 = 0.11285196989774704 + 100.0 * 6.227457523345947
Epoch 1460, val loss: 1.0384495258331299
Epoch 1470, training loss: 622.8093872070312 = 0.11027315258979797 + 100.0 * 6.226990699768066
Epoch 1470, val loss: 1.0418838262557983
Epoch 1480, training loss: 623.549560546875 = 0.10776310414075851 + 100.0 * 6.234417915344238
Epoch 1480, val loss: 1.0455024242401123
Epoch 1490, training loss: 622.9862670898438 = 0.10529181361198425 + 100.0 * 6.228809833526611
Epoch 1490, val loss: 1.0490442514419556
Epoch 1500, training loss: 622.7996826171875 = 0.10285634547472 + 100.0 * 6.226968288421631
Epoch 1500, val loss: 1.0526360273361206
Epoch 1510, training loss: 622.6893310546875 = 0.10055719316005707 + 100.0 * 6.225887298583984
Epoch 1510, val loss: 1.0567063093185425
Epoch 1520, training loss: 622.8692626953125 = 0.09831533581018448 + 100.0 * 6.2277092933654785
Epoch 1520, val loss: 1.060676097869873
Epoch 1530, training loss: 622.7528686523438 = 0.09608400613069534 + 100.0 * 6.226568222045898
Epoch 1530, val loss: 1.0637176036834717
Epoch 1540, training loss: 622.6963500976562 = 0.09390322864055634 + 100.0 * 6.226024150848389
Epoch 1540, val loss: 1.0675657987594604
Epoch 1550, training loss: 623.2553100585938 = 0.09181230515241623 + 100.0 * 6.231635093688965
Epoch 1550, val loss: 1.0711312294006348
Epoch 1560, training loss: 622.8570556640625 = 0.08973057568073273 + 100.0 * 6.227673053741455
Epoch 1560, val loss: 1.0751862525939941
Epoch 1570, training loss: 622.5699462890625 = 0.08773686736822128 + 100.0 * 6.224822521209717
Epoch 1570, val loss: 1.0788425207138062
Epoch 1580, training loss: 622.5015869140625 = 0.08581782132387161 + 100.0 * 6.224157810211182
Epoch 1580, val loss: 1.082587718963623
Epoch 1590, training loss: 622.6082153320312 = 0.08395326882600784 + 100.0 * 6.225243091583252
Epoch 1590, val loss: 1.0865551233291626
Epoch 1600, training loss: 622.6044311523438 = 0.0821145549416542 + 100.0 * 6.225223541259766
Epoch 1600, val loss: 1.090416431427002
Epoch 1610, training loss: 622.7410278320312 = 0.08032000064849854 + 100.0 * 6.226607322692871
Epoch 1610, val loss: 1.0942649841308594
Epoch 1620, training loss: 622.6993408203125 = 0.07855914533138275 + 100.0 * 6.226207733154297
Epoch 1620, val loss: 1.0977814197540283
Epoch 1630, training loss: 622.3636474609375 = 0.07685477286577225 + 100.0 * 6.222867965698242
Epoch 1630, val loss: 1.1017717123031616
Epoch 1640, training loss: 622.2626342773438 = 0.07521423697471619 + 100.0 * 6.221874237060547
Epoch 1640, val loss: 1.105331301689148
Epoch 1650, training loss: 622.34130859375 = 0.07362687587738037 + 100.0 * 6.222677230834961
Epoch 1650, val loss: 1.1090515851974487
Epoch 1660, training loss: 622.5758666992188 = 0.07206322997808456 + 100.0 * 6.225037574768066
Epoch 1660, val loss: 1.112640619277954
Epoch 1670, training loss: 622.519287109375 = 0.07054374366998672 + 100.0 * 6.2244873046875
Epoch 1670, val loss: 1.116817593574524
Epoch 1680, training loss: 622.671875 = 0.069041408598423 + 100.0 * 6.2260284423828125
Epoch 1680, val loss: 1.120422124862671
Epoch 1690, training loss: 622.1912231445312 = 0.06756161153316498 + 100.0 * 6.221236705780029
Epoch 1690, val loss: 1.1240205764770508
Epoch 1700, training loss: 622.1973266601562 = 0.06615564227104187 + 100.0 * 6.221311569213867
Epoch 1700, val loss: 1.1280438899993896
Epoch 1710, training loss: 622.3789672851562 = 0.06479308009147644 + 100.0 * 6.223141670227051
Epoch 1710, val loss: 1.1317139863967896
Epoch 1720, training loss: 622.051513671875 = 0.06344631314277649 + 100.0 * 6.2198805809021
Epoch 1720, val loss: 1.1356126070022583
Epoch 1730, training loss: 622.1732788085938 = 0.06215648725628853 + 100.0 * 6.221111297607422
Epoch 1730, val loss: 1.1393373012542725
Epoch 1740, training loss: 623.0712890625 = 0.0608941949903965 + 100.0 * 6.230103492736816
Epoch 1740, val loss: 1.1434199810028076
Epoch 1750, training loss: 622.31005859375 = 0.05959372967481613 + 100.0 * 6.222504615783691
Epoch 1750, val loss: 1.1467660665512085
Epoch 1760, training loss: 621.9173583984375 = 0.05837460607290268 + 100.0 * 6.218590259552002
Epoch 1760, val loss: 1.150708556175232
Epoch 1770, training loss: 621.878662109375 = 0.05721237510442734 + 100.0 * 6.218214511871338
Epoch 1770, val loss: 1.1544777154922485
Epoch 1780, training loss: 621.910888671875 = 0.05608890950679779 + 100.0 * 6.218547821044922
Epoch 1780, val loss: 1.1582902669906616
Epoch 1790, training loss: 622.8985595703125 = 0.05499989539384842 + 100.0 * 6.228435516357422
Epoch 1790, val loss: 1.1616326570510864
Epoch 1800, training loss: 622.21337890625 = 0.053859077394008636 + 100.0 * 6.221595764160156
Epoch 1800, val loss: 1.1654795408248901
Epoch 1810, training loss: 622.2138671875 = 0.05279093235731125 + 100.0 * 6.2216105461120605
Epoch 1810, val loss: 1.169345498085022
Epoch 1820, training loss: 621.8366088867188 = 0.051729992032051086 + 100.0 * 6.217848300933838
Epoch 1820, val loss: 1.1727772951126099
Epoch 1830, training loss: 621.8287963867188 = 0.05072357505559921 + 100.0 * 6.217781066894531
Epoch 1830, val loss: 1.1769201755523682
Epoch 1840, training loss: 621.8209228515625 = 0.049751631915569305 + 100.0 * 6.217711925506592
Epoch 1840, val loss: 1.1806514263153076
Epoch 1850, training loss: 622.0767822265625 = 0.0487949475646019 + 100.0 * 6.220280170440674
Epoch 1850, val loss: 1.184380292892456
Epoch 1860, training loss: 621.9752807617188 = 0.04784705117344856 + 100.0 * 6.219274044036865
Epoch 1860, val loss: 1.1875194311141968
Epoch 1870, training loss: 621.83837890625 = 0.04692571610212326 + 100.0 * 6.217914581298828
Epoch 1870, val loss: 1.1912572383880615
Epoch 1880, training loss: 621.7257080078125 = 0.04602586477994919 + 100.0 * 6.216796875
Epoch 1880, val loss: 1.1950738430023193
Epoch 1890, training loss: 621.8114013671875 = 0.045159824192523956 + 100.0 * 6.217662334442139
Epoch 1890, val loss: 1.199238896369934
Epoch 1900, training loss: 621.76416015625 = 0.04430501163005829 + 100.0 * 6.217198371887207
Epoch 1900, val loss: 1.2028230428695679
Epoch 1910, training loss: 621.964599609375 = 0.04347934573888779 + 100.0 * 6.219211578369141
Epoch 1910, val loss: 1.2063173055648804
Epoch 1920, training loss: 621.9360961914062 = 0.04264663904905319 + 100.0 * 6.218934535980225
Epoch 1920, val loss: 1.2093514204025269
Epoch 1930, training loss: 621.6045532226562 = 0.04184116795659065 + 100.0 * 6.2156267166137695
Epoch 1930, val loss: 1.2134031057357788
Epoch 1940, training loss: 621.5282592773438 = 0.041068244725465775 + 100.0 * 6.214871883392334
Epoch 1940, val loss: 1.216848611831665
Epoch 1950, training loss: 621.5524291992188 = 0.040329694747924805 + 100.0 * 6.215120792388916
Epoch 1950, val loss: 1.2205597162246704
Epoch 1960, training loss: 622.1389770507812 = 0.039610348641872406 + 100.0 * 6.220993518829346
Epoch 1960, val loss: 1.2241302728652954
Epoch 1970, training loss: 621.6444091796875 = 0.038865603506565094 + 100.0 * 6.216055393218994
Epoch 1970, val loss: 1.2279129028320312
Epoch 1980, training loss: 621.541259765625 = 0.038159918040037155 + 100.0 * 6.215031147003174
Epoch 1980, val loss: 1.2314432859420776
Epoch 1990, training loss: 621.6874389648438 = 0.03748698532581329 + 100.0 * 6.2164998054504395
Epoch 1990, val loss: 1.2352044582366943
Epoch 2000, training loss: 621.3626098632812 = 0.03680827096104622 + 100.0 * 6.213257789611816
Epoch 2000, val loss: 1.238551378250122
Epoch 2010, training loss: 621.5062255859375 = 0.036166951060295105 + 100.0 * 6.214700222015381
Epoch 2010, val loss: 1.24213707447052
Epoch 2020, training loss: 621.90673828125 = 0.035532064735889435 + 100.0 * 6.218712329864502
Epoch 2020, val loss: 1.2459474802017212
Epoch 2030, training loss: 621.5454711914062 = 0.03489277511835098 + 100.0 * 6.2151055335998535
Epoch 2030, val loss: 1.2487667798995972
Epoch 2040, training loss: 621.3499755859375 = 0.03428751975297928 + 100.0 * 6.213156700134277
Epoch 2040, val loss: 1.2524583339691162
Epoch 2050, training loss: 621.5874633789062 = 0.03370139002799988 + 100.0 * 6.2155375480651855
Epoch 2050, val loss: 1.2556945085525513
Epoch 2060, training loss: 621.3956298828125 = 0.033116333186626434 + 100.0 * 6.213624954223633
Epoch 2060, val loss: 1.2592304944992065
Epoch 2070, training loss: 621.3843994140625 = 0.0325474888086319 + 100.0 * 6.2135186195373535
Epoch 2070, val loss: 1.2627313137054443
Epoch 2080, training loss: 621.464599609375 = 0.03199741616845131 + 100.0 * 6.214325904846191
Epoch 2080, val loss: 1.2655125856399536
Epoch 2090, training loss: 621.2490844726562 = 0.03145356848835945 + 100.0 * 6.212176322937012
Epoch 2090, val loss: 1.2695764303207397
Epoch 2100, training loss: 621.3373413085938 = 0.030926145613193512 + 100.0 * 6.213063716888428
Epoch 2100, val loss: 1.2730810642242432
Epoch 2110, training loss: 621.480712890625 = 0.030408496037125587 + 100.0 * 6.214503288269043
Epoch 2110, val loss: 1.2761247158050537
Epoch 2120, training loss: 621.60986328125 = 0.02989809587597847 + 100.0 * 6.215799331665039
Epoch 2120, val loss: 1.279545545578003
Epoch 2130, training loss: 621.1436157226562 = 0.029391679912805557 + 100.0 * 6.211142063140869
Epoch 2130, val loss: 1.2828489542007446
Epoch 2140, training loss: 621.0828857421875 = 0.028912758454680443 + 100.0 * 6.2105393409729
Epoch 2140, val loss: 1.2863514423370361
Epoch 2150, training loss: 621.0725708007812 = 0.02845229022204876 + 100.0 * 6.2104411125183105
Epoch 2150, val loss: 1.2897430658340454
Epoch 2160, training loss: 621.3328857421875 = 0.028007058426737785 + 100.0 * 6.213048458099365
Epoch 2160, val loss: 1.2931369543075562
Epoch 2170, training loss: 621.1793212890625 = 0.027552414685487747 + 100.0 * 6.211517810821533
Epoch 2170, val loss: 1.2962316274642944
Epoch 2180, training loss: 621.404296875 = 0.027113476768136024 + 100.0 * 6.213771820068359
Epoch 2180, val loss: 1.2991061210632324
Epoch 2190, training loss: 621.2724609375 = 0.026671936735510826 + 100.0 * 6.212457656860352
Epoch 2190, val loss: 1.3024717569351196
Epoch 2200, training loss: 621.4500122070312 = 0.02624465525150299 + 100.0 * 6.214237689971924
Epoch 2200, val loss: 1.3059945106506348
Epoch 2210, training loss: 621.0487060546875 = 0.025828784331679344 + 100.0 * 6.21022891998291
Epoch 2210, val loss: 1.3088512420654297
Epoch 2220, training loss: 620.9137573242188 = 0.025428295135498047 + 100.0 * 6.208883285522461
Epoch 2220, val loss: 1.3122062683105469
Epoch 2230, training loss: 620.9599609375 = 0.025044158101081848 + 100.0 * 6.209349155426025
Epoch 2230, val loss: 1.31540846824646
Epoch 2240, training loss: 621.440673828125 = 0.0246709194034338 + 100.0 * 6.214160442352295
Epoch 2240, val loss: 1.318311095237732
Epoch 2250, training loss: 621.2875366210938 = 0.02427765354514122 + 100.0 * 6.212632179260254
Epoch 2250, val loss: 1.3215051889419556
Epoch 2260, training loss: 621.056884765625 = 0.023901579901576042 + 100.0 * 6.210329532623291
Epoch 2260, val loss: 1.3248716592788696
Epoch 2270, training loss: 620.910888671875 = 0.023539716377854347 + 100.0 * 6.208873271942139
Epoch 2270, val loss: 1.327820062637329
Epoch 2280, training loss: 620.9752197265625 = 0.02319447323679924 + 100.0 * 6.20952033996582
Epoch 2280, val loss: 1.3309844732284546
Epoch 2290, training loss: 621.3820190429688 = 0.022853750735521317 + 100.0 * 6.213592052459717
Epoch 2290, val loss: 1.3343873023986816
Epoch 2300, training loss: 620.91748046875 = 0.02250036410987377 + 100.0 * 6.208949565887451
Epoch 2300, val loss: 1.336488962173462
Epoch 2310, training loss: 620.7538452148438 = 0.02216912992298603 + 100.0 * 6.207316875457764
Epoch 2310, val loss: 1.3400248289108276
Epoch 2320, training loss: 620.7341918945312 = 0.021851330995559692 + 100.0 * 6.207123279571533
Epoch 2320, val loss: 1.3428955078125
Epoch 2330, training loss: 621.2546997070312 = 0.02155468612909317 + 100.0 * 6.212331771850586
Epoch 2330, val loss: 1.3452448844909668
Epoch 2340, training loss: 620.7824096679688 = 0.021229831501841545 + 100.0 * 6.207611560821533
Epoch 2340, val loss: 1.348989725112915
Epoch 2350, training loss: 620.9633178710938 = 0.020922401919960976 + 100.0 * 6.209424018859863
Epoch 2350, val loss: 1.3514213562011719
Epoch 2360, training loss: 620.8603515625 = 0.020624715834856033 + 100.0 * 6.208397388458252
Epoch 2360, val loss: 1.3548328876495361
Epoch 2370, training loss: 620.7399291992188 = 0.02032722719013691 + 100.0 * 6.207196235656738
Epoch 2370, val loss: 1.3574517965316772
Epoch 2380, training loss: 620.8764038085938 = 0.020048392936587334 + 100.0 * 6.208563804626465
Epoch 2380, val loss: 1.360534429550171
Epoch 2390, training loss: 621.023193359375 = 0.019774915650486946 + 100.0 * 6.210034370422363
Epoch 2390, val loss: 1.3630790710449219
Epoch 2400, training loss: 620.8057861328125 = 0.019497599452733994 + 100.0 * 6.2078633308410645
Epoch 2400, val loss: 1.3661079406738281
Epoch 2410, training loss: 620.675048828125 = 0.019222136586904526 + 100.0 * 6.2065582275390625
Epoch 2410, val loss: 1.3692755699157715
Epoch 2420, training loss: 620.74755859375 = 0.01896720565855503 + 100.0 * 6.2072858810424805
Epoch 2420, val loss: 1.372101068496704
Epoch 2430, training loss: 621.3087768554688 = 0.018715551123023033 + 100.0 * 6.212900638580322
Epoch 2430, val loss: 1.374974250793457
Epoch 2440, training loss: 620.758056640625 = 0.01844732090830803 + 100.0 * 6.207396030426025
Epoch 2440, val loss: 1.3776519298553467
Epoch 2450, training loss: 620.5568237304688 = 0.018196478486061096 + 100.0 * 6.205386638641357
Epoch 2450, val loss: 1.380499243736267
Epoch 2460, training loss: 620.5308227539062 = 0.01796094700694084 + 100.0 * 6.2051286697387695
Epoch 2460, val loss: 1.3835266828536987
Epoch 2470, training loss: 620.648193359375 = 0.01773451268672943 + 100.0 * 6.20630407333374
Epoch 2470, val loss: 1.3864481449127197
Epoch 2480, training loss: 621.0927124023438 = 0.01750323735177517 + 100.0 * 6.210752010345459
Epoch 2480, val loss: 1.3890825510025024
Epoch 2490, training loss: 620.8739624023438 = 0.01726732961833477 + 100.0 * 6.208567142486572
Epoch 2490, val loss: 1.3911417722702026
Epoch 2500, training loss: 620.6115112304688 = 0.017038529738783836 + 100.0 * 6.205945014953613
Epoch 2500, val loss: 1.3944753408432007
Epoch 2510, training loss: 620.7510986328125 = 0.016827000305056572 + 100.0 * 6.207342624664307
Epoch 2510, val loss: 1.3967275619506836
Epoch 2520, training loss: 620.5426025390625 = 0.016607418656349182 + 100.0 * 6.205259799957275
Epoch 2520, val loss: 1.3992908000946045
Epoch 2530, training loss: 620.4832763671875 = 0.016390107572078705 + 100.0 * 6.2046685218811035
Epoch 2530, val loss: 1.4021679162979126
Epoch 2540, training loss: 620.499267578125 = 0.01618960127234459 + 100.0 * 6.204831123352051
Epoch 2540, val loss: 1.405055046081543
Epoch 2550, training loss: 621.235595703125 = 0.01599222980439663 + 100.0 * 6.212195873260498
Epoch 2550, val loss: 1.407882571220398
Epoch 2560, training loss: 620.6150512695312 = 0.015786360949277878 + 100.0 * 6.205992698669434
Epoch 2560, val loss: 1.4097344875335693
Epoch 2570, training loss: 620.452392578125 = 0.015588922426104546 + 100.0 * 6.2043681144714355
Epoch 2570, val loss: 1.4127012491226196
Epoch 2580, training loss: 621.0489501953125 = 0.01540860254317522 + 100.0 * 6.210335731506348
Epoch 2580, val loss: 1.4150390625
Epoch 2590, training loss: 620.4373779296875 = 0.015208126045763493 + 100.0 * 6.204221725463867
Epoch 2590, val loss: 1.417309045791626
Epoch 2600, training loss: 620.3846435546875 = 0.01502089574933052 + 100.0 * 6.203696250915527
Epoch 2600, val loss: 1.4199986457824707
Epoch 2610, training loss: 620.3981323242188 = 0.014843184500932693 + 100.0 * 6.203833103179932
Epoch 2610, val loss: 1.422827124595642
Epoch 2620, training loss: 620.5750732421875 = 0.014671070501208305 + 100.0 * 6.205604553222656
Epoch 2620, val loss: 1.4251580238342285
Epoch 2630, training loss: 620.525146484375 = 0.014494264498353004 + 100.0 * 6.205106735229492
Epoch 2630, val loss: 1.4281898736953735
Epoch 2640, training loss: 620.5186767578125 = 0.014319263398647308 + 100.0 * 6.205043315887451
Epoch 2640, val loss: 1.430317997932434
Epoch 2650, training loss: 620.390869140625 = 0.01415029913187027 + 100.0 * 6.2037672996521
Epoch 2650, val loss: 1.4325783252716064
Epoch 2660, training loss: 620.3056640625 = 0.01398756168782711 + 100.0 * 6.202917098999023
Epoch 2660, val loss: 1.4352203607559204
Epoch 2670, training loss: 620.9987182617188 = 0.013829441741108894 + 100.0 * 6.209848880767822
Epoch 2670, val loss: 1.4368363618850708
Epoch 2680, training loss: 620.5484008789062 = 0.013668162748217583 + 100.0 * 6.205347537994385
Epoch 2680, val loss: 1.440843939781189
Epoch 2690, training loss: 620.3405151367188 = 0.013503857888281345 + 100.0 * 6.203270435333252
Epoch 2690, val loss: 1.4423760175704956
Epoch 2700, training loss: 620.235107421875 = 0.013354788534343243 + 100.0 * 6.2022175788879395
Epoch 2700, val loss: 1.4454790353775024
Epoch 2710, training loss: 620.4119262695312 = 0.013208194635808468 + 100.0 * 6.2039875984191895
Epoch 2710, val loss: 1.4476085901260376
Epoch 2720, training loss: 620.3734130859375 = 0.013058103621006012 + 100.0 * 6.203603744506836
Epoch 2720, val loss: 1.450198769569397
Epoch 2730, training loss: 620.6559448242188 = 0.012908713892102242 + 100.0 * 6.206430912017822
Epoch 2730, val loss: 1.452285647392273
Epoch 2740, training loss: 620.5479736328125 = 0.01275930367410183 + 100.0 * 6.205352306365967
Epoch 2740, val loss: 1.454397439956665
Epoch 2750, training loss: 620.244384765625 = 0.01261343527585268 + 100.0 * 6.202317714691162
Epoch 2750, val loss: 1.4568449258804321
Epoch 2760, training loss: 620.1889038085938 = 0.012478599324822426 + 100.0 * 6.2017645835876465
Epoch 2760, val loss: 1.4595078229904175
Epoch 2770, training loss: 620.5980224609375 = 0.012347519397735596 + 100.0 * 6.205856800079346
Epoch 2770, val loss: 1.4618842601776123
Epoch 2780, training loss: 620.1214599609375 = 0.012205605395138264 + 100.0 * 6.20109224319458
Epoch 2780, val loss: 1.4630982875823975
Epoch 2790, training loss: 620.0816650390625 = 0.012072848156094551 + 100.0 * 6.200695514678955
Epoch 2790, val loss: 1.4662679433822632
Epoch 2800, training loss: 620.2167358398438 = 0.011947846971452236 + 100.0 * 6.202047824859619
Epoch 2800, val loss: 1.4682589769363403
Epoch 2810, training loss: 620.4039916992188 = 0.011823650449514389 + 100.0 * 6.203921794891357
Epoch 2810, val loss: 1.4702492952346802
Epoch 2820, training loss: 620.260009765625 = 0.011691858991980553 + 100.0 * 6.202483177185059
Epoch 2820, val loss: 1.4728230237960815
Epoch 2830, training loss: 620.1500854492188 = 0.011570293456315994 + 100.0 * 6.201385021209717
Epoch 2830, val loss: 1.474656581878662
Epoch 2840, training loss: 620.0913696289062 = 0.011451248079538345 + 100.0 * 6.200798988342285
Epoch 2840, val loss: 1.4773691892623901
Epoch 2850, training loss: 620.296142578125 = 0.011335602030158043 + 100.0 * 6.202847957611084
Epoch 2850, val loss: 1.4798003435134888
Epoch 2860, training loss: 620.452392578125 = 0.0112157566472888 + 100.0 * 6.204411506652832
Epoch 2860, val loss: 1.4812449216842651
Epoch 2870, training loss: 620.3479614257812 = 0.011096997186541557 + 100.0 * 6.203368663787842
Epoch 2870, val loss: 1.4833213090896606
Epoch 2880, training loss: 620.0589599609375 = 0.010978743433952332 + 100.0 * 6.200479507446289
Epoch 2880, val loss: 1.4854589700698853
Epoch 2890, training loss: 619.9478149414062 = 0.010867669247090816 + 100.0 * 6.199369430541992
Epoch 2890, val loss: 1.4881666898727417
Epoch 2900, training loss: 619.9431762695312 = 0.01076308824121952 + 100.0 * 6.199324607849121
Epoch 2900, val loss: 1.4901888370513916
Epoch 2910, training loss: 620.2994995117188 = 0.010661091655492783 + 100.0 * 6.202888488769531
Epoch 2910, val loss: 1.492262601852417
Epoch 2920, training loss: 620.1723022460938 = 0.01054957415908575 + 100.0 * 6.201617240905762
Epoch 2920, val loss: 1.4937278032302856
Epoch 2930, training loss: 620.0394287109375 = 0.010442017577588558 + 100.0 * 6.200290203094482
Epoch 2930, val loss: 1.495530605316162
Epoch 2940, training loss: 619.9568481445312 = 0.010339144617319107 + 100.0 * 6.199465274810791
Epoch 2940, val loss: 1.4983216524124146
Epoch 2950, training loss: 620.1940307617188 = 0.010239463299512863 + 100.0 * 6.20183801651001
Epoch 2950, val loss: 1.499773621559143
Epoch 2960, training loss: 620.0521850585938 = 0.010138535872101784 + 100.0 * 6.200420379638672
Epoch 2960, val loss: 1.501664638519287
Epoch 2970, training loss: 620.0092163085938 = 0.010041369125247002 + 100.0 * 6.199991703033447
Epoch 2970, val loss: 1.5036762952804565
Epoch 2980, training loss: 619.9544677734375 = 0.009945420548319817 + 100.0 * 6.1994452476501465
Epoch 2980, val loss: 1.5060131549835205
Epoch 2990, training loss: 620.28955078125 = 0.009855171665549278 + 100.0 * 6.2027974128723145
Epoch 2990, val loss: 1.5081032514572144
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 861.6251220703125 = 1.941751480102539 + 100.0 * 8.596833229064941
Epoch 0, val loss: 1.9403787851333618
Epoch 10, training loss: 861.5375366210938 = 1.9332927465438843 + 100.0 * 8.59604263305664
Epoch 10, val loss: 1.9314764738082886
Epoch 20, training loss: 861.0074462890625 = 1.9229127168655396 + 100.0 * 8.590845108032227
Epoch 20, val loss: 1.9200565814971924
Epoch 30, training loss: 857.5914306640625 = 1.9095481634140015 + 100.0 * 8.556818962097168
Epoch 30, val loss: 1.9050681591033936
Epoch 40, training loss: 836.4093627929688 = 1.8931000232696533 + 100.0 * 8.345162391662598
Epoch 40, val loss: 1.887437105178833
Epoch 50, training loss: 773.0086059570312 = 1.8733268976211548 + 100.0 * 7.711352825164795
Epoch 50, val loss: 1.866951584815979
Epoch 60, training loss: 745.2772216796875 = 1.856499433517456 + 100.0 * 7.434207439422607
Epoch 60, val loss: 1.85040283203125
Epoch 70, training loss: 721.0401611328125 = 1.8454670906066895 + 100.0 * 7.191946983337402
Epoch 70, val loss: 1.8394219875335693
Epoch 80, training loss: 706.9856567382812 = 1.8339546918869019 + 100.0 * 7.051517009735107
Epoch 80, val loss: 1.827481746673584
Epoch 90, training loss: 696.6471557617188 = 1.8226122856140137 + 100.0 * 6.948245048522949
Epoch 90, val loss: 1.8168855905532837
Epoch 100, training loss: 689.1740112304688 = 1.8129501342773438 + 100.0 * 6.873610973358154
Epoch 100, val loss: 1.807923436164856
Epoch 110, training loss: 682.9775390625 = 1.8045358657836914 + 100.0 * 6.811729431152344
Epoch 110, val loss: 1.800209879875183
Epoch 120, training loss: 677.9657592773438 = 1.7963804006576538 + 100.0 * 6.761693954467773
Epoch 120, val loss: 1.7926486730575562
Epoch 130, training loss: 672.4202880859375 = 1.7889034748077393 + 100.0 * 6.7063140869140625
Epoch 130, val loss: 1.7858119010925293
Epoch 140, training loss: 667.609375 = 1.7819904088974 + 100.0 * 6.658274173736572
Epoch 140, val loss: 1.7794042825698853
Epoch 150, training loss: 664.2407836914062 = 1.7743452787399292 + 100.0 * 6.624664306640625
Epoch 150, val loss: 1.7723596096038818
Epoch 160, training loss: 661.7089233398438 = 1.7655524015426636 + 100.0 * 6.599433422088623
Epoch 160, val loss: 1.764405369758606
Epoch 170, training loss: 659.3449096679688 = 1.7557343244552612 + 100.0 * 6.575891971588135
Epoch 170, val loss: 1.7556394338607788
Epoch 180, training loss: 657.2142944335938 = 1.7454040050506592 + 100.0 * 6.554688930511475
Epoch 180, val loss: 1.7463099956512451
Epoch 190, training loss: 655.2832641601562 = 1.7343707084655762 + 100.0 * 6.535488605499268
Epoch 190, val loss: 1.7364246845245361
Epoch 200, training loss: 653.5565795898438 = 1.722352385520935 + 100.0 * 6.518342018127441
Epoch 200, val loss: 1.7257084846496582
Epoch 210, training loss: 651.787841796875 = 1.7092866897583008 + 100.0 * 6.5007853507995605
Epoch 210, val loss: 1.7140253782272339
Epoch 220, training loss: 650.1373901367188 = 1.6951526403427124 + 100.0 * 6.48442268371582
Epoch 220, val loss: 1.7013953924179077
Epoch 230, training loss: 648.8120727539062 = 1.679695725440979 + 100.0 * 6.4713239669799805
Epoch 230, val loss: 1.6875876188278198
Epoch 240, training loss: 647.4529418945312 = 1.66283118724823 + 100.0 * 6.4579010009765625
Epoch 240, val loss: 1.672772765159607
Epoch 250, training loss: 646.2869873046875 = 1.6446645259857178 + 100.0 * 6.446423053741455
Epoch 250, val loss: 1.6568603515625
Epoch 260, training loss: 645.254150390625 = 1.62523353099823 + 100.0 * 6.436288833618164
Epoch 260, val loss: 1.6400710344314575
Epoch 270, training loss: 644.3605346679688 = 1.6046476364135742 + 100.0 * 6.427558422088623
Epoch 270, val loss: 1.6224101781845093
Epoch 280, training loss: 643.80029296875 = 1.582763433456421 + 100.0 * 6.422175407409668
Epoch 280, val loss: 1.6038838624954224
Epoch 290, training loss: 642.8496704101562 = 1.5599689483642578 + 100.0 * 6.412896633148193
Epoch 290, val loss: 1.5846530199050903
Epoch 300, training loss: 642.0230712890625 = 1.536529541015625 + 100.0 * 6.404865264892578
Epoch 300, val loss: 1.5650449991226196
Epoch 310, training loss: 641.6265258789062 = 1.5124152898788452 + 100.0 * 6.401141166687012
Epoch 310, val loss: 1.5450576543807983
Epoch 320, training loss: 640.7777709960938 = 1.4877012968063354 + 100.0 * 6.3929009437561035
Epoch 320, val loss: 1.5246734619140625
Epoch 330, training loss: 640.1971435546875 = 1.4628281593322754 + 100.0 * 6.387343406677246
Epoch 330, val loss: 1.5043342113494873
Epoch 340, training loss: 639.5853271484375 = 1.437801480293274 + 100.0 * 6.38147497177124
Epoch 340, val loss: 1.4840459823608398
Epoch 350, training loss: 639.1126098632812 = 1.4126633405685425 + 100.0 * 6.376999378204346
Epoch 350, val loss: 1.4639278650283813
Epoch 360, training loss: 638.5354614257812 = 1.3876287937164307 + 100.0 * 6.37147855758667
Epoch 360, val loss: 1.4439054727554321
Epoch 370, training loss: 638.0700073242188 = 1.362809181213379 + 100.0 * 6.367072105407715
Epoch 370, val loss: 1.424193024635315
Epoch 380, training loss: 637.795166015625 = 1.3381484746932983 + 100.0 * 6.364570140838623
Epoch 380, val loss: 1.4047828912734985
Epoch 390, training loss: 637.3050537109375 = 1.3137794733047485 + 100.0 * 6.359912872314453
Epoch 390, val loss: 1.3857392072677612
Epoch 400, training loss: 636.822021484375 = 1.2897677421569824 + 100.0 * 6.355322360992432
Epoch 400, val loss: 1.3671894073486328
Epoch 410, training loss: 636.4990844726562 = 1.2660859823226929 + 100.0 * 6.352329730987549
Epoch 410, val loss: 1.34895920753479
Epoch 420, training loss: 636.0471801757812 = 1.2428030967712402 + 100.0 * 6.348043918609619
Epoch 420, val loss: 1.3313484191894531
Epoch 430, training loss: 635.9501342773438 = 1.2199872732162476 + 100.0 * 6.347301483154297
Epoch 430, val loss: 1.3140689134597778
Epoch 440, training loss: 635.479736328125 = 1.1973246335983276 + 100.0 * 6.3428239822387695
Epoch 440, val loss: 1.2972015142440796
Epoch 450, training loss: 635.0750732421875 = 1.1752350330352783 + 100.0 * 6.338998317718506
Epoch 450, val loss: 1.2809503078460693
Epoch 460, training loss: 634.8238525390625 = 1.153586745262146 + 100.0 * 6.336702823638916
Epoch 460, val loss: 1.2651941776275635
Epoch 470, training loss: 634.4109497070312 = 1.1322591304779053 + 100.0 * 6.332787036895752
Epoch 470, val loss: 1.2498424053192139
Epoch 480, training loss: 634.4149169921875 = 1.111250638961792 + 100.0 * 6.333036422729492
Epoch 480, val loss: 1.2348008155822754
Epoch 490, training loss: 633.7465209960938 = 1.0906943082809448 + 100.0 * 6.3265581130981445
Epoch 490, val loss: 1.2203706502914429
Epoch 500, training loss: 633.5404663085938 = 1.07062566280365 + 100.0 * 6.324698448181152
Epoch 500, val loss: 1.2064235210418701
Epoch 510, training loss: 633.5107421875 = 1.0509865283966064 + 100.0 * 6.3245978355407715
Epoch 510, val loss: 1.1931291818618774
Epoch 520, training loss: 633.3800659179688 = 1.031333088874817 + 100.0 * 6.323487281799316
Epoch 520, val loss: 1.1794184446334839
Epoch 530, training loss: 632.8644409179688 = 1.0122629404067993 + 100.0 * 6.318521499633789
Epoch 530, val loss: 1.16668701171875
Epoch 540, training loss: 632.4909057617188 = 0.9936820268630981 + 100.0 * 6.314972400665283
Epoch 540, val loss: 1.154508352279663
Epoch 550, training loss: 632.239990234375 = 0.9754809141159058 + 100.0 * 6.312644958496094
Epoch 550, val loss: 1.1428309679031372
Epoch 560, training loss: 632.346435546875 = 0.9573725461959839 + 100.0 * 6.31389045715332
Epoch 560, val loss: 1.1313557624816895
Epoch 570, training loss: 631.8182373046875 = 0.9395501613616943 + 100.0 * 6.308786869049072
Epoch 570, val loss: 1.1201750040054321
Epoch 580, training loss: 631.6123046875 = 0.9222090244293213 + 100.0 * 6.306900978088379
Epoch 580, val loss: 1.1095383167266846
Epoch 590, training loss: 631.4721069335938 = 0.9052879214286804 + 100.0 * 6.305668354034424
Epoch 590, val loss: 1.0994048118591309
Epoch 600, training loss: 631.3312377929688 = 0.8885897994041443 + 100.0 * 6.304426193237305
Epoch 600, val loss: 1.0895617008209229
Epoch 610, training loss: 631.0949096679688 = 0.872118353843689 + 100.0 * 6.302227973937988
Epoch 610, val loss: 1.0801671743392944
Epoch 620, training loss: 630.9521484375 = 0.8560343384742737 + 100.0 * 6.300961494445801
Epoch 620, val loss: 1.0710924863815308
Epoch 630, training loss: 630.9468994140625 = 0.8399295210838318 + 100.0 * 6.301069736480713
Epoch 630, val loss: 1.0623185634613037
Epoch 640, training loss: 630.5455322265625 = 0.8242552876472473 + 100.0 * 6.297212600708008
Epoch 640, val loss: 1.0538232326507568
Epoch 650, training loss: 630.3021240234375 = 0.8089016079902649 + 100.0 * 6.2949323654174805
Epoch 650, val loss: 1.0459790229797363
Epoch 660, training loss: 630.0852661132812 = 0.793908953666687 + 100.0 * 6.292913913726807
Epoch 660, val loss: 1.0385113954544067
Epoch 670, training loss: 629.9065551757812 = 0.7791395783424377 + 100.0 * 6.291274547576904
Epoch 670, val loss: 1.0314207077026367
Epoch 680, training loss: 631.0029907226562 = 0.7644789814949036 + 100.0 * 6.302385330200195
Epoch 680, val loss: 1.0245651006698608
Epoch 690, training loss: 629.9677124023438 = 0.7497579455375671 + 100.0 * 6.292179584503174
Epoch 690, val loss: 1.017651915550232
Epoch 700, training loss: 629.5256958007812 = 0.7354140877723694 + 100.0 * 6.28790283203125
Epoch 700, val loss: 1.0112107992172241
Epoch 710, training loss: 629.2841796875 = 0.7214009165763855 + 100.0 * 6.285627841949463
Epoch 710, val loss: 1.00519859790802
Epoch 720, training loss: 629.4987182617188 = 0.7075614333152771 + 100.0 * 6.287911415100098
Epoch 720, val loss: 0.9995071887969971
Epoch 730, training loss: 629.431396484375 = 0.693763256072998 + 100.0 * 6.2873759269714355
Epoch 730, val loss: 0.9935526251792908
Epoch 740, training loss: 628.9708251953125 = 0.6800246834754944 + 100.0 * 6.282907485961914
Epoch 740, val loss: 0.9883416295051575
Epoch 750, training loss: 628.7762451171875 = 0.6665911674499512 + 100.0 * 6.281096935272217
Epoch 750, val loss: 0.9832125306129456
Epoch 760, training loss: 628.5647583007812 = 0.6533612608909607 + 100.0 * 6.27911376953125
Epoch 760, val loss: 0.9783884882926941
Epoch 770, training loss: 628.6390380859375 = 0.6402546167373657 + 100.0 * 6.279987812042236
Epoch 770, val loss: 0.9737294316291809
Epoch 780, training loss: 628.440673828125 = 0.6270571351051331 + 100.0 * 6.278135776519775
Epoch 780, val loss: 0.9691578149795532
Epoch 790, training loss: 628.3501586914062 = 0.6140620708465576 + 100.0 * 6.277360916137695
Epoch 790, val loss: 0.9647331237792969
Epoch 800, training loss: 628.1541748046875 = 0.6011379957199097 + 100.0 * 6.2755303382873535
Epoch 800, val loss: 0.9607266783714294
Epoch 810, training loss: 627.9454956054688 = 0.5883878469467163 + 100.0 * 6.273571014404297
Epoch 810, val loss: 0.9567890167236328
Epoch 820, training loss: 627.7932739257812 = 0.5757625102996826 + 100.0 * 6.272175312042236
Epoch 820, val loss: 0.9528620839118958
Epoch 830, training loss: 628.3024291992188 = 0.5631980895996094 + 100.0 * 6.277392864227295
Epoch 830, val loss: 0.9492970705032349
Epoch 840, training loss: 627.837646484375 = 0.5504869818687439 + 100.0 * 6.272871971130371
Epoch 840, val loss: 0.945235013961792
Epoch 850, training loss: 627.5136108398438 = 0.5380949974060059 + 100.0 * 6.2697553634643555
Epoch 850, val loss: 0.9419448375701904
Epoch 860, training loss: 627.6677856445312 = 0.5258011817932129 + 100.0 * 6.271419525146484
Epoch 860, val loss: 0.9387015104293823
Epoch 870, training loss: 627.3524780273438 = 0.5135592818260193 + 100.0 * 6.2683892250061035
Epoch 870, val loss: 0.9359245896339417
Epoch 880, training loss: 627.1768188476562 = 0.5014998912811279 + 100.0 * 6.26675271987915
Epoch 880, val loss: 0.9326344728469849
Epoch 890, training loss: 627.4776611328125 = 0.4895123243331909 + 100.0 * 6.269881725311279
Epoch 890, val loss: 0.9298028349876404
Epoch 900, training loss: 626.9506225585938 = 0.47762277722358704 + 100.0 * 6.264729976654053
Epoch 900, val loss: 0.9270360469818115
Epoch 910, training loss: 626.7918090820312 = 0.4659533202648163 + 100.0 * 6.263258457183838
Epoch 910, val loss: 0.9244107604026794
Epoch 920, training loss: 626.8864135742188 = 0.4544495642185211 + 100.0 * 6.26431941986084
Epoch 920, val loss: 0.9220967888832092
Epoch 930, training loss: 626.7070922851562 = 0.44310030341148376 + 100.0 * 6.26263952255249
Epoch 930, val loss: 0.9200232028961182
Epoch 940, training loss: 626.6837768554688 = 0.43184638023376465 + 100.0 * 6.262519359588623
Epoch 940, val loss: 0.917622447013855
Epoch 950, training loss: 626.3942260742188 = 0.4208396077156067 + 100.0 * 6.2597336769104
Epoch 950, val loss: 0.9156957864761353
Epoch 960, training loss: 626.31640625 = 0.41009512543678284 + 100.0 * 6.259063243865967
Epoch 960, val loss: 0.9139562249183655
Epoch 970, training loss: 626.6631469726562 = 0.3995663821697235 + 100.0 * 6.262636184692383
Epoch 970, val loss: 0.9125912189483643
Epoch 980, training loss: 626.2777709960938 = 0.3892251253128052 + 100.0 * 6.258885383605957
Epoch 980, val loss: 0.9113284945487976
Epoch 990, training loss: 626.2334594726562 = 0.37906068563461304 + 100.0 * 6.258543491363525
Epoch 990, val loss: 0.9104031920433044
Epoch 1000, training loss: 626.111083984375 = 0.3692125976085663 + 100.0 * 6.257418632507324
Epoch 1000, val loss: 0.9091821908950806
Epoch 1010, training loss: 626.223876953125 = 0.3594992160797119 + 100.0 * 6.258644104003906
Epoch 1010, val loss: 0.9081469178199768
Epoch 1020, training loss: 626.0245971679688 = 0.35004159808158875 + 100.0 * 6.2567458152771
Epoch 1020, val loss: 0.9077730774879456
Epoch 1030, training loss: 625.7828369140625 = 0.34087246656417847 + 100.0 * 6.254419803619385
Epoch 1030, val loss: 0.9069904088973999
Epoch 1040, training loss: 625.8226318359375 = 0.33190226554870605 + 100.0 * 6.254907131195068
Epoch 1040, val loss: 0.9071230292320251
Epoch 1050, training loss: 625.5960083007812 = 0.32322317361831665 + 100.0 * 6.252727508544922
Epoch 1050, val loss: 0.9070014953613281
Epoch 1060, training loss: 625.5645141601562 = 0.3146987855434418 + 100.0 * 6.252498149871826
Epoch 1060, val loss: 0.9071934819221497
Epoch 1070, training loss: 625.6678466796875 = 0.30647939443588257 + 100.0 * 6.2536139488220215
Epoch 1070, val loss: 0.9076656103134155
Epoch 1080, training loss: 625.405517578125 = 0.29839929938316345 + 100.0 * 6.251070976257324
Epoch 1080, val loss: 0.9081181287765503
Epoch 1090, training loss: 625.2568359375 = 0.29058489203453064 + 100.0 * 6.249662399291992
Epoch 1090, val loss: 0.9088337421417236
Epoch 1100, training loss: 625.4833984375 = 0.2830146849155426 + 100.0 * 6.2520036697387695
Epoch 1100, val loss: 0.909685492515564
Epoch 1110, training loss: 625.1217041015625 = 0.27564525604248047 + 100.0 * 6.24846076965332
Epoch 1110, val loss: 0.9111443161964417
Epoch 1120, training loss: 625.1383666992188 = 0.26847603917121887 + 100.0 * 6.248698711395264
Epoch 1120, val loss: 0.9124656915664673
Epoch 1130, training loss: 624.959716796875 = 0.26155519485473633 + 100.0 * 6.246981620788574
Epoch 1130, val loss: 0.9137733578681946
Epoch 1140, training loss: 625.11572265625 = 0.25490322709083557 + 100.0 * 6.248608112335205
Epoch 1140, val loss: 0.915370762348175
Epoch 1150, training loss: 625.130859375 = 0.24831143021583557 + 100.0 * 6.248825550079346
Epoch 1150, val loss: 0.9174015522003174
Epoch 1160, training loss: 624.8700561523438 = 0.2419464886188507 + 100.0 * 6.246281147003174
Epoch 1160, val loss: 0.918887197971344
Epoch 1170, training loss: 624.8513793945312 = 0.23576578497886658 + 100.0 * 6.246155738830566
Epoch 1170, val loss: 0.9211370348930359
Epoch 1180, training loss: 625.0339965820312 = 0.229843407869339 + 100.0 * 6.24804162979126
Epoch 1180, val loss: 0.9234533905982971
Epoch 1190, training loss: 624.822265625 = 0.22397461533546448 + 100.0 * 6.245982646942139
Epoch 1190, val loss: 0.9254909753799438
Epoch 1200, training loss: 624.6937255859375 = 0.21836143732070923 + 100.0 * 6.244753360748291
Epoch 1200, val loss: 0.9279051423072815
Epoch 1210, training loss: 624.5385131835938 = 0.2129012495279312 + 100.0 * 6.243256092071533
Epoch 1210, val loss: 0.9302719831466675
Epoch 1220, training loss: 624.58642578125 = 0.20766210556030273 + 100.0 * 6.24378776550293
Epoch 1220, val loss: 0.9331526756286621
Epoch 1230, training loss: 624.5399780273438 = 0.20251353085041046 + 100.0 * 6.243374347686768
Epoch 1230, val loss: 0.9361578822135925
Epoch 1240, training loss: 624.405029296875 = 0.19748686254024506 + 100.0 * 6.242075443267822
Epoch 1240, val loss: 0.9387053847312927
Epoch 1250, training loss: 624.2769775390625 = 0.19263988733291626 + 100.0 * 6.240843772888184
Epoch 1250, val loss: 0.9416477084159851
Epoch 1260, training loss: 624.9172973632812 = 0.18793387711048126 + 100.0 * 6.247293949127197
Epoch 1260, val loss: 0.9443372488021851
Epoch 1270, training loss: 624.4791870117188 = 0.18333853781223297 + 100.0 * 6.2429585456848145
Epoch 1270, val loss: 0.9478691816329956
Epoch 1280, training loss: 624.2741088867188 = 0.17886224389076233 + 100.0 * 6.240952491760254
Epoch 1280, val loss: 0.9508466720581055
Epoch 1290, training loss: 624.0176391601562 = 0.17455440759658813 + 100.0 * 6.238430976867676
Epoch 1290, val loss: 0.9543914794921875
Epoch 1300, training loss: 624.0247802734375 = 0.170414537191391 + 100.0 * 6.238543510437012
Epoch 1300, val loss: 0.9578301310539246
Epoch 1310, training loss: 624.3805541992188 = 0.1663585603237152 + 100.0 * 6.2421417236328125
Epoch 1310, val loss: 0.9610739350318909
Epoch 1320, training loss: 624.3453979492188 = 0.1624331772327423 + 100.0 * 6.241829872131348
Epoch 1320, val loss: 0.9644694924354553
Epoch 1330, training loss: 623.9380493164062 = 0.15855352580547333 + 100.0 * 6.237794876098633
Epoch 1330, val loss: 0.968101441860199
Epoch 1340, training loss: 624.0350952148438 = 0.15484699606895447 + 100.0 * 6.238802433013916
Epoch 1340, val loss: 0.9715153574943542
Epoch 1350, training loss: 623.8837280273438 = 0.15121892094612122 + 100.0 * 6.2373247146606445
Epoch 1350, val loss: 0.9752036929130554
Epoch 1360, training loss: 623.8414916992188 = 0.14770442247390747 + 100.0 * 6.236937999725342
Epoch 1360, val loss: 0.9792050123214722
Epoch 1370, training loss: 624.0208740234375 = 0.1442890167236328 + 100.0 * 6.238765716552734
Epoch 1370, val loss: 0.9829695224761963
Epoch 1380, training loss: 623.8460083007812 = 0.14094950258731842 + 100.0 * 6.237050533294678
Epoch 1380, val loss: 0.9863882660865784
Epoch 1390, training loss: 623.794189453125 = 0.1376810073852539 + 100.0 * 6.236564636230469
Epoch 1390, val loss: 0.9902293086051941
Epoch 1400, training loss: 623.5274658203125 = 0.13454876840114594 + 100.0 * 6.233929634094238
Epoch 1400, val loss: 0.994174063205719
Epoch 1410, training loss: 623.6541748046875 = 0.13152141869068146 + 100.0 * 6.235226631164551
Epoch 1410, val loss: 0.9979700446128845
Epoch 1420, training loss: 623.855224609375 = 0.1285448968410492 + 100.0 * 6.237267017364502
Epoch 1420, val loss: 1.0016915798187256
Epoch 1430, training loss: 623.5814819335938 = 0.12559540569782257 + 100.0 * 6.234558582305908
Epoch 1430, val loss: 1.0056569576263428
Epoch 1440, training loss: 623.4600219726562 = 0.12280489504337311 + 100.0 * 6.233372211456299
Epoch 1440, val loss: 1.0094822645187378
Epoch 1450, training loss: 623.987060546875 = 0.12007515132427216 + 100.0 * 6.2386698722839355
Epoch 1450, val loss: 1.0132125616073608
Epoch 1460, training loss: 623.503662109375 = 0.1173870861530304 + 100.0 * 6.23386287689209
Epoch 1460, val loss: 1.0175901651382446
Epoch 1470, training loss: 623.5177001953125 = 0.11478448659181595 + 100.0 * 6.2340288162231445
Epoch 1470, val loss: 1.0211055278778076
Epoch 1480, training loss: 623.3267822265625 = 0.11226516962051392 + 100.0 * 6.232145309448242
Epoch 1480, val loss: 1.0256017446517944
Epoch 1490, training loss: 623.3331298828125 = 0.10980476438999176 + 100.0 * 6.23223352432251
Epoch 1490, val loss: 1.0297484397888184
Epoch 1500, training loss: 623.2034301757812 = 0.10741740465164185 + 100.0 * 6.230959892272949
Epoch 1500, val loss: 1.033522367477417
Epoch 1510, training loss: 623.0792846679688 = 0.10508614033460617 + 100.0 * 6.22974157333374
Epoch 1510, val loss: 1.0378262996673584
Epoch 1520, training loss: 623.3577880859375 = 0.10284110903739929 + 100.0 * 6.23254919052124
Epoch 1520, val loss: 1.0419124364852905
Epoch 1530, training loss: 623.2451171875 = 0.10060139000415802 + 100.0 * 6.2314453125
Epoch 1530, val loss: 1.0458083152770996
Epoch 1540, training loss: 623.3270263671875 = 0.09842549264431 + 100.0 * 6.232285499572754
Epoch 1540, val loss: 1.0497312545776367
Epoch 1550, training loss: 622.9912719726562 = 0.09630119800567627 + 100.0 * 6.228949546813965
Epoch 1550, val loss: 1.0537083148956299
Epoch 1560, training loss: 622.941162109375 = 0.09428057074546814 + 100.0 * 6.228468418121338
Epoch 1560, val loss: 1.058095097541809
Epoch 1570, training loss: 623.053466796875 = 0.09230878204107285 + 100.0 * 6.229611873626709
Epoch 1570, val loss: 1.0620390176773071
Epoch 1580, training loss: 623.2564086914062 = 0.0903378427028656 + 100.0 * 6.231660842895508
Epoch 1580, val loss: 1.065938115119934
Epoch 1590, training loss: 622.984375 = 0.08841464668512344 + 100.0 * 6.228959083557129
Epoch 1590, val loss: 1.0705243349075317
Epoch 1600, training loss: 622.7520141601562 = 0.08657510578632355 + 100.0 * 6.226654529571533
Epoch 1600, val loss: 1.0744112730026245
Epoch 1610, training loss: 622.6787719726562 = 0.08479636162519455 + 100.0 * 6.225939750671387
Epoch 1610, val loss: 1.0786601305007935
Epoch 1620, training loss: 622.7864990234375 = 0.08307842165231705 + 100.0 * 6.227034568786621
Epoch 1620, val loss: 1.0830780267715454
Epoch 1630, training loss: 623.195068359375 = 0.0813562422990799 + 100.0 * 6.231137275695801
Epoch 1630, val loss: 1.0868977308273315
Epoch 1640, training loss: 622.7332763671875 = 0.07967888563871384 + 100.0 * 6.226536273956299
Epoch 1640, val loss: 1.0907031297683716
Epoch 1650, training loss: 622.6029052734375 = 0.07805878669023514 + 100.0 * 6.225248336791992
Epoch 1650, val loss: 1.094848394393921
Epoch 1660, training loss: 622.6558227539062 = 0.07649131864309311 + 100.0 * 6.225793361663818
Epoch 1660, val loss: 1.0988940000534058
Epoch 1670, training loss: 622.7102661132812 = 0.07496258616447449 + 100.0 * 6.226353168487549
Epoch 1670, val loss: 1.1030482053756714
Epoch 1680, training loss: 622.7633056640625 = 0.07345321774482727 + 100.0 * 6.226898670196533
Epoch 1680, val loss: 1.1075963973999023
Epoch 1690, training loss: 622.4728393554688 = 0.07198156416416168 + 100.0 * 6.224009037017822
Epoch 1690, val loss: 1.1111412048339844
Epoch 1700, training loss: 623.3954467773438 = 0.07058532536029816 + 100.0 * 6.233249187469482
Epoch 1700, val loss: 1.1151738166809082
Epoch 1710, training loss: 622.6412353515625 = 0.06912738084793091 + 100.0 * 6.2257208824157715
Epoch 1710, val loss: 1.1194251775741577
Epoch 1720, training loss: 622.3713989257812 = 0.06775504350662231 + 100.0 * 6.223036766052246
Epoch 1720, val loss: 1.1235284805297852
Epoch 1730, training loss: 622.3167724609375 = 0.06645146012306213 + 100.0 * 6.222503185272217
Epoch 1730, val loss: 1.1275460720062256
Epoch 1740, training loss: 622.3352661132812 = 0.06518126279115677 + 100.0 * 6.222701072692871
Epoch 1740, val loss: 1.1319928169250488
Epoch 1750, training loss: 623.2244262695312 = 0.06394065171480179 + 100.0 * 6.23160457611084
Epoch 1750, val loss: 1.1358429193496704
Epoch 1760, training loss: 622.475341796875 = 0.0626385509967804 + 100.0 * 6.224127292633057
Epoch 1760, val loss: 1.1396194696426392
Epoch 1770, training loss: 622.202880859375 = 0.06144866719841957 + 100.0 * 6.221414089202881
Epoch 1770, val loss: 1.1437046527862549
Epoch 1780, training loss: 622.1620483398438 = 0.0602952316403389 + 100.0 * 6.221017360687256
Epoch 1780, val loss: 1.1478760242462158
Epoch 1790, training loss: 622.9317626953125 = 0.05920463800430298 + 100.0 * 6.228725433349609
Epoch 1790, val loss: 1.1521177291870117
Epoch 1800, training loss: 622.4976806640625 = 0.05801134556531906 + 100.0 * 6.2243971824646
Epoch 1800, val loss: 1.155516505241394
Epoch 1810, training loss: 622.331787109375 = 0.05692821741104126 + 100.0 * 6.222748279571533
Epoch 1810, val loss: 1.159847378730774
Epoch 1820, training loss: 622.0828857421875 = 0.0558466762304306 + 100.0 * 6.220270156860352
Epoch 1820, val loss: 1.1637001037597656
Epoch 1830, training loss: 622.1594848632812 = 0.054839860647916794 + 100.0 * 6.2210469245910645
Epoch 1830, val loss: 1.1679627895355225
Epoch 1840, training loss: 622.3681640625 = 0.053830042481422424 + 100.0 * 6.223143577575684
Epoch 1840, val loss: 1.1718982458114624
Epoch 1850, training loss: 622.3029174804688 = 0.05282982438802719 + 100.0 * 6.222500324249268
Epoch 1850, val loss: 1.1756536960601807
Epoch 1860, training loss: 622.0770263671875 = 0.051858481019735336 + 100.0 * 6.220251560211182
Epoch 1860, val loss: 1.1796813011169434
Epoch 1870, training loss: 622.0103759765625 = 0.050911325961351395 + 100.0 * 6.219594955444336
Epoch 1870, val loss: 1.1833465099334717
Epoch 1880, training loss: 622.1517944335938 = 0.04999648407101631 + 100.0 * 6.221018314361572
Epoch 1880, val loss: 1.1873773336410522
Epoch 1890, training loss: 622.1666259765625 = 0.04911423102021217 + 100.0 * 6.221174716949463
Epoch 1890, val loss: 1.1914623975753784
Epoch 1900, training loss: 622.133056640625 = 0.04821430891752243 + 100.0 * 6.220848560333252
Epoch 1900, val loss: 1.1947163343429565
Epoch 1910, training loss: 622.0087890625 = 0.047360457479953766 + 100.0 * 6.219614028930664
Epoch 1910, val loss: 1.1986534595489502
Epoch 1920, training loss: 621.8250732421875 = 0.04652148485183716 + 100.0 * 6.217785358428955
Epoch 1920, val loss: 1.202480435371399
Epoch 1930, training loss: 621.8096313476562 = 0.04571700841188431 + 100.0 * 6.217638969421387
Epoch 1930, val loss: 1.2063090801239014
Epoch 1940, training loss: 622.7015991210938 = 0.04492293670773506 + 100.0 * 6.226566791534424
Epoch 1940, val loss: 1.2095180749893188
Epoch 1950, training loss: 622.0286254882812 = 0.04411137104034424 + 100.0 * 6.219844818115234
Epoch 1950, val loss: 1.2139514684677124
Epoch 1960, training loss: 621.8427734375 = 0.043331392109394073 + 100.0 * 6.217994213104248
Epoch 1960, val loss: 1.2173569202423096
Epoch 1970, training loss: 621.96630859375 = 0.042597729712724686 + 100.0 * 6.219237327575684
Epoch 1970, val loss: 1.220853567123413
Epoch 1980, training loss: 621.7156372070312 = 0.04186338558793068 + 100.0 * 6.216737747192383
Epoch 1980, val loss: 1.2246061563491821
Epoch 1990, training loss: 621.69873046875 = 0.04116425663232803 + 100.0 * 6.216576099395752
Epoch 1990, val loss: 1.228346347808838
Epoch 2000, training loss: 621.5626220703125 = 0.04047595337033272 + 100.0 * 6.215221405029297
Epoch 2000, val loss: 1.2320539951324463
Epoch 2010, training loss: 621.6900634765625 = 0.039807673543691635 + 100.0 * 6.216502666473389
Epoch 2010, val loss: 1.2357951402664185
Epoch 2020, training loss: 621.913330078125 = 0.03913581743836403 + 100.0 * 6.2187418937683105
Epoch 2020, val loss: 1.2393089532852173
Epoch 2030, training loss: 621.8273315429688 = 0.03846321254968643 + 100.0 * 6.217888832092285
Epoch 2030, val loss: 1.242796778678894
Epoch 2040, training loss: 621.7606201171875 = 0.03782505542039871 + 100.0 * 6.217227935791016
Epoch 2040, val loss: 1.2456666231155396
Epoch 2050, training loss: 621.6903076171875 = 0.03719267249107361 + 100.0 * 6.216531276702881
Epoch 2050, val loss: 1.2495543956756592
Epoch 2060, training loss: 621.4988403320312 = 0.03659098222851753 + 100.0 * 6.2146220207214355
Epoch 2060, val loss: 1.2530877590179443
Epoch 2070, training loss: 621.513916015625 = 0.03600611537694931 + 100.0 * 6.214778900146484
Epoch 2070, val loss: 1.256745457649231
Epoch 2080, training loss: 621.7721557617188 = 0.035431671887636185 + 100.0 * 6.217366695404053
Epoch 2080, val loss: 1.260136604309082
Epoch 2090, training loss: 621.780029296875 = 0.03486724570393562 + 100.0 * 6.217451572418213
Epoch 2090, val loss: 1.2630996704101562
Epoch 2100, training loss: 621.6986694335938 = 0.034291647374629974 + 100.0 * 6.216643810272217
Epoch 2100, val loss: 1.2671735286712646
Epoch 2110, training loss: 621.5415649414062 = 0.033750683069229126 + 100.0 * 6.215077877044678
Epoch 2110, val loss: 1.269625186920166
Epoch 2120, training loss: 621.4387817382812 = 0.03321114555001259 + 100.0 * 6.21405553817749
Epoch 2120, val loss: 1.2736139297485352
Epoch 2130, training loss: 621.779541015625 = 0.0327165313065052 + 100.0 * 6.21746826171875
Epoch 2130, val loss: 1.2770527601242065
Epoch 2140, training loss: 621.419189453125 = 0.032181721180677414 + 100.0 * 6.213870525360107
Epoch 2140, val loss: 1.2802932262420654
Epoch 2150, training loss: 621.3148803710938 = 0.03167736902832985 + 100.0 * 6.212831974029541
Epoch 2150, val loss: 1.2833950519561768
Epoch 2160, training loss: 621.240478515625 = 0.031199010089039803 + 100.0 * 6.212092876434326
Epoch 2160, val loss: 1.2871460914611816
Epoch 2170, training loss: 621.2064208984375 = 0.030732519924640656 + 100.0 * 6.211756706237793
Epoch 2170, val loss: 1.2904140949249268
Epoch 2180, training loss: 621.5633544921875 = 0.030285943299531937 + 100.0 * 6.215330600738525
Epoch 2180, val loss: 1.2937796115875244
Epoch 2190, training loss: 622.1873779296875 = 0.029800770804286003 + 100.0 * 6.221575736999512
Epoch 2190, val loss: 1.2968618869781494
Epoch 2200, training loss: 621.5942993164062 = 0.0293415654450655 + 100.0 * 6.215649127960205
Epoch 2200, val loss: 1.2996033430099487
Epoch 2210, training loss: 621.128662109375 = 0.02886943332850933 + 100.0 * 6.210998058319092
Epoch 2210, val loss: 1.3029489517211914
Epoch 2220, training loss: 621.1278076171875 = 0.028453130275011063 + 100.0 * 6.210993766784668
Epoch 2220, val loss: 1.3064520359039307
Epoch 2230, training loss: 621.1026000976562 = 0.02805168926715851 + 100.0 * 6.210745334625244
Epoch 2230, val loss: 1.3097457885742188
Epoch 2240, training loss: 621.4984130859375 = 0.02765319123864174 + 100.0 * 6.214707374572754
Epoch 2240, val loss: 1.3130199909210205
Epoch 2250, training loss: 621.0518798828125 = 0.02722654677927494 + 100.0 * 6.210246562957764
Epoch 2250, val loss: 1.3158085346221924
Epoch 2260, training loss: 621.0418090820312 = 0.02682901918888092 + 100.0 * 6.21014928817749
Epoch 2260, val loss: 1.3188642263412476
Epoch 2270, training loss: 621.054931640625 = 0.02644393965601921 + 100.0 * 6.210285186767578
Epoch 2270, val loss: 1.322044849395752
Epoch 2280, training loss: 621.5950927734375 = 0.02607649564743042 + 100.0 * 6.2156901359558105
Epoch 2280, val loss: 1.3250221014022827
Epoch 2290, training loss: 621.1100463867188 = 0.025699567049741745 + 100.0 * 6.210843563079834
Epoch 2290, val loss: 1.3281413316726685
Epoch 2300, training loss: 621.1356811523438 = 0.025334855541586876 + 100.0 * 6.211103439331055
Epoch 2300, val loss: 1.3312524557113647
Epoch 2310, training loss: 621.0363159179688 = 0.02497389167547226 + 100.0 * 6.210113525390625
Epoch 2310, val loss: 1.3342965841293335
Epoch 2320, training loss: 620.9486694335938 = 0.02462606318295002 + 100.0 * 6.209240436553955
Epoch 2320, val loss: 1.3372125625610352
Epoch 2330, training loss: 621.178955078125 = 0.02429683692753315 + 100.0 * 6.211546897888184
Epoch 2330, val loss: 1.340052843093872
Epoch 2340, training loss: 621.0499877929688 = 0.023955263197422028 + 100.0 * 6.21026086807251
Epoch 2340, val loss: 1.3431155681610107
Epoch 2350, training loss: 621.065185546875 = 0.02361922524869442 + 100.0 * 6.210415363311768
Epoch 2350, val loss: 1.3459552526474
Epoch 2360, training loss: 620.9368286132812 = 0.023285912349820137 + 100.0 * 6.209135055541992
Epoch 2360, val loss: 1.348880648612976
Epoch 2370, training loss: 621.3394775390625 = 0.022974317893385887 + 100.0 * 6.213165283203125
Epoch 2370, val loss: 1.3516720533370972
Epoch 2380, training loss: 620.957275390625 = 0.022659804672002792 + 100.0 * 6.209346294403076
Epoch 2380, val loss: 1.3545856475830078
Epoch 2390, training loss: 620.900634765625 = 0.022356024011969566 + 100.0 * 6.208783149719238
Epoch 2390, val loss: 1.3575520515441895
Epoch 2400, training loss: 621.0446166992188 = 0.022058460861444473 + 100.0 * 6.210226058959961
Epoch 2400, val loss: 1.3607375621795654
Epoch 2410, training loss: 621.1101684570312 = 0.021768461912870407 + 100.0 * 6.210884094238281
Epoch 2410, val loss: 1.3634958267211914
Epoch 2420, training loss: 621.0994873046875 = 0.02148124761879444 + 100.0 * 6.210780143737793
Epoch 2420, val loss: 1.366400957107544
Epoch 2430, training loss: 620.8629150390625 = 0.02119276486337185 + 100.0 * 6.2084174156188965
Epoch 2430, val loss: 1.3689244985580444
Epoch 2440, training loss: 621.0213012695312 = 0.02091878093779087 + 100.0 * 6.210003852844238
Epoch 2440, val loss: 1.3715806007385254
Epoch 2450, training loss: 620.890869140625 = 0.020644115284085274 + 100.0 * 6.208702087402344
Epoch 2450, val loss: 1.3742161989212036
Epoch 2460, training loss: 620.8757934570312 = 0.020380577072501183 + 100.0 * 6.208553791046143
Epoch 2460, val loss: 1.3772228956222534
Epoch 2470, training loss: 620.7469482421875 = 0.020118430256843567 + 100.0 * 6.207267761230469
Epoch 2470, val loss: 1.3799773454666138
Epoch 2480, training loss: 620.907958984375 = 0.019866997376084328 + 100.0 * 6.208880424499512
Epoch 2480, val loss: 1.3826806545257568
Epoch 2490, training loss: 620.7684326171875 = 0.019613754004240036 + 100.0 * 6.207488536834717
Epoch 2490, val loss: 1.3854631185531616
Epoch 2500, training loss: 620.970703125 = 0.01937764883041382 + 100.0 * 6.2095136642456055
Epoch 2500, val loss: 1.3885881900787354
Epoch 2510, training loss: 620.837890625 = 0.019126342609524727 + 100.0 * 6.208188056945801
Epoch 2510, val loss: 1.390578269958496
Epoch 2520, training loss: 620.7860107421875 = 0.018883589655160904 + 100.0 * 6.207671642303467
Epoch 2520, val loss: 1.3929542303085327
Epoch 2530, training loss: 620.9710083007812 = 0.01864650472998619 + 100.0 * 6.2095232009887695
Epoch 2530, val loss: 1.3953028917312622
Epoch 2540, training loss: 620.6904907226562 = 0.018417151644825935 + 100.0 * 6.20672082901001
Epoch 2540, val loss: 1.3982751369476318
Epoch 2550, training loss: 620.6318359375 = 0.018190089613199234 + 100.0 * 6.206136703491211
Epoch 2550, val loss: 1.4008182287216187
Epoch 2560, training loss: 620.5923461914062 = 0.017974231392145157 + 100.0 * 6.20574426651001
Epoch 2560, val loss: 1.4035234451293945
Epoch 2570, training loss: 620.8330688476562 = 0.017766624689102173 + 100.0 * 6.208153247833252
Epoch 2570, val loss: 1.405849814414978
Epoch 2580, training loss: 620.6516723632812 = 0.017542744055390358 + 100.0 * 6.206341743469238
Epoch 2580, val loss: 1.4085088968276978
Epoch 2590, training loss: 620.634521484375 = 0.01732553355395794 + 100.0 * 6.206171989440918
Epoch 2590, val loss: 1.4111602306365967
Epoch 2600, training loss: 620.7626953125 = 0.01712377555668354 + 100.0 * 6.207455158233643
Epoch 2600, val loss: 1.4138140678405762
Epoch 2610, training loss: 620.6703491210938 = 0.016921814531087875 + 100.0 * 6.206534385681152
Epoch 2610, val loss: 1.4161456823349
Epoch 2620, training loss: 620.6766967773438 = 0.01672716811299324 + 100.0 * 6.206599712371826
Epoch 2620, val loss: 1.4183639287948608
Epoch 2630, training loss: 620.74462890625 = 0.016525190323591232 + 100.0 * 6.20728063583374
Epoch 2630, val loss: 1.4210821390151978
Epoch 2640, training loss: 620.4608154296875 = 0.01632848009467125 + 100.0 * 6.204444885253906
Epoch 2640, val loss: 1.4234344959259033
Epoch 2650, training loss: 620.4590454101562 = 0.016146190464496613 + 100.0 * 6.204428672790527
Epoch 2650, val loss: 1.4258495569229126
Epoch 2660, training loss: 620.45263671875 = 0.01596505008637905 + 100.0 * 6.204366683959961
Epoch 2660, val loss: 1.4284031391143799
Epoch 2670, training loss: 620.9351196289062 = 0.01579347439110279 + 100.0 * 6.209193229675293
Epoch 2670, val loss: 1.4306498765945435
Epoch 2680, training loss: 620.9531860351562 = 0.015607144683599472 + 100.0 * 6.209375381469727
Epoch 2680, val loss: 1.4324126243591309
Epoch 2690, training loss: 620.7034912109375 = 0.015405384823679924 + 100.0 * 6.206881046295166
Epoch 2690, val loss: 1.435336947441101
Epoch 2700, training loss: 620.3410034179688 = 0.01523350365459919 + 100.0 * 6.2032575607299805
Epoch 2700, val loss: 1.4374804496765137
Epoch 2710, training loss: 620.3336181640625 = 0.015070893801748753 + 100.0 * 6.203185558319092
Epoch 2710, val loss: 1.4398789405822754
Epoch 2720, training loss: 620.4576416015625 = 0.01491323672235012 + 100.0 * 6.204427242279053
Epoch 2720, val loss: 1.4427261352539062
Epoch 2730, training loss: 620.8243408203125 = 0.014755710028111935 + 100.0 * 6.208095550537109
Epoch 2730, val loss: 1.4447953701019287
Epoch 2740, training loss: 620.6181030273438 = 0.014580479823052883 + 100.0 * 6.206035137176514
Epoch 2740, val loss: 1.4463801383972168
Epoch 2750, training loss: 621.0870971679688 = 0.014421754516661167 + 100.0 * 6.210726737976074
Epoch 2750, val loss: 1.4490875005722046
Epoch 2760, training loss: 620.4553833007812 = 0.014256401918828487 + 100.0 * 6.204411029815674
Epoch 2760, val loss: 1.4512970447540283
Epoch 2770, training loss: 620.269775390625 = 0.014105154201388359 + 100.0 * 6.202556610107422
Epoch 2770, val loss: 1.4534518718719482
Epoch 2780, training loss: 620.2769775390625 = 0.01395900547504425 + 100.0 * 6.202630043029785
Epoch 2780, val loss: 1.455656886100769
Epoch 2790, training loss: 620.332275390625 = 0.013818365521728992 + 100.0 * 6.203184604644775
Epoch 2790, val loss: 1.4581081867218018
Epoch 2800, training loss: 620.9710693359375 = 0.01367413904517889 + 100.0 * 6.209574222564697
Epoch 2800, val loss: 1.4601924419403076
Epoch 2810, training loss: 620.6558837890625 = 0.013521211221814156 + 100.0 * 6.206423282623291
Epoch 2810, val loss: 1.463035225868225
Epoch 2820, training loss: 620.3871459960938 = 0.013375404290854931 + 100.0 * 6.203737735748291
Epoch 2820, val loss: 1.4644585847854614
Epoch 2830, training loss: 620.301025390625 = 0.013239073567092419 + 100.0 * 6.202877998352051
Epoch 2830, val loss: 1.4670435190200806
Epoch 2840, training loss: 620.394775390625 = 0.013105286285281181 + 100.0 * 6.2038164138793945
Epoch 2840, val loss: 1.4689130783081055
Epoch 2850, training loss: 620.3424072265625 = 0.012970414012670517 + 100.0 * 6.203294277191162
Epoch 2850, val loss: 1.4713189601898193
Epoch 2860, training loss: 620.34375 = 0.012841927818953991 + 100.0 * 6.203309535980225
Epoch 2860, val loss: 1.4731913805007935
Epoch 2870, training loss: 620.3003540039062 = 0.012709854170680046 + 100.0 * 6.202876567840576
Epoch 2870, val loss: 1.4754639863967896
Epoch 2880, training loss: 620.526611328125 = 0.012578303925693035 + 100.0 * 6.205140590667725
Epoch 2880, val loss: 1.4777072668075562
Epoch 2890, training loss: 620.2235107421875 = 0.012450368143618107 + 100.0 * 6.202110767364502
Epoch 2890, val loss: 1.479406476020813
Epoch 2900, training loss: 620.6114501953125 = 0.01232908759266138 + 100.0 * 6.205991268157959
Epoch 2900, val loss: 1.481926679611206
Epoch 2910, training loss: 620.1638793945312 = 0.012202023528516293 + 100.0 * 6.201516628265381
Epoch 2910, val loss: 1.483482837677002
Epoch 2920, training loss: 620.1444091796875 = 0.012079852633178234 + 100.0 * 6.20132303237915
Epoch 2920, val loss: 1.4854358434677124
Epoch 2930, training loss: 620.111572265625 = 0.011965383775532246 + 100.0 * 6.200995922088623
Epoch 2930, val loss: 1.487884759902954
Epoch 2940, training loss: 620.5415649414062 = 0.011858778074383736 + 100.0 * 6.205297470092773
Epoch 2940, val loss: 1.4901213645935059
Epoch 2950, training loss: 620.1588745117188 = 0.011734874919056892 + 100.0 * 6.20147180557251
Epoch 2950, val loss: 1.4910807609558105
Epoch 2960, training loss: 620.1395263671875 = 0.011622993275523186 + 100.0 * 6.201279163360596
Epoch 2960, val loss: 1.4935535192489624
Epoch 2970, training loss: 620.62548828125 = 0.011520166881382465 + 100.0 * 6.20613956451416
Epoch 2970, val loss: 1.495391607284546
Epoch 2980, training loss: 620.2572631835938 = 0.011400183662772179 + 100.0 * 6.202458381652832
Epoch 2980, val loss: 1.4967617988586426
Epoch 2990, training loss: 620.0446166992188 = 0.011289051733911037 + 100.0 * 6.200333118438721
Epoch 2990, val loss: 1.4992908239364624
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8313125988402742
The final CL Acc:0.73457, 0.01968, The final GNN Acc:0.83483, 0.00252
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9422])
updated graph: torch.Size([2, 10510])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.614501953125 = 1.933455467224121 + 100.0 * 8.596810340881348
Epoch 0, val loss: 1.927155613899231
Epoch 10, training loss: 861.5090942382812 = 1.9257029294967651 + 100.0 * 8.595833778381348
Epoch 10, val loss: 1.9196076393127441
Epoch 20, training loss: 860.8292236328125 = 1.9159892797470093 + 100.0 * 8.589132308959961
Epoch 20, val loss: 1.9096652269363403
Epoch 30, training loss: 856.319091796875 = 1.9031982421875 + 100.0 * 8.544158935546875
Epoch 30, val loss: 1.8961678743362427
Epoch 40, training loss: 829.6481323242188 = 1.8872464895248413 + 100.0 * 8.277608871459961
Epoch 40, val loss: 1.8798143863677979
Epoch 50, training loss: 773.995849609375 = 1.8685039281845093 + 100.0 * 7.721273422241211
Epoch 50, val loss: 1.8620712757110596
Epoch 60, training loss: 729.291015625 = 1.858145833015442 + 100.0 * 7.274328708648682
Epoch 60, val loss: 1.8534187078475952
Epoch 70, training loss: 707.1044311523438 = 1.8510441780090332 + 100.0 * 7.0525336265563965
Epoch 70, val loss: 1.846633791923523
Epoch 80, training loss: 693.7908935546875 = 1.843538761138916 + 100.0 * 6.919473171234131
Epoch 80, val loss: 1.8391526937484741
Epoch 90, training loss: 682.9984741210938 = 1.8364570140838623 + 100.0 * 6.811619758605957
Epoch 90, val loss: 1.832229733467102
Epoch 100, training loss: 674.587890625 = 1.830417275428772 + 100.0 * 6.727574348449707
Epoch 100, val loss: 1.8261423110961914
Epoch 110, training loss: 668.0552368164062 = 1.8247020244598389 + 100.0 * 6.6623053550720215
Epoch 110, val loss: 1.8202736377716064
Epoch 120, training loss: 663.6409912109375 = 1.8193460702896118 + 100.0 * 6.618216514587402
Epoch 120, val loss: 1.8146945238113403
Epoch 130, training loss: 660.2380981445312 = 1.814039707183838 + 100.0 * 6.584240913391113
Epoch 130, val loss: 1.8092169761657715
Epoch 140, training loss: 657.4720458984375 = 1.8088234663009644 + 100.0 * 6.556632041931152
Epoch 140, val loss: 1.803826928138733
Epoch 150, training loss: 655.6742553710938 = 1.8036551475524902 + 100.0 * 6.538706302642822
Epoch 150, val loss: 1.7986118793487549
Epoch 160, training loss: 653.3226318359375 = 1.7983976602554321 + 100.0 * 6.515242099761963
Epoch 160, val loss: 1.7932641506195068
Epoch 170, training loss: 651.5863037109375 = 1.7930163145065308 + 100.0 * 6.4979329109191895
Epoch 170, val loss: 1.7879375219345093
Epoch 180, training loss: 650.641845703125 = 1.7873468399047852 + 100.0 * 6.488544940948486
Epoch 180, val loss: 1.7824857234954834
Epoch 190, training loss: 648.8126220703125 = 1.7812705039978027 + 100.0 * 6.470314025878906
Epoch 190, val loss: 1.7767482995986938
Epoch 200, training loss: 647.557373046875 = 1.7748184204101562 + 100.0 * 6.457825183868408
Epoch 200, val loss: 1.7706934213638306
Epoch 210, training loss: 646.4470825195312 = 1.767760992050171 + 100.0 * 6.446793079376221
Epoch 210, val loss: 1.7642672061920166
Epoch 220, training loss: 645.2874755859375 = 1.7602002620697021 + 100.0 * 6.435272693634033
Epoch 220, val loss: 1.757358431816101
Epoch 230, training loss: 644.3137817382812 = 1.7519675493240356 + 100.0 * 6.4256181716918945
Epoch 230, val loss: 1.7498804330825806
Epoch 240, training loss: 643.5760498046875 = 1.7429587841033936 + 100.0 * 6.418330669403076
Epoch 240, val loss: 1.7417978048324585
Epoch 250, training loss: 642.693359375 = 1.7331726551055908 + 100.0 * 6.409602165222168
Epoch 250, val loss: 1.7328907251358032
Epoch 260, training loss: 641.9398193359375 = 1.7224949598312378 + 100.0 * 6.4021735191345215
Epoch 260, val loss: 1.723311424255371
Epoch 270, training loss: 641.828857421875 = 1.710841417312622 + 100.0 * 6.401180267333984
Epoch 270, val loss: 1.7129416465759277
Epoch 280, training loss: 640.7982788085938 = 1.6982016563415527 + 100.0 * 6.391001224517822
Epoch 280, val loss: 1.701621651649475
Epoch 290, training loss: 640.1482543945312 = 1.684549331665039 + 100.0 * 6.384636878967285
Epoch 290, val loss: 1.6894017457962036
Epoch 300, training loss: 639.5519409179688 = 1.6699659824371338 + 100.0 * 6.378819465637207
Epoch 300, val loss: 1.6764569282531738
Epoch 310, training loss: 639.265869140625 = 1.654375672340393 + 100.0 * 6.376114845275879
Epoch 310, val loss: 1.6625791788101196
Epoch 320, training loss: 638.7088012695312 = 1.6377016305923462 + 100.0 * 6.370710849761963
Epoch 320, val loss: 1.6481635570526123
Epoch 330, training loss: 638.061279296875 = 1.620218276977539 + 100.0 * 6.364410400390625
Epoch 330, val loss: 1.632914423942566
Epoch 340, training loss: 637.9548950195312 = 1.6020070314407349 + 100.0 * 6.363529205322266
Epoch 340, val loss: 1.617096185684204
Epoch 350, training loss: 637.17431640625 = 1.5828776359558105 + 100.0 * 6.355914115905762
Epoch 350, val loss: 1.6008347272872925
Epoch 360, training loss: 636.8076782226562 = 1.5631930828094482 + 100.0 * 6.352445125579834
Epoch 360, val loss: 1.584206223487854
Epoch 370, training loss: 636.6063842773438 = 1.5429656505584717 + 100.0 * 6.3506340980529785
Epoch 370, val loss: 1.5674259662628174
Epoch 380, training loss: 636.07666015625 = 1.5222008228302002 + 100.0 * 6.345544338226318
Epoch 380, val loss: 1.5503507852554321
Epoch 390, training loss: 635.6385498046875 = 1.5012470483779907 + 100.0 * 6.341373443603516
Epoch 390, val loss: 1.5333141088485718
Epoch 400, training loss: 635.293212890625 = 1.4801456928253174 + 100.0 * 6.338130474090576
Epoch 400, val loss: 1.516516923904419
Epoch 410, training loss: 635.0166015625 = 1.4589158296585083 + 100.0 * 6.33557653427124
Epoch 410, val loss: 1.4998914003372192
Epoch 420, training loss: 634.7603149414062 = 1.4376435279846191 + 100.0 * 6.333227157592773
Epoch 420, val loss: 1.4835411310195923
Epoch 430, training loss: 634.310791015625 = 1.4164754152297974 + 100.0 * 6.328942775726318
Epoch 430, val loss: 1.4674937725067139
Epoch 440, training loss: 634.233154296875 = 1.3955767154693604 + 100.0 * 6.328375816345215
Epoch 440, val loss: 1.4520158767700195
Epoch 450, training loss: 634.2172241210938 = 1.3748141527175903 + 100.0 * 6.328423976898193
Epoch 450, val loss: 1.4365041255950928
Epoch 460, training loss: 633.5267944335938 = 1.3542451858520508 + 100.0 * 6.321725368499756
Epoch 460, val loss: 1.4218255281448364
Epoch 470, training loss: 633.1951904296875 = 1.3341928720474243 + 100.0 * 6.318610191345215
Epoch 470, val loss: 1.407710313796997
Epoch 480, training loss: 633.027587890625 = 1.314558982849121 + 100.0 * 6.317130088806152
Epoch 480, val loss: 1.3941712379455566
Epoch 490, training loss: 632.8572998046875 = 1.2951711416244507 + 100.0 * 6.315621376037598
Epoch 490, val loss: 1.3810553550720215
Epoch 500, training loss: 632.7064819335938 = 1.2761220932006836 + 100.0 * 6.314303874969482
Epoch 500, val loss: 1.3682689666748047
Epoch 510, training loss: 632.2957153320312 = 1.25754714012146 + 100.0 * 6.3103814125061035
Epoch 510, val loss: 1.356284260749817
Epoch 520, training loss: 632.3086547851562 = 1.239431619644165 + 100.0 * 6.310692310333252
Epoch 520, val loss: 1.3448961973190308
Epoch 530, training loss: 632.1259765625 = 1.2215399742126465 + 100.0 * 6.309043884277344
Epoch 530, val loss: 1.333439826965332
Epoch 540, training loss: 631.8597412109375 = 1.2038965225219727 + 100.0 * 6.306558132171631
Epoch 540, val loss: 1.3227442502975464
Epoch 550, training loss: 631.474853515625 = 1.1867281198501587 + 100.0 * 6.302881240844727
Epoch 550, val loss: 1.3124032020568848
Epoch 560, training loss: 631.5682373046875 = 1.1699578762054443 + 100.0 * 6.303982734680176
Epoch 560, val loss: 1.3024020195007324
Epoch 570, training loss: 631.2904663085938 = 1.1532303094863892 + 100.0 * 6.301372051239014
Epoch 570, val loss: 1.292855978012085
Epoch 580, training loss: 631.170166015625 = 1.1368142366409302 + 100.0 * 6.300333023071289
Epoch 580, val loss: 1.2832107543945312
Epoch 590, training loss: 630.8573608398438 = 1.1207448244094849 + 100.0 * 6.297366142272949
Epoch 590, val loss: 1.2742722034454346
Epoch 600, training loss: 631.1820068359375 = 1.1048426628112793 + 100.0 * 6.300771236419678
Epoch 600, val loss: 1.2653993368148804
Epoch 610, training loss: 630.6088256835938 = 1.0888999700546265 + 100.0 * 6.295198917388916
Epoch 610, val loss: 1.2568089962005615
Epoch 620, training loss: 630.4591674804688 = 1.0732946395874023 + 100.0 * 6.293858528137207
Epoch 620, val loss: 1.2484372854232788
Epoch 630, training loss: 630.45947265625 = 1.0578644275665283 + 100.0 * 6.294016361236572
Epoch 630, val loss: 1.2402645349502563
Epoch 640, training loss: 630.1852416992188 = 1.0423754453659058 + 100.0 * 6.291428565979004
Epoch 640, val loss: 1.2321656942367554
Epoch 650, training loss: 629.985107421875 = 1.027093529701233 + 100.0 * 6.289580345153809
Epoch 650, val loss: 1.2239683866500854
Epoch 660, training loss: 629.9327392578125 = 1.0119447708129883 + 100.0 * 6.289207935333252
Epoch 660, val loss: 1.2162069082260132
Epoch 670, training loss: 630.0380859375 = 0.9967211484909058 + 100.0 * 6.290413856506348
Epoch 670, val loss: 1.2083754539489746
Epoch 680, training loss: 629.592041015625 = 0.9814962148666382 + 100.0 * 6.286105155944824
Epoch 680, val loss: 1.2000021934509277
Epoch 690, training loss: 629.4043579101562 = 0.9663806557655334 + 100.0 * 6.284379959106445
Epoch 690, val loss: 1.1924619674682617
Epoch 700, training loss: 629.2745361328125 = 0.9514674544334412 + 100.0 * 6.283230304718018
Epoch 700, val loss: 1.1848129034042358
Epoch 710, training loss: 629.6832275390625 = 0.9364409446716309 + 100.0 * 6.2874674797058105
Epoch 710, val loss: 1.1770596504211426
Epoch 720, training loss: 629.146484375 = 0.921413779258728 + 100.0 * 6.282250881195068
Epoch 720, val loss: 1.1692196130752563
Epoch 730, training loss: 629.209716796875 = 0.9065477252006531 + 100.0 * 6.283031463623047
Epoch 730, val loss: 1.1613454818725586
Epoch 740, training loss: 628.7776489257812 = 0.8915270566940308 + 100.0 * 6.278861045837402
Epoch 740, val loss: 1.1536304950714111
Epoch 750, training loss: 628.6431884765625 = 0.8768262267112732 + 100.0 * 6.277663707733154
Epoch 750, val loss: 1.145916223526001
Epoch 760, training loss: 628.6863403320312 = 0.8623070120811462 + 100.0 * 6.278240203857422
Epoch 760, val loss: 1.1386455297470093
Epoch 770, training loss: 628.6972045898438 = 0.8475810289382935 + 100.0 * 6.278496265411377
Epoch 770, val loss: 1.1312323808670044
Epoch 780, training loss: 628.4105224609375 = 0.833029568195343 + 100.0 * 6.27577543258667
Epoch 780, val loss: 1.1239299774169922
Epoch 790, training loss: 628.265380859375 = 0.8187206387519836 + 100.0 * 6.274466514587402
Epoch 790, val loss: 1.1169384717941284
Epoch 800, training loss: 628.5629272460938 = 0.8047024607658386 + 100.0 * 6.277582168579102
Epoch 800, val loss: 1.1101964712142944
Epoch 810, training loss: 628.2583618164062 = 0.7907182574272156 + 100.0 * 6.274676322937012
Epoch 810, val loss: 1.1031484603881836
Epoch 820, training loss: 628.0413818359375 = 0.7768200635910034 + 100.0 * 6.272645950317383
Epoch 820, val loss: 1.0966973304748535
Epoch 830, training loss: 628.0115356445312 = 0.7632774710655212 + 100.0 * 6.272482395172119
Epoch 830, val loss: 1.0902163982391357
Epoch 840, training loss: 627.8556518554688 = 0.7499462366104126 + 100.0 * 6.27105712890625
Epoch 840, val loss: 1.0840857028961182
Epoch 850, training loss: 627.5859985351562 = 0.736876368522644 + 100.0 * 6.268491268157959
Epoch 850, val loss: 1.0782208442687988
Epoch 860, training loss: 628.36376953125 = 0.7240607738494873 + 100.0 * 6.276397228240967
Epoch 860, val loss: 1.0724663734436035
Epoch 870, training loss: 627.5776977539062 = 0.7111470699310303 + 100.0 * 6.268665790557861
Epoch 870, val loss: 1.0669195652008057
Epoch 880, training loss: 627.3018798828125 = 0.6987378001213074 + 100.0 * 6.266031742095947
Epoch 880, val loss: 1.0617910623550415
Epoch 890, training loss: 627.7555541992188 = 0.6866062879562378 + 100.0 * 6.270689964294434
Epoch 890, val loss: 1.0567728281021118
Epoch 900, training loss: 627.2359619140625 = 0.6746199727058411 + 100.0 * 6.265613555908203
Epoch 900, val loss: 1.0523765087127686
Epoch 910, training loss: 627.0306396484375 = 0.6628974080085754 + 100.0 * 6.263677597045898
Epoch 910, val loss: 1.0479434728622437
Epoch 920, training loss: 627.6556396484375 = 0.6512755155563354 + 100.0 * 6.270043849945068
Epoch 920, val loss: 1.0435419082641602
Epoch 930, training loss: 627.1805419921875 = 0.6397035121917725 + 100.0 * 6.265408515930176
Epoch 930, val loss: 1.039567232131958
Epoch 940, training loss: 626.7132568359375 = 0.6286654472351074 + 100.0 * 6.260846138000488
Epoch 940, val loss: 1.0356552600860596
Epoch 950, training loss: 626.6362915039062 = 0.6179636716842651 + 100.0 * 6.260183334350586
Epoch 950, val loss: 1.0320695638656616
Epoch 960, training loss: 626.6209106445312 = 0.6074466109275818 + 100.0 * 6.260134696960449
Epoch 960, val loss: 1.0289742946624756
Epoch 970, training loss: 626.6160278320312 = 0.5968739986419678 + 100.0 * 6.260191917419434
Epoch 970, val loss: 1.02583646774292
Epoch 980, training loss: 626.5272827148438 = 0.5864840149879456 + 100.0 * 6.259407997131348
Epoch 980, val loss: 1.0227818489074707
Epoch 990, training loss: 626.3244018554688 = 0.5765039324760437 + 100.0 * 6.257479190826416
Epoch 990, val loss: 1.0198113918304443
Epoch 1000, training loss: 626.5318603515625 = 0.5667014122009277 + 100.0 * 6.2596516609191895
Epoch 1000, val loss: 1.0173490047454834
Epoch 1010, training loss: 626.1596069335938 = 0.556868851184845 + 100.0 * 6.2560272216796875
Epoch 1010, val loss: 1.014665126800537
Epoch 1020, training loss: 626.2990112304688 = 0.5473132133483887 + 100.0 * 6.257516860961914
Epoch 1020, val loss: 1.0123127698898315
Epoch 1030, training loss: 626.12060546875 = 0.5378453135490417 + 100.0 * 6.2558274269104
Epoch 1030, val loss: 1.0101697444915771
Epoch 1040, training loss: 626.0988159179688 = 0.5286095142364502 + 100.0 * 6.255702018737793
Epoch 1040, val loss: 1.008001685142517
Epoch 1050, training loss: 625.9134521484375 = 0.5196385979652405 + 100.0 * 6.253937721252441
Epoch 1050, val loss: 1.0062439441680908
Epoch 1060, training loss: 626.0186767578125 = 0.5108088254928589 + 100.0 * 6.2550787925720215
Epoch 1060, val loss: 1.004642128944397
Epoch 1070, training loss: 625.8919067382812 = 0.5019845962524414 + 100.0 * 6.253899097442627
Epoch 1070, val loss: 1.0027692317962646
Epoch 1080, training loss: 625.9187622070312 = 0.4932447373867035 + 100.0 * 6.254255294799805
Epoch 1080, val loss: 1.0009092092514038
Epoch 1090, training loss: 625.7013549804688 = 0.48476722836494446 + 100.0 * 6.252166271209717
Epoch 1090, val loss: 0.9997660517692566
Epoch 1100, training loss: 625.5292358398438 = 0.47647032141685486 + 100.0 * 6.250527858734131
Epoch 1100, val loss: 0.9982143640518188
Epoch 1110, training loss: 626.3001098632812 = 0.46834641695022583 + 100.0 * 6.258317947387695
Epoch 1110, val loss: 0.9970377683639526
Epoch 1120, training loss: 625.6851196289062 = 0.4600943326950073 + 100.0 * 6.2522501945495605
Epoch 1120, val loss: 0.99602210521698
Epoch 1130, training loss: 625.495361328125 = 0.4520990550518036 + 100.0 * 6.25043249130249
Epoch 1130, val loss: 0.9947275519371033
Epoch 1140, training loss: 625.7136840820312 = 0.4443252682685852 + 100.0 * 6.2526936531066895
Epoch 1140, val loss: 0.9939481616020203
Epoch 1150, training loss: 625.2413940429688 = 0.4364955723285675 + 100.0 * 6.248048782348633
Epoch 1150, val loss: 0.9926392436027527
Epoch 1160, training loss: 625.3418579101562 = 0.42894598841667175 + 100.0 * 6.249128818511963
Epoch 1160, val loss: 0.9916675686836243
Epoch 1170, training loss: 625.4208984375 = 0.4213488698005676 + 100.0 * 6.249995708465576
Epoch 1170, val loss: 0.9910940527915955
Epoch 1180, training loss: 625.0554809570312 = 0.4137965440750122 + 100.0 * 6.2464165687561035
Epoch 1180, val loss: 0.9900785088539124
Epoch 1190, training loss: 624.9498291015625 = 0.4065661132335663 + 100.0 * 6.2454328536987305
Epoch 1190, val loss: 0.989859938621521
Epoch 1200, training loss: 624.906005859375 = 0.3994937837123871 + 100.0 * 6.245065212249756
Epoch 1200, val loss: 0.9892441630363464
Epoch 1210, training loss: 625.9376831054688 = 0.39242520928382874 + 100.0 * 6.255453109741211
Epoch 1210, val loss: 0.9895097613334656
Epoch 1220, training loss: 624.9893798828125 = 0.3851790428161621 + 100.0 * 6.246041774749756
Epoch 1220, val loss: 0.9875797033309937
Epoch 1230, training loss: 624.8858642578125 = 0.37826091051101685 + 100.0 * 6.2450761795043945
Epoch 1230, val loss: 0.9874239563941956
Epoch 1240, training loss: 624.6414794921875 = 0.37153422832489014 + 100.0 * 6.24269962310791
Epoch 1240, val loss: 0.9872899651527405
Epoch 1250, training loss: 624.76318359375 = 0.3649778664112091 + 100.0 * 6.243981838226318
Epoch 1250, val loss: 0.987030565738678
Epoch 1260, training loss: 624.8516235351562 = 0.35834258794784546 + 100.0 * 6.244932651519775
Epoch 1260, val loss: 0.9867653250694275
Epoch 1270, training loss: 624.5240478515625 = 0.3517255187034607 + 100.0 * 6.24172306060791
Epoch 1270, val loss: 0.9863690137863159
Epoch 1280, training loss: 624.46826171875 = 0.34534716606140137 + 100.0 * 6.24122953414917
Epoch 1280, val loss: 0.9865967035293579
Epoch 1290, training loss: 624.3968505859375 = 0.3391385078430176 + 100.0 * 6.240577697753906
Epoch 1290, val loss: 0.9863711595535278
Epoch 1300, training loss: 625.5512084960938 = 0.3329768180847168 + 100.0 * 6.252182483673096
Epoch 1300, val loss: 0.9862638115882874
Epoch 1310, training loss: 624.9152221679688 = 0.3266716003417969 + 100.0 * 6.245885372161865
Epoch 1310, val loss: 0.9867143630981445
Epoch 1320, training loss: 624.3883666992188 = 0.3205321729183197 + 100.0 * 6.240678310394287
Epoch 1320, val loss: 0.9862226843833923
Epoch 1330, training loss: 624.6690673828125 = 0.3146170377731323 + 100.0 * 6.243544578552246
Epoch 1330, val loss: 0.9864035248756409
Epoch 1340, training loss: 624.2333984375 = 0.3087359368801117 + 100.0 * 6.239246845245361
Epoch 1340, val loss: 0.9869384765625
Epoch 1350, training loss: 624.1179809570312 = 0.30303484201431274 + 100.0 * 6.238149642944336
Epoch 1350, val loss: 0.9873852729797363
Epoch 1360, training loss: 624.1826782226562 = 0.2974472939968109 + 100.0 * 6.238852500915527
Epoch 1360, val loss: 0.9875960946083069
Epoch 1370, training loss: 624.2892456054688 = 0.29186081886291504 + 100.0 * 6.239974021911621
Epoch 1370, val loss: 0.9880894422531128
Epoch 1380, training loss: 624.5785522460938 = 0.2862764000892639 + 100.0 * 6.242923259735107
Epoch 1380, val loss: 0.9885013103485107
Epoch 1390, training loss: 624.0861206054688 = 0.2807595431804657 + 100.0 * 6.238053798675537
Epoch 1390, val loss: 0.9886199831962585
Epoch 1400, training loss: 623.8896484375 = 0.2754521667957306 + 100.0 * 6.236141681671143
Epoch 1400, val loss: 0.9891805052757263
Epoch 1410, training loss: 623.8677978515625 = 0.2702988386154175 + 100.0 * 6.2359747886657715
Epoch 1410, val loss: 0.9900952577590942
Epoch 1420, training loss: 624.5695190429688 = 0.26519712805747986 + 100.0 * 6.243042945861816
Epoch 1420, val loss: 0.9908050298690796
Epoch 1430, training loss: 624.470947265625 = 0.26000434160232544 + 100.0 * 6.242109298706055
Epoch 1430, val loss: 0.9908185005187988
Epoch 1440, training loss: 623.8369750976562 = 0.2549343407154083 + 100.0 * 6.235820293426514
Epoch 1440, val loss: 0.9923704862594604
Epoch 1450, training loss: 623.701171875 = 0.25009050965309143 + 100.0 * 6.234511375427246
Epoch 1450, val loss: 0.9925251007080078
Epoch 1460, training loss: 623.6238403320312 = 0.24537703394889832 + 100.0 * 6.2337846755981445
Epoch 1460, val loss: 0.9938682317733765
Epoch 1470, training loss: 624.0037231445312 = 0.24073909223079681 + 100.0 * 6.2376298904418945
Epoch 1470, val loss: 0.994666337966919
Epoch 1480, training loss: 623.8893432617188 = 0.23597091436386108 + 100.0 * 6.2365336418151855
Epoch 1480, val loss: 0.9959624409675598
Epoch 1490, training loss: 623.673583984375 = 0.23133502900600433 + 100.0 * 6.23442268371582
Epoch 1490, val loss: 0.9966994524002075
Epoch 1500, training loss: 623.620849609375 = 0.22683390974998474 + 100.0 * 6.233940601348877
Epoch 1500, val loss: 0.9974603652954102
Epoch 1510, training loss: 623.5381469726562 = 0.22252656519412994 + 100.0 * 6.233156204223633
Epoch 1510, val loss: 0.9992037415504456
Epoch 1520, training loss: 623.7445678710938 = 0.21824520826339722 + 100.0 * 6.235263347625732
Epoch 1520, val loss: 1.0002785921096802
Epoch 1530, training loss: 623.36083984375 = 0.21400676667690277 + 100.0 * 6.231468200683594
Epoch 1530, val loss: 1.0017565488815308
Epoch 1540, training loss: 623.6041870117188 = 0.20989368855953217 + 100.0 * 6.233942985534668
Epoch 1540, val loss: 1.003040075302124
Epoch 1550, training loss: 623.818115234375 = 0.20579902827739716 + 100.0 * 6.236123085021973
Epoch 1550, val loss: 1.0045890808105469
Epoch 1560, training loss: 623.5549926757812 = 0.20172321796417236 + 100.0 * 6.233532428741455
Epoch 1560, val loss: 1.0061930418014526
Epoch 1570, training loss: 623.2949829101562 = 0.19778700172901154 + 100.0 * 6.230971813201904
Epoch 1570, val loss: 1.0070557594299316
Epoch 1580, training loss: 623.2379150390625 = 0.19398276507854462 + 100.0 * 6.23043966293335
Epoch 1580, val loss: 1.0088173151016235
Epoch 1590, training loss: 623.4844360351562 = 0.19025003910064697 + 100.0 * 6.232941627502441
Epoch 1590, val loss: 1.010377049446106
Epoch 1600, training loss: 623.29736328125 = 0.18653900921344757 + 100.0 * 6.23110818862915
Epoch 1600, val loss: 1.0126545429229736
Epoch 1610, training loss: 623.2930908203125 = 0.18288524448871613 + 100.0 * 6.231101989746094
Epoch 1610, val loss: 1.0140080451965332
Epoch 1620, training loss: 623.2633666992188 = 0.17930147051811218 + 100.0 * 6.23084020614624
Epoch 1620, val loss: 1.0156723260879517
Epoch 1630, training loss: 623.0244750976562 = 0.17583462595939636 + 100.0 * 6.228486061096191
Epoch 1630, val loss: 1.0178208351135254
Epoch 1640, training loss: 622.94482421875 = 0.17248211801052094 + 100.0 * 6.227723598480225
Epoch 1640, val loss: 1.0197683572769165
Epoch 1650, training loss: 623.2598266601562 = 0.1692337840795517 + 100.0 * 6.230905532836914
Epoch 1650, val loss: 1.0222288370132446
Epoch 1660, training loss: 623.287841796875 = 0.16593237221240997 + 100.0 * 6.2312188148498535
Epoch 1660, val loss: 1.0238780975341797
Epoch 1670, training loss: 623.31298828125 = 0.16266615688800812 + 100.0 * 6.231503009796143
Epoch 1670, val loss: 1.0260134935379028
Epoch 1680, training loss: 623.0275268554688 = 0.1594885140657425 + 100.0 * 6.22868013381958
Epoch 1680, val loss: 1.0282520055770874
Epoch 1690, training loss: 623.0684814453125 = 0.156422421336174 + 100.0 * 6.22912073135376
Epoch 1690, val loss: 1.0307857990264893
Epoch 1700, training loss: 622.854248046875 = 0.153409942984581 + 100.0 * 6.227008819580078
Epoch 1700, val loss: 1.0327577590942383
Epoch 1710, training loss: 622.881591796875 = 0.1504722684621811 + 100.0 * 6.227311134338379
Epoch 1710, val loss: 1.0346368551254272
Epoch 1720, training loss: 622.7998046875 = 0.14758312702178955 + 100.0 * 6.226521968841553
Epoch 1720, val loss: 1.0372951030731201
Epoch 1730, training loss: 622.9583129882812 = 0.14477017521858215 + 100.0 * 6.228135585784912
Epoch 1730, val loss: 1.03978431224823
Epoch 1740, training loss: 622.9888916015625 = 0.14194755256175995 + 100.0 * 6.228469371795654
Epoch 1740, val loss: 1.0418745279312134
Epoch 1750, training loss: 622.8296508789062 = 0.13920316100120544 + 100.0 * 6.226904392242432
Epoch 1750, val loss: 1.0450005531311035
Epoch 1760, training loss: 622.8175659179688 = 0.13651612401008606 + 100.0 * 6.226810455322266
Epoch 1760, val loss: 1.046558141708374
Epoch 1770, training loss: 622.6102294921875 = 0.13390620052814484 + 100.0 * 6.224762916564941
Epoch 1770, val loss: 1.0495994091033936
Epoch 1780, training loss: 623.4658813476562 = 0.13137514889240265 + 100.0 * 6.2333455085754395
Epoch 1780, val loss: 1.0522229671478271
Epoch 1790, training loss: 622.740478515625 = 0.1287812888622284 + 100.0 * 6.22611665725708
Epoch 1790, val loss: 1.0549362897872925
Epoch 1800, training loss: 622.4806518554688 = 0.12634459137916565 + 100.0 * 6.223543167114258
Epoch 1800, val loss: 1.0580536127090454
Epoch 1810, training loss: 622.4827880859375 = 0.12398627400398254 + 100.0 * 6.223588466644287
Epoch 1810, val loss: 1.060409426689148
Epoch 1820, training loss: 623.3018188476562 = 0.12164993584156036 + 100.0 * 6.231801509857178
Epoch 1820, val loss: 1.0639005899429321
Epoch 1830, training loss: 622.6278076171875 = 0.11928980052471161 + 100.0 * 6.225085258483887
Epoch 1830, val loss: 1.0655397176742554
Epoch 1840, training loss: 622.457763671875 = 0.11702340841293335 + 100.0 * 6.223407745361328
Epoch 1840, val loss: 1.0689505338668823
Epoch 1850, training loss: 622.8250122070312 = 0.11485423892736435 + 100.0 * 6.2271013259887695
Epoch 1850, val loss: 1.071132779121399
Epoch 1860, training loss: 622.3263549804688 = 0.11267051100730896 + 100.0 * 6.222136974334717
Epoch 1860, val loss: 1.0752432346343994
Epoch 1870, training loss: 622.3322143554688 = 0.1105712428689003 + 100.0 * 6.2222161293029785
Epoch 1870, val loss: 1.0773118734359741
Epoch 1880, training loss: 622.652587890625 = 0.10850723832845688 + 100.0 * 6.225440979003906
Epoch 1880, val loss: 1.0805209875106812
Epoch 1890, training loss: 623.0574951171875 = 0.1064295619726181 + 100.0 * 6.22951078414917
Epoch 1890, val loss: 1.0836560726165771
Epoch 1900, training loss: 622.3866577148438 = 0.10439088940620422 + 100.0 * 6.222822666168213
Epoch 1900, val loss: 1.0864225625991821
Epoch 1910, training loss: 622.1631469726562 = 0.10244586318731308 + 100.0 * 6.220607280731201
Epoch 1910, val loss: 1.0893114805221558
Epoch 1920, training loss: 622.1367797851562 = 0.1006038710474968 + 100.0 * 6.220361709594727
Epoch 1920, val loss: 1.092482089996338
Epoch 1930, training loss: 622.9752807617188 = 0.09878288954496384 + 100.0 * 6.22876501083374
Epoch 1930, val loss: 1.0954536199569702
Epoch 1940, training loss: 622.2615966796875 = 0.09686543047428131 + 100.0 * 6.221647262573242
Epoch 1940, val loss: 1.0990887880325317
Epoch 1950, training loss: 622.0443725585938 = 0.09509403258562088 + 100.0 * 6.2194929122924805
Epoch 1950, val loss: 1.1022307872772217
Epoch 1960, training loss: 622.014404296875 = 0.09338178485631943 + 100.0 * 6.219210147857666
Epoch 1960, val loss: 1.1053518056869507
Epoch 1970, training loss: 622.437255859375 = 0.0917198434472084 + 100.0 * 6.223455429077148
Epoch 1970, val loss: 1.1093649864196777
Epoch 1980, training loss: 622.0771484375 = 0.08998531103134155 + 100.0 * 6.219871997833252
Epoch 1980, val loss: 1.110930323600769
Epoch 1990, training loss: 622.1671142578125 = 0.08832352608442307 + 100.0 * 6.22078800201416
Epoch 1990, val loss: 1.114511489868164
Epoch 2000, training loss: 622.0211181640625 = 0.08671681582927704 + 100.0 * 6.219344139099121
Epoch 2000, val loss: 1.1181076765060425
Epoch 2010, training loss: 622.2622680664062 = 0.08515295386314392 + 100.0 * 6.221771240234375
Epoch 2010, val loss: 1.121329665184021
Epoch 2020, training loss: 622.01904296875 = 0.08361063152551651 + 100.0 * 6.219354152679443
Epoch 2020, val loss: 1.124480962753296
Epoch 2030, training loss: 622.0684204101562 = 0.08212650567293167 + 100.0 * 6.219862937927246
Epoch 2030, val loss: 1.1283621788024902
Epoch 2040, training loss: 622.1337890625 = 0.08064444363117218 + 100.0 * 6.220531463623047
Epoch 2040, val loss: 1.1307203769683838
Epoch 2050, training loss: 622.0087890625 = 0.07918907701969147 + 100.0 * 6.219296455383301
Epoch 2050, val loss: 1.1346957683563232
Epoch 2060, training loss: 621.7610473632812 = 0.07778845727443695 + 100.0 * 6.216832637786865
Epoch 2060, val loss: 1.1377317905426025
Epoch 2070, training loss: 621.791015625 = 0.07644230872392654 + 100.0 * 6.217145919799805
Epoch 2070, val loss: 1.1414506435394287
Epoch 2080, training loss: 622.4962768554688 = 0.07512585073709488 + 100.0 * 6.224211692810059
Epoch 2080, val loss: 1.1448440551757812
Epoch 2090, training loss: 622.0198364257812 = 0.07372712343931198 + 100.0 * 6.219460964202881
Epoch 2090, val loss: 1.1478725671768188
Epoch 2100, training loss: 621.7123413085938 = 0.07244811207056046 + 100.0 * 6.2163987159729
Epoch 2100, val loss: 1.15139639377594
Epoch 2110, training loss: 621.5992431640625 = 0.07121182978153229 + 100.0 * 6.215280055999756
Epoch 2110, val loss: 1.1546595096588135
Epoch 2120, training loss: 622.6770629882812 = 0.07003572583198547 + 100.0 * 6.226070404052734
Epoch 2120, val loss: 1.158024787902832
Epoch 2130, training loss: 622.0505981445312 = 0.06870889663696289 + 100.0 * 6.219818592071533
Epoch 2130, val loss: 1.1616891622543335
Epoch 2140, training loss: 621.6463012695312 = 0.0675099566578865 + 100.0 * 6.215787887573242
Epoch 2140, val loss: 1.1644630432128906
Epoch 2150, training loss: 621.498291015625 = 0.06636802852153778 + 100.0 * 6.214319229125977
Epoch 2150, val loss: 1.168697476387024
Epoch 2160, training loss: 621.4779052734375 = 0.06527616083621979 + 100.0 * 6.2141265869140625
Epoch 2160, val loss: 1.1721415519714355
Epoch 2170, training loss: 622.130615234375 = 0.06420104950666428 + 100.0 * 6.220664024353027
Epoch 2170, val loss: 1.1759374141693115
Epoch 2180, training loss: 621.5823974609375 = 0.06307017058134079 + 100.0 * 6.215193271636963
Epoch 2180, val loss: 1.178460717201233
Epoch 2190, training loss: 621.7246704101562 = 0.062004901468753815 + 100.0 * 6.2166266441345215
Epoch 2190, val loss: 1.183020830154419
Epoch 2200, training loss: 621.609130859375 = 0.06095394864678383 + 100.0 * 6.215481281280518
Epoch 2200, val loss: 1.185813307762146
Epoch 2210, training loss: 621.8482666015625 = 0.05994044989347458 + 100.0 * 6.217883110046387
Epoch 2210, val loss: 1.1894694566726685
Epoch 2220, training loss: 621.423095703125 = 0.058924950659275055 + 100.0 * 6.213642120361328
Epoch 2220, val loss: 1.192380666732788
Epoch 2230, training loss: 621.4237060546875 = 0.05796829238533974 + 100.0 * 6.213657379150391
Epoch 2230, val loss: 1.1960846185684204
Epoch 2240, training loss: 621.879150390625 = 0.057043228298425674 + 100.0 * 6.2182207107543945
Epoch 2240, val loss: 1.1992416381835938
Epoch 2250, training loss: 621.5442504882812 = 0.05607672780752182 + 100.0 * 6.214881896972656
Epoch 2250, val loss: 1.203163504600525
Epoch 2260, training loss: 621.3558959960938 = 0.055129524320364 + 100.0 * 6.213007926940918
Epoch 2260, val loss: 1.205952525138855
Epoch 2270, training loss: 621.4702758789062 = 0.05425097420811653 + 100.0 * 6.214160442352295
Epoch 2270, val loss: 1.2098288536071777
Epoch 2280, training loss: 621.5215454101562 = 0.053379692137241364 + 100.0 * 6.214681148529053
Epoch 2280, val loss: 1.2131004333496094
Epoch 2290, training loss: 621.2906494140625 = 0.05251244455575943 + 100.0 * 6.212381362915039
Epoch 2290, val loss: 1.216882348060608
Epoch 2300, training loss: 621.4671020507812 = 0.051692329347133636 + 100.0 * 6.214154243469238
Epoch 2300, val loss: 1.2198649644851685
Epoch 2310, training loss: 621.5516967773438 = 0.05086790397763252 + 100.0 * 6.21500825881958
Epoch 2310, val loss: 1.2233275175094604
Epoch 2320, training loss: 621.3160400390625 = 0.05003911629319191 + 100.0 * 6.21265983581543
Epoch 2320, val loss: 1.226784110069275
Epoch 2330, training loss: 621.6741333007812 = 0.04925290495157242 + 100.0 * 6.216248512268066
Epoch 2330, val loss: 1.22966468334198
Epoch 2340, training loss: 621.5997314453125 = 0.048473771661520004 + 100.0 * 6.215512752532959
Epoch 2340, val loss: 1.2341766357421875
Epoch 2350, training loss: 621.329345703125 = 0.04768123850226402 + 100.0 * 6.2128167152404785
Epoch 2350, val loss: 1.2373805046081543
Epoch 2360, training loss: 621.1721801757812 = 0.04693767800927162 + 100.0 * 6.211252689361572
Epoch 2360, val loss: 1.2403531074523926
Epoch 2370, training loss: 621.15673828125 = 0.046237632632255554 + 100.0 * 6.211104869842529
Epoch 2370, val loss: 1.2441734075546265
Epoch 2380, training loss: 621.2562255859375 = 0.045543987303972244 + 100.0 * 6.212106704711914
Epoch 2380, val loss: 1.247065544128418
Epoch 2390, training loss: 621.47314453125 = 0.04485055431723595 + 100.0 * 6.214282989501953
Epoch 2390, val loss: 1.2504826784133911
Epoch 2400, training loss: 621.5306396484375 = 0.04414881765842438 + 100.0 * 6.214865207672119
Epoch 2400, val loss: 1.254160761833191
Epoch 2410, training loss: 621.1557006835938 = 0.04345136508345604 + 100.0 * 6.211122512817383
Epoch 2410, val loss: 1.2570372819900513
Epoch 2420, training loss: 621.1638793945312 = 0.04281112179160118 + 100.0 * 6.21121072769165
Epoch 2420, val loss: 1.2610608339309692
Epoch 2430, training loss: 621.8253173828125 = 0.04216940328478813 + 100.0 * 6.217831134796143
Epoch 2430, val loss: 1.2633484601974487
Epoch 2440, training loss: 621.3698120117188 = 0.04150998964905739 + 100.0 * 6.213283061981201
Epoch 2440, val loss: 1.2676461935043335
Epoch 2450, training loss: 621.2645874023438 = 0.04087448865175247 + 100.0 * 6.2122368812561035
Epoch 2450, val loss: 1.2705923318862915
Epoch 2460, training loss: 620.9710693359375 = 0.04028164595365524 + 100.0 * 6.20930814743042
Epoch 2460, val loss: 1.274106740951538
Epoch 2470, training loss: 620.9892578125 = 0.03970556706190109 + 100.0 * 6.2094950675964355
Epoch 2470, val loss: 1.27772057056427
Epoch 2480, training loss: 621.996337890625 = 0.039138611406087875 + 100.0 * 6.219572067260742
Epoch 2480, val loss: 1.2805004119873047
Epoch 2490, training loss: 621.3720703125 = 0.0385332815349102 + 100.0 * 6.2133355140686035
Epoch 2490, val loss: 1.2836312055587769
Epoch 2500, training loss: 621.0252075195312 = 0.03796366602182388 + 100.0 * 6.209872245788574
Epoch 2500, val loss: 1.2873010635375977
Epoch 2510, training loss: 620.9639892578125 = 0.03743570297956467 + 100.0 * 6.20926570892334
Epoch 2510, val loss: 1.2904349565505981
Epoch 2520, training loss: 621.5833740234375 = 0.036914680153131485 + 100.0 * 6.2154645919799805
Epoch 2520, val loss: 1.2941502332687378
Epoch 2530, training loss: 621.0499877929688 = 0.036343999207019806 + 100.0 * 6.210136890411377
Epoch 2530, val loss: 1.2970004081726074
Epoch 2540, training loss: 620.9346923828125 = 0.035827361047267914 + 100.0 * 6.208988666534424
Epoch 2540, val loss: 1.3005715608596802
Epoch 2550, training loss: 620.9741821289062 = 0.03533010557293892 + 100.0 * 6.209388256072998
Epoch 2550, val loss: 1.3032453060150146
Epoch 2560, training loss: 620.8699340820312 = 0.0348367765545845 + 100.0 * 6.208351135253906
Epoch 2560, val loss: 1.3066214323043823
Epoch 2570, training loss: 621.1405029296875 = 0.034369636327028275 + 100.0 * 6.211061477661133
Epoch 2570, val loss: 1.3107192516326904
Epoch 2580, training loss: 620.76416015625 = 0.03386780619621277 + 100.0 * 6.207303047180176
Epoch 2580, val loss: 1.313188076019287
Epoch 2590, training loss: 620.836181640625 = 0.03340233862400055 + 100.0 * 6.2080278396606445
Epoch 2590, val loss: 1.3164689540863037
Epoch 2600, training loss: 621.3002319335938 = 0.03296332061290741 + 100.0 * 6.212672710418701
Epoch 2600, val loss: 1.3200386762619019
Epoch 2610, training loss: 621.2406616210938 = 0.03248635679483414 + 100.0 * 6.2120819091796875
Epoch 2610, val loss: 1.3217135667800903
Epoch 2620, training loss: 620.8399047851562 = 0.032022129744291306 + 100.0 * 6.208078384399414
Epoch 2620, val loss: 1.3255515098571777
Epoch 2630, training loss: 620.678466796875 = 0.03159740939736366 + 100.0 * 6.20646858215332
Epoch 2630, val loss: 1.3287837505340576
Epoch 2640, training loss: 620.7106323242188 = 0.031192952767014503 + 100.0 * 6.206794738769531
Epoch 2640, val loss: 1.3320298194885254
Epoch 2650, training loss: 621.4146118164062 = 0.030802365392446518 + 100.0 * 6.213837623596191
Epoch 2650, val loss: 1.3350157737731934
Epoch 2660, training loss: 620.822021484375 = 0.030353888869285583 + 100.0 * 6.207916736602783
Epoch 2660, val loss: 1.337628960609436
Epoch 2670, training loss: 620.7783203125 = 0.029956063255667686 + 100.0 * 6.207483291625977
Epoch 2670, val loss: 1.341119408607483
Epoch 2680, training loss: 621.103515625 = 0.029564116150140762 + 100.0 * 6.210739612579346
Epoch 2680, val loss: 1.3440824747085571
Epoch 2690, training loss: 621.57470703125 = 0.029174448922276497 + 100.0 * 6.215455055236816
Epoch 2690, val loss: 1.347687840461731
Epoch 2700, training loss: 620.8582153320312 = 0.028748350217938423 + 100.0 * 6.208294868469238
Epoch 2700, val loss: 1.3490853309631348
Epoch 2710, training loss: 620.63525390625 = 0.028379324823617935 + 100.0 * 6.206068992614746
Epoch 2710, val loss: 1.3535877466201782
Epoch 2720, training loss: 620.5524291992188 = 0.02803042158484459 + 100.0 * 6.205244064331055
Epoch 2720, val loss: 1.355850338935852
Epoch 2730, training loss: 620.7052001953125 = 0.02769484929740429 + 100.0 * 6.206775188446045
Epoch 2730, val loss: 1.3593099117279053
Epoch 2740, training loss: 621.0033569335938 = 0.027338841930031776 + 100.0 * 6.2097601890563965
Epoch 2740, val loss: 1.3622143268585205
Epoch 2750, training loss: 620.8252563476562 = 0.026970263570547104 + 100.0 * 6.207983016967773
Epoch 2750, val loss: 1.3647711277008057
Epoch 2760, training loss: 620.6525268554688 = 0.026637982577085495 + 100.0 * 6.206259250640869
Epoch 2760, val loss: 1.3680790662765503
Epoch 2770, training loss: 621.0386962890625 = 0.026322314515709877 + 100.0 * 6.2101240158081055
Epoch 2770, val loss: 1.37038254737854
Epoch 2780, training loss: 620.5471801757812 = 0.025965668261051178 + 100.0 * 6.205212116241455
Epoch 2780, val loss: 1.3738011121749878
Epoch 2790, training loss: 620.784912109375 = 0.025650767609477043 + 100.0 * 6.207592487335205
Epoch 2790, val loss: 1.3765324354171753
Epoch 2800, training loss: 620.6166381835938 = 0.0253351628780365 + 100.0 * 6.205913066864014
Epoch 2800, val loss: 1.3792487382888794
Epoch 2810, training loss: 620.7813720703125 = 0.025022463873028755 + 100.0 * 6.207563400268555
Epoch 2810, val loss: 1.3826079368591309
Epoch 2820, training loss: 620.5868530273438 = 0.024712279438972473 + 100.0 * 6.205621242523193
Epoch 2820, val loss: 1.3850523233413696
Epoch 2830, training loss: 620.829345703125 = 0.024416731670498848 + 100.0 * 6.208049297332764
Epoch 2830, val loss: 1.388792634010315
Epoch 2840, training loss: 620.5260009765625 = 0.02410961128771305 + 100.0 * 6.205018997192383
Epoch 2840, val loss: 1.3909966945648193
Epoch 2850, training loss: 620.4204711914062 = 0.023830149322748184 + 100.0 * 6.2039666175842285
Epoch 2850, val loss: 1.3937028646469116
Epoch 2860, training loss: 620.5691528320312 = 0.023559408262372017 + 100.0 * 6.205455780029297
Epoch 2860, val loss: 1.3969098329544067
Epoch 2870, training loss: 620.7911376953125 = 0.02327420935034752 + 100.0 * 6.20767879486084
Epoch 2870, val loss: 1.3993275165557861
Epoch 2880, training loss: 620.7091064453125 = 0.0229883324354887 + 100.0 * 6.2068610191345215
Epoch 2880, val loss: 1.4011975526809692
Epoch 2890, training loss: 620.7374877929688 = 0.022713325917720795 + 100.0 * 6.20714807510376
Epoch 2890, val loss: 1.4052730798721313
Epoch 2900, training loss: 620.572509765625 = 0.0224401094019413 + 100.0 * 6.205500602722168
Epoch 2900, val loss: 1.4072707891464233
Epoch 2910, training loss: 620.4500732421875 = 0.02217639982700348 + 100.0 * 6.204278945922852
Epoch 2910, val loss: 1.410241723060608
Epoch 2920, training loss: 620.3597412109375 = 0.02192286215722561 + 100.0 * 6.203378200531006
Epoch 2920, val loss: 1.4125490188598633
Epoch 2930, training loss: 620.4700317382812 = 0.021682364866137505 + 100.0 * 6.204483509063721
Epoch 2930, val loss: 1.4158823490142822
Epoch 2940, training loss: 620.8013916015625 = 0.021433863788843155 + 100.0 * 6.207799434661865
Epoch 2940, val loss: 1.4187581539154053
Epoch 2950, training loss: 620.357177734375 = 0.021176576614379883 + 100.0 * 6.203360080718994
Epoch 2950, val loss: 1.4204492568969727
Epoch 2960, training loss: 620.462646484375 = 0.020944636315107346 + 100.0 * 6.2044172286987305
Epoch 2960, val loss: 1.4237124919891357
Epoch 2970, training loss: 620.855224609375 = 0.020707936957478523 + 100.0 * 6.208345413208008
Epoch 2970, val loss: 1.4259954690933228
Epoch 2980, training loss: 620.3929443359375 = 0.020450271666049957 + 100.0 * 6.2037248611450195
Epoch 2980, val loss: 1.4281121492385864
Epoch 2990, training loss: 620.2401123046875 = 0.020226897671818733 + 100.0 * 6.2021989822387695
Epoch 2990, val loss: 1.4311513900756836
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 861.6107788085938 = 1.9294291734695435 + 100.0 * 8.596813201904297
Epoch 0, val loss: 1.9273064136505127
Epoch 10, training loss: 861.5015869140625 = 1.9213594198226929 + 100.0 * 8.595802307128906
Epoch 10, val loss: 1.9195917844772339
Epoch 20, training loss: 860.763427734375 = 1.9113413095474243 + 100.0 * 8.588521003723145
Epoch 20, val loss: 1.9097211360931396
Epoch 30, training loss: 855.4341430664062 = 1.898484230041504 + 100.0 * 8.535356521606445
Epoch 30, val loss: 1.8967506885528564
Epoch 40, training loss: 818.0607299804688 = 1.8824964761734009 + 100.0 * 8.161782264709473
Epoch 40, val loss: 1.8807435035705566
Epoch 50, training loss: 765.8974609375 = 1.8653978109359741 + 100.0 * 7.640320301055908
Epoch 50, val loss: 1.8646737337112427
Epoch 60, training loss: 731.378662109375 = 1.8538681268692017 + 100.0 * 7.295247554779053
Epoch 60, val loss: 1.8538395166397095
Epoch 70, training loss: 704.7239990234375 = 1.845062494277954 + 100.0 * 7.028789043426514
Epoch 70, val loss: 1.8446362018585205
Epoch 80, training loss: 691.7027587890625 = 1.8364535570144653 + 100.0 * 6.89866304397583
Epoch 80, val loss: 1.8360888957977295
Epoch 90, training loss: 682.4053344726562 = 1.8273189067840576 + 100.0 * 6.805779933929443
Epoch 90, val loss: 1.827291488647461
Epoch 100, training loss: 675.4599609375 = 1.819257140159607 + 100.0 * 6.7364068031311035
Epoch 100, val loss: 1.8192236423492432
Epoch 110, training loss: 669.89208984375 = 1.8115750551223755 + 100.0 * 6.680805206298828
Epoch 110, val loss: 1.8112897872924805
Epoch 120, training loss: 665.118896484375 = 1.8037525415420532 + 100.0 * 6.633151531219482
Epoch 120, val loss: 1.803185224533081
Epoch 130, training loss: 661.3011474609375 = 1.7959867715835571 + 100.0 * 6.5950517654418945
Epoch 130, val loss: 1.7950445413589478
Epoch 140, training loss: 658.29248046875 = 1.7878798246383667 + 100.0 * 6.5650458335876465
Epoch 140, val loss: 1.7866060733795166
Epoch 150, training loss: 655.6528930664062 = 1.7792654037475586 + 100.0 * 6.538736343383789
Epoch 150, val loss: 1.777753233909607
Epoch 160, training loss: 653.6524658203125 = 1.769932508468628 + 100.0 * 6.518825054168701
Epoch 160, val loss: 1.7683110237121582
Epoch 170, training loss: 651.7501831054688 = 1.7597122192382812 + 100.0 * 6.499904632568359
Epoch 170, val loss: 1.7580801248550415
Epoch 180, training loss: 650.0447387695312 = 1.7485800981521606 + 100.0 * 6.482961177825928
Epoch 180, val loss: 1.7470922470092773
Epoch 190, training loss: 648.912109375 = 1.736383080482483 + 100.0 * 6.471757411956787
Epoch 190, val loss: 1.735169529914856
Epoch 200, training loss: 647.5375366210938 = 1.7229220867156982 + 100.0 * 6.458146095275879
Epoch 200, val loss: 1.7221702337265015
Epoch 210, training loss: 646.3131103515625 = 1.7082555294036865 + 100.0 * 6.446048736572266
Epoch 210, val loss: 1.7081559896469116
Epoch 220, training loss: 645.2762451171875 = 1.6922746896743774 + 100.0 * 6.435839653015137
Epoch 220, val loss: 1.6929889917373657
Epoch 230, training loss: 644.2540893554688 = 1.6749913692474365 + 100.0 * 6.425791263580322
Epoch 230, val loss: 1.6767141819000244
Epoch 240, training loss: 643.3101806640625 = 1.6564109325408936 + 100.0 * 6.416537761688232
Epoch 240, val loss: 1.6593449115753174
Epoch 250, training loss: 642.77099609375 = 1.6365623474121094 + 100.0 * 6.411344528198242
Epoch 250, val loss: 1.6408249139785767
Epoch 260, training loss: 641.9899291992188 = 1.615478754043579 + 100.0 * 6.403744220733643
Epoch 260, val loss: 1.6214358806610107
Epoch 270, training loss: 641.0400390625 = 1.5933496952056885 + 100.0 * 6.394466876983643
Epoch 270, val loss: 1.6011557579040527
Epoch 280, training loss: 640.3740234375 = 1.5703603029251099 + 100.0 * 6.388036251068115
Epoch 280, val loss: 1.580297589302063
Epoch 290, training loss: 639.7704467773438 = 1.5465681552886963 + 100.0 * 6.382238864898682
Epoch 290, val loss: 1.5590057373046875
Epoch 300, training loss: 639.3048095703125 = 1.5222145318984985 + 100.0 * 6.377825736999512
Epoch 300, val loss: 1.5374536514282227
Epoch 310, training loss: 638.6747436523438 = 1.497445821762085 + 100.0 * 6.3717732429504395
Epoch 310, val loss: 1.5159562826156616
Epoch 320, training loss: 639.2593383789062 = 1.4725497961044312 + 100.0 * 6.377868175506592
Epoch 320, val loss: 1.4946095943450928
Epoch 330, training loss: 638.1389770507812 = 1.4474231004714966 + 100.0 * 6.366915225982666
Epoch 330, val loss: 1.473465919494629
Epoch 340, training loss: 637.2747802734375 = 1.4224194288253784 + 100.0 * 6.358523845672607
Epoch 340, val loss: 1.452813744544983
Epoch 350, training loss: 636.8718872070312 = 1.3976991176605225 + 100.0 * 6.35474157333374
Epoch 350, val loss: 1.4327073097229004
Epoch 360, training loss: 636.6734008789062 = 1.373164415359497 + 100.0 * 6.353002071380615
Epoch 360, val loss: 1.4131041765213013
Epoch 370, training loss: 636.1475830078125 = 1.3489222526550293 + 100.0 * 6.347986221313477
Epoch 370, val loss: 1.3940324783325195
Epoch 380, training loss: 635.720947265625 = 1.3249672651290894 + 100.0 * 6.343959808349609
Epoch 380, val loss: 1.37553071975708
Epoch 390, training loss: 636.0623779296875 = 1.3014485836029053 + 100.0 * 6.347609043121338
Epoch 390, val loss: 1.3576939105987549
Epoch 400, training loss: 635.1115112304688 = 1.2778770923614502 + 100.0 * 6.338335990905762
Epoch 400, val loss: 1.33993661403656
Epoch 410, training loss: 634.7613525390625 = 1.2548329830169678 + 100.0 * 6.3350653648376465
Epoch 410, val loss: 1.3228029012680054
Epoch 420, training loss: 634.8697509765625 = 1.2320547103881836 + 100.0 * 6.336376667022705
Epoch 420, val loss: 1.3061037063598633
Epoch 430, training loss: 634.1419677734375 = 1.209517240524292 + 100.0 * 6.329324245452881
Epoch 430, val loss: 1.2899246215820312
Epoch 440, training loss: 633.7489624023438 = 1.1872997283935547 + 100.0 * 6.325616359710693
Epoch 440, val loss: 1.2741082906723022
Epoch 450, training loss: 633.5784301757812 = 1.1653895378112793 + 100.0 * 6.324130058288574
Epoch 450, val loss: 1.2587331533432007
Epoch 460, training loss: 633.5304565429688 = 1.1436115503311157 + 100.0 * 6.323868274688721
Epoch 460, val loss: 1.243513822555542
Epoch 470, training loss: 633.4940795898438 = 1.1221061944961548 + 100.0 * 6.323719501495361
Epoch 470, val loss: 1.2287843227386475
Epoch 480, training loss: 632.8135375976562 = 1.10098135471344 + 100.0 * 6.3171257972717285
Epoch 480, val loss: 1.214432954788208
Epoch 490, training loss: 632.4542236328125 = 1.080191731452942 + 100.0 * 6.313740253448486
Epoch 490, val loss: 1.2005535364151
Epoch 500, training loss: 632.288818359375 = 1.0597691535949707 + 100.0 * 6.312290668487549
Epoch 500, val loss: 1.1870410442352295
Epoch 510, training loss: 632.1361083984375 = 1.0396289825439453 + 100.0 * 6.310965061187744
Epoch 510, val loss: 1.1740844249725342
Epoch 520, training loss: 631.7492065429688 = 1.019841194152832 + 100.0 * 6.30729341506958
Epoch 520, val loss: 1.161298155784607
Epoch 530, training loss: 631.5607299804688 = 1.0005370378494263 + 100.0 * 6.305602073669434
Epoch 530, val loss: 1.1493219137191772
Epoch 540, training loss: 631.730712890625 = 0.9814890623092651 + 100.0 * 6.307491779327393
Epoch 540, val loss: 1.137450098991394
Epoch 550, training loss: 631.2924194335938 = 0.9628462791442871 + 100.0 * 6.303296089172363
Epoch 550, val loss: 1.1262787580490112
Epoch 560, training loss: 630.9443359375 = 0.9445671439170837 + 100.0 * 6.299997329711914
Epoch 560, val loss: 1.1154040098190308
Epoch 570, training loss: 631.3621215820312 = 0.9267134666442871 + 100.0 * 6.304354190826416
Epoch 570, val loss: 1.1050913333892822
Epoch 580, training loss: 630.7996215820312 = 0.9092304706573486 + 100.0 * 6.298903942108154
Epoch 580, val loss: 1.0952529907226562
Epoch 590, training loss: 630.4244384765625 = 0.8920553922653198 + 100.0 * 6.295324325561523
Epoch 590, val loss: 1.0857954025268555
Epoch 600, training loss: 630.4942016601562 = 0.8753851652145386 + 100.0 * 6.2961883544921875
Epoch 600, val loss: 1.0770177841186523
Epoch 610, training loss: 630.24609375 = 0.8588385581970215 + 100.0 * 6.293872833251953
Epoch 610, val loss: 1.0681089162826538
Epoch 620, training loss: 629.8988647460938 = 0.8427433967590332 + 100.0 * 6.290561199188232
Epoch 620, val loss: 1.060299277305603
Epoch 630, training loss: 629.7845458984375 = 0.8269698023796082 + 100.0 * 6.289576053619385
Epoch 630, val loss: 1.0525776147842407
Epoch 640, training loss: 629.5364379882812 = 0.8115203976631165 + 100.0 * 6.2872490882873535
Epoch 640, val loss: 1.0453362464904785
Epoch 650, training loss: 629.8912963867188 = 0.7964416742324829 + 100.0 * 6.290948390960693
Epoch 650, val loss: 1.0386042594909668
Epoch 660, training loss: 629.5226440429688 = 0.7814311981201172 + 100.0 * 6.287412166595459
Epoch 660, val loss: 1.032279372215271
Epoch 670, training loss: 629.1842041015625 = 0.7667703628540039 + 100.0 * 6.28417444229126
Epoch 670, val loss: 1.0262035131454468
Epoch 680, training loss: 628.8939208984375 = 0.752450704574585 + 100.0 * 6.28141450881958
Epoch 680, val loss: 1.0207592248916626
Epoch 690, training loss: 629.184814453125 = 0.7384248971939087 + 100.0 * 6.284463882446289
Epoch 690, val loss: 1.0156712532043457
Epoch 700, training loss: 628.820556640625 = 0.7246407270431519 + 100.0 * 6.280958652496338
Epoch 700, val loss: 1.0109220743179321
Epoch 710, training loss: 628.57373046875 = 0.7110587954521179 + 100.0 * 6.278626918792725
Epoch 710, val loss: 1.006699800491333
Epoch 720, training loss: 628.9365844726562 = 0.6978642344474792 + 100.0 * 6.2823872566223145
Epoch 720, val loss: 1.0024755001068115
Epoch 730, training loss: 628.8445434570312 = 0.6847542524337769 + 100.0 * 6.281598091125488
Epoch 730, val loss: 0.9991791844367981
Epoch 740, training loss: 628.2921142578125 = 0.6719868183135986 + 100.0 * 6.276201248168945
Epoch 740, val loss: 0.9960880875587463
Epoch 750, training loss: 627.998291015625 = 0.6595315933227539 + 100.0 * 6.273387432098389
Epoch 750, val loss: 0.9932084679603577
Epoch 760, training loss: 627.9436645507812 = 0.6473711133003235 + 100.0 * 6.27296257019043
Epoch 760, val loss: 0.9907269477844238
Epoch 770, training loss: 628.5255126953125 = 0.6353939771652222 + 100.0 * 6.27890157699585
Epoch 770, val loss: 0.988616943359375
Epoch 780, training loss: 627.8798217773438 = 0.6235820055007935 + 100.0 * 6.272562503814697
Epoch 780, val loss: 0.986961305141449
Epoch 790, training loss: 627.6182861328125 = 0.6120331287384033 + 100.0 * 6.27006196975708
Epoch 790, val loss: 0.985463559627533
Epoch 800, training loss: 627.7218017578125 = 0.6007594466209412 + 100.0 * 6.271210193634033
Epoch 800, val loss: 0.984294056892395
Epoch 810, training loss: 627.5103759765625 = 0.5897061228752136 + 100.0 * 6.269206523895264
Epoch 810, val loss: 0.9836071729660034
Epoch 820, training loss: 627.2916259765625 = 0.5788798332214355 + 100.0 * 6.267127990722656
Epoch 820, val loss: 0.9829806685447693
Epoch 830, training loss: 627.2658081054688 = 0.5683025121688843 + 100.0 * 6.266974925994873
Epoch 830, val loss: 0.9829183220863342
Epoch 840, training loss: 627.4097900390625 = 0.5579442977905273 + 100.0 * 6.268518447875977
Epoch 840, val loss: 0.9828051328659058
Epoch 850, training loss: 627.197998046875 = 0.5477228760719299 + 100.0 * 6.266502857208252
Epoch 850, val loss: 0.982791006565094
Epoch 860, training loss: 627.2669677734375 = 0.5376789569854736 + 100.0 * 6.2672929763793945
Epoch 860, val loss: 0.983199954032898
Epoch 870, training loss: 626.9921875 = 0.5278869271278381 + 100.0 * 6.264642715454102
Epoch 870, val loss: 0.9840896129608154
Epoch 880, training loss: 626.7349243164062 = 0.5182823538780212 + 100.0 * 6.2621660232543945
Epoch 880, val loss: 0.9849429726600647
Epoch 890, training loss: 626.8160400390625 = 0.5089040398597717 + 100.0 * 6.263071537017822
Epoch 890, val loss: 0.9860987663269043
Epoch 900, training loss: 626.6734619140625 = 0.49966391921043396 + 100.0 * 6.261738300323486
Epoch 900, val loss: 0.9873277544975281
Epoch 910, training loss: 626.6325073242188 = 0.49062055349349976 + 100.0 * 6.26141881942749
Epoch 910, val loss: 0.9889227747917175
Epoch 920, training loss: 626.8035888671875 = 0.4817190170288086 + 100.0 * 6.263218879699707
Epoch 920, val loss: 0.990850031375885
Epoch 930, training loss: 627.1926879882812 = 0.47294214367866516 + 100.0 * 6.267197132110596
Epoch 930, val loss: 0.9923200607299805
Epoch 940, training loss: 626.3888549804688 = 0.46425938606262207 + 100.0 * 6.259246349334717
Epoch 940, val loss: 0.9940580129623413
Epoch 950, training loss: 626.13818359375 = 0.4558325707912445 + 100.0 * 6.256823539733887
Epoch 950, val loss: 0.9964967966079712
Epoch 960, training loss: 626.1538696289062 = 0.44756847620010376 + 100.0 * 6.257062911987305
Epoch 960, val loss: 0.9985944032669067
Epoch 970, training loss: 626.29150390625 = 0.43934395909309387 + 100.0 * 6.258522033691406
Epoch 970, val loss: 1.0012054443359375
Epoch 980, training loss: 626.0075073242188 = 0.43126264214515686 + 100.0 * 6.255762577056885
Epoch 980, val loss: 1.0035473108291626
Epoch 990, training loss: 625.8826293945312 = 0.42336493730545044 + 100.0 * 6.2545928955078125
Epoch 990, val loss: 1.0066533088684082
Epoch 1000, training loss: 625.7631225585938 = 0.4156252443790436 + 100.0 * 6.253474712371826
Epoch 1000, val loss: 1.009528398513794
Epoch 1010, training loss: 626.5859375 = 0.4080381989479065 + 100.0 * 6.261779308319092
Epoch 1010, val loss: 1.0129339694976807
Epoch 1020, training loss: 626.2384643554688 = 0.40040165185928345 + 100.0 * 6.258380889892578
Epoch 1020, val loss: 1.0156004428863525
Epoch 1030, training loss: 625.6744384765625 = 0.39288750290870667 + 100.0 * 6.2528157234191895
Epoch 1030, val loss: 1.0187878608703613
Epoch 1040, training loss: 625.6604614257812 = 0.38561511039733887 + 100.0 * 6.252748489379883
Epoch 1040, val loss: 1.022526741027832
Epoch 1050, training loss: 625.6367797851562 = 0.37844523787498474 + 100.0 * 6.2525835037231445
Epoch 1050, val loss: 1.025865912437439
Epoch 1060, training loss: 625.5254516601562 = 0.3713729679584503 + 100.0 * 6.251540660858154
Epoch 1060, val loss: 1.0294009447097778
Epoch 1070, training loss: 625.2819213867188 = 0.36439454555511475 + 100.0 * 6.249175548553467
Epoch 1070, val loss: 1.0331475734710693
Epoch 1080, training loss: 625.8820190429688 = 0.3575710952281952 + 100.0 * 6.255244731903076
Epoch 1080, val loss: 1.0366731882095337
Epoch 1090, training loss: 625.4520874023438 = 0.3507333993911743 + 100.0 * 6.25101375579834
Epoch 1090, val loss: 1.0408084392547607
Epoch 1100, training loss: 625.2404174804688 = 0.3440277874469757 + 100.0 * 6.248963356018066
Epoch 1100, val loss: 1.0446487665176392
Epoch 1110, training loss: 625.1973876953125 = 0.33749258518218994 + 100.0 * 6.248599052429199
Epoch 1110, val loss: 1.0490909814834595
Epoch 1120, training loss: 625.5521850585938 = 0.33102861046791077 + 100.0 * 6.252211093902588
Epoch 1120, val loss: 1.0533037185668945
Epoch 1130, training loss: 625.0897827148438 = 0.3246108889579773 + 100.0 * 6.247652053833008
Epoch 1130, val loss: 1.0571238994598389
Epoch 1140, training loss: 624.909912109375 = 0.3183984160423279 + 100.0 * 6.245914936065674
Epoch 1140, val loss: 1.0619062185287476
Epoch 1150, training loss: 624.8369750976562 = 0.31231889128685 + 100.0 * 6.245246887207031
Epoch 1150, val loss: 1.0662025213241577
Epoch 1160, training loss: 625.47607421875 = 0.3063508868217468 + 100.0 * 6.251697540283203
Epoch 1160, val loss: 1.0707734823226929
Epoch 1170, training loss: 625.2142944335938 = 0.30021703243255615 + 100.0 * 6.249140739440918
Epoch 1170, val loss: 1.0752081871032715
Epoch 1180, training loss: 624.7759399414062 = 0.2943322956562042 + 100.0 * 6.244816303253174
Epoch 1180, val loss: 1.0799224376678467
Epoch 1190, training loss: 624.576171875 = 0.2886103391647339 + 100.0 * 6.242875576019287
Epoch 1190, val loss: 1.0847883224487305
Epoch 1200, training loss: 624.5526733398438 = 0.28301355242729187 + 100.0 * 6.242696762084961
Epoch 1200, val loss: 1.0897740125656128
Epoch 1210, training loss: 625.38671875 = 0.2775382399559021 + 100.0 * 6.251091957092285
Epoch 1210, val loss: 1.0949448347091675
Epoch 1220, training loss: 624.9840698242188 = 0.2719428241252899 + 100.0 * 6.2471208572387695
Epoch 1220, val loss: 1.0993857383728027
Epoch 1230, training loss: 624.816162109375 = 0.2665289342403412 + 100.0 * 6.2454962730407715
Epoch 1230, val loss: 1.1042569875717163
Epoch 1240, training loss: 624.32861328125 = 0.26122143864631653 + 100.0 * 6.240674018859863
Epoch 1240, val loss: 1.1097902059555054
Epoch 1250, training loss: 624.388427734375 = 0.25608909130096436 + 100.0 * 6.241323471069336
Epoch 1250, val loss: 1.1151806116104126
Epoch 1260, training loss: 624.5084228515625 = 0.2510126829147339 + 100.0 * 6.2425737380981445
Epoch 1260, val loss: 1.1206130981445312
Epoch 1270, training loss: 624.4437866210938 = 0.2459787279367447 + 100.0 * 6.241978168487549
Epoch 1270, val loss: 1.1259760856628418
Epoch 1280, training loss: 624.261962890625 = 0.241007998585701 + 100.0 * 6.240209102630615
Epoch 1280, val loss: 1.1307872533798218
Epoch 1290, training loss: 624.2246704101562 = 0.23616191744804382 + 100.0 * 6.239885330200195
Epoch 1290, val loss: 1.1362056732177734
Epoch 1300, training loss: 624.1127319335938 = 0.2314133495092392 + 100.0 * 6.238813400268555
Epoch 1300, val loss: 1.1422138214111328
Epoch 1310, training loss: 624.361328125 = 0.22679659724235535 + 100.0 * 6.241344928741455
Epoch 1310, val loss: 1.1476515531539917
Epoch 1320, training loss: 624.441650390625 = 0.22213919460773468 + 100.0 * 6.242195129394531
Epoch 1320, val loss: 1.1533501148223877
Epoch 1330, training loss: 624.09619140625 = 0.21754705905914307 + 100.0 * 6.238786697387695
Epoch 1330, val loss: 1.1586699485778809
Epoch 1340, training loss: 623.91357421875 = 0.2131722867488861 + 100.0 * 6.237003803253174
Epoch 1340, val loss: 1.1643747091293335
Epoch 1350, training loss: 623.777099609375 = 0.20890073478221893 + 100.0 * 6.235681533813477
Epoch 1350, val loss: 1.1703218221664429
Epoch 1360, training loss: 624.151123046875 = 0.20470808446407318 + 100.0 * 6.239463806152344
Epoch 1360, val loss: 1.1756105422973633
Epoch 1370, training loss: 623.8401489257812 = 0.20046988129615784 + 100.0 * 6.2363972663879395
Epoch 1370, val loss: 1.1817213296890259
Epoch 1380, training loss: 623.7069702148438 = 0.19636446237564087 + 100.0 * 6.235105991363525
Epoch 1380, val loss: 1.1874053478240967
Epoch 1390, training loss: 623.6469116210938 = 0.19238610565662384 + 100.0 * 6.2345452308654785
Epoch 1390, val loss: 1.1935973167419434
Epoch 1400, training loss: 623.8303833007812 = 0.18850848078727722 + 100.0 * 6.2364182472229
Epoch 1400, val loss: 1.1991515159606934
Epoch 1410, training loss: 623.7601928710938 = 0.18462593853473663 + 100.0 * 6.235755920410156
Epoch 1410, val loss: 1.2053629159927368
Epoch 1420, training loss: 623.6253051757812 = 0.18085938692092896 + 100.0 * 6.234444618225098
Epoch 1420, val loss: 1.2113549709320068
Epoch 1430, training loss: 623.6871948242188 = 0.17718876898288727 + 100.0 * 6.235100269317627
Epoch 1430, val loss: 1.2173644304275513
Epoch 1440, training loss: 623.7090454101562 = 0.17358048260211945 + 100.0 * 6.235354423522949
Epoch 1440, val loss: 1.2233585119247437
Epoch 1450, training loss: 623.4671020507812 = 0.17004527151584625 + 100.0 * 6.232970714569092
Epoch 1450, val loss: 1.229483723640442
Epoch 1460, training loss: 623.4874267578125 = 0.1666080206632614 + 100.0 * 6.233207702636719
Epoch 1460, val loss: 1.2354633808135986
Epoch 1470, training loss: 623.5263671875 = 0.16326484084129333 + 100.0 * 6.233631134033203
Epoch 1470, val loss: 1.2417956590652466
Epoch 1480, training loss: 623.333984375 = 0.15994998812675476 + 100.0 * 6.2317399978637695
Epoch 1480, val loss: 1.247389316558838
Epoch 1490, training loss: 624.1066284179688 = 0.15672586858272552 + 100.0 * 6.239498615264893
Epoch 1490, val loss: 1.253222107887268
Epoch 1500, training loss: 623.4619750976562 = 0.15350671112537384 + 100.0 * 6.233084678649902
Epoch 1500, val loss: 1.2595493793487549
Epoch 1510, training loss: 623.278564453125 = 0.15042132139205933 + 100.0 * 6.231281280517578
Epoch 1510, val loss: 1.265682578086853
Epoch 1520, training loss: 623.2224731445312 = 0.14742562174797058 + 100.0 * 6.230750560760498
Epoch 1520, val loss: 1.271667242050171
Epoch 1530, training loss: 623.3924560546875 = 0.1444910317659378 + 100.0 * 6.232479572296143
Epoch 1530, val loss: 1.2778398990631104
Epoch 1540, training loss: 623.1217651367188 = 0.14159689843654633 + 100.0 * 6.229801654815674
Epoch 1540, val loss: 1.2839597463607788
Epoch 1550, training loss: 623.9178466796875 = 0.13879482448101044 + 100.0 * 6.237790584564209
Epoch 1550, val loss: 1.290185570716858
Epoch 1560, training loss: 623.3700561523438 = 0.13597562909126282 + 100.0 * 6.2323408126831055
Epoch 1560, val loss: 1.29559326171875
Epoch 1570, training loss: 622.9881591796875 = 0.13324259221553802 + 100.0 * 6.228549480438232
Epoch 1570, val loss: 1.3021289110183716
Epoch 1580, training loss: 622.91015625 = 0.130637526512146 + 100.0 * 6.227795600891113
Epoch 1580, val loss: 1.3083325624465942
Epoch 1590, training loss: 623.0783081054688 = 0.12810082733631134 + 100.0 * 6.229502201080322
Epoch 1590, val loss: 1.3141286373138428
Epoch 1600, training loss: 623.3140258789062 = 0.1255647838115692 + 100.0 * 6.231884479522705
Epoch 1600, val loss: 1.3204032182693481
Epoch 1610, training loss: 623.1951904296875 = 0.12309379130601883 + 100.0 * 6.2307209968566895
Epoch 1610, val loss: 1.3272026777267456
Epoch 1620, training loss: 622.8655395507812 = 0.12066836655139923 + 100.0 * 6.2274489402771
Epoch 1620, val loss: 1.3327656984329224
Epoch 1630, training loss: 622.7910766601562 = 0.11835460364818573 + 100.0 * 6.22672700881958
Epoch 1630, val loss: 1.3397066593170166
Epoch 1640, training loss: 622.9610595703125 = 0.11609252542257309 + 100.0 * 6.228449821472168
Epoch 1640, val loss: 1.3456538915634155
Epoch 1650, training loss: 622.9395751953125 = 0.11382929980754852 + 100.0 * 6.228257179260254
Epoch 1650, val loss: 1.3517907857894897
Epoch 1660, training loss: 622.7124633789062 = 0.11160609871149063 + 100.0 * 6.226008415222168
Epoch 1660, val loss: 1.357500433921814
Epoch 1670, training loss: 622.6934814453125 = 0.10948249697685242 + 100.0 * 6.225839614868164
Epoch 1670, val loss: 1.3638118505477905
Epoch 1680, training loss: 623.135986328125 = 0.10740451514720917 + 100.0 * 6.23028564453125
Epoch 1680, val loss: 1.3697854280471802
Epoch 1690, training loss: 622.9529418945312 = 0.1053147092461586 + 100.0 * 6.228476524353027
Epoch 1690, val loss: 1.3759961128234863
Epoch 1700, training loss: 622.6981811523438 = 0.10328327119350433 + 100.0 * 6.225948810577393
Epoch 1700, val loss: 1.3817945718765259
Epoch 1710, training loss: 622.6412963867188 = 0.1013496145606041 + 100.0 * 6.225399017333984
Epoch 1710, val loss: 1.3882817029953003
Epoch 1720, training loss: 622.4906005859375 = 0.09946075081825256 + 100.0 * 6.223911285400391
Epoch 1720, val loss: 1.3947128057479858
Epoch 1730, training loss: 622.5667724609375 = 0.09763806313276291 + 100.0 * 6.224691390991211
Epoch 1730, val loss: 1.4010536670684814
Epoch 1740, training loss: 623.0748291015625 = 0.0958348959684372 + 100.0 * 6.229789733886719
Epoch 1740, val loss: 1.4073727130889893
Epoch 1750, training loss: 622.96630859375 = 0.09400981664657593 + 100.0 * 6.228723049163818
Epoch 1750, val loss: 1.4119359254837036
Epoch 1760, training loss: 622.894287109375 = 0.09224303811788559 + 100.0 * 6.228020668029785
Epoch 1760, val loss: 1.4185549020767212
Epoch 1770, training loss: 622.521484375 = 0.09052694588899612 + 100.0 * 6.22430944442749
Epoch 1770, val loss: 1.4247781038284302
Epoch 1780, training loss: 622.3541870117188 = 0.08888530731201172 + 100.0 * 6.222653388977051
Epoch 1780, val loss: 1.4312431812286377
Epoch 1790, training loss: 622.4324340820312 = 0.08728733658790588 + 100.0 * 6.223451614379883
Epoch 1790, val loss: 1.4373033046722412
Epoch 1800, training loss: 622.7268676757812 = 0.08571573346853256 + 100.0 * 6.226411819458008
Epoch 1800, val loss: 1.4436782598495483
Epoch 1810, training loss: 622.28759765625 = 0.08412716537714005 + 100.0 * 6.222034931182861
Epoch 1810, val loss: 1.448900580406189
Epoch 1820, training loss: 622.7887573242188 = 0.08262437582015991 + 100.0 * 6.2270612716674805
Epoch 1820, val loss: 1.4549278020858765
Epoch 1830, training loss: 622.4368286132812 = 0.0811266154050827 + 100.0 * 6.223556995391846
Epoch 1830, val loss: 1.4617172479629517
Epoch 1840, training loss: 622.3954467773438 = 0.07966305315494537 + 100.0 * 6.22315788269043
Epoch 1840, val loss: 1.467347264289856
Epoch 1850, training loss: 622.3240966796875 = 0.07824651151895523 + 100.0 * 6.222458362579346
Epoch 1850, val loss: 1.4740363359451294
Epoch 1860, training loss: 622.474609375 = 0.07685412466526031 + 100.0 * 6.223977565765381
Epoch 1860, val loss: 1.4795887470245361
Epoch 1870, training loss: 622.3626098632812 = 0.07548212260007858 + 100.0 * 6.222870826721191
Epoch 1870, val loss: 1.4854049682617188
Epoch 1880, training loss: 622.2971801757812 = 0.07414253801107407 + 100.0 * 6.222230434417725
Epoch 1880, val loss: 1.4913253784179688
Epoch 1890, training loss: 622.1641845703125 = 0.07282882183790207 + 100.0 * 6.220913410186768
Epoch 1890, val loss: 1.4969731569290161
Epoch 1900, training loss: 622.1050415039062 = 0.07157036662101746 + 100.0 * 6.220334529876709
Epoch 1900, val loss: 1.503117322921753
Epoch 1910, training loss: 622.2000122070312 = 0.0703331008553505 + 100.0 * 6.221296787261963
Epoch 1910, val loss: 1.5086841583251953
Epoch 1920, training loss: 622.16259765625 = 0.06910594552755356 + 100.0 * 6.220935344696045
Epoch 1920, val loss: 1.5145617723464966
Epoch 1930, training loss: 622.2449340820312 = 0.06791672110557556 + 100.0 * 6.2217698097229
Epoch 1930, val loss: 1.520253300666809
Epoch 1940, training loss: 622.3732299804688 = 0.06672277301549911 + 100.0 * 6.22306489944458
Epoch 1940, val loss: 1.5259695053100586
Epoch 1950, training loss: 622.0607299804688 = 0.06554568558931351 + 100.0 * 6.219951629638672
Epoch 1950, val loss: 1.532009482383728
Epoch 1960, training loss: 621.92626953125 = 0.06442634016275406 + 100.0 * 6.218618392944336
Epoch 1960, val loss: 1.53785240650177
Epoch 1970, training loss: 622.091796875 = 0.06335877627134323 + 100.0 * 6.220284461975098
Epoch 1970, val loss: 1.5439743995666504
Epoch 1980, training loss: 621.9718627929688 = 0.0622732900083065 + 100.0 * 6.2190961837768555
Epoch 1980, val loss: 1.549256443977356
Epoch 1990, training loss: 622.0263671875 = 0.06121828407049179 + 100.0 * 6.219651222229004
Epoch 1990, val loss: 1.5547515153884888
Epoch 2000, training loss: 622.3731079101562 = 0.060191310942173004 + 100.0 * 6.2231292724609375
Epoch 2000, val loss: 1.5605312585830688
Epoch 2010, training loss: 622.0134887695312 = 0.05915472283959389 + 100.0 * 6.21954345703125
Epoch 2010, val loss: 1.5652024745941162
Epoch 2020, training loss: 622.455810546875 = 0.05815645679831505 + 100.0 * 6.2239766120910645
Epoch 2020, val loss: 1.5705244541168213
Epoch 2030, training loss: 621.9231567382812 = 0.05717109143733978 + 100.0 * 6.2186598777771
Epoch 2030, val loss: 1.5770103931427002
Epoch 2040, training loss: 621.7996826171875 = 0.0562206469476223 + 100.0 * 6.217434883117676
Epoch 2040, val loss: 1.5819813013076782
Epoch 2050, training loss: 621.6871337890625 = 0.05530029907822609 + 100.0 * 6.216318607330322
Epoch 2050, val loss: 1.587506890296936
Epoch 2060, training loss: 621.7855834960938 = 0.0544099435210228 + 100.0 * 6.217311859130859
Epoch 2060, val loss: 1.5929185152053833
Epoch 2070, training loss: 622.227294921875 = 0.05352022871375084 + 100.0 * 6.221737384796143
Epoch 2070, val loss: 1.5977815389633179
Epoch 2080, training loss: 621.938720703125 = 0.052624333649873734 + 100.0 * 6.218861103057861
Epoch 2080, val loss: 1.6031087636947632
Epoch 2090, training loss: 621.6683349609375 = 0.05176832154393196 + 100.0 * 6.216165542602539
Epoch 2090, val loss: 1.6091282367706299
Epoch 2100, training loss: 621.6851806640625 = 0.050944745540618896 + 100.0 * 6.216342449188232
Epoch 2100, val loss: 1.6145433187484741
Epoch 2110, training loss: 622.8064575195312 = 0.05013909563422203 + 100.0 * 6.227563381195068
Epoch 2110, val loss: 1.6197565793991089
Epoch 2120, training loss: 621.850830078125 = 0.04927000775933266 + 100.0 * 6.218015670776367
Epoch 2120, val loss: 1.6236299276351929
Epoch 2130, training loss: 621.5711059570312 = 0.048481471836566925 + 100.0 * 6.215226650238037
Epoch 2130, val loss: 1.6293772459030151
Epoch 2140, training loss: 621.66455078125 = 0.047722067683935165 + 100.0 * 6.216168403625488
Epoch 2140, val loss: 1.6343058347702026
Epoch 2150, training loss: 621.8033447265625 = 0.046967215836048126 + 100.0 * 6.217563629150391
Epoch 2150, val loss: 1.6393624544143677
Epoch 2160, training loss: 621.5563354492188 = 0.046219225972890854 + 100.0 * 6.21510124206543
Epoch 2160, val loss: 1.6450411081314087
Epoch 2170, training loss: 621.82666015625 = 0.0454951748251915 + 100.0 * 6.2178120613098145
Epoch 2170, val loss: 1.649127721786499
Epoch 2180, training loss: 621.5690307617188 = 0.044776786118745804 + 100.0 * 6.215242385864258
Epoch 2180, val loss: 1.6546108722686768
Epoch 2190, training loss: 621.8377075195312 = 0.044082511216402054 + 100.0 * 6.2179365158081055
Epoch 2190, val loss: 1.65985906124115
Epoch 2200, training loss: 621.5392456054688 = 0.04338237643241882 + 100.0 * 6.214958190917969
Epoch 2200, val loss: 1.6643449068069458
Epoch 2210, training loss: 621.3732299804688 = 0.04272199049592018 + 100.0 * 6.2133049964904785
Epoch 2210, val loss: 1.6702821254730225
Epoch 2220, training loss: 621.5076293945312 = 0.04207754135131836 + 100.0 * 6.214655876159668
Epoch 2220, val loss: 1.6751936674118042
Epoch 2230, training loss: 622.1400146484375 = 0.0414278544485569 + 100.0 * 6.2209858894348145
Epoch 2230, val loss: 1.679306149482727
Epoch 2240, training loss: 621.508544921875 = 0.0407601073384285 + 100.0 * 6.214677810668945
Epoch 2240, val loss: 1.6838146448135376
Epoch 2250, training loss: 621.30859375 = 0.04013846814632416 + 100.0 * 6.212684154510498
Epoch 2250, val loss: 1.688948392868042
Epoch 2260, training loss: 621.26220703125 = 0.03953976184129715 + 100.0 * 6.212226867675781
Epoch 2260, val loss: 1.693480372428894
Epoch 2270, training loss: 621.8548583984375 = 0.03896516561508179 + 100.0 * 6.218159198760986
Epoch 2270, val loss: 1.697994351387024
Epoch 2280, training loss: 621.2835693359375 = 0.03836315870285034 + 100.0 * 6.212451934814453
Epoch 2280, val loss: 1.7030613422393799
Epoch 2290, training loss: 621.3478393554688 = 0.03778882324695587 + 100.0 * 6.213100433349609
Epoch 2290, val loss: 1.7076318264007568
Epoch 2300, training loss: 621.5670776367188 = 0.037228360772132874 + 100.0 * 6.215298175811768
Epoch 2300, val loss: 1.7125210762023926
Epoch 2310, training loss: 621.21533203125 = 0.03666697442531586 + 100.0 * 6.211786270141602
Epoch 2310, val loss: 1.716443419456482
Epoch 2320, training loss: 621.74609375 = 0.0361454077064991 + 100.0 * 6.217099666595459
Epoch 2320, val loss: 1.7208575010299683
Epoch 2330, training loss: 621.5894775390625 = 0.0356014184653759 + 100.0 * 6.21553897857666
Epoch 2330, val loss: 1.7250524759292603
Epoch 2340, training loss: 621.2277221679688 = 0.035065386444330215 + 100.0 * 6.211925983428955
Epoch 2340, val loss: 1.7295150756835938
Epoch 2350, training loss: 621.2001953125 = 0.03455852344632149 + 100.0 * 6.21165657043457
Epoch 2350, val loss: 1.7341376543045044
Epoch 2360, training loss: 621.6719970703125 = 0.03406880795955658 + 100.0 * 6.216379165649414
Epoch 2360, val loss: 1.7376221418380737
Epoch 2370, training loss: 621.233154296875 = 0.03356572613120079 + 100.0 * 6.211996078491211
Epoch 2370, val loss: 1.7426422834396362
Epoch 2380, training loss: 621.0802612304688 = 0.03308510035276413 + 100.0 * 6.2104716300964355
Epoch 2380, val loss: 1.7477257251739502
Epoch 2390, training loss: 621.1605834960938 = 0.032622236758470535 + 100.0 * 6.21127986907959
Epoch 2390, val loss: 1.7517242431640625
Epoch 2400, training loss: 622.1791381835938 = 0.03216923400759697 + 100.0 * 6.221469879150391
Epoch 2400, val loss: 1.7553951740264893
Epoch 2410, training loss: 621.4503784179688 = 0.03168129920959473 + 100.0 * 6.214187145233154
Epoch 2410, val loss: 1.7596186399459839
Epoch 2420, training loss: 621.1683959960938 = 0.0312334094196558 + 100.0 * 6.211371421813965
Epoch 2420, val loss: 1.7646539211273193
Epoch 2430, training loss: 621.0432739257812 = 0.03079463727772236 + 100.0 * 6.210124492645264
Epoch 2430, val loss: 1.7680093050003052
Epoch 2440, training loss: 621.021728515625 = 0.030382562428712845 + 100.0 * 6.20991325378418
Epoch 2440, val loss: 1.7724887132644653
Epoch 2450, training loss: 621.86083984375 = 0.02998281456530094 + 100.0 * 6.218308448791504
Epoch 2450, val loss: 1.7760995626449585
Epoch 2460, training loss: 621.577880859375 = 0.029548726975917816 + 100.0 * 6.21548318862915
Epoch 2460, val loss: 1.7809842824935913
Epoch 2470, training loss: 621.1558227539062 = 0.029114890843629837 + 100.0 * 6.211266994476318
Epoch 2470, val loss: 1.78363037109375
Epoch 2480, training loss: 620.992919921875 = 0.028721557930111885 + 100.0 * 6.209641933441162
Epoch 2480, val loss: 1.7882375717163086
Epoch 2490, training loss: 620.930908203125 = 0.02834167145192623 + 100.0 * 6.209025859832764
Epoch 2490, val loss: 1.792196273803711
Epoch 2500, training loss: 621.2379760742188 = 0.027971195057034492 + 100.0 * 6.212100028991699
Epoch 2500, val loss: 1.79594886302948
Epoch 2510, training loss: 620.9627685546875 = 0.027579907327890396 + 100.0 * 6.209352016448975
Epoch 2510, val loss: 1.7993439435958862
Epoch 2520, training loss: 621.5950927734375 = 0.027212494984269142 + 100.0 * 6.215678691864014
Epoch 2520, val loss: 1.8025201559066772
Epoch 2530, training loss: 621.2203979492188 = 0.026837991550564766 + 100.0 * 6.211935520172119
Epoch 2530, val loss: 1.8074098825454712
Epoch 2540, training loss: 620.9426879882812 = 0.02646891586482525 + 100.0 * 6.209161758422852
Epoch 2540, val loss: 1.8112914562225342
Epoch 2550, training loss: 620.8381958007812 = 0.026126570999622345 + 100.0 * 6.208120822906494
Epoch 2550, val loss: 1.8150562047958374
Epoch 2560, training loss: 620.873046875 = 0.025794433429837227 + 100.0 * 6.20847225189209
Epoch 2560, val loss: 1.8188506364822388
Epoch 2570, training loss: 621.8927612304688 = 0.025463853031396866 + 100.0 * 6.218673229217529
Epoch 2570, val loss: 1.8217647075653076
Epoch 2580, training loss: 621.0758056640625 = 0.02510572224855423 + 100.0 * 6.210507392883301
Epoch 2580, val loss: 1.82587730884552
Epoch 2590, training loss: 620.858154296875 = 0.024781249463558197 + 100.0 * 6.208333492279053
Epoch 2590, val loss: 1.8293222188949585
Epoch 2600, training loss: 620.8101196289062 = 0.02446761541068554 + 100.0 * 6.207856178283691
Epoch 2600, val loss: 1.8326667547225952
Epoch 2610, training loss: 621.5328979492188 = 0.024166572839021683 + 100.0 * 6.215087413787842
Epoch 2610, val loss: 1.835490345954895
Epoch 2620, training loss: 620.8792724609375 = 0.023845279589295387 + 100.0 * 6.208553791046143
Epoch 2620, val loss: 1.840262770652771
Epoch 2630, training loss: 620.8004150390625 = 0.023539399728178978 + 100.0 * 6.207768440246582
Epoch 2630, val loss: 1.843398094177246
Epoch 2640, training loss: 621.0297241210938 = 0.02324243076145649 + 100.0 * 6.210064888000488
Epoch 2640, val loss: 1.8466275930404663
Epoch 2650, training loss: 620.8472290039062 = 0.022944210097193718 + 100.0 * 6.208242893218994
Epoch 2650, val loss: 1.850687026977539
Epoch 2660, training loss: 621.1694946289062 = 0.022662149742245674 + 100.0 * 6.21146821975708
Epoch 2660, val loss: 1.8544541597366333
Epoch 2670, training loss: 620.7024536132812 = 0.02236662060022354 + 100.0 * 6.206800937652588
Epoch 2670, val loss: 1.8570410013198853
Epoch 2680, training loss: 620.6582641601562 = 0.022093871608376503 + 100.0 * 6.206361770629883
Epoch 2680, val loss: 1.8609287738800049
Epoch 2690, training loss: 620.6885986328125 = 0.021830009296536446 + 100.0 * 6.206667423248291
Epoch 2690, val loss: 1.8639755249023438
Epoch 2700, training loss: 621.1673583984375 = 0.021573400124907494 + 100.0 * 6.211458206176758
Epoch 2700, val loss: 1.8664419651031494
Epoch 2710, training loss: 621.0413818359375 = 0.021303238347172737 + 100.0 * 6.210200786590576
Epoch 2710, val loss: 1.8705651760101318
Epoch 2720, training loss: 620.8678588867188 = 0.02103925682604313 + 100.0 * 6.208467960357666
Epoch 2720, val loss: 1.8736991882324219
Epoch 2730, training loss: 620.6472778320312 = 0.020778512582182884 + 100.0 * 6.206264972686768
Epoch 2730, val loss: 1.876485824584961
Epoch 2740, training loss: 620.9443359375 = 0.020533179864287376 + 100.0 * 6.209238052368164
Epoch 2740, val loss: 1.8792593479156494
Epoch 2750, training loss: 620.7646484375 = 0.020283292979002 + 100.0 * 6.207443714141846
Epoch 2750, val loss: 1.8831908702850342
Epoch 2760, training loss: 620.648681640625 = 0.020042095333337784 + 100.0 * 6.206286907196045
Epoch 2760, val loss: 1.886844277381897
Epoch 2770, training loss: 620.5100708007812 = 0.01980762742459774 + 100.0 * 6.204902648925781
Epoch 2770, val loss: 1.8894599676132202
Epoch 2780, training loss: 621.0349731445312 = 0.01958920620381832 + 100.0 * 6.210153579711914
Epoch 2780, val loss: 1.8927446603775024
Epoch 2790, training loss: 620.8511352539062 = 0.019356744363904 + 100.0 * 6.208317756652832
Epoch 2790, val loss: 1.8957290649414062
Epoch 2800, training loss: 620.7548217773438 = 0.01911160536110401 + 100.0 * 6.207356929779053
Epoch 2800, val loss: 1.8979315757751465
Epoch 2810, training loss: 620.5746459960938 = 0.018888935446739197 + 100.0 * 6.205557823181152
Epoch 2810, val loss: 1.9019737243652344
Epoch 2820, training loss: 620.5211181640625 = 0.01867310330271721 + 100.0 * 6.205024719238281
Epoch 2820, val loss: 1.9046716690063477
Epoch 2830, training loss: 620.8457641601562 = 0.01846969500184059 + 100.0 * 6.208272457122803
Epoch 2830, val loss: 1.9079619646072388
Epoch 2840, training loss: 620.4972534179688 = 0.018254822120070457 + 100.0 * 6.204790115356445
Epoch 2840, val loss: 1.9102448225021362
Epoch 2850, training loss: 620.6881713867188 = 0.01805127039551735 + 100.0 * 6.206700801849365
Epoch 2850, val loss: 1.9127883911132812
Epoch 2860, training loss: 620.5684814453125 = 0.017848001793026924 + 100.0 * 6.205506324768066
Epoch 2860, val loss: 1.9158849716186523
Epoch 2870, training loss: 620.5831298828125 = 0.017652511596679688 + 100.0 * 6.205655097961426
Epoch 2870, val loss: 1.919332504272461
Epoch 2880, training loss: 620.907470703125 = 0.017462875694036484 + 100.0 * 6.208900451660156
Epoch 2880, val loss: 1.9223277568817139
Epoch 2890, training loss: 620.7019653320312 = 0.01725231297314167 + 100.0 * 6.206846714019775
Epoch 2890, val loss: 1.9240496158599854
Epoch 2900, training loss: 621.2371826171875 = 0.01706814020872116 + 100.0 * 6.212201118469238
Epoch 2900, val loss: 1.92690110206604
Epoch 2910, training loss: 620.5533447265625 = 0.01686609350144863 + 100.0 * 6.205365180969238
Epoch 2910, val loss: 1.9292486906051636
Epoch 2920, training loss: 620.37890625 = 0.016683347523212433 + 100.0 * 6.203622341156006
Epoch 2920, val loss: 1.9319665431976318
Epoch 2930, training loss: 620.3405151367188 = 0.016507891938090324 + 100.0 * 6.203240394592285
Epoch 2930, val loss: 1.9344850778579712
Epoch 2940, training loss: 620.4617309570312 = 0.016337445005774498 + 100.0 * 6.204453945159912
Epoch 2940, val loss: 1.9368690252304077
Epoch 2950, training loss: 620.8695068359375 = 0.01616082899272442 + 100.0 * 6.20853328704834
Epoch 2950, val loss: 1.9389655590057373
Epoch 2960, training loss: 620.578857421875 = 0.015985891222953796 + 100.0 * 6.205628395080566
Epoch 2960, val loss: 1.9431487321853638
Epoch 2970, training loss: 620.3582153320312 = 0.015810955315828323 + 100.0 * 6.203423976898193
Epoch 2970, val loss: 1.945024013519287
Epoch 2980, training loss: 620.5938110351562 = 0.015652356669306755 + 100.0 * 6.205781936645508
Epoch 2980, val loss: 1.9482136964797974
Epoch 2990, training loss: 620.7099609375 = 0.015484890900552273 + 100.0 * 6.206944465637207
Epoch 2990, val loss: 1.9504636526107788
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 861.6292114257812 = 1.9488494396209717 + 100.0 * 8.596803665161133
Epoch 0, val loss: 1.956404209136963
Epoch 10, training loss: 861.52734375 = 1.9404418468475342 + 100.0 * 8.595869064331055
Epoch 10, val loss: 1.9476653337478638
Epoch 20, training loss: 860.9356079101562 = 1.929524540901184 + 100.0 * 8.59006118774414
Epoch 20, val loss: 1.936140537261963
Epoch 30, training loss: 857.16064453125 = 1.9149975776672363 + 100.0 * 8.552456855773926
Epoch 30, val loss: 1.9206156730651855
Epoch 40, training loss: 832.960693359375 = 1.897361159324646 + 100.0 * 8.310633659362793
Epoch 40, val loss: 1.9017200469970703
Epoch 50, training loss: 750.4439086914062 = 1.8768763542175293 + 100.0 * 7.48567008972168
Epoch 50, val loss: 1.8801707029342651
Epoch 60, training loss: 729.5901489257812 = 1.862398386001587 + 100.0 * 7.27727746963501
Epoch 60, val loss: 1.8664230108261108
Epoch 70, training loss: 715.490478515625 = 1.852027416229248 + 100.0 * 7.136384010314941
Epoch 70, val loss: 1.8562395572662354
Epoch 80, training loss: 701.96875 = 1.8405169248580933 + 100.0 * 7.001282215118408
Epoch 80, val loss: 1.8446996212005615
Epoch 90, training loss: 688.7338256835938 = 1.8309667110443115 + 100.0 * 6.869028568267822
Epoch 90, val loss: 1.8353862762451172
Epoch 100, training loss: 679.8297119140625 = 1.8237252235412598 + 100.0 * 6.780059814453125
Epoch 100, val loss: 1.8278064727783203
Epoch 110, training loss: 673.8604736328125 = 1.8173190355300903 + 100.0 * 6.720431804656982
Epoch 110, val loss: 1.8208155632019043
Epoch 120, training loss: 668.7388916015625 = 1.809890627861023 + 100.0 * 6.669290065765381
Epoch 120, val loss: 1.8133076429367065
Epoch 130, training loss: 664.8429565429688 = 1.8027242422103882 + 100.0 * 6.630402088165283
Epoch 130, val loss: 1.8060975074768066
Epoch 140, training loss: 661.8724975585938 = 1.7961549758911133 + 100.0 * 6.60076379776001
Epoch 140, val loss: 1.799557089805603
Epoch 150, training loss: 659.6189575195312 = 1.7895269393920898 + 100.0 * 6.578293800354004
Epoch 150, val loss: 1.7930481433868408
Epoch 160, training loss: 657.5897827148438 = 1.7823536396026611 + 100.0 * 6.558074474334717
Epoch 160, val loss: 1.7863755226135254
Epoch 170, training loss: 655.6560668945312 = 1.7748616933822632 + 100.0 * 6.538812160491943
Epoch 170, val loss: 1.7797002792358398
Epoch 180, training loss: 654.020751953125 = 1.7670361995697021 + 100.0 * 6.5225372314453125
Epoch 180, val loss: 1.7729506492614746
Epoch 190, training loss: 652.5623168945312 = 1.7587459087371826 + 100.0 * 6.508036136627197
Epoch 190, val loss: 1.7658826112747192
Epoch 200, training loss: 650.8826293945312 = 1.749987244606018 + 100.0 * 6.491326332092285
Epoch 200, val loss: 1.75848388671875
Epoch 210, training loss: 649.481201171875 = 1.7405723333358765 + 100.0 * 6.477406024932861
Epoch 210, val loss: 1.7506752014160156
Epoch 220, training loss: 648.60693359375 = 1.730440378189087 + 100.0 * 6.468764781951904
Epoch 220, val loss: 1.742384672164917
Epoch 230, training loss: 647.2875366210938 = 1.7193697690963745 + 100.0 * 6.455681800842285
Epoch 230, val loss: 1.7333077192306519
Epoch 240, training loss: 646.0028076171875 = 1.707518458366394 + 100.0 * 6.442952632904053
Epoch 240, val loss: 1.7236757278442383
Epoch 250, training loss: 644.9187622070312 = 1.6947003602981567 + 100.0 * 6.4322404861450195
Epoch 250, val loss: 1.7133370637893677
Epoch 260, training loss: 644.0699462890625 = 1.6809349060058594 + 100.0 * 6.423890113830566
Epoch 260, val loss: 1.7023695707321167
Epoch 270, training loss: 643.47900390625 = 1.6659584045410156 + 100.0 * 6.418130397796631
Epoch 270, val loss: 1.6904877424240112
Epoch 280, training loss: 642.5523681640625 = 1.6498644351959229 + 100.0 * 6.409025192260742
Epoch 280, val loss: 1.677857518196106
Epoch 290, training loss: 641.7833251953125 = 1.6326687335968018 + 100.0 * 6.401506423950195
Epoch 290, val loss: 1.6644424200057983
Epoch 300, training loss: 641.2103881835938 = 1.6143754720687866 + 100.0 * 6.395959854125977
Epoch 300, val loss: 1.6502782106399536
Epoch 310, training loss: 640.6021118164062 = 1.5950995683670044 + 100.0 * 6.39007043838501
Epoch 310, val loss: 1.6355352401733398
Epoch 320, training loss: 640.3618774414062 = 1.5747394561767578 + 100.0 * 6.387871265411377
Epoch 320, val loss: 1.6201694011688232
Epoch 330, training loss: 639.5039672851562 = 1.5535005331039429 + 100.0 * 6.379504680633545
Epoch 330, val loss: 1.6041972637176514
Epoch 340, training loss: 638.9572143554688 = 1.5314518213272095 + 100.0 * 6.374257564544678
Epoch 340, val loss: 1.5877779722213745
Epoch 350, training loss: 639.0410766601562 = 1.508772850036621 + 100.0 * 6.3753228187561035
Epoch 350, val loss: 1.5710264444351196
Epoch 360, training loss: 638.0950927734375 = 1.4850291013717651 + 100.0 * 6.366100311279297
Epoch 360, val loss: 1.5538798570632935
Epoch 370, training loss: 637.6149291992188 = 1.4610768556594849 + 100.0 * 6.361538410186768
Epoch 370, val loss: 1.5367766618728638
Epoch 380, training loss: 637.16357421875 = 1.4368411302566528 + 100.0 * 6.357267379760742
Epoch 380, val loss: 1.5196913480758667
Epoch 390, training loss: 637.338623046875 = 1.4125380516052246 + 100.0 * 6.3592610359191895
Epoch 390, val loss: 1.502817153930664
Epoch 400, training loss: 636.4821166992188 = 1.387677550315857 + 100.0 * 6.3509440422058105
Epoch 400, val loss: 1.4857537746429443
Epoch 410, training loss: 636.0214233398438 = 1.3631412982940674 + 100.0 * 6.346582412719727
Epoch 410, val loss: 1.469274878501892
Epoch 420, training loss: 635.6184692382812 = 1.338818907737732 + 100.0 * 6.342796325683594
Epoch 420, val loss: 1.4532999992370605
Epoch 430, training loss: 635.9303588867188 = 1.314736247062683 + 100.0 * 6.346156120300293
Epoch 430, val loss: 1.4377442598342896
Epoch 440, training loss: 635.0609130859375 = 1.290891170501709 + 100.0 * 6.337699890136719
Epoch 440, val loss: 1.422487735748291
Epoch 450, training loss: 634.6204223632812 = 1.2675310373306274 + 100.0 * 6.333528995513916
Epoch 450, val loss: 1.4079865217208862
Epoch 460, training loss: 634.6058959960938 = 1.2446528673171997 + 100.0 * 6.33361291885376
Epoch 460, val loss: 1.3941030502319336
Epoch 470, training loss: 634.4188842773438 = 1.2224276065826416 + 100.0 * 6.33196496963501
Epoch 470, val loss: 1.380928874015808
Epoch 480, training loss: 633.76416015625 = 1.2004212141036987 + 100.0 * 6.325637340545654
Epoch 480, val loss: 1.3682289123535156
Epoch 490, training loss: 633.5067749023438 = 1.179166555404663 + 100.0 * 6.323276519775391
Epoch 490, val loss: 1.3562147617340088
Epoch 500, training loss: 633.349365234375 = 1.158558964729309 + 100.0 * 6.321907997131348
Epoch 500, val loss: 1.344931721687317
Epoch 510, training loss: 633.5388793945312 = 1.1382616758346558 + 100.0 * 6.3240065574646
Epoch 510, val loss: 1.3339941501617432
Epoch 520, training loss: 632.8780517578125 = 1.118475317955017 + 100.0 * 6.317595958709717
Epoch 520, val loss: 1.3236210346221924
Epoch 530, training loss: 632.53466796875 = 1.0993034839630127 + 100.0 * 6.3143534660339355
Epoch 530, val loss: 1.3139903545379639
Epoch 540, training loss: 632.33544921875 = 1.080710768699646 + 100.0 * 6.31254768371582
Epoch 540, val loss: 1.3048475980758667
Epoch 550, training loss: 632.220458984375 = 1.062501072883606 + 100.0 * 6.311579704284668
Epoch 550, val loss: 1.2961690425872803
Epoch 560, training loss: 632.0191040039062 = 1.0446432828903198 + 100.0 * 6.309744834899902
Epoch 560, val loss: 1.2876434326171875
Epoch 570, training loss: 631.6498413085938 = 1.0272544622421265 + 100.0 * 6.306225299835205
Epoch 570, val loss: 1.2798335552215576
Epoch 580, training loss: 631.4894409179688 = 1.0103899240493774 + 100.0 * 6.304790496826172
Epoch 580, val loss: 1.2725032567977905
Epoch 590, training loss: 631.8153076171875 = 0.993869960308075 + 100.0 * 6.30821418762207
Epoch 590, val loss: 1.2651373147964478
Epoch 600, training loss: 631.31982421875 = 0.9774309992790222 + 100.0 * 6.303423881530762
Epoch 600, val loss: 1.2582629919052124
Epoch 610, training loss: 630.9694213867188 = 0.9615753889083862 + 100.0 * 6.300078868865967
Epoch 610, val loss: 1.2518552541732788
Epoch 620, training loss: 630.7296142578125 = 0.9462496638298035 + 100.0 * 6.2978339195251465
Epoch 620, val loss: 1.2460260391235352
Epoch 630, training loss: 630.5475463867188 = 0.9312430024147034 + 100.0 * 6.296163558959961
Epoch 630, val loss: 1.24055016040802
Epoch 640, training loss: 630.5083618164062 = 0.9165180921554565 + 100.0 * 6.2959184646606445
Epoch 640, val loss: 1.235361099243164
Epoch 650, training loss: 630.7047119140625 = 0.9019395709037781 + 100.0 * 6.298027992248535
Epoch 650, val loss: 1.2299083471298218
Epoch 660, training loss: 630.2349243164062 = 0.8874858021736145 + 100.0 * 6.293474197387695
Epoch 660, val loss: 1.224832534790039
Epoch 670, training loss: 629.9481201171875 = 0.8735575079917908 + 100.0 * 6.290745735168457
Epoch 670, val loss: 1.220554232597351
Epoch 680, training loss: 629.8869018554688 = 0.8600369095802307 + 100.0 * 6.290268421173096
Epoch 680, val loss: 1.2167165279388428
Epoch 690, training loss: 629.681884765625 = 0.8466200232505798 + 100.0 * 6.2883524894714355
Epoch 690, val loss: 1.2128430604934692
Epoch 700, training loss: 629.49951171875 = 0.8333519101142883 + 100.0 * 6.286661148071289
Epoch 700, val loss: 1.209202527999878
Epoch 710, training loss: 629.3802490234375 = 0.8205119371414185 + 100.0 * 6.285597324371338
Epoch 710, val loss: 1.2060270309448242
Epoch 720, training loss: 629.7152099609375 = 0.8077807426452637 + 100.0 * 6.289073944091797
Epoch 720, val loss: 1.2027603387832642
Epoch 730, training loss: 629.4666748046875 = 0.7951189875602722 + 100.0 * 6.286715507507324
Epoch 730, val loss: 1.1996099948883057
Epoch 740, training loss: 629.197998046875 = 0.7826796174049377 + 100.0 * 6.284153461456299
Epoch 740, val loss: 1.1968954801559448
Epoch 750, training loss: 628.8377685546875 = 0.7705745697021484 + 100.0 * 6.280672073364258
Epoch 750, val loss: 1.1947789192199707
Epoch 760, training loss: 628.666259765625 = 0.7586812973022461 + 100.0 * 6.279076099395752
Epoch 760, val loss: 1.1926534175872803
Epoch 770, training loss: 628.7855834960938 = 0.7470348477363586 + 100.0 * 6.280385971069336
Epoch 770, val loss: 1.1907026767730713
Epoch 780, training loss: 628.4675903320312 = 0.7354404926300049 + 100.0 * 6.277321815490723
Epoch 780, val loss: 1.1888707876205444
Epoch 790, training loss: 628.5975341796875 = 0.724000871181488 + 100.0 * 6.278735637664795
Epoch 790, val loss: 1.1871249675750732
Epoch 800, training loss: 628.294921875 = 0.7127606868743896 + 100.0 * 6.275821685791016
Epoch 800, val loss: 1.1861461400985718
Epoch 810, training loss: 628.1602783203125 = 0.7018090486526489 + 100.0 * 6.274584770202637
Epoch 810, val loss: 1.185225248336792
Epoch 820, training loss: 628.1704711914062 = 0.6909112930297852 + 100.0 * 6.2747955322265625
Epoch 820, val loss: 1.1841312646865845
Epoch 830, training loss: 628.119384765625 = 0.6800594925880432 + 100.0 * 6.274393558502197
Epoch 830, val loss: 1.1831169128417969
Epoch 840, training loss: 627.8977661132812 = 0.6694415211677551 + 100.0 * 6.272283554077148
Epoch 840, val loss: 1.182978630065918
Epoch 850, training loss: 627.6636352539062 = 0.6590087413787842 + 100.0 * 6.270046234130859
Epoch 850, val loss: 1.1826164722442627
Epoch 860, training loss: 627.645263671875 = 0.6487568616867065 + 100.0 * 6.269965171813965
Epoch 860, val loss: 1.1825604438781738
Epoch 870, training loss: 627.6524047851562 = 0.6385419368743896 + 100.0 * 6.270138263702393
Epoch 870, val loss: 1.1826725006103516
Epoch 880, training loss: 627.4871826171875 = 0.6282929182052612 + 100.0 * 6.268589019775391
Epoch 880, val loss: 1.1824206113815308
Epoch 890, training loss: 627.2745971679688 = 0.6182334423065186 + 100.0 * 6.266563415527344
Epoch 890, val loss: 1.1825112104415894
Epoch 900, training loss: 627.1669311523438 = 0.6084436178207397 + 100.0 * 6.265584468841553
Epoch 900, val loss: 1.1830580234527588
Epoch 910, training loss: 627.829345703125 = 0.5986943244934082 + 100.0 * 6.272306442260742
Epoch 910, val loss: 1.1833335161209106
Epoch 920, training loss: 627.1399536132812 = 0.5890166163444519 + 100.0 * 6.265509605407715
Epoch 920, val loss: 1.1842012405395508
Epoch 930, training loss: 626.9515991210938 = 0.5794409513473511 + 100.0 * 6.263721466064453
Epoch 930, val loss: 1.1852092742919922
Epoch 940, training loss: 626.9508056640625 = 0.5700387954711914 + 100.0 * 6.26380729675293
Epoch 940, val loss: 1.1861451864242554
Epoch 950, training loss: 626.7088012695312 = 0.5606979727745056 + 100.0 * 6.261481285095215
Epoch 950, val loss: 1.1870940923690796
Epoch 960, training loss: 626.7505493164062 = 0.5514329671859741 + 100.0 * 6.261991024017334
Epoch 960, val loss: 1.1880220174789429
Epoch 970, training loss: 626.9136352539062 = 0.5422776341438293 + 100.0 * 6.263713359832764
Epoch 970, val loss: 1.189308524131775
Epoch 980, training loss: 626.7792358398438 = 0.5330689549446106 + 100.0 * 6.2624616622924805
Epoch 980, val loss: 1.1898645162582397
Epoch 990, training loss: 626.3704223632812 = 0.5240662693977356 + 100.0 * 6.2584638595581055
Epoch 990, val loss: 1.191629409790039
Epoch 1000, training loss: 626.3098754882812 = 0.5152319669723511 + 100.0 * 6.257946014404297
Epoch 1000, val loss: 1.1932063102722168
Epoch 1010, training loss: 626.428466796875 = 0.506452739238739 + 100.0 * 6.259220123291016
Epoch 1010, val loss: 1.1947252750396729
Epoch 1020, training loss: 626.0994873046875 = 0.49762898683547974 + 100.0 * 6.25601863861084
Epoch 1020, val loss: 1.1960070133209229
Epoch 1030, training loss: 626.248779296875 = 0.4889126420021057 + 100.0 * 6.257598876953125
Epoch 1030, val loss: 1.1971604824066162
Epoch 1040, training loss: 626.1317749023438 = 0.48034870624542236 + 100.0 * 6.256514072418213
Epoch 1040, val loss: 1.1992437839508057
Epoch 1050, training loss: 626.0191650390625 = 0.47178885340690613 + 100.0 * 6.255473613739014
Epoch 1050, val loss: 1.2005362510681152
Epoch 1060, training loss: 626.56005859375 = 0.46322834491729736 + 100.0 * 6.26096773147583
Epoch 1060, val loss: 1.2020835876464844
Epoch 1070, training loss: 625.7611083984375 = 0.45473426580429077 + 100.0 * 6.253063678741455
Epoch 1070, val loss: 1.203534483909607
Epoch 1080, training loss: 625.6881103515625 = 0.44647952914237976 + 100.0 * 6.252416133880615
Epoch 1080, val loss: 1.2052479982376099
Epoch 1090, training loss: 625.5614624023438 = 0.43839195370674133 + 100.0 * 6.251230239868164
Epoch 1090, val loss: 1.2074180841445923
Epoch 1100, training loss: 625.928466796875 = 0.4304080605506897 + 100.0 * 6.254980564117432
Epoch 1100, val loss: 1.209696888923645
Epoch 1110, training loss: 625.8707885742188 = 0.4221351444721222 + 100.0 * 6.254486560821533
Epoch 1110, val loss: 1.2099483013153076
Epoch 1120, training loss: 625.383056640625 = 0.41413572430610657 + 100.0 * 6.24968957901001
Epoch 1120, val loss: 1.2123116254806519
Epoch 1130, training loss: 625.3399047851562 = 0.40631917119026184 + 100.0 * 6.249336242675781
Epoch 1130, val loss: 1.214502215385437
Epoch 1140, training loss: 625.4898071289062 = 0.3985808193683624 + 100.0 * 6.250912189483643
Epoch 1140, val loss: 1.2162327766418457
Epoch 1150, training loss: 625.2857055664062 = 0.3908556401729584 + 100.0 * 6.248948097229004
Epoch 1150, val loss: 1.2178999185562134
Epoch 1160, training loss: 625.2125854492188 = 0.3832225501537323 + 100.0 * 6.248293399810791
Epoch 1160, val loss: 1.219840407371521
Epoch 1170, training loss: 625.2560424804688 = 0.37576642632484436 + 100.0 * 6.248802661895752
Epoch 1170, val loss: 1.2219042778015137
Epoch 1180, training loss: 625.002685546875 = 0.36842745542526245 + 100.0 * 6.246342658996582
Epoch 1180, val loss: 1.224213719367981
Epoch 1190, training loss: 624.95654296875 = 0.361228883266449 + 100.0 * 6.24595308303833
Epoch 1190, val loss: 1.2262862920761108
Epoch 1200, training loss: 625.1499633789062 = 0.3540990352630615 + 100.0 * 6.247958660125732
Epoch 1200, val loss: 1.2281854152679443
Epoch 1210, training loss: 625.3193969726562 = 0.3469884693622589 + 100.0 * 6.249724388122559
Epoch 1210, val loss: 1.2303576469421387
Epoch 1220, training loss: 625.0267944335938 = 0.3399520516395569 + 100.0 * 6.246868133544922
Epoch 1220, val loss: 1.2324559688568115
Epoch 1230, training loss: 624.6474609375 = 0.3331187665462494 + 100.0 * 6.243143081665039
Epoch 1230, val loss: 1.234791874885559
Epoch 1240, training loss: 624.626953125 = 0.3265179991722107 + 100.0 * 6.243004322052002
Epoch 1240, val loss: 1.237318754196167
Epoch 1250, training loss: 625.0176391601562 = 0.32001981139183044 + 100.0 * 6.246976375579834
Epoch 1250, val loss: 1.2397207021713257
Epoch 1260, training loss: 624.620849609375 = 0.31349456310272217 + 100.0 * 6.2430739402771
Epoch 1260, val loss: 1.2419425249099731
Epoch 1270, training loss: 624.62841796875 = 0.3071844279766083 + 100.0 * 6.2432122230529785
Epoch 1270, val loss: 1.24468994140625
Epoch 1280, training loss: 624.4855346679688 = 0.3009452819824219 + 100.0 * 6.241845607757568
Epoch 1280, val loss: 1.2470890283584595
Epoch 1290, training loss: 624.3959350585938 = 0.2948921322822571 + 100.0 * 6.2410101890563965
Epoch 1290, val loss: 1.2498928308486938
Epoch 1300, training loss: 624.6555786132812 = 0.28900107741355896 + 100.0 * 6.24366569519043
Epoch 1300, val loss: 1.2527520656585693
Epoch 1310, training loss: 624.2888793945312 = 0.2830224335193634 + 100.0 * 6.240058422088623
Epoch 1310, val loss: 1.2546862363815308
Epoch 1320, training loss: 624.3114013671875 = 0.27731022238731384 + 100.0 * 6.2403411865234375
Epoch 1320, val loss: 1.2577120065689087
Epoch 1330, training loss: 624.724853515625 = 0.27166199684143066 + 100.0 * 6.244531631469727
Epoch 1330, val loss: 1.2603896856307983
Epoch 1340, training loss: 624.2273559570312 = 0.2660299241542816 + 100.0 * 6.239613056182861
Epoch 1340, val loss: 1.2630800008773804
Epoch 1350, training loss: 624.0760498046875 = 0.2606435716152191 + 100.0 * 6.238154411315918
Epoch 1350, val loss: 1.266059398651123
Epoch 1360, training loss: 624.0921630859375 = 0.2554439604282379 + 100.0 * 6.238367557525635
Epoch 1360, val loss: 1.2694778442382812
Epoch 1370, training loss: 624.54248046875 = 0.2503121793270111 + 100.0 * 6.242921829223633
Epoch 1370, val loss: 1.2728816270828247
Epoch 1380, training loss: 624.1283569335938 = 0.24519939720630646 + 100.0 * 6.238831996917725
Epoch 1380, val loss: 1.275399088859558
Epoch 1390, training loss: 624.130859375 = 0.2402982860803604 + 100.0 * 6.238905906677246
Epoch 1390, val loss: 1.279086947441101
Epoch 1400, training loss: 623.9383544921875 = 0.23546665906906128 + 100.0 * 6.237029075622559
Epoch 1400, val loss: 1.282410740852356
Epoch 1410, training loss: 623.8801879882812 = 0.23075111210346222 + 100.0 * 6.236494064331055
Epoch 1410, val loss: 1.285660982131958
Epoch 1420, training loss: 623.9171142578125 = 0.22617046535015106 + 100.0 * 6.23690938949585
Epoch 1420, val loss: 1.2892142534255981
Epoch 1430, training loss: 624.00537109375 = 0.22169223427772522 + 100.0 * 6.237836837768555
Epoch 1430, val loss: 1.2929519414901733
Epoch 1440, training loss: 623.8065185546875 = 0.21725574135780334 + 100.0 * 6.235892295837402
Epoch 1440, val loss: 1.2964836359024048
Epoch 1450, training loss: 623.716552734375 = 0.21291586756706238 + 100.0 * 6.235036849975586
Epoch 1450, val loss: 1.300107479095459
Epoch 1460, training loss: 623.6092529296875 = 0.20870637893676758 + 100.0 * 6.234005451202393
Epoch 1460, val loss: 1.303989291191101
Epoch 1470, training loss: 623.794189453125 = 0.20465277135372162 + 100.0 * 6.235895156860352
Epoch 1470, val loss: 1.308228611946106
Epoch 1480, training loss: 623.5577392578125 = 0.20054811239242554 + 100.0 * 6.233571529388428
Epoch 1480, val loss: 1.3112339973449707
Epoch 1490, training loss: 623.849609375 = 0.1965627670288086 + 100.0 * 6.236530780792236
Epoch 1490, val loss: 1.3147917985916138
Epoch 1500, training loss: 623.6241455078125 = 0.19267502427101135 + 100.0 * 6.234314441680908
Epoch 1500, val loss: 1.3190929889678955
Epoch 1510, training loss: 623.4712524414062 = 0.18882392346858978 + 100.0 * 6.232824325561523
Epoch 1510, val loss: 1.322877287864685
Epoch 1520, training loss: 623.312744140625 = 0.18513916432857513 + 100.0 * 6.231276512145996
Epoch 1520, val loss: 1.327260971069336
Epoch 1530, training loss: 623.3602294921875 = 0.1815640926361084 + 100.0 * 6.231786251068115
Epoch 1530, val loss: 1.3316699266433716
Epoch 1540, training loss: 623.83154296875 = 0.1780533492565155 + 100.0 * 6.23653507232666
Epoch 1540, val loss: 1.335923433303833
Epoch 1550, training loss: 623.4934692382812 = 0.17445576190948486 + 100.0 * 6.233190536499023
Epoch 1550, val loss: 1.3393118381500244
Epoch 1560, training loss: 623.2907104492188 = 0.17100349068641663 + 100.0 * 6.231196880340576
Epoch 1560, val loss: 1.3437734842300415
Epoch 1570, training loss: 623.191650390625 = 0.16766870021820068 + 100.0 * 6.2302398681640625
Epoch 1570, val loss: 1.3481765985488892
Epoch 1580, training loss: 623.2803955078125 = 0.16443969309329987 + 100.0 * 6.231159687042236
Epoch 1580, val loss: 1.3523107767105103
Epoch 1590, training loss: 623.3340454101562 = 0.16123054921627045 + 100.0 * 6.2317280769348145
Epoch 1590, val loss: 1.3567349910736084
Epoch 1600, training loss: 623.3744506835938 = 0.15809965133666992 + 100.0 * 6.232163429260254
Epoch 1600, val loss: 1.3614271879196167
Epoch 1610, training loss: 623.2763061523438 = 0.15497875213623047 + 100.0 * 6.231213092803955
Epoch 1610, val loss: 1.365400791168213
Epoch 1620, training loss: 622.9675903320312 = 0.15197747945785522 + 100.0 * 6.228156089782715
Epoch 1620, val loss: 1.3706778287887573
Epoch 1630, training loss: 623.0301513671875 = 0.14906319975852966 + 100.0 * 6.228811264038086
Epoch 1630, val loss: 1.3751860857009888
Epoch 1640, training loss: 623.2561645507812 = 0.1461784392595291 + 100.0 * 6.231099605560303
Epoch 1640, val loss: 1.3798582553863525
Epoch 1650, training loss: 622.9950561523438 = 0.14331531524658203 + 100.0 * 6.228517532348633
Epoch 1650, val loss: 1.3840872049331665
Epoch 1660, training loss: 622.9593505859375 = 0.14053809642791748 + 100.0 * 6.2281880378723145
Epoch 1660, val loss: 1.388455867767334
