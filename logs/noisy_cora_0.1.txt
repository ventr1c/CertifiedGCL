Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10548])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8024039268493652 = 1.9427192211151123 + 0.1 * 8.596846580505371
Epoch 0, val loss: 1.9435081481933594
Epoch 10, training loss: 2.7977583408355713 = 1.9380767345428467 + 0.1 * 8.59681510925293
Epoch 10, val loss: 1.9392248392105103
Epoch 20, training loss: 2.7927145957946777 = 1.9330418109893799 + 0.1 * 8.59672737121582
Epoch 20, val loss: 1.934478759765625
Epoch 30, training loss: 2.786731004714966 = 1.927083134651184 + 0.1 * 8.596478462219238
Epoch 30, val loss: 1.92875075340271
Epoch 40, training loss: 2.779202699661255 = 1.9196466207504272 + 0.1 * 8.595560073852539
Epoch 40, val loss: 1.9215110540390015
Epoch 50, training loss: 2.7692270278930664 = 1.9100549221038818 + 0.1 * 8.591720581054688
Epoch 50, val loss: 1.9120995998382568
Epoch 60, training loss: 2.755197048187256 = 1.8974988460540771 + 0.1 * 8.576980590820312
Epoch 60, val loss: 1.8997787237167358
Epoch 70, training loss: 2.734553337097168 = 1.8812859058380127 + 0.1 * 8.532674789428711
Epoch 70, val loss: 1.8840086460113525
Epoch 80, training loss: 2.702518939971924 = 1.8617764711380005 + 0.1 * 8.407423973083496
Epoch 80, val loss: 1.8654505014419556
Epoch 90, training loss: 2.6523234844207764 = 1.8400955200195312 + 0.1 * 8.122279167175293
Epoch 90, val loss: 1.8454018831253052
Epoch 100, training loss: 2.621950626373291 = 1.8179529905319214 + 0.1 * 8.0399751663208
Epoch 100, val loss: 1.8255646228790283
Epoch 110, training loss: 2.589677333831787 = 1.797536849975586 + 0.1 * 7.9214043617248535
Epoch 110, val loss: 1.8072073459625244
Epoch 120, training loss: 2.553016185760498 = 1.779126763343811 + 0.1 * 7.738895416259766
Epoch 120, val loss: 1.7901586294174194
Epoch 130, training loss: 2.5146193504333496 = 1.7615522146224976 + 0.1 * 7.530670166015625
Epoch 130, val loss: 1.7738397121429443
Epoch 140, training loss: 2.4765496253967285 = 1.742999792098999 + 0.1 * 7.33549690246582
Epoch 140, val loss: 1.7571152448654175
Epoch 150, training loss: 2.439115047454834 = 1.7217402458190918 + 0.1 * 7.173748016357422
Epoch 150, val loss: 1.7389204502105713
Epoch 160, training loss: 2.402860403060913 = 1.6961544752120972 + 0.1 * 7.067059516906738
Epoch 160, val loss: 1.7177919149398804
Epoch 170, training loss: 2.3664402961730957 = 1.6651111841201782 + 0.1 * 7.0132904052734375
Epoch 170, val loss: 1.6920721530914307
Epoch 180, training loss: 2.3270909786224365 = 1.6288293600082397 + 0.1 * 6.982616424560547
Epoch 180, val loss: 1.6618074178695679
Epoch 190, training loss: 2.283848285675049 = 1.5882892608642578 + 0.1 * 6.9555888175964355
Epoch 190, val loss: 1.6284528970718384
Epoch 200, training loss: 2.237257242202759 = 1.5446144342422485 + 0.1 * 6.926427364349365
Epoch 200, val loss: 1.5927047729492188
Epoch 210, training loss: 2.188854694366455 = 1.4991836547851562 + 0.1 * 6.8967108726501465
Epoch 210, val loss: 1.5555968284606934
Epoch 220, training loss: 2.1404759883880615 = 1.453458309173584 + 0.1 * 6.870176792144775
Epoch 220, val loss: 1.5187002420425415
Epoch 230, training loss: 2.0935146808624268 = 1.4086267948150635 + 0.1 * 6.848877906799316
Epoch 230, val loss: 1.4826780557632446
Epoch 240, training loss: 2.048800468444824 = 1.3656929731369019 + 0.1 * 6.831076145172119
Epoch 240, val loss: 1.4485582113265991
Epoch 250, training loss: 2.005596399307251 = 1.3242229223251343 + 0.1 * 6.813734531402588
Epoch 250, val loss: 1.4158495664596558
Epoch 260, training loss: 1.9632920026779175 = 1.283814549446106 + 0.1 * 6.794774532318115
Epoch 260, val loss: 1.3843519687652588
Epoch 270, training loss: 1.9216928482055664 = 1.2440764904022217 + 0.1 * 6.776163101196289
Epoch 270, val loss: 1.3536529541015625
Epoch 280, training loss: 1.8809120655059814 = 1.205149531364441 + 0.1 * 6.757626056671143
Epoch 280, val loss: 1.3240201473236084
Epoch 290, training loss: 1.8407766819000244 = 1.1665006875991821 + 0.1 * 6.7427592277526855
Epoch 290, val loss: 1.2948346138000488
Epoch 300, training loss: 1.8009026050567627 = 1.127795934677124 + 0.1 * 6.731067180633545
Epoch 300, val loss: 1.265740990638733
Epoch 310, training loss: 1.7607431411743164 = 1.0889739990234375 + 0.1 * 6.717690944671631
Epoch 310, val loss: 1.2366405725479126
Epoch 320, training loss: 1.7205777168273926 = 1.0497632026672363 + 0.1 * 6.708144664764404
Epoch 320, val loss: 1.2071503400802612
Epoch 330, training loss: 1.6804735660552979 = 1.0105639696121216 + 0.1 * 6.699096202850342
Epoch 330, val loss: 1.1776200532913208
Epoch 340, training loss: 1.6406183242797852 = 0.9715244770050049 + 0.1 * 6.690937519073486
Epoch 340, val loss: 1.148120403289795
Epoch 350, training loss: 1.6011173725128174 = 0.9328103065490723 + 0.1 * 6.683070182800293
Epoch 350, val loss: 1.11883544921875
Epoch 360, training loss: 1.5624308586120605 = 0.8947954773902893 + 0.1 * 6.676353931427002
Epoch 360, val loss: 1.0903066396713257
Epoch 370, training loss: 1.5247249603271484 = 0.8576580286026001 + 0.1 * 6.670669078826904
Epoch 370, val loss: 1.0623762607574463
Epoch 380, training loss: 1.4877246618270874 = 0.8216633200645447 + 0.1 * 6.660613536834717
Epoch 380, val loss: 1.0355027914047241
Epoch 390, training loss: 1.4528950452804565 = 0.7867301106452942 + 0.1 * 6.661649227142334
Epoch 390, val loss: 1.009685754776001
Epoch 400, training loss: 1.418045997619629 = 0.7533283233642578 + 0.1 * 6.647176742553711
Epoch 400, val loss: 0.9852046370506287
Epoch 410, training loss: 1.3852819204330444 = 0.7212344408035278 + 0.1 * 6.640474796295166
Epoch 410, val loss: 0.9620457291603088
Epoch 420, training loss: 1.3537898063659668 = 0.6904449462890625 + 0.1 * 6.633448600769043
Epoch 420, val loss: 0.9400769472122192
Epoch 430, training loss: 1.3237411975860596 = 0.6611089110374451 + 0.1 * 6.6263227462768555
Epoch 430, val loss: 0.9196736812591553
Epoch 440, training loss: 1.2950726747512817 = 0.6329823732376099 + 0.1 * 6.620903015136719
Epoch 440, val loss: 0.9003773331642151
Epoch 450, training loss: 1.2671473026275635 = 0.6061081886291504 + 0.1 * 6.610391139984131
Epoch 450, val loss: 0.8823625445365906
Epoch 460, training loss: 1.2412350177764893 = 0.5803993344306946 + 0.1 * 6.608356475830078
Epoch 460, val loss: 0.865607500076294
Epoch 470, training loss: 1.2157871723175049 = 0.5558211803436279 + 0.1 * 6.599660396575928
Epoch 470, val loss: 0.8499107360839844
Epoch 480, training loss: 1.1912798881530762 = 0.5321506857872009 + 0.1 * 6.591291904449463
Epoch 480, val loss: 0.8352253437042236
Epoch 490, training loss: 1.1693470478057861 = 0.509246289730072 + 0.1 * 6.60100793838501
Epoch 490, val loss: 0.8213974833488464
Epoch 500, training loss: 1.1459192037582397 = 0.487301766872406 + 0.1 * 6.586174488067627
Epoch 500, val loss: 0.8086268305778503
Epoch 510, training loss: 1.124302625656128 = 0.4661019444465637 + 0.1 * 6.582006454467773
Epoch 510, val loss: 0.7967050075531006
Epoch 520, training loss: 1.1025395393371582 = 0.4455883204936981 + 0.1 * 6.569512367248535
Epoch 520, val loss: 0.7855203747749329
Epoch 530, training loss: 1.0819618701934814 = 0.425647109746933 + 0.1 * 6.56314754486084
Epoch 530, val loss: 0.7751571536064148
Epoch 540, training loss: 1.0623881816864014 = 0.40627819299697876 + 0.1 * 6.561099052429199
Epoch 540, val loss: 0.7654438614845276
Epoch 550, training loss: 1.0426830053329468 = 0.3875049352645874 + 0.1 * 6.551780700683594
Epoch 550, val loss: 0.7565131187438965
Epoch 560, training loss: 1.024000883102417 = 0.3692774772644043 + 0.1 * 6.5472331047058105
Epoch 560, val loss: 0.7482736110687256
Epoch 570, training loss: 1.006097674369812 = 0.35162970423698425 + 0.1 * 6.544679641723633
Epoch 570, val loss: 0.7406933903694153
Epoch 580, training loss: 0.9885601997375488 = 0.3346715271472931 + 0.1 * 6.538886547088623
Epoch 580, val loss: 0.733899712562561
Epoch 590, training loss: 0.971428394317627 = 0.3183627426624298 + 0.1 * 6.530656337738037
Epoch 590, val loss: 0.7277766466140747
Epoch 600, training loss: 0.9555253982543945 = 0.3027072250843048 + 0.1 * 6.528181076049805
Epoch 600, val loss: 0.7223663330078125
Epoch 610, training loss: 0.9405088424682617 = 0.2876652479171753 + 0.1 * 6.528435707092285
Epoch 610, val loss: 0.7175804972648621
Epoch 620, training loss: 0.92545086145401 = 0.2732957601547241 + 0.1 * 6.52155065536499
Epoch 620, val loss: 0.7134212255477905
Epoch 630, training loss: 0.910901665687561 = 0.25958511233329773 + 0.1 * 6.5131659507751465
Epoch 630, val loss: 0.7099122405052185
Epoch 640, training loss: 0.8985381126403809 = 0.24650652706623077 + 0.1 * 6.520315647125244
Epoch 640, val loss: 0.7069652676582336
Epoch 650, training loss: 0.8849532604217529 = 0.2341039478778839 + 0.1 * 6.508493423461914
Epoch 650, val loss: 0.7046201825141907
Epoch 660, training loss: 0.8730856776237488 = 0.22234082221984863 + 0.1 * 6.507448673248291
Epoch 660, val loss: 0.7028177976608276
Epoch 670, training loss: 0.8611247539520264 = 0.21117767691612244 + 0.1 * 6.4994707107543945
Epoch 670, val loss: 0.7015103101730347
Epoch 680, training loss: 0.8498640060424805 = 0.200590580701828 + 0.1 * 6.492733955383301
Epoch 680, val loss: 0.7006808519363403
Epoch 690, training loss: 0.8415025472640991 = 0.1905495673418045 + 0.1 * 6.5095295906066895
Epoch 690, val loss: 0.700292706489563
Epoch 700, training loss: 0.8305107951164246 = 0.18108247220516205 + 0.1 * 6.494283199310303
Epoch 700, val loss: 0.7002626657485962
Epoch 710, training loss: 0.8203119039535522 = 0.17212550342082977 + 0.1 * 6.481863498687744
Epoch 710, val loss: 0.7007228136062622
Epoch 720, training loss: 0.8143919706344604 = 0.16364650428295135 + 0.1 * 6.5074543952941895
Epoch 720, val loss: 0.7014554142951965
Epoch 730, training loss: 0.8035430908203125 = 0.1556636542081833 + 0.1 * 6.478794097900391
Epoch 730, val loss: 0.7025715708732605
Epoch 740, training loss: 0.7956961393356323 = 0.1481131911277771 + 0.1 * 6.475829601287842
Epoch 740, val loss: 0.7040121555328369
Epoch 750, training loss: 0.7880814671516418 = 0.14096100628376007 + 0.1 * 6.47120475769043
Epoch 750, val loss: 0.7056824564933777
Epoch 760, training loss: 0.7842028737068176 = 0.13418947160243988 + 0.1 * 6.500133991241455
Epoch 760, val loss: 0.7076244950294495
Epoch 770, training loss: 0.7752165198326111 = 0.12782295048236847 + 0.1 * 6.473935604095459
Epoch 770, val loss: 0.7097929120063782
Epoch 780, training loss: 0.7681081295013428 = 0.12181336432695389 + 0.1 * 6.462947368621826
Epoch 780, val loss: 0.7122414112091064
Epoch 790, training loss: 0.7637706398963928 = 0.11613152176141739 + 0.1 * 6.476390838623047
Epoch 790, val loss: 0.7148066759109497
Epoch 800, training loss: 0.7567043304443359 = 0.11077215522527695 + 0.1 * 6.459321975708008
Epoch 800, val loss: 0.7175782918930054
Epoch 810, training loss: 0.7513360381126404 = 0.10570170730352402 + 0.1 * 6.456343173980713
Epoch 810, val loss: 0.7205247282981873
Epoch 820, training loss: 0.747189462184906 = 0.10091004520654678 + 0.1 * 6.462793827056885
Epoch 820, val loss: 0.7235721349716187
Epoch 830, training loss: 0.7415633201599121 = 0.0963888093829155 + 0.1 * 6.45174503326416
Epoch 830, val loss: 0.7268227338790894
Epoch 840, training loss: 0.7380809783935547 = 0.09211714565753937 + 0.1 * 6.459638595581055
Epoch 840, val loss: 0.7301442623138428
Epoch 850, training loss: 0.7332500219345093 = 0.08808935433626175 + 0.1 * 6.451606750488281
Epoch 850, val loss: 0.7335665822029114
Epoch 860, training loss: 0.7287874221801758 = 0.08427780866622925 + 0.1 * 6.445096015930176
Epoch 860, val loss: 0.7371225953102112
Epoch 870, training loss: 0.72523033618927 = 0.08066798746585846 + 0.1 * 6.445623397827148
Epoch 870, val loss: 0.7407320141792297
Epoch 880, training loss: 0.7219621539115906 = 0.07725387066602707 + 0.1 * 6.44708251953125
Epoch 880, val loss: 0.7443965673446655
Epoch 890, training loss: 0.7180995941162109 = 0.07402834296226501 + 0.1 * 6.4407124519348145
Epoch 890, val loss: 0.7481480240821838
Epoch 900, training loss: 0.7151695489883423 = 0.07097169756889343 + 0.1 * 6.441978931427002
Epoch 900, val loss: 0.7519623637199402
Epoch 910, training loss: 0.7120680212974548 = 0.0680769830942154 + 0.1 * 6.439910411834717
Epoch 910, val loss: 0.7557818293571472
Epoch 920, training loss: 0.7091790437698364 = 0.06533760577440262 + 0.1 * 6.438414573669434
Epoch 920, val loss: 0.7596021890640259
Epoch 930, training loss: 0.7058567404747009 = 0.06274701654911041 + 0.1 * 6.431097507476807
Epoch 930, val loss: 0.7635084390640259
Epoch 940, training loss: 0.7043756246566772 = 0.06028882414102554 + 0.1 * 6.4408674240112305
Epoch 940, val loss: 0.7674069404602051
Epoch 950, training loss: 0.7011111378669739 = 0.05796203762292862 + 0.1 * 6.431490421295166
Epoch 950, val loss: 0.7712979316711426
Epoch 960, training loss: 0.6986032724380493 = 0.055753424763679504 + 0.1 * 6.4284987449646
Epoch 960, val loss: 0.7752380967140198
Epoch 970, training loss: 0.6965000629425049 = 0.05365649610757828 + 0.1 * 6.428435325622559
Epoch 970, val loss: 0.7791591286659241
Epoch 980, training loss: 0.6941468715667725 = 0.05166590213775635 + 0.1 * 6.424809455871582
Epoch 980, val loss: 0.7830799221992493
Epoch 990, training loss: 0.6922747492790222 = 0.04977462813258171 + 0.1 * 6.4250006675720215
Epoch 990, val loss: 0.7869900465011597
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 2.7929282188415527 = 1.9332411289215088 + 0.1 * 8.596869468688965
Epoch 0, val loss: 1.9202847480773926
Epoch 10, training loss: 2.7883028984069824 = 1.928617238998413 + 0.1 * 8.596857070922852
Epoch 10, val loss: 1.9155268669128418
Epoch 20, training loss: 2.7832651138305664 = 1.9235835075378418 + 0.1 * 8.59681510925293
Epoch 20, val loss: 1.9103275537490845
Epoch 30, training loss: 2.777358055114746 = 1.9176859855651855 + 0.1 * 8.596720695495605
Epoch 30, val loss: 1.9042726755142212
Epoch 40, training loss: 2.7701313495635986 = 1.9104872941970825 + 0.1 * 8.596441268920898
Epoch 40, val loss: 1.8969109058380127
Epoch 50, training loss: 2.760988712310791 = 1.9014594554901123 + 0.1 * 8.595291137695312
Epoch 50, val loss: 1.8876956701278687
Epoch 60, training loss: 2.7490005493164062 = 1.8899896144866943 + 0.1 * 8.590108871459961
Epoch 60, val loss: 1.876099705696106
Epoch 70, training loss: 2.732494831085205 = 1.8754909038543701 + 0.1 * 8.570037841796875
Epoch 70, val loss: 1.8617275953292847
Epoch 80, training loss: 2.708845615386963 = 1.857921838760376 + 0.1 * 8.509237289428711
Epoch 80, val loss: 1.8448995351791382
Epoch 90, training loss: 2.670374631881714 = 1.8385896682739258 + 0.1 * 8.317850112915039
Epoch 90, val loss: 1.8274022340774536
Epoch 100, training loss: 2.6268484592437744 = 1.8186993598937988 + 0.1 * 8.081490516662598
Epoch 100, val loss: 1.8106939792633057
Epoch 110, training loss: 2.600125789642334 = 1.8003102540969849 + 0.1 * 7.998156547546387
Epoch 110, val loss: 1.7964085340499878
Epoch 120, training loss: 2.567025899887085 = 1.7832049131393433 + 0.1 * 7.838210582733154
Epoch 120, val loss: 1.7833797931671143
Epoch 130, training loss: 2.524502992630005 = 1.76603102684021 + 0.1 * 7.584719181060791
Epoch 130, val loss: 1.7696788311004639
Epoch 140, training loss: 2.4794421195983887 = 1.7475895881652832 + 0.1 * 7.31852388381958
Epoch 140, val loss: 1.7542712688446045
Epoch 150, training loss: 2.4398398399353027 = 1.7265363931655884 + 0.1 * 7.133033275604248
Epoch 150, val loss: 1.736332654953003
Epoch 160, training loss: 2.4060966968536377 = 1.7015427350997925 + 0.1 * 7.0455403327941895
Epoch 160, val loss: 1.7151660919189453
Epoch 170, training loss: 2.3718700408935547 = 1.6717182397842407 + 0.1 * 7.001518726348877
Epoch 170, val loss: 1.6902246475219727
Epoch 180, training loss: 2.3335249423980713 = 1.6367541551589966 + 0.1 * 6.967708587646484
Epoch 180, val loss: 1.6610749959945679
Epoch 190, training loss: 2.2904064655303955 = 1.5966700315475464 + 0.1 * 6.937364101409912
Epoch 190, val loss: 1.6275503635406494
Epoch 200, training loss: 2.243075370788574 = 1.552235722541809 + 0.1 * 6.908397674560547
Epoch 200, val loss: 1.590883493423462
Epoch 210, training loss: 2.1930904388427734 = 1.5049196481704712 + 0.1 * 6.881707191467285
Epoch 210, val loss: 1.5529557466506958
Epoch 220, training loss: 2.1418826580047607 = 1.4559578895568848 + 0.1 * 6.859247207641602
Epoch 220, val loss: 1.5142985582351685
Epoch 230, training loss: 2.0900144577026367 = 1.4059219360351562 + 0.1 * 6.840926170349121
Epoch 230, val loss: 1.4749865531921387
Epoch 240, training loss: 2.0377984046936035 = 1.3550912141799927 + 0.1 * 6.827073097229004
Epoch 240, val loss: 1.4355263710021973
Epoch 250, training loss: 1.985938310623169 = 1.304228663444519 + 0.1 * 6.817095756530762
Epoch 250, val loss: 1.396682858467102
Epoch 260, training loss: 1.9340739250183105 = 1.2532153129577637 + 0.1 * 6.8085856437683105
Epoch 260, val loss: 1.3577436208724976
Epoch 270, training loss: 1.8824691772460938 = 1.2023634910583496 + 0.1 * 6.801055908203125
Epoch 270, val loss: 1.3191032409667969
Epoch 280, training loss: 1.831859827041626 = 1.1523702144622803 + 0.1 * 6.794896602630615
Epoch 280, val loss: 1.2815171480178833
Epoch 290, training loss: 1.783014178276062 = 1.1041390895843506 + 0.1 * 6.788750648498535
Epoch 290, val loss: 1.2456707954406738
Epoch 300, training loss: 1.7359979152679443 = 1.057724118232727 + 0.1 * 6.7827372550964355
Epoch 300, val loss: 1.211482048034668
Epoch 310, training loss: 1.6910574436187744 = 1.0134069919586182 + 0.1 * 6.776503562927246
Epoch 310, val loss: 1.1791458129882812
Epoch 320, training loss: 1.6484155654907227 = 0.9711812734603882 + 0.1 * 6.772342681884766
Epoch 320, val loss: 1.1486196517944336
Epoch 330, training loss: 1.6075044870376587 = 0.9310455322265625 + 0.1 * 6.764589309692383
Epoch 330, val loss: 1.1198118925094604
Epoch 340, training loss: 1.5687992572784424 = 0.8926693797111511 + 0.1 * 6.761298656463623
Epoch 340, val loss: 1.0921968221664429
Epoch 350, training loss: 1.5313085317611694 = 0.8561154007911682 + 0.1 * 6.751931190490723
Epoch 350, val loss: 1.0660160779953003
Epoch 360, training loss: 1.4958596229553223 = 0.8212517499923706 + 0.1 * 6.7460784912109375
Epoch 360, val loss: 1.0410982370376587
Epoch 370, training loss: 1.4614553451538086 = 0.7878283262252808 + 0.1 * 6.736269474029541
Epoch 370, val loss: 1.017449975013733
Epoch 380, training loss: 1.4289145469665527 = 0.7555915713310242 + 0.1 * 6.733230113983154
Epoch 380, val loss: 0.9948664903640747
Epoch 390, training loss: 1.3969414234161377 = 0.724824845790863 + 0.1 * 6.721166133880615
Epoch 390, val loss: 0.9736021757125854
Epoch 400, training loss: 1.3666449785232544 = 0.6952036023139954 + 0.1 * 6.714413642883301
Epoch 400, val loss: 0.9536595344543457
Epoch 410, training loss: 1.3374354839324951 = 0.6665822863578796 + 0.1 * 6.708531379699707
Epoch 410, val loss: 0.9346837997436523
Epoch 420, training loss: 1.309453010559082 = 0.6390640735626221 + 0.1 * 6.703888893127441
Epoch 420, val loss: 0.9170745611190796
Epoch 430, training loss: 1.281943917274475 = 0.6123918890953064 + 0.1 * 6.695519924163818
Epoch 430, val loss: 0.9005147218704224
Epoch 440, training loss: 1.2566807270050049 = 0.586480438709259 + 0.1 * 6.70200252532959
Epoch 440, val loss: 0.8850042223930359
Epoch 450, training loss: 1.2302252054214478 = 0.5616766810417175 + 0.1 * 6.685484886169434
Epoch 450, val loss: 0.8706812858581543
Epoch 460, training loss: 1.2057605981826782 = 0.5377454161643982 + 0.1 * 6.68015193939209
Epoch 460, val loss: 0.8576197028160095
Epoch 470, training loss: 1.1819730997085571 = 0.5145135521888733 + 0.1 * 6.674595355987549
Epoch 470, val loss: 0.8455304503440857
Epoch 480, training loss: 1.1609039306640625 = 0.49196356534957886 + 0.1 * 6.689403057098389
Epoch 480, val loss: 0.8344205021858215
Epoch 490, training loss: 1.1379106044769287 = 0.4704103171825409 + 0.1 * 6.6750030517578125
Epoch 490, val loss: 0.8243992328643799
Epoch 500, training loss: 1.1157206296920776 = 0.449631005525589 + 0.1 * 6.660896301269531
Epoch 500, val loss: 0.8153865337371826
Epoch 510, training loss: 1.0951564311981201 = 0.4294704496860504 + 0.1 * 6.656859874725342
Epoch 510, val loss: 0.8072068095207214
Epoch 520, training loss: 1.0765150785446167 = 0.40989282727241516 + 0.1 * 6.66622257232666
Epoch 520, val loss: 0.7997889518737793
Epoch 530, training loss: 1.0571796894073486 = 0.39115703105926514 + 0.1 * 6.660226821899414
Epoch 530, val loss: 0.7931766510009766
Epoch 540, training loss: 1.037669062614441 = 0.3731271028518677 + 0.1 * 6.645419597625732
Epoch 540, val loss: 0.7872722148895264
Epoch 550, training loss: 1.0193328857421875 = 0.35569554567337036 + 0.1 * 6.636373043060303
Epoch 550, val loss: 0.7819854021072388
Epoch 560, training loss: 1.0019445419311523 = 0.33884501457214355 + 0.1 * 6.63099479675293
Epoch 560, val loss: 0.7773995399475098
Epoch 570, training loss: 0.9857937097549438 = 0.3225764334201813 + 0.1 * 6.632172107696533
Epoch 570, val loss: 0.7734686136245728
Epoch 580, training loss: 0.9698880314826965 = 0.30709725618362427 + 0.1 * 6.627907752990723
Epoch 580, val loss: 0.7701593637466431
Epoch 590, training loss: 0.954149603843689 = 0.29236435890197754 + 0.1 * 6.617852210998535
Epoch 590, val loss: 0.7675377130508423
Epoch 600, training loss: 0.939284086227417 = 0.27825209498405457 + 0.1 * 6.6103196144104
Epoch 600, val loss: 0.7654557228088379
Epoch 610, training loss: 0.9253849983215332 = 0.2647361755371094 + 0.1 * 6.606488227844238
Epoch 610, val loss: 0.7640058398246765
Epoch 620, training loss: 0.9121929407119751 = 0.2518517076969147 + 0.1 * 6.603411674499512
Epoch 620, val loss: 0.7631109952926636
Epoch 630, training loss: 0.900281548500061 = 0.23967355489730835 + 0.1 * 6.606080055236816
Epoch 630, val loss: 0.7627915143966675
Epoch 640, training loss: 0.8875113725662231 = 0.2281581461429596 + 0.1 * 6.593532085418701
Epoch 640, val loss: 0.7630693912506104
Epoch 650, training loss: 0.8757457733154297 = 0.21722286939620972 + 0.1 * 6.58522891998291
Epoch 650, val loss: 0.7638339400291443
Epoch 660, training loss: 0.8655482530593872 = 0.20683404803276062 + 0.1 * 6.587142467498779
Epoch 660, val loss: 0.7651476860046387
Epoch 670, training loss: 0.8549764156341553 = 0.19701030850410461 + 0.1 * 6.579660415649414
Epoch 670, val loss: 0.7668877243995667
Epoch 680, training loss: 0.8451906442642212 = 0.18771645426750183 + 0.1 * 6.574741363525391
Epoch 680, val loss: 0.7691581845283508
Epoch 690, training loss: 0.836760938167572 = 0.17892388999462128 + 0.1 * 6.578370094299316
Epoch 690, val loss: 0.7718095779418945
Epoch 700, training loss: 0.8273833990097046 = 0.170628160238266 + 0.1 * 6.567552089691162
Epoch 700, val loss: 0.7748894691467285
Epoch 710, training loss: 0.8188039660453796 = 0.16277599334716797 + 0.1 * 6.560279846191406
Epoch 710, val loss: 0.7783165574073792
Epoch 720, training loss: 0.8108135461807251 = 0.155345618724823 + 0.1 * 6.5546793937683105
Epoch 720, val loss: 0.7820949554443359
Epoch 730, training loss: 0.8033466339111328 = 0.14832383394241333 + 0.1 * 6.550228118896484
Epoch 730, val loss: 0.7861262559890747
Epoch 740, training loss: 0.7965801954269409 = 0.1417001336812973 + 0.1 * 6.548800468444824
Epoch 740, val loss: 0.7905268669128418
Epoch 750, training loss: 0.7900122404098511 = 0.1354254186153412 + 0.1 * 6.545867919921875
Epoch 750, val loss: 0.7951022386550903
Epoch 760, training loss: 0.783332884311676 = 0.12949199974536896 + 0.1 * 6.5384087562561035
Epoch 760, val loss: 0.7999212145805359
Epoch 770, training loss: 0.7771930694580078 = 0.12387688457965851 + 0.1 * 6.5331621170043945
Epoch 770, val loss: 0.804992139339447
Epoch 780, training loss: 0.77205890417099 = 0.11854918301105499 + 0.1 * 6.535097122192383
Epoch 780, val loss: 0.8102091550827026
Epoch 790, training loss: 0.7665634751319885 = 0.11351387202739716 + 0.1 * 6.530495643615723
Epoch 790, val loss: 0.8155543208122253
Epoch 800, training loss: 0.7611762285232544 = 0.10875451564788818 + 0.1 * 6.524217128753662
Epoch 800, val loss: 0.8211343288421631
Epoch 810, training loss: 0.7562623620033264 = 0.10423364490270615 + 0.1 * 6.520287036895752
Epoch 810, val loss: 0.8267778158187866
Epoch 820, training loss: 0.7516375184059143 = 0.09993613511323929 + 0.1 * 6.517014026641846
Epoch 820, val loss: 0.8326112031936646
Epoch 830, training loss: 0.7468560934066772 = 0.09585931897163391 + 0.1 * 6.50996732711792
Epoch 830, val loss: 0.8384647965431213
Epoch 840, training loss: 0.7427184581756592 = 0.09199460595846176 + 0.1 * 6.507238388061523
Epoch 840, val loss: 0.8445029258728027
Epoch 850, training loss: 0.7387488484382629 = 0.0883181244134903 + 0.1 * 6.504307270050049
Epoch 850, val loss: 0.8505502939224243
Epoch 860, training loss: 0.7380083203315735 = 0.08482333272695541 + 0.1 * 6.5318498611450195
Epoch 860, val loss: 0.8566312193870544
Epoch 870, training loss: 0.7317004799842834 = 0.08151734620332718 + 0.1 * 6.5018310546875
Epoch 870, val loss: 0.8627850413322449
Epoch 880, training loss: 0.7279577255249023 = 0.07837435603141785 + 0.1 * 6.495833396911621
Epoch 880, val loss: 0.868959903717041
Epoch 890, training loss: 0.7275107502937317 = 0.07538046687841415 + 0.1 * 6.521302700042725
Epoch 890, val loss: 0.8751319646835327
Epoch 900, training loss: 0.7222590446472168 = 0.07253864407539368 + 0.1 * 6.497204303741455
Epoch 900, val loss: 0.881365180015564
Epoch 910, training loss: 0.7187851667404175 = 0.06983289122581482 + 0.1 * 6.489522933959961
Epoch 910, val loss: 0.8875939249992371
Epoch 920, training loss: 0.7158607840538025 = 0.06725048273801804 + 0.1 * 6.486103057861328
Epoch 920, val loss: 0.8938249349594116
Epoch 930, training loss: 0.7136233448982239 = 0.06478680670261383 + 0.1 * 6.488365650177002
Epoch 930, val loss: 0.9000718593597412
Epoch 940, training loss: 0.7102281451225281 = 0.0624387189745903 + 0.1 * 6.477893829345703
Epoch 940, val loss: 0.906341016292572
Epoch 950, training loss: 0.7085371613502502 = 0.06019660085439682 + 0.1 * 6.483405590057373
Epoch 950, val loss: 0.9126187562942505
Epoch 960, training loss: 0.7064138054847717 = 0.05805940553545952 + 0.1 * 6.483543872833252
Epoch 960, val loss: 0.9188143610954285
Epoch 970, training loss: 0.7062620520591736 = 0.056023579090833664 + 0.1 * 6.502384185791016
Epoch 970, val loss: 0.925055980682373
Epoch 980, training loss: 0.7020714282989502 = 0.05408960580825806 + 0.1 * 6.479817867279053
Epoch 980, val loss: 0.9311746954917908
Epoch 990, training loss: 0.6993151903152466 = 0.05224493518471718 + 0.1 * 6.470702171325684
Epoch 990, val loss: 0.9373596906661987
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 2.8093628883361816 = 1.9496790170669556 + 0.1 * 8.596837997436523
Epoch 0, val loss: 1.9518901109695435
Epoch 10, training loss: 2.8047077655792236 = 1.9450256824493408 + 0.1 * 8.596819877624512
Epoch 10, val loss: 1.9472925662994385
Epoch 20, training loss: 2.7996513843536377 = 1.9399759769439697 + 0.1 * 8.59675407409668
Epoch 20, val loss: 1.942265272140503
Epoch 30, training loss: 2.7936131954193115 = 1.933955430984497 + 0.1 * 8.596576690673828
Epoch 30, val loss: 1.936265230178833
Epoch 40, training loss: 2.786040782928467 = 1.9264461994171143 + 0.1 * 8.595946311950684
Epoch 40, val loss: 1.9287620782852173
Epoch 50, training loss: 2.776028871536255 = 1.9167169332504272 + 0.1 * 8.593118667602539
Epoch 50, val loss: 1.919000506401062
Epoch 60, training loss: 2.761936664581299 = 1.9038419723510742 + 0.1 * 8.58094596862793
Epoch 60, val loss: 1.9060876369476318
Epoch 70, training loss: 2.7407214641571045 = 1.8869634866714478 + 0.1 * 8.537579536437988
Epoch 70, val loss: 1.8893346786499023
Epoch 80, training loss: 2.70424747467041 = 1.8662068843841553 + 0.1 * 8.380404472351074
Epoch 80, val loss: 1.8693263530731201
Epoch 90, training loss: 2.6478211879730225 = 1.843093991279602 + 0.1 * 8.047272682189941
Epoch 90, val loss: 1.847653865814209
Epoch 100, training loss: 2.6109414100646973 = 1.8204139471054077 + 0.1 * 7.905274868011475
Epoch 100, val loss: 1.8269401788711548
Epoch 110, training loss: 2.566060781478882 = 1.8000731468200684 + 0.1 * 7.659876346588135
Epoch 110, val loss: 1.8086133003234863
Epoch 120, training loss: 2.5222949981689453 = 1.7813501358032227 + 0.1 * 7.40944766998291
Epoch 120, val loss: 1.7918285131454468
Epoch 130, training loss: 2.4861538410186768 = 1.7632471323013306 + 0.1 * 7.229067802429199
Epoch 130, val loss: 1.7758581638336182
Epoch 140, training loss: 2.4557747840881348 = 1.7439367771148682 + 0.1 * 7.118378639221191
Epoch 140, val loss: 1.7590429782867432
Epoch 150, training loss: 2.427248954772949 = 1.7217003107070923 + 0.1 * 7.055485248565674
Epoch 150, val loss: 1.7394558191299438
Epoch 160, training loss: 2.396246910095215 = 1.6954494714736938 + 0.1 * 7.007974147796631
Epoch 160, val loss: 1.7161996364593506
Epoch 170, training loss: 2.3615336418151855 = 1.664676308631897 + 0.1 * 6.968574047088623
Epoch 170, val loss: 1.6893634796142578
Epoch 180, training loss: 2.3225715160369873 = 1.6291555166244507 + 0.1 * 6.934159755706787
Epoch 180, val loss: 1.6591852903366089
Epoch 190, training loss: 2.2796225547790527 = 1.5892201662063599 + 0.1 * 6.904024600982666
Epoch 190, val loss: 1.6258877515792847
Epoch 200, training loss: 2.233854293823242 = 1.545992374420166 + 0.1 * 6.878618240356445
Epoch 200, val loss: 1.5901941061019897
Epoch 210, training loss: 2.186640739440918 = 1.5008814334869385 + 0.1 * 6.857593536376953
Epoch 210, val loss: 1.5532548427581787
Epoch 220, training loss: 2.1391096115112305 = 1.455199956893921 + 0.1 * 6.839096546173096
Epoch 220, val loss: 1.516264796257019
Epoch 230, training loss: 2.0923280715942383 = 1.4099578857421875 + 0.1 * 6.823700904846191
Epoch 230, val loss: 1.480221152305603
Epoch 240, training loss: 2.0465304851531982 = 1.366053819656372 + 0.1 * 6.804766654968262
Epoch 240, val loss: 1.4459298849105835
Epoch 250, training loss: 2.001927137374878 = 1.3232269287109375 + 0.1 * 6.787001609802246
Epoch 250, val loss: 1.413106918334961
Epoch 260, training loss: 1.9587616920471191 = 1.2811815738677979 + 0.1 * 6.775801658630371
Epoch 260, val loss: 1.3810967206954956
Epoch 270, training loss: 1.9161765575408936 = 1.240525722503662 + 0.1 * 6.756507873535156
Epoch 270, val loss: 1.3507441282272339
Epoch 280, training loss: 1.874786138534546 = 1.2005869150161743 + 0.1 * 6.741992473602295
Epoch 280, val loss: 1.320945143699646
Epoch 290, training loss: 1.833949089050293 = 1.1610411405563354 + 0.1 * 6.729078769683838
Epoch 290, val loss: 1.2916247844696045
Epoch 300, training loss: 1.794006109237671 = 1.1220476627349854 + 0.1 * 6.719583511352539
Epoch 300, val loss: 1.2627257108688354
Epoch 310, training loss: 1.7546381950378418 = 1.0838392972946167 + 0.1 * 6.707988739013672
Epoch 310, val loss: 1.2347157001495361
Epoch 320, training loss: 1.7158913612365723 = 1.0461785793304443 + 0.1 * 6.697126865386963
Epoch 320, val loss: 1.2069910764694214
Epoch 330, training loss: 1.6788967847824097 = 1.0095133781433105 + 0.1 * 6.693833827972412
Epoch 330, val loss: 1.179925799369812
Epoch 340, training loss: 1.6421282291412354 = 0.9741328358650208 + 0.1 * 6.6799540519714355
Epoch 340, val loss: 1.1539853811264038
Epoch 350, training loss: 1.6067078113555908 = 0.9395553469657898 + 0.1 * 6.671524524688721
Epoch 350, val loss: 1.1284525394439697
Epoch 360, training loss: 1.5727111101150513 = 0.9059433341026306 + 0.1 * 6.667677402496338
Epoch 360, val loss: 1.103639006614685
Epoch 370, training loss: 1.5390002727508545 = 0.8735083341598511 + 0.1 * 6.654918670654297
Epoch 370, val loss: 1.0799648761749268
Epoch 380, training loss: 1.5066308975219727 = 0.8418667316436768 + 0.1 * 6.647641658782959
Epoch 380, val loss: 1.056860089302063
Epoch 390, training loss: 1.475653052330017 = 0.810957133769989 + 0.1 * 6.64695930480957
Epoch 390, val loss: 1.0344171524047852
Epoch 400, training loss: 1.4444780349731445 = 0.7811725735664368 + 0.1 * 6.633054733276367
Epoch 400, val loss: 1.0130772590637207
Epoch 410, training loss: 1.414689302444458 = 0.7523178458213806 + 0.1 * 6.623713970184326
Epoch 410, val loss: 0.99282306432724
Epoch 420, training loss: 1.38608980178833 = 0.7242776155471802 + 0.1 * 6.618122100830078
Epoch 420, val loss: 0.9734208583831787
Epoch 430, training loss: 1.358344554901123 = 0.6973462700843811 + 0.1 * 6.609982013702393
Epoch 430, val loss: 0.9553090333938599
Epoch 440, training loss: 1.3315585851669312 = 0.6714560389518738 + 0.1 * 6.601025581359863
Epoch 440, val loss: 0.9386530518531799
Epoch 450, training loss: 1.3062536716461182 = 0.6463364362716675 + 0.1 * 6.599172115325928
Epoch 450, val loss: 0.9229744076728821
Epoch 460, training loss: 1.2813105583190918 = 0.6220906972885132 + 0.1 * 6.592197895050049
Epoch 460, val loss: 0.9085867404937744
Epoch 470, training loss: 1.257121205329895 = 0.5987016558647156 + 0.1 * 6.584195613861084
Epoch 470, val loss: 0.8955077528953552
Epoch 480, training loss: 1.2333722114562988 = 0.5760449767112732 + 0.1 * 6.573271751403809
Epoch 480, val loss: 0.8836245536804199
Epoch 490, training loss: 1.2124624252319336 = 0.553909420967102 + 0.1 * 6.585529327392578
Epoch 490, val loss: 0.8726570010185242
Epoch 500, training loss: 1.1887664794921875 = 0.5324022769927979 + 0.1 * 6.563642501831055
Epoch 500, val loss: 0.8627279996871948
Epoch 510, training loss: 1.1668033599853516 = 0.5113239884376526 + 0.1 * 6.554792881011963
Epoch 510, val loss: 0.8537514209747314
Epoch 520, training loss: 1.1456586122512817 = 0.49052804708480835 + 0.1 * 6.551305294036865
Epoch 520, val loss: 0.8454738855361938
Epoch 530, training loss: 1.125567078590393 = 0.470071941614151 + 0.1 * 6.554951190948486
Epoch 530, val loss: 0.8378999829292297
Epoch 540, training loss: 1.1043134927749634 = 0.45000725984573364 + 0.1 * 6.543062210083008
Epoch 540, val loss: 0.8309494853019714
Epoch 550, training loss: 1.0858819484710693 = 0.4302416145801544 + 0.1 * 6.556402683258057
Epoch 550, val loss: 0.8247103095054626
Epoch 560, training loss: 1.0638025999069214 = 0.41089317202568054 + 0.1 * 6.529094696044922
Epoch 560, val loss: 0.8190857172012329
Epoch 570, training loss: 1.0447487831115723 = 0.39189496636390686 + 0.1 * 6.528538227081299
Epoch 570, val loss: 0.8140923976898193
Epoch 580, training loss: 1.0271072387695312 = 0.3733009099960327 + 0.1 * 6.538062572479248
Epoch 580, val loss: 0.8096240162849426
Epoch 590, training loss: 1.0074464082717896 = 0.3552874028682709 + 0.1 * 6.521589756011963
Epoch 590, val loss: 0.8059314489364624
Epoch 600, training loss: 0.9892188310623169 = 0.33779051899909973 + 0.1 * 6.514283180236816
Epoch 600, val loss: 0.8028209805488586
Epoch 610, training loss: 0.9721548557281494 = 0.3208425045013428 + 0.1 * 6.513123512268066
Epoch 610, val loss: 0.800329864025116
Epoch 620, training loss: 0.956576943397522 = 0.3046368956565857 + 0.1 * 6.519400596618652
Epoch 620, val loss: 0.7984020709991455
Epoch 630, training loss: 0.9392900466918945 = 0.28921443223953247 + 0.1 * 6.500755786895752
Epoch 630, val loss: 0.797353208065033
Epoch 640, training loss: 0.9241951704025269 = 0.27448147535324097 + 0.1 * 6.497137069702148
Epoch 640, val loss: 0.7967142462730408
Epoch 650, training loss: 0.910060703754425 = 0.2604706883430481 + 0.1 * 6.4959001541137695
Epoch 650, val loss: 0.7966580986976624
Epoch 660, training loss: 0.8965579271316528 = 0.24722698330879211 + 0.1 * 6.493309497833252
Epoch 660, val loss: 0.7973201870918274
Epoch 670, training loss: 0.8836336135864258 = 0.23467867076396942 + 0.1 * 6.489549160003662
Epoch 670, val loss: 0.7984740734100342
Epoch 680, training loss: 0.8729139566421509 = 0.2228097766637802 + 0.1 * 6.501041412353516
Epoch 680, val loss: 0.8001016974449158
Epoch 690, training loss: 0.8598580360412598 = 0.21165643632411957 + 0.1 * 6.482015609741211
Epoch 690, val loss: 0.8023126721382141
Epoch 700, training loss: 0.849490761756897 = 0.2011508345603943 + 0.1 * 6.483398914337158
Epoch 700, val loss: 0.8049845099449158
Epoch 710, training loss: 0.8388111591339111 = 0.19126805663108826 + 0.1 * 6.475431442260742
Epoch 710, val loss: 0.8080134391784668
Epoch 720, training loss: 0.8300308585166931 = 0.18196582794189453 + 0.1 * 6.480650424957275
Epoch 720, val loss: 0.8115370869636536
Epoch 730, training loss: 0.8206567168235779 = 0.17322389781475067 + 0.1 * 6.47432804107666
Epoch 730, val loss: 0.8153497576713562
Epoch 740, training loss: 0.8120005130767822 = 0.1650027632713318 + 0.1 * 6.469977378845215
Epoch 740, val loss: 0.8195751905441284
Epoch 750, training loss: 0.8052240610122681 = 0.15726764500141144 + 0.1 * 6.479564189910889
Epoch 750, val loss: 0.8240019679069519
Epoch 760, training loss: 0.7965585589408875 = 0.15001142024993896 + 0.1 * 6.465471267700195
Epoch 760, val loss: 0.8287505507469177
Epoch 770, training loss: 0.7889829874038696 = 0.14318062365055084 + 0.1 * 6.458023548126221
Epoch 770, val loss: 0.8337424993515015
Epoch 780, training loss: 0.7825800776481628 = 0.13673637807369232 + 0.1 * 6.458436965942383
Epoch 780, val loss: 0.8389582633972168
Epoch 790, training loss: 0.7760040760040283 = 0.1306711584329605 + 0.1 * 6.453329086303711
Epoch 790, val loss: 0.8442939519882202
Epoch 800, training loss: 0.7701627612113953 = 0.12496253103017807 + 0.1 * 6.45200252532959
Epoch 800, val loss: 0.8499377369880676
Epoch 810, training loss: 0.7643085718154907 = 0.11957164853811264 + 0.1 * 6.447369575500488
Epoch 810, val loss: 0.8556732535362244
Epoch 820, training loss: 0.7598466873168945 = 0.11448819935321808 + 0.1 * 6.453585147857666
Epoch 820, val loss: 0.861473023891449
Epoch 830, training loss: 0.7543537616729736 = 0.10970570147037506 + 0.1 * 6.446480751037598
Epoch 830, val loss: 0.8674914240837097
Epoch 840, training loss: 0.7491989135742188 = 0.10518315434455872 + 0.1 * 6.440157413482666
Epoch 840, val loss: 0.8735648989677429
Epoch 850, training loss: 0.7455005645751953 = 0.10089708864688873 + 0.1 * 6.446034908294678
Epoch 850, val loss: 0.8796834945678711
Epoch 860, training loss: 0.7409802079200745 = 0.09684912860393524 + 0.1 * 6.441310405731201
Epoch 860, val loss: 0.8858440518379211
Epoch 870, training loss: 0.7368523478507996 = 0.0930190160870552 + 0.1 * 6.438333511352539
Epoch 870, val loss: 0.8922063112258911
Epoch 880, training loss: 0.7325567603111267 = 0.08938522636890411 + 0.1 * 6.431715488433838
Epoch 880, val loss: 0.8985280990600586
Epoch 890, training loss: 0.7292425036430359 = 0.08593188226222992 + 0.1 * 6.433105945587158
Epoch 890, val loss: 0.904896080493927
Epoch 900, training loss: 0.7258759140968323 = 0.08265940099954605 + 0.1 * 6.432164669036865
Epoch 900, val loss: 0.9112902879714966
Epoch 910, training loss: 0.7224599719047546 = 0.07955817133188248 + 0.1 * 6.429017543792725
Epoch 910, val loss: 0.9177664518356323
Epoch 920, training loss: 0.7213366031646729 = 0.0766100138425827 + 0.1 * 6.447265625
Epoch 920, val loss: 0.9241538047790527
Epoch 930, training loss: 0.716108500957489 = 0.0738149955868721 + 0.1 * 6.4229350090026855
Epoch 930, val loss: 0.9306276440620422
Epoch 940, training loss: 0.7132952809333801 = 0.07115628570318222 + 0.1 * 6.421390056610107
Epoch 940, val loss: 0.9371267557144165
Epoch 950, training loss: 0.7105386853218079 = 0.06862204521894455 + 0.1 * 6.419166088104248
Epoch 950, val loss: 0.9435553550720215
Epoch 960, training loss: 0.7099815011024475 = 0.06620583683252335 + 0.1 * 6.437756538391113
Epoch 960, val loss: 0.9499723315238953
Epoch 970, training loss: 0.706124484539032 = 0.0639076754450798 + 0.1 * 6.422167778015137
Epoch 970, val loss: 0.9564064145088196
Epoch 980, training loss: 0.7034439444541931 = 0.061715323477983475 + 0.1 * 6.417286396026611
Epoch 980, val loss: 0.962890625
Epoch 990, training loss: 0.701235830783844 = 0.05962198227643967 + 0.1 * 6.416138172149658
Epoch 990, val loss: 0.9692419171333313
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.812335266209805
The final CL Acc:0.78272, 0.00349, The final GNN Acc:0.81462, 0.00194
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13260])
remove edge: torch.Size([2, 7864])
updated graph: torch.Size([2, 10568])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.8117175102233887 = 1.952032446861267 + 0.1 * 8.59684944152832
Epoch 0, val loss: 1.9483836889266968
Epoch 10, training loss: 2.8067853450775146 = 1.9471031427383423 + 0.1 * 8.596822738647461
Epoch 10, val loss: 1.9436613321304321
Epoch 20, training loss: 2.8014049530029297 = 1.9417307376861572 + 0.1 * 8.596742630004883
Epoch 20, val loss: 1.9383800029754639
Epoch 30, training loss: 2.795119524002075 = 1.9354667663574219 + 0.1 * 8.596527099609375
Epoch 30, val loss: 1.9321609735488892
Epoch 40, training loss: 2.7874443531036377 = 1.9278647899627686 + 0.1 * 8.595794677734375
Epoch 40, val loss: 1.9244859218597412
Epoch 50, training loss: 2.777625560760498 = 1.9183509349822998 + 0.1 * 8.592744827270508
Epoch 50, val loss: 1.9148062467575073
Epoch 60, training loss: 2.764209747314453 = 1.9061872959136963 + 0.1 * 8.580223083496094
Epoch 60, val loss: 1.9024264812469482
Epoch 70, training loss: 2.744285821914673 = 1.890552282333374 + 0.1 * 8.537334442138672
Epoch 70, val loss: 1.886656641960144
Epoch 80, training loss: 2.711197853088379 = 1.8712117671966553 + 0.1 * 8.399861335754395
Epoch 80, val loss: 1.8676714897155762
Epoch 90, training loss: 2.654930830001831 = 1.8489534854888916 + 0.1 * 8.059773445129395
Epoch 90, val loss: 1.8465561866760254
Epoch 100, training loss: 2.6120989322662354 = 1.8262323141098022 + 0.1 * 7.85866641998291
Epoch 100, val loss: 1.8257150650024414
Epoch 110, training loss: 2.5679335594177246 = 1.8052117824554443 + 0.1 * 7.627217769622803
Epoch 110, val loss: 1.8067216873168945
Epoch 120, training loss: 2.5266919136047363 = 1.7849401235580444 + 0.1 * 7.417518615722656
Epoch 120, val loss: 1.788987636566162
Epoch 130, training loss: 2.4896862506866455 = 1.7647922039031982 + 0.1 * 7.248940944671631
Epoch 130, val loss: 1.7720282077789307
Epoch 140, training loss: 2.456181526184082 = 1.7440133094787598 + 0.1 * 7.121682167053223
Epoch 140, val loss: 1.7547781467437744
Epoch 150, training loss: 2.4245691299438477 = 1.7211096286773682 + 0.1 * 7.03459358215332
Epoch 150, val loss: 1.7355183362960815
Epoch 160, training loss: 2.3918612003326416 = 1.6947505474090576 + 0.1 * 6.971106052398682
Epoch 160, val loss: 1.7128525972366333
Epoch 170, training loss: 2.356340169906616 = 1.663877248764038 + 0.1 * 6.924629211425781
Epoch 170, val loss: 1.6860988140106201
Epoch 180, training loss: 2.3168723583221436 = 1.6277201175689697 + 0.1 * 6.8915228843688965
Epoch 180, val loss: 1.654997706413269
Epoch 190, training loss: 2.2727389335632324 = 1.5859874486923218 + 0.1 * 6.867514610290527
Epoch 190, val loss: 1.6196383237838745
Epoch 200, training loss: 2.2244226932525635 = 1.539540410041809 + 0.1 * 6.848823070526123
Epoch 200, val loss: 1.5806118249893188
Epoch 210, training loss: 2.1732258796691895 = 1.4900540113449097 + 0.1 * 6.831719398498535
Epoch 210, val loss: 1.5393640995025635
Epoch 220, training loss: 2.1204113960266113 = 1.4389798641204834 + 0.1 * 6.814313888549805
Epoch 220, val loss: 1.4968788623809814
Epoch 230, training loss: 2.0671911239624023 = 1.3874890804290771 + 0.1 * 6.797019958496094
Epoch 230, val loss: 1.4542350769042969
Epoch 240, training loss: 2.014335870742798 = 1.33622407913208 + 0.1 * 6.781117916107178
Epoch 240, val loss: 1.4122521877288818
Epoch 250, training loss: 1.9623188972473145 = 1.2853654623031616 + 0.1 * 6.769534111022949
Epoch 250, val loss: 1.3708257675170898
Epoch 260, training loss: 1.911573886871338 = 1.235711932182312 + 0.1 * 6.758620262145996
Epoch 260, val loss: 1.3305587768554688
Epoch 270, training loss: 1.862187147140503 = 1.1871601343154907 + 0.1 * 6.750270366668701
Epoch 270, val loss: 1.2913522720336914
Epoch 280, training loss: 1.8140616416931152 = 1.139910101890564 + 0.1 * 6.741515159606934
Epoch 280, val loss: 1.2533560991287231
Epoch 290, training loss: 1.7677350044250488 = 1.0944710969924927 + 0.1 * 6.732639789581299
Epoch 290, val loss: 1.21713387966156
Epoch 300, training loss: 1.723304271697998 = 1.0510261058807373 + 0.1 * 6.722781658172607
Epoch 300, val loss: 1.1827467679977417
Epoch 310, training loss: 1.6809722185134888 = 1.009499192237854 + 0.1 * 6.714730262756348
Epoch 310, val loss: 1.1500356197357178
Epoch 320, training loss: 1.640378475189209 = 0.9700568914413452 + 0.1 * 6.703215599060059
Epoch 320, val loss: 1.1191035509109497
Epoch 330, training loss: 1.6016051769256592 = 0.9323141574859619 + 0.1 * 6.692909240722656
Epoch 330, val loss: 1.089627742767334
Epoch 340, training loss: 1.5646638870239258 = 0.8959794044494629 + 0.1 * 6.686844825744629
Epoch 340, val loss: 1.0613020658493042
Epoch 350, training loss: 1.528929591178894 = 0.8614261746406555 + 0.1 * 6.675034046173096
Epoch 350, val loss: 1.0344440937042236
Epoch 360, training loss: 1.494588851928711 = 0.828080952167511 + 0.1 * 6.665079116821289
Epoch 360, val loss: 1.0085996389389038
Epoch 370, training loss: 1.4619388580322266 = 0.7958288192749023 + 0.1 * 6.661099433898926
Epoch 370, val loss: 0.983748733997345
Epoch 380, training loss: 1.4297864437103271 = 0.7649743556976318 + 0.1 * 6.648120880126953
Epoch 380, val loss: 0.9602951407432556
Epoch 390, training loss: 1.399383544921875 = 0.7352777123451233 + 0.1 * 6.64105749130249
Epoch 390, val loss: 0.9380173087120056
Epoch 400, training loss: 1.3706893920898438 = 0.7068760395050049 + 0.1 * 6.6381330490112305
Epoch 400, val loss: 0.9171717762947083
Epoch 410, training loss: 1.3431406021118164 = 0.6801360845565796 + 0.1 * 6.630044937133789
Epoch 410, val loss: 0.8981220126152039
Epoch 420, training loss: 1.3170459270477295 = 0.6547244191169739 + 0.1 * 6.623214244842529
Epoch 420, val loss: 0.8805584907531738
Epoch 430, training loss: 1.292435646057129 = 0.6306677460670471 + 0.1 * 6.617678642272949
Epoch 430, val loss: 0.8645880818367004
Epoch 440, training loss: 1.2693994045257568 = 0.6080475449562073 + 0.1 * 6.613518238067627
Epoch 440, val loss: 0.8502055406570435
Epoch 450, training loss: 1.2473433017730713 = 0.5865955948829651 + 0.1 * 6.607477188110352
Epoch 450, val loss: 0.8371512293815613
Epoch 460, training loss: 1.2273473739624023 = 0.566239595413208 + 0.1 * 6.611076831817627
Epoch 460, val loss: 0.8254371285438538
Epoch 470, training loss: 1.207095742225647 = 0.5470531582832336 + 0.1 * 6.600425720214844
Epoch 470, val loss: 0.8150053024291992
Epoch 480, training loss: 1.18808913230896 = 0.5288031101226807 + 0.1 * 6.592859268188477
Epoch 480, val loss: 0.8056098222732544
Epoch 490, training loss: 1.1702367067337036 = 0.5113484263420105 + 0.1 * 6.588882923126221
Epoch 490, val loss: 0.7971543073654175
Epoch 500, training loss: 1.153584599494934 = 0.49460628628730774 + 0.1 * 6.589783191680908
Epoch 500, val loss: 0.78956139087677
Epoch 510, training loss: 1.1376140117645264 = 0.47859978675842285 + 0.1 * 6.590141296386719
Epoch 510, val loss: 0.7827132940292358
Epoch 520, training loss: 1.1211926937103271 = 0.46325382590293884 + 0.1 * 6.579388618469238
Epoch 520, val loss: 0.7764378786087036
Epoch 530, training loss: 1.1054867506027222 = 0.44832292199134827 + 0.1 * 6.571638107299805
Epoch 530, val loss: 0.770629346370697
Epoch 540, training loss: 1.0905057191848755 = 0.43365857005119324 + 0.1 * 6.5684709548950195
Epoch 540, val loss: 0.765164852142334
Epoch 550, training loss: 1.0758997201919556 = 0.41921401023864746 + 0.1 * 6.566856861114502
Epoch 550, val loss: 0.7599761486053467
Epoch 560, training loss: 1.0612993240356445 = 0.40499117970466614 + 0.1 * 6.56308126449585
Epoch 560, val loss: 0.755060076713562
Epoch 570, training loss: 1.0467348098754883 = 0.39092791080474854 + 0.1 * 6.558068752288818
Epoch 570, val loss: 0.7503323554992676
Epoch 580, training loss: 1.0318615436553955 = 0.37686994671821594 + 0.1 * 6.549915790557861
Epoch 580, val loss: 0.7456980347633362
Epoch 590, training loss: 1.0177669525146484 = 0.3627558648586273 + 0.1 * 6.550110340118408
Epoch 590, val loss: 0.7411602735519409
Epoch 600, training loss: 1.0037381649017334 = 0.34870314598083496 + 0.1 * 6.550349712371826
Epoch 600, val loss: 0.7367289066314697
Epoch 610, training loss: 0.9886515736579895 = 0.3347378969192505 + 0.1 * 6.53913688659668
Epoch 610, val loss: 0.7323875427246094
Epoch 620, training loss: 0.9741559028625488 = 0.3208406865596771 + 0.1 * 6.5331525802612305
Epoch 620, val loss: 0.7282130122184753
Epoch 630, training loss: 0.9606959819793701 = 0.30704376101493835 + 0.1 * 6.536522388458252
Epoch 630, val loss: 0.7241912484169006
Epoch 640, training loss: 0.9476712942123413 = 0.2934856116771698 + 0.1 * 6.5418572425842285
Epoch 640, val loss: 0.7204423546791077
Epoch 650, training loss: 0.932769775390625 = 0.2802616059780121 + 0.1 * 6.525081634521484
Epoch 650, val loss: 0.7169954180717468
Epoch 660, training loss: 0.9195408225059509 = 0.26736587285995483 + 0.1 * 6.521749496459961
Epoch 660, val loss: 0.7138792872428894
Epoch 670, training loss: 0.9066843390464783 = 0.2548511028289795 + 0.1 * 6.518332481384277
Epoch 670, val loss: 0.711177408695221
Epoch 680, training loss: 0.8943066596984863 = 0.2427619993686676 + 0.1 * 6.515446186065674
Epoch 680, val loss: 0.7088132500648499
Epoch 690, training loss: 0.8824222087860107 = 0.23112264275550842 + 0.1 * 6.51299524307251
Epoch 690, val loss: 0.7068994045257568
Epoch 700, training loss: 0.87078458070755 = 0.21992909908294678 + 0.1 * 6.508554458618164
Epoch 700, val loss: 0.7053815722465515
Epoch 710, training loss: 0.8594740033149719 = 0.20918059349060059 + 0.1 * 6.502933979034424
Epoch 710, val loss: 0.7042726874351501
Epoch 720, training loss: 0.8495855927467346 = 0.19888436794281006 + 0.1 * 6.507011890411377
Epoch 720, val loss: 0.7036088705062866
Epoch 730, training loss: 0.8393716812133789 = 0.18906769156455994 + 0.1 * 6.503039360046387
Epoch 730, val loss: 0.7032843828201294
Epoch 740, training loss: 0.8292670845985413 = 0.17971622943878174 + 0.1 * 6.495508193969727
Epoch 740, val loss: 0.7033587098121643
Epoch 750, training loss: 0.8199402093887329 = 0.17081218957901 + 0.1 * 6.4912800788879395
Epoch 750, val loss: 0.7037740349769592
Epoch 760, training loss: 0.8110480308532715 = 0.16233184933662415 + 0.1 * 6.487161636352539
Epoch 760, val loss: 0.7045349478721619
Epoch 770, training loss: 0.8035109043121338 = 0.15427154302597046 + 0.1 * 6.492393493652344
Epoch 770, val loss: 0.7056103348731995
Epoch 780, training loss: 0.7951481938362122 = 0.1466248482465744 + 0.1 * 6.485233306884766
Epoch 780, val loss: 0.706937849521637
Epoch 790, training loss: 0.7878893613815308 = 0.13937075436115265 + 0.1 * 6.4851861000061035
Epoch 790, val loss: 0.70858234167099
Epoch 800, training loss: 0.7805014252662659 = 0.132512629032135 + 0.1 * 6.479887962341309
Epoch 800, val loss: 0.7104588747024536
Epoch 810, training loss: 0.7735810875892639 = 0.12601475417613983 + 0.1 * 6.475663185119629
Epoch 810, val loss: 0.7125604748725891
Epoch 820, training loss: 0.7670347094535828 = 0.11985336244106293 + 0.1 * 6.471813201904297
Epoch 820, val loss: 0.7149121761322021
Epoch 830, training loss: 0.7615276575088501 = 0.11402180790901184 + 0.1 * 6.475058078765869
Epoch 830, val loss: 0.7174627780914307
Epoch 840, training loss: 0.7564143538475037 = 0.1085161343216896 + 0.1 * 6.478981971740723
Epoch 840, val loss: 0.7201394438743591
Epoch 850, training loss: 0.7508638501167297 = 0.10333312302827835 + 0.1 * 6.475307464599609
Epoch 850, val loss: 0.7229857444763184
Epoch 860, training loss: 0.7446643114089966 = 0.09843626618385315 + 0.1 * 6.4622802734375
Epoch 860, val loss: 0.7259167432785034
Epoch 870, training loss: 0.7395215630531311 = 0.0937933698296547 + 0.1 * 6.457281589508057
Epoch 870, val loss: 0.7290006875991821
Epoch 880, training loss: 0.7354133129119873 = 0.08938030898571014 + 0.1 * 6.460329532623291
Epoch 880, val loss: 0.7321946024894714
Epoch 890, training loss: 0.7314091920852661 = 0.0852300301194191 + 0.1 * 6.461791515350342
Epoch 890, val loss: 0.7355047464370728
Epoch 900, training loss: 0.7267178893089294 = 0.08132530748844147 + 0.1 * 6.453925609588623
Epoch 900, val loss: 0.7388892769813538
Epoch 910, training loss: 0.7232574820518494 = 0.07763706147670746 + 0.1 * 6.456203937530518
Epoch 910, val loss: 0.7423803210258484
Epoch 920, training loss: 0.7189830541610718 = 0.074156254529953 + 0.1 * 6.448267459869385
Epoch 920, val loss: 0.7459664940834045
Epoch 930, training loss: 0.7160511016845703 = 0.07087000459432602 + 0.1 * 6.451810836791992
Epoch 930, val loss: 0.749616265296936
Epoch 940, training loss: 0.7121504545211792 = 0.06777184456586838 + 0.1 * 6.443786144256592
Epoch 940, val loss: 0.7533166408538818
Epoch 950, training loss: 0.7095370292663574 = 0.06484346836805344 + 0.1 * 6.446935653686523
Epoch 950, val loss: 0.7570630311965942
Epoch 960, training loss: 0.7064163088798523 = 0.06208020821213722 + 0.1 * 6.443360805511475
Epoch 960, val loss: 0.7608523964881897
Epoch 970, training loss: 0.702950119972229 = 0.05947360396385193 + 0.1 * 6.434764862060547
Epoch 970, val loss: 0.7646622657775879
Epoch 980, training loss: 0.7007986903190613 = 0.05700914189219475 + 0.1 * 6.43789529800415
Epoch 980, val loss: 0.7685089111328125
Epoch 990, training loss: 0.697931170463562 = 0.054683126509189606 + 0.1 * 6.4324798583984375
Epoch 990, val loss: 0.7723917365074158
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 2.805053234100342 = 1.9453668594360352 + 0.1 * 8.59686279296875
Epoch 0, val loss: 1.9382987022399902
Epoch 10, training loss: 2.800076961517334 = 1.940392255783081 + 0.1 * 8.596846580505371
Epoch 10, val loss: 1.9336020946502686
Epoch 20, training loss: 2.7947373390197754 = 1.93505859375 + 0.1 * 8.59678840637207
Epoch 20, val loss: 1.9284390211105347
Epoch 30, training loss: 2.788541316986084 = 1.9288769960403442 + 0.1 * 8.596643447875977
Epoch 30, val loss: 1.92239511013031
Epoch 40, training loss: 2.780966281890869 = 1.9213495254516602 + 0.1 * 8.596168518066406
Epoch 40, val loss: 1.9149689674377441
Epoch 50, training loss: 2.771254539489746 = 1.911833643913269 + 0.1 * 8.594208717346191
Epoch 50, val loss: 1.9054946899414062
Epoch 60, training loss: 2.758143424987793 = 1.899546504020691 + 0.1 * 8.585968971252441
Epoch 60, val loss: 1.8932653665542603
Epoch 70, training loss: 2.73929500579834 = 1.8837039470672607 + 0.1 * 8.55591106414795
Epoch 70, val loss: 1.8776524066925049
Epoch 80, training loss: 2.709641933441162 = 1.8639780282974243 + 0.1 * 8.456640243530273
Epoch 80, val loss: 1.858718752861023
Epoch 90, training loss: 2.656822681427002 = 1.841354250907898 + 0.1 * 8.154684066772461
Epoch 90, val loss: 1.8377565145492554
Epoch 100, training loss: 2.6174721717834473 = 1.8176597356796265 + 0.1 * 7.998124599456787
Epoch 100, val loss: 1.8166366815567017
Epoch 110, training loss: 2.5721471309661865 = 1.7949905395507812 + 0.1 * 7.771564960479736
Epoch 110, val loss: 1.7969852685928345
Epoch 120, training loss: 2.522171974182129 = 1.7731690406799316 + 0.1 * 7.490027904510498
Epoch 120, val loss: 1.7779438495635986
Epoch 130, training loss: 2.4789209365844727 = 1.7510383129119873 + 0.1 * 7.278824806213379
Epoch 130, val loss: 1.7585384845733643
Epoch 140, training loss: 2.4409468173980713 = 1.7272162437438965 + 0.1 * 7.137304782867432
Epoch 140, val loss: 1.7377283573150635
Epoch 150, training loss: 2.4044222831726074 = 1.700745701789856 + 0.1 * 7.036766052246094
Epoch 150, val loss: 1.7145724296569824
Epoch 160, training loss: 2.367422580718994 = 1.670233130455017 + 0.1 * 6.971895694732666
Epoch 160, val loss: 1.6877883672714233
Epoch 170, training loss: 2.3274946212768555 = 1.6344621181488037 + 0.1 * 6.930325031280518
Epoch 170, val loss: 1.6563529968261719
Epoch 180, training loss: 2.2830965518951416 = 1.5930591821670532 + 0.1 * 6.9003729820251465
Epoch 180, val loss: 1.6199891567230225
Epoch 190, training loss: 2.2340798377990723 = 1.5464105606079102 + 0.1 * 6.876693248748779
Epoch 190, val loss: 1.5791395902633667
Epoch 200, training loss: 2.181199312210083 = 1.4955378770828247 + 0.1 * 6.856614112854004
Epoch 200, val loss: 1.535121202468872
Epoch 210, training loss: 2.1258859634399414 = 1.4419997930526733 + 0.1 * 6.838862895965576
Epoch 210, val loss: 1.4896328449249268
Epoch 220, training loss: 2.069688320159912 = 1.387374758720398 + 0.1 * 6.8231353759765625
Epoch 220, val loss: 1.4440698623657227
Epoch 230, training loss: 2.0139636993408203 = 1.3330366611480713 + 0.1 * 6.809269905090332
Epoch 230, val loss: 1.3994601964950562
Epoch 240, training loss: 1.9596450328826904 = 1.2799066305160522 + 0.1 * 6.797384262084961
Epoch 240, val loss: 1.3565990924835205
Epoch 250, training loss: 1.9079593420028687 = 1.2291209697723389 + 0.1 * 6.788383483886719
Epoch 250, val loss: 1.3162767887115479
Epoch 260, training loss: 1.8590354919433594 = 1.1811639070510864 + 0.1 * 6.77871561050415
Epoch 260, val loss: 1.2784212827682495
Epoch 270, training loss: 1.8131401538848877 = 1.1361974477767944 + 0.1 * 6.769427299499512
Epoch 270, val loss: 1.2433202266693115
Epoch 280, training loss: 1.7706527709960938 = 1.0944050550460815 + 0.1 * 6.762476444244385
Epoch 280, val loss: 1.2110896110534668
Epoch 290, training loss: 1.7312960624694824 = 1.0560622215270996 + 0.1 * 6.75233793258667
Epoch 290, val loss: 1.181952714920044
Epoch 300, training loss: 1.6951136589050293 = 1.0207723379135132 + 0.1 * 6.74341344833374
Epoch 300, val loss: 1.155447006225586
Epoch 310, training loss: 1.6615979671478271 = 0.9879189729690552 + 0.1 * 6.736789703369141
Epoch 310, val loss: 1.1310936212539673
Epoch 320, training loss: 1.6300560235977173 = 0.9572212100028992 + 0.1 * 6.728348255157471
Epoch 320, val loss: 1.1087979078292847
Epoch 330, training loss: 1.599955677986145 = 0.9279545545578003 + 0.1 * 6.720011234283447
Epoch 330, val loss: 1.087852120399475
Epoch 340, training loss: 1.5707690715789795 = 0.8995087146759033 + 0.1 * 6.712603569030762
Epoch 340, val loss: 1.0677542686462402
Epoch 350, training loss: 1.54221773147583 = 0.8715556263923645 + 0.1 * 6.706620693206787
Epoch 350, val loss: 1.048277735710144
Epoch 360, training loss: 1.5142582654953003 = 0.8440128564834595 + 0.1 * 6.702454090118408
Epoch 360, val loss: 1.029483675956726
Epoch 370, training loss: 1.4861630201339722 = 0.8165333271026611 + 0.1 * 6.696296691894531
Epoch 370, val loss: 1.0109004974365234
Epoch 380, training loss: 1.4589073657989502 = 0.7891489863395691 + 0.1 * 6.6975836753845215
Epoch 380, val loss: 0.9927277565002441
Epoch 390, training loss: 1.4311020374298096 = 0.7621213793754578 + 0.1 * 6.6898064613342285
Epoch 390, val loss: 0.9751529097557068
Epoch 400, training loss: 1.4037384986877441 = 0.7351941466331482 + 0.1 * 6.685442924499512
Epoch 400, val loss: 0.9579857587814331
Epoch 410, training loss: 1.3765206336975098 = 0.7083443999290466 + 0.1 * 6.681761741638184
Epoch 410, val loss: 0.9412822723388672
Epoch 420, training loss: 1.3494336605072021 = 0.6815977692604065 + 0.1 * 6.678359031677246
Epoch 420, val loss: 0.9251091480255127
Epoch 430, training loss: 1.3237876892089844 = 0.6551597118377686 + 0.1 * 6.686279296875
Epoch 430, val loss: 0.909559965133667
Epoch 440, training loss: 1.2968475818634033 = 0.6292650699615479 + 0.1 * 6.675825595855713
Epoch 440, val loss: 0.894814133644104
Epoch 450, training loss: 1.2706456184387207 = 0.6036195158958435 + 0.1 * 6.670261383056641
Epoch 450, val loss: 0.8806642889976501
Epoch 460, training loss: 1.244847059249878 = 0.578183114528656 + 0.1 * 6.6666388511657715
Epoch 460, val loss: 0.8670918345451355
Epoch 470, training loss: 1.219910740852356 = 0.5529610514640808 + 0.1 * 6.669497013092041
Epoch 470, val loss: 0.8540753126144409
Epoch 480, training loss: 1.1951351165771484 = 0.5284574627876282 + 0.1 * 6.66677713394165
Epoch 480, val loss: 0.8417791128158569
Epoch 490, training loss: 1.1702730655670166 = 0.5045602917671204 + 0.1 * 6.6571269035339355
Epoch 490, val loss: 0.8302112817764282
Epoch 500, training loss: 1.146571397781372 = 0.4811546802520752 + 0.1 * 6.6541666984558105
Epoch 500, val loss: 0.8193036317825317
Epoch 510, training loss: 1.1232924461364746 = 0.45826733112335205 + 0.1 * 6.650250434875488
Epoch 510, val loss: 0.8090956211090088
Epoch 520, training loss: 1.1005877256393433 = 0.43594685196876526 + 0.1 * 6.646409034729004
Epoch 520, val loss: 0.7996105551719666
Epoch 530, training loss: 1.0793980360031128 = 0.4142521917819977 + 0.1 * 6.651458740234375
Epoch 530, val loss: 0.7908927798271179
Epoch 540, training loss: 1.0577348470687866 = 0.3934403955936432 + 0.1 * 6.642943859100342
Epoch 540, val loss: 0.7830337882041931
Epoch 550, training loss: 1.0370090007781982 = 0.3734472692012787 + 0.1 * 6.635616779327393
Epoch 550, val loss: 0.7760569453239441
Epoch 560, training loss: 1.0172243118286133 = 0.3541858494281769 + 0.1 * 6.6303839683532715
Epoch 560, val loss: 0.7699480056762695
Epoch 570, training loss: 0.9981247782707214 = 0.3356142044067383 + 0.1 * 6.625105381011963
Epoch 570, val loss: 0.7646402716636658
Epoch 580, training loss: 0.9805402755737305 = 0.31772711873054504 + 0.1 * 6.62813138961792
Epoch 580, val loss: 0.7601108551025391
Epoch 590, training loss: 0.9626830816268921 = 0.3007206618785858 + 0.1 * 6.61962366104126
Epoch 590, val loss: 0.7563856244087219
Epoch 600, training loss: 0.9458456039428711 = 0.2845262587070465 + 0.1 * 6.613193035125732
Epoch 600, val loss: 0.7533689737319946
Epoch 610, training loss: 0.9297045469284058 = 0.26903975009918213 + 0.1 * 6.606647968292236
Epoch 610, val loss: 0.7510672807693481
Epoch 620, training loss: 0.9159561395645142 = 0.2542589008808136 + 0.1 * 6.61697244644165
Epoch 620, val loss: 0.7493829131126404
Epoch 630, training loss: 0.9000611901283264 = 0.24028412997722626 + 0.1 * 6.5977702140808105
Epoch 630, val loss: 0.7483044266700745
Epoch 640, training loss: 0.8861876130104065 = 0.22704695165157318 + 0.1 * 6.591406345367432
Epoch 640, val loss: 0.7477588653564453
Epoch 650, training loss: 0.8731942176818848 = 0.21451926231384277 + 0.1 * 6.58674955368042
Epoch 650, val loss: 0.7477880120277405
Epoch 660, training loss: 0.8618283867835999 = 0.2027236372232437 + 0.1 * 6.591047286987305
Epoch 660, val loss: 0.748310923576355
Epoch 670, training loss: 0.8499689102172852 = 0.19167789816856384 + 0.1 * 6.582910537719727
Epoch 670, val loss: 0.7492763996124268
Epoch 680, training loss: 0.8388742208480835 = 0.18132059276103973 + 0.1 * 6.575535774230957
Epoch 680, val loss: 0.750665009021759
Epoch 690, training loss: 0.8288633823394775 = 0.17159625887870789 + 0.1 * 6.572671413421631
Epoch 690, val loss: 0.7524222135543823
Epoch 700, training loss: 0.8198598027229309 = 0.16251836717128754 + 0.1 * 6.573413848876953
Epoch 700, val loss: 0.7544842958450317
Epoch 710, training loss: 0.810292661190033 = 0.1540447622537613 + 0.1 * 6.562479019165039
Epoch 710, val loss: 0.7568135261535645
Epoch 720, training loss: 0.8018462657928467 = 0.14609751105308533 + 0.1 * 6.557487964630127
Epoch 720, val loss: 0.7594441175460815
Epoch 730, training loss: 0.7941921353340149 = 0.13865013420581818 + 0.1 * 6.555419921875
Epoch 730, val loss: 0.7623347043991089
Epoch 740, training loss: 0.7867480516433716 = 0.1316882073879242 + 0.1 * 6.55059814453125
Epoch 740, val loss: 0.7654098868370056
Epoch 750, training loss: 0.779791533946991 = 0.12514972686767578 + 0.1 * 6.546417713165283
Epoch 750, val loss: 0.7687026262283325
Epoch 760, training loss: 0.7743856906890869 = 0.11901457607746124 + 0.1 * 6.5537109375
Epoch 760, val loss: 0.7721781134605408
Epoch 770, training loss: 0.7675341963768005 = 0.11328418552875519 + 0.1 * 6.542500019073486
Epoch 770, val loss: 0.7757605314254761
Epoch 780, training loss: 0.7615602612495422 = 0.10789962112903595 + 0.1 * 6.536606311798096
Epoch 780, val loss: 0.779480516910553
Epoch 790, training loss: 0.7561089992523193 = 0.10282640159130096 + 0.1 * 6.532825946807861
Epoch 790, val loss: 0.7833845019340515
Epoch 800, training loss: 0.7521157264709473 = 0.09804282337427139 + 0.1 * 6.540728569030762
Epoch 800, val loss: 0.7873926758766174
Epoch 810, training loss: 0.7469226717948914 = 0.09355403482913971 + 0.1 * 6.53368616104126
Epoch 810, val loss: 0.7914600372314453
Epoch 820, training loss: 0.7421514987945557 = 0.08933542668819427 + 0.1 * 6.528160572052002
Epoch 820, val loss: 0.7955947518348694
Epoch 830, training loss: 0.7377356290817261 = 0.08535448461771011 + 0.1 * 6.523810863494873
Epoch 830, val loss: 0.7998182773590088
Epoch 840, training loss: 0.7338505983352661 = 0.08160797506570816 + 0.1 * 6.522426128387451
Epoch 840, val loss: 0.8040995001792908
Epoch 850, training loss: 0.7297662496566772 = 0.0780852735042572 + 0.1 * 6.516809940338135
Epoch 850, val loss: 0.808408796787262
Epoch 860, training loss: 0.7261304259300232 = 0.07475576549768448 + 0.1 * 6.51374626159668
Epoch 860, val loss: 0.8127642869949341
Epoch 870, training loss: 0.7240819931030273 = 0.07160434126853943 + 0.1 * 6.524776935577393
Epoch 870, val loss: 0.817202627658844
Epoch 880, training loss: 0.7195640206336975 = 0.06863401085138321 + 0.1 * 6.5092997550964355
Epoch 880, val loss: 0.821618378162384
Epoch 890, training loss: 0.716598391532898 = 0.06582725793123245 + 0.1 * 6.507711410522461
Epoch 890, val loss: 0.8260459899902344
Epoch 900, training loss: 0.7139993906021118 = 0.06316694617271423 + 0.1 * 6.50832462310791
Epoch 900, val loss: 0.830517590045929
Epoch 910, training loss: 0.711993396282196 = 0.060653772205114365 + 0.1 * 6.513396263122559
Epoch 910, val loss: 0.8349783420562744
Epoch 920, training loss: 0.708332359790802 = 0.05828041955828667 + 0.1 * 6.500519275665283
Epoch 920, val loss: 0.8394214510917664
Epoch 930, training loss: 0.7056573033332825 = 0.05602848902344704 + 0.1 * 6.496287822723389
Epoch 930, val loss: 0.8438693881034851
Epoch 940, training loss: 0.7052268981933594 = 0.05389005318284035 + 0.1 * 6.513368606567383
Epoch 940, val loss: 0.8483447432518005
Epoch 950, training loss: 0.7016292810440063 = 0.05186579376459122 + 0.1 * 6.497634410858154
Epoch 950, val loss: 0.8527948260307312
Epoch 960, training loss: 0.6989421844482422 = 0.04994447901844978 + 0.1 * 6.4899773597717285
Epoch 960, val loss: 0.8572054505348206
Epoch 970, training loss: 0.6984288096427917 = 0.04811689630150795 + 0.1 * 6.503118515014648
Epoch 970, val loss: 0.8616240620613098
Epoch 980, training loss: 0.695275068283081 = 0.04638456553220749 + 0.1 * 6.48890495300293
Epoch 980, val loss: 0.8660139441490173
Epoch 990, training loss: 0.6930299401283264 = 0.04473625123500824 + 0.1 * 6.482936859130859
Epoch 990, val loss: 0.8703945875167847
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8296296296296297
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 2.8038272857666016 = 1.9441434144973755 + 0.1 * 8.59683895111084
Epoch 0, val loss: 1.9424160718917847
Epoch 10, training loss: 2.7991671562194824 = 1.93948495388031 + 0.1 * 8.596820831298828
Epoch 10, val loss: 1.937663197517395
Epoch 20, training loss: 2.7942633628845215 = 1.9345885515213013 + 0.1 * 8.596748352050781
Epoch 20, val loss: 1.9326072931289673
Epoch 30, training loss: 2.7886457443237305 = 1.9289891719818115 + 0.1 * 8.596564292907715
Epoch 30, val loss: 1.9268046617507935
Epoch 40, training loss: 2.781846046447754 = 1.9222511053085327 + 0.1 * 8.595949172973633
Epoch 40, val loss: 1.9197973012924194
Epoch 50, training loss: 2.7731122970581055 = 1.9137734174728394 + 0.1 * 8.593389511108398
Epoch 50, val loss: 1.9109834432601929
Epoch 60, training loss: 2.7611336708068848 = 1.9028489589691162 + 0.1 * 8.582846641540527
Epoch 60, val loss: 1.8997018337249756
Epoch 70, training loss: 2.7433876991271973 = 1.888697624206543 + 0.1 * 8.546899795532227
Epoch 70, val loss: 1.8853065967559814
Epoch 80, training loss: 2.7146241664886475 = 1.8709874153137207 + 0.1 * 8.436367988586426
Epoch 80, val loss: 1.8677141666412354
Epoch 90, training loss: 2.6617207527160645 = 1.850235939025879 + 0.1 * 8.114849090576172
Epoch 90, val loss: 1.8476592302322388
Epoch 100, training loss: 2.6201672554016113 = 1.8281248807907104 + 0.1 * 7.9204230308532715
Epoch 100, val loss: 1.8270992040634155
Epoch 110, training loss: 2.5759592056274414 = 1.8066219091415405 + 0.1 * 7.6933722496032715
Epoch 110, val loss: 1.8078486919403076
Epoch 120, training loss: 2.5283899307250977 = 1.7862646579742432 + 0.1 * 7.4212517738342285
Epoch 120, val loss: 1.790167212486267
Epoch 130, training loss: 2.485952138900757 = 1.766682744026184 + 0.1 * 7.192693710327148
Epoch 130, val loss: 1.7740416526794434
Epoch 140, training loss: 2.4518399238586426 = 1.7466206550598145 + 0.1 * 7.052191734313965
Epoch 140, val loss: 1.757821798324585
Epoch 150, training loss: 2.4231624603271484 = 1.7238150835037231 + 0.1 * 6.993474006652832
Epoch 150, val loss: 1.73860764503479
Epoch 160, training loss: 2.393641471862793 = 1.6965690851211548 + 0.1 * 6.970724582672119
Epoch 160, val loss: 1.7147586345672607
Epoch 170, training loss: 2.360105514526367 = 1.6641905307769775 + 0.1 * 6.959149360656738
Epoch 170, val loss: 1.6864606142044067
Epoch 180, training loss: 2.3215060234069824 = 1.6263880729675293 + 0.1 * 6.951179027557373
Epoch 180, val loss: 1.6541398763656616
Epoch 190, training loss: 2.278029203414917 = 1.5836384296417236 + 0.1 * 6.943907260894775
Epoch 190, val loss: 1.6180819272994995
Epoch 200, training loss: 2.230588436126709 = 1.5369131565093994 + 0.1 * 6.936751365661621
Epoch 200, val loss: 1.578652024269104
Epoch 210, training loss: 2.180297374725342 = 1.4874241352081299 + 0.1 * 6.9287309646606445
Epoch 210, val loss: 1.5369274616241455
Epoch 220, training loss: 2.127903699874878 = 1.4359594583511353 + 0.1 * 6.919442653656006
Epoch 220, val loss: 1.4936301708221436
Epoch 230, training loss: 2.073885440826416 = 1.3830822706222534 + 0.1 * 6.9080328941345215
Epoch 230, val loss: 1.449191927909851
Epoch 240, training loss: 2.018627166748047 = 1.3292287588119507 + 0.1 * 6.893982887268066
Epoch 240, val loss: 1.4041664600372314
Epoch 250, training loss: 1.962284803390503 = 1.2745881080627441 + 0.1 * 6.876967430114746
Epoch 250, val loss: 1.3586156368255615
Epoch 260, training loss: 1.9050509929656982 = 1.2193056344985962 + 0.1 * 6.857453346252441
Epoch 260, val loss: 1.312867283821106
Epoch 270, training loss: 1.8477246761322021 = 1.1638753414154053 + 0.1 * 6.838493347167969
Epoch 270, val loss: 1.2672070264816284
Epoch 280, training loss: 1.791689395904541 = 1.1094677448272705 + 0.1 * 6.822215557098389
Epoch 280, val loss: 1.2232346534729004
Epoch 290, training loss: 1.7363653182983398 = 1.0559935569763184 + 0.1 * 6.803718090057373
Epoch 290, val loss: 1.1804373264312744
Epoch 300, training loss: 1.6827659606933594 = 1.003890872001648 + 0.1 * 6.788750648498535
Epoch 300, val loss: 1.1391828060150146
Epoch 310, training loss: 1.6309893131256104 = 0.9539220929145813 + 0.1 * 6.770671367645264
Epoch 310, val loss: 1.1002377271652222
Epoch 320, training loss: 1.5816247463226318 = 0.9060189127922058 + 0.1 * 6.7560577392578125
Epoch 320, val loss: 1.063239574432373
Epoch 330, training loss: 1.5349634885787964 = 0.8608210682868958 + 0.1 * 6.741424083709717
Epoch 330, val loss: 1.028899073600769
Epoch 340, training loss: 1.491246223449707 = 0.8183068633079529 + 0.1 * 6.72939395904541
Epoch 340, val loss: 0.996955931186676
Epoch 350, training loss: 1.4509440660476685 = 0.7783716320991516 + 0.1 * 6.725724220275879
Epoch 350, val loss: 0.9672731757164001
Epoch 360, training loss: 1.4121898412704468 = 0.7413432598114014 + 0.1 * 6.708465576171875
Epoch 360, val loss: 0.9404342174530029
Epoch 370, training loss: 1.3763446807861328 = 0.7066879868507385 + 0.1 * 6.696566104888916
Epoch 370, val loss: 0.9157808423042297
Epoch 380, training loss: 1.344191312789917 = 0.6741213798522949 + 0.1 * 6.700699329376221
Epoch 380, val loss: 0.8931533098220825
Epoch 390, training loss: 1.31211256980896 = 0.6438659429550171 + 0.1 * 6.682465553283691
Epoch 390, val loss: 0.8727643489837646
Epoch 400, training loss: 1.2823981046676636 = 0.6154125332832336 + 0.1 * 6.66985559463501
Epoch 400, val loss: 0.8542967438697815
Epoch 410, training loss: 1.2544889450073242 = 0.5883980989456177 + 0.1 * 6.660907745361328
Epoch 410, val loss: 0.837430477142334
Epoch 420, training loss: 1.2282240390777588 = 0.5627657175064087 + 0.1 * 6.654583930969238
Epoch 420, val loss: 0.8220612406730652
Epoch 430, training loss: 1.203354001045227 = 0.5385593175888062 + 0.1 * 6.647946834564209
Epoch 430, val loss: 0.8083131313323975
Epoch 440, training loss: 1.1807602643966675 = 0.5155100226402283 + 0.1 * 6.652502059936523
Epoch 440, val loss: 0.7957921624183655
Epoch 450, training loss: 1.157038688659668 = 0.49366915225982666 + 0.1 * 6.633695125579834
Epoch 450, val loss: 0.78467857837677
Epoch 460, training loss: 1.1352908611297607 = 0.47276124358177185 + 0.1 * 6.625296115875244
Epoch 460, val loss: 0.774681031703949
Epoch 470, training loss: 1.1149702072143555 = 0.4526612162590027 + 0.1 * 6.62308931350708
Epoch 470, val loss: 0.7656068801879883
Epoch 480, training loss: 1.0948058366775513 = 0.4334784150123596 + 0.1 * 6.613274097442627
Epoch 480, val loss: 0.7574966549873352
Epoch 490, training loss: 1.076488971710205 = 0.41509801149368286 + 0.1 * 6.613908767700195
Epoch 490, val loss: 0.7502979636192322
Epoch 500, training loss: 1.0580390691757202 = 0.397512823343277 + 0.1 * 6.605262279510498
Epoch 500, val loss: 0.7438533306121826
Epoch 510, training loss: 1.040281891822815 = 0.38061755895614624 + 0.1 * 6.596642971038818
Epoch 510, val loss: 0.738152801990509
Epoch 520, training loss: 1.0252642631530762 = 0.36433884501457214 + 0.1 * 6.609253883361816
Epoch 520, val loss: 0.7329890727996826
Epoch 530, training loss: 1.0076591968536377 = 0.3487757444381714 + 0.1 * 6.5888352394104
Epoch 530, val loss: 0.7285031080245972
Epoch 540, training loss: 0.9916895031929016 = 0.3337884545326233 + 0.1 * 6.579010486602783
Epoch 540, val loss: 0.7245106101036072
Epoch 550, training loss: 0.9766616225242615 = 0.3193258047103882 + 0.1 * 6.573358058929443
Epoch 550, val loss: 0.7210017442703247
Epoch 560, training loss: 0.9660074710845947 = 0.30538704991340637 + 0.1 * 6.606204509735107
Epoch 560, val loss: 0.7179529666900635
Epoch 570, training loss: 0.9488915801048279 = 0.2921620011329651 + 0.1 * 6.567295551300049
Epoch 570, val loss: 0.7154062986373901
Epoch 580, training loss: 0.9356851577758789 = 0.27949240803718567 + 0.1 * 6.561927795410156
Epoch 580, val loss: 0.7132797241210938
Epoch 590, training loss: 0.9233875274658203 = 0.2673157751560211 + 0.1 * 6.560717582702637
Epoch 590, val loss: 0.7115247249603271
Epoch 600, training loss: 0.9108464121818542 = 0.2556471824645996 + 0.1 * 6.551991939544678
Epoch 600, val loss: 0.7101514935493469
Epoch 610, training loss: 0.8991548418998718 = 0.24445480108261108 + 0.1 * 6.547000408172607
Epoch 610, val loss: 0.7091947793960571
Epoch 620, training loss: 0.8883671760559082 = 0.23375512659549713 + 0.1 * 6.546120643615723
Epoch 620, val loss: 0.7085167765617371
Epoch 630, training loss: 0.8773941993713379 = 0.22355535626411438 + 0.1 * 6.538388729095459
Epoch 630, val loss: 0.7082532048225403
Epoch 640, training loss: 0.8667271137237549 = 0.21378439664840698 + 0.1 * 6.5294270515441895
Epoch 640, val loss: 0.7082555890083313
Epoch 650, training loss: 0.8580151796340942 = 0.20442530512809753 + 0.1 * 6.5358991622924805
Epoch 650, val loss: 0.708557665348053
Epoch 660, training loss: 0.8495427370071411 = 0.1955241560935974 + 0.1 * 6.540185451507568
Epoch 660, val loss: 0.7091608047485352
Epoch 670, training loss: 0.8393151760101318 = 0.18709024786949158 + 0.1 * 6.5222487449646
Epoch 670, val loss: 0.7100600004196167
Epoch 680, training loss: 0.8305861949920654 = 0.17904379963874817 + 0.1 * 6.5154242515563965
Epoch 680, val loss: 0.7111716270446777
Epoch 690, training loss: 0.8231245279312134 = 0.17135360836982727 + 0.1 * 6.517709255218506
Epoch 690, val loss: 0.7125275135040283
Epoch 700, training loss: 0.8153313398361206 = 0.16403689980506897 + 0.1 * 6.51294469833374
Epoch 700, val loss: 0.7140756845474243
Epoch 710, training loss: 0.8077540397644043 = 0.15706273913383484 + 0.1 * 6.506913185119629
Epoch 710, val loss: 0.7158620357513428
Epoch 720, training loss: 0.801156222820282 = 0.15042614936828613 + 0.1 * 6.50730037689209
Epoch 720, val loss: 0.7177850604057312
Epoch 730, training loss: 0.7941505312919617 = 0.14413006603717804 + 0.1 * 6.500204563140869
Epoch 730, val loss: 0.7199456691741943
Epoch 740, training loss: 0.7876850962638855 = 0.1381279081106186 + 0.1 * 6.495571613311768
Epoch 740, val loss: 0.7222341299057007
Epoch 750, training loss: 0.7818912267684937 = 0.13239923119544983 + 0.1 * 6.494919300079346
Epoch 750, val loss: 0.7246643304824829
Epoch 760, training loss: 0.7761624455451965 = 0.1269504576921463 + 0.1 * 6.492119789123535
Epoch 760, val loss: 0.7272495031356812
Epoch 770, training loss: 0.770775318145752 = 0.12177396565675735 + 0.1 * 6.490013599395752
Epoch 770, val loss: 0.7299390435218811
Epoch 780, training loss: 0.7652992606163025 = 0.11685590445995331 + 0.1 * 6.484433174133301
Epoch 780, val loss: 0.7327650189399719
Epoch 790, training loss: 0.7602773904800415 = 0.1121634766459465 + 0.1 * 6.481139183044434
Epoch 790, val loss: 0.7356683611869812
Epoch 800, training loss: 0.7555190324783325 = 0.10768384486436844 + 0.1 * 6.478351593017578
Epoch 800, val loss: 0.7386763691902161
Epoch 810, training loss: 0.7510489225387573 = 0.10341796278953552 + 0.1 * 6.476309299468994
Epoch 810, val loss: 0.7417691349983215
Epoch 820, training loss: 0.7471702694892883 = 0.09936623275279999 + 0.1 * 6.4780402183532715
Epoch 820, val loss: 0.7449540495872498
Epoch 830, training loss: 0.743873119354248 = 0.09550480544567108 + 0.1 * 6.483682632446289
Epoch 830, val loss: 0.7481893301010132
Epoch 840, training loss: 0.7388025522232056 = 0.09182659536600113 + 0.1 * 6.469759464263916
Epoch 840, val loss: 0.7514886856079102
Epoch 850, training loss: 0.7348158955574036 = 0.08831263333559036 + 0.1 * 6.465032577514648
Epoch 850, val loss: 0.7548624873161316
Epoch 860, training loss: 0.7325432300567627 = 0.08496484905481339 + 0.1 * 6.475783348083496
Epoch 860, val loss: 0.7582219243049622
Epoch 870, training loss: 0.7276118397712708 = 0.08178234845399857 + 0.1 * 6.458294868469238
Epoch 870, val loss: 0.7616682648658752
Epoch 880, training loss: 0.7245416045188904 = 0.07874156534671783 + 0.1 * 6.458000659942627
Epoch 880, val loss: 0.7651355862617493
Epoch 890, training loss: 0.7221605181694031 = 0.075835682451725 + 0.1 * 6.463248252868652
Epoch 890, val loss: 0.7685951590538025
Epoch 900, training loss: 0.7186433672904968 = 0.07306049019098282 + 0.1 * 6.455828666687012
Epoch 900, val loss: 0.7721233367919922
Epoch 910, training loss: 0.7161070704460144 = 0.07040943950414658 + 0.1 * 6.456976413726807
Epoch 910, val loss: 0.7756657600402832
Epoch 920, training loss: 0.7134431600570679 = 0.06788184493780136 + 0.1 * 6.455613136291504
Epoch 920, val loss: 0.779190182685852
Epoch 930, training loss: 0.7100873589515686 = 0.06547007709741592 + 0.1 * 6.44617223739624
Epoch 930, val loss: 0.7827702760696411
Epoch 940, training loss: 0.7088370323181152 = 0.06316434592008591 + 0.1 * 6.456726551055908
Epoch 940, val loss: 0.7863356471061707
Epoch 950, training loss: 0.7051621079444885 = 0.06096228212118149 + 0.1 * 6.441998481750488
Epoch 950, val loss: 0.7899025082588196
Epoch 960, training loss: 0.7032779455184937 = 0.058855000883340836 + 0.1 * 6.4442291259765625
Epoch 960, val loss: 0.7935106754302979
Epoch 970, training loss: 0.7013859152793884 = 0.056840162724256516 + 0.1 * 6.445457458496094
Epoch 970, val loss: 0.7970914244651794
Epoch 980, training loss: 0.6988037824630737 = 0.05491188168525696 + 0.1 * 6.4389190673828125
Epoch 980, val loss: 0.8006887435913086
Epoch 990, training loss: 0.6980763077735901 = 0.053062986582517624 + 0.1 * 6.450133323669434
Epoch 990, val loss: 0.8042680621147156
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8344754876120191
The final CL Acc:0.81481, 0.01839, The final GNN Acc:0.83588, 0.00108
