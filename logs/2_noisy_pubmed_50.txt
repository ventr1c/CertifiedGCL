Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97612])
remove edge: torch.Size([2, 79702])
updated graph: torch.Size([2, 88666])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.205322265625 = 1.0933228731155396 + 50.0 * 10.582240104675293
Epoch 0, val loss: 1.0925633907318115
Epoch 10, training loss: 530.1397705078125 = 1.0874968767166138 + 50.0 * 10.581045150756836
Epoch 10, val loss: 1.0866916179656982
Epoch 20, training loss: 529.618896484375 = 1.0806853771209717 + 50.0 * 10.57076358795166
Epoch 20, val loss: 1.0798320770263672
Epoch 30, training loss: 526.3829345703125 = 1.0724139213562012 + 50.0 * 10.506211280822754
Epoch 30, val loss: 1.0715067386627197
Epoch 40, training loss: 515.0914306640625 = 1.0645769834518433 + 50.0 * 10.280536651611328
Epoch 40, val loss: 1.064031958580017
Epoch 50, training loss: 495.54229736328125 = 1.0582737922668457 + 50.0 * 9.889679908752441
Epoch 50, val loss: 1.0579450130462646
Epoch 60, training loss: 475.82244873046875 = 1.0516780614852905 + 50.0 * 9.495415687561035
Epoch 60, val loss: 1.0513209104537964
Epoch 70, training loss: 465.7083435058594 = 1.0425161123275757 + 50.0 * 9.293316841125488
Epoch 70, val loss: 1.0421960353851318
Epoch 80, training loss: 462.00836181640625 = 1.0332010984420776 + 50.0 * 9.219503402709961
Epoch 80, val loss: 1.0331614017486572
Epoch 90, training loss: 460.3658142089844 = 1.024951457977295 + 50.0 * 9.186817169189453
Epoch 90, val loss: 1.0251891613006592
Epoch 100, training loss: 457.7833251953125 = 1.0178335905075073 + 50.0 * 9.135310173034668
Epoch 100, val loss: 1.0183302164077759
Epoch 110, training loss: 454.0921936035156 = 1.0113590955734253 + 50.0 * 9.061616897583008
Epoch 110, val loss: 1.0120755434036255
Epoch 120, training loss: 447.54693603515625 = 1.005936861038208 + 50.0 * 8.930819511413574
Epoch 120, val loss: 1.0070172548294067
Epoch 130, training loss: 441.958740234375 = 1.0025383234024048 + 50.0 * 8.819124221801758
Epoch 130, val loss: 1.0036399364471436
Epoch 140, training loss: 439.9923095703125 = 0.9966968894004822 + 50.0 * 8.779911994934082
Epoch 140, val loss: 0.9975343942642212
Epoch 150, training loss: 438.5574035644531 = 0.9877287149429321 + 50.0 * 8.75139331817627
Epoch 150, val loss: 0.9885199666023254
Epoch 160, training loss: 437.31451416015625 = 0.9770719408988953 + 50.0 * 8.7267484664917
Epoch 160, val loss: 0.9781193733215332
Epoch 170, training loss: 436.05499267578125 = 0.9659643173217773 + 50.0 * 8.701780319213867
Epoch 170, val loss: 0.9673994779586792
Epoch 180, training loss: 434.90155029296875 = 0.9548010230064392 + 50.0 * 8.678935050964355
Epoch 180, val loss: 0.9565622806549072
Epoch 190, training loss: 434.00372314453125 = 0.9429435133934021 + 50.0 * 8.661215782165527
Epoch 190, val loss: 0.9449792504310608
Epoch 200, training loss: 433.0663757324219 = 0.9299232363700867 + 50.0 * 8.642728805541992
Epoch 200, val loss: 0.9322750568389893
Epoch 210, training loss: 432.1792297363281 = 0.9161205291748047 + 50.0 * 8.625262260437012
Epoch 210, val loss: 0.918898344039917
Epoch 220, training loss: 431.40826416015625 = 0.9017115831375122 + 50.0 * 8.61013126373291
Epoch 220, val loss: 0.9049720764160156
Epoch 230, training loss: 430.7511291503906 = 0.8864787220954895 + 50.0 * 8.59729290008545
Epoch 230, val loss: 0.8901662826538086
Epoch 240, training loss: 430.0360412597656 = 0.8703793287277222 + 50.0 * 8.58331298828125
Epoch 240, val loss: 0.874674379825592
Epoch 250, training loss: 429.3419494628906 = 0.8538944721221924 + 50.0 * 8.569761276245117
Epoch 250, val loss: 0.8587485551834106
Epoch 260, training loss: 428.7642517089844 = 0.8370989561080933 + 50.0 * 8.55854320526123
Epoch 260, val loss: 0.8425709009170532
Epoch 270, training loss: 428.4139709472656 = 0.8199149966239929 + 50.0 * 8.551880836486816
Epoch 270, val loss: 0.8259255886077881
Epoch 280, training loss: 427.83294677734375 = 0.8023518323898315 + 50.0 * 8.54061222076416
Epoch 280, val loss: 0.8089966773986816
Epoch 290, training loss: 427.36663818359375 = 0.7848904132843018 + 50.0 * 8.531635284423828
Epoch 290, val loss: 0.7922058701515198
Epoch 300, training loss: 426.9356994628906 = 0.7675697207450867 + 50.0 * 8.523362159729004
Epoch 300, val loss: 0.7755461931228638
Epoch 310, training loss: 426.7539367675781 = 0.7504239678382874 + 50.0 * 8.52007007598877
Epoch 310, val loss: 0.7591007947921753
Epoch 320, training loss: 426.2337951660156 = 0.7333866953849792 + 50.0 * 8.510007858276367
Epoch 320, val loss: 0.742697536945343
Epoch 330, training loss: 425.89508056640625 = 0.7167348861694336 + 50.0 * 8.50356674194336
Epoch 330, val loss: 0.7267690300941467
Epoch 340, training loss: 425.57855224609375 = 0.7007286548614502 + 50.0 * 8.497556686401367
Epoch 340, val loss: 0.7115190029144287
Epoch 350, training loss: 425.37744140625 = 0.6852540969848633 + 50.0 * 8.493844032287598
Epoch 350, val loss: 0.6967805027961731
Epoch 360, training loss: 425.0264892578125 = 0.6701755523681641 + 50.0 * 8.487126350402832
Epoch 360, val loss: 0.6825389862060547
Epoch 370, training loss: 424.765625 = 0.6559059619903564 + 50.0 * 8.482193946838379
Epoch 370, val loss: 0.6690655946731567
Epoch 380, training loss: 424.49285888671875 = 0.6424259543418884 + 50.0 * 8.477008819580078
Epoch 380, val loss: 0.6564516425132751
Epoch 390, training loss: 424.3123474121094 = 0.6297533512115479 + 50.0 * 8.473651885986328
Epoch 390, val loss: 0.6446748375892639
Epoch 400, training loss: 424.04888916015625 = 0.6178312301635742 + 50.0 * 8.468621253967285
Epoch 400, val loss: 0.6335696578025818
Epoch 410, training loss: 423.8049011230469 = 0.6067156195640564 + 50.0 * 8.463963508605957
Epoch 410, val loss: 0.6233740448951721
Epoch 420, training loss: 423.5206604003906 = 0.5965238213539124 + 50.0 * 8.45848274230957
Epoch 420, val loss: 0.6140791773796082
Epoch 430, training loss: 423.38916015625 = 0.5871405005455017 + 50.0 * 8.456040382385254
Epoch 430, val loss: 0.6055237054824829
Epoch 440, training loss: 423.1982116699219 = 0.5783481597900391 + 50.0 * 8.452397346496582
Epoch 440, val loss: 0.5976129174232483
Epoch 450, training loss: 422.84234619140625 = 0.5702506899833679 + 50.0 * 8.445442199707031
Epoch 450, val loss: 0.5903582572937012
Epoch 460, training loss: 422.67822265625 = 0.5627999305725098 + 50.0 * 8.44230842590332
Epoch 460, val loss: 0.5837655067443848
Epoch 470, training loss: 422.5796203613281 = 0.5558001399040222 + 50.0 * 8.440476417541504
Epoch 470, val loss: 0.5776153802871704
Epoch 480, training loss: 422.3489074707031 = 0.5492460131645203 + 50.0 * 8.435993194580078
Epoch 480, val loss: 0.5719143748283386
Epoch 490, training loss: 422.2165832519531 = 0.5432466864585876 + 50.0 * 8.433466911315918
Epoch 490, val loss: 0.5667765140533447
Epoch 500, training loss: 422.05694580078125 = 0.5376433730125427 + 50.0 * 8.430386543273926
Epoch 500, val loss: 0.5619975328445435
Epoch 510, training loss: 421.9264831542969 = 0.5324127674102783 + 50.0 * 8.427881240844727
Epoch 510, val loss: 0.5576019883155823
Epoch 520, training loss: 421.8340759277344 = 0.5275228023529053 + 50.0 * 8.426131248474121
Epoch 520, val loss: 0.5535678267478943
Epoch 530, training loss: 421.7320251464844 = 0.5228355526924133 + 50.0 * 8.42418384552002
Epoch 530, val loss: 0.5497391223907471
Epoch 540, training loss: 421.6976623535156 = 0.5184084177017212 + 50.0 * 8.423584938049316
Epoch 540, val loss: 0.5461999773979187
Epoch 550, training loss: 421.52349853515625 = 0.5143465995788574 + 50.0 * 8.420183181762695
Epoch 550, val loss: 0.5430063605308533
Epoch 560, training loss: 421.39117431640625 = 0.5105465054512024 + 50.0 * 8.417612075805664
Epoch 560, val loss: 0.5400819778442383
Epoch 570, training loss: 421.4254150390625 = 0.5069489479064941 + 50.0 * 8.41836929321289
Epoch 570, val loss: 0.5373905897140503
Epoch 580, training loss: 421.3882141113281 = 0.5034488439559937 + 50.0 * 8.417695045471191
Epoch 580, val loss: 0.5347565412521362
Epoch 590, training loss: 421.1646423339844 = 0.5001526474952698 + 50.0 * 8.413290023803711
Epoch 590, val loss: 0.5323671102523804
Epoch 600, training loss: 421.03948974609375 = 0.49705731868743896 + 50.0 * 8.410848617553711
Epoch 600, val loss: 0.530104398727417
Epoch 610, training loss: 421.06854248046875 = 0.4941229224205017 + 50.0 * 8.41148853302002
Epoch 610, val loss: 0.5280815958976746
Epoch 620, training loss: 421.0863342285156 = 0.4912101924419403 + 50.0 * 8.41190242767334
Epoch 620, val loss: 0.5259953737258911
Epoch 630, training loss: 420.8342590332031 = 0.4884546101093292 + 50.0 * 8.406915664672852
Epoch 630, val loss: 0.5241467952728271
Epoch 640, training loss: 420.732177734375 = 0.4858850836753845 + 50.0 * 8.404926300048828
Epoch 640, val loss: 0.5224514603614807
Epoch 650, training loss: 420.6539306640625 = 0.4834328591823578 + 50.0 * 8.403409957885742
Epoch 650, val loss: 0.5208591818809509
Epoch 660, training loss: 420.617919921875 = 0.48106443881988525 + 50.0 * 8.40273666381836
Epoch 660, val loss: 0.5193530321121216
Epoch 670, training loss: 420.6639709472656 = 0.4787233769893646 + 50.0 * 8.403704643249512
Epoch 670, val loss: 0.5178723931312561
Epoch 680, training loss: 420.51861572265625 = 0.47643399238586426 + 50.0 * 8.400843620300293
Epoch 680, val loss: 0.5164608955383301
Epoch 690, training loss: 420.406982421875 = 0.4742693603038788 + 50.0 * 8.398653984069824
Epoch 690, val loss: 0.5151508450508118
Epoch 700, training loss: 420.3616943359375 = 0.47219377756118774 + 50.0 * 8.39778995513916
Epoch 700, val loss: 0.513957142829895
Epoch 710, training loss: 420.51654052734375 = 0.4701615571975708 + 50.0 * 8.400927543640137
Epoch 710, val loss: 0.5127440094947815
Epoch 720, training loss: 420.3497314453125 = 0.4681495726108551 + 50.0 * 8.397631645202637
Epoch 720, val loss: 0.511542558670044
Epoch 730, training loss: 420.1898193359375 = 0.46620747447013855 + 50.0 * 8.394472122192383
Epoch 730, val loss: 0.5103852152824402
Epoch 740, training loss: 420.1610107421875 = 0.46436047554016113 + 50.0 * 8.393933296203613
Epoch 740, val loss: 0.5094029307365417
Epoch 750, training loss: 420.1741943359375 = 0.46255242824554443 + 50.0 * 8.394232749938965
Epoch 750, val loss: 0.5083943009376526
Epoch 760, training loss: 420.04010009765625 = 0.46077603101730347 + 50.0 * 8.391586303710938
Epoch 760, val loss: 0.5074335336685181
Epoch 770, training loss: 420.1139831542969 = 0.45905163884162903 + 50.0 * 8.393098831176758
Epoch 770, val loss: 0.5065156817436218
Epoch 780, training loss: 419.9494323730469 = 0.45734140276908875 + 50.0 * 8.38984203338623
Epoch 780, val loss: 0.5055062770843506
Epoch 790, training loss: 419.8843688964844 = 0.45569702982902527 + 50.0 * 8.38857364654541
Epoch 790, val loss: 0.5046446919441223
Epoch 800, training loss: 420.15380859375 = 0.45409056544303894 + 50.0 * 8.393994331359863
Epoch 800, val loss: 0.5037774443626404
Epoch 810, training loss: 419.9187316894531 = 0.45242899656295776 + 50.0 * 8.389326095581055
Epoch 810, val loss: 0.5028923153877258
Epoch 820, training loss: 419.762451171875 = 0.4508647918701172 + 50.0 * 8.386231422424316
Epoch 820, val loss: 0.5020368099212646
Epoch 830, training loss: 419.6929931640625 = 0.44935980439186096 + 50.0 * 8.384872436523438
Epoch 830, val loss: 0.5012210607528687
Epoch 840, training loss: 419.67352294921875 = 0.44788262248039246 + 50.0 * 8.384512901306152
Epoch 840, val loss: 0.500464141368866
Epoch 850, training loss: 419.7399597167969 = 0.4463842511177063 + 50.0 * 8.385871887207031
Epoch 850, val loss: 0.4996649920940399
Epoch 860, training loss: 419.65301513671875 = 0.4448789656162262 + 50.0 * 8.384162902832031
Epoch 860, val loss: 0.49886447191238403
Epoch 870, training loss: 419.518798828125 = 0.44343698024749756 + 50.0 * 8.38150691986084
Epoch 870, val loss: 0.4980998635292053
Epoch 880, training loss: 419.4857177734375 = 0.44203832745552063 + 50.0 * 8.380873680114746
Epoch 880, val loss: 0.49740469455718994
Epoch 890, training loss: 419.5230407714844 = 0.44065943360328674 + 50.0 * 8.381647109985352
Epoch 890, val loss: 0.4966649115085602
Epoch 900, training loss: 419.78253173828125 = 0.43923962116241455 + 50.0 * 8.386865615844727
Epoch 900, val loss: 0.49590128660202026
Epoch 910, training loss: 419.57373046875 = 0.4377661347389221 + 50.0 * 8.382719039916992
Epoch 910, val loss: 0.4951036274433136
Epoch 920, training loss: 419.4169921875 = 0.4363865852355957 + 50.0 * 8.37961196899414
Epoch 920, val loss: 0.4942859709262848
Epoch 930, training loss: 419.3126220703125 = 0.43506959080696106 + 50.0 * 8.377551078796387
Epoch 930, val loss: 0.4936259686946869
Epoch 940, training loss: 419.2598876953125 = 0.4337767958641052 + 50.0 * 8.376522064208984
Epoch 940, val loss: 0.4929358959197998
Epoch 950, training loss: 419.2286376953125 = 0.4324904680252075 + 50.0 * 8.375923156738281
Epoch 950, val loss: 0.49225950241088867
Epoch 960, training loss: 419.19476318359375 = 0.4312034845352173 + 50.0 * 8.37527084350586
Epoch 960, val loss: 0.4915713667869568
Epoch 970, training loss: 419.376220703125 = 0.42991310358047485 + 50.0 * 8.378926277160645
Epoch 970, val loss: 0.49090245366096497
Epoch 980, training loss: 419.3444519042969 = 0.4285658001899719 + 50.0 * 8.378317832946777
Epoch 980, val loss: 0.49008283019065857
Epoch 990, training loss: 419.2308044433594 = 0.4272332191467285 + 50.0 * 8.376070976257324
Epoch 990, val loss: 0.48927924036979675
Epoch 1000, training loss: 419.1582946777344 = 0.42592477798461914 + 50.0 * 8.37464714050293
Epoch 1000, val loss: 0.48857468366622925
Epoch 1010, training loss: 419.0796203613281 = 0.4246542751789093 + 50.0 * 8.373099327087402
Epoch 1010, val loss: 0.48786234855651855
Epoch 1020, training loss: 419.03387451171875 = 0.4234229028224945 + 50.0 * 8.372208595275879
Epoch 1020, val loss: 0.4871998429298401
Epoch 1030, training loss: 419.0503234863281 = 0.4222009778022766 + 50.0 * 8.372562408447266
Epoch 1030, val loss: 0.486523300409317
Epoch 1040, training loss: 419.1944274902344 = 0.42094433307647705 + 50.0 * 8.375469207763672
Epoch 1040, val loss: 0.4858360290527344
Epoch 1050, training loss: 418.98016357421875 = 0.41965290904045105 + 50.0 * 8.371210098266602
Epoch 1050, val loss: 0.48507794737815857
Epoch 1060, training loss: 418.9420166015625 = 0.4184218645095825 + 50.0 * 8.370471954345703
Epoch 1060, val loss: 0.48428890109062195
Epoch 1070, training loss: 418.90179443359375 = 0.4172217845916748 + 50.0 * 8.369690895080566
Epoch 1070, val loss: 0.4836439788341522
Epoch 1080, training loss: 418.8642578125 = 0.416040301322937 + 50.0 * 8.368964195251465
Epoch 1080, val loss: 0.48297178745269775
Epoch 1090, training loss: 419.0723876953125 = 0.414859414100647 + 50.0 * 8.373150825500488
Epoch 1090, val loss: 0.48230767250061035
Epoch 1100, training loss: 418.8769836425781 = 0.4135960638523102 + 50.0 * 8.369267463684082
Epoch 1100, val loss: 0.48160040378570557
Epoch 1110, training loss: 418.85693359375 = 0.41235435009002686 + 50.0 * 8.368891716003418
Epoch 1110, val loss: 0.4809117913246155
Epoch 1120, training loss: 418.7696838378906 = 0.41117656230926514 + 50.0 * 8.367170333862305
Epoch 1120, val loss: 0.48026806116104126
Epoch 1130, training loss: 418.7301940917969 = 0.41001370549201965 + 50.0 * 8.366403579711914
Epoch 1130, val loss: 0.47966504096984863
Epoch 1140, training loss: 419.17132568359375 = 0.4088570475578308 + 50.0 * 8.375249862670898
Epoch 1140, val loss: 0.47900354862213135
Epoch 1150, training loss: 418.82122802734375 = 0.4076243042945862 + 50.0 * 8.368271827697754
Epoch 1150, val loss: 0.4783003032207489
Epoch 1160, training loss: 418.7217102050781 = 0.4064316749572754 + 50.0 * 8.366305351257324
Epoch 1160, val loss: 0.4777151048183441
Epoch 1170, training loss: 418.6214599609375 = 0.405287504196167 + 50.0 * 8.364323616027832
Epoch 1170, val loss: 0.47708451747894287
Epoch 1180, training loss: 418.6102600097656 = 0.4041677415370941 + 50.0 * 8.364121437072754
Epoch 1180, val loss: 0.4764983654022217
Epoch 1190, training loss: 418.8522033691406 = 0.40304479002952576 + 50.0 * 8.368983268737793
Epoch 1190, val loss: 0.47592681646347046
Epoch 1200, training loss: 418.766845703125 = 0.4018769860267639 + 50.0 * 8.36729907989502
Epoch 1200, val loss: 0.47534307837486267
Epoch 1210, training loss: 418.6180725097656 = 0.40070056915283203 + 50.0 * 8.364347457885742
Epoch 1210, val loss: 0.4746555685997009
Epoch 1220, training loss: 418.5273742675781 = 0.39956018328666687 + 50.0 * 8.362556457519531
Epoch 1220, val loss: 0.47407636046409607
Epoch 1230, training loss: 418.4883728027344 = 0.39845627546310425 + 50.0 * 8.361798286437988
Epoch 1230, val loss: 0.4735611379146576
Epoch 1240, training loss: 418.4503173828125 = 0.397368460893631 + 50.0 * 8.361059188842773
Epoch 1240, val loss: 0.47304007411003113
Epoch 1250, training loss: 419.08819580078125 = 0.39627012610435486 + 50.0 * 8.373838424682617
Epoch 1250, val loss: 0.4724947512149811
Epoch 1260, training loss: 418.66943359375 = 0.39508000016212463 + 50.0 * 8.365487098693848
Epoch 1260, val loss: 0.47177308797836304
Epoch 1270, training loss: 418.3785400390625 = 0.3939272463321686 + 50.0 * 8.359692573547363
Epoch 1270, val loss: 0.47129228711128235
Epoch 1280, training loss: 418.3733215332031 = 0.39284297823905945 + 50.0 * 8.359609603881836
Epoch 1280, val loss: 0.47074052691459656
Epoch 1290, training loss: 418.3232421875 = 0.3917790353298187 + 50.0 * 8.35862922668457
Epoch 1290, val loss: 0.470289021730423
Epoch 1300, training loss: 418.2945556640625 = 0.39072084426879883 + 50.0 * 8.358077049255371
Epoch 1300, val loss: 0.46981045603752136
Epoch 1310, training loss: 418.5107116699219 = 0.3896547853946686 + 50.0 * 8.362421035766602
Epoch 1310, val loss: 0.4693707525730133
Epoch 1320, training loss: 418.3914489746094 = 0.388521283864975 + 50.0 * 8.360058784484863
Epoch 1320, val loss: 0.4688478410243988
Epoch 1330, training loss: 418.3870849609375 = 0.3874051868915558 + 50.0 * 8.359993934631348
Epoch 1330, val loss: 0.468355268239975
Epoch 1340, training loss: 418.2184753417969 = 0.3862982392311096 + 50.0 * 8.356643676757812
Epoch 1340, val loss: 0.46785131096839905
Epoch 1350, training loss: 418.39434814453125 = 0.38521134853363037 + 50.0 * 8.360182762145996
Epoch 1350, val loss: 0.4673515558242798
Epoch 1360, training loss: 418.2342224121094 = 0.38410964608192444 + 50.0 * 8.357002258300781
Epoch 1360, val loss: 0.46694278717041016
Epoch 1370, training loss: 418.1564636230469 = 0.38301903009414673 + 50.0 * 8.35546875
Epoch 1370, val loss: 0.46646609902381897
Epoch 1380, training loss: 418.16851806640625 = 0.3819446563720703 + 50.0 * 8.355731010437012
Epoch 1380, val loss: 0.4660623073577881
Epoch 1390, training loss: 418.2685852050781 = 0.3808521330356598 + 50.0 * 8.357754707336426
Epoch 1390, val loss: 0.4656222462654114
Epoch 1400, training loss: 418.1744384765625 = 0.37971624732017517 + 50.0 * 8.355894088745117
Epoch 1400, val loss: 0.4652155041694641
Epoch 1410, training loss: 418.0816650390625 = 0.37859952449798584 + 50.0 * 8.354061126708984
Epoch 1410, val loss: 0.46475401520729065
Epoch 1420, training loss: 418.0154113769531 = 0.37752005457878113 + 50.0 * 8.352757453918457
Epoch 1420, val loss: 0.46437281370162964
Epoch 1430, training loss: 418.00750732421875 = 0.37645360827445984 + 50.0 * 8.352621078491211
Epoch 1430, val loss: 0.46400007605552673
Epoch 1440, training loss: 418.1302795410156 = 0.37538230419158936 + 50.0 * 8.355097770690918
Epoch 1440, val loss: 0.4635879695415497
Epoch 1450, training loss: 418.0281982421875 = 0.3742561340332031 + 50.0 * 8.353078842163086
Epoch 1450, val loss: 0.4632253348827362
Epoch 1460, training loss: 418.2030334472656 = 0.37313100695610046 + 50.0 * 8.356597900390625
Epoch 1460, val loss: 0.4628572463989258
Epoch 1470, training loss: 417.953369140625 = 0.37199607491493225 + 50.0 * 8.351627349853516
Epoch 1470, val loss: 0.46245166659355164
Epoch 1480, training loss: 417.8984069824219 = 0.37089109420776367 + 50.0 * 8.350550651550293
Epoch 1480, val loss: 0.46212995052337646
Epoch 1490, training loss: 417.8780212402344 = 0.36980852484703064 + 50.0 * 8.350164413452148
Epoch 1490, val loss: 0.46183523535728455
Epoch 1500, training loss: 418.00439453125 = 0.36872684955596924 + 50.0 * 8.352713584899902
Epoch 1500, val loss: 0.4615507125854492
Epoch 1510, training loss: 417.8923645019531 = 0.36759787797927856 + 50.0 * 8.350495338439941
Epoch 1510, val loss: 0.4611760079860687
Epoch 1520, training loss: 417.869384765625 = 0.3664631247520447 + 50.0 * 8.350058555603027
Epoch 1520, val loss: 0.4609038531780243
Epoch 1530, training loss: 417.8051452636719 = 0.3653624653816223 + 50.0 * 8.348795890808105
Epoch 1530, val loss: 0.46064141392707825
Epoch 1540, training loss: 417.9433288574219 = 0.3642728924751282 + 50.0 * 8.351581573486328
Epoch 1540, val loss: 0.4603598415851593
Epoch 1550, training loss: 417.7535095214844 = 0.36313071846961975 + 50.0 * 8.347807884216309
Epoch 1550, val loss: 0.46012261509895325
Epoch 1560, training loss: 417.73406982421875 = 0.36199888586997986 + 50.0 * 8.347441673278809
Epoch 1560, val loss: 0.4598217308521271
Epoch 1570, training loss: 417.7083740234375 = 0.3608996272087097 + 50.0 * 8.346949577331543
Epoch 1570, val loss: 0.45961669087409973
Epoch 1580, training loss: 417.73651123046875 = 0.3598138093948364 + 50.0 * 8.3475341796875
Epoch 1580, val loss: 0.45939692854881287
Epoch 1590, training loss: 417.8202819824219 = 0.3587113916873932 + 50.0 * 8.349231719970703
Epoch 1590, val loss: 0.459201842546463
Epoch 1600, training loss: 417.7195129394531 = 0.357589453458786 + 50.0 * 8.347238540649414
Epoch 1600, val loss: 0.459008127450943
Epoch 1610, training loss: 417.8764343261719 = 0.35648101568222046 + 50.0 * 8.350399017333984
Epoch 1610, val loss: 0.45876264572143555
Epoch 1620, training loss: 417.6813659667969 = 0.35534602403640747 + 50.0 * 8.34652042388916
Epoch 1620, val loss: 0.45845240354537964
Epoch 1630, training loss: 417.60003662109375 = 0.35423117876052856 + 50.0 * 8.344916343688965
Epoch 1630, val loss: 0.45829933881759644
Epoch 1640, training loss: 417.5816650390625 = 0.35313960909843445 + 50.0 * 8.34457015991211
Epoch 1640, val loss: 0.458029568195343
Epoch 1650, training loss: 417.6716003417969 = 0.35205575823783875 + 50.0 * 8.346390724182129
Epoch 1650, val loss: 0.4578673541545868
Epoch 1660, training loss: 417.68939208984375 = 0.3509264290332794 + 50.0 * 8.346769332885742
Epoch 1660, val loss: 0.4576501250267029
Epoch 1670, training loss: 417.5805969238281 = 0.349781334400177 + 50.0 * 8.344615936279297
Epoch 1670, val loss: 0.4574485421180725
Epoch 1680, training loss: 417.54913330078125 = 0.34867510199546814 + 50.0 * 8.344009399414062
Epoch 1680, val loss: 0.4572715163230896
Epoch 1690, training loss: 417.6770324707031 = 0.3475763499736786 + 50.0 * 8.346589088439941
Epoch 1690, val loss: 0.45714935660362244
Epoch 1700, training loss: 417.5004577636719 = 0.3464517295360565 + 50.0 * 8.343079566955566
Epoch 1700, val loss: 0.456856369972229
Epoch 1710, training loss: 417.51153564453125 = 0.3453417718410492 + 50.0 * 8.343323707580566
Epoch 1710, val loss: 0.4567263126373291
Epoch 1720, training loss: 417.54498291015625 = 0.34422820806503296 + 50.0 * 8.344015121459961
Epoch 1720, val loss: 0.45654577016830444
Epoch 1730, training loss: 417.49407958984375 = 0.3431045114994049 + 50.0 * 8.343019485473633
Epoch 1730, val loss: 0.45629653334617615
Epoch 1740, training loss: 417.7107849121094 = 0.3419771194458008 + 50.0 * 8.347375869750977
Epoch 1740, val loss: 0.45620107650756836
Epoch 1750, training loss: 417.6208190917969 = 0.34081554412841797 + 50.0 * 8.345600128173828
Epoch 1750, val loss: 0.4559413194656372
Epoch 1760, training loss: 417.44696044921875 = 0.3396555781364441 + 50.0 * 8.342145919799805
Epoch 1760, val loss: 0.4558050334453583
Epoch 1770, training loss: 417.3919982910156 = 0.3385274112224579 + 50.0 * 8.341069221496582
Epoch 1770, val loss: 0.4556806981563568
Epoch 1780, training loss: 417.3567810058594 = 0.3374083638191223 + 50.0 * 8.340387344360352
Epoch 1780, val loss: 0.45554545521736145
Epoch 1790, training loss: 417.3544921875 = 0.33629029989242554 + 50.0 * 8.340363502502441
Epoch 1790, val loss: 0.45538514852523804
Epoch 1800, training loss: 417.67095947265625 = 0.3351704776287079 + 50.0 * 8.346715927124023
Epoch 1800, val loss: 0.45516639947891235
Epoch 1810, training loss: 417.6887512207031 = 0.33397912979125977 + 50.0 * 8.347095489501953
Epoch 1810, val loss: 0.45522060990333557
Epoch 1820, training loss: 417.35552978515625 = 0.3327764570713043 + 50.0 * 8.340455055236816
Epoch 1820, val loss: 0.4548892378807068
Epoch 1830, training loss: 417.31109619140625 = 0.33161482214927673 + 50.0 * 8.339590072631836
Epoch 1830, val loss: 0.45476725697517395
Epoch 1840, training loss: 417.28271484375 = 0.33047452569007874 + 50.0 * 8.339044570922852
Epoch 1840, val loss: 0.45472413301467896
Epoch 1850, training loss: 417.25640869140625 = 0.3293370306491852 + 50.0 * 8.338541030883789
Epoch 1850, val loss: 0.45458629727363586
Epoch 1860, training loss: 417.6047668457031 = 0.3281998336315155 + 50.0 * 8.345531463623047
Epoch 1860, val loss: 0.4543905258178711
Epoch 1870, training loss: 417.4126892089844 = 0.32699498534202576 + 50.0 * 8.341713905334473
Epoch 1870, val loss: 0.45442360639572144
Epoch 1880, training loss: 417.2964782714844 = 0.32580453157424927 + 50.0 * 8.3394136428833
Epoch 1880, val loss: 0.4541689157485962
Epoch 1890, training loss: 417.35052490234375 = 0.32462939620018005 + 50.0 * 8.3405179977417
Epoch 1890, val loss: 0.4540959596633911
Epoch 1900, training loss: 417.30712890625 = 0.3234313726425171 + 50.0 * 8.33967399597168
Epoch 1900, val loss: 0.45397281646728516
Epoch 1910, training loss: 417.22015380859375 = 0.3222316801548004 + 50.0 * 8.337958335876465
Epoch 1910, val loss: 0.45378538966178894
Epoch 1920, training loss: 417.197021484375 = 0.3210453689098358 + 50.0 * 8.337519645690918
Epoch 1920, val loss: 0.4537626802921295
Epoch 1930, training loss: 417.167724609375 = 0.3198706805706024 + 50.0 * 8.336956977844238
Epoch 1930, val loss: 0.45371586084365845
Epoch 1940, training loss: 417.2917175292969 = 0.31869417428970337 + 50.0 * 8.339460372924805
Epoch 1940, val loss: 0.45357298851013184
Epoch 1950, training loss: 417.1663818359375 = 0.31747329235076904 + 50.0 * 8.3369779586792
Epoch 1950, val loss: 0.45348313450813293
Epoch 1960, training loss: 417.28997802734375 = 0.3162519037723541 + 50.0 * 8.33947467803955
Epoch 1960, val loss: 0.45332518219947815
Epoch 1970, training loss: 417.1500244140625 = 0.3150099813938141 + 50.0 * 8.336700439453125
Epoch 1970, val loss: 0.4532124996185303
Epoch 1980, training loss: 417.0860595703125 = 0.3137889802455902 + 50.0 * 8.335445404052734
Epoch 1980, val loss: 0.4530948996543884
Epoch 1990, training loss: 417.0771179199219 = 0.31257158517837524 + 50.0 * 8.335290908813477
Epoch 1990, val loss: 0.45306187868118286
Epoch 2000, training loss: 417.2791748046875 = 0.31135237216949463 + 50.0 * 8.339356422424316
Epoch 2000, val loss: 0.45299026370048523
Epoch 2010, training loss: 417.1787414550781 = 0.3100610077381134 + 50.0 * 8.337373733520508
Epoch 2010, val loss: 0.45283493399620056
Epoch 2020, training loss: 417.1239013671875 = 0.308773934841156 + 50.0 * 8.336302757263184
Epoch 2020, val loss: 0.45277494192123413
Epoch 2030, training loss: 417.04925537109375 = 0.30749091506004333 + 50.0 * 8.334835052490234
Epoch 2030, val loss: 0.452617883682251
Epoch 2040, training loss: 417.0026550292969 = 0.30623286962509155 + 50.0 * 8.333928108215332
Epoch 2040, val loss: 0.4525867700576782
Epoch 2050, training loss: 417.0238952636719 = 0.3049790859222412 + 50.0 * 8.334378242492676
Epoch 2050, val loss: 0.4525522291660309
Epoch 2060, training loss: 417.2731628417969 = 0.30370986461639404 + 50.0 * 8.339388847351074
Epoch 2060, val loss: 0.452534943819046
Epoch 2070, training loss: 417.0569763183594 = 0.3024035096168518 + 50.0 * 8.335091590881348
Epoch 2070, val loss: 0.4522339701652527
Epoch 2080, training loss: 417.0558776855469 = 0.3011057376861572 + 50.0 * 8.335095405578613
Epoch 2080, val loss: 0.4522080719470978
Epoch 2090, training loss: 417.0686950683594 = 0.29980161786079407 + 50.0 * 8.33537769317627
Epoch 2090, val loss: 0.45214366912841797
Epoch 2100, training loss: 417.0360107421875 = 0.2984982430934906 + 50.0 * 8.334750175476074
Epoch 2100, val loss: 0.45219656825065613
Epoch 2110, training loss: 416.9366149902344 = 0.29718127846717834 + 50.0 * 8.332788467407227
Epoch 2110, val loss: 0.451895534992218
Epoch 2120, training loss: 417.0316162109375 = 0.2958812415599823 + 50.0 * 8.334714889526367
Epoch 2120, val loss: 0.4519329369068146
Epoch 2130, training loss: 416.9969177246094 = 0.2945496439933777 + 50.0 * 8.334047317504883
Epoch 2130, val loss: 0.45180365443229675
Epoch 2140, training loss: 416.90985107421875 = 0.29320386052131653 + 50.0 * 8.332332611083984
Epoch 2140, val loss: 0.45161259174346924
Epoch 2150, training loss: 416.8805236816406 = 0.2918654680252075 + 50.0 * 8.331772804260254
Epoch 2150, val loss: 0.4515906274318695
Epoch 2160, training loss: 416.95428466796875 = 0.29053670167922974 + 50.0 * 8.333274841308594
Epoch 2160, val loss: 0.4515186846256256
Epoch 2170, training loss: 417.0710754394531 = 0.2891816198825836 + 50.0 * 8.335638046264648
Epoch 2170, val loss: 0.45145344734191895
Epoch 2180, training loss: 416.93804931640625 = 0.2878042459487915 + 50.0 * 8.33300495147705
Epoch 2180, val loss: 0.45144134759902954
Epoch 2190, training loss: 416.8587646484375 = 0.28644222021102905 + 50.0 * 8.331446647644043
Epoch 2190, val loss: 0.4514639377593994
Epoch 2200, training loss: 416.8915100097656 = 0.28508448600769043 + 50.0 * 8.332128524780273
Epoch 2200, val loss: 0.45145371556282043
Epoch 2210, training loss: 416.9322204589844 = 0.2837107181549072 + 50.0 * 8.332969665527344
Epoch 2210, val loss: 0.4515049457550049
Epoch 2220, training loss: 416.9035339355469 = 0.28232353925704956 + 50.0 * 8.33242416381836
Epoch 2220, val loss: 0.4515569508075714
Epoch 2230, training loss: 416.88848876953125 = 0.28092899918556213 + 50.0 * 8.332151412963867
Epoch 2230, val loss: 0.4515155255794525
Epoch 2240, training loss: 416.9891052246094 = 0.2795334756374359 + 50.0 * 8.33419132232666
Epoch 2240, val loss: 0.4514440894126892
Epoch 2250, training loss: 416.78448486328125 = 0.27811214327812195 + 50.0 * 8.330127716064453
Epoch 2250, val loss: 0.45160213112831116
Epoch 2260, training loss: 416.7842712402344 = 0.2767021358013153 + 50.0 * 8.330151557922363
Epoch 2260, val loss: 0.45159393548965454
Epoch 2270, training loss: 416.7496032714844 = 0.27530092000961304 + 50.0 * 8.329485893249512
Epoch 2270, val loss: 0.4517500400543213
Epoch 2280, training loss: 416.7248229980469 = 0.2738983929157257 + 50.0 * 8.329018592834473
Epoch 2280, val loss: 0.45186254382133484
Epoch 2290, training loss: 416.8536376953125 = 0.2724981904029846 + 50.0 * 8.331623077392578
Epoch 2290, val loss: 0.45209836959838867
Epoch 2300, training loss: 416.8559265136719 = 0.2710629999637604 + 50.0 * 8.331697463989258
Epoch 2300, val loss: 0.45198148488998413
Epoch 2310, training loss: 416.7068176269531 = 0.26960819959640503 + 50.0 * 8.328743934631348
Epoch 2310, val loss: 0.45221275091171265
Epoch 2320, training loss: 416.69915771484375 = 0.26817020773887634 + 50.0 * 8.328619956970215
Epoch 2320, val loss: 0.4521380662918091
Epoch 2330, training loss: 416.70062255859375 = 0.26674866676330566 + 50.0 * 8.3286771774292
Epoch 2330, val loss: 0.4523891508579254
Epoch 2340, training loss: 416.888427734375 = 0.26533380150794983 + 50.0 * 8.332462310791016
Epoch 2340, val loss: 0.45237186551094055
Epoch 2350, training loss: 416.71875 = 0.26386862993240356 + 50.0 * 8.329097747802734
Epoch 2350, val loss: 0.45257896184921265
Epoch 2360, training loss: 416.7007141113281 = 0.2624143660068512 + 50.0 * 8.328765869140625
Epoch 2360, val loss: 0.4526991844177246
Epoch 2370, training loss: 416.7607727050781 = 0.26097041368484497 + 50.0 * 8.329996109008789
Epoch 2370, val loss: 0.452755331993103
Epoch 2380, training loss: 416.6396789550781 = 0.25951531529426575 + 50.0 * 8.327603340148926
Epoch 2380, val loss: 0.4529702365398407
Epoch 2390, training loss: 416.6732482910156 = 0.25807255506515503 + 50.0 * 8.328303337097168
Epoch 2390, val loss: 0.4530870020389557
Epoch 2400, training loss: 416.8870544433594 = 0.2566468417644501 + 50.0 * 8.332608222961426
Epoch 2400, val loss: 0.4534296691417694
Epoch 2410, training loss: 416.73150634765625 = 0.25516313314437866 + 50.0 * 8.329526901245117
Epoch 2410, val loss: 0.4535205662250519
Epoch 2420, training loss: 416.620849609375 = 0.25369709730148315 + 50.0 * 8.327342987060547
Epoch 2420, val loss: 0.4536149203777313
Epoch 2430, training loss: 416.5797424316406 = 0.25224435329437256 + 50.0 * 8.326549530029297
Epoch 2430, val loss: 0.45387792587280273
Epoch 2440, training loss: 416.57666015625 = 0.25080424547195435 + 50.0 * 8.326517105102539
Epoch 2440, val loss: 0.45407092571258545
Epoch 2450, training loss: 417.0163879394531 = 0.24937132000923157 + 50.0 * 8.33534049987793
Epoch 2450, val loss: 0.4544226825237274
Epoch 2460, training loss: 416.6876220703125 = 0.2478952407836914 + 50.0 * 8.328794479370117
Epoch 2460, val loss: 0.45478346943855286
Epoch 2470, training loss: 416.595947265625 = 0.24641574919223785 + 50.0 * 8.326990127563477
Epoch 2470, val loss: 0.454917311668396
Epoch 2480, training loss: 416.53662109375 = 0.24495089054107666 + 50.0 * 8.325833320617676
Epoch 2480, val loss: 0.45537590980529785
Epoch 2490, training loss: 416.5134582519531 = 0.24348875880241394 + 50.0 * 8.325399398803711
Epoch 2490, val loss: 0.4555971026420593
Epoch 2500, training loss: 416.56756591796875 = 0.24203549325466156 + 50.0 * 8.326510429382324
Epoch 2500, val loss: 0.4559939503669739
Epoch 2510, training loss: 416.6556396484375 = 0.24056221544742584 + 50.0 * 8.328301429748535
Epoch 2510, val loss: 0.4562922716140747
Epoch 2520, training loss: 416.5425109863281 = 0.23907268047332764 + 50.0 * 8.326068878173828
Epoch 2520, val loss: 0.45671820640563965
Epoch 2530, training loss: 416.57916259765625 = 0.23759658634662628 + 50.0 * 8.326830863952637
Epoch 2530, val loss: 0.4570402204990387
Epoch 2540, training loss: 416.52972412109375 = 0.23611345887184143 + 50.0 * 8.325872421264648
Epoch 2540, val loss: 0.45734912157058716
Epoch 2550, training loss: 416.5460510253906 = 0.23463879525661469 + 50.0 * 8.326228141784668
Epoch 2550, val loss: 0.4576495885848999
Epoch 2560, training loss: 416.48968505859375 = 0.23315510153770447 + 50.0 * 8.325130462646484
Epoch 2560, val loss: 0.45821791887283325
Epoch 2570, training loss: 416.55535888671875 = 0.23169367015361786 + 50.0 * 8.326473236083984
Epoch 2570, val loss: 0.4588415324687958
Epoch 2580, training loss: 416.5609130859375 = 0.23020362854003906 + 50.0 * 8.326614379882812
Epoch 2580, val loss: 0.45908498764038086
Epoch 2590, training loss: 416.4983215332031 = 0.22871148586273193 + 50.0 * 8.325392723083496
Epoch 2590, val loss: 0.4594036340713501
Epoch 2600, training loss: 416.4471435546875 = 0.22722646594047546 + 50.0 * 8.324398040771484
Epoch 2600, val loss: 0.4600144922733307
Epoch 2610, training loss: 416.5048522949219 = 0.22574736177921295 + 50.0 * 8.325582504272461
Epoch 2610, val loss: 0.46032288670539856
Epoch 2620, training loss: 416.4173889160156 = 0.22426019608974457 + 50.0 * 8.323862075805664
Epoch 2620, val loss: 0.46086499094963074
Epoch 2630, training loss: 416.377685546875 = 0.22277890145778656 + 50.0 * 8.323098182678223
Epoch 2630, val loss: 0.46130406856536865
Epoch 2640, training loss: 416.6548767089844 = 0.22131939232349396 + 50.0 * 8.3286714553833
Epoch 2640, val loss: 0.4616270661354065
Epoch 2650, training loss: 416.4300842285156 = 0.2198343575000763 + 50.0 * 8.324204444885254
Epoch 2650, val loss: 0.46246233582496643
Epoch 2660, training loss: 416.4243469238281 = 0.21835124492645264 + 50.0 * 8.324119567871094
Epoch 2660, val loss: 0.4627513289451599
Epoch 2670, training loss: 416.4023132324219 = 0.21687866747379303 + 50.0 * 8.323708534240723
Epoch 2670, val loss: 0.4634566605091095
Epoch 2680, training loss: 416.34429931640625 = 0.21540169417858124 + 50.0 * 8.322578430175781
Epoch 2680, val loss: 0.46401339769363403
Epoch 2690, training loss: 416.3462219238281 = 0.21393132209777832 + 50.0 * 8.322646141052246
Epoch 2690, val loss: 0.4646393656730652
Epoch 2700, training loss: 416.5124816894531 = 0.2124728262424469 + 50.0 * 8.326000213623047
Epoch 2700, val loss: 0.4652077257633209
Epoch 2710, training loss: 416.671142578125 = 0.21099425852298737 + 50.0 * 8.329202651977539
Epoch 2710, val loss: 0.4658840298652649
Epoch 2720, training loss: 416.36297607421875 = 0.2094901204109192 + 50.0 * 8.32306957244873
Epoch 2720, val loss: 0.46655401587486267
Epoch 2730, training loss: 416.3020324707031 = 0.2080017775297165 + 50.0 * 8.321880340576172
Epoch 2730, val loss: 0.4671550989151001
Epoch 2740, training loss: 416.2629089355469 = 0.20651820302009583 + 50.0 * 8.321127891540527
Epoch 2740, val loss: 0.4679047465324402
Epoch 2750, training loss: 416.27392578125 = 0.20504793524742126 + 50.0 * 8.321377754211426
Epoch 2750, val loss: 0.46853500604629517
Epoch 2760, training loss: 416.5945129394531 = 0.2035917341709137 + 50.0 * 8.327818870544434
Epoch 2760, val loss: 0.46923375129699707
Epoch 2770, training loss: 416.4458923339844 = 0.2020900547504425 + 50.0 * 8.324875831604004
Epoch 2770, val loss: 0.46980130672454834
Epoch 2780, training loss: 416.2944030761719 = 0.20058096945285797 + 50.0 * 8.321876525878906
Epoch 2780, val loss: 0.4708365797996521
Epoch 2790, training loss: 416.2469177246094 = 0.19907814264297485 + 50.0 * 8.32095718383789
Epoch 2790, val loss: 0.4715113043785095
Epoch 2800, training loss: 416.2247009277344 = 0.19758497178554535 + 50.0 * 8.320542335510254
Epoch 2800, val loss: 0.47230347990989685
Epoch 2810, training loss: 416.5348205566406 = 0.19612926244735718 + 50.0 * 8.326773643493652
Epoch 2810, val loss: 0.4732413589954376
Epoch 2820, training loss: 416.24420166015625 = 0.19463010132312775 + 50.0 * 8.320991516113281
Epoch 2820, val loss: 0.4739549458026886
Epoch 2830, training loss: 416.2247314453125 = 0.1931401789188385 + 50.0 * 8.320631980895996
Epoch 2830, val loss: 0.47486716508865356
Epoch 2840, training loss: 416.20013427734375 = 0.1916608065366745 + 50.0 * 8.320169448852539
Epoch 2840, val loss: 0.4756743013858795
Epoch 2850, training loss: 416.2210388183594 = 0.19020192325115204 + 50.0 * 8.320616722106934
Epoch 2850, val loss: 0.4766674339771271
Epoch 2860, training loss: 416.4722900390625 = 0.18877233564853668 + 50.0 * 8.32567024230957
Epoch 2860, val loss: 0.4776875376701355
Epoch 2870, training loss: 416.26214599609375 = 0.1872948706150055 + 50.0 * 8.321496963500977
Epoch 2870, val loss: 0.4779989421367645
Epoch 2880, training loss: 416.36358642578125 = 0.18583840131759644 + 50.0 * 8.323554992675781
Epoch 2880, val loss: 0.47911837697029114
Epoch 2890, training loss: 416.1990051269531 = 0.18438084423542023 + 50.0 * 8.320292472839355
Epoch 2890, val loss: 0.4801273047924042
Epoch 2900, training loss: 416.1651611328125 = 0.18293610215187073 + 50.0 * 8.319644927978516
Epoch 2900, val loss: 0.4811447262763977
Epoch 2910, training loss: 416.1484069824219 = 0.1815052032470703 + 50.0 * 8.319337844848633
Epoch 2910, val loss: 0.48186856508255005
Epoch 2920, training loss: 416.2552185058594 = 0.18008942902088165 + 50.0 * 8.321502685546875
Epoch 2920, val loss: 0.48284631967544556
Epoch 2930, training loss: 416.1968994140625 = 0.17865724861621857 + 50.0 * 8.320364952087402
Epoch 2930, val loss: 0.4839537441730499
Epoch 2940, training loss: 416.174072265625 = 0.17723961174488068 + 50.0 * 8.319936752319336
Epoch 2940, val loss: 0.48505839705467224
Epoch 2950, training loss: 416.201171875 = 0.1758268028497696 + 50.0 * 8.320507049560547
Epoch 2950, val loss: 0.4859049320220947
Epoch 2960, training loss: 416.2309875488281 = 0.17441534996032715 + 50.0 * 8.321131706237793
Epoch 2960, val loss: 0.4871266782283783
Epoch 2970, training loss: 416.136474609375 = 0.1730072945356369 + 50.0 * 8.319269180297852
Epoch 2970, val loss: 0.4882197976112366
Epoch 2980, training loss: 416.1615295410156 = 0.1716102808713913 + 50.0 * 8.319798469543457
Epoch 2980, val loss: 0.48935872316360474
Epoch 2990, training loss: 416.1104736328125 = 0.17021943628787994 + 50.0 * 8.318804740905762
Epoch 2990, val loss: 0.49051356315612793
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.821917808219178
0.8438020720133305
=== training gcn model ===
Epoch 0, training loss: 530.203369140625 = 1.0922154188156128 + 50.0 * 10.582222938537598
Epoch 0, val loss: 1.0914634466171265
Epoch 10, training loss: 530.1364135742188 = 1.0862808227539062 + 50.0 * 10.581002235412598
Epoch 10, val loss: 1.0854612588882446
Epoch 20, training loss: 529.6123657226562 = 1.0792236328125 + 50.0 * 10.570662498474121
Epoch 20, val loss: 1.078352689743042
Epoch 30, training loss: 526.6864624023438 = 1.0708773136138916 + 50.0 * 10.512311935424805
Epoch 30, val loss: 1.0699766874313354
Epoch 40, training loss: 518.1569213867188 = 1.0635714530944824 + 50.0 * 10.341866493225098
Epoch 40, val loss: 1.0632667541503906
Epoch 50, training loss: 504.27569580078125 = 1.059738039970398 + 50.0 * 10.064319610595703
Epoch 50, val loss: 1.0597877502441406
Epoch 60, training loss: 490.86834716796875 = 1.0561513900756836 + 50.0 * 9.796243667602539
Epoch 60, val loss: 1.0560295581817627
Epoch 70, training loss: 475.13262939453125 = 1.0469926595687866 + 50.0 * 9.481712341308594
Epoch 70, val loss: 1.0463998317718506
Epoch 80, training loss: 468.2215881347656 = 1.0365742444992065 + 50.0 * 9.343700408935547
Epoch 80, val loss: 1.0363645553588867
Epoch 90, training loss: 460.5235290527344 = 1.0301225185394287 + 50.0 * 9.189867973327637
Epoch 90, val loss: 1.0304428339004517
Epoch 100, training loss: 454.7847900390625 = 1.0261123180389404 + 50.0 * 9.075173377990723
Epoch 100, val loss: 1.0265434980392456
Epoch 110, training loss: 449.4720458984375 = 1.0209068059921265 + 50.0 * 8.969022750854492
Epoch 110, val loss: 1.0210180282592773
Epoch 120, training loss: 445.7948303222656 = 1.0141714811325073 + 50.0 * 8.895613670349121
Epoch 120, val loss: 1.0142039060592651
Epoch 130, training loss: 442.10858154296875 = 1.0073195695877075 + 50.0 * 8.822025299072266
Epoch 130, val loss: 1.0076675415039062
Epoch 140, training loss: 439.5592346191406 = 1.0006463527679443 + 50.0 * 8.771171569824219
Epoch 140, val loss: 1.0012824535369873
Epoch 150, training loss: 437.9702453613281 = 0.9925850629806519 + 50.0 * 8.739553451538086
Epoch 150, val loss: 0.9933887720108032
Epoch 160, training loss: 436.5433349609375 = 0.9830919504165649 + 50.0 * 8.711204528808594
Epoch 160, val loss: 0.9841703772544861
Epoch 170, training loss: 435.2842712402344 = 0.9731155633926392 + 50.0 * 8.686223030090332
Epoch 170, val loss: 0.97459876537323
Epoch 180, training loss: 434.1416320800781 = 0.9628162980079651 + 50.0 * 8.663576126098633
Epoch 180, val loss: 0.9647337198257446
Epoch 190, training loss: 433.43572998046875 = 0.9516324996948242 + 50.0 * 8.64968204498291
Epoch 190, val loss: 0.9539822936058044
Epoch 200, training loss: 432.3477478027344 = 0.9396447539329529 + 50.0 * 8.628162384033203
Epoch 200, val loss: 0.942585825920105
Epoch 210, training loss: 431.40899658203125 = 0.9275069236755371 + 50.0 * 8.60962963104248
Epoch 210, val loss: 0.9310682415962219
Epoch 220, training loss: 430.5076599121094 = 0.9149459004402161 + 50.0 * 8.591854095458984
Epoch 220, val loss: 0.9192062020301819
Epoch 230, training loss: 429.5756530761719 = 0.9019556641578674 + 50.0 * 8.573473930358887
Epoch 230, val loss: 0.907034158706665
Epoch 240, training loss: 428.9506530761719 = 0.8886327147483826 + 50.0 * 8.561240196228027
Epoch 240, val loss: 0.8944300413131714
Epoch 250, training loss: 428.27593994140625 = 0.8747362494468689 + 50.0 * 8.54802417755127
Epoch 250, val loss: 0.8814021944999695
Epoch 260, training loss: 427.7496643066406 = 0.8606155514717102 + 50.0 * 8.53778076171875
Epoch 260, val loss: 0.8682237863540649
Epoch 270, training loss: 427.28863525390625 = 0.8464066982269287 + 50.0 * 8.528844833374023
Epoch 270, val loss: 0.8549983501434326
Epoch 280, training loss: 426.8695068359375 = 0.8321666121482849 + 50.0 * 8.520747184753418
Epoch 280, val loss: 0.8417484760284424
Epoch 290, training loss: 426.66204833984375 = 0.8178010582923889 + 50.0 * 8.516884803771973
Epoch 290, val loss: 0.8283870816230774
Epoch 300, training loss: 426.1185302734375 = 0.8035125732421875 + 50.0 * 8.50629997253418
Epoch 300, val loss: 0.8151425719261169
Epoch 310, training loss: 425.90673828125 = 0.7893924713134766 + 50.0 * 8.502346992492676
Epoch 310, val loss: 0.8020944595336914
Epoch 320, training loss: 425.55291748046875 = 0.7753477692604065 + 50.0 * 8.495551109313965
Epoch 320, val loss: 0.7889474034309387
Epoch 330, training loss: 425.2101135253906 = 0.7614301443099976 + 50.0 * 8.488973617553711
Epoch 330, val loss: 0.7760785818099976
Epoch 340, training loss: 424.94085693359375 = 0.7477752566337585 + 50.0 * 8.483861923217773
Epoch 340, val loss: 0.763411819934845
Epoch 350, training loss: 424.6705322265625 = 0.7343202829360962 + 50.0 * 8.478724479675293
Epoch 350, val loss: 0.7509546279907227
Epoch 360, training loss: 424.6615905761719 = 0.7211246490478516 + 50.0 * 8.478809356689453
Epoch 360, val loss: 0.7386913299560547
Epoch 370, training loss: 424.29119873046875 = 0.7080254554748535 + 50.0 * 8.471663475036621
Epoch 370, val loss: 0.7265684604644775
Epoch 380, training loss: 424.0444030761719 = 0.6953983902931213 + 50.0 * 8.46697998046875
Epoch 380, val loss: 0.7150020599365234
Epoch 390, training loss: 423.89093017578125 = 0.6833075284957886 + 50.0 * 8.464152336120605
Epoch 390, val loss: 0.7038929462432861
Epoch 400, training loss: 423.6741638183594 = 0.6715250611305237 + 50.0 * 8.460052490234375
Epoch 400, val loss: 0.6931890845298767
Epoch 410, training loss: 423.3985290527344 = 0.6603821516036987 + 50.0 * 8.454763412475586
Epoch 410, val loss: 0.6830164194107056
Epoch 420, training loss: 423.190185546875 = 0.6498004198074341 + 50.0 * 8.450807571411133
Epoch 420, val loss: 0.6734508872032166
Epoch 430, training loss: 423.49493408203125 = 0.6396985054016113 + 50.0 * 8.457104682922363
Epoch 430, val loss: 0.6643617153167725
Epoch 440, training loss: 422.9474182128906 = 0.6301020383834839 + 50.0 * 8.446346282958984
Epoch 440, val loss: 0.6557671427726746
Epoch 450, training loss: 422.6894836425781 = 0.6212712526321411 + 50.0 * 8.441364288330078
Epoch 450, val loss: 0.6479320526123047
Epoch 460, training loss: 422.4806823730469 = 0.6130328178405762 + 50.0 * 8.437353134155273
Epoch 460, val loss: 0.6406590342521667
Epoch 470, training loss: 422.5379638671875 = 0.6053188443183899 + 50.0 * 8.438652992248535
Epoch 470, val loss: 0.6338924765586853
Epoch 480, training loss: 422.1781921386719 = 0.5979422330856323 + 50.0 * 8.431605339050293
Epoch 480, val loss: 0.6274445652961731
Epoch 490, training loss: 422.0682678222656 = 0.5911579132080078 + 50.0 * 8.429542541503906
Epoch 490, val loss: 0.6216230988502502
Epoch 500, training loss: 421.8505859375 = 0.5849034190177917 + 50.0 * 8.425313949584961
Epoch 500, val loss: 0.6162804961204529
Epoch 510, training loss: 421.92974853515625 = 0.5790486931800842 + 50.0 * 8.427014350891113
Epoch 510, val loss: 0.6113591194152832
Epoch 520, training loss: 421.7973937988281 = 0.5734363794326782 + 50.0 * 8.424479484558105
Epoch 520, val loss: 0.606592059135437
Epoch 530, training loss: 421.53021240234375 = 0.5682549476623535 + 50.0 * 8.419239044189453
Epoch 530, val loss: 0.6023173928260803
Epoch 540, training loss: 421.38360595703125 = 0.5634724497795105 + 50.0 * 8.416402816772461
Epoch 540, val loss: 0.5984002351760864
Epoch 550, training loss: 421.25128173828125 = 0.5589926242828369 + 50.0 * 8.413846015930176
Epoch 550, val loss: 0.5947909951210022
Epoch 560, training loss: 421.1484680175781 = 0.5547780990600586 + 50.0 * 8.411873817443848
Epoch 560, val loss: 0.591432511806488
Epoch 570, training loss: 421.18048095703125 = 0.5506860613822937 + 50.0 * 8.412595748901367
Epoch 570, val loss: 0.5881415009498596
Epoch 580, training loss: 421.022216796875 = 0.5467069745063782 + 50.0 * 8.409510612487793
Epoch 580, val loss: 0.5850369930267334
Epoch 590, training loss: 420.9291076660156 = 0.543117880821228 + 50.0 * 8.407719612121582
Epoch 590, val loss: 0.5822843313217163
Epoch 600, training loss: 420.79559326171875 = 0.5397400259971619 + 50.0 * 8.40511703491211
Epoch 600, val loss: 0.5797218084335327
Epoch 610, training loss: 420.7109375 = 0.5365164875984192 + 50.0 * 8.403488159179688
Epoch 610, val loss: 0.5773149132728577
Epoch 620, training loss: 420.6329345703125 = 0.5334392786026001 + 50.0 * 8.401989936828613
Epoch 620, val loss: 0.575046718120575
Epoch 630, training loss: 421.1407470703125 = 0.5304532051086426 + 50.0 * 8.412205696105957
Epoch 630, val loss: 0.5728065371513367
Epoch 640, training loss: 420.5741271972656 = 0.527466893196106 + 50.0 * 8.400933265686035
Epoch 640, val loss: 0.5706343054771423
Epoch 650, training loss: 420.4358215332031 = 0.524695098400116 + 50.0 * 8.398222923278809
Epoch 650, val loss: 0.5686402320861816
Epoch 660, training loss: 420.34356689453125 = 0.5220503211021423 + 50.0 * 8.396430015563965
Epoch 660, val loss: 0.5667625069618225
Epoch 670, training loss: 420.307861328125 = 0.5195143222808838 + 50.0 * 8.395767211914062
Epoch 670, val loss: 0.565001368522644
Epoch 680, training loss: 420.2458801269531 = 0.5169565677642822 + 50.0 * 8.394577980041504
Epoch 680, val loss: 0.5631784200668335
Epoch 690, training loss: 420.2690124511719 = 0.5144591927528381 + 50.0 * 8.39509105682373
Epoch 690, val loss: 0.5614581108093262
Epoch 700, training loss: 420.1257629394531 = 0.5121256709098816 + 50.0 * 8.39227294921875
Epoch 700, val loss: 0.5598620772361755
Epoch 710, training loss: 420.0260314941406 = 0.5098682045936584 + 50.0 * 8.390323638916016
Epoch 710, val loss: 0.5583627820014954
Epoch 720, training loss: 419.9709777832031 = 0.50767982006073 + 50.0 * 8.389266014099121
Epoch 720, val loss: 0.5569230318069458
Epoch 730, training loss: 420.2878723144531 = 0.5055137276649475 + 50.0 * 8.395647048950195
Epoch 730, val loss: 0.5554769039154053
Epoch 740, training loss: 420.0281066894531 = 0.5033151507377625 + 50.0 * 8.390495300292969
Epoch 740, val loss: 0.5540393590927124
Epoch 750, training loss: 419.8533020019531 = 0.501197099685669 + 50.0 * 8.387042045593262
Epoch 750, val loss: 0.5526672601699829
Epoch 760, training loss: 419.77227783203125 = 0.49916937947273254 + 50.0 * 8.385461807250977
Epoch 760, val loss: 0.5513960719108582
Epoch 770, training loss: 419.742919921875 = 0.4971884787082672 + 50.0 * 8.38491439819336
Epoch 770, val loss: 0.5501629710197449
Epoch 780, training loss: 419.754150390625 = 0.49522316455841064 + 50.0 * 8.385178565979004
Epoch 780, val loss: 0.5489428043365479
Epoch 790, training loss: 419.6393737792969 = 0.49326974153518677 + 50.0 * 8.382922172546387
Epoch 790, val loss: 0.547716498374939
Epoch 800, training loss: 419.8371276855469 = 0.49134063720703125 + 50.0 * 8.386916160583496
Epoch 800, val loss: 0.5465360879898071
Epoch 810, training loss: 419.5693359375 = 0.4893893599510193 + 50.0 * 8.381599426269531
Epoch 810, val loss: 0.545307993888855
Epoch 820, training loss: 419.4994812011719 = 0.4875296652317047 + 50.0 * 8.380239486694336
Epoch 820, val loss: 0.544174313545227
Epoch 830, training loss: 419.4638977050781 = 0.48571014404296875 + 50.0 * 8.379563331604004
Epoch 830, val loss: 0.5431004762649536
Epoch 840, training loss: 419.70343017578125 = 0.4838963449001312 + 50.0 * 8.384390830993652
Epoch 840, val loss: 0.5419877171516418
Epoch 850, training loss: 419.4801940917969 = 0.4820449650287628 + 50.0 * 8.379962921142578
Epoch 850, val loss: 0.5408247709274292
Epoch 860, training loss: 419.3666687011719 = 0.48025014996528625 + 50.0 * 8.377728462219238
Epoch 860, val loss: 0.5397781133651733
Epoch 870, training loss: 419.375732421875 = 0.47849348187446594 + 50.0 * 8.377944946289062
Epoch 870, val loss: 0.53867107629776
Epoch 880, training loss: 419.39727783203125 = 0.47671833634376526 + 50.0 * 8.378411293029785
Epoch 880, val loss: 0.5376030802726746
Epoch 890, training loss: 419.324951171875 = 0.4749476909637451 + 50.0 * 8.376999855041504
Epoch 890, val loss: 0.5365527272224426
Epoch 900, training loss: 419.1917419433594 = 0.4732201099395752 + 50.0 * 8.374370574951172
Epoch 900, val loss: 0.5354928970336914
Epoch 910, training loss: 419.1650695800781 = 0.47153592109680176 + 50.0 * 8.373870849609375
Epoch 910, val loss: 0.5345016121864319
Epoch 920, training loss: 419.24322509765625 = 0.4698600471019745 + 50.0 * 8.375467300415039
Epoch 920, val loss: 0.5334879755973816
Epoch 930, training loss: 419.2065124511719 = 0.46814364194869995 + 50.0 * 8.374767303466797
Epoch 930, val loss: 0.5324706435203552
Epoch 940, training loss: 419.06268310546875 = 0.4664379954338074 + 50.0 * 8.371925354003906
Epoch 940, val loss: 0.5313634872436523
Epoch 950, training loss: 419.00762939453125 = 0.46478474140167236 + 50.0 * 8.370857238769531
Epoch 950, val loss: 0.5304215550422668
Epoch 960, training loss: 419.23138427734375 = 0.46315592527389526 + 50.0 * 8.375364303588867
Epoch 960, val loss: 0.5293936133384705
Epoch 970, training loss: 419.0721130371094 = 0.4614672064781189 + 50.0 * 8.372213363647461
Epoch 970, val loss: 0.528409481048584
Epoch 980, training loss: 418.9010314941406 = 0.4598286747932434 + 50.0 * 8.368824005126953
Epoch 980, val loss: 0.5274345874786377
Epoch 990, training loss: 418.9017028808594 = 0.45823460817337036 + 50.0 * 8.36886978149414
Epoch 990, val loss: 0.5265005826950073
Epoch 1000, training loss: 419.1850891113281 = 0.45662349462509155 + 50.0 * 8.374568939208984
Epoch 1000, val loss: 0.5255329012870789
Epoch 1010, training loss: 418.8802185058594 = 0.45496827363967896 + 50.0 * 8.368505477905273
Epoch 1010, val loss: 0.5244817733764648
Epoch 1020, training loss: 418.76470947265625 = 0.45339077711105347 + 50.0 * 8.366226196289062
Epoch 1020, val loss: 0.5235664248466492
Epoch 1030, training loss: 418.718994140625 = 0.4518498480319977 + 50.0 * 8.36534309387207
Epoch 1030, val loss: 0.5226722955703735
Epoch 1040, training loss: 418.7977600097656 = 0.450323224067688 + 50.0 * 8.366949081420898
Epoch 1040, val loss: 0.5218414068222046
Epoch 1050, training loss: 418.7747802734375 = 0.44874462485313416 + 50.0 * 8.366520881652832
Epoch 1050, val loss: 0.5208001136779785
Epoch 1060, training loss: 418.7899475097656 = 0.44716858863830566 + 50.0 * 8.36685562133789
Epoch 1060, val loss: 0.5199141502380371
Epoch 1070, training loss: 418.6042785644531 = 0.44560837745666504 + 50.0 * 8.363173484802246
Epoch 1070, val loss: 0.5189643502235413
Epoch 1080, training loss: 418.5380554199219 = 0.4441094696521759 + 50.0 * 8.361878395080566
Epoch 1080, val loss: 0.5180862545967102
Epoch 1090, training loss: 418.5189514160156 = 0.44263750314712524 + 50.0 * 8.361526489257812
Epoch 1090, val loss: 0.5172598958015442
Epoch 1100, training loss: 418.54217529296875 = 0.4411684274673462 + 50.0 * 8.362020492553711
Epoch 1100, val loss: 0.5164130330085754
Epoch 1110, training loss: 418.79461669921875 = 0.4396580457687378 + 50.0 * 8.367098808288574
Epoch 1110, val loss: 0.515527606010437
Epoch 1120, training loss: 418.4888610839844 = 0.4380832016468048 + 50.0 * 8.361015319824219
Epoch 1120, val loss: 0.5145158171653748
Epoch 1130, training loss: 418.44610595703125 = 0.4365767538547516 + 50.0 * 8.360190391540527
Epoch 1130, val loss: 0.5136291980743408
Epoch 1140, training loss: 418.3768310546875 = 0.4351283311843872 + 50.0 * 8.358834266662598
Epoch 1140, val loss: 0.5127986669540405
Epoch 1150, training loss: 418.3450012207031 = 0.43370407819747925 + 50.0 * 8.35822582244873
Epoch 1150, val loss: 0.511993944644928
Epoch 1160, training loss: 418.5596923828125 = 0.4322644770145416 + 50.0 * 8.362548828125
Epoch 1160, val loss: 0.5111899971961975
Epoch 1170, training loss: 418.51617431640625 = 0.4307853579521179 + 50.0 * 8.36170768737793
Epoch 1170, val loss: 0.5101923942565918
Epoch 1180, training loss: 418.23468017578125 = 0.4292934238910675 + 50.0 * 8.356107711791992
Epoch 1180, val loss: 0.5093918442726135
Epoch 1190, training loss: 418.2247009277344 = 0.4278668463230133 + 50.0 * 8.355937004089355
Epoch 1190, val loss: 0.5085562467575073
Epoch 1200, training loss: 418.1903076171875 = 0.4264642000198364 + 50.0 * 8.355277061462402
Epoch 1200, val loss: 0.5077556371688843
Epoch 1210, training loss: 418.1797180175781 = 0.4250670373439789 + 50.0 * 8.355093002319336
Epoch 1210, val loss: 0.5069324374198914
Epoch 1220, training loss: 418.4454040527344 = 0.4236619472503662 + 50.0 * 8.360434532165527
Epoch 1220, val loss: 0.5060598254203796
Epoch 1230, training loss: 418.2695007324219 = 0.42216378450393677 + 50.0 * 8.35694694519043
Epoch 1230, val loss: 0.5053027868270874
Epoch 1240, training loss: 418.0982971191406 = 0.42072418332099915 + 50.0 * 8.353551864624023
Epoch 1240, val loss: 0.5044708847999573
Epoch 1250, training loss: 418.0557861328125 = 0.41934719681739807 + 50.0 * 8.352728843688965
Epoch 1250, val loss: 0.5036472678184509
Epoch 1260, training loss: 418.03131103515625 = 0.4179934859275818 + 50.0 * 8.352266311645508
Epoch 1260, val loss: 0.5029639005661011
Epoch 1270, training loss: 418.1866149902344 = 0.41665154695510864 + 50.0 * 8.355399131774902
Epoch 1270, val loss: 0.5021771788597107
Epoch 1280, training loss: 418.1227722167969 = 0.4152233600616455 + 50.0 * 8.354150772094727
Epoch 1280, val loss: 0.5015291571617126
Epoch 1290, training loss: 417.9981994628906 = 0.4138270914554596 + 50.0 * 8.35168743133545
Epoch 1290, val loss: 0.5006925463676453
Epoch 1300, training loss: 417.9449157714844 = 0.4124801456928253 + 50.0 * 8.350648880004883
Epoch 1300, val loss: 0.4999435544013977
Epoch 1310, training loss: 417.9932556152344 = 0.41115522384643555 + 50.0 * 8.351641654968262
Epoch 1310, val loss: 0.4993125796318054
Epoch 1320, training loss: 418.06329345703125 = 0.40980392694473267 + 50.0 * 8.353070259094238
Epoch 1320, val loss: 0.4985933005809784
Epoch 1330, training loss: 418.0845031738281 = 0.4084443747997284 + 50.0 * 8.353521347045898
Epoch 1330, val loss: 0.4979373812675476
Epoch 1340, training loss: 417.8924255371094 = 0.4071018695831299 + 50.0 * 8.349706649780273
Epoch 1340, val loss: 0.4972878098487854
Epoch 1350, training loss: 417.8625183105469 = 0.4057968854904175 + 50.0 * 8.34913444519043
Epoch 1350, val loss: 0.49675580859184265
Epoch 1360, training loss: 417.8243713378906 = 0.40450650453567505 + 50.0 * 8.348397254943848
Epoch 1360, val loss: 0.4961071014404297
Epoch 1370, training loss: 418.0516052246094 = 0.4032100439071655 + 50.0 * 8.352968215942383
Epoch 1370, val loss: 0.4955589473247528
Epoch 1380, training loss: 417.8285827636719 = 0.4018760621547699 + 50.0 * 8.348533630371094
Epoch 1380, val loss: 0.49493202567100525
Epoch 1390, training loss: 417.7809143066406 = 0.4005868136882782 + 50.0 * 8.347606658935547
Epoch 1390, val loss: 0.49431389570236206
Epoch 1400, training loss: 417.72900390625 = 0.39931192994117737 + 50.0 * 8.346593856811523
Epoch 1400, val loss: 0.4938330054283142
Epoch 1410, training loss: 418.0572204589844 = 0.39804384112358093 + 50.0 * 8.35318374633789
Epoch 1410, val loss: 0.4933248460292816
Epoch 1420, training loss: 417.8795471191406 = 0.3966975510120392 + 50.0 * 8.34965705871582
Epoch 1420, val loss: 0.4926740527153015
Epoch 1430, training loss: 417.759521484375 = 0.39539551734924316 + 50.0 * 8.347282409667969
Epoch 1430, val loss: 0.49221277236938477
Epoch 1440, training loss: 417.7137756347656 = 0.3941212594509125 + 50.0 * 8.346392631530762
Epoch 1440, val loss: 0.49170002341270447
Epoch 1450, training loss: 417.65545654296875 = 0.3928670883178711 + 50.0 * 8.34525203704834
Epoch 1450, val loss: 0.4911804497241974
Epoch 1460, training loss: 417.7053527832031 = 0.39163368940353394 + 50.0 * 8.346274375915527
Epoch 1460, val loss: 0.4906666874885559
Epoch 1470, training loss: 417.62860107421875 = 0.3903740346431732 + 50.0 * 8.344764709472656
Epoch 1470, val loss: 0.49026578664779663
Epoch 1480, training loss: 417.650390625 = 0.38912633061408997 + 50.0 * 8.34522533416748
Epoch 1480, val loss: 0.4898569583892822
Epoch 1490, training loss: 417.7162780761719 = 0.38788512349128723 + 50.0 * 8.34656810760498
Epoch 1490, val loss: 0.48932895064353943
Epoch 1500, training loss: 417.578369140625 = 0.386627197265625 + 50.0 * 8.34383487701416
Epoch 1500, val loss: 0.48896071314811707
Epoch 1510, training loss: 417.7738952636719 = 0.3853948712348938 + 50.0 * 8.347769737243652
Epoch 1510, val loss: 0.4884781837463379
Epoch 1520, training loss: 417.5728454589844 = 0.38414108753204346 + 50.0 * 8.34377384185791
Epoch 1520, val loss: 0.4882010519504547
Epoch 1530, training loss: 417.5101013183594 = 0.3829236626625061 + 50.0 * 8.342543601989746
Epoch 1530, val loss: 0.4878009855747223
Epoch 1540, training loss: 417.54656982421875 = 0.38171616196632385 + 50.0 * 8.343297004699707
Epoch 1540, val loss: 0.487531453371048
Epoch 1550, training loss: 417.5801086425781 = 0.38048648834228516 + 50.0 * 8.343992233276367
Epoch 1550, val loss: 0.4871892035007477
Epoch 1560, training loss: 417.50579833984375 = 0.3792544901371002 + 50.0 * 8.342531204223633
Epoch 1560, val loss: 0.48679882287979126
Epoch 1570, training loss: 417.521728515625 = 0.3780432641506195 + 50.0 * 8.342873573303223
Epoch 1570, val loss: 0.4865458309650421
Epoch 1580, training loss: 417.6126403808594 = 0.37681370973587036 + 50.0 * 8.344717025756836
Epoch 1580, val loss: 0.4862622022628784
Epoch 1590, training loss: 417.4607849121094 = 0.375584214925766 + 50.0 * 8.341704368591309
Epoch 1590, val loss: 0.48591092228889465
Epoch 1600, training loss: 417.40191650390625 = 0.37438225746154785 + 50.0 * 8.340550422668457
Epoch 1600, val loss: 0.48566895723342896
Epoch 1610, training loss: 417.3818054199219 = 0.3731939494609833 + 50.0 * 8.340171813964844
Epoch 1610, val loss: 0.48540306091308594
Epoch 1620, training loss: 417.5739440917969 = 0.3719986379146576 + 50.0 * 8.344038963317871
Epoch 1620, val loss: 0.48514702916145325
Epoch 1630, training loss: 417.41705322265625 = 0.370758056640625 + 50.0 * 8.340926170349121
Epoch 1630, val loss: 0.4850003719329834
Epoch 1640, training loss: 417.473876953125 = 0.3695254623889923 + 50.0 * 8.342086791992188
Epoch 1640, val loss: 0.4846097528934479
Epoch 1650, training loss: 417.3952331542969 = 0.36829471588134766 + 50.0 * 8.34053897857666
Epoch 1650, val loss: 0.4843871295452118
Epoch 1660, training loss: 417.3133239746094 = 0.36708882451057434 + 50.0 * 8.338924407958984
Epoch 1660, val loss: 0.4840432405471802
Epoch 1670, training loss: 417.27618408203125 = 0.3658934235572815 + 50.0 * 8.338205337524414
Epoch 1670, val loss: 0.48385822772979736
Epoch 1680, training loss: 417.31756591796875 = 0.3647046983242035 + 50.0 * 8.339056968688965
Epoch 1680, val loss: 0.4836602509021759
Epoch 1690, training loss: 417.4549255371094 = 0.3634773790836334 + 50.0 * 8.341829299926758
Epoch 1690, val loss: 0.4833657145500183
Epoch 1700, training loss: 417.2698974609375 = 0.3622298836708069 + 50.0 * 8.338152885437012
Epoch 1700, val loss: 0.4831414222717285
Epoch 1710, training loss: 417.2169189453125 = 0.3610101640224457 + 50.0 * 8.337118148803711
Epoch 1710, val loss: 0.4829419255256653
Epoch 1720, training loss: 417.34588623046875 = 0.35980868339538574 + 50.0 * 8.3397216796875
Epoch 1720, val loss: 0.4828050136566162
Epoch 1730, training loss: 417.2579345703125 = 0.3585783839225769 + 50.0 * 8.337986946105957
Epoch 1730, val loss: 0.4824434220790863
Epoch 1740, training loss: 417.2677001953125 = 0.35734033584594727 + 50.0 * 8.338207244873047
Epoch 1740, val loss: 0.4823235273361206
Epoch 1750, training loss: 417.1842041015625 = 0.3561195433139801 + 50.0 * 8.336562156677246
Epoch 1750, val loss: 0.48221224546432495
Epoch 1760, training loss: 417.1451110839844 = 0.35491231083869934 + 50.0 * 8.335803985595703
Epoch 1760, val loss: 0.48210036754608154
Epoch 1770, training loss: 417.13446044921875 = 0.3537018299102783 + 50.0 * 8.335615158081055
Epoch 1770, val loss: 0.4819384217262268
Epoch 1780, training loss: 417.3875732421875 = 0.35247913002967834 + 50.0 * 8.340702056884766
Epoch 1780, val loss: 0.4817340672016144
Epoch 1790, training loss: 417.374267578125 = 0.3512033522129059 + 50.0 * 8.340461730957031
Epoch 1790, val loss: 0.48161566257476807
Epoch 1800, training loss: 417.1343994140625 = 0.3499008119106293 + 50.0 * 8.335689544677734
Epoch 1800, val loss: 0.48149093985557556
Epoch 1810, training loss: 417.08575439453125 = 0.3486606478691101 + 50.0 * 8.334741592407227
Epoch 1810, val loss: 0.48140472173690796
Epoch 1820, training loss: 417.0636291503906 = 0.3474370539188385 + 50.0 * 8.33432388305664
Epoch 1820, val loss: 0.481317937374115
Epoch 1830, training loss: 417.037109375 = 0.34620872139930725 + 50.0 * 8.333817481994629
Epoch 1830, val loss: 0.4812150001525879
Epoch 1840, training loss: 417.06878662109375 = 0.34497708082199097 + 50.0 * 8.334476470947266
Epoch 1840, val loss: 0.4811723530292511
Epoch 1850, training loss: 417.33447265625 = 0.34371501207351685 + 50.0 * 8.339815139770508
Epoch 1850, val loss: 0.48103567957878113
Epoch 1860, training loss: 417.0461120605469 = 0.34241345524787903 + 50.0 * 8.334074020385742
Epoch 1860, val loss: 0.4808858633041382
Epoch 1870, training loss: 417.01409912109375 = 0.34114930033683777 + 50.0 * 8.33345890045166
Epoch 1870, val loss: 0.48077085614204407
Epoch 1880, training loss: 417.00518798828125 = 0.33989381790161133 + 50.0 * 8.333305358886719
Epoch 1880, val loss: 0.4806963801383972
Epoch 1890, training loss: 417.3944396972656 = 0.3386460244655609 + 50.0 * 8.341115951538086
Epoch 1890, val loss: 0.48054659366607666
Epoch 1900, training loss: 417.1612243652344 = 0.3373076319694519 + 50.0 * 8.336478233337402
Epoch 1900, val loss: 0.4807281196117401
Epoch 1910, training loss: 416.97808837890625 = 0.3360014855861664 + 50.0 * 8.332841873168945
Epoch 1910, val loss: 0.48057541251182556
Epoch 1920, training loss: 416.92120361328125 = 0.33470213413238525 + 50.0 * 8.331729888916016
Epoch 1920, val loss: 0.4806184768676758
Epoch 1930, training loss: 416.9101257324219 = 0.33340850472450256 + 50.0 * 8.331534385681152
Epoch 1930, val loss: 0.4806758165359497
Epoch 1940, training loss: 417.1242370605469 = 0.3321154713630676 + 50.0 * 8.33584213256836
Epoch 1940, val loss: 0.48079898953437805
Epoch 1950, training loss: 416.89117431640625 = 0.330758661031723 + 50.0 * 8.331208229064941
Epoch 1950, val loss: 0.48062944412231445
Epoch 1960, training loss: 416.89654541015625 = 0.32941943407058716 + 50.0 * 8.331342697143555
Epoch 1960, val loss: 0.4807232618331909
Epoch 1970, training loss: 416.8816833496094 = 0.32809460163116455 + 50.0 * 8.331071853637695
Epoch 1970, val loss: 0.48071181774139404
Epoch 1980, training loss: 416.8879699707031 = 0.32677456736564636 + 50.0 * 8.331223487854004
Epoch 1980, val loss: 0.4808954894542694
Epoch 1990, training loss: 417.228271484375 = 0.3254412114620209 + 50.0 * 8.338056564331055
Epoch 1990, val loss: 0.4809481203556061
Epoch 2000, training loss: 416.89569091796875 = 0.3240615129470825 + 50.0 * 8.331432342529297
Epoch 2000, val loss: 0.4808413088321686
Epoch 2010, training loss: 416.8086242675781 = 0.3227057158946991 + 50.0 * 8.329718589782715
Epoch 2010, val loss: 0.4809480309486389
Epoch 2020, training loss: 416.7965087890625 = 0.321363627910614 + 50.0 * 8.329503059387207
Epoch 2020, val loss: 0.4810926616191864
Epoch 2030, training loss: 416.7843017578125 = 0.3200170695781708 + 50.0 * 8.329285621643066
Epoch 2030, val loss: 0.48118266463279724
Epoch 2040, training loss: 416.860107421875 = 0.31866779923439026 + 50.0 * 8.330828666687012
Epoch 2040, val loss: 0.4813438355922699
Epoch 2050, training loss: 416.84918212890625 = 0.3172765374183655 + 50.0 * 8.33063793182373
Epoch 2050, val loss: 0.4814487099647522
Epoch 2060, training loss: 416.8355712890625 = 0.31588438153266907 + 50.0 * 8.33039379119873
Epoch 2060, val loss: 0.48148804903030396
Epoch 2070, training loss: 416.7696533203125 = 0.31450584530830383 + 50.0 * 8.329102516174316
Epoch 2070, val loss: 0.48160848021507263
Epoch 2080, training loss: 416.7315368652344 = 0.31313982605934143 + 50.0 * 8.328368186950684
Epoch 2080, val loss: 0.4817904233932495
Epoch 2090, training loss: 416.7405700683594 = 0.31176814436912537 + 50.0 * 8.32857608795166
Epoch 2090, val loss: 0.48200297355651855
Epoch 2100, training loss: 417.25494384765625 = 0.31039750576019287 + 50.0 * 8.33889102935791
Epoch 2100, val loss: 0.4822375178337097
Epoch 2110, training loss: 416.8905029296875 = 0.3089682459831238 + 50.0 * 8.33163070678711
Epoch 2110, val loss: 0.4822007119655609
Epoch 2120, training loss: 416.7447509765625 = 0.3075611889362335 + 50.0 * 8.328743934631348
Epoch 2120, val loss: 0.4825640022754669
Epoch 2130, training loss: 416.764404296875 = 0.30617257952690125 + 50.0 * 8.329164505004883
Epoch 2130, val loss: 0.48285409808158875
Epoch 2140, training loss: 416.7991943359375 = 0.30477097630500793 + 50.0 * 8.329888343811035
Epoch 2140, val loss: 0.4830200672149658
Epoch 2150, training loss: 416.7248840332031 = 0.3033653497695923 + 50.0 * 8.32843017578125
Epoch 2150, val loss: 0.48304110765457153
Epoch 2160, training loss: 416.6711120605469 = 0.3019607365131378 + 50.0 * 8.327383041381836
Epoch 2160, val loss: 0.48344412446022034
Epoch 2170, training loss: 416.68017578125 = 0.30056580901145935 + 50.0 * 8.327591896057129
Epoch 2170, val loss: 0.48360300064086914
Epoch 2180, training loss: 416.9058837890625 = 0.29916849732398987 + 50.0 * 8.332134246826172
Epoch 2180, val loss: 0.4838479459285736
Epoch 2190, training loss: 416.70068359375 = 0.29773563146591187 + 50.0 * 8.328059196472168
Epoch 2190, val loss: 0.48420849442481995
Epoch 2200, training loss: 416.6289978027344 = 0.2963218688964844 + 50.0 * 8.326653480529785
Epoch 2200, val loss: 0.48439836502075195
Epoch 2210, training loss: 416.6352844238281 = 0.29491040110588074 + 50.0 * 8.326807022094727
Epoch 2210, val loss: 0.484695166349411
Epoch 2220, training loss: 416.9472351074219 = 0.29351624846458435 + 50.0 * 8.333074569702148
Epoch 2220, val loss: 0.48482516407966614
Epoch 2230, training loss: 416.6789855957031 = 0.2920636534690857 + 50.0 * 8.327738761901855
Epoch 2230, val loss: 0.48534804582595825
Epoch 2240, training loss: 416.5684814453125 = 0.29063427448272705 + 50.0 * 8.325556755065918
Epoch 2240, val loss: 0.4856702387332916
Epoch 2250, training loss: 416.55145263671875 = 0.28922417759895325 + 50.0 * 8.325244903564453
Epoch 2250, val loss: 0.4859903156757355
Epoch 2260, training loss: 416.550048828125 = 0.2878115475177765 + 50.0 * 8.325244903564453
Epoch 2260, val loss: 0.4864526391029358
Epoch 2270, training loss: 416.9738464355469 = 0.286409467458725 + 50.0 * 8.333748817443848
Epoch 2270, val loss: 0.48695263266563416
Epoch 2280, training loss: 416.6534118652344 = 0.28495025634765625 + 50.0 * 8.327369689941406
Epoch 2280, val loss: 0.4869605004787445
Epoch 2290, training loss: 416.590087890625 = 0.2835107147693634 + 50.0 * 8.326131820678711
Epoch 2290, val loss: 0.4875394105911255
Epoch 2300, training loss: 416.5447998046875 = 0.28208887577056885 + 50.0 * 8.325254440307617
Epoch 2300, val loss: 0.48785680532455444
Epoch 2310, training loss: 416.73272705078125 = 0.28069427609443665 + 50.0 * 8.32904052734375
Epoch 2310, val loss: 0.4885222017765045
Epoch 2320, training loss: 416.49993896484375 = 0.27922919392585754 + 50.0 * 8.324414253234863
Epoch 2320, val loss: 0.488463819026947
Epoch 2330, training loss: 416.509765625 = 0.27780017256736755 + 50.0 * 8.324639320373535
Epoch 2330, val loss: 0.4890330135822296
Epoch 2340, training loss: 416.5332336425781 = 0.276376336812973 + 50.0 * 8.3251371383667
Epoch 2340, val loss: 0.48933276534080505
Epoch 2350, training loss: 416.4990234375 = 0.2749518156051636 + 50.0 * 8.324481010437012
Epoch 2350, val loss: 0.48982134461402893
Epoch 2360, training loss: 416.775390625 = 0.2735406160354614 + 50.0 * 8.330037117004395
Epoch 2360, val loss: 0.4901305139064789
Epoch 2370, training loss: 416.502685546875 = 0.2720808684825897 + 50.0 * 8.32461166381836
Epoch 2370, val loss: 0.49069222807884216
Epoch 2380, training loss: 416.4410095214844 = 0.2706476151943207 + 50.0 * 8.323407173156738
Epoch 2380, val loss: 0.4911516010761261
Epoch 2390, training loss: 416.4306640625 = 0.26922184228897095 + 50.0 * 8.32322883605957
Epoch 2390, val loss: 0.49166372418403625
Epoch 2400, training loss: 416.4399108886719 = 0.26779893040657043 + 50.0 * 8.323442459106445
Epoch 2400, val loss: 0.492175817489624
Epoch 2410, training loss: 416.6315002441406 = 0.2663784325122833 + 50.0 * 8.327301979064941
Epoch 2410, val loss: 0.4926844835281372
Epoch 2420, training loss: 416.41497802734375 = 0.2649233639240265 + 50.0 * 8.32300090789795
Epoch 2420, val loss: 0.49305620789527893
Epoch 2430, training loss: 416.53082275390625 = 0.26348569989204407 + 50.0 * 8.325346946716309
Epoch 2430, val loss: 0.49355724453926086
Epoch 2440, training loss: 416.461669921875 = 0.2620340883731842 + 50.0 * 8.323992729187012
Epoch 2440, val loss: 0.4942626655101776
Epoch 2450, training loss: 416.4052429199219 = 0.26057952642440796 + 50.0 * 8.322893142700195
Epoch 2450, val loss: 0.4945313036441803
Epoch 2460, training loss: 416.466552734375 = 0.25914302468299866 + 50.0 * 8.324148178100586
Epoch 2460, val loss: 0.49506938457489014
Epoch 2470, training loss: 416.455322265625 = 0.2576957941055298 + 50.0 * 8.323952674865723
Epoch 2470, val loss: 0.4955752491950989
Epoch 2480, training loss: 416.4001159667969 = 0.25624945759773254 + 50.0 * 8.322876930236816
Epoch 2480, val loss: 0.49621614813804626
Epoch 2490, training loss: 416.3386535644531 = 0.2548048496246338 + 50.0 * 8.321677207946777
Epoch 2490, val loss: 0.4968394935131073
Epoch 2500, training loss: 416.3675537109375 = 0.2533586025238037 + 50.0 * 8.322283744812012
Epoch 2500, val loss: 0.4974742531776428
Epoch 2510, training loss: 416.64892578125 = 0.2519061267375946 + 50.0 * 8.327940940856934
Epoch 2510, val loss: 0.4980410039424896
Epoch 2520, training loss: 416.3898010253906 = 0.25042790174484253 + 50.0 * 8.322787284851074
Epoch 2520, val loss: 0.4986995756626129
Epoch 2530, training loss: 416.322509765625 = 0.2489641308784485 + 50.0 * 8.321471214294434
Epoch 2530, val loss: 0.4992833733558655
Epoch 2540, training loss: 416.2992248535156 = 0.24750445783138275 + 50.0 * 8.32103443145752
Epoch 2540, val loss: 0.4999431371688843
Epoch 2550, training loss: 416.5258483886719 = 0.24609686434268951 + 50.0 * 8.325594902038574
Epoch 2550, val loss: 0.5002779960632324
Epoch 2560, training loss: 416.307373046875 = 0.24459291994571686 + 50.0 * 8.321255683898926
Epoch 2560, val loss: 0.5015952587127686
Epoch 2570, training loss: 416.2821350097656 = 0.2431425005197525 + 50.0 * 8.320779800415039
Epoch 2570, val loss: 0.5019397139549255
Epoch 2580, training loss: 416.2785949707031 = 0.2416999340057373 + 50.0 * 8.320737838745117
Epoch 2580, val loss: 0.5030474662780762
Epoch 2590, training loss: 416.3862609863281 = 0.24026939272880554 + 50.0 * 8.322919845581055
Epoch 2590, val loss: 0.5037534832954407
Epoch 2600, training loss: 416.32916259765625 = 0.23882173001766205 + 50.0 * 8.321806907653809
Epoch 2600, val loss: 0.5044625401496887
Epoch 2610, training loss: 416.34027099609375 = 0.23738493025302887 + 50.0 * 8.322057723999023
Epoch 2610, val loss: 0.5050352215766907
Epoch 2620, training loss: 416.2713928222656 = 0.23594386875629425 + 50.0 * 8.320709228515625
Epoch 2620, val loss: 0.5060761570930481
Epoch 2630, training loss: 416.2686767578125 = 0.234511598944664 + 50.0 * 8.320683479309082
Epoch 2630, val loss: 0.5066894292831421
Epoch 2640, training loss: 416.2680969238281 = 0.23307709395885468 + 50.0 * 8.320700645446777
Epoch 2640, val loss: 0.5075979232788086
Epoch 2650, training loss: 416.3581848144531 = 0.2316524088382721 + 50.0 * 8.322530746459961
Epoch 2650, val loss: 0.5083842873573303
Epoch 2660, training loss: 416.2408142089844 = 0.2302073985338211 + 50.0 * 8.320212364196777
Epoch 2660, val loss: 0.5092251300811768
Epoch 2670, training loss: 416.3016662597656 = 0.22877028584480286 + 50.0 * 8.321457862854004
Epoch 2670, val loss: 0.5100644826889038
Epoch 2680, training loss: 416.207763671875 = 0.2273339331150055 + 50.0 * 8.319608688354492
Epoch 2680, val loss: 0.5109375715255737
Epoch 2690, training loss: 416.1740417480469 = 0.2259005606174469 + 50.0 * 8.318963050842285
Epoch 2690, val loss: 0.5117787718772888
Epoch 2700, training loss: 416.1843566894531 = 0.22447429597377777 + 50.0 * 8.319197654724121
Epoch 2700, val loss: 0.5126476287841797
Epoch 2710, training loss: 416.2747802734375 = 0.22304923832416534 + 50.0 * 8.32103443145752
Epoch 2710, val loss: 0.513535737991333
Epoch 2720, training loss: 416.17572021484375 = 0.22161203622817993 + 50.0 * 8.319082260131836
Epoch 2720, val loss: 0.5143772959709167
Epoch 2730, training loss: 416.3375244140625 = 0.2201927751302719 + 50.0 * 8.322346687316895
Epoch 2730, val loss: 0.5152313113212585
Epoch 2740, training loss: 416.37042236328125 = 0.21875105798244476 + 50.0 * 8.323033332824707
Epoch 2740, val loss: 0.5163315534591675
Epoch 2750, training loss: 416.2614440917969 = 0.21731851994991302 + 50.0 * 8.320882797241211
Epoch 2750, val loss: 0.5170295834541321
Epoch 2760, training loss: 416.1806335449219 = 0.2158786952495575 + 50.0 * 8.319294929504395
Epoch 2760, val loss: 0.5181065201759338
Epoch 2770, training loss: 416.1050109863281 = 0.21445782482624054 + 50.0 * 8.317811012268066
Epoch 2770, val loss: 0.5190680623054504
Epoch 2780, training loss: 416.09881591796875 = 0.213033065199852 + 50.0 * 8.317715644836426
Epoch 2780, val loss: 0.5202099084854126
Epoch 2790, training loss: 416.1884765625 = 0.21162088215351105 + 50.0 * 8.319537162780762
Epoch 2790, val loss: 0.5211275219917297
Epoch 2800, training loss: 416.26641845703125 = 0.2101975828409195 + 50.0 * 8.321124076843262
Epoch 2800, val loss: 0.5221624374389648
Epoch 2810, training loss: 416.1740417480469 = 0.20877142250537872 + 50.0 * 8.319305419921875
Epoch 2810, val loss: 0.5234017968177795
Epoch 2820, training loss: 416.18585205078125 = 0.20736314356327057 + 50.0 * 8.31956958770752
Epoch 2820, val loss: 0.524573564529419
Epoch 2830, training loss: 416.1456298828125 = 0.20594002306461334 + 50.0 * 8.318794250488281
Epoch 2830, val loss: 0.5254174470901489
Epoch 2840, training loss: 416.0536804199219 = 0.20452865958213806 + 50.0 * 8.316983222961426
Epoch 2840, val loss: 0.5263039469718933
Epoch 2850, training loss: 416.052001953125 = 0.20312567055225372 + 50.0 * 8.316977500915527
Epoch 2850, val loss: 0.5272835493087769
Epoch 2860, training loss: 416.10626220703125 = 0.20173723995685577 + 50.0 * 8.318090438842773
Epoch 2860, val loss: 0.5282849669456482
Epoch 2870, training loss: 416.343017578125 = 0.2003723680973053 + 50.0 * 8.322853088378906
Epoch 2870, val loss: 0.5293568968772888
Epoch 2880, training loss: 416.1443176269531 = 0.19892887771129608 + 50.0 * 8.318907737731934
Epoch 2880, val loss: 0.5312903523445129
Epoch 2890, training loss: 416.0688171386719 = 0.1975357085466385 + 50.0 * 8.317425727844238
Epoch 2890, val loss: 0.5317904949188232
Epoch 2900, training loss: 416.01104736328125 = 0.1961359977722168 + 50.0 * 8.316298484802246
Epoch 2900, val loss: 0.533439040184021
Epoch 2910, training loss: 416.0249328613281 = 0.19476330280303955 + 50.0 * 8.316603660583496
Epoch 2910, val loss: 0.5342538356781006
Epoch 2920, training loss: 416.3109436035156 = 0.19341397285461426 + 50.0 * 8.32235050201416
Epoch 2920, val loss: 0.5353941321372986
Epoch 2930, training loss: 416.1184997558594 = 0.19204548001289368 + 50.0 * 8.31852912902832
Epoch 2930, val loss: 0.5369225144386292
Epoch 2940, training loss: 416.0653076171875 = 0.19066719710826874 + 50.0 * 8.317492485046387
Epoch 2940, val loss: 0.5378197431564331
Epoch 2950, training loss: 415.99945068359375 = 0.18930457532405853 + 50.0 * 8.316203117370605
Epoch 2950, val loss: 0.5391067266464233
Epoch 2960, training loss: 415.9951477050781 = 0.187950998544693 + 50.0 * 8.316143989562988
Epoch 2960, val loss: 0.5402045845985413
Epoch 2970, training loss: 416.0664978027344 = 0.18660297989845276 + 50.0 * 8.317597389221191
Epoch 2970, val loss: 0.5415013432502747
Epoch 2980, training loss: 415.9587707519531 = 0.18524491786956787 + 50.0 * 8.315470695495605
Epoch 2980, val loss: 0.5428178310394287
Epoch 2990, training loss: 416.10980224609375 = 0.18392537534236908 + 50.0 * 8.318517684936523
Epoch 2990, val loss: 0.5442461371421814
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8102486047691526
0.8430051438093169
=== training gcn model ===
Epoch 0, training loss: 530.2129516601562 = 1.099503517150879 + 50.0 * 10.582268714904785
Epoch 0, val loss: 1.0983444452285767
Epoch 10, training loss: 530.1578979492188 = 1.0927248001098633 + 50.0 * 10.581303596496582
Epoch 10, val loss: 1.0916215181350708
Epoch 20, training loss: 529.739501953125 = 1.0848958492279053 + 50.0 * 10.573092460632324
Epoch 20, val loss: 1.0838953256607056
Epoch 30, training loss: 526.8099365234375 = 1.0758880376815796 + 50.0 * 10.514680862426758
Epoch 30, val loss: 1.0750250816345215
Epoch 40, training loss: 514.4580078125 = 1.066186547279358 + 50.0 * 10.267836570739746
Epoch 40, val loss: 1.0656750202178955
Epoch 50, training loss: 490.489501953125 = 1.0561217069625854 + 50.0 * 9.788667678833008
Epoch 50, val loss: 1.0558130741119385
Epoch 60, training loss: 478.91912841796875 = 1.047286868095398 + 50.0 * 9.5574369430542
Epoch 60, val loss: 1.0474720001220703
Epoch 70, training loss: 471.7252197265625 = 1.0400413274765015 + 50.0 * 9.413703918457031
Epoch 70, val loss: 1.0407110452651978
Epoch 80, training loss: 461.8388366699219 = 1.0345910787582397 + 50.0 * 9.216085433959961
Epoch 80, val loss: 1.0355380773544312
Epoch 90, training loss: 456.92022705078125 = 1.0302863121032715 + 50.0 * 9.117798805236816
Epoch 90, val loss: 1.031264066696167
Epoch 100, training loss: 451.5131530761719 = 1.0261495113372803 + 50.0 * 9.009739875793457
Epoch 100, val loss: 1.0272094011306763
Epoch 110, training loss: 449.5919189453125 = 1.0211676359176636 + 50.0 * 8.971414566040039
Epoch 110, val loss: 1.0220813751220703
Epoch 120, training loss: 447.2145690917969 = 1.0140007734298706 + 50.0 * 8.92401123046875
Epoch 120, val loss: 1.015332818031311
Epoch 130, training loss: 444.1235046386719 = 1.0094678401947021 + 50.0 * 8.86228084564209
Epoch 130, val loss: 1.0112501382827759
Epoch 140, training loss: 440.7462158203125 = 1.0054607391357422 + 50.0 * 8.794815063476562
Epoch 140, val loss: 1.0071724653244019
Epoch 150, training loss: 438.5128479003906 = 0.9995280504226685 + 50.0 * 8.750266075134277
Epoch 150, val loss: 1.001332402229309
Epoch 160, training loss: 436.6739501953125 = 0.9921878576278687 + 50.0 * 8.713635444641113
Epoch 160, val loss: 0.9941468834877014
Epoch 170, training loss: 435.1309509277344 = 0.9840158820152283 + 50.0 * 8.682938575744629
Epoch 170, val loss: 0.986264169216156
Epoch 180, training loss: 433.8567810058594 = 0.9752385020256042 + 50.0 * 8.657630920410156
Epoch 180, val loss: 0.9777796268463135
Epoch 190, training loss: 432.586181640625 = 0.965766429901123 + 50.0 * 8.632408142089844
Epoch 190, val loss: 0.9686065912246704
Epoch 200, training loss: 431.5310974121094 = 0.9552801251411438 + 50.0 * 8.611515998840332
Epoch 200, val loss: 0.9584464430809021
Epoch 210, training loss: 430.6473388671875 = 0.9437263011932373 + 50.0 * 8.594072341918945
Epoch 210, val loss: 0.9472801685333252
Epoch 220, training loss: 429.5911865234375 = 0.9312909245491028 + 50.0 * 8.573197364807129
Epoch 220, val loss: 0.935207188129425
Epoch 230, training loss: 428.8489990234375 = 0.9181278944015503 + 50.0 * 8.55861759185791
Epoch 230, val loss: 0.9224451780319214
Epoch 240, training loss: 428.1611328125 = 0.9040192365646362 + 50.0 * 8.54514217376709
Epoch 240, val loss: 0.9087432026863098
Epoch 250, training loss: 427.43084716796875 = 0.8890411257743835 + 50.0 * 8.53083610534668
Epoch 250, val loss: 0.8942880630493164
Epoch 260, training loss: 426.8458557128906 = 0.8734982013702393 + 50.0 * 8.519447326660156
Epoch 260, val loss: 0.8792480230331421
Epoch 270, training loss: 426.8760070800781 = 0.8573429584503174 + 50.0 * 8.520373344421387
Epoch 270, val loss: 0.8636108040809631
Epoch 280, training loss: 425.99151611328125 = 0.8403090238571167 + 50.0 * 8.503024101257324
Epoch 280, val loss: 0.8471480011940002
Epoch 290, training loss: 425.5893859863281 = 0.8231860995292664 + 50.0 * 8.49532413482666
Epoch 290, val loss: 0.8307473063468933
Epoch 300, training loss: 425.2408447265625 = 0.806195080280304 + 50.0 * 8.488693237304688
Epoch 300, val loss: 0.8144402503967285
Epoch 310, training loss: 424.94378662109375 = 0.7892783880233765 + 50.0 * 8.4830904006958
Epoch 310, val loss: 0.7982041239738464
Epoch 320, training loss: 425.287841796875 = 0.7724735736846924 + 50.0 * 8.490307807922363
Epoch 320, val loss: 0.7821103930473328
Epoch 330, training loss: 424.5654602050781 = 0.755932092666626 + 50.0 * 8.476190567016602
Epoch 330, val loss: 0.7662783861160278
Epoch 340, training loss: 424.26702880859375 = 0.7400807738304138 + 50.0 * 8.470539093017578
Epoch 340, val loss: 0.751205563545227
Epoch 350, training loss: 424.0106506347656 = 0.7249650359153748 + 50.0 * 8.465713500976562
Epoch 350, val loss: 0.7368497252464294
Epoch 360, training loss: 424.0696716308594 = 0.7105069160461426 + 50.0 * 8.467183113098145
Epoch 360, val loss: 0.7231526970863342
Epoch 370, training loss: 423.6663818359375 = 0.696808397769928 + 50.0 * 8.459391593933105
Epoch 370, val loss: 0.7102530002593994
Epoch 380, training loss: 423.4607238769531 = 0.6840775012969971 + 50.0 * 8.455533027648926
Epoch 380, val loss: 0.698319137096405
Epoch 390, training loss: 423.2947082519531 = 0.6720905900001526 + 50.0 * 8.452452659606934
Epoch 390, val loss: 0.6871095299720764
Epoch 400, training loss: 423.06610107421875 = 0.6609483957290649 + 50.0 * 8.448102951049805
Epoch 400, val loss: 0.6768063902854919
Epoch 410, training loss: 422.8981018066406 = 0.6506994366645813 + 50.0 * 8.444948196411133
Epoch 410, val loss: 0.6673662662506104
Epoch 420, training loss: 422.8051452636719 = 0.6412527561187744 + 50.0 * 8.443277359008789
Epoch 420, val loss: 0.6587218046188354
Epoch 430, training loss: 422.66729736328125 = 0.6324292421340942 + 50.0 * 8.44069766998291
Epoch 430, val loss: 0.6506993174552917
Epoch 440, training loss: 422.44818115234375 = 0.6243132948875427 + 50.0 * 8.436477661132812
Epoch 440, val loss: 0.6434447765350342
Epoch 450, training loss: 422.3356628417969 = 0.6168871521949768 + 50.0 * 8.434375762939453
Epoch 450, val loss: 0.636851966381073
Epoch 460, training loss: 422.506103515625 = 0.6100127696990967 + 50.0 * 8.437921524047852
Epoch 460, val loss: 0.6308154463768005
Epoch 470, training loss: 422.06451416015625 = 0.6035431027412415 + 50.0 * 8.429219245910645
Epoch 470, val loss: 0.6251912713050842
Epoch 480, training loss: 421.8935546875 = 0.5976908802986145 + 50.0 * 8.425917625427246
Epoch 480, val loss: 0.6201870441436768
Epoch 490, training loss: 421.7880859375 = 0.5922932028770447 + 50.0 * 8.42391586303711
Epoch 490, val loss: 0.6156286001205444
Epoch 500, training loss: 421.80181884765625 = 0.5871973633766174 + 50.0 * 8.42429256439209
Epoch 500, val loss: 0.6113863587379456
Epoch 510, training loss: 421.81201171875 = 0.5823695063591003 + 50.0 * 8.424592971801758
Epoch 510, val loss: 0.6074304580688477
Epoch 520, training loss: 421.47210693359375 = 0.5779067277908325 + 50.0 * 8.41788387298584
Epoch 520, val loss: 0.6038011908531189
Epoch 530, training loss: 421.2989807128906 = 0.573792576789856 + 50.0 * 8.414504051208496
Epoch 530, val loss: 0.6005736589431763
Epoch 540, training loss: 421.2392272949219 = 0.5699324011802673 + 50.0 * 8.413385391235352
Epoch 540, val loss: 0.597557783126831
Epoch 550, training loss: 421.2835693359375 = 0.566206157207489 + 50.0 * 8.414347648620605
Epoch 550, val loss: 0.5946515202522278
Epoch 560, training loss: 421.0482177734375 = 0.562667965888977 + 50.0 * 8.409710884094238
Epoch 560, val loss: 0.5919651985168457
Epoch 570, training loss: 420.9407958984375 = 0.5593816041946411 + 50.0 * 8.407628059387207
Epoch 570, val loss: 0.5894988179206848
Epoch 580, training loss: 420.87176513671875 = 0.5562703013420105 + 50.0 * 8.406310081481934
Epoch 580, val loss: 0.5871636867523193
Epoch 590, training loss: 420.8044738769531 = 0.5532532930374146 + 50.0 * 8.405024528503418
Epoch 590, val loss: 0.5849516987800598
Epoch 600, training loss: 420.75311279296875 = 0.5503778457641602 + 50.0 * 8.404054641723633
Epoch 600, val loss: 0.5828961730003357
Epoch 610, training loss: 420.8100891113281 = 0.54759681224823 + 50.0 * 8.40524959564209
Epoch 610, val loss: 0.5808257460594177
Epoch 620, training loss: 420.5738525390625 = 0.5449306964874268 + 50.0 * 8.400578498840332
Epoch 620, val loss: 0.5789024233818054
Epoch 630, training loss: 420.457763671875 = 0.5424168109893799 + 50.0 * 8.398306846618652
Epoch 630, val loss: 0.5771015286445618
Epoch 640, training loss: 420.35345458984375 = 0.5400023460388184 + 50.0 * 8.396268844604492
Epoch 640, val loss: 0.5754146575927734
Epoch 650, training loss: 420.8106384277344 = 0.5376499891281128 + 50.0 * 8.4054594039917
Epoch 650, val loss: 0.5737626552581787
Epoch 660, training loss: 420.25244140625 = 0.5352762937545776 + 50.0 * 8.394343376159668
Epoch 660, val loss: 0.5719883441925049
Epoch 670, training loss: 420.2389221191406 = 0.5330484509468079 + 50.0 * 8.39411735534668
Epoch 670, val loss: 0.5703924298286438
Epoch 680, training loss: 420.08892822265625 = 0.5309174060821533 + 50.0 * 8.391160011291504
Epoch 680, val loss: 0.5688871741294861
Epoch 690, training loss: 420.18963623046875 = 0.5288469791412354 + 50.0 * 8.393216133117676
Epoch 690, val loss: 0.5673549175262451
Epoch 700, training loss: 420.14862060546875 = 0.5267247557640076 + 50.0 * 8.392437934875488
Epoch 700, val loss: 0.5659406781196594
Epoch 710, training loss: 420.0466003417969 = 0.5246698260307312 + 50.0 * 8.3904390335083
Epoch 710, val loss: 0.5643672347068787
Epoch 720, training loss: 419.8821716308594 = 0.5227256417274475 + 50.0 * 8.387188911437988
Epoch 720, val loss: 0.5630231499671936
Epoch 730, training loss: 419.8630676269531 = 0.5208426713943481 + 50.0 * 8.386844635009766
Epoch 730, val loss: 0.5617079734802246
Epoch 740, training loss: 420.1447448730469 = 0.5189672708511353 + 50.0 * 8.392515182495117
Epoch 740, val loss: 0.560388445854187
Epoch 750, training loss: 419.8219909667969 = 0.5170823931694031 + 50.0 * 8.38609790802002
Epoch 750, val loss: 0.5590017437934875
Epoch 760, training loss: 419.7073059082031 = 0.5152790546417236 + 50.0 * 8.383840560913086
Epoch 760, val loss: 0.557746946811676
Epoch 770, training loss: 419.70001220703125 = 0.5135191082954407 + 50.0 * 8.383729934692383
Epoch 770, val loss: 0.5565381050109863
Epoch 780, training loss: 419.775146484375 = 0.5117794871330261 + 50.0 * 8.38526725769043
Epoch 780, val loss: 0.5552324056625366
Epoch 790, training loss: 419.65960693359375 = 0.5100031495094299 + 50.0 * 8.382991790771484
Epoch 790, val loss: 0.5540338158607483
Epoch 800, training loss: 419.55572509765625 = 0.5082810521125793 + 50.0 * 8.380949020385742
Epoch 800, val loss: 0.5528532862663269
Epoch 810, training loss: 419.5005187988281 = 0.5066176652908325 + 50.0 * 8.379878044128418
Epoch 810, val loss: 0.5516461133956909
Epoch 820, training loss: 419.51031494140625 = 0.5049780011177063 + 50.0 * 8.380106925964355
Epoch 820, val loss: 0.5505560040473938
Epoch 830, training loss: 419.5885314941406 = 0.5033278465270996 + 50.0 * 8.381704330444336
Epoch 830, val loss: 0.5493845343589783
Epoch 840, training loss: 419.4396667480469 = 0.5017033219337463 + 50.0 * 8.378759384155273
Epoch 840, val loss: 0.5481720566749573
Epoch 850, training loss: 419.2991027832031 = 0.5001105070114136 + 50.0 * 8.37597942352295
Epoch 850, val loss: 0.5471609234809875
Epoch 860, training loss: 419.3287658691406 = 0.49855419993400574 + 50.0 * 8.376604080200195
Epoch 860, val loss: 0.5460920333862305
Epoch 870, training loss: 419.5926818847656 = 0.49700257182121277 + 50.0 * 8.381913185119629
Epoch 870, val loss: 0.5449876189231873
Epoch 880, training loss: 419.331787109375 = 0.49538692831993103 + 50.0 * 8.376728057861328
Epoch 880, val loss: 0.543830931186676
Epoch 890, training loss: 419.1959533691406 = 0.49384093284606934 + 50.0 * 8.374042510986328
Epoch 890, val loss: 0.5428324937820435
Epoch 900, training loss: 419.2157287597656 = 0.4923504889011383 + 50.0 * 8.374467849731445
Epoch 900, val loss: 0.5417961478233337
Epoch 910, training loss: 419.1293029785156 = 0.4908389449119568 + 50.0 * 8.372769355773926
Epoch 910, val loss: 0.540804922580719
Epoch 920, training loss: 419.1437072753906 = 0.48935598134994507 + 50.0 * 8.373086929321289
Epoch 920, val loss: 0.5398363471031189
Epoch 930, training loss: 419.1563415527344 = 0.48787400126457214 + 50.0 * 8.373369216918945
Epoch 930, val loss: 0.5388376712799072
Epoch 940, training loss: 418.9862365722656 = 0.48641839623451233 + 50.0 * 8.369996070861816
Epoch 940, val loss: 0.5378047227859497
Epoch 950, training loss: 418.971923828125 = 0.48499974608421326 + 50.0 * 8.369738578796387
Epoch 950, val loss: 0.5368647575378418
Epoch 960, training loss: 418.94561767578125 = 0.48358654975891113 + 50.0 * 8.369240760803223
Epoch 960, val loss: 0.5359410643577576
Epoch 970, training loss: 419.5850524902344 = 0.4821614623069763 + 50.0 * 8.382058143615723
Epoch 970, val loss: 0.5350115895271301
Epoch 980, training loss: 419.0084533691406 = 0.4806530177593231 + 50.0 * 8.370555877685547
Epoch 980, val loss: 0.5341039299964905
Epoch 990, training loss: 418.81207275390625 = 0.4792395532131195 + 50.0 * 8.366656303405762
Epoch 990, val loss: 0.5331792831420898
Epoch 1000, training loss: 418.8080749511719 = 0.47787177562713623 + 50.0 * 8.36660385131836
Epoch 1000, val loss: 0.5322471261024475
Epoch 1010, training loss: 418.8490295410156 = 0.476518839597702 + 50.0 * 8.367449760437012
Epoch 1010, val loss: 0.5313707590103149
Epoch 1020, training loss: 418.85406494140625 = 0.47512286901474 + 50.0 * 8.367578506469727
Epoch 1020, val loss: 0.5305325984954834
Epoch 1030, training loss: 418.822509765625 = 0.4737199544906616 + 50.0 * 8.366975784301758
Epoch 1030, val loss: 0.5297254920005798
Epoch 1040, training loss: 418.6922912597656 = 0.47234201431274414 + 50.0 * 8.364398956298828
Epoch 1040, val loss: 0.5288964509963989
Epoch 1050, training loss: 418.7073669433594 = 0.4710002541542053 + 50.0 * 8.364727020263672
Epoch 1050, val loss: 0.5280199646949768
Epoch 1060, training loss: 418.92547607421875 = 0.4696229100227356 + 50.0 * 8.36911678314209
Epoch 1060, val loss: 0.5272403359413147
Epoch 1070, training loss: 418.6575622558594 = 0.46822285652160645 + 50.0 * 8.363786697387695
Epoch 1070, val loss: 0.5263826251029968
Epoch 1080, training loss: 418.57989501953125 = 0.4668845534324646 + 50.0 * 8.362259864807129
Epoch 1080, val loss: 0.525624692440033
Epoch 1090, training loss: 418.5413818359375 = 0.4655781090259552 + 50.0 * 8.361515998840332
Epoch 1090, val loss: 0.5248103141784668
Epoch 1100, training loss: 418.5470886230469 = 0.4642791152000427 + 50.0 * 8.361656188964844
Epoch 1100, val loss: 0.5240563154220581
Epoch 1110, training loss: 418.84326171875 = 0.4629499614238739 + 50.0 * 8.367606163024902
Epoch 1110, val loss: 0.5231920480728149
Epoch 1120, training loss: 418.5044860839844 = 0.4615405201911926 + 50.0 * 8.360858917236328
Epoch 1120, val loss: 0.5225189328193665
Epoch 1130, training loss: 418.46685791015625 = 0.460201621055603 + 50.0 * 8.360133171081543
Epoch 1130, val loss: 0.5217002034187317
Epoch 1140, training loss: 418.54852294921875 = 0.4588932394981384 + 50.0 * 8.36179256439209
Epoch 1140, val loss: 0.5209428071975708
Epoch 1150, training loss: 418.5111389160156 = 0.4575379490852356 + 50.0 * 8.361071586608887
Epoch 1150, val loss: 0.5201597809791565
Epoch 1160, training loss: 418.4234313964844 = 0.4561935067176819 + 50.0 * 8.359344482421875
Epoch 1160, val loss: 0.5194349884986877
Epoch 1170, training loss: 418.37750244140625 = 0.4548873007297516 + 50.0 * 8.358451843261719
Epoch 1170, val loss: 0.5187360644340515
Epoch 1180, training loss: 418.3853759765625 = 0.4535943865776062 + 50.0 * 8.358635902404785
Epoch 1180, val loss: 0.5180556774139404
Epoch 1190, training loss: 418.5838623046875 = 0.45226916670799255 + 50.0 * 8.362631797790527
Epoch 1190, val loss: 0.5173585414886475
Epoch 1200, training loss: 418.425537109375 = 0.45091503858566284 + 50.0 * 8.359492301940918
Epoch 1200, val loss: 0.5165063142776489
Epoch 1210, training loss: 418.4974365234375 = 0.4495774805545807 + 50.0 * 8.360957145690918
Epoch 1210, val loss: 0.5158289074897766
Epoch 1220, training loss: 418.2536315917969 = 0.448254257440567 + 50.0 * 8.356107711791992
Epoch 1220, val loss: 0.5151584148406982
Epoch 1230, training loss: 418.2159118652344 = 0.4469640851020813 + 50.0 * 8.355379104614258
Epoch 1230, val loss: 0.5144852995872498
Epoch 1240, training loss: 418.3170166015625 = 0.44567903876304626 + 50.0 * 8.357426643371582
Epoch 1240, val loss: 0.5137964487075806
Epoch 1250, training loss: 418.2862854003906 = 0.4443502426147461 + 50.0 * 8.356839179992676
Epoch 1250, val loss: 0.5131678581237793
Epoch 1260, training loss: 418.2383117675781 = 0.44301915168762207 + 50.0 * 8.355905532836914
Epoch 1260, val loss: 0.5125252604484558
Epoch 1270, training loss: 418.15478515625 = 0.4417063295841217 + 50.0 * 8.35426139831543
Epoch 1270, val loss: 0.511812686920166
Epoch 1280, training loss: 418.1458435058594 = 0.4404117166996002 + 50.0 * 8.354108810424805
Epoch 1280, val loss: 0.5112329721450806
Epoch 1290, training loss: 418.5281066894531 = 0.43909069895744324 + 50.0 * 8.361780166625977
Epoch 1290, val loss: 0.5106328129768372
Epoch 1300, training loss: 418.15576171875 = 0.43769076466560364 + 50.0 * 8.354361534118652
Epoch 1300, val loss: 0.5097571015357971
Epoch 1310, training loss: 418.0501708984375 = 0.4363543689250946 + 50.0 * 8.352276802062988
Epoch 1310, val loss: 0.509199321269989
Epoch 1320, training loss: 418.008544921875 = 0.435048371553421 + 50.0 * 8.351469993591309
Epoch 1320, val loss: 0.508529782295227
Epoch 1330, training loss: 418.03533935546875 = 0.43375200033187866 + 50.0 * 8.352031707763672
Epoch 1330, val loss: 0.5079637169837952
Epoch 1340, training loss: 418.37548828125 = 0.43241259455680847 + 50.0 * 8.358861923217773
Epoch 1340, val loss: 0.5072651505470276
Epoch 1350, training loss: 418.03887939453125 = 0.4310062825679779 + 50.0 * 8.352157592773438
Epoch 1350, val loss: 0.5066329836845398
Epoch 1360, training loss: 417.9592590332031 = 0.4296409487724304 + 50.0 * 8.350592613220215
Epoch 1360, val loss: 0.5060346126556396
Epoch 1370, training loss: 418.0421142578125 = 0.4283064305782318 + 50.0 * 8.352275848388672
Epoch 1370, val loss: 0.5055060386657715
Epoch 1380, training loss: 418.1318664550781 = 0.4269294738769531 + 50.0 * 8.354098320007324
Epoch 1380, val loss: 0.5049118399620056
Epoch 1390, training loss: 417.99945068359375 = 0.42554670572280884 + 50.0 * 8.351478576660156
Epoch 1390, val loss: 0.5042093992233276
Epoch 1400, training loss: 417.870849609375 = 0.424191415309906 + 50.0 * 8.348933219909668
Epoch 1400, val loss: 0.5036669969558716
Epoch 1410, training loss: 418.00408935546875 = 0.4228474795818329 + 50.0 * 8.351624488830566
Epoch 1410, val loss: 0.5030532479286194
Epoch 1420, training loss: 417.90350341796875 = 0.4214477837085724 + 50.0 * 8.349640846252441
Epoch 1420, val loss: 0.5025050044059753
Epoch 1430, training loss: 417.8367004394531 = 0.42005759477615356 + 50.0 * 8.348333358764648
Epoch 1430, val loss: 0.5020530819892883
Epoch 1440, training loss: 417.8286437988281 = 0.4186919331550598 + 50.0 * 8.348198890686035
Epoch 1440, val loss: 0.5014270544052124
Epoch 1450, training loss: 417.961669921875 = 0.4173099994659424 + 50.0 * 8.350887298583984
Epoch 1450, val loss: 0.5008967518806458
Epoch 1460, training loss: 417.84661865234375 = 0.4158937633037567 + 50.0 * 8.348614692687988
Epoch 1460, val loss: 0.5003308653831482
Epoch 1470, training loss: 418.0876159667969 = 0.41447338461875916 + 50.0 * 8.353463172912598
Epoch 1470, val loss: 0.4996527433395386
Epoch 1480, training loss: 417.7848815917969 = 0.41301000118255615 + 50.0 * 8.347436904907227
Epoch 1480, val loss: 0.4993261992931366
Epoch 1490, training loss: 417.6986389160156 = 0.4115981459617615 + 50.0 * 8.345741271972656
Epoch 1490, val loss: 0.4986763596534729
Epoch 1500, training loss: 417.6640625 = 0.4101974666118622 + 50.0 * 8.345077514648438
Epoch 1500, val loss: 0.4982006549835205
Epoch 1510, training loss: 417.6518249511719 = 0.40879520773887634 + 50.0 * 8.344861030578613
Epoch 1510, val loss: 0.4977070987224579
Epoch 1520, training loss: 417.78570556640625 = 0.40737399458885193 + 50.0 * 8.347566604614258
Epoch 1520, val loss: 0.49719932675361633
Epoch 1530, training loss: 417.7972717285156 = 0.4058742821216583 + 50.0 * 8.347827911376953
Epoch 1530, val loss: 0.4966554045677185
Epoch 1540, training loss: 417.69683837890625 = 0.404339462518692 + 50.0 * 8.345849990844727
Epoch 1540, val loss: 0.49613189697265625
Epoch 1550, training loss: 417.6613464355469 = 0.40288785099983215 + 50.0 * 8.345169067382812
Epoch 1550, val loss: 0.4956388473510742
Epoch 1560, training loss: 417.67572021484375 = 0.4014405906200409 + 50.0 * 8.34548568725586
Epoch 1560, val loss: 0.49507641792297363
Epoch 1570, training loss: 417.71600341796875 = 0.3999752104282379 + 50.0 * 8.346321105957031
Epoch 1570, val loss: 0.4945853352546692
Epoch 1580, training loss: 417.6194152832031 = 0.39848774671554565 + 50.0 * 8.3444185256958
Epoch 1580, val loss: 0.49426236748695374
Epoch 1590, training loss: 417.52557373046875 = 0.3970164656639099 + 50.0 * 8.342571258544922
Epoch 1590, val loss: 0.49368926882743835
Epoch 1600, training loss: 417.69940185546875 = 0.39555874466896057 + 50.0 * 8.346076965332031
Epoch 1600, val loss: 0.49329766631126404
Epoch 1610, training loss: 417.54058837890625 = 0.39404773712158203 + 50.0 * 8.342930793762207
Epoch 1610, val loss: 0.4927888810634613
Epoch 1620, training loss: 417.5478515625 = 0.39255428314208984 + 50.0 * 8.343106269836426
Epoch 1620, val loss: 0.49226194620132446
Epoch 1630, training loss: 417.8487548828125 = 0.39106982946395874 + 50.0 * 8.349153518676758
Epoch 1630, val loss: 0.4919748306274414
Epoch 1640, training loss: 417.48052978515625 = 0.3895249366760254 + 50.0 * 8.341819763183594
Epoch 1640, val loss: 0.49124255776405334
Epoch 1650, training loss: 417.4371643066406 = 0.3880271017551422 + 50.0 * 8.340982437133789
Epoch 1650, val loss: 0.49087539315223694
Epoch 1660, training loss: 417.4617004394531 = 0.38654568791389465 + 50.0 * 8.341503143310547
Epoch 1660, val loss: 0.49044784903526306
Epoch 1670, training loss: 417.549560546875 = 0.3850507438182831 + 50.0 * 8.343290328979492
Epoch 1670, val loss: 0.4899720847606659
Epoch 1680, training loss: 417.5782775878906 = 0.3835151791572571 + 50.0 * 8.343894958496094
Epoch 1680, val loss: 0.48952266573905945
Epoch 1690, training loss: 417.3975524902344 = 0.38197019696235657 + 50.0 * 8.340312004089355
Epoch 1690, val loss: 0.4891848862171173
Epoch 1700, training loss: 417.38995361328125 = 0.38045310974121094 + 50.0 * 8.340189933776855
Epoch 1700, val loss: 0.48880308866500854
Epoch 1710, training loss: 417.4707336425781 = 0.37893572449684143 + 50.0 * 8.341835975646973
Epoch 1710, val loss: 0.48843806982040405
Epoch 1720, training loss: 417.4621887207031 = 0.37739577889442444 + 50.0 * 8.341695785522461
Epoch 1720, val loss: 0.4881035387516022
Epoch 1730, training loss: 417.5707702636719 = 0.3758361339569092 + 50.0 * 8.34389877319336
Epoch 1730, val loss: 0.4876834750175476
Epoch 1740, training loss: 417.50128173828125 = 0.374252051115036 + 50.0 * 8.342540740966797
Epoch 1740, val loss: 0.48708364367485046
Epoch 1750, training loss: 417.39849853515625 = 0.37268126010894775 + 50.0 * 8.340516090393066
Epoch 1750, val loss: 0.48674631118774414
Epoch 1760, training loss: 417.3100891113281 = 0.3711247146129608 + 50.0 * 8.33877944946289
Epoch 1760, val loss: 0.48617246747016907
Epoch 1770, training loss: 417.3229064941406 = 0.36957091093063354 + 50.0 * 8.339066505432129
Epoch 1770, val loss: 0.4857933819293976
Epoch 1780, training loss: 417.53790283203125 = 0.36800873279571533 + 50.0 * 8.343398094177246
Epoch 1780, val loss: 0.48531943559646606
Epoch 1790, training loss: 417.38153076171875 = 0.36638426780700684 + 50.0 * 8.340302467346191
Epoch 1790, val loss: 0.4852115213871002
Epoch 1800, training loss: 417.2558288574219 = 0.36477845907211304 + 50.0 * 8.337821006774902
Epoch 1800, val loss: 0.4846518933773041
Epoch 1810, training loss: 417.21881103515625 = 0.3631982207298279 + 50.0 * 8.337112426757812
Epoch 1810, val loss: 0.484314501285553
Epoch 1820, training loss: 417.227783203125 = 0.36162927746772766 + 50.0 * 8.337323188781738
Epoch 1820, val loss: 0.48393604159355164
Epoch 1830, training loss: 417.53448486328125 = 0.3600521981716156 + 50.0 * 8.343488693237305
Epoch 1830, val loss: 0.4835318326950073
Epoch 1840, training loss: 417.28448486328125 = 0.3584202826023102 + 50.0 * 8.338521003723145
Epoch 1840, val loss: 0.4834161698818207
Epoch 1850, training loss: 417.5783996582031 = 0.3568076193332672 + 50.0 * 8.34443187713623
Epoch 1850, val loss: 0.48313212394714355
Epoch 1860, training loss: 417.2134704589844 = 0.3551357090473175 + 50.0 * 8.337166786193848
Epoch 1860, val loss: 0.4826176166534424
Epoch 1870, training loss: 417.16143798828125 = 0.353514164686203 + 50.0 * 8.336158752441406
Epoch 1870, val loss: 0.4822863042354584
Epoch 1880, training loss: 417.1826171875 = 0.3519023656845093 + 50.0 * 8.336614608764648
Epoch 1880, val loss: 0.481968492269516
Epoch 1890, training loss: 417.31951904296875 = 0.35028135776519775 + 50.0 * 8.339385032653809
Epoch 1890, val loss: 0.4816614091396332
Epoch 1900, training loss: 417.1180419921875 = 0.34862419962882996 + 50.0 * 8.33538818359375
Epoch 1900, val loss: 0.4815234839916229
Epoch 1910, training loss: 417.2514343261719 = 0.346988320350647 + 50.0 * 8.338088989257812
Epoch 1910, val loss: 0.48133713006973267
Epoch 1920, training loss: 417.2239990234375 = 0.3453255593776703 + 50.0 * 8.337573051452637
Epoch 1920, val loss: 0.4809563457965851
Epoch 1930, training loss: 417.16802978515625 = 0.34367600083351135 + 50.0 * 8.33648681640625
Epoch 1930, val loss: 0.48045065999031067
Epoch 1940, training loss: 417.19000244140625 = 0.34202009439468384 + 50.0 * 8.336959838867188
Epoch 1940, val loss: 0.48045244812965393
Epoch 1950, training loss: 417.1217956542969 = 0.34035515785217285 + 50.0 * 8.335628509521484
Epoch 1950, val loss: 0.48014160990715027
Epoch 1960, training loss: 417.0252990722656 = 0.33869999647140503 + 50.0 * 8.333731651306152
Epoch 1960, val loss: 0.4798740744590759
Epoch 1970, training loss: 417.0676574707031 = 0.33706265687942505 + 50.0 * 8.334611892700195
Epoch 1970, val loss: 0.479665070772171
Epoch 1980, training loss: 417.220703125 = 0.33541175723075867 + 50.0 * 8.337705612182617
Epoch 1980, val loss: 0.47944867610931396
Epoch 1990, training loss: 417.0624694824219 = 0.3337337374687195 + 50.0 * 8.334574699401855
Epoch 1990, val loss: 0.47913962602615356
Epoch 2000, training loss: 417.3011169433594 = 0.3320828676223755 + 50.0 * 8.339380264282227
Epoch 2000, val loss: 0.4788958728313446
Epoch 2010, training loss: 417.06298828125 = 0.3303571939468384 + 50.0 * 8.3346529006958
Epoch 2010, val loss: 0.4790917932987213
Epoch 2020, training loss: 417.01373291015625 = 0.3286847770214081 + 50.0 * 8.333701133728027
Epoch 2020, val loss: 0.4787287712097168
Epoch 2030, training loss: 416.98553466796875 = 0.3270203173160553 + 50.0 * 8.333169937133789
Epoch 2030, val loss: 0.4786714017391205
Epoch 2040, training loss: 417.1612243652344 = 0.3253858685493469 + 50.0 * 8.336716651916504
Epoch 2040, val loss: 0.4783467650413513
Epoch 2050, training loss: 416.9737243652344 = 0.32366493344306946 + 50.0 * 8.333001136779785
Epoch 2050, val loss: 0.47848060727119446
Epoch 2060, training loss: 417.0696105957031 = 0.3219758868217468 + 50.0 * 8.334952354431152
Epoch 2060, val loss: 0.4783549904823303
Epoch 2070, training loss: 417.0979309082031 = 0.32027146220207214 + 50.0 * 8.335553169250488
Epoch 2070, val loss: 0.4783851206302643
Epoch 2080, training loss: 416.9508972167969 = 0.3185702860355377 + 50.0 * 8.332646369934082
Epoch 2080, val loss: 0.478426069021225
Epoch 2090, training loss: 416.9090576171875 = 0.31688907742500305 + 50.0 * 8.331843376159668
Epoch 2090, val loss: 0.47852635383605957
Epoch 2100, training loss: 416.936767578125 = 0.3152160346508026 + 50.0 * 8.332430839538574
Epoch 2100, val loss: 0.47854381799697876
Epoch 2110, training loss: 417.18096923828125 = 0.3135366141796112 + 50.0 * 8.337348937988281
Epoch 2110, val loss: 0.4785521328449249
Epoch 2120, training loss: 416.8966064453125 = 0.3118176758289337 + 50.0 * 8.331695556640625
Epoch 2120, val loss: 0.47825974225997925
Epoch 2130, training loss: 416.8966979980469 = 0.31011489033699036 + 50.0 * 8.331731796264648
Epoch 2130, val loss: 0.4783560633659363
Epoch 2140, training loss: 417.1077880859375 = 0.30843624472618103 + 50.0 * 8.335987091064453
Epoch 2140, val loss: 0.4781668186187744
Epoch 2150, training loss: 416.8680725097656 = 0.30671051144599915 + 50.0 * 8.33122730255127
Epoch 2150, val loss: 0.47832053899765015
Epoch 2160, training loss: 416.871826171875 = 0.30500760674476624 + 50.0 * 8.331336975097656
Epoch 2160, val loss: 0.47843581438064575
Epoch 2170, training loss: 416.8808288574219 = 0.303329199552536 + 50.0 * 8.331550598144531
Epoch 2170, val loss: 0.47836771607398987
Epoch 2180, training loss: 416.93560791015625 = 0.3016386330127716 + 50.0 * 8.332679748535156
Epoch 2180, val loss: 0.4784920811653137
Epoch 2190, training loss: 416.949951171875 = 0.2999062240123749 + 50.0 * 8.333001136779785
Epoch 2190, val loss: 0.4789957106113434
Epoch 2200, training loss: 416.8253173828125 = 0.2981841564178467 + 50.0 * 8.33054256439209
Epoch 2200, val loss: 0.47895726561546326
Epoch 2210, training loss: 416.793701171875 = 0.2964732050895691 + 50.0 * 8.329944610595703
Epoch 2210, val loss: 0.47916632890701294
Epoch 2220, training loss: 416.86962890625 = 0.29477912187576294 + 50.0 * 8.331497192382812
Epoch 2220, val loss: 0.47938838601112366
Epoch 2230, training loss: 416.79754638671875 = 0.2930692136287689 + 50.0 * 8.330089569091797
Epoch 2230, val loss: 0.47944268584251404
Epoch 2240, training loss: 416.86505126953125 = 0.29136964678764343 + 50.0 * 8.331473350524902
Epoch 2240, val loss: 0.47957682609558105
Epoch 2250, training loss: 416.9775390625 = 0.28968560695648193 + 50.0 * 8.333757400512695
Epoch 2250, val loss: 0.4795219302177429
Epoch 2260, training loss: 416.8262634277344 = 0.2879246175289154 + 50.0 * 8.330766677856445
Epoch 2260, val loss: 0.4802049994468689
Epoch 2270, training loss: 416.72247314453125 = 0.2862224280834198 + 50.0 * 8.32872486114502
Epoch 2270, val loss: 0.4804118275642395
Epoch 2280, training loss: 416.6912841796875 = 0.2845402956008911 + 50.0 * 8.328134536743164
Epoch 2280, val loss: 0.4807484447956085
Epoch 2290, training loss: 416.757080078125 = 0.28286847472190857 + 50.0 * 8.329483985900879
Epoch 2290, val loss: 0.4813336730003357
Epoch 2300, training loss: 416.9835510253906 = 0.281197190284729 + 50.0 * 8.334047317504883
Epoch 2300, val loss: 0.48185673356056213
Epoch 2310, training loss: 416.7354736328125 = 0.27946463227272034 + 50.0 * 8.329120635986328
Epoch 2310, val loss: 0.48193854093551636
Epoch 2320, training loss: 416.75634765625 = 0.27776437997817993 + 50.0 * 8.329571723937988
Epoch 2320, val loss: 0.4823920726776123
Epoch 2330, training loss: 416.9158935546875 = 0.2761131823062897 + 50.0 * 8.332795143127441
Epoch 2330, val loss: 0.48319917917251587
Epoch 2340, training loss: 416.67401123046875 = 0.27437442541122437 + 50.0 * 8.32799243927002
Epoch 2340, val loss: 0.48288199305534363
Epoch 2350, training loss: 416.6183776855469 = 0.2726839780807495 + 50.0 * 8.326913833618164
Epoch 2350, val loss: 0.4834807813167572
Epoch 2360, training loss: 416.6239318847656 = 0.2710070013999939 + 50.0 * 8.327058792114258
Epoch 2360, val loss: 0.4839412271976471
Epoch 2370, training loss: 416.77508544921875 = 0.2693328559398651 + 50.0 * 8.33011531829834
Epoch 2370, val loss: 0.4843101501464844
Epoch 2380, training loss: 416.7872314453125 = 0.26763439178466797 + 50.0 * 8.330391883850098
Epoch 2380, val loss: 0.48514360189437866
Epoch 2390, training loss: 416.74176025390625 = 0.265911728143692 + 50.0 * 8.329517364501953
Epoch 2390, val loss: 0.48552653193473816
Epoch 2400, training loss: 416.58905029296875 = 0.264205664396286 + 50.0 * 8.326497077941895
Epoch 2400, val loss: 0.48556405305862427
Epoch 2410, training loss: 416.57257080078125 = 0.2625197768211365 + 50.0 * 8.326201438903809
Epoch 2410, val loss: 0.48621588945388794
Epoch 2420, training loss: 416.5911865234375 = 0.26085153222084045 + 50.0 * 8.326606750488281
Epoch 2420, val loss: 0.486782431602478
Epoch 2430, training loss: 416.8197937011719 = 0.25918564200401306 + 50.0 * 8.331212043762207
Epoch 2430, val loss: 0.4873083531856537
Epoch 2440, training loss: 416.6134948730469 = 0.2575037479400635 + 50.0 * 8.327119827270508
Epoch 2440, val loss: 0.48760440945625305
Epoch 2450, training loss: 416.595703125 = 0.25583329796791077 + 50.0 * 8.326797485351562
Epoch 2450, val loss: 0.48809242248535156
Epoch 2460, training loss: 416.77313232421875 = 0.25415846705436707 + 50.0 * 8.330379486083984
Epoch 2460, val loss: 0.4887085258960724
Epoch 2470, training loss: 416.52734375 = 0.2524752914905548 + 50.0 * 8.3254976272583
Epoch 2470, val loss: 0.4896373450756073
Epoch 2480, training loss: 416.5326232910156 = 0.2508252263069153 + 50.0 * 8.32563591003418
Epoch 2480, val loss: 0.490507036447525
Epoch 2490, training loss: 416.52093505859375 = 0.24919170141220093 + 50.0 * 8.325434684753418
Epoch 2490, val loss: 0.4912734925746918
Epoch 2500, training loss: 416.7355651855469 = 0.24759764969348907 + 50.0 * 8.32975959777832
Epoch 2500, val loss: 0.49219346046447754
Epoch 2510, training loss: 416.5796813964844 = 0.24590331315994263 + 50.0 * 8.326675415039062
Epoch 2510, val loss: 0.4921194612979889
Epoch 2520, training loss: 416.59686279296875 = 0.24425949156284332 + 50.0 * 8.327052116394043
Epoch 2520, val loss: 0.49301713705062866
Epoch 2530, training loss: 416.49163818359375 = 0.24261663854122162 + 50.0 * 8.324980735778809
Epoch 2530, val loss: 0.49356573820114136
Epoch 2540, training loss: 416.49456787109375 = 0.24098588526248932 + 50.0 * 8.325071334838867
Epoch 2540, val loss: 0.49413394927978516
Epoch 2550, training loss: 416.5665588378906 = 0.23938144743442535 + 50.0 * 8.326543807983398
Epoch 2550, val loss: 0.4952394664287567
Epoch 2560, training loss: 416.62518310546875 = 0.23777563869953156 + 50.0 * 8.32774829864502
Epoch 2560, val loss: 0.4961617887020111
Epoch 2570, training loss: 416.5913391113281 = 0.2361239492893219 + 50.0 * 8.327104568481445
Epoch 2570, val loss: 0.49649885296821594
Epoch 2580, training loss: 416.50238037109375 = 0.23450051248073578 + 50.0 * 8.325357437133789
Epoch 2580, val loss: 0.4967227876186371
Epoch 2590, training loss: 416.59698486328125 = 0.23288902640342712 + 50.0 * 8.327281951904297
Epoch 2590, val loss: 0.49770447611808777
Epoch 2600, training loss: 416.47467041015625 = 0.231265127658844 + 50.0 * 8.324868202209473
Epoch 2600, val loss: 0.49833667278289795
Epoch 2610, training loss: 416.4386291503906 = 0.22965110838413239 + 50.0 * 8.324179649353027
Epoch 2610, val loss: 0.4993302822113037
Epoch 2620, training loss: 416.3948974609375 = 0.228048637509346 + 50.0 * 8.323336601257324
Epoch 2620, val loss: 0.500210702419281
Epoch 2630, training loss: 416.3965148925781 = 0.2264481633901596 + 50.0 * 8.32340145111084
Epoch 2630, val loss: 0.5008456110954285
Epoch 2640, training loss: 416.616455078125 = 0.22486066818237305 + 50.0 * 8.327832221984863
Epoch 2640, val loss: 0.5013881921768188
Epoch 2650, training loss: 416.8993225097656 = 0.22335375845432281 + 50.0 * 8.333518981933594
Epoch 2650, val loss: 0.5014668703079224
Epoch 2660, training loss: 416.51983642578125 = 0.22165951132774353 + 50.0 * 8.325963973999023
Epoch 2660, val loss: 0.5040424466133118
Epoch 2670, training loss: 416.3896789550781 = 0.22004343569278717 + 50.0 * 8.323392868041992
Epoch 2670, val loss: 0.5038034915924072
Epoch 2680, training loss: 416.34405517578125 = 0.21846306324005127 + 50.0 * 8.322511672973633
Epoch 2680, val loss: 0.5050692558288574
Epoch 2690, training loss: 416.32916259765625 = 0.21689893305301666 + 50.0 * 8.322245597839355
Epoch 2690, val loss: 0.5060228705406189
Epoch 2700, training loss: 416.4068298339844 = 0.21534813940525055 + 50.0 * 8.323829650878906
Epoch 2700, val loss: 0.5071192979812622
Epoch 2710, training loss: 416.6042785644531 = 0.21378642320632935 + 50.0 * 8.327810287475586
Epoch 2710, val loss: 0.5076805949211121
Epoch 2720, training loss: 416.4375 = 0.21221882104873657 + 50.0 * 8.324505805969238
Epoch 2720, val loss: 0.5082172155380249
Epoch 2730, training loss: 416.3498229980469 = 0.21065083146095276 + 50.0 * 8.322783470153809
Epoch 2730, val loss: 0.5099233388900757
Epoch 2740, training loss: 416.3964538574219 = 0.20910446345806122 + 50.0 * 8.323746681213379
Epoch 2740, val loss: 0.5107635855674744
Epoch 2750, training loss: 416.4046630859375 = 0.2075970321893692 + 50.0 * 8.323941230773926
Epoch 2750, val loss: 0.5121477246284485
Epoch 2760, training loss: 416.397705078125 = 0.20603477954864502 + 50.0 * 8.323833465576172
Epoch 2760, val loss: 0.5127522349357605
Epoch 2770, training loss: 416.5480651855469 = 0.20454135537147522 + 50.0 * 8.326870918273926
Epoch 2770, val loss: 0.5142121315002441
Epoch 2780, training loss: 416.271240234375 = 0.20296710729599 + 50.0 * 8.321365356445312
Epoch 2780, val loss: 0.5144696831703186
Epoch 2790, training loss: 416.30450439453125 = 0.20146669447422028 + 50.0 * 8.322060585021973
Epoch 2790, val loss: 0.5153000354766846
Epoch 2800, training loss: 416.2545471191406 = 0.19995437562465668 + 50.0 * 8.321091651916504
Epoch 2800, val loss: 0.5164571404457092
Epoch 2810, training loss: 416.35595703125 = 0.1984822154045105 + 50.0 * 8.323149681091309
Epoch 2810, val loss: 0.5171929597854614
Epoch 2820, training loss: 416.402099609375 = 0.1970125287771225 + 50.0 * 8.324101448059082
Epoch 2820, val loss: 0.5181992650032043
Epoch 2830, training loss: 416.3826904296875 = 0.19549164175987244 + 50.0 * 8.32374382019043
Epoch 2830, val loss: 0.5198347568511963
Epoch 2840, training loss: 416.3039855957031 = 0.19400350749492645 + 50.0 * 8.322199821472168
Epoch 2840, val loss: 0.5210040211677551
Epoch 2850, training loss: 416.34930419921875 = 0.19252170622348785 + 50.0 * 8.323135375976562
Epoch 2850, val loss: 0.5220970511436462
Epoch 2860, training loss: 416.30975341796875 = 0.19104579091072083 + 50.0 * 8.32237434387207
Epoch 2860, val loss: 0.523652970790863
Epoch 2870, training loss: 416.21221923828125 = 0.18955130875110626 + 50.0 * 8.320453643798828
Epoch 2870, val loss: 0.5247074961662292
Epoch 2880, training loss: 416.205810546875 = 0.1880725771188736 + 50.0 * 8.320354461669922
Epoch 2880, val loss: 0.5258065462112427
Epoch 2890, training loss: 416.30950927734375 = 0.18663114309310913 + 50.0 * 8.322457313537598
Epoch 2890, val loss: 0.5276159644126892
Epoch 2900, training loss: 416.43408203125 = 0.18524566292762756 + 50.0 * 8.324976921081543
Epoch 2900, val loss: 0.5293522477149963
Epoch 2910, training loss: 416.2483215332031 = 0.18369396030902863 + 50.0 * 8.321292877197266
Epoch 2910, val loss: 0.5290554165840149
Epoch 2920, training loss: 416.17669677734375 = 0.18223987519741058 + 50.0 * 8.319889068603516
Epoch 2920, val loss: 0.5303716063499451
Epoch 2930, training loss: 416.150390625 = 0.18080492317676544 + 50.0 * 8.319391250610352
Epoch 2930, val loss: 0.5316024422645569
Epoch 2940, training loss: 416.17828369140625 = 0.1793878674507141 + 50.0 * 8.319977760314941
Epoch 2940, val loss: 0.5327708125114441
Epoch 2950, training loss: 416.3580017089844 = 0.1779940277338028 + 50.0 * 8.323599815368652
Epoch 2950, val loss: 0.5339529514312744
Epoch 2960, training loss: 416.1694641113281 = 0.17653918266296387 + 50.0 * 8.31985855102539
Epoch 2960, val loss: 0.5357538461685181
Epoch 2970, training loss: 416.4327697753906 = 0.17514653503894806 + 50.0 * 8.325152397155762
Epoch 2970, val loss: 0.5365323424339294
Epoch 2980, training loss: 416.2205505371094 = 0.17371132969856262 + 50.0 * 8.320937156677246
Epoch 2980, val loss: 0.538667619228363
Epoch 2990, training loss: 416.18011474609375 = 0.1723562479019165 + 50.0 * 8.320155143737793
Epoch 2990, val loss: 0.5407611131668091
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8082191780821917
0.8420633195682099
The final CL Acc:0.81346, 0.00604, The final GNN Acc:0.84296, 0.00071
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110592])
remove edge: torch.Size([2, 66390])
updated graph: torch.Size([2, 88334])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2245483398438 = 1.1117597818374634 + 50.0 * 10.582256317138672
Epoch 0, val loss: 1.1109230518341064
Epoch 10, training loss: 530.1597900390625 = 1.104153037071228 + 50.0 * 10.5811128616333
Epoch 10, val loss: 1.1032480001449585
Epoch 20, training loss: 529.628662109375 = 1.0949946641921997 + 50.0 * 10.570673942565918
Epoch 20, val loss: 1.0940462350845337
Epoch 30, training loss: 525.9910278320312 = 1.0840133428573608 + 50.0 * 10.498141288757324
Epoch 30, val loss: 1.0829756259918213
Epoch 40, training loss: 513.9109497070312 = 1.0716465711593628 + 50.0 * 10.256786346435547
Epoch 40, val loss: 1.0709275007247925
Epoch 50, training loss: 497.1936950683594 = 1.0594432353973389 + 50.0 * 9.922684669494629
Epoch 50, val loss: 1.0587209463119507
Epoch 60, training loss: 474.4910888671875 = 1.048312783241272 + 50.0 * 9.468855857849121
Epoch 60, val loss: 1.0478745698928833
Epoch 70, training loss: 463.5497741699219 = 1.036716341972351 + 50.0 * 9.250261306762695
Epoch 70, val loss: 1.0363305807113647
Epoch 80, training loss: 459.5538330078125 = 1.0254154205322266 + 50.0 * 9.170568466186523
Epoch 80, val loss: 1.0252606868743896
Epoch 90, training loss: 457.5249328613281 = 1.0152901411056519 + 50.0 * 9.130192756652832
Epoch 90, val loss: 1.0154012441635132
Epoch 100, training loss: 454.8344421386719 = 1.006522297859192 + 50.0 * 9.076558113098145
Epoch 100, val loss: 1.006931185722351
Epoch 110, training loss: 451.0106201171875 = 0.9990208148956299 + 50.0 * 9.000231742858887
Epoch 110, val loss: 0.9997425675392151
Epoch 120, training loss: 446.5714416503906 = 0.9924783706665039 + 50.0 * 8.911579132080078
Epoch 120, val loss: 0.9934901595115662
Epoch 130, training loss: 443.4268493652344 = 0.985813319683075 + 50.0 * 8.848820686340332
Epoch 130, val loss: 0.9869171380996704
Epoch 140, training loss: 441.6052551269531 = 0.9774918556213379 + 50.0 * 8.812555313110352
Epoch 140, val loss: 0.9785725474357605
Epoch 150, training loss: 440.4339904785156 = 0.9673959016799927 + 50.0 * 8.789331436157227
Epoch 150, val loss: 0.9684817790985107
Epoch 160, training loss: 439.03656005859375 = 0.9565380811691284 + 50.0 * 8.761600494384766
Epoch 160, val loss: 0.9578495025634766
Epoch 170, training loss: 437.109619140625 = 0.9465411305427551 + 50.0 * 8.723261833190918
Epoch 170, val loss: 0.9482852220535278
Epoch 180, training loss: 435.393310546875 = 0.9376135468482971 + 50.0 * 8.68911361694336
Epoch 180, val loss: 0.9398779273033142
Epoch 190, training loss: 432.7497863769531 = 0.927503764629364 + 50.0 * 8.636445999145508
Epoch 190, val loss: 0.9297852516174316
Epoch 200, training loss: 431.3162536621094 = 0.9152097105979919 + 50.0 * 8.608020782470703
Epoch 200, val loss: 0.9177917242050171
Epoch 210, training loss: 429.9411926269531 = 0.9014171361923218 + 50.0 * 8.580795288085938
Epoch 210, val loss: 0.9044302105903625
Epoch 220, training loss: 428.85565185546875 = 0.8867838978767395 + 50.0 * 8.559377670288086
Epoch 220, val loss: 0.8903511166572571
Epoch 230, training loss: 427.964111328125 = 0.8712849020957947 + 50.0 * 8.54185676574707
Epoch 230, val loss: 0.8754426836967468
Epoch 240, training loss: 427.12921142578125 = 0.855047345161438 + 50.0 * 8.525483131408691
Epoch 240, val loss: 0.8598556518554688
Epoch 250, training loss: 426.4002685546875 = 0.8381712436676025 + 50.0 * 8.511241912841797
Epoch 250, val loss: 0.843649685382843
Epoch 260, training loss: 425.8429870605469 = 0.8208011388778687 + 50.0 * 8.500443458557129
Epoch 260, val loss: 0.8270804286003113
Epoch 270, training loss: 425.3578796386719 = 0.802847146987915 + 50.0 * 8.491100311279297
Epoch 270, val loss: 0.8100084662437439
Epoch 280, training loss: 424.96875 = 0.7846822738647461 + 50.0 * 8.483681678771973
Epoch 280, val loss: 0.7927863597869873
Epoch 290, training loss: 424.5633544921875 = 0.7667618989944458 + 50.0 * 8.475932121276855
Epoch 290, val loss: 0.7758327722549438
Epoch 300, training loss: 424.1803283691406 = 0.7490378618240356 + 50.0 * 8.468626022338867
Epoch 300, val loss: 0.7591696977615356
Epoch 310, training loss: 424.3076171875 = 0.7316025495529175 + 50.0 * 8.47152042388916
Epoch 310, val loss: 0.7428693175315857
Epoch 320, training loss: 423.6224365234375 = 0.7144724726676941 + 50.0 * 8.458159446716309
Epoch 320, val loss: 0.7267489433288574
Epoch 330, training loss: 423.3487548828125 = 0.6981032490730286 + 50.0 * 8.45301342010498
Epoch 330, val loss: 0.7114818096160889
Epoch 340, training loss: 423.05218505859375 = 0.6824741363525391 + 50.0 * 8.447394371032715
Epoch 340, val loss: 0.6969226002693176
Epoch 350, training loss: 422.77801513671875 = 0.6675443649291992 + 50.0 * 8.442209243774414
Epoch 350, val loss: 0.683028519153595
Epoch 360, training loss: 422.7288818359375 = 0.653374433517456 + 50.0 * 8.441510200500488
Epoch 360, val loss: 0.669817328453064
Epoch 370, training loss: 422.4273376464844 = 0.6395825743675232 + 50.0 * 8.435754776000977
Epoch 370, val loss: 0.6571543216705322
Epoch 380, training loss: 422.1124267578125 = 0.6267971992492676 + 50.0 * 8.429712295532227
Epoch 380, val loss: 0.6453605890274048
Epoch 390, training loss: 421.9027404785156 = 0.6148682832717896 + 50.0 * 8.42575740814209
Epoch 390, val loss: 0.6343790888786316
Epoch 400, training loss: 421.9326477050781 = 0.6036350727081299 + 50.0 * 8.426580429077148
Epoch 400, val loss: 0.6240788698196411
Epoch 410, training loss: 421.49200439453125 = 0.5928775668144226 + 50.0 * 8.417983055114746
Epoch 410, val loss: 0.6141980886459351
Epoch 420, training loss: 421.2946472167969 = 0.5828811526298523 + 50.0 * 8.41423511505127
Epoch 420, val loss: 0.605141818523407
Epoch 430, training loss: 421.1352844238281 = 0.5736509561538696 + 50.0 * 8.411232948303223
Epoch 430, val loss: 0.5967777967453003
Epoch 440, training loss: 421.0616149902344 = 0.5649569034576416 + 50.0 * 8.409933090209961
Epoch 440, val loss: 0.5889384150505066
Epoch 450, training loss: 420.8626708984375 = 0.5567136406898499 + 50.0 * 8.406119346618652
Epoch 450, val loss: 0.5815268158912659
Epoch 460, training loss: 420.663330078125 = 0.5490982532501221 + 50.0 * 8.402284622192383
Epoch 460, val loss: 0.5747592449188232
Epoch 470, training loss: 420.51104736328125 = 0.5420508980751038 + 50.0 * 8.39937973022461
Epoch 470, val loss: 0.568526029586792
Epoch 480, training loss: 420.80804443359375 = 0.5353947281837463 + 50.0 * 8.405452728271484
Epoch 480, val loss: 0.5627250671386719
Epoch 490, training loss: 420.3204040527344 = 0.5290628671646118 + 50.0 * 8.395827293395996
Epoch 490, val loss: 0.5571631193161011
Epoch 500, training loss: 420.1370849609375 = 0.5232874751091003 + 50.0 * 8.3922758102417
Epoch 500, val loss: 0.5522095561027527
Epoch 510, training loss: 419.9704895019531 = 0.5179226398468018 + 50.0 * 8.38905143737793
Epoch 510, val loss: 0.5476275086402893
Epoch 520, training loss: 419.90118408203125 = 0.5128549337387085 + 50.0 * 8.38776683807373
Epoch 520, val loss: 0.5433695912361145
Epoch 530, training loss: 420.0758361816406 = 0.5079646110534668 + 50.0 * 8.391357421875
Epoch 530, val loss: 0.5392614006996155
Epoch 540, training loss: 419.7696533203125 = 0.5034208297729492 + 50.0 * 8.385324478149414
Epoch 540, val loss: 0.5355756282806396
Epoch 550, training loss: 419.5908508300781 = 0.49929699301719666 + 50.0 * 8.381831169128418
Epoch 550, val loss: 0.5322055816650391
Epoch 560, training loss: 419.46038818359375 = 0.49540114402770996 + 50.0 * 8.379300117492676
Epoch 560, val loss: 0.5290584564208984
Epoch 570, training loss: 419.44287109375 = 0.49171674251556396 + 50.0 * 8.379022598266602
Epoch 570, val loss: 0.5261644721031189
Epoch 580, training loss: 419.2778625488281 = 0.48813316226005554 + 50.0 * 8.375794410705566
Epoch 580, val loss: 0.5233360528945923
Epoch 590, training loss: 419.22393798828125 = 0.4848063886165619 + 50.0 * 8.37478256225586
Epoch 590, val loss: 0.5208383202552795
Epoch 600, training loss: 419.1384582519531 = 0.4817064106464386 + 50.0 * 8.37313461303711
Epoch 600, val loss: 0.5184569954872131
Epoch 610, training loss: 419.0446472167969 = 0.478771835565567 + 50.0 * 8.371317863464355
Epoch 610, val loss: 0.5163000226020813
Epoch 620, training loss: 419.0513610839844 = 0.47591978311538696 + 50.0 * 8.371508598327637
Epoch 620, val loss: 0.5141945481300354
Epoch 630, training loss: 418.9203796386719 = 0.4732092320919037 + 50.0 * 8.368943214416504
Epoch 630, val loss: 0.5122592449188232
Epoch 640, training loss: 418.8489990234375 = 0.4707072973251343 + 50.0 * 8.367566108703613
Epoch 640, val loss: 0.5104812383651733
Epoch 650, training loss: 419.07879638671875 = 0.46825990080833435 + 50.0 * 8.372210502624512
Epoch 650, val loss: 0.5087443590164185
Epoch 660, training loss: 418.70703125 = 0.4658982753753662 + 50.0 * 8.364822387695312
Epoch 660, val loss: 0.5072082281112671
Epoch 670, training loss: 418.61004638671875 = 0.46372613310813904 + 50.0 * 8.362926483154297
Epoch 670, val loss: 0.5057370066642761
Epoch 680, training loss: 418.7706298828125 = 0.4616166055202484 + 50.0 * 8.366180419921875
Epoch 680, val loss: 0.5043063759803772
Epoch 690, training loss: 418.5367736816406 = 0.4595842659473419 + 50.0 * 8.361543655395508
Epoch 690, val loss: 0.5030947327613831
Epoch 700, training loss: 418.4113464355469 = 0.45767828822135925 + 50.0 * 8.359073638916016
Epoch 700, val loss: 0.5018622875213623
Epoch 710, training loss: 418.4093933105469 = 0.45583850145339966 + 50.0 * 8.359070777893066
Epoch 710, val loss: 0.5007285475730896
Epoch 720, training loss: 418.51275634765625 = 0.4540262222290039 + 50.0 * 8.361174583435059
Epoch 720, val loss: 0.4995914101600647
Epoch 730, training loss: 418.4847412109375 = 0.4522443413734436 + 50.0 * 8.360650062561035
Epoch 730, val loss: 0.4986027479171753
Epoch 740, training loss: 418.1967468261719 = 0.4505440592765808 + 50.0 * 8.354924201965332
Epoch 740, val loss: 0.4975569546222687
Epoch 750, training loss: 418.15753173828125 = 0.44894614815711975 + 50.0 * 8.354171752929688
Epoch 750, val loss: 0.49664825201034546
Epoch 760, training loss: 418.29913330078125 = 0.44736167788505554 + 50.0 * 8.357035636901855
Epoch 760, val loss: 0.49575507640838623
Epoch 770, training loss: 418.0261535644531 = 0.445816308259964 + 50.0 * 8.351606369018555
Epoch 770, val loss: 0.49493709206581116
Epoch 780, training loss: 417.9981384277344 = 0.44434866309165955 + 50.0 * 8.351076126098633
Epoch 780, val loss: 0.4941442608833313
Epoch 790, training loss: 417.9460754394531 = 0.442912757396698 + 50.0 * 8.35006332397461
Epoch 790, val loss: 0.49341410398483276
Epoch 800, training loss: 418.03515625 = 0.4414941370487213 + 50.0 * 8.351873397827148
Epoch 800, val loss: 0.4926694631576538
Epoch 810, training loss: 417.9203186035156 = 0.4400923252105713 + 50.0 * 8.349604606628418
Epoch 810, val loss: 0.4919773042201996
Epoch 820, training loss: 418.2809753417969 = 0.4387221038341522 + 50.0 * 8.356844902038574
Epoch 820, val loss: 0.4912257194519043
Epoch 830, training loss: 417.9029541015625 = 0.437328040599823 + 50.0 * 8.349312782287598
Epoch 830, val loss: 0.49052461981773376
Epoch 840, training loss: 417.7472229003906 = 0.436054527759552 + 50.0 * 8.346222877502441
Epoch 840, val loss: 0.48992717266082764
Epoch 850, training loss: 417.67266845703125 = 0.4348091185092926 + 50.0 * 8.344757080078125
Epoch 850, val loss: 0.489300012588501
Epoch 860, training loss: 417.6265869140625 = 0.43359312415122986 + 50.0 * 8.343859672546387
Epoch 860, val loss: 0.4887622892856598
Epoch 870, training loss: 417.9525146484375 = 0.4323854446411133 + 50.0 * 8.35040283203125
Epoch 870, val loss: 0.4881707429885864
Epoch 880, training loss: 417.68389892578125 = 0.4311198592185974 + 50.0 * 8.34505558013916
Epoch 880, val loss: 0.4876371920108795
Epoch 890, training loss: 417.51513671875 = 0.42993706464767456 + 50.0 * 8.341704368591309
Epoch 890, val loss: 0.4870777726173401
Epoch 900, training loss: 417.4820861816406 = 0.4287906587123871 + 50.0 * 8.341065406799316
Epoch 900, val loss: 0.48652100563049316
Epoch 910, training loss: 417.4284973144531 = 0.427667498588562 + 50.0 * 8.34001636505127
Epoch 910, val loss: 0.48608583211898804
Epoch 920, training loss: 417.6547546386719 = 0.4265584349632263 + 50.0 * 8.344564437866211
Epoch 920, val loss: 0.48560357093811035
Epoch 930, training loss: 417.59161376953125 = 0.42538997530937195 + 50.0 * 8.343324661254883
Epoch 930, val loss: 0.484997034072876
Epoch 940, training loss: 417.8307800292969 = 0.4242555797100067 + 50.0 * 8.348130226135254
Epoch 940, val loss: 0.484421044588089
Epoch 950, training loss: 417.4482727050781 = 0.4231061339378357 + 50.0 * 8.340503692626953
Epoch 950, val loss: 0.48401880264282227
Epoch 960, training loss: 417.308349609375 = 0.42204344272613525 + 50.0 * 8.337725639343262
Epoch 960, val loss: 0.48348137736320496
Epoch 970, training loss: 417.24041748046875 = 0.42100316286087036 + 50.0 * 8.33638858795166
Epoch 970, val loss: 0.48311883211135864
Epoch 980, training loss: 417.18963623046875 = 0.41998228430747986 + 50.0 * 8.335392951965332
Epoch 980, val loss: 0.4826861321926117
Epoch 990, training loss: 417.21026611328125 = 0.41896307468414307 + 50.0 * 8.33582592010498
Epoch 990, val loss: 0.4822791814804077
Epoch 1000, training loss: 417.24853515625 = 0.4179235100746155 + 50.0 * 8.3366117477417
Epoch 1000, val loss: 0.48184242844581604
Epoch 1010, training loss: 417.4773864746094 = 0.4168795943260193 + 50.0 * 8.34121036529541
Epoch 1010, val loss: 0.4814112186431885
Epoch 1020, training loss: 417.099853515625 = 0.41580232977867126 + 50.0 * 8.333681106567383
Epoch 1020, val loss: 0.4808671772480011
Epoch 1030, training loss: 417.0472412109375 = 0.4147856831550598 + 50.0 * 8.332649230957031
Epoch 1030, val loss: 0.48042967915534973
Epoch 1040, training loss: 417.0238952636719 = 0.41379693150520325 + 50.0 * 8.332201957702637
Epoch 1040, val loss: 0.48003602027893066
Epoch 1050, training loss: 417.00201416015625 = 0.41281452775001526 + 50.0 * 8.33178424835205
Epoch 1050, val loss: 0.4796082079410553
Epoch 1060, training loss: 417.30670166015625 = 0.4118212163448334 + 50.0 * 8.337897300720215
Epoch 1060, val loss: 0.47925135493278503
Epoch 1070, training loss: 416.992919921875 = 0.4107784032821655 + 50.0 * 8.331643104553223
Epoch 1070, val loss: 0.4786651134490967
Epoch 1080, training loss: 416.97894287109375 = 0.4097799062728882 + 50.0 * 8.331382751464844
Epoch 1080, val loss: 0.4782525300979614
Epoch 1090, training loss: 417.1577453613281 = 0.40877091884613037 + 50.0 * 8.334979057312012
Epoch 1090, val loss: 0.47773873805999756
Epoch 1100, training loss: 416.9011535644531 = 0.40774327516555786 + 50.0 * 8.32986831665039
Epoch 1100, val loss: 0.47738146781921387
Epoch 1110, training loss: 416.8175048828125 = 0.40676310658454895 + 50.0 * 8.328214645385742
Epoch 1110, val loss: 0.4769197404384613
Epoch 1120, training loss: 416.8199462890625 = 0.40578749775886536 + 50.0 * 8.328283309936523
Epoch 1120, val loss: 0.4764960706233978
Epoch 1130, training loss: 416.7926940917969 = 0.40480345487594604 + 50.0 * 8.327757835388184
Epoch 1130, val loss: 0.4760775566101074
Epoch 1140, training loss: 417.0172424316406 = 0.4038078188896179 + 50.0 * 8.332268714904785
Epoch 1140, val loss: 0.47569945454597473
Epoch 1150, training loss: 416.9427795410156 = 0.40276214480400085 + 50.0 * 8.33080005645752
Epoch 1150, val loss: 0.47512194514274597
Epoch 1160, training loss: 416.7178649902344 = 0.40173155069351196 + 50.0 * 8.326322555541992
Epoch 1160, val loss: 0.4746720492839813
Epoch 1170, training loss: 416.6620178222656 = 0.40073922276496887 + 50.0 * 8.325225830078125
Epoch 1170, val loss: 0.47424155473709106
Epoch 1180, training loss: 416.6425476074219 = 0.399762362241745 + 50.0 * 8.32485580444336
Epoch 1180, val loss: 0.47380951046943665
Epoch 1190, training loss: 416.7262878417969 = 0.3987800180912018 + 50.0 * 8.326550483703613
Epoch 1190, val loss: 0.4733811020851135
Epoch 1200, training loss: 416.6980895996094 = 0.39774754643440247 + 50.0 * 8.326006889343262
Epoch 1200, val loss: 0.4729427099227905
Epoch 1210, training loss: 416.5969543457031 = 0.3967115879058838 + 50.0 * 8.324005126953125
Epoch 1210, val loss: 0.4724183976650238
Epoch 1220, training loss: 416.67901611328125 = 0.3956886827945709 + 50.0 * 8.325666427612305
Epoch 1220, val loss: 0.4719125032424927
Epoch 1230, training loss: 416.58123779296875 = 0.3946543037891388 + 50.0 * 8.323731422424316
Epoch 1230, val loss: 0.47145694494247437
Epoch 1240, training loss: 416.5971984863281 = 0.3936242163181305 + 50.0 * 8.324071884155273
Epoch 1240, val loss: 0.4709642827510834
Epoch 1250, training loss: 416.56842041015625 = 0.39258524775505066 + 50.0 * 8.323516845703125
Epoch 1250, val loss: 0.4704643189907074
Epoch 1260, training loss: 416.4737243652344 = 0.39154091477394104 + 50.0 * 8.321643829345703
Epoch 1260, val loss: 0.4699706733226776
Epoch 1270, training loss: 416.67034912109375 = 0.3904975354671478 + 50.0 * 8.325596809387207
Epoch 1270, val loss: 0.4695563018321991
Epoch 1280, training loss: 416.5508728027344 = 0.38938790559768677 + 50.0 * 8.323229789733887
Epoch 1280, val loss: 0.4688240587711334
Epoch 1290, training loss: 416.4866638183594 = 0.38827773928642273 + 50.0 * 8.321968078613281
Epoch 1290, val loss: 0.46834298968315125
Epoch 1300, training loss: 416.38623046875 = 0.38720446825027466 + 50.0 * 8.31998062133789
Epoch 1300, val loss: 0.4678284823894501
Epoch 1310, training loss: 416.3671569824219 = 0.3861433267593384 + 50.0 * 8.319620132446289
Epoch 1310, val loss: 0.4673064947128296
Epoch 1320, training loss: 416.3794250488281 = 0.3850717544555664 + 50.0 * 8.319887161254883
Epoch 1320, val loss: 0.46684369444847107
Epoch 1330, training loss: 416.78509521484375 = 0.38394537568092346 + 50.0 * 8.328022956848145
Epoch 1330, val loss: 0.4662927985191345
Epoch 1340, training loss: 416.3197326660156 = 0.38276612758636475 + 50.0 * 8.318739891052246
Epoch 1340, val loss: 0.4656517803668976
Epoch 1350, training loss: 416.3182373046875 = 0.3816375732421875 + 50.0 * 8.318732261657715
Epoch 1350, val loss: 0.4651040732860565
Epoch 1360, training loss: 416.2869567871094 = 0.3805159330368042 + 50.0 * 8.31812858581543
Epoch 1360, val loss: 0.4645918011665344
Epoch 1370, training loss: 416.2566833496094 = 0.37940075993537903 + 50.0 * 8.317545890808105
Epoch 1370, val loss: 0.46402332186698914
Epoch 1380, training loss: 416.4915466308594 = 0.3782632350921631 + 50.0 * 8.322265625
Epoch 1380, val loss: 0.46342703700065613
Epoch 1390, training loss: 416.3497314453125 = 0.37707778811454773 + 50.0 * 8.319453239440918
Epoch 1390, val loss: 0.4629080891609192
Epoch 1400, training loss: 416.31756591796875 = 0.37587660551071167 + 50.0 * 8.318833351135254
Epoch 1400, val loss: 0.462178111076355
Epoch 1410, training loss: 416.2114562988281 = 0.3746885061264038 + 50.0 * 8.31673526763916
Epoch 1410, val loss: 0.46156439185142517
Epoch 1420, training loss: 416.1695556640625 = 0.373503714799881 + 50.0 * 8.31592082977295
Epoch 1420, val loss: 0.46102964878082275
Epoch 1430, training loss: 416.1615905761719 = 0.3723154366016388 + 50.0 * 8.31578540802002
Epoch 1430, val loss: 0.4604004919528961
Epoch 1440, training loss: 416.3328552246094 = 0.3711141347885132 + 50.0 * 8.319234848022461
Epoch 1440, val loss: 0.4597671627998352
Epoch 1450, training loss: 416.4178771972656 = 0.369852215051651 + 50.0 * 8.320960998535156
Epoch 1450, val loss: 0.45913007855415344
Epoch 1460, training loss: 416.1747741699219 = 0.3685648441314697 + 50.0 * 8.316123962402344
Epoch 1460, val loss: 0.45858073234558105
Epoch 1470, training loss: 416.1142272949219 = 0.3673016130924225 + 50.0 * 8.31493854522705
Epoch 1470, val loss: 0.4579070508480072
Epoch 1480, training loss: 416.0778503417969 = 0.3660533130168915 + 50.0 * 8.31423568725586
Epoch 1480, val loss: 0.45731085538864136
Epoch 1490, training loss: 416.08905029296875 = 0.3648042380809784 + 50.0 * 8.314484596252441
Epoch 1490, val loss: 0.45671287178993225
Epoch 1500, training loss: 416.2249450683594 = 0.36352723836898804 + 50.0 * 8.317228317260742
Epoch 1500, val loss: 0.45612242817878723
Epoch 1510, training loss: 416.1104431152344 = 0.3622133731842041 + 50.0 * 8.314964294433594
Epoch 1510, val loss: 0.45548897981643677
Epoch 1520, training loss: 416.23944091796875 = 0.36091136932373047 + 50.0 * 8.317570686340332
Epoch 1520, val loss: 0.454937219619751
Epoch 1530, training loss: 416.1791687011719 = 0.35955217480659485 + 50.0 * 8.316391944885254
Epoch 1530, val loss: 0.4541831612586975
Epoch 1540, training loss: 416.0504150390625 = 0.3582058250904083 + 50.0 * 8.313843727111816
Epoch 1540, val loss: 0.45346736907958984
Epoch 1550, training loss: 415.9952697753906 = 0.35687267780303955 + 50.0 * 8.31276798248291
Epoch 1550, val loss: 0.45291396975517273
Epoch 1560, training loss: 415.9505310058594 = 0.3555532991886139 + 50.0 * 8.311899185180664
Epoch 1560, val loss: 0.4522337019443512
Epoch 1570, training loss: 415.9353942871094 = 0.35422512888908386 + 50.0 * 8.311623573303223
Epoch 1570, val loss: 0.4516616463661194
Epoch 1580, training loss: 416.17156982421875 = 0.3528813421726227 + 50.0 * 8.316373825073242
Epoch 1580, val loss: 0.4510996639728546
Epoch 1590, training loss: 416.0101318359375 = 0.35148119926452637 + 50.0 * 8.313173294067383
Epoch 1590, val loss: 0.45036956667900085
Epoch 1600, training loss: 415.9897766113281 = 0.3500865399837494 + 50.0 * 8.312793731689453
Epoch 1600, val loss: 0.4498567581176758
Epoch 1610, training loss: 416.00262451171875 = 0.3486904203891754 + 50.0 * 8.313078880310059
Epoch 1610, val loss: 0.4491305947303772
Epoch 1620, training loss: 415.8932800292969 = 0.34729433059692383 + 50.0 * 8.310919761657715
Epoch 1620, val loss: 0.4485340118408203
Epoch 1630, training loss: 415.861572265625 = 0.3459039032459259 + 50.0 * 8.31031322479248
Epoch 1630, val loss: 0.4478624761104584
Epoch 1640, training loss: 415.9895324707031 = 0.3445098400115967 + 50.0 * 8.31290054321289
Epoch 1640, val loss: 0.4472824037075043
Epoch 1650, training loss: 415.9345703125 = 0.3430761396884918 + 50.0 * 8.311829566955566
Epoch 1650, val loss: 0.4466179311275482
Epoch 1660, training loss: 415.8717956542969 = 0.34163200855255127 + 50.0 * 8.310603141784668
Epoch 1660, val loss: 0.4459928274154663
Epoch 1670, training loss: 415.98443603515625 = 0.34018686413764954 + 50.0 * 8.312885284423828
Epoch 1670, val loss: 0.4453963339328766
Epoch 1680, training loss: 415.79864501953125 = 0.3387235403060913 + 50.0 * 8.309198379516602
Epoch 1680, val loss: 0.4446500539779663
Epoch 1690, training loss: 415.75732421875 = 0.33727604150772095 + 50.0 * 8.308401107788086
Epoch 1690, val loss: 0.4440373480319977
Epoch 1700, training loss: 415.79119873046875 = 0.33583304286003113 + 50.0 * 8.309106826782227
Epoch 1700, val loss: 0.4434449374675751
Epoch 1710, training loss: 416.1034240722656 = 0.3343762159347534 + 50.0 * 8.315381050109863
Epoch 1710, val loss: 0.4427041709423065
Epoch 1720, training loss: 415.8002624511719 = 0.33285126090049744 + 50.0 * 8.309348106384277
Epoch 1720, val loss: 0.44226399064064026
Epoch 1730, training loss: 415.73223876953125 = 0.33136868476867676 + 50.0 * 8.30801773071289
Epoch 1730, val loss: 0.44161200523376465
Epoch 1740, training loss: 415.7265319824219 = 0.32989534735679626 + 50.0 * 8.30793285369873
Epoch 1740, val loss: 0.4409886300563812
Epoch 1750, training loss: 415.8998107910156 = 0.3284114599227905 + 50.0 * 8.31142807006836
Epoch 1750, val loss: 0.4403665363788605
Epoch 1760, training loss: 415.81268310546875 = 0.32689523696899414 + 50.0 * 8.309715270996094
Epoch 1760, val loss: 0.4399030804634094
Epoch 1770, training loss: 415.7359313964844 = 0.32537275552749634 + 50.0 * 8.308211326599121
Epoch 1770, val loss: 0.4392900764942169
Epoch 1780, training loss: 415.7357177734375 = 0.32386305928230286 + 50.0 * 8.308237075805664
Epoch 1780, val loss: 0.4387962818145752
Epoch 1790, training loss: 415.6995849609375 = 0.32234933972358704 + 50.0 * 8.307544708251953
Epoch 1790, val loss: 0.4382093548774719
Epoch 1800, training loss: 415.6903381347656 = 0.3208341896533966 + 50.0 * 8.307390213012695
Epoch 1800, val loss: 0.43764564394950867
Epoch 1810, training loss: 415.8080749511719 = 0.319317102432251 + 50.0 * 8.309775352478027
Epoch 1810, val loss: 0.43714281916618347
Epoch 1820, training loss: 415.6608581542969 = 0.3177732229232788 + 50.0 * 8.306861877441406
Epoch 1820, val loss: 0.43653640151023865
Epoch 1830, training loss: 415.7142333984375 = 0.3162400722503662 + 50.0 * 8.30795955657959
Epoch 1830, val loss: 0.4359561800956726
Epoch 1840, training loss: 415.62060546875 = 0.31468766927719116 + 50.0 * 8.30611801147461
Epoch 1840, val loss: 0.4354766011238098
Epoch 1850, training loss: 415.58050537109375 = 0.3131483495235443 + 50.0 * 8.305347442626953
Epoch 1850, val loss: 0.4350578188896179
Epoch 1860, training loss: 415.5575256347656 = 0.31161829829216003 + 50.0 * 8.30491828918457
Epoch 1860, val loss: 0.4345964193344116
Epoch 1870, training loss: 415.6134948730469 = 0.3100961446762085 + 50.0 * 8.306068420410156
Epoch 1870, val loss: 0.43409278988838196
Epoch 1880, training loss: 415.8551330566406 = 0.30856049060821533 + 50.0 * 8.310931205749512
Epoch 1880, val loss: 0.4334719181060791
Epoch 1890, training loss: 415.5634765625 = 0.3069658577442169 + 50.0 * 8.305130004882812
Epoch 1890, val loss: 0.4332816004753113
Epoch 1900, training loss: 415.5102233886719 = 0.30541762709617615 + 50.0 * 8.304096221923828
Epoch 1900, val loss: 0.43280136585235596
Epoch 1910, training loss: 415.497314453125 = 0.30388006567955017 + 50.0 * 8.303868293762207
Epoch 1910, val loss: 0.4323297142982483
Epoch 1920, training loss: 415.5094299316406 = 0.3023488223552704 + 50.0 * 8.304141998291016
Epoch 1920, val loss: 0.43204015493392944
Epoch 1930, training loss: 415.81390380859375 = 0.30080828070640564 + 50.0 * 8.310261726379395
Epoch 1930, val loss: 0.4317117929458618
Epoch 1940, training loss: 415.56048583984375 = 0.2992385923862457 + 50.0 * 8.305225372314453
Epoch 1940, val loss: 0.4312783181667328
Epoch 1950, training loss: 415.5455322265625 = 0.2976662814617157 + 50.0 * 8.304957389831543
Epoch 1950, val loss: 0.43067467212677
Epoch 1960, training loss: 415.6017761230469 = 0.29610779881477356 + 50.0 * 8.306113243103027
Epoch 1960, val loss: 0.4304009675979614
Epoch 1970, training loss: 415.4841613769531 = 0.29453417658805847 + 50.0 * 8.303792953491211
Epoch 1970, val loss: 0.43016475439071655
Epoch 1980, training loss: 415.4088439941406 = 0.2929874658584595 + 50.0 * 8.302316665649414
Epoch 1980, val loss: 0.42985665798187256
Epoch 1990, training loss: 415.4289855957031 = 0.2914399802684784 + 50.0 * 8.302750587463379
Epoch 1990, val loss: 0.4295705258846283
Epoch 2000, training loss: 415.6417541503906 = 0.28989970684051514 + 50.0 * 8.307037353515625
Epoch 2000, val loss: 0.4295046031475067
Epoch 2010, training loss: 415.50433349609375 = 0.288318395614624 + 50.0 * 8.304320335388184
Epoch 2010, val loss: 0.4291844964027405
Epoch 2020, training loss: 415.4095764160156 = 0.2867485284805298 + 50.0 * 8.302456855773926
Epoch 2020, val loss: 0.4287429451942444
Epoch 2030, training loss: 415.4026794433594 = 0.2851971685886383 + 50.0 * 8.302350044250488
Epoch 2030, val loss: 0.42861661314964294
Epoch 2040, training loss: 415.4421081542969 = 0.2836468517780304 + 50.0 * 8.303169250488281
Epoch 2040, val loss: 0.4284452199935913
Epoch 2050, training loss: 415.4225769042969 = 0.28209590911865234 + 50.0 * 8.302809715270996
Epoch 2050, val loss: 0.42810848355293274
Epoch 2060, training loss: 415.40435791015625 = 0.28053775429725647 + 50.0 * 8.302475929260254
Epoch 2060, val loss: 0.42796429991722107
Epoch 2070, training loss: 415.4349670410156 = 0.2789791524410248 + 50.0 * 8.303119659423828
Epoch 2070, val loss: 0.4278092086315155
Epoch 2080, training loss: 415.3906555175781 = 0.2774253189563751 + 50.0 * 8.302264213562012
Epoch 2080, val loss: 0.4276948571205139
Epoch 2090, training loss: 415.3263244628906 = 0.27586427330970764 + 50.0 * 8.301009178161621
Epoch 2090, val loss: 0.4276331961154938
Epoch 2100, training loss: 415.3046569824219 = 0.274319052696228 + 50.0 * 8.300606727600098
Epoch 2100, val loss: 0.4275883734226227
Epoch 2110, training loss: 415.3337707519531 = 0.2727798521518707 + 50.0 * 8.301219940185547
Epoch 2110, val loss: 0.42756691575050354
Epoch 2120, training loss: 415.3971252441406 = 0.2712342143058777 + 50.0 * 8.302517890930176
Epoch 2120, val loss: 0.4274633526802063
Epoch 2130, training loss: 415.3625183105469 = 0.26968511939048767 + 50.0 * 8.301856994628906
Epoch 2130, val loss: 0.42731615900993347
Epoch 2140, training loss: 415.6429748535156 = 0.26815328001976013 + 50.0 * 8.307496070861816
Epoch 2140, val loss: 0.4271819591522217
Epoch 2150, training loss: 415.32037353515625 = 0.26657092571258545 + 50.0 * 8.30107593536377
Epoch 2150, val loss: 0.42756447196006775
Epoch 2160, training loss: 415.2482604980469 = 0.2650303542613983 + 50.0 * 8.299664497375488
Epoch 2160, val loss: 0.42741018533706665
Epoch 2170, training loss: 415.20220947265625 = 0.2635023593902588 + 50.0 * 8.298774719238281
Epoch 2170, val loss: 0.4274613857269287
Epoch 2180, training loss: 415.2159729003906 = 0.2619801163673401 + 50.0 * 8.299079895019531
Epoch 2180, val loss: 0.4275459945201874
Epoch 2190, training loss: 415.436279296875 = 0.260460764169693 + 50.0 * 8.303516387939453
Epoch 2190, val loss: 0.4277195632457733
Epoch 2200, training loss: 415.3260192871094 = 0.2589133083820343 + 50.0 * 8.301342010498047
Epoch 2200, val loss: 0.42783278226852417
Epoch 2210, training loss: 415.2186279296875 = 0.25737082958221436 + 50.0 * 8.299224853515625
Epoch 2210, val loss: 0.427879273891449
Epoch 2220, training loss: 415.2297668457031 = 0.25583788752555847 + 50.0 * 8.299478530883789
Epoch 2220, val loss: 0.42788374423980713
Epoch 2230, training loss: 415.1749572753906 = 0.2543177902698517 + 50.0 * 8.298413276672363
Epoch 2230, val loss: 0.42811319231987
Epoch 2240, training loss: 415.2392272949219 = 0.2528104782104492 + 50.0 * 8.299728393554688
Epoch 2240, val loss: 0.4283270239830017
Epoch 2250, training loss: 415.14801025390625 = 0.2512912452220917 + 50.0 * 8.297934532165527
Epoch 2250, val loss: 0.4284375011920929
Epoch 2260, training loss: 415.27630615234375 = 0.24979349970817566 + 50.0 * 8.300530433654785
Epoch 2260, val loss: 0.42862173914909363
Epoch 2270, training loss: 415.3276062011719 = 0.24829471111297607 + 50.0 * 8.301586151123047
Epoch 2270, val loss: 0.42902296781539917
Epoch 2280, training loss: 415.144287109375 = 0.24676069617271423 + 50.0 * 8.297950744628906
Epoch 2280, val loss: 0.4287266731262207
Epoch 2290, training loss: 415.0857238769531 = 0.24526196718215942 + 50.0 * 8.296809196472168
Epoch 2290, val loss: 0.4290454685688019
Epoch 2300, training loss: 415.07244873046875 = 0.24377226829528809 + 50.0 * 8.296573638916016
Epoch 2300, val loss: 0.42920389771461487
Epoch 2310, training loss: 415.085205078125 = 0.24229176342487335 + 50.0 * 8.296858787536621
Epoch 2310, val loss: 0.4293974041938782
Epoch 2320, training loss: 415.3990783691406 = 0.24083004891872406 + 50.0 * 8.3031644821167
Epoch 2320, val loss: 0.4294927716255188
Epoch 2330, training loss: 415.0862121582031 = 0.23930424451828003 + 50.0 * 8.296937942504883
Epoch 2330, val loss: 0.43001100420951843
Epoch 2340, training loss: 415.0246276855469 = 0.2378188967704773 + 50.0 * 8.295736312866211
Epoch 2340, val loss: 0.430307000875473
Epoch 2350, training loss: 415.1580810546875 = 0.23633895814418793 + 50.0 * 8.29843521118164
Epoch 2350, val loss: 0.4306122362613678
Epoch 2360, training loss: 415.0628356933594 = 0.23484918475151062 + 50.0 * 8.29655933380127
Epoch 2360, val loss: 0.43114182353019714
Epoch 2370, training loss: 415.2002868652344 = 0.23336461186408997 + 50.0 * 8.299338340759277
Epoch 2370, val loss: 0.4313684105873108
Epoch 2380, training loss: 415.0186462402344 = 0.23185913264751434 + 50.0 * 8.295735359191895
Epoch 2380, val loss: 0.43172648549079895
Epoch 2390, training loss: 415.0194396972656 = 0.23037512600421906 + 50.0 * 8.295781135559082
Epoch 2390, val loss: 0.43192362785339355
Epoch 2400, training loss: 415.09613037109375 = 0.22891870141029358 + 50.0 * 8.297344207763672
Epoch 2400, val loss: 0.4326820373535156
Epoch 2410, training loss: 415.06915283203125 = 0.2274225354194641 + 50.0 * 8.296834945678711
Epoch 2410, val loss: 0.4327526092529297
Epoch 2420, training loss: 414.99945068359375 = 0.22594764828681946 + 50.0 * 8.295470237731934
Epoch 2420, val loss: 0.4328269362449646
Epoch 2430, training loss: 414.9482116699219 = 0.2244756817817688 + 50.0 * 8.294474601745605
Epoch 2430, val loss: 0.4333215355873108
Epoch 2440, training loss: 414.98419189453125 = 0.22301875054836273 + 50.0 * 8.295223236083984
Epoch 2440, val loss: 0.43391138315200806
Epoch 2450, training loss: 415.1293029785156 = 0.22156816720962524 + 50.0 * 8.298154830932617
Epoch 2450, val loss: 0.434460312128067
Epoch 2460, training loss: 415.2098388671875 = 0.220102459192276 + 50.0 * 8.299795150756836
Epoch 2460, val loss: 0.43481162190437317
Epoch 2470, training loss: 414.9599609375 = 0.21861928701400757 + 50.0 * 8.29482650756836
Epoch 2470, val loss: 0.4349082112312317
Epoch 2480, training loss: 414.89935302734375 = 0.21716421842575073 + 50.0 * 8.293643951416016
Epoch 2480, val loss: 0.4354836940765381
Epoch 2490, training loss: 414.9267272949219 = 0.215719074010849 + 50.0 * 8.294219970703125
Epoch 2490, val loss: 0.43596795201301575
Epoch 2500, training loss: 415.2342224121094 = 0.21429376304149628 + 50.0 * 8.300398826599121
Epoch 2500, val loss: 0.43678003549575806
Epoch 2510, training loss: 414.9758605957031 = 0.21282291412353516 + 50.0 * 8.295260429382324
Epoch 2510, val loss: 0.4368932545185089
Epoch 2520, training loss: 414.8861083984375 = 0.21137024462223053 + 50.0 * 8.293495178222656
Epoch 2520, val loss: 0.43739792704582214
Epoch 2530, training loss: 414.8711853027344 = 0.20993389189243317 + 50.0 * 8.293225288391113
Epoch 2530, val loss: 0.43782487511634827
Epoch 2540, training loss: 414.9674987792969 = 0.2085091769695282 + 50.0 * 8.295180320739746
Epoch 2540, val loss: 0.4381692409515381
Epoch 2550, training loss: 414.9461364746094 = 0.20706996321678162 + 50.0 * 8.294781684875488
Epoch 2550, val loss: 0.4387899935245514
Epoch 2560, training loss: 414.9111328125 = 0.2056259959936142 + 50.0 * 8.294110298156738
Epoch 2560, val loss: 0.4395825266838074
Epoch 2570, training loss: 414.94696044921875 = 0.2041957825422287 + 50.0 * 8.294855117797852
Epoch 2570, val loss: 0.4400019645690918
Epoch 2580, training loss: 414.87982177734375 = 0.20276768505573273 + 50.0 * 8.293540954589844
Epoch 2580, val loss: 0.44063887000083923
Epoch 2590, training loss: 414.8658142089844 = 0.20134568214416504 + 50.0 * 8.293289184570312
Epoch 2590, val loss: 0.4413518011569977
Epoch 2600, training loss: 414.9283752441406 = 0.19993482530117035 + 50.0 * 8.29456901550293
Epoch 2600, val loss: 0.44195085763931274
Epoch 2610, training loss: 414.8255920410156 = 0.19850872457027435 + 50.0 * 8.29254150390625
Epoch 2610, val loss: 0.4421491324901581
Epoch 2620, training loss: 414.8773498535156 = 0.19709832966327667 + 50.0 * 8.293604850769043
Epoch 2620, val loss: 0.4427796006202698
Epoch 2630, training loss: 414.8515930175781 = 0.1956825703382492 + 50.0 * 8.293118476867676
Epoch 2630, val loss: 0.44340983033180237
Epoch 2640, training loss: 414.99493408203125 = 0.19427110254764557 + 50.0 * 8.296012878417969
Epoch 2640, val loss: 0.44399499893188477
Epoch 2650, training loss: 414.8044738769531 = 0.19284558296203613 + 50.0 * 8.292232513427734
Epoch 2650, val loss: 0.4449300765991211
Epoch 2660, training loss: 414.75927734375 = 0.1914445012807846 + 50.0 * 8.291357040405273
Epoch 2660, val loss: 0.44569140672683716
Epoch 2670, training loss: 414.7720947265625 = 0.1900481879711151 + 50.0 * 8.291641235351562
Epoch 2670, val loss: 0.44646117091178894
Epoch 2680, training loss: 414.83367919921875 = 0.188656747341156 + 50.0 * 8.292900085449219
Epoch 2680, val loss: 0.4470381438732147
Epoch 2690, training loss: 414.78924560546875 = 0.18726536631584167 + 50.0 * 8.29203987121582
Epoch 2690, val loss: 0.44770923256874084
Epoch 2700, training loss: 414.7500305175781 = 0.185877725481987 + 50.0 * 8.291282653808594
Epoch 2700, val loss: 0.4486443102359772
Epoch 2710, training loss: 414.9376525878906 = 0.18451939523220062 + 50.0 * 8.295063018798828
Epoch 2710, val loss: 0.44964364171028137
Epoch 2720, training loss: 414.8748779296875 = 0.1831483691930771 + 50.0 * 8.293834686279297
Epoch 2720, val loss: 0.45027169585227966
Epoch 2730, training loss: 414.7507629394531 = 0.18175558745861053 + 50.0 * 8.291379928588867
Epoch 2730, val loss: 0.4505009055137634
Epoch 2740, training loss: 414.6973571777344 = 0.1803855150938034 + 50.0 * 8.290339469909668
Epoch 2740, val loss: 0.4516945481300354
Epoch 2750, training loss: 414.6636657714844 = 0.17901532351970673 + 50.0 * 8.289692878723145
Epoch 2750, val loss: 0.45218950510025024
Epoch 2760, training loss: 414.65484619140625 = 0.1776585727930069 + 50.0 * 8.289543151855469
Epoch 2760, val loss: 0.45310699939727783
Epoch 2770, training loss: 414.89129638671875 = 0.17631550133228302 + 50.0 * 8.294299125671387
Epoch 2770, val loss: 0.4537752866744995
Epoch 2780, training loss: 414.6678161621094 = 0.1749490201473236 + 50.0 * 8.289856910705566
Epoch 2780, val loss: 0.45468559861183167
Epoch 2790, training loss: 414.71771240234375 = 0.1736094355583191 + 50.0 * 8.290882110595703
Epoch 2790, val loss: 0.4552559554576874
Epoch 2800, training loss: 414.72625732421875 = 0.17226754128932953 + 50.0 * 8.2910795211792
Epoch 2800, val loss: 0.4561617970466614
Epoch 2810, training loss: 414.6593933105469 = 0.17091847956180573 + 50.0 * 8.289769172668457
Epoch 2810, val loss: 0.45721635222435
Epoch 2820, training loss: 414.6206970214844 = 0.16959044337272644 + 50.0 * 8.289022445678711
Epoch 2820, val loss: 0.45793992280960083
Epoch 2830, training loss: 414.65625 = 0.16828152537345886 + 50.0 * 8.289759635925293
Epoch 2830, val loss: 0.4587341845035553
Epoch 2840, training loss: 414.92034912109375 = 0.1670021414756775 + 50.0 * 8.295066833496094
Epoch 2840, val loss: 0.4595111310482025
Epoch 2850, training loss: 414.79058837890625 = 0.16565291583538055 + 50.0 * 8.292498588562012
Epoch 2850, val loss: 0.46120932698249817
Epoch 2860, training loss: 414.612548828125 = 0.16432397067546844 + 50.0 * 8.28896427154541
Epoch 2860, val loss: 0.46173709630966187
Epoch 2870, training loss: 414.56768798828125 = 0.16301603615283966 + 50.0 * 8.288093566894531
Epoch 2870, val loss: 0.4628896117210388
Epoch 2880, training loss: 414.55767822265625 = 0.1617191731929779 + 50.0 * 8.287919044494629
Epoch 2880, val loss: 0.46383482217788696
Epoch 2890, training loss: 414.5753479003906 = 0.16043540835380554 + 50.0 * 8.288298606872559
Epoch 2890, val loss: 0.464688241481781
Epoch 2900, training loss: 414.7611389160156 = 0.1591653674840927 + 50.0 * 8.292038917541504
Epoch 2900, val loss: 0.4656858444213867
Epoch 2910, training loss: 414.7866516113281 = 0.15789322555065155 + 50.0 * 8.292574882507324
Epoch 2910, val loss: 0.4669426381587982
Epoch 2920, training loss: 414.62255859375 = 0.15660226345062256 + 50.0 * 8.289319038391113
Epoch 2920, val loss: 0.4676051139831543
Epoch 2930, training loss: 414.54901123046875 = 0.15532822906970978 + 50.0 * 8.287873268127441
Epoch 2930, val loss: 0.4687960147857666
Epoch 2940, training loss: 414.53704833984375 = 0.15406155586242676 + 50.0 * 8.287659645080566
Epoch 2940, val loss: 0.4696158766746521
Epoch 2950, training loss: 414.7775573730469 = 0.15284046530723572 + 50.0 * 8.292494773864746
Epoch 2950, val loss: 0.4702974259853363
Epoch 2960, training loss: 414.6426086425781 = 0.15157121419906616 + 50.0 * 8.289820671081543
Epoch 2960, val loss: 0.47174736857414246
Epoch 2970, training loss: 414.530517578125 = 0.15031014382839203 + 50.0 * 8.287604331970215
Epoch 2970, val loss: 0.4727986752986908
Epoch 2980, training loss: 414.49169921875 = 0.14906185865402222 + 50.0 * 8.286852836608887
Epoch 2980, val loss: 0.47398364543914795
Epoch 2990, training loss: 414.477294921875 = 0.14781533181667328 + 50.0 * 8.286589622497559
Epoch 2990, val loss: 0.4747672379016876
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.828513444951801
0.866550749836992
=== training gcn model ===
Epoch 0, training loss: 530.2117309570312 = 1.0985203981399536 + 50.0 * 10.582263946533203
Epoch 0, val loss: 1.0980544090270996
Epoch 10, training loss: 530.1494140625 = 1.0923547744750977 + 50.0 * 10.581141471862793
Epoch 10, val loss: 1.0918378829956055
Epoch 20, training loss: 529.6488037109375 = 1.085250973701477 + 50.0 * 10.571270942687988
Epoch 20, val loss: 1.08463454246521
Epoch 30, training loss: 526.5237426757812 = 1.0767594575881958 + 50.0 * 10.508938789367676
Epoch 30, val loss: 1.0759886503219604
Epoch 40, training loss: 515.9904174804688 = 1.0675251483917236 + 50.0 * 10.298458099365234
Epoch 40, val loss: 1.0668667554855347
Epoch 50, training loss: 494.6849060058594 = 1.0588515996932983 + 50.0 * 9.87252140045166
Epoch 50, val loss: 1.0579955577850342
Epoch 60, training loss: 476.5099792480469 = 1.0499286651611328 + 50.0 * 9.509201049804688
Epoch 60, val loss: 1.0490573644638062
Epoch 70, training loss: 467.5165100097656 = 1.0391098260879517 + 50.0 * 9.329547882080078
Epoch 70, val loss: 1.038428544998169
Epoch 80, training loss: 461.52001953125 = 1.0288952589035034 + 50.0 * 9.209822654724121
Epoch 80, val loss: 1.028670310974121
Epoch 90, training loss: 458.41986083984375 = 1.0200639963150024 + 50.0 * 9.147995948791504
Epoch 90, val loss: 1.0202285051345825
Epoch 100, training loss: 455.6789245605469 = 1.0116314888000488 + 50.0 * 9.093345642089844
Epoch 100, val loss: 1.0120794773101807
Epoch 110, training loss: 451.15087890625 = 1.0033085346221924 + 50.0 * 9.002951622009277
Epoch 110, val loss: 1.0040534734725952
Epoch 120, training loss: 446.5985107421875 = 0.9956566691398621 + 50.0 * 8.912056922912598
Epoch 120, val loss: 0.9966993927955627
Epoch 130, training loss: 443.8974914550781 = 0.9874144196510315 + 50.0 * 8.858201026916504
Epoch 130, val loss: 0.9886257648468018
Epoch 140, training loss: 441.71197509765625 = 0.9775876998901367 + 50.0 * 8.814687728881836
Epoch 140, val loss: 0.9789180755615234
Epoch 150, training loss: 440.119873046875 = 0.966235339641571 + 50.0 * 8.783072471618652
Epoch 150, val loss: 0.9677666425704956
Epoch 160, training loss: 438.2321472167969 = 0.9544846415519714 + 50.0 * 8.745553016662598
Epoch 160, val loss: 0.9563581347465515
Epoch 170, training loss: 436.5316162109375 = 0.9425185918807983 + 50.0 * 8.711782455444336
Epoch 170, val loss: 0.9449637532234192
Epoch 180, training loss: 435.5751647949219 = 0.929042398929596 + 50.0 * 8.692922592163086
Epoch 180, val loss: 0.9317936897277832
Epoch 190, training loss: 434.3636779785156 = 0.9142738580703735 + 50.0 * 8.668988227844238
Epoch 190, val loss: 0.9174739718437195
Epoch 200, training loss: 433.3362731933594 = 0.8995572924613953 + 50.0 * 8.648734092712402
Epoch 200, val loss: 0.9033386707305908
Epoch 210, training loss: 431.8985290527344 = 0.8844166994094849 + 50.0 * 8.620282173156738
Epoch 210, val loss: 0.8887419104576111
Epoch 220, training loss: 430.7195739746094 = 0.8687354922294617 + 50.0 * 8.597016334533691
Epoch 220, val loss: 0.8737403154373169
Epoch 230, training loss: 429.7910461425781 = 0.8524103760719299 + 50.0 * 8.57877254486084
Epoch 230, val loss: 0.8581836819648743
Epoch 240, training loss: 428.9104309082031 = 0.8347793817520142 + 50.0 * 8.56151294708252
Epoch 240, val loss: 0.8408980965614319
Epoch 250, training loss: 428.2139892578125 = 0.8164536952972412 + 50.0 * 8.547950744628906
Epoch 250, val loss: 0.8234230279922485
Epoch 260, training loss: 427.67535400390625 = 0.7979157567024231 + 50.0 * 8.537549018859863
Epoch 260, val loss: 0.8057035207748413
Epoch 270, training loss: 427.18536376953125 = 0.7791121006011963 + 50.0 * 8.528124809265137
Epoch 270, val loss: 0.7877433896064758
Epoch 280, training loss: 426.7455749511719 = 0.7602895498275757 + 50.0 * 8.519705772399902
Epoch 280, val loss: 0.7698442339897156
Epoch 290, training loss: 426.337646484375 = 0.7415618300437927 + 50.0 * 8.511921882629395
Epoch 290, val loss: 0.7521163821220398
Epoch 300, training loss: 425.9508972167969 = 0.7230336666107178 + 50.0 * 8.504557609558105
Epoch 300, val loss: 0.7346184253692627
Epoch 310, training loss: 426.2189025878906 = 0.7046945691108704 + 50.0 * 8.510284423828125
Epoch 310, val loss: 0.7173887491226196
Epoch 320, training loss: 425.4124450683594 = 0.6865197420120239 + 50.0 * 8.494518280029297
Epoch 320, val loss: 0.7002934217453003
Epoch 330, training loss: 425.0041198730469 = 0.6691731810569763 + 50.0 * 8.486699104309082
Epoch 330, val loss: 0.6842603087425232
Epoch 340, training loss: 424.6827087402344 = 0.6526026129722595 + 50.0 * 8.480602264404297
Epoch 340, val loss: 0.668921947479248
Epoch 350, training loss: 424.3731689453125 = 0.6368270516395569 + 50.0 * 8.474726676940918
Epoch 350, val loss: 0.6544033288955688
Epoch 360, training loss: 424.07763671875 = 0.6219181418418884 + 50.0 * 8.469114303588867
Epoch 360, val loss: 0.6408571004867554
Epoch 370, training loss: 424.16766357421875 = 0.6078348159790039 + 50.0 * 8.471196174621582
Epoch 370, val loss: 0.6281139254570007
Epoch 380, training loss: 423.5421142578125 = 0.5945780873298645 + 50.0 * 8.458950996398926
Epoch 380, val loss: 0.616221010684967
Epoch 390, training loss: 423.24658203125 = 0.5824345350265503 + 50.0 * 8.453283309936523
Epoch 390, val loss: 0.6054320931434631
Epoch 400, training loss: 422.9751281738281 = 0.5711549520492554 + 50.0 * 8.448079109191895
Epoch 400, val loss: 0.5954834818840027
Epoch 410, training loss: 422.8916320800781 = 0.5606675744056702 + 50.0 * 8.446619033813477
Epoch 410, val loss: 0.5864182114601135
Epoch 420, training loss: 422.66375732421875 = 0.5507254004478455 + 50.0 * 8.4422607421875
Epoch 420, val loss: 0.5776679515838623
Epoch 430, training loss: 422.380126953125 = 0.541568398475647 + 50.0 * 8.436771392822266
Epoch 430, val loss: 0.5699508786201477
Epoch 440, training loss: 422.13079833984375 = 0.5331661701202393 + 50.0 * 8.431952476501465
Epoch 440, val loss: 0.5627976655960083
Epoch 450, training loss: 421.93206787109375 = 0.5253613591194153 + 50.0 * 8.428133964538574
Epoch 450, val loss: 0.5562747120857239
Epoch 460, training loss: 422.1585998535156 = 0.518065869808197 + 50.0 * 8.43281078338623
Epoch 460, val loss: 0.5502037405967712
Epoch 470, training loss: 421.6956787109375 = 0.5111537575721741 + 50.0 * 8.423690795898438
Epoch 470, val loss: 0.5445873737335205
Epoch 480, training loss: 421.42462158203125 = 0.5048869848251343 + 50.0 * 8.418395042419434
Epoch 480, val loss: 0.5396313071250916
Epoch 490, training loss: 421.3816833496094 = 0.49908679723739624 + 50.0 * 8.417652130126953
Epoch 490, val loss: 0.5350376963615417
Epoch 500, training loss: 421.1791687011719 = 0.49357911944389343 + 50.0 * 8.413711547851562
Epoch 500, val loss: 0.5306844115257263
Epoch 510, training loss: 421.00518798828125 = 0.4884541630744934 + 50.0 * 8.410334587097168
Epoch 510, val loss: 0.5268295407295227
Epoch 520, training loss: 420.8088684082031 = 0.4836789071559906 + 50.0 * 8.406503677368164
Epoch 520, val loss: 0.5230679512023926
Epoch 530, training loss: 420.7920227050781 = 0.47916659712791443 + 50.0 * 8.406257629394531
Epoch 530, val loss: 0.5195884704589844
Epoch 540, training loss: 420.8370666503906 = 0.4748048782348633 + 50.0 * 8.407245635986328
Epoch 540, val loss: 0.5165090560913086
Epoch 550, training loss: 420.47705078125 = 0.4706873297691345 + 50.0 * 8.400127410888672
Epoch 550, val loss: 0.5134426355361938
Epoch 560, training loss: 420.3167724609375 = 0.46687933802604675 + 50.0 * 8.396997451782227
Epoch 560, val loss: 0.5105904340744019
Epoch 570, training loss: 420.4084167480469 = 0.4632357954978943 + 50.0 * 8.398903846740723
Epoch 570, val loss: 0.5079490542411804
Epoch 580, training loss: 420.11981201171875 = 0.4597233235836029 + 50.0 * 8.39320182800293
Epoch 580, val loss: 0.5055734515190125
Epoch 590, training loss: 419.9526672363281 = 0.45641419291496277 + 50.0 * 8.389925003051758
Epoch 590, val loss: 0.5032052397727966
Epoch 600, training loss: 419.8934326171875 = 0.4532385766506195 + 50.0 * 8.388803482055664
Epoch 600, val loss: 0.5010478496551514
Epoch 610, training loss: 419.8074645996094 = 0.45008882880210876 + 50.0 * 8.387146949768066
Epoch 610, val loss: 0.4987383484840393
Epoch 620, training loss: 419.66851806640625 = 0.44709157943725586 + 50.0 * 8.384428977966309
Epoch 620, val loss: 0.4968376159667969
Epoch 630, training loss: 419.50946044921875 = 0.4442750811576843 + 50.0 * 8.381303787231445
Epoch 630, val loss: 0.49480313062667847
Epoch 640, training loss: 419.4021911621094 = 0.44156450033187866 + 50.0 * 8.379212379455566
Epoch 640, val loss: 0.4929719865322113
Epoch 650, training loss: 419.30767822265625 = 0.43893682956695557 + 50.0 * 8.377374649047852
Epoch 650, val loss: 0.4912741184234619
Epoch 660, training loss: 419.54815673828125 = 0.4363839626312256 + 50.0 * 8.382235527038574
Epoch 660, val loss: 0.48962855339050293
Epoch 670, training loss: 419.2464599609375 = 0.4337587356567383 + 50.0 * 8.376254081726074
Epoch 670, val loss: 0.48775380849838257
Epoch 680, training loss: 419.2236328125 = 0.4312768578529358 + 50.0 * 8.375846862792969
Epoch 680, val loss: 0.48611798882484436
Epoch 690, training loss: 419.0228576660156 = 0.42897695302963257 + 50.0 * 8.371877670288086
Epoch 690, val loss: 0.48447200655937195
Epoch 700, training loss: 418.93170166015625 = 0.4267705976963043 + 50.0 * 8.370099067687988
Epoch 700, val loss: 0.4830903708934784
Epoch 710, training loss: 418.8642883300781 = 0.42463213205337524 + 50.0 * 8.368793487548828
Epoch 710, val loss: 0.48174306750297546
Epoch 720, training loss: 418.7969055175781 = 0.42254599928855896 + 50.0 * 8.367486953735352
Epoch 720, val loss: 0.48037198185920715
Epoch 730, training loss: 419.2018737792969 = 0.4204968810081482 + 50.0 * 8.375627517700195
Epoch 730, val loss: 0.4791986048221588
Epoch 740, training loss: 418.93927001953125 = 0.4184178411960602 + 50.0 * 8.370416641235352
Epoch 740, val loss: 0.4775865375995636
Epoch 750, training loss: 418.62164306640625 = 0.416454553604126 + 50.0 * 8.364104270935059
Epoch 750, val loss: 0.47639691829681396
Epoch 760, training loss: 418.5539855957031 = 0.414590060710907 + 50.0 * 8.362788200378418
Epoch 760, val loss: 0.47520172595977783
Epoch 770, training loss: 418.5006103515625 = 0.4127860367298126 + 50.0 * 8.361756324768066
Epoch 770, val loss: 0.474061518907547
Epoch 780, training loss: 418.9247741699219 = 0.4110229015350342 + 50.0 * 8.370275497436523
Epoch 780, val loss: 0.4729961156845093
Epoch 790, training loss: 418.46002197265625 = 0.4091983139514923 + 50.0 * 8.361016273498535
Epoch 790, val loss: 0.471675306558609
Epoch 800, training loss: 418.4072265625 = 0.4074850082397461 + 50.0 * 8.359994888305664
Epoch 800, val loss: 0.47063007950782776
Epoch 810, training loss: 418.3140869140625 = 0.4058513343334198 + 50.0 * 8.35816478729248
Epoch 810, val loss: 0.46953895688056946
Epoch 820, training loss: 418.4256591796875 = 0.40425795316696167 + 50.0 * 8.360427856445312
Epoch 820, val loss: 0.4686179459095001
Epoch 830, training loss: 418.2240905761719 = 0.4026709794998169 + 50.0 * 8.356428146362305
Epoch 830, val loss: 0.46743568778038025
Epoch 840, training loss: 418.1747741699219 = 0.4011417031288147 + 50.0 * 8.355472564697266
Epoch 840, val loss: 0.4664885103702545
Epoch 850, training loss: 418.6330261230469 = 0.39963749051094055 + 50.0 * 8.364667892456055
Epoch 850, val loss: 0.46526339650154114
Epoch 860, training loss: 418.24603271484375 = 0.39809340238571167 + 50.0 * 8.356958389282227
Epoch 860, val loss: 0.4646608531475067
Epoch 870, training loss: 418.0483093261719 = 0.3966653347015381 + 50.0 * 8.353033065795898
Epoch 870, val loss: 0.4635074734687805
Epoch 880, training loss: 417.9841613769531 = 0.3952702581882477 + 50.0 * 8.351778030395508
Epoch 880, val loss: 0.4626522660255432
Epoch 890, training loss: 417.9429626464844 = 0.3939193785190582 + 50.0 * 8.350980758666992
Epoch 890, val loss: 0.4617826044559479
Epoch 900, training loss: 418.0211486816406 = 0.3925899565219879 + 50.0 * 8.352571487426758
Epoch 900, val loss: 0.46092846989631653
Epoch 910, training loss: 418.0431823730469 = 0.39120954275131226 + 50.0 * 8.353039741516113
Epoch 910, val loss: 0.4600207209587097
Epoch 920, training loss: 417.8920593261719 = 0.3898768126964569 + 50.0 * 8.350044250488281
Epoch 920, val loss: 0.45916569232940674
Epoch 930, training loss: 417.8303527832031 = 0.38860562443733215 + 50.0 * 8.348834991455078
Epoch 930, val loss: 0.45824649930000305
Epoch 940, training loss: 417.7829895019531 = 0.38736000657081604 + 50.0 * 8.347912788391113
Epoch 940, val loss: 0.45745849609375
Epoch 950, training loss: 418.148193359375 = 0.38614293932914734 + 50.0 * 8.355240821838379
Epoch 950, val loss: 0.45644837617874146
Epoch 960, training loss: 417.8206481933594 = 0.3848549425601959 + 50.0 * 8.348715782165527
Epoch 960, val loss: 0.4560142457485199
Epoch 970, training loss: 417.7342529296875 = 0.38367632031440735 + 50.0 * 8.34701156616211
Epoch 970, val loss: 0.4549446702003479
Epoch 980, training loss: 417.65850830078125 = 0.38249918818473816 + 50.0 * 8.34552001953125
Epoch 980, val loss: 0.4543434977531433
Epoch 990, training loss: 417.6332702636719 = 0.3813587427139282 + 50.0 * 8.345038414001465
Epoch 990, val loss: 0.4535108804702759
Epoch 1000, training loss: 417.9863586425781 = 0.3802020847797394 + 50.0 * 8.352123260498047
Epoch 1000, val loss: 0.45276376605033875
Epoch 1010, training loss: 417.7321472167969 = 0.3790401220321655 + 50.0 * 8.347062110900879
Epoch 1010, val loss: 0.45214512944221497
Epoch 1020, training loss: 417.5710754394531 = 0.3779093623161316 + 50.0 * 8.343863487243652
Epoch 1020, val loss: 0.45127958059310913
Epoch 1030, training loss: 417.5023193359375 = 0.3768129348754883 + 50.0 * 8.342510223388672
Epoch 1030, val loss: 0.45054009556770325
Epoch 1040, training loss: 417.4879150390625 = 0.3757477104663849 + 50.0 * 8.342243194580078
Epoch 1040, val loss: 0.44981807470321655
Epoch 1050, training loss: 417.6117858886719 = 0.374693900346756 + 50.0 * 8.344741821289062
Epoch 1050, val loss: 0.449032187461853
Epoch 1060, training loss: 417.82098388671875 = 0.37358587980270386 + 50.0 * 8.348947525024414
Epoch 1060, val loss: 0.4484797716140747
Epoch 1070, training loss: 417.5162658691406 = 0.3724956810474396 + 50.0 * 8.342875480651855
Epoch 1070, val loss: 0.4478543698787689
Epoch 1080, training loss: 417.384765625 = 0.37145739793777466 + 50.0 * 8.340266227722168
Epoch 1080, val loss: 0.44704124331474304
Epoch 1090, training loss: 417.3335876464844 = 0.3704390525817871 + 50.0 * 8.339262962341309
Epoch 1090, val loss: 0.44651100039482117
Epoch 1100, training loss: 417.31787109375 = 0.3694392144680023 + 50.0 * 8.338968276977539
Epoch 1100, val loss: 0.4458983540534973
Epoch 1110, training loss: 417.5115966796875 = 0.36844173073768616 + 50.0 * 8.342863082885742
Epoch 1110, val loss: 0.44543084502220154
Epoch 1120, training loss: 417.3123779296875 = 0.367392897605896 + 50.0 * 8.338899612426758
Epoch 1120, val loss: 0.44455012679100037
Epoch 1130, training loss: 417.3521423339844 = 0.3663857579231262 + 50.0 * 8.339715003967285
Epoch 1130, val loss: 0.4440896511077881
Epoch 1140, training loss: 417.4273681640625 = 0.3653663992881775 + 50.0 * 8.341239929199219
Epoch 1140, val loss: 0.44342443346977234
Epoch 1150, training loss: 417.2615966796875 = 0.36438679695129395 + 50.0 * 8.337944030761719
Epoch 1150, val loss: 0.44262856245040894
Epoch 1160, training loss: 417.2205810546875 = 0.36342841386795044 + 50.0 * 8.337142944335938
Epoch 1160, val loss: 0.44220539927482605
Epoch 1170, training loss: 417.327392578125 = 0.3624776601791382 + 50.0 * 8.339298248291016
Epoch 1170, val loss: 0.4415544867515564
Epoch 1180, training loss: 417.1510314941406 = 0.3615081012248993 + 50.0 * 8.335790634155273
Epoch 1180, val loss: 0.4409126341342926
Epoch 1190, training loss: 417.1304931640625 = 0.3605612814426422 + 50.0 * 8.33539867401123
Epoch 1190, val loss: 0.4404054284095764
Epoch 1200, training loss: 417.13995361328125 = 0.3596218228340149 + 50.0 * 8.335606575012207
Epoch 1200, val loss: 0.43992549180984497
Epoch 1210, training loss: 417.2252502441406 = 0.35869258642196655 + 50.0 * 8.33733081817627
Epoch 1210, val loss: 0.43941304087638855
Epoch 1220, training loss: 417.11468505859375 = 0.3577304184436798 + 50.0 * 8.335139274597168
Epoch 1220, val loss: 0.4387301206588745
Epoch 1230, training loss: 417.09381103515625 = 0.35680028796195984 + 50.0 * 8.334739685058594
Epoch 1230, val loss: 0.43806761503219604
Epoch 1240, training loss: 417.0665588378906 = 0.3558836877346039 + 50.0 * 8.334213256835938
Epoch 1240, val loss: 0.4376448094844818
Epoch 1250, training loss: 417.1308288574219 = 0.3549613058567047 + 50.0 * 8.335517883300781
Epoch 1250, val loss: 0.43713781237602234
Epoch 1260, training loss: 417.1250305175781 = 0.35403376817703247 + 50.0 * 8.335419654846191
Epoch 1260, val loss: 0.43672871589660645
Epoch 1270, training loss: 416.97393798828125 = 0.35311394929885864 + 50.0 * 8.332416534423828
Epoch 1270, val loss: 0.43599411845207214
Epoch 1280, training loss: 416.9613342285156 = 0.35221803188323975 + 50.0 * 8.332182884216309
Epoch 1280, val loss: 0.43550005555152893
Epoch 1290, training loss: 417.1952209472656 = 0.3513215184211731 + 50.0 * 8.336877822875977
Epoch 1290, val loss: 0.43501409888267517
Epoch 1300, training loss: 417.1308898925781 = 0.35037609934806824 + 50.0 * 8.335610389709473
Epoch 1300, val loss: 0.43478310108184814
Epoch 1310, training loss: 416.9385681152344 = 0.34948405623435974 + 50.0 * 8.331781387329102
Epoch 1310, val loss: 0.4339083433151245
Epoch 1320, training loss: 416.87109375 = 0.3485942482948303 + 50.0 * 8.330450057983398
Epoch 1320, val loss: 0.4335729479789734
Epoch 1330, training loss: 416.8919982910156 = 0.3477328419685364 + 50.0 * 8.33088493347168
Epoch 1330, val loss: 0.4330422878265381
Epoch 1340, training loss: 416.9745178222656 = 0.34684038162231445 + 50.0 * 8.33255386352539
Epoch 1340, val loss: 0.432555228471756
Epoch 1350, training loss: 416.96142578125 = 0.3459399938583374 + 50.0 * 8.33230972290039
Epoch 1350, val loss: 0.4320985972881317
Epoch 1360, training loss: 416.8638916015625 = 0.3450487554073334 + 50.0 * 8.330376625061035
Epoch 1360, val loss: 0.431624174118042
Epoch 1370, training loss: 416.75677490234375 = 0.344157874584198 + 50.0 * 8.328252792358398
Epoch 1370, val loss: 0.4312203824520111
Epoch 1380, training loss: 416.7208251953125 = 0.34329044818878174 + 50.0 * 8.327550888061523
Epoch 1380, val loss: 0.4308199882507324
Epoch 1390, training loss: 416.771484375 = 0.3424239158630371 + 50.0 * 8.328580856323242
Epoch 1390, val loss: 0.4303053617477417
Epoch 1400, training loss: 417.0013122558594 = 0.34151268005371094 + 50.0 * 8.333195686340332
Epoch 1400, val loss: 0.42976173758506775
Epoch 1410, training loss: 416.7379150390625 = 0.3405481278896332 + 50.0 * 8.327947616577148
Epoch 1410, val loss: 0.4293675422668457
Epoch 1420, training loss: 416.69842529296875 = 0.33964455127716064 + 50.0 * 8.327176094055176
Epoch 1420, val loss: 0.42902669310569763
Epoch 1430, training loss: 416.6356506347656 = 0.3387613892555237 + 50.0 * 8.325937271118164
Epoch 1430, val loss: 0.42865240573883057
Epoch 1440, training loss: 416.6163330078125 = 0.33789366483688354 + 50.0 * 8.325569152832031
Epoch 1440, val loss: 0.42818787693977356
Epoch 1450, training loss: 416.6629638671875 = 0.3370198905467987 + 50.0 * 8.326519012451172
Epoch 1450, val loss: 0.4277830421924591
Epoch 1460, training loss: 416.8326721191406 = 0.3361118733882904 + 50.0 * 8.329931259155273
Epoch 1460, val loss: 0.42732226848602295
Epoch 1470, training loss: 416.59503173828125 = 0.3351748287677765 + 50.0 * 8.325197219848633
Epoch 1470, val loss: 0.42706620693206787
Epoch 1480, training loss: 416.5386047363281 = 0.33426839113235474 + 50.0 * 8.324087142944336
Epoch 1480, val loss: 0.42663413286209106
Epoch 1490, training loss: 416.5179443359375 = 0.33337485790252686 + 50.0 * 8.323691368103027
Epoch 1490, val loss: 0.42627036571502686
Epoch 1500, training loss: 416.4957580566406 = 0.3324832022190094 + 50.0 * 8.323265075683594
Epoch 1500, val loss: 0.42593955993652344
Epoch 1510, training loss: 416.87432861328125 = 0.3315783143043518 + 50.0 * 8.330855369567871
Epoch 1510, val loss: 0.42579934000968933
Epoch 1520, training loss: 416.6410217285156 = 0.330647736787796 + 50.0 * 8.326207160949707
Epoch 1520, val loss: 0.4251329004764557
Epoch 1530, training loss: 416.5443115234375 = 0.32971271872520447 + 50.0 * 8.324292182922363
Epoch 1530, val loss: 0.4248431622982025
Epoch 1540, training loss: 416.73699951171875 = 0.32878828048706055 + 50.0 * 8.328164100646973
Epoch 1540, val loss: 0.4244999885559082
Epoch 1550, training loss: 416.47149658203125 = 0.3278524577617645 + 50.0 * 8.32287311553955
Epoch 1550, val loss: 0.4239484667778015
Epoch 1560, training loss: 416.4080810546875 = 0.3269425630569458 + 50.0 * 8.321622848510742
Epoch 1560, val loss: 0.4236193299293518
Epoch 1570, training loss: 416.36175537109375 = 0.3260328471660614 + 50.0 * 8.320714950561523
Epoch 1570, val loss: 0.4233468770980835
Epoch 1580, training loss: 416.39984130859375 = 0.32512733340263367 + 50.0 * 8.321494102478027
Epoch 1580, val loss: 0.42312508821487427
Epoch 1590, training loss: 416.68536376953125 = 0.3241965174674988 + 50.0 * 8.327223777770996
Epoch 1590, val loss: 0.42288464307785034
Epoch 1600, training loss: 416.4559631347656 = 0.32322508096694946 + 50.0 * 8.322654724121094
Epoch 1600, val loss: 0.4221735894680023
Epoch 1610, training loss: 416.35107421875 = 0.32228586077690125 + 50.0 * 8.320575714111328
Epoch 1610, val loss: 0.42198866605758667
Epoch 1620, training loss: 416.317626953125 = 0.3213566839694977 + 50.0 * 8.319925308227539
Epoch 1620, val loss: 0.42156293988227844
Epoch 1630, training loss: 416.44873046875 = 0.3204260468482971 + 50.0 * 8.322566032409668
Epoch 1630, val loss: 0.42121151089668274
Epoch 1640, training loss: 416.2833557128906 = 0.3194596469402313 + 50.0 * 8.3192777633667
Epoch 1640, val loss: 0.4209083616733551
Epoch 1650, training loss: 416.28216552734375 = 0.31851112842559814 + 50.0 * 8.319272994995117
Epoch 1650, val loss: 0.4206213057041168
Epoch 1660, training loss: 416.34625244140625 = 0.31755897402763367 + 50.0 * 8.320573806762695
Epoch 1660, val loss: 0.4204329252243042
Epoch 1670, training loss: 416.3556823730469 = 0.3165721893310547 + 50.0 * 8.320782661437988
Epoch 1670, val loss: 0.4200160801410675
Epoch 1680, training loss: 416.2567138671875 = 0.3156013786792755 + 50.0 * 8.318821907043457
Epoch 1680, val loss: 0.4194709360599518
Epoch 1690, training loss: 416.177734375 = 0.31463345885276794 + 50.0 * 8.317261695861816
Epoch 1690, val loss: 0.41923588514328003
Epoch 1700, training loss: 416.15106201171875 = 0.31367871165275574 + 50.0 * 8.316747665405273
Epoch 1700, val loss: 0.41898682713508606
Epoch 1710, training loss: 416.2102355957031 = 0.31272193789482117 + 50.0 * 8.317950248718262
Epoch 1710, val loss: 0.418547123670578
Epoch 1720, training loss: 416.4799499511719 = 0.3117271363735199 + 50.0 * 8.3233642578125
Epoch 1720, val loss: 0.41810914874076843
Epoch 1730, training loss: 416.16552734375 = 0.31068792939186096 + 50.0 * 8.317096710205078
Epoch 1730, val loss: 0.41791045665740967
Epoch 1740, training loss: 416.1403503417969 = 0.3096948266029358 + 50.0 * 8.31661319732666
Epoch 1740, val loss: 0.4175805151462555
Epoch 1750, training loss: 416.0845031738281 = 0.308707058429718 + 50.0 * 8.315515518188477
Epoch 1750, val loss: 0.41722753643989563
Epoch 1760, training loss: 416.0752258300781 = 0.30772390961647034 + 50.0 * 8.315350532531738
Epoch 1760, val loss: 0.4169348180294037
Epoch 1770, training loss: 416.1616516113281 = 0.30673953890800476 + 50.0 * 8.317098617553711
Epoch 1770, val loss: 0.4165147840976715
Epoch 1780, training loss: 416.24273681640625 = 0.3057091236114502 + 50.0 * 8.318740844726562
Epoch 1780, val loss: 0.4161771237850189
Epoch 1790, training loss: 416.1363220214844 = 0.30466586351394653 + 50.0 * 8.316633224487305
Epoch 1790, val loss: 0.4160962402820587
Epoch 1800, training loss: 416.12139892578125 = 0.3036462664604187 + 50.0 * 8.316354751586914
Epoch 1800, val loss: 0.4156838357448578
Epoch 1810, training loss: 416.17608642578125 = 0.3026064336299896 + 50.0 * 8.317469596862793
Epoch 1810, val loss: 0.4154869019985199
Epoch 1820, training loss: 416.065673828125 = 0.3015844225883484 + 50.0 * 8.315281867980957
Epoch 1820, val loss: 0.4150410592556
Epoch 1830, training loss: 415.9934387207031 = 0.3005508482456207 + 50.0 * 8.313858032226562
Epoch 1830, val loss: 0.41486409306526184
Epoch 1840, training loss: 416.1064758300781 = 0.29953157901763916 + 50.0 * 8.316139221191406
Epoch 1840, val loss: 0.4144462049007416
Epoch 1850, training loss: 415.99456787109375 = 0.29846590757369995 + 50.0 * 8.313921928405762
Epoch 1850, val loss: 0.4143159091472626
Epoch 1860, training loss: 415.96710205078125 = 0.29742398858070374 + 50.0 * 8.313393592834473
Epoch 1860, val loss: 0.41404077410697937
Epoch 1870, training loss: 415.9400634765625 = 0.29638293385505676 + 50.0 * 8.312873840332031
Epoch 1870, val loss: 0.4137788414955139
Epoch 1880, training loss: 416.1218566894531 = 0.29534435272216797 + 50.0 * 8.316530227661133
Epoch 1880, val loss: 0.4134528636932373
Epoch 1890, training loss: 416.1802978515625 = 0.2942790687084198 + 50.0 * 8.317720413208008
Epoch 1890, val loss: 0.4131276607513428
Epoch 1900, training loss: 415.9605407714844 = 0.2931794822216034 + 50.0 * 8.313346862792969
Epoch 1900, val loss: 0.4130661189556122
Epoch 1910, training loss: 415.86871337890625 = 0.29211586713790894 + 50.0 * 8.311532020568848
Epoch 1910, val loss: 0.4127655029296875
Epoch 1920, training loss: 415.8488464355469 = 0.29105526208877563 + 50.0 * 8.311156272888184
Epoch 1920, val loss: 0.4125573933124542
Epoch 1930, training loss: 415.84735107421875 = 0.2899942696094513 + 50.0 * 8.31114673614502
Epoch 1930, val loss: 0.41234275698661804
Epoch 1940, training loss: 416.57427978515625 = 0.2889373302459717 + 50.0 * 8.325706481933594
Epoch 1940, val loss: 0.41238296031951904
Epoch 1950, training loss: 416.0516357421875 = 0.2877856194972992 + 50.0 * 8.315277099609375
Epoch 1950, val loss: 0.41171857714653015
Epoch 1960, training loss: 415.8332824707031 = 0.2866760492324829 + 50.0 * 8.310932159423828
Epoch 1960, val loss: 0.41159430146217346
Epoch 1970, training loss: 415.7811279296875 = 0.2855870723724365 + 50.0 * 8.309910774230957
Epoch 1970, val loss: 0.4114522337913513
Epoch 1980, training loss: 415.7644958496094 = 0.2845053970813751 + 50.0 * 8.309599876403809
Epoch 1980, val loss: 0.41129231452941895
Epoch 1990, training loss: 415.7522888183594 = 0.2834201157093048 + 50.0 * 8.309377670288086
Epoch 1990, val loss: 0.41106751561164856
Epoch 2000, training loss: 415.84356689453125 = 0.28233179450035095 + 50.0 * 8.311224937438965
Epoch 2000, val loss: 0.41073858737945557
Epoch 2010, training loss: 415.9407653808594 = 0.2812005281448364 + 50.0 * 8.313191413879395
Epoch 2010, val loss: 0.41055721044540405
Epoch 2020, training loss: 415.8627624511719 = 0.28005459904670715 + 50.0 * 8.311654090881348
Epoch 2020, val loss: 0.41032856702804565
Epoch 2030, training loss: 415.7868347167969 = 0.2789100706577301 + 50.0 * 8.310158729553223
Epoch 2030, val loss: 0.41029518842697144
Epoch 2040, training loss: 415.75665283203125 = 0.2777831554412842 + 50.0 * 8.309577941894531
Epoch 2040, val loss: 0.41012635827064514
Epoch 2050, training loss: 415.6861572265625 = 0.2766728401184082 + 50.0 * 8.308189392089844
Epoch 2050, val loss: 0.4099231958389282
Epoch 2060, training loss: 415.69561767578125 = 0.2755608558654785 + 50.0 * 8.308401107788086
Epoch 2060, val loss: 0.4097684323787689
Epoch 2070, training loss: 415.9028625488281 = 0.27444782853126526 + 50.0 * 8.312568664550781
Epoch 2070, val loss: 0.40943267941474915
Epoch 2080, training loss: 415.7645263671875 = 0.2732711732387543 + 50.0 * 8.30982494354248
Epoch 2080, val loss: 0.4095976948738098
Epoch 2090, training loss: 415.64703369140625 = 0.2721116840839386 + 50.0 * 8.30749797821045
Epoch 2090, val loss: 0.4094269871711731
Epoch 2100, training loss: 415.6544494628906 = 0.2709678113460541 + 50.0 * 8.307669639587402
Epoch 2100, val loss: 0.4092397689819336
Epoch 2110, training loss: 415.6202392578125 = 0.26981422305107117 + 50.0 * 8.307008743286133
Epoch 2110, val loss: 0.40925005078315735
Epoch 2120, training loss: 415.6786193847656 = 0.26866257190704346 + 50.0 * 8.308198928833008
Epoch 2120, val loss: 0.40916943550109863
Epoch 2130, training loss: 415.8048400878906 = 0.2674880027770996 + 50.0 * 8.310747146606445
Epoch 2130, val loss: 0.40917253494262695
Epoch 2140, training loss: 415.6807861328125 = 0.26628658175468445 + 50.0 * 8.308289527893066
Epoch 2140, val loss: 0.40881332755088806
Epoch 2150, training loss: 415.6502685546875 = 0.2651008665561676 + 50.0 * 8.307703018188477
Epoch 2150, val loss: 0.4087514877319336
Epoch 2160, training loss: 415.6289978027344 = 0.26391685009002686 + 50.0 * 8.30730152130127
Epoch 2160, val loss: 0.40868252515792847
Epoch 2170, training loss: 415.559326171875 = 0.26272961497306824 + 50.0 * 8.30593204498291
Epoch 2170, val loss: 0.4086206257343292
Epoch 2180, training loss: 415.7634582519531 = 0.2615559697151184 + 50.0 * 8.310037612915039
Epoch 2180, val loss: 0.4083329439163208
Epoch 2190, training loss: 415.5787048339844 = 0.26032865047454834 + 50.0 * 8.306367874145508
Epoch 2190, val loss: 0.40848594903945923
Epoch 2200, training loss: 415.5491943359375 = 0.25912216305732727 + 50.0 * 8.305801391601562
Epoch 2200, val loss: 0.408622682094574
Epoch 2210, training loss: 415.5176696777344 = 0.25792238116264343 + 50.0 * 8.305194854736328
Epoch 2210, val loss: 0.40848708152770996
Epoch 2220, training loss: 415.53619384765625 = 0.25672486424446106 + 50.0 * 8.30558967590332
Epoch 2220, val loss: 0.4085354208946228
Epoch 2230, training loss: 415.6745910644531 = 0.25551387667655945 + 50.0 * 8.308381080627441
Epoch 2230, val loss: 0.4086190164089203
Epoch 2240, training loss: 415.6492919921875 = 0.25428688526153564 + 50.0 * 8.307900428771973
Epoch 2240, val loss: 0.4083508253097534
Epoch 2250, training loss: 415.5357666015625 = 0.25306394696235657 + 50.0 * 8.305654525756836
Epoch 2250, val loss: 0.4083273410797119
Epoch 2260, training loss: 415.4517822265625 = 0.2518385350704193 + 50.0 * 8.303998947143555
Epoch 2260, val loss: 0.4083578884601593
Epoch 2270, training loss: 415.4545593261719 = 0.2506266236305237 + 50.0 * 8.304078102111816
Epoch 2270, val loss: 0.4084084928035736
Epoch 2280, training loss: 415.5499267578125 = 0.24941255152225494 + 50.0 * 8.306010246276855
Epoch 2280, val loss: 0.40832921862602234
Epoch 2290, training loss: 415.6265869140625 = 0.24818962812423706 + 50.0 * 8.307567596435547
Epoch 2290, val loss: 0.4083029627799988
Epoch 2300, training loss: 415.39569091796875 = 0.2469301074743271 + 50.0 * 8.30297565460205
Epoch 2300, val loss: 0.4085187613964081
Epoch 2310, training loss: 415.406982421875 = 0.24570056796073914 + 50.0 * 8.30322551727295
Epoch 2310, val loss: 0.40871644020080566
Epoch 2320, training loss: 415.3682861328125 = 0.24446998536586761 + 50.0 * 8.302475929260254
Epoch 2320, val loss: 0.408720999956131
Epoch 2330, training loss: 415.3667297363281 = 0.24324128031730652 + 50.0 * 8.302469253540039
Epoch 2330, val loss: 0.4087509214878082
Epoch 2340, training loss: 415.803466796875 = 0.24202071130275726 + 50.0 * 8.31122875213623
Epoch 2340, val loss: 0.4089246690273285
Epoch 2350, training loss: 415.507080078125 = 0.24074804782867432 + 50.0 * 8.305326461791992
Epoch 2350, val loss: 0.40879538655281067
Epoch 2360, training loss: 415.3823547363281 = 0.23948350548744202 + 50.0 * 8.302857398986816
Epoch 2360, val loss: 0.4089111089706421
Epoch 2370, training loss: 415.3349304199219 = 0.23823608458042145 + 50.0 * 8.301933288574219
Epoch 2370, val loss: 0.4091193675994873
Epoch 2380, training loss: 415.55072021484375 = 0.23700080811977386 + 50.0 * 8.3062744140625
Epoch 2380, val loss: 0.40948063135147095
Epoch 2390, training loss: 415.373291015625 = 0.2357097715139389 + 50.0 * 8.302751541137695
Epoch 2390, val loss: 0.40925103425979614
Epoch 2400, training loss: 415.30523681640625 = 0.23445330560207367 + 50.0 * 8.30141544342041
Epoch 2400, val loss: 0.40922704339027405
Epoch 2410, training loss: 415.27166748046875 = 0.233194962143898 + 50.0 * 8.300769805908203
Epoch 2410, val loss: 0.4094487726688385
Epoch 2420, training loss: 415.26287841796875 = 0.23194722831249237 + 50.0 * 8.300619125366211
Epoch 2420, val loss: 0.40960586071014404
Epoch 2430, training loss: 415.2564697265625 = 0.23070158064365387 + 50.0 * 8.300515174865723
Epoch 2430, val loss: 0.4099384844303131
Epoch 2440, training loss: 415.56744384765625 = 0.22946518659591675 + 50.0 * 8.30675983428955
Epoch 2440, val loss: 0.4103769063949585
Epoch 2450, training loss: 415.22357177734375 = 0.22816553711891174 + 50.0 * 8.299907684326172
Epoch 2450, val loss: 0.40999799966812134
Epoch 2460, training loss: 415.3190002441406 = 0.2269095629453659 + 50.0 * 8.301841735839844
Epoch 2460, val loss: 0.4101318418979645
Epoch 2470, training loss: 415.2643127441406 = 0.22561801970005035 + 50.0 * 8.300773620605469
Epoch 2470, val loss: 0.4105731248855591
Epoch 2480, training loss: 415.218017578125 = 0.22435645759105682 + 50.0 * 8.299873352050781
Epoch 2480, val loss: 0.41074126958847046
Epoch 2490, training loss: 415.1877136230469 = 0.2230885773897171 + 50.0 * 8.29929256439209
Epoch 2490, val loss: 0.41093355417251587
Epoch 2500, training loss: 415.1745300292969 = 0.2218291312456131 + 50.0 * 8.299054145812988
Epoch 2500, val loss: 0.41112929582595825
Epoch 2510, training loss: 415.2782287597656 = 0.22057794034481049 + 50.0 * 8.301153182983398
Epoch 2510, val loss: 0.41117531061172485
Epoch 2520, training loss: 415.3611145019531 = 0.21928921341896057 + 50.0 * 8.302836418151855
Epoch 2520, val loss: 0.4114309549331665
Epoch 2530, training loss: 415.1892395019531 = 0.21798565983772278 + 50.0 * 8.29942512512207
Epoch 2530, val loss: 0.4117979109287262
Epoch 2540, training loss: 415.1903076171875 = 0.2167017012834549 + 50.0 * 8.299471855163574
Epoch 2540, val loss: 0.41205456852912903
Epoch 2550, training loss: 415.1673278808594 = 0.21542391180992126 + 50.0 * 8.29903793334961
Epoch 2550, val loss: 0.4121001362800598
Epoch 2560, training loss: 415.19921875 = 0.2141495943069458 + 50.0 * 8.299701690673828
Epoch 2560, val loss: 0.4124298691749573
Epoch 2570, training loss: 415.23980712890625 = 0.21286821365356445 + 50.0 * 8.300539016723633
Epoch 2570, val loss: 0.4127024710178375
Epoch 2580, training loss: 415.0876159667969 = 0.21157599985599518 + 50.0 * 8.297520637512207
Epoch 2580, val loss: 0.41292861104011536
Epoch 2590, training loss: 415.15045166015625 = 0.21030330657958984 + 50.0 * 8.298803329467773
Epoch 2590, val loss: 0.4130646884441376
Epoch 2600, training loss: 415.373291015625 = 0.2090490460395813 + 50.0 * 8.303284645080566
Epoch 2600, val loss: 0.4129824638366699
Epoch 2610, training loss: 415.2652282714844 = 0.20771589875221252 + 50.0 * 8.30115032196045
Epoch 2610, val loss: 0.41366249322891235
Epoch 2620, training loss: 415.1145324707031 = 0.20641747117042542 + 50.0 * 8.298162460327148
Epoch 2620, val loss: 0.41398218274116516
Epoch 2630, training loss: 415.0603332519531 = 0.20513686537742615 + 50.0 * 8.297103881835938
Epoch 2630, val loss: 0.41445085406303406
Epoch 2640, training loss: 415.09423828125 = 0.20386363565921783 + 50.0 * 8.297807693481445
Epoch 2640, val loss: 0.41464436054229736
Epoch 2650, training loss: 415.2392883300781 = 0.2026020884513855 + 50.0 * 8.30073356628418
Epoch 2650, val loss: 0.41529619693756104
Epoch 2660, training loss: 415.1303405761719 = 0.20130278170108795 + 50.0 * 8.29858112335205
Epoch 2660, val loss: 0.4152764081954956
Epoch 2670, training loss: 415.072509765625 = 0.20002678036689758 + 50.0 * 8.297450065612793
Epoch 2670, val loss: 0.4154147207736969
Epoch 2680, training loss: 415.2322998046875 = 0.1987607330083847 + 50.0 * 8.300670623779297
Epoch 2680, val loss: 0.41577717661857605
Epoch 2690, training loss: 415.0462951660156 = 0.1974674016237259 + 50.0 * 8.296976089477539
Epoch 2690, val loss: 0.416420042514801
Epoch 2700, training loss: 414.98773193359375 = 0.19619175791740417 + 50.0 * 8.295830726623535
Epoch 2700, val loss: 0.4167366921901703
Epoch 2710, training loss: 414.9697265625 = 0.19492092728614807 + 50.0 * 8.295495986938477
Epoch 2710, val loss: 0.4171093702316284
Epoch 2720, training loss: 415.09039306640625 = 0.1936674863100052 + 50.0 * 8.297934532165527
Epoch 2720, val loss: 0.41726356744766235
Epoch 2730, training loss: 415.1610107421875 = 0.19238588213920593 + 50.0 * 8.299372673034668
Epoch 2730, val loss: 0.41773873567581177
Epoch 2740, training loss: 414.9833068847656 = 0.19108903408050537 + 50.0 * 8.295844078063965
Epoch 2740, val loss: 0.4181441366672516
Epoch 2750, training loss: 414.9491271972656 = 0.18980781733989716 + 50.0 * 8.295186042785645
Epoch 2750, val loss: 0.4189038872718811
Epoch 2760, training loss: 414.9363098144531 = 0.18854306638240814 + 50.0 * 8.294955253601074
Epoch 2760, val loss: 0.41914060711860657
Epoch 2770, training loss: 415.0101623535156 = 0.18729659914970398 + 50.0 * 8.296457290649414
Epoch 2770, val loss: 0.4199286699295044
Epoch 2780, training loss: 415.1972961425781 = 0.1860440969467163 + 50.0 * 8.300225257873535
Epoch 2780, val loss: 0.4203491508960724
Epoch 2790, training loss: 415.035400390625 = 0.18477609753608704 + 50.0 * 8.297012329101562
Epoch 2790, val loss: 0.42015185952186584
Epoch 2800, training loss: 414.9656066894531 = 0.18350273370742798 + 50.0 * 8.295641899108887
Epoch 2800, val loss: 0.4208492934703827
Epoch 2810, training loss: 415.07025146484375 = 0.1822473704814911 + 50.0 * 8.297760009765625
Epoch 2810, val loss: 0.421161025762558
Epoch 2820, training loss: 414.9225769042969 = 0.18098364770412445 + 50.0 * 8.294832229614258
Epoch 2820, val loss: 0.4215913712978363
Epoch 2830, training loss: 414.9078674316406 = 0.17972970008850098 + 50.0 * 8.294563293457031
Epoch 2830, val loss: 0.42215150594711304
Epoch 2840, training loss: 414.9305114746094 = 0.17848359048366547 + 50.0 * 8.295040130615234
Epoch 2840, val loss: 0.4226706326007843
Epoch 2850, training loss: 414.9960021972656 = 0.1772357076406479 + 50.0 * 8.296375274658203
Epoch 2850, val loss: 0.423252671957016
Epoch 2860, training loss: 415.0578918457031 = 0.17598548531532288 + 50.0 * 8.297637939453125
Epoch 2860, val loss: 0.423686146736145
Epoch 2870, training loss: 414.8487243652344 = 0.17471817135810852 + 50.0 * 8.293479919433594
Epoch 2870, val loss: 0.42392513155937195
Epoch 2880, training loss: 414.892578125 = 0.17347070574760437 + 50.0 * 8.294382095336914
Epoch 2880, val loss: 0.42419591546058655
Epoch 2890, training loss: 415.1713562011719 = 0.1722533106803894 + 50.0 * 8.299982070922852
Epoch 2890, val loss: 0.42447274923324585
Epoch 2900, training loss: 414.8874816894531 = 0.17098022997379303 + 50.0 * 8.294329643249512
Epoch 2900, val loss: 0.42545390129089355
Epoch 2910, training loss: 414.81964111328125 = 0.16973990201950073 + 50.0 * 8.292998313903809
Epoch 2910, val loss: 0.42571550607681274
Epoch 2920, training loss: 414.8033752441406 = 0.16850616037845612 + 50.0 * 8.292696952819824
Epoch 2920, val loss: 0.4263095259666443
Epoch 2930, training loss: 414.8967590332031 = 0.1672898530960083 + 50.0 * 8.294589042663574
Epoch 2930, val loss: 0.42704159021377563
Epoch 2940, training loss: 415.0694885253906 = 0.16607755422592163 + 50.0 * 8.298068046569824
Epoch 2940, val loss: 0.42766132950782776
Epoch 2950, training loss: 414.8935546875 = 0.16483497619628906 + 50.0 * 8.294574737548828
Epoch 2950, val loss: 0.42792829871177673
Epoch 2960, training loss: 414.8553771972656 = 0.1636084020137787 + 50.0 * 8.293835639953613
Epoch 2960, val loss: 0.42823976278305054
Epoch 2970, training loss: 414.87255859375 = 0.16239920258522034 + 50.0 * 8.294203758239746
Epoch 2970, val loss: 0.429039865732193
Epoch 2980, training loss: 414.7897033691406 = 0.1611916422843933 + 50.0 * 8.292570114135742
Epoch 2980, val loss: 0.42951494455337524
Epoch 2990, training loss: 414.74951171875 = 0.15999414026737213 + 50.0 * 8.291790008544922
Epoch 2990, val loss: 0.42997047305107117
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8437341451040081
0.8674201260595523
=== training gcn model ===
Epoch 0, training loss: 530.2098999023438 = 1.0972415208816528 + 50.0 * 10.582253456115723
Epoch 0, val loss: 1.0967856645584106
Epoch 10, training loss: 530.1358642578125 = 1.0908286571502686 + 50.0 * 10.580901145935059
Epoch 10, val loss: 1.0903044939041138
Epoch 20, training loss: 529.52490234375 = 1.0831509828567505 + 50.0 * 10.568835258483887
Epoch 20, val loss: 1.0825634002685547
Epoch 30, training loss: 525.5789184570312 = 1.07378351688385 + 50.0 * 10.490102767944336
Epoch 30, val loss: 1.0731353759765625
Epoch 40, training loss: 510.5733642578125 = 1.063651204109192 + 50.0 * 10.190194129943848
Epoch 40, val loss: 1.063089370727539
Epoch 50, training loss: 483.4496765136719 = 1.0528197288513184 + 50.0 * 9.647936820983887
Epoch 50, val loss: 1.0521445274353027
Epoch 60, training loss: 480.3265686035156 = 1.0423945188522339 + 50.0 * 9.585683822631836
Epoch 60, val loss: 1.0418297052383423
Epoch 70, training loss: 477.9811096191406 = 1.0328482389450073 + 50.0 * 9.538965225219727
Epoch 70, val loss: 1.0325409173965454
Epoch 80, training loss: 473.1686706542969 = 1.0250340700149536 + 50.0 * 9.442873001098633
Epoch 80, val loss: 1.0249935388565063
Epoch 90, training loss: 463.6438293457031 = 1.0182819366455078 + 50.0 * 9.252511024475098
Epoch 90, val loss: 1.0185505151748657
Epoch 100, training loss: 456.5682067871094 = 1.011367917060852 + 50.0 * 9.111136436462402
Epoch 100, val loss: 1.0119013786315918
Epoch 110, training loss: 452.8831481933594 = 1.003604769706726 + 50.0 * 9.037590980529785
Epoch 110, val loss: 1.0044546127319336
Epoch 120, training loss: 447.6457824707031 = 0.9976628422737122 + 50.0 * 8.932962417602539
Epoch 120, val loss: 0.9988033771514893
Epoch 130, training loss: 444.5821838378906 = 0.9920257329940796 + 50.0 * 8.871803283691406
Epoch 130, val loss: 0.9929880499839783
Epoch 140, training loss: 441.995361328125 = 0.9837954044342041 + 50.0 * 8.820231437683105
Epoch 140, val loss: 0.9846341013908386
Epoch 150, training loss: 440.7159118652344 = 0.9731093049049377 + 50.0 * 8.794856071472168
Epoch 150, val loss: 0.974288284778595
Epoch 160, training loss: 438.688720703125 = 0.962374210357666 + 50.0 * 8.75452709197998
Epoch 160, val loss: 0.9641174674034119
Epoch 170, training loss: 436.5173034667969 = 0.9529736638069153 + 50.0 * 8.711286544799805
Epoch 170, val loss: 0.9551308155059814
Epoch 180, training loss: 434.6685485839844 = 0.9433776140213013 + 50.0 * 8.674503326416016
Epoch 180, val loss: 0.9458139538764954
Epoch 190, training loss: 433.3973083496094 = 0.9322916269302368 + 50.0 * 8.649300575256348
Epoch 190, val loss: 0.9349766373634338
Epoch 200, training loss: 432.3382873535156 = 0.9194916486740112 + 50.0 * 8.628376007080078
Epoch 200, val loss: 0.9225269556045532
Epoch 210, training loss: 431.49493408203125 = 0.9053665399551392 + 50.0 * 8.611791610717773
Epoch 210, val loss: 0.9089088439941406
Epoch 220, training loss: 430.7187194824219 = 0.8901680707931519 + 50.0 * 8.59657096862793
Epoch 220, val loss: 0.8942497372627258
Epoch 230, training loss: 430.1825866699219 = 0.8739417195320129 + 50.0 * 8.586173057556152
Epoch 230, val loss: 0.8785794973373413
Epoch 240, training loss: 429.8121337890625 = 0.8566159009933472 + 50.0 * 8.579110145568848
Epoch 240, val loss: 0.8618456721305847
Epoch 250, training loss: 429.055908203125 = 0.8388612270355225 + 50.0 * 8.564340591430664
Epoch 250, val loss: 0.8447648286819458
Epoch 260, training loss: 428.3905334472656 = 0.821134090423584 + 50.0 * 8.551387786865234
Epoch 260, val loss: 0.8278063535690308
Epoch 270, training loss: 428.0381774902344 = 0.8032985329627991 + 50.0 * 8.544697761535645
Epoch 270, val loss: 0.8106831908226013
Epoch 280, training loss: 427.2852478027344 = 0.7848259806632996 + 50.0 * 8.530008316040039
Epoch 280, val loss: 0.7931778430938721
Epoch 290, training loss: 426.6518859863281 = 0.7664356827735901 + 50.0 * 8.517708778381348
Epoch 290, val loss: 0.775711715221405
Epoch 300, training loss: 426.4114685058594 = 0.7482065558433533 + 50.0 * 8.513265609741211
Epoch 300, val loss: 0.7584962844848633
Epoch 310, training loss: 425.6784973144531 = 0.7301034331321716 + 50.0 * 8.498968124389648
Epoch 310, val loss: 0.7415096163749695
Epoch 320, training loss: 425.1407470703125 = 0.7125257253646851 + 50.0 * 8.488564491271973
Epoch 320, val loss: 0.7250567078590393
Epoch 330, training loss: 424.6885986328125 = 0.6954435706138611 + 50.0 * 8.479863166809082
Epoch 330, val loss: 0.7091710567474365
Epoch 340, training loss: 424.7397155761719 = 0.6785632967948914 + 50.0 * 8.481223106384277
Epoch 340, val loss: 0.6934219002723694
Epoch 350, training loss: 424.1258544921875 = 0.6620541214942932 + 50.0 * 8.469276428222656
Epoch 350, val loss: 0.6783022880554199
Epoch 360, training loss: 423.79736328125 = 0.6467339396476746 + 50.0 * 8.4630126953125
Epoch 360, val loss: 0.6642927527427673
Epoch 370, training loss: 423.5146179199219 = 0.632355272769928 + 50.0 * 8.457645416259766
Epoch 370, val loss: 0.6512997150421143
Epoch 380, training loss: 423.2664794921875 = 0.6187863349914551 + 50.0 * 8.452954292297363
Epoch 380, val loss: 0.6390726566314697
Epoch 390, training loss: 423.03839111328125 = 0.6060671210289001 + 50.0 * 8.448646545410156
Epoch 390, val loss: 0.627710223197937
Epoch 400, training loss: 423.3150634765625 = 0.5941936373710632 + 50.0 * 8.45441722869873
Epoch 400, val loss: 0.6172191500663757
Epoch 410, training loss: 422.7725830078125 = 0.5829157829284668 + 50.0 * 8.443793296813965
Epoch 410, val loss: 0.6073579788208008
Epoch 420, training loss: 422.4624938964844 = 0.5726562142372131 + 50.0 * 8.437796592712402
Epoch 420, val loss: 0.5984565615653992
Epoch 430, training loss: 422.3252258300781 = 0.5631982684135437 + 50.0 * 8.435240745544434
Epoch 430, val loss: 0.5904361605644226
Epoch 440, training loss: 422.2974548339844 = 0.5542466640472412 + 50.0 * 8.434864044189453
Epoch 440, val loss: 0.5827690362930298
Epoch 450, training loss: 421.9792785644531 = 0.5459608435630798 + 50.0 * 8.428666114807129
Epoch 450, val loss: 0.5758454203605652
Epoch 460, training loss: 421.7891845703125 = 0.5384281277656555 + 50.0 * 8.425015449523926
Epoch 460, val loss: 0.5697206854820251
Epoch 470, training loss: 421.6177062988281 = 0.531446635723114 + 50.0 * 8.421725273132324
Epoch 470, val loss: 0.5640683174133301
Epoch 480, training loss: 421.74395751953125 = 0.524933934211731 + 50.0 * 8.4243803024292
Epoch 480, val loss: 0.5587789416313171
Epoch 490, training loss: 421.4250183105469 = 0.5186271667480469 + 50.0 * 8.41812801361084
Epoch 490, val loss: 0.5539287328720093
Epoch 500, training loss: 421.2259216308594 = 0.5128703117370605 + 50.0 * 8.414260864257812
Epoch 500, val loss: 0.5493915677070618
Epoch 510, training loss: 421.0913391113281 = 0.5075792670249939 + 50.0 * 8.411675453186035
Epoch 510, val loss: 0.5454346537590027
Epoch 520, training loss: 420.9853820800781 = 0.5025836825370789 + 50.0 * 8.409655570983887
Epoch 520, val loss: 0.5416975617408752
Epoch 530, training loss: 421.39190673828125 = 0.497806578874588 + 50.0 * 8.417881965637207
Epoch 530, val loss: 0.538199245929718
Epoch 540, training loss: 420.7638244628906 = 0.49319592118263245 + 50.0 * 8.405412673950195
Epoch 540, val loss: 0.5348092317581177
Epoch 550, training loss: 420.693115234375 = 0.48895153403282166 + 50.0 * 8.404083251953125
Epoch 550, val loss: 0.5318519473075867
Epoch 560, training loss: 420.5408630371094 = 0.484941691160202 + 50.0 * 8.401118278503418
Epoch 560, val loss: 0.5290461182594299
Epoch 570, training loss: 420.6413879394531 = 0.481117844581604 + 50.0 * 8.403205871582031
Epoch 570, val loss: 0.5263993740081787
Epoch 580, training loss: 420.4906921386719 = 0.4773521423339844 + 50.0 * 8.400266647338867
Epoch 580, val loss: 0.5240474939346313
Epoch 590, training loss: 420.2890625 = 0.4738216996192932 + 50.0 * 8.396305084228516
Epoch 590, val loss: 0.5216566920280457
Epoch 600, training loss: 420.16790771484375 = 0.47051626443862915 + 50.0 * 8.39394760131836
Epoch 600, val loss: 0.5196213126182556
Epoch 610, training loss: 420.1182861328125 = 0.46732887625694275 + 50.0 * 8.393019676208496
Epoch 610, val loss: 0.5177355408668518
Epoch 620, training loss: 420.07373046875 = 0.46413150429725647 + 50.0 * 8.392191886901855
Epoch 620, val loss: 0.5156586766242981
Epoch 630, training loss: 419.9295959472656 = 0.4610651433467865 + 50.0 * 8.389370918273926
Epoch 630, val loss: 0.5138449668884277
Epoch 640, training loss: 419.8397521972656 = 0.4582262933254242 + 50.0 * 8.387630462646484
Epoch 640, val loss: 0.5123012065887451
Epoch 650, training loss: 419.75531005859375 = 0.4555007517337799 + 50.0 * 8.385995864868164
Epoch 650, val loss: 0.5107709169387817
Epoch 660, training loss: 419.7388610839844 = 0.4528481960296631 + 50.0 * 8.385720252990723
Epoch 660, val loss: 0.5092801451683044
Epoch 670, training loss: 419.677734375 = 0.4502001106739044 + 50.0 * 8.384551048278809
Epoch 670, val loss: 0.5080012083053589
Epoch 680, training loss: 419.5807800292969 = 0.4476436674594879 + 50.0 * 8.382662773132324
Epoch 680, val loss: 0.5064665079116821
Epoch 690, training loss: 419.4822692871094 = 0.44523364305496216 + 50.0 * 8.380741119384766
Epoch 690, val loss: 0.5052037239074707
Epoch 700, training loss: 419.4101867675781 = 0.4429045617580414 + 50.0 * 8.379345893859863
Epoch 700, val loss: 0.5041482448577881
Epoch 710, training loss: 419.74688720703125 = 0.4406394362449646 + 50.0 * 8.386124610900879
Epoch 710, val loss: 0.5031687021255493
Epoch 720, training loss: 419.4543762207031 = 0.43830597400665283 + 50.0 * 8.380321502685547
Epoch 720, val loss: 0.5017277002334595
Epoch 730, training loss: 419.29876708984375 = 0.43612030148506165 + 50.0 * 8.377252578735352
Epoch 730, val loss: 0.5006311535835266
Epoch 740, training loss: 419.1938781738281 = 0.4340588450431824 + 50.0 * 8.37519645690918
Epoch 740, val loss: 0.4997214376926422
Epoch 750, training loss: 419.2950439453125 = 0.4320523142814636 + 50.0 * 8.377260208129883
Epoch 750, val loss: 0.49877625703811646
Epoch 760, training loss: 419.1046447753906 = 0.43002161383628845 + 50.0 * 8.373492240905762
Epoch 760, val loss: 0.49788251519203186
Epoch 770, training loss: 419.0658874511719 = 0.42809075117111206 + 50.0 * 8.372756004333496
Epoch 770, val loss: 0.49702513217926025
Epoch 780, training loss: 418.96441650390625 = 0.4262368381023407 + 50.0 * 8.370763778686523
Epoch 780, val loss: 0.49619728326797485
Epoch 790, training loss: 418.8899230957031 = 0.42444032430648804 + 50.0 * 8.369309425354004
Epoch 790, val loss: 0.49542808532714844
Epoch 800, training loss: 418.933837890625 = 0.4226883053779602 + 50.0 * 8.370223045349121
Epoch 800, val loss: 0.49464112520217896
Epoch 810, training loss: 419.0935363769531 = 0.4208501875400543 + 50.0 * 8.373454093933105
Epoch 810, val loss: 0.49395641684532166
Epoch 820, training loss: 418.78363037109375 = 0.41908007860183716 + 50.0 * 8.367291450500488
Epoch 820, val loss: 0.4930362403392792
Epoch 830, training loss: 418.689697265625 = 0.41745319962501526 + 50.0 * 8.365445137023926
Epoch 830, val loss: 0.4924452006816864
Epoch 840, training loss: 418.625244140625 = 0.41588094830513 + 50.0 * 8.364187240600586
Epoch 840, val loss: 0.4918149411678314
Epoch 850, training loss: 418.58306884765625 = 0.4143529534339905 + 50.0 * 8.363373756408691
Epoch 850, val loss: 0.49121642112731934
Epoch 860, training loss: 418.6040344238281 = 0.4128510057926178 + 50.0 * 8.363823890686035
Epoch 860, val loss: 0.4906332492828369
Epoch 870, training loss: 418.6500244140625 = 0.41125836968421936 + 50.0 * 8.364775657653809
Epoch 870, val loss: 0.49011847376823425
Epoch 880, training loss: 418.51922607421875 = 0.40967363119125366 + 50.0 * 8.362191200256348
Epoch 880, val loss: 0.4894089698791504
Epoch 890, training loss: 418.439208984375 = 0.4082504212856293 + 50.0 * 8.360618591308594
Epoch 890, val loss: 0.48882073163986206
Epoch 900, training loss: 418.3768005371094 = 0.40689146518707275 + 50.0 * 8.359397888183594
Epoch 900, val loss: 0.48835116624832153
Epoch 910, training loss: 418.3223571777344 = 0.4055625796318054 + 50.0 * 8.358336448669434
Epoch 910, val loss: 0.4878767430782318
Epoch 920, training loss: 418.2801208496094 = 0.40425950288772583 + 50.0 * 8.35751724243164
Epoch 920, val loss: 0.48738664388656616
Epoch 930, training loss: 418.7983093261719 = 0.40296974778175354 + 50.0 * 8.36790657043457
Epoch 930, val loss: 0.48689204454421997
Epoch 940, training loss: 418.3194580078125 = 0.4015604555606842 + 50.0 * 8.358358383178711
Epoch 940, val loss: 0.48639976978302
Epoch 950, training loss: 418.234375 = 0.4002625346183777 + 50.0 * 8.356681823730469
Epoch 950, val loss: 0.486009418964386
Epoch 960, training loss: 418.15740966796875 = 0.3990427553653717 + 50.0 * 8.355167388916016
Epoch 960, val loss: 0.48557430505752563
Epoch 970, training loss: 418.1212158203125 = 0.397867351770401 + 50.0 * 8.354467391967773
Epoch 970, val loss: 0.48509886860847473
Epoch 980, training loss: 418.44989013671875 = 0.3966844975948334 + 50.0 * 8.361063957214355
Epoch 980, val loss: 0.4846239984035492
Epoch 990, training loss: 418.0436096191406 = 0.39544326066970825 + 50.0 * 8.3529634475708
Epoch 990, val loss: 0.4842785596847534
Epoch 1000, training loss: 418.00091552734375 = 0.39429333806037903 + 50.0 * 8.352132797241211
Epoch 1000, val loss: 0.48391297459602356
Epoch 1010, training loss: 417.9722595214844 = 0.3931857943534851 + 50.0 * 8.351581573486328
Epoch 1010, val loss: 0.48338866233825684
Epoch 1020, training loss: 418.49200439453125 = 0.39208856225013733 + 50.0 * 8.361998558044434
Epoch 1020, val loss: 0.48290959000587463
Epoch 1030, training loss: 418.0448913574219 = 0.39088472723960876 + 50.0 * 8.353079795837402
Epoch 1030, val loss: 0.482600599527359
Epoch 1040, training loss: 417.924072265625 = 0.38977107405662537 + 50.0 * 8.350686073303223
Epoch 1040, val loss: 0.4823032021522522
Epoch 1050, training loss: 417.8292541503906 = 0.3887130618095398 + 50.0 * 8.348811149597168
Epoch 1050, val loss: 0.48190000653266907
Epoch 1060, training loss: 417.7773742675781 = 0.3876831531524658 + 50.0 * 8.347793579101562
Epoch 1060, val loss: 0.4814693331718445
Epoch 1070, training loss: 417.7698059082031 = 0.38665828108787537 + 50.0 * 8.347662925720215
Epoch 1070, val loss: 0.4812145531177521
Epoch 1080, training loss: 418.1324157714844 = 0.3855952322483063 + 50.0 * 8.354936599731445
Epoch 1080, val loss: 0.4809219539165497
Epoch 1090, training loss: 417.7372131347656 = 0.38447970151901245 + 50.0 * 8.347054481506348
Epoch 1090, val loss: 0.4804096221923828
Epoch 1100, training loss: 417.6837158203125 = 0.38343819975852966 + 50.0 * 8.3460054397583
Epoch 1100, val loss: 0.47993388772010803
Epoch 1110, training loss: 417.6189270019531 = 0.38244345784187317 + 50.0 * 8.34472942352295
Epoch 1110, val loss: 0.4797038435935974
Epoch 1120, training loss: 417.704345703125 = 0.38146349787712097 + 50.0 * 8.346457481384277
Epoch 1120, val loss: 0.4793103337287903
Epoch 1130, training loss: 417.5542297363281 = 0.38040289282798767 + 50.0 * 8.343476295471191
Epoch 1130, val loss: 0.47889068722724915
Epoch 1140, training loss: 417.613037109375 = 0.3793462812900543 + 50.0 * 8.344674110412598
Epoch 1140, val loss: 0.4784483015537262
Epoch 1150, training loss: 417.5214538574219 = 0.37836408615112305 + 50.0 * 8.342862129211426
Epoch 1150, val loss: 0.4781522750854492
Epoch 1160, training loss: 417.44268798828125 = 0.3774110674858093 + 50.0 * 8.34130573272705
Epoch 1160, val loss: 0.4778127670288086
Epoch 1170, training loss: 417.4602966308594 = 0.37646424770355225 + 50.0 * 8.341676712036133
Epoch 1170, val loss: 0.4775385856628418
Epoch 1180, training loss: 417.5211486816406 = 0.3754630982875824 + 50.0 * 8.342913627624512
Epoch 1180, val loss: 0.4772631824016571
Epoch 1190, training loss: 417.3710632324219 = 0.3744376599788666 + 50.0 * 8.339932441711426
Epoch 1190, val loss: 0.47668758034706116
Epoch 1200, training loss: 417.3587341308594 = 0.37346982955932617 + 50.0 * 8.339705467224121
Epoch 1200, val loss: 0.4763118028640747
Epoch 1210, training loss: 417.29833984375 = 0.3725184202194214 + 50.0 * 8.338516235351562
Epoch 1210, val loss: 0.47588640451431274
Epoch 1220, training loss: 417.5286560058594 = 0.3715794086456299 + 50.0 * 8.343141555786133
Epoch 1220, val loss: 0.4753366708755493
Epoch 1230, training loss: 417.47784423828125 = 0.3705444633960724 + 50.0 * 8.342145919799805
Epoch 1230, val loss: 0.47531846165657043
Epoch 1240, training loss: 417.2257385253906 = 0.3695186376571655 + 50.0 * 8.337124824523926
Epoch 1240, val loss: 0.4746639132499695
Epoch 1250, training loss: 417.1990051269531 = 0.3685588538646698 + 50.0 * 8.33660888671875
Epoch 1250, val loss: 0.47430604696273804
Epoch 1260, training loss: 417.3193054199219 = 0.3676217198371887 + 50.0 * 8.339034080505371
Epoch 1260, val loss: 0.4739563763141632
Epoch 1270, training loss: 417.1489562988281 = 0.36662691831588745 + 50.0 * 8.335646629333496
Epoch 1270, val loss: 0.47362402081489563
Epoch 1280, training loss: 417.115966796875 = 0.3656444251537323 + 50.0 * 8.335006713867188
Epoch 1280, val loss: 0.473100483417511
Epoch 1290, training loss: 417.1029968261719 = 0.36468860507011414 + 50.0 * 8.334766387939453
Epoch 1290, val loss: 0.4728114902973175
Epoch 1300, training loss: 417.1639099121094 = 0.3637344539165497 + 50.0 * 8.336003303527832
Epoch 1300, val loss: 0.4723961651325226
Epoch 1310, training loss: 417.04248046875 = 0.3627709448337555 + 50.0 * 8.33359432220459
Epoch 1310, val loss: 0.47186729311943054
Epoch 1320, training loss: 417.0569763183594 = 0.36182183027267456 + 50.0 * 8.333903312683105
Epoch 1320, val loss: 0.4714807868003845
Epoch 1330, training loss: 417.18896484375 = 0.3608575165271759 + 50.0 * 8.336562156677246
Epoch 1330, val loss: 0.4709981381893158
Epoch 1340, training loss: 417.0545654296875 = 0.35986098647117615 + 50.0 * 8.333893775939941
Epoch 1340, val loss: 0.47075262665748596
Epoch 1350, training loss: 416.98968505859375 = 0.35889676213264465 + 50.0 * 8.332615852355957
Epoch 1350, val loss: 0.47013044357299805
Epoch 1360, training loss: 416.96337890625 = 0.3579270541667938 + 50.0 * 8.332108497619629
Epoch 1360, val loss: 0.46981626749038696
Epoch 1370, training loss: 416.96185302734375 = 0.3569623827934265 + 50.0 * 8.332098007202148
Epoch 1370, val loss: 0.4695796072483063
Epoch 1380, training loss: 417.1787109375 = 0.35598665475845337 + 50.0 * 8.336454391479492
Epoch 1380, val loss: 0.4689684808254242
Epoch 1390, training loss: 416.89678955078125 = 0.35496705770492554 + 50.0 * 8.330836296081543
Epoch 1390, val loss: 0.46870067715644836
Epoch 1400, training loss: 416.8451843261719 = 0.3539818227291107 + 50.0 * 8.329824447631836
Epoch 1400, val loss: 0.46804821491241455
Epoch 1410, training loss: 416.81121826171875 = 0.35300880670547485 + 50.0 * 8.329164505004883
Epoch 1410, val loss: 0.46787717938423157
Epoch 1420, training loss: 416.93658447265625 = 0.3520364463329315 + 50.0 * 8.331690788269043
Epoch 1420, val loss: 0.46733540296554565
Epoch 1430, training loss: 416.8573913574219 = 0.3510073125362396 + 50.0 * 8.330127716064453
Epoch 1430, val loss: 0.46692031621932983
Epoch 1440, training loss: 416.7784118652344 = 0.3499867618083954 + 50.0 * 8.328568458557129
Epoch 1440, val loss: 0.4664391577243805
Epoch 1450, training loss: 416.8508605957031 = 0.34898754954338074 + 50.0 * 8.330037117004395
Epoch 1450, val loss: 0.46618741750717163
Epoch 1460, training loss: 416.7513122558594 = 0.34796756505966187 + 50.0 * 8.3280668258667
Epoch 1460, val loss: 0.46555641293525696
Epoch 1470, training loss: 416.71160888671875 = 0.346969872713089 + 50.0 * 8.327292442321777
Epoch 1470, val loss: 0.46521106362342834
Epoch 1480, training loss: 416.6611633300781 = 0.345975786447525 + 50.0 * 8.326303482055664
Epoch 1480, val loss: 0.4647577106952667
Epoch 1490, training loss: 416.76348876953125 = 0.34498268365859985 + 50.0 * 8.328370094299316
Epoch 1490, val loss: 0.46441590785980225
Epoch 1500, training loss: 416.8497009277344 = 0.3439297378063202 + 50.0 * 8.33011531829834
Epoch 1500, val loss: 0.4638633728027344
Epoch 1510, training loss: 416.65533447265625 = 0.34285369515419006 + 50.0 * 8.326249122619629
Epoch 1510, val loss: 0.46346724033355713
Epoch 1520, training loss: 416.60101318359375 = 0.3418301045894623 + 50.0 * 8.325183868408203
Epoch 1520, val loss: 0.46309757232666016
Epoch 1530, training loss: 416.61474609375 = 0.34081393480300903 + 50.0 * 8.325478553771973
Epoch 1530, val loss: 0.4627487361431122
Epoch 1540, training loss: 416.71087646484375 = 0.33979618549346924 + 50.0 * 8.327422142028809
Epoch 1540, val loss: 0.46245166659355164
Epoch 1550, training loss: 416.5728454589844 = 0.3387521803379059 + 50.0 * 8.324682235717773
Epoch 1550, val loss: 0.4617052376270294
Epoch 1560, training loss: 416.74139404296875 = 0.3377103805541992 + 50.0 * 8.328073501586914
Epoch 1560, val loss: 0.46137312054634094
Epoch 1570, training loss: 416.6509094238281 = 0.3366353511810303 + 50.0 * 8.326285362243652
Epoch 1570, val loss: 0.46089595556259155
Epoch 1580, training loss: 416.4980163574219 = 0.3355626165866852 + 50.0 * 8.323248863220215
Epoch 1580, val loss: 0.46043628454208374
Epoch 1590, training loss: 416.4762878417969 = 0.33451998233795166 + 50.0 * 8.322834968566895
Epoch 1590, val loss: 0.4601641297340393
Epoch 1600, training loss: 416.53619384765625 = 0.3334815502166748 + 50.0 * 8.324053764343262
Epoch 1600, val loss: 0.45969921350479126
Epoch 1610, training loss: 416.69403076171875 = 0.3323991000652313 + 50.0 * 8.327232360839844
Epoch 1610, val loss: 0.45937085151672363
Epoch 1620, training loss: 416.5009460449219 = 0.3313036262989044 + 50.0 * 8.323392868041992
Epoch 1620, val loss: 0.4587637484073639
Epoch 1630, training loss: 416.45123291015625 = 0.3302178680896759 + 50.0 * 8.322420120239258
Epoch 1630, val loss: 0.4582861661911011
Epoch 1640, training loss: 416.6338195800781 = 0.3291296064853668 + 50.0 * 8.326093673706055
Epoch 1640, val loss: 0.45796018838882446
Epoch 1650, training loss: 416.45526123046875 = 0.32801756262779236 + 50.0 * 8.322545051574707
Epoch 1650, val loss: 0.45761772990226746
Epoch 1660, training loss: 416.3930969238281 = 0.3269282281398773 + 50.0 * 8.32132339477539
Epoch 1660, val loss: 0.457252562046051
Epoch 1670, training loss: 416.5770568847656 = 0.3258345425128937 + 50.0 * 8.325024604797363
Epoch 1670, val loss: 0.4570365250110626
Epoch 1680, training loss: 416.3785705566406 = 0.324701726436615 + 50.0 * 8.321077346801758
Epoch 1680, val loss: 0.4562702775001526
Epoch 1690, training loss: 416.5038146972656 = 0.32358190417289734 + 50.0 * 8.323604583740234
Epoch 1690, val loss: 0.4560270309448242
Epoch 1700, training loss: 416.3261413574219 = 0.3224339783191681 + 50.0 * 8.320074081420898
Epoch 1700, val loss: 0.4554988741874695
Epoch 1710, training loss: 416.3144226074219 = 0.3213135302066803 + 50.0 * 8.319862365722656
Epoch 1710, val loss: 0.4553508460521698
Epoch 1720, training loss: 416.4539794921875 = 0.32019850611686707 + 50.0 * 8.322675704956055
Epoch 1720, val loss: 0.4550791382789612
Epoch 1730, training loss: 416.3141174316406 = 0.31903013586997986 + 50.0 * 8.319901466369629
Epoch 1730, val loss: 0.45439353585243225
Epoch 1740, training loss: 416.22991943359375 = 0.3178796172142029 + 50.0 * 8.318241119384766
Epoch 1740, val loss: 0.4540027678012848
Epoch 1750, training loss: 416.2094421386719 = 0.31675150990486145 + 50.0 * 8.317853927612305
Epoch 1750, val loss: 0.4537639319896698
Epoch 1760, training loss: 416.2415771484375 = 0.3156244158744812 + 50.0 * 8.318519592285156
Epoch 1760, val loss: 0.4533761739730835
Epoch 1770, training loss: 416.4658508300781 = 0.3144689202308655 + 50.0 * 8.323027610778809
Epoch 1770, val loss: 0.45308512449264526
Epoch 1780, training loss: 416.4657897949219 = 0.3132706582546234 + 50.0 * 8.323050498962402
Epoch 1780, val loss: 0.45254746079444885
Epoch 1790, training loss: 416.21875 = 0.3120696544647217 + 50.0 * 8.318133354187012
Epoch 1790, val loss: 0.4521619379520416
Epoch 1800, training loss: 416.1618347167969 = 0.3108983039855957 + 50.0 * 8.317018508911133
Epoch 1800, val loss: 0.451816201210022
Epoch 1810, training loss: 416.1410217285156 = 0.3097419738769531 + 50.0 * 8.316625595092773
Epoch 1810, val loss: 0.45154887437820435
Epoch 1820, training loss: 416.1993713378906 = 0.30858173966407776 + 50.0 * 8.317815780639648
Epoch 1820, val loss: 0.451286256313324
Epoch 1830, training loss: 416.21124267578125 = 0.307388037443161 + 50.0 * 8.318077087402344
Epoch 1830, val loss: 0.45087936520576477
Epoch 1840, training loss: 416.1343688964844 = 0.3061947226524353 + 50.0 * 8.316563606262207
Epoch 1840, val loss: 0.45018842816352844
Epoch 1850, training loss: 416.1311340332031 = 0.30499541759490967 + 50.0 * 8.316522598266602
Epoch 1850, val loss: 0.4501298666000366
Epoch 1860, training loss: 416.0812683105469 = 0.30378368496894836 + 50.0 * 8.315549850463867
Epoch 1860, val loss: 0.44974610209465027
Epoch 1870, training loss: 416.49786376953125 = 0.3025663495063782 + 50.0 * 8.323905944824219
Epoch 1870, val loss: 0.4493434727191925
Epoch 1880, training loss: 416.0832824707031 = 0.30131009221076965 + 50.0 * 8.31563949584961
Epoch 1880, val loss: 0.44911301136016846
Epoch 1890, training loss: 415.9852600097656 = 0.30008476972579956 + 50.0 * 8.313703536987305
Epoch 1890, val loss: 0.4488556683063507
Epoch 1900, training loss: 415.95684814453125 = 0.29888105392456055 + 50.0 * 8.313158988952637
Epoch 1900, val loss: 0.44845885038375854
Epoch 1910, training loss: 415.9503173828125 = 0.2976760268211365 + 50.0 * 8.313053131103516
Epoch 1910, val loss: 0.4483068585395813
Epoch 1920, training loss: 416.16162109375 = 0.29646044969558716 + 50.0 * 8.317303657531738
Epoch 1920, val loss: 0.4479728937149048
Epoch 1930, training loss: 416.0006103515625 = 0.2951745092868805 + 50.0 * 8.314108848571777
Epoch 1930, val loss: 0.44753652811050415
Epoch 1940, training loss: 416.07916259765625 = 0.29389488697052 + 50.0 * 8.315705299377441
Epoch 1940, val loss: 0.447267085313797
Epoch 1950, training loss: 415.93841552734375 = 0.2926238477230072 + 50.0 * 8.312915802001953
Epoch 1950, val loss: 0.4469011127948761
Epoch 1960, training loss: 415.9183349609375 = 0.2913775146007538 + 50.0 * 8.312539100646973
Epoch 1960, val loss: 0.4466797113418579
Epoch 1970, training loss: 415.9562683105469 = 0.29012513160705566 + 50.0 * 8.313323020935059
Epoch 1970, val loss: 0.4462433457374573
Epoch 1980, training loss: 416.03466796875 = 0.2888464331626892 + 50.0 * 8.314916610717773
Epoch 1980, val loss: 0.44590720534324646
Epoch 1990, training loss: 416.02484130859375 = 0.2875426411628723 + 50.0 * 8.314745903015137
Epoch 1990, val loss: 0.44573551416397095
Epoch 2000, training loss: 415.8940124511719 = 0.28623560070991516 + 50.0 * 8.312155723571777
Epoch 2000, val loss: 0.44525811076164246
Epoch 2010, training loss: 415.8482971191406 = 0.2849296033382416 + 50.0 * 8.311266899108887
Epoch 2010, val loss: 0.44512149691581726
Epoch 2020, training loss: 415.8431091308594 = 0.2836418151855469 + 50.0 * 8.311189651489258
Epoch 2020, val loss: 0.444997102022171
Epoch 2030, training loss: 415.8766784667969 = 0.28234440088272095 + 50.0 * 8.31188678741455
Epoch 2030, val loss: 0.4447430670261383
Epoch 2040, training loss: 415.9321594238281 = 0.28103870153427124 + 50.0 * 8.31302261352539
Epoch 2040, val loss: 0.4446108341217041
Epoch 2050, training loss: 415.8554992675781 = 0.27970144152641296 + 50.0 * 8.311515808105469
Epoch 2050, val loss: 0.44417181611061096
Epoch 2060, training loss: 415.79693603515625 = 0.2783789038658142 + 50.0 * 8.310371398925781
Epoch 2060, val loss: 0.443813681602478
Epoch 2070, training loss: 415.74395751953125 = 0.27707022428512573 + 50.0 * 8.309337615966797
Epoch 2070, val loss: 0.44372522830963135
Epoch 2080, training loss: 415.79400634765625 = 0.2757711410522461 + 50.0 * 8.310364723205566
Epoch 2080, val loss: 0.44365647435188293
Epoch 2090, training loss: 415.8841857910156 = 0.2744402289390564 + 50.0 * 8.31219482421875
Epoch 2090, val loss: 0.443362295627594
Epoch 2100, training loss: 415.7315673828125 = 0.27308422327041626 + 50.0 * 8.30916976928711
Epoch 2100, val loss: 0.44281306862831116
Epoch 2110, training loss: 415.7112731933594 = 0.2717434763908386 + 50.0 * 8.308791160583496
Epoch 2110, val loss: 0.44276002049446106
Epoch 2120, training loss: 415.7861328125 = 0.2704211175441742 + 50.0 * 8.310314178466797
Epoch 2120, val loss: 0.44249269366264343
Epoch 2130, training loss: 415.7306213378906 = 0.2690736651420593 + 50.0 * 8.30923080444336
Epoch 2130, val loss: 0.4425118863582611
Epoch 2140, training loss: 415.982177734375 = 0.26772749423980713 + 50.0 * 8.314289093017578
Epoch 2140, val loss: 0.4425588548183441
Epoch 2150, training loss: 415.71893310546875 = 0.2663545608520508 + 50.0 * 8.309051513671875
Epoch 2150, val loss: 0.44214561581611633
Epoch 2160, training loss: 415.6387634277344 = 0.26500338315963745 + 50.0 * 8.307475090026855
Epoch 2160, val loss: 0.4423503279685974
Epoch 2170, training loss: 415.60882568359375 = 0.2636525332927704 + 50.0 * 8.306903839111328
Epoch 2170, val loss: 0.4420751929283142
Epoch 2180, training loss: 415.6551818847656 = 0.26230865716934204 + 50.0 * 8.307857513427734
Epoch 2180, val loss: 0.44209960103034973
Epoch 2190, training loss: 415.92291259765625 = 0.26095330715179443 + 50.0 * 8.313239097595215
Epoch 2190, val loss: 0.44227293133735657
Epoch 2200, training loss: 415.6859436035156 = 0.2595584988594055 + 50.0 * 8.308527946472168
Epoch 2200, val loss: 0.44175440073013306
Epoch 2210, training loss: 415.6273193359375 = 0.2581678330898285 + 50.0 * 8.307382583618164
Epoch 2210, val loss: 0.44183313846588135
Epoch 2220, training loss: 415.75616455078125 = 0.25679734349250793 + 50.0 * 8.30998706817627
Epoch 2220, val loss: 0.441927045583725
Epoch 2230, training loss: 415.6578063964844 = 0.25540828704833984 + 50.0 * 8.308048248291016
Epoch 2230, val loss: 0.44172781705856323
Epoch 2240, training loss: 415.5667419433594 = 0.2540161609649658 + 50.0 * 8.306254386901855
Epoch 2240, val loss: 0.4416450560092926
Epoch 2250, training loss: 415.6048278808594 = 0.25263774394989014 + 50.0 * 8.30704402923584
Epoch 2250, val loss: 0.4417043924331665
Epoch 2260, training loss: 415.592529296875 = 0.25125372409820557 + 50.0 * 8.306825637817383
Epoch 2260, val loss: 0.4416272044181824
Epoch 2270, training loss: 415.508056640625 = 0.24986891448497772 + 50.0 * 8.305163383483887
Epoch 2270, val loss: 0.44156375527381897
Epoch 2280, training loss: 415.52117919921875 = 0.2484920471906662 + 50.0 * 8.305453300476074
Epoch 2280, val loss: 0.4415244162082672
Epoch 2290, training loss: 415.5567321777344 = 0.24710839986801147 + 50.0 * 8.306192398071289
Epoch 2290, val loss: 0.44158196449279785
Epoch 2300, training loss: 415.5721435546875 = 0.24571958184242249 + 50.0 * 8.306528091430664
Epoch 2300, val loss: 0.4418679475784302
Epoch 2310, training loss: 415.740966796875 = 0.24431581795215607 + 50.0 * 8.309932708740234
Epoch 2310, val loss: 0.44176435470581055
Epoch 2320, training loss: 415.5716247558594 = 0.24290889501571655 + 50.0 * 8.306573867797852
Epoch 2320, val loss: 0.44150447845458984
Epoch 2330, training loss: 415.4847717285156 = 0.24149273335933685 + 50.0 * 8.304865837097168
Epoch 2330, val loss: 0.44187650084495544
Epoch 2340, training loss: 415.4162292480469 = 0.24010159075260162 + 50.0 * 8.303522109985352
Epoch 2340, val loss: 0.4419800639152527
Epoch 2350, training loss: 415.4070739746094 = 0.23872728645801544 + 50.0 * 8.303366661071777
Epoch 2350, val loss: 0.44227877259254456
Epoch 2360, training loss: 415.4602966308594 = 0.2373574674129486 + 50.0 * 8.304458618164062
Epoch 2360, val loss: 0.44253721833229065
Epoch 2370, training loss: 415.6592102050781 = 0.235993891954422 + 50.0 * 8.308464050292969
Epoch 2370, val loss: 0.44307437539100647
Epoch 2380, training loss: 415.661376953125 = 0.23454837501049042 + 50.0 * 8.308536529541016
Epoch 2380, val loss: 0.44270089268684387
Epoch 2390, training loss: 415.4145812988281 = 0.23313328623771667 + 50.0 * 8.303628921508789
Epoch 2390, val loss: 0.4426853656768799
Epoch 2400, training loss: 415.3599853515625 = 0.23174650967121124 + 50.0 * 8.30256462097168
Epoch 2400, val loss: 0.44295287132263184
Epoch 2410, training loss: 415.3720397949219 = 0.23037134110927582 + 50.0 * 8.302833557128906
Epoch 2410, val loss: 0.4433499574661255
Epoch 2420, training loss: 415.4847106933594 = 0.22898325324058533 + 50.0 * 8.30511474609375
Epoch 2420, val loss: 0.4434472620487213
Epoch 2430, training loss: 415.44085693359375 = 0.22757862508296967 + 50.0 * 8.304265975952148
Epoch 2430, val loss: 0.4434654116630554
Epoch 2440, training loss: 415.39654541015625 = 0.22617067396640778 + 50.0 * 8.303407669067383
Epoch 2440, val loss: 0.4439587891101837
Epoch 2450, training loss: 415.4188537597656 = 0.22477206587791443 + 50.0 * 8.303881645202637
Epoch 2450, val loss: 0.44446343183517456
Epoch 2460, training loss: 415.35760498046875 = 0.22336474061012268 + 50.0 * 8.302684783935547
Epoch 2460, val loss: 0.4446597099304199
Epoch 2470, training loss: 415.34173583984375 = 0.22196772694587708 + 50.0 * 8.302395820617676
Epoch 2470, val loss: 0.444711297750473
Epoch 2480, training loss: 415.2946472167969 = 0.2205716371536255 + 50.0 * 8.301481246948242
Epoch 2480, val loss: 0.44519543647766113
Epoch 2490, training loss: 415.462890625 = 0.21919164061546326 + 50.0 * 8.304874420166016
Epoch 2490, val loss: 0.4455915093421936
Epoch 2500, training loss: 415.39849853515625 = 0.2177874892950058 + 50.0 * 8.303614616394043
Epoch 2500, val loss: 0.44616448879241943
Epoch 2510, training loss: 415.2611999511719 = 0.21636560559272766 + 50.0 * 8.300896644592285
Epoch 2510, val loss: 0.4462473690509796
Epoch 2520, training loss: 415.24224853515625 = 0.21496103703975677 + 50.0 * 8.300545692443848
Epoch 2520, val loss: 0.4465060532093048
Epoch 2530, training loss: 415.2248229980469 = 0.21357521414756775 + 50.0 * 8.300225257873535
Epoch 2530, val loss: 0.4471445381641388
Epoch 2540, training loss: 415.26751708984375 = 0.21219095587730408 + 50.0 * 8.301106452941895
Epoch 2540, val loss: 0.447756826877594
Epoch 2550, training loss: 415.37957763671875 = 0.21081778407096863 + 50.0 * 8.303375244140625
Epoch 2550, val loss: 0.4481316804885864
Epoch 2560, training loss: 415.35748291015625 = 0.20938028395175934 + 50.0 * 8.302962303161621
Epoch 2560, val loss: 0.44799286127090454
Epoch 2570, training loss: 415.2994079589844 = 0.2079828679561615 + 50.0 * 8.301828384399414
Epoch 2570, val loss: 0.4484989047050476
Epoch 2580, training loss: 415.2516174316406 = 0.20659801363945007 + 50.0 * 8.30090045928955
Epoch 2580, val loss: 0.4488988518714905
Epoch 2590, training loss: 415.17974853515625 = 0.20521292090415955 + 50.0 * 8.299490928649902
Epoch 2590, val loss: 0.4495217800140381
Epoch 2600, training loss: 415.1604309082031 = 0.2038426250219345 + 50.0 * 8.299131393432617
Epoch 2600, val loss: 0.4498516321182251
Epoch 2610, training loss: 415.4085693359375 = 0.20248505473136902 + 50.0 * 8.304121971130371
Epoch 2610, val loss: 0.4501100778579712
Epoch 2620, training loss: 415.15374755859375 = 0.2010941207408905 + 50.0 * 8.299053192138672
Epoch 2620, val loss: 0.45083117485046387
Epoch 2630, training loss: 415.1780700683594 = 0.19971610605716705 + 50.0 * 8.299567222595215
Epoch 2630, val loss: 0.45140379667282104
Epoch 2640, training loss: 415.1519470214844 = 0.19833625853061676 + 50.0 * 8.299072265625
Epoch 2640, val loss: 0.4518355429172516
Epoch 2650, training loss: 415.1080017089844 = 0.19697049260139465 + 50.0 * 8.29822063446045
Epoch 2650, val loss: 0.45259222388267517
Epoch 2660, training loss: 415.1866760253906 = 0.19561126828193665 + 50.0 * 8.299820899963379
Epoch 2660, val loss: 0.4532931447029114
Epoch 2670, training loss: 415.20501708984375 = 0.19424737989902496 + 50.0 * 8.300215721130371
Epoch 2670, val loss: 0.4541245996952057
Epoch 2680, training loss: 415.2163391113281 = 0.19286061823368073 + 50.0 * 8.300469398498535
Epoch 2680, val loss: 0.45399796962738037
Epoch 2690, training loss: 415.1360168457031 = 0.19148670136928558 + 50.0 * 8.298890113830566
Epoch 2690, val loss: 0.4545581340789795
Epoch 2700, training loss: 415.2351989746094 = 0.19012796878814697 + 50.0 * 8.300901412963867
Epoch 2700, val loss: 0.45539650321006775
Epoch 2710, training loss: 415.0479431152344 = 0.18875493109226227 + 50.0 * 8.297183990478516
Epoch 2710, val loss: 0.45566365122795105
Epoch 2720, training loss: 415.03057861328125 = 0.18740756809711456 + 50.0 * 8.296863555908203
Epoch 2720, val loss: 0.4568674564361572
Epoch 2730, training loss: 415.04730224609375 = 0.18607072532176971 + 50.0 * 8.297224998474121
Epoch 2730, val loss: 0.4575510323047638
Epoch 2740, training loss: 415.1524353027344 = 0.1847255527973175 + 50.0 * 8.299354553222656
Epoch 2740, val loss: 0.4580968916416168
Epoch 2750, training loss: 415.09912109375 = 0.18336622416973114 + 50.0 * 8.298315048217773
Epoch 2750, val loss: 0.4583653509616852
Epoch 2760, training loss: 415.2340393066406 = 0.182017982006073 + 50.0 * 8.301040649414062
Epoch 2760, val loss: 0.4594907760620117
Epoch 2770, training loss: 415.1690979003906 = 0.18066011369228363 + 50.0 * 8.299768447875977
Epoch 2770, val loss: 0.4598681330680847
Epoch 2780, training loss: 415.01727294921875 = 0.1792972832918167 + 50.0 * 8.296759605407715
Epoch 2780, val loss: 0.46044647693634033
Epoch 2790, training loss: 414.9618225097656 = 0.17795252799987793 + 50.0 * 8.295677185058594
Epoch 2790, val loss: 0.4614165425300598
Epoch 2800, training loss: 414.9384460449219 = 0.17661221325397491 + 50.0 * 8.295236587524414
Epoch 2800, val loss: 0.46193164587020874
Epoch 2810, training loss: 414.9409484863281 = 0.17528720200061798 + 50.0 * 8.295312881469727
Epoch 2810, val loss: 0.4628486931324005
Epoch 2820, training loss: 415.1663513183594 = 0.17397446930408478 + 50.0 * 8.299847602844238
Epoch 2820, val loss: 0.46347764134407043
Epoch 2830, training loss: 414.9888610839844 = 0.1726408153772354 + 50.0 * 8.296324729919434
Epoch 2830, val loss: 0.46465981006622314
Epoch 2840, training loss: 414.9518127441406 = 0.17129552364349365 + 50.0 * 8.295610427856445
Epoch 2840, val loss: 0.465274840593338
Epoch 2850, training loss: 414.9930419921875 = 0.16997765004634857 + 50.0 * 8.29646110534668
Epoch 2850, val loss: 0.466303288936615
Epoch 2860, training loss: 414.9555969238281 = 0.16866223514080048 + 50.0 * 8.295738220214844
Epoch 2860, val loss: 0.46731770038604736
Epoch 2870, training loss: 415.0639343261719 = 0.16736088693141937 + 50.0 * 8.297931671142578
Epoch 2870, val loss: 0.4681999385356903
Epoch 2880, training loss: 415.0177917480469 = 0.1660424768924713 + 50.0 * 8.297035217285156
Epoch 2880, val loss: 0.4680633842945099
Epoch 2890, training loss: 414.9148864746094 = 0.16473300755023956 + 50.0 * 8.295002937316895
Epoch 2890, val loss: 0.46968165040016174
Epoch 2900, training loss: 414.8632507324219 = 0.1634369194507599 + 50.0 * 8.29399585723877
Epoch 2900, val loss: 0.4704582095146179
Epoch 2910, training loss: 414.9479064941406 = 0.16215476393699646 + 50.0 * 8.29571533203125
Epoch 2910, val loss: 0.47176796197891235
Epoch 2920, training loss: 414.8954162597656 = 0.16085685789585114 + 50.0 * 8.29469108581543
Epoch 2920, val loss: 0.4724843204021454
Epoch 2930, training loss: 414.9050598144531 = 0.15956272184848785 + 50.0 * 8.294909477233887
Epoch 2930, val loss: 0.47298046946525574
Epoch 2940, training loss: 415.0336608886719 = 0.15828701853752136 + 50.0 * 8.297507286071777
Epoch 2940, val loss: 0.4744994342327118
Epoch 2950, training loss: 415.0415954589844 = 0.1569904386997223 + 50.0 * 8.29769229888916
Epoch 2950, val loss: 0.47493216395378113
Epoch 2960, training loss: 414.829833984375 = 0.15568672120571136 + 50.0 * 8.293482780456543
Epoch 2960, val loss: 0.4757361114025116
Epoch 2970, training loss: 414.789306640625 = 0.15440873801708221 + 50.0 * 8.29269790649414
Epoch 2970, val loss: 0.4769349992275238
Epoch 2980, training loss: 414.77740478515625 = 0.15314821898937225 + 50.0 * 8.292485237121582
Epoch 2980, val loss: 0.4780837595462799
Epoch 2990, training loss: 414.8126220703125 = 0.1518954336643219 + 50.0 * 8.293214797973633
Epoch 2990, val loss: 0.4792948067188263
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.839167935058346
0.867275230022459
The final CL Acc:0.83714, 0.00638, The final GNN Acc:0.86708, 0.00038
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97660])
remove edge: torch.Size([2, 79972])
updated graph: torch.Size([2, 88984])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2146606445312 = 1.1042511463165283 + 50.0 * 10.582208633422852
Epoch 0, val loss: 1.1033722162246704
Epoch 10, training loss: 530.1387329101562 = 1.0969550609588623 + 50.0 * 10.580835342407227
Epoch 10, val loss: 1.0960705280303955
Epoch 20, training loss: 529.5131225585938 = 1.0881036520004272 + 50.0 * 10.568500518798828
Epoch 20, val loss: 1.08725106716156
Epoch 30, training loss: 525.3799438476562 = 1.0774677991867065 + 50.0 * 10.48604965209961
Epoch 30, val loss: 1.0767241716384888
Epoch 40, training loss: 510.7974548339844 = 1.067629098892212 + 50.0 * 10.194596290588379
Epoch 40, val loss: 1.0674118995666504
Epoch 50, training loss: 491.6589660644531 = 1.0593065023422241 + 50.0 * 9.811993598937988
Epoch 50, val loss: 1.059441089630127
Epoch 60, training loss: 468.6968994140625 = 1.0535945892333984 + 50.0 * 9.352866172790527
Epoch 60, val loss: 1.053770899772644
Epoch 70, training loss: 456.9460754394531 = 1.047129511833191 + 50.0 * 9.117979049682617
Epoch 70, val loss: 1.047316074371338
Epoch 80, training loss: 451.1984558105469 = 1.0415380001068115 + 50.0 * 9.003138542175293
Epoch 80, val loss: 1.0419996976852417
Epoch 90, training loss: 445.8108825683594 = 1.037928819656372 + 50.0 * 8.895459175109863
Epoch 90, val loss: 1.0385299921035767
Epoch 100, training loss: 441.6710510253906 = 1.0350313186645508 + 50.0 * 8.81272029876709
Epoch 100, val loss: 1.0356159210205078
Epoch 110, training loss: 438.4697570800781 = 1.031965970993042 + 50.0 * 8.748756408691406
Epoch 110, val loss: 1.0326063632965088
Epoch 120, training loss: 436.10009765625 = 1.028727412223816 + 50.0 * 8.701427459716797
Epoch 120, val loss: 1.0293992757797241
Epoch 130, training loss: 434.2386474609375 = 1.0250070095062256 + 50.0 * 8.664273262023926
Epoch 130, val loss: 1.02569580078125
Epoch 140, training loss: 432.80938720703125 = 1.0207995176315308 + 50.0 * 8.635771751403809
Epoch 140, val loss: 1.0215526819229126
Epoch 150, training loss: 431.601318359375 = 1.01641047000885 + 50.0 * 8.611698150634766
Epoch 150, val loss: 1.0171942710876465
Epoch 160, training loss: 430.716552734375 = 1.0118366479873657 + 50.0 * 8.594094276428223
Epoch 160, val loss: 1.0127314329147339
Epoch 170, training loss: 429.76885986328125 = 1.0069552659988403 + 50.0 * 8.575238227844238
Epoch 170, val loss: 1.0078637599945068
Epoch 180, training loss: 429.1365966796875 = 1.001580834388733 + 50.0 * 8.562700271606445
Epoch 180, val loss: 1.0025948286056519
Epoch 190, training loss: 428.6913146972656 = 0.9955757856369019 + 50.0 * 8.553915023803711
Epoch 190, val loss: 0.9967253804206848
Epoch 200, training loss: 427.9719543457031 = 0.9890239238739014 + 50.0 * 8.539658546447754
Epoch 200, val loss: 0.9903550148010254
Epoch 210, training loss: 427.3748779296875 = 0.9821020364761353 + 50.0 * 8.527854919433594
Epoch 210, val loss: 0.9836094975471497
Epoch 220, training loss: 427.1655578613281 = 0.9746807813644409 + 50.0 * 8.523818016052246
Epoch 220, val loss: 0.976404070854187
Epoch 230, training loss: 426.4583435058594 = 0.9664239287376404 + 50.0 * 8.509838104248047
Epoch 230, val loss: 0.9683308005332947
Epoch 240, training loss: 425.8885192871094 = 0.9575278162956238 + 50.0 * 8.49862003326416
Epoch 240, val loss: 0.9597271680831909
Epoch 250, training loss: 425.45147705078125 = 0.9480050206184387 + 50.0 * 8.490069389343262
Epoch 250, val loss: 0.9504815936088562
Epoch 260, training loss: 425.2493896484375 = 0.9376073479652405 + 50.0 * 8.486235618591309
Epoch 260, val loss: 0.9404575824737549
Epoch 270, training loss: 424.8046569824219 = 0.9264379739761353 + 50.0 * 8.477563858032227
Epoch 270, val loss: 0.929665207862854
Epoch 280, training loss: 424.44427490234375 = 0.9147064685821533 + 50.0 * 8.47059154510498
Epoch 280, val loss: 0.9183588624000549
Epoch 290, training loss: 424.19134521484375 = 0.9023395776748657 + 50.0 * 8.465780258178711
Epoch 290, val loss: 0.9064921140670776
Epoch 300, training loss: 424.24560546875 = 0.8891497254371643 + 50.0 * 8.46712875366211
Epoch 300, val loss: 0.8938270211219788
Epoch 310, training loss: 423.72998046875 = 0.87526935338974 + 50.0 * 8.457094192504883
Epoch 310, val loss: 0.8806182742118835
Epoch 320, training loss: 423.5407409667969 = 0.8611838817596436 + 50.0 * 8.453591346740723
Epoch 320, val loss: 0.8672190308570862
Epoch 330, training loss: 423.3052673339844 = 0.8467984199523926 + 50.0 * 8.449169158935547
Epoch 330, val loss: 0.8535944223403931
Epoch 340, training loss: 423.49334716796875 = 0.8321568369865417 + 50.0 * 8.453224182128906
Epoch 340, val loss: 0.8397171497344971
Epoch 350, training loss: 423.06414794921875 = 0.8170965313911438 + 50.0 * 8.444940567016602
Epoch 350, val loss: 0.8255874514579773
Epoch 360, training loss: 422.7896423339844 = 0.8023249506950378 + 50.0 * 8.439745903015137
Epoch 360, val loss: 0.8118597865104675
Epoch 370, training loss: 422.5521240234375 = 0.787847638130188 + 50.0 * 8.435285568237305
Epoch 370, val loss: 0.7984504103660583
Epoch 380, training loss: 422.7348937988281 = 0.7734104990959167 + 50.0 * 8.439229965209961
Epoch 380, val loss: 0.785123884677887
Epoch 390, training loss: 422.2374267578125 = 0.759097158908844 + 50.0 * 8.429566383361816
Epoch 390, val loss: 0.7719215154647827
Epoch 400, training loss: 422.10162353515625 = 0.7455769181251526 + 50.0 * 8.42712116241455
Epoch 400, val loss: 0.7596785426139832
Epoch 410, training loss: 421.8946533203125 = 0.7326041460037231 + 50.0 * 8.423240661621094
Epoch 410, val loss: 0.7480202913284302
Epoch 420, training loss: 421.9696350097656 = 0.7200014591217041 + 50.0 * 8.424992561340332
Epoch 420, val loss: 0.7368248701095581
Epoch 430, training loss: 421.551513671875 = 0.7079164385795593 + 50.0 * 8.416872024536133
Epoch 430, val loss: 0.7261009812355042
Epoch 440, training loss: 421.4366455078125 = 0.6965686082839966 + 50.0 * 8.414801597595215
Epoch 440, val loss: 0.7160770297050476
Epoch 450, training loss: 421.2721252441406 = 0.6857233643531799 + 50.0 * 8.411727905273438
Epoch 450, val loss: 0.7067962288856506
Epoch 460, training loss: 421.4434814453125 = 0.6754347681999207 + 50.0 * 8.415360450744629
Epoch 460, val loss: 0.6979312300682068
Epoch 470, training loss: 421.040771484375 = 0.6652716398239136 + 50.0 * 8.407509803771973
Epoch 470, val loss: 0.6892777681350708
Epoch 480, training loss: 420.96826171875 = 0.6559094190597534 + 50.0 * 8.40624713897705
Epoch 480, val loss: 0.6815661191940308
Epoch 490, training loss: 420.7747802734375 = 0.6472539901733398 + 50.0 * 8.40255069732666
Epoch 490, val loss: 0.6744464039802551
Epoch 500, training loss: 420.8016357421875 = 0.639028787612915 + 50.0 * 8.403251647949219
Epoch 500, val loss: 0.6677379608154297
Epoch 510, training loss: 420.7240905761719 = 0.6310521960258484 + 50.0 * 8.401861190795898
Epoch 510, val loss: 0.6614499092102051
Epoch 520, training loss: 420.4679870605469 = 0.6236512064933777 + 50.0 * 8.396886825561523
Epoch 520, val loss: 0.6557163000106812
Epoch 530, training loss: 420.4100036621094 = 0.6167445182800293 + 50.0 * 8.395865440368652
Epoch 530, val loss: 0.6503680944442749
Epoch 540, training loss: 420.6295471191406 = 0.6101120114326477 + 50.0 * 8.400388717651367
Epoch 540, val loss: 0.6455003619194031
Epoch 550, training loss: 420.2879638671875 = 0.6037667989730835 + 50.0 * 8.393684387207031
Epoch 550, val loss: 0.6406113505363464
Epoch 560, training loss: 420.1944885253906 = 0.597903311252594 + 50.0 * 8.391931533813477
Epoch 560, val loss: 0.63631272315979
Epoch 570, training loss: 420.087646484375 = 0.5924371480941772 + 50.0 * 8.389904022216797
Epoch 570, val loss: 0.6324246525764465
Epoch 580, training loss: 420.0539245605469 = 0.5872671008110046 + 50.0 * 8.38933277130127
Epoch 580, val loss: 0.6287739276885986
Epoch 590, training loss: 420.01129150390625 = 0.5822705626487732 + 50.0 * 8.388580322265625
Epoch 590, val loss: 0.6253194808959961
Epoch 600, training loss: 419.8694152832031 = 0.5775321125984192 + 50.0 * 8.38583755493164
Epoch 600, val loss: 0.6221806406974792
Epoch 610, training loss: 419.8099060058594 = 0.5731421113014221 + 50.0 * 8.384735107421875
Epoch 610, val loss: 0.6193228363990784
Epoch 620, training loss: 419.9141540527344 = 0.5689871907234192 + 50.0 * 8.386902809143066
Epoch 620, val loss: 0.6165907382965088
Epoch 630, training loss: 419.870849609375 = 0.5648308396339417 + 50.0 * 8.386120796203613
Epoch 630, val loss: 0.6139340996742249
Epoch 640, training loss: 419.69378662109375 = 0.5610228776931763 + 50.0 * 8.382655143737793
Epoch 640, val loss: 0.6114841103553772
Epoch 650, training loss: 419.5541076660156 = 0.5575841665267944 + 50.0 * 8.37993049621582
Epoch 650, val loss: 0.6094353795051575
Epoch 660, training loss: 419.5042724609375 = 0.5543221831321716 + 50.0 * 8.378998756408691
Epoch 660, val loss: 0.6075704097747803
Epoch 670, training loss: 419.43170166015625 = 0.5512425303459167 + 50.0 * 8.377609252929688
Epoch 670, val loss: 0.6057718396186829
Epoch 680, training loss: 419.4046630859375 = 0.5482938885688782 + 50.0 * 8.377127647399902
Epoch 680, val loss: 0.6040639877319336
Epoch 690, training loss: 419.4014892578125 = 0.5453702807426453 + 50.0 * 8.377121925354004
Epoch 690, val loss: 0.6022748351097107
Epoch 700, training loss: 419.3377380371094 = 0.5425713658332825 + 50.0 * 8.375903129577637
Epoch 700, val loss: 0.6007519364356995
Epoch 710, training loss: 419.3489074707031 = 0.5400083661079407 + 50.0 * 8.376177787780762
Epoch 710, val loss: 0.5991379618644714
Epoch 720, training loss: 419.34454345703125 = 0.5373706817626953 + 50.0 * 8.376143455505371
Epoch 720, val loss: 0.597757875919342
Epoch 730, training loss: 419.2207336425781 = 0.5349268913269043 + 50.0 * 8.373716354370117
Epoch 730, val loss: 0.5963324904441833
Epoch 740, training loss: 419.0752258300781 = 0.5326806306838989 + 50.0 * 8.370850563049316
Epoch 740, val loss: 0.5950537919998169
Epoch 750, training loss: 419.0931091308594 = 0.5305218696594238 + 50.0 * 8.371252059936523
Epoch 750, val loss: 0.5938178896903992
Epoch 760, training loss: 419.16949462890625 = 0.5283397436141968 + 50.0 * 8.372822761535645
Epoch 760, val loss: 0.5926395654678345
Epoch 770, training loss: 418.9527282714844 = 0.526201069355011 + 50.0 * 8.3685302734375
Epoch 770, val loss: 0.5913861393928528
Epoch 780, training loss: 418.918701171875 = 0.5242434144020081 + 50.0 * 8.367889404296875
Epoch 780, val loss: 0.5902718305587769
Epoch 790, training loss: 418.88104248046875 = 0.5223268270492554 + 50.0 * 8.36717414855957
Epoch 790, val loss: 0.5891051888465881
Epoch 800, training loss: 419.05096435546875 = 0.5204422473907471 + 50.0 * 8.370610237121582
Epoch 800, val loss: 0.5880306959152222
Epoch 810, training loss: 418.8268737792969 = 0.5185356140136719 + 50.0 * 8.366167068481445
Epoch 810, val loss: 0.5869858860969543
Epoch 820, training loss: 418.740478515625 = 0.516759991645813 + 50.0 * 8.364474296569824
Epoch 820, val loss: 0.5858437418937683
Epoch 830, training loss: 418.8658142089844 = 0.515014111995697 + 50.0 * 8.367015838623047
Epoch 830, val loss: 0.584709107875824
Epoch 840, training loss: 418.7789001464844 = 0.5132114887237549 + 50.0 * 8.365313529968262
Epoch 840, val loss: 0.5837660431861877
Epoch 850, training loss: 418.78704833984375 = 0.5115054249763489 + 50.0 * 8.365510940551758
Epoch 850, val loss: 0.5824437737464905
Epoch 860, training loss: 418.60272216796875 = 0.5098034143447876 + 50.0 * 8.361858367919922
Epoch 860, val loss: 0.5815329551696777
Epoch 870, training loss: 418.5585021972656 = 0.5082222819328308 + 50.0 * 8.361005783081055
Epoch 870, val loss: 0.5806342363357544
Epoch 880, training loss: 418.4842529296875 = 0.5067040920257568 + 50.0 * 8.359550476074219
Epoch 880, val loss: 0.5796225070953369
Epoch 890, training loss: 418.5624694824219 = 0.5052136778831482 + 50.0 * 8.36114501953125
Epoch 890, val loss: 0.5786034464836121
Epoch 900, training loss: 418.6190490722656 = 0.5036220550537109 + 50.0 * 8.362308502197266
Epoch 900, val loss: 0.5775147080421448
Epoch 910, training loss: 418.62322998046875 = 0.5020316243171692 + 50.0 * 8.36242389678955
Epoch 910, val loss: 0.5765705108642578
Epoch 920, training loss: 418.3699035644531 = 0.5004909038543701 + 50.0 * 8.357388496398926
Epoch 920, val loss: 0.5754404067993164
Epoch 930, training loss: 418.3491516113281 = 0.49906405806541443 + 50.0 * 8.357002258300781
Epoch 930, val loss: 0.5744389295578003
Epoch 940, training loss: 418.2724609375 = 0.49767976999282837 + 50.0 * 8.35549545288086
Epoch 940, val loss: 0.5735412836074829
Epoch 950, training loss: 418.38385009765625 = 0.4963085949420929 + 50.0 * 8.35775089263916
Epoch 950, val loss: 0.5727168321609497
Epoch 960, training loss: 418.3210754394531 = 0.49475690722465515 + 50.0 * 8.356526374816895
Epoch 960, val loss: 0.5713536143302917
Epoch 970, training loss: 418.27288818359375 = 0.4932745099067688 + 50.0 * 8.355591773986816
Epoch 970, val loss: 0.5703474879264832
Epoch 980, training loss: 418.1839599609375 = 0.49191951751708984 + 50.0 * 8.353840827941895
Epoch 980, val loss: 0.5693851113319397
Epoch 990, training loss: 418.1592712402344 = 0.4905901551246643 + 50.0 * 8.353373527526855
Epoch 990, val loss: 0.5684047341346741
Epoch 1000, training loss: 418.4124450683594 = 0.4892302453517914 + 50.0 * 8.358464241027832
Epoch 1000, val loss: 0.5673241019248962
Epoch 1010, training loss: 418.16497802734375 = 0.4878125786781311 + 50.0 * 8.353543281555176
Epoch 1010, val loss: 0.5664010643959045
Epoch 1020, training loss: 418.0648498535156 = 0.4864795506000519 + 50.0 * 8.351567268371582
Epoch 1020, val loss: 0.5653356909751892
Epoch 1030, training loss: 418.0739440917969 = 0.48515084385871887 + 50.0 * 8.351776123046875
Epoch 1030, val loss: 0.5644623637199402
Epoch 1040, training loss: 418.01275634765625 = 0.48371422290802 + 50.0 * 8.350581169128418
Epoch 1040, val loss: 0.5631916522979736
Epoch 1050, training loss: 418.1094970703125 = 0.4822942018508911 + 50.0 * 8.352543830871582
Epoch 1050, val loss: 0.562187671661377
Epoch 1060, training loss: 418.0071105957031 = 0.48092859983444214 + 50.0 * 8.350523948669434
Epoch 1060, val loss: 0.5610772967338562
Epoch 1070, training loss: 417.9156494140625 = 0.4796043634414673 + 50.0 * 8.34872055053711
Epoch 1070, val loss: 0.5601812601089478
Epoch 1080, training loss: 417.9058532714844 = 0.4782986044883728 + 50.0 * 8.348550796508789
Epoch 1080, val loss: 0.5591861009597778
Epoch 1090, training loss: 417.90216064453125 = 0.47697463631629944 + 50.0 * 8.348503112792969
Epoch 1090, val loss: 0.5581580996513367
Epoch 1100, training loss: 418.10089111328125 = 0.4756125211715698 + 50.0 * 8.352505683898926
Epoch 1100, val loss: 0.5570753812789917
Epoch 1110, training loss: 417.9039306640625 = 0.47410157322883606 + 50.0 * 8.348596572875977
Epoch 1110, val loss: 0.5557499527931213
Epoch 1120, training loss: 417.83331298828125 = 0.4726926386356354 + 50.0 * 8.347212791442871
Epoch 1120, val loss: 0.554719090461731
Epoch 1130, training loss: 417.7704162597656 = 0.47134095430374146 + 50.0 * 8.34598159790039
Epoch 1130, val loss: 0.5536882877349854
Epoch 1140, training loss: 417.7532653808594 = 0.47001415491104126 + 50.0 * 8.345664978027344
Epoch 1140, val loss: 0.5526615977287292
Epoch 1150, training loss: 418.0644836425781 = 0.4686368703842163 + 50.0 * 8.351917266845703
Epoch 1150, val loss: 0.5516712665557861
Epoch 1160, training loss: 417.7911376953125 = 0.4671713411808014 + 50.0 * 8.346479415893555
Epoch 1160, val loss: 0.5504394173622131
Epoch 1170, training loss: 417.70758056640625 = 0.465772807598114 + 50.0 * 8.344836235046387
Epoch 1170, val loss: 0.5492368936538696
Epoch 1180, training loss: 417.731689453125 = 0.464368611574173 + 50.0 * 8.345346450805664
Epoch 1180, val loss: 0.5481434464454651
Epoch 1190, training loss: 417.6409606933594 = 0.4629553556442261 + 50.0 * 8.343560218811035
Epoch 1190, val loss: 0.547074019908905
Epoch 1200, training loss: 417.6584777832031 = 0.4615596830844879 + 50.0 * 8.343938827514648
Epoch 1200, val loss: 0.5460057854652405
Epoch 1210, training loss: 417.7262878417969 = 0.4601048529148102 + 50.0 * 8.34532356262207
Epoch 1210, val loss: 0.5449492931365967
Epoch 1220, training loss: 417.62188720703125 = 0.4586307406425476 + 50.0 * 8.343265533447266
Epoch 1220, val loss: 0.5438618063926697
Epoch 1230, training loss: 417.61083984375 = 0.45718154311180115 + 50.0 * 8.343072891235352
Epoch 1230, val loss: 0.5428054928779602
Epoch 1240, training loss: 417.62744140625 = 0.45571476221084595 + 50.0 * 8.34343433380127
Epoch 1240, val loss: 0.5416218638420105
Epoch 1250, training loss: 417.55926513671875 = 0.45423054695129395 + 50.0 * 8.342101097106934
Epoch 1250, val loss: 0.5403831005096436
Epoch 1260, training loss: 417.4855651855469 = 0.45277532935142517 + 50.0 * 8.340655326843262
Epoch 1260, val loss: 0.5392856597900391
Epoch 1270, training loss: 418.0066223144531 = 0.4513126015663147 + 50.0 * 8.351105690002441
Epoch 1270, val loss: 0.537857711315155
Epoch 1280, training loss: 417.6334228515625 = 0.4496850371360779 + 50.0 * 8.343674659729004
Epoch 1280, val loss: 0.536981999874115
Epoch 1290, training loss: 417.4391784667969 = 0.44817468523979187 + 50.0 * 8.33981990814209
Epoch 1290, val loss: 0.5357028245925903
Epoch 1300, training loss: 417.39593505859375 = 0.4467071294784546 + 50.0 * 8.338984489440918
Epoch 1300, val loss: 0.534538984298706
Epoch 1310, training loss: 417.51031494140625 = 0.4452390968799591 + 50.0 * 8.341300964355469
Epoch 1310, val loss: 0.5334824323654175
Epoch 1320, training loss: 417.4022216796875 = 0.443654865026474 + 50.0 * 8.339171409606934
Epoch 1320, val loss: 0.5321938395500183
Epoch 1330, training loss: 417.39208984375 = 0.4420951008796692 + 50.0 * 8.33899974822998
Epoch 1330, val loss: 0.5310594439506531
Epoch 1340, training loss: 417.3503112792969 = 0.44059181213378906 + 50.0 * 8.338194847106934
Epoch 1340, val loss: 0.5299969911575317
Epoch 1350, training loss: 417.3089599609375 = 0.43909868597984314 + 50.0 * 8.337397575378418
Epoch 1350, val loss: 0.5288582444190979
Epoch 1360, training loss: 417.6056823730469 = 0.4376002848148346 + 50.0 * 8.343361854553223
Epoch 1360, val loss: 0.5279056429862976
Epoch 1370, training loss: 417.37530517578125 = 0.4359223246574402 + 50.0 * 8.338788032531738
Epoch 1370, val loss: 0.5263035893440247
Epoch 1380, training loss: 417.3509826660156 = 0.434337854385376 + 50.0 * 8.338333129882812
Epoch 1380, val loss: 0.5254055857658386
Epoch 1390, training loss: 417.24822998046875 = 0.4327986538410187 + 50.0 * 8.336308479309082
Epoch 1390, val loss: 0.5241647362709045
Epoch 1400, training loss: 417.2161865234375 = 0.43128135800361633 + 50.0 * 8.335698127746582
Epoch 1400, val loss: 0.5231047868728638
Epoch 1410, training loss: 417.51544189453125 = 0.4297363758087158 + 50.0 * 8.341713905334473
Epoch 1410, val loss: 0.522021472454071
Epoch 1420, training loss: 417.2810974121094 = 0.4281056225299835 + 50.0 * 8.33705997467041
Epoch 1420, val loss: 0.5205998420715332
Epoch 1430, training loss: 417.24896240234375 = 0.4265110492706299 + 50.0 * 8.336448669433594
Epoch 1430, val loss: 0.5195314884185791
Epoch 1440, training loss: 417.3858947753906 = 0.424905389547348 + 50.0 * 8.33922004699707
Epoch 1440, val loss: 0.5182732343673706
Epoch 1450, training loss: 417.18896484375 = 0.42328590154647827 + 50.0 * 8.33531379699707
Epoch 1450, val loss: 0.5173984169960022
Epoch 1460, training loss: 417.1206359863281 = 0.42169731855392456 + 50.0 * 8.333978652954102
Epoch 1460, val loss: 0.5162209272384644
Epoch 1470, training loss: 417.12646484375 = 0.42012739181518555 + 50.0 * 8.334126472473145
Epoch 1470, val loss: 0.5152650475502014
Epoch 1480, training loss: 417.489990234375 = 0.4185195863246918 + 50.0 * 8.341429710388184
Epoch 1480, val loss: 0.514369010925293
Epoch 1490, training loss: 417.1319580078125 = 0.41678404808044434 + 50.0 * 8.334303855895996
Epoch 1490, val loss: 0.5127929449081421
Epoch 1500, training loss: 417.0924377441406 = 0.4151502847671509 + 50.0 * 8.333545684814453
Epoch 1500, val loss: 0.5118301510810852
Epoch 1510, training loss: 417.0850524902344 = 0.41354313492774963 + 50.0 * 8.333430290222168
Epoch 1510, val loss: 0.5107754468917847
Epoch 1520, training loss: 417.11895751953125 = 0.4119357466697693 + 50.0 * 8.33414077758789
Epoch 1520, val loss: 0.5097030401229858
Epoch 1530, training loss: 417.0979919433594 = 0.4103054404258728 + 50.0 * 8.33375358581543
Epoch 1530, val loss: 0.5086429715156555
Epoch 1540, training loss: 417.041015625 = 0.40866589546203613 + 50.0 * 8.332647323608398
Epoch 1540, val loss: 0.5075364708900452
Epoch 1550, training loss: 416.9879150390625 = 0.4070368707180023 + 50.0 * 8.33161735534668
Epoch 1550, val loss: 0.5064195990562439
Epoch 1560, training loss: 417.1615295410156 = 0.4054170250892639 + 50.0 * 8.335122108459473
Epoch 1560, val loss: 0.5054419040679932
Epoch 1570, training loss: 416.96954345703125 = 0.4037443995475769 + 50.0 * 8.331315994262695
Epoch 1570, val loss: 0.5045313835144043
Epoch 1580, training loss: 416.92779541015625 = 0.4020932912826538 + 50.0 * 8.330513954162598
Epoch 1580, val loss: 0.5034820437431335
Epoch 1590, training loss: 416.94000244140625 = 0.40047603845596313 + 50.0 * 8.330790519714355
Epoch 1590, val loss: 0.5024110674858093
Epoch 1600, training loss: 417.1324768066406 = 0.3988440930843353 + 50.0 * 8.334672927856445
Epoch 1600, val loss: 0.5014495849609375
Epoch 1610, training loss: 416.9214172363281 = 0.39719632267951965 + 50.0 * 8.330484390258789
Epoch 1610, val loss: 0.5006995797157288
Epoch 1620, training loss: 416.8669128417969 = 0.3955773711204529 + 50.0 * 8.329426765441895
Epoch 1620, val loss: 0.49969127774238586
Epoch 1630, training loss: 417.20660400390625 = 0.3939864933490753 + 50.0 * 8.336252212524414
Epoch 1630, val loss: 0.49915841221809387
Epoch 1640, training loss: 416.98602294921875 = 0.3922733664512634 + 50.0 * 8.33187484741211
Epoch 1640, val loss: 0.4975913166999817
Epoch 1650, training loss: 416.8699035644531 = 0.3906492292881012 + 50.0 * 8.329585075378418
Epoch 1650, val loss: 0.49694764614105225
Epoch 1660, training loss: 416.8190612792969 = 0.38905903697013855 + 50.0 * 8.32859992980957
Epoch 1660, val loss: 0.4959830045700073
Epoch 1670, training loss: 416.9251403808594 = 0.3874892294406891 + 50.0 * 8.330753326416016
Epoch 1670, val loss: 0.49523505568504333
Epoch 1680, training loss: 417.02691650390625 = 0.3858451545238495 + 50.0 * 8.3328218460083
Epoch 1680, val loss: 0.49433133006095886
Epoch 1690, training loss: 416.7945251464844 = 0.3841792047023773 + 50.0 * 8.328207015991211
Epoch 1690, val loss: 0.49346938729286194
Epoch 1700, training loss: 416.76220703125 = 0.38258495926856995 + 50.0 * 8.327591896057129
Epoch 1700, val loss: 0.4926961362361908
Epoch 1710, training loss: 416.7218017578125 = 0.3810235559940338 + 50.0 * 8.326815605163574
Epoch 1710, val loss: 0.4918721616268158
Epoch 1720, training loss: 416.7032470703125 = 0.379481703042984 + 50.0 * 8.326475143432617
Epoch 1720, val loss: 0.4912055432796478
Epoch 1730, training loss: 416.77801513671875 = 0.3779337704181671 + 50.0 * 8.328001976013184
Epoch 1730, val loss: 0.49038687348365784
Epoch 1740, training loss: 416.8587951660156 = 0.37633708119392395 + 50.0 * 8.329648971557617
Epoch 1740, val loss: 0.4897327423095703
Epoch 1750, training loss: 416.7524719238281 = 0.37470176815986633 + 50.0 * 8.327555656433105
Epoch 1750, val loss: 0.4891173243522644
Epoch 1760, training loss: 416.708984375 = 0.3730911910533905 + 50.0 * 8.3267183303833
Epoch 1760, val loss: 0.48817211389541626
Epoch 1770, training loss: 416.7008972167969 = 0.3715376853942871 + 50.0 * 8.326586723327637
Epoch 1770, val loss: 0.48750731348991394
Epoch 1780, training loss: 416.7310791015625 = 0.37000852823257446 + 50.0 * 8.327221870422363
Epoch 1780, val loss: 0.4870506823062897
Epoch 1790, training loss: 416.66729736328125 = 0.3684561252593994 + 50.0 * 8.325976371765137
Epoch 1790, val loss: 0.4863128960132599
Epoch 1800, training loss: 416.6723937988281 = 0.3669051229953766 + 50.0 * 8.326109886169434
Epoch 1800, val loss: 0.485503613948822
Epoch 1810, training loss: 416.6843566894531 = 0.365365594625473 + 50.0 * 8.326379776000977
Epoch 1810, val loss: 0.48480838537216187
Epoch 1820, training loss: 416.5942687988281 = 0.3638226091861725 + 50.0 * 8.32460880279541
Epoch 1820, val loss: 0.4842303693294525
Epoch 1830, training loss: 416.64459228515625 = 0.36230024695396423 + 50.0 * 8.325645446777344
Epoch 1830, val loss: 0.483638197183609
Epoch 1840, training loss: 416.76837158203125 = 0.3607644736766815 + 50.0 * 8.32815170288086
Epoch 1840, val loss: 0.48293542861938477
Epoch 1850, training loss: 416.7358703613281 = 0.3592214584350586 + 50.0 * 8.327532768249512
Epoch 1850, val loss: 0.48283857107162476
Epoch 1860, training loss: 416.5508117675781 = 0.3576745390892029 + 50.0 * 8.32386302947998
Epoch 1860, val loss: 0.48200005292892456
Epoch 1870, training loss: 416.5206604003906 = 0.356183260679245 + 50.0 * 8.32328987121582
Epoch 1870, val loss: 0.481656014919281
Epoch 1880, training loss: 416.60589599609375 = 0.3547051250934601 + 50.0 * 8.325023651123047
Epoch 1880, val loss: 0.4808829426765442
Epoch 1890, training loss: 416.6550598144531 = 0.3531787097454071 + 50.0 * 8.326037406921387
Epoch 1890, val loss: 0.480509877204895
Epoch 1900, training loss: 416.553955078125 = 0.3516891896724701 + 50.0 * 8.324045181274414
Epoch 1900, val loss: 0.48037952184677124
Epoch 1910, training loss: 416.48822021484375 = 0.35018569231033325 + 50.0 * 8.322760581970215
Epoch 1910, val loss: 0.47967028617858887
Epoch 1920, training loss: 416.5043640136719 = 0.3487282991409302 + 50.0 * 8.323112487792969
Epoch 1920, val loss: 0.4794401526451111
Epoch 1930, training loss: 416.68585205078125 = 0.34724918007850647 + 50.0 * 8.32677173614502
Epoch 1930, val loss: 0.4788675606250763
Epoch 1940, training loss: 416.5572814941406 = 0.3457637429237366 + 50.0 * 8.324230194091797
Epoch 1940, val loss: 0.4784584641456604
Epoch 1950, training loss: 416.46240234375 = 0.34428802132606506 + 50.0 * 8.322361946105957
Epoch 1950, val loss: 0.47805091738700867
Epoch 1960, training loss: 416.4647521972656 = 0.3428319990634918 + 50.0 * 8.32243824005127
Epoch 1960, val loss: 0.47755616903305054
Epoch 1970, training loss: 416.537109375 = 0.3413830101490021 + 50.0 * 8.323914527893066
Epoch 1970, val loss: 0.47728049755096436
Epoch 1980, training loss: 416.4148864746094 = 0.33994099497795105 + 50.0 * 8.32149887084961
Epoch 1980, val loss: 0.47712063789367676
Epoch 1990, training loss: 416.35711669921875 = 0.33850863575935364 + 50.0 * 8.320372581481934
Epoch 1990, val loss: 0.4766581654548645
Epoch 2000, training loss: 416.43548583984375 = 0.33708876371383667 + 50.0 * 8.321968078613281
Epoch 2000, val loss: 0.47630321979522705
Epoch 2010, training loss: 416.43603515625 = 0.3356407880783081 + 50.0 * 8.32200813293457
Epoch 2010, val loss: 0.4760241210460663
Epoch 2020, training loss: 416.3717346191406 = 0.3341822922229767 + 50.0 * 8.320751190185547
Epoch 2020, val loss: 0.4755217730998993
Epoch 2030, training loss: 416.5819091796875 = 0.332741916179657 + 50.0 * 8.324983596801758
Epoch 2030, val loss: 0.47506824135780334
Epoch 2040, training loss: 416.4216003417969 = 0.33126991987228394 + 50.0 * 8.321806907653809
Epoch 2040, val loss: 0.4747113585472107
Epoch 2050, training loss: 416.3341064453125 = 0.3298254907131195 + 50.0 * 8.320085525512695
Epoch 2050, val loss: 0.4744569659233093
Epoch 2060, training loss: 416.2841796875 = 0.32841551303863525 + 50.0 * 8.319114685058594
Epoch 2060, val loss: 0.47440966963768005
Epoch 2070, training loss: 416.2773132324219 = 0.3270203173160553 + 50.0 * 8.319005966186523
Epoch 2070, val loss: 0.47412994503974915
Epoch 2080, training loss: 416.4521789550781 = 0.3256610929965973 + 50.0 * 8.322530746459961
Epoch 2080, val loss: 0.4743618965148926
Epoch 2090, training loss: 416.2573547363281 = 0.32418888807296753 + 50.0 * 8.318663597106934
Epoch 2090, val loss: 0.4735657572746277
Epoch 2100, training loss: 416.3525085449219 = 0.3227795958518982 + 50.0 * 8.320594787597656
Epoch 2100, val loss: 0.47309210896492004
Epoch 2110, training loss: 416.47283935546875 = 0.3213652968406677 + 50.0 * 8.323029518127441
Epoch 2110, val loss: 0.4731205403804779
Epoch 2120, training loss: 416.29766845703125 = 0.3199590742588043 + 50.0 * 8.319554328918457
Epoch 2120, val loss: 0.473125696182251
Epoch 2130, training loss: 416.2462158203125 = 0.31857386231422424 + 50.0 * 8.31855297088623
Epoch 2130, val loss: 0.47292953729629517
Epoch 2140, training loss: 416.28375244140625 = 0.31718674302101135 + 50.0 * 8.319331169128418
Epoch 2140, val loss: 0.472732275724411
Epoch 2150, training loss: 416.23736572265625 = 0.31581294536590576 + 50.0 * 8.31843090057373
Epoch 2150, val loss: 0.472705602645874
Epoch 2160, training loss: 416.2219543457031 = 0.3144180178642273 + 50.0 * 8.318150520324707
Epoch 2160, val loss: 0.47228264808654785
Epoch 2170, training loss: 416.396484375 = 0.3130350112915039 + 50.0 * 8.32166862487793
Epoch 2170, val loss: 0.4720781147480011
Epoch 2180, training loss: 416.23052978515625 = 0.31164008378982544 + 50.0 * 8.318377494812012
Epoch 2180, val loss: 0.4720078110694885
Epoch 2190, training loss: 416.2108459472656 = 0.31025636196136475 + 50.0 * 8.318012237548828
Epoch 2190, val loss: 0.47187864780426025
Epoch 2200, training loss: 416.30999755859375 = 0.308881938457489 + 50.0 * 8.320022583007812
Epoch 2200, val loss: 0.4719570577144623
Epoch 2210, training loss: 416.17742919921875 = 0.3075186014175415 + 50.0 * 8.317398071289062
Epoch 2210, val loss: 0.47215351462364197
Epoch 2220, training loss: 416.1712341308594 = 0.30615949630737305 + 50.0 * 8.317301750183105
Epoch 2220, val loss: 0.47207698225975037
Epoch 2230, training loss: 416.1858215332031 = 0.3048025667667389 + 50.0 * 8.317620277404785
Epoch 2230, val loss: 0.472098171710968
Epoch 2240, training loss: 416.13043212890625 = 0.30344250798225403 + 50.0 * 8.316539764404297
Epoch 2240, val loss: 0.47194212675094604
Epoch 2250, training loss: 416.1823425292969 = 0.30208781361579895 + 50.0 * 8.317605018615723
Epoch 2250, val loss: 0.47171324491500854
Epoch 2260, training loss: 416.340087890625 = 0.30073562264442444 + 50.0 * 8.320786476135254
Epoch 2260, val loss: 0.4711303412914276
Epoch 2270, training loss: 416.12542724609375 = 0.29937899112701416 + 50.0 * 8.316520690917969
Epoch 2270, val loss: 0.4718232750892639
Epoch 2280, training loss: 416.0929260253906 = 0.29802632331848145 + 50.0 * 8.315897941589355
Epoch 2280, val loss: 0.47146496176719666
Epoch 2290, training loss: 416.2923583984375 = 0.29671019315719604 + 50.0 * 8.319912910461426
Epoch 2290, val loss: 0.47139328718185425
Epoch 2300, training loss: 416.0467529296875 = 0.29535743594169617 + 50.0 * 8.315028190612793
Epoch 2300, val loss: 0.4716077148914337
Epoch 2310, training loss: 416.052490234375 = 0.2940398156642914 + 50.0 * 8.315169334411621
Epoch 2310, val loss: 0.4718392789363861
Epoch 2320, training loss: 416.0288391113281 = 0.2927151322364807 + 50.0 * 8.314722061157227
Epoch 2320, val loss: 0.4717288613319397
Epoch 2330, training loss: 416.02459716796875 = 0.2914170026779175 + 50.0 * 8.314663887023926
Epoch 2330, val loss: 0.47197651863098145
Epoch 2340, training loss: 416.289306640625 = 0.29011571407318115 + 50.0 * 8.31998348236084
Epoch 2340, val loss: 0.4719370901584625
Epoch 2350, training loss: 416.0496826171875 = 0.2888011634349823 + 50.0 * 8.315217971801758
Epoch 2350, val loss: 0.4724084138870239
Epoch 2360, training loss: 416.0705261230469 = 0.2874942719936371 + 50.0 * 8.31566047668457
Epoch 2360, val loss: 0.47238612174987793
Epoch 2370, training loss: 416.1418762207031 = 0.2862299978733063 + 50.0 * 8.317112922668457
Epoch 2370, val loss: 0.47305840253829956
Epoch 2380, training loss: 416.1486511230469 = 0.2849056124687195 + 50.0 * 8.317275047302246
Epoch 2380, val loss: 0.4727262258529663
Epoch 2390, training loss: 415.98065185546875 = 0.2835994064807892 + 50.0 * 8.31394100189209
Epoch 2390, val loss: 0.4727173149585724
Epoch 2400, training loss: 415.947265625 = 0.2823198735713959 + 50.0 * 8.313299179077148
Epoch 2400, val loss: 0.47262832522392273
Epoch 2410, training loss: 415.9835205078125 = 0.2810508608818054 + 50.0 * 8.31404972076416
Epoch 2410, val loss: 0.47263070940971375
Epoch 2420, training loss: 416.1341857910156 = 0.279782235622406 + 50.0 * 8.31708812713623
Epoch 2420, val loss: 0.47296708822250366
Epoch 2430, training loss: 416.13848876953125 = 0.27856016159057617 + 50.0 * 8.317198753356934
Epoch 2430, val loss: 0.4740414023399353
Epoch 2440, training loss: 416.0523376464844 = 0.27720698714256287 + 50.0 * 8.315502166748047
Epoch 2440, val loss: 0.4730103611946106
Epoch 2450, training loss: 415.970458984375 = 0.27593860030174255 + 50.0 * 8.31389045715332
Epoch 2450, val loss: 0.4735354781150818
Epoch 2460, training loss: 415.9169921875 = 0.2746936082839966 + 50.0 * 8.312846183776855
Epoch 2460, val loss: 0.4737989604473114
Epoch 2470, training loss: 415.9038391113281 = 0.2734479010105133 + 50.0 * 8.312607765197754
Epoch 2470, val loss: 0.4739520251750946
Epoch 2480, training loss: 415.9652099609375 = 0.27221202850341797 + 50.0 * 8.313859939575195
Epoch 2480, val loss: 0.47427898645401
Epoch 2490, training loss: 416.0014343261719 = 0.2709575593471527 + 50.0 * 8.31460952758789
Epoch 2490, val loss: 0.4745252728462219
Epoch 2500, training loss: 415.9660339355469 = 0.2696968615055084 + 50.0 * 8.313926696777344
Epoch 2500, val loss: 0.47461599111557007
Epoch 2510, training loss: 415.9981994628906 = 0.2684408724308014 + 50.0 * 8.314595222473145
Epoch 2510, val loss: 0.4747682213783264
Epoch 2520, training loss: 415.91650390625 = 0.2671797573566437 + 50.0 * 8.312986373901367
Epoch 2520, val loss: 0.4754076600074768
Epoch 2530, training loss: 415.9304504394531 = 0.2659517824649811 + 50.0 * 8.313289642333984
Epoch 2530, val loss: 0.4759436547756195
Epoch 2540, training loss: 416.0162048339844 = 0.26469704508781433 + 50.0 * 8.315030097961426
Epoch 2540, val loss: 0.47572922706604004
Epoch 2550, training loss: 415.8922424316406 = 0.26343658566474915 + 50.0 * 8.312576293945312
Epoch 2550, val loss: 0.4762021601200104
Epoch 2560, training loss: 415.8362731933594 = 0.262210875749588 + 50.0 * 8.311481475830078
Epoch 2560, val loss: 0.47695091366767883
Epoch 2570, training loss: 415.80792236328125 = 0.26097601652145386 + 50.0 * 8.310938835144043
Epoch 2570, val loss: 0.47717082500457764
Epoch 2580, training loss: 415.8038024902344 = 0.25976207852363586 + 50.0 * 8.310880661010742
Epoch 2580, val loss: 0.4776885509490967
Epoch 2590, training loss: 415.9615173339844 = 0.25855380296707153 + 50.0 * 8.314059257507324
Epoch 2590, val loss: 0.47816911339759827
Epoch 2600, training loss: 415.81634521484375 = 0.2573031485080719 + 50.0 * 8.31118106842041
Epoch 2600, val loss: 0.4781360924243927
Epoch 2610, training loss: 415.89453125 = 0.25607526302337646 + 50.0 * 8.312768936157227
Epoch 2610, val loss: 0.4785196781158447
Epoch 2620, training loss: 415.9149475097656 = 0.2548491060733795 + 50.0 * 8.313201904296875
Epoch 2620, val loss: 0.479267418384552
Epoch 2630, training loss: 415.8636474609375 = 0.2536347210407257 + 50.0 * 8.312200546264648
Epoch 2630, val loss: 0.47963544726371765
Epoch 2640, training loss: 415.9757995605469 = 0.25243645906448364 + 50.0 * 8.314467430114746
Epoch 2640, val loss: 0.47986191511154175
Epoch 2650, training loss: 415.8461608886719 = 0.2512414753437042 + 50.0 * 8.311898231506348
Epoch 2650, val loss: 0.4812619686126709
Epoch 2660, training loss: 415.8559265136719 = 0.2500670254230499 + 50.0 * 8.312117576599121
Epoch 2660, val loss: 0.48162317276000977
Epoch 2670, training loss: 415.72991943359375 = 0.2488352358341217 + 50.0 * 8.309621810913086
Epoch 2670, val loss: 0.4814669191837311
Epoch 2680, training loss: 415.7313537597656 = 0.24765512347221375 + 50.0 * 8.309674263000488
Epoch 2680, val loss: 0.4820915162563324
Epoch 2690, training loss: 415.76116943359375 = 0.24647386372089386 + 50.0 * 8.310294151306152
Epoch 2690, val loss: 0.4824571907520294
Epoch 2700, training loss: 415.9036865234375 = 0.24529755115509033 + 50.0 * 8.313167572021484
Epoch 2700, val loss: 0.48311448097229004
Epoch 2710, training loss: 415.8320007324219 = 0.2441021054983139 + 50.0 * 8.311758041381836
Epoch 2710, val loss: 0.4837374985218048
Epoch 2720, training loss: 415.7363586425781 = 0.24291260540485382 + 50.0 * 8.309868812561035
Epoch 2720, val loss: 0.48438259959220886
Epoch 2730, training loss: 415.70941162109375 = 0.24173490703105927 + 50.0 * 8.309353828430176
Epoch 2730, val loss: 0.4849614202976227
Epoch 2740, training loss: 415.6926574707031 = 0.2405586540699005 + 50.0 * 8.309041976928711
Epoch 2740, val loss: 0.4854341149330139
Epoch 2750, training loss: 415.76141357421875 = 0.23941098153591156 + 50.0 * 8.310440063476562
Epoch 2750, val loss: 0.4862895905971527
Epoch 2760, training loss: 415.7538146972656 = 0.23824459314346313 + 50.0 * 8.310311317443848
Epoch 2760, val loss: 0.486720472574234
Epoch 2770, training loss: 415.80279541015625 = 0.23706941306591034 + 50.0 * 8.311314582824707
Epoch 2770, val loss: 0.4872715175151825
Epoch 2780, training loss: 415.8078918457031 = 0.23594391345977783 + 50.0 * 8.311439514160156
Epoch 2780, val loss: 0.48816657066345215
Epoch 2790, training loss: 415.73565673828125 = 0.23474185168743134 + 50.0 * 8.310018539428711
Epoch 2790, val loss: 0.4875960648059845
Epoch 2800, training loss: 415.71551513671875 = 0.23359178006649017 + 50.0 * 8.309638977050781
Epoch 2800, val loss: 0.488547682762146
Epoch 2810, training loss: 415.7878723144531 = 0.2324550449848175 + 50.0 * 8.311108589172363
Epoch 2810, val loss: 0.489277720451355
Epoch 2820, training loss: 415.6863098144531 = 0.23130744695663452 + 50.0 * 8.309100151062012
Epoch 2820, val loss: 0.49003538489341736
Epoch 2830, training loss: 415.6695861816406 = 0.2301674485206604 + 50.0 * 8.308788299560547
Epoch 2830, val loss: 0.49066752195358276
Epoch 2840, training loss: 415.80047607421875 = 0.22902654111385345 + 50.0 * 8.311429023742676
Epoch 2840, val loss: 0.4910937547683716
Epoch 2850, training loss: 415.61737060546875 = 0.22787998616695404 + 50.0 * 8.30778980255127
Epoch 2850, val loss: 0.4918522536754608
Epoch 2860, training loss: 415.5981750488281 = 0.22674217820167542 + 50.0 * 8.307428359985352
Epoch 2860, val loss: 0.49305203557014465
Epoch 2870, training loss: 415.5917053222656 = 0.2256082445383072 + 50.0 * 8.307321548461914
Epoch 2870, val loss: 0.49379539489746094
Epoch 2880, training loss: 415.6734313964844 = 0.22449222207069397 + 50.0 * 8.308979034423828
Epoch 2880, val loss: 0.49480873346328735
Epoch 2890, training loss: 415.69915771484375 = 0.2233601212501526 + 50.0 * 8.309515953063965
Epoch 2890, val loss: 0.49540889263153076
Epoch 2900, training loss: 415.6460266113281 = 0.22221146523952484 + 50.0 * 8.308476448059082
Epoch 2900, val loss: 0.4953494071960449
Epoch 2910, training loss: 415.63055419921875 = 0.2210671752691269 + 50.0 * 8.308189392089844
Epoch 2910, val loss: 0.49646231532096863
Epoch 2920, training loss: 415.93206787109375 = 0.21993930637836456 + 50.0 * 8.314242362976074
Epoch 2920, val loss: 0.4974864423274994
Epoch 2930, training loss: 415.67919921875 = 0.21883895993232727 + 50.0 * 8.30920696258545
Epoch 2930, val loss: 0.4988657534122467
Epoch 2940, training loss: 415.5883483886719 = 0.2176460176706314 + 50.0 * 8.307414054870605
Epoch 2940, val loss: 0.4988343417644501
Epoch 2950, training loss: 415.6220397949219 = 0.2165248841047287 + 50.0 * 8.308110237121582
Epoch 2950, val loss: 0.5000074505805969
Epoch 2960, training loss: 415.6127014160156 = 0.21539756655693054 + 50.0 * 8.30794620513916
Epoch 2960, val loss: 0.5006346702575684
Epoch 2970, training loss: 415.5189514160156 = 0.21424220502376556 + 50.0 * 8.3060941696167
Epoch 2970, val loss: 0.5011128783226013
Epoch 2980, training loss: 415.59002685546875 = 0.21312522888183594 + 50.0 * 8.307538032531738
Epoch 2980, val loss: 0.5018443465232849
Epoch 2990, training loss: 415.6473083496094 = 0.2120254635810852 + 50.0 * 8.30870532989502
Epoch 2990, val loss: 0.5036193132400513
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8229325215626585
0.8404694631601827
=== training gcn model ===
Epoch 0, training loss: 530.2054443359375 = 1.0944445133209229 + 50.0 * 10.582221031188965
Epoch 0, val loss: 1.0944294929504395
Epoch 10, training loss: 530.1396484375 = 1.0880552530288696 + 50.0 * 10.581031799316406
Epoch 10, val loss: 1.0879532098770142
Epoch 20, training loss: 529.6420288085938 = 1.0801994800567627 + 50.0 * 10.571236610412598
Epoch 20, val loss: 1.0799899101257324
Epoch 30, training loss: 526.3330688476562 = 1.0706429481506348 + 50.0 * 10.5052490234375
Epoch 30, val loss: 1.07036292552948
Epoch 40, training loss: 512.656982421875 = 1.061126947402954 + 50.0 * 10.231917381286621
Epoch 40, val loss: 1.061155080795288
Epoch 50, training loss: 481.2324523925781 = 1.0521776676177979 + 50.0 * 9.603605270385742
Epoch 50, val loss: 1.052319049835205
Epoch 60, training loss: 469.7542419433594 = 1.0444908142089844 + 50.0 * 9.374195098876953
Epoch 60, val loss: 1.0448625087738037
Epoch 70, training loss: 461.73150634765625 = 1.0365368127822876 + 50.0 * 9.213899612426758
Epoch 70, val loss: 1.0372954607009888
Epoch 80, training loss: 459.6789245605469 = 1.0288913249969482 + 50.0 * 9.17300033569336
Epoch 80, val loss: 1.0299994945526123
Epoch 90, training loss: 457.1212463378906 = 1.021612286567688 + 50.0 * 9.121993064880371
Epoch 90, val loss: 1.0230921506881714
Epoch 100, training loss: 453.13134765625 = 1.0156663656234741 + 50.0 * 9.042313575744629
Epoch 100, val loss: 1.01759934425354
Epoch 110, training loss: 447.9116516113281 = 1.011672019958496 + 50.0 * 8.937999725341797
Epoch 110, val loss: 1.013982892036438
Epoch 120, training loss: 443.98907470703125 = 1.008350133895874 + 50.0 * 8.859614372253418
Epoch 120, val loss: 1.010804533958435
Epoch 130, training loss: 440.9383239746094 = 1.0032793283462524 + 50.0 * 8.798701286315918
Epoch 130, val loss: 1.0057982206344604
Epoch 140, training loss: 438.74017333984375 = 0.9970443844795227 + 50.0 * 8.754862785339355
Epoch 140, val loss: 0.9998176097869873
Epoch 150, training loss: 436.5630187988281 = 0.9911054372787476 + 50.0 * 8.711438179016113
Epoch 150, val loss: 0.9943102598190308
Epoch 160, training loss: 434.817138671875 = 0.9850748181343079 + 50.0 * 8.676641464233398
Epoch 160, val loss: 0.9884909391403198
Epoch 170, training loss: 432.93878173828125 = 0.9779905676841736 + 50.0 * 8.639215469360352
Epoch 170, val loss: 0.9818536639213562
Epoch 180, training loss: 431.33990478515625 = 0.9708319902420044 + 50.0 * 8.607381820678711
Epoch 180, val loss: 0.9750890135765076
Epoch 190, training loss: 429.9671630859375 = 0.9630212783813477 + 50.0 * 8.580082893371582
Epoch 190, val loss: 0.9677096605300903
Epoch 200, training loss: 429.15570068359375 = 0.9541316032409668 + 50.0 * 8.564031600952148
Epoch 200, val loss: 0.9592085480690002
Epoch 210, training loss: 428.42242431640625 = 0.9437326788902283 + 50.0 * 8.54957389831543
Epoch 210, val loss: 0.9492953419685364
Epoch 220, training loss: 427.88067626953125 = 0.9322304129600525 + 50.0 * 8.538969039916992
Epoch 220, val loss: 0.9383814930915833
Epoch 230, training loss: 427.4269104003906 = 0.920053243637085 + 50.0 * 8.530137062072754
Epoch 230, val loss: 0.926871120929718
Epoch 240, training loss: 427.06170654296875 = 0.9073765873908997 + 50.0 * 8.523086547851562
Epoch 240, val loss: 0.9149332642555237
Epoch 250, training loss: 426.6171569824219 = 0.8941523432731628 + 50.0 * 8.514459609985352
Epoch 250, val loss: 0.902549147605896
Epoch 260, training loss: 426.2657165527344 = 0.8804900050163269 + 50.0 * 8.507704734802246
Epoch 260, val loss: 0.8897513151168823
Epoch 270, training loss: 425.8059387207031 = 0.8664731383323669 + 50.0 * 8.498788833618164
Epoch 270, val loss: 0.8766080141067505
Epoch 280, training loss: 425.517333984375 = 0.8521671295166016 + 50.0 * 8.493303298950195
Epoch 280, val loss: 0.8632842302322388
Epoch 290, training loss: 425.1236267089844 = 0.837580680847168 + 50.0 * 8.48572063446045
Epoch 290, val loss: 0.8497128486633301
Epoch 300, training loss: 424.7933044433594 = 0.822849690914154 + 50.0 * 8.479409217834473
Epoch 300, val loss: 0.8360634446144104
Epoch 310, training loss: 424.44281005859375 = 0.8081225156784058 + 50.0 * 8.47269344329834
Epoch 310, val loss: 0.8224290609359741
Epoch 320, training loss: 424.3707580566406 = 0.7934597134590149 + 50.0 * 8.471546173095703
Epoch 320, val loss: 0.8088766932487488
Epoch 330, training loss: 423.8998718261719 = 0.7788066864013672 + 50.0 * 8.462421417236328
Epoch 330, val loss: 0.7955082654953003
Epoch 340, training loss: 423.75762939453125 = 0.7645127177238464 + 50.0 * 8.459861755371094
Epoch 340, val loss: 0.782505989074707
Epoch 350, training loss: 423.44140625 = 0.750629723072052 + 50.0 * 8.453815460205078
Epoch 350, val loss: 0.769902765750885
Epoch 360, training loss: 423.2142028808594 = 0.7372326254844666 + 50.0 * 8.449539184570312
Epoch 360, val loss: 0.7578632831573486
Epoch 370, training loss: 423.0735168457031 = 0.7243191003799438 + 50.0 * 8.44698429107666
Epoch 370, val loss: 0.7462813258171082
Epoch 380, training loss: 422.8346252441406 = 0.711931049823761 + 50.0 * 8.442453384399414
Epoch 380, val loss: 0.7352924346923828
Epoch 390, training loss: 422.59527587890625 = 0.7002802491188049 + 50.0 * 8.437899589538574
Epoch 390, val loss: 0.7250530123710632
Epoch 400, training loss: 422.84930419921875 = 0.6893399357795715 + 50.0 * 8.443199157714844
Epoch 400, val loss: 0.7154067158699036
Epoch 410, training loss: 422.30908203125 = 0.6787359714508057 + 50.0 * 8.43260669708252
Epoch 410, val loss: 0.7062830328941345
Epoch 420, training loss: 422.0389404296875 = 0.6689207553863525 + 50.0 * 8.427400588989258
Epoch 420, val loss: 0.697956919670105
Epoch 430, training loss: 421.8511657714844 = 0.659839391708374 + 50.0 * 8.423826217651367
Epoch 430, val loss: 0.6903162002563477
Epoch 440, training loss: 421.75030517578125 = 0.651369035243988 + 50.0 * 8.421978950500488
Epoch 440, val loss: 0.6832109689712524
Epoch 450, training loss: 421.708251953125 = 0.6433195471763611 + 50.0 * 8.42129898071289
Epoch 450, val loss: 0.6765410304069519
Epoch 460, training loss: 421.494140625 = 0.6358127593994141 + 50.0 * 8.417166709899902
Epoch 460, val loss: 0.6703917980194092
Epoch 470, training loss: 421.2672424316406 = 0.6288782954216003 + 50.0 * 8.41276741027832
Epoch 470, val loss: 0.6648202538490295
Epoch 480, training loss: 421.1633605957031 = 0.6224545240402222 + 50.0 * 8.410818099975586
Epoch 480, val loss: 0.6597328186035156
Epoch 490, training loss: 421.171630859375 = 0.6163469552993774 + 50.0 * 8.41110610961914
Epoch 490, val loss: 0.6549733281135559
Epoch 500, training loss: 420.91619873046875 = 0.6106168031692505 + 50.0 * 8.406111717224121
Epoch 500, val loss: 0.6504471898078918
Epoch 510, training loss: 420.79736328125 = 0.6053223609924316 + 50.0 * 8.403841018676758
Epoch 510, val loss: 0.6464482545852661
Epoch 520, training loss: 420.6606140136719 = 0.6004089713096619 + 50.0 * 8.401204109191895
Epoch 520, val loss: 0.6427609324455261
Epoch 530, training loss: 420.7295227050781 = 0.5957867503166199 + 50.0 * 8.402674674987793
Epoch 530, val loss: 0.639356255531311
Epoch 540, training loss: 420.76568603515625 = 0.5913295149803162 + 50.0 * 8.403487205505371
Epoch 540, val loss: 0.6361174583435059
Epoch 550, training loss: 420.5299072265625 = 0.5870901942253113 + 50.0 * 8.398856163024902
Epoch 550, val loss: 0.6330673694610596
Epoch 560, training loss: 420.3504943847656 = 0.5831509232521057 + 50.0 * 8.395346641540527
Epoch 560, val loss: 0.6302393674850464
Epoch 570, training loss: 420.2139587402344 = 0.5794755816459656 + 50.0 * 8.39268970489502
Epoch 570, val loss: 0.6277072429656982
Epoch 580, training loss: 420.1612854003906 = 0.5760067701339722 + 50.0 * 8.391705513000488
Epoch 580, val loss: 0.6253706216812134
Epoch 590, training loss: 420.1304931640625 = 0.572608232498169 + 50.0 * 8.391158103942871
Epoch 590, val loss: 0.6230309009552002
Epoch 600, training loss: 420.0086975097656 = 0.5693361759185791 + 50.0 * 8.388787269592285
Epoch 600, val loss: 0.620826780796051
Epoch 610, training loss: 419.9313659667969 = 0.5662733316421509 + 50.0 * 8.387301445007324
Epoch 610, val loss: 0.6187664270401001
Epoch 620, training loss: 419.83740234375 = 0.5633634924888611 + 50.0 * 8.385480880737305
Epoch 620, val loss: 0.6168858408927917
Epoch 630, training loss: 420.1132507324219 = 0.5605452656745911 + 50.0 * 8.391054153442383
Epoch 630, val loss: 0.6150301098823547
Epoch 640, training loss: 419.7352294921875 = 0.5577384233474731 + 50.0 * 8.383549690246582
Epoch 640, val loss: 0.613166093826294
Epoch 650, training loss: 419.67291259765625 = 0.555070161819458 + 50.0 * 8.382356643676758
Epoch 650, val loss: 0.6114904880523682
Epoch 660, training loss: 419.5793151855469 = 0.5525412559509277 + 50.0 * 8.380535125732422
Epoch 660, val loss: 0.6098846197128296
Epoch 670, training loss: 419.67034912109375 = 0.5500922203063965 + 50.0 * 8.382405281066895
Epoch 670, val loss: 0.6083354353904724
Epoch 680, training loss: 419.8608093261719 = 0.5476740598678589 + 50.0 * 8.386262893676758
Epoch 680, val loss: 0.6067072153091431
Epoch 690, training loss: 419.4105529785156 = 0.545254647731781 + 50.0 * 8.37730598449707
Epoch 690, val loss: 0.605178713798523
Epoch 700, training loss: 419.3641052246094 = 0.5429821014404297 + 50.0 * 8.376422882080078
Epoch 700, val loss: 0.6037513017654419
Epoch 710, training loss: 419.2767333984375 = 0.5408120155334473 + 50.0 * 8.37471866607666
Epoch 710, val loss: 0.6023900508880615
Epoch 720, training loss: 419.8304443359375 = 0.5386863946914673 + 50.0 * 8.385834693908691
Epoch 720, val loss: 0.6011000871658325
Epoch 730, training loss: 419.2485046386719 = 0.536391019821167 + 50.0 * 8.374242782592773
Epoch 730, val loss: 0.5994683504104614
Epoch 740, training loss: 419.187255859375 = 0.5342502593994141 + 50.0 * 8.37306022644043
Epoch 740, val loss: 0.5980793833732605
Epoch 750, training loss: 419.0789489746094 = 0.5322397351264954 + 50.0 * 8.37093448638916
Epoch 750, val loss: 0.5967940092086792
Epoch 760, training loss: 419.0083312988281 = 0.5302935242652893 + 50.0 * 8.369560241699219
Epoch 760, val loss: 0.5955531597137451
Epoch 770, training loss: 419.3392028808594 = 0.5283620357513428 + 50.0 * 8.376216888427734
Epoch 770, val loss: 0.594237744808197
Epoch 780, training loss: 418.9886169433594 = 0.5263322591781616 + 50.0 * 8.369245529174805
Epoch 780, val loss: 0.592981219291687
Epoch 790, training loss: 418.90802001953125 = 0.5243842005729675 + 50.0 * 8.36767292022705
Epoch 790, val loss: 0.5916791558265686
Epoch 800, training loss: 418.8252868652344 = 0.5225210785865784 + 50.0 * 8.366055488586426
Epoch 800, val loss: 0.5904293656349182
Epoch 810, training loss: 418.7770690917969 = 0.5206966996192932 + 50.0 * 8.365127563476562
Epoch 810, val loss: 0.5892351865768433
Epoch 820, training loss: 419.2161560058594 = 0.5188627243041992 + 50.0 * 8.373946189880371
Epoch 820, val loss: 0.5880040526390076
Epoch 830, training loss: 418.72076416015625 = 0.5169380903244019 + 50.0 * 8.364076614379883
Epoch 830, val loss: 0.586645245552063
Epoch 840, training loss: 418.7393798828125 = 0.5151023268699646 + 50.0 * 8.364485740661621
Epoch 840, val loss: 0.5853791236877441
Epoch 850, training loss: 418.6383056640625 = 0.5133193135261536 + 50.0 * 8.362500190734863
Epoch 850, val loss: 0.5841756463050842
Epoch 860, training loss: 418.6096496582031 = 0.5115811824798584 + 50.0 * 8.361961364746094
Epoch 860, val loss: 0.5830466151237488
Epoch 870, training loss: 418.6889953613281 = 0.5098189115524292 + 50.0 * 8.3635835647583
Epoch 870, val loss: 0.5818341374397278
Epoch 880, training loss: 418.6031799316406 = 0.5080419182777405 + 50.0 * 8.361902236938477
Epoch 880, val loss: 0.5805661678314209
Epoch 890, training loss: 418.4688720703125 = 0.5062912106513977 + 50.0 * 8.359251976013184
Epoch 890, val loss: 0.5793514847755432
Epoch 900, training loss: 418.5205993652344 = 0.5045934915542603 + 50.0 * 8.360320091247559
Epoch 900, val loss: 0.5781851410865784
Epoch 910, training loss: 418.4349365234375 = 0.5028597116470337 + 50.0 * 8.358641624450684
Epoch 910, val loss: 0.5769985914230347
Epoch 920, training loss: 418.3689880371094 = 0.501140832901001 + 50.0 * 8.357357025146484
Epoch 920, val loss: 0.5757842063903809
Epoch 930, training loss: 418.33465576171875 = 0.49949368834495544 + 50.0 * 8.356703758239746
Epoch 930, val loss: 0.5746636986732483
Epoch 940, training loss: 418.438232421875 = 0.4978578984737396 + 50.0 * 8.358807563781738
Epoch 940, val loss: 0.5735394954681396
Epoch 950, training loss: 418.2723083496094 = 0.49617114663124084 + 50.0 * 8.355522155761719
Epoch 950, val loss: 0.5723838210105896
Epoch 960, training loss: 418.2470703125 = 0.4945192337036133 + 50.0 * 8.355051040649414
Epoch 960, val loss: 0.5712456703186035
Epoch 970, training loss: 418.6051025390625 = 0.4928693175315857 + 50.0 * 8.362244606018066
Epoch 970, val loss: 0.570014476776123
Epoch 980, training loss: 418.17022705078125 = 0.49114882946014404 + 50.0 * 8.353581428527832
Epoch 980, val loss: 0.5689524412155151
Epoch 990, training loss: 418.1396179199219 = 0.48951971530914307 + 50.0 * 8.353001594543457
Epoch 990, val loss: 0.5678433179855347
Epoch 1000, training loss: 418.09344482421875 = 0.48793306946754456 + 50.0 * 8.352109909057617
Epoch 1000, val loss: 0.566758394241333
Epoch 1010, training loss: 418.05322265625 = 0.48636433482170105 + 50.0 * 8.351337432861328
Epoch 1010, val loss: 0.5657182335853577
Epoch 1020, training loss: 418.2771301269531 = 0.48478055000305176 + 50.0 * 8.355847358703613
Epoch 1020, val loss: 0.564605176448822
Epoch 1030, training loss: 418.2346496582031 = 0.48305559158325195 + 50.0 * 8.355031967163086
Epoch 1030, val loss: 0.5634472966194153
Epoch 1040, training loss: 418.0431213378906 = 0.4813486337661743 + 50.0 * 8.351235389709473
Epoch 1040, val loss: 0.5623112320899963
Epoch 1050, training loss: 417.9801940917969 = 0.47973528504371643 + 50.0 * 8.350008964538574
Epoch 1050, val loss: 0.5611926317214966
Epoch 1060, training loss: 418.11492919921875 = 0.47814440727233887 + 50.0 * 8.35273551940918
Epoch 1060, val loss: 0.5601595044136047
Epoch 1070, training loss: 417.9236755371094 = 0.4765058159828186 + 50.0 * 8.348943710327148
Epoch 1070, val loss: 0.5590850114822388
Epoch 1080, training loss: 417.8710021972656 = 0.47489845752716064 + 50.0 * 8.347922325134277
Epoch 1080, val loss: 0.5579766035079956
Epoch 1090, training loss: 417.86907958984375 = 0.4733075499534607 + 50.0 * 8.347915649414062
Epoch 1090, val loss: 0.5569424033164978
Epoch 1100, training loss: 418.1419982910156 = 0.4716987907886505 + 50.0 * 8.353405952453613
Epoch 1100, val loss: 0.5558695197105408
Epoch 1110, training loss: 417.982666015625 = 0.47001078724861145 + 50.0 * 8.350253105163574
Epoch 1110, val loss: 0.5548739433288574
Epoch 1120, training loss: 417.80548095703125 = 0.46835413575172424 + 50.0 * 8.346742630004883
Epoch 1120, val loss: 0.5537005662918091
Epoch 1130, training loss: 417.9629821777344 = 0.46673819422721863 + 50.0 * 8.34992504119873
Epoch 1130, val loss: 0.5525721907615662
Epoch 1140, training loss: 417.7490539550781 = 0.4650737941265106 + 50.0 * 8.34567928314209
Epoch 1140, val loss: 0.5516567230224609
Epoch 1150, training loss: 417.7024230957031 = 0.46345600485801697 + 50.0 * 8.344779014587402
Epoch 1150, val loss: 0.5505492687225342
Epoch 1160, training loss: 417.6865539550781 = 0.4618661403656006 + 50.0 * 8.344493865966797
Epoch 1160, val loss: 0.5495450496673584
Epoch 1170, training loss: 417.69219970703125 = 0.46028992533683777 + 50.0 * 8.344637870788574
Epoch 1170, val loss: 0.5486055016517639
Epoch 1180, training loss: 417.8312683105469 = 0.45866304636001587 + 50.0 * 8.347452163696289
Epoch 1180, val loss: 0.5475635528564453
Epoch 1190, training loss: 417.6419677734375 = 0.45698609948158264 + 50.0 * 8.34369945526123
Epoch 1190, val loss: 0.5464823842048645
Epoch 1200, training loss: 417.7646484375 = 0.45534375309944153 + 50.0 * 8.346185684204102
Epoch 1200, val loss: 0.5454630255699158
Epoch 1210, training loss: 417.636962890625 = 0.4536752998828888 + 50.0 * 8.343666076660156
Epoch 1210, val loss: 0.5443037748336792
Epoch 1220, training loss: 417.57196044921875 = 0.45203036069869995 + 50.0 * 8.342398643493652
Epoch 1220, val loss: 0.5433796048164368
Epoch 1230, training loss: 417.7098693847656 = 0.45040038228034973 + 50.0 * 8.345189094543457
Epoch 1230, val loss: 0.5422793626785278
Epoch 1240, training loss: 417.5224304199219 = 0.44870710372924805 + 50.0 * 8.341474533081055
Epoch 1240, val loss: 0.5413310527801514
Epoch 1250, training loss: 417.5079040527344 = 0.4470434784889221 + 50.0 * 8.341217041015625
Epoch 1250, val loss: 0.5402780175209045
Epoch 1260, training loss: 417.51702880859375 = 0.44540494680404663 + 50.0 * 8.341432571411133
Epoch 1260, val loss: 0.5393070578575134
Epoch 1270, training loss: 417.5555725097656 = 0.4437611699104309 + 50.0 * 8.342236518859863
Epoch 1270, val loss: 0.5383375883102417
Epoch 1280, training loss: 417.5110778808594 = 0.4421004354953766 + 50.0 * 8.341379165649414
Epoch 1280, val loss: 0.5374191999435425
Epoch 1290, training loss: 417.461181640625 = 0.44041600823402405 + 50.0 * 8.340415000915527
Epoch 1290, val loss: 0.5363976955413818
Epoch 1300, training loss: 417.4461364746094 = 0.4387402832508087 + 50.0 * 8.340147972106934
Epoch 1300, val loss: 0.5353307723999023
Epoch 1310, training loss: 417.4934387207031 = 0.43707314133644104 + 50.0 * 8.341127395629883
Epoch 1310, val loss: 0.5343658328056335
Epoch 1320, training loss: 417.41241455078125 = 0.435386598110199 + 50.0 * 8.339540481567383
Epoch 1320, val loss: 0.533229649066925
Epoch 1330, training loss: 417.51422119140625 = 0.43368369340896606 + 50.0 * 8.3416109085083
Epoch 1330, val loss: 0.5322043895721436
Epoch 1340, training loss: 417.3611145019531 = 0.43197065591812134 + 50.0 * 8.338582992553711
Epoch 1340, val loss: 0.5312448143959045
Epoch 1350, training loss: 417.3297424316406 = 0.4302932620048523 + 50.0 * 8.33798885345459
Epoch 1350, val loss: 0.5302677750587463
Epoch 1360, training loss: 417.4165954589844 = 0.4286149740219116 + 50.0 * 8.339759826660156
Epoch 1360, val loss: 0.5292050838470459
Epoch 1370, training loss: 417.34454345703125 = 0.42689448595046997 + 50.0 * 8.338353157043457
Epoch 1370, val loss: 0.5282742381095886
Epoch 1380, training loss: 417.3118896484375 = 0.4251716434955597 + 50.0 * 8.33773422241211
Epoch 1380, val loss: 0.527116596698761
Epoch 1390, training loss: 417.254150390625 = 0.423452228307724 + 50.0 * 8.336613655090332
Epoch 1390, val loss: 0.526183009147644
Epoch 1400, training loss: 417.2587890625 = 0.4217575490474701 + 50.0 * 8.336740493774414
Epoch 1400, val loss: 0.5252546668052673
Epoch 1410, training loss: 417.3394775390625 = 0.4200478196144104 + 50.0 * 8.338388442993164
Epoch 1410, val loss: 0.5242413282394409
Epoch 1420, training loss: 417.2790832519531 = 0.418317973613739 + 50.0 * 8.337215423583984
Epoch 1420, val loss: 0.5232093334197998
Epoch 1430, training loss: 417.2901306152344 = 0.41658303141593933 + 50.0 * 8.337471008300781
Epoch 1430, val loss: 0.5222024917602539
Epoch 1440, training loss: 417.218505859375 = 0.41483983397483826 + 50.0 * 8.336073875427246
Epoch 1440, val loss: 0.521152138710022
Epoch 1450, training loss: 417.11376953125 = 0.4131128191947937 + 50.0 * 8.334012985229492
Epoch 1450, val loss: 0.5203179121017456
Epoch 1460, training loss: 417.1251220703125 = 0.41141337156295776 + 50.0 * 8.334274291992188
Epoch 1460, val loss: 0.5194008350372314
Epoch 1470, training loss: 417.2536926269531 = 0.40970221161842346 + 50.0 * 8.33687973022461
Epoch 1470, val loss: 0.5185341238975525
Epoch 1480, training loss: 417.17901611328125 = 0.4079118072986603 + 50.0 * 8.33542251586914
Epoch 1480, val loss: 0.5173768997192383
Epoch 1490, training loss: 417.06304931640625 = 0.4061172902584076 + 50.0 * 8.333138465881348
Epoch 1490, val loss: 0.5164664387702942
Epoch 1500, training loss: 417.03076171875 = 0.4043639302253723 + 50.0 * 8.332528114318848
Epoch 1500, val loss: 0.5154308676719666
Epoch 1510, training loss: 417.026611328125 = 0.40264567732810974 + 50.0 * 8.332479476928711
Epoch 1510, val loss: 0.5146001577377319
Epoch 1520, training loss: 417.3342590332031 = 0.40091943740844727 + 50.0 * 8.338666915893555
Epoch 1520, val loss: 0.5136489272117615
Epoch 1530, training loss: 417.130859375 = 0.3991158604621887 + 50.0 * 8.334634780883789
Epoch 1530, val loss: 0.5126702785491943
Epoch 1540, training loss: 417.0589904785156 = 0.3973488211631775 + 50.0 * 8.333232879638672
Epoch 1540, val loss: 0.511674165725708
Epoch 1550, training loss: 417.0592956542969 = 0.3956066966056824 + 50.0 * 8.333273887634277
Epoch 1550, val loss: 0.5107179880142212
Epoch 1560, training loss: 416.9666748046875 = 0.39386364817619324 + 50.0 * 8.331456184387207
Epoch 1560, val loss: 0.5099412202835083
Epoch 1570, training loss: 417.0826110839844 = 0.39213019609451294 + 50.0 * 8.333809852600098
Epoch 1570, val loss: 0.5090340971946716
Epoch 1580, training loss: 416.98907470703125 = 0.390377402305603 + 50.0 * 8.331974029541016
Epoch 1580, val loss: 0.508136510848999
Epoch 1590, training loss: 416.92864990234375 = 0.388627290725708 + 50.0 * 8.33080005645752
Epoch 1590, val loss: 0.5073299407958984
Epoch 1600, training loss: 416.93463134765625 = 0.38690197467803955 + 50.0 * 8.330954551696777
Epoch 1600, val loss: 0.5065630674362183
Epoch 1610, training loss: 417.12158203125 = 0.3851676285266876 + 50.0 * 8.334728240966797
Epoch 1610, val loss: 0.5056086182594299
Epoch 1620, training loss: 416.9337463378906 = 0.38339996337890625 + 50.0 * 8.33100700378418
Epoch 1620, val loss: 0.5046760439872742
Epoch 1630, training loss: 416.875732421875 = 0.3816710412502289 + 50.0 * 8.329880714416504
Epoch 1630, val loss: 0.5039900541305542
Epoch 1640, training loss: 416.8684387207031 = 0.37996771931648254 + 50.0 * 8.329769134521484
Epoch 1640, val loss: 0.5030598640441895
Epoch 1650, training loss: 417.1749572753906 = 0.3782595098018646 + 50.0 * 8.335933685302734
Epoch 1650, val loss: 0.502152681350708
Epoch 1660, training loss: 416.8871154785156 = 0.37649428844451904 + 50.0 * 8.330212593078613
Epoch 1660, val loss: 0.5015726089477539
Epoch 1670, training loss: 416.8072814941406 = 0.37476876378059387 + 50.0 * 8.32865047454834
Epoch 1670, val loss: 0.5006543397903442
Epoch 1680, training loss: 416.8011169433594 = 0.37307974696159363 + 50.0 * 8.328560829162598
Epoch 1680, val loss: 0.4999513328075409
Epoch 1690, training loss: 417.0373840332031 = 0.37138739228248596 + 50.0 * 8.333319664001465
Epoch 1690, val loss: 0.49920815229415894
Epoch 1700, training loss: 416.8427734375 = 0.36965346336364746 + 50.0 * 8.329462051391602
Epoch 1700, val loss: 0.49841275811195374
Epoch 1710, training loss: 416.8153381347656 = 0.36795857548713684 + 50.0 * 8.328948020935059
Epoch 1710, val loss: 0.49767470359802246
Epoch 1720, training loss: 416.8901062011719 = 0.3662675619125366 + 50.0 * 8.330476760864258
Epoch 1720, val loss: 0.49691736698150635
Epoch 1730, training loss: 416.76580810546875 = 0.3645678162574768 + 50.0 * 8.328024864196777
Epoch 1730, val loss: 0.49634596705436707
Epoch 1740, training loss: 416.7452392578125 = 0.362891286611557 + 50.0 * 8.32764720916748
Epoch 1740, val loss: 0.49560391902923584
Epoch 1750, training loss: 416.84075927734375 = 0.3612149655818939 + 50.0 * 8.329590797424316
Epoch 1750, val loss: 0.494872123003006
Epoch 1760, training loss: 416.8946228027344 = 0.359517365694046 + 50.0 * 8.33070182800293
Epoch 1760, val loss: 0.4941983222961426
Epoch 1770, training loss: 416.7507019042969 = 0.35781702399253845 + 50.0 * 8.327857971191406
Epoch 1770, val loss: 0.4934315085411072
Epoch 1780, training loss: 416.6982421875 = 0.35614410042762756 + 50.0 * 8.326842308044434
Epoch 1780, val loss: 0.4930281639099121
Epoch 1790, training loss: 416.7358093261719 = 0.35450243949890137 + 50.0 * 8.32762622833252
Epoch 1790, val loss: 0.49228060245513916
Epoch 1800, training loss: 416.7062072753906 = 0.35285326838493347 + 50.0 * 8.327067375183105
Epoch 1800, val loss: 0.49182015657424927
Epoch 1810, training loss: 416.6724548339844 = 0.3512137830257416 + 50.0 * 8.326424598693848
Epoch 1810, val loss: 0.491131067276001
Epoch 1820, training loss: 416.721923828125 = 0.3495679199695587 + 50.0 * 8.327446937561035
Epoch 1820, val loss: 0.49058666825294495
Epoch 1830, training loss: 416.6537780761719 = 0.34791481494903564 + 50.0 * 8.326117515563965
Epoch 1830, val loss: 0.49019795656204224
Epoch 1840, training loss: 416.6108093261719 = 0.34627774357795715 + 50.0 * 8.32529067993164
Epoch 1840, val loss: 0.4896604120731354
Epoch 1850, training loss: 416.6936340332031 = 0.34465640783309937 + 50.0 * 8.326979637145996
Epoch 1850, val loss: 0.48907870054244995
Epoch 1860, training loss: 416.6639709472656 = 0.3430095613002777 + 50.0 * 8.32641887664795
Epoch 1860, val loss: 0.4886932969093323
Epoch 1870, training loss: 416.6798095703125 = 0.3413681387901306 + 50.0 * 8.32676887512207
Epoch 1870, val loss: 0.48813432455062866
Epoch 1880, training loss: 416.67755126953125 = 0.33972859382629395 + 50.0 * 8.326756477355957
Epoch 1880, val loss: 0.4875679910182953
Epoch 1890, training loss: 416.6093444824219 = 0.33807793259620667 + 50.0 * 8.325425148010254
Epoch 1890, val loss: 0.4872390925884247
Epoch 1900, training loss: 416.5443420410156 = 0.33647051453590393 + 50.0 * 8.32415771484375
Epoch 1900, val loss: 0.48695293068885803
Epoch 1910, training loss: 416.5291748046875 = 0.3348844349384308 + 50.0 * 8.323885917663574
Epoch 1910, val loss: 0.4865909218788147
Epoch 1920, training loss: 416.56304931640625 = 0.3333016335964203 + 50.0 * 8.324594497680664
Epoch 1920, val loss: 0.48612141609191895
Epoch 1930, training loss: 416.654541015625 = 0.33170101046562195 + 50.0 * 8.326457023620605
Epoch 1930, val loss: 0.4856537878513336
Epoch 1940, training loss: 416.6302795410156 = 0.3300853669643402 + 50.0 * 8.326004028320312
Epoch 1940, val loss: 0.4853150546550751
Epoch 1950, training loss: 416.5416564941406 = 0.3284786641597748 + 50.0 * 8.324263572692871
Epoch 1950, val loss: 0.4849538803100586
Epoch 1960, training loss: 416.48419189453125 = 0.32690247893333435 + 50.0 * 8.323145866394043
Epoch 1960, val loss: 0.48486456274986267
Epoch 1970, training loss: 416.5604248046875 = 0.3253478705883026 + 50.0 * 8.324701309204102
Epoch 1970, val loss: 0.4845736622810364
Epoch 1980, training loss: 416.5209655761719 = 0.32375797629356384 + 50.0 * 8.323944091796875
Epoch 1980, val loss: 0.48412683606147766
Epoch 1990, training loss: 416.476318359375 = 0.3221774995326996 + 50.0 * 8.32308292388916
Epoch 1990, val loss: 0.48379290103912354
Epoch 2000, training loss: 416.4631652832031 = 0.32062283158302307 + 50.0 * 8.322851181030273
Epoch 2000, val loss: 0.4835955798625946
Epoch 2010, training loss: 416.6120300292969 = 0.3190710246562958 + 50.0 * 8.325859069824219
Epoch 2010, val loss: 0.48341605067253113
Epoch 2020, training loss: 416.47052001953125 = 0.3175058364868164 + 50.0 * 8.323060035705566
Epoch 2020, val loss: 0.48342663049697876
Epoch 2030, training loss: 416.4427795410156 = 0.31593143939971924 + 50.0 * 8.322537422180176
Epoch 2030, val loss: 0.48304587602615356
Epoch 2040, training loss: 416.5609436035156 = 0.3143870532512665 + 50.0 * 8.324931144714355
Epoch 2040, val loss: 0.48296117782592773
Epoch 2050, training loss: 416.39312744140625 = 0.31282439827919006 + 50.0 * 8.321605682373047
Epoch 2050, val loss: 0.4825701117515564
Epoch 2060, training loss: 416.3721923828125 = 0.31130021810531616 + 50.0 * 8.32121753692627
Epoch 2060, val loss: 0.4825000762939453
Epoch 2070, training loss: 416.3711242675781 = 0.3097921311855316 + 50.0 * 8.321227073669434
Epoch 2070, val loss: 0.4822600185871124
Epoch 2080, training loss: 416.5762023925781 = 0.30829182267189026 + 50.0 * 8.325358390808105
Epoch 2080, val loss: 0.48192453384399414
Epoch 2090, training loss: 416.4378967285156 = 0.30673015117645264 + 50.0 * 8.322623252868652
Epoch 2090, val loss: 0.48231855034828186
Epoch 2100, training loss: 416.36138916015625 = 0.30518409609794617 + 50.0 * 8.321124076843262
Epoch 2100, val loss: 0.4820418953895569
Epoch 2110, training loss: 416.35498046875 = 0.30366504192352295 + 50.0 * 8.321026802062988
Epoch 2110, val loss: 0.4820309281349182
Epoch 2120, training loss: 416.3499755859375 = 0.30215826630592346 + 50.0 * 8.320956230163574
Epoch 2120, val loss: 0.48195892572402954
Epoch 2130, training loss: 416.4450378417969 = 0.30066096782684326 + 50.0 * 8.322887420654297
Epoch 2130, val loss: 0.4817313253879547
Epoch 2140, training loss: 416.3340759277344 = 0.2991308271884918 + 50.0 * 8.320698738098145
Epoch 2140, val loss: 0.4818940758705139
Epoch 2150, training loss: 416.3163757324219 = 0.2976208031177521 + 50.0 * 8.320375442504883
Epoch 2150, val loss: 0.481876015663147
Epoch 2160, training loss: 416.25994873046875 = 0.29611966013908386 + 50.0 * 8.319276809692383
Epoch 2160, val loss: 0.48183873295783997
Epoch 2170, training loss: 416.31494140625 = 0.29463276267051697 + 50.0 * 8.320405960083008
Epoch 2170, val loss: 0.48181936144828796
Epoch 2180, training loss: 416.539306640625 = 0.2931285798549652 + 50.0 * 8.324923515319824
Epoch 2180, val loss: 0.48183661699295044
Epoch 2190, training loss: 416.3369140625 = 0.29159578680992126 + 50.0 * 8.320906639099121
Epoch 2190, val loss: 0.48186856508255005
Epoch 2200, training loss: 416.23907470703125 = 0.290086954832077 + 50.0 * 8.318979263305664
Epoch 2200, val loss: 0.4819496273994446
Epoch 2210, training loss: 416.2214050292969 = 0.28860899806022644 + 50.0 * 8.318655967712402
Epoch 2210, val loss: 0.48196473717689514
Epoch 2220, training loss: 416.3049011230469 = 0.28715255856513977 + 50.0 * 8.320355415344238
Epoch 2220, val loss: 0.48223957419395447
Epoch 2230, training loss: 416.2674560546875 = 0.2856706976890564 + 50.0 * 8.319635391235352
Epoch 2230, val loss: 0.48220330476760864
Epoch 2240, training loss: 416.3439025878906 = 0.28418827056884766 + 50.0 * 8.321194648742676
Epoch 2240, val loss: 0.48217830061912537
Epoch 2250, training loss: 416.2582092285156 = 0.2826985716819763 + 50.0 * 8.319510459899902
Epoch 2250, val loss: 0.4823668599128723
Epoch 2260, training loss: 416.2120056152344 = 0.28121501207351685 + 50.0 * 8.318615913391113
Epoch 2260, val loss: 0.482278048992157
Epoch 2270, training loss: 416.24212646484375 = 0.27975374460220337 + 50.0 * 8.319247245788574
Epoch 2270, val loss: 0.4824378788471222
Epoch 2280, training loss: 416.2362976074219 = 0.2782862186431885 + 50.0 * 8.319160461425781
Epoch 2280, val loss: 0.4827321767807007
Epoch 2290, training loss: 416.30517578125 = 0.27684104442596436 + 50.0 * 8.320566177368164
Epoch 2290, val loss: 0.4831010103225708
Epoch 2300, training loss: 416.1405944824219 = 0.27533864974975586 + 50.0 * 8.317305564880371
Epoch 2300, val loss: 0.48276758193969727
Epoch 2310, training loss: 416.13494873046875 = 0.27387452125549316 + 50.0 * 8.317221641540527
Epoch 2310, val loss: 0.48297521471977234
Epoch 2320, training loss: 416.14337158203125 = 0.2724211513996124 + 50.0 * 8.317419052124023
Epoch 2320, val loss: 0.4830586016178131
Epoch 2330, training loss: 416.4599609375 = 0.27097564935684204 + 50.0 * 8.323780059814453
Epoch 2330, val loss: 0.4831106960773468
Epoch 2340, training loss: 416.2462158203125 = 0.26949232816696167 + 50.0 * 8.319534301757812
Epoch 2340, val loss: 0.4835195243358612
Epoch 2350, training loss: 416.126953125 = 0.26801425218582153 + 50.0 * 8.317178726196289
Epoch 2350, val loss: 0.4836188852787018
Epoch 2360, training loss: 416.0838317871094 = 0.26656851172447205 + 50.0 * 8.31634521484375
Epoch 2360, val loss: 0.4839123487472534
Epoch 2370, training loss: 416.0716247558594 = 0.2651393413543701 + 50.0 * 8.316129684448242
Epoch 2370, val loss: 0.48406434059143066
Epoch 2380, training loss: 416.214599609375 = 0.2637246251106262 + 50.0 * 8.31901741027832
Epoch 2380, val loss: 0.48404890298843384
Epoch 2390, training loss: 416.12884521484375 = 0.262262225151062 + 50.0 * 8.317331314086914
Epoch 2390, val loss: 0.48461365699768066
Epoch 2400, training loss: 416.100341796875 = 0.2608177065849304 + 50.0 * 8.316790580749512
Epoch 2400, val loss: 0.4851717948913574
Epoch 2410, training loss: 416.0625305175781 = 0.25937625765800476 + 50.0 * 8.316062927246094
Epoch 2410, val loss: 0.48524999618530273
Epoch 2420, training loss: 416.07232666015625 = 0.2579623758792877 + 50.0 * 8.31628704071045
Epoch 2420, val loss: 0.48571768403053284
Epoch 2430, training loss: 416.0805358886719 = 0.2565522789955139 + 50.0 * 8.316479682922363
Epoch 2430, val loss: 0.48599129915237427
Epoch 2440, training loss: 416.1322326660156 = 0.2551421821117401 + 50.0 * 8.31754207611084
Epoch 2440, val loss: 0.4864186644554138
Epoch 2450, training loss: 416.0899963378906 = 0.2537139058113098 + 50.0 * 8.316725730895996
Epoch 2450, val loss: 0.48663851618766785
Epoch 2460, training loss: 416.0559997558594 = 0.2523021399974823 + 50.0 * 8.31607437133789
Epoch 2460, val loss: 0.48718640208244324
Epoch 2470, training loss: 415.9685363769531 = 0.2508823573589325 + 50.0 * 8.314352989196777
Epoch 2470, val loss: 0.48735949397087097
Epoch 2480, training loss: 416.0013122558594 = 0.24949008226394653 + 50.0 * 8.31503677368164
Epoch 2480, val loss: 0.4878133535385132
Epoch 2490, training loss: 416.35589599609375 = 0.24811218678951263 + 50.0 * 8.322155952453613
Epoch 2490, val loss: 0.4883408844470978
Epoch 2500, training loss: 416.0435791015625 = 0.24668072164058685 + 50.0 * 8.315937995910645
Epoch 2500, val loss: 0.4883542060852051
Epoch 2510, training loss: 415.9505310058594 = 0.24527890980243683 + 50.0 * 8.314105033874512
Epoch 2510, val loss: 0.48894065618515015
Epoch 2520, training loss: 415.92584228515625 = 0.24389472603797913 + 50.0 * 8.313638687133789
Epoch 2520, val loss: 0.4893553853034973
Epoch 2530, training loss: 416.0517883300781 = 0.24252237379550934 + 50.0 * 8.316184997558594
Epoch 2530, val loss: 0.4895103871822357
Epoch 2540, training loss: 415.97979736328125 = 0.24112144112586975 + 50.0 * 8.314773559570312
Epoch 2540, val loss: 0.4899901747703552
Epoch 2550, training loss: 415.9122009277344 = 0.23971304297447205 + 50.0 * 8.31344985961914
Epoch 2550, val loss: 0.4906298816204071
Epoch 2560, training loss: 415.9027404785156 = 0.23832713067531586 + 50.0 * 8.313287734985352
Epoch 2560, val loss: 0.491049200296402
Epoch 2570, training loss: 415.88873291015625 = 0.23695698380470276 + 50.0 * 8.313035011291504
Epoch 2570, val loss: 0.491616427898407
Epoch 2580, training loss: 415.9507141113281 = 0.23559820652008057 + 50.0 * 8.314302444458008
Epoch 2580, val loss: 0.49204033613204956
Epoch 2590, training loss: 416.04132080078125 = 0.23422540724277496 + 50.0 * 8.316142082214355
Epoch 2590, val loss: 0.49239620566368103
Epoch 2600, training loss: 415.97039794921875 = 0.23284396529197693 + 50.0 * 8.314750671386719
Epoch 2600, val loss: 0.493000328540802
Epoch 2610, training loss: 415.9883117675781 = 0.2314750701189041 + 50.0 * 8.315136909484863
Epoch 2610, val loss: 0.4937222898006439
Epoch 2620, training loss: 415.9112854003906 = 0.23011113703250885 + 50.0 * 8.313623428344727
Epoch 2620, val loss: 0.49450042843818665
Epoch 2630, training loss: 415.8976745605469 = 0.2287493497133255 + 50.0 * 8.31337833404541
Epoch 2630, val loss: 0.49495184421539307
Epoch 2640, training loss: 416.0016784667969 = 0.22739027440547943 + 50.0 * 8.315485954284668
Epoch 2640, val loss: 0.4953080415725708
Epoch 2650, training loss: 415.8371276855469 = 0.22601871192455292 + 50.0 * 8.312222480773926
Epoch 2650, val loss: 0.495833158493042
Epoch 2660, training loss: 415.82696533203125 = 0.22466568648815155 + 50.0 * 8.31204605102539
Epoch 2660, val loss: 0.4963609576225281
Epoch 2670, training loss: 415.9178161621094 = 0.22332856059074402 + 50.0 * 8.313889503479004
Epoch 2670, val loss: 0.49709978699684143
Epoch 2680, training loss: 415.8988037109375 = 0.2219771146774292 + 50.0 * 8.313536643981934
Epoch 2680, val loss: 0.49767643213272095
Epoch 2690, training loss: 415.84930419921875 = 0.22062990069389343 + 50.0 * 8.312573432922363
Epoch 2690, val loss: 0.4983394145965576
Epoch 2700, training loss: 415.9105529785156 = 0.2193172723054886 + 50.0 * 8.313824653625488
Epoch 2700, val loss: 0.49932700395584106
Epoch 2710, training loss: 415.8583679199219 = 0.21795643866062164 + 50.0 * 8.3128080368042
Epoch 2710, val loss: 0.4995044767856598
Epoch 2720, training loss: 415.8047180175781 = 0.21660847961902618 + 50.0 * 8.311761856079102
Epoch 2720, val loss: 0.49978798627853394
Epoch 2730, training loss: 415.8182373046875 = 0.21527834236621857 + 50.0 * 8.31205940246582
Epoch 2730, val loss: 0.5007397532463074
Epoch 2740, training loss: 415.9921569824219 = 0.21396666765213013 + 50.0 * 8.315564155578613
Epoch 2740, val loss: 0.5014311075210571
Epoch 2750, training loss: 415.8668518066406 = 0.2126304805278778 + 50.0 * 8.313084602355957
Epoch 2750, val loss: 0.5021948218345642
Epoch 2760, training loss: 415.7640075683594 = 0.21129082143306732 + 50.0 * 8.311054229736328
Epoch 2760, val loss: 0.502721905708313
Epoch 2770, training loss: 415.7508850097656 = 0.20996448397636414 + 50.0 * 8.310818672180176
Epoch 2770, val loss: 0.5033930540084839
Epoch 2780, training loss: 415.81060791015625 = 0.20865292847156525 + 50.0 * 8.312039375305176
Epoch 2780, val loss: 0.5042367577552795
Epoch 2790, training loss: 415.8045349121094 = 0.20733505487442017 + 50.0 * 8.311944007873535
Epoch 2790, val loss: 0.5049132704734802
Epoch 2800, training loss: 415.8721923828125 = 0.20604006946086884 + 50.0 * 8.313323020935059
Epoch 2800, val loss: 0.5058561563491821
Epoch 2810, training loss: 415.7247009277344 = 0.20469917356967926 + 50.0 * 8.310400009155273
Epoch 2810, val loss: 0.506560742855072
Epoch 2820, training loss: 415.75335693359375 = 0.2033872753381729 + 50.0 * 8.310998916625977
Epoch 2820, val loss: 0.5072767734527588
Epoch 2830, training loss: 415.830078125 = 0.2020917683839798 + 50.0 * 8.312560081481934
Epoch 2830, val loss: 0.5081665515899658
Epoch 2840, training loss: 415.8494873046875 = 0.20077118277549744 + 50.0 * 8.312973976135254
Epoch 2840, val loss: 0.5087019205093384
Epoch 2850, training loss: 415.71832275390625 = 0.1994447261095047 + 50.0 * 8.310378074645996
Epoch 2850, val loss: 0.5091511011123657
Epoch 2860, training loss: 415.68310546875 = 0.1981353610754013 + 50.0 * 8.309700012207031
Epoch 2860, val loss: 0.510296642780304
Epoch 2870, training loss: 415.7164611816406 = 0.1968407779932022 + 50.0 * 8.310392379760742
Epoch 2870, val loss: 0.5109478831291199
Epoch 2880, training loss: 415.8658142089844 = 0.1955629140138626 + 50.0 * 8.31340503692627
Epoch 2880, val loss: 0.5114695429801941
Epoch 2890, training loss: 415.719482421875 = 0.1942497342824936 + 50.0 * 8.310504913330078
Epoch 2890, val loss: 0.5126051902770996
Epoch 2900, training loss: 415.6922302246094 = 0.19295266270637512 + 50.0 * 8.309985160827637
Epoch 2900, val loss: 0.5136620998382568
Epoch 2910, training loss: 415.7047119140625 = 0.1916663497686386 + 50.0 * 8.310260772705078
Epoch 2910, val loss: 0.5146581530570984
Epoch 2920, training loss: 415.7239074707031 = 0.1903843879699707 + 50.0 * 8.310669898986816
Epoch 2920, val loss: 0.5154228210449219
Epoch 2930, training loss: 415.65521240234375 = 0.18909206986427307 + 50.0 * 8.309322357177734
Epoch 2930, val loss: 0.5164551138877869
Epoch 2940, training loss: 415.66180419921875 = 0.18781819939613342 + 50.0 * 8.309479713439941
Epoch 2940, val loss: 0.5174757838249207
Epoch 2950, training loss: 415.7474670410156 = 0.18655622005462646 + 50.0 * 8.31121826171875
Epoch 2950, val loss: 0.5184978246688843
Epoch 2960, training loss: 415.7218017578125 = 0.18529203534126282 + 50.0 * 8.31072998046875
Epoch 2960, val loss: 0.5195125341415405
Epoch 2970, training loss: 415.6786804199219 = 0.18400408327579498 + 50.0 * 8.309893608093262
Epoch 2970, val loss: 0.5200309157371521
Epoch 2980, training loss: 415.6234436035156 = 0.18273769319057465 + 50.0 * 8.30881404876709
Epoch 2980, val loss: 0.5212765336036682
Epoch 2990, training loss: 415.6380310058594 = 0.18147650361061096 + 50.0 * 8.309130668640137
Epoch 2990, val loss: 0.5221107602119446
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8259766615930999
0.8420633195682099
=== training gcn model ===
Epoch 0, training loss: 530.213623046875 = 1.10225248336792 + 50.0 * 10.58222770690918
Epoch 0, val loss: 1.1017775535583496
Epoch 10, training loss: 530.1466674804688 = 1.095287799835205 + 50.0 * 10.58102798461914
Epoch 10, val loss: 1.0947556495666504
Epoch 20, training loss: 529.6707763671875 = 1.0870239734649658 + 50.0 * 10.571675300598145
Epoch 20, val loss: 1.0864202976226807
Epoch 30, training loss: 526.413330078125 = 1.0772792100906372 + 50.0 * 10.506721496582031
Epoch 30, val loss: 1.0766102075576782
Epoch 40, training loss: 510.727783203125 = 1.0668585300445557 + 50.0 * 10.193218231201172
Epoch 40, val loss: 1.0663541555404663
Epoch 50, training loss: 478.4680480957031 = 1.055898904800415 + 50.0 * 9.548242568969727
Epoch 50, val loss: 1.0555598735809326
Epoch 60, training loss: 468.8603515625 = 1.047167181968689 + 50.0 * 9.356263160705566
Epoch 60, val loss: 1.0473946332931519
Epoch 70, training loss: 462.3057556152344 = 1.0398497581481934 + 50.0 * 9.22531795501709
Epoch 70, val loss: 1.0403960943222046
Epoch 80, training loss: 461.4403991699219 = 1.032814621925354 + 50.0 * 9.208151817321777
Epoch 80, val loss: 1.0337101221084595
Epoch 90, training loss: 459.4653625488281 = 1.026484727859497 + 50.0 * 9.168777465820312
Epoch 90, val loss: 1.0277823209762573
Epoch 100, training loss: 456.70672607421875 = 1.0210338830947876 + 50.0 * 9.113714218139648
Epoch 100, val loss: 1.0226751565933228
Epoch 110, training loss: 451.75421142578125 = 1.0163806676864624 + 50.0 * 9.014756202697754
Epoch 110, val loss: 1.018362283706665
Epoch 120, training loss: 447.5306701660156 = 1.012831687927246 + 50.0 * 8.930356979370117
Epoch 120, val loss: 1.015009880065918
Epoch 130, training loss: 444.6683349609375 = 1.0081067085266113 + 50.0 * 8.873204231262207
Epoch 130, val loss: 1.0103267431259155
Epoch 140, training loss: 440.9482421875 = 1.0030810832977295 + 50.0 * 8.798903465270996
Epoch 140, val loss: 1.0056936740875244
Epoch 150, training loss: 439.03753662109375 = 0.998744785785675 + 50.0 * 8.760775566101074
Epoch 150, val loss: 1.001512885093689
Epoch 160, training loss: 437.30828857421875 = 0.9925417900085449 + 50.0 * 8.726314544677734
Epoch 160, val loss: 0.9955068230628967
Epoch 170, training loss: 435.9019775390625 = 0.9851373434066772 + 50.0 * 8.698336601257324
Epoch 170, val loss: 0.9883940815925598
Epoch 180, training loss: 434.5079345703125 = 0.9772636294364929 + 50.0 * 8.670613288879395
Epoch 180, val loss: 0.9808549880981445
Epoch 190, training loss: 433.4152526855469 = 0.9688660502433777 + 50.0 * 8.648927688598633
Epoch 190, val loss: 0.9728208780288696
Epoch 200, training loss: 432.4403076171875 = 0.9593896269798279 + 50.0 * 8.629618644714355
Epoch 200, val loss: 0.963621973991394
Epoch 210, training loss: 431.5262451171875 = 0.9488736391067505 + 50.0 * 8.611547470092773
Epoch 210, val loss: 0.9534905552864075
Epoch 220, training loss: 430.6627197265625 = 0.937881588935852 + 50.0 * 8.594496726989746
Epoch 220, val loss: 0.9429029226303101
Epoch 230, training loss: 429.8684997558594 = 0.9265425801277161 + 50.0 * 8.578839302062988
Epoch 230, val loss: 0.9319931864738464
Epoch 240, training loss: 429.30047607421875 = 0.9144527912139893 + 50.0 * 8.567720413208008
Epoch 240, val loss: 0.9202830195426941
Epoch 250, training loss: 428.669921875 = 0.9012944102287292 + 50.0 * 8.55537223815918
Epoch 250, val loss: 0.9075185656547546
Epoch 260, training loss: 428.0436096191406 = 0.8872873783111572 + 50.0 * 8.543126106262207
Epoch 260, val loss: 0.8939443826675415
Epoch 270, training loss: 427.7347106933594 = 0.8726381659507751 + 50.0 * 8.537240982055664
Epoch 270, val loss: 0.8798055648803711
Epoch 280, training loss: 427.1267395019531 = 0.8573684692382812 + 50.0 * 8.52538776397705
Epoch 280, val loss: 0.865092396736145
Epoch 290, training loss: 426.7477722167969 = 0.8418123126029968 + 50.0 * 8.518118858337402
Epoch 290, val loss: 0.8500661253929138
Epoch 300, training loss: 426.50726318359375 = 0.8259608745574951 + 50.0 * 8.513626098632812
Epoch 300, val loss: 0.8347237706184387
Epoch 310, training loss: 426.143310546875 = 0.8098482489585876 + 50.0 * 8.506669044494629
Epoch 310, val loss: 0.8191571235656738
Epoch 320, training loss: 425.8636169433594 = 0.7938649654388428 + 50.0 * 8.501395225524902
Epoch 320, val loss: 0.8037014603614807
Epoch 330, training loss: 425.6920471191406 = 0.7780861854553223 + 50.0 * 8.498279571533203
Epoch 330, val loss: 0.7884218692779541
Epoch 340, training loss: 425.4183654785156 = 0.7623487114906311 + 50.0 * 8.493120193481445
Epoch 340, val loss: 0.7732170224189758
Epoch 350, training loss: 425.1686706542969 = 0.7471662759780884 + 50.0 * 8.48843002319336
Epoch 350, val loss: 0.7586203217506409
Epoch 360, training loss: 424.8395080566406 = 0.7326520681381226 + 50.0 * 8.482136726379395
Epoch 360, val loss: 0.7447064518928528
Epoch 370, training loss: 424.5799255371094 = 0.7187069058418274 + 50.0 * 8.477224349975586
Epoch 370, val loss: 0.7313520312309265
Epoch 380, training loss: 424.5113830566406 = 0.7052204608917236 + 50.0 * 8.476122856140137
Epoch 380, val loss: 0.7183494567871094
Epoch 390, training loss: 424.2004699707031 = 0.6922078132629395 + 50.0 * 8.470165252685547
Epoch 390, val loss: 0.7059750556945801
Epoch 400, training loss: 423.86065673828125 = 0.6800976395606995 + 50.0 * 8.463611602783203
Epoch 400, val loss: 0.6945947408676147
Epoch 410, training loss: 423.61138916015625 = 0.6687601208686829 + 50.0 * 8.458852767944336
Epoch 410, val loss: 0.683897852897644
Epoch 420, training loss: 423.7938232421875 = 0.6580275297164917 + 50.0 * 8.462716102600098
Epoch 420, val loss: 0.6737864017486572
Epoch 430, training loss: 423.29376220703125 = 0.6477725505828857 + 50.0 * 8.452919960021973
Epoch 430, val loss: 0.6642352342605591
Epoch 440, training loss: 422.99688720703125 = 0.6383447051048279 + 50.0 * 8.447171211242676
Epoch 440, val loss: 0.6555853486061096
Epoch 450, training loss: 422.7603759765625 = 0.629613995552063 + 50.0 * 8.442615509033203
Epoch 450, val loss: 0.6476279497146606
Epoch 460, training loss: 422.9283142089844 = 0.6214349269866943 + 50.0 * 8.446137428283691
Epoch 460, val loss: 0.6402149796485901
Epoch 470, training loss: 422.4465637207031 = 0.6135908365249634 + 50.0 * 8.436659812927246
Epoch 470, val loss: 0.6332261562347412
Epoch 480, training loss: 422.2934875488281 = 0.6064209342002869 + 50.0 * 8.433741569519043
Epoch 480, val loss: 0.6268323063850403
Epoch 490, training loss: 422.1085510253906 = 0.599858283996582 + 50.0 * 8.430173873901367
Epoch 490, val loss: 0.6211056709289551
Epoch 500, training loss: 422.2057189941406 = 0.593660831451416 + 50.0 * 8.432241439819336
Epoch 500, val loss: 0.6157742142677307
Epoch 510, training loss: 421.90216064453125 = 0.5877264738082886 + 50.0 * 8.426288604736328
Epoch 510, val loss: 0.610628068447113
Epoch 520, training loss: 421.7002258300781 = 0.582330584526062 + 50.0 * 8.422357559204102
Epoch 520, val loss: 0.60613614320755
Epoch 530, training loss: 421.590576171875 = 0.5773605108261108 + 50.0 * 8.42026424407959
Epoch 530, val loss: 0.6019642353057861
Epoch 540, training loss: 421.5080871582031 = 0.5725880265235901 + 50.0 * 8.418709754943848
Epoch 540, val loss: 0.598063051700592
Epoch 550, training loss: 421.347900390625 = 0.5681378841400146 + 50.0 * 8.415595054626465
Epoch 550, val loss: 0.5944235920906067
Epoch 560, training loss: 421.21051025390625 = 0.5640780925750732 + 50.0 * 8.412928581237793
Epoch 560, val loss: 0.5912553071975708
Epoch 570, training loss: 421.1077575683594 = 0.5602673292160034 + 50.0 * 8.41094970703125
Epoch 570, val loss: 0.5881462693214417
Epoch 580, training loss: 421.0715637207031 = 0.556653618812561 + 50.0 * 8.410298347473145
Epoch 580, val loss: 0.5853143334388733
Epoch 590, training loss: 420.9834899902344 = 0.5531507730484009 + 50.0 * 8.40860652923584
Epoch 590, val loss: 0.5826729536056519
Epoch 600, training loss: 420.8660888671875 = 0.5499225854873657 + 50.0 * 8.406323432922363
Epoch 600, val loss: 0.5801279544830322
Epoch 610, training loss: 420.625244140625 = 0.5469133257865906 + 50.0 * 8.401566505432129
Epoch 610, val loss: 0.5780076384544373
Epoch 620, training loss: 420.5954895019531 = 0.5441010594367981 + 50.0 * 8.40102767944336
Epoch 620, val loss: 0.5759373307228088
Epoch 630, training loss: 420.4866638183594 = 0.5413721203804016 + 50.0 * 8.398905754089355
Epoch 630, val loss: 0.5739178657531738
Epoch 640, training loss: 420.5010986328125 = 0.53878253698349 + 50.0 * 8.399246215820312
Epoch 640, val loss: 0.5720199346542358
Epoch 650, training loss: 420.3437805175781 = 0.5362586379051208 + 50.0 * 8.396150588989258
Epoch 650, val loss: 0.5702750086784363
Epoch 660, training loss: 420.1929626464844 = 0.5338979959487915 + 50.0 * 8.393180847167969
Epoch 660, val loss: 0.5686307549476624
Epoch 670, training loss: 420.0715026855469 = 0.5316962599754333 + 50.0 * 8.390795707702637
Epoch 670, val loss: 0.5671603679656982
Epoch 680, training loss: 420.2635192871094 = 0.5295790433883667 + 50.0 * 8.394679069519043
Epoch 680, val loss: 0.5659147500991821
Epoch 690, training loss: 419.9712829589844 = 0.5273338556289673 + 50.0 * 8.38887882232666
Epoch 690, val loss: 0.564098060131073
Epoch 700, training loss: 419.8515319824219 = 0.5252407789230347 + 50.0 * 8.386526107788086
Epoch 700, val loss: 0.5627886652946472
Epoch 710, training loss: 419.7491455078125 = 0.5233493447303772 + 50.0 * 8.384515762329102
Epoch 710, val loss: 0.5615593791007996
Epoch 720, training loss: 419.678466796875 = 0.521538257598877 + 50.0 * 8.383138656616211
Epoch 720, val loss: 0.5603533983230591
Epoch 730, training loss: 419.6472473144531 = 0.5197587609291077 + 50.0 * 8.382550239562988
Epoch 730, val loss: 0.5592200756072998
Epoch 740, training loss: 419.6379699707031 = 0.5179383754730225 + 50.0 * 8.382400512695312
Epoch 740, val loss: 0.5579848885536194
Epoch 750, training loss: 419.60845947265625 = 0.5161446928977966 + 50.0 * 8.38184642791748
Epoch 750, val loss: 0.5569427013397217
Epoch 760, training loss: 419.4249572753906 = 0.5144907832145691 + 50.0 * 8.378209114074707
Epoch 760, val loss: 0.5558419823646545
Epoch 770, training loss: 419.3779602050781 = 0.5129071474075317 + 50.0 * 8.377301216125488
Epoch 770, val loss: 0.554876983165741
Epoch 780, training loss: 419.7402038574219 = 0.5113219618797302 + 50.0 * 8.384577751159668
Epoch 780, val loss: 0.5539212226867676
Epoch 790, training loss: 419.32122802734375 = 0.5096701383590698 + 50.0 * 8.37623119354248
Epoch 790, val loss: 0.5528774857521057
Epoch 800, training loss: 419.2753601074219 = 0.5081393122673035 + 50.0 * 8.375344276428223
Epoch 800, val loss: 0.5519697666168213
Epoch 810, training loss: 419.18206787109375 = 0.5066822171211243 + 50.0 * 8.373507499694824
Epoch 810, val loss: 0.5510865449905396
Epoch 820, training loss: 419.4009704589844 = 0.5052378177642822 + 50.0 * 8.377914428710938
Epoch 820, val loss: 0.5502802133560181
Epoch 830, training loss: 419.122802734375 = 0.5037537813186646 + 50.0 * 8.372381210327148
Epoch 830, val loss: 0.5492760539054871
Epoch 840, training loss: 419.0128173828125 = 0.5023720860481262 + 50.0 * 8.370208740234375
Epoch 840, val loss: 0.5484976172447205
Epoch 850, training loss: 418.9959716796875 = 0.5010232329368591 + 50.0 * 8.369898796081543
Epoch 850, val loss: 0.5476389527320862
Epoch 860, training loss: 419.1202392578125 = 0.49966105818748474 + 50.0 * 8.372411727905273
Epoch 860, val loss: 0.5468425750732422
Epoch 870, training loss: 418.928955078125 = 0.4983006417751312 + 50.0 * 8.368613243103027
Epoch 870, val loss: 0.5461658835411072
Epoch 880, training loss: 418.8414611816406 = 0.49698808789253235 + 50.0 * 8.366889953613281
Epoch 880, val loss: 0.5453396439552307
Epoch 890, training loss: 418.86114501953125 = 0.4957268238067627 + 50.0 * 8.367308616638184
Epoch 890, val loss: 0.5446668267250061
Epoch 900, training loss: 418.883544921875 = 0.4944285750389099 + 50.0 * 8.367782592773438
Epoch 900, val loss: 0.5439512729644775
Epoch 910, training loss: 418.7849426269531 = 0.4931279122829437 + 50.0 * 8.365836143493652
Epoch 910, val loss: 0.543188214302063
Epoch 920, training loss: 418.7695007324219 = 0.4919019341468811 + 50.0 * 8.365551948547363
Epoch 920, val loss: 0.542533278465271
Epoch 930, training loss: 418.6636962890625 = 0.4906562268733978 + 50.0 * 8.363460540771484
Epoch 930, val loss: 0.5417571067810059
Epoch 940, training loss: 418.6389465332031 = 0.4894484281539917 + 50.0 * 8.362990379333496
Epoch 940, val loss: 0.5410628914833069
Epoch 950, training loss: 418.94952392578125 = 0.48823562264442444 + 50.0 * 8.36922550201416
Epoch 950, val loss: 0.5402719974517822
Epoch 960, training loss: 418.65765380859375 = 0.4869786202907562 + 50.0 * 8.36341381072998
Epoch 960, val loss: 0.5397796034812927
Epoch 970, training loss: 418.50811767578125 = 0.4857739806175232 + 50.0 * 8.36044692993164
Epoch 970, val loss: 0.539058268070221
Epoch 980, training loss: 418.60479736328125 = 0.48461052775382996 + 50.0 * 8.362403869628906
Epoch 980, val loss: 0.5383480191230774
Epoch 990, training loss: 418.46173095703125 = 0.48340246081352234 + 50.0 * 8.359566688537598
Epoch 990, val loss: 0.5377002358436584
Epoch 1000, training loss: 418.4194030761719 = 0.4822119176387787 + 50.0 * 8.358743667602539
Epoch 1000, val loss: 0.5370398163795471
Epoch 1010, training loss: 418.4229431152344 = 0.4810761511325836 + 50.0 * 8.358837127685547
Epoch 1010, val loss: 0.5364019274711609
Epoch 1020, training loss: 418.5439758300781 = 0.47992557287216187 + 50.0 * 8.361281394958496
Epoch 1020, val loss: 0.535727322101593
Epoch 1030, training loss: 418.5310974121094 = 0.47875308990478516 + 50.0 * 8.36104679107666
Epoch 1030, val loss: 0.5350959300994873
Epoch 1040, training loss: 418.338623046875 = 0.47759193181991577 + 50.0 * 8.357220649719238
Epoch 1040, val loss: 0.5345015525817871
Epoch 1050, training loss: 418.3184814453125 = 0.4764762818813324 + 50.0 * 8.356840133666992
Epoch 1050, val loss: 0.5338711142539978
Epoch 1060, training loss: 418.2723693847656 = 0.4753788709640503 + 50.0 * 8.355939865112305
Epoch 1060, val loss: 0.5332935452461243
Epoch 1070, training loss: 418.38812255859375 = 0.4742989242076874 + 50.0 * 8.3582763671875
Epoch 1070, val loss: 0.5328547358512878
Epoch 1080, training loss: 418.2986145019531 = 0.473132461309433 + 50.0 * 8.3565092086792
Epoch 1080, val loss: 0.5319225192070007
Epoch 1090, training loss: 418.3583068847656 = 0.47202178835868835 + 50.0 * 8.357726097106934
Epoch 1090, val loss: 0.5315079689025879
Epoch 1100, training loss: 418.19683837890625 = 0.4708825647830963 + 50.0 * 8.35451889038086
Epoch 1100, val loss: 0.5307166576385498
Epoch 1110, training loss: 418.1619567871094 = 0.469804584980011 + 50.0 * 8.353842735290527
Epoch 1110, val loss: 0.5301387906074524
Epoch 1120, training loss: 418.27880859375 = 0.46874114871025085 + 50.0 * 8.356201171875
Epoch 1120, val loss: 0.5295024514198303
Epoch 1130, training loss: 418.13653564453125 = 0.4676223695278168 + 50.0 * 8.353378295898438
Epoch 1130, val loss: 0.5289601683616638
Epoch 1140, training loss: 418.10211181640625 = 0.4665343463420868 + 50.0 * 8.35271167755127
Epoch 1140, val loss: 0.5283214449882507
Epoch 1150, training loss: 418.06005859375 = 0.46548596024513245 + 50.0 * 8.35189151763916
Epoch 1150, val loss: 0.5277636051177979
Epoch 1160, training loss: 418.48175048828125 = 0.4644383490085602 + 50.0 * 8.360345840454102
Epoch 1160, val loss: 0.5271071195602417
Epoch 1170, training loss: 418.25732421875 = 0.4632760286331177 + 50.0 * 8.355880737304688
Epoch 1170, val loss: 0.5265366435050964
Epoch 1180, training loss: 417.9922790527344 = 0.4621701240539551 + 50.0 * 8.350602149963379
Epoch 1180, val loss: 0.5258941054344177
Epoch 1190, training loss: 417.95977783203125 = 0.4611464738845825 + 50.0 * 8.34997272491455
Epoch 1190, val loss: 0.5252464413642883
Epoch 1200, training loss: 417.93365478515625 = 0.46014687418937683 + 50.0 * 8.349470138549805
Epoch 1200, val loss: 0.5247633457183838
Epoch 1210, training loss: 418.1037292480469 = 0.4591248035430908 + 50.0 * 8.35289192199707
Epoch 1210, val loss: 0.5240959525108337
Epoch 1220, training loss: 418.00994873046875 = 0.45803651213645935 + 50.0 * 8.351037979125977
Epoch 1220, val loss: 0.5235325694084167
Epoch 1230, training loss: 417.9767761230469 = 0.4569559395313263 + 50.0 * 8.350396156311035
Epoch 1230, val loss: 0.5229035019874573
Epoch 1240, training loss: 417.864501953125 = 0.4559098482131958 + 50.0 * 8.348172187805176
Epoch 1240, val loss: 0.522372305393219
Epoch 1250, training loss: 417.82464599609375 = 0.4548919200897217 + 50.0 * 8.347394943237305
Epoch 1250, val loss: 0.5217769145965576
Epoch 1260, training loss: 417.8961181640625 = 0.45388180017471313 + 50.0 * 8.348844528198242
Epoch 1260, val loss: 0.5212324857711792
Epoch 1270, training loss: 417.8453674316406 = 0.45282313227653503 + 50.0 * 8.347850799560547
Epoch 1270, val loss: 0.5205981731414795
Epoch 1280, training loss: 417.7798156738281 = 0.4517602324485779 + 50.0 * 8.346561431884766
Epoch 1280, val loss: 0.5199153423309326
Epoch 1290, training loss: 417.80267333984375 = 0.45072582364082336 + 50.0 * 8.347039222717285
Epoch 1290, val loss: 0.5193060040473938
Epoch 1300, training loss: 417.823486328125 = 0.4496738016605377 + 50.0 * 8.3474760055542
Epoch 1300, val loss: 0.5186521410942078
Epoch 1310, training loss: 417.7224426269531 = 0.4486331641674042 + 50.0 * 8.345476150512695
Epoch 1310, val loss: 0.5181771516799927
Epoch 1320, training loss: 417.69903564453125 = 0.44760942459106445 + 50.0 * 8.3450288772583
Epoch 1320, val loss: 0.5175073146820068
Epoch 1330, training loss: 417.947509765625 = 0.44658544659614563 + 50.0 * 8.350018501281738
Epoch 1330, val loss: 0.5169334411621094
Epoch 1340, training loss: 417.6869812011719 = 0.44550395011901855 + 50.0 * 8.344829559326172
Epoch 1340, val loss: 0.516225278377533
Epoch 1350, training loss: 417.6197814941406 = 0.4444821774959564 + 50.0 * 8.343505859375
Epoch 1350, val loss: 0.5157175064086914
Epoch 1360, training loss: 417.6210021972656 = 0.44348177313804626 + 50.0 * 8.343550682067871
Epoch 1360, val loss: 0.5151041746139526
Epoch 1370, training loss: 417.9635314941406 = 0.4424608051776886 + 50.0 * 8.350420951843262
Epoch 1370, val loss: 0.5145829916000366
Epoch 1380, training loss: 417.6226806640625 = 0.44133850932121277 + 50.0 * 8.343626976013184
Epoch 1380, val loss: 0.5136852264404297
Epoch 1390, training loss: 417.5314025878906 = 0.44030681252479553 + 50.0 * 8.341821670532227
Epoch 1390, val loss: 0.5131015181541443
Epoch 1400, training loss: 417.5369567871094 = 0.4393089711666107 + 50.0 * 8.34195327758789
Epoch 1400, val loss: 0.5125726461410522
Epoch 1410, training loss: 417.7515869140625 = 0.4382951557636261 + 50.0 * 8.34626579284668
Epoch 1410, val loss: 0.5118220448493958
Epoch 1420, training loss: 417.52435302734375 = 0.4372263550758362 + 50.0 * 8.341742515563965
Epoch 1420, val loss: 0.5113439559936523
Epoch 1430, training loss: 417.4637145996094 = 0.43617692589759827 + 50.0 * 8.340550422668457
Epoch 1430, val loss: 0.5106074810028076
Epoch 1440, training loss: 417.4437561035156 = 0.43516334891319275 + 50.0 * 8.340171813964844
Epoch 1440, val loss: 0.5100387334823608
Epoch 1450, training loss: 417.55194091796875 = 0.43415117263793945 + 50.0 * 8.342355728149414
Epoch 1450, val loss: 0.5094430446624756
Epoch 1460, training loss: 417.47943115234375 = 0.43307632207870483 + 50.0 * 8.340927124023438
Epoch 1460, val loss: 0.5086391568183899
Epoch 1470, training loss: 417.60162353515625 = 0.432010680437088 + 50.0 * 8.343392372131348
Epoch 1470, val loss: 0.5080966353416443
Epoch 1480, training loss: 417.5118408203125 = 0.43091118335723877 + 50.0 * 8.341618537902832
Epoch 1480, val loss: 0.5072801113128662
Epoch 1490, training loss: 417.3817138671875 = 0.4298562705516815 + 50.0 * 8.33903694152832
Epoch 1490, val loss: 0.5066649913787842
Epoch 1500, training loss: 417.3585510253906 = 0.4288298189640045 + 50.0 * 8.338594436645508
Epoch 1500, val loss: 0.5059994459152222
Epoch 1510, training loss: 417.3760986328125 = 0.42780372500419617 + 50.0 * 8.338966369628906
Epoch 1510, val loss: 0.5053501129150391
Epoch 1520, training loss: 417.57269287109375 = 0.42674168944358826 + 50.0 * 8.34291934967041
Epoch 1520, val loss: 0.5046815276145935
Epoch 1530, training loss: 417.3504333496094 = 0.4256352484226227 + 50.0 * 8.338496208190918
Epoch 1530, val loss: 0.5040012001991272
Epoch 1540, training loss: 417.2964782714844 = 0.42457160353660583 + 50.0 * 8.337438583374023
Epoch 1540, val loss: 0.5033384561538696
Epoch 1550, training loss: 417.52191162109375 = 0.42351841926574707 + 50.0 * 8.341967582702637
Epoch 1550, val loss: 0.5027039051055908
Epoch 1560, training loss: 417.3605651855469 = 0.42238786816596985 + 50.0 * 8.338763236999512
Epoch 1560, val loss: 0.5018947720527649
Epoch 1570, training loss: 417.3469543457031 = 0.42129555344581604 + 50.0 * 8.338513374328613
Epoch 1570, val loss: 0.5011768341064453
Epoch 1580, training loss: 417.28997802734375 = 0.4202115833759308 + 50.0 * 8.337395668029785
Epoch 1580, val loss: 0.5004867911338806
Epoch 1590, training loss: 417.2406005859375 = 0.4191437363624573 + 50.0 * 8.33642864227295
Epoch 1590, val loss: 0.49989211559295654
Epoch 1600, training loss: 417.33270263671875 = 0.41806694865226746 + 50.0 * 8.338293075561523
Epoch 1600, val loss: 0.49920836091041565
Epoch 1610, training loss: 417.21087646484375 = 0.4169571101665497 + 50.0 * 8.335878372192383
Epoch 1610, val loss: 0.49854332208633423
Epoch 1620, training loss: 417.2720031738281 = 0.41584888100624084 + 50.0 * 8.337122917175293
Epoch 1620, val loss: 0.49777621030807495
Epoch 1630, training loss: 417.3850402832031 = 0.41472315788269043 + 50.0 * 8.33940601348877
Epoch 1630, val loss: 0.4970252811908722
Epoch 1640, training loss: 417.2568359375 = 0.4135836660861969 + 50.0 * 8.336865425109863
Epoch 1640, val loss: 0.49635574221611023
Epoch 1650, training loss: 417.1693115234375 = 0.41246896982192993 + 50.0 * 8.335136413574219
Epoch 1650, val loss: 0.4956764876842499
Epoch 1660, training loss: 417.15936279296875 = 0.4113677442073822 + 50.0 * 8.334959983825684
Epoch 1660, val loss: 0.4950704276561737
Epoch 1670, training loss: 417.2772521972656 = 0.41025370359420776 + 50.0 * 8.337340354919434
Epoch 1670, val loss: 0.49436381459236145
Epoch 1680, training loss: 417.151611328125 = 0.40910303592681885 + 50.0 * 8.334850311279297
Epoch 1680, val loss: 0.4937531650066376
Epoch 1690, training loss: 417.15704345703125 = 0.4079574942588806 + 50.0 * 8.334981918334961
Epoch 1690, val loss: 0.4930291473865509
Epoch 1700, training loss: 417.11346435546875 = 0.40681418776512146 + 50.0 * 8.33413314819336
Epoch 1700, val loss: 0.49230748414993286
Epoch 1710, training loss: 417.1159362792969 = 0.4056817889213562 + 50.0 * 8.334205627441406
Epoch 1710, val loss: 0.4916505217552185
Epoch 1720, training loss: 417.0938415527344 = 0.4045282006263733 + 50.0 * 8.333786010742188
Epoch 1720, val loss: 0.49097388982772827
Epoch 1730, training loss: 417.0908203125 = 0.40336668491363525 + 50.0 * 8.333748817443848
Epoch 1730, val loss: 0.4902855455875397
Epoch 1740, training loss: 417.20745849609375 = 0.4021967649459839 + 50.0 * 8.336105346679688
Epoch 1740, val loss: 0.4896184802055359
Epoch 1750, training loss: 417.0740051269531 = 0.40095943212509155 + 50.0 * 8.333460807800293
Epoch 1750, val loss: 0.4887637495994568
Epoch 1760, training loss: 417.0284423828125 = 0.399752140045166 + 50.0 * 8.332573890686035
Epoch 1760, val loss: 0.488178014755249
Epoch 1770, training loss: 416.98980712890625 = 0.39857950806617737 + 50.0 * 8.33182430267334
Epoch 1770, val loss: 0.4874093532562256
Epoch 1780, training loss: 416.96295166015625 = 0.39742183685302734 + 50.0 * 8.331310272216797
Epoch 1780, val loss: 0.4868081212043762
Epoch 1790, training loss: 417.0011291503906 = 0.39626529812812805 + 50.0 * 8.332097053527832
Epoch 1790, val loss: 0.4862292408943176
Epoch 1800, training loss: 417.146240234375 = 0.3950522840023041 + 50.0 * 8.335023880004883
Epoch 1800, val loss: 0.485512375831604
Epoch 1810, training loss: 416.9734802246094 = 0.39378949999809265 + 50.0 * 8.33159351348877
Epoch 1810, val loss: 0.4847937524318695
Epoch 1820, training loss: 416.94970703125 = 0.3925771713256836 + 50.0 * 8.33114242553711
Epoch 1820, val loss: 0.484111487865448
Epoch 1830, training loss: 417.1288146972656 = 0.3913637399673462 + 50.0 * 8.334749221801758
Epoch 1830, val loss: 0.4834761917591095
Epoch 1840, training loss: 417.10626220703125 = 0.3901013433933258 + 50.0 * 8.334322929382324
Epoch 1840, val loss: 0.48269209265708923
Epoch 1850, training loss: 416.9056396484375 = 0.38886675238609314 + 50.0 * 8.33033561706543
Epoch 1850, val loss: 0.48204854130744934
Epoch 1860, training loss: 416.8806457519531 = 0.38765567541122437 + 50.0 * 8.329859733581543
Epoch 1860, val loss: 0.48137354850769043
Epoch 1870, training loss: 416.8594665527344 = 0.3864510953426361 + 50.0 * 8.329460144042969
Epoch 1870, val loss: 0.48072004318237305
Epoch 1880, training loss: 416.975830078125 = 0.38523444533348083 + 50.0 * 8.331811904907227
Epoch 1880, val loss: 0.48006775975227356
Epoch 1890, training loss: 416.8705749511719 = 0.3839547634124756 + 50.0 * 8.329732894897461
Epoch 1890, val loss: 0.4793584942817688
Epoch 1900, training loss: 416.8983154296875 = 0.382680743932724 + 50.0 * 8.330312728881836
Epoch 1900, val loss: 0.47878411412239075
Epoch 1910, training loss: 416.90728759765625 = 0.3814131021499634 + 50.0 * 8.330517768859863
Epoch 1910, val loss: 0.4780732989311218
Epoch 1920, training loss: 416.8857116699219 = 0.38013756275177 + 50.0 * 8.330111503601074
Epoch 1920, val loss: 0.4774787127971649
Epoch 1930, training loss: 416.8014221191406 = 0.3788663446903229 + 50.0 * 8.328451156616211
Epoch 1930, val loss: 0.47692787647247314
Epoch 1940, training loss: 416.8324890136719 = 0.3775947690010071 + 50.0 * 8.329097747802734
Epoch 1940, val loss: 0.4762372374534607
Epoch 1950, training loss: 416.8045349121094 = 0.3763130307197571 + 50.0 * 8.328564643859863
Epoch 1950, val loss: 0.4756389260292053
Epoch 1960, training loss: 416.7985534667969 = 0.37501978874206543 + 50.0 * 8.328470230102539
Epoch 1960, val loss: 0.47491276264190674
Epoch 1970, training loss: 416.76519775390625 = 0.3737279772758484 + 50.0 * 8.327829360961914
Epoch 1970, val loss: 0.47436580061912537
Epoch 1980, training loss: 417.0794372558594 = 0.37241628766059875 + 50.0 * 8.33414077758789
Epoch 1980, val loss: 0.4738038182258606
Epoch 1990, training loss: 416.80731201171875 = 0.3710542917251587 + 50.0 * 8.32872486114502
Epoch 1990, val loss: 0.47317904233932495
Epoch 2000, training loss: 416.7018127441406 = 0.3697241246700287 + 50.0 * 8.326642036437988
Epoch 2000, val loss: 0.47243013978004456
Epoch 2010, training loss: 416.6860046386719 = 0.36842361092567444 + 50.0 * 8.326351165771484
Epoch 2010, val loss: 0.4718979299068451
Epoch 2020, training loss: 416.82379150390625 = 0.36711931228637695 + 50.0 * 8.329133033752441
Epoch 2020, val loss: 0.4712730050086975
Epoch 2030, training loss: 416.7693176269531 = 0.3657732605934143 + 50.0 * 8.328070640563965
Epoch 2030, val loss: 0.47079411149024963
Epoch 2040, training loss: 416.6460266113281 = 0.36440712213516235 + 50.0 * 8.325632095336914
Epoch 2040, val loss: 0.4700973927974701
Epoch 2050, training loss: 416.63970947265625 = 0.3630799353122711 + 50.0 * 8.325532913208008
Epoch 2050, val loss: 0.46961355209350586
Epoch 2060, training loss: 416.6139831542969 = 0.3617567718029022 + 50.0 * 8.325044631958008
Epoch 2060, val loss: 0.46903079748153687
Epoch 2070, training loss: 416.61962890625 = 0.36043432354927063 + 50.0 * 8.325183868408203
Epoch 2070, val loss: 0.4685467779636383
Epoch 2080, training loss: 417.02264404296875 = 0.3590984046459198 + 50.0 * 8.333271026611328
Epoch 2080, val loss: 0.4679587483406067
Epoch 2090, training loss: 416.8730773925781 = 0.35766908526420593 + 50.0 * 8.330307960510254
Epoch 2090, val loss: 0.4673961102962494
Epoch 2100, training loss: 416.7088928222656 = 0.3562437891960144 + 50.0 * 8.32705307006836
Epoch 2100, val loss: 0.46675926446914673
Epoch 2110, training loss: 416.6108093261719 = 0.354859322309494 + 50.0 * 8.325119018554688
Epoch 2110, val loss: 0.4663531482219696
Epoch 2120, training loss: 416.5626525878906 = 0.3535003960132599 + 50.0 * 8.324182510375977
Epoch 2120, val loss: 0.46587827801704407
Epoch 2130, training loss: 416.5798034667969 = 0.3521510064601898 + 50.0 * 8.324553489685059
Epoch 2130, val loss: 0.4654526114463806
Epoch 2140, training loss: 416.71612548828125 = 0.3507864475250244 + 50.0 * 8.327306747436523
Epoch 2140, val loss: 0.46500295400619507
Epoch 2150, training loss: 416.5784606933594 = 0.3493722379207611 + 50.0 * 8.32458209991455
Epoch 2150, val loss: 0.4643648564815521
Epoch 2160, training loss: 416.6484069824219 = 0.3479750454425812 + 50.0 * 8.326008796691895
Epoch 2160, val loss: 0.46390435099601746
Epoch 2170, training loss: 416.5658264160156 = 0.3465721607208252 + 50.0 * 8.324385643005371
Epoch 2170, val loss: 0.4635068476200104
Epoch 2180, training loss: 416.56964111328125 = 0.34516769647598267 + 50.0 * 8.32448959350586
Epoch 2180, val loss: 0.46296364068984985
Epoch 2190, training loss: 416.5049743652344 = 0.34376877546310425 + 50.0 * 8.323224067687988
Epoch 2190, val loss: 0.46262210607528687
Epoch 2200, training loss: 416.5559997558594 = 0.34238365292549133 + 50.0 * 8.324272155761719
Epoch 2200, val loss: 0.46227383613586426
Epoch 2210, training loss: 416.5587158203125 = 0.34096211194992065 + 50.0 * 8.324355125427246
Epoch 2210, val loss: 0.4617556035518646
Epoch 2220, training loss: 416.5392761230469 = 0.3395357131958008 + 50.0 * 8.323994636535645
Epoch 2220, val loss: 0.4612825810909271
Epoch 2230, training loss: 416.7237243652344 = 0.3381325602531433 + 50.0 * 8.327712059020996
Epoch 2230, val loss: 0.46096667647361755
Epoch 2240, training loss: 416.4732971191406 = 0.3366881012916565 + 50.0 * 8.322731971740723
Epoch 2240, val loss: 0.46043047308921814
Epoch 2250, training loss: 416.43670654296875 = 0.33527839183807373 + 50.0 * 8.322029113769531
Epoch 2250, val loss: 0.46009308099746704
Epoch 2260, training loss: 416.4397888183594 = 0.3338794410228729 + 50.0 * 8.322117805480957
Epoch 2260, val loss: 0.45977917313575745
Epoch 2270, training loss: 416.5137939453125 = 0.3324716091156006 + 50.0 * 8.323626518249512
Epoch 2270, val loss: 0.45937228202819824
Epoch 2280, training loss: 416.5082092285156 = 0.33103522658348083 + 50.0 * 8.323543548583984
Epoch 2280, val loss: 0.45904630422592163
Epoch 2290, training loss: 416.4676513671875 = 0.3296080231666565 + 50.0 * 8.322760581970215
Epoch 2290, val loss: 0.45876941084861755
Epoch 2300, training loss: 416.457763671875 = 0.3281910717487335 + 50.0 * 8.322591781616211
Epoch 2300, val loss: 0.4584543704986572
Epoch 2310, training loss: 416.40625 = 0.32678550481796265 + 50.0 * 8.321589469909668
Epoch 2310, val loss: 0.4581911861896515
Epoch 2320, training loss: 416.399658203125 = 0.3253781497478485 + 50.0 * 8.32148551940918
Epoch 2320, val loss: 0.4578424096107483
Epoch 2330, training loss: 416.6098327636719 = 0.3239702880382538 + 50.0 * 8.325716972351074
Epoch 2330, val loss: 0.45767176151275635
Epoch 2340, training loss: 416.42559814453125 = 0.32251662015914917 + 50.0 * 8.322061538696289
Epoch 2340, val loss: 0.45730602741241455
Epoch 2350, training loss: 416.3679504394531 = 0.32109197974205017 + 50.0 * 8.320937156677246
Epoch 2350, val loss: 0.457143634557724
Epoch 2360, training loss: 416.3476867675781 = 0.3196788728237152 + 50.0 * 8.320560455322266
Epoch 2360, val loss: 0.45689627528190613
Epoch 2370, training loss: 416.46148681640625 = 0.3182770609855652 + 50.0 * 8.322864532470703
Epoch 2370, val loss: 0.45681482553482056
Epoch 2380, training loss: 416.3883361816406 = 0.3168492615222931 + 50.0 * 8.321429252624512
Epoch 2380, val loss: 0.4565601348876953
Epoch 2390, training loss: 416.433837890625 = 0.3154100477695465 + 50.0 * 8.322368621826172
Epoch 2390, val loss: 0.4561816453933716
Epoch 2400, training loss: 416.3116760253906 = 0.3139808177947998 + 50.0 * 8.319953918457031
Epoch 2400, val loss: 0.45613396167755127
Epoch 2410, training loss: 416.34796142578125 = 0.31255996227264404 + 50.0 * 8.320708274841309
Epoch 2410, val loss: 0.45592206716537476
Epoch 2420, training loss: 416.42315673828125 = 0.31113940477371216 + 50.0 * 8.322240829467773
Epoch 2420, val loss: 0.45575860142707825
Epoch 2430, training loss: 416.3221435546875 = 0.3097105622291565 + 50.0 * 8.3202486038208
Epoch 2430, val loss: 0.45576080679893494
Epoch 2440, training loss: 416.2895812988281 = 0.3082866966724396 + 50.0 * 8.319625854492188
Epoch 2440, val loss: 0.45553290843963623
Epoch 2450, training loss: 416.43548583984375 = 0.3068698048591614 + 50.0 * 8.322571754455566
Epoch 2450, val loss: 0.45530569553375244
Epoch 2460, training loss: 416.3038024902344 = 0.3054395020008087 + 50.0 * 8.319967269897461
Epoch 2460, val loss: 0.4554639756679535
Epoch 2470, training loss: 416.3214416503906 = 0.3040165901184082 + 50.0 * 8.320348739624023
Epoch 2470, val loss: 0.4551607370376587
Epoch 2480, training loss: 416.27960205078125 = 0.30260568857192993 + 50.0 * 8.319540023803711
Epoch 2480, val loss: 0.45516878366470337
Epoch 2490, training loss: 416.30859375 = 0.3012101352214813 + 50.0 * 8.320147514343262
Epoch 2490, val loss: 0.4552422761917114
Epoch 2500, training loss: 416.272705078125 = 0.29979977011680603 + 50.0 * 8.3194580078125
Epoch 2500, val loss: 0.4551241397857666
Epoch 2510, training loss: 416.2234191894531 = 0.2983901798725128 + 50.0 * 8.318500518798828
Epoch 2510, val loss: 0.4551396071910858
Epoch 2520, training loss: 416.27032470703125 = 0.29699182510375977 + 50.0 * 8.319466590881348
Epoch 2520, val loss: 0.4551827907562256
Epoch 2530, training loss: 416.4839172363281 = 0.2955927848815918 + 50.0 * 8.323766708374023
Epoch 2530, val loss: 0.45512324571609497
Epoch 2540, training loss: 416.2589416503906 = 0.2941521108150482 + 50.0 * 8.319295883178711
Epoch 2540, val loss: 0.45504289865493774
Epoch 2550, training loss: 416.2031555175781 = 0.2927420735359192 + 50.0 * 8.318207740783691
Epoch 2550, val loss: 0.45503300428390503
Epoch 2560, training loss: 416.2001647949219 = 0.2913503050804138 + 50.0 * 8.31817626953125
Epoch 2560, val loss: 0.4551764726638794
Epoch 2570, training loss: 416.2552185058594 = 0.28995630145072937 + 50.0 * 8.319305419921875
Epoch 2570, val loss: 0.45514294505119324
Epoch 2580, training loss: 416.193115234375 = 0.2885533571243286 + 50.0 * 8.31809139251709
Epoch 2580, val loss: 0.45529377460479736
Epoch 2590, training loss: 416.2730407714844 = 0.28716811537742615 + 50.0 * 8.319717407226562
Epoch 2590, val loss: 0.4553367793560028
Epoch 2600, training loss: 416.2043151855469 = 0.2857573330402374 + 50.0 * 8.318370819091797
Epoch 2600, val loss: 0.45536431670188904
Epoch 2610, training loss: 416.2032775878906 = 0.28436288237571716 + 50.0 * 8.318378448486328
Epoch 2610, val loss: 0.4554816782474518
Epoch 2620, training loss: 416.1357421875 = 0.2829694151878357 + 50.0 * 8.317055702209473
Epoch 2620, val loss: 0.4556371867656708
Epoch 2630, training loss: 416.109375 = 0.2815842032432556 + 50.0 * 8.316555976867676
Epoch 2630, val loss: 0.4558083713054657
Epoch 2640, training loss: 416.2459716796875 = 0.2802111804485321 + 50.0 * 8.319314956665039
Epoch 2640, val loss: 0.4559396803379059
Epoch 2650, training loss: 416.16217041015625 = 0.2788093090057373 + 50.0 * 8.317667007446289
Epoch 2650, val loss: 0.4560835063457489
Epoch 2660, training loss: 416.0864562988281 = 0.27740171551704407 + 50.0 * 8.316181182861328
Epoch 2660, val loss: 0.45624619722366333
Epoch 2670, training loss: 416.089599609375 = 0.2760116159915924 + 50.0 * 8.316271781921387
Epoch 2670, val loss: 0.4565798044204712
Epoch 2680, training loss: 416.2430114746094 = 0.2746424674987793 + 50.0 * 8.319367408752441
Epoch 2680, val loss: 0.4570055603981018
Epoch 2690, training loss: 416.1416931152344 = 0.27321913838386536 + 50.0 * 8.31736946105957
Epoch 2690, val loss: 0.45674824714660645
Epoch 2700, training loss: 416.169189453125 = 0.271824449300766 + 50.0 * 8.317947387695312
Epoch 2700, val loss: 0.4569813907146454
Epoch 2710, training loss: 416.0741271972656 = 0.27043768763542175 + 50.0 * 8.316073417663574
Epoch 2710, val loss: 0.45724573731422424
Epoch 2720, training loss: 416.07232666015625 = 0.2690555453300476 + 50.0 * 8.316065788269043
Epoch 2720, val loss: 0.4575498700141907
Epoch 2730, training loss: 416.1619567871094 = 0.2676853835582733 + 50.0 * 8.317885398864746
Epoch 2730, val loss: 0.45781266689300537
Epoch 2740, training loss: 416.05438232421875 = 0.2662842571735382 + 50.0 * 8.31576156616211
Epoch 2740, val loss: 0.45802998542785645
Epoch 2750, training loss: 416.130126953125 = 0.26490318775177 + 50.0 * 8.317304611206055
Epoch 2750, val loss: 0.4584318995475769
Epoch 2760, training loss: 416.04315185546875 = 0.26350927352905273 + 50.0 * 8.315592765808105
Epoch 2760, val loss: 0.4586828351020813
Epoch 2770, training loss: 416.0503234863281 = 0.26213356852531433 + 50.0 * 8.315763473510742
Epoch 2770, val loss: 0.4591168761253357
Epoch 2780, training loss: 416.1287536621094 = 0.2607628107070923 + 50.0 * 8.317359924316406
Epoch 2780, val loss: 0.4594096839427948
Epoch 2790, training loss: 416.2548828125 = 0.25937339663505554 + 50.0 * 8.319910049438477
Epoch 2790, val loss: 0.4594956040382385
Epoch 2800, training loss: 416.029541015625 = 0.2579837441444397 + 50.0 * 8.315430641174316
Epoch 2800, val loss: 0.460167795419693
Epoch 2810, training loss: 415.9680480957031 = 0.25659623742103577 + 50.0 * 8.314229011535645
Epoch 2810, val loss: 0.46039822697639465
Epoch 2820, training loss: 415.95361328125 = 0.25522661209106445 + 50.0 * 8.31396770477295
Epoch 2820, val loss: 0.4608750343322754
Epoch 2830, training loss: 415.945556640625 = 0.2538527250289917 + 50.0 * 8.313834190368652
Epoch 2830, val loss: 0.46121370792388916
Epoch 2840, training loss: 415.9549255371094 = 0.2524808347225189 + 50.0 * 8.314048767089844
Epoch 2840, val loss: 0.4616648256778717
Epoch 2850, training loss: 416.4082946777344 = 0.25113585591316223 + 50.0 * 8.323143005371094
Epoch 2850, val loss: 0.46213671565055847
Epoch 2860, training loss: 416.2035217285156 = 0.24974414706230164 + 50.0 * 8.319075584411621
Epoch 2860, val loss: 0.462568461894989
Epoch 2870, training loss: 416.02154541015625 = 0.24834424257278442 + 50.0 * 8.31546401977539
Epoch 2870, val loss: 0.4630594849586487
Epoch 2880, training loss: 415.93804931640625 = 0.2469453662633896 + 50.0 * 8.313821792602539
Epoch 2880, val loss: 0.4633830487728119
Epoch 2890, training loss: 415.93634033203125 = 0.2455616146326065 + 50.0 * 8.313815116882324
Epoch 2890, val loss: 0.4637482762336731
Epoch 2900, training loss: 415.9479675292969 = 0.24419160187244415 + 50.0 * 8.314075469970703
Epoch 2900, val loss: 0.46435028314590454
Epoch 2910, training loss: 416.1282653808594 = 0.24283945560455322 + 50.0 * 8.317708969116211
Epoch 2910, val loss: 0.46497881412506104
Epoch 2920, training loss: 415.9390869140625 = 0.24144455790519714 + 50.0 * 8.313952445983887
Epoch 2920, val loss: 0.4652484655380249
Epoch 2930, training loss: 415.9368896484375 = 0.24008364975452423 + 50.0 * 8.313936233520508
Epoch 2930, val loss: 0.4659246504306793
Epoch 2940, training loss: 416.0162048339844 = 0.2387229949235916 + 50.0 * 8.315549850463867
Epoch 2940, val loss: 0.4664139449596405
Epoch 2950, training loss: 415.8806457519531 = 0.23734521865844727 + 50.0 * 8.3128662109375
Epoch 2950, val loss: 0.4667212963104248
Epoch 2960, training loss: 415.9272155761719 = 0.23600152134895325 + 50.0 * 8.313824653625488
Epoch 2960, val loss: 0.46729525923728943
Epoch 2970, training loss: 416.0191955566406 = 0.23465991020202637 + 50.0 * 8.315690994262695
Epoch 2970, val loss: 0.4679006338119507
Epoch 2980, training loss: 416.01287841796875 = 0.2333226501941681 + 50.0 * 8.315590858459473
Epoch 2980, val loss: 0.46871957182884216
Epoch 2990, training loss: 415.8831481933594 = 0.23197264969348907 + 50.0 * 8.313023567199707
Epoch 2990, val loss: 0.4690786898136139
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8214104515474379
0.8411214953271029
The final CL Acc:0.82344, 0.00190, The final GNN Acc:0.84122, 0.00065
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110896])
remove edge: torch.Size([2, 66498])
updated graph: torch.Size([2, 88746])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2119140625 = 1.0985453128814697 + 50.0 * 10.582266807556152
Epoch 0, val loss: 1.0988702774047852
Epoch 10, training loss: 530.149169921875 = 1.091692566871643 + 50.0 * 10.58115005493164
Epoch 10, val loss: 1.0919355154037476
Epoch 20, training loss: 529.6166381835938 = 1.0833652019500732 + 50.0 * 10.57066535949707
Epoch 20, val loss: 1.0834859609603882
Epoch 30, training loss: 526.0689086914062 = 1.0734307765960693 + 50.0 * 10.499909400939941
Epoch 30, val loss: 1.073346734046936
Epoch 40, training loss: 513.7432861328125 = 1.0630853176116943 + 50.0 * 10.2536039352417
Epoch 40, val loss: 1.0629806518554688
Epoch 50, training loss: 489.6858215332031 = 1.0541189908981323 + 50.0 * 9.772634506225586
Epoch 50, val loss: 1.0538300275802612
Epoch 60, training loss: 476.9432067871094 = 1.045557975769043 + 50.0 * 9.517952919006348
Epoch 60, val loss: 1.0450941324234009
Epoch 70, training loss: 471.91363525390625 = 1.0359416007995605 + 50.0 * 9.417553901672363
Epoch 70, val loss: 1.0356922149658203
Epoch 80, training loss: 466.72381591796875 = 1.0276285409927368 + 50.0 * 9.313923835754395
Epoch 80, val loss: 1.0277719497680664
Epoch 90, training loss: 461.99871826171875 = 1.0203014612197876 + 50.0 * 9.219568252563477
Epoch 90, val loss: 1.02070152759552
Epoch 100, training loss: 458.476806640625 = 1.0128248929977417 + 50.0 * 9.149279594421387
Epoch 100, val loss: 1.0133028030395508
Epoch 110, training loss: 455.85980224609375 = 1.0054951906204224 + 50.0 * 9.097085952758789
Epoch 110, val loss: 1.0059962272644043
Epoch 120, training loss: 450.6407470703125 = 0.9979847073554993 + 50.0 * 8.992855072021484
Epoch 120, val loss: 0.9985162019729614
Epoch 130, training loss: 443.9017028808594 = 0.9913663268089294 + 50.0 * 8.858206748962402
Epoch 130, val loss: 0.9921748638153076
Epoch 140, training loss: 439.88525390625 = 0.985936164855957 + 50.0 * 8.777986526489258
Epoch 140, val loss: 0.9869228005409241
Epoch 150, training loss: 437.5726318359375 = 0.9784165620803833 + 50.0 * 8.731884002685547
Epoch 150, val loss: 0.9791518449783325
Epoch 160, training loss: 435.9389343261719 = 0.9678075909614563 + 50.0 * 8.699422836303711
Epoch 160, val loss: 0.9686469435691833
Epoch 170, training loss: 434.47021484375 = 0.9571037888526917 + 50.0 * 8.670262336730957
Epoch 170, val loss: 0.9583311080932617
Epoch 180, training loss: 432.9347839355469 = 0.9469499588012695 + 50.0 * 8.639756202697754
Epoch 180, val loss: 0.948589026927948
Epoch 190, training loss: 431.39422607421875 = 0.9365343451499939 + 50.0 * 8.609153747558594
Epoch 190, val loss: 0.9384721517562866
Epoch 200, training loss: 430.1184387207031 = 0.9253948330879211 + 50.0 * 8.583861351013184
Epoch 200, val loss: 0.9276627898216248
Epoch 210, training loss: 428.9671936035156 = 0.9133033156394958 + 50.0 * 8.561078071594238
Epoch 210, val loss: 0.9157534241676331
Epoch 220, training loss: 427.9393310546875 = 0.9002642035484314 + 50.0 * 8.540781021118164
Epoch 220, val loss: 0.903293251991272
Epoch 230, training loss: 427.06622314453125 = 0.8864428400993347 + 50.0 * 8.523595809936523
Epoch 230, val loss: 0.8899460434913635
Epoch 240, training loss: 426.46563720703125 = 0.8715890049934387 + 50.0 * 8.511880874633789
Epoch 240, val loss: 0.8754808306694031
Epoch 250, training loss: 425.8031311035156 = 0.8560285568237305 + 50.0 * 8.498942375183105
Epoch 250, val loss: 0.8606943488121033
Epoch 260, training loss: 425.2286682128906 = 0.8402085304260254 + 50.0 * 8.48776912689209
Epoch 260, val loss: 0.8455691337585449
Epoch 270, training loss: 424.73492431640625 = 0.8239708542823792 + 50.0 * 8.478219032287598
Epoch 270, val loss: 0.8300423622131348
Epoch 280, training loss: 424.5650329589844 = 0.8073593378067017 + 50.0 * 8.475152969360352
Epoch 280, val loss: 0.8142369985580444
Epoch 290, training loss: 424.23345947265625 = 0.7901771664619446 + 50.0 * 8.468865394592285
Epoch 290, val loss: 0.7978427410125732
Epoch 300, training loss: 423.7181396484375 = 0.772977352142334 + 50.0 * 8.458903312683105
Epoch 300, val loss: 0.7816718220710754
Epoch 310, training loss: 423.48773193359375 = 0.7558441758155823 + 50.0 * 8.45463752746582
Epoch 310, val loss: 0.7655187845230103
Epoch 320, training loss: 423.1471252441406 = 0.7384864091873169 + 50.0 * 8.448172569274902
Epoch 320, val loss: 0.7493364214897156
Epoch 330, training loss: 422.7725830078125 = 0.7215249538421631 + 50.0 * 8.441020965576172
Epoch 330, val loss: 0.733448326587677
Epoch 340, training loss: 422.4632873535156 = 0.7048590779304504 + 50.0 * 8.435168266296387
Epoch 340, val loss: 0.7180743217468262
Epoch 350, training loss: 422.583251953125 = 0.6885814666748047 + 50.0 * 8.437893867492676
Epoch 350, val loss: 0.7031229734420776
Epoch 360, training loss: 422.0022277832031 = 0.6727967262268066 + 50.0 * 8.426589012145996
Epoch 360, val loss: 0.6884323358535767
Epoch 370, training loss: 421.75238037109375 = 0.6577688455581665 + 50.0 * 8.421892166137695
Epoch 370, val loss: 0.6747667789459229
Epoch 380, training loss: 421.5450439453125 = 0.6434957385063171 + 50.0 * 8.418030738830566
Epoch 380, val loss: 0.6617618203163147
Epoch 390, training loss: 421.7547607421875 = 0.6299067735671997 + 50.0 * 8.422496795654297
Epoch 390, val loss: 0.6494603157043457
Epoch 400, training loss: 421.25537109375 = 0.6170105338096619 + 50.0 * 8.41276741027832
Epoch 400, val loss: 0.6378270983695984
Epoch 410, training loss: 421.0240478515625 = 0.6050599813461304 + 50.0 * 8.408379554748535
Epoch 410, val loss: 0.6272088885307312
Epoch 420, training loss: 420.85369873046875 = 0.5939377546310425 + 50.0 * 8.405195236206055
Epoch 420, val loss: 0.6172459721565247
Epoch 430, training loss: 420.8443908691406 = 0.5835312008857727 + 50.0 * 8.405217170715332
Epoch 430, val loss: 0.6081266403198242
Epoch 440, training loss: 420.8481750488281 = 0.573729395866394 + 50.0 * 8.405488967895508
Epoch 440, val loss: 0.5992587804794312
Epoch 450, training loss: 420.517578125 = 0.5646659135818481 + 50.0 * 8.39905834197998
Epoch 450, val loss: 0.5914742946624756
Epoch 460, training loss: 420.36297607421875 = 0.5563992857933044 + 50.0 * 8.39613151550293
Epoch 460, val loss: 0.5843326449394226
Epoch 470, training loss: 420.25445556640625 = 0.5487287044525146 + 50.0 * 8.39411449432373
Epoch 470, val loss: 0.5777626633644104
Epoch 480, training loss: 420.23828125 = 0.5416256189346313 + 50.0 * 8.393933296203613
Epoch 480, val loss: 0.571753740310669
Epoch 490, training loss: 420.15972900390625 = 0.5348970890045166 + 50.0 * 8.392496109008789
Epoch 490, val loss: 0.565919041633606
Epoch 500, training loss: 420.0176696777344 = 0.528706431388855 + 50.0 * 8.389779090881348
Epoch 500, val loss: 0.5607873201370239
Epoch 510, training loss: 419.87188720703125 = 0.5230369567871094 + 50.0 * 8.386977195739746
Epoch 510, val loss: 0.5560678243637085
Epoch 520, training loss: 419.7969970703125 = 0.517768919467926 + 50.0 * 8.385584831237793
Epoch 520, val loss: 0.5517218112945557
Epoch 530, training loss: 419.84619140625 = 0.5128265023231506 + 50.0 * 8.386667251586914
Epoch 530, val loss: 0.5477433800697327
Epoch 540, training loss: 419.930908203125 = 0.5080916881561279 + 50.0 * 8.388456344604492
Epoch 540, val loss: 0.5437276363372803
Epoch 550, training loss: 419.576904296875 = 0.50365149974823 + 50.0 * 8.381464958190918
Epoch 550, val loss: 0.5402840971946716
Epoch 560, training loss: 419.485107421875 = 0.49961233139038086 + 50.0 * 8.37971019744873
Epoch 560, val loss: 0.5370354056358337
Epoch 570, training loss: 419.39849853515625 = 0.49581965804100037 + 50.0 * 8.378053665161133
Epoch 570, val loss: 0.5340690612792969
Epoch 580, training loss: 419.3352355957031 = 0.49223795533180237 + 50.0 * 8.376859664916992
Epoch 580, val loss: 0.5312753319740295
Epoch 590, training loss: 419.5065612792969 = 0.4888158440589905 + 50.0 * 8.380354881286621
Epoch 590, val loss: 0.5285488963127136
Epoch 600, training loss: 419.3501281738281 = 0.48544326424598694 + 50.0 * 8.377293586730957
Epoch 600, val loss: 0.5260966420173645
Epoch 610, training loss: 419.3681335449219 = 0.48227617144584656 + 50.0 * 8.377717018127441
Epoch 610, val loss: 0.5236272811889648
Epoch 620, training loss: 419.1230163574219 = 0.47925445437431335 + 50.0 * 8.372875213623047
Epoch 620, val loss: 0.5213997960090637
Epoch 630, training loss: 419.0146179199219 = 0.47642552852630615 + 50.0 * 8.370763778686523
Epoch 630, val loss: 0.5192336440086365
Epoch 640, training loss: 418.9362487792969 = 0.47371962666511536 + 50.0 * 8.369250297546387
Epoch 640, val loss: 0.5172919034957886
Epoch 650, training loss: 419.22314453125 = 0.47110095620155334 + 50.0 * 8.375041007995605
Epoch 650, val loss: 0.5153307318687439
Epoch 660, training loss: 419.0591125488281 = 0.4683949649333954 + 50.0 * 8.371814727783203
Epoch 660, val loss: 0.5133256912231445
Epoch 670, training loss: 418.79742431640625 = 0.46587806940078735 + 50.0 * 8.366630554199219
Epoch 670, val loss: 0.5115223526954651
Epoch 680, training loss: 418.6985168457031 = 0.46350806951522827 + 50.0 * 8.364700317382812
Epoch 680, val loss: 0.5098350048065186
Epoch 690, training loss: 418.6272888183594 = 0.46122172474861145 + 50.0 * 8.363321304321289
Epoch 690, val loss: 0.5082607865333557
Epoch 700, training loss: 418.5704345703125 = 0.45900192856788635 + 50.0 * 8.362228393554688
Epoch 700, val loss: 0.5065858960151672
Epoch 710, training loss: 419.2073974609375 = 0.45678505301475525 + 50.0 * 8.375012397766113
Epoch 710, val loss: 0.504980206489563
Epoch 720, training loss: 418.5146789550781 = 0.4544273912906647 + 50.0 * 8.361205101013184
Epoch 720, val loss: 0.5033667683601379
Epoch 730, training loss: 418.4169616699219 = 0.4522612988948822 + 50.0 * 8.359293937683105
Epoch 730, val loss: 0.5018875002861023
Epoch 740, training loss: 418.3645324707031 = 0.45023179054260254 + 50.0 * 8.358285903930664
Epoch 740, val loss: 0.5003983974456787
Epoch 750, training loss: 418.307373046875 = 0.4482574164867401 + 50.0 * 8.357182502746582
Epoch 750, val loss: 0.4990825951099396
Epoch 760, training loss: 418.2608642578125 = 0.4463166296482086 + 50.0 * 8.356290817260742
Epoch 760, val loss: 0.4977087676525116
Epoch 770, training loss: 418.9534606933594 = 0.44437456130981445 + 50.0 * 8.370182037353516
Epoch 770, val loss: 0.4962293803691864
Epoch 780, training loss: 418.3483581542969 = 0.44223636388778687 + 50.0 * 8.358122825622559
Epoch 780, val loss: 0.49473294615745544
Epoch 790, training loss: 418.171142578125 = 0.44027286767959595 + 50.0 * 8.35461711883545
Epoch 790, val loss: 0.4934622049331665
Epoch 800, training loss: 418.1124572753906 = 0.43845850229263306 + 50.0 * 8.353480339050293
Epoch 800, val loss: 0.4922042191028595
Epoch 810, training loss: 418.0487976074219 = 0.43668922781944275 + 50.0 * 8.352242469787598
Epoch 810, val loss: 0.49105796217918396
Epoch 820, training loss: 417.993408203125 = 0.4349486827850342 + 50.0 * 8.35116958618164
Epoch 820, val loss: 0.4898512661457062
Epoch 830, training loss: 417.9504089355469 = 0.43321365118026733 + 50.0 * 8.350343704223633
Epoch 830, val loss: 0.488674134016037
Epoch 840, training loss: 418.3258056640625 = 0.43146324157714844 + 50.0 * 8.357887268066406
Epoch 840, val loss: 0.4874688982963562
Epoch 850, training loss: 418.01116943359375 = 0.42965683341026306 + 50.0 * 8.351630210876465
Epoch 850, val loss: 0.4862980544567108
Epoch 860, training loss: 418.055419921875 = 0.4279431104660034 + 50.0 * 8.35254955291748
Epoch 860, val loss: 0.484912246465683
Epoch 870, training loss: 417.82763671875 = 0.4261780083179474 + 50.0 * 8.348029136657715
Epoch 870, val loss: 0.48406434059143066
Epoch 880, training loss: 417.7889709472656 = 0.4245479106903076 + 50.0 * 8.347288131713867
Epoch 880, val loss: 0.4826812744140625
Epoch 890, training loss: 417.7554626464844 = 0.4229411780834198 + 50.0 * 8.346650123596191
Epoch 890, val loss: 0.48181504011154175
Epoch 900, training loss: 418.0184326171875 = 0.4213370382785797 + 50.0 * 8.35194206237793
Epoch 900, val loss: 0.48058071732521057
Epoch 910, training loss: 417.746826171875 = 0.41969993710517883 + 50.0 * 8.346542358398438
Epoch 910, val loss: 0.47973838448524475
Epoch 920, training loss: 417.62347412109375 = 0.4181496798992157 + 50.0 * 8.344106674194336
Epoch 920, val loss: 0.4785347282886505
Epoch 930, training loss: 417.5965576171875 = 0.41662395000457764 + 50.0 * 8.343598365783691
Epoch 930, val loss: 0.47775715589523315
Epoch 940, training loss: 417.65802001953125 = 0.4151128828525543 + 50.0 * 8.344858169555664
Epoch 940, val loss: 0.4766213297843933
Epoch 950, training loss: 417.63909912109375 = 0.4135919213294983 + 50.0 * 8.344510078430176
Epoch 950, val loss: 0.4755997359752655
Epoch 960, training loss: 417.4824523925781 = 0.4120108187198639 + 50.0 * 8.341408729553223
Epoch 960, val loss: 0.47457078099250793
Epoch 970, training loss: 417.4471740722656 = 0.4105035364627838 + 50.0 * 8.340733528137207
Epoch 970, val loss: 0.47367435693740845
Epoch 980, training loss: 417.3985595703125 = 0.40908241271972656 + 50.0 * 8.339789390563965
Epoch 980, val loss: 0.4727083146572113
Epoch 990, training loss: 417.36419677734375 = 0.40767785906791687 + 50.0 * 8.339130401611328
Epoch 990, val loss: 0.47188031673431396
Epoch 1000, training loss: 417.7496643066406 = 0.40626513957977295 + 50.0 * 8.346868515014648
Epoch 1000, val loss: 0.47090181708335876
Epoch 1010, training loss: 417.3653259277344 = 0.404802143573761 + 50.0 * 8.339210510253906
Epoch 1010, val loss: 0.4698871374130249
Epoch 1020, training loss: 417.2736511230469 = 0.40339139103889465 + 50.0 * 8.33740520477295
Epoch 1020, val loss: 0.4691516160964966
Epoch 1030, training loss: 417.2092590332031 = 0.40204158425331116 + 50.0 * 8.33614444732666
Epoch 1030, val loss: 0.46832454204559326
Epoch 1040, training loss: 417.2008056640625 = 0.4007191061973572 + 50.0 * 8.3360013961792
Epoch 1040, val loss: 0.4676012396812439
Epoch 1050, training loss: 417.33685302734375 = 0.399354487657547 + 50.0 * 8.338749885559082
Epoch 1050, val loss: 0.4667699635028839
Epoch 1060, training loss: 417.6763916015625 = 0.3979477882385254 + 50.0 * 8.345568656921387
Epoch 1060, val loss: 0.46593043208122253
Epoch 1070, training loss: 417.3170471191406 = 0.3965120315551758 + 50.0 * 8.338410377502441
Epoch 1070, val loss: 0.4648226201534271
Epoch 1080, training loss: 417.10784912109375 = 0.3951674699783325 + 50.0 * 8.334253311157227
Epoch 1080, val loss: 0.46387025713920593
Epoch 1090, training loss: 417.0184631347656 = 0.3939197361469269 + 50.0 * 8.332490921020508
Epoch 1090, val loss: 0.46313825249671936
Epoch 1100, training loss: 416.97772216796875 = 0.3926803767681122 + 50.0 * 8.331701278686523
Epoch 1100, val loss: 0.46244508028030396
Epoch 1110, training loss: 416.9643859863281 = 0.39145007729530334 + 50.0 * 8.331459045410156
Epoch 1110, val loss: 0.46171697974205017
Epoch 1120, training loss: 417.4629821777344 = 0.39019861817359924 + 50.0 * 8.341455459594727
Epoch 1120, val loss: 0.46082013845443726
Epoch 1130, training loss: 417.02606201171875 = 0.3888257145881653 + 50.0 * 8.332744598388672
Epoch 1130, val loss: 0.46007710695266724
Epoch 1140, training loss: 416.92059326171875 = 0.3875438868999481 + 50.0 * 8.330660820007324
Epoch 1140, val loss: 0.4592653214931488
Epoch 1150, training loss: 417.0186462402344 = 0.38628873229026794 + 50.0 * 8.332647323608398
Epoch 1150, val loss: 0.4584915041923523
Epoch 1160, training loss: 416.81866455078125 = 0.3850354850292206 + 50.0 * 8.328672409057617
Epoch 1160, val loss: 0.45783722400665283
Epoch 1170, training loss: 416.8325500488281 = 0.3838277757167816 + 50.0 * 8.328974723815918
Epoch 1170, val loss: 0.4571186304092407
Epoch 1180, training loss: 416.8396911621094 = 0.3826110064983368 + 50.0 * 8.329141616821289
Epoch 1180, val loss: 0.45641934871673584
Epoch 1190, training loss: 417.1521301269531 = 0.381376177072525 + 50.0 * 8.33541488647461
Epoch 1190, val loss: 0.45560672879219055
Epoch 1200, training loss: 416.8741149902344 = 0.38002559542655945 + 50.0 * 8.32988166809082
Epoch 1200, val loss: 0.4548628330230713
Epoch 1210, training loss: 416.7942810058594 = 0.37877005338668823 + 50.0 * 8.328310012817383
Epoch 1210, val loss: 0.45407670736312866
Epoch 1220, training loss: 416.69775390625 = 0.37757059931755066 + 50.0 * 8.326403617858887
Epoch 1220, val loss: 0.4534066617488861
Epoch 1230, training loss: 416.68145751953125 = 0.37639397382736206 + 50.0 * 8.326101303100586
Epoch 1230, val loss: 0.45275765657424927
Epoch 1240, training loss: 416.8948669433594 = 0.3751884698867798 + 50.0 * 8.33039379119873
Epoch 1240, val loss: 0.4520833194255829
Epoch 1250, training loss: 416.6832580566406 = 0.37396422028541565 + 50.0 * 8.326186180114746
Epoch 1250, val loss: 0.4512854814529419
Epoch 1260, training loss: 416.60235595703125 = 0.37276020646095276 + 50.0 * 8.324591636657715
Epoch 1260, val loss: 0.45064571499824524
Epoch 1270, training loss: 416.7174987792969 = 0.37157630920410156 + 50.0 * 8.326918601989746
Epoch 1270, val loss: 0.4500214159488678
Epoch 1280, training loss: 416.61761474609375 = 0.3703201711177826 + 50.0 * 8.324945449829102
Epoch 1280, val loss: 0.4491029381752014
Epoch 1290, training loss: 416.5929870605469 = 0.3690803050994873 + 50.0 * 8.324478149414062
Epoch 1290, val loss: 0.4486304223537445
Epoch 1300, training loss: 416.533447265625 = 0.3679053783416748 + 50.0 * 8.323310852050781
Epoch 1300, val loss: 0.44779929518699646
Epoch 1310, training loss: 416.5030822753906 = 0.366733580827713 + 50.0 * 8.32272720336914
Epoch 1310, val loss: 0.4472419023513794
Epoch 1320, training loss: 416.7474670410156 = 0.36556538939476013 + 50.0 * 8.327637672424316
Epoch 1320, val loss: 0.44660380482673645
Epoch 1330, training loss: 416.599609375 = 0.36433112621307373 + 50.0 * 8.324706077575684
Epoch 1330, val loss: 0.4457469880580902
Epoch 1340, training loss: 416.4688415527344 = 0.3631105422973633 + 50.0 * 8.322114944458008
Epoch 1340, val loss: 0.4451010525226593
Epoch 1350, training loss: 416.4071350097656 = 0.3619428277015686 + 50.0 * 8.320903778076172
Epoch 1350, val loss: 0.44446083903312683
Epoch 1360, training loss: 416.40191650390625 = 0.3607896566390991 + 50.0 * 8.320822715759277
Epoch 1360, val loss: 0.4439229667186737
Epoch 1370, training loss: 416.66326904296875 = 0.35962289571762085 + 50.0 * 8.326072692871094
Epoch 1370, val loss: 0.44338470697402954
Epoch 1380, training loss: 416.5400085449219 = 0.3583911657333374 + 50.0 * 8.32363224029541
Epoch 1380, val loss: 0.44226041436195374
Epoch 1390, training loss: 416.45068359375 = 0.3571784794330597 + 50.0 * 8.321869850158691
Epoch 1390, val loss: 0.4417876899242401
Epoch 1400, training loss: 416.441650390625 = 0.3559768497943878 + 50.0 * 8.3217134475708
Epoch 1400, val loss: 0.44104108214378357
Epoch 1410, training loss: 416.32550048828125 = 0.3547991216182709 + 50.0 * 8.319414138793945
Epoch 1410, val loss: 0.44033077359199524
Epoch 1420, training loss: 416.3099060058594 = 0.353651225566864 + 50.0 * 8.319125175476074
Epoch 1420, val loss: 0.43977344036102295
Epoch 1430, training loss: 416.2786865234375 = 0.35252106189727783 + 50.0 * 8.318523406982422
Epoch 1430, val loss: 0.43909722566604614
Epoch 1440, training loss: 416.4534606933594 = 0.35138505697250366 + 50.0 * 8.322041511535645
Epoch 1440, val loss: 0.4384979009628296
Epoch 1450, training loss: 416.22882080078125 = 0.35018450021743774 + 50.0 * 8.317572593688965
Epoch 1450, val loss: 0.4377530813217163
Epoch 1460, training loss: 416.2364196777344 = 0.34902581572532654 + 50.0 * 8.317748069763184
Epoch 1460, val loss: 0.43713536858558655
Epoch 1470, training loss: 416.5395812988281 = 0.34787771105766296 + 50.0 * 8.323834419250488
Epoch 1470, val loss: 0.4364398717880249
Epoch 1480, training loss: 416.21630859375 = 0.34667137265205383 + 50.0 * 8.317392349243164
Epoch 1480, val loss: 0.4358685314655304
Epoch 1490, training loss: 416.16058349609375 = 0.3455391228199005 + 50.0 * 8.316300392150879
Epoch 1490, val loss: 0.4352448880672455
Epoch 1500, training loss: 416.1387023925781 = 0.34441834688186646 + 50.0 * 8.315885543823242
Epoch 1500, val loss: 0.4346722364425659
Epoch 1510, training loss: 416.1356506347656 = 0.34331247210502625 + 50.0 * 8.31584644317627
Epoch 1510, val loss: 0.4340776205062866
Epoch 1520, training loss: 416.4711608886719 = 0.3421891927719116 + 50.0 * 8.322579383850098
Epoch 1520, val loss: 0.43349212408065796
Epoch 1530, training loss: 416.2824401855469 = 0.3409979045391083 + 50.0 * 8.318828582763672
Epoch 1530, val loss: 0.43274039030075073
Epoch 1540, training loss: 416.2650146484375 = 0.3398421108722687 + 50.0 * 8.318503379821777
Epoch 1540, val loss: 0.4321848750114441
Epoch 1550, training loss: 416.1271057128906 = 0.3386957347393036 + 50.0 * 8.315768241882324
Epoch 1550, val loss: 0.4316380023956299
Epoch 1560, training loss: 416.1302795410156 = 0.3375721871852875 + 50.0 * 8.3158540725708
Epoch 1560, val loss: 0.4309943616390228
Epoch 1570, training loss: 416.06951904296875 = 0.33645355701446533 + 50.0 * 8.314661026000977
Epoch 1570, val loss: 0.4304361045360565
Epoch 1580, training loss: 416.07098388671875 = 0.33534929156303406 + 50.0 * 8.314712524414062
Epoch 1580, val loss: 0.42981165647506714
Epoch 1590, training loss: 416.1858215332031 = 0.33422988653182983 + 50.0 * 8.317031860351562
Epoch 1590, val loss: 0.4292518198490143
Epoch 1600, training loss: 416.0773620605469 = 0.3330835998058319 + 50.0 * 8.314886093139648
Epoch 1600, val loss: 0.4287209212779999
Epoch 1610, training loss: 415.99298095703125 = 0.33195552229881287 + 50.0 * 8.313220024108887
Epoch 1610, val loss: 0.4280852973461151
Epoch 1620, training loss: 416.04144287109375 = 0.33084553480148315 + 50.0 * 8.31421184539795
Epoch 1620, val loss: 0.4275864362716675
Epoch 1630, training loss: 416.0364074707031 = 0.3297255039215088 + 50.0 * 8.314133644104004
Epoch 1630, val loss: 0.42708995938301086
Epoch 1640, training loss: 415.9893798828125 = 0.32860448956489563 + 50.0 * 8.313215255737305
Epoch 1640, val loss: 0.4263514578342438
Epoch 1650, training loss: 416.2751159667969 = 0.32748669385910034 + 50.0 * 8.318952560424805
Epoch 1650, val loss: 0.4259321391582489
Epoch 1660, training loss: 416.0260009765625 = 0.3263217508792877 + 50.0 * 8.313993453979492
Epoch 1660, val loss: 0.42496809363365173
Epoch 1670, training loss: 415.9419860839844 = 0.3251968324184418 + 50.0 * 8.312335968017578
Epoch 1670, val loss: 0.42456838488578796
Epoch 1680, training loss: 416.1285705566406 = 0.32410094141960144 + 50.0 * 8.316089630126953
Epoch 1680, val loss: 0.4237692058086395
Epoch 1690, training loss: 415.90570068359375 = 0.3229426443576813 + 50.0 * 8.311655044555664
Epoch 1690, val loss: 0.4236051142215729
Epoch 1700, training loss: 415.8851013183594 = 0.3218311071395874 + 50.0 * 8.311264991760254
Epoch 1700, val loss: 0.42286065220832825
Epoch 1710, training loss: 416.0400390625 = 0.3207191824913025 + 50.0 * 8.314386367797852
Epoch 1710, val loss: 0.4223848879337311
Epoch 1720, training loss: 415.82354736328125 = 0.3195727467536926 + 50.0 * 8.310079574584961
Epoch 1720, val loss: 0.42192429304122925
Epoch 1730, training loss: 415.855224609375 = 0.31846171617507935 + 50.0 * 8.310735702514648
Epoch 1730, val loss: 0.42137739062309265
Epoch 1740, training loss: 415.8475646972656 = 0.3173545300960541 + 50.0 * 8.310604095458984
Epoch 1740, val loss: 0.4209436774253845
Epoch 1750, training loss: 415.9420471191406 = 0.31623777747154236 + 50.0 * 8.312516212463379
Epoch 1750, val loss: 0.4204266369342804
Epoch 1760, training loss: 415.85174560546875 = 0.3150991201400757 + 50.0 * 8.3107328414917
Epoch 1760, val loss: 0.419663667678833
Epoch 1770, training loss: 415.82281494140625 = 0.3139706552028656 + 50.0 * 8.310176849365234
Epoch 1770, val loss: 0.4192850589752197
Epoch 1780, training loss: 415.76519775390625 = 0.3128608167171478 + 50.0 * 8.309046745300293
Epoch 1780, val loss: 0.41877281665802
Epoch 1790, training loss: 415.90386962890625 = 0.3117595613002777 + 50.0 * 8.31184196472168
Epoch 1790, val loss: 0.41838183999061584
Epoch 1800, training loss: 415.87274169921875 = 0.31062737107276917 + 50.0 * 8.31124210357666
Epoch 1800, val loss: 0.41758573055267334
Epoch 1810, training loss: 415.95526123046875 = 0.3094745874404907 + 50.0 * 8.312915802001953
Epoch 1810, val loss: 0.4173787534236908
Epoch 1820, training loss: 415.7288513183594 = 0.3083310127258301 + 50.0 * 8.30841064453125
Epoch 1820, val loss: 0.41665351390838623
Epoch 1830, training loss: 415.6900329589844 = 0.307230681180954 + 50.0 * 8.307656288146973
Epoch 1830, val loss: 0.41630154848098755
Epoch 1840, training loss: 415.6958923339844 = 0.306144118309021 + 50.0 * 8.307794570922852
Epoch 1840, val loss: 0.41587507724761963
Epoch 1850, training loss: 415.79534912109375 = 0.30505236983299255 + 50.0 * 8.309805870056152
Epoch 1850, val loss: 0.4152846932411194
Epoch 1860, training loss: 415.79949951171875 = 0.30391860008239746 + 50.0 * 8.309911727905273
Epoch 1860, val loss: 0.4148577153682709
Epoch 1870, training loss: 415.6834716796875 = 0.3027866780757904 + 50.0 * 8.307613372802734
Epoch 1870, val loss: 0.41451773047447205
Epoch 1880, training loss: 415.6960144042969 = 0.30168384313583374 + 50.0 * 8.307886123657227
Epoch 1880, val loss: 0.41407087445259094
Epoch 1890, training loss: 415.7153625488281 = 0.30057528614997864 + 50.0 * 8.308296203613281
Epoch 1890, val loss: 0.4136231541633606
Epoch 1900, training loss: 415.71240234375 = 0.2994517683982849 + 50.0 * 8.308259010314941
Epoch 1900, val loss: 0.41308194398880005
Epoch 1910, training loss: 415.6743469238281 = 0.29833585023880005 + 50.0 * 8.307519912719727
Epoch 1910, val loss: 0.41266268491744995
Epoch 1920, training loss: 415.7912292480469 = 0.29721787571907043 + 50.0 * 8.309880256652832
Epoch 1920, val loss: 0.4123395085334778
Epoch 1930, training loss: 415.6265869140625 = 0.29608526825904846 + 50.0 * 8.306610107421875
Epoch 1930, val loss: 0.4118027985095978
Epoch 1940, training loss: 415.58282470703125 = 0.29497429728507996 + 50.0 * 8.305756568908691
Epoch 1940, val loss: 0.41139233112335205
Epoch 1950, training loss: 415.7816467285156 = 0.293873131275177 + 50.0 * 8.309755325317383
Epoch 1950, val loss: 0.4108957052230835
Epoch 1960, training loss: 415.5600280761719 = 0.29274100065231323 + 50.0 * 8.30534553527832
Epoch 1960, val loss: 0.4107135534286499
Epoch 1970, training loss: 415.5743408203125 = 0.291640967130661 + 50.0 * 8.305654525756836
Epoch 1970, val loss: 0.41034215688705444
Epoch 1980, training loss: 415.8064880371094 = 0.2905387878417969 + 50.0 * 8.310318946838379
Epoch 1980, val loss: 0.4099656939506531
Epoch 1990, training loss: 415.5265808105469 = 0.289411336183548 + 50.0 * 8.304743766784668
Epoch 1990, val loss: 0.4094390571117401
Epoch 2000, training loss: 415.5030517578125 = 0.2883196175098419 + 50.0 * 8.30429458618164
Epoch 2000, val loss: 0.4091069996356964
Epoch 2010, training loss: 415.50531005859375 = 0.2872421145439148 + 50.0 * 8.304361343383789
Epoch 2010, val loss: 0.4087845981121063
Epoch 2020, training loss: 415.7036437988281 = 0.2861655056476593 + 50.0 * 8.308349609375
Epoch 2020, val loss: 0.40830665826797485
Epoch 2030, training loss: 415.5516052246094 = 0.2850402593612671 + 50.0 * 8.305331230163574
Epoch 2030, val loss: 0.4078937768936157
Epoch 2040, training loss: 415.48712158203125 = 0.2839311361312866 + 50.0 * 8.30406379699707
Epoch 2040, val loss: 0.4077538251876831
Epoch 2050, training loss: 415.6803894042969 = 0.28283965587615967 + 50.0 * 8.307950973510742
Epoch 2050, val loss: 0.407528281211853
Epoch 2060, training loss: 415.53485107421875 = 0.2817284166812897 + 50.0 * 8.305062294006348
Epoch 2060, val loss: 0.40676790475845337
Epoch 2070, training loss: 415.4994812011719 = 0.280622273683548 + 50.0 * 8.304377555847168
Epoch 2070, val loss: 0.4066442847251892
Epoch 2080, training loss: 415.45391845703125 = 0.2795324921607971 + 50.0 * 8.303487777709961
Epoch 2080, val loss: 0.4062834680080414
Epoch 2090, training loss: 415.42059326171875 = 0.27845191955566406 + 50.0 * 8.30284309387207
Epoch 2090, val loss: 0.4060405194759369
Epoch 2100, training loss: 415.5144348144531 = 0.27737852931022644 + 50.0 * 8.304740905761719
Epoch 2100, val loss: 0.4058893322944641
Epoch 2110, training loss: 415.6166076660156 = 0.276269793510437 + 50.0 * 8.306806564331055
Epoch 2110, val loss: 0.405403196811676
Epoch 2120, training loss: 415.4800109863281 = 0.2751498222351074 + 50.0 * 8.304097175598145
Epoch 2120, val loss: 0.4049195945262909
Epoch 2130, training loss: 415.417724609375 = 0.2740512490272522 + 50.0 * 8.302873611450195
Epoch 2130, val loss: 0.40484344959259033
Epoch 2140, training loss: 415.37060546875 = 0.27297770977020264 + 50.0 * 8.301952362060547
Epoch 2140, val loss: 0.40435823798179626
Epoch 2150, training loss: 415.4113464355469 = 0.2719050645828247 + 50.0 * 8.302788734436035
Epoch 2150, val loss: 0.4042018949985504
Epoch 2160, training loss: 415.5050048828125 = 0.2708130478858948 + 50.0 * 8.304683685302734
Epoch 2160, val loss: 0.40389129519462585
Epoch 2170, training loss: 415.5848693847656 = 0.2697097957134247 + 50.0 * 8.306303024291992
Epoch 2170, val loss: 0.40354636311531067
Epoch 2180, training loss: 415.4096984863281 = 0.2685915231704712 + 50.0 * 8.30282211303711
Epoch 2180, val loss: 0.40352684259414673
Epoch 2190, training loss: 415.3360290527344 = 0.2675008177757263 + 50.0 * 8.301370620727539
Epoch 2190, val loss: 0.403182715177536
Epoch 2200, training loss: 415.2999572753906 = 0.2664283215999603 + 50.0 * 8.300670623779297
Epoch 2200, val loss: 0.40292027592658997
Epoch 2210, training loss: 415.2840576171875 = 0.2653607726097107 + 50.0 * 8.300374031066895
Epoch 2210, val loss: 0.4027356803417206
Epoch 2220, training loss: 415.4122009277344 = 0.2642960548400879 + 50.0 * 8.302958488464355
Epoch 2220, val loss: 0.4023967981338501
Epoch 2230, training loss: 415.35498046875 = 0.26318177580833435 + 50.0 * 8.301836013793945
Epoch 2230, val loss: 0.40225204825401306
Epoch 2240, training loss: 415.3590393066406 = 0.2620677649974823 + 50.0 * 8.301939964294434
Epoch 2240, val loss: 0.401917964220047
Epoch 2250, training loss: 415.37451171875 = 0.2609644830226898 + 50.0 * 8.302270889282227
Epoch 2250, val loss: 0.4019664525985718
Epoch 2260, training loss: 415.2823791503906 = 0.2598685324192047 + 50.0 * 8.300450325012207
Epoch 2260, val loss: 0.4017619490623474
Epoch 2270, training loss: 415.37060546875 = 0.2587922513484955 + 50.0 * 8.302236557006836
Epoch 2270, val loss: 0.4016035199165344
Epoch 2280, training loss: 415.3088684082031 = 0.25769320130348206 + 50.0 * 8.301023483276367
Epoch 2280, val loss: 0.4012989401817322
Epoch 2290, training loss: 415.2507019042969 = 0.2565981447696686 + 50.0 * 8.299881935119629
Epoch 2290, val loss: 0.40119484066963196
Epoch 2300, training loss: 415.21905517578125 = 0.2555185556411743 + 50.0 * 8.299270629882812
Epoch 2300, val loss: 0.40107476711273193
Epoch 2310, training loss: 415.19189453125 = 0.25444597005844116 + 50.0 * 8.298748970031738
Epoch 2310, val loss: 0.400920033454895
Epoch 2320, training loss: 415.3202819824219 = 0.2533804774284363 + 50.0 * 8.301338195800781
Epoch 2320, val loss: 0.40089693665504456
Epoch 2330, training loss: 415.3900146484375 = 0.25227633118629456 + 50.0 * 8.302754402160645
Epoch 2330, val loss: 0.40050849318504333
Epoch 2340, training loss: 415.22271728515625 = 0.25114905834198 + 50.0 * 8.299430847167969
Epoch 2340, val loss: 0.4004797637462616
Epoch 2350, training loss: 415.1888427734375 = 0.25005412101745605 + 50.0 * 8.298775672912598
Epoch 2350, val loss: 0.4001486599445343
Epoch 2360, training loss: 415.1623229980469 = 0.2489810287952423 + 50.0 * 8.298266410827637
Epoch 2360, val loss: 0.4002205431461334
Epoch 2370, training loss: 415.1476135253906 = 0.24791963398456573 + 50.0 * 8.297993659973145
Epoch 2370, val loss: 0.39988723397254944
Epoch 2380, training loss: 415.2309875488281 = 0.2468610256910324 + 50.0 * 8.2996826171875
Epoch 2380, val loss: 0.39963892102241516
Epoch 2390, training loss: 415.30157470703125 = 0.24577565491199493 + 50.0 * 8.301115989685059
Epoch 2390, val loss: 0.3996136486530304
Epoch 2400, training loss: 415.1744689941406 = 0.24467165768146515 + 50.0 * 8.298596382141113
Epoch 2400, val loss: 0.3997960686683655
Epoch 2410, training loss: 415.2243957519531 = 0.24359042942523956 + 50.0 * 8.299615859985352
Epoch 2410, val loss: 0.3998027741909027
Epoch 2420, training loss: 415.13671875 = 0.24250884354114532 + 50.0 * 8.297883987426758
Epoch 2420, val loss: 0.39939674735069275
Epoch 2430, training loss: 415.08087158203125 = 0.24143534898757935 + 50.0 * 8.296789169311523
Epoch 2430, val loss: 0.399474561214447
Epoch 2440, training loss: 415.1144104003906 = 0.24037574231624603 + 50.0 * 8.297480583190918
Epoch 2440, val loss: 0.39935585856437683
Epoch 2450, training loss: 415.4036560058594 = 0.23931030929088593 + 50.0 * 8.3032865524292
Epoch 2450, val loss: 0.3991923928260803
Epoch 2460, training loss: 415.200927734375 = 0.23821230232715607 + 50.0 * 8.299254417419434
Epoch 2460, val loss: 0.3991316258907318
Epoch 2470, training loss: 415.0978088378906 = 0.23712466657161713 + 50.0 * 8.297213554382324
Epoch 2470, val loss: 0.39933162927627563
Epoch 2480, training loss: 415.0555419921875 = 0.23605482280254364 + 50.0 * 8.29638957977295
Epoch 2480, val loss: 0.39892709255218506
Epoch 2490, training loss: 415.1227111816406 = 0.2349969744682312 + 50.0 * 8.297754287719727
Epoch 2490, val loss: 0.3989517092704773
Epoch 2500, training loss: 415.2245788574219 = 0.23392276465892792 + 50.0 * 8.299813270568848
Epoch 2500, val loss: 0.3989018499851227
Epoch 2510, training loss: 415.2069091796875 = 0.23284514248371124 + 50.0 * 8.299481391906738
Epoch 2510, val loss: 0.39887651801109314
Epoch 2520, training loss: 415.04217529296875 = 0.23176179826259613 + 50.0 * 8.296208381652832
Epoch 2520, val loss: 0.3987601101398468
Epoch 2530, training loss: 414.9913024902344 = 0.2306991070508957 + 50.0 * 8.295211791992188
Epoch 2530, val loss: 0.3987259864807129
Epoch 2540, training loss: 415.00244140625 = 0.22965209186077118 + 50.0 * 8.295455932617188
Epoch 2540, val loss: 0.3986916244029999
Epoch 2550, training loss: 415.20843505859375 = 0.228615403175354 + 50.0 * 8.299596786499023
Epoch 2550, val loss: 0.39852505922317505
Epoch 2560, training loss: 414.9820251464844 = 0.22753214836120605 + 50.0 * 8.295089721679688
Epoch 2560, val loss: 0.3987926244735718
Epoch 2570, training loss: 415.0798034667969 = 0.22648082673549652 + 50.0 * 8.297066688537598
Epoch 2570, val loss: 0.39894190430641174
Epoch 2580, training loss: 415.1881103515625 = 0.2254127562046051 + 50.0 * 8.299254417419434
Epoch 2580, val loss: 0.3987824618816376
Epoch 2590, training loss: 415.01824951171875 = 0.2243385761976242 + 50.0 * 8.295878410339355
Epoch 2590, val loss: 0.3985529839992523
Epoch 2600, training loss: 414.9790344238281 = 0.22328118979930878 + 50.0 * 8.29511547088623
Epoch 2600, val loss: 0.3988450765609741
Epoch 2610, training loss: 414.9353332519531 = 0.22223161160945892 + 50.0 * 8.294261932373047
Epoch 2610, val loss: 0.3985765278339386
Epoch 2620, training loss: 414.9181213378906 = 0.22118273377418518 + 50.0 * 8.293938636779785
Epoch 2620, val loss: 0.39880210161209106
Epoch 2630, training loss: 415.0987548828125 = 0.22015370428562164 + 50.0 * 8.297572135925293
Epoch 2630, val loss: 0.3991423547267914
Epoch 2640, training loss: 414.97711181640625 = 0.21907348930835724 + 50.0 * 8.295160293579102
Epoch 2640, val loss: 0.3984162211418152
Epoch 2650, training loss: 414.9399108886719 = 0.2179986983537674 + 50.0 * 8.294438362121582
Epoch 2650, val loss: 0.3991507887840271
Epoch 2660, training loss: 414.9248352050781 = 0.21693472564220428 + 50.0 * 8.294157981872559
Epoch 2660, val loss: 0.39866241812705994
Epoch 2670, training loss: 414.94512939453125 = 0.21588799357414246 + 50.0 * 8.294585227966309
Epoch 2670, val loss: 0.39901864528656006
Epoch 2680, training loss: 415.3503112792969 = 0.21484053134918213 + 50.0 * 8.302709579467773
Epoch 2680, val loss: 0.39885213971138
Epoch 2690, training loss: 415.00799560546875 = 0.213758185505867 + 50.0 * 8.29588508605957
Epoch 2690, val loss: 0.39899134635925293
Epoch 2700, training loss: 414.89324951171875 = 0.2126978635787964 + 50.0 * 8.293610572814941
Epoch 2700, val loss: 0.3992217481136322
Epoch 2710, training loss: 414.8548889160156 = 0.21164153516292572 + 50.0 * 8.292864799499512
Epoch 2710, val loss: 0.3990972638130188
Epoch 2720, training loss: 414.9548034667969 = 0.2105957269668579 + 50.0 * 8.294883728027344
Epoch 2720, val loss: 0.3993939161300659
Epoch 2730, training loss: 414.9165344238281 = 0.20952336490154266 + 50.0 * 8.294139862060547
Epoch 2730, val loss: 0.3991575241088867
Epoch 2740, training loss: 414.9178466796875 = 0.20845367014408112 + 50.0 * 8.294187545776367
Epoch 2740, val loss: 0.3994060456752777
Epoch 2750, training loss: 414.90087890625 = 0.20737598836421967 + 50.0 * 8.293869972229004
Epoch 2750, val loss: 0.39942365884780884
Epoch 2760, training loss: 414.8443908691406 = 0.2063063383102417 + 50.0 * 8.29276180267334
Epoch 2760, val loss: 0.3998231589794159
Epoch 2770, training loss: 414.8331604003906 = 0.20523867011070251 + 50.0 * 8.292558670043945
Epoch 2770, val loss: 0.3997669816017151
Epoch 2780, training loss: 414.8417663574219 = 0.20418167114257812 + 50.0 * 8.29275131225586
Epoch 2780, val loss: 0.40011972188949585
Epoch 2790, training loss: 415.0133056640625 = 0.2031300663948059 + 50.0 * 8.29620361328125
Epoch 2790, val loss: 0.4003629982471466
Epoch 2800, training loss: 415.0079650878906 = 0.20204147696495056 + 50.0 * 8.29611873626709
Epoch 2800, val loss: 0.39988958835601807
Epoch 2810, training loss: 414.8653259277344 = 0.20095476508140564 + 50.0 * 8.29328727722168
Epoch 2810, val loss: 0.40034985542297363
Epoch 2820, training loss: 414.7998352050781 = 0.19987742602825165 + 50.0 * 8.291998863220215
Epoch 2820, val loss: 0.40039706230163574
Epoch 2830, training loss: 414.7894287109375 = 0.1988159567117691 + 50.0 * 8.2918119430542
Epoch 2830, val loss: 0.4006049633026123
Epoch 2840, training loss: 415.0436706542969 = 0.1977752149105072 + 50.0 * 8.296917915344238
Epoch 2840, val loss: 0.4009047746658325
Epoch 2850, training loss: 414.7845153808594 = 0.19667856395244598 + 50.0 * 8.291756629943848
Epoch 2850, val loss: 0.4006030857563019
Epoch 2860, training loss: 414.74224853515625 = 0.1956067830324173 + 50.0 * 8.290932655334473
Epoch 2860, val loss: 0.40098127722740173
Epoch 2870, training loss: 414.7274475097656 = 0.19454602897167206 + 50.0 * 8.290657997131348
Epoch 2870, val loss: 0.40115609765052795
Epoch 2880, training loss: 414.73712158203125 = 0.1934869885444641 + 50.0 * 8.290872573852539
Epoch 2880, val loss: 0.40138718485832214
Epoch 2890, training loss: 414.9147033691406 = 0.19244138896465302 + 50.0 * 8.294445037841797
Epoch 2890, val loss: 0.4017110764980316
Epoch 2900, training loss: 414.825927734375 = 0.1913653165102005 + 50.0 * 8.292691230773926
Epoch 2900, val loss: 0.4013938009738922
Epoch 2910, training loss: 415.08343505859375 = 0.19032765924930573 + 50.0 * 8.29786205291748
Epoch 2910, val loss: 0.4024129807949066
Epoch 2920, training loss: 414.7586364746094 = 0.18922331929206848 + 50.0 * 8.291388511657715
Epoch 2920, val loss: 0.40169820189476013
Epoch 2930, training loss: 414.7101135253906 = 0.1881525069475174 + 50.0 * 8.29043960571289
Epoch 2930, val loss: 0.40230581164360046
Epoch 2940, training loss: 414.68463134765625 = 0.1870996057987213 + 50.0 * 8.289950370788574
Epoch 2940, val loss: 0.4025331139564514
Epoch 2950, training loss: 414.67193603515625 = 0.18605078756809235 + 50.0 * 8.289717674255371
Epoch 2950, val loss: 0.40259411931037903
Epoch 2960, training loss: 414.671630859375 = 0.1850038468837738 + 50.0 * 8.289732933044434
Epoch 2960, val loss: 0.4029451906681061
Epoch 2970, training loss: 414.8085021972656 = 0.18395571410655975 + 50.0 * 8.29249095916748
Epoch 2970, val loss: 0.4031156599521637
Epoch 2980, training loss: 414.7513122558594 = 0.1828926056623459 + 50.0 * 8.29136848449707
Epoch 2980, val loss: 0.4034169018268585
Epoch 2990, training loss: 414.8260192871094 = 0.1818358600139618 + 50.0 * 8.29288387298584
Epoch 2990, val loss: 0.40372732281684875
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8366311516996447
0.867275230022459
=== training gcn model ===
Epoch 0, training loss: 530.204345703125 = 1.0909380912780762 + 50.0 * 10.582267761230469
Epoch 0, val loss: 1.0900852680206299
Epoch 10, training loss: 530.148193359375 = 1.085017204284668 + 50.0 * 10.581263542175293
Epoch 10, val loss: 1.0842525959014893
Epoch 20, training loss: 529.7042236328125 = 1.0784152746200562 + 50.0 * 10.572515487670898
Epoch 20, val loss: 1.077748417854309
Epoch 30, training loss: 526.8800659179688 = 1.0708506107330322 + 50.0 * 10.516183853149414
Epoch 30, val loss: 1.070261836051941
Epoch 40, training loss: 517.8311767578125 = 1.0629788637161255 + 50.0 * 10.335363388061523
Epoch 40, val loss: 1.0627033710479736
Epoch 50, training loss: 505.398193359375 = 1.0566874742507935 + 50.0 * 10.086830139160156
Epoch 50, val loss: 1.056404709815979
Epoch 60, training loss: 484.2718505859375 = 1.05089271068573 + 50.0 * 9.664419174194336
Epoch 60, val loss: 1.0504413843154907
Epoch 70, training loss: 467.68011474609375 = 1.0414944887161255 + 50.0 * 9.332772254943848
Epoch 70, val loss: 1.0409003496170044
Epoch 80, training loss: 456.4069519042969 = 1.0309298038482666 + 50.0 * 9.10752010345459
Epoch 80, val loss: 1.0307098627090454
Epoch 90, training loss: 449.61407470703125 = 1.0214403867721558 + 50.0 * 8.97185230255127
Epoch 90, val loss: 1.0217809677124023
Epoch 100, training loss: 447.94586181640625 = 1.0126432180404663 + 50.0 * 8.938664436340332
Epoch 100, val loss: 1.013347864151001
Epoch 110, training loss: 445.7876281738281 = 1.003775954246521 + 50.0 * 8.895676612854004
Epoch 110, val loss: 1.004703402519226
Epoch 120, training loss: 443.74737548828125 = 0.994632363319397 + 50.0 * 8.85505485534668
Epoch 120, val loss: 0.9957125186920166
Epoch 130, training loss: 442.20147705078125 = 0.9847086071968079 + 50.0 * 8.824335098266602
Epoch 130, val loss: 0.986014187335968
Epoch 140, training loss: 441.0534973144531 = 0.9738601446151733 + 50.0 * 8.801592826843262
Epoch 140, val loss: 0.975547730922699
Epoch 150, training loss: 439.4033508300781 = 0.9627000093460083 + 50.0 * 8.768813133239746
Epoch 150, val loss: 0.9648517966270447
Epoch 160, training loss: 437.2933044433594 = 0.9516207575798035 + 50.0 * 8.72683334350586
Epoch 160, val loss: 0.9542074799537659
Epoch 170, training loss: 435.816650390625 = 0.9403873682022095 + 50.0 * 8.697525024414062
Epoch 170, val loss: 0.9433168768882751
Epoch 180, training loss: 434.80389404296875 = 0.9280027747154236 + 50.0 * 8.677517890930176
Epoch 180, val loss: 0.9311349987983704
Epoch 190, training loss: 433.28265380859375 = 0.9148868322372437 + 50.0 * 8.647355079650879
Epoch 190, val loss: 0.9184368848800659
Epoch 200, training loss: 432.0226745605469 = 0.9021308422088623 + 50.0 * 8.622410774230957
Epoch 200, val loss: 0.9061534404754639
Epoch 210, training loss: 431.07293701171875 = 0.888835608959198 + 50.0 * 8.603682518005371
Epoch 210, val loss: 0.8934229612350464
Epoch 220, training loss: 430.37530517578125 = 0.8739477396011353 + 50.0 * 8.59002685546875
Epoch 220, val loss: 0.878937304019928
Epoch 230, training loss: 429.85882568359375 = 0.8578160405158997 + 50.0 * 8.5800199508667
Epoch 230, val loss: 0.8633262515068054
Epoch 240, training loss: 429.26336669921875 = 0.8414909243583679 + 50.0 * 8.568437576293945
Epoch 240, val loss: 0.8478268384933472
Epoch 250, training loss: 428.7248229980469 = 0.8255916237831116 + 50.0 * 8.557984352111816
Epoch 250, val loss: 0.8327971696853638
Epoch 260, training loss: 428.2216796875 = 0.8097986578941345 + 50.0 * 8.548237800598145
Epoch 260, val loss: 0.8178412914276123
Epoch 270, training loss: 427.9216613769531 = 0.7936475276947021 + 50.0 * 8.542560577392578
Epoch 270, val loss: 0.8025235533714294
Epoch 280, training loss: 427.44476318359375 = 0.7771708965301514 + 50.0 * 8.53335189819336
Epoch 280, val loss: 0.7870276570320129
Epoch 290, training loss: 427.1484375 = 0.7610829472541809 + 50.0 * 8.52774715423584
Epoch 290, val loss: 0.772027850151062
Epoch 300, training loss: 426.50653076171875 = 0.7453027367591858 + 50.0 * 8.51522445678711
Epoch 300, val loss: 0.7575380802154541
Epoch 310, training loss: 426.0319519042969 = 0.7303319573402405 + 50.0 * 8.50603199005127
Epoch 310, val loss: 0.743849515914917
Epoch 320, training loss: 425.6394348144531 = 0.7160608172416687 + 50.0 * 8.498467445373535
Epoch 320, val loss: 0.730864405632019
Epoch 330, training loss: 425.15301513671875 = 0.7019872069358826 + 50.0 * 8.489020347595215
Epoch 330, val loss: 0.7181258201599121
Epoch 340, training loss: 424.6903076171875 = 0.6883854866027832 + 50.0 * 8.4800386428833
Epoch 340, val loss: 0.705852210521698
Epoch 350, training loss: 424.4405212402344 = 0.6752247214317322 + 50.0 * 8.475305557250977
Epoch 350, val loss: 0.6940768957138062
Epoch 360, training loss: 423.90765380859375 = 0.6622428894042969 + 50.0 * 8.464908599853516
Epoch 360, val loss: 0.6826567649841309
Epoch 370, training loss: 423.6169738769531 = 0.649857759475708 + 50.0 * 8.459342002868652
Epoch 370, val loss: 0.6717276573181152
Epoch 380, training loss: 423.499755859375 = 0.6377699375152588 + 50.0 * 8.457240104675293
Epoch 380, val loss: 0.6612021923065186
Epoch 390, training loss: 423.1599426269531 = 0.6261069774627686 + 50.0 * 8.450676918029785
Epoch 390, val loss: 0.6511054039001465
Epoch 400, training loss: 422.8537902832031 = 0.615342378616333 + 50.0 * 8.444768905639648
Epoch 400, val loss: 0.6418858170509338
Epoch 410, training loss: 422.6131286621094 = 0.6053048968315125 + 50.0 * 8.440155982971191
Epoch 410, val loss: 0.6334482431411743
Epoch 420, training loss: 422.87493896484375 = 0.5958646535873413 + 50.0 * 8.445581436157227
Epoch 420, val loss: 0.6255732774734497
Epoch 430, training loss: 422.1499938964844 = 0.5868449807167053 + 50.0 * 8.431262969970703
Epoch 430, val loss: 0.6180201768875122
Epoch 440, training loss: 421.9202880859375 = 0.5786477327346802 + 50.0 * 8.426833152770996
Epoch 440, val loss: 0.6113147735595703
Epoch 450, training loss: 421.6823425292969 = 0.5711408853530884 + 50.0 * 8.422224044799805
Epoch 450, val loss: 0.6052682399749756
Epoch 460, training loss: 421.5440368652344 = 0.5641059875488281 + 50.0 * 8.419598579406738
Epoch 460, val loss: 0.59971684217453
Epoch 470, training loss: 421.472412109375 = 0.5571377277374268 + 50.0 * 8.418305397033691
Epoch 470, val loss: 0.5940577387809753
Epoch 480, training loss: 421.2130432128906 = 0.5506079196929932 + 50.0 * 8.413249015808105
Epoch 480, val loss: 0.5888998508453369
Epoch 490, training loss: 420.93756103515625 = 0.5446904301643372 + 50.0 * 8.407857894897461
Epoch 490, val loss: 0.5843505263328552
Epoch 500, training loss: 420.78387451171875 = 0.539129376411438 + 50.0 * 8.404894828796387
Epoch 500, val loss: 0.5801057815551758
Epoch 510, training loss: 420.96002197265625 = 0.5337830185890198 + 50.0 * 8.408524513244629
Epoch 510, val loss: 0.5760093331336975
Epoch 520, training loss: 420.5125427246094 = 0.5285383462905884 + 50.0 * 8.399680137634277
Epoch 520, val loss: 0.5720322132110596
Epoch 530, training loss: 420.3659973144531 = 0.523737907409668 + 50.0 * 8.396844863891602
Epoch 530, val loss: 0.5684689283370972
Epoch 540, training loss: 420.296630859375 = 0.5191853046417236 + 50.0 * 8.395548820495605
Epoch 540, val loss: 0.5651903748512268
Epoch 550, training loss: 420.2830505371094 = 0.5146821141242981 + 50.0 * 8.395367622375488
Epoch 550, val loss: 0.5617567300796509
Epoch 560, training loss: 420.027587890625 = 0.5104206800460815 + 50.0 * 8.39034366607666
Epoch 560, val loss: 0.5587109327316284
Epoch 570, training loss: 419.9637451171875 = 0.5064871311187744 + 50.0 * 8.389144897460938
Epoch 570, val loss: 0.5559223294258118
Epoch 580, training loss: 419.943359375 = 0.5027071833610535 + 50.0 * 8.388813018798828
Epoch 580, val loss: 0.5532315969467163
Epoch 590, training loss: 419.7376708984375 = 0.49903470277786255 + 50.0 * 8.384773254394531
Epoch 590, val loss: 0.5506417751312256
Epoch 600, training loss: 419.71282958984375 = 0.4955897033214569 + 50.0 * 8.384345054626465
Epoch 600, val loss: 0.5482125878334045
Epoch 610, training loss: 419.59356689453125 = 0.4922545552253723 + 50.0 * 8.382026672363281
Epoch 610, val loss: 0.5459379553794861
Epoch 620, training loss: 419.9304504394531 = 0.4890555739402771 + 50.0 * 8.38882827758789
Epoch 620, val loss: 0.5437988042831421
Epoch 630, training loss: 419.4159851074219 = 0.48584994673728943 + 50.0 * 8.378602981567383
Epoch 630, val loss: 0.5415540337562561
Epoch 640, training loss: 419.3289489746094 = 0.48289573192596436 + 50.0 * 8.376920700073242
Epoch 640, val loss: 0.539509654045105
Epoch 650, training loss: 419.312255859375 = 0.48010003566741943 + 50.0 * 8.376643180847168
Epoch 650, val loss: 0.537646472454071
Epoch 660, training loss: 419.4027099609375 = 0.47731325030326843 + 50.0 * 8.378507614135742
Epoch 660, val loss: 0.5357624888420105
Epoch 670, training loss: 419.18170166015625 = 0.47456902265548706 + 50.0 * 8.37414264678955
Epoch 670, val loss: 0.5338587164878845
Epoch 680, training loss: 419.0691833496094 = 0.47203388810157776 + 50.0 * 8.371942520141602
Epoch 680, val loss: 0.5321944952011108
Epoch 690, training loss: 419.0058898925781 = 0.4695674777030945 + 50.0 * 8.370726585388184
Epoch 690, val loss: 0.5305489301681519
Epoch 700, training loss: 419.0859375 = 0.4671511650085449 + 50.0 * 8.37237548828125
Epoch 700, val loss: 0.528879702091217
Epoch 710, training loss: 418.9973449707031 = 0.4647136330604553 + 50.0 * 8.370652198791504
Epoch 710, val loss: 0.5272645950317383
Epoch 720, training loss: 418.8870544433594 = 0.46239057183265686 + 50.0 * 8.36849308013916
Epoch 720, val loss: 0.5257344245910645
Epoch 730, training loss: 418.7845153808594 = 0.4600691795349121 + 50.0 * 8.366488456726074
Epoch 730, val loss: 0.524214506149292
Epoch 740, training loss: 418.69134521484375 = 0.4578477740287781 + 50.0 * 8.364669799804688
Epoch 740, val loss: 0.5226969718933105
Epoch 750, training loss: 418.59661865234375 = 0.45578280091285706 + 50.0 * 8.36281681060791
Epoch 750, val loss: 0.5214238166809082
Epoch 760, training loss: 418.54010009765625 = 0.4537770450115204 + 50.0 * 8.361726760864258
Epoch 760, val loss: 0.5200884342193604
Epoch 770, training loss: 418.5434875488281 = 0.45181095600128174 + 50.0 * 8.361833572387695
Epoch 770, val loss: 0.518757164478302
Epoch 780, training loss: 418.50885009765625 = 0.4497848451137543 + 50.0 * 8.361181259155273
Epoch 780, val loss: 0.5175564289093018
Epoch 790, training loss: 418.4931335449219 = 0.44780534505844116 + 50.0 * 8.360906600952148
Epoch 790, val loss: 0.5161039233207703
Epoch 800, training loss: 418.3119201660156 = 0.44590896368026733 + 50.0 * 8.357319831848145
Epoch 800, val loss: 0.5149755477905273
Epoch 810, training loss: 418.2507629394531 = 0.4441000819206238 + 50.0 * 8.356133460998535
Epoch 810, val loss: 0.5137997269630432
Epoch 820, training loss: 418.4733581542969 = 0.4423193037509918 + 50.0 * 8.360620498657227
Epoch 820, val loss: 0.5125528573989868
Epoch 830, training loss: 418.240966796875 = 0.44039031863212585 + 50.0 * 8.356011390686035
Epoch 830, val loss: 0.5113274455070496
Epoch 840, training loss: 418.216064453125 = 0.4385627508163452 + 50.0 * 8.355549812316895
Epoch 840, val loss: 0.5100531578063965
Epoch 850, training loss: 418.0582580566406 = 0.436904639005661 + 50.0 * 8.35242748260498
Epoch 850, val loss: 0.509034276008606
Epoch 860, training loss: 417.9739074707031 = 0.4352777302265167 + 50.0 * 8.350772857666016
Epoch 860, val loss: 0.5079452991485596
Epoch 870, training loss: 417.9213562011719 = 0.43369749188423157 + 50.0 * 8.349753379821777
Epoch 870, val loss: 0.5069036483764648
Epoch 880, training loss: 417.8992004394531 = 0.4321330785751343 + 50.0 * 8.34934139251709
Epoch 880, val loss: 0.5059047341346741
Epoch 890, training loss: 417.9827880859375 = 0.4305180609226227 + 50.0 * 8.351045608520508
Epoch 890, val loss: 0.5047773718833923
Epoch 900, training loss: 417.96539306640625 = 0.4288419783115387 + 50.0 * 8.350730895996094
Epoch 900, val loss: 0.5035995841026306
Epoch 910, training loss: 417.7737731933594 = 0.42721861600875854 + 50.0 * 8.346931457519531
Epoch 910, val loss: 0.5024351477622986
Epoch 920, training loss: 417.7113342285156 = 0.4256942868232727 + 50.0 * 8.345712661743164
Epoch 920, val loss: 0.5013738870620728
Epoch 930, training loss: 417.6698913574219 = 0.424221396446228 + 50.0 * 8.344913482666016
Epoch 930, val loss: 0.5003306865692139
Epoch 940, training loss: 417.9727783203125 = 0.42273572087287903 + 50.0 * 8.351000785827637
Epoch 940, val loss: 0.49926847219467163
Epoch 950, training loss: 417.621337890625 = 0.4211520552635193 + 50.0 * 8.344003677368164
Epoch 950, val loss: 0.4981427490711212
Epoch 960, training loss: 417.5202941894531 = 0.41969624161720276 + 50.0 * 8.342011451721191
Epoch 960, val loss: 0.4971593916416168
Epoch 970, training loss: 417.555419921875 = 0.418273389339447 + 50.0 * 8.342742919921875
Epoch 970, val loss: 0.49604663252830505
Epoch 980, training loss: 417.6217346191406 = 0.416781485080719 + 50.0 * 8.344099044799805
Epoch 980, val loss: 0.49502936005592346
Epoch 990, training loss: 417.4649963378906 = 0.41530895233154297 + 50.0 * 8.340993881225586
Epoch 990, val loss: 0.4939376711845398
Epoch 1000, training loss: 417.39532470703125 = 0.4139077067375183 + 50.0 * 8.339628219604492
Epoch 1000, val loss: 0.4929579198360443
Epoch 1010, training loss: 417.3716735839844 = 0.4125308394432068 + 50.0 * 8.33918285369873
Epoch 1010, val loss: 0.49196937680244446
Epoch 1020, training loss: 417.51812744140625 = 0.41112568974494934 + 50.0 * 8.342140197753906
Epoch 1020, val loss: 0.49079373478889465
Epoch 1030, training loss: 417.29595947265625 = 0.40968069434165955 + 50.0 * 8.337725639343262
Epoch 1030, val loss: 0.4899428188800812
Epoch 1040, training loss: 417.24847412109375 = 0.40829378366470337 + 50.0 * 8.336803436279297
Epoch 1040, val loss: 0.488849014043808
Epoch 1050, training loss: 417.27911376953125 = 0.4069440960884094 + 50.0 * 8.337443351745605
Epoch 1050, val loss: 0.4879184663295746
Epoch 1060, training loss: 417.24578857421875 = 0.4054848551750183 + 50.0 * 8.336806297302246
Epoch 1060, val loss: 0.4867938756942749
Epoch 1070, training loss: 417.1462707519531 = 0.4040512144565582 + 50.0 * 8.334844589233398
Epoch 1070, val loss: 0.48575034737586975
Epoch 1080, training loss: 417.1058044433594 = 0.40269970893859863 + 50.0 * 8.334061622619629
Epoch 1080, val loss: 0.4847373962402344
Epoch 1090, training loss: 417.05474853515625 = 0.4013912081718445 + 50.0 * 8.333066940307617
Epoch 1090, val loss: 0.48384779691696167
Epoch 1100, training loss: 417.136962890625 = 0.4000813364982605 + 50.0 * 8.334737777709961
Epoch 1100, val loss: 0.48281997442245483
Epoch 1110, training loss: 417.0563659667969 = 0.39867252111434937 + 50.0 * 8.33315372467041
Epoch 1110, val loss: 0.48176050186157227
Epoch 1120, training loss: 417.0675964355469 = 0.3972933888435364 + 50.0 * 8.333405494689941
Epoch 1120, val loss: 0.48079541325569153
Epoch 1130, training loss: 416.9969482421875 = 0.3959618806838989 + 50.0 * 8.332019805908203
Epoch 1130, val loss: 0.47987455129623413
Epoch 1140, training loss: 416.91424560546875 = 0.39464688301086426 + 50.0 * 8.330391883850098
Epoch 1140, val loss: 0.47888848185539246
Epoch 1150, training loss: 416.9320373535156 = 0.3933573365211487 + 50.0 * 8.33077335357666
Epoch 1150, val loss: 0.4779670238494873
Epoch 1160, training loss: 417.1310119628906 = 0.39202165603637695 + 50.0 * 8.334779739379883
Epoch 1160, val loss: 0.477037638425827
Epoch 1170, training loss: 416.9500732421875 = 0.39064911007881165 + 50.0 * 8.331188201904297
Epoch 1170, val loss: 0.47603464126586914
Epoch 1180, training loss: 416.83978271484375 = 0.3893466591835022 + 50.0 * 8.329009056091309
Epoch 1180, val loss: 0.47512388229370117
Epoch 1190, training loss: 416.7696228027344 = 0.38806313276290894 + 50.0 * 8.327630996704102
Epoch 1190, val loss: 0.4742143750190735
Epoch 1200, training loss: 416.7433166503906 = 0.38681119680404663 + 50.0 * 8.327130317687988
Epoch 1200, val loss: 0.47338730096817017
Epoch 1210, training loss: 416.9931640625 = 0.38549351692199707 + 50.0 * 8.3321533203125
Epoch 1210, val loss: 0.4725020229816437
Epoch 1220, training loss: 416.771484375 = 0.38412049412727356 + 50.0 * 8.327747344970703
Epoch 1220, val loss: 0.47154679894447327
Epoch 1230, training loss: 416.719482421875 = 0.3828137516975403 + 50.0 * 8.326733589172363
Epoch 1230, val loss: 0.47058603167533875
Epoch 1240, training loss: 416.7787780761719 = 0.38151827454566956 + 50.0 * 8.3279447555542
Epoch 1240, val loss: 0.46967753767967224
Epoch 1250, training loss: 416.6150207519531 = 0.3802480101585388 + 50.0 * 8.324695587158203
Epoch 1250, val loss: 0.4689093828201294
Epoch 1260, training loss: 416.6293640136719 = 0.3790002465248108 + 50.0 * 8.325007438659668
Epoch 1260, val loss: 0.4681345224380493
Epoch 1270, training loss: 416.6535339355469 = 0.3777499198913574 + 50.0 * 8.325515747070312
Epoch 1270, val loss: 0.4673447608947754
Epoch 1280, training loss: 416.69256591796875 = 0.3764348030090332 + 50.0 * 8.326322555541992
Epoch 1280, val loss: 0.46626055240631104
Epoch 1290, training loss: 416.5355529785156 = 0.3751387298107147 + 50.0 * 8.32320785522461
Epoch 1290, val loss: 0.4654924273490906
Epoch 1300, training loss: 416.47003173828125 = 0.37388876080513 + 50.0 * 8.321922302246094
Epoch 1300, val loss: 0.4646497666835785
Epoch 1310, training loss: 416.4614562988281 = 0.3726697862148285 + 50.0 * 8.321775436401367
Epoch 1310, val loss: 0.46377116441726685
Epoch 1320, training loss: 416.5055236816406 = 0.37144777178764343 + 50.0 * 8.322681427001953
Epoch 1320, val loss: 0.4630742073059082
Epoch 1330, training loss: 416.6459045410156 = 0.3701699674129486 + 50.0 * 8.325514793395996
Epoch 1330, val loss: 0.46218323707580566
Epoch 1340, training loss: 416.5880126953125 = 0.3688630163669586 + 50.0 * 8.324382781982422
Epoch 1340, val loss: 0.46144500374794006
Epoch 1350, training loss: 416.4156188964844 = 0.3675694167613983 + 50.0 * 8.320960998535156
Epoch 1350, val loss: 0.46053969860076904
Epoch 1360, training loss: 416.3927001953125 = 0.36631566286087036 + 50.0 * 8.320528030395508
Epoch 1360, val loss: 0.4598644971847534
Epoch 1370, training loss: 416.5049743652344 = 0.36506396532058716 + 50.0 * 8.322798728942871
Epoch 1370, val loss: 0.4590606093406677
Epoch 1380, training loss: 416.3398132324219 = 0.3637958765029907 + 50.0 * 8.319519996643066
Epoch 1380, val loss: 0.45825353264808655
Epoch 1390, training loss: 416.27557373046875 = 0.36256158351898193 + 50.0 * 8.318260192871094
Epoch 1390, val loss: 0.4575517475605011
Epoch 1400, training loss: 416.2918701171875 = 0.3613297641277313 + 50.0 * 8.318611145019531
Epoch 1400, val loss: 0.4568939805030823
Epoch 1410, training loss: 416.4579162597656 = 0.3600618541240692 + 50.0 * 8.321956634521484
Epoch 1410, val loss: 0.45620018243789673
Epoch 1420, training loss: 416.3368225097656 = 0.3587634265422821 + 50.0 * 8.319561004638672
Epoch 1420, val loss: 0.45518919825553894
Epoch 1430, training loss: 416.47674560546875 = 0.3574405908584595 + 50.0 * 8.322385787963867
Epoch 1430, val loss: 0.45447126030921936
Epoch 1440, training loss: 416.2439880371094 = 0.35612228512763977 + 50.0 * 8.317757606506348
Epoch 1440, val loss: 0.4537585973739624
Epoch 1450, training loss: 416.15228271484375 = 0.3548692464828491 + 50.0 * 8.315948486328125
Epoch 1450, val loss: 0.4529882073402405
Epoch 1460, training loss: 416.1215515136719 = 0.3536432385444641 + 50.0 * 8.31535816192627
Epoch 1460, val loss: 0.45225340127944946
Epoch 1470, training loss: 416.1139831542969 = 0.3524252474308014 + 50.0 * 8.315231323242188
Epoch 1470, val loss: 0.4516209363937378
Epoch 1480, training loss: 416.39923095703125 = 0.3511869013309479 + 50.0 * 8.320960998535156
Epoch 1480, val loss: 0.4507736265659332
Epoch 1490, training loss: 416.2376403808594 = 0.34986504912376404 + 50.0 * 8.317755699157715
Epoch 1490, val loss: 0.4502112865447998
Epoch 1500, training loss: 416.1132507324219 = 0.34856969118118286 + 50.0 * 8.315293312072754
Epoch 1500, val loss: 0.4494139850139618
Epoch 1510, training loss: 416.04937744140625 = 0.3473100960254669 + 50.0 * 8.314041137695312
Epoch 1510, val loss: 0.4487684667110443
Epoch 1520, training loss: 416.0687255859375 = 0.3460652232170105 + 50.0 * 8.314453125
Epoch 1520, val loss: 0.4481131136417389
Epoch 1530, training loss: 416.2482604980469 = 0.34479692578315735 + 50.0 * 8.318069458007812
Epoch 1530, val loss: 0.44741615653038025
Epoch 1540, training loss: 416.14752197265625 = 0.3434794247150421 + 50.0 * 8.316081047058105
Epoch 1540, val loss: 0.4464761018753052
Epoch 1550, training loss: 416.03387451171875 = 0.34217169880867004 + 50.0 * 8.313834190368652
Epoch 1550, val loss: 0.44596195220947266
Epoch 1560, training loss: 415.9740295410156 = 0.3409076929092407 + 50.0 * 8.312662124633789
Epoch 1560, val loss: 0.44532135128974915
Epoch 1570, training loss: 415.94000244140625 = 0.3396593928337097 + 50.0 * 8.312006950378418
Epoch 1570, val loss: 0.44468462467193604
Epoch 1580, training loss: 416.0700378417969 = 0.33840885758399963 + 50.0 * 8.314632415771484
Epoch 1580, val loss: 0.44411101937294006
Epoch 1590, training loss: 415.9391784667969 = 0.3370796740055084 + 50.0 * 8.312042236328125
Epoch 1590, val loss: 0.44335538148880005
Epoch 1600, training loss: 415.90765380859375 = 0.3357715904712677 + 50.0 * 8.311437606811523
Epoch 1600, val loss: 0.4427390694618225
Epoch 1610, training loss: 415.8760681152344 = 0.3344966769218445 + 50.0 * 8.310831069946289
Epoch 1610, val loss: 0.44211363792419434
Epoch 1620, training loss: 415.8578186035156 = 0.33323919773101807 + 50.0 * 8.310491561889648
Epoch 1620, val loss: 0.44148585200309753
Epoch 1630, training loss: 416.1646728515625 = 0.3319777250289917 + 50.0 * 8.316654205322266
Epoch 1630, val loss: 0.4410385191440582
Epoch 1640, training loss: 415.9915771484375 = 0.330640584230423 + 50.0 * 8.31321907043457
Epoch 1640, val loss: 0.4402090311050415
Epoch 1650, training loss: 415.8323669433594 = 0.32933226227760315 + 50.0 * 8.310060501098633
Epoch 1650, val loss: 0.43959948420524597
Epoch 1660, training loss: 415.793212890625 = 0.32805490493774414 + 50.0 * 8.309303283691406
Epoch 1660, val loss: 0.4390658140182495
Epoch 1670, training loss: 415.99212646484375 = 0.3267783224582672 + 50.0 * 8.31330680847168
Epoch 1670, val loss: 0.4384062588214874
Epoch 1680, training loss: 415.85430908203125 = 0.3254510760307312 + 50.0 * 8.310577392578125
Epoch 1680, val loss: 0.4376409351825714
Epoch 1690, training loss: 415.76129150390625 = 0.3241414725780487 + 50.0 * 8.308743476867676
Epoch 1690, val loss: 0.4372880756855011
Epoch 1700, training loss: 415.72515869140625 = 0.3228593170642853 + 50.0 * 8.308046340942383
Epoch 1700, val loss: 0.43652257323265076
Epoch 1710, training loss: 415.8580322265625 = 0.32158592343330383 + 50.0 * 8.310729026794434
Epoch 1710, val loss: 0.4359130561351776
Epoch 1720, training loss: 415.7689208984375 = 0.32024961709976196 + 50.0 * 8.30897331237793
Epoch 1720, val loss: 0.4354100525379181
Epoch 1730, training loss: 415.6929931640625 = 0.31892532110214233 + 50.0 * 8.307480812072754
Epoch 1730, val loss: 0.43486395478248596
Epoch 1740, training loss: 415.68316650390625 = 0.3176288902759552 + 50.0 * 8.307311058044434
Epoch 1740, val loss: 0.4341717064380646
Epoch 1750, training loss: 415.6999206542969 = 0.31634584069252014 + 50.0 * 8.307671546936035
Epoch 1750, val loss: 0.43371349573135376
Epoch 1760, training loss: 415.8091735839844 = 0.3150443732738495 + 50.0 * 8.309883117675781
Epoch 1760, val loss: 0.4329717457294464
Epoch 1770, training loss: 415.7469482421875 = 0.31371670961380005 + 50.0 * 8.308664321899414
Epoch 1770, val loss: 0.4327293038368225
Epoch 1780, training loss: 415.7994689941406 = 0.31238284707069397 + 50.0 * 8.309741973876953
Epoch 1780, val loss: 0.4320485293865204
Epoch 1790, training loss: 415.6212158203125 = 0.3110571801662445 + 50.0 * 8.30620288848877
Epoch 1790, val loss: 0.43142059445381165
Epoch 1800, training loss: 415.55499267578125 = 0.3097555935382843 + 50.0 * 8.30490493774414
Epoch 1800, val loss: 0.43101686239242554
Epoch 1810, training loss: 415.53985595703125 = 0.3084712028503418 + 50.0 * 8.304627418518066
Epoch 1810, val loss: 0.43042632937431335
Epoch 1820, training loss: 415.5761413574219 = 0.3071860373020172 + 50.0 * 8.305378913879395
Epoch 1820, val loss: 0.4298330247402191
Epoch 1830, training loss: 416.1573486328125 = 0.30585527420043945 + 50.0 * 8.31702995300293
Epoch 1830, val loss: 0.4294627010822296
Epoch 1840, training loss: 415.55914306640625 = 0.30444735288619995 + 50.0 * 8.305093765258789
Epoch 1840, val loss: 0.4287295341491699
Epoch 1850, training loss: 415.5129089355469 = 0.3031138777732849 + 50.0 * 8.30419635772705
Epoch 1850, val loss: 0.42847609519958496
Epoch 1860, training loss: 415.4916076660156 = 0.3018200695514679 + 50.0 * 8.30379581451416
Epoch 1860, val loss: 0.42775440216064453
Epoch 1870, training loss: 415.4550476074219 = 0.30053964257240295 + 50.0 * 8.30309009552002
Epoch 1870, val loss: 0.4273685812950134
Epoch 1880, training loss: 415.4383239746094 = 0.29925933480262756 + 50.0 * 8.302781105041504
Epoch 1880, val loss: 0.4268416166305542
Epoch 1890, training loss: 415.5633544921875 = 0.29797106981277466 + 50.0 * 8.305307388305664
Epoch 1890, val loss: 0.42630234360694885
Epoch 1900, training loss: 415.5093688964844 = 0.2966096103191376 + 50.0 * 8.304255485534668
Epoch 1900, val loss: 0.42604780197143555
Epoch 1910, training loss: 415.4859619140625 = 0.2952558398246765 + 50.0 * 8.303813934326172
Epoch 1910, val loss: 0.42536669969558716
Epoch 1920, training loss: 415.4229736328125 = 0.2939334809780121 + 50.0 * 8.302580833435059
Epoch 1920, val loss: 0.42497387528419495
Epoch 1930, training loss: 415.3760986328125 = 0.2926402986049652 + 50.0 * 8.301669120788574
Epoch 1930, val loss: 0.4245118498802185
Epoch 1940, training loss: 415.3856201171875 = 0.2913514971733093 + 50.0 * 8.301885604858398
Epoch 1940, val loss: 0.42400234937667847
Epoch 1950, training loss: 415.63275146484375 = 0.29004842042922974 + 50.0 * 8.306854248046875
Epoch 1950, val loss: 0.42344149947166443
Epoch 1960, training loss: 415.60015869140625 = 0.28871601819992065 + 50.0 * 8.306228637695312
Epoch 1960, val loss: 0.4233234226703644
Epoch 1970, training loss: 415.3943786621094 = 0.28734490275382996 + 50.0 * 8.302140235900879
Epoch 1970, val loss: 0.4227007329463959
Epoch 1980, training loss: 415.3338623046875 = 0.2860203981399536 + 50.0 * 8.300956726074219
Epoch 1980, val loss: 0.4223508834838867
Epoch 1990, training loss: 415.3125915527344 = 0.28471890091896057 + 50.0 * 8.300557136535645
Epoch 1990, val loss: 0.4219810664653778
Epoch 2000, training loss: 415.2984619140625 = 0.2834341526031494 + 50.0 * 8.300300598144531
Epoch 2000, val loss: 0.42158806324005127
Epoch 2010, training loss: 415.5342102050781 = 0.2821478247642517 + 50.0 * 8.305041313171387
Epoch 2010, val loss: 0.42125263810157776
Epoch 2020, training loss: 415.27587890625 = 0.2807965874671936 + 50.0 * 8.299901962280273
Epoch 2020, val loss: 0.4207499921321869
Epoch 2030, training loss: 415.2569885253906 = 0.27947065234184265 + 50.0 * 8.29955005645752
Epoch 2030, val loss: 0.42052698135375977
Epoch 2040, training loss: 415.2433776855469 = 0.27816182374954224 + 50.0 * 8.299304008483887
Epoch 2040, val loss: 0.42005592584609985
Epoch 2050, training loss: 415.3228454589844 = 0.2768622934818268 + 50.0 * 8.300919532775879
Epoch 2050, val loss: 0.41970816254615784
Epoch 2060, training loss: 415.37158203125 = 0.27553242444992065 + 50.0 * 8.301920890808105
Epoch 2060, val loss: 0.4193417429924011
Epoch 2070, training loss: 415.257568359375 = 0.27419450879096985 + 50.0 * 8.299667358398438
Epoch 2070, val loss: 0.41919273138046265
Epoch 2080, training loss: 415.21875 = 0.2728699743747711 + 50.0 * 8.298917770385742
Epoch 2080, val loss: 0.4187658131122589
Epoch 2090, training loss: 415.27764892578125 = 0.2715654969215393 + 50.0 * 8.300121307373047
Epoch 2090, val loss: 0.4187864661216736
Epoch 2100, training loss: 415.2669372558594 = 0.2702351212501526 + 50.0 * 8.299934387207031
Epoch 2100, val loss: 0.418252170085907
Epoch 2110, training loss: 415.2235412597656 = 0.26889902353286743 + 50.0 * 8.299093246459961
Epoch 2110, val loss: 0.41778942942619324
Epoch 2120, training loss: 415.2733459472656 = 0.26757097244262695 + 50.0 * 8.300115585327148
Epoch 2120, val loss: 0.4176737070083618
Epoch 2130, training loss: 415.2062072753906 = 0.266239196062088 + 50.0 * 8.298799514770508
Epoch 2130, val loss: 0.4171845316886902
Epoch 2140, training loss: 415.18267822265625 = 0.264911413192749 + 50.0 * 8.298355102539062
Epoch 2140, val loss: 0.4171466827392578
Epoch 2150, training loss: 415.1354064941406 = 0.2635931968688965 + 50.0 * 8.297436714172363
Epoch 2150, val loss: 0.4167996048927307
Epoch 2160, training loss: 415.25640869140625 = 0.2622869312763214 + 50.0 * 8.299881935119629
Epoch 2160, val loss: 0.41656118631362915
Epoch 2170, training loss: 415.29949951171875 = 0.26094576716423035 + 50.0 * 8.30077075958252
Epoch 2170, val loss: 0.4165644645690918
Epoch 2180, training loss: 415.1001281738281 = 0.25959616899490356 + 50.0 * 8.2968111038208
Epoch 2180, val loss: 0.41616320610046387
Epoch 2190, training loss: 415.0541076660156 = 0.2582738697528839 + 50.0 * 8.295916557312012
Epoch 2190, val loss: 0.41599276661872864
Epoch 2200, training loss: 415.0528869628906 = 0.25697144865989685 + 50.0 * 8.295918464660645
Epoch 2200, val loss: 0.41579940915107727
Epoch 2210, training loss: 415.0917053222656 = 0.2556750476360321 + 50.0 * 8.296720504760742
Epoch 2210, val loss: 0.4155093729496002
Epoch 2220, training loss: 415.3874206542969 = 0.25436678528785706 + 50.0 * 8.302660942077637
Epoch 2220, val loss: 0.41533300280570984
Epoch 2230, training loss: 415.105224609375 = 0.25302571058273315 + 50.0 * 8.297043800354004
Epoch 2230, val loss: 0.41536062955856323
Epoch 2240, training loss: 415.04443359375 = 0.25170964002609253 + 50.0 * 8.295854568481445
Epoch 2240, val loss: 0.41512319445610046
Epoch 2250, training loss: 415.0238952636719 = 0.2504190504550934 + 50.0 * 8.295469284057617
Epoch 2250, val loss: 0.4150543212890625
Epoch 2260, training loss: 415.23046875 = 0.2491302490234375 + 50.0 * 8.299627304077148
Epoch 2260, val loss: 0.41487348079681396
Epoch 2270, training loss: 414.9933166503906 = 0.24780915677547455 + 50.0 * 8.294910430908203
Epoch 2270, val loss: 0.41476625204086304
Epoch 2280, training loss: 415.0166015625 = 0.24651005864143372 + 50.0 * 8.295401573181152
Epoch 2280, val loss: 0.4147438406944275
Epoch 2290, training loss: 415.11065673828125 = 0.24522052705287933 + 50.0 * 8.297308921813965
Epoch 2290, val loss: 0.41469234228134155
Epoch 2300, training loss: 414.9678039550781 = 0.2439265251159668 + 50.0 * 8.294477462768555
Epoch 2300, val loss: 0.414303719997406
Epoch 2310, training loss: 415.0263671875 = 0.24265693128108978 + 50.0 * 8.295674324035645
Epoch 2310, val loss: 0.4140695035457611
Epoch 2320, training loss: 415.0943603515625 = 0.24137599766254425 + 50.0 * 8.297060012817383
Epoch 2320, val loss: 0.4142308235168457
Epoch 2330, training loss: 414.9079895019531 = 0.2400885969400406 + 50.0 * 8.293357849121094
Epoch 2330, val loss: 0.41422438621520996
Epoch 2340, training loss: 414.9344787597656 = 0.23882026970386505 + 50.0 * 8.293912887573242
Epoch 2340, val loss: 0.41434162855148315
Epoch 2350, training loss: 414.9682312011719 = 0.23756210505962372 + 50.0 * 8.2946138381958
Epoch 2350, val loss: 0.4143980145454407
Epoch 2360, training loss: 415.2187805175781 = 0.23627860844135284 + 50.0 * 8.299650192260742
Epoch 2360, val loss: 0.4140118360519409
Epoch 2370, training loss: 414.9234619140625 = 0.23498228192329407 + 50.0 * 8.293769836425781
Epoch 2370, val loss: 0.4140808880329132
Epoch 2380, training loss: 414.8680419921875 = 0.23369508981704712 + 50.0 * 8.292686462402344
Epoch 2380, val loss: 0.4140933156013489
Epoch 2390, training loss: 414.84722900390625 = 0.2324419915676117 + 50.0 * 8.292295455932617
Epoch 2390, val loss: 0.4141228199005127
Epoch 2400, training loss: 414.84405517578125 = 0.23118895292282104 + 50.0 * 8.292257308959961
Epoch 2400, val loss: 0.41402459144592285
Epoch 2410, training loss: 414.9989318847656 = 0.2299441248178482 + 50.0 * 8.295379638671875
Epoch 2410, val loss: 0.41388678550720215
Epoch 2420, training loss: 414.9029846191406 = 0.2286587357521057 + 50.0 * 8.293486595153809
Epoch 2420, val loss: 0.4144468605518341
Epoch 2430, training loss: 414.90386962890625 = 0.22737731039524078 + 50.0 * 8.293529510498047
Epoch 2430, val loss: 0.4141370356082916
Epoch 2440, training loss: 414.93304443359375 = 0.22610236704349518 + 50.0 * 8.29413890838623
Epoch 2440, val loss: 0.4142678678035736
Epoch 2450, training loss: 414.8917541503906 = 0.22483108937740326 + 50.0 * 8.293338775634766
Epoch 2450, val loss: 0.41420984268188477
Epoch 2460, training loss: 414.7974853515625 = 0.223558709025383 + 50.0 * 8.291478157043457
Epoch 2460, val loss: 0.41439518332481384
Epoch 2470, training loss: 414.8175964355469 = 0.22230583429336548 + 50.0 * 8.291905403137207
Epoch 2470, val loss: 0.41437703371047974
Epoch 2480, training loss: 414.9673156738281 = 0.22105194628238678 + 50.0 * 8.294925689697266
Epoch 2480, val loss: 0.41440993547439575
Epoch 2490, training loss: 414.86785888671875 = 0.21978679299354553 + 50.0 * 8.292961120605469
Epoch 2490, val loss: 0.41433078050613403
Epoch 2500, training loss: 414.8397521972656 = 0.2185164988040924 + 50.0 * 8.292425155639648
Epoch 2500, val loss: 0.41458266973495483
Epoch 2510, training loss: 414.9966735839844 = 0.21727077662944794 + 50.0 * 8.295587539672852
Epoch 2510, val loss: 0.41487815976142883
Epoch 2520, training loss: 414.7318420410156 = 0.2159879058599472 + 50.0 * 8.29031753540039
Epoch 2520, val loss: 0.4149450659751892
Epoch 2530, training loss: 414.7408447265625 = 0.21474336087703705 + 50.0 * 8.290521621704102
Epoch 2530, val loss: 0.4150702953338623
Epoch 2540, training loss: 414.7174987792969 = 0.213496133685112 + 50.0 * 8.290080070495605
Epoch 2540, val loss: 0.4152964651584625
Epoch 2550, training loss: 414.6934509277344 = 0.2122606337070465 + 50.0 * 8.289624214172363
Epoch 2550, val loss: 0.4154265820980072
Epoch 2560, training loss: 414.7040100097656 = 0.21103090047836304 + 50.0 * 8.289859771728516
Epoch 2560, val loss: 0.4154937267303467
Epoch 2570, training loss: 415.03277587890625 = 0.2098141312599182 + 50.0 * 8.296459197998047
Epoch 2570, val loss: 0.4154723584651947
Epoch 2580, training loss: 414.9197082519531 = 0.20855650305747986 + 50.0 * 8.294222831726074
Epoch 2580, val loss: 0.41602200269699097
Epoch 2590, training loss: 414.9576110839844 = 0.20730946958065033 + 50.0 * 8.295005798339844
Epoch 2590, val loss: 0.41643479466438293
Epoch 2600, training loss: 414.6959533691406 = 0.20606330037117004 + 50.0 * 8.28979778289795
Epoch 2600, val loss: 0.41630256175994873
Epoch 2610, training loss: 414.64532470703125 = 0.20483599603176117 + 50.0 * 8.288809776306152
Epoch 2610, val loss: 0.4165586531162262
Epoch 2620, training loss: 414.64825439453125 = 0.20362715423107147 + 50.0 * 8.28889274597168
Epoch 2620, val loss: 0.4168124794960022
Epoch 2630, training loss: 414.67742919921875 = 0.20242564380168915 + 50.0 * 8.28950023651123
Epoch 2630, val loss: 0.4168425500392914
Epoch 2640, training loss: 414.88677978515625 = 0.20123279094696045 + 50.0 * 8.293710708618164
Epoch 2640, val loss: 0.41682854294776917
Epoch 2650, training loss: 414.7176513671875 = 0.1999976634979248 + 50.0 * 8.290352821350098
Epoch 2650, val loss: 0.4177727699279785
Epoch 2660, training loss: 414.67169189453125 = 0.1987861692905426 + 50.0 * 8.289458274841309
Epoch 2660, val loss: 0.41759034991264343
Epoch 2670, training loss: 414.68017578125 = 0.19757549464702606 + 50.0 * 8.289651870727539
Epoch 2670, val loss: 0.41813912987709045
Epoch 2680, training loss: 414.6851806640625 = 0.19637461006641388 + 50.0 * 8.289775848388672
Epoch 2680, val loss: 0.41834279894828796
Epoch 2690, training loss: 414.6299133300781 = 0.19517892599105835 + 50.0 * 8.288694381713867
Epoch 2690, val loss: 0.41884544491767883
Epoch 2700, training loss: 414.6437683105469 = 0.19398464262485504 + 50.0 * 8.288995742797852
Epoch 2700, val loss: 0.4192931652069092
Epoch 2710, training loss: 414.5731506347656 = 0.19279062747955322 + 50.0 * 8.287607192993164
Epoch 2710, val loss: 0.4194622039794922
Epoch 2720, training loss: 414.8660888671875 = 0.19162268936634064 + 50.0 * 8.293489456176758
Epoch 2720, val loss: 0.4200301468372345
Epoch 2730, training loss: 414.672607421875 = 0.19040855765342712 + 50.0 * 8.289644241333008
Epoch 2730, val loss: 0.42016157507896423
Epoch 2740, training loss: 414.59832763671875 = 0.18920454382896423 + 50.0 * 8.288182258605957
Epoch 2740, val loss: 0.4203609228134155
Epoch 2750, training loss: 414.532958984375 = 0.18800488114356995 + 50.0 * 8.286898612976074
Epoch 2750, val loss: 0.42080938816070557
Epoch 2760, training loss: 414.5296936035156 = 0.18680928647518158 + 50.0 * 8.286857604980469
Epoch 2760, val loss: 0.42122146487236023
Epoch 2770, training loss: 414.9184875488281 = 0.18563857674598694 + 50.0 * 8.294656753540039
Epoch 2770, val loss: 0.4211886525154114
Epoch 2780, training loss: 414.6624450683594 = 0.1844288855791092 + 50.0 * 8.289560317993164
Epoch 2780, val loss: 0.42212146520614624
Epoch 2790, training loss: 414.5687255859375 = 0.1832273155450821 + 50.0 * 8.287710189819336
Epoch 2790, val loss: 0.42226889729499817
Epoch 2800, training loss: 414.505126953125 = 0.18203625082969666 + 50.0 * 8.28646183013916
Epoch 2800, val loss: 0.42293357849121094
Epoch 2810, training loss: 414.5118408203125 = 0.18085269629955292 + 50.0 * 8.286620140075684
Epoch 2810, val loss: 0.4233483672142029
Epoch 2820, training loss: 414.7038269042969 = 0.17967912554740906 + 50.0 * 8.290482521057129
Epoch 2820, val loss: 0.4238891005516052
Epoch 2830, training loss: 414.6286315917969 = 0.17849183082580566 + 50.0 * 8.289002418518066
Epoch 2830, val loss: 0.4238402545452118
Epoch 2840, training loss: 414.5161437988281 = 0.17730607092380524 + 50.0 * 8.286776542663574
Epoch 2840, val loss: 0.4245910346508026
Epoch 2850, training loss: 414.4712219238281 = 0.17612673342227936 + 50.0 * 8.28590202331543
Epoch 2850, val loss: 0.4250534772872925
Epoch 2860, training loss: 414.4528503417969 = 0.1749601811170578 + 50.0 * 8.285557746887207
Epoch 2860, val loss: 0.425691157579422
Epoch 2870, training loss: 414.52435302734375 = 0.17380167543888092 + 50.0 * 8.28701114654541
Epoch 2870, val loss: 0.4263591766357422
Epoch 2880, training loss: 414.5770263671875 = 0.17262989282608032 + 50.0 * 8.288087844848633
Epoch 2880, val loss: 0.42641040682792664
Epoch 2890, training loss: 414.55712890625 = 0.1714733988046646 + 50.0 * 8.287713050842285
Epoch 2890, val loss: 0.42688924074172974
Epoch 2900, training loss: 414.4556579589844 = 0.17031735181808472 + 50.0 * 8.285706520080566
Epoch 2900, val loss: 0.4277059733867645
Epoch 2910, training loss: 414.41851806640625 = 0.16916869580745697 + 50.0 * 8.284987449645996
Epoch 2910, val loss: 0.4280094504356384
Epoch 2920, training loss: 414.4450988769531 = 0.16802969574928284 + 50.0 * 8.285541534423828
Epoch 2920, val loss: 0.4288388788700104
Epoch 2930, training loss: 414.5879821777344 = 0.16690200567245483 + 50.0 * 8.288421630859375
Epoch 2930, val loss: 0.42927682399749756
Epoch 2940, training loss: 414.4884948730469 = 0.1657635122537613 + 50.0 * 8.286454200744629
Epoch 2940, val loss: 0.42970871925354004
Epoch 2950, training loss: 414.4163818359375 = 0.16462405025959015 + 50.0 * 8.285035133361816
Epoch 2950, val loss: 0.43050992488861084
Epoch 2960, training loss: 414.48211669921875 = 0.16349883377552032 + 50.0 * 8.286372184753418
Epoch 2960, val loss: 0.4312700927257538
Epoch 2970, training loss: 414.49029541015625 = 0.16237609088420868 + 50.0 * 8.286558151245117
Epoch 2970, val loss: 0.4319009780883789
Epoch 2980, training loss: 414.38372802734375 = 0.1612486094236374 + 50.0 * 8.284449577331543
Epoch 2980, val loss: 0.43234625458717346
Epoch 2990, training loss: 414.35357666015625 = 0.16013029217720032 + 50.0 * 8.283868789672852
Epoch 2990, val loss: 0.433164119720459
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8513444951801116
0.8668405419111788
=== training gcn model ===
Epoch 0, training loss: 530.2196044921875 = 1.106372356414795 + 50.0 * 10.582263946533203
Epoch 0, val loss: 1.1055355072021484
Epoch 10, training loss: 530.1644287109375 = 1.0991758108139038 + 50.0 * 10.581305503845215
Epoch 10, val loss: 1.0983084440231323
Epoch 20, training loss: 529.7271118164062 = 1.090528130531311 + 50.0 * 10.572731971740723
Epoch 20, val loss: 1.0896345376968384
Epoch 30, training loss: 526.7051391601562 = 1.0802098512649536 + 50.0 * 10.51249885559082
Epoch 30, val loss: 1.079241394996643
Epoch 40, training loss: 515.3740844726562 = 1.0689963102340698 + 50.0 * 10.286102294921875
Epoch 40, val loss: 1.068303108215332
Epoch 50, training loss: 496.5931701660156 = 1.0596634149551392 + 50.0 * 9.910670280456543
Epoch 50, val loss: 1.0590717792510986
Epoch 60, training loss: 480.2279968261719 = 1.052289366722107 + 50.0 * 9.583514213562012
Epoch 60, val loss: 1.051902413368225
Epoch 70, training loss: 468.1539306640625 = 1.044238805770874 + 50.0 * 9.342193603515625
Epoch 70, val loss: 1.0439339876174927
Epoch 80, training loss: 460.7975769042969 = 1.0359605550765991 + 50.0 * 9.195232391357422
Epoch 80, val loss: 1.0358057022094727
Epoch 90, training loss: 458.50262451171875 = 1.027439832687378 + 50.0 * 9.149503707885742
Epoch 90, val loss: 1.0275388956069946
Epoch 100, training loss: 456.17425537109375 = 1.019225835800171 + 50.0 * 9.103100776672363
Epoch 100, val loss: 1.0197334289550781
Epoch 110, training loss: 452.16522216796875 = 1.0121912956237793 + 50.0 * 9.02306079864502
Epoch 110, val loss: 1.0131456851959229
Epoch 120, training loss: 446.2587585449219 = 1.0066335201263428 + 50.0 * 8.90504264831543
Epoch 120, val loss: 1.0078986883163452
Epoch 130, training loss: 441.40185546875 = 1.001430630683899 + 50.0 * 8.808008193969727
Epoch 130, val loss: 1.0027247667312622
Epoch 140, training loss: 439.27984619140625 = 0.994099497795105 + 50.0 * 8.765714645385742
Epoch 140, val loss: 0.995305597782135
Epoch 150, training loss: 436.63861083984375 = 0.9845812320709229 + 50.0 * 8.713080406188965
Epoch 150, val loss: 0.9860401749610901
Epoch 160, training loss: 434.5185241699219 = 0.9754633903503418 + 50.0 * 8.67086124420166
Epoch 160, val loss: 0.977328360080719
Epoch 170, training loss: 433.0892333984375 = 0.9660628437995911 + 50.0 * 8.642463684082031
Epoch 170, val loss: 0.9682255387306213
Epoch 180, training loss: 431.65875244140625 = 0.9556338787078857 + 50.0 * 8.614062309265137
Epoch 180, val loss: 0.9581324458122253
Epoch 190, training loss: 430.7464294433594 = 0.9444892406463623 + 50.0 * 8.596038818359375
Epoch 190, val loss: 0.9473260045051575
Epoch 200, training loss: 430.0754089355469 = 0.9323112368583679 + 50.0 * 8.58286190032959
Epoch 200, val loss: 0.9355376958847046
Epoch 210, training loss: 429.6673583984375 = 0.9189527630805969 + 50.0 * 8.574968338012695
Epoch 210, val loss: 0.9225723147392273
Epoch 220, training loss: 428.90570068359375 = 0.904641330242157 + 50.0 * 8.56002140045166
Epoch 220, val loss: 0.9088636040687561
Epoch 230, training loss: 428.2286376953125 = 0.8898298144340515 + 50.0 * 8.546775817871094
Epoch 230, val loss: 0.8946642875671387
Epoch 240, training loss: 427.873779296875 = 0.8745569586753845 + 50.0 * 8.539984703063965
Epoch 240, val loss: 0.8801211714744568
Epoch 250, training loss: 426.9071960449219 = 0.858725368976593 + 50.0 * 8.52096939086914
Epoch 250, val loss: 0.8648846745491028
Epoch 260, training loss: 426.2445373535156 = 0.8425565361976624 + 50.0 * 8.508039474487305
Epoch 260, val loss: 0.8494017124176025
Epoch 270, training loss: 425.6614074707031 = 0.8260110020637512 + 50.0 * 8.496707916259766
Epoch 270, val loss: 0.8336806893348694
Epoch 280, training loss: 425.3709716796875 = 0.8089931011199951 + 50.0 * 8.491239547729492
Epoch 280, val loss: 0.8174471259117126
Epoch 290, training loss: 424.72393798828125 = 0.7915451526641846 + 50.0 * 8.47864818572998
Epoch 290, val loss: 0.8009066581726074
Epoch 300, training loss: 424.4185485839844 = 0.7740430235862732 + 50.0 * 8.47288990020752
Epoch 300, val loss: 0.7843081951141357
Epoch 310, training loss: 423.97808837890625 = 0.7562354207038879 + 50.0 * 8.464437484741211
Epoch 310, val loss: 0.7675449252128601
Epoch 320, training loss: 423.5974426269531 = 0.7386777400970459 + 50.0 * 8.457175254821777
Epoch 320, val loss: 0.7510267496109009
Epoch 330, training loss: 423.2670593261719 = 0.7214946746826172 + 50.0 * 8.450911521911621
Epoch 330, val loss: 0.7349476218223572
Epoch 340, training loss: 423.3583679199219 = 0.7046689987182617 + 50.0 * 8.453073501586914
Epoch 340, val loss: 0.7191650867462158
Epoch 350, training loss: 422.7518310546875 = 0.6880603432655334 + 50.0 * 8.441275596618652
Epoch 350, val loss: 0.703805148601532
Epoch 360, training loss: 422.4416809082031 = 0.6722362637519836 + 50.0 * 8.435388565063477
Epoch 360, val loss: 0.6892123222351074
Epoch 370, training loss: 422.17193603515625 = 0.6571263670921326 + 50.0 * 8.430295944213867
Epoch 370, val loss: 0.6753472089767456
Epoch 380, training loss: 422.02374267578125 = 0.642470121383667 + 50.0 * 8.42762565612793
Epoch 380, val loss: 0.6619117856025696
Epoch 390, training loss: 421.8414001464844 = 0.6283865571022034 + 50.0 * 8.424260139465332
Epoch 390, val loss: 0.6490899324417114
Epoch 400, training loss: 421.5755920410156 = 0.6152995228767395 + 50.0 * 8.419205665588379
Epoch 400, val loss: 0.6373242139816284
Epoch 410, training loss: 421.3793029785156 = 0.6030237674713135 + 50.0 * 8.415525436401367
Epoch 410, val loss: 0.626365065574646
Epoch 420, training loss: 421.2024230957031 = 0.5914536118507385 + 50.0 * 8.412219047546387
Epoch 420, val loss: 0.6160749793052673
Epoch 430, training loss: 421.0536804199219 = 0.5806020498275757 + 50.0 * 8.409461975097656
Epoch 430, val loss: 0.6065024137496948
Epoch 440, training loss: 421.0038146972656 = 0.570356547832489 + 50.0 * 8.408669471740723
Epoch 440, val loss: 0.5975093841552734
Epoch 450, training loss: 420.8236999511719 = 0.5608001947402954 + 50.0 * 8.405258178710938
Epoch 450, val loss: 0.5892335772514343
Epoch 460, training loss: 420.6278076171875 = 0.552091121673584 + 50.0 * 8.401514053344727
Epoch 460, val loss: 0.5817837715148926
Epoch 470, training loss: 420.52606201171875 = 0.5440660715103149 + 50.0 * 8.399640083312988
Epoch 470, val loss: 0.5749490261077881
Epoch 480, training loss: 420.5299377441406 = 0.5365114212036133 + 50.0 * 8.399868965148926
Epoch 480, val loss: 0.5685425400733948
Epoch 490, training loss: 420.3829650878906 = 0.5294662714004517 + 50.0 * 8.397069931030273
Epoch 490, val loss: 0.562637209892273
Epoch 500, training loss: 420.2032165527344 = 0.5230509638786316 + 50.0 * 8.393603324890137
Epoch 500, val loss: 0.5573490262031555
Epoch 510, training loss: 420.06744384765625 = 0.5171298384666443 + 50.0 * 8.391006469726562
Epoch 510, val loss: 0.5525621771812439
Epoch 520, training loss: 420.0009460449219 = 0.5116652250289917 + 50.0 * 8.389785766601562
Epoch 520, val loss: 0.5480921864509583
Epoch 530, training loss: 419.9483642578125 = 0.5064826011657715 + 50.0 * 8.388837814331055
Epoch 530, val loss: 0.5440081357955933
Epoch 540, training loss: 419.8307189941406 = 0.5017087459564209 + 50.0 * 8.386580467224121
Epoch 540, val loss: 0.540122926235199
Epoch 550, training loss: 419.6729736328125 = 0.4973468482494354 + 50.0 * 8.383512496948242
Epoch 550, val loss: 0.5367228388786316
Epoch 560, training loss: 419.6338195800781 = 0.49329885840415955 + 50.0 * 8.382810592651367
Epoch 560, val loss: 0.5335355401039124
Epoch 570, training loss: 419.6175842285156 = 0.4894242286682129 + 50.0 * 8.382563591003418
Epoch 570, val loss: 0.5306180715560913
Epoch 580, training loss: 419.5357666015625 = 0.48578065633773804 + 50.0 * 8.380999565124512
Epoch 580, val loss: 0.5276800394058228
Epoch 590, training loss: 419.3602600097656 = 0.4824122190475464 + 50.0 * 8.377556800842285
Epoch 590, val loss: 0.5252269506454468
Epoch 600, training loss: 419.26470947265625 = 0.4792771339416504 + 50.0 * 8.37570858001709
Epoch 600, val loss: 0.5229548215866089
Epoch 610, training loss: 419.2848205566406 = 0.476328581571579 + 50.0 * 8.37617015838623
Epoch 610, val loss: 0.5207380056381226
Epoch 620, training loss: 419.07220458984375 = 0.4734537601470947 + 50.0 * 8.37197494506836
Epoch 620, val loss: 0.518602728843689
Epoch 630, training loss: 419.0196228027344 = 0.4707750380039215 + 50.0 * 8.370977401733398
Epoch 630, val loss: 0.516608476638794
Epoch 640, training loss: 418.9412536621094 = 0.4682786166667938 + 50.0 * 8.36945915222168
Epoch 640, val loss: 0.5148574113845825
Epoch 650, training loss: 419.1327819824219 = 0.46587783098220825 + 50.0 * 8.373337745666504
Epoch 650, val loss: 0.5131317377090454
Epoch 660, training loss: 418.8738098144531 = 0.4635085165500641 + 50.0 * 8.368206024169922
Epoch 660, val loss: 0.5113714933395386
Epoch 670, training loss: 418.7178039550781 = 0.4612797796726227 + 50.0 * 8.365130424499512
Epoch 670, val loss: 0.5097675919532776
Epoch 680, training loss: 418.6205139160156 = 0.4591733515262604 + 50.0 * 8.363226890563965
Epoch 680, val loss: 0.5082976818084717
Epoch 690, training loss: 418.8032531738281 = 0.457109659910202 + 50.0 * 8.366922378540039
Epoch 690, val loss: 0.506903886795044
Epoch 700, training loss: 418.5958251953125 = 0.4549948275089264 + 50.0 * 8.36281681060791
Epoch 700, val loss: 0.5051952004432678
Epoch 710, training loss: 418.4549255371094 = 0.45302367210388184 + 50.0 * 8.360037803649902
Epoch 710, val loss: 0.5038368105888367
Epoch 720, training loss: 418.366455078125 = 0.45118674635887146 + 50.0 * 8.358305931091309
Epoch 720, val loss: 0.5025191903114319
Epoch 730, training loss: 418.2826843261719 = 0.4494149386882782 + 50.0 * 8.35666561126709
Epoch 730, val loss: 0.5012661814689636
Epoch 740, training loss: 418.2557067871094 = 0.4476781487464905 + 50.0 * 8.356160163879395
Epoch 740, val loss: 0.500084400177002
Epoch 750, training loss: 418.6138610839844 = 0.4459312856197357 + 50.0 * 8.363358497619629
Epoch 750, val loss: 0.4989500939846039
Epoch 760, training loss: 418.3166198730469 = 0.44416743516921997 + 50.0 * 8.357449531555176
Epoch 760, val loss: 0.4973680377006531
Epoch 770, training loss: 418.0957946777344 = 0.4424806535243988 + 50.0 * 8.353066444396973
Epoch 770, val loss: 0.4961782693862915
Epoch 780, training loss: 418.0599670410156 = 0.44087857007980347 + 50.0 * 8.352381706237793
Epoch 780, val loss: 0.4951625168323517
Epoch 790, training loss: 418.0249938964844 = 0.4393254220485687 + 50.0 * 8.351713180541992
Epoch 790, val loss: 0.493941068649292
Epoch 800, training loss: 418.07501220703125 = 0.43775475025177 + 50.0 * 8.352745056152344
Epoch 800, val loss: 0.4928753077983856
Epoch 810, training loss: 417.8752746582031 = 0.4361996054649353 + 50.0 * 8.34878158569336
Epoch 810, val loss: 0.49174338579177856
Epoch 820, training loss: 417.81768798828125 = 0.43472516536712646 + 50.0 * 8.34765911102295
Epoch 820, val loss: 0.49068745970726013
Epoch 830, training loss: 418.0207824707031 = 0.43327027559280396 + 50.0 * 8.351750373840332
Epoch 830, val loss: 0.4896564781665802
Epoch 840, training loss: 417.8764343261719 = 0.43177181482315063 + 50.0 * 8.348893165588379
Epoch 840, val loss: 0.4885020852088928
Epoch 850, training loss: 417.6969299316406 = 0.4303399324417114 + 50.0 * 8.345332145690918
Epoch 850, val loss: 0.48755598068237305
Epoch 860, training loss: 417.6523742675781 = 0.4289674758911133 + 50.0 * 8.344468116760254
Epoch 860, val loss: 0.4865988790988922
Epoch 870, training loss: 418.19091796875 = 0.42756155133247375 + 50.0 * 8.355267524719238
Epoch 870, val loss: 0.4856798052787781
Epoch 880, training loss: 417.5997314453125 = 0.4261089265346527 + 50.0 * 8.343472480773926
Epoch 880, val loss: 0.48451095819473267
Epoch 890, training loss: 417.54522705078125 = 0.4247705638408661 + 50.0 * 8.342409133911133
Epoch 890, val loss: 0.48352155089378357
Epoch 900, training loss: 417.4519348144531 = 0.42346715927124023 + 50.0 * 8.340569496154785
Epoch 900, val loss: 0.48265260457992554
Epoch 910, training loss: 417.4270935058594 = 0.42217734456062317 + 50.0 * 8.34009838104248
Epoch 910, val loss: 0.48186615109443665
Epoch 920, training loss: 417.8139343261719 = 0.42087045311927795 + 50.0 * 8.347861289978027
Epoch 920, val loss: 0.4811303913593292
Epoch 930, training loss: 417.41461181640625 = 0.41951486468315125 + 50.0 * 8.3399019241333
Epoch 930, val loss: 0.4798780083656311
Epoch 940, training loss: 417.3230895996094 = 0.41823768615722656 + 50.0 * 8.338096618652344
Epoch 940, val loss: 0.4789671003818512
Epoch 950, training loss: 417.45623779296875 = 0.4169902801513672 + 50.0 * 8.340785026550293
Epoch 950, val loss: 0.4782118499279022
Epoch 960, training loss: 417.25714111328125 = 0.41568562388420105 + 50.0 * 8.33682918548584
Epoch 960, val loss: 0.4773493707180023
Epoch 970, training loss: 417.19451904296875 = 0.41443511843681335 + 50.0 * 8.335601806640625
Epoch 970, val loss: 0.4763329029083252
Epoch 980, training loss: 417.1512451171875 = 0.41322073340415955 + 50.0 * 8.334760665893555
Epoch 980, val loss: 0.475577175617218
Epoch 990, training loss: 417.3412170410156 = 0.4120196998119354 + 50.0 * 8.338583946228027
Epoch 990, val loss: 0.4747757911682129
Epoch 1000, training loss: 417.1690368652344 = 0.410748690366745 + 50.0 * 8.335165977478027
Epoch 1000, val loss: 0.4738912284374237
Epoch 1010, training loss: 417.1159973144531 = 0.40952742099761963 + 50.0 * 8.334129333496094
Epoch 1010, val loss: 0.47307950258255005
Epoch 1020, training loss: 417.3679504394531 = 0.4083233177661896 + 50.0 * 8.339192390441895
Epoch 1020, val loss: 0.4724220931529999
Epoch 1030, training loss: 417.0689697265625 = 0.40707626938819885 + 50.0 * 8.333237648010254
Epoch 1030, val loss: 0.47134536504745483
Epoch 1040, training loss: 416.96514892578125 = 0.40587931871414185 + 50.0 * 8.331185340881348
Epoch 1040, val loss: 0.4706651568412781
Epoch 1050, training loss: 416.8927001953125 = 0.4047211706638336 + 50.0 * 8.32975959777832
Epoch 1050, val loss: 0.4698936343193054
Epoch 1060, training loss: 416.8531188964844 = 0.4035607576370239 + 50.0 * 8.328990936279297
Epoch 1060, val loss: 0.4691851735115051
Epoch 1070, training loss: 417.3059997558594 = 0.4023860991001129 + 50.0 * 8.338072776794434
Epoch 1070, val loss: 0.4684367775917053
Epoch 1080, training loss: 417.00274658203125 = 0.40111616253852844 + 50.0 * 8.332032203674316
Epoch 1080, val loss: 0.46743059158325195
Epoch 1090, training loss: 416.8152160644531 = 0.3998914957046509 + 50.0 * 8.328306198120117
Epoch 1090, val loss: 0.4667757451534271
Epoch 1100, training loss: 416.7313232421875 = 0.39873600006103516 + 50.0 * 8.326651573181152
Epoch 1100, val loss: 0.4660361707210541
Epoch 1110, training loss: 416.7494201660156 = 0.3975851833820343 + 50.0 * 8.32703685760498
Epoch 1110, val loss: 0.46535956859588623
Epoch 1120, training loss: 417.0235595703125 = 0.3963945209980011 + 50.0 * 8.33254337310791
Epoch 1120, val loss: 0.46470990777015686
Epoch 1130, training loss: 416.78912353515625 = 0.39518657326698303 + 50.0 * 8.327878952026367
Epoch 1130, val loss: 0.4637118875980377
Epoch 1140, training loss: 416.6781005859375 = 0.39400535821914673 + 50.0 * 8.325681686401367
Epoch 1140, val loss: 0.4629971981048584
Epoch 1150, training loss: 416.8110656738281 = 0.3928428292274475 + 50.0 * 8.328364372253418
Epoch 1150, val loss: 0.46208155155181885
Epoch 1160, training loss: 416.5854187011719 = 0.39162832498550415 + 50.0 * 8.323875427246094
Epoch 1160, val loss: 0.4615248143672943
Epoch 1170, training loss: 416.6135559082031 = 0.3904695510864258 + 50.0 * 8.324461936950684
Epoch 1170, val loss: 0.46077337861061096
Epoch 1180, training loss: 416.6940612792969 = 0.3892671465873718 + 50.0 * 8.326095581054688
Epoch 1180, val loss: 0.45985639095306396
Epoch 1190, training loss: 416.5369873046875 = 0.3880314230918884 + 50.0 * 8.322978973388672
Epoch 1190, val loss: 0.45927226543426514
Epoch 1200, training loss: 416.4775085449219 = 0.3868606686592102 + 50.0 * 8.321812629699707
Epoch 1200, val loss: 0.45853081345558167
Epoch 1210, training loss: 416.4537048339844 = 0.3857019543647766 + 50.0 * 8.321359634399414
Epoch 1210, val loss: 0.45784372091293335
Epoch 1220, training loss: 416.7328796386719 = 0.3845199942588806 + 50.0 * 8.326967239379883
Epoch 1220, val loss: 0.45719704031944275
Epoch 1230, training loss: 416.40069580078125 = 0.3832950294017792 + 50.0 * 8.320347785949707
Epoch 1230, val loss: 0.4562988877296448
Epoch 1240, training loss: 416.3424377441406 = 0.3821088373661041 + 50.0 * 8.319206237792969
Epoch 1240, val loss: 0.45557713508605957
Epoch 1250, training loss: 416.3800354003906 = 0.3809419274330139 + 50.0 * 8.319981575012207
Epoch 1250, val loss: 0.45477303862571716
Epoch 1260, training loss: 416.5447082519531 = 0.37974056601524353 + 50.0 * 8.323299407958984
Epoch 1260, val loss: 0.4540482759475708
Epoch 1270, training loss: 416.4004821777344 = 0.37849390506744385 + 50.0 * 8.320440292358398
Epoch 1270, val loss: 0.45354345440864563
Epoch 1280, training loss: 416.3324279785156 = 0.37726935744285583 + 50.0 * 8.319103240966797
Epoch 1280, val loss: 0.4527246952056885
Epoch 1290, training loss: 416.311767578125 = 0.3760643005371094 + 50.0 * 8.318714141845703
Epoch 1290, val loss: 0.45187708735466003
Epoch 1300, training loss: 416.2847595214844 = 0.3748578429222107 + 50.0 * 8.318198204040527
Epoch 1300, val loss: 0.45125043392181396
Epoch 1310, training loss: 416.3067932128906 = 0.37364661693573 + 50.0 * 8.318662643432617
Epoch 1310, val loss: 0.45058444142341614
Epoch 1320, training loss: 416.3292236328125 = 0.3723985254764557 + 50.0 * 8.319136619567871
Epoch 1320, val loss: 0.4497682750225067
Epoch 1330, training loss: 416.192138671875 = 0.3711635172367096 + 50.0 * 8.31641960144043
Epoch 1330, val loss: 0.44895583391189575
Epoch 1340, training loss: 416.1886291503906 = 0.36994388699531555 + 50.0 * 8.316373825073242
Epoch 1340, val loss: 0.44833219051361084
Epoch 1350, training loss: 416.3079528808594 = 0.3687232732772827 + 50.0 * 8.318784713745117
Epoch 1350, val loss: 0.4475735127925873
Epoch 1360, training loss: 416.0981750488281 = 0.3674541711807251 + 50.0 * 8.314614295959473
Epoch 1360, val loss: 0.4468524158000946
Epoch 1370, training loss: 416.10235595703125 = 0.36621329188346863 + 50.0 * 8.314723014831543
Epoch 1370, val loss: 0.44618621468544006
Epoch 1380, training loss: 416.2420959472656 = 0.3649625778198242 + 50.0 * 8.317543029785156
Epoch 1380, val loss: 0.44555893540382385
Epoch 1390, training loss: 416.0409851074219 = 0.36369287967681885 + 50.0 * 8.313546180725098
Epoch 1390, val loss: 0.4446612596511841
Epoch 1400, training loss: 416.1195983886719 = 0.36245274543762207 + 50.0 * 8.315142631530762
Epoch 1400, val loss: 0.44380584359169006
Epoch 1410, training loss: 416.1546936035156 = 0.3611597418785095 + 50.0 * 8.315871238708496
Epoch 1410, val loss: 0.44324883818626404
Epoch 1420, training loss: 416.01409912109375 = 0.359876424074173 + 50.0 * 8.313084602355957
Epoch 1420, val loss: 0.44251829385757446
Epoch 1430, training loss: 415.9499816894531 = 0.358620285987854 + 50.0 * 8.311827659606934
Epoch 1430, val loss: 0.44185373187065125
Epoch 1440, training loss: 416.0169677734375 = 0.3573673665523529 + 50.0 * 8.313192367553711
Epoch 1440, val loss: 0.44136643409729004
Epoch 1450, training loss: 416.1421203613281 = 0.35607483983039856 + 50.0 * 8.315720558166504
Epoch 1450, val loss: 0.44060173630714417
Epoch 1460, training loss: 416.05609130859375 = 0.3547487258911133 + 50.0 * 8.314026832580566
Epoch 1460, val loss: 0.43981486558914185
Epoch 1470, training loss: 415.9241943359375 = 0.35344794392585754 + 50.0 * 8.31141471862793
Epoch 1470, val loss: 0.439010888338089
Epoch 1480, training loss: 415.888916015625 = 0.35218510031700134 + 50.0 * 8.310734748840332
Epoch 1480, val loss: 0.43846145272254944
Epoch 1490, training loss: 415.920654296875 = 0.35093775391578674 + 50.0 * 8.311393737792969
Epoch 1490, val loss: 0.43770352005958557
Epoch 1500, training loss: 416.0263671875 = 0.3496522009372711 + 50.0 * 8.3135347366333
Epoch 1500, val loss: 0.43711066246032715
Epoch 1510, training loss: 415.9258117675781 = 0.34834399819374084 + 50.0 * 8.311549186706543
Epoch 1510, val loss: 0.4364907145500183
Epoch 1520, training loss: 415.80596923828125 = 0.34705251455307007 + 50.0 * 8.309178352355957
Epoch 1520, val loss: 0.43582814931869507
Epoch 1530, training loss: 415.8003845214844 = 0.34578120708465576 + 50.0 * 8.309091567993164
Epoch 1530, val loss: 0.4352320730686188
Epoch 1540, training loss: 415.8529357910156 = 0.3445185422897339 + 50.0 * 8.310168266296387
Epoch 1540, val loss: 0.43456223607063293
Epoch 1550, training loss: 416.0138244628906 = 0.34321463108062744 + 50.0 * 8.3134126663208
Epoch 1550, val loss: 0.4339931607246399
Epoch 1560, training loss: 415.8095397949219 = 0.34187349677085876 + 50.0 * 8.30935287475586
Epoch 1560, val loss: 0.43351733684539795
Epoch 1570, training loss: 415.733642578125 = 0.34058690071105957 + 50.0 * 8.307861328125
Epoch 1570, val loss: 0.4328562617301941
Epoch 1580, training loss: 415.7744140625 = 0.33931443095207214 + 50.0 * 8.308701515197754
Epoch 1580, val loss: 0.43248769640922546
Epoch 1590, training loss: 415.81219482421875 = 0.3380189538002014 + 50.0 * 8.309483528137207
Epoch 1590, val loss: 0.43184396624565125
Epoch 1600, training loss: 415.7841491699219 = 0.3367220163345337 + 50.0 * 8.308948516845703
Epoch 1600, val loss: 0.4310828447341919
Epoch 1610, training loss: 415.73016357421875 = 0.335428386926651 + 50.0 * 8.307894706726074
Epoch 1610, val loss: 0.43061062693595886
Epoch 1620, training loss: 415.80316162109375 = 0.3341365456581116 + 50.0 * 8.309380531311035
Epoch 1620, val loss: 0.4300174415111542
Epoch 1630, training loss: 415.7016296386719 = 0.33283087611198425 + 50.0 * 8.30737590789795
Epoch 1630, val loss: 0.42956721782684326
Epoch 1640, training loss: 415.61541748046875 = 0.3315573036670685 + 50.0 * 8.30567741394043
Epoch 1640, val loss: 0.4289548695087433
Epoch 1650, training loss: 415.6488342285156 = 0.330303817987442 + 50.0 * 8.306370735168457
Epoch 1650, val loss: 0.42833396792411804
Epoch 1660, training loss: 415.79998779296875 = 0.32903215289115906 + 50.0 * 8.309418678283691
Epoch 1660, val loss: 0.4278387129306793
Epoch 1670, training loss: 415.59027099609375 = 0.32771652936935425 + 50.0 * 8.305251121520996
Epoch 1670, val loss: 0.42736172676086426
Epoch 1680, training loss: 415.67803955078125 = 0.32643598318099976 + 50.0 * 8.307031631469727
Epoch 1680, val loss: 0.4268530607223511
Epoch 1690, training loss: 415.7247619628906 = 0.3251565992832184 + 50.0 * 8.307991981506348
Epoch 1690, val loss: 0.42631492018699646
Epoch 1700, training loss: 415.60919189453125 = 0.32386478781700134 + 50.0 * 8.305706977844238
Epoch 1700, val loss: 0.42602285742759705
Epoch 1710, training loss: 415.57342529296875 = 0.32259443402290344 + 50.0 * 8.30501651763916
Epoch 1710, val loss: 0.4253460466861725
Epoch 1720, training loss: 415.55828857421875 = 0.32134318351745605 + 50.0 * 8.304738998413086
Epoch 1720, val loss: 0.4247756004333496
Epoch 1730, training loss: 415.6644592285156 = 0.32008329033851624 + 50.0 * 8.30688762664795
Epoch 1730, val loss: 0.4241917133331299
Epoch 1740, training loss: 415.53118896484375 = 0.3187832832336426 + 50.0 * 8.304247856140137
Epoch 1740, val loss: 0.4240501821041107
Epoch 1750, training loss: 415.531982421875 = 0.3175186514854431 + 50.0 * 8.304289817810059
Epoch 1750, val loss: 0.42347466945648193
Epoch 1760, training loss: 415.6919250488281 = 0.31624385714530945 + 50.0 * 8.307513236999512
Epoch 1760, val loss: 0.4231536090373993
Epoch 1770, training loss: 415.455078125 = 0.3149470090866089 + 50.0 * 8.302803039550781
Epoch 1770, val loss: 0.4226543605327606
Epoch 1780, training loss: 415.4486389160156 = 0.3136882185935974 + 50.0 * 8.302699089050293
Epoch 1780, val loss: 0.42218783497810364
Epoch 1790, training loss: 415.5120544433594 = 0.3124408721923828 + 50.0 * 8.30399227142334
Epoch 1790, val loss: 0.4220297634601593
Epoch 1800, training loss: 415.5190124511719 = 0.3111613094806671 + 50.0 * 8.304157257080078
Epoch 1800, val loss: 0.4214102625846863
Epoch 1810, training loss: 415.4697265625 = 0.3098905682563782 + 50.0 * 8.303196907043457
Epoch 1810, val loss: 0.42082148790359497
Epoch 1820, training loss: 415.5105285644531 = 0.3086235523223877 + 50.0 * 8.304038047790527
Epoch 1820, val loss: 0.42043206095695496
Epoch 1830, training loss: 415.38067626953125 = 0.3073584735393524 + 50.0 * 8.30146598815918
Epoch 1830, val loss: 0.4200151562690735
Epoch 1840, training loss: 415.5243835449219 = 0.30610430240631104 + 50.0 * 8.304366111755371
Epoch 1840, val loss: 0.41983628273010254
Epoch 1850, training loss: 415.4703674316406 = 0.3048235774040222 + 50.0 * 8.303311347961426
Epoch 1850, val loss: 0.41915345191955566
Epoch 1860, training loss: 415.3766784667969 = 0.3035426139831543 + 50.0 * 8.30146312713623
Epoch 1860, val loss: 0.4190029203891754
Epoch 1870, training loss: 415.3306579589844 = 0.3022845983505249 + 50.0 * 8.300567626953125
Epoch 1870, val loss: 0.41849783062934875
Epoch 1880, training loss: 415.368408203125 = 0.30104365944862366 + 50.0 * 8.301346778869629
Epoch 1880, val loss: 0.4181627929210663
Epoch 1890, training loss: 415.4181213378906 = 0.29979172348976135 + 50.0 * 8.302366256713867
Epoch 1890, val loss: 0.41793137788772583
Epoch 1900, training loss: 415.4844970703125 = 0.2985230088233948 + 50.0 * 8.303719520568848
Epoch 1900, val loss: 0.4173951745033264
Epoch 1910, training loss: 415.3148498535156 = 0.2972537577152252 + 50.0 * 8.300352096557617
Epoch 1910, val loss: 0.41697531938552856
Epoch 1920, training loss: 415.2679138183594 = 0.29600366950035095 + 50.0 * 8.2994384765625
Epoch 1920, val loss: 0.41667747497558594
Epoch 1930, training loss: 415.38629150390625 = 0.2947612404823303 + 50.0 * 8.301830291748047
Epoch 1930, val loss: 0.4163292944431305
Epoch 1940, training loss: 415.2796325683594 = 0.293491005897522 + 50.0 * 8.299722671508789
Epoch 1940, val loss: 0.41595393419265747
Epoch 1950, training loss: 415.2869873046875 = 0.2922230362892151 + 50.0 * 8.299895286560059
Epoch 1950, val loss: 0.41569235920906067
Epoch 1960, training loss: 415.413330078125 = 0.2909567654132843 + 50.0 * 8.302447319030762
Epoch 1960, val loss: 0.41547760367393494
Epoch 1970, training loss: 415.2071228027344 = 0.28968989849090576 + 50.0 * 8.298348426818848
Epoch 1970, val loss: 0.4151093363761902
Epoch 1980, training loss: 415.2073974609375 = 0.28845128417015076 + 50.0 * 8.298378944396973
Epoch 1980, val loss: 0.4148552417755127
Epoch 1990, training loss: 415.2136535644531 = 0.28722110390663147 + 50.0 * 8.298528671264648
Epoch 1990, val loss: 0.41467511653900146
Epoch 2000, training loss: 415.3741455078125 = 0.2859867215156555 + 50.0 * 8.301763534545898
Epoch 2000, val loss: 0.41455015540122986
Epoch 2010, training loss: 415.3585205078125 = 0.28473055362701416 + 50.0 * 8.301475524902344
Epoch 2010, val loss: 0.4139692783355713
Epoch 2020, training loss: 415.24005126953125 = 0.2834675908088684 + 50.0 * 8.299131393432617
Epoch 2020, val loss: 0.41359609365463257
Epoch 2030, training loss: 415.1868591308594 = 0.282236248254776 + 50.0 * 8.29809284210205
Epoch 2030, val loss: 0.41323181986808777
Epoch 2040, training loss: 415.57659912109375 = 0.2810065746307373 + 50.0 * 8.305912017822266
Epoch 2040, val loss: 0.41298171877861023
Epoch 2050, training loss: 415.2194519042969 = 0.27971699833869934 + 50.0 * 8.298794746398926
Epoch 2050, val loss: 0.41304656863212585
Epoch 2060, training loss: 415.1412353515625 = 0.27848634123802185 + 50.0 * 8.297255516052246
Epoch 2060, val loss: 0.412627637386322
Epoch 2070, training loss: 415.0928039550781 = 0.2772737145423889 + 50.0 * 8.296310424804688
Epoch 2070, val loss: 0.4125688970088959
Epoch 2080, training loss: 415.0804748535156 = 0.27607160806655884 + 50.0 * 8.296088218688965
Epoch 2080, val loss: 0.41233599185943604
Epoch 2090, training loss: 415.381591796875 = 0.2748647630214691 + 50.0 * 8.30213451385498
Epoch 2090, val loss: 0.41221827268600464
Epoch 2100, training loss: 415.2457275390625 = 0.2736165225505829 + 50.0 * 8.299442291259766
Epoch 2100, val loss: 0.41185617446899414
Epoch 2110, training loss: 415.09991455078125 = 0.27237769961357117 + 50.0 * 8.296550750732422
Epoch 2110, val loss: 0.41174718737602234
Epoch 2120, training loss: 415.053955078125 = 0.2711732089519501 + 50.0 * 8.295655250549316
Epoch 2120, val loss: 0.4115176200866699
Epoch 2130, training loss: 415.0721740722656 = 0.2699803113937378 + 50.0 * 8.296043395996094
Epoch 2130, val loss: 0.41132044792175293
Epoch 2140, training loss: 415.28936767578125 = 0.2687819004058838 + 50.0 * 8.30041217803955
Epoch 2140, val loss: 0.4109920859336853
Epoch 2150, training loss: 415.05230712890625 = 0.26753926277160645 + 50.0 * 8.295695304870605
Epoch 2150, val loss: 0.41086819767951965
Epoch 2160, training loss: 415.1927490234375 = 0.2663158178329468 + 50.0 * 8.298528671264648
Epoch 2160, val loss: 0.4107343256473541
Epoch 2170, training loss: 415.07257080078125 = 0.26508405804634094 + 50.0 * 8.296150207519531
Epoch 2170, val loss: 0.4107987582683563
Epoch 2180, training loss: 414.9972839355469 = 0.2638714015483856 + 50.0 * 8.294668197631836
Epoch 2180, val loss: 0.41045188903808594
Epoch 2190, training loss: 414.96527099609375 = 0.26267749071121216 + 50.0 * 8.294052124023438
Epoch 2190, val loss: 0.41033270955085754
Epoch 2200, training loss: 414.98455810546875 = 0.2614911198616028 + 50.0 * 8.294461250305176
Epoch 2200, val loss: 0.41016265749931335
Epoch 2210, training loss: 415.1765441894531 = 0.2603102922439575 + 50.0 * 8.298324584960938
Epoch 2210, val loss: 0.4098409116268158
Epoch 2220, training loss: 414.977783203125 = 0.25906673073768616 + 50.0 * 8.294374465942383
Epoch 2220, val loss: 0.41030794382095337
Epoch 2230, training loss: 415.14154052734375 = 0.25786733627319336 + 50.0 * 8.297673225402832
Epoch 2230, val loss: 0.4103069007396698
Epoch 2240, training loss: 414.964599609375 = 0.25663819909095764 + 50.0 * 8.294158935546875
Epoch 2240, val loss: 0.40984398126602173
Epoch 2250, training loss: 414.90435791015625 = 0.2554311752319336 + 50.0 * 8.292978286743164
Epoch 2250, val loss: 0.40979790687561035
Epoch 2260, training loss: 414.8984680175781 = 0.2542445957660675 + 50.0 * 8.292884826660156
Epoch 2260, val loss: 0.40984490513801575
Epoch 2270, training loss: 414.8930358886719 = 0.2530655264854431 + 50.0 * 8.292799949645996
Epoch 2270, val loss: 0.4096190631389618
Epoch 2280, training loss: 415.0518798828125 = 0.25188300013542175 + 50.0 * 8.295999526977539
Epoch 2280, val loss: 0.409626305103302
Epoch 2290, training loss: 414.96710205078125 = 0.25067296624183655 + 50.0 * 8.294328689575195
Epoch 2290, val loss: 0.4096577763557434
Epoch 2300, training loss: 415.0586853027344 = 0.24945612251758575 + 50.0 * 8.296184539794922
Epoch 2300, val loss: 0.4097464084625244
Epoch 2310, training loss: 414.9299011230469 = 0.2482408583164215 + 50.0 * 8.293633460998535
Epoch 2310, val loss: 0.4095396101474762
Epoch 2320, training loss: 414.8604736328125 = 0.2470594346523285 + 50.0 * 8.292267799377441
Epoch 2320, val loss: 0.40956225991249084
Epoch 2330, training loss: 414.82733154296875 = 0.2458927035331726 + 50.0 * 8.29162883758545
Epoch 2330, val loss: 0.40950697660446167
Epoch 2340, training loss: 414.8662109375 = 0.24472633004188538 + 50.0 * 8.29242992401123
Epoch 2340, val loss: 0.4095638692378998
Epoch 2350, training loss: 415.125244140625 = 0.24354536831378937 + 50.0 * 8.29763412475586
Epoch 2350, val loss: 0.4095219075679779
Epoch 2360, training loss: 414.88885498046875 = 0.242324560880661 + 50.0 * 8.292930603027344
Epoch 2360, val loss: 0.4095967710018158
Epoch 2370, training loss: 414.81695556640625 = 0.24113546311855316 + 50.0 * 8.291516304016113
Epoch 2370, val loss: 0.4096072316169739
Epoch 2380, training loss: 414.785400390625 = 0.23995423316955566 + 50.0 * 8.290908813476562
Epoch 2380, val loss: 0.4096468389034271
Epoch 2390, training loss: 414.79803466796875 = 0.23877765238285065 + 50.0 * 8.29118537902832
Epoch 2390, val loss: 0.40967515110969543
Epoch 2400, training loss: 415.03204345703125 = 0.23759736120700836 + 50.0 * 8.295888900756836
Epoch 2400, val loss: 0.4099157750606537
Epoch 2410, training loss: 415.18719482421875 = 0.23639126121997833 + 50.0 * 8.299015998840332
Epoch 2410, val loss: 0.41009321808815
Epoch 2420, training loss: 414.8309020996094 = 0.23515917360782623 + 50.0 * 8.291914939880371
Epoch 2420, val loss: 0.4094597399234772
Epoch 2430, training loss: 414.74090576171875 = 0.2339620441198349 + 50.0 * 8.290139198303223
Epoch 2430, val loss: 0.4097580909729004
Epoch 2440, training loss: 414.7259216308594 = 0.23278523981571198 + 50.0 * 8.289862632751465
Epoch 2440, val loss: 0.4098486304283142
Epoch 2450, training loss: 414.7471923828125 = 0.23161284625530243 + 50.0 * 8.290311813354492
Epoch 2450, val loss: 0.4098009169101715
Epoch 2460, training loss: 415.0219421386719 = 0.2304450273513794 + 50.0 * 8.295829772949219
Epoch 2460, val loss: 0.40975505113601685
Epoch 2470, training loss: 414.874267578125 = 0.22922225296497345 + 50.0 * 8.292901039123535
Epoch 2470, val loss: 0.4099028706550598
Epoch 2480, training loss: 414.83563232421875 = 0.2280246615409851 + 50.0 * 8.292152404785156
Epoch 2480, val loss: 0.40999919176101685
Epoch 2490, training loss: 414.7525939941406 = 0.22683121263980865 + 50.0 * 8.290514945983887
Epoch 2490, val loss: 0.41044744849205017
Epoch 2500, training loss: 414.7403259277344 = 0.22565512359142303 + 50.0 * 8.29029369354248
Epoch 2500, val loss: 0.41043415665626526
Epoch 2510, training loss: 414.7559509277344 = 0.2244821935892105 + 50.0 * 8.290629386901855
Epoch 2510, val loss: 0.4104262888431549
Epoch 2520, training loss: 414.7347717285156 = 0.22330446541309357 + 50.0 * 8.290229797363281
Epoch 2520, val loss: 0.4107554852962494
Epoch 2530, training loss: 414.72381591796875 = 0.22213996946811676 + 50.0 * 8.290033340454102
Epoch 2530, val loss: 0.41113370656967163
Epoch 2540, training loss: 414.7741394042969 = 0.22096973657608032 + 50.0 * 8.29106330871582
Epoch 2540, val loss: 0.41119489073753357
Epoch 2550, training loss: 414.7069091796875 = 0.2197883427143097 + 50.0 * 8.289742469787598
Epoch 2550, val loss: 0.41087597608566284
Epoch 2560, training loss: 414.6353759765625 = 0.21862031519412994 + 50.0 * 8.288334846496582
Epoch 2560, val loss: 0.4112146198749542
Epoch 2570, training loss: 414.67694091796875 = 0.2174685299396515 + 50.0 * 8.289189338684082
Epoch 2570, val loss: 0.41145750880241394
Epoch 2580, training loss: 415.0430908203125 = 0.21631671488285065 + 50.0 * 8.29653549194336
Epoch 2580, val loss: 0.41156554222106934
Epoch 2590, training loss: 414.7220153808594 = 0.21513307094573975 + 50.0 * 8.290138244628906
Epoch 2590, val loss: 0.4115907549858093
Epoch 2600, training loss: 414.6278381347656 = 0.2139621376991272 + 50.0 * 8.288277626037598
Epoch 2600, val loss: 0.411772757768631
Epoch 2610, training loss: 414.59710693359375 = 0.21281203627586365 + 50.0 * 8.287686347961426
Epoch 2610, val loss: 0.41195783019065857
Epoch 2620, training loss: 414.65582275390625 = 0.21167422831058502 + 50.0 * 8.288883209228516
Epoch 2620, val loss: 0.4119391143321991
Epoch 2630, training loss: 414.7073669433594 = 0.2105134129524231 + 50.0 * 8.289937019348145
Epoch 2630, val loss: 0.41233497858047485
Epoch 2640, training loss: 414.7320556640625 = 0.20935127139091492 + 50.0 * 8.290453910827637
Epoch 2640, val loss: 0.41320672631263733
Epoch 2650, training loss: 414.6280517578125 = 0.20816820859909058 + 50.0 * 8.288397789001465
Epoch 2650, val loss: 0.41297250986099243
Epoch 2660, training loss: 414.5546569824219 = 0.2070063352584839 + 50.0 * 8.28695297241211
Epoch 2660, val loss: 0.4133501946926117
Epoch 2670, training loss: 414.6321105957031 = 0.2058582305908203 + 50.0 * 8.288524627685547
Epoch 2670, val loss: 0.4136365056037903
Epoch 2680, training loss: 414.6552734375 = 0.204689621925354 + 50.0 * 8.28901195526123
Epoch 2680, val loss: 0.41390934586524963
Epoch 2690, training loss: 414.5646057128906 = 0.20352046191692352 + 50.0 * 8.287221908569336
Epoch 2690, val loss: 0.4141673147678375
Epoch 2700, training loss: 414.6042175292969 = 0.20237798988819122 + 50.0 * 8.288036346435547
Epoch 2700, val loss: 0.41441208124160767
Epoch 2710, training loss: 414.7704162597656 = 0.20124052464962006 + 50.0 * 8.291383743286133
Epoch 2710, val loss: 0.4146021902561188
Epoch 2720, training loss: 414.53790283203125 = 0.20007218420505524 + 50.0 * 8.28675651550293
Epoch 2720, val loss: 0.41499167680740356
Epoch 2730, training loss: 414.5238037109375 = 0.19893138110637665 + 50.0 * 8.286497116088867
Epoch 2730, val loss: 0.41548609733581543
Epoch 2740, training loss: 414.5021057128906 = 0.19780489802360535 + 50.0 * 8.286086082458496
Epoch 2740, val loss: 0.41584599018096924
Epoch 2750, training loss: 414.6922607421875 = 0.19669291377067566 + 50.0 * 8.289911270141602
Epoch 2750, val loss: 0.4163241684436798
Epoch 2760, training loss: 414.5086669921875 = 0.19554339349269867 + 50.0 * 8.286262512207031
Epoch 2760, val loss: 0.4166206121444702
Epoch 2770, training loss: 414.4975891113281 = 0.19441305100917816 + 50.0 * 8.286063194274902
Epoch 2770, val loss: 0.41716116666793823
Epoch 2780, training loss: 414.5635986328125 = 0.1932903528213501 + 50.0 * 8.287405967712402
Epoch 2780, val loss: 0.4171757698059082
Epoch 2790, training loss: 414.6622009277344 = 0.19220155477523804 + 50.0 * 8.289400100708008
Epoch 2790, val loss: 0.41701215505599976
Epoch 2800, training loss: 414.4703674316406 = 0.19103331863880157 + 50.0 * 8.2855863571167
Epoch 2800, val loss: 0.4182443916797638
Epoch 2810, training loss: 414.489501953125 = 0.18992407619953156 + 50.0 * 8.285991668701172
Epoch 2810, val loss: 0.4185381233692169
Epoch 2820, training loss: 414.6726989746094 = 0.188823863863945 + 50.0 * 8.289677619934082
Epoch 2820, val loss: 0.41885536909103394
Epoch 2830, training loss: 414.4271240234375 = 0.18770794570446014 + 50.0 * 8.284788131713867
Epoch 2830, val loss: 0.41961753368377686
Epoch 2840, training loss: 414.4557800292969 = 0.18660415709018707 + 50.0 * 8.285383224487305
Epoch 2840, val loss: 0.4197206497192383
Epoch 2850, training loss: 414.4399108886719 = 0.18551605939865112 + 50.0 * 8.285087585449219
Epoch 2850, val loss: 0.42039695382118225
Epoch 2860, training loss: 414.59747314453125 = 0.1844458132982254 + 50.0 * 8.288260459899902
Epoch 2860, val loss: 0.4212447702884674
Epoch 2870, training loss: 414.48883056640625 = 0.18333573639392853 + 50.0 * 8.286109924316406
Epoch 2870, val loss: 0.4210015535354614
Epoch 2880, training loss: 414.3782653808594 = 0.18223775923252106 + 50.0 * 8.283920288085938
Epoch 2880, val loss: 0.4218094050884247
Epoch 2890, training loss: 414.45404052734375 = 0.1811644583940506 + 50.0 * 8.285457611083984
Epoch 2890, val loss: 0.4222269654273987
Epoch 2900, training loss: 414.5120544433594 = 0.18008917570114136 + 50.0 * 8.286639213562012
Epoch 2900, val loss: 0.4226495623588562
Epoch 2910, training loss: 414.4668273925781 = 0.17901788651943207 + 50.0 * 8.28575611114502
Epoch 2910, val loss: 0.42337697744369507
Epoch 2920, training loss: 414.4288024902344 = 0.17795072495937347 + 50.0 * 8.285017013549805
Epoch 2920, val loss: 0.42346158623695374
Epoch 2930, training loss: 414.5610656738281 = 0.1768883764743805 + 50.0 * 8.287683486938477
Epoch 2930, val loss: 0.42421576380729675
Epoch 2940, training loss: 414.5062561035156 = 0.17582133412361145 + 50.0 * 8.286608695983887
Epoch 2940, val loss: 0.4250324070453644
Epoch 2950, training loss: 414.4132385253906 = 0.1747579723596573 + 50.0 * 8.284770011901855
Epoch 2950, val loss: 0.42543652653694153
Epoch 2960, training loss: 414.3306579589844 = 0.17370522022247314 + 50.0 * 8.2831392288208
Epoch 2960, val loss: 0.42607381939888
Epoch 2970, training loss: 414.3416442871094 = 0.17267179489135742 + 50.0 * 8.283379554748535
Epoch 2970, val loss: 0.4268079102039337
Epoch 2980, training loss: 414.55621337890625 = 0.17164964973926544 + 50.0 * 8.287691116333008
Epoch 2980, val loss: 0.427570641040802
Epoch 2990, training loss: 414.45220947265625 = 0.17059512436389923 + 50.0 * 8.285632133483887
Epoch 2990, val loss: 0.4277576804161072
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8376458650431253
0.8671303339853655
The final CL Acc:0.84187, 0.00671, The final GNN Acc:0.86708, 0.00018
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97298])
remove edge: torch.Size([2, 79646])
updated graph: torch.Size([2, 88296])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2301635742188 = 1.1187571287155151 + 50.0 * 10.58222770690918
Epoch 0, val loss: 1.1188701391220093
Epoch 10, training loss: 530.1520385742188 = 1.1106693744659424 + 50.0 * 10.580827713012695
Epoch 10, val loss: 1.110650658607483
Epoch 20, training loss: 529.5137939453125 = 1.1010427474975586 + 50.0 * 10.568255424499512
Epoch 20, val loss: 1.1008927822113037
Epoch 30, training loss: 524.9414672851562 = 1.0896518230438232 + 50.0 * 10.477036476135254
Epoch 30, val loss: 1.0892993211746216
Epoch 40, training loss: 506.5954895019531 = 1.0777287483215332 + 50.0 * 10.110355377197266
Epoch 40, val loss: 1.0772761106491089
Epoch 50, training loss: 478.44366455078125 = 1.0649900436401367 + 50.0 * 9.54757308959961
Epoch 50, val loss: 1.0646859407424927
Epoch 60, training loss: 468.0360107421875 = 1.0556690692901611 + 50.0 * 9.339607238769531
Epoch 60, val loss: 1.0553913116455078
Epoch 70, training loss: 461.9615173339844 = 1.0472782850265503 + 50.0 * 9.218284606933594
Epoch 70, val loss: 1.0471452474594116
Epoch 80, training loss: 460.3287048339844 = 1.0392955541610718 + 50.0 * 9.18578815460205
Epoch 80, val loss: 1.0393036603927612
Epoch 90, training loss: 457.631103515625 = 1.0319429636001587 + 50.0 * 9.131982803344727
Epoch 90, val loss: 1.032141089439392
Epoch 100, training loss: 453.7881774902344 = 1.0255260467529297 + 50.0 * 9.055253028869629
Epoch 100, val loss: 1.025944709777832
Epoch 110, training loss: 448.68896484375 = 1.0199772119522095 + 50.0 * 8.95337963104248
Epoch 110, val loss: 1.0206454992294312
Epoch 120, training loss: 445.50616455078125 = 1.0146877765655518 + 50.0 * 8.889829635620117
Epoch 120, val loss: 1.0154731273651123
Epoch 130, training loss: 442.2552490234375 = 1.0091221332550049 + 50.0 * 8.824922561645508
Epoch 130, val loss: 1.0099585056304932
Epoch 140, training loss: 439.321533203125 = 1.0030546188354492 + 50.0 * 8.766369819641113
Epoch 140, val loss: 1.0040428638458252
Epoch 150, training loss: 437.6354675292969 = 0.9963300824165344 + 50.0 * 8.732782363891602
Epoch 150, val loss: 0.9973722696304321
Epoch 160, training loss: 436.3989562988281 = 0.9882224798202515 + 50.0 * 8.70821475982666
Epoch 160, val loss: 0.9895510673522949
Epoch 170, training loss: 435.273193359375 = 0.9791686534881592 + 50.0 * 8.685880661010742
Epoch 170, val loss: 0.9805722236633301
Epoch 180, training loss: 434.3979797363281 = 0.9694047570228577 + 50.0 * 8.668571472167969
Epoch 180, val loss: 0.9711002111434937
Epoch 190, training loss: 433.56707763671875 = 0.95921391248703 + 50.0 * 8.652156829833984
Epoch 190, val loss: 0.9610106945037842
Epoch 200, training loss: 432.9154052734375 = 0.9483959078788757 + 50.0 * 8.6393404006958
Epoch 200, val loss: 0.9502923488616943
Epoch 210, training loss: 431.9329833984375 = 0.9367067813873291 + 50.0 * 8.619925498962402
Epoch 210, val loss: 0.9388560652732849
Epoch 220, training loss: 431.0339050292969 = 0.9243355989456177 + 50.0 * 8.602190971374512
Epoch 220, val loss: 0.9267469644546509
Epoch 230, training loss: 430.1995544433594 = 0.9112129211425781 + 50.0 * 8.585766792297363
Epoch 230, val loss: 0.9137909412384033
Epoch 240, training loss: 429.4186706542969 = 0.8972309231758118 + 50.0 * 8.570428848266602
Epoch 240, val loss: 0.8999983072280884
Epoch 250, training loss: 428.64910888671875 = 0.8824580907821655 + 50.0 * 8.555333137512207
Epoch 250, val loss: 0.88554847240448
Epoch 260, training loss: 428.1438903808594 = 0.8669925332069397 + 50.0 * 8.545537948608398
Epoch 260, val loss: 0.8703429698944092
Epoch 270, training loss: 427.6146240234375 = 0.85053551197052 + 50.0 * 8.535282135009766
Epoch 270, val loss: 0.8544009923934937
Epoch 280, training loss: 427.0591735839844 = 0.8336322903633118 + 50.0 * 8.524511337280273
Epoch 280, val loss: 0.8378785848617554
Epoch 290, training loss: 426.608154296875 = 0.8163178563117981 + 50.0 * 8.515836715698242
Epoch 290, val loss: 0.821058988571167
Epoch 300, training loss: 426.25579833984375 = 0.7987990379333496 + 50.0 * 8.509140014648438
Epoch 300, val loss: 0.8040592074394226
Epoch 310, training loss: 426.1353454589844 = 0.7811635732650757 + 50.0 * 8.507083892822266
Epoch 310, val loss: 0.7869824171066284
Epoch 320, training loss: 425.6580810546875 = 0.7636438608169556 + 50.0 * 8.497888565063477
Epoch 320, val loss: 0.7699954509735107
Epoch 330, training loss: 425.3646240234375 = 0.7465350031852722 + 50.0 * 8.492362022399902
Epoch 330, val loss: 0.7535205483436584
Epoch 340, training loss: 425.2911071777344 = 0.7298944592475891 + 50.0 * 8.49122428894043
Epoch 340, val loss: 0.737584114074707
Epoch 350, training loss: 424.9565734863281 = 0.7138482928276062 + 50.0 * 8.484854698181152
Epoch 350, val loss: 0.722032368183136
Epoch 360, training loss: 424.6604919433594 = 0.6984582543373108 + 50.0 * 8.479240417480469
Epoch 360, val loss: 0.7072799205780029
Epoch 370, training loss: 424.518798828125 = 0.6838825941085815 + 50.0 * 8.476698875427246
Epoch 370, val loss: 0.6933576464653015
Epoch 380, training loss: 424.3085632324219 = 0.6700283288955688 + 50.0 * 8.472770690917969
Epoch 380, val loss: 0.6802883148193359
Epoch 390, training loss: 424.0098571777344 = 0.657129168510437 + 50.0 * 8.46705436706543
Epoch 390, val loss: 0.6680105924606323
Epoch 400, training loss: 423.8199768066406 = 0.6451184749603271 + 50.0 * 8.463497161865234
Epoch 400, val loss: 0.6566370129585266
Epoch 410, training loss: 423.9113464355469 = 0.6339600682258606 + 50.0 * 8.465547561645508
Epoch 410, val loss: 0.6461778283119202
Epoch 420, training loss: 423.5257263183594 = 0.6235600709915161 + 50.0 * 8.458043098449707
Epoch 420, val loss: 0.6363918781280518
Epoch 430, training loss: 423.3164978027344 = 0.6140364408493042 + 50.0 * 8.454049110412598
Epoch 430, val loss: 0.6274539828300476
Epoch 440, training loss: 423.9600830078125 = 0.6052378416061401 + 50.0 * 8.467097282409668
Epoch 440, val loss: 0.619411289691925
Epoch 450, training loss: 423.2498474121094 = 0.5970281958580017 + 50.0 * 8.453056335449219
Epoch 450, val loss: 0.6117479205131531
Epoch 460, training loss: 422.8982849121094 = 0.5896016955375671 + 50.0 * 8.446173667907715
Epoch 460, val loss: 0.6049493551254272
Epoch 470, training loss: 422.6943054199219 = 0.5828533172607422 + 50.0 * 8.442229270935059
Epoch 470, val loss: 0.5987715721130371
Epoch 480, training loss: 422.55413818359375 = 0.5766503214836121 + 50.0 * 8.439549446105957
Epoch 480, val loss: 0.5931561589241028
Epoch 490, training loss: 422.56207275390625 = 0.5709424018859863 + 50.0 * 8.43982219696045
Epoch 490, val loss: 0.5880514979362488
Epoch 500, training loss: 422.4950256347656 = 0.5655888319015503 + 50.0 * 8.438589096069336
Epoch 500, val loss: 0.5833032131195068
Epoch 510, training loss: 422.3553161621094 = 0.5606732368469238 + 50.0 * 8.435893058776855
Epoch 510, val loss: 0.5788918733596802
Epoch 520, training loss: 422.10772705078125 = 0.5561259388923645 + 50.0 * 8.431032180786133
Epoch 520, val loss: 0.575021505355835
Epoch 530, training loss: 421.94146728515625 = 0.5519598722457886 + 50.0 * 8.427789688110352
Epoch 530, val loss: 0.5714223980903625
Epoch 540, training loss: 421.8089599609375 = 0.5481194257736206 + 50.0 * 8.425216674804688
Epoch 540, val loss: 0.5681682229042053
Epoch 550, training loss: 422.08282470703125 = 0.544529914855957 + 50.0 * 8.430766105651855
Epoch 550, val loss: 0.5650411248207092
Epoch 560, training loss: 421.6875305175781 = 0.5410727858543396 + 50.0 * 8.422928810119629
Epoch 560, val loss: 0.5622450113296509
Epoch 570, training loss: 421.5832824707031 = 0.5378710627555847 + 50.0 * 8.420907974243164
Epoch 570, val loss: 0.5596226453781128
Epoch 580, training loss: 421.4077453613281 = 0.5348875522613525 + 50.0 * 8.417457580566406
Epoch 580, val loss: 0.5571929812431335
Epoch 590, training loss: 421.2619934082031 = 0.5321117043495178 + 50.0 * 8.414597511291504
Epoch 590, val loss: 0.5549759268760681
Epoch 600, training loss: 421.1988525390625 = 0.5294748544692993 + 50.0 * 8.413387298583984
Epoch 600, val loss: 0.5529412031173706
Epoch 610, training loss: 421.1462707519531 = 0.5269311666488647 + 50.0 * 8.412386894226074
Epoch 610, val loss: 0.5508697032928467
Epoch 620, training loss: 421.3851318359375 = 0.5244888663291931 + 50.0 * 8.417213439941406
Epoch 620, val loss: 0.5491247773170471
Epoch 630, training loss: 421.0945739746094 = 0.5220986604690552 + 50.0 * 8.411449432373047
Epoch 630, val loss: 0.5472113490104675
Epoch 640, training loss: 420.80975341796875 = 0.5198794603347778 + 50.0 * 8.405797958374023
Epoch 640, val loss: 0.5455061793327332
Epoch 650, training loss: 420.71746826171875 = 0.517794132232666 + 50.0 * 8.403993606567383
Epoch 650, val loss: 0.5439268946647644
Epoch 660, training loss: 420.61102294921875 = 0.5157828330993652 + 50.0 * 8.401905059814453
Epoch 660, val loss: 0.542506754398346
Epoch 670, training loss: 420.5816955566406 = 0.513832688331604 + 50.0 * 8.401357650756836
Epoch 670, val loss: 0.541155219078064
Epoch 680, training loss: 420.5326843261719 = 0.5118781924247742 + 50.0 * 8.400416374206543
Epoch 680, val loss: 0.5397202968597412
Epoch 690, training loss: 420.409423828125 = 0.5099575519561768 + 50.0 * 8.397989273071289
Epoch 690, val loss: 0.538391649723053
Epoch 700, training loss: 420.3352355957031 = 0.5081438422203064 + 50.0 * 8.396541595458984
Epoch 700, val loss: 0.5371404886245728
Epoch 710, training loss: 421.17254638671875 = 0.5063912272453308 + 50.0 * 8.413323402404785
Epoch 710, val loss: 0.5360105633735657
Epoch 720, training loss: 420.22882080078125 = 0.5045506954193115 + 50.0 * 8.394485473632812
Epoch 720, val loss: 0.5347006320953369
Epoch 730, training loss: 420.1479187011719 = 0.5028334259986877 + 50.0 * 8.392901420593262
Epoch 730, val loss: 0.5335302948951721
Epoch 740, training loss: 420.0619201660156 = 0.5011988282203674 + 50.0 * 8.391214370727539
Epoch 740, val loss: 0.5324174165725708
Epoch 750, training loss: 419.9872741699219 = 0.4996109902858734 + 50.0 * 8.389753341674805
Epoch 750, val loss: 0.531380832195282
Epoch 760, training loss: 419.95489501953125 = 0.4980522096157074 + 50.0 * 8.389137268066406
Epoch 760, val loss: 0.5303451418876648
Epoch 770, training loss: 420.0250244140625 = 0.4964660704135895 + 50.0 * 8.390571594238281
Epoch 770, val loss: 0.5293146371841431
Epoch 780, training loss: 419.93768310546875 = 0.49486109614372253 + 50.0 * 8.388855934143066
Epoch 780, val loss: 0.5281964540481567
Epoch 790, training loss: 419.81103515625 = 0.4933427572250366 + 50.0 * 8.386353492736816
Epoch 790, val loss: 0.5272674560546875
Epoch 800, training loss: 419.7359313964844 = 0.4918889105319977 + 50.0 * 8.384881019592285
Epoch 800, val loss: 0.5263316035270691
Epoch 810, training loss: 419.737548828125 = 0.4904531240463257 + 50.0 * 8.384942054748535
Epoch 810, val loss: 0.5254902243614197
Epoch 820, training loss: 419.7489013671875 = 0.488994836807251 + 50.0 * 8.385198593139648
Epoch 820, val loss: 0.5245203375816345
Epoch 830, training loss: 419.6050109863281 = 0.48754531145095825 + 50.0 * 8.382349014282227
Epoch 830, val loss: 0.523644745349884
Epoch 840, training loss: 419.5832214355469 = 0.4861438572406769 + 50.0 * 8.381941795349121
Epoch 840, val loss: 0.5227484107017517
Epoch 850, training loss: 419.6250915527344 = 0.4847487509250641 + 50.0 * 8.382806777954102
Epoch 850, val loss: 0.521940290927887
Epoch 860, training loss: 419.5201416015625 = 0.4833643138408661 + 50.0 * 8.380735397338867
Epoch 860, val loss: 0.5210534334182739
Epoch 870, training loss: 419.6892395019531 = 0.4820014536380768 + 50.0 * 8.38414478302002
Epoch 870, val loss: 0.5203413963317871
Epoch 880, training loss: 419.4590148925781 = 0.48060616850852966 + 50.0 * 8.379568099975586
Epoch 880, val loss: 0.5193816423416138
Epoch 890, training loss: 419.3719482421875 = 0.4792715013027191 + 50.0 * 8.377853393554688
Epoch 890, val loss: 0.5186859369277954
Epoch 900, training loss: 419.3714599609375 = 0.4779566824436188 + 50.0 * 8.377869606018066
Epoch 900, val loss: 0.517923891544342
Epoch 910, training loss: 419.4840393066406 = 0.4766422212123871 + 50.0 * 8.380147933959961
Epoch 910, val loss: 0.5172432065010071
Epoch 920, training loss: 419.3073425292969 = 0.475312739610672 + 50.0 * 8.376640319824219
Epoch 920, val loss: 0.5163005590438843
Epoch 930, training loss: 419.2396545410156 = 0.47401463985443115 + 50.0 * 8.375312805175781
Epoch 930, val loss: 0.5157017707824707
Epoch 940, training loss: 419.3385314941406 = 0.4727363586425781 + 50.0 * 8.377315521240234
Epoch 940, val loss: 0.5149174928665161
Epoch 950, training loss: 419.4685363769531 = 0.471425861120224 + 50.0 * 8.379941940307617
Epoch 950, val loss: 0.5141040682792664
Epoch 960, training loss: 419.2113952636719 = 0.470114141702652 + 50.0 * 8.374825477600098
Epoch 960, val loss: 0.5134151577949524
Epoch 970, training loss: 419.08270263671875 = 0.4688423275947571 + 50.0 * 8.37227725982666
Epoch 970, val loss: 0.5126138925552368
Epoch 980, training loss: 419.0490417480469 = 0.46759456396102905 + 50.0 * 8.371628761291504
Epoch 980, val loss: 0.5120277404785156
Epoch 990, training loss: 419.022216796875 = 0.46636509895324707 + 50.0 * 8.371116638183594
Epoch 990, val loss: 0.5113410949707031
Epoch 1000, training loss: 419.3409118652344 = 0.465109258890152 + 50.0 * 8.37751579284668
Epoch 1000, val loss: 0.5107153058052063
Epoch 1010, training loss: 419.1806640625 = 0.4637952148914337 + 50.0 * 8.374337196350098
Epoch 1010, val loss: 0.5099378228187561
Epoch 1020, training loss: 418.95672607421875 = 0.4625036120414734 + 50.0 * 8.369884490966797
Epoch 1020, val loss: 0.5092074275016785
Epoch 1030, training loss: 418.9978942871094 = 0.4612284004688263 + 50.0 * 8.370733261108398
Epoch 1030, val loss: 0.5085323452949524
Epoch 1040, training loss: 418.9964294433594 = 0.45995280146598816 + 50.0 * 8.370729446411133
Epoch 1040, val loss: 0.5078917145729065
Epoch 1050, training loss: 419.0057067871094 = 0.4586949646472931 + 50.0 * 8.370940208435059
Epoch 1050, val loss: 0.5072087049484253
Epoch 1060, training loss: 418.8265380859375 = 0.45739999413490295 + 50.0 * 8.367383003234863
Epoch 1060, val loss: 0.5066187977790833
Epoch 1070, training loss: 418.77392578125 = 0.4561275541782379 + 50.0 * 8.366355895996094
Epoch 1070, val loss: 0.5059310793876648
Epoch 1080, training loss: 418.76153564453125 = 0.45487073063850403 + 50.0 * 8.366133689880371
Epoch 1080, val loss: 0.5052905678749084
Epoch 1090, training loss: 419.0093994140625 = 0.45360106229782104 + 50.0 * 8.371115684509277
Epoch 1090, val loss: 0.5046113133430481
Epoch 1100, training loss: 418.7359924316406 = 0.45228832960128784 + 50.0 * 8.365674018859863
Epoch 1100, val loss: 0.5040934085845947
Epoch 1110, training loss: 418.8377380371094 = 0.4509890377521515 + 50.0 * 8.367734909057617
Epoch 1110, val loss: 0.5035197138786316
Epoch 1120, training loss: 418.6791076660156 = 0.44967034459114075 + 50.0 * 8.364588737487793
Epoch 1120, val loss: 0.5027223825454712
Epoch 1130, training loss: 418.6859130859375 = 0.4483751058578491 + 50.0 * 8.364750862121582
Epoch 1130, val loss: 0.5021287202835083
Epoch 1140, training loss: 418.6059875488281 = 0.4470869302749634 + 50.0 * 8.363178253173828
Epoch 1140, val loss: 0.5015565752983093
Epoch 1150, training loss: 418.5748291015625 = 0.44580939412117004 + 50.0 * 8.362580299377441
Epoch 1150, val loss: 0.5009370446205139
Epoch 1160, training loss: 418.6290283203125 = 0.44452306628227234 + 50.0 * 8.363690376281738
Epoch 1160, val loss: 0.5003588795661926
Epoch 1170, training loss: 418.7347412109375 = 0.4431851804256439 + 50.0 * 8.36583137512207
Epoch 1170, val loss: 0.4997609555721283
Epoch 1180, training loss: 418.5534973144531 = 0.44178804755210876 + 50.0 * 8.362234115600586
Epoch 1180, val loss: 0.49908146262168884
Epoch 1190, training loss: 418.565185546875 = 0.4404350221157074 + 50.0 * 8.362495422363281
Epoch 1190, val loss: 0.4985179901123047
Epoch 1200, training loss: 418.4898376464844 = 0.4391047954559326 + 50.0 * 8.361014366149902
Epoch 1200, val loss: 0.4979567527770996
Epoch 1210, training loss: 418.645751953125 = 0.4377593994140625 + 50.0 * 8.36415958404541
Epoch 1210, val loss: 0.4973468482494354
Epoch 1220, training loss: 418.49658203125 = 0.436380535364151 + 50.0 * 8.361204147338867
Epoch 1220, val loss: 0.4966621696949005
Epoch 1230, training loss: 418.4396057128906 = 0.43499669432640076 + 50.0 * 8.360092163085938
Epoch 1230, val loss: 0.49608945846557617
Epoch 1240, training loss: 418.5490417480469 = 0.4336131513118744 + 50.0 * 8.362308502197266
Epoch 1240, val loss: 0.4955473244190216
Epoch 1250, training loss: 418.4151306152344 = 0.43217310309410095 + 50.0 * 8.359659194946289
Epoch 1250, val loss: 0.4949922263622284
Epoch 1260, training loss: 418.3460693359375 = 0.4307372570037842 + 50.0 * 8.358306884765625
Epoch 1260, val loss: 0.49430108070373535
Epoch 1270, training loss: 418.36932373046875 = 0.42930519580841064 + 50.0 * 8.358800888061523
Epoch 1270, val loss: 0.4936806857585907
Epoch 1280, training loss: 418.40179443359375 = 0.4278489649295807 + 50.0 * 8.359478950500488
Epoch 1280, val loss: 0.49311313033103943
Epoch 1290, training loss: 418.5016784667969 = 0.4263605773448944 + 50.0 * 8.361506462097168
Epoch 1290, val loss: 0.4926300644874573
Epoch 1300, training loss: 418.3072509765625 = 0.4248611629009247 + 50.0 * 8.357647895812988
Epoch 1300, val loss: 0.4920113980770111
Epoch 1310, training loss: 418.2397766113281 = 0.4233822226524353 + 50.0 * 8.356328010559082
Epoch 1310, val loss: 0.491471529006958
Epoch 1320, training loss: 418.3886413574219 = 0.4218927323818207 + 50.0 * 8.359334945678711
Epoch 1320, val loss: 0.49076780676841736
Epoch 1330, training loss: 418.2443542480469 = 0.4203152060508728 + 50.0 * 8.356480598449707
Epoch 1330, val loss: 0.49040040373802185
Epoch 1340, training loss: 418.23583984375 = 0.41872474551200867 + 50.0 * 8.356342315673828
Epoch 1340, val loss: 0.48958343267440796
Epoch 1350, training loss: 418.1562194824219 = 0.41716432571411133 + 50.0 * 8.354781150817871
Epoch 1350, val loss: 0.4891985058784485
Epoch 1360, training loss: 418.11895751953125 = 0.4156108796596527 + 50.0 * 8.354066848754883
Epoch 1360, val loss: 0.4886299669742584
Epoch 1370, training loss: 418.3986511230469 = 0.4140402376651764 + 50.0 * 8.359692573547363
Epoch 1370, val loss: 0.48813191056251526
Epoch 1380, training loss: 418.1512451171875 = 0.4123937487602234 + 50.0 * 8.354777336120605
Epoch 1380, val loss: 0.4874992072582245
Epoch 1390, training loss: 418.1700134277344 = 0.4107447862625122 + 50.0 * 8.355185508728027
Epoch 1390, val loss: 0.4869011640548706
Epoch 1400, training loss: 418.2712707519531 = 0.40908223390579224 + 50.0 * 8.357243537902832
Epoch 1400, val loss: 0.4863576292991638
Epoch 1410, training loss: 418.0903015136719 = 0.40738674998283386 + 50.0 * 8.353658676147461
Epoch 1410, val loss: 0.4859694838523865
Epoch 1420, training loss: 418.017822265625 = 0.40571334958076477 + 50.0 * 8.352242469787598
Epoch 1420, val loss: 0.48539087176322937
Epoch 1430, training loss: 418.14788818359375 = 0.40403997898101807 + 50.0 * 8.354876518249512
Epoch 1430, val loss: 0.4849116802215576
Epoch 1440, training loss: 418.0137634277344 = 0.4023158550262451 + 50.0 * 8.352229118347168
Epoch 1440, val loss: 0.48442262411117554
Epoch 1450, training loss: 417.9669189453125 = 0.4005885422229767 + 50.0 * 8.351326942443848
Epoch 1450, val loss: 0.4838373064994812
Epoch 1460, training loss: 418.1763610839844 = 0.3988597095012665 + 50.0 * 8.355549812316895
Epoch 1460, val loss: 0.48330023884773254
Epoch 1470, training loss: 418.0354919433594 = 0.3970800042152405 + 50.0 * 8.352767944335938
Epoch 1470, val loss: 0.4829234480857849
Epoch 1480, training loss: 418.1342468261719 = 0.39529353380203247 + 50.0 * 8.354779243469238
Epoch 1480, val loss: 0.4822715222835541
Epoch 1490, training loss: 417.8934631347656 = 0.39348605275154114 + 50.0 * 8.34999942779541
Epoch 1490, val loss: 0.48185625672340393
Epoch 1500, training loss: 417.8960876464844 = 0.3917034864425659 + 50.0 * 8.350088119506836
Epoch 1500, val loss: 0.48138803243637085
Epoch 1510, training loss: 417.8816223144531 = 0.3899063169956207 + 50.0 * 8.349834442138672
Epoch 1510, val loss: 0.4809401333332062
Epoch 1520, training loss: 418.0602111816406 = 0.3880801200866699 + 50.0 * 8.353442192077637
Epoch 1520, val loss: 0.48042744398117065
Epoch 1530, training loss: 417.96746826171875 = 0.3862040638923645 + 50.0 * 8.351625442504883
Epoch 1530, val loss: 0.4799034595489502
Epoch 1540, training loss: 418.0661315917969 = 0.38430967926979065 + 50.0 * 8.353636741638184
Epoch 1540, val loss: 0.47948235273361206
Epoch 1550, training loss: 417.88555908203125 = 0.3824004530906677 + 50.0 * 8.35006332397461
Epoch 1550, val loss: 0.47889813780784607
Epoch 1560, training loss: 417.8067321777344 = 0.38049983978271484 + 50.0 * 8.348525047302246
Epoch 1560, val loss: 0.47842976450920105
Epoch 1570, training loss: 417.9164733886719 = 0.3786084055900574 + 50.0 * 8.350757598876953
Epoch 1570, val loss: 0.4780593514442444
Epoch 1580, training loss: 417.7455749511719 = 0.3766770362854004 + 50.0 * 8.34737777709961
Epoch 1580, val loss: 0.4776340425014496
Epoch 1590, training loss: 417.8125 = 0.37475043535232544 + 50.0 * 8.3487548828125
Epoch 1590, val loss: 0.47727376222610474
Epoch 1600, training loss: 417.9591064453125 = 0.3727772831916809 + 50.0 * 8.351726531982422
Epoch 1600, val loss: 0.47669121623039246
Epoch 1610, training loss: 417.7439880371094 = 0.3707655668258667 + 50.0 * 8.347464561462402
Epoch 1610, val loss: 0.476174533367157
Epoch 1620, training loss: 417.6954345703125 = 0.368797242641449 + 50.0 * 8.346532821655273
Epoch 1620, val loss: 0.4759160280227661
Epoch 1630, training loss: 417.65692138671875 = 0.366827130317688 + 50.0 * 8.345802307128906
Epoch 1630, val loss: 0.47549277544021606
Epoch 1640, training loss: 417.7016296386719 = 0.3648577630519867 + 50.0 * 8.346735000610352
Epoch 1640, val loss: 0.47520536184310913
Epoch 1650, training loss: 417.8672180175781 = 0.36284711956977844 + 50.0 * 8.35008716583252
Epoch 1650, val loss: 0.47484153509140015
Epoch 1660, training loss: 417.8118896484375 = 0.3607938885688782 + 50.0 * 8.349021911621094
Epoch 1660, val loss: 0.47425389289855957
Epoch 1670, training loss: 417.80047607421875 = 0.3587378263473511 + 50.0 * 8.348834991455078
Epoch 1670, val loss: 0.47409412264823914
Epoch 1680, training loss: 417.695068359375 = 0.356687992811203 + 50.0 * 8.34676742553711
Epoch 1680, val loss: 0.47367697954177856
Epoch 1690, training loss: 417.65228271484375 = 0.3546416163444519 + 50.0 * 8.345952987670898
Epoch 1690, val loss: 0.4733572006225586
Epoch 1700, training loss: 417.561279296875 = 0.352590411901474 + 50.0 * 8.344173431396484
Epoch 1700, val loss: 0.4730871915817261
Epoch 1710, training loss: 417.6033935546875 = 0.35054901242256165 + 50.0 * 8.345056533813477
Epoch 1710, val loss: 0.47282615303993225
Epoch 1720, training loss: 417.59930419921875 = 0.34848377108573914 + 50.0 * 8.345016479492188
Epoch 1720, val loss: 0.47252750396728516
Epoch 1730, training loss: 417.54534912109375 = 0.3464105427265167 + 50.0 * 8.343978881835938
Epoch 1730, val loss: 0.4724797010421753
Epoch 1740, training loss: 417.638916015625 = 0.3443455398082733 + 50.0 * 8.345890998840332
Epoch 1740, val loss: 0.47208935022354126
Epoch 1750, training loss: 417.6331481933594 = 0.3422258496284485 + 50.0 * 8.345818519592285
Epoch 1750, val loss: 0.4720444083213806
Epoch 1760, training loss: 417.5888977050781 = 0.34007373452186584 + 50.0 * 8.344976425170898
Epoch 1760, val loss: 0.4716825783252716
Epoch 1770, training loss: 417.52496337890625 = 0.3379687964916229 + 50.0 * 8.34373950958252
Epoch 1770, val loss: 0.47161757946014404
Epoch 1780, training loss: 417.42926025390625 = 0.3358754813671112 + 50.0 * 8.341867446899414
Epoch 1780, val loss: 0.4714047908782959
Epoch 1790, training loss: 417.6208190917969 = 0.33379653096199036 + 50.0 * 8.34574031829834
Epoch 1790, val loss: 0.4712142050266266
Epoch 1800, training loss: 417.42352294921875 = 0.33166736364364624 + 50.0 * 8.341836929321289
Epoch 1800, val loss: 0.4711434841156006
Epoch 1810, training loss: 417.41253662109375 = 0.32955053448677063 + 50.0 * 8.341659545898438
Epoch 1810, val loss: 0.47106361389160156
Epoch 1820, training loss: 417.3949890136719 = 0.32744258642196655 + 50.0 * 8.341350555419922
Epoch 1820, val loss: 0.47097650170326233
Epoch 1830, training loss: 417.41070556640625 = 0.3253444731235504 + 50.0 * 8.341707229614258
Epoch 1830, val loss: 0.4709267318248749
Epoch 1840, training loss: 417.51019287109375 = 0.32322561740875244 + 50.0 * 8.34373950958252
Epoch 1840, val loss: 0.4708220064640045
Epoch 1850, training loss: 417.47015380859375 = 0.3210771679878235 + 50.0 * 8.342981338500977
Epoch 1850, val loss: 0.47047263383865356
Epoch 1860, training loss: 417.6213684082031 = 0.31893065571784973 + 50.0 * 8.346048355102539
Epoch 1860, val loss: 0.47068095207214355
Epoch 1870, training loss: 417.31842041015625 = 0.31675177812576294 + 50.0 * 8.340033531188965
Epoch 1870, val loss: 0.4706166982650757
Epoch 1880, training loss: 417.279541015625 = 0.3146297037601471 + 50.0 * 8.339298248291016
Epoch 1880, val loss: 0.47071120142936707
Epoch 1890, training loss: 417.2509765625 = 0.3125315308570862 + 50.0 * 8.33876895904541
Epoch 1890, val loss: 0.47070008516311646
Epoch 1900, training loss: 417.31463623046875 = 0.3104381263256073 + 50.0 * 8.340084075927734
Epoch 1900, val loss: 0.4706833064556122
Epoch 1910, training loss: 417.47857666015625 = 0.3083028197288513 + 50.0 * 8.343405723571777
Epoch 1910, val loss: 0.4707154333591461
Epoch 1920, training loss: 417.3141784667969 = 0.3061462342739105 + 50.0 * 8.340160369873047
Epoch 1920, val loss: 0.4709535241127014
Epoch 1930, training loss: 417.2267761230469 = 0.3040057122707367 + 50.0 * 8.338455200195312
Epoch 1930, val loss: 0.47105687856674194
Epoch 1940, training loss: 417.2960510253906 = 0.30190449953079224 + 50.0 * 8.339882850646973
Epoch 1940, val loss: 0.4712798297405243
Epoch 1950, training loss: 417.2500915527344 = 0.2997840642929077 + 50.0 * 8.339006423950195
Epoch 1950, val loss: 0.471309095621109
Epoch 1960, training loss: 417.4217834472656 = 0.29767921566963196 + 50.0 * 8.342482566833496
Epoch 1960, val loss: 0.47138068079948425
Epoch 1970, training loss: 417.22784423828125 = 0.2955438196659088 + 50.0 * 8.338645935058594
Epoch 1970, val loss: 0.4717227518558502
Epoch 1980, training loss: 417.1808166503906 = 0.29343727231025696 + 50.0 * 8.337747573852539
Epoch 1980, val loss: 0.4719671905040741
Epoch 1990, training loss: 417.1372375488281 = 0.291347473859787 + 50.0 * 8.336917877197266
Epoch 1990, val loss: 0.4722558557987213
Epoch 2000, training loss: 417.2873840332031 = 0.28927358984947205 + 50.0 * 8.339962005615234
Epoch 2000, val loss: 0.472565621137619
Epoch 2010, training loss: 417.3787536621094 = 0.2871522605419159 + 50.0 * 8.341832160949707
Epoch 2010, val loss: 0.4724141061306
Epoch 2020, training loss: 417.16021728515625 = 0.28502875566482544 + 50.0 * 8.337503433227539
Epoch 2020, val loss: 0.47300997376441956
Epoch 2030, training loss: 417.0977783203125 = 0.28294724225997925 + 50.0 * 8.336296081542969
Epoch 2030, val loss: 0.47309809923171997
Epoch 2040, training loss: 417.0630187988281 = 0.2808972895145416 + 50.0 * 8.33564281463623
Epoch 2040, val loss: 0.4735291600227356
Epoch 2050, training loss: 417.0675048828125 = 0.27885425090789795 + 50.0 * 8.335773468017578
Epoch 2050, val loss: 0.47371432185173035
Epoch 2060, training loss: 417.6658020019531 = 0.2768218219280243 + 50.0 * 8.347779273986816
Epoch 2060, val loss: 0.47397974133491516
Epoch 2070, training loss: 417.15557861328125 = 0.27471330761909485 + 50.0 * 8.337616920471191
Epoch 2070, val loss: 0.4747905433177948
Epoch 2080, training loss: 417.0120849609375 = 0.27262046933174133 + 50.0 * 8.334789276123047
Epoch 2080, val loss: 0.475189745426178
Epoch 2090, training loss: 417.0124816894531 = 0.27059614658355713 + 50.0 * 8.334837913513184
Epoch 2090, val loss: 0.47556036710739136
Epoch 2100, training loss: 417.18121337890625 = 0.26860925555229187 + 50.0 * 8.338252067565918
Epoch 2100, val loss: 0.4762630760669708
Epoch 2110, training loss: 417.0621337890625 = 0.2665521800518036 + 50.0 * 8.335911750793457
Epoch 2110, val loss: 0.47635817527770996
Epoch 2120, training loss: 417.0213623046875 = 0.2645249664783478 + 50.0 * 8.335136413574219
Epoch 2120, val loss: 0.47705888748168945
Epoch 2130, training loss: 417.10400390625 = 0.2625213861465454 + 50.0 * 8.336830139160156
Epoch 2130, val loss: 0.4774438142776489
Epoch 2140, training loss: 416.9582824707031 = 0.2605140209197998 + 50.0 * 8.333954811096191
Epoch 2140, val loss: 0.478018581867218
Epoch 2150, training loss: 416.95233154296875 = 0.2585354447364807 + 50.0 * 8.33387565612793
Epoch 2150, val loss: 0.47865626215934753
Epoch 2160, training loss: 416.9892578125 = 0.2565648555755615 + 50.0 * 8.334653854370117
Epoch 2160, val loss: 0.4791781008243561
Epoch 2170, training loss: 417.1255187988281 = 0.2545880675315857 + 50.0 * 8.337418556213379
Epoch 2170, val loss: 0.4797755479812622
Epoch 2180, training loss: 417.0190734863281 = 0.25261420011520386 + 50.0 * 8.335329055786133
Epoch 2180, val loss: 0.48050764203071594
Epoch 2190, training loss: 416.9945373535156 = 0.2506379783153534 + 50.0 * 8.334877967834473
Epoch 2190, val loss: 0.48107028007507324
Epoch 2200, training loss: 417.1014099121094 = 0.248674675822258 + 50.0 * 8.337054252624512
Epoch 2200, val loss: 0.48148417472839355
Epoch 2210, training loss: 416.9148254394531 = 0.24670252203941345 + 50.0 * 8.333362579345703
Epoch 2210, val loss: 0.4818703532218933
Epoch 2220, training loss: 416.92999267578125 = 0.24476781487464905 + 50.0 * 8.333703994750977
Epoch 2220, val loss: 0.4826488792896271
Epoch 2230, training loss: 416.9448547363281 = 0.24283753335475922 + 50.0 * 8.334040641784668
Epoch 2230, val loss: 0.4832276701927185
Epoch 2240, training loss: 416.9241943359375 = 0.24092118442058563 + 50.0 * 8.333664894104004
Epoch 2240, val loss: 0.48408257961273193
Epoch 2250, training loss: 416.977294921875 = 0.23899735510349274 + 50.0 * 8.334766387939453
Epoch 2250, val loss: 0.4846673905849457
Epoch 2260, training loss: 416.87213134765625 = 0.23708471655845642 + 50.0 * 8.332700729370117
Epoch 2260, val loss: 0.4854746460914612
Epoch 2270, training loss: 416.81488037109375 = 0.23518986999988556 + 50.0 * 8.33159351348877
Epoch 2270, val loss: 0.48608389496803284
Epoch 2280, training loss: 416.7823181152344 = 0.23331616818904877 + 50.0 * 8.33098030090332
Epoch 2280, val loss: 0.4867139160633087
Epoch 2290, training loss: 416.8753356933594 = 0.2314782589673996 + 50.0 * 8.332877159118652
Epoch 2290, val loss: 0.4877435863018036
Epoch 2300, training loss: 416.8875732421875 = 0.2296244502067566 + 50.0 * 8.333159446716309
Epoch 2300, val loss: 0.4884074926376343
Epoch 2310, training loss: 416.8017883300781 = 0.227738618850708 + 50.0 * 8.331480979919434
Epoch 2310, val loss: 0.4889955520629883
Epoch 2320, training loss: 416.80914306640625 = 0.2258990854024887 + 50.0 * 8.3316650390625
Epoch 2320, val loss: 0.4894193708896637
Epoch 2330, training loss: 416.95391845703125 = 0.2240935117006302 + 50.0 * 8.334596633911133
Epoch 2330, val loss: 0.49054068326950073
Epoch 2340, training loss: 416.8159484863281 = 0.22227013111114502 + 50.0 * 8.331873893737793
Epoch 2340, val loss: 0.4913271367549896
Epoch 2350, training loss: 416.7872009277344 = 0.22046560049057007 + 50.0 * 8.331335067749023
Epoch 2350, val loss: 0.49209505319595337
Epoch 2360, training loss: 416.7685241699219 = 0.21865501999855042 + 50.0 * 8.330997467041016
Epoch 2360, val loss: 0.4927002191543579
Epoch 2370, training loss: 416.79248046875 = 0.21687036752700806 + 50.0 * 8.331512451171875
Epoch 2370, val loss: 0.4934505820274353
Epoch 2380, training loss: 416.7303771972656 = 0.21508120000362396 + 50.0 * 8.330306053161621
Epoch 2380, val loss: 0.4940977096557617
Epoch 2390, training loss: 416.8719177246094 = 0.21332743763923645 + 50.0 * 8.333171844482422
Epoch 2390, val loss: 0.49455368518829346
Epoch 2400, training loss: 416.79327392578125 = 0.2115466147661209 + 50.0 * 8.331634521484375
Epoch 2400, val loss: 0.4957793056964874
Epoch 2410, training loss: 416.71490478515625 = 0.2098034918308258 + 50.0 * 8.33010196685791
Epoch 2410, val loss: 0.4969537556171417
Epoch 2420, training loss: 416.7737121582031 = 0.2080593705177307 + 50.0 * 8.331313133239746
Epoch 2420, val loss: 0.4976763427257538
Epoch 2430, training loss: 416.8368225097656 = 0.2063232660293579 + 50.0 * 8.332610130310059
Epoch 2430, val loss: 0.497921884059906
Epoch 2440, training loss: 416.6934814453125 = 0.20456552505493164 + 50.0 * 8.329778671264648
Epoch 2440, val loss: 0.49935051798820496
Epoch 2450, training loss: 416.658447265625 = 0.20285829901695251 + 50.0 * 8.32911205291748
Epoch 2450, val loss: 0.5004516839981079
Epoch 2460, training loss: 416.6278076171875 = 0.20115482807159424 + 50.0 * 8.328533172607422
Epoch 2460, val loss: 0.5010473132133484
Epoch 2470, training loss: 416.65411376953125 = 0.1994757354259491 + 50.0 * 8.329092979431152
Epoch 2470, val loss: 0.5020716190338135
Epoch 2480, training loss: 416.8115539550781 = 0.1977962851524353 + 50.0 * 8.332275390625
Epoch 2480, val loss: 0.5029965043067932
Epoch 2490, training loss: 416.7321472167969 = 0.19609762728214264 + 50.0 * 8.330720901489258
Epoch 2490, val loss: 0.5041971206665039
Epoch 2500, training loss: 416.6055908203125 = 0.19441452622413635 + 50.0 * 8.32822322845459
Epoch 2500, val loss: 0.5050584077835083
Epoch 2510, training loss: 416.6969299316406 = 0.19277285039424896 + 50.0 * 8.330082893371582
Epoch 2510, val loss: 0.5064154267311096
Epoch 2520, training loss: 416.79217529296875 = 0.19111207127571106 + 50.0 * 8.332021713256836
Epoch 2520, val loss: 0.5070346593856812
Epoch 2530, training loss: 416.6269836425781 = 0.18944957852363586 + 50.0 * 8.328750610351562
Epoch 2530, val loss: 0.5080147981643677
Epoch 2540, training loss: 416.5797424316406 = 0.18781678378582 + 50.0 * 8.327838897705078
Epoch 2540, val loss: 0.5090997219085693
Epoch 2550, training loss: 416.5651550292969 = 0.18620119988918304 + 50.0 * 8.327579498291016
Epoch 2550, val loss: 0.5101826190948486
Epoch 2560, training loss: 416.6337585449219 = 0.1846030056476593 + 50.0 * 8.328983306884766
Epoch 2560, val loss: 0.5108782052993774
Epoch 2570, training loss: 416.8225402832031 = 0.18301071226596832 + 50.0 * 8.33279037475586
Epoch 2570, val loss: 0.5120517015457153
Epoch 2580, training loss: 416.6067810058594 = 0.18139998614788055 + 50.0 * 8.328507423400879
Epoch 2580, val loss: 0.5136436223983765
Epoch 2590, training loss: 416.5080261230469 = 0.17978540062904358 + 50.0 * 8.32656478881836
Epoch 2590, val loss: 0.5143260359764099
Epoch 2600, training loss: 416.5097351074219 = 0.1782442033290863 + 50.0 * 8.326629638671875
Epoch 2600, val loss: 0.5160394310951233
Epoch 2610, training loss: 416.790771484375 = 0.1767590194940567 + 50.0 * 8.332280158996582
Epoch 2610, val loss: 0.517634928226471
Epoch 2620, training loss: 416.5034484863281 = 0.17511191964149475 + 50.0 * 8.326566696166992
Epoch 2620, val loss: 0.5172988176345825
Epoch 2630, training loss: 416.45306396484375 = 0.17356738448143005 + 50.0 * 8.325590133666992
Epoch 2630, val loss: 0.5189867615699768
Epoch 2640, training loss: 416.5291442871094 = 0.1720583140850067 + 50.0 * 8.327141761779785
Epoch 2640, val loss: 0.5201359391212463
Epoch 2650, training loss: 416.8044128417969 = 0.17054401338100433 + 50.0 * 8.332677841186523
Epoch 2650, val loss: 0.5209240317344666
Epoch 2660, training loss: 416.575927734375 = 0.16900676488876343 + 50.0 * 8.32813835144043
Epoch 2660, val loss: 0.5223257541656494
Epoch 2670, training loss: 416.4724426269531 = 0.16748292744159698 + 50.0 * 8.326099395751953
Epoch 2670, val loss: 0.5233586430549622
Epoch 2680, training loss: 416.46331787109375 = 0.1659868359565735 + 50.0 * 8.325946807861328
Epoch 2680, val loss: 0.5246402025222778
Epoch 2690, training loss: 416.5434875488281 = 0.1645055115222931 + 50.0 * 8.327579498291016
Epoch 2690, val loss: 0.5257207751274109
Epoch 2700, training loss: 416.4200744628906 = 0.1630088984966278 + 50.0 * 8.325141906738281
Epoch 2700, val loss: 0.5269713401794434
Epoch 2710, training loss: 416.4476623535156 = 0.1615324169397354 + 50.0 * 8.325722694396973
Epoch 2710, val loss: 0.5281082391738892
Epoch 2720, training loss: 416.62811279296875 = 0.1600780040025711 + 50.0 * 8.329360961914062
Epoch 2720, val loss: 0.5296818017959595
Epoch 2730, training loss: 416.55609130859375 = 0.15862131118774414 + 50.0 * 8.327949523925781
Epoch 2730, val loss: 0.5312060713768005
Epoch 2740, training loss: 416.5860290527344 = 0.1571485847234726 + 50.0 * 8.328577995300293
Epoch 2740, val loss: 0.5320935845375061
Epoch 2750, training loss: 416.4241943359375 = 0.15568803250789642 + 50.0 * 8.325369834899902
Epoch 2750, val loss: 0.5333962440490723
Epoch 2760, training loss: 416.3639221191406 = 0.15424667298793793 + 50.0 * 8.324193954467773
Epoch 2760, val loss: 0.5343999862670898
Epoch 2770, training loss: 416.4057922363281 = 0.15283261239528656 + 50.0 * 8.325058937072754
Epoch 2770, val loss: 0.5357900261878967
Epoch 2780, training loss: 416.6097106933594 = 0.15143069624900818 + 50.0 * 8.3291654586792
Epoch 2780, val loss: 0.5372219681739807
Epoch 2790, training loss: 416.4006652832031 = 0.15000784397125244 + 50.0 * 8.325013160705566
Epoch 2790, val loss: 0.5388990640640259
Epoch 2800, training loss: 416.3468322753906 = 0.14859800040721893 + 50.0 * 8.323965072631836
Epoch 2800, val loss: 0.5398133397102356
Epoch 2810, training loss: 416.4820251464844 = 0.1472187638282776 + 50.0 * 8.326696395874023
Epoch 2810, val loss: 0.5410768389701843
Epoch 2820, training loss: 416.4454345703125 = 0.14583754539489746 + 50.0 * 8.3259916305542
Epoch 2820, val loss: 0.543289065361023
Epoch 2830, training loss: 416.40716552734375 = 0.14445367455482483 + 50.0 * 8.325254440307617
Epoch 2830, val loss: 0.5444571375846863
Epoch 2840, training loss: 416.3889465332031 = 0.14309078454971313 + 50.0 * 8.32491683959961
Epoch 2840, val loss: 0.5460870862007141
Epoch 2850, training loss: 416.357421875 = 0.1417497992515564 + 50.0 * 8.324313163757324
Epoch 2850, val loss: 0.547768235206604
Epoch 2860, training loss: 416.28350830078125 = 0.14038708806037903 + 50.0 * 8.32286262512207
Epoch 2860, val loss: 0.548766553401947
Epoch 2870, training loss: 416.33502197265625 = 0.1390596479177475 + 50.0 * 8.323919296264648
Epoch 2870, val loss: 0.550123393535614
Epoch 2880, training loss: 416.511474609375 = 0.13774316012859344 + 50.0 * 8.327474594116211
Epoch 2880, val loss: 0.5514833927154541
Epoch 2890, training loss: 416.4197692871094 = 0.13641563057899475 + 50.0 * 8.325667381286621
Epoch 2890, val loss: 0.552293062210083
Epoch 2900, training loss: 416.3634338378906 = 0.13508673012256622 + 50.0 * 8.324566841125488
Epoch 2900, val loss: 0.554478645324707
Epoch 2910, training loss: 416.2999572753906 = 0.13378380239009857 + 50.0 * 8.323323249816895
Epoch 2910, val loss: 0.5559404492378235
Epoch 2920, training loss: 416.2484436035156 = 0.13246890902519226 + 50.0 * 8.322319030761719
Epoch 2920, val loss: 0.5569115281105042
Epoch 2930, training loss: 416.32757568359375 = 0.13119539618492126 + 50.0 * 8.323927879333496
Epoch 2930, val loss: 0.5585922598838806
Epoch 2940, training loss: 416.4356689453125 = 0.12994137406349182 + 50.0 * 8.326114654541016
Epoch 2940, val loss: 0.5603783130645752
Epoch 2950, training loss: 416.3113098144531 = 0.12864810228347778 + 50.0 * 8.323653221130371
Epoch 2950, val loss: 0.5613153576850891
Epoch 2960, training loss: 416.3004455566406 = 0.12740077078342438 + 50.0 * 8.323460578918457
Epoch 2960, val loss: 0.5623283982276917
Epoch 2970, training loss: 416.3684387207031 = 0.12616665661334991 + 50.0 * 8.324845314025879
Epoch 2970, val loss: 0.5638353228569031
Epoch 2980, training loss: 416.240478515625 = 0.12492065131664276 + 50.0 * 8.322311401367188
Epoch 2980, val loss: 0.5661303400993347
Epoch 2990, training loss: 416.1878662109375 = 0.12368495017290115 + 50.0 * 8.321283340454102
Epoch 2990, val loss: 0.567285418510437
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7884322678843226
0.8423531116423967
=== training gcn model ===
Epoch 0, training loss: 530.2125244140625 = 1.0997350215911865 + 50.0 * 10.582256317138672
Epoch 0, val loss: 1.1001317501068115
Epoch 10, training loss: 530.1506958007812 = 1.0932990312576294 + 50.0 * 10.581147193908691
Epoch 10, val loss: 1.0936241149902344
Epoch 20, training loss: 529.6572265625 = 1.085716724395752 + 50.0 * 10.571430206298828
Epoch 20, val loss: 1.0859163999557495
Epoch 30, training loss: 526.3583984375 = 1.0764079093933105 + 50.0 * 10.50563907623291
Epoch 30, val loss: 1.0763987302780151
Epoch 40, training loss: 512.4185180664062 = 1.0656262636184692 + 50.0 * 10.227058410644531
Epoch 40, val loss: 1.0655373334884644
Epoch 50, training loss: 484.4824523925781 = 1.0540177822113037 + 50.0 * 9.66856861114502
Epoch 50, val loss: 1.053704023361206
Epoch 60, training loss: 478.01513671875 = 1.0437034368515015 + 50.0 * 9.5394287109375
Epoch 60, val loss: 1.0433776378631592
Epoch 70, training loss: 471.4329528808594 = 1.0344959497451782 + 50.0 * 9.40796947479248
Epoch 70, val loss: 1.0343420505523682
Epoch 80, training loss: 463.1402282714844 = 1.0265588760375977 + 50.0 * 9.242273330688477
Epoch 80, val loss: 1.0267447233200073
Epoch 90, training loss: 461.61273193359375 = 1.0184638500213623 + 50.0 * 9.211885452270508
Epoch 90, val loss: 1.018798828125
Epoch 100, training loss: 459.53717041015625 = 1.0089846849441528 + 50.0 * 9.170563697814941
Epoch 100, val loss: 1.009462594985962
Epoch 110, training loss: 456.9009704589844 = 0.9991394877433777 + 50.0 * 9.118036270141602
Epoch 110, val loss: 0.9999626278877258
Epoch 120, training loss: 451.916748046875 = 0.9895223379135132 + 50.0 * 9.01854419708252
Epoch 120, val loss: 0.9909439086914062
Epoch 130, training loss: 448.8412170410156 = 0.9800820350646973 + 50.0 * 8.957222938537598
Epoch 130, val loss: 0.9818663001060486
Epoch 140, training loss: 445.5903625488281 = 0.9691690802574158 + 50.0 * 8.892423629760742
Epoch 140, val loss: 0.9709396958351135
Epoch 150, training loss: 442.70263671875 = 0.9578291773796082 + 50.0 * 8.834896087646484
Epoch 150, val loss: 0.9601134657859802
Epoch 160, training loss: 440.9274597167969 = 0.9459429383277893 + 50.0 * 8.799630165100098
Epoch 160, val loss: 0.9486563205718994
Epoch 170, training loss: 438.90472412109375 = 0.9326266646385193 + 50.0 * 8.759442329406738
Epoch 170, val loss: 0.9358901381492615
Epoch 180, training loss: 436.8585205078125 = 0.9185798764228821 + 50.0 * 8.718798637390137
Epoch 180, val loss: 0.9224550724029541
Epoch 190, training loss: 435.2456970214844 = 0.903830885887146 + 50.0 * 8.686837196350098
Epoch 190, val loss: 0.9083215594291687
Epoch 200, training loss: 434.03802490234375 = 0.8877758383750916 + 50.0 * 8.663004875183105
Epoch 200, val loss: 0.8929064869880676
Epoch 210, training loss: 433.19720458984375 = 0.8699216842651367 + 50.0 * 8.64654541015625
Epoch 210, val loss: 0.8757027387619019
Epoch 220, training loss: 432.3625793457031 = 0.8507294654846191 + 50.0 * 8.630236625671387
Epoch 220, val loss: 0.8573697805404663
Epoch 230, training loss: 431.6117248535156 = 0.8309423923492432 + 50.0 * 8.615615844726562
Epoch 230, val loss: 0.838518500328064
Epoch 240, training loss: 431.49493408203125 = 0.8108419179916382 + 50.0 * 8.61368179321289
Epoch 240, val loss: 0.8193815350532532
Epoch 250, training loss: 430.50115966796875 = 0.7902918457984924 + 50.0 * 8.594217300415039
Epoch 250, val loss: 0.7998257279396057
Epoch 260, training loss: 429.92352294921875 = 0.7698008418083191 + 50.0 * 8.583074569702148
Epoch 260, val loss: 0.7803430557250977
Epoch 270, training loss: 429.3597106933594 = 0.7495282292366028 + 50.0 * 8.572203636169434
Epoch 270, val loss: 0.7611809968948364
Epoch 280, training loss: 429.4620361328125 = 0.7295705676078796 + 50.0 * 8.5746488571167
Epoch 280, val loss: 0.7423781752586365
Epoch 290, training loss: 428.6203308105469 = 0.7100005149841309 + 50.0 * 8.558206558227539
Epoch 290, val loss: 0.7238455414772034
Epoch 300, training loss: 428.0943298339844 = 0.6912504434585571 + 50.0 * 8.54806137084961
Epoch 300, val loss: 0.7061497569084167
Epoch 310, training loss: 427.6220703125 = 0.6734653115272522 + 50.0 * 8.538971900939941
Epoch 310, val loss: 0.6893836259841919
Epoch 320, training loss: 427.847412109375 = 0.6566700339317322 + 50.0 * 8.543814659118652
Epoch 320, val loss: 0.6735132932662964
Epoch 330, training loss: 426.9432373046875 = 0.640756368637085 + 50.0 * 8.526049613952637
Epoch 330, val loss: 0.6586006879806519
Epoch 340, training loss: 426.57489013671875 = 0.6260709166526794 + 50.0 * 8.518976211547852
Epoch 340, val loss: 0.6448552012443542
Epoch 350, training loss: 426.2712707519531 = 0.6125795841217041 + 50.0 * 8.513174057006836
Epoch 350, val loss: 0.632220983505249
Epoch 360, training loss: 426.3311462402344 = 0.6000833511352539 + 50.0 * 8.514620780944824
Epoch 360, val loss: 0.6206291913986206
Epoch 370, training loss: 425.7143859863281 = 0.5885906219482422 + 50.0 * 8.50251579284668
Epoch 370, val loss: 0.6098731160163879
Epoch 380, training loss: 425.4255676269531 = 0.5782038569450378 + 50.0 * 8.496947288513184
Epoch 380, val loss: 0.6003100872039795
Epoch 390, training loss: 425.15289306640625 = 0.5688009858131409 + 50.0 * 8.491682052612305
Epoch 390, val loss: 0.591683030128479
Epoch 400, training loss: 424.93804931640625 = 0.5602356791496277 + 50.0 * 8.487556457519531
Epoch 400, val loss: 0.583916425704956
Epoch 410, training loss: 424.70989990234375 = 0.5523677468299866 + 50.0 * 8.483150482177734
Epoch 410, val loss: 0.5766845941543579
Epoch 420, training loss: 424.49896240234375 = 0.5451474189758301 + 50.0 * 8.479076385498047
Epoch 420, val loss: 0.5702406167984009
Epoch 430, training loss: 424.3090515136719 = 0.5386489629745483 + 50.0 * 8.475408554077148
Epoch 430, val loss: 0.5645286440849304
Epoch 440, training loss: 424.35009765625 = 0.5327528119087219 + 50.0 * 8.476346969604492
Epoch 440, val loss: 0.5593876838684082
Epoch 450, training loss: 423.9498291015625 = 0.5273029804229736 + 50.0 * 8.468450546264648
Epoch 450, val loss: 0.5547613501548767
Epoch 460, training loss: 423.76544189453125 = 0.5223508477210999 + 50.0 * 8.464861869812012
Epoch 460, val loss: 0.5505222082138062
Epoch 470, training loss: 423.63238525390625 = 0.517793595790863 + 50.0 * 8.462291717529297
Epoch 470, val loss: 0.546750009059906
Epoch 480, training loss: 423.50213623046875 = 0.5135672688484192 + 50.0 * 8.459771156311035
Epoch 480, val loss: 0.5433667898178101
Epoch 490, training loss: 423.378173828125 = 0.5096507668495178 + 50.0 * 8.45737075805664
Epoch 490, val loss: 0.5402640700340271
Epoch 500, training loss: 423.25146484375 = 0.5060215592384338 + 50.0 * 8.454909324645996
Epoch 500, val loss: 0.5374816656112671
Epoch 510, training loss: 422.9651794433594 = 0.5026584267616272 + 50.0 * 8.449250221252441
Epoch 510, val loss: 0.5348089933395386
Epoch 520, training loss: 422.7595520019531 = 0.499550998210907 + 50.0 * 8.445199966430664
Epoch 520, val loss: 0.5325490832328796
Epoch 530, training loss: 422.7156982421875 = 0.4966658055782318 + 50.0 * 8.444380760192871
Epoch 530, val loss: 0.5305029153823853
Epoch 540, training loss: 422.62139892578125 = 0.49393871426582336 + 50.0 * 8.442549705505371
Epoch 540, val loss: 0.5285727381706238
Epoch 550, training loss: 422.3984680175781 = 0.4913736879825592 + 50.0 * 8.438141822814941
Epoch 550, val loss: 0.5267927050590515
Epoch 560, training loss: 422.3907165527344 = 0.48896142840385437 + 50.0 * 8.438035011291504
Epoch 560, val loss: 0.5251829624176025
Epoch 570, training loss: 422.5887451171875 = 0.4866590201854706 + 50.0 * 8.442041397094727
Epoch 570, val loss: 0.5235764384269714
Epoch 580, training loss: 422.1640319824219 = 0.484424352645874 + 50.0 * 8.433591842651367
Epoch 580, val loss: 0.5222159028053284
Epoch 590, training loss: 421.9433288574219 = 0.48236793279647827 + 50.0 * 8.429219245910645
Epoch 590, val loss: 0.5209514498710632
Epoch 600, training loss: 421.8741760253906 = 0.4804355502128601 + 50.0 * 8.427874565124512
Epoch 600, val loss: 0.5198479294776917
Epoch 610, training loss: 422.3610534667969 = 0.4785810112953186 + 50.0 * 8.437649726867676
Epoch 610, val loss: 0.518774688243866
Epoch 620, training loss: 421.6887512207031 = 0.47674450278282166 + 50.0 * 8.424240112304688
Epoch 620, val loss: 0.5176076889038086
Epoch 630, training loss: 421.63641357421875 = 0.4750276505947113 + 50.0 * 8.423227310180664
Epoch 630, val loss: 0.5166478753089905
Epoch 640, training loss: 421.6143493652344 = 0.47340136766433716 + 50.0 * 8.422819137573242
Epoch 640, val loss: 0.515721321105957
Epoch 650, training loss: 421.52899169921875 = 0.4717981219291687 + 50.0 * 8.421143531799316
Epoch 650, val loss: 0.5148568749427795
Epoch 660, training loss: 421.3901062011719 = 0.47024649381637573 + 50.0 * 8.418396949768066
Epoch 660, val loss: 0.514045774936676
Epoch 670, training loss: 421.29754638671875 = 0.4687831401824951 + 50.0 * 8.41657543182373
Epoch 670, val loss: 0.5132704377174377
Epoch 680, training loss: 421.4479675292969 = 0.46737855672836304 + 50.0 * 8.419611930847168
Epoch 680, val loss: 0.5125291347503662
Epoch 690, training loss: 421.3014221191406 = 0.46597781777381897 + 50.0 * 8.416708946228027
Epoch 690, val loss: 0.5118265151977539
Epoch 700, training loss: 421.14239501953125 = 0.4646354019641876 + 50.0 * 8.413555145263672
Epoch 700, val loss: 0.5111391544342041
Epoch 710, training loss: 421.3232727050781 = 0.4633302390575409 + 50.0 * 8.41719913482666
Epoch 710, val loss: 0.5105316042900085
Epoch 720, training loss: 421.0346374511719 = 0.4620422124862671 + 50.0 * 8.411452293395996
Epoch 720, val loss: 0.5097219347953796
Epoch 730, training loss: 420.95819091796875 = 0.46081387996673584 + 50.0 * 8.409947395324707
Epoch 730, val loss: 0.5091372132301331
Epoch 740, training loss: 420.92901611328125 = 0.4596165418624878 + 50.0 * 8.409387588500977
Epoch 740, val loss: 0.5085104703903198
Epoch 750, training loss: 420.9837341308594 = 0.45842400193214417 + 50.0 * 8.410506248474121
Epoch 750, val loss: 0.5078498721122742
Epoch 760, training loss: 420.7674560546875 = 0.4572361409664154 + 50.0 * 8.406204223632812
Epoch 760, val loss: 0.5073285698890686
Epoch 770, training loss: 420.7127685546875 = 0.456101655960083 + 50.0 * 8.405133247375488
Epoch 770, val loss: 0.5068129897117615
Epoch 780, training loss: 420.7947998046875 = 0.4549922049045563 + 50.0 * 8.4067964553833
Epoch 780, val loss: 0.5062708258628845
Epoch 790, training loss: 420.71240234375 = 0.4538750648498535 + 50.0 * 8.405170440673828
Epoch 790, val loss: 0.5057140588760376
Epoch 800, training loss: 420.6572570800781 = 0.45276302099227905 + 50.0 * 8.40408992767334
Epoch 800, val loss: 0.5051387548446655
Epoch 810, training loss: 420.4944763183594 = 0.4516858756542206 + 50.0 * 8.400856018066406
Epoch 810, val loss: 0.5044331550598145
Epoch 820, training loss: 420.41925048828125 = 0.4506389796733856 + 50.0 * 8.399372100830078
Epoch 820, val loss: 0.5040361285209656
Epoch 830, training loss: 420.7370910644531 = 0.44960787892341614 + 50.0 * 8.405749320983887
Epoch 830, val loss: 0.503401517868042
Epoch 840, training loss: 420.5176696777344 = 0.44853419065475464 + 50.0 * 8.401382446289062
Epoch 840, val loss: 0.5028347969055176
Epoch 850, training loss: 420.2662048339844 = 0.4474920332431793 + 50.0 * 8.396374702453613
Epoch 850, val loss: 0.5024768114089966
Epoch 860, training loss: 420.2603759765625 = 0.4464859664440155 + 50.0 * 8.396278381347656
Epoch 860, val loss: 0.5019524097442627
Epoch 870, training loss: 420.39312744140625 = 0.4454789459705353 + 50.0 * 8.398953437805176
Epoch 870, val loss: 0.5015276074409485
Epoch 880, training loss: 420.4214172363281 = 0.4444409906864166 + 50.0 * 8.399539947509766
Epoch 880, val loss: 0.5007768273353577
Epoch 890, training loss: 420.16455078125 = 0.4434013068675995 + 50.0 * 8.394423484802246
Epoch 890, val loss: 0.5003706812858582
Epoch 900, training loss: 420.0495300292969 = 0.4424006938934326 + 50.0 * 8.392142295837402
Epoch 900, val loss: 0.4998571574687958
Epoch 910, training loss: 419.9714050292969 = 0.44141438603401184 + 50.0 * 8.390600204467773
Epoch 910, val loss: 0.499387264251709
Epoch 920, training loss: 420.0377197265625 = 0.4404277205467224 + 50.0 * 8.391945838928223
Epoch 920, val loss: 0.49894267320632935
Epoch 930, training loss: 420.1405944824219 = 0.43939322233200073 + 50.0 * 8.394023895263672
Epoch 930, val loss: 0.4984821677207947
Epoch 940, training loss: 419.9830627441406 = 0.4383283853530884 + 50.0 * 8.390894889831543
Epoch 940, val loss: 0.49774593114852905
Epoch 950, training loss: 419.7844543457031 = 0.4373208284378052 + 50.0 * 8.386942863464355
Epoch 950, val loss: 0.49732255935668945
Epoch 960, training loss: 419.7516784667969 = 0.4363352656364441 + 50.0 * 8.386306762695312
Epoch 960, val loss: 0.49687498807907104
Epoch 970, training loss: 419.74383544921875 = 0.43535253405570984 + 50.0 * 8.38616943359375
Epoch 970, val loss: 0.4963904619216919
Epoch 980, training loss: 420.14813232421875 = 0.4343469440937042 + 50.0 * 8.394275665283203
Epoch 980, val loss: 0.49588829278945923
Epoch 990, training loss: 419.8206787109375 = 0.4332975149154663 + 50.0 * 8.387747764587402
Epoch 990, val loss: 0.4952143728733063
Epoch 1000, training loss: 419.62774658203125 = 0.43226543068885803 + 50.0 * 8.383910179138184
Epoch 1000, val loss: 0.4948243200778961
Epoch 1010, training loss: 419.57000732421875 = 0.4312736690044403 + 50.0 * 8.382774353027344
Epoch 1010, val loss: 0.4943593144416809
Epoch 1020, training loss: 419.5234375 = 0.4302952289581299 + 50.0 * 8.38186264038086
Epoch 1020, val loss: 0.49390655755996704
Epoch 1030, training loss: 419.727783203125 = 0.42931216955184937 + 50.0 * 8.385969161987305
Epoch 1030, val loss: 0.4935632050037384
Epoch 1040, training loss: 419.63201904296875 = 0.4282640814781189 + 50.0 * 8.384075164794922
Epoch 1040, val loss: 0.4929513931274414
Epoch 1050, training loss: 419.4768981933594 = 0.427219957113266 + 50.0 * 8.380993843078613
Epoch 1050, val loss: 0.4923974871635437
Epoch 1060, training loss: 419.38006591796875 = 0.4262109398841858 + 50.0 * 8.379076957702637
Epoch 1060, val loss: 0.4918506443500519
Epoch 1070, training loss: 419.33612060546875 = 0.42521974444389343 + 50.0 * 8.378217697143555
Epoch 1070, val loss: 0.49148115515708923
Epoch 1080, training loss: 419.44512939453125 = 0.4242224097251892 + 50.0 * 8.380417823791504
Epoch 1080, val loss: 0.49097201228141785
Epoch 1090, training loss: 419.2843017578125 = 0.42319098114967346 + 50.0 * 8.377222061157227
Epoch 1090, val loss: 0.49046915769577026
Epoch 1100, training loss: 419.9159240722656 = 0.42216113209724426 + 50.0 * 8.389875411987305
Epoch 1100, val loss: 0.4897763133049011
Epoch 1110, training loss: 419.4112548828125 = 0.4210677742958069 + 50.0 * 8.379803657531738
Epoch 1110, val loss: 0.48948755860328674
Epoch 1120, training loss: 419.2239074707031 = 0.4200250804424286 + 50.0 * 8.376077651977539
Epoch 1120, val loss: 0.4889522194862366
Epoch 1130, training loss: 419.13275146484375 = 0.4190102219581604 + 50.0 * 8.374275207519531
Epoch 1130, val loss: 0.48852530121803284
Epoch 1140, training loss: 419.0928649902344 = 0.41799038648605347 + 50.0 * 8.373497009277344
Epoch 1140, val loss: 0.4880288243293762
Epoch 1150, training loss: 419.0902404785156 = 0.41696038842201233 + 50.0 * 8.373465538024902
Epoch 1150, val loss: 0.48757490515708923
Epoch 1160, training loss: 419.47906494140625 = 0.4159034490585327 + 50.0 * 8.381263732910156
Epoch 1160, val loss: 0.48710915446281433
Epoch 1170, training loss: 419.176513671875 = 0.41479766368865967 + 50.0 * 8.375234603881836
Epoch 1170, val loss: 0.48669663071632385
Epoch 1180, training loss: 419.10870361328125 = 0.413705974817276 + 50.0 * 8.373900413513184
Epoch 1180, val loss: 0.4861556589603424
Epoch 1190, training loss: 418.9745788574219 = 0.41261595487594604 + 50.0 * 8.371238708496094
Epoch 1190, val loss: 0.485600084066391
Epoch 1200, training loss: 418.9948425292969 = 0.4115440249443054 + 50.0 * 8.371665954589844
Epoch 1200, val loss: 0.4851295053958893
Epoch 1210, training loss: 419.08465576171875 = 0.4104672372341156 + 50.0 * 8.373483657836914
Epoch 1210, val loss: 0.4847087264060974
Epoch 1220, training loss: 419.06805419921875 = 0.409340500831604 + 50.0 * 8.373174667358398
Epoch 1220, val loss: 0.48412302136421204
Epoch 1230, training loss: 418.93023681640625 = 0.4082045555114746 + 50.0 * 8.370440483093262
Epoch 1230, val loss: 0.4834197461605072
Epoch 1240, training loss: 418.8403015136719 = 0.40710678696632385 + 50.0 * 8.368663787841797
Epoch 1240, val loss: 0.48300668597221375
Epoch 1250, training loss: 418.85235595703125 = 0.40600648522377014 + 50.0 * 8.368927001953125
Epoch 1250, val loss: 0.4824790060520172
Epoch 1260, training loss: 419.1330871582031 = 0.4048803150653839 + 50.0 * 8.374564170837402
Epoch 1260, val loss: 0.48187193274497986
Epoch 1270, training loss: 418.8882751464844 = 0.4037000834941864 + 50.0 * 8.369691848754883
Epoch 1270, val loss: 0.4813802242279053
Epoch 1280, training loss: 418.763427734375 = 0.4025411009788513 + 50.0 * 8.367218017578125
Epoch 1280, val loss: 0.48088404536247253
Epoch 1290, training loss: 418.7523193359375 = 0.40139997005462646 + 50.0 * 8.367018699645996
Epoch 1290, val loss: 0.48046040534973145
Epoch 1300, training loss: 419.0032043457031 = 0.40025821328163147 + 50.0 * 8.372058868408203
Epoch 1300, val loss: 0.48004135489463806
Epoch 1310, training loss: 418.6622314453125 = 0.3990364670753479 + 50.0 * 8.365263938903809
Epoch 1310, val loss: 0.47927311062812805
Epoch 1320, training loss: 418.63653564453125 = 0.3978564143180847 + 50.0 * 8.364773750305176
Epoch 1320, val loss: 0.478730171918869
Epoch 1330, training loss: 418.6609802246094 = 0.3966929614543915 + 50.0 * 8.365285873413086
Epoch 1330, val loss: 0.47831809520721436
Epoch 1340, training loss: 418.9141845703125 = 0.395504355430603 + 50.0 * 8.370373725891113
Epoch 1340, val loss: 0.477651447057724
Epoch 1350, training loss: 418.5791015625 = 0.39427629113197327 + 50.0 * 8.363696098327637
Epoch 1350, val loss: 0.4772270619869232
Epoch 1360, training loss: 418.5482482910156 = 0.3930675685405731 + 50.0 * 8.363103866577148
Epoch 1360, val loss: 0.4767163395881653
Epoch 1370, training loss: 418.5562438964844 = 0.39185982942581177 + 50.0 * 8.363287925720215
Epoch 1370, val loss: 0.4762327969074249
Epoch 1380, training loss: 418.57177734375 = 0.3906446695327759 + 50.0 * 8.363622665405273
Epoch 1380, val loss: 0.4757835268974304
Epoch 1390, training loss: 418.763427734375 = 0.389387309551239 + 50.0 * 8.367481231689453
Epoch 1390, val loss: 0.47510623931884766
Epoch 1400, training loss: 418.7012023925781 = 0.3880884349346161 + 50.0 * 8.366262435913086
Epoch 1400, val loss: 0.4745385944843292
Epoch 1410, training loss: 418.5132751464844 = 0.3867896497249603 + 50.0 * 8.362529754638672
Epoch 1410, val loss: 0.47409045696258545
Epoch 1420, training loss: 418.4382629394531 = 0.3855256736278534 + 50.0 * 8.361054420471191
Epoch 1420, val loss: 0.4736591875553131
Epoch 1430, training loss: 418.4144592285156 = 0.384253591299057 + 50.0 * 8.360604286193848
Epoch 1430, val loss: 0.4731444716453552
Epoch 1440, training loss: 418.66900634765625 = 0.38297492265701294 + 50.0 * 8.365720748901367
Epoch 1440, val loss: 0.4726962149143219
Epoch 1450, training loss: 418.5511779785156 = 0.3816364109516144 + 50.0 * 8.363390922546387
Epoch 1450, val loss: 0.4722121059894562
Epoch 1460, training loss: 418.4026184082031 = 0.38029444217681885 + 50.0 * 8.36044692993164
Epoch 1460, val loss: 0.47158196568489075
Epoch 1470, training loss: 418.3134460449219 = 0.3789748549461365 + 50.0 * 8.358689308166504
Epoch 1470, val loss: 0.47120770812034607
Epoch 1480, training loss: 418.3191223144531 = 0.3776548504829407 + 50.0 * 8.358829498291016
Epoch 1480, val loss: 0.47069793939590454
Epoch 1490, training loss: 418.8069763183594 = 0.3763195872306824 + 50.0 * 8.368613243103027
Epoch 1490, val loss: 0.4700343906879425
Epoch 1500, training loss: 418.4178466796875 = 0.37489596009254456 + 50.0 * 8.360858917236328
Epoch 1500, val loss: 0.46984103322029114
Epoch 1510, training loss: 418.2803955078125 = 0.37350961565971375 + 50.0 * 8.358138084411621
Epoch 1510, val loss: 0.46941936016082764
Epoch 1520, training loss: 418.22637939453125 = 0.3721311390399933 + 50.0 * 8.357085227966309
Epoch 1520, val loss: 0.4688373804092407
Epoch 1530, training loss: 418.2287902832031 = 0.37075918912887573 + 50.0 * 8.357160568237305
Epoch 1530, val loss: 0.4685053825378418
Epoch 1540, training loss: 418.4144287109375 = 0.36937415599823 + 50.0 * 8.36090087890625
Epoch 1540, val loss: 0.4680834114551544
Epoch 1550, training loss: 418.2981872558594 = 0.3679560422897339 + 50.0 * 8.358604431152344
Epoch 1550, val loss: 0.46783241629600525
Epoch 1560, training loss: 418.30206298828125 = 0.3665158152580261 + 50.0 * 8.358711242675781
Epoch 1560, val loss: 0.46730995178222656
Epoch 1570, training loss: 418.1680603027344 = 0.3650627136230469 + 50.0 * 8.356060028076172
Epoch 1570, val loss: 0.46670833230018616
Epoch 1580, training loss: 418.1199645996094 = 0.36364132165908813 + 50.0 * 8.35512638092041
Epoch 1580, val loss: 0.4663563370704651
Epoch 1590, training loss: 418.1178283691406 = 0.36221304535865784 + 50.0 * 8.355112075805664
Epoch 1590, val loss: 0.4659617841243744
Epoch 1600, training loss: 418.20294189453125 = 0.36076560616493225 + 50.0 * 8.356842994689941
Epoch 1600, val loss: 0.4655287265777588
Epoch 1610, training loss: 418.42950439453125 = 0.3592807948589325 + 50.0 * 8.361404418945312
Epoch 1610, val loss: 0.4649680554866791
Epoch 1620, training loss: 418.1719055175781 = 0.3577699363231659 + 50.0 * 8.356283187866211
Epoch 1620, val loss: 0.4648057818412781
Epoch 1630, training loss: 418.12408447265625 = 0.35625210404396057 + 50.0 * 8.355356216430664
Epoch 1630, val loss: 0.4641704559326172
Epoch 1640, training loss: 418.01611328125 = 0.3547438681125641 + 50.0 * 8.353227615356445
Epoch 1640, val loss: 0.46394282579421997
Epoch 1650, training loss: 417.9693603515625 = 0.3532394468784332 + 50.0 * 8.352322578430176
Epoch 1650, val loss: 0.4635925590991974
Epoch 1660, training loss: 417.9892883300781 = 0.35172685980796814 + 50.0 * 8.352751731872559
Epoch 1660, val loss: 0.46317970752716064
Epoch 1670, training loss: 418.32427978515625 = 0.35019147396087646 + 50.0 * 8.359481811523438
Epoch 1670, val loss: 0.46274667978286743
Epoch 1680, training loss: 418.14007568359375 = 0.34860149025917053 + 50.0 * 8.355829238891602
Epoch 1680, val loss: 0.4623713791370392
Epoch 1690, training loss: 418.0798645019531 = 0.3470173180103302 + 50.0 * 8.354657173156738
Epoch 1690, val loss: 0.4622667133808136
Epoch 1700, training loss: 417.91888427734375 = 0.34543377161026 + 50.0 * 8.351469039916992
Epoch 1700, val loss: 0.46188053488731384
Epoch 1710, training loss: 417.95428466796875 = 0.3438764810562134 + 50.0 * 8.352208137512207
Epoch 1710, val loss: 0.46164342761039734
Epoch 1720, training loss: 418.0632019042969 = 0.3423081934452057 + 50.0 * 8.35441780090332
Epoch 1720, val loss: 0.46142494678497314
Epoch 1730, training loss: 417.8971862792969 = 0.3406940996646881 + 50.0 * 8.351129531860352
Epoch 1730, val loss: 0.46095845103263855
Epoch 1740, training loss: 418.0325622558594 = 0.3390922248363495 + 50.0 * 8.353869438171387
Epoch 1740, val loss: 0.4606916904449463
Epoch 1750, training loss: 417.8316650390625 = 0.3374771475791931 + 50.0 * 8.349884033203125
Epoch 1750, val loss: 0.4605821669101715
Epoch 1760, training loss: 417.9072265625 = 0.33587050437927246 + 50.0 * 8.35142707824707
Epoch 1760, val loss: 0.4604383707046509
Epoch 1770, training loss: 418.07861328125 = 0.3342444896697998 + 50.0 * 8.354887008666992
Epoch 1770, val loss: 0.46034038066864014
Epoch 1780, training loss: 417.8599548339844 = 0.3325403034687042 + 50.0 * 8.350547790527344
Epoch 1780, val loss: 0.4597174823284149
Epoch 1790, training loss: 417.78253173828125 = 0.3308802545070648 + 50.0 * 8.34903335571289
Epoch 1790, val loss: 0.45973068475723267
Epoch 1800, training loss: 417.7975769042969 = 0.32921233773231506 + 50.0 * 8.349367141723633
Epoch 1800, val loss: 0.4594864845275879
Epoch 1810, training loss: 417.991455078125 = 0.3275246024131775 + 50.0 * 8.353279113769531
Epoch 1810, val loss: 0.45932909846305847
Epoch 1820, training loss: 417.79217529296875 = 0.32581478357315063 + 50.0 * 8.349327087402344
Epoch 1820, val loss: 0.4592351019382477
Epoch 1830, training loss: 417.7425231933594 = 0.32409098744392395 + 50.0 * 8.348368644714355
Epoch 1830, val loss: 0.45903289318084717
Epoch 1840, training loss: 417.90191650390625 = 0.3223637044429779 + 50.0 * 8.351591110229492
Epoch 1840, val loss: 0.45876768231391907
Epoch 1850, training loss: 417.8177185058594 = 0.32061031460762024 + 50.0 * 8.349942207336426
Epoch 1850, val loss: 0.45871779322624207
Epoch 1860, training loss: 417.6685791015625 = 0.3188546895980835 + 50.0 * 8.346994400024414
Epoch 1860, val loss: 0.45881447196006775
Epoch 1870, training loss: 417.6629638671875 = 0.3171144425868988 + 50.0 * 8.346917152404785
Epoch 1870, val loss: 0.4588547348976135
Epoch 1880, training loss: 417.66314697265625 = 0.3153706192970276 + 50.0 * 8.346955299377441
Epoch 1880, val loss: 0.45892518758773804
Epoch 1890, training loss: 417.79193115234375 = 0.3136076331138611 + 50.0 * 8.349566459655762
Epoch 1890, val loss: 0.4589215815067291
Epoch 1900, training loss: 417.6609191894531 = 0.3117777407169342 + 50.0 * 8.346982955932617
Epoch 1900, val loss: 0.45861488580703735
Epoch 1910, training loss: 417.96759033203125 = 0.3099671006202698 + 50.0 * 8.35315227508545
Epoch 1910, val loss: 0.4584188759326935
Epoch 1920, training loss: 417.6316833496094 = 0.30812108516693115 + 50.0 * 8.346470832824707
Epoch 1920, val loss: 0.4586075246334076
Epoch 1930, training loss: 417.58453369140625 = 0.3063022196292877 + 50.0 * 8.345564842224121
Epoch 1930, val loss: 0.45871058106422424
Epoch 1940, training loss: 417.5483703613281 = 0.30448710918426514 + 50.0 * 8.344878196716309
Epoch 1940, val loss: 0.45862719416618347
Epoch 1950, training loss: 417.528076171875 = 0.3026827275753021 + 50.0 * 8.344508171081543
Epoch 1950, val loss: 0.4588242173194885
Epoch 1960, training loss: 417.5317077636719 = 0.3008689880371094 + 50.0 * 8.344616889953613
Epoch 1960, val loss: 0.45889848470687866
Epoch 1970, training loss: 417.7685546875 = 0.29905086755752563 + 50.0 * 8.349390029907227
Epoch 1970, val loss: 0.45883414149284363
Epoch 1980, training loss: 417.71441650390625 = 0.29717984795570374 + 50.0 * 8.348344802856445
Epoch 1980, val loss: 0.45927271246910095
Epoch 1990, training loss: 417.6446838378906 = 0.2952965497970581 + 50.0 * 8.3469877243042
Epoch 1990, val loss: 0.4593767821788788
Epoch 2000, training loss: 417.5275573730469 = 0.2934409976005554 + 50.0 * 8.344682693481445
Epoch 2000, val loss: 0.4597776234149933
Epoch 2010, training loss: 417.51873779296875 = 0.29156604409217834 + 50.0 * 8.34454345703125
Epoch 2010, val loss: 0.45971980690956116
Epoch 2020, training loss: 417.50689697265625 = 0.2897174060344696 + 50.0 * 8.344344139099121
Epoch 2020, val loss: 0.460184246301651
Epoch 2030, training loss: 417.5328063964844 = 0.2878398895263672 + 50.0 * 8.34489917755127
Epoch 2030, val loss: 0.46011441946029663
Epoch 2040, training loss: 417.57110595703125 = 0.2859634757041931 + 50.0 * 8.345703125
Epoch 2040, val loss: 0.46025189757347107
Epoch 2050, training loss: 417.4962463378906 = 0.2840732932090759 + 50.0 * 8.344243049621582
Epoch 2050, val loss: 0.4606757164001465
Epoch 2060, training loss: 417.4250793457031 = 0.282195121049881 + 50.0 * 8.342857360839844
Epoch 2060, val loss: 0.46115487813949585
Epoch 2070, training loss: 417.4335632324219 = 0.28032195568084717 + 50.0 * 8.343064308166504
Epoch 2070, val loss: 0.4615024924278259
Epoch 2080, training loss: 417.6282958984375 = 0.2784521281719208 + 50.0 * 8.346997261047363
Epoch 2080, val loss: 0.4618576467037201
Epoch 2090, training loss: 417.44622802734375 = 0.2765354514122009 + 50.0 * 8.343393325805664
Epoch 2090, val loss: 0.46179577708244324
Epoch 2100, training loss: 417.4690856933594 = 0.27464306354522705 + 50.0 * 8.343888282775879
Epoch 2100, val loss: 0.462020605802536
Epoch 2110, training loss: 417.54595947265625 = 0.27275243401527405 + 50.0 * 8.345463752746582
Epoch 2110, val loss: 0.4626765251159668
Epoch 2120, training loss: 417.3258972167969 = 0.2708662450313568 + 50.0 * 8.341100692749023
Epoch 2120, val loss: 0.46312737464904785
Epoch 2130, training loss: 417.33843994140625 = 0.269001305103302 + 50.0 * 8.341388702392578
Epoch 2130, val loss: 0.463594526052475
Epoch 2140, training loss: 417.3039245605469 = 0.26714399456977844 + 50.0 * 8.34073543548584
Epoch 2140, val loss: 0.46412500739097595
Epoch 2150, training loss: 417.4431457519531 = 0.26532602310180664 + 50.0 * 8.34355640411377
Epoch 2150, val loss: 0.4649626612663269
Epoch 2160, training loss: 417.3904113769531 = 0.26343831419944763 + 50.0 * 8.34253978729248
Epoch 2160, val loss: 0.4650925397872925
Epoch 2170, training loss: 417.359619140625 = 0.2615627348423004 + 50.0 * 8.341960906982422
Epoch 2170, val loss: 0.46562185883522034
Epoch 2180, training loss: 417.31646728515625 = 0.25967735052108765 + 50.0 * 8.34113597869873
Epoch 2180, val loss: 0.46555209159851074
Epoch 2190, training loss: 417.3284606933594 = 0.2578440010547638 + 50.0 * 8.341412544250488
Epoch 2190, val loss: 0.46649569272994995
Epoch 2200, training loss: 417.36041259765625 = 0.255996972322464 + 50.0 * 8.342087745666504
Epoch 2200, val loss: 0.46673333644866943
Epoch 2210, training loss: 417.36492919921875 = 0.25415387749671936 + 50.0 * 8.342215538024902
Epoch 2210, val loss: 0.4674587845802307
Epoch 2220, training loss: 417.2460632324219 = 0.2523004710674286 + 50.0 * 8.339875221252441
Epoch 2220, val loss: 0.4679804742336273
Epoch 2230, training loss: 417.52545166015625 = 0.25053682923316956 + 50.0 * 8.345498085021973
Epoch 2230, val loss: 0.4691270887851715
Epoch 2240, training loss: 417.2270812988281 = 0.24861159920692444 + 50.0 * 8.339569091796875
Epoch 2240, val loss: 0.46873238682746887
Epoch 2250, training loss: 417.2159118652344 = 0.24678659439086914 + 50.0 * 8.33938217163086
Epoch 2250, val loss: 0.4695122241973877
Epoch 2260, training loss: 417.17010498046875 = 0.24497006833553314 + 50.0 * 8.338502883911133
Epoch 2260, val loss: 0.47017425298690796
Epoch 2270, training loss: 417.2762145996094 = 0.24316607415676117 + 50.0 * 8.34066104888916
Epoch 2270, val loss: 0.4707842469215393
Epoch 2280, training loss: 417.35113525390625 = 0.24134908616542816 + 50.0 * 8.342195510864258
Epoch 2280, val loss: 0.47145310044288635
Epoch 2290, training loss: 417.3096008300781 = 0.2395164519548416 + 50.0 * 8.341402053833008
Epoch 2290, val loss: 0.4723363518714905
Epoch 2300, training loss: 417.2460021972656 = 0.23770232498645782 + 50.0 * 8.340166091918945
Epoch 2300, val loss: 0.4734015166759491
Epoch 2310, training loss: 417.1307678222656 = 0.2358679473400116 + 50.0 * 8.337898254394531
Epoch 2310, val loss: 0.4738094210624695
Epoch 2320, training loss: 417.16058349609375 = 0.23406417667865753 + 50.0 * 8.338530540466309
Epoch 2320, val loss: 0.47455015778541565
Epoch 2330, training loss: 417.3951721191406 = 0.23232534527778625 + 50.0 * 8.343256950378418
Epoch 2330, val loss: 0.476106196641922
Epoch 2340, training loss: 417.17901611328125 = 0.23047296702861786 + 50.0 * 8.338971138000488
Epoch 2340, val loss: 0.4763256311416626
Epoch 2350, training loss: 417.0860595703125 = 0.22866329550743103 + 50.0 * 8.33714771270752
Epoch 2350, val loss: 0.47676417231559753
Epoch 2360, training loss: 417.0976257324219 = 0.22688452899456024 + 50.0 * 8.337414741516113
Epoch 2360, val loss: 0.47765636444091797
Epoch 2370, training loss: 417.1977233886719 = 0.22512269020080566 + 50.0 * 8.339451789855957
Epoch 2370, val loss: 0.47857216000556946
Epoch 2380, training loss: 417.314453125 = 0.2233642339706421 + 50.0 * 8.341821670532227
Epoch 2380, val loss: 0.47979894280433655
Epoch 2390, training loss: 417.13275146484375 = 0.22159335017204285 + 50.0 * 8.338223457336426
Epoch 2390, val loss: 0.48074275255203247
Epoch 2400, training loss: 417.08984375 = 0.21980354189872742 + 50.0 * 8.337400436401367
Epoch 2400, val loss: 0.4807540476322174
Epoch 2410, training loss: 417.0399169921875 = 0.2180546224117279 + 50.0 * 8.336437225341797
Epoch 2410, val loss: 0.4819154143333435
Epoch 2420, training loss: 417.090087890625 = 0.21631568670272827 + 50.0 * 8.337475776672363
Epoch 2420, val loss: 0.48299717903137207
Epoch 2430, training loss: 417.0416259765625 = 0.21456372737884521 + 50.0 * 8.336541175842285
Epoch 2430, val loss: 0.48389342427253723
Epoch 2440, training loss: 417.2416076660156 = 0.21289406716823578 + 50.0 * 8.340574264526367
Epoch 2440, val loss: 0.48584040999412537
Epoch 2450, training loss: 417.04681396484375 = 0.21107527613639832 + 50.0 * 8.336714744567871
Epoch 2450, val loss: 0.4857090413570404
Epoch 2460, training loss: 417.01806640625 = 0.20933325588703156 + 50.0 * 8.336174964904785
Epoch 2460, val loss: 0.486510306596756
Epoch 2470, training loss: 416.96185302734375 = 0.20762617886066437 + 50.0 * 8.335084915161133
Epoch 2470, val loss: 0.4879167973995209
Epoch 2480, training loss: 416.9539489746094 = 0.2059026062488556 + 50.0 * 8.3349609375
Epoch 2480, val loss: 0.4884676933288574
Epoch 2490, training loss: 417.00335693359375 = 0.20420366525650024 + 50.0 * 8.335983276367188
Epoch 2490, val loss: 0.48992636799812317
Epoch 2500, training loss: 417.2322082519531 = 0.2024931013584137 + 50.0 * 8.340594291687012
Epoch 2500, val loss: 0.49039897322654724
Epoch 2510, training loss: 417.0572509765625 = 0.2007686048746109 + 50.0 * 8.337129592895508
Epoch 2510, val loss: 0.4917083978652954
Epoch 2520, training loss: 416.9438781738281 = 0.19904804229736328 + 50.0 * 8.3348970413208
Epoch 2520, val loss: 0.49291107058525085
Epoch 2530, training loss: 416.93853759765625 = 0.19734898209571838 + 50.0 * 8.334823608398438
Epoch 2530, val loss: 0.4939495921134949
Epoch 2540, training loss: 417.0233154296875 = 0.1956661194562912 + 50.0 * 8.336552619934082
Epoch 2540, val loss: 0.49525728821754456
Epoch 2550, training loss: 417.01007080078125 = 0.1939772516489029 + 50.0 * 8.336321830749512
Epoch 2550, val loss: 0.4962259531021118
Epoch 2560, training loss: 417.0438537597656 = 0.19229340553283691 + 50.0 * 8.337031364440918
Epoch 2560, val loss: 0.4974671006202698
Epoch 2570, training loss: 417.09478759765625 = 0.19067704677581787 + 50.0 * 8.338082313537598
Epoch 2570, val loss: 0.49951761960983276
Epoch 2580, training loss: 417.0216979980469 = 0.18896791338920593 + 50.0 * 8.336654663085938
Epoch 2580, val loss: 0.5000568628311157
Epoch 2590, training loss: 416.8707580566406 = 0.18728800117969513 + 50.0 * 8.333669662475586
Epoch 2590, val loss: 0.5005216598510742
Epoch 2600, training loss: 416.85809326171875 = 0.18565092980861664 + 50.0 * 8.333449363708496
Epoch 2600, val loss: 0.5018365383148193
Epoch 2610, training loss: 416.8244323730469 = 0.18402022123336792 + 50.0 * 8.332808494567871
Epoch 2610, val loss: 0.5030712485313416
Epoch 2620, training loss: 416.84552001953125 = 0.1824018955230713 + 50.0 * 8.33326244354248
Epoch 2620, val loss: 0.5040572881698608
Epoch 2630, training loss: 417.0979919433594 = 0.18081101775169373 + 50.0 * 8.338343620300293
Epoch 2630, val loss: 0.504848062992096
Epoch 2640, training loss: 416.97900390625 = 0.17917554080486298 + 50.0 * 8.335996627807617
Epoch 2640, val loss: 0.5062205791473389
Epoch 2650, training loss: 416.959228515625 = 0.1775413602590561 + 50.0 * 8.335633277893066
Epoch 2650, val loss: 0.5076152086257935
Epoch 2660, training loss: 416.87591552734375 = 0.17593827843666077 + 50.0 * 8.333999633789062
Epoch 2660, val loss: 0.5097628831863403
Epoch 2670, training loss: 416.7849426269531 = 0.17431560158729553 + 50.0 * 8.332212448120117
Epoch 2670, val loss: 0.5106081366539001
Epoch 2680, training loss: 416.81512451171875 = 0.17271903157234192 + 50.0 * 8.332847595214844
Epoch 2680, val loss: 0.5114870667457581
Epoch 2690, training loss: 416.9516296386719 = 0.17112675309181213 + 50.0 * 8.335610389709473
Epoch 2690, val loss: 0.512938916683197
Epoch 2700, training loss: 416.7843017578125 = 0.16953206062316895 + 50.0 * 8.332295417785645
Epoch 2700, val loss: 0.5146003365516663
Epoch 2710, training loss: 416.9597473144531 = 0.16795632243156433 + 50.0 * 8.335835456848145
Epoch 2710, val loss: 0.5157301425933838
Epoch 2720, training loss: 416.83544921875 = 0.1663740873336792 + 50.0 * 8.333381652832031
Epoch 2720, val loss: 0.516302764415741
Epoch 2730, training loss: 416.84039306640625 = 0.16481448709964752 + 50.0 * 8.333511352539062
Epoch 2730, val loss: 0.5177860260009766
Epoch 2740, training loss: 416.829833984375 = 0.16325317323207855 + 50.0 * 8.333332061767578
Epoch 2740, val loss: 0.5191496014595032
Epoch 2750, training loss: 416.7051696777344 = 0.16168589890003204 + 50.0 * 8.330869674682617
Epoch 2750, val loss: 0.5211452841758728
Epoch 2760, training loss: 416.7569580078125 = 0.16016125679016113 + 50.0 * 8.33193588256836
Epoch 2760, val loss: 0.522987961769104
Epoch 2770, training loss: 416.7837829589844 = 0.1586315780878067 + 50.0 * 8.332503318786621
Epoch 2770, val loss: 0.5242118239402771
Epoch 2780, training loss: 417.2124328613281 = 0.1571827083826065 + 50.0 * 8.341104507446289
Epoch 2780, val loss: 0.5244927406311035
Epoch 2790, training loss: 416.7904357910156 = 0.1555931121110916 + 50.0 * 8.332696914672852
Epoch 2790, val loss: 0.5274571776390076
Epoch 2800, training loss: 416.7171325683594 = 0.15408256649971008 + 50.0 * 8.331260681152344
Epoch 2800, val loss: 0.5291040539741516
Epoch 2810, training loss: 416.6788635253906 = 0.15258067846298218 + 50.0 * 8.330525398254395
Epoch 2810, val loss: 0.5302562117576599
Epoch 2820, training loss: 416.67950439453125 = 0.151105135679245 + 50.0 * 8.330568313598633
Epoch 2820, val loss: 0.5321685075759888
Epoch 2830, training loss: 417.0505676269531 = 0.149738147854805 + 50.0 * 8.338016510009766
Epoch 2830, val loss: 0.5350657105445862
Epoch 2840, training loss: 416.71453857421875 = 0.14816869795322418 + 50.0 * 8.331327438354492
Epoch 2840, val loss: 0.5351645946502686
Epoch 2850, training loss: 416.64202880859375 = 0.14669513702392578 + 50.0 * 8.329906463623047
Epoch 2850, val loss: 0.5368801951408386
Epoch 2860, training loss: 416.6318359375 = 0.14524109661579132 + 50.0 * 8.329731941223145
Epoch 2860, val loss: 0.5384275317192078
Epoch 2870, training loss: 416.7215576171875 = 0.1438027322292328 + 50.0 * 8.331555366516113
Epoch 2870, val loss: 0.5397658348083496
Epoch 2880, training loss: 416.87921142578125 = 0.14237156510353088 + 50.0 * 8.334736824035645
Epoch 2880, val loss: 0.5406781435012817
Epoch 2890, training loss: 416.7724609375 = 0.14095930755138397 + 50.0 * 8.332630157470703
Epoch 2890, val loss: 0.5420246124267578
Epoch 2900, training loss: 416.6458740234375 = 0.1395188868045807 + 50.0 * 8.330126762390137
Epoch 2900, val loss: 0.5455456376075745
Epoch 2910, training loss: 416.5875549316406 = 0.13806279003620148 + 50.0 * 8.32898998260498
Epoch 2910, val loss: 0.5465011596679688
Epoch 2920, training loss: 416.6045227050781 = 0.1366683691740036 + 50.0 * 8.329357147216797
Epoch 2920, val loss: 0.5485918521881104
Epoch 2930, training loss: 416.9500732421875 = 0.1353701800107956 + 50.0 * 8.336294174194336
Epoch 2930, val loss: 0.5511516332626343
Epoch 2940, training loss: 416.8061218261719 = 0.13389483094215393 + 50.0 * 8.333444595336914
Epoch 2940, val loss: 0.5502705574035645
Epoch 2950, training loss: 416.66290283203125 = 0.13248862326145172 + 50.0 * 8.330608367919922
Epoch 2950, val loss: 0.5533517599105835
Epoch 2960, training loss: 416.5717468261719 = 0.13110895454883575 + 50.0 * 8.328812599182129
Epoch 2960, val loss: 0.5542528033256531
Epoch 2970, training loss: 416.5494689941406 = 0.12974198162555695 + 50.0 * 8.328394889831543
Epoch 2970, val loss: 0.5565417408943176
Epoch 2980, training loss: 416.6749267578125 = 0.1284460723400116 + 50.0 * 8.33092975616455
Epoch 2980, val loss: 0.5568170547485352
Epoch 2990, training loss: 416.8898010253906 = 0.12711025774478912 + 50.0 * 8.335253715515137
Epoch 2990, val loss: 0.5591778755187988
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8077118214104515
0.8414837354198363
=== training gcn model ===
Epoch 0, training loss: 530.2205200195312 = 1.1069656610488892 + 50.0 * 10.582270622253418
Epoch 0, val loss: 1.1062418222427368
Epoch 10, training loss: 530.1671752929688 = 1.1003694534301758 + 50.0 * 10.581336975097656
Epoch 10, val loss: 1.09962797164917
Epoch 20, training loss: 529.72900390625 = 1.0925935506820679 + 50.0 * 10.572728157043457
Epoch 20, val loss: 1.091770052909851
Epoch 30, training loss: 526.9312744140625 = 1.0828665494918823 + 50.0 * 10.5169677734375
Epoch 30, val loss: 1.081894874572754
Epoch 40, training loss: 517.2845458984375 = 1.07206392288208 + 50.0 * 10.324249267578125
Epoch 40, val loss: 1.071287751197815
Epoch 50, training loss: 504.91943359375 = 1.0622522830963135 + 50.0 * 10.077143669128418
Epoch 50, val loss: 1.061533808708191
Epoch 60, training loss: 488.5354309082031 = 1.0534549951553345 + 50.0 * 9.749639511108398
Epoch 60, val loss: 1.052424430847168
Epoch 70, training loss: 471.43048095703125 = 1.0443692207336426 + 50.0 * 9.407722473144531
Epoch 70, val loss: 1.0431939363479614
Epoch 80, training loss: 463.73529052734375 = 1.0340701341629028 + 50.0 * 9.254024505615234
Epoch 80, val loss: 1.0330740213394165
Epoch 90, training loss: 462.0152587890625 = 1.024630069732666 + 50.0 * 9.219812393188477
Epoch 90, val loss: 1.0238300561904907
Epoch 100, training loss: 460.56890869140625 = 1.01537024974823 + 50.0 * 9.191070556640625
Epoch 100, val loss: 1.0146329402923584
Epoch 110, training loss: 458.67633056640625 = 1.0061604976654053 + 50.0 * 9.153403282165527
Epoch 110, val loss: 1.0055086612701416
Epoch 120, training loss: 456.1854248046875 = 0.9972510933876038 + 50.0 * 9.103763580322266
Epoch 120, val loss: 0.9967312812805176
Epoch 130, training loss: 451.7723388671875 = 0.9890336990356445 + 50.0 * 9.015666007995605
Epoch 130, val loss: 0.9887659549713135
Epoch 140, training loss: 446.2557067871094 = 0.9823052287101746 + 50.0 * 8.905467987060547
Epoch 140, val loss: 0.9822552800178528
Epoch 150, training loss: 442.6822509765625 = 0.9748875498771667 + 50.0 * 8.834147453308105
Epoch 150, val loss: 0.9747611284255981
Epoch 160, training loss: 439.99920654296875 = 0.9650840759277344 + 50.0 * 8.780682563781738
Epoch 160, val loss: 0.9648312330245972
Epoch 170, training loss: 437.68731689453125 = 0.9537864923477173 + 50.0 * 8.734670639038086
Epoch 170, val loss: 0.9534915089607239
Epoch 180, training loss: 435.84527587890625 = 0.9415646195411682 + 50.0 * 8.698074340820312
Epoch 180, val loss: 0.9411398768424988
Epoch 190, training loss: 434.0435485839844 = 0.9282540678977966 + 50.0 * 8.66230583190918
Epoch 190, val loss: 0.9279334545135498
Epoch 200, training loss: 432.9296875 = 0.9138404726982117 + 50.0 * 8.6403169631958
Epoch 200, val loss: 0.9133623242378235
Epoch 210, training loss: 431.8822326660156 = 0.8980907201766968 + 50.0 * 8.619682312011719
Epoch 210, val loss: 0.8976797461509705
Epoch 220, training loss: 430.8786926269531 = 0.8815827965736389 + 50.0 * 8.599942207336426
Epoch 220, val loss: 0.8812538981437683
Epoch 230, training loss: 430.2940673828125 = 0.8644359707832336 + 50.0 * 8.588592529296875
Epoch 230, val loss: 0.8642516732215881
Epoch 240, training loss: 429.42083740234375 = 0.8463918566703796 + 50.0 * 8.571488380432129
Epoch 240, val loss: 0.8465144634246826
Epoch 250, training loss: 428.7762756347656 = 0.8277881741523743 + 50.0 * 8.558969497680664
Epoch 250, val loss: 0.8282137513160706
Epoch 260, training loss: 428.24261474609375 = 0.8087483048439026 + 50.0 * 8.548677444458008
Epoch 260, val loss: 0.8095106482505798
Epoch 270, training loss: 427.75762939453125 = 0.7893137335777283 + 50.0 * 8.539366722106934
Epoch 270, val loss: 0.7905277609825134
Epoch 280, training loss: 427.56292724609375 = 0.7696502208709717 + 50.0 * 8.535865783691406
Epoch 280, val loss: 0.7713505625724792
Epoch 290, training loss: 427.0885314941406 = 0.7499639391899109 + 50.0 * 8.526771545410156
Epoch 290, val loss: 0.7522175908088684
Epoch 300, training loss: 426.666748046875 = 0.7306868433952332 + 50.0 * 8.518721580505371
Epoch 300, val loss: 0.7335535883903503
Epoch 310, training loss: 426.25384521484375 = 0.7118976712226868 + 50.0 * 8.510839462280273
Epoch 310, val loss: 0.715491533279419
Epoch 320, training loss: 425.96697998046875 = 0.6937445998191833 + 50.0 * 8.505464553833008
Epoch 320, val loss: 0.6980250477790833
Epoch 330, training loss: 425.62353515625 = 0.6764324903488159 + 50.0 * 8.498942375183105
Epoch 330, val loss: 0.6814008951187134
Epoch 340, training loss: 425.61187744140625 = 0.6600220203399658 + 50.0 * 8.49903678894043
Epoch 340, val loss: 0.6655897498130798
Epoch 350, training loss: 425.11810302734375 = 0.6444435715675354 + 50.0 * 8.489473342895508
Epoch 350, val loss: 0.6509175896644592
Epoch 360, training loss: 424.8009033203125 = 0.6300600171089172 + 50.0 * 8.483416557312012
Epoch 360, val loss: 0.6372822523117065
Epoch 370, training loss: 424.5611267089844 = 0.6167474985122681 + 50.0 * 8.478887557983398
Epoch 370, val loss: 0.6246904134750366
Epoch 380, training loss: 424.3337097167969 = 0.6044390201568604 + 50.0 * 8.47458553314209
Epoch 380, val loss: 0.6131944060325623
Epoch 390, training loss: 424.28887939453125 = 0.5931352376937866 + 50.0 * 8.473915100097656
Epoch 390, val loss: 0.6027238368988037
Epoch 400, training loss: 424.2261657714844 = 0.5827330946922302 + 50.0 * 8.472868919372559
Epoch 400, val loss: 0.5929816961288452
Epoch 410, training loss: 423.853515625 = 0.5731306672096252 + 50.0 * 8.465607643127441
Epoch 410, val loss: 0.5842741131782532
Epoch 420, training loss: 423.60528564453125 = 0.5644575953483582 + 50.0 * 8.460816383361816
Epoch 420, val loss: 0.5764439702033997
Epoch 430, training loss: 423.4445495605469 = 0.556545615196228 + 50.0 * 8.457759857177734
Epoch 430, val loss: 0.5693787336349487
Epoch 440, training loss: 423.4341125488281 = 0.5492467284202576 + 50.0 * 8.457696914672852
Epoch 440, val loss: 0.562934398651123
Epoch 450, training loss: 423.2232666015625 = 0.5424965620040894 + 50.0 * 8.453615188598633
Epoch 450, val loss: 0.5571253895759583
Epoch 460, training loss: 423.03656005859375 = 0.5363235473632812 + 50.0 * 8.450004577636719
Epoch 460, val loss: 0.5518491864204407
Epoch 470, training loss: 423.4322814941406 = 0.5306416749954224 + 50.0 * 8.458032608032227
Epoch 470, val loss: 0.547039270401001
Epoch 480, training loss: 422.8744812011719 = 0.5252708196640015 + 50.0 * 8.44698429107666
Epoch 480, val loss: 0.5426881313323975
Epoch 490, training loss: 422.64117431640625 = 0.5203508138656616 + 50.0 * 8.442416191101074
Epoch 490, val loss: 0.538757860660553
Epoch 500, training loss: 422.527587890625 = 0.5157822370529175 + 50.0 * 8.44023609161377
Epoch 500, val loss: 0.535252034664154
Epoch 510, training loss: 422.4110107421875 = 0.5115023851394653 + 50.0 * 8.437990188598633
Epoch 510, val loss: 0.5319976806640625
Epoch 520, training loss: 422.7340087890625 = 0.5074537396430969 + 50.0 * 8.444531440734863
Epoch 520, val loss: 0.5289286971092224
Epoch 530, training loss: 422.28125 = 0.5035724639892578 + 50.0 * 8.435553550720215
Epoch 530, val loss: 0.526114821434021
Epoch 540, training loss: 422.1222229003906 = 0.4999447464942932 + 50.0 * 8.432445526123047
Epoch 540, val loss: 0.523537814617157
Epoch 550, training loss: 422.0050964355469 = 0.4965389370918274 + 50.0 * 8.430171012878418
Epoch 550, val loss: 0.5211808681488037
Epoch 560, training loss: 422.01312255859375 = 0.4933127462863922 + 50.0 * 8.43039608001709
Epoch 560, val loss: 0.5189488530158997
Epoch 570, training loss: 422.1064147949219 = 0.49019360542297363 + 50.0 * 8.432324409484863
Epoch 570, val loss: 0.5169930458068848
Epoch 580, training loss: 421.8193664550781 = 0.487209290266037 + 50.0 * 8.426643371582031
Epoch 580, val loss: 0.5150231719017029
Epoch 590, training loss: 421.6794738769531 = 0.4844070374965668 + 50.0 * 8.423901557922363
Epoch 590, val loss: 0.5132497549057007
Epoch 600, training loss: 421.5691223144531 = 0.48174285888671875 + 50.0 * 8.421747207641602
Epoch 600, val loss: 0.511576771736145
Epoch 610, training loss: 421.5831298828125 = 0.4791898727416992 + 50.0 * 8.422079086303711
Epoch 610, val loss: 0.5100444555282593
Epoch 620, training loss: 421.5737609863281 = 0.4767056107521057 + 50.0 * 8.421940803527832
Epoch 620, val loss: 0.5084272027015686
Epoch 630, training loss: 421.51611328125 = 0.47431260347366333 + 50.0 * 8.420836448669434
Epoch 630, val loss: 0.5070032477378845
Epoch 640, training loss: 421.3277893066406 = 0.47199445962905884 + 50.0 * 8.417116165161133
Epoch 640, val loss: 0.5057170987129211
Epoch 650, training loss: 421.1992492675781 = 0.4697711169719696 + 50.0 * 8.414589881896973
Epoch 650, val loss: 0.5043550133705139
Epoch 660, training loss: 421.1283874511719 = 0.4676465392112732 + 50.0 * 8.413214683532715
Epoch 660, val loss: 0.5031349062919617
Epoch 670, training loss: 421.0892028808594 = 0.4655963182449341 + 50.0 * 8.412471771240234
Epoch 670, val loss: 0.5019933581352234
Epoch 680, training loss: 421.3172607421875 = 0.4635966718196869 + 50.0 * 8.417073249816895
Epoch 680, val loss: 0.5008725523948669
Epoch 690, training loss: 421.02532958984375 = 0.4616257846355438 + 50.0 * 8.411273956298828
Epoch 690, val loss: 0.4997710883617401
Epoch 700, training loss: 420.8716735839844 = 0.4597342908382416 + 50.0 * 8.408238410949707
Epoch 700, val loss: 0.49871766567230225
Epoch 710, training loss: 420.9805603027344 = 0.4579203426837921 + 50.0 * 8.410452842712402
Epoch 710, val loss: 0.49771344661712646
Epoch 720, training loss: 420.7589416503906 = 0.4561172425746918 + 50.0 * 8.40605640411377
Epoch 720, val loss: 0.49678969383239746
Epoch 730, training loss: 420.701171875 = 0.4543822705745697 + 50.0 * 8.404935836791992
Epoch 730, val loss: 0.4958244860172272
Epoch 740, training loss: 420.6425476074219 = 0.45270776748657227 + 50.0 * 8.403797149658203
Epoch 740, val loss: 0.49495574831962585
Epoch 750, training loss: 420.8232421875 = 0.45105811953544617 + 50.0 * 8.40744400024414
Epoch 750, val loss: 0.4941045939922333
Epoch 760, training loss: 420.5149841308594 = 0.44941139221191406 + 50.0 * 8.401311874389648
Epoch 760, val loss: 0.493213951587677
Epoch 770, training loss: 420.4510498046875 = 0.44782981276512146 + 50.0 * 8.400064468383789
Epoch 770, val loss: 0.4924032390117645
Epoch 780, training loss: 420.4234313964844 = 0.4463009238243103 + 50.0 * 8.399542808532715
Epoch 780, val loss: 0.4916262626647949
Epoch 790, training loss: 420.5145263671875 = 0.44478482007980347 + 50.0 * 8.401394844055176
Epoch 790, val loss: 0.4908553659915924
Epoch 800, training loss: 420.2968444824219 = 0.4432784616947174 + 50.0 * 8.397071838378906
Epoch 800, val loss: 0.49007847905158997
Epoch 810, training loss: 420.2437438964844 = 0.44182899594306946 + 50.0 * 8.396038055419922
Epoch 810, val loss: 0.48935285210609436
Epoch 820, training loss: 420.18841552734375 = 0.4404146075248718 + 50.0 * 8.394959449768066
Epoch 820, val loss: 0.48866036534309387
Epoch 830, training loss: 420.33502197265625 = 0.43900975584983826 + 50.0 * 8.397920608520508
Epoch 830, val loss: 0.4879673421382904
Epoch 840, training loss: 420.1396484375 = 0.4375884532928467 + 50.0 * 8.394041061401367
Epoch 840, val loss: 0.48725101351737976
Epoch 850, training loss: 420.0745544433594 = 0.4362046420574188 + 50.0 * 8.392766952514648
Epoch 850, val loss: 0.486557275056839
Epoch 860, training loss: 419.9623718261719 = 0.4348646402359009 + 50.0 * 8.390549659729004
Epoch 860, val loss: 0.48592403531074524
Epoch 870, training loss: 420.008544921875 = 0.4335554838180542 + 50.0 * 8.391499519348145
Epoch 870, val loss: 0.48532533645629883
Epoch 880, training loss: 420.0184631347656 = 0.4322204887866974 + 50.0 * 8.391724586486816
Epoch 880, val loss: 0.4845949411392212
Epoch 890, training loss: 419.9253234863281 = 0.43089213967323303 + 50.0 * 8.389888763427734
Epoch 890, val loss: 0.4840438961982727
Epoch 900, training loss: 419.81268310546875 = 0.4296122193336487 + 50.0 * 8.38766098022461
Epoch 900, val loss: 0.4833565354347229
Epoch 910, training loss: 419.7492980957031 = 0.42837023735046387 + 50.0 * 8.386418342590332
Epoch 910, val loss: 0.48280173540115356
Epoch 920, training loss: 419.8254089355469 = 0.4271461069583893 + 50.0 * 8.387965202331543
Epoch 920, val loss: 0.4821975827217102
Epoch 930, training loss: 419.7170104980469 = 0.42590174078941345 + 50.0 * 8.385822296142578
Epoch 930, val loss: 0.4816606640815735
Epoch 940, training loss: 419.7474670410156 = 0.42466801404953003 + 50.0 * 8.386455535888672
Epoch 940, val loss: 0.48105213046073914
Epoch 950, training loss: 419.82684326171875 = 0.42343923449516296 + 50.0 * 8.388068199157715
Epoch 950, val loss: 0.48049119114875793
Epoch 960, training loss: 419.60150146484375 = 0.42222222685813904 + 50.0 * 8.383585929870605
Epoch 960, val loss: 0.4799307584762573
Epoch 970, training loss: 419.53240966796875 = 0.4210432767868042 + 50.0 * 8.382226943969727
Epoch 970, val loss: 0.47937121987342834
Epoch 980, training loss: 419.4922790527344 = 0.41989123821258545 + 50.0 * 8.381447792053223
Epoch 980, val loss: 0.47886863350868225
Epoch 990, training loss: 419.51513671875 = 0.41875654458999634 + 50.0 * 8.381927490234375
Epoch 990, val loss: 0.47835439443588257
Epoch 1000, training loss: 419.5965576171875 = 0.41760334372520447 + 50.0 * 8.38357925415039
Epoch 1000, val loss: 0.4778657555580139
Epoch 1010, training loss: 419.43634033203125 = 0.41643670201301575 + 50.0 * 8.38039779663086
Epoch 1010, val loss: 0.47729599475860596
Epoch 1020, training loss: 419.37554931640625 = 0.41531720757484436 + 50.0 * 8.379204750061035
Epoch 1020, val loss: 0.4768652319908142
Epoch 1030, training loss: 419.3142395019531 = 0.4142269790172577 + 50.0 * 8.378000259399414
Epoch 1030, val loss: 0.4763997495174408
Epoch 1040, training loss: 419.30181884765625 = 0.41314753890037537 + 50.0 * 8.37777328491211
Epoch 1040, val loss: 0.4759807884693146
Epoch 1050, training loss: 419.5475769042969 = 0.412058025598526 + 50.0 * 8.382710456848145
Epoch 1050, val loss: 0.4755750596523285
Epoch 1060, training loss: 419.43951416015625 = 0.41093844175338745 + 50.0 * 8.380571365356445
Epoch 1060, val loss: 0.4750078022480011
Epoch 1070, training loss: 419.36944580078125 = 0.4098232090473175 + 50.0 * 8.379192352294922
Epoch 1070, val loss: 0.47452524304389954
Epoch 1080, training loss: 419.37945556640625 = 0.4087167978286743 + 50.0 * 8.379414558410645
Epoch 1080, val loss: 0.4741164445877075
Epoch 1090, training loss: 419.21337890625 = 0.4076312780380249 + 50.0 * 8.376114845275879
Epoch 1090, val loss: 0.4736901819705963
Epoch 1100, training loss: 419.1259765625 = 0.4065658152103424 + 50.0 * 8.374388694763184
Epoch 1100, val loss: 0.47321709990501404
Epoch 1110, training loss: 419.0804138183594 = 0.40552034974098206 + 50.0 * 8.37349796295166
Epoch 1110, val loss: 0.47282320261001587
Epoch 1120, training loss: 419.0580139160156 = 0.4044809639453888 + 50.0 * 8.37307071685791
Epoch 1120, val loss: 0.4724452793598175
Epoch 1130, training loss: 419.282958984375 = 0.40343576669692993 + 50.0 * 8.37759017944336
Epoch 1130, val loss: 0.4720388948917389
Epoch 1140, training loss: 419.32952880859375 = 0.4023670554161072 + 50.0 * 8.37854290008545
Epoch 1140, val loss: 0.4716368615627289
Epoch 1150, training loss: 418.9959716796875 = 0.40128010511398315 + 50.0 * 8.371893882751465
Epoch 1150, val loss: 0.47119390964508057
Epoch 1160, training loss: 418.95458984375 = 0.40022820234298706 + 50.0 * 8.371087074279785
Epoch 1160, val loss: 0.4708278775215149
Epoch 1170, training loss: 418.93389892578125 = 0.39919087290763855 + 50.0 * 8.370694160461426
Epoch 1170, val loss: 0.47045236825942993
Epoch 1180, training loss: 418.92510986328125 = 0.39816293120384216 + 50.0 * 8.370538711547852
Epoch 1180, val loss: 0.4700545370578766
Epoch 1190, training loss: 419.15728759765625 = 0.3971351683139801 + 50.0 * 8.375203132629395
Epoch 1190, val loss: 0.4697015583515167
Epoch 1200, training loss: 419.0831604003906 = 0.3960622251033783 + 50.0 * 8.37374210357666
Epoch 1200, val loss: 0.4693242311477661
Epoch 1210, training loss: 418.8309326171875 = 0.3949940502643585 + 50.0 * 8.368719100952148
Epoch 1210, val loss: 0.4689640402793884
Epoch 1220, training loss: 418.8316650390625 = 0.39395198225975037 + 50.0 * 8.368754386901855
Epoch 1220, val loss: 0.46862688660621643
Epoch 1230, training loss: 418.9357604980469 = 0.39291509985923767 + 50.0 * 8.370857238769531
Epoch 1230, val loss: 0.4683210849761963
Epoch 1240, training loss: 418.912109375 = 0.3918595314025879 + 50.0 * 8.370405197143555
Epoch 1240, val loss: 0.4678880274295807
Epoch 1250, training loss: 418.7297668457031 = 0.39079511165618896 + 50.0 * 8.366779327392578
Epoch 1250, val loss: 0.4675697684288025
Epoch 1260, training loss: 418.7324523925781 = 0.38975661993026733 + 50.0 * 8.366853713989258
Epoch 1260, val loss: 0.4671803116798401
Epoch 1270, training loss: 418.8543701171875 = 0.3887088894844055 + 50.0 * 8.36931324005127
Epoch 1270, val loss: 0.4668210744857788
Epoch 1280, training loss: 418.7348937988281 = 0.38766032457351685 + 50.0 * 8.366944313049316
Epoch 1280, val loss: 0.46656396985054016
Epoch 1290, training loss: 418.85711669921875 = 0.38659411668777466 + 50.0 * 8.369410514831543
Epoch 1290, val loss: 0.4661514163017273
Epoch 1300, training loss: 418.6602783203125 = 0.38551267981529236 + 50.0 * 8.365495681762695
Epoch 1300, val loss: 0.46583324670791626
Epoch 1310, training loss: 418.5670166015625 = 0.384461373090744 + 50.0 * 8.363651275634766
Epoch 1310, val loss: 0.4655311405658722
Epoch 1320, training loss: 418.5957946777344 = 0.3834182024002075 + 50.0 * 8.36424732208252
Epoch 1320, val loss: 0.4651918113231659
Epoch 1330, training loss: 418.71197509765625 = 0.38235998153686523 + 50.0 * 8.366592407226562
Epoch 1330, val loss: 0.46489712595939636
Epoch 1340, training loss: 418.5892333984375 = 0.3812759518623352 + 50.0 * 8.364158630371094
Epoch 1340, val loss: 0.4645140469074249
Epoch 1350, training loss: 418.5741271972656 = 0.38020890951156616 + 50.0 * 8.36387825012207
Epoch 1350, val loss: 0.4642205536365509
Epoch 1360, training loss: 418.5276794433594 = 0.37913453578948975 + 50.0 * 8.362971305847168
Epoch 1360, val loss: 0.46390125155448914
Epoch 1370, training loss: 418.42913818359375 = 0.37806084752082825 + 50.0 * 8.361021995544434
Epoch 1370, val loss: 0.4636109471321106
Epoch 1380, training loss: 418.58123779296875 = 0.3769868314266205 + 50.0 * 8.36408519744873
Epoch 1380, val loss: 0.46334007382392883
Epoch 1390, training loss: 418.51776123046875 = 0.3758769929409027 + 50.0 * 8.362837791442871
Epoch 1390, val loss: 0.46303442120552063
Epoch 1400, training loss: 418.47216796875 = 0.3747541308403015 + 50.0 * 8.361948013305664
Epoch 1400, val loss: 0.4626624882221222
Epoch 1410, training loss: 418.4238586425781 = 0.3736461400985718 + 50.0 * 8.361003875732422
Epoch 1410, val loss: 0.46239379048347473
Epoch 1420, training loss: 418.4560546875 = 0.3725382089614868 + 50.0 * 8.36167049407959
Epoch 1420, val loss: 0.4621085524559021
Epoch 1430, training loss: 418.4241638183594 = 0.3714215159416199 + 50.0 * 8.361054420471191
Epoch 1430, val loss: 0.4618056118488312
Epoch 1440, training loss: 418.31610107421875 = 0.3703064024448395 + 50.0 * 8.358916282653809
Epoch 1440, val loss: 0.4615061581134796
Epoch 1450, training loss: 418.28912353515625 = 0.36919939517974854 + 50.0 * 8.3583984375
Epoch 1450, val loss: 0.46120473742485046
Epoch 1460, training loss: 418.5008239746094 = 0.3680901825428009 + 50.0 * 8.362654685974121
Epoch 1460, val loss: 0.46095409989356995
Epoch 1470, training loss: 418.3534240722656 = 0.36694708466529846 + 50.0 * 8.359729766845703
Epoch 1470, val loss: 0.46061384677886963
Epoch 1480, training loss: 418.2955627441406 = 0.3658083975315094 + 50.0 * 8.35859489440918
Epoch 1480, val loss: 0.46026942133903503
Epoch 1490, training loss: 418.28924560546875 = 0.3646690845489502 + 50.0 * 8.358491897583008
Epoch 1490, val loss: 0.460024356842041
Epoch 1500, training loss: 418.2664794921875 = 0.36352160573005676 + 50.0 * 8.35805892944336
Epoch 1500, val loss: 0.45973289012908936
Epoch 1510, training loss: 418.25225830078125 = 0.36236393451690674 + 50.0 * 8.357797622680664
Epoch 1510, val loss: 0.4594590663909912
Epoch 1520, training loss: 418.1474304199219 = 0.3611981272697449 + 50.0 * 8.355724334716797
Epoch 1520, val loss: 0.45919281244277954
Epoch 1530, training loss: 418.2337646484375 = 0.3600378930568695 + 50.0 * 8.357474327087402
Epoch 1530, val loss: 0.4589051306247711
Epoch 1540, training loss: 418.1907043457031 = 0.35885488986968994 + 50.0 * 8.356637001037598
Epoch 1540, val loss: 0.45865848660469055
Epoch 1550, training loss: 418.15972900390625 = 0.35766851902008057 + 50.0 * 8.356040954589844
Epoch 1550, val loss: 0.4585038721561432
Epoch 1560, training loss: 418.2187805175781 = 0.35646867752075195 + 50.0 * 8.357246398925781
Epoch 1560, val loss: 0.45815765857696533
Epoch 1570, training loss: 418.2097473144531 = 0.3552602231502533 + 50.0 * 8.35708999633789
Epoch 1570, val loss: 0.45793238282203674
Epoch 1580, training loss: 418.1157531738281 = 0.35403648018836975 + 50.0 * 8.355234146118164
Epoch 1580, val loss: 0.45773789286613464
Epoch 1590, training loss: 418.0458068847656 = 0.35282135009765625 + 50.0 * 8.353859901428223
Epoch 1590, val loss: 0.457539439201355
Epoch 1600, training loss: 418.0117492675781 = 0.35160425305366516 + 50.0 * 8.353202819824219
Epoch 1600, val loss: 0.4572787284851074
Epoch 1610, training loss: 418.03912353515625 = 0.3503861725330353 + 50.0 * 8.353775024414062
Epoch 1610, val loss: 0.4571078419685364
Epoch 1620, training loss: 418.22576904296875 = 0.34914639592170715 + 50.0 * 8.357532501220703
Epoch 1620, val loss: 0.45685604214668274
Epoch 1630, training loss: 418.0533447265625 = 0.34787386655807495 + 50.0 * 8.354109764099121
Epoch 1630, val loss: 0.456603467464447
Epoch 1640, training loss: 417.9385070800781 = 0.3465912640094757 + 50.0 * 8.351838111877441
Epoch 1640, val loss: 0.4564517140388489
Epoch 1650, training loss: 417.9368896484375 = 0.34531235694885254 + 50.0 * 8.351831436157227
Epoch 1650, val loss: 0.4562113285064697
Epoch 1660, training loss: 418.0584411621094 = 0.3440329432487488 + 50.0 * 8.354288101196289
Epoch 1660, val loss: 0.45602715015411377
Epoch 1670, training loss: 417.9854431152344 = 0.34272149205207825 + 50.0 * 8.35285472869873
Epoch 1670, val loss: 0.455786794424057
Epoch 1680, training loss: 417.9073791503906 = 0.34140315651893616 + 50.0 * 8.351319313049316
Epoch 1680, val loss: 0.455543577671051
Epoch 1690, training loss: 417.9383850097656 = 0.3400842547416687 + 50.0 * 8.35196590423584
Epoch 1690, val loss: 0.4554322063922882
Epoch 1700, training loss: 417.91668701171875 = 0.33875271677970886 + 50.0 * 8.351558685302734
Epoch 1700, val loss: 0.4552425444126129
Epoch 1710, training loss: 417.86474609375 = 0.337405800819397 + 50.0 * 8.350546836853027
Epoch 1710, val loss: 0.455030232667923
Epoch 1720, training loss: 417.9388427734375 = 0.33605727553367615 + 50.0 * 8.352055549621582
Epoch 1720, val loss: 0.4549075961112976
Epoch 1730, training loss: 417.91754150390625 = 0.33467966318130493 + 50.0 * 8.351656913757324
Epoch 1730, val loss: 0.4547138214111328
Epoch 1740, training loss: 417.812744140625 = 0.3332940340042114 + 50.0 * 8.349589347839355
Epoch 1740, val loss: 0.4546007513999939
Epoch 1750, training loss: 417.7685546875 = 0.33190450072288513 + 50.0 * 8.348732948303223
Epoch 1750, val loss: 0.45441532135009766
Epoch 1760, training loss: 417.7700500488281 = 0.33052554726600647 + 50.0 * 8.348790168762207
Epoch 1760, val loss: 0.4542309641838074
Epoch 1770, training loss: 418.0379943847656 = 0.3291332721710205 + 50.0 * 8.354177474975586
Epoch 1770, val loss: 0.4540901482105255
Epoch 1780, training loss: 417.8118896484375 = 0.32769984006881714 + 50.0 * 8.34968376159668
Epoch 1780, val loss: 0.4539400339126587
Epoch 1790, training loss: 417.7986755371094 = 0.32626330852508545 + 50.0 * 8.349448204040527
Epoch 1790, val loss: 0.4538082778453827
Epoch 1800, training loss: 418.01806640625 = 0.3248186707496643 + 50.0 * 8.353864669799805
Epoch 1800, val loss: 0.45365357398986816
Epoch 1810, training loss: 417.7781982421875 = 0.3233448565006256 + 50.0 * 8.34909725189209
Epoch 1810, val loss: 0.45352235436439514
Epoch 1820, training loss: 417.7038269042969 = 0.321878045797348 + 50.0 * 8.347639083862305
Epoch 1820, val loss: 0.4534773826599121
Epoch 1830, training loss: 417.6686096191406 = 0.320405513048172 + 50.0 * 8.346963882446289
Epoch 1830, val loss: 0.45336300134658813
Epoch 1840, training loss: 417.7267761230469 = 0.31892627477645874 + 50.0 * 8.348156929016113
Epoch 1840, val loss: 0.4533284902572632
Epoch 1850, training loss: 417.7768859863281 = 0.3174140155315399 + 50.0 * 8.349189758300781
Epoch 1850, val loss: 0.4531933069229126
Epoch 1860, training loss: 417.7616882324219 = 0.31587955355644226 + 50.0 * 8.348916053771973
Epoch 1860, val loss: 0.4530847370624542
Epoch 1870, training loss: 417.7070617675781 = 0.3143284022808075 + 50.0 * 8.347854614257812
Epoch 1870, val loss: 0.45301100611686707
Epoch 1880, training loss: 417.66802978515625 = 0.31277137994766235 + 50.0 * 8.347105026245117
Epoch 1880, val loss: 0.45295602083206177
Epoch 1890, training loss: 417.60711669921875 = 0.3112054467201233 + 50.0 * 8.345917701721191
Epoch 1890, val loss: 0.45293989777565
Epoch 1900, training loss: 417.6489562988281 = 0.30963099002838135 + 50.0 * 8.346786499023438
Epoch 1900, val loss: 0.45290428400039673
Epoch 1910, training loss: 417.6346740722656 = 0.30803680419921875 + 50.0 * 8.346532821655273
Epoch 1910, val loss: 0.45282408595085144
Epoch 1920, training loss: 417.8033447265625 = 0.3064382076263428 + 50.0 * 8.34993839263916
Epoch 1920, val loss: 0.4527806043624878
Epoch 1930, training loss: 417.5918273925781 = 0.3047850430011749 + 50.0 * 8.345741271972656
Epoch 1930, val loss: 0.4527607858181
Epoch 1940, training loss: 417.54608154296875 = 0.3031400442123413 + 50.0 * 8.34485912322998
Epoch 1940, val loss: 0.4526326358318329
Epoch 1950, training loss: 417.50531005859375 = 0.3015040457248688 + 50.0 * 8.344076156616211
Epoch 1950, val loss: 0.4527079164981842
Epoch 1960, training loss: 417.4898986816406 = 0.29986444115638733 + 50.0 * 8.34380054473877
Epoch 1960, val loss: 0.45275044441223145
Epoch 1970, training loss: 417.50146484375 = 0.2982213795185089 + 50.0 * 8.344064712524414
Epoch 1970, val loss: 0.45279350876808167
Epoch 1980, training loss: 417.861328125 = 0.29657256603240967 + 50.0 * 8.351295471191406
Epoch 1980, val loss: 0.45284247398376465
Epoch 1990, training loss: 417.7082824707031 = 0.2948576807975769 + 50.0 * 8.348268508911133
Epoch 1990, val loss: 0.4526084363460541
Epoch 2000, training loss: 417.4613952636719 = 0.2931404709815979 + 50.0 * 8.343364715576172
Epoch 2000, val loss: 0.45284226536750793
Epoch 2010, training loss: 417.4493408203125 = 0.29143789410591125 + 50.0 * 8.343157768249512
Epoch 2010, val loss: 0.45289096236228943
Epoch 2020, training loss: 417.76275634765625 = 0.2897477149963379 + 50.0 * 8.34946060180664
Epoch 2020, val loss: 0.4529459774494171
Epoch 2030, training loss: 417.43780517578125 = 0.28797394037246704 + 50.0 * 8.342996597290039
Epoch 2030, val loss: 0.4530198872089386
Epoch 2040, training loss: 417.41522216796875 = 0.2862209379673004 + 50.0 * 8.34257984161377
Epoch 2040, val loss: 0.4531586468219757
Epoch 2050, training loss: 417.3862609863281 = 0.284473180770874 + 50.0 * 8.342035293579102
Epoch 2050, val loss: 0.45331430435180664
Epoch 2060, training loss: 417.3663024902344 = 0.2827194631099701 + 50.0 * 8.34167194366455
Epoch 2060, val loss: 0.45347779989242554
Epoch 2070, training loss: 417.4693298339844 = 0.28096437454223633 + 50.0 * 8.343767166137695
Epoch 2070, val loss: 0.45365312695503235
Epoch 2080, training loss: 417.47979736328125 = 0.27917712926864624 + 50.0 * 8.344012260437012
Epoch 2080, val loss: 0.45382124185562134
Epoch 2090, training loss: 417.4087829589844 = 0.27736973762512207 + 50.0 * 8.342628479003906
Epoch 2090, val loss: 0.4539353847503662
Epoch 2100, training loss: 417.42449951171875 = 0.27556365728378296 + 50.0 * 8.342978477478027
Epoch 2100, val loss: 0.454138845205307
Epoch 2110, training loss: 417.3395080566406 = 0.27374979853630066 + 50.0 * 8.341315269470215
Epoch 2110, val loss: 0.4543059766292572
Epoch 2120, training loss: 417.36773681640625 = 0.27194032073020935 + 50.0 * 8.34191608428955
Epoch 2120, val loss: 0.45455464720726013
Epoch 2130, training loss: 417.3659362792969 = 0.2701350152492523 + 50.0 * 8.34191608428955
Epoch 2130, val loss: 0.4548831880092621
Epoch 2140, training loss: 417.4705810546875 = 0.26832741498947144 + 50.0 * 8.34404468536377
Epoch 2140, val loss: 0.45522961020469666
Epoch 2150, training loss: 417.2931823730469 = 0.2664785385131836 + 50.0 * 8.340534210205078
Epoch 2150, val loss: 0.455272376537323
Epoch 2160, training loss: 417.303466796875 = 0.2646458148956299 + 50.0 * 8.340776443481445
Epoch 2160, val loss: 0.4555708169937134
Epoch 2170, training loss: 417.4046936035156 = 0.26282238960266113 + 50.0 * 8.3428373336792
Epoch 2170, val loss: 0.45604392886161804
Epoch 2180, training loss: 417.34564208984375 = 0.2609710693359375 + 50.0 * 8.341693878173828
Epoch 2180, val loss: 0.45622602105140686
Epoch 2190, training loss: 417.4032287597656 = 0.25911998748779297 + 50.0 * 8.34288215637207
Epoch 2190, val loss: 0.45650240778923035
Epoch 2200, training loss: 417.3174133300781 = 0.25725987553596497 + 50.0 * 8.341202735900879
Epoch 2200, val loss: 0.45675450563430786
Epoch 2210, training loss: 417.36151123046875 = 0.25540611147880554 + 50.0 * 8.342122077941895
Epoch 2210, val loss: 0.45702457427978516
Epoch 2220, training loss: 417.2166442871094 = 0.25354358553886414 + 50.0 * 8.339262008666992
Epoch 2220, val loss: 0.4575153887271881
Epoch 2230, training loss: 417.19769287109375 = 0.25169506669044495 + 50.0 * 8.338919639587402
Epoch 2230, val loss: 0.45779696106910706
Epoch 2240, training loss: 417.2141418457031 = 0.24985240399837494 + 50.0 * 8.339285850524902
Epoch 2240, val loss: 0.45821842551231384
Epoch 2250, training loss: 417.4884033203125 = 0.24801941215991974 + 50.0 * 8.344807624816895
Epoch 2250, val loss: 0.45865926146507263
Epoch 2260, training loss: 417.30029296875 = 0.2461499571800232 + 50.0 * 8.341082572937012
Epoch 2260, val loss: 0.45899951457977295
Epoch 2270, training loss: 417.1915283203125 = 0.24427922070026398 + 50.0 * 8.338944435119629
Epoch 2270, val loss: 0.459721177816391
Epoch 2280, training loss: 417.13299560546875 = 0.24241942167282104 + 50.0 * 8.337811470031738
Epoch 2280, val loss: 0.46004369854927063
Epoch 2290, training loss: 417.17156982421875 = 0.24057723581790924 + 50.0 * 8.33862018585205
Epoch 2290, val loss: 0.46066704392433167
