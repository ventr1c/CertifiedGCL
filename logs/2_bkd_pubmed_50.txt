Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 805.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 801.69 MiB free; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 797.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 801.69 MiB free; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 797.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 797.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 797.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 801.69 MiB free; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 801.69 MiB free; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 523.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 523.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 523.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0645751953125 = 1.1137841939926147 + 50.0 * 10.359016418457031
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 197.69 MiB free; 6.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.05859375 = 1.1124480962753296 + 50.0 * 10.358922958374023
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 197.69 MiB free; 6.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0477294921875 = 1.0969890356063843 + 50.0 * 10.359015464782715
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 197.69 MiB free; 6.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0517578125 = 1.0976754426956177 + 50.0 * 10.359082221984863
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 121.69 MiB free; 6.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0420532226562 = 1.0906113386154175 + 50.0 * 10.359028816223145
Epoch 0, val loss: 1.089429497718811
Epoch 10, training loss: 518.8194580078125 = 1.0812844038009644 + 50.0 * 10.35476303100586
Epoch 10, val loss: 1.0799472332000732
Epoch 20, training loss: 515.7723999023438 = 1.0696518421173096 + 50.0 * 10.294054985046387
Epoch 20, val loss: 1.0686768293380737
Epoch 30, training loss: 496.2684631347656 = 1.0625337362289429 + 50.0 * 9.904118537902832
Epoch 30, val loss: 1.0621334314346313
Epoch 40, training loss: 479.7689514160156 = 1.053713321685791 + 50.0 * 9.574304580688477
Epoch 40, val loss: 1.0526970624923706
Epoch 50, training loss: 473.9915466308594 = 1.0373311042785645 + 50.0 * 9.459084510803223
Epoch 50, val loss: 1.0359894037246704
Epoch 60, training loss: 471.3240966796875 = 1.019239902496338 + 50.0 * 9.406097412109375
Epoch 60, val loss: 1.0183378458023071
Epoch 70, training loss: 469.35614013671875 = 1.0030654668807983 + 50.0 * 9.367061614990234
Epoch 70, val loss: 1.0028756856918335
Epoch 80, training loss: 466.5521240234375 = 0.9914832711219788 + 50.0 * 9.311212539672852
Epoch 80, val loss: 0.9919548034667969
Epoch 90, training loss: 464.251708984375 = 0.9814997315406799 + 50.0 * 9.265403747558594
Epoch 90, val loss: 0.9817841649055481
Epoch 100, training loss: 462.5845642089844 = 0.9647565484046936 + 50.0 * 9.232396125793457
Epoch 100, val loss: 0.9650619626045227
Epoch 110, training loss: 461.02117919921875 = 0.9455642104148865 + 50.0 * 9.201512336730957
Epoch 110, val loss: 0.9466097950935364
Epoch 120, training loss: 460.0005187988281 = 0.9250770211219788 + 50.0 * 9.181509017944336
Epoch 120, val loss: 0.926889181137085
Epoch 130, training loss: 459.33184814453125 = 0.9007419347763062 + 50.0 * 9.168622016906738
Epoch 130, val loss: 0.9031408429145813
Epoch 140, training loss: 458.82049560546875 = 0.8733732104301453 + 50.0 * 9.158942222595215
Epoch 140, val loss: 0.876519501209259
Epoch 150, training loss: 458.2621154785156 = 0.8439969420433044 + 50.0 * 9.148362159729004
Epoch 150, val loss: 0.8483620285987854
Epoch 160, training loss: 457.8753967285156 = 0.8133454918861389 + 50.0 * 9.141241073608398
Epoch 160, val loss: 0.8188261985778809
Epoch 170, training loss: 457.60662841796875 = 0.7813748121261597 + 50.0 * 9.136505126953125
Epoch 170, val loss: 0.7882381677627563
Epoch 180, training loss: 457.2797546386719 = 0.7489452362060547 + 50.0 * 9.130616188049316
Epoch 180, val loss: 0.7576192617416382
Epoch 190, training loss: 456.9723205566406 = 0.717389702796936 + 50.0 * 9.125099182128906
Epoch 190, val loss: 0.7280468940734863
Epoch 200, training loss: 456.7156677246094 = 0.6869909167289734 + 50.0 * 9.120573997497559
Epoch 200, val loss: 0.6998295187950134
Epoch 210, training loss: 456.5281677246094 = 0.657839298248291 + 50.0 * 9.117406845092773
Epoch 210, val loss: 0.6731598377227783
Epoch 220, training loss: 456.30487060546875 = 0.6304638385772705 + 50.0 * 9.11348819732666
Epoch 220, val loss: 0.6485265493392944
Epoch 230, training loss: 456.1318054199219 = 0.6056000590324402 + 50.0 * 9.11052417755127
Epoch 230, val loss: 0.6264891028404236
Epoch 240, training loss: 456.0229187011719 = 0.5829643607139587 + 50.0 * 9.10879898071289
Epoch 240, val loss: 0.6067005395889282
Epoch 250, training loss: 455.8525695800781 = 0.562671959400177 + 50.0 * 9.10579776763916
Epoch 250, val loss: 0.5894383788108826
Epoch 260, training loss: 455.6884765625 = 0.5448318719863892 + 50.0 * 9.102872848510742
Epoch 260, val loss: 0.5744774341583252
Epoch 270, training loss: 455.7489318847656 = 0.5290887355804443 + 50.0 * 9.10439682006836
Epoch 270, val loss: 0.5615187883377075
Epoch 280, training loss: 455.51470947265625 = 0.5150611996650696 + 50.0 * 9.099992752075195
Epoch 280, val loss: 0.5505039095878601
Epoch 290, training loss: 455.38079833984375 = 0.5029863119125366 + 50.0 * 9.097556114196777
Epoch 290, val loss: 0.5411294102668762
Epoch 300, training loss: 455.2842712402344 = 0.49243274331092834 + 50.0 * 9.095836639404297
Epoch 300, val loss: 0.5331208109855652
Epoch 310, training loss: 455.3337707519531 = 0.48305127024650574 + 50.0 * 9.097014427185059
Epoch 310, val loss: 0.5261563658714294
Epoch 320, training loss: 455.15460205078125 = 0.4746320843696594 + 50.0 * 9.093599319458008
Epoch 320, val loss: 0.5201177597045898
Epoch 330, training loss: 455.07232666015625 = 0.46728360652923584 + 50.0 * 9.092101097106934
Epoch 330, val loss: 0.514657199382782
Epoch 340, training loss: 454.99578857421875 = 0.460696280002594 + 50.0 * 9.090702056884766
Epoch 340, val loss: 0.5101414322853088
Epoch 350, training loss: 454.90191650390625 = 0.45480111241340637 + 50.0 * 9.088942527770996
Epoch 350, val loss: 0.5059922933578491
Epoch 360, training loss: 454.9148864746094 = 0.44945794343948364 + 50.0 * 9.089308738708496
Epoch 360, val loss: 0.5021549463272095
Epoch 370, training loss: 454.8687438964844 = 0.4444037675857544 + 50.0 * 9.088486671447754
Epoch 370, val loss: 0.4988924264907837
Epoch 380, training loss: 454.7243957519531 = 0.4398191571235657 + 50.0 * 9.085691452026367
Epoch 380, val loss: 0.495646595954895
Epoch 390, training loss: 454.6410217285156 = 0.4356294870376587 + 50.0 * 9.084107398986816
Epoch 390, val loss: 0.4926356375217438
Epoch 400, training loss: 454.67578125 = 0.43169769644737244 + 50.0 * 9.084881782531738
Epoch 400, val loss: 0.4898768961429596
Epoch 410, training loss: 454.5846862792969 = 0.42785242199897766 + 50.0 * 9.083136558532715
Epoch 410, val loss: 0.4875231087207794
Epoch 420, training loss: 454.5144958496094 = 0.42427152395248413 + 50.0 * 9.081804275512695
Epoch 420, val loss: 0.4847707152366638
Epoch 430, training loss: 454.42401123046875 = 0.4208768606185913 + 50.0 * 9.080062866210938
Epoch 430, val loss: 0.4824364185333252
Epoch 440, training loss: 454.3753967285156 = 0.41763198375701904 + 50.0 * 9.079154968261719
Epoch 440, val loss: 0.4800390601158142
Epoch 450, training loss: 454.330810546875 = 0.41436225175857544 + 50.0 * 9.078329086303711
Epoch 450, val loss: 0.47806549072265625
Epoch 460, training loss: 454.2508850097656 = 0.4112781286239624 + 50.0 * 9.076791763305664
Epoch 460, val loss: 0.4756721258163452
Epoch 470, training loss: 454.3548278808594 = 0.40831390023231506 + 50.0 * 9.078929901123047
Epoch 470, val loss: 0.47366270422935486
Epoch 480, training loss: 454.1994323730469 = 0.40532055497169495 + 50.0 * 9.075881958007812
Epoch 480, val loss: 0.471522718667984
Epoch 490, training loss: 454.1018371582031 = 0.4024890959262848 + 50.0 * 9.073987007141113
Epoch 490, val loss: 0.4692802429199219
Epoch 500, training loss: 454.0665588378906 = 0.39972352981567383 + 50.0 * 9.073336601257324
Epoch 500, val loss: 0.46752431988716125
Epoch 510, training loss: 454.0503234863281 = 0.39693814516067505 + 50.0 * 9.073067665100098
Epoch 510, val loss: 0.46537384390830994
Epoch 520, training loss: 454.0127258300781 = 0.3941967189311981 + 50.0 * 9.072370529174805
Epoch 520, val loss: 0.4634321630001068
Epoch 530, training loss: 453.9336853027344 = 0.39155396819114685 + 50.0 * 9.070842742919922
Epoch 530, val loss: 0.46154212951660156
Epoch 540, training loss: 453.9931335449219 = 0.3889228403568268 + 50.0 * 9.072084426879883
Epoch 540, val loss: 0.45961683988571167
Epoch 550, training loss: 453.8984069824219 = 0.3863154947757721 + 50.0 * 9.070241928100586
Epoch 550, val loss: 0.45772498846054077
Epoch 560, training loss: 453.958740234375 = 0.383695513010025 + 50.0 * 9.071500778198242
Epoch 560, val loss: 0.45574647188186646
Epoch 570, training loss: 453.7947998046875 = 0.38106632232666016 + 50.0 * 9.06827449798584
Epoch 570, val loss: 0.4540184736251831
Epoch 580, training loss: 453.7440185546875 = 0.378552109003067 + 50.0 * 9.067309379577637
Epoch 580, val loss: 0.4522455036640167
Epoch 590, training loss: 453.726318359375 = 0.3760688602924347 + 50.0 * 9.067005157470703
Epoch 590, val loss: 0.4503783583641052
Epoch 600, training loss: 453.6913757324219 = 0.3735243082046509 + 50.0 * 9.066356658935547
Epoch 600, val loss: 0.448660284280777
Epoch 610, training loss: 453.643798828125 = 0.3710186779499054 + 50.0 * 9.065455436706543
Epoch 610, val loss: 0.44691041111946106
Epoch 620, training loss: 453.619873046875 = 0.36854907870292664 + 50.0 * 9.06502628326416
Epoch 620, val loss: 0.4453873038291931
Epoch 630, training loss: 453.6236267089844 = 0.3660358786582947 + 50.0 * 9.065152168273926
Epoch 630, val loss: 0.4434044361114502
Epoch 640, training loss: 453.57708740234375 = 0.3635326623916626 + 50.0 * 9.064270973205566
Epoch 640, val loss: 0.4418504238128662
Epoch 650, training loss: 453.50750732421875 = 0.36108410358428955 + 50.0 * 9.062928199768066
Epoch 650, val loss: 0.4402247369289398
Epoch 660, training loss: 453.528076171875 = 0.3586682379245758 + 50.0 * 9.063387870788574
Epoch 660, val loss: 0.4384552836418152
Epoch 670, training loss: 453.4574279785156 = 0.3561825752258301 + 50.0 * 9.06202507019043
Epoch 670, val loss: 0.4370078146457672
Epoch 680, training loss: 453.5257873535156 = 0.3537372946739197 + 50.0 * 9.063441276550293
Epoch 680, val loss: 0.4353888928890228
Epoch 690, training loss: 453.4143981933594 = 0.35127609968185425 + 50.0 * 9.061262130737305
Epoch 690, val loss: 0.4339909255504608
Epoch 700, training loss: 453.42596435546875 = 0.34887588024139404 + 50.0 * 9.061541557312012
Epoch 700, val loss: 0.432646244764328
Epoch 710, training loss: 453.3721008300781 = 0.346478670835495 + 50.0 * 9.06051254272461
Epoch 710, val loss: 0.4308406412601471
Epoch 720, training loss: 453.3749694824219 = 0.3440950810909271 + 50.0 * 9.060617446899414
Epoch 720, val loss: 0.42946964502334595
Epoch 730, training loss: 453.316650390625 = 0.34171929955482483 + 50.0 * 9.05949878692627
Epoch 730, val loss: 0.4280696511268616
Epoch 740, training loss: 453.3074951171875 = 0.3393876254558563 + 50.0 * 9.059362411499023
Epoch 740, val loss: 0.4265243113040924
Epoch 750, training loss: 453.3182678222656 = 0.33702847361564636 + 50.0 * 9.059624671936035
Epoch 750, val loss: 0.4255136549472809
Epoch 760, training loss: 453.2625427246094 = 0.3346703350543976 + 50.0 * 9.058557510375977
Epoch 760, val loss: 0.42395472526550293
Epoch 770, training loss: 453.2467956542969 = 0.33236417174339294 + 50.0 * 9.05828857421875
Epoch 770, val loss: 0.4226307272911072
Epoch 780, training loss: 453.2135925292969 = 0.33004283905029297 + 50.0 * 9.057670593261719
Epoch 780, val loss: 0.4215255379676819
Epoch 790, training loss: 453.1780090332031 = 0.32775387167930603 + 50.0 * 9.057004928588867
Epoch 790, val loss: 0.42025813460350037
Epoch 800, training loss: 453.154541015625 = 0.32551440596580505 + 50.0 * 9.056580543518066
Epoch 800, val loss: 0.41914498805999756
Epoch 810, training loss: 453.2311096191406 = 0.3232884109020233 + 50.0 * 9.05815601348877
Epoch 810, val loss: 0.4179622232913971
Epoch 820, training loss: 453.23406982421875 = 0.32100558280944824 + 50.0 * 9.058260917663574
Epoch 820, val loss: 0.4171706736087799
Epoch 830, training loss: 453.09857177734375 = 0.31876614689826965 + 50.0 * 9.055596351623535
Epoch 830, val loss: 0.415910005569458
Epoch 840, training loss: 453.0774841308594 = 0.3165925443172455 + 50.0 * 9.055217742919922
Epoch 840, val loss: 0.414854496717453
Epoch 850, training loss: 453.07452392578125 = 0.3144531846046448 + 50.0 * 9.055201530456543
Epoch 850, val loss: 0.4137917459011078
Epoch 860, training loss: 453.1669616699219 = 0.3123090863227844 + 50.0 * 9.057092666625977
Epoch 860, val loss: 0.4126863181591034
Epoch 870, training loss: 453.0432434082031 = 0.310146301984787 + 50.0 * 9.054661750793457
Epoch 870, val loss: 0.41209810972213745
Epoch 880, training loss: 453.015869140625 = 0.30803635716438293 + 50.0 * 9.054156303405762
Epoch 880, val loss: 0.4113732874393463
Epoch 890, training loss: 453.0584716796875 = 0.3059493601322174 + 50.0 * 9.05505084991455
Epoch 890, val loss: 0.4105657935142517
Epoch 900, training loss: 453.03045654296875 = 0.30386438965797424 + 50.0 * 9.054532051086426
Epoch 900, val loss: 0.40995168685913086
Epoch 910, training loss: 452.9963073730469 = 0.30178600549697876 + 50.0 * 9.053890228271484
Epoch 910, val loss: 0.4091237187385559
Epoch 920, training loss: 452.9569091796875 = 0.29973360896110535 + 50.0 * 9.053143501281738
Epoch 920, val loss: 0.4083373546600342
Epoch 930, training loss: 452.9456787109375 = 0.2977137863636017 + 50.0 * 9.052959442138672
Epoch 930, val loss: 0.4079836905002594
Epoch 940, training loss: 452.9759521484375 = 0.2956817150115967 + 50.0 * 9.053605079650879
Epoch 940, val loss: 0.4069939851760864
Epoch 950, training loss: 452.8900451660156 = 0.2936597764492035 + 50.0 * 9.05192756652832
Epoch 950, val loss: 0.40659448504447937
Epoch 960, training loss: 452.8804016113281 = 0.29167625308036804 + 50.0 * 9.051774024963379
Epoch 960, val loss: 0.4058406949043274
Epoch 970, training loss: 452.8650207519531 = 0.28971371054649353 + 50.0 * 9.051506042480469
Epoch 970, val loss: 0.4055056869983673
Epoch 980, training loss: 453.0471496582031 = 0.2877746820449829 + 50.0 * 9.055187225341797
Epoch 980, val loss: 0.4048721194267273
Epoch 990, training loss: 452.858642578125 = 0.2857750952243805 + 50.0 * 9.051457405090332
Epoch 990, val loss: 0.4043917655944824
Epoch 1000, training loss: 452.8117370605469 = 0.28383883833885193 + 50.0 * 9.050558090209961
Epoch 1000, val loss: 0.40416258573532104
Epoch 1010, training loss: 452.8519287109375 = 0.281933069229126 + 50.0 * 9.051400184631348
Epoch 1010, val loss: 0.4033971130847931
Epoch 1020, training loss: 452.78948974609375 = 0.2800053060054779 + 50.0 * 9.050189971923828
Epoch 1020, val loss: 0.4035668969154358
Epoch 1030, training loss: 452.7928161621094 = 0.27810561656951904 + 50.0 * 9.050293922424316
Epoch 1030, val loss: 0.4033530354499817
Epoch 1040, training loss: 452.7774963378906 = 0.27620425820350647 + 50.0 * 9.050025939941406
Epoch 1040, val loss: 0.40270963311195374
Epoch 1050, training loss: 452.798828125 = 0.2743271589279175 + 50.0 * 9.050490379333496
Epoch 1050, val loss: 0.40218105912208557
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.31 GiB already allocated; 111.69 MiB free; 7.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 879.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 879.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 875.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 879.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 875.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 875.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 879.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 941.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.23 GiB already allocated; 273.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 941.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 941.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0661010742188 = 1.1111133098602295 + 50.0 * 10.359100341796875
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 509.69 MiB free; 6.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 941.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 269.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 269.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 645.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.054931640625 = 1.100693941116333 + 50.0 * 10.359084129333496
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 531.69 MiB free; 6.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.05908203125 = 1.1047977209091187 + 50.0 * 10.359086036682129
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 531.69 MiB free; 6.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0633544921875 = 1.1081236600875854 + 50.0 * 10.359105110168457
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 531.69 MiB free; 6.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.038818359375 = 1.0898045301437378 + 50.0 * 10.358981132507324
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 541.69 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0484008789062 = 1.1026912927627563 + 50.0 * 10.358915328979492
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 541.69 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 267.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 267.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 267.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 267.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 227.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 227.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 505.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 505.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 899.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0579223632812 = 1.1035065650939941 + 50.0 * 10.359087944030762
Epoch 0, val loss: 1.102469563484192
Epoch 10, training loss: 518.9024658203125 = 1.0916748046875 + 50.0 * 10.356215476989746
Epoch 10, val loss: 1.0902351140975952
Epoch 20, training loss: 516.7001953125 = 1.0757111310958862 + 50.0 * 10.312490463256836
Epoch 20, val loss: 1.0741724967956543
Epoch 30, training loss: 493.9768371582031 = 1.061663269996643 + 50.0 * 9.85830307006836
Epoch 30, val loss: 1.0603523254394531
Epoch 40, training loss: 481.8836975097656 = 1.0481319427490234 + 50.0 * 9.616711616516113
Epoch 40, val loss: 1.0470606088638306
Epoch 50, training loss: 476.3264465332031 = 1.034156322479248 + 50.0 * 9.50584602355957
Epoch 50, val loss: 1.033559799194336
Epoch 60, training loss: 472.7653503417969 = 1.0206536054611206 + 50.0 * 9.434893608093262
Epoch 60, val loss: 1.0204126834869385
Epoch 70, training loss: 470.9412536621094 = 1.0041764974594116 + 50.0 * 9.398741722106934
Epoch 70, val loss: 1.0042808055877686
Epoch 80, training loss: 468.5934143066406 = 0.9872282147407532 + 50.0 * 9.352124214172363
Epoch 80, val loss: 0.9880250692367554
Epoch 90, training loss: 465.7488708496094 = 0.9709872603416443 + 50.0 * 9.295557975769043
Epoch 90, val loss: 0.972377598285675
Epoch 100, training loss: 463.5368347167969 = 0.9531738758087158 + 50.0 * 9.251672744750977
Epoch 100, val loss: 0.9553382992744446
Epoch 110, training loss: 462.4737243652344 = 0.9323314428329468 + 50.0 * 9.230827331542969
Epoch 110, val loss: 0.9354407787322998
Epoch 120, training loss: 461.80328369140625 = 0.9061530828475952 + 50.0 * 9.217942237854004
Epoch 120, val loss: 0.9103359580039978
Epoch 130, training loss: 461.21282958984375 = 0.8764408826828003 + 50.0 * 9.206727981567383
Epoch 130, val loss: 0.8821301460266113
Epoch 140, training loss: 460.6524353027344 = 0.8462047576904297 + 50.0 * 9.196125030517578
Epoch 140, val loss: 0.8540849685668945
Epoch 150, training loss: 460.1822204589844 = 0.8153624534606934 + 50.0 * 9.187336921691895
Epoch 150, val loss: 0.8257901668548584
Epoch 160, training loss: 459.82391357421875 = 0.7830316424369812 + 50.0 * 9.180817604064941
Epoch 160, val loss: 0.7961613535881042
Epoch 170, training loss: 459.5191345214844 = 0.7500042915344238 + 50.0 * 9.175382614135742
Epoch 170, val loss: 0.7664462327957153
Epoch 180, training loss: 459.23480224609375 = 0.71810382604599 + 50.0 * 9.170333862304688
Epoch 180, val loss: 0.7381988167762756
Epoch 190, training loss: 458.8688049316406 = 0.6880731582641602 + 50.0 * 9.163614273071289
Epoch 190, val loss: 0.7120876908302307
Epoch 200, training loss: 458.5326843261719 = 0.6602441072463989 + 50.0 * 9.157448768615723
Epoch 200, val loss: 0.6881988644599915
Epoch 210, training loss: 458.2635192871094 = 0.6346085071563721 + 50.0 * 9.152578353881836
Epoch 210, val loss: 0.6665473580360413
Epoch 220, training loss: 458.00006103515625 = 0.6107399463653564 + 50.0 * 9.147786140441895
Epoch 220, val loss: 0.6467921733856201
Epoch 230, training loss: 457.8017272949219 = 0.588998556137085 + 50.0 * 9.144254684448242
Epoch 230, val loss: 0.6291738748550415
Epoch 240, training loss: 457.6111145019531 = 0.569526195526123 + 50.0 * 9.14083194732666
Epoch 240, val loss: 0.6137941479682922
Epoch 250, training loss: 457.46258544921875 = 0.5523754954338074 + 50.0 * 9.138204574584961
Epoch 250, val loss: 0.6004769802093506
Epoch 260, training loss: 457.3023376464844 = 0.5375403761863708 + 50.0 * 9.135295867919922
Epoch 260, val loss: 0.5892701745033264
Epoch 270, training loss: 457.1612548828125 = 0.524513304233551 + 50.0 * 9.132735252380371
Epoch 270, val loss: 0.5797316431999207
Epoch 280, training loss: 457.1054382324219 = 0.5129684209823608 + 50.0 * 9.13184928894043
Epoch 280, val loss: 0.5714156031608582
Epoch 290, training loss: 456.98394775390625 = 0.5024262070655823 + 50.0 * 9.129630088806152
Epoch 290, val loss: 0.5639588832855225
Epoch 300, training loss: 456.8324890136719 = 0.4932177662849426 + 50.0 * 9.126785278320312
Epoch 300, val loss: 0.5577616095542908
Epoch 310, training loss: 456.73651123046875 = 0.48506078124046326 + 50.0 * 9.125029563903809
Epoch 310, val loss: 0.5524240732192993
Epoch 320, training loss: 456.6543273925781 = 0.4776456356048584 + 50.0 * 9.123534202575684
Epoch 320, val loss: 0.5476616621017456
Epoch 330, training loss: 456.578125 = 0.4706960618495941 + 50.0 * 9.122148513793945
Epoch 330, val loss: 0.5430367588996887
Epoch 340, training loss: 456.532958984375 = 0.46426981687545776 + 50.0 * 9.121374130249023
Epoch 340, val loss: 0.539070725440979
Epoch 350, training loss: 456.4264831542969 = 0.4584912657737732 + 50.0 * 9.119359970092773
Epoch 350, val loss: 0.535639762878418
Epoch 360, training loss: 456.34051513671875 = 0.4531417191028595 + 50.0 * 9.11774730682373
Epoch 360, val loss: 0.5324414968490601
Epoch 370, training loss: 456.2664489746094 = 0.44813159108161926 + 50.0 * 9.116366386413574
Epoch 370, val loss: 0.5294814109802246
Epoch 380, training loss: 456.3103332519531 = 0.44333362579345703 + 50.0 * 9.117340087890625
Epoch 380, val loss: 0.5266566872596741
Epoch 390, training loss: 456.1468200683594 = 0.4386996328830719 + 50.0 * 9.11416244506836
Epoch 390, val loss: 0.5239113569259644
Epoch 400, training loss: 456.03961181640625 = 0.43444016575813293 + 50.0 * 9.112103462219238
Epoch 400, val loss: 0.5214021801948547
Epoch 410, training loss: 455.9610900878906 = 0.43038609623908997 + 50.0 * 9.110613822937012
Epoch 410, val loss: 0.5190600156784058
Epoch 420, training loss: 456.1832580566406 = 0.42643940448760986 + 50.0 * 9.11513614654541
Epoch 420, val loss: 0.516690194606781
Epoch 430, training loss: 455.9302978515625 = 0.4223119020462036 + 50.0 * 9.110159873962402
Epoch 430, val loss: 0.5142337083816528
Epoch 440, training loss: 455.74658203125 = 0.41849714517593384 + 50.0 * 9.106561660766602
Epoch 440, val loss: 0.5119941234588623
Epoch 450, training loss: 455.7049865722656 = 0.4148494601249695 + 50.0 * 9.105802536010742
Epoch 450, val loss: 0.5098317265510559
Epoch 460, training loss: 455.63702392578125 = 0.4111560583114624 + 50.0 * 9.104516983032227
Epoch 460, val loss: 0.5076096057891846
Epoch 470, training loss: 455.56097412109375 = 0.40750595927238464 + 50.0 * 9.103069305419922
Epoch 470, val loss: 0.5053400993347168
Epoch 480, training loss: 455.5635986328125 = 0.40393030643463135 + 50.0 * 9.103193283081055
Epoch 480, val loss: 0.5032591223716736
Epoch 490, training loss: 455.5657958984375 = 0.40028002858161926 + 50.0 * 9.103310585021973
Epoch 490, val loss: 0.5010469555854797
Epoch 500, training loss: 455.4202880859375 = 0.396654337644577 + 50.0 * 9.100472450256348
Epoch 500, val loss: 0.49858197569847107
Epoch 510, training loss: 455.3431701660156 = 0.3931529223918915 + 50.0 * 9.098999977111816
Epoch 510, val loss: 0.4964890778064728
Epoch 520, training loss: 455.28955078125 = 0.38968637585639954 + 50.0 * 9.097997665405273
Epoch 520, val loss: 0.4943118691444397
Epoch 530, training loss: 455.2472839355469 = 0.38620543479919434 + 50.0 * 9.097221374511719
Epoch 530, val loss: 0.4921431243419647
Epoch 540, training loss: 455.2574462890625 = 0.382686048746109 + 50.0 * 9.097495079040527
Epoch 540, val loss: 0.48986485600471497
Epoch 550, training loss: 455.2356872558594 = 0.379149466753006 + 50.0 * 9.09713077545166
Epoch 550, val loss: 0.48782840371131897
Epoch 560, training loss: 455.230224609375 = 0.3755381405353546 + 50.0 * 9.09709358215332
Epoch 560, val loss: 0.48509982228279114
Epoch 570, training loss: 455.10211181640625 = 0.37205788493156433 + 50.0 * 9.094600677490234
Epoch 570, val loss: 0.4829612374305725
Epoch 580, training loss: 455.0428161621094 = 0.36868560314178467 + 50.0 * 9.093482971191406
Epoch 580, val loss: 0.4808809161186218
Epoch 590, training loss: 454.9931335449219 = 0.36533334851264954 + 50.0 * 9.09255599975586
Epoch 590, val loss: 0.47879770398139954
Epoch 600, training loss: 454.9776611328125 = 0.3619782328605652 + 50.0 * 9.092313766479492
Epoch 600, val loss: 0.476470410823822
Epoch 610, training loss: 455.0992736816406 = 0.35846444964408875 + 50.0 * 9.094816207885742
Epoch 610, val loss: 0.4748227298259735
Epoch 620, training loss: 454.940185546875 = 0.35500994324684143 + 50.0 * 9.091703414916992
Epoch 620, val loss: 0.472327321767807
Epoch 630, training loss: 454.8497009277344 = 0.35173115134239197 + 50.0 * 9.089959144592285
Epoch 630, val loss: 0.47026771306991577
Epoch 640, training loss: 454.8212890625 = 0.34850847721099854 + 50.0 * 9.089455604553223
Epoch 640, val loss: 0.4684564769268036
Epoch 650, training loss: 454.7829284667969 = 0.3452971577644348 + 50.0 * 9.088752746582031
Epoch 650, val loss: 0.46675434708595276
Epoch 660, training loss: 454.7489013671875 = 0.3420899510383606 + 50.0 * 9.088135719299316
Epoch 660, val loss: 0.4650401175022125
Epoch 670, training loss: 455.0006103515625 = 0.3388635814189911 + 50.0 * 9.09323501586914
Epoch 670, val loss: 0.46332183480262756
Epoch 680, training loss: 454.7241516113281 = 0.33555248379707336 + 50.0 * 9.087772369384766
Epoch 680, val loss: 0.46153968572616577
Epoch 690, training loss: 454.661865234375 = 0.3324265778064728 + 50.0 * 9.086588859558105
Epoch 690, val loss: 0.4601265490055084
Epoch 700, training loss: 454.621337890625 = 0.32937443256378174 + 50.0 * 9.08583927154541
Epoch 700, val loss: 0.45871463418006897
Epoch 710, training loss: 454.5899963378906 = 0.32634830474853516 + 50.0 * 9.085272789001465
Epoch 710, val loss: 0.4574410319328308
Epoch 720, training loss: 454.7333679199219 = 0.3233157992362976 + 50.0 * 9.088201522827148
Epoch 720, val loss: 0.4560254216194153
Epoch 730, training loss: 454.6174011230469 = 0.32020366191864014 + 50.0 * 9.085944175720215
Epoch 730, val loss: 0.45480358600616455
Epoch 740, training loss: 454.54168701171875 = 0.3172006905078888 + 50.0 * 9.084489822387695
Epoch 740, val loss: 0.4539308249950409
Epoch 750, training loss: 454.4904479980469 = 0.31428277492523193 + 50.0 * 9.083523750305176
Epoch 750, val loss: 0.45269376039505005
Epoch 760, training loss: 454.49481201171875 = 0.31138160824775696 + 50.0 * 9.08366870880127
Epoch 760, val loss: 0.4517679214477539
Epoch 770, training loss: 454.42462158203125 = 0.3084867596626282 + 50.0 * 9.08232307434082
Epoch 770, val loss: 0.45080798864364624
Epoch 780, training loss: 454.3992004394531 = 0.30563971400260925 + 50.0 * 9.081871032714844
Epoch 780, val loss: 0.4497775137424469
Epoch 790, training loss: 454.57684326171875 = 0.3027905225753784 + 50.0 * 9.085480690002441
Epoch 790, val loss: 0.4488902986049652
Epoch 800, training loss: 454.34771728515625 = 0.2998906672000885 + 50.0 * 9.08095645904541
Epoch 800, val loss: 0.4479847252368927
Epoch 810, training loss: 454.31890869140625 = 0.29713302850723267 + 50.0 * 9.080435752868652
Epoch 810, val loss: 0.44754961133003235
Epoch 820, training loss: 454.2864990234375 = 0.29442235827445984 + 50.0 * 9.079841613769531
Epoch 820, val loss: 0.4469553828239441
Epoch 830, training loss: 454.25146484375 = 0.2917206287384033 + 50.0 * 9.079195022583008
Epoch 830, val loss: 0.4463813006877899
Epoch 840, training loss: 454.2335205078125 = 0.28901833295822144 + 50.0 * 9.078889846801758
Epoch 840, val loss: 0.44580164551734924
Epoch 850, training loss: 454.4146423339844 = 0.28629252314567566 + 50.0 * 9.08256721496582
Epoch 850, val loss: 0.4453555941581726
Epoch 860, training loss: 454.2391052246094 = 0.2835044264793396 + 50.0 * 9.07911205291748
Epoch 860, val loss: 0.4445783793926239
Epoch 870, training loss: 454.2044372558594 = 0.28082969784736633 + 50.0 * 9.078472137451172
Epoch 870, val loss: 0.4443614184856415
Epoch 880, training loss: 454.132568359375 = 0.2781943678855896 + 50.0 * 9.07708740234375
Epoch 880, val loss: 0.44394776225090027
Epoch 890, training loss: 454.11981201171875 = 0.27559828758239746 + 50.0 * 9.076884269714355
Epoch 890, val loss: 0.4439084529876709
Epoch 900, training loss: 454.1391906738281 = 0.27299121022224426 + 50.0 * 9.077323913574219
Epoch 900, val loss: 0.4436527490615845
Epoch 910, training loss: 454.06964111328125 = 0.2703680694103241 + 50.0 * 9.0759859085083
Epoch 910, val loss: 0.44346368312835693
Epoch 920, training loss: 454.0895080566406 = 0.2677771747112274 + 50.0 * 9.076434135437012
Epoch 920, val loss: 0.44319620728492737
Epoch 930, training loss: 454.04937744140625 = 0.2651884853839874 + 50.0 * 9.07568359375
Epoch 930, val loss: 0.4431450366973877
Epoch 940, training loss: 454.0079345703125 = 0.2626168727874756 + 50.0 * 9.074906349182129
Epoch 940, val loss: 0.4431978464126587
Epoch 950, training loss: 454.0133361816406 = 0.26005521416664124 + 50.0 * 9.075065612792969
Epoch 950, val loss: 0.4432111084461212
Epoch 960, training loss: 454.0281066894531 = 0.25750330090522766 + 50.0 * 9.075411796569824
Epoch 960, val loss: 0.4431367516517639
Epoch 970, training loss: 454.00189208984375 = 0.25492823123931885 + 50.0 * 9.074939727783203
Epoch 970, val loss: 0.44324955344200134
Epoch 980, training loss: 453.9256896972656 = 0.2523892819881439 + 50.0 * 9.073466300964355
Epoch 980, val loss: 0.44309717416763306
Epoch 990, training loss: 453.896240234375 = 0.2498876005411148 + 50.0 * 9.072927474975586
Epoch 990, val loss: 0.44339582324028015
Epoch 1000, training loss: 453.8717346191406 = 0.24739284813404083 + 50.0 * 9.072486877441406
Epoch 1000, val loss: 0.443612664937973
Epoch 1010, training loss: 453.9399719238281 = 0.24488872289657593 + 50.0 * 9.073901176452637
Epoch 1010, val loss: 0.4439834654331207
Epoch 1020, training loss: 453.89776611328125 = 0.24234668910503387 + 50.0 * 9.073108673095703
Epoch 1020, val loss: 0.44381582736968994
Epoch 1030, training loss: 453.8285217285156 = 0.23982594907283783 + 50.0 * 9.071773529052734
Epoch 1030, val loss: 0.44454148411750793
Epoch 1040, training loss: 453.8286437988281 = 0.23734891414642334 + 50.0 * 9.071825981140137
Epoch 1040, val loss: 0.4449519217014313
Epoch 1050, training loss: 453.8137512207031 = 0.23485569655895233 + 50.0 * 9.071578025817871
Epoch 1050, val loss: 0.44498297572135925
Epoch 1060, training loss: 453.7518005371094 = 0.2323659509420395 + 50.0 * 9.070388793945312
Epoch 1060, val loss: 0.4455953538417816
Epoch 1070, training loss: 453.72760009765625 = 0.22990459203720093 + 50.0 * 9.069953918457031
Epoch 1070, val loss: 0.44573619961738586
Epoch 1080, training loss: 453.72747802734375 = 0.22744956612586975 + 50.0 * 9.070000648498535
Epoch 1080, val loss: 0.44629889726638794
Epoch 1090, training loss: 453.75799560546875 = 0.22496166825294495 + 50.0 * 9.070660591125488
Epoch 1090, val loss: 0.44724777340888977
Epoch 1100, training loss: 453.66534423828125 = 0.22247418761253357 + 50.0 * 9.068857192993164
Epoch 1100, val loss: 0.44752979278564453
Epoch 1110, training loss: 453.7120666503906 = 0.22003699839115143 + 50.0 * 9.069840431213379
Epoch 1110, val loss: 0.44901350140571594
Epoch 1120, training loss: 453.6722412109375 = 0.2175469845533371 + 50.0 * 9.069093704223633
Epoch 1120, val loss: 0.448783278465271
Epoch 1130, training loss: 453.6388244628906 = 0.215091273188591 + 50.0 * 9.068474769592285
Epoch 1130, val loss: 0.4498058557510376
Epoch 1140, training loss: 453.60028076171875 = 0.2126578837633133 + 50.0 * 9.067752838134766
Epoch 1140, val loss: 0.4506794214248657
Epoch 1150, training loss: 453.57073974609375 = 0.21023029088974 + 50.0 * 9.06721019744873
Epoch 1150, val loss: 0.4514687657356262
Epoch 1160, training loss: 453.8727722167969 = 0.20784173905849457 + 50.0 * 9.073298454284668
Epoch 1160, val loss: 0.4525228440761566
Epoch 1170, training loss: 453.57244873046875 = 0.20531240105628967 + 50.0 * 9.067342758178711
Epoch 1170, val loss: 0.45294612646102905
Epoch 1180, training loss: 453.5507507324219 = 0.20288503170013428 + 50.0 * 9.066957473754883
Epoch 1180, val loss: 0.4542635977268219
Epoch 1190, training loss: 453.51190185546875 = 0.20047664642333984 + 50.0 * 9.066228866577148
Epoch 1190, val loss: 0.45519211888313293
Epoch 1200, training loss: 453.4789733886719 = 0.1980714648962021 + 50.0 * 9.065618515014648
Epoch 1200, val loss: 0.4562731385231018
Epoch 1210, training loss: 453.7298583984375 = 0.19569164514541626 + 50.0 * 9.070683479309082
Epoch 1210, val loss: 0.45759308338165283
Epoch 1220, training loss: 453.5132141113281 = 0.19323255121707916 + 50.0 * 9.066399574279785
Epoch 1220, val loss: 0.45830339193344116
Epoch 1230, training loss: 453.4639892578125 = 0.19083614647388458 + 50.0 * 9.065463066101074
Epoch 1230, val loss: 0.45977702736854553
Epoch 1240, training loss: 453.4110107421875 = 0.18844258785247803 + 50.0 * 9.064451217651367
Epoch 1240, val loss: 0.46097636222839355
Epoch 1250, training loss: 453.4031066894531 = 0.1860562562942505 + 50.0 * 9.064340591430664
Epoch 1250, val loss: 0.46242040395736694
Epoch 1260, training loss: 453.6020202636719 = 0.1837012618780136 + 50.0 * 9.068366050720215
Epoch 1260, val loss: 0.4642406105995178
Epoch 1270, training loss: 453.3782653808594 = 0.18128015100955963 + 50.0 * 9.063940048217773
Epoch 1270, val loss: 0.46470868587493896
Epoch 1280, training loss: 453.36566162109375 = 0.17892014980316162 + 50.0 * 9.063735008239746
Epoch 1280, val loss: 0.4658178389072418
Epoch 1290, training loss: 453.35540771484375 = 0.17658700048923492 + 50.0 * 9.063576698303223
Epoch 1290, val loss: 0.4677296578884125
Epoch 1300, training loss: 453.4522705078125 = 0.1742650866508484 + 50.0 * 9.065560340881348
Epoch 1300, val loss: 0.4694535732269287
Epoch 1310, training loss: 453.3124694824219 = 0.17187714576721191 + 50.0 * 9.062811851501465
Epoch 1310, val loss: 0.4705755114555359
Epoch 1320, training loss: 453.2847900390625 = 0.1695544421672821 + 50.0 * 9.062304496765137
Epoch 1320, val loss: 0.47216862440109253
Epoch 1330, training loss: 453.271484375 = 0.16724911332130432 + 50.0 * 9.062085151672363
Epoch 1330, val loss: 0.47385174036026
Epoch 1340, training loss: 453.2506408691406 = 0.1649361252784729 + 50.0 * 9.061714172363281
Epoch 1340, val loss: 0.4756300151348114
Epoch 1350, training loss: 453.3216247558594 = 0.16268105804920197 + 50.0 * 9.063179016113281
Epoch 1350, val loss: 0.47839146852493286
Epoch 1360, training loss: 453.3857421875 = 0.16037097573280334 + 50.0 * 9.064507484436035
Epoch 1360, val loss: 0.4776313304901123
Epoch 1370, training loss: 453.2225646972656 = 0.15810027718544006 + 50.0 * 9.061288833618164
Epoch 1370, val loss: 0.4800359308719635
Epoch 1380, training loss: 453.200439453125 = 0.15586932003498077 + 50.0 * 9.060891151428223
Epoch 1380, val loss: 0.482543021440506
Epoch 1390, training loss: 453.16754150390625 = 0.15365341305732727 + 50.0 * 9.060277938842773
Epoch 1390, val loss: 0.4842854142189026
Epoch 1400, training loss: 453.15728759765625 = 0.15144948661327362 + 50.0 * 9.0601167678833
Epoch 1400, val loss: 0.48579779267311096
Epoch 1410, training loss: 453.347900390625 = 0.14928336441516876 + 50.0 * 9.063972473144531
Epoch 1410, val loss: 0.48825138807296753
Epoch 1420, training loss: 453.2085266113281 = 0.1470872014760971 + 50.0 * 9.06122875213623
Epoch 1420, val loss: 0.4895325303077698
Epoch 1430, training loss: 453.1083679199219 = 0.1449221521615982 + 50.0 * 9.059268951416016
Epoch 1430, val loss: 0.4915337860584259
Epoch 1440, training loss: 453.10858154296875 = 0.1427917331457138 + 50.0 * 9.05931568145752
Epoch 1440, val loss: 0.49367666244506836
Epoch 1450, training loss: 453.2814636230469 = 0.1406920999288559 + 50.0 * 9.06281566619873
Epoch 1450, val loss: 0.49614909291267395
Epoch 1460, training loss: 453.158203125 = 0.13857510685920715 + 50.0 * 9.060392379760742
Epoch 1460, val loss: 0.49737778306007385
Epoch 1470, training loss: 453.07159423828125 = 0.13646696507930756 + 50.0 * 9.05870246887207
Epoch 1470, val loss: 0.49932077527046204
Epoch 1480, training loss: 453.0614013671875 = 0.1344015747308731 + 50.0 * 9.058540344238281
Epoch 1480, val loss: 0.5016146898269653
Epoch 1490, training loss: 453.1224365234375 = 0.13237576186656952 + 50.0 * 9.05980110168457
Epoch 1490, val loss: 0.5032323002815247
Epoch 1500, training loss: 453.0386047363281 = 0.13032694160938263 + 50.0 * 9.058165550231934
Epoch 1500, val loss: 0.5064327716827393
Epoch 1510, training loss: 453.0831298828125 = 0.12832435965538025 + 50.0 * 9.059096336364746
Epoch 1510, val loss: 0.5083800554275513
Epoch 1520, training loss: 452.9829406738281 = 0.12632399797439575 + 50.0 * 9.057132720947266
Epoch 1520, val loss: 0.5104557871818542
Epoch 1530, training loss: 453.04541015625 = 0.12437219172716141 + 50.0 * 9.05842113494873
Epoch 1530, val loss: 0.513323187828064
Epoch 1540, training loss: 452.9747619628906 = 0.1224130317568779 + 50.0 * 9.057046890258789
Epoch 1540, val loss: 0.5149351358413696
Epoch 1550, training loss: 452.9821472167969 = 0.12049954384565353 + 50.0 * 9.057232856750488
Epoch 1550, val loss: 0.5178235173225403
Epoch 1560, training loss: 452.9291076660156 = 0.11858890950679779 + 50.0 * 9.0562105178833
Epoch 1560, val loss: 0.5198252201080322
Epoch 1570, training loss: 452.9183654785156 = 0.11671005934476852 + 50.0 * 9.05603313446045
Epoch 1570, val loss: 0.5220321416854858
Epoch 1580, training loss: 453.1254577636719 = 0.1148902177810669 + 50.0 * 9.060211181640625
Epoch 1580, val loss: 0.5245755314826965
Epoch 1590, training loss: 452.9626770019531 = 0.11303086578845978 + 50.0 * 9.056992530822754
Epoch 1590, val loss: 0.5281168222427368
Epoch 1600, training loss: 452.89068603515625 = 0.1111924797296524 + 50.0 * 9.05558967590332
Epoch 1600, val loss: 0.5297825336456299
Epoch 1610, training loss: 452.9459533691406 = 0.10944126546382904 + 50.0 * 9.056730270385742
Epoch 1610, val loss: 0.5334298014640808
Epoch 1620, training loss: 452.8646240234375 = 0.1076316311955452 + 50.0 * 9.055139541625977
Epoch 1620, val loss: 0.5348593592643738
Epoch 1630, training loss: 452.8685607910156 = 0.1058918908238411 + 50.0 * 9.055253028869629
Epoch 1630, val loss: 0.5370246767997742
Epoch 1640, training loss: 452.91900634765625 = 0.10417698323726654 + 50.0 * 9.056296348571777
Epoch 1640, val loss: 0.5396060347557068
Epoch 1650, training loss: 452.848388671875 = 0.10247889161109924 + 50.0 * 9.05491828918457
Epoch 1650, val loss: 0.5421391129493713
Epoch 1660, training loss: 452.84765625 = 0.1007981225848198 + 50.0 * 9.054937362670898
Epoch 1660, val loss: 0.5451568365097046
Epoch 1670, training loss: 452.8586120605469 = 0.09916626662015915 + 50.0 * 9.05518913269043
Epoch 1670, val loss: 0.5478160381317139
Epoch 1680, training loss: 452.79315185546875 = 0.09751534461975098 + 50.0 * 9.053913116455078
Epoch 1680, val loss: 0.5515021085739136
Epoch 1690, training loss: 452.7744445800781 = 0.09593147039413452 + 50.0 * 9.053570747375488
Epoch 1690, val loss: 0.5545320510864258
Epoch 1700, training loss: 452.805419921875 = 0.09435522556304932 + 50.0 * 9.054221153259277
Epoch 1700, val loss: 0.5568137168884277
Epoch 1710, training loss: 452.79974365234375 = 0.09282144159078598 + 50.0 * 9.05413818359375
Epoch 1710, val loss: 0.5597641468048096
Epoch 1720, training loss: 452.7643737792969 = 0.09127353131771088 + 50.0 * 9.053462028503418
Epoch 1720, val loss: 0.561721920967102
Epoch 1730, training loss: 452.73504638671875 = 0.0897534191608429 + 50.0 * 9.052906036376953
Epoch 1730, val loss: 0.5655696988105774
Epoch 1740, training loss: 452.7605285644531 = 0.0882795974612236 + 50.0 * 9.053444862365723
Epoch 1740, val loss: 0.5672088861465454
Epoch 1750, training loss: 452.7211608886719 = 0.0867953896522522 + 50.0 * 9.052687644958496
Epoch 1750, val loss: 0.5710875988006592
Epoch 1760, training loss: 452.70538330078125 = 0.0853596180677414 + 50.0 * 9.052400588989258
Epoch 1760, val loss: 0.573195219039917
Epoch 1770, training loss: 452.7255554199219 = 0.08393219858407974 + 50.0 * 9.05283260345459
Epoch 1770, val loss: 0.5774083733558655
Epoch 1780, training loss: 452.7139892578125 = 0.08254066109657288 + 50.0 * 9.052628517150879
Epoch 1780, val loss: 0.5805083513259888
Epoch 1790, training loss: 452.6754150390625 = 0.08116313815116882 + 50.0 * 9.051884651184082
Epoch 1790, val loss: 0.5839561223983765
Epoch 1800, training loss: 452.6588134765625 = 0.07980399578809738 + 50.0 * 9.051580429077148
Epoch 1800, val loss: 0.5857123136520386
Epoch 1810, training loss: 452.6817626953125 = 0.07847432792186737 + 50.0 * 9.0520658493042
Epoch 1810, val loss: 0.5904667973518372
Epoch 1820, training loss: 452.6683349609375 = 0.07715209573507309 + 50.0 * 9.051823616027832
Epoch 1820, val loss: 0.59187912940979
Epoch 1830, training loss: 452.622802734375 = 0.0758548155426979 + 50.0 * 9.050938606262207
Epoch 1830, val loss: 0.595676600933075
Epoch 1840, training loss: 452.60333251953125 = 0.07457757741212845 + 50.0 * 9.050575256347656
Epoch 1840, val loss: 0.598427951335907
Epoch 1850, training loss: 452.7305908203125 = 0.0733526200056076 + 50.0 * 9.053144454956055
Epoch 1850, val loss: 0.6026843190193176
Epoch 1860, training loss: 452.6639099121094 = 0.07211904972791672 + 50.0 * 9.051836013793945
Epoch 1860, val loss: 0.6040196418762207
Epoch 1870, training loss: 452.59234619140625 = 0.07088755071163177 + 50.0 * 9.050429344177246
Epoch 1870, val loss: 0.6079482436180115
Epoch 1880, training loss: 452.5622253417969 = 0.06969434767961502 + 50.0 * 9.049850463867188
Epoch 1880, val loss: 0.6102710366249084
Epoch 1890, training loss: 452.5715026855469 = 0.06853365153074265 + 50.0 * 9.05005931854248
Epoch 1890, val loss: 0.6128077507019043
Epoch 1900, training loss: 452.6414794921875 = 0.06739135831594467 + 50.0 * 9.051482200622559
Epoch 1900, val loss: 0.6165668964385986
Epoch 1910, training loss: 452.53741455078125 = 0.06625938415527344 + 50.0 * 9.049423217773438
Epoch 1910, val loss: 0.6190435290336609
Epoch 1920, training loss: 452.5314636230469 = 0.0651560127735138 + 50.0 * 9.049325942993164
Epoch 1920, val loss: 0.6240788102149963
Epoch 1930, training loss: 452.5105285644531 = 0.06406199932098389 + 50.0 * 9.048929214477539
Epoch 1930, val loss: 0.6268291473388672
Epoch 1940, training loss: 452.6772766113281 = 0.06301749497652054 + 50.0 * 9.052285194396973
Epoch 1940, val loss: 0.6298809051513672
Epoch 1950, training loss: 452.5823669433594 = 0.06195588782429695 + 50.0 * 9.050408363342285
Epoch 1950, val loss: 0.6318922638893127
Epoch 1960, training loss: 452.50750732421875 = 0.06091345474123955 + 50.0 * 9.048932075500488
Epoch 1960, val loss: 0.6360349059104919
Epoch 1970, training loss: 452.6027526855469 = 0.05996531993150711 + 50.0 * 9.05085563659668
Epoch 1970, val loss: 0.6379492878913879
Epoch 1980, training loss: 452.4776611328125 = 0.05890122801065445 + 50.0 * 9.048375129699707
Epoch 1980, val loss: 0.6418719291687012
Epoch 1990, training loss: 452.4501647949219 = 0.0579131655395031 + 50.0 * 9.047844886779785
Epoch 1990, val loss: 0.6452670097351074
Epoch 2000, training loss: 452.4416198730469 = 0.05694723129272461 + 50.0 * 9.047693252563477
Epoch 2000, val loss: 0.6481382250785828
Epoch 2010, training loss: 452.466064453125 = 0.056019920855760574 + 50.0 * 9.048200607299805
Epoch 2010, val loss: 0.6520062685012817
Epoch 2020, training loss: 452.47607421875 = 0.055079136043787 + 50.0 * 9.048419952392578
Epoch 2020, val loss: 0.6544036269187927
Epoch 2030, training loss: 452.45343017578125 = 0.05417047068476677 + 50.0 * 9.047985076904297
Epoch 2030, val loss: 0.6570616364479065
Epoch 2040, training loss: 452.5547180175781 = 0.05332193896174431 + 50.0 * 9.050027847290039
Epoch 2040, val loss: 0.6626598834991455
Epoch 2050, training loss: 452.4525146484375 = 0.052392322570085526 + 50.0 * 9.048002243041992
Epoch 2050, val loss: 0.6626133918762207
Epoch 2060, training loss: 452.4403076171875 = 0.051534030586481094 + 50.0 * 9.047775268554688
Epoch 2060, val loss: 0.6673541069030762
Epoch 2070, training loss: 452.44818115234375 = 0.05067545175552368 + 50.0 * 9.04794979095459
Epoch 2070, val loss: 0.6689705848693848
Epoch 2080, training loss: 452.4322509765625 = 0.049849268049001694 + 50.0 * 9.047648429870605
Epoch 2080, val loss: 0.6719130277633667
Epoch 2090, training loss: 452.36553955078125 = 0.049018193036317825 + 50.0 * 9.046330451965332
Epoch 2090, val loss: 0.6759517788887024
Epoch 2100, training loss: 452.34552001953125 = 0.04820878058671951 + 50.0 * 9.04594612121582
Epoch 2100, val loss: 0.6790326833724976
Epoch 2110, training loss: 452.340576171875 = 0.047421179711818695 + 50.0 * 9.045863151550293
Epoch 2110, val loss: 0.6810787320137024
Epoch 2120, training loss: 452.3987121582031 = 0.04666316509246826 + 50.0 * 9.047040939331055
Epoch 2120, val loss: 0.6838330030441284
Epoch 2130, training loss: 452.3935546875 = 0.0459282323718071 + 50.0 * 9.046952247619629
Epoch 2130, val loss: 0.6891077160835266
Epoch 2140, training loss: 452.3483581542969 = 0.045146748423576355 + 50.0 * 9.046064376831055
Epoch 2140, val loss: 0.6905947923660278
Epoch 2150, training loss: 452.3670654296875 = 0.04444042220711708 + 50.0 * 9.046452522277832
Epoch 2150, val loss: 0.6948351860046387
Epoch 2160, training loss: 452.3227233886719 = 0.04368529096245766 + 50.0 * 9.045580863952637
Epoch 2160, val loss: 0.6964816451072693
Epoch 2170, training loss: 452.4201354980469 = 0.04299188777804375 + 50.0 * 9.047542572021484
Epoch 2170, val loss: 0.6999599933624268
Epoch 2180, training loss: 452.30780029296875 = 0.04228596389293671 + 50.0 * 9.045310020446777
Epoch 2180, val loss: 0.7030760645866394
Epoch 2190, training loss: 452.28106689453125 = 0.041597720235586166 + 50.0 * 9.04478931427002
Epoch 2190, val loss: 0.7060933113098145
Epoch 2200, training loss: 452.2635498046875 = 0.04092242196202278 + 50.0 * 9.044452667236328
Epoch 2200, val loss: 0.7092929482460022
Epoch 2210, training loss: 452.2744140625 = 0.04026725888252258 + 50.0 * 9.044683456420898
Epoch 2210, val loss: 0.7117160558700562
Epoch 2220, training loss: 452.3699035644531 = 0.03964254632592201 + 50.0 * 9.046605110168457
Epoch 2220, val loss: 0.7155746817588806
Epoch 2230, training loss: 452.39080810546875 = 0.03905216231942177 + 50.0 * 9.047035217285156
Epoch 2230, val loss: 0.7196028828620911
Epoch 2240, training loss: 452.2853088378906 = 0.03837232664227486 + 50.0 * 9.044939041137695
Epoch 2240, val loss: 0.7199857234954834
Epoch 2250, training loss: 452.2466125488281 = 0.0377553291618824 + 50.0 * 9.044177055358887
Epoch 2250, val loss: 0.7245163321495056
Epoch 2260, training loss: 452.271240234375 = 0.037156056612730026 + 50.0 * 9.044681549072266
Epoch 2260, val loss: 0.7271971702575684
Epoch 2270, training loss: 452.21588134765625 = 0.03656010329723358 + 50.0 * 9.043586730957031
Epoch 2270, val loss: 0.7295193076133728
Epoch 2280, training loss: 452.2146911621094 = 0.0359828807413578 + 50.0 * 9.043574333190918
Epoch 2280, val loss: 0.7324169874191284
Epoch 2290, training loss: 452.23187255859375 = 0.03542806953191757 + 50.0 * 9.043929100036621
Epoch 2290, val loss: 0.7344923615455627
Epoch 2300, training loss: 452.24822998046875 = 0.034873273223638535 + 50.0 * 9.044266700744629
Epoch 2300, val loss: 0.7386681437492371
Epoch 2310, training loss: 452.19354248046875 = 0.034326571971178055 + 50.0 * 9.043184280395508
Epoch 2310, val loss: 0.7418035864830017
Epoch 2320, training loss: 452.2525634765625 = 0.03379179164767265 + 50.0 * 9.0443754196167
Epoch 2320, val loss: 0.7441858649253845
Epoch 2330, training loss: 452.2207946777344 = 0.033254291862249374 + 50.0 * 9.043750762939453
Epoch 2330, val loss: 0.7468822002410889
Epoch 2340, training loss: 452.1924743652344 = 0.032741617411375046 + 50.0 * 9.043194770812988
Epoch 2340, val loss: 0.7506250143051147
Epoch 2350, training loss: 452.1487731933594 = 0.03221914544701576 + 50.0 * 9.042330741882324
Epoch 2350, val loss: 0.7523360848426819
Epoch 2360, training loss: 452.1537170410156 = 0.03173116222023964 + 50.0 * 9.042439460754395
Epoch 2360, val loss: 0.7555974125862122
Epoch 2370, training loss: 452.2103576660156 = 0.03126511722803116 + 50.0 * 9.04358196258545
Epoch 2370, val loss: 0.7582079172134399
Epoch 2380, training loss: 452.1680603027344 = 0.030786707997322083 + 50.0 * 9.042745590209961
Epoch 2380, val loss: 0.7628963589668274
Epoch 2390, training loss: 452.2010192871094 = 0.03029908798635006 + 50.0 * 9.043414115905762
Epoch 2390, val loss: 0.7646036148071289
Epoch 2400, training loss: 452.1304626464844 = 0.029824325814843178 + 50.0 * 9.042013168334961
Epoch 2400, val loss: 0.7662197947502136
Epoch 2410, training loss: 452.22674560546875 = 0.029381196945905685 + 50.0 * 9.043947219848633
Epoch 2410, val loss: 0.7688820958137512
Epoch 2420, training loss: 452.1148681640625 = 0.028928762301802635 + 50.0 * 9.041718482971191
Epoch 2420, val loss: 0.7726858854293823
Epoch 2430, training loss: 452.0896911621094 = 0.028488263487815857 + 50.0 * 9.041223526000977
Epoch 2430, val loss: 0.7746444940567017
Epoch 2440, training loss: 452.08502197265625 = 0.02806173451244831 + 50.0 * 9.041138648986816
Epoch 2440, val loss: 0.7777662873268127
Epoch 2450, training loss: 452.16632080078125 = 0.027655258774757385 + 50.0 * 9.042773246765137
Epoch 2450, val loss: 0.780576765537262
Epoch 2460, training loss: 452.08050537109375 = 0.027237307280302048 + 50.0 * 9.041065216064453
Epoch 2460, val loss: 0.7834179997444153
Epoch 2470, training loss: 452.0995178222656 = 0.026852566748857498 + 50.0 * 9.04145336151123
Epoch 2470, val loss: 0.7873338460922241
Epoch 2480, training loss: 452.0938720703125 = 0.02645197883248329 + 50.0 * 9.041348457336426
Epoch 2480, val loss: 0.7894930839538574
Epoch 2490, training loss: 452.1549377441406 = 0.026073947548866272 + 50.0 * 9.042577743530273
Epoch 2490, val loss: 0.7905939817428589
Epoch 2500, training loss: 452.12689208984375 = 0.02567928098142147 + 50.0 * 9.042024612426758
Epoch 2500, val loss: 0.7945764660835266
Epoch 2510, training loss: 452.0425720214844 = 0.025288349017500877 + 50.0 * 9.040345191955566
Epoch 2510, val loss: 0.7962110638618469
Epoch 2520, training loss: 452.0318908691406 = 0.02492406964302063 + 50.0 * 9.040139198303223
Epoch 2520, val loss: 0.79933762550354
Epoch 2530, training loss: 452.05194091796875 = 0.024571556597948074 + 50.0 * 9.040547370910645
Epoch 2530, val loss: 0.8016551733016968
Epoch 2540, training loss: 452.08856201171875 = 0.02422955073416233 + 50.0 * 9.04128646850586
Epoch 2540, val loss: 0.8048048615455627
Epoch 2550, training loss: 452.11968994140625 = 0.023881807923316956 + 50.0 * 9.041915893554688
Epoch 2550, val loss: 0.8085996508598328
Epoch 2560, training loss: 452.04913330078125 = 0.02353312447667122 + 50.0 * 9.040512084960938
Epoch 2560, val loss: 0.8091863393783569
Epoch 2570, training loss: 452.0422668457031 = 0.023205367848277092 + 50.0 * 9.04038143157959
Epoch 2570, val loss: 0.8139622807502747
Epoch 2580, training loss: 452.0447082519531 = 0.022866876795887947 + 50.0 * 9.040436744689941
Epoch 2580, val loss: 0.8149300217628479
Epoch 2590, training loss: 452.0053405761719 = 0.022542687132954597 + 50.0 * 9.039655685424805
Epoch 2590, val loss: 0.818029522895813
Epoch 2600, training loss: 452.0933837890625 = 0.022243479266762733 + 50.0 * 9.041422843933105
Epoch 2600, val loss: 0.820595920085907
Epoch 2610, training loss: 452.0195617675781 = 0.021921273320913315 + 50.0 * 9.039953231811523
Epoch 2610, val loss: 0.8220553994178772
Epoch 2620, training loss: 452.13604736328125 = 0.021706916391849518 + 50.0 * 9.04228687286377
Epoch 2620, val loss: 0.8229091763496399
Epoch 2630, training loss: 451.99700927734375 = 0.02133316732943058 + 50.0 * 9.03951358795166
Epoch 2630, val loss: 0.8288791179656982
Epoch 2640, training loss: 451.963134765625 = 0.021032629534602165 + 50.0 * 9.03884220123291
Epoch 2640, val loss: 0.8296522498130798
Epoch 2650, training loss: 452.01373291015625 = 0.020754771307110786 + 50.0 * 9.039859771728516
Epoch 2650, val loss: 0.8321564197540283
Epoch 2660, training loss: 451.9690856933594 = 0.020467104390263557 + 50.0 * 9.038971900939941
Epoch 2660, val loss: 0.8348685503005981
Epoch 2670, training loss: 451.9375305175781 = 0.020190561190247536 + 50.0 * 9.038346290588379
Epoch 2670, val loss: 0.8372878432273865
Epoch 2680, training loss: 451.9393310546875 = 0.019922204315662384 + 50.0 * 9.0383882522583
Epoch 2680, val loss: 0.8395091891288757
Epoch 2690, training loss: 452.0816345214844 = 0.01969566009938717 + 50.0 * 9.041238784790039
Epoch 2690, val loss: 0.8415724039077759
Epoch 2700, training loss: 451.9864196777344 = 0.01940804161131382 + 50.0 * 9.039340019226074
Epoch 2700, val loss: 0.8457089066505432
Epoch 2710, training loss: 451.93841552734375 = 0.01914728432893753 + 50.0 * 9.038385391235352
Epoch 2710, val loss: 0.8469071388244629
Epoch 2720, training loss: 451.9343566894531 = 0.018896248191595078 + 50.0 * 9.038309097290039
Epoch 2720, val loss: 0.850125789642334
Epoch 2730, training loss: 451.955322265625 = 0.0186566561460495 + 50.0 * 9.03873348236084
Epoch 2730, val loss: 0.852161169052124
Epoch 2740, training loss: 451.9062805175781 = 0.01841212995350361 + 50.0 * 9.037757873535156
Epoch 2740, val loss: 0.854955792427063
Epoch 2750, training loss: 451.9466552734375 = 0.018175482749938965 + 50.0 * 9.038569450378418
Epoch 2750, val loss: 0.856913149356842
Epoch 2760, training loss: 451.90545654296875 = 0.017940616235136986 + 50.0 * 9.037750244140625
Epoch 2760, val loss: 0.8592978119850159
Epoch 2770, training loss: 452.0630798339844 = 0.017740599811077118 + 50.0 * 9.04090690612793
Epoch 2770, val loss: 0.8616790175437927
Epoch 2780, training loss: 451.9189758300781 = 0.017492026090621948 + 50.0 * 9.038029670715332
Epoch 2780, val loss: 0.8625324964523315
Epoch 2790, training loss: 451.875732421875 = 0.017266010865569115 + 50.0 * 9.037169456481934
Epoch 2790, val loss: 0.8657370209693909
Epoch 2800, training loss: 451.85394287109375 = 0.017048606649041176 + 50.0 * 9.036737442016602
Epoch 2800, val loss: 0.8675704002380371
Epoch 2810, training loss: 451.9036560058594 = 0.016844799742102623 + 50.0 * 9.037735939025879
Epoch 2810, val loss: 0.8695111870765686
Epoch 2820, training loss: 451.87939453125 = 0.016632599756121635 + 50.0 * 9.03725528717041
Epoch 2820, val loss: 0.8714873194694519
Epoch 2830, training loss: 451.8811340332031 = 0.016428833827376366 + 50.0 * 9.037294387817383
Epoch 2830, val loss: 0.8728811144828796
Epoch 2840, training loss: 451.83819580078125 = 0.016221342608332634 + 50.0 * 9.036438941955566
Epoch 2840, val loss: 0.8761509656906128
Epoch 2850, training loss: 451.82476806640625 = 0.01602635160088539 + 50.0 * 9.036174774169922
Epoch 2850, val loss: 0.8784995675086975
Epoch 2860, training loss: 451.93585205078125 = 0.015855498611927032 + 50.0 * 9.038399696350098
Epoch 2860, val loss: 0.881367564201355
Epoch 2870, training loss: 451.9081115722656 = 0.015670116990804672 + 50.0 * 9.037849426269531
Epoch 2870, val loss: 0.883471667766571
Epoch 2880, training loss: 451.8479919433594 = 0.015459452755749226 + 50.0 * 9.036650657653809
Epoch 2880, val loss: 0.883551299571991
Epoch 2890, training loss: 451.82855224609375 = 0.01527963113039732 + 50.0 * 9.03626537322998
Epoch 2890, val loss: 0.8857118487358093
Epoch 2900, training loss: 451.9687805175781 = 0.015153742395341396 + 50.0 * 9.039072036743164
Epoch 2900, val loss: 0.8867247700691223
Epoch 2910, training loss: 451.8392028808594 = 0.014927249401807785 + 50.0 * 9.03648567199707
Epoch 2910, val loss: 0.8917279243469238
Epoch 2920, training loss: 451.8058776855469 = 0.014751755632460117 + 50.0 * 9.035822868347168
Epoch 2920, val loss: 0.8917070627212524
Epoch 2930, training loss: 451.8806457519531 = 0.014579317532479763 + 50.0 * 9.037321090698242
Epoch 2930, val loss: 0.8938425183296204
Epoch 2940, training loss: 451.78466796875 = 0.014404296875 + 50.0 * 9.035405158996582
Epoch 2940, val loss: 0.8962942361831665
Epoch 2950, training loss: 451.8211364746094 = 0.014247988350689411 + 50.0 * 9.036137580871582
Epoch 2950, val loss: 0.898488461971283
Epoch 2960, training loss: 451.7942810058594 = 0.014079653657972813 + 50.0 * 9.035604476928711
Epoch 2960, val loss: 0.8994035720825195
Epoch 2970, training loss: 451.7833557128906 = 0.013918244279921055 + 50.0 * 9.035388946533203
Epoch 2970, val loss: 0.902385413646698
Epoch 2980, training loss: 451.822021484375 = 0.013769692741334438 + 50.0 * 9.036165237426758
Epoch 2980, val loss: 0.9032360911369324
Epoch 2990, training loss: 451.7723693847656 = 0.01361124124377966 + 50.0 * 9.035175323486328
Epoch 2990, val loss: 0.9060543775558472
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8280
Overall ASR: 0.7191
Flip ASR: 0.6499/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 519.0474243164062 = 1.0923036336898804 + 50.0 * 10.359103202819824
Epoch 0, val loss: 1.0905834436416626
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.31 GiB already allocated; 501.69 MiB free; 7.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.052734375 = 1.0998879671096802 + 50.0 * 10.35905647277832
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 687.69 MiB free; 6.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0487060546875 = 1.0994880199432373 + 50.0 * 10.35898494720459
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 687.69 MiB free; 6.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0540161132812 = 1.099113941192627 + 50.0 * 10.359098434448242
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 15.69 MiB free; 6.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0454711914062 = 1.0910851955413818 + 50.0 * 10.359087944030762
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 13.69 MiB free; 6.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 875.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 875.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 877.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 877.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 877.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 469.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 469.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 469.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0504760742188 = 1.101828694343567 + 50.0 * 10.358973503112793
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 79.69 MiB free; 6.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 475.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 477.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 477.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 873.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 473.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0551147460938 = 1.1056442260742188 + 50.0 * 10.358988761901855
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 209.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0255126953125 = 1.0821830034255981 + 50.0 * 10.358867645263672
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 209.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0413818359375 = 1.0921655893325806 + 50.0 * 10.35898494720459
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 209.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0609741210938 = 1.1062394380569458 + 50.0 * 10.35909366607666
Epoch 0, val loss: 1.1053341627120972
Epoch 10, training loss: 518.9200439453125 = 1.0947552919387817 + 50.0 * 10.35650634765625
Epoch 10, val loss: 1.0936368703842163
Epoch 20, training loss: 516.8878173828125 = 1.079441785812378 + 50.0 * 10.316166877746582
Epoch 20, val loss: 1.0784456729888916
Epoch 30, training loss: 495.91180419921875 = 1.0665438175201416 + 50.0 * 9.896904945373535
Epoch 30, val loss: 1.065902829170227
Epoch 40, training loss: 478.0189208984375 = 1.0540757179260254 + 50.0 * 9.539297103881836
Epoch 40, val loss: 1.0532536506652832
Epoch 50, training loss: 472.9984130859375 = 1.0381394624710083 + 50.0 * 9.439205169677734
Epoch 50, val loss: 1.037089228630066
Epoch 60, training loss: 470.8622131347656 = 1.022146463394165 + 50.0 * 9.396800994873047
Epoch 60, val loss: 1.0214956998825073
Epoch 70, training loss: 467.4114685058594 = 1.0085489749908447 + 50.0 * 9.328058242797852
Epoch 70, val loss: 1.0083189010620117
Epoch 80, training loss: 464.8570556640625 = 0.9953699111938477 + 50.0 * 9.277234077453613
Epoch 80, val loss: 0.9951695799827576
Epoch 90, training loss: 463.4638977050781 = 0.9773505330085754 + 50.0 * 9.249731063842773
Epoch 90, val loss: 0.9770745038986206
Epoch 100, training loss: 462.30126953125 = 0.955528736114502 + 50.0 * 9.226914405822754
Epoch 100, val loss: 0.9558085799217224
Epoch 110, training loss: 461.2661437988281 = 0.9328773021697998 + 50.0 * 9.2066650390625
Epoch 110, val loss: 0.9339332580566406
Epoch 120, training loss: 460.7271423339844 = 0.9074997901916504 + 50.0 * 9.196393013000488
Epoch 120, val loss: 0.9089194536209106
Epoch 130, training loss: 460.25274658203125 = 0.8771237134933472 + 50.0 * 9.187512397766113
Epoch 130, val loss: 0.8790830969810486
Epoch 140, training loss: 459.8497314453125 = 0.8438130617141724 + 50.0 * 9.180118560791016
Epoch 140, val loss: 0.8466764092445374
Epoch 150, training loss: 459.46441650390625 = 0.8093430995941162 + 50.0 * 9.173101425170898
Epoch 150, val loss: 0.8136239051818848
Epoch 160, training loss: 459.1930847167969 = 0.774215042591095 + 50.0 * 9.168377876281738
Epoch 160, val loss: 0.7800639867782593
Epoch 170, training loss: 458.85333251953125 = 0.7386621236801147 + 50.0 * 9.162293434143066
Epoch 170, val loss: 0.7462692856788635
Epoch 180, training loss: 458.71234130859375 = 0.7038501501083374 + 50.0 * 9.16016960144043
Epoch 180, val loss: 0.7134112119674683
Epoch 190, training loss: 458.403564453125 = 0.6703982353210449 + 50.0 * 9.1546630859375
Epoch 190, val loss: 0.6825162768363953
Epoch 200, training loss: 458.1305236816406 = 0.6395617127418518 + 50.0 * 9.149819374084473
Epoch 200, val loss: 0.6543822288513184
Epoch 210, training loss: 457.9151611328125 = 0.61161208152771 + 50.0 * 9.146071434020996
Epoch 210, val loss: 0.6290505528450012
Epoch 220, training loss: 457.71881103515625 = 0.5864859223365784 + 50.0 * 9.142646789550781
Epoch 220, val loss: 0.6066144108772278
Epoch 230, training loss: 457.7226867675781 = 0.5641716122627258 + 50.0 * 9.143170356750488
Epoch 230, val loss: 0.586944043636322
Epoch 240, training loss: 457.5321350097656 = 0.5442721843719482 + 50.0 * 9.13975715637207
Epoch 240, val loss: 0.5697132349014282
Epoch 250, training loss: 457.3013916015625 = 0.5269941687583923 + 50.0 * 9.13548755645752
Epoch 250, val loss: 0.5550219416618347
Epoch 260, training loss: 457.16802978515625 = 0.5120752453804016 + 50.0 * 9.133118629455566
Epoch 260, val loss: 0.5427003502845764
Epoch 270, training loss: 457.05511474609375 = 0.49901145696640015 + 50.0 * 9.131121635437012
Epoch 270, val loss: 0.5320254564285278
Epoch 280, training loss: 457.0796813964844 = 0.4874502122402191 + 50.0 * 9.131844520568848
Epoch 280, val loss: 0.5227606892585754
Epoch 290, training loss: 456.87384033203125 = 0.47692200541496277 + 50.0 * 9.127938270568848
Epoch 290, val loss: 0.5144736766815186
Epoch 300, training loss: 456.777587890625 = 0.46766117215156555 + 50.0 * 9.126198768615723
Epoch 300, val loss: 0.5073572397232056
Epoch 310, training loss: 456.6726379394531 = 0.45927056670188904 + 50.0 * 9.124267578125
Epoch 310, val loss: 0.5010709762573242
Epoch 320, training loss: 456.58563232421875 = 0.4515356719493866 + 50.0 * 9.122681617736816
Epoch 320, val loss: 0.4953528940677643
Epoch 330, training loss: 456.5020751953125 = 0.4442459046840668 + 50.0 * 9.121156692504883
Epoch 330, val loss: 0.4900948405265808
Epoch 340, training loss: 456.456298828125 = 0.4374227821826935 + 50.0 * 9.120377540588379
Epoch 340, val loss: 0.4851321876049042
Epoch 350, training loss: 456.3650817871094 = 0.4311698377132416 + 50.0 * 9.118678092956543
Epoch 350, val loss: 0.4807398319244385
Epoch 360, training loss: 456.433837890625 = 0.4252709746360779 + 50.0 * 9.120171546936035
Epoch 360, val loss: 0.4767356216907501
Epoch 370, training loss: 456.2033996582031 = 0.4195522367954254 + 50.0 * 9.115676879882812
Epoch 370, val loss: 0.47268715500831604
Epoch 380, training loss: 456.1438903808594 = 0.41429534554481506 + 50.0 * 9.114591598510742
Epoch 380, val loss: 0.46910086274147034
Epoch 390, training loss: 456.0880432128906 = 0.4093027412891388 + 50.0 * 9.113574981689453
Epoch 390, val loss: 0.4657497704029083
Epoch 400, training loss: 456.026123046875 = 0.4044153392314911 + 50.0 * 9.112434387207031
Epoch 400, val loss: 0.462740957736969
Epoch 410, training loss: 455.9350280761719 = 0.3997959494590759 + 50.0 * 9.11070442199707
Epoch 410, val loss: 0.4596152901649475
Epoch 420, training loss: 455.8726501464844 = 0.39544031023979187 + 50.0 * 9.109543800354004
Epoch 420, val loss: 0.45684051513671875
Epoch 430, training loss: 455.8125305175781 = 0.3912619650363922 + 50.0 * 9.10842514038086
Epoch 430, val loss: 0.45425093173980713
Epoch 440, training loss: 455.9951171875 = 0.3871743679046631 + 50.0 * 9.11215877532959
Epoch 440, val loss: 0.45165112614631653
Epoch 450, training loss: 455.769775390625 = 0.3830658495426178 + 50.0 * 9.107734680175781
Epoch 450, val loss: 0.449108362197876
Epoch 460, training loss: 455.6770935058594 = 0.37924543023109436 + 50.0 * 9.10595703125
Epoch 460, val loss: 0.4469470679759979
Epoch 470, training loss: 455.62017822265625 = 0.3755935728549957 + 50.0 * 9.104891777038574
Epoch 470, val loss: 0.44491228461265564
Epoch 480, training loss: 455.58782958984375 = 0.3720325827598572 + 50.0 * 9.104315757751465
Epoch 480, val loss: 0.4429563283920288
Epoch 490, training loss: 455.6059265136719 = 0.3685412108898163 + 50.0 * 9.104747772216797
Epoch 490, val loss: 0.44098061323165894
Epoch 500, training loss: 455.5162048339844 = 0.36507704854011536 + 50.0 * 9.103022575378418
Epoch 500, val loss: 0.4390750825405121
Epoch 510, training loss: 455.4124755859375 = 0.36181747913360596 + 50.0 * 9.10101318359375
Epoch 510, val loss: 0.43743976950645447
Epoch 520, training loss: 455.39453125 = 0.3586560785770416 + 50.0 * 9.100717544555664
Epoch 520, val loss: 0.43592143058776855
Epoch 530, training loss: 455.361328125 = 0.35545089840888977 + 50.0 * 9.100117683410645
Epoch 530, val loss: 0.43416517972946167
Epoch 540, training loss: 455.2862243652344 = 0.3523915410041809 + 50.0 * 9.098676681518555
Epoch 540, val loss: 0.43273574113845825
Epoch 550, training loss: 455.2547302246094 = 0.3494613766670227 + 50.0 * 9.098105430603027
Epoch 550, val loss: 0.43144893646240234
Epoch 560, training loss: 455.2762145996094 = 0.3465938866138458 + 50.0 * 9.098592758178711
Epoch 560, val loss: 0.43008318543434143
Epoch 570, training loss: 455.188720703125 = 0.34373384714126587 + 50.0 * 9.09689998626709
Epoch 570, val loss: 0.42877766489982605
Epoch 580, training loss: 455.12890625 = 0.34098514914512634 + 50.0 * 9.095758438110352
Epoch 580, val loss: 0.4276137351989746
Epoch 590, training loss: 455.1058654785156 = 0.3383261263370514 + 50.0 * 9.095351219177246
Epoch 590, val loss: 0.4267050623893738
Epoch 600, training loss: 455.06488037109375 = 0.3355901539325714 + 50.0 * 9.094585418701172
Epoch 600, val loss: 0.4251146614551544
Epoch 610, training loss: 455.0409851074219 = 0.33296462893486023 + 50.0 * 9.094160079956055
Epoch 610, val loss: 0.42422229051589966
Epoch 620, training loss: 454.9871520996094 = 0.3304350972175598 + 50.0 * 9.093133926391602
Epoch 620, val loss: 0.42351865768432617
Epoch 630, training loss: 454.9429016113281 = 0.3279658854007721 + 50.0 * 9.09229850769043
Epoch 630, val loss: 0.42262181639671326
Epoch 640, training loss: 454.9838562011719 = 0.3254983127117157 + 50.0 * 9.093167304992676
Epoch 640, val loss: 0.42176294326782227
Epoch 650, training loss: 454.94091796875 = 0.3230416774749756 + 50.0 * 9.092357635498047
Epoch 650, val loss: 0.42090705037117004
Epoch 660, training loss: 454.84539794921875 = 0.320636510848999 + 50.0 * 9.090495109558105
Epoch 660, val loss: 0.41975167393684387
Epoch 670, training loss: 454.8809509277344 = 0.3183175027370453 + 50.0 * 9.091252326965332
Epoch 670, val loss: 0.4190373122692108
Epoch 680, training loss: 454.7945251464844 = 0.3160034418106079 + 50.0 * 9.089570045471191
Epoch 680, val loss: 0.4188375771045685
Epoch 690, training loss: 454.75921630859375 = 0.3137444257736206 + 50.0 * 9.088909149169922
Epoch 690, val loss: 0.4177551865577698
Epoch 700, training loss: 454.7642517089844 = 0.3114929795265198 + 50.0 * 9.089055061340332
Epoch 700, val loss: 0.41738900542259216
Epoch 710, training loss: 454.7810974121094 = 0.3092605471611023 + 50.0 * 9.089436531066895
Epoch 710, val loss: 0.41682353615760803
Epoch 720, training loss: 454.6907653808594 = 0.3070184886455536 + 50.0 * 9.087675094604492
Epoch 720, val loss: 0.4164220094680786
Epoch 730, training loss: 454.63592529296875 = 0.3048763573169708 + 50.0 * 9.086621284484863
Epoch 730, val loss: 0.41572582721710205
Epoch 740, training loss: 454.5988464355469 = 0.30279937386512756 + 50.0 * 9.085921287536621
Epoch 740, val loss: 0.41555604338645935
Epoch 750, training loss: 454.5647277832031 = 0.30073294043540955 + 50.0 * 9.085280418395996
Epoch 750, val loss: 0.41491132974624634
Epoch 760, training loss: 454.62811279296875 = 0.2986826002597809 + 50.0 * 9.086588859558105
Epoch 760, val loss: 0.4144759774208069
Epoch 770, training loss: 454.58978271484375 = 0.29654067754745483 + 50.0 * 9.085865020751953
Epoch 770, val loss: 0.4143217206001282
Epoch 780, training loss: 454.4899597167969 = 0.29449936747550964 + 50.0 * 9.083909034729004
Epoch 780, val loss: 0.4138288199901581
Epoch 790, training loss: 454.4936218261719 = 0.2925129234790802 + 50.0 * 9.084022521972656
Epoch 790, val loss: 0.4134809374809265
Epoch 800, training loss: 454.50836181640625 = 0.2905074954032898 + 50.0 * 9.084357261657715
Epoch 800, val loss: 0.4131579101085663
Epoch 810, training loss: 454.44647216796875 = 0.2885100245475769 + 50.0 * 9.083159446716309
Epoch 810, val loss: 0.41318124532699585
Epoch 820, training loss: 454.48602294921875 = 0.2865372598171234 + 50.0 * 9.083990097045898
Epoch 820, val loss: 0.41259801387786865
Epoch 830, training loss: 454.4232482910156 = 0.28456762433052063 + 50.0 * 9.082773208618164
Epoch 830, val loss: 0.4127138555049896
Epoch 840, training loss: 454.3573913574219 = 0.28262823820114136 + 50.0 * 9.08149528503418
Epoch 840, val loss: 0.4123958945274353
Epoch 850, training loss: 454.3312683105469 = 0.280728280544281 + 50.0 * 9.081010818481445
Epoch 850, val loss: 0.41241005063056946
Epoch 860, training loss: 454.39208984375 = 0.2788136601448059 + 50.0 * 9.082265853881836
Epoch 860, val loss: 0.4123741090297699
Epoch 870, training loss: 454.31878662109375 = 0.27687427401542664 + 50.0 * 9.080838203430176
Epoch 870, val loss: 0.4120008647441864
Epoch 880, training loss: 454.35406494140625 = 0.2749756872653961 + 50.0 * 9.081582069396973
Epoch 880, val loss: 0.4124941825866699
Epoch 890, training loss: 454.25946044921875 = 0.273036926984787 + 50.0 * 9.079728126525879
Epoch 890, val loss: 0.4119577407836914
Epoch 900, training loss: 454.2225036621094 = 0.27117037773132324 + 50.0 * 9.079026222229004
Epoch 900, val loss: 0.41173994541168213
Epoch 910, training loss: 454.1965026855469 = 0.26932087540626526 + 50.0 * 9.078543663024902
Epoch 910, val loss: 0.4119664132595062
Epoch 920, training loss: 454.2865295410156 = 0.26745471358299255 + 50.0 * 9.080381393432617
Epoch 920, val loss: 0.41204458475112915
Epoch 930, training loss: 454.183837890625 = 0.26555004715919495 + 50.0 * 9.078365325927734
Epoch 930, val loss: 0.41177698969841003
Epoch 940, training loss: 454.1706237792969 = 0.2636847198009491 + 50.0 * 9.078139305114746
Epoch 940, val loss: 0.41217276453971863
Epoch 950, training loss: 454.12298583984375 = 0.26181700825691223 + 50.0 * 9.077223777770996
Epoch 950, val loss: 0.41179579496383667
Epoch 960, training loss: 454.1689147949219 = 0.25998374819755554 + 50.0 * 9.078178405761719
Epoch 960, val loss: 0.4115219712257385
Epoch 970, training loss: 454.1407470703125 = 0.2580632269382477 + 50.0 * 9.077653884887695
Epoch 970, val loss: 0.41209855675697327
Epoch 980, training loss: 454.0570373535156 = 0.256211519241333 + 50.0 * 9.076016426086426
Epoch 980, val loss: 0.41201725602149963
Epoch 990, training loss: 454.02978515625 = 0.2543904483318329 + 50.0 * 9.075508117675781
Epoch 990, val loss: 0.4123077988624573
Epoch 1000, training loss: 454.00689697265625 = 0.25257644057273865 + 50.0 * 9.07508659362793
Epoch 1000, val loss: 0.4125656485557556
Epoch 1010, training loss: 454.07891845703125 = 0.2507559359073639 + 50.0 * 9.076562881469727
Epoch 1010, val loss: 0.4129178822040558
Epoch 1020, training loss: 454.0005798339844 = 0.24885761737823486 + 50.0 * 9.075034141540527
Epoch 1020, val loss: 0.4122968018054962
Epoch 1030, training loss: 453.98809814453125 = 0.24700458347797394 + 50.0 * 9.074821472167969
Epoch 1030, val loss: 0.41274985671043396
Epoch 1040, training loss: 453.95233154296875 = 0.24516108632087708 + 50.0 * 9.074143409729004
Epoch 1040, val loss: 0.41281139850616455
Epoch 1050, training loss: 454.00445556640625 = 0.24331969022750854 + 50.0 * 9.075222969055176
Epoch 1050, val loss: 0.4131264388561249
Epoch 1060, training loss: 454.0158386230469 = 0.24142515659332275 + 50.0 * 9.075488090515137
Epoch 1060, val loss: 0.41333651542663574
Epoch 1070, training loss: 453.8861999511719 = 0.2395390272140503 + 50.0 * 9.072933197021484
Epoch 1070, val loss: 0.4132138192653656
Epoch 1080, training loss: 453.8455810546875 = 0.2376970648765564 + 50.0 * 9.072157859802246
Epoch 1080, val loss: 0.41365447640419006
Epoch 1090, training loss: 453.83013916015625 = 0.2358592450618744 + 50.0 * 9.071885108947754
Epoch 1090, val loss: 0.41385847330093384
Epoch 1100, training loss: 453.9210510253906 = 0.23400840163230896 + 50.0 * 9.07374095916748
Epoch 1100, val loss: 0.41406670212745667
Epoch 1110, training loss: 453.90264892578125 = 0.2320747971534729 + 50.0 * 9.073410987854004
Epoch 1110, val loss: 0.41381722688674927
Epoch 1120, training loss: 453.77667236328125 = 0.23018817603588104 + 50.0 * 9.070929527282715
Epoch 1120, val loss: 0.41458240151405334
Epoch 1130, training loss: 453.7681884765625 = 0.22833457589149475 + 50.0 * 9.070796966552734
Epoch 1130, val loss: 0.414690762758255
Epoch 1140, training loss: 453.7510986328125 = 0.22648578882217407 + 50.0 * 9.0704927444458
Epoch 1140, val loss: 0.41469672322273254
Epoch 1150, training loss: 453.8395080566406 = 0.22461871802806854 + 50.0 * 9.072298049926758
Epoch 1150, val loss: 0.4151534140110016
Epoch 1160, training loss: 453.7174072265625 = 0.22271853685379028 + 50.0 * 9.069893836975098
Epoch 1160, val loss: 0.41575124859809875
Epoch 1170, training loss: 453.6894226074219 = 0.22084501385688782 + 50.0 * 9.069371223449707
Epoch 1170, val loss: 0.41603705286979675
Epoch 1180, training loss: 453.6919250488281 = 0.21901030838489532 + 50.0 * 9.0694580078125
Epoch 1180, val loss: 0.4156619608402252
Epoch 1190, training loss: 453.7246398925781 = 0.21713829040527344 + 50.0 * 9.070150375366211
Epoch 1190, val loss: 0.4161740839481354
Epoch 1200, training loss: 453.8194580078125 = 0.2152436077594757 + 50.0 * 9.072084426879883
Epoch 1200, val loss: 0.4176831543445587
Epoch 1210, training loss: 453.6657409667969 = 0.21333901584148407 + 50.0 * 9.069047927856445
Epoch 1210, val loss: 0.41754186153411865
Epoch 1220, training loss: 453.6116638183594 = 0.21147502958774567 + 50.0 * 9.06800365447998
Epoch 1220, val loss: 0.41813069581985474
Epoch 1230, training loss: 453.63592529296875 = 0.20963260531425476 + 50.0 * 9.068526268005371
Epoch 1230, val loss: 0.41841670870780945
Epoch 1240, training loss: 453.5659484863281 = 0.20774304866790771 + 50.0 * 9.067164421081543
Epoch 1240, val loss: 0.4193229079246521
Epoch 1250, training loss: 453.5599060058594 = 0.20588365197181702 + 50.0 * 9.0670804977417
Epoch 1250, val loss: 0.4198780655860901
Epoch 1260, training loss: 453.6064758300781 = 0.20402413606643677 + 50.0 * 9.068049430847168
Epoch 1260, val loss: 0.42020943760871887
Epoch 1270, training loss: 453.5727844238281 = 0.20211775600910187 + 50.0 * 9.067413330078125
Epoch 1270, val loss: 0.4208427965641022
Epoch 1280, training loss: 453.5155334472656 = 0.20022885501384735 + 50.0 * 9.066306114196777
Epoch 1280, val loss: 0.4214610159397125
Epoch 1290, training loss: 453.4865417480469 = 0.19836074113845825 + 50.0 * 9.065763473510742
Epoch 1290, val loss: 0.422349214553833
Epoch 1300, training loss: 453.4862365722656 = 0.19650547206401825 + 50.0 * 9.065794944763184
Epoch 1300, val loss: 0.4234142005443573
Epoch 1310, training loss: 453.5707092285156 = 0.19464999437332153 + 50.0 * 9.067521095275879
Epoch 1310, val loss: 0.42405542731285095
Epoch 1320, training loss: 453.4851989746094 = 0.19272689521312714 + 50.0 * 9.065849304199219
Epoch 1320, val loss: 0.4241723120212555
Epoch 1330, training loss: 453.43988037109375 = 0.1908423900604248 + 50.0 * 9.064980506896973
Epoch 1330, val loss: 0.4250026345252991
Epoch 1340, training loss: 453.4258117675781 = 0.18900282680988312 + 50.0 * 9.064736366271973
Epoch 1340, val loss: 0.42614734172821045
Epoch 1350, training loss: 453.4833068847656 = 0.18715211749076843 + 50.0 * 9.065922737121582
Epoch 1350, val loss: 0.4270848333835602
Epoch 1360, training loss: 453.4170837402344 = 0.18526741862297058 + 50.0 * 9.06463623046875
Epoch 1360, val loss: 0.4271191954612732
Epoch 1370, training loss: 453.3887634277344 = 0.18342366814613342 + 50.0 * 9.064106941223145
Epoch 1370, val loss: 0.4287373423576355
Epoch 1380, training loss: 453.380859375 = 0.18156497180461884 + 50.0 * 9.063985824584961
Epoch 1380, val loss: 0.4294561445713043
Epoch 1390, training loss: 453.35748291015625 = 0.17969681322574615 + 50.0 * 9.063555717468262
Epoch 1390, val loss: 0.4293723702430725
Epoch 1400, training loss: 453.34814453125 = 0.17784184217453003 + 50.0 * 9.063405990600586
Epoch 1400, val loss: 0.43061673641204834
Epoch 1410, training loss: 453.3537902832031 = 0.17599652707576752 + 50.0 * 9.063555717468262
Epoch 1410, val loss: 0.43089455366134644
Epoch 1420, training loss: 453.38653564453125 = 0.17414359748363495 + 50.0 * 9.064248085021973
Epoch 1420, val loss: 0.4329449236392975
Epoch 1430, training loss: 453.2876281738281 = 0.1722893863916397 + 50.0 * 9.06230640411377
Epoch 1430, val loss: 0.43358442187309265
Epoch 1440, training loss: 453.2553405761719 = 0.1704413890838623 + 50.0 * 9.061697959899902
Epoch 1440, val loss: 0.4342801570892334
Epoch 1450, training loss: 453.2411193847656 = 0.1686019003391266 + 50.0 * 9.061450004577637
Epoch 1450, val loss: 0.4352441728115082
Epoch 1460, training loss: 453.3299255371094 = 0.16682565212249756 + 50.0 * 9.063261985778809
Epoch 1460, val loss: 0.4353032410144806
Epoch 1470, training loss: 453.2609558105469 = 0.16494238376617432 + 50.0 * 9.061920166015625
Epoch 1470, val loss: 0.4381225109100342
Epoch 1480, training loss: 453.2608947753906 = 0.1631275713443756 + 50.0 * 9.061955451965332
Epoch 1480, val loss: 0.4383993148803711
Epoch 1490, training loss: 453.20562744140625 = 0.1613152176141739 + 50.0 * 9.06088638305664
Epoch 1490, val loss: 0.440242201089859
Epoch 1500, training loss: 453.1736755371094 = 0.15949536859989166 + 50.0 * 9.060283660888672
Epoch 1500, val loss: 0.4408983886241913
Epoch 1510, training loss: 453.1965637207031 = 0.1577112227678299 + 50.0 * 9.060776710510254
Epoch 1510, val loss: 0.44255727529525757
Epoch 1520, training loss: 453.1669921875 = 0.15591838955879211 + 50.0 * 9.060221672058105
Epoch 1520, val loss: 0.44317469000816345
Epoch 1530, training loss: 453.1767883300781 = 0.15411919355392456 + 50.0 * 9.060453414916992
Epoch 1530, val loss: 0.44489941000938416
Epoch 1540, training loss: 453.10308837890625 = 0.1523217260837555 + 50.0 * 9.059015274047852
Epoch 1540, val loss: 0.4461733400821686
Epoch 1550, training loss: 453.1020202636719 = 0.15055324137210846 + 50.0 * 9.059029579162598
Epoch 1550, val loss: 0.4471622407436371
Epoch 1560, training loss: 453.32171630859375 = 0.14882878959178925 + 50.0 * 9.063457489013672
Epoch 1560, val loss: 0.4488103687763214
Epoch 1570, training loss: 453.16668701171875 = 0.14705458283424377 + 50.0 * 9.060392379760742
Epoch 1570, val loss: 0.4496830999851227
Epoch 1580, training loss: 453.0599670410156 = 0.14525683224201202 + 50.0 * 9.058294296264648
Epoch 1580, val loss: 0.45141440629959106
Epoch 1590, training loss: 453.05535888671875 = 0.14350801706314087 + 50.0 * 9.058237075805664
Epoch 1590, val loss: 0.45255324244499207
Epoch 1600, training loss: 453.1825256347656 = 0.14182674884796143 + 50.0 * 9.060813903808594
Epoch 1600, val loss: 0.453287810087204
Epoch 1610, training loss: 453.03076171875 = 0.14003987610340118 + 50.0 * 9.057814598083496
Epoch 1610, val loss: 0.45563483238220215
Epoch 1620, training loss: 452.9983215332031 = 0.13830871880054474 + 50.0 * 9.05720043182373
Epoch 1620, val loss: 0.4572047293186188
Epoch 1630, training loss: 453.09429931640625 = 0.13664889335632324 + 50.0 * 9.059152603149414
Epoch 1630, val loss: 0.4573639929294586
Epoch 1640, training loss: 452.9987487792969 = 0.1349140852689743 + 50.0 * 9.057276725769043
Epoch 1640, val loss: 0.46087446808815
Epoch 1650, training loss: 452.9795837402344 = 0.1332419067621231 + 50.0 * 9.056926727294922
Epoch 1650, val loss: 0.46093448996543884
Epoch 1660, training loss: 452.9608154296875 = 0.1315481811761856 + 50.0 * 9.056585311889648
Epoch 1660, val loss: 0.4628356695175171
Epoch 1670, training loss: 453.0865173339844 = 0.13019907474517822 + 50.0 * 9.059126853942871
Epoch 1670, val loss: 0.46243321895599365
Epoch 1680, training loss: 452.97662353515625 = 0.12838979065418243 + 50.0 * 9.056964874267578
Epoch 1680, val loss: 0.46837133169174194
Epoch 1690, training loss: 452.97515869140625 = 0.12670046091079712 + 50.0 * 9.056968688964844
Epoch 1690, val loss: 0.4675927758216858
Epoch 1700, training loss: 452.9526062011719 = 0.12507136166095734 + 50.0 * 9.056550979614258
Epoch 1700, val loss: 0.4700772166252136
Epoch 1710, training loss: 452.9621276855469 = 0.1234891414642334 + 50.0 * 9.05677318572998
Epoch 1710, val loss: 0.4710136950016022
Epoch 1720, training loss: 452.92169189453125 = 0.12189812958240509 + 50.0 * 9.05599594116211
Epoch 1720, val loss: 0.47293370962142944
Epoch 1730, training loss: 452.8704833984375 = 0.12032454460859299 + 50.0 * 9.05500316619873
Epoch 1730, val loss: 0.4750261902809143
Epoch 1740, training loss: 452.8487243652344 = 0.11875665187835693 + 50.0 * 9.05459976196289
Epoch 1740, val loss: 0.4765152037143707
Epoch 1750, training loss: 452.8868408203125 = 0.1172223687171936 + 50.0 * 9.055392265319824
Epoch 1750, val loss: 0.4787391424179077
Epoch 1760, training loss: 452.9581604003906 = 0.11569731682538986 + 50.0 * 9.056849479675293
Epoch 1760, val loss: 0.48037388920783997
Epoch 1770, training loss: 452.8583984375 = 0.11416156589984894 + 50.0 * 9.054884910583496
Epoch 1770, val loss: 0.48249897360801697
Epoch 1780, training loss: 452.828369140625 = 0.11263212561607361 + 50.0 * 9.054314613342285
Epoch 1780, val loss: 0.4834021329879761
Epoch 1790, training loss: 452.7919006347656 = 0.11111217737197876 + 50.0 * 9.05361557006836
Epoch 1790, val loss: 0.48592305183410645
Epoch 1800, training loss: 453.02593994140625 = 0.1096627339720726 + 50.0 * 9.05832576751709
Epoch 1800, val loss: 0.4877347946166992
Epoch 1810, training loss: 452.9278259277344 = 0.10816207528114319 + 50.0 * 9.05639362335205
Epoch 1810, val loss: 0.48955246806144714
Epoch 1820, training loss: 452.7551574707031 = 0.10664453357458115 + 50.0 * 9.052969932556152
Epoch 1820, val loss: 0.49174872040748596
Epoch 1830, training loss: 452.7613220214844 = 0.10518156737089157 + 50.0 * 9.053122520446777
Epoch 1830, val loss: 0.4935929477214813
Epoch 1840, training loss: 452.7467346191406 = 0.10372532904148102 + 50.0 * 9.052860260009766
Epoch 1840, val loss: 0.4954136610031128
Epoch 1850, training loss: 452.96014404296875 = 0.1023346483707428 + 50.0 * 9.057156562805176
Epoch 1850, val loss: 0.4980223476886749
Epoch 1860, training loss: 452.7297058105469 = 0.10086235404014587 + 50.0 * 9.052577018737793
Epoch 1860, val loss: 0.4996122717857361
Epoch 1870, training loss: 452.7079772949219 = 0.0994391143321991 + 50.0 * 9.052170753479004
Epoch 1870, val loss: 0.5014504790306091
Epoch 1880, training loss: 452.7018737792969 = 0.09803738445043564 + 50.0 * 9.052077293395996
Epoch 1880, val loss: 0.5029763579368591
Epoch 1890, training loss: 452.7886047363281 = 0.09668577462434769 + 50.0 * 9.053838729858398
Epoch 1890, val loss: 0.5040121078491211
Epoch 1900, training loss: 452.69964599609375 = 0.09532652050256729 + 50.0 * 9.052085876464844
Epoch 1900, val loss: 0.5097931027412415
Epoch 1910, training loss: 452.74786376953125 = 0.09397917985916138 + 50.0 * 9.053077697753906
Epoch 1910, val loss: 0.5106167197227478
Epoch 1920, training loss: 452.6901550292969 = 0.0925886482000351 + 50.0 * 9.05195140838623
Epoch 1920, val loss: 0.5117133855819702
Epoch 1930, training loss: 452.6474609375 = 0.09125951677560806 + 50.0 * 9.051124572753906
Epoch 1930, val loss: 0.514683187007904
Epoch 1940, training loss: 452.6542663574219 = 0.09001315385103226 + 50.0 * 9.051284790039062
Epoch 1940, val loss: 0.5180166363716125
Epoch 1950, training loss: 452.7606506347656 = 0.088752381503582 + 50.0 * 9.053438186645508
Epoch 1950, val loss: 0.5190626978874207
Epoch 1960, training loss: 452.6268005371094 = 0.08740144222974777 + 50.0 * 9.050787925720215
Epoch 1960, val loss: 0.5203253030776978
Epoch 1970, training loss: 452.6033630371094 = 0.08614430576562881 + 50.0 * 9.050344467163086
Epoch 1970, val loss: 0.5230822563171387
Epoch 1980, training loss: 452.7265319824219 = 0.08498027920722961 + 50.0 * 9.052830696105957
Epoch 1980, val loss: 0.5266166925430298
Epoch 1990, training loss: 452.6153869628906 = 0.08371756225824356 + 50.0 * 9.050633430480957
Epoch 1990, val loss: 0.5255789756774902
Epoch 2000, training loss: 452.5765075683594 = 0.08246612548828125 + 50.0 * 9.049880981445312
Epoch 2000, val loss: 0.5299509167671204
Epoch 2010, training loss: 452.58099365234375 = 0.08125589042901993 + 50.0 * 9.049994468688965
Epoch 2010, val loss: 0.5308581590652466
Epoch 2020, training loss: 452.64971923828125 = 0.08008438348770142 + 50.0 * 9.051392555236816
Epoch 2020, val loss: 0.5334959626197815
Epoch 2030, training loss: 452.5804443359375 = 0.07890480011701584 + 50.0 * 9.050030708312988
Epoch 2030, val loss: 0.5360550880432129
Epoch 2040, training loss: 452.56622314453125 = 0.07774447649717331 + 50.0 * 9.049769401550293
Epoch 2040, val loss: 0.5386753082275391
Epoch 2050, training loss: 452.55517578125 = 0.07664603739976883 + 50.0 * 9.049570083618164
Epoch 2050, val loss: 0.5422077178955078
Epoch 2060, training loss: 452.6833801269531 = 0.07575047016143799 + 50.0 * 9.052152633666992
Epoch 2060, val loss: 0.5465822815895081
Epoch 2070, training loss: 452.5821228027344 = 0.07453214377164841 + 50.0 * 9.050151824951172
Epoch 2070, val loss: 0.5419610142707825
Epoch 2080, training loss: 452.5459289550781 = 0.07331395149230957 + 50.0 * 9.049452781677246
Epoch 2080, val loss: 0.5485668778419495
Epoch 2090, training loss: 452.56341552734375 = 0.07222753763198853 + 50.0 * 9.049823760986328
Epoch 2090, val loss: 0.5494790077209473
Epoch 2100, training loss: 452.50030517578125 = 0.07116218656301498 + 50.0 * 9.048583030700684
Epoch 2100, val loss: 0.5520190596580505
Epoch 2110, training loss: 452.48760986328125 = 0.07014352828264236 + 50.0 * 9.048349380493164
Epoch 2110, val loss: 0.5550912022590637
Epoch 2120, training loss: 452.4622802734375 = 0.0690847858786583 + 50.0 * 9.047863960266113
Epoch 2120, val loss: 0.5561603307723999
Epoch 2130, training loss: 452.4643859863281 = 0.06807611882686615 + 50.0 * 9.04792594909668
Epoch 2130, val loss: 0.5590186715126038
Epoch 2140, training loss: 452.6072082519531 = 0.06710448116064072 + 50.0 * 9.050802230834961
Epoch 2140, val loss: 0.5617532134056091
Epoch 2150, training loss: 452.5267028808594 = 0.06615718454122543 + 50.0 * 9.049210548400879
Epoch 2150, val loss: 0.5645100474357605
Epoch 2160, training loss: 452.47119140625 = 0.06513053923845291 + 50.0 * 9.048121452331543
Epoch 2160, val loss: 0.5649073719978333
Epoch 2170, training loss: 452.4303283691406 = 0.06418047100305557 + 50.0 * 9.047323226928711
Epoch 2170, val loss: 0.5671438574790955
Epoch 2180, training loss: 452.4212951660156 = 0.06322421878576279 + 50.0 * 9.047161102294922
Epoch 2180, val loss: 0.5694971680641174
Epoch 2190, training loss: 452.4508056640625 = 0.062312640249729156 + 50.0 * 9.047769546508789
Epoch 2190, val loss: 0.5736786723136902
Epoch 2200, training loss: 452.4098205566406 = 0.06141196936368942 + 50.0 * 9.046968460083008
Epoch 2200, val loss: 0.5764302015304565
Epoch 2210, training loss: 452.4359130859375 = 0.060500044375658035 + 50.0 * 9.047508239746094
Epoch 2210, val loss: 0.5782287120819092
Epoch 2220, training loss: 452.40692138671875 = 0.059598956257104874 + 50.0 * 9.04694652557373
Epoch 2220, val loss: 0.5781340003013611
Epoch 2230, training loss: 452.4864807128906 = 0.058806419372558594 + 50.0 * 9.048553466796875
Epoch 2230, val loss: 0.580045759677887
Epoch 2240, training loss: 452.3871765136719 = 0.057874757796525955 + 50.0 * 9.046586036682129
Epoch 2240, val loss: 0.5850368738174438
Epoch 2250, training loss: 452.44647216796875 = 0.057043131440877914 + 50.0 * 9.047788619995117
Epoch 2250, val loss: 0.5863162875175476
Epoch 2260, training loss: 452.3369140625 = 0.05616307631134987 + 50.0 * 9.045615196228027
Epoch 2260, val loss: 0.589542031288147
Epoch 2270, training loss: 452.3267822265625 = 0.05533467233181 + 50.0 * 9.045429229736328
Epoch 2270, val loss: 0.5916128754615784
Epoch 2280, training loss: 452.3161315917969 = 0.05452633276581764 + 50.0 * 9.045231819152832
Epoch 2280, val loss: 0.5931397676467896
Epoch 2290, training loss: 452.31829833984375 = 0.05372199788689613 + 50.0 * 9.045291900634766
Epoch 2290, val loss: 0.5953541398048401
Epoch 2300, training loss: 452.49285888671875 = 0.05315191298723221 + 50.0 * 9.04879379272461
Epoch 2300, val loss: 0.5945002436637878
Epoch 2310, training loss: 452.359130859375 = 0.052237529307603836 + 50.0 * 9.046137809753418
Epoch 2310, val loss: 0.6040629744529724
Epoch 2320, training loss: 452.3276062011719 = 0.051418546587228775 + 50.0 * 9.045523643493652
Epoch 2320, val loss: 0.6032200455665588
Epoch 2330, training loss: 452.34674072265625 = 0.05065595358610153 + 50.0 * 9.045921325683594
Epoch 2330, val loss: 0.6061769127845764
Epoch 2340, training loss: 452.32196044921875 = 0.04991472512483597 + 50.0 * 9.045440673828125
Epoch 2340, val loss: 0.6089008450508118
Epoch 2350, training loss: 452.29437255859375 = 0.04919848218560219 + 50.0 * 9.044903755187988
Epoch 2350, val loss: 0.6119361519813538
Epoch 2360, training loss: 452.3142395019531 = 0.04854672774672508 + 50.0 * 9.045313835144043
Epoch 2360, val loss: 0.6160047650337219
Epoch 2370, training loss: 452.26385498046875 = 0.047757185995578766 + 50.0 * 9.04432201385498
Epoch 2370, val loss: 0.6149306297302246
Epoch 2380, training loss: 452.3021545410156 = 0.047102682292461395 + 50.0 * 9.045101165771484
Epoch 2380, val loss: 0.6167961955070496
Epoch 2390, training loss: 452.25396728515625 = 0.04639718681573868 + 50.0 * 9.044151306152344
Epoch 2390, val loss: 0.6217206716537476
Epoch 2400, training loss: 452.2665710449219 = 0.04573957622051239 + 50.0 * 9.044416427612305
Epoch 2400, val loss: 0.6237772107124329
Epoch 2410, training loss: 452.28472900390625 = 0.04506368190050125 + 50.0 * 9.044793128967285
Epoch 2410, val loss: 0.6246204376220703
Epoch 2420, training loss: 452.2503967285156 = 0.04441266506910324 + 50.0 * 9.044119834899902
Epoch 2420, val loss: 0.6279630064964294
Epoch 2430, training loss: 452.23895263671875 = 0.04378456994891167 + 50.0 * 9.043903350830078
Epoch 2430, val loss: 0.628515362739563
Epoch 2440, training loss: 452.3987121582031 = 0.043174076825380325 + 50.0 * 9.047110557556152
Epoch 2440, val loss: 0.6322524547576904
Epoch 2450, training loss: 452.264404296875 = 0.04255213588476181 + 50.0 * 9.044437408447266
Epoch 2450, val loss: 0.635856032371521
Epoch 2460, training loss: 452.19366455078125 = 0.04191375896334648 + 50.0 * 9.043035507202148
Epoch 2460, val loss: 0.6371498107910156
Epoch 2470, training loss: 452.1686096191406 = 0.04130549356341362 + 50.0 * 9.042546272277832
Epoch 2470, val loss: 0.6391457319259644
Epoch 2480, training loss: 452.19635009765625 = 0.04072429984807968 + 50.0 * 9.043112754821777
Epoch 2480, val loss: 0.6410529017448425
Epoch 2490, training loss: 452.23388671875 = 0.040154147893190384 + 50.0 * 9.043874740600586
Epoch 2490, val loss: 0.6441556215286255
Epoch 2500, training loss: 452.1636657714844 = 0.039573486894369125 + 50.0 * 9.042481422424316
Epoch 2500, val loss: 0.6461738348007202
Epoch 2510, training loss: 452.1411437988281 = 0.03900464251637459 + 50.0 * 9.04204273223877
Epoch 2510, val loss: 0.6479681134223938
Epoch 2520, training loss: 452.21185302734375 = 0.038469888269901276 + 50.0 * 9.04346752166748
Epoch 2520, val loss: 0.649965226650238
Epoch 2530, training loss: 452.1261291503906 = 0.03791038319468498 + 50.0 * 9.041764259338379
Epoch 2530, val loss: 0.653997540473938
Epoch 2540, training loss: 452.14337158203125 = 0.037378136068582535 + 50.0 * 9.042119979858398
Epoch 2540, val loss: 0.6553320288658142
Epoch 2550, training loss: 452.181884765625 = 0.036857154220342636 + 50.0 * 9.042900085449219
Epoch 2550, val loss: 0.6576007008552551
Epoch 2560, training loss: 452.130859375 = 0.03632467985153198 + 50.0 * 9.041891098022461
Epoch 2560, val loss: 0.6599741578102112
Epoch 2570, training loss: 452.1634216308594 = 0.03582511469721794 + 50.0 * 9.04255199432373
Epoch 2570, val loss: 0.6622087359428406
Epoch 2580, training loss: 452.1542053222656 = 0.03533550351858139 + 50.0 * 9.042377471923828
Epoch 2580, val loss: 0.666943371295929
Epoch 2590, training loss: 452.2044982910156 = 0.035121090710163116 + 50.0 * 9.043387413024902
Epoch 2590, val loss: 0.6734147667884827
Epoch 2600, training loss: 452.0968322753906 = 0.03435984626412392 + 50.0 * 9.04124927520752
Epoch 2600, val loss: 0.6685819029808044
Epoch 2610, training loss: 452.08642578125 = 0.033870697021484375 + 50.0 * 9.041050910949707
Epoch 2610, val loss: 0.6734166741371155
Epoch 2620, training loss: 452.193359375 = 0.03347814083099365 + 50.0 * 9.043197631835938
Epoch 2620, val loss: 0.6776071786880493
Epoch 2630, training loss: 452.07373046875 = 0.03296894580125809 + 50.0 * 9.040815353393555
Epoch 2630, val loss: 0.6758784651756287
Epoch 2640, training loss: 452.0606994628906 = 0.03251686319708824 + 50.0 * 9.040563583374023
Epoch 2640, val loss: 0.6801565289497375
Epoch 2650, training loss: 452.0701599121094 = 0.032070063054561615 + 50.0 * 9.040761947631836
Epoch 2650, val loss: 0.6807728409767151
Epoch 2660, training loss: 452.176025390625 = 0.03164554014801979 + 50.0 * 9.042887687683105
Epoch 2660, val loss: 0.683664083480835
Epoch 2670, training loss: 452.0933532714844 = 0.031223511323332787 + 50.0 * 9.041242599487305
Epoch 2670, val loss: 0.6855684518814087
Epoch 2680, training loss: 452.0887756347656 = 0.030798830091953278 + 50.0 * 9.041159629821777
Epoch 2680, val loss: 0.6885662078857422
Epoch 2690, training loss: 452.0561828613281 = 0.030392294749617577 + 50.0 * 9.040515899658203
Epoch 2690, val loss: 0.6918816566467285
Epoch 2700, training loss: 452.0902404785156 = 0.03005017526447773 + 50.0 * 9.041203498840332
Epoch 2700, val loss: 0.6956285834312439
Epoch 2710, training loss: 452.0205078125 = 0.029575999826192856 + 50.0 * 9.03981876373291
Epoch 2710, val loss: 0.6934511661529541
Epoch 2720, training loss: 452.0338134765625 = 0.029221437871456146 + 50.0 * 9.040091514587402
Epoch 2720, val loss: 0.6951815485954285
Epoch 2730, training loss: 452.0320129394531 = 0.028800638392567635 + 50.0 * 9.040063858032227
Epoch 2730, val loss: 0.6985602378845215
Epoch 2740, training loss: 452.0261535644531 = 0.02842961810529232 + 50.0 * 9.03995418548584
Epoch 2740, val loss: 0.7025198936462402
Epoch 2750, training loss: 452.0431213378906 = 0.028118783608078957 + 50.0 * 9.040300369262695
Epoch 2750, val loss: 0.7052813172340393
Epoch 2760, training loss: 452.1020812988281 = 0.027840038761496544 + 50.0 * 9.041484832763672
Epoch 2760, val loss: 0.7005437016487122
Epoch 2770, training loss: 451.98748779296875 = 0.027346961200237274 + 50.0 * 9.039202690124512
Epoch 2770, val loss: 0.7095881104469299
Epoch 2780, training loss: 451.96588134765625 = 0.026977619156241417 + 50.0 * 9.038778305053711
Epoch 2780, val loss: 0.7090055346488953
Epoch 2790, training loss: 451.9580383300781 = 0.02662336640059948 + 50.0 * 9.038628578186035
Epoch 2790, val loss: 0.7121734023094177
Epoch 2800, training loss: 452.21759033203125 = 0.02635747380554676 + 50.0 * 9.043824195861816
Epoch 2800, val loss: 0.7115025520324707
Epoch 2810, training loss: 452.1011962890625 = 0.026009410619735718 + 50.0 * 9.04150390625
Epoch 2810, val loss: 0.7180390357971191
Epoch 2820, training loss: 451.9404602050781 = 0.025631573051214218 + 50.0 * 9.038296699523926
Epoch 2820, val loss: 0.7193669676780701
Epoch 2830, training loss: 451.9468078613281 = 0.025306381285190582 + 50.0 * 9.038430213928223
Epoch 2830, val loss: 0.7197461128234863
Epoch 2840, training loss: 451.9225158691406 = 0.0249793604016304 + 50.0 * 9.03795051574707
Epoch 2840, val loss: 0.7224283814430237
Epoch 2850, training loss: 451.95611572265625 = 0.02467411942780018 + 50.0 * 9.038628578186035
Epoch 2850, val loss: 0.7251921892166138
Epoch 2860, training loss: 451.9716491699219 = 0.024380739778280258 + 50.0 * 9.038945198059082
Epoch 2860, val loss: 0.7275924682617188
Epoch 2870, training loss: 451.9306945800781 = 0.02406175620853901 + 50.0 * 9.038132667541504
Epoch 2870, val loss: 0.7281990051269531
Epoch 2880, training loss: 451.91925048828125 = 0.02375822141766548 + 50.0 * 9.037909507751465
Epoch 2880, val loss: 0.7297141551971436
Epoch 2890, training loss: 451.90869140625 = 0.0234634168446064 + 50.0 * 9.037704467773438
Epoch 2890, val loss: 0.7313647866249084
Epoch 2900, training loss: 452.0456848144531 = 0.023188864812254906 + 50.0 * 9.040450096130371
Epoch 2900, val loss: 0.7339820861816406
Epoch 2910, training loss: 452.0120849609375 = 0.022915342822670937 + 50.0 * 9.039783477783203
Epoch 2910, val loss: 0.7382132411003113
Epoch 2920, training loss: 451.91473388671875 = 0.022656336426734924 + 50.0 * 9.037841796875
Epoch 2920, val loss: 0.7357944846153259
Epoch 2930, training loss: 451.8988037109375 = 0.022331571206450462 + 50.0 * 9.037528991699219
Epoch 2930, val loss: 0.7400979995727539
Epoch 2940, training loss: 451.91143798828125 = 0.0220728050917387 + 50.0 * 9.037787437438965
Epoch 2940, val loss: 0.7406522631645203
Epoch 2950, training loss: 451.8976135253906 = 0.021799514070153236 + 50.0 * 9.037516593933105
Epoch 2950, val loss: 0.7430484890937805
Epoch 2960, training loss: 451.88177490234375 = 0.02153225988149643 + 50.0 * 9.03720474243164
Epoch 2960, val loss: 0.7450389266014099
Epoch 2970, training loss: 451.95892333984375 = 0.02128615230321884 + 50.0 * 9.038752555847168
Epoch 2970, val loss: 0.7486899495124817
Epoch 2980, training loss: 451.8816833496094 = 0.021041808649897575 + 50.0 * 9.037213325500488
Epoch 2980, val loss: 0.750831663608551
Epoch 2990, training loss: 451.87371826171875 = 0.020781511440873146 + 50.0 * 9.03705883026123
Epoch 2990, val loss: 0.7518839836120605
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8529
Overall ASR: 0.6902
Flip ASR: 0.6145/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 519.0536499023438 = 1.0989335775375366 + 50.0 * 10.35909366607666
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.20 GiB already allocated; 161.69 MiB free; 6.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0484008789062 = 1.096727967262268 + 50.0 * 10.359033584594727
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 255.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.15 GiB already allocated; 535.69 MiB free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 877.69 MiB free; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 599.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0552368164062 = 1.1032119989395142 + 50.0 * 10.359040260314941
Epoch 0, val loss: 1.102441430091858
Epoch 10, training loss: 518.8215942382812 = 1.0928288698196411 + 50.0 * 10.354575157165527
Epoch 10, val loss: 1.0917707681655884
Epoch 20, training loss: 514.7593994140625 = 1.0790835618972778 + 50.0 * 10.273605346679688
Epoch 20, val loss: 1.0780036449432373
Epoch 30, training loss: 485.8439636230469 = 1.0652996301651 + 50.0 * 9.695572853088379
Epoch 30, val loss: 1.0641565322875977
Epoch 40, training loss: 472.48992919921875 = 1.051410436630249 + 50.0 * 9.428770065307617
Epoch 40, val loss: 1.0501375198364258
Epoch 50, training loss: 469.2647399902344 = 1.036319613456726 + 50.0 * 9.364568710327148
Epoch 50, val loss: 1.0350384712219238
Epoch 60, training loss: 468.0002136230469 = 1.024001121520996 + 50.0 * 9.339524269104004
Epoch 60, val loss: 1.0231472253799438
Epoch 70, training loss: 466.2726745605469 = 1.017108678817749 + 50.0 * 9.305110931396484
Epoch 70, val loss: 1.0165053606033325
Epoch 80, training loss: 464.010009765625 = 1.012619137763977 + 50.0 * 9.259947776794434
Epoch 80, val loss: 1.011914610862732
Epoch 90, training loss: 460.99493408203125 = 1.0068531036376953 + 50.0 * 9.199761390686035
Epoch 90, val loss: 1.0062857866287231
Epoch 100, training loss: 459.0082092285156 = 1.0009737014770508 + 50.0 * 9.160144805908203
Epoch 100, val loss: 1.000597596168518
Epoch 110, training loss: 458.0508117675781 = 0.993722140789032 + 50.0 * 9.141141891479492
Epoch 110, val loss: 0.9935086965560913
Epoch 120, training loss: 457.5224609375 = 0.984549343585968 + 50.0 * 9.130758285522461
Epoch 120, val loss: 0.9841779470443726
Epoch 130, training loss: 457.1310119628906 = 0.9740772843360901 + 50.0 * 9.123138427734375
Epoch 130, val loss: 0.9737818241119385
Epoch 140, training loss: 456.7908020019531 = 0.9634100794792175 + 50.0 * 9.116547584533691
Epoch 140, val loss: 0.9633256196975708
Epoch 150, training loss: 456.4210205078125 = 0.9528382420539856 + 50.0 * 9.109363555908203
Epoch 150, val loss: 0.9529865980148315
Epoch 160, training loss: 456.07550048828125 = 0.942254364490509 + 50.0 * 9.102664947509766
Epoch 160, val loss: 0.9423905611038208
Epoch 170, training loss: 455.8293151855469 = 0.9306439161300659 + 50.0 * 9.097973823547363
Epoch 170, val loss: 0.9309446811676025
Epoch 180, training loss: 455.54302978515625 = 0.9181734919548035 + 50.0 * 9.092496871948242
Epoch 180, val loss: 0.918656051158905
Epoch 190, training loss: 455.4176940917969 = 0.9053866267204285 + 50.0 * 9.090246200561523
Epoch 190, val loss: 0.9060254096984863
Epoch 200, training loss: 455.08282470703125 = 0.8919886946678162 + 50.0 * 9.083816528320312
Epoch 200, val loss: 0.8928297162055969
Epoch 210, training loss: 454.85992431640625 = 0.8782919049263 + 50.0 * 9.079632759094238
Epoch 210, val loss: 0.87956303358078
Epoch 220, training loss: 455.03228759765625 = 0.8641847372055054 + 50.0 * 9.083361625671387
Epoch 220, val loss: 0.865608811378479
Epoch 230, training loss: 454.49981689453125 = 0.8485462665557861 + 50.0 * 9.073025703430176
Epoch 230, val loss: 0.8505325317382812
Epoch 240, training loss: 454.3619689941406 = 0.8330511450767517 + 50.0 * 9.070578575134277
Epoch 240, val loss: 0.8356609344482422
Epoch 250, training loss: 454.1895751953125 = 0.817446768283844 + 50.0 * 9.067442893981934
Epoch 250, val loss: 0.8205323219299316
Epoch 260, training loss: 454.2265319824219 = 0.8010952472686768 + 50.0 * 9.068509101867676
Epoch 260, val loss: 0.8046906590461731
Epoch 270, training loss: 453.9842834472656 = 0.7838031053543091 + 50.0 * 9.064009666442871
Epoch 270, val loss: 0.7882380485534668
Epoch 280, training loss: 453.8196105957031 = 0.7666091918945312 + 50.0 * 9.061059951782227
Epoch 280, val loss: 0.7718878984451294
Epoch 290, training loss: 453.6965026855469 = 0.749526858329773 + 50.0 * 9.058939933776855
Epoch 290, val loss: 0.755575954914093
Epoch 300, training loss: 453.57562255859375 = 0.7322249412536621 + 50.0 * 9.056867599487305
Epoch 300, val loss: 0.7391800284385681
Epoch 310, training loss: 453.8304443359375 = 0.7145897150039673 + 50.0 * 9.06231689453125
Epoch 310, val loss: 0.7226386070251465
Epoch 320, training loss: 453.383056640625 = 0.696476399898529 + 50.0 * 9.053731918334961
Epoch 320, val loss: 0.7055472135543823
Epoch 330, training loss: 453.30230712890625 = 0.679516077041626 + 50.0 * 9.05245590209961
Epoch 330, val loss: 0.6896886229515076
Epoch 340, training loss: 453.18792724609375 = 0.6629737019538879 + 50.0 * 9.050498962402344
Epoch 340, val loss: 0.6743535399436951
Epoch 350, training loss: 453.17474365234375 = 0.6461751461029053 + 50.0 * 9.05057144165039
Epoch 350, val loss: 0.6587452292442322
Epoch 360, training loss: 453.0653991699219 = 0.6295934319496155 + 50.0 * 9.048715591430664
Epoch 360, val loss: 0.6435338258743286
Epoch 370, training loss: 452.9693908691406 = 0.6139592528343201 + 50.0 * 9.04710865020752
Epoch 370, val loss: 0.6292245984077454
Epoch 380, training loss: 452.85986328125 = 0.5987027287483215 + 50.0 * 9.045223236083984
Epoch 380, val loss: 0.6152847409248352
Epoch 390, training loss: 452.800537109375 = 0.584057092666626 + 50.0 * 9.044329643249512
Epoch 390, val loss: 0.6020023226737976
Epoch 400, training loss: 452.8504943847656 = 0.569681704044342 + 50.0 * 9.045616149902344
Epoch 400, val loss: 0.5890385508537292
Epoch 410, training loss: 452.7137756347656 = 0.5557425022125244 + 50.0 * 9.043160438537598
Epoch 410, val loss: 0.5765939354896545
Epoch 420, training loss: 452.6156311035156 = 0.5429742932319641 + 50.0 * 9.04145336151123
Epoch 420, val loss: 0.5652474164962769
Epoch 430, training loss: 452.5485534667969 = 0.5308427810668945 + 50.0 * 9.040353775024414
Epoch 430, val loss: 0.5545451045036316
Epoch 440, training loss: 452.7382507324219 = 0.519010603427887 + 50.0 * 9.044384956359863
Epoch 440, val loss: 0.5441405773162842
Epoch 450, training loss: 452.5212707519531 = 0.5073604583740234 + 50.0 * 9.040278434753418
Epoch 450, val loss: 0.5339213609695435
Epoch 460, training loss: 452.3970642089844 = 0.4969838559627533 + 50.0 * 9.038002014160156
Epoch 460, val loss: 0.525093674659729
Epoch 470, training loss: 452.3465881347656 = 0.4873991012573242 + 50.0 * 9.03718376159668
Epoch 470, val loss: 0.516896665096283
Epoch 480, training loss: 452.3072509765625 = 0.4782216250896454 + 50.0 * 9.036581039428711
Epoch 480, val loss: 0.509080708026886
Epoch 490, training loss: 452.43133544921875 = 0.4694112539291382 + 50.0 * 9.039237976074219
Epoch 490, val loss: 0.5015738010406494
Epoch 500, training loss: 452.4076232910156 = 0.4607592523097992 + 50.0 * 9.03893756866455
Epoch 500, val loss: 0.49454283714294434
Epoch 510, training loss: 452.2508544921875 = 0.45283937454223633 + 50.0 * 9.03596019744873
Epoch 510, val loss: 0.48792943358421326
Epoch 520, training loss: 452.1659240722656 = 0.44573912024497986 + 50.0 * 9.034403800964355
Epoch 520, val loss: 0.4821341633796692
Epoch 530, training loss: 452.11151123046875 = 0.4390515685081482 + 50.0 * 9.033449172973633
Epoch 530, val loss: 0.47675588726997375
Epoch 540, training loss: 452.0846862792969 = 0.43265247344970703 + 50.0 * 9.033041000366211
Epoch 540, val loss: 0.4716787040233612
Epoch 550, training loss: 452.2129821777344 = 0.42650771141052246 + 50.0 * 9.03572940826416
Epoch 550, val loss: 0.46688899397850037
Epoch 560, training loss: 452.0805358886719 = 0.4204462468624115 + 50.0 * 9.033202171325684
Epoch 560, val loss: 0.4620016813278198
Epoch 570, training loss: 452.0206298828125 = 0.414970338344574 + 50.0 * 9.032113075256348
Epoch 570, val loss: 0.45778509974479675
Epoch 580, training loss: 451.9617919921875 = 0.40997180342674255 + 50.0 * 9.031036376953125
Epoch 580, val loss: 0.45396849513053894
Epoch 590, training loss: 452.0174865722656 = 0.40513697266578674 + 50.0 * 9.032246589660645
Epoch 590, val loss: 0.4503948390483856
Epoch 600, training loss: 452.00018310546875 = 0.4003860652446747 + 50.0 * 9.03199577331543
Epoch 600, val loss: 0.44694623351097107
Epoch 610, training loss: 451.885498046875 = 0.39595383405685425 + 50.0 * 9.029790878295898
Epoch 610, val loss: 0.4436478912830353
Epoch 620, training loss: 451.8494567871094 = 0.3918687403202057 + 50.0 * 9.029151916503906
Epoch 620, val loss: 0.4407722055912018
Epoch 630, training loss: 451.8493957519531 = 0.3879544734954834 + 50.0 * 9.029229164123535
Epoch 630, val loss: 0.43798384070396423
Epoch 640, training loss: 451.833984375 = 0.38407811522483826 + 50.0 * 9.028998374938965
Epoch 640, val loss: 0.4352884590625763
Epoch 650, training loss: 451.8133850097656 = 0.3804224133491516 + 50.0 * 9.028658866882324
Epoch 650, val loss: 0.43277886509895325
Epoch 660, training loss: 451.7446594238281 = 0.3769718110561371 + 50.0 * 9.027353286743164
Epoch 660, val loss: 0.4304299056529999
Epoch 670, training loss: 451.7417297363281 = 0.3736954629421234 + 50.0 * 9.027360916137695
Epoch 670, val loss: 0.4283022880554199
Epoch 680, training loss: 451.7371520996094 = 0.37047484517097473 + 50.0 * 9.02733325958252
Epoch 680, val loss: 0.42621853947639465
Epoch 690, training loss: 451.68670654296875 = 0.3674202263355255 + 50.0 * 9.026385307312012
Epoch 690, val loss: 0.42415347695350647
Epoch 700, training loss: 451.6639404296875 = 0.3644854426383972 + 50.0 * 9.025989532470703
Epoch 700, val loss: 0.4222753643989563
Epoch 710, training loss: 451.67218017578125 = 0.36163610219955444 + 50.0 * 9.02621078491211
Epoch 710, val loss: 0.42047324776649475
Epoch 720, training loss: 451.7575988769531 = 0.358858197927475 + 50.0 * 9.027975082397461
Epoch 720, val loss: 0.41890212893486023
Epoch 730, training loss: 451.63275146484375 = 0.35617196559906006 + 50.0 * 9.025531768798828
Epoch 730, val loss: 0.4172804653644562
Epoch 740, training loss: 451.54913330078125 = 0.3536785840988159 + 50.0 * 9.023909568786621
Epoch 740, val loss: 0.41584742069244385
Epoch 750, training loss: 451.53094482421875 = 0.3512927293777466 + 50.0 * 9.023592948913574
Epoch 750, val loss: 0.4144993722438812
Epoch 760, training loss: 451.62713623046875 = 0.34894511103630066 + 50.0 * 9.025564193725586
Epoch 760, val loss: 0.4132479727268219
Epoch 770, training loss: 451.53564453125 = 0.34654700756073 + 50.0 * 9.023781776428223
Epoch 770, val loss: 0.4119228720664978
Epoch 780, training loss: 451.5482482910156 = 0.344326913356781 + 50.0 * 9.024078369140625
Epoch 780, val loss: 0.4107738435268402
Epoch 790, training loss: 451.4627380371094 = 0.34213733673095703 + 50.0 * 9.022412300109863
Epoch 790, val loss: 0.409584641456604
Epoch 800, training loss: 451.45513916015625 = 0.340056449174881 + 50.0 * 9.02230167388916
Epoch 800, val loss: 0.40850135684013367
Epoch 810, training loss: 451.4716491699219 = 0.33801472187042236 + 50.0 * 9.022672653198242
Epoch 810, val loss: 0.40749600529670715
Epoch 820, training loss: 451.4650573730469 = 0.3359832167625427 + 50.0 * 9.022582054138184
Epoch 820, val loss: 0.4065236449241638
Epoch 830, training loss: 451.4966735839844 = 0.334011435508728 + 50.0 * 9.023253440856934
Epoch 830, val loss: 0.4057801067829132
Epoch 840, training loss: 451.3881530761719 = 0.3321002125740051 + 50.0 * 9.02112102508545
Epoch 840, val loss: 0.4046575427055359
Epoch 850, training loss: 451.3541259765625 = 0.33026358485221863 + 50.0 * 9.020477294921875
Epoch 850, val loss: 0.4039320647716522
Epoch 860, training loss: 451.4326171875 = 0.3284635543823242 + 50.0 * 9.022083282470703
Epoch 860, val loss: 0.40315791964530945
Epoch 870, training loss: 451.4481506347656 = 0.3266225755214691 + 50.0 * 9.022430419921875
Epoch 870, val loss: 0.4023209512233734
Epoch 880, training loss: 451.30426025390625 = 0.3248935639858246 + 50.0 * 9.019587516784668
Epoch 880, val loss: 0.40164095163345337
Epoch 890, training loss: 451.2943115234375 = 0.32325872778892517 + 50.0 * 9.019420623779297
Epoch 890, val loss: 0.40097886323928833
Epoch 900, training loss: 451.2715759277344 = 0.3216365575790405 + 50.0 * 9.018999099731445
Epoch 900, val loss: 0.40025758743286133
Epoch 910, training loss: 451.582275390625 = 0.320016473531723 + 50.0 * 9.025245666503906
Epoch 910, val loss: 0.39954036474227905
Epoch 920, training loss: 451.3425598144531 = 0.3183329105377197 + 50.0 * 9.020484924316406
Epoch 920, val loss: 0.39918750524520874
Epoch 930, training loss: 451.2365417480469 = 0.3168129622936249 + 50.0 * 9.018394470214844
Epoch 930, val loss: 0.39853909611701965
Epoch 940, training loss: 451.2227783203125 = 0.3153204917907715 + 50.0 * 9.018149375915527
Epoch 940, val loss: 0.3979228138923645
Epoch 950, training loss: 451.31927490234375 = 0.3138256371021271 + 50.0 * 9.020109176635742
Epoch 950, val loss: 0.39739227294921875
Epoch 960, training loss: 451.25616455078125 = 0.31231170892715454 + 50.0 * 9.018877029418945
Epoch 960, val loss: 0.39697977900505066
Epoch 970, training loss: 451.2174377441406 = 0.3108706474304199 + 50.0 * 9.018131256103516
Epoch 970, val loss: 0.3964853584766388
Epoch 980, training loss: 451.1773986816406 = 0.3094576597213745 + 50.0 * 9.017358779907227
Epoch 980, val loss: 0.39603161811828613
Epoch 990, training loss: 451.24322509765625 = 0.3080548942089081 + 50.0 * 9.01870346069336
Epoch 990, val loss: 0.3955293595790863
Epoch 1000, training loss: 451.2277526855469 = 0.306644469499588 + 50.0 * 9.01842212677002
Epoch 1000, val loss: 0.39507153630256653
Epoch 1010, training loss: 451.14739990234375 = 0.30528953671455383 + 50.0 * 9.016841888427734
Epoch 1010, val loss: 0.3947469890117645
Epoch 1020, training loss: 451.1188049316406 = 0.3039722144603729 + 50.0 * 9.01629638671875
Epoch 1020, val loss: 0.39429599046707153
Epoch 1030, training loss: 451.1065979003906 = 0.30265578627586365 + 50.0 * 9.01607894897461
Epoch 1030, val loss: 0.39397960901260376
Epoch 1040, training loss: 451.36419677734375 = 0.3013363778591156 + 50.0 * 9.021257400512695
Epoch 1040, val loss: 0.39372095465660095
Epoch 1050, training loss: 451.1536865234375 = 0.30002328753471375 + 50.0 * 9.017073631286621
Epoch 1050, val loss: 0.39320307970046997
Epoch 1060, training loss: 451.086181640625 = 0.29876944422721863 + 50.0 * 9.015748023986816
Epoch 1060, val loss: 0.39292871952056885
Epoch 1070, training loss: 451.1363830566406 = 0.2975270450115204 + 50.0 * 9.016777038574219
Epoch 1070, val loss: 0.3926326334476471
Epoch 1080, training loss: 451.0533752441406 = 0.29630130529403687 + 50.0 * 9.015141487121582
Epoch 1080, val loss: 0.39233681559562683
Epoch 1090, training loss: 451.0929870605469 = 0.2951003313064575 + 50.0 * 9.015957832336426
Epoch 1090, val loss: 0.39197075366973877
Epoch 1100, training loss: 451.12677001953125 = 0.2938744127750397 + 50.0 * 9.016657829284668
Epoch 1100, val loss: 0.39174458384513855
Epoch 1110, training loss: 451.03216552734375 = 0.2926832139492035 + 50.0 * 9.014789581298828
Epoch 1110, val loss: 0.39163443446159363
Epoch 1120, training loss: 451.00830078125 = 0.29151806235313416 + 50.0 * 9.014335632324219
Epoch 1120, val loss: 0.3913073241710663
Epoch 1130, training loss: 451.0118408203125 = 0.290361225605011 + 50.0 * 9.014429092407227
Epoch 1130, val loss: 0.3910658359527588
Epoch 1140, training loss: 451.1742248535156 = 0.28920096158981323 + 50.0 * 9.0177001953125
Epoch 1140, val loss: 0.39092016220092773
Epoch 1150, training loss: 451.0413513183594 = 0.28804919123649597 + 50.0 * 9.015066146850586
Epoch 1150, val loss: 0.3907335698604584
Epoch 1160, training loss: 451.0160217285156 = 0.28691691160202026 + 50.0 * 9.014581680297852
Epoch 1160, val loss: 0.3905309736728668
Epoch 1170, training loss: 451.0556945800781 = 0.2858043313026428 + 50.0 * 9.015398025512695
Epoch 1170, val loss: 0.3903258442878723
Epoch 1180, training loss: 450.9967041015625 = 0.2847016751766205 + 50.0 * 9.014240264892578
Epoch 1180, val loss: 0.3902839720249176
Epoch 1190, training loss: 450.9931945800781 = 0.2836064398288727 + 50.0 * 9.014191627502441
Epoch 1190, val loss: 0.39015281200408936
Epoch 1200, training loss: 450.9649658203125 = 0.2825208604335785 + 50.0 * 9.013648986816406
Epoch 1200, val loss: 0.3899664580821991
Epoch 1210, training loss: 450.9413146972656 = 0.28144124150276184 + 50.0 * 9.013197898864746
Epoch 1210, val loss: 0.3896578550338745
Epoch 1220, training loss: 451.0417175292969 = 0.2803650200366974 + 50.0 * 9.015227317810059
Epoch 1220, val loss: 0.38968417048454285
Epoch 1230, training loss: 451.0068664550781 = 0.2793094515800476 + 50.0 * 9.014551162719727
Epoch 1230, val loss: 0.3897823691368103
Epoch 1240, training loss: 450.9500732421875 = 0.2782508134841919 + 50.0 * 9.013436317443848
Epoch 1240, val loss: 0.3894626498222351
Epoch 1250, training loss: 450.9091796875 = 0.2772335410118103 + 50.0 * 9.012639045715332
Epoch 1250, val loss: 0.3893382251262665
Epoch 1260, training loss: 450.882080078125 = 0.27620986104011536 + 50.0 * 9.012117385864258
Epoch 1260, val loss: 0.3892388641834259
Epoch 1270, training loss: 450.8797912597656 = 0.27518394589424133 + 50.0 * 9.012092590332031
Epoch 1270, val loss: 0.38912439346313477
Epoch 1280, training loss: 451.05828857421875 = 0.2741604745388031 + 50.0 * 9.015682220458984
Epoch 1280, val loss: 0.38884082436561584
Epoch 1290, training loss: 451.0000305175781 = 0.2731391489505768 + 50.0 * 9.014537811279297
Epoch 1290, val loss: 0.38924071192741394
Epoch 1300, training loss: 450.8885803222656 = 0.2721392512321472 + 50.0 * 9.0123291015625
Epoch 1300, val loss: 0.3892255127429962
Epoch 1310, training loss: 450.8784484863281 = 0.2711523473262787 + 50.0 * 9.01214599609375
Epoch 1310, val loss: 0.38908571004867554
Epoch 1320, training loss: 450.8379821777344 = 0.270182728767395 + 50.0 * 9.011356353759766
Epoch 1320, val loss: 0.3890288770198822
Epoch 1330, training loss: 450.8586730957031 = 0.26920390129089355 + 50.0 * 9.011789321899414
Epoch 1330, val loss: 0.3891013562679291
Epoch 1340, training loss: 450.9378356933594 = 0.26822784543037415 + 50.0 * 9.013392448425293
Epoch 1340, val loss: 0.38919517397880554
Epoch 1350, training loss: 450.84429931640625 = 0.2672598958015442 + 50.0 * 9.011540412902832
Epoch 1350, val loss: 0.3890175223350525
Epoch 1360, training loss: 450.8193664550781 = 0.2663143277168274 + 50.0 * 9.01106071472168
Epoch 1360, val loss: 0.38898155093193054
Epoch 1370, training loss: 450.93585205078125 = 0.2653786838054657 + 50.0 * 9.013409614562988
Epoch 1370, val loss: 0.3890012800693512
Epoch 1380, training loss: 450.893310546875 = 0.2644161283969879 + 50.0 * 9.012578010559082
Epoch 1380, val loss: 0.38911041617393494
Epoch 1390, training loss: 450.8128662109375 = 0.2634987533092499 + 50.0 * 9.010987281799316
Epoch 1390, val loss: 0.38912609219551086
Epoch 1400, training loss: 450.7837219238281 = 0.26256611943244934 + 50.0 * 9.010422706604004
Epoch 1400, val loss: 0.38930362462997437
Epoch 1410, training loss: 450.7713317871094 = 0.2616402804851532 + 50.0 * 9.010193824768066
Epoch 1410, val loss: 0.38927653431892395
Epoch 1420, training loss: 450.8403015136719 = 0.26071643829345703 + 50.0 * 9.011591911315918
Epoch 1420, val loss: 0.38948971033096313
Epoch 1430, training loss: 450.7779846191406 = 0.2597813606262207 + 50.0 * 9.010363578796387
Epoch 1430, val loss: 0.3894108533859253
Epoch 1440, training loss: 450.7892761230469 = 0.25886884331703186 + 50.0 * 9.010607719421387
Epoch 1440, val loss: 0.3893786370754242
Epoch 1450, training loss: 450.7726135253906 = 0.2579647898674011 + 50.0 * 9.010293006896973
Epoch 1450, val loss: 0.38954412937164307
Epoch 1460, training loss: 450.7648620605469 = 0.2570687532424927 + 50.0 * 9.01015567779541
Epoch 1460, val loss: 0.38977929949760437
Epoch 1470, training loss: 450.8062438964844 = 0.256176620721817 + 50.0 * 9.011001586914062
Epoch 1470, val loss: 0.38985535502433777
Epoch 1480, training loss: 450.75787353515625 = 0.25527718663215637 + 50.0 * 9.010051727294922
Epoch 1480, val loss: 0.3897489011287689
Epoch 1490, training loss: 450.7284240722656 = 0.2543971836566925 + 50.0 * 9.009480476379395
Epoch 1490, val loss: 0.3900333344936371
Epoch 1500, training loss: 450.76068115234375 = 0.2535126507282257 + 50.0 * 9.010143280029297
Epoch 1500, val loss: 0.39028093218803406
Epoch 1510, training loss: 450.7286376953125 = 0.2526271641254425 + 50.0 * 9.009520530700684
Epoch 1510, val loss: 0.3902403712272644
Epoch 1520, training loss: 450.7013244628906 = 0.25175780057907104 + 50.0 * 9.008991241455078
Epoch 1520, val loss: 0.39031916856765747
Epoch 1530, training loss: 450.6934509277344 = 0.2508814334869385 + 50.0 * 9.008851051330566
Epoch 1530, val loss: 0.3905782997608185
Epoch 1540, training loss: 450.76202392578125 = 0.2500098645687103 + 50.0 * 9.01024055480957
Epoch 1540, val loss: 0.3907029926776886
Epoch 1550, training loss: 450.6860656738281 = 0.2491367906332016 + 50.0 * 9.00873851776123
Epoch 1550, val loss: 0.3908507823944092
Epoch 1560, training loss: 450.7313232421875 = 0.24827945232391357 + 50.0 * 9.009660720825195
Epoch 1560, val loss: 0.391123503446579
Epoch 1570, training loss: 450.76739501953125 = 0.2474299967288971 + 50.0 * 9.010398864746094
Epoch 1570, val loss: 0.3911955952644348
Epoch 1580, training loss: 450.6953430175781 = 0.24658387899398804 + 50.0 * 9.0089750289917
Epoch 1580, val loss: 0.3914523720741272
Epoch 1590, training loss: 450.67095947265625 = 0.2457365095615387 + 50.0 * 9.008504867553711
Epoch 1590, val loss: 0.39161738753318787
Epoch 1600, training loss: 450.6942138671875 = 0.2448958307504654 + 50.0 * 9.008986473083496
Epoch 1600, val loss: 0.3917926549911499
Epoch 1610, training loss: 450.68792724609375 = 0.24405322968959808 + 50.0 * 9.008877754211426
Epoch 1610, val loss: 0.39210185408592224
Epoch 1620, training loss: 450.6730041503906 = 0.24322077631950378 + 50.0 * 9.00859546661377
Epoch 1620, val loss: 0.3921349346637726
Epoch 1630, training loss: 450.6518249511719 = 0.24238207936286926 + 50.0 * 9.00818920135498
Epoch 1630, val loss: 0.3922925591468811
Epoch 1640, training loss: 450.6502380371094 = 0.2415458858013153 + 50.0 * 9.008173942565918
Epoch 1640, val loss: 0.3925172686576843
Epoch 1650, training loss: 450.69390869140625 = 0.24071507155895233 + 50.0 * 9.009063720703125
Epoch 1650, val loss: 0.3928617537021637
Epoch 1660, training loss: 450.69598388671875 = 0.23989884555339813 + 50.0 * 9.009121894836426
Epoch 1660, val loss: 0.3934380114078522
Epoch 1670, training loss: 450.6261291503906 = 0.23906037211418152 + 50.0 * 9.00774097442627
Epoch 1670, val loss: 0.3933050036430359
Epoch 1680, training loss: 450.59912109375 = 0.2382495254278183 + 50.0 * 9.007217407226562
Epoch 1680, val loss: 0.39385145902633667
Epoch 1690, training loss: 450.6255187988281 = 0.2374383956193924 + 50.0 * 9.00776195526123
Epoch 1690, val loss: 0.39416834712028503
Epoch 1700, training loss: 450.65850830078125 = 0.23661589622497559 + 50.0 * 9.008438110351562
Epoch 1700, val loss: 0.3943762183189392
Epoch 1710, training loss: 450.63421630859375 = 0.23580467700958252 + 50.0 * 9.007967948913574
Epoch 1710, val loss: 0.394369900226593
Epoch 1720, training loss: 450.615966796875 = 0.23499859869480133 + 50.0 * 9.007619857788086
Epoch 1720, val loss: 0.3949047029018402
Epoch 1730, training loss: 450.6046142578125 = 0.23419378697872162 + 50.0 * 9.007408142089844
Epoch 1730, val loss: 0.3951480984687805
Epoch 1740, training loss: 450.6395568847656 = 0.23339948058128357 + 50.0 * 9.008123397827148
Epoch 1740, val loss: 0.395284503698349
Epoch 1750, training loss: 450.57049560546875 = 0.23258845508098602 + 50.0 * 9.006758689880371
Epoch 1750, val loss: 0.3957386314868927
Epoch 1760, training loss: 450.5500793457031 = 0.23179104924201965 + 50.0 * 9.006365776062012
Epoch 1760, val loss: 0.3960784077644348
Epoch 1770, training loss: 450.56878662109375 = 0.23099131882190704 + 50.0 * 9.006755828857422
Epoch 1770, val loss: 0.3962264060974121
Epoch 1780, training loss: 450.67803955078125 = 0.23019753396511078 + 50.0 * 9.008956909179688
Epoch 1780, val loss: 0.39694449305534363
Epoch 1790, training loss: 450.5554504394531 = 0.2294158786535263 + 50.0 * 9.006521224975586
Epoch 1790, val loss: 0.3969746530056
Epoch 1800, training loss: 450.5329895019531 = 0.2286352962255478 + 50.0 * 9.006087303161621
Epoch 1800, val loss: 0.39762988686561584
Epoch 1810, training loss: 450.5222473144531 = 0.22784405946731567 + 50.0 * 9.005887985229492
Epoch 1810, val loss: 0.39773502945899963
Epoch 1820, training loss: 450.55169677734375 = 0.22706107795238495 + 50.0 * 9.006492614746094
Epoch 1820, val loss: 0.3980417847633362
Epoch 1830, training loss: 450.5863037109375 = 0.22627903521060944 + 50.0 * 9.007200241088867
Epoch 1830, val loss: 0.39860543608665466
Epoch 1840, training loss: 450.50494384765625 = 0.22551240026950836 + 50.0 * 9.00558853149414
Epoch 1840, val loss: 0.39883890748023987
Epoch 1850, training loss: 450.5376892089844 = 0.22473812103271484 + 50.0 * 9.006258964538574
Epoch 1850, val loss: 0.3993159532546997
Epoch 1860, training loss: 450.5863342285156 = 0.22398506104946136 + 50.0 * 9.007246971130371
Epoch 1860, val loss: 0.3996182084083557
Epoch 1870, training loss: 450.48126220703125 = 0.22319374978542328 + 50.0 * 9.00516128540039
Epoch 1870, val loss: 0.40022119879722595
Epoch 1880, training loss: 450.4781799316406 = 0.22242102026939392 + 50.0 * 9.005115509033203
Epoch 1880, val loss: 0.4006829261779785
Epoch 1890, training loss: 450.4800109863281 = 0.22164620459079742 + 50.0 * 9.005167007446289
Epoch 1890, val loss: 0.4010395109653473
Epoch 1900, training loss: 450.59564208984375 = 0.22087764739990234 + 50.0 * 9.007494926452637
Epoch 1900, val loss: 0.40162724256515503
Epoch 1910, training loss: 450.5774230957031 = 0.22011612355709076 + 50.0 * 9.007145881652832
Epoch 1910, val loss: 0.40196725726127625
Epoch 1920, training loss: 450.53021240234375 = 0.2193675935268402 + 50.0 * 9.006217002868652
Epoch 1920, val loss: 0.4022361934185028
Epoch 1930, training loss: 450.48748779296875 = 0.21860353648662567 + 50.0 * 9.005377769470215
Epoch 1930, val loss: 0.4029230773448944
Epoch 1940, training loss: 450.52105712890625 = 0.21784719824790955 + 50.0 * 9.006064414978027
Epoch 1940, val loss: 0.4032382369041443
Epoch 1950, training loss: 450.4777526855469 = 0.2170967310667038 + 50.0 * 9.005212783813477
Epoch 1950, val loss: 0.4036746621131897
Epoch 1960, training loss: 450.4822082519531 = 0.21634770929813385 + 50.0 * 9.005317687988281
Epoch 1960, val loss: 0.40423905849456787
Epoch 1970, training loss: 450.45196533203125 = 0.21559755504131317 + 50.0 * 9.004727363586426
Epoch 1970, val loss: 0.40490230917930603
Epoch 1980, training loss: 450.5333251953125 = 0.21485581994056702 + 50.0 * 9.006369590759277
Epoch 1980, val loss: 0.4050171673297882
Epoch 1990, training loss: 450.44976806640625 = 0.21410046517848969 + 50.0 * 9.00471305847168
Epoch 1990, val loss: 0.40569883584976196
Epoch 2000, training loss: 450.42291259765625 = 0.2133554071187973 + 50.0 * 9.004191398620605
Epoch 2000, val loss: 0.40639975666999817
Epoch 2010, training loss: 450.4078674316406 = 0.21260786056518555 + 50.0 * 9.003905296325684
Epoch 2010, val loss: 0.4068562686443329
Epoch 2020, training loss: 450.4078674316406 = 0.21185971796512604 + 50.0 * 9.003920555114746
Epoch 2020, val loss: 0.4073737859725952
Epoch 2030, training loss: 450.607421875 = 0.21113167703151703 + 50.0 * 9.007925987243652
Epoch 2030, val loss: 0.4081992506980896
Epoch 2040, training loss: 450.47552490234375 = 0.21037626266479492 + 50.0 * 9.005302429199219
Epoch 2040, val loss: 0.4082421660423279
Epoch 2050, training loss: 450.4222412109375 = 0.20964069664478302 + 50.0 * 9.004251480102539
Epoch 2050, val loss: 0.40908104181289673
Epoch 2060, training loss: 450.39892578125 = 0.20890432596206665 + 50.0 * 9.003800392150879
Epoch 2060, val loss: 0.4095326066017151
Epoch 2070, training loss: 450.4652099609375 = 0.20817342400550842 + 50.0 * 9.005141258239746
Epoch 2070, val loss: 0.41018563508987427
Epoch 2080, training loss: 450.3810119628906 = 0.20743314921855927 + 50.0 * 9.003471374511719
Epoch 2080, val loss: 0.4108176529407501
Epoch 2090, training loss: 450.4275817871094 = 0.2067030817270279 + 50.0 * 9.004417419433594
Epoch 2090, val loss: 0.41149264574050903
Epoch 2100, training loss: 450.4191589355469 = 0.20597630739212036 + 50.0 * 9.004263877868652
Epoch 2100, val loss: 0.41213807463645935
Epoch 2110, training loss: 450.4277648925781 = 0.20523490011692047 + 50.0 * 9.004450798034668
Epoch 2110, val loss: 0.4124671518802643
Epoch 2120, training loss: 450.40863037109375 = 0.20451173186302185 + 50.0 * 9.004082679748535
Epoch 2120, val loss: 0.41307464241981506
Epoch 2130, training loss: 450.4088439941406 = 0.2037871778011322 + 50.0 * 9.004100799560547
Epoch 2130, val loss: 0.4136219322681427
Epoch 2140, training loss: 450.36279296875 = 0.20306161046028137 + 50.0 * 9.003194808959961
Epoch 2140, val loss: 0.41419702768325806
Epoch 2150, training loss: 450.3728942871094 = 0.2023380696773529 + 50.0 * 9.003411293029785
Epoch 2150, val loss: 0.41473188996315
Epoch 2160, training loss: 450.3987121582031 = 0.20162153244018555 + 50.0 * 9.003941535949707
Epoch 2160, val loss: 0.4152795970439911
Epoch 2170, training loss: 450.3759460449219 = 0.20089653134346008 + 50.0 * 9.003500938415527
Epoch 2170, val loss: 0.41643160581588745
Epoch 2180, training loss: 450.4051208496094 = 0.2001764178276062 + 50.0 * 9.004098892211914
Epoch 2180, val loss: 0.4170167148113251
Epoch 2190, training loss: 450.39349365234375 = 0.19946648180484772 + 50.0 * 9.003880500793457
Epoch 2190, val loss: 0.4171728789806366
Epoch 2200, training loss: 450.3466796875 = 0.19874975085258484 + 50.0 * 9.002958297729492
Epoch 2200, val loss: 0.4184221625328064
Epoch 2210, training loss: 450.3584289550781 = 0.19803233444690704 + 50.0 * 9.00320816040039
Epoch 2210, val loss: 0.41893646121025085
Epoch 2220, training loss: 450.34375 = 0.19732804596424103 + 50.0 * 9.002928733825684
Epoch 2220, val loss: 0.4195718467235565
Epoch 2230, training loss: 450.337158203125 = 0.19660450518131256 + 50.0 * 9.002811431884766
Epoch 2230, val loss: 0.42006805539131165
Epoch 2240, training loss: 450.52850341796875 = 0.19594097137451172 + 50.0 * 9.006650924682617
Epoch 2240, val loss: 0.42068126797676086
Epoch 2250, training loss: 450.355712890625 = 0.1952132135629654 + 50.0 * 9.003210067749023
Epoch 2250, val loss: 0.4219561219215393
Epoch 2260, training loss: 450.2967224121094 = 0.19448862969875336 + 50.0 * 9.002044677734375
Epoch 2260, val loss: 0.42238956689834595
Epoch 2270, training loss: 450.28094482421875 = 0.19377917051315308 + 50.0 * 9.00174331665039
Epoch 2270, val loss: 0.4232565462589264
Epoch 2280, training loss: 450.2808532714844 = 0.19306428730487823 + 50.0 * 9.001755714416504
Epoch 2280, val loss: 0.4238916337490082
Epoch 2290, training loss: 450.4453125 = 0.19236382842063904 + 50.0 * 9.005059242248535
Epoch 2290, val loss: 0.4247921407222748
Epoch 2300, training loss: 450.3495788574219 = 0.19165854156017303 + 50.0 * 9.003158569335938
Epoch 2300, val loss: 0.42561885714530945
Epoch 2310, training loss: 450.3414001464844 = 0.19095535576343536 + 50.0 * 9.003008842468262
Epoch 2310, val loss: 0.4263480603694916
Epoch 2320, training loss: 450.3092346191406 = 0.19026444852352142 + 50.0 * 9.002379417419434
Epoch 2320, val loss: 0.42714419960975647
Epoch 2330, training loss: 450.2984313964844 = 0.18956878781318665 + 50.0 * 9.002177238464355
Epoch 2330, val loss: 0.42799773812294006
Epoch 2340, training loss: 450.3157653808594 = 0.18887561559677124 + 50.0 * 9.002537727355957
Epoch 2340, val loss: 0.42861953377723694
Epoch 2350, training loss: 450.3286437988281 = 0.18817195296287537 + 50.0 * 9.002809524536133
Epoch 2350, val loss: 0.4292137026786804
Epoch 2360, training loss: 450.2770080566406 = 0.18747934699058533 + 50.0 * 9.001791000366211
Epoch 2360, val loss: 0.43031322956085205
Epoch 2370, training loss: 450.2476806640625 = 0.18678472936153412 + 50.0 * 9.00121784210205
Epoch 2370, val loss: 0.43087467551231384
Epoch 2380, training loss: 450.24078369140625 = 0.18608911335468292 + 50.0 * 9.001093864440918
Epoch 2380, val loss: 0.43161705136299133
Epoch 2390, training loss: 450.25921630859375 = 0.1853908896446228 + 50.0 * 9.001476287841797
Epoch 2390, val loss: 0.432475209236145
Epoch 2400, training loss: 450.33306884765625 = 0.1846979856491089 + 50.0 * 9.002967834472656
Epoch 2400, val loss: 0.43334630131721497
Epoch 2410, training loss: 450.3240661621094 = 0.18404287099838257 + 50.0 * 9.002799987792969
Epoch 2410, val loss: 0.4346044659614563
Epoch 2420, training loss: 450.2844543457031 = 0.18333227932453156 + 50.0 * 9.002022743225098
Epoch 2420, val loss: 0.43545588850975037
Epoch 2430, training loss: 450.2586669921875 = 0.182643860578537 + 50.0 * 9.001520156860352
Epoch 2430, val loss: 0.4359382688999176
Epoch 2440, training loss: 450.2206726074219 = 0.18195423483848572 + 50.0 * 9.000774383544922
Epoch 2440, val loss: 0.43677642941474915
Epoch 2450, training loss: 450.2091064453125 = 0.18126623332500458 + 50.0 * 9.000556945800781
Epoch 2450, val loss: 0.4376975893974304
Epoch 2460, training loss: 450.34088134765625 = 0.18058516085147858 + 50.0 * 9.003206253051758
Epoch 2460, val loss: 0.4386146366596222
Epoch 2470, training loss: 450.2529602050781 = 0.17989486455917358 + 50.0 * 9.001461029052734
Epoch 2470, val loss: 0.4392509162425995
Epoch 2480, training loss: 450.22747802734375 = 0.17922396957874298 + 50.0 * 9.000965118408203
Epoch 2480, val loss: 0.4407306909561157
Epoch 2490, training loss: 450.21990966796875 = 0.17854274809360504 + 50.0 * 9.00082778930664
Epoch 2490, val loss: 0.4415106177330017
Epoch 2500, training loss: 450.2261657714844 = 0.17786183953285217 + 50.0 * 9.00096607208252
Epoch 2500, val loss: 0.44244322180747986
Epoch 2510, training loss: 450.3145751953125 = 0.1771901696920395 + 50.0 * 9.002747535705566
Epoch 2510, val loss: 0.44332006573677063
Epoch 2520, training loss: 450.2445983886719 = 0.1765013337135315 + 50.0 * 9.001361846923828
Epoch 2520, val loss: 0.4444027245044708
Epoch 2530, training loss: 450.21087646484375 = 0.17582739889621735 + 50.0 * 9.000700950622559
Epoch 2530, val loss: 0.445157915353775
Epoch 2540, training loss: 450.189453125 = 0.17515356838703156 + 50.0 * 9.000286102294922
Epoch 2540, val loss: 0.44612807035446167
Epoch 2550, training loss: 450.1760559082031 = 0.17447352409362793 + 50.0 * 9.000031471252441
Epoch 2550, val loss: 0.44707930088043213
Epoch 2560, training loss: 450.2198181152344 = 0.1737978309392929 + 50.0 * 9.000920295715332
Epoch 2560, val loss: 0.448177307844162
Epoch 2570, training loss: 450.2100830078125 = 0.17311808466911316 + 50.0 * 9.000739097595215
Epoch 2570, val loss: 0.4488186538219452
Epoch 2580, training loss: 450.2327880859375 = 0.172464057803154 + 50.0 * 9.001206398010254
Epoch 2580, val loss: 0.449672669172287
Epoch 2590, training loss: 450.2226257324219 = 0.1717737466096878 + 50.0 * 9.001016616821289
Epoch 2590, val loss: 0.45093637704849243
Epoch 2600, training loss: 450.1989440917969 = 0.17113251984119415 + 50.0 * 9.000555992126465
Epoch 2600, val loss: 0.4515451490879059
Epoch 2610, training loss: 450.2054443359375 = 0.17044010758399963 + 50.0 * 9.000699996948242
Epoch 2610, val loss: 0.4526929259300232
Epoch 2620, training loss: 450.2301025390625 = 0.16977764666080475 + 50.0 * 9.001206398010254
Epoch 2620, val loss: 0.4539702534675598
Epoch 2630, training loss: 450.1522521972656 = 0.16910982131958008 + 50.0 * 8.999663352966309
Epoch 2630, val loss: 0.4551648497581482
Epoch 2640, training loss: 450.1347961425781 = 0.16844043135643005 + 50.0 * 8.999327659606934
Epoch 2640, val loss: 0.4560980498790741
Epoch 2650, training loss: 450.14697265625 = 0.16776439547538757 + 50.0 * 8.999584197998047
Epoch 2650, val loss: 0.45692548155784607
Epoch 2660, training loss: 450.3023376464844 = 0.1670999974012375 + 50.0 * 9.002704620361328
Epoch 2660, val loss: 0.4578397572040558
Epoch 2670, training loss: 450.1759948730469 = 0.16646450757980347 + 50.0 * 9.000190734863281
Epoch 2670, val loss: 0.4596520960330963
Epoch 2680, training loss: 450.14892578125 = 0.1657802164554596 + 50.0 * 8.999663352966309
Epoch 2680, val loss: 0.4604921340942383
Epoch 2690, training loss: 450.1755676269531 = 0.1651499718427658 + 50.0 * 9.000207901000977
Epoch 2690, val loss: 0.4618702232837677
Epoch 2700, training loss: 450.1206970214844 = 0.1644645482301712 + 50.0 * 8.999124526977539
Epoch 2700, val loss: 0.4627986550331116
Epoch 2710, training loss: 450.2245178222656 = 0.16381888091564178 + 50.0 * 9.001214027404785
Epoch 2710, val loss: 0.46415629982948303
Epoch 2720, training loss: 450.1386413574219 = 0.16314779222011566 + 50.0 * 8.999509811401367
Epoch 2720, val loss: 0.4646323323249817
Epoch 2730, training loss: 450.12939453125 = 0.162483811378479 + 50.0 * 8.999338150024414
Epoch 2730, val loss: 0.4658578336238861
Epoch 2740, training loss: 450.1441345214844 = 0.16184315085411072 + 50.0 * 8.999646186828613
Epoch 2740, val loss: 0.467281311750412
Epoch 2750, training loss: 450.127685546875 = 0.16118685901165009 + 50.0 * 8.999329566955566
Epoch 2750, val loss: 0.4683622121810913
Epoch 2760, training loss: 450.1589050292969 = 0.16052940487861633 + 50.0 * 8.999967575073242
Epoch 2760, val loss: 0.46920105814933777
Epoch 2770, training loss: 450.14251708984375 = 0.159873366355896 + 50.0 * 8.999652862548828
Epoch 2770, val loss: 0.4704010784626007
Epoch 2780, training loss: 450.083740234375 = 0.1592176854610443 + 50.0 * 8.998490333557129
Epoch 2780, val loss: 0.47152164578437805
Epoch 2790, training loss: 450.0829772949219 = 0.15856504440307617 + 50.0 * 8.998488426208496
Epoch 2790, val loss: 0.47269999980926514
Epoch 2800, training loss: 450.1134338378906 = 0.1579146683216095 + 50.0 * 8.999110221862793
Epoch 2800, val loss: 0.4739914536476135
Epoch 2810, training loss: 450.15777587890625 = 0.15726985037326813 + 50.0 * 9.00001049041748
Epoch 2810, val loss: 0.47498512268066406
Epoch 2820, training loss: 450.16552734375 = 0.15662460029125214 + 50.0 * 9.000178337097168
Epoch 2820, val loss: 0.47647568583488464
Epoch 2830, training loss: 450.17034912109375 = 0.15600468218326569 + 50.0 * 9.000287055969238
Epoch 2830, val loss: 0.4777391850948334
Epoch 2840, training loss: 450.11395263671875 = 0.15533871948719025 + 50.0 * 8.99917221069336
Epoch 2840, val loss: 0.4787457585334778
Epoch 2850, training loss: 450.06329345703125 = 0.15470080077648163 + 50.0 * 8.99817180633545
Epoch 2850, val loss: 0.48015424609184265
Epoch 2860, training loss: 450.0525207519531 = 0.15405067801475525 + 50.0 * 8.997969627380371
Epoch 2860, val loss: 0.4813183546066284
Epoch 2870, training loss: 450.0766906738281 = 0.1534241884946823 + 50.0 * 8.998465538024902
Epoch 2870, val loss: 0.482852965593338
Epoch 2880, training loss: 450.1900634765625 = 0.15283384919166565 + 50.0 * 9.000744819641113
Epoch 2880, val loss: 0.4842917323112488
Epoch 2890, training loss: 450.090576171875 = 0.15214131772518158 + 50.0 * 8.99876880645752
Epoch 2890, val loss: 0.48441872000694275
Epoch 2900, training loss: 450.069580078125 = 0.15149438381195068 + 50.0 * 8.998361587524414
Epoch 2900, val loss: 0.486207515001297
Epoch 2910, training loss: 450.1152648925781 = 0.15085439383983612 + 50.0 * 8.999288558959961
Epoch 2910, val loss: 0.4870339035987854
Epoch 2920, training loss: 450.05340576171875 = 0.15022343397140503 + 50.0 * 8.998063087463379
Epoch 2920, val loss: 0.48909592628479004
Epoch 2930, training loss: 450.1673278808594 = 0.14961141347885132 + 50.0 * 9.000354766845703
Epoch 2930, val loss: 0.4903720021247864
Epoch 2940, training loss: 450.0727233886719 = 0.14895184338092804 + 50.0 * 8.998475074768066
Epoch 2940, val loss: 0.49102041125297546
Epoch 2950, training loss: 450.0311584472656 = 0.14831823110580444 + 50.0 * 8.99765682220459
Epoch 2950, val loss: 0.49274536967277527
Epoch 2960, training loss: 450.0224914550781 = 0.14767767488956451 + 50.0 * 8.997496604919434
Epoch 2960, val loss: 0.49382731318473816
Epoch 2970, training loss: 450.0748596191406 = 0.14706215262413025 + 50.0 * 8.998556137084961
Epoch 2970, val loss: 0.4954414665699005
Epoch 2980, training loss: 450.0469970703125 = 0.14642439782619476 + 50.0 * 8.998011589050293
Epoch 2980, val loss: 0.4966224133968353
Epoch 2990, training loss: 450.0528564453125 = 0.145779550075531 + 50.0 * 8.998141288757324
Epoch 2990, val loss: 0.49770107865333557
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8478
Overall ASR: 0.7003
Flip ASR: 0.6255/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.26 GiB already allocated; 677.69 MiB free; 5.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 315.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 315.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 313.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 313.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 313.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.052490234375 = 1.1003609895706177 + 50.0 * 10.35904312133789
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 29.69 MiB free; 6.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.049560546875 = 1.0942602157592773 + 50.0 * 10.359106063842773
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 29.69 MiB free; 6.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.04345703125 = 1.0888841152191162 + 50.0 * 10.359091758728027
Epoch 0, val loss: 1.0886940956115723
Epoch 10, training loss: 518.8834228515625 = 1.0795894861221313 + 50.0 * 10.35607624053955
Epoch 10, val loss: 1.0790120363235474
Epoch 20, training loss: 516.5560913085938 = 1.0670067071914673 + 50.0 * 10.309782028198242
Epoch 20, val loss: 1.0660842657089233
Epoch 30, training loss: 498.7189636230469 = 1.055503249168396 + 50.0 * 9.953269004821777
Epoch 30, val loss: 1.0544979572296143
Epoch 40, training loss: 479.9272766113281 = 1.0420126914978027 + 50.0 * 9.577705383300781
Epoch 40, val loss: 1.0405373573303223
Epoch 50, training loss: 473.11444091796875 = 1.024707555770874 + 50.0 * 9.441794395446777
Epoch 50, val loss: 1.0232672691345215
Epoch 60, training loss: 470.0389099121094 = 1.007125973701477 + 50.0 * 9.380635261535645
Epoch 60, val loss: 1.006062626838684
Epoch 70, training loss: 468.85516357421875 = 0.989354133605957 + 50.0 * 9.357316017150879
Epoch 70, val loss: 0.9885014891624451
Epoch 80, training loss: 466.9435729980469 = 0.9723366498947144 + 50.0 * 9.319424629211426
Epoch 80, val loss: 0.9717815518379211
Epoch 90, training loss: 464.9613037109375 = 0.9582436084747314 + 50.0 * 9.280060768127441
Epoch 90, val loss: 0.9579329490661621
Epoch 100, training loss: 463.220703125 = 0.9431367516517639 + 50.0 * 9.245551109313965
Epoch 100, val loss: 0.9427850246429443
Epoch 110, training loss: 461.5791931152344 = 0.9236124753952026 + 50.0 * 9.213111877441406
Epoch 110, val loss: 0.9234926700592041
Epoch 120, training loss: 460.2071533203125 = 0.9027236700057983 + 50.0 * 9.186088562011719
Epoch 120, val loss: 0.9032264351844788
Epoch 130, training loss: 459.3517150878906 = 0.8819956183433533 + 50.0 * 9.169394493103027
Epoch 130, val loss: 0.8830487132072449
Epoch 140, training loss: 458.8114318847656 = 0.8586427569389343 + 50.0 * 9.159055709838867
Epoch 140, val loss: 0.8602120876312256
Epoch 150, training loss: 458.3247375488281 = 0.8320980072021484 + 50.0 * 9.149852752685547
Epoch 150, val loss: 0.8347339034080505
Epoch 160, training loss: 457.9537658691406 = 0.8040201663970947 + 50.0 * 9.14299488067627
Epoch 160, val loss: 0.8081192970275879
Epoch 170, training loss: 457.6916809082031 = 0.7757407426834106 + 50.0 * 9.13831901550293
Epoch 170, val loss: 0.7815431952476501
Epoch 180, training loss: 457.3387756347656 = 0.7481361627578735 + 50.0 * 9.131813049316406
Epoch 180, val loss: 0.7557423114776611
Epoch 190, training loss: 457.0406799316406 = 0.7216724157333374 + 50.0 * 9.12637996673584
Epoch 190, val loss: 0.7312109470367432
Epoch 200, training loss: 456.9991760253906 = 0.6962023377418518 + 50.0 * 9.126059532165527
Epoch 200, val loss: 0.7078178524971008
Epoch 210, training loss: 456.58013916015625 = 0.6716678142547607 + 50.0 * 9.118169784545898
Epoch 210, val loss: 0.6855083703994751
Epoch 220, training loss: 456.3445739746094 = 0.6493069529533386 + 50.0 * 9.113905906677246
Epoch 220, val loss: 0.6654892563819885
Epoch 230, training loss: 456.1105041503906 = 0.6291210055351257 + 50.0 * 9.109627723693848
Epoch 230, val loss: 0.6478227972984314
Epoch 240, training loss: 455.90625 = 0.6105291843414307 + 50.0 * 9.105914115905762
Epoch 240, val loss: 0.6315328478813171
Epoch 250, training loss: 455.77069091796875 = 0.5932446122169495 + 50.0 * 9.103549003601074
Epoch 250, val loss: 0.6166636347770691
Epoch 260, training loss: 455.5960998535156 = 0.577567458152771 + 50.0 * 9.100370407104492
Epoch 260, val loss: 0.6035283207893372
Epoch 270, training loss: 455.4464416503906 = 0.5631036758422852 + 50.0 * 9.09766674041748
Epoch 270, val loss: 0.5913096070289612
Epoch 280, training loss: 455.3619689941406 = 0.5497698187828064 + 50.0 * 9.096243858337402
Epoch 280, val loss: 0.5804871916770935
Epoch 290, training loss: 455.2031555175781 = 0.5379665493965149 + 50.0 * 9.093303680419922
Epoch 290, val loss: 0.5708631873130798
Epoch 300, training loss: 455.0791015625 = 0.5273473858833313 + 50.0 * 9.091034889221191
Epoch 300, val loss: 0.5623595714569092
Epoch 310, training loss: 454.9613342285156 = 0.5176411271095276 + 50.0 * 9.088873863220215
Epoch 310, val loss: 0.5547147989273071
Epoch 320, training loss: 454.8778381347656 = 0.5084848999977112 + 50.0 * 9.087387084960938
Epoch 320, val loss: 0.5473247170448303
Epoch 330, training loss: 454.7633056640625 = 0.49971896409988403 + 50.0 * 9.085271835327148
Epoch 330, val loss: 0.5406075716018677
Epoch 340, training loss: 454.67041015625 = 0.4921365976333618 + 50.0 * 9.083565711975098
Epoch 340, val loss: 0.534609854221344
Epoch 350, training loss: 454.5714416503906 = 0.48519623279571533 + 50.0 * 9.081725120544434
Epoch 350, val loss: 0.5293206572532654
Epoch 360, training loss: 454.47802734375 = 0.47863051295280457 + 50.0 * 9.079987525939941
Epoch 360, val loss: 0.5241008996963501
Epoch 370, training loss: 454.39752197265625 = 0.4723552167415619 + 50.0 * 9.078503608703613
Epoch 370, val loss: 0.519277036190033
Epoch 380, training loss: 454.3401184082031 = 0.46632447838783264 + 50.0 * 9.077475547790527
Epoch 380, val loss: 0.5146581530570984
Epoch 390, training loss: 454.37939453125 = 0.46031713485717773 + 50.0 * 9.078381538391113
Epoch 390, val loss: 0.5098589658737183
Epoch 400, training loss: 454.2531433105469 = 0.45454835891723633 + 50.0 * 9.075971603393555
Epoch 400, val loss: 0.5054945349693298
Epoch 410, training loss: 454.1475524902344 = 0.4492392838001251 + 50.0 * 9.073966026306152
Epoch 410, val loss: 0.5014726519584656
Epoch 420, training loss: 454.08941650390625 = 0.44416147470474243 + 50.0 * 9.072905540466309
Epoch 420, val loss: 0.49761733412742615
Epoch 430, training loss: 454.06390380859375 = 0.43925589323043823 + 50.0 * 9.072492599487305
Epoch 430, val loss: 0.4939405024051666
Epoch 440, training loss: 454.1190185546875 = 0.43428659439086914 + 50.0 * 9.073694229125977
Epoch 440, val loss: 0.4899066686630249
Epoch 450, training loss: 453.93505859375 = 0.4295840561389923 + 50.0 * 9.070109367370605
Epoch 450, val loss: 0.4864672124385834
Epoch 460, training loss: 453.8862609863281 = 0.425192654132843 + 50.0 * 9.069221496582031
Epoch 460, val loss: 0.4831685423851013
Epoch 470, training loss: 453.82818603515625 = 0.42095109820365906 + 50.0 * 9.068144798278809
Epoch 470, val loss: 0.47996678948402405
Epoch 480, training loss: 453.896240234375 = 0.4168008863925934 + 50.0 * 9.069588661193848
Epoch 480, val loss: 0.47696202993392944
Epoch 490, training loss: 453.7713317871094 = 0.4125472903251648 + 50.0 * 9.06717586517334
Epoch 490, val loss: 0.47370585799217224
Epoch 500, training loss: 453.6943359375 = 0.40854915976524353 + 50.0 * 9.065715789794922
Epoch 500, val loss: 0.4707770347595215
Epoch 510, training loss: 453.63671875 = 0.4047293961048126 + 50.0 * 9.064640045166016
Epoch 510, val loss: 0.46802791953086853
Epoch 520, training loss: 453.7843017578125 = 0.4010080397129059 + 50.0 * 9.067666053771973
Epoch 520, val loss: 0.46545353531837463
Epoch 530, training loss: 453.5470886230469 = 0.39708220958709717 + 50.0 * 9.062999725341797
Epoch 530, val loss: 0.4626464545726776
Epoch 540, training loss: 453.52203369140625 = 0.3934595286846161 + 50.0 * 9.06257152557373
Epoch 540, val loss: 0.4600662589073181
Epoch 550, training loss: 453.4684753417969 = 0.39001187682151794 + 50.0 * 9.061569213867188
Epoch 550, val loss: 0.4575922191143036
Epoch 560, training loss: 453.4200439453125 = 0.3866436779499054 + 50.0 * 9.060667991638184
Epoch 560, val loss: 0.4552555978298187
Epoch 570, training loss: 453.39312744140625 = 0.38332879543304443 + 50.0 * 9.060195922851562
Epoch 570, val loss: 0.4529584050178528
Epoch 580, training loss: 453.38690185546875 = 0.3798834979534149 + 50.0 * 9.060140609741211
Epoch 580, val loss: 0.4510706961154938
Epoch 590, training loss: 453.3404541015625 = 0.3764931559562683 + 50.0 * 9.059279441833496
Epoch 590, val loss: 0.4486956000328064
Epoch 600, training loss: 453.30035400390625 = 0.37335243821144104 + 50.0 * 9.058540344238281
Epoch 600, val loss: 0.44669637084007263
Epoch 610, training loss: 453.25994873046875 = 0.3702850043773651 + 50.0 * 9.057793617248535
Epoch 610, val loss: 0.44476696848869324
Epoch 620, training loss: 453.26983642578125 = 0.3671953082084656 + 50.0 * 9.058053016662598
Epoch 620, val loss: 0.4428434669971466
Epoch 630, training loss: 453.19580078125 = 0.3641376495361328 + 50.0 * 9.056632995605469
Epoch 630, val loss: 0.4410853981971741
Epoch 640, training loss: 453.1558532714844 = 0.3612048327922821 + 50.0 * 9.055892944335938
Epoch 640, val loss: 0.439340740442276
Epoch 650, training loss: 453.1424560546875 = 0.35831978917121887 + 50.0 * 9.055683135986328
Epoch 650, val loss: 0.4377320110797882
Epoch 660, training loss: 453.1168518066406 = 0.35537758469581604 + 50.0 * 9.055229187011719
Epoch 660, val loss: 0.43612733483314514
Epoch 670, training loss: 453.06951904296875 = 0.3525138795375824 + 50.0 * 9.054340362548828
Epoch 670, val loss: 0.4345739781856537
Epoch 680, training loss: 453.0442810058594 = 0.3497686982154846 + 50.0 * 9.053890228271484
Epoch 680, val loss: 0.4331188499927521
Epoch 690, training loss: 453.0608825683594 = 0.3470541536808014 + 50.0 * 9.054276466369629
Epoch 690, val loss: 0.43174654245376587
Epoch 700, training loss: 452.98321533203125 = 0.34428274631500244 + 50.0 * 9.052779197692871
Epoch 700, val loss: 0.43047717213630676
Epoch 710, training loss: 452.96746826171875 = 0.3416137993335724 + 50.0 * 9.05251693725586
Epoch 710, val loss: 0.4292003810405731
Epoch 720, training loss: 452.9300231933594 = 0.3390113413333893 + 50.0 * 9.051819801330566
Epoch 720, val loss: 0.4280194938182831
Epoch 730, training loss: 453.0400085449219 = 0.33643263578414917 + 50.0 * 9.054071426391602
Epoch 730, val loss: 0.42698317766189575
Epoch 740, training loss: 453.0016784667969 = 0.3337815999984741 + 50.0 * 9.05335807800293
Epoch 740, val loss: 0.4259941875934601
Epoch 750, training loss: 452.8907165527344 = 0.3311935067176819 + 50.0 * 9.051190376281738
Epoch 750, val loss: 0.42461517453193665
Epoch 760, training loss: 452.8306884765625 = 0.32872509956359863 + 50.0 * 9.050039291381836
Epoch 760, val loss: 0.4238733649253845
Epoch 770, training loss: 452.7967834472656 = 0.3262976109981537 + 50.0 * 9.049409866333008
Epoch 770, val loss: 0.4228394031524658
Epoch 780, training loss: 452.7747497558594 = 0.3238995373249054 + 50.0 * 9.049016952514648
Epoch 780, val loss: 0.42190438508987427
Epoch 790, training loss: 452.8191833496094 = 0.32151320576667786 + 50.0 * 9.04995346069336
Epoch 790, val loss: 0.420804888010025
Epoch 800, training loss: 452.7891845703125 = 0.3190416395664215 + 50.0 * 9.049403190612793
Epoch 800, val loss: 0.42063114047050476
Epoch 810, training loss: 452.7782287597656 = 0.3166234791278839 + 50.0 * 9.049232482910156
Epoch 810, val loss: 0.4194645285606384
Epoch 820, training loss: 452.8001708984375 = 0.3142555058002472 + 50.0 * 9.049717903137207
Epoch 820, val loss: 0.41880422830581665
Epoch 830, training loss: 452.6991882324219 = 0.3119356632232666 + 50.0 * 9.047744750976562
Epoch 830, val loss: 0.41782525181770325
Epoch 840, training loss: 452.6535949707031 = 0.3096711337566376 + 50.0 * 9.046878814697266
Epoch 840, val loss: 0.41740429401397705
Epoch 850, training loss: 452.62701416015625 = 0.30744120478630066 + 50.0 * 9.046391487121582
Epoch 850, val loss: 0.4167617857456207
Epoch 860, training loss: 452.745849609375 = 0.3052120506763458 + 50.0 * 9.048812866210938
Epoch 860, val loss: 0.41613486409187317
Epoch 870, training loss: 452.71600341796875 = 0.3029065430164337 + 50.0 * 9.048261642456055
Epoch 870, val loss: 0.4155444800853729
Epoch 880, training loss: 452.57647705078125 = 0.3006895184516907 + 50.0 * 9.045516014099121
Epoch 880, val loss: 0.4149285852909088
Epoch 890, training loss: 452.5484924316406 = 0.29853665828704834 + 50.0 * 9.044999122619629
Epoch 890, val loss: 0.4144198000431061
Epoch 900, training loss: 452.5324401855469 = 0.29641059041023254 + 50.0 * 9.044720649719238
Epoch 900, val loss: 0.41392189264297485
Epoch 910, training loss: 452.5470275878906 = 0.2943148612976074 + 50.0 * 9.04505443572998
Epoch 910, val loss: 0.41342273354530334
Epoch 920, training loss: 452.504150390625 = 0.2921378016471863 + 50.0 * 9.04423999786377
Epoch 920, val loss: 0.4131169021129608
Epoch 930, training loss: 452.5129089355469 = 0.2900238335132599 + 50.0 * 9.04445743560791
Epoch 930, val loss: 0.412515789270401
Epoch 940, training loss: 452.4755554199219 = 0.2879754602909088 + 50.0 * 9.04375171661377
Epoch 940, val loss: 0.4121556282043457
Epoch 950, training loss: 452.451416015625 = 0.285957008600235 + 50.0 * 9.043309211730957
Epoch 950, val loss: 0.41182225942611694
Epoch 960, training loss: 452.5800476074219 = 0.2839350700378418 + 50.0 * 9.04592227935791
Epoch 960, val loss: 0.41164660453796387
Epoch 970, training loss: 452.4281005859375 = 0.28187423944473267 + 50.0 * 9.042924880981445
Epoch 970, val loss: 0.41112443804740906
Epoch 980, training loss: 452.4101867675781 = 0.27988627552986145 + 50.0 * 9.042606353759766
Epoch 980, val loss: 0.41071757674217224
Epoch 990, training loss: 452.3753356933594 = 0.2779264450073242 + 50.0 * 9.041948318481445
Epoch 990, val loss: 0.4104161262512207
Epoch 1000, training loss: 452.3670349121094 = 0.27598628401756287 + 50.0 * 9.041820526123047
Epoch 1000, val loss: 0.4101680815219879
Epoch 1010, training loss: 452.59100341796875 = 0.27402785420417786 + 50.0 * 9.046339988708496
Epoch 1010, val loss: 0.40987104177474976
Epoch 1020, training loss: 452.35308837890625 = 0.27202796936035156 + 50.0 * 9.041621208190918
Epoch 1020, val loss: 0.40974661707878113
Epoch 1030, training loss: 452.3274230957031 = 0.2701076865196228 + 50.0 * 9.041146278381348
Epoch 1030, val loss: 0.40938296914100647
Epoch 1040, training loss: 452.30609130859375 = 0.2682206630706787 + 50.0 * 9.040757179260254
Epoch 1040, val loss: 0.40914198756217957
Epoch 1050, training loss: 452.2954406738281 = 0.2663465738296509 + 50.0 * 9.040581703186035
Epoch 1050, val loss: 0.408875048160553
Epoch 1060, training loss: 452.41900634765625 = 0.26445016264915466 + 50.0 * 9.0430908203125
Epoch 1060, val loss: 0.4084974527359009
Epoch 1070, training loss: 452.2654113769531 = 0.2625300884246826 + 50.0 * 9.040057182312012
Epoch 1070, val loss: 0.4084574282169342
Epoch 1080, training loss: 452.2610168457031 = 0.26067689061164856 + 50.0 * 9.040006637573242
Epoch 1080, val loss: 0.408466637134552
Epoch 1090, training loss: 452.2313232421875 = 0.258835107088089 + 50.0 * 9.039449691772461
Epoch 1090, val loss: 0.4082808196544647
Epoch 1100, training loss: 452.2666931152344 = 0.257013738155365 + 50.0 * 9.040193557739258
Epoch 1100, val loss: 0.40836581587791443
Epoch 1110, training loss: 452.2225036621094 = 0.2551625967025757 + 50.0 * 9.039346694946289
Epoch 1110, val loss: 0.4082387685775757
Epoch 1120, training loss: 452.2541809082031 = 0.2533281147480011 + 50.0 * 9.040017127990723
Epoch 1120, val loss: 0.40825995802879333
Epoch 1130, training loss: 452.1854553222656 = 0.2514854073524475 + 50.0 * 9.038679122924805
Epoch 1130, val loss: 0.4075154662132263
Epoch 1140, training loss: 452.177734375 = 0.24969054758548737 + 50.0 * 9.03856086730957
Epoch 1140, val loss: 0.40781649947166443
Epoch 1150, training loss: 452.2697448730469 = 0.24789546430110931 + 50.0 * 9.040436744689941
Epoch 1150, val loss: 0.4076303541660309
Epoch 1160, training loss: 452.1688537597656 = 0.24607673287391663 + 50.0 * 9.038455963134766
Epoch 1160, val loss: 0.407656729221344
Epoch 1170, training loss: 452.13018798828125 = 0.24430586397647858 + 50.0 * 9.037717819213867
Epoch 1170, val loss: 0.4076631963253021
Epoch 1180, training loss: 452.1106262207031 = 0.24252915382385254 + 50.0 * 9.037362098693848
Epoch 1180, val loss: 0.40758374333381653
Epoch 1190, training loss: 452.181640625 = 0.24077028036117554 + 50.0 * 9.038817405700684
Epoch 1190, val loss: 0.40725451707839966
Epoch 1200, training loss: 452.1276550292969 = 0.23899178206920624 + 50.0 * 9.037773132324219
Epoch 1200, val loss: 0.40813589096069336
Epoch 1210, training loss: 452.131591796875 = 0.23721738159656525 + 50.0 * 9.037887573242188
Epoch 1210, val loss: 0.40791723132133484
Epoch 1220, training loss: 452.0692138671875 = 0.23546524345874786 + 50.0 * 9.036674499511719
Epoch 1220, val loss: 0.40801864862442017
Epoch 1230, training loss: 452.0578308105469 = 0.233737975358963 + 50.0 * 9.036481857299805
Epoch 1230, val loss: 0.4082832336425781
Epoch 1240, training loss: 452.138427734375 = 0.23201686143875122 + 50.0 * 9.038127899169922
Epoch 1240, val loss: 0.4085477888584137
Epoch 1250, training loss: 452.0432434082031 = 0.23026633262634277 + 50.0 * 9.036259651184082
Epoch 1250, val loss: 0.40823596715927124
Epoch 1260, training loss: 452.0417785644531 = 0.22855201363563538 + 50.0 * 9.036264419555664
Epoch 1260, val loss: 0.4085206091403961
Epoch 1270, training loss: 452.0985107421875 = 0.22684969007968903 + 50.0 * 9.037433624267578
Epoch 1270, val loss: 0.40802693367004395
Epoch 1280, training loss: 452.0156555175781 = 0.22513307631015778 + 50.0 * 9.035810470581055
Epoch 1280, val loss: 0.4090225100517273
Epoch 1290, training loss: 451.9854736328125 = 0.223440483212471 + 50.0 * 9.035240173339844
Epoch 1290, val loss: 0.40919598937034607
Epoch 1300, training loss: 451.9609069824219 = 0.221759632229805 + 50.0 * 9.034782409667969
Epoch 1300, val loss: 0.40934863686561584
Epoch 1310, training loss: 452.05548095703125 = 0.22009392082691193 + 50.0 * 9.036707878112793
Epoch 1310, val loss: 0.40969809889793396
Epoch 1320, training loss: 451.9847412109375 = 0.21839748322963715 + 50.0 * 9.035326957702637
Epoch 1320, val loss: 0.4100494384765625
Epoch 1330, training loss: 451.96881103515625 = 0.21673759818077087 + 50.0 * 9.035041809082031
Epoch 1330, val loss: 0.4103952646255493
Epoch 1340, training loss: 451.9462890625 = 0.21507541835308075 + 50.0 * 9.034624099731445
Epoch 1340, val loss: 0.410724937915802
Epoch 1350, training loss: 451.9122009277344 = 0.2134130746126175 + 50.0 * 9.033975601196289
Epoch 1350, val loss: 0.4106830656528473
Epoch 1360, training loss: 451.95196533203125 = 0.21177096664905548 + 50.0 * 9.034804344177246
Epoch 1360, val loss: 0.4106534421443939
Epoch 1370, training loss: 451.92156982421875 = 0.21011139452457428 + 50.0 * 9.034229278564453
Epoch 1370, val loss: 0.4111553430557251
Epoch 1380, training loss: 451.87603759765625 = 0.20844906568527222 + 50.0 * 9.03335189819336
Epoch 1380, val loss: 0.41167476773262024
Epoch 1390, training loss: 451.89654541015625 = 0.20681072771549225 + 50.0 * 9.033794403076172
Epoch 1390, val loss: 0.41223785281181335
Epoch 1400, training loss: 451.9160461425781 = 0.20518788695335388 + 50.0 * 9.03421688079834
Epoch 1400, val loss: 0.4122656285762787
Epoch 1410, training loss: 451.89154052734375 = 0.20356705784797668 + 50.0 * 9.033760070800781
Epoch 1410, val loss: 0.4136085510253906
Epoch 1420, training loss: 451.8419189453125 = 0.2019430249929428 + 50.0 * 9.03279972076416
Epoch 1420, val loss: 0.41367295384407043
Epoch 1430, training loss: 451.84039306640625 = 0.20034517347812653 + 50.0 * 9.032800674438477
Epoch 1430, val loss: 0.4143642783164978
Epoch 1440, training loss: 451.99896240234375 = 0.19879800081253052 + 50.0 * 9.036003112792969
Epoch 1440, val loss: 0.4154565930366516
Epoch 1450, training loss: 451.8037414550781 = 0.19714194536209106 + 50.0 * 9.032132148742676
Epoch 1450, val loss: 0.41511213779449463
Epoch 1460, training loss: 451.7987976074219 = 0.1955731362104416 + 50.0 * 9.032064437866211
Epoch 1460, val loss: 0.4155762493610382
Epoch 1470, training loss: 451.77947998046875 = 0.19400149583816528 + 50.0 * 9.031709671020508
Epoch 1470, val loss: 0.4163312017917633
Epoch 1480, training loss: 451.7990417480469 = 0.19243746995925903 + 50.0 * 9.032132148742676
Epoch 1480, val loss: 0.4163801074028015
Epoch 1490, training loss: 451.7922058105469 = 0.19086302816867828 + 50.0 * 9.032027244567871
Epoch 1490, val loss: 0.41732439398765564
Epoch 1500, training loss: 451.7839050292969 = 0.1893012672662735 + 50.0 * 9.031891822814941
Epoch 1500, val loss: 0.4177019000053406
Epoch 1510, training loss: 451.7471008300781 = 0.1877467930316925 + 50.0 * 9.031187057495117
Epoch 1510, val loss: 0.4187072813510895
Epoch 1520, training loss: 451.7728576660156 = 0.1861962229013443 + 50.0 * 9.031733512878418
Epoch 1520, val loss: 0.4190773367881775
Epoch 1530, training loss: 451.76458740234375 = 0.1846466064453125 + 50.0 * 9.031599044799805
Epoch 1530, val loss: 0.41957488656044006
Epoch 1540, training loss: 451.7453308105469 = 0.18309786915779114 + 50.0 * 9.031244277954102
Epoch 1540, val loss: 0.420537531375885
Epoch 1550, training loss: 451.7477722167969 = 0.18154963850975037 + 50.0 * 9.03132438659668
Epoch 1550, val loss: 0.4209707975387573
Epoch 1560, training loss: 451.7314758300781 = 0.18000522255897522 + 50.0 * 9.03102970123291
Epoch 1560, val loss: 0.4216826260089874
Epoch 1570, training loss: 451.7242736816406 = 0.1784663200378418 + 50.0 * 9.030916213989258
Epoch 1570, val loss: 0.4220505952835083
Epoch 1580, training loss: 451.72747802734375 = 0.17692656815052032 + 50.0 * 9.031010627746582
Epoch 1580, val loss: 0.42275500297546387
Epoch 1590, training loss: 451.6756896972656 = 0.17538946866989136 + 50.0 * 9.030006408691406
Epoch 1590, val loss: 0.42420509457588196
Epoch 1600, training loss: 451.726806640625 = 0.17388297617435455 + 50.0 * 9.031058311462402
Epoch 1600, val loss: 0.42523622512817383
Epoch 1610, training loss: 451.6812438964844 = 0.17233872413635254 + 50.0 * 9.03017807006836
Epoch 1610, val loss: 0.42522260546684265
Epoch 1620, training loss: 451.67510986328125 = 0.1708492487668991 + 50.0 * 9.030085563659668
Epoch 1620, val loss: 0.4271470904350281
Epoch 1630, training loss: 451.6327819824219 = 0.1693062037229538 + 50.0 * 9.029269218444824
Epoch 1630, val loss: 0.4270687401294708
Epoch 1640, training loss: 451.655029296875 = 0.16780246794223785 + 50.0 * 9.029744148254395
Epoch 1640, val loss: 0.4280634820461273
Epoch 1650, training loss: 451.641357421875 = 0.1663188934326172 + 50.0 * 9.029500961303711
Epoch 1650, val loss: 0.42938533425331116
Epoch 1660, training loss: 451.62054443359375 = 0.16479764878749847 + 50.0 * 9.029114723205566
Epoch 1660, val loss: 0.429336279630661
Epoch 1670, training loss: 451.7378845214844 = 0.1633194088935852 + 50.0 * 9.03149127960205
Epoch 1670, val loss: 0.4305415153503418
Epoch 1680, training loss: 451.63946533203125 = 0.16181878745555878 + 50.0 * 9.029553413391113
Epoch 1680, val loss: 0.43116942048072815
Epoch 1690, training loss: 451.604248046875 = 0.1603330820798874 + 50.0 * 9.028878211975098
Epoch 1690, val loss: 0.432253360748291
Epoch 1700, training loss: 451.6053466796875 = 0.15887194871902466 + 50.0 * 9.028929710388184
Epoch 1700, val loss: 0.4336228668689728
Epoch 1710, training loss: 451.6333312988281 = 0.15739227831363678 + 50.0 * 9.029519081115723
Epoch 1710, val loss: 0.4343918263912201
Epoch 1720, training loss: 451.5646667480469 = 0.15590037405490875 + 50.0 * 9.028175354003906
Epoch 1720, val loss: 0.4347734749317169
Epoch 1730, training loss: 451.5365295410156 = 0.1544317752122879 + 50.0 * 9.027642250061035
Epoch 1730, val loss: 0.4362468719482422
Epoch 1740, training loss: 451.5293273925781 = 0.15297429263591766 + 50.0 * 9.02752685546875
Epoch 1740, val loss: 0.4373130202293396
Epoch 1750, training loss: 451.58984375 = 0.15154609084129333 + 50.0 * 9.028765678405762
Epoch 1750, val loss: 0.4390157461166382
Epoch 1760, training loss: 451.5640869140625 = 0.15007852017879486 + 50.0 * 9.028280258178711
Epoch 1760, val loss: 0.4381331205368042
Epoch 1770, training loss: 451.55474853515625 = 0.1486528217792511 + 50.0 * 9.028121948242188
Epoch 1770, val loss: 0.4405903220176697
Epoch 1780, training loss: 451.534912109375 = 0.1472082883119583 + 50.0 * 9.027753829956055
Epoch 1780, val loss: 0.4413842558860779
Epoch 1790, training loss: 451.52740478515625 = 0.14579083025455475 + 50.0 * 9.027632713317871
Epoch 1790, val loss: 0.44191840291023254
Epoch 1800, training loss: 451.4940185546875 = 0.14437533915042877 + 50.0 * 9.026992797851562
Epoch 1800, val loss: 0.443174809217453
Epoch 1810, training loss: 451.5354309082031 = 0.1429736167192459 + 50.0 * 9.027849197387695
Epoch 1810, val loss: 0.4442627429962158
Epoch 1820, training loss: 451.47528076171875 = 0.14156194031238556 + 50.0 * 9.026674270629883
Epoch 1820, val loss: 0.44576990604400635
Epoch 1830, training loss: 451.4665222167969 = 0.1401769369840622 + 50.0 * 9.026527404785156
Epoch 1830, val loss: 0.44722527265548706
Epoch 1840, training loss: 451.4657897949219 = 0.13880082964897156 + 50.0 * 9.02653980255127
Epoch 1840, val loss: 0.44889035820961
Epoch 1850, training loss: 451.4692687988281 = 0.1374119222164154 + 50.0 * 9.026637077331543
Epoch 1850, val loss: 0.4499264657497406
Epoch 1860, training loss: 451.49798583984375 = 0.13604424893856049 + 50.0 * 9.027238845825195
Epoch 1860, val loss: 0.4513024687767029
Epoch 1870, training loss: 451.4851379394531 = 0.1346682459115982 + 50.0 * 9.027009010314941
Epoch 1870, val loss: 0.4518948793411255
Epoch 1880, training loss: 451.4345397949219 = 0.13330715894699097 + 50.0 * 9.02602481842041
Epoch 1880, val loss: 0.45355695486068726
Epoch 1890, training loss: 451.4137878417969 = 0.13195425271987915 + 50.0 * 9.025636672973633
Epoch 1890, val loss: 0.45496806502342224
Epoch 1900, training loss: 451.4355773925781 = 0.13061143457889557 + 50.0 * 9.02609920501709
Epoch 1900, val loss: 0.4564521610736847
Epoch 1910, training loss: 451.48809814453125 = 0.12928898632526398 + 50.0 * 9.027175903320312
Epoch 1910, val loss: 0.4575966000556946
Epoch 1920, training loss: 451.4109191894531 = 0.12794172763824463 + 50.0 * 9.025659561157227
Epoch 1920, val loss: 0.45828354358673096
Epoch 1930, training loss: 451.398193359375 = 0.12660524249076843 + 50.0 * 9.025431632995605
Epoch 1930, val loss: 0.4599032998085022
Epoch 1940, training loss: 451.3799133300781 = 0.12528474628925323 + 50.0 * 9.025093078613281
Epoch 1940, val loss: 0.4613134264945984
Epoch 1950, training loss: 451.3698425292969 = 0.12397307902574539 + 50.0 * 9.024917602539062
Epoch 1950, val loss: 0.4627173840999603
Epoch 1960, training loss: 451.4744873046875 = 0.12274570018053055 + 50.0 * 9.027034759521484
Epoch 1960, val loss: 0.4655457139015198
Epoch 1970, training loss: 451.4102478027344 = 0.12140709906816483 + 50.0 * 9.025776863098145
Epoch 1970, val loss: 0.46585220098495483
Epoch 1980, training loss: 451.3627624511719 = 0.1201176643371582 + 50.0 * 9.024852752685547
Epoch 1980, val loss: 0.46691465377807617
Epoch 1990, training loss: 451.3361511230469 = 0.11883972585201263 + 50.0 * 9.024346351623535
Epoch 1990, val loss: 0.4683546721935272
Epoch 2000, training loss: 451.3667907714844 = 0.11759214848279953 + 50.0 * 9.024984359741211
Epoch 2000, val loss: 0.4705130159854889
Epoch 2010, training loss: 451.3572082519531 = 0.11631827056407928 + 50.0 * 9.02481746673584
Epoch 2010, val loss: 0.4712224304676056
Epoch 2020, training loss: 451.3102722167969 = 0.115069679915905 + 50.0 * 9.023903846740723
Epoch 2020, val loss: 0.47293779253959656
Epoch 2030, training loss: 451.32806396484375 = 0.11382214725017548 + 50.0 * 9.024284362792969
Epoch 2030, val loss: 0.4742511808872223
Epoch 2040, training loss: 451.3778076171875 = 0.11259295046329498 + 50.0 * 9.025304794311523
Epoch 2040, val loss: 0.47542279958724976
Epoch 2050, training loss: 451.3456115722656 = 0.11136890202760696 + 50.0 * 9.02468490600586
Epoch 2050, val loss: 0.47688400745391846
Epoch 2060, training loss: 451.3385925292969 = 0.11016470938920975 + 50.0 * 9.024568557739258
Epoch 2060, val loss: 0.47944867610931396
Epoch 2070, training loss: 451.2908935546875 = 0.10893150418996811 + 50.0 * 9.023639678955078
Epoch 2070, val loss: 0.48032671213150024
Epoch 2080, training loss: 451.2747802734375 = 0.10772702097892761 + 50.0 * 9.023341178894043
Epoch 2080, val loss: 0.4821142554283142
Epoch 2090, training loss: 451.32928466796875 = 0.1065419390797615 + 50.0 * 9.024455070495605
Epoch 2090, val loss: 0.4835510551929474
Epoch 2100, training loss: 451.2570495605469 = 0.10533735156059265 + 50.0 * 9.02303409576416
Epoch 2100, val loss: 0.4844834804534912
Epoch 2110, training loss: 451.3236389160156 = 0.1041928380727768 + 50.0 * 9.024389266967773
Epoch 2110, val loss: 0.48532211780548096
Epoch 2120, training loss: 451.2506408691406 = 0.10299920290708542 + 50.0 * 9.022953033447266
Epoch 2120, val loss: 0.48766157031059265
Epoch 2130, training loss: 451.2638854980469 = 0.10185154527425766 + 50.0 * 9.02324104309082
Epoch 2130, val loss: 0.4886223375797272
Epoch 2140, training loss: 451.26104736328125 = 0.10069779306650162 + 50.0 * 9.02320671081543
Epoch 2140, val loss: 0.49037373065948486
Epoch 2150, training loss: 451.22418212890625 = 0.099554143846035 + 50.0 * 9.022492408752441
Epoch 2150, val loss: 0.4933815896511078
Epoch 2160, training loss: 451.23577880859375 = 0.09842504560947418 + 50.0 * 9.022747039794922
Epoch 2160, val loss: 0.49498409032821655
Epoch 2170, training loss: 451.28961181640625 = 0.09729733318090439 + 50.0 * 9.023846626281738
Epoch 2170, val loss: 0.49587249755859375
Epoch 2180, training loss: 451.24249267578125 = 0.09620939195156097 + 50.0 * 9.02292537689209
Epoch 2180, val loss: 0.4969998002052307
Epoch 2190, training loss: 451.1964416503906 = 0.09508738666772842 + 50.0 * 9.022027015686035
Epoch 2190, val loss: 0.49957075715065
Epoch 2200, training loss: 451.1849670410156 = 0.09397853910923004 + 50.0 * 9.021820068359375
Epoch 2200, val loss: 0.5009239315986633
Epoch 2210, training loss: 451.24560546875 = 0.09290691465139389 + 50.0 * 9.023054122924805
Epoch 2210, val loss: 0.5027879476547241
Epoch 2220, training loss: 451.1723937988281 = 0.09181635081768036 + 50.0 * 9.021611213684082
Epoch 2220, val loss: 0.5043219923973083
Epoch 2230, training loss: 451.1567687988281 = 0.09075041860342026 + 50.0 * 9.021320343017578
Epoch 2230, val loss: 0.5061029195785522
Epoch 2240, training loss: 451.1530456542969 = 0.08968482911586761 + 50.0 * 9.02126693725586
Epoch 2240, val loss: 0.507908821105957
Epoch 2250, training loss: 451.3044128417969 = 0.08871336281299591 + 50.0 * 9.024313926696777
Epoch 2250, val loss: 0.5106757283210754
Epoch 2260, training loss: 451.1964416503906 = 0.08760908246040344 + 50.0 * 9.022176742553711
Epoch 2260, val loss: 0.5106521844863892
Epoch 2270, training loss: 451.1662902832031 = 0.0865844264626503 + 50.0 * 9.021594047546387
Epoch 2270, val loss: 0.5134156346321106
Epoch 2280, training loss: 451.1783752441406 = 0.0855671614408493 + 50.0 * 9.021856307983398
Epoch 2280, val loss: 0.5156617164611816
Epoch 2290, training loss: 451.152587890625 = 0.0845569297671318 + 50.0 * 9.021360397338867
Epoch 2290, val loss: 0.5170252323150635
Epoch 2300, training loss: 451.156494140625 = 0.08353887498378754 + 50.0 * 9.021459579467773
Epoch 2300, val loss: 0.5184952616691589
Epoch 2310, training loss: 451.13397216796875 = 0.08256472647190094 + 50.0 * 9.021028518676758
Epoch 2310, val loss: 0.521245002746582
Epoch 2320, training loss: 451.1307678222656 = 0.08156300336122513 + 50.0 * 9.020983695983887
Epoch 2320, val loss: 0.5224966406822205
Epoch 2330, training loss: 451.1548156738281 = 0.08059060573577881 + 50.0 * 9.021484375
Epoch 2330, val loss: 0.5242056250572205
Epoch 2340, training loss: 451.16943359375 = 0.07963398098945618 + 50.0 * 9.021796226501465
Epoch 2340, val loss: 0.5258775353431702
Epoch 2350, training loss: 451.1520080566406 = 0.07872194051742554 + 50.0 * 9.021465301513672
Epoch 2350, val loss: 0.5287041068077087
Epoch 2360, training loss: 451.10980224609375 = 0.07772533595561981 + 50.0 * 9.020641326904297
Epoch 2360, val loss: 0.5287294983863831
Epoch 2370, training loss: 451.0883483886719 = 0.07678277045488358 + 50.0 * 9.020231246948242
Epoch 2370, val loss: 0.5309826731681824
Epoch 2380, training loss: 451.09869384765625 = 0.07586454600095749 + 50.0 * 9.020456314086914
Epoch 2380, val loss: 0.5335027575492859
Epoch 2390, training loss: 451.1346740722656 = 0.07495155930519104 + 50.0 * 9.021194458007812
Epoch 2390, val loss: 0.5352135896682739
Epoch 2400, training loss: 451.10430908203125 = 0.07405342906713486 + 50.0 * 9.020605087280273
Epoch 2400, val loss: 0.5354615449905396
Epoch 2410, training loss: 451.06640625 = 0.07311873883008957 + 50.0 * 9.019865989685059
Epoch 2410, val loss: 0.5382986068725586
Epoch 2420, training loss: 451.08099365234375 = 0.0722358450293541 + 50.0 * 9.020174980163574
Epoch 2420, val loss: 0.5393967032432556
Epoch 2430, training loss: 451.0874938964844 = 0.07136260718107224 + 50.0 * 9.020322799682617
Epoch 2430, val loss: 0.5411674380302429
Epoch 2440, training loss: 451.0953063964844 = 0.0705057680606842 + 50.0 * 9.020496368408203
Epoch 2440, val loss: 0.5448715090751648
Epoch 2450, training loss: 451.0553283691406 = 0.06962426006793976 + 50.0 * 9.01971435546875
Epoch 2450, val loss: 0.5459240674972534
Epoch 2460, training loss: 451.0411071777344 = 0.06875710189342499 + 50.0 * 9.019447326660156
Epoch 2460, val loss: 0.5480406284332275
Epoch 2470, training loss: 451.1030578613281 = 0.06793615967035294 + 50.0 * 9.020702362060547
Epoch 2470, val loss: 0.5498807430267334
Epoch 2480, training loss: 451.1089172363281 = 0.06709681451320648 + 50.0 * 9.020835876464844
Epoch 2480, val loss: 0.5510815978050232
Epoch 2490, training loss: 451.0285949707031 = 0.06625457108020782 + 50.0 * 9.019247055053711
Epoch 2490, val loss: 0.5533992052078247
Epoch 2500, training loss: 451.0001220703125 = 0.06543188542127609 + 50.0 * 9.018693923950195
Epoch 2500, val loss: 0.5548102259635925
Epoch 2510, training loss: 450.9879150390625 = 0.06461801379919052 + 50.0 * 9.018465995788574
Epoch 2510, val loss: 0.5567609071731567
Epoch 2520, training loss: 451.00714111328125 = 0.06382343918085098 + 50.0 * 9.018866539001465
Epoch 2520, val loss: 0.5581963062286377
Epoch 2530, training loss: 451.119873046875 = 0.0631093680858612 + 50.0 * 9.021135330200195
Epoch 2530, val loss: 0.5589483976364136
Epoch 2540, training loss: 451.00347900390625 = 0.06226043775677681 + 50.0 * 9.018824577331543
Epoch 2540, val loss: 0.5629932880401611
Epoch 2550, training loss: 450.9912414550781 = 0.061495061963796616 + 50.0 * 9.018594741821289
Epoch 2550, val loss: 0.5647348761558533
Epoch 2560, training loss: 450.9833068847656 = 0.06071830168366432 + 50.0 * 9.018451690673828
Epoch 2560, val loss: 0.5662701725959778
Epoch 2570, training loss: 451.01446533203125 = 0.05997070297598839 + 50.0 * 9.019089698791504
Epoch 2570, val loss: 0.5681171417236328
Epoch 2580, training loss: 450.9577941894531 = 0.059227801859378815 + 50.0 * 9.01797103881836
Epoch 2580, val loss: 0.5706527829170227
Epoch 2590, training loss: 451.0492248535156 = 0.058582328259944916 + 50.0 * 9.01981258392334
Epoch 2590, val loss: 0.573627769947052
Epoch 2600, training loss: 450.9630432128906 = 0.057769957929849625 + 50.0 * 9.018105506896973
Epoch 2600, val loss: 0.5734500288963318
Epoch 2610, training loss: 450.9496154785156 = 0.05705879256129265 + 50.0 * 9.017850875854492
Epoch 2610, val loss: 0.5760153532028198
Epoch 2620, training loss: 450.9842529296875 = 0.05636531114578247 + 50.0 * 9.01855754852295
Epoch 2620, val loss: 0.5780996084213257
Epoch 2630, training loss: 450.9656982421875 = 0.05566830933094025 + 50.0 * 9.018200874328613
Epoch 2630, val loss: 0.5797463059425354
Epoch 2640, training loss: 450.9654235839844 = 0.054979465901851654 + 50.0 * 9.018208503723145
Epoch 2640, val loss: 0.5813875794410706
Epoch 2650, training loss: 450.9371643066406 = 0.054294005036354065 + 50.0 * 9.017657279968262
Epoch 2650, val loss: 0.5840264558792114
Epoch 2660, training loss: 450.9950256347656 = 0.053649574518203735 + 50.0 * 9.018827438354492
Epoch 2660, val loss: 0.5863552689552307
Epoch 2670, training loss: 450.9433898925781 = 0.05299195647239685 + 50.0 * 9.017807960510254
Epoch 2670, val loss: 0.5884464979171753
Epoch 2680, training loss: 450.95037841796875 = 0.0523514561355114 + 50.0 * 9.017960548400879
Epoch 2680, val loss: 0.5909432172775269
Epoch 2690, training loss: 450.953369140625 = 0.05166662856936455 + 50.0 * 9.018033981323242
Epoch 2690, val loss: 0.5917022824287415
Epoch 2700, training loss: 450.92181396484375 = 0.05102744698524475 + 50.0 * 9.017416000366211
Epoch 2700, val loss: 0.593560516834259
Epoch 2710, training loss: 450.8892517089844 = 0.05038933455944061 + 50.0 * 9.016777038574219
Epoch 2710, val loss: 0.5957186222076416
Epoch 2720, training loss: 450.89892578125 = 0.04977186769247055 + 50.0 * 9.016983032226562
Epoch 2720, val loss: 0.5981143712997437
Epoch 2730, training loss: 451.02923583984375 = 0.049190010875463486 + 50.0 * 9.019600868225098
Epoch 2730, val loss: 0.6001657843589783
Epoch 2740, training loss: 450.92779541015625 = 0.04856753721833229 + 50.0 * 9.017584800720215
Epoch 2740, val loss: 0.6000832915306091
Epoch 2750, training loss: 450.885986328125 = 0.0479431189596653 + 50.0 * 9.01676082611084
Epoch 2750, val loss: 0.6038156151771545
Epoch 2760, training loss: 450.886962890625 = 0.04735592380166054 + 50.0 * 9.016792297363281
Epoch 2760, val loss: 0.6046833395957947
Epoch 2770, training loss: 450.94091796875 = 0.046804338693618774 + 50.0 * 9.017882347106934
Epoch 2770, val loss: 0.6060845255851746
Epoch 2780, training loss: 450.9515686035156 = 0.046203821897506714 + 50.0 * 9.018107414245605
Epoch 2780, val loss: 0.6092998385429382
Epoch 2790, training loss: 450.86126708984375 = 0.04562273994088173 + 50.0 * 9.016312599182129
Epoch 2790, val loss: 0.6116993427276611
Epoch 2800, training loss: 450.8603820800781 = 0.045052144676446915 + 50.0 * 9.01630687713623
Epoch 2800, val loss: 0.6135419011116028
Epoch 2810, training loss: 450.9185791015625 = 0.04450834169983864 + 50.0 * 9.017481803894043
Epoch 2810, val loss: 0.615079939365387
Epoch 2820, training loss: 450.8667297363281 = 0.04395703598856926 + 50.0 * 9.01645565032959
Epoch 2820, val loss: 0.6176066398620605
Epoch 2830, training loss: 450.8526306152344 = 0.043421223759651184 + 50.0 * 9.016183853149414
Epoch 2830, val loss: 0.6186480522155762
Epoch 2840, training loss: 450.8735046386719 = 0.04289506748318672 + 50.0 * 9.01661205291748
Epoch 2840, val loss: 0.6209394931793213
Epoch 2850, training loss: 450.8891906738281 = 0.04240332916378975 + 50.0 * 9.016936302185059
Epoch 2850, val loss: 0.6246339082717896
Epoch 2860, training loss: 450.8542785644531 = 0.04184550419449806 + 50.0 * 9.01624870300293
Epoch 2860, val loss: 0.6253807544708252
Epoch 2870, training loss: 450.85943603515625 = 0.041327957063913345 + 50.0 * 9.016362190246582
Epoch 2870, val loss: 0.6270590424537659
Epoch 2880, training loss: 450.8190002441406 = 0.04081423208117485 + 50.0 * 9.01556396484375
Epoch 2880, val loss: 0.6294352412223816
Epoch 2890, training loss: 450.85675048828125 = 0.04032554477453232 + 50.0 * 9.016328811645508
Epoch 2890, val loss: 0.6317782998085022
Epoch 2900, training loss: 450.8155517578125 = 0.039824824780225754 + 50.0 * 9.015514373779297
Epoch 2900, val loss: 0.6336814165115356
Epoch 2910, training loss: 450.8233947753906 = 0.03935854509472847 + 50.0 * 9.015680313110352
Epoch 2910, val loss: 0.6363546848297119
Epoch 2920, training loss: 450.8254089355469 = 0.03891213238239288 + 50.0 * 9.015729904174805
Epoch 2920, val loss: 0.6384369134902954
Epoch 2930, training loss: 450.8326110839844 = 0.03839780017733574 + 50.0 * 9.015884399414062
Epoch 2930, val loss: 0.6392694115638733
Epoch 2940, training loss: 450.8640441894531 = 0.03794854134321213 + 50.0 * 9.016522407531738
Epoch 2940, val loss: 0.6416745781898499
Epoch 2950, training loss: 450.8027038574219 = 0.03747851029038429 + 50.0 * 9.015304565429688
Epoch 2950, val loss: 0.6436305642127991
Epoch 2960, training loss: 450.7966613769531 = 0.03702455013990402 + 50.0 * 9.015192985534668
Epoch 2960, val loss: 0.6452609896659851
Epoch 2970, training loss: 450.91497802734375 = 0.0366535522043705 + 50.0 * 9.017566680908203
Epoch 2970, val loss: 0.6454589366912842
Epoch 2980, training loss: 450.8127746582031 = 0.03615720197558403 + 50.0 * 9.015532493591309
Epoch 2980, val loss: 0.6490287184715271
Epoch 2990, training loss: 450.7747802734375 = 0.0357249416410923 + 50.0 * 9.01478099822998
Epoch 2990, val loss: 0.6519481539726257
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8529
Overall ASR: 0.6978
Flip ASR: 0.6223/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.34 GiB already allocated; 415.69 MiB free; 3.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 699.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 105.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 101.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 105.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 101.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0556640625 = 1.1009520292282104 + 50.0 * 10.35909366607666
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 497.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0482788085938 = 1.0909987688064575 + 50.0 * 10.359145164489746
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 497.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0617065429688 = 1.1116129159927368 + 50.0 * 10.359001159667969
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 493.69 MiB free; 6.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0480346679688 = 1.1005921363830566 + 50.0 * 10.358948707580566
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 497.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0576171875 = 1.1075135469436646 + 50.0 * 10.359001159667969
Epoch 0, val loss: 1.1064083576202393
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.21 GiB already allocated; 207.69 MiB free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 101.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 101.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 105.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 445.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 167.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 839.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 839.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 839.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 839.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 839.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 167.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 445.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 167.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 167.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 167.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 165.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 165.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 837.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 837.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 837.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 837.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 779.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 789.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 789.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.05615234375 = 1.1000237464904785 + 50.0 * 10.359122276306152
Epoch 0, val loss: 1.0998399257659912
Epoch 10, training loss: 518.9281005859375 = 1.089449167251587 + 50.0 * 10.356772422790527
Epoch 10, val loss: 1.0888779163360596
Epoch 20, training loss: 516.9801025390625 = 1.0744353532791138 + 50.0 * 10.31811237335205
Epoch 20, val loss: 1.073636770248413
Epoch 30, training loss: 495.4994201660156 = 1.0599722862243652 + 50.0 * 9.888789176940918
Epoch 30, val loss: 1.0593401193618774
Epoch 40, training loss: 479.7355041503906 = 1.046195387840271 + 50.0 * 9.573785781860352
Epoch 40, val loss: 1.0459083318710327
Epoch 50, training loss: 473.8531494140625 = 1.031614899635315 + 50.0 * 9.456430435180664
Epoch 50, val loss: 1.0313684940338135
Epoch 60, training loss: 472.0221862792969 = 1.0148117542266846 + 50.0 * 9.420147895812988
Epoch 60, val loss: 1.01491379737854
Epoch 70, training loss: 469.35009765625 = 0.9981544017791748 + 50.0 * 9.36703872680664
Epoch 70, val loss: 0.9990348815917969
Epoch 80, training loss: 466.0844421386719 = 0.9836455583572388 + 50.0 * 9.302016258239746
Epoch 80, val loss: 0.9853552579879761
Epoch 90, training loss: 464.0498046875 = 0.9681562185287476 + 50.0 * 9.261632919311523
Epoch 90, val loss: 0.9703831672668457
Epoch 100, training loss: 462.7362365722656 = 0.9479018449783325 + 50.0 * 9.235766410827637
Epoch 100, val loss: 0.9508228302001953
Epoch 110, training loss: 461.8550109863281 = 0.9245260953903198 + 50.0 * 9.218609809875488
Epoch 110, val loss: 0.9285072684288025
Epoch 120, training loss: 461.2387390136719 = 0.8996266722679138 + 50.0 * 9.206782341003418
Epoch 120, val loss: 0.9049020409584045
Epoch 130, training loss: 460.67535400390625 = 0.8732789158821106 + 50.0 * 9.196041107177734
Epoch 130, val loss: 0.8801714777946472
Epoch 140, training loss: 460.12677001953125 = 0.8459486961364746 + 50.0 * 9.185616493225098
Epoch 140, val loss: 0.8547630310058594
Epoch 150, training loss: 459.61822509765625 = 0.8174536824226379 + 50.0 * 9.176015853881836
Epoch 150, val loss: 0.8284026980400085
Epoch 160, training loss: 459.22393798828125 = 0.7876923680305481 + 50.0 * 9.16872501373291
Epoch 160, val loss: 0.8009862899780273
Epoch 170, training loss: 459.02081298828125 = 0.7568052411079407 + 50.0 * 9.16528034210205
Epoch 170, val loss: 0.7727152109146118
Epoch 180, training loss: 458.6304931640625 = 0.7248641848564148 + 50.0 * 9.158112525939941
Epoch 180, val loss: 0.7438182234764099
Epoch 190, training loss: 458.41412353515625 = 0.6938491463661194 + 50.0 * 9.15440559387207
Epoch 190, val loss: 0.7160899043083191
Epoch 200, training loss: 458.1726989746094 = 0.6645492911338806 + 50.0 * 9.150162696838379
Epoch 200, val loss: 0.6902440190315247
Epoch 210, training loss: 457.9766540527344 = 0.6372314691543579 + 50.0 * 9.146788597106934
Epoch 210, val loss: 0.6665457487106323
Epoch 220, training loss: 457.7908630371094 = 0.6121367812156677 + 50.0 * 9.143574714660645
Epoch 220, val loss: 0.6450616121292114
Epoch 230, training loss: 457.62542724609375 = 0.5895045399665833 + 50.0 * 9.140718460083008
Epoch 230, val loss: 0.6262111067771912
Epoch 240, training loss: 457.5997314453125 = 0.5692949891090393 + 50.0 * 9.140608787536621
Epoch 240, val loss: 0.6095980405807495
Epoch 250, training loss: 457.38140869140625 = 0.5515088438987732 + 50.0 * 9.136597633361816
Epoch 250, val loss: 0.5956279039382935
Epoch 260, training loss: 457.2217712402344 = 0.5362940430641174 + 50.0 * 9.133709907531738
Epoch 260, val loss: 0.5838951468467712
Epoch 270, training loss: 457.0857849121094 = 0.5230752229690552 + 50.0 * 9.131254196166992
Epoch 270, val loss: 0.5741271376609802
Epoch 280, training loss: 457.02178955078125 = 0.5115602612495422 + 50.0 * 9.130204200744629
Epoch 280, val loss: 0.565898597240448
Epoch 290, training loss: 456.8640441894531 = 0.5012840032577515 + 50.0 * 9.1272554397583
Epoch 290, val loss: 0.5588229298591614
Epoch 300, training loss: 456.7561950683594 = 0.4924090802669525 + 50.0 * 9.125275611877441
Epoch 300, val loss: 0.5530716776847839
Epoch 310, training loss: 456.6509094238281 = 0.4846819341182709 + 50.0 * 9.123324394226074
Epoch 310, val loss: 0.5483196973800659
Epoch 320, training loss: 456.5448303222656 = 0.47778621315956116 + 50.0 * 9.12134075164795
Epoch 320, val loss: 0.5442702174186707
Epoch 330, training loss: 456.54461669921875 = 0.4714653491973877 + 50.0 * 9.12146282196045
Epoch 330, val loss: 0.5406414270401001
Epoch 340, training loss: 456.3866882324219 = 0.46558502316474915 + 50.0 * 9.118422508239746
Epoch 340, val loss: 0.5374802350997925
Epoch 350, training loss: 456.2756652832031 = 0.46037253737449646 + 50.0 * 9.11630630493164
Epoch 350, val loss: 0.5348265767097473
Epoch 360, training loss: 456.20379638671875 = 0.45557916164398193 + 50.0 * 9.114964485168457
Epoch 360, val loss: 0.5324444770812988
Epoch 370, training loss: 456.1187744140625 = 0.450982004404068 + 50.0 * 9.11335563659668
Epoch 370, val loss: 0.5302379727363586
Epoch 380, training loss: 456.070556640625 = 0.4466659724712372 + 50.0 * 9.112478256225586
Epoch 380, val loss: 0.5281020402908325
Epoch 390, training loss: 455.9883728027344 = 0.4426664710044861 + 50.0 * 9.11091423034668
Epoch 390, val loss: 0.5262536406517029
Epoch 400, training loss: 456.14129638671875 = 0.43885165452957153 + 50.0 * 9.114048957824707
Epoch 400, val loss: 0.5244385600090027
Epoch 410, training loss: 455.8980407714844 = 0.43496838212013245 + 50.0 * 9.109261512756348
Epoch 410, val loss: 0.5225669145584106
Epoch 420, training loss: 455.82769775390625 = 0.43142491579055786 + 50.0 * 9.107925415039062
Epoch 420, val loss: 0.5208858251571655
Epoch 430, training loss: 455.74176025390625 = 0.42806610465049744 + 50.0 * 9.106273651123047
Epoch 430, val loss: 0.5193384885787964
Epoch 440, training loss: 455.6805725097656 = 0.4247824251651764 + 50.0 * 9.10511589050293
Epoch 440, val loss: 0.5177746415138245
Epoch 450, training loss: 455.8642272949219 = 0.4215152859687805 + 50.0 * 9.108854293823242
Epoch 450, val loss: 0.5162391662597656
Epoch 460, training loss: 455.610107421875 = 0.4181314706802368 + 50.0 * 9.103839874267578
Epoch 460, val loss: 0.5143694877624512
Epoch 470, training loss: 455.5385437011719 = 0.4149826467037201 + 50.0 * 9.102471351623535
Epoch 470, val loss: 0.5127244591712952
Epoch 480, training loss: 455.4810485839844 = 0.4119194447994232 + 50.0 * 9.1013822555542
Epoch 480, val loss: 0.5110921859741211
Epoch 490, training loss: 455.5307312011719 = 0.4088604748249054 + 50.0 * 9.102437019348145
Epoch 490, val loss: 0.5093357563018799
Epoch 500, training loss: 455.4320983886719 = 0.4056893289089203 + 50.0 * 9.1005277633667
Epoch 500, val loss: 0.5076926946640015
Epoch 510, training loss: 455.3505859375 = 0.4026428461074829 + 50.0 * 9.098958969116211
Epoch 510, val loss: 0.5059369802474976
Epoch 520, training loss: 455.40631103515625 = 0.39962220191955566 + 50.0 * 9.100133895874023
Epoch 520, val loss: 0.5040823817253113
Epoch 530, training loss: 455.3130798339844 = 0.3965388238430023 + 50.0 * 9.0983304977417
Epoch 530, val loss: 0.5022730231285095
Epoch 540, training loss: 455.2318115234375 = 0.39350131154060364 + 50.0 * 9.096766471862793
Epoch 540, val loss: 0.5006420016288757
Epoch 550, training loss: 455.2004699707031 = 0.39052480459213257 + 50.0 * 9.096199035644531
Epoch 550, val loss: 0.49893879890441895
Epoch 560, training loss: 455.1527404785156 = 0.38756436109542847 + 50.0 * 9.095303535461426
Epoch 560, val loss: 0.4971480667591095
Epoch 570, training loss: 455.12652587890625 = 0.3846000134944916 + 50.0 * 9.09483814239502
Epoch 570, val loss: 0.4954879581928253
Epoch 580, training loss: 455.15966796875 = 0.38151630759239197 + 50.0 * 9.095562934875488
Epoch 580, val loss: 0.49335455894470215
Epoch 590, training loss: 455.0792541503906 = 0.3783721327781677 + 50.0 * 9.09401798248291
Epoch 590, val loss: 0.491514652967453
Epoch 600, training loss: 455.0221252441406 = 0.37544500827789307 + 50.0 * 9.092933654785156
Epoch 600, val loss: 0.48974403738975525
Epoch 610, training loss: 454.9859619140625 = 0.3725516200065613 + 50.0 * 9.092267990112305
Epoch 610, val loss: 0.48797619342803955
Epoch 620, training loss: 454.9560852050781 = 0.369640588760376 + 50.0 * 9.091729164123535
Epoch 620, val loss: 0.4862090051174164
Epoch 630, training loss: 455.0794677734375 = 0.36670631170272827 + 50.0 * 9.094255447387695
Epoch 630, val loss: 0.484240859746933
Epoch 640, training loss: 454.9453125 = 0.3635979890823364 + 50.0 * 9.091634750366211
Epoch 640, val loss: 0.4825736880302429
Epoch 650, training loss: 454.896240234375 = 0.36063873767852783 + 50.0 * 9.090712547302246
Epoch 650, val loss: 0.4807303547859192
Epoch 660, training loss: 454.8643493652344 = 0.35772043466567993 + 50.0 * 9.090132713317871
Epoch 660, val loss: 0.4788936674594879
Epoch 670, training loss: 454.8401794433594 = 0.3547593355178833 + 50.0 * 9.08970832824707
Epoch 670, val loss: 0.4770357012748718
Epoch 680, training loss: 454.9439392089844 = 0.35179242491722107 + 50.0 * 9.091842651367188
Epoch 680, val loss: 0.47521865367889404
Epoch 690, training loss: 454.7782287597656 = 0.34877121448516846 + 50.0 * 9.08858871459961
Epoch 690, val loss: 0.4733215272426605
Epoch 700, training loss: 454.7427978515625 = 0.34587517380714417 + 50.0 * 9.08793830871582
Epoch 700, val loss: 0.4714774191379547
Epoch 710, training loss: 454.7193603515625 = 0.34299686551094055 + 50.0 * 9.08752727508545
Epoch 710, val loss: 0.4698250889778137
Epoch 720, training loss: 454.7738952636719 = 0.34005051851272583 + 50.0 * 9.088676452636719
Epoch 720, val loss: 0.46792879700660706
Epoch 730, training loss: 454.664794921875 = 0.33707886934280396 + 50.0 * 9.086554527282715
Epoch 730, val loss: 0.4660716950893402
Epoch 740, training loss: 454.63519287109375 = 0.33422374725341797 + 50.0 * 9.086019515991211
Epoch 740, val loss: 0.46444082260131836
Epoch 750, training loss: 454.6007080078125 = 0.3313775658607483 + 50.0 * 9.085386276245117
Epoch 750, val loss: 0.46283847093582153
Epoch 760, training loss: 454.6231689453125 = 0.32853129506111145 + 50.0 * 9.085892677307129
Epoch 760, val loss: 0.4610922038555145
Epoch 770, training loss: 454.5879211425781 = 0.3255562484264374 + 50.0 * 9.085247039794922
Epoch 770, val loss: 0.45927006006240845
Epoch 780, training loss: 454.56695556640625 = 0.3226698637008667 + 50.0 * 9.084885597229004
Epoch 780, val loss: 0.45766282081604004
Epoch 790, training loss: 454.5381774902344 = 0.31983593106269836 + 50.0 * 9.084366798400879
Epoch 790, val loss: 0.4559505581855774
Epoch 800, training loss: 454.508544921875 = 0.3170303404331207 + 50.0 * 9.083830833435059
Epoch 800, val loss: 0.4543272852897644
Epoch 810, training loss: 454.46942138671875 = 0.3142206370830536 + 50.0 * 9.083104133605957
Epoch 810, val loss: 0.45276591181755066
Epoch 820, training loss: 454.48333740234375 = 0.31143417954444885 + 50.0 * 9.0834379196167
Epoch 820, val loss: 0.4511354863643646
Epoch 830, training loss: 454.4103698730469 = 0.3086141049861908 + 50.0 * 9.082035064697266
Epoch 830, val loss: 0.4496353268623352
Epoch 840, training loss: 454.4190368652344 = 0.3058585226535797 + 50.0 * 9.082263946533203
Epoch 840, val loss: 0.4480046033859253
Epoch 850, training loss: 454.4376220703125 = 0.3030667304992676 + 50.0 * 9.082691192626953
Epoch 850, val loss: 0.44653668999671936
Epoch 860, training loss: 454.36419677734375 = 0.3002662658691406 + 50.0 * 9.081278800964355
Epoch 860, val loss: 0.44518592953681946
Epoch 870, training loss: 454.3661804199219 = 0.2975698709487915 + 50.0 * 9.081372261047363
Epoch 870, val loss: 0.4441159963607788
Epoch 880, training loss: 454.3095397949219 = 0.2948540151119232 + 50.0 * 9.080293655395508
Epoch 880, val loss: 0.442473441362381
Epoch 890, training loss: 454.2943420410156 = 0.29219940304756165 + 50.0 * 9.080042839050293
Epoch 890, val loss: 0.44106122851371765
Epoch 900, training loss: 454.4116516113281 = 0.289546400308609 + 50.0 * 9.082442283630371
Epoch 900, val loss: 0.440103679895401
Epoch 910, training loss: 454.25341796875 = 0.2868327498435974 + 50.0 * 9.079331398010254
Epoch 910, val loss: 0.4383639097213745
Epoch 920, training loss: 454.2109069824219 = 0.2842290997505188 + 50.0 * 9.078533172607422
Epoch 920, val loss: 0.4370735287666321
Epoch 930, training loss: 454.27001953125 = 0.2816593647003174 + 50.0 * 9.079767227172852
Epoch 930, val loss: 0.4356301724910736
Epoch 940, training loss: 454.2019348144531 = 0.2789636552333832 + 50.0 * 9.078459739685059
Epoch 940, val loss: 0.43545836210250854
Epoch 950, training loss: 454.17205810546875 = 0.27640125155448914 + 50.0 * 9.077913284301758
Epoch 950, val loss: 0.43354058265686035
Epoch 960, training loss: 454.1451416015625 = 0.27384689450263977 + 50.0 * 9.077425956726074
Epoch 960, val loss: 0.43310749530792236
Epoch 970, training loss: 454.2669372558594 = 0.2712809145450592 + 50.0 * 9.079913139343262
Epoch 970, val loss: 0.4319538176059723
Epoch 980, training loss: 454.10247802734375 = 0.26869580149650574 + 50.0 * 9.076675415039062
Epoch 980, val loss: 0.4306999742984772
Epoch 990, training loss: 454.09149169921875 = 0.26619458198547363 + 50.0 * 9.076505661010742
Epoch 990, val loss: 0.4300161302089691
Epoch 1000, training loss: 454.0428161621094 = 0.26372283697128296 + 50.0 * 9.075581550598145
Epoch 1000, val loss: 0.4293063282966614
Epoch 1010, training loss: 454.0395812988281 = 0.26125863194465637 + 50.0 * 9.075566291809082
Epoch 1010, val loss: 0.4283808469772339
Epoch 1020, training loss: 454.1416320800781 = 0.2587634325027466 + 50.0 * 9.077657699584961
Epoch 1020, val loss: 0.42780953645706177
Epoch 1030, training loss: 454.12103271484375 = 0.2562650144100189 + 50.0 * 9.077295303344727
Epoch 1030, val loss: 0.42705222964286804
Epoch 1040, training loss: 453.98980712890625 = 0.25376763939857483 + 50.0 * 9.074721336364746
Epoch 1040, val loss: 0.42623716592788696
Epoch 1050, training loss: 453.9617004394531 = 0.25134560465812683 + 50.0 * 9.074207305908203
Epoch 1050, val loss: 0.4256950616836548
Epoch 1060, training loss: 453.93231201171875 = 0.24893102049827576 + 50.0 * 9.073667526245117
Epoch 1060, val loss: 0.4250032305717468
Epoch 1070, training loss: 454.1354675292969 = 0.24651667475700378 + 50.0 * 9.077778816223145
Epoch 1070, val loss: 0.424413800239563
Epoch 1080, training loss: 453.93621826171875 = 0.2440250813961029 + 50.0 * 9.073843955993652
Epoch 1080, val loss: 0.42420995235443115
Epoch 1090, training loss: 453.88848876953125 = 0.24162691831588745 + 50.0 * 9.07293701171875
Epoch 1090, val loss: 0.42372238636016846
Epoch 1100, training loss: 453.861328125 = 0.23924681544303894 + 50.0 * 9.072441101074219
Epoch 1100, val loss: 0.42333194613456726
Epoch 1110, training loss: 453.9268798828125 = 0.23687629401683807 + 50.0 * 9.073800086975098
Epoch 1110, val loss: 0.4230004847049713
Epoch 1120, training loss: 453.8536071777344 = 0.23446178436279297 + 50.0 * 9.072382926940918
Epoch 1120, val loss: 0.42281612753868103
Epoch 1130, training loss: 453.85894775390625 = 0.23211678862571716 + 50.0 * 9.07253646850586
Epoch 1130, val loss: 0.4229186773300171
Epoch 1140, training loss: 453.9082336425781 = 0.22983521223068237 + 50.0 * 9.073568344116211
Epoch 1140, val loss: 0.42307448387145996
Epoch 1150, training loss: 453.79364013671875 = 0.22743210196495056 + 50.0 * 9.071324348449707
Epoch 1150, val loss: 0.42214235663414
Epoch 1160, training loss: 453.7633056640625 = 0.2251361608505249 + 50.0 * 9.07076358795166
Epoch 1160, val loss: 0.42225417494773865
Epoch 1170, training loss: 453.7380676269531 = 0.22285720705986023 + 50.0 * 9.070303916931152
Epoch 1170, val loss: 0.42212849855422974
Epoch 1180, training loss: 453.7301940917969 = 0.22057759761810303 + 50.0 * 9.070192337036133
Epoch 1180, val loss: 0.4224042296409607
Epoch 1190, training loss: 453.8531188964844 = 0.21829678118228912 + 50.0 * 9.072696685791016
Epoch 1190, val loss: 0.4222021996974945
Epoch 1200, training loss: 453.7237243652344 = 0.21600136160850525 + 50.0 * 9.070154190063477
Epoch 1200, val loss: 0.42267197370529175
Epoch 1210, training loss: 453.7057189941406 = 0.21375012397766113 + 50.0 * 9.069839477539062
Epoch 1210, val loss: 0.42266348004341125
Epoch 1220, training loss: 453.6626892089844 = 0.21153120696544647 + 50.0 * 9.069023132324219
Epoch 1220, val loss: 0.42277851700782776
Epoch 1230, training loss: 453.6673583984375 = 0.2093576341867447 + 50.0 * 9.069160461425781
Epoch 1230, val loss: 0.4225662350654602
Epoch 1240, training loss: 453.68304443359375 = 0.20718611776828766 + 50.0 * 9.069517135620117
Epoch 1240, val loss: 0.42297065258026123
Epoch 1250, training loss: 453.6268310546875 = 0.20495137572288513 + 50.0 * 9.068437576293945
Epoch 1250, val loss: 0.4245850741863251
Epoch 1260, training loss: 453.6153564453125 = 0.2027777135372162 + 50.0 * 9.068251609802246
Epoch 1260, val loss: 0.42468950152397156
Epoch 1270, training loss: 453.6452331542969 = 0.20066095888614655 + 50.0 * 9.068891525268555
Epoch 1270, val loss: 0.4254355728626251
Epoch 1280, training loss: 453.58746337890625 = 0.19849546253681183 + 50.0 * 9.067779541015625
Epoch 1280, val loss: 0.42540404200553894
Epoch 1290, training loss: 453.62701416015625 = 0.1964196413755417 + 50.0 * 9.068612098693848
Epoch 1290, val loss: 0.42671263217926025
Epoch 1300, training loss: 453.53466796875 = 0.19425086677074432 + 50.0 * 9.066808700561523
Epoch 1300, val loss: 0.4267159104347229
Epoch 1310, training loss: 453.51373291015625 = 0.19215932488441467 + 50.0 * 9.066431045532227
Epoch 1310, val loss: 0.4274633824825287
Epoch 1320, training loss: 453.5894775390625 = 0.19010043144226074 + 50.0 * 9.067987442016602
Epoch 1320, val loss: 0.4286070168018341
Epoch 1330, training loss: 453.5384521484375 = 0.18799997866153717 + 50.0 * 9.067008972167969
Epoch 1330, val loss: 0.4289693534374237
Epoch 1340, training loss: 453.54864501953125 = 0.18593324720859528 + 50.0 * 9.067254066467285
Epoch 1340, val loss: 0.4295749068260193
Epoch 1350, training loss: 453.46990966796875 = 0.18386872112751007 + 50.0 * 9.065720558166504
Epoch 1350, val loss: 0.4303828775882721
Epoch 1360, training loss: 453.4434509277344 = 0.18183691799640656 + 50.0 * 9.065232276916504
Epoch 1360, val loss: 0.4310036897659302
Epoch 1370, training loss: 453.43585205078125 = 0.17981687188148499 + 50.0 * 9.065120697021484
Epoch 1370, val loss: 0.4321708679199219
Epoch 1380, training loss: 453.477294921875 = 0.17784875631332397 + 50.0 * 9.065988540649414
Epoch 1380, val loss: 0.43363502621650696
Epoch 1390, training loss: 453.4159851074219 = 0.1758282333612442 + 50.0 * 9.064803123474121
Epoch 1390, val loss: 0.4345532953739166
Epoch 1400, training loss: 453.4466552734375 = 0.17381955683231354 + 50.0 * 9.06545639038086
Epoch 1400, val loss: 0.43507230281829834
Epoch 1410, training loss: 453.38433837890625 = 0.17184704542160034 + 50.0 * 9.064249992370605
Epoch 1410, val loss: 0.4350414574146271
Epoch 1420, training loss: 453.41363525390625 = 0.16989398002624512 + 50.0 * 9.064874649047852
Epoch 1420, val loss: 0.43683597445487976
Epoch 1430, training loss: 453.3528747558594 = 0.16794314980506897 + 50.0 * 9.063698768615723
Epoch 1430, val loss: 0.43715932965278625
Epoch 1440, training loss: 453.3379821777344 = 0.1660187840461731 + 50.0 * 9.06343936920166
Epoch 1440, val loss: 0.43882080912590027
Epoch 1450, training loss: 453.3622741699219 = 0.16410204768180847 + 50.0 * 9.063963890075684
Epoch 1450, val loss: 0.4392525255680084
Epoch 1460, training loss: 453.36529541015625 = 0.162215918302536 + 50.0 * 9.064062118530273
Epoch 1460, val loss: 0.440249502658844
Epoch 1470, training loss: 453.2840270996094 = 0.16029223799705505 + 50.0 * 9.062475204467773
Epoch 1470, val loss: 0.4413604438304901
Epoch 1480, training loss: 453.2701721191406 = 0.1584159880876541 + 50.0 * 9.062234878540039
Epoch 1480, val loss: 0.44360384345054626
Epoch 1490, training loss: 453.2520446777344 = 0.15651901066303253 + 50.0 * 9.061910629272461
Epoch 1490, val loss: 0.44394755363464355
Epoch 1500, training loss: 453.3155212402344 = 0.15471214056015015 + 50.0 * 9.063216209411621
Epoch 1500, val loss: 0.4463339149951935
Epoch 1510, training loss: 453.25738525390625 = 0.15282320976257324 + 50.0 * 9.062090873718262
Epoch 1510, val loss: 0.4462345838546753
Epoch 1520, training loss: 453.2471008300781 = 0.1510183960199356 + 50.0 * 9.061921119689941
Epoch 1520, val loss: 0.4483574628829956
Epoch 1530, training loss: 453.25189208984375 = 0.14920535683631897 + 50.0 * 9.062053680419922
Epoch 1530, val loss: 0.4486461281776428
Epoch 1540, training loss: 453.24713134765625 = 0.1473996788263321 + 50.0 * 9.061994552612305
Epoch 1540, val loss: 0.45047757029533386
Epoch 1550, training loss: 453.2121887207031 = 0.14567682147026062 + 50.0 * 9.06132984161377
Epoch 1550, val loss: 0.4526837170124054
Epoch 1560, training loss: 453.1873779296875 = 0.14385487139225006 + 50.0 * 9.060870170593262
Epoch 1560, val loss: 0.45238563418388367
Epoch 1570, training loss: 453.2343444824219 = 0.1420949399471283 + 50.0 * 9.061844825744629
Epoch 1570, val loss: 0.45398563146591187
Epoch 1580, training loss: 453.15313720703125 = 0.14034996926784515 + 50.0 * 9.060256004333496
Epoch 1580, val loss: 0.45636558532714844
Epoch 1590, training loss: 453.1235656738281 = 0.13859909772872925 + 50.0 * 9.059699058532715
Epoch 1590, val loss: 0.4571765065193176
Epoch 1600, training loss: 453.1294860839844 = 0.1368914097547531 + 50.0 * 9.05985164642334
Epoch 1600, val loss: 0.4590441882610321
Epoch 1610, training loss: 453.3819274902344 = 0.13545846939086914 + 50.0 * 9.064929008483887
Epoch 1610, val loss: 0.4616336226463318
Epoch 1620, training loss: 453.1240539550781 = 0.13351477682590485 + 50.0 * 9.059810638427734
Epoch 1620, val loss: 0.4618784785270691
Epoch 1630, training loss: 453.0906066894531 = 0.1318463236093521 + 50.0 * 9.059175491333008
Epoch 1630, val loss: 0.4633893668651581
Epoch 1640, training loss: 453.07318115234375 = 0.1301986277103424 + 50.0 * 9.058859825134277
Epoch 1640, val loss: 0.4645408093929291
Epoch 1650, training loss: 453.047607421875 = 0.12854592502117157 + 50.0 * 9.058381080627441
Epoch 1650, val loss: 0.46626272797584534
Epoch 1660, training loss: 453.05755615234375 = 0.12692852318286896 + 50.0 * 9.058612823486328
Epoch 1660, val loss: 0.46752581000328064
Epoch 1670, training loss: 453.1606140136719 = 0.12541134655475616 + 50.0 * 9.060704231262207
Epoch 1670, val loss: 0.4688253700733185
Epoch 1680, training loss: 453.0340881347656 = 0.12372727692127228 + 50.0 * 9.058207511901855
Epoch 1680, val loss: 0.4712725877761841
Epoch 1690, training loss: 453.0458984375 = 0.12216173112392426 + 50.0 * 9.05847454071045
Epoch 1690, val loss: 0.4732726812362671
Epoch 1700, training loss: 453.0201110839844 = 0.12058186531066895 + 50.0 * 9.057991027832031
Epoch 1700, val loss: 0.4752192199230194
Epoch 1710, training loss: 453.0259094238281 = 0.11900357156991959 + 50.0 * 9.058137893676758
Epoch 1710, val loss: 0.476425439119339
Epoch 1720, training loss: 453.0414733886719 = 0.11745070666074753 + 50.0 * 9.058480262756348
Epoch 1720, val loss: 0.4780676066875458
Epoch 1730, training loss: 453.00921630859375 = 0.11590564250946045 + 50.0 * 9.057866096496582
Epoch 1730, val loss: 0.4797155261039734
Epoch 1740, training loss: 453.0655212402344 = 0.11439255625009537 + 50.0 * 9.059022903442383
Epoch 1740, val loss: 0.4819093644618988
Epoch 1750, training loss: 452.98345947265625 = 0.11285365372896194 + 50.0 * 9.057412147521973
Epoch 1750, val loss: 0.4835397005081177
Epoch 1760, training loss: 452.9539794921875 = 0.11135212332010269 + 50.0 * 9.056852340698242
Epoch 1760, val loss: 0.4852515757083893
Epoch 1770, training loss: 452.98394775390625 = 0.10986402630805969 + 50.0 * 9.05748176574707
Epoch 1770, val loss: 0.48680758476257324
Epoch 1780, training loss: 452.9185485839844 = 0.10837485641241074 + 50.0 * 9.056203842163086
Epoch 1780, val loss: 0.4887561500072479
Epoch 1790, training loss: 452.96783447265625 = 0.10690460354089737 + 50.0 * 9.057218551635742
Epoch 1790, val loss: 0.4910002648830414
Epoch 1800, training loss: 452.9017028808594 = 0.1054469421505928 + 50.0 * 9.055925369262695
Epoch 1800, val loss: 0.49282386898994446
Epoch 1810, training loss: 452.8783264160156 = 0.10400690883398056 + 50.0 * 9.055486679077148
Epoch 1810, val loss: 0.49603623151779175
Epoch 1820, training loss: 452.9646301269531 = 0.10269081592559814 + 50.0 * 9.057238578796387
Epoch 1820, val loss: 0.49883031845092773
Epoch 1830, training loss: 452.8951721191406 = 0.10121019929647446 + 50.0 * 9.055879592895508
Epoch 1830, val loss: 0.49806687235832214
Epoch 1840, training loss: 452.8622741699219 = 0.0998082309961319 + 50.0 * 9.055249214172363
Epoch 1840, val loss: 0.5022786259651184
Epoch 1850, training loss: 452.8493957519531 = 0.09842079877853394 + 50.0 * 9.05501937866211
Epoch 1850, val loss: 0.5029219388961792
Epoch 1860, training loss: 452.9730529785156 = 0.09709283709526062 + 50.0 * 9.05751895904541
Epoch 1860, val loss: 0.5050888657569885
Epoch 1870, training loss: 452.89862060546875 = 0.09573255479335785 + 50.0 * 9.056057929992676
Epoch 1870, val loss: 0.5081852674484253
Epoch 1880, training loss: 452.8223876953125 = 0.09439172595739365 + 50.0 * 9.054559707641602
Epoch 1880, val loss: 0.509725034236908
Epoch 1890, training loss: 452.80352783203125 = 0.09306885302066803 + 50.0 * 9.054208755493164
Epoch 1890, val loss: 0.5123275518417358
Epoch 1900, training loss: 452.86676025390625 = 0.09179434180259705 + 50.0 * 9.055499076843262
Epoch 1900, val loss: 0.5141252279281616
Epoch 1910, training loss: 452.83575439453125 = 0.09051144868135452 + 50.0 * 9.05490493774414
Epoch 1910, val loss: 0.5159974098205566
Epoch 1920, training loss: 452.7767028808594 = 0.08921436965465546 + 50.0 * 9.053750038146973
Epoch 1920, val loss: 0.5193688869476318
Epoch 1930, training loss: 452.77203369140625 = 0.08794324845075607 + 50.0 * 9.053681373596191
Epoch 1930, val loss: 0.520925760269165
Epoch 1940, training loss: 452.91229248046875 = 0.0867156982421875 + 50.0 * 9.056510925292969
Epoch 1940, val loss: 0.5237430930137634
Epoch 1950, training loss: 452.81890869140625 = 0.08547745645046234 + 50.0 * 9.054668426513672
Epoch 1950, val loss: 0.525689959526062
Epoch 1960, training loss: 452.74114990234375 = 0.0842321589589119 + 50.0 * 9.053138732910156
Epoch 1960, val loss: 0.5280175805091858
Epoch 1970, training loss: 452.71563720703125 = 0.08301626890897751 + 50.0 * 9.052652359008789
Epoch 1970, val loss: 0.530101478099823
Epoch 1980, training loss: 452.8524475097656 = 0.0818764716386795 + 50.0 * 9.055411338806152
Epoch 1980, val loss: 0.5314552187919617
Epoch 1990, training loss: 452.7624816894531 = 0.08066288381814957 + 50.0 * 9.05363655090332
Epoch 1990, val loss: 0.535076379776001
Epoch 2000, training loss: 452.72149658203125 = 0.07950184494256973 + 50.0 * 9.052840232849121
Epoch 2000, val loss: 0.5364236831665039
Epoch 2010, training loss: 452.6804504394531 = 0.07832465320825577 + 50.0 * 9.052042007446289
Epoch 2010, val loss: 0.5386696457862854
Epoch 2020, training loss: 452.80572509765625 = 0.07728809118270874 + 50.0 * 9.05456829071045
Epoch 2020, val loss: 0.5402274131774902
Epoch 2030, training loss: 452.70001220703125 = 0.07607080787420273 + 50.0 * 9.052478790283203
Epoch 2030, val loss: 0.5443381071090698
Epoch 2040, training loss: 452.7027893066406 = 0.07504858076572418 + 50.0 * 9.052555084228516
Epoch 2040, val loss: 0.5445955991744995
Epoch 2050, training loss: 452.669677734375 = 0.07387397438287735 + 50.0 * 9.051916122436523
Epoch 2050, val loss: 0.5486885905265808
Epoch 2060, training loss: 452.70819091796875 = 0.07281588017940521 + 50.0 * 9.05270767211914
Epoch 2060, val loss: 0.5503793358802795
Epoch 2070, training loss: 452.66607666015625 = 0.07175389677286148 + 50.0 * 9.051886558532715
Epoch 2070, val loss: 0.553534209728241
Epoch 2080, training loss: 452.6353759765625 = 0.0707072839140892 + 50.0 * 9.05129337310791
Epoch 2080, val loss: 0.554840624332428
Epoch 2090, training loss: 452.61578369140625 = 0.06965736299753189 + 50.0 * 9.050922393798828
Epoch 2090, val loss: 0.557523787021637
Epoch 2100, training loss: 452.6075134277344 = 0.06864675879478455 + 50.0 * 9.050777435302734
Epoch 2100, val loss: 0.5592624545097351
Epoch 2110, training loss: 452.6891174316406 = 0.0676637664437294 + 50.0 * 9.05242919921875
Epoch 2110, val loss: 0.5618911385536194
Epoch 2120, training loss: 452.6224670410156 = 0.06672301143407822 + 50.0 * 9.051115036010742
Epoch 2120, val loss: 0.566804051399231
Epoch 2130, training loss: 452.61517333984375 = 0.06570123881101608 + 50.0 * 9.050989151000977
Epoch 2130, val loss: 0.567773163318634
Epoch 2140, training loss: 452.65765380859375 = 0.06494896113872528 + 50.0 * 9.051854133605957
Epoch 2140, val loss: 0.5723354816436768
Epoch 2150, training loss: 452.5796813964844 = 0.06382808089256287 + 50.0 * 9.05031681060791
Epoch 2150, val loss: 0.57058185338974
Epoch 2160, training loss: 452.56842041015625 = 0.06285864114761353 + 50.0 * 9.050110816955566
Epoch 2160, val loss: 0.5739224553108215
Epoch 2170, training loss: 452.6449890136719 = 0.061964575201272964 + 50.0 * 9.051660537719727
Epoch 2170, val loss: 0.5768921971321106
Epoch 2180, training loss: 452.56134033203125 = 0.06104760244488716 + 50.0 * 9.050005912780762
Epoch 2180, val loss: 0.5787946581840515
Epoch 2190, training loss: 452.5503234863281 = 0.06016514077782631 + 50.0 * 9.049803733825684
Epoch 2190, val loss: 0.5821738243103027
Epoch 2200, training loss: 452.6095886230469 = 0.05932113528251648 + 50.0 * 9.051005363464355
Epoch 2200, val loss: 0.5845954418182373
Epoch 2210, training loss: 452.55792236328125 = 0.05843155458569527 + 50.0 * 9.049989700317383
Epoch 2210, val loss: 0.5859500169754028
Epoch 2220, training loss: 452.5647888183594 = 0.05757253244519234 + 50.0 * 9.05014419555664
Epoch 2220, val loss: 0.5882055759429932
Epoch 2230, training loss: 452.5149841308594 = 0.05672568082809448 + 50.0 * 9.049164772033691
Epoch 2230, val loss: 0.5907885432243347
Epoch 2240, training loss: 452.5546569824219 = 0.055933628231287 + 50.0 * 9.04997444152832
Epoch 2240, val loss: 0.59217768907547
Epoch 2250, training loss: 452.5137939453125 = 0.05512642115354538 + 50.0 * 9.049173355102539
Epoch 2250, val loss: 0.5945297479629517
Epoch 2260, training loss: 452.6366271972656 = 0.054467834532260895 + 50.0 * 9.051643371582031
Epoch 2260, val loss: 0.5962264537811279
Epoch 2270, training loss: 452.500732421875 = 0.05355459079146385 + 50.0 * 9.048943519592285
Epoch 2270, val loss: 0.6015605330467224
Epoch 2280, training loss: 452.4653015136719 = 0.05273694545030594 + 50.0 * 9.048251152038574
Epoch 2280, val loss: 0.6024443507194519
Epoch 2290, training loss: 452.54644775390625 = 0.051999516785144806 + 50.0 * 9.049888610839844
Epoch 2290, val loss: 0.6046047806739807
Epoch 2300, training loss: 452.4386291503906 = 0.05123230814933777 + 50.0 * 9.047747611999512
Epoch 2300, val loss: 0.6081476807594299
Epoch 2310, training loss: 452.4370422363281 = 0.05049002543091774 + 50.0 * 9.047731399536133
Epoch 2310, val loss: 0.6093963384628296
Epoch 2320, training loss: 452.5802917480469 = 0.04979294165968895 + 50.0 * 9.050609588623047
Epoch 2320, val loss: 0.6125401854515076
Epoch 2330, training loss: 452.45001220703125 = 0.049056537449359894 + 50.0 * 9.048019409179688
Epoch 2330, val loss: 0.6147226691246033
Epoch 2340, training loss: 452.4188537597656 = 0.04834204539656639 + 50.0 * 9.047410011291504
Epoch 2340, val loss: 0.6169288754463196
Epoch 2350, training loss: 452.4292907714844 = 0.04765530303120613 + 50.0 * 9.047632217407227
Epoch 2350, val loss: 0.6192517876625061
Epoch 2360, training loss: 452.5552978515625 = 0.047131508588790894 + 50.0 * 9.050163269042969
Epoch 2360, val loss: 0.6238830089569092
Epoch 2370, training loss: 452.3928527832031 = 0.046308763325214386 + 50.0 * 9.046931266784668
Epoch 2370, val loss: 0.6238412857055664
Epoch 2380, training loss: 452.397216796875 = 0.04565510153770447 + 50.0 * 9.04703140258789
Epoch 2380, val loss: 0.6258640289306641
Epoch 2390, training loss: 452.3810119628906 = 0.04499012231826782 + 50.0 * 9.046720504760742
Epoch 2390, val loss: 0.6285204887390137
Epoch 2400, training loss: 452.54833984375 = 0.04438546299934387 + 50.0 * 9.050079345703125
Epoch 2400, val loss: 0.6314868330955505
Epoch 2410, training loss: 452.44158935546875 = 0.04373636841773987 + 50.0 * 9.047957420349121
Epoch 2410, val loss: 0.6328737735748291
Epoch 2420, training loss: 452.3934631347656 = 0.04311085864901543 + 50.0 * 9.047006607055664
Epoch 2420, val loss: 0.6350371241569519
Epoch 2430, training loss: 452.3548889160156 = 0.04250429943203926 + 50.0 * 9.046247482299805
Epoch 2430, val loss: 0.6366880536079407
Epoch 2440, training loss: 452.4292907714844 = 0.042055483907461166 + 50.0 * 9.047744750976562
Epoch 2440, val loss: 0.6374465227127075
Epoch 2450, training loss: 452.36309814453125 = 0.041335247457027435 + 50.0 * 9.046435356140137
Epoch 2450, val loss: 0.6445110440254211
Epoch 2460, training loss: 452.357177734375 = 0.040731385350227356 + 50.0 * 9.0463285446167
Epoch 2460, val loss: 0.6439782977104187
Epoch 2470, training loss: 452.333740234375 = 0.04017280414700508 + 50.0 * 9.04587173461914
Epoch 2470, val loss: 0.6479394435882568
Epoch 2480, training loss: 452.4200134277344 = 0.03963121399283409 + 50.0 * 9.047607421875
Epoch 2480, val loss: 0.6489851474761963
Epoch 2490, training loss: 452.32440185546875 = 0.03906073048710823 + 50.0 * 9.045706748962402
Epoch 2490, val loss: 0.6517446041107178
Epoch 2500, training loss: 452.29620361328125 = 0.038516853004693985 + 50.0 * 9.045153617858887
Epoch 2500, val loss: 0.6531625390052795
Epoch 2510, training loss: 452.3334045410156 = 0.03800911456346512 + 50.0 * 9.045907974243164
Epoch 2510, val loss: 0.6545639634132385
Epoch 2520, training loss: 452.40191650390625 = 0.03749820590019226 + 50.0 * 9.047287940979004
Epoch 2520, val loss: 0.6581115126609802
Epoch 2530, training loss: 452.3081359863281 = 0.036963388323783875 + 50.0 * 9.04542350769043
Epoch 2530, val loss: 0.6602239012718201
Epoch 2540, training loss: 452.2784729003906 = 0.03644614294171333 + 50.0 * 9.044840812683105
Epoch 2540, val loss: 0.6630321741104126
Epoch 2550, training loss: 452.2698974609375 = 0.03596214950084686 + 50.0 * 9.044678688049316
Epoch 2550, val loss: 0.6636893153190613
Epoch 2560, training loss: 452.32598876953125 = 0.035478461533784866 + 50.0 * 9.045809745788574
Epoch 2560, val loss: 0.6669601798057556
Epoch 2570, training loss: 452.30059814453125 = 0.03500781208276749 + 50.0 * 9.04531192779541
Epoch 2570, val loss: 0.6688310503959656
Epoch 2580, training loss: 452.25897216796875 = 0.03451116010546684 + 50.0 * 9.044488906860352
Epoch 2580, val loss: 0.670926034450531
Epoch 2590, training loss: 452.2657470703125 = 0.03406372666358948 + 50.0 * 9.044633865356445
Epoch 2590, val loss: 0.673101544380188
Epoch 2600, training loss: 452.3223571777344 = 0.03368638828396797 + 50.0 * 9.04577350616455
Epoch 2600, val loss: 0.6739574670791626
Epoch 2610, training loss: 452.2712707519531 = 0.03316127881407738 + 50.0 * 9.044761657714844
Epoch 2610, val loss: 0.6796370148658752
Epoch 2620, training loss: 452.2413635253906 = 0.03270874544978142 + 50.0 * 9.044173240661621
Epoch 2620, val loss: 0.6795986890792847
Epoch 2630, training loss: 452.2633361816406 = 0.03227212280035019 + 50.0 * 9.044621467590332
Epoch 2630, val loss: 0.6829169988632202
Epoch 2640, training loss: 452.27105712890625 = 0.03189760074019432 + 50.0 * 9.044783592224121
Epoch 2640, val loss: 0.6865503191947937
Epoch 2650, training loss: 452.27032470703125 = 0.031442996114492416 + 50.0 * 9.044777870178223
Epoch 2650, val loss: 0.6859146356582642
Epoch 2660, training loss: 452.21820068359375 = 0.031005676835775375 + 50.0 * 9.043744087219238
Epoch 2660, val loss: 0.6894032955169678
Epoch 2670, training loss: 452.20855712890625 = 0.030608655884861946 + 50.0 * 9.043559074401855
Epoch 2670, val loss: 0.6901119351387024
Epoch 2680, training loss: 452.40679931640625 = 0.030321812257170677 + 50.0 * 9.047529220581055
Epoch 2680, val loss: 0.691909670829773
Epoch 2690, training loss: 452.2076110839844 = 0.02982630394399166 + 50.0 * 9.043556213378906
Epoch 2690, val loss: 0.6956419348716736
Epoch 2700, training loss: 452.1678466796875 = 0.02942327968776226 + 50.0 * 9.042768478393555
Epoch 2700, val loss: 0.6985884308815002
Epoch 2710, training loss: 452.174072265625 = 0.029063330963253975 + 50.0 * 9.042900085449219
Epoch 2710, val loss: 0.6984311938285828
Epoch 2720, training loss: 452.50665283203125 = 0.028781278058886528 + 50.0 * 9.04955768585205
Epoch 2720, val loss: 0.7009977102279663
Epoch 2730, training loss: 452.21502685546875 = 0.028341194614768028 + 50.0 * 9.043733596801758
Epoch 2730, val loss: 0.7050397992134094
Epoch 2740, training loss: 452.1505432128906 = 0.027953721582889557 + 50.0 * 9.042451858520508
Epoch 2740, val loss: 0.7058172225952148
Epoch 2750, training loss: 452.14874267578125 = 0.027599677443504333 + 50.0 * 9.042423248291016
Epoch 2750, val loss: 0.7079795598983765
Epoch 2760, training loss: 452.1789245605469 = 0.027274353429675102 + 50.0 * 9.0430326461792
Epoch 2760, val loss: 0.7110480666160583
Epoch 2770, training loss: 452.1885986328125 = 0.026930907741189003 + 50.0 * 9.043233871459961
Epoch 2770, val loss: 0.7125794291496277
Epoch 2780, training loss: 452.130859375 = 0.026578055694699287 + 50.0 * 9.042085647583008
Epoch 2780, val loss: 0.7152382731437683
Epoch 2790, training loss: 452.1227111816406 = 0.02625265158712864 + 50.0 * 9.041929244995117
Epoch 2790, val loss: 0.7171692848205566
Epoch 2800, training loss: 452.2617492675781 = 0.02598462626338005 + 50.0 * 9.044715881347656
Epoch 2800, val loss: 0.7206134796142578
Epoch 2810, training loss: 452.1883239746094 = 0.025642545893788338 + 50.0 * 9.043253898620605
Epoch 2810, val loss: 0.7189533710479736
Epoch 2820, training loss: 452.150146484375 = 0.0252982284873724 + 50.0 * 9.042496681213379
Epoch 2820, val loss: 0.7232188582420349
Epoch 2830, training loss: 452.1080627441406 = 0.02498035691678524 + 50.0 * 9.041661262512207
Epoch 2830, val loss: 0.7246159315109253
Epoch 2840, training loss: 452.1214904785156 = 0.024682633578777313 + 50.0 * 9.041935920715332
Epoch 2840, val loss: 0.7262954115867615
Epoch 2850, training loss: 452.2348327636719 = 0.024456815794110298 + 50.0 * 9.044207572937012
Epoch 2850, val loss: 0.727303147315979
Epoch 2860, training loss: 452.1264953613281 = 0.024099867790937424 + 50.0 * 9.042047500610352
Epoch 2860, val loss: 0.7310980558395386
Epoch 2870, training loss: 452.0967102050781 = 0.023804863914847374 + 50.0 * 9.041458129882812
Epoch 2870, val loss: 0.7328625917434692
Epoch 2880, training loss: 452.137451171875 = 0.02354286052286625 + 50.0 * 9.042278289794922
Epoch 2880, val loss: 0.7353120446205139
Epoch 2890, training loss: 452.094970703125 = 0.02325569838285446 + 50.0 * 9.041434288024902
Epoch 2890, val loss: 0.7373619079589844
Epoch 2900, training loss: 452.117431640625 = 0.022989053279161453 + 50.0 * 9.041889190673828
Epoch 2900, val loss: 0.7372333407402039
Epoch 2910, training loss: 452.0828857421875 = 0.022699683904647827 + 50.0 * 9.041203498840332
Epoch 2910, val loss: 0.7407447695732117
Epoch 2920, training loss: 452.0879211425781 = 0.022447172552347183 + 50.0 * 9.041309356689453
Epoch 2920, val loss: 0.7430247664451599
Epoch 2930, training loss: 452.1360168457031 = 0.022235985845327377 + 50.0 * 9.042275428771973
Epoch 2930, val loss: 0.7456240057945251
Epoch 2940, training loss: 452.1642761230469 = 0.02195732854306698 + 50.0 * 9.0428466796875
Epoch 2940, val loss: 0.747870922088623
Epoch 2950, training loss: 452.09124755859375 = 0.021666988730430603 + 50.0 * 9.041391372680664
Epoch 2950, val loss: 0.746992826461792
Epoch 2960, training loss: 452.04364013671875 = 0.021412445232272148 + 50.0 * 9.040444374084473
Epoch 2960, val loss: 0.7503906488418579
Epoch 2970, training loss: 452.0299377441406 = 0.021164316684007645 + 50.0 * 9.040175437927246
Epoch 2970, val loss: 0.7512232661247253
Epoch 2980, training loss: 452.0560302734375 = 0.020930306985974312 + 50.0 * 9.040701866149902
Epoch 2980, val loss: 0.7536619901657104
Epoch 2990, training loss: 452.12554931640625 = 0.020705431699752808 + 50.0 * 9.042097091674805
Epoch 2990, val loss: 0.7555046081542969
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.7246
Flip ASR: 0.6557/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.26 GiB already allocated; 247.69 MiB free; 5.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 769.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 661.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 727.69 MiB free; 3.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0347900390625 = 1.0870712995529175 + 50.0 * 10.358953475952148
Epoch 0, val loss: 1.0872570276260376
Epoch 10, training loss: 518.78076171875 = 1.0781817436218262 + 50.0 * 10.35405158996582
Epoch 10, val loss: 1.0779834985733032
Epoch 20, training loss: 514.138671875 = 1.0657155513763428 + 50.0 * 10.261458396911621
Epoch 20, val loss: 1.0651917457580566
Epoch 30, training loss: 480.9955749511719 = 1.0516277551651 + 50.0 * 9.598878860473633
Epoch 30, val loss: 1.050855278968811
Epoch 40, training loss: 474.5467529296875 = 1.0386968851089478 + 50.0 * 9.470161437988281
Epoch 40, val loss: 1.0380769968032837
Epoch 50, training loss: 469.68255615234375 = 1.0272634029388428 + 50.0 * 9.373106002807617
Epoch 50, val loss: 1.0268937349319458
Epoch 60, training loss: 466.9414367675781 = 1.0160062313079834 + 50.0 * 9.318509101867676
Epoch 60, val loss: 1.0157356262207031
Epoch 70, training loss: 464.1500549316406 = 1.0064233541488647 + 50.0 * 9.262872695922852
Epoch 70, val loss: 1.0063273906707764
Epoch 80, training loss: 462.6579284667969 = 0.9982080459594727 + 50.0 * 9.233194351196289
Epoch 80, val loss: 0.9981358051300049
Epoch 90, training loss: 461.03802490234375 = 0.9895818829536438 + 50.0 * 9.200968742370605
Epoch 90, val loss: 0.9897242784500122
Epoch 100, training loss: 459.8966369628906 = 0.9808343052864075 + 50.0 * 9.178316116333008
Epoch 100, val loss: 0.981232762336731
Epoch 110, training loss: 458.9254150390625 = 0.9728975296020508 + 50.0 * 9.159049987792969
Epoch 110, val loss: 0.9736891388893127
Epoch 120, training loss: 458.19915771484375 = 0.9655396938323975 + 50.0 * 9.144672393798828
Epoch 120, val loss: 0.9665271043777466
Epoch 130, training loss: 457.5821838378906 = 0.9565334320068359 + 50.0 * 9.132513046264648
Epoch 130, val loss: 0.9576802253723145
Epoch 140, training loss: 457.1502990722656 = 0.9454801678657532 + 50.0 * 9.124096870422363
Epoch 140, val loss: 0.9468615055084229
Epoch 150, training loss: 456.813720703125 = 0.9329752326011658 + 50.0 * 9.11761474609375
Epoch 150, val loss: 0.9347167015075684
Epoch 160, training loss: 456.5660705566406 = 0.9197939038276672 + 50.0 * 9.11292552947998
Epoch 160, val loss: 0.9219870567321777
Epoch 170, training loss: 456.2664489746094 = 0.9064768552780151 + 50.0 * 9.107199668884277
Epoch 170, val loss: 0.9091310501098633
Epoch 180, training loss: 456.1138000488281 = 0.8928514719009399 + 50.0 * 9.104418754577637
Epoch 180, val loss: 0.8959174156188965
Epoch 190, training loss: 455.8489990234375 = 0.8780232071876526 + 50.0 * 9.099419593811035
Epoch 190, val loss: 0.881582498550415
Epoch 200, training loss: 455.6761169433594 = 0.8624008893966675 + 50.0 * 9.096274375915527
Epoch 200, val loss: 0.8664900660514832
Epoch 210, training loss: 455.6029052734375 = 0.8461549878120422 + 50.0 * 9.095134735107422
Epoch 210, val loss: 0.8507996201515198
Epoch 220, training loss: 455.4217224121094 = 0.8289151787757874 + 50.0 * 9.091856002807617
Epoch 220, val loss: 0.8341712355613708
Epoch 230, training loss: 455.2840881347656 = 0.8112553358078003 + 50.0 * 9.089456558227539
Epoch 230, val loss: 0.8172248601913452
Epoch 240, training loss: 455.1379699707031 = 0.7934252023696899 + 50.0 * 9.086891174316406
Epoch 240, val loss: 0.8001407384872437
Epoch 250, training loss: 455.027587890625 = 0.775226891040802 + 50.0 * 9.085046768188477
Epoch 250, val loss: 0.782715916633606
Epoch 260, training loss: 455.0088195800781 = 0.7564758062362671 + 50.0 * 9.085046768188477
Epoch 260, val loss: 0.7647377848625183
Epoch 270, training loss: 454.8484191894531 = 0.7372748851776123 + 50.0 * 9.082222938537598
Epoch 270, val loss: 0.7464736104011536
Epoch 280, training loss: 454.70794677734375 = 0.7183688282966614 + 50.0 * 9.079791069030762
Epoch 280, val loss: 0.7285247445106506
Epoch 290, training loss: 454.6156921386719 = 0.6995816826820374 + 50.0 * 9.078322410583496
Epoch 290, val loss: 0.7107283473014832
Epoch 300, training loss: 454.78619384765625 = 0.6807515025138855 + 50.0 * 9.082108497619629
Epoch 300, val loss: 0.6928439736366272
Epoch 310, training loss: 454.52777099609375 = 0.6614432334899902 + 50.0 * 9.077326774597168
Epoch 310, val loss: 0.6746465563774109
Epoch 320, training loss: 454.3462219238281 = 0.6430978178977966 + 50.0 * 9.07406234741211
Epoch 320, val loss: 0.6575794219970703
Epoch 330, training loss: 454.2699279785156 = 0.6256216168403625 + 50.0 * 9.07288646697998
Epoch 330, val loss: 0.6413102149963379
Epoch 340, training loss: 454.2508850097656 = 0.6085205674171448 + 50.0 * 9.072847366333008
Epoch 340, val loss: 0.6254022717475891
Epoch 350, training loss: 454.1622619628906 = 0.5917099714279175 + 50.0 * 9.0714111328125
Epoch 350, val loss: 0.6099292635917664
Epoch 360, training loss: 454.0672607421875 = 0.5755200982093811 + 50.0 * 9.06983470916748
Epoch 360, val loss: 0.5950620770454407
Epoch 370, training loss: 453.9744873046875 = 0.5601511001586914 + 50.0 * 9.068286895751953
Epoch 370, val loss: 0.5810480117797852
Epoch 380, training loss: 453.99725341796875 = 0.5456621646881104 + 50.0 * 9.069031715393066
Epoch 380, val loss: 0.5678862929344177
Epoch 390, training loss: 453.8818664550781 = 0.531742513179779 + 50.0 * 9.067002296447754
Epoch 390, val loss: 0.555394172668457
Epoch 400, training loss: 453.7869567871094 = 0.5187065005302429 + 50.0 * 9.065364837646484
Epoch 400, val loss: 0.5437912940979004
Epoch 410, training loss: 453.8465270996094 = 0.5063912272453308 + 50.0 * 9.066802978515625
Epoch 410, val loss: 0.5328841209411621
Epoch 420, training loss: 453.7032775878906 = 0.4946000576019287 + 50.0 * 9.064173698425293
Epoch 420, val loss: 0.5224946737289429
Epoch 430, training loss: 453.7016906738281 = 0.483673632144928 + 50.0 * 9.064360618591309
Epoch 430, val loss: 0.512992262840271
Epoch 440, training loss: 453.59991455078125 = 0.47343847155570984 + 50.0 * 9.062529563903809
Epoch 440, val loss: 0.5041663646697998
Epoch 450, training loss: 453.5331115722656 = 0.4639435410499573 + 50.0 * 9.061383247375488
Epoch 450, val loss: 0.49610334634780884
Epoch 460, training loss: 453.5263366699219 = 0.45501580834388733 + 50.0 * 9.061426162719727
Epoch 460, val loss: 0.48858946561813354
Epoch 470, training loss: 453.50634765625 = 0.44643735885620117 + 50.0 * 9.061198234558105
Epoch 470, val loss: 0.4812605381011963
Epoch 480, training loss: 453.42828369140625 = 0.43846848607063293 + 50.0 * 9.059796333312988
Epoch 480, val loss: 0.47478508949279785
Epoch 490, training loss: 453.3931884765625 = 0.43120065331459045 + 50.0 * 9.059239387512207
Epoch 490, val loss: 0.46899712085723877
Epoch 500, training loss: 453.35260009765625 = 0.424340158700943 + 50.0 * 9.058565139770508
Epoch 500, val loss: 0.46339234709739685
Epoch 510, training loss: 453.346923828125 = 0.4179005026817322 + 50.0 * 9.05858039855957
Epoch 510, val loss: 0.45817023515701294
Epoch 520, training loss: 453.2789611816406 = 0.41182902455329895 + 50.0 * 9.057342529296875
Epoch 520, val loss: 0.45358771085739136
Epoch 530, training loss: 453.2723388671875 = 0.406145304441452 + 50.0 * 9.057323455810547
Epoch 530, val loss: 0.449070006608963
Epoch 540, training loss: 453.2452697753906 = 0.40070027112960815 + 50.0 * 9.056891441345215
Epoch 540, val loss: 0.44495052099227905
Epoch 550, training loss: 453.167724609375 = 0.3956407904624939 + 50.0 * 9.055441856384277
Epoch 550, val loss: 0.44113439321517944
Epoch 560, training loss: 453.1289367675781 = 0.3909187316894531 + 50.0 * 9.054759979248047
Epoch 560, val loss: 0.4376599192619324
Epoch 570, training loss: 453.1968078613281 = 0.3863866925239563 + 50.0 * 9.056208610534668
Epoch 570, val loss: 0.43440574407577515
Epoch 580, training loss: 453.113525390625 = 0.38204848766326904 + 50.0 * 9.0546293258667
Epoch 580, val loss: 0.43112558126449585
Epoch 590, training loss: 453.0283203125 = 0.37804093956947327 + 50.0 * 9.05300521850586
Epoch 590, val loss: 0.4284592270851135
Epoch 600, training loss: 453.00946044921875 = 0.3742813467979431 + 50.0 * 9.052703857421875
Epoch 600, val loss: 0.425892174243927
Epoch 610, training loss: 453.0357666015625 = 0.37066546082496643 + 50.0 * 9.053301811218262
Epoch 610, val loss: 0.42335695028305054
Epoch 620, training loss: 453.00750732421875 = 0.3671349585056305 + 50.0 * 9.052807807922363
Epoch 620, val loss: 0.4209754765033722
Epoch 630, training loss: 452.9715576171875 = 0.3637825846672058 + 50.0 * 9.052155494689941
Epoch 630, val loss: 0.4188861548900604
Epoch 640, training loss: 452.8904724121094 = 0.36062800884246826 + 50.0 * 9.050597190856934
Epoch 640, val loss: 0.416852205991745
Epoch 650, training loss: 452.8624572753906 = 0.3576376438140869 + 50.0 * 9.05009651184082
Epoch 650, val loss: 0.41486066579818726
Epoch 660, training loss: 453.05279541015625 = 0.35471871495246887 + 50.0 * 9.053961753845215
Epoch 660, val loss: 0.41290482878685
Epoch 670, training loss: 452.8292236328125 = 0.35181987285614014 + 50.0 * 9.049548149108887
Epoch 670, val loss: 0.4114486277103424
Epoch 680, training loss: 452.81475830078125 = 0.3491820693016052 + 50.0 * 9.049311637878418
Epoch 680, val loss: 0.40981218218803406
Epoch 690, training loss: 452.7545166015625 = 0.3466731607913971 + 50.0 * 9.04815673828125
Epoch 690, val loss: 0.40838858485221863
Epoch 700, training loss: 452.73199462890625 = 0.34422409534454346 + 50.0 * 9.047755241394043
Epoch 700, val loss: 0.4069380462169647
Epoch 710, training loss: 452.91583251953125 = 0.34183546900749207 + 50.0 * 9.051480293273926
Epoch 710, val loss: 0.4053665101528168
Epoch 720, training loss: 452.7661437988281 = 0.3393884599208832 + 50.0 * 9.048535346984863
Epoch 720, val loss: 0.4043761193752289
Epoch 730, training loss: 452.701416015625 = 0.33716368675231934 + 50.0 * 9.047285079956055
Epoch 730, val loss: 0.4031996726989746
Epoch 740, training loss: 452.66192626953125 = 0.3350352346897125 + 50.0 * 9.046538352966309
Epoch 740, val loss: 0.4021194577217102
Epoch 750, training loss: 452.8023986816406 = 0.3329316973686218 + 50.0 * 9.049388885498047
Epoch 750, val loss: 0.4010547995567322
Epoch 760, training loss: 452.6279602050781 = 0.33084630966186523 + 50.0 * 9.045942306518555
Epoch 760, val loss: 0.399984210729599
Epoch 770, training loss: 452.5729675292969 = 0.3288855254650116 + 50.0 * 9.044881820678711
Epoch 770, val loss: 0.3989907503128052
Epoch 780, training loss: 452.5611877441406 = 0.32696056365966797 + 50.0 * 9.044684410095215
Epoch 780, val loss: 0.3980708122253418
Epoch 790, training loss: 452.68206787109375 = 0.3250260055065155 + 50.0 * 9.047141075134277
Epoch 790, val loss: 0.39718976616859436
Epoch 800, training loss: 452.5799560546875 = 0.3231327533721924 + 50.0 * 9.045136451721191
Epoch 800, val loss: 0.3962700366973877
Epoch 810, training loss: 452.51104736328125 = 0.3213631808757782 + 50.0 * 9.043793678283691
Epoch 810, val loss: 0.3955320715904236
Epoch 820, training loss: 452.47137451171875 = 0.3196251690387726 + 50.0 * 9.043035507202148
Epoch 820, val loss: 0.3947882652282715
Epoch 830, training loss: 452.465576171875 = 0.3179054260253906 + 50.0 * 9.042953491210938
Epoch 830, val loss: 0.3940257728099823
Epoch 840, training loss: 452.6234130859375 = 0.31618186831474304 + 50.0 * 9.046144485473633
Epoch 840, val loss: 0.393210768699646
Epoch 850, training loss: 452.46270751953125 = 0.3144662380218506 + 50.0 * 9.042964935302734
Epoch 850, val loss: 0.39267224073410034
Epoch 860, training loss: 452.420654296875 = 0.31284278631210327 + 50.0 * 9.042156219482422
Epoch 860, val loss: 0.3921242654323578
Epoch 870, training loss: 452.39044189453125 = 0.3112604022026062 + 50.0 * 9.041584014892578
Epoch 870, val loss: 0.3914797008037567
Epoch 880, training loss: 452.3866882324219 = 0.3096913695335388 + 50.0 * 9.041540145874023
Epoch 880, val loss: 0.3908129930496216
Epoch 890, training loss: 452.469482421875 = 0.3081221878528595 + 50.0 * 9.043227195739746
Epoch 890, val loss: 0.3902639150619507
Epoch 900, training loss: 452.3771057128906 = 0.30657896399497986 + 50.0 * 9.041410446166992
Epoch 900, val loss: 0.3898160457611084
Epoch 910, training loss: 452.34356689453125 = 0.30508944392204285 + 50.0 * 9.040769577026367
Epoch 910, val loss: 0.3893010914325714
Epoch 920, training loss: 452.3578186035156 = 0.30360645055770874 + 50.0 * 9.041084289550781
Epoch 920, val loss: 0.38878297805786133
Epoch 930, training loss: 452.31646728515625 = 0.302145779132843 + 50.0 * 9.04028606414795
Epoch 930, val loss: 0.3883972465991974
Epoch 940, training loss: 452.2853088378906 = 0.3007195293903351 + 50.0 * 9.039691925048828
Epoch 940, val loss: 0.38796016573905945
Epoch 950, training loss: 452.29107666015625 = 0.29930081963539124 + 50.0 * 9.039835929870605
Epoch 950, val loss: 0.38745877146720886
Epoch 960, training loss: 452.27392578125 = 0.2979077398777008 + 50.0 * 9.039520263671875
Epoch 960, val loss: 0.38703444600105286
Epoch 970, training loss: 452.2349548339844 = 0.2965161204338074 + 50.0 * 9.038768768310547
Epoch 970, val loss: 0.3867168426513672
Epoch 980, training loss: 452.232666015625 = 0.29516005516052246 + 50.0 * 9.038749694824219
Epoch 980, val loss: 0.386431485414505
Epoch 990, training loss: 452.2829284667969 = 0.29380592703819275 + 50.0 * 9.039782524108887
Epoch 990, val loss: 0.385993093252182
Epoch 1000, training loss: 452.2259216308594 = 0.29246586561203003 + 50.0 * 9.038668632507324
Epoch 1000, val loss: 0.38558435440063477
Epoch 1010, training loss: 452.2193298339844 = 0.29114991426467896 + 50.0 * 9.03856372833252
Epoch 1010, val loss: 0.3853189945220947
Epoch 1020, training loss: 452.154296875 = 0.289860337972641 + 50.0 * 9.037288665771484
Epoch 1020, val loss: 0.3850637972354889
Epoch 1030, training loss: 452.14794921875 = 0.2885856628417969 + 50.0 * 9.037187576293945
Epoch 1030, val loss: 0.38480129837989807
Epoch 1040, training loss: 452.203857421875 = 0.28730425238609314 + 50.0 * 9.038331031799316
Epoch 1040, val loss: 0.3845027983188629
Epoch 1050, training loss: 452.1640625 = 0.28603801131248474 + 50.0 * 9.03756046295166
Epoch 1050, val loss: 0.3843391239643097
Epoch 1060, training loss: 452.09075927734375 = 0.28478679060935974 + 50.0 * 9.03611946105957
Epoch 1060, val loss: 0.3841050863265991
Epoch 1070, training loss: 452.0740051269531 = 0.2835567593574524 + 50.0 * 9.035808563232422
Epoch 1070, val loss: 0.3837195336818695
Epoch 1080, training loss: 452.0644836425781 = 0.2823348641395569 + 50.0 * 9.035642623901367
Epoch 1080, val loss: 0.3835326135158539
Epoch 1090, training loss: 452.1829528808594 = 0.28109651803970337 + 50.0 * 9.038037300109863
Epoch 1090, val loss: 0.38339558243751526
Epoch 1100, training loss: 452.0196228027344 = 0.2798863351345062 + 50.0 * 9.034794807434082
Epoch 1100, val loss: 0.38317951560020447
Epoch 1110, training loss: 452.0111999511719 = 0.278708279132843 + 50.0 * 9.034649848937988
Epoch 1110, val loss: 0.3831323981285095
Epoch 1120, training loss: 451.98797607421875 = 0.27751755714416504 + 50.0 * 9.034209251403809
Epoch 1120, val loss: 0.3828839063644409
Epoch 1130, training loss: 452.0814514160156 = 0.2763477563858032 + 50.0 * 9.036102294921875
Epoch 1130, val loss: 0.38283172249794006
Epoch 1140, training loss: 452.05230712890625 = 0.2751426696777344 + 50.0 * 9.035543441772461
Epoch 1140, val loss: 0.3826441466808319
Epoch 1150, training loss: 451.9691162109375 = 0.2740022540092468 + 50.0 * 9.033902168273926
Epoch 1150, val loss: 0.3824574053287506
Epoch 1160, training loss: 451.9700012207031 = 0.2728702127933502 + 50.0 * 9.033943176269531
Epoch 1160, val loss: 0.3824050724506378
Epoch 1170, training loss: 451.9656982421875 = 0.2717328667640686 + 50.0 * 9.033879280090332
Epoch 1170, val loss: 0.38233256340026855
Epoch 1180, training loss: 451.9248046875 = 0.2705971896648407 + 50.0 * 9.03308391571045
Epoch 1180, val loss: 0.382122278213501
Epoch 1190, training loss: 451.95648193359375 = 0.269471675157547 + 50.0 * 9.033740043640137
Epoch 1190, val loss: 0.3819085359573364
Epoch 1200, training loss: 451.9961242675781 = 0.2683537006378174 + 50.0 * 9.034555435180664
Epoch 1200, val loss: 0.3820587396621704
Epoch 1210, training loss: 451.90289306640625 = 0.2672378122806549 + 50.0 * 9.032712936401367
Epoch 1210, val loss: 0.38192883133888245
Epoch 1220, training loss: 451.8792724609375 = 0.2661421000957489 + 50.0 * 9.032262802124023
Epoch 1220, val loss: 0.3820159435272217
Epoch 1230, training loss: 452.0262756347656 = 0.2650487720966339 + 50.0 * 9.035224914550781
Epoch 1230, val loss: 0.3821426331996918
Epoch 1240, training loss: 451.90264892578125 = 0.263945072889328 + 50.0 * 9.032773971557617
Epoch 1240, val loss: 0.38203614950180054
Epoch 1250, training loss: 451.8230895996094 = 0.26287657022476196 + 50.0 * 9.031204223632812
Epoch 1250, val loss: 0.38208168745040894
Epoch 1260, training loss: 451.8163146972656 = 0.26180508732795715 + 50.0 * 9.031089782714844
Epoch 1260, val loss: 0.38187068700790405
Epoch 1270, training loss: 451.9610595703125 = 0.2607341408729553 + 50.0 * 9.034006118774414
Epoch 1270, val loss: 0.3819262683391571
Epoch 1280, training loss: 451.8377990722656 = 0.2596740424633026 + 50.0 * 9.031562805175781
Epoch 1280, val loss: 0.3820285201072693
Epoch 1290, training loss: 451.8014831542969 = 0.2586223781108856 + 50.0 * 9.03085708618164
Epoch 1290, val loss: 0.3821139931678772
Epoch 1300, training loss: 451.8167419433594 = 0.25757721066474915 + 50.0 * 9.031183242797852
Epoch 1300, val loss: 0.3821963965892792
Epoch 1310, training loss: 451.7696838378906 = 0.2565307319164276 + 50.0 * 9.03026294708252
Epoch 1310, val loss: 0.3817458152770996
Epoch 1320, training loss: 451.7974853515625 = 0.2554943561553955 + 50.0 * 9.030839920043945
Epoch 1320, val loss: 0.38212016224861145
Epoch 1330, training loss: 451.762939453125 = 0.25446948409080505 + 50.0 * 9.030169486999512
Epoch 1330, val loss: 0.38220110535621643
Epoch 1340, training loss: 451.72259521484375 = 0.2534516453742981 + 50.0 * 9.029382705688477
Epoch 1340, val loss: 0.38243046402931213
Epoch 1350, training loss: 451.7015380859375 = 0.2524358332157135 + 50.0 * 9.028982162475586
Epoch 1350, val loss: 0.3822336196899414
Epoch 1360, training loss: 451.69964599609375 = 0.25142648816108704 + 50.0 * 9.028964042663574
Epoch 1360, val loss: 0.38237497210502625
Epoch 1370, training loss: 451.79766845703125 = 0.25041651725769043 + 50.0 * 9.03094482421875
Epoch 1370, val loss: 0.3822481632232666
Epoch 1380, training loss: 451.75103759765625 = 0.24941356480121613 + 50.0 * 9.03003215789795
Epoch 1380, val loss: 0.38287353515625
Epoch 1390, training loss: 451.7281494140625 = 0.24841296672821045 + 50.0 * 9.029594421386719
Epoch 1390, val loss: 0.38260552287101746
Epoch 1400, training loss: 451.67852783203125 = 0.24742791056632996 + 50.0 * 9.028621673583984
Epoch 1400, val loss: 0.38297131657600403
Epoch 1410, training loss: 451.68426513671875 = 0.24644958972930908 + 50.0 * 9.028756141662598
Epoch 1410, val loss: 0.38303422927856445
Epoch 1420, training loss: 451.62408447265625 = 0.24546684324741364 + 50.0 * 9.027572631835938
Epoch 1420, val loss: 0.3832484781742096
Epoch 1430, training loss: 451.64593505859375 = 0.24449284374713898 + 50.0 * 9.02802848815918
Epoch 1430, val loss: 0.38353508710861206
Epoch 1440, training loss: 451.718505859375 = 0.24351295828819275 + 50.0 * 9.029500007629395
Epoch 1440, val loss: 0.38347533345222473
Epoch 1450, training loss: 451.6310119628906 = 0.24253708124160767 + 50.0 * 9.027770042419434
Epoch 1450, val loss: 0.383807510137558
Epoch 1460, training loss: 451.5973815917969 = 0.24158214032649994 + 50.0 * 9.027115821838379
Epoch 1460, val loss: 0.3839718699455261
Epoch 1470, training loss: 451.6347961425781 = 0.24062518775463104 + 50.0 * 9.027883529663086
Epoch 1470, val loss: 0.38416239619255066
Epoch 1480, training loss: 451.5831604003906 = 0.2396717071533203 + 50.0 * 9.026869773864746
Epoch 1480, val loss: 0.38439756631851196
Epoch 1490, training loss: 451.5896911621094 = 0.23871910572052002 + 50.0 * 9.027019500732422
Epoch 1490, val loss: 0.38446474075317383
Epoch 1500, training loss: 451.54315185546875 = 0.23777179419994354 + 50.0 * 9.026107788085938
Epoch 1500, val loss: 0.3848157525062561
Epoch 1510, training loss: 451.54901123046875 = 0.23682576417922974 + 50.0 * 9.026244163513184
Epoch 1510, val loss: 0.3850144147872925
Epoch 1520, training loss: 451.601806640625 = 0.23588795959949493 + 50.0 * 9.027318000793457
Epoch 1520, val loss: 0.38535866141319275
Epoch 1530, training loss: 451.5621032714844 = 0.23494310677051544 + 50.0 * 9.026542663574219
Epoch 1530, val loss: 0.38540327548980713
Epoch 1540, training loss: 451.50811767578125 = 0.23400886356830597 + 50.0 * 9.025482177734375
Epoch 1540, val loss: 0.38567355275154114
Epoch 1550, training loss: 451.53033447265625 = 0.23308028280735016 + 50.0 * 9.025944709777832
Epoch 1550, val loss: 0.3858305513858795
Epoch 1560, training loss: 451.5526428222656 = 0.23215699195861816 + 50.0 * 9.026410102844238
Epoch 1560, val loss: 0.38614216446876526
Epoch 1570, training loss: 451.49761962890625 = 0.23123469948768616 + 50.0 * 9.025327682495117
Epoch 1570, val loss: 0.3864051103591919
Epoch 1580, training loss: 451.4610290527344 = 0.2303202599287033 + 50.0 * 9.024614334106445
Epoch 1580, val loss: 0.3867506980895996
Epoch 1590, training loss: 451.5728454589844 = 0.22940456867218018 + 50.0 * 9.02686882019043
Epoch 1590, val loss: 0.3870203197002411
Epoch 1600, training loss: 451.46197509765625 = 0.22848761081695557 + 50.0 * 9.024669647216797
Epoch 1600, val loss: 0.3873541057109833
Epoch 1610, training loss: 451.45501708984375 = 0.22759707272052765 + 50.0 * 9.024548530578613
Epoch 1610, val loss: 0.38775408267974854
Epoch 1620, training loss: 451.4223937988281 = 0.22669267654418945 + 50.0 * 9.023914337158203
Epoch 1620, val loss: 0.3879346549510956
Epoch 1630, training loss: 451.4135437011719 = 0.22578515112400055 + 50.0 * 9.023755073547363
Epoch 1630, val loss: 0.38815733790397644
Epoch 1640, training loss: 451.6501159667969 = 0.22488686442375183 + 50.0 * 9.028504371643066
Epoch 1640, val loss: 0.3881933093070984
Epoch 1650, training loss: 451.4638366699219 = 0.22398246824741364 + 50.0 * 9.024797439575195
Epoch 1650, val loss: 0.3888755440711975
Epoch 1660, training loss: 451.3927307128906 = 0.22309759259223938 + 50.0 * 9.023392677307129
Epoch 1660, val loss: 0.38958311080932617
Epoch 1670, training loss: 451.39312744140625 = 0.22220812737941742 + 50.0 * 9.023418426513672
Epoch 1670, val loss: 0.3898838758468628
Epoch 1680, training loss: 451.4150390625 = 0.22131814062595367 + 50.0 * 9.023874282836914
Epoch 1680, val loss: 0.39027175307273865
Epoch 1690, training loss: 451.3936767578125 = 0.22042874991893768 + 50.0 * 9.023465156555176
Epoch 1690, val loss: 0.3903862237930298
Epoch 1700, training loss: 451.3740234375 = 0.2195497751235962 + 50.0 * 9.023089408874512
Epoch 1700, val loss: 0.39070042967796326
Epoch 1710, training loss: 451.3788146972656 = 0.21867278218269348 + 50.0 * 9.023202896118164
Epoch 1710, val loss: 0.3915497660636902
Epoch 1720, training loss: 451.4086608886719 = 0.21779413521289825 + 50.0 * 9.02381706237793
Epoch 1720, val loss: 0.3914804458618164
Epoch 1730, training loss: 451.3272399902344 = 0.21691393852233887 + 50.0 * 9.02220630645752
Epoch 1730, val loss: 0.3922501504421234
Epoch 1740, training loss: 451.3191833496094 = 0.21605244278907776 + 50.0 * 9.022062301635742
Epoch 1740, val loss: 0.3922850489616394
Epoch 1750, training loss: 451.3062438964844 = 0.2151850461959839 + 50.0 * 9.021821022033691
Epoch 1750, val loss: 0.3930262625217438
Epoch 1760, training loss: 451.319091796875 = 0.21430926024913788 + 50.0 * 9.022095680236816
Epoch 1760, val loss: 0.3932122588157654
Epoch 1770, training loss: 451.394287109375 = 0.21344605088233948 + 50.0 * 9.023616790771484
Epoch 1770, val loss: 0.39392492175102234
Epoch 1780, training loss: 451.3576354980469 = 0.21256953477859497 + 50.0 * 9.02290153503418
Epoch 1780, val loss: 0.3941514194011688
Epoch 1790, training loss: 451.2986145019531 = 0.211711585521698 + 50.0 * 9.021738052368164
Epoch 1790, val loss: 0.3947151303291321
Epoch 1800, training loss: 451.2810363769531 = 0.2108682096004486 + 50.0 * 9.021403312683105
Epoch 1800, val loss: 0.3952030539512634
Epoch 1810, training loss: 451.2473449707031 = 0.21000532805919647 + 50.0 * 9.020747184753418
Epoch 1810, val loss: 0.395673006772995
Epoch 1820, training loss: 451.30419921875 = 0.2091505080461502 + 50.0 * 9.02190113067627
Epoch 1820, val loss: 0.3963218629360199
Epoch 1830, training loss: 451.2526550292969 = 0.20829054713249207 + 50.0 * 9.02088737487793
Epoch 1830, val loss: 0.39675116539001465
Epoch 1840, training loss: 451.22998046875 = 0.2074447125196457 + 50.0 * 9.020450592041016
Epoch 1840, val loss: 0.39724478125572205
Epoch 1850, training loss: 451.2410583496094 = 0.20659920573234558 + 50.0 * 9.020689010620117
Epoch 1850, val loss: 0.3976611793041229
Epoch 1860, training loss: 451.2900695800781 = 0.20575495064258575 + 50.0 * 9.021686553955078
Epoch 1860, val loss: 0.39820215106010437
Epoch 1870, training loss: 451.1989440917969 = 0.20490890741348267 + 50.0 * 9.019881248474121
Epoch 1870, val loss: 0.39874017238616943
Epoch 1880, training loss: 451.22216796875 = 0.20406760275363922 + 50.0 * 9.02036190032959
Epoch 1880, val loss: 0.3992787003517151
Epoch 1890, training loss: 451.3553466796875 = 0.20322765409946442 + 50.0 * 9.023042678833008
Epoch 1890, val loss: 0.3999378979206085
Epoch 1900, training loss: 451.2989196777344 = 0.20239964127540588 + 50.0 * 9.021930694580078
Epoch 1900, val loss: 0.40066176652908325
Epoch 1910, training loss: 451.20184326171875 = 0.20156678557395935 + 50.0 * 9.020005226135254
Epoch 1910, val loss: 0.4009895622730255
Epoch 1920, training loss: 451.1802978515625 = 0.20073159039020538 + 50.0 * 9.019591331481934
Epoch 1920, val loss: 0.4016071557998657
Epoch 1930, training loss: 451.2393798828125 = 0.19989711046218872 + 50.0 * 9.020790100097656
Epoch 1930, val loss: 0.402178555727005
Epoch 1940, training loss: 451.1519775390625 = 0.19906704127788544 + 50.0 * 9.019058227539062
Epoch 1940, val loss: 0.4026887118816376
Epoch 1950, training loss: 451.1769104003906 = 0.1982428878545761 + 50.0 * 9.019573211669922
Epoch 1950, val loss: 0.40325143933296204
Epoch 1960, training loss: 451.1720275878906 = 0.1974138617515564 + 50.0 * 9.019492149353027
Epoch 1960, val loss: 0.40404561161994934
Epoch 1970, training loss: 451.1382141113281 = 0.19659417867660522 + 50.0 * 9.018832206726074
Epoch 1970, val loss: 0.4046861529350281
Epoch 1980, training loss: 451.1484069824219 = 0.1957729011774063 + 50.0 * 9.019052505493164
Epoch 1980, val loss: 0.40541619062423706
Epoch 1990, training loss: 451.204833984375 = 0.1949620544910431 + 50.0 * 9.020196914672852
Epoch 1990, val loss: 0.40622684359550476
Epoch 2000, training loss: 451.1239929199219 = 0.19413073360919952 + 50.0 * 9.018597602844238
Epoch 2000, val loss: 0.4064566195011139
Epoch 2010, training loss: 451.1048583984375 = 0.19332285225391388 + 50.0 * 9.018230438232422
Epoch 2010, val loss: 0.40748515725135803
Epoch 2020, training loss: 451.1536560058594 = 0.19250324368476868 + 50.0 * 9.0192232131958
Epoch 2020, val loss: 0.4082629382610321
Epoch 2030, training loss: 451.1900329589844 = 0.19168533384799957 + 50.0 * 9.019967079162598
Epoch 2030, val loss: 0.4083435535430908
Epoch 2040, training loss: 451.12261962890625 = 0.19090069830417633 + 50.0 * 9.018634796142578
Epoch 2040, val loss: 0.4094308018684387
Epoch 2050, training loss: 451.07806396484375 = 0.190089613199234 + 50.0 * 9.017759323120117
Epoch 2050, val loss: 0.4099031090736389
Epoch 2060, training loss: 451.0554504394531 = 0.18928304314613342 + 50.0 * 9.01732349395752
Epoch 2060, val loss: 0.4107349216938019
Epoch 2070, training loss: 451.0963134765625 = 0.18847908079624176 + 50.0 * 9.018157005310059
Epoch 2070, val loss: 0.4116222560405731
Epoch 2080, training loss: 451.1033020019531 = 0.18767651915550232 + 50.0 * 9.018312454223633
Epoch 2080, val loss: 0.41231265664100647
Epoch 2090, training loss: 451.0564880371094 = 0.18688197433948517 + 50.0 * 9.0173921585083
Epoch 2090, val loss: 0.4127506613731384
Epoch 2100, training loss: 451.05499267578125 = 0.18608374893665314 + 50.0 * 9.017377853393555
Epoch 2100, val loss: 0.4137119948863983
Epoch 2110, training loss: 451.1767578125 = 0.1852891892194748 + 50.0 * 9.019828796386719
Epoch 2110, val loss: 0.41445568203926086
Epoch 2120, training loss: 451.09039306640625 = 0.18450404703617096 + 50.0 * 9.018117904663086
Epoch 2120, val loss: 0.414827436208725
Epoch 2130, training loss: 451.0401611328125 = 0.1837228238582611 + 50.0 * 9.017128944396973
Epoch 2130, val loss: 0.4160566031932831
Epoch 2140, training loss: 451.04241943359375 = 0.18293485045433044 + 50.0 * 9.017189979553223
Epoch 2140, val loss: 0.41662561893463135
Epoch 2150, training loss: 451.00750732421875 = 0.18214935064315796 + 50.0 * 9.016507148742676
Epoch 2150, val loss: 0.4175184369087219
Epoch 2160, training loss: 451.00543212890625 = 0.18136507272720337 + 50.0 * 9.016481399536133
Epoch 2160, val loss: 0.418035626411438
Epoch 2170, training loss: 451.0355529785156 = 0.18058134615421295 + 50.0 * 9.017099380493164
Epoch 2170, val loss: 0.41886237263679504
Epoch 2180, training loss: 451.0335998535156 = 0.17979465425014496 + 50.0 * 9.017075538635254
Epoch 2180, val loss: 0.41977986693382263
Epoch 2190, training loss: 451.0351867675781 = 0.17901954054832458 + 50.0 * 9.017123222351074
Epoch 2190, val loss: 0.42051538825035095
Epoch 2200, training loss: 450.9814453125 = 0.17825156450271606 + 50.0 * 9.016063690185547
Epoch 2200, val loss: 0.4215543866157532
Epoch 2210, training loss: 450.9805908203125 = 0.17747299373149872 + 50.0 * 9.01606273651123
Epoch 2210, val loss: 0.42225882411003113
Epoch 2220, training loss: 450.9892883300781 = 0.1767040491104126 + 50.0 * 9.016251564025879
Epoch 2220, val loss: 0.42292165756225586
Epoch 2230, training loss: 451.0035400390625 = 0.1759369671344757 + 50.0 * 9.016551971435547
Epoch 2230, val loss: 0.4240518808364868
Epoch 2240, training loss: 451.0090026855469 = 0.17518113553524017 + 50.0 * 9.016676902770996
Epoch 2240, val loss: 0.42495977878570557
Epoch 2250, training loss: 450.96649169921875 = 0.1744118183851242 + 50.0 * 9.015841484069824
Epoch 2250, val loss: 0.4261071979999542
Epoch 2260, training loss: 450.965087890625 = 0.17365115880966187 + 50.0 * 9.015829086303711
Epoch 2260, val loss: 0.42668232321739197
Epoch 2270, training loss: 450.9728698730469 = 0.17289458215236664 + 50.0 * 9.015999794006348
Epoch 2270, val loss: 0.42771074175834656
Epoch 2280, training loss: 450.9574890136719 = 0.17214305698871613 + 50.0 * 9.015707015991211
Epoch 2280, val loss: 0.4285567104816437
Epoch 2290, training loss: 450.9172058105469 = 0.17138724029064178 + 50.0 * 9.01491641998291
Epoch 2290, val loss: 0.42928260564804077
Epoch 2300, training loss: 450.8896789550781 = 0.1706300973892212 + 50.0 * 9.014381408691406
Epoch 2300, val loss: 0.4302048087120056
Epoch 2310, training loss: 450.9045104980469 = 0.16988015174865723 + 50.0 * 9.014692306518555
Epoch 2310, val loss: 0.4311193525791168
Epoch 2320, training loss: 451.0413513183594 = 0.16914744675159454 + 50.0 * 9.017443656921387
Epoch 2320, val loss: 0.4319930374622345
Epoch 2330, training loss: 450.99542236328125 = 0.16838884353637695 + 50.0 * 9.01654052734375
Epoch 2330, val loss: 0.4331759214401245
Epoch 2340, training loss: 450.9114685058594 = 0.16765551269054413 + 50.0 * 9.014876365661621
Epoch 2340, val loss: 0.4341847002506256
Epoch 2350, training loss: 450.8740539550781 = 0.16690495610237122 + 50.0 * 9.014142990112305
Epoch 2350, val loss: 0.4348362982273102
Epoch 2360, training loss: 450.8992614746094 = 0.16616572439670563 + 50.0 * 9.01466178894043
Epoch 2360, val loss: 0.43598875403404236
Epoch 2370, training loss: 450.94110107421875 = 0.16542868316173553 + 50.0 * 9.01551342010498
Epoch 2370, val loss: 0.4370698928833008
Epoch 2380, training loss: 450.8795166015625 = 0.1647016704082489 + 50.0 * 9.014296531677246
Epoch 2380, val loss: 0.4377014935016632
Epoch 2390, training loss: 450.9518127441406 = 0.16397497057914734 + 50.0 * 9.015756607055664
Epoch 2390, val loss: 0.43887004256248474
Epoch 2400, training loss: 450.8400573730469 = 0.16324903070926666 + 50.0 * 9.01353645324707
Epoch 2400, val loss: 0.43997177481651306
Epoch 2410, training loss: 450.8274841308594 = 0.16252511739730835 + 50.0 * 9.013298988342285
Epoch 2410, val loss: 0.4410313665866852
Epoch 2420, training loss: 450.844970703125 = 0.16180478036403656 + 50.0 * 9.013663291931152
Epoch 2420, val loss: 0.44229087233543396
Epoch 2430, training loss: 450.93243408203125 = 0.16110345721244812 + 50.0 * 9.015426635742188
Epoch 2430, val loss: 0.44340842962265015
Epoch 2440, training loss: 450.83868408203125 = 0.16036009788513184 + 50.0 * 9.013566017150879
Epoch 2440, val loss: 0.44332155585289
Epoch 2450, training loss: 450.8227844238281 = 0.15964412689208984 + 50.0 * 9.013262748718262
Epoch 2450, val loss: 0.4448908269405365
Epoch 2460, training loss: 450.9289855957031 = 0.15893954038619995 + 50.0 * 9.015400886535645
Epoch 2460, val loss: 0.44560202956199646
Epoch 2470, training loss: 450.8926696777344 = 0.158230260014534 + 50.0 * 9.014688491821289
Epoch 2470, val loss: 0.4475928544998169
Epoch 2480, training loss: 450.80914306640625 = 0.15750817954540253 + 50.0 * 9.013032913208008
Epoch 2480, val loss: 0.44768187403678894
Epoch 2490, training loss: 450.7907409667969 = 0.1567935198545456 + 50.0 * 9.012679100036621
Epoch 2490, val loss: 0.44933179020881653
Epoch 2500, training loss: 450.7749938964844 = 0.15608225762844086 + 50.0 * 9.012377738952637
Epoch 2500, val loss: 0.4499627947807312
Epoch 2510, training loss: 450.87628173828125 = 0.15537649393081665 + 50.0 * 9.014418601989746
Epoch 2510, val loss: 0.4509407877922058
Epoch 2520, training loss: 450.8152160644531 = 0.15468114614486694 + 50.0 * 9.01321029663086
Epoch 2520, val loss: 0.45287004113197327
Epoch 2530, training loss: 450.8052978515625 = 0.15397922694683075 + 50.0 * 9.013026237487793
Epoch 2530, val loss: 0.45338138937950134
Epoch 2540, training loss: 450.7705078125 = 0.1532810926437378 + 50.0 * 9.012344360351562
Epoch 2540, val loss: 0.4547858238220215
Epoch 2550, training loss: 450.74615478515625 = 0.15257543325424194 + 50.0 * 9.011871337890625
Epoch 2550, val loss: 0.45568230748176575
Epoch 2560, training loss: 450.8252868652344 = 0.15188899636268616 + 50.0 * 9.013467788696289
Epoch 2560, val loss: 0.45687952637672424
Epoch 2570, training loss: 450.7948303222656 = 0.1511945128440857 + 50.0 * 9.012872695922852
Epoch 2570, val loss: 0.4581533968448639
Epoch 2580, training loss: 450.7432556152344 = 0.15049974620342255 + 50.0 * 9.011855125427246
Epoch 2580, val loss: 0.45930278301239014
Epoch 2590, training loss: 450.727294921875 = 0.14980670809745789 + 50.0 * 9.011549949645996
Epoch 2590, val loss: 0.460466593503952
Epoch 2600, training loss: 450.73138427734375 = 0.14911922812461853 + 50.0 * 9.011645317077637
Epoch 2600, val loss: 0.4614493250846863
Epoch 2610, training loss: 450.9558410644531 = 0.14845353364944458 + 50.0 * 9.01614761352539
Epoch 2610, val loss: 0.4628548324108124
Epoch 2620, training loss: 450.80194091796875 = 0.1477513462305069 + 50.0 * 9.013083457946777
Epoch 2620, val loss: 0.4640410840511322
Epoch 2630, training loss: 450.7153015136719 = 0.14707492291927338 + 50.0 * 9.011364936828613
Epoch 2630, val loss: 0.46517321467399597
Epoch 2640, training loss: 450.704345703125 = 0.14638686180114746 + 50.0 * 9.01115894317627
Epoch 2640, val loss: 0.4665881395339966
Epoch 2650, training loss: 450.7452392578125 = 0.1457076519727707 + 50.0 * 9.011990547180176
Epoch 2650, val loss: 0.4679829776287079
Epoch 2660, training loss: 450.72857666015625 = 0.14502985775470734 + 50.0 * 9.01167106628418
Epoch 2660, val loss: 0.4690724313259125
Epoch 2670, training loss: 450.7900695800781 = 0.1443537026643753 + 50.0 * 9.012914657592773
Epoch 2670, val loss: 0.47012385725975037
Epoch 2680, training loss: 450.70867919921875 = 0.14368383586406708 + 50.0 * 9.011300086975098
Epoch 2680, val loss: 0.4715105891227722
Epoch 2690, training loss: 450.6893310546875 = 0.14301525056362152 + 50.0 * 9.010926246643066
Epoch 2690, val loss: 0.47268879413604736
Epoch 2700, training loss: 450.7413635253906 = 0.14234969019889832 + 50.0 * 9.011980056762695
Epoch 2700, val loss: 0.4740554392337799
Epoch 2710, training loss: 450.6897277832031 = 0.1416735053062439 + 50.0 * 9.010961532592773
Epoch 2710, val loss: 0.47530072927474976
Epoch 2720, training loss: 450.7082214355469 = 0.14102107286453247 + 50.0 * 9.011343955993652
Epoch 2720, val loss: 0.4768713712692261
Epoch 2730, training loss: 450.6814270019531 = 0.14035867154598236 + 50.0 * 9.010821342468262
Epoch 2730, val loss: 0.4778607487678528
Epoch 2740, training loss: 450.6913146972656 = 0.13971294462680817 + 50.0 * 9.011032104492188
Epoch 2740, val loss: 0.47854363918304443
Epoch 2750, training loss: 450.6678161621094 = 0.1390482783317566 + 50.0 * 9.010575294494629
Epoch 2750, val loss: 0.48027142882347107
Epoch 2760, training loss: 450.6405029296875 = 0.138393372297287 + 50.0 * 9.010042190551758
Epoch 2760, val loss: 0.48209571838378906
Epoch 2770, training loss: 450.6405029296875 = 0.13773764669895172 + 50.0 * 9.010055541992188
Epoch 2770, val loss: 0.48309922218322754
Epoch 2780, training loss: 450.74749755859375 = 0.1370902955532074 + 50.0 * 9.012207984924316
Epoch 2780, val loss: 0.4841723144054413
Epoch 2790, training loss: 450.6686096191406 = 0.13643842935562134 + 50.0 * 9.010643005371094
Epoch 2790, val loss: 0.4859929084777832
Epoch 2800, training loss: 450.65655517578125 = 0.13579240441322327 + 50.0 * 9.010415077209473
Epoch 2800, val loss: 0.48709189891815186
Epoch 2810, training loss: 450.64422607421875 = 0.1351492702960968 + 50.0 * 9.010181427001953
Epoch 2810, val loss: 0.4888787567615509
Epoch 2820, training loss: 450.71258544921875 = 0.13450756669044495 + 50.0 * 9.011561393737793
Epoch 2820, val loss: 0.48971518874168396
Epoch 2830, training loss: 450.6317138671875 = 0.1338646411895752 + 50.0 * 9.009957313537598
Epoch 2830, val loss: 0.49143797159194946
Epoch 2840, training loss: 450.61920166015625 = 0.13322041928768158 + 50.0 * 9.009719848632812
Epoch 2840, val loss: 0.49280476570129395
Epoch 2850, training loss: 450.74530029296875 = 0.132595032453537 + 50.0 * 9.012253761291504
Epoch 2850, val loss: 0.4943220317363739
Epoch 2860, training loss: 450.619140625 = 0.1319478154182434 + 50.0 * 9.009743690490723
Epoch 2860, val loss: 0.4953770041465759
Epoch 2870, training loss: 450.5827331542969 = 0.13130898773670197 + 50.0 * 9.009028434753418
Epoch 2870, val loss: 0.49679601192474365
Epoch 2880, training loss: 450.5813903808594 = 0.1306702047586441 + 50.0 * 9.009014129638672
Epoch 2880, val loss: 0.49839574098587036
Epoch 2890, training loss: 450.7059631347656 = 0.13004672527313232 + 50.0 * 9.011518478393555
Epoch 2890, val loss: 0.4998862147331238
Epoch 2900, training loss: 450.6000671386719 = 0.12942256033420563 + 50.0 * 9.00941276550293
Epoch 2900, val loss: 0.5008834004402161
Epoch 2910, training loss: 450.6006164550781 = 0.1287861466407776 + 50.0 * 9.00943660736084
Epoch 2910, val loss: 0.5024247169494629
Epoch 2920, training loss: 450.5889892578125 = 0.12816421687602997 + 50.0 * 9.00921630859375
Epoch 2920, val loss: 0.5035978555679321
Epoch 2930, training loss: 450.6329345703125 = 0.1275462508201599 + 50.0 * 9.01010799407959
Epoch 2930, val loss: 0.5052143931388855
Epoch 2940, training loss: 450.5995178222656 = 0.12691561877727509 + 50.0 * 9.009451866149902
Epoch 2940, val loss: 0.5072638988494873
Epoch 2950, training loss: 450.656982421875 = 0.12629735469818115 + 50.0 * 9.010613441467285
Epoch 2950, val loss: 0.5081844925880432
Epoch 2960, training loss: 450.56201171875 = 0.1256827861070633 + 50.0 * 9.008727073669434
Epoch 2960, val loss: 0.5098062753677368
Epoch 2970, training loss: 450.5635070800781 = 0.1250552386045456 + 50.0 * 9.008769035339355
Epoch 2970, val loss: 0.511099100112915
Epoch 2980, training loss: 450.63189697265625 = 0.12444938719272614 + 50.0 * 9.010149002075195
Epoch 2980, val loss: 0.5126787424087524
Epoch 2990, training loss: 450.55169677734375 = 0.1238325834274292 + 50.0 * 9.008557319641113
Epoch 2990, val loss: 0.5145385265350342
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8524
Overall ASR: 0.7378
Flip ASR: 0.6725/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.30 GiB already allocated; 735.69 MiB free; 4.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 445.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 445.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 445.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 447.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 469.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0431518554688 = 1.0976319313049316 + 50.0 * 10.358909606933594
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 59.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0562744140625 = 1.102213978767395 + 50.0 * 10.359081268310547
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 59.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0494384765625 = 1.0964006185531616 + 50.0 * 10.359061241149902
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 731.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.049072265625 = 1.0953800678253174 + 50.0 * 10.359073638916016
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 737.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0524291992188 = 1.097976565361023 + 50.0 * 10.359089851379395
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 721.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0411987304688 = 1.0928022861480713 + 50.0 * 10.358968734741211
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 79.69 MiB free; 6.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.0535278320312 = 1.1096086502075195 + 50.0 * 10.358878135681152
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 79.69 MiB free; 6.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 477.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 873.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 873.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 873.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 875.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 875.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 875.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 875.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 875.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.15 GiB already allocated; 489.69 MiB free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
