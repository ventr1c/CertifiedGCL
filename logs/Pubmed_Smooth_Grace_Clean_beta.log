nohup: ignoring input
run_robust_acc.py:14: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0, add_edge_rate_2=0, attack='none', base_model='GCN', cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0005, cl_num_epochs=200, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, clf_weight=1, config='config.yaml', cont_batch_size=0, cont_weight=1, cuda=True, dataset='Pubmed', debug=True, device_id=1, drop_edge_rate_1=0.2, drop_edge_rate_2=0, drop_feat_rate_1=0.3, drop_feat_rate_2=0.2, dropout=0.5, encoder_model='Grace', hidden=128, if_smoothed=True, inv_weight=1, no_cuda=False, noisy_level=0.3, num_hidden=128, num_proj_hidden=128, prob=0.8, seed=10, select_target_ratio=0.1, tau=0.1, test_model='GCN', train_lr=0.01, weight_decay=0.0005)
beta 0.5
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582391738891602
Epoch 10, training loss: 10.041973114013672
Epoch 20, training loss: 9.284623146057129
Epoch 30, training loss: 8.929883003234863
Epoch 40, training loss: 8.541865348815918
Epoch 50, training loss: 8.238887786865234
Epoch 60, training loss: 8.022133827209473
Epoch 70, training loss: 7.883643627166748
Epoch 80, training loss: 7.7002081871032715
Epoch 90, training loss: 7.759626388549805
Epoch 100, training loss: 7.476424217224121
Epoch 110, training loss: 7.329494476318359
Epoch 120, training loss: 7.275381088256836
Epoch 130, training loss: 7.175076007843018
Epoch 140, training loss: 7.139319896697998
Epoch 150, training loss: 7.065032958984375
Epoch 160, training loss: 7.017000198364258
Epoch 170, training loss: 7.066646099090576
Epoch 180, training loss: 6.942699432373047
Epoch 190, training loss: 6.9204936027526855
Epoch 200, training loss: 6.917232036590576
Epoch 210, training loss: 6.913443088531494
Epoch 220, training loss: 6.891992092132568
Epoch 230, training loss: 6.813711166381836
Epoch 240, training loss: 6.791624069213867
Epoch 250, training loss: 6.8805108070373535
Epoch 260, training loss: 6.77884578704834
Epoch 270, training loss: 6.800869941711426
Epoch 280, training loss: 6.868161201477051
Epoch 290, training loss: 6.792848587036133
Epoch 300, training loss: 6.699071407318115
Epoch 310, training loss: 6.795436859130859
Epoch 320, training loss: 6.726398944854736
Epoch 330, training loss: 6.636111259460449
Epoch 340, training loss: 6.670800685882568
Epoch 350, training loss: 6.714531898498535
Epoch 360, training loss: 6.637650489807129
Epoch 370, training loss: 6.665347576141357
Epoch 380, training loss: 6.599872589111328
Epoch 390, training loss: 6.675457954406738
Epoch 400, training loss: 6.68339204788208
Epoch 410, training loss: 6.61415433883667
Epoch 420, training loss: 6.622400283813477
Epoch 430, training loss: 6.589802265167236
Epoch 440, training loss: 6.597940444946289
Epoch 450, training loss: 6.630661487579346
Epoch 460, training loss: 6.635181903839111
Epoch 470, training loss: 6.623780250549316
Epoch 480, training loss: 6.5934529304504395
Epoch 490, training loss: 6.620516300201416
none
Accuracy: 0.751
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582281112670898
Epoch 10, training loss: 9.952967643737793
Epoch 20, training loss: 9.343057632446289
Epoch 30, training loss: 9.103348731994629
Epoch 40, training loss: 8.813090324401855
Epoch 50, training loss: 8.564931869506836
Epoch 60, training loss: 8.31773853302002
Epoch 70, training loss: 8.154054641723633
Epoch 80, training loss: 7.974788665771484
Epoch 90, training loss: 7.921468734741211
Epoch 100, training loss: 7.747882843017578
Epoch 110, training loss: 7.7144341468811035
Epoch 120, training loss: 7.538589000701904
Epoch 130, training loss: 7.481042861938477
Epoch 140, training loss: 7.417768478393555
Epoch 150, training loss: 7.4531331062316895
Epoch 160, training loss: 7.3576579093933105
Epoch 170, training loss: 7.311983585357666
Epoch 180, training loss: 7.215367794036865
Epoch 190, training loss: 7.1749396324157715
Epoch 200, training loss: 7.117217540740967
Epoch 210, training loss: 7.151562690734863
Epoch 220, training loss: 7.035213947296143
Epoch 230, training loss: 7.042313098907471
Epoch 240, training loss: 7.014151096343994
Epoch 250, training loss: 6.980765342712402
Epoch 260, training loss: 7.0425286293029785
Epoch 270, training loss: 6.951618671417236
Epoch 280, training loss: 6.881329536437988
Epoch 290, training loss: 6.946633338928223
Epoch 300, training loss: 6.860296726226807
Epoch 310, training loss: 6.869941234588623
Epoch 320, training loss: 6.883583068847656
Epoch 330, training loss: 6.821786403656006
Epoch 340, training loss: 6.723820209503174
Epoch 350, training loss: 6.779552459716797
Epoch 360, training loss: 6.770081996917725
Epoch 370, training loss: 6.7359395027160645
Epoch 380, training loss: 6.7461724281311035
Epoch 390, training loss: 6.696156024932861
Epoch 400, training loss: 6.680068492889404
Epoch 410, training loss: 6.639199256896973
Epoch 420, training loss: 6.678172588348389
Epoch 430, training loss: 6.722489833831787
Epoch 440, training loss: 6.635420799255371
Epoch 450, training loss: 6.621817111968994
Epoch 460, training loss: 6.640406131744385
Epoch 470, training loss: 6.596225261688232
Epoch 480, training loss: 6.58824348449707
Epoch 490, training loss: 6.548372745513916
none
Accuracy: 0.788
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582265853881836
Epoch 10, training loss: 10.039872169494629
Epoch 20, training loss: 9.351062774658203
Epoch 30, training loss: 8.906761169433594
Epoch 40, training loss: 8.531774520874023
Epoch 50, training loss: 8.235607147216797
Epoch 60, training loss: 8.00639820098877
Epoch 70, training loss: 7.916328430175781
Epoch 80, training loss: 7.835324764251709
Epoch 90, training loss: 7.734884738922119
Epoch 100, training loss: 7.631658554077148
Epoch 110, training loss: 7.572238922119141
Epoch 120, training loss: 7.566804885864258
Epoch 130, training loss: 7.517122268676758
Epoch 140, training loss: 7.4298415184021
Epoch 150, training loss: 7.3836870193481445
Epoch 160, training loss: 7.500933647155762
Epoch 170, training loss: 7.410986423492432
Epoch 180, training loss: 7.325551986694336
Epoch 190, training loss: 7.290871620178223
Epoch 200, training loss: 7.26005744934082
Epoch 210, training loss: 7.291268348693848
Epoch 220, training loss: 7.298619270324707
Epoch 230, training loss: 7.19966459274292
Epoch 240, training loss: 7.169607639312744
Epoch 250, training loss: 7.162385940551758
Epoch 260, training loss: 7.117711067199707
Epoch 270, training loss: 7.186805725097656
Epoch 280, training loss: 7.117734909057617
Epoch 290, training loss: 6.990213394165039
Epoch 300, training loss: 6.961803436279297
Epoch 310, training loss: 6.976062774658203
Epoch 320, training loss: 6.923357963562012
Epoch 330, training loss: 6.908545017242432
Epoch 340, training loss: 6.799040794372559
Epoch 350, training loss: 6.774501323699951
Epoch 360, training loss: 6.789797782897949
Epoch 370, training loss: 6.76443338394165
Epoch 380, training loss: 6.819786071777344
Epoch 390, training loss: 6.7579169273376465
Epoch 400, training loss: 6.662299156188965
Epoch 410, training loss: 6.65910530090332
Epoch 420, training loss: 6.631563663482666
Epoch 430, training loss: 6.661098480224609
Epoch 440, training loss: 6.6166253089904785
Epoch 450, training loss: 6.704883575439453
Epoch 460, training loss: 6.611546993255615
Epoch 470, training loss: 6.643301963806152
Epoch 480, training loss: 6.635066986083984
Epoch 490, training loss: 6.577661514282227
none
Accuracy: 0.769
beta 0.6
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582253456115723
Epoch 10, training loss: 10.036513328552246
Epoch 20, training loss: 9.297694206237793
Epoch 30, training loss: 8.869871139526367
Epoch 40, training loss: 8.656439781188965
Epoch 50, training loss: 8.375455856323242
Epoch 60, training loss: 8.11811351776123
Epoch 70, training loss: 7.948204040527344
Epoch 80, training loss: 7.771792888641357
Epoch 90, training loss: 7.639974594116211
Epoch 100, training loss: 7.597966194152832
Epoch 110, training loss: 7.4292402267456055
Epoch 120, training loss: 7.3457489013671875
Epoch 130, training loss: 7.319296360015869
Epoch 140, training loss: 7.260862350463867
Epoch 150, training loss: 7.287522315979004
Epoch 160, training loss: 7.276632308959961
Epoch 170, training loss: 7.167061805725098
Epoch 180, training loss: 7.131215572357178
Epoch 190, training loss: 7.142308235168457
Epoch 200, training loss: 7.121603488922119
Epoch 210, training loss: 7.168206691741943
Epoch 220, training loss: 7.051788330078125
Epoch 230, training loss: 7.021092891693115
Epoch 240, training loss: 7.04203462600708
Epoch 250, training loss: 6.941723823547363
Epoch 260, training loss: 6.98529052734375
Epoch 270, training loss: 6.925820827484131
Epoch 280, training loss: 6.9492506980896
Epoch 290, training loss: 6.907113075256348
Epoch 300, training loss: 6.953450679779053
Epoch 310, training loss: 6.839532375335693
Epoch 320, training loss: 6.858007907867432
Epoch 330, training loss: 6.909066677093506
Epoch 340, training loss: 6.871121883392334
Epoch 350, training loss: 6.744680404663086
Epoch 360, training loss: 6.7544450759887695
Epoch 370, training loss: 6.682393550872803
Epoch 380, training loss: 6.780572414398193
Epoch 390, training loss: 6.718135356903076
Epoch 400, training loss: 6.683333396911621
Epoch 410, training loss: 6.666233539581299
Epoch 420, training loss: 6.6650543212890625
Epoch 430, training loss: 6.652066707611084
Epoch 440, training loss: 6.630324840545654
Epoch 450, training loss: 6.653310775756836
Epoch 460, training loss: 6.606644153594971
Epoch 470, training loss: 6.621432304382324
Epoch 480, training loss: 6.563475608825684
Epoch 490, training loss: 6.67053747177124
none
Accuracy: 0.745
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.58220100402832
Epoch 10, training loss: 9.966654777526855
Epoch 20, training loss: 9.252952575683594
Epoch 30, training loss: 8.985511779785156
Epoch 40, training loss: 8.613431930541992
Epoch 50, training loss: 8.378181457519531
Epoch 60, training loss: 8.20847225189209
Epoch 70, training loss: 8.008005142211914
Epoch 80, training loss: 7.857499122619629
Epoch 90, training loss: 7.742503643035889
Epoch 100, training loss: 7.653099060058594
Epoch 110, training loss: 7.511603355407715
Epoch 120, training loss: 7.403354644775391
Epoch 130, training loss: 7.30638313293457
Epoch 140, training loss: 7.248884201049805
Epoch 150, training loss: 7.197662830352783
Epoch 160, training loss: 7.143184661865234
Epoch 170, training loss: 7.047567844390869
Epoch 180, training loss: 7.019546031951904
Epoch 190, training loss: 6.933876037597656
Epoch 200, training loss: 6.955266952514648
Epoch 210, training loss: 6.904975891113281
Epoch 220, training loss: 6.8742289543151855
Epoch 230, training loss: 6.907791614532471
Epoch 240, training loss: 6.889388084411621
Epoch 250, training loss: 6.844268321990967
Epoch 260, training loss: 6.840714931488037
Epoch 270, training loss: 6.763120174407959
Epoch 280, training loss: 6.784632205963135
Epoch 290, training loss: 6.764771461486816
Epoch 300, training loss: 6.788091659545898
Epoch 310, training loss: 6.700074672698975
Epoch 320, training loss: 6.763800621032715
Epoch 330, training loss: 6.711887359619141
Epoch 340, training loss: 6.746893405914307
Epoch 350, training loss: 6.720115661621094
Epoch 360, training loss: 6.766916751861572
Epoch 370, training loss: 6.6879754066467285
Epoch 380, training loss: 6.643049240112305
Epoch 390, training loss: 6.749428749084473
Epoch 400, training loss: 6.6402130126953125
Epoch 410, training loss: 6.69419002532959
Epoch 420, training loss: 6.6920013427734375
Epoch 430, training loss: 6.718001842498779
Epoch 440, training loss: 6.629488468170166
Epoch 450, training loss: 6.5918989181518555
Epoch 460, training loss: 6.668306827545166
Epoch 470, training loss: 6.6205363273620605
Epoch 480, training loss: 6.604667663574219
Epoch 490, training loss: 6.579008102416992
none
Accuracy: 0.795
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582159996032715
Epoch 10, training loss: 10.000492095947266
Epoch 20, training loss: 9.320128440856934
Epoch 30, training loss: 8.906771659851074
Epoch 40, training loss: 8.454920768737793
Epoch 50, training loss: 8.069183349609375
Epoch 60, training loss: 7.805363178253174
Epoch 70, training loss: 7.68580436706543
Epoch 80, training loss: 7.55694580078125
Epoch 90, training loss: 7.472100734710693
Epoch 100, training loss: 7.328285217285156
Epoch 110, training loss: 7.439505100250244
Epoch 120, training loss: 7.283401012420654
Epoch 130, training loss: 7.209332466125488
Epoch 140, training loss: 7.2792744636535645
Epoch 150, training loss: 7.163088321685791
Epoch 160, training loss: 7.047497272491455
Epoch 170, training loss: 7.106032371520996
Epoch 180, training loss: 6.952070236206055
Epoch 190, training loss: 6.92664909362793
Epoch 200, training loss: 6.94365930557251
Epoch 210, training loss: 6.923352241516113
Epoch 220, training loss: 6.854671955108643
Epoch 230, training loss: 6.8582682609558105
Epoch 240, training loss: 6.818593502044678
Epoch 250, training loss: 6.793869495391846
Epoch 260, training loss: 6.8029398918151855
Epoch 270, training loss: 6.810450553894043
Epoch 280, training loss: 6.7071452140808105
Epoch 290, training loss: 6.753170013427734
Epoch 300, training loss: 6.697507381439209
Epoch 310, training loss: 6.763493537902832
Epoch 320, training loss: 6.701502799987793
Epoch 330, training loss: 6.711363315582275
Epoch 340, training loss: 6.6705217361450195
Epoch 350, training loss: 6.611298561096191
Epoch 360, training loss: 6.705413818359375
Epoch 370, training loss: 6.588888645172119
Epoch 380, training loss: 6.6646928787231445
Epoch 390, training loss: 6.692207336425781
Epoch 400, training loss: 6.588564395904541
Epoch 410, training loss: 6.662083625793457
Epoch 420, training loss: 6.588274955749512
Epoch 430, training loss: 6.549252510070801
Epoch 440, training loss: 6.628335475921631
Epoch 450, training loss: 6.568630695343018
Epoch 460, training loss: 6.584605693817139
Epoch 470, training loss: 6.511075973510742
Epoch 480, training loss: 6.515701770782471
Epoch 490, training loss: 6.530129432678223
none
Accuracy: 0.764
beta 0.7
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582330703735352
Epoch 10, training loss: 9.904404640197754
Epoch 20, training loss: 9.267417907714844
Epoch 30, training loss: 8.866238594055176
Epoch 40, training loss: 8.46767520904541
Epoch 50, training loss: 8.168468475341797
Epoch 60, training loss: 7.990403175354004
Epoch 70, training loss: 7.882915019989014
Epoch 80, training loss: 7.751460552215576
Epoch 90, training loss: 7.789499759674072
Epoch 100, training loss: 7.632442951202393
Epoch 110, training loss: 7.599760055541992
Epoch 120, training loss: 7.523226261138916
Epoch 130, training loss: 7.40822172164917
Epoch 140, training loss: 7.268675327301025
Epoch 150, training loss: 7.240159511566162
Epoch 160, training loss: 7.352860450744629
Epoch 170, training loss: 7.1785478591918945
Epoch 180, training loss: 7.121649742126465
Epoch 190, training loss: 7.150351047515869
Epoch 200, training loss: 7.056280612945557
Epoch 210, training loss: 7.103150367736816
Epoch 220, training loss: 7.041652202606201
Epoch 230, training loss: 7.003043174743652
Epoch 240, training loss: 6.987940311431885
Epoch 250, training loss: 7.004019260406494
Epoch 260, training loss: 7.027389049530029
Epoch 270, training loss: 6.931471824645996
Epoch 280, training loss: 6.961416721343994
Epoch 290, training loss: 6.979018688201904
Epoch 300, training loss: 6.923451900482178
Epoch 310, training loss: 6.890664100646973
Epoch 320, training loss: 6.840206146240234
Epoch 330, training loss: 6.8315749168396
Epoch 340, training loss: 6.885244846343994
Epoch 350, training loss: 6.893884658813477
Epoch 360, training loss: 6.837102890014648
Epoch 370, training loss: 6.92997407913208
Epoch 380, training loss: 6.8984222412109375
Epoch 390, training loss: 6.881560325622559
Epoch 400, training loss: 6.802281856536865
Epoch 410, training loss: 6.75690221786499
Epoch 420, training loss: 6.726806163787842
Epoch 430, training loss: 6.7274274826049805
Epoch 440, training loss: 6.7615461349487305
Epoch 450, training loss: 6.696432590484619
Epoch 460, training loss: 6.689020156860352
Epoch 470, training loss: 6.670543670654297
Epoch 480, training loss: 6.647614002227783
Epoch 490, training loss: 6.638024806976318
none
Accuracy: 0.724
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582262992858887
Epoch 10, training loss: 9.929140090942383
Epoch 20, training loss: 9.211106300354004
Epoch 30, training loss: 8.820993423461914
Epoch 40, training loss: 8.623269081115723
Epoch 50, training loss: 8.397872924804688
Epoch 60, training loss: 8.255111694335938
Epoch 70, training loss: 8.01806926727295
Epoch 80, training loss: 7.821449279785156
Epoch 90, training loss: 7.796986103057861
Epoch 100, training loss: 7.648343086242676
Epoch 110, training loss: 7.632095813751221
Epoch 120, training loss: 7.559079647064209
Epoch 130, training loss: 7.496685981750488
Epoch 140, training loss: 7.454916954040527
Epoch 150, training loss: 7.353228569030762
Epoch 160, training loss: 7.407379150390625
Epoch 170, training loss: 7.3327317237854
Epoch 180, training loss: 7.264021873474121
Epoch 190, training loss: 7.277078628540039
Epoch 200, training loss: 7.2119340896606445
Epoch 210, training loss: 7.195531845092773
Epoch 220, training loss: 7.08548641204834
Epoch 230, training loss: 7.062572956085205
Epoch 240, training loss: 7.093730449676514
Epoch 250, training loss: 7.09416389465332
Epoch 260, training loss: 7.129973411560059
Epoch 270, training loss: 6.988306522369385
Epoch 280, training loss: 7.008589744567871
Epoch 290, training loss: 6.946710109710693
Epoch 300, training loss: 6.882491111755371
Epoch 310, training loss: 6.87089729309082
Epoch 320, training loss: 6.847281455993652
Epoch 330, training loss: 6.90023946762085
Epoch 340, training loss: 6.791724681854248
Epoch 350, training loss: 6.7822771072387695
Epoch 360, training loss: 6.770931243896484
Epoch 370, training loss: 6.740736961364746
Epoch 380, training loss: 6.712740898132324
Epoch 390, training loss: 6.7913126945495605
Epoch 400, training loss: 6.717166900634766
Epoch 410, training loss: 6.75975227355957
Epoch 420, training loss: 6.7097859382629395
Epoch 430, training loss: 6.642577648162842
Epoch 440, training loss: 6.733410835266113
Epoch 450, training loss: 6.654941082000732
Epoch 460, training loss: 6.602365970611572
Epoch 470, training loss: 6.557816028594971
Epoch 480, training loss: 6.580881118774414
Epoch 490, training loss: 6.684210777282715
none
Accuracy: 0.769
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582128524780273
Epoch 10, training loss: 9.914756774902344
Epoch 20, training loss: 9.32827091217041
Epoch 30, training loss: 8.96451473236084
Epoch 40, training loss: 8.604669570922852
Epoch 50, training loss: 8.314077377319336
Epoch 60, training loss: 8.083693504333496
Epoch 70, training loss: 7.8252787590026855
Epoch 80, training loss: 7.717155456542969
Epoch 90, training loss: 7.599897861480713
Epoch 100, training loss: 7.478322982788086
Epoch 110, training loss: 7.535969257354736
Epoch 120, training loss: 7.308712959289551
Epoch 130, training loss: 7.214670658111572
Epoch 140, training loss: 7.185797691345215
Epoch 150, training loss: 7.108953475952148
Epoch 160, training loss: 7.0613789558410645
Epoch 170, training loss: 7.0504655838012695
Epoch 180, training loss: 6.984149932861328
Epoch 190, training loss: 6.987914085388184
Epoch 200, training loss: 6.902116775512695
Epoch 210, training loss: 6.9067840576171875
Epoch 220, training loss: 6.863430976867676
Epoch 230, training loss: 6.8312530517578125
Epoch 240, training loss: 6.842874526977539
Epoch 250, training loss: 6.79237174987793
Epoch 260, training loss: 6.814002513885498
Epoch 270, training loss: 6.786438465118408
Epoch 280, training loss: 6.773933410644531
Epoch 290, training loss: 6.751781463623047
Epoch 300, training loss: 6.713055610656738
Epoch 310, training loss: 6.767104625701904
Epoch 320, training loss: 6.7312846183776855
Epoch 330, training loss: 6.708966255187988
Epoch 340, training loss: 6.649339199066162
Epoch 350, training loss: 6.660332679748535
Epoch 360, training loss: 6.749883651733398
Epoch 370, training loss: 6.719639301300049
Epoch 380, training loss: 6.675172328948975
Epoch 390, training loss: 6.667916774749756
Epoch 400, training loss: 6.666820526123047
Epoch 410, training loss: 6.7055277824401855
Epoch 420, training loss: 6.6519293785095215
Epoch 430, training loss: 6.63112211227417
Epoch 440, training loss: 6.587764263153076
Epoch 450, training loss: 6.597878932952881
Epoch 460, training loss: 6.671498775482178
Epoch 470, training loss: 6.5902862548828125
Epoch 480, training loss: 6.599170684814453
Epoch 490, training loss: 6.606134414672852
none
Accuracy: 0.766
beta 0.9
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582061767578125
Epoch 10, training loss: 9.991113662719727
Epoch 20, training loss: 9.33080768585205
Epoch 30, training loss: 8.841456413269043
Epoch 40, training loss: 8.467711448669434
Epoch 50, training loss: 8.231693267822266
Epoch 60, training loss: 8.010363578796387
Epoch 70, training loss: 7.900204181671143
Epoch 80, training loss: 7.828152179718018
Epoch 90, training loss: 7.683271408081055
Epoch 100, training loss: 7.655172348022461
Epoch 110, training loss: 7.636040687561035
Epoch 120, training loss: 7.546522617340088
Epoch 130, training loss: 7.498989582061768
Epoch 140, training loss: 7.39046573638916
Epoch 150, training loss: 7.310685157775879
Epoch 160, training loss: 7.281294345855713
Epoch 170, training loss: 7.216382026672363
Epoch 180, training loss: 7.229907989501953
Epoch 190, training loss: 7.174417972564697
Epoch 200, training loss: 7.106014251708984
Epoch 210, training loss: 7.009888648986816
Epoch 220, training loss: 7.045970916748047
Epoch 230, training loss: 7.040412425994873
Epoch 240, training loss: 6.985745906829834
Epoch 250, training loss: 6.963717937469482
Epoch 260, training loss: 6.933715343475342
Epoch 270, training loss: 6.887454509735107
Epoch 280, training loss: 6.882700443267822
Epoch 290, training loss: 6.832432270050049
Epoch 300, training loss: 6.872900009155273
Epoch 310, training loss: 6.781867980957031
Epoch 320, training loss: 6.73780632019043
Epoch 330, training loss: 6.782629013061523
Epoch 340, training loss: 6.670341491699219
Epoch 350, training loss: 6.707729339599609
Epoch 360, training loss: 6.651447772979736
Epoch 370, training loss: 6.6296796798706055
Epoch 380, training loss: 6.636918544769287
Epoch 390, training loss: 6.591920852661133
Epoch 400, training loss: 6.589398384094238
Epoch 410, training loss: 6.5677571296691895
Epoch 420, training loss: 6.567960262298584
Epoch 430, training loss: 6.585086345672607
Epoch 440, training loss: 6.511384010314941
Epoch 450, training loss: 6.5553507804870605
Epoch 460, training loss: 6.476629734039307
Epoch 470, training loss: 6.54710578918457
Epoch 480, training loss: 6.490182399749756
Epoch 490, training loss: 6.541177272796631
none
Accuracy: 0.779
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582074165344238
Epoch 10, training loss: 10.021554946899414
Epoch 20, training loss: 9.289981842041016
Epoch 30, training loss: 8.852932929992676
Epoch 40, training loss: 8.559402465820312
Epoch 50, training loss: 8.27965259552002
Epoch 60, training loss: 8.089445114135742
Epoch 70, training loss: 8.072492599487305
Epoch 80, training loss: 7.810500144958496
Epoch 90, training loss: 7.693804740905762
Epoch 100, training loss: 7.6430182456970215
Epoch 110, training loss: 7.522586822509766
Epoch 120, training loss: 7.477652549743652
Epoch 130, training loss: 7.391724109649658
Epoch 140, training loss: 7.358510494232178
Epoch 150, training loss: 7.25349235534668
Epoch 160, training loss: 7.291184425354004
Epoch 170, training loss: 7.242791652679443
Epoch 180, training loss: 7.12145471572876
Epoch 190, training loss: 7.156170845031738
Epoch 200, training loss: 7.072432994842529
Epoch 210, training loss: 7.088698863983154
Epoch 220, training loss: 7.020719051361084
Epoch 230, training loss: 7.087604522705078
Epoch 240, training loss: 7.0089111328125
Epoch 250, training loss: 6.945931911468506
Epoch 260, training loss: 6.969264984130859
Epoch 270, training loss: 6.931849956512451
Epoch 280, training loss: 6.872613430023193
Epoch 290, training loss: 6.922219753265381
Epoch 300, training loss: 6.8598737716674805
Epoch 310, training loss: 6.873775005340576
Epoch 320, training loss: 6.831284046173096
Epoch 330, training loss: 6.755173206329346
Epoch 340, training loss: 6.7391886711120605
Epoch 350, training loss: 6.734009742736816
Epoch 360, training loss: 6.695937633514404
Epoch 370, training loss: 6.708428859710693
Epoch 380, training loss: 6.681294918060303
Epoch 390, training loss: 6.641571044921875
Epoch 400, training loss: 6.6333184242248535
Epoch 410, training loss: 6.686680793762207
Epoch 420, training loss: 6.694008827209473
Epoch 430, training loss: 6.602950572967529
Epoch 440, training loss: 6.666900634765625
Epoch 450, training loss: 6.593320369720459
Epoch 460, training loss: 6.520838737487793
Epoch 470, training loss: 6.557252407073975
Epoch 480, training loss: 6.565524101257324
Epoch 490, training loss: 6.48261833190918
none
Accuracy: 0.76
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 10.582387924194336
Epoch 10, training loss: 10.023262023925781
Epoch 20, training loss: 9.3484525680542
Epoch 30, training loss: 9.015775680541992
Epoch 40, training loss: 8.654911994934082
Epoch 50, training loss: 8.447793006896973
Epoch 60, training loss: 8.261150360107422
Epoch 70, training loss: 8.10191535949707
Epoch 80, training loss: 7.941571235656738
Epoch 90, training loss: 7.930477142333984
Epoch 100, training loss: 7.7706780433654785
Epoch 110, training loss: 7.635337829589844
Epoch 120, training loss: 7.589073181152344
Epoch 130, training loss: 7.518013000488281
Epoch 140, training loss: 7.428414344787598
Epoch 150, training loss: 7.3425164222717285
Epoch 160, training loss: 7.288300514221191
Epoch 170, training loss: 7.271042346954346
Epoch 180, training loss: 7.1980509757995605
Epoch 190, training loss: 7.171319484710693
Epoch 200, training loss: 7.1713337898254395
Epoch 210, training loss: 7.134018898010254
Epoch 220, training loss: 7.047747611999512
Epoch 230, training loss: 7.092472553253174
Epoch 240, training loss: 7.018949508666992
Epoch 250, training loss: 6.966158866882324
Epoch 260, training loss: 6.964996337890625
Epoch 270, training loss: 6.90748929977417
Epoch 280, training loss: 6.899590015411377
Epoch 290, training loss: 6.884832859039307
Epoch 300, training loss: 6.853013038635254
Epoch 310, training loss: 6.803851127624512
Epoch 320, training loss: 6.839973449707031
Epoch 330, training loss: 6.824718475341797
Epoch 340, training loss: 6.77744722366333
Epoch 350, training loss: 6.740322589874268
Epoch 360, training loss: 6.767627716064453
Epoch 370, training loss: 6.731565952301025
Epoch 380, training loss: 6.725987434387207
Epoch 390, training loss: 6.621451377868652
Epoch 400, training loss: 6.687192440032959
Epoch 410, training loss: 6.586121082305908
Epoch 420, training loss: 6.667311191558838
Epoch 430, training loss: 6.654891014099121
Epoch 440, training loss: 6.6885223388671875
Epoch 450, training loss: 6.606213569641113
Epoch 460, training loss: 6.609837532043457
Epoch 470, training loss: 6.558802604675293
Epoch 480, training loss: 6.60569429397583
Epoch 490, training loss: 6.496860980987549
none
Accuracy:   
